{"docstring": "\"\"\"\nRuns when user clicks on an \"open\" button next to a row in the my missions view\nSwitches view to a detailed view about the selected mission\n\"\"\"\n#Switch view\n", "func_signal": "def on_mission_info_button_clicked(self, widget, data=None):\n", "code": "self.main_notebook.set_current_page(4)\n#Sets texts in the new view\nif (self.mission_notebook.get_current_page() == 2):\n    self.selected_mission = self.finished_missions[self.mission_selected]\n\nelif (self.mission_notebook.get_current_page() == 1):\n    self.selected_mission = self.missions[self.mission_selected]\nelse:\n    self.selected_mission = self.my_missions[self.mission_selected]\n    \nself.builder.get_object(\"mission_dialog_title\").set_markup(\"<big>Uppdrag: \"+self.selected_mission.title+\"</big>\")\nself.builder.get_object(\"mission_dialog_status\").set_text(self.selected_mission.status_object.name)\n\n#Display description texts\nself.builder.get_object(\"mission_dialog_description\").get_buffer().set_text(self.selected_mission.descr)\n\n#Display mission images\nself.mission_images_liststore.clear()\nfor image in self.selected_mission.images:\n    self.mission_images_liststore.append((gtk.gdk.pixbuf_new_from_file(\"db/images/thumb_\"+image.filename), image.title, image.id))\n    \nself.mission_images_liststore.append((gtk.gdk.pixbuf_new_from_file(\"ikoner/camera.png\"),\"Markera ikonen och tryck \\n enter f\u00f6r att ta ny bild\", -1))", "path": "client\\src\\gui.py", "repo_name": "tddd36gr1/tddd36gr1", "stars": 8, "license": "None", "language": "python", "size": 16432}
{"docstring": "\"\"\"\nRuns when user presses save button in the detailed mission view,\nsaves all changes to the mission\n\"\"\"\n", "func_signal": "def mission_save(self, widget, data=None):\n", "code": "self.change_mission_status()\nbuffer = self.builder.get_object(\"mission_dialog_description\").get_buffer()\nself.selected_mission.descr = buffer.get_text(buffer.get_start_iter(),buffer.get_end_iter())\nself.db.add_or_update(self.selected_mission)", "path": "client\\src\\gui.py", "repo_name": "tddd36gr1/tddd36gr1", "stars": 8, "license": "None", "language": "python", "size": 16432}
{"docstring": "\"\"\"Constructor setting variables\"\"\"\n", "func_signal": "def __init__(self, title, long, lat, rad, status, descr):\n", "code": "self.title = title\nself.long = long\nself.lat = lat\nself.rad = rad\nself.status = status\nself.descr = descr", "path": "server\\src\\class_\\base_objects.py", "repo_name": "tddd36gr1/tddd36gr1", "stars": 8, "license": "None", "language": "python", "size": 16432}
{"docstring": "\"\"\"\nSetups main window of the program, links exit buttons\n\"\"\"\n", "func_signal": "def setup_window(self):\n", "code": "self.window = hildon.Window()\nself.window.set_title(\"TDDD36gr1 - Ignis ALPHA\")\n#Adds window to program\nself.add_window(self.window)\nself.window.fullscreen()\nself.window.connect(\"destroy\", gtk.main_quit)", "path": "client\\src\\gui.py", "repo_name": "tddd36gr1/tddd36gr1", "stars": 8, "license": "None", "language": "python", "size": 16432}
{"docstring": "\"\"\"Constructor setting variables\"\"\"\n", "func_signal": "def __init__(self, n810mac, fname, lname, online=False):\n", "code": "self.fname = fname\nself.lname = lname\nself.n810mac = n810mac\nself.online = online", "path": "server\\src\\class_\\base_objects.py", "repo_name": "tddd36gr1/tddd36gr1", "stars": 8, "license": "None", "language": "python", "size": 16432}
{"docstring": "\"\"\"\nRuns when the user selects a mission row in the Finished Missions-view\n\"\"\"\n", "func_signal": "def on_missions_finished_selected_changed(self, widget, data=None):\n", "code": "model, iter = self.mission_finished_treeview.get_selection().get_selected()\nself.mission_selected = model.get_path(iter)[0]\nself.mission_finished_button_layout.move(self.mission_finished_info_button, 0, self.mission_selected*32)", "path": "client\\src\\gui.py", "repo_name": "tddd36gr1/tddd36gr1", "stars": 8, "license": "None", "language": "python", "size": 16432}
{"docstring": "\"\"\"\nRuns when the user presses an \"open\"-button in the message inbox or sent-view\n\"\"\"\n\n#Switch view\n", "func_signal": "def open_message(self, widget, data=None):\n", "code": "self.main_notebook.set_current_page(5)\n\n#Sets texts in the new view\nif (self.message_notebook.get_current_page() == 0):\n    self.selected_message = self.inbox_messages[self.message_selected]\n    self.builder.get_object(\"message_top_label\").set_markup(\"<big>Meddelande fr\u00e5n: \"+self.selected_message.src_object.fname+\" \"+self.selected_message.src_object.lname+\"</big>\")\n\nelif (self.message_notebook.get_current_page() == 2):\n    self.selected_message = self.sent_messages[self.message_selected]\n    self.builder.get_object(\"message_top_label\").set_markup(\"<big>Meddelande till: \"+self.selected_message.dst_object.fname+\" \"+self.selected_message.dst_object.lname+\"</big>\")\n\nself.builder.get_object(\"message_text_label\").set_markup(self.selected_message.msg)", "path": "client\\src\\gui.py", "repo_name": "tddd36gr1/tddd36gr1", "stars": 8, "license": "None", "language": "python", "size": 16432}
{"docstring": "\"\"\"\nOpens up mission image in full screen when user presses enter or double-clicks a mission image\n\"\"\"\n", "func_signal": "def on_mission_image_activated(self, widget, data=None):\n", "code": "i = self.builder.get_object(\"mission_image_iconview\").get_selected_items()[0][0]\nif (i >= len(self.selected_mission.images)):\n    self.take_picture()\n    return\nself.main_notebook.set_current_page(6)\nimage = self.selected_mission.images[i]\nself.builder.get_object(\"mission_image_title\").set_markup(\"<big>\"+image.title+\"</big>\")\nself.builder.get_object(\"mission_full_image\").set_from_file(\"db/images/\"+image.filename)", "path": "client\\src\\gui.py", "repo_name": "tddd36gr1/tddd36gr1", "stars": 8, "license": "None", "language": "python", "size": 16432}
{"docstring": "\"\"\"String-representation of object in xml\"\"\"\n", "func_signal": "def __repr__(self):\n", "code": "s = \"<Employee>\"\ns += \"\\n\\t<id>%s</id>\" % (self.id)\ns += \"\\n\\t<n810mac>%s</n810mac>\" % (self.n810mac)\ns += \"\\n\\t<fname>%s</fname>\" % (self.fname)\ns += \"\\n\\t<lname>%s</lname>\" % (self.lname)\ns += \"\\n\\t<online>%s</online>\" % (self.online)\ns += \"\\n\\t<ip>%s</ip>\" % (self.ip)\ns += \"\\n</Employee>\"\nreturn s", "path": "server\\src\\class_\\base_objects.py", "repo_name": "tddd36gr1/tddd36gr1", "stars": 8, "license": "None", "language": "python", "size": 16432}
{"docstring": "\"\"\"String-representation of object in xml\"\"\"\n", "func_signal": "def __repr__(self):\n", "code": "s = \"<TextMessage>\"\ns += \"\\n\\t<id>%s</id>\" % (self.id)\ns += \"\\n\\t<source>%s</source>\" % (self.src)\ns += \"\\n\\t<destination>%s</destination>\" % (self.dst)\ns += \"\\n\\t<message>%s</message>\" % (self.msg)\ns += \"\\n</TextMessage>\"\nreturn s", "path": "server\\src\\class_\\base_objects.py", "repo_name": "tddd36gr1/tddd36gr1", "stars": 8, "license": "None", "language": "python", "size": 16432}
{"docstring": "\"\"\"\nRuns when the user selects a mission row in the My Missions-view\n\"\"\"\n", "func_signal": "def on_missions_my_selected_changed(self, widget, data=None):\n", "code": "model, iter = self.mission_my_treeview.get_selection().get_selected()\nself.mission_selected = model.get_path(iter)[0]\nself.mission_my_button_layout.move(self.mission_my_info_button, 0, self.mission_selected*32)", "path": "client\\src\\gui.py", "repo_name": "tddd36gr1/tddd36gr1", "stars": 8, "license": "None", "language": "python", "size": 16432}
{"docstring": "\"\"\"\nRuns when the user selects a message row in the Inbox Messages-view\n\"\"\"\n", "func_signal": "def on_message_inbox_selected_changed(self, widget, data=None):\n", "code": "model, iter = self.message_inbox_treeview.get_selection().get_selected()\nself.message_selected = model.get_path(iter)[0]\nself.message_inbox_button_layout.move(self.message_inbox_open_button, 0, self.message_selected*32)", "path": "client\\src\\gui.py", "repo_name": "tddd36gr1/tddd36gr1", "stars": 8, "license": "None", "language": "python", "size": 16432}
{"docstring": "\"\"\"\nRuns when user presses the clear button in the new message view\n\"\"\"\n", "func_signal": "def clear_message(self, widget, data=None):\n", "code": "entry = self.builder.get_object(\"message_new_receiver\")\nentry.set_text(\"\")\nentry.modify_base(gtk.STATE_NORMAL, gtk.gdk.color_parse(\"white\"))\nbuffer = self.builder.get_object(\"message_new_textview\").get_buffer()\nbuffer.set_text(\"\")\nself.builder.get_object(\"message_new_title\").set_markup(\"<big>Nytt meddelande</big>\")", "path": "client\\src\\gui.py", "repo_name": "tddd36gr1/tddd36gr1", "stars": 8, "license": "None", "language": "python", "size": 16432}
{"docstring": "\"\"\"\nPopulates the missions view\n\"\"\"\n", "func_signal": "def insert_missions(self):\n", "code": "self.mission_my_liststore.clear()\nself.my_missions = self.db.get_one_by_id(Employee, self.employee_id).missions\nfor mission in self.my_missions:\n    self.mission_my_liststore.append((mission.title, mission.status_object.name))\n    \nself.mission_all_liststore.clear()\nself.missions = self.db.get_all(Mission)\nfor mission in self.missions:\n    self.mission_all_liststore.append((mission.title, mission.status_object.name))\n\nself.mission_finished_liststore.clear()        \nself.finished_missions = self.db.get_all_finished_missions()\nfor mission in self.finished_missions:\n    self.mission_finished_liststore.append((mission.title, mission.status_object.name))", "path": "client\\src\\gui.py", "repo_name": "tddd36gr1/tddd36gr1", "stars": 8, "license": "None", "language": "python", "size": 16432}
{"docstring": "\"\"\"\nRuns when the messaging button in the main menu is clicked, change to messaging view\n\"\"\"\n", "func_signal": "def on_messaging_button_clicked(self, widget, data=None):\n", "code": "self.main_notebook.set_current_page(3)\nif (self.message_notified == True):\n    self.builder.get_object(\"messaging_button_img\").set_from_file(\"menybilder/brev.png\")\n    self.message_notified = False", "path": "client\\src\\gui.py", "repo_name": "tddd36gr1/tddd36gr1", "stars": 8, "license": "None", "language": "python", "size": 16432}
{"docstring": "\"\"\"Constructor setting variables\"\"\"\n", "func_signal": "def __init__(self, src, dst, msg):\n", "code": "self.src = src\nself.dst = dst\nself.msg = msg", "path": "server\\src\\class_\\base_objects.py", "repo_name": "tddd36gr1/tddd36gr1", "stars": 8, "license": "None", "language": "python", "size": 16432}
{"docstring": "\"\"\"\nRuns when the mission button in the main menu is clicked, change to mission view\n\"\"\"\n", "func_signal": "def on_mission_button_clicked(self, widget, data=None):\n", "code": "self.main_notebook.set_current_page(1)\nif (self.mission_notified == True):\n    self.builder.get_object(\"mission_button_img\").set_from_file(\"menybilder/uppdrag.png\")\n    self.mission_notified = False", "path": "client\\src\\gui.py", "repo_name": "tddd36gr1/tddd36gr1", "stars": 8, "license": "None", "language": "python", "size": 16432}
{"docstring": "\"\"\"\nRuns when the user selects a mission row in the Sent Messages-view\n\"\"\"\n", "func_signal": "def on_message_sent_selected_changed(self, widget, data=None):\n", "code": "model, iter = self.message_sent_treeview.get_selection().get_selected()\nself.message_selected = model.get_path(iter)[0]\nself.message_sent_button_layout.move(self.message_sent_open_button, 0, self.message_selected*32)", "path": "client\\src\\gui.py", "repo_name": "tddd36gr1/tddd36gr1", "stars": 8, "license": "None", "language": "python", "size": 16432}
{"docstring": "\"\"\"\nSwitch to map view and zoom to the mission's placemark in the opened mission view\n\"\"\"\n", "func_signal": "def mission_zoom_to_map(self, widget, data=None):\n", "code": "self.mapwidget.set_focus((float(self.selected_mission.lat), float(self.selected_mission.long)))\nself.mapwidget.set_zoom_level(15)\nself.main_notebook.set_current_page(0)", "path": "client\\src\\gui.py", "repo_name": "tddd36gr1/tddd36gr1", "stars": 8, "license": "None", "language": "python", "size": 16432}
{"docstring": "\"\"\"\nThis function is used to populate various data models\n\"\"\"\n", "func_signal": "def insert_data(self):\n", "code": "self.insert_missions()\nself.insert_messages()\nself.insert_statuscodes()", "path": "client\\src\\gui.py", "repo_name": "tddd36gr1/tddd36gr1", "stars": 8, "license": "None", "language": "python", "size": 16432}
{"docstring": "\"\"\" Locking is implemented via in-memory caches. No data is written to disk.  \"\"\"\n\n", "func_signal": "def do_LOCK(self):\n", "code": "dc = self.IFACE_CLASS\n\nlog.debug('LOCKing resource %s' % self.headers)\n\nbody = None\nif self.headers.has_key('Content-Length'):\n    l = self.headers['Content-Length']\n    body = self.rfile.read(atoi(l))\n\ndepth = self.headers.get('Depth', 'infinity')\n\nuri = urlparse.urljoin(self.get_baseuri(dc), self.path)\nuri = urllib.unquote(uri)\nlog.debug('do_LOCK: uri = %s' % uri)\n\nifheader = self.headers.get('If')\nalreadylocked = self._l_isLocked(uri)\nlog.debug('do_LOCK: alreadylocked = %s' % alreadylocked)\n\nif body and alreadylocked:\n    # Full LOCK request but resource already locked\n    self.responses[423] = ('Locked', 'Already locked')\n    return self.send_status(423)\n\nelif body and not ifheader:\n    # LOCK with XML information\n    data = self._lock_unlock_parse(body)\n    token, result = self._lock_unlock_create(uri, 'unknown', depth, data)\n\n    if result:\n        self.send_body(result, '207', 'Error', 'Error',\n                        'text/xml; charset=\"utf-8\"')\n\n    else:\n        lock = self._l_getLock(token)\n        self.send_body(lock.asXML(), '200', 'OK', 'OK',\n                        'text/xml; charset=\"utf-8\"',\n                        {'Lock-Token' : '<opaquelocktoken:%s>' % token})\n\n\nelse:\n    # refresh request - refresh lock timeout\n    taglist = IfParser(ifheader)\n    found = 0\n    for tag in taglist:\n        for listitem in tag.list:\n            token = tokenFinder(listitem)\n            if token and self._l_hasLock(token):\n                lock = self._l_getLock(token)\n                timeout = self.headers.get('Timeout', 'Infinite')\n                lock.setTimeout(timeout) # automatically refreshes\n                found = 1\n\n                self.send_body(lock.asXML(), \n                                '200', 'OK', 'OK', 'text/xml; encoding=\"utf-8\"')\n                break\n        if found: \n            break\n\n    # we didn't find any of the tokens mentioned - means\n    # that table was cleared or another error\n    if not found:\n        self.send_status(412) # precondition failed", "path": "DAV\\locks.py", "repo_name": "ashtons/pywebdav-iphone", "stars": 10, "license": "None", "language": "python", "size": 303}
{"docstring": "\"\"\" create a multistatus response for the prop names \"\"\"\n\n", "func_signal": "def create_propname(self):\n", "code": "dc=self._dataclass\n# create the document generator\ndoc = domimpl.createDocument(None, \"multistatus\", None)\nms = doc.documentElement\nms.setAttribute(\"xmlns:D\", \"DAV:\")\nms.tagName = 'D:multistatus'\n\nif self._depth==\"0\":\n    pnames=dc.get_propnames(self._uri)\n    re=self.mk_propname_response(self._uri,pnames, doc)\n    ms.appendChild(re)\n\nelif self._depth==\"1\":\n    pnames=dc.get_propnames(self._uri)\n    re=self.mk_propname_response(self._uri,pnames, doc)\n    ms.appendChild(re)\n\n    for newuri in dc.get_childs(self._uri):\n        pnames=dc.get_propnames(newuri)\n        re=self.mk_propname_response(newuri,pnames, doc)\n        ms.appendChild(re)\nelif self._depth=='infinity':\n    uri_list = [self._uri]\n    while uri_list:\n        uri = uri_list.pop()\n        pnames=dc.get_propnames(uri)\n        re=self.mk_propname_response(uri,pnames, doc)\n        ms.appendChild(re)\n        uri_childs = self._dataclass.get_childs(uri)\n        if uri_childs:\n            uri_list.extend(uri_childs)\n\nreturn doc.toxml(encoding=\"utf-8\")", "path": "DAV\\propfind.py", "repo_name": "ashtons/pywebdav-iphone", "stars": 10, "license": "None", "language": "python", "size": 303}
{"docstring": "\"\"\" Create the multistatus response \n\nThis will be delegated to the specific method\ndepending on which request (allprop, propname, prop)\nwas found.\n\nIf we get a PROPNAME then we simply return the list with empty\nvalues which we get from the interface class\n\nIf we get an ALLPROP we first get the list of properties and then\nwe do the same as with a PROP method.\n\n\"\"\"\n\n# check if resource exists\n", "func_signal": "def createResponse(self):\n", "code": "if not self._dataclass.exists(self._uri):\n    raise DAV_NotFound\n\ndf = None\nif self.request_type==RT_ALLPROP:\n    df = self.create_allprop()\n\nif self.request_type==RT_PROPNAME:\n    df = self.create_propname()\n\nif self.request_type==RT_PROP:\n    df = self.create_prop()\n\nif df != None:\n    return df\n\n# no body means ALLPROP!\ndf = self.create_allprop()\nreturn df", "path": "DAV\\propfind.py", "repo_name": "ashtons/pywebdav-iphone", "stars": 10, "license": "None", "language": "python", "size": 303}
{"docstring": "\"\"\" delete a resource \"\"\"\n\n", "func_signal": "def delone(self):\n", "code": "dc=self.__dataclass\nreturn dc.delone(self.__uri)", "path": "DAV\\delete.py", "repo_name": "ashtons/pywebdav-iphone", "stars": 10, "license": "None", "language": "python", "size": 303}
{"docstring": "\"\"\" delete a collection \"\"\"\n\n", "func_signal": "def delcol(self):\n", "code": "dc=self.__dataclass\nresult=dc.deltree(self.__uri)\n\nif not len(result.items()):\n    return None # everything ok\n\n# create the result element\nreturn make_xmlresponse(result)", "path": "build\\lib\\DAV\\delete.py", "repo_name": "ashtons/pywebdav-iphone", "stars": 10, "license": "None", "language": "python", "size": 303}
{"docstring": "\"\"\" Unlocks given resource \"\"\"\n\n", "func_signal": "def do_UNLOCK(self):\n", "code": "dc = self.IFACE_CLASS\n\nif self._config.DAV.getboolean('verbose') is True:\n    log.info('UNLOCKing resource %s' % self.headers)\n\nuri = urlparse.urljoin(self.get_baseuri(dc), self.path)\nuri = urllib.unquote(uri)\n\ntoken = tokenFinder(self.headers.get('Lock-Token'))\nif self._l_isLocked(uri):\n    self._l_delLock(token)\n\nself.send_body(None, '204', 'Ok', 'Ok')", "path": "DAV\\locks.py", "repo_name": "ashtons/pywebdav-iphone", "stars": 10, "license": "None", "language": "python", "size": 303}
{"docstring": "\"\"\" return a list of all properties \"\"\"\n", "func_signal": "def create_allprop(self):\n", "code": "self.proplist={}\nself.namespaces=[]\nfor ns,plist in self._dataclass.get_propnames(self._uri).items():\n    self.proplist[ns]=plist\n    self.namespaces.append(ns)\n\nreturn self.create_prop()", "path": "DAV\\propfind.py", "repo_name": "ashtons/pywebdav-iphone", "stars": 10, "license": "None", "language": "python", "size": 303}
{"docstring": "\"\"\" delete a resource \"\"\"\n\n", "func_signal": "def delone(self):\n", "code": "dc=self.__dataclass\nreturn dc.delone(self.__uri)", "path": "build\\lib\\DAV\\delete.py", "repo_name": "ashtons/pywebdav-iphone", "stars": 10, "license": "None", "language": "python", "size": 303}
{"docstring": "\"\"\" return the creationdate of a resource \"\"\"\n", "func_signal": "def _get_dav_creationdate(self,uri):\n", "code": "d=self.get_creationdate(uri)\n# format it\nreturn time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime(d))", "path": "build\\lib\\DAV\\iface.py", "repo_name": "ashtons/pywebdav-iphone", "stars": 10, "license": "None", "language": "python", "size": 303}
{"docstring": "\"\"\" make a new <prop> result element \n\nWe differ between the good props and the bad ones for\neach generating an extra <propstat>-Node (for each error\none, that means).\n\n\"\"\"\n", "func_signal": "def mk_prop_response(self,uri,good_props,bad_props,doc):\n", "code": "re=doc.createElement(\"D:response\")\n# append namespaces to response\nnsnum=0\nfor nsname in self.namespaces:\n    if nsname != 'DAV:':\n        re.setAttribute(\"xmlns:ns\"+str(nsnum),nsname)\n    nsnum=nsnum+1\n\n# write href information\nuparts=urlparse.urlparse(uri)\nfileloc=uparts[2]\nhref=doc.createElement(\"D:href\")\nhuri=doc.createTextNode(uparts[0]+'://'+'/'.join(uparts[1:2]) + urllib.quote(fileloc))\nhref.appendChild(huri)\nre.appendChild(href)\n\n# write good properties\nps=doc.createElement(\"D:propstat\")\nif good_props:\n    re.appendChild(ps)\n\ngp=doc.createElement(\"D:prop\")\nfor ns in good_props.keys():\n    if ns != 'DAV:':\n        ns_prefix=\"ns\"+str(self.namespaces.index(ns))+\":\"\n    else:\n        ns_prefix = 'D:'\n    for p,v in good_props[ns].items():\n\n        pe=doc.createElement(ns_prefix+str(p))\n        if hasattr(v, '__class__') and v.__class__.__name__ == 'Element':\n            pe.appendChild(v)\n        elif isinstance(v, list):\n            for val in v:\n                pe.appendChild(val)\n        else:\n            if p==\"resourcetype\":\n                if v==1:\n                    ve=doc.createElement(\"D:collection\")\n                    pe.appendChild(ve)\n            else:\n                ve=doc.createTextNode(v)\n                pe.appendChild(ve)\n\n        gp.appendChild(pe)\n\nps.appendChild(gp)\ns=doc.createElement(\"D:status\")\nt=doc.createTextNode(\"HTTP/1.1 200 OK\")\ns.appendChild(t)\nps.appendChild(s)\nre.appendChild(ps)\n\n# now write the errors!\nif len(bad_props.items()):\n\n    # write a propstat for each error code\n    for ecode in bad_props.keys():\n        ps=doc.createElement(\"D:propstat\")\n        re.appendChild(ps)\n        bp=doc.createElement(\"D:prop\")\n        ps.appendChild(bp)\n\n        for ns in bad_props[ecode].keys():\n            if ns != 'DAV:':\n                ns_prefix=\"ns\"+str(self.namespaces.index(ns))+\":\"\n            else:\n                ns_prefix = 'D:'\n\n            for p in bad_props[ecode][ns]:\n                pe=doc.createElement(ns_prefix+str(p))\n                bp.appendChild(pe)\n\n        s=doc.createElement(\"D:status\")\n        t=doc.createTextNode(utils.gen_estring(ecode))\n        s.appendChild(t)\n        ps.appendChild(s)\n        re.appendChild(ps)\n\n# return the new response element\nreturn re", "path": "DAV\\propfind.py", "repo_name": "ashtons/pywebdav-iphone", "stars": 10, "license": "None", "language": "python", "size": 303}
{"docstring": "\"\"\" Handle a HTTP request \"\"\"\n", "func_signal": "def handle(self):\n", "code": "try:\n    self._init_buffer()\n    BaseHTTPRequestHandler.handle(self)\n    self._flush()\nexcept Exception:\n    pass", "path": "build\\lib\\DAV\\BufferingHTTPServer.py", "repo_name": "ashtons/pywebdav-iphone", "stars": 10, "license": "None", "language": "python", "size": 303}
{"docstring": "\"\"\" return the last modified date of a resource \"\"\"\n", "func_signal": "def _get_dav_getlastmodified(self,uri):\n", "code": "d=self.get_lastmodified(uri)\n# format it\nreturn time.strftime(\"%a, %d %b %Y %H:%M:%S GMT\", time.gmtime(d))", "path": "build\\lib\\DAV\\iface.py", "repo_name": "ashtons/pywebdav-iphone", "stars": 10, "license": "None", "language": "python", "size": 303}
{"docstring": "\"\"\"Send a MIME header.\"\"\"\n", "func_signal": "def send_header(self, keyword, value):\n", "code": "if self.request_version != 'HTTP/0.9':\n    self._append(\"%s: %s\\r\\n\" % (keyword, value))", "path": "build\\lib\\DAV\\BufferingHTTPServer.py", "repo_name": "ashtons/pywebdav-iphone", "stars": 10, "license": "None", "language": "python", "size": 303}
{"docstring": "\"\"\" return the value of a given property\n\nuri        -- uri of the object to get the property of\nns        -- namespace of the property\npname        -- name of the property\n\"\"\"\n", "func_signal": "def get_prop(self,uri,ns,propname):\n", "code": "if self.M_NS.has_key(ns):\n    prefix=self.M_NS[ns]\nelse:\n    raise DAV_NotFound\nmname=prefix+\"_\"+propname.replace('-', '_')\ntry:\n    m=getattr(self,mname)            \n    r=m(uri)\n    return r\nexcept AttributeError:\n    raise DAV_NotFound", "path": "build\\lib\\DAV\\iface.py", "repo_name": "ashtons/pywebdav-iphone", "stars": 10, "license": "None", "language": "python", "size": 303}
{"docstring": "\"\"\" delete a collection \"\"\"\n\n", "func_signal": "def delcol(self):\n", "code": "dc=self.__dataclass\nresult=dc.deltree(self.__uri)\n\nif not len(result.items()):\n    return None # everything ok\n\n# create the result element\nreturn make_xmlresponse(result)", "path": "DAV\\delete.py", "repo_name": "ashtons/pywebdav-iphone", "stars": 10, "license": "None", "language": "python", "size": 303}
{"docstring": "\"\"\" handle a <prop> request\n\nThis will\n\n1. set up the <multistatus>-Framework\n\n2. read the property values for each URI \n   (which is dependant on the Depth header)\n   This is done by the get_propvalues() method.\n\n3. For each URI call the append_result() method\n   to append the actual <result>-Tag to the result\n   document.\n\nWe differ between \"good\" properties, which have been\nassigned a value by the interface class and \"bad\" \nproperties, which resulted in an error, either 404\n(Not Found) or 403 (Forbidden).\n\n\"\"\"\n\n\n# create the document generator\n", "func_signal": "def create_prop(self):\n", "code": "doc = domimpl.createDocument(None, \"multistatus\", None)\nms = doc.documentElement\nms.setAttribute(\"xmlns:D\", \"DAV:\")\nms.tagName = 'D:multistatus'\n\nif self._depth==\"0\":\n    gp,bp=self.get_propvalues(self._uri)\n    res=self.mk_prop_response(self._uri,gp,bp,doc)\n    ms.appendChild(res)\n\nelif self._depth==\"1\":\n    gp,bp=self.get_propvalues(self._uri)\n    res=self.mk_prop_response(self._uri,gp,bp,doc)\n    ms.appendChild(res)\n\n    for newuri in self._dataclass.get_childs(self._uri):\n        gp,bp=self.get_propvalues(newuri)\n        res=self.mk_prop_response(newuri,gp,bp,doc)\n        ms.appendChild(res)\nelif self._depth=='infinity':\n    uri_list = [self._uri]\n    while uri_list:\n        uri = uri_list.pop()\n        gp,bp=self.get_propvalues(uri)\n        res=self.mk_prop_response(uri,gp,bp,doc)\n        ms.appendChild(res)\n        uri_childs = self._dataclass.get_childs(uri)\n        if uri_childs:\n            uri_list.extend(uri_childs)\n\nreturn doc.toxml(encoding=\"utf-8\")", "path": "DAV\\propfind.py", "repo_name": "ashtons/pywebdav-iphone", "stars": 10, "license": "None", "language": "python", "size": 303}
{"docstring": "\"\"\" authenticate user \"\"\"\n\n", "func_signal": "def get_userinfo(self,user,pw,command):\n", "code": "if user == self._config.DAV.user and pw == self._config.DAV.password:\n    log.info('Successfully authenticated user %s' % user)\n    return 1\n\nlog.info('Authentication failed for user %s' % user)\nreturn 0", "path": "DAVServer\\fileauth.py", "repo_name": "ashtons/pywebdav-iphone", "stars": 10, "license": "None", "language": "python", "size": 303}
{"docstring": "\"\"\" return the value of a property\n\n\n\"\"\"\n", "func_signal": "def get_prop2(self,uri,ns,pname):\n", "code": "if lower(ns)==\"dav:\": return self.get_dav(uri,pname)\n\nraise DAV_NotFound", "path": "build\\lib\\DAV\\iface.py", "repo_name": "ashtons/pywebdav-iphone", "stars": 10, "license": "None", "language": "python", "size": 303}
{"docstring": "\"\"\" flush the buffer to wfile \"\"\"\n", "func_signal": "def _flush(self):\n", "code": "self.wfile.write(self.__buffer)\nself.wfile.flush()\nself.__buffer=\"\"", "path": "build\\lib\\DAV\\BufferingHTTPServer.py", "repo_name": "ashtons/pywebdav-iphone", "stars": 10, "license": "None", "language": "python", "size": 303}
{"docstring": "\"\"\"Send the blank line ending the MIME headers.\"\"\"\n", "func_signal": "def end_headers(self):\n", "code": "if self.request_version != 'HTTP/0.9':\n    self._append(\"\\r\\n\")", "path": "build\\lib\\DAV\\BufferingHTTPServer.py", "repo_name": "ashtons/pywebdav-iphone", "stars": 10, "license": "None", "language": "python", "size": 303}
{"docstring": "\"\"\" Return an SSLFactory, based on if M2Crypto is available. \"\"\"\n", "func_signal": "def get_factory(ssl_ca_cert = None, ssl_context = None):\n", "code": "if have_m2crypto:\n    return M2SSLFactory(ssl_ca_cert, ssl_context)\nelse:\n    # Log here if someone provides the args but we don't use them.\n    if ssl_ca_cert or ssl_context:\n        if DEBUG:\n            DEBUG.warning(\"SSL arguments supplied, but M2Crypto is not available. \"\n                    \"Using Python SSL.\")\n    return SSLFactory()", "path": "sslfactory.py", "repo_name": "excid3/urlgrabber", "stars": 8, "license": "None", "language": "python", "size": 115}
{"docstring": "\"\"\"Convert a range tuple to a Range header value.\nReturn a string of the form \"bytes=<firstbyte>-<lastbyte>\" or None\nif no range is needed.\n\"\"\"\n", "func_signal": "def range_tuple_to_header(range_tup):\n", "code": "if range_tup is None: return None\nrange_tup = range_tuple_normalize(range_tup)\nif range_tup:\n    if range_tup[1]: \n        range_tup = (range_tup[0],range_tup[1] - 1)\n    return 'bytes=%s-%s' % range_tup", "path": "byterange.py", "repo_name": "excid3/urlgrabber", "stars": 8, "license": "None", "language": "python", "size": 115}
{"docstring": "# OVERRIDE IDEAS:\n#   shuffle gr list\n", "func_signal": "def _load_gr(self, gr):\n", "code": "self._lock.acquire()\ngr.mirrors = list(self.mirrors)\ngr._next = self._next\nself._lock.release()", "path": "mirror.py", "repo_name": "excid3/urlgrabber", "stars": 8, "license": "None", "language": "python", "size": 115}
{"docstring": "\"\"\"Create a RangeableFileObject.\nfo       -- a file like object. only the read() method need be \n            supported but supporting an optimized seek() is \n            preferable.\nrangetup -- a (firstbyte,lastbyte) tuple specifying the range\n            to work over.\nThe file object provided is assumed to be at byte offset 0.\n\"\"\"\n", "func_signal": "def __init__(self, fo, rangetup):\n", "code": "self.fo = fo\n(self.firstbyte, self.lastbyte) = range_tuple_normalize(rangetup)\nself.realpos = 0\nself._do_seek(self.firstbyte)", "path": "byterange.py", "repo_name": "excid3/urlgrabber", "stars": 8, "license": "None", "language": "python", "size": 115}
{"docstring": "\"\"\"Handles calculating the amount of data to read based on\nthe range.\n\"\"\"\n", "func_signal": "def _calc_read_size(self, size):\n", "code": "if self.lastbyte:\n    if size > -1:\n        if ((self.realpos + size) >= self.lastbyte):\n            size = (self.lastbyte - self.realpos)\n    else:\n        size = (self.lastbyte - self.realpos)\nreturn size", "path": "byterange.py", "repo_name": "excid3/urlgrabber", "stars": 8, "license": "None", "language": "python", "size": 115}
{"docstring": "\"\"\"Seek based on whether wrapped object supports seek().\noffset is relative to the current position (self.realpos).\n\"\"\"\n", "func_signal": "def _do_seek(self,offset):\n", "code": "assert offset >= 0\nif not hasattr(self.fo, 'seek'):\n    self._poor_mans_seek(offset)\nelse:\n    self.fo.seek(self.realpos + offset)\nself.realpos+= offset", "path": "byterange.py", "repo_name": "excid3/urlgrabber", "stars": 8, "license": "None", "language": "python", "size": 115}
{"docstring": "\"\"\"Seek within the byte range.\nPositioning is identical to that described under tell().\n\"\"\"\n", "func_signal": "def seek(self,offset,whence=0):\n", "code": "assert whence in (0, 1, 2)\nif whence == 0:   # absolute seek\n    realoffset = self.firstbyte + offset\nelif whence == 1: # relative seek\n    realoffset = self.realpos + offset\nelif whence == 2: # absolute from end of file\n    # XXX: are we raising the right Error here?\n    raise IOError('seek from end of file not supported.')\n\n# do not allow seek past lastbyte in range\nif self.lastbyte and (realoffset >= self.lastbyte):\n    realoffset = self.lastbyte\n\nself._do_seek(realoffset - self.realpos)", "path": "byterange.py", "repo_name": "excid3/urlgrabber", "stars": 8, "license": "None", "language": "python", "size": 115}
{"docstring": "\"\"\"Get a (firstbyte,lastbyte) tuple from a Range header value.\n\nRange headers have the form \"bytes=<firstbyte>-<lastbyte>\". This\nfunction pulls the firstbyte and lastbyte values and returns\na (firstbyte,lastbyte) tuple. If lastbyte is not specified in\nthe header value, it is returned as an empty string in the\ntuple.\n\nReturn None if range_header is None\nReturn () if range_header does not conform to the range spec \npattern.\n\n\"\"\"\n", "func_signal": "def range_header_to_tuple(range_header):\n", "code": "global _rangere\nif range_header is None: return None\nif _rangere is None:\n    import re\n    _rangere = re.compile(r'^bytes=(\\d{1,})-(\\d*)')\nmatch = _rangere.match(range_header)\nif match: \n    tup = range_tuple_normalize(match.group(1,2))\n    if tup and tup[1]: \n        tup = (tup[0],tup[1]+1)\n    return tup\nreturn ()", "path": "byterange.py", "repo_name": "excid3/urlgrabber", "stars": 8, "license": "None", "language": "python", "size": 115}
{"docstring": "\"\"\"Read within the range.\nThis method will limit the size read based on the range.\n\"\"\"\n", "func_signal": "def read(self, size=-1):\n", "code": "size = self._calc_read_size(size)\nrslt = self.fo.read(size)\nself.realpos += len(rslt)\nreturn rslt", "path": "byterange.py", "repo_name": "excid3/urlgrabber", "stars": 8, "license": "None", "language": "python", "size": 115}
{"docstring": "\"\"\"Initialize the MirrorGroup object.\n\nREQUIRED ARGUMENTS\n\n  grabber  - URLGrabber instance\n  mirrors  - a list of mirrors\n\nOPTIONAL ARGUMENTS\n\n  failure_callback  - callback to be used when a mirror fails\n  default_action    - dict of failure actions\n\nSee the module-level and class level documentation for more\ndetails.\n\"\"\"\n\n# OVERRIDE IDEAS:\n#   shuffle the list to randomize order\n", "func_signal": "def __init__(self, grabber, mirrors, **kwargs):\n", "code": "self.grabber = grabber\nself.mirrors = self._parse_mirrors(mirrors)\nself._next = 0\nself._lock = thread.allocate_lock()\nself.default_action = None\nself._process_kwargs(kwargs)", "path": "mirror.py", "repo_name": "excid3/urlgrabber", "stars": 8, "license": "None", "language": "python", "size": 115}
{"docstring": "# OVERRIDE IDEAS:\n#   return a random mirror so that multiple mirrors get used\n#   even without failures.\n", "func_signal": "def _get_mirror(self, gr):\n", "code": "if not gr.mirrors:\n    raise URLGrabError(256, _('No more mirrors to try.'))\nreturn gr.mirrors[gr._next]", "path": "mirror.py", "repo_name": "excid3/urlgrabber", "stars": 8, "license": "None", "language": "python", "size": 115}
{"docstring": "\"\"\"Initialize the object\n\nThe arguments for intialization are the same as for MirrorGroup\n\"\"\"\n", "func_signal": "def __init__(self, grabber, mirrors, **kwargs):\n", "code": "MirrorGroup.__init__(self, grabber, mirrors, **kwargs)\nrandom.shuffle(self.mirrors)", "path": "mirror.py", "repo_name": "excid3/urlgrabber", "stars": 8, "license": "None", "language": "python", "size": 115}
{"docstring": "\"\"\"Seek by calling the wrapped file objects read() method.\nThis is used for file like objects that do not have native\nseek support. The wrapped objects read() method is called\nto manually seek to the desired position.\noffset -- read this number of bytes from the wrapped\n          file object.\nraise RangeError if we encounter EOF before reaching the \nspecified offset.\n\"\"\"\n", "func_signal": "def _poor_mans_seek(self,offset):\n", "code": "pos = 0\nbufsize = 1024\nwhile pos < offset:\n    if (pos + bufsize) > offset:\n        bufsize = offset - pos\n    buf = self.fo.read(bufsize)\n    if len(buf) != bufsize:\n        raise RangeError('Requested Range Not Satisfiable')\n    pos+= bufsize", "path": "byterange.py", "repo_name": "excid3/urlgrabber", "stars": 8, "license": "None", "language": "python", "size": 115}
{"docstring": "\"\"\"Read lines within the range.\nThis method will limit the size read based on the range.\n\"\"\"\n", "func_signal": "def readline(self, size=-1):\n", "code": "size = self._calc_read_size(size)\nrslt = self.fo.readline(size)\nself.realpos += len(rslt)\nreturn rslt", "path": "byterange.py", "repo_name": "excid3/urlgrabber", "stars": 8, "license": "None", "language": "python", "size": 115}
{"docstring": "\"\"\"Initialize the object\n\nThe arguments for intialization are the same as for MirrorGroup\n\"\"\"\n", "func_signal": "def __init__(self, grabber, mirrors, **kwargs):\n", "code": "MirrorGroup.__init__(self, grabber, mirrors, **kwargs)\nself._next = random.randrange(len(mirrors))", "path": "mirror.py", "repo_name": "excid3/urlgrabber", "stars": 8, "license": "None", "language": "python", "size": 115}
{"docstring": "\"\"\"\nCreate an ssl context using the CA cert file or ssl context.\n\nThe CA cert is used first if it was passed as an option. If not,\nthen the supplied ssl context is used. If no ssl context was supplied,\nNone is returned.\n\"\"\"\n", "func_signal": "def _get_ssl_context(self, ssl_ca_cert, ssl_context):\n", "code": "if ssl_ca_cert:\n    context = SSL.Context()\n    context.load_verify_locations(ssl_ca_cert)\n    context.set_verify(SSL.verify_peer, -1)\n    return context\nelse:\n    return ssl_context", "path": "sslfactory.py", "repo_name": "excid3/urlgrabber", "stars": 8, "license": "None", "language": "python", "size": 115}
{"docstring": "# 206 Partial Content Response\n", "func_signal": "def http_error_206(self, req, fp, code, msg, hdrs):\n", "code": "r = urllib.addinfourl(fp, hdrs, req.get_full_url())\nr.code = code\nr.msg = msg\nreturn r", "path": "byterange.py", "repo_name": "excid3/urlgrabber", "stars": 8, "license": "None", "language": "python", "size": 115}
{"docstring": "\"\"\"Tell the mirror object increment the mirror index\n\nThis increments the mirror index, which amounts to telling the\nmirror object to use a different mirror (for this and future\ndownloads).\n\nThis is a SEMI-public method.  It will be called internally,\nand you may never need to call it.  However, it is provided\n(and is made public) so that the calling program can increment\nthe mirror choice for methods like urlopen.  For example, with\nurlopen, there's no good way for the mirror group to know that\nan error occurs mid-download (it's already returned and given\nyou the file object).\n\nremove  ---  can have several values\n   0   do not remove the mirror from the list\n   1   remove the mirror for this download only\n   2   remove the mirror permanently\n\nbeware of remove=0 as it can lead to infinite loops\n\"\"\"\n", "func_signal": "def increment_mirror(self, gr, action={}):\n", "code": "badmirror = gr.mirrors[gr._next]\n\nself._lock.acquire()\ntry:\n    ind = self.mirrors.index(badmirror)\nexcept ValueError:\n    pass\nelse:\n    if action.get('remove_master', 0):\n        del self.mirrors[ind]\n    elif self._next == ind and action.get('increment_master', 1):\n        self._next += 1\n    if self._next >= len(self.mirrors): self._next = 0\nself._lock.release()\n\nif action.get('remove', 1):\n    del gr.mirrors[gr._next]\nelif action.get('increment', 1):\n    gr._next += 1\nif gr._next >= len(gr.mirrors): gr._next = 0\n\nif DEBUG:\n    grm = [m['mirror'] for m in gr.mirrors]\n    DEBUG.info('GR   mirrors: [%s] %i', ' '.join(grm), gr._next)\n    selfm = [m['mirror'] for m in self.mirrors]\n    DEBUG.info('MAIN mirrors: [%s] %i', ' '.join(selfm), self._next)", "path": "mirror.py", "repo_name": "excid3/urlgrabber", "stars": 8, "license": "None", "language": "python", "size": 115}
{"docstring": "# OVERRIDE IDEAS:\n#   inspect the error - remove=1 for 404, remove=2 for connection\n#                       refused, etc. (this can also be done via\n#                       the callback)\n", "func_signal": "def _failure(self, gr, cb_obj):\n", "code": "cb = gr.kw.get('failure_callback') or self.failure_callback\nif cb:\n    if type(cb) == type( () ):\n        cb, args, kwargs = cb\n    else:\n        args, kwargs = (), {}\n    action = cb(cb_obj, *args, **kwargs) or {}\nelse:\n    action = {}\n# XXXX - decide - there are two ways to do this\n# the first is action-overriding as a whole - use the entire action\n# or fall back on module level defaults\n#action = action or gr.kw.get('default_action') or self.default_action\n# the other is to fall through for each element in the action dict\na = dict(self.default_action or {})\na.update(gr.kw.get('default_action', {}))\na.update(action)\naction = a\nself.increment_mirror(gr, action)\nif action and action.get('fail', 0): raise", "path": "mirror.py", "repo_name": "excid3/urlgrabber", "stars": 8, "license": "None", "language": "python", "size": 115}
{"docstring": "\"\"\"Normalize a (first_byte,last_byte) range tuple.\nReturn a tuple whose first element is guaranteed to be an int\nand whose second element will be '' (meaning: the last byte) or \nan int. Finally, return None if the normalized tuple == (0,'')\nas that is equivelant to retrieving the entire file.\n\"\"\"\n", "func_signal": "def range_tuple_normalize(range_tup):\n", "code": "if range_tup is None: return None\n# handle first byte\nfb = range_tup[0]\nif fb in (None,''): fb = 0\nelse: fb = int(fb)\n# handle last byte\ntry: lb = range_tup[1]\nexcept IndexError: lb = ''\nelse:  \n    if lb is None: lb = ''\n    elif lb != '': lb = int(lb)\n# check if range is over the entire file\nif (fb,lb) == (0,''): return None\n# check that the range is valid\nif lb < fb: raise RangeError('Invalid byte range: %s-%s' % (fb,lb))\nreturn (fb,lb)", "path": "byterange.py", "repo_name": "excid3/urlgrabber", "stars": 8, "license": "None", "language": "python", "size": 115}
{"docstring": "\"\"\"returns the attributes of the object at `path`.\"\"\"\n", "func_signal": "def getattr(self, path):\n", "code": "if path == \"/\":\n\treturn self.root_meta\ntry:\n\treturn _MetaData(self._bt.get_meta_data(path))\nexcept KeyError:\n\t_raise_io(errno.ENOENT, path)", "path": "kvfs.py", "repo_name": "qznc/kvfs", "stars": 10, "license": "None", "language": "python", "size": 110}
{"docstring": "\"\"\"removes a file or directory\"\"\"\n", "func_signal": "def remove(self, path):\n", "code": "try:\n\tself._bt.unlink(path)\nexcept KeyError:\n\t_raise_io(errno.ENOENT, path)", "path": "kvfs.py", "repo_name": "qznc/kvfs", "stars": 10, "license": "None", "language": "python", "size": 110}
{"docstring": "\"\"\"return meta data from data object at path\"\"\"\n", "func_signal": "def get_meta_data(self, path):\n", "code": "blob_line = self._get_blob_line(path)\ndir = blob_line[-2]\nname = os.path.basename(path)\nreturn dir.get_meta(name)", "path": "blobtree.py", "repo_name": "qznc/kvfs", "stars": 10, "license": "None", "language": "python", "size": 110}
{"docstring": "\"\"\"overwrite meta data\"\"\"\n", "func_signal": "def set_meta_data(self, path, meta):\n", "code": "blob_line = self._get_blob_line(path)\ndir = blob_line[-2]\nname = os.path.basename(path)\ndirname = os.path.dirname(path)\nif dirname.endswith(os.sep):\n\tdirname = dirname[:-len(os.sep)]\ndir.set_meta(name, meta)\nself._kv[dir.id] = str(dir)\nself._save_path(dirname, dir)", "path": "blobtree.py", "repo_name": "qznc/kvfs", "stars": 10, "license": "None", "language": "python", "size": 110}
{"docstring": "\"\"\"create a tree object at path\"\"\"\n", "func_signal": "def create_subtree(self, path, meta=\"\"):\n", "code": "end_blob = _TreeBlob(dict())\nself._kv[end_blob.id] = str(end_blob)\nself._save_path(path, end_blob, meta)", "path": "blobtree.py", "repo_name": "qznc/kvfs", "stars": 10, "license": "None", "language": "python", "size": 110}
{"docstring": "\"\"\"read data from a file\"\"\"\n", "func_signal": "def read(self, path, length=2000000000, offset=0):\n", "code": "data = self._get_data(path)\nreturn data[offset:offset+length]", "path": "kvfs.py", "repo_name": "qznc/kvfs", "stars": 10, "license": "None", "language": "python", "size": 110}
{"docstring": "# behave according to Fuse\n", "func_signal": "def _raise_io(number, path=None):\n", "code": "msg = os.strerror(number)\nif path:\n\tmsg = path+\": \"+msg\nerr = IOError(msg)\nerr.errno = number\nraise err", "path": "kvfs.py", "repo_name": "qznc/kvfs", "stars": 10, "license": "None", "language": "python", "size": 110}
{"docstring": "\"\"\"put data into data object at path\"\"\"\n", "func_signal": "def set_data(self, path, data, meta=None):\n", "code": "new_blob = _DataBlob(data)\nself._kv[new_blob.id] = str(new_blob)\nself._save_path(path, new_blob, meta)", "path": "blobtree.py", "repo_name": "qznc/kvfs", "stars": 10, "license": "None", "language": "python", "size": 110}
{"docstring": "\"\"\"rename a file (note that directories may change)\"\"\"\n", "func_signal": "def rename(self, old, new):\n", "code": "try:\n\tself._bt.rename(old, new)\nexcept KeyError:\n\t_raise_io(errno.ENOENT, old)", "path": "kvfs.py", "repo_name": "qznc/kvfs", "stars": 10, "license": "None", "language": "python", "size": 110}
{"docstring": "\"\"\"create a data object at path (reset to empty if already exists)\"\"\"\n", "func_signal": "def create_data(self, path, meta=\"\"):\n", "code": "end_blob = _DataBlob(\"\")\nself._kv[end_blob.id] = str(end_blob)\nself._save_path(path, end_blob, meta)", "path": "blobtree.py", "repo_name": "qznc/kvfs", "stars": 10, "license": "None", "language": "python", "size": 110}
{"docstring": "\"\"\"insert an existing blob child by id\"\"\"\n", "func_signal": "def insert(self, name, id, meta):\n", "code": "assert not os.path.sep in name\nself.contents[name] = (id,meta)", "path": "blobtree.py", "repo_name": "qznc/kvfs", "stars": 10, "license": "None", "language": "python", "size": 110}
{"docstring": "\"\"\"create a file\"\"\"\n", "func_signal": "def create(self, path):\n", "code": "if self._bt.exists(path):\n\t_raise_io(errno.EEXIST, path)\nm = _MetaData()\nm['st_mode'] = m['st_mode'] | stat.S_IFREG\nm['st_size'] = 0\nself._bt.create_data(path, str(m))", "path": "kvfs.py", "repo_name": "qznc/kvfs", "stars": 10, "license": "None", "language": "python", "size": 110}
{"docstring": "\"\"\"returns list of sub elements of `path`\"\"\"\n", "func_signal": "def list_dir(self, path):\n", "code": "blob_line = self._get_blob_line(path)\nreturn blob_line[-1].list_childs()", "path": "blobtree.py", "repo_name": "qznc/kvfs", "stars": 10, "license": "None", "language": "python", "size": 110}
{"docstring": "\"\"\"return data from data object at path\"\"\"\n", "func_signal": "def get_data(self, path):\n", "code": "blob_line = self._get_blob_line(path)\nif not isinstance(blob_line[-1], _DataBlob):\n\traise TypeError(\"not a data object at \"+path)\nreturn blob_line[-1].data", "path": "blobtree.py", "repo_name": "qznc/kvfs", "stars": 10, "license": "None", "language": "python", "size": 110}
{"docstring": "\"\"\"get a list of blobs representing the path\"\"\"\n", "func_signal": "def _get_blob_line(self, path):\n", "code": "if path.endswith(os.sep):\n\tpath = path[:-len(os.sep)]\n\t# path may be '' now\nif not path:\n\treturn [self._get_root_blob()]\nassert path.startswith(os.sep)\npath = path[len(os.sep):]\ncurrent = self._get_root_blob()\nline = [current]\npath, id, meta = current.resolve(path)\nwhile path:\n\tcurrent = _parse(self._kv[id])\n\tline.append(current)\n\tpath, id, meta = current.resolve(path)\nline.append(_parse(self._kv[id]))\nreturn line", "path": "blobtree.py", "repo_name": "qznc/kvfs", "stars": 10, "license": "None", "language": "python", "size": 110}
{"docstring": "\"\"\"create a symbolic link target->name\"\"\"\n", "func_signal": "def symlink(self, target, name):\n", "code": "if self._bt.exists(target):\n\t_raise_io(errno.EEXIST, target)\nm = _MetaData()\n# use attributes to save target and link property\nm['symlink'] = name\nm['st_mode'] = m['st_mode'] | stat.S_IFLNK\nself._bt.create_data(target, str(m))", "path": "kvfs.py", "repo_name": "qznc/kvfs", "stars": 10, "license": "None", "language": "python", "size": 110}
{"docstring": "\"\"\"read contents of a directory\"\"\"\n", "func_signal": "def readdir(self, path):\n", "code": "try:\n\tfiles = self._bt.list_dir(path)\nexcept KeyError:\n\t_raise_io(errno.ENOENT, path)\nyield '.'\nyield '..'\nfor f in files:\n\tyield f", "path": "kvfs.py", "repo_name": "qznc/kvfs", "stars": 10, "license": "None", "language": "python", "size": 110}
{"docstring": "\"\"\"testing basic root properties\"\"\"\n", "func_signal": "def test_basic_root():\n", "code": "K = KVFS(dict())\nattr = K.getattr(\"/\")\nassert stat.S_ISDIR(attr['st_mode'])\ndir = list(K.readdir(\"/\"))\nassert len(dir) == 2\nassert '.' in dir\nassert '..' in dir", "path": "kvfs_test.py", "repo_name": "qznc/kvfs", "stars": 10, "license": "None", "language": "python", "size": 110}
{"docstring": "\"\"\"get meta data of child\"\"\"\n", "func_signal": "def get_meta(self, name):\n", "code": "id, meta = self.contents[name]\nreturn meta", "path": "blobtree.py", "repo_name": "qznc/kvfs", "stars": 10, "license": "None", "language": "python", "size": 110}
{"docstring": "\"\"\"get data from path or raise IOERROR\"\"\"\n", "func_signal": "def _get_data(self, path):\n", "code": "try:\n\treturn self._bt.get_data(path)\nexcept KeyError:\n\t_raise_io(errno.ENOENT, path)\nexcept TypeError:\n\t_raise_io(errno.EISDIR, path)", "path": "kvfs.py", "repo_name": "qznc/kvfs", "stars": 10, "license": "None", "language": "python", "size": 110}
{"docstring": "# Adding field 'Word.language'\n", "func_signal": "def backwards(self, orm):\n", "code": "db.add_column('minerva_word', 'language', self.gf('django.db.models.fields.CharField')(default=1, max_length=3), keep_default=False)\n\n# Changing field 'Word.lang_code'\ndb.alter_column('minerva_word', 'lang_code_id', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['minerva.Language'], null=True))", "path": "minerva\\migrations\\0015_remove_language_from_word_model.py", "repo_name": "malcolmt/remember_me", "stars": 9, "license": "other", "language": "python", "size": 881}
{"docstring": "\"\"\"\nNull out all the Word.lang_code fields after updating language fields.\n\"\"\"\n", "func_signal": "def backwards(self, orm):\n", "code": "for code in (\"zho\", \"jpn\"):\n    linked_pk = orm.Language.objects.get(code=code).pk\n    orm.Word.objects.filter(lang_code=linked_pk).update(language=code)\norm.Word.objects.update(lang_code=None)", "path": "minerva\\migrations\\0014_populate_word_lang_code.py", "repo_name": "malcolmt/remember_me", "stars": 9, "license": "other", "language": "python", "size": 881}
{"docstring": "\"\"\"\nCopy all UserProfile objects over to Profile objects.\n\"\"\"\n", "func_signal": "def forwards(self, orm):\n", "code": "for obj in orm.UserProfile.objects.all():\n    orm.Profile(user_id=obj.student_id, language=obj.language).save()", "path": "minerva\\migrations\\0018_copy_userprofile_data_to_profile.py", "repo_name": "malcolmt/remember_me", "stars": 9, "license": "other", "language": "python", "size": 881}
{"docstring": "# Deleting model 'Word'\n", "func_signal": "def backwards(self, orm):\n", "code": "        db.delete_table('minerva_word')\n\n        # Deleting model 'Progress'\n        db.delete_table('minerva_progress')", "path": "minerva\\migrations\\0001_initial.py", "repo_name": "malcolmt/remember_me", "stars": 9, "license": "other", "language": "python", "size": 881}
{"docstring": "\"\"\"\nFor now, just update the correct answer with the data.\n\"\"\"\n# FIXME - how do I get a question form to validate across fields and\n# against the db?\n", "func_signal": "def validate_answer(request, query_base):\n", "code": "query = dict(query_base)\nform = QuestionForm(request.POST)\nif not form.is_valid():\n    # Either form-tampering or submitting without data. In either case,\n    # we'll just ignore it and generate a new question.\n    return {}\ndata = form.cleaned_data\nword = Word.objects.get(id=data[\"meta\"][0])\n\nprocess_answer(query, data)\nquery['word'] = word\n\nprogress, _ = Progress.objects.get_or_create(**query)\nprogress.attempts += 1\nresult = {\n        \"prev_id\": word.id,\n        \"prev_word\": word.word,\n        \"prev_meaning\": word.meaning\n        }\nif int(form.cleaned_data['answer']) == word.pk:\n    progress.correct += 1\n    result[\"prev_result\"] = True\nelse:\n    result[\"prev_result\"] = False\n    \nprogress.save()\nreturn result", "path": "minerva\\views.py", "repo_name": "malcolmt/remember_me", "stars": 9, "license": "other", "language": "python", "size": 881}
{"docstring": "\"\"\"\nReturns a question and multiple plausible answers, given the user, their\npreferred language and targeted skill level.\n\nThe returned data structure is a nested pair:\n    (\n        [question id, question string],\n        [(answer id, answer string), ...]\n    )\n\"\"\"\n# TODO: Right now, this is very simple (doesn't take into account the\n# user's performance at all). Eventually, it will be a graded selection\n# based on what the user finds difficult, etc.\n", "func_signal": "def create_question(user, language, level, num_choices=4):\n", "code": "pks = word_keys(language, level)\nsampled_words = list(Word.objects.filter(pk__in=random.sample(pks,\n        num_choices)))\nquestion_attribute, answer_attribute = random.choice(QUESTION_SCENARIOS)\nquestion = random.choice(sampled_words)\nquestion_data = (question.pk, getattr(question, question_attribute))\nanswers = [(item.pk, getattr(item, answer_attribute)) \n        for item in sampled_words]\nreturn question_data, answers", "path": "minerva\\questions.py", "repo_name": "malcolmt/remember_me", "stars": 9, "license": "other", "language": "python", "size": 881}
{"docstring": "# Adding field 'Progress.anon_student'\n", "func_signal": "def forwards(self, orm):\n", "code": "        db.add_column('minerva_progress', 'anon_student', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['sessions.Session'], null=True, blank=True), keep_default=False)\n\n        # Changing field 'Progress.student'\n        db.alter_column('minerva_progress', 'student_id', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['auth.User'], null=True))", "path": "minerva\\migrations\\0005_anon_users_progress_tracking.py", "repo_name": "malcolmt/remember_me", "stars": 9, "license": "other", "language": "python", "size": 881}
{"docstring": "# Adding model 'Language'\n", "func_signal": "def forwards(self, orm):\n", "code": "        db.create_table('minerva_language', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('code', self.gf('django.db.models.fields.CharField')(max_length=3)),\n            ('descriptive_name', self.gf('django.db.models.fields.CharField')(max_length=50)),\n        ))\n        db.send_create_signal('minerva', ['Language'])", "path": "minerva\\migrations\\0011_add_language_model.py", "repo_name": "malcolmt/remember_me", "stars": 9, "license": "other", "language": "python", "size": 881}
{"docstring": "# Deleting field 'Progress.anon_student'\n", "func_signal": "def backwards(self, orm):\n", "code": "        db.delete_column('minerva_progress', 'anon_student_id')\n\n        # Changing field 'Progress.student'\n        db.alter_column('minerva_progress', 'student_id', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['auth.User']))", "path": "minerva\\migrations\\0005_anon_users_progress_tracking.py", "repo_name": "malcolmt/remember_me", "stars": 9, "license": "other", "language": "python", "size": 881}
{"docstring": "\"\"\"\nFlatten the question meta data so that we know which choices were\npresented to the user without having to store anything on the server.\n\"\"\"\n", "func_signal": "def _pack_meta_data(self, correct, possible_answers):\n", "code": "data = [str(correct[0])]\ndata.extend([str(item[0]) for item in possible_answers])\nreturn \"|\".join(data)", "path": "minerva\\forms.py", "repo_name": "malcolmt/remember_me", "stars": 9, "license": "other", "language": "python", "size": 881}
{"docstring": "\"\"\"\nCreate an empty Profile whenever a new User is created.\n\"\"\"\n# pylint: disable-msg=W0613\n", "func_signal": "def create_user_profile(sender, **kwargs):\n", "code": "from minerva import models\ntry:\n    ContentType.objects.get_by_natural_key(\"minerva\", \"profile\")\nexcept ContentType.DoesNotExist:\n    # Minerva app hasn't been installed yet.\n    return\nif not kwargs[\"created\"]:\n    return\nmodels.Profile(user=kwargs[\"instance\"]).save()", "path": "minerva\\signal_handlers.py", "repo_name": "malcolmt/remember_me", "stars": 9, "license": "other", "language": "python", "size": 881}
{"docstring": "\"\"\"\nPopulate the Word.lang_code field, based on the current Word.language\nsetting.\n\"\"\"\n", "func_signal": "def forwards(self, orm):\n", "code": "for code in (\"zho\", \"jpn\"):\n    linked_pk = orm.Language.objects.get(code=code).pk\n    orm.Word.objects.filter(language=code).update(lang_code=linked_pk)", "path": "minerva\\migrations\\0014_populate_word_lang_code.py", "repo_name": "malcolmt/remember_me", "stars": 9, "license": "other", "language": "python", "size": 881}
{"docstring": "# Adding model 'Word'\n", "func_signal": "def forwards(self, orm):\n", "code": "        db.create_table('minerva_word', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('word', self.gf('django.db.models.fields.CharField')(max_length=50)),\n            ('meaning', self.gf('django.db.models.fields.TextField')()),\n            ('level', self.gf('django.db.models.fields.PositiveIntegerField')()),\n            ('language', self.gf('django.db.models.fields.CharField')(max_length=3)),\n        ))\n        db.send_create_signal('minerva', ['Word'])\n\n        # Adding model 'Progress'\n        db.create_table('minerva_progress', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('student', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['auth.User'])),\n            ('word', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['minerva.Word'])),\n            ('correct', self.gf('django.db.models.fields.IntegerField')(default=0)),\n            ('attempts', self.gf('django.db.models.fields.PositiveIntegerField')()),\n        ))\n        db.send_create_signal('minerva', ['Progress'])", "path": "minerva\\migrations\\0001_initial.py", "repo_name": "malcolmt/remember_me", "stars": 9, "license": "other", "language": "python", "size": 881}
{"docstring": "# FIXME - big assumption here, that the progress object exists\n", "func_signal": "def process_answer(query_base, data):\n", "code": "correct_answer = data['meta'][0]\nquery = dict(query_base)\nquery['word'] = correct_answer\nprogress_on_correct_answer = SessionProgress.objects.get(**query)\nif int(correct_answer) != int(data['answer']):\n    # data['answer'] is the selected answer\n    # Relax the weights because we got them wrong, we want to be more\n    # likely to select it next time.\n    query['word'] = data['answer']\n    progress_on_incorrect_select = SessionProgress.objects.get(**query)\n    decrement_weight(progress_on_correct_answer)\n    decrement_weight(progress_on_incorrect_select)\nelse:\n    progress_on_correct_answer.correct += 1\n    if progress_on_correct_answer.correct > 5:\n        # We have seen this word enough times and answered correctly.\n        # Remove it from the fold.\n        progress_on_correct_answer.delete()\n    else:\n        progress_on_correct_answer.weight += 10 + (10 * random.random())\n        progress_on_correct_answer.save()", "path": "minerva\\questions.py", "repo_name": "malcolmt/remember_me", "stars": 9, "license": "other", "language": "python", "size": 881}
{"docstring": "\"\"\"\nLoads an external settings file, based on machine hostname, and pulls all\nthe all-caps names into the passed-in namespace.\n\"\"\"\n# Convert machine hostname to valid Python module name.\n", "func_signal": "def load_external_settings(module_root, caller_globals):\n", "code": "hostname = socket.gethostname().replace(\".\", \"_\").replace(\"-\", \"_\")\ntry:\n    module = __import__(\"%s.%s\" % (module_root, hostname), caller_globals,\n            {}, [])\n    local_settings = getattr(module, hostname)\n    for setting in dir(local_settings):\n        if setting.upper() == setting:\n            caller_globals[setting] = getattr(local_settings, setting)\nexcept ImportError:\n    # File may not exist, so we ignore all import errors. If you make a\n    # Python syntax error in the file, it will be swallowed silently. So\n    # don't do that.\n    pass", "path": "utils.py", "repo_name": "malcolmt/remember_me", "stars": 9, "license": "other", "language": "python", "size": 881}
{"docstring": "\"\"\"\nRetrieves all the pk values for words for the language \"code\" and \"level\".\nThis interacts sensibly with the cache system to avoid unnecessary database\nround trips.\n\nReturns a list of pk values.\n\"\"\"\n", "func_signal": "def word_keys(lang_id, level):\n", "code": "full_list = cache.get(lang_id)\nif not full_list:\n    full_list = tuple(Word.objects.filter(lang_code__id=lang_id). \\\n            values_list(\"level\", \"pk\").order_by(\"level\"))\n    cache.set(lang_id, full_list, settings.LANG_CACHE_TIMEOUT)\nstart = bisect.bisect_right(full_list, (level - 1, 0))\nend = bisect.bisect_left(full_list, (level + 1, 0))\nreturn [item[1] for item in full_list[start : end]]", "path": "minerva\\questions.py", "repo_name": "malcolmt/remember_me", "stars": 9, "license": "other", "language": "python", "size": 881}
{"docstring": "# Load json fixture manually so that we can create new pk value before\n# saving (avoids conflict with existing data).\n", "func_signal": "def forwards(self, orm):\n", "code": "import minerva\nbase = os.path.dirname(minerva.__file__)\nfixture = os.path.join(base, \"fixtures/zho_data.json.gz\")\ndata = json.loads(gzip.open(fixture).read())\nfor elt in data:\n    assert elt[\"model\"] == \"minerva.word\", elt\n    obj_data = dict([(str(k), v) for k, v in elt[\"fields\"].items()])\n    orm.Word(**obj_data).save(force_insert=True)", "path": "minerva\\migrations\\0003_load_zho.py", "repo_name": "malcolmt/remember_me", "stars": 9, "license": "other", "language": "python", "size": 881}
{"docstring": "# Adding model 'SessionProgress'\n", "func_signal": "def forwards(self, orm):\n", "code": "        db.create_table('minerva_sessionprogress', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('student', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['auth.User'], null=True, blank=True)),\n            ('anon_student', self.gf('django.db.models.fields.CharField')(max_length=32, null=True, blank=True)),\n            ('word', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['minerva.Word'])),\n            ('weight', self.gf('django.db.models.fields.IntegerField')(default=0)),\n            ('correct', self.gf('django.db.models.fields.IntegerField')(default=0)),\n        ))\n        db.send_create_signal('minerva', ['SessionProgress'])", "path": "minerva\\migrations\\0016_session_tracking_model.py", "repo_name": "malcolmt/remember_me", "stars": 9, "license": "other", "language": "python", "size": 881}
{"docstring": "\"\"\"\nCopy all Profile data to UserProfile objects.\n\"\"\"\n", "func_signal": "def backwards(self, orm):\n", "code": "for obj in orm.Profile.objects.all():\n    orm.UserProfile(student_id=obj.user_id,\n            language=obj.language).save()", "path": "minerva\\migrations\\0018_copy_userprofile_data_to_profile.py", "repo_name": "malcolmt/remember_me", "stars": 9, "license": "other", "language": "python", "size": 881}
{"docstring": "# Deleting field 'Word.language'\n", "func_signal": "def forwards(self, orm):\n", "code": "db.delete_column('minerva_word', 'language')\n\n# Changing field 'Word.lang_code'\ndb.alter_column('minerva_word', 'lang_code_id', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['minerva.Language']))", "path": "minerva\\migrations\\0015_remove_language_from_word_model.py", "repo_name": "malcolmt/remember_me", "stars": 9, "license": "other", "language": "python", "size": 881}
{"docstring": "# Make the RSA key\n", "func_signal": "def _create(self, hostname='localhost'):\n", "code": "self.rsakey = RSA.gen_key(2048, m2.RSA_F4)\nif not self.ephemeral:\n    self.rsakey.save_key(self.keyfile, 'aes_256_cbc')\n# Make the public key\npkey = EVP.PKey()\npkey.assign_rsa(self.rsakey, 0)\n# Generate the certificate\nself.cert = X509.X509()\nself.cert.set_serial_number(long(bttime()))\nself.cert.set_version(0x2)\nself.cert.set_pubkey(pkey)\n# Set the name on the certificate\nname = X509.X509_Name()\nname.CN = hostname\nself.cert.set_subject(name)\nself.cert.set_issuer(name)\n# Set the period of time the cert is valid for (5 years from issue)\nnotBefore = m2.x509_get_not_before(self.cert.x509)\nnotAfter = m2.x509_get_not_after(self.cert.x509)\n#XXX: Randomize validity dates?\nm2.x509_gmtime_adj(notBefore, -60*60*24)\nm2.x509_gmtime_adj(notAfter, 60*60*24*365*5)\n# Sign the certificate\nself.cert.sign(pkey, 'sha1')\nif not self.ephemeral:\n    # Save it\n    self.cert.save_pem(self.certfile)", "path": "Anomos\\Crypto\\_Certificate.py", "repo_name": "Miserlou/Anomos", "stars": 10, "license": "gpl-3.0", "language": "python", "size": 6188}
{"docstring": "\"\"\"\nCalled by NatCheck after testing a peer.\n@param result: Result of NatCheck. True on successful connection\n               False if the peer is unreachable.\n@type result: boolean\n\"\"\"\n", "func_signal": "def natcheck_cb(self, peerid, result):\n", "code": "record = self.get(peerid)\nif record is not None:\n    record.nat = not result\n    record.num_natcheck += 1\n    if not (record.nat or peerid in self.reachable):\n        # Peer is not NAT'd and we don't have them in the reachable list\n        self.reachable.add(peerid)", "path": "Anomos\\NetworkModel.py", "repo_name": "Miserlou/Anomos", "stars": 10, "license": "gpl-3.0", "language": "python", "size": 6188}
{"docstring": "\"\"\"Attempts to load the certificate and key from self.certfile and self.keyfile,\n   Generates the certificate and key if they don't exist\"\"\"\n# Allow 3 attempts before quitting\n", "func_signal": "def _load(self):\n", "code": "i = 0\nwhile i < 3:\n    try:\n        self.rsakey = RSA.load_key(self.keyfile)\n        break\n    except RSA.RSAError:\n        i += 1\nelse:\n    log.warning(\"\\nInvalid password entered, exiting.\")\n    sys.exit() #XXX: Is there any chance we need to do some cleanup before this?\nself.cert = X509.load_cert(self.certfile)", "path": "Anomos\\Crypto\\_Certificate.py", "repo_name": "Miserlou/Anomos", "stars": 10, "license": "gpl-3.0", "language": "python", "size": 6188}
{"docstring": "# returns (begin, length)\n", "func_signal": "def new_request(self, index):\n", "code": "if self.inactive_requests[index] == 1:\n    self._make_inactive(index)\nself.numactive[index] += 1\nself.stat_active[index] = 1\nif index not in self.stat_dirty:\n    self.stat_new[index] = 1\nrs = self.inactive_requests[index]\nr = min(rs)\nrs.remove(r)\nself.amount_inactive -= r[1]\nif self.amount_inactive == 0:\n    self.endgame = True\nreturn r", "path": "Anomos\\StorageWrapper.py", "repo_name": "Miserlou/Anomos", "stars": 10, "license": "gpl-3.0", "language": "python", "size": 6188}
{"docstring": "\"\"\"\nCreates connection between two nodes and selects Neighbor ID (NID).\n@param v1: Peer ID\n@type v1: string\n@param v2: Peer ID\n@type v2: string\n\"\"\"\n", "func_signal": "def connect(self, v1, v2):\n", "code": "p1 = self.get(v1)\np2 = self.get(v2)\nnidsP1 = p1.get_avail_nids()\nnidsP2 = p2.get_avail_nids()\nl = list(nidsP1.intersection(nidsP2))\nif len(l):\n    nid = random.choice(l)\n    p1.add_neighbor(v2, nid, p2.ip, p2.port)\n    p2.add_neighbor(v1, nid, p1.ip, p2.port)\nelse:\n    raise RuntimeError(\"No available NeighborIDs. It's possible the \\\n                        network is being attacked.\")", "path": "Anomos\\NetworkModel.py", "repo_name": "Miserlou/Anomos", "stars": 10, "license": "gpl-3.0", "language": "python", "size": 6188}
{"docstring": "\"\"\"\n@param name: Peer ID to be assigned to this SimPeer\n@type name: string\n@param pubkey: RSA Public Key to use when encrypting to this peer\n@type pubkey: Anomos.Crypto.RSAPubKey\n\"\"\"\n", "func_signal": "def __init__(self, name, pubkey, ip, port, sid):\n", "code": "self.name = name\nself.ip = ip\nself.port = port\nself.pubkey = Anomos.Crypto.PeerCert(pubkey)\nself.neighbors = {} # {PeerID: {nid:#, ip:\"\", port:#}}\nself.id_map = {}    # {NeighborID : PeerID}\nself.infohashes = {} # {infohash: (uploaded, downloaded, left)}\nself.relayed_total = 0\nself.last_seen = 0  # Time of last client announce\nself.last_modified = bttime() # Time when client was last modified\nself.failed_nbrs = []\nself.nbrs_needed = 0\nself.sessionid = sid\nself.num_natcheck = 0\nself.nat = True # assume NAT", "path": "Anomos\\NetworkModel.py", "repo_name": "Miserlou/Anomos", "stars": 10, "license": "gpl-3.0", "language": "python", "size": 6188}
{"docstring": "\"\"\"\nRemove connection to neighbor\n@type peerid: string\n\"\"\"\n", "func_signal": "def rm_neighbor(self, peerid):\n", "code": "edge = self.neighbors.get(peerid)\nif edge:\n    del self.id_map[edge['nid']]\n    del self.neighbors[peerid]\n    self.last_modified = bttime()\n    self.nbrs_needed += 1", "path": "Anomos\\NetworkModel.py", "repo_name": "Miserlou/Anomos", "stars": 10, "license": "gpl-3.0", "language": "python", "size": 6188}
{"docstring": "\"\"\"\nRemoves designated peer from network\n@param peerid: Peer ID (str) of peer to be removed\n\"\"\"\n", "func_signal": "def disconnect(self, peerid):\n", "code": "simpeer = self.get(peerid)\nif simpeer is None:\n    return\n# Remove disconnecting peer from any neighbor lists it's a part of\nfor neighborOf in self.names[peerid].get_nbrs():\n    if self.names.has_key(neighborOf):\n        self.names[neighborOf].rm_neighbor(peerid)\n# Remove disconnecting peer from all swarms\nfor infohash in simpeer.get_torrents():\n    self.remove_from_swarm(peerid, infohash)\n# Remove peer from reachable set\nif peerid in self.reachable:\n    self.reachable.remove(peerid)\n# Delete the disconnecting peer's SimPeer object\ndel self.names[peerid]", "path": "Anomos\\NetworkModel.py", "repo_name": "Miserlou/Anomos", "stars": 10, "license": "gpl-3.0", "language": "python", "size": 6188}
{"docstring": "\"\"\"\n@return: set object containing NIDs in range 0 -> 254 which are not in use\n@rtype: set of ints\n\"\"\"\n", "func_signal": "def get_avail_nids(self):\n", "code": "used = set(self.id_map.keys())\nreturn SimPeer.ID_RANGE.copy() - used", "path": "Anomos\\NetworkModel.py", "repo_name": "Miserlou/Anomos", "stars": 10, "license": "gpl-3.0", "language": "python", "size": 6188}
{"docstring": "#XXX: We don't actually support IPv6 yet...\n", "func_signal": "def is_valid_ipv6(ip):\n", "code": "try:\n    socket.inet_pton(socket.AF_INET6, ip)\nexcept socket.error:\n    return False\nelse:\n    return True", "path": "Anomos\\__init__.py", "repo_name": "Miserlou/Anomos", "stars": 10, "license": "gpl-3.0", "language": "python", "size": 6188}
{"docstring": "\"\"\"\nReturns an encrypted tracking code\n@see: http://anomos.info/wp/2008/06/19/tracking-codes-revised/\n\n@param pathByNames: List of peer id's belonging to members of chain\n@type pathByNames:  list\n@param plaintext:   Message to be encrypted at innermost onion layer.\n@type plaintext:    str\n@return: E_a(\\\\x0 + SID_a + TC_b + E_b(\\\\x0 + SID_b + TC_c + \\\\\n            E_c(\\\\x1 + SID_c + plaintext)))\n@rtype: string\n\"\"\"\n", "func_signal": "def encrypt_tc(self, pathByNames, prevNbr=None, plaintext='#', msglen=4096):\n", "code": "message = plaintext\npeername = pathByNames.pop(-1)\npeerobj = self.get(peername)\nsid = peerobj.get_session_id()\nif prevNbr:\n    nid = str(prevNbr.get_nid(peername))\n    tocrypt = chr(0) + sid + nid + message\n    recvMsgLen = len(sid + nid) + 1 # The 'message' data is for the\n                                    # next recipient, not this one.\n    message = peerobj.pubkey.encrypt(tocrypt, recvMsgLen)\nelse:\n    tocrypt = chr(1) + sid + message\n    message = peerobj.pubkey.encrypt(tocrypt, len(tocrypt))\nprevNbr = peerobj\nif len(pathByNames) > 0:\n    return self.encrypt_tc(pathByNames, prevNbr, message, msglen)\nelif len(message) < msglen:\n    # Pad to msglen\n    return message + Anomos.Crypto.get_rand(msglen-len(message))\nelse:\n    # XXX: Disallow messages longer than msglen?\n    return message", "path": "Anomos\\NetworkModel.py", "repo_name": "Miserlou/Anomos", "stars": 10, "license": "gpl-3.0", "language": "python", "size": 6188}
{"docstring": "\"\"\"\n@type peerid: string\n@param pubkey: public key to use when encrypting to this peer\n@type pubkey: Anomos.Crypto.RSAPubKey\n@returns: a reference to the created peer\n@rtype: SimPeer\n\"\"\"\n", "func_signal": "def init_peer(self, peerid, pubkey, ip, port, sid, num_neighbors=4):\n", "code": "self.names[peerid] = SimPeer(peerid, pubkey, ip, port, sid)\nself.rand_connect(peerid, num_neighbors)\nreturn self.names[peerid]", "path": "Anomos\\NetworkModel.py", "repo_name": "Miserlou/Anomos", "stars": 10, "license": "gpl-3.0", "language": "python", "size": 6188}
{"docstring": "\"\"\"\nAssign 'numpeers' many randomly selected neighbors to\npeer with id == peerid\n\"\"\"\n", "func_signal": "def rand_connect(self, peerid, numpeers):\n", "code": "peer = self.get(peerid)\ncandidates = random.sample(self.reachable, len(self.reachable))\nallow_close = self.config.get('allow_close_neighbors')\nif not allow_close:\n    used_ips = [i['ip'] for i in peer.neighbors.values()]\n    used_ips.append(peer.ip)\nelse:\n    used_ips = []\nfor opid in candidates:\n    if numpeers <= 0:\n        break\n    # Don't connect peers to: Themselves, peers who\n    # they're already neighbors with, peers they've failed\n    # to make connections to in the past.\n    if  opid == peerid or \\\n        opid in peer.neighbors.keys() or \\\n        opid in peer.failed_nbrs:\n            continue\n    # Don't connect peers to other peers at the same IP or\n    # peers with the same IP as a neighbor they already have\n    opid_ip = self.get(opid).ip\n    if not allow_close and opid_ip in used_ips:\n        continue\n    self.connect(peerid, opid)\n    used_ips.append(opid_ip)\n    numpeers -= 1", "path": "Anomos\\NetworkModel.py", "repo_name": "Miserlou/Anomos", "stars": 10, "license": "gpl-3.0", "language": "python", "size": 6188}
{"docstring": "# Separate by downloading/seeding status\n", "func_signal": "def update_torrent_widgets(self):\n", "code": "dl = [t for _,t in self.torrents.items() if t.completion < 1]\nsd = [t for _,t in self.torrents.items() if t.completion >= 1]\nif self.dlclicked:\n    to_show, to_hide = dl, sd\nelse:\n    to_show, to_hide = sd, dl\nfor i,s in enumerate(to_show):\n    if s.widget:\n        s.widget.show()\n        try:\n            self.runbox.reorder_child(s.widget, i)\n        except ValueError:\n            pass\nfor h in to_hide:\n    if h.widget:\n        h.widget.hide()", "path": "anondownloadgui.py", "repo_name": "Miserlou/Anomos", "stars": 10, "license": "gpl-3.0", "language": "python", "size": 6188}
{"docstring": "##Improve\n", "func_signal": "def remove(self):\n", "code": "self.main.torrentqueue.remove_torrent(self.infohash)\nif len(self.main.torrentqueue.wrapped.running_torrents) <= 1:\n    self.main.stausIcon = _(\"Anomos has no running torrents\")", "path": "anondownloadgui.py", "repo_name": "Miserlou/Anomos", "stars": 10, "license": "gpl-3.0", "language": "python", "size": 6188}
{"docstring": "# might raise an IOError\n", "func_signal": "def write(self, pos, s):\n", "code": "total = 0\nfor filename, begin, end in self._intervals(pos, len(s)):\n    h = self._get_file_handle(filename, True)\n    h.seek(begin)\n    h.write(s[total: total + end - begin])\n    total += end - begin", "path": "Anomos\\Storage.py", "repo_name": "Miserlou/Anomos", "stars": 10, "license": "gpl-3.0", "language": "python", "size": 6188}
{"docstring": "\"\"\"Logs a message when the decorated method is called\"\"\"\n", "func_signal": "def log_on_call(fn):\n", "code": "def ret_fn(self, *args, **kwargs):\n    LOG.info(self.uniq_id() + \" calling %s\" % fn.__name__)\n    return fn(self, *args, **kwargs)\nreturn ret_fn", "path": "Anomos\\__init__.py", "repo_name": "Miserlou/Anomos", "stars": 10, "license": "gpl-3.0", "language": "python", "size": 6188}
{"docstring": "\"\"\"Starts PDB when the decorated method is called\"\"\"\n", "func_signal": "def trace_on_call(fn):\n", "code": "import pdb\ndef ret_fn(self, *args, **kwargs):\n    pdb.set_trace()\n    return fn(self, *args, **kwargs)\nreturn ret_fn", "path": "Anomos\\__init__.py", "repo_name": "Miserlou/Anomos", "stars": 10, "license": "gpl-3.0", "language": "python", "size": 6188}
{"docstring": "\"\"\"\nAssign Neighbor ID to peer\n@type peerid: string\n@type nid: int\n\"\"\"\n", "func_signal": "def add_neighbor(self, peerid, nid, ip, port):\n", "code": "self.neighbors.setdefault(peerid, {'nid':nid,'ip':ip, 'port':port})\nself.id_map[nid] = peerid\nself.last_modified = bttime()\nif self.nbrs_needed > 0:\n    self.nbrs_needed -= 1", "path": "Anomos\\NetworkModel.py", "repo_name": "Miserlou/Anomos", "stars": 10, "license": "gpl-3.0", "language": "python", "size": 6188}
{"docstring": "# get the path to file\n", "func_signal": "def get_file_path_from_dnd_dropped_uri(self, uri):\n", "code": "path = \"\"\nif uri.startswith('file:\\\\\\\\\\\\'): # windows\n    path = uri[8:] # 8 is len('file:///')\nelif uri.startswith('file://'): # nautilus, rox\n    path = uri[7:] # 7 is len('file://')\nelif uri.startswith('file:'): # xffm\n    path = uri[5:] # 5 is len('file:')\n\npath = url2pathname(path) # escape special chars\npath = path.strip('\\r\\n\\x00') # remove \\r\\n and NULL\nreturn path", "path": "anondownloadgui.py", "repo_name": "Miserlou/Anomos", "stars": 10, "license": "gpl-3.0", "language": "python", "size": 6188}
{"docstring": "\"\"\"<US zip code | US/Canada city, state | Foreign city, country>\n\nReturns the approximate weather conditions for a given city.\n\"\"\"\n", "func_signal": "def cnn(self, irc, msg, args, loc):\n", "code": "if ' ' in loc:\n    #If we received more than 1 argument, then we got a city with a\n    #multi-word name.  ie ['Garden', 'City', 'KS'] instead of\n    #['Liberal', 'KS'].\n    loc = utils.str.rsplit(loc, None, 1)\n    state = loc.pop().lower()\n    city = ' '.join(loc)\n    city = city.rstrip(',').lower()\n    if state in self._cnnCountryMap:\n        state = self._cnnCountryMap[state]\n    loc = ' '.join([city, state])\nelse:\n    #We received a single argument.  Zipcode or station id.\n    loc = loc.replace(',', '')\nurl = self._cnnSearchUrl % (utils.web.urlquote(loc))\njson = simplejson.loads(utils.web.getUrl(url))\nif not json:\n    self._noLocation()\njson = json[0]\nurl = self._cnnUrl % (json['locCode'], json['zip'])\ntext = utils.web.getUrl(url)\nlocation = ', '.join([json['city'], json['stateOrCountry']])\ntemp = self._cnnFTemp.search(text)\nconds = self._cnnCond.search(text)\nhumidity = self._cnnHumid.search(text)\nwind = self._cnnWind.search(text)\nif location and temp:\n    (temp, deg) = temp.groups()\n    unit = 'F'\n    temp = self._getTemp(float(temp), deg, unit, msg.args[0])\n    resp = [format('The current temperature in %s is %s.',\n                   location, temp)]\n    if conds is not None:\n        resp.append(format('Conditions: %s.', conds.group(1)))\n    if humidity is not None:\n        resp.append(format('Humidity: %s.', humidity.group(1)))\n    if wind is not None:\n        resp.append(format('Wind: %s.', wind.group(1)))\n    resp = map(utils.web.htmlToText, resp)\n    irc.reply(' '.join(resp))\nelse:\n    irc.errorPossibleBug('Could not find weather information.')", "path": "plugin.py", "repo_name": "Supybot/Weather", "stars": 15, "license": "None", "language": "python", "size": 184}
{"docstring": "\"\"\"Treat a bogus SGML declaration as raw data. Treat a CDATA\ndeclaration as regular data.\"\"\"\n", "func_signal": "def parse_declaration(self, i):\n", "code": "j = None\nif self.rawdata[i:i+9] == '<![CDATA[':\n     k = self.rawdata.find(']]>', i)\n     if k == -1:\n         k = len(self.rawdata)\n     self.handle_data(self.rawdata[i+9:k])\n     j = k+3\nelse:\n    try:\n        j = SGMLParser.parse_declaration(self, i)\n    except SGMLParseError:\n        toHandle = self.rawdata[i:]\n        self.handle_data(toHandle)\n        j = i + len(toHandle)\nreturn j", "path": "local\\BeautifulSoup.py", "repo_name": "Supybot/Weather", "stars": 15, "license": "None", "language": "python", "size": 184}
{"docstring": "#print 'looking for %s in %s' % (howToMatch, chunk)\n#\n# If given a list of items, return true if the list contains a\n# text element that matches.\n", "func_signal": "def _matches(self, chunk, howToMatch):\n", "code": "if isList(chunk) and not isinstance(chunk, Tag):\n    for tag in chunk:\n        if isinstance(tag, NavigableText) and self._matches(tag, howToMatch):\n            return True\n    return False\nif callable(howToMatch):\n    return howToMatch(chunk)\nif isinstance(chunk, Tag):\n    #Custom match methods take the tag as an argument, but all other\n    #ways of matching match the tag name as a string\n    chunk = chunk.name\n#Now we know that chunk is a string\nif not isinstance(chunk, basestring):\n    chunk = str(chunk)\nif hasattr(howToMatch, 'match'):\n    # It's a regexp object.\n    return howToMatch.search(chunk)\nif isList(howToMatch):\n    return chunk in howToMatch\nif hasattr(howToMatch, 'items'):\n    return howToMatch.has_key(chunk)\n#It's just a string\nreturn str(howToMatch) == chunk", "path": "local\\BeautifulSoup.py", "repo_name": "Supybot/Weather", "stars": 15, "license": "None", "language": "python", "size": 184}
{"docstring": "\"\"\"<US zip code | US/Canada city, state | Foreign city, country>\n\nReturns the approximate weather conditions for a given city.\n\"\"\"\n", "func_signal": "def rss(self, irc, msg, args, loc):\n", "code": "url = self._rsswunderUrl % utils.web.urlquote(loc)\nurl = url.replace('%20', '+')\ntext = utils.web.getUrl(url)\nif 'Search not found' in text or \\\n   re.search(r'size=\"2\"> Place </font>', text, re.I):\n    Weather._noLocation()\nif 'Search Results' in text:\n    m = self._backupUrl.search(text)\n    if m is not None:\n        url = 'http://www.wunderground.com' + m.group(1)\n        text = utils.web.getUrl(url)\n    else:\n        Weather._noLocation()\nself._rss(irc, text)", "path": "plugin.py", "repo_name": "Supybot/Weather", "stars": 15, "license": "None", "language": "python", "size": 184}
{"docstring": "\"\"\"Convenience method that works with all 2.x versions of Python\nto determine whether or not something is listlike.\"\"\"\n", "func_signal": "def isList(l):\n", "code": "return hasattr(l, '__iter__') \\\n       or (type(l) in (types.ListType, types.TupleType))", "path": "local\\BeautifulSoup.py", "repo_name": "Supybot/Weather", "stars": 15, "license": "None", "language": "python", "size": 184}
{"docstring": "\"\"\"Sets up the initial relations between this element and\nother elements.\"\"\"\n", "func_signal": "def setup(self, parent=Null, previous=Null):\n", "code": "self.parent = parent\nself.previous = previous\nself.next = Null\nself.previousSibling = Null\nself.nextSibling = Null\nif self.parent and self.parent.contents:\n    self.previousSibling = self.parent.contents[-1]\n    self.previousSibling.nextSibling = self", "path": "local\\BeautifulSoup.py", "repo_name": "Supybot/Weather", "stars": 15, "license": "None", "language": "python", "size": 184}
{"docstring": "\"\"\"Returns the closest parent of this Tag that matches the given\ncriteria.\"\"\"\n", "func_signal": "def findParent(self, name=None, attrs={}):\n", "code": "r = Null\nl = self.fetchParents(name, attrs, 1)\nif l:\n    r = l[0]\nreturn r", "path": "local\\BeautifulSoup.py", "repo_name": "Supybot/Weather", "stars": 15, "license": "None", "language": "python", "size": 184}
{"docstring": "\"\"\"<US zip code | US/Canada city, state | Foreign city, country>\n\nReturns the approximate weather conditions for a given city.\n\"\"\"\n", "func_signal": "def ham(self, irc, msg, args, loc):\n", "code": "url = 'http://www.hamweather.net/cgi-bin/hw3/hw3.cgi?' \\\n      'config=&forecast=zandh&pands=%s&Submit=GO' % \\\n      utils.web.urlquote(loc.lower())\nhtml = utils.web.getUrl(url)\nif 'was not found' in html:\n    self._noLocation()\n\n# ham seems to automatically return a location for duplicate names with\n# no list of other possibilities anymore, so this code may not be\n# needed\nif 'Multiple Locations for' in html:\n    m = self._hamMultiLoc.search(html)\n    if m:\n        url = 'http://www.hamweather.net/%s' % m.group(1)\n        html = utils.web.getUrl(url)\n    else:\n        self._noLocation()\nheadData = self._hamLoc.search(html)\nif headData is not None:\n    (city, state, country) = headData.groups()\nelse:\n    headData = self._interregex.search(html)\n    if headData:\n        (city, state) = headData.groups()\n    else:\n        self._noLocation()\ncity = utils.web.htmlToText(city.strip())\nstate = utils.web.htmlToText(state.strip())\ntemp = self._hamTemp.search(html)\nif temp is not None:\n    (temp, deg, unit) = temp.groups()\n    deg = utils.web.htmlToText(deg)\n    temp = self._getTemp(float(temp), deg, unit, msg.args[0])\nconds = self._hamCond.search(html)\nif conds is not None:\n    conds = conds.group(1)\nindex = ''\nchill = self._hamChill.search(html)\nif chill is not None:\n    chill = chill.group(1)\n    chill = utils.web.htmlToText(chill)\n    tempsplit = self._temp.search(chill)\n    if tempsplit:\n        (chill, deg, unit) = tempsplit.groups()\n        chill = self._getTemp(float(chill), deg, unit,msg.args[0])\n    if float(chill[:-2]) < float(temp[:-2]):\n        index = format(' (Wind Chill: %s)', chill)\nheat = self._hamHeat.search(html)\nif heat is not None:\n    heat = heat.group(1)\n    heat = utils.web.htmlToText(heat)\n    tempsplit = self._temp.search(heat)\n    if tempsplit:\n        (heat, deg, unit) = tempsplit.groups()\n        heat= self._getTemp(float(heat), deg, unit,msg.args[0])\n    if float(heat[:-2]) > float(temp[:-2]):\n        index = format(' (Heat Index: %s)', heat)\nif temp and conds and city and state:\n    conds = conds.replace('Tsra', 'Thunderstorms')\n    conds = conds.replace('Ts', 'Thunderstorms')\n    s = format('The current temperature in %s, %s is %s%s. '\n               'Conditions: %s.',\n               city, state, temp, index, conds)\n    irc.reply(s.decode('latin1').encode('utf-8'))\nelse:\n    irc.errorPossibleBug('The format of the page was odd.')", "path": "plugin.py", "repo_name": "Supybot/Weather", "stars": 15, "license": "None", "language": "python", "size": 184}
{"docstring": "\"\"\"Called when you're done parsing, so that the unclosed tags can be\ncorrectly processed.\"\"\"\n", "func_signal": "def done(self):\n", "code": "self.endData() #NEW\nwhile self.currentTag.name != self.ROOT_TAG_NAME:\n    self.popTag()", "path": "local\\BeautifulSoup.py", "repo_name": "Supybot/Weather", "stars": 15, "license": "None", "language": "python", "size": 184}
{"docstring": "\"\"\"Return only the first child of this\nTag matching the given criteria.\"\"\"\n", "func_signal": "def first(self, name=None, attrs={}, recursive=True, text=None):\n", "code": "r = Null\nl = self.fetch(name, attrs, recursive, text, 1)\nif l:\n    r = l[0]\nreturn r", "path": "local\\BeautifulSoup.py", "repo_name": "Supybot/Weather", "stars": 15, "license": "None", "language": "python", "size": 184}
{"docstring": "\"\"\"<US zip code | US/Canada city, state | Foreign city, country>\n\nReturns the approximate weather conditions for a given city.\n\"\"\"\n", "func_signal": "def wunder(self, irc, msg, args, loc):\n", "code": "url = '%s%s' % (self._wunderUrl, utils.web.urlquote(loc))\ntext = utils.web.getUrl(url)\nif 'Search not found' in text or \\\n   re.search(r'size=\"2\"> Place </font>', text, re.I):\n    Weather._noLocation()\nif 'Place: Temperature' in text:\n    m = self._backupUrl.search(text)\n    if m is not None:\n        url = 'http://mobile.wunderground.com' + m.group(1)\n        text = utils.web.getUrl(url)\nsevere = ''\nm = self._wunderSevere.search(text)\nif m:\n    severe = ircutils.bold(format('  %s', m.group(1)))\ntext = self._formatSymbols(text)\nsoup = BeautifulSoup.BeautifulSoup()\nsoup.feed(text)\n# Get the table with all the weather info\ntable = soup.first('table', {'border':'1'})\nif not table:\n    Weather._noLocation()\ntrs = table.fetch('tr')\n(time, location) = trs.pop(0).fetch('b')\ntime = time.string\nlocation = location.string\ninfo = {}\ndef isText(t):\n    return not isinstance(t, BeautifulSoup.NavigableText) \\\n           and t.contents\ndef getText(t):\n    s = t.string\n    if s is BeautifulSoup.Null:\n        t = t.contents\n        num = t[0].string\n        units = t[1].string\n        # htmlToText strips leading whitespace, so we have to\n        # handle strings with &nbsp; differently.\n        if units.startswith('&nbsp;'):\n            units = utils.web.htmlToText(units)\n            s = ' '.join((num, units))\n        else:\n            units = utils.web.htmlToText(units)\n            s = ' '.join((num, units[0], units[1:]))\n    return s\nfor tr in trs:\n    k = tr.td.string\n    v = filter(isText, tr.fetch('td')[1].contents)\n    value = map(getText, v)\n    info[k] = ' '.join(value)\ntemp = info['Temperature']\nif location and temp:\n    (temp, deg, unit) = temp.split()[3:] # We only want temp format\n    temp = Weather._getTemp(float(temp), deg, unit, msg.args[0])\n    resp = ['The current temperature in %s is %s (%s).' %\\\n            (location, temp, time)]\n    conds = info['Conditions']\n    resp.append('Conditions: %s.' % info['Conditions'])\n    humidity = info['Humidity']\n    resp.append('Humidity: %s.' % info['Humidity'])\n    # Apparently, the \"Dew Point\" and \"Wind\" categories are\n    # occasionally set to \"-\" instead of an actual reading. So,\n    # we'll just catch the ValueError from trying to unpack a tuple\n    # of the wrong size.\n    try:\n        (dew, deg, unit) = info['Dew Point'].split()[3:]\n        dew = Weather._getTemp(float(dew), deg, unit, msg.args[0])\n        resp.append('Dew Point: %s.' % dew)\n    except (ValueError, KeyError):\n        pass\n    try:\n        wind = 'Wind: %s at %s %s.' % tuple(info['Wind'].split())\n        resp.append(wind)\n    except (ValueError, TypeError):\n        pass\n    try:\n        (chill, deg, unit) = info['Windchill'].split()[3:]\n        chill = Weather._getTemp(float(chill), deg,\n                                 unit, msg.args[0])\n        resp.append('Windchill: %s.' % chill)\n    except (ValueError, KeyError):\n        pass\n    if info['Pressure']:\n        resp.append('Pressure: %s.' % info['Pressure'])\n    resp.append(severe)\n    resp = map(utils.web.htmlToText, resp)\n    irc.reply(' '.join(resp).decode('latin1').encode('utf-8'))\nelse:\n    Weather._noLocation()", "path": "plugin.py", "repo_name": "Supybot/Weather", "stars": 15, "license": "None", "language": "python", "size": 184}
{"docstring": "\"\"\"Pops the tag stack up to and including the most recent\ninstance of the given tag. If inclusivePop is false, pops the tag\nstack up to but *not* including the most recent instqance of\nthe given tag.\"\"\"\n", "func_signal": "def _popToTag(self, name, inclusivePop=True):\n", "code": "if name == self.ROOT_TAG_NAME:\n    return            \n\nnumPops = 0\nmostRecentTag = None\nfor i in range(len(self.tagStack)-1, 0, -1):\n    if name == self.tagStack[i].name:\n        numPops = len(self.tagStack)-i\n        break\nif not inclusivePop:\n    numPops = numPops - 1\n\nfor i in range(0, numPops):\n    mostRecentTag = self.popTag()\nreturn mostRecentTag", "path": "local\\BeautifulSoup.py", "repo_name": "Supybot/Weather", "stars": 15, "license": "None", "language": "python", "size": 184}
{"docstring": "\"\"\"Turns a list of maps, lists, or scalars into a single map.\nUsed to build the SELF_CLOSING_TAGS and NESTABLE_TAGS maps out\nof lists and partial maps.\"\"\"\n", "func_signal": "def buildTagMap(default, *args):\n", "code": "built = {}\nfor portion in args:\n    if hasattr(portion, 'items'):\n        #It's a map. Merge it.\n        for k,v in portion.items():\n            built[k] = v\n    elif isList(portion):\n        #It's a list. Map each item to the default.\n        for k in portion:\n            built[k] = default\n    else:\n        #It's a scalar. Map it to the default.\n        built[portion] = default\nreturn built", "path": "local\\BeautifulSoup.py", "repo_name": "Supybot/Weather", "stars": 15, "license": "None", "language": "python", "size": 184}
{"docstring": "\"\"\"This method routes method call requests to either the SGMLParser\nsuperclass or the Tag superclass, depending on the method name.\"\"\"\n", "func_signal": "def __getattr__(self, methodName):\n", "code": "if methodName.find('start_') == 0 or methodName.find('end_') == 0 \\\n       or methodName.find('do_') == 0:\n    return SGMLParser.__getattr__(self, methodName)\nelif methodName.find('__') != 0:\n    return Tag.__getattr__(self, methodName)\nelse:\n    raise AttributeError", "path": "local\\BeautifulSoup.py", "repo_name": "Supybot/Weather", "stars": 15, "license": "None", "language": "python", "size": 184}
{"docstring": "\"\"\"<US zip code | US/Canada city, state | Foreign city, country>\n\nReturns the approximate weather conditions for a given city.\n\"\"\"\n", "func_signal": "def weather(self, irc, msg, args, location):\n", "code": "channel = None\nif irc.isChannel(msg.args[0]):\n    channel = msg.args[0]\nif not location:\n    location = self.userValue('lastLocation', msg.prefix)\nif not location:\n    raise callbacks.ArgumentError\nself.setUserValue('lastLocation', msg.prefix,\n                  location, ignoreNoUser=True)\nargs = [location]\ncommandName = self.registryValue('command', channel)\nfirstCommand = commandName\ncommand = self.getCommandMethod(commandName.split())\ntry:\n    command(irc, msg, args[:])\nexcept (NoLocation, utils.web.Error):\n    self.log.info('%s lookup failed, Trying others.', firstCommand)\n    for commandName in self.weatherCommands:\n        if commandName != firstCommand:\n            self.log.info('Trying %s.', commandName)\n            try:\n                command = self.getCommandMethod(commandName.split())\n                command(irc, msg, args[:])\n                self.log.info('%s lookup succeeded.', commandName)\n                return\n            except NoLocation:\n                self.log.info('%s lookup failed as backup.',\n                              commandName)\n    irc.error(format('Could not retrieve weather for %q.', location))", "path": "plugin.py", "repo_name": "Supybot/Weather", "stars": 15, "license": "None", "language": "python", "size": 184}
{"docstring": "\"\"\"Returns true iff this tag has the same name, the same attributes,\nand the same contents (recursively) as the given tag.\n\nNOTE: right now this will return false if two tags have the\nsame attributes in a different order. Should this be fixed?\"\"\"\n", "func_signal": "def __eq__(self, other):\n", "code": "if not hasattr(other, 'name') or not hasattr(other, 'attrs') or not hasattr(other, 'contents') or self.name != other.name or self.attrs != other.attrs or len(self) != len(other):\n    return False\nfor i in range(0, len(self.contents)):\n    if self.contents[i] != other.contents[i]:\n        return False\nreturn True", "path": "local\\BeautifulSoup.py", "repo_name": "Supybot/Weather", "stars": 15, "license": "None", "language": "python", "size": 184}
{"docstring": "\"\"\"Renders the contents of this tag as a (possibly Unicode) \nstring.\"\"\"\n", "func_signal": "def renderContents(self, showStructureIndent=None, needUnicode=None):\n", "code": "s=[]\nfor c in self:\n    text = None\n    if isinstance(c, NavigableUnicodeString) or type(c) == types.UnicodeType:\n        text = unicode(c)\n    elif isinstance(c, Tag):\n        s.append(c.__str__(needUnicode, showStructureIndent))\n    elif needUnicode:\n        text = unicode(c)\n    else:\n        text = str(c)\n    if text:\n        if showStructureIndent != None:\n            if text[-1] == '\\n':\n                text = text[:-1]\n        s.append(text)\nreturn ''.join(s)", "path": "local\\BeautifulSoup.py", "repo_name": "Supybot/Weather", "stars": 15, "license": "None", "language": "python", "size": 184}
{"docstring": "\"\"\"Returns a string or Unicode representation of this tag and\nits contents.\n\nNOTE: since Python's HTML parser consumes whitespace, this\nmethod is not certain to reproduce the whitespace present in\nthe original string.\"\"\"\n\n", "func_signal": "def __str__(self, needUnicode=None, showStructureIndent=None):\n", "code": "attrs = []\nif self.attrs:\n    for key, val in self.attrs:\n        attrs.append('%s=\"%s\"' % (key, val))\nclose = ''\ncloseTag = ''\nif self.isSelfClosing():\n    close = ' /'\nelse:\n    closeTag = '</%s>' % self.name\nindentIncrement = None        \nif showStructureIndent != None:\n    indentIncrement = showStructureIndent\n    if not self.hidden:\n        indentIncrement += 1\ncontents = self.renderContents(indentIncrement, needUnicode=needUnicode)        \nif showStructureIndent:\n    space = '\\n%s' % (' ' * showStructureIndent)\nif self.hidden:\n    s = contents\nelse:\n    s = []\n    attributeString = ''\n    if attrs:\n        attributeString = ' ' + ' '.join(attrs)            \n    if showStructureIndent:\n        s.append(space)\n    s.append('<%s%s%s>' % (self.name, attributeString, close))\n    s.append(contents)\n    if closeTag and showStructureIndent != None:\n        s.append(space)\n    s.append(closeTag)\n    s = ''.join(s)\nisUnicode = type(s) == types.UnicodeType\nif needUnicode and not isUnicode:\n    s = unicode(s)\nelif isUnicode and needUnicode==False:\n    s = str(s)\nreturn s", "path": "local\\BeautifulSoup.py", "repo_name": "Supybot/Weather", "stars": 15, "license": "None", "language": "python", "size": 184}
{"docstring": "\"\"\"Initializes a map representation of this tag's attributes,\nif not already initialized.\"\"\"\n", "func_signal": "def _getAttrMap(self):\n", "code": "if not getattr(self, 'attrMap'):\n    self.attrMap = {}\n    for (key, value) in self.attrs:\n        self.attrMap[key] = value \nreturn self.attrMap", "path": "local\\BeautifulSoup.py", "repo_name": "Supybot/Weather", "stars": 15, "license": "None", "language": "python", "size": 184}
{"docstring": "#print \"Push\", tag.name\n", "func_signal": "def pushTag(self, tag):\n", "code": "if self.currentTag:\n    self.currentTag.append(tag)\nself.tagStack.append(tag)\nself.currentTag = self.tagStack[-1]", "path": "local\\BeautifulSoup.py", "repo_name": "Supybot/Weather", "stars": 15, "license": "None", "language": "python", "size": 184}
{"docstring": "\"\"\"Simple wrapper around the ``textwrap.wrap`` function in the standard\nlibrary. This version does not wrap lines on hyphens in words.\n\n:param text: the text to wrap\n:param width: the maximum line width\n:param initial_indent: string that will be prepended to the first line of\n                       wrapped output\n:param subsequent_indent: string that will be prepended to all lines save\n                          the first of wrapped output\n:return: a list of lines\n:rtype: `list`\n\"\"\"\n", "func_signal": "def wraptext(text, width=70, initial_indent='', subsequent_indent=''):\n", "code": "wrapper = TextWrapper(width=width, initial_indent=initial_indent,\n                      subsequent_indent=subsequent_indent,\n                      break_long_words=False)\nreturn wrapper.wrap(text)", "path": "app\\distlib\\babel\\util.py", "repo_name": "kosmikko/bloggart-tipfy", "stars": 9, "license": "None", "language": "python", "size": 3792}
{"docstring": "\"\"\"Deduce the encoding of a source file from magic comment.\n\nIt does this in the same way as the `Python interpreter`__\n\n.. __: http://docs.python.org/ref/encodings.html\n\nThe ``fp`` argument should be a seekable file object.\n\n(From Jeff Dairiki)\n\"\"\"\n", "func_signal": "def parse_encoding(fp):\n", "code": "pos = fp.tell()\nfp.seek(0)\ntry:\n    line1 = fp.readline()\n    has_bom = line1.startswith(codecs.BOM_UTF8)\n    if has_bom:\n        line1 = line1[len(codecs.BOM_UTF8):]\n\n    m = PYTHON_MAGIC_COMMENT_re.match(line1)\n    if not m:\n        try:\n            import parser\n            parser.suite(line1)\n        except (ImportError, SyntaxError):\n            # Either it's a real syntax error, in which case the source is\n            # not valid python source, or line2 is a continuation of line1,\n            # in which case we don't want to scan line2 for a magic\n            # comment.\n            pass\n        else:\n            line2 = fp.readline()\n            m = PYTHON_MAGIC_COMMENT_re.match(line2)\n\n    if has_bom:\n        if m:\n            raise SyntaxError(\n                \"python refuses to compile code with both a UTF8 \"\n                \"byte-order-mark and a magic encoding comment\")\n        return 'utf_8'\n    elif m:\n        return m.group(1)\n    else:\n        return None\nfinally:\n    fp.seek(pos)", "path": "app\\distlib\\babel\\util.py", "repo_name": "kosmikko/bloggart-tipfy", "stars": 9, "license": "None", "language": "python", "size": 3792}
{"docstring": "\"\"\"Compare Messages, taking into account plural ids\"\"\"\n", "func_signal": "def __cmp__(self, obj):\n", "code": "if isinstance(obj, Message):\n    plural = self.pluralizable\n    obj_plural = obj.pluralizable\n    if plural and obj_plural:\n        return cmp(self.id[0], obj.id[0])\n    elif plural:\n        return cmp(self.id[0], obj.id)\n    elif obj_plural:\n        return cmp(self.id, obj.id[0])\nreturn cmp(self.id, obj.id)", "path": "app\\distlib\\babel\\messages\\catalog.py", "repo_name": "kosmikko/bloggart-tipfy", "stars": 9, "license": "None", "language": "python", "size": 3792}
{"docstring": "\"\"\"Iterates through all the entries in the catalog, in the order they\nwere added, yielding a `Message` object for every entry.\n\n:rtype: ``iterator``\n\"\"\"\n", "func_signal": "def __iter__(self):\n", "code": "buf = []\nfor name, value in self.mime_headers:\n    buf.append('%s: %s' % (name, value))\nflags = set()\nif self.fuzzy:\n    flags |= set(['fuzzy'])\nyield Message(u'', '\\n'.join(buf), flags=flags)\nfor key in self._messages:\n    yield self._messages[key]", "path": "app\\distlib\\babel\\messages\\catalog.py", "repo_name": "kosmikko/bloggart-tipfy", "stars": 9, "license": "None", "language": "python", "size": 3792}
{"docstring": "\"\"\"Extended pathname pattern matching.\n\nThis function is similar to what is provided by the ``fnmatch`` module in\nthe Python standard library, but:\n\n * can match complete (relative or absolute) path names, and not just file\n   names, and\n * also supports a convenience pattern (\"**\") to match files at any\n   directory level.\n\nExamples:\n\n>>> pathmatch('**.py', 'bar.py')\nTrue\n>>> pathmatch('**.py', 'foo/bar/baz.py')\nTrue\n>>> pathmatch('**.py', 'templates/index.html')\nFalse\n\n>>> pathmatch('**/templates/*.html', 'templates/index.html')\nTrue\n>>> pathmatch('**/templates/*.html', 'templates/foo/bar.html')\nFalse\n\n:param pattern: the glob pattern\n:param filename: the path name of the file to match against\n:return: `True` if the path name matches the pattern, `False` otherwise\n:rtype: `bool`\n\"\"\"\n", "func_signal": "def pathmatch(pattern, filename):\n", "code": "symbols = {\n    '?':   '[^/]',\n    '?/':  '[^/]/',\n    '*':   '[^/]+',\n    '*/':  '[^/]+/',\n    '**/': '(?:.+/)*?',\n    '**':  '(?:.+/)*?[^/]+',\n}\nbuf = []\nfor idx, part in enumerate(re.split('([?*]+/?)', pattern)):\n    if idx % 2:\n        buf.append(symbols[part])\n    elif part:\n        buf.append(re.escape(part))\nmatch = re.match(''.join(buf) + '$', filename.replace(os.sep, '/'))\nreturn match is not None", "path": "app\\distlib\\babel\\util.py", "repo_name": "kosmikko/bloggart-tipfy", "stars": 9, "license": "None", "language": "python", "size": 3792}
{"docstring": "\"\"\"Run the application and conserve the traceback frames.\"\"\"\n", "func_signal": "def debug_application(self, environ, start_response):\n", "code": "app_iter = None\ntry:\n    app_iter = self.app(environ, start_response)\n    for item in app_iter:\n        yield item\n    if hasattr(app_iter, 'close'):\n        app_iter.close()\nexcept:\n    if hasattr(app_iter, 'close'):\n        app_iter.close()\n    traceback = get_current_traceback(skip=1, show_hidden_frames=\n                                      self.show_hidden_frames,\n                                      ignore_system_exceptions=True)\n    for frame in traceback.frames:\n        self.frames[frame.id] = frame\n    self.tracebacks[traceback.id] = traceback\n\n    try:\n        start_response('500 INTERNAL SERVER ERROR', [\n            ('Content-Type', 'text/html; charset=utf-8')\n        ])\n    except:\n        # if we end up here there has been output but an error\n        # occurred.  in that situation we can do nothing fancy any\n        # more, better log something into the error log and fall\n        # back gracefully.\n        environ['wsgi.errors'].write(\n            'Debugging middleware caught exception in streamed '\n            'response at a point where response headers were already '\n            'sent.\\n')\n    else:\n        yield traceback.render_full(evalex=self.evalex) \\\n                       .encode('utf-8', 'replace')\n\n    traceback.log(environ['wsgi.errors'])", "path": "app\\distlib\\werkzeug\\debug\\__init__.py", "repo_name": "kosmikko/bloggart-tipfy", "stars": 9, "license": "None", "language": "python", "size": 3792}
{"docstring": "\"\"\"Paste the traceback and return a JSON response.\"\"\"\n", "func_signal": "def paste_traceback(self, request, traceback):\n", "code": "paste_id = traceback.paste()\nreturn Response('{\"url\": \"http://paste.pocoo.org/show/%s/\", \"id\": %s}'\n                % (paste_id, paste_id), mimetype='application/json')", "path": "app\\distlib\\werkzeug\\debug\\__init__.py", "repo_name": "kosmikko/bloggart-tipfy", "stars": 9, "license": "None", "language": "python", "size": 3792}
{"docstring": "\"\"\"\nReturn lines of a single file as list of Unicode strings.\n\"\"\"\n", "func_signal": "def readlines(self):\n", "code": "try:\n    lines = self.source.readlines()\nfinally:\n    if self.autoclose:\n        self.close()\nreturn [self.decode(line) for line in lines]", "path": "app\\lib\\docutils\\io.py", "repo_name": "kosmikko/bloggart-tipfy", "stars": 9, "license": "None", "language": "python", "size": 3792}
{"docstring": "\"\"\"Dispatch the requests.\"\"\"\n# important: don't ever access a function here that reads the incoming\n# form data!  Otherwise the application won't have access to that data\n# any more!\n", "func_signal": "def __call__(self, environ, start_response):\n", "code": "request = Request(environ)\nresponse = self.debug_application\nif self.evalex and self.console_path is not None and \\\n   request.path == self.console_path:\n    response = self.display_console(request)\nelif request.path.rstrip('/').endswith('/__debugger__'):\n    cmd = request.args.get('cmd')\n    arg = request.args.get('f')\n    traceback = self.tracebacks.get(request.args.get('tb', type=int))\n    frame = self.frames.get(request.args.get('frm', type=int))\n    if cmd == 'resource' and arg:\n        response = self.get_resource(request, arg)\n    elif cmd == 'paste' and traceback is not None:\n        response = self.paste_traceback(request, traceback)\n    elif cmd == 'source' and frame:\n        response = self.get_source(request, frame)\n    elif self.evalex and cmd is not None and frame is not None:\n        response = self.execute_command(request, cmd, frame)\nreturn response(environ, start_response)", "path": "app\\distlib\\werkzeug\\debug\\__init__.py", "repo_name": "kosmikko/bloggart-tipfy", "stars": 9, "license": "None", "language": "python", "size": 3792}
{"docstring": "\"\"\"Yield all items in an iterable collection that are distinct.\n\nUnlike when using sets for a similar effect, the original ordering of the\nitems in the collection is preserved by this function.\n\n>>> print list(distinct([1, 2, 1, 3, 4, 4]))\n[1, 2, 3, 4]\n>>> print list(distinct('foobar'))\n['f', 'o', 'b', 'a', 'r']\n\n:param iterable: the iterable collection providing the data\n:return: the distinct items in the collection\n:rtype: ``iterator``\n\"\"\"\n", "func_signal": "def distinct(iterable):\n", "code": "seen = set()\nfor item in iter(iterable):\n    if item not in seen:\n        yield item\n        seen.add(item)", "path": "app\\distlib\\babel\\util.py", "repo_name": "kosmikko/bloggart-tipfy", "stars": 9, "license": "None", "language": "python", "size": 3792}
{"docstring": "\"\"\"Delete the message with the specified ID.\"\"\"\n", "func_signal": "def __delitem__(self, id):\n", "code": "key = self._key_for(id)\nif key in self._messages:\n    del self._messages[key]", "path": "app\\distlib\\babel\\messages\\catalog.py", "repo_name": "kosmikko/bloggart-tipfy", "stars": 9, "license": "None", "language": "python", "size": 3792}
{"docstring": "\"\"\"The key for a message is just the singular ID even for pluralizable\nmessages.\n\"\"\"\n", "func_signal": "def _key_for(self, id):\n", "code": "key = id\nif isinstance(key, (list, tuple)):\n    key = id[0]\nreturn key", "path": "app\\distlib\\babel\\messages\\catalog.py", "repo_name": "kosmikko/bloggart-tipfy", "stars": 9, "license": "None", "language": "python", "size": 3792}
{"docstring": "\"\"\"\nRead and decode a single file and return the data (Unicode string).\n\"\"\"\n", "func_signal": "def read(self):\n", "code": "try:\n    data = self.source.read()\nfinally:\n    if self.autoclose:\n        self.close()\nreturn self.decode(data)", "path": "app\\lib\\docutils\\io.py", "repo_name": "kosmikko/bloggart-tipfy", "stars": 9, "license": "None", "language": "python", "size": 3792}
{"docstring": "\"\"\"Encode `data`, store it in `self.destination`, and return it.\"\"\"\n", "func_signal": "def write(self, data):\n", "code": "self.destination = self.encode(data)\nreturn self.destination", "path": "app\\lib\\docutils\\io.py", "repo_name": "kosmikko/bloggart-tipfy", "stars": 9, "license": "None", "language": "python", "size": 3792}
{"docstring": "\"\"\"\nTry to determine the encoding of `data` by looking *in* `data`.\nCheck for a byte order mark (BOM) or an encoding declaration.\n\"\"\"\n# check for a byte order mark:\n", "func_signal": "def determine_encoding_from_data(self, data):\n", "code": "for start_bytes, encoding in self.byte_order_marks:\n    if data.startswith(start_bytes):\n        return encoding\n# check for an encoding declaration pattern in first 2 lines of file:\nfor line in data.splitlines()[:2]:\n    match = self.coding_slug.search(line)\n    if match:\n        return match.group(1).decode('ascii')\nreturn None", "path": "app\\lib\\docutils\\io.py", "repo_name": "kosmikko/bloggart-tipfy", "stars": 9, "license": "None", "language": "python", "size": 3792}
{"docstring": "\"\"\"Update the catalog based on the given template catalog.\n\n>>> from babel.messages import Catalog\n>>> template = Catalog()\n>>> template.add('green', locations=[('main.py', 99)])\n>>> template.add('blue', locations=[('main.py', 100)])\n>>> template.add(('salad', 'salads'), locations=[('util.py', 42)])\n>>> catalog = Catalog(locale='de_DE')\n>>> catalog.add('blue', u'blau', locations=[('main.py', 98)])\n>>> catalog.add('head', u'Kopf', locations=[('util.py', 33)])\n>>> catalog.add(('salad', 'salads'), (u'Salat', u'Salate'),\n...             locations=[('util.py', 38)])\n\n>>> catalog.update(template)\n>>> len(catalog)\n3\n\n>>> msg1 = catalog['green']\n>>> msg1.string\n>>> msg1.locations\n[('main.py', 99)]\n\n>>> msg2 = catalog['blue']\n>>> msg2.string\nu'blau'\n>>> msg2.locations\n[('main.py', 100)]\n\n>>> msg3 = catalog['salad']\n>>> msg3.string\n(u'Salat', u'Salate')\n>>> msg3.locations\n[('util.py', 42)]\n\nMessages that are in the catalog but not in the template are removed\nfrom the main collection, but can still be accessed via the `obsolete`\nmember:\n\n>>> 'head' in catalog\nFalse\n>>> catalog.obsolete.values()\n[<Message 'head' (flags: [])>]\n\n:param template: the reference catalog, usually read from a POT file\n:param no_fuzzy_matching: whether to use fuzzy matching of message IDs\n\"\"\"\n", "func_signal": "def update(self, template, no_fuzzy_matching=False):\n", "code": "messages = self._messages\nremaining = messages.copy()\nself._messages = odict()\n\n# Prepare for fuzzy matching\nfuzzy_candidates = []\nif not no_fuzzy_matching:\n    fuzzy_candidates = [\n        self._key_for(msgid) for msgid in messages\n        if msgid and messages[msgid].string\n    ]\nfuzzy_matches = set()\n\ndef _merge(message, oldkey, newkey):\n    message = message.clone()\n    fuzzy = False\n    if oldkey != newkey:\n        fuzzy = True\n        fuzzy_matches.add(oldkey)\n        oldmsg = messages.get(oldkey)\n        if isinstance(oldmsg.id, basestring):\n            message.previous_id = [oldmsg.id]\n        else:\n            message.previous_id = list(oldmsg.id)\n    else:\n        oldmsg = remaining.pop(oldkey, None)\n    message.string = oldmsg.string\n    if isinstance(message.id, (list, tuple)):\n        if not isinstance(message.string, (list, tuple)):\n            fuzzy = True\n            message.string = tuple(\n                [message.string] + ([u''] * (len(message.id) - 1))\n            )\n        elif len(message.string) != self.num_plurals:\n            fuzzy = True\n            message.string = tuple(message.string[:len(oldmsg.string)])\n    elif isinstance(message.string, (list, tuple)):\n        fuzzy = True\n        message.string = message.string[0]\n    message.flags |= oldmsg.flags\n    if fuzzy:\n        message.flags |= set([u'fuzzy'])\n    self[message.id] = message\n\nfor message in template:\n    if message.id:\n        key = self._key_for(message.id)\n        if key in messages:\n            _merge(message, key, key)\n        else:\n            if no_fuzzy_matching is False:\n                # do some fuzzy matching with difflib\n                matches = get_close_matches(key.lower().strip(),\n                                            fuzzy_candidates, 1)\n                if matches:\n                    _merge(message, matches[0], key)\n                    continue\n\n            self[message.id] = message\n\nself.obsolete = odict()\nfor msgid in remaining:\n    if no_fuzzy_matching or msgid not in fuzzy_matches:\n        self.obsolete[msgid] = remaining[msgid]", "path": "app\\distlib\\babel\\messages\\catalog.py", "repo_name": "kosmikko/bloggart-tipfy", "stars": 9, "license": "None", "language": "python", "size": 3792}
{"docstring": "\"\"\"Encode `data`, write it to a single file, and return it.\"\"\"\n", "func_signal": "def write(self, data):\n", "code": "output = self.encode(data)\nif not self.opened:\n    self.open()\ntry:\n    self.destination.write(output)\nfinally:\n    if self.autoclose:\n        self.close()\nreturn output", "path": "app\\lib\\docutils\\io.py", "repo_name": "kosmikko/bloggart-tipfy", "stars": 9, "license": "None", "language": "python", "size": 3792}
{"docstring": "\"\"\"Run various validation checks on the translations in the catalog.\n\nFor every message which fails validation, this method yield a\n``(message, errors)`` tuple, where ``message`` is the `Message` object\nand ``errors`` is a sequence of `TranslationError` objects.\n\n:rtype: ``iterator``\n\"\"\"\n", "func_signal": "def check(self):\n", "code": "for message in self._messages.values():\n    errors = message.check(catalog=self)\n    if errors:\n        yield message, errors", "path": "app\\distlib\\babel\\messages\\catalog.py", "repo_name": "kosmikko/bloggart-tipfy", "stars": 9, "license": "None", "language": "python", "size": 3792}
{"docstring": "\"\"\"Add or update the message with the specified ID.\n\n>>> catalog = Catalog()\n>>> catalog[u'foo'] = Message(u'foo')\n>>> catalog[u'foo']\n<Message u'foo' (flags: [])>\n\nIf a message with that ID is already in the catalog, it is updated\nto include the locations and flags of the new message.\n\n>>> catalog = Catalog()\n>>> catalog[u'foo'] = Message(u'foo', locations=[('main.py', 1)])\n>>> catalog[u'foo'].locations\n[('main.py', 1)]\n>>> catalog[u'foo'] = Message(u'foo', locations=[('utils.py', 5)])\n>>> catalog[u'foo'].locations\n[('main.py', 1), ('utils.py', 5)]\n\n:param id: the message ID\n:param message: the `Message` object\n\"\"\"\n", "func_signal": "def __setitem__(self, id, message):\n", "code": "assert isinstance(message, Message), 'expected a Message object'\nkey = self._key_for(id)\ncurrent = self._messages.get(key)\nif current:\n    if message.pluralizable and not current.pluralizable:\n        # The new message adds pluralization\n        current.id = message.id\n        current.string = message.string\n    current.locations = list(distinct(current.locations +\n                                      message.locations))\n    current.auto_comments = list(distinct(current.auto_comments +\n                                          message.auto_comments))\n    current.user_comments = list(distinct(current.user_comments +\n                                          message.user_comments))\n    current.flags |= message.flags\n    message = current\nelif id == '':\n    # special treatment for the header message\n    headers = message_from_string(message.string.encode(self.charset))\n    self.mime_headers = headers.items()\n    self.header_comment = '\\n'.join(['# %s' % comment for comment\n                                     in message.user_comments])\n    self.fuzzy = message.fuzzy\nelse:\n    if isinstance(id, (list, tuple)):\n        assert isinstance(message.string, (list, tuple)), \\\n            'Expected sequence but got %s' % type(message.string)\n    self._messages[key] = message", "path": "app\\distlib\\babel\\messages\\catalog.py", "repo_name": "kosmikko/bloggart-tipfy", "stars": 9, "license": "None", "language": "python", "size": 3792}
{"docstring": "\"\"\"Return a static resource from the shared folder.\"\"\"\n", "func_signal": "def get_resource(self, request, filename):\n", "code": "filename = join(dirname(__file__), 'shared', basename(filename))\nif isfile(filename):\n    mimetype = mimetypes.guess_type(filename)[0] \\\n        or 'application/octet-stream'\n    f = file(filename, 'rb')\n    try:\n        return Response(f.read(), mimetype=mimetype)\n    finally:\n        f.close()\nreturn Response('Not Found', status=404)", "path": "app\\distlib\\werkzeug\\debug\\__init__.py", "repo_name": "kosmikko/bloggart-tipfy", "stars": 9, "license": "None", "language": "python", "size": 3792}
{"docstring": "\"\"\"Set up environment for a performance test.\n\nThis should not normally be implemented in subclasses - instead,\nimplement the pre_test() method, which is called by this method after\nperforming the standard test setup process.\n\nVarious directories are available:\n\n - self.tempdir: A temporary directory, which will be cleared after\n   each test is run.\n - self.sourcedatadir: A directory containing fixed source data (same\n   directory for all tests).\n - self.builtdatadir: A directory for storing persistent generated data\n   for a test.\n\n\"\"\"\n", "func_signal": "def setUp(self):\n", "code": "self.tempdir = tempfile.mkdtemp()\ntestdatadir = os.path.join(os.path.dirname(__file__), 'data')\nself.sourcedatadir = os.path.join(testdatadir, 'source')\nself.builtdatadir = os.path.join(testdatadir, 'built',\n                                 'test_' + type(self).__name__)\nhashpath = os.path.join(testdatadir, 'built',\n                        'hash_' + type(self).__name__ + '.txt')\n\nif not os.path.exists(self.builtdatadir):\n    current_hash = None\nelse:\n    try:\n        current_hash = open(hashpath).read()\n    except IOError:\n        current_hash = None\n\nnew_hash = self.hash_data()\nif current_hash is None or current_hash != new_hash:\n    try:\n        if os.path.exists(self.builtdatadir):\n            shutil.rmtree(self.builtdatadir)\n        os.makedirs(self.builtdatadir)\n        self.build_data()\n    except:\n        if os.path.exists(hashpath):\n            os.unlink(hashpath)\n        raise\n\n    fd = open(hashpath, 'w')\n    fd.write(new_hash)\n    fd.close()\n\nself.pre_test()\nself.timers = {}\nself.timer_order = []\nself.start_timer('main')", "path": "xappy\\perftest\\harness.py", "repo_name": "miracle2k/xappy", "stars": 8, "license": "gpl-2.0", "language": "python", "size": 40557}
{"docstring": "\"\"\"Test that an empty query is connected when unserialised.\n\n\"\"\"\n", "func_signal": "def test_empty_query_connected(self):\n", "code": "q1 = xappy.Query()\ns1 = q1.evalable_repr()\nself.assertEqual(s1, 'Query()')\nq2 = self.sconn.query_from_evalable(s1)\nr1 = q2.search(0, 10)\nself.assertEqual(r1.endrank, 0)", "path": "xappy\\unittests\\query_serialise.py", "repo_name": "miracle2k/xappy", "stars": 8, "license": "gpl-2.0", "language": "python", "size": 40557}
{"docstring": "\"\"\"Test the SearchConnection.query_id() method.\n\n\"\"\"\n", "func_signal": "def test_query_id(self):\n", "code": "query = self.sconn.query_id([\"121\", \"123\"])\nresults = query.search(0, 10)\nself.assertEqual([r.id for r in results], [\"121\", \"123\"])\n\nquery = self.sconn.query_id([\"121\"])\nresults = query.search(0, 10)\nself.assertEqual([r.id for r in results], [\"121\"])\n\nquery = self.sconn.query_id(\"121\")\nresults = query.search(0, 10)\nself.assertEqual([r.id for r in results], [\"121\"])\n\nself.assertEqual(query.evalable_repr(), \"conn.query_id('121')\")", "path": "xappy\\unittests\\query_id.py", "repo_name": "miracle2k/xappy", "stars": 8, "license": "gpl-2.0", "language": "python", "size": 40557}
{"docstring": "\"\"\"Index a path.\"\"\"\n", "func_signal": "def index_path(iconn, docpath):\n", "code": "count = 0\nfor dirpath, dirnames, filenames in os.walk(docpath):\n    for filename in filenames:\n        filepath = os.path.join(dirpath, filename)\n        index_file(iconn, filepath)\n        count += 1\nreturn count", "path": "examples\\fileindex.py", "repo_name": "miracle2k/xappy", "stars": 8, "license": "gpl-2.0", "language": "python", "size": 40557}
{"docstring": "\"\"\"Test serialising of composed queries.\n\n\"\"\"\n", "func_signal": "def test_query_compose_serialise(self):\n", "code": "q1 = self.sconn.query_field('a', 'A1')\nq2 = self.sconn.query_field('a', 'A2')\nq3 = self.sconn.query_field('a', 'A3')\nq4 = self.sconn.query_field('a', 'A4')\n\nq = xappy.Query.compose(xappy.Query.OP_OR, (q1,))\nr = q.evalable_repr()\nself.assertEqual(r, dedent(\"\"\"\n    conn.query_field('a', value='A1')\n\"\"\"))\n\nq = xappy.Query.compose(xappy.Query.OP_OR, (q1, q2))\nr = q.evalable_repr()\nself.assertEqual(r, dedent(\"\"\"\n    (conn.query_field('a', value='A1') |\n     conn.query_field('a', value='A2'))\n\"\"\"))\n\nq = xappy.Query.compose(xappy.Query.OP_OR, (q1,\n    xappy.Query.compose(xappy.Query.OP_OR, (q2, q3, q4))))\nr = q.evalable_repr()\nself.assertEqual(r, dedent(\"\"\"\n    Query.compose(Query.OP_OR,\n                  (conn.query_field('a', value='A1'),\n                   conn.query_field('a', value='A2'),\n                   conn.query_field('a', value='A3'),\n                   conn.query_field('a', value='A4')))\n\"\"\"))\n\nq = xappy.Query.compose(xappy.Query.OP_OR, (q1,\n    xappy.Query.compose(xappy.Query.OP_AND, (q2, q3, q4))))\nr = q.evalable_repr()\nself.assertEqual(r, dedent(\"\"\"\n                  (conn.query_field('a', value='A1') |\n                  Query.compose(Query.OP_AND,\n                                (conn.query_field('a', value='A2'),\n                                conn.query_field('a', value='A3'),\n                                conn.query_field('a', value='A4'))))\n\"\"\"))\n\nq = xappy.Query.compose(xappy.Query.OP_OR, (q1,\n    xappy.Query.compose(xappy.Query.OP_AND, (q2, q3))))\nr = q.evalable_repr()\nself.assertEqual(r, dedent(\"\"\"\n                  (conn.query_field('a', value='A1') |\n                   (conn.query_field('a', value='A2') &\n                    conn.query_field('a', value='A3')))\n\"\"\"))", "path": "xappy\\unittests\\query_serialise.py", "repo_name": "miracle2k/xappy", "stars": 8, "license": "gpl-2.0", "language": "python", "size": 40557}
{"docstring": "\"\"\"\n\n - description: textual description of this run (excluding information\n   about other parameters specified here)\n - flushspeed: frequency (ie, number of adds) with which to explicitly\n   call flush() on the database.\n - maxdocs: maximum number of documents to add (stop automatically\n   after this many).\n - logspeed: make a log entry each time we add \"logspeed\" documents.\n - noindex: If True, don't do an indexing run (use existing database)\n\n\"\"\"\n", "func_signal": "def __init__(self, inputfile, description, flushspeed=10000, maxdocs=None, logspeed=1000, noindex=False):\n", "code": "self.inputfile = os.path.abspath(inputfile)\nself.description = description\nself.flushspeed = flushspeed\nself.maxdocs = maxdocs\nself.logspeed = logspeed\nself.noindex = noindex\nself.queryruns = []", "path": "perftest\\perftest.py", "repo_name": "miracle2k/xappy", "stars": 8, "license": "gpl-2.0", "language": "python", "size": 40557}
{"docstring": "\"\"\"Prepare to iterate by document ID.\n\nThis makes a Xapian database, in which each document represents a\ncached query, and is indexed by terms corresponding to the document IDs\nof the making terms.\n\nThis is used to get the inverse of the queryid->docid list mapping\nprovided to the cache.\n\n\"\"\"\n", "func_signal": "def prepare_iter_by_docid(self):\n", "code": "if self.inverted:\n    return\n\nif not os.path.exists(self.dbpath):\n    self.inverted = True\n    return\n\nshutil.rmtree(self.inverted_db_path, ignore_errors=True)\ninvdb = xapian.WritableDatabase(self.inverted_db_path,\n                                xapian.DB_CREATE_OR_OPEN)\ntry:\n    for qid in self.iter_queryids():\n        doc = xapian.Document()\n        for rank, docid in enumerate(self.get_hits(qid)):\n            # We store the docid encoded as the term (encoded such that\n            # it will sort lexicographically into numeric order), and\n            # the rank as the wdf.\n            term = '%x' % docid\n            term = ('%x' % len(term)) + term\n            doc.add_term(term, rank)\n        newdocid = invdb.add_document(doc)\n\n        assert(newdocid == qid + 1)\n    invdb.flush()\nfinally:\n    invdb.close()\n\nself.inverted = True", "path": "xappy\\cachemanager\\xapian_manager.py", "repo_name": "miracle2k/xappy", "stars": 8, "license": "gpl-2.0", "language": "python", "size": 40557}
{"docstring": "\"\"\"Start the named timer.\n\n`desc` is a description of what is being timed.  This is ignored if the\ntimer has been started before.\n\n\"\"\"\n", "func_signal": "def start_timer(self, timer, desc=None):\n", "code": "if desc is None:\n    desc = timer\ntry:\n    data = self.timers[timer]\n    data[2] = time.time()\nexcept KeyError:\n    data = [0, 0.0, time.time(), desc]\n    self.timers[timer] = data\n    self.timer_order.append(timer)", "path": "xappy\\perftest\\harness.py", "repo_name": "miracle2k/xappy", "stars": 8, "license": "gpl-2.0", "language": "python", "size": 40557}
{"docstring": "\"\"\"Format the timers for display.\n\n\"\"\"\n", "func_signal": "def format_timers(self):\n", "code": "result = []\nfor timer in self.timer_order:\n    count, totaltime, starttime, desc = self.timers[timer]\n    if count == 1:\n        result.append('%12.6fs: %s' % (totaltime, desc))\n    else:\n        result.append('%12.6fs: %s (%d instances)' % (totaltime, desc, count))\nreturn '\\n'.join(result)", "path": "xappy\\perftest\\harness.py", "repo_name": "miracle2k/xappy", "stars": 8, "license": "gpl-2.0", "language": "python", "size": 40557}
{"docstring": "\"\"\"Set up sys.path to allow us to import Xappy when run uninstalled.\n\n\"\"\"\n", "func_signal": "def _setup_path():\n", "code": "abspath = os.path.abspath(__file__)\ndirname = os.path.dirname(abspath)\ndirname, ourdir = os.path.split(dirname)\ndirname, parentdir = os.path.split(dirname)\nif (parentdir, ourdir) == ('xappy', 'examples'):\n    sys.path.insert(0, '..')", "path": "examples\\fileindex.py", "repo_name": "miracle2k/xappy", "stars": 8, "license": "gpl-2.0", "language": "python", "size": 40557}
{"docstring": "\"\"\"Index a file.\"\"\"\n", "func_signal": "def index_file(iconn, filepath):\n", "code": "filepath = canonical_path(filepath)\ndoc = xappy.UnprocessedDocument()\ndoc.fields.append(xappy.Field('path', filepath))\n\ncomponents = filepath\nwhile True:\n    components, dirname = os.path.split(components)\n    if len(dirname) == 0 or components == '/':\n        break\n    doc.fields.append(xappy.Field('pathcomponent', components))\n\nindex_content(doc, filepath)\niconn.add(doc)\n\nreturn 1", "path": "examples\\fileindex.py", "repo_name": "miracle2k/xappy", "stars": 8, "license": "gpl-2.0", "language": "python", "size": 40557}
{"docstring": "# Generate a \"total time\" versus \"total documents\" plot\n", "func_signal": "def generate_figures(log, outprefix, pretitle):\n", "code": "total_times = [row.tottime for row in log]\nquery_times = [row.time for row in log]\n\npylab.figure(figsize=(8,12))\npylab.subplot(311)\npylab.plot(total_times)\npylab.xlabel('Queries completed')\npylab.ylabel('Total time (seconds)')\npylab.title(pretitle + '\\nQueries completed after given time')\n\npylab.subplot(312)\npylab.axis([0, len(query_times), 0, max(query_times) * 1.05])\npylab.plot(query_times)\npylab.xlabel('Queries completed')\npylab.ylabel('Query time (seconds)')\npylab.title('Query times')\n\npylab.subplot(313)\npylab.axis([0, len(query_times), 0, 50])\npylab.hist(query_times, 50, log=\"true\")\npylab.xlabel('Query time (seconds)')\npylab.ylabel('Queries')\npylab.title('Query time histogram')\npylab.savefig(outprefix + \"query_times.png\", format=\"png\")", "path": "perftest\\analyse_searchlogs.py", "repo_name": "miracle2k/xappy", "stars": 8, "license": "gpl-2.0", "language": "python", "size": 40557}
{"docstring": "\"\"\"Reset the named timers to 0.\n\n\"\"\"\n", "func_signal": "def reset_timers(self, timers):\n", "code": "for timer in timers:\n    data = self.timers.get(timer, None)\n    if data is not None:\n        data[0] = 0\n        data[1] = 0.0", "path": "xappy\\perftest\\harness.py", "repo_name": "miracle2k/xappy", "stars": 8, "license": "gpl-2.0", "language": "python", "size": 40557}
{"docstring": "\"\"\"Stop the named timer.\n\n\"\"\"\n", "func_signal": "def stop_timer(self, timer):\n", "code": "data = self.timers[timer]\nif data[2] is None:\n    return\nnow = time.time()\ndata[0] += 1\ndata[1] += now - data[2]\ndata[2] = None", "path": "xappy\\perftest\\harness.py", "repo_name": "miracle2k/xappy", "stars": 8, "license": "gpl-2.0", "language": "python", "size": 40557}
{"docstring": "\"\"\"Set up sys.path to allow us to import Xappy when run uninstalled.\n\n\"\"\"\n", "func_signal": "def setup_path():\n", "code": "abspath = os.path.abspath(__file__)\ndirname = os.path.dirname(abspath)\ndirname = os.path.dirname(dirname)\nif os.path.exists(os.path.join(dirname, 'xappy')):\n    sys.path.insert(0, dirname)", "path": "perftest\\setuppaths.py", "repo_name": "miracle2k/xappy", "stars": 8, "license": "gpl-2.0", "language": "python", "size": 40557}
{"docstring": "\"\"\"Index the content of the file.\"\"\"\n", "func_signal": "def index_content(doc, filepath):\n", "code": "fd = open(filepath)\ncontents = fd.read()\nfd.close()\ntry:\n    contents = unicode(contents)\nexcept UnicodeDecodeError:\n    return\ndoc.fields.append(xappy.Field('text', contents))", "path": "examples\\fileindex.py", "repo_name": "miracle2k/xappy", "stars": 8, "license": "gpl-2.0", "language": "python", "size": 40557}
{"docstring": "\"\"\"Convert a path to a canonical form.\"\"\"\n", "func_signal": "def canonical_path(path):\n", "code": "path = os.path.realpath(path)\npath = os.path.normpath(path)\npath = os.path.normcase(path)\nreturn path", "path": "examples\\fileindex.py", "repo_name": "miracle2k/xappy", "stars": 8, "license": "gpl-2.0", "language": "python", "size": 40557}
{"docstring": "\"\"\"Implementation of iter_by_docid() which uses a temporary Xapian\ndatabase to perform the inverting of the queryid->docid list mapping,\nto return the docid->queryid list mapping.\n\nThis uses an on-disk database, so is probably a bit slower than the\nnaive implementation for small cases, but should scale arbitrarily (as\nwell as Xapian does, anyway).\n\nIt would be faster if we could tell Xapian not to perform fsyncs for\nthe temporary database.\n\n\"\"\"\n", "func_signal": "def iter_by_docid(self):\n", "code": "self.prepare_iter_by_docid()\n\nif os.path.exists(self.dbpath):\n    invdb = xapian.Database(self.inverted_db_path)\nelse:\n    invdb = xapian.Database()\n\ntry:\n\n    for item in invdb.allterms():\n        docid = int(item.term[1:], 16)\n        items = tuple((item.docid - 1, item.wdf) for item in invdb.postlist(item.term))\n        yield docid, items\n    invdb.close()\n\nfinally:\n    invdb.close()", "path": "xappy\\cachemanager\\xapian_manager.py", "repo_name": "miracle2k/xappy", "stars": 8, "license": "gpl-2.0", "language": "python", "size": 40557}
{"docstring": "\"\"\"Create a new index, and set up its field structure.\n\n\"\"\"\n", "func_signal": "def create_index(dbpath):\n", "code": "iconn = xappy.IndexerConnection(dbpath)\n\niconn.add_field_action('path', xappy.FieldActions.STORE_CONTENT)\niconn.add_field_action('path', xappy.FieldActions.INDEX_EXACT)\niconn.add_field_action('pathcomponent', xappy.FieldActions.INDEX_EXACT)\niconn.add_field_action('text', xappy.FieldActions.STORE_CONTENT)\niconn.add_field_action('text', xappy.FieldActions.INDEX_FREETEXT, language='en')\n\niconn.close()", "path": "examples\\fileindex.py", "repo_name": "miracle2k/xappy", "stars": 8, "license": "gpl-2.0", "language": "python", "size": 40557}
{"docstring": "\"\"\"Test serialising of queries.\n\n\"\"\"\n", "func_signal": "def test_query_serialise(self):\n", "code": "q1 = self.sconn.query_field('a', 'America')\nq2 = self.sconn.query_field('b', 'America')\nq3 = self.sconn.query_range('c', 'Ache', 'Atlas')\nq4 = self.sconn.query_range('d', 0.0, 2.0)\nq5 = self.sconn.query_range('d', 0.0, None)\nq6 = self.sconn.query_range('d', None, 2.0)\nq7 = self.sconn.query_range('d', None, None)\nq8 = self.sconn.query_facet('e', 'Atlantic')\nq9 = self.sconn.query_facet('f', (0.0, 2.0))\nq10 = self.sconn.query_field('g')\nq11 = self.sconn.query_none()\nq12 = self.sconn.query_all()\nq13 = xappy.Query()\n\nqueries = (q1, q2, q3, q4, q5, q6, q7, q8, q9, q10, q11, q12, q13,\n           q1 | q2,\n           q1 & q2,\n           q1 ^ q2,\n           q1.adjust(q10),\n           q1.filter(q2),\n           q1.and_not(q2),\n           q1.and_maybe(q2),\n           xappy.Query.compose(xappy.Query.OP_OR, (q1, q2, q3, q4)),\n           xappy.Query.compose(xappy.Query.OP_OR, (q1,\n                 xappy.Query.compose(xappy.Query.OP_OR, (q2, q3, q4)))),\n          )\n\nfor q in queries:\n    q_repr = q.evalable_repr()\n    q_unrepr = self.sconn.query_from_evalable(q_repr)\n    q_repr2 = q_unrepr.evalable_repr()\n    self.assertEqual(repr(q), repr(q_unrepr))\n    self.assertEqual(q_repr, q_repr2)", "path": "xappy\\unittests\\query_serialise.py", "repo_name": "miracle2k/xappy", "stars": 8, "license": "gpl-2.0", "language": "python", "size": 40557}
{"docstring": "\"\"\"Make sure we are connected.\n\n\"\"\"\n", "func_signal": "def _CheckSocket(self):\n", "code": "if self.socket is None:\n  raise ProtocolError(\"Connection is closed\")", "path": "lib\\luxi.py", "repo_name": "sigmike/ganeti", "stars": 8, "license": "gpl-2.0", "language": "python", "size": 4840}
{"docstring": "\"\"\"(Re)initialize the transport if needed.\n\n\"\"\"\n", "func_signal": "def _InitTransport(self):\n", "code": "if self.transport is None:\n  self.transport = self.transport_class(self.address,\n                                        timeouts=self.timeouts)", "path": "lib\\luxi.py", "repo_name": "sigmike/ganeti", "stars": 8, "license": "gpl-2.0", "language": "python", "size": 4840}
{"docstring": "\"\"\"Show the list of mapped resources.\n\n@return: a dictionary with 'name' and 'uri' keys for each of them.\n\n\"\"\"\n", "func_signal": "def GET():\n", "code": "root_pattern = re.compile('^R_([a-zA-Z0-9]+)$')\n\nrootlist = []\nfor handler in CONNECTOR.values():\n  m = root_pattern.match(handler.__name__)\n  if m:\n    name = m.group(1)\n    if name != 'root':\n      rootlist.append(name)\n\nreturn baserlib.BuildUriList(rootlist, \"/%s\")", "path": "lib\\rapi\\connector.py", "repo_name": "sigmike/ganeti", "stars": 8, "license": "gpl-2.0", "language": "python", "size": 4840}
{"docstring": "\"\"\"Resource mapper constructor.\n\n@param connector: a dictionary, mapping method name with URL path regexp\n\n\"\"\"\n", "func_signal": "def __init__(self, connector=None):\n", "code": "if connector is None:\n  connector = CONNECTOR\nself._connector = connector", "path": "lib\\rapi\\connector.py", "repo_name": "sigmike/ganeti", "stars": 8, "license": "gpl-2.0", "language": "python", "size": 4840}
{"docstring": "\"\"\"Open and lock job queue.\n\nIf necessary, the queue is automatically initialized.\n\n@type must_lock: bool\n@param must_lock: Whether an exclusive lock must be held.\n@rtype: utils.FileLock\n@return: Lock object for the queue. This can be used to change the\n         locking mode.\n\n\"\"\"\n", "func_signal": "def InitAndVerifyQueue(must_lock):\n", "code": "getents = runtime.GetEnts()\n\n# Lock queue\nqueue_lock = utils.FileLock.Open(constants.JOB_QUEUE_LOCK_FILE)\ntry:\n  # The queue needs to be locked in exclusive mode to write to the serial and\n  # version files.\n  if must_lock:\n    queue_lock.Exclusive(blocking=True)\n    holding_lock = True\n  else:\n    try:\n      queue_lock.Exclusive(blocking=False)\n      holding_lock = True\n    except errors.LockError:\n      # Ignore errors and assume the process keeping the lock checked\n      # everything.\n      holding_lock = False\n\n  if holding_lock:\n    # Verify version\n    version = ReadVersion()\n    if version is None:\n      # Write new version file\n      utils.WriteFile(constants.JOB_QUEUE_VERSION_FILE,\n                      uid=getents.masterd_uid, gid=getents.masterd_gid,\n                      data=\"%s\\n\" % constants.JOB_QUEUE_VERSION)\n\n      # Read again\n      version = ReadVersion()\n\n    if version != constants.JOB_QUEUE_VERSION:\n      raise errors.JobQueueError(\"Found job queue version %s, expected %s\",\n                                 version, constants.JOB_QUEUE_VERSION)\n\n    serial = ReadSerial()\n    if serial is None:\n      # Write new serial file\n      utils.WriteFile(constants.JOB_QUEUE_SERIAL_FILE,\n                      uid=getents.masterd_uid, gid=getents.masterd_gid,\n                      data=\"%s\\n\" % 0)\n\n      # Read again\n      serial = ReadSerial()\n\n    if serial is None:\n      # There must be a serious problem\n      raise errors.JobQueueError(\"Can't read/parse the job queue\"\n                                 \" serial file\")\n\n    if not must_lock:\n      # There's no need for more error handling. Closing the lock\n      # file below in case of an error will unlock it anyway.\n      queue_lock.Unlock()\n\nexcept:\n  queue_lock.Close()\n  raise\n\nreturn queue_lock", "path": "lib\\jstore.py", "repo_name": "sigmike/ganeti", "stars": 8, "license": "gpl-2.0", "language": "python", "size": 4840}
{"docstring": "\"\"\"Return a list of resources underneath given id.\n\nThis is to generalize querying of version resources lists.\n\n@return: a list of resources names.\n\n\"\"\"\n", "func_signal": "def _getResources(id_):\n", "code": "r_pattern = re.compile('^R_%s_([a-zA-Z0-9]+)$' % id_)\n\nrlist = []\nfor handler in CONNECTOR.values():\n  m = r_pattern.match(handler.__name__)\n  if m:\n    name = m.group(1)\n    rlist.append(name)\n\nreturn rlist", "path": "lib\\rapi\\connector.py", "repo_name": "sigmike/ganeti", "stars": 8, "license": "gpl-2.0", "language": "python", "size": 4840}
{"docstring": "\"\"\"Test automatic restart of instance by ganeti-watcher.\n\n\"\"\"\n", "func_signal": "def TestInstanceAutomaticRestart(node, instance):\n", "code": "master = qa_config.GetMasterNode()\ninst_name = qa_utils.ResolveInstanceName(instance)\n\n_ResetWatcherDaemon()\n_XmShutdownInstance(node, inst_name)\n\n_RunWatcherDaemon()\ntime.sleep(5)\n\nif not _InstanceRunning(node, inst_name):\n  raise qa_error.Error(\"Daemon didn't restart instance\")\n\ncmd = ['gnt-instance', 'info', inst_name]\nAssertEqual(StartSSH(master['primary'],\n                     utils.ShellQuoteArgs(cmd)).wait(), 0)", "path": "qa\\qa_daemon.py", "repo_name": "sigmike/ganeti", "stars": 8, "license": "gpl-2.0", "language": "python", "size": 4840}
{"docstring": "\"\"\"Close the transport, ignoring errors.\n\n\"\"\"\n", "func_signal": "def _CloseTransport(self):\n", "code": "if self.transport is None:\n  return\ntry:\n  old_transp = self.transport\n  self.transport = None\n  old_transp.Close()\nexcept Exception: # pylint: disable-msg=W0703\n  pass", "path": "lib\\luxi.py", "repo_name": "sigmike/ganeti", "stars": 8, "license": "gpl-2.0", "language": "python", "size": 4840}
{"docstring": "\"\"\"Find method for a given URI.\n\n@param uri: string with URI\n\n@return: None if no method is found or a tuple containing\n    the following fields:\n        - method: name of method mapped to URI\n        - items: a list of variable intems in the path\n        - args: a dictionary with additional parameters from URL\n\n\"\"\"\n", "func_signal": "def getController(self, uri):\n", "code": "if '?' in uri:\n  (path, query) = uri.split('?', 1)\n  args = cgi.parse_qs(query)\nelse:\n  path = uri\n  query = None\n  args = {}\n\nresult = None\n\nfor key, handler in self._connector.iteritems():\n  # Regex objects\n  if hasattr(key, \"match\"):\n    m = key.match(path)\n    if m:\n      result = (handler, list(m.groups()), args)\n      break\n\n  # String objects\n  elif key == path:\n    result = (handler, [], args)\n    break\n\nif result:\n  return result\nelse:\n  raise http.HttpNotFound()", "path": "lib\\rapi\\connector.py", "repo_name": "sigmike/ganeti", "stars": 8, "license": "gpl-2.0", "language": "python", "size": 4840}
{"docstring": "# Send request and wait for response\n", "func_signal": "def _SendMethodCall(self, data):\n", "code": "try:\n  self._InitTransport()\n  return self.transport.Call(data)\nexcept Exception:\n  self._CloseTransport()\n  raise", "path": "lib\\luxi.py", "repo_name": "sigmike/ganeti", "stars": 8, "license": "gpl-2.0", "language": "python", "size": 4840}
{"docstring": "\"\"\"Test five consecutive instance failures.\n\n\"\"\"\n", "func_signal": "def TestInstanceConsecutiveFailures(node, instance):\n", "code": "master = qa_config.GetMasterNode()\ninst_name = qa_utils.ResolveInstanceName(instance)\n\n_ResetWatcherDaemon()\n\nfor should_start in ([True] * 5) + [False]:\n  _XmShutdownInstance(node, inst_name)\n  _RunWatcherDaemon()\n  time.sleep(5)\n\n  if bool(_InstanceRunning(node, inst_name)) != should_start:\n    if should_start:\n      msg = \"Instance not started when it should\"\n    else:\n      msg = \"Instance started when it shouldn't\"\n    raise qa_error.Error(msg)\n\ncmd = ['gnt-instance', 'info', inst_name]\nAssertEqual(StartSSH(master['primary'],\n                     utils.ShellQuoteArgs(cmd)).wait(), 0)", "path": "qa\\qa_daemon.py", "repo_name": "sigmike/ganeti", "stars": 8, "license": "gpl-2.0", "language": "python", "size": 4840}
{"docstring": "\"\"\"Constructor for the Client class.\n\nArguments:\n  - address: a valid address the the used transport class\n  - timeout: a list of timeouts, to be used on connect and read/write\n\nThere are two timeouts used since we might want to wait for a long\ntime for a response, but the connect timeout should be lower.\n\nIf not passed, we use a default of 10 and respectively 60 seconds.\n\nNote that on reading data, since the timeout applies to an\ninvidual receive, it might be that the total duration is longer\nthan timeout value passed (we make a hard limit at twice the read\ntimeout).\n\n\"\"\"\n", "func_signal": "def __init__(self, address, timeouts=None):\n", "code": "self.address = address\nif timeouts is None:\n  self._ctimeout, self._rwtimeout = DEF_CTMO, DEF_RWTO\nelse:\n  self._ctimeout, self._rwtimeout = timeouts\n\nself.socket = None\nself._buffer = \"\"\nself._msgs = collections.deque()\n\ntry:\n  self.socket = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\n\n  # Try to connect\n  try:\n    utils.Retry(self._Connect, 1.0, self._ctimeout,\n                args=(self.socket, address, self._ctimeout))\n  except utils.RetryTimeout:\n    raise TimeoutError(\"Connect timed out\")\n\n  self.socket.settimeout(self._rwtimeout)\nexcept (socket.error, NoMasterError):\n  if self.socket is not None:\n    self.socket.close()\n  self.socket = None\n  raise", "path": "lib\\luxi.py", "repo_name": "sigmike/ganeti", "stars": 8, "license": "gpl-2.0", "language": "python", "size": 4840}
{"docstring": "\"\"\"Send a message and wait for the response.\n\nThis is just a wrapper over Send and Recv.\n\n\"\"\"\n", "func_signal": "def Call(self, msg):\n", "code": "self.Send(msg)\nreturn self.Recv()", "path": "lib\\luxi.py", "repo_name": "sigmike/ganeti", "stars": 8, "license": "gpl-2.0", "language": "python", "size": 4840}
{"docstring": "\"\"\"Formats a LUXI request message.\n\n\"\"\"\n# Build request\n", "func_signal": "def FormatRequest(method, args):\n", "code": "request = {\n  KEY_METHOD: method,\n  KEY_ARGS: args,\n  }\n\n# Serialize the request\nreturn serializer.DumpJson(request, indent=False)", "path": "lib\\luxi.py", "repo_name": "sigmike/ganeti", "stars": 8, "license": "gpl-2.0", "language": "python", "size": 4840}
{"docstring": "\"\"\"Removes the watcher daemon's state file.\n\nArgs:\n  node: Node to be reset\n\"\"\"\n", "func_signal": "def _ResetWatcherDaemon():\n", "code": "master = qa_config.GetMasterNode()\n\ncmd = ['rm', '-f', constants.WATCHER_STATEFILE]\nAssertEqual(StartSSH(master['primary'],\n                     utils.ShellQuoteArgs(cmd)).wait(), 0)", "path": "qa\\qa_daemon.py", "repo_name": "sigmike/ganeti", "stars": 8, "license": "gpl-2.0", "language": "python", "size": 4840}
{"docstring": "\"\"\"Send a LUXI request via a transport and return the response.\n\n\"\"\"\n", "func_signal": "def CallLuxiMethod(transport_cb, method, args):\n", "code": "assert callable(transport_cb)\n\nrequest_msg = FormatRequest(method, args)\n\n# Send request and wait for response\nresponse_msg = transport_cb(request_msg)\n\n(success, result) = ParseResponse(response_msg)\n\nif success:\n  return result\n\nerrors.MaybeRaise(result)\nraise RequestError(result)", "path": "lib\\luxi.py", "repo_name": "sigmike/ganeti", "stars": 8, "license": "gpl-2.0", "language": "python", "size": 4840}
{"docstring": "\"\"\"Loads an external module by filename.\n\nUse this function with caution. Python will always write the compiled source\nto a file named \"${filename}c\".\n\n@type filename: string\n@param filename: Path to module\n\n\"\"\"\n", "func_signal": "def LoadModule(filename):\n", "code": "(name, ext) = os.path.splitext(filename)\n\nfh = open(filename, \"U\")\ntry:\n  return imp.load_module(name, fh, filename, (ext, \"U\", imp.PY_SOURCE))\nfinally:\n  fh.close()", "path": "lib\\build\\__init__.py", "repo_name": "sigmike/ganeti", "stars": 8, "license": "gpl-2.0", "language": "python", "size": 4840}
{"docstring": "\"\"\"Checks whether an instance is running.\n\nArgs:\n  node: Node the instance runs on\n  name: Full name of Xen instance\n\"\"\"\n", "func_signal": "def _InstanceRunning(node, name):\n", "code": "cmd = utils.ShellQuoteArgs(['xm', 'list', name]) + ' >/dev/null'\nret = StartSSH(node['primary'], cmd).wait()\nreturn ret == 0", "path": "qa\\qa_daemon.py", "repo_name": "sigmike/ganeti", "stars": 8, "license": "gpl-2.0", "language": "python", "size": 4840}
{"docstring": "\"\"\"Close the socket\"\"\"\n", "func_signal": "def Close(self):\n", "code": "if self.socket is not None:\n  self.socket.close()\n  self.socket = None", "path": "lib\\luxi.py", "repo_name": "sigmike/ganeti", "stars": 8, "license": "gpl-2.0", "language": "python", "size": 4840}
{"docstring": "\"\"\"Shuts down instance using \"xm\" and waits for completion.\n\nArgs:\n  node: Node the instance runs on\n  name: Full name of Xen instance\n\"\"\"\n", "func_signal": "def _XmShutdownInstance(node, name):\n", "code": "master = qa_config.GetMasterNode()\n\ncmd = ['xm', 'shutdown', name]\nAssertEqual(StartSSH(node['primary'],\n                     utils.ShellQuoteArgs(cmd)).wait(), 0)\n\n# Wait up to a minute\nend = time.time() + 60\nwhile time.time() <= end:\n  if not _InstanceRunning(node, name):\n    break\n  time.sleep(5)\nelse:\n  raise qa_error.Error(\"xm shutdown failed\")", "path": "qa\\qa_daemon.py", "repo_name": "sigmike/ganeti", "stars": 8, "license": "gpl-2.0", "language": "python", "size": 4840}
{"docstring": "\"\"\"Disconnect all receivers from a sender.\"\"\"\n", "func_signal": "def _cleanup_sender(self, sender_ref):\n", "code": "sender_id = sender_ref.sender_id\nassert sender_id != ANY_ID\nself._weak_senders.pop(sender_id, None)\nfor receiver_id in self._by_sender.pop(sender_id, ()):\n    self._by_receiver[receiver_id].discard(sender_id)", "path": "flatland\\util\\signals.py", "repo_name": "jek/flatland", "stars": 9, "license": "mit", "language": "python", "size": 1344}
{"docstring": "\"\"\"Emit this signal on behalf of *sender*, passing on \\*\\*kwargs.\n\nReturns a list of 2-tuples, pairing receivers with their return\nvalue. The ordering of receiver notification is undefined.\n\n\"\"\"\n", "func_signal": "def send(self, sender=None, **kwargs):\n", "code": "if not self.receivers:\n    return []\nelse:\n    return [(receiver, receiver(sender=sender, **kwargs))\n            for receiver in self.receivers_for(sender)]", "path": "flatland\\util\\signals.py", "repo_name": "jek/flatland", "stars": 9, "license": "mit", "language": "python", "size": 1344}
{"docstring": "\"\"\"Return the :class:`NamedSignal` *name*, creating it if required.\"\"\"\n", "func_signal": "def signal(self, name, doc=None):\n", "code": "lock = self.lock\nlock.acquire()\ntry:\n    return self.signals[name]\nexcept KeyError:\n    self.signals[name] = instance = self.signal_factory(name, doc)\n    return instance\nfinally:\n    lock.release()", "path": "flatland\\util\\signals.py", "repo_name": "jek/flatland", "stars": 9, "license": "mit", "language": "python", "size": 1344}
{"docstring": "\"\"\"Decorate __compound_init__ with a status setter & classmethod.\"\"\"\n", "func_signal": "def _wrap_compound_init(fn):\n", "code": "if isinstance(fn, classmethod):\n    fn = fn.__get__(str).im_func  # type doesn't matter here\ndef __compound_init__(cls):\n    res = fn(cls)\n    cls._compound_prepared = True\n    return res\nupdate_wrapper(__compound_init__, fn)\nreturn classmethod(__compound_init__)", "path": "flatland\\schema\\compound.py", "repo_name": "jek/flatland", "stars": 9, "license": "mit", "language": "python", "size": 1344}
{"docstring": "\"\"\"Throw away all signal state.  Useful for unit tests.\"\"\"\n", "func_signal": "def _clear_state(self):\n", "code": "self._weak_senders.clear()\nself.receivers.clear()\nself._by_sender.clear()\nself._by_receiver.clear()", "path": "flatland\\util\\signals.py", "repo_name": "jek/flatland", "stars": 9, "license": "mit", "language": "python", "size": 1344}
{"docstring": "# pruned won't insert empty elements for a skipped index or empty rhs\n", "func_signal": "def test_set_flat_pruned():\n", "code": "pairs = [(u'l_0_i', u'0'), (u'l_2_i', u''), (u'l_3_i', u'3')]\n\nschema = List.named(u'l').of(Integer.named(u'i'))\nel = schema.from_flat(pairs)\n\neq_(len(el), 2)\neq_(el.value, [0, 3])\n\nschema2 = schema.using(maximum_set_flat_members=1)\nel = schema2.from_flat(pairs)\n\neq_(len(el), 1)\neq_(el.value, [0])", "path": "tests\\schema\\test_lists.py", "repo_name": "jek/flatland", "stars": 9, "license": "mit", "language": "python", "size": 1344}
{"docstring": "\"\"\"Return an annotated weak ref.\"\"\"\n", "func_signal": "def reference(object, callback=None, **annotations):\n", "code": "if callable(object):\n    weak = callable_reference(object, callback)\nelse:\n    weak = annotatable_weakref(object, callback)\nfor key, value in annotations.items():\n    setattr(weak, key, value)\nreturn weak", "path": "flatland\\util\\weakrefs.py", "repo_name": "jek/flatland", "stars": 9, "license": "mit", "language": "python", "size": 1344}
{"docstring": "\"\"\"Run __compound_init__ on first instance construction.\"\"\"\n\n# Find **kw that would override existing class properties and\n# remove them from kw.\n", "func_signal": "def __call__(cls, value=Unspecified, **kw):\n", "code": "overrides = {}\nfor key in kw.keys():\n    if hasattr(cls, key):\n        overrides[key] = kw.pop(key)\n\nif overrides:\n    # If there are overrides, construct a subtype on the fly\n    # so that __compound_init__ has a chance to run again with\n    # this new configuration.\n    cls = cls.using(**overrides)\n    cls.__compound_init__()\nelif not cls.__dict__.get('_compound_prepared'):\n    # If this class hasn't been prepared yet, prepare it.\n    lock = cls._lock\n    lock.acquire()\n    try:\n        if not cls.__dict__.get('_compound_prepared'):\n            cls.__compound_init__()\n    finally:\n        lock.release()\n\n# Finally, implement type.__call__, invoking __init__ with the\n# members of kw that were not class overrides.\nself = cls.__new__(cls, value, **kw)\nif self.__class__ is cls:\n    if value is Unspecified:\n        self.__init__(**kw)\n    else:\n        self.__init__(value, **kw)\nreturn self", "path": "flatland\\schema\\compound.py", "repo_name": "jek/flatland", "stars": 9, "license": "mit", "language": "python", "size": 1344}
{"docstring": "\"\"\"Construct a validator.\n\n:param \\*\\*kw: override any extant class attribute on this instance.\n\n\"\"\"\n", "func_signal": "def __init__(self, **kw):\n", "code": "cls = type(self)\nfor attr, value in kw.iteritems():\n    if hasattr(cls, attr):\n        setattr(self, attr, value)\n    else:\n        raise TypeError(\"%s has no attribute %r, can not override.\" % (\n            cls.__name__, attr))", "path": "flatland\\validation\\base.py", "repo_name": "jek/flatland", "stars": 9, "license": "mit", "language": "python", "size": 1344}
{"docstring": "\"\"\"Record a validation warning message on an element.\n\n:param element:\n  An :class:`~flatland.schema.base.Element` instance.\n\n:param state:\n  an arbitrary object.  Supplied by :meth:`Element.validate\n  <flatland.schema.base.Element.validate>`.\n\n:param key: semi-optional, default None.\n  The name of a message-holding attribute on this instance.  Will be\n  used to ``message = getattr(self, key)``.\n\n:param message: semi-optional, default None.  A validation\n  message.  Use to provide a specific message rather than look\n  one up by *key*.\n\n:param \\*\\*info: optional.\n  Additional data to make available to validation message\n  string formatting.\n\n:returns: False\n\nEither *key* or *message* is required.  The message will have\nformatting expanded by :meth:`expand_message` and be appended to\n:attr:`element.warnings <flatland.schema.base.Element.warnings>`.\n\nAlways returns False.\n\"\"\"\n", "func_signal": "def note_warning(self, element, state, key=None, message=None, **info):\n", "code": "message = message or getattr(self, key)\nif message:\n    element.add_warning(\n        self.expand_message(element, state, message, **info))\nreturn False", "path": "flatland\\validation\\base.py", "repo_name": "jek/flatland", "stars": 9, "license": "mit", "language": "python", "size": 1344}
{"docstring": "\"\"\"Apply formatting to a validation message.\n\n:param element:\n  an :class:`~flatland.schema.base.Element` instance.\n\n:param state:\n  an arbitrary object.  Supplied by\n  :meth:`Element.validate <flatland.schema.base.Element.validate>`.\n\n:param message: a string, 3-tuple or callable.\n  If a 3-tuple, must be of the form ('single form', 'plural form',\n  n_key).\n\n  If callable, will be called with 2 positional arguments (*element*,\n  *state*) and must return a string or 3-tuple.\n\n:param \\*\\*extra_format_args: optional.\n  Additional data to make available to validation message\n  string formatting.\n\n:returns: the formatted string\n\nSee :ref:`Message Templating`, :ref:`Message Pluralization` and\n:ref:`Message Internationalization` for full information on how\nmessages are expanded.\n\n\"\"\"\n", "func_signal": "def expand_message(self, element, state, message, **extra_format_args):\n", "code": "if callable(message):\n    message = message(element, state)\n\nugettext = self.find_transformer('ugettext', element, state, message)\n\nformat_map = as_format_mapping(\n    extra_format_args, state, self, element,\n    transform=ugettext)\n\nif isinstance(message, tuple):\n    ungettext = self.find_transformer(\n        'ungettext', element, state, message)\n\n    single, plural, n_key = message\n    try:\n        n = format_map[n_key]\n        try:\n            n = int(n)\n        except TypeError:\n            pass\n    except KeyError:\n        n = n_key\n\n    if ungettext:\n        message = ungettext(single, plural, n)\n    else:\n        if ugettext:\n            single = ugettext(single)\n            plural = ugettext(plural)\n        message = single if n == 1 else plural\nelif ugettext:\n    message = ugettext(message)\n\nreturn message % format_map", "path": "flatland\\validation\\base.py", "repo_name": "jek/flatland", "stars": 9, "license": "mit", "language": "python", "size": 1344}
{"docstring": "\"\"\"Return an annotated weak ref, supporting bound instance methods.\"\"\"\n", "func_signal": "def callable_reference(object, callback=None):\n", "code": "if hasattr(object, 'im_self') and object.im_self is not None:\n    return _saferef.BoundMethodWeakref(target=object, on_delete=callback)\nreturn annotatable_weakref(object, callback)", "path": "flatland\\util\\weakrefs.py", "repo_name": "jek/flatland", "stars": 9, "license": "mit", "language": "python", "size": 1344}
{"docstring": "\"\"\"Connect *receiver* to signal events send by *sender*.\n\n:param receiver: A callable.  Will be invoked by :meth:`send`.\n  Will be invoked with `sender=` as a named argument and any\n  \\*\\*kwargs that were provided to a call to :meth:`send`.\n\n:param sender: Any object or :attr:`Signal.ANY`.  Restricts\n  notifications to *receiver* to only those :meth:`send`\n  emissions sent by *sender*.  If ``ANY``, the receiver will\n  always be notified.  A *receiver* may be connected to\n  multiple *sender* on the same Signal.  Defaults to ``ANY``.\n\n:param weak: If true, the Signal will hold a weakref to\n  *receiver* and automatically disconnect when *receiver* goes\n  out of scope or is garbage collected.  Defaults to True.\n\n\"\"\"\n", "func_signal": "def connect(self, receiver, sender=ANY, weak=True):\n", "code": "receiver_id = _hashable_identity(receiver)\nif weak:\n    receiver_ref = weakrefs.reference(receiver, self._cleanup_receiver)\n    receiver_ref.receiver_id = receiver_id\nelse:\n    receiver_ref = receiver\nsender_id = ANY_ID if sender is ANY else _hashable_identity(sender)\n\nself.receivers.setdefault(receiver_id, receiver_ref)\nself._by_sender[sender_id].add(receiver_id)\nself._by_receiver[receiver_id].add(sender_id)\ndel receiver_ref\n\nif sender is not ANY and sender_id not in self._weak_senders:\n    # wire together a cleanup for weakref-able senders\n    try:\n        sender_ref = weakrefs.reference(sender, self._cleanup_sender)\n        sender_ref.sender_id = sender_id\n    except TypeError:\n        pass\n    else:\n        self._weak_senders.setdefault(sender_id, sender_ref)\n        del sender_ref\n\n# broadcast this connection.  if receivers raise, disconnect.\nif receiver_connected.receivers and self is not receiver_connected:\n    try:\n        receiver_connected.send(self,\n                                receiver_arg=receiver,\n                                sender_arg=sender,\n                                weak_arg=weak)\n    except:\n        self.disconnect(receiver, sender)\n        raise\nreturn receiver", "path": "flatland\\util\\signals.py", "repo_name": "jek/flatland", "stars": 9, "license": "mit", "language": "python", "size": 1344}
{"docstring": "\"\"\"True if there is probably a receiver for *sender*.\n\nPerforms an optimistic check for receivers only.  Does not guarantee\nthat all weakly referenced receivers are still alive.  See\n:meth:`receivers_for` for a stronger search.\n\n\"\"\"\n", "func_signal": "def has_receivers_for(self, sender):\n", "code": "if not self.receivers:\n    return False\nif self._by_sender[ANY_ID]:\n    return True\nif sender is ANY:\n    return False\nreturn _hashable_identity(sender) in self._by_sender", "path": "flatland\\util\\signals.py", "repo_name": "jek/flatland", "stars": 9, "license": "mit", "language": "python", "size": 1344}
{"docstring": "\"\"\"Iterate all live receivers listening for *sender*.\"\"\"\n", "func_signal": "def receivers_for(self, sender):\n", "code": "if self.receivers:\n    sender_id = _hashable_identity(sender)\n    if sender_id in self._by_sender:\n        ids = (self._by_sender[ANY_ID] |\n               self._by_sender[sender_id])\n    else:\n        ids = self._by_sender[ANY_ID].copy()\n    for receiver_id in ids:\n        receiver = self.receivers.get(receiver_id)\n        if receiver is None:\n            continue\n        if isinstance(receiver, weakrefs.WeakTypes):\n            strong = receiver()\n            if strong is None:\n                self._disconnect(receiver_id, ANY_ID)\n                continue\n            receiver = strong\n        yield receiver", "path": "flatland\\util\\signals.py", "repo_name": "jek/flatland", "stars": 9, "license": "mit", "language": "python", "size": 1344}
{"docstring": "\"\"\"Locate a message-transforming function, such as ugettext.\n\nReturns None or a callable.  The callable must return a\nmessage.  The call signature of the callable is expected to\nmatch ``ugettext`` or ``ungettext``:\n\n- If *type* is 'ugettext', the callable should take a message\n  as a positional argument.\n\n- If *type* is 'ungettext', the callable should take three\n  positional arguments: a message for the singular form, a\n  message for the plural form, and an integer.\n\nSubclasses may override this method to provide advanced\nmessage transformation and translation functionality, on a\nper-element or per-message granularity if desired.\n\nThe default implementation uses the following logic to locate\na transformer:\n\n1.  If *state* has an attribute or item named *type*, return that.\n\n2.  If the *element* or any of its parents have an attribute\n    named *type*, return that.\n\n3.  If the schema of *element* or the schema of any of its\n    parents have an attribute named *type*, return that.\n\n4.  If *type* is in ``__builtin__``, return that.\n\n5.  Otherwise return ``None``.\n\n\"\"\"\n", "func_signal": "def find_transformer(self, type, element, state, message):\n", "code": "if hasattr(state, type):\n    return getattr(state, type)\nif hasattr(state, '__getitem__'):\n    try:\n        return state[type]\n    except KeyError:\n        pass\n\nif type == 'ugettext':\n    finder = _ugettext_finder\nelif type == 'ungettext':\n    finder = _ungettext_finder\nelse:\n    raise RuntimeError(\"Unknown transformation %r\" % type)\nreturn find_i18n_function(element, finder)", "path": "flatland\\validation\\base.py", "repo_name": "jek/flatland", "stars": 9, "license": "mit", "language": "python", "size": 1344}
{"docstring": "\"\"\"Disconnect *receiver* from this signal's events.\"\"\"\n", "func_signal": "def disconnect(self, receiver, sender=ANY):\n", "code": "sender_id = ANY_ID if sender is ANY else _hashable_identity(sender)\nreceiver_id = _hashable_identity(receiver)\nself._disconnect(receiver_id, sender_id)", "path": "flatland\\util\\signals.py", "repo_name": "jek/flatland", "stars": 9, "license": "mit", "language": "python", "size": 1344}
{"docstring": "\"\"\"Return a '<partial' opener tag with no terminator.\"\"\"\n", "func_signal": "def _open(self, bind, kwargs):\n", "code": "contents = kwargs.pop('contents', None)\nattributes = _unicode_keyed(kwargs)\ntagname = self.tagname\nnew_contents = transform(\n    tagname, attributes, contents, self._context, bind)\n\nif not new_contents:\n    new_contents = u''\nelif hasattr(new_contents, '__html__'):\n    new_contents = new_contents.__html__()\nself.contents = self._markup(new_contents)\n\nif self._context['ordered_attributes']:\n    pairs = sorted(attributes.items(), key=_attribute_sort_key)\nelse:\n    pairs = attributes.iteritems()\nguts = u' '.join(u'%s=\"%s\"' % (k, _attribute_escape(v))\n                 for k, v in pairs)\nif guts:\n    return u'<' + tagname + u' ' + guts\nelse:\n    return u'<' + tagname", "path": "flatland\\out\\markup.py", "repo_name": "jek/flatland", "stars": 9, "license": "mit", "language": "python", "size": 1344}
{"docstring": "\"\"\"Construct a MapEqual.\n\n:param \\*field_paths: a sequence of 2 or more elements names or paths.\n\n:param \\*\\*kw: passed to :meth:`Validator.__init__`.\n\n\"\"\"\n", "func_signal": "def __init__(self, *field_paths, **kw):\n", "code": "if not field_paths:\n    assert self.field_paths, 'at least 2 element paths required.'\nelse:\n    assert len(field_paths) > 1, 'at least 2 element paths required.'\n    self.field_paths = field_paths\nValidator.__init__(self, **kw)", "path": "flatland\\validation\\scalars.py", "repo_name": "jek/flatland", "stars": 9, "license": "mit", "language": "python", "size": 1344}
{"docstring": "\"\"\"Register the flatland directives with a template.\n\n:param template: a `Template` instance\n\"\"\"\n", "func_signal": "def setup(template, use_version=None):\n", "code": "if use_version is None:\n    use_version = 6 if hasattr(template, 'add_directives') else 5\n\nif use_version == 6:\n    from flatland.out.genshi_06 import setup\n    setup(template)\nelse:\n    install_element_mixin()\n    template.filters.append(flatland_filter)", "path": "flatland\\out\\genshi\\__init__.py", "repo_name": "jek/flatland", "stars": 9, "license": "mit", "language": "python", "size": 1344}
{"docstring": "\"\"\"Try to downgrade a unicode value to ISO-8859-1 for\ncompatibility with HTTP/1.0.\"\"\"\n\n", "func_signal": "def iso_8859_1(value):\n", "code": "try:\n    if isinstance(value, unicode):\n        return value.encode('iso-8859-1')\nexcept UnicodeEncodeError:\n    value = value.encode('utf-8')\nreturn value", "path": "sasl\\digest_md5.py", "repo_name": "thisismedium/python-sasl", "stars": 12, "license": "bsd-2-clause", "language": "python", "size": 644}
{"docstring": "\"\"\"Find the production for a value based on an item's name.\nIf no rule is found, try to fall back on a default rule.\"\"\"\n\n", "func_signal": "def production(self, name):\n", "code": "result = self.grammar.get(name) or self.default\nif not result:\n    raise ReadError('Unrecognized rule %r.' % name)\nreturn result", "path": "sasl\\rfc.py", "repo_name": "thisismedium/python-sasl", "stars": 12, "license": "bsd-2-clause", "language": "python", "size": 644}
{"docstring": "\"\"\"Test basic expectations about a challenge.\"\"\"\n\n", "func_signal": "def test_challenge(self):\n", "code": "state = self.mech.challenge()\nchallenge = dict(rfc.data(self.mech.CHALLENGE, state.data))\n\n## Nonce is random, so it can't be compared for equality.\nself.assert_(challenge.pop('nonce'))\n\nself.assertEqual(sorted(challenge.items()), [\n    ('algorithm', 'md5-sess'),\n    ('charset', 'utf-8'),\n    ('realm', 'test-service@example.net')\n])", "path": "sasl\\tests.py", "repo_name": "thisismedium/python-sasl", "stars": 12, "license": "bsd-2-clause", "language": "python", "size": 644}
{"docstring": "\"\"\"Look up a production by name.\"\"\"\n\n", "func_signal": "def production(name, *args, **kwargs):\n", "code": "result = name if is_production(name) else PRODUCTIONS.get(name)\nif isinstance(result, type) and issubclass(result, Parameterized):\n    return result(*args, **kwargs)\nreturn result", "path": "sasl\\rfc.py", "repo_name": "thisismedium/python-sasl", "stars": 12, "license": "bsd-2-clause", "language": "python", "size": 644}
{"docstring": "\"\"\"Register a SASL mechanism.\"\"\"\n\n", "func_signal": "def register(name, cls):\n", "code": "MECHANISMS[name] = cls\nreturn cls", "path": "sasl\\mechanism.py", "repo_name": "thisismedium/python-sasl", "stars": 12, "license": "bsd-2-clause", "language": "python", "size": 644}
{"docstring": "\"\"\"\n>>> int = maybe_unquote(integer)\n>>> int.read('123')\n(123, 3)\n>>> int.read(\"123\")\n(123, 3)\n>>> int.read(\"foo\")\n(None, 0)\n\"\"\"\n", "func_signal": "def maybe_unquote(prod):\n", "code": "prod = production(prod)\nreturn prod if getattr(prod, 'QUOTES', False) else MaybeUnquote(prod)", "path": "sasl\\rfc.py", "repo_name": "thisismedium/python-sasl", "stars": 12, "license": "bsd-2-clause", "language": "python", "size": 644}
{"docstring": "\"\"\"Final response hash (see RFC-2831 page 10).\"\"\"\n\n", "func_signal": "def response_hash(a1, a2, nonce, nc, cnonce, qop):\n", "code": "return colons(\n    md5, a1.hexdigest(),\n    nonce, '%08x' % nc,\n    cnonce, qop, a2.hexdigest()\n)", "path": "sasl\\digest_md5.py", "repo_name": "thisismedium/python-sasl", "stars": 12, "license": "bsd-2-clause", "language": "python", "size": 644}
{"docstring": "\"\"\"Test basic expectations about a response.\"\"\"\n\n", "func_signal": "def test_response(self):\n", "code": "state = self.mech.challenge()\nwith fluid.let((USER, 'user@example.net'), (PASS, 'secret')):\n    rstate = self.mech.respond(state.data)\n    resp = dict(rfc.data(self.mech.RESPOND, rstate.data))\n\nself.assertNotEqual(resp['nonce'], resp['cnonce'])\n\n## These are random, so pop them off before equality\n## comparison.\nself.assert_(resp.pop('nonce'))\nself.assert_(resp.pop('cnonce'))\nself.assert_(resp.pop('response'))\n\nself.assertEqual(sorted(resp.items()), [\n    (u'charset', u'utf-8'),\n    (u'digest-uri', u'test-service/example.net'),\n    (u'nc', 1),\n    (u'realm', u'test-service@example.net'),\n    (u'username', u'user@example.net')\n])", "path": "sasl\\tests.py", "repo_name": "thisismedium/python-sasl", "stars": 12, "license": "bsd-2-clause", "language": "python", "size": 644}
{"docstring": "\"\"\"Test that the nonce is random.\"\"\"\n\n", "func_signal": "def test_nonce(self):\n", "code": "self.assertNotEqual(\n    self.mech.make_nonce(),\n    self.mech.make_nonce()\n)", "path": "sasl\\tests.py", "repo_name": "thisismedium/python-sasl", "stars": 12, "license": "bsd-2-clause", "language": "python", "size": 644}
{"docstring": "\"\"\"Verify that the user's password matches and that this user\nis authorized to act on behalf of authorize.\"\"\"\n\n", "func_signal": "def verify_password(self, authorize, user, passwd):\n", "code": "probe = self.get_password(user)\nreturn (\n    probe is not None\n    and self._compare_passwords(user, passwd, probe)\n    and self._verify_authorization(user, authorize)\n)", "path": "sasl\\auth.py", "repo_name": "thisismedium/python-sasl", "stars": 12, "license": "bsd-2-clause", "language": "python", "size": 644}
{"docstring": "\"\"\"Create a hash over a sequence of values.  Colons are\ninterspersed between sequence elements.\"\"\"\n\n", "func_signal": "def colons(make, v1, *values):\n", "code": "result = make(v1)\nfor value in values:\n    if value:\n        result.update(':')\n        result.update(value)\nreturn result", "path": "sasl\\digest_md5.py", "repo_name": "thisismedium/python-sasl", "stars": 12, "license": "bsd-2-clause", "language": "python", "size": 644}
{"docstring": "\"\"\"\n>>> mechanism_name(\"FooBar\")\n'FOO-BAR'\n>>> mechanism_name('FOO-BAR')\n'FOO-BAR'\n\"\"\"\n", "func_signal": "def mechanism_name(obj):\n", "code": "name = getattr(obj, '__name__', None) or str(obj)\nreturn CAMEL.sub(r'\\1-\\2', name).strip('_').replace('_', '-').upper()", "path": "sasl\\mechanism.py", "repo_name": "thisismedium/python-sasl", "stars": 12, "license": "bsd-2-clause", "language": "python", "size": 644}
{"docstring": "\"\"\"Parse data using production and return the unboxed value from\nthe result of read().\"\"\"\n\n", "func_signal": "def data(prod, data):\n", "code": "(result, _) = prod.read(data)\nreturn result", "path": "sasl\\rfc.py", "repo_name": "thisismedium/python-sasl", "stars": 12, "license": "bsd-2-clause", "language": "python", "size": 644}
{"docstring": "\"\"\"Use a regular expression to drive a read() method.\"\"\"\n\n", "func_signal": "def regular(pattern):\n", "code": "PATTERN = re.compile(pattern)\ndef decorator(proc):\n\n    @classmethod\n    @functools.wraps(proc)\n    def internal(cls, data, pos=0):\n        probe = PATTERN.match(data, pos)\n        value = probe and proc(cls, probe)\n        if value is None:\n            return (value, pos)\n        return (value, pos + len(probe.group(0)))\n\n    return internal\nreturn decorator", "path": "sasl\\rfc.py", "repo_name": "thisismedium/python-sasl", "stars": 12, "license": "bsd-2-clause", "language": "python", "size": 644}
{"docstring": "\"\"\"User hash, specified as part of A1 in RFC-2831, but implemented\nindependently so passwords can be hashed and stored.\"\"\"\n\n", "func_signal": "def user_hash(user, realm, passwd, encoding='utf-8'):\n", "code": "if encoding == 'utf-8':\n    user = iso_8859_1(user)\n    realm = iso_8859_1(realm)\n    passwd = iso_8859_1(passwd)\n\nreturn colons(md5, user, realm, passwd)", "path": "sasl\\digest_md5.py", "repo_name": "thisismedium/python-sasl", "stars": 12, "license": "bsd-2-clause", "language": "python", "size": 644}
{"docstring": "\"\"\"Return True if obj is a Production.\"\"\"\n\n", "func_signal": "def is_production(obj):\n", "code": "return (\n    (isinstance(obj, type) and issubclass(obj, Production))\n    or isinstance(obj, Production)\n)", "path": "sasl\\rfc.py", "repo_name": "thisismedium/python-sasl", "stars": 12, "license": "bsd-2-clause", "language": "python", "size": 644}
{"docstring": "\"\"\"Normally this negotiation would take place over a network.\nThe `sdata' and `cdata' variables are the data that could be\nsent from the server and from the client.  The `sk' and `ck'\nvariables represent the \"continuation\" of the server and\nclient.\n\nA continuation is a procedure if the exchange should continue,\nFalse if authentication failed, or True if authentication\nsucceeded, or None if the decision is left up to the other end\nof the exchange.\"\"\"\n\n## Server issues a challenge.\n", "func_signal": "def negotiate(self, user, passwd):\n", "code": "sk = self.mech.challenge()\n\n## Client responds.\nwith fluid.let((USER, user), (PASS, passwd)):\n    ck = self.mech.respond(sk.data)\n\nwhile not (sk.finished() and ck.finished()):\n    if not sk.finished():\n        sk = sk(ck.data)\n\n    if not ck.finished():\n        ck = ck(sk.data)\n\nreturn (sk, ck)", "path": "sasl\\tests.py", "repo_name": "thisismedium/python-sasl", "stars": 12, "license": "bsd-2-clause", "language": "python", "size": 644}
{"docstring": "\"\"\"A class decorator that registers a SASL mechanism.\"\"\"\n\n", "func_signal": "def define(name=None):\n", "code": "def decorator(cls):\n    return register(mechanism_name(name or cls), cls)\n\nreturn decorator", "path": "sasl\\mechanism.py", "repo_name": "thisismedium/python-sasl", "stars": 12, "license": "bsd-2-clause", "language": "python", "size": 644}
{"docstring": "\"\"\"Require a production to succeed.  If it fails, raise a BadToken\nexception.\"\"\"\n\n", "func_signal": "def require(prod, data, pos):\n", "code": "result = prod.read(data, pos)\nif result[0] is None:\n    raise BadToken(prod, data, pos)\nreturn result", "path": "sasl\\rfc.py", "repo_name": "thisismedium/python-sasl", "stars": 12, "license": "bsd-2-clause", "language": "python", "size": 644}
{"docstring": "\"\"\"Create a hash over a sequence of values.\"\"\"\n\n", "func_signal": "def md5(v1, *values):\n", "code": "result = hashlib.md5(v1)\nfor value in values:\n    result.update(value)\nreturn result", "path": "sasl\\digest_md5.py", "repo_name": "thisismedium/python-sasl", "stars": 12, "license": "bsd-2-clause", "language": "python", "size": 644}
{"docstring": "\"\"\"Returns a palette that is a sequence of 3-tuples or 4-tuples,\nsynthesizing it from the ``PLTE`` and ``tRNS`` chunks.  These\nchunks should have already been processed (for example, by\ncalling the :meth:`preamble` method).  All the tuples are the\nsame size, 3-tuples if there is no ``tRNS`` chunk, 4-tuples when\nthere is a ``tRNS`` chunk.  Assumes that the image is colour type\n3 and therefore a ``PLTE`` chunk is required.\n\nIf the `alpha` argument is ``'force'`` then an alpha channel is\nalways added, forcing the result to be a sequence of 4-tuples.\n\"\"\"\n\n", "func_signal": "def palette(self, alpha='natural'):\n", "code": "if not self.plte:\n    raise Error(\n        \"required PLTE chunk is missing in colour type 3 image\")\nplte = group(array('B', self.plte), 3)\nif self.trns or alpha == 'force':\n    trns = array('B', self.trns or '')\n    trns.extend([255]*(len(plte)-len(trns)))\n    plte = map(operator.add, plte, group(trns, 1))\nreturn plte", "path": "png.py", "repo_name": "paulhammond/avcol", "stars": 8, "license": "None", "language": "python", "size": 281}
{"docstring": "\"\"\"Checks that a colour argument for transparent or\nbackground options is the right form.  Also \"corrects\" bare\nintegers to 1-tuples.\n\"\"\"\n\n", "func_signal": "def check_color(c, which):\n", "code": "if c is None:\n    return c\nif greyscale:\n    try:\n        l = len(c)\n    except TypeError:\n        c = (c,)\n    if len(c) != 1:\n        raise ValueError(\"%s for greyscale must be 1-tuple\" %\n            which)\n    if not isinteger(c[0]):\n        raise ValueError(\n            \"%s colour for greyscale must be integer\" %\n            which)\nelse:\n    if not (len(c) == 3 and\n            isinteger(c[0]) and\n            isinteger(c[1]) and\n            isinteger(c[2])):\n        raise ValueError(\n            \"%s colour must be a triple of integers\" %\n            which)\nreturn c", "path": "png.py", "repo_name": "paulhammond/avcol", "stars": 8, "license": "None", "language": "python", "size": 281}
{"docstring": "\"\"\"Liberally convert from hex string to binary string.\"\"\"\n", "func_signal": "def _dehex(s):\n", "code": "import re\n\n# Remove all non-hexadecimal digits\ns = re.sub(r'[^a-fA-F\\d]', '', s)\nreturn s.decode('hex')", "path": "png.py", "repo_name": "paulhammond/avcol", "stars": 8, "license": "None", "language": "python", "size": 281}
{"docstring": "\"\"\"\nRead a PNM header, returning (format,width,height,depth,maxval).\n`width` and `height` are in pixels.  `depth` is the number of\nchannels in the image; for PBM and PGM it is synthesized as 1, for\nPPM as 3; for PAM images it is read from the header.  `maxval` is\nsynthesized (as 1) for PBM images.\n\"\"\"\n\n# Generally, see http://netpbm.sourceforge.net/doc/ppm.html\n# and http://netpbm.sourceforge.net/doc/pam.html\n\n# Technically 'P7' must be followed by a newline, so by using\n# rstrip() we are being liberal in what we accept.  I think this\n# is acceptable.\n", "func_signal": "def read_pnm_header(infile, supported=('P5','P6')):\n", "code": "type = infile.read(3).rstrip()\nif type not in supported:\n    raise NotImplementedError('file format %s not supported' % type)\nif type == 'P7':\n    # PAM header parsing is completely different.\n    return read_pam_header(infile)\n# Expected number of tokens in header (3 for P4, 4 for P6)\nexpected = 4\npbm = ('P1', 'P4')\nif type in pbm:\n    expected = 3\nheader = [type]\n\n# We have to read the rest of the header byte by byte because the\n# final whitespace character (immediately following the MAXVAL in\n# the case of P6) may not be a newline.  Of course all PNM files in\n# the wild use a newline at this point, so it's tempting to use\n# readline; but it would be wrong.\ndef getc():\n    c = infile.read(1)\n    if c == '':\n        raise Error('premature EOF reading PNM header')\n    return c\n\nc = getc()\nwhile True:\n    # Skip whitespace that precedes a token.\n    while c.isspace():\n        c = getc()\n    # Skip comments.\n    while c == '#':\n        while c not in '\\n\\r':\n            c = getc()\n    if not c.isdigit():\n        raise Error('unexpected character %s found in header' % c)\n    # According to the specification it is legal to have comments\n    # that appear in the middle of a token.\n    # This is bonkers; I've never seen it; and it's a bit awkward to\n    # code good lexers in Python (no goto).  So we break on such\n    # cases.\n    token = ''\n    while c.isdigit():\n        token += c\n        c = getc()\n    # Slight hack.  All \"tokens\" are decimal integers, so convert\n    # them here.\n    header.append(int(token))\n    if len(header) == expected:\n        break\n# Skip comments (again)\nwhile c == '#':\n    while c not in '\\n\\r':\n        c = getc()\nif not c.isspace():\n    raise Error('expected header to end with whitespace, not %s' % c)\n\nif type in pbm:\n    # synthesize a MAXVAL\n    header.append(1)\ndepth = (1,3)[type == 'P6']\nreturn header[0], header[1], header[2], depth, header[3]", "path": "png.py", "repo_name": "paulhammond/avcol", "stars": 8, "license": "None", "language": "python", "size": 281}
{"docstring": "\"\"\"Helper used by :meth:`asRGB8` and :meth:`asRGBA8`.\"\"\"\n\n", "func_signal": "def _as_rescale(self, get, targetbitdepth):\n", "code": "width,height,pixels,meta = get()\nmaxval = 2**meta['bitdepth'] - 1\ntargetmaxval = 2**targetbitdepth - 1\nfactor = float(targetmaxval) / float(maxval)\nmeta['bitdepth'] = targetbitdepth\ndef iterscale():\n    for row in pixels:\n        yield map(lambda x: int(round(x*factor)), row)\nreturn width, height, iterscale(), meta", "path": "png.py", "repo_name": "paulhammond/avcol", "stars": 8, "license": "None", "language": "python", "size": 281}
{"docstring": "\"\"\"numpy uint16.\"\"\"\n\n", "func_signal": "def testNumpyuint16(self):\n", "code": "try:\n    import numpy\nexcept ImportError:\n    print >>sys.stderr, \"skipping numpy test\"\n    return\n\nrows = [map(numpy.uint16, range(0,0x10000,0x5555))]\nb = topngbytes('numpyuint16.png', rows, 4, 1,\n    greyscale=True, alpha=False, bitdepth=16)", "path": "png.py", "repo_name": "paulhammond/avcol", "stars": 8, "license": "None", "language": "python", "size": 281}
{"docstring": "\"\"\"Write a PNM file.\"\"\"\n", "func_signal": "def write_pnm(file, width, height, pixels, meta):\n", "code": "bitdepth = meta['bitdepth']\nmaxval = 2**bitdepth - 1\n# Rudely, the number of image planes can be used to determine\n# whether we are L (PGM), LA (PAM), RGB (PPM), or RGBA (PAM).\nplanes = meta['planes']\n# Can be an assert as long as we assumes that pixels and meta came\n# from a PNG file.\nassert planes in (1,2,3,4)\nif planes in (1,3):\n    if 1 == planes:\n        # PGM\n        # Could generate PBM if maxval is 1, but we don't (for one\n        # thing, we'd have to convert the data, not just blat it\n        # out).\n        fmt = 'P5'\n    else:\n        # PPM\n        fmt = 'P6'\n    file.write('%s %d %d %d\\n' % (fmt, width, height, maxval))\nif planes in (2,4):\n    # PAM\n    # See http://netpbm.sourceforge.net/doc/pam.html\n    if 2 == planes:\n        tupltype = 'GRAYSCALE_ALPHA'\n    else:\n        tupltype = 'RGB_ALPHA'\n    file.write('P7\\nWIDTH %d\\nHEIGHT %d\\nDEPTH %d\\nMAXVAL %d\\n'\n               'TUPLTYPE %s\\nENDHDR\\n' %\n               (width, height, planes, maxval, tupltype))\n# Values per row\nvpr = planes * width\n# struct format\nfmt = '>%d' % vpr\nif maxval > 0xff:\n    fmt = fmt + 'H'\nelse:\n    fmt = fmt + 'B'\nfor row in pixels:\n    file.write(struct.pack(fmt, *row))\nfile.flush()", "path": "png.py", "repo_name": "paulhammond/avcol", "stars": 8, "license": "None", "language": "python", "size": 281}
{"docstring": "\"\"\"Create an LA image with bitdepth 4.\"\"\"\n", "func_signal": "def testLA4(self):\n", "code": "bytes = topngbytes('la4.png', [[5, 12]], 1, 1,\n  greyscale=True, alpha=True, bitdepth=4)\nsbit = Reader(bytes=bytes).chunk('sBIT')[1]\nself.assertEqual(sbit, '\\x04\\x04')", "path": "png.py", "repo_name": "paulhammond/avcol", "stars": 8, "license": "None", "language": "python", "size": 281}
{"docstring": "\"\"\"Create a PNG file by writing out the chunks.\"\"\"\n\n", "func_signal": "def write_chunks(out, chunks):\n", "code": "out.write(_signature)\nfor chunk in chunks:\n    write_chunk(out, *chunk)", "path": "png.py", "repo_name": "paulhammond/avcol", "stars": 8, "license": "None", "language": "python", "size": 281}
{"docstring": "\"\"\"Same as ``isinstance(x, array)`` except on Python 2.2, where it\nalways returns ``False``.  This helps PyPNG work on Python 2.2.\n\"\"\"\n\n", "func_signal": "def isarray(x):\n", "code": "try:\n    return isinstance(x, array)\nexcept:\n    return False", "path": "png.py", "repo_name": "paulhammond/avcol", "stars": 8, "license": "None", "language": "python", "size": 281}
{"docstring": "\"\"\"\nGenerates boxed rows (flat pixels) from flat rows (flat pixels)\nin an array.\n\"\"\"\n\n# Values per row\n", "func_signal": "def array_scanlines(self, pixels):\n", "code": "vpr = self.width * self.planes\nstop = 0\nfor y in range(self.height):\n    start = stop\n    stop = start + vpr\n    yield pixels[start:stop]", "path": "png.py", "repo_name": "paulhammond/avcol", "stars": 8, "license": "None", "language": "python", "size": 281}
{"docstring": "\"\"\"numpy bool.\"\"\"\n\n", "func_signal": "def testNumpybool(self):\n", "code": "try:\n    import numpy\nexcept ImportError:\n    print >>sys.stderr, \"skipping numpy test\"\n    return\n\nrows = [map(numpy.bool, [0,1])]\nb = topngbytes('numpybool.png', rows, 2, 1,\n    greyscale=True, alpha=False, bitdepth=1)", "path": "png.py", "repo_name": "paulhammond/avcol", "stars": 8, "license": "None", "language": "python", "size": 281}
{"docstring": "\"\"\"\nCreate a PNG decoder object.\n\nThe constructor expects exactly one keyword argument. If you\nsupply a positional argument instead, it will guess the input\ntype. You can choose among the following keyword arguments:\n\nfilename\n  Name of input file (a PNG file).\nfile\n  A file-like object (object with a read() method).\nbytes\n  ``array`` or ``string`` with PNG data.\n\n\"\"\"\n", "func_signal": "def __init__(self, _guess=None, **kw):\n", "code": "if ((_guess is not None and len(kw) != 0) or\n    (_guess is None and len(kw) != 1)):\n    raise TypeError(\"Reader() takes exactly 1 argument\")\n\n# Will be the first 8 bytes, later on.  See validate_signature.\nself.signature = None\nself.transparent = None\n# A pair of (len,type) if a chunk has been read but its data and\n# checksum have not (in other words the file position is just\n# past the 4 bytes that specify the chunk type).  See preamble\n# method for how this is used.\nself.atchunk = None\n\nif _guess is not None:\n    if isarray(_guess):\n        kw[\"bytes\"] = _guess\n    elif isinstance(_guess, str):\n        kw[\"filename\"] = _guess\n    elif isinstance(_guess, file):\n        kw[\"file\"] = _guess\n\nif \"filename\" in kw:\n    self.file = file(kw[\"filename\"], \"rb\")\nelif \"file\" in kw:\n    self.file = kw[\"file\"]\nelif \"bytes\" in kw:\n    self.file = _readable(kw[\"bytes\"])\nelse:\n    raise TypeError(\"expecting filename, file or bytes array\")", "path": "png.py", "repo_name": "paulhammond/avcol", "stars": 8, "license": "None", "language": "python", "size": 281}
{"docstring": "\"\"\"Returns the image data as a direct representation of an\n``x * y * planes`` array.  This method is intended to remove the\nneed for callers to deal with palettes and transparency\nthemselves.  Images with a palette (colour type 3)\nare converted to RGB or RGBA; images with transparency (a\n``tRNS`` chunk) are converted to LA or RGBA as appropriate.\nWhen returned in this format the pixel values represent the\ncolour value directly without needing to refer to palettes or\ntransparency information.\n\nLike the :meth:`read` method this method returns a 4-tuple:\n\n(*width*, *height*, *pixels*, *meta*)\n\nThis method normally returns pixel values with the bit depth\nthey have in the source image, but when the source PNG has an\n``sBIT`` chunk it is inspected and can reduce the bit depth of\nthe result pixels; pixel values will be reduced according to\nthe bit depth specified in the ``sBIT`` chunk (PNG nerds should\nnote a single result bit depth is used for all channels; the\nmaximum of the ones specified in the ``sBIT`` chunk.  An RGB565\nimage will be rescaled to 6-bit RGB666).\n\nThe *meta* dictionary that is returned reflects the `direct`\nformat and not the original source image.  For example, an RGB\nsource image with a ``tRNS`` chunk to represent a transparent\ncolour, will have ``planes=3`` and ``alpha=False`` for the\nsource image, but the *meta* dictionary returned by this method\nwill have ``planes=4`` and ``alpha=True`` because an alpha\nchannel is synthesized and added.\n\n*pixels* is the pixel data in boxed row flat pixel format (just\nlike the :meth:`read` method).\n\nAll the other aspects of the image data are not changed.\n\"\"\"\n\n", "func_signal": "def asDirect(self):\n", "code": "self.preamble()\n\n# Simple case, no conversion necessary.\nif not self.colormap and not self.trns and not self.sbit:\n    return self.read()\n\nx,y,pixels,meta = self.read()\n\nif self.colormap:\n    meta['colormap'] = False\n    meta['alpha'] = bool(self.trns)\n    meta['bitdepth'] = 8\n    meta['planes'] = 3 + bool(self.trns)\n    plte = self.palette()\n    def iterpal(pixels):\n        for row in pixels:\n            row = map(plte.__getitem__, row)\n            yield array('B', itertools.chain(*row))\n    pixels = iterpal(pixels)\nelif self.trns:\n    # It would be nice if there was some reasonable way of doing\n    # this without generating a whole load of intermediate tuples.\n    # But tuples does seem like the easiest way, with no other way\n    # clearly much simpler or much faster.  (Actually, the L to LA\n    # conversion could perhaps go faster (all those 1-tuples!), but\n    # I still wonder whether the code proliferation is worth it)\n    it = self.transparent\n    maxval = 2**meta['bitdepth']-1\n    planes = meta['planes']\n    meta['alpha'] = True\n    meta['planes'] += 1\n    def itertrns(pixels):\n        for row in pixels:\n            # For each row we group it into pixels, then form a\n            # characterisation vector that says whether each pixel\n            # is opaque or not.  Then we convert True/False to\n            # 0/maxval (by multiplication), and add it as the extra\n            # channel.\n            row = group(row, planes)\n            opa = map(it.__ne__, row)\n            opa = map(maxval.__mul__, opa)\n            opa = zip(opa) # convert to 1-tuples\n            yield itertools.chain(*map(operator.add, row, opa))\n    pixels = itertrns(pixels)\ntargetbitdepth = None\nif self.sbit:\n    sbit = struct.unpack('%dB' % len(self.sbit), self.sbit)\n    targetbitdepth = max(sbit)\n    if targetbitdepth > meta['bitdepth']:\n        raise Error('sBIT chunk %r exceeds bitdepth %d' %\n            (sbit,self.bitdepth))\n    if min(sbit) <= 0:\n        raise Error('sBIT chunk %r has a 0-entry' % sbit)\n    if targetbitdepth == meta['bitdepth']:\n        targetbitdepth = None\nif targetbitdepth:\n    shift = meta['bitdepth'] - targetbitdepth\n    meta['bitdepth'] = targetbitdepth\n    def itershift(pixels):\n        for row in pixels:\n            yield map(shift.__rrshift__, row)\n    pixels = itershift(pixels)\nreturn x,y,pixels,meta", "path": "png.py", "repo_name": "paulhammond/avcol", "stars": 8, "license": "None", "language": "python", "size": 281}
{"docstring": "\"\"\"Create the byte sequences for a ``PLTE`` and if necessary a\n``tRNS`` chunk.  Returned as a pair (*p*, *t*).  *t* will be\n``None`` if no ``tRNS`` chunk is necessary.\n\"\"\"\n\n", "func_signal": "def make_palette(self):\n", "code": "p = array('B')\nt = array('B')\n\nfor x in self.palette:\n    p.extend(x[0:3])\n    if len(x) > 3:\n        t.append(x[3])\np = tostring(p)\nt = tostring(t)\nif t:\n    return p,t\nreturn p,None", "path": "png.py", "repo_name": "paulhammond/avcol", "stars": 8, "license": "None", "language": "python", "size": 281}
{"docstring": "\"\"\"\nCreate a PNG test image and write the file to stdout.\n\"\"\"\n\n# Below is a big stack of test image generators.\n# They're all really tiny, so PEP 8 rules are suspended.\n\n", "func_signal": "def test_suite(options, args):\n", "code": "def test_gradient_horizontal_lr(x, y): return x\ndef test_gradient_horizontal_rl(x, y): return 1-x\ndef test_gradient_vertical_tb(x, y): return y\ndef test_gradient_vertical_bt(x, y): return 1-y\ndef test_radial_tl(x, y): return max(1-math.sqrt(x*x+y*y), 0.0)\ndef test_radial_center(x, y): return test_radial_tl(x-0.5, y-0.5)\ndef test_radial_tr(x, y): return test_radial_tl(1-x, y)\ndef test_radial_bl(x, y): return test_radial_tl(x, 1-y)\ndef test_radial_br(x, y): return test_radial_tl(1-x, 1-y)\ndef test_stripe(x, n): return float(int(x*n) & 1)\ndef test_stripe_h_2(x, y): return test_stripe(x, 2)\ndef test_stripe_h_4(x, y): return test_stripe(x, 4)\ndef test_stripe_h_10(x, y): return test_stripe(x, 10)\ndef test_stripe_v_2(x, y): return test_stripe(y, 2)\ndef test_stripe_v_4(x, y): return test_stripe(y, 4)\ndef test_stripe_v_10(x, y): return test_stripe(y, 10)\ndef test_stripe_lr_10(x, y): return test_stripe(x+y, 10)\ndef test_stripe_rl_10(x, y): return test_stripe(1+x-y, 10)\ndef test_checker(x, y, n): return float((int(x*n) & 1) ^ (int(y*n) & 1))\ndef test_checker_8(x, y): return test_checker(x, y, 8)\ndef test_checker_15(x, y): return test_checker(x, y, 15)\ndef test_zero(x, y): return 0\ndef test_one(x, y): return 1\n\ntest_patterns = {\n    'GLR': test_gradient_horizontal_lr,\n    'GRL': test_gradient_horizontal_rl,\n    'GTB': test_gradient_vertical_tb,\n    'GBT': test_gradient_vertical_bt,\n    'RTL': test_radial_tl,\n    'RTR': test_radial_tr,\n    'RBL': test_radial_bl,\n    'RBR': test_radial_br,\n    'RCTR': test_radial_center,\n    'HS2': test_stripe_h_2,\n    'HS4': test_stripe_h_4,\n    'HS10': test_stripe_h_10,\n    'VS2': test_stripe_v_2,\n    'VS4': test_stripe_v_4,\n    'VS10': test_stripe_v_10,\n    'LRS': test_stripe_lr_10,\n    'RLS': test_stripe_rl_10,\n    'CK8': test_checker_8,\n    'CK15': test_checker_15,\n    'ZERO': test_zero,\n    'ONE': test_one,\n    }\n\ndef test_pattern(width, height, bitdepth, pattern):\n    \"\"\"Create a single plane (monochrome) test pattern.  Returns a\n    flat row flat pixel array.\n    \"\"\"\n\n    maxval = 2**bitdepth-1\n    if maxval > 255:\n        a = array('H')\n    else:\n        a = array('B')\n    fw = float(width)\n    fh = float(height)\n    pfun = test_patterns[pattern]\n    for y in range(height):\n        fy = float(y)/fh\n        for x in range(width):\n            a.append(int(round(pfun(float(x)/fw, fy) * maxval)))\n    return a\n\ndef test_rgba(size=256, bitdepth=8,\n                red=\"GTB\", green=\"GLR\", blue=\"RTL\", alpha=None):\n    \"\"\"\n    Create a test image.  Each channel is generated from the\n    specified pattern; any channel apart from red can be set to\n    None, which will cause it not to be in the image.  It\n    is possible to create all PNG channel types (L, RGB, LA, RGBA),\n    as well as non PNG channel types (RGA, and so on).\n    \"\"\"\n\n    i = test_pattern(size, size, bitdepth, red)\n    psize = 1\n    for channel in (green, blue, alpha):\n        if channel:\n            c = test_pattern(size, size, bitdepth, channel)\n            i = interleave_planes(i, c, psize, 1)\n            psize += 1\n    return i\n\ndef pngsuite_image(name):\n    \"\"\"\n    Create a test image by reading an internal copy of the files\n    from the PngSuite.  Returned in flat row flat pixel format.\n    \"\"\"\n\n    if name not in _pngsuite:\n        raise NotImplementedError(\"cannot find PngSuite file %s (use -L for a list)\" % name)\n    r = Reader(bytes=_pngsuite[name])\n    w,h,pixels,meta = r.asDirect()\n    assert w == h\n    # LAn for n < 8 is a special case for which we need to rescale\n    # the data.\n    if meta['greyscale'] and meta['alpha'] and meta['bitdepth'] < 8:\n        factor = 255 // (2**meta['bitdepth']-1)\n        def rescale(data):\n            for row in data:\n                yield map(factor.__mul__, row)\n        pixels = rescale(pixels)\n        meta['bitdepth'] = 8\n    arraycode = 'BH'[meta['bitdepth']>8]\n    return w, array(arraycode, itertools.chain(*pixels)), meta\n\n# The body of test_suite()\nsize = 256\nif options.test_size:\n    size = options.test_size\noptions.bitdepth = 8\nif options.test_deep:\n    options.bitdepth = 16\noptions.greyscale=bool(options.test_black)\n\nkwargs = {}\nif options.test_red:\n    kwargs[\"red\"] = options.test_red\nif options.test_green:\n    kwargs[\"green\"] = options.test_green\nif options.test_blue:\n    kwargs[\"blue\"] = options.test_blue\nif options.test_alpha:\n    kwargs[\"alpha\"] = options.test_alpha\nif options.greyscale:\n    if options.test_red or options.test_green or options.test_blue:\n        raise ValueError(\"cannot specify colours (R, G, B) when greyscale image (black channel, K) is specified\")\n    kwargs[\"red\"] = options.test_black\n    kwargs[\"green\"] = None\n    kwargs[\"blue\"] = None\noptions.alpha = bool(options.test_alpha)\nif not args:\n    pixels = test_rgba(size, options.bitdepth, **kwargs)\nelse:\n    size,pixels,meta = pngsuite_image(args[0])\n    for k in ['bitdepth', 'alpha', 'greyscale']:\n        setattr(options, k, meta[k])\n\nwriter = Writer(size, size,\n                bitdepth=options.bitdepth,\n                transparent=options.transparent,\n                background=options.background,\n                gamma=options.gamma,\n                greyscale=options.greyscale,\n                alpha=options.alpha,\n                compression=options.compression,\n                interlace=options.interlace)\nwriter.write_array(sys.stdout, pixels)", "path": "png.py", "repo_name": "paulhammond/avcol", "stars": 8, "license": "None", "language": "python", "size": 281}
{"docstring": "\"\"\"Iterator that yields each scanline in boxed row flat pixel\nformat.  `rows` should be an iterator that yields the bytes of\neach row in turn.\n\"\"\"\n\n", "func_signal": "def iterboxed(self, rows):\n", "code": "def asvalues(raw):\n    \"\"\"Convert a row of raw bytes into a flat row.  Result may\n    or may not share with argument\"\"\"\n\n    if self.bitdepth == 8:\n        return raw\n    if self.bitdepth == 16:\n        raw = tostring(raw)\n        return array('H', struct.unpack('!%dH' % (len(raw)//2), raw))\n    assert self.bitdepth < 8\n    width = self.width\n    # Samples per byte\n    spb = 8//self.bitdepth\n    out = array('B')\n    mask = 2**self.bitdepth - 1\n    shifts = map(self.bitdepth.__mul__, reversed(range(spb)))\n    for o in raw:\n        out.extend(map(lambda i: mask&(o>>i), shifts))\n    return out[:width]\n\nreturn itertools.imap(asvalues, rows)", "path": "png.py", "repo_name": "paulhammond/avcol", "stars": 8, "license": "None", "language": "python", "size": 281}
{"docstring": "\"\"\"\nExtract the image metadata by reading the initial part of the PNG\nfile up to the start of the ``IDAT`` chunk.  All the chunks that\nprecede the ``IDAT`` chunk are read and either processed for\nmetadata or discarded.\n\"\"\"\n\n", "func_signal": "def preamble(self):\n", "code": "self.validate_signature()\n\nwhile True:\n    if not self.atchunk:\n        self.atchunk = self.chunklentype()\n    if self.atchunk[1] == 'IDAT':\n        return\n    self.process_chunk()", "path": "png.py", "repo_name": "paulhammond/avcol", "stars": 8, "license": "None", "language": "python", "size": 281}
{"docstring": "\"\"\"\nConvert a PNM file containing raw pixel data into a PNG file\nwith the parameters set in the writer object.  Works for\n(binary) PGM, PPM, and PAM formats.\n\"\"\"\n\n", "func_signal": "def convert_pnm(self, infile, outfile):\n", "code": "if self.interlace:\n    pixels = array('B')\n    pixels.fromfile(infile,\n                    (self.bitdepth/8) * self.color_planes *\n                    self.width * self.height)\n    self.write_passes(outfile, self.array_scanlines_interlace(pixels))\nelse:\n    self.write_passes(outfile, self.file_scanlines(infile))", "path": "png.py", "repo_name": "paulhammond/avcol", "stars": 8, "license": "None", "language": "python", "size": 281}
{"docstring": "\"\"\"Test that the command line tool can read PGM files.\"\"\"\n", "func_signal": "def testPGMin(self):\n", "code": "def do():\n    return _main(['testPGMin'])\ns = StringIO()\ns.write('P5 2 2 3\\n')\ns.write('\\x00\\x01\\x02\\x03')\ns.flush()\ns.seek(0)\no = StringIO()\ntestWithIO(s, o, do)\nr = Reader(bytes=o.getvalue())\nx,y,pixels,meta = r.read()\nself.assert_(r.greyscale)\nself.assertEqual(r.bitdepth, 2)", "path": "png.py", "repo_name": "paulhammond/avcol", "stars": 8, "license": "None", "language": "python", "size": 281}
{"docstring": "#width = self.clientRect.width\n", "func_signal": "def OnSize(self, event):\n", "code": "ShowScrollBar(self.handle, SB_HORZ, False)\nself.SetRedraw(0)\nfor i in range(len(columnDefs) - 1):\n    #self.SetColumnWidth(i, width / len(columnDefs))\n    self.SetColumnWidth(i, -2)\nself.SetColumnWidth(len(columnDefs) - 1, -2)\nself.SetRedraw(1)\nevent.handled = False", "path": "test\\test_list.py", "repo_name": "toymachine/venster", "stars": 10, "license": "mit", "language": "python", "size": 485}
{"docstring": "#print \"lpaint\", self.handle\n", "func_signal": "def OnPaint(self, event):\n", "code": "width = self.clientRect.width\nfor i in range(len(columnDefs) - 1):\n    #self.SetColumnWidth(i, width / len(columnDefs))\n    self.SetColumnWidth(i, -2)\nself.SetColumnWidth(len(columnDefs) - 1, -2)\n\nevent.handled = False", "path": "test\\test_list.py", "repo_name": "toymachine/venster", "stars": 10, "license": "mit", "language": "python", "size": 485}
{"docstring": "#current tab changing\n", "func_signal": "def OnSelChanging(self, event):\n", "code": "child = self.GetChildAt(self.GetCurSel())\nif child:\n    child.ShowWindow(SW_HIDE)", "path": "venster\\lib\\notebook.py", "repo_name": "toymachine/venster", "stars": 10, "license": "mit", "language": "python", "size": 485}
{"docstring": "\"\"\"returns a list of this windows child windows\"\"\"\n", "func_signal": "def EnumChildWindows(self):\n", "code": "childWindows = []\ndef enumChildProc(hWnd, lParam):\n    childWindows.append(Window(hWnd = hWnd))\nEnumChildWindows(self.handle, EnumChildProc(enumChildProc), 0)\nreturn childWindows", "path": "venster\\wtl.py", "repo_name": "toymachine/venster", "stars": 10, "license": "mit", "language": "python", "size": 485}
{"docstring": "\"\"\"gets the position of the mouse cursor in screen coords\"\"\"\n", "func_signal": "def GetCursorPos():\n", "code": "pt = POINT()\nGetCursorPos(byref(pt))\nreturn pt.x, pt.y", "path": "venster\\wtl.py", "repo_name": "toymachine/venster", "stars": 10, "license": "mit", "language": "python", "size": 485}
{"docstring": "\"\"\"inserts columns into list view, based on colDef\ncolDef is a list of tuples (title, width)\"\"\"\n", "func_signal": "def InsertColumns(self, colDefs):\n", "code": "col = LVCOLUMN()\ni = 0\nfor colDef in colDefs:\n    title, width = colDef[:2]\n    col.clear()\n    col.text = title\n    col.width = width\n    if len(colDef) == 3:\n        fmt = colDef[2]\n        col.format = fmt\n    self.InsertColumn(i, col)\n    i += 1", "path": "venster\\lib\\list.py", "repo_name": "toymachine/venster", "stars": 10, "license": "mit", "language": "python", "size": 485}
{"docstring": "#print \"qcd\", fEscapePressed, grfKeyState\n", "func_signal": "def QueryContinueDrag(self, this, fEscapePressed, grfKeyState):\n", "code": "if fEscapePressed:\n    return DRAGDROP_S_CANCEL\nelif not (grfKeyState & MK_LBUTTON):\n    return DRAGDROP_S_DROP\nelse:\n    return S_OK", "path": "test\\test_dragdrop.py", "repo_name": "toymachine/venster", "stars": 10, "license": "mit", "language": "python", "size": 485}
{"docstring": "#evaluates the line as a python expression and show the results\n", "func_signal": "def OnEval(self, event):\n", "code": "try:\n    evalResult = str(eval(self.ctrlEditEval.GetText()))\nexcept:\n    #dump the stacktrace in a string for display\n    import traceback\n    import StringIO\n    tmp = StringIO.StringIO()\n    traceback.print_exc(file = tmp)\n    #multiline edit box wants '\\r\\n' as linebreak:\n    evalResult = tmp.getvalue().replace('\\n', '\\r\\n') \n\nself.ctrlEditEvalRes.SetText(evalResult)", "path": "test\\test_dialog2.py", "repo_name": "toymachine/venster", "stars": 10, "license": "mit", "language": "python", "size": 485}
{"docstring": "\"\"\"sets the current tab to index\"\"\"\n", "func_signal": "def SetCurrentTab(self, index):\n", "code": "if index == self.GetCurSel(): return\nself.OnSelChanging(None) #simulate\nself.SetCurSel(index) #does not cause sel changing and sel change events\nself.OnSelChange(None) #simulate", "path": "venster\\lib\\notebook.py", "repo_name": "toymachine/venster", "stars": 10, "license": "mit", "language": "python", "size": 485}
{"docstring": "#print \"del item\"\n", "func_signal": "def OnDeleteItem(self, event):\n", "code": "nmtv = event.structure(comctl.NMTREEVIEW)\ni = nmtv.itemOld.lParam\nif i != 0:\n    ti = from_pointer(i)\n    #print ti", "path": "test\\test_tree.py", "repo_name": "toymachine/venster", "stars": 10, "license": "mit", "language": "python", "size": 485}
{"docstring": "#make sure splitter window is at bottom\n", "func_signal": "def Add(self, index, ctrl):\n", "code": "self.SetWindowPos(HWND_BOTTOM, 0, 0, 0, 0,\n                  SWP_NOACTIVATE|SWP_NOMOVE|SWP_NOREDRAW|SWP_NOSIZE)\nself.m_views[index] = ctrl", "path": "venster\\lib\\splitter.py", "repo_name": "toymachine/venster", "stars": 10, "license": "mit", "language": "python", "size": 485}
{"docstring": "\"\"\"returns a list of rectangles representing the little\ndrag rectangles at the corners of a selected item\"\"\"\n", "func_signal": "def GetDragRects(self, rc):\n", "code": "return [RECT(rc.left - 2, rc.top - 2, rc.left + 4, rc.top + 4),\n        RECT(rc.right - 3, rc.top - 2, rc.right + 3, rc.top + 4),\n        RECT(rc.right - 3, rc.bottom -2, rc.right +3, rc.bottom + 4),\n        RECT(rc.left -2, rc.bottom - 2, rc.left + 4, rc.bottom + 4)]", "path": "test\\test_resed.py", "repo_name": "toymachine/venster", "stars": 10, "license": "mit", "language": "python", "size": 485}
{"docstring": "\"\"\"draws a marker around a selected item\"\"\"\n", "func_signal": "def DrawSelectRect(self, hdc, rc):\n", "code": "oldPen = SelectObject(hdc, self.hatchPen.handle)\nMoveToEx(hdc, rc.left, rc.top, 0)\nLineTo(hdc, rc.right, rc.top)\nLineTo(hdc, rc.right, rc.bottom)\nLineTo(hdc, rc.left, rc.bottom)\nLineTo(hdc, rc.left, rc.top)\nSelectObject(hdc, oldPen)\nfor dragRc in self.GetDragRects(rc):\n    FillRect(hdc, byref(dragRc), self.rectBrush.handle)", "path": "test\\test_resed.py", "repo_name": "toymachine/venster", "stars": 10, "license": "mit", "language": "python", "size": 485}
{"docstring": "\"\"\"sets the row at index i, row is a list of strings\"\"\"\n", "func_signal": "def SetRow(self, i, row):\n", "code": "item = LVITEM()\nitem.mask = LVIF_TEXT\nitem.iItem = i\nfor iSubItem in range(len(row)):\n    item.iSubItem = iSubItem\n    item.pszText = row[iSubItem]\n    self.SetItem(item)", "path": "venster\\lib\\list.py", "repo_name": "toymachine/venster", "stars": 10, "license": "mit", "language": "python", "size": 485}
{"docstring": "#get the values from box A and B and put the sum in the result box\n", "func_signal": "def OnSum(self, event):\n", "code": "try:\n    a = float(self.ctrlEditA.GetText())\n    b = float(self.ctrlEditB.GetText())\n    c = str(a + b)\nexcept:\n    c = \"Error!\"\n\nself.ctrlEditSum.SetText(c)", "path": "test\\test_dialog2.py", "repo_name": "toymachine/venster", "stars": 10, "license": "mit", "language": "python", "size": 485}
{"docstring": "#TODO move these kwargs to init, use default values\n", "func_signal": "def __init__(self, **kwargs):\n", "code": "hfont = CreateFont(kwargs.get('height', 0),\n                   kwargs.get('width', 0),\n                   kwargs.get('escapement', 0),\n                   kwargs.get('orientation', 0),\n                   kwargs.get('weight', 0),\n                   kwargs.get('italic', 0),\n                   kwargs.get('underline', 0),\n                   kwargs.get('strikeout', 0),\n                   kwargs.get('charset', ANSI_CHARSET),\n                   kwargs.get('outputPrecision', OUT_DEFAULT_PRECIS),\n                   kwargs.get('clipPrecision', CLIP_DEFAULT_PRECIS),\n                   kwargs.get('quality', DEFAULT_QUALITY),\n                   kwargs.get('pitchAndFamily', DEFAULT_PITCH|FF_DONTCARE),\n                   kwargs.get('face', \"\"))\nWindowsObject.__init__(self, hfont)", "path": "venster\\gdi.py", "repo_name": "toymachine/venster", "stars": 10, "license": "mit", "language": "python", "size": 485}
{"docstring": "\"\"\"centers this window in its parent window, or the given\nparent window. If no parent window is given or found, the\nwindow is centered on the desktop\"\"\"\n", "func_signal": "def CenterWindow(self, parent = None):\n", "code": "if not parent:\n    parent = self.GetParent()\nif not parent:\n    parent = Window(hWnd = GetDesktopWindow())\n\nx = (parent.windowRect.width / 2) - (self.windowRect.width / 2)\ny = (parent.windowRect.height / 2) - (self.windowRect.height / 2)\nself.SetWindowPos(0,\n                  parent.windowRect.left + x,\n                  parent.windowRect.top + y, 0, 0,\n                  SWP_NOACTIVATE | SWP_NOSIZE | SWP_NOZORDER)", "path": "venster\\wtl.py", "repo_name": "toymachine/venster", "stars": 10, "license": "mit", "language": "python", "size": 485}
{"docstring": "\"\"\"test if mouse moves out of current popup menu and cancels\nit when so\"\"\"\n", "func_signal": "def OnHookMouseMove(self, event):\n", "code": "pt = GET_POINT_LPARAM(event.lParam)\nself.ScreenToClient(pt)\nhit = self.HitTest(pt)\nif hit >= 0 and hit != self.iCurMenu:\n    self.Cancel()\n    \nevent.handled = 0", "path": "venster\\lib\\coolbar.py", "repo_name": "toymachine/venster", "stars": 10, "license": "mit", "language": "python", "size": 485}
{"docstring": "\"\"\"inserts a row at index i, row is a list of strings\"\"\"\n", "func_signal": "def InsertRow(self, i, row, lParam = 0):\n", "code": "item = LVITEM()\nitem.mask = LVIF_TEXT | LVIF_PARAM\nitem.iItem = i\nitem.lParam = lParam\nitem.iSubItem = 0\nitem.pszText = row[0]\nself.InsertItem(item)\n#if i was -1, iItem wil now contain the index at which the row was inserted\nfor iSubItem in range(len(row) - 1):\n    item.mask = LVIF_TEXT\n    item.iSubItem = iSubItem + 1\n    item.pszText = row[iSubItem + 1]\n    self.SetItem(item)", "path": "venster\\lib\\list.py", "repo_name": "toymachine/venster", "stars": 10, "license": "mit", "language": "python", "size": 485}
{"docstring": "#figure out where start and end points are\n", "func_signal": "def DrawRect(self, hdc):\n", "code": "startX = min(self.dragStart.x, self.lastDragEnd.x)\nstartY = min(self.dragStart.y, self.lastDragEnd.y)\nendX = max(self.dragStart.x, self.lastDragEnd.x)\nendY = max(self.dragStart.y, self.lastDragEnd.y)\n#draw the rubberband\nDrawFocusRect(hdc, byref(RECT(startX, startY, endX, endY)))", "path": "test\\test_resed.py", "repo_name": "toymachine/venster", "stars": 10, "license": "mit", "language": "python", "size": 485}
{"docstring": "''' UI commands to put on the toolbar of this viewer. '''\n", "func_signal": "def createToolBarUICommands(self):\n", "code": "searchUICommand = uicommand.Search(viewer=self, settings=self.settings)\nreturn super(SearchableViewerMixin, self).createToolBarUICommands() + \\\n    [None, searchUICommand]", "path": "taskcoachlib\\gui\\viewer\\mixin.py", "repo_name": "wdmchaft/taskcoach", "stars": 11, "license": "gpl-3.0", "language": "python", "size": 3196}
{"docstring": "# pylint: disable-msg=W0201\n", "func_signal": "def _createTimers(self):\n", "code": "self._midnightTimer = PeriodicTimer(self.notifyMidnightObservers, 'day')\nself._midnightTimer.Start()\nself._secondTimer = PeriodicTimer(self.notifySecondObservers, 'second')\nself._minuteTimer = PeriodicTimer(self.notifyMinuteObservers, 'minute')\nself._scheduledTimer = ScheduledTimer(self.notifySpecificTimeObservers)", "path": "taskcoachlib\\domain\\date\\clock.py", "repo_name": "wdmchaft/taskcoach", "stars": 11, "license": "gpl-3.0", "language": "python", "size": 3196}
{"docstring": "''' This method is called by the widget when a mail message is dropped\n    on an item. '''\n", "func_signal": "def onDropMail(self, item, mail, **kwargs):\n", "code": "att = attachment.MailAttachment(mail)\nsubject, content = att.read()\nself._addAttachments([att], item, subject=subject, description=content, **kwargs)", "path": "taskcoachlib\\gui\\viewer\\mixin.py", "repo_name": "wdmchaft/taskcoach", "stars": 11, "license": "gpl-3.0", "language": "python", "size": 3196}
{"docstring": "# Make sure that if the period is daily or more, we fire after the start\n# of the new day by adding 10 seconds to the start of the next period:\n", "func_signal": "def _startOfPeriodArguments(self, period):\n", "code": "startOfPeriod = dict(year=0, month=0, day=0, hour=0, minute=0,\n                     second=10 if period in ['year', 'month', 'day'] else 0,\n                     microsecond=0)\nkeywordArguments = dict()\nperiods = self.periodsAllowed + ['microsecond']\nsmallerPeriods = periods[periods.index(period)+1:]\nfor eachSmallerPeriod in smallerPeriods:\n    keywordArguments[eachSmallerPeriod] = startOfPeriod[eachSmallerPeriod]\nreturn keywordArguments", "path": "taskcoachlib\\domain\\date\\clock.py", "repo_name": "wdmchaft/taskcoach", "stars": 11, "license": "gpl-3.0", "language": "python", "size": 3196}
{"docstring": "''' desktop.open will open a browser or other program and we\n    don't want that during unit testing. So we provide a non-existing\n    desktop, which will cause desktop.open to raise an exception. '''\n\n", "func_signal": "def testOpenByForcingAnException(self):\n", "code": "try:\n    desktop.open('http://www.taskcoach.org', desktop='Force exception')\n    self.fail('desktop.open() ignored '\n              'our non-existing desktop?!') # pragma: no cover\nexcept OSError:\n    pass", "path": "tests\\unittests\\thirdPartySoftwareTests\\DesktopTest.py", "repo_name": "wdmchaft/taskcoach", "stars": 11, "license": "gpl-3.0", "language": "python", "size": 3196}
{"docstring": "''' In tskversion <=13 category nodes were subnodes of task nodes. '''\n", "func_signal": "def _parseCategoryNodesFromTaskNodes(self, root, tasks):\n", "code": "taskNodes = root.findall('.//task')\ncategoryMapping = self._parseCategoryNodesWithinTaskNodes(taskNodes)\nsubjectCategoryMapping = {}\nfor taskId, categories in categoryMapping.items():\n    for subject in categories:\n        if subject in subjectCategoryMapping:\n            cat = subjectCategoryMapping[subject]\n        else:\n            cat = category.Category(subject)\n            subjectCategoryMapping[subject] = cat\n        theTask = tasks[taskId]\n        cat.addCategorizable(theTask)\n        theTask.addCategory(cat)\nreturn subjectCategoryMapping.values()", "path": "taskcoachlib\\persistence\\xml\\reader.py", "repo_name": "wdmchaft/taskcoach", "stars": 11, "license": "gpl-3.0", "language": "python", "size": 3196}
{"docstring": "# To allow for arbitrary large intervals, we divide the interval \n# into pieces of at most self.maxMilliseconds:\n", "func_signal": "def _startInterval(self):\n", "code": "nextInterval = min(self.millisecondsToGo, \n                   self.maxMillisecondsPerInterval)\nself.millisecondsToGo -= nextInterval\nsuper(LargeIntervalTimer, self).Start(nextInterval, oneShot=True)", "path": "taskcoachlib\\domain\\date\\clock.py", "repo_name": "wdmchaft/taskcoach", "stars": 11, "license": "gpl-3.0", "language": "python", "size": 3196}
{"docstring": "''' Parse the attributes and child notes from the noteNode. '''\n", "func_signal": "def _parseNoteNode(self, noteNode):\n", "code": "kwargs = self._parseBaseCompositeAttributes(noteNode, self._parseNoteNodes)\nif self.__tskversion > 20:\n    kwargs['attachments'] = self._parseAttachmentNodes(noteNode)\nreturn note.Note(**kwargs) # pylint: disable-msg=W0142", "path": "taskcoachlib\\persistence\\xml\\reader.py", "repo_name": "wdmchaft/taskcoach", "stars": 11, "license": "gpl-3.0", "language": "python", "size": 3196}
{"docstring": "\"\"\"Same as _parseBaseAttributes, but also parse children and expandedContexts.\"\"\"\n", "func_signal": "def _parseBaseCompositeAttributes(self, node, parseChildren, *parseChildrenArgs):\n", "code": "kwargs = self._parseBaseAttributes(node)\nkwargs['children'] = parseChildren(node, *parseChildrenArgs)\nkwargs['expandedContexts'] = self._parseTuple(node.attrib.get('expandedContexts', ''), [])\nreturn kwargs", "path": "taskcoachlib\\persistence\\xml\\reader.py", "repo_name": "wdmchaft/taskcoach", "stars": 11, "license": "gpl-3.0", "language": "python", "size": 3196}
{"docstring": "''' HACK WARNING! bdist_rpm is difficult to override because its\nmethods are much too long. We need to copy the desktop file in \naddition to the icon, so we override copy_file, check whether the \nicon is being copied, and if so, also copy the desktop file.'''\n", "func_signal": "def copy_file(self, source, dest):\n", "code": "bdist_rpm.copy_file(self, source, dest)\nif source == self.icon:\n    bdist_rpm.copy_file(self, self.desktop_file, dest)", "path": "buildlib\\bdist_rpm_fedora.py", "repo_name": "wdmchaft/taskcoach", "stars": 11, "license": "gpl-3.0", "language": "python", "size": 3196}
{"docstring": "''' This method is called by the widget when a URL is dropped on an \n    item. '''\n", "func_signal": "def onDropURL(self, item, url, **kwargs):\n", "code": "attachments = [attachment.URIAttachment(url)]\nself._addAttachments(attachments, item, **kwargs)", "path": "taskcoachlib\\gui\\viewer\\mixin.py", "repo_name": "wdmchaft/taskcoach", "stars": 11, "license": "gpl-3.0", "language": "python", "size": 3196}
{"docstring": "''' Create the UICommands for changing what the items are sorted by,\n    i.e. the columns. '''\n", "func_signal": "def createSortByUICommands(self):\n", "code": "return [uicommand.ViewerSortByCommand(viewer=self, value='subject',\n            menuText=_('Sub&ject'),\n            helpText=self.sortBySubjectHelpText),\n        uicommand.ViewerSortByCommand(viewer=self, value='description',\n            menuText=_('&Description'),\n            helpText=self.sortByDescriptionHelpText)]", "path": "taskcoachlib\\gui\\viewer\\mixin.py", "repo_name": "wdmchaft/taskcoach", "stars": 11, "license": "gpl-3.0", "language": "python", "size": 3196}
{"docstring": "''' In tskversion <=13 category nodes were subnodes of task nodes. '''\n", "func_signal": "def _parseCategoryNodesWithinTaskNodes(self, taskNodes):\n", "code": "categoryMapping = {}\nfor node in taskNodes:\n    taskId = node.attrib['id']\n    categories = [child.text for child in node.findall('category')]\n    categoryMapping.setdefault(taskId, []).extend(categories)\nreturn categoryMapping", "path": "taskcoachlib\\persistence\\xml\\reader.py", "repo_name": "wdmchaft/taskcoach", "stars": 11, "license": "gpl-3.0", "language": "python", "size": 3196}
{"docstring": "''' Parse the attributes all composite domain objects share, such as\n    id, subject, description, and return them as a \n    keyword arguments dictionary that can be passed to the domain \n    object constructor. '''\n", "func_signal": "def _parseBaseAttributes(self, node):\n", "code": "bgColorAttribute = 'color' if self.__tskversion <= 27 else 'bgColor'\nattributes = dict(id=node.attrib.get('id', ''),\n    subject=node.attrib.get('subject', ''),\n    description=self._parseDescription(node),\n    fgColor=self._parseTuple(node.attrib.get('fgColor', ''), None),\n    bgColor=self._parseTuple(node.attrib.get(bgColorAttribute, ''), None),\n    font=self._parseFontDesc(node.attrib.get('font', '')),\n    icon=node.attrib.get('icon', ''),\n    selectedIcon=node.attrib.get('selectedIcon', ''))\n\nif self.__tskversion <= 20:\n    attributes['attachments'] = self._parseAttachmentsBeforeVersion21(node)\nif self.__tskversion >= 22:\n    attributes['status'] = int(node.attrib.get('status', '1'))\n\nreturn attributes", "path": "taskcoachlib\\persistence\\xml\\reader.py", "repo_name": "wdmchaft/taskcoach", "stars": 11, "license": "gpl-3.0", "language": "python", "size": 3196}
{"docstring": "''' This method is called by the widget when one or more files\n    are dropped on an item. '''\n", "func_signal": "def onDropFiles(self, item, filenames, **kwargs):\n", "code": "attachmentBase = self.settings.get('file', 'attachmentbase')\nif attachmentBase:\n    filenames = [attachment.getRelativePath(filename, attachmentBase) \\\n                 for filename in filenames]\nattachments = [attachment.FileAttachment(filename) for filename in filenames]\nself._addAttachments(attachments, item, **kwargs)", "path": "taskcoachlib\\gui\\viewer\\mixin.py", "repo_name": "wdmchaft/taskcoach", "stars": 11, "license": "gpl-3.0", "language": "python", "size": 3196}
{"docstring": "''' Add attachments. If index refers to an existing domain object, \n    add the attachments to that object. If index is None, use the \n    newItemDialog to create a new domain object and add the attachments\n    to that new object. '''\n", "func_signal": "def _addAttachments(self, attachments, item, **itemDialogKwargs):\n", "code": "if item is None:\n    itemDialogKwargs['subject'] = attachments[0].subject()\n    newItemDialog = self.newItemDialog(bitmap='new',\n        attachments=attachments, **itemDialogKwargs)\n    newItemDialog.Show()\nelse:\n    addAttachment = command.AddAttachmentCommand(self.presentation(),\n        [item], attachments=attachments)\n    addAttachment.do()", "path": "taskcoachlib\\gui\\viewer\\mixin.py", "repo_name": "wdmchaft/taskcoach", "stars": 11, "license": "gpl-3.0", "language": "python", "size": 3196}
{"docstring": "# If the user clicks the same column for the third time, toggle\n# the SortyByTaskStatusFirst setting:\n", "func_signal": "def sortBy(self, sortKey):\n", "code": "if self.isSortedBy(sortKey):\n    self.__sortKeyUnchangedCount += 1\nelse:\n    self.__sortKeyUnchangedCount = 0\nif self.__sortKeyUnchangedCount > 1:\n    self.setSortByTaskStatusFirst(not self.isSortByTaskStatusFirst())\n    self.__sortKeyUnchangedCount = 0\nsuper(SortableViewerForTasksMixin, self).sortBy(sortKey)", "path": "taskcoachlib\\gui\\viewer\\mixin.py", "repo_name": "wdmchaft/taskcoach", "stars": 11, "license": "gpl-3.0", "language": "python", "size": 3196}
{"docstring": "''' Remove spurious newlines from element tags. '''\n", "func_signal": "def _fixBrokenLines(self):\n", "code": "self.__origFd = self.__fd # pylint: disable-msg=W0201\nself.__fd = StringIO.StringIO()\nlines = self.__origFd.readlines()\nfor index in xrange(len(lines)):\n    if lines[index].endswith('<TaskCoach-\\n') or lines[index].endswith('</TaskCoach-\\n'):\n        lines[index] = lines[index][:-1] # Remove newline\n        lines[index+1] = lines[index+1][:-1] # Remove newline\nself.__fd.write(''.join(lines))\nself.__fd.seek(0)", "path": "taskcoachlib\\persistence\\xml\\reader.py", "repo_name": "wdmchaft/taskcoach", "stars": 11, "license": "gpl-3.0", "language": "python", "size": 3196}
{"docstring": "\"\"\"Returns a SyncItem instance representing the first domain\nobject in the 'ls' sequence.\"\"\"\n\n", "func_signal": "def _getItem(self, ls):\n", "code": "try:\n    obj = ls.pop(0)\nexcept IndexError:\n    return None\n\nitem = SyncItem(obj.id())\n\nif obj.getStatus() == obj.STATUS_NONE:\n    item.state = STATE_NONE\nelif obj.getStatus() == obj.STATUS_NEW:\n    item.state = STATE_NEW\nelif obj.getStatus() == obj.STATUS_CHANGED:\n    item.state = STATE_UPDATED\nelif obj.getStatus() == obj.STATUS_DELETED:\n    item.state = STATE_DELETED\n\nself.updateItemProperties(item, obj)\n\nreturn item", "path": "taskcoachlib\\syncml\\basesource.py", "repo_name": "wdmchaft/taskcoach", "stars": 11, "license": "gpl-3.0", "language": "python", "size": 3196}
{"docstring": "''' Return the ordinal number of the day, plus a fraction between 0 and\n    1 for parts of the day. '''\n", "func_signal": "def toordinal(self):\n", "code": "ordinal = super(DateTime, self).toordinal()\nseconds = self.hour * self.secondsPerHour + \\\n          self.minute * self.secondsPerMinute + \\\n          self.second\nreturn ordinal + (seconds / float(self.secondsPerDay))", "path": "taskcoachlib\\domain\\date\\dateandtime.py", "repo_name": "wdmchaft/taskcoach", "stars": 11, "license": "gpl-3.0", "language": "python", "size": 3196}
{"docstring": "\"\"\"Get the lines for a file from the cache.\nUpdate the cache if it doesn't contain an entry for this file already.\"\"\"\n\n", "func_signal": "def getlines(filename, module_globals=None):\n", "code": "if filename in cache:\n    return cache[filename][2]\nelse:\n    return updatecache(filename, module_globals)", "path": "unittest\\Lib\\linecache.py", "repo_name": "jschementi/pycon2010", "stars": 9, "license": "None", "language": "python", "size": 24594}
{"docstring": "# sanity checks\n", "func_signal": "def addTest(self, test):\n", "code": "if not hasattr(test, '__call__'):\n    raise TypeError(\"the test to add must be callable\")\nif (isinstance(test, (type, types.ClassType)) and\n    issubclass(test, (TestCase, TestSuite))):\n    raise TypeError(\"TestCases and TestSuites must be instantiated \"\n                    \"before passing them to addTest()\")\nself._tests.append(test)", "path": "unittest\\Lib\\unittest.py", "repo_name": "jschementi/pycon2010", "stars": 9, "license": "None", "language": "python", "size": 24594}
{"docstring": "\"\"\"\nThis decorator is used to specify a CustomAttribute for a type or method.\n\"\"\"\n", "func_signal": "def attribute(attrib_type):\n", "code": "def make_decorator(*args, **kwargs):\n    return CustomAttributeDecorator(attrib_type, *args, **kwargs)\nreturn make_decorator", "path": "clrtype.py", "repo_name": "jschementi/pycon2010", "stars": 9, "license": "None", "language": "python", "size": 24594}
{"docstring": "\"\"\"Clear the cache entirely.\"\"\"\n\n", "func_signal": "def clearcache():\n", "code": "global cache\ncache = {}", "path": "unittest\\Lib\\linecache.py", "repo_name": "jschementi/pycon2010", "stars": 9, "license": "None", "language": "python", "size": 24594}
{"docstring": "\"\"\"\nGet all the methods with @accepts (and @returns) decorators\nFunctions are assumed to be instance methods, unless decorated with @staticmethod\n\"\"\"\n\n# We avoid using the \"types\" library as it is not a builtin\n", "func_signal": "def get_typed_methods(self):\n", "code": "FunctionType = type(ClrType.__dict__[\"dummy_function\"])\n\nfor item_name, item in self.__dict__.items():\n    function = None\n    is_static = False\n    if isinstance(item, FunctionType):\n        function, is_static = item, False\n    elif isinstance(item, staticmethod):\n        function, is_static = getattr(self, item_name), True\n    elif isinstance(item, property):\n        if item.fget and self.is_typed_method(item.fget):\n            if item.fget.func_name == item_name:\n                # The property hides the getter. So yield the getter\n                yield TypedFunction(item.fget, False, item_name, None)\n        if item.fset and self.is_typed_method(item.fset):\n            if item.fset.func_name == item_name:\n                # The property hides the setter. So yield the setter\n                yield TypedFunction(item.fset, False, None, item_name)\n        continue\n    else:\n        continue\n    if self.is_typed_method(function):\n        yield TypedFunction(function, is_static)", "path": "clrtype.py", "repo_name": "jschementi/pycon2010", "stars": 9, "license": "None", "language": "python", "size": 24594}
{"docstring": "\"\"\"\nTODO - needs to be merged with clr.accepts\n\"\"\"\n", "func_signal": "def accepts(*args):\n", "code": "validate_clr_types(args, True)\ndef decorator(function):\n    function.arg_types = args\n    return function\nreturn decorator", "path": "clrtype.py", "repo_name": "jschementi/pycon2010", "stars": 9, "license": "None", "language": "python", "size": 24594}
{"docstring": "\"\"\"\nUse this if you replace a function in a type with ClrInterface or ClrClass as the metaclass.\nThis will typically be needed if you are defining a decorator which wraps functions with\nnew functions, and want it to work in conjunction with clrtype\n\"\"\"\n", "func_signal": "def propagate_attributes(old_function, new_function):\n", "code": "if hasattr(old_function, \"return_type\"):\n    new_function.func_name = old_function.func_name\n    new_function.return_type = old_function.return_type\n    new_function.arg_types = old_function.arg_types\nif hasattr(old_function, \"CustomAttributeBuilders\"):\n    new_function.CustomAttributeBuilders = old_function.CustomAttributeBuilders\nif hasattr(old_function, \"CustomAttributeBuilders\"):\n    new_function.DllImportAttributeDecorator = old_function.DllImportAttributeDecorator", "path": "clrtype.py", "repo_name": "jschementi/pycon2010", "stars": 9, "license": "None", "language": "python", "size": 24594}
{"docstring": "# We need to track the generated methods so that we can emit properties\n# referring these methods. \n# Also, the hash is indexed by name *and signature*. Even though Python does \n# not have method overloading, property getter and setter functions can have \n# the same func_name attribute\n", "func_signal": "def emit_methods(self, typebld):\n", "code": "self.emitted_methods = {}\nfor function_info in self.get_typed_methods():\n    method_builder = self.emit_method(typebld, function_info)\n    function = function_info.function\n    if self.emitted_methods.has_key((function.func_name, function.arg_types)):\n        raise TypeError(\"methods with clashing names\")\n    self.emitted_methods[(function.func_name, function.arg_types)] = method_builder", "path": "clrtype.py", "repo_name": "jschementi/pycon2010", "stars": 9, "license": "None", "language": "python", "size": 24594}
{"docstring": "\"\"\"Discard cache entries that are out of date.\n(This is not checked upon each call!)\"\"\"\n\n", "func_signal": "def checkcache(filename=None):\n", "code": "if filename is None:\n    filenames = cache.keys()\nelse:\n    if filename in cache:\n        filenames = [filename]\n    else:\n        return\n\nfor filename in filenames:\n    size, mtime, lines, fullname = cache[filename]\n    if mtime is None:\n        continue   # no-op for files loaded via a __loader__\n    try:\n        stat = os.stat(fullname)\n    except os.error:\n        del cache[filename]\n        continue\n    if size != stat.st_size or mtime != stat.st_mtime:\n        del cache[filename]", "path": "unittest\\Lib\\linecache.py", "repo_name": "jschementi/pycon2010", "stars": 9, "license": "None", "language": "python", "size": 24594}
{"docstring": "\"\"\"Return a suite of all tests cases given a string specifier.\n\nThe name may resolve either to a module, a test case class, a\ntest method within a test case class, or a callable object which\nreturns a TestCase or TestSuite instance.\n\nThe method optionally resolves the names relative to a given module.\n\"\"\"\n", "func_signal": "def loadTestsFromName(self, name, module=None):\n", "code": "parts = name.split('.')\nif module is None:\n    parts_copy = parts[:]\n    while parts_copy:\n        try:\n            module = __import__('.'.join(parts_copy))\n            break\n        except ImportError:\n            del parts_copy[-1]\n            if not parts_copy: raise\n    parts = parts[1:]\nobj = module\nfor part in parts:\n    parent, obj = obj, getattr(obj, part)\n\nif type(obj) == types.ModuleType:\n    return self.loadTestsFromModule(obj)\nelif (isinstance(obj, (type, types.ClassType)) and\n      issubclass(obj, TestCase)):\n    return self.loadTestsFromTestCase(obj)\nelif (type(obj) == types.UnboundMethodType and\n      isinstance(parent, (type, types.ClassType)) and\n      issubclass(parent, TestCase)):\n    return TestSuite([parent(obj.__name__)])\nelif isinstance(obj, TestSuite):\n    return obj\nelif hasattr(obj, '__call__'):\n    test = obj()\n    if isinstance(test, TestSuite):\n        return test\n    elif isinstance(test, TestCase):\n        return TestSuite([test])\n    else:\n        raise TypeError(\"calling %s returned %s, not a test\" %\n                        (obj, test))\nelse:\n    raise TypeError(\"don't know how to make test from: %s\" % obj)", "path": "unittest\\Lib\\unittest.py", "repo_name": "jschementi/pycon2010", "stars": 9, "license": "None", "language": "python", "size": 24594}
{"docstring": "\"\"\"Returns a one-line description of the test, or None if no\ndescription has been provided.\n\nThe default implementation of this method returns the first line of\nthe specified test method's docstring.\n\"\"\"\n", "func_signal": "def shortDescription(self):\n", "code": "doc = self._testMethodDoc\nreturn doc and doc.split(\"\\n\")[0].strip() or None", "path": "unittest\\Lib\\unittest.py", "repo_name": "jschementi/pycon2010", "stars": 9, "license": "None", "language": "python", "size": 24594}
{"docstring": "# Make progressively weaker assumptions about \"other\"\n", "func_signal": "def update(self, other=None, **kwargs):\n", "code": "if other is None:\n    pass\nelif hasattr(other, 'iteritems'):  # iteritems saves memory and lookups\n    for k, v in other.iteritems():\n        self[k] = v\nelif hasattr(other, 'keys'):\n    for k in other.keys():\n        self[k] = other[k]\nelse:\n    for k, v in other:\n        self[k] = v\nif kwargs:\n    self.update(kwargs)", "path": "unittest\\Lib\\UserDict.py", "repo_name": "jschementi/pycon2010", "stars": 9, "license": "None", "language": "python", "size": 24594}
{"docstring": "\"\"\"Run the test without collecting errors in a TestResult\"\"\"\n", "func_signal": "def debug(self):\n", "code": "self.setUp()\ngetattr(self, self._testMethodName)()\nself.tearDown()", "path": "unittest\\Lib\\unittest.py", "repo_name": "jschementi/pycon2010", "stars": 9, "license": "None", "language": "python", "size": 24594}
{"docstring": "\"\"\"Return a suite of all tests cases contained in the given module\"\"\"\n", "func_signal": "def loadTestsFromModule(self, module):\n", "code": "tests = []\nfor name in dir(module):\n    obj = getattr(module, name)\n    if (isinstance(obj, (type, types.ClassType)) and\n        issubclass(obj, TestCase)):\n        tests.append(self.loadTestsFromTestCase(obj))\nreturn self.suiteClass(tests)", "path": "unittest\\Lib\\unittest.py", "repo_name": "jschementi/pycon2010", "stars": 9, "license": "None", "language": "python", "size": 24594}
{"docstring": "\"\"\"Return a suite of all tests cases found using the given sequence\nof string specifiers. See 'loadTestsFromName()'.\n\"\"\"\n", "func_signal": "def loadTestsFromNames(self, names, module=None):\n", "code": "suites = [self.loadTestsFromName(name, module) for name in names]\nreturn self.suiteClass(suites)", "path": "unittest\\Lib\\unittest.py", "repo_name": "jschementi/pycon2010", "stars": 9, "license": "None", "language": "python", "size": 24594}
{"docstring": "\"\"\"Converts a sys.exc_info()-style tuple of values into a string.\"\"\"\n", "func_signal": "def _exc_info_to_string(self, err, test):\n", "code": "exctype, value, tb = err\n# Skip test runner traceback levels\nwhile tb and self._is_relevant_tb_level(tb):\n    tb = tb.tb_next\nif exctype is test.failureException:\n    # Skip assert*() traceback levels\n    length = self._count_relevant_tb_levels(tb)\n    return ''.join(traceback.format_exception(exctype, value, tb, length))\nreturn ''.join(traceback.format_exception(exctype, value, tb))", "path": "unittest\\Lib\\unittest.py", "repo_name": "jschementi/pycon2010", "stars": 9, "license": "None", "language": "python", "size": 24594}
{"docstring": "# CFoo below will use ClrInterface as its metaclass, but the user will not expect CFoo\n# to be an interface in this case:\n#\n#   class IFoo(object):\n#     __metaclass__ = ClrInterface\n#   class CFoo(IFoo): pass\n", "func_signal": "def __clrtype__(self):\n", "code": "if not \"__metaclass__\" in self.__dict__:\n    return super(ClrInterface, self).__clrtype__()\n\nbases = list(self.__bases__)\nbases.remove(object)\nbases = tuple(bases)\nif False: # Snippets currently does not support creating interfaces\n    typegen = Snippets.Shared.DefineType(self.get_clr_type_name(), bases, True, False)\n    typebld = typegen.TypeBuilder\nelse:\n    typebld = ClrInterface.define_interface(self.get_clr_type_name(), bases)\nclr_type = self.create_type(typebld)\nself.map_clr_type(clr_type)\nreturn clr_type", "path": "clrtype.py", "repo_name": "jschementi/pycon2010", "stars": 9, "license": "None", "language": "python", "size": 24594}
{"docstring": "\"\"\"Return a suite of all tests cases contained in testCaseClass\"\"\"\n", "func_signal": "def loadTestsFromTestCase(self, testCaseClass):\n", "code": "if issubclass(testCaseClass, TestSuite):\n    raise TypeError(\"Test cases should not be derived from TestSuite. Maybe you meant to derive from TestCase?\")\ntestCaseNames = self.getTestCaseNames(testCaseClass)\nif not testCaseNames and hasattr(testCaseClass, 'runTest'):\n    testCaseNames = ['runTest']\nreturn self.suiteClass(map(testCaseClass, testCaseNames))", "path": "unittest\\Lib\\unittest.py", "repo_name": "jschementi/pycon2010", "stars": 9, "license": "None", "language": "python", "size": 24594}
{"docstring": "\"\"\"\nTODO - needs to be merged with clr.returns\n\"\"\"\n", "func_signal": "def returns(return_type = Void):\n", "code": "if return_type != Void:\n    validate_clr_types(return_type)\ndef decorator(function):\n    function.return_type = return_type\n    return function\nreturn decorator", "path": "clrtype.py", "repo_name": "jschementi/pycon2010", "stars": 9, "license": "None", "language": "python", "size": 24594}
{"docstring": "\"\"\"Return a sorted sequence of method names found within testCaseClass\n\"\"\"\n", "func_signal": "def getTestCaseNames(self, testCaseClass):\n", "code": "def isTestMethod(attrname, testCaseClass=testCaseClass, prefix=self.testMethodPrefix):\n    return attrname.startswith(prefix) and hasattr(getattr(testCaseClass, attrname), '__call__')\ntestFnNames = filter(isTestMethod, dir(testCaseClass))\nif self.sortTestMethodsUsing:\n    testFnNames.sort(key=_CmpToKey(self.sortTestMethodsUsing))\nreturn testFnNames", "path": "unittest\\Lib\\unittest.py", "repo_name": "jschementi/pycon2010", "stars": 9, "license": "None", "language": "python", "size": 24594}
{"docstring": "# start off in a known, mildly interesting state\n", "func_signal": "def test_update_tags_with_none(self):\n", "code": "Tag.objects.update_tags(self.dead_parrot, 'foo bar baz')\ntags = Tag.objects.get_for_object(self.dead_parrot)\nself.assertEquals(len(tags), 3)\nself.failUnless(get_tag('bar') in tags)\nself.failUnless(get_tag('baz') in tags)\nself.failUnless(get_tag('foo') in tags)\n\nTag.objects.update_tags(self.dead_parrot, None)\ntags = Tag.objects.get_for_object(self.dead_parrot)\nself.assertEquals(len(tags), 0)", "path": "erp\\tagging\\tests\\tests.py", "repo_name": "leohemanth/django-based-retail-ERP", "stars": 13, "license": "None", "language": "python", "size": 1946}
{"docstring": "# Once again, with feeling (strings)\n", "func_signal": "def test_related_for_model_with_tag_strings_as_input(self):\n", "code": "related_tags = Tag.objects.related_for_model('bar', Parrot, counts=True)\nrelevant_attribute_list = [(tag.name, tag.count) for tag in related_tags]\nself.assertEquals(len(relevant_attribute_list), 3)\nself.failUnless((u'baz', 1) in relevant_attribute_list)\nself.failUnless((u'foo', 1) in relevant_attribute_list)\nself.failUnless((u'ter', 2) in relevant_attribute_list)\n\nrelated_tags = Tag.objects.related_for_model('bar', Parrot, min_count=2)\nrelevant_attribute_list = [(tag.name, tag.count) for tag in related_tags]\nself.assertEquals(len(relevant_attribute_list), 1)\nself.failUnless((u'ter', 2) in relevant_attribute_list)\n\nrelated_tags = Tag.objects.related_for_model('bar', Parrot, counts=False)\nrelevant_attribute_list = [tag.name for tag in related_tags]\nself.assertEquals(len(relevant_attribute_list), 3)\nself.failUnless(u'baz' in relevant_attribute_list)\nself.failUnless(u'foo' in relevant_attribute_list)\nself.failUnless(u'ter' in relevant_attribute_list)\n\nrelated_tags = Tag.objects.related_for_model(['bar', 'ter'], Parrot, counts=True)\nrelevant_attribute_list = [(tag.name, tag.count) for tag in related_tags]\nself.assertEquals(len(relevant_attribute_list), 1)\nself.failUnless((u'baz', 1) in relevant_attribute_list)\n\nrelated_tags = Tag.objects.related_for_model(['bar', 'ter', 'baz'], Parrot, counts=True)\nrelevant_attribute_list = [(tag.name, tag.count) for tag in related_tags]\nself.assertEquals(len(relevant_attribute_list), 0)", "path": "erp\\tagging\\tests\\tests.py", "repo_name": "leohemanth/django-based-retail-ERP", "stars": 13, "license": "None", "language": "python", "size": 1946}
{"docstring": "\"\"\" Test with double-quoted multiple words.\n    A completed quote will trigger this.  Unclosed quotes are ignored. \"\"\"\n    \n", "func_signal": "def test_with_double_quoted_multiple_words(self):\n", "code": "self.assertEquals(parse_tag_input('\"one'), [u'one'])\nself.assertEquals(parse_tag_input('\"one two'), [u'one', u'two'])\nself.assertEquals(parse_tag_input('\"one two three'), [u'one', u'three', u'two'])\nself.assertEquals(parse_tag_input('\"one two\"'), [u'one two'])\nself.assertEquals(parse_tag_input('a-one \"a-two and a-three\"'),\n    [u'a-one', u'a-two and a-three'])", "path": "erp\\tagging\\tests\\tests.py", "repo_name": "leohemanth/django-based-retail-ERP", "stars": 13, "license": "None", "language": "python", "size": 1946}
{"docstring": "\"\"\"\nRetrieves ``ContentType`` and content objects for the given list of\n``TaggedItems``, grouping the retrieval of content objects by model\ntype to reduce the number of queries executed.\n\nThis results in ``number_of_content_types + 1`` queries rather than\nthe ``number_of_tagged_items * 2`` queries you'd get by iterating\nover the list and accessing each item's ``object`` attribute.\n\nA ``select_related_for`` argument can be used to specify a list of\nof model names (corresponding to the ``model`` field of a\n``ContentType``) for which ``select_related`` should be used when\nretrieving model instances.\n\"\"\"\n", "func_signal": "def fetch_content_objects(tagged_items, select_related_for=None):\n", "code": "if select_related_for is None: select_related_for = []\n\n# Group content object pks by their content type pks\nobjects = {}\nfor item in tagged_items:\n    objects.setdefault(item.content_type_id, []).append(item.object_id)\n\n# Retrieve content types and content objects in bulk\ncontent_types = ContentType._default_manager.in_bulk(objects.keys())\nfor content_type_pk, object_pks in objects.iteritems():\n    model = content_types[content_type_pk].model_class()\n    if content_types[content_type_pk].model in select_related_for:\n        objects[content_type_pk] = model._default_manager.select_related().in_bulk(object_pks)\n    else:\n        objects[content_type_pk] = model._default_manager.in_bulk(object_pks)\n\n# Set content types and content objects in the appropriate cache\n# attributes, so accessing the 'content_type' and 'object'\n# attributes on each tagged item won't result in further database\n# hits.\nfor item in tagged_items:\n    item._object_cache = objects[item.content_type_id][item.object_id]\n    item._content_type_cache = content_types[item.content_type_id]", "path": "erp\\tagging\\generic.py", "repo_name": "leohemanth/django-based-retail-ERP", "stars": 13, "license": "None", "language": "python", "size": 1946}
{"docstring": "\"\"\"\nAdd a ``font_size`` attribute to each tag according to the\nfrequency of its use, as indicated by its ``count``\nattribute.\n\n``steps`` defines the range of font sizes - ``font_size`` will\nbe an integer between 1 and ``steps`` (inclusive).\n\n``distribution`` defines the type of font size distribution\nalgorithm which will be used - logarithmic or linear. It must be\none of ``tagging.utils.LOGARITHMIC`` or ``tagging.utils.LINEAR``.\n\"\"\"\n", "func_signal": "def calculate_cloud(tags, steps=4, distribution=LOGARITHMIC):\n", "code": "if len(tags) > 0:\n    counts = [tag.count for tag in tags]\n    min_weight = float(min(counts))\n    max_weight = float(max(counts))\n    thresholds = _calculate_thresholds(min_weight, max_weight, steps)\n    for tag in tags:\n        font_set = False\n        tag_weight = _calculate_tag_weight(tag.count, max_weight, distribution)\n        for i in range(steps):\n            if not font_set and tag_weight <= thresholds[i]:\n                tag.font_size = i + 1\n                font_set = True\nreturn tags", "path": "erp\\tagging\\utils.py", "repo_name": "leohemanth/django-based-retail-ERP", "stars": 13, "license": "None", "language": "python", "size": 1946}
{"docstring": "\"\"\"\nUtility function for accepting single tag input in a flexible\nmanner.\n\nIf a ``Tag`` object is given it will be returned as-is; if a\nstring or integer are given, they will be used to lookup the\nappropriate ``Tag``.\n\nIf no matching tag can be found, ``None`` will be returned.\n\"\"\"\n", "func_signal": "def get_tag(tag):\n", "code": "from tagging.models import Tag\nif isinstance(tag, Tag):\n    return tag\n\ntry:\n    if isinstance(tag, types.StringTypes):\n        return Tag.objects.get(name=tag)\n    elif isinstance(tag, (types.IntType, types.LongType)):\n        return Tag.objects.get(id=tag)\nexcept Tag.DoesNotExist:\n    pass\n\nreturn None", "path": "erp\\tagging\\utils.py", "repo_name": "leohemanth/django-based-retail-ERP", "stars": 13, "license": "None", "language": "python", "size": 1946}
{"docstring": "\"\"\"\nRetrieves a list of ``Tag`` objects associated with a given model\nand stores them in a context variable.\n\nUsage::\n\n   {% tags_for_model [model] as [varname] %}\n\nThe model is specified in ``[appname].[modelname]`` format.\n\nExtended usage::\n\n   {% tags_for_model [model] as [varname] with counts %}\n\nIf specified - by providing extra ``with counts`` arguments - adds\na ``count`` attribute to each tag containing the number of\ninstances of the given model which have been tagged with it.\n\nExamples::\n\n   {% tags_for_model products.Widget as widget_tags %}\n   {% tags_for_model products.Widget as widget_tags with counts %}\n\n\"\"\"\n", "func_signal": "def do_tags_for_model(parser, token):\n", "code": "bits = token.contents.split()\nlen_bits = len(bits)\nif len_bits not in (4, 6):\n    raise TemplateSyntaxError(_('%s tag requires either three or five arguments') % bits[0])\nif bits[2] != 'as':\n    raise TemplateSyntaxError(_(\"second argument to %s tag must be 'as'\") % bits[0])\nif len_bits == 6:\n    if bits[4] != 'with':\n        raise TemplateSyntaxError(_(\"if given, fourth argument to %s tag must be 'with'\") % bits[0])\n    if bits[5] != 'counts':\n        raise TemplateSyntaxError(_(\"if given, fifth argument to %s tag must be 'counts'\") % bits[0])\nif len_bits == 4:\n    return TagsForModelNode(bits[1], bits[3], counts=False)\nelse:\n    return TagsForModelNode(bits[1], bits[3], counts=True)", "path": "erp\\tagging\\templatetags\\tagging_tags.py", "repo_name": "leohemanth/django-based-retail-ERP", "stars": 13, "license": "None", "language": "python", "size": 1946}
{"docstring": "\"\"\" Test forcing tags to lowercase. \"\"\"\n\n", "func_signal": "def test_force_lowercase_tags(self):\n", "code": "settings.FORCE_LOWERCASE_TAGS = True\n\nTag.objects.update_tags(self.dead_parrot, 'foO bAr Ter')\ntags = Tag.objects.get_for_object(self.dead_parrot)\nself.assertEquals(len(tags), 3)\nfoo_tag = get_tag('foo')\nbar_tag = get_tag('bar')\nter_tag = get_tag('ter')\nself.failUnless(foo_tag in tags)\nself.failUnless(bar_tag in tags)\nself.failUnless(ter_tag in tags)\n\nTag.objects.update_tags(self.dead_parrot, 'foO bAr baZ')\ntags = Tag.objects.get_for_object(self.dead_parrot)\nbaz_tag = get_tag('baz')\nself.assertEquals(len(tags), 3)\nself.failUnless(bar_tag in tags)\nself.failUnless(baz_tag in tags)\nself.failUnless(foo_tag in tags)\n\nTag.objects.add_tag(self.dead_parrot, 'FOO')\ntags = Tag.objects.get_for_object(self.dead_parrot)\nself.assertEquals(len(tags), 3)\nself.failUnless(bar_tag in tags)\nself.failUnless(baz_tag in tags)\nself.failUnless(foo_tag in tags)\n\nTag.objects.add_tag(self.dead_parrot, 'Zip')\ntags = Tag.objects.get_for_object(self.dead_parrot)\nself.assertEquals(len(tags), 4)\nzip_tag = get_tag('zip')\nself.failUnless(bar_tag in tags)\nself.failUnless(baz_tag in tags)\nself.failUnless(foo_tag in tags)\nself.failUnless(zip_tag in tags)\n\nf1 = FormTest.objects.create()\nf1.tags = u'TEST5'\nf1.save()\ntags = Tag.objects.get_for_object(f1)\ntest5_tag = get_tag('test5')\nself.assertEquals(len(tags), 1)\nself.failUnless(test5_tag in tags)\nself.assertEquals(f1.tags, u'test5')", "path": "erp\\tagging\\tests\\tests.py", "repo_name": "leohemanth/django-based-retail-ERP", "stars": 13, "license": "None", "language": "python", "size": 1946}
{"docstring": "\"\"\"\nRetrieves a list of ``Tag`` objects associated with an object and\nstores them in a context variable.\n\nUsage::\n\n   {% tags_for_object [object] as [varname] %}\n\nExample::\n\n    {% tags_for_object foo_object as tag_list %}\n\"\"\"\n", "func_signal": "def do_tags_for_object(parser, token):\n", "code": "bits = token.contents.split()\nif len(bits) != 4:\n    raise TemplateSyntaxError(_('%s tag requires exactly three arguments') % bits[0])\nif bits[2] != 'as':\n    raise TemplateSyntaxError(_(\"second argument to %s tag must be 'as'\") % bits[0])\nreturn TagsForObjectNode(bits[1], bits[3])", "path": "erp\\tagging\\templatetags\\tagging_tags.py", "repo_name": "leohemanth/django-based-retail-ERP", "stars": 13, "license": "None", "language": "python", "size": 1946}
{"docstring": "\"\"\"\nGiven a ``QuerySet`` or a ``Model``, returns a two-tuple of\n(queryset, model).\n\nIf a ``Model`` is given, the ``QuerySet`` returned will be created\nusing its default manager.\n\"\"\"\n", "func_signal": "def get_queryset_and_model(queryset_or_model):\n", "code": "try:\n    return queryset_or_model, queryset_or_model.model\nexcept AttributeError:\n    return queryset_or_model._default_manager.all(), queryset_or_model", "path": "erp\\tagging\\utils.py", "repo_name": "leohemanth/django-based-retail-ERP", "stars": 13, "license": "None", "language": "python", "size": 1946}
{"docstring": "\"\"\" Test with comma-delimited multiple words.\n    An unquoted comma in the input will trigger this. \"\"\"\n    \n", "func_signal": "def test_with_comma_delimited_multiple_words(self):\n", "code": "self.assertEquals(parse_tag_input(',one'), [u'one'])\nself.assertEquals(parse_tag_input(',one two'), [u'one two'])\nself.assertEquals(parse_tag_input(',one two three'), [u'one two three'])\nself.assertEquals(parse_tag_input('a-one, a-two and a-three'),\n    [u'a-one', u'a-two and a-three'])", "path": "erp\\tagging\\tests\\tests.py", "repo_name": "leohemanth/django-based-retail-ERP", "stars": 13, "license": "None", "language": "python", "size": 1946}
{"docstring": "\"\"\"\nSet an object's tags.\n\"\"\"\n", "func_signal": "def __set__(self, instance, value):\n", "code": "if instance is None:\n    raise AttributeError(_('%s can only be set on instances.') % self.name)\nif settings.FORCE_LOWERCASE_TAGS and value is not None:\n    value = value.lower()\nself._set_instance_tag_cache(instance, value)", "path": "erp\\tagging\\fields.py", "repo_name": "leohemanth/django-based-retail-ERP", "stars": 13, "license": "None", "language": "python", "size": 1946}
{"docstring": "# Ensure that automatically created forms use TagField\n", "func_signal": "def test_tag_field_in_modelform(self):\n", "code": "class TestForm(forms.ModelForm):\n    class Meta:\n        model = FormTest\n        \nform = TestForm()\nself.assertEquals(form.fields['tags'].__class__.__name__, 'TagField')", "path": "erp\\tagging\\tests\\tests.py", "repo_name": "leohemanth/django-based-retail-ERP", "stars": 13, "license": "None", "language": "python", "size": 1946}
{"docstring": "\"\"\"\nRetrieves a list of ``Tag`` objects for a given model, with tag\ncloud attributes set, and stores them in a context variable.\n\nUsage::\n\n   {% tag_cloud_for_model [model] as [varname] %}\n\nThe model is specified in ``[appname].[modelname]`` format.\n\nExtended usage::\n\n   {% tag_cloud_for_model [model] as [varname] with [options] %}\n\nExtra options can be provided after an optional ``with`` argument,\nwith each option being specified in ``[name]=[value]`` format. Valid\nextra options are:\n\n   ``steps``\n      Integer. Defines the range of font sizes.\n\n   ``min_count``\n      Integer. Defines the minimum number of times a tag must have\n      been used to appear in the cloud.\n\n   ``distribution``\n      One of ``linear`` or ``log``. Defines the font-size\n      distribution algorithm to use when generating the tag cloud.\n\nExamples::\n\n   {% tag_cloud_for_model products.Widget as widget_tags %}\n   {% tag_cloud_for_model products.Widget as widget_tags with steps=9 min_count=3 distribution=log %}\n\n\"\"\"\n", "func_signal": "def do_tag_cloud_for_model(parser, token):\n", "code": "bits = token.contents.split()\nlen_bits = len(bits)\nif len_bits != 4 and len_bits not in range(6, 9):\n    raise TemplateSyntaxError(_('%s tag requires either three or between five and seven arguments') % bits[0])\nif bits[2] != 'as':\n    raise TemplateSyntaxError(_(\"second argument to %s tag must be 'as'\") % bits[0])\nkwargs = {}\nif len_bits > 5:\n    if bits[4] != 'with':\n        raise TemplateSyntaxError(_(\"if given, fourth argument to %s tag must be 'with'\") % bits[0])\n    for i in range(5, len_bits):\n        try:\n            name, value = bits[i].split('=')\n            if name == 'steps' or name == 'min_count':\n                try:\n                    kwargs[str(name)] = int(value)\n                except ValueError:\n                    raise TemplateSyntaxError(_(\"%(tag)s tag's '%(option)s' option was not a valid integer: '%(value)s'\") % {\n                        'tag': bits[0],\n                        'option': name,\n                        'value': value,\n                    })\n            elif name == 'distribution':\n                if value in ['linear', 'log']:\n                    kwargs[str(name)] = {'linear': LINEAR, 'log': LOGARITHMIC}[value]\n                else:\n                    raise TemplateSyntaxError(_(\"%(tag)s tag's '%(option)s' option was not a valid choice: '%(value)s'\") % {\n                        'tag': bits[0],\n                        'option': name,\n                        'value': value,\n                    })\n            else:\n                raise TemplateSyntaxError(_(\"%(tag)s tag was given an invalid option: '%(option)s'\") % {\n                    'tag': bits[0],\n                    'option': name,\n                })\n        except ValueError:\n            raise TemplateSyntaxError(_(\"%(tag)s tag was given a badly formatted option: '%(option)s'\") % {\n                'tag': bits[0],\n                'option': bits[i],\n            })\nreturn TagCloudForModelNode(bits[1], bits[3], **kwargs)", "path": "erp\\tagging\\templatetags\\tagging_tags.py", "repo_name": "leohemanth/django-based-retail-ERP", "stars": 13, "license": "None", "language": "python", "size": 1946}
{"docstring": "# This fails on Oracle because it has no support for a 'LIMIT' clause.\n# See http://asktom.oracle.com/pls/asktom/f?p=100:11:0::::P11_QUESTION_ID:127412348064\n\n# ask for no more than 1 result\n", "func_signal": "def test_get_related_objects_of_same_model_limited_number_of_results(self):\n", "code": "related_objects = TaggedItem.objects.get_related(self.l1, Link, num=1)\nself.assertEquals(len(related_objects), 1)\nself.failUnless(self.l2 in related_objects)", "path": "erp\\tagging\\tests\\tests.py", "repo_name": "leohemanth/django-based-retail-ERP", "stars": 13, "license": "None", "language": "python", "size": 1946}
{"docstring": "\"\"\"\nGiven list of ``Tag`` instances, creates a string representation of\nthe list suitable for editing by the user, such that submitting the\ngiven string representation back without changing it will give the\nsame list of tags.\n\nTag names which contain commas will be double quoted.\n\nIf any tag name which isn't being quoted contains whitespace, the\nresulting string of tag names will be comma-delimited, otherwise\nit will be space-delimited.\n\"\"\"\n", "func_signal": "def edit_string_for_tags(tags):\n", "code": "names = []\nuse_commas = False\nfor tag in tags:\n    name = tag.name\n    if u',' in name:\n        names.append('\"%s\"' % name)\n        continue\n    elif u' ' in name:\n        if not use_commas:\n            use_commas = True\n    names.append(name)\nif use_commas:\n    glue = u', '\nelse:\n    glue = u' '\nreturn glue.join(names)", "path": "erp\\tagging\\utils.py", "repo_name": "leohemanth/django-based-retail-ERP", "stars": 13, "license": "None", "language": "python", "size": 1946}
{"docstring": "\"\"\" Double quotes can contain commas \"\"\"\n", "func_signal": "def test_tags_with_double_quotes_can_contain_commas(self):\n", "code": "self.assertEquals(parse_tag_input('a-one \"a-two, and a-three\"'),\n    [u'a-one', u'a-two, and a-three'])\nself.assertEquals(parse_tag_input('\"two\", one, one, two, \"one\"'),\n    [u'one', u'two'])", "path": "erp\\tagging\\tests\\tests.py", "repo_name": "leohemanth/django-based-retail-ERP", "stars": 13, "license": "None", "language": "python", "size": 1946}
{"docstring": "\"\"\"\nRetrieves a list of instances of a given model which are tagged with\na given ``Tag`` and stores them in a context variable.\n\nUsage::\n\n   {% tagged_objects [tag] in [model] as [varname] %}\n\nThe model is specified in ``[appname].[modelname]`` format.\n\nThe tag must be an instance of a ``Tag``, not the name of a tag.\n\nExample::\n\n    {% tagged_objects comedy_tag in tv.Show as comedies %}\n\n\"\"\"\n", "func_signal": "def do_tagged_objects(parser, token):\n", "code": "bits = token.contents.split()\nif len(bits) != 6:\n    raise TemplateSyntaxError(_('%s tag requires exactly five arguments') % bits[0])\nif bits[2] != 'in':\n    raise TemplateSyntaxError(_(\"second argument to %s tag must be 'in'\") % bits[0])\nif bits[4] != 'as':\n    raise TemplateSyntaxError(_(\"fourth argument to %s tag must be 'as'\") % bits[0])\nreturn TaggedObjectsNode(bits[1], bits[3], bits[5])", "path": "erp\\tagging\\templatetags\\tagging_tags.py", "repo_name": "leohemanth/django-based-retail-ERP", "stars": 13, "license": "None", "language": "python", "size": 1946}
{"docstring": "\"\"\"\nTag getter. Returns an instance's tags if accessed on an instance, and\nall of a model's tags if called on a class. That is, this model::\n\n   class Link(models.Model):\n       ...\n       tags = TagField()\n\nLets you do both of these::\n\n   >>> l = Link.objects.get(...)\n   >>> l.tags\n   'tag1 tag2 tag3'\n\n   >>> Link.tags\n   'tag1 tag2 tag3 tag4'\n\n\"\"\"\n# Handle access on the model (i.e. Link.tags)\n", "func_signal": "def __get__(self, instance, owner=None):\n", "code": "if instance is None:\n    return edit_string_for_tags(Tag.objects.usage_for_model(owner))\n\ntags = self._get_instance_tag_cache(instance)\nif tags is None:\n    if instance.pk is None:\n        self._set_instance_tag_cache(instance, '')\n    else:\n        self._set_instance_tag_cache(\n            instance, edit_string_for_tags(Tag.objects.get_for_object(instance)))\nreturn self._get_instance_tag_cache(instance)", "path": "erp\\tagging\\fields.py", "repo_name": "leohemanth/django-based-retail-ERP", "stars": 13, "license": "None", "language": "python", "size": 1946}
{"docstring": "\"\"\"\nLogarithmic tag weight calculation is based on code from the\n`Tag Cloud`_ plugin for Mephisto, by Sven Fuchs.\n\n.. _`Tag Cloud`: http://www.artweb-design.de/projects/mephisto-plugin-tag-cloud\n\"\"\"\n", "func_signal": "def _calculate_tag_weight(weight, max_weight, distribution):\n", "code": "if distribution == LINEAR or max_weight == 1:\n    return weight\nelif distribution == LOGARITHMIC:\n    return math.log(weight) * max_weight / math.log(max_weight)\nraise ValueError(_('Invalid distribution algorithm specified: %s.') % distribution)", "path": "erp\\tagging\\utils.py", "repo_name": "leohemanth/django-based-retail-ERP", "stars": 13, "license": "None", "language": "python", "size": 1946}
{"docstring": "\"\"\"Compares two objects to ensure they are identical.\n\nArgs:\n  orig: The original object, must be an instance of db.Model.\n  new: The new object, must be an instance of db.Model.\n  format: The serialization format being tested, used to make error output\n    more helpful.\n\nRaises:\n  The function has no return value, but will raise assertion errors if the\n  objects do not match correctly.\n\"\"\"\n", "func_signal": "def compareObjects(self, orig, new, format=\"unknown\"):\n", "code": "if orig.key().name():\n  # Only compare object keys when the key is named. Key IDs are not static\n  # and will change between dump/load. If you want stable Keys they need to\n  # be named!\n  self.assertEqual(orig.key(), new.key(),\n                   \"keys not equal after %s serialization: %s != %s\" %\n                   (format, repr(orig.key()), repr(new.key())))\n\nfor key in orig.properties().keys():\n  oval = getattr(orig, key)\n  nval = getattr(new, key)\n  if isinstance(orig.properties()[key], db.Reference):\n    # Need to compare object keys not the objects themselves.\n    oval = oval.key()\n    nval = nval.key()\n  self.assertEqual(oval, nval, \"%s attribute differs after %s \"\n                   \"serialization: %s != %s\" % (key, format, oval, nval))", "path": "code\\appengine_django\\tests\\serialization_test.py", "repo_name": "blossom/deck-django-gae", "stars": 10, "license": "None", "language": "python", "size": 4506}
{"docstring": "\"\"\"Tests that a reference specified as a list in json/yaml can be loaded OK.\"\"\"\n", "func_signal": "def runCreateKeyReferenceFromListTest(self, format):\n", "code": "self.doLookupDeserialisationReferenceTest(self.SERIALIZED_WITH_KEY_AS_LIST,\n                                          format)", "path": "code\\appengine_django\\tests\\serialization_test.py", "repo_name": "blossom/deck-django-gae", "stars": 10, "license": "None", "language": "python", "size": 4506}
{"docstring": "\"\"\"Parses the cookie set by the official Facebook JavaScript SDK.\n\ncookies should be a dictionary-like object mapping cookie names to\ncookie values.\n\nIf the user is logged in via Facebook, we return a dictionary with the\nkeys \"uid\" and \"access_token\". The former is the user's Facebook ID,\nand the latter can be used to make authenticated requests to the Graph API.\nIf the user is not logged in, we return None.\n\nDownload the official Facebook JavaScript SDK at\nhttp://github.com/facebook/connect-js/. Read more about Facebook\nauthentication at http://developers.facebook.com/docs/authentication/.\n\"\"\"\n", "func_signal": "def get_user_from_cookie(cookies, app_id, app_secret):\n", "code": "cookie = cookies.get(\"fbs_\" + app_id, \"\")\nif not cookie: return None\nargs = dict((k, v[-1]) for k, v in cgi.parse_qs(cookie.strip('\"')).items())\npayload = \"\".join(k + \"=\" + args[k] for k in sorted(args.keys())\n                  if k != \"sig\")\nsig = hashlib.md5(payload + app_secret).hexdigest()\nexpires = int(args[\"expires\"])\nif sig == args.get(\"sig\") and (expires == 0 or time.time() < expires):\n    return args\nelse:\n    return None", "path": "code\\lib\\facebook.py", "repo_name": "blossom/deck-django-gae", "stars": 10, "license": "None", "language": "python", "size": 4506}
{"docstring": "\"\"\"Extends base test functions to be called for every serialisation format.\n\nLooks for functions matching 'run.*Test', where the wildcard in the middle\nmatches the desired test name and ensures that a test case is setup to call\nthat function once for every defined serialisation format. The test case\nthat is created will be called 'test<format><name>'. Eg, for the function\n'runKeyedObjectTest' functions like 'testJsonKeyedObject' will be created.\n\"\"\"\n", "func_signal": "def __new__(cls, name, bases, attrs):\n", "code": "test_formats = serializers.get_serializer_formats()\ntest_formats.remove(\"python\")  # Python serializer is only used indirectly.\n\nfor func_name in attrs.keys():\n  m = re.match(\"^run(.*)Test$\", func_name)\n  if not m:\n    continue\n  for format in test_formats:\n    test_name = \"test%s%s\" % (format.title(), m.group(1))\n    test_func = eval(\"lambda self: getattr(self, \\\"%s\\\")(\\\"%s\\\")\" %\n                     (func_name, format))\n    attrs[test_name] = test_func\n\nreturn super(TestAllFormats, cls).__new__(cls, name, bases, attrs)", "path": "code\\appengine_django\\tests\\serialization_test.py", "repo_name": "blossom/deck-django-gae", "stars": 10, "license": "None", "language": "python", "size": 4506}
{"docstring": "\"\"\"Tests that a reference specified as repr(Key) in can loaded OK.\"\"\"\n", "func_signal": "def runCreateKeyReferenceFromReprTest(self, format):\n", "code": "self.doLookupDeserialisationReferenceTest(self.SERIALIZED_WITH_KEY_REPR,\n                                          format)", "path": "code\\appengine_django\\tests\\serialization_test.py", "repo_name": "blossom/deck-django-gae", "stars": 10, "license": "None", "language": "python", "size": 4506}
{"docstring": "\"\"\"Test serialization of a basic object with a named key.\"\"\"\n", "func_signal": "def runKeyedObjectTest(self, format):\n", "code": "obj = ModelA(description=\"test object\", key_name=\"test\")\nobj.put()\nself.doSerialisationTest(format, obj)", "path": "code\\appengine_django\\tests\\serialization_test.py", "repo_name": "blossom/deck-django-gae", "stars": 10, "license": "None", "language": "python", "size": 4506}
{"docstring": "# Ensure the Django zipfile is in the path if required.\n", "func_signal": "def main():\n", "code": "if have_django_zip and django_zip_path not in sys.path:\n  sys.path.insert(1, django_zip_path)\n\n# Create a Django application for WSGI.\napplication = django.core.handlers.wsgi.WSGIHandler()\n\n# Run the WSGI CGI handler with that application.\nutil.run_wsgi_app(application)", "path": "code\\main.py", "repo_name": "blossom/deck-django-gae", "stars": 10, "license": "None", "language": "python", "size": 4506}
{"docstring": "\"\"\"Test deserialization of an object referencing a non-existant parent.\"\"\"\n", "func_signal": "def runObjectWithNonExistantParentTest(self, format):\n", "code": "self.doModelKeyDeserialisationReferenceTest(\n    self.SERIALIZED_WITH_NON_EXISTANT_PARENT, format)", "path": "code\\appengine_django\\tests\\serialization_test.py", "repo_name": "blossom/deck-django-gae", "stars": 10, "license": "None", "language": "python", "size": 4506}
{"docstring": "\"\"\"Tests the Key reference is loaded OK for a format.\n\nArgs:\n  lookup_dict: A dictionary indexed by format containing serialized strings\n    of the objects to load.\n  format: The format to extract from the dict and deserialize.\n\nRaises:\n  This function has no return value but raises assertion errors if the\n  string cannot be deserialized correctly or the resulting object does not\n  reference the object correctly.\n\"\"\"\n", "func_signal": "def doLookupDeserialisationReferenceTest(self, lookup_dict, format):\n", "code": "if format not in lookup_dict:\n  # Check not valid for this format.\n  return\nobj = ModelA(description=\"test object\", key_name=\"test\")\nobj.put()\ns = lookup_dict[format]\nresult = list(serializers.deserialize(format, StringIO(s)))\nself.assertEqual(1, len(result), \"expected 1 object from %s\" % format)\nresult[0].save()\nself.compareObjects(obj, result[0].object.friend, format)", "path": "code\\appengine_django\\tests\\serialization_test.py", "repo_name": "blossom/deck-django-gae", "stars": 10, "license": "None", "language": "python", "size": 4506}
{"docstring": "\"\"\"Tests a model with a key can be loaded OK for a format.\n\nArgs:\n  lookup_dict: A dictionary indexed by format containing serialized strings\n    of the objects to load.\n  format: The format to extract from the dict and deserialize.\n\nReturns:\n  This function has no return value but raises assertion errors if the\n  string cannot be deserialized correctly or the resulting object is not an\n  instance of ModelA with a key named 'test'.\n\"\"\"\n", "func_signal": "def doModelKeyDeserialisationReferenceTest(self, lookup_dict, format):\n", "code": "if format not in lookup_dict:\n  # Check not valid for this format.\n  return\ns = lookup_dict[format]\nresult = list(serializers.deserialize(format, StringIO(s)))\nself.assertEqual(1, len(result), \"expected 1 object from %s\" % format)\nresult[0].save()\nself.assert_(isinstance(result[0].object, ModelA))\nself.assertEqual(\"test\", result[0].object.key().name())", "path": "code\\appengine_django\\tests\\serialization_test.py", "repo_name": "blossom/deck-django-gae", "stars": 10, "license": "None", "language": "python", "size": 4506}
{"docstring": "\"\"\"Test serialization of an object that references another object.\"\"\"\n", "func_signal": "def runObjectWithReferenceTest(self, format):\n", "code": "obj = ModelA(description=\"test object\", key_name=\"test\")\nobj.put()\nobj2 = ModelB(description=\"friend object\", friend=obj)\nobj2.put()\nself.doSerialisationTest(format, obj2, \"friend\", obj)", "path": "code\\appengine_django\\tests\\serialization_test.py", "repo_name": "blossom/deck-django-gae", "stars": 10, "license": "None", "language": "python", "size": 4506}
{"docstring": "\"\"\"Writes the given object to the graph, connected to the given parent.\n\nFor example,\n\n    graph.put_object(\"me\", \"feed\", message=\"Hello, world\")\n\nwrites \"Hello, world\" to the active user's wall. Likewise, this\nwill comment on a the first post of the active user's feed:\n\n    feed = graph.get_connections(\"me\", \"feed\")\n    post = feed[\"data\"][0]\n    graph.put_object(post[\"id\"], \"comments\", message=\"First!\")\n\nSee http://developers.facebook.com/docs/api#publishing for all of\nthe supported writeable objects.\n\nMost write operations require extended permissions. For example,\npublishing wall posts requires the \"publish_stream\" permission. See\nhttp://developers.facebook.com/docs/authentication/ for details about\nextended permissions.\n\"\"\"\n", "func_signal": "def put_object(self, parent_object, connection_name, **data):\n", "code": "assert self.access_token, \"Write operations require an access token\"\nreturn self.request(parent_object + \"/\" + connection_name, post_args=data)", "path": "code\\lib\\facebook.py", "repo_name": "blossom/deck-django-gae", "stars": 10, "license": "None", "language": "python", "size": 4506}
{"docstring": "\"\"\"Fetches the given path in the Graph API.\n\nWe translate args to a valid query string. If post_args is given,\nwe send a POST request to the given path with the given arguments.\n\"\"\"\n", "func_signal": "def request(self, path, args=None, post_args=None):\n", "code": "if not args: args = {}\nif self.access_token:\n    if post_args is not None:\n        post_args[\"access_token\"] = self.access_token\n    else:\n        args[\"access_token\"] = self.access_token\npost_data = None if post_args is None else urllib.urlencode(post_args)\nfile = urllib.urlopen(\"https://graph.facebook.com/\" + path + \"?\" +\n                      urllib.urlencode(args), post_data)\ntry:\n    response = _parse_json(file.read())\nfinally:\n    file.close()\nif response.get(\"error\"):\n    raise GraphAPIError(response[\"error\"][\"type\"],\n                        response[\"error\"][\"message\"])\nreturn response", "path": "code\\lib\\facebook.py", "repo_name": "blossom/deck-django-gae", "stars": 10, "license": "None", "language": "python", "size": 4506}
{"docstring": "\"\"\"Test serialization of an object that has a parent object reference.\"\"\"\n", "func_signal": "def runObjectWithParentTest(self, format):\n", "code": "obj = ModelA(description=\"parent object\", key_name=\"parent\")\nobj.put()\nobj2 = ModelA(description=\"child object\", key_name=\"child\", parent=obj)\nobj2.put()\nself.doSerialisationTest(format, obj2, \"parent\", obj)", "path": "code\\appengine_django\\tests\\serialization_test.py", "repo_name": "blossom/deck-django-gae", "stars": 10, "license": "None", "language": "python", "size": 4506}
{"docstring": "\"\"\"Tests that a model key specified as a repr(Key) can be loaded OK.\"\"\"\n", "func_signal": "def runCreateModelKeyFromReprTest(self, format):\n", "code": "self.doModelKeyDeserialisationReferenceTest(\n    self.MK_SERIALIZED_WITH_KEY_REPR, format)", "path": "code\\appengine_django\\tests\\serialization_test.py", "repo_name": "blossom/deck-django-gae", "stars": 10, "license": "None", "language": "python", "size": 4506}
{"docstring": "\"\"\"Test serialization of a basic object with a numeric ID key.\"\"\"\n", "func_signal": "def runObjectWithIdTest(self, format):\n", "code": "obj = ModelA(description=\"test object\")\nobj.put()\nself.doSerialisationTest(format, obj)", "path": "code\\appengine_django\\tests\\serialization_test.py", "repo_name": "blossom/deck-django-gae", "stars": 10, "license": "None", "language": "python", "size": 4506}
{"docstring": "\"\"\"Tests that a model key specified as a list can be loaded OK.\"\"\"\n", "func_signal": "def runCreateModelKeyFromListTest(self, format):\n", "code": "self.doModelKeyDeserialisationReferenceTest(self.MK_SERIALIZED_WITH_LIST,\n                                            format)", "path": "code\\appengine_django\\tests\\serialization_test.py", "repo_name": "blossom/deck-django-gae", "stars": 10, "license": "None", "language": "python", "size": 4506}
{"docstring": "\"\"\"Runs a serialization test on an object for the specified format.\n\nArgs:\n  format: The name of the Django serialization class to use.\n  obj: The object to {,de}serialize, must be an instance of db.Model.\n  rel_attr: Name of the attribute of obj references another model.\n  obj_ref: The expected object reference, must be an instance of db.Model.\n\nRaises:\n  The function has no return value but raises assertion errors if the\n  object cannot be successfully serialized and then deserialized back to an\n  identical object. If rel_attr and obj_ref are specified the deserialized\n  object must also retain the references from the original object.\n\"\"\"\n", "func_signal": "def doSerialisationTest(self, format, obj, rel_attr=None, obj_ref=None):\n", "code": "serialised = serializers.serialize(format, [obj])\n# Try and get the object back from the serialized string.\nresult = list(serializers.deserialize(format, StringIO(serialised)))\nself.assertEqual(1, len(result),\n                 \"%s serialization should create 1 object\" % format)\nresult[0].save()  # Must save back into the database to get a Key.\nself.compareObjects(obj, result[0].object, format)\nif rel_attr and obj_ref:\n  rel = getattr(result[0].object, rel_attr)\n  if callable(rel):\n    rel = rel()\n  self.compareObjects(rel, obj_ref, format)", "path": "code\\appengine_django\\tests\\serialization_test.py", "repo_name": "blossom/deck-django-gae", "stars": 10, "license": "None", "language": "python", "size": 4506}
{"docstring": "\"\"\"Fetchs all of the given object from the graph.\n\nWe return a map from ID to object. If any of the IDs are invalid,\nwe raise an exception.\n\"\"\"\n", "func_signal": "def get_objects(self, ids, **args):\n", "code": "args[\"ids\"] = \",\".join(ids)\nreturn self.request(\"\", args)", "path": "code\\lib\\facebook.py", "repo_name": "blossom/deck-django-gae", "stars": 10, "license": "None", "language": "python", "size": 4506}
{"docstring": "\"\"\"Tests that a reference specified as a plain key_name loads OK.\"\"\"\n", "func_signal": "def runCreateModelKeyFromTextTest(self, format):\n", "code": "self.doModelKeyDeserialisationReferenceTest(\n    self.MK_SERIALIZED_WITH_KEY_AS_TEXT, format)", "path": "code\\appengine_django\\tests\\serialization_test.py", "repo_name": "blossom/deck-django-gae", "stars": 10, "license": "None", "language": "python", "size": 4506}
{"docstring": "''' Use NLTK to tokenize titles from Feed Set '''\n", "func_signal": "def tokenize(feedset):\n", "code": "from utils.nlp import progwordpunct_tokenize\n\ntitletokens = []\nfor (title, id) in feedset:\n   tokens = progwordpunct_tokenize(title)\n   titletokens.extend(tokens)\n\nreturn titletokens", "path": "src\\visualizations\\labelcloud\\refresh.py", "repo_name": "colgur/reader_pipeline", "stars": 8, "license": "None", "language": "python", "size": 148}
{"docstring": "''' Use NLTK to discover the top ten most frequent terms '''\n", "func_signal": "def top_content(tokens):\n", "code": "content = getcontent(tokens)\n\nfdist = nltk.FreqDist(content)\nvocab = iter(fdist.keys())\n\nwords = {}\nfrequency = 0\nwhile frequency < 50:\n   try:\n      word = vocab.next()\n   except StopIteration:\n      break\n\n   word_lower = word.lower()\n   if word_lower in words:\n      words[word_lower] = words[word_lower] + fdist[word]\n   else:\n      words[word_lower] = fdist[word]\n\n   frequency = frequency + 1\n\nprintable_output = []\nfor word in words:\n   output_str = '%s : %d' % (word, words[word])\n   printable_output.append(output_str)\n\nreturn printable_output", "path": "src\\utils\\nlp.py", "repo_name": "colgur/reader_pipeline", "stars": 8, "license": "None", "language": "python", "size": 148}
{"docstring": "''' Parse Reader response to login for session cookie\n'''\n", "func_signal": "def _initcookie(auth_resp_content):\n", "code": "auth_resp_dict = dict(x.split('=') for x in auth_resp_content.split('\\n') if x)\nSID = auth_resp_dict[\"SID\"]\n\n# Create a cookie in the header using the SID\n__header__['Cookie'] = 'Name=SID;SID=%s;Domain=.google.com;Path=/;Expires=160000000000' % SID", "path": "src\\repo\\google\\reader\\access.py", "repo_name": "colgur/reader_pipeline", "stars": 8, "license": "None", "language": "python", "size": 148}
{"docstring": "''' Use NLTK to drive off stopwords '''\n", "func_signal": "def get_content(tokens):\n", "code": "content = [w for w in tokens if w.lower() not in unicode(__ignored_words__)]\nlongcontent = [w for w in content if len(w) > 3]\n\nreturn longcontent", "path": "src\\utils\\nlp.py", "repo_name": "colgur/reader_pipeline", "stars": 8, "license": "None", "language": "python", "size": 148}
{"docstring": "''' Sample function from NLTK section 2.4.1 '''\n", "func_signal": "def fraction(self, tokens):\n", "code": "from decimal import Decimal\n\ncontent = self.words(tokens)\nreturn Decimal(len(content)) / Decimal(len(tokens))", "path": "src\\utils\\nlp.py", "repo_name": "colgur/reader_pipeline", "stars": 8, "license": "None", "language": "python", "size": 148}
{"docstring": "# TODO: Allow clients to extend Ignored Words but only store the corpus\n", "func_signal": "def __init__(self, locale='english'):\n", "code": "self.locale = locale\nself.lower_words = {}\nself.ignored_words = stopwords.words(self.locale)", "path": "src\\utils\\nlp.py", "repo_name": "colgur/reader_pipeline", "stars": 8, "license": "None", "language": "python", "size": 148}
{"docstring": "''' Helper formats request'''\n", "func_signal": "def request(api_id):\n", "code": "parameters = urllib.urlencode({'output': 'xml'})\nfunction = '%s%s?%s' % (access.URI_API, api_id, parameters)\n\nresponse = access.request(function)\nreturn response", "path": "src\\repo\\google\\reader\\list.py", "repo_name": "colgur/reader_pipeline", "stars": 8, "license": "None", "language": "python", "size": 148}
{"docstring": "''' Program entry point '''\n", "func_signal": "def main():\n", "code": "logging.basicConfig(level=logging.DEBUG,\n                    format='%(asctime)s : %(message)s',\n                    datefmt='%H:%M:%S')\n\ntry:\n   (username, password) = parse_credentials()\nexcept ValueError:\n    sys.exit()\n\nreader_feeds = atom.feeds(username, password)\nfeedset = create_feedset(reader_feeds)\n\nfeed_tokens = tokenize(feedset)\n\ndb_user = clear_labels(username)\napply_labels(db_user, feed_tokens, feedset)", "path": "src\\visualizations\\labelcloud\\refresh.py", "repo_name": "colgur/reader_pipeline", "stars": 8, "license": "None", "language": "python", "size": 148}
{"docstring": "''' Generic URL request\n'''\n", "func_signal": "def request(url, data=None):\n", "code": "reader_req = urllib2.Request(url, data, __header__)\nreader_resp = urllib2.urlopen(reader_req)\nreader_resp_content = reader_resp.read()\n\nreturn reader_resp_content", "path": "src\\repo\\google\\reader\\access.py", "repo_name": "colgur/reader_pipeline", "stars": 8, "license": "None", "language": "python", "size": 148}
{"docstring": "'''Perform a transition and execute action.'''\n", "func_signal": "def execute(self, input):\n", "code": "si = (self.state, input)\nsn = (self.state, None)\n\n# exact state match?\nif self.states.has_key(si):\n   newstate, action = self.states[si]\n# no, how about a None (catch-all) match?\nelif self.states.has_key(sn):\n   newstate, action = self.states[sn]\n\nif self.dbg != None:\n   self.dbg.write('State: %s / Input: %s /'\n                  'Next State: %s / Action: %s\\n' %\n                  (self.state, input, newstate, action))\n\n# crc: Seemed strange to manually change state when no associated action\n# so add this check to Skip's implementation\nif action != None:\n   action(self.state, input)\n\nself.state = newstate", "path": "src\\utils\\fsm.py", "repo_name": "colgur/reader_pipeline", "stars": 8, "license": "None", "language": "python", "size": 148}
{"docstring": "'''Dictionary accessor'''\n", "func_signal": "def frequency(self, word):\n", "code": "wordfreq = 0\ntry:\n   wordfreq = self.lower_words[word]\nexcept KeyError:\n   # Not really an error: word doesn't appear then frequency is 0\n   pass\nreturn wordfreq", "path": "src\\utils\\nlp.py", "repo_name": "colgur/reader_pipeline", "stars": 8, "license": "None", "language": "python", "size": 148}
{"docstring": "''' Adopted from nltk.Text method by the same name'''\n", "func_signal": "def collocations(tokens, num=20, window_size=2):\n", "code": "from nltk.collocations import BigramCollocationFinder, bigram_measures\nfinder = BigramCollocationFinder.from_words(tokens, window_size)\nfinder.apply_freq_filter(2) \nfinder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in __ignored_words__) \n\nreturn finder.nbest(bigram_measures.likelihood_ratio, num)", "path": "src\\utils\\nlp.py", "repo_name": "colgur/reader_pipeline", "stars": 8, "license": "None", "language": "python", "size": 148}
{"docstring": "'''Call the proc with List of Subscriptions.'''\n", "func_signal": "def subscriptions(sub_xml=None):\n", "code": "if sub_xml == None:\n   sub_xml = request(SUBSCRIPTIONS)\n\nfilter = (Subscription.ID,\\\n          Subscription.TITLE)\nsubs = []\n\n# Closure instantiates Subscription upon ExpatFsm event\ndef append(tag_dict):\n   sub = Subscription(tag_dict)\n   subs.append(sub)\n\nparser = AttributeFilter(filter, append)\nparser.parse(sub_xml)\n\nreturn subs", "path": "src\\repo\\google\\reader\\list.py", "repo_name": "colgur/reader_pipeline", "stars": 8, "license": "None", "language": "python", "size": 148}
{"docstring": "'''Query Data Model for any existing Labels'''\n", "func_signal": "def clear_labels(username):\n", "code": "logging.info(\"Clearing existing Labels\")\nuser_list = User.objects.all()\ncurrent_user = None\n\nuser_pattern = '%s%s%s' % ('^', username, '$')\nuser_re = re.compile(user_pattern)\n\n# Should probably loop a different way here: no sense continuing once User is located\nfor eachuser in user_list:\n   user_match = user_re.match(eachuser.name)\n   if user_match is not None:\n      current_user = eachuser\n\nif current_user is None:\n   return current_user\n\nlabel_list = current_user.label_set.all()\nfor eachlabel in label_list:\n   edit.remove_label(eachlabel)\n\nlabel_list.delete()\n\nlogging.info(\"Done\")\nreturn current_user", "path": "src\\visualizations\\labelcloud\\refresh.py", "repo_name": "colgur/reader_pipeline", "stars": 8, "license": "None", "language": "python", "size": 148}
{"docstring": "'''Parse login info from options\n'''\n", "func_signal": "def parse_credentials():\n", "code": "import optparse\n\nparser = optparse.OptionParser()\nparser.add_option(\"-u\", \"--user\", dest=\"username\", \n                  help=\"USER associated with Feed Set\", metavar=\"USER\")\nparser.add_option(\"-p\", \"--password\", dest=\"password\", \n                  help=\"PWD associated with Feed Set\", metavar=\"PWD\")\n(options, args) = parser.parse_args()\n\nif options.username == None or options.password == None:\n   parser.print_help()\n   raise ValueError\n\nreturn (options.username, options.password)", "path": "src\\visualizations\\labelcloud\\refresh.py", "repo_name": "colgur/reader_pipeline", "stars": 8, "license": "None", "language": "python", "size": 148}
{"docstring": "''' Authenticate user and return corresponding header\n'''\n", "func_signal": "def login(login, password):\n", "code": "if login==None or password==None:\n   #TODO: throw\n   return\n\nglobal __login__\n__login__ = login\nauth_req_data = urllib.urlencode({'Email': login, 'Passwd': password})\n# TODO: Catch HttpError here\nauth_req = urllib2.Request(URI_LOGIN, data=auth_req_data)\n\nauth_resp = urllib2.urlopen(auth_req)\nauth_resp_content = auth_resp.read()\n\n_initcookie(auth_resp_content)", "path": "src\\repo\\google\\reader\\access.py", "repo_name": "colgur/reader_pipeline", "stars": 8, "license": "None", "language": "python", "size": 148}
{"docstring": "'''Expect clients to Extend ignored_words.\nThis method returns that custom list'''\n", "func_signal": "def custom_stopwords(self):\n", "code": "custom = set(self.ignored_words)\ndefault = set(stopwords.words(self.locale))\ndifference = custom.difference(default)\n\nreturn [w for w in iter(difference)]", "path": "src\\utils\\nlp.py", "repo_name": "colgur/reader_pipeline", "stars": 8, "license": "None", "language": "python", "size": 148}
{"docstring": "'''Help initialize Expat'''\n", "func_signal": "def __parser_init(self):\n", "code": "self._parser = xml.parsers.expat.ParserCreate('utf-8')\nself._parser.buffer_text = True\nself._parser.StartElementHandler = self.start_element\nself._parser.EndElementHandler = self.end_element\nself._parser.CharacterDataHandler = self.char_data", "path": "src\\repo\\google\\reader\\list.py", "repo_name": "colgur/reader_pipeline", "stars": 8, "license": "None", "language": "python", "size": 148}
{"docstring": "'''Label Feed Items in Google Reader'''\n", "func_signal": "def apply_labels(db_user, feed_tokens, feedset):\n", "code": "from utils.nlp import Content\nfeed_content = Content()\n\nlogging.info(\"Calculating Top Content\")\ntop_content = feed_content.top(feed_tokens, lowest_rank=20)\nfor eachlabel in top_content:\n   label_str = eachlabel\n   uri_str = '%s%s%s' % (access.URI_VIEW, 'user/-/label/', eachlabel)\n   freq_str = feed_content.frequency(eachlabel)\n   db_user.label_set.create(label=label_str, uri=uri_str, frequency=freq_str)\n   for (title, id) in feedset:\n      r = re.compile(eachlabel, re.I).search(title)\n      if r is not None:\n         logging.info(\"Labeling '%s' as '%s'\", id, label_str)\n         edit.add_label(eachlabel, id)\nlogging.info(\"Done\")\n\nlogging.info(\"Calculating Collocations\")\ncollocations = feed_content.collocations(feed_tokens)\nfor eachtuple in collocations:\n   for (title, id) in feedset:\n      phrase = '%s%s%s' % (eachtuple[0], ' ', eachtuple[1])\n      r = re.compile(phrase).search(title)\n      if r is not None:\n         label = '%s%s%s' % (eachtuple[0], '_', eachtuple[1])\n         edit.add_label(label, id)\nlogging.info(\"Done\")", "path": "src\\visualizations\\labelcloud\\refresh.py", "repo_name": "colgur/reader_pipeline", "stars": 8, "license": "None", "language": "python", "size": 148}
{"docstring": "''' Sample function from NLTK section 2.4.1 '''\n", "func_signal": "def content_fraction(tokens):\n", "code": "from decimal import Decimal\n\ncontent = get_content(tokens)\nreturn Decimal(len(content)) / Decimal(len(tokens))", "path": "src\\utils\\nlp.py", "repo_name": "colgur/reader_pipeline", "stars": 8, "license": "None", "language": "python", "size": 148}
{"docstring": "\"\"\"decorator to be used on decorators, it preserves the docstring and\nfunction attributes of functions to which it is applied.\"\"\"\n", "func_signal": "def decorator(function):\n", "code": "def new_decorator(f):\n    g = function(f)\n    g.__name__ = f.__name__\n    g.__doc__ = f.__doc__\n    g.__dict__.update(f.__dict__)\n    return g\nnew_decorator.__name__ = function.__name__\nnew_decorator.__doc__ = function.__doc__\nnew_decorator.__dict__.update(function.__dict__)\nreturn new_decorator", "path": "papyon\\util\\decorator.py", "repo_name": "Kjir/papyon", "stars": 11, "license": "gpl-2.0", "language": "python", "size": 1211}
{"docstring": "\"\"\"Requests multiple security tokens from the single sign on service.\n    @param callback: tuple(callable, *args)\n    @param errback: tuple(callable, *args)\n    @param services: one or more L{LiveService}\"\"\"\n\n#FIXME: we should instead check what are the common requested tokens\n# and if some tokens are not common then do a parallel request, needs\n# to be fixed later, for now, we just queue the requests\n", "func_signal": "def RequestMultipleSecurityTokens(self, callback, errback, *services):\n", "code": "if self.__pending_response:\n    self.__pending_requests.append((callback, errback, services))\n    return\n\nresponse_tokens = {}\n\nrequested_services = services\nservices = list(services)\nfor service in services: # filter already available tokens\n    service_url = service[0]\n    if service_url in self.__storage:\n        try:\n            token = self.__storage[service_url]\n        except KeyError:\n            continue\n        if not token.is_expired():\n            services.remove(service)\n            response_tokens[service] = token\n\nif len(services) == 0:\n    self._HandleRequestMultipleSecurityTokensResponse(callback,\n            errback, [], (requested_services, response_tokens))\n    return\n\nself.__pending_response = True\nmethod = self._service.RequestMultipleSecurityTokens\nself._soap_request(method, self.__credentials, services, callback,\n        errback, (requested_services, response_tokens))", "path": "papyon\\service\\SingleSignOn.py", "repo_name": "Kjir/papyon", "stars": 11, "license": "gpl-2.0", "language": "python", "size": 1211}
{"docstring": "\"\"\"initializer\n\n    @param host: the hostname to connect to.\n    @type host: string\n\n    @param port: the port number to connect to.\n    @type port: integer > 0 and < 65536\"\"\"\n", "func_signal": "def __init__(self, host, port):\n", "code": "SSLSocketClient.__init__(self, host, port, AF_INET, SOCK_STREAM)\nProxyfiableClient.__init__(self)", "path": "papyon\\gnet\\io\\ssl_tcp.py", "repo_name": "Kjir/papyon", "stars": 11, "license": "gpl-2.0", "language": "python", "size": 1211}
{"docstring": "\"\"\"Returns the SOAP xml body\"\"\"\n\n", "func_signal": "def soap_body(group_id, contact_id):\n", "code": "return \"\"\" \n    <ABGroupContactDelete xmlns=\"http://www.msn.com/webservices/AddressBook\">\n        <abId>\n            00000000-0000-0000-0000-000000000000\n        </abId>\n        <contacts>\n            <Contact>\n                <contactId>\n                    %s\n                </contactId>\n            </Contact>\n        </contacts>\n        <groupFilter>\n            <groupIds>\n                <guid>\n                    %s\n                </guid>\n            </groupIds>\n        </groupFilter>\n    </ABGroupContactDelete>\"\"\" % (contact_id, group_id)", "path": "papyon\\service\\description\\AB\\ABGroupContactDelete.py", "repo_name": "Kjir/papyon", "stars": 11, "license": "gpl-2.0", "language": "python", "size": 1211}
{"docstring": "\"\"\"Close the session and all contained streams.\"\"\"\n\n", "func_signal": "def close(self):\n", "code": "for stream in self._streams[:]:\n    self.remove_stream(stream)", "path": "papyon\\media\\session.py", "repo_name": "Kjir/papyon", "stars": 11, "license": "gpl-2.0", "language": "python", "size": 1211}
{"docstring": "\"\"\"Find a stream by its name.\n\n   @param name: Name of the stream to find\n   @type name: string\"\"\"\n\n", "func_signal": "def get_stream(self, name):\n", "code": "matching = filter(lambda x: x.name == name, self._streams)\nif not matching:\n    return None\nelse:\n    return matching[0]", "path": "papyon\\media\\session.py", "repo_name": "Kjir/papyon", "stars": 11, "license": "gpl-2.0", "language": "python", "size": 1211}
{"docstring": "\"\"\"This is a decorator which can be used to mark functions as deprecated.\nIt will result in a warning being emitted when the function is used.\"\"\"\n", "func_signal": "def deprecated(func):\n", "code": "def new_function(*args, **kwargs):\n    warnings.warn(\"Call to deprecated function %s.\" % func.__name__,\n                  category=DeprecationWarning)\n    return func(*args, **kwargs)\nreturn new_function", "path": "papyon\\util\\decorator.py", "repo_name": "Kjir/papyon", "stars": 11, "license": "gpl-2.0", "language": "python", "size": 1211}
{"docstring": "\"\"\"Returns the SOAP xml body\"\"\"\n\n", "func_signal": "def soap_body(group_id, group_name):\n", "code": "return \"\"\"\n    <ABGroupUpdate xmlns=\"http://www.msn.com/webservices/AddressBook\">\n        <abId>\n            00000000-0000-0000-0000-000000000000\n        </abId>\n        <groups>\n            <Group>\n                <groupId>\n                    %(group_id)s\n                </groupId>\n                <groupInfo>\n                    <name>\n                        %(group_name)s\n                    </name>\n                </groupInfo>\n                <propertiesChanged>\n                    GroupName\n                </propertiesChanged>\n            </Group>\n        </groups>\n    </ABGroupUpdate>\"\"\" % { 'group_id' : group_id,\n                            'group_name' : xml.escape(group_name) }", "path": "papyon\\service\\description\\AB\\ABGroupUpdate.py", "repo_name": "Kjir/papyon", "stars": 11, "license": "gpl-2.0", "language": "python", "size": 1211}
{"docstring": "\"\"\"Are all streams prepared\n   @rtype: bool\"\"\"\n", "func_signal": "def prepared(self):\n", "code": "if self._processing_message:\n    return False\nfor stream in self._streams:\n    if not stream.prepared:\n        return False\nreturn True", "path": "papyon\\media\\session.py", "repo_name": "Kjir/papyon", "stars": 11, "license": "gpl-2.0", "language": "python", "size": 1211}
{"docstring": "\"\"\"Parse the received session message and create media streams\n   accordingly. The created streams are added but we only emit the\n   'prepared' and 'ready' signals once all the descriptions are\n   processed. \n\n   @param msg: Session message received from a peer\n   @type msg: L{papyon.media.message.MediaSessionMessage}\n   @param initial: Whether or not this is the first message received\n   @type initial: boolean\"\"\"\n\n", "func_signal": "def process_remote_message(self, msg, initial=False):\n", "code": "if not msg.descriptions:\n    raise ValueError(\"Session message does not contain any information\")\n\nself._processing_message = True\n\nfor desc in msg.descriptions:\n    stream = self.get_stream(desc.name)\n    if stream is None:\n        if initial:\n            stream = self.create_stream(desc.name, desc.direction)\n            self.add_stream(stream)\n        else:\n            raise ValueError('Invalid stream \"%s\" in session message' % desc.name)\n    stream.process_remote_description(desc)\n\nself._processing_message = False\n\nif initial:\n    if self.prepared:\n        self.emit(\"prepared\")\n    if self.ready:\n        self.emit(\"ready\")", "path": "papyon\\media\\session.py", "repo_name": "Kjir/papyon", "stars": 11, "license": "gpl-2.0", "language": "python", "size": 1211}
{"docstring": "\"\"\"Checks the pending invitations.\n\n    @param sharing: the membership service\n    @param callback: tuple(callable, *args)\n    @param errback: tuple(callable, *args)\n\"\"\"\n", "func_signal": "def __init__(self, sharing, callback, errback):\n", "code": "BaseScenario.__init__(self, Scenario.MESSENGER_PENDING_LIST, callback, errback)\nself.__sharing = sharing", "path": "papyon\\service\\AddressBook\\scenario\\contacts\\check_pending_invite.py", "repo_name": "Kjir/papyon", "stars": 11, "license": "gpl-2.0", "language": "python", "size": 1211}
{"docstring": "\"\"\"Add a stream to the session and signal it that we are ready to\n   handle its signals.\n\n   @param stream: Stream to add\n   @type stream: L{papyon.media.stream.MediaStream}\"\"\"\n\n", "func_signal": "def add_stream(self, stream):\n", "code": "sp = stream.connect(\"prepared\", self.on_stream_prepared)\nsr = stream.connect(\"ready\", self.on_stream_ready)\nself._streams.append(stream)\nself._signals[stream.name] = [sp, sr]\nself._dispatch(\"on_stream_added\", stream)\nstream.activate()\nreturn stream", "path": "papyon\\media\\session.py", "repo_name": "Kjir/papyon", "stars": 11, "license": "gpl-2.0", "language": "python", "size": 1211}
{"docstring": "\"\"\"Close a stream and remove it from the session.\n\n   @param stream: Stream to remove\n   @type stream: L{papyon.media.stream.MediaStream}\"\"\"\n\n", "func_signal": "def remove_stream(self, stream):\n", "code": "name = stream.name\nfor handler_id in self._signals[name]:\n    stream.disconnect(handler_id)\ndel self._signals[name]\nstream.close()\nself._streams.remove(stream)\nself._dispatch(\"on_stream_removed\", stream)", "path": "papyon\\media\\session.py", "repo_name": "Kjir/papyon", "stars": 11, "license": "gpl-2.0", "language": "python", "size": 1211}
{"docstring": "\"\"\"Are all streams ready\n   @rtype: bool\"\"\"\n", "func_signal": "def ready(self):\n", "code": "if self._processing_message:\n    return False\nfor stream in self._streams:\n    if not stream.ready:\n        return False\nreturn True", "path": "papyon\\media\\session.py", "repo_name": "Kjir/papyon", "stars": 11, "license": "gpl-2.0", "language": "python", "size": 1211}
{"docstring": "\"\"\"The content roaming object\"\"\"\n", "func_signal": "def __init__(self, sso, ab, proxies=None):\n", "code": "gobject.GObject.__init__(self)\n\nself._storage = storage.Storage(sso, proxies)\nself._ab = ab\n\nself.__state = ContentRoamingState.NOT_SYNCHRONIZED\n\nself.__display_name = ''\nself.__personal_message = ''\nself.__display_picture = None\n\nself._profile_id = None\nself._expression_profile_id = None\nself._display_picture_id = None", "path": "papyon\\service\\ContentRoaming\\content_roaming.py", "repo_name": "Kjir/papyon", "stars": 11, "license": "gpl-2.0", "language": "python", "size": 1211}
{"docstring": "\"\"\"Returns the SOAP xml header\"\"\"\n\n", "func_signal": "def soap_header(scenario, security_token):\n", "code": "return \"\"\"<StorageApplicationHeader xmlns=\"http://www.msn.com/webservices/storage/w10\">\n        <ApplicationID>Messenger Client 8.0</ApplicationID>\n        <Scenario>\n            %s\n        </Scenario>\n    </StorageApplicationHeader>\n    <StorageUserHeader xmlns=\"http://www.msn.com/webservices/storage/w10\">\n        <Puid>0</Puid>\n        <TicketToken>\n            %s\n        </TicketToken>\n    </StorageUserHeader>\"\"\" % (xml.escape(scenario), xml.escape(security_token))", "path": "papyon\\service\\description\\SchematizedStore\\common.py", "repo_name": "Kjir/papyon", "stars": 11, "license": "gpl-2.0", "language": "python", "size": 1211}
{"docstring": "\"\"\"This is a decorator which can be used to mark functions as unstable API\nwise. It will result in a warning being emitted when the function is used.\"\"\"\n", "func_signal": "def unstable(func):\n", "code": "def new_function(*args, **kwargs):\n    warnings.warn(\"Call to unstable API function %s.\" % func.__name__,\n                  category=FutureWarning)\n    return func(*args, **kwargs)\nreturn new_function", "path": "papyon\\util\\decorator.py", "repo_name": "Kjir/papyon", "stars": 11, "license": "gpl-2.0", "language": "python", "size": 1211}
{"docstring": "\"\"\"Create a new media stream with the given name and direction.\n   The created stream need to be added to the session using add_stream.\n\n   @param name: Name of the stream (e.g. audio, video...)\n   @type name: string\n   @param direction: Direction of the stream\n   @type direction: L{papyon.media.constants.MediaStreamDirection}\n   @param created_locally: Created locally (outgoing call)\n   @type created_locally: boolean\n\n   @returns the new stream\"\"\"\n\n", "func_signal": "def create_stream(self, name, direction, created_locally=False):\n", "code": "logger.debug(\"Create stream %s\" % name)\nstream = MediaStream(self, name, direction, created_locally)\nself._dispatch(\"on_stream_created\", stream)\nreturn stream", "path": "papyon\\media\\session.py", "repo_name": "Kjir/papyon", "stars": 11, "license": "gpl-2.0", "language": "python", "size": 1211}
{"docstring": "\"\"\"Initializer\"\"\"\n", "func_signal": "def __init__(self, client):\n", "code": "gobject.GObject.__init__(self)\n\nself._client = client\nself._sessions = weakref.WeakValueDictionary() # session_id => session\nself._handlers = []\nself._transport_manager = P2PTransportManager(self._client)\nself._transport_manager.connect(\"blob-received\",\n        lambda tr, blob: self._on_blob_received(blob))\nself._transport_manager.connect(\"blob-sent\",\n        lambda tr, blob: self._on_blob_sent(blob))\nself._transport_manager.connect(\"chunk-transferred\",\n        lambda tr, chunk: self._on_chunk_transferred(chunk))", "path": "papyon\\msnp2p\\session_manager.py", "repo_name": "Kjir/papyon", "stars": 11, "license": "gpl-2.0", "language": "python", "size": 1211}
{"docstring": "\"\"\"Initialize the session\n\n   @param type: Session type\n   @type type: L{papyon.media.constants.MediaSessionType}\n   @param msg_class: The session message class to use (e.g SDPMessage)\n   @type msg_class: subclass of L{papyon.media.message.MediaSessionMessage}\"\"\"\n\n", "func_signal": "def __init__(self, type):\n", "code": "gobject.GObject.__init__(self)\nEventsDispatcher.__init__(self)\nself._type = type\n\nself._streams = []\nself._processing_message = False\nself._signals = {}", "path": "papyon\\media\\session.py", "repo_name": "Kjir/papyon", "stars": 11, "license": "gpl-2.0", "language": "python", "size": 1211}
{"docstring": "\"\"\"decodes the data to get back the session dict \"\"\"\n", "func_signal": "def decode(self, session_data):\n", "code": "pickled = base64.decodestring(session_data)\nreturn pickle.loads(pickled)", "path": "vendor\\web\\session.py", "repo_name": "anotherjesse/fobliki", "stars": 11, "license": "None", "language": "python", "size": 148}
{"docstring": "\"\"\"find name of the module name from fvars.\"\"\"\n", "func_signal": "def modname(fvars):\n", "code": "file, name = fvars['__file__'], fvars['__name__']\nif name == '__main__':\n    # Since the __main__ module can't be reloaded, the module has \n    # to be imported using its file name.                    \n    name = os.path.splitext(os.path.basename(file))[0]\nreturn name", "path": "vendor\\web\\application.py", "repo_name": "anotherjesse/fobliki", "stars": 11, "license": "None", "language": "python", "size": 148}
{"docstring": "\"\"\"Creates a new SQLQuery.\n\n    >>> SQLQuery(\"x\")\n    <sql: 'x'>\n    >>> q = SQLQuery(['SELECT * FROM ', 'test', ' WHERE x=', SQLParam(1)])\n    >>> q\n    <sql: 'SELECT * FROM test WHERE x=1'>\n    >>> q.query(), q.values()\n    ('SELECT * FROM test WHERE x=%s', [1])\n    >>> SQLQuery(SQLParam(1))\n    <sql: '1'>\n\"\"\"\n", "func_signal": "def __init__(self, items=[]):\n", "code": "if isinstance(items, list):\n    self.items = items\nelif isinstance(items, SQLParam):\n    self.items = [items]\nelif isinstance(items, SQLQuery):\n    self.items = list(items.items)\nelse:\n    self.items = [str(items)]\n    \n# Take care of SQLLiterals\nfor i, item in enumerate(self.items):\n    if isinstance(item, SQLParam) and isinstance(item.value, SQLLiteral):\n        self.items[i] = item.value.v", "path": "vendor\\web\\db.py", "repo_name": "anotherjesse/fobliki", "stars": 11, "license": "None", "language": "python", "size": 148}
{"docstring": "\"\"\"returns var, literal, paren, dict, or list\"\"\"\n", "func_signal": "def atomr(self):\n", "code": "o = (\n  self.varq() or \n  self.parenq() or\n  self.dictq() or\n  self.listq() or\n  self.literalq()\n)\nif o is False: self.Error('atom')\nreturn o", "path": "vendor\\web\\template.py", "repo_name": "anotherjesse/fobliki", "stars": 11, "license": "None", "language": "python", "size": 148}
{"docstring": "\"\"\"Generate a random id for session\"\"\"\n\n", "func_signal": "def _generate_session_id(self):\n", "code": "while True:\n    rand = random.random()\n    now = time.time()\n    secret_key = self._config.secret_key\n    session_id = md5.new(\"%s%s%s%s\" %(rand, now, web.ctx.ip, secret_key))\n    session_id = session_id.hexdigest()\n    if session_id not in self.store:\n        break\nreturn session_id", "path": "vendor\\web\\session.py", "repo_name": "anotherjesse/fobliki", "stars": 11, "license": "None", "language": "python", "size": 148}
{"docstring": "\"\"\"Initializes ctx using env.\"\"\"\n", "func_signal": "def load(self, env):\n", "code": "ctx = web.ctx\n\nctx.status = '200 OK'\nctx.headers = []\nctx.output = ''\nctx.environ = ctx.env = env\nctx.host = env.get('HTTP_HOST')\nctx.protocol = env.get('HTTPS') and 'https' or 'http'\nctx.homedomain = ctx.protocol + '://' + env.get('HTTP_HOST', '[unknown]')\nctx.homepath = os.environ.get('REAL_SCRIPT_NAME', env.get('SCRIPT_NAME', ''))\nctx.home = ctx.homedomain + ctx.homepath\nctx.ip = env.get('REMOTE_ADDR')\nctx.method = env.get('REQUEST_METHOD')\nctx.path = env.get('PATH_INFO')\n# http://trac.lighttpd.net/trac/ticket/406 requires:\nif env.get('SERVER_SOFTWARE', '').startswith('lighttpd/'):\n    ctx.path = lstrips(env.get('REQUEST_URI').split('?')[0], ctx.homepath)\n\nif env.get('QUERY_STRING'):\n    ctx.query = '?' + env.get('QUERY_STRING', '')\nelse:\n    ctx.query = ''\n\nctx.fullpath = ctx.path + ctx.query", "path": "vendor\\web\\application.py", "repo_name": "anotherjesse/fobliki", "stars": 11, "license": "None", "language": "python", "size": 148}
{"docstring": "\"\"\"Kill the session, make it no longer available\"\"\"\n", "func_signal": "def kill(self):\n", "code": "del self.store[self.session_id]\nself._killed = True", "path": "vendor\\web\\session.py", "repo_name": "anotherjesse/fobliki", "stars": 11, "license": "None", "language": "python", "size": 148}
{"docstring": "\"\"\"Cleanup the stored sessions\"\"\"\n", "func_signal": "def _cleanup(self):\n", "code": "current_time = time.time()\ntimeout = self._config.timeout\nif self._last_cleanup_time + timeout > current_time:\n    self.store.cleanup(timeout)\n    self._last_cleanup_time = current_time", "path": "vendor\\web\\session.py", "repo_name": "anotherjesse/fobliki", "stars": 11, "license": "None", "language": "python", "size": 148}
{"docstring": "\"\"\"loadhook to reload mapping and fvars.\"\"\"\n", "func_signal": "def reload_mapping():\n", "code": "mod = __import__(module_name)\nself.fvars = mod.__dict__\nself.mapping = getattr(mod, mapping_name)", "path": "vendor\\web\\application.py", "repo_name": "anotherjesse/fobliki", "stars": 11, "license": "None", "language": "python", "size": 148}
{"docstring": "\"\"\"encodes session dict as a string\"\"\"\n", "func_signal": "def encode(self, session_dict):\n", "code": "pickled = pickle.dumps(session_dict)\nreturn base64.encodestring(pickled)", "path": "vendor\\web\\session.py", "repo_name": "anotherjesse/fobliki", "stars": 11, "license": "None", "language": "python", "size": 148}
{"docstring": "# if the storage root doesn't exists, create it.\n", "func_signal": "def __init__(self, root):\n", "code": "if not os.path.exists(root):\n    os.mkdir(root)\nself.root = root", "path": "vendor\\web\\session.py", "repo_name": "anotherjesse/fobliki", "stars": 11, "license": "None", "language": "python", "size": 148}
{"docstring": "# check for change of IP\n", "func_signal": "def _validate_ip(self):\n", "code": "if self.session_id and self.get('ip', None) != web.ctx.ip:\n    if self._config.ignore_change_ip:\n        self.session_id = None\n    else:\n       return self.expired()", "path": "vendor\\web\\session.py", "repo_name": "anotherjesse/fobliki", "stars": 11, "license": "None", "language": "python", "size": 148}
{"docstring": "\"\"\"\nJoins multiple queries.\n\n>>> SQLQuery.join(['a', 'b'], ', ')\n<sql: 'a, b'>\n\"\"\"\n", "func_signal": "def join(items, sep=' '):\n", "code": "if len(items) == 0:\n    return SQLQuery(\"\")\n    \nq = SQLQuery(items[0])\nfor i in items[1:]:\n    q = q + sep + i\n\nreturn q", "path": "vendor\\web\\db.py", "repo_name": "anotherjesse/fobliki", "stars": 11, "license": "None", "language": "python", "size": 148}
{"docstring": "\"\"\"Converts an unload hook into an application processor.\n\n    >>> app = auto_application()\n    >>> def f(): \"something done after handling request\"\n    ...\n    >>> app.add_processor(unloadhook(f))    \n\"\"\"\n", "func_signal": "def unloadhook(h):\n", "code": "def processor(handler):\n    try:\n        return handler()\n    finally:\n        h()\n        \nreturn processor", "path": "vendor\\web\\application.py", "repo_name": "anotherjesse/fobliki", "stars": 11, "license": "None", "language": "python", "size": 148}
{"docstring": "\"\"\"Application processor to setup session for every request\"\"\"\n", "func_signal": "def _processor(self, handler):\n", "code": "self._cleanup()\nself._load()\n\ntry:\n    return handler()\nfinally:\n    self._save()", "path": "vendor\\web\\session.py", "repo_name": "anotherjesse/fobliki", "stars": 11, "license": "None", "language": "python", "size": 148}
{"docstring": "\"\"\"\n`left is a SQL clause like `tablename.arg = ` \nand `lst` is a list of values. Returns a reparam-style\npair featuring the SQL that ORs together the clause\nfor each item in the lst.\n\n    >>> sqlors('foo = ', [])\n    <sql: '2+2=5'>\n    >>> sqlors('foo = ', [1])\n    <sql: 'foo = 1'>\n    >>> sqlors('foo = ', 1)\n    <sql: 'foo = 1'>\n    >>> sqlors('foo = ', [1,2,3])\n    <sql: '(foo = 1 OR foo = 2 OR foo = 3)'>\n\"\"\"\n", "func_signal": "def sqlors(left, lst):\n", "code": "if isinstance(lst, iters):\n    lst = list(lst)\n    ln = len(lst)\n    if ln == 0:\n        return SQLQuery(\"2+2=5\")\n    if ln == 1:\n        lst = lst[0]\n\nif isinstance(lst, iters):\n    return '(' + SQLQuery.join([left + sqlparam(x) for x in lst], ' OR ') + ')'\nelse:\n    return left + sqlparam(lst)", "path": "vendor\\web\\db.py", "repo_name": "anotherjesse/fobliki", "stars": 11, "license": "None", "language": "python", "size": 148}
{"docstring": "# check for expiry\n", "func_signal": "def _check_expiry(self):\n", "code": "if self.session_id and self.session_id not in self.store:\n    if self._config.ignore_expiry:\n        self.session_id = None\n    else:\n        return self.expired()", "path": "vendor\\web\\session.py", "repo_name": "anotherjesse/fobliki", "stars": 11, "license": "None", "language": "python", "size": 148}
{"docstring": "# firebird doesn't support using clause\n", "func_signal": "def delete(self, table, where=None, using=None, vars=None, _test=False):\n", "code": "using=None\nreturn DB.delete(self, table, where, using, vars, _test)", "path": "vendor\\web\\db.py", "repo_name": "anotherjesse/fobliki", "stars": 11, "license": "None", "language": "python", "size": 148}
{"docstring": "\"\"\"\nReturn a CGI handler. This is mostly useful with Google App Engine.\nThere you can just do:\n\n    main = app.cgirun()\n\"\"\"\n", "func_signal": "def cgirun(self, *middleware):\n", "code": "wsgiapp = self.wsgifunc(*middleware)\n\ntry:\n    import google.appengine.ext.utils\n    return google.appengine.ext.utils.run_wsgi_app(wsgiapp, debug=True)\nexcept ImportError:\n    # we're not running from within Google App Engine\n    return wsgiref.handlers.CGIHandler().run(wsgiapp)", "path": "vendor\\web\\application.py", "repo_name": "anotherjesse/fobliki", "stars": 11, "license": "None", "language": "python", "size": 148}
{"docstring": "\"\"\"\nReturns the query part of the sql query.\n    >>> q = SQLQuery([\"SELECT * FROM test WHERE name=\", SQLParam('joe')])\n    >>> q.query()\n    'SELECT * FROM test WHERE name=%s'\n    >>> q.query(paramstyle='qmark')\n    'SELECT * FROM test WHERE name=?'\n\"\"\"\n", "func_signal": "def query(self, paramstyle=None):\n", "code": "s = ''\nfor x in self.items:\n    if isinstance(x, SQLParam):\n        x = x.get_marker(paramstyle)\n    s += x\nreturn s", "path": "vendor\\web\\db.py", "repo_name": "anotherjesse/fobliki", "stars": 11, "license": "None", "language": "python", "size": 148}
{"docstring": "\"\"\"Ensure that Python interactive shell sessions are put in\ncode blocks -- even if not properly indented.\n\"\"\"\n", "func_signal": "def _prepare_pyshell_blocks(self, text):\n", "code": "if \">>>\" not in text:\n    return text\n\nless_than_tab = self.tab_width - 1\n_pyshell_block_re = re.compile(r\"\"\"\n    ^([ ]{0,%d})>>>[ ].*\\n   # first line\n    ^(\\1.*\\S+.*\\n)*         # any number of subsequent lines\n    ^\\n                     # ends with a blank line\n    \"\"\" % less_than_tab, re.M | re.X)\n\nreturn _pyshell_block_re.sub(self._pyshell_block_sub, text)", "path": "Plugin\\markdown2\\markdown2.py", "repo_name": "davea/qlmarkdownpython", "stars": 8, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"Turn Markdown link shortcuts into XHTML <a> and <img> tags.\n\nThis is a combination of Markdown.pl's _DoAnchors() and\n_DoImages(). They are done together because that simplified the\napproach. It was necessary to use a different approach than\nMarkdown.pl because of the lack of atomic matching support in\nPython's regex engine used in $g_nested_brackets.\n\"\"\"\n", "func_signal": "def _do_links(self, text):\n", "code": "MAX_LINK_TEXT_SENTINEL = 300\n\n# `anchor_allowed_pos` is used to support img links inside\n# anchors, but not anchors inside anchors. An anchor's start\n# pos must be `>= anchor_allowed_pos`.\nanchor_allowed_pos = 0\n\ncurr_pos = 0\nwhile True: # Handle the next link.\n    # The next '[' is the start of:\n    # - an inline anchor:   [text](url \"title\")\n    # - a reference anchor: [text][id]\n    # - an inline img:      ![text](url \"title\")\n    # - a reference img:    ![text][id]\n    # - a footnote ref:     [^id]\n    #   (Only if 'footnotes' extra enabled)\n    # - a footnote defn:    [^id]: ...\n    #   (Only if 'footnotes' extra enabled) These have already\n    #   been stripped in _strip_footnote_definitions() so no\n    #   need to watch for them.\n    # - a link definition:  [id]: url \"title\"\n    #   These have already been stripped in\n    #   _strip_link_definitions() so no need to watch for them.\n    # - not markup:         [...anything else...\n    try:\n        start_idx = text.index('[', curr_pos)\n    except ValueError:\n        break\n    text_length = len(text)\n\n    # Find the matching closing ']'.\n    # Markdown.pl allows *matching* brackets in link text so we\n    # will here too. Markdown.pl *doesn't* currently allow\n    # matching brackets in img alt text -- we'll differ in that\n    # regard.\n    bracket_depth = 0\n    for p in range(start_idx+1, min(start_idx+MAX_LINK_TEXT_SENTINEL, \n                                    text_length)):\n        ch = text[p]\n        if ch == ']':\n            bracket_depth -= 1\n            if bracket_depth < 0:\n                break\n        elif ch == '[':\n            bracket_depth += 1\n    else:\n        # Closing bracket not found within sentinel length.\n        # This isn't markup.\n        curr_pos = start_idx + 1\n        continue\n    link_text = text[start_idx+1:p]\n\n    # Possibly a footnote ref?\n    if \"footnotes\" in self.extras and link_text.startswith(\"^\"):\n        normed_id = re.sub(r'\\W', '-', link_text[1:])\n        if normed_id in self.footnotes:\n            self.footnote_ids.append(normed_id)\n            result = '<sup class=\"footnote-ref\" id=\"fnref-%s\">' \\\n                     '<a href=\"#fn-%s\">%s</a></sup>' \\\n                     % (normed_id, normed_id, len(self.footnote_ids))\n            text = text[:start_idx] + result + text[p+1:]\n        else:\n            # This id isn't defined, leave the markup alone.\n            curr_pos = p+1\n        continue\n\n    # Now determine what this is by the remainder.\n    p += 1\n    if p == text_length:\n        return text\n\n    # Inline anchor or img?\n    if text[p] == '(': # attempt at perf improvement\n        match = self._tail_of_inline_link_re.match(text, p)\n        if match:\n            # Handle an inline anchor or img.\n            is_img = start_idx > 0 and text[start_idx-1] == \"!\"\n            if is_img:\n                start_idx -= 1\n\n            url, title = match.group(\"url\"), match.group(\"title\")\n            if url and url[0] == '<':\n                url = url[1:-1]  # '<url>' -> 'url'\n            # We've got to encode these to avoid conflicting\n            # with italics/bold.\n            url = url.replace('*', g_escape_table['*']) \\\n                     .replace('_', g_escape_table['_'])\n            if title:\n                title_str = ' title=\"%s\"' \\\n                    % title.replace('*', g_escape_table['*']) \\\n                           .replace('_', g_escape_table['_']) \\\n                           .replace('\"', '&quot;')\n            else:\n                title_str = ''\n            if is_img:\n                result = '<img src=\"%s\" alt=\"%s\"%s%s' \\\n                    % (url, link_text.replace('\"', '&quot;'),\n                       title_str, self.empty_element_suffix)\n                curr_pos = start_idx + len(result)\n                text = text[:start_idx] + result + text[match.end():]\n            elif start_idx >= anchor_allowed_pos:\n                result_head = '<a href=\"%s\"%s>' % (url, title_str)\n                result = '%s%s</a>' % (result_head, link_text)\n                # <img> allowed from curr_pos on, <a> from\n                # anchor_allowed_pos on.\n                curr_pos = start_idx + len(result_head)\n                anchor_allowed_pos = start_idx + len(result)\n                text = text[:start_idx] + result + text[match.end():]\n            else:\n                # Anchor not allowed here.\n                curr_pos = start_idx + 1\n            continue\n\n    # Reference anchor or img?\n    else:\n        match = self._tail_of_reference_link_re.match(text, p)\n        if match:\n            # Handle a reference-style anchor or img.\n            is_img = start_idx > 0 and text[start_idx-1] == \"!\"\n            if is_img:\n                start_idx -= 1\n            link_id = match.group(\"id\").lower()\n            if not link_id:\n                link_id = link_text.lower()  # for links like [this][]\n            if link_id in self.urls:\n                url = self.urls[link_id]\n                # We've got to encode these to avoid conflicting\n                # with italics/bold.\n                url = url.replace('*', g_escape_table['*']) \\\n                         .replace('_', g_escape_table['_'])\n                title = self.titles.get(link_id)\n                if title:\n                    title = title.replace('*', g_escape_table['*']) \\\n                                 .replace('_', g_escape_table['_'])\n                    title_str = ' title=\"%s\"' % title\n                else:\n                    title_str = ''\n                if is_img:\n                    result = '<img src=\"%s\" alt=\"%s\"%s%s' \\\n                        % (url, link_text.replace('\"', '&quot;'),\n                           title_str, self.empty_element_suffix)\n                    curr_pos = start_idx + len(result)\n                    text = text[:start_idx] + result + text[match.end():]\n                elif start_idx >= anchor_allowed_pos:\n                    result = '<a href=\"%s\"%s>%s</a>' \\\n                        % (url, title_str, link_text)\n                    result_head = '<a href=\"%s\"%s>' % (url, title_str)\n                    result = '%s%s</a>' % (result_head, link_text)\n                    # <img> allowed from curr_pos on, <a> from\n                    # anchor_allowed_pos on.\n                    curr_pos = start_idx + len(result_head)\n                    anchor_allowed_pos = start_idx + len(result)\n                    text = text[:start_idx] + result + text[match.end():]\n                else:\n                    # Anchor not allowed here.\n                    curr_pos = start_idx + 1\n            else:\n                # This id isn't defined, leave the markup alone.\n                curr_pos = match.end()\n            continue\n\n    # Otherwise, it isn't markup.\n    curr_pos = start_idx + 1\n\nreturn text", "path": "Plugin\\markdown2\\markdown2.py", "repo_name": "davea/qlmarkdownpython", "stars": 8, "license": "None", "language": "python", "size": 108}
{"docstring": "# Strips link definitions from text, stores the URLs and titles in\n# hash references.\n", "func_signal": "def _strip_link_definitions(self, text):\n", "code": "less_than_tab = self.tab_width - 1\n    \n# Link defs are in the form:\n#   [id]: url \"optional title\"\n_link_def_re = re.compile(r\"\"\"\n    ^[ ]{0,%d}\\[(.+)\\]: # id = \\1\n      [ \\t]*\n      \\n?               # maybe *one* newline\n      [ \\t]*\n    <?(.+?)>?           # url = \\2\n      [ \\t]*\n    (?:\n        \\n?             # maybe one newline\n        [ \\t]*\n        (?<=\\s)         # lookbehind for whitespace\n        ['\"(]\n        ([^\\n]*)        # title = \\3\n        ['\")]\n        [ \\t]*\n    )?  # title is optional\n    (?:\\n+|\\Z)\n    \"\"\" % less_than_tab, re.X | re.M | re.U)\nreturn _link_def_re.sub(self._extract_link_def_sub, text)", "path": "Plugin\\markdown2\\markdown2.py", "repo_name": "davea/qlmarkdownpython", "stars": 8, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"A footnote definition looks like this:\n\n    [^note-id]: Text of the note.\n\n        May include one or more indented paragraphs.\n\nWhere,\n- The 'note-id' can be pretty much anything, though typically it\n  is the number of the footnote.\n- The first paragraph may start on the next line, like so:\n    \n    [^note-id]:\n        Text of the note.\n\"\"\"\n", "func_signal": "def _strip_footnote_definitions(self, text):\n", "code": "less_than_tab = self.tab_width - 1\nfootnote_def_re = re.compile(r'''\n    ^[ ]{0,%d}\\[\\^(.+)\\]:   # id = \\1\n    [ \\t]*\n    (                       # footnote text = \\2\n      # First line need not start with the spaces.\n      (?:\\s*.*\\n+)\n      (?:\n        (?:[ ]{%d} | \\t)  # Subsequent lines must be indented.\n        .*\\n+\n      )*\n    )\n    # Lookahead for non-space at line-start, or end of doc.\n    (?:(?=^[ ]{0,%d}\\S)|\\Z)\n    ''' % (less_than_tab, self.tab_width, self.tab_width),\n    re.X | re.M)\nreturn footnote_def_re.sub(self._extract_footnote_def_sub, text)", "path": "Plugin\\markdown2\\markdown2.py", "repo_name": "davea/qlmarkdownpython", "stars": 8, "license": "None", "language": "python", "size": 108}
{"docstring": "# These are all the transformations that form block-level\n# tags like paragraphs, headers, and list items.\n\n", "func_signal": "def _run_block_gamut(self, text):\n", "code": "text = self._do_headers(text)\n\n# Do Horizontal Rules:\nhr = \"\\n<hr\"+self.empty_element_suffix+\"\\n\"\nfor hr_re in self._hr_res:\n    text = hr_re.sub(hr, text)\n\ntext = self._do_lists(text)\n\nif \"pyshell\" in self.extras:\n    text = self._prepare_pyshell_blocks(text)\n\ntext = self._do_code_blocks(text)\n\ntext = self._do_block_quotes(text)\n\n# We already ran _HashHTMLBlocks() before, in Markdown(), but that\n# was to escape raw HTML in the original Markdown source. This time,\n# we're escaping the markup we've just created, so that we don't wrap\n# <p> tags around block-level tags.\ntext = self._hash_html_blocks(text)\n\ntext = self._form_paragraphs(text)\n\nreturn text", "path": "Plugin\\markdown2\\markdown2.py", "repo_name": "davea/qlmarkdownpython", "stars": 8, "license": "None", "language": "python", "size": 108}
{"docstring": "# Python markdown note: the HTML tokenization here differs from\n# that in Markdown.pl, hence the behaviour for subtle cases can\n# differ (I believe the tokenizer here does a better job because\n# it isn't susceptible to unmatched '<' and '>' in HTML tags).\n# Note, however, that '>' is not allowed in an auto-link URL\n# here.\n", "func_signal": "def _escape_special_chars(self, text):\n", "code": "escaped = []\nis_html_markup = False\nfor token in self._sorta_html_tokenize_re.split(text):\n    if is_html_markup:\n        # Within tags/HTML-comments/auto-links, encode * and _\n        # so they don't conflict with their use in Markdown for\n        # italics and strong.  We're replacing each such\n        # character with its corresponding MD5 checksum value;\n        # this is likely overkill, but it should prevent us from\n        # colliding with the escape values by accident.\n        escaped.append(token.replace('*', g_escape_table['*'])\n                            .replace('_', g_escape_table['_']))\n    else:\n        escaped.append(self._encode_backslash_escapes(token))\n    is_html_markup = not is_html_markup\nreturn ''.join(escaped)", "path": "Plugin\\markdown2\\markdown2.py", "repo_name": "davea/qlmarkdownpython", "stars": 8, "license": "None", "language": "python", "size": 108}
{"docstring": "# Form HTML ordered (numbered) and unordered (bulleted) lists.\n\n", "func_signal": "def _do_lists(self, text):\n", "code": "for marker_pat in (self._marker_ul, self._marker_ol):\n    # Re-usable pattern to match any entire ul or ol list:\n    less_than_tab = self.tab_width - 1\n    whole_list = r'''\n        (                   # \\1 = whole list\n          (                 # \\2\n            [ ]{0,%d}\n            (%s)            # \\3 = first list item marker\n            [ \\t]+\n          )\n          (?:.+?)\n          (                 # \\4\n              \\Z\n            |\n              \\n{2,}\n              (?=\\S)\n              (?!           # Negative lookahead for another list item marker\n                [ \\t]*\n                %s[ \\t]+\n              )\n          )\n        )\n    ''' % (less_than_tab, marker_pat, marker_pat)\n\n    # We use a different prefix before nested lists than top-level lists.\n    # See extended comment in _process_list_items().\n    #\n    # Note: There's a bit of duplication here. My original implementation\n    # created a scalar regex pattern as the conditional result of the test on\n    # $g_list_level, and then only ran the $text =~ s{...}{...}egmx\n    # substitution once, using the scalar as the pattern. This worked,\n    # everywhere except when running under MT on my hosting account at Pair\n    # Networks. There, this caused all rebuilds to be killed by the reaper (or\n    # perhaps they crashed, but that seems incredibly unlikely given that the\n    # same script on the same server ran fine *except* under MT. I've spent\n    # more time trying to figure out why this is happening than I'd like to\n    # admit. My only guess, backed up by the fact that this workaround works,\n    # is that Perl optimizes the substition when it can figure out that the\n    # pattern will never change, and when this optimization isn't on, we run\n    # afoul of the reaper. Thus, the slightly redundant code to that uses two\n    # static s/// patterns rather than one conditional pattern.\n\n    if self.list_level:\n        sub_list_re = re.compile(\"^\"+whole_list, re.X | re.M | re.S)\n        text = sub_list_re.sub(self._list_sub, text)\n    else:\n        list_re = re.compile(r\"(?:(?<=\\n\\n)|\\A\\n?)\"+whole_list,\n                             re.X | re.M | re.S)\n        text = list_re.sub(self._list_sub, text)\n\nreturn text", "path": "Plugin\\markdown2\\markdown2.py", "repo_name": "davea/qlmarkdownpython", "stars": 8, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"_dedent(text, tabsize=8, skip_first_line=False) -> dedented text\n\n    \"text\" is the text to dedent.\n    \"tabsize\" is the tab width to use for indent width calculations.\n    \"skip_first_line\" is a boolean indicating if the first line should\n        be skipped for calculating the indent width and for dedenting.\n        This is sometimes useful for docstrings and similar.\n\ntextwrap.dedent(s), but don't expand tabs to spaces\n\"\"\"\n", "func_signal": "def _dedent(text, tabsize=8, skip_first_line=False):\n", "code": "lines = text.splitlines(1)\n_dedentlines(lines, tabsize=tabsize, skip_first_line=skip_first_line)\nreturn ''.join(lines)", "path": "Plugin\\markdown2\\markdown2.py", "repo_name": "davea/qlmarkdownpython", "stars": 8, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"Encode/escape certain characters inside Markdown code runs.\nThe point is that in code, these characters are literals,\nand lose their special Markdown meanings.\n\"\"\"\n", "func_signal": "def _encode_code(self, text):\n", "code": "replacements = [\n    # Encode all ampersands; HTML entities are not\n    # entities within a Markdown code span.\n    ('&', '&amp;'),\n    # Do the angle bracket song and dance:\n    ('<', '&lt;'),\n    ('>', '&gt;'),\n    # Now, escape characters that are magic in Markdown:\n    ('*', g_escape_table['*']),\n    ('_', g_escape_table['_']),\n    ('{', g_escape_table['{']),\n    ('}', g_escape_table['}']),\n    ('[', g_escape_table['[']),\n    (']', g_escape_table[']']),\n    ('\\\\', g_escape_table['\\\\']),\n]\nfor before, after in replacements:\n    text = text.replace(before, after)\nreturn text", "path": "Plugin\\markdown2\\markdown2.py", "repo_name": "davea/qlmarkdownpython", "stars": 8, "license": "None", "language": "python", "size": 108}
{"docstring": "# Smart processing for ampersands and angle brackets that need\n# to be encoded.\n", "func_signal": "def _encode_amps_and_angles(self, text):\n", "code": "text = self._ampersand_re.sub('&amp;', text)\n    \n# Encode naked <'s\ntext = self._naked_lt_re.sub('&lt;', text)\n\n# Encode naked >'s\n# Note: Other markdown implementations (e.g. Markdown.pl, PHP\n# Markdown) don't do this.\ntext = self._naked_gt_re.sub('&gt;', text)\nreturn text", "path": "Plugin\\markdown2\\markdown2.py", "repo_name": "davea/qlmarkdownpython", "stars": 8, "license": "None", "language": "python", "size": 108}
{"docstring": "# <strong> must go first:\n", "func_signal": "def _do_italics_and_bold(self, text):\n", "code": "if \"code-friendly\" in self.extras:\n    text = self._code_friendly_strong_re.sub(r\"<strong>\\1</strong>\", text)\n    text = self._code_friendly_em_re.sub(r\"<em>\\1</em>\", text)\nelse:\n    text = self._strong_re.sub(r\"<strong>\\2</strong>\", text)\n    text = self._em_re.sub(r\"<em>\\2</em>\", text)\nreturn text", "path": "Plugin\\markdown2\\markdown2.py", "repo_name": "davea/qlmarkdownpython", "stars": 8, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"'foo'    -> re.compile(re.escape('foo'))\n   '/foo/'  -> re.compile('foo')\n   '/foo/i' -> re.compile('foo', re.I)\n\"\"\"\n", "func_signal": "def _regex_from_encoded_pattern(s):\n", "code": "if s.startswith('/') and s.rfind('/') != 0:\n    # Parse it: /PATTERN/FLAGS\n    idx = s.rfind('/')\n    pattern, flags_str = s[1:idx], s[idx+1:]\n    flag_from_char = {\n        \"i\": re.IGNORECASE,\n        \"l\": re.LOCALE,\n        \"s\": re.DOTALL,\n        \"m\": re.MULTILINE,\n        \"u\": re.UNICODE,\n    }\n    flags = 0\n    for char in flags_str:\n        try:\n            flags |= flag_from_char[char]\n        except KeyError:\n            raise ValueError(\"unsupported regex flag: '%s' in '%s' \"\n                             \"(must be one of '%s')\"\n                             % (char, s, ''.join(flag_from_char.keys())))\n    return re.compile(s[1:idx], flags)\nelse: # not an encoded regex\n    return re.compile(re.escape(s))", "path": "Plugin\\markdown2\\markdown2.py", "repo_name": "davea/qlmarkdownpython", "stars": 8, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"Caveat emptor: there isn't much guarding against link\npatterns being formed inside other standard Markdown links, e.g.\ninside a [link def][like this].\n\nDev Notes: *Could* consider prefixing regexes with a negative\nlookbehind assertion to attempt to guard against this.\n\"\"\"\n", "func_signal": "def _do_link_patterns(self, text):\n", "code": "link_from_hash = {}\nfor regex, repl in self.link_patterns:\n    replacements = []\n    for match in regex.finditer(text):\n        if hasattr(repl, \"__call__\"):\n            href = repl(match)\n        else:\n            href = match.expand(repl)\n        replacements.append((match.span(), href))\n    for (start, end), href in reversed(replacements):\n        escaped_href = (\n            href.replace('\"', '&quot;')  # b/c of attr quote\n                # To avoid markdown <em> and <strong>:\n                .replace('*', g_escape_table['*'])\n                .replace('_', g_escape_table['_']))\n        link = '<a href=\"%s\">%s</a>' % (escaped_href, text[start:end])\n        hash = md5(link).hexdigest()\n        link_from_hash[hash] = link\n        text = text[:start] + hash + text[end:]\nfor hash, link in link_from_hash.items():\n    text = text.replace(hash, link)\nreturn text", "path": "Plugin\\markdown2\\markdown2.py", "repo_name": "davea/qlmarkdownpython", "stars": 8, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"Convert the given text.\"\"\"\n# Main function. The order in which other subs are called here is\n# essential. Link and image substitutions need to happen before\n# _EscapeSpecialChars(), so that any *'s or _'s in the <a>\n# and <img> tags get encoded.\n\n# Clear the global hashes. If we don't clear these, you get conflicts\n# from other articles when generating a page which contains more than\n# one article (e.g. an index page that shows the N most recent\n# articles):\n", "func_signal": "def convert(self, text):\n", "code": "self.reset()\n\nif not isinstance(text, unicode):\n    #TODO: perhaps shouldn't presume UTF-8 for string input?\n    text = unicode(text, 'utf-8')\n\nif self.use_file_vars:\n    # Look for emacs-style file variable hints.\n    emacs_vars = self._get_emacs_vars(text)\n    if \"markdown-extras\" in emacs_vars:\n        splitter = re.compile(\"[ ,]+\")\n        for e in splitter.split(emacs_vars[\"markdown-extras\"]):\n            if '=' in e:\n                ename, earg = e.split('=', 1)\n                try:\n                    earg = int(earg)\n                except ValueError:\n                    pass\n            else:\n                ename, earg = e, None\n            self.extras[ename] = earg\n\n# Standardize line endings:\ntext = re.sub(\"\\r\\n|\\r\", \"\\n\", text)\n\n# Make sure $text ends with a couple of newlines:\ntext += \"\\n\\n\"\n\n# Convert all tabs to spaces.\ntext = self._detab(text)\n\n# Strip any lines consisting only of spaces and tabs.\n# This makes subsequent regexen easier to write, because we can\n# match consecutive blank lines with /\\n+/ instead of something\n# contorted like /[ \\t]*\\n+/ .\ntext = self._ws_only_line_re.sub(\"\", text)\n\nif self.safe_mode:\n    text = self._hash_html_spans(text)\n\n# Turn block-level HTML blocks into hash entries\ntext = self._hash_html_blocks(text, raw=True)\n\n# Strip link definitions, store in hashes.\nif \"footnotes\" in self.extras:\n    # Must do footnotes first because an unlucky footnote defn\n    # looks like a link defn:\n    #   [^4]: this \"looks like a link defn\"\n    text = self._strip_footnote_definitions(text)\ntext = self._strip_link_definitions(text)\n\ntext = self._run_block_gamut(text)\n\ntext = self._unescape_special_chars(text)\n\nif \"footnotes\" in self.extras:\n    text = self._add_footnotes(text)\n\nif self.safe_mode:\n    text = self._unhash_html_spans(text)\n\ntext += \"\\n\"\nreturn text", "path": "Plugin\\markdown2\\markdown2.py", "repo_name": "davea/qlmarkdownpython", "stars": 8, "license": "None", "language": "python", "size": 108}
{"docstring": "# Strip leading and trailing lines:\n", "func_signal": "def _form_paragraphs(self, text):\n", "code": "text = text.strip('\\n')\n\n# Wrap <p> tags.\ngrafs = re.split(r\"\\n{2,}\", text)\nfor i, graf in enumerate(grafs):\n    if graf in self.html_blocks:\n        # Unhashify HTML blocks\n        grafs[i] = self.html_blocks[graf]\n    else:\n        # Wrap <p> tags.\n        graf = self._run_span_gamut(graf)\n        grafs[i] = \"<p>\" + graf.lstrip(\" \\t\") + \"</p>\"\n\nreturn \"\\n\\n\".join(grafs)", "path": "Plugin\\markdown2\\markdown2.py", "repo_name": "davea/qlmarkdownpython", "stars": 8, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"Return a dictionary of emacs-style local variables.\n\nParsing is done loosely according to this spec (and according to\nsome in-practice deviations from this):\nhttp://www.gnu.org/software/emacs/manual/html_node/emacs/Specifying-File-Variables.html#Specifying-File-Variables\n\"\"\"\n", "func_signal": "def _get_emacs_vars(self, text):\n", "code": "emacs_vars = {}\nSIZE = pow(2, 13) # 8kB\n\n# Search near the start for a '-*-'-style one-liner of variables.\nhead = text[:SIZE]\nif \"-*-\" in head:\n    match = self._emacs_oneliner_vars_pat.search(head)\n    if match:\n        emacs_vars_str = match.group(1)\n        assert '\\n' not in emacs_vars_str\n        emacs_var_strs = [s.strip() for s in emacs_vars_str.split(';')\n                          if s.strip()]\n        if len(emacs_var_strs) == 1 and ':' not in emacs_var_strs[0]:\n            # While not in the spec, this form is allowed by emacs:\n            #   -*- Tcl -*-\n            # where the implied \"variable\" is \"mode\". This form\n            # is only allowed if there are no other variables.\n            emacs_vars[\"mode\"] = emacs_var_strs[0].strip()\n        else:\n            for emacs_var_str in emacs_var_strs:\n                try:\n                    variable, value = emacs_var_str.strip().split(':', 1)\n                except ValueError:\n                    log.debug(\"emacs variables error: malformed -*- \"\n                              \"line: %r\", emacs_var_str)\n                    continue\n                # Lowercase the variable name because Emacs allows \"Mode\"\n                # or \"mode\" or \"MoDe\", etc.\n                emacs_vars[variable.lower()] = value.strip()\n\ntail = text[-SIZE:]\nif \"Local Variables\" in tail:\n    match = self._emacs_local_vars_pat.search(tail)\n    if match:\n        prefix = match.group(\"prefix\")\n        suffix = match.group(\"suffix\")\n        lines = match.group(\"content\").splitlines(0)\n        #print \"prefix=%r, suffix=%r, content=%r, lines: %s\"\\\n        #      % (prefix, suffix, match.group(\"content\"), lines)\n\n        # Validate the Local Variables block: proper prefix and suffix\n        # usage.\n        for i, line in enumerate(lines):\n            if not line.startswith(prefix):\n                log.debug(\"emacs variables error: line '%s' \"\n                          \"does not use proper prefix '%s'\"\n                          % (line, prefix))\n                return {}\n            # Don't validate suffix on last line. Emacs doesn't care,\n            # neither should we.\n            if i != len(lines)-1 and not line.endswith(suffix):\n                log.debug(\"emacs variables error: line '%s' \"\n                          \"does not use proper suffix '%s'\"\n                          % (line, suffix))\n                return {}\n\n        # Parse out one emacs var per line.\n        continued_for = None\n        for line in lines[:-1]: # no var on the last line (\"PREFIX End:\")\n            if prefix: line = line[len(prefix):] # strip prefix\n            if suffix: line = line[:-len(suffix)] # strip suffix\n            line = line.strip()\n            if continued_for:\n                variable = continued_for\n                if line.endswith('\\\\'):\n                    line = line[:-1].rstrip()\n                else:\n                    continued_for = None\n                emacs_vars[variable] += ' ' + line\n            else:\n                try:\n                    variable, value = line.split(':', 1)\n                except ValueError:\n                    log.debug(\"local variables error: missing colon \"\n                              \"in local variables entry: '%s'\" % line)\n                    continue\n                # Do NOT lowercase the variable name, because Emacs only\n                # allows \"mode\" (and not \"Mode\", \"MoDe\", etc.) in this block.\n                value = value.strip()\n                if value.endswith('\\\\'):\n                    value = value[:-1].rstrip()\n                    continued_for = variable\n                else:\n                    continued_for = None\n                emacs_vars[variable] = value\n\n# Unquote values.\nfor var, val in emacs_vars.items():\n    if len(val) > 1 and (val.startswith('\"') and val.endswith('\"')\n       or val.startswith('\"') and val.endswith('\"')):\n        emacs_vars[var] = val[1:-1]\n\nreturn emacs_vars", "path": "Plugin\\markdown2\\markdown2.py", "repo_name": "davea/qlmarkdownpython", "stars": 8, "license": "None", "language": "python", "size": 108}
{"docstring": "# These are all the transformations that occur *within* block-level\n# tags like paragraphs, headers, and list items.\n    \n", "func_signal": "def _run_span_gamut(self, text):\n", "code": "text = self._do_code_spans(text)\n    \ntext = self._escape_special_chars(text)\n    \n# Process anchor and image tags.\ntext = self._do_links(text)\n    \n# Make links out of things like `<http://example.com/>`\n# Must come after _do_links(), because you can use < and >\n# delimiters in inline links like [this](<url>).\ntext = self._do_auto_links(text)\n\nif \"link-patterns\" in self.extras:\n    text = self._do_link_patterns(text)\n    \ntext = self._encode_amps_and_angles(text)\n    \ntext = self._do_italics_and_bold(text)\n    \n# Do hard breaks:\ntext = re.sub(r\" {2,}\\n\", \" <br%s\\n\" % self.empty_element_suffix, text)\n    \nreturn text", "path": "Plugin\\markdown2\\markdown2.py", "repo_name": "davea/qlmarkdownpython", "stars": 8, "license": "None", "language": "python", "size": 108}
{"docstring": "#  Input: an email address, e.g. \"foo@example.com\"\n#\n#  Output: the email address as a mailto link, with each character\n#      of the address encoded as either a decimal or hex entity, in\n#      the hopes of foiling most address harvesting spam bots. E.g.:\n#\n#    <a href=\"&#x6D;&#97;&#105;&#108;&#x74;&#111;:&#102;&#111;&#111;&#64;&#101;\n#       x&#x61;&#109;&#x70;&#108;&#x65;&#x2E;&#99;&#111;&#109;\">&#102;&#111;&#111;\n#       &#64;&#101;x&#x61;&#109;&#x70;&#108;&#x65;&#x2E;&#99;&#111;&#109;</a>\n#\n#  Based on a filter by Matthew Wickline, posted to the BBEdit-Talk\n#  mailing list: <http://tinyurl.com/yu7ue>\n", "func_signal": "def _encode_email_address(self, addr):\n", "code": "chars = [_xml_encode_email_char_at_random(ch)\n         for ch in \"mailto:\" + addr]\n# Strip the mailto: from the visible part.\naddr = '<a href=\"%s\">%s</a>' \\\n       % (''.join(chars), ''.join(chars[7:]))\nreturn addr", "path": "Plugin\\markdown2\\markdown2.py", "repo_name": "davea/qlmarkdownpython", "stars": 8, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"A function for use in a Pygments Formatter which\nwraps in <code> tags.\n\"\"\"\n", "func_signal": "def _wrap_code(self, inner):\n", "code": "yield 0, \"<code>\"\nfor tup in inner:\n    yield tup \nyield 0, \"</code>\"", "path": "Plugin\\markdown2\\markdown2.py", "repo_name": "davea/qlmarkdownpython", "stars": 8, "license": "None", "language": "python", "size": 108}
{"docstring": "# Used for safe_mode.\n\n", "func_signal": "def _hash_html_spans(self, text):\n", "code": "def _is_auto_link(s):\n    if ':' in s and self._auto_link_re.match(s):\n        return True\n    elif '@' in s and self._auto_email_link_re.match(s):\n        return True\n    return False\n\ntokens = []\nis_html_markup = False\nfor token in self._sorta_html_tokenize_re.split(text):\n    if is_html_markup and not _is_auto_link(token):\n        sanitized = self._sanitize_html(token)\n        key = _hash_text(sanitized)\n        self.html_spans[key] = sanitized\n        tokens.append(key)\n    else:\n        tokens.append(token)\n    is_html_markup = not is_html_markup\nreturn ''.join(tokens)", "path": "Plugin\\markdown2\\markdown2.py", "repo_name": "davea/qlmarkdownpython", "stars": 8, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\" Open this file, read all bytes, return them as a string. \"\"\"\n", "func_signal": "def bytes(self):\n", "code": "f = self.open('rb')\ntry:\n    return f.read()\nfinally:\n    f.close()", "path": "mongosearch\\path.py", "repo_name": "glamkit/python-mongo-search", "stars": 9, "license": "other", "language": "python", "size": 366}
{"docstring": "\"\"\"\nSearch, returning full result sets and limiting by the supplied id_list\n\"\"\"\n", "func_signal": "def search_by_ids(collection, search_query_string, id_list=None):\n", "code": "raw_search_results = raw_search(collection, search_query_string)\nsearch_coll_name = raw_search_results.name\nmap_js = Code(\"function() { mft.get('search')._searchMap.call(this) }\")\nreduce_js = Code(\"function(k, v) { return mft.get('search')._searchReduce(k, v) }\")\nscope =  {'coll_name': collection.name}\ndb = collection.database\nsorting = {'value.score': pymongo.DESCENDING}\nif id_list is None:\n    id_query_obj = {}\nelse:\n    id_query_obj = {'_id': {'$in': id_list}}\nres_coll = db[search_coll_name].map_reduce(map_js, reduce_js, \n    query=id_query_obj, scope=scope, sort=sorting)\n#should we be ensuring an index here? or just leave it?\n# res_coll.ensure_index([('value.score', pymongo.ASCENDING)])\nreturn res_coll.find()", "path": "mongosearch\\mongo_search.py", "repo_name": "glamkit/python-mongo-search", "stars": 9, "license": "other", "language": "python", "size": 366}
{"docstring": "\"\"\" Return this path as a relative path,\nbased from the current working directory.\n\"\"\"\n", "func_signal": "def relpath(self):\n", "code": "cwd = self.__class__(os.getcwd())\nreturn cwd.relpathto(self)", "path": "mongosearch\\path.py", "repo_name": "glamkit/python-mongo-search", "stars": 9, "license": "other", "language": "python", "size": 366}
{"docstring": "\"\"\" D.walk() -> iterator over files and subdirs, recursively.\n\nThe iterator yields path objects naming each child item of\nthis directory and its descendants.  This requires that\nD.isdir().\n\nThis performs a depth-first traversal of the directory tree.\nEach directory is returned just before all its children.\n\nThe errors= keyword argument controls behavior when an\nerror occurs.  The default is 'strict', which causes an\nexception.  The other allowed values are 'warn', which\nreports the error via warnings.warn(), and 'ignore'.\n\"\"\"\n", "func_signal": "def walk(self, pattern=None, errors='strict'):\n", "code": "if errors not in ('strict', 'warn', 'ignore'):\n    raise ValueError(\"invalid errors parameter\")\n\ntry:\n    childList = self.listdir()\nexcept Exception:\n    if errors == 'ignore':\n        return\n    elif errors == 'warn':\n        warnings.warn(\n            \"Unable to list directory '%s': %s\"\n            % (self, sys.exc_info()[1]),\n            TreeWalkWarning)\n        return\n    else:\n        raise\n\nfor child in childList:\n    if pattern is None or child.fnmatch(pattern):\n        yield child\n    try:\n        isdir = child.isdir()\n    except Exception:\n        if errors == 'ignore':\n            isdir = False\n        elif errors == 'warn':\n            warnings.warn(\n                \"Unable to access '%s': %s\"\n                % (child, sys.exc_info()[1]),\n                TreeWalkWarning)\n            isdir = False\n        else:\n            raise\n\n    if isdir:\n        for item in child.walk(pattern, errors):\n            yield item", "path": "mongosearch\\path.py", "repo_name": "glamkit/python-mongo-search", "stars": 9, "license": "other", "language": "python", "size": 366}
{"docstring": "\"\"\" Open this file and write the given bytes to it.\n\nDefault behavior is to overwrite any existing file.\nCall p.write_bytes(bytes, append=True) to append instead.\n\"\"\"\n", "func_signal": "def write_bytes(self, bytes, append=False):\n", "code": "if append:\n    mode = 'ab'\nelse:\n    mode = 'wb'\nf = self.open(mode)\ntry:\n    f.write(bytes)\nfinally:\n    f.close()", "path": "mongosearch\\path.py", "repo_name": "glamkit/python-mongo-search", "stars": 9, "license": "other", "language": "python", "size": 366}
{"docstring": "\"\"\" D.walkfiles() -> iterator over files in D, recursively.\n\nThe optional argument, pattern, limits the results to files\nwith names that match the pattern.  For example,\nmydir.walkfiles('*.tmp') yields only files with the .tmp\nextension.\n\"\"\"\n", "func_signal": "def walkfiles(self, pattern=None, errors='strict'):\n", "code": "if errors not in ('strict', 'warn', 'ignore'):\n    raise ValueError(\"invalid errors parameter\")\n\ntry:\n    childList = self.listdir()\nexcept Exception:\n    if errors == 'ignore':\n        return\n    elif errors == 'warn':\n        warnings.warn(\n            \"Unable to list directory '%s': %s\"\n            % (self, sys.exc_info()[1]),\n            TreeWalkWarning)\n        return\n    else:\n        raise\n\nfor child in childList:\n    try:\n        isfile = child.isfile()\n        isdir = not isfile and child.isdir()\n    except:\n        if errors == 'ignore':\n            continue\n        elif errors == 'warn':\n            warnings.warn(\n                \"Unable to access '%s': %s\"\n                % (self, sys.exc_info()[1]),\n                TreeWalkWarning)\n            continue\n        else:\n            raise\n\n    if isfile:\n        if pattern is None or child.fnmatch(pattern):\n            yield child\n    elif isdir:\n        for f in child.walkfiles(pattern, errors):\n            yield f", "path": "mongosearch\\path.py", "repo_name": "glamkit/python-mongo-search", "stars": 9, "license": "other", "language": "python", "size": 366}
{"docstring": "\"\"\" Return a list of path objects that match the pattern.\n\npattern - a path relative to this directory, with wildcards.\n\nFor example, path('/users').glob('*/bin/*') returns a list\nof all the files users have in their bin directories.\n\"\"\"\n", "func_signal": "def glob(self, pattern):\n", "code": "cls = self.__class__\nreturn [cls(s) for s in glob.glob(_base(self / pattern))]", "path": "mongosearch\\path.py", "repo_name": "glamkit/python-mongo-search", "stars": 9, "license": "other", "language": "python", "size": 366}
{"docstring": "\"\"\"\ngiven the defined list of submodules in `self.gitsubmodules`,\nensure they are all checked out. (simplistic style - simply checks for\nany non-hidden files using `glob`.)\n\"\"\"\n", "func_signal": "def ensure_submodules(self, *args, **kwargs):\n", "code": "import os\nfrom glob import glob\nfrom distutils.util import convert_path\nmissing_submodule = False\nfor submodule_path in self.git_submodules:\n    file_list = glob(convert_path(\n      os.path.join(submodule_path, '*')\n    ))\n    if len(file_list) : continue\n    log.debug('missing submodule %s' % submodule_path)\n    missing_submodule = True\nif missing_submodule:\n    self.update_submodules()", "path": "pavement.py", "repo_name": "glamkit/python-mongo-search", "stars": 9, "license": "other", "language": "python", "size": 366}
{"docstring": "\"\"\"\nConfigure the text search index named `index_name` on the supplied `collection`.\n\n`fields_json` should be dict\nwith fieldnames as keys and integers as values -- eg:\n    \"{'content': 1, 'title': 5}\"\n    \nre-implementation of the JS function 'search.configureSearchIndexFields'\n\"\"\"\n", "func_signal": "def configure_text_index_fields(collection, fields, index_name=None):\n", "code": "if index_name is None:\n    index_name = DEFAULT_INDEX_NAME\n# do some basic validation here, to try and catch errors that might ocur in the JS\nif not isinstance(fields, dict):\n    raise InvalidSearchFieldConfiguration(\"Fields must be a dictionary of fieldname/weighting pairs\")\nfor fieldname, fieldvalue in fields.iteritems():\n    if not isinstance(fieldname, str) and not isinstance(fieldname, unicode):\n        raise InvalidSearchFieldConfiguration(\"Field names (the keys of the `fields` dict)\"\n            \"must be strings or unicode objects. You supplied %r of type %s\" % (fieldname, type(fieldname)))\n    if not isinstance(fieldvalue, int):\n        raise InvalidSearchFieldConfiguration(\"Field value (the keys of the `fields` dict)\"\n            \"must be integers. You supplied %r of type %s\" % (fieldvalue, type(fieldvalue)))\ndb = collection.database\ncoll_name_spec = {'collection_name': collection.name}\ncollection_conf = db[CONFIG_COLLECTION].find_one(coll_name_spec);\nif collection_conf is None:\n    collection_conf = {'collection_name': collection.name}\nif 'indexes' not in collection_conf: \n    collection_conf['indexes'] = { }\ncollection_conf['indexes'][index_name] = { }\ncollection_conf['indexes'][index_name]['fields'] = fields\ndb[CONFIG_COLLECTION].update(coll_name_spec, collection_conf, upsert=True);", "path": "mongosearch\\mongo_search.py", "repo_name": "glamkit/python-mongo-search", "stars": 9, "license": "other", "language": "python", "size": 366}
{"docstring": "\"\"\" p.splitpath() -> Return (p.parent, p.name). \"\"\"\n", "func_signal": "def splitpath(self):\n", "code": "parent, child = os.path.split(self)\nreturn self.__class__(parent), child", "path": "mongosearch\\path.py", "repo_name": "glamkit/python-mongo-search", "stars": 9, "license": "other", "language": "python", "size": 366}
{"docstring": "\"\"\" Calculate the md5 hash for this file.\n\nThis reads through the entire file.\n\"\"\"\n", "func_signal": "def read_md5(self):\n", "code": "f = self.open('rb')\ntry:\n    m = md5.new()\n    while True:\n        d = f.read(8192)\n        if not d:\n            break\n        m.update(d)\nfinally:\n    f.close()\nreturn m.digest()", "path": "mongosearch\\path.py", "repo_name": "glamkit/python-mongo-search", "stars": 9, "license": "other", "language": "python", "size": 366}
{"docstring": "\"\"\"\nLimit the search to supplied number of results.\n\nThis is useful for pagination. This operated the same way as .limit() on \na regular cursor.\n\"\"\"\n", "func_signal": "def limit(self, limit):\n", "code": "if self._actual_result_cursor is not None:\n    raise InvalidSearchOperation(\"Cannot set search options after\"\n     \" executing SearchQuery\")\nself._limit = limit\nreturn self", "path": "mongosearch\\mongo_search.py", "repo_name": "glamkit/python-mongo-search", "stars": 9, "license": "other", "language": "python", "size": 366}
{"docstring": "\"\"\"\nactually do submodule updating.\nseparate functnion call so we can isolate teh GitPython import\n\nTested with git.__version__=='0.2.0-beta1'\n\"\"\"\n", "func_signal": "def update_submodules(self, *args, **kwargs):\n", "code": "import git\n\nrepo = git.Git('.')\nrepo.submodule('init')\nrepo.submodule('update')", "path": "pavement.py", "repo_name": "glamkit/python-mongo-search", "stars": 9, "license": "other", "language": "python", "size": 366}
{"docstring": "\"\"\"\nSearch, returning full result sets and limiting by the supplied id_list\nA re-implementation of the javascript function search.mapReduceSearch.\n\"\"\"\n# because we only have access to the index collection later, we have to convert \n# the query_obj to an id list\n", "func_signal": "def search_by_query(collection, search_query_string, query_obj):\n", "code": "id_list = [rec['_id'] for rec in collection.find(query_obj, ['_id'])]\nreturn search_by_ids(collection, search_query_string, id_list)", "path": "mongosearch\\mongo_search.py", "repo_name": "glamkit/python-mongo-search", "stars": 9, "license": "other", "language": "python", "size": 366}
{"docstring": "\"\"\" D.walkdirs() -> iterator over subdirs, recursively.\n\nWith the optional 'pattern' argument, this yields only\ndirectories whose names match the given pattern.  For\nexample, mydir.walkdirs('*test') yields only directories\nwith names ending in 'test'.\n\nThe errors= keyword argument controls behavior when an\nerror occurs.  The default is 'strict', which causes an\nexception.  The other allowed values are 'warn', which\nreports the error via warnings.warn(), and 'ignore'.\n\"\"\"\n", "func_signal": "def walkdirs(self, pattern=None, errors='strict'):\n", "code": "if errors not in ('strict', 'warn', 'ignore'):\n    raise ValueError(\"invalid errors parameter\")\n\ntry:\n    dirs = self.dirs()\nexcept Exception:\n    if errors == 'ignore':\n        return\n    elif errors == 'warn':\n        warnings.warn(\n            \"Unable to list directory '%s': %s\"\n            % (self, sys.exc_info()[1]),\n            TreeWalkWarning)\n        return\n    else:\n        raise\n\nfor child in dirs:\n    if pattern is None or child.fnmatch(pattern):\n        yield child\n    for subsubdir in child.walkdirs(pattern, errors):\n        yield subsubdir", "path": "mongosearch\\path.py", "repo_name": "glamkit/python-mongo-search", "stars": 9, "license": "other", "language": "python", "size": 366}
{"docstring": "\"\"\" p.splitdrive() -> Return (p.drive, <the rest of p>).\n\nSplit the drive specifier from this path.  If there is\nno drive specifier, p.drive is empty, so the return value\nis simply (path(''), p).  This is always the case on Unix.\n\"\"\"\n", "func_signal": "def splitdrive(self):\n", "code": "drive, rel = os.path.splitdrive(self)\nreturn self.__class__(drive), rel", "path": "mongosearch\\path.py", "repo_name": "glamkit/python-mongo-search", "stars": 9, "license": "other", "language": "python", "size": 366}
{"docstring": "\"\"\"\nwe hack the normal build to force the git submodule to be updated.\n\"\"\"\n", "func_signal": "def run(self, *args, **kwargs):\n", "code": "self.ensure_submodules()\nsuper(build_py, self).run(*args, **kwargs)", "path": "pavement.py", "repo_name": "glamkit/python-mongo-search", "stars": 9, "license": "other", "language": "python", "size": 366}
{"docstring": "\"\"\"\nRe-implmentation of JS function search.mapReduceRawSearch\n\"\"\"\n#TODO: add in a spec param here - we may as well pre-filter this search too\n# this means we can also then sort the output and just use relevant ones later\n# when we have to do limit etc.\n", "func_signal": "def raw_search(collection, search_query):\n", "code": "search_query_terms = process_query_string(search_query)\nindex_name = DEFAULT_INDEX_NAME # assuem this for now -this is a legacy interface we can sdelete soon.\nmap_js = Code(\"function() { mft.get('search')._rawSearchMap.call(this) }\")\nreduce_js = Code(\"function(k, v) { return mft.get('search')._rawSearchReduce(k, v) }\")\nscope =  {'search_terms': search_query_terms, 'coll_name': collection.name, 'index_name': index_name}\n#   lazily assuming \"$all\" (i.e. AND search) \nquery_obj = {'value._extracted_terms': {'$all': search_query_terms}}\ndb = collection.database\nres = db[index_coll_name(collection, index_name)].map_reduce(\n  map_js, reduce_js, scope=scope, query=query_obj)\nres.ensure_index([('value.score', pymongo.ASCENDING)]) # can't demand backgrounding in python seemingly?\n# should we be returning a verbose result, or just the collection here?\nreturn res", "path": "mongosearch\\mongo_search.py", "repo_name": "glamkit/python-mongo-search", "stars": 9, "license": "other", "language": "python", "size": 366}
{"docstring": "\"\"\" D.listdir() -> List of items in this directory.\n\nUse D.files() or D.dirs() instead if you want a listing\nof just files or just subdirectories.\n\nThe elements of the list are path objects.\n\nWith the optional 'pattern' argument, this only lists\nitems whose names match the given pattern.\n\"\"\"\n", "func_signal": "def listdir(self, pattern=None):\n", "code": "names = os.listdir(self)\nif pattern is not None:\n    names = fnmatch.filter(names, pattern)\nreturn [self / child for child in names]", "path": "mongosearch\\path.py", "repo_name": "glamkit/python-mongo-search", "stars": 9, "license": "other", "language": "python", "size": 366}
{"docstring": "\"\"\" Return a relative path from self to dest.\n\nIf there is no relative path from self to dest, for example if\nthey reside on different drives in Windows, then this returns\ndest.abspath().\n\"\"\"\n", "func_signal": "def relpathto(self, dest):\n", "code": "origin = self.abspath()\ndest = self.__class__(dest).abspath()\n\norig_list = origin.normcase().splitall()\n# Don't normcase dest!  We want to preserve the case.\ndest_list = dest.splitall()\n\nif orig_list[0] != os.path.normcase(dest_list[0]):\n    # Can't get here from there.\n    return dest\n\n# Find the location where the two paths start to differ.\ni = 0\nfor start_seg, dest_seg in zip(orig_list, dest_list):\n    if start_seg != os.path.normcase(dest_seg):\n        break\n    i += 1\n\n# Now i is the point where the two paths diverge.\n# Need a certain number of \"os.pardir\"s to work up\n# from the origin to the point of divergence.\nsegments = [os.pardir] * (len(orig_list) - i)\n# Need to add the diverging part of dest_list.\nsegments += dest_list[i:]\nif len(segments) == 0:\n    # If they happen to be identical, use os.curdir.\n    relpath = os.curdir\nelse:\n    relpath = os.path.join(*segments)\nreturn self.__class__(relpath)", "path": "mongosearch\\path.py", "repo_name": "glamkit/python-mongo-search", "stars": 9, "license": "other", "language": "python", "size": 366}
{"docstring": "\"\"\"\nEvaluates the kernel at a set of location pairs\n\nParameters\n----------\nx1, x2 : array\n\tthese are locations at which to evaluate the kernel\n\t\nNotes\n----------\nThe inputs are n x d arrays where n is the number of locations and d\nis the dimension of the space. Therefore, the second dimension of \nthese arrays has to match!\n\"\"\"\n", "func_signal": "def __call__(self,x1,x2):\n", "code": "if len(x1.shape)>1:\n\tN1,D1 = x1.shape\n\tN2,D2 = x2.shape\nelse:\n\tN1,D1 = x1.shape[0],1\n\tN2,D2 = x2.shape[0],1\n\t\nassert D1==D2, \"Vectors must be of matching dimension\"\n# use broadcasting to avoid for loops. should be uber fast\ndiff = x1.reshape(N1,1,D1) - x2.reshape(1,N2,D2)\n# evaluate the kernel at each point\nK = self.alpha*np.exp(-np.sum(np.square(diff),-1)*self.gamma)\nreturn K", "path": "src\\pyGP\\kernels\\univariate_RBF.py", "repo_name": "jameshensman/pyGP", "stars": 8, "license": "gpl-3.0", "language": "python", "size": 136}
{"docstring": "\"\"\"\nMake sure Y is valid - should be +/- 1.\n\nParameters\n----------\nnewY :\n\"\"\"\n", "func_signal": "def setY(self,newY):\n", "code": "N,Ydim = newY.shape\nassert Ydim ==1, \"Binary classification only with this class (Y.shape is wrong).\"\nassert N == self.N, \"bad shape\"\nself.Y = newY.copy()", "path": "src\\pyGP\\classification\\binary_laplace.py", "repo_name": "jameshensman/pyGP", "stars": 8, "license": "gpl-3.0", "language": "python", "size": 136}
{"docstring": "\"\"\"d^2/df^2 log p(y|f)\"\"\"\n", "func_signal": "def log_gradient2(self,y,f):\n", "code": "pi = self(np.ones(f.shape),f)\nreturn -pi*(1-pi)", "path": "src\\pyGP\\classification\\binary_laplace.py", "repo_name": "jameshensman/pyGP", "stars": 8, "license": "gpl-3.0", "language": "python", "size": 136}
{"docstring": "\"\"\"Make a prediction upon new data points\"\"\"\n", "func_signal": "def predict(self,x_star):\n", "code": "x_star = (np.asarray(x_star)-self.xmean)/self.xstd\n\n#Kernel matrix k(X_*,X)\nk_x_star_x = self.kernel(x_star,self.X) \nk_x_star_x_star = self.kernel(x_star,x_star) \n\n#find the means and covs of the projection...\nmeans = np.dot(k_x_star_x, self.A)\nmeans *= self.ystd\nmeans += self.ymean\n\nv = np.linalg.solve(self.L,k_x_star_x.T)\nvariances = (\n\tnp.diag( \n\t\tk_x_star_x_star - np.dot(v.T,v)\n\t).reshape(x_star.shape[0],1) + 1./self.beta\n) * self.ystd.reshape(1,self.Ydim)\nreturn means,variances", "path": "src\\pyGP\\regression\\basic_regression.py", "repo_name": "jameshensman/pyGP", "stars": 8, "license": "gpl-3.0", "language": "python", "size": 136}
{"docstring": "\"\"\"do the matrix manipulation required in order to calculate\ngradients\"\"\"\n", "func_signal": "def update_grad(self):\n", "code": "self.Kinv = np.linalg.solve(\n\tself.L.T,\n\tnp.linalg.solve(self.L,np.eye(self.L.shape[0]))\n)\nself.alphalphK = np.dot(self.A,self.A.T)-self.Ydim*self.Kinv", "path": "src\\pyGP\\regression\\basic_regression.py", "repo_name": "jameshensman/pyGP", "stars": 8, "license": "gpl-3.0", "language": "python", "size": 136}
{"docstring": "\"\"\"A cost function to optimise for setting the kernel parameters. \nUses current parameter values if none are passed \"\"\"\n", "func_signal": "def ll(self,params=None):\n", "code": "if not params == None:\n\tself.set_params(params)\ntry:\n\tself.update()\nexcept:\n\treturn np.inf\nreturn -self.marginal() - self.hyper_prior()", "path": "src\\pyGP\\regression\\basic_regression.py", "repo_name": "jameshensman/pyGP", "stars": 8, "license": "gpl-3.0", "language": "python", "size": 136}
{"docstring": "\"\"\"\nzero means and normalises Y\n\nParameters\n----------\nnewY :\n\"\"\"\n", "func_signal": "def setY(self,newY):\n", "code": "self.Y = newY.copy()\nN,self.Ydim = newY.shape\nassert N == self.N, \"bad shape\"\n#normalise...\nself.ymean = self.Y.mean(0)\nself.ystd = self.Y.std(0)\nself.Y -= self.ymean\nself.Y /= self.ystd", "path": "src\\pyGP\\regression\\basic_regression.py", "repo_name": "jameshensman/pyGP", "stars": 8, "license": "gpl-3.0", "language": "python", "size": 136}
{"docstring": "\"\"\" the gradient of the ll function, for use with conjugate gradient \noptimisation. uses current values of parameters if none are passed \"\"\"\n", "func_signal": "def ll_grad(self,params=None):\n", "code": "if not params == None:\n\tself.set_params(params)\ntry:\n\tself.update()\nexcept:\n\treturn np.ones(params.shape)*np.NaN\nself.update_grad()\nmatrix_grads = [e for e in self.kernel.gradients(self.X)]\n#noise gradient matrix\nmatrix_grads.append(-np.eye(self.K.shape[0])/self.beta) \n\ngrads = [0.5*np.trace(np.dot(self.alphalphK,e)) for e in matrix_grads]\n\t\nreturn -np.array(grads) - self.hyper_prior_grad()", "path": "src\\pyGP\\regression\\basic_regression.py", "repo_name": "jameshensman/pyGP", "stars": 8, "license": "gpl-3.0", "language": "python", "size": 136}
{"docstring": "\"\"\"return the log of the current hyper paramters under their prior\"\"\"\n", "func_signal": "def hyper_prior(self):\n", "code": "return  self.prior_const - 0.5*np.dot(\n\tself.parameter_prior_widths,\n\tnp.square(self.get_params())\n)", "path": "src\\pyGP\\regression\\basic_regression.py", "repo_name": "jameshensman/pyGP", "stars": 8, "license": "gpl-3.0", "language": "python", "size": 136}
{"docstring": "\"\"\"d/df log p(y|f)\"\"\"\n", "func_signal": "def log_gradient(self,y,f):\n", "code": "t = (y+1)/2.\npi = self(np.ones(f.shape),f)\nreturn t - pi", "path": "src\\pyGP\\classification\\binary_laplace.py", "repo_name": "jameshensman/pyGP", "stars": 8, "license": "gpl-3.0", "language": "python", "size": 136}
{"docstring": "\"\"\"The Marginal Likelihood. Useful for optimising Kernel parameters\"\"\"\n", "func_signal": "def marginal(self):\n", "code": "return -self.Ydim*np.sum(np.log(np.diag(self.L)))\\\n \t- 0.5*np.trace(np.dot(self.Y.T,self.A)) - self.n2ln2pi", "path": "src\\pyGP\\regression\\basic_regression.py", "repo_name": "jameshensman/pyGP", "stars": 8, "license": "gpl-3.0", "language": "python", "size": 136}
{"docstring": "\"\"\"Calculate the gradient of the kernel matrix wrt the (log of the) parameters\"\"\"\n", "func_signal": "def gradients(self,x1):\n", "code": "dalpha = self(x1,x1)-self.bias\ndbias = np.ones((x1.shape[0],x1.shape[0]))*self.bias\nreturn dalpha, dbias", "path": "src\\pyGP\\kernels\\linear.py", "repo_name": "jameshensman/pyGP", "stars": 8, "license": "gpl-3.0", "language": "python", "size": 136}
{"docstring": "\"\"\"Calculate the gradient of the matrix K wrt the (log of the) free \nparameters\"\"\"\n", "func_signal": "def gradients(self,x1):\n", "code": "N1,D1 = x1.shape\ndiff = x1.reshape(N1,1,D1)-x1.reshape(1,N1,D1)\ndiff = np.sum(np.square(diff),-1)\ndalpha = self.alpha*np.exp(-diff*self.gamma)\ndgamma = -self.alpha*self.gamma*diff*np.exp(-diff*self.gamma)\nreturn np.array([dalpha, dgamma])", "path": "src\\pyGP\\kernels\\univariate_RBF.py", "repo_name": "jameshensman/pyGP", "stars": 8, "license": "gpl-3.0", "language": "python", "size": 136}
{"docstring": "\"\"\"compute the derivative matrix of the kernel wrt the _data_. This \nreturns a list of matrices: each matrix is NxN, and there are N*D \nof them.\"\"\"\n", "func_signal": "def gradients_wrt_data(self,x1,indexn=None,indexd=None):\n", "code": "N1,D1 = x1.shape\ndiff = x1.reshape(N1,1,D1)-x1.reshape(1,N1,D1)\ndiff = np.sum(np.square(diff),-1)\nexpdiff = np.exp(-self.gamma*diff)\n\nif (indexn==None) and(indexd==None):#calculate all gradients\n\trets = []\n\tfor n in range(N1):\n\t\tfor d in range(D1):\n\t\t\tK = np.zeros((N1,N1))\n\t\t\tK[n,:] = -2*self.alpha*self.gamma*(x1[n,d]-x1[:,d])*expdiff[n,:]\n\t\t\tK[:,n] = K[n,:]\n\t\t\trets.append(K.copy())\n\treturn rets\nelse:\n\tK = np.zeros((N1,N1))\n\tK[indexn,:] = -2*self.alpha*self.gamma*(x1[indexn,indexd]-x1[:,indexd])*expdiff[indexn,:]\n\tK[:,indexn] = K[indexn,:]\n\treturn", "path": "src\\pyGP\\kernels\\univariate_RBF.py", "repo_name": "jameshensman/pyGP", "stars": 8, "license": "gpl-3.0", "language": "python", "size": 136}
{"docstring": "\"\"\"do the Cholesky decomposition as required to make predictions and \ncalculate the marginal likelihood\n\"\"\"\n", "func_signal": "def update(self):\n", "code": "self.K = self.kernel(self.X,self.X)  \nself.K += np.eye(self.K.shape[0])/self.beta\nself.L = np.linalg.cholesky(self.K)\nself.A = linalg.cho_solve((self.L,1),self.Y)", "path": "src\\pyGP\\regression\\basic_regression.py", "repo_name": "jameshensman/pyGP", "stars": 8, "license": "gpl-3.0", "language": "python", "size": 136}
{"docstring": "\"\"\"Make a prediction for a test case. Just uses the sigmoid of the MAP estimate of F. Fine for doing 'most probable' classification.\"\"\"\n", "func_signal": "def predict_MAP(self,Xtest):\n", "code": "Xtest = (Xtest - self.xmean)/self.xstd\nKtest_train = self.kernel(Xtest,self.X)\n#Ktest_test = self.kernel(Xtest,Xtest)\nFtest = np.dot(Ktest_train,np.dot(self.Kinv,self.f_hat))\nreturn self.sigmoid(np.ones(Ftest.shape),Ftest)", "path": "src\\pyGP\\classification\\binary_laplace.py", "repo_name": "jameshensman/pyGP", "stars": 8, "license": "gpl-3.0", "language": "python", "size": 136}
{"docstring": "\"\"\"\nParameters\n----------\nX : array (None)\n\tinput\nY : array (None)\n\tobservations\nkernel : Kernel object (None)\n\tThe covariance function\nparameter_prior : (None)\n\t\n\nNotes\n----------\nIf you don't supply a kernel then a multivariate squared exponential \nkernel will be generated, with the appropriate dimension taken from X.\nSo if you want to use the default kernel, then you must supply some\ninput/observation data.\n\nSee Also\n----------\npyGP.kernels : for a selection of covariance functions\n\"\"\"\n", "func_signal": "def __init__(self, X=None, Y=None, kernel=None, parameter_priors=None, prior_mean=None, sigmoid=None):\n", "code": "if (X is not None) and (Y is not None):\n\tself.N = Y.shape[0]\n\tself.setY(Y)\n\tself.setX(X)\n\t\nif kernel==None:\n\tself.kernel = kernels.full_RBF(-1,-np.ones(self.Xdim))\nelse:\n\tself.kernel = kernel\nif parameter_priors==None:\n\tself.parameter_prior_widths = np.ones(self.kernel.nparams)\nelse:\n\tassert parameter_priors.size==(self.kernel.nparams)\n\tself.parameter_prior_widths = np.array(parameter_priors).flatten()\n\t\nif prior_mean is None:\n\tself.prior_mean = lambda X: np.zeros(len(X))\nelse:\n\tself.prior_mean = prior_mean\n\t\nif (X is not None) and (Y is not None):\n\tself.update()\n\t# constant in the marginal. precompute for convenience. \n\tself.n2ln2pi = 0.5*self.N*np.log(2*np.pi) \n\t\n#constant in the kernel parameter prior. precompute for convenience\nself.prior_const = -0.5*(self.kernel.nparams+1)*np.log(2*np.pi) - 0.5*np.log(np.prod(self.parameter_prior_widths))\n\n#default to a logistic sigmoid\nself.sigmoid = sigmoid or logistic()\n\n#initialise fhat to something sensible\nself.f_hat = self.Y.copy()", "path": "src\\pyGP\\classification\\binary_laplace.py", "repo_name": "jameshensman/pyGP", "stars": 8, "license": "gpl-3.0", "language": "python", "size": 136}
{"docstring": "\"\"\"return the log of the marginal likelihood (see R&W page 48)\"\"\"\n", "func_signal": "def marginal(self):\n", "code": "Wsqrt = np.diag(np.sqrt(-self.sigmoid.log_gradient2(self.Y,self.f_hat)[:,0]))\nreturn -0.5*np.dot(self.f_hat.T,np.dot(self.Kinv,self.f_hat))\\\n\t+np.sum(self.sigmoid.log(self.Y,self.f_hat)) \\\n\t-0.5*np.log(np.linalg.det(np.eye(self.N) + np.dot(Wsqrt,np.dot(self.K,Wsqrt)) ))", "path": "src\\pyGP\\classification\\binary_laplace.py", "repo_name": "jameshensman/pyGP", "stars": 8, "license": "gpl-3.0", "language": "python", "size": 136}
{"docstring": "\"\"\"\nzero means and normalises X\n\nParameters\n----------\nnewX :\n\"\"\"\n", "func_signal": "def setX(self,newX):\n", "code": "self.X = newX.copy()\nN,self.Xdim = newX.shape\nassert N == self.N, \"bad shape\"\n# zero mean and normalise...\nself.xmean = self.X.mean(0)\nself.xstd = self.X.std(0)\nself.X -= self.xmean\nself.X /= self.xstd", "path": "src\\pyGP\\classification\\binary_laplace.py", "repo_name": "jameshensman/pyGP", "stars": 8, "license": "gpl-3.0", "language": "python", "size": 136}
{"docstring": "\"\"\" set the kernel parameters and the noise parameter beta\"\"\"\n", "func_signal": "def set_params(self,params):\n", "code": "assert params.size==self.kernel.nparams+1\nself.beta = np.exp(params[-1]).real\nself.kernel.set_params(params[:-1])", "path": "src\\pyGP\\regression\\basic_regression.py", "repo_name": "jameshensman/pyGP", "stars": 8, "license": "gpl-3.0", "language": "python", "size": 136}
{"docstring": "\"\"\"Limit generator *item count* between min and max.\n\n:param generator: Generator\n:type generator: generator\n\n:param min: Minimum amount of items in generator.\n:type min: int, or None\n\n:param max: Maximum amount of items.\n:type max: int, or None\n\n:note: If both are ``None`` this returns the same generator.\n:raise ValueError: Raised when minimum is not met.\n\n\"\"\"\n", "func_signal": "def genlimit(generator, min, max):\n", "code": "if (min is None) and (max is None):\n    return generator\n\nif min is not None:\n    generator = genmin(generator, min)\n\nif max is not None:\n    generator = genmax(generator, max)\n\nreturn generator", "path": "src\\mpeg1audio\\utils.py", "repo_name": "Ciantic/mpeg1audio", "stars": 11, "license": "other", "language": "python", "size": 330}
{"docstring": "\"\"\"Get layer from MPEG Header bits.\n\n:param bits: Two layer bits in MPEG header.\n:type bits: int\n\n:return: MPEG Layer, one of the following values: ``'1', '2', '3'``.\n:rtype: string\n\n:raise mpeg1audio.MPEGAudioHeaderException: Raised when layer cannot be\n    determined.\n\n\"\"\"\n\n\n", "func_signal": "def get_layer(bits):\n", "code": "try:\n    return LAYERS[bits]\nexcept (KeyError, IndexError):\n    raise MPEGAudioHeaderException('Unknown Layer version')", "path": "src\\mpeg1audio\\headers.py", "repo_name": "Ciantic/mpeg1audio", "stars": 11, "license": "other", "language": "python", "size": 330}
{"docstring": "\"\"\"Joins list and generator.\n\n:param iterable1: List to be appended.\n:type iterable1: Generator\n\n:param iterable2: Generator to be appended.\n:type iterable2: generator\n\n:return: Generator yielding first iterable1, and then following iterable2.\n:rtype: generator\n\n\"\"\"\n", "func_signal": "def join_iterators(iterable1, iterable2):\n", "code": "for item1 in iterable1:\n    yield item1\n\nfor item2 in iterable2:\n    yield item2", "path": "src\\mpeg1audio\\utils.py", "repo_name": "Ciantic/mpeg1audio", "stars": 11, "license": "other", "language": "python", "size": 330}
{"docstring": "\"\"\" Get bitrate from given header data.\n\n:param mpeg_version: Version of the MPEG, as returned by\n    :func:`get_mpeg_version`\n:type mpeg_version: string\n\n:param layer: Layer of the MPEG as returned by :func:`get_layer`.\n:type layer: string\n\n:param bitrate_bits: Four bitrate related bits in MPEG header.\n:type bitrate_bits: int\n\n:return: Bitrate in *kilobits* per second.\n:rtype: int\n\n:raise mpeg1audio.MPEGAudioHeaderException: Raised when bitrate cannot be\n    determined.\n\n\"\"\"\n\n# TODO: LOW: Free bitrate\n", "func_signal": "def get_bitrate(mpeg_version, layer, bitrate_bits):\n", "code": "if bitrate_bits == 0:\n    raise MPEGAudioHeaderException(\n                    'Free bitrate is not implemented, sorry.')\n\ntry:\n    return BITRATE[mpeg_version][layer][bitrate_bits]\nexcept (KeyError, IndexError):\n    raise MPEGAudioHeaderException('Bitrate cannot be determined.')", "path": "src\\mpeg1audio\\headers.py", "repo_name": "Ciantic/mpeg1audio", "stars": 11, "license": "other", "language": "python", "size": 330}
{"docstring": "\"\"\"Calculate duration from constant bitrate and MPEG Size.\n\n:param mpeg_size: MPEG Size in bytes.\n:type mpeg_size: int\n\n:param bitrate: Bitrate in kilobits per second, for example 192.\n:type bitrate: int\n\n:raise mpeg1audio.MPEGAudioHeaderException: Raised if duration cannot be \n    determined.\n\n:return: Duration of the MPEG, with second accuracy.\n:rtype: datetime.timedelta\n\n\"\"\"\n", "func_signal": "def get_duration_from_size_bitrate(mpeg_size, bitrate):\n", "code": "try:\n    return timedelta(seconds=(mpeg_size / (bitrate * 1000) * 8))\nexcept ZeroDivisionError:\n    raise MPEGAudioHeaderException('Duration cannot be determined.')", "path": "src\\mpeg1audio\\headers.py", "repo_name": "Ciantic/mpeg1audio", "stars": 11, "license": "other", "language": "python", "size": 330}
{"docstring": "\"\"\"Ensures that generator has min amount of items left.\n\n    >>> def yrange(n): # Note that xrange doesn't work, requires next()\n    ...     for i in range(n):\n    ...         yield i\n    ... \n    >>> genmin(yrange(5), min=4) #doctest: +ELLIPSIS\n    <generator object join_iterators at ...>\n    >>> genmin(yrange(5), min=5) #doctest: +ELLIPSIS\n    <generator object join_iterators at ...>\n    >>> genmin(yrange(5), min=6)\n    Traceback (most recent call last):\n      ...\n    ValueError: Minimum amount not met.\n    >>> \n    \n:param generator: Generator to be ensured.\n:type generator: generator\n\n:param min: Minimum amount of items in generator.\n:type min: int\n\n:raise ValueError: Raised when minimum is not met.\n\n\"\"\"\n", "func_signal": "def genmin(generator, min):\n", "code": "cache = []\nfor index in range(min): #@UnusedVariable\n    try:\n        cache.append(generator.next())\n    except StopIteration:\n        raise ValueError('Minimum amount not met.')\n\nreturn join_iterators(cache, generator)", "path": "src\\mpeg1audio\\utils.py", "repo_name": "Ciantic/mpeg1audio", "stars": 11, "license": "other", "language": "python", "size": 330}
{"docstring": "\"\"\"Check if given bits has sync bits.\n\n:param bits: bits to check for sync bits.\n:type bits: int\n\n:raise mpeg1audio.MPEGAudioHeaderException: Raised if bits does not contain\n    sync bits.\n\n\"\"\"\n", "func_signal": "def check_sync_bits(bits):\n", "code": "if (bits & 2047) != 2047:\n    raise MPEGAudioHeaderException('Sync bits does not match.')", "path": "src\\mpeg1audio\\headers.py", "repo_name": "Ciantic/mpeg1audio", "stars": 11, "license": "other", "language": "python", "size": 330}
{"docstring": "\"\"\"Get average bitrate of VBR file.\n\n:param mpeg_size: Size of MPEG in bytes.\n:type mpeg_size: number\n\n:param sample_count: Count of samples.\n:type sample_count: number\n\n:param sample_rate: Sample rate in Hz.\n:type sample_rate: number\n\n:return: Average bitrate in kilobits per second.\n:rtype: float\n\n\"\"\"\n", "func_signal": "def get_vbr_bitrate(mpeg_size, sample_count, sample_rate):\n", "code": "bytes_per_sample = float(mpeg_size) / float(sample_count)\nbytes_per_second = bytes_per_sample * float(sample_rate)\nbits_per_second = bytes_per_second * 8\nreturn bits_per_second / 1000", "path": "src\\mpeg1audio\\headers.py", "repo_name": "Ciantic/mpeg1audio", "stars": 11, "license": "other", "language": "python", "size": 330}
{"docstring": "\"\"\"Get samples per frame.\n\n:param mpeg_version: Version of the mpeg, as returned by \n    :func:`get_mpeg_version`\n:type mpeg_version: string\n\n:param layer: Layer of the MPEG as returned by :func:`get_layer`.\n:type layer: string\n\n:rtype: int\n:return: Samples per frame.\n\n:raise mpeg1audio.MPEGAudioHeaderException: Raised if samples per frame\n    cannot be determined.\n\n\"\"\"\n", "func_signal": "def get_samples_per_frame(mpeg_version, layer):\n", "code": "try:\n    return SAMPLES_PER_FRAME[mpeg_version][layer]\nexcept (IndexError):\n    raise MPEGAudioHeaderException(\n                        'Samples per frame cannot be determined.')", "path": "src\\mpeg1audio\\headers.py", "repo_name": "Ciantic/mpeg1audio", "stars": 11, "license": "other", "language": "python", "size": 330}
{"docstring": "\"\"\"Get emphasis of audio.\n\n:param bits: Emphasis bits in MPEG header.\n:type bits: int\n\n:return: Returns emphasis, one of the following: ``\"none\", \"50/15 ms\", \n    \"reserved\", \"CCIT J.17\"``\n:rtype: string \n\n:raise mpeg1audio.MPEGAudioHeaderException: Raised when emphasis cannot be\n    determined.\n\n\"\"\"\n\n\n", "func_signal": "def get_emphasis(bits):\n", "code": "try:\n    return EMPHASES[bits]\nexcept (TypeError, IndexError):\n    raise MPEGAudioHeaderException('Emphasis cannot be determined.')", "path": "src\\mpeg1audio\\headers.py", "repo_name": "Ciantic/mpeg1audio", "stars": 11, "license": "other", "language": "python", "size": 330}
{"docstring": "\"\"\"Get size.\n\n:param mpeg_version: Version of the MPEG, as returned by \n    :func:`get_mpeg_version`\n:type mpeg_version: string\n\n:param layer: Layer of the MPEG as returned by :func:`get_layer`.\n:type layer: string\n\n:param sample_rate: Sampling rate in Hz.\n:type sample_rate: int\n\n:param bitrate: Bitrate in kilobits per second.\n:type bitrate: int\n\n:param padding_size: Size of header padding. Always either ``1`` or ``0``.\n:type padding_size: int\n\n:return: Frame size in bytes.\n:rtype: int\n\n:raise mpeg1audio.MPEGAudioHeaderException: Raised when frame size cannot be \n    determined.\n\n\"\"\"\n", "func_signal": "def get_frame_size(mpeg_version, layer, sample_rate, bitrate, padding_size):\n", "code": "try:\n    coeff = SLOT_COEFFS[mpeg_version][layer]\n    slotsize = SLOTS[layer]\nexcept (IndexError, KeyError, TypeError):\n    raise MPEGAudioHeaderException('Frame size cannot be determined.')\n\nbitrate_k = bitrate * 1000\n\nframesize = int((coeff * bitrate_k / sample_rate) + padding_size) * slotsize\nif framesize <= 0:\n    raise MPEGAudioHeaderException('Frame size cannot be calculated.')\nreturn framesize", "path": "src\\mpeg1audio\\headers.py", "repo_name": "Ciantic/mpeg1audio", "stars": 11, "license": "other", "language": "python", "size": 330}
{"docstring": "\"\"\"Get channel mode.\n\n:param bits: Mode bits in MPEG header.\n:type bits: int\n\n:return: Returns one of the following: ``\"stereo\"``, ``\"joint stereo\"``, \n    ``\"dual channel\"``, ``\"mono\"``. \n:rtype: string\n\n:raise mpeg1audio.MPEGAudioHeaderException: Raised if channel mode cannot be \n    determined.\n\"\"\"\n\n\n", "func_signal": "def get_channel_mode(bits):\n", "code": "try:\n    return CHANNEL_MODES[bits]\nexcept (IndexError, TypeError):\n    raise MPEGAudioHeaderException(\n                        'Channel channel_mode cannot be determined.')", "path": "src\\mpeg1audio\\headers.py", "repo_name": "Ciantic/mpeg1audio", "stars": 11, "license": "other", "language": "python", "size": 330}
{"docstring": "\"\"\"Ensures that generator does not exceed given max when yielding.\n\nFor example when you have generator that goes to infinity, you might want to\ninstead only get 100 first instead.\n\n    >>> list(genmax(xrange(100), max=3))\n    [0, 1, 2]\n    \n:param generator: Generator\n:type generator: generator\n\n:param max: Maximum amount of items yields.\n:type max: int\n\n:rtype: generator\n:return: Generator limited by max.\n\n\"\"\"\n", "func_signal": "def genmax(generator, max):\n", "code": "for index, item in enumerate(generator):\n    yield item\n    if index + 1 >= max:\n        return", "path": "src\\mpeg1audio\\utils.py", "repo_name": "Ciantic/mpeg1audio", "stars": 11, "license": "other", "language": "python", "size": 330}
{"docstring": "\"\"\"\n:param message: Message of the exception.\n:type message: string\n\n:keyword mpeg_offset: Offset of the MPEG Frame in file.\n:type mpeg_offset: int \n\n:keyword bad_offset: Bad offset of the MPEG Frame in file.\n:type bad_offset: int\n\n\"\"\"\n", "func_signal": "def __init__(self, message, mpeg_offset=None, bad_offset=None):\n", "code": "super(MPEGAudioHeaderException, self).__init__(message)\n\nself.mpeg_offset = mpeg_offset\n\"\"\"MPEG Offset within file\n\n:type: int\"\"\"\n\nself.bad_offset = bad_offset\n\"\"\"Bad offset within file\n\n:type: int\"\"\"", "path": "src\\mpeg1audio\\headers.py", "repo_name": "Ciantic/mpeg1audio", "stars": 11, "license": "other", "language": "python", "size": 330}
{"docstring": "\"\"\"Find and parse VBRI header in MPEG File.\n\n:param file: File object.\n:type file: file object\n\n:param first_frame_offset: Offset of first mpeg frame in file.\n:type first_frame_offset: int\n\n:return: XING Header in given file.\n:rtype: :class:`XING`\n\n:raise VBRIHeaderException: Raised if VBRI Header cannot be \n    parsed or found.\n    \n\"\"\"\n", "func_signal": "def find_and_parse(cls, file, first_frame_offset):\n", "code": "file.seek(first_frame_offset)\nchunk_offset, chunk = file.tell(), file.read(1024)\n\nbeginning_of_vbri = 4 + 32 # Header 4 bytes, VBRI is in 32nd byte.\n\n# If positive match for VBRI\nif chunk[beginning_of_vbri:beginning_of_vbri + 4] == \"VBRI\":\n    self = VBRI()\n    self.offset = chunk_offset + beginning_of_vbri\n    self.size = 26\n\n    if len(chunk) < 24:\n        raise VBRIHeaderException('VBRI EOF')\n\n    fcur = beginning_of_vbri\n    fcur += 4 # Size of \"VBRI\"\n    entries_in_toc = 0 #@UnusedVariable\n    scale_factor_of_toc = 0 #@UnusedVariable\n    size_per_table = 0 #@UnusedVariable\n    frames_per_table = 0 #@UnusedVariable\n\n    (self.version, self.delay, self.quality, self.mpeg_size,\n     self.frame_count, entries_in_toc, #@UnusedVariable\n     scale_factor_of_toc, size_per_table, #@UnusedVariable\n     frames_per_table) = struct.unpack('>HHHIIHHHH', #@UnusedVariable \n                                       chunk[fcur:fcur + 22])\n\n    # TODO: TOC!\n\n    return self\n\nraise VBRIHeaderException('VBRI Header not found')", "path": "src\\mpeg1audio\\vbri.py", "repo_name": "Ciantic/mpeg1audio", "stars": 11, "license": "other", "language": "python", "size": 330}
{"docstring": "\"\"\"Get MPEG version from header bits.\n\n:param bits: Two version bits in MPEG header.\n:type bits: int\n\n:return: MPEG Version, one of the following values: ``\"2.5\", \"2\", \"1\"``. \n:rtype: string\n\n:todo: Ponder about the usefulness of this being string. Same with\n    :func:`get_layer`\n\n:raise mpeg1audio.MPEGAudioHeaderException: Raised when layer cannot be\n    determined.\n\n\"\"\"\n\n", "func_signal": "def get_mpeg_version(bits):\n", "code": "try:\n    return MPEG_VERSIONS[bits]\nexcept (KeyError, IndexError):\n    raise MPEGAudioHeaderException('Unknown MPEG version.')", "path": "src\\mpeg1audio\\headers.py", "repo_name": "Ciantic/mpeg1audio", "stars": 11, "license": "other", "language": "python", "size": 330}
{"docstring": "\"\"\"Find all overlapping occurrences.\n\n:param string: String to be searched.\n:type string: string\n\n:param occurrence: Occurrence to search.\n:type occurrence: string\n\n:return: generator yielding *positions of occurence*\n:rtype: generator of int\n\n\"\"\"\n", "func_signal": "def find_all_overlapping(string, occurrence):\n", "code": "found = 0\n\nwhile True:\n    found = string.find(occurrence, found)\n    if found != -1:\n        yield found\n    else:\n        return\n\n    found += 1", "path": "src\\mpeg1audio\\utils.py", "repo_name": "Ciantic/mpeg1audio", "stars": 11, "license": "other", "language": "python", "size": 330}
{"docstring": "\"\"\"Unpacks MPEG Frame header bytes from chunk of data.\n\nValue can then be used to parse and verify the bits.\n    \n:param header_offset: Position *within a chunk* where to look for header \n    bytes.\n:type header_offset: int\n\n:param chunk: Chunk of data where to get header bytes.\n:type chunk: string\n\n:return: Header bytes. Used by :func:`MPEGAudioFrame.parse`.\n:rtype: int\n\n:raise mpeg1audio.MPEGAudioHeaderEOFException: Raised when end of chunk was \n    reached.\n    \n:see: :func:`MPEGAudioFrame.parse`\n:see: :func:`MPEGAudioFrame.find_and_parse`\n\n\"\"\"\n# Get first four bytes\n", "func_signal": "def get_bytes(header_offset, chunk):\n", "code": "header = chunk[header_offset:header_offset + 4]\nif len(header) != 4:\n    raise MPEGAudioHeaderEOFException(\n                            'End of chunk reached, header not found.')\n\n# Unpack 4 bytes (the header size)\n(header_bytes,) = struct.unpack(\">I\", header)\nreturn header_bytes", "path": "src\\mpeg1audio\\headers.py", "repo_name": "Ciantic/mpeg1audio", "stars": 11, "license": "other", "language": "python", "size": 330}
{"docstring": "\"\"\"Get sample rate by MPEG version and given MPEG Header sample rate bits.\n\n:param mpeg_version: Version of the MPEG, as returned by \n    :func:`get_mpeg_version`\n:type mpeg_version: string\n\n:param bits: Sample rate bits in MPEG header.\n:type bits: int\n\n:return: Sample rate in Hz\n:rtype: int\n\n:raise mpeg1audio.MPEGAudioHeaderException: Raised when sample rate cannot\n    be determined.\n\n\"\"\"\n\n", "func_signal": "def get_sample_rate(mpeg_version, bits):\n", "code": "try:\n    return SAMPLERATE[mpeg_version][bits]\nexcept (KeyError, TypeError, IndexError):\n    raise MPEGAudioHeaderException('Sample rate cannot be determined.')", "path": "src\\mpeg1audio\\headers.py", "repo_name": "Ciantic/mpeg1audio", "stars": 11, "license": "other", "language": "python", "size": 330}
{"docstring": "\"\"\"Get channel mode extension.\n\n:param layer: Layer of the MPEG as returned by \n    :func:`get_layer`.\n:type layer: string\n\n:param bits: Extension mode bits in MPEG header.\n:type bits: int\n\n:rtype: string \n:return: Channel extension mode. One of the following values: ``\"4-31\", \n    \"8-31\", \"12-31\", \"16-31\", \"\", \"IS\", \"MS\", \"IS+MS\"``\n   \n:raise mpeg1audio.MPEGAudioHeaderException: Raised if channel mode extension\n    cannot be determined.\n    \n\"\"\"\n\n", "func_signal": "def get_channel_mode_ext(layer, bits):\n", "code": "try:\n    return CHANNEL_MODE_EXT[layer][bits]\nexcept (KeyError, TypeError, IndexError):\n    raise MPEGAudioHeaderException(\n                            'Channel mode ext. cannot be determined.')", "path": "src\\mpeg1audio\\headers.py", "repo_name": "Ciantic/mpeg1audio", "stars": 11, "license": "other", "language": "python", "size": 330}
{"docstring": "\"\"\"Given a filename, read a file in binary mode. It returns a single string.\"\"\"\n", "func_signal": "def readbinary(filename):\n", "code": "filehandle = open(filename, 'rb')\nthisfile = filehandle.read()\nfilehandle.close()\nreturn thisfile", "path": "pathutils.py", "repo_name": "dabrahams/s3backup", "stars": 9, "license": "None", "language": "python", "size": 92}
{"docstring": "\"\"\"\nReturn a list of the path components in loc. (Used by relpath_).\n\nThe first item in the list will be  either ``os.curdir``, ``os.pardir``, empty,\nor the root directory of loc (for example, ``/`` or ``C:\\\\).\n\nThe other items in the list will be strings.\n    \nAdapted from *path.py* by Jason Orendorff.\n\"\"\"\n", "func_signal": "def splitall(loc):\n", "code": "parts = []\nwhile loc != os.curdir and loc != os.pardir:\n    prev = loc\n    loc, child = os.path.split(prev)\n    if loc == prev:\n        break\n    parts.append(child)\nparts.append(loc)\nparts.reverse()\nreturn parts", "path": "pathutils.py", "repo_name": "dabrahams/s3backup", "stars": 9, "license": "None", "language": "python", "size": 92}
{"docstring": "\"\"\"\nGiven an integer (probably a long integer returned by os.getsize() )\nit returns a tuple of (megabytes, kilobytes, bytes).\n\nThis can be more easily converted into a formatted string to display the\nsize of the file.\n\"\"\"\n", "func_signal": "def bytedivider(nbytes):\n", "code": "mb, remainder = divmod(nbytes, 1048576)\nkb, rb = divmod(remainder, 1024)\nreturn (mb, kb, rb)", "path": "pathutils.py", "repo_name": "dabrahams/s3backup", "stars": 9, "license": "None", "language": "python", "size": 92}
{"docstring": "\"\"\"Passed a filename, it reads it, and returns a list of lines. (Read in text mode)\"\"\"\n", "func_signal": "def readlines(filename):\n", "code": "filehandle = open(filename, 'r')\noutfile = filehandle.readlines()\nfilehandle.close()\nreturn outfile", "path": "pathutils.py", "repo_name": "dabrahams/s3backup", "stars": 9, "license": "None", "language": "python", "size": 92}
{"docstring": "\"\"\"delegate appropriate method/attribute calls to the file.\"\"\"\n", "func_signal": "def __getattr__(self, name):\n", "code": "if name not in self.__dict__:\n    return getattr(self._file, name)\nelse:\n    return self.__dict__[self, name]", "path": "pathutils.py", "repo_name": "dabrahams/s3backup", "stars": 9, "license": "None", "language": "python", "size": 92}
{"docstring": "\"\"\"Auto unlock when object is deleted.\"\"\"\n", "func_signal": "def __del__(self):\n", "code": "if self.locked:\n    self.unlock()", "path": "pathutils.py", "repo_name": "dabrahams/s3backup", "stars": 9, "license": "None", "language": "python", "size": 92}
{"docstring": "\"\"\"Auto unlock (and close file) when object is deleted.\"\"\"\n", "func_signal": "def __del__(self):\n", "code": "if self.locked:\n    self.unlock()\n    self._file.close()", "path": "pathutils.py", "repo_name": "dabrahams/s3backup", "stars": 9, "license": "None", "language": "python", "size": 92}
{"docstring": "\"\"\"\nCreate a ``Lock`` object on file ``filename``\n\n``timeout`` is the time in seconds to wait before timing out, when\nattempting to acquire the lock.\n\n``step`` is the number of seconds to wait in between each attempt to\nacquire the lock.\n\n\"\"\"\n", "func_signal": "def __init__(self, filename, timeout=5, step=0.1):\n", "code": "self.timeout = timeout\nself.step = step\nself.filename = filename\nself.locked = False", "path": "pathutils.py", "repo_name": "dabrahams/s3backup", "stars": 9, "license": "None", "language": "python", "size": 92}
{"docstring": "\"\"\"Given a filename, read a file in text mode. It returns a single string.\"\"\"\n", "func_signal": "def readfile(filename):\n", "code": "filehandle = open(filename, 'r')\noutfile = filehandle.read()\nfilehandle.close()\nreturn outfile", "path": "pathutils.py", "repo_name": "dabrahams/s3backup", "stars": 9, "license": "None", "language": "python", "size": 92}
{"docstring": "\"\"\"\nCopy file from src to dst.\n\nIf the dst directory doesn't exist, we will attempt to create it using makedirs.\n\"\"\"\n", "func_signal": "def fullcopy(src, dst):\n", "code": "import shutil\nif not os.path.isdir(os.path.dirname(dst)):\n    os.makedirs(os.path.dirname(dst))\nshutil.copy(src, dst)", "path": "pathutils.py", "repo_name": "dabrahams/s3backup", "stars": 9, "license": "None", "language": "python", "size": 92}
{"docstring": "\"\"\"Only allow attribute setting that don't clash with the file.\"\"\"\n", "func_signal": "def __setattr__(self, name, value):\n", "code": "if not '_file' in self.__dict__:\n    Lock.__setattr__(self, name, value)\nelif hasattr(self._file, name):\n    return setattr(self._file, name, value)\nelse:\n    Lock.__setattr__(self, name, value)", "path": "pathutils.py", "repo_name": "dabrahams/s3backup", "stars": 9, "license": "None", "language": "python", "size": 92}
{"docstring": "\"\"\"\nwalkfiles(D) -> iterator over files in D, recursively. Yields full file paths.\n\nAdapted from path.py by Jason Orendorff.\n\"\"\"\n", "func_signal": "def walkfiles(thisdir):\n", "code": "for child in os.listdir(thisdir):\n    thischild = join(thisdir, child)\n    if isfile(thischild):\n        yield thischild\n    elif isdir(thischild):\n        for f in walkfiles(thischild):\n            yield f", "path": "pathutils.py", "repo_name": "dabrahams/s3backup", "stars": 9, "license": "None", "language": "python", "size": 92}
{"docstring": "\"\"\"\nclose the file and release the lock.\n\nignore has the same meaning as for ``Lock.unlock``\n\"\"\"\n", "func_signal": "def close(self, ignore=True):\n", "code": "self._file.close()\nself.unlock(ignore)", "path": "pathutils.py", "repo_name": "dabrahams/s3backup", "stars": 9, "license": "None", "language": "python", "size": 92}
{"docstring": "\"\"\"\nReturn the relative path between origin and dest.\n\nIf it's not possible return dest.\n\n\nIf they are identical return ``os.curdir``\n\nAdapted from `path.py <http://www.jorendorff.com/articles/python/path/>`_ by Jason Orendorff. \n\"\"\"\n", "func_signal": "def relpath(origin, dest):\n", "code": "origin = os.path.abspath(origin).replace('\\\\', '/')\ndest = os.path.abspath(dest).replace('\\\\', '/')\n#\norig_list = splitall(os.path.normcase(origin))\n# Don't normcase dest!  We want to preserve the case.\ndest_list = splitall(dest)\n#\nif orig_list[0] != os.path.normcase(dest_list[0]):\n    # Can't get here from there.\n    return dest\n#\n# Find the location where the two paths start to differ.\ni = 0\nfor start_seg, dest_seg in zip(orig_list, dest_list):\n    if start_seg != os.path.normcase(dest_seg):\n        break\n    i += 1\n#\n# Now i is the point where the two paths diverge.\n# Need a certain number of \"os.pardir\"s to work up\n# from the origin to the point of divergence.\nsegments = [os.pardir] * (len(orig_list) - i)\n# Need to add the diverging part of dest_list.\nsegments += dest_list[i:]\nif len(segments) == 0:\n    # If they happen to be identical, use os.curdir.\n    return os.curdir\nelse:\n    return os.path.join(*segments).replace('\\\\', '/')", "path": "pathutils.py", "repo_name": "dabrahams/s3backup", "stars": 9, "license": "None", "language": "python", "size": 92}
{"docstring": "\"\"\"\nGiven a filename and a list of lines it writes the file. (In text mode)\n\nIf ``newline`` is ``True`` (default is ``False``) it adds a newline to each\nline.\n\"\"\"\n", "func_signal": "def writelines(filename, infile, newline=False):\n", "code": "filehandle = open(filename, 'w')\nif newline:\n    infile = [line + '\\n' for line in infile]\nfilehandle.writelines(infile)\nfilehandle.close()", "path": "pathutils.py", "repo_name": "dabrahams/s3backup", "stars": 9, "license": "None", "language": "python", "size": 92}
{"docstring": "\"\"\"\nAdd a trailing slash (``/``) to a path if it lacks one.\n\nIt doesn't use ``os.sep`` because you end up in trouble on windoze, when you\nwant separators for URLs.\n\"\"\"\n", "func_signal": "def tslash(apath):\n", "code": "if apath and apath != '.' and not apath.endswith('/') and not apath.endswith('\\\\'):\n    return apath + '/'\nelse:\n    return apath", "path": "pathutils.py", "repo_name": "dabrahams/s3backup", "stars": 9, "license": "None", "language": "python", "size": 92}
{"docstring": "\"\"\"Return the script directory - whether we're frozen or not.\"\"\"\n", "func_signal": "def get_main_dir():\n", "code": "if main_is_frozen():\n    return os.path.abspath(os.path.dirname(sys.executable))\nreturn os.path.abspath(os.path.dirname(sys.argv[0]))", "path": "pathutils.py", "repo_name": "dabrahams/s3backup", "stars": 9, "license": "None", "language": "python", "size": 92}
{"docstring": "\"\"\"Given a filename and a string, write the file in binary mode. \"\"\"\n", "func_signal": "def writebinary(filename, infile):\n", "code": "filehandle = open(filename, 'wb')\nfilehandle.write(infile)\nfilehandle.close()", "path": "pathutils.py", "repo_name": "dabrahams/s3backup", "stars": 9, "license": "None", "language": "python", "size": 92}
{"docstring": "\"\"\"\nGiven a file size in either (mb, kb) or (kb, bytes) - round it\nappropriately.\n\"\"\"\n# divide an int by a float... get a float\n", "func_signal": "def stringround(main, rest):\n", "code": "value = main + rest/1024.0\nreturn str(round(value, 1))", "path": "pathutils.py", "repo_name": "dabrahams/s3backup", "stars": 9, "license": "None", "language": "python", "size": 92}
{"docstring": "\"\"\"\nGiven a file size as an integer, return a nicely formatted string that\nrepresents the size. Has various options to control it's output.\n\nYou can pass in a dictionary of arguments or keyword arguments. Keyword\narguments override the dictionary and there are sensible defaults for options\nyou don't set.\n\nOptions and defaults are as follows :\n\n*    ``forcekb = False`` -         If set this forces the output to be in terms\nof kilobytes and bytes only.\n\n*    ``largestonly = True`` -    If set, instead of outputting \n    ``1 Mbytes, 307 Kbytes, 478 bytes`` it outputs using only the largest \n    denominator - e.g. ``1.3 Mbytes`` or ``17.2 Kbytes``\n\n*    ``kiloname = 'Kbytes'`` -    The string to use for kilobytes\n\n*    ``meganame = 'Mbytes'`` - The string to use for Megabytes\n\n*    ``bytename = 'bytes'`` -     The string to use for bytes\n\n*    ``nospace = True`` -        If set it outputs ``1Mbytes, 307Kbytes``, \n    notice there is no space.\n\nExample outputs : ::\n\n    19Mbytes, 75Kbytes, 255bytes\n    2Kbytes, 0bytes\n    23.8Mbytes\n\n.. note::\n\n    It currently uses the plural form even for singular.\n\"\"\"\n", "func_signal": "def formatbytes(sizeint, configdict=None, **configs):\n", "code": "defaultconfigs = {  'forcekb' : False,\n                    'largestonly' : True,\n                    'kiloname' : 'Kbytes',\n                    'meganame' : 'Mbytes',\n                    'bytename' : 'bytes',\n                    'nospace' : True}\nif configdict is None:\n    configdict = {}\nfor entry in configs:\n    # keyword parameters override the dictionary passed in\n    configdict[entry] = configs[entry]\n#\nfor keyword in defaultconfigs:\n    if not configdict.has_key(keyword):\n        configdict[keyword] = defaultconfigs[keyword]\n#\nif configdict['nospace']:\n    space = ''\nelse:\n    space = ' '\n#\nmb, kb, rb = bytedivider(sizeint)\nif configdict['largestonly']:\n    if mb and not configdict['forcekb']:\n        return stringround(mb, kb)+ space + configdict['meganame']\n    elif kb or configdict['forcekb']:\n        if mb and configdict['forcekb']:\n            kb += 1024*mb\n        return stringround(kb, rb) + space+ configdict['kiloname']\n    else:\n        return str(rb) + space + configdict['bytename']\nelse:\n    outstr = ''\n    if mb and not configdict['forcekb']:\n        outstr = str(mb) + space + configdict['meganame'] +', '\n    if kb or configdict['forcekb'] or mb:\n        if configdict['forcekb']:\n            kb += 1024*mb\n        outstr += str(kb) + space + configdict['kiloname'] +', '\n    return outstr + str(rb) + space + configdict['bytename']", "path": "pathutils.py", "repo_name": "dabrahams/s3backup", "stars": 9, "license": "None", "language": "python", "size": 92}
{"docstring": "\"\"\"\nGiven a graph definition in Graphviz's dot language (and an optional\nfilename) will generate an image of the graph.\n\"\"\"\n", "func_signal": "def create_graph(dot, filename=\"network\"):\n", "code": "proc = subprocess.Popen('dot -Tpng > %s.png' % filename,\n                        shell=True,\n                        stdin=subprocess.PIPE\n                    )\nproc.communicate(dot.encode('utf_8'))\nexecvp('open', ['open', '%s.png'%filename,])", "path": "social-graph\\dojo.py", "repo_name": "ntoll/code-dojo", "stars": 13, "license": "mit", "language": "python", "size": 3123}
{"docstring": "# failed all the above, \n", "func_signal": "def default(self, line):\n", "code": "if self.guesser is not None:\n    # let's use Bayes\n    all_item_names['north'] = 'N'\n    all_item_names['east'] = 'E'\n    all_item_names['west'] = 'W'\n    all_item_names['south'] = 'S'\n    all_item_names['N'] = 'N'\n    all_item_names['E'] = 'E'\n    all_item_names['W'] = 'W'\n    all_item_names['S'] = 'S'\n    for name in all_item_names:\n        if re.search(r'\\b%s\\b' % re.escape(name), line, re.I):\n            guesses = self.guesser.guess(line.replace(name,''))\n            if guesses:\n                method_name = guesses[0][0]\n                getattr(self, method_name)(all_item_names[name])\n                return", "path": "adventure\\week4\\adventure.py", "repo_name": "ntoll/code-dojo", "stars": 13, "license": "mit", "language": "python", "size": 3123}
{"docstring": "#start_room = _create_universe()\n\n", "func_signal": "def play(gamefile):\n", "code": "player_name = raw_input('Player name?: ') or 'No name'\ng = Game(gamefile, player_name)    \n\ng.cmdloop()", "path": "adventure\\week3\\team5\\adventure.py", "repo_name": "ntoll/code-dojo", "stars": 13, "license": "mit", "language": "python", "size": 3123}
{"docstring": "# Some more contrived examples\n", "func_signal": "def create_styled_dot_file_test():\n", "code": "users = {\n    'ntoll': {'following': 53, 'followers': 172, 'full name': 'Nicholas Tollervey'}, \n    'bob': {'full name': 'Bob T.Builder'}, \n    'fred': {'full name': 'Fred Blogs'}\n    }\nedges = [\n        ['ntoll', 'fred'], \n        ['fred', 'ntoll'], \n        ['ntoll', 'bob'], \n        ['fred', 'bob'],\n        ]\nexpected = 'digraph G { node [ fontname = \"Helvetica\" fontsize = 8 shape = \"plaintext\" ] ntoll [label=< <table border=\"0\" cellborder=\"0\" cellspacing=\"0\" bgcolor=\"#CCCCCC\"> <tr> <td colspan=\"2\" cellpadding=\"2\" align=\"center\" bgcolor=\"#33CCFF\"> <font face=\"Helvetica Bold\">ntoll</font> </td> </tr> <tr> <td align=\"left\" cellpadding=\"2\"><font face=\"Helvetica Bold\">following</font></td> <td align=\"left\" cellpadding=\"2\">53</td> </tr>\\n<tr> <td align=\"left\" cellpadding=\"2\"><font face=\"Helvetica Bold\">followers</font></td> <td align=\"left\" cellpadding=\"2\">172</td> </tr>\\n<tr> <td align=\"left\" cellpadding=\"2\"><font face=\"Helvetica Bold\">full name</font></td> <td align=\"left\" cellpadding=\"2\">Nicholas Tollervey</td> </tr> </table> >]\\nbob [label=< <table border=\"0\" cellborder=\"0\" cellspacing=\"0\" bgcolor=\"#CCCCCC\"> <tr> <td colspan=\"2\" cellpadding=\"2\" align=\"center\" bgcolor=\"#33CCFF\"> <font face=\"Helvetica Bold\">bob</font> </td> </tr> <tr> <td align=\"left\" cellpadding=\"2\"><font face=\"Helvetica Bold\">full name</font></td> <td align=\"left\" cellpadding=\"2\">Bob T.Builder</td> </tr> </table> >]\\nfred [label=< <table border=\"0\" cellborder=\"0\" cellspacing=\"0\" bgcolor=\"#CCCCCC\"> <tr> <td colspan=\"2\" cellpadding=\"2\" align=\"center\" bgcolor=\"#33CCFF\"> <font face=\"Helvetica Bold\">fred</font> </td> </tr> <tr> <td align=\"left\" cellpadding=\"2\"><font face=\"Helvetica Bold\">full name</font></td> <td align=\"left\" cellpadding=\"2\">Fred Blogs</td> </tr> </table> >] ntoll -> fred; fred -> ntoll; ntoll -> bob; fred -> bob; }'\nassert expected == dojo.create_styled_dot_file(users, edges)", "path": "social-graph\\test_dojo.py", "repo_name": "ntoll/code-dojo", "stars": 13, "license": "mit", "language": "python", "size": 3123}
{"docstring": "\"\"\"\nGenerates a representation of the network in Graphviz's dot language\n\nedge_list: a list of the connections (edges in the directed graph) within\nthe network. e.g. [[12345, 54321], [12345, 98765]] (using Twitter user ids)\n\nroot_node: the important \"user\" node in the graph (optionally adds visual\nemphasis) \n\"\"\"\n# Generate the dot language input - could have used a template language like\n# Cheetah but decided this could be an exercise for the user... using\n# Python's built-in template string handling\n", "func_signal": "def create_dot_file(edge_list, root_node=None):\n", "code": "edges = ' '.join(['%s -> %s;' % (src, tgt) for src, tgt in edge_list])\nif root_node:\n    # Visually identify the important \"root\" node\n    node_def = 'node [shape = doublecircle]; %s; node [shape = circle];'%root_node\n    graph = 'digraph G { %s %s }' % (node_def, edges)\nelse:\n    graph = 'digraph G { %s }'%edges\nreturn graph", "path": "social-graph\\dojo.py", "repo_name": "ntoll/code-dojo", "stars": 13, "license": "mit", "language": "python", "size": 3123}
{"docstring": "#start_room = _create_universe()\n\n", "func_signal": "def play(gamefile):\n", "code": "player_name = raw_input('Player name?: ') or 'No name'\ng = Game(gamefile, player_name)    \n\ng.cmdloop()", "path": "adventure\\week4\\team4\\adventure.py", "repo_name": "ntoll/code-dojo", "stars": 13, "license": "mit", "language": "python", "size": 3123}
{"docstring": "#initial setup parameters\n", "func_signal": "def __init__(self, junk):\n", "code": "        self.my_plant = None\n        self.mode = 1\n        self.target_range = random.randrange(50, 1000)", "path": "cells\\team4.py", "repo_name": "ntoll/code-dojo", "stars": 13, "license": "mit", "language": "python", "size": 3123}
{"docstring": "# failed all the above, \n", "func_signal": "def default(self, line):\n", "code": "if self.guesser is not None:\n    # let's use Bayes\n    all_item_names['north'] = 'N'\n    all_item_names['east'] = 'E'\n    all_item_names['west'] = 'W'\n    all_item_names['south'] = 'S'\n    all_item_names['N'] = 'N'\n    all_item_names['E'] = 'E'\n    all_item_names['W'] = 'W'\n    all_item_names['S'] = 'S'\n    for name in all_item_names:\n        if re.search(r'\\b%s\\b' % re.escape(name), line, re.I):\n            guesses = self.guesser.guess(line.replace(name,''))\n            if guesses:\n                method_name = guesses[0][0]\n                getattr(self, method_name)(all_item_names[name])\n                return", "path": "adventure\\week5\\adventure.py", "repo_name": "ntoll/code-dojo", "stars": 13, "license": "mit", "language": "python", "size": 3123}
{"docstring": "#start_room = _create_universe()\n\n", "func_signal": "def play(gamefile):\n", "code": "player_name = raw_input('Player name?: ') or 'No name'\ng = Game(gamefile, player_name)    \n\ng.cmdloop()", "path": "adventure\\week3\\team3\\adventure.py", "repo_name": "ntoll/code-dojo", "stars": 13, "license": "mit", "language": "python", "size": 3123}
{"docstring": "# A very contrived directed graph as a list...\n", "func_signal": "def create_dot_file_test():\n", "code": "edges = [\n        [1, 2], \n        [1, 3],\n        [1, 4],\n        [1, 5],\n        [1, 6],\n        [2, 1],\n        [2, 7],\n        [3, 2],\n        [3, 4],\n        [3, 7],\n        [4, 1],\n        [4, 7],\n        [5, 6],\n        [5, 7],\n        [6, 1],\n        [7, 6],\n        ]\n# Test without root node\nexpected = 'digraph G { 1 -> 2; 1 -> 3; 1 -> 4; 1 -> 5; 1 -> 6; 2 -> 1; 2'\\\n        ' -> 7; 3 -> 2; 3 -> 4; 3 -> 7; 4 -> 1; 4 -> 7; 5 -> 6; 5 -> 7; 6'\\\n        ' -> 1; 7 -> 6; }'\nassert expected == dojo.create_dot_file(edges)\n\n# Test with root node\nexpected = 'digraph G { node [shape = doublecircle]; 3; node [shape ='\\\n        ' circle]; 1 -> 2; 1 -> 3; 1 -> 4; 1 -> 5; 1 -> 6; 2 -> 1; 2 -> 7;'\\\n        ' 3 -> 2; 3 -> 4; 3 -> 7; 4 -> 1; 4 -> 7; 5 -> 6; 5 -> 7; 6 -> 1;'\\\n        ' 7 -> 6; }'\nassert expected == dojo.create_dot_file(edges, root_node=3)", "path": "social-graph\\test_dojo.py", "repo_name": "ntoll/code-dojo", "stars": 13, "license": "mit", "language": "python", "size": 3123}
{"docstring": "#start_room = _create_universe()\n\n", "func_signal": "def play(gamefile):\n", "code": "player_name = raw_input('Player name?: ') or 'No name'\ng = Game(gamefile, player_name)    \n\ng.cmdloop()", "path": "adventure\\week5\\adventure.py", "repo_name": "ntoll/code-dojo", "stars": 13, "license": "mit", "language": "python", "size": 3123}
{"docstring": "#start_room = _create_universe()\n\n", "func_signal": "def play(gamefile):\n", "code": "player_name = raw_input('Player name?: ') or 'No name'\ng = Game(gamefile, player_name)    \n\ng.cmdloop()", "path": "adventure\\week6\\adventure.py", "repo_name": "ntoll/code-dojo", "stars": 13, "license": "mit", "language": "python", "size": 3123}
{"docstring": "#start_room = _create_universe()\n\n", "func_signal": "def play(gamefile):\n", "code": "player_name = raw_input('Player name?: ') or 'No name'\ng = Game(gamefile, player_name)    \n\ng.cmdloop()", "path": "adventure\\week3\\adventure.py", "repo_name": "ntoll/code-dojo", "stars": 13, "license": "mit", "language": "python", "size": 3123}
{"docstring": "# decide if you're a sitting Buddha or hungry ghost\n", "func_signal": "def __init__(self, args):\n", "code": "self.buddha = (random.random() > 0.9)\nself.my_plant = None\nself.target_range = random.randrange(100,300)", "path": "cells\\team1.py", "repo_name": "ntoll/code-dojo", "stars": 13, "license": "mit", "language": "python", "size": 3123}
{"docstring": "\"\"\"\nGenerates a representation of the network in Graphviz's dot language with\nadditional stylistic enhancements to make it look pretty. \n\nuser_list: a dictionary of dictionaries where the key is the twitter\nusername and the referenced dictionary is a collection of attributes to\ndisplay. e.g. { 'ntoll': {'id': 12345, 'fullname': 'Nicholas Tollervey'}}\n\nedge_list: a list of the connections (edges in the directed graph) within\nthe network. e.g. [['ntoll', 'tartley'], ['ntoll', 'voidspace']] (using \nTwitter user-names )\n\"\"\"\n \n", "func_signal": "def create_styled_dot_file(user_list, edge_list):\n", "code": "NODE = '$id [label=< <table border=\"0\" cellborder=\"0\" cellspacing=\"0\"'\\\n        ' bgcolor=\"#CCCCCC\"> <tr> <td colspan=\"2\" cellpadding=\"2\"'\\\n        ' align=\"center\" bgcolor=\"#33CCFF\"> <font face=\"Helvetica Bold\">'\\\n        '$id</font> </td> </tr> $rows </table> >]'\nATTRIBUTE = '<tr> <td align=\"left\" cellpadding=\"2\"><font face=\"Helvetica'\\\n        ' Bold\">$key</font></td> <td align=\"left\" cellpadding=\"2\">$value'\\\n        '</td> </tr>'\nnode = Template(NODE)\nattribute = Template(ATTRIBUTE)\nnodes = '\\n'.join([node.substitute(id=u,\n    rows='\\n'.join([attribute.substitute(key=k, value=v) for k, v in\n    d.iteritems()])) for u, d in user_list.iteritems()])\nedges = ' '.join(['%s -> %s;'%(src, tgt) for src, tgt in edge_list])\ngraph = 'digraph G { node [ fontname = \"Helvetica\" fontsize = 8 shape ='\\\n        ' \"plaintext\" ] %s %s }'%(nodes, edges)\nreturn graph", "path": "social-graph\\dojo.py", "repo_name": "ntoll/code-dojo", "stars": 13, "license": "mit", "language": "python", "size": 3123}
{"docstring": "#start_room = _create_universe()\n\n", "func_signal": "def play(gamefile):\n", "code": "player_name = raw_input('Player name?: ') or 'No name'\ng = Game(gamefile, player_name)    \n\ng.cmdloop()", "path": "adventure\\week3\\team2\\adventure.py", "repo_name": "ntoll/code-dojo", "stars": 13, "license": "mit", "language": "python", "size": 3123}
{"docstring": "#start_room = _create_universe()\n\n", "func_signal": "def play(gamefile):\n", "code": "player_name = raw_input('Player name?: ') or 'No name'\ng = Game(gamefile, player_name)    \n\ng.cmdloop()", "path": "adventure\\week2\\adventure.py", "repo_name": "ntoll/code-dojo", "stars": 13, "license": "mit", "language": "python", "size": 3123}
{"docstring": "#start_room = _create_universe()\n\n", "func_signal": "def play(gamefile):\n", "code": "player_name = raw_input('Player name?: ') or 'No name'\ng = Game(gamefile, player_name)    \n\ng.cmdloop()", "path": "adventure\\week4\\adventure.py", "repo_name": "ntoll/code-dojo", "stars": 13, "license": "mit", "language": "python", "size": 3123}
{"docstring": "# find the verb\n", "func_signal": "def get_verb(self, phrase):\n", "code": "verb = self.words[0]\nreturn(verb)", "path": "adventure\\week4\\team4\\adventure.py", "repo_name": "ntoll/code-dojo", "stars": 13, "license": "mit", "language": "python", "size": 3123}
{"docstring": "# find the nound\n#words = phrase.split(' ')\n", "func_signal": "def get_nouns(self, phrase):\n", "code": "for word in phrase:\n    if word.lower() in self.player.items.keys() + VOCABULARY['nouns']:\n        return word\n    else:\n        if len(phrase) > 1:\n            return phrase[1]\n        return \"\"", "path": "adventure\\week4\\team4\\adventure.py", "repo_name": "ntoll/code-dojo", "stars": 13, "license": "mit", "language": "python", "size": 3123}
{"docstring": "\"\"\"\nPlay a game of Dominion. Return a dictionary mapping players to scores.\n\"\"\"\n", "func_signal": "def run(self):\n", "code": "game = self\nwhile not game.over():\n    game = game.take_turn()\nscores = [(state.player, state.score()) for state in game.playerstates]\nself.log.info(\"End of game.\")\nself.log.info(\"Scores: %s\" % scores)\nreturn scores", "path": "dominiate\\game.py", "repo_name": "rspeer/dominiate-python", "stars": 15, "license": "None", "language": "python", "size": 118}
{"docstring": "\"\"\"\nDiscard a single card from the hand.\n\"\"\"\n", "func_signal": "def discard_card(self, card):\n", "code": "index = list(self.hand).index(card)\nnewhand = self.hand[:index] + self.hand[index+1:]\nreturn PlayerState(\n  self.player, newhand, self.drawpile, self.discard+(card,),\n  self.tableau, self.actions, self.buys, self.coins\n)", "path": "dominiate\\game.py", "repo_name": "rspeer/dominiate-python", "stars": 15, "license": "None", "language": "python", "size": 118}
{"docstring": "\"\"\"Are there actions left to take with this hand?\"\"\"\n", "func_signal": "def actionable(self):\n", "code": "return (self.actions > 0 \n        and any(c.isAction() for c in self.hand))", "path": "dominiate\\game.py", "repo_name": "rspeer/dominiate-python", "stars": 15, "license": "None", "language": "python", "size": 118}
{"docstring": "\"\"\"\nChange the number of actions, buys, cards, or coins available on this\nturn.\n\"\"\"\n", "func_signal": "def change(self, delta_actions=0, delta_buys=0, delta_cards=0, delta_coins=0):\n", "code": "state= PlayerState(self.player, self.hand, self.drawpile, self.discard,\n                   self.tableau, self.actions+delta_actions,\n                   self.buys+delta_buys, self.coins+delta_coins)\nassert delta_cards >= 0\nif delta_cards > 0:\n    return state.draw(delta_cards)\nelse: return state", "path": "dominiate\\game.py", "repo_name": "rspeer/dominiate-python", "stars": 15, "license": "None", "language": "python", "size": 118}
{"docstring": "\"\"\"\nRun through all the decisions the current player has to make, and\nreturn the state where the player buys stuff.\n\"\"\"\n", "func_signal": "def simulate_partial_turn(self):\n", "code": "if not self.simulated: self = self.simulated_copy()\nstate = self.state()\ndecisiontype = state.next_decision()\nif decisiontype is None:\n    assert False, \"BuyDecision never happened this turn\"\nif decisiontype is BuyDecision:\n    return state\ndecision = decisiontype(self)\nnewgame = self.current_player().make_decision(decision)\nreturn newgame.simulate_partial_turn()", "path": "dominiate\\game.py", "repo_name": "rspeer/dominiate-python", "stars": 15, "license": "None", "language": "python", "size": 118}
{"docstring": "\"\"\"\nReturns a new PlayerState in which n cards have been drawn (shuffling\nif necessary).\n\"\"\"\n", "func_signal": "def draw(self, n=1):\n", "code": "if len(self.drawpile) >= n:\n    return PlayerState(\n      self.player, self.hand+self.drawpile[:n], self.drawpile[n:],\n      self.discard, self.tableau, self.actions, self.buys, self.coins\n    )\nelif self.discard:\n    got = self.drawpile\n    newdraw = list(self.discard)\n    random.shuffle(newdraw)\n\n    state2 = PlayerState(\n      self.player, self.hand+got, tuple(newdraw), (), self.tableau,\n      self.actions, self.buys, self.coins\n    )\n    return state2.draw(n-len(got))\nelse:\n    return PlayerState(\n      self.player, self.hand+self.drawpile, (), (), self.tableau,\n      self.actions, self.buys, self.coins\n    )", "path": "dominiate\\game.py", "repo_name": "rspeer/dominiate-python", "stars": 15, "license": "None", "language": "python", "size": 118}
{"docstring": "\"\"\"\nRun through all the decisions the current player has to make, and\nreturn the number of coins and buys they end up with. Useful for\nthe BigMoney strategy.\n\"\"\"\n", "func_signal": "def simulate_turn(self):\n", "code": "if not self.simulated: self = self.simulated_copy()\nstate = self.state()\ndecisiontype = state.next_decision()\nif decisiontype is None:\n    assert False, \"BuyDecision never happened this turn\"\nif decisiontype is BuyDecision:\n    return (state.hand_value(), state.buys)\ndecision = decisiontype(self)\nnewgame = self.current_player().make_decision(decision)\nreturn newgame.simulate_turn()", "path": "dominiate\\game.py", "repo_name": "rspeer/dominiate-python", "stars": 15, "license": "None", "language": "python", "size": 118}
{"docstring": "\"\"\"\nSimulate n hands with certain cards in them, yielding the number of\ncoins and buys they end with.\n\"\"\"\n", "func_signal": "def simulate_hands(self, n=100, cards=()):\n", "code": "for i in xrange(n):\n    # make sure there are cards to gain, even though we haven't\n    # kept track of the real game state\n    game = Game([self.simulation_state(cards)],\n                {province: 12, duchy: 12, estate: 12,\n                 copper: 12, silver: 12, gold: 12},\n                simulated=True\n    )\n    coins, buys = game.simulate_turn()\n    yield coins, buys", "path": "dominiate\\game.py", "repo_name": "rspeer/dominiate-python", "stars": 15, "license": "None", "language": "python", "size": 118}
{"docstring": "\"\"\"\nPlay a card from the hand into the tableau.\n\nDecreasing the number of actions available is handled in\nplay_action(card).\n\"\"\"\n\n", "func_signal": "def play_card(self, card):\n", "code": "index = list(self.hand).index(card)\nnewhand = self.hand[:index] + self.hand[index+1:]\nresult = PlayerState(\n  self.player, newhand, self.drawpile, self.discard,\n  self.tableau+(card,), self.actions, self.buys, self.coins\n)\nassert len(self) == len(result)\nreturn result", "path": "dominiate\\game.py", "repo_name": "rspeer/dominiate-python", "stars": 15, "license": "None", "language": "python", "size": 118}
{"docstring": "\"\"\"\nRemove a single card from the table.\n\"\"\"\n", "func_signal": "def remove_card(self, card):\n", "code": "new_counts = self.card_counts.copy()\nnew_counts[card] -= 1\nassert new_counts[card] >= 0\nreturn Game(self.playerstates[:], new_counts, self.player_turn, self.simulated)", "path": "dominiate\\game.py", "repo_name": "rspeer/dominiate-python", "stars": 15, "license": "None", "language": "python", "size": 118}
{"docstring": "\"\"\"\nGet a copy of this game, but with the `simulated` flag set to True\nand no information that the current player should not have. This\nprevents accidentally cheating when looking at the implications of\nvarious actions.\n\"\"\"\n", "func_signal": "def simulated_copy(self):\n", "code": "return Game(\n    [state.simulated_from_here() if state is self.state()\n                                 else state.simulate()\n     for state in self.playerstates],\n    self.card_counts,\n    self.turn,\n    simulated=True\n)", "path": "dominiate\\game.py", "repo_name": "rspeer/dominiate-python", "stars": 15, "license": "None", "language": "python", "size": 118}
{"docstring": "\"\"\"\nApply a function to all other states, with no decisions to be made.\n\nThis does not work for attacks, because other players might have a\ncounter that requires them to make a decision. Implement attacks using\nthe attack_with_decision method instead.\n\"\"\"\n", "func_signal": "def transform_other_states(self, func, attack=False):\n", "code": "newgame = self.copy()\nfor i in xrange(self.num_players()):\n    if i == self.player_turn: continue\n    newgame.playerstates[i] = func(newgame.playerstates[i])\nreturn newgame", "path": "dominiate\\game.py", "repo_name": "rspeer/dominiate-python", "stars": 15, "license": "None", "language": "python", "size": 118}
{"docstring": "\"\"\"\nRun through all the decisions the current player has to make, and\nreturn the resulting state.\n\"\"\"\n", "func_signal": "def run_decisions(self):\n", "code": "state = self.state()\ndecisiontype = state.next_decision()\nif decisiontype is None: return self\ndecision = decisiontype(self)\nnewgame = self.current_player().make_decision(decision)\nreturn newgame.run_decisions()", "path": "dominiate\\game.py", "repo_name": "rspeer/dominiate-python", "stars": 15, "license": "None", "language": "python", "size": 118}
{"docstring": "\"\"\"\nRemove a card from the game.\n\"\"\"\n", "func_signal": "def trash_card(self, card):\n", "code": "index = list(self.hand).index(card)\nnewhand = self.hand[:index] + self.hand[index+1:]\nreturn PlayerState(\n  self.player, newhand, self.drawpile, self.discard,\n  self.tableau, self.actions, self.buys, self.coins\n)", "path": "dominiate\\game.py", "repo_name": "rspeer/dominiate-python", "stars": 15, "license": "None", "language": "python", "size": 118}
{"docstring": "\"\"\"\nPlay an entire turn, including drawing cards at the end. Return\nthe game state where it is the next player's turn.\n\"\"\"\n", "func_signal": "def take_turn(self):\n", "code": "self.log.info(\"\")\nself.log.info(\"Round %d / player %d: %s\" % (\n  (self.round + 1),\n  (self.player_turn+1), self.current_player().name\n))\n\nself.log.info(\"%d provinces left\" % self.card_counts[province])\n\n# Run AI hooks that need to happen before the turn.\nself.current_player().before_turn(self)\nendturn = self.run_decisions()\n\nnext_turn = (self.turn + 1)\n\nnewgame = Game(endturn.playerstates[:], endturn.card_counts,\n               next_turn, self.simulated)\n# mutate the new game object since nobody cares yet\nnewgame.playerstates[self.player_turn] =\\\n  newgame.playerstates[self.player_turn].next_turn()\n\n# Run AI hooks that need to happen after the turn.\nself.current_player().after_turn(newgame)\nreturn newgame", "path": "dominiate\\game.py", "repo_name": "rspeer/dominiate-python", "stars": 15, "license": "None", "language": "python", "size": 118}
{"docstring": "\"\"\"\nFirst, discard everything. Then, get 5 cards, 1 action, and 1 buy.\n\"\"\"\n", "func_signal": "def next_turn(self):\n", "code": "return PlayerState(\n  self.player, (), self.drawpile, self.discard+self.hand+self.tableau,\n  (), actions=1, buys=1, coins=0\n).draw(5)", "path": "dominiate\\game.py", "repo_name": "rspeer/dominiate-python", "stars": 15, "license": "None", "language": "python", "size": 118}
{"docstring": "\"\"\"\nDo something with the current player's state and make a new overall\ngame state from it.\n\"\"\"\n", "func_signal": "def replace_states(self, newstates):\n", "code": "newgame = self.copy()\nnewgame.playerstates = newstates\nreturn newgame", "path": "dominiate\\game.py", "repo_name": "rspeer/dominiate-python", "stars": 15, "license": "None", "language": "python", "size": 118}
{"docstring": "\"\"\"\nGet a state with a freshly-shuffled deck, a new turn, and certain cards\non top of the deck. Generally useful for simulating the effect of\ngaining a new card.\n\"\"\"\n", "func_signal": "def simulation_state(self, cards=()):\n", "code": "state = PlayerState(self.player, (), cards, self.all_cards(), (),\n                    1, 1, 0)\nreturn state.draw(5)", "path": "dominiate\\game.py", "repo_name": "rspeer/dominiate-python", "stars": 15, "license": "None", "language": "python", "size": 118}
{"docstring": "# put it all in the discard pile so it auto-shuffles, then draw\n", "func_signal": "def initial_state(player):\n", "code": "return PlayerState(player, hand=(), drawpile=(),\ndiscard=(copper,)*7 + (estate,)*3, tableau=()).next_turn()", "path": "dominiate\\game.py", "repo_name": "rspeer/dominiate-python", "stars": 15, "license": "None", "language": "python", "size": 118}
{"docstring": "\"\"\"\nReturn the next decision that must be made. This will be either\nan ActDecision or a BuyDecision; other kinds of decisions only happen\nas a result of ActDecisions.\n\"\"\"\n", "func_signal": "def next_decision(self):\n", "code": "if self.actionable():\n    return ActDecision\nelif self.buyable():\n    return BuyDecision\nelse: return None", "path": "dominiate\\game.py", "repo_name": "rspeer/dominiate-python", "stars": 15, "license": "None", "language": "python", "size": 118}
{"docstring": "# pyclass\n", "func_signal": "def __init__(self):\n", "code": "self._workflowMessage = None\nreturn", "path": "src\\vmw\\vco\\generated\\VSOWebControlService_types.py", "repo_name": "sigma/vmw.vco", "stars": 13, "license": "mit", "language": "python", "size": 237}
{"docstring": "# pyclass\n", "func_signal": "def __init__(self):\n", "code": "self._type = None\nself._query = None\nself._username = None\nself._password = None\nreturn", "path": "src\\vmw\\vco\\generated\\VSOWebControlService_types.py", "repo_name": "sigma/vmw.vco", "stars": 13, "license": "mit", "language": "python", "size": 237}
{"docstring": "# pyclass\n", "func_signal": "def __init__(self):\n", "code": "self._type = None\nself._id = None\nself._properties = None\nself._dunesUri = None\nreturn", "path": "src\\vmw\\vco\\generated\\VSOWebControlService_types.py", "repo_name": "sigma/vmw.vco", "stars": 13, "license": "mit", "language": "python", "size": 237}
{"docstring": "# pyclass\n", "func_signal": "def __init__(self):\n", "code": "self._workflowTokenIds = []\nself._username = None\nself._password = None\nreturn", "path": "src\\vmw\\vco\\generated\\VSOWebControlService_types.py", "repo_name": "sigma/vmw.vco", "stars": 13, "license": "mit", "language": "python", "size": 237}
{"docstring": "# pyclass\n", "func_signal": "def __init__(self):\n", "code": "self._findReturn = None\nreturn", "path": "src\\vmw\\vco\\generated\\VSOWebControlService_types.py", "repo_name": "sigma/vmw.vco", "stars": 13, "license": "mit", "language": "python", "size": 237}
{"docstring": "# pyclass\n", "func_signal": "def __init__(self):\n", "code": "self._getAllPluginReturn = []\nreturn", "path": "src\\vmw\\vco\\generated\\VSOWebControlService_types.py", "repo_name": "sigma/vmw.vco", "stars": 13, "license": "mit", "language": "python", "size": 237}
{"docstring": "# pyclass\n", "func_signal": "def __init__(self):\n", "code": "self._totalCount = None\nself._elements = None\nreturn", "path": "src\\vmw\\vco\\generated\\VSOWebControlService_types.py", "repo_name": "sigma/vmw.vco", "stars": 13, "license": "mit", "language": "python", "size": 237}
{"docstring": "# pyclass\n", "func_signal": "def __init__(self):\n", "code": "self._findRelationReturn = []\nreturn", "path": "src\\vmw\\vco\\generated\\VSOWebControlService_types.py", "repo_name": "sigma/vmw.vco", "stars": 13, "license": "mit", "language": "python", "size": 237}
{"docstring": "# pyclass\n", "func_signal": "def __init__(self):\n", "code": "self._id = None\nself._title = None\nself._workflowId = None\nself._currentItemName = None\nself._currentItemState = None\nself._globalState = None\nself._startDate = None\nself._endDate = None\nself._xmlContent = None\nreturn", "path": "src\\vmw\\vco\\generated\\VSOWebControlService_types.py", "repo_name": "sigma/vmw.vco", "stars": 13, "license": "mit", "language": "python", "size": 237}
{"docstring": "# pyclass\n", "func_signal": "def __init__(self):\n", "code": "self._name = None\nself._type = None\nself._value = None\nreturn", "path": "src\\vmw\\vco\\generated\\VSOWebControlService_types.py", "repo_name": "sigma/vmw.vco", "stars": 13, "license": "mit", "language": "python", "size": 237}
{"docstring": "# pyclass\n", "func_signal": "def __init__(self):\n", "code": "self._eventName = None\nself._serializedProperties = None\nreturn", "path": "src\\vmw\\vco\\generated\\VSOWebControlService_types.py", "repo_name": "sigma/vmw.vco", "stars": 13, "license": "mit", "language": "python", "size": 237}
{"docstring": "# pyclass\n", "func_signal": "def __init__(self):\n", "code": "self._username = None\nself._password = None\nreturn", "path": "src\\vmw\\vco\\generated\\VSOWebControlService_types.py", "repo_name": "sigma/vmw.vco", "stars": 13, "license": "mit", "language": "python", "size": 237}
{"docstring": "# pyclass\n", "func_signal": "def __init__(self):\n", "code": "self._getWorkflowsWithNameReturn = []\nreturn", "path": "src\\vmw\\vco\\generated\\VSOWebControlService_types.py", "repo_name": "sigma/vmw.vco", "stars": 13, "license": "mit", "language": "python", "size": 237}
{"docstring": "# pyclass\n", "func_signal": "def __init__(self):\n", "code": "self._getWorkflowTokenForIdReturn = None\nreturn", "path": "src\\vmw\\vco\\generated\\VSOWebControlService_types.py", "repo_name": "sigma/vmw.vco", "stars": 13, "license": "mit", "language": "python", "size": 237}
{"docstring": "# pyclass\n", "func_signal": "def __init__(self):\n", "code": "self._hasRightsReturn = None\nreturn", "path": "src\\vmw\\vco\\generated\\VSOWebControlService_types.py", "repo_name": "sigma/vmw.vco", "stars": 13, "license": "mit", "language": "python", "size": 237}
{"docstring": "# pyclass\n", "func_signal": "def __init__(self):\n", "code": "self._type = None\nself._id = None\nself._username = None\nself._password = None\nreturn", "path": "src\\vmw\\vco\\generated\\VSOWebControlService_types.py", "repo_name": "sigma/vmw.vco", "stars": 13, "license": "mit", "language": "python", "size": 237}
{"docstring": "# pyclass\n", "func_signal": "def __init__(self):\n", "code": "self._message = None\nreturn", "path": "src\\vmw\\vco\\generated\\VSOWebControlService_types.py", "repo_name": "sigma/vmw.vco", "stars": 13, "license": "mit", "language": "python", "size": 237}
{"docstring": "# pyclass\n", "func_signal": "def __init__(self):\n", "code": "self._getAllWorkflowsReturn = []\nreturn", "path": "src\\vmw\\vco\\generated\\VSOWebControlService_types.py", "repo_name": "sigma/vmw.vco", "stars": 13, "license": "mit", "language": "python", "size": 237}
{"docstring": "# pyclass\n", "func_signal": "def __init__(self):\n", "code": "self._moduleName = None\nself._moduleVersion = None\nself._moduleDescription = None\nself._moduleDisplayName = None\nreturn", "path": "src\\vmw\\vco\\generated\\VSOWebControlService_types.py", "repo_name": "sigma/vmw.vco", "stars": 13, "license": "mit", "language": "python", "size": 237}
{"docstring": "# pyclass\n", "func_signal": "def __init__(self):\n", "code": "self._id = None\nself._name = None\nself._description = None\nself._inParameters = None\nself._outParameters = None\nself._attributes = None\nreturn", "path": "src\\vmw\\vco\\generated\\VSOWebControlService_types.py", "repo_name": "sigma/vmw.vco", "stars": 13, "license": "mit", "language": "python", "size": 237}
{"docstring": "\"\"\"\n    Private function to convert cached ENSO indices.\n    This function is used to ensure that:\n    * the frequency of the indices matches the frequency of the indicator.\n    * the date range of the indices matches the date range of the indicator.\n    * the shape of the indices matches the shape of the indicator.\n\"\"\"\n", "func_signal": "def _fix_cachedcurrent(self, ensoindices):\n", "code": "_currentfreq = ensoindices._dates.freq\n_freq = self._dates.freq\n# Check the frequency, and convert if needed\nif _currentfreq > _freq:\n    # To a lower frequency ('Q'/'A')\n    if self.ndim == 2:\n        conversionfunc = None\n    else:\n        conversionfunc = lambda x: mode(x)[0].squeeze()\n    ensoindices = ensoindices.convert(_freq, func=conversionfunc)\nelif _currentfreq < _freq:\n    # To a higher frequency (eg, 'D')\n    ensoindices = backward_fill(ensoindices.convert(_freq))\n# Reset to the original frequency if needed...\n(start, end) = self._dates.flat[[0, -1]]\nif tuple(ensoindices._dates.flat[[0, -1]]) != (start, end):\n    ensoindices = ts.adjust_endpoints(ensoindices,\n                                      start_date=start, end_date=end)\n# Reset the shape of the indices\nif ensoindices.shape != self.shape:\n    ensoindices.shape = self.shape\nreturn ensoindices", "path": "scikits\\hydroclimpy\\enso\\ensobase.py", "repo_name": "pierregm/scikits.hydroclimpy", "stars": 13, "license": "None", "language": "python", "size": 602}
{"docstring": "\"\"\"\nTransforms a character string to a list of months indices.\n\nThe string should consist of uppercase letters, one letter per month, in\nchronological order.\n\nExamples\n--------\n>>> code2months('OND')\narray([10,11,12])\n>>> code2months('NDJ')\narray([11,12,1])\n>>> code2months('SOD')\nValueError\n\n\"\"\"\n", "func_signal": "def code2months(code):\n", "code": "strg = 'JFMAMJJASONDJFMAMJJASOND'\nif not isinstance(code, basestring):\n    raise TypeError(\"Unrecognized string ('%s')\" % code)\ncode = code.upper()\nidx = strg.find(code)\nif idx != -1:\n    months = np.arange(len(code)) + idx + 1\nelse:\n    raise ValueError(\"Unrecognized string ('%s')\" % code)\nmonths[months > 12] -= 12\nreturn tuple(months)", "path": "scikits\\hydroclimpy\\enso\\ensobase.py", "repo_name": "pierregm/scikits.hydroclimpy", "stars": 13, "license": "None", "language": "python", "size": 602}
{"docstring": "\"\"\"\nLoads the ONI 3-m averaged monthly SST anomalies over the Ni\u00f1o-3.4 region\nand returns a :class:`~scikits.hydroclimpy.enso.ENSOIndicator` object.\n\nTwo modes are accepted as arguments:\n\n- in the ``standard`` mode, the SSTs are retrieved from the original CPC\n  website_.\n  Data are available from Jan. 1950 to present.\n- in the ``backup`` mode, the SSTs are retrieved from the CPC `ftp site <ftpsite>`_.\n  Data are available from Jan. 1900 to present.\n\n.. _website : http://www.cpc.noaa.gov/products/analysis_monitoring/ensostuff/ensoyears.shtml\n.. _ftpsite : ftp://eclipse.ncdc.noaa.gov/pub/ersst/pdo/el_nino_v3.dat.\n\n\nParameters\n----------\nmode : {'standard','backup'}, optional\n    Mode describing the data to download.\noptions : dictionary\n    Optional parameters to parse to the ENSOIndicator for the definition of\n    ENSO indices.\nthresholds : tuple of floats, optional\n    Low and high temperature thresholds for the definition of El Ni\u00f1o and\n    La Ni\u00f1a conditions.\n    By default, the CPC uses -0.5oC and +0.5oC.\nminimum_size : int, optional\n    Minimum number of consecutive months in El Ni\u00f1o / La Ni\u00f1a conditions\n    required for the definition of an episode.\n    By default, the CPC use 5 consecutive months.\nreference_season : string or tuple, optional\n    Months that must be in an episode for it to be valid.\n    By default, the CPC uses None (no restriction on the months).\nfull_year : boolean, optional\n    The CPC uses ``full_year=False``.\n\nReferences\n----------\nXue, Y., T. M. Smith, and R. W. Reynolds, 2003: Interdecadal changes of 30-yr\nSST normals during 1871-2000. *J. Climate*, 16, 1601-1612.\n\n\"\"\"\n# Initialization .......................\n", "func_signal": "def load_oni(mode='standard', **options):\n", "code": "ensoarchive = dict(config.items('ENSO'))['ensoarchive']\nif ensoarchive[-4:].lower() != '.zip':\n    ensoarchive += '.zip'\n#\nmode = mode.lower()\ncfg = dict(config.items('ENSO.ONI'))\ncfg.update(options)\ntry:\n    from BeautifulSoup import BeautifulSoup, SoupStrainer\nexcept ImportError:\n    warnings.warn(\"The module 'BeautifulSoup' is unavailable.\\n\"\\\n                  \"Reverting to backup mode\")\n    mode = 'backup'\n#\ndatadir = cfg['datadir']\nif mode == 'standard':\n    netfile = cfg['netfile']\n    archive = cfg['archive']\nelse:\n    netfile = cfg['netfile_backup']\n    archive = cfg['archive_backup']\n# Try to open an existing ENSOIndicator\n\nensoarchive = dict(config.items('ENSO'))['ensoarchive']\nif ensoarchive[-4:].lower() != '.zip':\n    ensoarchive += '.zip'\n#\ntry:\n    zipf = zipfile.ZipFile(ensoarchive, 'r')\n    ensoi = cPickle.loads(zipf.read(archive))\n    ensologger.info(\"... Loading from existing archived file\")\nexcept IOError:\n    zipf = zipfile.ZipFile(ensoarchive, 'w')\n    ensologger.info(\"... Creating archive\")\nexcept KeyError:\n    zipf = zipfile.ZipFile(ensoarchive, 'a')\n    ensologger.info(\"... Appending to archive\")\nelse:\n    if isinstance(ensoi, enso.ENSOIndicator):\n        return ensoi\n#\nsourcedir = np.lib._datasource.DataSource(datadir)\ndfile = sourcedir.open(netfile)\n#\n#\nif mode == 'standard':\n    # Load the file as a tree, but only take the SST table (border=1)\n    table = BeautifulSoup(dfile.read(),\n                          parseOnlyThese=SoupStrainer(\"table\", border=1))\n    # Separate it by rows, but skip the first one (the header)\n    years = []\n    data = []\n    indices = []\n    color = {'red':+1, 'white':0, 'blue':-1}\n    deft = [(None, 'color:white')]\n    for row in table.findAll(\"tr\")[1:]:\n        cols = row.findAll('td')\n        years.append(int(cols.pop(0).strong.string))\n        data.append([float(_.fetchText()[-1].string.replace('&nbsp;', '99.9'))\n                     for _ in cols])\n        indices.append([color[getattr(_.span, 'attrs', deft)[0][-1].split(':')[-1]]\n                            for _ in cols])\n    #\n    start_date = Date('M', year=years[0], month=1)\n    ensoi = enso.ENSOIndicator(ma.masked_values(data, 99.9).ravel(),\n                             start_date=start_date,)", "path": "scikits\\hydroclimpy\\enso\\ensodata.py", "repo_name": "pierregm/scikits.hydroclimpy", "stars": 13, "license": "None", "language": "python", "size": 602}
{"docstring": "\"\"\"\n    Gets the subset of the series falling during the given episode.\n\n    Parameters\n    ----------\n    index : {-1,0,+1}\nIndex corresponding to the desired ENSO phase:\n\n* -1 : La Ni\\~na\n*  0 : Neutral\n* +1 : El Ni\\~no\n\n\"\"\"\n", "func_signal": "def _phase(self, index):\n", "code": "indices = self.ensoindices\nresult = self.copy().view(TimeSeries)\nresult[(indices != index).filled(True)] = masked\nreturn result", "path": "scikits\\hydroclimpy\\enso\\ensobase.py", "repo_name": "pierregm/scikits.hydroclimpy", "stars": 13, "license": "None", "language": "python", "size": 602}
{"docstring": "\"\"\"\nReturns an array of default meridians at the given resolution for the current\nbasemap.\n\nParameters\n----------\nbasemap : Basemap object\n    Basemap on which to draw.\nresol : {float}, optional\n    Space between two meridians (in degrees).\n\"\"\"\n", "func_signal": "def get_default_meridians(basemap, resol=1):\n", "code": "return np.arange(np.floor(basemap.llcrnrlon / 5.) * 5.,\n                 np.ceil(basemap.urcrnrlon / 5.) * 5., float(resol))", "path": "scikits\\hydroclimpy\\plotlib\\maptools.py", "repo_name": "pierregm/scikits.hydroclimpy", "stars": 13, "license": "None", "language": "python", "size": 602}
{"docstring": "\"\"\"\n\nTransforms a :class:`~scikits.timeseries.TimeSeries` into a \n:class:`~scikits.hydroclimpy.enso.ClimateSeries`, by setting an ENSO indicator.\n\nParameters\n----------\nseries : TimeSeries\n    The :class:`~scikits.timeseries.TimeSeries` object to transform.\nindicator : ENSOIndicator\n    The :class:`~scikits.hydroclimpy.enso.ENSOIndicator` object we want to set.\n\nRaises\n------\nTypeError\n    If ``series`` is not a :class:`~scikits.timeseries.TimeSeries`,\n    or ``indicator`` not an :class:`~scikits.hydroclimpy.enso.ENSOIndicator`.\n\nReturns\n-------\nclimateseries\n    A :class:`~scikits.hydroclimpy.enso.ClimateSeries` object.\n\n\"\"\"\n", "func_signal": "def set_ensoindicator(series, indicator):\n", "code": "if not isinstance(indicator, ENSOIndicator):\n    return TypeError(\"Argument is not a valid ENSO indicator !\")\nif not isinstance(series, TimeSeries):\n    raise TypeError(\"Time series should be a valid TimeSeries!\")\n_series = series.view(ClimateSeries)\n_series.ensoindicator = indicator\n_series.refperiod = getattr(series, 'refperiod', None)\nreturn _series", "path": "scikits\\hydroclimpy\\enso\\ensobase.py", "repo_name": "pierregm/scikits.hydroclimpy", "stars": 13, "license": "None", "language": "python", "size": 602}
{"docstring": "\"\"\"\n    Sets the ENSO indices from the current ENSO indicator.\n\n    Returns\n    -------\n    ensoindices : TimeSeries\nSeries of ENSO indices : +1 for El Ni\\~no, 0 for Neutral, -1 for La Ni\\~na.\n\n\"\"\"\n", "func_signal": "def set_ensoindices(self, **parameters):\n", "code": "ensoindicator = self.optinfo.get('ensoindicator', None)\nif ensoindicator is None:\n    errmsg = \"The ENSO indicator hasn't been initialized yet!\"\n    raise InvalidENSOError(errmsg)\nreturn ensoindicator.set_indices(**parameters)", "path": "scikits\\hydroclimpy\\enso\\ensobase.py", "repo_name": "pierregm/scikits.hydroclimpy", "stars": 13, "license": "None", "language": "python", "size": 602}
{"docstring": "\"\"\"\nLoad the JMA 5-month running mean monthly SST anomalies over the Nino-3 zone\n(5N-5S, 150W-90W) and returns a :class:`~scikits.hydroclimpy.enso.ENSOIndicator`\nobject.\nThree different modes are available:\n\n``coaps``\n    The data is downloaded from the \n    `COAPS website <ftp://www.coaps.fsu.edu/pub/JMA_SST_Index/jmasst1868-today.filter-5>`_.\n``standard``\n    The data is downloaded from the JMA site and corresponds to the SST\n    anomalies computed using a \n    `fixed reference <http://ds.data.jma.go.jp/tcc/tcc/products/elnino/index/sstindex/base_period_7100/Nino_3/5rmean>`_\n    period (1971-2000).\n``sliding``\n    The data is also downloaded from the JMA site, but corresponds this time\n    to anomalies computed using a \n    `sliding 30-year rule <http://ds.data.jma.go.jp/tcc/tcc/products/elnino/index/sstindex/sliding_30year_period/Nino_3/5rmean>`_.\n\nBy default, the configuration options are read from the configuration file.\nThese options can be overwritten by the optional input parameters.\n\n\nParameters\n----------\nmode : {'coaps','standard','sliding'}, optional\n    Mode describing the data to download.\noptions : dictionary\n    Optional parameters to parse to the ENSOIndicator for the definition of\n    ENSO indices.\nthresholds : tuple\n    Low and high temperature thresholds for the definition of El Ni\u00f1o or\n    La Ni\u00f1a conditions.\n    By default, the JMA uses -0.5oC and +0.5oC.\nminimum_size : int\n    Minimum number of consecutive months in El Ni\u00f1o/La Ni\u00f1a conditions\n    required for the definition of an episode.\n    By default, the JMA use 6 consecutive months.\nreference_season : string, tuple \n    Months that must be in an episode for it to be valid.\n    By default, the original JMA COAPS index uses ``'OND'`` (October to\n    December included), while the newer COAPS index uses ``'NDJ'`` (November\n    to January included).\nfull_year : boolean\n    Whether episodes are defined from Oct. 1st to the following Sep. 30th,\n    or last only until the SST anomalies conditions are no longer met.\n    The original JMA COAPS index uses ``full_year=True``, while the newer\n    index uses ``full_year=False``.\n\n\"\"\"\n#\n", "func_signal": "def load_jma(mode='COAPS', **options):\n", "code": "ensoarchive = dict(config.items('ENSO'))['ensoarchive']\nif ensoarchive[-4:].lower() != '.zip':\n    ensoarchive += '.zip'\n# Check the mode and load the proper options\nmodes = ('coaps', 'standard', 'sliding')\nmode = mode.lower()\nif mode not in modes:\n    raise ValueError(\"Invalid '%s' mode for the JMA indices: \"\\\n                     \"it should be in %s\" % (mode, modes))\nif mode == 'coaps':\n    cfg = dict(config.items('ENSO.JMA.COAPS'))\nelif mode == 'standard':\n    cfg = dict(config.items('ENSO.JMA.Standard'))\nelif mode == 'sliding':\n    cfg = dict(config.items('ENSO.JMA.Sliding'))\nelse:\n    raise ValueError(\"Invalid '%s' mode for the JMA indices: \"\\\n                     \"it should be in %s\" % (mode, modes))\ncfg.update(options)\n#\ndatadir = cfg['datadir']\nnetfile = cfg['netfile']\narchive = cfg['archive']\nscalefactor = float(cfg.get('scalefactor', 1.))\n#\ntry:\n    zipf = zipfile.ZipFile(ensoarchive, 'r')\n    ensoi = cPickle.loads(zipf.read(archive))\n    errmsg = \"... Loading from existing archived file '%s'\" % ensoarchive\n    ensologger.info(errmsg)\nexcept (IOError, ValueError):\n    zipf = zipfile.ZipFile(ensoarchive, 'w')\n    ensologger.info(\"... Creating archive\")\nexcept KeyError:\n    zipf = zipfile.ZipFile(ensoarchive, 'a')\n    ensologger.info(\"... Appending to archive\")\nelse:\n    if isinstance(ensoi, enso.ENSOIndicator):\n        ensoi.set_indices()\n        return ensoi\n#\nsourcedir = np.lib._datasource.DataSource(datadir)\ndfile = sourcedir.open(netfile)\n#\ntmp = ts.tsfromtxt(dfile, delimiter=None, dtype=float, datecols=0, freq='A',\n                   skiprows=int(cfg.get('skipline', 0)),\n                   missing=cfg.get('nodata', '999'),)\nif tmp.varshape != (12,):\n    msg = \"Unrecognized shape: it should have been (x, 12).\"\n    raise TypeError(msg)\nensoi = enso.ENSOIndicator(tmp._series.ravel(),\n                           start_date=tmp._dates[0].asfreq('M', 'START'))\nensoi *= scalefactor\n_set_ensoindicator_options(ensoi, **cfg)\nensoi.set_indices()\n#\n# Store in the archive\nensologger.info(\"... Saving to archive\")\nzipf.writestr(archive, cPickle.dumps(ensoi))\nzipf.close()\nreturn ensoi", "path": "scikits\\hydroclimpy\\enso\\ensodata.py", "repo_name": "pierregm/scikits.hydroclimpy", "stars": 13, "license": "None", "language": "python", "size": 602}
{"docstring": "\"\"\"\nSets some options of an :class:`~scikits.hydroclimpy.enso.ENSOIndicator`\n\n\"\"\"\n# Define some defaults..................\n", "func_signal": "def _set_ensoindicator_options(ensoindicator, **options):\n", "code": "defaults = dict(ensotag='(undefined)',\n                full_year=False,\n                thresholds=(None, None),\n                minimum_size=None,\n                reference_season=None,\n                reference_period=None,\n                )\noptinfo = ensoindicator.optinfo\n# Set the defaults......................\ndefaults.update([(k, v) for (k, v) in options.items() if k in defaults])\noptinfo.update(defaults)\n# Fix some issues.......................\nensoindicator.minimum_size = optinfo['minimum_size']\nensoindicator.refseason = optinfo['reference_season']\ntry:\n    ensoindicator.full_year = eval(optinfo['full_year'])\nexcept NameError:\n    optinfo['full_year'] = False\nexcept TypeError:\n    ensoindicator.full_year = optinfo['full_year']\n#\nrefstring = (optinfo['reference_period'] or '').split(',')\nif refstring == ['']:\n    ensoindicator.refperiod = None\nelse:\n    if len(refstring) != 2 and refstring != ['']:\n        raise ValueError(\"The starting and ending dates of the reference \"\n                         \"period should be separated bv a comma\")\n    start_date = refstring[0]\n    if len(start_date) == 4:\n        start_date = Date('M', year=int(start_date), month=1)\n    elif len(start_date) >= 7:\n        start_date = Date('M', year=int(start_date[:4]),\n                               month=int(start_date[5:7]))\n    else:\n        start_date = None\n    end_date = refstring[1]\n    if len(end_date) == 4:\n        end_date = Date('M', year=int(end_date), month=12)\n    elif len(end_date) >= 7:\n        end_date = Date('M', year=int(end_date[:4]),\n                             month=int(end_date[5:7]))\n    else:\n        end_date = None\n    ensoindicator.refperiod = (start_date, end_date)\n#\nopt_thresholds = optinfo['thresholds']\nif opt_thresholds != (None, None):\n    if isinstance(opt_thresholds, tuple):\n        ensoindicator.thresholds = opt_thresholds\n    else:\n        ensoindicator.thresholds = eval(opt_thresholds)\nreturn", "path": "scikits\\hydroclimpy\\enso\\ensodata.py", "repo_name": "pierregm/scikits.hydroclimpy", "stars": 13, "license": "None", "language": "python", "size": 602}
{"docstring": "\"\"\"\nDraws parallels on the given basemap and axis.\n\nParameters\n----------\nbasemap : Basemap object\n    Basemap on which to draw.\nparallels : {None, sequence}, optional\n    List of meridians to draw. \n    If None, default parallels are determined from the current basemap \n    at the given resolution.\nresol : {float}, optional\n    Space between two parallels (in degrees).\nax : {None, Axe}, optional\n    Axe on which to draw the parallels.\n    If None, the current axe is selected.\n\"\"\"\n", "func_signal": "def draw_default_parallels(basemap, parallels=None, resol=1., ax=None, **kwargs):\n", "code": "if parallels is None:\n    parallels = get_default_parallels(basemap, resol=resol)\nif ax is None:\n    ax = pyplot.gca()\nlabels = [int(ax.is_first_col()), int(ax.is_last_col()), 0, 0]\nkwargs.update(dict(labels=labels, ax=ax))\nbasemap.drawparallels(parallels, **kwargs)\nreturn", "path": "scikits\\hydroclimpy\\plotlib\\maptools.py", "repo_name": "pierregm/scikits.hydroclimpy", "stars": 13, "license": "None", "language": "python", "size": 602}
{"docstring": "\"\"\"\n    Sets the ENSO indices per periods of 12 months, starting at the first \n    month of the reference season if any, otherwise at October.\n\n    The same steps are followed as for :meth:`set_monthly_indices`.\n\n    Parameters\n    ----------\n    minimum_size : {None, int}, optional\nMinimum size for the groups of consecutive values.\nIf None, defaults to :attr:`minimum_size`.\n    reference_season : {None, string or sequence}, optional\nReference season.\nIf None, defaults to :attr:`self.reference_season`.\n    lag : {integer}, optional\nNumber of months of lag for ENSO indices. For example,\nif lag=6, the ENSO phase starting in Oct. 2001 is applied starting on \nApr. 2002.\n\n    See Also\n    --------\n    :meth:`set_monthly_indices`\nSets the ENSO indices for each month.\n\n    \"\"\"\n", "func_signal": "def set_annual_indices(self, minimum_size=None, reference_season=None, lag=0):\n", "code": "return self.set_indices(full_year=True,\n                        reference_season=reference_season,\n                        minimum_size=minimum_size, lag=lag)", "path": "scikits\\hydroclimpy\\enso\\ensobase.py", "repo_name": "pierregm/scikits.hydroclimpy", "stars": 13, "license": "None", "language": "python", "size": 602}
{"docstring": "\"\"\"\n    Returns the indices of the ENSO indicator grouped by values.\n\n\"\"\"\n", "func_signal": "def _clustered_values(self):\n", "code": "if self._cachedclustered is None:\n    # Mask data if less than 6 consecutive months with the same index\n    # Remmbr: `longenough` is a MASK, so True means \"NOT in a cluster\"\n    _monthly = ma.zeros(self.size, dtype=int)\n    _monthly._mask = self._mask\n    _monthly[(self >= self.thresholds[-1]).filled(False)] = +1\n    _monthly[(self <= self.thresholds[0]).filled(False)] = -1\n    self._cachedclustered = Cluster(_monthly.filled(), increment=0)\nreturn self._cachedclustered", "path": "scikits\\hydroclimpy\\enso\\ensobase.py", "repo_name": "pierregm/scikits.hydroclimpy", "stars": 13, "license": "None", "language": "python", "size": 602}
{"docstring": "\"\"\"abfunc(fillx, filly) must be defined.\n   abinop(x, filly) = x for all x to enable reduce.\n\"\"\"\n", "func_signal": "def __init__ (self, methodname, onindices=True):\n", "code": "self.__name__ = methodname\nself._onindices = onindices\nif __doc__:\n    self.__doc__ = getattr(TimeSeries, methodname).__doc__ + \\\n                   _csmethod.__extradoc__\nself.obj = None", "path": "scikits\\hydroclimpy\\enso\\ensobase.py", "repo_name": "pierregm/scikits.hydroclimpy", "stars": 13, "license": "None", "language": "python", "size": 602}
{"docstring": "\"\"\"\n    Sets the ENSO indices per periods of 12 months, starting at the first \n    month of the reference season if any, otherwise at October.\n\n    The same steps are followed as for :meth:`set_monthly_indices`.\n\n    Parameters\n    ----------\n    minimum_size : {None, int}, optional\nMinimum size for the groups of consecutive values.\nIf None, defaults to :attr:`minimum_size`.\n    reference_season : {None, string or sequence}, optional\nReference season.\nIf None, defaults to :attr:`reference_season`.\n\n    See Also\n    --------\n    :meth:`set_monthly_indices`\nSets the ENSO indices for each month.\n\n    \"\"\"\n# Get the monthly indices .....\n", "func_signal": "def _set_annual_indices(self, minimum_size=None, reference_season=None):\n", "code": "_monthly = self.set_monthly_indices(minimum_size=minimum_size,\n                                    reference_season=reference_season)\n# Make sure we reset the full_year flag to True (we lost it w/ set_monthly\nself.full_year = True\n# Get the annual indices\nrefseason = self.refseason\nif refseason:\n    _annual = _monthly[self.months == refseason[0]]\n    refseason = months2code(refseason)\nelse:\n    _annual = _monthly[self.months == 10]\n_annual = adjust_endpoints(forward_fill(fill_missing_dates(_annual)),\n                           self._dates[0], self._dates[-1])\n# Cache the results ...........\nself._cachedmonthly['indices_annual'] = _annual\nreturn _annual", "path": "scikits\\hydroclimpy\\enso\\ensobase.py", "repo_name": "pierregm/scikits.hydroclimpy", "stars": 13, "license": "None", "language": "python", "size": 602}
{"docstring": "\"\"\"\nReturns the CDF values for aseries.\n\nParameters\n----------\naseries : TimeSeries\n    Annual series of data (one column per period)\ncondition : TimeSeries\n    Period mask.\n\"\"\"\n# Mask the months for which no precipitations were recorded\n", "func_signal": "def _get_gamma_cdf(aseries, condition):\n", "code": "aseries_ = ma.masked_values(aseries, 0)\n# Get the proportion of 0 precipitation for each period (MM/WW)\npzero = 1. - aseries_.count(axis=0) / aseries.count(axis=0).astype(float)\n# Mask outside the reference period\naseries_._mask |= condition._data\nmeanrain = aseries_.mean(axis=0)\naleph = ma.log(meanrain) - ma.log(aseries_).mean(axis=0)\nalpha = (1. + ma.sqrt(1.+4./3*aleph)) / (4.*aleph)\nbeta = meanrain/alpha\n# Get the Gamma CDF (per month)\ngcdf = pzero + (1.-pzero) * ssd.gamma.cdf(aseries,alpha,scale=beta)\nreturn gcdf", "path": "scikits\\hydroclimpy\\lib\\spi.py", "repo_name": "pierregm/scikits.hydroclimpy", "stars": 13, "license": "None", "language": "python", "size": 602}
{"docstring": "\"\"\"\n    Sets the ENSO indices per groups of months.\n    \n    In a first step, each month is first qualified as +1 (-1) if the\n    corresponding value of the indicator is greater (less) than the upper (lower)\n    threshold, or as 0 otherwise.\n    Consecutive values are then grouped according to their value.\n    If the size of a group is less than the ``minimum_size`` parameters, \n    the value of each element of this group is set to 0.\n    \n    In a third step, groups of size larger than ``minimum_size`` are checked\n    whether they cover the reference season.\n    If they don't, the value of each of their elements is then also set to 0.\n\n    Parameters\n    ----------\n    minimum_size : {None, int}, optional\nMinimum size for the groups of consecutive values.\nIf None, defaults to ``self.minimum_size``.\n    reference_season : {None, string or sequence}, optional\nReference season.\nIf None, defaults to ``self.reference_season``.\n\n    Notes\n    -----\n    * If the ``reference_season`` parameter is a string, it should be correspond \n      to a series of consecutive months. For example, 'JFM', 'OND', 'NDJF' are\n      all valid reference seasons (Jan. to Mar., Oct. to Dec., Nov. to Feb. \n      respectively), but 'JAM', 'SND' or 'XYZ' are not.\n    * If the ``reference_season`` parameter is a sequence, it should be a sequence\n      of integers between 1 (Jan.) and 12 (dec.)\n\n\"\"\"\n# Get the cluster size ........\n", "func_signal": "def _set_monthly_indices(self, minimum_size=None, reference_season=None):\n", "code": "_minimum_size = self.optinfo.get('minimum_size', None)\nminsize = minimum_size or _minimum_size\nif minsize != _minimum_size:\n    self.minimum_size = minsize\n# Check the season ............\n_refseason = self.optinfo.get('reference_season', None)\nrefseason = reference_season or _refseason\nif tuple(_refseason or ()) != tuple(refseason or ()):\n    self.refseason = refseason\nrefseason = self.optinfo.get('reference_season', None)\n# Flatten the area (it should be 1D anyway), but save the initial shape\nif self._varshape != ():\n    raise NotImplementedError(\"The ENSO indicator should be 1D!\")\ninishape = self.shape\nself.shape = -1\n# Check that the frequency is actually monthly\nif self.freq == 'M':\n    klust = self._clustered_values()\n    (_mask, _dates, months) = (self._mask, self._dates, self.months)\nelse:\n    # OK, so force to monthly for now...\n    _self = self.convert('M', func=ma.mean)\n    _self._optinfo = self._optinfo\n    klust = _self._clustered_values()\n    (_mask, _dates, months) = (_self._mask, _self._dates, _self.months)\n_indices = np.concatenate(klust.clustered)\n# Find the clusters larger than minimum_size\n_slices = klust.slices[klust.sizes >= minsize]\n# Find the large clusters that fall after min_month\nvalid = np.zeros(months.shape, bool)\nif refseason is not None:\n    for s in _slices:\n        valid[s] = np.logical_and.reduce([(months[s] == m).any()\n                                          for m in refseason])\nelse:\n    for s in _slices:\n        valid[s] = True\n_indices[np.logical_not(valid)] = 0\n_indices = _indices.view(TimeSeries)\n_indices._mask = _mask\n_indices._dates = _dates\n# Cache the results............\nself._cachedmonthly['indices_monthly'] = _indices\n# And restore the shape\nself.shape = inishape\nreturn _indices", "path": "scikits\\hydroclimpy\\enso\\ensobase.py", "repo_name": "pierregm/scikits.hydroclimpy", "stars": 13, "license": "None", "language": "python", "size": 602}
{"docstring": "\"\"\"\n    Returns the indices currently cached in :attr:'_cachedcurrent'.\n\n    If the ENSO indices are not set (:attr:'_cachedcurrent' is None), they are\n    set with the :meth:'set_indices' method.\n\"\"\"\n", "func_signal": "def get_indices(self):\n", "code": "if self._cachedcurrent is not None:\n    self._cachedcurrent = self._fix_cachedcurrent(self._cachedcurrent)\n    return self._cachedcurrent\nreturn self.set_indices()", "path": "scikits\\hydroclimpy\\enso\\ensobase.py", "repo_name": "pierregm/scikits.hydroclimpy", "stars": 13, "license": "None", "language": "python", "size": 602}
{"docstring": "\"\"\"\nReturns an array of default parallels at the given resolution\nfor the current basemap.\n\nParameters\n----------\nbasemap : Basemap object\n    Basemap on which to draw.\nresol : {float}, optional\n    Space between two parallels (in degrees).\n\"\"\"\n", "func_signal": "def get_default_parallels(basemap, resol=1):\n", "code": "return np.arange(np.floor(basemap.llcrnrlat / 5.) * 5.,\n                 np.ceil(basemap.urcrnrlat / 5.) * 5., float(resol))", "path": "scikits\\hydroclimpy\\plotlib\\maptools.py", "repo_name": "pierregm/scikits.hydroclimpy", "stars": 13, "license": "None", "language": "python", "size": 602}
{"docstring": "\"\"\"\nDownloads hydrological data from the USGS site.\n\nParameters\n----------\nsite_no : str\n    2- to 15-digit identification number of the site whose information must\n    be downloaded.\nnetfile : {string}, optional\n    URL of the USGS flow gage site.\n    By default, the value is read from the configuration file.\nbegin_date : {string}, optional\n    First date to be retrieved.\n    By default, the value is read from the configuration file.\nend_date : {None, string}, optional\n    Last date to be retrieved.\n    If None, uses the current date.\ndatadir : {None, string}, optional\n    Directory where to store the output.\n    If None, uses the current directory.\ncommentchar : {string}, optional\n    Character corresponding to a comment.\nheaderlines : {int}, optional\n    Number of uncommented lines to skip from the beginning of the file.\narchive : {None, string}, optional\n    Name of the ZIP archive where to read pre-stored data or when to store \n    data newly retrieved.\n    The default is read from the configuration file.\n\nNotes\n-----\nThe list of data codes valid for the ``data_type`` parameters are available\non the `USGS site <http://waterdata.usgs.gov/nwis/pmcodes/pmcodes?pm_group=All+--+include+all+parameter+groups&pm_search=&format=html_table&show=parameter_group_nm&show=parameter_nm>`_\n\n\"\"\"\n# Force the site_no to a 0-padded, 8-character string\n", "func_signal": "def load_usgs_flows(site_no, cfgdict=usgs_config):\n", "code": "site_no = \"%08i\" % int(site_no)\n#data_type = cfgdict.get('data_type', '00060')\ndata_type = '00060'\ndata_letter = data_dict[data_type]\nusgsarchv = _check_archive(cfgdict)\narchfname = \"%s%sD\" % (data_letter, site_no)\n#\ntry:\n    zipf = zipfile.ZipFile(usgsarchv, 'r')\n    series = cPickle.loads(zipf.read(archfname))\nexcept IOError:\n    zipf = zipfile.ZipFile(usgsarchv, 'w')\nexcept KeyError:\n    zipf = zipfile.ZipFile(usgsarchv, 'a')\nelse:\n    if series.size > 0:\n        return series\n    else:\n        zipf = zipfile.ZipFile(usgsarchv, 'a')\n#\noptions = dict(usgs_site=cfgdict['netfile'],\n               begin_date=cfgdict['begin_date'],\n               data_type=data_type,\n               site_no= site_no)\nend_date = cfgdict.get('end_date', None)\nif end_date is None:\n    end_date = ts.now('D').strftime(\"%Y-%m-%d\")\noptions['end_date'] = end_date\ncommd = \"%(usgs_site)s/dv?format=rdb&cb_%(data_type)s=on&\" + \\\n        \"begin_date=%(begin_date)s&end_date=%(end_date)s&\" + \\\n        \"site_no=%(site_no)s\"\ndatadir = cfgdict.get('datadir', os.getcwd())\n# We can't use np.lib._datasource.DataSource, the downloaded file has always\n# the same name and would be cached. \nsourcedir = np.lib._datasource.DataSource(datadir)\ndfile = urllib.urlopen(commd % options)\n#\nheaders = int(cfgdict['headerlines'])\n#\n(datelist, datalist) = ([], [])\nfor line in dfile.readlines():\n    line = line.strip().split(\"\\t\")\n    if (not line) or (line[0][0] == '#'):\n        continue\n    try:\n        datalist.append(line[3])\n    except IndexError:\n        continue\n    else:\n        datelist.append(line[2])\nseries = ts.time_series([float(_) for _ in datalist[headers:]],\n                        dates=[ts.Date('D', _) for _ in datelist[headers:]],\n                        freq='D')\n#\nzipf.writestr(archfname, cPickle.dumps(series))\nzipf.close()\nreturn series", "path": "scikits\\hydroclimpy\\io\\usgs.py", "repo_name": "pierregm/scikits.hydroclimpy", "stars": 13, "license": "None", "language": "python", "size": 602}
{"docstring": "\"\"\"\nDraws meridians on the given basemap and axis.\n\nParameters\n----------\nbasemap : Basemap object\n    Basemap on which to draw.\nmeridians : {None, sequence}, optional\n    List of meridians to draw. \n    If None, default meridians are determined from the current basemap \n    at the given resolution.\nresol : {float}, optional\n    Space between two parallels (in degrees).\nax : {None, Axe}, optional\n    Axe on which to draw the parallels.\n    If None, the current axe is selected.\n\"\"\"\n", "func_signal": "def draw_default_meridians(basemap, meridians=None, resol=1., ax=None, **kwargs):\n", "code": "if meridians is None:\n    meridians = get_default_meridians(basemap, resol=resol)\nif ax is None:\n    ax = pyplot.gca()\nlabels = [0, 0, int(ax.is_first_row()), int(ax.is_last_row())]\nkwargs.update(dict(labels=labels, ax=ax))\nbasemap.drawmeridians(meridians, **kwargs)\nreturn", "path": "scikits\\hydroclimpy\\plotlib\\maptools.py", "repo_name": "pierregm/scikits.hydroclimpy", "stars": 13, "license": "None", "language": "python", "size": 602}
{"docstring": "\"\"\"\nOutput the graph for HTML.  This will insert a PNG with clickable\nimage map.\n\"\"\"\n", "func_signal": "def html_output_graph(self, node):\n", "code": "graph = node['graph']\nparts = node['parts']\n\ngraph_hash = get_graph_hash(node)\nname = \"inheritance%s\" % graph_hash\npng_path = os.path.join('_static', name + \".png\")\n\npath = '_static'\nsource = self.document.attributes['source']\ncount = source.split('/doc/')[-1].count('/')\nfor i in range(count):\n    if os.path.exists(path): break\n    path = '../'+path\npath = '../'+path #specifically added for matplotlib\n\n# Create a mapping from fully-qualified class names to URLs.\nurls = {}\nfor child in node:\n    if child.get('refuri') is not None:\n        urls[child['reftitle']] = child.get('refuri')\n    elif child.get('refid') is not None:\n        urls[child['reftitle']] = '#' + child.get('refid')\n\n# These arguments to dot will save a PNG file to disk and write\n# an HTML image map to stdout.\nimage_map = graph.run_dot(['-Tpng', '-o%s' % png_path, '-Tcmapx'],\n                          name, parts, urls)\nreturn ('<img src=\"%s/%s.png\" usemap=\"#%s\" class=\"inheritance\"/>%s' %\n        (path, name, name, image_map))", "path": "source\\sphinxext\\inheritance_diagram.py", "repo_name": "GaelVaroquaux/scipy-tutorials", "stars": 12, "license": "None", "language": "python", "size": 8429}
{"docstring": "\"\"\"Parallel FastICA.\n\nUsed internally by FastICA.\n\n\"\"\"\n", "func_signal": "def _ica_par(X, tol, g, gprime, fun_args, maxit, w_init):\n", "code": "n,p = X.shape\n\nW = _sym_decorrelation(w_init)\n\n# we set lim to tol+1 to be sure to enter at least once in next while\nlim = tol + 1 \nit = 0\nwhile ((lim > tol) and (it < (maxit-1))):\n    wtx = np.dot(W, X).A  # .A transforms to array type\n    gwtx = g(wtx, fun_args)\n    g_wtx = gprime(wtx, fun_args)\n    W1 = np.dot(gwtx, X.T)/float(p) - np.dot(np.diag(g_wtx.mean(axis=1)), W)\n \n    W1 = _sym_decorrelation(W1)\n    \n    lim = max(abs(abs(np.diag(np.dot(W1, W.T))) - 1))\n    W = W1\n    it = it + 1\n\nreturn W", "path": "source\\profiling\\ica.py", "repo_name": "GaelVaroquaux/scipy-tutorials", "stars": 12, "license": "None", "language": "python", "size": 8429}
{"docstring": "\"\"\" Quick sort: returns a sorted copy of the list.\n\"\"\"\n", "func_signal": "def qsort(lst):\n", "code": "if len(lst) <= 1:\n    return lst\npivot, rest    = lst[0], lst[1:]\n\n# Could use list comprehension:\n# less_than      = [ lt for lt in rest if lt < pivot ]\n\nless_than = []\nfor lt in rest:\n    if lt < pivot:\n        less_than.append(lt)\n\n# Could use list comprehension:\n# greater_equal  = [ ge for ge in rest if ge >= pivot ]\n\ngreater_equal = []\nfor ge in rest:\n    if ge >= pivot:\n        greater_equal.append(ge)\nreturn qsort(less_than) + [pivot] + qsort(greater_equal)", "path": "source\\solutions\\quick_sort.py", "repo_name": "GaelVaroquaux/scipy-tutorials", "stars": 12, "license": "None", "language": "python", "size": 8429}
{"docstring": "\"\"\"\n*class_names* is a list of child classes to show bases from.\n\nIf *show_builtins* is True, then Python builtins will be shown\nin the graph.\n\"\"\"\n", "func_signal": "def __init__(self, class_names, show_builtins=False):\n", "code": "self.class_names = class_names\nself.classes = self._import_classes(class_names)\nself.all_classes = self._all_classes(self.classes)\nif len(self.all_classes) == 0:\n    raise ValueError(\"No classes found for inheritance diagram\")\nself.show_builtins = show_builtins", "path": "source\\sphinxext\\inheritance_diagram.py", "repo_name": "GaelVaroquaux/scipy-tutorials", "stars": 12, "license": "None", "language": "python", "size": 8429}
{"docstring": "# Test that non '.py' files are not filtered.\n", "func_signal": "def test_filter_and_sort():\n", "code": "file_list = ['a', 'aaa', 'aa', '', 'z', 'zzzzz']\nfile_list2 = dir_sort.filter_and_sort(file_list)\nassert len(file_list2) == 0\n\n# Test that the otuput file list is ordered by length.\nfile_list = [ n + '.py' for n in file_list]\nfile_list2 = dir_sort.filter_and_sort(file_list)\nname1 = file_list2.pop(0)\nfor name in file_list2:\n    assert len(name1) <= len(name)", "path": "source\\solutions\\test_dir_sort.py", "repo_name": "GaelVaroquaux/scipy-tutorials", "stars": 12, "license": "None", "language": "python", "size": 8429}
{"docstring": "\"\"\"Deflationary FastICA using fun approx to neg-entropy function\n\nUsed internally by FastICA.\n\"\"\"\n\n", "func_signal": "def _ica_def(X, tol, g, gprime, fun_args, maxit, w_init):\n", "code": "n_comp = w_init.shape[0]\nW = np.zeros((n_comp, n_comp), dtype=float)\n\n# j is the index of the extracted component\nfor j in range(n_comp):\n    w = w_init[j, :].copy()\n    w /= np.sqrt((w**2).sum())\n\n    n_iterations = 0\n    # we set lim to tol+1 to be sure to enter at least once in next while\n    lim = tol + 1 \n    while ((lim > tol) & (n_iterations < (maxit-1))):\n        wtx = np.dot(w.T, X)\n        gwtx = g(wtx, fun_args)\n        g_wtx = gprime(wtx, fun_args)\n        w1 = (X * gwtx).mean(axis=1) - g_wtx.mean() * w\n        \n        _gs_decorrelation(w1, W, j)\n        \n        w1 /= np.sqrt((w1**2).sum())\n\n        lim = np.abs(np.abs((w1 * w).sum()) - 1)\n        w = w1\n        n_iterations = n_iterations + 1\n        \n    W[j, :] = w\n\nreturn W", "path": "source\\profiling\\ica.py", "repo_name": "GaelVaroquaux/scipy-tutorials", "stars": 12, "license": "None", "language": "python", "size": 8429}
{"docstring": "\"\"\"close() -> parent\n\nClose element and return first non-full element.\"\"\"\n\n", "func_signal": "def close(self):\n", "code": "parent = self.parent\nwhile parent.full():\n    parent = parent.parent\nreturn parent", "path": "source\\sphinxext\\mathml.py", "repo_name": "GaelVaroquaux/scipy-tutorials", "stars": 12, "license": "None", "language": "python", "size": 8429}
{"docstring": "\"\"\"\nReturn a list of all classes that are ancestors of *classes*.\n\"\"\"\n", "func_signal": "def _all_classes(self, classes):\n", "code": "all_classes = {}\n\ndef recurse(cls):\n    all_classes[cls] = None\n    for c in cls.__bases__:\n        if c not in all_classes:\n            recurse(c)\n\nfor cls in classes:\n    recurse(cls)\n\nreturn all_classes.keys()", "path": "source\\sphinxext\\inheritance_diagram.py", "repo_name": "GaelVaroquaux/scipy-tutorials", "stars": 12, "license": "None", "language": "python", "size": 8429}
{"docstring": "\"\"\"append(child) -> element\n\nAppends child and returns self if self is not full or first\nnon-full parent.\"\"\"\n\n", "func_signal": "def append(self, child):\n", "code": "assert not self.full()\nself.children.append(child)\nchild.parent = self\nnode = self\nwhile node.full():\n    node = node.parent\nreturn node", "path": "source\\sphinxext\\mathml.py", "repo_name": "GaelVaroquaux/scipy-tutorials", "stars": 12, "license": "None", "language": "python", "size": 8429}
{"docstring": "\"\"\"parse_latex_math(string [,inline]) -> MathML-tree\n\nReturns a MathML-tree parsed from string.  inline=True is for\ninline math and inline=False is for displayed math.\n\ntree is the whole tree and node is the current element.\"\"\"\n\n# Normalize white-space:\n", "func_signal": "def parse_latex_math(string, inline=True):\n", "code": "string = ' '.join(string.split())\n\nif inline:\n    node = mrow()\n    tree = math(node, inline=True)\nelse:\n    node = mtd()\n    tree = math(mtable(mtr(node)), inline=False)\n\nwhile len(string) > 0:\n    n = len(string)\n    c = string[0]\n    skip = 1  # number of characters consumed\n    if n > 1:\n        c2 = string[1]\n    else:\n        c2 = ''", "path": "source\\sphinxext\\mathml.py", "repo_name": "GaelVaroquaux/scipy-tutorials", "stars": 12, "license": "None", "language": "python", "size": 8429}
{"docstring": "\"\"\"math([children]) -> MathML element\n\nchildren can be one child or a list of children.\"\"\"\n\n", "func_signal": "def __init__(self, children=None, inline=None):\n", "code": "self.children = []\nif children is not None:\n    if type(children) is list:\n        for child in children:\n            self.append(child)\n    else:\n        # Only one child:\n        self.append(children)\n\nif inline is not None:\n    self.inline = inline", "path": "source\\sphinxext\\mathml.py", "repo_name": "GaelVaroquaux/scipy-tutorials", "stars": 12, "license": "None", "language": "python", "size": 8429}
{"docstring": "\"\"\" Out of a list of file names, returns only the ones ending by\n    '.py', ordered with increasing file name length.\n\"\"\"\n", "func_signal": "def filter_and_sort(file_list):\n", "code": "file_list = [filename for filename in file_list \n                      if filename.endswith('.py')]\n\ndef key(item):\n    return len(item)\n\nfile_list.sort(key=key)\nreturn file_list", "path": "source\\solutions\\dir_sort.py", "repo_name": "GaelVaroquaux/scipy-tutorials", "stars": 12, "license": "None", "language": "python", "size": 8429}
{"docstring": "\"\"\"\nRun when the inheritance_diagram directive is first encountered.\n\"\"\"\n", "func_signal": "def inheritance_diagram_directive_run(class_names, options, state):\n", "code": "node = inheritance_diagram()\n\n# Create a graph starting with the list of classes\ngraph = InheritanceGraph(class_names)\n\n# Create xref nodes for each target of the graph's image map and\n# add them to the doc tree so that Sphinx can resolve the\n# references to real URLs later.  These nodes will eventually be\n# removed from the doctree after we're done with them.\nfor name in graph.get_all_class_names():\n    refnodes, x = xfileref_role(\n        'class', ':class:`%s`' % name, name, 0, state)\n    node.extend(refnodes)\n# Store the graph object so we can use it to generate the\n# dot file later\nnode['graph'] = graph\n# Store the original content for use as a hash\nnode['parts'] = options.get('parts', 0)\nnode['content'] = \" \".join(class_names)\nreturn [node]", "path": "source\\sphinxext\\inheritance_diagram.py", "repo_name": "GaelVaroquaux/scipy-tutorials", "stars": 12, "license": "None", "language": "python", "size": 8429}
{"docstring": "\"\"\" Symmetric decorrelation \"\"\"\n", "func_signal": "def _sym_decorrelation(W):\n", "code": "K = np.dot(W, W.T)\ns, u = np.linalg.eigh(K) \n# u (resp. s) contains the eigenvectors (resp. square roots of \n# the eigenvalues) of W * W.T \nu, W = [np.asmatrix(e) for e in (u, W)]\nW = (u * np.diag(1.0/np.sqrt(s)) * u.T) * W  # W = (W * W.T) ^{-1/2} * W\nreturn W", "path": "source\\profiling\\ica.py", "repo_name": "GaelVaroquaux/scipy-tutorials", "stars": 12, "license": "None", "language": "python", "size": 8429}
{"docstring": "\"\"\"\nImport a list of classes.\n\"\"\"\n", "func_signal": "def _import_classes(self, class_names):\n", "code": "classes = []\nfor name in class_names:\n    classes.extend(self._import_class_or_module(name))\nreturn classes", "path": "source\\sphinxext\\inheritance_diagram.py", "repo_name": "GaelVaroquaux/scipy-tutorials", "stars": 12, "license": "None", "language": "python", "size": 8429}
{"docstring": "\"\"\"\nGiven a class object, return a fully-qualified name.  This\nworks for things I've tested in matplotlib so far, but may not\nbe completely general.\n\"\"\"\n", "func_signal": "def class_name(self, cls, parts=0):\n", "code": "module = cls.__module__\nif module == '__builtin__':\n    fullname = cls.__name__\nelse:\n    fullname = \"%s.%s\" % (module, cls.__name__)\nif parts == 0:\n    return fullname\nname_parts = fullname.split('.')\nreturn '.'.join(name_parts[-parts:])", "path": "source\\sphinxext\\inheritance_diagram.py", "repo_name": "GaelVaroquaux/scipy-tutorials", "stars": 12, "license": "None", "language": "python", "size": 8429}
{"docstring": "\"\"\"delete_child() -> child\n\nDelete last child and return it.\"\"\"\n\n", "func_signal": "def delete_child(self):\n", "code": "child = self.children[-1]\ndel self.children[-1]\nreturn child", "path": "source\\sphinxext\\mathml.py", "repo_name": "GaelVaroquaux/scipy-tutorials", "stars": 12, "license": "None", "language": "python", "size": 8429}
{"docstring": "\"\"\"\nImport a class using its fully-qualified *name*.\n\"\"\"\n", "func_signal": "def _import_class_or_module(self, name):\n", "code": "try:\n    path, base = self.py_sig_re.match(name).groups()\nexcept:\n    raise ValueError(\n        \"Invalid class or module '%s' specified for inheritance diagram\" % name)\nfullname = (path or '') + base\npath = (path and path.rstrip('.'))\nif not path:\n    path = base\ntry:\n    module = __import__(path, None, None, [])\nexcept ImportError:\n    raise ValueError(\n        \"Could not import class or module '%s' specified for inheritance diagram\" % name)\n\ntry:\n    todoc = module\n    for comp in fullname.split('.')[1:]:\n        todoc = getattr(todoc, comp)\nexcept AttributeError:\n    raise ValueError(\n        \"Could not find class or module '%s' specified for inheritance diagram\" % name)\n\n# If a class, just return it\nif inspect.isclass(todoc):\n    return [todoc]\nelif inspect.ismodule(todoc):\n    classes = []\n    for cls in todoc.__dict__.values():\n        if inspect.isclass(cls) and cls.__module__ == todoc.__name__:\n            classes.append(cls)\n    return classes\nraise ValueError(\n    \"'%s' does not resolve to a class or module\" % name)", "path": "source\\sphinxext\\inheritance_diagram.py", "repo_name": "GaelVaroquaux/scipy-tutorials", "stars": 12, "license": "None", "language": "python", "size": 8429}
{"docstring": "\"\"\" Gram-Schmidt-like decorrelation. \"\"\"\n", "func_signal": "def _gs_decorrelation(w, W, j):\n", "code": "t = np.zeros_like(w)\nfor u in range(j):\n    t = t + np.dot(w, W[u]) * W[u]\n    w -= t\nreturn w", "path": "source\\profiling\\ica.py", "repo_name": "GaelVaroquaux/scipy-tutorials", "stars": 12, "license": "None", "language": "python", "size": 8429}
{"docstring": "\"\"\"\nOutput the graph for LaTeX.  This will insert a PDF.\n\"\"\"\n", "func_signal": "def latex_output_graph(self, node):\n", "code": "graph = node['graph']\nparts = node['parts']\n\ngraph_hash = get_graph_hash(node)\nname = \"inheritance%s\" % graph_hash\npdf_path = os.path.join('_static', name + \".pdf\")\n\ngraph.run_dot(['-Tpdf', '-o%s' % pdf_path],\n              name, parts, graph_options={'size': '\"6.0,6.0\"'})\nreturn '\\\\includegraphics{../../%s}' % pdf_path", "path": "source\\sphinxext\\inheritance_diagram.py", "repo_name": "GaelVaroquaux/scipy-tutorials", "stars": 12, "license": "None", "language": "python", "size": 8429}
{"docstring": "\"\"\"Puts URL components into the form http(s)://full.host.strinf/uri/path\n\nUsed to construct a roughly canonical URL so that URLs which begin with \n'http://example.com/' can be compared to a uri of '/' when the host is \nset to 'example.com'\n\nIf the uri contains 'http://host' already, the host and ssl parameters\nare ignored.\n\nArgs:\n  uri: str The path component of the URL, examples include '/'\n  host: str (optional) The host name which should prepend the URL. Example:\n      'example.com'\n  ssl: boolean (optional) If true, the returned URL will begin with https\n      instead of http.\n\nReturns:\n  String which has the form http(s)://example.com/uri/string/contents\n\"\"\"\n", "func_signal": "def _ConstructFullUrlBase(uri, host=None, ssl=False):\n", "code": "if uri.startswith('http'):\n  return uri\nif ssl:\n  return 'https://%s%s' % (host, uri)\nelse:\n  return 'http://%s%s' % (host, uri)", "path": "atom\\mock_service.py", "repo_name": "berinhard/YouCover", "stars": 10, "license": "None", "language": "python", "size": 3003}
{"docstring": "\"\"\"Retrieve all groups that belong to the given member_id.\n\nArgs:\n  member_id: The member's email address (e.g. member@example.com).\n  direct_only: Boolean whether only return groups that this member directly belongs to.\n\nReturns:\n  A list containing the result of the retrieve operation.\n\"\"\"\n", "func_signal": "def RetrieveGroups(self, member_id, direct_only=False):\n", "code": "uri = self._ServiceUrl('group', True, '', member_id, '', direct_only=direct_only)\nreturn self._GetPropertiesList(uri)", "path": "gdata\\apps\\groups\\service.py", "repo_name": "berinhard/YouCover", "stars": 10, "license": "None", "language": "python", "size": 3003}
{"docstring": "\"\"\"Retrieve all members in the given group.\n\nArgs:\n  group_id: The ID of the group (e.g. us-sales).\n  suspended_users: A boolean; should we include any suspended users in\n    the membership list returned?\n\nReturns:\n  A list containing the result of the retrieve operation.\n\"\"\"\n", "func_signal": "def RetrieveAllMembers(self, group_id, suspended_users=False):\n", "code": "uri = self._ServiceUrl('member', True, group_id, '', '',\n                       suspended_users=suspended_users)\nreturn self._GetPropertiesList(uri)", "path": "gdata\\apps\\groups\\service.py", "repo_name": "berinhard/YouCover", "stars": 10, "license": "None", "language": "python", "size": 3003}
{"docstring": "\"\"\"Returns a link object which has the desired rel value.\n\nIf you are interested in the URL instead of the link object,\nconsider using find_url instead.\n\"\"\"\n", "func_signal": "def get_link(self, rel):\n", "code": "for link in self.link:\n  if link.rel == rel and link.href:\n    return link\nreturn None", "path": "atom\\data.py", "repo_name": "berinhard/YouCover", "stars": 10, "license": "None", "language": "python", "size": 3003}
{"docstring": "\"\"\"Retrieve all groups in the domain.\n\nArgs:\n  None\n\nReturns:\n  A list containing the result of the retrieve operation.\n\"\"\"\n", "func_signal": "def RetrieveAllGroups(self):\n", "code": "uri = self._ServiceUrl('group', True, '', '', '')\nreturn self._GetPropertiesList(uri)", "path": "gdata\\apps\\groups\\service.py", "repo_name": "berinhard/YouCover", "stars": 10, "license": "None", "language": "python", "size": 3003}
{"docstring": "\"\"\"Create a group.\n\nArgs:\n  group_id: The ID of the group (e.g. us-sales).\n  group_name: The name of the group.\n  description: A description of the group\n  email_permission: The subscription permission of the group.\n\nReturns:\n  A dict containing the result of the create operation.\n\"\"\"\n", "func_signal": "def CreateGroup(self, group_id, group_name, description, email_permission):\n", "code": "uri = self._ServiceUrl('group', False, group_id, '', '')\nproperties = {}\nproperties['groupId'] = group_id\nproperties['groupName'] = group_name\nproperties['description'] = description\nproperties['emailPermission'] = email_permission\nreturn self._PostProperties(uri, properties)", "path": "gdata\\apps\\groups\\service.py", "repo_name": "berinhard/YouCover", "stars": 10, "license": "None", "language": "python", "size": 3003}
{"docstring": "\"\"\"Retrieve the given member in the given group.\n\nArgs:\n  member_id: The member's email address (e.g. member@example.com).\n  group_id: The ID of the group (e.g. us-sales).\n\nReturns:\n  A dict containing the result of the retrieve operation.\n\"\"\"\n", "func_signal": "def RetrieveMember(self, member_id, group_id):\n", "code": "uri = self._ServiceUrl('member', True, group_id, member_id, '')\nreturn self._GetProperties(uri)", "path": "gdata\\apps\\groups\\service.py", "repo_name": "berinhard/YouCover", "stars": 10, "license": "None", "language": "python", "size": 3003}
{"docstring": "\"\"\"Update a group's name, description and/or permission.\n\nArgs:\n  group_id: The ID of the group (e.g. us-sales).\n  group_name: The name of the group.\n  description: A description of the group\n  email_permission: The subscription permission of the group.\n\nReturns:\n  A dict containing the result of the update operation.\n\"\"\"\n", "func_signal": "def UpdateGroup(self, group_id, group_name, description, email_permission):\n", "code": "uri = self._ServiceUrl('group', True, group_id, '', '')\nproperties = {}\nproperties['groupId'] = group_id\nproperties['groupName'] = group_name\nproperties['description'] = description\nproperties['emailPermission'] = email_permission\nreturn self._PutProperties(uri, properties)", "path": "gdata\\apps\\groups\\service.py", "repo_name": "berinhard/YouCover", "stars": 10, "license": "None", "language": "python", "size": 3003}
{"docstring": "\"\"\"Check whether the given member an owner of the given group.\n\nArgs:\n  owner_email: The email address of a group owner.\n  group_id: The ID of the group (e.g. us-sales).\n\nReturns:\n  True if the member is an owner of the given group.  False otherwise.\n\"\"\"\n", "func_signal": "def IsOwner(self, owner_email, group_id):\n", "code": "uri = self._ServiceUrl('owner', True, group_id, '', owner_email)\nreturn self._IsExisted(uri)", "path": "gdata\\apps\\groups\\service.py", "repo_name": "berinhard/YouCover", "stars": 10, "license": "None", "language": "python", "size": 3003}
{"docstring": "\"\"\"Remove the given member from the given group.\n\nArgs:\n  member_id: The member's email address (e.g. member@example.com).\n  group_id: The ID of the group (e.g. us-sales).\n\nReturns:\n  A dict containing the result of the remove operation.\n\"\"\"\n", "func_signal": "def RemoveMemberFromGroup(self, member_id, group_id):\n", "code": "uri = self._ServiceUrl('member', True, group_id, member_id, '')\nreturn self._DeleteProperties(uri)", "path": "gdata\\apps\\groups\\service.py", "repo_name": "berinhard/YouCover", "stars": 10, "license": "None", "language": "python", "size": 3003}
{"docstring": "\"\"\"Retrieve one page of owners of the given group.\n\nArgs:\n  group_id: The ID of the group (e.g. us-sales).\n  suspended_users: A boolean; should we include any suspended users in\n    the ownership list returned?\n  start: The key to continue for pagination through all owners.\n\nReturns:\n  A feed object containing the result of the retrieve operation.\n\"\"\"\n", "func_signal": "def RetrievePageOfOwners(self, group_id, suspended_users=False, start=None):\n", "code": "uri = self._ServiceUrl('owner', True, group_id, '', '',\n                       suspended_users=suspended_users)\nif start is not None:\n  if suspended_users:\n    uri += \"&start=\"+start\n  else:\n    uri += \"?start=\"+start\nproperty_feed = self._GetPropertyFeed(uri)\nreturn property_feed", "path": "gdata\\apps\\groups\\service.py", "repo_name": "berinhard/YouCover", "stars": 10, "license": "None", "language": "python", "size": 3003}
{"docstring": "\"\"\"Retrieve all owners of the given group.\n\nArgs:\n  group_id: The ID of the group (e.g. us-sales).\n  suspended_users: A boolean; should we include any suspended users in\n    the ownership list returned?\n\nReturns:\n  A list containing the result of the retrieve operation.\n\"\"\"\n", "func_signal": "def RetrieveAllOwners(self, group_id, suspended_users=False):\n", "code": "uri = self._ServiceUrl('owner', True, group_id, '', '',\n                       suspended_users=suspended_users)\nreturn self._GetPropertiesList(uri)", "path": "gdata\\apps\\groups\\service.py", "repo_name": "berinhard/YouCover", "stars": 10, "license": "None", "language": "python", "size": 3003}
{"docstring": "\"\"\"Check whether the given member already exists in the given group.\n\nArgs:\n  member_id: The member's email address (e.g. member@example.com).\n  group_id: The ID of the group (e.g. us-sales).\n\nReturns:\n  True if the member exists in the group.  False otherwise.\n\"\"\"\n", "func_signal": "def IsMember(self, member_id, group_id):\n", "code": "uri = self._ServiceUrl('member', True, group_id, member_id, '')\nreturn self._IsExisted(uri)", "path": "gdata\\apps\\groups\\service.py", "repo_name": "berinhard/YouCover", "stars": 10, "license": "None", "language": "python", "size": 3003}
{"docstring": "\"\"\"Retrieve the given owner in the given group.\n\nArgs:\n  owner_email: The email address of a group owner.\n  group_id: The ID of the group (e.g. us-sales).\n\nReturns:\n  A dict containing the result of the retrieve operation.\n\"\"\"\n", "func_signal": "def RetrieveOwner(self, owner_email, group_id):\n", "code": "uri = self._ServiceUrl('owner', True, group_id, '', owner_email)\nreturn self._GetProperties(uri)", "path": "gdata\\apps\\groups\\service.py", "repo_name": "berinhard/YouCover", "stars": 10, "license": "None", "language": "python", "size": 3003}
{"docstring": "\"\"\"Delete a group based on its ID.\n\nArgs:\n  group_id: The ID of the group (e.g. us-sales).\n\nReturns:\n  A dict containing the result of the delete operation.\n\"\"\"\n", "func_signal": "def DeleteGroup(self, group_id):\n", "code": "uri = self._ServiceUrl('group', True, group_id, '', '')\nreturn self._DeleteProperties(uri)", "path": "gdata\\apps\\groups\\service.py", "repo_name": "berinhard/YouCover", "stars": 10, "license": "None", "language": "python", "size": 3003}
{"docstring": "\"\"\"Construct a mock HTTPResponse and set members.\n\nArgs:\n  body: str (optional) The HTTP body of the server's response. \n  status: int (optional) \n  reason: str (optional)\n  headers: dict (optional)\n\"\"\"\n", "func_signal": "def __init__(self, body=None, status=None, reason=None, headers=None):\n", "code": "self.body = body\nself.status = status\nself.reason = reason\nself.headers = headers or {}", "path": "atom\\mock_service.py", "repo_name": "berinhard/YouCover", "stars": 10, "license": "None", "language": "python", "size": 3003}
{"docstring": "\"\"\"Retrieve a group based on its ID.\n\nArgs:\n  group_id: The ID of the group (e.g. us-sales).\n\nReturns:\n  A dict containing the result of the retrieve operation.\n\"\"\"\n", "func_signal": "def RetrieveGroup(self, group_id):\n", "code": "uri = self._ServiceUrl('group', True, group_id, '', '')\nreturn self._GetProperties(uri)", "path": "gdata\\apps\\groups\\service.py", "repo_name": "berinhard/YouCover", "stars": 10, "license": "None", "language": "python", "size": 3003}
{"docstring": "\"\"\"Add a member to a group.\n\nArgs:\n  member_id: The member's email address (e.g. member@example.com).\n  group_id: The ID of the group (e.g. us-sales).\n\nReturns:\n  A dict containing the result of the add operation.\n\"\"\"\n", "func_signal": "def AddMemberToGroup(self, member_id, group_id):\n", "code": "uri = self._ServiceUrl('member', False, group_id, member_id, '')\nproperties = {}\nproperties['memberId'] = member_id\nreturn self._PostProperties(uri, properties)", "path": "gdata\\apps\\groups\\service.py", "repo_name": "berinhard/YouCover", "stars": 10, "license": "None", "language": "python", "size": 3003}
{"docstring": "\"\"\"Check to see if the other_request is equivalent to this request.\n\nUsed to determine if a recording matches an incoming request so that a\nrecorded response should be sent to the client.\n\nThe matching is not exact, only the operation and URL are examined \ncurrently.\n\nArgs:\n  other_request: MockRequest The request which we want to check this\n      (self) MockRequest against to see if they are equivalent.\n\"\"\"\n# More accurate matching logic will likely be required.\n", "func_signal": "def IsMatch(self, other_request):\n", "code": "return (self.operation == other_request.operation and self.uri == \n    other_request.uri)", "path": "atom\\mock_service.py", "repo_name": "berinhard/YouCover", "stars": 10, "license": "None", "language": "python", "size": 3003}
{"docstring": "\"\"\"Retrieve one page of members of a given group.\n\nArgs:\n  group_id: The ID of the group (e.g. us-sales).\n  suspended_users: A boolean; should we include any suspended users in\n    the membership list returned?\n  start: The key to continue for pagination through all members.\n\nReturns:\n  A feed object containing the result of the retrieve operation.\n\"\"\"\n\n", "func_signal": "def RetrievePageOfMembers(self, group_id, suspended_users=False, start=None):\n", "code": "uri = self._ServiceUrl('member', True, group_id, '', '',\n                       suspended_users=suspended_users)\nif start is not None:\n  if suspended_users:\n    uri += \"&start=\"+start\n  else:\n    uri += \"?start=\"+start\nproperty_feed = self._GetPropertyFeed(uri)\nreturn property_feed", "path": "gdata\\apps\\groups\\service.py", "repo_name": "berinhard/YouCover", "stars": 10, "license": "None", "language": "python", "size": 3003}
{"docstring": "# Create a test symbol which is a constant number\n", "func_signal": "def create_constant_test(c):\n", "code": "return (symbol.test,\n        (symbol.or_test,\n         (symbol.and_test,\n          (symbol.not_test,\n           (symbol.comparison,\n            (symbol.expr,\n             (symbol.xor_expr,\n              (symbol.and_expr,\n               (symbol.shift_expr,\n                (symbol.arith_expr,\n                 (symbol.term,\n                  (symbol.factor,\n                   (symbol.power,\n                    (symbol.atom,\n                     (token.NUMBER, str(c))))))))))))))))", "path": "lib\\reinteract\\rewrite.py", "repo_name": "jbaayen/reinteract", "stars": 8, "license": "bsd-2-clause", "language": "python", "size": 836}
{"docstring": "# Make sure our \"mutation\" isn't something like \"asdfa\".length(); we\n# will miss some valid mutations that we could handle like\n# (some_list).append(5). If such cases ever become and issue, we\n# could add some code here to simplify them into a \"normal\" form.\n", "func_signal": "def add_mutated(self, path):\n", "code": "if path[0][1][0] == token.NAME:\n    if not path in self.mutated:\n        self.mutated.append(path)", "path": "lib\\reinteract\\rewrite.py", "repo_name": "jbaayen/reinteract", "stars": 8, "license": "bsd-2-clause", "language": "python", "size": 836}
{"docstring": "# stmt: simple_stmt | compound_stmt\n", "func_signal": "def _rewrite_stmt(t, state):\n", "code": "return _rewrite_tree(t, state,\n                     { symbol.simple_stmt:   _rewrite_simple_stmt,\n                       symbol.compound_stmt: _rewrite_compound_stmt })", "path": "lib\\reinteract\\rewrite.py", "repo_name": "jbaayen/reinteract", "stars": 8, "license": "bsd-2-clause", "language": "python", "size": 836}
{"docstring": "\"\"\"Figure out what path points to, and open it appropriately\"\"\"\n\n", "func_signal": "def open_path(self, path):\n", "code": "absolute = os.path.abspath(path)\nbasename, dirname = os.path.split(absolute)\n\nif basename.lower() == \"index.rnb\":\n    notebook_path, relative = dirname, None\nelse:\n    notebook_path, relative = self.find_notebook_path(absolute)\n\nif notebook_path:\n    window = self.open_notebook(notebook_path)\n    if relative and relative in window.notebook.files:\n        window.open_file(window.notebook.files[relative])\nelse:\n    global EditorWindow\n    from editor_window import EditorWindow\n\n    window = EditorWindow()\n    if not window.load(absolute):\n        window.window.destroy()\n        return False\n\n    window.show()\n    self.windows.add(window)\n\nreturn True", "path": "lib\\reinteract\\application.py", "repo_name": "jbaayen/reinteract", "stars": 8, "license": "bsd-2-clause", "language": "python", "size": 836}
{"docstring": "# First add prefixes - if a.b.c is mutated, then we need to\n# shallow-copy first a and then a.b\n\n", "func_signal": "def _compile_mutations(paths, copy_func_name):\n", "code": "paths_to_copy = set()\n\n# path: atom trailer*\n# trailer: '(' [arglist] ')' | '[' subscriptlist ']' | '.' NAME\n#\n# We normally chop of trailers one by one, but if we have\n# .NAME(...) (two trailers) then we chop that off as one piece\n#\nfor path in paths:\n    while True:\n        # Dont' try to copy things that don't look like they\n        # can be assigned to\n        if path[-1][1][0] != token.LPAR:\n            paths_to_copy.add(path)\n\n        if len(path) == 1:\n            break\n\n        if (path[-1][1][0] == token.LPAR and\n            len(path) > 2 and\n            path[-2][1][0] == token.DOT):\n\n            path = path[0:-2]\n        else:\n            path = path[0:-1]\n\n# Sort the paths with shorter paths earlier so that we copy prefixes\n# before longer versions\npaths_to_copy = sorted(paths_to_copy, lambda x,y: cmp(len(x),len(y)))\n\nreturn [(_get_path_root(path),\n         _describe_path(path),\n         _compile_copy_code(path, copy_func_name)) for path in paths_to_copy]", "path": "lib\\reinteract\\rewrite.py", "repo_name": "jbaayen/reinteract", "stars": 8, "license": "bsd-2-clause", "language": "python", "size": 836}
{"docstring": "# file_input: (NEWLINE | stmt)* ENDMARKER\n", "func_signal": "def _rewrite_file_input(t, state):\n", "code": "if state.future_features:\n    # Ideally, we'd be able to pass in flags to the AST.compile() operation as we can with the\n    # builtin compile() function. Lacking that ability, we just munge an import statement into\n    # the start of the syntax tree\n    return ((symbol.file_input, _create_future_import_statement(state.future_features)) +\n            tuple((_rewrite_stmt(x, state) if x[0] == symbol.stmt else x) for x in t[1:]))\n    \nelse:\n    return _rewrite_tree(t, state, { symbol.stmt: _rewrite_stmt })", "path": "lib\\reinteract\\rewrite.py", "repo_name": "jbaayen/reinteract", "stars": 8, "license": "bsd-2-clause", "language": "python", "size": 836}
{"docstring": "\"\"\"\nAdd files that set a list of patterns into the temprary tree\n\n@param sourcedir: source directory that the patterns are relative (must be absolute)\n@param patterns: list of patterns. These can contain shell-style '*' globs\n@param directory: directory put files into (relative to the top of the temporary tree)\n@param attributes: attributes passed to add_file() for each file\n\n\"\"\"\n\n", "func_signal": "def add_matching_files(self, sourcedir, patterns, directory, **attributes):\n", "code": "for f in patterns:\n    absf = os.path.join(sourcedir, f)\n    if f.find('*') >= 0:\n        for ff in glob.iglob(absf):\n            relative = ff[len(sourcedir) + 1:]\n            if os.path.isdir(ff):\n                self.add_files_from_directory(ff, os.path.join(directory, relative), **attributes)\n            else:\n                destdir = os.path.join(directory, os.path.dirname(relative))\n                self.add_file(ff, destdir, **attributes)\n    elif os.path.isdir(absf):\n        self.add_files_from_directory(absf, os.path.join(directory, f), **attributes)\n    else:\n        destdir = os.path.join(directory, os.path.dirname(f))\n        self.add_file(absf, destdir, **attributes)", "path": "tools\\common\\builder.py", "repo_name": "jbaayen/reinteract", "stars": 8, "license": "bsd-2-clause", "language": "python", "size": 836}
{"docstring": "# Match an AST tree against a pattern. Along with symbol/token names, patterns\n# can contain strings:\n#\n#  '': ignore the matched item\n#  'name': store the matched item into the result dict under 'name'\n#  '*': matches multiple items (greedy); ignore matched items\n#  '*name': matches items (greedy); store the matched items as a sequence into the result dict\n#\n# Returns None if nothing matched or a dict of key/value pairs\n#\n# start_pos/start_pattern_index are used to match a trailing portion of the\n# tree against a trailing portion of the pattern; this is used internally to implement\n# non-terminal wildcards in patterns.\n#\n", "func_signal": "def _do_match(t, pattern, start_pos=0, start_pattern_index=0):\n", "code": "if start_pattern_index == 0:\n    if (t[0] != pattern[0]):\n        return None\n\nresult = {}\npos = max(1, start_pos)\nfor i in xrange(max(1, start_pattern_index), len(pattern)):\n    if isinstance(pattern[i], tuple):\n        if pos >= len(t):\n            return None\n        subresult = _do_match(t[pos], pattern[i])\n        if subresult is None:\n            return None\n        result.update(subresult)\n    else:\n        if len(pattern[i]) > 0 and pattern[i][0] == '*':\n            if i + 1 < len(pattern):\n                # Non-final *, need to find where the tail portion matches, start\n                # backwards from the end to implement a greedy match\n                for tail_pos in xrange(len(t) - 1, pos - 1, -1):\n                    tail_result = _do_match(t, pattern,\n                                            start_pos=tail_pos,\n                                            start_pattern_index=i + 1)\n                    if tail_result is not None:\n                        result.update(tail_result)\n                        break\n                else:\n                    return None\n            else:\n                tail_pos = len(t)\n\n            if pattern[i] != '*':\n                result[pattern[i][1:]] = t[pos:tail_pos]\n\n            return result\n        else:\n            if pos >= len(t):\n                return None\n            if pattern[i] != '':\n                result[pattern[i]] = t[pos]\n\n    pos += 1\n\nif pos > len(t):\n    return None\nelse:\n    return result", "path": "lib\\reinteract\\rewrite.py", "repo_name": "jbaayen/reinteract", "stars": 8, "license": "bsd-2-clause", "language": "python", "size": 836}
{"docstring": "# print_stmt: 'print' ( [ test (',' test)* [','] ] |\n#                       '>>' test [ (',' test)+ [','] ] )\n", "func_signal": "def _rewrite_print_stmt(t, state):\n", "code": "if state.print_func_name !=None and t[2][0] == symbol.test:\n    return _create_funccall_expr_stmt(state.print_func_name, filter(lambda x: type(x) != int and x[0] == symbol.test, t))\nelse:\n    return t", "path": "lib\\reinteract\\rewrite.py", "repo_name": "jbaayen/reinteract", "stars": 8, "license": "bsd-2-clause", "language": "python", "size": 836}
{"docstring": "\"\"\"\nGet the attributes dictionary for a file\n\n@param relative: location of the file in the temporary tree\n\n\"\"\"\n# .pyc/.pyo are added when we byte-compile the .py files, so they\n# may not be in file_attributes[], so look for the base .py instead\n# to figure out the right feature\n", "func_signal": "def get_file_attributes(self, relative):\n", "code": "if relative.endswith(\".pyc\") or relative.endswith(\".pyo\"):\n    relative = relative[:-3] + \"py\"\n    # Handle byte compiled .pyw, though they don't seem to be\n    # generated in practice\n    if not relative in self.file_attributes:\n        relative += \"w\"\n\nreturn self.file_attributes[relative]", "path": "tools\\common\\builder.py", "repo_name": "jbaayen/reinteract", "stars": 8, "license": "bsd-2-clause", "language": "python", "size": 836}
{"docstring": "\"\"\"\nGet the Reinteract version from configure.ac\n\"\"\"\n\n", "func_signal": "def get_version(self):\n", "code": "ac_file = os.path.join(self.topdir, 'configure.ac')\nf = open(ac_file, \"r\")\ncontents = f.read()\nf.close()\nm = re.search(r'^\\s*AC_INIT\\s*\\(\\s*[A-Za-z0-9_.-]+\\s*,\\s*([0-9.]+)\\s*\\)\\s*$', contents, re.MULTILINE)\nassert m\nreturn m.group(1)", "path": "tools\\common\\builder.py", "repo_name": "jbaayen/reinteract", "stars": 8, "license": "bsd-2-clause", "language": "python", "size": 836}
{"docstring": "\"\"\"\nAdd files listed in a Makefile.am into the temporary tree\n\nThe files that are added are files that are listed as _DATA or _PYTHON.\nRecursion is done via SUBDIRS. automake conditionals are ignored.\n\n@param relative: path relative to the Reinteract source topdir of the\n  directory where the Makefile.am lives\n@param directory: directory to copy files into (relative to tempory tree)\n@param attributes: attributes passed to add_file() for each file\n\n\"\"\"\n\n", "func_signal": "def add_files_from_am(self, relative, directory, **attributes):\n", "code": "am_file = os.path.join(self.topdir, relative, 'Makefile.am')\nam_parser = AMParser(am_file,\n   {\n        'bindir' : 'bin',\n        'docdir' : 'doc',\n        'examplesdir' : 'examples',\n        'pkgdatadir' : '.',\n        'datadir' : '.',\n        'pythondir' : 'python',\n        'REINTERACT_PACKAGE_DIR' : 'python/reinteract',\n\n        # Some config variables irrelevant for our purposes\n        'PYTHON_INCLUDES' : '',\n        'PYTHON_LIBS' : '',\n        'WRAPPER_CFLAGS' : ''\n   })\nif relative == '':\n    self.main_am = am_parser\n\nfor k, v in am_parser.iteritems():\n    if k.endswith(\"_DATA\"):\n        base = k[:-5]\n        dir = am_parser[base + 'dir']\n        for x in v.split():\n            self.add_file(os.path.join(relative, x), os.path.join(directory, dir), **attributes)\n    elif k.endswith(\"_PYTHON\"):\n        base = k[:-7]\n        dir = am_parser[base + 'dir']\n        for x in v.split():\n            self.add_file(os.path.join(relative, x), os.path.join(directory, dir), **attributes)\n\nif 'SUBDIRS' in am_parser:\n    for subdir in am_parser['SUBDIRS'].split():\n        if subdir == '.':\n            continue\n        self.add_files_from_am(os.path.join(relative, subdir), directory, **attributes)", "path": "tools\\common\\builder.py", "repo_name": "jbaayen/reinteract", "stars": 8, "license": "bsd-2-clause", "language": "python", "size": 836}
{"docstring": "# Check if the given AST is a \"test\" of the form 'a...b.c()' If it\n# matches, returns ('a...b', 'c'), otherwise returns None\n", "func_signal": "def _is_test_method_call(t):\n", "code": "args = _do_match(t, _method_call_pattern)\nif args is None:\n    return None\nelse:\n    return args['path'], args['method_name']", "path": "lib\\reinteract\\rewrite.py", "repo_name": "jbaayen/reinteract", "stars": 8, "license": "bsd-2-clause", "language": "python", "size": 836}
{"docstring": "# Given a path, possibly inside a notebook, find the notebook and the relative\n# path of the notebook inside the file\n", "func_signal": "def find_notebook_path(self, path):\n", "code": "relative = None\ntmp = path\nwhile True:\n    if os.path.isdir(tmp):\n        if os.path.exists(os.path.join(tmp, \"index.rnb\")):\n            return tmp, relative\n    parent, basename = os.path.split(tmp)\n    if parent == tmp: # At the root\n        # As a transition thing, for now allow specifying a folder without\n        # an index.rnb as a folder\n        if os.path.isdir(path):\n            return path, None\n        else:\n            return None, None\n\n    tmp = parent\n    if relative is None:\n        relative = basename\n    else:\n        relative = os.path.join(basename, relative)\n\nreturn tmp, relative", "path": "lib\\reinteract\\application.py", "repo_name": "jbaayen/reinteract", "stars": 8, "license": "bsd-2-clause", "language": "python", "size": 836}
{"docstring": "\"\"\"Initialize the Rewriter object\n\n@param code: the text to compile\n@param encoding: the encoding of the text\n@param future_features: a list of names from the __future__ module\n\n\"\"\"\n", "func_signal": "def __init__(self, code, encoding=\"utf8\", future_features=None):\n", "code": "if (isinstance(code, unicode)):\n    code = code.encode(\"utf8\")\n    encoding = \"utf8\"\n\nself.code = code\nself.encoding = encoding\nself.future_features = future_features\nself.original = parser.suite(code).totuple()", "path": "lib\\reinteract\\rewrite.py", "repo_name": "jbaayen/reinteract", "stars": 8, "license": "bsd-2-clause", "language": "python", "size": 836}
{"docstring": "\"\"\"\nAdd a directory of files into the temporary tree\n\n@param source: the directory to add. If relative, it is taken to be relative\n  to the top of the Reinteract source distribution.\n@param directory: directory put files into (relative to the top of the temporary tree)\n@param attributes: attributes passed to add_file() for each file\n\n\"\"\"\n\n", "func_signal": "def add_files_from_directory(self, sourcedir, directory, **attributes):\n", "code": "if os.path.isabs(sourcedir):\n    abssourcedir = sourcedir\nelse:\n    abssourcedir = os.path.join(self.topdir, sourcedir)\n\nfor f in os.listdir(abssourcedir):\n    absf = os.path.join(abssourcedir, f)\n    if os.path.isdir(absf):\n        self.add_files_from_directory(absf, os.path.join(directory, f), **attributes)\n    else:\n        self.add_file(absf, directory, **attributes)", "path": "tools\\common\\builder.py", "repo_name": "jbaayen/reinteract", "stars": 8, "license": "bsd-2-clause", "language": "python", "size": 836}
{"docstring": "# Turn a path into a (skeletal) textual description\n\n# path: atom trailer*\n\n# atom: ('(' [yield_expr|testlist_gexp] ')' |\n#       '[' [listmaker] ']' |\n#       '{' [dictmaker] '}' |\n#       '`' testlist1 '`' |\n#       NAME | NUMBER | STRING+)\n", "func_signal": "def _describe_path(path):\n", "code": "atom_value = path[0][1]\nif atom_value[0] == token.NAME:\n    result = atom_value[1]\nelif atom_value[0] == token.LPAR:\n    result = \"(...)\"\nelif atom_value[0] == token.LSQB:\n    result = \"[...]\"\nelif atom_value[0] == token.LBRACE:\n    result = \"{...}\"\nelif atom_value[0] == token.BACKQUOTE:\n    result = \"`...`\"\nelif atom_value[0] == token.NUMBER:\n    result = str(atom_value[1])\nelif atom_value[0] == token.STRING:\n    result = '\"...\"'\n\n# trailer: '(' [arglist] ')' | '[' subscriptlist ']' | '.' NAME\nfor trailer in path[1:]:\n    trailer_value = trailer[1]\n    if trailer_value[0] == token.LPAR:\n        result += \"(...)\"\n    elif trailer_value[0] == token.LSQB:\n        result += \"[...]\"\n    elif trailer_value[0] == token.DOT:\n        result += \".\" + trailer[2][1]\n\nreturn result", "path": "lib\\reinteract\\rewrite.py", "repo_name": "jbaayen/reinteract", "stars": 8, "license": "bsd-2-clause", "language": "python", "size": 836}
{"docstring": "# Generic rewriting of an AST, actions is a map of symbol/token type to function\n# to call to produce a modified version of the the subtree\n", "func_signal": "def _rewrite_tree(t, state, actions):\n", "code": "result = t\nfor i in xrange(1, len(t)):\n    subnode = t[i]\n    subtype = subnode[0]\n    if actions.has_key(subtype):\n        filtered = actions[subtype](subnode, state)\n        if filtered != subnode:\n            if result is t:\n                result = list(t)\n            result[i] = filtered\n            \nreturn result", "path": "lib\\reinteract\\rewrite.py", "repo_name": "jbaayen/reinteract", "stars": 8, "license": "bsd-2-clause", "language": "python", "size": 836}
{"docstring": "# Check if the given AST is a \"test\" of the form 'a...b.c' If it\n# matches, returns 'a...b', otherwise returns None\n", "func_signal": "def _is_test_attribute(t):\n", "code": "args = _do_match(t, _attribute_pattern)\n\nif args is None:\n    return None\nelse:\n    return args['path']", "path": "lib\\reinteract\\rewrite.py", "repo_name": "jbaayen/reinteract", "stars": 8, "license": "bsd-2-clause", "language": "python", "size": 836}
{"docstring": "\"\"\"Allocate an index to be used when displaying an unsaved object (\"Unsaved Worksheet 1\")\"\"\"\n\n", "func_signal": "def allocate_unsaved_index(self):\n", "code": "for i in xrange(0, len(self.__unsaved_indices)):\n    if not self.__unsaved_indices[i]:\n        self.__unsaved_indices[i] = True\n        return i + 1\nself.__unsaved_indices.append(True)\nreturn len(self.__unsaved_indices)", "path": "lib\\reinteract\\application.py", "repo_name": "jbaayen/reinteract", "stars": 8, "license": "bsd-2-clause", "language": "python", "size": 836}
{"docstring": "\"\"\"Get the description.\"\"\"\n", "func_signal": "def get_description(self, environ):\n", "code": "environ = _get_environ(environ)\nreturn self.description", "path": "flapp\\project_template\\lib\\werkzeug\\exceptions.py", "repo_name": "unbracketed/flapp", "stars": 9, "license": "None", "language": "python", "size": 572}
{"docstring": "\"\"\"Quote the value for the cookie.  This can be any object supported\nby :attr:`serialization_method`.\n\n:param value: the value to quote.\n\"\"\"\n", "func_signal": "def quote(cls, value):\n", "code": "if cls.serialization_method is not None:\n    value = cls.serialization_method.dumps(value)\nif cls.quote_base64:\n    value = ''.join(value.encode('base64').splitlines()).strip()\nreturn value", "path": "flapp\\project_template\\lib\\werkzeug\\contrib\\securecookie.py", "repo_name": "unbracketed/flapp", "stars": 9, "license": "None", "language": "python", "size": 572}
{"docstring": "\"\"\"Unquote the value for the cookie.  If unquoting does not work a\n:exc:`UnquoteError` is raised.\n\n:param value: the value to unquote.\n\"\"\"\n", "func_signal": "def unquote(cls, value):\n", "code": "try:\n    if cls.quote_base64:\n        value = value.decode('base64')\n    if cls.serialization_method is not None:\n        value = cls.serialization_method.loads(value)\n    return value\nexcept:\n    # unfortunately pickle and other serialization modules can\n    # cause pretty every error here.  if we get one we catch it\n    # and convert it into an UnquoteError\n    raise UnquoteError()", "path": "flapp\\project_template\\lib\\werkzeug\\contrib\\securecookie.py", "repo_name": "unbracketed/flapp", "stars": 9, "license": "None", "language": "python", "size": 572}
{"docstring": "\"\"\"Call the exception as WSGI application.\n\n:param environ: the WSGI environment.\n:param start_response: the response callable provided by the WSGI\n                       server.\n\"\"\"\n", "func_signal": "def __call__(self, environ, start_response):\n", "code": "response = self.get_response(environ)\nreturn response(environ, start_response)", "path": "flapp\\project_template\\lib\\werkzeug\\exceptions.py", "repo_name": "unbracketed/flapp", "stars": 9, "license": "None", "language": "python", "size": 572}
{"docstring": "\"\"\"Return a signature object for the function.\"\"\"\n", "func_signal": "def _parse_signature(func):\n", "code": "if hasattr(func, 'im_func'):\n    func = func.im_func\n\n# if we have a cached validator for this function, return it\nparse = _signature_cache.get(func)\nif parse is not None:\n    return parse\n\n# inspect the function signature and collect all the information\npositional, vararg_var, kwarg_var, defaults = inspect.getargspec(func)\ndefaults = defaults or ()\narg_count = len(positional)\narguments = []\nfor idx, name in enumerate(positional):\n    if isinstance(name, list):\n        raise TypeError('cannot parse functions that unpack tuples '\n                        'in the function signature')\n    try:\n        default = defaults[idx - arg_count]\n    except IndexError:\n        param = (name, False, None)\n    else:\n        param = (name, True, default)\n    arguments.append(param)\narguments = tuple(arguments)\n\ndef parse(args, kwargs):\n    new_args = []\n    missing = []\n    extra = {}\n\n    # consume as many arguments as positional as possible\n    for idx, (name, has_default, default) in enumerate(arguments):\n        try:\n            new_args.append(args[idx])\n        except IndexError:\n            try:\n                new_args.append(kwargs.pop(name))\n            except KeyError:\n                if has_default:\n                    new_args.append(default)\n                else:\n                    missing.append(name)\n        else:\n            if name in kwargs:\n                extra[name] = kwargs.pop(name)\n\n    # handle extra arguments\n    extra_positional = args[arg_count:]\n    if vararg_var is not None:\n        new_args.extend(extra_positional)\n        extra_positional = ()\n    if kwargs and not kwarg_var is not None:\n        extra.update(kwargs)\n        kwargs = {}\n\n    return new_args, kwargs, missing, extra, extra_positional, \\\n           arguments, vararg_var, kwarg_var\n_signature_cache[func] = parse\nreturn parse", "path": "flapp\\project_template\\lib\\werkzeug\\_internal.py", "repo_name": "unbracketed/flapp", "stars": 9, "license": "None", "language": "python", "size": 572}
{"docstring": "\"\"\"This method returns a new subclass of the exception provided that\nalso is a subclass of `BadRequest`.\n\"\"\"\n", "func_signal": "def wrap(cls, exception, name=None):\n", "code": "class newcls(cls, exception):\n    def __init__(self, arg=None, description=None):\n        cls.__init__(self, description)\n        exception.__init__(self, arg)\nnewcls.__module__ = sys._getframe(1).f_globals.get('__name__')\nnewcls.__name__ = name or cls.__name__ + exception.__name__\nreturn newcls", "path": "flapp\\project_template\\lib\\werkzeug\\exceptions.py", "repo_name": "unbracketed/flapp", "stars": 9, "license": "None", "language": "python", "size": 572}
{"docstring": "\"\"\"Converts a timetuple, integer or datetime object into the seconds from\nepoch in utc.\n\"\"\"\n", "func_signal": "def _date_to_unix(arg):\n", "code": "if isinstance(arg, datetime):\n    arg = arg.utctimetuple()\nelif isinstance(arg, (int, long, float)):\n    return int(arg)\nyear, month, day, hour, minute, second = arg[:6]\ndays = date(year, month, 1).toordinal() - _epoch_ord + day - 1\nhours = days * 24 + hour\nminutes = hours * 60 + minute\nseconds = minutes * 60 + second\nreturn seconds", "path": "flapp\\project_template\\lib\\werkzeug\\_internal.py", "repo_name": "unbracketed/flapp", "stars": 9, "license": "None", "language": "python", "size": 572}
{"docstring": "\"\"\"Log into the internal werkzeug logger.\"\"\"\n", "func_signal": "def _log(type, message, *args, **kwargs):\n", "code": "global _logger\nif _logger is None:\n    import logging\n    _logger = logging.getLogger('werkzeug')\n    # Only set up a default log handler if the\n    # end-user application didn't set anything up.\n    if not logging.root.handlers and _logger.level == logging.NOTSET:\n        _logger.setLevel(logging.INFO)\n        handler = logging.StreamHandler()\n        _logger.addHandler(handler)\ngetattr(_logger, type)(message.rstrip(), *args, **kwargs)", "path": "flapp\\project_template\\lib\\werkzeug\\_internal.py", "repo_name": "unbracketed/flapp", "stars": 9, "license": "None", "language": "python", "size": 572}
{"docstring": "\"\"\"Get the HTML body.\"\"\"\n", "func_signal": "def get_body(self, environ):\n", "code": "return (\n    '<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\">\\n'\n    '<title>%(code)s %(name)s</title>\\n'\n    '<h1>%(name)s</h1>\\n'\n    '%(description)s\\n'\n) % {\n    'code':         self.code,\n    'name':         escape(self.name),\n    'description':  self.get_description(environ)\n}", "path": "flapp\\project_template\\lib\\werkzeug\\exceptions.py", "repo_name": "unbracketed/flapp", "stars": 9, "license": "None", "language": "python", "size": 572}
{"docstring": "\"\"\"Serialize the secure cookie into a string.\n\nIf expires is provided, the session will be automatically invalidated\nafter expiration when you unseralize it. This provides better\nprotection against session cookie theft.\n\n:param expires: an optional expiration date for the cookie (a\n                :class:`datetime.datetime` object)\n\"\"\"\n", "func_signal": "def serialize(self, expires=None):\n", "code": "if self.secret_key is None:\n    raise RuntimeError('no secret key defined')\nif expires:\n    self['_expires'] = _date_to_unix(expires)\nresult = []\nmac = hmac(self.secret_key, None, self.hash_method)\nfor key, value in sorted(self.items()):\n    result.append('%s=%s' % (\n        url_quote_plus(key),\n        self.quote(value)\n    ))\n    mac.update('|' + result[-1])\nreturn '%s?%s' % (\n    mac.digest().encode('base64').strip(),\n    '&'.join(result)\n)", "path": "flapp\\project_template\\lib\\werkzeug\\contrib\\securecookie.py", "repo_name": "unbracketed/flapp", "stars": 9, "license": "None", "language": "python", "size": 572}
{"docstring": "\"\"\"Takes an optional list of valid http methods\nstarting with werkzeug 0.3 the list will be mandatory.\"\"\"\n", "func_signal": "def __init__(self, valid_methods=None, description=None):\n", "code": "HTTPException.__init__(self, description)\nself.valid_methods = valid_methods", "path": "flapp\\project_template\\lib\\werkzeug\\exceptions.py", "repo_name": "unbracketed/flapp", "stars": 9, "license": "None", "language": "python", "size": 572}
{"docstring": "\"\"\"Add a new entry to the feed.  This function can either be called\nwith a :class:`FeedEntry` or some keyword and positional arguments\nthat are forwarded to the :class:`FeedEntry` constructor.\n\"\"\"\n", "func_signal": "def add(self, *args, **kwargs):\n", "code": "if len(args) == 1 and not kwargs and isinstance(args[0], FeedEntry):\n    self.entries.append(args[0])\nelse:\n    kwargs['feed_url'] = self.feed_url\n    self.entries.append(FeedEntry(*args, **kwargs))", "path": "flapp\\project_template\\lib\\werkzeug\\contrib\\atom.py", "repo_name": "unbracketed/flapp", "stars": 9, "license": "None", "language": "python", "size": 572}
{"docstring": "\"\"\"Used for `http_date` and `cookie_date`.\"\"\"\n", "func_signal": "def _dump_date(d, delim):\n", "code": "if d is None:\n    d = gmtime()\nelif isinstance(d, datetime):\n    d = d.utctimetuple()\nelif isinstance(d, (int, long, float)):\n    d = gmtime(d)\nreturn '%s, %02d%s%s%s%s %02d:%02d:%02d GMT' % (\n    ('Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun')[d.tm_wday],\n    d.tm_mday, delim,\n    ('Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep',\n     'Oct', 'Nov', 'Dec')[d.tm_mon - 1],\n    delim, str(d.tm_year), d.tm_hour, d.tm_min, d.tm_sec\n)", "path": "flapp\\project_template\\lib\\werkzeug\\_internal.py", "repo_name": "unbracketed/flapp", "stars": 9, "license": "None", "language": "python", "size": 572}
{"docstring": "\"\"\"Return a generator that yields pieces of XML.\"\"\"\n# atom demands either an author element in every entry or a global one\n", "func_signal": "def generate(self):\n", "code": "if not self.author:\n    if False in map(lambda e: bool(e.author), self.entries):\n        self.author = ({'name': u'unbekannter Autor'},)\n\nif not self.updated:\n    dates = sorted([entry.updated for entry in self.entries])\n    self.updated = dates and dates[-1] or datetime.utcnow()\n\nyield u'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n'\nyield u'<feed xmlns=\"http://www.w3.org/2005/Atom\">\\n'\nyield '  ' + _make_text_block('title', self.title, self.title_type)\nyield u'  <id>%s</id>\\n' % escape(self.id)\nyield u'  <updated>%s</updated>\\n' % format_iso8601(self.updated)\nif self.url:\n    yield u'  <link href=\"%s\" />\\n' % escape(self.url, True)\nif self.feed_url:\n    yield u'  <link href=\"%s\" rel=\"self\" />\\n' % \\\n        escape(self.feed_url, True)\nfor link in self.links:\n    yield u'  <link %s/>\\n' % ''.join('%s=\"%s\" ' % \\\n        (k, escape(link[k], True)) for k in link)\nfor author in self.author:\n    yield u'  <author>\\n'\n    yield u'    <name>%s</name>\\n' % escape(author['name'])\n    if 'uri' in author:\n        yield u'    <uri>%s</uri>\\n' % escape(author['uri'])\n    if 'email' in author:\n        yield '    <email>%s</email>\\n' % escape(author['email'])\n    yield '  </author>\\n'\nif self.subtitle:\n    yield '  ' + _make_text_block('subtitle', self.subtitle,\n                                  self.subtitle_type)\nif self.icon:\n    yield u'  <icon>%s</icon>\\n' % escape(self.icon)\nif self.logo:\n    yield u'  <logo>%s</logo>\\n' % escape(self.logo)\nif self.rights:\n    yield '  ' + _make_text_block('rights', self.rights,\n                                  self.rights_type)\ngenerator_name, generator_url, generator_version = self.generator\nif generator_name or generator_url or generator_version:\n    tmp = [u'  <generator']\n    if generator_url:\n        tmp.append(u' uri=\"%s\"' % escape(generator_url, True))\n    if generator_version:\n        tmp.append(u' version=\"%s\"' % escape(generator_version, True))\n    tmp.append(u'>%s</generator>\\n' % escape(generator_name))\n    yield u''.join(tmp)\nfor entry in self.entries:\n    for line in entry.generate():\n        yield u'  ' + line\nyield u'</feed>\\n'", "path": "flapp\\project_template\\lib\\werkzeug\\contrib\\atom.py", "repo_name": "unbracketed/flapp", "stars": 9, "license": "None", "language": "python", "size": 572}
{"docstring": "\"\"\"Loads a :class:`SecureCookie` from a cookie in request.  If the\ncookie is not set, a new :class:`SecureCookie` instanced is\nreturned.\n\n:param request: a request object that has a `cookies` attribute\n                which is a dict of all cookie values.\n:param key: the name of the cookie.\n:param secret_key: the secret key used to unquote the cookie.\n                   Always provide the value even though it has\n                   no default!\n\"\"\"\n", "func_signal": "def load_cookie(cls, request, key='session', secret_key=None):\n", "code": "data = request.cookies.get(key)\nif not data:\n    return cls(secret_key=secret_key)\nreturn cls.unserialize(data, secret_key)", "path": "flapp\\project_template\\lib\\werkzeug\\contrib\\securecookie.py", "repo_name": "unbracketed/flapp", "stars": 9, "license": "None", "language": "python", "size": 572}
{"docstring": "\"\"\"Helper function that forwards all the function details to the\ndecorated function.\"\"\"\n", "func_signal": "def _patch_wrapper(old, new):\n", "code": "try:\n    new.__name__ = old.__name__\n    new.__module__ = old.__module__\n    new.__doc__ = old.__doc__\n    new.__dict__ = old.__dict__\nexcept:\n    pass\nreturn new", "path": "flapp\\project_template\\lib\\werkzeug\\_internal.py", "repo_name": "unbracketed/flapp", "stars": 9, "license": "None", "language": "python", "size": 572}
{"docstring": "\"\"\"Yields pieces of ATOM XML.\"\"\"\n", "func_signal": "def generate(self):\n", "code": "base = ''\nif self.xml_base:\n    base = ' xml:base=\"%s\"' % escape(self.xml_base, True)\nyield u'<entry%s>\\n' % base\nyield u'  ' + _make_text_block('title', self.title, self.title_type)\nyield u'  <id>%s</id>\\n' % escape(self.id)\nyield u'  <updated>%s</updated>\\n' % format_iso8601(self.updated)\nif self.published:\n    yield u'  <published>%s</published>\\n' % \\\n          format_iso8601(self.published)\nif self.url:\n    yield u'  <link href=\"%s\" />\\n' % escape(self.url)\nfor author in self.author:\n    yield u'  <author>\\n'\n    yield u'    <name>%s</name>\\n' % escape(author['name'])\n    if 'uri' in author:\n        yield u'    <uri>%s</uri>\\n' % escape(author['uri'])\n    if 'email' in author:\n        yield u'    <email>%s</email>\\n' % escape(author['email'])\n    yield u'  </author>\\n'\nfor link in self.links:\n    yield u'  <link %s/>\\n' % ''.join('%s=\"%s\" ' % \\\n        (k, escape(link[k], True)) for k in link)\nif self.summary:\n    yield u'  ' + _make_text_block('summary', self.summary,\n                                   self.summary_type)\nif self.content:\n    yield u'  ' + _make_text_block('content', self.content,\n                                   self.content_type)\nyield u'</entry>\\n'", "path": "flapp\\project_template\\lib\\werkzeug\\contrib\\atom.py", "repo_name": "unbracketed/flapp", "stars": 9, "license": "None", "language": "python", "size": 572}
{"docstring": "\"\"\"Get a response object.\n\n:param environ: the environ for the request.\n:return: a :class:`BaseResponse` object or a subclass thereof.\n\"\"\"\n# lazily imported for various reasons.  For one, we can use the exceptions\n# with custom responses (testing exception instances against types) and\n# so we don't ever have to import the wrappers, but also because there\n# are circular dependencies when bootstrapping the module.\n", "func_signal": "def get_response(self, environ):\n", "code": "environ = _get_environ(environ)\nfrom werkzeug.wrappers import BaseResponse\nheaders = self.get_headers(environ)\nreturn BaseResponse(self.get_body(environ), self.code, headers)", "path": "flapp\\project_template\\lib\\werkzeug\\exceptions.py", "repo_name": "unbracketed/flapp", "stars": 9, "license": "None", "language": "python", "size": 572}
{"docstring": "\"\"\"Helper function for the builder that creates an XML text block.\"\"\"\n", "func_signal": "def _make_text_block(name, content, content_type=None):\n", "code": "if content_type == 'xhtml':\n    return u'<%s type=\"xhtml\"><div xmlns=\"%s\">%s</div></%s>\\n' % \\\n           (name, XHTML_NAMESPACE, content, name)\nif not content_type:\n    return u'<%s>%s</%s>\\n' % (name, escape(content), name)\nreturn u'<%s type=\"%s\">%s</%s>\\n' % (name, content_type,\n                                     escape(content), name)", "path": "flapp\\project_template\\lib\\werkzeug\\contrib\\atom.py", "repo_name": "unbracketed/flapp", "stars": 9, "license": "None", "language": "python", "size": 572}
{"docstring": "\"\"\"Iterate over all modules in a package.\"\"\"\n", "func_signal": "def _iter_modules(path):\n", "code": "import os\nimport pkgutil\nif hasattr(pkgutil, 'iter_modules'):\n    for importer, modname, ispkg in pkgutil.iter_modules(path):\n        yield modname, ispkg\n    return\nfrom inspect import getmodulename\nfrom pydoc import ispackage\nfound = set()\nfor path in path:\n    for filename in os.listdir(path):\n        p = os.path.join(path, filename)\n        modname = getmodulename(filename)\n        if modname and modname != '__init__':\n            if modname not in found:\n                found.add(modname)\n                yield modname, ispackage(modname)", "path": "flapp\\project_template\\lib\\werkzeug\\_internal.py", "repo_name": "unbracketed/flapp", "stars": 9, "license": "None", "language": "python", "size": 572}
{"docstring": "# opcodes which take arguments\n", "func_signal": "def read_argc(op, it):\n", "code": "argc = 0\nif op >= dis.HAVE_ARGUMENT:\n    argc += it.next()\n    argc += (it.next() * 256)\nreturn argc", "path": "modipyd\\bytecode.py", "repo_name": "ishikawa/modipyd", "stars": 10, "license": "mit", "language": "python", "size": 632}
{"docstring": "\"\"\"\nLoading BytecodeProcessor from modipyd.BYTECODE_PROCESSORS\nsettings. Return ChainedBytecodeProcessor instance holds\nall loaded processors.\n\"\"\"\n", "func_signal": "def load_bytecode_processors():\n", "code": "if (BYTECODE_PROCESSORS and \n        (len(BYTECODE_PROCESSORS_CACHE) != len(BYTECODE_PROCESSORS))):\n    del BYTECODE_PROCESSORS_CACHE[:]\n    for i, name in enumerate(BYTECODE_PROCESSORS[:]):\n        LOGGER.info(\"Loading BytecodeProcesser '%s'\" % name)\n        try:\n            klass = utils.import_component(name)\n        except (ImportError, AttributeError):\n            LOGGER.warn(\n                \"Loading BytecodeProcesser '%s' failed. \"\n                \"This setting is removed\" % name,\n                exc_info=True)\n            del BYTECODE_PROCESSORS[i]\n        else:\n            BYTECODE_PROCESSORS_CACHE.append(klass)\n\nprocessors = []\nfor klass in BYTECODE_PROCESSORS_CACHE:\n    processors.append(klass())\nreturn bc.ChainedBytecodeProcessor(processors)", "path": "modipyd\\module.py", "repo_name": "ishikawa/modipyd", "stars": 10, "license": "mit", "language": "python", "size": 632}
{"docstring": "\"\"\"\nReturns a sequence (usually tuple) whose element is ``obj``.\nIf a callable argument ``copy`` is specified, it is called\nwith the sequence as 1st argument, and returns its result.\n\n>>> sequence(None)[0] is None\nTrue\n>>> sequence(\"\")\n('',)\n>>> sequence([])\n[]\n>>> sequence(())\n()\n>>> sequence(123, copy=tuple)\n(123,)\n>>> sequence(123, copy=list)\n[123]\n\"\"\"\n", "func_signal": "def sequence(obj, copy=None):\n", "code": "if isinstance(obj, (list, tuple)):\n    seq = obj\nelse:\n    seq = (obj,)\n\nif copy is not tuple and callable(copy):\n    seq = copy(seq)\n\nreturn seq", "path": "modipyd\\utils\\core.py", "repo_name": "ishikawa/modipyd", "stars": 10, "license": "mit", "language": "python", "size": 632}
{"docstring": "\"\"\"\n``collect_files()`` generates the file names in a directory tree.\nNote: ``collect_files()`` will not visit symbolic links to\nsubdirectories. *ignore_list* argument is ignore filename patterns\n(using fnmatch).\n\"\"\"\n", "func_signal": "def collect_files(filepath_or_list, ignore_list=None):\n", "code": "import fnmatch\n\ndef ignore(filename):\n    if ignore_list:\n        for pattern in ignore_list:\n            if fnmatch.fnmatch(filename, pattern):\n                return True\n    return False\n\nfor filepath in sequence(filepath_or_list):\n\n    if ignore(os.path.basename(filepath)):\n        continue\n\n    if not os.path.exists(filepath):\n        from errno import ENOENT\n        raise IOError(ENOENT, \"No such file or directory\", filepath)\n    elif not os.path.isdir(filepath):\n        yield filepath\n    else:\n        # pylint: disable-msg=W0612\n        for dirpath, dirnames, filenames in os.walk(filepath):\n            # For in-place deletion (avoids copying the list),\n            # Don't delete anything earlier in the list than\n            # the current element through.\n            count = len(dirnames)\n            for i, dirname in enumerate(reversed(dirnames)):\n                if ignore(dirname):\n                    # don't visit ignore directory\n                    del dirnames[count-1-i]\n            for filename in filenames:\n                if not ignore(filename):\n                    yield os.path.join(dirpath, filename)", "path": "modipyd\\utils\\__init__.py", "repo_name": "ishikawa/modipyd", "stars": 10, "license": "mit", "language": "python", "size": 632}
{"docstring": "\"\"\"\n>>> split_module_name('')\n('', '')\n>>> split_module_name('module')\n('', 'module')\n>>> split_module_name('module.a')\n('module', 'a')\n>>> split_module_name('module.a.b')\n('module.a', 'b')\n\"\"\"\n", "func_signal": "def split_module_name(module_name):\n", "code": "i = module_name.rfind('.')\nif i == -1:\n    return ('', module_name)\nelse:\n    return (module_name[:i], module_name[i+1:])", "path": "modipyd\\utils\\__init__.py", "repo_name": "ishikawa/modipyd", "stars": 10, "license": "mit", "language": "python", "size": 632}
{"docstring": "\"\"\"\nTry to detect the module name from *filepath* on\nthe search path ``search_paths``. If *search_paths*\nis omitted or ``None``, ``sys.path`` is used.\n\nNotice: This function is provided for convenience.\nYou should use ``modipyd.resolve.ModuleNameResolver``\nclass for better performance.\n\"\"\"\n", "func_signal": "def resolve_modulename(filepath, search_paths=None):\n", "code": "from modipyd.resolve import ModuleNameResolver\nreturn ModuleNameResolver(search_paths).resolve(filepath)[0]", "path": "modipyd\\utils\\__init__.py", "repo_name": "ishikawa/modipyd", "stars": 10, "license": "mit", "language": "python", "size": 632}
{"docstring": "\"\"\"\nReturn a relative version of the pathname ``path``.\n\n>>> relativepath('')\n''\n>>> relativepath('/usr/local/bin', '/usr')\n'local/bin'\n>>> relativepath('/usr/local/bin', '/usr/local/')\n'bin'\n>>> relativepath('/usr/local/bin2', '/usr/local/bin')\n'/usr/local/bin2'\n\"\"\"\n", "func_signal": "def relativepath(path, base=None):\n", "code": "if not isinstance(path, basestring):\n    raise TypeError(\"path must be instance of basestring\")\n\nif not base:\n    base = os.getcwd()\n\n# normalize\npath = os.path.abspath(path)\nbase = os.path.abspath(base)\n\nitems = path.split('/')\nif items:\n    for name in base.split('/'):\n        if items[0] != name:\n            return path\n        del items[0]\n\nreturn '/'.join(items)", "path": "modipyd\\utils\\__init__.py", "repo_name": "ishikawa/modipyd", "stars": 10, "license": "mit", "language": "python", "size": 632}
{"docstring": "\"\"\"\nReturn ``True`` if *dirpath* refers to an existing directory that\ncontains Python module named *modulename*.\n\"\"\"\n# os.path.isdir() check is not needed\n", "func_signal": "def python_module_exists(dirpath, modulename):\n", "code": "from os.path import isfile, join\nreturn (isfile(join(dirpath, '%s.py' % modulename)) or\n             isfile(join(dirpath, '%s.pyc' % modulename)) or\n             isfile(join(dirpath, '%s.pyo' % modulename)))", "path": "modipyd\\utils\\__init__.py", "repo_name": "ishikawa/modipyd", "stars": 10, "license": "mit", "language": "python", "size": 632}
{"docstring": "\"\"\"\nReturn ``True`` if path is an existing Python compiled\nbytecode file (*.pyc* or *.pyo*). This follows symbolic links.\n\"\"\"\n", "func_signal": "def python_compiled_file(filepath):\n", "code": "try:\n    st = os.stat(filepath)\nexcept (TypeError, os.error):\n    return False\nelse:\n    return (stat.S_ISREG(st.st_mode) and\n               (filepath.endswith('.pyc') or\n                filepath.endswith('.pyo')))", "path": "modipyd\\utils\\__init__.py", "repo_name": "ishikawa/modipyd", "stars": 10, "license": "mit", "language": "python", "size": 632}
{"docstring": "\"\"\"\nReturn loaded module if module is already loaded in\n``sys.modules``, otherwise load module and return it.\n\"\"\"\n", "func_signal": "def import_module(modulename):\n", "code": "if modulename not in sys.modules:\n    __import__(modulename)\nreturn sys.modules[modulename]", "path": "modipyd\\utils\\__init__.py", "repo_name": "ishikawa/modipyd", "stars": 10, "license": "mit", "language": "python", "size": 632}
{"docstring": "\"\"\"Remove *descriptor*, and clear dependencies\"\"\"\n", "func_signal": "def remove(self, descriptor):\n", "code": "descriptors, filenames = self.descriptors, self.__filenames\nfilename = splitext(descriptor.filename)[0]\n\nif (descriptor.name not in descriptors or \n        filename not in filenames):\n    raise KeyError(\n        \"No monitoring descriptor '%s'\" % \\\n        descriptor.name)\n\nLOGGER.debug(\"Removed: %s\" % descriptor.describe())\ndescriptor.clear_dependencies()\ndel descriptors[descriptor.name]\ndel filenames[filename]", "path": "modipyd\\monitor.py", "repo_name": "ishikawa/modipyd", "stars": 10, "license": "mit", "language": "python", "size": 632}
{"docstring": "\"\"\"\nInstanciates and initialize ``ModuleCode`` object\n\n>>> code = compile(\n...     \"import os;\"\n...     \"from os.path import join as join_path\",\n...     '<string>', 'exec')\n>>> modcode = ModuleCode('__main__', '', code.co_filename, code)\n>>> modcode.name\n'__main__'\n>>> modcode.filename\n'<string>'\n>>> imports = modcode.context['imports']\n>>> len(imports)\n2\n>>> imports[0]\n('os', 'os', -1)\n>>> imports[1]\n('join_path', 'os.path.join', -1)\n\"\"\"\n", "func_signal": "def __init__(self, modulename, packagename, filename, code):\n", "code": "super(ModuleCode, self).__init__()\nself.name = modulename\nself.package_name = packagename\nself.filename = filename\nself.context = {}\n\nif code is None:\n    # Maybe source file contains SyntaxError?\n    pass\nelse:\n    self.update_code(code)", "path": "modipyd\\module.py", "repo_name": "ishikawa/modipyd", "stars": 10, "license": "mit", "language": "python", "size": 632}
{"docstring": "\"\"\"Add *descriptor*, but doesn't update dependencies\"\"\"\n", "func_signal": "def add(self, descriptor):\n", "code": "descriptors, filenames = self.descriptors, self.__filenames\nfilename = splitext(descriptor.filename)[0]\n\ndescriptors[descriptor.name] = descriptor\nfilenames[filename] = descriptor", "path": "modipyd\\monitor.py", "repo_name": "ishikawa/modipyd", "stars": 10, "license": "mit", "language": "python", "size": 632}
{"docstring": "\"\"\"\nCompile the source file at *filepath* into a code object.\nCode objects can be executed by an exec statement or\nevaluated by a call to ``eval()``.\n\"\"\"\n# line endings must be represented by a single newline character ('\\n'),\n# and the input must be terminated by at least one newline character.\n", "func_signal": "def compile_source(filepath):\n", "code": "fp = open(filepath, 'U')\ntry:\n    return compile(fp.read() + '\\n', filepath, 'exec')\nfinally:\n    fp.close()", "path": "modipyd\\module.py", "repo_name": "ishikawa/modipyd", "stars": 10, "license": "mit", "language": "python", "size": 632}
{"docstring": "# import_name is an empty string in\n# ``from .+ import `` statement\n", "func_signal": "def store_name(self, name):\n", "code": "if self.import_name is None:\n    return\n\n# The Absolute and Relative Imports future has been\n# implemented in Python 2.5\n#\n# http://docs.python.org/whatsnew/pep-328.html\nlevel = -1\nassert len(self.consts) >= 1\nif HAS_RELATIVE_IMPORTS:\n    assert len(self.consts) >= 2\n    level = self.consts[-2]\n    assert level >= -1, \\\n        \"import level is illegal: %s (%s)\" % \\\n        (str(level), str(self.consts))\n\nif not self.fromname:\n    # import ...\n    symbol = self.fqn[self.store:]\n    symbol.insert(0, name)\n    self.imports.append(('.'.join(symbol), '.'.join(self.fqn), level))\n    #print \"-> import %s\" % str(self.imports[-1])\n    self.clear_states()\nelse:\n    # from ... import ...\n\n    # import_name is an empty string in\n    # ``from .+ import `` statement\n    if self.import_name:\n        fqn = '.'.join(self.fqn + [self.fromname])\n    else:\n        fqn = self.fromname\n    self.imports.append((name, fqn, level))\n    #print \"-> import %s\" % str(self.imports[-1])\n    self.fromname = None", "path": "modipyd\\bytecode.py", "repo_name": "ishikawa/modipyd", "stars": 10, "license": "mit", "language": "python", "size": 632}
{"docstring": "\"\"\"\nReturn ``True`` if path is an existing Python source\nfile (*.py*). This follows symbolic links.\n\"\"\"\n", "func_signal": "def python_module_file(filepath):\n", "code": "return (python_source_file(filepath) or\n        python_compiled_file(filepath))", "path": "modipyd\\utils\\__init__.py", "repo_name": "ishikawa/modipyd", "stars": 10, "license": "mit", "language": "python", "size": 632}
{"docstring": "\"\"\"\nLets you annotate function with argument type requirements.\nThese type requirements are automatically checked by the system at\nfunction invocation time.\n\"\"\"\n\n# pylint: disable-msg=W0622\n", "func_signal": "def require(**types):\n", "code": "def decorator(fn):\n    code = fn.func_code\n    argnames = code.co_varnames[:code.co_argcount]\n    argmaps = dict((name, i) for i, name in enumerate(argnames))\n\n    # None -> NoneType convertion\n    for name, constraints in types.iteritems():\n        constraints = sequence(constraints)\n        constraints = [c is None and NoneType or c for c in constraints]\n        if len(constraints) == 1:\n            types[name] = constraints[0]\n        else:\n            types[name] = tuple(constraints)\n\n    # pylint: disable-msg=W0621\n    # :W0621: *Redefining name %r from outer scope (line %s)*\n    def type_checker(*args, **kwargs):\n        for name in argmaps:\n            i = argmaps[name]\n            if i < len(args):\n                value = args[i]\n            elif name in kwargs:\n                value = kwargs[name]\n            else:\n                # maybe default value\n                continue\n\n            try:\n                constraint = types[name]\n                if (callable(constraint) and\n                        not isinstance(constraint, type)):\n                    if not constraint(value): \n                        raise TypeError(\"Type checking of '%s' was failed: \"\n                            \"%s(%s)\" % (name, value, type(value)))\n                elif not isinstance(value, constraint):\n                    raise TypeError(\n                            \"Expected '%s' to be %s, but was %s.\" %\n                            (name, types[name], type(value)))\n            except KeyError:\n                pass\n\n        return fn(*args, **kwargs)\n\n    type_checker.__name__ = fn.__name__\n    type_checker.__module__ = fn.__module__\n    type_checker.__dict__ = fn.__dict__\n    type_checker.__doc__ = fn.__doc__\n    return type_checker\n\nreturn decorator", "path": "modipyd\\utils\\decorators.py", "repo_name": "ishikawa/modipyd", "stars": 10, "license": "mit", "language": "python", "size": 632}
{"docstring": "\"\"\"\nReturn ``True`` if path is an existing Python source\nfile (*.py*). This follows symbolic links.\n\"\"\"\n", "func_signal": "def python_source_file(filepath):\n", "code": "try:\n    st = os.stat(filepath)\nexcept (TypeError, os.error):\n    return False\nelse:\n    return (stat.S_ISREG(st.st_mode) and\n            filepath.endswith('.py'))", "path": "modipyd\\utils\\__init__.py", "repo_name": "ishikawa/modipyd", "stars": 10, "license": "mit", "language": "python", "size": 632}
{"docstring": "\"\"\"\nAll the monitoring modules. This is dictionary,\nmaps name and module descriptors.\n\"\"\"\n", "func_signal": "def descriptors(self):\n", "code": "if self.__descriptors is None:\n    self.__descriptors = {}\n    entries = list(self.refresh())\n    LOGGER.debug(\"%d descriptoes\" % len(entries))\nreturn self.__descriptors", "path": "modipyd\\monitor.py", "repo_name": "ishikawa/modipyd", "stars": 10, "license": "mit", "language": "python", "size": 632}
{"docstring": "# print \"scan_code: %s\" % co.co_filename\n", "func_signal": "def _scan_code(co, processor, context):\n", "code": "assert co and context is not None\ncode_it = code_iter(co)\nprocessor.enter(co)\nfor op in code_it:\n    # print dis.opname[op], argc\n    argc = read_argc(op, code_it)\n    processor.process(op, argc, co)\nprocessor.exit(co)\n\nfor c in co.co_consts:\n    if isinstance(c, type(co)):\n        _scan_code(c, processor, context)", "path": "modipyd\\bytecode.py", "repo_name": "ishikawa/modipyd", "stars": 10, "license": "mit", "language": "python", "size": 632}
{"docstring": "# Yes, that's how the spec has it defined. This is wacky.\n# Why would ONLY the stop name be defined?\n\n", "func_signal": "def __init__(self, stop_name, route = None, direction = None, stop_num = None):\n", "code": "log.debug(\"route in init: \" + str(route))\n\nif (route != None):\n    self.route = str(route)\nelse:\n    self.route = None\n\nif (direction != None):\n    self.direction = str(direction)\nelse:\n    self.direction = None\n\nif (stop_num != None):\n    self.stop_num = int(stop_num)\nelse:\n    self.stop_num = None\n\nself.stop_name = str(stop_name)", "path": "ctabustracker.py", "repo_name": "cswingler/ctabustracker-py", "stars": 10, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"\nReturns the response from the API.\ncommand: str, command from the base URL.\nparam_dict: dict, keys and results to be submitted as\nHTTP parameters\n\"\"\"\n\n", "func_signal": "def __get_api_response(self, command, param_dict=None):\n", "code": "url = self.__api_url + command + \"?key=\" + self.__api_key\nif(param_dict != None):\n    for dkey in param_dict:\n        url += \"&\" + urllib2.quote(dkey) + \"=\" + urllib2.quote(param_dict[dkey])\n\nlog.debug(\"Generated URL: \"+ url)\n\nlog_http_time = time.time()\nresponse = self.__get_http_response(url)\nlog.info(\"API response time: \" + str(time.time() - log_http_time))\nreturn response", "path": "ctabustracker.py", "repo_name": "cswingler/ctabustracker-py", "stars": 10, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"\nReturns predictions for the given vehicle_ids\n\"\"\"\n\n", "func_signal": "def getpredictions_vehicle(self, *vehicle_ids):\n", "code": "if (len(vehicle_ids) > 10 or len(vehicle_ids) < 1):\n    raise ImproperNumberOfItemsException(len(vehicle_ids))\n\nvehicle_ids_str = str()\nfor vehicle_id in vehicle_ids:\n    vehicle_ids_str = vehicle_ids_str + str(vehicle_id) + \",\"\nvehicle_ids_str = vehicle_ids_str.rstrip(\",\")\n\nquerydict = {\"vid\":vehicle_ids_str}\n\napi_result = self.__get_api_response(\"getpredictions\", querydict)\n                                           \nroot = etree.fromstring(api_result)\n\npredictions = list()\n\npredictions_xml = root.findall('prd')\n\npredictions_list = list()\nfor prediction in predictions_xml:\n    pred_obj = Prediction(timestamp = prediction.find('tmstmp').text,\n                          prediction_type = prediction.find('typ').text,\n                          stop_id = prediction.find('stpid').text,\n                          stop_name = prediction.find('stpnm').text,\n                          vehicle_id = prediction.find('vid').text,\n                          distance_to_stop = prediction.find('dstp').text,\n                          route = prediction.find('rt').text,\n                          route_dir = prediction.find('rtdir').text,\n                          destination = prediction.find('des').text,\n                          predicted_eta = prediction.find('prdtm').text)\n    if (prediction.find('dly') != None):\n        pred_obj.delayed = True\n\n    predictions_list.append(pred_obj)\n\nreturn predictions_list", "path": "ctabustracker.py", "repo_name": "cswingler/ctabustracker-py", "stars": 10, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"\nReturns the time (in minutes) that a vehicle is\nexpected to arrive.\n\nRequires a Prediction object as a parameter, and\ncalculates the difference agains the CTA's clock if\nuse_cta_clock is True.  If it is False, it uses the  local system's\nclock.\n\"\"\"\n\n", "func_signal": "def geteta_from_prediction(self, prediction, use_cta_clock = True):\n", "code": "if (use_cta_clock == True):\n    timenow = self.gettime()\nelse:\n    timenow = time.localtime()\n\nreturn prediction.estimated_time_to_arrival(timenow)", "path": "ctabustracker.py", "repo_name": "cswingler/ctabustracker-py", "stars": 10, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"\nReturns the time that the bus is expected to arrive for this \nPrediction (in minutes).\n\nIf ctatime is unspecified, returns ETA based on the system's clock,\notherwise, matches difference against specified time.\n\"\"\"\n\n", "func_signal": "def estimated_time_to_arrival(self, ctatime = None):\n", "code": "if (ctatime == None):\n    ctatime = time.localtime()\n\nelif (type(ctatime) == str):\n    # Presuming this is a human-readable time string as \n    # returned from the API\n    ctatime == convert_time(ctatime)\n\neta_seconds = time.mktime(self.predicted_eta) - time.mktime(ctatime)\nreturn int(eta_seconds/60)", "path": "ctabustracker.py", "repo_name": "cswingler/ctabustracker-py", "stars": 10, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"\nConstructor for Point.\n\"\"\"\n", "func_signal": "def __init__(self, seq, ptype, lat, long, stop_id = None, stop_name = None, pattern_distance = None):\n", "code": "self.seq = int(seq)\nself.ptype = self.__handle_pattern_type(ptype)\nif (stop_id != None):\n    self.stop_id = str(stop_id)\nelse:\n    self.stop_id = None\nif (stop_name != None):\n    self.stop_name = str(stop_name)\nelse:\n    self.stop_id = None\n\nif (pattern_distance != None):\n    self.pattern_distance = float(pattern_distance)\nelse:\n    self.pattern_distance = None\nself.lat = float(lat)\nself.long = float(long)\nreturn", "path": "ctabustracker.py", "repo_name": "cswingler/ctabustracker-py", "stars": 10, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"\nThrilling string creation stuff! Just dumps the information out in a readable manner.\n\"\"\"\n", "func_signal": "def __str__(self):\n", "code": "return \"Bus number: %s \\\n        \\nTime of update: %s \\\n        \\nLatitude: %s \\\n        \\nLongitude: %s \\\n        \\nHeading: %s \\\n        \\nPattern_ID: %s \\\n        \\nDistance traveled: %s \\\n        \\nRoute: %s \\\n        \\nDestination: %s  \\\n        \\nDelayed?: %s \" \\\n        % (self.vehicle_id, time.asctime(self.timestamp), self.lat, \\\n           self.long, self.heading, self.pattern_id, \\\n           self.pattern_distance, self.route, self.dest, self.delayed)", "path": "ctabustracker.py", "repo_name": "cswingler/ctabustracker-py", "stars": 10, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"\nInitializes the API key for the CTA bus tracker.\nThis module will not work without a valid key.\nIf you do not have an API key, you can request one from\nhttp://www.transitchicago.com/developers/bustracker.aspx\n\napi_url is the base API url to access. If it is not\nspecified, the default is used:\nhttp://www.ctabustracker.com/bustime/api/v1/\n\"\"\"\n", "func_signal": "def __init__(self, api_key, api_url = None):\n", "code": "self.__api_key = api_key\nif (api_url != None):\n    self.__api_url = api_url\nreturn", "path": "ctabustracker.py", "repo_name": "cswingler/ctabustracker-py", "stars": 10, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"\nCreates a Route object\n\"\"\"\n", "func_signal": "def __init__(self, stop_id, stop_name, lat, long):\n", "code": "self.stop_id = int(stop_id)\nself.stop_name = str(stop_name)\nself.lat = str(lat)\nself.long = str(long)\n\nreturn", "path": "ctabustracker.py", "repo_name": "cswingler/ctabustracker-py", "stars": 10, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"\nAppends a point to this pattern\n\"\"\"\n# TODO: Might want to have this sort out the points every time?\n", "func_signal": "def append(self, point):\n", "code": "self.points.append(point)\nreturn", "path": "ctabustracker.py", "repo_name": "cswingler/ctabustracker-py", "stars": 10, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"\nReturns patterns given a list of up to 10 pattern ids\n\"\"\"\n\n", "func_signal": "def getpatterns_pid(self, *patternids):\n", "code": "if (len(patternids) > 10):\n    raise ImproperNumberOfItemsException(len(patternids))\n\npid_query_string = str()\nfor pid in patternids:\n    pid_query_string = pid_query_string + str(pid) + \", \"\npid_query_string.rstrip(\",\")\n\nquerydict = {\"pid\": pid_query_string}\n\napi_result = self.__get_api_response(\"getpatterns\", querydict)\n\nroot = etree.fromstring(api_result)\n\npatterns = list()\npatterns_xml = root.findall('ptr')\n\nfor pattern in patterns_xml:\n    pat_obj = Pattern(pattern_id = pattern.find('pid').text, \\\n                      length = pattern.find('ln').text,\n                      direction = pattern.find('rtdir').text)\n    for point in pattern.findall('pt'):\n        if (point.find('stpid') != None):\n            stop_id = point.find('stpid').text\n        else:\n            stop_id = None\n        if (point.find('stpnm') != None):\n            stop_name = point.find('stpnm').text\n        else:\n            stop_name = None\n        if (point.find('pdist') != None):\n            pattern_distance = point.find('pdist').text\n        else:\n            pattern_distance = None\n            \n        point_obj = Point(seq = point.find('seq').text, \\\n                          ptype = point.find('typ').text, \\\n                          lat = point.find('lat').text, \\\n                          long = point.find('lon').text, \\\n                          stop_id = stop_id,\\\n                          stop_name = stop_name,\\\n                          pattern_distance = pattern_distance)\n        pat_obj.append(point_obj)\n    patterns.append(pat_obj)\nreturn patterns", "path": "ctabustracker.py", "repo_name": "cswingler/ctabustracker-py", "stars": 10, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"\nGets bulletins related to given stop ids. \n\"\"\"\n\n# The spec doesn't list a maximum, but I'm going to presume\n# it's still 10.\n", "func_signal": "def getbulletins_stops(self, *stopids):\n", "code": "if (len(stopids) > 10 or len(stopids) < 1):\n    raise ImproperNumberOfItemsException(len(stopids))\n\nstopids_str = str()\n\nfor stop in stopids:\n    stopids_str += str(stop) + \",\"\nstopids_str.rstrip(\",\")\n\nquerydict = {'rt':stopids_str}\n\napi_result = self.__get_api_response(\"getservicebulletins\", querydict)\n\nroot = etree.fromstring(api_result)\n\nbulletins_xml = root.findall('sb')\n\nbulletins_list = list()\n\nfor bulletin in bulletins_xml:\n    bulletin_obj = Service_Bulletin(name = bulletin.find('nm').text,\n                                    subject = bulletin.find('sbj').text,\n                                    detail = bulletin.find('dtl').text,\n                                    brief = bulletin.find('brf').text,\n                                    priority = bulletin.find('prty').text)\n    for sb in bulletin.findall('srvc'):\n        route = None\n        direction = None\n        stop_num = None\n        stop_name = None\n\n\n        if (sb.find('rt') != None):\n            route = sb.find('rt').text\n        if (sb.find('rtdir') != None):\n            direction = sb.find('rtdir').text\n        if (sb.find('stpid') != None):\n            stop_num = sb.find('stpid').text\n        if (sb.find('stpnm') != None):\n            stop_name = sb.find('stpnm').text\n            \n        bulletin_obj.append(route = route,\n                            direction = direction,\n                            stop_num = stop_num,\n                            stop_name = stop_name)\n\n    bulletins_list.append(bulletin_obj)\n\nreturn bulletins_list", "path": "ctabustracker.py", "repo_name": "cswingler/ctabustracker-py", "stars": 10, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"\nRetrieves bus locations for the given bus IDs.  vehicle_ids\ncan either be a single integer (for one bus) or a list of up to\n10 elements.\n\"\"\"\n\n", "func_signal": "def getvehicles_vid(self, *vehicleids):\n", "code": "if (len(vehicleids) > 10 or len(vehicleids) < 0):\n    raise ImproperNumberOfItemsException(len(vehicleids))\n\nval = str()\nfor vid in vehicleids:\n    val = val + str(vid) + \",\"\nval = val.rstrip(\",\")\n\nquerydict = {\"vid\": val}\n\napi_result = self.__get_api_response(\"getvehicles\",querydict)\n\ndebug_start_time = time.time()\nroot = etree.fromstring(api_result)\nvehicles_xml = root.findall('vehicle')\n\nvehicles = list()\nfor vehicle in vehicles_xml:\n    # Yank the delay flag out\n    try:\n        vehicle.find('dly').text\n    except AttributeError:\n        delayed = False\n    else:\n        delayed = True\n    # Create an empty vehicle object\n    v_obj = Vehicle(vehicle_id = vehicle.find('vid').text, \\\n                    timestamp = vehicle.find('tmstmp').text, \\\n                    lat = vehicle.find('lat').text, \\\n                    long = vehicle.find('lon').text, \\\n                    heading = vehicle.find('hdg').text, \\\n                    pattern_id = vehicle.find('pid').text, \\\n                    pattern_distance = vehicle.find('pdist').text, \\\n                    route = vehicle.find('rt').text, \\\n                    dest = vehicle.find('des').text, \\\n                    delayed = delayed)\n    # Append it to the vehicles list\n    vehicles.append(v_obj)\n\nlog.info(\"XML Processing time for getvehicles_vid(): \" + str(time.time() - debug_start_time))\nreturn vehicles", "path": "ctabustracker.py", "repo_name": "cswingler/ctabustracker-py", "stars": 10, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"\nConverts a CTA time stamp from XML into a\ntime_struct\n\"\"\"\n", "func_signal": "def convert_time(timestring):\n", "code": "try:\n    return time.strptime(timestring, \"%Y%m%d %H:%M:%S\")\nexcept ValueError:\n    # For some reason, the API supports seconds in some places,\n    # and not elsewhere.\n    return time.strptime(timestring, \"%Y%m%d %H:%M\")", "path": "ctabustracker.py", "repo_name": "cswingler/ctabustracker-py", "stars": 10, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"\nDefines a patern. points can be a list of points, or None.\n\"\"\"\n", "func_signal": "def __init__(self, pattern_id, length, direction, points = None):\n", "code": "self.pattern_id = int(pattern_id)\nself.length = int(float(length)) # The API spec says this an int, but returns a float.\nself.direction = str(direction)\nif (points != None):\n    for point in points:\n        self.append(points)\n\nreturn", "path": "ctabustracker.py", "repo_name": "cswingler/ctabustracker-py", "stars": 10, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"\nGets bulletins related to routes.\n\"\"\"\n\n", "func_signal": "def getbulletins_route(self, *routes):\n", "code": "if (len(routes) > 10 or len(routes) < 1):\n    raise ImproperNumberOfItemsException(len(routes))\n\nroutes_str = str()\n\nfor route in routes:\n    routes_str += str(route) + \",\"\nroutes_str.rstrip(\",\")\n\nquerydict = {'rt':routes_str}\n\napi_result = self.__get_api_response(\"getservicebulletins\", querydict)\n\nroot = etree.fromstring(api_result)\n\nbulletins_xml = root.findall('sb')\n\nbulletins_list = list()\n\nfor bulletin in bulletins_xml:\n    bulletin_obj = Service_Bulletin(name = bulletin.find('nm').text,\n                                    subject = bulletin.find('sbj').text,\n                                    detail = bulletin.find('dtl').text,\n                                    brief = bulletin.find('brf').text,\n                                    priority = bulletin.find('prty').text)\n    for sb in bulletin.findall('srvc'):\n        route = None\n        direction = None\n        stop_num = None\n        stop_name = None\n\n\n        if (sb.find('rt') != None):\n            route = sb.find('rt').text\n        if (sb.find('rtdir') != None):\n            direction = sb.find('rtdir').text\n        if (sb.find('stpid') != None):\n            stop_num = sb.find('stpid').text\n        if (sb.find('stpnm') != None):\n            stop_name = sb.find('stpnm').text\n            \n        bulletin_obj.append(route = route,\n                            direction = direction,\n                            stop_num = stop_num,\n                            stop_name = stop_name)\n\n    bulletins_list.append(bulletin_obj)\n\nreturn bulletins_list", "path": "ctabustracker.py", "repo_name": "cswingler/ctabustracker-py", "stars": 10, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"\nConverts a CTA time stamp from XML into a\ntime_struct\n\"\"\"\n", "func_signal": "def __convert_time(self, timestring):\n", "code": "try:\n    return time.strptime(timestring, \"%Y%m%d %H:%M:%S\")\nexcept ValueError:\n    # For some reason, the API supports seconds in some places,\n    # and not elsewhere.\n    return time.strptime(timestring, \"%Y%m%d %H:%M\")", "path": "ctabustracker.py", "repo_name": "cswingler/ctabustracker-py", "stars": 10, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"\nAppends an SB_Service object to this bulletin\n\"\"\"\n", "func_signal": "def append(self, stop_name, route = None, direction = None, stop_num = None):\n", "code": "log.debug(\"Route in append: \" + str(route))\nlog.debug(\"direction in append: \" + str(direction))\nnew_sb = SB_Service(route = route,\n                    direction = direction,\n                    stop_num = stop_num,\n                    stop_name = stop_name)\nself.affected_services.append(new_sb)\nreturn", "path": "ctabustracker.py", "repo_name": "cswingler/ctabustracker-py", "stars": 10, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"\nReturns the time (as a time object) \nas according to the BusTracker system.\n\"\"\"\n\n", "func_signal": "def gettime(self):\n", "code": "local_time = time.localtime()\n\nurl = self.__api_url + \"gettime?key=\" + self.__api_key\nresponse = self.__get_http_response(url)\n\ntree = etree.fromstring(response)\ntimestring = tree.findtext(\"tm\")\ncta_time = time.strptime(timestring, \"%Y%m%d %H:%M:%S\")\n\nlog.debug(\"TIME CALLED:\")\nlog.debug(\"Local System time: \" + time.strftime(\"%Y%m%d %H:%M:%S\", local_time))\nlog.debug(\"CTA time: \" + time.strftime(\"%Y%m%d %H:%M:%S\", cta_time))\n\ntime_diff = abs(time.mktime(local_time) - time.mktime(cta_time))\nlog.debug(\"Time difference: \" + str(time_diff))\nif ( time_diff > 5 ):\n    log.warn(\"Time difference between CTA and local system clock is greater than 5 seconds!\") \n\nreturn cta_time", "path": "ctabustracker.py", "repo_name": "cswingler/ctabustracker-py", "stars": 10, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\"\nReturns a list of Stop objects on a given route\ndirection must be a string specifing the direction of the bus.\n\nThe returned list is unordered. Ordering is accomplished\nby constructing a pattern.\n\"\"\"\n", "func_signal": "def getroute_stops(self, route, direction):\n", "code": "route = str(route)\n\nquerydict = {\"rt\": route, \"dir\": direction}\napi_result = self.__get_api_response(\"getstops\", querydict)\n\nroot = etree.fromstring(api_result)\n\nstops = list()\nstops_xml = root.findall('stop')\n\nfor stop in stops_xml:\n   s_obj = Stop(stop_id = stop.find('stpid').text, \\\n                stop_name = stop.find('stpnm').text,\n                lat = stop.find('lat').text,\n                long = stop.find('lon').text)\n   stops.append(s_obj)\nreturn stops", "path": "ctabustracker.py", "repo_name": "cswingler/ctabustracker-py", "stars": 10, "license": "None", "language": "python", "size": 108}
{"docstring": "\"\"\" p.splitext() -> Return (p.stripext(), p.ext).\n\nSplit the filename extension from this path and return\nthe two parts.  Either part may be empty.\n\nThe extension is everything from '.' to the end of the\nlast path segment.  This has the property that if\n(a, b) == p.splitext(), then a + b == p.\n\"\"\"\n", "func_signal": "def splitext(self):\n", "code": "filename, ext = os.path.splitext(self)\nreturn self.__class__(filename), ext", "path": "imobilesync\\path.py", "repo_name": "bryanforbes/imobilesync", "stars": 14, "license": "None", "language": "python", "size": 158}
{"docstring": "# Check for vendor and product id\n", "func_signal": "def is_interesting(self, udi, props):\n", "code": "if props.get(\"usb_device.vendor_id\") == self.__vendor_id:\n    if props.get(\"usb_device.product_id\") in self.__supported_devices:\n        return True\nreturn False", "path": "conduit\\iDeviceModule.py", "repo_name": "bryanforbes/imobilesync", "stars": 14, "license": "None", "language": "python", "size": 158}
{"docstring": "\"\"\" D.listdir() -> List of items in this directory.\n\nUse D.files() or D.dirs() instead if you want a listing\nof just files or just subdirectories.\n\nThe elements of the list are path objects.\n\nWith the optional 'pattern' argument, this only lists\nitems whose names match the given pattern.\n\"\"\"\n", "func_signal": "def listdir(self, pattern=None):\n", "code": "names = os.listdir(self)\nif pattern is not None:\n    names = fnmatch.filter(names, pattern)\nreturn [self / child for child in names]", "path": "imobilesync\\path.py", "repo_name": "bryanforbes/imobilesync", "stars": 14, "license": "None", "language": "python", "size": 158}
{"docstring": "\"\"\"\nIf the executable given isn't an absolute path, search $PATH for the interpreter\n\"\"\"\n", "func_signal": "def resolve_interpreter(exe):\n", "code": "if os.path.abspath(exe) != exe:\n    paths = os.environ.get('PATH', '').split(os.pathsep)\n    for path in paths:\n        if os.path.exists(os.path.join(path, exe)):\n            exe = os.path.join(path, exe)\n            break\nif not os.path.exists(exe):\n    logger.fatal('The executable %s (from --python=%s) does not exist' % (exe, exe))\n    sys.exit(3)\nreturn exe", "path": "bootstrap.py", "repo_name": "bryanforbes/imobilesync", "stars": 14, "license": "None", "language": "python", "size": 158}
{"docstring": "\"\"\"\nMakes the already-existing environment use relative paths, and takes out \nthe #!-based environment selection in scripts.\n\"\"\"\n", "func_signal": "def make_environment_relocatable(home_dir):\n", "code": "activate_this = os.path.join(home_dir, 'bin', 'activate_this.py')\nif not os.path.exists(activate_this):\n    logger.fatal(\n        'The environment doesn\\'t have a file %s -- please re-run virtualenv '\n        'on this environment to update it' % activate_this)\nfixup_scripts(home_dir)\nfixup_pth_and_egg_link(home_dir)\n## FIXME: need to fix up distutils.cfg", "path": "bootstrap.py", "repo_name": "bryanforbes/imobilesync", "stars": 14, "license": "None", "language": "python", "size": 158}
{"docstring": "\"\"\" Return the path to which this symbolic link points.\n\nThe result is always an absolute path.\n\"\"\"\n", "func_signal": "def readlinkabs(self):\n", "code": "p = self.readlink()\nif p.isabs():\n    return p\nelse:\n    return (self.parent / p).abspath()", "path": "imobilesync\\path.py", "repo_name": "bryanforbes/imobilesync", "stars": 14, "license": "None", "language": "python", "size": 158}
{"docstring": "\"\"\" p.splitdrive() -> Return (p.drive, <the rest of p>).\n\nSplit the drive specifier from this path.  If there is\nno drive specifier, p.drive is empty, so the return value\nis simply (path(''), p).  This is always the case on Unix.\n\"\"\"\n", "func_signal": "def splitdrive(self):\n", "code": "drive, rel = os.path.splitdrive(self)\nreturn self.__class__(drive), rel", "path": "imobilesync\\path.py", "repo_name": "bryanforbes/imobilesync", "stars": 14, "license": "None", "language": "python", "size": 158}
{"docstring": "\"\"\"If we are in a progress scope, and no log messages have been\nshown, write out another '.'\"\"\"\n", "func_signal": "def show_progress(self):\n", "code": "if self.in_progress_hanging:\n    sys.stdout.write('.')\n    sys.stdout.flush()", "path": "bootstrap.py", "repo_name": "bryanforbes/imobilesync", "stars": 14, "license": "None", "language": "python", "size": 158}
{"docstring": "\"\"\"\nSome platforms (particularly Gentoo on x64) put things in lib64/pythonX.Y\ninstead of lib/pythonX.Y.  If this is such a platform we'll just create a\nsymlink so lib64 points to lib\n\"\"\"\n", "func_signal": "def fix_lib64(lib_dir):\n", "code": "if [p for p in distutils.sysconfig.get_config_vars().values() \n    if isinstance(p, basestring) and 'lib64' in p]:\n    logger.debug('This system uses lib64; symlinking lib64 to lib')\n    assert os.path.basename(lib_dir) == 'python%s' % sys.version[:3], (\n        \"Unexpected python lib dir: %r\" % lib_dir)\n    lib_parent = os.path.dirname(lib_dir)\n    assert os.path.basename(lib_parent) == 'lib', (\n        \"Unexpected parent dir: %r\" % lib_parent)\n    copyfile(lib_parent, os.path.join(os.path.dirname(lib_parent), 'lib64'))", "path": "bootstrap.py", "repo_name": "bryanforbes/imobilesync", "stars": 14, "license": "None", "language": "python", "size": 158}
{"docstring": "\"\"\" Return a relative path from self to dest.\n\nIf there is no relative path from self to dest, for example if\nthey reside on different drives in Windows, then this returns\ndest.abspath().\n\"\"\"\n", "func_signal": "def relpathto(self, dest):\n", "code": "origin = self.abspath()\ndest = self.__class__(dest).abspath()\n\norig_list = origin.normcase().splitall()\n# Don't normcase dest!  We want to preserve the case.\ndest_list = dest.splitall()\n\nif orig_list[0] != os.path.normcase(dest_list[0]):\n    # Can't get here from there.\n    return dest\n\n# Find the location where the two paths start to differ.\ni = 0\nfor start_seg, dest_seg in zip(orig_list, dest_list):\n    if start_seg != os.path.normcase(dest_seg):\n        break\n    i += 1\n\n# Now i is the point where the two paths diverge.\n# Need a certain number of \"os.pardir\"s to work up\n# from the origin to the point of divergence.\nsegments = [os.pardir] * (len(orig_list) - i)\n# Need to add the diverging part of dest_list.\nsegments += dest_list[i:]\nif len(segments) == 0:\n    # If they happen to be identical, use os.curdir.\n    relpath = os.curdir\nelse:\n    relpath = os.path.join(*segments)\nreturn self.__class__(relpath)", "path": "imobilesync\\path.py", "repo_name": "bryanforbes/imobilesync", "stars": 14, "license": "None", "language": "python", "size": 158}
{"docstring": "\"\"\" Open this file, read all bytes, return them as a string. \"\"\"\n", "func_signal": "def bytes(self):\n", "code": "f = self.open('rb')\ntry:\n    return f.read()\nfinally:\n    f.close()", "path": "imobilesync\\path.py", "repo_name": "bryanforbes/imobilesync", "stars": 14, "license": "None", "language": "python", "size": 158}
{"docstring": "\"\"\" D.walkfiles() -> iterator over files in D, recursively.\n\nThe optional argument, pattern, limits the results to files\nwith names that match the pattern.  For example,\nmydir.walkfiles('*.tmp') yields only files with the .tmp\nextension.\n\"\"\"\n", "func_signal": "def walkfiles(self, pattern=None, errors='strict'):\n", "code": "if errors not in ('strict', 'warn', 'ignore'):\n    raise ValueError(\"invalid errors parameter\")\n\ntry:\n    childList = self.listdir()\nexcept Exception:\n    if errors == 'ignore':\n        return\n    elif errors == 'warn':\n        warnings.warn(\n            \"Unable to list directory '%s': %s\"\n            % (self, sys.exc_info()[1]),\n            TreeWalkWarning)\n        return\n    else:\n        raise\n\nfor child in childList:\n    try:\n        isfile = child.isfile()\n        isdir = not isfile and child.isdir()\n    except:\n        if errors == 'ignore':\n            continue\n        elif errors == 'warn':\n            warnings.warn(\n                \"Unable to access '%s': %s\"\n                % (self, sys.exc_info()[1]),\n                TreeWalkWarning)\n            continue\n        else:\n            raise\n\n    if isfile:\n        if pattern is None or child.fnmatch(pattern):\n            yield child\n    elif isdir:\n        for f in child.walkfiles(pattern, errors):\n            yield f", "path": "imobilesync\\path.py", "repo_name": "bryanforbes/imobilesync", "stars": 14, "license": "None", "language": "python", "size": 158}
{"docstring": "\"\"\"Makes .pth and .egg-link files use relative paths\"\"\"\n", "func_signal": "def fixup_pth_and_egg_link(home_dir):\n", "code": "home_dir = os.path.normcase(os.path.abspath(home_dir))\nfor path in sys.path:\n    if not path:\n        path = '.'\n    if not os.path.isdir(path):\n        continue\n    path = os.path.normcase(os.path.abspath(path))\n    if not path.startswith(home_dir):\n        logger.debug('Skipping system (non-environment) directory %s' % path)\n        continue\n    for filename in os.listdir(path):\n        filename = os.path.join(path, filename)\n        if filename.endswith('.pth'):\n            if not os.access(filename, os.W_OK):\n                logger.warn('Cannot write .pth file %s, skipping' % filename)\n            else:\n                fixup_pth_file(filename)\n        if filename.endswith('.egg-link'):\n            if not os.access(filename, os.W_OK):\n                logger.warn('Cannot write .egg-link file %s, skipping' % filename)\n            else:\n                fixup_egg_link(filename)", "path": "bootstrap.py", "repo_name": "bryanforbes/imobilesync", "stars": 14, "license": "None", "language": "python", "size": 158}
{"docstring": "\"\"\" Calculate the md5 hash for this file.\n\nThis reads through the entire file.\n\"\"\"\n", "func_signal": "def read_md5(self):\n", "code": "f = self.open('rb')\ntry:\n    m = hashlib.md5()\n    while True:\n        d = f.read(8192)\n        if not d:\n            break\n        m.update(d)\nfinally:\n    f.close()\nreturn m.digest()", "path": "imobilesync\\path.py", "repo_name": "bryanforbes/imobilesync", "stars": 14, "license": "None", "language": "python", "size": 158}
{"docstring": "\"\"\" Return a list of path objects that match the pattern.\n\npattern - a path relative to this directory, with wildcards.\n\nFor example, path('/users').glob('*/bin/*') returns a list\nof all the files users have in their bin directories.\n\"\"\"\n", "func_signal": "def glob(self, pattern):\n", "code": "cls = self.__class__\nreturn [cls(s) for s in glob.glob(_base(self / pattern))]", "path": "imobilesync\\path.py", "repo_name": "bryanforbes/imobilesync", "stars": 14, "license": "None", "language": "python", "size": 158}
{"docstring": "\"\"\"\n>>> l = Logger()\n>>> l.level_matches(3, 4)\nFalse\n>>> l.level_matches(3, 2)\nTrue\n>>> l.level_matches(slice(None, 3), 3)\nFalse\n>>> l.level_matches(slice(None, 3), 2)\nTrue\n>>> l.level_matches(slice(1, 3), 1)\nTrue\n>>> l.level_matches(slice(2, 3), 1)\nFalse\n\"\"\"\n", "func_signal": "def level_matches(self, level, consumer_level):\n", "code": "if isinstance(level, slice):\n    start, stop = level.start, level.stop\n    if start is not None and start > consumer_level:\n        return False\n    if stop is not None or stop <= consumer_level:\n        return False\n    return True\nelse:\n    return level >= consumer_level", "path": "bootstrap.py", "repo_name": "bryanforbes/imobilesync", "stars": 14, "license": "None", "language": "python", "size": 158}
{"docstring": "\"\"\"\nMake a filename relative, where the filename is dest, and it is\nbeing referred to from the filename source.\n\n    >>> make_relative_path('/usr/share/something/a-file.pth',\n    ...                    '/usr/share/another-place/src/Directory')\n    '../another-place/src/Directory'\n    >>> make_relative_path('/usr/share/something/a-file.pth',\n    ...                    '/home/user/src/Directory')\n    '../../../home/user/src/Directory'\n    >>> make_relative_path('/usr/share/a-file.pth', '/usr/share/')\n    './'\n\"\"\"\n", "func_signal": "def make_relative_path(source, dest, dest_is_directory=True):\n", "code": "source = os.path.dirname(source)\nif not dest_is_directory:\n    dest_filename = os.path.basename(dest)\n    dest = os.path.dirname(dest)\ndest = os.path.normpath(os.path.abspath(dest))\nsource = os.path.normpath(os.path.abspath(source))\ndest_parts = dest.strip(os.path.sep).split(os.path.sep)\nsource_parts = source.strip(os.path.sep).split(os.path.sep)\nwhile dest_parts and source_parts and dest_parts[0] == source_parts[0]:\n    dest_parts.pop(0)\n    source_parts.pop(0)\nfull_parts = ['..']*len(source_parts) + dest_parts\nif not dest_is_directory:\n    full_parts.append(dest_filename)\nif not full_parts:\n    # Special case for the current directory (otherwise it'd be '')\n    return './'\nreturn os.path.sep.join(full_parts)", "path": "bootstrap.py", "repo_name": "bryanforbes/imobilesync", "stars": 14, "license": "None", "language": "python", "size": 158}
{"docstring": "\"\"\" p.splitpath() -> Return (p.parent, p.name). \"\"\"\n", "func_signal": "def splitpath(self):\n", "code": "parent, child = os.path.split(self)\nreturn self.__class__(parent), child", "path": "imobilesync\\path.py", "repo_name": "bryanforbes/imobilesync", "stars": 14, "license": "None", "language": "python", "size": 158}
{"docstring": "\"\"\"Return the path locations for the environment (where libraries are,\nwhere scripts go, etc)\"\"\"\n# XXX: We'd use distutils.sysconfig.get_python_inc/lib but its\n# prefix arg is broken: http://bugs.python.org/issue3386\n", "func_signal": "def path_locations(home_dir):\n", "code": "if sys.platform == 'win32':\n    # Windows has lots of problems with executables with spaces in\n    # the name; this function will remove them (using the ~1\n    # format):\n    mkdir(home_dir)\n    import win32api\n    home_dir = win32api.GetShortPathName(home_dir)\n    lib_dir = join(home_dir, 'Lib')\n    inc_dir = join(home_dir, 'Include')\n    bin_dir = join(home_dir, 'Scripts')\nelif is_jython:\n    lib_dir = join(home_dir, 'Lib')\n    inc_dir = join(home_dir, 'Include')\n    bin_dir = join(home_dir, 'bin')\nelse:\n    lib_dir = join(home_dir, 'lib', py_version)\n    inc_dir = join(home_dir, 'include', py_version)\n    bin_dir = join(home_dir, 'bin')\nreturn home_dir, lib_dir, inc_dir, bin_dir", "path": "bootstrap.py", "repo_name": "bryanforbes/imobilesync", "stars": 14, "license": "None", "language": "python", "size": 158}
{"docstring": "\"\"\"\nCreates a bootstrap script, which is like this script but with\nextend_parser, adjust_options, and after_install hooks.\n\nThis returns a string that (written to disk of course) can be used\nas a bootstrap script with your own customizations.  The script\nwill be the standard virtualenv.py script, with your extra text\nadded (your extra text should be Python code).\n\nIf you include these functions, they will be called:\n\n``extend_parser(optparse_parser)``:\n    You can add or remove options from the parser here.\n\n``adjust_options(options, args)``:\n    You can change options here, or change the args (if you accept\n    different kinds of arguments, be sure you modify ``args`` so it is\n    only ``[DEST_DIR]``).\n\n``after_install(options, home_dir)``:\n\n    After everything is installed, this function is called.  This\n    is probably the function you are most likely to use.  An\n    example would be::\n\n        def after_install(options, home_dir):\n            subprocess.call([join(home_dir, 'bin', 'easy_install'),\n                             'MyPackage'])\n            subprocess.call([join(home_dir, 'bin', 'my-package-script'),\n                             'setup', home_dir])\n\n    This example immediately installs a package, and runs a setup\n    script from that package.\n\nIf you provide something like ``python_version='2.4'`` then the\nscript will start with ``#!/usr/bin/env python2.4`` instead of\n``#!/usr/bin/env python``.  You can use this when the script must\nbe run with a particular Python version.\n\"\"\"\n", "func_signal": "def create_bootstrap_script(extra_text, python_version=''):\n", "code": "filename = __file__\nif filename.endswith('.pyc'):\n    filename = filename[:-1]\nf = open(filename, 'rb')\ncontent = f.read()\nf.close()\npy_exe = 'python%s' % python_version\ncontent = (('#!/usr/bin/env %s\\n' % py_exe)\n           + '## WARNING: This file is generated\\n'\n           + content)\nreturn content.replace('##EXT' 'END##', extra_text)", "path": "bootstrap.py", "repo_name": "bryanforbes/imobilesync", "stars": 14, "license": "None", "language": "python", "size": 158}
{"docstring": "\"\"\"\nBuild the indicator vector, each N entries\nindicates N possible values for each cell\n\"\"\"\n", "func_signal": "def to_indicator_vector(self):\n", "code": "Acols = self.N**3\nv = matrix(0, (Acols,1), 'd')\nfor ix, e in enumerate(self.entries):\n    if e:\n        v[ix*self.N + e-1] = 1\nreturn v", "path": "sudoku.py", "repo_name": "benmoran/L1-Sudoku", "stars": 8, "license": "None", "language": "python", "size": 87}
{"docstring": "\"\"\"\nTest the problem matrix generated by the Problem\n\"\"\"\n", "func_signal": "def test_matrix(self):\n", "code": "m = self.sudoku.matrix()\nself.assertTrue(isinstance(m ,matrix ))\n\nnentries = len([e for e in self.sudoku.entries if e])\nN = self.sudoku.N\nself.assertEqual((4 * N**2 + nentries, N**3), m.size)", "path": "tests.py", "repo_name": "benmoran/L1-Sudoku", "stars": 8, "license": "None", "language": "python", "size": 87}
{"docstring": "\"\"\"\nFind x with min l1 such that Ax=y,\nusing iteratively reweighted l1 minimization\n\"\"\"\n#Reweighted l1 approach from Candes Wakin and Boyd Enhancing sparsity by reweighted l1 minimization. J. Fourier Anal. Appl., 14 877-905. and http://www.acm.caltech.edu/~emmanuel/papers/rwl1.pdf:\n\n## from http://sites.google.com/site/stephanegchretien/alternatingl1\n", "func_signal": "def solve_iter_reweighted_l1(A, y, solver='glpk', iters=4):\n", "code": "n = A.size[1]\n\nc0 = ones_v(2*n)\n\nG1 = concathoriz(A,-A)\nG2 = concathoriz(-A,A)\nG3 = -eye(2*n)\nG = reduce(concatvert, [G1,G2,G3])\nhh = reduce(concatvert, [y, -y, zeros_v(2*n)])\n\nsol = cvxopt.solvers.lp(c0, G, hh, solver=solver)\n\ndoublexlone = sol['x'][:2*n] #.trans()[0]\nxlone = concathoriz(eye(n),-eye(n)) * doublexlone\n\nxtmp = doublexlone\nfor l in range(iters):\n    #c1u = (abs(doublexlone)+.1)**-1\n    c1u = (abs(xtmp)+.1)**-1 # should it be this?\n    sol = cvxopt.solvers.lp(c1u, G, hh)\n    solstixdt =  sol['x'][:2*n] #.trans()[0]\n    xlone = concathoriz(eye(n),-eye(n)) * solstixdt\n    xtmp = solstixdt        \n\nv = sol['x'][:n]\n\nreturn v", "path": "sudoku.py", "repo_name": "benmoran/L1-Sudoku", "stars": 8, "license": "None", "language": "python", "size": 87}
{"docstring": "\"\"\"\nCheck the matrix constraint that ensures\neach row contains all digits\n\"\"\"\n# - Construct a Problem with all cells filled\n# - Get the all_cells matrix\n# - Multiply it with the indicator vector\n# - check the result is all ones\n# - blank a cell, repeat\n# - check the result is not all ones\n", "func_signal": "def test_col_digits_constraint(self):\n", "code": "N = 9\n\nentries = reduce(operator.__add__,\n                 [list(itertools.repeat(i,N)) for  i in range(1,N+1)])\nsudoku = Problem(entries)        \nv = sudoku.get_result(col_digits=True)\nself.assertOnes(v)\nv = self.sudoku.get_result(col_digits=True)\nself.assertNotOnes(v)", "path": "tests.py", "repo_name": "benmoran/L1-Sudoku", "stars": 8, "license": "None", "language": "python", "size": 87}
{"docstring": "\"\"\"\nConcatenate two matrices horizontally.\n\n>>> print concathoriz(eye(3),eye(3))\n[   1.000    0.000    0.000    1.000    0.000    0.000]\n[   0.000    1.000    0.000    0.000    1.000    0.000]\n[   0.000    0.000    1.000    0.000    0.000    1.000]\n<BLANKLINE>\n\"\"\"\n", "func_signal": "def concathoriz(m1, m2):\n", "code": "r1, c1 = m1.size\nr2, c2 = m2.size\nif  r1 != r2:\n    raise TypeError('Heights don''t match, %d and %d' % (r1,r2))\nreturn matrix(list(m1)+list(m2), (r1,c1+c2), 'd')", "path": "sudoku.py", "repo_name": "benmoran/L1-Sudoku", "stars": 8, "license": "None", "language": "python", "size": 87}
{"docstring": "\"\"\"\nCheck the matrix constraint that ensures\neach box contains all digits\n\"\"\"\n# - Construct a Problem with all cells filled\n# - Get the all_cells matrix\n# - Multiply it with the indicator vector\n# - check the result is all ones\n# - blank a cell, repeat\n# - check the result is not all ones\n", "func_signal": "def test_box_digits_constraint(self):\n", "code": "N = 9\nsudoku = Problem(\"123123123\"\n                 \"456456456\"\n                 \"789789789\"\n                 \"123123123\"\n                 \"456456456\"\n                 \"789789789\"\n                 \"123123123\"\n                 \"456456456\"\n                 \"789789789\")\nv = sudoku.get_result(box_digits=True)\nself.assertOnes(v)\nv = self.sudoku.get_result(box_digits=True)\nself.assertNotOnes(v)", "path": "tests.py", "repo_name": "benmoran/L1-Sudoku", "stars": 8, "license": "None", "language": "python", "size": 87}
{"docstring": "\"\"\"\nCheck the matrix constraint that ensures\nall cells are filled.\n\"\"\"\n# - Construct a Problem with all cells filled\n# - Get the all_cells matrix\n# - Multiply it with the indicator vector\n# - check the result is all ones\n# - blank a cell, repeat\n# - check the result is not all ones\n", "func_signal": "def test_all_cells_constraint(self):\n", "code": "N = self.N\nsudoku = Problem(\"1\"*N**2)\nv = sudoku.get_result(all_cells=True)\nself.assertOnes(v)\nv = self.sudoku.get_result(all_cells=True)\nself.assertNotOnes(v)", "path": "tests.py", "repo_name": "benmoran/L1-Sudoku", "stars": 8, "license": "None", "language": "python", "size": 87}
{"docstring": "\"\"\"\nReturn a sample Problem instance.\n\"\"\"\n", "func_signal": "def sample():\n", "code": "return Problem(\n    \"...15..7.\"\n    \"1.6...82.\"\n    \"3..86..4.\"\n    \"9..4..567\"\n    \"..47.83..\"\n    \"732..6..4\"\n    \".4..81..9\"\n    \".17...2.8\"\n    \".5..37...\")", "path": "sudoku.py", "repo_name": "benmoran/L1-Sudoku", "stars": 8, "license": "None", "language": "python", "size": 87}
{"docstring": "\"\"\"\nReturn a \"tricky\" Problem instance from the paper.\n\"\"\"\n", "func_signal": "def sample_tricky():\n", "code": "return Problem(\n    \"..3..9.81\"\n    \"...2...6.\"\n    \"5...1.7..\"\n    \"89.......\"\n    \"..56.12..\"\n    \".......37\"\n    \"..9..2..8\"\n    \".7...4...\"\n    \"25.8..6..\")", "path": "sudoku.py", "repo_name": "benmoran/L1-Sudoku", "stars": 8, "license": "None", "language": "python", "size": 87}
{"docstring": "\"\"\"\nCheck the matrix constraint that ensures\neach row contains all digits\n\"\"\"\n# - Construct a Problem with all cells filled\n# - Get the all_cells matrix\n# - Multiply it with the indicator vector\n# - check the result is all ones\n# - blank a cell, repeat\n# - check the result is not all ones\n", "func_signal": "def test_row_digits_constraint(self):\n", "code": "N = 9\nsudoku = Problem(\"123456789\"*N)\nv = sudoku.get_result(row_digits=True)\nself.assertOnes(v)\nv = self.sudoku.get_result(row_digits=True)\nself.assertNotOnes(v)", "path": "tests.py", "repo_name": "benmoran/L1-Sudoku", "stars": 8, "license": "None", "language": "python", "size": 87}
{"docstring": "\"\"\"\nTest solving the Problem.\n\"\"\"\n# Easy test\n", "func_signal": "def test_solve(self):\n", "code": "self.sudoku = Problem('.81.749....4.193.7379.85.14..7831...238456179..69274..843562791762198543..5743862')\nanswer = self.sudoku.solve()\ncheckAnswer = Problem('681374925524619387379285614497831256238456179156927438843562791762198543915743862')\n\nself.assertEqual(unicode(checkAnswer), unicode(answer) )\n\nanswer = self.sudoku.solve()\ncheckAnswer = Problem('681374925524619387379285614497831256238456179156927438843562791762198543915743862')\n\nself.assertEqual(unicode(checkAnswer), unicode(answer) )", "path": "tests.py", "repo_name": "benmoran/L1-Sudoku", "stars": 8, "license": "None", "language": "python", "size": 87}
{"docstring": "\"\"\"\nConcatenate two matrices vertically.\n\n>>> print concatvert(eye(3),eye(3))\n[   1.000    0.000    0.000]\n[   0.000    1.000    0.000]\n[   0.000    0.000    1.000]\n[   1.000    0.000    0.000]\n[   0.000    1.000    0.000]\n[   0.000    0.000    1.000]\n<BLANKLINE>\n\"\"\"\n", "func_signal": "def concatvert(m1,m2):\n", "code": "r1, c1 = m1.size\nr2, c2 = m2.size\nif  c1 != c2:\n    raise TypeError('Widths don''t match, %d and %d' % (c1, c2))\n\nreturn concathoriz(m1.trans(), m2.trans()).trans()", "path": "sudoku.py", "repo_name": "benmoran/L1-Sudoku", "stars": 8, "license": "None", "language": "python", "size": 87}
{"docstring": "\"\"\"\nConvert an indicator vector to a Problem\ninstance.\n\"\"\"\n", "func_signal": "def from_indicator_vector(cls, v):\n", "code": "N3 = len(v)\nN = int(round(pow(N3, 1/3.0)))\nif N**3 != N3:\n    raise ValueError(\"Vector must be cube length but is %d\" % N3)\ndef getnum(l):\n    try:\n        return list(l).index(1)+1\n    except ValueError:\n        return None\nfor i, e in enumerate(v):\n    v[i] = int(round(e))\nentries = [getnum(v[N*e:N*(e+1)]) for e in range(0,N**2)]\nreturn Problem(entries,N)", "path": "sudoku.py", "repo_name": "benmoran/L1-Sudoku", "stars": 8, "license": "None", "language": "python", "size": 87}
{"docstring": "\"\"\"\nReturn the numbers of cells in the NxN grid\ncorresponding to non-overlapping squares of\nsize sqrt(N)xsqrt(N)\n\"\"\"\n", "func_signal": "def get_box_defs(self):\n", "code": "N = self.N\nboxsize = self.get_box_size()\nboxes = []\nfor boxnum in xrange(N):\n    bx = boxnum/boxsize\n    by = boxnum % boxsize\n    boxes.append([i+(bx*boxsize)+N*(j+by*boxsize)\n                  for i in range(boxsize)\n                  for j in range(boxsize)])\nreturn boxes", "path": "sudoku.py", "repo_name": "benmoran/L1-Sudoku", "stars": 8, "license": "None", "language": "python", "size": 87}
{"docstring": "\"\"\"\nCheck the matrix constraint that ensures\nthe answer is consistent with the clues.\n\"\"\"\n", "func_signal": "def test_clues_constraint(self):\n", "code": "N = 9\nnum = randint(1,N)\npos = randint(0,N**2-1)\nsudoku = make_problem(chr(ord('0') + num), pos)\n\nv = sudoku.get_result(clues=True)\nself.assertOnes(v)", "path": "tests.py", "repo_name": "benmoran/L1-Sudoku", "stars": 8, "license": "None", "language": "python", "size": 87}
{"docstring": "\"\"\"\nReturn the matrix that checks each cell is filled.\n\"\"\"\n", "func_signal": "def get_all_cells_matrix(self):\n", "code": "M = matrix(0,(self.N**2,self.N**3), 'd')\nfor i in xrange(self.N**2):\n    M[i, self.N*i:self.N*(i+1)] = 1\nreturn M", "path": "sudoku.py", "repo_name": "benmoran/L1-Sudoku", "stars": 8, "license": "None", "language": "python", "size": 87}
{"docstring": "\"\"\"\nReturn the matrix that checks each column contains\nall digits.\n\"\"\"\n", "func_signal": "def get_col_digits_matrix(self):\n", "code": "N = self.N\nM = matrix(0, (N**2,N**3), 'd')\n\n# I 0 0 I 0 0 I 0 0\n# 0 I 0 0 I 0 0 I 0\n# 0 0 I 0 0 I 0 0 I\n\nreturn reduce(concathoriz, [eye(N**2)] * N)", "path": "sudoku.py", "repo_name": "benmoran/L1-Sudoku", "stars": 8, "license": "None", "language": "python", "size": 87}
{"docstring": "\"\"\"\nTest the problem matrix generated by the Problem\n\"\"\"\n", "func_signal": "def test_to_indicator(self):\n", "code": "N = self.N\nnum = randint(1,N)\npos = randint(0,N**2-1)\nsudoku = make_problem(chr(ord('0') + num), pos)\n\niv = sudoku.to_indicator_vector()\nself.assertTrue(isinstance(iv ,matrix ))\nself.assertEqual((N**3,1), iv.size)\nself.assertEqual(1, sum(iv))\nself.assertEqual(N*pos+num-1, list(iv).index(1))\n\ns2 = Problem.from_indicator_vector(iv)\n\nself.assertEqual(unicode(sudoku),unicode(s2))", "path": "tests.py", "repo_name": "benmoran/L1-Sudoku", "stars": 8, "license": "None", "language": "python", "size": 87}
{"docstring": "\"\"\"\nCheck the Problem correctly counts the number\nof clue entries.        \n\"\"\"\n", "func_signal": "def test_nentries(self):\n", "code": "sudoku = Problem('1' + '.'*(self.N **2 -1), N=self.N)\nself.assertEqual(1, sudoku.num_entries())\n\nrp = random_puzzle().strip()\nnentries_rp = len([e for e in rp if e != '.'])\nsudoku = Problem(rp)\nself.assertEqual(nentries_rp, sudoku.num_entries())", "path": "tests.py", "repo_name": "benmoran/L1-Sudoku", "stars": 8, "license": "None", "language": "python", "size": 87}
{"docstring": "\"\"\"\nReturn the matrix that checks each row contains\nall digits.\n\"\"\"\n", "func_signal": "def get_row_digits_matrix(self):\n", "code": "N = self.N\nM = matrix(0, (N**2,N**3), 'd')\neyes = reduce(concathoriz, [eye(N)] * N)\n\n# I I I 0 0 0 0 0 0\n# 0 0 0 I I I 0 0 0\n# 0 0 0 0 0 0 I I I\n\nfor i in xrange(N):\n    M[ N*i: N*i+N , (N**2)*i : (N**2)*(i+1) ] = eyes\nreturn M", "path": "sudoku.py", "repo_name": "benmoran/L1-Sudoku", "stars": 8, "license": "None", "language": "python", "size": 87}
{"docstring": "# if event happens outside of axes, px and/or py may be None\n", "func_signal": "def move_crosshairs(self, px, py):\n", "code": "if px is not None: self.px = px\nif py is not None: self.py = py\nrow_data, col_data = self._crosshairs_data(self.px, self.py)\nrow_line, col_line = self.crosshairs\nrow_line.set_data(*row_data)\ncol_line.set_data(*col_data)\nself._draw_crosshairs()", "path": "xipy\\vis\\single_slice_plot.py", "repo_name": "miketrumpis/xipy", "stars": 9, "license": "None", "language": "python", "size": 14000}
{"docstring": "\"\"\"\nfunc_name : Descriptive text\n    continued text\nanother_func_name : Descriptive text\nfunc_name1, func_name2, :meth:`func_name`, func_name3\n\n\"\"\"\n", "func_signal": "def _parse_see_also(self, content):\n", "code": "items = []\n\ndef parse_item_name(text):\n    \"\"\"Match ':role:`name`' or 'name'\"\"\"\n    m = self._name_rgx.match(text)\n    if m:\n        g = m.groups()\n        if g[1] is None:\n            return g[3], None\n        else:\n            return g[2], g[1]\n    raise ValueError(\"%s is not a item name\" % text)\n\ndef push_item(name, rest):\n    if not name:\n        return\n    name, role = parse_item_name(name)\n    items.append((name, list(rest), role))\n    del rest[:]\n\ncurrent_func = None\nrest = []\n\nfor line in content:\n    if not line.strip(): continue\n\n    m = self._name_rgx.match(line)\n    if m and line[m.end():].strip().startswith(':'):\n        push_item(current_func, rest)\n        current_func, line = line[:m.end()], line[m.end():]\n        rest = [line.split(':', 1)[1].strip()]\n        if not rest[0]:\n            rest = []\n    elif not line.startswith(' '):\n        push_item(current_func, rest)\n        current_func = None\n        if ',' in line:\n            for func in line.split(','):\n                push_item(func, [])\n        elif line.strip():\n            current_func = line\n    elif current_func is not None:\n        rest.append(line.strip())\npush_item(current_func, rest)\nreturn items", "path": "doc\\sphinxext\\docscrape.py", "repo_name": "miketrumpis/xipy", "stars": 9, "license": "None", "language": "python", "size": 14000}
{"docstring": "# be lax if slice_list comes in as a non-nested list of arrays\n", "func_signal": "def set_data(self, slice_list):\n", "code": "if len(self._slice_images)==1 and type(slice_list) not in (list,tuple):\n    slice_list = [slice_list]\nfor img, data in zip(self._slice_images, slice_list):\n    img.set_data(data)", "path": "xipy\\vis\\single_slice_plot.py", "repo_name": "miketrumpis/xipy", "stars": 9, "license": "None", "language": "python", "size": 14000}
{"docstring": "\"\"\"\n.. index: default\n   :refguide: something, else, and more\n\n\"\"\"\n", "func_signal": "def _parse_index(self, section, content):\n", "code": "def strip_each_in(lst):\n    return [s.strip() for s in lst]\n\nout = {}\nsection = section.split('::')\nif len(section) > 1:\n    out['default'] = strip_each_in(section[1].split(','))[0]\nfor line in content:\n    line = line.split(':')\n    if len(line) > 2:\n        out[line[1]] = strip_each_in(line[2].split(','))\nreturn out", "path": "doc\\sphinxext\\docscrape.py", "repo_name": "miketrumpis/xipy", "stars": 9, "license": "None", "language": "python", "size": 14000}
{"docstring": "\"\"\"Convert IPython prompts to python ones in a string.\"\"\"\n", "func_signal": "def __call__(self, ds):\n", "code": "pyps1 = '>>> '\npyps2 = '... '\npyout = ''\n\ndnew = ds\ndnew = self.ps1.sub('>>> ', dnew)\ndnew = self.ps2.sub('... ', dnew)\ndnew = self.out.sub('', dnew)\nreturn dnew", "path": "xipy\\external\\decotest.py", "repo_name": "miketrumpis/xipy", "stars": 9, "license": "None", "language": "python", "size": 14000}
{"docstring": "\"\"\"\nParameters\n----------\nimage : Image\n   Image to be interpolated\norder : int\n   order of spline interpolation as used in scipy.ndimage\n\"\"\"\n", "func_signal": "def __init__(self, image, order=3, use_mmap=False):\n", "code": "self.image = image\nself.order = order\nself._datafile = None\nself._buildknots(use_mmap)", "path": "xipy\\external\\interpolation.py", "repo_name": "miketrumpis/xipy", "stars": 9, "license": "None", "language": "python", "size": 14000}
{"docstring": "\"\"\"Grab signature (if given) and summary\"\"\"\n", "func_signal": "def _parse_summary(self):\n", "code": "if self._is_at_section():\n    return\n\nsummary = self._doc.read_to_next_empty_line()\nsummary_str = \" \".join([s.strip() for s in summary]).strip()\nif re.compile('^([\\w., ]+=)?\\s*[\\w\\.]+\\(.*\\)$').match(summary_str):\n    self['Signature'] = summary_str\n    if not self._is_at_section():\n        self['Summary'] = self._doc.read_to_next_empty_line()\nelse:\n    self['Summary'] = summary\n\nif not self._is_at_section():\n    self['Extended Summary'] = self._read_to_next_section()", "path": "doc\\sphinxext\\docscrape.py", "repo_name": "miketrumpis/xipy", "stars": 9, "license": "None", "language": "python", "size": 14000}
{"docstring": "\"\"\"Decorator to make a simple function into a normal test via unittest.\"\"\"\n", "func_signal": "def as_unittest(func):\n", "code": "class Tester(unittest.TestCase):\n    def test(self):\n        func()\n\nTester.__name__ = func.func_name\n\nreturn Tester", "path": "xipy\\external\\decotest.py", "repo_name": "miketrumpis/xipy", "stars": 9, "license": "None", "language": "python", "size": 14000}
{"docstring": "\"\"\"Use as a decorator: doctest a function's docstring as a unittest.\n\nThis version runs normal doctests, but the idea is to make it later run\nipython syntax instead.\"\"\"\n\n# Capture the enclosing instance with a different name, so the new\n# class below can see it without confusion regarding its own 'self'\n# that will point to the test instance at runtime\n", "func_signal": "def __call__(self, func):\n", "code": "d2u = self\n\n# Rewrite the function's docstring to have python syntax\nif func.__doc__ is not None:\n    func.__doc__ = ip2py(func.__doc__)\n\n# Now, create a tester object that is a real unittest instance, so\n# normal unittest machinery (or Nose, or Trial) can find it.\nclass Tester(unittest.TestCase):\n    def test(self):\n        # Make a new runner per function to be tested\n        runner = DocTestRunner(verbose=d2u.verbose)\n        map(runner.run, d2u.finder.find(func, func.func_name))\n        failed = count_failures(runner)\n        if failed:\n            # Since we only looked at a single function's docstring,\n            # failed should contain at most one item.  More than that\n            # is a case we can't handle and should error out on\n            if len(failed) > 1:\n                err = \"Invalid number of test results:\" % failed\n                raise ValueError(err)\n            # Report a normal failure.\n            self.fail('failed doctests: %s' % str(failed[0]))\n            \n# Rename it so test reports have the original signature.\nTester.__name__ = func.func_name\nreturn Tester", "path": "xipy\\external\\decotest.py", "repo_name": "miketrumpis/xipy", "stars": 9, "license": "None", "language": "python", "size": 14000}
{"docstring": "\"\"\"New decorator.\n\nParameters\n----------\n\nverbose : boolean, optional (False)\n  Passed to the doctest finder and runner to control verbosity.\n\"\"\"\n", "func_signal": "def __init__(self, verbose=False):\n", "code": "self.verbose = verbose\n# We can reuse the same finder for all instances\nself.finder = DocTestFinder(verbose=verbose, recurse=False)", "path": "xipy\\external\\decotest.py", "repo_name": "miketrumpis/xipy", "stars": 9, "license": "None", "language": "python", "size": 14000}
{"docstring": "# But if we have a test generator, we iterate it ourselves\n", "func_signal": "def run_parametric(self, result, testMethod):\n", "code": "testgen = testMethod()\nwhile True:\n    try:\n        # Initialize test\n        result.startTest(self)\n\n        # SetUp\n        try:\n            self.setUp()\n        except KeyboardInterrupt:\n            raise\n        except:\n            result.addError(self, self._exc_info())\n            return\n        # Test execution\n        ok = False\n        try:\n            testgen.next()\n            ok = True\n        except StopIteration:\n            # We stop the loop\n            break\n        except self.failureException:\n            result.addFailure(self, self._exc_info())\n        except KeyboardInterrupt:\n            raise\n        except:\n            result.addError(self, self._exc_info())\n        # TearDown\n        try:\n            self.tearDown()\n        except KeyboardInterrupt:\n            raise\n        except:\n            result.addError(self, self._exc_info())\n            ok = False\n        if ok: result.addSuccess(self)\n        \n    finally:\n        result.stopTest(self)", "path": "xipy\\external\\decotest.py", "repo_name": "miketrumpis/xipy", "stars": 9, "license": "None", "language": "python", "size": 14000}
{"docstring": "# Set main_rgba into scalar_data.\n\n# changes\n# 1) from size 1 to size 1\n# 2) from size 1 to size 2\n# 3) from size 1 to 0 (with over_rgba)\n# 4) from size 1 to 0 (without over_rgba)\n# 5) from 0 to size 1\n\n\n# cases 1, 2, 5\n", "func_signal": "def _set_main_array(self):\n", "code": "if self.blender.main_rgba.size:\n    self._change_primary_scalars(self.blender.main_rgba,\n                                 self.main_channel)\n\nelif not self.blender.over_rgba.size:\n    #self.flush_arrays()\n    self.safe_remove_arrays()\n# cases 3, 4 will be triggered if and when over_rgba changes", "path": "xipy\\colors\\mayavi_tools.py", "repo_name": "miketrumpis/xipy", "stars": 9, "license": "None", "language": "python", "size": 14000}
{"docstring": "\"\"\"\nSets up a new point data channel in this object's ImageData\n\nParameters\n----------\narr : 4-component ndarray with dtype = uint8\n  The `arr` parameter must be shaped (npts x 4), or shaped\n  (nz, ny, nx, 4) in C-order (like BlendedImage RGBA arrays when\n  vtk_order is True)\n\nname : str\n  name of the array\n\n\"\"\"\n", "func_signal": "def set_new_array(self, arr, name, update=True):\n", "code": "pdata = self.data.point_data\nif len(arr.shape) > 2:\n    if len(arr.shape) > 3:\n        flat_arr = arr.reshape(np.prod(arr.shape[:3]), 4)\n    else:\n        flat_arr = arr.ravel()\nelse:\n    flat_arr = arr\nchan = pdata.get_array(name)\nif chan:\n    chan.from_array(flat_arr)\nelse:\n    n = pdata.add_array(flat_arr)\n    pdata.get_array(n).name = name\nif update:\n    self._push_changes()", "path": "xipy\\colors\\mayavi_tools.py", "repo_name": "miketrumpis/xipy", "stars": 9, "license": "None", "language": "python", "size": 14000}
{"docstring": "##     warnings.filterwarnings('ignore',\n##                             message='Warning: divide by zero encountered in divide')\n", "func_signal": "def intersecting_tracks(tracks, p0, normal):\n", "code": "    p0 = np.asarray(p0)\n    normal = np.asarray(normal)\n    warnings.simplefilter('ignore')\n    p0.shape = (1,3)    \n    i_tracks = []\n\n    if np.dot(normal,normal) == 1 and (normal==1).any():\n        col = np.argwhere(normal==1)[0][0]\n        slicer = (slice(None), slice(col, col+1))\n        dot_func = lambda x: x[slicer]\n    else:\n        dot_func = lambda x: np.dot(x,normal)\n    \n    for n, line in enumerate(tracks):\n        d0 = p0 - line[:-1]\n        d1 = line[1:] - line[:-1]\n        num = dot_func(d0)\n        den = dot_func(d1)\n        u = num/den\n        if ( (u<=1) & (u>=0) ).any():\n            i_tracks.append(n)\n        \n    p0.shape = (3,)\n    warnings.resetwarnings()\n    return i_tracks", "path": "scratch\\plane_intersection.py", "repo_name": "miketrumpis/xipy", "stars": 9, "license": "None", "language": "python", "size": 14000}
{"docstring": "\"\"\"Change the function docstring via ip2py.\n\"\"\"\n", "func_signal": "def ipdocstring(func):\n", "code": "if func.__doc__ is not None:\n    func.__doc__ = ip2py(func.__doc__)\nreturn func", "path": "xipy\\external\\decotest.py", "repo_name": "miketrumpis/xipy", "stars": 9, "license": "None", "language": "python", "size": 14000}
{"docstring": "\"\"\"Decorator to make a simple function into a normal test via unittest.\"\"\"\n", "func_signal": "def parametric(func):\n", "code": "class Tester(ParametricTestCase):\n    test = staticmethod(func)\n\nTester.__name__ = func.func_name\n\nreturn Tester", "path": "xipy\\external\\decotest.py", "repo_name": "miketrumpis/xipy", "stars": 9, "license": "None", "language": "python", "size": 14000}
{"docstring": "\"\"\"Call this function when you change the array data\nin-place.\"\"\"\n", "func_signal": "def update(self):\n", "code": "d = self.image_data\nd.modified()\npd = d.point_data\nif self.scalar_data is not None:\n    pd.scalars.modified()\nif self.vector_data is not None:\n    pd.vectors.modified()", "path": "xipy\\colors\\mayavi_tools.py", "repo_name": "miketrumpis/xipy", "stars": 9, "license": "None", "language": "python", "size": 14000}
{"docstring": "# this should be called when..\n# * arrays are added/removed\n", "func_signal": "def _push_changes(self):\n", "code": "self._update_data()\nself.pipeline_changed = True\nself.colors_changed = True", "path": "xipy\\colors\\mayavi_tools.py", "repo_name": "miketrumpis/xipy", "stars": 9, "license": "None", "language": "python", "size": 14000}
{"docstring": "\"\"\"Override to select with selector, unless\nconfig.getTestCaseNamesCompat is True\n\"\"\"\n", "func_signal": "def getTestCaseNames(self, testCaseClass):\n", "code": "if self.config.getTestCaseNamesCompat:\n    return unittest.TestLoader.getTestCaseNames(self, testCaseClass)\n\ndef wanted(attr, cls=testCaseClass, sel=self.selector):\n    item = getattr(cls, attr, None)\n    # MONKEYPATCH: replace this:\n    #if not ismethod(item):\n    # With:\n    if not hasattr(item, '__call__'):\n    # END MONKEYPATCH\n        return False\n    return sel.wantMethod(item)\ncases = filter(wanted, dir(testCaseClass))\nfor base in testCaseClass.__bases__:\n    for case in self.getTestCaseNames(base):\n        if case not in cases:\n            cases.append(case)\n# add runTest if nothing else picked\nif not cases and hasattr(testCaseClass, 'runTest'):\n    cases = ['runTest']\nif self.sortTestMethodsUsing:\n    cases.sort(self.sortTestMethodsUsing)\nreturn cases", "path": "xipy\\external\\decotest.py", "repo_name": "miketrumpis/xipy", "stars": 9, "license": "None", "language": "python", "size": 14000}
{"docstring": "\"\"\"\nParameters\n----------\npoints : ndarray, shape ( npts x nD )\n    values in self.image.coordmap.function_range\n\nReturns\n-------\nV: ndarray\n   interpolator of self.image evaluated at points\n\"\"\"\n##         points = np.array(points, np.float64)\n##         output_shape = points.shape[:-1]\n##         points.shape = (np.product(output_shape), points.shape[-1])\n", "func_signal": "def evaluate(self, points, **interp_kws):\n", "code": "voxels = self.image.coordmap.inverse()(points).T\nV = ndimage.map_coordinates(self.data,\n                            voxels,\n                            order=self.order,\n                            prefilter=False,\n                            output=self.data.dtype,\n                            **interp_kws)\n# ndimage.map_coordinates returns a flat array,\n# it needs to be reshaped to the original shape", "path": "xipy\\external\\interpolation.py", "repo_name": "miketrumpis/xipy", "stars": 9, "license": "None", "language": "python", "size": 14000}
{"docstring": "\"\"\"Changes a MS smart quote character to an XML or HTML\nentity.\"\"\"\n", "func_signal": "def _subMSChar(self, orig):\n", "code": "sub = self.MS_CHARS.get(orig)\nif type(sub) == types.TupleType:\n    if self.smartQuotesTo == 'xml':\n        sub = '&#x%s;' % sub[1]\n    else:\n        sub = '&%s;' % sub[0]\nreturn sub", "path": "vendor\\beautifulsoup\\BeautifulSoup.py", "repo_name": "lemonad/jaikuengine", "stars": 11, "license": "apache-2.0", "language": "python", "size": 1724}
{"docstring": "\"\"\" expects to get a channel in one of the following forms:\n\npopular\n#popular\n#popular@example.com\n\"\"\"\n", "func_signal": "def channel(value, message='Invalid channel name'):\n", "code": "value = encoding.smart_unicode(value)\nif not value.startswith('#'):\n  value = '#%s' % value\n \nif not value.endswith('@%s' % settings.NS_DOMAIN):\n  value = '%s@%s' % (value, settings.NS_DOMAIN)\n\nmatch = channel_re.match(value)\nif not match:\n  raise exception.ValidationError(message)\n\nreturn value", "path": "common\\clean.py", "repo_name": "lemonad/jaikuengine", "stars": 11, "license": "apache-2.0", "language": "python", "size": 1724}
{"docstring": "#print \"Push\", tag.name\n", "func_signal": "def pushTag(self, tag):\n", "code": "if self.currentTag:\n    self.currentTag.contents.append(tag)\nself.tagStack.append(tag)\nself.currentTag = self.tagStack[-1]", "path": "vendor\\beautifulsoup\\BeautifulSoup.py", "repo_name": "lemonad/jaikuengine", "stars": 11, "license": "apache-2.0", "language": "python", "size": 1724}
{"docstring": "\"\"\"Returns the parents of this Tag that match the given\ncriteria.\"\"\"\n\n", "func_signal": "def findParents(self, name=None, attrs={}, limit=None, **kwargs):\n", "code": "return self._findAll(name, attrs, None, limit, self.parentGenerator,\n                     **kwargs)", "path": "vendor\\beautifulsoup\\BeautifulSoup.py", "repo_name": "lemonad/jaikuengine", "stars": 11, "license": "apache-2.0", "language": "python", "size": 1724}
{"docstring": "\"\"\" expects to get a nick in one of the following forms:\n\npopular\npopular@example.com\n\"\"\"\n", "func_signal": "def user(value, message='Invalid nick'):\n", "code": "value = encoding.smart_unicode(value)\nif not value.endswith('@%s' % settings.NS_DOMAIN):\n  value = '%s@%s' % (value, settings.NS_DOMAIN)\n\nmatch = USER_COMPILED.match(value)\nif not match:\n  raise exception.ValidationError(message)\n\nreturn value", "path": "common\\clean.py", "repo_name": "lemonad/jaikuengine", "stars": 11, "license": "apache-2.0", "language": "python", "size": 1724}
{"docstring": "\"\"\"Convert value to one of: ('no-repeat', '').\"\"\"\n", "func_signal": "def bg_repeat(value):\n", "code": "if value == 'no-repeat':\n  return value\nreturn ''", "path": "common\\clean.py", "repo_name": "lemonad/jaikuengine", "stars": 11, "license": "apache-2.0", "language": "python", "size": 1724}
{"docstring": "\"\"\" expects to get a nick in one of the following forms:\n\npopular\n#popular\npopular@example.com\n#popular@example.com\n\"\"\"\n", "func_signal": "def nick(value, message='Invalid nick'):\n", "code": "value = encoding.smart_unicode(value)\ntry:\n  return user(value, message=message)\nexcept exception.ValidationError:\n  return channel(value, message=message)", "path": "common\\clean.py", "repo_name": "lemonad/jaikuengine", "stars": 11, "license": "apache-2.0", "language": "python", "size": 1724}
{"docstring": "\"\"\"This method fixes a bug in Python's SGMLParser.\"\"\"\n", "func_signal": "def convert_charref(self, name):\n", "code": "try:\n    n = int(name)\nexcept ValueError:\n    return\nif not 0 <= n <= 127 : # ASCII ends at 127, not 255\n    return\nreturn self.convert_codepoint(n)", "path": "vendor\\beautifulsoup\\BeautifulSoup.py", "repo_name": "lemonad/jaikuengine", "stars": 11, "license": "apache-2.0", "language": "python", "size": 1724}
{"docstring": "\"\"\"Returns the closest sibling to this Tag that matches the\ngiven criteria and appears before this Tag in the document.\"\"\"\n", "func_signal": "def findPreviousSibling(self, name=None, attrs={}, text=None, **kwargs):\n", "code": "return self._findOne(self.findPreviousSiblings, name, attrs, text,\n                     **kwargs)", "path": "vendor\\beautifulsoup\\BeautifulSoup.py", "repo_name": "lemonad/jaikuengine", "stars": 11, "license": "apache-2.0", "language": "python", "size": 1724}
{"docstring": "\"\"\"Convenience method that works with all 2.x versions of Python\nto determine whether or not something is listlike.\"\"\"\n", "func_signal": "def isList(l):\n", "code": "return hasattr(l, '__iter__') \\\n       or (type(l) in (types.ListType, types.TupleType))", "path": "vendor\\beautifulsoup\\BeautifulSoup.py", "repo_name": "lemonad/jaikuengine", "stars": 11, "license": "apache-2.0", "language": "python", "size": 1724}
{"docstring": "# we only want to do this once, but due to weird method\n# caching behavior it sometimes gets blown away\n", "func_signal": "def main():\n", "code": "_add_zip_files_to_path()\n\n# Create a Django application for WSGI.\napplication = django.core.handlers.wsgi.WSGIHandler()\n\n# Run the WSGI CGI handler with that application.\nutil.run_wsgi_app(application)", "path": "main.py", "repo_name": "lemonad/jaikuengine", "stars": 11, "license": "apache-2.0", "language": "python", "size": 1724}
{"docstring": "\"\"\" test that api.post creates a task and additional calls resume\n\"\"\"\n", "func_signal": "def test_task_post(self):\n", "code": "nick = 'popular@example.com'\nuuid = 'HOWNOW'\nmessage = 'BROWNCOW'\n\nactor_ref = api.actor_get(api.ROOT, nick)\n\n# DROP\nold_max = api.MAX_FOLLOWERS_PER_INBOX\napi.MAX_FOLLOWERS_PER_INBOX = 1\n\ntry:\n  entry_ref = api.post(actor_ref, nick=nick, uuid=uuid, message=message)\n  self.assertEqual(entry_ref.extra['title'], message)\n\n  # make sure we can repeat\n  two_entry_ref = api.post(actor_ref, nick=nick, uuid=uuid, message=message)\n  self.assertEqual(entry_ref.uuid, two_entry_ref.uuid)\n\n  # and that task_process_actor works\n  # and run out the queue\n  for i in range(5):\n    api.task_process_actor(api.ROOT, nick)\n\n  self.assertRaises(exception.ApiNoTasks,\n                    lambda: api.task_process_actor(api.ROOT, nick))\nfinally:\n  api.MAX_FOLLOWERS_PER_INBOX = old_max", "path": "common\\test\\queue.py", "repo_name": "lemonad/jaikuengine", "stars": 11, "license": "apache-2.0", "language": "python", "size": 1724}
{"docstring": "\"\"\"Initializes a map representation of this tag's attributes,\nif not already initialized.\"\"\"\n", "func_signal": "def _getAttrMap(self):\n", "code": "if not getattr(self, 'attrMap'):\n    self.attrMap = {}\n    for (key, value) in self.attrs:\n        self.attrMap[key] = value\nreturn self.attrMap", "path": "vendor\\beautifulsoup\\BeautifulSoup.py", "repo_name": "lemonad/jaikuengine", "stars": 11, "license": "apache-2.0", "language": "python", "size": 1724}
{"docstring": "\"\"\" ensure that the value is in ICONS and turned into an int, 0 if non int\"\"\"\n", "func_signal": "def icon(value):\n", "code": "if str(value) in display.ICONS_BY_ID:\n  return str(value)\nreturn display.ICONS.get(str(value), (0,))[0]", "path": "common\\clean.py", "repo_name": "lemonad/jaikuengine", "stars": 11, "license": "apache-2.0", "language": "python", "size": 1724}
{"docstring": "# Convert the document to Unicode.\n", "func_signal": "def _feed(self, inDocumentEncoding=None, isHTML=False):\n", "code": "markup = self.markup\nif isinstance(markup, unicode):\n    if not hasattr(self, 'originalEncoding'):\n        self.originalEncoding = None\nelse:\n    dammit = UnicodeDammit\\\n             (markup, [self.fromEncoding, inDocumentEncoding],\n              smartQuotesTo=self.smartQuotesTo, isHTML=isHTML)\n    markup = dammit.unicode\n    self.originalEncoding = dammit.originalEncoding\n    self.declaredHTMLEncoding = dammit.declaredHTMLEncoding\nif markup:\n    if self.markupMassage:\n        if not isList(self.markupMassage):\n            self.markupMassage = self.MARKUP_MASSAGE\n        for fix, m in self.markupMassage:\n            markup = fix.sub(m, markup)\n        # TODO: We get rid of markupMassage so that the\n        # soup object can be deepcopied later on. Some\n        # Python installations can't copy regexes. If anyone\n        # was relying on the existence of markupMassage, this\n        # might cause problems.\n        del(self.markupMassage)\nself.reset()\n\nSGMLParser.feed(self, markup)\n# Close out any unfinished strings and close all the open tags.\nself.endData()\nwhile self.currentTag.name != self.ROOT_TAG_NAME:\n    self.popTag()", "path": "vendor\\beautifulsoup\\BeautifulSoup.py", "repo_name": "lemonad/jaikuengine", "stars": 11, "license": "apache-2.0", "language": "python", "size": 1724}
{"docstring": "\"\"\"Returns true iff the given string is the name of a\nself-closing tag according to this parser.\"\"\"\n", "func_signal": "def isSelfClosingTag(self, name):\n", "code": "return self.SELF_CLOSING_TAGS.has_key(name) \\\n       or self.instanceSelfClosingTags.has_key(name)", "path": "vendor\\beautifulsoup\\BeautifulSoup.py", "repo_name": "lemonad/jaikuengine", "stars": 11, "license": "apache-2.0", "language": "python", "size": 1724}
{"docstring": "\"\"\"Recursively destroys the contents of this tree.\"\"\"\n", "func_signal": "def decompose(self):\n", "code": "contents = [i for i in self.contents]\nfor i in contents:\n    if isinstance(i, Tag):\n        i.decompose()\n    else:\n        i.extract()\nself.extract()", "path": "vendor\\beautifulsoup\\BeautifulSoup.py", "repo_name": "lemonad/jaikuengine", "stars": 11, "license": "apache-2.0", "language": "python", "size": 1724}
{"docstring": "\"\"\"Encodes an object to a string in some encoding, or to Unicode.\n.\"\"\"\n", "func_signal": "def toEncoding(self, s, encoding=None):\n", "code": "if isinstance(s, unicode):\n    if encoding:\n        s = s.encode(encoding)\nelif isinstance(s, str):\n    if encoding:\n        s = s.encode(encoding)\n    else:\n        s = unicode(s)\nelse:\n    if encoding:\n        s  = self.toEncoding(str(s), encoding)\n    else:\n        s = unicode(s)\nreturn s", "path": "vendor\\beautifulsoup\\BeautifulSoup.py", "repo_name": "lemonad/jaikuengine", "stars": 11, "license": "apache-2.0", "language": "python", "size": 1724}
{"docstring": "\"\"\"Beautiful Soup can detect a charset included in a META tag,\ntry to convert the document to that charset, and re-parse the\ndocument from the beginning.\"\"\"\n", "func_signal": "def start_meta(self, attrs):\n", "code": "httpEquiv = None\ncontentType = None\ncontentTypeIndex = None\ntagNeedsEncodingSubstitution = False\n\nfor i in range(0, len(attrs)):\n    key, value = attrs[i]\n    key = key.lower()\n    if key == 'http-equiv':\n        httpEquiv = value\n    elif key == 'content':\n        contentType = value\n        contentTypeIndex = i\n\nif httpEquiv and contentType: # It's an interesting meta tag.\n    match = self.CHARSET_RE.search(contentType)\n    if match:\n        if (self.declaredHTMLEncoding is not None or\n            self.originalEncoding == self.fromEncoding):\n            # An HTML encoding was sniffed while converting\n            # the document to Unicode, or an HTML encoding was\n            # sniffed during a previous pass through the\n            # document, or an encoding was specified\n            # explicitly and it worked. Rewrite the meta tag.\n            def rewrite(match):\n                return match.group(1) + \"%SOUP-ENCODING%\"\n            newAttr = self.CHARSET_RE.sub(rewrite, contentType)\n            attrs[contentTypeIndex] = (attrs[contentTypeIndex][0],\n                                       newAttr)\n            tagNeedsEncodingSubstitution = True\n        else:\n            # This is our first pass through the document.\n            # Go through it again with the encoding information.\n            newCharset = match.group(3)\n            if newCharset and newCharset != self.originalEncoding:\n                self.declaredHTMLEncoding = newCharset\n                self._feed(self.declaredHTMLEncoding)\n                raise StopParsing\n            pass\ntag = self.unknown_starttag(\"meta\", attrs)\nif tag and tagNeedsEncodingSubstitution:\n    tag.containsSubstitutions = True", "path": "vendor\\beautifulsoup\\BeautifulSoup.py", "repo_name": "lemonad/jaikuengine", "stars": 11, "license": "apache-2.0", "language": "python", "size": 1724}
{"docstring": "\"\"\"function to append the applicable referenced items to the entry\"\"\"\n", "func_signal": "def prep_entry(entry, streams, actors):\n", "code": "entry.stream_ref = streams[entry.stream]\nentry.owner_ref = actors[entry.owner]\nentry.actor_ref = actors[entry.actor]\nreturn entry", "path": "common\\display.py", "repo_name": "lemonad/jaikuengine", "stars": 11, "license": "apache-2.0", "language": "python", "size": 1724}
{"docstring": "\"\"\"\nSet the chart's padding.\n\n@param padding: the padding in px\n@type padding: int in [0, 100] (default: 16).\n\"\"\"\n", "func_signal": "def set_padding(self, padding):\n", "code": "self.set_property(\"padding\", padding)\nself.queue_draw()", "path": "src\\pygtk_chart\\chart.py", "repo_name": "notmyname/pygtkChart", "stars": 12, "license": "None", "language": "python", "size": 4223}
{"docstring": "\"\"\"\nDraw basic things that every plot has (background, title, ...).\n\n@type context: cairo.Context\n@param context: The context to draw on.\n@type rect: gtk.gdk.Rectangle\n@param rect: A rectangle representing the charts area.\n\"\"\"\n", "func_signal": "def draw_basics(self, context, rect):\n", "code": "self.background.draw(context, rect)\nself.title.draw(context, rect)\n\n#calculate the rectangle that's available for drawing the chart\ntitle_height = self.title.get_real_dimensions()[1]\nrect_height = int(rect.height - 3 * self._padding - title_height)\nrect_width = int(rect.width - 2 * self._padding)\nrect_x = int(rect.x + self._padding)\nrect_y = int(rect.y + title_height + 2 * self._padding)\nreturn gtk.gdk.Rectangle(rect_x, rect_y, rect_width, rect_height)", "path": "src\\pygtk_chart\\chart.py", "repo_name": "notmyname/pygtkChart", "stars": 12, "license": "None", "language": "python", "size": 4223}
{"docstring": "\"\"\"\nDo all the drawing stuff.\n\n@type context: cairo.Context\n@param context: The context to draw on.\n@type rect: gtk.gdk.Rectangle\n@param rect: A rectangle representing the charts area.\n\"\"\"\n", "func_signal": "def _do_draw(self, context, rect):\n", "code": "if self._color != None:\n    #set source color\n    context.set_source_rgb(*color_gdk_to_cairo(self._color))\nelif self._gradient != None:\n    #set source gradient\n    cs = color_gdk_to_cairo(self._gradient[0])\n    ce = color_gdk_to_cairo(self._gradient[1])\n    gradient = cairo.LinearGradient(0, 0, 0, rect.height)\n    gradient.add_color_stop_rgb(0, cs[0], cs[1], cs[2])\n    gradient.add_color_stop_rgb(1, ce[0], ce[1], ce[2])\n    context.set_source(gradient)\nelif self._pixbuf:\n    context.set_source_pixbuf(self._pixbuf, 0, 0)\nelse:\n    context.set_source_rgb(1, 1, 1) #fallback to white bg\n#create the background rectangle and fill it:\ncontext.rectangle(0, 0, rect.width, rect.height)\ncontext.fill()", "path": "src\\pygtk_chart\\chart.py", "repo_name": "notmyname/pygtkChart", "stars": 12, "license": "None", "language": "python", "size": 4223}
{"docstring": "\"\"\"\nDraw the widget. This method is called automatically. Don't call it\nyourself. If you want to force a redrawing of the widget, call\nthe queue_draw() method.\n\n@type context: cairo.Context\n@param context: The context to draw on.\n\"\"\"\n", "func_signal": "def draw(self, context):\n", "code": "label.begin_drawing()\n\nrect = self.get_allocation()\nrect = gtk.gdk.Rectangle(0, 0, rect.width, rect.height) #transform rect to context coordinates\ncontext.set_line_width(1)\n                            \nrect = self.draw_basics(context, rect)\nmaximum_value = max(bar.get_value() for bar in self._bars)\n#find out the size of the value labels\nvalue_label_size = 0\nif self._draw_labels:\n    for bar in self._bars:\n        value_label_size = max(value_label_size, bar.get_value_label_size(context, rect, self._mode, len(self._bars), self._bar_padding))\n    value_label_size += 3\n    \n#find out the size of the labels:\nlabel_size = 0\nif self._draw_labels:\n    for bar in self._bars:\n        label_size = max(label_size, bar.get_label_size(context, rect, self._mode, len(self._bars), self._bar_padding))\n    label_size += 3\n\nrect = self._do_draw_grid(context, rect, maximum_value, value_label_size, label_size)\nself._do_draw_bars(context, rect, maximum_value, value_label_size, label_size)\n\nlabel.finish_drawing()\n\nif self._mode == MODE_VERTICAL:\n    n = len(self._bars)\n    minimum_width = rect.x + self._padding + (n - 1) * self._bar_padding + n * 10\n    minimum_height = 100 + self._padding + rect.y\nelif self._mode == MODE_HORIZONTAL:\n    n = len(self._bars)\n    minimum_width = rect.x + self._bar_padding + 100\n    minimum_height = rect.y + self._padding + (n - 1) * self._bar_padding + n * 10\nself.set_size_request(minimum_width, minimum_height)", "path": "src\\pygtk_chart\\bar_chart.py", "repo_name": "notmyname/pygtkChart", "stars": 12, "license": "None", "language": "python", "size": 4223}
{"docstring": "\"\"\"\nSet the style of the grid lines. style has to be one of\n - pygtk_chart.LINE_STYLE_SOLID (default)\n - pygtk_chart.LINE_STYLE_DOTTED\n - pygtk_chart.LINE_STYLE_DASHED\n - pygtk_chart.LINE_STYLE_DASHED_ASYMMETRIC\n\n@param style: the new line style\n@type style: one of the constants above.\n\"\"\"\n", "func_signal": "def set_line_style(self, style):\n", "code": "self.set_property(\"line-style\", style)\nself.emit(\"appearance_changed\")", "path": "src\\pygtk_chart\\bar_chart.py", "repo_name": "notmyname/pygtkChart", "stars": 12, "license": "None", "language": "python", "size": 4223}
{"docstring": "\"\"\"\nDraw the widget. This method is called automatically. Don't call it\nyourself. If you want to force a redrawing of the widget, call\nthe queue_draw() method.\n\n@type context: cairo.Context\n@param context: The context to draw on.\n\"\"\"\n", "func_signal": "def draw(self, context):\n", "code": "rect = self.get_allocation()\nrect = gtk.gdk.Rectangle(0, 0, rect.width, rect.height) #transform rect to context coordinates\ncontext.set_line_width(1)\nrect = self.draw_basics(context, rect)", "path": "src\\pygtk_chart\\chart.py", "repo_name": "notmyname/pygtkChart", "stars": 12, "license": "None", "language": "python", "size": 4223}
{"docstring": "\"\"\"\nDraws a rectangle with rounded corners to context. radius specifies\nthe corner radius in px.\n\n@param context: the context to draw on\n@type context: CairoContext\n@param x: x coordinate of the upper left corner\n@type x: float\n@param y: y coordinate of the upper left corner\n@type y: float\n@param width: width of the rectangle in px\n@type width: float\n@param height: height of the rectangle in px\n@type height: float\n@param radius: corner radius in px (default: 0)\n@type radius: float.\n\"\"\"\n", "func_signal": "def draw_rounded_rectangle(context, x, y, width, height, radius=0):\n", "code": "if radius == 0:\n    context.rectangle(x, y, width, height)\nelse:\n    context.move_to(x, y + radius)\n    context.arc(x + radius, y + radius, radius, math.pi, 1.5 * math.pi)\n    context.rel_line_to(width - 2 * radius, 0)\n    context.arc(x + width - radius, y + radius, radius, 1.5 * math.pi, 2 * math.pi)\n    context.rel_line_to(0, height - 2 * radius)\n    context.arc(x + width - radius, y + height - radius, radius, 0, 0.5 * math.pi)\n    context.rel_line_to(-(width - 2 * radius), 0)\n    context.arc(x + radius, y + height - radius, radius, 0.5 * math.pi, math.pi)\n    context.close_path()", "path": "src\\pygtk_chart\\bar_chart.py", "repo_name": "notmyname/pygtkChart", "stars": 12, "license": "None", "language": "python", "size": 4223}
{"docstring": "\"\"\"\nSet the label for the area.\n\n@param label: the new label\n@type label: string.\n\"\"\"\n", "func_signal": "def set_label(self, label):\n", "code": "self.set_property(\"label\", label)\nself.emit(\"appearance_changed\")", "path": "src\\pygtk_chart\\chart.py", "repo_name": "notmyname/pygtkChart", "stars": 12, "license": "None", "language": "python", "size": 4223}
{"docstring": "\"\"\"\nSet whether the area should be highlighted.\n\n@type highlighted: boolean.\n\"\"\"\n", "func_signal": "def set_highlighted(self, highlighted):\n", "code": "self.set_property(\"highlighted\", highlighted)\nself.emit(\"appearance_changed\")", "path": "src\\pygtk_chart\\chart.py", "repo_name": "notmyname/pygtkChart", "stars": 12, "license": "None", "language": "python", "size": 4223}
{"docstring": "\"\"\"\nSet the grid's padding.\n\n@type padding: int in [0, 100].\n\"\"\"\n", "func_signal": "def set_padding(self, padding):\n", "code": "self.set_property(\"padding\", padding)\nself.emit(\"appearance_changed\")", "path": "src\\pygtk_chart\\bar_chart.py", "repo_name": "notmyname/pygtkChart", "stars": 12, "license": "None", "language": "python", "size": 4223}
{"docstring": "\"\"\"\nDraw basic things that every plot has (background, title, ...).\n\n@type context: cairo.Context\n@param context: The context to draw on.\n@type rect: gtk.gdk.Rectangle\n@param rect: A rectangle representing the charts area.\n\"\"\"\n", "func_signal": "def draw_basics(self, context, rect):\n", "code": "self.background.draw(context, rect)\nself.title.draw(context, rect, self._padding)\n\n#calculate the rectangle that's available for drawing the chart\ntitle_height = self.title.get_real_dimensions()[1]\nrect_height = int(rect.height - 3 * self._padding - title_height)\nrect_width = int(rect.width - 2 * self._padding)\nrect_x = int(rect.x + self._padding)\nrect_y = int(rect.y + title_height + 2 * self._padding)\nreturn gtk.gdk.Rectangle(rect_x, rect_y, rect_width, rect_height)", "path": "src\\pygtk_chart\\bar_chart.py", "repo_name": "notmyname/pygtkChart", "stars": 12, "license": "None", "language": "python", "size": 4223}
{"docstring": "\"\"\"\nThis method is called when an instance of Chart receives\nthe gtk expose_event.\n\n@type widget: gtk.Widget\n@param widget: The widget that received the event.\n@type event: gtk.Event\n@param event: The event.\n\"\"\"\n", "func_signal": "def _cb_expose_event(self, widget, event):\n", "code": "self.context = widget.window.cairo_create()\nself.context.rectangle(event.area.x, event.area.y, \\\n                        event.area.width, event.area.height)\nself.context.clip()\nself.draw(self.context)\nreturn False", "path": "src\\pygtk_chart\\chart.py", "repo_name": "notmyname/pygtkChart", "stars": 12, "license": "None", "language": "python", "size": 4223}
{"docstring": "\"\"\"\nSaves the contents of the widget to svg file. The size of the image\nwill be the size of the widget.\n\n@type filename: string\n@param filename: The path to the file where you want the chart to be saved.\n@type size: tuple\n@param size: Optional parameter to give the desired height and width of the image.\n\"\"\"\n", "func_signal": "def export_svg(self, filename, size=None):\n", "code": "if size is None:\n    rect = self.get_allocation()\n    width = rect.width\n    height = rect.height\nelse:\n    width, height = size\n    old_alloc = self.get_allocation\n    self.get_allocation = lambda: gtk.gdk.Rectangle(0, 0, width, height)\nsurface = cairo.SVGSurface(filename, width, height)\nctx = cairo.Context(surface)\ncontext = pangocairo.CairoContext(ctx)\nself.draw(context)\nsurface.finish()\nif size is not None:\n    self.get_allocation = old_alloc", "path": "src\\pygtk_chart\\chart.py", "repo_name": "notmyname/pygtkChart", "stars": 12, "license": "None", "language": "python", "size": 4223}
{"docstring": "\"\"\"\nSet the color of the grid lines.\n\n@param color: the grid lines' color\n@type color: gtk.gdk.Color.\n\"\"\"\n", "func_signal": "def set_color(self, color):\n", "code": "self.set_property(\"color\", color)\nself.emit(\"appearance_changed\")", "path": "src\\pygtk_chart\\bar_chart.py", "repo_name": "notmyname/pygtkChart", "stars": 12, "license": "None", "language": "python", "size": 4223}
{"docstring": "\"\"\"\nSet the radius of the bar's corners in px (default: 0).\n\n@param radius: radius of the corners\n@type radius: int in [0, 100].\n\"\"\"\n", "func_signal": "def set_corner_radius(self, radius):\n", "code": "self.set_property(\"corner-radius\", radius)\nself.emit(\"appearance_changed\")", "path": "src\\pygtk_chart\\bar_chart.py", "repo_name": "notmyname/pygtkChart", "stars": 12, "license": "None", "language": "python", "size": 4223}
{"docstring": "\"\"\"\nSaves the contents of the widget to png file. The size of the image\nwill be the size of the widget.\n\n@type filename: string\n@param filename: The path to the file where you want the chart to be saved.\n@type size: tuple\n@param size: Optional parameter to give the desired height and width of the image.\n\"\"\"\n", "func_signal": "def export_png(self, filename, size=None):\n", "code": "if size is None:\n    rect = self.get_allocation()\n    width = rect.width\n    height = rect.height\nelse:\n    width, height = size\n    old_alloc = self.get_allocation\n    self.get_allocation = lambda: gtk.gdk.Rectangle(0, 0, width, height)\nsurface = cairo.ImageSurface(cairo.FORMAT_ARGB32, width, height)\nctx = cairo.Context(surface)\ncontext = pangocairo.CairoContext(ctx)\nself.set_size_request(width, height)\nself.draw(context)\nsurface.write_to_png(filename)\nif size is not None:\n    self.get_allocation = old_alloc", "path": "src\\pygtk_chart\\chart.py", "repo_name": "notmyname/pygtkChart", "stars": 12, "license": "None", "language": "python", "size": 4223}
{"docstring": "\"\"\"\nThe set_color() method can be used to change the color of the\nbackground.\n\n@type color: gtk.gdk.Color\n@param color: Set the background to be filles with this color.\n\"\"\"\n", "func_signal": "def set_color(self, color):\n", "code": "self.set_property(\"color\", color)\nself.set_property(\"gradient\", None)\nself.set_property(\"image\", \"\")\nself.emit(\"appearance_changed\")", "path": "src\\pygtk_chart\\chart.py", "repo_name": "notmyname/pygtkChart", "stars": 12, "license": "None", "language": "python", "size": 4223}
{"docstring": "\"\"\"\nUse set_gradient() to define a vertical gradient as the background.\n\n@type color_start: gtk.gdk.Color\n@param color_start: The starting (top) color of the gradient.\n@type color_end: gtk.gdk.Color\n@param color_end: The ending (bottom) color of the gradient.\n\"\"\"\n", "func_signal": "def set_gradient(self, color_start, color_end):\n", "code": "self.set_property(\"color\", None)\nself.set_property(\"gradient\", (color_start, color_end))\nself.set_property(\"image\", \"\")\nself.emit(\"appearance_changed\")", "path": "src\\pygtk_chart\\chart.py", "repo_name": "notmyname/pygtkChart", "stars": 12, "license": "None", "language": "python", "size": 4223}
{"docstring": "\"\"\"\nThe set_image() method sets the background to be filled with an\nimage.\n\n@type filename: string\n@param filename: Path to the file you want to use as background\nimage. If the file does not exists, the background is set to white.\n\"\"\"\n", "func_signal": "def set_image(self, filename):\n", "code": "try:\n    self._pixbuf = gtk.gdk.pixbuf_new_from_file(filename)\nexcept:\n    self._pixbuf = None\n\nself.set_property(\"color\", None)\nself.set_property(\"gradient\", None)\nself.set_property(\"image\", filename)\nself.emit(\"appearance_changed\")", "path": "src\\pygtk_chart\\chart.py", "repo_name": "notmyname/pygtkChart", "stars": 12, "license": "None", "language": "python", "size": 4223}
{"docstring": "\"\"\"\nSet the value of the area.\n\n@type value: float.\n\"\"\"\n", "func_signal": "def set_value(self, value):\n", "code": "self.set_property(\"value\", value)\nself.emit(\"appearance_changed\")", "path": "src\\pygtk_chart\\chart.py", "repo_name": "notmyname/pygtkChart", "stars": 12, "license": "None", "language": "python", "size": 4223}
{"docstring": "\"\"\"Utility function to write a file.\"\"\"\n", "func_signal": "def write_file(self, dirname, filename, content):\n", "code": "filename = os.path.join(dirname, filename)\nFILE = open(filename, \"w\")\nFILE.write(content.encode(\"utf-8\"))\nFILE.close()", "path": "epub\\epub.py", "repo_name": "martinbudden/epub", "stars": 8, "license": "None", "language": "python", "size": 285}
{"docstring": "\"\"\"Write a file for a chapter in the publication.\"\"\"\n", "func_signal": "def write_chapter(self, section):\n", "code": "css = self.stylesheets['main']\n#if section['title'].find(\"section\")==-1:\n#<span class=\"translation\">{{ translation }}</span> <span class=\"count\">{{ count }}</span>\n#extra = extra % {'translation': self.sectionTranslation, 'count': section['count']}\ntemplate = self.env.get_template(self.templates['main'])\ns = template.render({'css': css, 'id': section['id'], 'class': section['class'], 'title': section['title'], 'text': section['text']})\nself.write_file(self.opsdir, section['file'] + self.xml_ext, s)", "path": "epub\\epub.py", "repo_name": "martinbudden/epub", "stars": 8, "license": "None", "language": "python", "size": 285}
{"docstring": "\"\"\"Write the publication's titlepage.\"\"\"\n", "func_signal": "def write_title(self, filename):\n", "code": "css = self.stylesheets['titlepage']\ntemplate = self.env.get_template(self.templates['titlepage'])\ns = template.render({'title': self.title, 'css': css, 'author': self.author, 'published': self.published, 'source': self.source})\nself.write_file(self.opsdir, filename, s)", "path": "epub\\epub.py", "repo_name": "martinbudden/epub", "stars": 8, "license": "None", "language": "python", "size": 285}
{"docstring": "\"\"\"Zip up the epub directory contents into an .epub file\"\"\"\n", "func_signal": "def zip_epub(self):\n", "code": "zip_file = zipfile.ZipFile(self.title + '.epub', 'w')\ncwd = os.getcwd()\nos.chdir(self.bookdir)\nfor root, dirs, files in os.walk('.'):\n    for filename in files:\n        if filename[0] != '.':\n            zip_file.write(os.path.join(root, filename))\nzip_file.close()\nos.chdir(cwd)", "path": "epub\\epub.py", "repo_name": "martinbudden/epub", "stars": 8, "license": "None", "language": "python", "size": 285}
{"docstring": "\"\"\"Set the common attributes for the publication.\"\"\"\n", "func_signal": "def set(self, title, author, author_as, published, source):\n", "code": "self.title = title\nself.author = author\nself.author_as = author_as\nself.published = published\nself.source = source", "path": "epub\\epub.py", "repo_name": "martinbudden/epub", "stars": 8, "license": "None", "language": "python", "size": 285}
{"docstring": "\"\"\"Add a section to the publication.\"\"\"\n", "func_signal": "def add_section(self, section):\n", "code": "section['playorder'] = str(self.playorder)\nif self.playorder == 1:\n    section['class'] = 'title'\n    section['file'] = 'titlepage'\n    section['type'] = 'cover'\n    section['id'] = 'level1-title'\nelse:\n    section['class'] = 'chapter'\n    section['file'] = 'main' + str(self.playorder - 1)\n    section['type'] = 'text'\n    section['count'] = str(self.playorder - 1)\n    if section['id'] == None:\n        section['id'] = 'level1-chapter' + str(self.playorder - 1)\nself.playorder += 1\nself.sections.append(section)", "path": "epub\\epub.py", "repo_name": "martinbudden/epub", "stars": 8, "license": "None", "language": "python", "size": 285}
{"docstring": "\"\"\"\nAdd sections to a work. Each section is from a MediaWiki page.\n\"\"\"\n\n", "func_signal": "def add_sections(pub, sections, dirname):\n", "code": "count = 1\nfor i in sections:\n    # get each section (typically a chapter) and add it to the epub object\n    # each section is a separate wiki page\n    textxml = read_file(dirname + i['wikisubpage'] + \".xml\")\n    text2 = mediawikibook.parse_mediawiki_xml(textxml)\n    text = read_file(dirname + i['wikisubpage'] + \".txt\")\n    #print \"=====================\"\n    #print text.encode(\"utf-8\")\n    #print \"---------------------\"\n    #print text2.encode(\"utf-8\")\n    #print \"=====================\"\n    #assert text == text2\n    text = mediawikibook.remove_templates(text)\n    pub.add_section({'class': \"chapter\", 'type': \"text\", 'id': \"level1-s\" + str(count),\n        'playorder': str(count + 1), 'count': str(count), 'title': i['title'],\n        'file': \"main\" + str(count), 'text': text})\n    count += 1", "path": "test\\test_wsb.py", "repo_name": "martinbudden/epub", "stars": 8, "license": "None", "language": "python", "size": 285}
{"docstring": "\"\"\"Write the publication's meta information file.\"\"\"\n", "func_signal": "def write_meta_inf(self):\n", "code": "metainfdir = os.path.join(self.bookdir, \"META-INF\")\nif not os.path.isdir(metainfdir):\n    os.makedirs(metainfdir)\ntemplate = self.env.get_template(self.templates['container'])\ns = template.render({'file': self.opffilename})\nself.write_file(metainfdir, \"container\" + self.xml_ext, s)", "path": "epub\\epub.py", "repo_name": "martinbudden/epub", "stars": 8, "license": "None", "language": "python", "size": 285}
{"docstring": "\"\"\"\nTest the parsing of wikisource book contents pages.\n\"\"\"\n\n", "func_signal": "def main():\n", "code": "test_Through_the_Looking_Glass()\ntest_Great_Expectations()\ntest_Treasure_Island()", "path": "test\\test_wsb.py", "repo_name": "martinbudden/epub", "stars": 8, "license": "None", "language": "python", "size": 285}
{"docstring": "\"\"\"Test using data based on Great Expectations\"\"\"\n", "func_signal": "def test_Great_Expectations():\n", "code": "text = read_file(\"test/data/Great Expectations.txt\")\ninfo = mediawikibook.parse_wikisource_contents(text)\nassert info['title'] == \"Great Expectations\"\nassert info['author'] == \"Charles Dickens\"\nsections = info['sections']\nassert sections[0] == {'wikisubpage': '/Chapter_I', 'title': 'Chapter I'}", "path": "test\\test_wsb.py", "repo_name": "martinbudden/epub", "stars": 8, "license": "None", "language": "python", "size": 285}
{"docstring": "\"\"\"Write the ops(Open Publication Structure) file.\"\"\"\n", "func_signal": "def write_ops(self):\n", "code": "self.opsdir = os.path.join(self.bookdir, \"OPS\")\nif not os.path.isdir(self.opsdir):\n    os.makedirs(self.opsdir)\nself.write_opf()\nself.write_ncx()\nif self.dtdfilename:\n    self.write_dtd()\nself.write_stylesheets()\n#self.writeImages()\nself.write_content()", "path": "epub\\epub.py", "repo_name": "martinbudden/epub", "stars": 8, "license": "None", "language": "python", "size": 285}
{"docstring": "\"\"\"Write the opf(Open Packaging Format) file.\"\"\"\n", "func_signal": "def write_opf(self):\n", "code": "template = self.env.get_template(\"content.opf\")\ns = template.render({'title': self.title, 'uuid': self.uuid, 'language': self.language,\n    'author': self.author, 'author_as': self.author_as, 'description': self.description,\n    'publisher': self.publisher, 'source': self.source, 'published': self.published,\n    'ncxfile': self.ncxfilename, 'rights': self.rights, 'stylesheets': self.stylesheets,\n    'sections': self.sections})\nself.write_file(self.opsdir, self.opffilename, s)", "path": "epub\\epub.py", "repo_name": "martinbudden/epub", "stars": 8, "license": "None", "language": "python", "size": 285}
{"docstring": "\"\"\"Write an empty image directory.\"\"\"\n", "func_signal": "def write_images(self):\n", "code": "imagedir = os.path.join(self.opsdir, \"images\")\nif not os.path.isdir(imagedir):\n    os.makedirs(imagedir)", "path": "epub\\epub.py", "repo_name": "martinbudden/epub", "stars": 8, "license": "None", "language": "python", "size": 285}
{"docstring": "\"\"\"Write out the epub directory.\"\"\"\n", "func_signal": "def write_epub(self):\n", "code": "self.write_mime_type()\nself.write_meta_inf()\nself.write_ops()", "path": "epub\\epub.py", "repo_name": "martinbudden/epub", "stars": 8, "license": "None", "language": "python", "size": 285}
{"docstring": "\"\"\"Read a file and return its contents.\"\"\"\n", "func_signal": "def read_file(filename):\n", "code": "FILE = open(filename, \"r\")\ntext = FILE.read()\nFILE.close()\n#text = text.decode(\"utf-8\")  # from utf-8 to unicode\ntext = text.decode(\"mac_roman\")  # from utf-8 to unicode\nreturn text", "path": "test\\test_wsb.py", "repo_name": "martinbudden/epub", "stars": 8, "license": "None", "language": "python", "size": 285}
{"docstring": "\"\"\"Iterate through the sections, writing each out appropriately.\"\"\"\n", "func_signal": "def write_content(self):\n", "code": "for i in self.sections:\n    if i['class'] == \"title\":\n        self.write_title(i['file'] + self.xml_ext)\n    else:\n        self.write_chapter(i)", "path": "epub\\epub.py", "repo_name": "martinbudden/epub", "stars": 8, "license": "None", "language": "python", "size": 285}
{"docstring": "\"\"\"Test using data based on Treasure Island\"\"\"\n", "func_signal": "def test_Treasure_Island():\n", "code": "text = read_file(\"test/data/Treasure Island.txt\")\ninfo = mediawikibook.parse_wikisource_contents(text)\nassert info['title'] == \"Treasure Island\"\nassert info['author'] == \"Robert Louis Stevenson\"\nsections = info['sections']\nassert sections[0] == {'wikisubpage': '/Chapter_1',\n                       'title': 'Chapter 1: The Old Sea-dog at the Admiral Benbow'}", "path": "test\\test_wsb.py", "repo_name": "martinbudden/epub", "stars": 8, "license": "None", "language": "python", "size": 285}
{"docstring": "\"\"\"\nWrite out the stylesheets for the publication, using the templates.\n\"\"\"\n", "func_signal": "def write_stylesheets(self):\n", "code": "cssdir = os.path.join(self.opsdir, \"css\")\nif not os.path.isdir(cssdir):\n    os.makedirs(cssdir)\nfor i in self.stylesheets:\n    template = self.env.get_template(self.stylesheets[i])\n    s = template.render()\n    self.write_file(cssdir, self.stylesheets[i], s)", "path": "epub\\epub.py", "repo_name": "martinbudden/epub", "stars": 8, "license": "None", "language": "python", "size": 285}
{"docstring": "\"\"\"Write the ncx(Navigation Center for XML) file.\"\"\"\n", "func_signal": "def write_ncx(self):\n", "code": "template = self.env.get_template(\"toc.ncx\")\ns = template.render({'uid': self.uuid, 'depth': self.depth, 'title': self.title,\n    'author': self.author_as, 'sections': self.sections})\nself.write_file(self.opsdir, self.ncxfilename, s)", "path": "epub\\epub.py", "repo_name": "martinbudden/epub", "stars": 8, "license": "None", "language": "python", "size": 285}
{"docstring": "\"\"\"Write the dtd file.\"\"\"\n", "func_signal": "def write_dtd(self):\n", "code": "template = self.env.get_template(\"play.dtd\")\ns = template.render()\nself.write_file(self.opsdir, self.dtdfilename, s)", "path": "epub\\epub.py", "repo_name": "martinbudden/epub", "stars": 8, "license": "None", "language": "python", "size": 285}
{"docstring": "\"\"\"Adds a certain piece of text to the tree as a NavigableString\nsubclass.\"\"\"\n", "func_signal": "def _toStringSubclass(self, text, subclass):\n", "code": "self.soup.endData()\nself.handle_data(text)\nself.soup.endData(subclass)", "path": "downpy\\beautifulsoup\\BeautifulSoup.py", "repo_name": "rogeriopvl/Downpy", "stars": 9, "license": "other", "language": "python", "size": 461}
{"docstring": "\"\"\"Treat a bogus SGML declaration as raw data. Treat a CDATA\ndeclaration as a CData object.\"\"\"\n", "func_signal": "def parse_declaration(self, i):\n", "code": "j = None\nif self.rawdata[i:i+9] == '<![CDATA[':\n     k = self.rawdata.find(']]>', i)\n     if k == -1:\n         k = len(self.rawdata)\n     data = self.rawdata[i+9:k]\n     j = k+3\n     self._toStringSubclass(data, CData)\nelse:\n    try:\n        j = HTMLParser.parse_declaration(self, i)\n    except HTMLParseError:\n        toHandle = self.rawdata[i:]\n        self.handle_data(toHandle)\n        j = i + len(toHandle)\nreturn j", "path": "downpy\\beautifulsoup\\BeautifulSoup.py", "repo_name": "rogeriopvl/Downpy", "stars": 9, "license": "other", "language": "python", "size": 461}
{"docstring": "#print \"Matching %s against %s\" % (markup, matchAgainst)\n", "func_signal": "def _matches(self, markup, matchAgainst):\n", "code": "result = False\nif matchAgainst == True and type(matchAgainst) == types.BooleanType:\n    result = markup != None\nelif callable(matchAgainst):\n    result = matchAgainst(markup)\nelse:\n    #Custom match methods take the tag as an argument, but all\n    #other ways of matching match the tag name as a string.\n    if isinstance(markup, Tag):\n        markup = markup.name\n    if markup is not None and not isString(markup):\n        markup = unicode(markup)\n    #Now we know that chunk is either a string, or None.\n    if hasattr(matchAgainst, 'match'):\n        # It's a regexp object.\n        result = markup and matchAgainst.search(markup)\n    elif (isList(matchAgainst)\n          and (markup is not None or not isString(matchAgainst))):\n        result = markup in matchAgainst\n    elif hasattr(matchAgainst, 'items'):\n        result = markup.has_key(matchAgainst)\n    elif matchAgainst and isString(markup):\n        if isinstance(markup, unicode):\n            matchAgainst = unicode(matchAgainst)\n        else:\n            matchAgainst = str(matchAgainst)\n\n    if not result:\n        result = matchAgainst == markup\nreturn result", "path": "downpy\\beautifulsoup\\BeautifulSoup.py", "repo_name": "rogeriopvl/Downpy", "stars": 9, "license": "other", "language": "python", "size": 461}
{"docstring": "# Convert the document to Unicode.\n", "func_signal": "def _feed(self, inDocumentEncoding=None, isHTML=False):\n", "code": "markup = self.markup\nif isinstance(markup, unicode):\n    if not hasattr(self, 'originalEncoding'):\n        self.originalEncoding = None\nelse:\n    dammit = UnicodeDammit\\\n             (markup, [self.fromEncoding, inDocumentEncoding],\n              smartQuotesTo=self.smartQuotesTo, isHTML=isHTML)\n    markup = dammit.unicode\n    self.originalEncoding = dammit.originalEncoding\n    self.declaredHTMLEncoding = dammit.declaredHTMLEncoding\nif markup:\n    if self.markupMassage:\n        if not isList(self.markupMassage):\n            self.markupMassage = self.MARKUP_MASSAGE\n        for fix, m in self.markupMassage:\n            markup = fix.sub(m, markup)\n        # TODO: We get rid of markupMassage so that the\n        # soup object can be deepcopied later on. Some\n        # Python installations can't copy regexes. If anyone\n        # was relying on the existence of markupMassage, this\n        # might cause problems.\n        del(self.markupMassage)\nself.builder.reset()\n\nself.builder.feed(markup)\n# Close out any unfinished strings and close all the open tags.\nself.endData()\nwhile self.currentTag.name != self.ROOT_TAG_NAME:\n    self.popTag()", "path": "downpy\\beautifulsoup\\BeautifulSoup.py", "repo_name": "rogeriopvl/Downpy", "stars": 9, "license": "other", "language": "python", "size": 461}
{"docstring": "\"\"\"Handle a processing instruction as a ProcessingInstruction\nobject, possibly one with a %SOUP-ENCODING% slot into which an\nencoding will be plugged later.\"\"\"\n", "func_signal": "def handle_pi(self, text):\n", "code": "if text[:3] == \"xml\":\n    text = u\"xml version='1.0' encoding='%SOUP-ENCODING%'\"\nself._toStringSubclass(text, ProcessingInstruction)", "path": "downpy\\beautifulsoup\\BeautifulSoup.py", "repo_name": "rogeriopvl/Downpy", "stars": 9, "license": "other", "language": "python", "size": 461}
{"docstring": "\"\"\"Returns true iff the given string is the name of a\nself-closing tag according to this parser.\"\"\"\n", "func_signal": "def isSelfClosingTag(self, name):\n", "code": "return self.SELF_CLOSING_TAGS.has_key(name) \\\n       or self.instanceSelfClosingTags.has_key(name)", "path": "downpy\\beautifulsoup\\BeautifulSoup.py", "repo_name": "rogeriopvl/Downpy", "stars": 9, "license": "other", "language": "python", "size": 461}
{"docstring": "\"\"\"Recursively destroys the contents of this tree.\"\"\"\n", "func_signal": "def decompose(self):\n", "code": "contents = [i for i in self.contents]\nfor i in contents:\n    if isinstance(i, Tag):\n        i.decompose()\n    else:\n        i.extract()\nself.extract()", "path": "downpy\\beautifulsoup\\BeautifulSoup.py", "repo_name": "rogeriopvl/Downpy", "stars": 9, "license": "other", "language": "python", "size": 461}
{"docstring": "\"\"\"Convenience method that works with all 2.x versions of Python\nto determine whether or not something is stringlike.\"\"\"\n", "func_signal": "def isString(s):\n", "code": "try:\n    return isinstance(s, unicode) or isinstance(s, basestring)\nexcept NameError:\n    return isinstance(s, str)", "path": "downpy\\beautifulsoup\\BeautifulSoup.py", "repo_name": "rogeriopvl/Downpy", "stars": 9, "license": "other", "language": "python", "size": 461}
{"docstring": "\"\"\"Returns the closest parent of this Tag that matches the given\ncriteria.\"\"\"\n# NOTE: We can't use _findOne because findParents takes a different\n# set of arguments.\n", "func_signal": "def findParent(self, name=None, attrs={}, **kwargs):\n", "code": "r = None\nl = self.findParents(name, attrs, 1)\nif l:\n    r = l[0]\nreturn r", "path": "downpy\\beautifulsoup\\BeautifulSoup.py", "repo_name": "rogeriopvl/Downpy", "stars": 9, "license": "other", "language": "python", "size": 461}
{"docstring": "\"\"\"Changes a MS smart quote character to an XML or HTML\nentity.\"\"\"\n", "func_signal": "def _subMSChar(self, match):\n", "code": "orig = match.group(1)\nsub = self.MS_CHARS.get(orig)\nif type(sub) == types.TupleType:\n    if self.smartQuotesTo == 'xml':\n        sub = '&#x'.encode() + sub[1].encode() + ';'.encode()\n    else:\n        sub = '&'.encode() + sub[0].encode() + ';'.encode()\nelse:\n    sub = sub.encode()\nreturn sub", "path": "downpy\\beautifulsoup\\BeautifulSoup.py", "repo_name": "rogeriopvl/Downpy", "stars": 9, "license": "other", "language": "python", "size": 461}
{"docstring": "\"\"\"Opens a file with the download links and rebuilds the queue\"\"\"\n", "func_signal": "def recoverState(self, folderName):\n", "code": "try:\n\tfd = open(folderName+'/download.dat', \"r\")\n\tself.items = deque(pickle.load(fd))\n\tfd.close()\n\treturn True\nexcept:\n\treturn False", "path": "downpy\\urlqueue.py", "repo_name": "rogeriopvl/Downpy", "stars": 9, "license": "other", "language": "python", "size": 461}
{"docstring": "\"\"\"Given a document, tries to detect its XML encoding.\"\"\"\n", "func_signal": "def _detectEncoding(self, xml_data, isHTML=False):\n", "code": "xml_encoding = sniffed_xml_encoding = None\ntry:\n    if xml_data[:4] == '\\x4c\\x6f\\xa7\\x94':\n        # EBCDIC\n        xml_data = self._ebcdic_to_ascii(xml_data)\n    elif xml_data[:4] == '\\x00\\x3c\\x00\\x3f':\n        # UTF-16BE\n        sniffed_xml_encoding = 'utf-16be'\n        xml_data = unicode(xml_data, 'utf-16be').encode('utf-8')\n    elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xfe\\xff') \\\n             and (xml_data[2:4] != '\\x00\\x00'):\n        # UTF-16BE with BOM\n        sniffed_xml_encoding = 'utf-16be'\n        xml_data = unicode(xml_data[2:], 'utf-16be').encode('utf-8')\n    elif xml_data[:4] == '\\x3c\\x00\\x3f\\x00':\n        # UTF-16LE\n        sniffed_xml_encoding = 'utf-16le'\n        xml_data = unicode(xml_data, 'utf-16le').encode('utf-8')\n    elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xff\\xfe') and \\\n             (xml_data[2:4] != '\\x00\\x00'):\n        # UTF-16LE with BOM\n        sniffed_xml_encoding = 'utf-16le'\n        xml_data = unicode(xml_data[2:], 'utf-16le').encode('utf-8')\n    elif xml_data[:4] == '\\x00\\x00\\x00\\x3c':\n        # UTF-32BE\n        sniffed_xml_encoding = 'utf-32be'\n        xml_data = unicode(xml_data, 'utf-32be').encode('utf-8')\n    elif xml_data[:4] == '\\x3c\\x00\\x00\\x00':\n        # UTF-32LE\n        sniffed_xml_encoding = 'utf-32le'\n        xml_data = unicode(xml_data, 'utf-32le').encode('utf-8')\n    elif xml_data[:4] == '\\x00\\x00\\xfe\\xff':\n        # UTF-32BE with BOM\n        sniffed_xml_encoding = 'utf-32be'\n        xml_data = unicode(xml_data[4:], 'utf-32be').encode('utf-8')\n    elif xml_data[:4] == '\\xff\\xfe\\x00\\x00':\n        # UTF-32LE with BOM\n        sniffed_xml_encoding = 'utf-32le'\n        xml_data = unicode(xml_data[4:], 'utf-32le').encode('utf-8')\n    elif xml_data[:3] == '\\xef\\xbb\\xbf':\n        # UTF-8 with BOM\n        sniffed_xml_encoding = 'utf-8'\n        xml_data = unicode(xml_data[3:], 'utf-8').encode('utf-8')\n    else:\n        sniffed_xml_encoding = 'ascii'\n        pass\nexcept:\n    xml_encoding_match = None\nxml_encoding_re = '^<\\?.*encoding=[\\'\"](.*?)[\\'\"].*\\?>'.encode()\nxml_encoding_match = re.compile(xml_encoding_re).match(xml_data)\nif not xml_encoding_match and isHTML:\n    meta_re = '<\\s*meta[^>]+charset=([^>]*?)[;\\'\">]'.encode()\n    regexp = re.compile(meta_re, re.I)\n    xml_encoding_match = regexp.search(xml_data)\nif xml_encoding_match is not None:\n    xml_encoding = xml_encoding_match.groups()[0].decode(\n        'ascii').lower()\n    if isHTML:\n        self.declaredHTMLEncoding = xml_encoding\n    if sniffed_xml_encoding and \\\n       (xml_encoding in ('iso-10646-ucs-2', 'ucs-2', 'csunicode',\n                         'iso-10646-ucs-4', 'ucs-4', 'csucs4',\n                         'utf-16', 'utf-32', 'utf_16', 'utf_32',\n                         'utf16', 'u16')):\n        xml_encoding = sniffed_xml_encoding\nreturn xml_data, xml_encoding, sniffed_xml_encoding", "path": "downpy\\beautifulsoup\\BeautifulSoup.py", "repo_name": "rogeriopvl/Downpy", "stars": 9, "license": "other", "language": "python", "size": 461}
{"docstring": "\"\"\"Returns the parents of this Tag that match the given\ncriteria.\"\"\"\n\n", "func_signal": "def findParents(self, name=None, attrs={}, limit=None, **kwargs):\n", "code": "return self._findAll(name, attrs, None, limit, self.parentGenerator,\n                     **kwargs)", "path": "downpy\\beautifulsoup\\BeautifulSoup.py", "repo_name": "rogeriopvl/Downpy", "stars": 9, "license": "other", "language": "python", "size": 461}
{"docstring": "\"\"\"Returns true iff this tag has the same name, the same attributes,\nand the same contents (recursively) as the given tag.\n\nNOTE: right now this will return false if two tags have the\nsame attributes in a different order. Should this be fixed?\"\"\"\n", "func_signal": "def __eq__(self, other):\n", "code": "if not hasattr(other, 'name') or not hasattr(other, 'attrs') or not hasattr(other, 'contents') or self.name != other.name or self.attrs != other.attrs or len(self) != len(other):\n    return False\nfor i in range(0, len(self.contents)):\n    if self.contents[i] != other.contents[i]:\n        return False\nreturn True", "path": "downpy\\beautifulsoup\\BeautifulSoup.py", "repo_name": "rogeriopvl/Downpy", "stars": 9, "license": "other", "language": "python", "size": 461}
{"docstring": "\"\"\"Initializes a map representation of this tag's attributes,\nif not already initialized.\"\"\"\n", "func_signal": "def _getAttrMap(self):\n", "code": "if not getattr(self, 'attrMap'):\n    self.attrMap = {}\n    for (key, value) in self.attrs:\n        self.attrMap[key] = value\nreturn self.attrMap", "path": "downpy\\beautifulsoup\\BeautifulSoup.py", "repo_name": "rogeriopvl/Downpy", "stars": 9, "license": "other", "language": "python", "size": 461}
{"docstring": "\"\"\"Convenience method that works with all 2.x versions of Python\nto determine whether or not something is listlike.\"\"\"\n", "func_signal": "def isList(l):\n", "code": "return ((hasattr(l, '__iter__') and not isString(l))\n        or (type(l) in (types.ListType, types.TupleType)))", "path": "downpy\\beautifulsoup\\BeautifulSoup.py", "repo_name": "rogeriopvl/Downpy", "stars": 9, "license": "other", "language": "python", "size": 461}
{"docstring": "\"\"\"Beautiful Soup can detect a charset included in a META tag,\ntry to convert the document to that charset, and re-parse the\ndocument from the beginning.\"\"\"\n", "func_signal": "def extractCharsetFromMeta(self, attrs):\n", "code": "httpEquiv = None\ncontentType = None\ncontentTypeIndex = None\ntagNeedsEncodingSubstitution = False\n\nfor i in range(0, len(attrs)):\n    key, value = attrs[i]\n    key = key.lower()\n    if key == 'http-equiv':\n        httpEquiv = value\n    elif key == 'content':\n        contentType = value\n        contentTypeIndex = i\n\nif httpEquiv and contentType: # It's an interesting meta tag.\n    match = self.CHARSET_RE.search(contentType)\n    if match:\n        if (self.declaredHTMLEncoding is not None or\n            self.originalEncoding == self.fromEncoding):\n            # An HTML encoding was sniffed while converting\n            # the document to Unicode, or an HTML encoding was\n            # sniffed during a previous pass through the\n            # document, or an encoding was specified\n            # explicitly and it worked. Rewrite the meta tag.\n            def rewrite(match):\n                return match.group(1) + \"%SOUP-ENCODING%\"\n            newAttr = self.CHARSET_RE.sub(rewrite, contentType)\n            attrs[contentTypeIndex] = (attrs[contentTypeIndex][0],\n                                       newAttr)\n            tagNeedsEncodingSubstitution = True\n        else:\n            # This is our first pass through the document.\n            # Go through it again with the encoding information.\n            newCharset = match.group(3)\n            if newCharset and newCharset != self.originalEncoding:\n                self.declaredHTMLEncoding = newCharset\n                self._feed(self.declaredHTMLEncoding)\n                raise StopParsing\n            pass\ntag = self.unknown_starttag(\"meta\", attrs)\nif tag and tagNeedsEncodingSubstitution:\n    tag.containsSubstitutions = True", "path": "downpy\\beautifulsoup\\BeautifulSoup.py", "repo_name": "rogeriopvl/Downpy", "stars": 9, "license": "other", "language": "python", "size": 461}
{"docstring": "\"\"\"Handle entity references as data, possibly converting known\nHTML and/or XML entity references to the corresponding Unicode\ncharacters.\"\"\"\n", "func_signal": "def handle_entityref(self, ref):\n", "code": "data = None\nif self.soup.convertHTMLEntities:\n    try:\n        data = unichr(name2codepoint[ref])\n    except KeyError:\n        pass\n\nif not data and self.soup.convertXMLEntities:\n        data = self.soup.XML_ENTITIES_TO_SPECIAL_CHARS.get(ref)\n\nif not data and self.soup.convertHTMLEntities and \\\n    not self.soup.XML_ENTITIES_TO_SPECIAL_CHARS.get(ref):\n        # TODO: We've got a problem here. We're told this is\n        # an entity reference, but it's not an XML entity\n        # reference or an HTML entity reference. Nonetheless,\n        # the logical thing to do is to pass it through as an\n        # unrecognized entity reference.\n        #\n        # Except: when the input is \"&carol;\" this function\n        # will be called with input \"carol\". When the input is\n        # \"AT&T\", this function will be called with input\n        # \"T\". We have no way of knowing whether a semicolon\n        # was present originally, so we don't know whether\n        # this is an unknown entity or just a misplaced\n        # ampersand.\n        #\n        # The more common case is a misplaced ampersand, so I\n        # escape the ampersand and omit the trailing semicolon.\n        data = \"&amp;%s\" % ref\nif not data:\n    # This case is different from the one above, because we\n    # haven't already gone through a supposedly comprehensive\n    # mapping of entities to Unicode characters. We might not\n    # have gone through any mapping at all. So the chances are\n    # very high that this is a real entity, and not a\n    # misplaced ampersand.\n    data = \"&%s;\" % ref\nself.handle_data(data)", "path": "downpy\\beautifulsoup\\BeautifulSoup.py", "repo_name": "rogeriopvl/Downpy", "stars": 9, "license": "other", "language": "python", "size": 461}
{"docstring": "\"\"\"Destructively rips this element out of the tree.\"\"\"\n", "func_signal": "def extract(self):\n", "code": "if self.parent:\n    try:\n        self.parent.contents.remove(self)\n    except ValueError:\n        pass\n\n#Find the two elements that would be next to each other if\n#this element (and any children) hadn't been parsed. Connect\n#the two.\nlastChild = self._lastRecursiveChild()\nnextElement = lastChild.next\n\nif self.previous:\n    self.previous.next = nextElement\nif nextElement:\n    nextElement.previous = self.previous\nself.previous = None\nlastChild.next = None\n\nself.parent = None\nif self.previousSibling:\n    self.previousSibling.nextSibling = self.nextSibling\nif self.nextSibling:\n    self.nextSibling.previousSibling = self.previousSibling\nself.previousSibling = self.nextSibling = None\nreturn self", "path": "downpy\\beautifulsoup\\BeautifulSoup.py", "repo_name": "rogeriopvl/Downpy", "stars": 9, "license": "other", "language": "python", "size": 461}
{"docstring": "\"\"\"Setting tag[key] sets the value of the 'key' attribute for the\ntag.\"\"\"\n", "func_signal": "def __setitem__(self, key, value):\n", "code": "self._getAttrMap()\nself.attrMap[key] = value\nfound = False\nfor i in range(0, len(self.attrs)):\n    if self.attrs[i][0] == key:\n        self.attrs[i] = (key, value)\n        found = True\nif not found:\n    self.attrs.append((key, value))\nself._getAttrMap()[key] = value", "path": "downpy\\beautifulsoup\\BeautifulSoup.py", "repo_name": "rogeriopvl/Downpy", "stars": 9, "license": "other", "language": "python", "size": 461}
{"docstring": "# mark priority order dirty if value changed\n", "func_signal": "def set_priority(self,value):\n", "code": "if self._priority != value:\n\tprogram.Program.priority_order_dirty = True\nself._priority = value", "path": "fenix\\process.py", "repo_name": "Fiona/pygame-fenix", "stars": 11, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\" Returns a real-world reprisentation of graphic including size and angle \"\"\"\n\n", "func_signal": "def get_real_surface(self):\n", "code": "if (self.size == 100 and self.angle == 0 and self.flags == 0 and self.alpha >= 255):\n\treturn self.graph\nelif self.redraw_transform_graph == False:\n\treturn self.transform_graph_cached\n\ngraph_size = self.graph.get_size()\t\t\ntransform_graph = self.graph.copy()\n\nif self.size != 100:\n\tif self.size < 0: self.size = 0\n\tnew_width = int(graph_size[0] * (self.size / 100.0))\n\tnew_height = int(graph_size[1] * (self.size / 100.0))\n\ttransform_graph = pygame.transform.scale(transform_graph, (new_width, new_height))\n\t\nif self.angle != 0:\n\ttransform_graph = pygame.transform.rotate(transform_graph, self.angle / 1000)\n\t\nif self.flags != 0:\n\ttransform_graph = pygame.transform.flip(transform_graph, (True if self.flags & B_HMIRROR else False), (True if self.flags & B_VMIRROR else False))\n\nself.special_flags = 0\n\nif self.flags & B_ABLEND:\n\tself.special_flags = BLEND_ADD\n\t\nif self.flags & B_TRANSLUCENT:\n\ttransform_graph.set_alpha(int(round(255/2)))\nelse:\n\ttransform_graph.set_alpha(self.alpha)\n\nself.transform_graph_cached = transform_graph\nself.redraw_transform_graph = False\n\nreturn transform_graph", "path": "fenix\\process.py", "repo_name": "Fiona/pygame-fenix", "stars": 11, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\" This is where the main code for the process sits \"\"\"\n", "func_signal": "def begin(self):\n", "code": "while True:\n\tyield", "path": "fenix\\process.py", "repo_name": "Fiona/pygame-fenix", "stars": 11, "license": "None", "language": "python", "size": 176}
{"docstring": "# set up the screen\n", "func_signal": "def begin(self):\n", "code": "Program.set_mode((640,480))\nProgram.set_fps(30)\n\n# Create the ... things\nfor x in range(10):\n    Box()\n\n# game loop\nwhile True:\n    \n    # Check for pressing escape\n    if Program.key(K_ESCAPE):\n        Program.quit()\n        \n    # Leave frame\n    yield", "path": "examples\\size_angle_alpha.py", "repo_name": "Fiona/pygame-fenix", "stars": 11, "license": "None", "language": "python", "size": 176}
{"docstring": "# mark for graph redraw if value changed\n", "func_signal": "def set_size(self,value):\n", "code": "if self._size != value:\n\tself.redraw_transform_graph = True\nself._size = value", "path": "fenix\\process.py", "repo_name": "Fiona/pygame-fenix", "stars": 11, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\" Process will move along the defined angle x number of pixels \"\"\"\n", "func_signal": "def xadvance(self, angle, distance):\n", "code": "self.x += int(distance * math.cos(math.radians(angle/1000.0)))\nself.y -= int(distance * math.sin(math.radians(angle/1000.0)))", "path": "fenix\\process.py", "repo_name": "Fiona/pygame-fenix", "stars": 11, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\" Worst Spanish translation ever. This will kill all processes \n\texcept for the one calling this. \"\"\"\n", "func_signal": "def let_me_alone(self):\n", "code": "import copy\nprocess_iter = copy.copy(program.Program.processes)\n\nfor obj in process_iter:\n\tif program.Program.processes[obj] != self:\n\t\tprogram.Program.single_object_signal(program.Program.processes[obj], S_KILL)", "path": "fenix\\process.py", "repo_name": "Fiona/pygame-fenix", "stars": 11, "license": "None", "language": "python", "size": 176}
{"docstring": "# set up the screen\n", "func_signal": "def begin(self):\n", "code": "Program.set_mode((640,480))\nProgram.set_fps(30)\n\n# Create the players\nGuy(270, 180)\n\n# game loop\nwhile True:\n    \n    # Check for pressing escape\n    if Program.key(K_ESCAPE):\n        Program.quit()\n        \n    # Leave frame\n    yield", "path": "examples\\process_interaction.py", "repo_name": "Fiona/pygame-fenix", "stars": 11, "license": "None", "language": "python", "size": 176}
{"docstring": "# mark for graph redraw if value changed\n", "func_signal": "def set_graph(self,value):\n", "code": "if self._graph != value:\n\tself.redraw_transform_graph = True\nself._graph = value", "path": "fenix\\process.py", "repo_name": "Fiona/pygame-fenix", "stars": 11, "license": "None", "language": "python", "size": 176}
{"docstring": "# mark for graph redraw if value changed\n", "func_signal": "def set_flags(self,value):\n", "code": "if self._flags != value:\n\tself.redraw_transform_graph = True\nself._flags = value", "path": "fenix\\process.py", "repo_name": "Fiona/pygame-fenix", "stars": 11, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\" Copy paste saving function used for collision() \"\"\"\n\n# First check for box collisions - fast and easy\n", "func_signal": "def single_object_collision(self, other, box):\n", "code": "if self.rect.colliderect(other.rect):\n\t\n\tif box == True:\n\t\treturn True\n\t\n\t# if we have box collisioning, we can try pixel-perfect\n\tmymask = pygame.mask.from_surface(self.get_real_surface())\n\tothermask = pygame.mask.from_surface(other.get_real_surface())\n\t\n\tmycenter = mymask.get_size()\n\tothercenter = othermask.get_size()\n\t \n\toffset = (\n\t\t\t  (self.x - mycenter[0]/2) - (other.x - othercenter[0]/2),\n\t\t\t  (self.y - mycenter[1]/2) - (other.y - othercenter[1]/2)\n\t\t\t  )\n\n\tpixels = othermask.overlap_area(mymask, offset)\n\n\treturn False if pixels == 0 else other\n\nelse:\n\treturn False", "path": "fenix\\process.py", "repo_name": "Fiona/pygame-fenix", "stars": 11, "license": "None", "language": "python", "size": 176}
{"docstring": "# mark for graph redraw if value changed\n", "func_signal": "def set_angle(self,value):\n", "code": "if self._angle != value:\n\tself.redraw_transform_graph = True\nself._angle = value", "path": "fenix\\process.py", "repo_name": "Fiona/pygame-fenix", "stars": 11, "license": "None", "language": "python", "size": 176}
{"docstring": "# Set the resolution and the frames per second\n", "func_signal": "def begin(self):\n", "code": "Program.set_mode((800, 600))\nProgram.set_fps(30)\n\n# load the graphics and store a pointer to the loaded surfaces.\n# Assigning them to the main game object is purely a convention\n# and is not required for pygame-fenix. They are set as members of\n# the game because it is a persistant process that lasts for the\n# length of the game.\nself.g_player = Program.load_png(\"player.png\")\nself.g_enemy = Program.load_png(\"enemy.png\")\nself.g_bullet = Program.load_png(\"bullet.png\")\n\n# Because it in persistent and holds references to all our graphic\n# surfaces, it makes sense to pass it to all of our objects.\n# Again this is purely a convention. A recommended convention, but\n# entirely optional nontheless.\nPlayer(self)\n\n# We create the invaders along the top of the screen.\nfor x in range(100, 601, 50):\n\tEnemy(self, x)\n\n# Note that we do not save references to the processes. Simply the act\n# of initialising one will cause it to exist. It's internal loop will\n# execute immediately and it can happily act independantly of everything\n# else.\n\nwhile True:\n\t# This is the main loop\n\n\t# Simple input check\n\tif Program.key(K_ESCAPE):\n\t\tProgram.exit()\n\n\t# The yield statement signifies that this object is finished for the\n\t# current frame, on the next frame the loop will resume here until it\n\t# hits the yield statement again. All objects are designed to act this way.\n\tyield", "path": "examples\\invaders\\invaders.py", "repo_name": "Fiona/pygame-fenix", "stars": 11, "license": "None", "language": "python", "size": 176}
{"docstring": "# mark z order dirty if value changed\n", "func_signal": "def set_z(self,value):\n", "code": "if self._z != value:\n\tprogram.Program.z_order_dirty = True\nself._z = value", "path": "fenix\\process.py", "repo_name": "Fiona/pygame-fenix", "stars": 11, "license": "None", "language": "python", "size": 176}
{"docstring": "# set up the screen\n", "func_signal": "def begin(self):\n", "code": "Program.set_mode((640,480))\nProgram.set_fps(30)\n\n# Create the player\nGuy()\n\n# game loop\nwhile True:\n    \n    # Check for pressing escape\n    if Program.key(K_ESCAPE):\n        Program.quit()\n        \n    # Leave frame\n    yield", "path": "examples\\basic.py", "repo_name": "Fiona/pygame-fenix", "stars": 11, "license": "None", "language": "python", "size": 176}
{"docstring": "# mark for graph redraw if value changed\n", "func_signal": "def set_alpha(self,value):\n", "code": "if self._alpha != value:\n\tself.redraw_transform_graph = True\nself._alpha = value", "path": "fenix\\process.py", "repo_name": "Fiona/pygame-fenix", "stars": 11, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\" Check for collision with other object.\n\tWill accept a process instance or an ID number to check against one,\n\tor a process type as a string to check for all of a specific type\n\tIt return False if none found, or a reference to the object that was collided with.\"\"\"\n\t  \n", "func_signal": "def collision(self, other, box = False):\n", "code": "if type(other) == type(\"\"):\n\n\t# TODO: Ouch. Maybe hash by process type name?\n\tfor obj in program.Program.processes:\n\t\tif obj != self.id:\n\t\t\tif program.Program.processes[obj].__class__.__name__ == other:\t\t\t\t\t\n\t\t\t\tcheck = self.single_object_collision(program.Program.processes[obj], box)\n\t\t\t\tif check != False:\n\t\t\t\t\treturn check\n\t\t\n\treturn False\n\nelif type(other) == type(1):\n\t\n\tother = program.Program.p(other)\n\t\n\tif other != None and other != self:\n\t\treturn self.single_object_collision(other, box)\n\telse:\n\t\treturn False\n\t\nelse:\n\tif other != None and other != self:\n\t\treturn self.single_object_collision(other, box)\n\telse:\n\t\treturn False", "path": "fenix\\process.py", "repo_name": "Fiona/pygame-fenix", "stars": 11, "license": "None", "language": "python", "size": 176}
{"docstring": "# set up the screen\n", "func_signal": "def begin(self):\n", "code": "Program.set_mode((640,480))\nProgram.set_fps(30)\n\n# Create the players\nGuy(100, 150)\nGuy(600, 200)\nGuy(400, 300)\n\n# game loop\nwhile True:\n    \n    # Check for pressing escape\n    if Program.key(K_ESCAPE):\n        Program.quit()\n        \n    # Leave frame\n    yield", "path": "examples\\basic2.py", "repo_name": "Fiona/pygame-fenix", "stars": 11, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\" \nsuper fast collision check, will only check against a single point - a tuple of x,y\nChecks for collision against rectangle defined when \"draw\" is called - in other \nwords, checks for collision with position and orientation object was last drawn at \n\"\"\"\n\n", "func_signal": "def point_collision(self, point, box = False):\n", "code": "if self.rect.collidepoint(point):\n\t\n\tif box == True:\n\t\treturn True\n\n\tpoint = (\n\t\t\t point[0]-self.rect.left,\n\t\t\t point[1]-self.rect.top\n\t\t\t )\n\t\"\"\"\n\tTODO: this check could be avoided if self.rect was guaranteed to be \n\tthe same size as the surface. It appears to be a pixel out sometimes.\n\t\"\"\"\n\tsurface = self.get_real_surface() \n\tif point[0]<surface.get_width() and point[1]<surface.get_height():\n\t\tif self.get_real_surface().get_at(point) == (255, 0, 255, 255):\n\t\t\treturn False\t\t\t\t\n\t\telse:\n\t\t\treturn True\n\telse:\n\t\treturn False\t\t\t\nelse:\n\treturn False", "path": "fenix\\process.py", "repo_name": "Fiona/pygame-fenix", "stars": 11, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\" Checks if the item is out of the region, region_id defaults to current region.\nCan be useful for checking if something is off-screen too. \"\"\"\n", "func_signal": "def out_region(self, region_id = None):\n", "code": "region_id = self.region if region_id == None else self.region\nreturn False if self.rect.colliderect(program.Program.regions[region_id]) else True", "path": "fenix\\process.py", "repo_name": "Fiona/pygame-fenix", "stars": 11, "license": "None", "language": "python", "size": 176}
{"docstring": "# Python 2.5 does not have maxsize\n", "func_signal": "def test_overflow(self):\n", "code": "maxsize = getattr(sys, 'maxsize', sys.maxint)\nself.assertRaises(OverflowError, json.decoder.scanstring, \"xxx\",\n                  maxsize + 1)", "path": "simplejson\\tests\\test_scanstring.py", "repo_name": "adewale/Buzz-Chat-Bot", "stars": 13, "license": "None", "language": "python", "size": 787}
{"docstring": "\"\"\"Modify the request headers\"\"\"\n", "func_signal": "def request(self, method, request_uri, headers, content, cnonce = None):\n", "code": "H = lambda x: _md5(x).hexdigest()\nKD = lambda s, d: H(\"%s:%s\" % (s, d))\nA2 = \"\".join([method, \":\", request_uri])\nself.challenge['cnonce'] = cnonce or _cnonce() \nrequest_digest  = '\"%s\"' % KD(H(self.A1), \"%s:%s:%s:%s:%s\" % (self.challenge['nonce'], \n            '%08x' % self.challenge['nc'], \n            self.challenge['cnonce'], \n            self.challenge['qop'], H(A2)\n            )) \nheaders['Authorization'] = 'Digest username=\"%s\", realm=\"%s\", nonce=\"%s\", uri=\"%s\", algorithm=%s, response=%s, qop=%s, nc=%08x, cnonce=\"%s\"' % (\n        self.credentials[0], \n        self.challenge['realm'],\n        self.challenge['nonce'],\n        request_uri, \n        self.challenge['algorithm'],\n        request_digest,\n        self.challenge['qop'],\n        self.challenge['nc'],\n        self.challenge['cnonce'],\n        )\nself.challenge['nc'] += 1", "path": "httplib2\\__init__.py", "repo_name": "adewale/Buzz-Chat-Bot", "stars": 13, "license": "None", "language": "python", "size": 787}
{"docstring": "\"\"\"A generator that creates Authorization objects\n   that can be applied to requests.\n\"\"\"\n", "func_signal": "def _auth_from_challenge(self, host, request_uri, headers, response, content):\n", "code": "challenges = _parse_www_authenticate(response, 'www-authenticate')\nfor cred in self.credentials.iter(host):\n    for scheme in AUTH_SCHEME_ORDER:\n        if challenges.has_key(scheme):\n            yield AUTH_SCHEME_CLASSES[scheme](cred, host, request_uri, headers, response, content, self)", "path": "httplib2\\__init__.py", "repo_name": "adewale/Buzz-Chat-Bot", "stars": 13, "license": "None", "language": "python", "size": 787}
{"docstring": "\"\"\"Returns a dictionary of dictionaries, one dict\nper auth_scheme.\"\"\"\n", "func_signal": "def _parse_www_authenticate(headers, headername='www-authenticate'):\n", "code": "retval = {}\nif headers.has_key(headername):\n    authenticate = headers[headername].strip()\n    www_auth = USE_WWW_AUTH_STRICT_PARSING and WWW_AUTH_STRICT or WWW_AUTH_RELAXED\n    while authenticate:\n        # Break off the scheme at the beginning of the line\n        if headername == 'authentication-info':\n            (auth_scheme, the_rest) = ('digest', authenticate)                \n        else:\n            (auth_scheme, the_rest) = authenticate.split(\" \", 1)\n        # Now loop over all the key value pairs that come after the scheme, \n        # being careful not to roll into the next scheme\n        match = www_auth.search(the_rest)\n        auth_params = {}\n        while match:\n            if match and len(match.groups()) == 3:\n                (key, value, the_rest) = match.groups()\n                auth_params[key.lower()] = UNQUOTE_PAIRS.sub(r'\\1', value) # '\\\\'.join([x.replace('\\\\', '') for x in value.split('\\\\\\\\')])\n            match = www_auth.search(the_rest)\n        retval[auth_scheme.lower()] = auth_params\n        authenticate = the_rest.strip()\nreturn retval", "path": "httplib2\\__init__.py", "repo_name": "adewale/Buzz-Chat-Bot", "stars": 13, "license": "None", "language": "python", "size": 787}
{"docstring": "\"\"\"The value of proxy_info is a ProxyInfo instance.\n\nIf 'cache' is a string then it is used as a directory name\nfor a disk cache. Otherwise it must be an object that supports\nthe same interface as FileCache.\"\"\"\n", "func_signal": "def __init__(self, cache=None, timeout=None, proxy_info=None):\n", "code": "self.proxy_info = proxy_info\n# Map domain name to an httplib connection\nself.connections = {}\n# The location of the cache, for now a directory\n# where cached responses are held.\nif cache and isinstance(cache, str):\n    self.cache = FileCache(cache)\nelse:\n    self.cache = cache\n\n# Name/password\nself.credentials = Credentials()\n\n# Key/cert\nself.certificates = KeyCerts()\n\n# authorization objects\nself.authorizations = []\n\n# If set to False then no redirects are followed, even safe ones.\nself.follow_redirects = True\n\n# Which HTTP methods do we apply optimistic concurrency to, i.e.\n# which methods get an \"if-match:\" etag header added to them.\nself.optimistic_concurrency_methods = [\"PUT\"]\n\n# If 'follow_redirects' is True, and this is set to True then\n# all redirecs are followed, including unsafe ones.\nself.follow_all_redirects = False\n\nself.ignore_etag = False\n\nself.force_exception_to_status_code = False \n\nself.timeout = timeout", "path": "httplib2\\__init__.py", "repo_name": "adewale/Buzz-Chat-Bot", "stars": 13, "license": "None", "language": "python", "size": 787}
{"docstring": "# XXX Should we normalize the request_uri?\n", "func_signal": "def inscope(self, host, request_uri):\n", "code": "(scheme, authority, path, query, fragment) = parse_uri(request_uri)\nreturn (host == self.host) and path.startswith(self.path)", "path": "httplib2\\__init__.py", "repo_name": "adewale/Buzz-Chat-Bot", "stars": 13, "license": "None", "language": "python", "size": 787}
{"docstring": "\"\"\"Return a filename suitable for the cache.\n\nStrips dangerous and common characters to create a filename we\ncan use to store the cache in.\n\"\"\"\n\n", "func_signal": "def safename(filename):\n", "code": "try:\n    if re_url_scheme.match(filename):\n        if isinstance(filename,str):\n            filename = filename.decode('utf-8')\n            filename = filename.encode('idna')\n        else:\n            filename = filename.encode('idna')\nexcept UnicodeError:\n    pass\nif isinstance(filename,unicode):\n    filename=filename.encode('utf-8')\nfilemd5 = _md5(filename).hexdigest()\nfilename = re_url_scheme.sub(\"\", filename)\nfilename = re_slash.sub(\",\", filename)\n\n# limit length of filename\nif len(filename)>200:\n    filename=filename[:200]\nreturn \",\".join((filename, filemd5))", "path": "httplib2\\__init__.py", "repo_name": "adewale/Buzz-Chat-Bot", "stars": 13, "license": "None", "language": "python", "size": 787}
{"docstring": "\"\"\"Create a new resource in this collection\"\"\"\n", "func_signal": "def post(self):\n", "code": "logging.info(\"Headers were: %s\" % str(self.request.headers))\nid = self.request.get('id')\nlogging.debug(\"Request id = '%s'\", id)\n\nsubscription = xmpp.Subscription.get_by_id(int(id))\nif not subscription:\n  self.response.set_status(404)\n  self.response.out.write(\"No such subscription\")\n  logging.warning('No subscription for %s' % id)\n  return\n\nsubscriber = subscription.subscriber\nsearch_term = subscription.search_term\nparser = pshb.ContentParser(self.request.body, settings.DEFAULT_HUB, settings.ALWAYS_USE_DEFAULT_HUB)\nurl = parser.extractFeedUrl()\n\nif not parser.dataValid():\n  parser.logErrors()\n  self.response.out.write(\"Bad entries: %s\" % parser.data)\n  return\nelse:\n  posts = parser.extractPosts()\n  logging.info(\"Successfully received %s posts for subscription: %s\" % (len(posts), url))\n  xmpp.send_posts(posts, subscriber, search_term)\n  self.response.set_status(200)", "path": "main.py", "repo_name": "adewale/Buzz-Chat-Bot", "stars": 13, "license": "None", "language": "python", "size": 787}
{"docstring": "\"\"\"Determine freshness from the Date, Expires and Cache-Control headers.\n\nWe don't handle the following:\n\n1. Cache-Control: max-stale\n2. Age: headers are not used in the calculations.\n\nNot that this algorithm is simpler than you might think \nbecause we are operating as a private (non-shared) cache.\nThis lets us ignore 's-maxage'. We can also ignore\n'proxy-invalidate' since we aren't a proxy.\nWe will never return a stale document as \nfresh as a design decision, and thus the non-implementation \nof 'max-stale'. This also lets us safely ignore 'must-revalidate' \nsince we operate as if every server has sent 'must-revalidate'.\nSince we are private we get to ignore both 'public' and\n'private' parameters. We also ignore 'no-transform' since\nwe don't do any transformations.    \nThe 'no-store' parameter is handled at a higher level.\nSo the only Cache-Control parameters we look at are:\n\nno-cache\nonly-if-cached\nmax-age\nmin-fresh\n\"\"\"\n\n", "func_signal": "def _entry_disposition(response_headers, request_headers):\n", "code": "retval = \"STALE\"\ncc = _parse_cache_control(request_headers)\ncc_response = _parse_cache_control(response_headers)\n\nif request_headers.has_key('pragma') and request_headers['pragma'].lower().find('no-cache') != -1:\n    retval = \"TRANSPARENT\"\n    if 'cache-control' not in request_headers:\n        request_headers['cache-control'] = 'no-cache'\nelif cc.has_key('no-cache'):\n    retval = \"TRANSPARENT\"\nelif cc_response.has_key('no-cache'):\n    retval = \"STALE\"\nelif cc.has_key('only-if-cached'):\n    retval = \"FRESH\"\nelif response_headers.has_key('date'):\n    date = calendar.timegm(email.Utils.parsedate_tz(response_headers['date']))\n    now = time.time()\n    current_age = max(0, now - date)\n    if cc_response.has_key('max-age'):\n        try:\n            freshness_lifetime = int(cc_response['max-age'])\n        except ValueError:\n            freshness_lifetime = 0\n    elif response_headers.has_key('expires'):\n        expires = email.Utils.parsedate_tz(response_headers['expires'])\n        if None == expires:\n            freshness_lifetime = 0\n        else:\n            freshness_lifetime = max(0, calendar.timegm(expires) - date)\n    else:\n        freshness_lifetime = 0\n    if cc.has_key('max-age'):\n        try:\n            freshness_lifetime = int(cc['max-age'])\n        except ValueError:\n            freshness_lifetime = 0\n    if cc.has_key('min-fresh'):\n        try:\n            min_fresh = int(cc['min-fresh'])\n        except ValueError:\n            min_fresh = 0\n        current_age += min_fresh \n    if freshness_lifetime > current_age:\n        retval = \"FRESH\"\nreturn retval", "path": "httplib2\\__init__.py", "repo_name": "adewale/Buzz-Chat-Bot", "stars": 13, "license": "None", "language": "python", "size": 787}
{"docstring": "\"\"\"Show all the resources in this collection\"\"\"\n", "func_signal": "def get(self):\n", "code": "logging.info(\"Headers were: %s\" % str(self.request.headers))\nlogging.info('Request: %s' % str(self.request))\nid = self.request.get('id')\nlogging.debug(\"Request id = '%s'\", id)\n\n# If this is a hub challenge\nif self.request.get('hub.challenge'):\n# If this subscription exists\n  mode = self.request.get('hub.mode')\n  topic = self.request.get('hub.topic')\n  if mode == \"subscribe\" and xmpp.Subscription.get_by_id(int(id)):\n    self.response.out.write(self.request.get('hub.challenge'))\n    logging.info(\"Successfully accepted %s challenge for feed: %s\" % (mode, topic))\n  elif mode == \"unsubscribe\" and not xmpp.Subscription.get_by_id(int(id)):\n    self.response.out.write(self.request.get('hub.challenge'))\n    logging.info(\"Successfully accepted %s challenge for feed: %s\" % (mode, topic))\n  else:\n    self.response.set_status(404)\n    self.response.out.write(\"Challenge failed\")\n    logging.info(\"Challenge failed for feed: %s\" % topic)\n  # Once a challenge has been issued there's no point in returning anything other than challenge passed or failed\n  return", "path": "main.py", "repo_name": "adewale/Buzz-Chat-Bot", "stars": 13, "license": "None", "language": "python", "size": 787}
{"docstring": "# info is either an email.Message or \n# an httplib.HTTPResponse object.\n", "func_signal": "def __init__(self, info):\n", "code": "if isinstance(info, httplib.HTTPResponse):\n    for key, value in info.getheaders(): \n        self[key.lower()] = value \n    self.status = info.status\n    self['status'] = str(self.status)\n    self.reason = info.reason\n    self.version = info.version\nelif isinstance(info, email.Message.Message):\n    for key, value in info.items(): \n        self[key] = value \n    self.status = int(self['status'])\nelse:\n    for key, value in info.iteritems(): \n        self[key] = value \n    self.status = int(self.get('status', self.status))", "path": "httplib2\\__init__.py", "repo_name": "adewale/Buzz-Chat-Bot", "stars": 13, "license": "None", "language": "python", "size": 787}
{"docstring": "# test in/out equivalence and parsing\n", "func_signal": "def test_parse(self):\n", "code": "res = json.loads(JSON)\nout = json.dumps(res)\nself.assertEquals(res, json.loads(out))\ntry:\n    json.dumps(res, allow_nan=False)\nexcept ValueError:\n    pass\nelse:\n    self.fail(\"23456789012E666 should be out of range\")", "path": "simplejson\\tests\\test_pass1.py", "repo_name": "adewale/Buzz-Chat-Bot", "stars": 13, "license": "None", "language": "python", "size": 787}
{"docstring": "# Several optimizations were made that skip over calls to\n# the whitespace regex, so this test is designed to try and\n# exercise the uncommon cases. The array cases are already covered.\n", "func_signal": "def test_decoder_optimizations(self):\n", "code": "rval = json.loads('{   \"key\"    :    \"value\"    ,  \"k\":\"v\"    }')\nself.assertEquals(rval, {\"key\":\"value\", \"k\":\"v\"})", "path": "simplejson\\tests\\test_decode.py", "repo_name": "adewale/Buzz-Chat-Bot", "stars": 13, "license": "None", "language": "python", "size": 787}
{"docstring": "\"\"\"Modify the request headers to add the appropriate\nAuthorization header.\"\"\"\n", "func_signal": "def request(self, method, request_uri, headers, content):\n", "code": "headers['Authorization'] = 'WSSE profile=\"UsernameToken\"'\niso_now = time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())\ncnonce = _cnonce()\npassword_digest = _wsse_username_token(cnonce, iso_now, self.credentials[1])\nheaders['X-WSSE'] = 'UsernameToken Username=\"%s\", PasswordDigest=\"%s\", Nonce=\"%s\", Created=\"%s\"' % (\n        self.credentials[0],\n        password_digest,\n        cnonce,\n        iso_now)", "path": "httplib2\\__init__.py", "repo_name": "adewale/Buzz-Chat-Bot", "stars": 13, "license": "None", "language": "python", "size": 787}
{"docstring": "\"\"\"Return list of (header, value) tuples.\"\"\"\n", "func_signal": "def HTTPResponse__getheaders(self):\n", "code": "if self.msg is None:\n    raise httplib.ResponseNotReady()\nreturn self.msg.items()", "path": "httplib2\\__init__.py", "repo_name": "adewale/Buzz-Chat-Bot", "stars": 13, "license": "None", "language": "python", "size": 787}
{"docstring": "\"\"\"Modify the request headers\"\"\"\n", "func_signal": "def request(self, method, request_uri, headers, content):\n", "code": "keys = _get_end2end_headers(headers)\nkeylist = \"\".join([\"%s \" % k for k in keys])\nheaders_val = \"\".join([headers[k] for k in keys])\ncreated = time.strftime('%Y-%m-%dT%H:%M:%SZ',time.gmtime())\ncnonce = _cnonce()\nrequest_digest = \"%s:%s:%s:%s:%s\" % (method, request_uri, cnonce, self.challenge['snonce'], headers_val)\nrequest_digest  = hmac.new(self.key, request_digest, self.hashmod).hexdigest().lower()\nheaders['Authorization'] = 'HMACDigest username=\"%s\", realm=\"%s\", snonce=\"%s\", cnonce=\"%s\", uri=\"%s\", created=\"%s\", response=\"%s\", headers=\"%s\"' % (\n        self.credentials[0], \n        self.challenge['realm'],\n        self.challenge['snonce'],\n        cnonce,\n        request_uri, \n        created,\n        request_digest,\n        keylist,\n        )", "path": "httplib2\\__init__.py", "repo_name": "adewale/Buzz-Chat-Bot", "stars": 13, "license": "None", "language": "python", "size": 787}
{"docstring": "\"\"\"Do the actual request using the connection object\nand also follow one level of redirects if necessary\"\"\"\n\n", "func_signal": "def _request(self, conn, host, absolute_uri, request_uri, method, body, headers, redirections, cachekey):\n", "code": "auths = [(auth.depth(request_uri), auth) for auth in self.authorizations if auth.inscope(host, request_uri)]\nauth = auths and sorted(auths)[0][1] or None\nif auth: \n    auth.request(method, request_uri, headers, body)\n\n(response, content) = self._conn_request(conn, request_uri, method, body, headers)\n\nif auth: \n    if auth.response(response, body):\n        auth.request(method, request_uri, headers, body)\n        (response, content) = self._conn_request(conn, request_uri, method, body, headers )\n        response._stale_digest = 1\n\nif response.status == 401:\n    for authorization in self._auth_from_challenge(host, request_uri, headers, response, content):\n        authorization.request(method, request_uri, headers, body) \n        (response, content) = self._conn_request(conn, request_uri, method, body, headers, )\n        if response.status != 401:\n            self.authorizations.append(authorization)\n            authorization.response(response, body)\n            break\n\nif (self.follow_all_redirects or (method in [\"GET\", \"HEAD\"]) or response.status == 303):\n    if self.follow_redirects and response.status in [300, 301, 302, 303, 307]:\n        # Pick out the location header and basically start from the beginning\n        # remembering first to strip the ETag header and decrement our 'depth'\n        if redirections:\n            if not response.has_key('location') and response.status != 300:\n                raise RedirectMissingLocation( _(\"Redirected but the response is missing a Location: header.\"), response, content)\n            # Fix-up relative redirects (which violate an RFC 2616 MUST)\n            if response.has_key('location'):\n                location = response['location']\n                (scheme, authority, path, query, fragment) = parse_uri(location)\n                if authority == None:\n                    response['location'] = urlparse.urljoin(absolute_uri, location)\n            if response.status == 301 and method in [\"GET\", \"HEAD\"]:\n                response['-x-permanent-redirect-url'] = response['location']\n                if not response.has_key('content-location'):\n                    response['content-location'] = absolute_uri \n                _updateCache(headers, response, content, self.cache, cachekey)\n            if headers.has_key('if-none-match'):\n                del headers['if-none-match']\n            if headers.has_key('if-modified-since'):\n                del headers['if-modified-since']\n            if response.has_key('location'):\n                location = response['location']\n                old_response = copy.deepcopy(response)\n                if not old_response.has_key('content-location'):\n                    old_response['content-location'] = absolute_uri \n                redirect_method = ((response.status == 303) and (method not in [\"GET\", \"HEAD\"])) and \"GET\" or method\n                (response, content) = self.request(location, redirect_method, body=body, headers = headers, redirections = redirections - 1)\n                response.previous = old_response\n        else:\n            raise RedirectLimit( _(\"Redirected more times than rediection_limit allows.\"), response, content)\n    elif response.status in [200, 203] and method == \"GET\":\n        # Don't cache 206's since we aren't going to handle byte range requests\n        if not response.has_key('content-location'):\n            response['content-location'] = absolute_uri \n        _updateCache(headers, response, content, self.cache, cachekey)\n\nreturn (response, content)", "path": "httplib2\\__init__.py", "repo_name": "adewale/Buzz-Chat-Bot", "stars": 13, "license": "None", "language": "python", "size": 787}
{"docstring": "\"\"\"Parses a URI using the regex given in Appendix B of RFC 3986.\n\n    (scheme, authority, path, query, fragment) = parse_uri(uri)\n\"\"\"\n", "func_signal": "def parse_uri(uri):\n", "code": "groups = URI.match(uri).groups()\nreturn (groups[1], groups[3], groups[4], groups[6], groups[8])", "path": "httplib2\\__init__.py", "repo_name": "adewale/Buzz-Chat-Bot", "stars": 13, "license": "None", "language": "python", "size": 787}
{"docstring": "# http://bugs.python.org/issue6105\n", "func_signal": "def test_ordered_dict(self):\n", "code": "items = [('one', 1), ('two', 2), ('three', 3), ('four', 4), ('five', 5)]\ns = json.dumps(json.OrderedDict(items))\nself.assertEqual(s, '{\"one\": 1, \"two\": 2, \"three\": 3, \"four\": 4, \"five\": 5}')", "path": "simplejson\\tests\\test_dump.py", "repo_name": "adewale/Buzz-Chat-Bot", "stars": 13, "license": "None", "language": "python", "size": 787}
{"docstring": "\"\"\"Remove all the names and passwords\nthat are used for authentication\"\"\"\n", "func_signal": "def clear_credentials(self):\n", "code": "self.credentials.clear()\nself.authorizations = []", "path": "httplib2\\__init__.py", "repo_name": "adewale/Buzz-Chat-Bot", "stars": 13, "license": "None", "language": "python", "size": 787}
{"docstring": "\"\"\" Returns WayData for the way. \"\"\"\n", "func_signal": "def _DomParseWay(self, DomElement):\n", "code": "result = self._DomGetAttributes(DomElement)\nresult[u\"tag\"] = self._DomGetTag(DomElement)\nresult[u\"nd\"]  = self._DomGetNd(DomElement)        \nreturn result", "path": "src\\OsmApi.py", "repo_name": "lizzard/mainbrace", "stars": 8, "license": "None", "language": "python", "size": 836}
{"docstring": "\"\"\" Returns ChangesetData for changeset #ChangesetId. \"\"\"\n", "func_signal": "def ChangesetGet(self, ChangesetId):\n", "code": "data = self._get(\"/api/0.6/changeset/\"+str(ChangesetId))\ndata = xml.dom.minidom.parseString(data)\ndata = data.getElementsByTagName(\"osm\")[0].getElementsByTagName(\"changeset\")[0]\nreturn self._DomParseChangeset(data)", "path": "src\\OsmApi.py", "repo_name": "lizzard/mainbrace", "stars": 8, "license": "None", "language": "python", "size": 836}
{"docstring": "\"\"\" Return full data for relation RelationId as list of {type: node|way|relation, data: {}}. \"\"\"\n", "func_signal": "def RelationFull(self, RelationId):\n", "code": "uri = \"/api/0.6/relation/\"+str(RelationId)+\"/full\"\ndata = self._get(uri)\nreturn self.ParseOsm(data)", "path": "src\\OsmApi.py", "repo_name": "lizzard/mainbrace", "stars": 8, "license": "None", "language": "python", "size": 836}
{"docstring": "\"\"\" Parse osm data. Returns list of dict {type: node|way|relation, data: {}}. \"\"\"\n", "func_signal": "def ParseOsm(self, data):\n", "code": "data = xml.dom.minidom.parseString(data)\ndata = data.getElementsByTagName(\"osm\")[0]\nresult = []\nfor elem in data.childNodes:\n    if elem.nodeName == u\"node\":\n        result.append({u\"type\": elem.nodeName, u\"data\": self._DomParseNode(elem)})\n    elif elem.nodeName == u\"way\":\n        result.append({u\"type\": elem.nodeName, u\"data\": self._DomParseWay(elem)})                        \n    elif elem.nodeName == u\"relation\":\n        result.append({u\"type\": elem.nodeName, u\"data\": self._DomParseRelation(elem)})\nreturn result", "path": "src\\OsmApi.py", "repo_name": "lizzard/mainbrace", "stars": 8, "license": "None", "language": "python", "size": 836}
{"docstring": "\"\"\" Runs test suite for file \"\"\"\n", "func_signal": "def handle_test(self, v):\n", "code": "import doctest\nimport unittest\nsuite = unittest.defaultTestLoader.loadTestsFromModule(sys.modules.get(__name__))\nsuite.addTest(doctest.DocTestSuite())\nrunner = unittest.TextTestRunner()\nrunner.run(suite)\nsys.exit(0)", "path": "src\\mainbrace.py", "repo_name": "lizzard/mainbrace", "stars": 8, "license": "None", "language": "python", "size": 836}
{"docstring": "\"\"\" Parse osc data. Returns list of dict {type: node|way|relation, action: create|delete|modify, data: {}}. \"\"\"\n", "func_signal": "def ParseOsc(self, data):\n", "code": "data = xml.dom.minidom.parseString(data)\ndata = data.getElementsByTagName(\"osmChange\")[0]\nresult = []\nfor action in data.childNodes:\n    if action.nodeName == u\"#text\": continue\n    for elem in action.childNodes:\n        if elem.nodeName == u\"node\":\n            result.append({u\"action\":action.nodeName, u\"type\": elem.nodeName, u\"data\": self._DomParseNode(elem)})\n        elif elem.nodeName == u\"way\":\n            result.append({u\"action\":action.nodeName, u\"type\": elem.nodeName, u\"data\": self._DomParseWay(elem)})                        \n        elif elem.nodeName == u\"relation\":\n            result.append({u\"action\":action.nodeName, u\"type\": elem.nodeName, u\"data\": self._DomParseRelation(elem)})\nreturn result", "path": "src\\OsmApi.py", "repo_name": "lizzard/mainbrace", "stars": 8, "license": "None", "language": "python", "size": 836}
{"docstring": "\"\"\" Returns [WayData, ... ] containing node #NodeId. \"\"\"\n", "func_signal": "def NodeWays(self, NodeId):\n", "code": "uri = \"/api/0.6/node/%d/ways\"%NodeId\ndata = self._get(uri)\ndata = xml.dom.minidom.parseString(data)\nresult = []\nfor data in data.getElementsByTagName(\"osm\")[0].getElementsByTagName(\"way\"):\n    data = self._DomParseRelation(data)\n    result.append(data)\nreturn result", "path": "src\\OsmApi.py", "repo_name": "lizzard/mainbrace", "stars": 8, "license": "None", "language": "python", "size": 836}
{"docstring": "\"\"\" Return full data for way WayId as list of {type: node|way|relation, data: {}}. \"\"\"\n", "func_signal": "def WayFull(self, WayId):\n", "code": "uri = \"/api/0.6/way/\"+str(WayId)+\"/full\"\ndata = self._get(uri)\nreturn self.ParseOsm(data)", "path": "src\\OsmApi.py", "repo_name": "lizzard/mainbrace", "stars": 8, "license": "None", "language": "python", "size": 836}
{"docstring": "\"\"\" Returns RelationData for the relation. \"\"\"\n", "func_signal": "def _DomParseRelation(self, DomElement):\n", "code": "result = self._DomGetAttributes(DomElement)\nresult[u\"tag\"]    = self._DomGetTag(DomElement)\nresult[u\"member\"] = self._DomGetMember(DomElement)\nreturn result", "path": "src\\OsmApi.py", "repo_name": "lizzard/mainbrace", "stars": 8, "license": "None", "language": "python", "size": 836}
{"docstring": "\"\"\" Returns dict(WayId: WayData) for each way in WayIdList \"\"\"\n", "func_signal": "def WaysGet(self, WayIdList):\n", "code": "uri = \"/api/0.6/ways?ways=\" + \",\".join([str(x) for x in WayIdList])\ndata = self._get(uri)\ndata = xml.dom.minidom.parseString(data)\nresult = {}\nfor data in data.getElementsByTagName(\"osm\")[0].getElementsByTagName(\"way\"):\n    data = self._DomParseWay(data)\n    result[data[u\"id\"]] = data\nreturn result", "path": "src\\OsmApi.py", "repo_name": "lizzard/mainbrace", "stars": 8, "license": "None", "language": "python", "size": 836}
{"docstring": "\"\"\" Returns [RelationData, ...] containing way #WayId. \"\"\"\n", "func_signal": "def WayRelations(self, WayId):\n", "code": "uri = \"/api/0.6/way/%d/relations\"%WayId\ndata = self._get(uri)\ndata = xml.dom.minidom.parseString(data)\nresult = []\nfor data in data.getElementsByTagName(\"osm\")[0].getElementsByTagName(\"relation\"):\n    data = self._DomParseRelation(data)\n    result.append(data)\nreturn result", "path": "src\\OsmApi.py", "repo_name": "lizzard/mainbrace", "stars": 8, "license": "None", "language": "python", "size": 836}
{"docstring": "\"\"\" Returns NodeData for the node. \"\"\"\n", "func_signal": "def _DomParseNode(self, DomElement):\n", "code": "result = self._DomGetAttributes(DomElement)\nresult[u\"tag\"] = self._DomGetTag(DomElement)\nreturn result", "path": "src\\OsmApi.py", "repo_name": "lizzard/mainbrace", "stars": 8, "license": "None", "language": "python", "size": 836}
{"docstring": "\"\"\" Returns dict(NodeId: NodeData) for each node in NodeIdList \"\"\"\n", "func_signal": "def NodesGet(self, NodeIdList):\n", "code": "uri  = \"/api/0.6/nodes?nodes=\" + \",\".join([str(x) for x in NodeIdList])\ndata = self._get(uri)\ndata = xml.dom.minidom.parseString(data)\nresult = {}\nfor data in data.getElementsByTagName(\"osm\")[0].getElementsByTagName(\"node\"):\n    data = self._DomParseNode(data)\n    result[data[u\"id\"]] = data\nreturn result", "path": "src\\OsmApi.py", "repo_name": "lizzard/mainbrace", "stars": 8, "license": "None", "language": "python", "size": 836}
{"docstring": "\"\"\" Returns dict(NodeVerrsion: NodeData). \"\"\"\n", "func_signal": "def NodeHistory(self, NodeId):\n", "code": "uri = \"/api/0.6/node/\"+str(NodeId)+\"/history\"\ndata = self._get(uri)\ndata = xml.dom.minidom.parseString(data)\nresult = {}\nfor data in data.getElementsByTagName(\"osm\")[0].getElementsByTagName(\"node\"):\n    data = self._DomParseNode(data)\n    result[data[u\"version\"]] = data\nreturn result", "path": "src\\OsmApi.py", "repo_name": "lizzard/mainbrace", "stars": 8, "license": "None", "language": "python", "size": 836}
{"docstring": "\"\"\" Download data from a changeset. Returns list of dict {type: node|way|relation, action: create|delete|modify, data: {}}. \"\"\"\n", "func_signal": "def ChangesetDownload(self, ChangesetId):\n", "code": "uri = \"/api/0.6/changeset/\"+str(ChangesetId)+\"/download\"\ndata = self._get(uri)\nreturn self.ParseOsc(data)", "path": "src\\OsmApi.py", "repo_name": "lizzard/mainbrace", "stars": 8, "license": "None", "language": "python", "size": 836}
{"docstring": "\"\"\" Returns dict(RelationVerrsion: RelationData). \"\"\"\n", "func_signal": "def RelationHistory(self, RelationId):\n", "code": "uri = \"/api/0.6/relation/\"+str(RelationId)+\"/history\"\ndata = self._get(uri)\ndata = xml.dom.minidom.parseString(data)\nresult = {}\nfor data in data.getElementsByTagName(\"osm\")[0].getElementsByTagName(\"relation\"):\n    data = self._DomParseRelation(data)\n    result[data[u\"version\"]] = data\nreturn result", "path": "src\\OsmApi.py", "repo_name": "lizzard/mainbrace", "stars": 8, "license": "None", "language": "python", "size": 836}
{"docstring": "\"\"\" Returns ChangesetData for the changeset. \"\"\"\n", "func_signal": "def _DomParseChangeset(self, DomElement):\n", "code": "result = self._DomGetAttributes(DomElement)\nresult[u\"tag\"] = self._DomGetTag(DomElement)\nreturn result", "path": "src\\OsmApi.py", "repo_name": "lizzard/mainbrace", "stars": 8, "license": "None", "language": "python", "size": 836}
{"docstring": "\"\"\" Returns a list of relation members. \"\"\"\n", "func_signal": "def _DomGetMember(self, DomElement):\n", "code": "result = []\nfor m in DomElement.getElementsByTagName(\"member\"):\n    result.append(self._DomGetAttributes(m))\nreturn result", "path": "src\\OsmApi.py", "repo_name": "lizzard/mainbrace", "stars": 8, "license": "None", "language": "python", "size": 836}
{"docstring": "\"\"\" Returns list of RelationData containing relation #RelationId. \"\"\"\n", "func_signal": "def RelationRelations(self, RelationId):\n", "code": "uri = \"/api/0.6/relation/%d/relations\"%RelationId\ndata = self._get(uri)\ndata = xml.dom.minidom.parseString(data)\nresult = []\nfor data in data.getElementsByTagName(\"osm\")[0].getElementsByTagName(\"relation\"):\n    data = self._DomParseRelation(data)\n    result.append(data)\nreturn result", "path": "src\\OsmApi.py", "repo_name": "lizzard/mainbrace", "stars": 8, "license": "None", "language": "python", "size": 836}
{"docstring": "\"\"\" Returns dict(RelationId: RelationData) for each relation in RelationIdList \"\"\"\n", "func_signal": "def RelationsGet(self, RelationIdList):\n", "code": "uri = \"/api/0.6/relations?relations=\" + \",\".join([str(x) for x in RelationIdList])\ndata = self._get(uri)\ndata = xml.dom.minidom.parseString(data)\nresult = {}\nfor data in data.getElementsByTagName(\"osm\")[0].getElementsByTagName(\"relation\"):\n    data = self._DomParseRelation(data)\n    result[data[u\"id\"]] = data            \nreturn result", "path": "src\\OsmApi.py", "repo_name": "lizzard/mainbrace", "stars": 8, "license": "None", "language": "python", "size": 836}
{"docstring": "\"\"\"preprocess all objects contained within this ``UOWDependencyProcessor``s target task.\n\nThis may locate additional objects which should be part of the\ntransaction, such as those affected deletes, orphans to be\ndeleted, etc.\n\nOnce an object is preprocessed, its ``UOWTaskElement`` is marked as processed.  If subsequent \nchanges occur to the ``UOWTaskElement``, its processed flag is reset, and will require processing\nagain.\n\nReturn True if any objects were preprocessed, or False if no\nobjects were preprocessed.  If True is returned, the parent ``UOWTransaction`` will\nultimately call ``preexecute()`` again on all processors until no new objects are processed.\n\"\"\"\n\n", "func_signal": "def preexecute(self, trans):\n", "code": "def getobj(elem):\n    elem.mark_preprocessed(self)\n    return elem.state\n\nret = False\nelements = [getobj(elem) for elem in self.targettask.polymorphic_tosave_elements if elem.state is not None and not elem.is_preprocessed(self)]\nif elements:\n    ret = True\n    self.processor.preprocess_dependencies(self.targettask, elements, trans, delete=False)\n\nelements = [getobj(elem) for elem in self.targettask.polymorphic_todelete_elements if elem.state is not None and not elem.is_preprocessed(self)]\nif elements:\n    ret = True\n    self.processor.preprocess_dependencies(self.targettask, elements, trans, delete=True)\nreturn ret", "path": "iphone-build\\directory-template\\usr\\lib\\python2.5\\site-packages\\SQLAlchemy-0.4.7p1-py2.5.egg\\sqlalchemy\\orm\\unitofwork.py", "repo_name": "makiaea/ankimini", "stars": 10, "license": "None", "language": "python", "size": 1111}
{"docstring": "# if object is not in the overall session, do nothing\n", "func_signal": "def register_object(self, state, isdelete = False, listonly = False, postupdate=False, post_update_cols=None, **kwargs):\n", "code": "if not self.uow._is_valid(state):\n    if self._should_log_debug:\n        self.logger.debug(\"object %s not part of session, not registering for flush\" % (mapperutil.state_str(state)))\n    return\n\nif self._should_log_debug:\n    self.logger.debug(\"register object for flush: %s isdelete=%s listonly=%s postupdate=%s\" % (mapperutil.state_str(state), isdelete, listonly, postupdate))\n\nmapper = _state_mapper(state)\n\ntask = self.get_task_by_mapper(mapper)\nif postupdate:\n    task.append_postupdate(state, post_update_cols)\nelse:\n    task.append(state, listonly, isdelete=isdelete, **kwargs)", "path": "iphone-build\\directory-template\\usr\\lib\\python2.5\\site-packages\\SQLAlchemy-0.4.7p1-py2.5.egg\\sqlalchemy\\orm\\unitofwork.py", "repo_name": "makiaea/ankimini", "stars": 10, "license": "None", "language": "python", "size": 1111}
{"docstring": "\"\"\"create a dependency tree of all pending SQL operations within this unit of work and execute.\"\"\"\n\n", "func_signal": "def flush(self, session, objects=None):\n", "code": "dirty = [x for x in self.identity_map.all_states()\n    if x.modified\n    or (x.class_._class_state.has_mutable_scalars and x.is_modified())\n]\n\nif not dirty and not self.deleted and not self.new:\n    return\n\ndeleted = util.Set(self.deleted)\nnew = util.Set(self.new)\n\ndirty = util.Set(dirty).difference(deleted)\n\nflush_context = UOWTransaction(self, session)\n\nif session.extension is not None:\n    session.extension.before_flush(session, flush_context, objects)\n\n# create the set of all objects we want to operate upon\nif objects:\n    # specific list passed in\n    objset = util.Set([o._state for o in objects])\nelse:\n    # or just everything\n    objset = util.Set(self.identity_map.all_states()).union(new)\n    \n# store objects whose fate has been decided\nprocessed = util.Set()\n\n# put all saves/updates into the flush context.  detect top-level orphans and throw them into deleted.\nfor state in new.union(dirty).intersection(objset).difference(deleted):\n    if state in processed:\n        continue\n\n    obj = state.obj()\n    is_orphan = _state_mapper(state)._is_orphan(obj)\n    if is_orphan and not has_identity(obj):\n        raise exceptions.FlushError(\"instance %s is an unsaved, pending instance and is an orphan (is not attached to %s)\" %\n            (\n                obj,\n                \", nor \".join([\"any parent '%s' instance via that classes' '%s' attribute\" % (klass.__name__, key) for (key,klass) in _state_mapper(state).delete_orphans])\n            ))\n    flush_context.register_object(state, isdelete=is_orphan)\n    processed.add(state)\n\n# put all remaining deletes into the flush context.\nfor state in deleted.intersection(objset).difference(processed):\n    flush_context.register_object(state, isdelete=True)\n\nif len(flush_context.tasks) == 0:\n    return\n    \nsession.create_transaction(autoflush=False)\nflush_context.transaction = session.transaction\ntry:\n    flush_context.execute()\n    \n    if session.extension is not None:\n        session.extension.after_flush(session, flush_context)\n    session.commit()\nexcept:\n    session.rollback()\n    raise\n\nflush_context.post_exec()\n\nif session.extension is not None:\n    session.extension.after_flush_postexec(session, flush_context)", "path": "iphone-build\\directory-template\\usr\\lib\\python2.5\\site-packages\\SQLAlchemy-0.4.7p1-py2.5.egg\\sqlalchemy\\orm\\unitofwork.py", "repo_name": "makiaea/ankimini", "stars": 10, "license": "None", "language": "python", "size": 1111}
{"docstring": "# process \"save_update\" cascade rules for when an instance is attached to another instance\n", "func_signal": "def set(self, obj, newvalue, oldvalue, initiator):\n", "code": "if oldvalue is newvalue:\n    return\nsess = object_session(obj)\nif sess:\n    if newvalue is not None and self.cascade.save_update and newvalue not in sess:\n        sess.save_or_update(newvalue, entity_name=self._target_mapper(obj).entity_name)\n    if self.cascade.delete_orphan and oldvalue in sess.new:\n        sess.expunge(oldvalue)", "path": "iphone-build\\directory-template\\usr\\lib\\python2.5\\site-packages\\SQLAlchemy-0.4.7p1-py2.5.egg\\sqlalchemy\\orm\\unitofwork.py", "repo_name": "makiaea/ankimini", "stars": 10, "license": "None", "language": "python", "size": 1111}
{"docstring": "# process \"save_update\" cascade rules for when an instance is appended to the list of another instance\n", "func_signal": "def append(self, obj, item, initiator):\n", "code": "sess = object_session(obj)\nif sess:\n    if self.cascade.save_update and item not in sess:\n        sess.save_or_update(item, entity_name=self._target_mapper(obj).entity_name)", "path": "iphone-build\\directory-template\\usr\\lib\\python2.5\\site-packages\\SQLAlchemy-0.4.7p1-py2.5.egg\\sqlalchemy\\orm\\unitofwork.py", "repo_name": "makiaea/ankimini", "stars": 10, "license": "None", "language": "python", "size": 1111}
{"docstring": "\"\"\"return true if the given state is marked as deleted within this UOWTransaction.\"\"\"\n\n", "func_signal": "def is_deleted(self, state):\n", "code": "mapper = _state_mapper(state)\ntask = self.get_task_by_mapper(mapper)\nreturn task.is_deleted(state)", "path": "iphone-build\\directory-template\\usr\\lib\\python2.5\\site-packages\\SQLAlchemy-0.4.7p1-py2.5.egg\\sqlalchemy\\orm\\unitofwork.py", "repo_name": "makiaea/ankimini", "stars": 10, "license": "None", "language": "python", "size": 1111}
{"docstring": "\"\"\"overrides attributes.register_attribute() to add UOW event handlers\nto new InstrumentedAttributes.\n\"\"\"\n\n", "func_signal": "def register_attribute(class_, key, *args, **kwargs):\n", "code": "cascade = kwargs.pop('cascade', None)\nuseobject = kwargs.get('useobject', False)\nif useobject:\n    # for object-holding attributes, instrument UOWEventHandler\n    # to process per-attribute cascades\n    extension = util.to_list(kwargs.pop('extension', None) or [])\n    extension.insert(0, UOWEventHandler(key, class_, cascade=cascade))\n    kwargs['extension'] = extension\nreturn attributes.register_attribute(class_, key, *args, **kwargs)", "path": "iphone-build\\directory-template\\usr\\lib\\python2.5\\site-packages\\SQLAlchemy-0.4.7p1-py2.5.egg\\sqlalchemy\\orm\\unitofwork.py", "repo_name": "makiaea/ankimini", "stars": 10, "license": "None", "language": "python", "size": 1111}
{"docstring": "\"\"\"Removes unreferenced instances cached in a strong-referencing identity map.\n\nNote that this method is only meaningful if \"weak_identity_map\"\non the parent Session is set to False and therefore this UnitOfWork's\nidentity map is a regular dictionary\n\nRemoves any object in the identity map that is not referenced\nin user code or scheduled for a unit of work operation.  Returns\nthe number of objects pruned.\n\"\"\"\n\n", "func_signal": "def prune_identity_map(self):\n", "code": "if isinstance(self.identity_map, attributes.WeakInstanceDict):\n    return 0\nref_count = len(self.identity_map)\ndirty = self.locate_dirty()\nkeepers = weakref.WeakValueDictionary(self.identity_map)\nself.identity_map.clear()\nself.identity_map.update(keepers)\nreturn ref_count - len(self.identity_map)", "path": "iphone-build\\directory-template\\usr\\lib\\python2.5\\site-packages\\SQLAlchemy-0.4.7p1-py2.5.egg\\sqlalchemy\\orm\\unitofwork.py", "repo_name": "makiaea/ankimini", "stars": 10, "license": "None", "language": "python", "size": 1111}
{"docstring": "\"\"\"return an iterator of UOWTask objects corresponding to the inheritance sequence\nof this UOWTask's mapper.\n\n    e.g. if mapper B and mapper C inherit from mapper A, and mapper D inherits from B:\n    \n        mapperA -> mapperB -> mapperD\n                -> mapperC \n                           \n    the inheritance sequence starting at mapper A is a depth-first traversal:\n    \n        [mapperA, mapperB, mapperD, mapperC]\n        \n    this method will therefore return\n    \n        [UOWTask(mapperA), UOWTask(mapperB), UOWTask(mapperD), UOWTask(mapperC)]\n        \nThe concept of \"polymporphic iteration\" is adapted into several property-based \niterators which return object instances, UOWTaskElements and UOWDependencyProcessors\nin an order corresponding to this sequence of parent UOWTasks.  This is used to issue\noperations related to inheritance-chains of mappers in the proper order based on \ndependencies between those mappers.\n\n\"\"\"\n\n", "func_signal": "def polymorphic_tasks(self):\n", "code": "for mapper in self.mapper.polymorphic_iterator():\n    t = self.base_task._inheriting_tasks.get(mapper, None)\n    if t is not None:\n        yield t", "path": "iphone-build\\directory-template\\usr\\lib\\python2.5\\site-packages\\SQLAlchemy-0.4.7p1-py2.5.egg\\sqlalchemy\\orm\\unitofwork.py", "repo_name": "makiaea/ankimini", "stars": 10, "license": "None", "language": "python", "size": 1111}
{"docstring": "\"\"\"return a set of all persistent instances within this unit of work which \neither contain changes or are marked as deleted.\n\"\"\"\n\n# a little bit of inlining for speed\n", "func_signal": "def locate_dirty(self):\n", "code": "return util.IdentitySet([x for x in self.identity_map.values() \n    if x._state not in self.deleted \n    and (\n        x._state.modified\n        or (x.__class__._class_state.has_mutable_scalars and x._state.is_modified())\n    )\n    ])", "path": "iphone-build\\directory-template\\usr\\lib\\python2.5\\site-packages\\SQLAlchemy-0.4.7p1-py2.5.egg\\sqlalchemy\\orm\\unitofwork.py", "repo_name": "makiaea/ankimini", "stars": 10, "license": "None", "language": "python", "size": 1111}
{"docstring": "\"\"\"mark a deleted object as a 'row switch'.\n\nthis indicates that an INSERT statement elsewhere corresponds to this DELETE;\nthe INSERT is converted to an UPDATE and the DELETE does not occur.\n\"\"\"\n", "func_signal": "def set_row_switch(self, state):\n", "code": "mapper = _state_mapper(state)\ntask = self.get_task_by_mapper(mapper)\ntaskelement = task._objects[state]\ntaskelement.isdelete = \"rowswitch\"", "path": "iphone-build\\directory-template\\usr\\lib\\python2.5\\site-packages\\SQLAlchemy-0.4.7p1-py2.5.egg\\sqlalchemy\\orm\\unitofwork.py", "repo_name": "makiaea/ankimini", "stars": 10, "license": "None", "language": "python", "size": 1111}
{"docstring": "\"\"\"return True if the given object is contained within this UOWTask or inheriting tasks.\"\"\"\n\n", "func_signal": "def __contains__(self, state):\n", "code": "for task in self.polymorphic_tasks():\n    if state in task._objects:\n        return True\nelse:\n    return False", "path": "iphone-build\\directory-template\\usr\\lib\\python2.5\\site-packages\\SQLAlchemy-0.4.7p1-py2.5.egg\\sqlalchemy\\orm\\unitofwork.py", "repo_name": "makiaea/ankimini", "stars": 10, "license": "None", "language": "python", "size": 1111}
{"docstring": "\"\"\"process all objects contained within this ``UOWDependencyProcessor``s target task.\"\"\"\n\n", "func_signal": "def execute(self, trans, delete):\n", "code": "if not delete:\n    self.processor.process_dependencies(self.targettask, [elem.state for elem in self.targettask.polymorphic_tosave_elements if elem.state is not None], trans, delete=False)\nelse:\n    self.processor.process_dependencies(self.targettask, [elem.state for elem in self.targettask.polymorphic_todelete_elements if elem.state is not None], trans, delete=True)", "path": "iphone-build\\directory-template\\usr\\lib\\python2.5\\site-packages\\SQLAlchemy-0.4.7p1-py2.5.egg\\sqlalchemy\\orm\\unitofwork.py", "repo_name": "makiaea/ankimini", "stars": 10, "license": "None", "language": "python", "size": 1111}
{"docstring": "\"\"\"Create a hierarchical tree of *subtasks*\nwhich associate specific dependency actions with individual\nobjects.  This is used for a *cyclical* task, or a task where\nelements of its object list contain dependencies on each\nother.\n\nThis is not the normal case; this logic only kicks in when\nsomething like a hierarchical tree is being represented.\n\"\"\"\n", "func_signal": "def _sort_circular_dependencies(self, trans, cycles):\n", "code": "allobjects = []\nfor task in cycles:\n    allobjects += [e.state for e in task.polymorphic_elements]\ntuples = []\n\ncycles = util.Set(cycles)\n\nextradeplist = []\ndependencies = {}\n\ndef get_dependency_task(state, depprocessor):\n    try:\n        dp = dependencies[state]\n    except KeyError:\n        dp = dependencies.setdefault(state, {})\n    try:\n        l = dp[depprocessor]\n    except KeyError:\n        l = UOWTask(self.uowtransaction, depprocessor.targettask.mapper)\n        dp[depprocessor] = l\n    return l\n\ndef dependency_in_cycles(dep):\n    proctask = trans.get_task_by_mapper(dep.processor.mapper.base_mapper, True)\n    targettask = trans.get_task_by_mapper(dep.targettask.mapper.base_mapper, True)\n    return targettask in cycles and (proctask is not None and proctask in cycles)\n\n# organize all original UOWDependencyProcessors by their target task\ndeps_by_targettask = {}\nfor task in cycles:\n    for dep in task.polymorphic_dependencies:\n        if not dependency_in_cycles(dep):\n            extradeplist.append(dep)\n        for t in dep.targettask.polymorphic_tasks():\n            l = deps_by_targettask.setdefault(t, [])\n            l.append(dep)\n\nobject_to_original_task = {}\n\nfor task in cycles:\n    for subtask in task.polymorphic_tasks():\n        for taskelement in subtask.elements:\n            state = taskelement.state\n            object_to_original_task[state] = subtask\n            for dep in deps_by_targettask.get(subtask, []):\n                # is this dependency involved in one of the cycles ?\n                # (don't count the DetectKeySwitch prop)\n                if dep.processor.no_dependencies or not dependency_in_cycles(dep):\n                    continue\n                (processor, targettask) = (dep.processor, dep.targettask)\n                isdelete = taskelement.isdelete\n\n                # list of dependent objects from this object\n                (added, unchanged, deleted) = dep.get_object_dependencies(state, trans, passive=True)\n                if not added and not unchanged and not deleted:\n                    continue\n                    \n                # the task corresponding to saving/deleting of those dependent objects\n                childtask = trans.get_task_by_mapper(processor.mapper)\n\n                childlist = added + unchanged + deleted\n\n                for o in childlist:\n                    # other object is None.  this can occur if the relationship is many-to-one\n                    # or one-to-one, and None was set.  the \"removed\" object will be picked\n                    # up in this iteration via the deleted_items() part of the collection.\n                    if o is None:\n                        continue\n\n                    # the other object is not in the UOWTransaction !  but if we are many-to-one,\n                    # we need a task in order to attach dependency operations, so establish a \"listonly\"\n                    # task\n                    if o not in childtask:\n                        childtask.append(o, listonly=True)\n                        object_to_original_task[o] = childtask\n\n                    # create a tuple representing the \"parent/child\"\n                    whosdep = dep.whose_dependent_on_who(state, o)\n                    if whosdep is not None:\n                        # append the tuple to the partial ordering.\n                        tuples.append(whosdep)\n\n                        # create a UOWDependencyProcessor representing this pair of objects.\n                        # append it to a UOWTask\n                        if whosdep[0] is state:\n                            get_dependency_task(whosdep[0], dep).append(whosdep[0], isdelete=isdelete)\n                        else:\n                            get_dependency_task(whosdep[0], dep).append(whosdep[1], isdelete=isdelete)\n                    else:\n                        # TODO: no test coverage here\n                        get_dependency_task(state, dep).append(state, isdelete=isdelete)\n\nhead = topological.sort_as_tree(tuples, allobjects)\n\nused_tasks = util.Set()\ndef make_task_tree(node, parenttask, nexttasks):\n    (state, cycles, children) = node\n    originating_task = object_to_original_task[state]\n    used_tasks.add(originating_task)\n    t = nexttasks.get(originating_task, None)\n    if t is None:\n        t = UOWTask(self.uowtransaction, originating_task.mapper)\n        nexttasks[originating_task] = t\n        parenttask._append_cyclical_childtask(t)\n    t.append(state, originating_task._objects[state].listonly, isdelete=originating_task._objects[state].isdelete)\n\n    if state in dependencies:\n        for depprocessor, deptask in dependencies[state].iteritems():\n            t.cyclical_dependencies.add(depprocessor.branch(deptask))\n    nd = {}\n    for n in children:\n        t2 = make_task_tree(n, t, nd)\n    return t\n\nt = UOWTask(self.uowtransaction, self.mapper)\n\n# stick the non-circular dependencies onto the new UOWTask\nfor d in extradeplist:\n    t.dependencies.add(d)\n\nif head is not None:\n    make_task_tree(head, t, {})\n\nret = [t]\n\n# add tasks that were in the cycle, but didnt get assembled\n# into the cyclical tree, to the start of the list\nfor t2 in cycles:\n    if t2 not in used_tasks and t2 is not self:\n        localtask = UOWTask(self.uowtransaction, t2.mapper)\n        for state in t2.elements:\n            localtask.append(state, t2.listonly, isdelete=t2._objects[state].isdelete)\n        for dep in t2.dependencies:\n            localtask.dependencies.add(dep)\n        ret.insert(0, localtask)\n\nreturn ret", "path": "iphone-build\\directory-template\\usr\\lib\\python2.5\\site-packages\\SQLAlchemy-0.4.7p1-py2.5.egg\\sqlalchemy\\orm\\unitofwork.py", "repo_name": "makiaea/ankimini", "stars": 10, "license": "None", "language": "python", "size": 1111}
{"docstring": "\"\"\"mark processed objects as clean / deleted after a successful flush().\n\nthis method is called within the flush() method after the \nexecute() method has succeeded and the transaction has been committed.\n\"\"\"\n\n", "func_signal": "def post_exec(self):\n", "code": "for task in self.tasks.values():\n    for elem in task.elements:\n        if elem.state is None:\n            continue\n        if elem.isdelete:\n            self.uow._remove_deleted(elem.state)\n        else:\n            self.uow._register_clean(elem.state)", "path": "iphone-build\\directory-template\\usr\\lib\\python2.5\\site-packages\\SQLAlchemy-0.4.7p1-py2.5.egg\\sqlalchemy\\orm\\unitofwork.py", "repo_name": "makiaea/ankimini", "stars": 10, "license": "None", "language": "python", "size": 1111}
{"docstring": "\"\"\"return a property that will adapt the collection returned by the\ngiven callable into a polymorphic traversal.\"\"\"\n\n", "func_signal": "def _polymorphic_collection(callable):\n", "code": "def collection(self):\n    for task in self.polymorphic_tasks():\n        for rec in callable(task):\n            yield rec\nreturn property(collection)", "path": "iphone-build\\directory-template\\usr\\lib\\python2.5\\site-packages\\SQLAlchemy-0.4.7p1-py2.5.egg\\sqlalchemy\\orm\\unitofwork.py", "repo_name": "makiaea/ankimini", "stars": 10, "license": "None", "language": "python", "size": 1111}
{"docstring": "\"\"\"register a dependency processor, corresponding to dependencies between\nthe two given mappers.\n\n\"\"\"\n\n# correct for primary mapper\n", "func_signal": "def register_processor(self, mapper, processor, mapperfrom):\n", "code": "mapper = mapper.primary_mapper()\nmapperfrom = mapperfrom.primary_mapper()\n\ntask = self.get_task_by_mapper(mapper)\ntargettask = self.get_task_by_mapper(mapperfrom)\nup = UOWDependencyProcessor(processor, targettask)\ntask.dependencies.add(up)", "path": "iphone-build\\directory-template\\usr\\lib\\python2.5\\site-packages\\SQLAlchemy-0.4.7p1-py2.5.egg\\sqlalchemy\\orm\\unitofwork.py", "repo_name": "makiaea/ankimini", "stars": 10, "license": "None", "language": "python", "size": 1111}
{"docstring": "\"\"\"return True if the given object is marked as to be deleted within this UOWTask.\"\"\"\n\n", "func_signal": "def is_deleted(self, state):\n", "code": "try:\n    return self._objects[state].isdelete\nexcept KeyError:\n    return False", "path": "iphone-build\\directory-template\\usr\\lib\\python2.5\\site-packages\\SQLAlchemy-0.4.7p1-py2.5.egg\\sqlalchemy\\orm\\unitofwork.py", "repo_name": "makiaea/ankimini", "stars": 10, "license": "None", "language": "python", "size": 1111}
{"docstring": "\"\"\"Execute this UOWTransaction.\n\nThis will organize all collected UOWTasks into a dependency-sorted\nlist which is then traversed using the traversal scheme\nencoded in the UOWExecutor class.  Operations to mappers and dependency\nprocessors are fired off in order to issue SQL to the database and \nsynchronize instance attributes with database values and related\nforeign key values.\"\"\"\n\n# pre-execute dependency processors.  this process may\n# result in new tasks, objects and/or dependency processors being added,\n# particularly with 'delete-orphan' cascade rules.\n# keep running through the full list of tasks until all\n# objects have been processed.\n", "func_signal": "def execute(self):\n", "code": "while True:\n    ret = False\n    for task in self.tasks.values():\n        for up in list(task.dependencies):\n            if up.preexecute(self):\n                ret = True\n    if not ret:\n        break\n\ntasks = self._sort_dependencies()\nif self._should_log_info:\n    self.logger.info(\"Task dump:\\n\" + self._dump(tasks))\nUOWExecutor().execute(self, tasks)\nif self._should_log_info:\n    self.logger.info(\"Execute Complete\")", "path": "iphone-build\\directory-template\\usr\\lib\\python2.5\\site-packages\\SQLAlchemy-0.4.7p1-py2.5.egg\\sqlalchemy\\orm\\unitofwork.py", "repo_name": "makiaea/ankimini", "stars": 10, "license": "None", "language": "python", "size": 1111}
{"docstring": "\"\"\"register the given object as 'new' (i.e. unsaved) within this unit of work.\"\"\"\n\n", "func_signal": "def register_new(self, obj):\n", "code": "if hasattr(obj, '_instance_key'):\n    raise exceptions.InvalidRequestError(\"Object '%s' already has an identity - it can't be registered as new\" % repr(obj))\nif obj._state not in self.new:\n    self.new[obj._state] = obj\n    obj._state.insert_order = len(self.new)", "path": "iphone-build\\directory-template\\usr\\lib\\python2.5\\site-packages\\SQLAlchemy-0.4.7p1-py2.5.egg\\sqlalchemy\\orm\\unitofwork.py", "repo_name": "makiaea/ankimini", "stars": 10, "license": "None", "language": "python", "size": 1111}
{"docstring": "\"\"\"Inserts a set of FeedToFetch entities for a set of topics.\n\nOverwrites any existing entities that are already there.\n\nArgs:\n\ttopic_list: List of the topic URLs of feeds that need to be fetched.\n\tsource_dict: Dictionary of sources for the feed. Defaults to an empty\n\t\tdictionary.\n\"\"\"\n", "func_signal": "def insert(cls, topic_list, source_dict=None):\n", "code": "if not topic_list:\n\treturn\n\nif source_dict:\n\tsource_keys, source_values = zip(*source_dict.items())\t# Yay Python!\nelse:\n\tsource_keys, source_values = [], []\n\nfeed_list = [\n\t\tcls(key_name=utils.get_hash_key_name(topic),\n\t\t\t\ttopic=topic,\n\t\t\t\tsource_keys=list(source_keys),\n\t\t\t\tsource_values=list(source_values))\n\t\tfor topic in set(topic_list)]\ndb.put(feed_list)\n# TODO(bslatkin): Use a bulk interface or somehow merge combined fetches\n# into a single task.\nfor feed in feed_list:\n\tfeed._enqueue_task()", "path": "hub\\hubmodel.py", "repo_name": "PaulKinlan/Amplifriend", "stars": 10, "license": "None", "language": "python", "size": 1835}
{"docstring": "\"\"\"Records that a callback URL needs to be unsubscribed.\n\nCreates a new request to unsubscribe a callback URL from a topic (where\nverification should happen asynchronously). If an unsubscribe request\nhas already been made, this method will do nothing.\n\nArgs:\n\tcallback: URL that will receive callbacks.\n\ttopic: The topic to subscribe to.\n\tverify_token: The verification token to use to confirm the\n\t\tunsubscription request.\n\nReturns:\n\tTrue if the Subscription to remove actually exists, False otherwise.\n\"\"\"\n", "func_signal": "def request_remove(cls, callback, topic, verify_token):\n", "code": "key_name = cls.create_key_name(callback, topic)\ndef txn():\n\tsub = cls.get_by_key_name(key_name)\n\tif sub is not None:\n\t\tsub.put()\n\t\treturn (True, sub)\n\treturn (False, sub)\nremoved, sub = db.run_in_transaction(txn)\n# Note: This enqueuing must come *after* the transaction is submitted, or\n# else we'll actually run the task *before* the transaction is submitted.\nif sub:\n\tsub.enqueue_task(cls.STATE_TO_DELETE)\nreturn removed", "path": "hub\\hubmodel.py", "repo_name": "PaulKinlan/Amplifriend", "stars": 10, "license": "None", "language": "python", "size": 1835}
{"docstring": "\"\"\"Enqueues a task to fetch this feed.\"\"\"\n# TODO(bslatkin): Remove these retries when they're not needed in userland.\n", "func_signal": "def _enqueue_task(self):\n", "code": "RETRIES = 3\ntarget_queue = os.environ.get('X_APPENGINE_QUEUENAME', constants.FEED_QUEUE)\nfor i in xrange(RETRIES):\n\ttry:\n\t\ttaskqueue.Task(\n\t\t\t\turl='/work/pull_feeds',\n\t\t\t\teta=self.eta,\n\t\t\t\tparams={'topic': self.topic}\n\t\t\t\t).add(target_queue)\n\texcept (taskqueue.Error, apiproxy_errors.Error):\n\t\tlogging.exception('Could not insert task to fetch topic = %s',\n\t\t\t\t\t\t\t\t\t\t\tself.topic)\n\t\tif i == (RETRIES - 1):\n\t\t\traise\n\telse:\n\t\treturn", "path": "hub\\hubmodel.py", "repo_name": "PaulKinlan/Amplifriend", "stars": 10, "license": "None", "language": "python", "size": 1835}
{"docstring": "\"\"\"Enqueues a task to confirm this Subscription.\n\nArgs:\n\tnext_state: The next state this subscription should be in.\n\"\"\"\n# TODO(bslatkin): Remove these retries when they're not needed in userland.\n", "func_signal": "def enqueue_task(self, next_state):\n", "code": "RETRIES = 3\ntarget_queue = os.environ.get('X_APPENGINE_QUEUENAME', SUBSCRIPTION_QUEUE)\nfor i in xrange(RETRIES):\n\ttry:\n\t\ttaskqueue.Task(\n\t\t\t\turl='/work/subscriptions',\n\t\t\t\teta=self.eta,\n\t\t\t\tparams={'subscription_key_name': self.key().name(),\n\t\t\t\t\t\t\t\t'next_state': next_state}\n\t\t\t\t).add(target_queue)\n\texcept (taskqueue.Error, apiproxy_errors.Error):\n\t\tlogging.exception('Could not insert task to confirm '\n\t\t\t\t\t\t\t\t\t\t\t'topic = %s, callback = %s',\n\t\t\t\t\t\t\t\t\t\t\tself.topic, self.callback)\n\t\tif i == (RETRIES - 1):\n\t\t\traise\n\telse:\n\t\treturn", "path": "hub\\hubmodel.py", "repo_name": "PaulKinlan/Amplifriend", "stars": 10, "license": "None", "language": "python", "size": 1835}
{"docstring": "\"\"\"Creates a new Key for a FeedEntryRecord entity.\n\nArgs:\n\ttopic: The topic URL to retrieve entries for.\n\tentry_id: String containing the entry_id.\n\nReturns:\n\tKey instance for this FeedEntryRecord.\n\"\"\"\n", "func_signal": "def create_key(cls, topic, entry_id):\n", "code": "return db.Key.from_path(\n\t\tFeedRecord.kind(),\n\t\tFeedRecord.create_key_name(topic),\n\t\tcls.kind(),\n\t\tget_hash_key_name(entry_id))", "path": "hub\\hubmodel.py", "repo_name": "PaulKinlan/Amplifriend", "stars": 10, "license": "None", "language": "python", "size": 1835}
{"docstring": "\"\"\"\nLog the current session out.\n\"\"\"\n\n", "func_signal": "def get(self):\n", "code": "self.SessionObj.delete()\n\nself.redirect(\"/\")", "path": "session\\DeleteSessionHandler.py", "repo_name": "PaulKinlan/Amplifriend", "stars": 10, "license": "None", "language": "python", "size": 1835}
{"docstring": "\"\"\"Gets multiple FeedEntryRecord entities for a topic by their entry_ids.\n\nArgs:\n\ttopic: The topic URL to retrieve entries for.\n\tentry_id_list: Sequence of entry_ids to retrieve.\n\nReturns:\n\tList of FeedEntryRecords that were found, if any.\n\"\"\"\n", "func_signal": "def get_entries_for_topic(cls, topic, entry_id_list):\n", "code": "results = cls.get([cls.create_key(topic, entry_id)\n\t\t\t\t\t\t\t\t\t for entry_id in entry_id_list])\n# Filter out those pesky Nones.\nreturn [r for r in results if r]", "path": "hub\\hubmodel.py", "repo_name": "PaulKinlan/Amplifriend", "stars": 10, "license": "None", "language": "python", "size": 1835}
{"docstring": "\"\"\"Decorator to memoize functions using memcache.\"\"\"\n", "func_signal": "def selfmemoize(keyformat, time=60):\n", "code": "def decorator(fxn):\n\tdef wrapper(self, *args, **kwargs):\n\t\tord = keyformat.count('%')\n\t\tkey = \"\"\n\t\t\n\t\tif ord > 0:\n\t\t\tkey = keyformat % args[0:ord]\n\t\telse:\n\t\t\tkey = keyformat\n\t\t\t\n\t\tkey = key + str(kwargs)\n\t\t\n\t\t#logging.info(\"SELFMEMO %s\" % key)\n\t\t\t\n\t\tm = md5.new()\n\t\tm.update(key)\n\t\t\n\t\tkey_digest = m.hexdigest()\n\t\t\t\n\t\tdata = memcache.get(key_digest)\n\t\tif data is not None:\n\t\t\t#logging.info(\"Cache Hit for key %s\" % key_digest)\n\t\t\treturn data\n\t\t\t\n\t\t#logging.info(\"Cache Miss for key %s\" % key_digest)\n\t\tdata = fxn(self, *args, **kwargs)\n\t\tmemcache.set(key_digest, data, time)\n\t\treturn data\n\treturn wrapper\nreturn decorator", "path": "utils.py", "repo_name": "PaulKinlan/Amplifriend", "stars": 10, "license": "None", "language": "python", "size": 1835}
{"docstring": "#Get the session parameters\n", "func_signal": "def wrapper(self, *args, **kwargs):\n", "code": "\t\t\tauth_id = self.request.cookies.get('auth_id', '')\n\t\t\tsession_id = self.request.cookies.get('session_id', '')\n\t\t\t\n\t\t\t#Check the db for the session\n\t\t\tsession = Session.GetSession(session_id, auth_id)\t\t\t\n\t\t\t\n\t\t\tif session is None:\n\t\t\t\tself.redirect(redirectTo)\n\t\t\t\treturn\n\t\t\telse:\n\t\t\t\tif session.user is None:\n\t\t\t\t\tself.redirect(redirectTo)\n\t\t\t\t\treturn\n\t\t\t\t\t\n\t\t\t\tusername = self.SessionObj.user.username\n\t\t\t\t\n\t\t\t\tif len(args) > 0:\t\t\t\t\n\t\t\t\t\tif username != args[0]:\n\t\t\t\t\t\t# The user is allowed to view this page.\n\t\t\t\t\t\tself.redirect(redirectTo)\n\t\t\t\t\t\treturn\n\t\t\t\t\n\t\t\tresult = method(self, *args, **kwargs)\n\t\t\t\t\n\t\t\treturn result", "path": "webdecorators.py", "repo_name": "PaulKinlan/Amplifriend", "stars": 10, "license": "None", "language": "python", "size": 1835}
{"docstring": "\"\"\"\nA Decorator that calls a method on none exception of method call, calls another method if it throws a wobbler.\n\nparent key is the object that we are working against.\ntype will probably be configuration\ndata is a string of the message that we wish to store.\n\"\"\"\n", "func_signal": "def audit(model = None, type = type, parent_key = \"app\", data = \"\"):\n", "code": "def factory(method):\n\t@functools.wraps(method)\n\tdef wrapper(self, *args, **kwargs):\n\t\n\t\t# Call the method\n\t\tresult = None\n\t\tkey = self.request.get(parent_key)\n\t\tparent = None\n\t\tip_address = self.request.remote_addr\n\t\tnew_data = \"%s: %s\" % (ip_address, data)\n\t\t\n\t\tif model is None:\n\t\t\t#If the parent is None, then it means that the key being passed is a proper DB key\n\t\t\tparent = db.get(db.Key(key))\n\t\telse:\n\t\t\tparent = model().GetByKey(key)\n\t\t\n\t\ttry:\n\t\t\tEvent.Create(type, \"started\", parent, data = new_data)\n\t\t\n\t\t\tresult = method(self, *args, **kwargs)\n\t\texcept:\n\t\t\tEvent.Create(type, \"errored\", parent, data = new_data)\n\t\telse:\n\t\t\tEvent.Create(type, \"finished\", parent, data = new_data)\n\t\n\t\treturn result\n\treturn wrapper\nreturn factory", "path": "webdecorators.py", "repo_name": "PaulKinlan/Amplifriend", "stars": 10, "license": "None", "language": "python", "size": 1835}
{"docstring": "\"\"\"Enqueues a Task that will execute this EventToDeliver.\"\"\"\n# TODO(bslatkin): Remove these retries when they're not needed in userland.\n", "func_signal": "def enqueue(self):\n", "code": "RETRIES = 3\ntarget_queue = os.environ.get('X_APPENGINE_QUEUENAME', constants.EVENT_QUEUE)\nfor i in xrange(RETRIES):\n\ttry:\n\t\ttaskqueue.Task(\n\t\t\t\turl='/work/push_events',\n\t\t\t\teta=self.last_modified,\n\t\t\t\tparams={'event_key': self.key()}\n\t\t\t\t).add(target_queue)\n\texcept (taskqueue.Error, apiproxy_errors.Error):\n\t\tlogging.exception('Could not insert task to deliver '\n\t\t\t\t\t\t\t\t\t\t\t'events for topic = %s', self.topic)\n\t\tif i == (RETRIES - 1):\n\t\t\traise\n\telse:\n\t\treturn", "path": "hub\\hubmodel.py", "repo_name": "PaulKinlan/Amplifriend", "stars": 10, "license": "None", "language": "python", "size": 1835}
{"docstring": "\"\"\"Returns the current PollingMarker, creating it if it doesn't exist.\n\nArgs:\n\tnow: Returns the current time as a UTC datetime.\n\"\"\"\n", "func_signal": "def get(cls, now=datetime.datetime.utcnow):\n", "code": "key_name = 'The Mark'\nthe_mark = db.get(datastore_types.Key.from_path(cls.kind(), key_name))\nif the_mark is None:\n\tnext_start = now() - datetime.timedelta(seconds=60)\n\tthe_mark = PollingMarker(key_name=key_name,\n\t\t\t\t\t\t\t\t\t\t\t\t\t next_start=next_start,\n\t\t\t\t\t\t\t\t\t\t\t\t\t current_key=None)\nreturn the_mark", "path": "hub\\hubmodel.py", "repo_name": "PaulKinlan/Amplifriend", "stars": 10, "license": "None", "language": "python", "size": 1835}
{"docstring": "\"\"\"Gets the list of subscribers starting at an offset.\n\nArgs:\n\ttopic: The topic URL to retrieve subscribers for.\n\tcount: How many subscribers to retrieve.\n\tstarting_at_callback: A string containing the callback hash to offset\n\t\tto when retrieving more subscribers. The callback at the given offset\n\t\t*will* be included in the results. If None, then subscribers will\n\t\tbe retrieved from the beginning.\n\nReturns:\n\tList of Subscription objects that were found, or an empty list if none\n\twere found.\n\"\"\"\n", "func_signal": "def get_subscribers(cls, topic, count, starting_at_callback=None):\n", "code": "query = cls.all()\nquery.filter('topic_hash =', utils.sha1_hash(topic))\nquery.filter('subscription_state = ', cls.STATE_VERIFIED)\nif starting_at_callback:\n\tquery.filter('callback_hash >=', utils.sha1_hash(starting_at_callback))\nquery.order('callback_hash')\n\nreturn query.fetch(count)", "path": "hub\\hubmodel.py", "repo_name": "PaulKinlan/Amplifriend", "stars": 10, "license": "None", "language": "python", "size": 1835}
{"docstring": "\"\"\"Check if a topic URL has verified subscribers.\n\nArgs:\n\ttopic: The topic URL to check for subscribers.\n\nReturns:\n\tTrue if it has verified subscribers, False otherwise.\n\"\"\"\n", "func_signal": "def has_subscribers(cls, topic):\n", "code": "if (cls.all().filter('topic_hash =', utils.sha1_hash(topic))\n\t\t.filter('subscription_state =', cls.STATE_VERIFIED).get() is not None):\n\treturn True\nelse:\n\treturn False", "path": "hub\\hubmodel.py", "repo_name": "PaulKinlan/Amplifriend", "stars": 10, "license": "None", "language": "python", "size": 1835}
{"docstring": "\"\"\"Updates the polling record of this feed.\n\nThis method will *not* insert this instance into the Datastore.\n\nArgs:\n\theaders: Dictionary of response headers from the feed that should be used\n\t\tto determine how to poll the feed in the future.\n\theader_footer: Contents of the feed's XML document minus the entry data;\n\t\tif not supplied, the old value will remain.\n\"\"\"\n", "func_signal": "def update(self, headers, header_footer=None):\n", "code": "self.content_type = headers.get('Content-Type', '').lower()\nself.last_modified = headers.get('Last-Modified')\nself.etag = headers.get('ETag')\nif header_footer is not None:\n\tself.header_footer = header_footer", "path": "hub\\hubmodel.py", "repo_name": "PaulKinlan/Amplifriend", "stars": 10, "license": "None", "language": "python", "size": 1835}
{"docstring": "\"\"\"Retrieve the next set of subscribers to attempt delivery for this event.\n\nArgs:\n\tchunk_size: How many subscribers to retrieve at a time while delivering\n\t\tthe event. Defaults to EVENT_SUBSCRIBER_CHUNK_SIZE.\n\nReturns:\n\tTuple (more_subscribers, subscription_list) where:\n\t\tmore_subscribers: True if there are more subscribers to deliver to\n\t\t\tafter the returned 'subscription_list' has been contacted; this value\n\t\t\tshould be passed to update() after the delivery is attempted.\n\t\tsubscription_list: List of Subscription entities to attempt to contact\n\t\t\tfor this event.\n\"\"\"\n", "func_signal": "def get_next_subscribers(self, chunk_size=None):\n", "code": "if chunk_size is None:\n\tchunk_size = constants.EVENT_SUBSCRIBER_CHUNK_SIZE\n\nif self.delivery_mode == EventToDeliver.NORMAL:\n\tall_subscribers = Subscription.get_subscribers(\n\t\t\tself.topic, chunk_size + 1, starting_at_callback=self.last_callback)\n\tif all_subscribers:\n\t\tself.last_callback = all_subscribers[-1].callback\n\telse:\n\t\tself.last_callback = ''\n\n\tmore_subscribers = len(all_subscribers) > chunk_size\n\tsubscription_list = all_subscribers[:chunk_size]\nelif self.delivery_mode == EventToDeliver.RETRY:\n\tnext_chunk = self.failed_callbacks[:chunk_size]\n\tmore_subscribers = len(self.failed_callbacks) > len(next_chunk)\n\n\tif self.last_callback:\n\t\t# If the final index is present in the next chunk, that means we've\n\t\t# wrapped back around to the beginning and will need to do more\n\t\t# exponential backoff. This also requires updating the last_callback\n\t\t# in the update() method, since we do not know which callbacks from\n\t\t# the next chunk will end up failing.\n\t\tfinal_subscription_key = datastore_types.Key.from_path(\n\t\t\t\tSubscription.__name__,\n\t\t\t\tSubscription.create_key_name(self.last_callback, self.topic))\n\t\ttry:\n\t\t\tfinal_index = next_chunk.index(final_subscription_key)\n\t\texcept ValueError:\n\t\t\tpass\n\t\telse:\n\t\t\tmore_subscribers = False\n\t\t\tnext_chunk = next_chunk[:final_index]\n\n\tsubscription_list = [x for x in db.get(next_chunk) if x is not None]\n\tif subscription_list and not self.last_callback:\n\t\t# This must be the first time through the current iteration where we do\n\t\t# not yet know a sentinal value in the list that represents the starting\n\t\t# point.\n\t\tself.last_callback = subscription_list[0].callback\n\n\t# If the failed callbacks fail again, they will be added back to the\n\t# end of the list.\n\tself.failed_callbacks = self.failed_callbacks[len(next_chunk):]\n\nreturn more_subscribers, subscription_list", "path": "hub\\hubmodel.py", "repo_name": "PaulKinlan/Amplifriend", "stars": 10, "license": "None", "language": "python", "size": 1835}
{"docstring": "# test in/out equivalence and parsing\n", "func_signal": "def test_parse(self):\n", "code": "res = S.loads(JSON)\nout = S.dumps(res)\nself.assertEquals(res, S.loads(out))\ntry:\n    S.dumps(res, allow_nan=False)\nexcept ValueError:\n    pass\nelse:\n    self.fail(\"23456789012E666 should be out of range\")", "path": "simplejson\\tests\\test_pass1.py", "repo_name": "PaulKinlan/Amplifriend", "stars": 10, "license": "None", "language": "python", "size": 1835}
{"docstring": "\"\"\"Decorator to memoize functions using memcache.\"\"\"\n", "func_signal": "def memoize(keyformat, time=60):\n", "code": "def decorator(fxn):\n\tdef wrapper(*args, **kwargs):\n\t\t\n\t\tkey = (keyformat % args[0:keyformat.count('%')]) + str(kwargs)\n\t\t\n\t\t#logging.info(\"MEMO KEY %s\" % key)\n\t\t\n\t\tm = md5.new()\n\t\tm.update(key)\n\t\t\n\t\tkey_digest = m.hexdigest()\n\t\t\n\t\tdata = memcache.get(key_digest)\n\t\tif data is not None:\n\t\t\t#logging.info(\"Cache Hit for key %s\" % key_digest)\n\t\t\treturn data\n\t\t\n\t\t#logging.info(\"Cache Miss for key %s\" % key_digest)\n\t\t\t\n\t\tdata = fxn(*args, **kwargs)\n\t\tmemcache.set(key_digest, data, time)\n\t\treturn data\n\treturn wrapper\nreturn decorator", "path": "utils.py", "repo_name": "PaulKinlan/Amplifriend", "stars": 10, "license": "None", "language": "python", "size": 1835}
{"docstring": "\"\"\"Returns the request headers that should be used to pull this feed.\n\nReturns:\n\tDictionary of request header values.\n\"\"\"\n", "func_signal": "def get_request_headers(self):\n", "code": "headers = {\n\t'Cache-Control': 'no-cache no-store max-age=1',\n\t'Connection': 'cache-control',\n}\nif self.last_modified:\n\theaders['If-Modified-Since'] = self.last_modified\nif self.etag:\n\theaders['If-None-Match'] = self.etag\nreturn headers", "path": "hub\\hubmodel.py", "repo_name": "PaulKinlan/Amplifriend", "stars": 10, "license": "None", "language": "python", "size": 1835}
{"docstring": "\"\"\"\nDeletes the specified service\n\"\"\"\n", "func_signal": "def post(self):\n", "code": "service = self.request.get(\"service\")\n\nservice_instance = model.Service.GetById(service)\n\nservice_instance.delete()\n\nself.redirect(self.request.headers['referer'])", "path": "service\\DeleteServiceHandler.py", "repo_name": "PaulKinlan/Amplifriend", "stars": 10, "license": "None", "language": "python", "size": 1835}
{"docstring": "''' Built-in multi-selection listbox '''\n", "func_signal": "def menu_list1():\n", "code": "items = appuifw.multi_selection_list(my_list, style='checkbox', search_field=1)\n# Show what happened\nappuifw.note(u\"Selected: %s\" % unicode(str(items)))", "path": "Code\\Python\\code\\t_mslist.py", "repo_name": "miclovich/EPROM", "stars": 10, "license": "None", "language": "python", "size": 47609}
{"docstring": "# create a list to be used in 'combo' selection mode\n", "func_signal": "def forming():\n", "code": "model = [u'6600', u'6630', u'7610', u'N90', u'N70']\n\n# define the field list (consists of tuples: (label, type ,value)); label is a unicode string\n# type is one of the following strings: 'text', 'number', 'date', 'time',or 'combo'\ndata = [(u'Mobile','text', u'Nokia'),(u'Model','combo', (model,0)),(u'Amount','number', 5),(u'Date','date'),(u'Time','time')]\n\n# set the view/edit mode of the form  \nflags = appuifw.FFormEditModeOnly\n\n# creates the form\nf = appuifw.Form(data, flags)\n\n# make the form visible on the UI\nf.execute()", "path": "Code\\Python\\code\\form.py", "repo_name": "miclovich/EPROM", "stars": 10, "license": "None", "language": "python", "size": 47609}
{"docstring": "''' Closing listbox, clean up and bring back main view '''\n# Restore Main view\n", "func_signal": "def cb_return():\n", "code": "global appuifw\nappuifw.app.body = g_t\nappuifw.app.menu = self_options[0]\nappuifw.app.exit_key_handler = cb_quit\n\n# Save selections from listbox\nglobal my_items\nmy_items = ()\nfor index in range(len(e3)):\n    if e3[index][1] == icon_on:\n        my_items += (index, )\n\n# Show what happened\nappuifw.note(u\"Selected: %s\" % unicode(str(my_items)))", "path": "Code\\Python\\code\\t_mslist.py", "repo_name": "miclovich/EPROM", "stars": 10, "license": "None", "language": "python", "size": 47609}
{"docstring": "''' More paparazzis and faster '''\n", "func_signal": "def update_interval():\n", "code": "timer_papa.cancel()\n# Add new paparazzis\nglobal my_interval\nif my_interval > 0.05:\n    my_interval -= 0.05\n\n# BUG: should verify star and (x,y) are not overlapping\nx = random.choice(range(screen_x))\ny = random.choice(range(screen_y))\ng_papa.append((x, y))\nx = random.choice(range(screen_x))\ny = random.choice(range(screen_y))\ng_papa.append((x, y))\nglobal g_papa_count\ng_papa_count += 2\n\ntimer_papa.after(my_interval, update_papa)\ntimer_interval.after(1, update_interval)\ne32.reset_inactivity()", "path": "Code\\Python\\code\\parapazzi\\paparazzi.py", "repo_name": "miclovich/EPROM", "stars": 10, "license": "None", "language": "python", "size": 47609}
{"docstring": "#print \"Sending data...\"\n", "func_signal": "def senddata(self,data):\n", "code": "self.log(\"Content-Length: %d\"%len(data)+\"\\n\")\nself.write('Content-Length: '+str(len(data))+'\\n'+\n           'CRC32: '+str(binascii.crc32(data))+'\\n')\nsentbytes=0        \n# Send the data in little bits because the Bluetooth serial\n# connection may lose data on large sends.\nMAX_SEND=2048\nwhile sentbytes<len(data):\n    n=min(len(data)-sentbytes,MAX_SEND)\n    self.write(data[sentbytes:sentbytes+n])\n    sentbytes+=n\n    self.log(\"Sent: %d bytes (%3.1f%%)\\r\"%(sentbytes,(100.*sentbytes/len(data))))\n    sys.stdout.flush()\nself.log(\"Sent: %d bytes.            \"%(sentbytes)+\"\\n\")", "path": "Code\\J2ME\\Vidya Lectures\\Lecture2\\Examples\\fileserver\\fileclient.py", "repo_name": "miclovich/EPROM", "stars": 10, "license": "None", "language": "python", "size": 47609}
{"docstring": "''' Custom multi-selection listbox '''\n", "func_signal": "def menu_list3():\n", "code": "global e3, lb3\ne3 = []\n\n# Mark initial selections\nfor item in range(len(my_list)):\n    if item in my_items:\n        icon = get_checkbox(True)\n    else:\n        icon = get_checkbox(False)\n    e3.append((my_list[item], icon))\n\nglobal appuifw\nlb3 = appuifw.Listbox(e3, cb_select)\nappuifw.app.body = lb3\nappuifw.app.menu = self_options[1]\nappuifw.app.exit_key_handler = cb_return\n\n# Several ways to select item\n# BUG: One press of Enter/Select gives two events: KeyDown and KeyUp\n# BUG: ...or even 3+ with key repeat\n# Fix: write own key handler, react only on KeyUp event\n# Fix: use timer to separate \"one\" key press\nlb3.bind(key_codes.EKeyRightArrow, cb_select)\nlb3.bind(key_codes.EKeyEnter, cb_select)\nlb3.bind(key_codes.EKeySelect, cb_select)", "path": "Code\\Python\\code\\t_mslist.py", "repo_name": "miclovich/EPROM", "stars": 10, "license": "None", "language": "python", "size": 47609}
{"docstring": "''' Move paparazzis closer '''\n", "func_signal": "def update_papa():\n", "code": "for i in range(g_papa_count):\n    x, y = g_papa[i]\n    dx = max(5, abs(star_x - x))\n    dy = max(5, abs(star_y - y))\n    if x < star_x:\n        x = int(x + dx/5)\n    elif x > star_x:\n        x = int(x - dx/5)\n    if y < star_y:\n        y = int(y + dy/5)\n    elif y > star_y:\n        y = int(y - dy/5)\n    g_papa[i] = (x, y)", "path": "Code\\Python\\code\\parapazzi\\paparazzi.py", "repo_name": "miclovich/EPROM", "stars": 10, "license": "None", "language": "python", "size": 47609}
{"docstring": "''' Callback for menu item About '''\n", "func_signal": "def menu_about():\n", "code": "appuifw.note(u'Multi-selection List Test v' + VERSION + u'\\n' +\\\n    u'jouni.miettunen.googlepages.com\\n\\u00a92009 Jouni Miettunen')", "path": "Code\\Python\\code\\t_mslist.py", "repo_name": "miclovich/EPROM", "stars": 10, "license": "None", "language": "python", "size": 47609}
{"docstring": "#self.log(\"Waiting for content length...\"\n", "func_signal": "def recvdata(self):\n", "code": "header_lines=[self.readline() for x in range(2)]\nheader=dict([x.rstrip().split(': ') for x in header_lines])\ncontent_length=int(header['Content-Length'])\ncrc32=int(header['CRC32'])\nself.log(\"Content-Length: %d\"%content_length+\"\\n\")\nrecvbytes=0\ncontent=[]\n#self.log(\"Receiving data...\")\nwhile recvbytes<content_length:\n    recvstring=self.sock.recv(min(content_length-recvbytes,2048))\n    recvbytes+=len(recvstring)\n    self.log(\"Received: %d bytes (%3.1f%%)\\r\"%(recvbytes,(100.*recvbytes/content_length)))\n    sys.stdout.flush()\n    content.append(recvstring)\nself.log(\"Received: %d bytes.        \"%(recvbytes)+\"\\n\")\ncontent=''.join(content)\nif crc32 != binascii.crc32(content):\n    raise IOError(\"CRC error while receiving data\")\nreturn content", "path": "Code\\J2ME\\Vidya Lectures\\Lecture2\\Examples\\fileserver\\fileclient.py", "repo_name": "miclovich/EPROM", "stars": 10, "license": "None", "language": "python", "size": 47609}
{"docstring": "''' Callback for menu item About '''\n", "func_signal": "def menu_about():\n", "code": "appuifw.note(u'Paparazzi v'+VERSION+u'\\n'+\\\n    u'jouni.miettunen.googlepages.com\\n\\u00a9 2008-2009 Jouni Miettunen')", "path": "Code\\Python\\code\\parapazzi\\paparazzi.py", "repo_name": "miclovich/EPROM", "stars": 10, "license": "None", "language": "python", "size": 47609}
{"docstring": "''' Init new game '''\n", "func_signal": "def game_init():\n", "code": "global g_papa_count, star_x, star_y, g_papa\n\ntimer_interval.cancel()\ntimer_papa.cancel()\n\nstar_x = random.choice(range(screen_x))\nstar_y = random.choice(range(screen_y))\n# BUG: should verify star and ab are not overlapping\nx = random.choice(range(screen_x))\ny = random.choice(range(screen_y))\ng_papa_count = 1\ng_papa = []\ng_papa.append((x, y))\n\nglobal g_time, my_interval\ng_time = time.clock()\nmy_interval = 0.5\nif SENSOR_ACC:\n    sensor_acc.connect(handle_sensor_data)\ntimer_interval.after(1, update_interval)\ntimer_papa.after(my_interval, update_papa)\ndraw_frame()", "path": "Code\\Python\\code\\parapazzi\\paparazzi.py", "repo_name": "miclovich/EPROM", "stars": 10, "license": "None", "language": "python", "size": 47609}
{"docstring": "''' Checkbox icon: selected or not '''\n", "func_signal": "def get_checkbox(a_value):\n", "code": "global icon_on, icon_off\n# Create only once, reuse after that\nif not icon_on:\n    # See avkon2.mbm content (old version)\n    # http://alindh.iki.fi/symbian/avkon2.mbm/\n\n    try:\n        # webkit checkbox looks better, but might not exist\n        icon_off = appuifw.Icon(u\"z:\\\\resource\\\\apps\\\\webkit.mbm\", 12, 31)\n        icon_on = appuifw.Icon(u\"z:\\\\resource\\\\apps\\\\webkit.mbm\", 13, 32)\n    except:\n        # Counting on avkon2 to be there, hopefully with checkbox\n        icon_off = appuifw.Icon(u\"z:\\\\resource\\\\apps\\\\avkon2.mbm\", 103, 104)\n        icon_on = appuifw.Icon(u\"z:\\\\resource\\\\apps\\\\avkon2.mbm\", 109, 110)\n\nif a_value:\n    return icon_on\nelse:\n    return icon_off", "path": "Code\\Python\\code\\t_mslist.py", "repo_name": "miclovich/EPROM", "stars": 10, "license": "None", "language": "python", "size": 47609}
{"docstring": "''' Overwrite default screen redraw event handler '''\n", "func_signal": "def cb_handle_redraw(dummy=(0, 0, 0, 0)):\n", "code": "if img:\n    canvas.blit(img)", "path": "Code\\Python\\code\\parapazzi\\paparazzi.py", "repo_name": "miclovich/EPROM", "stars": 10, "license": "None", "language": "python", "size": 47609}
{"docstring": "# clear screen to black\n", "func_signal": "def refresh(rect):\n", "code": "c.clear(0)\n# draw the photo there\nc.blit(photo)\n# print some explanatory white text\nc.text((10,10),u'Select to take picture', fill=0xffffff) \nc.text((10,20),u'Left/Right to rotate', fill=0xffffff) \nc.text((10,30),u'Down to save', fill=0xffffff) \nc.text((10,40),u'Up to load', fill=0xffffff)", "path": "Code\\Python\\code\\imgage_rotation_descr.py", "repo_name": "miclovich/EPROM", "stars": 10, "license": "None", "language": "python", "size": 47609}
{"docstring": "''' Draw everything on-screen '''\n", "func_signal": "def draw_frame():\n", "code": "img.clear(RGB_BLACK)\nglobal star_x, star_y\n\n# Paparazzis\ndone = False\nif pic:\n    limit = 8\nelse:\n    limit = 5\nfor i in range(g_papa_count):\n    x, y = g_papa[i]\n    if pic:\n        img.blit(pic, target=(x, y), source=(0,2,15,13))\n    else:\n        img.point((x, y), width=limit, outline=RGB_RED)\n    if abs(star_x-x) < limit and abs(star_y-y) < limit:\n        done = True\n\n# Movie star (on top)\nif pic:\n    star_x = min(screen_x-16, star_x)\n    star_y = min(screen_y-16, star_y)\n    if done:\n        img.point((star_x+7, star_y+7), width=20, outline=RGB_RED)\n    img.blit(pic, target=(star_x, star_y), source=(16,0,31,15), mask=pic_mask)\nelse:\n    if done:\n        img.point((star_x, star_y), width=20, outline=RGB_RED)\n    img.point((star_x, star_y), width=9, outline=RGB_YELLOW)\n\n# Running time\nimg.text((10, 25), unicode(str(\"%4.2f\" % (time.clock() - g_time))), RGB_WHITE, 'normal')\n\ncb_handle_redraw()\nif done:\n    if SENSOR_ACC:\n        sensor_acc.disconnect()\n    timer_interval.cancel()\n    timer_papa.cancel()\n    appuifw.note(u'Flash! Paparazzis got you! Flash! Smile! Zap! Comments? Ping!')", "path": "Code\\Python\\code\\parapazzi\\paparazzi.py", "repo_name": "miclovich/EPROM", "stars": 10, "license": "None", "language": "python", "size": 47609}
{"docstring": "''' Callback for listbox item selection event '''\n\n### keypress handler trick, to allow Enter/Select work\n", "func_signal": "def cb_select():\n", "code": "global g_time\nmy_timer.cancel()\n# Ignore first and non-timer trickered events\nif (not g_time) or (time.clock() - g_time < 0.1):\n    g_time = time.clock()\n    # Should be more than start keyrepeat rate\n    my_timer.after(0.15, cb_select)\n    return\ng_time = 0\n### keypress handler trick, done\n\n# Current listbox selection\nindex = lb3.current()\n\n# Change selected item icon: on <-> off\nif e3[index][1] == icon_on:\n    new_icon = icon_off\nelse:\n    new_icon = icon_on\n\ne3[index] = (e3[index][0], new_icon)\n\n# Show new list, same item selected\nlb3.set_list(e3, index)\nappuifw.app.body = lb3\n\n# Make it visible\ne32.ao_yield()", "path": "Code\\Python\\code\\t_mslist.py", "repo_name": "miclovich/EPROM", "stars": 10, "license": "None", "language": "python", "size": 47609}
{"docstring": "''' Cleanup before exit '''\n# ALWAYS cancel your timers before exit.\n", "func_signal": "def cb_quit():\n", "code": "my_timer.cancel()\n\n# User might NOT have initialized lb3 in custom listbox\n# Crash when unbinding non-binded keys\nglobal e3, lb3, icon_on, icon_off\ntry:\n    lb3.bind(key_codes.EKeyRightArrow, None)\n    lb3.bind(key_codes.EKeyEnter, None)\n    lb3.bind(key_codes.EKeySelect, None)\nexcept:\n    pass\n\n# Help system clean-up, delete stuff by yourself\ne3 = []\ndel lb3\ndel icon_on\ndel icon_off\n\n# Done, continue exit\napp_lock.signal()", "path": "Code\\Python\\code\\t_mslist.py", "repo_name": "miclovich/EPROM", "stars": 10, "license": "None", "language": "python", "size": 47609}
{"docstring": "''' Built-in multi-selection listbox '''\n", "func_signal": "def menu_list2():\n", "code": "items = appuifw.multi_selection_list(my_list, style='checkmark')\n# Show what happened\nappuifw.note(u\"Selected: %s\" % unicode(str(items)))", "path": "Code\\Python\\code\\t_mslist.py", "repo_name": "miclovich/EPROM", "stars": 10, "license": "None", "language": "python", "size": 47609}
{"docstring": "''' Raw data as absolute location '''\n", "func_signal": "def handle_sensor_data(a_data):\n", "code": "d1 = a_data['data_1']   # x-axis\nd2 = a_data['data_2']   # y-axis\nglobal star_x, star_y\nd1 = max(-100, d1)\nd1 = min(100, d1)\nd2 = max(-100, d2)\nd2 = min(100, d2)\nstar_x = screen_x - ((d2 + 100) * screen_x) / 200\nstar_y = screen_y - ((d1 + 100) * screen_y) / 200\ndraw_frame()", "path": "Code\\Python\\code\\parapazzi\\paparazzi.py", "repo_name": "miclovich/EPROM", "stars": 10, "license": "None", "language": "python", "size": 47609}
{"docstring": "''' Clean-up before exit '''\n", "func_signal": "def cb_quit():\n", "code": "if SENSOR_ACC:\n    sensor_acc.disconnect()\ntimer_papa.cancel()\ntimer_interval.cancel()\napp_lock.signal()", "path": "Code\\Python\\code\\parapazzi\\paparazzi.py", "repo_name": "miclovich/EPROM", "stars": 10, "license": "None", "language": "python", "size": 47609}
{"docstring": "\"\"\" Tests whether user action was performed on time \n\nRequired arguments are:\n\n* ``type``: type of interaction user was supposed to perform\n* ``deadline``: the original deadline in seconds\n\n``type`` value should be one of the types used for ``set_interaction``\nmethod.\n\nIf type does not match the type of outstanding interaction,\n``UserInteractionError`` is raised.\n\nIf use met the deadline, ``True`` is returned, and ``False`` is\nreturned otherwise. Deadline is counted from the time action was\nregistered using ``set_interaction`` or any of its wrappers.\n\n\"\"\"\n\n", "func_signal": "def is_interaction_timely(self, type, deadline):\n", "code": "if self._act_type is None:\n    raise UserInteractionError(\"There is no registered action.\")\n\nif not type[:1] == self._act_type:\n    raise UserInteractionError(\"Action '%s' is not registered.\" % type)\n\nnow_time = datetime.datetime.now()\ndeadline_time = self._act_time + datetime.timedelta(seconds=deadline)\n\nreturn now_time < deadline_time", "path": "authenticationpy\\auth.py", "repo_name": "foxbunny/acl.py", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 290}
{"docstring": "\"\"\" Stores a user account \"\"\"\n", "func_signal": "def store(self):\n", "code": "if self._dirty_fields:\n    # Destroy the cache in an unlikely event that the username or\n    # email matches the cached username or email respectively (this\n    # happens in tests).\n    if web.ctx.auth_user_cache.get('username') == self.username or \\\n       web.ctx.auth_user_cache.get('email') == self.email:\n        web.ctx.auth_user_cache = {}\n\n    transaction = db.transaction()\n    try:\n        if self._new_account:\n            if not self.password:\n                raise UserAccountError('Password cannot be blank.')\n            db.insert(TABLE, **self._data_to_insert)\n            record = db.where(TABLE, what='id',\n                              limit=1,\n                              username=self.username)[0]\n            self._account_id = record.id\n        else:\n            db.update(TABLE, where='id = $id',\n                      vars={'id': self._account_id},\n                      **self._data_to_store)\n    except:\n        transaction.rollback()\n        raise\n    else:\n        transaction.commit()\n\n# nothing to store\npass", "path": "authenticationpy\\auth.py", "repo_name": "foxbunny/acl.py", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 290}
{"docstring": "\"\"\" Encrypts the ``cleartext`` password and returns it \"\"\"\n\n# TODO: maybe find a better salt generation code, or use longer salt\n", "func_signal": "def _encrypt_password(username, cleartext):\n", "code": "salt = ''.join([random.choice('abcdefghijklmnopqrstuvwxyz0123456789') for i in range(16)])\nhexdigest = _password_hexdigest(username, salt, cleartext) \nreturn '%s$%s' % (salt, hexdigest)", "path": "authenticationpy\\auth.py", "repo_name": "foxbunny/acl.py", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 290}
{"docstring": "\"\"\" Returns a dictionary of dirty field names and values \"\"\"\n", "func_signal": "def _data_to_store(self):\n", "code": "store_dict = {}\nfor field in self._dirty_fields:\n    store_dict[field[1]] = self.__dict__[field[0]]\nreturn store_dict", "path": "authenticationpy\\auth.py", "repo_name": "foxbunny/acl.py", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 290}
{"docstring": "\"\"\" Test ``password`` and return boolean success status \"\"\"\n", "func_signal": "def authenticate(self, password):\n", "code": "if not self.active:\n    raise UserAccountError('Cannot authenticate inactive account')\nsalt, crypt = self.password.split('$')\nreturn _password_hexdigest(self.username,\n                           salt,\n                           password) == crypt", "path": "authenticationpy\\auth.py", "repo_name": "foxbunny/acl.py", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 290}
{"docstring": "# create table for User object\n", "func_signal": "def setup_table():\n", "code": "database.query(\"\"\"\n               DROP TABLE IF EXISTS authenticationpy_users CASCADE;\n               CREATE TABLE authenticationpy_users (\n                 id               SERIAL PRIMARY KEY,\n                 username         VARCHAR(40) NOT NULL UNIQUE,\n                 email            VARCHAR(80) NOT NULL UNIQUE,\n                 password         CHAR(81) NOT NULL,\n                 pending_pwd      CHAR(81),\n                 act_code         CHAR(64),\n                 act_time         TIMESTAMP,\n                 act_type         CHAR(1),\n                 registered_at    TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                 active           BOOLEAN DEFAULT 'false' \n               );\n               CREATE UNIQUE INDEX username_index ON authenticationpy_users\n               USING btree (username);\n               CREATE UNIQUE INDEX email_index ON authenticationpy_users\n               USING btree (username);\n               \"\"\")", "path": "authenticationpy\\test_auth.py", "repo_name": "foxbunny/acl.py", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 290}
{"docstring": "\"\"\" Get user from the database and return ``User`` instance\n\nFinds a user account that matches either or both of the optional\narguments:\n\n* ``username``: username of the account\n* ``email``: user's e-mail address\n\nIf neither of the arguments are supplied, ``UserAccountError`` is\nraised. If no match is found, ``None`` is returned.\n\nReturned accounts always have ``_new_account`` property set to False,\nbut this is meant for internal use only. Nevertheless, if you need to\nknow if the account is a new (unsaved) one, you can access this\nproperty.\n\nA successful query returns a ``User`` instance.\n\n\"\"\"\n\n", "func_signal": "def get_user(cls, username=None, email=None):\n", "code": "if username is None and email is None:\n    raise UserAccountError('No user account information to look for')\n\nselect_dict = {}\nif username:\n    if not cls._validate_username(username):\n        raise ValueError(\"'%s' does not look like a valid username\" % username)\n    if web.ctx.auth_user_cache.get('username') == username:\n        return web.ctx.auth_user_cache['object']\n    select_dict['username'] = username\nif email:\n    if not cls._validate_email(email):\n        raise ValueError(\"'%s' does not look like a valid e-mail\" % email)\n    if web.ctx.auth_user_cache.get('email') == email:\n        return web.ctx.auth_user_cache['object']\n    select_dict['email'] = email\n\nrecords = db.where(TABLE, **select_dict)\n\nif not records:\n    # There is nothing to return\n    web.ctx.auth_user_cache = {}\n    return None\n\nreturn cls._cache_and_return(records[0])", "path": "authenticationpy\\auth.py", "repo_name": "foxbunny/acl.py", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 290}
{"docstring": "\"\"\" Send an arbitrary e-mail message to the user \n\nRequired argument for this method are:\n\n* ``message``: the body of the e-mail\n* ``subject``: e-mail's subject\n\n``sender`` argument is optional, and it defaults to\n``web.config.authmail['sender']``, which is usually the e-mail address\nof your site.\n\nOptionally, you can use ``kwargs`` to set template variables. The\ntemplate variables follow the ``$varname`` pattern used by Python's\nbuilt-in string formatting facilities. Any occurence of ``$varname`` in\nyour message string will be replaced by appropriate variables you\nspecify in ``kwargs``. For example::\n\n    >>> user.send_email(message='Hi, $username!', subject='Hi',\n    ...                 username='some_user')\n    # results in a message 'Hi, some_user!'\n\nIf ``kwargs`` are omitted, the default variables are provided. Those\nare:\n\n* ``$sender``: the sender's e-mail address\n* ``$username``: username of the receiving user\n* ``$email``: e-mail address of the receiving user\n\nIf for some reason, the e-mail cannot be sent (e.g, because\n``sendmail`` is not available on your system of SMTP parameters are\nincorrect, ``send_email`` will not raise any exceptions. The best way\nto make sure ``send_email`` is working is to send yourself a message.\n\nFor information on how to set up web.py's e-mail sending facilities,\nread the web.py API documentation on `web.utils module\n<http://webpy.org/docs/0.3/api#web.utils>`.\n\n\"\"\"\n", "func_signal": "def send_email(self, message, subject, sender=sender, **kwargs):\n", "code": "if not kwargs:\n    kwargs = {'sender': sender,\n              'username': self.username,\n              'email': self.email }\ntemplate = string.Template(message)\nbody = template.substitute(**kwargs)\ntry:\n    \n    web.utils.sendmail(from_address=sender,\n                       to_address=self.email,\n                       subject=subject,\n                       message=body)\nexcept OSError:\n    pass", "path": "authenticationpy\\auth.py", "repo_name": "foxbunny/acl.py", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 290}
{"docstring": "\"\"\" Generate interaction code for use as a URL suffix\n\nThis method returns a tuple of the code to be stored in the database, and\nthe hexdigest to be used as the URL.\n\nThe code consists of the code generation timestamp in\n``YYYY_mm_dd_HH_MM_ssss`` format, and the hexdigest itself separated by the\ndollar sign ``$``. Hexdigest is the SHA-256 hexdigest, and it is 64\ncharacters long.\n\n\"\"\"\n\n", "func_signal": "def _generate_interaction_code(username):\n", "code": "timestamp = datetime.datetime.now()\nformatted_timestamp = timestamp.strftime('%Y%m%d%H%M%s')\nhexdigest = hashlib.sha256('%s%s' % (username, timestamp)).hexdigest()\nreturn (timestamp, hexdigest)", "path": "authenticationpy\\auth.py", "repo_name": "foxbunny/acl.py", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 290}
{"docstring": "\"\"\" Assign pending password as new \"\"\"\n", "func_signal": "def confirm_reset(self):\n", "code": "self.clear_interaction()\nobject.__setattr__(self, 'password', self._pending_pwd)\nobject.__setattr__(self, '_pending_pwd', None)\nself._dirty_fields.extend([('password', 'password'), \n                           ('_pending_pwd', 'pending_pwd')])", "path": "authenticationpy\\auth.py", "repo_name": "foxbunny/acl.py", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 290}
{"docstring": "\"\"\" Sets interaction data\n\nThe only required argument is ``type``, which must be one of the\nfollowing:\n\n* 'activate' ('a' for short)\n* 'delete' ('d' for short)\n* 'reset' ('r' for short)\n\nIf the ``type`` doesn't match any of the allowed values, ``ValueError``\nis raised.\n\nThis method registers the type of the interaction, as well as the time\nof registration, and action code, and returns the activation code. \nThose pieces of information are stored in ``_act_type``, ``_act_time``, \nand ``_act_code`` properties respectively.\n\n\"\"\"\n\n", "func_signal": "def set_interaction(self, type):\n", "code": "if not type in ['activate', 'delete', 'reset', 'a', 'd', 'r']:\n    raise ValueError(\"Interaction type must be 'activate', 'delete', or 'reset'.\")\nself._act_time, self._act_code = _generate_interaction_code(self.username)\nself._act_type = type[:1]\nreturn self._act_code", "path": "authenticationpy\\auth.py", "repo_name": "foxbunny/acl.py", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 290}
{"docstring": "\"\"\" Resets the user password \n\n``password`` argument is optional and it is the new user password to be\nsaved. If no password is supplied, a random password will be generated.\n\nIf you specify the ``message``, an e-mail is sent to the user's e-mail\naddress. The ``message`` is a string, and it may contain template\nvariables in ``$varname`` form. Following variables are available:\n\n* ``$username``: username of the user to be created\n* ``$email``: user's e-mail address\n* ``$password``: user's clear text password\n* ``$url``: confirmation url\n\nOptional argument ``confirmation`` can be used to disable setting the\npassword. When ``confirmation`` is set to ``True`` actual password is\nnot set. It is hashed and stored in a separate column, and can be set\nlater by using the ``confirm_set_pwd`` method. By default\n``confirmation`` argument is ``False``. However, if you pass the\n``message`` argument, the default changes to ``True``. You only need to\nset ``confirmation`` to ``True`` if you are not passing the ``message``\nargument.\n\n\"\"\"\n\n", "func_signal": "def reset_password(self, password=None, message=None, confirmation=None):\n", "code": "if not password:\n    password = _generate_password()\n\nif confirmation is None and message:\n    confirmation = True\n\nif confirmation:\n    self._pending_pwd = password\nelse:\n    self.password = password\n\nif message:\n    self.set_reset()\n    self.send_email(message=message,\n                    subject=rst_subject,\n                    username=self.username,\n                    email=self.email,\n                    password=self._cleartext,\n                    url=self._act_code)\n\nself.store()", "path": "authenticationpy\\auth.py", "repo_name": "foxbunny/acl.py", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 290}
{"docstring": "\"\"\" Deletes user account optionally sending an e-mail\n\nYou must supply either ``username`` or ``email`` arguments to delete a\nuser account. In case either of the required arguments are missing,\n``UserAccountError`` is raised.\n\nIf you specify a ``message`` argument, the user account is not deleted,\nbut an e-mail is sent to the user. The ``message`` template may contain\nthe following template variables in ``$varname`` form:\n\n* ``$username``: account's username\n* ``$email``: user's e-mail address\n* ``$url``: account removal confirmation URL suffix\n\nYou can use the optional ``confirmation`` argument to disable user\naccount deletion even if the ``message`` argument is missing, in cases\nwhere you have a confirmation mechanism of your own. You can also use\nthe ``confirmation`` argument to force account removal even if you\nspecify the ``message``.\n\nThe default value of ``confirmation`` argument is ``False``, but it\ndefaults to ``True`` if ``message`` argument is used.\n\n\"\"\"\n\n", "func_signal": "def delete(cls, username=None, email=None, message=None, confirmation=None):\n", "code": "if username is None and email is None:\n    raise UserAccountError('No user information for deletion.')\n       \ndelete_dict = {}\n\nif username:\n    if web.ctx.auth_user_cache.get('username') == username:\n        # Reset the chace before we delete the user\n        web.ctx.auth_user_cache = {}\n    delete_dict['username'] = username\n\nif email:\n    if web.ctx.auth_user_cache.get('email') == email:\n        # Reset the chace before we delete the user\n        web.ctx.auth_user_cache = {}\n    delete_dict['email'] = email\n\nif confirmation is None and message:\n    confirmation = True\n\nif message:\n    if username:\n        user = cls.get_user(username=username)\n    else:\n        user = cls.get_user(email=email)\n\n    user.set_delete()\n    user.send_email(message=message,\n                    subject=del_subject,\n                    username=user.username,\n                    email=user.email,\n                    url=user._act_code)\n    user.store()\n\nif not confirmation:\n    db.delete(TABLE, where=web.db.sqlwhere(delete_dict))", "path": "authenticationpy\\auth.py", "repo_name": "foxbunny/acl.py", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 290}
{"docstring": "# These properties are set directly during __init__\n", "func_signal": "def __init__(self, username, email):\n", "code": "object.__setattr__(self, 'password', None)\nobject.__setattr__(self, 'registered_at', None)\nobject.__setattr__(self, 'active', False)\nobject.__setattr__(self, '_act_code', None)\nobject.__setattr__(self, '_act_time', None)\nobject.__setattr__(self, '_act_type', None)\nobject.__setattr__(self, '_cleartext', None)\nobject.__setattr__(self, '_account_id', None)\nobject.__setattr__(self, '_dirty_fields', [])\nobject.__setattr__(self, '_pending_pwd', None)\n\nself.username = username\nself.email = email\n\n# Reset ``_dirty_fields`` so it's empty after initialization\nobject.__setattr__(self, '_dirty_fields', [])\n\n# Ref the cache object\nobject.__setattr__(self, '_cache', globals().get('web.ctx.auth_user_cache'))", "path": "authenticationpy\\auth.py", "repo_name": "foxbunny/acl.py", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 290}
{"docstring": "\"\"\" Maps user records to instance properties \"\"\"\n\n", "func_signal": "def _map_user_properties(cls, user_account):\n", "code": "try:\n    user_username = user_account.username\n    user_email = user_account.email\n    user_dict = {\n        '_account_id': user_account.id,\n        'password': user_account.password,\n        '_pending_pwd': user_account.pending_pwd,\n        '_act_code': user_account.act_code,\n        '_act_time': user_account.act_time,\n        '_act_type': user_account.act_type,\n        'registered_at': user_account.registered_at,\n        'active': user_account.active,\n    }\nexcept AttributeError:\n    raise UserAccountError('Missing data for user with id %s)' % user_account.id)\n\nuser = User(username=user_username,\n            email=user_email)\n\nfor key in user_dict.keys():\n    object.__setattr__(user, key, user_dict[key])\n\nreturn user", "path": "authenticationpy\\auth.py", "repo_name": "foxbunny/acl.py", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 290}
{"docstring": "\"\"\" Clears all interaction-related data \"\"\"\n", "func_signal": "def clear_interaction(self):\n", "code": "self._act_type = None\nself._act_time = None\nself._act_code = None", "path": "authenticationpy\\auth.py", "repo_name": "foxbunny/acl.py", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 290}
{"docstring": "\"\"\" Stores a new user optionally gerating a password \n\nThis method fails with ``UserAccountError`` if the ``User`` instance is\nmarked as an existing account.\n\nIf ``message`` argument is passed, it is treated as a e-mail message,\nand is automatically sent to user's e-mail address. The message can\ncontain template variables (in ``$varname`` form). The variables can\nbe one or more of the following:\n\n* ``$username``: username of the user to be created\n* ``$email``: user's e-mail address\n* ``$password``: user's clear text password\n* ``$usr``: activation URL\n\nActivation URL is generated only when a ``message`` argument is passed,\nand stored in database.\n\nIf ``activated`` argument is set to True, the user account will be\nactivated upon creation. Otherwise, it is *not* activated, which is the\ndefault. Note that an unactivated account can be always activated\nlater.\n\n\"\"\"\n\n", "func_signal": "def create(self, message=None, activated=False):\n", "code": "if db.where(TABLE, what='username', limit=1, username=self.username):\n    raise DuplicateUserError(\"Username '%s' already exists\" % self.username)\n\nif db.where(TABLE, what='email', limit=1, email=self.email):\n    raise DuplicateEmailError(\"Email '%s' already exists\" % self.email)\n\nif not self._new_account:\n    raise UserAccountError('Account for %s (%s) is not new' % (self.username,\n                                                               self.email))\nif not self.password:\n    self._cleartext = _generate_password()\n    self.password = self._cleartext\n\nif activated:\n    self.activate()\n\nif message:\n    self.set_activation()\n    self.send_email(message=message,\n                    subject=act_subject,\n                    username=self.username,\n                    email=self.email,\n                    password=self._cleartext,\n                    url=self._act_code)\n\nself.store()", "path": "authenticationpy\\auth.py", "repo_name": "foxbunny/acl.py", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 290}
{"docstring": "\"\"\" Gets a user account by interaction code \"\"\"\n", "func_signal": "def get_user_by_act_code(cls, act_code):\n", "code": "if not re.match(r'^[a-f0-9]{64}$', act_code):\n    raise UserAccountError('Action code is not the right format.')\n\nif web.ctx.auth_user_cache.get('act_code') == act_code:\n    return web.ctx.auth_user_cache['object']\n\nrecords = db.where(TABLE, act_code=act_code)\n\nif not records:\n    # There is nothing to return\n    web.ctx.auth_user_cache = {}\n    return None\n\nreturn cls._cache_and_return(records[0])", "path": "authenticationpy\\auth.py", "repo_name": "foxbunny/acl.py", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 290}
{"docstring": "\"\"\" Returns a dictionary of data to insert \"\"\"\n", "func_signal": "def _data_to_insert(self):\n", "code": "insert_dict = {'username': self.username,\n               'email': self.email,\n               'password': self.password,\n               'active': self.active}\nif self._act_code:\n    insert_dict['act_code'] = self._act_code\n    insert_dict['act_time'] = self._act_time\n    insert_dict['act_type'] = self._act_type\nreturn insert_dict", "path": "authenticationpy\\auth.py", "repo_name": "foxbunny/acl.py", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 290}
{"docstring": "\"\"\" Suspend a user account\n\nRequired arguments are ``username`` and ``email``. If either is\nmissing, ``UserAccountError`` will be raised.\n\nIf you specify a ``message``, an e-mail message will be sent to the\naffected account. You can include template variables in your\n``message`` text in ``$varname`` format. The available variables are:\n\n* ``$username``: account's username\n* ``$email``: user's e-mail address\n\nNote that an account will be suspended regardless of whether you pass\nthe ``message`` argument or not.\n\n\"\"\"\n\n", "func_signal": "def suspend(cls, username=None, email=None, message=None):\n", "code": "if not username and not email:\n    raise UserAccountError('No information for account suspension')\n\nsuspend_dict = {}\nif username:\n    suspend_dict['username'] = username\nif email:\n    suspend_dict['email'] = email\n\nif message:\n    if username:\n        user = cls.get_user(username=username)\n    else:\n        user = cls.get_user(email=email)\n\n    user.send_email(message=message,\n                    subject=ssp_subject,\n                    username=user.username,\n                    email=user.email)\n\n# Destroy the cache\nweb.ctx.auth_user_cache = {}\n\ndb.update(TABLE, where=web.db.sqlwhere(suspend_dict), active=False)", "path": "authenticationpy\\auth.py", "repo_name": "foxbunny/acl.py", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 290}
{"docstring": "\"\"\"Set the session ID, and return the form containing the current administrators\"\"\"\n", "func_signal": "def cmdFirstStage(self,conn,request):\n", "code": "if request.getFrom().getStripped() in config.admins:\n   # Setup session ready for form reply\n   session = self.getSessionID()\n   self.sessions[session] = {'jid':request.getFrom(),'actions':{'cancel':self.cmdCancel,'next':self.cmdSecondStage,'execute':self.cmdSecondStage}}\n   # Setup form with existing data in\n   reply = request.buildReply('result')\n   form = DataForm(title='Editing the Admin List',data=['Fill out this form to edit the list of entities who have administrative privileges', DataField(typ='hidden',name='FORM_TYPE',value=NS_ADMIN),DataField(desc='The Admin List', typ='jid-multi', name='adminjids',value=config.admins)])\n   replypayload = [Node('actions',attrs={'execute':'next'},payload=[Node('next')]),form]\n   reply.addChild(name='command',namespace=NS_COMMANDS,attrs={'node':request.getTagAttr('command','node'),'sessionid':session,'status':'executing'},payload=replypayload)\n   self._owner.send(reply)\nelse:\n   self._owner.send(Error(request,ERR_FORBIDDEN))\nraise NodeProcessed", "path": "jep0133.py", "repo_name": "xmpppy/yahoo-transport", "stars": 8, "license": "None", "language": "python", "size": 389}
{"docstring": "\"\"\"Set the session ID, and return the form containing the shutdown reason\"\"\"\n", "func_signal": "def cmdFirstStage(self,conn,request):\n", "code": "if request.getFrom().getStripped() in config.admins:\n   # Setup session ready for form reply\n   session = self.getSessionID()\n   self.sessions[session] = {'jid':request.getFrom(),'actions':{'cancel':self.cmdCancel,'next':self.cmdSecondStage,'execute':self.cmdSecondStage}}\n   # Setup form with existing data in\n   reply = request.buildReply('result')\n   form = DataForm(title='Shutting Down the Service',data=['Fill out this form to shut down the service', DataField(typ='hidden',name='FORM_TYPE',value=NS_ADMIN),DataField(desc='Announcement', typ='text-multi', name='announcement')])\n   replypayload = [Node('actions',attrs={'execute':'next'},payload=[Node('next')]),form]\n   reply.addChild(name='command',namespace=NS_COMMANDS,attrs={'node':request.getTagAttr('command','node'),'sessionid':session,'status':'executing'},payload=replypayload)\n   self._owner.send(reply)\nelse:\n   self._owner.send(Error(request,ERR_FORBIDDEN))\nraise NodeProcessed", "path": "jep0133.py", "repo_name": "xmpppy/yahoo-transport", "stars": 8, "license": "None", "language": "python", "size": 389}
{"docstring": "\"\"\"Initialise the command object\"\"\"\n", "func_signal": "def __init__(self,userfile):\n", "code": "xmpp.commands.Command_Handler_Prototype.__init__(self,config.jid)\nself.initial = { 'execute':self.cmdFirstStage }\nself.userfile = userfile", "path": "adhoc.py", "repo_name": "xmpppy/yahoo-transport", "stars": 8, "license": "None", "language": "python", "size": 389}
{"docstring": "\"\"\"Apply and save the config\"\"\"\n", "func_signal": "def cmdSecondStage(self,conn,request):\n", "code": "form = DataForm(node=request.getTag(name='command').getTag(name='x',namespace=NS_DATA))\nsession = request.getTagAttr('command','sessionid')\nif self.sessions.has_key(session):\n    if self.sessions[session]['jid'] == request.getFrom():\n        self.transport.offlinemsg = '\\n'.join(form.getField('announcement').getValues())\n        self.transport.restart = 1\n        self.transport.online = 0\n        reply = request.buildReply('result')\n        reply.addChild(name='command',namespace=NS_COMMANDS,attrs={'node':request.getTagAttr('command','node'),'sessionid':session,'status':'completed'})\n        self._owner.send(reply)\n    else:\n        self._owner.send(Error(request,ERR_BAD_REQUEST))\nelse:\n    self._owner.send(Error(request,ERR_BAD_REQUEST))   \nraise NodeProcessed", "path": "jep0133.py", "repo_name": "xmpppy/yahoo-transport", "stars": 8, "license": "None", "language": "python", "size": 389}
{"docstring": "\"\"\"Initialise the command object\"\"\"\n", "func_signal": "def __init__(self,userfile,jid=''):\n", "code": "xmpp.commands.Command_Handler_Prototype.__init__(self,jid)\nself.initial = { 'execute':self.cmdFirstStage }\nself.userfile = userfile", "path": "jep0133.py", "repo_name": "xmpppy/yahoo-transport", "stars": 8, "license": "None", "language": "python", "size": 389}
{"docstring": "\"\"\"The handler for discovery events\"\"\"\n", "func_signal": "def _DiscoHandler(self,conn,request,type):\n", "code": "if request.getFrom().getStripped() in config.admins:\n    return xmpp.commands.Command_Handler_Prototype._DiscoHandler(self,conn,request,type)\nelse:\n    return None", "path": "jep0133.py", "repo_name": "xmpppy/yahoo-transport", "stars": 8, "license": "None", "language": "python", "size": 389}
{"docstring": "\"\"\"Initialise the command object\"\"\"\n", "func_signal": "def __init__(self,transport,jid=''):\n", "code": "xmpp.commands.Command_Handler_Prototype.__init__(self,jid)\nself.initial = {'execute':self.cmdFirstStage }\nself.transport = transport", "path": "jep0133.py", "repo_name": "xmpppy/yahoo-transport", "stars": 8, "license": "None", "language": "python", "size": 389}
{"docstring": "\"\"\"Build the reply to complete the request\"\"\"\n", "func_signal": "def cmdFirstStage(self,conn,request):\n", "code": "if request.getFrom().getStripped() in config.admins:\n    reply = request.buildReply('result')\n    form = DataForm(typ='result',data=[DataField(typ='hidden',name='FORM_TYPE',value=NS_ADMIN),DataField(desc='The list of registered users',name='registereduserjids',value=self.userfile.keys(),typ='jid-multi')])\n    reply.addChild(name='command',namespace=NS_COMMANDS,attrs={'node':request.getTagAttr('command','node'),'sessionid':self.getSessionID(),'status':'completed'},payload=[form])\n    self._owner.send(reply)\nelse:\n    self._owner.send(Error(request,ERR_FORBIDDEN))\nraise NodeProcessed", "path": "jep0133.py", "repo_name": "xmpppy/yahoo-transport", "stars": 8, "license": "None", "language": "python", "size": 389}
{"docstring": "\"\"\"Build the reply to complete the request\"\"\"\n", "func_signal": "def cmdFirstStage(self,conn,request):\n", "code": "if request.getFrom().getStripped() in config.admins:\n    for each in self.userfile.keys():\n        conn.send(Presence(to=each, frm = config.jid, typ = 'probe'))\n        if self.userfile[each].has_key('servers'):\n            for server in self.userfile[each]['servers']:\n                conn.send(Presence(to=each, frm = '%s@%s'%(server,config.jid), typ = 'probe'))\n    reply = request.buildReply('result')\n    form = DataForm(typ='result',data=[DataField(value='Command completed.',typ='fixed')])\n    reply.addChild(name='command',namespace=NS_COMMANDS,attrs={'node':request.getTagAttr('command','node'),'sessionid':self.getSessionID(),'status':'completed'},payload=[form])\n    self._owner.send(reply)\nelse:\n    self._owner.send(Error(request,ERR_FORBIDDEN))\nraise NodeProcessed", "path": "adhoc.py", "repo_name": "xmpppy/yahoo-transport", "stars": 8, "license": "None", "language": "python", "size": 389}
{"docstring": "\"\"\"The handler for discovery events\"\"\"\n", "func_signal": "def _DiscoHandler(self,conn,request,type):\n", "code": "if request.getFrom().getStripped() in config.admins:\n    return xmpp.commands.Command_Handler_Prototype._DiscoHandler(self,conn,request,type)\nelse:\n    return None", "path": "adhoc.py", "repo_name": "xmpppy/yahoo-transport", "stars": 8, "license": "None", "language": "python", "size": 389}
{"docstring": "\"\"\"The handler for discovery events\"\"\"\n", "func_signal": "def _DiscoHandler(self,conn,request,type):\n", "code": "if request.getFrom().getStripped() in config.admins:\n    return xmpp.commands.Command_Handler_Prototype._DiscoHandler(self,conn,request,type)\nelse:\n    return None", "path": "jep0133.py", "repo_name": "xmpppy/yahoo-transport", "stars": 8, "license": "None", "language": "python", "size": 389}
{"docstring": "\"\"\"Initialise the command object\"\"\"\n", "func_signal": "def __init__(self,users,jid=''):\n", "code": "xmpp.commands.Command_Handler_Prototype.__init__(self,jid)\nself.initial = { 'execute':self.cmdFirstStage }\nself.users = users", "path": "jep0133.py", "repo_name": "xmpppy/yahoo-transport", "stars": 8, "license": "None", "language": "python", "size": 389}
{"docstring": "\"\"\"Initialise the command object\"\"\"\n", "func_signal": "def __init__(self,transport,jid=''):\n", "code": "xmpp.commands.Command_Handler_Prototype.__init__(self,jid)\nself.initial = {'execute':self.cmdFirstStage }\nself.transport = transport", "path": "jep0133.py", "repo_name": "xmpppy/yahoo-transport", "stars": 8, "license": "None", "language": "python", "size": 389}
{"docstring": "\"\"\"Build the reply to complete the request\"\"\"\n", "func_signal": "def cmdFirstStage(self,conn,request):\n", "code": "if request.getFrom().getStripped() in config.admins:\n    reply = request.buildReply('result')\n    form = DataForm(typ='result',data=[DataField(typ='hidden',name='FORM_TYPE',value=NS_ADMIN),DataField(desc='The list of active users',name='activeuserjids',value=self.users.keys(),typ='jid-multi')])\n    reply.addChild(name='command',namespace=NS_COMMANDS,attrs={'node':request.getTagAttr('command','node'),'sessionid':self.getSessionID(),'status':'completed'},payload=[form])\n    self._owner.send(reply)\nelse:\n    self._owner.send(Error(request,ERR_FORBIDDEN))\nraise NodeProcessed", "path": "jep0133.py", "repo_name": "xmpppy/yahoo-transport", "stars": 8, "license": "None", "language": "python", "size": 389}
{"docstring": "\"\"\"The handler for discovery events\"\"\"\n", "func_signal": "def _DiscoHandler(self,conn,request,type):\n", "code": "if request.getFrom().getStripped() in config.admins:\n    return xmpp.commands.Command_Handler_Prototype._DiscoHandler(self,conn,request,type)\nelse:\n    return None", "path": "jep0133.py", "repo_name": "xmpppy/yahoo-transport", "stars": 8, "license": "None", "language": "python", "size": 389}
{"docstring": "\"\"\"Set the session ID, and return the form containing the restart reason\"\"\"\n", "func_signal": "def cmdFirstStage(self,conn,request):\n", "code": "if request.getFrom().getStripped() in config.admins:\n   # Setup session ready for form reply\n   session = self.getSessionID()\n   self.sessions[session] = {'jid':request.getFrom(),'actions':{'cancel':self.cmdCancel,'next':self.cmdSecondStage,'execute':self.cmdSecondStage}}\n   # Setup form with existing data in\n   reply = request.buildReply('result')\n   form = DataForm(title='Restarting the Service',data=['Fill out this form to restart the service', DataField(typ='hidden',name='FORM_TYPE',value=NS_ADMIN),DataField(desc='Announcement', typ='text-multi', name='announcement')])\n   replypayload = [Node('actions',attrs={'execute':'next'},payload=[Node('next')]),form]\n   reply.addChild(name='command',namespace=NS_COMMANDS,attrs={'node':request.getTagAttr('command','node'),'sessionid':session,'status':'executing'},payload=replypayload)\n   self._owner.send(reply)\nelse:\n   self._owner.send(Error(request,ERR_FORBIDDEN))\nraise NodeProcessed", "path": "jep0133.py", "repo_name": "xmpppy/yahoo-transport", "stars": 8, "license": "None", "language": "python", "size": 389}
{"docstring": "\"\"\"Initialise the command object\"\"\"\n", "func_signal": "def __init__(self,jid=''):\n", "code": "xmpp.commands.Command_Handler_Prototype.__init__(self,jid)\nself.initial = {'execute':self.cmdFirstStage }", "path": "jep0133.py", "repo_name": "xmpppy/yahoo-transport", "stars": 8, "license": "None", "language": "python", "size": 389}
{"docstring": "\"\"\"The handler for discovery events\"\"\"\n", "func_signal": "def _DiscoHandler(self,conn,request,type):\n", "code": "if request.getFrom().getStripped() in config.admins:\n    return xmpp.commands.Command_Handler_Prototype._DiscoHandler(self,conn,request,type)\nelse:\n    return None", "path": "jep0133.py", "repo_name": "xmpppy/yahoo-transport", "stars": 8, "license": "None", "language": "python", "size": 389}
{"docstring": "\"\"\"Apply and save the config\"\"\"\n", "func_signal": "def cmdSecondStage(self,conn,request):\n", "code": "form = DataForm(node=request.getTag(name='command').getTag(name='x',namespace=NS_DATA))\nsession = request.getTagAttr('command','sessionid')\nif self.sessions.has_key(session):\n    if self.sessions[session]['jid'] == request.getFrom():\n        self.transport.offlinemsg = '\\n'.join(form.getField('announcement').getValues())\n        self.transport.online = 0\n        reply = request.buildReply('result')\n        reply.addChild(name='command',namespace=NS_COMMANDS,attrs={'node':request.getTagAttr('command','node'),'sessionid':session,'status':'completed'})\n        self._owner.send(reply)\n    else:\n        self._owner.send(Error(request,ERR_BAD_REQUEST))\nelse:\n    self._owner.send(Error(request,ERR_BAD_REQUEST))   \nraise NodeProcessed", "path": "jep0133.py", "repo_name": "xmpppy/yahoo-transport", "stars": 8, "license": "None", "language": "python", "size": 389}
{"docstring": "\"\"\"Build the reply to complete the request\"\"\"\n", "func_signal": "def cmdFirstStage(self,conn,request):\n", "code": "if request.getFrom().getStripped() in config.admins:\n    reply = request.buildReply('result')\n    form = DataForm(typ='result',data=[DataField(typ='hidden',name='FORM_TYPE',value=NS_ADMIN),DataField(desc='The list of online users',name='onlineuserjids',value=self.users.keys(),typ='jid-multi')])\n    reply.addChild(name='command',namespace=NS_COMMANDS,attrs={'node':request.getTagAttr('command','node'),'sessionid':self.getSessionID(),'status':'completed'},payload=[form])\n    self._owner.send(reply)\nelse:\n    self._owner.send(Error(request,ERR_FORBIDDEN))\nraise NodeProcessed", "path": "jep0133.py", "repo_name": "xmpppy/yahoo-transport", "stars": 8, "license": "None", "language": "python", "size": 389}
{"docstring": "\"\"\"returns True if `address` is a valid IPv4 address\"\"\"\n", "func_signal": "def validipaddr(address):\n", "code": "try:\n    octets = address.split('.')\n    assert len(octets) == 4\n    for x in octets:\n        assert 0 <= int(x) <= 255\nexcept (AssertionError, ValueError):\n    return False\nreturn True", "path": "server\\web\\net.py", "repo_name": "Sjors/openstreetmap-route-altitude-profile", "stars": 9, "license": "None", "language": "python", "size": 220}
{"docstring": "\"\"\"Decode the 'chunked' transfer coding.\"\"\"\n", "func_signal": "def decode_chunked(self):\n", "code": "cl = 0\ndata = StringIO.StringIO()\nwhile True:\n    line = self.rfile.readline().strip().split(\";\", 1)\n    chunk_size = int(line.pop(0), 16)\n    if chunk_size <= 0:\n        break", "path": "server\\web\\wsgiserver\\__init__.py", "repo_name": "Sjors/openstreetmap-route-altitude-profile", "stars": 9, "license": "None", "language": "python", "size": 220}
{"docstring": "\"\"\"\nDeletes from `table` with clauses `where` and `using`.\n\n    >>> name = 'Joe'\n    >>> delete('foo', where='name = $name', vars=locals(), _test=True)\n    <sql: \"DELETE FROM foo WHERE name = 'Joe'\">\n\"\"\"\n", "func_signal": "def delete(table, where=None, using=None, vars=None, _test=False):\n", "code": "if vars is None: vars = {}\n\nif isinstance(where, (int, long)):\n    where = \"id = \" + sqlquote(where)\nelif isinstance(where, (list, tuple)) and len(where) == 2:\n    where = SQLQuery(where[0], where[1])\nelif isinstance(where, SQLQuery):\n    pass\nelif where is None:\n    pass\nelse:\n    where = reparam(where, vars)\n\nq = 'DELETE FROM ' + table\nif where:\n    q += ' WHERE ' + where\nif using and web.ctx.get('db_name') != \"firebird\":\n    q += ' USING ' + sqllist(using)\n\nif _test: return q\n\ndb_cursor = web.ctx.db_cursor()\nweb.ctx.db_execute(db_cursor, q)\n\nif not web.ctx.db_transaction: web.ctx.db.commit()\nreturn db_cursor.rowcount", "path": "server\\web\\db.py", "repo_name": "Sjors/openstreetmap-route-altitude-profile", "stars": 9, "license": "None", "language": "python", "size": 220}
{"docstring": "\"\"\"\nTakes a string and a dictionary and interpolates the string\nusing values from the dictionary. Returns an `SQLQuery` for the result.\n\n    >>> reparam(\"s = $s\", dict(s=True))\n    <sql: \"s = 't'\">\n\"\"\"\n# making a copy of dictionary because eval mangles it\n", "func_signal": "def reparam(string_, dictionary):\n", "code": "dictionary = dictionary.copy()\nvals = []\nresult = []\nfor live, chunk in _interpolate(string_):\n    if live:\n        result.append(aparam())\n        vals.append(eval(chunk, dictionary))\n    else: result.append(chunk)\nreturn SQLQuery(''.join(result), vals)", "path": "server\\web\\db.py", "repo_name": "Sjors/openstreetmap-route-altitude-profile", "stars": 9, "license": "None", "language": "python", "size": 220}
{"docstring": "\"\"\"Write a simple response back to the client.\"\"\"\n", "func_signal": "def simple_response(self, status, msg=\"\"):\n", "code": "status = str(status)\nbuf = [\"%s %s\\r\\n\" % (self.connection.server.protocol, status),\n       \"Content-Length: %s\\r\\n\" % len(msg)]\n\nif status[:3] == \"413\" and self.response_protocol == 'HTTP/1.1':\n    # Request Entity Too Large\n    self.close_connection = True\n    buf.append(\"Connection: close\\r\\n\")\n\nbuf.append(\"\\r\\n\")\nif msg:\n    buf.append(msg)\nself.sendall(\"\".join(buf))", "path": "server\\web\\wsgiserver\\__init__.py", "repo_name": "Sjors/openstreetmap-route-altitude-profile", "stars": 9, "license": "None", "language": "python", "size": 220}
{"docstring": "\"\"\"returns True if `port` is a valid IPv4 port\"\"\"\n", "func_signal": "def validipport(port):\n", "code": "try:\n    assert 0 <= int(port) <= 65535\nexcept (AssertionError, ValueError):\n    return False\nreturn True", "path": "server\\web\\net.py", "repo_name": "Sjors/openstreetmap-route-altitude-profile", "stars": 9, "license": "None", "language": "python", "size": 220}
{"docstring": "\"\"\"\nQuotes a string for use in a URL.\n\n    >>> urlquote('://?f=1&j=1')\n    '%3A//%3Ff%3D1%26j%3D1'\n    >>> urlquote(None)\n    ''\n    >>> urlquote(u'\\u203d')\n    '%E2%80%BD'\n\"\"\"\n", "func_signal": "def urlquote(val):\n", "code": "if val is None: return ''\nif not isinstance(val, unicode): val = str(val)\nelse: val = val.encode('utf-8')\nreturn urllib.quote(val)", "path": "server\\web\\net.py", "repo_name": "Sjors/openstreetmap-route-altitude-profile", "stars": 9, "license": "None", "language": "python", "size": 220}
{"docstring": "# Determine whether the post request is a protocol buffer or\n# an XML document. (I am sure there is a more elegant way)\n", "func_signal": "def POST(self, output_format, input_format):\n", "code": "try: \n  postdata =  web.input()['protobuf']\nexcept:\n  try:\n    postdata = '<?xml version=' + web.input()['<?xml version']\n  except:\n    web.header('Content-Type', 'html/txt')\n    web.output(\"Wrong input format\")\n    #web.internalerror() \n    return\n    \nres = altitude.page_profile(db, utils, postdata, output_format, input_format)\nheader = res[0]\nbody = res[1]\nweb.header('Content-Type', header)\nweb.output(body)", "path": "server\\apache.py", "repo_name": "Sjors/openstreetmap-route-altitude-profile", "stars": 9, "license": "None", "language": "python", "size": 220}
{"docstring": "\"\"\"Call the appropriate WSGI app and write its iterable output.\"\"\"\n", "func_signal": "def respond(self):\n", "code": "response = self.wsgi_app(self.environ, self.start_response)\ntry:\n    for chunk in response:\n        # \"The start_response callable must not actually transmit\n        # the response headers. Instead, it must store them for the\n        # server or gateway to transmit only after the first\n        # iteration of the application return value that yields\n        # a NON-EMPTY string, or upon the application's first\n        # invocation of the write() callable.\" (PEP 333)\n        if chunk:\n            self.write(chunk)\nfinally:\n    if hasattr(response, \"close\"):\n        response.close()\nif (self.ready and not self.sent_headers\n        and not self.connection.server.interrupt):\n    self.sent_headers = True\n    self.send_headers()\nif self.chunked_write:\n    self.sendall(\"0\\r\\n\\r\\n\")", "path": "server\\web\\wsgiserver\\__init__.py", "repo_name": "Sjors/openstreetmap-route-altitude-profile", "stars": 9, "license": "None", "language": "python", "size": 220}
{"docstring": "\"\"\"\nTakes a format string and returns a list of 2-tuples of the form\n(boolean, string) where boolean says whether string should be evaled\nor not.\n\nfrom <http://lfw.org/python/Itpl.py> (public domain, Ka-Ping Yee)\n\"\"\"\n", "func_signal": "def _interpolate(format):\n", "code": "from tokenize import tokenprog\n\ndef matchorfail(text, pos):\n    match = tokenprog.match(text, pos)\n    if match is None:\n        raise _ItplError(text, pos)\n    return match, match.end()\n\nnamechars = \"abcdefghijklmnopqrstuvwxyz\" \\\n    \"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_\";\nchunks = []\npos = 0\n\nwhile 1:\n    dollar = format.find(\"$\", pos)\n    if dollar < 0: \n        break\n    nextchar = format[dollar + 1]\n\n    if nextchar == \"{\":\n        chunks.append((0, format[pos:dollar]))\n        pos, level = dollar + 2, 1\n        while level:\n            match, pos = matchorfail(format, pos)\n            tstart, tend = match.regs[3]\n            token = format[tstart:tend]\n            if token == \"{\": \n                level = level + 1\n            elif token == \"}\":  \n                level = level - 1\n        chunks.append((1, format[dollar + 2:pos - 1]))\n\n    elif nextchar in namechars:\n        chunks.append((0, format[pos:dollar]))\n        match, pos = matchorfail(format, dollar + 1)\n        while pos < len(format):\n            if format[pos] == \".\" and \\\n                pos + 1 < len(format) and format[pos + 1] in namechars:\n                match, pos = matchorfail(format, pos + 1)\n            elif format[pos] in \"([\":\n                pos, level = pos + 1, 1\n                while level:\n                    match, pos = matchorfail(format, pos)\n                    tstart, tend = match.regs[3]\n                    token = format[tstart:tend]\n                    if token[0] in \"([\": \n                        level = level + 1\n                    elif token[0] in \")]\":  \n                        level = level - 1\n            else: \n                break\n        chunks.append((1, format[dollar + 1:pos]))\n\n    else:\n        chunks.append((0, format[pos:dollar + 1]))\n        pos = dollar + 1 + (nextchar == \"$\")\n\nif pos < len(format): \n    chunks.append((0, format[pos:]))\nreturn chunks", "path": "server\\web\\db.py", "repo_name": "Sjors/openstreetmap-route-altitude-profile", "stars": 9, "license": "None", "language": "python", "size": 220}
{"docstring": "\"\"\"Read header lines from the incoming stream.\"\"\"\n", "func_signal": "def read_headers(self):\n", "code": "environ = self.environ\n\nwhile True:\n    line = self.rfile.readline()\n    if not line:\n        # No more data--illegal end of headers\n        raise ValueError(\"Illegal end of headers.\")\n\n    if line == '\\r\\n':\n        # Normal end of headers\n        break\n\n    if line[0] in ' \\t':\n        # It's a continuation line.\n        v = line.strip()\n    else:\n        k, v = line.split(\":\", 1)\n        k, v = k.strip().upper(), v.strip()\n        envname = \"HTTP_\" + k.replace(\"-\", \"_\")\n\n    if k in comma_separated_headers:\n        existing = environ.get(envname)\n        if existing:\n            v = \", \".join((existing, v))\n    environ[envname] = v\n\nct = environ.pop(\"HTTP_CONTENT_TYPE\", None)\nif ct:\n    environ[\"CONTENT_TYPE\"] = ct\ncl = environ.pop(\"HTTP_CONTENT_LENGTH\", None)\nif cl:\n    environ[\"CONTENT_LENGTH\"] = cl", "path": "server\\web\\wsgiserver\\__init__.py", "repo_name": "Sjors/openstreetmap-route-altitude-profile", "stars": 9, "license": "None", "language": "python", "size": 220}
{"docstring": "\"\"\"\nConverts a `dictionary` to an SQL WHERE clause `SQLQuery`.\n\n    >>> sqlwhere({'cust_id': 2, 'order_id':3})\n    <sql: 'order_id = 3 AND cust_id = 2'>\n    >>> sqlwhere({'cust_id': 2, 'order_id':3}, grouping=', ')\n    <sql: 'order_id = 3, cust_id = 2'>\n\"\"\"\n\n", "func_signal": "def sqlwhere(dictionary, grouping=' AND '):\n", "code": "return SQLQuery(grouping.join([\n  '%s = %s' % (k, aparam()) for k in dictionary.keys()\n]), dictionary.values())", "path": "server\\web\\db.py", "repo_name": "Sjors/openstreetmap-route-altitude-profile", "stars": 9, "license": "None", "language": "python", "size": 220}
{"docstring": "\"\"\"Assert, process, and send the HTTP response message-headers.\"\"\"\n", "func_signal": "def send_headers(self):\n", "code": "hkeys = [key.lower() for key, value in self.outheaders]\nstatus = int(self.status[:3])\n\nif status == 413:\n    # Request Entity Too Large. Close conn to avoid garbage.\n    self.close_connection = True\nelif \"content-length\" not in hkeys:\n    # \"All 1xx (informational), 204 (no content),\n    # and 304 (not modified) responses MUST NOT\n    # include a message-body.\" So no point chunking.\n    if status < 200 or status in (204, 205, 304):\n        pass\n    else:\n        if self.response_protocol == 'HTTP/1.1':\n            # Use the chunked transfer-coding\n            self.chunked_write = True\n            self.outheaders.append((\"Transfer-Encoding\", \"chunked\"))\n        else:\n            # Closing the conn is the only way to determine len.\n            self.close_connection = True\n\nif \"connection\" not in hkeys:\n    if self.response_protocol == 'HTTP/1.1':\n        if self.close_connection:\n            self.outheaders.append((\"Connection\", \"close\"))\n    else:\n        if not self.close_connection:\n            self.outheaders.append((\"Connection\", \"Keep-Alive\"))\n\nif \"date\" not in hkeys:\n    self.outheaders.append((\"Date\", rfc822.formatdate()))\n\nserver = self.connection.server\n\nif \"server\" not in hkeys:\n    self.outheaders.append((\"Server\", server.version))\n\nbuf = [server.protocol, \" \", self.status, \"\\r\\n\"]\ntry:\n    buf += [k + \": \" + v + \"\\r\\n\" for k, v in self.outheaders]\nexcept TypeError:\n    if not isinstance(k, str):\n        raise TypeError(\"WSGI response header key %r is not a string.\")\n    if not isinstance(v, str):\n        raise TypeError(\"WSGI response header value %r is not a string.\")\n    else:\n        raise\nbuf.append(\"\\r\\n\")\nself.sendall(\"\".join(buf))", "path": "server\\web\\wsgiserver\\__init__.py", "repo_name": "Sjors/openstreetmap-route-altitude-profile", "stars": 9, "license": "None", "language": "python", "size": 220}
{"docstring": "\"\"\"\nUpdate `tables` with clause `where` (interpolated using `vars`)\nand setting `values`.\n\n    >>> joe = 'Joseph'\n    >>> update('foo', where='name = $joe', name='bob', age=5,\n    ...   vars=locals(), _test=True)\n    <sql: \"UPDATE foo SET age = 5, name = 'bob' WHERE name = 'Joseph'\">\n\"\"\"\n", "func_signal": "def update(tables, where, vars=None, _test=False, **values):\n", "code": "if vars is None: vars = {}\n\nif isinstance(where, (int, long)):\n    where = \"id = \" + sqlquote(where)\nelif isinstance(where, (list, tuple)) and len(where) == 2:\n    where = SQLQuery(where[0], where[1])\nelif isinstance(where, SQLQuery):\n    pass\nelse:\n    where = reparam(where, vars)\n\nquery = (\n  \"UPDATE \" + sqllist(tables) + \n  \" SET \" + sqlwhere(values, ', ') + \n  \" WHERE \" + where)\n\nif _test: return query\n\ndb_cursor = web.ctx.db_cursor()\nweb.ctx.db_execute(db_cursor, query)\n\nif not web.ctx.db_transaction: web.ctx.db.commit()\nreturn db_cursor.rowcount", "path": "server\\web\\db.py", "repo_name": "Sjors/openstreetmap-route-altitude-profile", "stars": 9, "license": "None", "language": "python", "size": 220}
{"docstring": "\"\"\"\nreturns either (ip_address, port) or \"/path/to/socket\" from string_\n\n    >>> validaddr('/path/to/socket')\n    '/path/to/socket'\n    >>> validaddr('8000')\n    ('0.0.0.0', 8000)\n    >>> validaddr('127.0.0.1')\n    ('127.0.0.1', 8080)\n    >>> validaddr('127.0.0.1:8000')\n    ('127.0.0.1', 8000)\n    >>> validaddr('fff')\n    Traceback (most recent call last):\n        ...\n    ValueError: fff is not a valid IP address/port\n\"\"\"\n", "func_signal": "def validaddr(string_):\n", "code": "if '/' in string_:\n    return string_\nelse:\n    return validip(string_)", "path": "server\\web\\net.py", "repo_name": "Sjors/openstreetmap-route-altitude-profile", "stars": 9, "license": "None", "language": "python", "size": 220}
{"docstring": "\"\"\"\nconverts `obj` to its proper SQL version\n\n    >>> sqlify(None)\n    'NULL'\n    >>> sqlify(True)\n    \"'t'\"\n    >>> sqlify(3)\n    '3'\n\"\"\"\n\n# because `1 == True and hash(1) == hash(True)`\n# we have to do this the hard way...\n\n", "func_signal": "def sqlify(obj):\n", "code": "if obj is None:\n    return 'NULL'\nelif obj is True:\n    return \"'t'\"\nelif obj is False:\n    return \"'f'\"\nelif datetime and isinstance(obj, datetime.datetime):\n    return repr(obj.isoformat())\nelse:\n    return repr(obj)", "path": "server\\web\\db.py", "repo_name": "Sjors/openstreetmap-route-altitude-profile", "stars": 9, "license": "None", "language": "python", "size": 220}
{"docstring": "\"\"\"Start a transaction.\"\"\"\n", "func_signal": "def transact():\n", "code": "if not web.ctx.db_transaction:\n    # commit everything up to now, so we don't rollback it later\n    if hasattr(web.ctx.db, 'commit'): \n        web.ctx.db.commit()\nelse:\n    db_cursor = web.ctx.db_cursor()\n    web.ctx.db_execute(db_cursor, \n        SQLQuery(\"SAVEPOINT webpy_sp_%s\" % web.ctx.db_transaction))\nweb.ctx.db_transaction += 1", "path": "server\\web\\db.py", "repo_name": "Sjors/openstreetmap-route-altitude-profile", "stars": 9, "license": "None", "language": "python", "size": 220}
{"docstring": "\"\"\"\nDecodes `text` that's HTML quoted.\n\n    >>> htmlunquote('&lt;&#39;&amp;&quot;&gt;')\n    '<\\\\'&\">'\n\"\"\"\n", "func_signal": "def htmlunquote(text):\n", "code": "text = text.replace(\"&quot;\", '\"')\ntext = text.replace(\"&#39;\", \"'\")\ntext = text.replace(\"&gt;\", \">\")\ntext = text.replace(\"&lt;\", \"<\")\ntext = text.replace(\"&amp;\", \"&\") # Must be done last!\nreturn text", "path": "server\\web\\net.py", "repo_name": "Sjors/openstreetmap-route-altitude-profile", "stars": 9, "license": "None", "language": "python", "size": 220}
{"docstring": "\"\"\"\nExecute SQL query `sql_query` using dictionary `vars` to interpolate it.\nIf `processed=True`, `vars` is a `reparam`-style list to use \ninstead of interpolating.\n\n    >>> query(\"SELECT * FROM foo\", _test=True)\n    <sql: 'SELECT * FROM foo'>\n    >>> query(\"SELECT * FROM foo WHERE x = $x\", vars=dict(x='f'), _test=True)\n    <sql: \"SELECT * FROM foo WHERE x = 'f'\">\n    >>> query(\"SELECT * FROM foo WHERE x = \" + sqlquote('f'), _test=True)\n    <sql: \"SELECT * FROM foo WHERE x = 'f'\">\n\"\"\"\n", "func_signal": "def query(sql_query, vars=None, processed=False, _test=False):\n", "code": "if vars is None: vars = {}\n\nif not processed and not isinstance(sql_query, SQLQuery):\n    sql_query = reparam(sql_query, vars)\n\nif _test: return sql_query\n\ndb_cursor = web.ctx.db_cursor()\nweb.ctx.db_execute(db_cursor, sql_query)\n\nif db_cursor.description:\n    names = [x[0] for x in db_cursor.description]\n    def iterwrapper():\n        row = db_cursor.fetchone()\n        while row:\n            yield storage(dict(zip(names, row)))\n            row = db_cursor.fetchone()\n    out = iterbetter(iterwrapper())\n    if web.ctx.db_name != \"sqlite\":\n        out.__len__ = lambda: int(db_cursor.rowcount)\n    out.list = lambda: [storage(dict(zip(names, x))) \\\n                       for x in db_cursor.fetchall()]\nelse:\n    out = db_cursor.rowcount\n\nif not web.ctx.db_transaction: web.ctx.db.commit()\nreturn out", "path": "server\\web\\db.py", "repo_name": "Sjors/openstreetmap-route-altitude-profile", "stars": 9, "license": "None", "language": "python", "size": 220}
{"docstring": "\"\"\"\nConverts the arguments for use in something like a WHERE clause.\n\n    >>> sqllist(['a', 'b'])\n    'a, b'\n    >>> sqllist('foo')\n    'foo'\n    >>> sqllist(u'foo')\n    u'foo'\n\"\"\"\n", "func_signal": "def sqllist(lst):\n", "code": "if isinstance(lst, (str, unicode)): \n    return lst\nelse:\n    return ', '.join(lst)", "path": "server\\web\\db.py", "repo_name": "Sjors/openstreetmap-route-altitude-profile", "stars": 9, "license": "None", "language": "python", "size": 220}
{"docstring": "\"\"\" Returns a list of descendants that pass the test function \"\"\"\n", "func_signal": "def find(self, test, depth=0):\n", "code": "matched_nodes = []\nfor child in self.childNodes:\n    if test(child):\n        matched_nodes.append(child)\n    if child.type == \"element\":\n        matched_nodes += child.find(test, depth+1)\nreturn matched_nodes", "path": "markdown.py", "repo_name": "vbabiy/django_blog", "stars": 14, "license": "None", "language": "python", "size": 814}
{"docstring": "''' Report info about instance. Markdown always returns unicode. '''\n", "func_signal": "def __str__(self):\n", "code": "if self.source is None:\n    status = 'in which no source text has been assinged.'\nelse:\n    status = 'which contains %d chars and %d line(s) of source.'%\\\n             (len(self.source), self.source.count('\\n')+1)\nreturn 'An instance of \"%s\" %s'% (self.__class__, status)", "path": "markdown.py", "repo_name": "vbabiy/django_blog", "stars": 14, "license": "None", "language": "python", "size": 814}
{"docstring": "\"\"\" A utility function to break a list of lines upon the\n    first line that satisfied a condition.  The condition\n    argument should be a predicate function.\n    \"\"\"\n\n", "func_signal": "def _linesUntil(self, lines, condition):\n", "code": "i = -1\nfor line in lines:\n    i += 1\n    if condition(line): break\nelse:\n    i += 1\nreturn lines[:i], lines[i:]", "path": "markdown.py", "repo_name": "vbabiy/django_blog", "stars": 14, "license": "None", "language": "python", "size": 814}
{"docstring": "\"\"\"\nReturns a list of records for the provided thread, of if none is provided,\nreturns a list for the current thread.\n\"\"\"\n", "func_signal": "def get_records(self, thread=None):\n", "code": "if thread is None:\n    thread = threading.currentThread()\nif thread not in self.records:\n    self.records[thread] = []\nreturn self.records[thread]", "path": "debug_toolbar\\panels\\logger.py", "repo_name": "vbabiy/django_blog", "stars": 14, "license": "None", "language": "python", "size": 814}
{"docstring": "''' Basic html escaping '''\n", "func_signal": "def escape(self, html):\n", "code": "html = html.replace('&', '&amp;')\nhtml = html.replace('<', '&lt;')\nhtml = html.replace('>', '&gt;')\nreturn html.replace('\"', '&quot;')", "path": "markdown.py", "repo_name": "vbabiy/django_blog", "stars": 14, "license": "None", "language": "python", "size": 814}
{"docstring": "\"\"\"\nRetrieves ``num`` random objects from a given model, and stores\nthem in a context variable.\n\nSyntax::\n\n    {% get_random_objects [app_name].[model_name] [num] as [varname] %}\n\nExample::\n\n    {% get_random_objects comments.freecomment 5 as random_comments %}\n\n\"\"\"\n", "func_signal": "def do_random_objects(parser, token):\n", "code": "bits = token.contents.split()\nif len(bits) != 5:\n    raise template.TemplateSyntaxError(\"'%s' tag takes four arguments\" % bits[0])\nif bits [3] != 'as':\n    raise template.TemplateSyntaxError(\"third argument to '%s' tag must be 'as'\" % bits[0])\nreturn RandomObjectsNode(bits[1], bits[2], bits[4])", "path": "template_utils\\templatetags\\generic_content.py", "repo_name": "vbabiy/django_blog", "stars": 14, "license": "None", "language": "python", "size": 814}
{"docstring": "\"\"\"\nRetrieves the latest object from a given model, in that model's\ndefault ordering, and stores it in a context variable.\n\nSyntax::\n\n    {% get_latest_object [app_name].[model_name] as [varname] %}\n\nExample::\n\n    {% get_latest_object comments.freecomment as latest_comment %}\n\n\"\"\"\n", "func_signal": "def do_latest_object(parser, token):\n", "code": "bits = token.contents.split()\nif len(bits) != 4:\n    raise template.TemplateSyntaxError(\"'%s' tag takes three arguments\" % bits[0])\nif bits [2] != 'as':\n    raise template.TemplateSyntaxError(\"second argument to '%s' tag must be 'as'\" % bits[0])\nreturn GenericContentNode(bits[1], 1, bits[3])", "path": "template_utils\\templatetags\\generic_content.py", "repo_name": "vbabiy/django_blog", "stars": 14, "license": "None", "language": "python", "size": 814}
{"docstring": "\"\"\"\nApplies text-to-HTML conversion.\n\nTakes an optional argument to specify the name of a filter to use.\n\n\"\"\"\n", "func_signal": "def apply_markup(value, arg=None):\n", "code": "if arg is not None:\n    return mark_safe(formatter(value, filter_name=arg))\nreturn mark_safe(formatter(value))", "path": "template_utils\\templatetags\\generic_markup.py", "repo_name": "vbabiy/django_blog", "stars": 14, "license": "None", "language": "python", "size": 814}
{"docstring": "\"\"\"\nSimilar to string.replace() but is case insensitive\nCode borrowed from: http://forums.devshed.com/python-programming-11/case-insensitive-string-replace-490921.html\n\"\"\"\n", "func_signal": "def replace_insensitive(string, target, replacement):\n", "code": "no_case = string.lower()\nindex = no_case.find(target.lower())\nif index >= 0:\n    return string[:index] + replacement + string[index + len(target):]\nelse: # no results so return the original string\n    return string", "path": "debug_toolbar\\middleware.py", "repo_name": "vbabiy/django_blog", "stars": 14, "license": "None", "language": "python", "size": 814}
{"docstring": "\"\"\" Removes quotes from around a string \"\"\"\n", "func_signal": "def dequote(string):\n", "code": "if ( ( string.startswith('\"') and string.endswith('\"'))\n     or (string.startswith(\"'\") and string.endswith(\"'\")) ):\n    return string[1:-1]\nelse:\n    return string", "path": "markdown.py", "repo_name": "vbabiy/django_blog", "stars": 14, "license": "None", "language": "python", "size": 814}
{"docstring": "\"\"\"\nApplies SmartyPants to a piece of text, applying typographic\nniceties.\n\nRequires the Python SmartyPants library to be installed; see\nhttp://web.chad.org/projects/smartypants.py/\n\n\"\"\"\n", "func_signal": "def smartypants(value):\n", "code": "try:\n    from smartypants import smartyPants\nexcept ImportError:\n    if settings.DEBUG:\n        raise template.TemplateSyntaxError(\"Error in smartypants filter: the Python smartypants module is not installed or could not be imported\")\n    return value\nelse:\n    return mark_safe(smartyPants(value))", "path": "template_utils\\templatetags\\generic_markup.py", "repo_name": "vbabiy/django_blog", "stars": 14, "license": "None", "language": "python", "size": 814}
{"docstring": "\"\"\"\nRetrieves the latest ``num`` objects from a given model, in that\nmodel's default ordering, and stores them in a context variable.\n\nSyntax::\n\n    {% get_latest_objects [app_name].[model_name] [num] as [varname] %}\n\nExample::\n\n    {% get_latest_objects comments.freecomment 5 as latest_comments %}\n\n\"\"\"\n", "func_signal": "def do_latest_objects(parser, token):\n", "code": "bits = token.contents.split()\nif len(bits) != 5:\n    raise template.TemplateSyntaxError(\"'%s' tag takes four arguments\" % bits[0])\nif bits [3] != 'as':\n    raise template.TemplateSyntaxError(\"third argument to '%s' tag must be 'as'\" % bits[0])\nreturn GenericContentNode(bits[1], bits[2], bits[4])", "path": "template_utils\\templatetags\\generic_content.py", "repo_name": "vbabiy/django_blog", "stars": 14, "license": "None", "language": "python", "size": 814}
{"docstring": "\"\"\"Saves an HTML segment for later reinsertion.  Returns a\n   placeholder string that needs to be inserted into the\n   document.\n\n   @param html: an html segment\n   @param safe: label an html segment as safe for safemode\n   @param inline: label a segmant as inline html\n   @returns : a placeholder string \"\"\"\n", "func_signal": "def store(self, html, safe=False):\n", "code": "self.rawHtmlBlocks.append((html, safe))\nplaceholder = HTML_PLACEHOLDER % self.html_counter\nself.html_counter += 1\nreturn placeholder", "path": "markdown.py", "repo_name": "vbabiy/django_blog", "stars": 14, "license": "None", "language": "python", "size": 814}
{"docstring": "\"\"\"Transforms the Markdown text into a XHTML body document\n\n   @returns: A NanoDom Document \"\"\"\n\n# Setup the document\n\n", "func_signal": "def _transform(self):\n", "code": "self.doc = Document()\nself.top_element = self.doc.createElement(\"span\")\nself.top_element.appendChild(self.doc.createTextNode('\\n'))\nself.top_element.setAttribute('class', 'markdown')\nself.doc.appendChild(self.top_element)\n\n# Fixup the source text\ntext = self.source\ntext = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\ntext += \"\\n\\n\"\ntext = text.expandtabs(TAB_LENGTH)\n\n# Split into lines and run the preprocessors that will work with\n# self.lines\n\nself.lines = text.split(\"\\n\")\n\n# Run the pre-processors on the lines\nfor prep in self.preprocessors :\n    self.lines = prep.run(self.lines)\n\n# Create a NanoDom tree from the lines and attach it to Document\n\n\nbuffer = []\nfor line in self.lines:\n    if line.startswith(\"#\"):\n        self._processSection(self.top_element, buffer)\n        buffer = [line]\n    else:\n        buffer.append(line)\nself._processSection(self.top_element, buffer)\n\n#self._processSection(self.top_element, self.lines)\n\n# Not sure why I put this in but let's leave it for now.\nself.top_element.appendChild(self.doc.createTextNode('\\n'))\n\n# Run the post-processors\nfor postprocessor in self.postprocessors:\n    postprocessor.run(self.doc)\n\nreturn self.doc", "path": "markdown.py", "repo_name": "vbabiy/django_blog", "stars": 14, "license": "None", "language": "python", "size": 814}
{"docstring": "\"\"\"\nRetrieves a random object from a given model, and stores it in a\ncontext variable.\n\nSyntax::\n\n    {% get_random_object [app_name].[model_name] as [varname] %}\n\nExample::\n\n    {% get_random_object comments.freecomment as random_comment %}\n\n\"\"\"\n", "func_signal": "def do_random_object(parser, token):\n", "code": "bits = token.contents.split()\nif len(bits) != 4:\n    raise template.TemplateSyntaxError(\"'%s' tag takes three arguments\" % bits[0])\nif bits [2] != 'as':\n    raise template.TemplateSyntaxError(\"second argument to '%s' tag must be 'as'\" % bits[0])\nreturn RandomObjectsNode(bits[1], 1, bits[3])", "path": "template_utils\\templatetags\\generic_content.py", "repo_name": "vbabiy/django_blog", "stars": 14, "license": "None", "language": "python", "size": 814}
{"docstring": "\"\"\"Transform a Markdown line with inline elements to an XHTML\nfragment.\n\nThis function uses auxiliary objects called inline patterns.\nSee notes on inline patterns above.\n\n@param line: A line of Markdown text\n@param patternIndex: The index of the inlinePattern to start with\n@return: A list of NanoDom nodes \"\"\"\n\n\n", "func_signal": "def _handleInline (self, line, patternIndex=0):\n", "code": "parts = [line]\n\nwhile patternIndex < len(self.inlinePatterns):\n\n    i = 0\n\n    while i < len(parts):\n        \n        x = parts[i]\n\n        if isinstance(x, (str, unicode)):\n            result = self._applyPattern(x, \\\n                        self.inlinePatterns[patternIndex], \\\n                        patternIndex)\n\n            if result:\n                i -= 1\n                parts.remove(x)\n                for y in result:\n                    parts.insert(i+1,y)\n\n        i += 1\n    patternIndex += 1\n\nfor i in range(len(parts)):\n    x = parts[i]\n    if isinstance(x, (str, unicode)):\n        parts[i] = self.doc.createTextNode(x)\n\nreturn parts", "path": "markdown.py", "repo_name": "vbabiy/django_blog", "stars": 14, "license": "None", "language": "python", "size": 814}
{"docstring": "\"\"\"Given a list of document lines starting with a list item,\n   finds the end of the list, breaks it up, and recursively\n   processes each list item and the remainder of the text file.\n\n   @param parent_elem: A dom element to which the content will be added\n   @param lines: a list of lines\n   @param inList: a level\n   @returns: None\"\"\"\n\n", "func_signal": "def _processList(self, parent_elem, lines, inList, listexpr, tag):\n", "code": "ul = self.doc.createElement(tag)  # ul might actually be '<ol>'\nparent_elem.appendChild(ul)\n\nlooseList = 0\n\n# Make a list of list items\nitems = []\nitem = -1\n\ni = 0  # a counter to keep track of where we are\n\nfor line in lines: \n\n    loose = 0\n    if not line.strip():\n        # If we see a blank line, this _might_ be the end of the list\n        i += 1\n        loose = 1\n\n        # Find the next non-blank line\n        for j in range(i, len(lines)):\n            if lines[j].strip():\n                next = lines[j]\n                break\n        else:\n            # There is no more text => end of the list\n            break\n\n        # Check if the next non-blank line is still a part of the list\n        if ( RE.regExp['ul'].match(next) or\n             RE.regExp['ol'].match(next) or \n             RE.regExp['tabbed'].match(next) ):\n            # get rid of any white space in the line\n            items[item].append(line.strip())\n            looseList = loose or looseList\n            continue\n        else:\n            break # found end of the list\n\n    # Now we need to detect list items (at the current level)\n    # while also detabing child elements if necessary\n\n    for expr in ['ul', 'ol', 'tabbed']:\n\n        m = RE.regExp[expr].match(line)\n        if m:\n            if expr in ['ul', 'ol']:  # We are looking at a new item\n                #if m.group(1) :\n                # Removed the check to allow for a blank line\n                # at the beginning of the list item\n                items.append([m.group(1)])\n                item += 1\n            elif expr == 'tabbed':  # This line needs to be detabbed\n                items[item].append(m.group(4)) #after the 'tab'\n\n            i += 1\n            break\n    else:\n        items[item].append(line)  # Just regular continuation\n        i += 1 # added on 2006.02.25\nelse:\n    i += 1\n\n# Add the dom elements\nfor item in items:\n    li = self.doc.createElement(\"li\")\n    ul.appendChild(li)\n\n    self._processSection(li, item, inList + 1, looseList = looseList)\n\n# Process the remaining part of the section\n\nself._processSection(parent_elem, lines[i:], inList)", "path": "markdown.py", "repo_name": "vbabiy/django_blog", "stars": 14, "license": "None", "language": "python", "size": 814}
{"docstring": "\"\"\"Return the document in XHTML format.\n\n@returns: A serialized XHTML body.\"\"\"\n\n", "func_signal": "def convert (self, source = None):\n", "code": "if source is not None: #Allow blank string\n    self.source = source\n\nif not self.source:\n    return u\"\"\n\ntry:\n    self.source = unicode(self.source)\nexcept UnicodeDecodeError:\n    message(CRITICAL, 'UnicodeDecodeError: Markdown only accepts unicode or ascii  input.')\n    return u\"\"\n\nfor pp in self.textPreprocessors:\n    self.source = pp.run(self.source)\n\ndoc = self._transform()\nxml = doc.toxml()\n\n\n# Return everything but the top level tag\n\nif self.stripTopLevelTags:\n    xml = xml.strip()[23:-7] + \"\\n\"\n\nfor pp in self.textPostprocessors:\n    xml = pp.run(xml)\n\nreturn (self.docType + xml).strip()", "path": "markdown.py", "repo_name": "vbabiy/django_blog", "stars": 14, "license": "None", "language": "python", "size": 814}
{"docstring": "\"\"\" An auxiliary method to be passed to _findHead \"\"\"\n", "func_signal": "def detabbed_fn(self, line):\n", "code": "m = RE.regExp['tabbed'].match(line)\nif m:\n    return m.group(4)\nelse:\n    return None", "path": "markdown.py", "repo_name": "vbabiy/django_blog", "stars": 14, "license": "None", "language": "python", "size": 814}
{"docstring": "\"\"\"Determines if a block should be replaced with an <HR>\"\"\"\n", "func_signal": "def _isLine(self, block):\n", "code": "if block.startswith(\"    \"): return 0  # a code block\ntext = \"\".join([x for x in block if not x.isspace()])\nif len(text) <= 2:\n    return 0\nfor pattern in ['isline1', 'isline2', 'isline3']:\n    m = RE.regExp[pattern].match(text)\n    if (m and m.group(1)):\n        return 1\nelse:\n    return 0", "path": "markdown.py", "repo_name": "vbabiy/django_blog", "stars": 14, "license": "None", "language": "python", "size": 814}
{"docstring": "\"\"\"Get contents of the packed-refs file.\n\n:return: Dictionary mapping ref names to SHA1s\n\n:note: Will return an empty dictionary when no packed-refs file is \n    present.\n\"\"\"\n", "func_signal": "def get_packed_refs(self):\n", "code": "path = os.path.join(self.controldir(), 'packed-refs')\nif not os.path.exists(path):\n    return {}\nret = {}\nf = open(path, 'rb')\ntry:\n    for entry in read_packed_refs(f):\n        ret[entry[1]] = entry[0]\n    return ret\nfinally:\n    f.close()", "path": "dulwich\\repo.py", "repo_name": "schacon/agitmemnon-server", "stars": 13, "license": "gpl-2.0", "language": "python", "size": 556}
{"docstring": "\"\"\"The SHA1 object that is the name of this object.\"\"\"\n", "func_signal": "def sha(self):\n", "code": "if self._needs_serialization or self._sha is None:\n    self._sha = make_sha()\n    self._sha.update(self._header())\n    self._sha.update(self.as_raw_string())\nreturn self._sha", "path": "dulwich\\objects.py", "repo_name": "schacon/agitmemnon-server", "stars": 13, "license": "gpl-2.0", "language": "python", "size": 556}
{"docstring": "\"\"\"Return a list of parents of this commit.\"\"\"\n", "func_signal": "def set_parents(self, value):\n", "code": "self._ensure_parsed()\nself._needs_serialization = True\nself._parents = value", "path": "dulwich\\objects.py", "repo_name": "schacon/agitmemnon-server", "stars": 13, "license": "gpl-2.0", "language": "python", "size": 556}
{"docstring": "\"\"\"Return a list of tuples describing the tree entries\"\"\"\n", "func_signal": "def entries(self):\n", "code": "self._ensure_parsed()\n# The order of this is different from iteritems() for historical reasons\nreturn [(mode, name, hexsha) for (name, mode, hexsha) in self.iteritems()]", "path": "dulwich\\objects.py", "repo_name": "schacon/agitmemnon-server", "stars": 13, "license": "gpl-2.0", "language": "python", "size": 556}
{"docstring": "\"\"\"Returns the object pointed by this tag, represented as a tuple(type, sha)\"\"\"\n", "func_signal": "def get_object(self):\n", "code": "self._ensure_parsed()\nreturn (self._object_type, self._object_sha)", "path": "dulwich\\objects.py", "repo_name": "schacon/agitmemnon-server", "stars": 13, "license": "gpl-2.0", "language": "python", "size": 556}
{"docstring": "\"\"\"Get the contents of a SHA file on disk\"\"\"\n", "func_signal": "def from_file(cls, filename):\n", "code": "size = os.path.getsize(filename)\nf = open(filename, 'rb')\ntry:\n    map = mmap.mmap(f.fileno(), size, access=mmap.ACCESS_READ)\n    shafile = cls._parse_file(map)\n    return shafile\nfinally:\n    f.close()", "path": "dulwich\\objects.py", "repo_name": "schacon/agitmemnon-server", "stars": 13, "license": "gpl-2.0", "language": "python", "size": 556}
{"docstring": "\"\"\"Parse a legacy object, creating it and setting object._text\"\"\"\n", "func_signal": "def _parse_legacy_object(cls, map):\n", "code": "text = _decompress(map)\nobject = None\nfor posstype in type_map.keys():\n    if text.startswith(posstype):\n        object = type_map[posstype]()\n        text = text[len(posstype):]\n        break\nassert object is not None, \"%s is not a known object type\" % text[:9]\nassert text[0] == ' ', \"%s is not a space\" % text[0]\ntext = text[1:]\nsize = 0\ni = 0\nwhile text[0] >= '0' and text[0] <= '9':\n    if i > 0 and size == 0:\n        raise AssertionError(\"Size is not in canonical format\")\n    size = (size * 10) + int(text[0])\n    text = text[1:]\n    i += 1\nobject._size = size\nassert text[0] == \"\\0\", \"Size not followed by null\"\ntext = text[1:]\nobject.set_raw_string(text)\nreturn object", "path": "dulwich\\objects.py", "repo_name": "schacon/agitmemnon-server", "stars": 13, "license": "gpl-2.0", "language": "python", "size": 556}
{"docstring": "\"\"\"Takes a string and returns the hex of the sha within\"\"\"\n", "func_signal": "def sha_to_hex(sha):\n", "code": "hexsha = binascii.hexlify(sha)\nassert len(hexsha) == 40, \"Incorrect length of sha1 string: %d\" % hexsha\nreturn hexsha", "path": "dulwich\\objects.py", "repo_name": "schacon/agitmemnon-server", "stars": 13, "license": "gpl-2.0", "language": "python", "size": 556}
{"docstring": "\"\"\"Read a packed refs file.\n\nYields tuples with ref names and SHA1s.\n\n:param f: file-like object to read from\n\"\"\"\n", "func_signal": "def read_packed_refs(f):\n", "code": "l = f.readline()\nfor l in f.readlines():\n    if l[0] == \"#\":\n        # Comment\n        continue\n    if l[0] == \"^\":\n        # FIXME: Return somehow\n        continue\n    yield tuple(l.rstrip(\"\\n\").split(\" \", 2))", "path": "dulwich\\repo.py", "repo_name": "schacon/agitmemnon-server", "stars": 13, "license": "gpl-2.0", "language": "python", "size": 556}
{"docstring": "\"\"\"Create a blob from a string.\"\"\"\n", "func_signal": "def from_string(cls, string):\n", "code": "shafile = cls()\nshafile.set_raw_string(string)\nreturn shafile", "path": "dulwich\\objects.py", "repo_name": "schacon/agitmemnon-server", "stars": 13, "license": "gpl-2.0", "language": "python", "size": 556}
{"docstring": "\"\"\"Fetch a pack from the remote host.\n\n:param path: Path of the reposiutory on the remote host\n:param determine_wants: Callback that receives available refs dict and \n    should return list of sha's to fetch.\n:param graph_walker: GraphWalker instance used to find missing shas\n:param pack_data: Callback for writing pack data\n:param progress: Callback for writing progress\n\"\"\"\n", "func_signal": "def fetch_pack(self, path, determine_wants, graph_walker, pack_data, progress):\n", "code": "self.proto.send_cmd(\"git-upload-pack\", path, \"host=%s\" % self.host)\nreturn super(TCPGitClient, self).fetch_pack(path, determine_wants,\n    graph_walker, pack_data, progress)", "path": "dulwich\\client.py", "repo_name": "schacon/agitmemnon-server", "stars": 13, "license": "gpl-2.0", "language": "python", "size": 556}
{"docstring": "\"\"\"Grab the entries in the tree\"\"\"\n", "func_signal": "def _parse_text(self):\n", "code": "self._entries = parse_tree(self._text)\nself._needs_parsing = False", "path": "dulwich\\objects.py", "repo_name": "schacon/agitmemnon-server", "stars": 13, "license": "gpl-2.0", "language": "python", "size": 556}
{"docstring": "\"\"\"Grab the metadata attached to the tag\"\"\"\n", "func_signal": "def _parse_text(self):\n", "code": "self._tagger = None\nf = StringIO(self._text)\nfor l in f:\n    l = l.rstrip(\"\\n\")\n    if l == \"\":\n        break # empty line indicates end of headers\n    (field, value) = l.split(\" \", 1)\n    if field == OBJECT_ID:\n        self._object_sha = value\n    elif field == TYPE_ID:\n        self._object_type = type_map[value]\n    elif field == TAG_ID:\n        self._name = value\n    elif field == TAGGER_ID:\n        sep = value.index(\"> \")\n        self._tagger = value[0:sep+1]\n        (timetext, timezonetext) = value[sep+2:].rsplit(\" \", 1)\n        try:\n            self._tag_time = int(timetext)\n        except ValueError: #Not a unix timestamp\n            self._tag_time = time.strptime(timetext)\n        self._tag_timezone = parse_timezone(timezonetext)\n    else:\n        raise AssertionError(\"Unknown field %s\" % field)\nself._message = f.read()\nself._needs_parsing = False", "path": "dulwich\\objects.py", "repo_name": "schacon/agitmemnon-server", "stars": 13, "license": "gpl-2.0", "language": "python", "size": 556}
{"docstring": "\"\"\"Parse a new style object , creating it and setting object._text\"\"\"\n", "func_signal": "def _parse_object(cls, map):\n", "code": "used = 0\nbyte = ord(map[used])\nused += 1\nnum_type = (byte >> 4) & 7\ntry:\n    object = num_type_map[num_type]()\nexcept KeyError:\n    raise AssertionError(\"Not a known type: %d\" % num_type)\nwhile (byte & 0x80) != 0:\n    byte = ord(map[used])\n    used += 1\nraw = map[used:]\nobject.set_raw_string(_decompress(raw))\nreturn object", "path": "dulwich\\objects.py", "repo_name": "schacon/agitmemnon-server", "stars": 13, "license": "gpl-2.0", "language": "python", "size": 556}
{"docstring": "\"\"\"Return a list of parents of this commit.\"\"\"\n", "func_signal": "def get_parents(self):\n", "code": "self._ensure_parsed()\nreturn self._parents", "path": "dulwich\\objects.py", "repo_name": "schacon/agitmemnon-server", "stars": 13, "license": "gpl-2.0", "language": "python", "size": 556}
{"docstring": "\"\"\"Return the SHA1 a ref is pointing to.\"\"\"\n", "func_signal": "def ref(self, name):\n", "code": "try:\n    return self.refs.follow(name)\nexcept KeyError:\n    return self.get_packed_refs()[name]", "path": "dulwich\\repo.py", "repo_name": "schacon/agitmemnon-server", "stars": 13, "license": "gpl-2.0", "language": "python", "size": 556}
{"docstring": "#FIXME: This has no way to deal with passwords..\n", "func_signal": "def connect_ssh(self, host, command, username=None, port=None):\n", "code": "args = ['ssh', '-x']\nif port is not None:\n    args.extend(['-p', str(port)])\nif username is not None:\n    host = \"%s@%s\" % (username, host)\nargs.append(host)\nproc = subprocess.Popen(args + command,\n                        stdin=subprocess.PIPE,\n                        stdout=subprocess.PIPE)\nreturn SSHSubprocess(proc)", "path": "dulwich\\client.py", "repo_name": "schacon/agitmemnon-server", "stars": 13, "license": "gpl-2.0", "language": "python", "size": 556}
{"docstring": "\"\"\"Takes a hex sha and returns a binary sha\"\"\"\n", "func_signal": "def hex_to_sha(hex):\n", "code": "assert len(hex) == 40, \"Incorrent length of hexsha: %s\" % hex\nreturn binascii.unhexlify(hex)", "path": "dulwich\\objects.py", "repo_name": "schacon/agitmemnon-server", "stars": 13, "license": "gpl-2.0", "language": "python", "size": 556}
{"docstring": "\"\"\"Get dictionary with all refs.\"\"\"\n", "func_signal": "def get_refs(self):\n", "code": "ret = {}\ntry:\n    if self.head():\n        ret['HEAD'] = self.head()\nexcept KeyError:\n    pass\nret.update(self.refs.as_dict())\nret.update(self.get_packed_refs())\nreturn ret", "path": "dulwich\\repo.py", "repo_name": "schacon/agitmemnon-server", "stars": 13, "license": "gpl-2.0", "language": "python", "size": 556}
{"docstring": "\"\"\"Open the index for this repository.\"\"\"\n", "func_signal": "def open_index(self):\n", "code": "from dulwich.index import Index\nreturn Index(self.index_path())", "path": "dulwich\\repo.py", "repo_name": "schacon/agitmemnon-server", "stars": 13, "license": "gpl-2.0", "language": "python", "size": 556}
{"docstring": "\"\"\"Determines whether the given cells are collinear along a dimension.\n\nReturns True if the given cells are in the same row (column_test=False)\nor in the same column (column_test=True).\n\nArgs:\n  cell1: The first geocell string.\n  cell2: The second geocell string.\n  column_test: A boolean, where False invokes a row collinearity test\n      and 1 invokes a column collinearity test.\n\nReturns:\n  A bool indicating whether or not the given cells are collinear in the given\n  dimension.\n\"\"\"\n", "func_signal": "def collinear(cell1, cell2, column_test):\n", "code": "for i in range(min(len(cell1), len(cell2))):\n  x1, y1 = _subdiv_xy(cell1[i])\n  x2, y2 = _subdiv_xy(cell2[i])\n\n  # Check row collinearity (assure y's are always the same).\n  if not column_test and y1 != y2:\n    return False\n\n  # Check column collinearity (assure x's are always the same).\n  if column_test and x1 != x2:\n    return False\n\nreturn True", "path": "service\\google-app-engine\\geo\\geocell.py", "repo_name": "timburks/stickup", "stars": 12, "license": "None", "language": "python", "size": 218}
{"docstring": "\"\"\"Computes the number of cells in the grid formed between two given cells.\n\nComputes the number of cells in the grid created by interpolating from the\ngiven Northeast geocell to the given Southwest geocell. Assumes the Northeast\ngeocell is actually Northeast of Southwest geocell.\n\nArguments:\n  cell_ne: The Northeast geocell string.\n  cell_sw: The Southwest geocell string.\n\nReturns:\n  An int, indicating the number of geocells in the interpolation.\n\"\"\"\n", "func_signal": "def interpolation_count(cell_ne, cell_sw):\n", "code": "bbox_ne = compute_box(cell_ne)\nbbox_sw = compute_box(cell_sw)\n\ncell_lat_span = bbox_sw.north - bbox_sw.south\ncell_lon_span = bbox_sw.east - bbox_sw.west\n\nnum_cols = int((bbox_ne.east - bbox_sw.west) / cell_lon_span)\nnum_rows = int((bbox_ne.north - bbox_sw.south) / cell_lat_span)\n\nreturn num_cols * num_rows", "path": "service\\google-app-engine\\geo\\geocell.py", "repo_name": "timburks/stickup", "stars": 12, "license": "None", "language": "python", "size": 218}
{"docstring": "\"\"\"Returns an efficient set of geocells to search in a bounding box query.\n\nThis method is guaranteed to return a set of geocells having the same\nresolution.\n\nArgs:\n  bbox: A geotypes.Box indicating the bounding box being searched.\n  cost_function: A function that accepts two arguments:\n      * num_cells: the number of cells to search\n      * resolution: the resolution of each cell to search\n      and returns the 'cost' of querying against this number of cells\n      at the given resolution.\n\nReturns:\n  A list of geocell strings that contain the given box.\n\"\"\"\n", "func_signal": "def best_bbox_search_cells(bbox, cost_function):\n", "code": "cell_ne = compute(bbox.north_east, resolution=MAX_GEOCELL_RESOLUTION)\ncell_sw = compute(bbox.south_west, resolution=MAX_GEOCELL_RESOLUTION)\n\n# The current lowest BBOX-search cost found; start with practical infinity.\nmin_cost = 1e10000\n\n# The set of cells having the lowest calculated BBOX-search cost.\nmin_cost_cell_set = None\n\n# First find the common prefix, if there is one.. this will be the base\n# resolution.. i.e. we don't have to look at any higher resolution cells.\nmin_resolution = len(os.path.commonprefix([cell_sw, cell_ne]))\n\n# Iteravely calculate all possible sets of cells that wholely contain\n# the requested bounding box.\nfor cur_resolution in range(min_resolution, MAX_GEOCELL_RESOLUTION + 1):\n  cur_ne = cell_ne[:cur_resolution]\n  cur_sw = cell_sw[:cur_resolution]\n\n  num_cells = interpolation_count(cur_ne, cur_sw)\n  if num_cells > MAX_FEASIBLE_BBOX_SEARCH_CELLS:\n    continue\n\n  cell_set = sorted(interpolate(cur_ne, cur_sw))\n  simplified_cells = []\n\n  cost = cost_function(num_cells=len(cell_set), resolution=cur_resolution)\n\n  # TODO(romannurik): See if this resolution is even possible, as in the\n  # future cells at certain resolutions may not be stored.\n  if cost <= min_cost:\n    min_cost = cost\n    min_cost_cell_set = cell_set\n  else:\n    # Once the cost starts rising, we won't be able to do better, so abort.\n    break\n\nreturn min_cost_cell_set", "path": "service\\google-app-engine\\geo\\geocell.py", "repo_name": "timburks/stickup", "stars": 12, "license": "None", "language": "python", "size": 218}
{"docstring": "\"\"\"Return a JSON string representation of a Python data structure.\n\n>>> JSONEncoder().encode({\"foo\": [\"bar\", \"baz\"]})\n'{\"foo\": [\"bar\", \"baz\"]}'\n\n\"\"\"\n# This is for extremely simple cases and benchmarks.\n", "func_signal": "def encode(self, o):\n", "code": "if isinstance(o, basestring):\n    if isinstance(o, str):\n        _encoding = self.encoding\n        if (_encoding is not None\n                and not (_encoding == 'utf-8')):\n            o = o.decode(_encoding)\n    if self.ensure_ascii:\n        return encode_basestring_ascii(o)\n    else:\n        return encode_basestring(o)\n# This doesn't pass the iterator directly to ''.join() because the\n# exceptions aren't as detailed.  The list call should be roughly\n# equivalent to the PySequence_Fast that ''.join() would do.\nchunks = self.iterencode(o, _one_shot=True)\nif not isinstance(chunks, (list, tuple)):\n    chunks = list(chunks)\nreturn ''.join(chunks)", "path": "service\\google-app-engine\\simplejson\\encoder.py", "repo_name": "timburks/stickup", "stars": 12, "license": "None", "language": "python", "size": 218}
{"docstring": "# a valid geocell\n", "func_signal": "def test_compute(self):\n", "code": "cell = geocell.compute(geotypes.Point(37, -122), 14)\nself.assertEqual(14, len(cell))\nself.assertTrue(geocell.is_valid(cell))\nself.assertTrue(geocell.contains_point(cell, geotypes.Point(37, -122)))\n\n# a lower resolution cell should be a prefix to a higher resolution\n# cell containing the same point\nlowres_cell = geocell.compute(geotypes.Point(37, -122), 8)\nself.assertTrue(cell.startswith(lowres_cell))\nself.assertTrue(geocell.contains_point(lowres_cell,\n                                      geotypes.Point(37, -122)))\n\n# an invalid geocell\ncell = geocell.compute(geotypes.Point(0, 0), 0)\nself.assertEqual(0, len(cell))\nself.assertFalse(geocell.is_valid(cell))", "path": "service\\google-app-engine\\geo\\tests\\geocell_test.py", "repo_name": "timburks/stickup", "stars": 12, "license": "None", "language": "python", "size": 218}
{"docstring": "\"\"\"Encode the given object and yield each string\nrepresentation as available.\n\nFor example::\n\n    for chunk in JSONEncoder().iterencode(bigobject):\n        mysocket.write(chunk)\n\n\"\"\"\n", "func_signal": "def iterencode(self, o, _one_shot=False):\n", "code": "if self.check_circular:\n    markers = {}\nelse:\n    markers = None\nif self.ensure_ascii:\n    _encoder = encode_basestring_ascii\nelse:\n    _encoder = encode_basestring\nif self.encoding != 'utf-8':\n    def _encoder(o, _orig_encoder=_encoder, _encoding=self.encoding):\n        if isinstance(o, str):\n            o = o.decode(_encoding)\n        return _orig_encoder(o)\n\ndef floatstr(o, allow_nan=self.allow_nan, _repr=FLOAT_REPR, _inf=INFINITY, _neginf=-INFINITY):\n    # Check for specials.  Note that this type of test is processor- and/or\n    # platform-specific, so do tests which don't depend on the internals.\n\n    if o != o:\n        text = 'NaN'\n    elif o == _inf:\n        text = 'Infinity'\n    elif o == _neginf:\n        text = '-Infinity'\n    else:\n        return _repr(o)\n\n    if not allow_nan:\n        raise ValueError(\n            \"Out of range float values are not JSON compliant: \" +\n            repr(o))\n\n    return text\n\n\nif _one_shot and c_make_encoder is not None and not self.indent and not self.sort_keys:\n    _iterencode = c_make_encoder(\n        markers, self.default, _encoder, self.indent,\n        self.key_separator, self.item_separator, self.sort_keys,\n        self.skipkeys, self.allow_nan)\nelse:\n    _iterencode = _make_iterencode(\n        markers, self.default, _encoder, self.indent, floatstr,\n        self.key_separator, self.item_separator, self.sort_keys,\n        self.skipkeys, _one_shot)\nreturn _iterencode(o, 0)", "path": "service\\google-app-engine\\simplejson\\encoder.py", "repo_name": "timburks/stickup", "stars": 12, "license": "None", "language": "python", "size": 218}
{"docstring": "\"\"\"Computes the rectangular boundaries (bounding box) of the given geocell.\n\nArgs:\n  cell: The geocell string whose boundaries are to be computed.\n\nReturns:\n  A geotypes.Box corresponding to the rectangular boundaries of the geocell.\n\"\"\"\n", "func_signal": "def compute_box(cell):\n", "code": "if cell is None:\n  return None\n\nbbox = geotypes.Box(90.0, 180.0, -90.0, -180.0)\n\nwhile len(cell) > 0:\n  subcell_lon_span = (bbox.east - bbox.west) / _GEOCELL_GRID_SIZE\n  subcell_lat_span = (bbox.north - bbox.south) / _GEOCELL_GRID_SIZE\n\n  x, y = _subdiv_xy(cell[0])\n\n  bbox = geotypes.Box(bbox.south + subcell_lat_span * (y + 1),\n                      bbox.west  + subcell_lon_span * (x + 1),\n                      bbox.south + subcell_lat_span * y,\n                      bbox.west  + subcell_lon_span * x)\n\n  cell = cell[1:]\n\nreturn bbox", "path": "service\\google-app-engine\\geo\\geocell.py", "repo_name": "timburks/stickup", "stars": 12, "license": "None", "language": "python", "size": 218}
{"docstring": "# an invalid point\n", "func_signal": "def test_Point(self):\n", "code": "self.assertRaises(ValueError, geotypes.Point, 95, 0)\nself.assertRaises(ValueError, geotypes.Point, 0, 185)\n\n# a valid point\npoint = geotypes.Point(37, -122)\nself.assertEquals(37, point.lat)\nself.assertEquals(-122, point.lon)\n\nself.assertTrue(isinstance(point.__str__(), str))\n\nself.assertEquals(geotypes.Point(37, -122), geotypes.Point(37, -122))\nself.assertNotEquals(geotypes.Point(37, -122), geotypes.Point(0, 0))", "path": "service\\google-app-engine\\geo\\tests\\geotypes_test.py", "repo_name": "timburks/stickup", "stars": 12, "license": "None", "language": "python", "size": 218}
{"docstring": "\"\"\"Decode a JSON document from ``s`` (a ``str`` or ``unicode`` beginning\nwith a JSON document) and return a 2-tuple of the Python\nrepresentation and the index in ``s`` where the document ended.\n\nThis can be used to decode a JSON document from a string that may\nhave extraneous data at the end.\n\n\"\"\"\n", "func_signal": "def raw_decode(self, s, idx=0):\n", "code": "try:\n    obj, end = self.scan_once(s, idx)\nexcept StopIteration:\n    raise ValueError(\"No JSON object could be decoded\")\nreturn obj, end", "path": "service\\google-app-engine\\simplejson\\decoder.py", "repo_name": "timburks/stickup", "stars": 12, "license": "None", "language": "python", "size": 218}
{"docstring": "\"\"\"Merges an arbitrary number of pre-sorted lists in-place, into the first\nlist, possibly pruning out duplicates. Source lists must not have\nduplicates.\n\nArgs:\n  list1: The first, sorted list into which the other lists should be merged.\n  list2: A subsequent, sorted list to merge into the first.\n  ...\n  listn:  \"   \"\n  cmp_fn: An optional binary comparison function that compares objects across\n      lists and determines the merged list's sort order.\n  dup_fn: An optional binary comparison function that should return True if\n      the given objects are equivalent and one of them can be pruned from the\n      resulting merged list.\n\nReturns:\n  list1, in-placed merged wit the other lists, or an empty list if no lists\n  were specified.\n\"\"\"\n", "func_signal": "def merge_in_place(*lists, **kwargs):\n", "code": "cmp_fn = kwargs.get('cmp_fn') or cmp\ndup_fn = kwargs.get('dup_fn') or None\n\nif not lists:\n  return []\n\nreverse_indices = [len(arr) for arr in lists]\naggregate_reverse_index = sum(reverse_indices)\n\nwhile aggregate_reverse_index > 0:\n  pull_arr_index = None\n  pull_val = None\n\n  for i in range(len(lists)):\n    if reverse_indices[i] == 0:\n      # Reached the end of this list.\n      pass\n    elif (pull_arr_index is not None and\n          dup_fn and dup_fn(lists[i][-reverse_indices[i]], pull_val)):\n      # Found a duplicate, advance the index of the list in which the\n      # duplicate was found.\n      reverse_indices[i] -= 1\n      aggregate_reverse_index -= 1\n    elif (pull_arr_index is None or\n          cmp_fn(lists[i][-reverse_indices[i]], pull_val) < 0):\n      # Found a lower value.\n      pull_arr_index = i\n      pull_val = lists[i][-reverse_indices[i]]\n\n  if pull_arr_index != 0:\n    # Add the lowest found value in place into the first array.\n    lists[0].insert(len(lists[0]) - reverse_indices[0], pull_val)\n\n  aggregate_reverse_index -= 1\n  reverse_indices[pull_arr_index] -= 1\n\nreturn lists[0]", "path": "service\\google-app-engine\\geo\\util.py", "repo_name": "timburks/stickup", "stars": 12, "license": "None", "language": "python", "size": 218}
{"docstring": "# test in/out equivalence and parsing\n", "func_signal": "def test_parse(self):\n", "code": "res = json.loads(JSON)\nout = json.dumps(res)\nself.assertEquals(res, json.loads(out))", "path": "service\\google-app-engine\\simplejson\\tests\\test_pass3.py", "repo_name": "timburks/stickup", "stars": 12, "license": "None", "language": "python", "size": 218}
{"docstring": "\"\"\"Calculates the grid of cells formed between the two given cells.\n\nGenerates the set of cells in the grid created by interpolating from the\ngiven Northeast geocell to the given Southwest geocell.\n\nAssumes the Northeast geocell is actually Northeast of Southwest geocell.\n\nArguments:\n  cell_ne: The Northeast geocell string.\n  cell_sw: The Southwest geocell string.\n\nReturns:\n  A list of geocell strings in the interpolation.\n\"\"\"\n# 2D array, will later be flattened.\n", "func_signal": "def interpolate(cell_ne, cell_sw):\n", "code": "cell_set = [[cell_sw]]\n\n# First get adjacent geocells across until Southeast--collinearity with\n# Northeast in vertical direction (0) means we're at Southeast.\nwhile not collinear(cell_set[0][-1], cell_ne, True):\n  cell_tmp = adjacent(cell_set[0][-1], (1, 0))\n  if cell_tmp is None:\n    break\n  cell_set[0].append(cell_tmp)\n\n# Then get adjacent geocells upwards.\nwhile cell_set[-1][-1] != cell_ne:\n  cell_tmp_row = [adjacent(g, (0, 1)) for g in cell_set[-1]]\n  if cell_tmp_row[0] is None:\n    break\n  cell_set.append(cell_tmp_row)\n\n# Flatten cell_set, since it's currently a 2D array.\nreturn [g for inner in cell_set for g in inner]", "path": "service\\google-app-engine\\geo\\geocell.py", "repo_name": "timburks/stickup", "stars": 12, "license": "None", "language": "python", "size": 218}
{"docstring": "\"\"\"Returns the edges of the rectangular region containing all of the\ngiven geocells, sorted by distance from the given point, along with\nthe actual distances from the point to these edges.\n\nArgs:\n  cells: The cells (should be adjacent) defining the rectangular region\n      whose edge distances are requested.\n  point: The point that should determine the edge sort order.\n\nReturns:\n  A list of (direction, distance) tuples, where direction is the edge\n  and distance is the distance from the point to that edge. A direction\n  value of (0,-1), for example, corresponds to the South edge of the\n  rectangular region containing all of the given geocells.\n\"\"\"\n# TODO(romannurik): Assert that lat,lon are actually inside the geocell.\n", "func_signal": "def distance_sorted_edges(cells, point):\n", "code": "boxes = [geocell.compute_box(cell) for cell in cells]\n\nmax_box = geotypes.Box(max([box.north for box in boxes]),\n                       max([box.east for box in boxes]),\n                       min([box.south for box in boxes]),\n                       min([box.west for box in boxes]))\nreturn zip(*sorted([\n    ((0,-1), geomath.distance(geotypes.Point(max_box.south, point.lon),\n                              point)),\n    ((0,1),  geomath.distance(geotypes.Point(max_box.north, point.lon),\n                              point)),\n    ((-1,0), geomath.distance(geotypes.Point(point.lat, max_box.west),\n                              point)),\n    ((1,0),  geomath.distance(geotypes.Point(point.lat, max_box.east),\n                              point))],\n    lambda x, y: cmp(x[1], y[1])))", "path": "service\\google-app-engine\\geo\\util.py", "repo_name": "timburks/stickup", "stars": 12, "license": "None", "language": "python", "size": 218}
{"docstring": "\"\"\"Calculates all of the given geocell's adjacent geocells.\n\nArgs:\n  cell: The geocell string for which to calculate adjacent/neighboring cells.\n\nReturns:\n  A list of 8 geocell strings and/or None values indicating adjacent cells.\n\"\"\"\n", "func_signal": "def all_adjacents(cell):\n", "code": "return [adjacent(cell, d) for d in [NORTHWEST, NORTH, NORTHEAST, EAST,\n                                    SOUTHEAST, SOUTH, SOUTHWEST, WEST]]", "path": "service\\google-app-engine\\geo\\geocell.py", "repo_name": "timburks/stickup", "stars": 12, "license": "None", "language": "python", "size": 218}
{"docstring": "# an invalid box\n", "func_signal": "def test_Box(self):\n", "code": "self.assertRaises(ValueError, geotypes.Box, 95, 0, 0, 0)\n\n# a valid box\nbox = geotypes.Box(37, -122, 34, -125)\nself.assertEquals(37, box.north)\nself.assertEquals(34, box.south)\nself.assertEquals(-122, box.east)\nself.assertEquals(-125, box.west)\n\n# assert north can't be under south\nself.assertRaises(ValueError, box._set_north, 32)\nself.assertRaises(ValueError, box._set_south, 39)\n\nself.assertTrue(isinstance(box.__str__(), str))\n\n# valid boxes\nself.assertEquals(\n    geotypes.Box(37, -122, 34, -125),\n    geotypes.Box(34, -122, 37, -125))", "path": "service\\google-app-engine\\geo\\tests\\geotypes_test.py", "repo_name": "timburks/stickup", "stars": 12, "license": "None", "language": "python", "size": 218}
{"docstring": "\"\"\"Returns whether or not the given geocell string defines a valid geocell.\"\"\"\n", "func_signal": "def is_valid(cell):\n", "code": "return bool(cell and reduce(lambda val, c: val and c in _GEOCELL_ALPHABET,\n                            cell, True))", "path": "service\\google-app-engine\\geo\\geocell.py", "repo_name": "timburks/stickup", "stars": 12, "license": "None", "language": "python", "size": 218}
{"docstring": "\"\"\"Calculates the great circle distance between two points (law of cosines).\n\nArgs:\n  p1: A geotypes.Point or db.GeoPt indicating the first point.\n  p2: A geotypes.Point or db.GeoPt indicating the second point.\n\nReturns:\n  The 2D great-circle distance between the two given points, in meters.\n\"\"\"\n", "func_signal": "def distance(p1, p2):\n", "code": "p1lat, p1lon = math.radians(p1.lat), math.radians(p1.lon)\np2lat, p2lon = math.radians(p2.lat), math.radians(p2.lon)\nreturn RADIUS * math.acos(math.sin(p1lat) * math.sin(p2lat) +\n    math.cos(p1lat) * math.cos(p2lat) * math.cos(p2lon - p1lon))", "path": "service\\google-app-engine\\geo\\geomath.py", "repo_name": "timburks/stickup", "stars": 12, "license": "None", "language": "python", "size": 218}
{"docstring": "\"\"\"Return an ASCII-only JSON representation of a Python string\n\n\"\"\"\n", "func_signal": "def py_encode_basestring_ascii(s):\n", "code": "if isinstance(s, str) and HAS_UTF8.search(s) is not None:\n    s = s.decode('utf-8')\ndef replace(match):\n    s = match.group(0)\n    try:\n        return ESCAPE_DCT[s]\n    except KeyError:\n        n = ord(s)\n        if n < 0x10000:\n            #return '\\\\u{0:04x}'.format(n)\n            return '\\\\u%04x' % (n,)\n        else:\n            # surrogate pair\n            n -= 0x10000\n            s1 = 0xd800 | ((n >> 10) & 0x3ff)\n            s2 = 0xdc00 | (n & 0x3ff)\n            #return '\\\\u{0:04x}\\\\u{1:04x}'.format(s1, s2)\n            return '\\\\u%04x\\\\u%04x' % (s1, s2)\nreturn '\"' + str(ESCAPE_ASCII.sub(replace, s)) + '\"'", "path": "service\\google-app-engine\\simplejson\\encoder.py", "repo_name": "timburks/stickup", "stars": 12, "license": "None", "language": "python", "size": 218}
{"docstring": "# Note that this function is called from _speedups\n", "func_signal": "def errmsg(msg, doc, pos, end=None):\n", "code": "lineno, colno = linecol(doc, pos)\nif end is None:\n    #fmt = '{0}: line {1} column {2} (char {3})'\n    #return fmt.format(msg, lineno, colno, pos)\n    fmt = '%s: line %d column %d (char %d)'\n    return fmt % (msg, lineno, colno, pos)\nendlineno, endcolno = linecol(doc, end)\n#fmt = '{0}: line {1} column {2} - line {3} column {4} (char {5} - {6})'\n#return fmt.format(msg, lineno, colno, endlineno, endcolno, pos, end)\nfmt = '%s: line %d column %d - line %d column %d (char %d - %d)'\nreturn fmt % (msg, lineno, colno, endlineno, endcolno, pos, end)", "path": "service\\google-app-engine\\simplejson\\decoder.py", "repo_name": "timburks/stickup", "stars": 12, "license": "None", "language": "python", "size": 218}
{"docstring": "\"\"\"Return a JSON representation of a Python string\n\n\"\"\"\n", "func_signal": "def encode_basestring(s):\n", "code": "def replace(match):\n    return ESCAPE_DCT[match.group(0)]\nreturn '\"' + ESCAPE.sub(replace, s) + '\"'", "path": "service\\google-app-engine\\simplejson\\encoder.py", "repo_name": "timburks/stickup", "stars": 12, "license": "None", "language": "python", "size": 218}
{"docstring": "\"\"\"Defines a song in the Roomba's repertoire\"\"\"\n", "func_signal": "def define_song(self, songID, notes, durations):\n", "code": "packingScheme = 'B' * (2 * len(notes) + 3)\ncomposition = list(reduce(lambda x, y: x + y, zip(notes, durations)))\nself.send(packingScheme, 140, songID, len(notes), *composition)", "path": "pyroomba\\roomba.py", "repo_name": "jon/pyroomba", "stars": 9, "license": "None", "language": "python", "size": 147}
{"docstring": "\"\"\"Sets the status of the Roomba's LEDs. \n\nColor and intensity must be in the range (0-255). Color ranges from\nsolid green to solid red. Intensity ranges from completely dark to\nfully illuminated. Other LEDs are specified as either on (True) or off\n(False).\"\"\"\n", "func_signal": "def leds(self, color = 0, intensity = 0, check_robot = False, dock = False, spot = False, debris = False):\n", "code": "bits = 0\nbits |= debris and 1 or 0\nbits |= spot and 2 or 0\nbits |= dock and 4 or 0\nbits |= check_robot and 8 or 0\nself.send('BBBB', 139, bits, color, intensity)", "path": "pyroomba\\roomba.py", "repo_name": "jon/pyroomba", "stars": 9, "license": "None", "language": "python", "size": 147}
{"docstring": "\"\"\"Changes the baudrate at which the Roomba communicates\"\"\"\n", "func_signal": "def baud(self, baud_rate):\n", "code": "if not baud_rate in self.BAUD_RATES:\n    raise 'Invalid baud rate specified'\nself.send('BB', 128, self.BAUD_RATES[baud_rate])\nsleep(0.1)\nself.port.setBaudrate(baud_rate)\nsleep(0.1)", "path": "pyroomba\\roomba.py", "repo_name": "jon/pyroomba", "stars": 9, "license": "None", "language": "python", "size": 147}
{"docstring": "\"\"\"Displays up to 4 ASCII characters using the 7-segment displays on scheduling Roombas. \n\nOnly a limited subset of ASCII is supported. See the Roomba OI\nspecification for details, but you can safely use all letters,\nnumbers, and most punctuation.\"\"\"\n", "func_signal": "def display_ascii(self, text):\n", "code": "text = text[0:4].encode('ascii').upper()\npadding = 4 - len(text)\nself.send('B4s', 164, text + ' ' * padding) # struct.pack() will pad for us, but we want space padding, not NUL padding", "path": "pyroomba\\roomba.py", "repo_name": "jon/pyroomba", "stars": 9, "license": "None", "language": "python", "size": 147}
{"docstring": "\"\"\"Take full control of the robot; wait for a tenth of a second to allow the Roomba to change modes.\n\nFull control mode disables all of the Roomba's self-preservation\nfeatures. In full mode the robot will be perfectly happy to burn out\nits motors or run off a cliff.\"\"\"\n", "func_signal": "def full(self):\n", "code": "self.cmd(132)\nsleep(0.1)", "path": "pyroomba\\roomba.py", "repo_name": "jon/pyroomba", "stars": 9, "license": "None", "language": "python", "size": 147}
{"docstring": "\"\"\"Controls the Roomba's motors in PWM (pulse-width modulation) mode.\n\nThis effectively specifies the pulse-width directly to the motor\ncontroller, with values ranging from -255 to 255. Positive widths\nspecify forward motion; negative widths specify backward motion.\n\nPulse-width modulation is a method of power control in which motors\n(or any current sink, but in our case motors) are pulsed alternately\nto full on and full off repeatedly. Control can be thought of as\noccuring in fixed time intervals (i.e., the period of the control\nsignal). The duty cycle (pulse-width) specifies for what fraction of\neach of these periods the motor will be on. So for the Roomba a\npulse-width of 1 means the motor is fully powered 1/255th of the\ntime.\"\"\"\n", "func_signal": "def drive_pwm(self, right, left):\n", "code": "if abs(right) > 255:\n    right = 255 * sign(right)\nif abs(left) > 255:\n    left = 255 * sign(left)\nself.send('>Bhh', 146, right, left)", "path": "pyroomba\\roomba.py", "repo_name": "jon/pyroomba", "stars": 9, "license": "None", "language": "python", "size": 147}
{"docstring": "\"\"\"Instantiate a new Roomba on a given port at a given speed.\n\nArguments are:\n port: The serial port to which the robot is connected (e.g., \n    '/dev/tty.roomba' or 'COM1')\n baud: The baud rate or speed at which to communicate with the robot.\n    This defaults to 115200 which should be correct for 500 series\n    robots. Ealier models communicated at 57600.\"\"\"\n", "func_signal": "def __init__(self, port, baud = 115200, timeout = 0.030, serial_port = None):\n", "code": "self._running = False\nif not serial_port:\n    self.port = Serial(port, baudrate = baud, timeout = timeout) # Anything we ask the robot to do it should reply within 0.015 seconds. We give it a buffer of twice that.\nelse:\n    self.port = serial_port # Mostly useful for testing, but also if\n    # you have a pyserial compliant class for communicating over some\n    # other medium. Mostly it needs to support blocking reads and\n    # writes, as well as .flushInput()", "path": "pyroomba\\roomba.py", "repo_name": "jon/pyroomba", "stars": 9, "license": "None", "language": "python", "size": 147}
{"docstring": "\"\"\"Reads a list of sensor values and returns the associated dictionary\"\"\"\n#packet_list = [ packet for packet, format, name in sensors ]\n", "func_signal": "def _read_sensor_list(self, sensors):\n", "code": "formats = [ format for packet, format, name in sensors ]\nnames = [ name for packet, format, name in sensors ]\n\nresponse_format = '>' + ''.join(formats)\nresponse = self.port.read(calcsize(response_format))\nvalues = unpack(response_format, response)\nreturn dict(zip(names, values))", "path": "pyroomba\\roomba.py", "repo_name": "jon/pyroomba", "stars": 9, "license": "None", "language": "python", "size": 147}
{"docstring": "\"\"\"Controls the Roomba's cleaning motors in PWM mode. \n\nSee the documentation for drive_pwm for a description of PWM. Ranges\nfor the main and side motors are -127 to 127; range for the vacuum is\n0 to 127\"\"\"\n", "func_signal": "def motors_pwm(self, main = 0, side = 0, vacuum = 0):\n", "code": "if abs(main) > 127:\n    main = 127 * sign(main)\nif abs(side) > 127:\n    side = 127 * sign(side)\nif vacuum < 0:\n    vacuum = 0\nif vacuum > 127:\n    vacuum = 127\nself.send('BbbB', 144, main, side, vacuum)", "path": "pyroomba\\roomba.py", "repo_name": "jon/pyroomba", "stars": 9, "license": "None", "language": "python", "size": 147}
{"docstring": "\"\"\"Pauses the sample stream (if any) coming from the Roomba\"\"\"\n", "func_signal": "def pause_stream(self):\n", "code": "self.send('BB', 150, 0)\nsleep(0.1)\nself.port.flushInput()", "path": "pyroomba\\roomba.py", "repo_name": "jon/pyroomba", "stars": 9, "license": "None", "language": "python", "size": 147}
{"docstring": "\"\"\"Starts streaming sensor data from the Roomba at a rate of one reading every 15ms (the Roomba's internal update rate).\n\nAfter this method has executed you should call poll() at least once\nevery 15ms to access the returned sensor data. To halt the stream call\nstream_pause(). To result the stream with the same packet list call\nstream_resume().\"\"\"\n", "func_signal": "def stream_samples(self, *sensors):\n", "code": "packet_list = [ packet for packet, format, name in sensors ]\ncount = len(packet_list)\nformat = 'BB' + ('B' * count)\nself.send(format, 148, count, *packet_list)", "path": "pyroomba\\roomba.py", "repo_name": "jon/pyroomba", "stars": 9, "license": "None", "language": "python", "size": 147}
{"docstring": "\"\"\"Request a single sensor packet\"\"\"\n", "func_signal": "def sensors(self, sensor):\n", "code": "sensor_id, format, name = sensor\nself.send('BB', 142, sensor_id)\n# It's ugly to check for things like this\nif isinstance(format, str):\n    format = '>' + format\n    return self.port.read(calcsize(format))\nelif isinstance(format, list):\n    # Some packets return a list of results (particularly on SCI robots)\n    return self._read_sensor_list(format)\nelse:\n    raise 'Unknown sensor format type'", "path": "pyroomba\\roomba.py", "repo_name": "jon/pyroomba", "stars": 9, "license": "None", "language": "python", "size": 147}
{"docstring": "\"\"\"Returns the sign of x, either 1 or -1\"\"\"\n", "func_signal": "def sign(x):\n", "code": "if x > 0:\n    return 1\nif x < 0:\n    return -1\nreturn 0", "path": "pyroomba\\roomba.py", "repo_name": "jon/pyroomba", "stars": 9, "license": "None", "language": "python", "size": 147}
{"docstring": "\"\"\"Converts direct motor drive commands into radius/speed commands\"\"\"\n", "func_signal": "def drive_direct(self, right, left):\n", "code": "if left == right:\n    # Special case to handle equal\n    self.drive(left, 0x8000)\n    return\nelif abs(left) == abs(right):\n    # The only way this is true if the above wasn't is if the wheels are equal and opposite\n    if left < right:\n        self.drive(right, -1)\n    else:\n        self.drive(left, 1)\n    return\naverage = int((abs(right) + abs(left)) / 2)\nslope = (right - left) / (2 * self._radius)\ny_intercept = right - slope*self._radius\nradius = y_intercept / slope\nself.drive(sign(left + right) * average, radius)", "path": "pyroomba\\roomba.py", "repo_name": "jon/pyroomba", "stars": 9, "license": "None", "language": "python", "size": 147}
{"docstring": "\"\"\"Takes a blocking sample of a collection of Roomba's sensors, specified using the constants defined in this module\"\"\"\n", "func_signal": "def query_list(self, *sensors):\n", "code": "packet_list = [ packet for packet, format, name in sensors ]\ncount = len(packet_list)\nformat = 'BB' + 'B' * count\nself.send(format, 149, count, *packet_list)\n\nreturn self._read_sensor_list(sensors)", "path": "pyroomba\\roomba.py", "repo_name": "jon/pyroomba", "stars": 9, "license": "None", "language": "python", "size": 147}
{"docstring": "\"\"\"Simulates pressing the Roombas buttons for at most 1/6th of a second. \n\nPass True to push the button, False to release it (or not push it)\"\"\"\n", "func_signal": "def buttons(self, clean = False, spot = False, dock = False, minute = False, hour = False, day = False, schedule = False, clock = False):\n", "code": "bits = 0\nbits |= clean and 1 or 0\nbits |= spot and 2 or 0\nbits |= dock and 4 or 0\nbits |= minute and 8 or 0\nbits |= hour and 0x10 or 0\nbits |= day and 0x20 or 0\nbits |= schedule and 0x40 or 0\nbits |= clock and 0x80 or 0\nself.send('BB', 165, bits)", "path": "pyroomba\\roomba.py", "repo_name": "jon/pyroomba", "stars": 9, "license": "None", "language": "python", "size": 147}
{"docstring": "\"\"\"Start controlling the robot; wait for a tenth of a second to allow the Roomba to change modes\"\"\"\n", "func_signal": "def start(self):\n", "code": "self.cmd(128)\nsleep(0.1)  \nself.cmd(130) # Necessary for SCI robots (no harm on OI?)\nsleep(0.1)", "path": "pyroomba\\roomba.py", "repo_name": "jon/pyroomba", "stars": 9, "license": "None", "language": "python", "size": 147}
{"docstring": "\"\"\"Polls the robot at most once every 15ms, updating stored sensor data and optionally calling an idle function\"\"\"\n", "func_signal": "def run(self, sensors = sensor_list.ALL_SCI, idle_func = None):\n", "code": "if self._running:\n    return\nself._running = True\nwhile self._running:\n    start = time()\n    self.latest = self.sensors(sensors)\n    if idle_func:\n        idle_func()\n    stop = time()\n    remaining = 0.015 - (start - stop)\n    if remaining > 0:\n        sleep(remaining)", "path": "pyroomba\\roomba.py", "repo_name": "jon/pyroomba", "stars": 9, "license": "None", "language": "python", "size": 147}
{"docstring": "\"\"\"Closes the serial port used to control the Roomba\"\"\"\n", "func_signal": "def close(self):\n", "code": "self.stop()\nself.port.close()", "path": "pyroomba\\roomba.py", "repo_name": "jon/pyroomba", "stars": 9, "license": "None", "language": "python", "size": 147}
{"docstring": "\"\"\"Turns the Roomba's cleaning motors (i.e., brushes and vacuum) on or off at full speed. \n\nA value of True indicates that the motor should be\nturned on. The value of reverse_main determines the direction of\nthe main brush. The value of side_clockwise determines the rotation\ndirection of the side brush.\"\"\"\n", "func_signal": "def motors(self, main = False, side = False, vacuum = False, reverse_main = False, side_clockwise = False):\n", "code": "state = 0\nstate |= side and 1 or 0\nstate |= vacuum and 2 or 0\nstate |= main and 4 or 0\nstate |= side_clockwise and 8 or 0\nstate |= reverse_main and 16 or 0\nself.send('BB', 138, state)", "path": "pyroomba\\roomba.py", "repo_name": "jon/pyroomba", "stars": 9, "license": "None", "language": "python", "size": 147}
{"docstring": "\"\"\"Return the style uniquely identified by the name/family pair. If\nthe argument is already a style object, it will return it.\n\nIf the name is None, the default style is fetched.\n\nIf the name is not the internal name but the name you gave in the\ndesktop application, set display_name to True.\n\nArguments:\n\n    name_or_element -- unicode, odf_style or None\n\n    family -- 'paragraph', 'text',  'graphic', 'table', 'list',\n              'number', 'page-layout', 'master-page'\n\n    display_name -- bool\n\nReturn: odf_style or None if not found\n\"\"\"\n", "func_signal": "def get_style(self, family, name_or_element=None, display_name=False):\n", "code": "for context in self._get_style_contexts(family):\n    if context is None:\n        continue\n    style = context.get_style(family,\n            name_or_element=name_or_element,\n            display_name=display_name)\n    if style is not None:\n        return style\nreturn None", "path": "tools\\lpod\\styles.py", "repo_name": "kiniou/blender-smooth-slides", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 7770}
{"docstring": "\"\"\"Match an intermediate regex.\n\"\"\"\n", "func_signal": "def test_intermediate_regex(self):\n", "code": "match = self.paragraph.match(u'moustache (blanche|rouge)')\nreturn self.assertTrue(match)", "path": "tools\\lpod\\test\\test_element.py", "repo_name": "kiniou/blender-smooth-slides", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 7770}
{"docstring": "\"\"\"Search text in a span.\n\"\"\"\n", "func_signal": "def test_match_span(self):\n", "code": "pos = self.span.search(u'moust')\nreturn self.assertEqual(pos, 0)", "path": "tools\\lpod\\test\\test_element.py", "repo_name": "kiniou/blender-smooth-slides", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 7770}
{"docstring": "\"\"\"Return the style uniquely identified by the name/family pair. If\nthe argument is already a style object, it will return it.\n\nIf the name is not the internal name but the name you gave in the\ndesktop application, use display_name instead.\n\nArguments:\n\n    family -- 'paragraph', 'text', 'graphic', 'table', 'list',\n              'number'\n\n    name_or_element -- unicode or odf_style\n\n    display_name -- unicode\n\nReturn: odf_style or None if not found\n\"\"\"\n", "func_signal": "def get_style(self, family, name_or_element=None, display_name=None):\n", "code": "for context in self._get_style_contexts(family):\n    if context is None:\n        continue\n    style = context.get_style(family,\n            name_or_element=name_or_element,\n            display_name=display_name)\n    if style is not None:\n        return style\nreturn None", "path": "tools\\lpod\\content.py", "repo_name": "kiniou/blender-smooth-slides", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 7770}
{"docstring": "\"\"\"Return the list of styles in the Content part, optionally limited\nto the given family.\n\nArguments:\n\n    family -- str\n\nReturn: list of odf_style\n\"\"\"\n", "func_signal": "def get_style_list(self, family=None, automatic=False):\n", "code": "result = []\nfor context in self._get_style_contexts(family, automatic=automatic):\n    if context is None:\n        continue\n    result.extend(context.get_style_list(family=family))\nreturn result", "path": "tools\\lpod\\styles.py", "repo_name": "kiniou/blender-smooth-slides", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 7770}
{"docstring": "# Copy extra parts (images...)\n", "func_signal": "def _add_pictures(document, output_doc):\n", "code": "container = document.container\nfor partname in container.get_contents():\n    if partname.startswith('Pictures/'):\n        data = container.get_part(partname)\n        # Suppose uniqueness\n        output_doc.container.set_part(partname, data)", "path": "tools\\lpod\\scripts\\lpod-merge.py", "repo_name": "kiniou/blender-smooth-slides", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 7770}
{"docstring": "\"\"\"Match text in a span from the parent paragraph.\n\"\"\"\n", "func_signal": "def test_match_inner_span(self):\n", "code": "match = self.paragraph.match(u'roug')\nreturn self.assertTrue(match)", "path": "tools\\lpod\\test\\test_element.py", "repo_name": "kiniou/blender-smooth-slides", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 7770}
{"docstring": "\"\"\"Search text in a paragraph.\n\"\"\"\n", "func_signal": "def test_search_paragraph(self):\n", "code": "pos = self.paragraph.search(u'\u00e8re')\nreturn self.assertEqual(pos, 4)", "path": "tools\\lpod\\test\\test_element.py", "repo_name": "kiniou/blender-smooth-slides", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 7770}
{"docstring": "# Text mode\n", "func_signal": "def init_doc(mimetype):\n", "code": "if mimetype == \"application/vnd.oasis.opendocument.text\":\n    output_doc = odf_new_document_from_type(\"text\")\n\n    # Begin with a TOC\n    output_body = output_doc.get_body()\n    output_body.append_element(odf_create_toc())\n# Spreadsheet mode\nelif mimetype in (\"application/vnd.oasis.opendocument.spreadsheet\",\n                  \"text/csv\"):\n    output_doc = odf_new_document_from_type(\"spreadsheet\")\n# Presentation mode\nelse:\n    output_doc = odf_new_document_from_type(\"presentation\")\n\nreturn output_doc", "path": "tools\\lpod\\scripts\\lpod-merge.py", "repo_name": "kiniou/blender-smooth-slides", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 7770}
{"docstring": "\"\"\"Search text in a span from the parent paragraph.\n\"\"\"\n", "func_signal": "def test_match_inner_span(self):\n", "code": "pos = self.paragraph.search(u'roug')\nreturn self.assertEqual(pos, 29)", "path": "tools\\lpod\\test\\test_element.py", "repo_name": "kiniou/blender-smooth-slides", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 7770}
{"docstring": "\"\"\"Apply the given style to text content matching the regex OR the\npositional arguments offset and length.\n\nArguments:\n\n    style -- style element or name\n\n    regex -- unicode regular expression\n\n    offset -- int\n\n    length -- int\n\"\"\"\n# XXX FIX ME cyclic import\n", "func_signal": "def set_span(self, style, regex=None, offset=None, length=0):\n", "code": "from span import odf_create_span\n\nif isinstance(style, odf_style):\n    style = style.get_style_name()\nif offset:\n    # XXX quickly hacking the offset\n    text = self.get_text()\n    if length:\n        regex = text[offset:offset + length]\n    else:\n        regex = text[offset:]\n    regex = escape(regex)\nif regex:\n    pattern = compile(unicode(regex))\n    for text in self.xpath('descendant::text()'):\n        # Static information about the text node\n        container = text.get_parent()\n        wrapper = container.get_parent()\n        is_text = text.is_text()\n        # Group positions are calculated and static, so apply in\n        # reverse order to preserve positions\n        for group in reversed(list(pattern.finditer(text))):\n            start, end = group.span()\n            # Do not use the text node as it changes at each loop\n            if is_text:\n                text = container.get_text()\n            else:\n                text = container.get_tail()\n            before = text[:start]\n            match = text[start:end]\n            after = text[end:]\n            span = odf_create_span(match, style=style)\n            span.set_tail(after)\n            if is_text:\n                container.set_text(before)\n                # Insert as first child\n                container.insert_element(span, position=0)\n            else:\n                container.set_tail(before)\n                # Insert as next sibling\n                index = wrapper.index(container)\n                wrapper.insert_element(span, position=index + 1)", "path": "tools\\lpod\\paragraph.py", "repo_name": "kiniou/blender-smooth-slides", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 7770}
{"docstring": "\"\"\"Match text in a span.\n\"\"\"\n", "func_signal": "def test_match_span(self):\n", "code": "match = self.span.match(u'moust')\nreturn self.assertTrue(match)", "path": "tools\\lpod\\test\\test_element.py", "repo_name": "kiniou/blender-smooth-slides", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 7770}
{"docstring": "\"\"\"Match a simple regex.\n\"\"\"\n", "func_signal": "def test_simple_regex(self):\n", "code": "match = self.paragraph.match(u'che roug')\nreturn self.assertTrue(match)", "path": "tools\\lpod\\test\\test_element.py", "repo_name": "kiniou/blender-smooth-slides", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 7770}
{"docstring": "\"\"\"Create a paragraph element of the given style containing the optional\ngiven text.\n\nArguments:\n\n    style -- unicode\n\n    text -- unicode\n\nReturn: odf_element\n\"\"\"\n", "func_signal": "def odf_create_paragraph(text=None, style=None):\n", "code": "element = odf_create_element('<text:p/>')\nif text:\n    element.set_text(text)\nif style:\n    element.set_text_style(style)\nreturn element", "path": "tools\\lpod\\paragraph.py", "repo_name": "kiniou/blender-smooth-slides", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 7770}
{"docstring": "\"\"\"Match text in a paragraph.\n\"\"\"\n", "func_signal": "def test_match_paragraph(self):\n", "code": "match = self.paragraph.match(u'\u00e8re')\nreturn self.assertTrue(match)", "path": "tools\\lpod\\test\\test_element.py", "repo_name": "kiniou/blender-smooth-slides", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 7770}
{"docstring": "\"\"\"Return the list of styles in the Content part, optionally limited\nto the given family.\n\nArguments:\n\n    family -- str\n\nReturn: list of odf_style\n\"\"\"\n", "func_signal": "def get_style_list(self, family=None):\n", "code": "result = []\nfor context in self._get_style_contexts(family):\n    if context is None:\n        continue\n    result.extend(context.get_style_list(family=family))\nreturn result", "path": "tools\\lpod\\content.py", "repo_name": "kiniou/blender-smooth-slides", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 7770}
{"docstring": "\"\"\"Test a regex that doesn't match.\n\"\"\"\n", "func_signal": "def test_failing_match(self):\n", "code": "pos = self.paragraph.search(u'Le P\u00e8re moustache')\nreturn self.assert_(pos is None)", "path": "tools\\lpod\\test\\test_element.py", "repo_name": "kiniou/blender-smooth-slides", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 7770}
{"docstring": "#pprint(self.get_attributes() )\n", "func_signal": "def get_formatted_text(self,context):\n", "code": "text = {}\ntext['text-content'] = u\"\\n\"\nreturn text", "path": "odp_prepare.py", "repo_name": "kiniou/blender-smooth-slides", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 7770}
{"docstring": "\"\"\"Test a regex that doesn't match.\n\"\"\"\n", "func_signal": "def test_failing_match(self):\n", "code": "match = self.paragraph.match(u'Le P\u00e8re moustache')\nreturn self.assertFalse(match)", "path": "tools\\lpod\\test\\test_element.py", "repo_name": "kiniou/blender-smooth-slides", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 7770}
{"docstring": "\"\"\"Search an intermediate regex.\n\"\"\"\n", "func_signal": "def test_intermediate_regex(self):\n", "code": "pos = self.paragraph.search(u'moustache (blanche|rouge)')\nreturn self.assertEqual(pos, 19)", "path": "tools\\lpod\\test\\test_element.py", "repo_name": "kiniou/blender-smooth-slides", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 7770}
{"docstring": "\"\"\"\nRemove a host from groups\n\"\"\"\n", "func_signal": "def remove_host(self,group,host,save=True):\n", "code": "if not self.__groups.has_key(group) or not host in self.__groups[group]:\n    return (False,\"Non existing group or name\")\n\n#remove the machine from there\nself.__groups[group].remove(host)\n#save to config file\nif save:\n    self.save_changes()\n\nreturn (True,'')", "path": "func\\overlord\\group\\conf_backend.py", "repo_name": "makkalot/func", "stars": 15, "license": "gpl-2.0", "language": "python", "size": 1769}
{"docstring": "\"\"\"\nReturns a list of all signed certs on this minion\n\"\"\"\n", "func_signal": "def get_signed_certs(self):\n", "code": "cm = certmaster.CertMaster()\nreturn cm.get_signed_certs()", "path": "func\\minion\\modules\\certmastermod.py", "repo_name": "makkalot/func", "stars": 15, "license": "gpl-2.0", "language": "python", "size": 1769}
{"docstring": "\"\"\"\nAdds a vlan to an interface\n\nKeyword arguments:\ninterface -- interface to add vlan to (string, for example: \"eth0\")\nvlanid -- ID of the vlan to add (string, for example: \"1100\")\n\"\"\"\n", "func_signal": "def add(self, interface, vlanid):\n", "code": "if vlanid not in self.options.ignorevlans:\n    exitcode = os.spawnv(os.P_WAIT, self.options.vconfig, [ self.options.vconfig, \"add\", interface, str(vlanid)] )\nelse:\n    exitcode = -1\n\nreturn exitcode", "path": "func\\minion\\modules\\vlan.py", "repo_name": "makkalot/func", "stars": 15, "license": "gpl-2.0", "language": "python", "size": 1769}
{"docstring": "\"\"\"\nReturns a dictionary of permanent VLANs, return format is the same as\nin the list() method.\n\"\"\"\n", "func_signal": "def list_permanent(self):\n", "code": "retlist = {}\npattern = re.compile('ifcfg-([a-z0-9]+)\\.([0-9]+)')\n\nfor item in os.listdir(\"/etc/sysconfig/network-scripts\"):\n    match = pattern.match(item)\n    if match:\n        interface = match.group(1)\n        vlanid = match.group(2)\n\n        if interface not in retlist:\n            retlist[interface] = [ vlanid ]\n        else:\n            retlist[interface].append(vlanid)\n\nreturn retlist", "path": "func\\minion\\modules\\vlan.py", "repo_name": "makkalot/func", "stars": 15, "license": "gpl-2.0", "language": "python", "size": 1769}
{"docstring": "\"\"\"\nApplies the supplied configuration to the system.\n\nKeyword arguments:\nconfiguration  -- dictionary, elements should look like this: key: interface, value: [id1, id2, id3]\n\"\"\"\n\n", "func_signal": "def make_it_so(self, configuration):\n", "code": "currentconfig = self.list()\nnewconfig = {}\n\n# Convert the supplied configuration to strings.\nfor interface, vlans in configuration.iteritems():\n    newconfig[interface] = []\n    for vlan in vlans:\n        newconfig[interface].append(str(vlan))\n\nconfiguration = newconfig\n\n# First, remove all VLANs present in current configuration, that are\n# not present in new configuration.\nfor interface, vlans in currentconfig.iteritems():\n    if interface not in configuration:\n        # Remove all the vlans from this interface\n        for vlan in vlans:\n            self.delete(interface, vlan)\n\n    else:\n        for vlan in vlans:\n            if vlan not in configuration[interface]:\n                # VLAN not in new configuration, remove it.\n                self.delete(interface, vlan)\n\n# Second, add all VLANs present in new configuration, that are not\n# present in current configuration\nfor interface, vlans in configuration.iteritems():\n    if interface not in currentconfig:\n        # Add all VLANs for this interface\n        for vlan in vlans:\n            self.add(interface, vlan)\n\n    else:\n        for vlan in vlans:\n            if vlan not in currentconfig[interface]:\n                # VLAN not in current configuration, add it.\n                self.add(interface, vlan)\n\n# Todo: Compare the current configuration to the supplied configuration\nreturn self.list()", "path": "func\\minion\\modules\\vlan.py", "repo_name": "makkalot/func", "stars": 15, "license": "gpl-2.0", "language": "python", "size": 1769}
{"docstring": "\"\"\"\nRemove the peer certificates for each host in 'peers'\n\"\"\"\n", "func_signal": "def remove_peer_certs(self, peers):\n", "code": "cm = certmaster.CertMaster()\nfor p in peers:\n    certname = \"%s.%s\" % (p, cm.cfg.cert_extension)\n    certname = os.path.join(cm.cfg.peerroot, certname)\n    try:\n        os.unlink(certname)\n    except OSError:\n        # cert doesn't exist\n        pass\nreturn True", "path": "func\\minion\\modules\\certmastermod.py", "repo_name": "makkalot/func", "stars": 15, "license": "gpl-2.0", "language": "python", "size": 1769}
{"docstring": "\"\"\"\nExport certmaster module \n\"\"\"\n\n", "func_signal": "def register_method_args(self):\n", "code": "list_of_hosts = {\n        'type':'list',\n        'optional':False,\n        'description':'A list of hosts to apply the operation'\n        }\n\nreturn {\n        'get_hosts_to_sign':{\n            'args':{},\n            'description':\"Returns a list of hosts to sign\"\n            },\n        'get_signed_certs':{\n            'args':{},\n            'description':\"Get the certs you signed\"\n            },\n        'sign_hosts':{\n            'args':{\n                'list_of_hosts':list_of_hosts\n                },\n            'description':\"Sign a list of hosts\"\n            },\n        'cleanup_hosts':{\n            'args':{\n                'list_of_hosts':list_of_hosts\n                },\n            'description':\"Clean the certs for specified hosts\"\n            },\n        'peering_enabled':{\n            'args':{},\n            'description':\"Whether or not peering is enabled\"\n            },\n        'known_peers':{\n            'args':{},\n            'description':\"What peers are known\"\n            },\n        'remove_peer_certs':{\n            'args':{\n                'peers':'List of peers to remove',\n                },\n            'description':'Remove peer certificate for one or more peers'\n            },\n        'copy_peer_cert':{\n            'args':{\n                'peer':'Name of the peer',\n                'certblob':'Certificate data',\n                },\n            'description':'Copy certblob for peer'\n            }\n        }", "path": "func\\minion\\modules\\certmastermod.py", "repo_name": "makkalot/func", "stars": 15, "license": "gpl-2.0", "language": "python", "size": 1769}
{"docstring": "\"\"\"\nMarks a vlan interface as down\n\nKeyword arguments:\ninterface -- interface this vlan resides on (string, example: \"eth0\")\nvlanid -- ID for this vlan (string, example: \"1100\")\n\"\"\"\n", "func_signal": "def down(self, interface, vlanid):\n", "code": "vintfname = interface + \".\" + str(vlanid)\nif vlanid not in self.options.ignorevlans:\n    exitcode = os.spawnv(os.P_WAIT, self.options.ip, [ self.options.ip, \"link\", \"set\", vintfname, \"down\" ])\nelse:\n    exitcode = -1\n\nreturn exitcode", "path": "func\\minion\\modules\\vlan.py", "repo_name": "makkalot/func", "stars": 15, "license": "gpl-2.0", "language": "python", "size": 1769}
{"docstring": "\"\"\"\nDeletes a vlan from an interface.\n\nKeyword arguments:\ninterface -- Interface to delete vlan from (string, example: \"eth0\")\nvlanid -- Vlan ID to remove (string, example: \"1100\")\n\"\"\"\n", "func_signal": "def delete(self, interface, vlanid):\n", "code": "vintfname = interface + \".\" + str(vlanid)\nif vlanid not in self.options.ignorevlans:\n    exitcode = os.spawnv(os.P_WAIT, self.options.vconfig, [ self.options.vconfig, \"rem\", vintfname] )\nelse:\n    exitcode = -1\n\nreturn exitcode", "path": "func\\minion\\modules\\vlan.py", "repo_name": "makkalot/func", "stars": 15, "license": "gpl-2.0", "language": "python", "size": 1769}
{"docstring": "\"\"\"\n...\n\"\"\"\n", "func_signal": "def cleanup_hosts(self, list_of_hosts):\n", "code": "list_of_hosts = self.__listify(list_of_hosts)\ncm = certmaster.CertMaster()\nfor x in list_of_hosts:\n   cm.remove_this_cert(x)\nreturn True", "path": "func\\minion\\modules\\certmastermod.py", "repo_name": "makkalot/func", "stars": 15, "license": "gpl-2.0", "language": "python", "size": 1769}
{"docstring": "\"\"\"\nWrite changes to disk\n\"\"\"\n", "func_signal": "def save_changes(self):\n", "code": "for group_name,group_hosts in self.__groups.iteritems():\n    #if we have added a new group add it to config object\n    if not group_name in self.cp.sections():\n        self.cp.add_section(group_name)\n    self.cp.set(group_name,\"host\",\",\".join(group_hosts))\n\n#store tha changes\nconf_file = open(self.config, \"w\")\nself.cp.write(conf_file)", "path": "func\\overlord\\group\\conf_backend.py", "repo_name": "makkalot/func", "stars": 15, "license": "gpl-2.0", "language": "python", "size": 1769}
{"docstring": "\"\"\"\nA beautiful recursive tree like hash producer\n\"\"\"\n", "func_signal": "def produce_res_rec(result_pack):\n", "code": "send_list = []\nglobal global_max\n#print \"The pack is counter:\",counter\n#print \"The pack is result_pack:\",result_pack\n#print \"The pack is global_result:\",global_result\n#the final step of the execution\nif type(result_pack) != list and type(result_pack) != dict:\n    global_max = global_max + 1\n    return {'id':global_max,'text':str(result_pack)}\n\nelif type(result_pack) == list :\n    for result_list in result_pack:\n        if type(result_list) == list:\n            #if there is a new list then the new parent trick\n            global_max = global_max +1\n            tmp_parent = {}\n            tmp_parent['id'] = global_max\n            tmp_parent['text'] = 'leaf_result%s'%(global_max)\n\n            tmp_list_result = produce_res_rec(result_list)\n\n            if tmp_list_result and type(tmp_list_result) == list:\n                tmp_parent['item'] = []\n                tmp_parent['item'].extend(tmp_list_result)\n            elif tmp_list_result:\n                tmp_parent['item'] = []\n                tmp_parent['item'].append(tmp_list_result)\n            #appended to the parent\n            send_list.append(tmp_parent)\n\n        else:\n            tmp_list_result = produce_res_rec(result_list)\n            if type(tmp_list_result) == list:\n                send_list.extend(tmp_list_result)\n            else:\n                send_list.append(tmp_list_result)\n\nelif type(result_pack) == dict :\n    for key_result,value_result in result_pack.iteritems():\n        #a new key added\n        global_max = global_max +1\n        tmp_parent = {}\n        tmp_parent ['id'] = global_max\n        tmp_parent ['text'] = str(key_result)\n       \n        tmp_dict_res = produce_res_rec(value_result)\n            \n        if tmp_dict_res and type(tmp_dict_res) == list :\n            tmp_parent ['item'] = []\n            tmp_parent['item'].extend(tmp_dict_res)\n        elif tmp_dict_res:\n            tmp_parent ['item'] = []\n            tmp_parent['item'].append(tmp_dict_res)\n\n        send_list.append(tmp_parent)\n\nelse: #shouldnt come here !\n    return {}\n\nreturn send_list", "path": "funcweb\\funcweb\\result_handler.py", "repo_name": "makkalot/func", "stars": 15, "license": "gpl-2.0", "language": "python", "size": 1769}
{"docstring": "\"\"\"\nIdentify common network errors that mean we cannot connect to the server\n\"\"\"\n\n", "func_signal": "def canIgnoreSocketError(e):\n", "code": "try:\n    if e[0] == 111:     # Connection refused\n        return True\n    elif e[0] == 104:   # Connection reset by peer\n        return True\n    elif e[0] == 61:    # Connection refused\n        return True\nexcept IndexError:\n    return True\n\nreturn False", "path": "func\\CommonErrors.py", "repo_name": "makkalot/func", "stars": 15, "license": "gpl-2.0", "language": "python", "size": 1769}
{"docstring": "\"\"\"\nAdds a host to a group\n\"\"\"\n", "func_signal": "def add_host_to_group(self,group,host,save=True):\n", "code": "if not self.__groups.has_key(group):\n    self.__groups[group] = []\n\n#dont want duplicates\nif not host in self.__groups[group]:\n    if host:\n        self.__groups[group].append(host)\nelse:\n    return (False,\"Host is already in database : %s\"%host)\n\nif save:\n    self.save_changes()\nreturn (True,\"\")", "path": "func\\overlord\\group\\conf_backend.py", "repo_name": "makkalot/func", "stars": 15, "license": "gpl-2.0", "language": "python", "size": 1769}
{"docstring": "\"\"\"\n...\n\"\"\"\n", "func_signal": "def get_hosts_to_sign(self):\n", "code": "cm = certmaster.CertMaster()\nreturn cm.get_csrs_waiting()", "path": "func\\minion\\modules\\certmastermod.py", "repo_name": "makkalot/func", "stars": 15, "license": "gpl-2.0", "language": "python", "size": 1769}
{"docstring": "\"\"\"\nPermanently removes a vlan from an interface. This is useful when the vlan is configured through an ifcfg-file.\n\nKeyword arguments:\ninterface -- interface to delete vlan from (string, example: \"eth0\")\nvlanid -- Vlan ID to remove (string, example: \"1100\")\n\"\"\"\n\n", "func_signal": "def delete_permanent(self, interface, vlanid):\n", "code": "if vlanid not in self.options.ignorevlans:\n    device = \"%s.%s\" % (interface, vlanid)\n    filename = \"/etc/sysconfig/network-scripts/ifcfg-%s\" % device\n    self.down(interface, vlanid)\n    self.delete(interface, vlanid)\n    if os.path.exists(filename):\n        os.remove(filename)\n    exitcode = 0\nelse:\n    exitcode = -1\nreturn exitcode", "path": "func\\minion\\modules\\vlan.py", "repo_name": "makkalot/func", "stars": 15, "license": "gpl-2.0", "language": "python", "size": 1769}
{"docstring": "\"\"\"\nGet a list of groups\n\n@param pattern : You may request to get an exact host or\n                 a one in proper pattern .\n@param exact   : Related to pattern if you should do exact \n                 matching or related one.\n@param exclude : A list to be excluded from final set\n\n\"\"\"\n", "func_signal": "def get_groups(self,pattern=None,exact=True,exclude=None):\n", "code": "if not pattern:\n    #return all of them\n    if not exclude:\n        return self.__groups.keys()\n    else:\n        #get the difference of 2 sets \n        return list(set(self.__groups.keys()).difference(set(exclude)))\nelse:\n    #it seems there is a pattern\n    if exact:\n        #there is no mean to check here for\n        #exclude list ...\n        for g in self.__groups.keys():\n            if g == pattern:\n                return [g]\n        return []\n\n    else:#not exact match\n        if not exclude:#there is no list to exclude\n            tmp_l = set()\n            for g in self.__groups.keys():\n                if pattern.lower() in g.lower():\n                    tmp_l.add(g)\n            return list(tmp_l)\n        else:\n            tmp_l = set()\n            for g in self.__groups.keys():\n                if pattern.lower() in g.lower():\n                    tmp_l.add(g)\n            #get the difference of 2 sets\n            return list(tmp_l.difference(set(exclude)))\n\n    #shouldnt come here actually\n    return []", "path": "func\\overlord\\group\\conf_backend.py", "repo_name": "makkalot/func", "stars": 15, "license": "gpl-2.0", "language": "python", "size": 1769}
{"docstring": "\"\"\"\nInstall certblob as the certificate for peer\n\"\"\"\n", "func_signal": "def copy_peer_cert(self, peer, certblob):\n", "code": "import func.minion.modules.copyfile as copyfile\ncm = certmaster.CertMaster()\ncertname = '%s.%s' % (peer, cm.cfg.cert_extension)\npath = os.path.join(cm.cfg.peerroot, certname)\ncf = copyfile.CopyFile()\nreturn cf.copyfile(path, certblob)", "path": "func\\minion\\modules\\certmastermod.py", "repo_name": "makkalot/func", "stars": 15, "license": "gpl-2.0", "language": "python", "size": 1769}
{"docstring": "\"\"\"\nIdentify common network errors that mean we cannot connect to the server\n\"\"\"\n\n# This is a bit complicated by the fact that different versions of\n# M2Crypto & OpenSSL seem to return different error codes for the\n# same type of error\n", "func_signal": "def canIgnoreSSLError(e):\n", "code": "s = \"%s\" % e\nif e[0] == 104:     # Connection refused\n    return True\nelif e[0] == 111:   # Connection reset by peer\n    return True\nelif e[0] == 61:    # Connection refused\n    return True\nelif e[0] == 54:    # Connection reset by peer\n    return True\nelif s == \"no certificate returned\":\n    return True\nelif s == \"wrong version number\":\n    return True\nelif s == \"unexpected eof\":\n    return True\n\nreturn False", "path": "func\\CommonErrors.py", "repo_name": "makkalot/func", "stars": 15, "license": "gpl-2.0", "language": "python", "size": 1769}
{"docstring": "\"\"\"\nPermanently adds a VLAN by writing to an ifcfg-file\n\nKeyword arguments:\ninterface -- interface to add vlan to (string, for example: \"eth0\")\nvlanid -- ID of the vlan to add (string, for example: \"1100\")\nipaddr -- IP-address for this VLAN (string)\nnetmask -- Netmask for this VLAN (string)\ngateway -- Gateway for this VLAN (string)\n\"\"\"\n", "func_signal": "def add_permanent(self, interface, vlanid, ipaddr=None, netmask=None, gateway=None):\n", "code": "alreadyup = False\nlist = self.list()\nif interface in list:\n    thisinterface = list[interface]\n    if vlanid in thisinterface:\n        alreadyup = True\n\ndevice = \"%s.%s\" % (interface, vlanid)\nif vlanid not in self.options.ignorevlans:\n    filename = \"/etc/sysconfig/network-scripts/ifcfg-%s\" % device\n    fp = open(filename, \"w\")\n    filelines = [ \"DEVICE=%s\\n\" % device, \"VLAN=yes\\n\", \"ONBOOT=yes\\n\" ]\n    if ipaddr != None:\n        filelines.append(\"BOOTPROTO=static\\n\")\n        filelines.append(\"IPADDR=%s\\n\" % ipaddr)\n    else:\n        filelines.append(\"BOOTPROTO=none\\n\")\n    if netmask != None:\n        filelines.append(\"NETMASK=%s\\n\" % netmask)\n    if gateway != None:\n        filelines.append(\"GATEWAY=%s\\n\" % gateway)\n    fp.writelines(filelines)\n    fp.close()\n\n    if alreadyup:\n        # Don't run ifup, this will confuse the OS\n        exitcode = self.up(interface, vlanid)\n    else:\n        exitcode = os.spawnv(os.P_WAIT, self.options.ifup, [ self.options.ifup, device ])\nelse:\n    exitcode = -1\nreturn exitcode", "path": "func\\minion\\modules\\vlan.py", "repo_name": "makkalot/func", "stars": 15, "license": "gpl-2.0", "language": "python", "size": 1769}
{"docstring": "\"\"\"Find index of the opcode calling the value pushed at opcode idx.\n\nThis method finds the position of the opcode that calls a function\npushed onto the stack by opcode 'idx'.  If we cannot reliably find\nsuch an opcode (due to weird branching etc) then None is returned.\n\"\"\"\n", "func_signal": "def _find_callsite(self,idx,code):\n", "code": "try:\n    callsite = idx + 1\n    try:\n        (curop,curarg) = code[callsite]\n    except IndexError:\n        return None\n    (pop,push) = getse(curop,curarg)\n    curstack = push - pop\n    while curstack > 0 or curop != CALL_FUNCTION:\n        callsite += 1\n        try:\n            (curop,curarg) = code[callsite]\n        except IndexError:\n            return None\n        (pop,push) = getse(curop,curarg)\n        curstack = curstack + push - pop\n    if curstack == 0:\n        return callsite\n    else:\n        return None\nexcept ValueError:\n    return None", "path": "promise\\__init__.py", "repo_name": "sipefree/d00ks", "stars": 14, "license": "lgpl-3.0", "language": "python", "size": 528}
{"docstring": "\"\"\"Install byteplay to automatically reassemble all functions when a module\nis imported.\n\nThis is useful for testing.\n\"\"\"\n", "func_signal": "def install():\n", "code": "import gc\nimport __builtin__\nimport sys\nimport atexit\n\norig_importer = __builtin__.__import__\nreassembled_ids = set()\n\ndef reassemble_all_funcs():\n    \"\"\"Reassemble all function objects not reassembled before.\n\n    We store ids of already reassembled functions, so a function won't\n    be reassembled if another function with the same id was already\n    reassembled.\n    \"\"\"\n    funcs = [x for x in gc.get_objects()\n             if isinstance(x, new.function)\n             and id(x) not in reassembled_ids\n             ]\n    for f in funcs:\n        f.func_code = Code.from_code(f.func_code).to_code()\n        reassembled_ids.add(id(f))\n\ndef patched_importer(*args, **kwargs):\n    prevlen = len(sys.modules)\n    r = orig_importer(*args, **kwargs)\n    curlen = len(sys.modules)\n    if curlen > prevlen:\n        reassemble_all_funcs()\n    return r\n\ndef print_howmany_reassembled():\n    print >> sys.stderr, \"Reassembled %d functions.\" % len(reassembled_ids)\n\n__builtin__.__import__ = patched_importer\natexit.register(print_howmany_reassembled)\nreassemble_all_funcs()", "path": "promise\\byteplay.py", "repo_name": "sipefree/d00ks", "stars": 14, "license": "lgpl-3.0", "language": "python", "size": 528}
{"docstring": "\"\"\"Find the offsets in a byte code which are start of lines in the\nsource.\n\nGenerate pairs (offset, lineno) as described in Python/compile.c.\n\nThis is a modified version of dis.findlinestarts, which allows multiple\n\"line starts\" with the same line number.\n\"\"\"\n", "func_signal": "def _findlinestarts(code):\n", "code": "byte_increments = [ord(c) for c in code.co_lnotab[0::2]]\nline_increments = [ord(c) for c in code.co_lnotab[1::2]]\n\nlineno = code.co_firstlineno\naddr = 0\nfor byte_incr, line_incr in zip(byte_increments, line_increments):\n    if byte_incr:\n        yield (addr, lineno)\n        addr += byte_incr\n    lineno += line_incr\nyield (addr, lineno)", "path": "promise\\byteplay.py", "repo_name": "sipefree/d00ks", "stars": 14, "license": "lgpl-3.0", "language": "python", "size": 528}
{"docstring": "\"\"\"Simulate (LOAD_DEREF,name) on the given function.\"\"\"\n#  Determine index of cell matching given name\n", "func_signal": "def _load_name_deref(self,func,name):\n", "code": "try:\n    idx = func.func_code.co_cellvars.index(name)\nexcept ValueError:\n    try:\n        idx = func.func_code.co_freevars.index(name)\n        idx -= len(func.func_code.co_cellvars)\n    except ValueError:\n        raise NameError(name)\nreturn func.func_closure[idx].cell_contents", "path": "promise\\__init__.py", "repo_name": "sipefree/d00ks", "stars": 14, "license": "lgpl-3.0", "language": "python", "size": 528}
{"docstring": "\"\"\"Find an inlinable call to func in the given code.\n\nIf such a call is found, a tuple (loadsite,callsite) is returned\ngiving the position of the LOAD_CONST on the function and the matching\nCALL_FUNCTION.  If no inlinable call is found, returns None.\n\"\"\"\n", "func_signal": "def _find_inlinable_call(self,func,code):\n", "code": "for (i,(op,arg)) in enumerate(code.code):\n    if op == LOAD_CONST and arg == func:\n        loadsite = i\n        callsite = self._find_callsite(loadsite,code.code)\n        if callsite is not None:\n            (op,arg) = code.code[callsite]\n            #  Can't currently inline kwdargs\n            if arg == (arg & 0xFF):\n                return (loadsite,callsite)\nreturn None", "path": "promise\\__init__.py", "repo_name": "sipefree/d00ks", "stars": 14, "license": "lgpl-3.0", "language": "python", "size": 528}
{"docstring": "# Make a copy of the macro token sequence\n", "func_signal": "def macro_expand_args(self,macro,args):\n", "code": "rep = [copy.copy(_x) for _x in macro.value]\n\n# Make string expansion patches.  These do not alter the length of the replacement sequence\n\nstr_expansion = {}\nfor argnum, i in macro.str_patch:\n    if argnum not in str_expansion:\n        str_expansion[argnum] = ('\"%s\"' % \"\".join([x.value for x in args[argnum]])).replace(\"\\\\\",\"\\\\\\\\\")\n    rep[i] = copy.copy(rep[i])\n    rep[i].value = str_expansion[argnum]\n\n# Make the variadic macro comma patch.  If the variadic macro argument is empty, we get rid\ncomma_patch = False\nif macro.variadic and not args[-1]:\n    for i in macro.var_comma_patch:\n        rep[i] = None\n        comma_patch = True\n\n# Make all other patches.   The order of these matters.  It is assumed that the patch list\n# has been sorted in reverse order of patch location since replacements will cause the\n# size of the replacement sequence to expand from the patch point.\n\nexpanded = { }\nfor ptype, argnum, i in macro.patch:\n    # Concatenation.   Argument is left unexpanded\n    if ptype == 'c':\n        rep[i:i+1] = args[argnum]\n    # Normal expansion.  Argument is macro expanded first\n    elif ptype == 'e':\n        if argnum not in expanded:\n            expanded[argnum] = self.expand_macros(args[argnum])\n        rep[i:i+1] = expanded[argnum]\n\n# Get rid of removed comma if necessary\nif comma_patch:\n    rep = [_i for _i in rep if _i]\n\nreturn rep", "path": "ply\\cpp.py", "repo_name": "sipefree/d00ks", "stars": 14, "license": "lgpl-3.0", "language": "python", "size": 528}
{"docstring": "\"\"\"Look up the given name in the scope of the given function.\n\nThis is an attempt to replicate the name lookup rules of LOAD_NAME,\nLOAD_GLOBAL and friends.  If a specific bytecode op is specified,\nonly the rules for that operation are applied.\n\nIf the name cannot be found, NameError is raised.\n\"\"\"\n", "func_signal": "def _load_name(self,func,name,op=None):\n", "code": "if op in (None,LOAD_NAME,LOAD_DEREF):\n   try:\n       return self._load_name_deref(func,name)\n   except NameError:\n       pass\nif op in (None,LOAD_NAME,LOAD_GLOBAL):\n   try:\n       return self._load_name_global(func,name)\n   except NameError:\n       pass\nraise NameError(name)", "path": "promise\\__init__.py", "repo_name": "sipefree/d00ks", "stars": 14, "license": "lgpl-3.0", "language": "python", "size": 528}
{"docstring": "\"\"\"Make _promise_fold_constant method for the given pure function.\"\"\"\n", "func_signal": "def _make_fold_method(self,source_func):\n", "code": "def fold(dest_func,dest_code):\n    \"\"\"Inline the code of source_func into the given bytecode.\"\"\"\n    #  Apply any deferred promises to source_func.\n    #  Since it's pure, we can simply call it to force this.\n    if hasattr(source_func,\"_promise_deferred\"):\n        try:\n            source_func(*([None]*source_func.func_code.co_argcount))\n        except Exception:\n            pass\n    #  Inline the function at every callsite\n    toinline = self._find_inlinable_call(source_func,dest_code)\n    while toinline is not None:\n        (loadsite,callsite) = toinline\n        #  Give new names to the locals in the source bytecode\n        source_code = Code.from_code(source_func.func_code)\n        name_map = self._rename_local_vars(source_code)\n        #  Remove any setlineno ops from the source bytecode\n        new_code = [c for c in source_code.code if c[0] != SetLineno]\n        source_code.code[:] = new_code\n        #  Pop the function arguments directly from the stack.\n        #  Keyword args are currently not supported.\n        numargs = dest_code.code[callsite][1] & 0xFF\n        for i in xrange(numargs):\n            argname = source_func.func_code.co_varnames[i]\n            source_code.code.insert(0,(STORE_FAST,name_map[argname]))\n        #  Fill in any missing args from the function defaults\n        numreqd = source_func.func_code.co_argcount\n        for i in xrange(numargs,numreqd):\n            argname = source_func.func_code.co_varnames[i]\n            defidx = i - numreqd + len(source_func.func_defaults)\n            defval = source_func.func_defaults[defidx]\n            source_code.code.insert(0,(STORE_FAST,name_map[argname]))\n            source_code.code.insert(0,(LOAD_CONST,defval))\n        #  Munge the source bytecode to leave return value on stack\n        end = Label()\n        source_code.code.append((end,None))\n        for (i,(op,arg)) in enumerate(source_code.code):\n            if op == RETURN_VALUE:\n                source_code.code[i] = (JUMP_ABSOLUTE,end)\n        #  Replace the callsite with the inlined code\n        dest_code.code[callsite:callsite+1] = source_code.code\n        del dest_code.code[loadsite]\n        #  Rinse and repeat\n        toinline = self._find_inlinable_call(source_func,dest_code)\nreturn fold", "path": "promise\\__init__.py", "repo_name": "sipefree/d00ks", "stars": 14, "license": "lgpl-3.0", "language": "python", "size": 528}
{"docstring": "\"\"\"Apply any deferred promises attached to a function.\"\"\"\n#  Get the code object before checking for deferred promises.\n#  This prevents race conditions if several threads apply them at once.\n", "func_signal": "def apply_deferred_promises(func):\n", "code": "c = Code.from_code(func.func_code)\ntry:\n    deferred = func._promise_deferred\nexcept AttributeError:\n    pass\nelse:\n    del func._promise_deferred\n    #  Remove the bootstrapping code inserted by Promise.defer()\n    idx = c.code.index((POP_TOP,None))\n    del c.code[:idx+1]\n    #  Apply each promise in turn\n    for p in deferred:\n        p.apply(func,c)\n    #  Use the transformed bytecode in subsequent calls to func\n    func.func_code = c.to_code()\npass", "path": "promise\\__init__.py", "repo_name": "sipefree/d00ks", "stars": 14, "license": "lgpl-3.0", "language": "python", "size": 528}
{"docstring": "\"\"\"Apply this promise to a function, module, dict, etc.\n\nCalling a promise arranges for it to be applied to any functions\nfound in the given arguments.  Each argument can be a raw function,\nor a class, module or iterable of functions.\n\"\"\"\n", "func_signal": "def __call__(self,*args):\n", "code": "if not args:\n    return None\nfor arg in args:\n    if isinstance(arg,types.FunctionType):\n        self.decorate(arg)\n    else:\n        try:\n            subargs = arg.itervalues()\n        except (AttributeError,TypeError):\n            subargs =  (getattr(arg,nm) for nm in dir(arg))\n        for subarg in subargs:\n            if isinstance(subarg,types.FunctionType):\n                self(subarg)\nreturn args[0]", "path": "promise\\__init__.py", "repo_name": "sipefree/d00ks", "stars": 14, "license": "lgpl-3.0", "language": "python", "size": 528}
{"docstring": "\"\"\"Apply this promise, or defer it if others are already deferred.\n\nIt's generally a good idea to use this instead of directly applying\na promise, since it ensures that individual promises will be applied\nin the order in which they appear in code.\n\"\"\"\n", "func_signal": "def apply_or_defer(self,func):\n", "code": "try:\n    deferred = func._promise_deferred\nexcept AttributeError:\n    code = Code.from_code(func.func_code)\n    self.apply(func,code)\n    func.func_code = code.to_code()\nelse:\n    deferred.append(self)", "path": "promise\\__init__.py", "repo_name": "sipefree/d00ks", "stars": 14, "license": "lgpl-3.0", "language": "python", "size": 528}
{"docstring": "\"\"\"Generator producing unique ids.\"\"\"\n", "func_signal": "def _ids():\n", "code": "i = 0\nwhile True:\n    i += 1\n    yield i", "path": "promise\\__init__.py", "repo_name": "sipefree/d00ks", "stars": 14, "license": "lgpl-3.0", "language": "python", "size": 528}
{"docstring": "\"\"\"Generate a new unique variable name\n\nIf the given name is not None, it is included in the generated name for\nease of reference in e.g. tracebacks or bytecode inspection.\n\"\"\"\n", "func_signal": "def new_name(name=None):\n", "code": "if name is None:\n    return \"_promise_var%s\" % (_ids.next(),)\nelse:\n    return \"_promise_var%s_%s\" % (_ids.next(),name,)", "path": "promise\\__init__.py", "repo_name": "sipefree/d00ks", "stars": 14, "license": "lgpl-3.0", "language": "python", "size": 528}
{"docstring": "# Pull off the first character to see if s looks like a string\n", "func_signal": "def input(self,s):\n", "code": "c = s[:1]\nif not isinstance(c,StringTypes):\n    raise ValueError(\"Expected a string\")\nself.lexdata = s\nself.lexpos = 0\nself.lexlen = len(s)", "path": "ply\\lex.py", "repo_name": "sipefree/d00ks", "stars": 14, "license": "lgpl-3.0", "language": "python", "size": 528}
{"docstring": "\"\"\"Rename the local variables in the given code to new unique names.\n\nReturns a dictionary mapping old names to new names.\n\"\"\"\n", "func_signal": "def _rename_local_vars(self,code):\n", "code": "name_map = {}\nfor nm in code.to_code().co_varnames:\n    name_map[nm] = new_name(nm)\nfor (i,(op,arg)) in enumerate(code.code):\n    if op in (LOAD_FAST,STORE_FAST,DELETE_FAST):\n        try:\n            newarg = name_map[arg]\n        except KeyError:\n            newarg = new_name(arg)\n            name_map[arg] = newarg\n        code.code[i] = (op,newarg)\nreturn name_map", "path": "promise\\__init__.py", "repo_name": "sipefree/d00ks", "stars": 14, "license": "lgpl-3.0", "language": "python", "size": 528}
{"docstring": "# Make local copies of frequently referenced attributes\n", "func_signal": "def token(self):\n", "code": "lexpos    = self.lexpos\nlexlen    = self.lexlen\nlexignore = self.lexignore\nlexdata   = self.lexdata\n\nwhile lexpos < lexlen:\n    # This code provides some short-circuit code for whitespace, tabs, and other ignored characters\n    if lexdata[lexpos] in lexignore:\n        lexpos += 1\n        continue\n\n    # Look for a regular expression match\n    for lexre,lexindexfunc in self.lexre:\n        m = lexre.match(lexdata,lexpos)\n        if not m: continue\n\n        # Create a token for return\n        tok = LexToken()\n        tok.value = m.group()\n        tok.lineno = self.lineno\n        tok.lexpos = lexpos\n\n        i = m.lastindex\n        func,tok.type = lexindexfunc[i]\n\n        if not func:\n           # If no token type was set, it's an ignored token\n           if tok.type:\n              self.lexpos = m.end()\n              return tok\n           else:\n              lexpos = m.end()\n              break\n\n        lexpos = m.end()\n\n        # If token is processed by a function, call it\n\n        tok.lexer = self      # Set additional attributes useful in token rules\n        self.lexmatch = m\n        self.lexpos = lexpos\n\n        newtok = func(tok)\n\n        # Every function must return a token, if nothing, we just move to next token\n        if not newtok:\n            lexpos    = self.lexpos         # This is here in case user has updated lexpos.\n            lexignore = self.lexignore      # This is here in case there was a state change\n            break\n\n        # Verify type of the token.  If not in the token map, raise an error\n        if not self.lexoptimize:\n            if not newtok.type in self.lextokens:\n                raise LexError(\"%s:%d: Rule '%s' returned an unknown token type '%s'\" % (\n                    func_code(func).co_filename, func_code(func).co_firstlineno,\n                    func.__name__, newtok.type),lexdata[lexpos:])\n\n        return newtok\n    else:\n        # No match, see if in literals\n        if lexdata[lexpos] in self.lexliterals:\n            tok = LexToken()\n            tok.value = lexdata[lexpos]\n            tok.lineno = self.lineno\n            tok.type = tok.value\n            tok.lexpos = lexpos\n            self.lexpos = lexpos + 1\n            return tok\n\n        # No match. Call t_error() if defined.\n        if self.lexerrorf:\n            tok = LexToken()\n            tok.value = self.lexdata[lexpos:]\n            tok.lineno = self.lineno\n            tok.type = \"error\"\n            tok.lexer = self\n            tok.lexpos = lexpos\n            self.lexpos = lexpos\n            newtok = self.lexerrorf(tok)\n            if lexpos == self.lexpos:\n                # Error method didn't change text position at all. This is an error.\n                raise LexError(\"Scanning error. Illegal character '%s'\" % (lexdata[lexpos]), lexdata[lexpos:])\n            lexpos = self.lexpos\n            if not newtok: continue\n            return newtok\n\n        self.lexpos = lexpos\n        raise LexError(\"Illegal character '%s' at index %d\" % (lexdata[lexpos],lexpos), lexdata[lexpos:])\n\nself.lexpos = lexpos + 1\nif self.lexdata is None:\n     raise RuntimeError(\"No input string given with input()\")\nreturn None", "path": "ply\\lex.py", "repo_name": "sipefree/d00ks", "stars": 14, "license": "lgpl-3.0", "language": "python", "size": 528}
{"docstring": "\"\"\"Simulate (LOAD_GLOBAL,name) on the given function.\"\"\"\n", "func_signal": "def _load_name_global(self,func,name):\n", "code": "try:\n    try:\n        return func.func_globals[name]\n    except KeyError:\n        return __builtins__[name]\nexcept KeyError:\n    raise NameError(name)", "path": "promise\\__init__.py", "repo_name": "sipefree/d00ks", "stars": 14, "license": "lgpl-3.0", "language": "python", "size": 528}
{"docstring": "# tokens = tokenize(line)\n# Search for defined macros\n", "func_signal": "def evalexpr(self,tokens):\n", "code": "i = 0\nwhile i < len(tokens):\n    if tokens[i].type == self.t_ID and tokens[i].value == 'defined':\n        j = i + 1\n        needparen = False\n        result = \"0L\"\n        while j < len(tokens):\n            if tokens[j].type in self.t_WS:\n                j += 1\n                continue\n            elif tokens[j].type == self.t_ID:\n                if tokens[j].value in self.macros:\n                    result = \"1L\"\n                else:\n                    result = \"0L\"\n                if not needparen: break\n            elif tokens[j].value == '(':\n                needparen = True\n            elif tokens[j].value == ')':\n                break\n            else:\n                self.error(self.source,tokens[i].lineno,\"Malformed defined()\")\n            j += 1\n        tokens[i].type = self.t_INTEGER\n        tokens[i].value = self.t_INTEGER_TYPE(result)\n        del tokens[i+1:j+1]\n    i += 1\ntokens = self.expand_macros(tokens)\nfor i,t in enumerate(tokens):\n    if t.type == self.t_ID:\n        tokens[i] = copy.copy(t)\n        tokens[i].type = self.t_INTEGER\n        tokens[i].value = self.t_INTEGER_TYPE(\"0L\")\n    elif t.type == self.t_INTEGER:\n        tokens[i] = copy.copy(t)\n        # Strip off any trailing suffixes\n        tokens[i].value = str(tokens[i].value)\n        while tokens[i].value[-1] not in \"0123456789abcdefABCDEF\":\n            tokens[i].value = tokens[i].value[:-1]\n\nexpr = \"\".join([str(x.value) for x in tokens])\nexpr = expr.replace(\"&&\",\" and \")\nexpr = expr.replace(\"||\",\" or \")\nexpr = expr.replace(\"!\",\" not \")\ntry:\n    result = eval(expr)\nexcept StandardError:\n    self.error(self.source,tokens[0].lineno,\"Couldn't evaluate expression\")\n    result = 0\nreturn result", "path": "ply\\cpp.py", "repo_name": "sipefree/d00ks", "stars": 14, "license": "lgpl-3.0", "language": "python", "size": 528}
{"docstring": "\"\"\"Get a code list. Print it nicely.\"\"\"\n\n", "func_signal": "def printcodelist(codelist, to=sys.stdout):\n", "code": "labeldict = {}\npendinglabels = []\nfor i, (op, arg) in enumerate(codelist):\n    if isinstance(op, Label):\n        pendinglabels.append(op)\n    elif op is SetLineno:\n        pass\n    else:\n        while pendinglabels:\n            labeldict[pendinglabels.pop()] = i\n\nlineno = None\nislabel = False\nfor i, (op, arg) in enumerate(codelist):\n    if op is SetLineno:\n        lineno = arg\n        print >> to\n        continue\n\n    if isinstance(op, Label):\n        islabel = True\n        continue\n\n    if lineno is None:\n        linenostr = ''\n    else:\n        linenostr = str(lineno)\n        lineno = None\n\n    if islabel:\n        islabelstr = '>>'\n        islabel = False\n    else:\n        islabelstr = ''\n\n    if op in hasconst:\n        argstr = repr(arg)\n    elif op in hasjump:\n        try:\n            argstr = 'to ' + str(labeldict[arg])\n        except KeyError:\n            argstr = repr(arg)\n    elif op in hasarg:\n        argstr = str(arg)\n    else:\n        argstr = ''\n\n    print >> to, '%3s     %2s %4d %-20s %s' % (\n        linenostr,\n        islabelstr,\n        i,\n        op,\n        argstr)", "path": "promise\\byteplay.py", "repo_name": "sipefree/d00ks", "stars": 14, "license": "lgpl-3.0", "language": "python", "size": 528}
{"docstring": "\"\"\"Defer the application of this promise func is first executed.\"\"\"\n# Try to be thread-safe by using setdefault(), which is implemented\n# in C and is therefore non-interruptible.\n", "func_signal": "def defer(self,func):\n", "code": "default = []\ndeferred = func.__dict__.setdefault(\"_promise_deferred\",default)\ndeferred.append(self)\nif deferred is default:\n    #  Add code to apply the promise when func is first executed.\n    #  These opcodes are removed by apply_deferred_promises()\n    c = Code.from_code(func.func_code)\n    c.code.insert(0,(LOAD_CONST,apply_deferred_promises))\n    c.code.insert(1,(LOAD_CONST,func))\n    c.code.insert(2,(CALL_FUNCTION,1))\n    c.code.insert(3,(POP_TOP,None))\n    func.func_code = c.to_code()", "path": "promise\\__init__.py", "repo_name": "sipefree/d00ks", "stars": 14, "license": "lgpl-3.0", "language": "python", "size": 528}
{"docstring": "\"\"\"    River continuation bet.\"\"\"\n", "func_signal": "def cb3(stat_dict, player):\n", "code": "stat = 0.0\ntry:\n    stat = float(stat_dict[player]['cb_3'])/float(stat_dict[player]['cb_opp_3'])\n    return (stat,\n            '%3.1f'         % (100.0*stat),\n            'cb3=%3.1f%%'   % (100.0*stat),\n            'cb_3=%3.1f%%'  % (100.0*stat),\n            '(%d/%d)'       % (stat_dict[player]['cb_3'], stat_dict[player]['cb_opp_3']),\n            _('% continuation bet river/6th'))\nexcept:\n    return (stat,\n            'NA',\n            'cb3=NA',\n            'cb_3=NA',\n            '(0/0)',\n            _('% continuation bet river/6th'))", "path": "pyfpdb\\Stats.py", "repo_name": "sqlcoder/fpdb-sql", "stars": 8, "license": "None", "language": "python", "size": 9492}
{"docstring": "\"\"\"    Folded BB to steal.\"\"\"\n", "func_signal": "def f_BB_steal(stat_dict, player):\n", "code": "stat = 0.0\ntry:\n    stat = float(stat_dict[player]['bbnotdef'])/float(stat_dict[player]['bbstolen'])\n    return (stat,\n            '%3.1f'         % (100.0*stat),\n            'fBB=%3.1f%%'   % (100.0*stat),\n            'fBB_s=%3.1f%%' % (100.0*stat),\n            '(%d/%d)'       % (stat_dict[player]['bbnotdef'], stat_dict[player]['bbstolen']),\n            _('% folded BB to steal'))\nexcept:\n    return (stat,\n            'NA',\n            'fBB=NA',\n            'fBB_s=NA',\n            '(0/0)',\n            _('% folded BB to steal'))", "path": "pyfpdb\\Stats.py", "repo_name": "sqlcoder/fpdb-sql", "stars": 8, "license": "None", "language": "python", "size": 9492}
{"docstring": "\"\"\"    7th street continuation bet.\"\"\"\n", "func_signal": "def cb4(stat_dict, player):\n", "code": "stat = 0.0\ntry:\n    stat = float(stat_dict[player]['cb_4'])/float(stat_dict[player]['cb_opp_4'])\n    return (stat,\n            '%3.1f'         % (100.0*stat),\n            'cb4=%3.1f%%'   % (100.0*stat),\n            'cb_4=%3.1f%%'  % (100.0*stat),\n            '(%d/%d)'      % (stat_dict[player]['cb_4'], stat_dict[player]['cb_opp_4']),\n            _('% continuation bet 7th'))\nexcept:\n    return (stat,\n            'NA',\n            'cb4=NA',\n            'cb_4=NA',\n            '(0/0)',\n            _('% continuation bet 7th'))", "path": "pyfpdb\\Stats.py", "repo_name": "sqlcoder/fpdb-sql", "stars": 8, "license": "None", "language": "python", "size": 9492}
{"docstring": "\"\"\"    Three bet preflop/3rd.\"\"\"\n", "func_signal": "def three_B(stat_dict, player):\n", "code": "stat = 0.0\ntry:\n    stat = float(stat_dict[player]['tb_0'])/float(stat_dict[player]['tb_opp_0'])\n    return (stat,\n            '%3.1f'         % (100.0*stat),\n            '3B=%3.1f%%'    % (100.0*stat),\n            '3B_pf=%3.1f%%' % (100.0*stat),\n            '(%d/%d)'       % (stat_dict[player]['tb_0'], stat_dict[player]['tb_opp_0']),\n            _('% 3/4 Bet preflop/3rd'))\nexcept:\n    return (stat,\n            'NA',\n            '3B=NA',\n            '3B_pf=NA',\n            '(0/0)',\n            _('% 3/4 Bet preflop/3rd'))", "path": "pyfpdb\\Stats.py", "repo_name": "sqlcoder/fpdb-sql", "stars": 8, "license": "None", "language": "python", "size": 9492}
{"docstring": "\"\"\"    Turn continuation bet.\"\"\"\n", "func_signal": "def cb2(stat_dict, player):\n", "code": "stat = 0.0\ntry:\n    stat = float(stat_dict[player]['cb_2'])/float(stat_dict[player]['cb_opp_2'])\n    return (stat,\n            '%3.1f'         % (100.0*stat),\n            'cb2=%3.1f%%'   % (100.0*stat),\n            'cb_2=%3.1f%%'  % (100.0*stat),\n            '(%d/%d)'       % (stat_dict[player]['cb_2'], stat_dict[player]['cb_opp_2']),\n            _('% continuation bet turn/5th'))\nexcept:\n    return (stat,\n            'NA',\n            'cb2=NA',\n            'cb_2=NA',\n            '(0/0)',\n            _('% continuation bet turn/5th'))", "path": "pyfpdb\\Stats.py", "repo_name": "sqlcoder/fpdb-sql", "stars": 8, "license": "None", "language": "python", "size": 9492}
{"docstring": "\"\"\"    Voluntarily put $ in the pot pre-flop.\"\"\"\n", "func_signal": "def vpip(stat_dict, player):\n", "code": "stat = 0.0\ntry:\n    stat = float(stat_dict[player]['vpip'])/float(stat_dict[player]['n'])\n    return (stat,\n            '%3.1f'         % (100.0*stat),\n            'v=%3.1f%%'     % (100.0*stat),\n            'vpip=%3.1f%%'  % (100.0*stat),\n            '(%d/%d)'       % (stat_dict[player]['vpip'], stat_dict[player]['n']),\n            _('Voluntarily Put In Pot Pre-Flop%')\n            )\nexcept: return (stat,\n                'NA',\n                'v=NA',\n                'vpip=NA',\n                '(0/0)',\n                _('Voluntarily Put In Pot Pre-Flop%')\n                )", "path": "pyfpdb\\Stats.py", "repo_name": "sqlcoder/fpdb-sql", "stars": 8, "license": "None", "language": "python", "size": 9492}
{"docstring": "\"\"\"    Turn/5th aggression frequency.\"\"\"\n", "func_signal": "def a_freq2(stat_dict, player):\n", "code": "stat = 0.0\ntry:\n    stat = float(stat_dict[player]['aggr_2'])/float(stat_dict[player]['saw_2'])\n    return (stat,\n            '%3.1f'             % (100.0*stat),\n            'a2=%3.1f%%'        % (100.0*stat),\n            'a_fq_2=%3.1f%%'    % (100.0*stat),\n            '(%d/%d)'           % (stat_dict[player]['aggr_2'], stat_dict[player]['saw_2']),\n            _('Aggression Freq turn/5th'))\nexcept:\n    return (stat,\n            'NA',\n            'a2=NA',\n            'a_fq_2=NA',\n            '(0/0)',\n            _('Aggression Freq turn/5th'))", "path": "pyfpdb\\Stats.py", "repo_name": "sqlcoder/fpdb-sql", "stars": 8, "license": "None", "language": "python", "size": 9492}
{"docstring": "\"\"\"    Turn/5th fold frequency.\"\"\"\n", "func_signal": "def ffreq2(stat_dict, player):\n", "code": "stat = 0.0\ntry:\n    stat = float(stat_dict[player]['f_freq_2'])/float(stat_dict[player]['was_raised_2'])\n    return (stat,\n            '%3.1f'         % (100.0*stat),\n            'ff2=%3.1f%%'   % (100.0*stat),\n            'ff_2=%3.1f%%'  % (100.0*stat),\n            '(%d/%d)'       % (stat_dict[player]['f_freq_2'], stat_dict[player]['was_raised_2']),\n            _('% fold frequency turn/5th'))\nexcept:\n    return (stat,\n            'NA',\n            'ff2=NA',\n            'ff_2=NA',\n            '(0/0)',\n            _('% fold frequency turn/5th'))", "path": "pyfpdb\\Stats.py", "repo_name": "sqlcoder/fpdb-sql", "stars": 8, "license": "None", "language": "python", "size": 9492}
{"docstring": "\"\"\"    Folded blind to steal.\"\"\"\n", "func_signal": "def f_steal(stat_dict, player):\n", "code": "stat = 0.0\ntry:\n    folded_blind = stat_dict[player]['sbnotdef'] + stat_dict[player]['bbnotdef']\n    blind_stolen = stat_dict[player]['sbstolen'] + stat_dict[player]['bbstolen']\n    \n    stat = float(folded_blind)/float(blind_stolen)\n    return (stat,\n            '%3.1f'         % (100.0*stat),\n            'fB=%3.1f%%'    % (100.0*stat),\n            'fB_s=%3.1f%%'  % (100.0*stat),\n            '(%d/%d)'       % (folded_blind, blind_stolen),\n            _('% folded blind to steal'))\nexcept:\n    return (stat,\n            'NA',\n            'fB=NA',\n            'fB_s=NA',\n            '(0/0)',\n            _('% folded blind to steal'))", "path": "pyfpdb\\Stats.py", "repo_name": "sqlcoder/fpdb-sql", "stars": 8, "license": "None", "language": "python", "size": 9492}
{"docstring": "\"\"\"    Flop continuation bet.\"\"\"\n", "func_signal": "def cb1(stat_dict, player):\n", "code": "stat = 0.0\ntry:\n    stat = float(stat_dict[player]['cb_1'])/float(stat_dict[player]['cb_opp_1'])\n    return (stat,\n            '%3.1f'         % (100.0*stat),\n            'cb1=%3.1f%%'   % (100.0*stat),\n            'cb_1=%3.1f%%'  % (100.0*stat),\n            '(%d/%d)'       % (stat_dict[player]['cb_1'], stat_dict[player]['cb_opp_1']),\n            _('% continuation bet flop/4th'))\nexcept:\n    return (stat,\n            'NA',\n            'cb1=NA',\n            'cb_1=NA',\n            '(0/0)',\n            _('% continuation bet flop/4th'))", "path": "pyfpdb\\Stats.py", "repo_name": "sqlcoder/fpdb-sql", "stars": 8, "license": "None", "language": "python", "size": 9492}
{"docstring": "\"\"\"    River/6th aggression frequency.\"\"\"\n", "func_signal": "def a_freq3(stat_dict, player):\n", "code": "stat = 0.0\ntry:\n    stat = float(stat_dict[player]['aggr_3'])/float(stat_dict[player]['saw_3'])\n    return (stat,\n            '%3.1f'             % (100.0*stat),\n            'a3=%3.1f%%'        % (100.0*stat),\n            'a_fq_3=%3.1f%%'    % (100.0*stat),\n            '(%d/%d)'      % (stat_dict[player]['aggr_3'], stat_dict[player]['saw_3']),\n            _('Aggression Freq river/6th'))\nexcept:\n    return (stat,\n            'NA',\n            'a3=NA',\n            'a_fq_3=NA',\n            '(0/0)',\n            _('Aggression Freq river/6th'))", "path": "pyfpdb\\Stats.py", "repo_name": "sqlcoder/fpdb-sql", "stars": 8, "license": "None", "language": "python", "size": 9492}
{"docstring": "\"\"\"    Preflop (3rd street) raise.\"\"\"\n", "func_signal": "def pfr(stat_dict, player):\n", "code": "stat = 0.0\ntry:\n    stat = float(stat_dict[player]['pfr'])/float(stat_dict[player]['n'])\n    return (stat,\n            '%3.1f'         % (100.0*stat),\n            'p=%3.1f%%'     % (100.0*stat),\n            'pfr=%3.1f%%'   % (100.0*stat),\n            '(%d/%d)'    % (stat_dict[player]['pfr'], stat_dict[player]['n']),\n            _('Pre-Flop Raise %')\n            )\nexcept: \n    return (stat,\n            'NA',\n            'p=NA',\n            'pfr=NA',\n            '(0/0)',\n            _('Pre-Flop Raise %')\n            )", "path": "pyfpdb\\Stats.py", "repo_name": "sqlcoder/fpdb-sql", "stars": 8, "license": "None", "language": "python", "size": 9492}
{"docstring": "\"\"\"    River/6th fold frequency.\"\"\"\n", "func_signal": "def ffreq3(stat_dict, player):\n", "code": "stat = 0.0\ntry:\n    stat = float(stat_dict[player]['f_freq_3'])/float(stat_dict[player]['was_raised_3'])\n    return (stat,\n            '%3.1f'         % (100.0*stat),\n            'ff3=%3.1f%%'   % (100.0*stat),\n            'ff_3=%3.1f%%'  % (100.0*stat),\n            '(%d/%d)'       % (stat_dict[player]['f_freq_3'], stat_dict[player]['was_raised_3']),\n            _('% fold frequency river/6th'))\nexcept:\n    return (stat,\n            'NA',\n            'ff3=NA',\n            'ff_3=NA',\n            '(0/0)',\n            _('% fold frequency river/6th'))", "path": "pyfpdb\\Stats.py", "repo_name": "sqlcoder/fpdb-sql", "stars": 8, "license": "None", "language": "python", "size": 9492}
{"docstring": "\"\"\"    Player Name.\"\"\"\n", "func_signal": "def playername(stat_dict, player):\n", "code": "return (stat_dict[player]['screen_name'],\n        stat_dict[player]['screen_name'],\n        stat_dict[player]['screen_name'],\n        stat_dict[player]['screen_name'],\n        stat_dict[player]['screen_name'],\n        stat_dict[player]['screen_name'])", "path": "pyfpdb\\Stats.py", "repo_name": "sqlcoder/fpdb-sql", "stars": 8, "license": "None", "language": "python", "size": 9492}
{"docstring": "\"\"\"    big blinds won per 100 hands.\"\"\"\n", "func_signal": "def bbper100(stat_dict, player):\n", "code": "stat = 0.0\ntry:\n    stat = 100.0 * float(stat_dict[player]['net']) / float(stat_dict[player]['bigblind'])\n    return (stat,\n            '%5.3f'         % (stat),\n            'bb100=%5.3f'   % (stat),\n            'bb100=%5.3f'   % (stat),\n            '(%d,%d)'       % (100*stat_dict[player]['net'],stat_dict[player]['bigblind']),\n            _('big blinds/100 hands')\n            )\nexcept:\n    log.info(\"exception calcing bb/100: \"+str(stat_dict[player]))\n    return (stat,\n            'NA',\n            'bb100=NA',\n            'bb100=NA',\n            '(--)',\n            _('big blinds/100 hands')\n            )", "path": "pyfpdb\\Stats.py", "repo_name": "sqlcoder/fpdb-sql", "stars": 8, "license": "None", "language": "python", "size": 9492}
{"docstring": "\"\"\"    Post-Flop aggression frequency.\"\"\"\n", "func_signal": "def a_freq_123(stat_dict, player):\n", "code": "stat = 0.0\ntry:\n    stat = float(  stat_dict[player]['aggr_1'] + stat_dict[player]['aggr_2'] + stat_dict[player]['aggr_3']\n                ) / float(  stat_dict[player]['saw_1'] + stat_dict[player]['saw_2'] + stat_dict[player]['saw_3']);\n    return (stat,\n            '%3.1f'                 % (100.0*stat),\n            'afq=%3.1f%%'           % (100.0*stat),\n            'postf_aggfq=%3.1f%%'   % (100.0*stat),\n            '(%d/%d)'           % (  stat_dict[player]['aggr_1']\n                                   + stat_dict[player]['aggr_2']\n                                   + stat_dict[player]['aggr_3']\n                                  ,  stat_dict[player]['saw_1']\n                                   + stat_dict[player]['saw_2']\n                                   + stat_dict[player]['saw_3']\n                                  ),\n            _('Post-Flop Aggression Freq'))\nexcept:\n    return (stat,\n            'NA',\n            'a3=NA',\n            'a_fq_3=NA',\n            '(0/0)',\n            _('Post-Flop Aggression Freq'))", "path": "pyfpdb\\Stats.py", "repo_name": "sqlcoder/fpdb-sql", "stars": 8, "license": "None", "language": "python", "size": 9492}
{"docstring": "\"\"\"    Saw flop/4th.\"\"\"\n", "func_signal": "def saw_f(stat_dict, player):\n", "code": "try:\n    num = float(stat_dict[player]['saw_f'])\n    den = float(stat_dict[player]['n'])\n    stat = num/den\n    return (stat,\n        '%3.1f'         % (100.0*stat),\n        'sf=%3.1f%%'    % (100.0*stat),\n        'saw_f=%3.1f%%' % (100.0*stat),\n        '(%d/%d)'       % (stat_dict[player]['saw_f'], stat_dict[player]['n']),\n        _('Flop Seen %')\n        )\nexcept:\n    stat = 0.0\n    return (stat,\n        'NA',\n        'sf=NA',\n        'saw_f=NA',\n        '(0/0)',\n        _('Flop Seen %')\n        )", "path": "pyfpdb\\Stats.py", "repo_name": "sqlcoder/fpdb-sql", "stars": 8, "license": "None", "language": "python", "size": 9492}
{"docstring": "\"\"\"    Steal %.\"\"\"\n", "func_signal": "def steal(stat_dict, player):\n", "code": "stat = 0.0\ntry:\n    stat = float(stat_dict[player]['steal'])/float(stat_dict[player]['steal_opp'])\n    return (stat,\n            '%3.1f'         % (100.0*stat),\n            'st=%3.1f%%'    % (100.0*stat),\n            'steal=%3.1f%%' % (100.0*stat),\n            '(%d/%d)'       % (stat_dict[player]['steal'], stat_dict[player]['steal_opp']),\n            _('% steal attempted')\n            )\nexcept:\n    return (stat, 'NA', 'st=NA', 'steal=NA', '(0/0)', '% steal attempted')", "path": "pyfpdb\\Stats.py", "repo_name": "sqlcoder/fpdb-sql", "stars": 8, "license": "None", "language": "python", "size": 9492}
{"docstring": "\"\"\"    Went to SD when saw flop/4th.\"\"\"\n", "func_signal": "def wtsd(stat_dict, player):\n", "code": "stat = 0.0\ntry:\n    stat = float(stat_dict[player]['sd'])/float(stat_dict[player]['saw_f'])\n    return (stat,\n            '%3.1f'         % (100.0*stat),\n            'w=%3.1f%%'     % (100.0*stat),\n            'wtsd=%3.1f%%'  % (100.0*stat),\n            '(%d/%d)'       % (stat_dict[player]['sd'], stat_dict[player]['saw_f']),\n            _('% went to showdown')\n            )\nexcept:\n    return (stat,\n            'NA',\n            'w=NA',\n            'wtsd=NA',\n            '(0/0)',\n            _('% went to showdown')\n            )", "path": "pyfpdb\\Stats.py", "repo_name": "sqlcoder/fpdb-sql", "stars": 8, "license": "None", "language": "python", "size": 9492}
{"docstring": "\"\"\"    Big Bets won per 100 hands.\"\"\"\n", "func_signal": "def BBper100(stat_dict, player):\n", "code": "stat = 0.0\ntry:\n    stat = 50 * float(stat_dict[player]['net']) / float(stat_dict[player]['bigblind'])\n    return (stat,\n            '%5.3f'         % (stat),\n            'BB100=%5.3f'   % (stat),\n            'BB100=%5.3f'   % (stat),\n            '(%d,%d)'       % (100*stat_dict[player]['net'],2*stat_dict[player]['bigblind']),\n            _('Big Bets/100 hands')\n            )\nexcept:\n    log.info(_(\"exception calcing BB/100: \")+str(stat_dict[player]))\n    return (stat,\n            'NA',\n            'BB100=NA',\n            'BB100=NA',\n            '(--)',\n            _('Big Bets/100 hands')\n            )", "path": "pyfpdb\\Stats.py", "repo_name": "sqlcoder/fpdb-sql", "stars": 8, "license": "None", "language": "python", "size": 9492}
{"docstring": "\"\"\"\nAutomatically assigns local variables to `self`.\n\n    >>> self = storage()\n    >>> autoassign(self, dict(a=1, b=2))\n    >>> self\n    <Storage {'a': 1, 'b': 2}>\n\nGenerally used in `__init__` methods, as in:\n\n    def __init__(self, foo, bar, baz=1): autoassign(self, locals())\n\"\"\"\n", "func_signal": "def autoassign(self, locals):\n", "code": "for (key, value) in locals.iteritems():\n    if key == 'self': \n        continue\n    setattr(self, key, value)", "path": "service\\vendor\\web\\utils.py", "repo_name": "anotherjesse/firenomics", "stars": 11, "license": "None", "language": "python", "size": 982}
{"docstring": "\"\"\"\nSends the email message `message` with mail and envelope headers\nfor from `from_address_` to `to_address` with `subject`. \nAdditional email headers can be specified with the dictionary \n`headers.\n\nIf `web.config.smtp_server` is set, it will send the message\nto that SMTP server. Otherwise it will look for \n`/usr/lib/sendmail`, the typical location for the sendmail-style\nbinary.\n\"\"\"\n", "func_signal": "def sendmail(from_address, to_address, subject, message, headers=None, **kw):\n", "code": "try:\n    import webapi\nexcept ImportError:\n    webapi = Storage(config=Storage())\n\nif headers is None: headers = {}\n\ncc = kw.get('cc', [])\nbcc = kw.get('bcc', [])\n\ndef listify(x):\n    if not isinstance(x, list):\n        return [x]\n    else:\n        return x\n\nto_address = listify(to_address)\ncc = listify(cc)\nbcc = listify(bcc)\n\nrecipients = to_address + cc + bcc\n\nheaders = dictadd({\n  'MIME-Version': '1.0',\n  'Content-Type': 'text/plain; charset=UTF-8',\n  'Content-Disposition': 'inline',\n  'From': from_address,\n  'To': \", \".join(to_address),\n  'Subject': subject\n}, headers)\n\nif cc:\n    headers['Cc'] = \", \".join(cc)\n\nimport email.Utils\nfrom_address = email.Utils.parseaddr(from_address)[1]\nrecipients = [email.Utils.parseaddr(r)[1] for r in recipients]\nmessage = ('\\n'.join(['%s: %s' % x for x in headers.iteritems()])\n  + \"\\n\\n\" +  message)\n\nif webapi.config.get('smtp_server'):\n    server = webapi.config.get('smtp_server')\n    port = webapi.config.get('smtp_port', 0)\n    username = webapi.config.get('smtp_username') \n    password = webapi.config.get('smtp_password')\n    debug_level = webapi.config.get('smtp_debuglevel', None)\n    starttls = webapi.config.get('smtp_starttls', False)\n\n    import smtplib\n    smtpserver = smtplib.SMTP(server, port)\n\n    if debug_level:\n        smtpserver.set_debuglevel(debug_level)\n\n    if starttls:\n        smtpserver.ehlo()\n        smtpserver.starttls()\n        smtpserver.ehlo()\n\n    if username and password:\n        smtpserver.login(username, password)\n\n    smtpserver.sendmail(from_address, recipients, message)\n    smtpserver.quit()\nelse:\n    assert not from_address.startswith('-'), 'security'\n    for r in recipients:\n        assert not r.startswith('-'), 'security'\n\n    i, o = os.popen2([\"/usr/lib/sendmail\", '-f', from_address] + recipients)\n    i.write(message)\n    i.close()\n    o.close()\n    del i, o", "path": "service\\vendor\\web\\utils.py", "repo_name": "anotherjesse/firenomics", "stars": 11, "license": "None", "language": "python", "size": 982}
{"docstring": "\"\"\"Load the session from the store, by the id from cookie\"\"\"\n", "func_signal": "def _load(self):\n", "code": "cookie_name = self._config.cookie_name\ncookie_domain = self._config.cookie_domain\nself.session_id = web.cookies().get(cookie_name)\n\nself._check_expiry()\nif self.session_id:\n    d = self.store[self.session_id]\n    self.update(d)\n    self._validate_ip()\n\nif not self.session_id:\n    self.session_id = self._generate_session_id()\n\n    if self._initializer:\n        if isinstance(self._initializer, dict):\n            self.update(self._initializer)\n        elif hasattr(self._initializer, '__call__'):\n            self._initializer()\n \nself.ip = web.ctx.ip", "path": "service\\vendor\\web\\session.py", "repo_name": "anotherjesse/firenomics", "stars": 11, "license": "None", "language": "python", "size": 982}
{"docstring": "\"\"\"\nFormats `string` according to `pattern`, where the letter X gets replaced\nby characters from `string`.\n\n    >>> denumify(\"8005551212\", \"(XXX) XXX-XXXX\")\n    '(800) 555-1212'\n\n\"\"\"\n", "func_signal": "def denumify(string, pattern):\n", "code": "out = []\nfor c in pattern:\n    if c == \"X\":\n        out.append(string[0])\n        string = string[1:]\n    else:\n        out.append(c)\nreturn ''.join(out)", "path": "service\\vendor\\web\\utils.py", "repo_name": "anotherjesse/firenomics", "stars": 11, "license": "None", "language": "python", "size": 982}
{"docstring": "\"\"\"\nReturns a key whose value in `dictionary` is `element` \nor, if none exists, None.\n\n    >>> d = {1:2, 3:4}\n    >>> dictfind(d, 4)\n    3\n    >>> dictfind(d, 5)\n\"\"\"\n", "func_signal": "def dictfind(dictionary, element):\n", "code": "for (key, value) in dictionary.iteritems():\n    if element is value: \n        return key", "path": "service\\vendor\\web\\utils.py", "repo_name": "anotherjesse/firenomics", "stars": 11, "license": "None", "language": "python", "size": 982}
{"docstring": "# check for change of IP\n", "func_signal": "def _validate_ip(self):\n", "code": "if self.session_id and self.get('ip', None) != web.ctx.ip:\n    if self._config.ignore_change_ip:\n        self.session_id = None\n    else:\n       return self.expired()", "path": "service\\vendor\\web\\session.py", "repo_name": "anotherjesse/firenomics", "stars": 11, "license": "None", "language": "python", "size": 982}
{"docstring": "\"\"\"decodes the data to get back the session dict \"\"\"\n", "func_signal": "def decode(self, session_data):\n", "code": "pickled = base64.decodestring(session_data)\nreturn pickle.loads(pickled)", "path": "service\\vendor\\web\\session.py", "repo_name": "anotherjesse/firenomics", "stars": 11, "license": "None", "language": "python", "size": 982}
{"docstring": "\"\"\"\nConverts a (UTC) datetime object to a nice string representation.\n\n    >>> from datetime import datetime, timedelta\n    >>> d = datetime(1970, 5, 1)\n    >>> datestr(d, now=d)\n    '0 microseconds ago'\n    >>> for t, v in {\n    ...   timedelta(microseconds=1): '1 microsecond ago',\n    ...   timedelta(microseconds=2): '2 microseconds ago',\n    ...   -timedelta(microseconds=1): '1 microsecond from now',\n    ...   -timedelta(microseconds=2): '2 microseconds from now',\n    ...   timedelta(microseconds=2000): '2 milliseconds ago',\n    ...   timedelta(seconds=2): '2 seconds ago',\n    ...   timedelta(seconds=2*60): '2 minutes ago',\n    ...   timedelta(seconds=2*60*60): '2 hours ago',\n    ...   timedelta(days=2): '2 days ago',\n    ... }.iteritems():\n    ...     assert datestr(d, now=d+t) == v\n    >>> datestr(datetime(1970, 1, 1), now=d)\n    'January  1'\n    >>> datestr(datetime(1969, 1, 1), now=d)\n    'January  1, 1969'\n    >>> datestr(datetime(1970, 6, 1), now=d)\n    'June  1, 1970'\n\"\"\"\n", "func_signal": "def datestr(then, now=None):\n", "code": "def agohence(n, what, divisor=None):\n    if divisor: n = n // divisor\n\n    out = str(abs(n)) + ' ' + what       # '2 day'\n    if abs(n) != 1: out += 's'           # '2 days'\n    out += ' '                           # '2 days '\n    if n < 0:\n        out += 'from now'\n    else:\n        out += 'ago'\n    return out                           # '2 days ago'\n\noneday = 24 * 60 * 60\n\nif not now: now = datetime.datetime.utcnow()\nif type(now).__name__ == \"DateTime\":\n    now = datetime.datetime.fromtimestamp(now)\nif type(then).__name__ == \"DateTime\":\n    then = datetime.datetime.fromtimestamp(then)\ndelta = now - then\ndeltaseconds = int(delta.days * oneday + delta.seconds + delta.microseconds * 1e-06)\ndeltadays = abs(deltaseconds) // oneday\nif deltaseconds < 0: deltadays *= -1 # fix for oddity of floor\n\nif deltadays:\n    if abs(deltadays) < 4:\n        return agohence(deltadays, 'day')\n\n    out = then.strftime('%B %e') # e.g. 'June 13'\n    if then.year != now.year or deltadays < 0:\n        out += ', %s' % then.year\n    return out\n\nif int(deltaseconds):\n    if abs(deltaseconds) > (60 * 60):\n        return agohence(deltaseconds, 'hour', 60 * 60)\n    elif abs(deltaseconds) > 60:\n        return agohence(deltaseconds, 'minute', 60)\n    else:\n        return agohence(deltaseconds, 'second')\n\ndeltamicroseconds = delta.microseconds\nif delta.days: deltamicroseconds = int(delta.microseconds - 1e6) # datetime oddity\nif abs(deltamicroseconds) > 1000:\n    return agohence(deltamicroseconds, 'millisecond', 1000)\n\nreturn agohence(deltamicroseconds, 'microsecond')", "path": "service\\vendor\\web\\utils.py", "repo_name": "anotherjesse/firenomics", "stars": 11, "license": "None", "language": "python", "size": 982}
{"docstring": "\"\"\"Encodes text in utf-8.\n    \n    >> utf8(u'\\u1234') # doctest doesn't seem to like utf-8\n    '\\xe1\\x88\\xb4'\n\n    >>> utf8('hello')\n    'hello'\n    >>> utf8(42)\n    '42'\n\"\"\"\n", "func_signal": "def utf8(text):\n", "code": "if isinstance(text, unicode):\n    return text.encode('utf-8')\nelif isinstance(text, str):\n    return text\nelse:\n    return str(text)", "path": "service\\vendor\\web\\utils.py", "repo_name": "anotherjesse/firenomics", "stars": 11, "license": "None", "language": "python", "size": 982}
{"docstring": "\"\"\"Generate a random id for session\"\"\"\n\n", "func_signal": "def _generate_session_id(self):\n", "code": "while True:\n    rand = random.random()\n    now = time.time()\n    secret_key = self._config.secret_key\n    session_id = md5.new(\"%s%s%s%s\" %(rand, now, web.ctx.ip, secret_key))\n    session_id = session_id.hexdigest()\n    if session_id not in self.store:\n        break\nreturn session_id", "path": "service\\vendor\\web\\session.py", "repo_name": "anotherjesse/firenomics", "stars": 11, "license": "None", "language": "python", "size": 982}
{"docstring": "\"\"\"\nLike re.sub, but returns the replacement _and_ the match object.\n\n    >>> t, m = re_subm('g(oo+)fball', r'f\\\\1lish', 'goooooofball')\n    >>> t\n    'foooooolish'\n    >>> m.groups()\n    ('oooooo',)\n\"\"\"\n", "func_signal": "def re_subm(pat, repl, string):\n", "code": "compiled_pat = re_compile(pat)\nproxy = _re_subm_proxy()\ncompiled_pat.sub(proxy.__call__, string)\nreturn compiled_pat.sub(repl, string), proxy.match", "path": "service\\vendor\\web\\utils.py", "repo_name": "anotherjesse/firenomics", "stars": 11, "license": "None", "language": "python", "size": 982}
{"docstring": "\"\"\"\nReturns the keys whose values in `dictionary` are `element`\nor, if none exists, [].\n\n    >>> d = {1:4, 3:4}\n    >>> dictfindall(d, 4)\n    [1, 3]\n    >>> dictfindall(d, 5)\n    []\n\"\"\"\n", "func_signal": "def dictfindall(dictionary, element):\n", "code": "res = []\nfor (key, value) in dictionary.iteritems():\n    if element is value:\n        res.append(key)\nreturn res", "path": "service\\vendor\\web\\utils.py", "repo_name": "anotherjesse/firenomics", "stars": 11, "license": "None", "language": "python", "size": 982}
{"docstring": "\"\"\"\nIncrements `element` in `dictionary`, \nsetting it to one if it doesn't exist.\n\n    >>> d = {1:2, 3:4}\n    >>> dictincr(d, 1)\n    3\n    >>> d[1]\n    3\n    >>> dictincr(d, 5)\n    1\n    >>> d[5]\n    1\n\"\"\"\n", "func_signal": "def dictincr(dictionary, element):\n", "code": "dictionary.setdefault(element, 0)\ndictionary[element] += 1\nreturn dictionary[element]", "path": "service\\vendor\\web\\utils.py", "repo_name": "anotherjesse/firenomics", "stars": 11, "license": "None", "language": "python", "size": 982}
{"docstring": "\"\"\"Kill the session, make it no longer available\"\"\"\n", "func_signal": "def kill(self):\n", "code": "del self.store[self.session_id]\nself._killed = True", "path": "service\\vendor\\web\\session.py", "repo_name": "anotherjesse/firenomics", "stars": 11, "license": "None", "language": "python", "size": 982}
{"docstring": "\"\"\"\nReturns `integer` as an int or `default` if it can't.\n\n    >>> intget('3')\n    3\n    >>> intget('3a')\n    >>> intget('3a', 0)\n    0\n\"\"\"\n", "func_signal": "def intget(integer, default=None):\n", "code": "try:\n    return int(integer)\nexcept (TypeError, ValueError):\n    return default", "path": "service\\vendor\\web\\utils.py", "repo_name": "anotherjesse/firenomics", "stars": 11, "license": "None", "language": "python", "size": 982}
{"docstring": "# check for expiry\n", "func_signal": "def _check_expiry(self):\n", "code": "if self.session_id and self.session_id not in self.store:\n    if self._config.ignore_expiry:\n        self.session_id = None\n    else:\n        return self.expired()", "path": "service\\vendor\\web\\session.py", "repo_name": "anotherjesse/firenomics", "stars": 11, "license": "None", "language": "python", "size": 982}
{"docstring": "\"\"\"\nReturns `lst[ind]` if it exists, `default` otherwise.\n\n    >>> listget(['a'], 0)\n    'a'\n    >>> listget(['a'], 1)\n    >>> listget(['a'], 1, 'b')\n    'b'\n\"\"\"\n", "func_signal": "def listget(lst, ind, default=None):\n", "code": "if len(lst)-1 < ind: \n    return default\nreturn lst[ind]", "path": "service\\vendor\\web\\utils.py", "repo_name": "anotherjesse/firenomics", "stars": 11, "license": "None", "language": "python", "size": 982}
{"docstring": "\"\"\"Cleanup the stored sessions\"\"\"\n", "func_signal": "def _cleanup(self):\n", "code": "current_time = time.time()\ntimeout = self._config.timeout\nif self._last_cleanup_time + timeout > current_time:\n    self.store.cleanup(timeout)\n    self._last_cleanup_time = current_time", "path": "service\\vendor\\web\\session.py", "repo_name": "anotherjesse/firenomics", "stars": 11, "license": "None", "language": "python", "size": 982}
{"docstring": "\"\"\"\nReturns a dictionary consisting of the keys in the argument dictionaries.\nIf they share a key, the value from the last argument is used.\n\n    >>> dictadd({1: 0, 2: 0}, {2: 1, 3: 1})\n    {1: 0, 2: 1, 3: 1}\n\"\"\"\n", "func_signal": "def dictadd(*dicts):\n", "code": "result = {}\nfor dct in dicts:\n    result.update(dct)\nreturn result", "path": "service\\vendor\\web\\utils.py", "repo_name": "anotherjesse/firenomics", "stars": 11, "license": "None", "language": "python", "size": 982}
{"docstring": "\"\"\"\nConverts text to HTML following the rules of Markdown, but blocking any\noutside HTML input, so that only the things supported by Markdown\ncan be used. Also converts raw URLs to links.\n\n(requires [markdown.py](http://webpy.org/markdown.py))\n\"\"\"\n", "func_signal": "def safemarkdown(text):\n", "code": "from markdown import markdown\nif text:\n    text = text.replace('<', '&lt;')\n    # TODO: automatically get page title?\n    text = r_url.sub(r'<\\1>', text)\n    text = markdown(text)\n    return text", "path": "service\\vendor\\web\\utils.py", "repo_name": "anotherjesse/firenomics", "stars": 11, "license": "None", "language": "python", "size": 982}
{"docstring": "# assert x isa docval\n", "func_signal": "def typeOf(x):\n", "code": "if isinstance(x, Doc):\n    return x.tag\nelse:\n    return typeSymbols[type(x)]", "path": "logix\\trunk\\logix\\data.py", "repo_name": "tslocke/Logix", "stars": 10, "license": "None", "language": "python", "size": 539}
{"docstring": "# `pyfunc` is already an ast, other args need translating\n", "func_signal": "def compileFunctionCall(pyfunc, argxs, kwxs, starArg=None, dstarArg=None):\n", "code": "for a in itools.chain(argxs, kwxs.values()):\n    assertResult(a, 'pass')\n\nif starArg:\n    assertResult(starArg, \"pass\")\n\nif dstarArg:\n    assertResult(dstarArg, \"pass\")\n\nargsc = map(topy, argxs)\nkeywords = [ast.Keyword(str(n), topy(v)) for n,v in kwxs.items()]\n\nreturn ast.CallFunc(pyfunc, argsc + keywords,\n                    starArg and topy(starArg),\n                    dstarArg and topy (dstarArg))", "path": "logix\\branches\\tom\\logix\\pycompile.py", "repo_name": "tslocke/Logix", "stars": 10, "license": "None", "language": "python", "size": 539}
{"docstring": "#print '---CACHE STORE--- %s:' % self\n#print blocktext.splitlines()[0]\n#print '...'\n#print blocktext.splitlines()[-1]\n#print\n\n", "func_signal": "def setBlockCache(self, blocktext, lines, lineno):\n", "code": "textid = id(blocktext)\ncopy = [data.copy(line) for line in lines]\nself._blockCache[blocktext] = (copy, lineno, textid)\nself._blockCacheUsed.append(textid)\nself._blockCacheSize += len(blocktext)\n\n# {{{ remove old caches if cache is too big\nwhile self._blockCacheSize > LanguageImpl.maxCacheSize:\n    delid = self._blockCacheUsed[0]\n    for text, (lines, lineno, textid) in self._blockCache.items():\n        if textid == delid:\n            del self._blockCache[text]\n            del self._blockCacheUsed[0]\n            self._blockCacheSize -= len(text)\n            break\n    else:\n        assert 0, \"id not in _blockCache\"\n# }}}", "path": "logix\\trunk\\logix\\language.py", "repo_name": "tslocke/Logix", "stars": 10, "license": "None", "language": "python", "size": 539}
{"docstring": "\"\"\"\nThis is where the meat is.  Basically the data_files list must\nnow be a list of tuples of 3 entries.  The first\nentry is one of 'base', 'platbase', etc, which indicates which\nbase to install from.  The second entry is the path to install\ntoo.  The third entry is a list of files to install.\n\"\"\"\n", "func_signal": "def run(self):\n", "code": "for lof in self.data_files:\n    if lof[0]:\n        base = getattr(self, 'install_' + lof[0])\n    else:\n        base = getattr(self, 'install_base')\n    dir = convert_path(lof[1])\n    if not os.path.isabs(dir):\n        dir = os.path.join(base, dir)\n    elif self.root:\n        dir = change_root(self.root, dir)\n    self.mkpath(dir)\n\n    files = lof[2]\n    if len(files) == 0:\n        # If there are no files listed, the user must be\n        # trying to create an empty directory, so add the the\n        # directory to the list of output files.\n        self.outfiles.append(dir)\n    else:\n        # Copy files, adding them to the list of output files.\n        for f in files:\n            f = convert_path(f)\n            (out, _) = self.copy_file(f, dir)\n            #print \"DEBUG: \", out  # dbg\n            self.outfiles.append(out)\n            \n\nreturn self.outfiles", "path": "logixtest\\trunk\\setup.py", "repo_name": "tslocke/Logix", "stars": 10, "license": "None", "language": "python", "size": 539}
{"docstring": "\"\"\"\nThis is where the meat is.  Basically the data_files list must\nnow be a list of tuples of 3 entries.  The first\nentry is one of 'base', 'platbase', etc, which indicates which\nbase to install from.  The second entry is the path to install\ntoo.  The third entry is a list of files to install.\n\"\"\"\n", "func_signal": "def run(self):\n", "code": "for lof in self.data_files:\n    if lof[0]:\n        base = getattr(self, 'install_' + lof[0])\n    else:\n        base = getattr(self, 'install_base')\n    dir = convert_path(lof[1])\n    if not os.path.isabs(dir):\n        dir = os.path.join(base, dir)\n    elif self.root:\n        dir = change_root(self.root, dir)\n    self.mkpath(dir)\n\n    files = lof[2]\n    if len(files) == 0:\n        # If there are no files listed, the user must be\n        # trying to create an empty directory, so add the the\n        # directory to the list of output files.\n        self.outfiles.append(dir)\n    else:\n        # Copy files, adding them to the list of output files.\n        for f in files:\n            f = convert_path(f)\n            (out, _) = self.copy_file(f, dir)\n            #print \"DEBUG: \", out  # dbg\n            self.outfiles.append(out)\n            \n\nreturn self.outfiles", "path": "logix\\branches\\tom\\setup.py", "repo_name": "tslocke/Logix", "stars": 10, "license": "None", "language": "python", "size": 539}
{"docstring": "# ensure the queue is current\n", "func_signal": "def setMark(self):\n", "code": "t = self.nextToken()\n\nif isinstance(t, LayoutToken):\n    queue = self.tokenqueue\n    col = self.col\nelse:\n    _, col = self.getPos()\n    queue = []\n\nself.marks.append((self.lineno,\n                   col,\n                   self.linestart,\n                   self.blocks[:],\n                   queue[:],\n                   ))", "path": "logix\\trunk\\logix\\parser.py", "repo_name": "tslocke/Logix", "stars": 10, "license": "None", "language": "python", "size": 539}
{"docstring": "\"\"\"The main compiler function - convert a code-doc into an ast node\"\"\"\n\n# {{{ ln = line number if available (also set global \\lineno)\n", "func_signal": "def topy(obj):\n", "code": "global lineno\nln = getattr(obj, 'lineno', None)\nif ln != None:\n    lineno = ln\n# }}}\n\ntyp = type(obj).__name__\nmethod = (getattr(objtopy, typ, None) or\n          getattr(objtopy, \"_\" + typ, None))\ncompilefunc = method and method.im_func\n\nif compilefunc:\n    node = compilefunc(obj)\nelse:\n    debug()\n    raise CompileError(\"Cannot compile %s (of type %s)\" % (obj, typ))\n\nif ln:\n    node.lineno = ln\n\nreturn node", "path": "logix\\branches\\tom\\logix\\pycompile.py", "repo_name": "tslocke/Logix", "stars": 10, "license": "None", "language": "python", "size": 539}
{"docstring": "\"\"\"\nGenerate code to build a Doc like `doc` with quote-escapes\nreplaced by runtime values.\n\"\"\"\n\n# Each element of contentParts is either:\n#  an AST - meaning a spliced value - the resulting content\n#           should be `extend`ed by that value\n#  a list of ASTs - a list of regular elements, the resulting\n#                   content should be `extend`ed by an ast.List\n#                   with these elements\n# {{{ contentParts = \n", "func_signal": "def quotedDoc(doc, depth):\n", "code": "contentParts = [[]]\n\nfor x in doc.content():\n    if isDoc(x, rootops.escape):\n        escapelevel = 1 + len(x.get('extra', []))\n        if escapelevel > depth:\n            raise CompileError(\"more quote escapes than quotes\")\n        escape = escapelevel == depth\n    else:\n        escape = False\n        \n    if escape:\n        if x.hasProperty(\"splice\"):\n            assertResult(x[0], \"insert into quote\")\n            contentParts.append(topy(x[0]))\n            contentParts.append([])\n        elif x.hasProperty(\"localmodule\"):\n            contentParts[-1].append(topy(localModuleQuote()))\n        else:\n            assertResult(x[0], \"insert into quote\")\n            contentParts[-1].append(topy(x[0]))\n        \n    else:\n        contentParts[-1].append(quote(x, depth))\n# }}}\n\n# These properties are added to the result doc *after*\n# any spliced in docs, so they overwrite any spliced properties\nfor v in doc.propertyValues():\n    assertResult(v, \"use as operand\")\nproperties = ast.Dict([(compileSymbol(n), quote(v, depth))\n                       for n, v in doc.properties()])\n    \n# assert isinstance(contentParts[0], list)\n\nif contentParts == [[]]:\n    return ast.CallFunc(logixglobal(\"Doc\"),\n                        [compileSymbol(doc.tag),\n                         properties])\nelse:\n    if len(contentParts[0]) > 0:\n        docArg = ast.List(contentParts[0])\n        rest = contentParts[1:]\n    elif len(contentParts) > 0:\n        docArg = contentParts[1]\n        rest = contentParts[2:]\n    \n    if len(rest) == 0:\n        return ast.CallFunc(logixglobal(\"Doc\"),\n                            [compileSymbol(doc.tag),\n                             docArg,\n                             properties])\n    else:\n        val = macros.gensym(\"val\")\n        stmts = [ast.Assign([compilePlace(val)],\n                            ast.CallFunc(logixglobal(\"Doc\"),\n                                         [compileSymbol(doc.tag),\n                                          docArg]))]\n        for part in rest:\n            if isinstance(part, list):\n                if len(part) == 0:\n                    continue\n                ext = ast.List(part)\n            else:\n                ext = part\n            stmts.append(ast.Discard(ast.CallFunc(ast.Getattr(topy(val),\n                                                              \"extend\"),\n                                                  [ext])))\n    \n        stmts.append(ast.Discard(ast.CallFunc(ast.Getattr(topy(val),\n                                                          \"extend\"),\n                                              [properties])))\n        stmts.append(topy(val))\n        return ast.Stmt(stmts)", "path": "logix\\branches\\tom\\logix\\pycompile.py", "repo_name": "tslocke/Logix", "stars": 10, "license": "None", "language": "python", "size": 539}
{"docstring": "\"\"\"\nThis is where the meat is.  Basically the data_files list must\nnow be a list of tuples of 3 entries.  The first\nentry is one of 'base', 'platbase', etc, which indicates which\nbase to install from.  The second entry is the path to install\ntoo.  The third entry is a list of files to install.\n\"\"\"\n", "func_signal": "def run(self):\n", "code": "for lof in self.data_files:\n    if lof[0]:\n        base = getattr(self, 'install_' + lof[0])\n    else:\n        base = getattr(self, 'install_base')\n    dir = convert_path(lof[1])\n    if not os.path.isabs(dir):\n        dir = os.path.join(base, dir)\n    elif self.root:\n        dir = change_root(self.root, dir)\n    self.mkpath(dir)\n\n    files = lof[2]\n    if len(files) == 0:\n        # If there are no files listed, the user must be\n        # trying to create an empty directory, so add the the\n        # directory to the list of output files.\n        self.outfiles.append(dir)\n    else:\n        # Copy files, adding them to the list of output files.\n        for f in files:\n            f = convert_path(f)\n            (out, _) = self.copy_file(f, dir)\n            #print \"DEBUG: \", out  # dbg\n            self.outfiles.append(out)\n            \n\nreturn self.outfiles", "path": "logixtest\\tags\\release-0.4\\setup.py", "repo_name": "tslocke/Logix", "stars": 10, "license": "None", "language": "python", "size": 539}
{"docstring": "#Q: Is getops handled correctly here?\n\n", "func_signal": "def processLine(self, line, execenv, tokens):\n", "code": "if isDoc(line, rootops.setlang):\n    exp = macros.expand(line)\n    newlang = parseEval(exp[0], tokens, execenv)\n    if not isinstance(newlang, Language):\n        raise ParseError(\"setlang to non-language: %r\", line.lineno)\n    \n    return [], newlang\nelse:\n    exp = macros.expand(line)\n    if isinstance(exp, macros.splice):\n        lines = exp.items\n    else:\n        lines = [exp]\n\n    for line in lines:\n        if isDoc(line, rootops.deflang):\n            raise ParseError(\"deflang must be at the top-level\", line.lineno)\n        elif isDoc(line, rootops.setlang):\n            raise ParseError(\"invalid setlang in splice\", line.lineno)\n\n        # {{{ maybe add custom atributes to line\n        if isDoc(line, (rootops.defop, rootops.getops)):\n            langmod = self.newlang.__module__\n            newlangname = self.newlang.__impl__.name\n            if langmod == execenv['__name__']:\n                lang = newlangname\n            else:\n                assert 0, (\"PROBE: How could the language in a deflang\"\n                           \"come from another module?!\")\n                lang = \"%s.%s\" % (langmod, newlangname)\n            line['lang'] = lang\n        # }}}\n\n        parseEval(line, tokens, execenv, self.locals)\n\n        if isDoc(line, (rootops.getops, rootops.defop)):\n            # New operators(s) may mean an already matched token\n            # is wrong\n            tokens.clearTokenQueue()\n\n        lang = self.newlang\n        for var in self.locals:\n            if ((not hasattr(lang, var))\n                or getattr(lang, var) is not self.locals[var]):\n                setattr(lang, var, self.locals[var])\n\n    return lines, None", "path": "logix\\trunk\\logix\\parser.py", "repo_name": "tslocke/Logix", "stars": 10, "license": "None", "language": "python", "size": 539}
{"docstring": "#print '---CACHE STORE--- %s:' % self\n#print blocktext.splitlines()[0]\n#print '...'\n#print blocktext.splitlines()[-1]\n#print\n\n", "func_signal": "def setBlockCache(self, blocktext, lines, lineno):\n", "code": "textid = id(blocktext)\ncopy = [data.copy(line) for line in lines]\nself._blockCache[blocktext] = (copy, lineno, textid)\nself._blockCacheUsed.append(textid)\nself._blockCacheSize += len(blocktext)\n\n# {{{ remove old caches if cache is too big\nwhile self._blockCacheSize > LanguageImpl.maxCacheSize:\n    delid = self._blockCacheUsed[0]\n    for text, (lines, lineno, textid) in self._blockCache.items():\n        if textid == delid:\n            del self._blockCache[text]\n            del self._blockCacheUsed[0]\n            self._blockCacheSize -= len(text)\n            break\n    else:\n        assert 0, \"id not in _blockCache\"\n# }}}", "path": "logix\\branches\\tom\\logix\\language.py", "repo_name": "tslocke/Logix", "stars": 10, "license": "None", "language": "python", "size": 539}
{"docstring": "# We should also corresponding stuff tokens from self.toks and self.toks\n# but we don't because a) we don't have enough info to know what to remove\n# and b) semantics are not effected (only regex performance)\n\n", "func_signal": "def delOpXXX(self, token):\n", "code": "if token in self.operators or not self.getOp(token) != None:\n    raise ValueError(\"no such inherited operator '%s'\" % token)\nelse:\n    self.operators[token] = False", "path": "logix\\branches\\tom\\logix\\language.py", "repo_name": "tslocke/Logix", "stars": 10, "license": "None", "language": "python", "size": 539}
{"docstring": "#print '---CACHE STORE--- %s:' % self\n#print blocktext.splitlines()[0]\n#print '...'\n#print blocktext.splitlines()[-1]\n#print\n\n", "func_signal": "def setBlockCache(self, blocktext, lines, lineno):\n", "code": "textid = id(blocktext)\ncopy = [codeCopy(line) for line in lines]\nself._blockCache[blocktext] = (copy, lineno, textid)\nself._blockCacheUsed.append(textid)\nself._blockCacheSize += len(blocktext)\n\n# {{{ remove old caches if cache is too big\nwhile self._blockCacheSize > LanguageImpl.maxCacheSize:\n    delid = self._blockCacheUsed[0]\n    for text, (lines, lineno, textid) in self._blockCache.items():\n        if textid == delid:\n            del self._blockCache[text]\n            del self._blockCacheUsed[0]\n            self._blockCacheSize -= len(text)\n            break\n    else:\n        assert 0, \"id not in _blockCache\"\n# }}}", "path": "logix\\tags\\release-0.4.2\\logix\\language.py", "repo_name": "tslocke/Logix", "stars": 10, "license": "None", "language": "python", "size": 539}
{"docstring": "# [-2::-1] gives a list with the last element dropped, and then reversed\n", "func_signal": "def outerLanguage(self):\n", "code": "for lang in self.languageStack[-2::-1]:\n    if lang != self.currentLanguage():\n        return lang\nreturn self.currentLanguage()", "path": "logix\\trunk\\logix\\parser.py", "repo_name": "tslocke/Logix", "stars": 10, "license": "None", "language": "python", "size": 539}
{"docstring": "# {{{ return True if parent terminator says so\n", "func_signal": "def terminator(nextOp=None, prefixBinding=None):\n", "code": "if prefixBinding is None:\n    prefixBind = opPrefixBinding\nelif opPrefixBinding is None:\n    prefixBind = prefixBinding\nelse:\n    prefixBind = min(opPrefixBinding, prefixBinding)\nif isTerminator(nextOp, prefixBind):\n    return True\n# }}}\n\nif nextOp: # continuation operator - no token\n    nextToken = None\nelse:\n    nextToken = tokens.nextToken() \n    nextOp = tokens.nextOperator()\n\nif nextOp is failed:\n    return True\nif nextOp:\n    # Encountered a new operator while parsing rhs of \\operator\n    # return True to stop parsing rhs of \\operator\n    #    i.e. \\operator and its rhs become the lhs sub-expr\n    #    of \\nextOp\n    # return False to keep parsing rhs of \\operator\n    #    i.e. \\nextOp becomes a sub-expression in the rhs\n    #    of \\operator\n\n    nextSyn = nextOp.syntax\n\n    # prefix operator always starts sub-expression\n    if nextSyn.isPrefixOp(nextToken):\n        return False\n\n    # if nextOp is inside any prefix operator, it cannot\n    # terminate this operator unless it binds looser than\n    # that prefix op\n    if prefixBinding is not None and nextSyn.binding > prefixBinding:\n        return False\n\n    # end operator here if next operator binds more loosely\n    # continue if it binds more tightly\n\n    # {{{ determin if current and next are packed\n    currentPacked = opsyn.packed(optoken)\n    if opsyn.smartspace and not currentPacked:\n        # current operator is not allowed to pack\n        # so neither is next\n        nextPacked = False\n    else:\n        nextPacked = nextSyn.packed(nextToken)\n    # }}}\n\n    if currentPacked and not nextPacked:\n        return True\n    elif not currentPacked and nextPacked:\n        return False\n    elif (nextSyn.binding < opsyn.binding\n          or (nextSyn.binding == opsyn.binding\n              and opsyn.associatesLeft())):\n        return True\n    else:\n        return False\n\nelse:\n    return False", "path": "logix\\trunk\\logix\\parser.py", "repo_name": "tslocke/Logix", "stars": 10, "license": "None", "language": "python", "size": 539}
{"docstring": "# We should also corresponding stuff tokens from self.toks and self.toks\n# but we don't because a) we don't have enough info to know what to remove\n# and b) semantics are not effected (only regex performance)\n\n", "func_signal": "def delOpXXX(self, token):\n", "code": "if token in self.operators or not self.getOp(token) != None:\n    raise ValueError(\"no such inherited operator '%s'\" % token)\nelse:\n    self.operators[token] = False", "path": "logix\\tags\\release-0.4.2\\logix\\language.py", "repo_name": "tslocke/Logix", "stars": 10, "license": "None", "language": "python", "size": 539}
{"docstring": "# We should also corresponding stuff tokens from self.toks and self.toks\n# but we don't because a) we don't have enough info to know what to remove\n# and b) semantics are not effected (only regex performance)\n\n", "func_signal": "def delOpXXX(self, token):\n", "code": "if token in self.operators or not self.getOp(token) != None:\n    raise ValueError(\"no such inherited operator '%s'\" % token)\nelse:\n    self.operators[token] = False", "path": "logix\\trunk\\logix\\language.py", "repo_name": "tslocke/Logix", "stars": 10, "license": "None", "language": "python", "size": 539}
{"docstring": "# {{{ return True if parent terminator says so\n", "func_signal": "def terminator(nextOp=None, prefixBinding=None):\n", "code": "if prefixBinding is None:\n    prefixBind = opPrefixBinding\nelif opPrefixBinding is None:\n    prefixBind = prefixBinding\nelse:\n    prefixBind = min(opPrefixBinding, prefixBinding)\nif isTerminator(nextOp, prefixBind):\n    return True\n# }}}\n\nif nextOp: # continuation operator - no token\n    nextToken = None\nelse:\n    nextToken = tokens.nextToken() \n    nextOp = tokens.nextOperator()\n\nif nextOp is failed:\n    return True\nif nextOp:\n    # Encountered a new operator while parsing rhs of \\operator\n    # return True to stop parsing rhs of \\operator\n    #    i.e. \\operator and its rhs become the lhs sub-expr\n    #    of \\nextOp\n    # return False to keep parsing rhs of \\operator\n    #    i.e. \\nextOp becomes a sub-expression in the rhs\n    #    of \\operator\n\n    nextSyn = nextOp.syntax\n\n    # prefix operator always starts sub-expression\n    if nextSyn.isPrefixOp(nextToken):\n        return False\n\n    # if nextOp is inside any prefix operator, it cannot\n    # terminate this operator unless it binds looser than\n    # that prefix op\n    if prefixBinding is not None and nextSyn.binding > prefixBinding:\n        return False\n\n    # end operator here if next operator binds more loosely\n    # continue if it binds more tightly\n\n    # {{{ determin if current and next are packed\n    currentPacked = opsyn.packed(optoken)\n    if opsyn.smartspace and not currentPacked:\n        # current operator is not allowed to pack\n        # so neither is next\n        nextPacked = False\n    else:\n        nextPacked = nextSyn.packed(nextToken)\n    # }}}\n\n    if currentPacked and not nextPacked:\n        return True\n    elif not currentPacked and nextPacked:\n        return False\n    elif (nextSyn.binding < opsyn.binding\n          or (nextSyn.binding == opsyn.binding\n              and opsyn.associatesLeft())):\n        return True\n    else:\n        return False\n\nelse:\n    return False", "path": "logix\\branches\\tom\\logix\\parser.py", "repo_name": "tslocke/Logix", "stars": 10, "license": "None", "language": "python", "size": 539}
{"docstring": "#Q: Is getops handled correctly here?\n\n", "func_signal": "def processLine(self, line, execenv, tokens):\n", "code": "if isDoc(line, rootops.setlang):\n    exp = macros.expand(line)\n    newlang = parseEval(exp[0], tokens, execenv)\n    if not isinstance(newlang, Language):\n        raise ParseError(\"setlang to non-language: %r\", line.lineno)\n    \n    return [], newlang\nelse:\n    exp = macros.expand(line)\n    if isinstance(exp, macros.splice):\n        lines = exp.items\n    else:\n        lines = [exp]\n\n    for line in lines:\n        if isDoc(line, rootops.deflang):\n            raise ParseError(\"deflang must be at the top-level\", line.lineno)\n        elif isDoc(line, rootops.setlang):\n            raise ParseError(\"invalid setlang in splice\", line.lineno)\n\n        # {{{ maybe add custom atributes to line\n        if isDoc(line, (rootops.defop, rootops.getops)):\n            langmod = self.newlang.__module__\n            newlangname = self.newlang.__impl__.name\n            if langmod == execenv['__name__']:\n                lang = newlangname\n            else:\n                assert 0, (\"PROBE: How could the language in a deflang\"\n                           \"come from another module?!\")\n                lang = \"%s.%s\" % (langmod, newlangname)\n            line['lang'] = lang\n        # }}}\n\n        parseEval(line, tokens, execenv, self.locals)\n\n        if isDoc(line, (rootops.getops, rootops.defop)):\n            # New operators(s) may mean an already matched token\n            # is wrong\n            tokens.clearTokenQueue()\n\n        lang = self.newlang\n        for var in self.locals:\n            if ((not hasattr(lang, var))\n                or getattr(lang, var) is not self.locals[var]):\n                setattr(lang, var, self.locals[var])\n\n    return lines, None", "path": "logix\\branches\\tom\\logix\\parser.py", "repo_name": "tslocke/Logix", "stars": 10, "license": "None", "language": "python", "size": 539}
{"docstring": "# [-2::-1] gives a list with the last element dropped, and then reversed\n", "func_signal": "def outerLanguage(self):\n", "code": "for lang in self.languageStack[-2::-1]:\n    if lang != self.currentLanguage():\n        return lang\nreturn self.currentLanguage()", "path": "logix\\branches\\tom\\logix\\parser.py", "repo_name": "tslocke/Logix", "stars": 10, "license": "None", "language": "python", "size": 539}
{"docstring": "'''Restores UI state from the last time it was opened.\n'''\n", "func_signal": "def restoreUiState(self):\n", "code": "index = int(self.settings.value('ui_state/selected_book_index', -1).toPyObject())\n\nif index != -1:\n    self.ui.selectBook.setCurrentIndex(index)", "path": "src\\manabidict\\ui\\dictionary.py", "repo_name": "aehlke/manabi-dict", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 2358}
{"docstring": "'''Display preferences dialog.\n'''\n", "func_signal": "def on_actionPreferences_triggered(self):\n", "code": "prefs = Preferences(self.book_manager, parent=self)\n#prefs.setModal(True)\n#prefs.show()\nprefs.exec_()\nself.reload_books_list()\nself.reload_search_menu()", "path": "src\\manabidict\\ui\\dictionary.py", "repo_name": "aehlke/manabi-dict", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 2358}
{"docstring": "'''Selects the book given an EpwingBook instance, or everything if `all_books` is True.\nWill select a category instead if `category` is not None.\n'''\n", "func_signal": "def select_book(self, book=None, all_books=False, category=None):\n", "code": "sb = self.ui.selectBook\nif all_books:\n    index = 0\nelif category:\n    index = sb.findText(category.label)\nelse:\n    index = sb.findData(book.id)\nif index != -1:\n    sb.setCurrentIndex(index)", "path": "src\\manabidict\\ui\\dictionary.py", "repo_name": "aehlke/manabi-dict", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 2358}
{"docstring": "'''Sets up the JS bridge for entryView.\n'''\n", "func_signal": "def setupJavaScriptBridge(self):\n", "code": "self.javascript_bridge = JavaScriptBridge(self)\nself._setupJavaScriptBridge()\nself.ui.entryView.page().mainFrame().javaScriptWindowObjectCleared.connect(self._setupJavaScriptBridge)", "path": "src\\manabidict\\ui\\dictionary.py", "repo_name": "aehlke/manabi-dict", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 2358}
{"docstring": "'''Go forward if positive, back if negative.\n'''\n#self.push_history()\n", "func_signal": "def _go_history(self, index):\n", "code": "self.stage_history(clear_forward_items=False)\ntry:\n    history_item = self.history.go(index)\n    #print 'going {0} to:'.format(index)#,\n    #print back_item\n    #print len(self.history)\n    self.load_history_item(history_item)\n    #if 'search_results' in back_item:\n        #self.show_results(back_item['search_results'])\n        #self.ui.searchResults.setCurrentRow(back_item['search_results_current_row'])\n    #self.show_entry(back_item['entry'])\n    #if 'entry_hash' in back_item:\n        #self.ui.entryView.scrollToAnchor(back_item['entry_hash'])\n    #if 'book' in back_item:\n        #self.select_book(book=back_item['book'])\nexcept IndexError:\n    # already at edge of history, ignore request\n    pass\nself.refresh_history_buttons()", "path": "src\\manabidict\\ui\\dictionary.py", "repo_name": "aehlke/manabi-dict", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 2358}
{"docstring": "'''Sets the list of search results, displaying them in the list box.\n'''\n", "func_signal": "def show_results(self, results):\n", "code": "ran_at = time.time()\nself._results_last_shown_at = ran_at\nself._current_results = results\n#self._show_results_queue.append(queue_time)\nsr = self.ui.searchResults\nsr.clear()\nq_app = QApplication.instance()\nq_app.processEvents()\n\ni = 0\n\nhtml_hints = ['<', '>', '&lt;', '&gt;']\ndef add_result(heading, result):\n    # add an HTML item if it contains HTML\n\n    if any(imap(lambda x: x in heading, html_hints)):\n        sr.addHtmlItem(heading, result)\n    else:\n        item = QListWidgetItem(heading)\n        item.setData(Qt.UserRole, result)\n        sr.addItem(item)\n\nif isinstance(results, dict):\n    def sort_key(e):\n        return e[1]\n        #print e\n        #key = e[0]\n        #return key.lower().strip()\n\n    for heading, entries in sorted(results.items(), key=sort_key):\n        if ran_at < self._results_last_shown_at:\n            return\n        add_result(heading, entries)\n        if not i % 3:\n            q_app.processEvents()\n        i += 1\nelse:\n    for result in results:\n        if ran_at < self._results_last_shown_at:\n            return\n        add_result(result.heading, result)\n        if not i % 3:\n            q_app.processEvents()\n        i += 1\n\nsr.scrollToItem(self.ui.searchResults.item(0))", "path": "src\\manabidict\\ui\\dictionary.py", "repo_name": "aehlke/manabi-dict", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 2358}
{"docstring": "'''Adds a new QTextEdit widget containing the given HTMl as an item in the list.\n'''\n", "func_signal": "def addHtmlItem(self, html, data):\n", "code": "item = QListWidgetItem()\nitem.setData(Qt.UserRole, data)\nself.addItem(item)\nitem.setSizeHint(self._htmlItemWidget.frameSize())\n\nlabel = QLabel()\nlabel.setTextInteractionFlags(Qt.NoTextInteraction)\n\nhtml = u'<html><body style=\"margin:0px 0px 0px 4px\">{0}</body></html>'.format(html)\n\nif '<img' in html:\n    self._htmlItemWidgetFinishedLoading = False\n    self._htmlItemWidget.setHtml(html)\n\n    pm = QImage(self._htmlItemWidget.size(), QImage.Format_ARGB32)\n    pm.fill(Qt.transparent)\n\n    # wait for it to finish loading, then render\n    q_app = QApplication.instance()\n    while not self._htmlItemWidgetFinishedLoading:\n        q_app.processEvents(QEventLoop.WaitForMoreEvents | QEventLoop.ExcludeUserInputEvents)\n    \n    self._htmlItemWidget.render(pm, flags=QWidget.DrawChildren)\n\n    label.setPixmap(QPixmap.fromImage(pm))\nelse:\n    label.setText(html)\n\nself.setItemWidget(item, label)", "path": "src\\manabidict\\ui\\mlistwidget.py", "repo_name": "aehlke/manabi-dict", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 2358}
{"docstring": "'''Pushes the currently staged back item to the history manager.\n'''\n#print self.history\n#if self._staged_back_item\n#FIXME\n", "func_signal": "def push_history(self):\n", "code": "if self.history.current_location:\n    self.history.push()\n\nself.refresh_history_buttons()", "path": "src\\manabidict\\ui\\dictionary.py", "repo_name": "aehlke/manabi-dict", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 2358}
{"docstring": "'''Set the staged back item to the current entry and search context.\n`label` is what the history item will show as in menus. Defaults to the entry heading.\n'''\n", "func_signal": "def stage_history(self, clear_forward_items=True, label=None, url=None):\n", "code": "if self._push_history_on_next_stage:\n    self.push_history()\n    self._push_history_on_next_stage = False\n\nitem = self.create_history_item(url=url)\nself.history.current_location = item\n\nif clear_forward_items:\n    self.history.forward_items = []\n\n        #lines = strip_tags(entry.text.replace('<br>', '\\n')).split()\n#'search_results': self._current_results,\n#'search_results_current_row': self.ui.searchResults.currentRow(),\n    #'':", "path": "src\\manabidict\\ui\\dictionary.py", "repo_name": "aehlke/manabi-dict", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 2358}
{"docstring": "'''Performs search.\n'''\n", "func_signal": "def do_search(self, query=None, search_method=None, max_results_per_book=25):\n", "code": "selected_book = self.selected_book()\n#if not selected_book: return\n\nif not query:\n    query = unicode(self.ui.searchField.search_field.text())\n\nif self._last_query is None:\n    self._last_query = query\nelif query != self._last_query:\n    self._push_history_on_next_stage = True\n    self._last_query = query\n\nif not search_method:\n    search_method = self.selected_search_method()\n\nif selected_book is None:\n    # search all\n    results = self.book_manager.search_all(\n            query, search_method=search_method, max_results_per_book=max_results_per_book)\nelif inspect.isclass(selected_book) and issubclass(selected_book, BookCategory):\n    # category search\n    results = self.book_manager.search_category(\n            selected_book, query, search_method=search_method, max_results_per_book=max_results_per_book)\nelse:\n    results = list(islice(selected_book.search(\n            query, search_method=search_method), 0, max_results_per_book))\n\nself.show_results(results)", "path": "src\\manabidict\\ui\\dictionary.py", "repo_name": "aehlke/manabi-dict", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 2358}
{"docstring": "'''Returns the currently selected book's EpwingBook instance,\nor BookCategory instance if a category is selected instead.\n'''\n", "func_signal": "def selected_book(self):\n", "code": "sb = self.ui.selectBook\nif not sb.count(): return None\n\nselected_data = sb.itemData(sb.currentIndex())\nif selected_data.type() == QVariant.String:\n    selected_data = selected_data.toString()\n    return self.book_manager.books[unicode(selected_data)]\nelse:\n    return selected_data.toPyObject()", "path": "src\\manabidict\\ui\\dictionary.py", "repo_name": "aehlke/manabi-dict", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 2358}
{"docstring": "'''Fills ui.searchMethod with the available methods for the current book.\nCurrently a stub that just fills it with some defaults.\n'''\n", "func_signal": "def reload_search_methods_list(self):\n", "code": "methods = [('prefix', 'Begins with'), ('suffix', 'Ends with'), ('exact', 'Exactly')]\nsm = self.ui.searchMethod\nsm.clear()\nfor id, name in methods:\n    sm.addItem(name, id)", "path": "src\\manabidict\\ui\\dictionary.py", "repo_name": "aehlke/manabi-dict", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 2358}
{"docstring": "'''Move focus to searchField if text is being entered.\nOnly handle events for .\n'''\n", "func_signal": "def eventFilter(self, obj, event):\n", "code": "if self.parent.isActiveWindow():\n    #print event\n    #print event.type() == QEvent.Gesture\n    search_field = self.parent.ui.searchField.search_field\n    if event.type() == QEvent.KeyPress:\n        key_event = QKeyEvent(event)\n\n        if obj is not search_field \\\n                and key_event.text() \\\n                and key_event.text() not in self.control_chars:\n            search_field.setFocus()\n            search_field.clear()\n            search_field.keyPressEvent(event)\n            return True\n        elif key_event.key() in [Qt.Key_PageUp, Qt.Key_PageDown, Qt.Key_Home, Qt.Key_End]:\n            # scroll the entryView\n            return self.parent.ui.entryView.event(event)\n        #else:\n            #print str(QApplication.instance().focusWidget())\n    elif event.type() == QEvent.InputMethod:\n        input_method_event = QInputMethodEvent(event)\n\n        if obj is not search_field:\n            #search_field.setFocus()\n            #search_field.clear()\n            search_field.inputMethodEvent(input_method_event)\n            return True\n    elif event.type() == QEvent.Gesture:\n        swipe = event.gesture(Qt.SwipeGesture)\n\n        if swipe and swipe.state() == Qt.GestureFinished:\n            # go back or forward in history\n            if swipe.horizontalDirection() == swipe.Left:\n                self.parent.go_back()\n            elif swipe.horizontalDirection() == swipe.Right:\n                self.parent.go_forward()\n            return True\n\nreturn QObject.eventFilter(self, obj, event)", "path": "src\\manabidict\\ui\\dictionary.py", "repo_name": "aehlke/manabi-dict", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 2358}
{"docstring": "'''Selects the search field, so the user can start searching for something new.\n'''\n", "func_signal": "def on_actionSearchForANewWord_triggered(self):\n", "code": "self.ui.searchField.search_field.setFocus()\nself.ui.searchField.search_field.selectAll()", "path": "src\\manabidict\\ui\\dictionary.py", "repo_name": "aehlke/manabi-dict", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 2358}
{"docstring": "'''Same as `reload_books_list` except for the menu bar Search item.\n'''\n", "func_signal": "def reload_search_menu(self):\n", "code": "sm = self.ui.menuSearch\n\nsm.clear()\n\nnumber = 0\ndef get_shortcut(number):\n    if number <= 9:\n        return [QKeySequence(Qt.CTRL + getattr(Qt, 'Key_' + str(number)))]\n\n# add 'All' options\naction_all = sm.addAction('All Dictionaries')\naction_all.triggered.connect(partial(self.select_book, all_books=True))\nshortcut = get_shortcut(number)\nif shortcut:\n    action_all.setShortcuts(shortcut)\n    number += 1\nsm.addSeparator()\n\n\n# add categories\nfor category in self.book_manager.categories:\n    action = sm.addAction(category.label)\n    action.triggered.connect(partial(self.select_book, category=category))\n    shortcut = get_shortcut(number)\n    if shortcut:\n        action.setShortcuts(shortcut)\n        number += 1\nsm.addSeparator()\n\n# add books\nfor book in self.book_manager.books.values():\n    action = sm.addAction(book.name)\n    action.triggered.connect(partial(self.select_book, book=book))\n    shortcut = get_shortcut(number)\n    if shortcut:\n        action.setShortcuts(shortcut)\n        number += 1\nsm.addSeparator()\n\n# prev / next\naction = sm.addAction('Select Next Dictionary')\naction.triggered.connect(self.select_next_book)\naction.setShortcuts([QKeySequence(Qt.CTRL + Qt.Key_BraceRight)])\naction = sm.addAction('Select Previous Dictionary')\naction.triggered.connect(self.select_previous_book)\naction.setShortcuts([QKeySequence(Qt.CTRL + Qt.Key_BraceLeft)])", "path": "src\\manabidict\\ui\\dictionary.py", "repo_name": "aehlke/manabi-dict", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 2358}
{"docstring": "'''Fills the book combobox with available books.\nCall this after updating installed book preferences, and on first launch.\n\nAlso adds the categories of available books, for searching across multiple books.\n\nThis will duplicate the entries into the menu bar as well, under the Search item.\n'''\n", "func_signal": "def reload_books_list(self):\n", "code": "sb = self.ui.selectBook\n\nselected_data = sb.itemData(sb.currentIndex())\nif selected_data.type() == QVariant.String:\n    selected_data = selected_data.toString()\nelse:\n    selected_data = selected_data.toPyObject()\n#current_book_id = unicode(sb.itemData(sb.currentIndex()).toString())# if sb.currentIndex() else None\n\nsb.clear()\n\n# add 'all' option\nsb.addItem('All', None)\nsb.insertSeparator(sb.count())\n\n# add categories\nfor category in self.book_manager.categories:\n    sb.addItem(category.label, category)\nelse:\n    # add separator\n    sb.insertSeparator(sb.count())\n\n# add books\nfor book_id, book in self.book_manager.books.items():\n    sb.addItem(book.name, book_id)\n\nindex = sb.findData(selected_data)\nif index != -1:\n    sb.setCurrentIndex(index)\nelse:\n    sb.setCurrentIndex(0)\n    self.do_search()", "path": "src\\manabidict\\ui\\dictionary.py", "repo_name": "aehlke/manabi-dict", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 2358}
{"docstring": "'''`direction` is 1 or -1 and indicates if this is for forward or back.\n'''\n", "func_signal": "def _refresh_history_menu(self, menu, history_items, direction):\n", "code": "menu.clear()\nfor index, item in enumerate(history_items, start=1):\n    action = QAction(item.label, menu)\n    func = partial(self._go_history, index * direction)\n    action.triggered.connect(func)\n    menu.addAction(action)", "path": "src\\manabidict\\ui\\dictionary.py", "repo_name": "aehlke/manabi-dict", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 2358}
{"docstring": "'''`url` is a string.\n'''\n", "func_signal": "def load_url(self, url, show_loading_message=True, stage_history=True):\n", "code": "if show_loading_message:\n    self._show_loading_message()\n\nself.ui.entryView.load(QUrl(url))\n\nif stage_history:\n    self.stage_history(url=url)", "path": "src\\manabidict\\ui\\dictionary.py", "repo_name": "aehlke/manabi-dict", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 2358}
{"docstring": "'''Connect text edit and webkit signals to menu action slots, particularly for enable/disable.\n'''\n", "func_signal": "def setupActions(self):\n", "code": "sf = self.ui.searchField.search_field\nev = self.ui.entryView\n\n# edit menu for search field text selection\n#for action in [self.ui.actionCut, self.ui.actionCopy, self.ui.actionDelete]:\n    #sf.selectionChanged.connect(lambda: action.setEnabled(sf.hasSelectedText()))\n\n# set enabled based on focus events\nfor action in [self.ui.actionUndo, self.ui.actionRedo, self.ui.actionPaste, self.ui.actionSelectAll]:\n    action.setEnabled(False)\n    sf.lostFocus.connect(partial(action.setEnabled, False))\n    sf.gotFocus.connect(partial(action.setEnabled, True))\n\ndef enable_search_field_action(action):\n    action.setEnabled(bool(self.ui.searchField.search_field.selectedText()))\n\nfor action in [self.ui.actionCut, self.ui.actionCopy, self.ui.actionDelete]:\n    action.setEnabled(False)\n    sf.selectionChanged.connect(partial(enable_search_field_action, action))\n    sf.lostFocus.connect(partial(action.setEnabled, False))\n    sf.gotFocus.connect(partial(enable_search_field_action, action))\n\n# Select All for entryView\nev.lostFocus.connect(partial(self.ui.actionSelectAll.setEnabled, False))\nev.gotFocus.connect(partial(self.ui.actionSelectAll.setEnabled, True))\n\n# enable Copy for webkit text selection\nev.page().selectionChanged.connect(\n        lambda: self.ui.actionCopy.setEnabled(bool(self.ui.entryView.selectedText())))\n\n# keyboard shortcuts for back/forward\nshortcut = [QKeySequence(Qt.Key_BracketLeft), QKeySequence(Qt.Key_Left)]\nself.ui.actionBack.setShortcuts(shortcut)\nshortcut = [QKeySequence(Qt.Key_BracketRight), QKeySequence(Qt.Key_Right)]\nself.ui.actionForward.setShortcuts(shortcut)", "path": "src\\manabidict\\ui\\dictionary.py", "repo_name": "aehlke/manabi-dict", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 2358}
{"docstring": "'''Enable or disable the history buttons depending on the history state.\n'''\n", "func_signal": "def refresh_history_buttons(self):\n", "code": "back_enabled = bool(self.history.back_items)\nself.ui.actionBack.setEnabled(back_enabled)\nself.ui.history_buttons.left_button.setEnabled(back_enabled)\n\nforward_enabled = bool(self.history.forward_items)\nself.ui.actionForward.setEnabled(forward_enabled)\nself.ui.history_buttons.right_button.setEnabled(forward_enabled)", "path": "src\\manabidict\\ui\\dictionary.py", "repo_name": "aehlke/manabi-dict", "stars": 9, "license": "gpl-3.0", "language": "python", "size": 2358}
{"docstring": "\"\"\"Cast all members of the given list to float.\"\"\"\n", "func_signal": "def floatify(list_):\n", "code": "floatified = [float(e) for e in list_]\nreturn floatified", "path": "utils.py", "repo_name": "dukebody/pyyahoofinance", "stars": 12, "license": "None", "language": "python", "size": 87}
{"docstring": "\"\"\"The performance of the market for a given period is\ncalculated as the mean of the performances of all stocks in\nthe market along the mentioned period.\"\"\"\n", "func_signal": "def getPerformance(self, start_day, end_day):\n", "code": "stocks_performances = [s.getPerformance(start_day, end_day) for s in self.stocks]\nreturn utils.mean(stocks_performances)", "path": "stockmarket.py", "repo_name": "dukebody/pyyahoofinance", "stars": 12, "license": "None", "language": "python", "size": 87}
{"docstring": "\"\"\"Given a list of lists with the same length, return a list where each value is a mean of the given ones, i.e::\n>>> list_ = [[1, 2], [3, 4]]\n>>> point_mean(list_) = [mean([1, 2], mean([3, 4])]\n\"\"\"\n", "func_signal": "def point_mean(list_of_lists):\n", "code": "points = zip(*list_of_lists)\nreturn [mean(p) for p in points]", "path": "utils.py", "repo_name": "dukebody/pyyahoofinance", "stars": 12, "license": "None", "language": "python", "size": 87}
{"docstring": "\"\"\"The daily values for the market are calculated as the mean\nof the daily values of all stocks in the market.\"\"\"\n", "func_signal": "def _getValues(self):\n", "code": "stocks_values = [s.values for s in self.stocks]\nreturn utils.point_mean(stocks_values)", "path": "stockmarket.py", "repo_name": "dukebody/pyyahoofinance", "stars": 12, "license": "None", "language": "python", "size": 87}
{"docstring": "\"\"\"Return the velocity of the provided closes.\"\"\"\n", "func_signal": "def get_diffs(closes):\n", "code": "diffs = [(closes[i] - closes[i-1])/closes[i]*100 for i in range(1, len(closes))]\nreturn diffs", "path": "utils.py", "repo_name": "dukebody/pyyahoofinance", "stars": 12, "license": "None", "language": "python", "size": 87}
{"docstring": "\"\"\"Return the absolute acceleration of the given\ncloses, i.e. deceleration is accounted as acceleration too.\"\"\"\n", "func_signal": "def get_abs_acceleration(closes, reference_closes):\n", "code": "assert len(closes) == len(reference_closes)\ndeviation = get_abs_deviations(closes, reference_closes)\nacceleration = [deviation[i] - deviation[i-1] for i in range(1, len(deviation))]\nreturn acceleration", "path": "utils.py", "repo_name": "dukebody/pyyahoofinance", "stars": 12, "license": "None", "language": "python", "size": 87}
{"docstring": "\"\"\"Return the mean acceleration at each point as a mean among the\ngiven list of closes.\n\"\"\"\n", "func_signal": "def get_mean_point_accelerations(closes_list, reference_closes, absolute=True):\n", "code": "assert len(closes_list[0]) == len(reference_closes)\nif absolute:\n    chosen_get_acceleration = get_abs_acceleration\nelse:\n    chosen_get_acceleration = get_acceleration\naccelerations = [chosen_get_acceleration(closes, reference_closes) for closes in closes_list]\nreturn point_mean(accelerations)", "path": "utils.py", "repo_name": "dukebody/pyyahoofinance", "stars": 12, "license": "None", "language": "python", "size": 87}
{"docstring": "\"\"\"Return a list of the the historical closing values for the\nstocks with the ticker provided, sorted by ascendent date.\n\"\"\"\n\n", "func_signal": "def get_closes(ticker):\n", "code": "f = file('%s/%s.csv' % (DATA_FOLDER, ticker), 'r')\nhistory = f.read()\n\nmeasures = history.split('\\n')\nmeasures = measures[1:-1] # the last row is empty and the first\n                          # one contains the labels\n\ncloses = [float(measure.split(',')[CLOSE_COLUMN]) for measure in measures]\n\ncloses.reverse()\nreturn closes", "path": "utils.py", "repo_name": "dukebody/pyyahoofinance", "stars": 12, "license": "None", "language": "python", "size": 87}
{"docstring": "\"\"\"Calculate the product of all the elements in the list.\"\"\"\n", "func_signal": "def contract(list_):\n", "code": "prod = 1\nfor element in list_:\n    prod *= element\nreturn prod", "path": "utils.py", "repo_name": "dukebody/pyyahoofinance", "stars": 12, "license": "None", "language": "python", "size": 87}
{"docstring": "\"\"\"Get the Standard & Poor stock tickers from disk.\"\"\"\n\n", "func_signal": "def get_tickers():\n", "code": "f = file('%s/tickers.txt' % DATA_FOLDER, 'r')\ndata = f.read()\nf.close()\ntickers = []\nstocks = data.split('\\r\\n')\nfor stock in stocks:\n    try:\n        ticker = stock.split(',')[TICKER_COLUMN]\n        ticker = ticker.replace('\"', '') # remove surrounding quotes\n        if ticker: # not empty ticker\n            tickers.append(ticker)\n    except IndexError: # empty row\n        pass\nreturn tickers", "path": "utils.py", "repo_name": "dukebody/pyyahoofinance", "stars": 12, "license": "None", "language": "python", "size": 87}
{"docstring": "\"\"\"Calculate the average performance along a given period as\nthe product of the daily returns for this period.\"\"\"\n", "func_signal": "def getPerformance(self, start_day, end_day):\n", "code": "selected_returns = self.returns[start_day:end_day]\nperformances = [r/100+1 for r in selected_returns]\nreturn (utils.contract(performances)-1)*100", "path": "stockmarket.py", "repo_name": "dukebody/pyyahoofinance", "stars": 12, "license": "None", "language": "python", "size": 87}
{"docstring": "\"\"\"The offensive benchmark of the market for a given period is\ncalculated as the mean of the market returns, taking into\naccount only the days where the market was raising.\"\"\"\n", "func_signal": "def getOffensiveBenchmark(self, start_day, end_day):\n", "code": "market_returns = self.returns[start_day:end_day]\noffensive_returns = [r for r in market_returns if r>=0]\noffensive_benchmark = utils.mean(offensive_returns)\nreturn offensive_benchmark", "path": "stockmarket.py", "repo_name": "dukebody/pyyahoofinance", "stars": 12, "license": "None", "language": "python", "size": 87}
{"docstring": "\"\"\"The defensive benchmark of the market for a given period is\ncalculated as the mean of the market returns, taking into\naccount only the days where the market was going down.\"\"\"\n", "func_signal": "def getDefensiveBenchmark(self, start_day, end_day):\n", "code": "market_returns = self.returns[start_day:end_day]\ndefensive_returns = [r for r in market_returns if r<=0]\ndefensive_benchmark = utils.mean(defensive_returns)\nreturn defensive_benchmark", "path": "stockmarket.py", "repo_name": "dukebody/pyyahoofinance", "stars": 12, "license": "None", "language": "python", "size": 87}
{"docstring": "\"\"\"Return the acceleration of the given closes.\"\"\"\n", "func_signal": "def get_acceleration(closes, reference_closes):\n", "code": "assert len(closes) == len(reference_closes)\ndeviation = get_deviations(closes, reference_closes)\nacceleration = [deviation[i] - deviation[i-1] for i in range(1, len(deviation))]\nreturn acceleration", "path": "utils.py", "repo_name": "dukebody/pyyahoofinance", "stars": 12, "license": "None", "language": "python", "size": 87}
{"docstring": "\"\"\"Cast all members of the given list to string.\"\"\"\n", "func_signal": "def stringify(list_):\n", "code": "stringified = [str(e) for e in list_]\nreturn stringified", "path": "utils.py", "repo_name": "dukebody/pyyahoofinance", "stars": 12, "license": "None", "language": "python", "size": 87}
{"docstring": "\"\"\"Dump the list of s&p tickers to disk.\"\"\"\n\n", "func_signal": "def download_sap_tickers():\n", "code": "data = []\nfor n in range(0, 500, 50):\n    url = urllib2.urlopen(\"http://download.finance.yahoo.com/d/quotes.csv?s=@%5EGSPC&f=sl1d1t1c1ohgv&e=.csv&h=PAGE\".replace('PAGE', str(n)))\n    data.append(url.read())\n\nf = file('%s/tickers.txt' % DATA_FOLDER, 'w')\nf.write(''.join(data))\nf.close()", "path": "utils.py", "repo_name": "dukebody/pyyahoofinance", "stars": 12, "license": "None", "language": "python", "size": 87}
{"docstring": "\"\"\"Return the list of dates with values for a certain ticker, in\nascendent order. This is designed to be used in the X axis label\nof graphs.\"\"\"\n\n", "func_signal": "def get_dates(ticker):\n", "code": "f = file('%s/%s.csv' % (DATA_FOLDER, ticker), 'r')\nhistory = f.read()\n\nmeasures = history.split('\\n')\nmeasures = measures[1:-1] # the last row is empty and the first\n                          # one contains the labels\n\ndate_column = 0  # dates are stored in the first column\ndates = [measure.split(',')[date_column] for measure in measures]\n\ndates.reverse()\nreturn dates", "path": "utils.py", "repo_name": "dukebody/pyyahoofinance", "stars": 12, "license": "None", "language": "python", "size": 87}
{"docstring": "\"\"\"The offensive score of a stock for a given period is the\nmean of the stocks returns compared to the offensive\nbenchmark, taking into account only the days when the market\nwas raising.\"\"\"\n", "func_signal": "def getOffensive(self, start_day, end_day):\n", "code": "stock_returns = self.returns[start_day:end_day]\nmarket_returns = self.market.returns[start_day:end_day]\noffensive_benchmark = self.market.getOffensiveBenchmark(start_day, end_day)\nstock_offensive_overperforms = [stock_returns[i]/offensive_benchmark*100 for i in range(0, len(stock_returns)) if market_returns[i] >= 0]\nreturn utils.mean(stock_offensive_overperforms)", "path": "stockmarket.py", "repo_name": "dukebody/pyyahoofinance", "stars": 12, "license": "None", "language": "python", "size": 87}
{"docstring": "\"\"\"The defensive score of a stock for a given period is the\nmean of the stocks returns compared to the defensive\nbenchmark, taking into account only the days when the market\nwas going down.\"\"\"\n", "func_signal": "def getDefensive(self, start_day, end_day):\n", "code": "stock_returns = self.returns[start_day:end_day]\nmarket_returns = self.market.returns[start_day:end_day]\ndefensive_benchmark = self.market.getDefensiveBenchmark(start_day, end_day)\nstock_defensive_overperforms = [stock_returns[i]/defensive_benchmark*100 for i in range(0, len(stock_returns)) if market_returns[i] <= 0]\nreturn utils.mean(stock_defensive_overperforms)", "path": "stockmarket.py", "repo_name": "dukebody/pyyahoofinance", "stars": 12, "license": "None", "language": "python", "size": 87}
{"docstring": "\"\"\"Return the deviations of the given diffs from the given reference.\"\"\"\n", "func_signal": "def get_deviations(closes, reference_closes):\n", "code": "assert len(closes) == len(reference_closes)\ndiffs = get_diffs(closes)\nreference_diffs = get_diffs(reference_closes)\ndeviation = [diffs[i] - reference_diffs[i] for i in range(0, len(diffs))]\nreturn deviation", "path": "utils.py", "repo_name": "dukebody/pyyahoofinance", "stars": 12, "license": "None", "language": "python", "size": 87}
{"docstring": "\"\"\"Moves the cursor to the current debugged line.\"\"\"\n\n", "func_signal": "def set_cursor_to_current_line(self):\n", "code": "self.open_file(self.current_filename)\nself.set_cursor_position(self.current_line, 0)", "path": "VimPdb.py", "repo_name": "vim-scripts/VimPdb", "stars": 11, "license": "None", "language": "python", "size": 101}
{"docstring": "\"\"\"Does step over (doesn't enter any functions in between).\"\"\"\n", "func_signal": "def do_step_over(self):\n", "code": "if (self.current_frame is None):\n\tself.print_message(self.MESSAGE_NOT_IN_DEBUG_MODE)\n\treturn\n\nself.set_next(self.current_frame)\nself.pause_debug = False", "path": "VimPdb.py", "repo_name": "vim-scripts/VimPdb", "stars": 11, "license": "None", "language": "python", "size": 101}
{"docstring": "\"\"\"Tests whether or not the current frame is of the exit frame.\"\"\"\n\n", "func_signal": "def is_exit_frame(self, frame):\n", "code": "if (self.canonic(frame.f_code.co_filename) == '<string>'):\n\treturn True\nelse:\n\treturn False", "path": "VimPdb.py", "repo_name": "vim-scripts/VimPdb", "stars": 11, "license": "None", "language": "python", "size": 101}
{"docstring": "\"\"\"Highlights the active breakpoints in the given file.\"\"\"\n", "func_signal": "def highlight_breakpoints(self, filename, regular_breakpoints, conditional_breakpoints, temporary_breakpoints):\n", "code": "self.clear_breakpoints_highlighting()\n\nself._set_lines_highlighting(regular_breakpoints, self.BREAKPOINT_GROUP)\nself._set_lines_highlighting(conditional_breakpoints, self.CONDITIONAL_BREAKPOINT_GROUP)\nself._set_lines_highlighting(temporary_breakpoints, self.TEMPORARY_BREAKPOINT_GROUP)", "path": "VimPdb.py", "repo_name": "vim-scripts/VimPdb", "stars": 11, "license": "None", "language": "python", "size": 101}
{"docstring": "\"\"\"Sets\\Adds breakpoints from a list of breakpoints.\"\"\"\n\n", "func_signal": "def set_breakpoints(self, breakpoints):\n", "code": "for breakpoint in breakpoints:\n\tcondition = None\n\ttemporary = False\n\n\tif (breakpoint['type'] == self.BREAKPOINT_TYPE_CONDITIONAL):\n\t\tcondition = breakpoint['condition']\n\telif (breakpoint['type'] == self.BREAKPOINT_TYPE_TEMPORARY):\n\t\ttemporary = True\n\n\t# Set the breakpoint\n\tself.set_break(breakpoint['filename'], breakpoint['line'], int(temporary), condition)\n\n# Re-highlight all of the breakpoints.\nself.highlight_breakpoints(self.get_active_filename(), *self.get_breakpoints_for_file(self.get_active_filename()))", "path": "VimPdb.py", "repo_name": "vim-scripts/VimPdb", "stars": 11, "license": "None", "language": "python", "size": 101}
{"docstring": "\"\"\"Continues running until returning from the current frame.\"\"\"\n", "func_signal": "def do_continue_until_return(self):\n", "code": "if (self.current_frame is None):\n\tself.print_message(self.MESSAGE_NOT_IN_DEBUG_MODE)\n\treturn\n\nself.set_return(self.current_frame)\nself.pause_debug = False", "path": "VimPdb.py", "repo_name": "vim-scripts/VimPdb", "stars": 11, "license": "None", "language": "python", "size": 101}
{"docstring": "\"\"\"Prints the stack trace.\"\"\"\n\n", "func_signal": "def do_print_stack_trace(self):\n", "code": "output_stack_traces = []\n\n# Prepare the stack trace string.\nfor current_stack_frame in self.stack[2:]: # Skip the first two entries (which aren't really part of the debugged code)\n\t(frame, line_number) = current_stack_frame\n\n\tif (frame is self.current_frame):\n\t\toutput_stack_traces.append(self.current_stack_entry_prefix + self.format_stack_entry(current_stack_frame))\n\telse:\n\t\toutput_stack_traces.append(self.stack_entry_prefix + self.format_stack_entry(current_stack_frame))\n\n\nfinal_stack_trace = self.stack_entries_joiner.join(output_stack_traces)\n\nself.print_message('Stack Trace:\\n' + final_stack_trace)", "path": "VimPdb.py", "repo_name": "vim-scripts/VimPdb", "stars": 11, "license": "None", "language": "python", "size": 101}
{"docstring": "\"\"\"Returns a tuple of (regular_breakpoints, conditional_breakpoints, temporary_breakpoints) for\na given filename.\"\"\"\n\n", "func_signal": "def get_breakpoints_for_file(self, filename):\n", "code": "regular_breakpoints = self.get_file_breaks(filename)[:] # Make a copy so we won't be affected by changes.\nconditional_breakpoints = self.get_conditional_breakpoints(filename)\ntemporary_breakpoints = self.get_temporary_breakpoints(filename)\n\n# Remove any breakpoints which appear in the regular_breakpoints list, and are actually\n# conditional or temporary breakpoints.\nfor breakpoint in regular_breakpoints:\n\tif ((breakpoint in conditional_breakpoints) or (breakpoint in temporary_breakpoints)):\n\t\tregular_breakpoints.remove(breakpoint)\n\nreturn (regular_breakpoints, conditional_breakpoints, temporary_breakpoints)", "path": "VimPdb.py", "repo_name": "vim-scripts/VimPdb", "stars": 11, "license": "None", "language": "python", "size": 101}
{"docstring": "\"\"\"Checks whether or not there active debugging currently enabled.\"\"\"\n#if ((not hasattr(self, 'quitting')) or (self.quitting) or (not self.current_frame)):\n", "func_signal": "def is_debugged(self):\n", "code": "if ((not hasattr(self, 'quitting')) or (self.quitting)):\n\treturn False\nelse:\n\treturn True", "path": "VimPdb.py", "repo_name": "vim-scripts/VimPdb", "stars": 11, "license": "None", "language": "python", "size": 101}
{"docstring": "\"\"\"Starts a debug session for a file. If stop_immediately is set, session is paused on the first line of program.\"\"\"\n", "func_signal": "def start_debugging(self, filename, stop_immediately = True, args = []):\n", "code": "self.print_message(self.MESSAGE_STARTING_DEBUG)\nnew_globals = { '__name__': '__main__' }\nnew_locals = new_globals\n\nself.wait_for_script_start = True # So we won't break before we reach the first line of the script being debugged.\nself.stop_immediately = stop_immediately\nself.main_filename = self.canonic(filename)\n\nself.current_filename = self.main_filename\nself.current_line = 1\n\n\n# Highlight the breakpoints.\nself.highlight_breakpoints(self.main_filename, *self.get_breakpoints_for_file(self.main_filename))\n\n# Replace main directory with running script's directory in front of module search path.\nsys.path[0] = os.path.dirname(self.main_filename)\n\ntry:\n\t# Set command line arguments.\n\tsys.argv = [self.main_filename] + args\n\t# Run the script.\n\tstatement = 'execfile(r\"%s\")' % (self.main_filename)\n\tself.run(statement, globals = new_globals, locals = new_locals)\n\n\t# Program ended.\n\tself.print_message(self.MESSAGE_PROGRAM_ENDED)\n\tself.clear_current_line_highlighting()\n\tself.clear_breakpoints_highlighting()\nexcept SystemExit:\n\tself.print_message(self.MESSAGE_PROGRAM_ENDED_VIA_SYS_EXIT % (sys.exc_info()[1]))\n\tself.clear_current_line_highlighting()\n\tself.clear_breakpoints_highlighting()\nexcept:\n\tself.print_message(self.MESSAGE_PROGRAM_ENDED_UNCAUGHT_EXCEPTION)\n\traise\n\tself.clear_current_line_highlighting()\n\tself.clear_breakpoints_highlighting()", "path": "VimPdb.py", "repo_name": "vim-scripts/VimPdb", "stars": 11, "license": "None", "language": "python", "size": 101}
{"docstring": "\"\"\"Loads breakpoints from a file.\"\"\"\n\n", "func_signal": "def load_breakpoints_from_file(self, filename):\n", "code": "if (not os.path.exists(filename)):\n\tself.print_message('Error: File \"%s\" does not exist!' % (filename))\n\treturn\n\nnew_breakpoints = []\n\n# First, clear all breakpoints.\n#self.do_clear_all_breakpoints()\n\nbreakpoints_file = open(filename, 'rb')\n\n# Load the breakpoints from the given file.\nindex = 0\nfor line in breakpoints_file.xreadlines():\n\tline = line.strip()\n\tindex += 1\n\n\tif (len(line) == 0):\n\t\tcontinue\n\n\tbreakpoint_properties = line.split('\\t')\n\n\tif ((len(breakpoint_properties) < 3) or (len(breakpoint_properties) > 4)):\n\t\tself.print_message('Error: Invalid line #%d at file \"%s\"' % (index, filename))\n\t\treturn\n\n\t(breakpoint_filename, breakpoint_line, breakpoint_type) = breakpoint_properties[:3]\n\tbreakpoint_type = breakpoint_type.lower()\n\ttry:\n\t\tbreakpoint_line = int(breakpoint_line)\n\texcept ValueError:\n\t\tself.print_message('Error: Invalid breakpoint line number in line #%d at file \"%s\"' % (index, filename))\n\t\treturn\n\n\tif (breakpoint_type not in self.BREAKPOINT_TYPES):\n\t\tself.print_message('Error: Invalid breakpoint type in line #%d at file \"%s\"' % (index, filename))\n\t\treturn\n\n\tif ((breakpoint_type == self.BREAKPOINT_TYPE_CONDITIONAL) and (len(breakpoint_properties) != 4)):\n\t\tself.print_message('Error: Missing/invalid breakpoint condition in line #%d at file \"%s\"' % (index, filename))\n\t\treturn\n\n\tcondition = None\n\ttemporary = False\n\n\tif (breakpoint_type == self.BREAKPOINT_TYPE_CONDITIONAL):\n\t\tcondition = breakpoint_properties[3]\n\telif (breakpoint_type == self.BREAKPOINT_TYPE_TEMPORARY):\n\t\ttemporary = True\n\n\tnew_breakpoint = {}\n\tnew_breakpoint['filename'] = breakpoint_filename\n\tnew_breakpoint['line'] = breakpoint_line\n\tnew_breakpoint['type'] = breakpoint_type\n\tnew_breakpoint['condition'] = condition\n\tnew_breakpoint['temporary'] = temporary\n\n\tnew_breakpoints.append(new_breakpoint)\n\nbreakpoints_file.close()\n\n# Set the loaded breakpoints.\nself.set_breakpoints(new_breakpoints)", "path": "VimPdb.py", "repo_name": "vim-scripts/VimPdb", "stars": 11, "license": "None", "language": "python", "size": 101}
{"docstring": "\"\"\"Saves all active breakpoints to a file.\"\"\"\n\n", "func_signal": "def save_breakpoints_to_file(self, filename):\n", "code": "breakpoints_file = open(filename, 'wb')\n\nbreakpoints = self.get_breakpoints()\n\nfor breakpoint in breakpoints:\n\tline = '%s\\t%s\\t%s' % (breakpoint['filename'], breakpoint['line'], breakpoint['type'])\n\tif (breakpoint['type'] == self.BREAKPOINT_TYPE_CONDITIONAL):\n\t\tline += '\\t' + breakpoint['condition']\n\n\tbreakpoints_file.write(line + '\\n')\n\nbreakpoints_file.close()", "path": "VimPdb.py", "repo_name": "vim-scripts/VimPdb", "stars": 11, "license": "None", "language": "python", "size": 101}
{"docstring": "\"\"\"Prints the condition of a breakpoint at the specified line number.\"\"\"\n", "func_signal": "def do_print_breakpoint_condition(self, filename, line_number):\n", "code": "if (not self.is_debugged()):\n\tself.print_message(self.MESSAGE_NOT_IN_DEBUG_MODE)\n\treturn\n\n# First, prepare a list of all available breakpoints for this file.\nconditional_breakpoints = self.get_conditional_breakpoints(filename)\n\nif (line_number not in conditional_breakpoints):\n\tself.print_message(self.MESSAGE_NO_CONDITIONAL_BREAKPOINT)\n\treturn\n\nbreakpoint_instances = self.get_breaks(filename, line_number)\n\nfor breakpoint in breakpoint_instances:\n\tif (breakpoint.cond):\n\t\tself.print_message(self.MESSAGE_BREAKPOINT_CONDITION % (breakpoint.cond))\n\t\treturn", "path": "VimPdb.py", "repo_name": "vim-scripts/VimPdb", "stars": 11, "license": "None", "language": "python", "size": 101}
{"docstring": "\"\"\"Loops as long as self.pause_debug is True.\"\"\"\n\n# Save the current frame, etc.\n", "func_signal": "def wait_in_debug(self, frame, traceback = None):\n", "code": "(self.stack, self.current_stack_index) = self.get_stack(frame, traceback)\nself.current_frame = self.stack[self.current_stack_index][0]\n\nself.goto_current_line(frame)\n\nwhile ((self.pause_debug) and (not self.quitting)):\n\ttime.sleep(self.PAUSE_DEBUG_WAIT_TIME)\n\n\t# Run any queued methods.\n\tself.run_queued_methods()\n\nself.pause_debug = True", "path": "VimPdb.py", "repo_name": "vim-scripts/VimPdb", "stars": 11, "license": "None", "language": "python", "size": 101}
{"docstring": "\"\"\"Returns a list of line numbers with temporary breakpoints for a given filename.\"\"\"\n\n", "func_signal": "def get_temporary_breakpoints(self, filename):\n", "code": "temporary_breakpoints = []\n\n# First, get the line numbers which have breakpoints set in them.\nfile_breaks = self.get_file_breaks(filename)\n\nfor line_number in file_breaks:\n\tbreakpoint_instances = self.get_breaks(filename, line_number)\n\n\tfor breakpoint in breakpoint_instances:\n\t\tif (breakpoint.temporary):\n\t\t\t# Found a temporary breakpoint - add it to the list.\n\t\t\ttemporary_breakpoints.append(line_number)\n\nreturn temporary_breakpoints", "path": "VimPdb.py", "repo_name": "vim-scripts/VimPdb", "stars": 11, "license": "None", "language": "python", "size": 101}
{"docstring": "\"\"\"Moves up one level in the stack frame.\"\"\"\n", "func_signal": "def do_move_up_in_stack_frame(self):\n", "code": "if (not self.is_debugged()):\n\tself.print_message(self.MESSAGE_NOT_IN_DEBUG_MODE)\n\treturn\n\nif (self.current_stack_index <= 2):\n\tself.print_message(self.MESSAGE_ALREADY_AT_OLDEST_FRAME)\n\treturn\n\nself.current_stack_index -= 1\nself.current_frame = self.stack[self.current_stack_index][0]\n\nself.goto_current_line(self.current_frame)", "path": "VimPdb.py", "repo_name": "vim-scripts/VimPdb", "stars": 11, "license": "None", "language": "python", "size": 101}
{"docstring": "\"\"\"Returns a list of line numbers with conditional breakpoints for a given filename.\"\"\"\n\n", "func_signal": "def get_conditional_breakpoints(self, filename):\n", "code": "conditional_breakpoints = []\n\n# First, get the line numbers which have breakpoints set in them.\nfile_breaks = self.get_file_breaks(filename)\n\nfor line_number in file_breaks:\n\tbreakpoint_instances = self.get_breaks(filename, line_number)\n\n\tfor breakpoint in breakpoint_instances:\n\t\tif (breakpoint.cond):\n\t\t\t# Found a conditional breakpoint - add it to the list.\n\t\t\tconditional_breakpoints.append(line_number)\n\nreturn conditional_breakpoints", "path": "VimPdb.py", "repo_name": "vim-scripts/VimPdb", "stars": 11, "license": "None", "language": "python", "size": 101}
{"docstring": "\"\"\"Stops the debugging session.\"\"\"\n", "func_signal": "def stop_debugging(self):\n", "code": "if (not self.is_debugged()):\n\tself.print_message(self.MESSAGE_NOT_IN_DEBUG_MODE)\n\treturn\n\nself.quitting = True", "path": "VimPdb.py", "repo_name": "vim-scripts/VimPdb", "stars": 11, "license": "None", "language": "python", "size": 101}
{"docstring": "\"\"\"Moves down one level in the stack frame.\"\"\"\n", "func_signal": "def do_move_down_in_stack_frame(self):\n", "code": "if (not self.is_debugged()):\n\tself.print_message(self.MESSAGE_NOT_IN_DEBUG_MODE)\n\treturn\n\nif (self.current_stack_index + 1== len(self.stack)):\n\tself.print_message(self.MESSAGE_ALREADY_AT_NEWEST_FRAME)\n\treturn\n\nself.current_stack_index += 1\nself.current_frame = self.stack[self.current_stack_index][0]\n\nself.goto_current_line(self.current_frame)", "path": "VimPdb.py", "repo_name": "vim-scripts/VimPdb", "stars": 11, "license": "None", "language": "python", "size": 101}
{"docstring": "\"\"\"Highlights current line for a given filename.\"\"\"\n\n", "func_signal": "def highlight_current_line_for_file(self, filename):\n", "code": "canonic_filename = self.canonic(filename)\nif (self.current_filename != canonic_filename):\n\t# The given filename is not the currently debugged file.\n\treturn\n\nself.highlight_current_line(canonic_filename, self.current_line)", "path": "VimPdb.py", "repo_name": "vim-scripts/VimPdb", "stars": 11, "license": "None", "language": "python", "size": 101}
{"docstring": "# use a different default url for feeds\n", "func_signal": "def __getattr__(cls, name):\n", "code": "if name == 'url':\n    return u'http://feeds/%s' % cls.name\nelse:\n    return type(File).__getattr__(cls, name)", "path": "feedplatform\\test\\__init__.py", "repo_name": "miracle2k/feedplatform", "stars": 10, "license": "bsd-2-clause", "language": "python", "size": 1624}
{"docstring": "\"\"\"Given the current size and the requested new size, will modify\nthe latter so that the proportions of the former are maintained.\n\nMakes sure that the returned size is always smaller (on either axis),\nnever larger then what was originally requested.\n\nThis is similar to code that the ``Image.py:Image.thumbnail()``\nfunction uses (in fact, has been adapted from there), except it also\nsupports enlarging images.\n\n>>> current = (100, 100)\n>>> _ensure_proportions(current, (700, 900))\n(700, 700)\n>>> _ensure_proportions(current, (700, 500))\n(500, 500)\n>>> _ensure_proportions(current, (70, 90))\n(70, 70)\n>>> _ensure_proportions(current, (70, 50))\n(50, 50)\n>>> _ensure_proportions(current, (200, 10))\n(10, 10)\n>>> _ensure_proportions(current, (10, 200))\n(10, 10)\n>>> _ensure_proportions(current, (50, 50))\n(50, 50)\n>>> _ensure_proportions(current, (100, 100))\n(100, 100)\n\"\"\"\n", "func_signal": "def _ensure_proportions(current_size, requested_size):\n", "code": "cx, cy = current_size\nrx, ry = requested_size\ncr = cx / float(cy)        # current ratio\nrr = rx / float(ry)        # requested ratio\nif cr > rr:\n    ry = max(cy * rx / cx, 1)\n    cx = rx\nif cr < rr:\n    rx = max(cx * ry / cy, 1)\n    cy = ry\nreturn rx, ry", "path": "feedplatform\\deps\\thumbnail\\_thumbnail.py", "repo_name": "miracle2k/feedplatform", "stars": 10, "license": "bsd-2-clause", "language": "python", "size": 1624}
{"docstring": "\"\"\"Manually setup the configuration.\n\nThis makes setting up a config dynamically much easier, since\nyou don't have to create a dummy module.\n\nIf a config is already set up, the options you specifiy will\nsimply be applied (i.e. added, or existing values overwritten).\n# TODO: The above is potentially the wrong thing to do; we should\nmaybe create a whole new config based on the defaults, with just\nthe requested changes, discarding the current config.\n\"\"\"\n", "func_signal": "def configure(self, **options):\n", "code": "if not self.configured:\n    self._target = Configuration()\nfor name, value in options.items():\n    setattr(self._target, name, value)", "path": "feedplatform\\conf\\__init__.py", "repo_name": "miracle2k/feedplatform", "stars": 10, "license": "bsd-2-clause", "language": "python", "size": 1624}
{"docstring": "\"\"\"Test the caller's module.\n\nDiffers from doctest's ``testmod``` in that it actually tests\nthe **caller** if not explicit module reference was passed,\nwhereas doctest would just use ``__main__``.\n\"\"\"\n\n# If no explicit module was passed, try to find the caller's\n# module by inspecting the stack.\n# Use the try-finally pattern explained in the docs:\n# http://docs.python.org/lib/inspect-stack.html\n", "func_signal": "def testmod(module=None):\n", "code": "if not module:\n    frame = inspect.stack()[1][0]\n    try:\n        module = sys.modules[frame.f_globals['__name__']]\n    finally:\n        del frame\n\nnamespace = dict([(name, getattr(module, name)) for name in dir(module)])\nreturn _collect_and_test(namespace)", "path": "feedplatform\\test\\__init__.py", "repo_name": "miracle2k/feedplatform", "stars": 10, "license": "bsd-2-clause", "language": "python", "size": 1624}
{"docstring": "# initial values are picked up\n", "func_signal": "def pass1(feed):\n", "code": "assert feed.title == 'org title'\nassert feed.last_updated.day == 15\nassert feed.prism_issn == '0066-6666'", "path": "tests\\lib\\feeds\\test_collect_feed_data.py", "repo_name": "miracle2k/feedplatform", "stars": 10, "license": "bsd-2-clause", "language": "python", "size": 1624}
{"docstring": "\"\"\"Resize the source image to fit the requested thumbnail size,\nbut crop when necessary to keep propertions the same.\n\"\"\"\n", "func_signal": "def crop(image, new_width, new_height):\n", "code": "image_width, image_height = image.size\nthumb_ratio = new_width / float(new_height)\nimage_ratio = image_width / float(image_height)\nif thumb_ratio < image_ratio:  # width needs to shrink\n    top = 0\n    bottom = image_height\n    thumb_width = int(org_width * crop_ratio)\n    left = (image_width - thumb_width) // 2\n    right = left + thumb_width\nelse:                          # height needs to shrink\n    left = 0\n    right = image_width\n    thumb_height = int(image_width * thumb_ratio)\n    top = (new_height - thumb_height) // 2\n    bottom = top + thumb_height\nreturn image.crop((left, top, right, bottom)).\\\n    resize((new_width, new_height), Image.ANTIALIAS)", "path": "feedplatform\\deps\\thumbnail\\_thumbnail.py", "repo_name": "miracle2k/feedplatform", "stars": 10, "license": "bsd-2-clause", "language": "python", "size": 1624}
{"docstring": "\"\"\"Iterator that returns only the feeds from the set of\nspecified ``File`` classes.\n\"\"\"\n", "func_signal": "def feeds(self):\n", "code": "for obj in self._files.itervalues():\n    if issubclass(obj, Feed):\n        yield obj", "path": "feedplatform\\test\\__init__.py", "repo_name": "miracle2k/feedplatform", "stars": 10, "license": "bsd-2-clause", "language": "python", "size": 1624}
{"docstring": "\"\"\"Loop forever, and update feeds.\n\n# TODO: take options from the command line that we pass on to\n``update_feed``, changing the parsing behavior (addins can\nuse the options to adjust their behavior).\n\"\"\"\n# Code below fails in sqlite because we can't update a row\n# while it is still part of queryset, IIRC:\n#feed = db.store.get_next_feed()\n#while feed:\n#    update_feed(feed)\n#    feed = db.store.get_next_feed()\n", "func_signal": "def run(self, *args, **options):\n", "code": "callback = self.callback\ndo_return = lambda: callback and callback(counter)\ncounter = 0\nwhile True:\n    feeds = db.store.find(db.models.Feed)\n    for i in xrange(0, feeds.count()):  # XXX: only do this in sqlite\n        feed = feeds[i]\n        counter += 1\n        parse.update_feed(feed)\n        if do_return() or self.stop_requested:\n            return\n    if do_return() or self.stop_requested:\n        return\n    if self.once:\n        return", "path": "feedplatform\\lib\\addins\\doers\\daemons.py", "repo_name": "miracle2k/feedplatform", "stars": 10, "license": "bsd-2-clause", "language": "python", "size": 1624}
{"docstring": "\"\"\"Handles functionality common to all thumbnail functions.\n\nProvides the keyword arguments ``save_to`` and ``force``.\n\"\"\"\n", "func_signal": "def _common(f):\n", "code": "def wrapped(image, new_width, new_height, *args, **kwargs):\n    save_to = kwargs.pop('save_to', None)\n    force = kwargs.pop('force', False)\n\n    # TODO: Instead of passing the image object to the save_to()\n    # call, we could simply pass the source filename. This would\n    # allow us to move this code further below so that we only\n    # open the image file once the timestamp comparison determined\n    # that we actually have to.\n    if isinstance(image, basestring):\n        source_filename = image\n        image = Image.open(image)\n    else:\n        source_filename = None\n        force = True  # no filename => detection disabled\n\n    thumb_filename = None\n    if save_to:\n        thumb_filename = save_to(image, new_width, new_height) \\\n            if callable(save_to) \\\n            else save_to\n\n    if save_to and not force:\n        if path.exists(thumb_filename):\n            if path.getmtime(source_filename) <= path.getmtime(thumb_filename):\n                return image\n\n    result = f(image, new_width, new_height, *args, **kwargs)\n\n    if result and save_to:\n        result.save(thumb_filename, image.format)\n    return result\nreturn wrapped", "path": "feedplatform\\deps\\thumbnail\\_thumbnail.py", "repo_name": "miracle2k/feedplatform", "stars": 10, "license": "bsd-2-clause", "language": "python", "size": 1624}
{"docstring": "\"\"\"Resize the source image while keeping propertions to fit either\nthe requested width or height, and then extend the canvas to fit\nthe requested thumbnail size.\n\nIf ``threshold`` is given, and the difference in proportion between\nthe source image and the requested thumbnail is smaller than this\nvalue, then no attempt will be made to keep propertions. This\nbasically is a mechanism to trade ugly white borders against minor\ncontortions.\n\nPart of this code was adapted from ``Image.py:Image.thumbnail()``,\nbut now supports enlarging of images, too.\n\"\"\"\n\n# enable use of default value\n", "func_signal": "def extend(image, new_width, new_height, threshold=None):\n", "code": "if threshold == True:\n    threshold = 0.1\n\n# unless below the threshold, adjust the target size so that it\n# matches the proportions of the source image.\ntarget_size = (new_width, new_height)\nif not threshold or \\\n   not abs(target_size[0] / float(target_size[1]) -\n           image.size[0] / float(image.size[1])) < threshold:\n    target_size = _ensure_proportions(image.size, (new_width, new_height))\n\n# try to load a fitting version of the image, then resize\nimage.draft(None, target_size)\nimage.load()\ntry:\n    thumb_img = image.resize(target_size, Image.ANTIALIAS)\nexcept ValueError:\n    thumb_img = image.resize(target_size, Image.NEAREST)  # fallback\n\n# if we already have the right size (the unmodified one that was\n# originally requested), we can simply return it.\nif thumb_img.size == (new_width, new_height):\n    return thumb_img\nelse:\n    # otherwise, we make it the right size by adding a border\n    thumb_width, thumb_height = thumb_img.size\n    result = Image.new(\"RGB\", (new_width, new_height), \"white\")\n    result.paste(thumb_img,\n        ((new_width-thumb_width)//2, (new_height-thumb_height)//2))\n    return result", "path": "feedplatform\\deps\\thumbnail\\_thumbnail.py", "repo_name": "miracle2k/feedplatform", "stars": 10, "license": "bsd-2-clause", "language": "python", "size": 1624}
{"docstring": "\"\"\"Reset and reinitialize the database for this test.\n\nThis also creates the feeds as rows in the database, and\nassigns the a **database feed object** to each **test feed\ndefinition/class**, i.e.\n\n    feed.dbobj for feed in self.feeds\n\"\"\"\n\n# Drop all existing tables; normally, since we are using a memory\n# sqlite db, a ``db.reconfigure()`` would be enough; we want to\n# optionally support other db setups as well, though.\n", "func_signal": "def _initdb(self):\n", "code": "result = db.store.execute(\"\"\"SELECT name FROM sqlite_master\n                          WHERE type='table' ORDER BY name\"\"\")\nfor row in result.get_all():\n    db.store.execute('DROP TABLE \"%s\"' % row[0])\n\n# recreate tables - since storm can't do schema creation, we\n# have to implement this in a very basic version ourselfs.\nfor model_name, model in db.models.iteritems():\n    field_sql = []\n    # TODO: all this is untested with model inheritance\n    for field in model._storm_columns:\n        field_name = field._detect_attr_name(model)  # field._name is None?\n        modifers = field._primary and ' PRIMARY KEY' or ''\n        try:\n            ctype = {stormvars.IntVariable: 'INTEGER',\n                     stormvars.UnicodeVariable: 'VARCHAR',\n                     stormvars.DateTimeVariable: 'TIMESTAMP',\n                        }[field._variable_class]\n        except KeyError:\n            raise TypeError(('Cannot build %s table, unknow field '\n                'type %s of %s. You probably want to extend the '\n                'test framework''s schema builder to support this '\n                'type.') % (model_name, field, field_name))\n        field_sql.append(\"%s %s%s\" % (field_name, ctype, modifers))\n\n    create_stmt = 'CREATE TABLE %s (%s)' % (\n        model_name.lower(), \", \".join(field_sql))\n    db.store.execute(create_stmt)\n\n# create feed rows\nfor feed in self.feeds:\n    dbobj = db.models.Feed()\n    dbobj.url = feed.url\n    db.store.add(dbobj)\n    feed.dbobj = dbobj\ndb.store.flush()\ndb.store.commit()", "path": "feedplatform\\test\\__init__.py", "repo_name": "miracle2k/feedplatform", "stars": 10, "license": "bsd-2-clause", "language": "python", "size": 1624}
{"docstring": "# assemble content\n", "func_signal": "def on_need_guid(self, feed, item_dict):\n", "code": "content = u\"\"\nfor field in self.fields:\n    value = item_dict.get(field)\n    if value:\n        content += unicode(value)\n\n# return has hash\nif content or self.allow_empty:\n    hash = md5(content.encode('ascii', 'ignore'))\n    result = u'%s%s' % (self.prefix or '', hash.hexdigest())\n    return result\n\nreturn None", "path": "feedplatform\\lib\\addins\\items\\guid.py", "repo_name": "miracle2k/feedplatform", "stars": 10, "license": "bsd-2-clause", "language": "python", "size": 1624}
{"docstring": "# current feed object does no longer exist\n", "func_signal": "def pass1(feed):\n", "code": "assert db.store.find(models.Feed, models.Feed.id == feed.id).count() == 0\n\n# there were two other feeds with the same url that we are\n# redirecting to, they both still exist (we have only deleted\n# ourselfs).\nassert db.store.find(models.Feed, models.Feed.url == u'http://new.org/feeds/rss').count() == 2", "path": "tests\\lib\\feeds\\http\\test_update_redirects.py", "repo_name": "miracle2k/feedplatform", "stars": 10, "license": "bsd-2-clause", "language": "python", "size": 1624}
{"docstring": "# provide default values for attributes a test feed\n# doesn't explicitely specify\n", "func_signal": "def __getattr__(cls, name):\n", "code": "if name == 'name':\n    return cls.__name__\nelif name == 'url':\n    return u'http://files/%s' % cls.name\nelif name == 'status':\n    return 200\nelif name == 'headers':\n    return {}\nelif name == 'content':\n    return \"\"\nelse:\n    raise AttributeError(\"%s\" % name)", "path": "feedplatform\\test\\__init__.py", "repo_name": "miracle2k/feedplatform", "stars": 10, "license": "bsd-2-clause", "language": "python", "size": 1624}
{"docstring": "\"\"\"Iterate over all files. Analogous to ``feeds``.\n\"\"\"\n", "func_signal": "def files(self):\n", "code": "for obj in self._files.itervalues():\n    yield obj", "path": "feedplatform\\test\\__init__.py", "repo_name": "miracle2k/feedplatform", "stars": 10, "license": "bsd-2-clause", "language": "python", "size": 1624}
{"docstring": "# changed values are picked up\n", "func_signal": "def pass2(feed):\n", "code": "assert feed.title == 'changed title'\nassert feed.last_updated.day == 17\nassert feed.prism_issn == '0099-9999'", "path": "tests\\lib\\feeds\\test_collect_feed_data.py", "repo_name": "miracle2k/feedplatform", "stars": 10, "license": "bsd-2-clause", "language": "python", "size": 1624}
{"docstring": "# find all the files/feeds defined in the module\n", "func_signal": "def _collect_and_test(ident_dict):\n", "code": "files = {}\nfor name, obj in ident_dict.iteritems():\n    if isinstance(obj, type):\n       if issubclass(obj, File):\n           files[obj.name] = obj\n\n# a testcase usually defines which addins it uses\naddins = ident_dict.get('ADDINS', [])\n\nreturn testcustom(files, addins)", "path": "feedplatform\\test\\__init__.py", "repo_name": "miracle2k/feedplatform", "stars": 10, "license": "bsd-2-clause", "language": "python", "size": 1624}
{"docstring": "\"\"\"Determines if a 304 Not Modified response code should be sent,\nby simply comparing the resource and request headers (\n``res_headers`` and ``req_headers``, respectively).\n\nReturns ``True`` if a 304 response should be sent.\n\nSupports both the etag and last-modified mechanisms. The latter\none is based on a simple string comparison, rather than a true\ndate comparison.\n\nOriginally based on code from:\n    http://midtoad.homelinux.org/FrogComplete/snakeserver/server.py\n\n\"\"\"\n", "func_signal": "def check_304(self, res_headers, req_headers):\n", "code": "etag = res_headers.get('etag')\nlast_modified = res_headers.get('last-modified')\n\nif_modified_since = req_headers.getheader(\"if-modified-since\")\nif_none_match = req_headers.getheader(\"if-none-match\")\nif_match = req_headers.getheader(\"if-match\")\n\n#print etag, last_modified\n#print if_modified_since, if_none_match, if_match\n\nif not (if_modified_since or if_none_match or if_match):\n    # no 304 relevant header in use\n    return False\n\nelse:\n    if if_modified_since:\n        # strip off IE-shit\n        index = if_modified_since.find(';')\n        if index >= 0:\n            if_modified_since = if_modified_since[:index]\n\n        if last_modified != if_modified_since:\n            return False\n\n    if if_none_match and if_none_match != '*':\n        tags = [tag.strip() for tag in if_none_match.split(',')]\n        if etag not in tags:\n            return False\n\n    if if_match:\n        tags = [tag.strip() for tag in if_match.split(',')]\n        if if_match == '*' or etag in tags:\n            return False\n\n    return True", "path": "feedplatform\\test\\__init__.py", "repo_name": "miracle2k/feedplatform", "stars": 10, "license": "bsd-2-clause", "language": "python", "size": 1624}
{"docstring": "\"\"\"Test a custom set of feed and file classes, using the specified\naddins.\n\nInstead of a set of classes, you may also pass a dict of\nname => class pairs. Otherwise the name is deferred from the class.\n\nExample:\n\n    testcustom([Feed1, Feed2, Feed3], addins=[myaddin])\n    testcustom({'Feed1:': Feed1}, addins=[myaddin])\n\"\"\"\n\n# internally, we need the name -> class syntax\n", "func_signal": "def testcustom(files, addins=[], run=True):\n", "code": "if not isinstance(files, dict):\n    files_dict = {}\n    for f in files:\n        files_dict[f.__name__] = f\n    files = files_dict\n\n# By this time the nose testrunner has redirected stdout. Reset\n# the log (it might still point to the real stdout) to make sure\n# that any messages will indeed be captured by nose.\nlog.reset(level=logging.DEBUG)\n\n# run the test case\ntest = FeedEvolutionTest(files, addins)\nif run:\n    test.run()\nreturn test", "path": "feedplatform\\test\\__init__.py", "repo_name": "miracle2k/feedplatform", "stars": 10, "license": "bsd-2-clause", "language": "python", "size": 1624}
{"docstring": "# must pass at least one\n", "func_signal": "def test_instantiation():\n", "code": "assert_raises(Exception, update_redirects)\n\n# only certain values for delete\nassert_raises(Exception, update_redirects, delete=\"invalid\")\n\n# can't pass more than one\nassert_raises(Exception, update_redirects, force=True, ignore=True)", "path": "tests\\lib\\feeds\\http\\test_update_redirects.py", "repo_name": "miracle2k/feedplatform", "stars": 10, "license": "bsd-2-clause", "language": "python", "size": 1624}
{"docstring": "# private!\n", "func_signal": "def _walk(self, name):\n", "code": "result = []\nif name is None or self.name == name:\n    result.append(self)\nfor subcomponent in self.subcomponents:\n    result += subcomponent._walk(name)\nreturn result", "path": "icalendar\\cal.py", "repo_name": "progrium/hd-events", "stars": 8, "license": "None", "language": "python", "size": 681}
{"docstring": "\"\"\"\nReturns properties in this component and subcomponents as:\n[(name, value), ...]\n\"\"\"\n", "func_signal": "def property_items(self):\n", "code": "vText = types_factory['text']\nproperties = [('BEGIN', vText(self.name).ical())]\nproperty_names = self.keys()\nproperty_names.sort()\nfor name in property_names:\n    values = self[name]\n    if type(values) == ListType:\n        # normally one property is one line\n        for value in values:\n            properties.append((name, value))\n    else:\n        properties.append((name, values))\n# recursion is fun!\nfor subcomponent in self.subcomponents:\n    properties += subcomponent.property_items()\nproperties.append(('END', vText(self.name).ical()))\nreturn properties", "path": "icalendar\\cal.py", "repo_name": "progrium/hd-events", "stars": 8, "license": "None", "language": "python", "size": 681}
{"docstring": "\"\"\"\nDecodes a named property or parameter value from an icalendar encoded\nstring to a primitive python type.\n\"\"\"\n", "func_signal": "def from_ical(self, name, value):\n", "code": "type_class = self.for_property(name)\ndecoded = type_class.from_ical(str(value))\nreturn decoded", "path": "icalendar\\prop.py", "repo_name": "progrium/hd-events", "stars": 8, "license": "None", "language": "python", "size": 681}
{"docstring": "\"\"\"\nConverts a list of values into comma seperated string and sets value to\nthat.\n\"\"\"\n", "func_signal": "def set_inline(self, name, values, encode=1):\n", "code": "if encode:\n    values = [self._encode(name, value, 1) for value in values]\njoined = q_join(values).encode(vText.encoding)\nself[name] = types_factory['inline'](joined)", "path": "icalendar\\cal.py", "repo_name": "progrium/hd-events", "stars": 8, "license": "None", "language": "python", "size": 681}
{"docstring": "\"\"\"\nPopulates the component recursively from a string\n\"\"\"\n", "func_signal": "def from_string(st, multiple=False):\n", "code": "stack = [] # a stack of components\ncomps = []\nfor line in Contentlines.from_string(st): # raw parsing\n    if not line:\n        continue\n    name, params, vals = line.parts()\n    uname = name.upper()\n    # check for start of component\n    if uname == 'BEGIN':\n        # try and create one of the components defined in the spec,\n        # otherwise get a general Components for robustness.\n        component_name = vals.upper()\n        component_class = component_factory.get(component_name, Component)\n        component = component_class()\n        if not getattr(component, 'name', ''): # for undefined components\n            component.name = component_name\n        stack.append(component)\n    # check for end of event\n    elif uname == 'END':\n        # we are done adding properties to this component\n        # so pop it from the stack and add it to the new top.\n        component = stack.pop()\n        if not stack: # we are at the end\n            comps.append(component)\n        else:\n            stack[-1].add_component(component)\n    # we are adding properties to the current top of the stack\n    else:\n        factory = types_factory.for_property(name)\n        vals = factory(factory.from_ical(vals))\n        vals.params = params\n        stack[-1].add(name, vals, encode=0)\nif multiple:\n    return comps\nif not len(comps) == 1:\n    raise ValueError('Found multiple components where '\n                     'only one is allowed')\nreturn comps[0]", "path": "icalendar\\cal.py", "repo_name": "progrium/hd-events", "stars": 8, "license": "None", "language": "python", "size": 681}
{"docstring": "\"\"\"convert a datetime into an RFC 822 formatted date\n\nInput date must be in GMT.\n\"\"\"\n# Looks like:\n#   Sat, 07 Sep 2002 00:00:01 GMT\n# Can't use strftime because that's locale dependent\n#\n# Isn't there a standard way to do this for Python?  The\n# rfc822 and email.Utils modules assume a timestamp.  The\n# following is based on the rfc822 module.\n", "func_signal": "def _format_date(dt):\n", "code": "return \"%s, %02d %s %04d %02d:%02d:%02d GMT\" % (\n        [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"][dt.weekday()],\n        dt.day,\n        [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\",\n         \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"][dt.month-1],\n        dt.year, dt.hour, dt.minute, dt.second)", "path": "PyRSS2Gen.py", "repo_name": "progrium/hd-events", "stars": 8, "license": "None", "language": "python", "size": 681}
{"docstring": "\"\"\"\nParses the data format from ical text format.\n\"\"\"\n", "func_signal": "def from_ical(ical):\n", "code": "try:\n    match = DURATION_REGEX.match(ical)\n    sign, weeks, days, hours, minutes, seconds = match.groups()\n    if weeks:\n        value = timedelta(weeks=int(weeks))\n    else:\n        value = timedelta(days=int(days or 0),\n                          hours=int(hours or 0),\n                          minutes=int(minutes or 0),\n                          seconds=int(seconds or 0))\n    if sign == '-':\n        value = -value\n    return value\nexcept:\n    raise ValueError('Invalid iCalendar duration: %s' % ical)", "path": "icalendar\\prop.py", "repo_name": "progrium/hd-events", "stars": 8, "license": "None", "language": "python", "size": 681}
{"docstring": "\"\"\"\nEncodes a named value from a primitive python type to an\nicalendar encoded string.\n\"\"\"\n", "func_signal": "def ical(self, name, value):\n", "code": "type_class = self.for_property(name)\nreturn type_class(value).ical()", "path": "icalendar\\prop.py", "repo_name": "progrium/hd-events", "stars": 8, "license": "None", "language": "python", "size": 681}
{"docstring": "# Ensure that applications serializing pytz instances as pickles\n# have no troubles upgrading to a new pytz release. These pickles\n# where created with pytz2006j\n", "func_signal": "def testOldPickles(self):\n", "code": "east1 = pickle.loads(\n        \"cpytz\\n_p\\np1\\n(S'US/Eastern'\\np2\\nI-18000\\n\"\n        \"I0\\nS'EST'\\np3\\ntRp4\\n.\"\n        )\neast2 = pytz.timezone('US/Eastern')\nself.failUnless(east1 is east2)\n\n# Confirm changes in name munging between 2006j and 2007c cause\n# no problems.\npap1 = pickle.loads(\n        \"cpytz\\n_p\\np1\\n(S'America/Port_minus_au_minus_Prince'\"\n        \"\\np2\\nI-17340\\nI0\\nS'PPMT'\\np3\\ntRp4\\n.\"\n        )\npap2 = pytz.timezone('America/Port-au-Prince')\nself.failUnless(pap1 is pap2)\n\ngmt1 = pickle.loads(\"cpytz\\n_p\\np1\\n(S'Etc/GMT_plus_10'\\np2\\ntRp3\\n.\")\ngmt2 = pytz.timezone('Etc/GMT+10')\nself.failUnless(gmt1 is gmt2)", "path": "pytz\\tests\\test_tzinfo.py", "repo_name": "progrium/hd-events", "stars": 8, "license": "None", "language": "python", "size": 681}
{"docstring": "# integers\n", "func_signal": "def parse_type(key, values):\n", "code": "parser = vRecur.types.get(key, vText)\nreturn [parser.from_ical(v) for v in values.split(',')]", "path": "icalendar\\prop.py", "repo_name": "progrium/hd-events", "stars": 8, "license": "None", "language": "python", "size": 681}
{"docstring": "# SequenceTypes\n", "func_signal": "def ical(self):\n", "code": "result = []\nfor key, vals in self.items():\n    typ = self.types[key]\n    if not type(vals) in SequenceTypes:\n        vals = [vals]\n    vals = ','.join([typ(val).ical() for val in vals])\n    result.append('%s=%s' % (key, vals))\nreturn ';'.join(result)", "path": "icalendar\\prop.py", "repo_name": "progrium/hd-events", "stars": 8, "license": "None", "language": "python", "size": 681}
{"docstring": "# Python's datetime library has a bug, where the hour before\n# a daylight savings transition is one hour out. For example,\n# at the end of US/Eastern daylight savings time, 01:00 EST\n# occurs twice (once at 05:00 UTC and once at 06:00 UTC),\n# whereas the first should actually be 01:00 EDT.\n# Note that this bug is by design - by accepting this ambiguity\n# for one hour one hour per year, an is_dst flag on datetime.time\n# became unnecessary.\n", "func_signal": "def testHourBefore(self):\n", "code": "self._test_all(\n        self.transition_time - timedelta(hours=1), self.after\n        )", "path": "pytz\\tests\\test_tzinfo.py", "repo_name": "progrium/hd-events", "stars": 8, "license": "None", "language": "python", "size": 681}
{"docstring": "\"\"\"\nRecursively traverses component and subcomponents. Returns sequence of\nsame. If name is passed, only components with name will be returned.\n\"\"\"\n", "func_signal": "def walk(self, name=None):\n", "code": "if not name is None:\n    name = name.upper()\nreturn self._walk(name)", "path": "icalendar\\cal.py", "repo_name": "progrium/hd-events", "stars": 8, "license": "None", "language": "python", "size": 681}
{"docstring": "# internal, for conditional convertion of values.\n", "func_signal": "def _encode(self, name, value, cond=1):\n", "code": "if cond:\n    klass = types_factory.for_property(name)\n    return klass(value)\nreturn value", "path": "icalendar\\cal.py", "repo_name": "progrium/hd-events", "stars": 8, "license": "None", "language": "python", "size": 681}
{"docstring": "\"\"\"\nReturns a list of values (split on comma).\n\"\"\"\n", "func_signal": "def get_inline(self, name, decode=1):\n", "code": "vals = [v.strip('\" ').encode(vText.encoding)\n          for v in q_split(self[name])]\nif decode:\n    return [self._decode(name, val) for val in vals]\nreturn vals", "path": "icalendar\\cal.py", "repo_name": "progrium/hd-events", "stars": 8, "license": "None", "language": "python", "size": 681}
{"docstring": "\"\"\"\nMultiple keys where key1.upper() == key2.upper() will be lost.\n\"\"\"\n", "func_signal": "def update(self, indict):\n", "code": "for entry in indict:\n    self[entry] = indict[entry]", "path": "icalendar\\caselessdict.py", "repo_name": "progrium/hd-events", "stars": 8, "license": "None", "language": "python", "size": 681}
{"docstring": "# It would be nice if this worked, but it doesn't.\n", "func_signal": "def no_testCreateLocaltime(self):\n", "code": "tz = pytz.timezone('Europe/Amsterdam')\ndt = datetime(2004, 10, 31, 2, 0, 0, tzinfo=tz)\nself.failUnlessEqual(\n        dt.strftime(fmt),\n        '2004-10-31 02:00:00 CET+0100'\n        )", "path": "pytz\\tests\\test_tzinfo.py", "repo_name": "progrium/hd-events", "stars": 8, "license": "None", "language": "python", "size": 681}
{"docstring": "# utcoffset in Amsterdam was not a whole minute until 1937\n# However, we fudge this by rounding them, as the Python\n# datetime library \n", "func_signal": "def testPartialMinuteOffsets(self):\n", "code": "tz = pytz.timezone('Europe/Amsterdam')\nutc_dt = datetime(1914, 1, 1, 13, 40, 28, tzinfo=UTC) # correct\nutc_dt = utc_dt.replace(second=0) # But we need to fudge it\nloc_dt = utc_dt.astimezone(tz)\nself.failUnlessEqual(\n        loc_dt.strftime('%Y-%m-%d %H:%M:%S %Z%z'),\n        '1914-01-01 14:00:00 AMT+0020'\n        )\n\n# And get back...\nutc_dt = loc_dt.astimezone(UTC)\nself.failUnlessEqual(\n        utc_dt.strftime('%Y-%m-%d %H:%M:%S %Z%z'),\n        '1914-01-01 13:40:00 UTC+0000'\n        )", "path": "pytz\\tests\\test_tzinfo.py", "repo_name": "progrium/hd-events", "stars": 8, "license": "None", "language": "python", "size": 681}
{"docstring": "# Hack the pickle to make it refer to a timezone abbreviation\n# that does not match anything. The unpickler should be able\n# to repair this case\n", "func_signal": "def testDatabaseFixes(self):\n", "code": "tz = pytz.timezone('Australia/Melbourne')\np = pickle.dumps(tz)\ntzname = tz._tzname\nhacked_p = p.replace(tzname, '???')\nself.failIfEqual(p, hacked_p)\nunpickled_tz = pickle.loads(hacked_p)\nself.failUnless(tz is unpickled_tz)\n\n# Simulate a database correction. In this case, the incorrect\n# data will continue to be used.\np = pickle.dumps(tz)\nnew_utcoffset = tz._utcoffset.seconds + 42\nhacked_p = p.replace(str(tz._utcoffset.seconds), str(new_utcoffset))\nself.failIfEqual(p, hacked_p)\nunpickled_tz = pickle.loads(hacked_p)\nself.failUnlessEqual(unpickled_tz._utcoffset.seconds, new_utcoffset)\nself.failUnless(tz is not unpickled_tz)", "path": "pytz\\tests\\test_tzinfo.py", "repo_name": "progrium/hd-events", "stars": 8, "license": "None", "language": "python", "size": 681}
{"docstring": "# Ensuring the correct version of pytz has been loaded\n", "func_signal": "def testVersion(self):\n", "code": "self.failUnlessEqual(EXPECTED_VERSION, pytz.__version__,\n        'Incorrect pytz version loaded. Import path is stuffed '\n        'or this test needs updating. (Wanted %s, got %s)'\n        % (EXPECTED_VERSION, pytz.__version__)\n        )", "path": "pytz\\tests\\test_tzinfo.py", "repo_name": "progrium/hd-events", "stars": 8, "license": "None", "language": "python", "size": 681}
{"docstring": "\"\"\"\nReturn a deepcopy of self as a dictionary.\n\nAll members that are ``Section`` instances are recursively turned to\nordinary dictionaries - by calling their ``dict`` method.\n\n>>> n = a.dict()\n>>> n == a\n1\n>>> n is a\n0\n\"\"\"\n", "func_signal": "def dict(self):\n", "code": "newdict = {}\nfor entry in self:\n    this_entry = self[entry]\n    if isinstance(this_entry, Section):\n        this_entry = this_entry.dict()\n    elif isinstance(this_entry, list):\n        # create a copy rather than a reference\n        this_entry = list(this_entry)\n    elif isinstance(this_entry, tuple):\n        # create a copy rather than a reference\n        this_entry = tuple(this_entry)\n    newdict[entry] = this_entry\nreturn newdict", "path": "external\\configobj.py", "repo_name": "olcai/ais-logger", "stars": 13, "license": "None", "language": "python", "size": 680}
{"docstring": "\"\"\"Write a section marker line\"\"\"\n", "func_signal": "def _write_marker(self, indent_string, depth, entry, comment):\n", "code": "return '%s%s%s%s%s' % (indent_string,\n                       self._a_to_u('[' * depth),\n                       self._quote(self._decode_element(entry), multiline=False),\n                       self._a_to_u(']' * depth),\n                       self._decode_element(comment))", "path": "external\\configobj.py", "repo_name": "olcai/ais-logger", "stars": 13, "license": "None", "language": "python", "size": 680}
{"docstring": "# This function decodes certain types of messages from the\n# receiver and returns the interesting data as a dictionary where\n# each key describes the information of each message part\n\n# Observe that the navigational status is set as an integer\n# according to ITU-R M.1371, and is thus converted for SAAB\n# PAIS messages to these values\n\n# For each value where we have a N/A-state None is returned\n\n# Convert the raw input string to a list of separated values\n", "func_signal": "def telegramparser(inputstring):\n", "code": "telegram = inputstring.split(',')\n\n# Depending on what sentence the list contains, extract the\n# information and create a dictionary with the MMSI number as key\n# and a value which contain another dictionary with the actual\n# data\n\n# If the sentence follows the SAAB TransponderTech standard:\nif telegram[0] == '$PAIS':\n    # Check the checksum\n    if not checksum(inputstring):\n        return\n\n    # Get the source MMSI number\n    mmsi = int(telegram[2],16)\n\n    # Extract the message type number and prefix the number with\n    # an 'S' to indicate SAAB messages\n    message = 'S' + telegram[1]\n\n    # Get current computer time to timestamp messages\n    timestamp = datetime.datetime.now()\n\n    # If the sentence contains 02 - AIS Standard Position:\n    if message == 'S02':\n        # Rate of turn in degrees/minute from -127 to +127 where 128=N/A\n        rateofturn = int(telegram[3], 16)\n        if rateofturn >=0 and rateofturn <128: # Turning right\n            # Convert between ROTais and ROTind\n            rateofturn = int(math.pow((rateofturn/4.733), 2))\n            if rateofturn > 720:\n                rateofturn = 720 # Full\n        elif rateofturn >128 and rateofturn <=255: # Turning left\n            rateofturn = 256 - rateofturn\n            # Convert between ROTais and ROTind\n            rateofturn = -int(math.pow((rateofturn/4.733), 2))\n            if rateofturn < -720:\n                rateofturn = -720 # Full\n        else:\n            rateofturn = None # N/A\n        # Navigation status converted to ITU-R M.1371 standard\n        navstatus = telegram[4]\n        if navstatus == '1': navstatus = 0 # Under Way\n        elif navstatus == '2': navstatus = 2 # Not Under Command\n        elif navstatus == '3': navstatus = 3 # Restricted Manoeuvrability\n        elif navstatus == '4': navstatus = 1 # At Anchor\n        elif navstatus == '5': navstatus = None # (MAYDAY?) sets to N/A\n        else: navstatus = None # N/A\n        # Latitude in decimal degrees (DD)\n        latitude = calclatitude(tobin(int(telegram[5],16),27))\n        # Longitude in decimal degrees (DD)\n        longitude = calclongitude(tobin(int(telegram[6],16),28))\n        # Speed over ground in 1/10 knots\n        sog = decimal.Decimal(int(telegram[7],16)) / 10\n        if sog > decimal.Decimal(\"102.2\"):\n            sog = None # N/A\n        # Course over ground in 1/10 degrees where 0=360\n        cog = decimal.Decimal(int(telegram[8],16)) / 10\n        if cog > 360: # 360 and above means 360=N/A\n            cog = None\n        # Heading in whole degrees between 0-359 and 511=N/A\n        heading = int(telegram[9],16)\n        if heading > 359:\n            heading = None # N/A\n        # Position accuracy where 0=bad and 1=good/DGPS\n        posacc = int(telegram[11])\n        # Return a dictionary with descriptive keys\n        return {'mmsi': mmsi,\n                'rot': rateofturn,\n                'navstatus': navstatus,\n                'latitude': latitude,\n                'longitude': longitude,\n                'sog': sog,\n                'cog': cog,\n                'heading': heading,\n                'posacc': posacc,\n                'time': timestamp,\n                'message': message}\n\n    # If the sentence contains message 04 - Addressed Text Telegram:\n    elif message == 'S04':\n        # Content of message in ASCII (replace any \" with ')\n        content = telegram[4].replace('''\"''',\"'\")\n        # Destination MMSI number\n        to_mmsi = int(telegram[5],16)\n        # Return a dictionary with descriptive keys\n        return {'mmsi': mmsi,\n                'content': content,\n                'to_mmsi': to_mmsi,\n                'time': timestamp,\n                'message': message}\n\n    # If the sentence contains message 06 - Broadcast Text Telegram:\n    elif message == 'S06':\n        # Content of message in ASCII (replace any \" with ')\n        content = str(telegram[4]).replace('''\"''',\"'\")\n        # Return a dictionary with descriptive keys\n        return {'mmsi': mmsi,\n                'content': content,\n                'time': timestamp,\n                'message': message}\n\n    # If the sentence contains message 07 - Addressed Binary Telegram:\n    elif message == 'S07':\n        # Binary data payload\n        payload = []\n        for char in telegram[4]:\n            payload.append(tobin(int(char,16),4))\n        payload = ''.join(payload)\n        # Destination MMSI number\n        to_mmsi = int(telegram[5],16)\n        # Application ID (Designated Area Code, DAC) + (Function\n        # Identification, FI)\n        appid = []\n        for char in telegram[7]:\n            appid.append(tobin(int(char,16),4))\n        appid = ''.join(appid)\n        dac = int(appid[0:10],2)\n        fi = int(appid[10:16],2)\n        # Try to decode message payload\n        decoded = binaryparser(dac,fi,payload)\n        # Return a dictionary with descriptive keys\n        return {'mmsi': mmsi,\n                'to_mmsi': to_mmsi,\n                'dac': dac,\n                'fi': fi,\n                'decoded': decoded,\n                'time': timestamp,\n                'message': message}\n\n    # If the sentence contains message 09 - Broadcast Binary Telegram:\n    elif message == 'S09':\n        # Binary data payload\n        payload = []\n        for char in telegram[4]:\n            payload.append(tobin(int(char,16),4))\n        payload = ''.join(payload)\n        # Application ID (Designated Area Code, DAC) + (Function\n        # Identification, FI)\n        appid = []\n        for char in telegram[6]:\n            appid.append(tobin(int(char,16),4))\n        appid = ''.join(appid)\n        dac = int(appid[0:10],2)\n        fi = int(appid[10:16],2)\n        # Try to decode message payload\n        decoded = binaryparser(dac,fi,payload)\n        # Return a dictionary with descriptive keys\n        return {'mmsi': mmsi,\n                'dac': dac,\n                'fi': fi,\n                'decoded': decoded,\n                'time': timestamp,\n                'message': message}\n\n    # If the sentence contains message 0D - Standard Position,\n    # aviation, or message 11 - SAR Standard Position\n    elif message == 'S0D' or message == 'S11':\n        # Latitude in decimal degrees (DD)\n        latitude = calclatitude(tobin(int(telegram[3],16),27))\n        # Longitude in decimal degrees (DD)\n        longitude = calclongitude(tobin(int(telegram[4],16),28))\n        # Speed over ground in knots\n        sog = int(telegram[5],16)\n        if sog > 1022:\n            sog = None # N/A\n        # Course over ground in 1/10 degrees where 0=360\n        cog = decimal.Decimal(int(telegram[6],16)) / 10\n        if cog > 360: # 360 and above means 360=N/A\n            cog = None\n        # Altitude in meters, 4095=N/A\n        altitude = int(telegram[7],16)\n        if altitude == 4095:\n            altitude = None # N/A\n        # Return a dictionary with descriptive keys\n        return {'mmsi': mmsi,\n                'altitude': altitude,\n                'sog': sog,\n                'latitude': latitude,\n                'longitude': longitude,\n                'cog': cog,\n                'time': timestamp,\n                'message': message}\n\n    # If the sentence contains 0E - Identification Data:\n    elif message == 'S0E':\n        # Name, removes the characters @, ' ' and \"\n        name = telegram[3].strip('''@ ''').replace('''\"''',\"'\")\n        # Callsign, removes the characters @, ' ' and \"\n        callsign = telegram[4].strip('''@ ''').replace('''\"''',\"'\")\n        # IMO number where 00000000=N/A\n        imo = int(telegram[5],16)\n        if imo == 0:\n            imo = None\n        # Return a dictionary with descriptive keys\n        return {'mmsi': mmsi,\n                'name': name,\n                'callsign': callsign,\n                'imo': imo,\n                'time': timestamp,\n                'message': message}\n\n    # If the sentence contains 0F - Vessel Data:\n    elif message == 'S0F':\n        # Ship type, a two-digit code where 00=N/A\n        type = int(telegram[3],16)\n        if type == 0:\n            type = None # N/A\n        # Draught in 1/10 meters, where 0.0 = N/A\n        draught = decimal.Decimal(int(telegram[4],16)) / 10\n        if draught == 0:\n            draught = None\n        # Calculate ship width and length in meters from\n        # antenna position in hex\n        # Convert hex->int->bits\n        ant_binnumber = tobin(int(telegram[5],16),count=30)\n        # Add integers from the two parts to form length\n        length = int(ant_binnumber[12:21],2) + int(ant_binnumber[21:30],2)\n        # Add integers from the two parts to form width\n        width = int(ant_binnumber[0:6],2) + int(ant_binnumber[6:12],2)\n        # Destination, removes the characters @, ' ' and \"\n        destination = telegram[6].strip('''@ ''').replace('''\"''',\"'\")\n        # Received estimated time of arrival in format\n        # month-day-hour-minute: MMDDHHMM where 00000000=N/A\n        eta = telegram[8]\n        if eta == '00000000':\n            eta = None\n        # Return a dictionary with descriptive keys\n        return {'mmsi': mmsi,\n                'type': type,\n                'draught': draught,\n                'length': length,\n                'width': width,\n                'destination': destination,\n                'eta': eta,\n                'time': timestamp,\n                'message': message}\n\n    else:\n        # If we don't decode the message, at least return message type\n        return {'mmsi': mmsi, 'time': timestamp, 'message': message, 'decoded': False}\n\n\n# If the sentence follows the ITU-R M.1371 standard:\nif telegram[0] == '!AIVDM':\n    # Check the checksum\n    if not checksum(inputstring):\n        return\n\n    # Convert the 6-bit string to a binary string\n    bindata = sixtobin(telegram[5])\n\n    # Extract the message type number\n    message = str(int(bindata[0:6],2))\n\n    # Get the source MMSI number\n    mmsi = int(bindata[8:38],2)\n\n    # Get current computer time to timestamp messages\n    timestamp = datetime.datetime.now()\n\n    # If the sentence contains message 1, 2 or 3 - Position Report:\n    if message == '1' or message == '2' or message == '3':\n        # Navigation status according to ITU-R M.1371\n        navstatus = int(bindata[38:42],2)\n        if navstatus == 0: navstatus = 0 # Under Way\n        elif navstatus == 1: navstatus = 1 # At Anchor\n        elif navstatus == 2: navstatus = 2 # Not Under Command\n        elif navstatus == 3: navstatus = 3 # Restricted Manoeuvrability\n        elif navstatus == 4: navstatus = 4 # Constrained by her draught\n        elif navstatus == 5: navstatus = 5 # Moored\n        elif navstatus == 6: navstatus = 6 # Aground\n        elif navstatus == 7: navstatus = 7 # Engaged in Fishing\n        elif navstatus == 8: navstatus = 8 # Under way sailing\n        else: navstatus = None # N/A\n        # Rate of turn in degrees/minute from -127 to +127 where 128=N/A\n        sign_rateofturn = int(bindata[42])\n        rateofturn = int(bindata[43:50],2)\n        if rateofturn > 126:\n            rateofturn = None # N/A\n        elif sign_rateofturn and rateofturn > 1:\n            # Turning left\n            rateofturn = 128 - rateofturn\n            # Convert between ROTais and ROTind\n            rateofturn = -int(math.pow((rateofturn/4.733), 2))\n            if rateofturn < -720:\n                rateofturn = -720 # Full\n        else:\n            # Turning right\n            # Convert between ROTais and ROTind\n            rateofturn = int(math.pow((rateofturn/4.733), 2))\n            if rateofturn > 720:\n                rateofturn = 720 # Full\n        # Speed over ground in 1/10 knots\n        sog = decimal.Decimal(int(bindata[50:60],2)) / 10\n        if sog > decimal.Decimal(\"102.2\"):\n            sog = None # N/A\n        # Position accuracy where 0=bad and 1=good/DGPS\n        posacc = int(bindata[60],2)\n        # Longitude in decimal degrees (DD)\n        longitude = calclongitude(bindata[61:89])\n        # Latitude in decimal degrees (DD)\n        latitude = calclatitude(bindata[89:116])\n        # Course over ground in 1/10 degrees between 0-359\n        cog = decimal.Decimal(int(bindata[116:128],2)) / 10\n        if cog > 360: # 360 and above means 360=N/A\n            cog = None\n        # Heading in whole degrees between 0-359 and 511=N/A\n        heading = int(bindata[128:137],2)\n        if heading > 359:\n            heading = None # N/A\n        # Return a dictionary with descriptive keys\n        return {'mmsi': mmsi,\n                'rot': rateofturn,\n                'navstatus': navstatus,\n                'latitude': latitude,\n                'longitude': longitude,\n                'sog': sog,\n                'cog': cog,\n                'heading': heading,\n                'posacc': posacc,\n                'time': timestamp,\n                'message': message}\n\n    # If the sentence contains message 4 - Base Station Report:\n    elif message == '4':\n        # Bits 38-78 contains current station time in UTC\n        try:\n            station_time = datetime.datetime(int(bindata[38:52],2),\n                                             int(bindata[52:56],2),\n                                             int(bindata[56:61],2),\n                                             int(bindata[61:66],2),\n                                             int(bindata[66:72],2),\n                                             int(bindata[72:78],2))\n        except ValueError:\n            station_time = None # N/A\n        # Position accuracy where 0=bad and 1=good/DGPS\n        posacc = int(bindata[78],2)\n        # Longitude in decimal degrees (DD)\n        longitude = calclongitude(bindata[79:107])\n        # Latitude in decimal degrees (DD)\n        latitude = calclatitude(bindata[107:134])\n        # Return a dictionary with descriptive keys\n        return {'mmsi': mmsi,\n                'station_time': station_time,\n                'posacc': posacc,\n                'latitude': latitude,\n                'longitude': longitude,\n                'time': timestamp,\n                'message': message}\n\n    # If the sentence contains message 5 - Ship Static and Voyage\n    # Related Data:\n    elif message == '5' and int(bindata[38:40],2) == 0:\n        # IMO number where 00000000=N/A\n        imo = int(bindata[40:70],2)\n        if imo == 0:\n            imo = None # N/A\n        # Callsign, removes the characters @, ' ' and \"\n        callsign = bintoascii(bindata[70:112]).strip('''@ ''').replace('''\"''',\"'\")\n        # Name, removes the characters @, ' ' and \"\n        name = bintoascii(bindata[112:232]).strip('''@ ''').replace('''\"''',\"'\")\n        # Ship type, a two-digit code where 00=N/A\n        type = int(bindata[232:240],2)\n        if type == 0:\n            type = None # N/A\n        # Ship length calculated from antenna position\n        length = (int(bindata[240:249],2) + int(bindata[249:258],2))\n        # Ship width calculated from antenna position\n        width = (int(bindata[258:264],2) + int(bindata[264:270],2))\n        # Received estimated time of arrival in format\n        # month-day-hour-minute: MMDDHHMM where 00000000=N/A\n        eta = (str(int(bindata[274:278],2)).zfill(2) +\n              str(int(bindata[278:283],2)).zfill(2) +\n              str(int(bindata[283:288],2)).zfill(2) +\n              str(int(bindata[288:294],2)).zfill(2))\n        if eta == '00000000':\n            eta = None\n        # Draught in 1/10 meters, where 0.0 == N/A\n        draught = decimal.Decimal(int(bindata[294:302],2)) / 10\n        if draught == 0:\n            draught = None\n        # Destination, removes the characters @, ' ' and \"\n        destination = bintoascii(bindata[302:422]).strip('''@ ''').replace('''\"''',\"'\")\n        # Return a dictionary with descriptive keys\n        return {'mmsi': mmsi,\n                'imo': imo,\n                'callsign': callsign,\n                'name': name,\n                'type': type,\n                'length': length,\n                'width': width,\n                'eta': eta,\n                'destination': destination,\n                'draught': draught,\n                'time': timestamp,\n                'message': message}\n\n    # If the sentence contains message 6 - Addressed Binary Message:\n    elif message == '6':\n        # Sequence number\n        sequence = int(bindata[38:40],2)\n        # Destination MMSI number\n        to_mmsi = int(bindata[40:70],2)\n        # Application ID (Designated Area Code, DAC) + (Function\n        # Identification, FI)\n        dac = int(bindata[72:82],2)\n        fi = int(bindata[82:88],2)\n        # Binary data payload\n        payload = bindata[88:1048]\n        # Try to decode message payload\n        decoded = binaryparser(dac,fi,payload)\n        # Return a dictionary with descriptive keys\n        return {'mmsi': mmsi,\n                'sequence': sequence,\n                'to_mmsi': to_mmsi,\n                'dac': dac,\n                'fi': fi,\n                'decoded': decoded,\n                'time': timestamp,\n                'message': message}\n\n    # If the sentence contains message 8 - Binary Broadcast Message:\n    elif message == '8':\n        # Application ID (Designated Area Code, DAC) + (Function\n        # Identification, FI)\n        dac = int(bindata[40:50],2)\n        fi = int(bindata[50:56],2)\n        # Binary data payload\n        payload = bindata[56:1008]\n        # Try to decode message payload\n        decoded = binaryparser(dac,fi,payload)\n        # Return a dictionary with descriptive keys\n        return {'mmsi': mmsi,\n                'dac': dac,\n                'fi': fi,\n                'decoded': decoded,\n                'time': timestamp,\n                'message': message}\n\n    # If the sentence contains message 9 - SAR Aircraft position\n    # report:\n    elif message == '9':\n        # Altitude in meters, 4095=N/A, 4094=>4094\n        altitude = int(bindata[38:50],2)\n        if altitude == 4095:\n            altitude = None # N/A\n        # Speed over ground in knots, 1023=N/A, 1022=>1022\n        sog = int(bindata[50:60],2)\n        if sog == 1023:\n            sog = None # N/A\n        # Position accuracy where 0=bad and 1=good/DGPS\n        posacc = int(bindata[60],2)\n        # Longitude in decimal degrees (DD)\n        longitude = calclongitude(bindata[61:89])\n        # Latitude in decimal degrees (DD)\n        latitude = calclatitude(bindata[89:116])\n        # Course over ground in 1/10 degrees between 0-359\n        cog = decimal.Decimal(int(bindata[116:128],2)) / 10\n        if cog > 360: # 360 and above means 360=N/A\n            cog = None\n        # Return a dictionary with descriptive keys\n        return {'mmsi': mmsi,\n                'altitude': altitude,\n                'sog': sog,\n                'posacc': posacc,\n                'latitude': latitude,\n                'longitude': longitude,\n                'cog': cog,\n                'time': timestamp,\n                'message': message}\n\n    # If the sentence contains message 12 - Addressed safety\n    # related message:\n    elif message == '12':\n        # Sequence number\n        sequence = int(bindata[38:40],2)\n        # Destination MMSI number\n        to_mmsi = int(bindata[40:70],2)\n        # Content of message in ASCII (replace any \" with ')\n        content = bintoascii(bindata[72:1008]).replace('''\"''',\"'\")\n        # Return a dictionary with descriptive keys\n        return {'mmsi': mmsi,\n                'sequence': sequence,\n                'to_mmsi': to_mmsi,\n                'content': content,\n                'time': timestamp,\n                'message': message}\n\n    # If the sentence contains message 14 - Safety related\n    # Broadcast Message:\n    elif message == '14':\n        # Content of message in ASCII (replace any \" with ')\n        content = bintoascii(bindata[40:1008]).replace('''\"''',\"'\")\n        # Return a dictionary with descriptive keys\n        return {'mmsi': mmsi,\n                'content': content,\n                'time': timestamp,\n                'message': message}\n\n    # If the sentence contains message 18 - Standard Class B CS\n    # Position Report:\n    elif message == '18':\n        # Speed over ground in 1/10 knots\n        sog = decimal.Decimal(int(bindata[46:56],2)) / 10\n        if sog > decimal.Decimal(\"102.2\"):\n            sog = None # N/A\n        # Position accuracy where 0=bad and 1=good/DGPS\n        posacc = int(bindata[56],2)\n        # Longitude in decimal degrees (DD)\n        longitude = calclongitude(bindata[57:85])\n        # Latitude in decimal degrees (DD)\n        latitude = calclatitude(bindata[85:112])\n        # Course over ground in 1/10 degrees between 0-359\n        cog = decimal.Decimal(int(bindata[112:124],2)) / 10\n        if cog > 360: # 360 and above means 360=N/A\n            cog = None\n        # Heading in whole degrees between 0-359 and 511=N/A\n        heading = int(bindata[124:133],2)\n        if heading > 359:\n            heading = None # N/A\n        # Return a dictionary with descriptive keys\n        return {'mmsi': mmsi,\n                'latitude': latitude,\n                'longitude': longitude,\n                'sog': sog,\n                'cog': cog,\n                'heading': heading,\n                'posacc': posacc,\n                'time': timestamp,\n                'message': message}\n\n    # If the sentence contains message 19 - Extended Class B\n    # Equipment Position Report:\n    elif message == '19':\n        # Speed over ground in 1/10 knots\n        sog = decimal.Decimal(int(bindata[46:56],2)) / 10\n        if sog > decimal.Decimal(\"102.2\"):\n            sog = None # N/A\n        # Position accuracy where 0=bad and 1=good/DGPS\n        posacc = int(bindata[56],2)\n        # Longitude in decimal degrees (DD)\n        longitude = calclongitude(bindata[57:85])\n        # Latitude in decimal degrees (DD)\n        latitude = calclatitude(bindata[85:112])\n        # Course over ground in 1/10 degrees between 0-359\n        cog = decimal.Decimal(int(bindata[112:124],2)) / 10\n        if cog > 360: # 360 and above means 360=N/A\n            cog = None\n        # Heading in whole degrees between 0-359 and 511=N/A\n        heading = int(bindata[124:133],2)\n        if heading > 359:\n            heading = None # N/A\n        # Name, removes the characters @, ' ' and \"\n        name = bintoascii(bindata[143:263]).strip('''@ ''').replace('''\"''',\"'\")\n        # Ship type, a two-digit code where 00=N/A\n        type = int(bindata[263:271],2)\n        if type == 0:\n            type = None # N/A\n        # Ship length calculated from antenna position\n        length = (int(bindata[271:280],2) + int(bindata[280:289],2))\n        # Ship width calculated from antenna position\n        width = (int(bindata[289:295],2) + int(bindata[295:301],2))\n        # Return a dictionary with descriptive keys\n        return {'mmsi': mmsi,\n                'latitude': latitude,\n                'longitude': longitude,\n                'sog': sog,\n                'cog': cog,\n                'heading': heading,\n                'posacc': posacc,\n                'name': name,\n                'type': type,\n                'length': length,\n                'width': width,\n                'time': timestamp,\n                'message': message}\n\n    # If the sentence contains message 24 - Class B CS Static Data\n    # Report:\n    elif message == '24':\n        # See if it is message part A or B\n        if int(bindata[38:40]) == 0: # Part A\n            # Name, removes the characters @, ' ' and \"\n            name = bintoascii(bindata[40:160]).strip('''@ ''').replace('''\"''',\"'\")\n            # Return a dictionary with descriptive keys\n            return {'mmsi': mmsi,\n                    'name': name,\n                    'time': timestamp,\n                    'message': message}\n        else: # Part B\n            # Ship type, a two-digit code where 00=N/A\n            type = int(bindata[40:48],2)\n            if type == 0:\n                type = None # N/A\n            # Vendor ID, removes the characters @, ' ' and \"\n            vendor = bintoascii(bindata[48:90]).strip('''@ ''').replace('''\"''',\"'\")\n            # Callsign, removes the characters @, ' ' and \"\n            callsign = bintoascii(bindata[90:132]).strip('''@ ''').replace('''\"''',\"'\")\n            # Ship length calculated from antenna position\n            length = (int(bindata[132:141],2) + int(bindata[141:150],2))\n            # Ship width calculated from antenna position\n            width = (int(bindata[150:156],2) + int(bindata[156:162],2))\n            # Return a dictionary with descriptive keys\n            return {'mmsi': mmsi,\n                    'type': type,\n                    'vendor': vendor,\n                    'callsign': callsign,\n                    'length': length,\n                    'width': width,\n                    'time': timestamp,\n                    'message': message}\n\n    else:\n        # If we don't decode the message, at least return message type\n        return {'mmsi': mmsi, 'time': timestamp, 'message': message, 'decoded': False}\n\n\n# If the sentence contains NMEA-compliant position data (from own GPS):\nif telegram[0] == '$GPGGA':\n    # Check the checksum\n    if not checksum(inputstring):\n        return\n    # Latitude\n    degree = int(telegram[2][0:2])\n    minutes = decimal.Decimal(telegram[2][2:9])\n    if telegram[3] == 'N':\n        latitude = degree + (minutes / 60)\n    else:\n        latitude = -(degree + (minutes / 60))\n    latitude = latitude.quantize(decimal.Decimal('1E-6'))\n    # Longitude\n    degree = int(telegram[4][0:3])\n    minutes = decimal.Decimal(telegram[4][3:10])\n    if telegram[5] == 'E':\n        longitude = degree + (minutes / 60)\n    else:\n        longitude = -(degree + (minutes / 60))\n    longitude = longitude.quantize(decimal.Decimal('1E-6'))\n    # Timestamp the message with local time\n    timestamp = datetime.datetime.now()\n    # Return a dictionary with descriptive keys\n    return {'ownlatitude': latitude, 'ownlongitude': longitude, 'time': timestamp}", "path": "aislogger\\decode.py", "repo_name": "olcai/ais-logger", "stars": 13, "license": "None", "language": "python", "size": 680}
{"docstring": "\"\"\"\nParse a config file or create a config file object.\n\n``ConfigObj(infile=None, options=None, **kwargs)``\n\"\"\"\n", "func_signal": "def __init__(self, infile=None, options=None, _inspec=False, **kwargs):\n", "code": "self._inspec = _inspec\n# init the superclass\nSection.__init__(self, self, 0, self)\n\ninfile = infile or []\noptions = dict(options or {})\n    \n# keyword arguments take precedence over an options dictionary\noptions.update(kwargs)\nif _inspec:\n    options['list_values'] = False\n\ndefaults = OPTION_DEFAULTS.copy()\n# TODO: check the values too.\nfor entry in options:\n    if entry not in defaults:\n        raise TypeError('Unrecognised option \"%s\".' % entry)\n\n# Add any explicit options to the defaults\ndefaults.update(options)\nself._initialise(defaults)\nconfigspec = defaults['configspec']\nself._original_configspec = configspec\nself._load(infile, configspec)", "path": "external\\configobj.py", "repo_name": "olcai/ais-logger", "stars": 13, "license": "None", "language": "python", "size": 680}
{"docstring": "\"\"\"\nGiven a value string, unquote, remove comment,\nhandle lists. (including empty and single member lists)\n\"\"\"\n", "func_signal": "def _handle_value(self, value):\n", "code": "if self._inspec:\n    # Parsing a configspec so don't handle comments\n    return (value, '')\n# do we look for lists in values ?\nif not self.list_values:\n    mat = self._nolistvalue.match(value)\n    if mat is None:\n        raise SyntaxError()\n    # NOTE: we don't unquote here\n    return mat.groups()\n#\nmat = self._valueexp.match(value)\nif mat is None:\n    # the value is badly constructed, probably badly quoted,\n    # or an invalid list\n    raise SyntaxError()\n(list_values, single, empty_list, comment) = mat.groups()\nif (list_values == '') and (single is None):\n    # change this if you want to accept empty values\n    raise SyntaxError()\n# NOTE: note there is no error handling from here if the regex\n# is wrong: then incorrect values will slip through\nif empty_list is not None:\n    # the single comma - meaning an empty list\n    return ([], comment)\nif single is not None:\n    # handle empty values\n    if list_values and not single:\n        # FIXME: the '' is a workaround because our regex now matches\n        #   '' at the end of a list if it has a trailing comma\n        single = None\n    else:\n        single = single or '\"\"'\n        single = self._unquote(single)\nif list_values == '':\n    # not a list value\n    return (single, comment)\nthe_list = self._listvalueexp.findall(list_values)\nthe_list = [self._unquote(val) for val in the_list]\nif single is not None:\n    the_list += [single]\nreturn (the_list, comment)", "path": "external\\configobj.py", "repo_name": "olcai/ais-logger", "stars": 13, "license": "None", "language": "python", "size": 680}
{"docstring": "\"\"\"Deal with a comment.\"\"\"\n", "func_signal": "def _handle_comment(self, comment):\n", "code": "if not comment:\n    return ''\nstart = self.indent_type\nif not comment.startswith('#'):\n    start += self._a_to_u(' # ')\nreturn (start + comment)", "path": "external\\configobj.py", "repo_name": "olcai/ais-logger", "stars": 13, "license": "None", "language": "python", "size": 680}
{"docstring": "# This function simplifies in checking for N/A-values and signs\n# Check if just ones, then return N/A (Nonetype)\n", "func_signal": "def standard_int_signed_field(data):\n", "code": "if data.count('1') == len(data):\n    return None\nelse:\n    # Return the signed integer\n    if data[0]:\n        # Positive\n        return int(data[1:],2)\n    else:\n        # Negative\n        return -int(data[1:],2)", "path": "aislogger\\decode.py", "repo_name": "olcai/ais-logger", "stars": 13, "license": "None", "language": "python", "size": 680}
{"docstring": "# This function simplifies in checking for N/A-values\n# Check if just ones, then return N/A (Nonetype)\n", "func_signal": "def standard_int_field(data):\n", "code": "if data.count('1') == len(data):\n    return None\nelse:\n    return int(data,2)", "path": "aislogger\\decode.py", "repo_name": "olcai/ais-logger", "stars": 13, "license": "None", "language": "python", "size": 680}
{"docstring": "\"\"\"Write an individual line, for the write method\"\"\"\n# NOTE: the calls to self._quote here handles non-StringType values.\n", "func_signal": "def _write_line(self, indent_string, entry, this_entry, comment):\n", "code": "if not self.unrepr:\n    val = self._decode_element(self._quote(this_entry))\nelse:\n    val = repr(this_entry)\nreturn '%s%s%s%s%s' % (indent_string,\n                       self._decode_element(self._quote(entry, multiline=False)),\n                       self._a_to_u(' = '),\n                       val,\n                       self._decode_element(comment))", "path": "external\\configobj.py", "repo_name": "olcai/ais-logger", "stars": 13, "license": "None", "language": "python", "size": 680}
{"docstring": "\"\"\"\nA version of update that uses our ``__setitem__``.\n\"\"\"\n", "func_signal": "def update(self, indict):\n", "code": "for entry in indict:\n    self[entry] = indict[entry]", "path": "external\\configobj.py", "repo_name": "olcai/ais-logger", "stars": 13, "license": "None", "language": "python", "size": 680}
{"docstring": "\"\"\"enumerate for Python 2.2.\"\"\"\n", "func_signal": "def enumerate(obj):\n", "code": "i = -1\nfor item in obj:\n    i += 1\n    yield i, item", "path": "external\\configobj.py", "repo_name": "olcai/ais-logger", "stars": 13, "license": "None", "language": "python", "size": 680}
{"docstring": "\"\"\"\nReturn a safely quoted version of a value.\n\nRaise a ConfigObjError if the value cannot be safely quoted.\nIf multiline is ``True`` (default) then use triple quotes\nif necessary.\n\n* Don't quote values that don't need it.\n* Recursively quote members of a list and return a comma joined list.\n* Multiline is ``False`` for lists.\n* Obey list syntax for empty and single member lists.\n\nIf ``list_values=False`` then the value is only quoted if it contains\na ``\\\\n`` (is multiline) or '#'.\n\nIf ``write_empty_values`` is set, and the value is an empty string, it\nwon't be quoted.\n\"\"\"\n", "func_signal": "def _quote(self, value, multiline=True):\n", "code": "if multiline and self.write_empty_values and value == '':\n    # Only if multiline is set, so that it is used for values not\n    # keys, and not values that are part of a list\n    return ''\n\nif multiline and isinstance(value, (list, tuple)):\n    if not value:\n        return ','\n    elif len(value) == 1:\n        return self._quote(value[0], multiline=False) + ','\n    return ', '.join([self._quote(val, multiline=False)\n        for val in value])\nif not isinstance(value, basestring):\n    if self.stringify:\n        value = str(value)\n    else:\n        raise TypeError('Value \"%s\" is not a string.' % value)\n\nif not value:\n    return '\"\"'\n\nno_lists_no_quotes = not self.list_values and '\\n' not in value and '#' not in value\nneed_triple = multiline and (((\"'\" in value) and ('\"' in value)) or ('\\n' in value ))\nhash_triple_quote = multiline and not need_triple and (\"'\" in value) and ('\"' in value) and ('#' in value)\ncheck_for_single = (no_lists_no_quotes or not need_triple) and not hash_triple_quote\n\nif check_for_single:\n    if not self.list_values:\n        # we don't quote if ``list_values=False``\n        quot = noquot\n    # for normal values either single or double quotes will do\n    elif '\\n' in value:\n        # will only happen if multiline is off - e.g. '\\n' in key\n        raise ConfigObjError('Value \"%s\" cannot be safely quoted.' % value)\n    elif ((value[0] not in wspace_plus) and\n            (value[-1] not in wspace_plus) and\n            (',' not in value)):\n        quot = noquot\n    else:\n        quot = self._get_single_quote(value)\nelse:\n    # if value has '\\n' or \"'\" *and* '\"', it will need triple quotes\n    quot = self._get_triple_quote(value)\n\nif quot == noquot and '#' in value and self.list_values:\n    quot = self._get_single_quote(value)\n        \nreturn quot % value", "path": "external\\configobj.py", "repo_name": "olcai/ais-logger", "stars": 13, "license": "None", "language": "python", "size": 680}
{"docstring": "\"\"\"Decode ASCII strings to unicode if a self.encoding is specified.\"\"\"\n", "func_signal": "def _a_to_u(self, aString):\n", "code": "if self.encoding:\n    return aString.decode('ascii')\nelse:\n    return aString", "path": "external\\configobj.py", "repo_name": "olcai/ais-logger", "stars": 13, "license": "None", "language": "python", "size": 680}
{"docstring": "\"\"\"Fetch the item and do string interpolation.\"\"\"\n", "func_signal": "def __getitem__(self, key):\n", "code": "val = dict.__getitem__(self, key)\nif self.main.interpolation and isinstance(val, basestring):\n    return self._interpolate(key, val)\nreturn val", "path": "external\\configobj.py", "repo_name": "olcai/ais-logger", "stars": 13, "license": "None", "language": "python", "size": 680}
{"docstring": "# Calculates longitude\n# First look at the signed bit\n", "func_signal": "def calclongitude(binary_longitude):\n", "code": "sign = int(binary_longitude[0])\nlongitude = int(binary_longitude[1:],2)\n# See how many bits we're looking at\nnr_bits = len(binary_longitude)\nif nr_bits == 25:\n    factor = 60000 # 1000 * 60\n    power = 24\nelif nr_bits == 28:\n    factor = 600000 # 10000 * 60\n    power = 27\nelse:\n    # Better to return None than a wrong value\n    return None\n# See if the longitude are undefined (long=181)\nif longitude == 181*factor:\n    return None # N/A\n# Else, calculate the longitude\nif sign: # Negative == West\n    longitude = pow(2,power) - longitude\n    degree = -decimal.Decimal(longitude) / factor\nelse: # Positive == East\n    degree = decimal.Decimal(longitude) / factor\n# Return a value quantized to six decimal digits\nreturn degree.quantize(decimal.Decimal('1E-6'))", "path": "aislogger\\decode.py", "repo_name": "olcai/ais-logger", "stars": 13, "license": "None", "language": "python", "size": 680}
{"docstring": "\"\"\"Clear ConfigObj instance and restore to 'freshly created' state.\"\"\"\n", "func_signal": "def reset(self):\n", "code": "self.clear()\nself._initialise()\n# FIXME: Should be done by '_initialise', but ConfigObj constructor (and reload)\n#        requires an empty dictionary\nself.configspec = None\n# Just to be sure ;-)\nself._original_configspec = None", "path": "external\\configobj.py", "repo_name": "olcai/ais-logger", "stars": 13, "license": "None", "language": "python", "size": 680}
{"docstring": "\"\"\"\n'D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised'\n\"\"\"\n", "func_signal": "def pop(self, key, *args):\n", "code": "val = dict.pop(self, key, *args)\nif key in self.scalars:\n    del self.comments[key]\n    del self.inline_comments[key]\n    self.scalars.remove(key)\nelif key in self.sections:\n    del self.comments[key]\n    del self.inline_comments[key]\n    self.sections.remove(key)\nif self.main.interpolation and isinstance(val, basestring):\n    return self._interpolate(key, val)\nreturn val", "path": "external\\configobj.py", "repo_name": "olcai/ais-logger", "stars": 13, "license": "None", "language": "python", "size": 680}
{"docstring": "\"\"\"\nRestore (and return) default value for the specified key.\n\nThis method will only work for a ConfigObj that was created\nwith a configspec and has been validated.\n\nIf there is no default value for this key, ``KeyError`` is raised.\n\"\"\"\n", "func_signal": "def restore_default(self, key):\n", "code": "default = self.default_values[key]\ndict.__setitem__(self, key, default)\nif key not in self.defaults:\n    self.defaults.append(key)\nreturn default", "path": "external\\configobj.py", "repo_name": "olcai/ais-logger", "stars": 13, "license": "None", "language": "python", "size": 680}
{"docstring": "\"\"\"A dummy check method, always returns the value unchanged.\"\"\"\n", "func_signal": "def check(self, check, member, missing=False):\n", "code": "if missing:\n    raise self.baseErrorClass()\nreturn member", "path": "external\\configobj.py", "repo_name": "olcai/ais-logger", "stars": 13, "license": "None", "language": "python", "size": 680}
{"docstring": "# This function decodes known binary messages and returns the\n# interesting data as a dictionary where each key describes\n# the information of each message part.\n\n# For each value where we have a N/A-state None is returned\n\n# Initiate a return dict\n", "func_signal": "def binaryparser(dac,fi,data):\n", "code": "retdict = {}\n\n# If the message is IFM 0: free text message\nif dac == 1 and fi == 0:\n    return {'text': bintoascii(data[12:]).strip('''@ ''').replace('''\"''',\"'\")}\n\n# If the message is an IMO Meterology and Hydrology Message,\n# as specified in IMO SN/Circ. 236, Annex 2, Application 1:\nelif dac == 1 and fi == 11:\n    # Latitude in decimal degrees (DD)\n    retdict['latitude'] = calclatitude(data[0:24])\n    # Longitude in decimal degrees (DD)\n    retdict['longitude'] = calclongitude(data[24:49])\n    # Bits 49-65 contains current station time in UTC (ddhhmm)\n    # We use computer time as a baseline for year and month\n    try:\n        station_time = datetime.datetime.utcnow()\n        station_time = station_time.replace(day=int(data[49:54],2),\n                                            hour=int(data[54:59],2),\n                                            minute=int(data[59:65],2),\n                                            second=0, microsecond=0)\n        retdict['station_time'] = station_time\n    except ValueError:\n        retdict['station_time'] = None # N/A\n    # Average of wind speed values for the last ten minutes, knots\n    retdict['average_wind_speed'] = standard_int_field(data[65:72])\n    # Wind gust (maximum wind speed value) during the last ten\n    # minutes, knots\n    retdict['wind_gust'] = standard_int_field(data[72:79])\n    # Wind direction in whole degrees\n    retdict['wind_direction'] = standard_int_field(data[79:88])\n    # Wind gust direction in whole degrees\n    retdict['wind_gust_direction'] = standard_int_field(data[88:97])\n    # Air temperature in 0.1 degrees Celsius from -60.0 to +60.0\n    retdict['air_temperature'] = standard_decimal_tenth_signed_field(data[97:108])\n    # Relative humidity in percent\n    retdict['relative_humidity'] = standard_int_field(data[108:115])\n    # Dew point in 0.1 degrees Celsius from -20.0 to +50.0\n    retdict['dew_point'] = standard_decimal_tenth_signed_field(data[115:125])\n    # Air pressure in whole hPa\n    retdict['air_pressure'] = standard_int_field(data[125:134])\n    # Air pressure tendency where 0=steady, 1=decreasing, 2=increasing\n    retdict['air_pressure_tendency'] = standard_int_field(data[134:136])\n    # Horizontal visibility in 0.1 NM steps\n    retdict['horizontal_visibility'] = standard_decimal_tenth_field(data[136:144])\n    # Water level including tide, deviation from local chart datum,\n    # in 0.1 m from -10.0 to 30.0 m\n    retdict['water_level_incl_tide'] = standard_decimal_tenth_signed_field(data[144:153])\n    # Water level trend where 0=steady, 1=decreasing, 2=increasing\n    retdict['water_level_trend'] = standard_int_field(data[153:155])\n    # Surface current speed including tide in 0.1 kt steps\n    retdict['surface_current_speed_incl_tide'] = standard_decimal_tenth_field(data[155:163])\n    # Surface current direction in whole degrees\n    retdict['surface_current_direction'] = standard_int_field(data[163:172])\n    # Current speed #2, chosen below sea surface, in 0.1 kt steps\n    retdict['current_speed_2'] = standard_decimal_tenth_field(data[172:180])\n    # Current direction #2, chosen below sea surface in whole degrees\n    retdict['current_direction_2'] = standard_int_field(data[180:189])\n    # Current measuring level #2, whole meters below sea surface\n    retdict['current_measuring_level_2'] = standard_int_field(data[189:194])\n    # Current speed #3, chosen below sea surface, in 0.1 kt steps\n    retdict['current_speed_3'] = standard_decimal_tenth_field(data[194:202])\n    # Current direction #3, chosen below sea surface in whole degrees\n    retdict['current_direction_3'] = standard_int_field(data[202:211])\n    # Current measuring level #3, whole meters below sea surface\n    retdict['current_measuring_level_3'] = standard_int_field(data[211:216])\n    # Significant wave height in 0.1 m steps\n    retdict['significant_wave_height'] = standard_decimal_tenth_field(data[216:224])\n    # Wave period in whole seconds\n    retdict['wave_period'] = standard_int_field(data[224:230])\n    # Wave direction in whole degrees\n    retdict['wave_direction'] = standard_int_field(data[230:239])\n    # Swell height in 0.1 m steps\n    retdict['swell_height'] = standard_decimal_tenth_field(data[239:247])\n    # Swell period in whole seconds\n    retdict['swell_period'] = standard_int_field(data[247:253])\n    # Swell direction in whole degrees\n    retdict['swell_direction'] = standard_int_field(data[253:262])\n    # Sea state according to Beaufort scale (0-12)\n    retdict['sea_state'] = standard_int_field(data[262:266])\n    # Water temperature in 0.1 degrees Celsius from -10.0 to +50.0\n    retdict['water_temperature'] = standard_decimal_tenth_signed_field(data[266:276])\n    # Precipitation type according to WMO\n    retdict['precipitation_type'] = standard_int_field(data[276:279])\n    # Salinity in parts per thousand from 0.0 to 50.0\n    retdict['salinity'] = standard_decimal_tenth_field(data[279:288])\n    # Ice, Yes/No\n    retdict['ice'] = standard_int_field(data[288:290])\n    # Return a dictionary with descriptive keys\n    return retdict\n\n# If we cannot decode the message, return None\nelse:\n    return None", "path": "aislogger\\decode.py", "repo_name": "olcai/ais-logger", "stars": 13, "license": "None", "language": "python", "size": 680}
{"docstring": "# Delete the ModelComparison and check if the Comparison still exists (by counting all (Model)Comparison objects)\n", "func_signal": "def test_delete(self):\n", "code": "response = self.client.get('/user/compare/model/delete/1/')\nself.assertEqual(ModelComparison.objects.all().count(), 0)\nself.assertEqual(Comparison.objects.all().count(),1)\n\n# Now delete the remaining Comparison as well\nresponse = self.client.get('/user/compare/delete/1/')\nself.assertEqual(ModelComparison.objects.all().count(),0)\nself.assertEqual(Comparison.objects.all().count(),0)", "path": "beat\\benchmarks\\tests.py", "repo_name": "ties/BeAT", "stars": 9, "license": "None", "language": "python", "size": 1707}
{"docstring": "# Set the mimetype of the HttpResponse\n", "func_signal": "def export(canvas, title, format='png'):\n", "code": "mimetype = ''\nif (format is 'png'):\n\tmimetype = 'image/png'\nelif (format is 'pdf'):\n\tmimetype = 'application/pdf'\nelif (format is ('ps' or 'eps')):\n\tmimetype = 'application/postscript'\nelif (format is 'svg'):\n\tmimetype = 'image/svg+xml'\nresponse = HttpResponse(content_type=mimetype)\n\n# Show the user a 'Save as..' dialogue if the graph is not PNG.\nif (format is not 'png'):\n\tresponse['Content-Disposition'] = 'attachment; filename=%s.%s' % (title, format)\n# Print to canvas with the right format\ncanvas.print_figure(filename=response, format=format, dpi=80)\nreturn response", "path": "beat\\tools\\graph.py", "repo_name": "ties/BeAT", "stars": 9, "license": "None", "language": "python", "size": 1707}
{"docstring": "\"\"\"\nOutput a graph for model comparison.\nEach seperate Model has one line; the data for this line is determined by Benchmarks that are filtered from the db.\n@param id ModelComparison ID from the database, used to filter the Benchmark data from the db.\n@param format The export format for the graph. Choices: ['png','pdf','ps','eps','svg']\n\"\"\"\n# General library stuff\n", "func_signal": "def graph_model(request, id, format='png'):\n", "code": "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\nfrom matplotlib.lines import Line2D\nfrom matplotlib.figure import Figure\nimport matplotlib\nimport matplotlib.pyplot as plt\n\n# DB stuff\nfrom django.db.models import Count\n\n# Take the ModelComparison from db and filter data\ncomparison = ModelComparison.objects.get(pk=id)\nc_tool = comparison.tool\nc_algo = comparison.algorithm\nc_type = comparison.type\nc_option = comparison.optionvalue\n\nfig=Figure(facecolor='w')\nax=fig.add_subplot(111)\n\n# Lists of colors, styles and markers to get a nice unique style for each line\ncolors = ('b', 'g', 'r', 'c', 'm', 'y', 'k')\nstyles = ['-', '--', ':']\nmarkers = ['+','o','x']\n\n# Plot data\naxisNum = 0 # Counts the number of lines (to produce a unique style for each line)\nmodelNames = Model.objects.values('name').annotate(num_models=Count('name'))\n\n# Plot a line for each model\nfor m in modelNames:\n\taxisNum += 1\n\tstyle = styles[axisNum % len(styles) ]\n\tcolor = colors[axisNum % len(colors) ]\n\tmarker = markers[axisNum % len(markers) ]\n\t\n\tbenchmarks = Benchmark.objects.filter(model__name__exact = m['name'])\n\t# Filter benchmarks based on the ModelComparison data\n\tbenchmarks = benchmarks.filter(algorithm_tool__algorithm = c_algo, algorithm_tool__tool = c_tool).order_by('algorithm_tool__date')\n\tbenchmarks = benchFind(benchmarks,[o.id for o in c_option.all()])\n\t\n\tif (len(benchmarks) != 0):\n\t\t\n\t\t# Static data types to plot in the graph\n\t\ttypes = []\n\t\tif (c_type == ModelComparison.TRANSITIONS):\n\t\t\ttypes = [b.transition_count for b in benchmarks]\n\t\telif (c_type == ModelComparison.STATES):\n\t\t\ttypes = [b.states_count for b in benchmarks]\n\t\telif (c_type == ModelComparison.VSIZE):\n\t\t\ttypes = [b.memory_VSIZE for b in benchmarks]\n\t\telif (c_type == ModelComparison.RSS):\n\t\t\ttypes = [b.memory_RSS for b in benchmarks]\n\t\telif (c_type == ModelComparison.ELAPSED_TIME):\n\t\t\ttypes = [b.elapsed_time for b in benchmarks]\n\t\telif (c_type == ModelComparison.TOTAL_TIME):\n\t\t\ttypes = [b.total_time for b in benchmarks]\n\t\t\n\t\t# Plot data\n\t\tlines = ax.plot(\n\t\t\t[b.algorithm_tool.date for b in benchmarks], \n\t\t\ttypes, \n\t\t\tmarker + style + color,\n\t\t\tlabel = m['name'])\n\n# Mark-up\ntitle = c_tool.name + c_algo.name\nif c_option.all():\n\toptions = [str(o) for o in c_option.all()]\n\ttitle = title + ' [' + ','.join(options) + ']'\nax.set_title(title)\n\n# Print legend for lines in the graph.\nleg = ax.legend(fancybox=True, loc='upper left',bbox_to_anchor = (1,1.15), markerscale=5)\nif leg:\n\tfor t in leg.get_texts():\n\t\tt.set_fontsize('xx-small')\n\t\n# Print labels for the axes.\ny_label = c_type\nfor l in ModelComparison.DATA_TYPES:\n\ta,b = l\n\tif a == c_type:\n\t\ty_label = b\nax.set_ylabel(y_label)\nax.set_xlabel('Revision date')\nfig.autofmt_xdate()\n\nfig.subplots_adjust(right=0.7)\n\n# Output\ncanvas = FigureCanvas(fig)\nresponse = graph.export(canvas, comparison.name, format)\nreturn response", "path": "beat\\comparisons\\views.py", "repo_name": "ties/BeAT", "stars": 9, "license": "None", "language": "python", "size": 1707}
{"docstring": "\"\"\"\nSwitches working repository\n@assure if folder contains a .git the working repository is the given repository with a repo. else you can now create a new repo here with clone_repository.\n\"\"\"\n", "func_signal": "def switch_repository(self, folder):\n", "code": "has_repository = True\nself.folder = folder\ntry:\n\tRepo(self.folder)\nexcept InvalidGitRepositoryError:\n\thas_repository = False", "path": "beat\\gitinterface.py", "repo_name": "ties/BeAT", "stars": 9, "license": "None", "language": "python", "size": 1707}
{"docstring": "# Create TestClient object\n", "func_signal": "def setUp(self):\n", "code": "self.client = Client()\n# Create dummy user\nself.u = User.objects.create_user('john', 'lennon@thebeatles.com', 'smith')\n# Log the user in\nself.client.post('/login/', {'username': 'john', 'password': 'smith'})\n# Create other required dummy objects\nself.a = Algorithm.objects.create(name=\"TestAlgorithm\")\nself.t = Tool.objects.create(name=\"TestTool\", version=1)\nself.c = Comparison.objects.create(user=self.u, benchmarks=1, date_time=\"01-01-2000 00:00:00\")\nself.m = ModelComparison.objects.create(user=self.u, type=\"states\", tool=self.t, algorithm=self.a, date_time=\"01-01-2000 00:00:00\")", "path": "beat\\benchmarks\\tests.py", "repo_name": "ties/BeAT", "stars": 9, "license": "None", "language": "python", "size": 1707}
{"docstring": "\"\"\"reads and returns the specified log\nThis function fetches a log specified by filename, reads it and returns it as a single string.\nThis function will always read from the HEAD commit.\nArguments:\n\tfilename\t\t\tthe name of the file being read,\nReturns:\n\tThe contents of the log.\nRaises:\n\tGitFileError\t\twhen the file does not exists, or when an exception is raised\nNOT IMPLEMENTED YET\n\"\"\"\n", "func_signal": "def get_log(repo, filename):\n", "code": "try:\n\tfor commit_id in repo.revision_history(repo.head()):\n\t\ttree = repo.tree(repo.commit(repo.head()).tree)\n\t\tif tree.__contains__(filename):\n\t\t\t(perms, file_blob_id)=tree.__getitem__(filename)\n\t\t\treturn repo.get_blob(file_blob_id).as_raw_string()\nexcept Exception as e:\n\traise GitFileError(e)", "path": "beat\\tools\\logsave.py", "repo_name": "ties/BeAT", "stars": 9, "license": "None", "language": "python", "size": 1707}
{"docstring": "\"\"\"\nReturns the git.Commit of the given git hash\n@requires folder to be a Sring with the location of a .git folder\n@returns the git.Commit of the object associated wiht the given hash. Will return the startnode of the tree if item does not exist.\n\"\"\"\n#iterator for all commits.\n", "func_signal": "def get_matching_item(self, hash):\n", "code": "i = Repo(self.folder).iter_commits()\nh = i.next()\ntry:\n\t# try to mach the srint with all commits.\n\twhile not(str(h.sha).startswith(hash)):\n\t\th = i.next()\nexcept StopIteration:\n\t# only gets here when nothing matches and does nothing\n\tpass\nreturn h", "path": "beat\\gitinterface.py", "repo_name": "ties/BeAT", "stars": 9, "license": "None", "language": "python", "size": 1707}
{"docstring": "\"\"\"\nDeletes a Comparison if model=False; else deletes a ModelComparison\n@param id The id of the (Model)Comparison that needs to be deleted\n@param model Boolean value to inidicate whether it is a ModelComparison or Comparison object being deleted.\n\"\"\"\n", "func_signal": "def comparison_delete(request, id, model=False):\n", "code": "if model:\n\tc = ModelComparison.objects.get(pk=id)\nelse:\n\tc = Comparison.objects.get(pk=id)\n\n# Check if the user is authorized to delete the object\nif c.user == request.user:\n\tc.delete()\n\treturn redirect('/user/compare/')\nelse:\n\treturn HttpResponseForbidden('<h1>You are not authorized to view this page.</h1>')", "path": "beat\\comparisons\\views.py", "repo_name": "ties/BeAT", "stars": 9, "license": "None", "language": "python", "size": 1707}
{"docstring": "\"\"\"\nFinds the benchmarks from b with the optionvalues as specified in ovs\n@param b Benchmark object\n@param ovs A list of OptionValue ids.\n\"\"\"\n\n", "func_signal": "def benchFind(b, ovs):\n", "code": "res = []\nfor benchmark in b:\n\topts = [o.id for o in benchmark.optionvalue.all()]\n\tif (set(opts) == set(ovs)):\n\t\tres.append(benchmark)\nreturn res", "path": "beat\\comparisons\\views.py", "repo_name": "ties/BeAT", "stars": 9, "license": "None", "language": "python", "size": 1707}
{"docstring": "\"\"\"\nCompareScatterplotForm handler. Creates a Comparison object if the form is valid and returns the graph.\n\"\"\"\n", "func_signal": "def compare_scatterplot(request):\n", "code": "if request.method == 'POST': # If the form has been submitted...\n\tform = CompareScatterplotForm(request.POST) # A form bound to the POST data\n\tif form.is_valid(): # All validation rules pass\n\t\tid_a = form.cleaned_data['a_algorithmtool']\n\t\tid_b = form.cleaned_data['b_algorithmtool']\n\t\t\n\t\t# Create a Comparison object with posted form data.\n\t\tc = Comparison.objects.create(\n\t\t\tuser = request.user, \n\t\t\talgorithm_tool_a = id_a,\n\t\t\talgorithm_tool_b = id_b,\n\t\t\tname = form.cleaned_data['name']\n\t\t)\n\t\t\n\t\t# Add Many-To-Many relations for the OptionValues.\n\t\tfor o_a in OptionValue.objects.filter(id__in=form.cleaned_data['a_options']):\n\t\t\tc.optionvalue_a.add(o_a)\n\t\tfor o_b in OptionValue.objects.filter(id__in=form.cleaned_data['b_options']):\n\t\t\tc.optionvalue_b.add(o_b)\n\t\t\n\t\t# Add many-to-many fields for \n\t\tc.hash = c.getHash()\n\t\tif (c.name == ''): \n\t\t\tc.name = str(c.id)\n\t\tc.save()\n\t\treturn redirect('detail_benchmark', id=c.id)\nelse:\n\tform = CompareScatterplotForm() # An unbound form\nreturn render_to_response('comparisons/compare_benchmarks_form.html', {\n\t'form': form,\n}, context_instance=RequestContext(request))", "path": "beat\\comparisons\\views.py", "repo_name": "ties/BeAT", "stars": 9, "license": "None", "language": "python", "size": 1707}
{"docstring": "\"\"\"\nCompareModelsForm handler. Creates a ModelComparison object if the form is valid and returns the graph.\n\"\"\"\n", "func_signal": "def compare_model(request):\n", "code": "if request.method == 'POST': # If the form has been submitted...\n\tform = CompareModelsForm(request.POST) # A form bound to the POST data\n\tif form.is_valid(): # All validation rules pass\n\t\t\n\t\t# Create a ModelComparison object with posted form data.\n\t\tc = ModelComparison.objects.create(\n\t\t\tuser = request.user, \n\t\t\talgorithm = form.cleaned_data['algorithm'],\n\t\t\ttool = form.cleaned_data['tool'],\n\t\t\ttype = form.cleaned_data['type'],\n\t\t\tname = form.cleaned_data['name']\n\t\t)\n\t\t\n\t\t# Add Many-To-Many relations for the OptionValues.\n\t\tfor o in OptionValue.objects.filter(id__in=form.cleaned_data['option']):\n\t\t\tc.optionvalue.add(o)\n\t\t\n\t\t# Fix the hash and name.\n\t\tc.hash = c.getHash()\n\t\tif (c.name == ''): \n\t\t\tc.name = str(c.id)\n\t\tc.save()\n\t\t\n\t\t#return render_to_response('compare_models.html', { 'id' : comparison.id }, context_instance=RequestContext(request))\n\t\treturn redirect('detail_model', id=c.id)\nelse:\n\tform = CompareModelsForm() # An unbound form\n\nreturn render_to_response('comparisons/compare_models_form.html', {\n\t'form': form, \n}, context_instance=RequestContext(request))", "path": "beat\\comparisons\\views.py", "repo_name": "ties/BeAT", "stars": 9, "license": "None", "language": "python", "size": 1707}
{"docstring": "\"\"\"\nThis function will return a list of objects.\nIt wil return the given code and number amount of parents of this node as a list newest item first.\n@requres code be a git.Commit item.\n\"\"\"\n", "func_signal": "def parents_list(self, code, number):\n", "code": "a = [code]\nif number > 0:\n\ta.extend(parents_list(code.parents[0],number-1))\nreturn a", "path": "beat\\gitinterface.py", "repo_name": "ties/BeAT", "stars": 9, "license": "None", "language": "python", "size": 1707}
{"docstring": "\"\"\"Return the last known tree.\n\t\"\"\"\n", "func_signal": "def get_latest_tree(repo):\n", "code": "commit_id = repo.head()\ncommit = repo.commit(commit_id)\ntree_id = commit.tree\nreturn repo.tree(tree_id)", "path": "beat\\tools\\logsave.py", "repo_name": "ties/BeAT", "stars": 9, "license": "None", "language": "python", "size": 1707}
{"docstring": "\"\"\"\nAllows pulling form git needs the git server as a string\n\"\"\"\n", "func_signal": "def pull_from_git(self, git):\n", "code": "chdir(self.folder)\nsystem(\"git pull %s master\" % git)", "path": "beat\\gitinterface.py", "repo_name": "ties/BeAT", "stars": 9, "license": "None", "language": "python", "size": 1707}
{"docstring": "\"\"\"\nPrefoms a git clone Action wiht the given repository and wil place it in the working directory.\n@require Given directory does not contain a .git folder.\n\"\"\"\n", "func_signal": "def clone_repository(self, git):\n", "code": "chdir(self.folder)\nsystem(\"git clone %s\" % git)\nhas_repository = True", "path": "beat\\gitinterface.py", "repo_name": "ties/BeAT", "stars": 9, "license": "None", "language": "python", "size": 1707}
{"docstring": "\"\"\"\nReturns a list of all the commits in the given folder.\n\"\"\"\n#iterator for all commits.\n", "func_signal": "def get_all_commits(self):\n", "code": "i = Repo(self.folder).iter_commits()\nh = []\ntry:\n\t# add every commit to a list.\n\twhile True:\n\t\th.append(i.next())\nexcept StopIteration:\n\t# terminates the loop when all items are done.\n\tpass\nreturn h", "path": "beat\\gitinterface.py", "repo_name": "ties/BeAT", "stars": 9, "license": "None", "language": "python", "size": 1707}
{"docstring": "\"\"\"\nTakes a Tag and gives the commit its part of.\n@requires a string representing the tag of your object\n@returns the item matching that tag or if non found the nieuwest tag.\n\"\"\"\n", "func_signal": "def match_from_tag(self, giventag):\n", "code": "repo = Repo(self.folder)\ntagref = TagReference.list_items(repo)\nx = 0\nb = 1 > 0\nwhile x < len(tagref)-1 and b:\n\tb = not(str(tagref[x].path).endswith(giventag))\n\tx = x + 1\nreturn tagref[x-1].commit", "path": "beat\\gitinterface.py", "repo_name": "ties/BeAT", "stars": 9, "license": "None", "language": "python", "size": 1707}
{"docstring": "\"\"\"\nShows the comparison graph that is saved by a user.\n@param id Comparison object id\n@param model Boolean value to inidicate whether it is a ModelComparison or Comparison object being viewed.\n\"\"\"\n\n# Check if the comparison is a ModelComparison or Comparison\n# Then retrieve the correct object and make a response object with it\n", "func_signal": "def compare_detail(request, id, model=False):\n", "code": "if model:\n\t# Find the ModelComparison or give a 404 error.\n\tc = get_object_or_404(ModelComparison,pk=id)\n\t\n\t# Filter selected OptionValues.\n\tbenchesNoOptionFilter = Benchmark.objects.filter(algorithm_tool__tool=c.tool, algorithm_tool__algorithm = c.algorithm).order_by('model__name')\n\tbenches = benchFind(benchesNoOptionFilter,[o.id for o in c.optionvalue.all()])\n\t\n\t# Filter the Model names.\n\tmodels = Model.objects.filter(id__in=[b.model.id for b in benches])\n\n\t# JSON serialization for flot graph.\n\tbenchjson = serializers.serialize(\"json\", benches)\t\t\n\tmodeljson = serializers.serialize(\"json\", models)\n\t\n\t# Empty export form, so the user can choose an export format.\n\tform = ExportGraphForm()\n\t\n\tresponse = render_to_response('comparisons/compare_models.html', { 'comparison' : c, 'form' : form, 'benches' : benches, 'benchjson' : benchjson, 'modeljson' : modeljson}, context_instance=RequestContext(request))\nelse:\n\t# Get the Comparison object or 404.\n\tc = get_object_or_404(Comparison,pk=id)\n\t\n\t# Filter the AlgorithmTool.\n\tat_a = c.algorithm_tool_a\n\tat_b = c.algorithm_tool_b\n\tb1 = Benchmark.objects.filter(algorithm_tool=at_a)\n\tb2 = Benchmark.objects.filter(algorithm_tool=at_b)\n\t\n\t# Filter on Models.\n\tb1 = b1.filter(model__in=[b.model.pk for b in b2])\n\tb2 = b2.filter(model__in=[b.model.pk for b in b1])\n\t\n\t# Filter on OptionValues.\n\tb1 = benchFind(b1,[o.id for o in c.optionvalue_a.all()])\n\tb2 = benchFind(b2,[o.id for o in c.optionvalue_b.all()])\n\n\t# Model should be consistent for b1 and b2, so doesn't matter which you pick.\n\tmodel = [b.model.name for b in b1]\n\t\n\t# Get memory values from Benchmark sets.\n\tm1 = [b.memory_VSIZE for b in b1]\n\tm2 = [b.memory_VSIZE for b in b2]\n\t\n\t# Make new arrays with only the elapsed time\n\tt1 = [(float(b.total_time)) for b in b1]\n\tt2 = [(float(b.total_time)) for b in b2]\n\t\n\tlist = zip(model,t1,t2,m1,m2)\n\t\n\t#less then cute.\n\tfrom StringIO import StringIO\n\tio = StringIO()\n\tjson.dumps(list)\n\tscatterjson = io.getvalue()\n\t\n\tform = ExportGraphForm()\n\tresponse = render_to_response('comparisons/compare.html', { 'comparison' : c, 'form' : form, 'list' : list, 'scatterdata' : scatterjson }, context_instance=RequestContext(request))\n\n# Check if the user has rights to see the results:\n#\t- Either the user provided a correct query string like ?auth=<hash>\n#\t- Or the user is the owner of this comparison\nif ((request.GET.__contains__('auth') and request.GET['auth'] == c.hash) or c.user == request.user):\n\treturn response\t\n\n# Otherwise, forbidden to see this page\nelse:\n\treturn HttpResponseForbidden('<h1>You are not authorized to view this page.</h1>')", "path": "beat\\comparisons\\views.py", "repo_name": "ties/BeAT", "stars": 9, "license": "None", "language": "python", "size": 1707}
{"docstring": "\"\"\"\n@require folder to be an existion folder in the filesystem\n@assure if folder contains a git repository it will use that repository. Else it sets the working directory there and makes has_repository false.\nto get a working repository use make_new_repository,switch_repository or clone_repository\n\"\"\"\n", "func_signal": "def __init__(self, folder):\n", "code": "self.folder = folder\ntry:\n\tRepo(self.folder)\nexcept InvalidGitRepositoryError:\n\tchdir(self.folder)\n\thas_repository = False", "path": "beat\\gitinterface.py", "repo_name": "ties/BeAT", "stars": 9, "license": "None", "language": "python", "size": 1707}
{"docstring": "\"\"\"\nMethod that handles clicking of the export graph button. It uses the ExportGraphForm that is defined in forms.py.\nThe format is taken from the POSTed form and used in the method to produce a graph.\n@param id The id of the ModelComparison or Comparison object. This ID is taken from the URL.\n@param model Boolean value to inidicate whether it is a ModelComparison or Comparison object being exported.\n\"\"\"\n", "func_signal": "def export_graph(request, id, model=False):\n", "code": "if request.method == 'POST': # If the form has been submitted...\n\tform = ExportGraphForm(request.POST) # A form bound to the POST data\n\tif form.is_valid(): # All validation rules pass\n\t\tformat = form.cleaned_data['format']\n\t\tif model:\n\t\t\treturn graph_model(request, id, format)\n\t\telse:\n\t\t\treturn scatterplot(request, id, format)\nelse:\n\tform = ExportGraphForm() # An unbound form\n\nreturn render_to_response('comparisons/compare.html', {\n\t'comparison' : Comparison.objects.get(pk=id), 'form': form,\n}, context_instance=RequestContext(request))", "path": "beat\\comparisons\\views.py", "repo_name": "ties/BeAT", "stars": 9, "license": "None", "language": "python", "size": 1707}
{"docstring": "\"\"\"Instantiate the class, passing the text to be formatted.\n    \nHere we pre-process the text and collect all the link\nlookups for later.\n\"\"\"\n", "func_signal": "def __init__(self, text=''):\n", "code": "self.text = text\n\n# Basic regular expressions.\nself.res = res\n\n# Smart searches.\nself.searches = {}\nself.searches['imdb']   = 'http://www.imdb.com/Find?for=%s'\nself.searches['google'] = 'http://www.google.com/search?q=%s'\nself.searches['python'] = 'http://www.python.org/doc/current/lib/module-%s.html'\nif amazon_associate_id:\n    self.searches['isbn']   = ''.join(['http://', AMAZON, '/exec/obidos/ASIN/%s/', amazon_associate_id])\n    self.searches['amazon'] = ''.join(['http://', AMAZON, '/exec/obidos/external-search?mode=blended&keyword=%s&tag=', amazon_associate_id])\nelse:\n    self.searches['isbn']   = ''.join(['http://', AMAZON, '/exec/obidos/ASIN/%s'])\n    self.searches['amazon'] = ''.join(['http://', AMAZON, '/exec/obidos/external-search?mode=blended&keyword=%s'])\n\n# These are the blocks we know.\nself.signatures = [\n                   # Paragraph.\n                   (r'''^p                       # Paragraph signature\n                        %(battr)s                # Paragraph attributes\n                        (?P<dot>\\.)              # .\n                        (?P<extend>\\.)?          # Extended paragraph denoted by a second dot\n                        \\s                       # whitespace\n                        (?P<text>.*)             # text\n                     ''' % self.res, self.paragraph),\n   \n                   # Pre-formatted text.\n                   (r'''^pre                     # Pre signature\n                        %(battr)s                # Pre attributes\n                        (?P<dot>\\.)              # .\n                        (?P<extend>\\.)?          # Extended pre denoted by a second dot\n                        \\s                       # whitespace\n                        (?P<text>.*)             # text\n                     ''' % self.res, self.pre),\n   \n                   # Block code.\n                   (r'''^bc                      # Blockcode signature\n                        %(battr)s                # Blockcode attributes\n                        (?P<dot>\\.)              # .\n                        (?P<extend>\\.)?          # Extended blockcode denoted by a second dot\n                        \\s                       # whitespace\n                        (?P<text>.*)             # text\n                     ''' % self.res, self.bc),\n   \n                   # Blockquote.\n                   (r'''^bq                      # Blockquote signature\n                        %(battr)s                # Blockquote attributes\n                        (?P<dot>\\.)              # .\n                        (?P<extend>\\.)?          # Extended blockquote denoted by a second dot\n                        (:(?P<cite>              # Optional cite attribute\n                        (                        #\n                            %(url)s              #     URL\n                        |   \"[\\w]+(?:\\s[\\w]+)*\"  #     \"Name inside quotes\"\n                        ))                       #\n                        )?                       #\n                        \\s                       # whitespace\n                        (?P<text>.*)             # text\n                     ''' % self.res, self.blockquote),\n   \n                   # Header.\n                   (r'''^h                       # Header signature\n                        (?P<header>\\d)           # Header number\n                        %(battr)s                # Header attributes\n                        (?P<dot>\\.)              # .\n                        (?P<extend>\\.)?          # Extended header denoted by a second dot\n                        \\s                       # whitespace\n                        (?P<text>.*)             # text\n                     ''' % self.res, self.header),\n   \n                   # Footnote.\n                   (r'''^fn                      # Footnote signature\n                        (?P<footnote>[\\d]+)      # Footnote number\n                        (?P<dot>\\.)              # .\n                        (?P<extend>\\.)?          # Extended footnote denoted by a second dot\n                        \\s                       # whitespace\n                        (?P<text>.*)             # text\n                     ''', self.footnote),\n   \n                   # Definition list.\n                   (r'''^dl                      # Definition list signature\n                        %(battr)s                # Definition list attributes\n                        (?P<dot>\\.)              # .\n                        (?P<extend>\\.)?          # Extended definition list denoted by a second dot\n                        \\s                       # whitespace\n                        (?P<text>.*)             # text\n                     ''' % self.res, self.dl),\n                   \n                   # Ordered list (attributes to first <li>).\n                   (r'''^%(olattr)s              # Ordered list attributes\n                        \\#                       # Ordered list signature\n                        %(liattr)s               # List item attributes\n                        (?P<dot>\\.)?             # .\n                        \\s                       # whitespace\n                        (?P<text>.*)             # text\n                     ''' % self.res, self.ol),\n   \n                   # Unordered list (attributes to first <li>).\n                   (r'''^%(olattr)s              # Unrdered list attributes\n                        \\*                       # Unordered list signature\n                        %(liattr)s               # Unordered list attributes\n                        (?P<dot>\\.)?             # .\n                        \\s                       # whitespace\n                        (?P<text>.*)             # text\n                     ''' % self.res, self.ul),\n   \n                   # Escaped text.\n                   (r'''^==?(?P<text>.*?)(==)?$  # Escaped text\n                     ''', self.escape),\n   \n                   (r'''^(?P<text><.*)$          # XHTML tag\n                     ''', self.escape),\n   \n                   # itex code.\n                   (r'''^(?P<text>               # itex code\n                        \\\\\\[                     # starts with \\[\n                        .*?                      # complicated mathematical equations go here\n                        \\\\\\])                    # ends with \\]\n                     ''', self.itex),\n   \n                   # Tables.\n                   (r'''^table                   # Table signature\n                        %(tattr)s                # Table attributes\n                        (?P<dot>\\.)              # .\n                        (?P<extend>\\.)?          # Extended blockcode denoted by a second dot\n                        \\s                       # whitespace\n                        (?P<text>.*)             # text\n                     ''' % self.res, self.table),\n                   \n                   # Simple tables.\n                   (r'''^(?P<text>\n                        \\|\n                        .*)\n                     ''', self.table),\n   \n                   # About.\n                   (r'''^(?P<text>tell\\sme\\sabout\\stextile\\.)$''', self.about),\n                  ]", "path": "textile.py", "repo_name": "ericw/onelinr", "stars": 13, "license": "mit", "language": "python", "size": 299}
{"docstring": "\"\"\"Build an ordered list.\n\nThis function basically just sets the <ol></ol> with the\nright attributes, and then pass everything inside to \n_build_li, which does the real tough recursive job.\n\n---\nh1. Ordered lists\n\nOrdered lists can be constructed this way:\n\npre. # Item number 1.\n# Item number 2.\n# Item number 3.\n\nAnd you get:\n\npre. <ol>\n<li>Item number 1.</li>\n<li>Item number 2.</li>\n<li>Item number 3.</li>\n</ol>\n\nIf you want a list to \"break\" an extended block, you should\nadd a period after the hash. This is useful for writing \nPython code:\n\npre.. bc[python].. #!/usr/bin/env python\n\n# This is a comment, not an ordered list!\n# So this won't break the extended \"bc\".\n\np. Lists can be nested:\n\npre. # Item number 1.\n## Item number 1a.\n## Item number 1b.\n# Item number 2.\n## Item number 2a.\n\nTextile will transform this to:\n\npre. <ol>\n<li>Item number 1.\n<ol>\n<li>Item number 1a.</li>\n<li>Item number 1b.</li>\n</ol>\n</li>\n<li>Item number 2.\n<ol>\n<li>Item number 2a.</li>\n</ol>\n</li>\n</ol>\n\nYou can also mix ordered and unordered lists:\n\npre. * To write well you need:\n*# to read every day\n*# to write every day\n*# and X\n\nYou'll get this:\n\npre. <ul>\n<li>To write well you need:\n<ol>\n<li>to read every day</li>\n<li>to write every day</li>\n<li>and X</li>\n</ol>\n</li>\n</ul>\n\nTo style a list, the parameters should go before the hash if you want\nto set the attributes on the @<ol>@ tag:\n\npre. (class#id)# one\n# two\n# three\n\nIf you want to customize the firsr @<li>@ tag, apply the parameters\nafter the hash:\n\npre. #(class#id) one\n# two\n# three\n\"\"\"\n# Get the attributes.\n", "func_signal": "def ol(self, text, liparameters=None, olparameters=None, clear=None):\n", "code": "olattributes = self.parse_params(olparameters, clear)\nliattributes = self.parse_params(liparameters)\n\n# Remove list depth.\nif text.startswith('#'):\n    text = text[1:]\n\nitems = text.split('\\n#')\n\n# Build the open tag.\nopen_tag = self.build_open_tag('ol', olattributes) + '\\n'\n\nclose_tag = '\\n</ol>'\n\n# Build the list items.\ntext = self.build_li(items, liattributes)\n\nreturn open_tag + text + close_tag", "path": "textile.py", "repo_name": "ericw/onelinr", "stars": 13, "license": "mit", "language": "python", "size": 299}
{"docstring": "\"\"\"Process images.\n\nThis function process images tags, with or without links. Images\ncan have vertical and/or horizontal alignment, and can be resized\nunefficiently using width and height tags.\n\n---\nh1. Images\n\nAn image is generated by enclosing the image source in @!@:\n\npre. !/path/to/image!\n\nYou may optionally specify an alternative text for the image, which\nwill also be used as its title:\n\npre. !image.jpg (Nice picture)!\n\nBecomes:\n\npre. <p><img src=\"image.jpg\" alt=\"Nice picture\" title=\"Nice picture\" /></p>\n\nIf you want to make the image point to a link, simply append a\ncomma and the URL(Universal Republic of Love) to the image:\n\npre. !image.jpg!:http://diveintopython.org\n\nImages can also be resized. These are all equivalent:\n\npre. !image.jpg 10x20!\n!image.jpg 10w 20h!\n!image.jpg 20h 10w!\n\nThe image @image.jpg@ will be resized to width 10 and height 20.\n\nModifiers to the @<img>@ tag go after the opening @!@:\n\npre. !(class#id)^image.jpg!\n\nAllowed modifiers include:\n\ndl. &lt;:Align the image to the left (causes the image to float if CSS options are enabled). \n&gt;:Align the image to the right (causes the image to float if CSS options are enabled). \n- (dash):Aligns the image to the middle. \n^:Aligns the image to the top. \n~ (tilde):Aligns the image to the bottom. \n{style rule}:Applies a CSS style rule to the image. \n(class) or (#id) or (class#id):Applies a CSS class and/or id to the image. \n( (one or more):Pads 1em on the left for each '(' character. \n) (one or more):Pads 1em on the right for each ')' character. \n\nImages receive the class \"top\" when using top alignment, \"bottom\" \nfor bottom alignment and \"middle\" for middle alignment.\n\"\"\"\n# Compile the beast.\n", "func_signal": "def images(self, text):\n", "code": "p = re.compile(r'''\\!               # Opening !\n                   %(iattr)s        # Image attributes\n                   (?P<src>%(url)s) # Image src\n                   \\s?              # Optional whitesapce\n                   (                #\n                       \\(           #\n                       (?P<alt>.*?) # Optional (alt) attribute\n                       \\)           #\n                   )?               #\n                   \\s?              # Optional whitespace\n                   %(resize)s       # Resize parameters\n                   \\!               # Closing !\n                   (                # Optional link\n                       :            #    starts with ':'\n                       (?P<link>    #    \n                       %(url)s      #    link HREF\n                       )            #\n                   )?               #\n                ''' % self.res, re.VERBOSE)\n\nfor m in p.finditer(text):\n    c = m.groupdict('')\n\n    # Build the parameters for the <img /> tag.\n    attributes = self.parse_params(c['parameters'], align_type='image')\n    attributes.update(c)\n    if attributes['alt']:\n        attributes['title'] = attributes['alt']\n\n    # Append height and width.\n    attributes['width'] = m.groups()[5] or m.groups()[7] or m.groups()[10]\n    attributes['height'] = m.groups()[6] or m.groups()[8] or m.groups()[9]\n\n    # Create the image tag.\n    tag = self.image(attributes)\n\n    text = text.replace(m.group(), tag)\n\nreturn text", "path": "textile.py", "repo_name": "ericw/onelinr", "stars": 13, "license": "mit", "language": "python", "size": 299}
{"docstring": "\"\"\"Pre-processing of the text.\n\nRemove whitespace, fix carriage returns.\n\"\"\"\n# Remove whitespace.\n", "func_signal": "def preprocess(self):\n", "code": "self.text = self.text.strip()\n\n# Zap carriage returns.\nself.text = self.text.replace(\"\\r\\n\", \"\\n\")\nself.text = self.text.replace(\"\\r\", \"\\n\")\n\n# Minor sanitizing.\nself.text = self.sanitize(self.text)", "path": "textile.py", "repo_name": "ericw/onelinr", "stars": 13, "license": "mit", "language": "python", "size": 299}
{"docstring": "\"\"\"Process pre-formatted text.\n\nThis function processes pre-formatted text into a <pre> tag.\nNo HTML is added for the lines, but @<@ and @>@ are translated into\nHTML entities.\n\n---\nh1. Pre-formatted text\n\nPre-formatted text can be specified using the @pre@ signature.\nInside a \"pre\" block, whitespace is preserved and @<@ and @>@ are\ntranslated into HTML(HyperText Markup Language) entities\nautomatically.\n\nText in a \"pre\" block is _not processed_ with any inline rule.\n\nHere's a simple example:\n\npre. pre. This text is pre-formatted.\nNothing interesting happens inside here...\n\nWill become:\n\npre. <pre>\nThis text is pre-formatted.\nNothing interesting happens inside here...\n</pre>\n\"\"\"\n\n# Remove trailing whitespace.\n", "func_signal": "def pre(self, text, parameters=None, clear=None):\n", "code": "text = text.rstrip()\n\n# Get the attributes.\nattributes = self.parse_params(parameters, clear)\n\n# Build the tag.\n#open_tag = self.build_open_tag('pre', attributes) + '\\n'\nopen_tag = self.build_open_tag('pre', attributes)\nclose_tag = '\\n</pre>'\n\n# Replace < and >.\ntext = text.replace('<', '&lt;')\ntext = text.replace('>', '&gt;')\n\nreturn open_tag + text + close_tag", "path": "textile.py", "repo_name": "ericw/onelinr", "stars": 13, "license": "mit", "language": "python", "size": 299}
{"docstring": "\"\"\"Process the text.\n\nHere we actually process the text, splitting the text in\nblocks and applying the corresponding function to each\none of them.\n\"\"\"\n# Basic global changes.\n", "func_signal": "def process(self, head_offset=HEAD_OFFSET, validate=VALIDATE, sanitize=SANITIZE, output=OUTPUT, encoding=ENCODING):\n", "code": "self.preprocess()\n\n# Grab lookup links and clean them from the text.\nself._links = self.grab_links()\n\n# Offset for the headers.\nself.head_offset = head_offset\n\n# Process each block.\nself.blocks = self.split_text()\n\ntext = []\nfor [function, captures] in self.blocks:\n    text.append(function(**captures))\n\ntext = '\\n\\n'.join(text)\n\n# Add titles to footnotes.\ntext = self.footnotes(text)\n\n# Convert to desired output.\n#text = unicode(text, encoding)\ntext = text.encode(output, 'xmlcharrefreplace')\n\n# Sanitize?\nif sanitize:\n    p = _HTMLSanitizer()\n    p.feed(text)\n    text = p.output()\n\n# Validate output.\nif _tidy and validate:\n    text = _tidy(text)\n\nreturn text", "path": "textile.py", "repo_name": "ericw/onelinr", "stars": 13, "license": "mit", "language": "python", "size": 299}
{"docstring": "\"\"\"Process block quote.\n\nThe block quote is inserted into a <blockquote> tag, and\nprocessed as a paragraph. An optional cite attribute can\nbe appended on the last line after two dashes (--), or\nafter the period following ':' for compatibility with the\nPerl version.\n\n---\nh1. Blockquote\n\nA blockquote is denoted by the signature @bq@. The text in this\nblock will be enclosed in @<blockquote></blockquote>@ and @<p></p>@,\nreceiving the same formatting as a paragraph. For example:\n\npre. bq. This is a blockquote.\n\nBecomes:\n\npre. <blockquote>\n<p>This is a blockquote.</p>\n</blockquote>\n\nYou can optionally specify the @cite@ attribute of the blockquote,\nusing the following syntax:\n\npre. bq.:http://example.com Some text.\n\npre. bq.:\"John Doe\" Some other text.\n\nBecomes:\n\npre. <blockquote cite=\"http://example.com\">\n<p>Some text.</p>\n</blockquote>\n\npre. <blockquote cite=\"John Doe\">\n<p>Some other text.</p>\n</blockquote>\n\nYou can also specify the @cite@ using a pair of dashes on the\nlast line of the blockquote:\n\npre. bq. Some text.\n-- http://example.com\n\"\"\"\n\n# Get the attributes.\n", "func_signal": "def blockquote(self, text, parameters=None, cite=None, clear=None):\n", "code": "attributes = self.parse_params(parameters, clear)\n\nif cite:\n    # Remove the quotes?\n    cite = cite.strip('\"')\n    attributes['cite'] = cite\nelse:\n    # The citation should be on the last line.\n    text = text.split('\\n')\n    if text[-1].startswith('-- '):\n        attributes['cite'] = text.pop()[3:]    \n\n    text = '\\n'.join(text)\n\n# Build the tag.\nopen_tag = self.build_open_tag('blockquote', attributes) + '\\n'\nclose_tag = '\\n</blockquote>'\n\n# Process the paragraph, passing the attributes.\n# Does it make sense to pass the id, class, etc. to\n# the paragraph instead of applying it to the\n# blockquote tag?\ntext = self.paragraph(text)\n\nreturn open_tag + text + close_tag", "path": "textile.py", "repo_name": "ericw/onelinr", "stars": 13, "license": "mit", "language": "python", "size": 299}
{"docstring": "\"\"\"Process a paragraph.\n\nThis function processes the paragraphs, enclosing the text in a \n<p> tag and breaking lines with <br />. Paragraphs are formatted\nwith all the inline rules.\n\n---\nh1. Paragraph\n\nThis is how you write a paragraph:\n\npre. p. This is a paragraph, although a short one.\n\nSince the paragraph is the default block, you can safely omit its\nsignature ([@p@]). Simply write:\n\npre. This is a paragraph, although a short one.\n\nText in a paragraph block is wrapped in @<p></p>@ tags, and\nnewlines receive a <br /> tag. In both cases Textile will process\nthe text to:\n\npre. <p>This is a paragraph, although a short one.</p>\n\nText in a paragraph block is processed with all the inline rules.\n\"\"\"\n# Split the lines.\n", "func_signal": "def paragraph(self, text, parameters=None, attributes=None, clear=None):\n", "code": "lines = re.split('\\n{2,}', text)\n\n# Get the attributes.\nattributes = attributes or self.parse_params(parameters, clear)\n\noutput = []\nfor line in lines:\n    if line:\n        # Clean the line.\n        line = line.strip()\n         \n        # Build the tag.\n        open_tag = self.build_open_tag('p', attributes)\n        close_tag = '</p>'\n\n        # Pop the id because it must be unique.\n        if attributes.has_key('id'): del attributes['id']\n\n        # Break lines. \n        line = preg_replace(r'(<br />|\\n)+', '<br />\\n', line)\n\n        # Remove <br /> from inside broken HTML tags.\n        line = preg_replace(r'(<[^>]*)<br />\\n(.*?>)', r'\\1 \\2', line)\n\n        # Inline formatting.\n        line = self.inline(line)\n\n        output.append(open_tag + line + close_tag)\n\nreturn '\\n\\n'.join(output)", "path": "textile.py", "repo_name": "ericw/onelinr", "stars": 13, "license": "mit", "language": "python", "size": 299}
{"docstring": "\"\"\"Process the blocks from the text.\n\nSplit the blocks according to the signatures, join extended\nblocks and associate each one of them with a function to\nprocess them.\n\n---\nh1. Blocks\n\nTextile process your text by dividing it in blocks. Each block\nis identified by a signature and separated from other blocks by\nan empty line.\n\nAll signatures should end with a period followed by a space. A\nheader @<h1></h1>@ can be done this way:\n\npre. h1. This is a header 1.\n\nBlocks may continue for multiple paragraphs of text. If you want\na block signature to stay \"active\", use two periods after the\nsignature instead of one. For example:\n\npre.. bq.. This is paragraph one of a block quote.\n\nThis is paragraph two of a block quote.\n\n=p. Now we're back to a regular paragraph.\n\np. Becomes:\n\npre.. <blockquote>\n<p>This is paragraph one of a block quote.</p>\n\n<p>This is paragraph two of a block quote.</p>\n</blockquote>\n\n<p>Now we&#8217;re back to a regular paragraph.</p>\n\np. The blocks can be customised by adding parameters between the\nsignature and the period. These include:\n\ndl. {style rule}:A CSS(Cascading Style Sheets) style rule.\n[ll]:A language identifier (for a \"lang\" attribute).\n(class) or (#id) or (class#id):For CSS(Cascading Style Sheets) class and id attributes.\n&gt;, &lt;, =, &lt;&gt;:Modifier characters for alignment. Right-justification, left-justification, centered, and full-justification. The paragraph will also receive the class names \"right\", \"left\", \"center\" and \"justify\", respectively.\n( (one or more):Adds padding on the left. 1em per \"(\" character is applied. When combined with the align-left or align-right modifier, it makes the block float. \n) (one or more):Adds padding on the right. 1em per \")\" character is applied. When combined with the align-left or align-right modifier, it makes the block float.\n\nHere's an overloaded example:\n\npre. p(())>(class#id)[en]{color:red}. A simple paragraph.\n\nBecomes:\n\npre. <p lang=\"en\" style=\"color:red;padding-left:2em;padding-right:2em;float:right;\" class=\"class right\" id=\"id\">A simple paragraph.</p>\n\"\"\"\n# Clear signature.\n", "func_signal": "def split_text(self):\n", "code": "clear_sig = r'''^clear(?P<alignment>[<>])?\\.$'''\nclear = None\n\nextending  = 0\n\n# We capture the \\n's because they are important inside \"pre..\".\nblocks = re.split(r'''((\\n\\s*){2,})''', self.text)\noutput = []\nfor block in blocks:\n    # Check for the clear signature.\n    m = re.match(clear_sig, block)\n    if m:\n        clear = m.group('alignment')\n        if clear:\n            clear = {'<': 'clear:left;', '>': 'clear:right;'}[clear]\n        else:\n            clear = 'clear:both;'\n\n    else:\n        # Check each of the code signatures.\n        for regexp, function in self.signatures:\n            p = re.compile(regexp, (re.VERBOSE | re.DOTALL))\n            m = p.match(block)\n            if m:\n                # Put everything in a dictionary.\n                captures = m.groupdict()\n\n                # If we are extending a block, we require a dot to\n                # break it, so we can start lines with '#' inside\n                # an extended <pre> without matching an ordered list.\n                if extending and not captures.get('dot', None):\n                    output[-1][1]['text'] += block\n                    break \n                elif captures.has_key('dot'):\n                    del captures['dot']\n                    \n                # If a signature matches, we are not extending a block.\n                extending = 0\n\n                # Check if we should extend this block.\n                if captures.has_key('extend'):\n                    extending = captures['extend']\n                    del captures['extend']\n                    \n                # Apply head_offset.\n                if captures.has_key('header'):\n                    captures['header'] = int(captures['header']) + self.head_offset\n\n                # Apply clear.\n                if clear:\n                    captures['clear'] = clear\n                    clear = None\n\n                # Save the block to be processed later.\n                output.append([function, captures])\n\n                break\n\n        else:\n            if extending:\n                # Append the text to the last block.\n                output[-1][1]['text'] += block\n            elif block.strip():\n                output.append([self.paragraph, {'text': block}])\n    \nreturn output", "path": "textile.py", "repo_name": "ericw/onelinr", "stars": 13, "license": "mit", "language": "python", "size": 299}
{"docstring": "# called for each start tag\n# attrs is a list of (attr, value) tuples\n# e.g. for <pre class=\"screen\">, tag=\"pre\", attrs=[(\"class\", \"screen\")]\n", "func_signal": "def unknown_starttag(self, tag, attrs):\n", "code": "strattrs = \"\".join([' %s=\"%s\"' % (key, value) for key, value in attrs])\nif tag in self.elements_no_end_tag:\n    self.pieces.append(\"<%(tag)s%(strattrs)s />\" % locals())\nelse:\n    self.pieces.append(\"<%(tag)s%(strattrs)s>\" % locals())", "path": "textile.py", "repo_name": "ericw/onelinr", "stars": 13, "license": "mit", "language": "python", "size": 299}
{"docstring": "\"\"\"Process links.\n\nThis function is responsible for processing links. It has\nsome nice shortcuts to Google, Amazon and IMDB queries.\n\n---\nh1. Links\n\nA links is done the following way:\n\npre. \"This is the text link\":http://example.com\n\nThe result from this markup is:\n\npre. <p><a href=\"http://example.com\">This is the text link</a></p>\n\nYou can add an optional @title@ attribute:\n\npre. \"This is the text link(This is the title)\":http://example.com\n\nThe link can be customised as well:\n\npre. \"(nospam)E-mail me please\":mailto:someone@example.com\n\nYou can use either single or double quotes. They must be enclosed in\nwhitespace, punctuation or brackets:\n\npre. You[\"gotta\":http://example.com]seethis!\n\nIf you are going to reference the same link a couple of times, you\ncan define a lookup list anywhere on your document:\n\npre. [python]http://www.python.org\n\nLinks to the Python website can then be defined the following way:\n\npre. \"Check this\":python\n\nThere are also shortcuts for Amazon, IMDB(Internet Movie DataBase) and\nGoogle queries:\n\npre. \"Has anyone seen this guy?\":imdb:Stephen+Fry\n\"Really nice book\":amazon:Goedel+Escher+Bach\n\"PyBlosxom\":google\n[\"Using Textile and Blosxom with Python\":google:python blosxom textile]\n\nBecomes:\n\npre. <a href=\"http://www.imdb.com/Find?for=Stephen+Fry\">Has anyone seen this guy?</a>\n<a href=\"http://www.amazon.com/exec/obidos/external-search?index=blended&amp;keyword=Goedel+Escher+Bach\">Really nice book</a>\n<a href=\"http://www.google.com/search?q=PyBlosxom\">PyBlosxom</a>\n<a href=\"http://www.google.com/search?q=python+blosxom+textile\">Using Textile and Blosxom with Python</a>\n\"\"\"\n", "func_signal": "def links(self, text):\n", "code": "linkres = [r'''\\[                           # [\n               (?P<quote>\"|')               # Opening quotes\n               %(lattr)s                    # Link attributes\n               (?P<text>[^\"]+?)             # Link text\n               \\s?                          # Optional whitespace\n               (?:\\((?P<title>[^\\)]+?)\\))?  # Optional (title)\n               (?P=quote)                   # Closing quotes\n               :                            # :\n               (?P<href>[^\\]]+)             # HREF\n               \\]                           # ]\n            ''' % self.res,\n           r'''(?P<quote>\"|')               # Opening quotes\n               %(lattr)s                    # Link attributes\n               (?P<text>[^\"]+?)             # Link text\n               \\s?                          # Optional whitespace\n               (?:\\((?P<title>[^\\)]+?)\\))?  # Optional (title)\n               (?P=quote)                   # Closing quotes\n               :                            # :\n               (?P<href>%(url)s)            # HREF\n            ''' % self.res]\n\nfor linkre in linkres:\n    p = re.compile(linkre, re.VERBOSE)\n    for m in p.finditer(text):\n        c = m.groupdict('')\n\n        attributes = self.parse_params(c['parameters'])\n        attributes['title'] = c['title'].replace('\"', '&quot;')\n\n        # Search lookup list.\n        link = self._links.get(c['href'], None) or c['href']\n\n        # Hyperlinks for Amazon, IMDB and Google searches.\n        parts = link.split(':', 1)\n        proto = parts[0]\n        if len(parts) == 2:\n            query = parts[1]\n        else:\n            query = c['text']\n\n        query = query.replace(' ', '+')\n\n        # Look for smart search.\n        if self.searches.has_key(proto):\n            link = self.searches[proto] % query\n        \n        # Fix URL.\n        attributes['href'] = preg_replace('&(?!(#|amp))', '&amp;', link)\n\n        open_tag = self.build_open_tag('a', attributes)\n        close_tag = '</a>'\n\n        repl = open_tag + c['text'] + close_tag\n\n        text = text.replace(m.group(), repl)\n\nreturn text", "path": "textile.py", "repo_name": "ericw/onelinr", "stars": 13, "license": "mit", "language": "python", "size": 299}
{"docstring": "\"\"\"Fix single tags.\n\nFix tags like <img />, <br /> and <hr />.\n\n---\nh1. Sanitizing\n\nTextile can help you generate valid XHTML(eXtensible HyperText Markup Language).\nIt will fix any single tags that are not properly closed, like\n@<img />@, @<br />@ and @<hr />@.\n\nIf you have \"mx.Tidy\":http://www.egenix.com/files/python/mxTidy.html\nand/or \"&micro;TidyLib\":http://utidylib.sourceforge.net/ installed,\nit also can optionally validade the generated code with these wrappers\nto ensure 100% valid XHTML(eXtensible HyperText Markup Language).\n\"\"\"\n# Fix single tags like <img /> and <br />.\n", "func_signal": "def sanitize(self, text):\n", "code": "text = preg_replace(r'''<(img|br|hr)(.*?)(?:\\s*/?\\s*)?>''', r'''<\\1\\2 />''', text)\n\n# Remove ampersands.\ntext = preg_replace(r'''&(?!#?[xX]?(?:[0-9a-fA-F]+|\\w{1,8});)''', r'''&amp;''', text)\n\nreturn text", "path": "textile.py", "repo_name": "ericw/onelinr", "stars": 13, "license": "mit", "language": "python", "size": 299}
{"docstring": "\"\"\"Quick tags formatting.\n\nThis function does the inline formatting of text, like\nbold, italic, strong and also itex code.\n\n---\nh1. Quick tags\n\nQuick tags allow you to format your text, making it bold, \nemphasized or small, for example. The quick tags operators\ninclude:\n\ndl. ==*strong*==:Translates into @<strong>strong</strong>@.\n==_emphasis_==:Translates into @<em>emphasis</em>@. \n==**bold**==:Translates into @<b>bold</b>@. \n==__italics__==:Translates into @<i>italics</i>@. \n==++bigger++==:Translates into @<big>bigger</big>@. \n==--smaller--==:Translates into: @<small>smaller</small>@. \n==-deleted text-==:Translates into @<del>deleted text</del>@. \n==+inserted text+==:Translates into @<ins>inserted text</ins>@. \n==^superscript^==:Translates into @<sup>superscript</sup>@. \n==~subscript~==:Translates into @<sub>subscript</sub>@. \n==%span%==:Translates into @<span>span</span>@. \n==@code@==:Translates into @<code>code</code>@. \n\nNote that within a \"==@==...==@==\" section, @<@ and @>@ are\ntranslated into HTML entities automatically. \n\nInline formatting operators accept the following modifiers:\n\ndl. {style rule}:A CSS(Cascading Style Sheets) style rule. \n[ll]:A language identifier (for a \"lang\" attribute). \n(class) or (#id) or (class#id):For CSS(Cascading Style Sheets) class and id attributes. \n\"\"\"\n# itex2mml.\n", "func_signal": "def qtags(self, text):\n", "code": "text = re.sub('\\$(.*?)\\$', lambda m: self.itex(m.group()), text)\n\n# Add span tags to upper-case words which don't have a description.\n#text = preg_replace(r'''(^|\\s)([A-Z]{3,})\\b(?!\\()''', r'''\\1<span class=\"caps\">\\2</span>''', text)\n\n# Quick tags.\nqtags = [('**', 'b',      {'qf': '(?<!\\*)\\*\\*(?!\\*)', 'cls': '\\*'}),\n         ('__', 'i',      {'qf': '(?<!_)__(?!_)', 'cls': '_'}),\n         ('??', 'cite',   {'qf': '\\?\\?(?!\\?)', 'cls': '\\?'}),\n         ('-',  'del',    {'qf': '(?<!\\-)\\-(?!\\-)', 'cls': '-'}),\n         ('+',  'ins',    {'qf': '(?<!\\+)\\+(?!\\+)', 'cls': '\\+'}),\n         ('*',  'strong', {'qf': '(?<!\\*)\\*(?!\\*)', 'cls': '\\*'}),\n         ('_',  'em',     {'qf': '(?<!_)_(?!_)', 'cls': '_'}),\n         ('++', 'big',    {'qf': '(?<!\\+)\\+\\+(?!\\+)', 'cls': '\\+\\+'}),\n         ('--', 'small',  {'qf': '(?<!\\-)\\-\\-(?!\\-)', 'cls': '\\-\\-'}),\n         ('~',  'sub',    {'qf': '(?<!\\~)\\~(?!(\\\\\\/~))', 'cls': '\\~'}),\n         ('@',  'code',   {'qf': '(?<!@)@(?!@)', 'cls': '@'}),\n         ('%',  'span',   {'qf': '(?<!%)%(?!%)', 'cls': '%'}),\n        ]\n\n# Superscript.\ntext = re.sub(r'''(?<!\\^)\\^(?!\\^)(.+?)(?<!\\^)\\^(?!\\^)''', r'''<sup>\\1</sup>''', text)\n\n# This is from the perl version of Textile.\nfor qtag, htmltag, redict in qtags:\n    self.res.update(redict)\n    p = re.compile(r'''(?:                          #\n                           ^                        # Start of string\n                           |                        #\n                           (?<=[\\s>'\"])             # Whitespace, end of tag, quotes\n                           |                        #\n                           (?P<pre>[{[])            # Surrounded by [ or {\n                           |                        #\n                           (?<=%(punct)s)           # Punctuation\n                       )                            #\n                       %(qf)s                       # opening tag\n                       %(qattr)s                    # attributes\n                       (?P<text>[^%(cls)s\\s].*?)    # text\n                       (?<=\\S)                      # non-whitespace\n                       %(qf)s                       # \n                       (?:                          #\n                           $                        # End of string\n                           |                        #\n                           (?P<post>[\\]}])          # Surrounded by ] or }\n                           |                        # \n                           (?=%(punct)s{1,2}|\\s)    # punctuation\n                        )                           #\n                     ''' % self.res, re.VERBOSE)\n\n    def _replace(m):\n        c = m.groupdict('')\n\n        attributes = self.parse_params(c['parameters'])\n        open_tag  = self.build_open_tag(htmltag, attributes) \n        close_tag = '</%s>' % htmltag\n\n        # Replace < and > inside <code></code>.\n        if htmltag == 'code':\n            c['text'] = c['text'].replace('<', '&lt;')\n            c['text'] = c['text'].replace('>', '&gt;')\n \n        return open_tag + c['text'] + close_tag\n\n    text = p.sub(_replace, text)\n\nreturn text", "path": "textile.py", "repo_name": "ericw/onelinr", "stars": 13, "license": "mit", "language": "python", "size": 299}
{"docstring": "\"\"\"Process block code.\n\nThis function processes block code into a <code> tag inside a\n<pre>. No HTML is added for the lines, but @<@ and @>@ are translated\ninto HTML entities.\n\n---\nh1. Block code\n\nA block code, specified by the @bc@ signature, is a block of\npre-formatted text which also receives a @<code></code>@ tag. As\nwith \"pre\", whitespace is preserved and @<@ and @>@ are translated\ninto HTML(HyperText Markup Language) entities automatically.\n\nText in a \"bc\" code is _not processed_ with the inline rules.\n\nIf you have \"Twisted\":http://www.twistedmatrix.com/ installed,\nTextile can automatically colorize your Python code if you\nspecify its language as \"Python\":\n\npre. bc[python]. from twisted.python import htmlizer\n\nThis will become:\n\npre. <pre>\n<code lang=\"python\">\n<span class=\"py-src-keyword\">from</span> <span class=\"py-src-variable\">twisted</span><span class=\"py-src-op\">.</span><span class=\"py-src-variable\">python</span> <span class=\"py-src-keyword\">import</span> <span class=\"py-src-variable\">htmlizer</span>\n</code>\n</pre>\n\nThe colors can be specified in your CSS(Cascading Style Sheets)\nfile. If you don't want to install Twisted, you can download just\nthe @htmlizer@ module \"independently\":http://dealmeida.net/code/htmlizer.py.txt.\n\"\"\"\n\n# Get the attributes.\n", "func_signal": "def bc(self, text, parameters=None, clear=None):\n", "code": "attributes = self.parse_params(parameters, clear)\n\n# XHTML <code> can't have the attribute lang.\nif attributes.has_key('lang'):\n    lang = attributes['lang']\n    del attributes['lang']\nelse:\n    lang = None\n\n# Build the tag.\nopen_tag = '<pre>\\n' + self.build_open_tag('code', attributes) + '\\n'\nclose_tag = '\\n</code>\\n</pre>'\n\n# Colorize Python code?\nif htmlizer and lang == 'python':\n    text = _color(text)\nelse:\n    # Replace < and >.\n    text = text.replace('<', '&lt;')\n    text = text.replace('>', '&gt;')\n\nreturn open_tag + text + close_tag", "path": "textile.py", "repo_name": "ericw/onelinr", "stars": 13, "license": "mit", "language": "python", "size": 299}
{"docstring": "\"\"\"Build the list item.\n\nThis function build the list item of an (un)ordered list. It\nworks by peeking at the next list item, and searching for a\nmulti-list. If a multi-list is found, it is processed and \nappended inside the list item tags, as it should be.\n\"\"\"\n", "func_signal": "def build_li(self, items, liattributes):\n", "code": "lines = []\nwhile len(items):\n    item = items.pop(0)\n\n    # Clean the line.\n    item = item.lstrip()\n    item = item.replace('\\n', '<br />\\n')\n\n    # Get list item attributes.\n    p = re.compile(r'''^%(liattr)s\\s''' % self.res, re.VERBOSE)\n    m = p.match(item)\n    if m:\n        c = m.groupdict('')\n        liparameters = c['liparameters']\n        item = p.sub('', item)\n    else:\n        liparameters = ''\n\n    liattributes = liattributes or self.parse_params(liparameters)\n    \n    # Build the item tag.\n    open_tag_li = self.build_open_tag('li', liattributes) \n\n    # Reset the attributes, which should be applied\n    # only to the first <li>.\n    liattributes = {}\n\n    # Build the closing tag.\n    close_tag_li = '</li>'\n\n    # Multi-list recursive routine.\n    # Here we check the _next_ items for a multi-list. If we\n    # find one, we extract all items of the multi-list and\n    # process them recursively.\n    if len(items):\n        inlist = []\n\n        # Grab all the items that start with # or *.\n        n_item = items.pop(0)\n\n        # Grab the <ol> parameters.\n        p = re.compile(r'''^%(olattr)s''' % self.res, re.VERBOSE)\n        m = p.match(n_item)\n        if m:\n            c = m.groupdict('')\n            olparameters = c['olparameters']\n            tmp = p.sub('', n_item)\n        else:\n            olparameters = ''\n\n        # Check for an ordered list inside this one.\n        if tmp.startswith('#'):\n            n_item = tmp\n            inlist.append(n_item)\n            while len(items):\n                # Peek into the next item.\n                n_item = items.pop(0)\n                if n_item.startswith('#'):\n                    inlist.append(n_item)\n                else:\n                    items.insert(0, n_item)\n                    break\n                \n            inlist = self.ol('\\n'.join(inlist), olparameters=olparameters)\n            item = item + '\\n' + inlist + '\\n'\n\n        # Check for an unordered list inside this one.\n        elif tmp.startswith('*'):\n            n_item = tmp\n            inlist.append(n_item)\n            while len(items):\n                # Peek into the next item.\n                n_item = items.pop(0)\n                if n_item.startswith('*'):\n                    inlist.append(n_item)\n                else:\n                    items.insert(0, n_item)\n                    break\n\n            inlist = self.ul('\\n'.join(inlist), olparameters=olparameters)\n            item = item + '\\n' + inlist + '\\n'\n\n        # Otherwise we just put it back in the list.\n        else:\n            items.insert(0, n_item)\n\n    item = self.inline(item)\n\n    item = open_tag_li + item + close_tag_li\n    lines.append(item)\n\nreturn '\\n'.join(lines)", "path": "textile.py", "repo_name": "ericw/onelinr", "stars": 13, "license": "mit", "language": "python", "size": 299}
{"docstring": "\"\"\"Quick macros.\n\nThis function replaces macros inside brackets using a built-in\ndictionary, and also unicode names if the key doesn't exist.\n\n---\nh1. Macros\n\nTextile has support for character macros, which should be enclosed\nin curly braces. A few useful ones are:\n\npre. {C=} or {=C}: euro sign\n{+-} or {-+}: plus-minus sign\n{L-} or {-L}: pound sign.\n\nYou can also make accented characters:\n\npre. Expos{e'}\n\nBecomes:\n\npre. <p>Expos&amp;#233;</p>\n\nYou can also specify Unicode names like:\n\npre. {umbrella}\n{white smiling face}\n\"\"\"\n", "func_signal": "def macros(self, m):\n", "code": "entity = m.group(1)\n\nmacros = {'c|': '&#162;',       # cent sign\n          '|c': '&#162;',       # cent sign\n          'L-': '&#163;',       # pound sign\n          '-L': '&#163;',       # pound sign\n          'Y=': '&#165;',       # yen sign\n          '=Y': '&#165;',       # yen sign\n          '(c)': '&#169;',      # copyright sign\n          '<<': '&#171;',       # left-pointing double angle quotation\n          '(r)': '&#174;',      # registered sign\n          '+_': '&#177;',       # plus-minus sign\n          '_+': '&#177;',       # plus-minus sign\n          '>>': '&#187;',       # right-pointing double angle quotation\n          '1/4': '&#188;',      # vulgar fraction one quarter\n          '1/2': '&#189;',      # vulgar fraction one half\n          '3/4': '&#190;',      # vulgar fraction three quarters\n          'A`': '&#192;',       # latin capital letter a with grave\n          '`A': '&#192;',       # latin capital letter a with grave\n          'A\\'': '&#193;',      # latin capital letter a with acute\n          '\\'A': '&#193;',      # latin capital letter a with acute\n          'A^': '&#194;',       # latin capital letter a with circumflex\n          '^A': '&#194;',       # latin capital letter a with circumflex\n          'A~': '&#195;',       # latin capital letter a with tilde\n          '~A': '&#195;',       # latin capital letter a with tilde\n          'A\"': '&#196;',       # latin capital letter a with diaeresis\n          '\"A': '&#196;',       # latin capital letter a with diaeresis\n          'Ao': '&#197;',       # latin capital letter a with ring above\n          'oA': '&#197;',       # latin capital letter a with ring above\n          'AE': '&#198;',       # latin capital letter ae\n          'C,': '&#199;',       # latin capital letter c with cedilla\n          ',C': '&#199;',       # latin capital letter c with cedilla\n          'E`': '&#200;',       # latin capital letter e with grave\n          '`E': '&#200;',       # latin capital letter e with grave\n          'E\\'': '&#201;',      # latin capital letter e with acute\n          '\\'E': '&#201;',      # latin capital letter e with acute\n          'E^': '&#202;',       # latin capital letter e with circumflex\n          '^E': '&#202;',       # latin capital letter e with circumflex\n          'E\"': '&#203;',       # latin capital letter e with diaeresis\n          '\"E': '&#203;',       # latin capital letter e with diaeresis\n          'I`': '&#204;',       # latin capital letter i with grave\n          '`I': '&#204;',       # latin capital letter i with grave\n          'I\\'': '&#205;',      # latin capital letter i with acute\n          '\\'I': '&#205;',      # latin capital letter i with acute\n          'I^': '&#206;',       # latin capital letter i with circumflex\n          '^I': '&#206;',       # latin capital letter i with circumflex\n          'I\"': '&#207;',       # latin capital letter i with diaeresis\n          '\"I': '&#207;',       # latin capital letter i with diaeresis\n          'D-': '&#208;',       # latin capital letter eth\n          '-D': '&#208;',       # latin capital letter eth\n          'N~': '&#209;',       # latin capital letter n with tilde\n          '~N': '&#209;',       # latin capital letter n with tilde\n          'O`': '&#210;',       # latin capital letter o with grave\n          '`O': '&#210;',       # latin capital letter o with grave\n          'O\\'': '&#211;',      # latin capital letter o with acute\n          '\\'O': '&#211;',      # latin capital letter o with acute\n          'O^': '&#212;',       # latin capital letter o with circumflex\n          '^O': '&#212;',       # latin capital letter o with circumflex\n          'O~': '&#213;',       # latin capital letter o with tilde\n          '~O': '&#213;',       # latin capital letter o with tilde\n          'O\"': '&#214;',       # latin capital letter o with diaeresis\n          '\"O': '&#214;',       # latin capital letter o with diaeresis\n          'O/': '&#216;',       # latin capital letter o with stroke\n          '/O': '&#216;',       # latin capital letter o with stroke\n          'U`':  '&#217;',      # latin capital letter u with grave\n          '`U':  '&#217;',      # latin capital letter u with grave\n          'U\\'': '&#218;',      # latin capital letter u with acute\n          '\\'U': '&#218;',      # latin capital letter u with acute\n          'U^': '&#219;',       # latin capital letter u with circumflex\n          '^U': '&#219;',       # latin capital letter u with circumflex\n          'U\"': '&#220;',       # latin capital letter u with diaeresis\n          '\"U': '&#220;',       # latin capital letter u with diaeresis\n          'Y\\'': '&#221;',      # latin capital letter y with acute\n          '\\'Y': '&#221;',      # latin capital letter y with acute\n          'a`': '&#224;',       # latin small letter a with grave\n          '`a': '&#224;',       # latin small letter a with grave\n          'a\\'': '&#225;',      # latin small letter a with acute\n          '\\'a': '&#225;',      # latin small letter a with acute\n          'a^': '&#226;',       # latin small letter a with circumflex\n          '^a': '&#226;',       # latin small letter a with circumflex\n          'a~': '&#227;',       # latin small letter a with tilde\n          '~a': '&#227;',       # latin small letter a with tilde\n          'a\"': '&#228;',       # latin small letter a with diaeresis\n          '\"a': '&#228;',       # latin small letter a with diaeresis\n          'ao': '&#229;',       # latin small letter a with ring above\n          'oa': '&#229;',       # latin small letter a with ring above\n          'ae': '&#230;',       # latin small letter ae\n          'c,': '&#231;',       # latin small letter c with cedilla\n          ',c': '&#231;',       # latin small letter c with cedilla\n          'e`': '&#232;',       # latin small letter e with grave\n          '`e': '&#232;',       # latin small letter e with grave\n          'e\\'': '&#233;',      # latin small letter e with acute\n          '\\'e': '&#233;',      # latin small letter e with acute\n          'e^': '&#234;',       # latin small letter e with circumflex\n          '^e': '&#234;',       # latin small letter e with circumflex\n          'e\"': '&#235;',       # latin small letter e with diaeresis\n          '\"e': '&#235;',       # latin small letter e with diaeresis\n          'i`': '&#236;',       # latin small letter i with grave\n          '`i': '&#236;',       # latin small letter i with grave\n          'i\\'': '&#237;',      # latin small letter i with acute\n          '\\'i': '&#237;',      # latin small letter i with acute\n          'i^': '&#238;',       # latin small letter i with circumflex\n          '^i': '&#238;',       # latin small letter i with circumflex\n          'i\"': '&#239;',       # latin small letter i with diaeresis\n          '\"i': '&#239;',       # latin small letter i with diaeresis\n          'n~': '&#241;',       # latin small letter n with tilde\n          '~n': '&#241;',       # latin small letter n with tilde\n          'o`': '&#242;',       # latin small letter o with grave\n          '`o': '&#242;',       # latin small letter o with grave\n          'o\\'': '&#243;',      # latin small letter o with acute\n          '\\'o': '&#243;',      # latin small letter o with acute\n          'o^': '&#244;',       # latin small letter o with circumflex\n          '^o': '&#244;',       # latin small letter o with circumflex\n          'o~': '&#245;',       # latin small letter o with tilde\n          '~o': '&#245;',       # latin small letter o with tilde\n          'o\"': '&#246;',       # latin small letter o with diaeresis\n          '\"o': '&#246;',       # latin small letter o with diaeresis\n          ':-': '&#247;',       # division sign\n          '-:': '&#247;',       # division sign\n          'o/': '&#248;',       # latin small letter o with stroke\n          '/o': '&#248;',       # latin small letter o with stroke\n          'u`': '&#249;',       # latin small letter u with grave\n          '`u': '&#249;',       # latin small letter u with grave\n          'u\\'': '&#250;',      # latin small letter u with acute\n          '\\'u': '&#250;',      # latin small letter u with acute\n          'u^': '&#251;',       # latin small letter u with circumflex\n          '^u': '&#251;',       # latin small letter u with circumflex\n          'u\"': '&#252;',       # latin small letter u with diaeresis\n          '\"u': '&#252;',       # latin small letter u with diaeresis\n          'y\\'': '&#253;',      # latin small letter y with acute\n          '\\'y': '&#253;',      # latin small letter y with acute\n          'y\"': '&#255',        # latin small letter y with diaeresis\n          '\"y': '&#255',        # latin small letter y with diaeresis\n          'OE': '&#338;',       # latin capital ligature oe\n          'oe': '&#339;',       # latin small ligature oe\n          '*': '&#8226;',       # bullet\n          'Fr': '&#8355;',      # french franc sign\n          'L=': '&#8356;',      # lira sign\n          '=L': '&#8356;',      # lira sign\n          'Rs': '&#8360;',      # rupee sign\n          'C=': '&#8364;',      # euro sign\n          '=C': '&#8364;',      # euro sign\n          'tm': '&#8482;',      # trade mark sign\n          '<-': '&#8592;',      # leftwards arrow\n          '->': '&#8594;',      # rightwards arrow\n          '<=': '&#8656;',      # leftwards double arrow\n          '=>': '&#8658;',      # rightwards double arrow\n          '=/': '&#8800;',      # not equal to\n          '/=': '&#8800;',      # not equal to\n          '<_': '&#8804;',      # less-than or equal to\n          '_<': '&#8804;',      # less-than or equal to\n          '>_': '&#8805;',      # greater-than or equal to\n          '_>': '&#8805;',      # greater-than or equal to\n          ':(': '&#9785;',      # white frowning face\n          ':)': '&#9786;',      # white smiling face\n          'spade': '&#9824;',   # black spade suit\n          'club': '&#9827;',    # black club suit\n          'heart': '&#9829;',   # black heart suit\n          'diamond': '&#9830;', # black diamond suit\n         }\n\ntry:\n    # Try the key.\n    entity = macros[entity]\nexcept KeyError:\n    try:\n        # Try a unicode entity.\n        entity = unicodedata.lookup(entity)\n        entity = entity.encode('ascii', 'xmlcharrefreplace')\n    except:\n        # Return the unmodified entity.\n        entity = '{%s}' % entity\n\nreturn entity", "path": "textile.py", "repo_name": "ericw/onelinr", "stars": 13, "license": "mit", "language": "python", "size": 299}
{"docstring": "\"\"\"Process each image.\n\nThis method builds the <img> tag for each image in the text. It's\nseparated from the 'images' method so it can be easily overriden when\nsubclassing Textiler. Useful if you want to download and/or process\nthe images, for example.\n\"\"\"\n", "func_signal": "def image(self, attributes):\n", "code": "link = attributes['link']\ndel attributes['link']\ndel attributes['parameters']\n\n# Build the tag.\ntag = self.build_open_tag('img', attributes, single=1)\n\nif link:\n    href = preg_replace('&(?!(#|amp))', '&amp;', link)\n    tag = '<a href=\"%s\">%s</a>' % (href, tag)\n\nreturn tag", "path": "textile.py", "repo_name": "ericw/onelinr", "stars": 13, "license": "mit", "language": "python", "size": 299}
{"docstring": "\"\"\"Process acronyms.\n\nAcronyms can have letters in upper and lower caps, or even numbers,\nprovided that the numbers and upper caps are the same in the\nabbreviation and in the description. For example:\n\n    XHTML(eXtensible HyperText Markup Language)\n    OPeNDAP(Open source Project for a Network Data Access Protocol)\n    L94(Levitus 94)\n\nare all valid acronyms.\n\n---\nh1. Acronyms\n\nYou can define acronyms in your text the following way:\n\npre. This is XHTML(eXtensible HyperText Markup Language).\n\nThe resulting code is:\n\npre. <p><acronym title=\"eXtensible HyperText Markup Language\"><span class=\"caps\">XHTML</span></acronym></p>\n\nAcronyms can have letters in upper and lower caps, or even numbers,\nprovided that the numbers and upper caps are the same in the\nabbreviation and in the description. For example:\n\npre. XHTML(eXtensible HyperText Markup Language)\nOPeNDAP(Open source Project for a Network Data Access Protocol)\nL94(Levitus 94)\n\nare all valid acronyms.\n\"\"\"\n# Find the acronyms.\n", "func_signal": "def acronym(self, text):\n", "code": "acronyms = r'''(?P<acronym>[\\w]+)\\((?P<definition>[^\\(\\)]+?)\\)'''\n\n# Check all acronyms.\nfor acronym, definition in re.findall(acronyms, text):\n    caps_acronym = ''.join(re.findall('[A-Z\\d]+', acronym))\n    caps_definition = ''.join(re.findall('[A-Z\\d]+', definition))\n    if caps_acronym and caps_acronym == caps_definition:\n        text = text.replace('%s(%s)' % (acronym, definition), '<acronym title=\"%s\">%s</acronym>' % (definition, acronym))\n\ntext = html_replace(r'''(^|\\s)([A-Z]{3,})\\b(?!\\()''', r'''\\1<span class=\"caps\">\\2</span>''', text)\n\nreturn text", "path": "textile.py", "repo_name": "ericw/onelinr", "stars": 13, "license": "mit", "language": "python", "size": 299}
{"docstring": "\"\"\"Parse the parameters from a block signature.\n\nThis function parses the parameters from a block signature,\nsplitting the information about class, id, language and\nstyle. The positioning (indentation and alignment) is parsed\nand stored in the style.\n\nA paragraph like:\n\n    p>(class#id){color:red}[en]. Paragraph.\n\nor:\n    \n    p{color:red}[en](class#id)>. Paragraph.\n\nwill have its parameters parsed to:\n\n    output = {'lang' : 'en',\n              'class': 'class',\n              'id'   : 'id',\n              'style': 'color:red;text-align:right;'}\n\nNote that order is not important.\n\"\"\"\n", "func_signal": "def parse_params(self, parameters, clear=None, align_type='block'):\n", "code": "if not parameters:\n    if clear:\n        return {'style': clear}\n    else:\n        return {}\n\noutput = {}\n\n# Match class from (class) or (class#id).\nm = re.search(r'''\\((?P<class>[\\w]+(\\s[\\w]+)*)(\\#[\\w]+)?\\)''', parameters)\nif m: output['class'] = m.group('class')\n\n# Match id from (#id) or (class#id).\nm = re.search(r'''\\([\\w]*(\\s[\\w]+)*\\#(?P<id>[\\w]+)\\)''', parameters)\nif m: output['id'] = m.group('id')\n\n# Match [language].\nm = re.search(r'''\\[(?P<lang>[\\w-]+)\\]''', parameters)\nif m: output['lang'] = m.group('lang')\n\n# Match {style}.\nm = re.search(r'''{(?P<style>[^\\}]+)}''', parameters)\nif m:\n    output['style'] = m.group('style').replace('\\n', '')\n\n    # If necessary, apppend a semi-comma to the style.\n    if not output['style'].endswith(';'):\n        output['style'] += ';'\n\n# Clear the block?\nif clear:\n    output['style'] = output.get('style', '') + clear\n\n# Remove classes, ids, langs and styles. This makes the \n# regular expression for the positioning much easier.\nparameters = preg_replace(r'''\\([\\#\\w\\s]+\\)''', '', parameters)\nparameters = preg_replace(r'''\\[[\\w-]+\\]''', '', parameters)\nparameters = preg_replace(r'''{[\\w:;#%-]+}''', '', parameters)\n\nstyle = []\n\n# Count the left indentation.\nl_indent = parameters.count('(')\nif l_indent: style.append('padding-left:%dem;' % l_indent)\n\n# Count the right indentation.\nr_indent = parameters.count(')')\nif r_indent: style.append('padding-right:%dem;' % r_indent)\n\n# Add alignment.\nif align_type == 'image':\n    align = [('<', 'float:left;', ' left'),\n             ('>', 'float:right;', ' right')]\n\n    valign = [('^', 'vertical-align:text-top;', ' top'),\n              ('-', 'vertical-align:middle;', ' middle'),\n              ('~', 'vertical-align:text-bottom;', ' bottom')]\n\n    # Images can have both a vertical and a horizontal alignment.\n    for alignments in [align, valign]:\n        for _align, _style, _class in alignments:\n            if parameters.count(_align):\n                style.append(_style)\n                \n                # Append a class name related to the alignment.\n                output['class'] = output.get('class', '') + _class\n                break\n\nelif align_type == 'table':\n    align = [('<', 'left'),\n             ('>', 'right'),\n             ('=', 'center'),\n             ('<>', 'justify')]\n\n    valign = [('^', 'top'),\n              ('~', 'bottom')]\n\n    # Horizontal alignment.\n    for _align, _style, in align:\n        if parameters.count(_align):\n            output['align'] = _style\n    \n    # Vertical alignment.\n    for _align, _style, in valign:\n        if parameters.count(_align):\n            output['valign'] = _style\n\n    # Colspan and rowspan.\n    m = re.search(r'''\\\\(\\d+)''', parameters)\n    if m:\n        #output['colspan'] = m.groups()\n        output['colspan'] = int(m.groups()[0])\n\n    m = re.search(r'''/(\\d+)''', parameters)\n    if m:\n        output['rowspan'] = int(m.groups()[0])\n\nelse:\n    if l_indent or r_indent:\n        alignments = [('<>', 'text-align:justify;', ' justify'),\n                      ('=', 'text-align:center;', ' center'),\n                      ('<', 'float:left;', ' left'),\n                      ('>', 'float:right;', ' right')]\n    else:\n        alignments = [('<>', 'text-align:justify;', ' justify'),\n                      ('=', 'text-align:center;', ' center'),\n                      ('<', 'text-align:left;', ' left'),\n                      ('>', 'text-align:right;', ' right')]\n\n    for _align, _style, _class in alignments:\n        if parameters.count(_align):\n            style.append(_style)\n\n            # Append a class name related to the alignment.\n            output['class'] = output.get('class', '') + _class\n            break\n\n# Join all the styles.\noutput['style'] = output.get('style', '') + ''.join(style)\n\n# Remove excess whitespace.\nif output.has_key('class'):\n    output['class'] = output['class'].strip()\n\nreturn output", "path": "textile.py", "repo_name": "ericw/onelinr", "stars": 13, "license": "mit", "language": "python", "size": 299}
{"docstring": "\"\"\"Build an unordered list.\n\nThis function basically just sets the <ul></ul> with the\nright attributes, and then pass everything inside to \n_build_li, which does the real tough recursive job.\n\n---\nh1. Unordered lists\n\nUnordered lists behave exactly like the ordered lists, and are\ndefined using a star:\n\npre. * Python\n* Perl\n* PHP\n\nBecomes:\n\npre. <ul>\n<li>Python</li>\n<li>Perl</li>\n<li><span class=\"caps\">PHP</span></li>\n</ul>\n\"\"\"\n# Get the attributes.\n", "func_signal": "def ul(self, text, liparameters=None, olparameters=None, clear=None):\n", "code": "olattributes = self.parse_params(olparameters, clear)\nliattributes = self.parse_params(liparameters)\n\n# Remove list depth.\nif text.startswith('*'):\n    text = text[1:]\n\nitems = text.split('\\n*')\n\n# Build the open tag.\nopen_tag = self.build_open_tag('ul', olattributes) + '\\n'\n\nclose_tag = '\\n</ul>'\n\n# Build the list items.\ntext = self.build_li(items, liattributes)\n\nreturn open_tag + text + close_tag", "path": "textile.py", "repo_name": "ericw/onelinr", "stars": 13, "license": "mit", "language": "python", "size": 299}
{"docstring": "\"\"\"Return a list of all track objects whose start or end (start +\nduration) are exactly equal to a given time\"\"\"\n", "func_signal": "def getObjsIncidentOnTime(self, time):\n", "code": "if time in self.by_time:\n    return self.by_time[time]\nreturn []", "path": "pitivi\\timeline\\timeline.py", "repo_name": "alessandrod/pitivi", "stars": 12, "license": "other", "language": "python", "size": 6340}
{"docstring": "\"\"\"Return a list of all track objects whose ends (start + duration)\nare equal to the given track object's start\"\"\"\n", "func_signal": "def getObjsAdjacentToStart(self, trackobj):\n", "code": "if trackobj.start in self.by_end:\n    return self.by_end[trackobj.start]\nreturn []", "path": "pitivi\\timeline\\timeline.py", "repo_name": "alessandrod/pitivi", "stars": 12, "license": "other", "language": "python", "size": 6340}
{"docstring": "\"\"\"\nRemove this object's start/stop values from the edges.\n\n@param timeline_object: The object whose start/stop we no longer want\nto track.\n@type timeline_object: L{TimelineObject}\n\"\"\"\n", "func_signal": "def removeTimelineObject(self, timeline_object):\n", "code": "self._disconnectFromTimelineObject(timeline_object)\nfor obj in timeline_object.track_objects:\n     self.removeTrackObject(obj)", "path": "pitivi\\timeline\\timeline.py", "repo_name": "alessandrod/pitivi", "stars": 12, "license": "other", "language": "python", "size": 6340}
{"docstring": "\"\"\"\nSets the priority of the object. 0 is the highest priority.\n\n@param priority: The priority (0 : highest)\n@type priority: L{int}\n@raises TimelineError: If the object doesn't control any C{TrackObject}s.\n\"\"\"\n", "func_signal": "def setPriority(self, priority):\n", "code": "if not self.track_objects:\n    raise TimelineError(\"TimelineObject doesn't control any TrackObjects\")\n\nfor track_object in self.track_objects:\n    track_object.setObjectPriority(priority)\n\nself.emit('priority-changed', priority)", "path": "pitivi\\timeline\\timeline.py", "repo_name": "alessandrod/pitivi", "stars": 12, "license": "other", "language": "python", "size": 6340}
{"docstring": "\"\"\"\nSet the start position of the object.\n\nIf snap is L{True}, then L{position} will be modified if it is close\nto a timeline edge.\n\n@param position: The position in nanoseconds.\n@type position: L{long}\n@param snap: Whether to snap to the nearest edge or not.\n@type snap: L{bool}\n@raises TimelineError: If the object doesn't control any C{TrackObject}s.\n\"\"\"\n", "func_signal": "def setStart(self, position, snap=False):\n", "code": "if not self.track_objects:\n    raise TimelineError(\"TimelineObject doesn't control any TrackObjects\")\n\nif snap:\n    position = self.timeline.snapToEdge(position, position + self.duration)\n\nif self.link is not None:\n    # if we're part of a link, we need to check if it's necessary to\n    # clamp position so that we don't push the earliest element before 0s\n    delta = position - self.start\n    off = self.link.earliest_start + delta\n    if off < 0:\n        # clamp so that the earliest element is shifted to 0s\n        position -= off\n\nfor track_object in self.track_objects:\n    track_object.setObjectStart(position)\n\nself.emit('start-changed', position)", "path": "pitivi\\timeline\\timeline.py", "repo_name": "alessandrod/pitivi", "stars": 12, "license": "other", "language": "python", "size": 6340}
{"docstring": "\"\"\"\nAdd EffectTracks corresponding to the effect from the factory to the corresponding\nL{TimelineObject}s on the timeline\n\n@param factory: The EffectFactory to add.\n@type factory: L{EffectFactory}\n@timeline_objects: The L{TimelineObject}s on whiches you want to add TrackObjects\n                   corresponding to the L{EffectFactory}\n@type timeline_objects: A C{List} of L{TimelineObject}s\n\n@raises TimelineError: if the factory doesn't have input or output streams\n@returns: A list of L{TimelineObject}, L{TrackObject} tuples\n\"\"\"\n#Note: We should maybe be able to handle several streams for effects which\n#are actually working on audio/video streams\n", "func_signal": "def addEffectFactoryOnObject(self, factory, timeline_objects):\n", "code": "self.debug(\"factory:%r\", factory)\n\noutput_stream = factory.getOutputStreams()\nif not output_stream:\n    raise TimelineError()\noutput_stream = output_stream[0]\n\ninput_stream = factory.getInputStreams()\nif not input_stream:\n    raise TimelineError()\ninput_stream = input_stream[0]\n\ntrack = None\nfor track_ in self.tracks:\n    if type(track_.stream) == type(input_stream):\n        track = track_\n        break\n\nif track is None:\n  raise TimelineError(\"There is no Track to add the effect to\")\n\nif not timeline_objects:\n  raise TimelineError(\"There is no timeline object to add effect to\")\n\nlistTimelineObjectTrackObject = []\ntrack_object = TrackEffect(factory, input_stream)\n\nfor obj in timeline_objects:\n    copy_track_obj = track_object.copy()\n    track.addTrackObject(copy_track_obj)\n    copy_track_obj.start = obj.start\n    copy_track_obj.duration = obj.duration\n    copy_track_obj.media_duration = obj.media_duration\n    obj.addTrackObject(copy_track_obj)\n    listTimelineObjectTrackObject.append((obj, copy_track_obj))\n\nself.debug(\"%s\", [\"TimelineObject %s => Track object: %s |\"\\\n                   %(listTo[0], listTo[1])\\\n                   for listTo in listTimelineObjectTrackObject])\nreturn listTimelineObjectTrackObject", "path": "pitivi\\timeline\\timeline.py", "repo_name": "alessandrod/pitivi", "stars": 12, "license": "other", "language": "python", "size": 6340}
{"docstring": "\"\"\"\nRemove the given object from the Timeline.\n\n@param obj: The object to remove\n@type obj: L{TimelineObject}\n@param deep: If C{True}, remove the L{TrackObject}s associated to the object.\n@type deep: C{bool}\n@raises TimelineError: If the object doesn't belong to the timeline.\n\"\"\"\n", "func_signal": "def removeTimelineObject(self, obj, deep=False):\n", "code": "try:\n    self.timeline_objects.remove(obj)\nexcept ValueError:\n    raise TimelineError(\"TimelineObject not controlled by this Timeline\")\n\nif obj.link is not None:\n    obj.link.removeTimelineObject(obj)\n\nself._disconnectFromTimelineObject(obj)\n\nobj.timeline = None\n\nself.edges.removeTimelineObject(obj)\n\nself.emit(\"timeline-object-removed\", obj)\n\nif deep:\n    for track_object in obj.track_objects:\n        track = track_object.track\n        track.removeTrackObject(track_object)", "path": "pitivi\\timeline\\timeline.py", "repo_name": "alessandrod/pitivi", "stars": 12, "license": "other", "language": "python", "size": 6340}
{"docstring": "\"\"\"\nSets the media-duration of the object.\n\n@param position: The position in nanoseconds.\n@type position: L{long}\n@raises TimelineError: If the object doesn't control any C{TrackObject}s.\n\"\"\"\n", "func_signal": "def setMediaDuration(self, position, snap=False):\n", "code": "if not self.track_objects:\n    raise TimelineError(\"TimelineObject doesn't control any TrackObjects\")\n\nfor track_object in self.track_objects:\n    track_object.setObjectMediaDuration(position)\n\nself.emit('media-duration-changed', position)", "path": "pitivi\\timeline\\timeline.py", "repo_name": "alessandrod/pitivi", "stars": 12, "license": "other", "language": "python", "size": 6340}
{"docstring": "\"\"\"Remove every instance factory in the timeline\n@param factory: the factory to remove from the timeline\n\"\"\"\n", "func_signal": "def removeFactory(self, factory):\n", "code": "objs = [obj for obj in self.timeline_objects if obj.factory is\n    factory]\nfor obj in objs:\n    self.removeTimelineObject(obj, deep=True)", "path": "pitivi\\timeline\\timeline.py", "repo_name": "alessandrod/pitivi", "stars": 12, "license": "other", "language": "python", "size": 6340}
{"docstring": "\"\"\"\nUnlink the currently selected timeline objects.\n\"\"\"\n", "func_signal": "def unlinkSelection(self):\n", "code": "empty_links = set()\nfor timeline_object in self.selection:\n    if timeline_object.link is None:\n        continue\n\n    link = timeline_object.link\n    link.removeTimelineObject(timeline_object)\n    if not link.timeline_objects:\n        empty_links.add(link)\n\nfor link in empty_links:\n    self.links.remove(link)\nself.emit(\"selection-changed\")", "path": "pitivi\\timeline\\timeline.py", "repo_name": "alessandrod/pitivi", "stars": 12, "license": "other", "language": "python", "size": 6340}
{"docstring": "\"\"\"Return a list of all track objects whose start property are\nadjacent to the given track object's end (start + duration)\"\"\"\n", "func_signal": "def getObjsAdjacentToEnd(self, trackobj):\n", "code": "end = trackobj.start + trackobj.duration\nif end in self.by_start:\n    return self.by_start[end]\nreturn []", "path": "pitivi\\timeline\\timeline.py", "repo_name": "alessandrod/pitivi", "stars": 12, "license": "other", "language": "python", "size": 6340}
{"docstring": "\"\"\"\nBlock internal updates. Use this when doing more than one consecutive\nmodification in the pipeline.\n\"\"\"\n", "func_signal": "def disableUpdates(self):\n", "code": "for track in self.tracks:\n    track.disableUpdates()\n\nself.edges.disableUpdates()\n\nself.emit(\"disable-updates\", True)", "path": "pitivi\\timeline\\timeline.py", "repo_name": "alessandrod/pitivi", "stars": 12, "license": "other", "language": "python", "size": 6340}
{"docstring": "\"\"\"\n@param timeline: the timeline to edit\n@type timeline: instance of L{pitivi.timeline.timeline.Timeline}\n\n@param focus: the TimelineObject or TrackObject which is to be the the\nmain target of interactive editing, such as the object directly under the\nmouse pointer\n@type focus: L{pitivi.timeline.timeline.TimelineObject} or\nL{pitivi.timeline.trackTrackObject}\n\n@param other: a set of objects which are the secondary targets of\ninteractive editing, such as objects in the current selection.\n@type other: a set() of L{TimelineObject}s or L{TrackObject}s\n\n@returns: An instance of L{pitivi.timeline.timeline.TimelineEditContex}\n\"\"\"\n\n# make sure focus is not in secondary object list\n", "func_signal": "def __init__(self, timeline, focus, other):\n", "code": "other.difference_update(set((focus,)))\n\nself.other = other\nself.focus = focus\nself.timeline = timeline\nself._snap = True\nself._mode = self.DEFAULT\nself._last_position = focus.start\nself._last_priority = focus.priority\n\nself.timeline.disableUpdates()", "path": "pitivi\\timeline\\timeline.py", "repo_name": "alessandrod/pitivi", "stars": 12, "license": "other", "language": "python", "size": 6340}
{"docstring": "\"\"\"\nJoins this Link with another and returns the resulting link.\n\n@type other_link: C{Link}\n@postcondition: L{self} and L{other_link} must not be used after\ncalling this method !!\n\"\"\"\n", "func_signal": "def join(self, other_link):\n", "code": "new_link = Link()\n\nfor timeline_object in list(self.timeline_objects):\n    self.removeTimelineObject(timeline_object)\n    new_link.addTimelineObject(timeline_object)\n\nfor timeline_object in list(other_link.timeline_objects):\n    other_link.removeTimelineObject(timeline_object)\n    new_link.addTimelineObject(timeline_object)\n\nreturn new_link", "path": "pitivi\\timeline\\timeline.py", "repo_name": "alessandrod/pitivi", "stars": 12, "license": "other", "language": "python", "size": 6340}
{"docstring": "\"\"\"\nAdd the track to the timeline.\n\n@param track: The track to add\n@type track: L{timeline.Track}\n@raises TimelineError: If the track is already in the timeline.\n\"\"\"\n", "func_signal": "def addTrack(self, track):\n", "code": "if track in self.tracks:\n    raise TimelineError(\"Provided track already controlled by the timeline\")\n\nself.tracks.append(track)\nself._updateDuration()\ntrack.connect('start-changed', self._trackDurationChangedCb)\ntrack.connect('duration-changed', self._trackDurationChangedCb)\n\nself.emit('track-added', track)", "path": "pitivi\\timeline\\timeline.py", "repo_name": "alessandrod/pitivi", "stars": 12, "license": "other", "language": "python", "size": 6340}
{"docstring": "\"\"\"\nAdd this object's start/stop values to the edges.\n\n@param timeline_object: The object whose start/stop we want to track.\n@type timeline_object: L{TimelineObject}\n\"\"\"\n", "func_signal": "def addTimelineObject(self, timeline_object):\n", "code": "for obj in timeline_object.track_objects:\n    self.addTrackObject(obj)\n\nself._connectToTimelineObject(timeline_object)", "path": "pitivi\\timeline\\timeline.py", "repo_name": "alessandrod/pitivi", "stars": 12, "license": "other", "language": "python", "size": 6340}
{"docstring": "\"\"\"\nUnblock internal updates. Use this after calling L{disableUpdates}.\n\"\"\"\n\n", "func_signal": "def enableUpdates(self):\n", "code": "for track in self.tracks:\n    track.enableUpdates()\n\nself.edges.enableUpdates()\n\nself.emit(\"disable-updates\", False)", "path": "pitivi\\timeline\\timeline.py", "repo_name": "alessandrod/pitivi", "stars": 12, "license": "other", "language": "python", "size": 6340}
{"docstring": "# track.stream -> track\n", "func_signal": "def _getSourceFactoryStreamMap(self, factory):\n", "code": "track_stream_to_track_map = dict((track.stream, track)\n        for track in self.tracks)\n\n# output_stream -> track.stream\noutput_stream_to_track_stream_map = \\\n        match_stream_groups_map(factory.output_streams,\n                [track.stream for track in self.tracks])\n\n# output_stream -> track (result)\noutput_stream_to_track_map = {}\nfor stream, track_stream in output_stream_to_track_stream_map.iteritems():\n    output_stream_to_track_map[stream] = \\\n            track_stream_to_track_map[track_stream]\n\nreturn output_stream_to_track_map", "path": "pitivi\\timeline\\timeline.py", "repo_name": "alessandrod/pitivi", "stars": 12, "license": "other", "language": "python", "size": 6340}
{"docstring": "\"\"\"\nSplits objects under the playehad. If the selection is not empty, the\nsplit only applies to selected clips. Otherwise it applies to all\nclips\"\"\"\n", "func_signal": "def split(self, time):\n", "code": "objs = set(self.getObjsAtTime(time))\nif len(self.selection):\n    objs = self.selection.selected.intersection(objs)\nfor obj in objs:\n    obj.split(time)", "path": "pitivi\\timeline\\timeline.py", "repo_name": "alessandrod/pitivi", "stars": 12, "license": "other", "language": "python", "size": 6340}
{"docstring": "\"\"\"\nCreates a TimelineObject for the given SourceFactory and adds it to the timeline.\n\n@param factory: The factory to add.\n@type factory: L{SourceFactory}\n@param stream_map: A mapping of factory streams to track streams.\n@type stream_map: C{dict} of MultimediaStream => MultimediaStream\n@param strict: If C{True} only add the factory if an exact stream mapping can be\ncalculated.\n@type strict: C{bool}\n@raises TimelineError: if C{strict} is True and no exact mapping could be calculated.\n\"\"\"\n", "func_signal": "def addSourceFactory(self, factory, stream_map=None, strict=False):\n", "code": "self.debug(\"factory:%r\", factory)\noutput_streams = factory.getOutputStreams()\nif not output_streams:\n    raise TimelineError(\"SourceFactory doesn't provide any Output Streams\")\n\nif stream_map is None:\n    stream_map = self._getSourceFactoryStreamMap(factory)\n    if len(stream_map) < len(output_streams):\n        # we couldn't assign each stream to a track automatically,\n        # error out and require the caller to pass a stream_map\n        self.error(\"Couldn't find a complete stream mapping (self:%d < factory:%d)\",\n                   len(stream_map), len(output_streams))\n        if strict:\n            raise TimelineError(\"Couldn't map all streams to available Tracks\")\n\ntimeline_object = TimelineObject(factory)\nstart = 0\nfor stream, track in stream_map.iteritems():\n    self.debug(\"Stream: %s, Track: %s, Track duration: %d\", str(stream),\n               str(track), track.duration)\n    start = max(start, track.duration)\n    track_object = SourceTrackObject(factory, stream)\n    track.addTrackObject(track_object)\n    timeline_object.addTrackObject(track_object)\n\ntimeline_object.start = start\nself.addTimelineObject(timeline_object)\nreturn timeline_object", "path": "pitivi\\timeline\\timeline.py", "repo_name": "alessandrod/pitivi", "stars": 12, "license": "other", "language": "python", "size": 6340}
{"docstring": "\"\"\"The about page.\"\"\"\n", "func_signal": "def about(request):\n", "code": "return render_to_response('maposmatic/about.html',\n                          context_instance=RequestContext(request))", "path": "www\\maposmatic\\views.py", "repo_name": "wonderchook/MapOSMatic", "stars": 8, "license": "agpl-3.0", "language": "python", "size": 11924}
{"docstring": "\"\"\"Returns a list of number.\nIt contains the id of the pages to display for a page given.\"\"\"\n\n# Navigation pages\n", "func_signal": "def get_pages_list(page, paginator):\n", "code": "nav = {}\npage_list = []\nlast = False\n\nfor i in [1, 2,\n          page.number-1, page.number, page.number+1,\n          paginator.num_pages-1, paginator.num_pages]:\n    nav[i] = True\n\nfor i in xrange(1, paginator.num_pages+1):\n    if nav.has_key(i):\n        if last and i - last > 1:\n            page_list.append('...')\n        page_list.append(i)\n        last = i\nreturn page_list", "path": "www\\maposmatic\\helpers.py", "repo_name": "wonderchook/MapOSMatic", "stars": 8, "license": "agpl-3.0", "language": "python", "size": 11924}
{"docstring": "\"\"\"Displays all maps, sorted alphabetically, eventually matching the search\nterms, when provided.\"\"\"\n\n", "func_signal": "def all_maps(request):\n", "code": "map_list = None\nform = forms.MapSearchForm(request.GET)\n\nif form.is_valid():\n    map_list = (models.MapRenderingJob.objects\n                .order_by('maptitle')\n                .filter(status=2)\n                .filter(maptitle__icontains=form.cleaned_data['query']))\n    if len(map_list) == 1:\n        return HttpResponseRedirect(reverse('job-by-id',\n                                            args=[map_list[0].id]))\nelse:\n    form = forms.MapSearchForm()\n\nif map_list is None:\n    map_list = (models.MapRenderingJob.objects.filter(status=2)\n                .filter(resultmsg=\"ok\")\n                .order_by('maptitle'))\npaginator = Paginator(map_list, www.settings.ITEMS_PER_PAGE)\n\ntry:\n    page = int(request.GET.get('page', '1'))\nexcept ValueError:\n    page = 1\n\ntry:\n    maps = paginator.page(page)\nexcept (EmptyPage, InvalidPage):\n    maps = paginator.page(paginator.num_pages)\n\nreturn render_to_response('maposmatic/all_maps.html',\n                          { 'maps': maps, 'form': form,\n                            'is_search': form.is_valid(),\n                            'pages': helpers.get_pages_list(maps, paginator) },\n                          context_instance=RequestContext(request))", "path": "www\\maposmatic\\views.py", "repo_name": "wonderchook/MapOSMatic", "stars": 8, "license": "agpl-3.0", "language": "python", "size": 11924}
{"docstring": "\"\"\"The main page.\"\"\"\n", "func_signal": "def index(request):\n", "code": "return render_to_response('maposmatic/index.html',\n                          context_instance=RequestContext(request))", "path": "www\\maposmatic\\views.py", "repo_name": "wonderchook/MapOSMatic", "stars": 8, "license": "agpl-3.0", "language": "python", "size": 11924}
{"docstring": "\"\"\"Tries to find the parent MapRenderingJob of a given file from its\nfilename. Both the job ID found in the first part of the prefix and the\nentire files_prefix is used to match a job.\"\"\"\n\n", "func_signal": "def get_by_filename(self, name):\n", "code": "try:\n    jobid = int(name.split('_', 1)[0])\n    job = MapRenderingJob.objects.get(id=jobid)\n    if name.startswith(job.files_prefix()):\n        return job\nexcept (ValueError, IndexError, MapRenderingJob.DoesNotExist):\n    pass\n\nreturn None", "path": "www\\maposmatic\\models.py", "repo_name": "wonderchook/MapOSMatic", "stars": 8, "license": "agpl-3.0", "language": "python", "size": 11924}
{"docstring": "\"\"\"Removes all the output files from this job, and returns the space\nsaved in bytes (Note: the thumbnail is not removed).\"\"\"\n\n", "func_signal": "def remove_all_files(self):\n", "code": "files = self.output_files()\nsaved = 0\nremoved = 0\n\nfor f in (files['maps'] + files['indeces']):\n    try:\n        os.remove(f[4])\n        removed += 1\n        saved += f[3]\n    except OSError:\n        pass\n\nself.status = 3\nself.save()\nreturn removed, saved", "path": "www\\maposmatic\\models.py", "repo_name": "wonderchook/MapOSMatic", "stars": 8, "license": "agpl-3.0", "language": "python", "size": 11924}
{"docstring": "\"\"\"Make sure that the supplied OSM Id is valid and can be accepted for\nrendering (bounding box not too large, etc.). Raise an exception in\ncase of error.\"\"\"\n\n# If no GIS database is configured, bypass the city_exists check by\n# returning True.\n", "func_signal": "def check_osm_id(osm_id, table='polygon'):\n", "code": "if not www.settings.has_gis_database():\n    raise ValueError(\"No GIS database available\")\n\nconn = psycopg2.connect(\"dbname='%s' user='%s' host='%s' password='%s'\" %\n                        (www.settings.GIS_DATABASE_NAME,\n                         www.settings.DATABASE_USER,\n                         www.settings.DATABASE_HOST,\n                         www.settings.DATABASE_PASSWORD))\n\ntry:\n    cursor = conn.cursor()\n    cursor.execute(\"\"\"select osm_id,st_astext(st_transform(st_envelope(way),\n                                                           4002))\n                      from planet_osm_%s where\n                      osm_id=%d\"\"\" % (table,int(osm_id)))\n    result = cursor.fetchall()\n    try:\n        ((ret_osm_id, envlp),) = result\n    except ValueError:\n        raise ValueError(\"Cannot lookup OSM ID in table %s\" % table)\n\n    assert ret_osm_id == osm_id\n\n    # Check bbox size\n    bbox = OCMBoundingBox.parse_wkt(envlp)\n    (metric_size_lat, metric_size_long) = bbox.spheric_sizes()\n    if metric_size_lat > www.settings.BBOX_MAXIMUM_LENGTH_IN_METERS or \\\n            metric_size_long > www.settings.BBOX_MAXIMUM_LENGTH_IN_METERS:\n        raise ValueError(\"Area too large\")\n\nfinally:\n    conn.close()", "path": "www\\maposmatic\\helpers.py", "repo_name": "wonderchook/MapOSMatic", "stars": 8, "license": "agpl-3.0", "language": "python", "size": 11924}
{"docstring": "\"\"\"\nTake a structure containing strings (dict, list, scalars, ...)\nand convert it into the same structure with the proper conversions\nto float or integers, etc.\n\"\"\"\n", "func_signal": "def _canonicalize_data(data):\n", "code": "if type(data) is tuple:\n    return tuple(_canonicalize_data(x) for x in data)\nelif type(data) is list:\n    return [_canonicalize_data(x) for x in data]\nelif type(data) is dict:\n    return dict([(_canonicalize_data(k),\n                  _canonicalize_data(v)) for k,v in data.iteritems()])\ntry:\n    return int(data)\nexcept ValueError:\n    try:\n        return float(data)\n    except ValueError:\n        pass\nreturn data", "path": "www\\maposmatic\\nominatim.py", "repo_name": "wonderchook/MapOSMatic", "stars": 8, "license": "agpl-3.0", "language": "python", "size": 11924}
{"docstring": "\"\"\"Checks the obsolete status of the given job.  If the job is in the\ndone state (status=2) and does not have all its output files, it's\nobselete and should be cleaned.\"\"\"\n", "func_signal": "def __check_obsolete(self, job):\n", "code": "if not job.is_done_ok():\n    return\n\nfiles = job.output_files()\nif (len(files['maps']) < len(RENDERING_RESULT_FORMATS)-1 or\n    (len(files['indeces']) > 0 and\n     len(files['indeces']) < len(RENDERING_RESULT_FORMATS))):\n    return self.handler.do_obsolete(job)", "path": "scripts\\dbck.py", "repo_name": "wonderchook/MapOSMatic", "stars": 8, "license": "agpl-3.0", "language": "python", "size": 11924}
{"docstring": "\"\"\"Displays all jobs from the last 24 hours.\"\"\"\n\n", "func_signal": "def all_jobs(request):\n", "code": "one_day_before = datetime.datetime.now() - datetime.timedelta(1)\njob_list = (models.MapRenderingJob.objects.all()\n            .order_by('-submission_time')\n            .filter(submission_time__gte=one_day_before))\npaginator = Paginator(job_list, www.settings.ITEMS_PER_PAGE)\n\ntry:\n    page = int(request.GET.get('page', '1'))\nexcept ValueError:\n    page = 1\n\ntry:\n    jobs = paginator.page(page)\nexcept (EmptyPage, InvalidPage):\n    jobs = paginator.page(paginator.num_pages)\n\nreturn render_to_response('maposmatic/all_jobs.html',\n                          { 'jobs': jobs,\n                            'pages': helpers.get_pages_list(jobs, paginator) },\n                          context_instance=RequestContext(request))", "path": "www\\maposmatic\\views.py", "repo_name": "wonderchook/MapOSMatic", "stars": 8, "license": "agpl-3.0", "language": "python", "size": 11924}
{"docstring": "\"\"\"Returns the ID of a rendering matching the given OpenStreetMap city ID\nfrom the last 24 hours, or None if no rendering can be found matching this\ncriteria.\"\"\"\n\n# First try to find rendered items\n", "func_signal": "def rendering_already_exists_by_osmid(osmid):\n", "code": "rendered_items = (MapRenderingJob.objects\n                  .filter(submission_time__gte=(datetime.datetime.now()\n                                               - datetime.timedelta(1)))\n                  .filter(administrative_osmid=osmid)\n                  .filter(status=2)\n                  .filter(resultmsg=\"ok\")\n                  .order_by(\"-submission_time\")[:1])\n\nif len(rendered_items) and rendered_items[0].has_output_files():\n    return rendered_items[0].id\n\n# Then try to find items being rendered or waiting for rendering\nrendered_items = (MapRenderingJob.objects\n                  .filter(submission_time__gte=(datetime.datetime.now()\n                                               - datetime.timedelta(1)))\n                  .filter(administrative_osmid=osmid)\n                  .filter(status__in=[0,1])\n                  .order_by(\"-submission_time\")[:1])\n\nif len(rendered_items):\n    return rendered_items[0].id\n\n# No rendering found\nreturn None", "path": "www\\maposmatic\\helpers.py", "repo_name": "wonderchook/MapOSMatic", "stars": 8, "license": "agpl-3.0", "language": "python", "size": 11924}
{"docstring": "\"\"\"This function tells whether this job still has its output files\navailable on the rendering storage.\n\nTheir actual presence is checked if the job is considered done and not\nyet obsolete.\"\"\"\n\n", "func_signal": "def has_output_files(self):\n", "code": "if self.is_done():\n    files = self.output_files()\n    return len(files['maps']) + len(files['indeces'])\n\nreturn False", "path": "www\\maposmatic\\models.py", "repo_name": "wonderchook/MapOSMatic", "stars": 8, "license": "agpl-3.0", "language": "python", "size": 11924}
{"docstring": "\"\"\"\nQuery the nominatim service for the given city query and return a\n(python) list of entries for the given squery (eg. \"Paris\"). Each\nentry is a dictionary key -> value (value is always a\nstring). When possible, we also try to uncover the OSM database\nIDs associated with the entries; in that case, an\n\"ocitysmap_params\" key is provided, which maps to a dictionary\ncontaining:\n  - key \"table\": when \"line\" -> refers to table \"planet_osm_line\";\n    when \"polygon\" -> \"planet_osm_polygon\"\n  - key \"id\": ID of the OSM database entry\n  - key \"admin_level\": The value stored in the OSM table for admin_level\n\"\"\"\n", "func_signal": "def query(query_text, with_polygons = False):\n", "code": "entries = _fetch_entries(query_text, with_polygons)\nreturn _canonicalize_data(_retrieve_missing_data_from_GIS(entries))", "path": "www\\maposmatic\\nominatim.py", "repo_name": "wonderchook/MapOSMatic", "stars": 8, "license": "agpl-3.0", "language": "python", "size": 11924}
{"docstring": "\"\"\"Make sure the job declares a map language.\"\"\"\n", "func_signal": "def __check_locale(self, job):\n", "code": "if not job.map_language:\n    return self.handler.do_locale(job)", "path": "scripts\\dbck.py", "repo_name": "wonderchook/MapOSMatic", "stars": 8, "license": "agpl-3.0", "language": "python", "size": 11924}
{"docstring": "\"\"\"\nQuery the nominatim service for the given city query and return a\n(python) list of entries for the given squery (eg. \"Paris\"). Each\nentry is a dictionary key -> value (value is always a\nstring).\n\"\"\"\n# For some reason, the \"xml\" nominatim output is ALWAYS used, even\n# though we will later (in views.py) transform this into\n# json. This is because we know that this xml output is correct\n# and complete (at least the \"osm_id\" field is missing from the\n# json output)\n", "func_signal": "def _fetch_entries(query_text, with_polygons):\n", "code": "query_tags = dict(q=query_text.encode(\"UTF-8\"),\n                  format='xml', addressdetails=1)\nif with_polygons:\n    query_tags['polygon']=1\n\nqdata = urlencode(query_tags)\nf = urllib2.urlopen(url=\"%s?%s\" % (NOMINATIM_BASE_URL, qdata))\n\nresult = []\nfor place in XMLTree(f).getroot().getchildren():\n    attribs = dict(place.attrib)\n    for elt in place.getchildren():\n        attribs[elt.tag] = elt.text\n    result.append(attribs)\n\nreturn result", "path": "www\\maposmatic\\nominatim.py", "repo_name": "wonderchook/MapOSMatic", "stars": 8, "license": "agpl-3.0", "language": "python", "size": 11924}
{"docstring": "\"\"\"If the job is obsolete, but not marked as such in the database (i.e.\nits status is 2 'Done', but it doesn't have all its output files\npresent), ask the job to remove all its files and mark itself as\nobsolete.\"\"\"\n", "func_signal": "def do_obsolete(self, job):\n", "code": "removed, saved = job.remove_all_files()\nreturn 'removed %d file(s) and marked obsolete.' % removed", "path": "scripts\\dbck.py", "repo_name": "wonderchook/MapOSMatic", "stars": 8, "license": "agpl-3.0", "language": "python", "size": 11924}
{"docstring": "\"\"\"If the job has no locale defined, make sure it has one, using the\nDEFAULT_LOCALE defined in RealDbckHandler.\"\"\"\n", "func_signal": "def do_locale(self, job):\n", "code": "job.map_language = self.DEFAULT_LOCALE\njob.save()\nreturn 'missed locale information, fell back to %s.' % \\\n    self.DEFAULT_LOCALE", "path": "scripts\\dbck.py", "repo_name": "wonderchook/MapOSMatic", "stars": 8, "license": "agpl-3.0", "language": "python", "size": 11924}
{"docstring": "# Do not add the useless overhead of parsing blog entries when generating\n# the rss feed\n", "func_signal": "def all(request):\n", "code": "if request.path == reverse('rss-feed', args=['maps']):\n    return {}\nreturn {\n    'randommap': MapRenderingJob.objects.get_random_with_thumbnail(),\n    'blogposts': get_latest_blog_posts(),\n    'MAPOSMATIC_DAEMON_RUNNING': www.settings.is_daemon_running(),\n}", "path": "www\\maposmatic\\context_processors.py", "repo_name": "wonderchook/MapOSMatic", "stars": 8, "license": "agpl-3.0", "language": "python", "size": 11924}
{"docstring": "\"\"\"The job details page.\n\nArgs:\n    job_id (int): the job ID in the database.\n\"\"\"\n\n", "func_signal": "def job(request, job_id, job_nonce=None):\n", "code": "job = get_object_or_404(models.MapRenderingJob, id=job_id)\nisredirected = request.session.get('redirected', False)\nrequest.session.pop('redirected', None)\n\nrefresh = www.settings.REFRESH_JOB_WAITING\nif job.is_rendering():\n    refresh = www.settings.REFRESH_JOB_RENDERING\n\nreturn render_to_response('maposmatic/job-page.html',\n                          { 'job': job, 'single': True,\n                            'redirected': isredirected, 'nonce': job_nonce,\n                            'refresh': refresh, 'refresh_ms': (refresh*1000) },\n                          context_instance=RequestContext(request))", "path": "www\\maposmatic\\views.py", "repo_name": "wonderchook/MapOSMatic", "stars": 8, "license": "agpl-3.0", "language": "python", "size": 11924}
{"docstring": "\"\"\"Make sure we don't have mutually exclusive admin/bbox data.\"\"\"\n", "func_signal": "def __check_admin_bbox(self, job):\n", "code": "if job.administrative_osmid and (job.lat_upper_left or\n                                 job.lon_upper_left or\n                                 job.lat_bottom_right or\n                                 job.lon_bottom_right):\n    return self.handler.do_admin_bbox(job)", "path": "scripts\\dbck.py", "repo_name": "wonderchook/MapOSMatic", "stars": 8, "license": "agpl-3.0", "language": "python", "size": 11924}
{"docstring": "\"\"\" clean all datas in buffer \"\"\"\n", "func_signal": "def clean(self):\n", "code": "self.prepare()\nself.buffer[:] = []\nself.firstwrite = 1", "path": "plugin\\debugger.py", "repo_name": "vim-scripts/DBGp-Remote-Debugger-Interface", "stars": 9, "license": "None", "language": "python", "size": 317}
{"docstring": "\"\"\" start debugger or continue \"\"\"\n", "func_signal": "def run(self):\n", "code": "if self.protocol.isconnected():\n  self.command('run')\n  self.command('stack_get')\nelse:\n  self.clear()\n  self.protocol.accept()\n  self.ui.debug_mode()\n  self.running = 1\n\n  self.recv(1)\n\n  # set max data to get with eval results\n  self.command('feature_set', '-n max_children -v ' + self.max_children)\n  self.command('feature_set', '-n max_data -v ' + self.max_data)\n  self.command('feature_set', '-n max_depth -v ' + self.max_depth)\n\n  self.command('step_into')\n\n  flag = 0\n  for bno in self.breakpt.list():\n      msgid = self.send_command('breakpoint_set', \\\n                              '-t line -f ' + self.breakpt.getfile(bno) + ' -n ' + str(self.breakpt.getline(bno)) + ' -s enabled', \\\n                              self.breakpt.getexp(bno))\n      self.bptsetlst[msgid] = bno\n      flag = 1\n  if flag:\n      self.recv()\n\n  self.ui.go_srcview()", "path": "plugin\\debugger.py", "repo_name": "vim-scripts/DBGp-Remote-Debugger-Interface", "stars": 9, "license": "None", "language": "python", "size": 317}
{"docstring": "\"\"\" restore mode to normal \"\"\"\n", "func_signal": "def normal_mode(self):\n", "code": "if self.mode == 0: # is normal mode ?\n  return\n\nvim.command('sign unplace 1')\nvim.command('sign unplace 2')\n\n# destory all created windows\nself.destroy()\n\n# go to the initial tab\nself.switch_working_tab()\n\nif self.dedicatedtab: # if using dedicated tab, just delete it\n  vim.command(\"tabclose\")\n  vim.command(\"tabn \" + self.origintab)\nelse: \n  self.restore_session()\n\nself.set_highlight()\nself.file    = None\nself.line    = None\nself.mode    = 0\nself.cursign = None", "path": "plugin\\debugger.py", "repo_name": "vim-scripts/DBGp-Remote-Debugger-Interface", "stars": 9, "license": "None", "language": "python", "size": 317}
{"docstring": "\"\"\" receive message until response is last transaction id or received count's message \"\"\"\n", "func_signal": "def recv(self, count=10000):\n", "code": "while count>0:\n  count = count - 1\n  # recv message and convert to XML object\n  txt = self.protocol.recv_msg()\n  res = xml.dom.minidom.parseString(txt)\n  # log messages {{{\n  self.ui.trace( str(self.msgid) + ' : recv <===== {{{   ' + txt)\n  self.ui.trace('}}}')\n  # handle message\n  self.handle_msg(res)\n  # exit, if response's transaction id == last transaction id\n  try:\n    if int(res.firstChild.getAttribute('transaction_id')) == int(self.msgid):\n      return\n  except:\n    pass", "path": "plugin\\debugger.py", "repo_name": "vim-scripts/DBGp-Remote-Debugger-Interface", "stars": 9, "license": "None", "language": "python", "size": 317}
{"docstring": "\"\"\"handle <response command=run> tag\n<response command=\"step_over\" reason=\"ok\" status=\"break\" transaction_id=\"1 \"/>\"\"\"\n", "func_signal": "def handle_response_run(self, res):\n", "code": "if res.firstChild.hasAttribute('status'):\n  self.status = res.firstChild.getAttribute('status')\n  return", "path": "plugin\\debugger.py", "repo_name": "vim-scripts/DBGp-Remote-Debugger-Interface", "stars": 9, "license": "None", "language": "python", "size": 317}
{"docstring": "\"\"\" initalize \"\"\"\n", "func_signal": "def __init__(self):\n", "code": "self.breakpt  = {}\nself.revmap   = {}\nself.startbno = 10000\nself.maxbno   = self.startbno", "path": "plugin\\debugger.py", "repo_name": "vim-scripts/DBGp-Remote-Debugger-Interface", "stars": 9, "license": "None", "language": "python", "size": 317}
{"docstring": "\"\"\" destroy window \"\"\"\n", "func_signal": "def destroy(self):\n", "code": "if self.buffer == None or len(dir(self.buffer)) == 0:\n  return\n#if self.name == 'LOG___WINDOW':\n#  self.command('hide')\n#else:\nself.command('bdelete ' + self.name)\nself.firstwrite = 1", "path": "plugin\\debugger.py", "repo_name": "vim-scripts/DBGp-Remote-Debugger-Interface", "stars": 9, "license": "None", "language": "python", "size": 317}
{"docstring": "\"\"\" create windows \"\"\"\n", "func_signal": "def create(self):\n", "code": "self.watchwin.create('vertical belowright new')\nself.helpwin.create('belowright new')\nself.stackwin.create('belowright new')\nif self.debugger.debug:\n    self.tracewin.create('belowright new')", "path": "plugin\\debugger.py", "repo_name": "vim-scripts/DBGp-Remote-Debugger-Interface", "stars": 9, "license": "None", "language": "python", "size": 317}
{"docstring": "\"\"\" find break point and return bno(breakpoint number) \"\"\"\n", "func_signal": "def find(self, file, line):\n", "code": "for bno in self.breakpt.keys():\n  if self.breakpt[bno]['file'] == file and self.breakpt[bno]['line'] == line:\n    return bno\nreturn None", "path": "plugin\\debugger.py", "repo_name": "vim-scripts/DBGp-Remote-Debugger-Interface", "stars": 9, "license": "None", "language": "python", "size": 317}
{"docstring": "\"\"\" create window \"\"\"\n", "func_signal": "def create(self, method = 'new'):\n", "code": "vim.command('silent ' + method + ' ' + self.name)\n#if self.name != 'LOG___WINDOW':\nvim.command(\"setlocal buftype=nofile\")\nself.buffer = vim.current.buffer\nself.width  = int( vim.eval(\"winwidth(0)\")  )\nself.height = int( vim.eval(\"winheight(0)\") )\nself.on_create()", "path": "plugin\\debugger.py", "repo_name": "vim-scripts/DBGp-Remote-Debugger-Interface", "stars": 9, "license": "None", "language": "python", "size": 317}
{"docstring": "\"\"\" destroy windows \"\"\"\n", "func_signal": "def destroy(self):\n", "code": "self.helpwin.destroy()\nself.watchwin.destroy()\nself.stackwin.destroy()\nif self.debugger.debug:\n  self.tracewin.destroy()", "path": "plugin\\debugger.py", "repo_name": "vim-scripts/DBGp-Remote-Debugger-Interface", "stars": 9, "license": "None", "language": "python", "size": 317}
{"docstring": "\"\"\" go to my window & execute command \"\"\"\n", "func_signal": "def command(self, cmd):\n", "code": "self.prepare()\nwinnr = self.getwinnr()\nif winnr != int(vim.eval(\"winnr()\")):\n  vim.command(str(winnr) + 'wincmd w')\nvim.command(cmd)", "path": "plugin\\debugger.py", "repo_name": "vim-scripts/DBGp-Remote-Debugger-Interface", "stars": 9, "license": "None", "language": "python", "size": 317}
{"docstring": "\"\"\" send message \"\"\"\n", "func_signal": "def send(self, msg):\n", "code": "self.protocol.send_msg(msg)\n# log message\nself.ui.trace(str(self.msgid) + ' : send =====> ' + msg)", "path": "plugin\\debugger.py", "repo_name": "vim-scripts/DBGp-Remote-Debugger-Interface", "stars": 9, "license": "None", "language": "python", "size": 317}
{"docstring": "#print \"Debugger is stopping\\n\"\n", "func_signal": "def quit(self):\n", "code": "if self.running == 0:\n  raise NotRunningException\n\nself.ui.normal_mode()\nself.clear()", "path": "plugin\\debugger.py", "repo_name": "vim-scripts/DBGp-Remote-Debugger-Interface", "stars": 9, "license": "None", "language": "python", "size": 317}
{"docstring": "\"\"\" set srcview windows to file:line and replace current sign \"\"\"\n\n", "func_signal": "def set_srcview(self, file, line):\n", "code": "if file == self.file and self.line == line:\n  return\n\n# there are bug with path string in Windows system, so if this is Windows, remove the initial '/'\nfile = urllib.unquote(file)\nif os.name == 'nt' and file[0] == \"/\":\n  file = file[1:]\n\nnextsign = self.next_sign()\n\nif file != self.file:\n  self.file = file\n  self.go_srcview()\n  vim.command('silent edit ' + file)\n\nvim.command('sign place ' + nextsign + ' name=current line='+str(line)+' file='+file)\nvim.command('sign unplace ' + self.cursign)\n\nvim.command('sign jump ' + nextsign + ' file='+file)\n#vim.command('normal z.')\n\nself.line    = line\nself.cursign = nextsign", "path": "plugin\\debugger.py", "repo_name": "vim-scripts/DBGp-Remote-Debugger-Interface", "stars": 9, "license": "None", "language": "python", "size": 317}
{"docstring": "\"\"\" initialize object \"\"\"\n", "func_signal": "def __init__(self, debugger, dedicatedtab, minibufexpl = 0):\n", "code": "self.debugger = debugger\nself.watchwin = WatchWindow(self)\nself.stackwin = StackWindow(self)\nself.tracewin = TraceWindow(self)\nself.helpwin  = HelpWindow(self, 'HELP__WINDOW')\nself.mode     = 0 # normal mode\nself.file     = None\nself.line     = None\nself.winbuf   = {}\nself.cursign  = None\nself.sessfile = \"/tmp/debugger_vim_saved_session.\" + str(os.getpid())\nself.minibufexpl = minibufexpl\n# tab stuff\nself.dedicatedtab = dedicatedtab\nself.origintab = 0\nself.debugtab = 0\nself.usetab = 0\nself.usesessiontab = 0", "path": "plugin\\debugger.py", "repo_name": "vim-scripts/DBGp-Remote-Debugger-Interface", "stars": 9, "license": "None", "language": "python", "size": 317}
{"docstring": "\"\"\" set vim highlight of debugger sign \"\"\"\n", "func_signal": "def set_highlight(self):\n", "code": "vim.command(\"highlight DbgCurrent term=reverse ctermfg=White ctermbg=Red gui=reverse\")\nvim.command(\"highlight DbgBreakPt term=reverse ctermfg=White ctermbg=Green gui=reverse\")", "path": "plugin\\debugger.py", "repo_name": "vim-scripts/DBGp-Remote-Debugger-Interface", "stars": 9, "license": "None", "language": "python", "size": 317}
{"docstring": "\"\"\" check window is OK \"\"\"\n", "func_signal": "def isprepared(self):\n", "code": "if self.buffer == None or len(dir(self.buffer)) == 0 or self.getwinnr() == -1:\n  return 0\nreturn 1", "path": "plugin\\debugger.py", "repo_name": "vim-scripts/DBGp-Remote-Debugger-Interface", "stars": 9, "license": "None", "language": "python", "size": 317}
{"docstring": "\"\"\" initialize Debugger \"\"\"\n", "func_signal": "def __init__(self, port = 9000, max_children = '32', max_data = '1024', max_depth = '1', timeout = 5, dedicatedtab = 1, minibufexpl = '0', debug = 0):\n", "code": "socket.setdefaulttimeout(timeout)\nself.port       = port\nself.debug      = debug\n\nself.current    = None\nself.file       = None\nself.lasterror  = None\nself.msgid      = 0\nself.running    = 0\nself.stacks     = []\nself.curstack   = 0\nself.laststack  = 0\nself.bptsetlst  = {} \n\nself.status        = None\nself.max_children  = max_children\nself.max_data      = max_data\nself.max_depth     = max_depth\n\nself.protocol   = DbgProtocol(self.port)\n\nself.ui         = DebugUI(self, dedicatedtab, minibufexpl)\nself.breakpt    = BreakPoint()\n\nvim.command('sign unplace *')", "path": "plugin\\debugger.py", "repo_name": "vim-scripts/DBGp-Remote-Debugger-Interface", "stars": 9, "license": "None", "language": "python", "size": 317}
{"docstring": "\"\"\" add break point at file:line \"\"\"\n", "func_signal": "def add(self, file, line, exp = ''):\n", "code": "self.maxbno = self.maxbno + 1\nself.breakpt[self.maxbno] = { 'file':file, 'line':line, 'exp':exp, 'id':None }\nreturn self.maxbno", "path": "plugin\\debugger.py", "repo_name": "vim-scripts/DBGp-Remote-Debugger-Interface", "stars": 9, "license": "None", "language": "python", "size": 317}
{"docstring": "\"\"\" Trim comments and whitespace from each end of a list of tokens.\n\"\"\"\n", "func_signal": "def trim_extra(tokens):\n", "code": "if len(tokens) == 0:\n    return tokens\n\nwhile tokens[0][0] in ('S', 'COMMENT'):\n    tokens = tokens[1:]\n\nwhile tokens[-1][0] in ('S', 'COMMENT'):\n    tokens = tokens[:-1]\n    \nreturn tokens", "path": "tags\\cascadenik\\0.1.0\\cascadenik\\style.py", "repo_name": "umidev/mapnik-utils", "stars": 14, "license": "None", "language": "python", "size": 67496}
{"docstring": "\"\"\"\n\"\"\"\n", "func_signal": "def application(environ, start_response):\n", "code": "global mapfile\n#global kwargs\nglobal MAP_CACHE\nreq = z.Request(environ)\n\n\nif req.method == 'POST' and not req.data == 'Paste mapfile here':\n  #import pdb;pdb.set_trace()\n  xml_string = req.data\n  mapfile = '/tmp/mapfile%s.xml' % random.random()\n  tmp = open(mapfile, 'w+b')\n  tmp.write(xml_string)\n  tmp.seek(0)\n  tmp.close()\n  MAP_CACHE.mapfile = mapfile\n  MAP_CACHE.load_map()\n\nif req.path in views:\n    resp = views[req.path](req)\nelif req.path == '/tiles':\n    # Used cached map and projection if instance exists\n    if not MAP_CACHE:\n      MAP_CACHE = MapResponse(req.values,mapfile)\n    resp = MAP_CACHE(req.values)\nelif req.path.rstrip('/').endswith('/js'):\n        arg = req.args.get('f')\n        resp = get_resource(req, arg)\nelif req.path.startswith('/images'):\n        #import pdb;pdb.set_trace()\n        resp = get_resource(req, req.path.lstrip('/'))\nelse:\n    resp = not_found(req)\n\n  \nreturn resp(environ, start_response)", "path": "sandbox\\wsgi_fun\\localserver_mapfish.py", "repo_name": "umidev/mapnik-utils", "stars": 14, "license": "None", "language": "python", "size": 67496}
{"docstring": "\"\"\"Update our built-in md5 registry\"\"\"\n\n", "func_signal": "def update_md5(filenames):\n", "code": "import re\nfrom md5 import md5\n\nfor name in filenames:\n    base = os.path.basename(name)\n    f = open(name,'rb')\n    md5_data[base] = md5(f.read()).hexdigest()\n    f.close()\n\ndata = [\"    %r: %r,\\n\" % it for it in md5_data.items()]\ndata.sort()\nrepl = \"\".join(data)\n\nimport inspect\nsrcfile = inspect.getsourcefile(sys.modules[__name__])\nf = open(srcfile, 'rb'); src = f.read(); f.close()\n\nmatch = re.search(\"\\nmd5_data = {\\n([^}]+)}\", src)\nif not match:\n    print >>sys.stderr, \"Internal error!\"\n    sys.exit(2)\n\nsrc = src[:match.start(1)] + repl + src[match.end(1):]\nf = open(srcfile,'w')\nf.write(src)\nf.close()", "path": "tags\\nik2img\\0.2.4\\ez_setup.py", "repo_name": "umidev/mapnik-utils", "stars": 14, "license": "None", "language": "python", "size": 67496}
{"docstring": "\"\"\"\n\"\"\"\n", "func_signal": "def postprocess_value(tokens, property, base=None, line=0, col=0):\n", "code": "tokens = trim_extra(tokens)\n\nif len(tokens) >= 2 and (tokens[-2] == ('CHAR', '!')) and (tokens[-1] == ('IDENT', 'important')):\n    important = True\n    tokens = trim_extra(tokens[:-2])\n\nelse:\n    important = False\n\nif properties[property.name] in (int, float) and len(tokens) == 2 and tokens[0] == ('CHAR', '-') and tokens[1][0] == 'NUMBER':\n    # put the negative sign on the number\n    tokens = [(tokens[1][0], '-' + tokens[1][1])]\n\nvalue = tokens\n\nif properties[property.name] in (int, float, str, color, uri, boolean) or type(properties[property.name]) is tuple:\n    if len(tokens) != 1:\n        raise ParseException('Single value only for property \"%(property)s\"' % locals(), line, col)\n\nif properties[property.name] is int:\n    if tokens[0][0] != 'NUMBER':\n        raise ParseException('Number value only for property \"%(property)s\"' % locals(), line, col)\n\n    value = int(tokens[0][1])\n\nelif properties[property.name] is float:\n    if tokens[0][0] != 'NUMBER':\n        raise ParseException('Number value only for property \"%(property)s\"' % locals(), line, col)\n\n    value = float(tokens[0][1])\n\nelif properties[property.name] is str:\n    if tokens[0][0] != 'STRING':\n        raise ParseException('String value only for property \"%(property)s\"' % locals(), line, col)\n\n    value = tokens[0][1][1:-1]\n\nelif properties[property.name] is color_transparent:\n    if tokens[0][0] != 'HASH' and (tokens[0][0] != 'IDENT' or tokens[0][1] != 'transparent'):\n        raise ParseException('Hash or transparent value only for property \"%(property)s\"' % locals(), line, col)\n\n    if tokens[0][0] == 'HASH':\n        if not re.match(r'^#([0-9a-f]{3}){1,2}$', tokens[0][1], re.I):\n            raise ParseException('Unrecognized color value for property \"%(property)s\"' % locals(), line, col)\n\n        hex = tokens[0][1][1:]\n        \n        if len(hex) == 3:\n            hex = hex[0]+hex[0] + hex[1]+hex[1] + hex[2]+hex[2]\n        \n        rgb = (ord(unhex(h)) for h in (hex[0:2], hex[2:4], hex[4:6]))\n        \n        value = color(*rgb)\n\n    else:\n        value = 'transparent'\n\nelif properties[property.name] is color:\n    if tokens[0][0] != 'HASH':\n        raise ParseException('Hash value only for property \"%(property)s\"' % locals(), line, col)\n\n    if not re.match(r'^#([0-9a-f]{3}){1,2}$', tokens[0][1], re.I):\n        raise ParseException('Unrecognized color value for property \"%(property)s\"' % locals(), line, col)\n\n    hex = tokens[0][1][1:]\n    \n    if len(hex) == 3:\n        hex = hex[0]+hex[0] + hex[1]+hex[1] + hex[2]+hex[2]\n    \n    rgb = (ord(unhex(h)) for h in (hex[0:2], hex[2:4], hex[4:6]))\n    \n    value = color(*rgb)\n\nelif properties[property.name] is uri:\n    if tokens[0][0] != 'URI':\n        raise ParseException('URI value only for property \"%(property)s\"' % locals(), line, col)\n\n    raw = tokens[0][1]\n\n    if raw.startswith('url(\"') and raw.endswith('\")'):\n        raw = raw[5:-2]\n        \n    elif raw.startswith(\"url('\") and raw.endswith(\"')\"):\n        raw = raw[5:-2]\n        \n    elif raw.startswith('url(') and raw.endswith(')'):\n        raw = raw[4:-1]\n\n    value = uri(raw, base)\n        \nelif properties[property.name] is boolean:\n    if tokens[0][0] != 'IDENT' or tokens[0][1] not in ('true', 'false'):\n        raise ParseException('true/false value only for property \"%(property)s\"' % locals(), line, col)\n\n    value = boolean(tokens[0][1] == 'true')\n        \nelif type(properties[property.name]) is tuple:\n    if tokens[0][0] != 'IDENT':\n        raise ParseException('Identifier value only for property \"%(property)s\"' % locals(), line, col)\n\n    if tokens[0][1] not in properties[property.name]:\n        raise ParseException('Unrecognized value for property \"%(property)s\"' % locals(), line, col)\n\n    value = tokens[0][1]\n        \nelif properties[property.name] is numbers:\n    values = []\n    \n    # strip the list down to what we think goes number, comma, number, etc.\n    relevant_tokens = [token for token in tokens\n                       if token[0] == 'NUMBER' or token == ('CHAR', ',')]\n    \n    for (i, token) in enumerate(relevant_tokens):\n        if (i % 2) == 0 and token[0] == 'NUMBER':\n            try:\n                value = int(token[1])\n            except ValueError:\n                value = float(token[1])\n\n            values.append(value)\n\n        elif (i % 2) == 1 and token[0] == 'CHAR':\n            # fine, it's a comma\n            continue\n\n        else:\n            raise ParseException('Value for property \"%(property)s\" should be a comma-delimited list of numbers' % locals(), line, col)\n\n    value = numbers(*values)\n\nreturn Value(value, important)", "path": "tags\\cascadenik\\0.1.0\\cascadenik\\style.py", "repo_name": "umidev/mapnik-utils", "stars": 14, "license": "None", "language": "python", "size": 67496}
{"docstring": "\"\"\"Return a static resource from the js folder.\"\"\"\n", "func_signal": "def get_resource(request, filename):\n", "code": "if not filename:\n  return z.Response('Not Found no filename', status=404)\nelse:\n  filename = os.path.join(os.path.dirname(__file__), 'js', filename)\n\nif os.path.isfile(filename):\n    mimetype = guess_type(filename)[0] or 'application/octet-stream'\n    f = file(filename, 'rb')\n    try:\n        return z.Response(f.read(), mimetype=mimetype)\n    finally:\n        f.close()\nreturn z.Response('Not Found: %s' % filename, status=404)", "path": "sandbox\\wsgi_fun\\localserver_mapfish.py", "repo_name": "umidev/mapnik-utils", "stars": 14, "license": "None", "language": "python", "size": 67496}
{"docstring": "\"\"\"\nRoutine to render the requested AGG format.\n\"\"\"\n", "func_signal": "def render_to_file(self,*args):\n", "code": "self.timer()\nmapnik.render_to_file(*args)\nself.stop()\nif self.world_file_ext:\n    self.write_wld(args[1])", "path": "tags\\nik2img\\0.5.0\\mapnik_utils\\renderer.py", "repo_name": "umidev/mapnik-utils", "stars": 14, "license": "None", "language": "python", "size": 67496}
{"docstring": "\"\"\"\n\"\"\"\n", "func_signal": "def testCompile3(self):\n", "code": "map = output.Map(layers=[\n    output.Layer('this',\n    output.Datasource(), [\n        output.Style('a style', [\n            output.Rule(\n                output.MinScaleDenominator(1),\n                output.MaxScaleDenominator(100),\n                output.Filter(\"[this] = 'that'\"),\n                [\n                    output.PolygonSymbolizer(color(0xCC, 0xCC, 0xCC))\n                ])\n            ])\n        ]),\n    output.Layer('that',\n    output.Datasource(), [\n        output.Style('another style', [\n            output.Rule(\n                output.MinScaleDenominator(101),\n                output.MaxScaleDenominator(200),\n                output.Filter(\"[this] = 2\"),\n                [\n                    output.PolygonSymbolizer(color(0x33, 0x33, 0x33)),\n                    output.LineSymbolizer(color(0x66, 0x66, 0x66), 2)\n                ])\n            ])\n        ])\n    ])\n\nimport mapnik\n\nmmap = mapnik.Map(640, 480)\nmap.to_mapnik(mmap)\n\n(handle, path) = tempfile.mkstemp(suffix='.xml', prefix='cascadenik-mapnik-')\nos.close(handle)\n\nmapnik.save_map(mmap, path)\ndoc = xml.etree.ElementTree.parse(path)\nmap_el = doc.getroot()\n\n# print open(path, 'r').read()\nos.unlink(path)\n\nself.assertEqual(2, len(map_el.findall('Style')))\nself.assertEqual(2, len(map_el.findall('Layer')))\n\nfor layer_el in map_el.findall('Layer'):\n    self.assertEqual(1, len(layer_el.findall('StyleName')))\n    self.assertTrue(layer_el.find('StyleName').text in [style_el.get('name') for style_el in map_el.findall('Style')])\n\nfor style_el in map_el.findall('Style'):\n    if style_el.get('name') == 'a style':\n        self.assertEqual(\"([this]='that')\", style_el.find('Rule').find('Filter').text)\n        self.assertEqual('1', style_el.find('Rule').find('MinScaleDenominator').text)\n        self.assertEqual('100', style_el.find('Rule').find('MaxScaleDenominator').text)\n        self.assertEqual(1, len(style_el.find('Rule').findall('PolygonSymbolizer')))\n\n    if style_el.get('name') == 'another style':\n        self.assertEqual('([this]=2)', style_el.find('Rule').find('Filter').text)\n        self.assertEqual('101', style_el.find('Rule').find('MinScaleDenominator').text)\n        self.assertEqual('200', style_el.find('Rule').find('MaxScaleDenominator').text)\n        self.assertEqual(1, len(style_el.find('Rule').findall('PolygonSymbolizer')))\n        self.assertEqual(1, len(style_el.find('Rule').findall('LineSymbolizer')))", "path": "branches\\cascadenik-xmlbad\\tests\\test.py", "repo_name": "umidev/mapnik-utils", "stars": 14, "license": "None", "language": "python", "size": 67496}
{"docstring": "\"\"\" Loosely based on http://www.w3.org/TR/REC-CSS2/cascade.html#specificity\n\"\"\"\n", "func_signal": "def specificity(self):\n", "code": "ids = sum(a.countIDs() for a in self.elements)\nnon_ids = sum((a.countNames() - a.countIDs()) for a in self.elements)\ntests = sum(len(a.tests) for a in self.elements)\n\nreturn (ids, non_ids, tests)", "path": "tags\\cascadenik\\0.1.0\\cascadenik\\style.py", "repo_name": "umidev/mapnik-utils", "stars": 14, "license": "None", "language": "python", "size": 67496}
{"docstring": "\"\"\"\n\"\"\"\n", "func_signal": "def inRange(self, value):\n", "code": "for test in self.rangeTests():\n    if not test.inRange(value):\n        return False\n\nreturn True", "path": "tags\\cascadenik\\0.1.0\\cascadenik\\style.py", "repo_name": "umidev/mapnik-utils", "stars": 14, "license": "None", "language": "python", "size": 67496}
{"docstring": "\"\"\"\nAccepts an integer key for one of several color choices along with the text string to color\n  keys = 1:red, 2:green, 3:yellow, 4: dark blue, 5:pink, 6:teal blue, 7:white\nReturns a colored string of text.\n\"\"\"\n", "func_signal": "def color_text(color, text, no_color=False):\n", "code": "if not os.name == 'nt' and not no_color:\n    return \"\\033[9%sm%s\\033[0m\" % (color,text)\nelse:\n    return text", "path": "tags\\nik2img\\0.4.0\\nik2img.py", "repo_name": "umidev/mapnik-utils", "stars": 14, "license": "None", "language": "python", "size": 67496}
{"docstring": "\"\"\" Convert a list of rulesets (as returned by stylesheet_rulesets)\n    into an ordered list of individual selectors and declarations.\n\"\"\"\n", "func_signal": "def rulesets_declarations(rulesets):\n", "code": "declarations = []\n\nfor ruleset in rulesets:\n    for declaration in ruleset['declarations']:\n        for selector in ruleset['selectors']:\n            declarations.append(Declaration(selector, declaration['property'], declaration['value'],\n                                            (declaration['value'].importance(), selector.specificity(), declaration['position'])))\n\n# sort by a css-like method\nreturn sorted(declarations, key=operator.attrgetter('sort_key'))", "path": "tags\\cascadenik\\0.1.0\\cascadenik\\style.py", "repo_name": "umidev/mapnik-utils", "stars": 14, "license": "None", "language": "python", "size": 67496}
{"docstring": "\"\"\"\n\"\"\"\n", "func_signal": "def testCompile2(self):\n", "code": "s = \"\"\"<?xml version=\"1.0\"?>\n    <Map>\n        <Stylesheet>\n            Map { map-bgcolor: #fff; }\n            \n            Layer\n            {\n                polygon-fill: #999;\n                polygon-opacity: 0.5;\n                line-color: #fff;\n                line-width: 2;\n                outline-color: #000;\n                outline-width: 1;\n            }\n            \n            Layer name\n            {\n                text-face-name: 'Comic Sans';\n                text-size: 14;\n                text-fill: #f90;\n            }\n        </Stylesheet>\n        <Layer>\n            <Datasource>\n                <Parameter name=\"plugin_name\">example</Parameter>\n            </Datasource>\n        </Layer>\n    </Map>\n\"\"\"\nmap = compile(s, self.tmpdir)\n\nimport mapnik\n\nmmap = mapnik.Map(640, 480)\nmap.to_mapnik(mmap)\n\n(handle, path) = tempfile.mkstemp(suffix='.xml', prefix='cascadenik-mapnik-')\nos.close(handle)\n\nmapnik.save_map(mmap, path)\ndoc = xml.etree.ElementTree.parse(path)\nmap_el = doc.getroot()\n\n# print open(path, 'r').read()\nos.unlink(path)\n\nself.assertEqual(3, len(map_el.findall('Style')))\nself.assertEqual(1, len(map_el.findall('Layer')))\nself.assertEqual(3, len(map_el.find('Layer').findall('StyleName')))\n\nfor stylename_el in map_el.find('Layer').findall('StyleName'):\n    self.assertTrue(stylename_el.text in [style_el.get('name') for style_el in map_el.findall('Style')])\n\nfor style_el in map_el.findall('Style'):\n    if style_el.get('name').startswith('polygon style '):\n        self.assertEqual(1, len(style_el.find('Rule').findall('PolygonSymbolizer')))\n\n    if style_el.get('name').startswith('line style '):\n        self.assertEqual(2, len(style_el.find('Rule').findall('LineSymbolizer')))\n\n    if style_el.get('name').startswith('text style '):\n        self.assertEqual(1, len(style_el.find('Rule').findall('TextSymbolizer')))", "path": "branches\\cascadenik-xmlbad\\tests\\test.py", "repo_name": "umidev/mapnik-utils", "stars": 14, "license": "None", "language": "python", "size": 67496}
{"docstring": "\"\"\"Begin reading RSS feeds\"\"\"\n", "func_signal": "def startrss(phenny, input):\n", "code": "global first_run, restarted, DEBUG, INTERVAL\n\nquery = input.group(2)\nif query == '-v':\n    DEBUG=True\nif query == '-q':\n    DEBUG=False\nif query == '-i':\n    INVERVAL = input.group(3)\n\nif first_run:\n  \n    if DEBUG:\n        phenny.say(\"Okay, I'll start rss fetching...\")\n    first_run = False\n    \n    for url in urls:\n        entry = Feed()\n        entry.url = url\n        feeds.append(entry)\nelse:\n    restarted = True      \n    if DEBUG:\n        phenny.say(\"Okay, I'll re-start rss...\")\n        \nwhile True:\n    if DEBUG:\n        phenny.say(\"Rechecking feeds\")\n    read_feeds(phenny)\n    time.sleep(INTERVAL)\n    \nif DEBUG:\n    phenny.say(\"Stopped checking\")", "path": "sandbox\\tools\\irc_tools\\mapnikrss.py", "repo_name": "umidev/mapnik-utils", "stars": 14, "license": "None", "language": "python", "size": 67496}
{"docstring": "\"\"\"\nAbstraction wrapper to allow for calling \nany requested AGG Formats.\n\"\"\"\n", "func_signal": "def call_agg(self, basename):\n", "code": "for k, v in self.AGG_FORMATS.iteritems():\n    path = '%s_%s.%s' % (basename,k,v)\n    self.render_to_file(self.m,path,k)", "path": "tags\\nik2img\\0.5.0\\mapnik_utils\\renderer.py", "repo_name": "umidev/mapnik-utils", "stars": 14, "license": "None", "language": "python", "size": 67496}
{"docstring": "\"\"\"\nOutputs an ESRI world file that can be used to load the resulting\nimage as a georeferenced raster in a variety of gis viewers.\n\n'.wld' is the most common extension used, but format-specific extensions\nare also looked for by some software, such as '.tfw' for tiff and '.pgw' for png\n\nA world file file is a plain ASCII text file consisting of six values separated\nby newlines. The format is: \n    pixel X size\n    rotation about the Y axis (usually 0.0)\n    rotation about the X axis (usually 0.0)\n    pixel Y size (negative when using North-Up data)\n    X coordinate of upper left pixel center\n    Y coordinate of upper left pixel center\n \nInfo from: http://gdal.osgeo.org/frmt_various.html#WLD\n\"\"\"\n", "func_signal": "def to_wld(self, x_rotation=0.0, y_rotation=0.0):\n", "code": "extent = self.envelope()\npixel_x_size = (extent.maxx - extent.minx)/self.width\npixel_y_size = (extent.maxy - extent.miny)/self.height\nupper_left_x_center = extent.minx + 0.5 * pixel_x_size + 0.5 * x_rotation\nupper_left_y_center = extent.maxy + 0.5 * (pixel_y_size*-1) + 0.5 * y_rotation\n# http://trac.osgeo.org/gdal/browser/trunk/gdal/gcore/gdal_misc.cpp#L1296\nwld_string = '''%.10f\\n%.10f\\n%.10f\\n-%.10f\\n%.10f\\n%.10f\\n''' % (\n        pixel_x_size, # geotransform[1] - width of pixel\n        y_rotation, # geotransform[4] - rotational coefficient, zero for north up images.\n        x_rotation, # geotransform[2] - rotational coefficient, zero for north up images.\n        pixel_y_size, # geotransform[5] - height of pixel (but negative)\n        upper_left_x_center, # geotransform[0] - x offset to center of top left pixel\n        upper_left_y_center # geotransform[3] - y offset to center of top left pixel.\n    )\nreturn wld_string", "path": "tags\\nik2img\\0.5.1\\mapnik_utils\\metaclass_injectors.py", "repo_name": "umidev/mapnik-utils", "stars": 14, "license": "None", "language": "python", "size": 67496}
{"docstring": "\"\"\"This function is designed to work around the fact that Python\n   in Windows does not handle binary output correctly. This function\n   will set the output to binary, and then write to stdout directly\n   rather than using print.\"\"\"\n", "func_signal": "def binaryPrint(binary_data):\n", "code": "try:\n    import msvcrt\n    msvcrt.setmode(sys.__stdout__.fileno(), os.O_BINARY)\nexcept:\n    pass\nsys.stdout.write(binary_data)", "path": "tags\\nik2img\\0.5.0\\mapnik_utils\\renderer.py", "repo_name": "umidev/mapnik-utils", "stars": 14, "license": "None", "language": "python", "size": 67496}
{"docstring": "\"\"\" Modify the tests on this selector to use mapnik-friendly\n    scale-denominator instead of shorthand zoom.\n\"\"\"\n# somewhat-fudged values for mapniks' scale denominator at a range\n# of zoom levels when using the Google/VEarth mercator projection.\n", "func_signal": "def convertZoomTests(self):\n", "code": "zooms = {\n     1: (200000000, 500000000),\n     2: (100000000, 200000000),\n     3: (50000000, 100000000),\n     4: (25000000, 50000000),\n     5: (12500000, 25000000),\n     6: (6500000, 12500000),\n     7: (3000000, 6500000),\n     8: (1500000, 3000000),\n     9: (750000, 1500000),\n    10: (400000, 750000),\n    11: (200000, 400000),\n    12: (100000, 200000),\n    13: (50000, 100000),\n    14: (25000, 50000),\n    15: (12500, 25000),\n    16: (5000, 12500),\n    17: (2500, 5000),\n    18: (1000, 2500)\n    }\n\nfor test in self.elements[0].tests:\n    if test.property == 'zoom':\n        test.property = 'scale-denominator'\n\n        if test.op == '=':\n            # zoom level equality implies two tests, so we add one and modify one\n            self.elements[0].addTest(SelectorAttributeTest('scale-denominator', '<', max(zooms[test.value])))\n            test.op, test.value = '>=', min(zooms[test.value])\n\n        elif test.op == '<':\n            test.op, test.value = '>=', max(zooms[test.value])\n        elif test.op == '<=':\n            test.op, test.value = '>=', min(zooms[test.value])\n        elif test.op == '>=':\n            test.op, test.value = '<', max(zooms[test.value])\n        elif test.op == '>':\n            test.op, test.value = '<', min(zooms[test.value])", "path": "tags\\cascadenik\\0.1.0\\cascadenik\\style.py", "repo_name": "umidev/mapnik-utils", "stars": 14, "license": "None", "language": "python", "size": 67496}
{"docstring": "# Layer[scale-denominator>1000][bar>1]\n", "func_signal": "def testCompatibility15(self):\n", "code": "s = Selector(SelectorElement(['Layer'], [SelectorAttributeTest('scale-denominator', '>', 1000), SelectorAttributeTest('bar', '<', 3)]))\n\n# [bar>=3][baz=quux][foo>1][scale-denominator>1000]\nf = Filter(SelectorAttributeTest('scale-denominator', '>', 1000), SelectorAttributeTest('bar', '>=', 3), SelectorAttributeTest('foo', '>', 1), SelectorAttributeTest('baz', '=', 'quux'))\n\nassert not is_applicable_selector(s, f)", "path": "branches\\cascadenik-xmlbad\\tests\\test.py", "repo_name": "umidev/mapnik-utils", "stars": 14, "license": "None", "language": "python", "size": 67496}
{"docstring": "\"\"\" Convert a one-element list of tokens into a Property.\n\"\"\"\n", "func_signal": "def postprocess_property(tokens, line=0, col=0):\n", "code": "tokens = trim_extra(tokens)\n\nif len(tokens) != 1:\n    raise ParseException('Too many tokens in property: ' + repr(tokens), line, col)\n\nif tokens[0][0] != 'IDENT':\n    raise ParseException('Incorrect type of token in property: ' + repr(tokens), line, col)\n\nif tokens[0][1] not in properties:\n    raise ParseException('\"%s\" is not a recognized property name' % tokens[0][1], line, col)\n\nreturn Property(tokens[0][1])", "path": "tags\\cascadenik\\0.1.0\\cascadenik\\style.py", "repo_name": "umidev/mapnik-utils", "stars": 14, "license": "None", "language": "python", "size": 67496}
{"docstring": "\"\"\"\nAbstraction wrapper to allow for the same call\nto any image and file formats requested from Cairo.\n\"\"\"\n", "func_signal": "def call_cairo(self, basename):\n", "code": "if not HAS_CAIRO:\n    raise ImportError('PyCairo is not installed or available, therefore you cannot write to svg, pdf, ps, or cairo-rendered png')\nelse:\n    for k, v in self.CAIRO_FILE_FORMATS.iteritems():\n        path = '%s_%s.%s' % (basename,k,v)\n        self.render_with_pycairo(self.m,path,k)\n    for k, v in self.CAIRO_IMAGE_FORMATS.iteritems():\n        path = '%s_%s.%s' % (basename,k,v)\n        self.render_with_pycairo(self.m,path,k)", "path": "tags\\nik2img\\0.5.0\\mapnik_utils\\renderer.py", "repo_name": "umidev/mapnik-utils", "stars": 14, "license": "None", "language": "python", "size": 67496}
{"docstring": "\"\"\"Skips all whitespace, including comments and unicode whitespace\n\nTakes a string and a starting index, and returns the index of the\nnext non-whitespace character.\n\nIf skip_comments is True and not running in strict JSON mode, then\ncomments will be skipped over just like whitespace.\n\n\"\"\"\n", "func_signal": "def skipws_any(self, txt, i=0, imax=None, skip_comments=True):\n", "code": "if imax is None:\n    imax = len(txt)\nwhile i < imax:\n    if txt[i] == '/':\n        cmt, i = self.skip_comment(txt, i)\n    if i < imax and self.isws(txt[i]):\n        i += 1\n    else:\n        break\nreturn i", "path": "src\\demjson.py", "repo_name": "nevyn/deathtroid", "stars": 11, "license": "None", "language": "python", "size": 7536}
{"docstring": "\"\"\"Determines if the given character is considered a line terminator.\n\nRef. ECMAScript section 7.3\n\n\"\"\"\n", "func_signal": "def islineterm(self, c):\n", "code": "if c == '\\r' or c == '\\n':\n    return True\nif c == u'\\u2028' or c == u'\\u2029': # unicodedata.category(c) in  ['Zl', 'Zp']\n    return True\nreturn False", "path": "src\\demjson.py", "repo_name": "nevyn/deathtroid", "stars": 11, "license": "None", "language": "python", "size": 7536}
{"docstring": "\"\"\"Intermediate-level decoder for JSON numeric literals.\n\nTakes a string and a starting index, and returns a Python\nsuitable numeric type and the index of the next unparsed character.\n\nThe returned numeric type can be either of a Python int,\nlong, or float.  In addition some special non-numbers may\nalso be returned such as nan, inf, and neginf (technically\nwhich are Python floats, but have no numeric value.)\n\nRef. ECMAScript section 8.5.\n\n\"\"\"\n", "func_signal": "def decode_number(self, s, i=0, imax=None):\n", "code": "if imax is None:\n    imax = len(s)\n# Detect initial sign character(s)\nif not self._allow_all_numeric_signs:\n    if s[i] == '+' or (s[i] == '-' and i+1 < imax and \\\n                       s[i+1] in '+-'):\n        raise JSONDecodeError('numbers in strict JSON may only have a single \"-\" as a sign prefix',s[i:])\nsign = +1\nj = i  # j will point after the sign prefix\nwhile j < imax and s[j] in '+-':\n    if s[j] == '-': sign = sign * -1\n    j += 1\n# Check for ECMAScript symbolic non-numbers\nif s[j:j+3] == 'NaN':\n    if self._allow_non_numbers:\n        return nan, j+3\n    else:\n        raise JSONDecodeError('NaN literals are not allowed in strict JSON')\nelif s[j:j+8] == 'Infinity':\n    if self._allow_non_numbers:\n        if sign < 0:\n            return neginf, j+8\n        else:\n            return inf, j+8\n    else:\n        raise JSONDecodeError('Infinity literals are not allowed in strict JSON')\nelif s[j:j+2] in ('0x','0X'):\n    if self._allow_hex_numbers:\n        k = j+2\n        while k < imax and s[k] in hexdigits:\n            k += 1\n        n = sign * decode_hex( s[j+2:k] )\n        return n, k\n    else:\n        raise JSONDecodeError('hexadecimal literals are not allowed in strict JSON',s[i:])\nelse:\n    # Decimal (or octal) number, find end of number.\n    # General syntax is:  \\d+[\\.\\d+][e[+-]?\\d+]\n    k = j   # will point to end of digit sequence\n    could_be_octal = ( k+1 < imax and s[k] == '0' )  # first digit is 0\n    decpt = None  # index into number of the decimal point, if any\n    ept = None # index into number of the e|E exponent start, if any\n    esign = '+' # sign of exponent\n    sigdigits = 0 # number of significant digits (approx, counts end zeros)\n    while k < imax and (s[k].isdigit() or s[k] in '.+-eE'):\n        c = s[k]\n        if c not in octaldigits:\n            could_be_octal = False\n        if c == '.':\n            if decpt is not None or ept is not None:\n                break\n            else:\n                decpt = k-j\n        elif c in 'eE':\n            if ept is not None:\n                break\n            else:\n                ept = k-j\n        elif c in '+-':\n            if not ept:\n                break\n            esign = c\n        else: #digit\n            if not ept:\n                sigdigits += 1\n        k += 1\n    number = s[j:k]  # The entire number as a string\n    #print 'NUMBER IS: ', repr(number), ', sign', sign, ', esign', esign, \\\n    #      ', sigdigits', sigdigits, \\\n    #      ', decpt', decpt, ', ept', ept\n\n    # Handle octal integers first as an exception.  If octal\n    # is not enabled (the ECMAScipt standard) then just do\n    # nothing and treat the string as a decimal number.\n    if could_be_octal and self._allow_octal_numbers:\n        n = sign * decode_octal( number )\n        return n, k\n\n    # A decimal number.  Do a quick check on JSON syntax restrictions.\n    if number[0] == '.' and not self._allow_initial_decimal_point:\n        raise JSONDecodeError('numbers in strict JSON must have at least one digit before the decimal point',s[i:])\n    elif number[0] == '0' and \\\n             len(number) > 1 and number[1].isdigit():\n        if self._allow_octal_numbers:\n            raise JSONDecodeError('initial zero digit is only allowed for octal integers',s[i:])\n        else:\n            raise JSONDecodeError('initial zero digit must not be followed by other digits (octal numbers are not permitted)',s[i:])\n    # Make sure decimal point is followed by a digit\n    if decpt is not None:\n        if decpt+1 >= len(number) or not number[decpt+1].isdigit():\n            raise JSONDecodeError('decimal point must be followed by at least one digit',s[i:])\n    # Determine the exponential part\n    if ept is not None:\n        if ept+1 >= len(number):\n            raise JSONDecodeError('exponent in number is truncated',s[i:])\n        try:\n            exponent = int(number[ept+1:])\n        except ValueError:\n            raise JSONDecodeError('not a valid exponent in number',s[i:])\n        ##print 'EXPONENT', exponent\n    else:\n        exponent = 0\n    # Try to make an int/long first.\n    if decpt is None and exponent >= 0:\n        # An integer\n        if ept:\n            n = int(number[:ept])\n        else:\n            n = int(number)\n        n *= sign\n        if exponent:\n            n *= 10**exponent\n        if n == 0 and sign < 0:\n            # minus zero, must preserve negative sign so make a float\n            n = -0.0\n    else:\n        try:\n            if decimal and (abs(exponent) > float_maxexp or sigdigits > float_sigdigits):\n                try:\n                    n = decimal.Decimal(number)\n                    n = n.normalize()\n                except decimal.Overflow:\n                    if sign<0:\n                        n = neginf\n                    else:\n                        n = inf\n                else:\n                    n *= sign\n            else:\n                n = float(number) * sign\n        except ValueError:\n            raise JSONDecodeError('not a valid JSON numeric literal', s[i:j])\n    return n, k", "path": "src\\demjson.py", "repo_name": "nevyn/deathtroid", "stars": 11, "license": "None", "language": "python", "size": 7536}
{"docstring": "#format: <byte size><data><byte size><data>\n", "func_signal": "def dnstxt_to_strings(txt):\n", "code": "strings = []\nwhile len(txt) > 1:\n  size = ord(txt[0])\n  str = txt[1:size+1]\n  strings.append(str)\n  txt = txt[size+1:]\nreturn strings", "path": "src\\zeroconf.py", "repo_name": "nevyn/deathtroid", "stars": 11, "license": "None", "language": "python", "size": 7536}
{"docstring": "\"\"\"This method is used to encode user-defined class objects.\n\nThe object being encoded should have a json_equivalent()\nmethod defined which returns another equivalent object which\nis easily JSON-encoded.  If the object in question has no\njson_equivalent() method available then None is returned\ninstead of a string so that the encoding will attempt the next\nstrategy.\n\nIf a caller wishes to disable the calling of json_equivalent()\nmethods, then subclass this class and override this method\nto just return None.\n\n\"\"\"\n", "func_signal": "def encode_equivalent( self, obj, nest_level=0 ):\n", "code": "if hasattr(obj, 'json_equivalent') \\\n       and callable(getattr(obj,'json_equivalent')):\n    obj2 = obj.json_equivalent()\n    if obj2 is obj:\n        # Try to prevent careless infinite recursion\n        raise JSONEncodeError('object has a json_equivalent() method that returns itself',obj)\n    json2 = self.encode( obj2, nest_level=nest_level )\n    return json2\nelse:\n    return None", "path": "src\\demjson.py", "repo_name": "nevyn/deathtroid", "stars": 11, "license": "None", "language": "python", "size": 7536}
{"docstring": "\"\"\"Decodes a UTF-32BE byte string into a Unicode string.\"\"\"\n", "func_signal": "def utf32be_decode( obj, errors='strict' ):\n", "code": "if len(obj) % 4 != 0:\n    raise UnicodeError('UTF-32 decode error, data length not a multiple of 4 bytes')\nimport struct\nunpack = struct.unpack\nchars = []\ni = 0\nfor i in range(0, len(obj), 4):\n    seq = obj[i:i+4]\n    n = unpack('>L',seq)[0]\n    chars.append( unichr(n) )\nreturn u''.join( chars )", "path": "src\\demjson.py", "repo_name": "nevyn/deathtroid", "stars": 11, "license": "None", "language": "python", "size": 7536}
{"docstring": "\"\"\"Intermediate-level decoder for JSON string literals.\n\nTakes a string and a starting index, and returns a Python\nstring (or unicode string) and the index of the next unparsed\ncharacter.\n\n\"\"\"\n", "func_signal": "def decode_string(self, s, i=0, imax=None):\n", "code": "if imax is None:\n    imax = len(s)\nif imax < i+2 or s[i] not in '\"\\'':\n    raise JSONDecodeError('string literal must be properly quoted',s[i:])\ncloser = s[i]\nif closer == '\\'' and not self._allow_single_quoted_strings:\n    raise JSONDecodeError('string literals must use double quotation marks in strict JSON',s[i:])\ni += 1 # skip quote\nif self._allow_js_string_escapes:\n    escapes = self._escapes_js\nelse:\n    escapes = self._escapes_json\nccallowed = self._allow_control_char_in_string\nchunks = []\n_append = chunks.append\ndone = False\nhigh_surrogate = None\nwhile i < imax:\n    c = s[i]\n    # Make sure a high surrogate is immediately followed by a low surrogate\n    if high_surrogate and (i+1 >= imax or s[i:i+2] != '\\\\u'):\n        raise JSONDecodeError('High unicode surrogate must be followed by a low surrogate',s[i:])\n    if c == closer:\n        i += 1 # skip end quote\n        done = True\n        break\n    elif c == '\\\\':\n        # Escaped character\n        i += 1\n        if i >= imax:\n            raise JSONDecodeError('escape in string literal is incomplete',s[i-1:])\n        c = s[i]\n\n        if '0' <= c <= '7' and self._allow_octal_numbers:\n            # Handle octal escape codes first so special \\0 doesn't kick in yet.\n            # Follow Annex B.1.2 of ECMAScript standard.\n            if '0' <= c <= '3':\n                maxdigits = 3\n            else:\n                maxdigits = 2\n            for k in range(i, i+maxdigits+1):\n                if k >= imax or s[k] not in octaldigits:\n                    break\n            n = decode_octal(s[i:k])\n            if n < 128:\n                _append( chr(n) )\n            else:\n                _append( unichr(n) )\n            i = k\n            continue\n\n        if escapes.has_key(c):\n            _append(escapes[c])\n            i += 1\n        elif c == 'u' or c == 'x':\n            i += 1\n            if c == 'u':\n                digits = 4\n            else: # c== 'x'\n                if not self._allow_js_string_escapes:\n                    raise JSONDecodeError(r'string literals may not use the \\x hex-escape in strict JSON',s[i-1:])\n                digits = 2\n            if i+digits >= imax:\n                raise JSONDecodeError('numeric character escape sequence is truncated',s[i-1:])\n            n = decode_hex( s[i:i+digits] )\n            if high_surrogate:\n                # Decode surrogate pair and clear high surrogate\n                _append( surrogate_pair_as_unicode( high_surrogate, unichr(n) ) )\n                high_surrogate = None\n            elif n < 128:\n                # ASCII chars always go in as a str\n                _append( chr(n) )\n            elif 0xd800 <= n <= 0xdbff: # high surrogate\n                if imax < i + digits + 2 or s[i+digits] != '\\\\' or s[i+digits+1] != 'u':\n                    raise JSONDecodeError('High unicode surrogate must be followed by a low surrogate',s[i-2:])\n                high_surrogate = unichr(n)  # remember until we get to the low surrogate\n            elif 0xdc00 <= n <= 0xdfff: # low surrogate\n                raise JSONDecodeError('Low unicode surrogate must be proceeded by a high surrogate',s[i-2:])\n            else:\n                # Other chars go in as a unicode char\n                _append( unichr(n) )\n            i += digits\n        else:\n            # Unknown escape sequence\n            if self._allow_nonescape_characters:\n                _append( c )\n                i += 1\n            else:\n                raise JSONDecodeError('unsupported escape code in JSON string literal',s[i-1:])\n    elif ord(c) <= 0x1f: # A control character\n        if self.islineterm(c):\n            raise JSONDecodeError('line terminator characters must be escaped inside string literals',s[i:])\n        elif ccallowed:\n            _append( c )\n            i += 1\n        else:\n            raise JSONDecodeError('control characters must be escaped inside JSON string literals',s[i:])\n    else: # A normal character; not an escape sequence or end-quote.\n        # Find a whole sequence of \"safe\" characters so we can append them\n        # all at once rather than one a time, for speed.\n        j = i\n        i += 1\n        while i < imax and s[i] not in unsafe_string_chars and s[i] != closer:\n            i += 1\n        _append(s[j:i])\nif not done:\n    raise JSONDecodeError('string literal is not terminated with a quotation mark',s)\ns = ''.join( chunks )\nreturn s, i", "path": "src\\demjson.py", "repo_name": "nevyn/deathtroid", "stars": 11, "license": "None", "language": "python", "size": 7536}
{"docstring": "\"\"\"Filters out all Unicode format control characters from the string.\n\nECMAScript permits any Unicode \"format control characters\" to\nappear at any place in the source code.  They are to be\nignored as if they are not there before any other lexical\ntokenization occurs.  Note that JSON does not allow them.\n\nRef. ECMAScript section 7.1.\n\n\"\"\"\n", "func_signal": "def strip_format_control_chars(self, txt):\n", "code": "import unicodedata\ntxt2 = filter( lambda c: unicodedata.category(unicode(c)) != 'Cf',\n               txt )\nreturn txt2", "path": "src\\demjson.py", "repo_name": "nevyn/deathtroid", "stars": 11, "license": "None", "language": "python", "size": 7536}
{"docstring": "\"\"\"Is the object of a Python string type?\"\"\"\n", "func_signal": "def isstringtype( obj ):\n", "code": "if isinstance(obj, basestring):\n    return True\n# Must also check for some other pseudo-string types\nimport types, UserString\nreturn isinstance(obj, types.StringTypes) \\\n       or isinstance(obj, UserString.UserString) \\\n       or isinstance(obj, UserString.MutableString)", "path": "src\\demjson.py", "repo_name": "nevyn/deathtroid", "stars": 11, "license": "None", "language": "python", "size": 7536}
{"docstring": "#asyncore.loop(timeout=0.01, count=2)\n", "func_signal": "def poll(self, asdf, sdRef):\n", "code": "asyncore.poll(0.01)\n  \nif sdRef:\n  ready = select.select([sdRef], [], [], 0)\n  if sdRef in ready[0]:\n    pybonjour.DNSServiceProcessResult(sdRef)", "path": "src\\network\\blip_on_asyncore_on_pyglet.py", "repo_name": "nevyn/deathtroid", "stars": 11, "license": "None", "language": "python", "size": 7536}
{"docstring": "\"\"\"docstring for __init\"\"\"\n", "func_signal": "def __init__(self, controller):\n", "code": "super(MenuScreen, self).__init__()\nself.controller = controller\nself.window = controller.window\nself.init()", "path": "src\\menu_view.py", "repo_name": "nevyn/deathtroid", "stars": 11, "license": "None", "language": "python", "size": 7536}
{"docstring": "\"\"\"Encodes the Python boolean into a JSON Boolean literal.\"\"\"\n", "func_signal": "def encode_boolean(self, b):\n", "code": "if bool(b):\n    return 'true'\nreturn 'false'", "path": "src\\demjson.py", "repo_name": "nevyn/deathtroid", "stars": 11, "license": "None", "language": "python", "size": 7536}
{"docstring": "\"\"\"Prevent the specified behavior (turn on a strictness check).\n\nThe list of all possible behaviors is available in the behaviors property.\nYou can see which behaviors are currently prevented by accessing the\nprevented_behaviors property.\n\n\"\"\"\n", "func_signal": "def prevent(self, behavior):\n", "code": "p = '_allow_' + behavior\nif hasattr(self, p):\n    setattr(self, p, False)\nelse:\n    raise AttributeError('Behavior is not known',behavior)", "path": "src\\demjson.py", "repo_name": "nevyn/deathtroid", "stars": 11, "license": "None", "language": "python", "size": 7536}
{"docstring": "# Enter host\n", "func_signal": "def init(self):\n", "code": "self.enter_host_image = pyglet.image.load('menu/enter_host.png')\n\nself.document2 = pyglet.text.document.FormattedDocument(\"localhost\")\nself.document2.set_style(0, 9, dict(color=(255, 255, 255, 255)))\nself.enter_host_layout = pyglet.text.layout.IncrementalTextLayout(self.document2, 100, 20)\nself.enter_host_layout.anchor_x=\"center\"\nself.enter_host_layout.view_x = 0\nself.enter_host_layout.x = 320\nself.enter_host_layout.y = 220\n\nself.caret2 = pyglet.text.caret.Caret(self.enter_host_layout, color=(1, 1, 1))\nself.caret2.visible = True\nself.caret2.position = len(self.document2.text)", "path": "src\\menu_view.py", "repo_name": "nevyn/deathtroid", "stars": 11, "license": "None", "language": "python", "size": 7536}
{"docstring": "\"\"\"Encodes the Python object into a JSON string representation.\n\nThis method will first attempt to encode an object by seeing\nif it has a json_equivalent() method.  If so than it will\ncall that method and then recursively attempt to encode\nthe object resulting from that call.\n\nNext it will attempt to determine if the object is a native\ntype or acts like a squence or dictionary.  If so it will\nencode that object directly.\n\nFinally, if no other strategy for encoding the object of that\ntype exists, it will call the encode_default() method.  That\nmethod currently raises an error, but it could be overridden\nby subclasses to provide a hook for extending the types which\ncan be encoded.\n\n\"\"\"\n", "func_signal": "def encode(self, obj, nest_level=0):\n", "code": "chunks = []\nself.encode_helper(chunks, obj, nest_level)\nreturn ''.join( chunks )", "path": "src\\demjson.py", "repo_name": "nevyn/deathtroid", "stars": 11, "license": "None", "language": "python", "size": 7536}
{"docstring": "#print 'encode_helper(chunklist=%r, obj=%r, nest_level=%r)'%(chunklist,obj,nest_level)\n", "func_signal": "def encode_helper(self, chunklist, obj, nest_level):\n", "code": "if hasattr(obj, 'json_equivalent'):\n    json = self.encode_equivalent( obj, nest_level=nest_level )\n    if json is not None:\n        chunklist.append( json )\n        return\nif obj is None:\n    chunklist.append( self.encode_null() )\nelif obj is undefined:\n    if self._allow_undefined_values:\n        chunklist.append( self.encode_undefined() )\n    else:\n        raise JSONEncodeError('strict JSON does not permit \"undefined\" values')\nelif isinstance(obj, bool):\n    chunklist.append( self.encode_boolean(obj) )\nelif isinstance(obj, (int,long,float,complex)) or \\\n         (decimal and isinstance(obj, decimal.Decimal)):\n    chunklist.append( self.encode_number(obj) )\nelif isinstance(obj, basestring) or isstringtype(obj):\n    chunklist.append( self.encode_string(obj) )\nelse:\n    self.encode_composite(chunklist, obj, nest_level)", "path": "src\\demjson.py", "repo_name": "nevyn/deathtroid", "stars": 11, "license": "None", "language": "python", "size": 7536}
{"docstring": "\"\"\"Allow the specified behavior (turn off a strictness check).\n\nThe list of all possible behaviors is available in the behaviors property.\nYou can see which behaviors are currently allowed by accessing the\nallowed_behaviors property.\n\n\"\"\"\n", "func_signal": "def allow(self, behavior):\n", "code": "p = '_allow_' + behavior\nif hasattr(self, p):\n    setattr(self, p, True)\nelse:\n    raise AttributeError('Behavior is not known',behavior)", "path": "src\\demjson.py", "repo_name": "nevyn/deathtroid", "stars": 11, "license": "None", "language": "python", "size": 7536}
{"docstring": "\"\"\"Encodes a Python string into a JSON string literal.\n\n\"\"\"\n# Must handle instances of UserString specially in order to be\n# able to use ord() on it's simulated \"characters\".\n", "func_signal": "def encode_string(self, s):\n", "code": "import UserString\nif isinstance(s, (UserString.UserString, UserString.MutableString)):\n    def tochar(c):\n        return c.data\nelse:\n    # Could use \"lambda c:c\", but that is too slow.  So we set to None\n    # and use an explicit if test inside the loop.\n    tochar = None\n\nchunks = []\nchunks.append('\"')\nrevesc = self._rev_escapes\nasciiencodable = self._asciiencodable\nencunicode = self._encode_unicode_as_escapes\ni = 0\nimax = len(s)\nwhile i < imax:\n    if tochar:\n        c = tochar(s[i])\n    else:\n        c = s[i]\n    cord = ord(c)\n    if cord < 256 and asciiencodable[cord] and isinstance(encunicode, bool):\n        # Contiguous runs of plain old printable ASCII can be copied\n        # directly to the JSON output without worry (unless the user\n        # has supplied a custom is-encodable function).\n        j = i\n        i += 1\n        while i < imax:\n            if tochar:\n                c = tochar(s[i])\n            else:\n                c = s[i]\n            cord = ord(c)\n            if cord < 256 and asciiencodable[cord]:\n                i += 1\n            else:\n                break\n        chunks.append( unicode(s[j:i]) )\n    elif revesc.has_key(c):\n        # Has a shortcut escape sequence, like \"\\n\"\n        chunks.append(revesc[c])\n        i += 1\n    elif cord <= 0x1F:\n        # Always unicode escape ASCII-control characters\n        chunks.append(r'\\u%04x' % cord)\n        i += 1\n    elif 0xD800 <= cord <= 0xDFFF:\n        # A raw surrogate character!  This should never happen\n        # and there's no way to include it in the JSON output.\n        # So all we can do is complain.\n        cname = 'U+%04X' % cord\n        raise JSONEncodeError('can not include or escape a Unicode surrogate character',cname)\n    elif cord <= 0xFFFF:\n        # Other BMP Unicode character\n        if isinstance(encunicode, bool):\n            doesc = encunicode\n        else:\n            doesc = encunicode( c )\n        if doesc:\n            chunks.append(r'\\u%04x' % cord)\n        else:\n            chunks.append( c )\n        i += 1\n    else: # ord(c) >= 0x10000\n        # Non-BMP Unicode\n        if isinstance(encunicode, bool):\n            doesc = encunicode\n        else:\n            doesc = encunicode( c )\n        if doesc:\n            for surrogate in unicode_as_surrogate_pair(c):\n                chunks.append(r'\\u%04x' % ord(surrogate))\n        else:\n            chunks.append( c )\n        i += 1\nchunks.append('\"')\nreturn ''.join( chunks )", "path": "src\\demjson.py", "repo_name": "nevyn/deathtroid", "stars": 11, "license": "None", "language": "python", "size": 7536}
{"docstring": "\"\"\"Skips whitespace.\n\"\"\"\n", "func_signal": "def skipws(self, txt, i=0, imax=None, skip_comments=True):\n", "code": "if not self._allow_comments and not self._allow_unicode_whitespace:\n    if imax is None:\n        imax = len(txt)\n    while i < imax and txt[i] in ' \\r\\n\\t':\n        i += 1\n    return i\nelse:\n    return self.skipws_any(txt, i, imax, skip_comments)", "path": "src\\demjson.py", "repo_name": "nevyn/deathtroid", "stars": 11, "license": "None", "language": "python", "size": 7536}
{"docstring": "\"\"\"Is the object of a Python number type (excluding complex)?\"\"\"\n", "func_signal": "def isnumbertype( obj ):\n", "code": "return isinstance(obj, (int,long,float)) \\\n       and not isinstance(obj, bool) \\\n       or obj is nan or obj is inf or obj is neginf", "path": "src\\demjson.py", "repo_name": "nevyn/deathtroid", "stars": 11, "license": "None", "language": "python", "size": 7536}
{"docstring": "\"\"\"\nGets the virtual node that the key maps to\n\n:Parameters:\n    key : str\n        The key name\n:rtype: object\n:returns: The node corresponding to the key\n\"\"\"\n", "func_signal": "def get_node(self, key):\n", "code": "logging.debug('getting the node key=%s sorted keys=%s' % (key,self.sorted_keys))\nif len(self.sorted_keys) == 0:\n    raise exceptions.ValueError('ring is empty cannot get %s' % key)\n\n# Find the first key greater than the key of the input string\nhash_key = self._gen_key(key)\npos = self._get_pos(hash_key)\n\nreturn self.ring[self.sorted_keys[pos]]", "path": "dynamo\\lib\\consistent_hash\\consistent_hash.py", "repo_name": "jnovatnack/python-dynamo", "stars": 11, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\"\nReconciles the conflict between a number of values.  Note\nthat currently this defaults to taking the last written value.\nIn the future this will be expanded to allow application specific\nconflict resolution\n\n:Parameters:\n    result : list(tuples)\n        A list of result tuples from the persistence layer in the form\n        [(id, \"value\", \"date\"), ...]\n:rtype: tuple(int, str)\n:returns An id, string tuple of the chosen version\n\"\"\"\n", "func_signal": "def _reconcile_conflict(self, result):\n", "code": "last_result = None\nlast_date = None\nfor res in result:\n    if last_result is None:\n        last_result = res[1]\n        last_date = self._parse_date(res[2])  \n    else:\n        date = self._parse_date(res[2])\n        if date > last_date:\n            last_date = date\n            last_result = res[1]\n \nreturn (last_result, last_date)", "path": "dynamo\\storage\\storage_node.py", "repo_name": "jnovatnack/python-dynamo", "stars": 11, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\"\nGets a key\n\n:Parameters:\n    key : str\n        The key value\n\"\"\"\n", "func_signal": "def get(self, key):\n", "code": "logging.debug('Getting key=%s' % key)\n# Make sure I am supposed to have this key\nrespon_node = self.datastore_view.get_node(key)\nif respon_node != self.my_name:\n    logging.info(\"I'm not responsible for %s (%s vs %s)\" % (key, \n                                                            respon_node, \n                                                            self.my_name))\n    return None\n\n# Read it from the database\nresult = self.persis.get_key(key)\n\n# If the contexts don't line up then return the most recent\nvalue = None\nif len(result) == 1:\n    value = result[0][1]\nelse:\n    value = self._reconcile_conflict(result)[0]\n\nlogging.debug('Returning value=%s' % value)        \nreturn value", "path": "dynamo\\storage\\storage_node.py", "repo_name": "jnovatnack/python-dynamo", "stars": 11, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\"\nCloses the db connection\n\"\"\"\n", "func_signal": "def close(self):\n", "code": "if self.conn:\n    self.conn.close()", "path": "dynamo\\storage\\persistence\\sqlite_persistence_layer.py", "repo_name": "jnovatnack/python-dynamo", "stars": 11, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\"\nGets the hash key for a node and replication number given the\npartitioning strategy\n\n:Parameters:\n    node : object\n        A node\n    rep_num : int\n        The replication number\n\"\"\"\n", "func_signal": "def _get_node_hash_key(self, node, rep_num):\n", "code": "if self.strategy == self.STRATEGY1:\n    hash_key = random.randint(0, 2**128)\nelse:             \n    hash_key = self._gen_key(self.REPLICATION_STR % (node, rep_num))\nreturn hash_key", "path": "dynamo\\lib\\consistent_hash\\consistent_hash.py", "repo_name": "jnovatnack/python-dynamo", "stars": 11, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\"\nParameters:\n    servers : list(str)\n        A list of servers.  Each server name is in the \n        format {host/ip}:port\n    port : int\n        Port number to start on\n\"\"\"\n", "func_signal": "def __init__(self, servers, port):\n", "code": "self.port = int(port)\nself.server = None\nif servers is None:\n    servers = []\n\n# Add myself to the servers list\nself.my_name = str(self)\nservers.append(self.my_name)\nself.datastore_view = DataStoreView(servers)\n\n# Load the persistence layer\nself._load_persistence_layer()", "path": "dynamo\\storage\\storage_node.py", "repo_name": "jnovatnack/python-dynamo", "stars": 11, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\"\nEnsures The hash throws an exception when the same name is\nadded twice.\n\"\"\"\n", "func_signal": "def test_adding_same_node(self):\n", "code": "cons_hash = ConsistentHash(2)\ncons_hash.add('192.168.1.1')        \n\nthrew_value_error = False\ntry:\n    cons_hash.add('192.168.1.1')\nexcept exceptions.ValueError:\n    threw_value_error = True\nself.assertTrue(threw_value_error) \n\nself.assertTrue(cons_hash._is_consistent())", "path": "dynamo\\lib\\consistent_hash\\test\\test_consistent_hash.py", "repo_name": "jnovatnack/python-dynamo", "stars": 11, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\"\nEnsures that the sorted list of keys and hash keys are in\nagreement\n\n:rtype: bool\n:returns: True if the ring and sorted keys are consistent\n\"\"\"\n", "func_signal": "def _is_consistent(self):\n", "code": "len_consistency = len(self.ring) == len(self.sorted_keys)\nhash_keys = self.ring.keys()\nhash_keys.sort()\nkey_consistency = hash_keys == self.sorted_keys\n\nreturn len_consistency and key_consistency", "path": "dynamo\\lib\\consistent_hash\\consistent_hash.py", "repo_name": "jnovatnack/python-dynamo", "stars": 11, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\"\nBuilds a string representation of the storage node\n\n:rtype: str\n:returns: A string representation of the storage node \n\"\"\"\n", "func_signal": "def __str__(self):\n", "code": "if getattr(self, 'port'):\n    return '%s:%s' % (socket.gethostbyname(socket.gethostname()), self.port)\nelse:\n    return '%s' % socket.gethostbyname(socket.gethostname())", "path": "dynamo\\storage\\storage_node.py", "repo_name": "jnovatnack/python-dynamo", "stars": 11, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\"\nAdds a node to the hash.  \n\nPartitioning strategies:\n    DETERMINISTC : token is created from the node name.  Note that this can\n                   result in non-uniform node distributions, but is necessary \n                   with messaging between storage nodes.\n    STRATEGY1    : For each node self.replication_factor T tokens are added \n                   to the hash with the keys randomly chosen from the hash \n                   space.  This represents partition strategy 1 from\n                   \"Dynamo : amazons highly available key-value store\"\n\nNote that the node must support str().  \n\n:Parameters:\n    node : object\n        Any object that you wish to add to the hash\n\"\"\"\n", "func_signal": "def add(self, node, strategy='deterministic'):\n", "code": "if self.node_tokens.get(node):\n    raise exceptions.ValueError('Node %s already in the consistent hash' % node) \n\nfor i in xrange(0, self.replication_factor):\n    hash_key = self._get_node_hash_key(node, i)\n    self.node_tokens[node].append(hash_key)\n    self.ring[hash_key] = node\n\n    # Add the key to the sorted list.  If the position is 0 must \n    # disambiguate between this being the largest and smallest key            \n    pos = self._get_pos(hash_key)\n    if pos == 0 and self.sorted_keys and hash_key > self.sorted_keys[-1]:\n        self.sorted_keys.append(hash_key)\n    else:\n        self.sorted_keys.insert(pos, hash_key)", "path": "dynamo\\lib\\consistent_hash\\consistent_hash.py", "repo_name": "jnovatnack/python-dynamo", "stars": 11, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\"\nEnsures multiple values with the same key are reconciled by date.\n\"\"\"\n", "func_signal": "def test_get_date_reconcile(self):\n", "code": "result = self.sn.put(\"foo\", \"bar\")\nself.assertEquals(result, '200')\nresult = self.sn.put(\"foo\", \"bar2\")\nself.assertEquals(result, '200')\nresult = self.sn.put(\"foo\", \"bar3\")\nself.assertEquals(result, '200')\n\nresult = self.sn.get(\"foo\")\nself.assertEquals(result, \"bar3\")", "path": "dynamo\\storage\\test\\test_storage_node.py", "repo_name": "jnovatnack/python-dynamo", "stars": 11, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\"\nPuts a key in the database\n\n:Parameters:\n    key : str\n        The key name\n    data : str\n        The data string\n\"\"\"\n", "func_signal": "def put_key(self, key, data):\n", "code": "if not self.conn:\n    logging.info('SQLite connection not open')\n\ntry:\n    now = datetime.datetime.now()\n    cur = self.conn.cursor()\n    cur.execute(\"INSERT INTO key_values(key, value, date) VALUES (?, ?, ?)\",\n                (key, data, now))\n    self.conn.commit()\n    result = True\nexcept:\n    logging.error('Error putting key=%s, data=%s' % (key, data))\n    result = False\n    \nreturn result", "path": "dynamo\\storage\\persistence\\sqlite_persistence_layer.py", "repo_name": "jnovatnack/python-dynamo", "stars": 11, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\"\nPuts a key value in the datastore\n\n:Parameters:\n    key : str\n        The key name\n    value : str\n        The value\n    context : str\n        Should be only be None for now.  In the future an application will be\n        able to add a custom context string\n        \n:rtype: str\n:returns 200 if the operation succeeded, 400 otherwise\n\"\"\"\n# Make sure I am supposed to have this key\n", "func_signal": "def put(self, key, value, context=None):\n", "code": "if self.datastore_view.get_node(key) != self.my_name:\n    logging.info(\"I'm not responsible for %s\" % key)\n    return None\n\nres_code = None\ntry:\n    # Read it from the database\n    result = self.persis.put_key(key, value)\n    res_code = '200'\nexcept:\n    logging.error('Error putting key=%s value=%s into the persistence layer' % \n                  (key, value))\n    res_code = '400'\n\nreturn res_code", "path": "dynamo\\storage\\storage_node.py", "repo_name": "jnovatnack/python-dynamo", "stars": 11, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\"\nInitializes the persistence layer.\n\"\"\"\n", "func_signal": "def init_persistence(self):\n", "code": "logging.info('Conncting to sqlite db %s' % self.conn_str)\nself.conn = sqlite3.connect(self.conn_str)\n\ntry:\n    f = open(os.path.join(os.path.dirname(os.path.abspath(__file__)), \n                          self.SQL_FILE))\n    command = []\n    for line in f:\n        line = line.strip()\n        command.append(line)\n        \n        if line.endswith(';'):\n            command = ' '.join(command)\n            self.conn.execute(command)\n            command = []\n            \n    self.conn.commit()\nexcept:\n    logging.error('Error loading %s' % self.SQL_FILE)", "path": "dynamo\\storage\\persistence\\sqlite_persistence_layer.py", "repo_name": "jnovatnack/python-dynamo", "stars": 11, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\"\nRemoves a node from the hash\n\n:Parameters:\n    obj : object\n        Any object that you wish to delete from the hash\n\"\"\"\n", "func_signal": "def remove(self, node):\n", "code": "for token in self.node_tokens.get(node, []):\n    if token in self.ring:\n        del self.ring[token]\n        self.sorted_keys.remove(token)\n    else:\n        logging.info('%s not found in the consistent hash' % str(obj))\ndel self.node_tokens[node]", "path": "dynamo\\lib\\consistent_hash\\consistent_hash.py", "repo_name": "jnovatnack/python-dynamo", "stars": 11, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\"\nGiven a key returns its long using the md5 hash\n\n:Parameters:\n    key : str\n        A key\n:rtype: long\n:returns: A long\n\"\"\"\n", "func_signal": "def _gen_key(self, key):\n", "code": "m = md5.new()\nm.update(key)\nreturn long(m.hexdigest(), 16)", "path": "dynamo\\lib\\consistent_hash\\consistent_hash.py", "repo_name": "jnovatnack/python-dynamo", "stars": 11, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\"\nEnsures we can get a value we wrote to our data store\n\"\"\"\n", "func_signal": "def test_simple_get(self):\n", "code": "result = self.sn.put(\"foo\", \"bar\")\nself.assertEquals(result, '200')\n\nresult = self.sn.get(\"foo\")\nself.assertEquals(result, \"bar\")", "path": "dynamo\\storage\\test\\test_storage_node.py", "repo_name": "jnovatnack/python-dynamo", "stars": 11, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\"\nMain storage node loop\n\"\"\"\n", "func_signal": "def run(self):\n", "code": "self.server = SimpleXMLRPCServer(('', self.port), allow_none=True)\nself.server.register_function(self.get, \"get\")\nself.server.register_function(self.put, \"put\")  \nself.server.serve_forever()", "path": "dynamo\\storage\\storage_node.py", "repo_name": "jnovatnack/python-dynamo", "stars": 11, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\"\nReads a key from the db.\n\n:Parameters:\n    key : str      \n:rtype: list(tuple)\n:returns A list of (id, blob, datetime) tuples for the key\n\"\"\"\n", "func_signal": "def get_key(self, key):\n", "code": "if not self.conn:\n    logging.info('SQLite connection not open')\n    return []\n\ntry:\n    cur = self.conn.cursor()\n    cur.execute(\"SELECT id,value,date FROM key_values WHERE key=?\", (key,))\n    result = [row for row in cur]\nexcept:\n    logging.error('Error getting key=%s' % key)\n    raise\n    result = []\n    \nreturn result", "path": "dynamo\\storage\\persistence\\sqlite_persistence_layer.py", "repo_name": "jnovatnack/python-dynamo", "stars": 11, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\"\nParses an iso formatted date\n\n:Parameters:\n    datestr : str\n        An iso formatted date\n:rtype: datetime\n:returns A date object\n\"\"\"\n", "func_signal": "def _parse_date(self, datestr):\n", "code": "date_str, micros = datestr.split('.')\ndate = datetime.strptime(date_str, \"%Y-%m-%d %H:%M:%S\")\ndate += timedelta(microseconds=float(micros))\n\nreturn date", "path": "dynamo\\storage\\storage_node.py", "repo_name": "jnovatnack/python-dynamo", "stars": 11, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\"\n    Returns the dates and data portion of the TimeSeries \"zipped\" up in\n    a list of standard python objects (eg. datetime, int, etc...).\n\n\"\"\"\n", "func_signal": "def tolist(self):\n", "code": "if self.ndim > 0:\n    return zip(self.dates.tolist(), self.series.tolist())\nelse:\n    return self.series.tolist()", "path": "scikits\\timeseries\\tseries.py", "repo_name": "pierregm/scikits.timeseries", "stars": 14, "license": "other", "language": "python", "size": 428}
{"docstring": "\"\"\"\n    Sort the series by chronological order (in place).\n\n    Notes\n    -----\n    This method sorts the series **in place**.\n    To sort the a copy of the series, use the :func:`sort_chronologically`\n    function.\n\n    See Also\n    --------\n    sort_chronologically\nEquivalent function.\n\"\"\"\n", "func_signal": "def sort_chronologically(self):\n", "code": "_dates = self._dates\n_series = self._series\nif not _dates.is_chronological():\n    _cached = _dates._cachedinfo\n    idx = _cached['chronidx']\n    if not self._varshape:\n        flatseries = _series.flat\n        flatseries[:] = flatseries[idx]\n    else:\n        inishape = self.shape\n        _series.shape = tuple([-1, ] + list(self._varshape))\n        _series[:] = _series[idx]\n        _series.shape = inishape\n    # Sort the dates and reset the cache\n    flatdates = _dates.ravel()\n    flatdates[:] = flatdates[idx]\n    _cached['chronidx'] = np.array([], dtype=int)\n    _cached['ischrono'] = True", "path": "scikits\\timeseries\\tseries.py", "repo_name": "pierregm/scikits.timeseries", "stars": 14, "license": "other", "language": "python", "size": 428}
{"docstring": "\"\"\"\nReturns the rolling log percentage change of the series. This is defined as\nthe log of the ratio of series[T]/series[T-nper]\n\nParameters\n----------\nseries : {TimeSeries}\n    TimeSeries object to to calculate log percentage chage for. Ignore this\n    parameter if calling this as a method.\nnper : {int}\n    Number of periods for percentage change.\n\nNotes\n-----\nSeries of integer types will be upcast\n1.0 == 100% in result\n\nExamples\n--------\n>>> series = ts.time_series(\n...     [2.,1.,2.,3.], start_date=ts.Date(freq='A', year=2005))\n>>> series.pct_log()\ntimeseries([-- -0.69314718056 0.69314718056 0.405465108108],\n           dates = [2005 ... 2008],\n           freq  = A-DEC)\n>>> series.pct_log(2)\ntimeseries([-- -- 0.0 1.09861228867],\n           dates = [2005 ... 2008],\n           freq  = A-DEC)\n\n\"\"\"\n", "func_signal": "def pct_log(series, nper=1):\n", "code": "def pct_func(series, nper):\n    return ma.log(series[nper:] / series[:-nper])\nreturn _pct_generic(series, nper, pct_func)", "path": "scikits\\timeseries\\tseries.py", "repo_name": "pierregm/scikits.timeseries", "stars": 14, "license": "other", "language": "python", "size": 428}
{"docstring": "\"\"\"\nReturns the rolling symmetric percentage change of the series. This is\ndefined as 2*(series[T] - series[T-nper])/(series[T] - series[T-nper])\n\nParameters\n----------\nseries : {TimeSeries}\n    TimeSeries object to to calculate symmetric percentage chage for. Ignore\n    this parameter if calling this as a method.\nnper : {int}\n    Number of periods for percentage change.\n\nNotes\n-----\nSeries of integer types will be upcast\n1.0 == 100% in result\n\nExamples\n--------\n>>> series = ts.time_series(\n...     [2.,1.,2.,3.], start_date=ts.Date(freq='A', year=2005))\n>>> series.pct_symmetric()\ntimeseries([-- -0.666666666667 0.666666666667 0.4],\n           dates = [2005 ... 2008],\n           freq  = A-DEC)\n>>> series.pct_symmetric(2)\ntimeseries([-- -- 0.0 1.0],\n           dates = [2005 ... 2008],\n           freq  = A-DEC)\n\n\"\"\"\n", "func_signal": "def pct_symmetric(series, nper=1):\n", "code": "def pct_func(series, nper):\n    return \\\n        2 * (series[nper:] - series[:-nper]) / \\\n            (series[nper:] + series[:-nper])\nreturn _pct_generic(series, nper, pct_func)", "path": "scikits\\timeseries\\tseries.py", "repo_name": "pierregm/scikits.timeseries", "stars": 14, "license": "other", "language": "python", "size": 428}
{"docstring": "\"\"\"\nPrivate used to force dtypes upcasting in certain functions\n(eg. int -> float in pct function).\nAdapted from function of the same name in the C source code.\n\"\"\"\n", "func_signal": "def _get_type_num_double(dtype):\n", "code": "if dtype.num < np.dtype('f').num:\n    return np.dtype('d')\nreturn dtype", "path": "scikits\\timeseries\\tseries.py", "repo_name": "pierregm/scikits.timeseries", "stars": 14, "license": "other", "language": "python", "size": 428}
{"docstring": "\"\"\"\n    Returns a time series containing the data of a, but with a new shape.\n\n    The result is a view to the original array; if this is not possible,\n    a ValueError is raised.\n\n    Parameters\n    ----------\n    shape : shape tuple or int\n       The new shape should be compatible with the original shape. If an\n       integer, then the result will be a 1D array of that length.\n    order : {'C', 'F'}, optional\nDetermines whether the array data should be viewed as in C\n(row-major) order or FORTRAN (column-major) order.\n\n    Returns\n    -------\n    reshaped_array : array\nA new view to the timeseries.\n\n    Warnings\n    --------\n    The `._dates` part is reshaped, but the order is NOT ensured.\n\n\"\"\"\n", "func_signal": "def reshape(self, *newshape, **kwargs):\n", "code": "kwargs.update(order=kwargs.get('order', 'C'))\nif self._varshape:\n    try:\n        bkdtype = (self.dtype, self._varshape)\n        _series = self._series.view([('', bkdtype)])\n        result = _series.reshape(*newshape, **kwargs)\n        result = result.view(dtype=bkdtype, type=type(self))\n        result._dates = ndarray.reshape(self._dates, *newshape, **kwargs)\n    except:\n        err_msg = \"Reshaping a nV/nD series is not implemented yet !\"\n        raise NotImplementedError(err_msg)\n# 1D series : reshape the dates as well\nelse:\n    result = MaskedArray.reshape(self, *newshape, **kwargs)\n    result._dates = ndarray.reshape(self._dates, *newshape, **kwargs)\n    result._varshape = ()\nreturn result", "path": "scikits\\timeseries\\tseries.py", "repo_name": "pierregm/scikits.timeseries", "stars": 14, "license": "other", "language": "python", "size": 428}
{"docstring": "\"\"\"\n\n    Restores the internal state of the TimeSeries, for pickling purposes.\n    `state` is typically the output of the ``__getstate__`` output, and is a 5-tuple:\n\n- class name\n- a tuple giving the shape of the data\n- a typecode for the data\n- a binary string for the data\n- a binary string for the mask.\n\"\"\"\n", "func_signal": "def __setstate__(self, state):\n", "code": "(ver, shp, typ, isf, raw, msk, flv, dsh, dtm, frq, infodict) = state\nMaskedArray.__setstate__(self, (ver, shp, typ, isf, raw, msk, flv))\n_dates = self._dates\n_dates.__setstate__((ver, dsh, dtype(int_), isf, dtm, frq))\n_dates.freq = frq\n_dates._cachedinfo.update(dict(full=None, hasdups=None, steps=None,\n                               toobj=None, toord=None, tostr=None))\n# Update the _optinfo dictionary\nself._optinfo.update(infodict)", "path": "scikits\\timeseries\\tseries.py", "repo_name": "pierregm/scikits.timeseries", "stars": 14, "license": "other", "language": "python", "size": 428}
{"docstring": "\"\"\"\nReturns an empty series with the same dtype, mask and dates as series.\n\"\"\"\n", "func_signal": "def empty_like(series):\n", "code": "result = np.empty_like(series).view(type(series))\nresult._dates = series._dates\nresult._mask = series._mask.copy()\nreturn result", "path": "scikits\\timeseries\\tseries.py", "repo_name": "pierregm/scikits.timeseries", "stars": 14, "license": "other", "language": "python", "size": 428}
{"docstring": "\"\"\"\nChecks the date compatibility of two TimeSeries object.\nReturns True if everything's fine, or raises an exception.\n\"\"\"\n#!!!: We need to use _varshape to simplify the analysis\n# Check the frequency ..............\n", "func_signal": "def _timeseriescompat(a, b, raise_error=True):\n", "code": "(afreq, bfreq) = (getattr(a, 'freq', None), getattr(b, 'freq', None))\nif afreq != bfreq:\n    if raise_error:\n        raise TimeSeriesCompatibilityError('freq', afreq, bfreq)\n    return False\n# Make sure a.freq is not None\nif afreq is None:\n    return True\n# Make sure that the series are sorted in chronological order\nif (not a.is_chronological()) or (not b.is_chronological()):\n    if raise_error:\n        raise TimeSeriesCompatiblityError('sort', None, None)\n    return False\n# Check the starting dates ..........\n(astart, bstart) = (getattr(a, 'start_date'), getattr(b, 'start_date'))\nif astart != bstart:\n    if raise_error:\n        raise TimeSeriesCompatibilityError('start_date', astart, bstart)\n    return False\n# Check the time steps ..............\nasteps = getattr(a, '_dates', a).get_steps()\nbsteps = getattr(b, '_dates', b).get_steps()\nstep_diff = (asteps != bsteps)\nif (step_diff is True) or \\\n   (hasattr(step_diff, \"any\") and step_diff.any()):\n    if raise_error:\n        raise TimeSeriesCompatibilityError('time_steps', asteps, bsteps)\n    return False\nelif a.shape != b.shape:\n    if raise_error:\n        raise TimeSeriesCompatibilityError('size', \"1: %s\" % str(a.shape),\n                                                   \"2: %s\" % str(b.shape))\n    return False\nreturn True", "path": "scikits\\timeseries\\tseries.py", "repo_name": "pierregm/scikits.timeseries", "stars": 14, "license": "other", "language": "python", "size": 428}
{"docstring": "\"\"\"\n    Returns an array of the same class as `_data`,  with masked values\n    filled with `fill_value`. Subclassing is preserved.\n\n    Parameters\n    ----------\n    fill_value : {None, singleton of type self.dtype}, optional\nThe value to fill in masked values with.\nIf `fill_value` is None, uses ``self.fill_value``.\n\n    \"\"\"\n", "func_signal": "def filled(self, fill_value=None):\n", "code": "result = self._series.filled(fill_value=fill_value).view(type(self))\nresult._dates = self._dates\nreturn result", "path": "scikits\\timeseries\\tseries.py", "repo_name": "pierregm/scikits.timeseries", "stars": 14, "license": "other", "language": "python", "size": 428}
{"docstring": "\"\"\"\n    Returns a ravelled view of the instance.\n\n    If the instance corresponds to one variable (e.g., ``self.varshape == ()``),\n    the result is the ravelled view of the input.\n    Otherwise, the result is actually a TimeSeries where the `series` attribute\n    is a reshaped view of the input and where the `dates` attribute is\n    a ravelled view of the input `dates` attribute.\n\n    Examples\n    --------\n    >>> start_date = ts.Date('M', '2001-01')\n    >>> dates = ts.date_array(start_date=start_date, length=4)\n    >>> series = ts.time_series([[1, 2], [3, 4]], dates=dates)\n    >>> series\n    timeseries(\n     [[1 2]\n     [3 4]],\ndates =\n     [[Jan-2001 Feb-2001] ... [Mar-2001 Apr-2001]],\nfreq  = M)\n    >>> series.ravel()\n    timeseries([1 2 3 4],\n       dates = [Jan-2001 ... Apr-2001],\n       freq  = M)\n    >>> series = ts.time_series([[1, 2], [3, 4]], start_date=start_date)\n    >>> series\n    timeseries(\n     [[1 2]\n     [3 4]],\ndates =\n     [Jan-2001 Feb-2001],\nfreq  = M)\n    >>> series.ravel()\n    timeseries(\n     [[1 2]\n     [3 4]],\ndates =\n     [Jan-2001 Feb-2001],\nfreq  = M)\n\"\"\"\n", "func_signal": "def ravel(self):\n", "code": "_varshape = self._varshape\nif _varshape:\n    newshape = tuple([-1] + [np.prod(_varshape), ])\n    result = MaskedArray.reshape(self, *newshape)\nelse:\n    result = MaskedArray.ravel(self)\nresult._dates = self._dates.ravel()\nreturn result", "path": "scikits\\timeseries\\tseries.py", "repo_name": "pierregm/scikits.timeseries", "stars": 14, "license": "other", "language": "python", "size": 428}
{"docstring": "\"\"\"this function is used for passing through attributes of one\ntime series to a new one being created\"\"\"\n", "func_signal": "def _attrib_dict(series, exclude=[]):\n", "code": "result = {'fill_value':series.fill_value}\nreturn dict(filter(lambda x: x[0] not in exclude, result.iteritems()))", "path": "scikits\\timeseries\\tseries.py", "repo_name": "pierregm/scikits.timeseries", "stars": 14, "license": "other", "language": "python", "size": 428}
{"docstring": "\"\"\"\n    Sets the dates to `value`.\n\"\"\"\n# Make sure it's a DateArray\n", "func_signal": "def __setdates__(self, value):\n", "code": "if not isinstance(value, DateArray):\n    err_msg = \"The input dates should be a valid \"\\\n              \"DateArray object (got %s instead)\" % type(value)\n    raise TypeError(err_msg)\n# Skip if dates is nodates (or empty)\\\nif value is nodates or not getattr(value, 'size', 0):\n    return super(TimeSeries, self).__setattr__('_dates', value)\n# Make sure it has the proper size\ntsize = getattr(value, 'size', 1)\n# Check the _varshape\nvarshape = self._varshape\nif not varshape:\n    # We may be using the default: retry\n    varshape = self._varshape = get_varshape(self, value)\n# Get the data length (independently of the nb of variables)\ndsize = self.size // int(np.prod(varshape))\nif tsize != dsize:\n    raise TimeSeriesCompatibilityError(\"size\",\n                                       \"data: %s\" % dsize,\n                                       \"dates: %s\" % tsize)", "path": "scikits\\timeseries\\tseries.py", "repo_name": "pierregm/scikits.timeseries", "stars": 14, "license": "other", "language": "python", "size": 428}
{"docstring": "\"\"\"\nReturns a copy of series, sorted chronologically.\n\"\"\"\n", "func_signal": "def sort_chronologically(series):\n", "code": "series = series.copy()\nseries.sort_chronologically()\nreturn series", "path": "scikits\\timeseries\\tseries.py", "repo_name": "pierregm/scikits.timeseries", "stars": 14, "license": "other", "language": "python", "size": 428}
{"docstring": "\"\"\"\nFlattens a (multi-) time series to 1D series.\n\n\"\"\"\n", "func_signal": "def flatten(series):\n", "code": "shp_ini = series.shape\n# Already flat time series....\nif len(shp_ini) == 1:\n    return series\n# Folded single time series ..\nnewdates = series._dates.ravel()\nif series._dates.size == series._series.size:\n    newshape = (series._series.size,)\nelse:\n    newshape = (np.asarray(shp_ini[:-1]).prod(), shp_ini[-1])\nnewseries = series._series.reshape(newshape)\nreturn time_series(newseries, newdates)", "path": "scikits\\timeseries\\tseries.py", "repo_name": "pierregm/scikits.timeseries", "stars": 14, "license": "other", "language": "python", "size": 428}
{"docstring": "\"\"\"Computes the cross-covariance function of two series x and y.\nThe computations are performed on anomalies (deviations from average).\nGaps in the series are filled first, anomalies are then computed and missing\nvalues filled with 0.\nIf x and y are valid TimeSeries object, they are aligned so that their starting\nand ending point match.\n\nThe crosscovariance at lag k, $\\hat{R_{x,y}}(k)$, of 2 series {x_1,...,x_n}\nand {y_1,...,y_n} with mean 0 is defined as:\n\\hat{R_{x,y}(k) = \\sum_{t=1}^{n-k}{x_t y_{t+k}} / \\sum_{t=1}^{n-k}{a_t b_{t+k}}\nwhere x_k (y_k) is set to 0 if x_k (y_k) is initially masked, where a_k = 1 if\nx_k is not masked, a_k = 0 if x_k is masked, b_k = 1 if y_k is not masked and\nb_k = 0 if y_k is masked.\n\nIf the optional parameter `periodogram` is True, the denominator of the previous\nexpression is $\\sum_{t=1}^{n-k}{a_t a_{t+k}} + k$.\n\nParameters\n----------\nx : sequence\n    Input data.\ny : sequence\n    Input data.\n    If y is longer than x, it is truncated to match the length of x.\n    If y is shorter than x, x is truncated.\nperiodogram : {True, False} optional\n    Whether to return a periodogram or a standard estimate of the autocovariance.\n\nReturns\n-------\ncvf : ma.array\n    Cross-covariance at lags [0,1,...,n,n-1,...,-1]\n\n\"\"\"\n#\n", "func_signal": "def cvf(x,y,periodogram=True):\n", "code": "x = ma.array(x, copy=False, subok=True, dtype=float)\ny = ma.array(y, copy=False, subok=True, dtype=float)\nif (x.ndim > 1) or (y.ndim > 1):\n    raise ValueError(\"Input arrays should be 1D! (got %iD-%iD)\" % \\\n                     (x.ndim, y.ndim))\n# Make sure the series have the same size .............\nif isinstance(x, TimeSeries):\n    if not isinstance(y, TimeSeries):\n        raise TypeError(\"The second input is NOT a valid TimeSeries\")\n    (x,y) = ts.align_series(x,y)\nelif isinstance(y, TimeSeries) and not isinstance(x, TimeSeries):\n    raise TypeError(\"The first input is NOT a valid TimeSeries\")\nelse:\n    if len(y) > len(x):\n        y = y[:len(x)]\n    else:\n        x = x[:len(y)]\n# Get the masks .......................................\nmx = np.logical_not(ma.getmaskarray(x)).astype(int)\nmy = np.logical_not(ma.getmaskarray(y)).astype(int)\n# Get the anomalies ...................................\nx = x.anom().filled(0).view(ndarray)\ny = y.anom().filled(0).view(ndarray)\nn = len(x)\ncvf_ = np.correlate(x, y, 'full')\ndnm_ = np.correlate(mx, my, 'full')\nif periodogram:\n    dnm_ += np.concatenate([np.arange(n-1,0,-1), np.arange(n)])\ncvf_ /= dnm_\nreturn ma.fix_invalid(np.concatenate([cvf_[n-1:],cvf_[:n-1]]))", "path": "scikits\\timeseries\\lib\\avcf.py", "repo_name": "pierregm/scikits.timeseries", "stars": 14, "license": "other", "language": "python", "size": 428}
{"docstring": "\"\"\"Split a multi-dimensional series into individual columns.\"\"\"\n", "func_signal": "def split(self):\n", "code": "if self.ndim == 1:\n    return [self]\nelse:\n    n = self.shape[1]\n    arr = hsplit(self, n)[0]\n    return [self.__class__(np.squeeze(a),\n                           self._dates,\n                           **_attrib_dict(self)) for a in arr]", "path": "scikits\\timeseries\\tseries.py", "repo_name": "pierregm/scikits.timeseries", "stars": 14, "license": "other", "language": "python", "size": 428}
{"docstring": "\"\"\"\n    Calculates the repr representation, using masked for fill if it is\n    enabled. Otherwise fill with fill value.\n    \"\"\"\n", "func_signal": "def __repr__(self):\n", "code": "_dates = self._dates\nif np.size(self._dates) > 2 and self.is_valid():\n    timestr = \"[%s ... %s]\" % (str(_dates[0]), str(_dates[-1]))\nelse:\n    timestr = str(_dates)\nkwargs = {'data': str(self._series), 'time': timestr,\n          'freq': self.freqstr, 'dtype': self.dtype}\nnames = kwargs['dtype'].names\nif self.ndim <= 1:\n    if names:\n        return _print_templates['desc_flx_short'] % kwargs\n    return _print_templates['desc_short'] % kwargs\nif names:\n    return _print_templates['desc_flx'] % kwargs\nreturn _print_templates['desc'] % kwargs", "path": "scikits\\timeseries\\tseries.py", "repo_name": "pierregm/scikits.timeseries", "stars": 14, "license": "other", "language": "python", "size": 428}
{"docstring": "\"\"\"\nReturn a dictionary (duplicated dates <> indices) for the input series.\n\nThe indices are given as a tuple of ndarrays, a la :meth:`nonzero`.\n\nParameters\n----------\nseries : TimeSeries, DateArray\n    A valid :class:`TimeSeries` or :class:`DateArray` object.\n\nExamples\n--------\n>>> series = time_series(np.arange(10),\n                        dates=[2000, 2001, 2002, 2003, 2003,\n                               2003, 2004, 2005, 2005, 2006], freq='A')\n>>> test = find_duplicated_dates(series)\n {<A-DEC : 2003>: (array([3, 4, 5]),), <A-DEC : 2005>: (array([7, 8]),)}\n\"\"\"\n", "func_signal": "def find_duplicated_dates(series):\n", "code": "dates = getattr(series, '_dates', series)\nsteps = dates.get_steps()\nduplicated_dates = tuple(set(dates[steps == 0]))\nindices = {}\nfor d in duplicated_dates:\n    indices[d] = (dates == d).nonzero()\nreturn indices", "path": "scikits\\timeseries\\tseries.py", "repo_name": "pierregm/scikits.timeseries", "stars": 14, "license": "other", "language": "python", "size": 428}
{"docstring": "\"\"\"\n    Converts the dates portion of the TimeSeries to another frequency.\n\n    The resulting TimeSeries will have the same shape and dimensions\n    as the original series (unlike the :meth:`convert` method).\n\n    Parameters\n    ----------\n    freq : {freq_spec}\n    relation : {'END', 'START'} (optional)\n\n    Returns\n    -------\n    A new TimeSeries with the :attr:`.dates` :class:`DateArray` at the\n    specified frequency (the :meth`.asfreq` method of the :attr:`.dates`\n    property will be called).\n    The data in the resulting series will be a VIEW of the original series.\n\n    Notes\n    -----\n    The parameters are the exact same as for\n    :meth:`~scikit.timeseries.DateArray.asfreq`. Please see the docstring for\n    that method for details on the parameters and how the actual conversion is\n    performed.\n\n    \"\"\"\n", "func_signal": "def asfreq(self, freq, relation=\"END\"):\n", "code": "if freq is None: return self\n\nreturn TimeSeries(self._series,\n                  dates=self._dates.asfreq(freq, relation=relation))", "path": "scikits\\timeseries\\tseries.py", "repo_name": "pierregm/scikits.timeseries", "stars": 14, "license": "other", "language": "python", "size": 428}
{"docstring": "\"\"\"Serialize as post data for a POST request.\"\"\"\n", "func_signal": "def to_postdata(self):\n", "code": "return '&'.join(['%s=%s' % (escape(str(k)), escape(str(v))) \\\n    for k, v in self.parameters.iteritems()])", "path": "multi_oauth\\oauth.py", "repo_name": "nerdsforever/django-multi-oauth", "stars": 11, "license": "other", "language": "python", "size": 103}
{"docstring": "\"\"\" Returns a token from something like:\noauth_token_secret=xxx&oauth_token=xxx\n\"\"\"\n", "func_signal": "def from_string(s):\n", "code": "params = cgi.parse_qs(s, keep_blank_values=False)\nkey = params['oauth_token'][0]\nsecret = params['oauth_token_secret'][0]\ntoken = OAuthToken(key, secret)\ntry:\n    token.callback_confirmed = params['oauth_callback_confirmed'][0]\nexcept KeyError:\n    pass # 1.0, no callback confirmed.\nreturn token", "path": "multi_oauth\\oauth.py", "repo_name": "nerdsforever/django-multi-oauth", "stars": 11, "license": "other", "language": "python", "size": 103}
{"docstring": "\"\"\"Serialize as a header for an HTTPAuth request.\"\"\"\n", "func_signal": "def to_header(self, realm=''):\n", "code": "auth_header = 'OAuth realm=\"%s\"' % realm\n# Add the oauth parameters.\nif self.parameters:\n    for k, v in self.parameters.iteritems():\n        if k[:6] == 'oauth_':\n            auth_header += ', %s=\"%s\"' % (k, escape(str(v)))\nreturn {'Authorization': auth_header}", "path": "multi_oauth\\oauth.py", "repo_name": "nerdsforever/django-multi-oauth", "stars": 11, "license": "other", "language": "python", "size": 103}
{"docstring": "\"\"\"Turn URL string into parameters.\"\"\"\n", "func_signal": "def _split_url_string(param_str):\n", "code": "parameters = cgi.parse_qs(param_str, keep_blank_values=False)\nfor k, v in parameters.iteritems():\n    parameters[k] = urllib.unquote(v[0])\nreturn parameters", "path": "multi_oauth\\oauth.py", "repo_name": "nerdsforever/django-multi-oauth", "stars": 11, "license": "other", "language": "python", "size": 103}
{"docstring": "\"\"\"Try to find the token for the provided request token key.\"\"\"\n", "func_signal": "def _get_token(self, oauth_request, token_type='access'):\n", "code": "token_field = oauth_request.get_parameter('oauth_token')\ntoken = self.data_store.lookup_token(token_type, token_field)\nif not token:\n    raise OAuthError('Invalid %s token: %s' % (token_type, token_field))\nreturn token", "path": "multi_oauth\\oauth.py", "repo_name": "nerdsforever/django-multi-oauth", "stars": 11, "license": "other", "language": "python", "size": 103}
{"docstring": "\"\"\"Concatenates the consumer key and secret.\"\"\"\n", "func_signal": "def build_signature_base_string(self, oauth_request, consumer, token):\n", "code": "sig = '%s&' % escape(consumer.secret)\nif token:\n    sig = sig + escape(token.secret)\nreturn sig, sig", "path": "multi_oauth\\oauth.py", "repo_name": "nerdsforever/django-multi-oauth", "stars": 11, "license": "other", "language": "python", "size": 103}
{"docstring": "\"\"\"Verify the correct version request for this server.\"\"\"\n", "func_signal": "def _get_version(self, oauth_request):\n", "code": "try:\n    version = oauth_request.get_parameter('oauth_version')\nexcept:\n    version = VERSION\nif version and version != self.version:\n    raise OAuthError('OAuth version %s not supported.' % str(version))\nreturn version", "path": "multi_oauth\\oauth.py", "repo_name": "nerdsforever/django-multi-oauth", "stars": 11, "license": "other", "language": "python", "size": 103}
{"docstring": "\"\"\"Set the signature parameter to the result of build_signature.\"\"\"\n# Set the signature method.\n", "func_signal": "def sign_request(self, signature_method, consumer, token):\n", "code": "self.set_parameter('oauth_signature_method',\n    signature_method.get_name())\n# Set the signature.\nself.set_parameter('oauth_signature',\n    self.build_signature(signature_method, consumer, token))", "path": "multi_oauth\\oauth.py", "repo_name": "nerdsforever/django-multi-oauth", "stars": 11, "license": "other", "language": "python", "size": 103}
{"docstring": "\"\"\"Turn Authorization: header into parameters.\"\"\"\n", "func_signal": "def _split_header(header):\n", "code": "params = {}\nparts = header.split(',')\nfor param in parts:\n    # Ignore realm parameter.\n    if param.find('realm') > -1:\n        continue\n    # Remove whitespace.\n    param = param.strip()\n    # Split key-value.\n    param_parts = param.split('=', 1)\n    # Remove quotes and unescape the value.\n    params[param_parts[0]] = urllib.unquote(param_parts[1].strip('\\\"'))\nreturn params", "path": "multi_oauth\\oauth.py", "repo_name": "nerdsforever/django-multi-oauth", "stars": 11, "license": "other", "language": "python", "size": 103}
{"docstring": "\"\"\"Builds the base signature string.\"\"\"\n", "func_signal": "def build_signature(self, oauth_request, consumer, token):\n", "code": "key, raw = self.build_signature_base_string(oauth_request, consumer,\n    token)\n\n# HMAC object.\ntry:\n    import hashlib # 2.5\n    hashed = hmac.new(key, raw, hashlib.sha1)\nexcept:\n    import sha # Deprecated\n    hashed = hmac.new(key, raw, sha)\n\n# Calculate the digest base 64.\nreturn binascii.b2a_base64(hashed.digest())[:-1]", "path": "multi_oauth\\oauth.py", "repo_name": "nerdsforever/django-multi-oauth", "stars": 11, "license": "other", "language": "python", "size": 103}
{"docstring": "\"\"\"Verifies an api call and checks all the parameters.\"\"\"\n# -> consumer and token\n", "func_signal": "def verify_request(self, oauth_request):\n", "code": "version = self._get_version(oauth_request)\nconsumer = self._get_consumer(oauth_request)\n# Get the access token.\ntoken = self._get_token(oauth_request, 'access')\nself._check_signature(oauth_request, consumer, token)\nparameters = oauth_request.get_nonoauth_parameters()\nreturn consumer, token, parameters", "path": "multi_oauth\\oauth.py", "repo_name": "nerdsforever/django-multi-oauth", "stars": 11, "license": "other", "language": "python", "size": 103}
{"docstring": "\"\"\"Get any non-OAuth parameters.\"\"\"\n", "func_signal": "def get_nonoauth_parameters(self):\n", "code": "parameters = {}\nfor k, v in self.parameters.iteritems():\n    # Ignore oauth parameters.\n    if k.find('oauth_') < 0:\n        parameters[k] = v\nreturn parameters", "path": "multi_oauth\\oauth.py", "repo_name": "nerdsforever/django-multi-oauth", "stars": 11, "license": "other", "language": "python", "size": 103}
{"docstring": "\"\"\"Convert unicode to utf-8.\"\"\"\n", "func_signal": "def _utf8_str(s):\n", "code": "if isinstance(s, unicode):\n    return s.encode(\"utf-8\")\nelse:\n    return str(s)", "path": "multi_oauth\\oauth.py", "repo_name": "nerdsforever/django-multi-oauth", "stars": 11, "license": "other", "language": "python", "size": 103}
{"docstring": "\"\"\"Verify that the nonce is uniqueish.\"\"\"\n", "func_signal": "def _check_nonce(self, consumer, token, nonce):\n", "code": "nonce = self.data_store.lookup_nonce(consumer, token, nonce)\nif nonce:\n    raise OAuthError('Nonce already used: %s' % str(nonce))", "path": "multi_oauth\\oauth.py", "repo_name": "nerdsforever/django-multi-oauth", "stars": 11, "license": "other", "language": "python", "size": 103}
{"docstring": "\"\"\"Parses the URL and rebuilds it to be scheme://host/path.\"\"\"\n", "func_signal": "def get_normalized_http_url(self):\n", "code": "parts = urlparse.urlparse(self.http_url)\nscheme, netloc, path = parts[:3]\n# Exclude default port numbers.\nif scheme == 'http' and netloc[-3:] == ':80':\n    netloc = netloc[:-3]\nelif scheme == 'https' and netloc[-4:] == ':443':\n    netloc = netloc[:-4]\nreturn '%s://%s%s' % (scheme, netloc, path)", "path": "multi_oauth\\oauth.py", "repo_name": "nerdsforever/django-multi-oauth", "stars": 11, "license": "other", "language": "python", "size": 103}
{"docstring": "\"\"\"Figure out the signature with some defaults.\"\"\"\n", "func_signal": "def _get_signature_method(self, oauth_request):\n", "code": "try:\n    signature_method = oauth_request.get_parameter(\n        'oauth_signature_method')\nexcept:\n    signature_method = SIGNATURE_METHOD\ntry:\n    # Get the signature method object.\n    signature_method = self.signature_methods[signature_method]\nexcept:\n    signature_method_names = ', '.join(self.signature_methods.keys())\n    raise OAuthError('Signature method %s not supported try one of the '\n        'following: %s' % (signature_method, signature_method_names))\n\nreturn signature_method", "path": "multi_oauth\\oauth.py", "repo_name": "nerdsforever/django-multi-oauth", "stars": 11, "license": "other", "language": "python", "size": 103}
{"docstring": "\"\"\"Verify that timestamp is recentish.\"\"\"\n", "func_signal": "def _check_timestamp(self, timestamp):\n", "code": "timestamp = int(timestamp)\nnow = int(time.time())\nlapsed = abs(now - timestamp)\nif lapsed > self.timestamp_threshold:\n    raise OAuthError('Expired timestamp: given %d and now %s has a '\n        'greater difference than threshold %d' %\n        (timestamp, now, self.timestamp_threshold))", "path": "multi_oauth\\oauth.py", "repo_name": "nerdsforever/django-multi-oauth", "stars": 11, "license": "other", "language": "python", "size": 103}
{"docstring": "\"\"\"Processes an access_token request and returns the\naccess token on success.\n\"\"\"\n", "func_signal": "def fetch_access_token(self, oauth_request):\n", "code": "version = self._get_version(oauth_request)\nconsumer = self._get_consumer(oauth_request)\ntry:\n    verifier = self._get_verifier(oauth_request)\nexcept OAuthError:\n    verifier = None\n# Get the request token.\ntoken = self._get_token(oauth_request, 'request')\nself._check_signature(oauth_request, consumer, token)\nnew_token = self.data_store.fetch_access_token(consumer, token, verifier)\nreturn new_token", "path": "multi_oauth\\oauth.py", "repo_name": "nerdsforever/django-multi-oauth", "stars": 11, "license": "other", "language": "python", "size": 103}
{"docstring": "\"\"\"Processes a request_token request and returns the\nrequest token on success.\n\"\"\"\n", "func_signal": "def fetch_request_token(self, oauth_request):\n", "code": "try:\n    # Get the request token for authorization.\n    token = self._get_token(oauth_request, 'request')\nexcept OAuthError:\n    # No token required for the initial token request.\n    version = self._get_version(oauth_request)\n    consumer = self._get_consumer(oauth_request)\n    try:\n        callback = self.get_callback(oauth_request)\n    except OAuthError:\n        callback = None # 1.0, no callback specified.\n    self._check_signature(oauth_request, consumer, None)\n    # Fetch a new token.\n    token = self.data_store.fetch_request_token(consumer, callback)\nreturn token", "path": "multi_oauth\\oauth.py", "repo_name": "nerdsforever/django-multi-oauth", "stars": 11, "license": "other", "language": "python", "size": 103}
{"docstring": "\"\"\"Return a string that contains the parameters that must be signed.\"\"\"\n", "func_signal": "def get_normalized_parameters(self):\n", "code": "params = self.parameters\ntry:\n    # Exclude the signature if it exists.\n    del params['oauth_signature']\nexcept:\n    pass\n# Escape key values before sorting.\nkey_values = [(escape(_utf8_str(k)), escape(_utf8_str(v))) \\\n    for k,v in params.items()]\n# Sort lexicographically, first after key, then after value.\nkey_values.sort()\n# Combine key value pairs into a string.\nreturn '&'.join(['%s=%s' % (k, v) for k, v in key_values])", "path": "multi_oauth\\oauth.py", "repo_name": "nerdsforever/django-multi-oauth", "stars": 11, "license": "other", "language": "python", "size": 103}
{"docstring": "\"\"\"\nMoves root node``node`` to a different tree, inserting it\nrelative to the given ``target`` node as specified by\n``position``.\n\n``node`` will be modified to reflect its new tree state in the\ndatabase.\n\"\"\"\n", "func_signal": "def _move_root_node(self, node, target, position):\n", "code": "left = getattr(node, self.left_attr)\nright = getattr(node, self.right_attr)\nlevel = getattr(node, self.level_attr)\ntree_id = getattr(node, self.tree_id_attr)\nnew_tree_id = getattr(target, self.tree_id_attr)\nwidth = right - left + 1\n\nif node == target:\n    raise InvalidMove(_('A node may not be made a child of itself.'))\nelif tree_id == new_tree_id:\n    raise InvalidMove(_('A node may not be made a child of any of its descendants.'))\n\nspace_target, level_change, left_right_change, parent = \\\n    self._calculate_inter_tree_move_values(node, target, position)\n\n# Create space for the tree which will be inserted\nself._create_space(width, space_target, new_tree_id)\n\n# Move the root node, making it a child node\nopts = self.model._meta\nmove_tree_query = \"\"\"\nUPDATE %(table)s\nSET %(level)s = %(level)s - %%s,\n    %(left)s = %(left)s - %%s,\n    %(right)s = %(right)s - %%s,\n    %(tree_id)s = %%s,\n    %(parent)s = CASE\n        WHEN %(pk)s = %%s\n            THEN %%s\n        ELSE %(parent)s END\nWHERE %(left)s >= %%s AND %(left)s <= %%s\n  AND %(tree_id)s = %%s\"\"\" % {\n    'table': qn(opts.db_table),\n    'level': qn(opts.get_field(self.level_attr).column),\n    'left': qn(opts.get_field(self.left_attr).column),\n    'right': qn(opts.get_field(self.right_attr).column),\n    'tree_id': qn(opts.get_field(self.tree_id_attr).column),\n    'parent': qn(opts.get_field(self.parent_attr).column),\n    'pk': qn(opts.pk.column),\n}\ncursor = connection.cursor()\ncursor.execute(move_tree_query, [level_change, left_right_change,\n    left_right_change, new_tree_id, node.pk, parent.pk, left, right,\n    tree_id])\n\n# Update the former root node to be consistent with the updated\n# tree in the database.\nsetattr(node, self.left_attr, left - left_right_change)\nsetattr(node, self.right_attr, right - left_right_change)\nsetattr(node, self.level_attr, level - level_change)\nsetattr(node, self.tree_id_attr, new_tree_id)\nsetattr(node, self.parent_attr, parent)", "path": "mptt\\managers.py", "repo_name": "bfirsh/django-mptt", "stars": 11, "license": "other", "language": "python", "size": 280}
{"docstring": "\"\"\"\nDetermines the next largest unused tree id for the tree managed\nby this manager.\n\"\"\"\n", "func_signal": "def _get_next_tree_id(self):\n", "code": "opts = self.model._meta\ncursor = connection.cursor()\ncursor.execute('SELECT MAX(%s) FROM %s' % (\n    qn(opts.get_field(self.tree_id_attr).column),\n    qn(opts.db_table)))\nrow = cursor.fetchone()\nreturn row[0] and (row[0] + 1) or 1", "path": "mptt\\managers.py", "repo_name": "bfirsh/django-mptt", "stars": 11, "license": "other", "language": "python", "size": 280}
{"docstring": "\"\"\"\nPopulates a template variable with a ``QuerySet`` containing the\nfull tree for a given model.\n\nUsage::\n\n   {% full_tree_for_model [model] as [varname] %}\n\nThe model is specified in ``[appname].[modelname]`` format.\n\nExample::\n\n   {% full_tree_for_model tests.Genre as genres %}\n\n\"\"\"\n", "func_signal": "def do_full_tree_for_model(parser, token):\n", "code": "bits = token.contents.split()\nif len(bits) != 4:\n    raise template.TemplateSyntaxError(_('%s tag requires three arguments') % bits[0])\nif bits[2] != 'as':\n    raise template.TemplateSyntaxError(_(\"second argument to %s tag must be 'as'\") % bits[0])\nreturn FullTreeForModelNode(bits[1], bits[3])", "path": "mptt\\templatetags\\mptt_tags.py", "repo_name": "bfirsh/django-mptt", "stars": 11, "license": "other", "language": "python", "size": 280}
{"docstring": "\"\"\"\nRemoves ``node`` from its tree, making it the root node of a new\ntree.\n\nIf ``new_tree_id`` is not specified a new tree id will be\ngenerated.\n\n``node`` will be modified to reflect its new tree state in the\ndatabase.\n\"\"\"\n", "func_signal": "def _make_child_root_node(self, node, new_tree_id=None):\n", "code": "left = getattr(node, self.left_attr)\nright = getattr(node, self.right_attr)\nlevel = getattr(node, self.level_attr)\ntree_id = getattr(node, self.tree_id_attr)\nif not new_tree_id:\n    new_tree_id = self._get_next_tree_id()\nleft_right_change = left - 1\n\nself._inter_tree_move_and_close_gap(node, level, left_right_change,\n                                    new_tree_id)\n\n# Update the node to be consistent with the updated\n# tree in the database.\nsetattr(node, self.left_attr, left - left_right_change)\nsetattr(node, self.right_attr, right - left_right_change)\nsetattr(node, self.level_attr, 0)\nsetattr(node, self.tree_id_attr, new_tree_id)\nsetattr(node, self.parent_attr, None)", "path": "mptt\\managers.py", "repo_name": "bfirsh/django-mptt", "stars": 11, "license": "other", "language": "python", "size": 280}
{"docstring": "\"\"\"\nCalculates values required when moving ``node`` relative to\n``target`` as specified by ``position``.\n\"\"\"\n", "func_signal": "def _calculate_inter_tree_move_values(self, node, target, position):\n", "code": "left = getattr(node, self.left_attr)\nlevel = getattr(node, self.level_attr)\ntarget_left = getattr(target, self.left_attr)\ntarget_right = getattr(target, self.right_attr)\ntarget_level = getattr(target, self.level_attr)\n\nif position == 'last-child' or position == 'first-child':\n    if position == 'last-child':\n        space_target = target_right - 1\n    else:\n        space_target = target_left\n    level_change = level - target_level - 1\n    parent = target\nelif position == 'left' or position == 'right':\n    if position == 'left':\n        space_target = target_left - 1\n    else:\n        space_target = target_right\n    level_change = level - target_level\n    parent = getattr(target, self.parent_attr)\nelse:\n    raise ValueError(_('An invalid position was given: %s.') % position)\n\nleft_right_change = left - space_target - 1\nreturn space_target, level_change, left_right_change, parent", "path": "mptt\\managers.py", "repo_name": "bfirsh/django-mptt", "stars": 11, "license": "other", "language": "python", "size": 280}
{"docstring": "\"\"\"\nManages spaces in the tree identified by ``tree_id`` by changing\nthe values of the left and right columns by ``size`` after the\ngiven ``target`` point.\n\"\"\"\n", "func_signal": "def _manage_space(self, size, target, tree_id):\n", "code": "opts = self.model._meta\nspace_query = \"\"\"\nUPDATE %(table)s\nSET %(left)s = CASE\n        WHEN %(left)s > %%s\n            THEN %(left)s + %%s\n        ELSE %(left)s END,\n    %(right)s = CASE\n        WHEN %(right)s > %%s\n            THEN %(right)s + %%s\n        ELSE %(right)s END\nWHERE %(tree_id)s = %%s\n  AND (%(left)s > %%s OR %(right)s > %%s)\"\"\" % {\n    'table': qn(opts.db_table),\n    'left': qn(opts.get_field(self.left_attr).column),\n    'right': qn(opts.get_field(self.right_attr).column),\n    'tree_id': qn(opts.get_field(self.tree_id_attr).column),\n}\ncursor = connection.cursor()\ncursor.execute(space_query, [target, size, target, size, tree_id,\n                             target, target])", "path": "mptt\\managers.py", "repo_name": "bfirsh/django-mptt", "stars": 11, "license": "other", "language": "python", "size": 280}
{"docstring": "\"\"\"\nMoves ``node`` relative to a given ``target`` node as specified\nby ``position`` (when appropriate), by examining both nodes and\ncalling the appropriate method to perform the move.\n\nA ``target`` of ``None`` indicates that ``node`` should be\nturned into a root node.\n\nValid values for ``position`` are ``'first-child'``,\n``'last-child'``, ``'left'`` or ``'right'``.\n\n``node`` will be modified to reflect its new tree state in the\ndatabase.\n\nThis method explicitly checks for ``node`` being made a sibling\nof a root node, as this is a special case due to our use of tree\nids to order root nodes.\n\"\"\"\n", "func_signal": "def move_node(self, node, target, position='last-child'):\n", "code": "if target is None:\n    if node.is_child_node():\n        self._make_child_root_node(node)\nelif target.is_root_node() and position in ['left', 'right']:\n    self._make_sibling_of_root_node(node, target, position)\nelse:\n    if node.is_root_node():\n        self._move_root_node(node, target, position)\n    else:\n        self._move_child_node(node, target, position)\ntransaction.commit_unless_managed()", "path": "mptt\\managers.py", "repo_name": "bfirsh/django-mptt", "stars": 11, "license": "other", "language": "python", "size": 280}
{"docstring": "\"\"\"\nSplit a pathname into components (the opposite of os.path.join) in a\nplatform-neutral way.\n\"\"\"\n", "func_signal": "def fullsplit(path, result=None):\n", "code": "if result is None:\n    result = []\nhead, tail = os.path.split(path)\nif head == '':\n    return [tail] + result\nif head == path:\n    return result\nreturn fullsplit(head, [tail] + result)", "path": "setup.py", "repo_name": "bfirsh/django-mptt", "stars": 11, "license": "other", "language": "python", "size": 280}
{"docstring": "\"\"\"\nMoves child node ``node`` to a different tree, inserting it\nrelative to the given ``target`` node in the new tree as\nspecified by ``position``.\n\n``node`` will be modified to reflect its new tree state in the\ndatabase.\n\"\"\"\n", "func_signal": "def _move_child_to_new_tree(self, node, target, position):\n", "code": "left = getattr(node, self.left_attr)\nright = getattr(node, self.right_attr)\nlevel = getattr(node, self.level_attr)\ntarget_left = getattr(target, self.left_attr)\ntarget_right = getattr(target, self.right_attr)\ntarget_level = getattr(target, self.level_attr)\ntree_id = getattr(node, self.tree_id_attr)\nnew_tree_id = getattr(target, self.tree_id_attr)\n\nspace_target, level_change, left_right_change, parent = \\\n    self._calculate_inter_tree_move_values(node, target, position)\n\ntree_width = right - left + 1\n\n# Make space for the subtree which will be moved\nself._create_space(tree_width, space_target, new_tree_id)\n# Move the subtree\nself._inter_tree_move_and_close_gap(node, level_change,\n    left_right_change, new_tree_id, parent.pk)\n\n# Update the node to be consistent with the updated\n# tree in the database.\nsetattr(node, self.left_attr, left - left_right_change)\nsetattr(node, self.right_attr, right - left_right_change)\nsetattr(node, self.level_attr, level - level_change)\nsetattr(node, self.tree_id_attr, new_tree_id)\nsetattr(node, self.parent_attr, parent)", "path": "mptt\\managers.py", "repo_name": "bfirsh/django-mptt", "stars": 11, "license": "other", "language": "python", "size": 280}
{"docstring": "\"\"\"\nMoves child node ``node`` within its current tree relative to\nthe given ``target`` node as specified by ``position``.\n\n``node`` will be modified to reflect its new tree state in the\ndatabase.\n\"\"\"\n", "func_signal": "def _move_child_within_tree(self, node, target, position):\n", "code": "left = getattr(node, self.left_attr)\nright = getattr(node, self.right_attr)\nlevel = getattr(node, self.level_attr)\nwidth = right - left + 1\ntree_id = getattr(node, self.tree_id_attr)\ntarget_left = getattr(target, self.left_attr)\ntarget_right = getattr(target, self.right_attr)\ntarget_level = getattr(target, self.level_attr)\n\nif position == 'last-child' or position == 'first-child':\n    if node == target:\n        raise InvalidMove(_('A node may not be made a child of itself.'))\n    elif left < target_left < right:\n        raise InvalidMove(_('A node may not be made a child of any of its descendants.'))\n    if position == 'last-child':\n        if target_right > right:\n            new_left = target_right - width\n            new_right = target_right - 1\n        else:\n            new_left = target_right\n            new_right = target_right + width - 1\n    else:\n        if target_left > left:\n            new_left = target_left - width + 1\n            new_right = target_left\n        else:\n            new_left = target_left + 1\n            new_right = target_left + width\n    level_change = level - target_level - 1\n    parent = target\nelif position == 'left' or position == 'right':\n    if node == target:\n        raise InvalidMove(_('A node may not be made a sibling of itself.'))\n    elif left < target_left < right:\n        raise InvalidMove(_('A node may not be made a sibling of any of its descendants.'))\n    if position == 'left':\n        if target_left > left:\n            new_left = target_left - width\n            new_right = target_left - 1\n        else:\n            new_left = target_left\n            new_right = target_left + width - 1\n    else:\n        if target_right > right:\n            new_left = target_right - width + 1\n            new_right = target_right\n        else:\n            new_left = target_right + 1\n            new_right = target_right + width\n    level_change = level - target_level\n    parent = getattr(target, self.parent_attr)\nelse:\n    raise ValueError(_('An invalid position was given: %s.') % position)\n\nleft_boundary = min(left, new_left)\nright_boundary = max(right, new_right)\nleft_right_change = new_left - left\ngap_size = width\nif left_right_change > 0:\n    gap_size = -gap_size\n\nopts = self.model._meta\n# The level update must come before the left update to keep\n# MySQL happy - left seems to refer to the updated value\n# immediately after its update has been specified in the query\n# with MySQL, but not with SQLite or Postgres.\nmove_subtree_query = \"\"\"\nUPDATE %(table)s\nSET %(level)s = CASE\n        WHEN %(left)s >= %%s AND %(left)s <= %%s\n          THEN %(level)s - %%s\n        ELSE %(level)s END,\n    %(left)s = CASE\n        WHEN %(left)s >= %%s AND %(left)s <= %%s\n          THEN %(left)s + %%s\n        WHEN %(left)s >= %%s AND %(left)s <= %%s\n          THEN %(left)s + %%s\n        ELSE %(left)s END,\n    %(right)s = CASE\n        WHEN %(right)s >= %%s AND %(right)s <= %%s\n          THEN %(right)s + %%s\n        WHEN %(right)s >= %%s AND %(right)s <= %%s\n          THEN %(right)s + %%s\n        ELSE %(right)s END,\n    %(parent)s = CASE\n        WHEN %(pk)s = %%s\n          THEN %%s\n        ELSE %(parent)s END\nWHERE %(tree_id)s = %%s\"\"\" % {\n    'table': qn(opts.db_table),\n    'level': qn(opts.get_field(self.level_attr).column),\n    'left': qn(opts.get_field(self.left_attr).column),\n    'right': qn(opts.get_field(self.right_attr).column),\n    'parent': qn(opts.get_field(self.parent_attr).column),\n    'pk': qn(opts.pk.column),\n    'tree_id': qn(opts.get_field(self.tree_id_attr).column),\n}\n\ncursor = connection.cursor()\ncursor.execute(move_subtree_query, [\n    left, right, level_change,\n    left, right, left_right_change,\n    left_boundary, right_boundary, gap_size,\n    left, right, left_right_change,\n    left_boundary, right_boundary, gap_size,\n    node.pk, parent.pk,\n    tree_id])\n\n# Update the node to be consistent with the updated\n# tree in the database.\nsetattr(node, self.left_attr, new_left)\nsetattr(node, self.right_attr, new_right)\nsetattr(node, self.level_attr, level - level_change)\nsetattr(node, self.parent_attr, parent)", "path": "mptt\\managers.py", "repo_name": "bfirsh/django-mptt", "stars": 11, "license": "other", "language": "python", "size": 280}
{"docstring": "\"\"\"\nPopulates a template variable with the drilldown tree for a given\nnode, optionally counting the number of items associated with its\nchildren.\n\nA drilldown tree consists of a node's ancestors, itself and its\nimmediate children. For example, a drilldown tree for a book\ncategory \"Personal Finance\" might look something like::\n\n   Books\n      Business, Finance & Law\n         Personal Finance\n            Budgeting (220)\n            Financial Planning (670)\n\nUsage::\n\n   {% drilldown_tree_for_node [node] as [varname] %}\n\nExtended usage::\n\n   {% drilldown_tree_for_node [node] as [varname] count [foreign_key] in [count_attr] %}\n   {% drilldown_tree_for_node [node] as [varname] cumulative count [foreign_key] in [count_attr] %}\n\nThe foreign key is specified in ``[appname].[modelname].[fieldname]``\nformat, where ``fieldname`` is the name of a field in the specified\nmodel which relates it to the given node's model.\n\nWhen this form is used, a ``count_attr`` attribute on each child of\nthe given node in the drilldown tree will contain a count of the\nnumber of items associated with it through the given foreign key.\n\nIf cumulative is also specified, this count will be for items\nrelated to the child node and all of its descendants.\n\nExamples::\n\n   {% drilldown_tree_for_node genre as drilldown %}\n   {% drilldown_tree_for_node genre as drilldown count tests.Game.genre in game_count %}\n   {% drilldown_tree_for_node genre as drilldown cumulative count tests.Game.genre in game_count %}\n\n\"\"\"\n", "func_signal": "def do_drilldown_tree_for_node(parser, token):\n", "code": "bits = token.contents.split()\nlen_bits = len(bits)\nif len_bits not in (4, 8, 9):\n    raise TemplateSyntaxError(_('%s tag requires either three, seven or eight arguments') % bits[0])\nif bits[2] != 'as':\n    raise TemplateSyntaxError(_(\"second argument to %s tag must be 'as'\") % bits[0])\nif len_bits == 8:\n    if bits[4] != 'count':\n        raise TemplateSyntaxError(_(\"if seven arguments are given, fourth argument to %s tag must be 'with'\") % bits[0])\n    if bits[6] != 'in':\n        raise TemplateSyntaxError(_(\"if seven arguments are given, sixth argument to %s tag must be 'in'\") % bits[0])\n    return DrilldownTreeForNodeNode(bits[1], bits[3], bits[5], bits[7])\nelif len_bits == 9:\n    if bits[4] != 'cumulative':\n        raise TemplateSyntaxError(_(\"if eight arguments are given, fourth argument to %s tag must be 'cumulative'\") % bits[0])\n    if bits[5] != 'count':\n        raise TemplateSyntaxError(_(\"if eight arguments are given, fifth argument to %s tag must be 'count'\") % bits[0])\n    if bits[7] != 'in':\n        raise TemplateSyntaxError(_(\"if eight arguments are given, seventh argument to %s tag must be 'in'\") % bits[0])\n    return DrilldownTreeForNodeNode(bits[1], bits[3], bits[6], bits[8], cumulative=True)\nelse:\n    return DrilldownTreeForNodeNode(bits[1], bits[3])", "path": "mptt\\templatetags\\mptt_tags.py", "repo_name": "bfirsh/django-mptt", "stars": 11, "license": "other", "language": "python", "size": 280}
{"docstring": "\"\"\"\nCreates space for a new tree by incrementing all tree ids\ngreater than ``target_tree_id``.\n\"\"\"\n", "func_signal": "def _create_tree_space(self, target_tree_id):\n", "code": "opts = self.model._meta\ncursor = connection.cursor()\ncursor.execute(\"\"\"\nUPDATE %(table)s\nSET %(tree_id)s = %(tree_id)s + 1\nWHERE %(tree_id)s > %%s\"\"\" % {\n    'table': qn(opts.db_table),\n    'tree_id': qn(opts.get_field(self.tree_id_attr).column),\n}, [target_tree_id])", "path": "mptt\\managers.py", "repo_name": "bfirsh/django-mptt", "stars": 11, "license": "other", "language": "python", "size": 280}
{"docstring": "\"\"\"\nGiven a list of tree items, iterates over the list, generating\ntwo-tuples of the current tree item and a ``dict`` containing\ninformation about the tree structure around the item, with the\nfollowing keys:\n\n   ``'new_level'`\n      ``True`` if the current item is the start of a new level in\n      the tree, ``False`` otherwise.\n\n   ``'closed_levels'``\n      A list of levels which end after the current item. This will\n      be an empty list if the next item is at the same level as the\n      current item.\n\nIf ``ancestors`` is ``True``, the following key will also be\navailable:\n\n   ``'ancestors'``\n      A list of unicode representations of the ancestors of the\n      current node, in descending order (root node first, immediate\n      parent last).\n\n      For example: given the sample tree below, the contents of the\n      list which would be available under the ``'ancestors'`` key\n      are given on the right::\n\n         Books                    ->  []\n            Sci-fi                ->  [u'Books']\n               Dystopian Futures  ->  [u'Books', u'Sci-fi']\n\n\"\"\"\n", "func_signal": "def tree_item_iterator(items, ancestors=False):\n", "code": "structure = {}\nopts = None\nfor previous, current, next in previous_current_next(items):\n    if opts is None:\n        opts = current._meta\n\n    current_level = getattr(current, opts.level_attr)\n    if previous:\n        structure['new_level'] = (getattr(previous,\n                                          opts.level_attr) < current_level)\n        if ancestors:\n            # If the previous node was the end of any number of\n            # levels, remove the appropriate number of ancestors\n            # from the list.\n            if structure['closed_levels']:\n                structure['ancestors'] = \\\n                    structure['ancestors'][:-len(structure['closed_levels'])]\n            # If the current node is the start of a new level, add its\n            # parent to the ancestors list.\n            if structure['new_level']:\n                structure['ancestors'].append(unicode(previous))\n    else:\n        structure['new_level'] = True\n        if ancestors:\n            # Set up the ancestors list on the first item\n            structure['ancestors'] = []\n\n    if next:\n        structure['closed_levels'] = range(current_level,\n                                           getattr(next,\n                                                   opts.level_attr), -1)\n    else:\n        # All remaining levels need to be closed\n        structure['closed_levels'] = range(current_level, -1, -1)\n\n    # Return a deep copy of the structure dict so this function can\n    # be used in situations where the iterator is consumed\n    # immediately.\n    yield current, copy.deepcopy(structure)", "path": "mptt\\utils.py", "repo_name": "bfirsh/django-mptt", "stars": 11, "license": "other", "language": "python", "size": 280}
{"docstring": "\"\"\"\nReturns the root node of the tree with the given id.\n\"\"\"\n", "func_signal": "def root_node(self, tree_id):\n", "code": "return self.get(**{\n    self.tree_id_attr: tree_id,\n    '%s__isnull' % self.parent_attr: True,\n})", "path": "mptt\\managers.py", "repo_name": "bfirsh/django-mptt", "stars": 11, "license": "other", "language": "python", "size": 280}
{"docstring": "\"\"\"\nMoves ``node``, making it a sibling of the given ``target`` root\nnode as specified by ``position``.\n\n``node`` will be modified to reflect its new tree state in the\ndatabase.\n\nSince we use tree ids to reduce the number of rows affected by\ntree mangement during insertion and deletion, root nodes are not\ntrue siblings; thus, making an item a sibling of a root node is\na special case which involves shuffling tree ids around.\n\"\"\"\n", "func_signal": "def _make_sibling_of_root_node(self, node, target, position):\n", "code": "if node == target:\n    raise InvalidMove(_('A node may not be made a sibling of itself.'))\n\nopts = self.model._meta\ntree_id = getattr(node, self.tree_id_attr)\ntarget_tree_id = getattr(target, self.tree_id_attr)\n\nif node.is_child_node():\n    if position == 'left':\n        space_target = target_tree_id - 1\n        new_tree_id = target_tree_id\n    elif position == 'right':\n        space_target = target_tree_id\n        new_tree_id = target_tree_id + 1\n    else:\n        raise ValueError(_('An invalid position was given: %s.') % position)\n\n    self._create_tree_space(space_target)\n    if tree_id > space_target:\n        # The node's tree id has been incremented in the\n        # database - this change must be reflected in the node\n        # object for the method call below to operate on the\n        # correct tree.\n        setattr(node, self.tree_id_attr, tree_id + 1)\n    self._make_child_root_node(node, new_tree_id)\nelse:\n    if position == 'left':\n        if target_tree_id > tree_id:\n            left_sibling = target.get_previous_sibling()\n            if node == left_sibling:\n                return\n            new_tree_id = getattr(left_sibling, self.tree_id_attr)\n            lower_bound, upper_bound = tree_id, new_tree_id\n            shift = -1\n        else:\n            new_tree_id = target_tree_id\n            lower_bound, upper_bound = new_tree_id, tree_id\n            shift = 1\n    elif position == 'right':\n        if target_tree_id > tree_id:\n            new_tree_id = target_tree_id\n            lower_bound, upper_bound = tree_id, target_tree_id\n            shift = -1\n        else:\n            right_sibling = target.get_next_sibling()\n            if node == right_sibling:\n                return\n            new_tree_id = getattr(right_sibling, self.tree_id_attr)\n            lower_bound, upper_bound = new_tree_id, tree_id\n            shift = 1\n    else:\n        raise ValueError(_('An invalid position was given: %s.') % position)\n\n    root_sibling_query = \"\"\"\n    UPDATE %(table)s\n    SET %(tree_id)s = CASE\n        WHEN %(tree_id)s = %%s\n            THEN %%s\n        ELSE %(tree_id)s + %%s END\n    WHERE %(tree_id)s >= %%s AND %(tree_id)s <= %%s\"\"\" % {\n        'table': qn(opts.db_table),\n        'tree_id': qn(opts.get_field(self.tree_id_attr).column),\n    }\n    cursor = connection.cursor()\n    cursor.execute(root_sibling_query, [tree_id, new_tree_id, shift,\n                                        lower_bound, upper_bound])\n    setattr(node, self.tree_id_attr, new_tree_id)", "path": "mptt\\managers.py", "repo_name": "bfirsh/django-mptt", "stars": 11, "license": "other", "language": "python", "size": 280}
{"docstring": "# Let any VariableDoesNotExist raised bubble up\n", "func_signal": "def render(self, context):\n", "code": "args = [self.node.resolve(context)]\n\nif self.foreign_key is not None:\n    app_label, model_name, fk_attr = self.foreign_key.split('.')\n    cls = get_model(app_label, model_name)\n    if cls is None:\n        raise template.TemplateSyntaxError(_('drilldown_tree_for_node tag was given an invalid model: %s') % '.'.join([app_label, model_name]))\n    try:\n        cls._meta.get_field(fk_attr)\n    except FieldDoesNotExist:\n        raise template.TemplateSyntaxError(_('drilldown_tree_for_node tag was given an invalid model field: %s') % fk_attr)\n    args.extend([cls, fk_attr, self.count_attr, self.cumulative])\n\ncontext[self.context_var] = drilldown_tree_for_node(*args)\nreturn ''", "path": "mptt\\templatetags\\mptt_tags.py", "repo_name": "bfirsh/django-mptt", "stars": 11, "license": "other", "language": "python", "size": 280}
{"docstring": "\"\"\"\nFrom http://www.wordaligned.org/articles/zippy-triples-served-with-python\n\nCreates an iterator which returns (previous, current, next) triples,\nwith ``None`` filling in when there is no previous or next\navailable.\n\"\"\"\n", "func_signal": "def previous_current_next(items):\n", "code": "extend = itertools.chain([None], items, [None])\nprevious, current, next = itertools.tee(extend, 3)\ntry:\n    current.next()\n    next.next()\n    next.next()\nexcept StopIteration:\n    pass\nreturn itertools.izip(previous, current, next)", "path": "mptt\\utils.py", "repo_name": "bfirsh/django-mptt", "stars": 11, "license": "other", "language": "python", "size": 280}
{"docstring": "\"\"\"\nGiven a list of tree items, produces doubles of a tree item and a\n``dict`` containing information about the tree structure around the\nitem, with the following contents:\n\n   new_level\n      ``True`` if the current item is the start of a new level in\n      the tree, ``False`` otherwise.\n\n   closed_levels\n      A list of levels which end after the current item. This will\n      be an empty list if the next item is at the same level as the\n      current item.\n\nUsing this filter with unpacking in a ``{% for %}`` tag, you should\nhave enough information about the tree structure to create a\nhierarchical representation of the tree.\n\nExample::\n\n   {% for genre,structure in genres|tree_info %}\n   {% if tree.new_level %}<ul><li>{% else %}</li><li>{% endif %}\n   {{ genre.name }}\n   {% for level in tree.closed_levels %}</li></ul>{% endfor %}\n   {% endfor %}\n\n\"\"\"\n", "func_signal": "def tree_info(items, features=None):\n", "code": "kwargs = {}\nif features:\n    feature_names = features.split(',')\n    if 'ancestors' in feature_names:\n        kwargs['ancestors'] = True\nreturn tree_item_iterator(items, **kwargs)", "path": "mptt\\templatetags\\mptt_tags.py", "repo_name": "bfirsh/django-mptt", "stars": 11, "license": "other", "language": "python", "size": 280}
{"docstring": "\"\"\"\nReturns a ``QuerySet`` which contains all tree items, ordered in\nsuch a way that that root nodes appear in tree id order and\ntheir subtrees appear in depth-first order.\n\"\"\"\n", "func_signal": "def get_query_set(self):\n", "code": "return super(TreeManager, self).get_query_set().order_by(\n    self.tree_id_attr, self.left_attr)", "path": "mptt\\managers.py", "repo_name": "bfirsh/django-mptt", "stars": 11, "license": "other", "language": "python", "size": 280}
{"docstring": "\"\"\"\nCalls the appropriate method to move child node ``node``\nrelative to the given ``target`` node as specified by\n``position``.\n\"\"\"\n", "func_signal": "def _move_child_node(self, node, target, position):\n", "code": "tree_id = getattr(node, self.tree_id_attr)\ntarget_tree_id = getattr(target, self.tree_id_attr)\n\nif (getattr(node, self.tree_id_attr) ==\n    getattr(target, self.tree_id_attr)):\n    self._move_child_within_tree(node, target, position)\nelse:\n    self._move_child_to_new_tree(node, target, position)", "path": "mptt\\managers.py", "repo_name": "bfirsh/django-mptt", "stars": 11, "license": "other", "language": "python", "size": 280}
{"docstring": "\"\"\"Retrieve all groups that belong to the given member_id.\n\nArgs:\n  member_id: The member's email address (e.g. member@example.com).\n  direct_only: Boolean whether only return groups that this member directly belongs to.\n\nReturns:\n  A list containing the result of the retrieve operation.\n\"\"\"\n", "func_signal": "def RetrieveGroups(self, member_id, direct_only=False):\n", "code": "uri = self._ServiceUrl('group', True, '', member_id, '', direct_only=direct_only)\nreturn self._GetPropertiesList(uri)", "path": "gdata\\apps\\groups\\service.py", "repo_name": "onyxfish/stovetop", "stars": 11, "license": "mit", "language": "python", "size": 720}
{"docstring": "\"\"\"Update a group's name, description and/or permission.\n\nArgs:\n  group_id: The ID of the group (e.g. us-sales).\n  group_name: The name of the group.\n  description: A description of the group\n  email_permission: The subscription permission of the group.\n\nReturns:\n  A dict containing the result of the update operation.\n\"\"\"\n", "func_signal": "def UpdateGroup(self, group_id, group_name, description, email_permission):\n", "code": "uri = self._ServiceUrl('group', True, group_id, '', '')\nproperties = {}\nproperties['groupId'] = group_id\nproperties['groupName'] = group_name\nproperties['description'] = description\nproperties['emailPermission'] = email_permission\nreturn self._PutProperties(uri, properties)", "path": "gdata\\apps\\groups\\service.py", "repo_name": "onyxfish/stovetop", "stars": 11, "license": "mit", "language": "python", "size": 720}
{"docstring": "\"\"\"Add a member to a group.\n\nArgs:\n  member_id: The member's email address (e.g. member@example.com).\n  group_id: The ID of the group (e.g. us-sales).\n\nReturns:\n  A dict containing the result of the add operation.\n\"\"\"\n", "func_signal": "def AddMemberToGroup(self, member_id, group_id):\n", "code": "uri = self._ServiceUrl('member', False, group_id, member_id, '')\nproperties = {}\nproperties['memberId'] = member_id\nreturn self._PostProperties(uri, properties)", "path": "gdata\\apps\\groups\\service.py", "repo_name": "onyxfish/stovetop", "stars": 11, "license": "mit", "language": "python", "size": 720}
{"docstring": "\"\"\"Extracts the postID string from the entry's Atom id.\n\nReturns: A string of digits which identify this post within the blog.\n\"\"\"\n", "func_signal": "def GetPostId(self):\n", "code": "if self.id.text:\n  return self.post_id_pattern.match(self.id.text).group(4)\nreturn None", "path": "gdata\\blogger\\__init__.py", "repo_name": "onyxfish/stovetop", "stars": 11, "license": "mit", "language": "python", "size": 720}
{"docstring": "\"\"\"\nFetch a document from memcache or Google Docs, transform, and serve.\n\"\"\"\n", "func_signal": "def get(self, document_name):\n", "code": "callback = self.request.get('callback')\n\nif not callback:\n    return self.error(400)\n\ntry:\n    options = config.DOCUMENTS[document_name]\nexcept KeyError:\n    return self.error(404)\n\ncontent = memcache.get(document_name)\n\nif not content:\n    csv_data = self.fetch_csv(options)\n\n    if options['convert']:\n        content = self.csv_to_json(csv_data)\n    else:\n        content = '\"%s\"' % escapejs(csv_data)\n\n    memcache.set(document_name, content, options['ttl'])\n\nself.response.headers[\"Content-Type\"] = \"application/javascript\"\nself.response.headers[\"Cache-Control\"] = \"no-cache\"\nself.response.headers[\"Pragma\"] = \"no-cache\"    \nself.response.out.write('%s(%s)' % (callback, content))", "path": "main.py", "repo_name": "onyxfish/stovetop", "stars": 11, "license": "mit", "language": "python", "size": 720}
{"docstring": "\"\"\"Retrieve all owners of the given group.\n\nArgs:\n  group_id: The ID of the group (e.g. us-sales).\n  suspended_users: A boolean; should we include any suspended users in\n    the ownership list returned?\n\nReturns:\n  A list containing the result of the retrieve operation.\n\"\"\"\n", "func_signal": "def RetrieveAllOwners(self, group_id, suspended_users=False):\n", "code": "uri = self._ServiceUrl('owner', True, group_id, '', '',\n                       suspended_users=suspended_users)\nreturn self._GetPropertiesList(uri)", "path": "gdata\\apps\\groups\\service.py", "repo_name": "onyxfish/stovetop", "stars": 11, "license": "mit", "language": "python", "size": 720}
{"docstring": "\"\"\"\nRetrieves a single Google spreadsheet as CSV using ClientLogin\nauthentication.\n\nTODO: handle retries and timeouts on auth calls\nTOOD: handle retries and timeouts on content fetching\n\"\"\"\n", "func_signal": "def fetch_csv(self, options):\n", "code": "client = gdata.docs.client.DocsClient()\nclient.ClientLogin(\n    config.USER_EMAIL, config.USER_PASSWORD, config.APP_DOMAIN)\n\nspreadsheets_client = gdata.spreadsheet.service.SpreadsheetsService()\nspreadsheets_client.ClientLogin(\n    config.USER_EMAIL, config.USER_PASSWORD, config.APP_DOMAIN)\n\ndocs_token = client.auth_token\nclient.auth_token = gdata.gauth.ClientLoginToken(\n    spreadsheets_client.GetClientLoginToken())\n\nreturn client.get_file_content(CSV_URL % options)", "path": "main.py", "repo_name": "onyxfish/stovetop", "stars": 11, "license": "mit", "language": "python", "size": 720}
{"docstring": "\"\"\"Retrieve one page of groups in the domain.\n\nArgs:\n  start_group: The key to continue for pagination through all groups.\n  \nReturns:\n  A feed object containing the result of the retrieve operation.\n\"\"\"\n", "func_signal": "def RetrievePageOfGroups(self, start_group=None):\n", "code": "uri = self._ServiceUrl('group', True, '', '', '')\nif start_group is not None:\n  uri += \"?start=\"+start_group\nproperty_feed = self._GetPropertyFeed(uri)\nreturn property_feed", "path": "gdata\\apps\\groups\\service.py", "repo_name": "onyxfish/stovetop", "stars": 11, "license": "mit", "language": "python", "size": 720}
{"docstring": "\"\"\"Creates a Url object which corresponds to the URL string.\n\nThis method can accept partial URLs, but it will leave missing\nmembers of the Url unset.\n\"\"\"\n", "func_signal": "def parse_url(url_string):\n", "code": "parts = urlparse.urlparse(url_string)\nurl = Url()\nif parts[0]:\n  url.protocol = parts[0]\nif parts[1]:\n  host_parts = parts[1].split(':')\n  if host_parts[0]:\n    url.host = host_parts[0]\n  if len(host_parts) > 1:\n    url.port = host_parts[1]\nif parts[2]:\n  url.path = parts[2]\nif parts[4]:\n  param_pairs = parts[4].split('&')\n  for pair in param_pairs:\n    pair_parts = pair.split('=')\n    if len(pair_parts) > 1:\n      url.params[urllib.unquote_plus(pair_parts[0])] = (\n          urllib.unquote_plus(pair_parts[1]))\n    elif len(pair_parts) == 1:\n      url.params[urllib.unquote_plus(pair_parts[0])] = None\nreturn url", "path": "atom\\url.py", "repo_name": "onyxfish/stovetop", "stars": 11, "license": "mit", "language": "python", "size": 720}
{"docstring": "\"\"\"Create a group.\n\nArgs:\n  group_id: The ID of the group (e.g. us-sales).\n  group_name: The name of the group.\n  description: A description of the group\n  email_permission: The subscription permission of the group.\n\nReturns:\n  A dict containing the result of the create operation.\n\"\"\"\n", "func_signal": "def CreateGroup(self, group_id, group_name, description, email_permission):\n", "code": "uri = self._ServiceUrl('group', False, group_id, '', '')\nproperties = {}\nproperties['groupId'] = group_id\nproperties['groupName'] = group_name\nproperties['description'] = description\nproperties['emailPermission'] = email_permission\nreturn self._PostProperties(uri, properties)", "path": "gdata\\apps\\groups\\service.py", "repo_name": "onyxfish/stovetop", "stars": 11, "license": "mit", "language": "python", "size": 720}
{"docstring": "\"\"\"Remove the given member from the given group.\n\nArgs:\n  member_id: The member's email address (e.g. member@example.com).\n  group_id: The ID of the group (e.g. us-sales).\n\nReturns:\n  A dict containing the result of the remove operation.\n\"\"\"\n", "func_signal": "def RemoveMemberFromGroup(self, member_id, group_id):\n", "code": "uri = self._ServiceUrl('member', True, group_id, member_id, '')\nreturn self._DeleteProperties(uri)", "path": "gdata\\apps\\groups\\service.py", "repo_name": "onyxfish/stovetop", "stars": 11, "license": "mit", "language": "python", "size": 720}
{"docstring": "\"\"\"\nConverts a Google-formatted CSV (\\n line-endings) to JSON.\n\"\"\"\n", "func_signal": "def csv_to_json(self, csv_data):\n", "code": "fileish = csv_data.split('\\n')\nreader = csv.DictReader(fileish)\nreturn [row for row in reader]", "path": "main.py", "repo_name": "onyxfish/stovetop", "stars": 11, "license": "mit", "language": "python", "size": 720}
{"docstring": "\"\"\"Returns the path with the parameters escaped and appended.\"\"\"\n", "func_signal": "def get_request_uri(self):\n", "code": "param_string = self.get_param_string()\nif param_string:\n  return '?'.join([self.path, param_string])\nelse:\n  return self.path", "path": "atom\\url.py", "repo_name": "onyxfish/stovetop", "stars": 11, "license": "mit", "language": "python", "size": 720}
{"docstring": "\"\"\"Retrieve one page of owners of the given group.\n\nArgs:\n  group_id: The ID of the group (e.g. us-sales).\n  suspended_users: A boolean; should we include any suspended users in\n    the ownership list returned?\n  start: The key to continue for pagination through all owners.\n\nReturns:\n  A feed object containing the result of the retrieve operation.\n\"\"\"\n", "func_signal": "def RetrievePageOfOwners(self, group_id, suspended_users=False, start=None):\n", "code": "uri = self._ServiceUrl('owner', True, group_id, '', '',\n                       suspended_users=suspended_users)\nif start is not None:\n  if suspended_users:\n    uri += \"&start=\"+start\n  else:\n    uri += \"?start=\"+start\nproperty_feed = self._GetPropertyFeed(uri)\nreturn property_feed", "path": "gdata\\apps\\groups\\service.py", "repo_name": "onyxfish/stovetop", "stars": 11, "license": "mit", "language": "python", "size": 720}
{"docstring": "\"\"\"Remove the given owner from the given group.\n\nArgs:\n  owner_email: The email address of a group owner.\n  group_id: The ID of the group (e.g. us-sales).\n\nReturns:\n  A dict containing the result of the remove operation.\n\"\"\"\n", "func_signal": "def RemoveOwnerFromGroup(self, owner_email, group_id):\n", "code": "uri = self._ServiceUrl('owner', True, group_id, '', owner_email)\nreturn self._DeleteProperties(uri)", "path": "gdata\\apps\\groups\\service.py", "repo_name": "onyxfish/stovetop", "stars": 11, "license": "mit", "language": "python", "size": 720}
{"docstring": "\"\"\"Retrieve a group based on its ID.\n\nArgs:\n  group_id: The ID of the group (e.g. us-sales).\n\nReturns:\n  A dict containing the result of the retrieve operation.\n\"\"\"\n", "func_signal": "def RetrieveGroup(self, group_id):\n", "code": "uri = self._ServiceUrl('group', True, group_id, '', '')\nreturn self._GetProperties(uri)", "path": "gdata\\apps\\groups\\service.py", "repo_name": "onyxfish/stovetop", "stars": 11, "license": "mit", "language": "python", "size": 720}
{"docstring": "\"\"\"Add an owner to a group.\n\nArgs:\n  owner_email: The email address of a group owner.\n  group_id: The ID of the group (e.g. us-sales).\n\nReturns:\n  A dict containing the result of the add operation.\n\"\"\"\n", "func_signal": "def AddOwnerToGroup(self, owner_email, group_id):\n", "code": "uri = self._ServiceUrl('owner', False, group_id, '', owner_email)\nproperties = {}\nproperties['email'] = owner_email\nreturn self._PostProperties(uri, properties)", "path": "gdata\\apps\\groups\\service.py", "repo_name": "onyxfish/stovetop", "stars": 11, "license": "mit", "language": "python", "size": 720}
{"docstring": "\"\"\"Changes the HTTP request before sending it to the server.\n\nSets the User-Agent HTTP header and fills in the HTTP host portion\nof the URL if one was not included in the request (for this it uses\nthe self.host member if one is set). This method is called in\nself.request.\n\nArgs:\n  http_request: An atom.http_core.HttpRequest() (optional) If one is\n                not provided, a new HttpRequest is instantiated.\n\nReturns:\n  An atom.http_core.HttpRequest() with the User-Agent header set and\n  if this client has a value in its host member, the host in the request\n  URL is set.\n\"\"\"\n", "func_signal": "def modify_request(self, http_request):\n", "code": "if http_request is None:\n  http_request = atom.http_core.HttpRequest()\n\nif self.host is not None and http_request.uri.host is None:\n  http_request.uri.host = self.host\n\n# Set the user agent header for logging purposes.\nif self.source:\n  http_request.headers['User-Agent'] = '%s gdata-py/2.0.12' % self.source\nelse:\n  http_request.headers['User-Agent'] = 'gdata-py/2.0.12'\n\nreturn http_request", "path": "atom\\client.py", "repo_name": "onyxfish/stovetop", "stars": 11, "license": "mit", "language": "python", "size": 720}
{"docstring": "\"\"\"Extracts the Blogger id of this blog.\nThis method is useful when contructing URLs by hand. The blog id is\noften used in blogger operation URLs. This should not be confused with\nthe id member of a BloggerBlog. The id element is the Atom id XML element.\nThe blog id which this method returns is a part of the Atom id.\n\nReturns:\n  The blog's unique id as a string.\n\"\"\"\n", "func_signal": "def GetBlogId(self):\n", "code": "if self.id.text:\n  match = self.blog_id_pattern.match(self.id.text)\n  if match:\n    return match.group(2)\n  else:\n    return self.blog_id2_pattern.match(self.id.text).group(2)\nreturn None", "path": "gdata\\blogger\\__init__.py", "repo_name": "onyxfish/stovetop", "stars": 11, "license": "mit", "language": "python", "size": 720}
{"docstring": "\"\"\"Check whether the given member an owner of the given group.\n\nArgs:\n  owner_email: The email address of a group owner.\n  group_id: The ID of the group (e.g. us-sales).\n\nReturns:\n  True if the member is an owner of the given group.  False otherwise.\n\"\"\"\n", "func_signal": "def IsOwner(self, owner_email, group_id):\n", "code": "uri = self._ServiceUrl('owner', True, group_id, '', owner_email)\nreturn self._IsExisted(uri)", "path": "gdata\\apps\\groups\\service.py", "repo_name": "onyxfish/stovetop", "stars": 11, "license": "mit", "language": "python", "size": 720}
{"docstring": "\"\"\"Initialize the server object.\n\n:param uri: the URI of the server (for example\n            ``http://localhost:5984/``)\n:param cache: either a cache directory path (as a string) or an object\n              compatible with the `httplib2.FileCache` interface. If\n              `None` (the default), no caching is performed.\n:param timeout: socket timeout in number of seconds, or `None` for no\n                timeout\n\"\"\"\n", "func_signal": "def __init__(self, uri=DEFAULT_BASE_URI, cache=None, timeout=None):\n", "code": "http = httplib2.Http(cache=cache, timeout=timeout)\nhttp.force_exception_to_status_code = False\nself.resource = Resource(http, uri)", "path": "src\\couchdb\\build\\lib\\couchdb\\client.py", "repo_name": "thraxil/canopy", "stars": 8, "license": "None", "language": "python", "size": 7632}
{"docstring": "\"\"\"Create or replace an attachment.\n\n:param doc: the dictionary or `Document` object representing the\n            document that the attachment should be added to\n:param content: the content to upload, either a file-like object or\n                a string\n:param filename: the name of the attachment file; if omitted, this\n                 function tries to get the filename from the file-like\n                 object passed as the `content` argument value\n:param content_type: content type of the attachment; if omitted, the\n                     MIME type is guessed based on the file name\n                     extension\n:since: 0.4.1\n\"\"\"\n", "func_signal": "def put_attachment(self, doc, content, filename=None, content_type=None):\n", "code": "if hasattr(content, 'read'):\n    content = content.read()\nif filename is None:\n    if hasattr(content, 'name'):\n        filename = content.name\n    else:\n        raise ValueError('no filename specified for attachment')\nif content_type is None:\n    content_type = ';'.join(filter(None, guess_type(filename)))\n\nresp, data = self.resource(doc['_id']).put(filename, content=content,\n                                           headers={\n    'Content-Type': content_type\n}, rev=doc['_rev'])\ndoc.update({'_rev': data['rev']})", "path": "src\\couchdb\\build\\lib\\couchdb\\client.py", "repo_name": "thraxil/canopy", "stars": 8, "license": "None", "language": "python", "size": 7632}
{"docstring": "\"\"\"Deletes and creates a `database` on a `server`\"\"\"\n", "func_signal": "def set_up_database(server, database):\n", "code": "if database in server:\n    del server[database]\n\nreturn server.create(database)", "path": "src\\couchdb\\build\\lib\\couchdb\\tools\\replication_helper_test.py", "repo_name": "thraxil/canopy", "stars": 8, "license": "None", "language": "python", "size": 7632}
{"docstring": "\"\"\"Remove the document with the specified ID from the database.\n\n:param id: the document ID\n\"\"\"\n", "func_signal": "def __delitem__(self, id):\n", "code": "resp, data = self.resource.head(id)\nself.resource.delete(id, rev=resp['etag'].strip('\"'))", "path": "src\\couchdb\\build\\lib\\couchdb\\client.py", "repo_name": "thraxil/canopy", "stars": 8, "license": "None", "language": "python", "size": 7632}
{"docstring": "\"\"\"Assemble a uri based on a base, any number of path segments, and query\nstring parameters.\n\n>>> uri('http://example.org/', '/_all_dbs')\n'http://example.org/_all_dbs'\n\"\"\"\n", "func_signal": "def uri(base, *path, **query):\n", "code": "if base and base.endswith('/'):\n    base = base[:-1]\nretval = [base]\n\n# build the path\npath = '/'.join([''] +\n                [unicode_quote(s.strip('/')) for s in path\n                 if s is not None])\nif path:\n    retval.append(path)\n\n# build the query string\nparams = []\nfor name, value in query.items():\n    if type(value) in (list, tuple):\n        params.extend([(name, i) for i in value if i is not None])\n    elif value is not None:\n        if value is True:\n            value = 'true'\n        elif value is False:\n            value = 'false'\n        params.append((name, value))\nif params:\n    retval.extend(['?', unicode_urlencode(params)])\n\nreturn ''.join(retval)", "path": "src\\couchdb\\build\\lib\\couchdb\\client.py", "repo_name": "thraxil/canopy", "stars": 8, "license": "None", "language": "python", "size": 7632}
{"docstring": "\"\"\"Return whether the server contains a database with the specified\nname.\n\n:param name: the database name\n:return: `True` if a database with the name exists, `False` otherwise\n\"\"\"\n", "func_signal": "def __contains__(self, name):\n", "code": "try:\n    self.resource.head(validate_dbname(name))\n    return True\nexcept ResourceNotFound:\n    return False", "path": "src\\couchdb\\build\\lib\\couchdb\\client.py", "repo_name": "thraxil/canopy", "stars": 8, "license": "None", "language": "python", "size": 7632}
{"docstring": "\"\"\"Return the document with the specified ID.\n\n:param id: the document ID\n:return: a `Row` object representing the requested document\n:rtype: `Document`\n\"\"\"\n", "func_signal": "def __getitem__(self, id):\n", "code": "resp, data = self.resource.get(id)\nreturn Document(data)", "path": "src\\couchdb\\build\\lib\\couchdb\\client.py", "repo_name": "thraxil/canopy", "stars": 8, "license": "None", "language": "python", "size": 7632}
{"docstring": "\"\"\"Return the number of databases.\"\"\"\n", "func_signal": "def __len__(self):\n", "code": "resp, data = self.resource.get('_all_dbs')\nreturn len(data)", "path": "src\\couchdb\\build\\lib\\couchdb\\client.py", "repo_name": "thraxil/canopy", "stars": 8, "license": "None", "language": "python", "size": 7632}
{"docstring": "\"\"\"Return whether the database contains a document with the specified\nID.\n\n:param id: the document ID\n:return: `True` if a document with the ID exists, `False` otherwise\n\"\"\"\n", "func_signal": "def __contains__(self, id):\n", "code": "try:\n    self.resource.head(id)\n    return True\nexcept ResourceNotFound:\n    return False", "path": "src\\couchdb\\build\\lib\\couchdb\\client.py", "repo_name": "thraxil/canopy", "stars": 8, "license": "None", "language": "python", "size": 7632}
{"docstring": "\"\"\"Store the document in the given database.\"\"\"\n", "func_signal": "def store(self, db):\n", "code": "if self.id is None:\n    docid = db.create(self._data)\n    self._data = db.get(docid)\nelse:\n    db[self.id] = self._data\nreturn self", "path": "src\\couchdb\\build\\lib\\couchdb\\schema.py", "repo_name": "thraxil/canopy", "stars": 8, "license": "None", "language": "python", "size": 7632}
{"docstring": "\"\"\"Create a new database with the given name.\n\n:param name: the name of the database\n:return: a `Database` object representing the created database\n:rtype: `Database`\n:raise ResourceConflict: if a database with that name already exists\n\"\"\"\n", "func_signal": "def create(self, name):\n", "code": "self.resource.put(validate_dbname(name))\nreturn self[name]", "path": "src\\couchdb\\build\\lib\\couchdb\\client.py", "repo_name": "thraxil/canopy", "stars": 8, "license": "None", "language": "python", "size": 7632}
{"docstring": "\"\"\"Return the number of documents in the database.\"\"\"\n", "func_signal": "def __len__(self):\n", "code": "resp, data = self.resource.get()\nreturn data['doc_count']", "path": "src\\couchdb\\build\\lib\\couchdb\\client.py", "repo_name": "thraxil/canopy", "stars": 8, "license": "None", "language": "python", "size": 7632}
{"docstring": "\"\"\"Perform a bulk update or insertion of the given documents using a\nsingle HTTP request.\n\n>>> server = Server('http://localhost:5984/')\n>>> db = server.create('python-tests')\n>>> for doc in db.update([\n...     Document(type='Person', name='John Doe'),\n...     Document(type='Person', name='Mary Jane'),\n...     Document(type='City', name='Gotham City')\n... ]):\n...     print repr(doc) #doctest: +ELLIPSIS\n<Document u'...'@u'...' {'type': 'Person', 'name': 'John Doe'}>\n<Document u'...'@u'...' {'type': 'Person', 'name': 'Mary Jane'}>\n<Document u'...'@u'...' {'type': 'City', 'name': 'Gotham City'}>\n\n>>> del server['python-tests']\n\nIf an object in the documents list is not a dictionary, this method\nlooks for an ``items()`` method that can be used to convert the object\nto a dictionary. In this case, the returned generator will not update\nand yield the original object, but rather yield a dictionary with\n``id`` and ``rev`` keys.\n\n:param documents: a sequence of dictionaries or `Document` objects, or\n                  objects providing a ``items()`` method that can be\n                  used to convert them to a dictionary\n:return: an iterable over the resulting documents\n:rtype: ``generator``\n\n:since: version 0.2\n\"\"\"\n", "func_signal": "def update(self, documents):\n", "code": "docs = []\nfor doc in documents:\n    if isinstance(doc, dict):\n        docs.append(doc)\n    elif hasattr(doc, 'items'):\n        docs.append(dict(doc.items()))\n    else:\n        raise TypeError('expected dict, got %s' % type(doc))\nresp, data = self.resource.post('_bulk_docs', content={'docs': docs})\nassert data['ok'] # FIXME: Should probably raise a proper exception\ndef _update():\n    for idx, result in enumerate(data['new_revs']):\n        doc = documents[idx]\n        if isinstance(doc, dict):\n            doc.update({'_id': result['id'], '_rev': result['rev']})\n            yield doc\n        else:\n            yield result\nreturn _update()", "path": "src\\couchdb\\build\\lib\\couchdb\\client.py", "repo_name": "thraxil/canopy", "stars": 8, "license": "None", "language": "python", "size": 7632}
{"docstring": "\"\"\"\nCreates a bootstrap script, which is like this script but with\nextend_parser, adjust_options, and after_install hooks.\n\nThis returns a string that (written to disk of course) can be used\nas a bootstrap script with your own customizations.  The script\nwill be the standard virtualenv.py script, with your extra text\nadded (your extra text should be Python code).\n\nIf you include these functions, they will be called:\n\n``extend_parser(optparse_parser)``:\n    You can add or remove options from the parser here.\n\n``adjust_options(options, args)``:\n    You can change options here, or change the args (if you accept\n    different kinds of arguments, be sure you modify ``args`` so it is\n    only ``[DEST_DIR]``).\n\n``after_install(options, home_dir)``:\n\n    After everything is installed, this function is called.  This\n    is probably the function you are most likely to use.  An\n    example would be::\n\n        def after_install(options, home_dir):\n            subprocess.call([join(home_dir, 'bin', 'easy_install'),\n                             'MyPackage'])\n            subprocess.call([join(home_dir, 'bin', 'my-package-script'),\n                             'setup', home_dir])\n\n    This example immediately installs a package, and runs a setup\n    script from that package.\n\"\"\"\n", "func_signal": "def create_bootstrap_script(extra_text):\n", "code": "filename = __file__\nif filename.endswith('.pyc'):\n    filename = filename[:-1]\nf = open(filename, 'rb')\ncontent = f.read()\nf.close()\ncontent = ('#!/usr/bin/env python\\n'\n           + '## WARNING: This file is generated\\n'\n           + content)\nreturn content.replace('##EXT' 'END##', extra_text)", "path": "ve-bootstrap.py", "repo_name": "thraxil/canopy", "stars": 8, "license": "None", "language": "python", "size": 7632}
{"docstring": "\"\"\"\nSome platforms (particularly Gentoo on x64) put things in lib64/pythonX.Y\ninstead of lib/pythonX.Y.  If this is such a platform we'll just create a\nsymlink so lib64 points to lib\n\"\"\"\n", "func_signal": "def fix_lib64(lib_dir):\n", "code": "if [(i,j) for (i,j) in distutils.sysconfig.get_config_vars().items() \n    if isinstance(j, basestring) and 'lib64' in j]:\n    logger.debug('This system uses lib64; symlinking lib64 to lib')\n    assert os.path.basename(lib_dir) == 'python%s' % sys.version[:3], (\n        \"Unexpected python lib dir: %r\" % lib_dir)\n    lib_parent = os.path.dirname(lib_dir)\n    assert os.path.basename(lib_parent) == 'lib', (\n        \"Unexpected parent dir: %r\" % lib_parent)\n    copyfile(lib_parent, os.path.join(os.path.dirname(lib_parent), 'lib64'))", "path": "ve-bootstrap.py", "repo_name": "thraxil/canopy", "stars": 8, "license": "None", "language": "python", "size": 7632}
{"docstring": "\"\"\"Delete the specified attachment.\n\n:param doc: the dictionary or `Document` object representing the\n            document that the attachment belongs to\n:param filename: the name of the attachment file\n:since: 0.4.1\n\"\"\"\n", "func_signal": "def delete_attachment(self, doc, filename):\n", "code": "resp, data = self.resource(doc['_id']).delete(filename, rev=doc['_rev'])\ndoc.update({'_rev': data['rev']})", "path": "src\\couchdb\\build\\lib\\couchdb\\client.py", "repo_name": "thraxil/canopy", "stars": 8, "license": "None", "language": "python", "size": 7632}
{"docstring": "\"\"\"If we are in a progress scope, and no log messages have been\nshown, write out another '.'\"\"\"\n", "func_signal": "def show_progress(self):\n", "code": "if self.in_progress_hanging:\n    sys.stdout.write('.')\n    sys.stdout.flush()", "path": "ve-bootstrap.py", "repo_name": "thraxil/canopy", "stars": 8, "license": "None", "language": "python", "size": 7632}
{"docstring": "\"\"\"Return the document with the specified ID.\n\n:param id: the document ID\n:param default: the default value to return when the document is not\n                found\n:return: a `Row` object representing the requested document, or `None`\n         if no document with the ID was found\n:rtype: `Document`\n\"\"\"\n", "func_signal": "def get(self, id, default=None, **options):\n", "code": "try:\n    resp, data = self.resource.get(id, **options)\nexcept ResourceNotFound:\n    return default\nelse:\n    return Document(data)", "path": "src\\couchdb\\build\\lib\\couchdb\\client.py", "repo_name": "thraxil/canopy", "stars": 8, "license": "None", "language": "python", "size": 7632}
{"docstring": "\"\"\"Create a new document in the database with a generated ID.\n\nAny keyword arguments are used to populate the fields of the new\ndocument.\n\n:param data: the data to store in the document\n:return: the ID of the created document\n:rtype: `unicode`\n\"\"\"\n", "func_signal": "def create(self, data):\n", "code": "resp, data = self.resource.post(content=data)\nreturn data['id']", "path": "src\\couchdb\\build\\lib\\couchdb\\client.py", "repo_name": "thraxil/canopy", "stars": 8, "license": "None", "language": "python", "size": 7632}
{"docstring": "\"\"\"Return the fields as a list of ``(name, value)`` tuples.\n\nThis method is provided to enable easy conversion to native dictionary\nobjects, for example to allow use of `schema.Document` instances with\n`client.Database.update`.\n\n>>> class Post(Document):\n...     title = TextField()\n...     author = TextField()\n>>> post = Post(id='foo-bar', title='Foo bar', author='Joe')\n>>> sorted(post.items())\n[('_id', 'foo-bar'), ('author', u'Joe'), ('title', u'Foo bar')]\n\n:return: a list of ``(name, value)`` tuples\n\"\"\"\n", "func_signal": "def items(self):\n", "code": "retval = []\nif self.id is not None:\n    retval.append(('_id', self.id))\n    if self.rev is not None:\n        retval.append(('_rev', self.rev))\nfor name, value in self._data.items():\n    if name not in ('_id', '_rev'):\n        retval.append((name, value))\nreturn retval", "path": "src\\couchdb\\build\\lib\\couchdb\\schema.py", "repo_name": "thraxil/canopy", "stars": 8, "license": "None", "language": "python", "size": 7632}
{"docstring": "\"\"\"Wraps generators that yield non-string objects and converts them to a string.\"\"\"\n\n", "func_signal": "def generateStrings(fn):\n", "code": "def wrapper():\n    for value in fn():\n        yield str(value)\n\nreturn wrapper", "path": "devon\\stream.py", "repo_name": "joehewitt/devon", "stars": 8, "license": "bsd-3-clause", "language": "python", "size": 432}
{"docstring": "\"\"\"Convenience function to recursively search for files that match a pattern\"\"\"\n\n", "func_signal": "def findFiles(rootPath, pattern):\n", "code": "def searchDir(rootPath, pattern, matches=None):\n    if not matches:\n        matches = []\n    \n    found = glob.glob(os.path.join(rootPath, pattern))\n    if found:\n        matches += found\n        \n    names = os.listdir(rootPath)\n    for name in names:\n        fullPath = os.path.join(rootPath, name)\n        if os.path.isdir(fullPath) and not name[0] == \".\":\n            matches = searchDir(fullPath, pattern, matches)\n            \n    return matches\n\nif os.path.isdir(rootPath):\n    return searchDir(rootPath, pattern)", "path": "devon\\builtin.py", "repo_name": "joehewitt/devon", "stars": 8, "license": "bsd-3-clause", "language": "python", "size": 432}
{"docstring": "\"\"\" Retrieves the project's dependent libraries. By default, a complete\n    list of library names (e.g. \"foopy.lib\") is returned, including\n    system libraries, such as gdi.lib. If absPaths is specified, we may\n    not be able to return the absolute paths of all explicit libs,\n    particularly system libraries.\n\"\"\"\n# XXXblake This is temporary pending ExternalProject.getBuildTargetS()\n", "func_signal": "def getLibs(self, deep=False, absPaths=False):\n", "code": "def getAbsPathsOfExplicitLibs(explicitLibs):\n    libs = []\n    for lib in explicitLibs:\n        # XXXblake Make getExportingProject() just take the export path,\n        # since we end up indexing slash again below\n        project = getExportingProject(lib)\n        if project: # XXXblake Not currently checking dups...is it faster to just pass dups to the linker?\n            libs.append(project.getBuildPath(lib[lib.rfind(\"/\")+1:]))\n        else:\n            libs.append(lib)\n    return libs\n\nif absPaths:\n    libs = getAbsPathsOfExplicitLibs(self.libs)\nelse:        \n    libs = self.libs[:]\n\n# XXXblake getDependencies() should probably also return any projects\n# culled from our explicit libs list\n\nfor dep in self.getDependencies(deep):\n    depExplicitLibs = dep.libs\n    if absPaths:\n        depExplicitLibs = getAbsPathsOfExplicitLibs(depExplicitLibs)\n    libs.extend(depExplicitLibs)\n\n    target = dep.getBuildTarget()\n    if target:    # If the project is built \n        if absPaths:\n            lib = target\n        else:               \n            lib = os.path.basename(target)\n\n        # XXXblake Do we really need this check if getDependencies() already does dup-checking?\n        if not lib in libs:\n            libs.append(lib)\n        \nreturn libs", "path": "devon\\projects.py", "repo_name": "joehewitt/devon", "stars": 8, "license": "bsd-3-clause", "language": "python", "size": 432}
{"docstring": "\"\"\" Generates a summary of bugs fixed in a project since the last commit.\"\"\"\n\n", "func_signal": "def getBugsFixedSummary(projectPath):\n", "code": "if not os.path.isdir(projectPath):\n    projectPath = os.path.dirname(projectPath)\n    if os.path.basename(projectPath) in [\".git\", \".svn\"]:\n        projectPath = os.path.dirname(projectPath)\n\nfiles = list(getBugsFixedSinceLastCommit(projectPath))\nchanges = []\nif files:\n    for title, bug, delta in files:\n        changes.append(\"* Fixed bug %s: %s\" % (bug, title))\n\nreturn \"\\n\".join(changes)", "path": "devon\\bugs.py", "repo_name": "joehewitt/devon", "stars": 8, "license": "bsd-3-clause", "language": "python", "size": 432}
{"docstring": "\"\"\" Gets the full path of a single target in the build directory of the build project.\n    All returned paths end with trailing slashes. \"\"\"\n\n", "func_signal": "def getBuildPath(self, targetPath=None):\n", "code": "if self.path.find(self.buildProject.path) == 0:\n    relativePath = self.path[len(self.buildProject.path)+1:] # +1 for the trailing slash\n    session = getSession()           \n    basePath = os.path.join(self.buildRootPath, self.getPlatformAbbreviation())\n    basePath = os.path.join(basePath, self.buildDir, relativePath)\nelse:\n    raise Exception(\"Not currently supported/tested\")\n\nif targetPath:\n    return os.path.join(basePath, targetPath)\n              \nreturn basePath", "path": "devon\\projects.py", "repo_name": "joehewitt/devon", "stars": 8, "license": "bsd-3-clause", "language": "python", "size": 432}
{"docstring": "\"\"\" Retrieves the project's absolute include paths. \"\"\"\n\n", "func_signal": "def getIncludePaths(self, deep = False, source = None):\n", "code": "includes = Project.getIncludePaths(self)\nincludes.append(self.path)\n\nif deep:\n    for dep in self.getDependencies(deep, source):\n        for include in dep.getIncludePaths():\n            if not include in includes:\n                includes.append(include)\n\nreturn includes", "path": "devon\\projects.py", "repo_name": "joehewitt/devon", "stars": 8, "license": "bsd-3-clause", "language": "python", "size": 432}
{"docstring": "\"\"\" Given an absolute or relative export path, returns the same path\n    without symlinks. This is similar to os.path.realpath(), but it's\n    much faster because it only works for known export paths.\n    \n    Examples:\n\n    INPUT                                  OUTPUT\n    \n    <Absolute Paths>\n    \n    /foo/shared/base/Suade/base/foopy.h  /foo/shared/base/include/foopy.h\n               \n    <Relative Paths>\n    \n    Suade/base/foopy.h                      include/foopy.h\n    \n\"\"\"\n\n", "func_signal": "def makeConcrete(self, source):\n", "code": "if os.path.isabs(source):\n    relSrcPath = self.getRelativePath(source)\n    absPaths = True\nelse:\n    relSrcPath = source\n    absPaths = False\n\n# Enumerate our real path -> virtual path mapping until we find the corresponding virtual\n# path, then splice in the real path in its place.\nfor relRealPath in self.exports:\n    relVirtualPath = self.exports[relRealPath]\n    if relVirtualPath:\n        rvpLen = len(relVirtualPath)\n        \n        # See if the source path begins with the real path. We have to check for a trailing\n        # slash to ensure we're actually matching directories. If we just compared rvpLen\n        # letters, we'd consider e.g. \"DevonLog/Foopy.h\" to match an export path of \"Devon\"\n        if relSrcPath[:rvpLen+1] == (relVirtualPath + \"/\"):\n            source = os.path.join(relRealPath, relSrcPath[rvpLen+1:])\n            \n            if absPaths:\n                source = self.getAbsolutePath(source)\n            \n            break\n    else:\n        # No virtual path specified. That means we're exporting the\n        # directory itself, so check and see if the head of the real path\n        # + the head of the source path exists. In other words, say the\n        # project exported { \"foopy/bar\": None } given foopy/bar/baz/blah.h\n        # and the source path is <bar/baz/blah.h>.\n        # XXXblake Not really making concrete here...\n        \n        relPath = os.path.join(os.path.dirname(relRealPath), relSrcPath)\n        absPath = self.getAbsolutePath(relPath)\n\n        if os.path.exists(absPath):\n            if absPaths:\n                source = absPath\n            else:\n                source = relPath\n            break\n        \nreturn source", "path": "devon\\projects.py", "repo_name": "joehewitt/devon", "stars": 8, "license": "bsd-3-clause", "language": "python", "size": 432}
{"docstring": "\"\"\" Gets the line number where a bug is defined.\"\"\"\n\n", "func_signal": "def getBugLineNumber(text, bugNumber):\n", "code": "reBugHeader = re.compile(r\".*?\\s*@%s\\s*\\n={3,}\\n\" % bugNumber, re.MULTILINE)\nm = reBugHeader.search(text)\nif m:\n    text = text[0:m.start()]\n    return len(text.split(\"\\n\"))", "path": "devon\\bugs.py", "repo_name": "joehewitt/devon", "stars": 8, "license": "bsd-3-clause", "language": "python", "size": 432}
{"docstring": "\"\"\" Given an absolute or relative concrete export path, returns the\n    same path with symlinks if possible. \"\"\"\n\n", "func_signal": "def makeVirtual(self, source):\n", "code": "if os.path.isabs(source):\n    absPaths = True\n    relPath = self.getRelativePath(source)\nelse:\n    absPaths = False\n    relPath = source\n    \n# If not, enumerate our real paths until we find which one this is    \nfor realPath in self.exports:\n    virtualPath = self.exports[realPath]\n    if not virtualPath:\n        continue\n\n    if os.path.dirname(relPath) == realPath:\n        path = os.path.join(virtualPath, os.path.basename(source))\n        if absPaths:\n            path = self.getAbsolutePath(os.path.join(realPath, kVirtualExportDir, path))\n        return path\n        \n# Can't be virtualized, so return the original source.\nreturn source", "path": "devon\\projects.py", "repo_name": "joehewitt/devon", "stars": 8, "license": "bsd-3-clause", "language": "python", "size": 432}
{"docstring": "\"\"\" Write the dependency map to the project's dependency file \"\"\"\n\n", "func_signal": "def writeDependencyFile(self):\n", "code": "dir = os.path.dirname(self.__dependencyFile)\nif not os.path.exists(self.__dependencyFile) and not os.path.exists(dir):\n    os.makedirs(dir)\n    \nf = file(self.__dependencyFile, \"w\")\nf.write(\"deps = {\")\nfor source, deps in self.deps.iteritems():\n    f.write(\"'%s': [\" % source)\n    for dep in deps:\n        f.write(\"'%s',\" % dep)\n    f.write(\"],\")\nf.write(\"}\")", "path": "devon\\projects.py", "repo_name": "joehewitt/devon", "stars": 8, "license": "bsd-3-clause", "language": "python", "size": 432}
{"docstring": "\"\"\" Gets the path of the bug history file that goes with a bug file.\"\"\"\n", "func_signal": "def getBugHistoryPath(bugFilePath):\n", "code": "fileName,fileExt = os.path.splitext(bugFilePath)\nreturn \"%s-History%s\" % (fileName, fileExt)", "path": "devon\\bugs.py", "repo_name": "joehewitt/devon", "stars": 8, "license": "bsd-3-clause", "language": "python", "size": 432}
{"docstring": "\"\"\"Loads the project that lives in a specified directory\"\"\"\n\n", "func_signal": "def load(projectPath = None, recurse = True):\n", "code": "if projectPath == None:\n    projectPath = os.getcwd()\n\nlocalProjectPath = __findProjectPath(projectPath, recurse)\nif localProjectPath == None:\n    # Either the project path is invalid, or it's an external project. If it's external, it\n    # should already be in our cache from when we read in the user file.\n    if projectPath in projectCache:\n        return projectCache[projectPath]\n    \n    raise Exception(\"%s is not a valid project path \" % projectPath)\n\nproject = __importProject(localProjectPath)\nreturn project", "path": "devon\\projects.py", "repo_name": "joehewitt/devon", "stars": 8, "license": "bsd-3-clause", "language": "python", "size": 432}
{"docstring": "\"\"\"Gets the path of the build target for a ManyToOne project\"\"\"\n\n", "func_signal": "def getBuildTarget(self):\n", "code": "if not self.build or isinstance(self.build, devon.maker.MakerOneToOne):\n    return None\n    \ntargetPath = self.build.getTarget(self)\nreturn self.getBuildPath(targetPath)", "path": "devon\\projects.py", "repo_name": "joehewitt/devon", "stars": 8, "license": "bsd-3-clause", "language": "python", "size": 432}
{"docstring": "\"\"\" Marks a bug fixed by moving it to the bug history file.\"\"\"\n\n", "func_signal": "def markBugFixed(text, path=\"\", lineNumber=0):\n", "code": "doneFile = getBugHistory(path, \"a\")\n\nlines = text.split(\"\\n\")\nlineNumber += 1\nfor i in xrange(0, lineNumber):\n    index = lineNumber-i\n    line = lines[index]\n    if reLine.match(line):\n        lines.insert(index+1, selectedSeparator)\n        break\n\ntext = \"\\n\".join(lines)\n\nfor title, bug, tag, timestamp, separator, body in parseBugsFromBugFile(text):\n    if body.find(selectedSeparator) != -1:\n        if doneFile:\n            body = body.replace(selectedSeparator+\"\\n\\n\", \"\")\n            timestamp = datetime.datetime.now().ctime()                \n            doneFile.write(\"%s%s%s <%s>\\n%s\\n%s\" \\\n                % (title, \" %s\" % bug if bug else \"\", \" %s\" % tag if tag else \"\",\n                    timestamp, separator, body))\n    else:\n        sys.stdout.write(\"%s%s%s\\n%s\\n%s\" \\\n            % (title, \" %s\" % bug if bug else \"\", \" %s\" % tag if tag else \"\", separator, body))", "path": "devon\\bugs.py", "repo_name": "joehewitt/devon", "stars": 8, "license": "bsd-3-clause", "language": "python", "size": 432}
{"docstring": "# Copy all of the variables from the project file onto the project object\n# if they are in the set of pre-defined project member variables\n", "func_signal": "def writeBranch(self, projectLocals):\n", "code": "for name in vars(self):\n    if name in projectLocals:\n        selfValue = getattr(self, name)\n        localValue = projectLocals[name]\n        \n        if isinstance(selfValue, ProjectBranch):\n            # If the member is a branch object, the project variable is expected to be\n            # a class whose members we copy directly onto the branch\n            if localValue and isinstance(localValue, types.ClassType):\n                selfValue.writeBranch(vars(localValue))\n        \n        else:\n            setattr(self, name, localValue)", "path": "devon\\projects.py", "repo_name": "joehewitt/devon", "stars": 8, "license": "bsd-3-clause", "language": "python", "size": 432}
{"docstring": "# A project should only be written if it's buildable, and a project is only buildable if it\n        # itself is buildable or if any of its children (recursively) are buildable.\n        # XXXblake This is slow because we pre-calculate all child projects. It would be faster if\n        # getChildProjects() was a generator, or if it cached the children.\n", "func_signal": "def writeProject(project, out, alwaysWrite=False):\n", "code": "        if not alwaysWrite:\n            alwaysWrite = project.build is not None\n            if not alwaysWrite:\n                for childProject in project.getChildProjects(True):\n                    if childProject.build:\n                        alwaysWrite = True\n                        break\n            \n        for childProject in project.getChildProjects(False):\n            shouldWrite = alwaysWrite or childProject.dist\n                \n            if shouldWrite:\n                title = os.path.basename(childProject.path)\n                path = childProject.path.replace(\"\\\\\", \"/\")\n                out << '{name: \"%s\", itemType: \"Project\", id: \"%s\", keepId: true, children: [\\n' \\\n                    % (title, path)\n            \n            writeProject(childProject, out)\n\n            if shouldWrite:\n                out << ']},\\n' << Flush", "path": "devon\\run.py", "repo_name": "joehewitt/devon", "stars": 8, "license": "bsd-3-clause", "language": "python", "size": 432}
{"docstring": "\"\"\" Instantiates each of the external projects found in the user file. \"\"\"\n\n", "func_signal": "def loadExternalProjects():\n", "code": "path = os.path.join(os.path.expanduser(devon.userPath), userFileName)\nlocals = __importProjectLocals(path)\n\nfor attr in locals:\n    obj = locals[attr]\n    \n    if not isinstance(obj, types.ClassType) \\\n        or not issubclass(obj, ExternalProject) \\\n        or obj is ExternalProject:\n\n        continue\n       \n    # Instantiate the external project. ExternalProject's constructor will\n    # do everything else.\n    obj = obj()", "path": "devon\\projects.py", "repo_name": "joehewitt/devon", "stars": 8, "license": "bsd-3-clause", "language": "python", "size": 432}
{"docstring": "\"\"\" Opens the bug history file that goes with a bug file.\"\"\"\n", "func_signal": "def getBugHistory(bugFilePath, mode=\"r\"):\n", "code": "if bugFilePath:\n    historyPath = getBugHistoryPath(bugFilePath)\n    return file(historyPath, mode)", "path": "devon\\bugs.py", "repo_name": "joehewitt/devon", "stars": 8, "license": "bsd-3-clause", "language": "python", "size": 432}
{"docstring": "\"\"\" Returns a list of the project's dependent projects. \"\"\"\n      \n# XXXblake The \"source\" param is a big, big hack. Basically what we're doing is calculating\n# one source's dependencies by retrieving all the *potential* dependencies, i.e. anything\n# exported by a project that exports one of the source's dependencies, recursively.\n      \n", "func_signal": "def getDependencies(self, deep = False, source = None, projects = None):\n", "code": "if projects is None:\n    projects = []\n\n# Ensure the dependency map is up-to-date\nself.__updateDependencies()\n\n# Now flatten the dependency map into a list, retrieve the project from\n# the dependency, and, if requested, get each dependency's dependencies\n\nfor src, deps in self.deps.iteritems():\n    if source and src[-2:] not in (\".h\", \".y\") and not src[-3:] == \".lex\" \\\n            and not src == source:\n        continue\n    \n    for dep in deps:\n        project = getExportingProject(dep)\n        \n        # Don't include ourselves or duplicates in the dependency list\n        if project and project is not self and not project in projects:\n            yield project\n            projects.append(project)\n\n            # XXXblake Profile to see if breadth approach (myProjects list) is faster\n            if deep:\n                for proj in project.getDependencies(deep, bool(source), projects):\n                    yield proj", "path": "devon\\projects.py", "repo_name": "joehewitt/devon", "stars": 8, "license": "bsd-3-clause", "language": "python", "size": 432}
{"docstring": "# XXX This really should only write out the log names for just the project that\n#     we are about to execute, but for now we only have a flat list of all log names\n\n", "func_signal": "def writeDisabledLogs(project, disabledLogs):\n", "code": "for childProject in project.getChildProjects():\n    if isinstance(childProject.build, devon.makers.link.Link):\n        linkCommand = devon.makers.link.LinkTestRunner()\n        commandName = linkCommand.getTarget(childProject)\n        buildPath = childProject.getBuildPath(commandName)\n        #logsFilePath = os.path.join(os.path.dirname(buildPath), logsFileName)\n        \n        logsFile = file(disabledLogsFilePath, \"w\")\n        for logName in disabledLogs:\n            logsFile.write(logName + \"\\n\")\n        logsFile.close()", "path": "devon\\run.py", "repo_name": "joehewitt/devon", "stars": 8, "license": "bsd-3-clause", "language": "python", "size": 432}
{"docstring": "\"\"\"Facebook API call. See http://developers.facebook.com/documentation.php?v=1.0&method=photos.upload\n\nsize -- an optional size (width, height) to resize the image to before uploading. Resizes by default\n        to Facebook's maximum display dimension of 720.\n\"\"\"\n", "func_signal": "def upload(self, image, aid=None, uid=None, caption=None, size=(720, 720), filename=None, callback=None):\n", "code": "args = {}\n\nif uid is not None:\n    args['uid'] = uid\n\nif aid is not None:\n    args['aid'] = aid\n\nif caption is not None:\n    args['caption'] = caption\n\nif self._client.oauth2:\n    url = self._client.facebook_secure_url\nelse:\n    url = self._client.facebook_url\n\nargs = self._client._build_post_args('facebook.photos.upload', self._client._add_session_args(args))\n\ntry:\n    import cStringIO as StringIO\nexcept ImportError:\n    import StringIO\n\n# check for a filename specified...if the user is passing binary data in\n# image then a filename will be specified\nif filename is None:\n    try:\n        import Image\n    except ImportError:\n        data = StringIO.StringIO(open(image, 'rb').read())\n    else:\n        img = Image.open(image)\n        if size:\n            img.thumbnail(size, Image.ANTIALIAS)\n        data = StringIO.StringIO()\n        img.save(data, img.format)\nelse:\n    # there was a filename specified, which indicates that image was not\n    # the path to an image file but rather the binary data of a file\n    data = StringIO.StringIO(image)\n    image = filename\n\ncontent_type, body = self.__encode_multipart_formdata(list(args.iteritems()), [(image, data)])\nurlinfo = urlparse.urlsplit(url)\ntry:\n    content_length = len(body)\n    chunk_size = 4096\n\n    if self._client.oauth2:\n        h = httplib.HTTPSConnection(urlinfo[1])\n    else:\n        h = httplib.HTTPConnection(urlinfo[1])\n    h.putrequest('POST', urlinfo[2])\n    h.putheader('Content-Type', content_type)\n    h.putheader('Content-Length', str(content_length))\n    h.putheader('MIME-Version', '1.0')\n    h.putheader('User-Agent', 'PyFacebook Client Library')\n    h.endheaders()\n\n    if callback:\n        count = 0\n        while len(body) > 0:\n            if len(body) < chunk_size:\n                data = body\n                body = ''\n            else:\n                data = body[0:chunk_size]\n                body = body[chunk_size:]\n\n            h.send(data)\n            count += 1\n            callback(count, chunk_size, content_length)\n    else:\n        h.send(body)\n\n    response = h.getresponse()\n\n    if response.status != 200:\n        raise Exception('Error uploading photo: Facebook returned HTTP %s (%s)' % (response.status, response.reason))\n    response = response.read()\nexcept:\n    # sending the photo failed, perhaps we are using GAE\n    try:\n        from google.appengine.api import urlfetch\n\n        try:\n            response = urlread(url=url,data=body,headers={'POST':urlinfo[2],'Content-Type':content_type,'MIME-Version':'1.0'})\n        except urllib2.URLError:\n            raise Exception('Error uploading photo: Facebook returned %s' % (response))\n    except ImportError:\n        # could not import from google.appengine.api, so we are not running in GAE\n        raise Exception('Error uploading photo.')\n\nreturn self._client._parse_response(response, 'facebook.photos.upload')", "path": "auth\\facebook.py", "repo_name": "rjernst/calvary-website", "stars": 10, "license": "None", "language": "python", "size": 2146}
{"docstring": "\"\"\"Facebook API call. See http://wiki.developers.facebook.com/index.php/Video.upload\"\"\"\n", "func_signal": "def upload(self, video, aid=None, title=None, description=None, filename=None, uid=None, privacy=None, callback=None):\n", "code": "args = {}\n\nif aid is not None:\n    args['aid'] = aid\n\nif title is not None:\n    args['title'] = title\n\nif description is not None:\n    args['description'] = title\n    \nif uid is not None:\n    args['uid'] = uid\n\nif privacy is not None:\n    args['privacy'] = privacy\n\nargs = self._client._build_post_args('facebook.video.upload', self._client._add_session_args(args))\n\ntry:\n    import cStringIO as StringIO\nexcept ImportError:\n    import StringIO\n\n# check for a filename specified...if the user is passing binary data in\n# video then a filename will be specified\nif filename is None:\n    data = StringIO.StringIO(open(video, 'rb').read())\nelse:\n    # there was a filename specified, which indicates that video was not\n    # the path to an video file but rather the binary data of a file\n    data = StringIO.StringIO(video)\n    video = filename\n\ncontent_type, body = self.__encode_multipart_formdata(list(args.iteritems()), [(video, data)])\n\n# Set the correct End Point Url, note this is different to all other FB EndPoints\nurlinfo = urlparse.urlsplit(FACEBOOK_VIDEO_URL)\ntry:\n    content_length = len(body)\n    chunk_size = 4096\n\n    h = httplib.HTTPConnection(urlinfo[1])\n    h.putrequest('POST', urlinfo[2])\n    h.putheader('Content-Type', content_type)\n    h.putheader('Content-Length', str(content_length))\n    h.putheader('MIME-Version', '1.0')\n    h.putheader('User-Agent', 'PyFacebook Client Library')\n    h.endheaders()\n\n    if callback:\n        count = 0\n        while len(body) > 0:\n            if len(body) < chunk_size:\n                data = body\n                body = ''\n            else:\n                data = body[0:chunk_size]\n                body = body[chunk_size:]\n\n            h.send(data)\n            count += 1\n            callback(count, chunk_size, content_length)\n    else:\n        h.send(body)\n\n    response = h.getresponse()\n\n    if response.status != 200:\n        raise Exception('Error uploading video: Facebook returned HTTP %s (%s)' % (response.status, response.reason))\n    response = response.read()\nexcept:\n    # sending the photo failed, perhaps we are using GAE\n    try:\n        from google.appengine.api import urlfetch\n\n        try:\n            response = urlread(url=FACEBOOK_VIDEO_URL,data=body,headers={'POST':urlinfo[2],'Content-Type':content_type,'MIME-Version':'1.0'})\n        except urllib2.URLError:\n            raise Exception('Error uploading video: Facebook returned %s' % (response))\n    except ImportError:\n        # could not import from google.appengine.api, so we are not running in GAE\n        raise Exception('Error uploading video.')\n\nreturn self._client._parse_response(response, 'facebook.video.upload')", "path": "auth\\facebook.py", "repo_name": "rjernst/calvary-website", "stars": 10, "license": "None", "language": "python", "size": 2146}
{"docstring": "\"\"\"Facebook API call. See http://developers.facebook.com/documentation.php?v=1.0&method=friends.get\"\"\"\n", "func_signal": "def get(self, **kwargs):\n", "code": "if not kwargs.get('flid') and self._client._friends:\n    return self._client._friends\nreturn super(FriendsProxy, self).get(**kwargs)", "path": "auth\\facebook.py", "repo_name": "rjernst/calvary-website", "stars": 10, "license": "None", "language": "python", "size": 2146}
{"docstring": "\"\"\"Parses an XML response node from Facebook.\"\"\"\n", "func_signal": "def _parse_response_item(self, node):\n", "code": "if node.nodeType == node.DOCUMENT_NODE and \\\n    node.childNodes[0].hasAttributes() and \\\n    node.childNodes[0].hasAttribute('list') and \\\n    node.childNodes[0].getAttribute('list') == \"true\":\n    return {node.childNodes[0].nodeName: self._parse_response_list(node.childNodes[0])}\nelif node.nodeType == node.ELEMENT_NODE and \\\n    node.hasAttributes() and \\\n    node.hasAttribute('list') and \\\n    node.getAttribute('list')==\"true\":\n    return self._parse_response_list(node)\nelif len(filter(lambda x: x.nodeType == x.ELEMENT_NODE, node.childNodes)) > 0:\n    return self._parse_response_dict(node)\nelse:\n    return ''.join(node.data for node in node.childNodes if node.nodeType == node.TEXT_NODE)", "path": "auth\\facebook.py", "repo_name": "rjernst/calvary-website", "stars": 10, "license": "None", "language": "python", "size": 2146}
{"docstring": "\"\"\"Hashes arguments by joining key=value pairs, appending a secret, and then taking the MD5 hex digest.\"\"\"\n# @author: houyr\n# fix for UnicodeEncodeError\n", "func_signal": "def _hash_args(self, args, secret=None):\n", "code": "hasher = hashlib.md5(''.join(['%s=%s' % (isinstance(x, unicode) and x.encode(\"utf-8\") or x, isinstance(args[x], unicode) and args[x].encode(\"utf-8\") or args[x]) for x in sorted(args.keys())]))\nif secret:\n    hasher.update(secret)\nelif self.secret:\n    hasher.update(self.secret)\nelse:\n    hasher.update(self.secret_key)\nreturn hasher.hexdigest()", "path": "auth\\facebook.py", "repo_name": "rjernst/calvary-website", "stars": 10, "license": "None", "language": "python", "size": 2146}
{"docstring": "\"\"\"\nReturns the URL that the user should be redirected to in order to grant an extended permission.\n\next_perm -- the name of the extended permission to request\nnext     -- the URL that Facebook should redirect to after login\n\n\"\"\"\n", "func_signal": "def get_ext_perm_url(self, ext_perm, next=None, popup=False):\n", "code": "args = {'ext_perm': ext_perm, 'api_key': self.api_key, 'v': '1.0'}\n\nif next is not None:\n    args['next'] = next\n\nif popup is True:\n    args['popup'] = 1\n\nreturn self.get_url('authorize', **args)", "path": "auth\\facebook.py", "repo_name": "rjernst/calvary-website", "stars": 10, "license": "None", "language": "python", "size": 2146}
{"docstring": "\"\"\"Encodes a multipart/form-data message to upload an image.\"\"\"\n", "func_signal": "def __encode_multipart_formdata(self, fields, files):\n", "code": "boundary = '-------tHISiStheMulTIFoRMbOUNDaRY'\ncrlf = '\\r\\n'\nl = []\n\nfor (key, value) in fields:\n    l.append('--' + boundary)\n    l.append('Content-Disposition: form-data; name=\"%s\"' % str(key))\n    l.append('')\n    l.append(str(value))\nfor (filename, value) in files:\n    l.append('--' + boundary)\n    l.append('Content-Disposition: form-data; filename=\"%s\"' % (str(filename), ))\n    l.append('Content-Type: %s' % self.__get_content_type(filename))\n    l.append('')\n    l.append(value.getvalue())\nl.append('--' + boundary + '--')\nl.append('')\nbody = crlf.join(l)\ncontent_type = 'multipart/form-data; boundary=%s' % boundary\nreturn content_type, body", "path": "auth\\facebook.py", "repo_name": "rjernst/calvary-website", "stars": 10, "license": "None", "language": "python", "size": 2146}
{"docstring": "\"\"\"\nValidates parameters passed to a Facebook connect app through cookies\n\n\"\"\"\n", "func_signal": "def validate_cookie(self, cookies):\n", "code": "import logging\nkey = \"fbs_%s\" % self.api_key\n# check for the hashed secret\nif key not in cookies:\n    return None\nlogging.info(cookies[key])\n\n# cutoff quotes frok cookie value\nreturn urlparse.parse_qs(cookies[key][1:-1])\n\n# TODO: make this work\n# check the hashes match before returning them\n#if self._hash_args(args) == cookies[self.api_key]:\n #   return args", "path": "auth\\facebook.py", "repo_name": "rjernst/calvary-website", "stars": 10, "license": "None", "language": "python", "size": 2146}
{"docstring": "\"\"\"Make a call to Facebook's REST server.\"\"\"\n# for Django templates, if this object is called without any arguments\n# return the object itself\n", "func_signal": "def __call__(self, method=None, args=None, secure=False, signed=False):\n", "code": "if method is None:\n    return self\n\n# __init__ hard-codes into en_US\nif args is not None and not args.has_key('locale'):\n    args['locale'] = self.locale\n\n# @author: houyr\n# fix for bug of UnicodeEncodeError\npost_data = self.unicode_urlencode(self._build_post_args(method, args, signed))\n\nif self.proxy:\n    proxy_handler = urllib2.ProxyHandler(self.proxy)\n    opener = urllib2.build_opener(proxy_handler)\n    if self.oauth2 or secure:\n        response = opener.open(self.facebook_secure_url, post_data).read()\n    else:\n        response = opener.open(self.facebook_url, post_data).read()\nelse:\n    if self.oauth2 or secure:\n        response = urlread(self.facebook_secure_url, post_data)\n    else:\n        response = urlread(self.facebook_url, post_data)\n\nreturn self._parse_response(response, method)", "path": "auth\\facebook.py", "repo_name": "rjernst/calvary-website", "stars": 10, "license": "None", "language": "python", "size": 2146}
{"docstring": "\"\"\"\n@author: houyr\nA unicode aware version of urllib.urlencode.\n\"\"\"\n", "func_signal": "def unicode_urlencode(self, params):\n", "code": "if isinstance(params, dict):\n    params = params.items()\nreturn urllib.urlencode([(k, isinstance(v, unicode) and v.encode('utf-8') or v)\n                  for k, v in params])", "path": "auth\\facebook.py", "repo_name": "rjernst/calvary-website", "stars": 10, "license": "None", "language": "python", "size": 2146}
{"docstring": "\"\"\"Checks if the given Facebook response is an error, and then raises the appropriate exception.\"\"\"\n", "func_signal": "def _check_error(self, response):\n", "code": "if type(response) is dict and response.has_key('error_code'):\n    raise FacebookError(response['error_code'], response['error_msg'], response['request_args'])", "path": "auth\\facebook.py", "repo_name": "rjernst/calvary-website", "stars": 10, "license": "None", "language": "python", "size": 2146}
{"docstring": "\"\"\"\nWe've called authorize, and received a code, now we need to convert\nthis to an access_token\n\n\"\"\"\n", "func_signal": "def oauth2_access_token(self, code, next, required_permissions=None):\n", "code": "args = {\n    'client_id': self.app_id,\n    'client_secret': self.secret_key,\n    'redirect_uri': next,\n    'code': code\n}\n\nif required_permissions:\n    args['scope'] = \",\".join(required_permissions)\n\n# TODO see if immediate param works as per OAuth 2.0 spec?\nurl = self.get_graph_url('oauth/access_token', **args)\n\nif self.proxy:\n    proxy_handler = urllib2.ProxyHandler(self.proxy)\n    opener = urllib2.build_opener(proxy_handler)\n    response = opener.open(url).read()\nelse:\n    response = urlread(url)\n\nresult = urlparse.parse_qs(response)\nself.oauth2_token = result['access_token'][0]\nself.oauth2_token_expires = time.time() + int(result['expires'][0])", "path": "auth\\facebook.py", "repo_name": "rjernst/calvary-website", "stars": 10, "license": "None", "language": "python", "size": 2146}
{"docstring": "\"\"\"Parses the response according to the given (optional) format, which should be either 'JSON' or 'XML'.\"\"\"\n", "func_signal": "def _parse_response(self, response, method, format=None):\n", "code": "if not format:\n    format = RESPONSE_FORMAT\n\nif format == 'JSON':\n    result = simplejson.loads(response)\n\n    self._check_error(result)\nelif format == 'XML':\n    dom = minidom.parseString(response)\n    result = self._parse_response_item(dom)\n    dom.unlink()\n\n    if 'error_response' in result:\n        self._check_error(result['error_response'])\n\n    result = result[method[9:].replace('.', '_') + '_response']\nelse:\n    raise RuntimeError('Invalid format specified.')\n\nreturn result", "path": "auth\\facebook.py", "repo_name": "rjernst/calvary-website", "stars": 10, "license": "None", "language": "python", "size": 2146}
{"docstring": "\"\"\"Adds 'session_key' and 'call_id' to args, which are used for API calls that need sessions.\"\"\"\n", "func_signal": "def _add_session_args(self, args=None):\n", "code": "if args is None:\n    args = {}\n\nif not self.session_key:\n    return args\n    #some calls don't need a session anymore. this might be better done in the markup\n    #raise RuntimeError('Session key not set. Make sure auth.getSession has been called.')\n\nif not self.oauth2 or not self.oauth2_token:\n    args['session_key'] = self.session_key\nargs['call_id'] = str(int(time.time() * 1000))\n\nreturn args", "path": "auth\\facebook.py", "repo_name": "rjernst/calvary-website", "stars": 10, "license": "None", "language": "python", "size": 2146}
{"docstring": "\"\"\"Parses an XML dictionary response node from Facebook.\"\"\"\n", "func_signal": "def _parse_response_dict(self, node):\n", "code": "result = {}\nfor item in filter(lambda x: x.nodeType == x.ELEMENT_NODE, node.childNodes):\n    result[item.nodeName] = self._parse_response_item(item)\nif node.nodeType == node.ELEMENT_NODE and node.hasAttributes():\n    if node.hasAttribute('id'):\n        result['id'] = node.getAttribute('id')\nreturn result", "path": "auth\\facebook.py", "repo_name": "rjernst/calvary-website", "stars": 10, "license": "None", "language": "python", "size": 2146}
{"docstring": "\"\"\"\nHash an email address in a format suitable for Facebook Connect.\n\n\"\"\"\n", "func_signal": "def hash_email(self, email):\n", "code": "email = email.lower().strip()\nreturn \"%s_%s\" % (\n    struct.unpack(\"I\", struct.pack(\"i\", binascii.crc32(email)))[0],\n    hashlib.md5(email).hexdigest(),\n)", "path": "auth\\facebook.py", "repo_name": "rjernst/calvary-website", "stars": 10, "license": "None", "language": "python", "size": 2146}
{"docstring": "\"\"\"Open a web browser telling the user to login to Facebook.\"\"\"\n", "func_signal": "def login(self, popup=False):\n", "code": "import webbrowser\nwebbrowser.open(self.get_login_url(popup=popup))", "path": "auth\\facebook.py", "repo_name": "rjernst/calvary-website", "stars": 10, "license": "None", "language": "python", "size": 2146}
{"docstring": "\"\"\"Open a web browser telling the user to grant an extended permission.\"\"\"\n", "func_signal": "def request_extended_permission(self, ext_perm, popup=False):\n", "code": "import webbrowser\nwebbrowser.open(self.get_ext_perm_url(ext_perm, popup=popup))", "path": "auth\\facebook.py", "repo_name": "rjernst/calvary-website", "stars": 10, "license": "None", "language": "python", "size": 2146}
{"docstring": "# for Django templates\n", "func_signal": "def __call__(self, method=None, args=None, add_session_args=True, signed=False):\n", "code": "if method is None:\n    return self\n\nif add_session_args:\n    self._client._add_session_args(args)\n\nreturn self._client('%s.%s' % (self._name, method), args, signed=signed)", "path": "auth\\facebook.py", "repo_name": "rjernst/calvary-website", "stars": 10, "license": "None", "language": "python", "size": 2146}
{"docstring": "\"\"\"Facebook API call. See http://developers.facebook.com/documentation.php?v=1.0&method=auth.getSession\"\"\"\n", "func_signal": "def getSession(self):\n", "code": "args = {}\ntry:\n    args['auth_token'] = self._client.auth_token\nexcept AttributeError:\n    raise RuntimeError('Client does not have auth_token set.')\ntry:\n    args['generate_session_secret'] = self._client.generate_session_secret\nexcept AttributeError:\n    pass\nresult = self._client('%s.getSession' % self._name, args)\nself._client.session_key = result['session_key']\nself._client.uid = result['uid']\nself._client.secret = result.get('secret')\nself._client.session_key_expires = result['expires']\nreturn result", "path": "auth\\facebook.py", "repo_name": "rjernst/calvary-website", "stars": 10, "license": "None", "language": "python", "size": 2146}
{"docstring": "\"\"\"Encodes a multipart/form-data message to upload an image.\"\"\"\n", "func_signal": "def __encode_multipart_formdata(self, fields, files):\n", "code": "boundary = '-------tHISiStheMulTIFoRMbOUNDaRY'\ncrlf = '\\r\\n'\nl = []\n\nfor (key, value) in fields:\n    l.append('--' + boundary)\n    l.append('Content-Disposition: form-data; name=\"%s\"' % str(key))\n    l.append('')\n    l.append(str(value))\nfor (filename, value) in files:\n    l.append('--' + boundary)\n    l.append('Content-Disposition: form-data; filename=\"%s\"' % (str(filename), ))\n    l.append('Content-Type: %s' % self.__get_content_type(filename))\n    l.append('')\n    l.append(value.getvalue())\nl.append('--' + boundary + '--')\nl.append('')\nbody = crlf.join(l)\ncontent_type = 'multipart/form-data; boundary=%s' % boundary\nreturn content_type, body", "path": "app\\utils\\facebook\\__init__.py", "repo_name": "stas/appengine-for-facebook", "stars": 8, "license": "None", "language": "python", "size": 485}
{"docstring": "\"\"\"Parses an XML dictionary response node from Facebook.\"\"\"\n", "func_signal": "def _parse_response_dict(self, node):\n", "code": "result = {}\nfor item in filter(lambda x: x.nodeType == x.ELEMENT_NODE, node.childNodes):\n    result[item.nodeName] = self._parse_response_item(item)\nif node.nodeType == node.ELEMENT_NODE and node.hasAttributes():\n    if node.hasAttribute('id'):\n        result['id'] = node.getAttribute('id')\nreturn result", "path": "app\\utils\\facebook\\__init__.py", "repo_name": "stas/appengine-for-facebook", "stars": 8, "license": "None", "language": "python", "size": 485}
{"docstring": "\"\"\"Parses the response according to the given (optional) format, which should be either 'JSON' or 'XML'.\"\"\"\n", "func_signal": "def _parse_response(self, response, method, format=None):\n", "code": "if not format:\n    format = RESPONSE_FORMAT\n\nif format == 'JSON':\n    result = simplejson.loads(response)\n\n    self._check_error(result)\nelif format == 'XML':\n    dom = minidom.parseString(response)\n    result = self._parse_response_item(dom)\n    dom.unlink()\n\n    if 'error_response' in result:\n        self._check_error(result['error_response'])\n\n    result = result[method[9:].replace('.', '_') + '_response']\nelse:\n    raise RuntimeError('Invalid format specified.')\n\nreturn result", "path": "app\\utils\\facebook\\__init__.py", "repo_name": "stas/appengine-for-facebook", "stars": 8, "license": "None", "language": "python", "size": 485}
{"docstring": "\"\"\"Checks if the given Facebook response is an error, and then raises the appropriate exception.\"\"\"\n", "func_signal": "def _check_error(self, response):\n", "code": "if type(response) is dict and response.has_key('error_code'):\n    raise FacebookError(response['error_code'], response['error_msg'], response['request_args'])", "path": "app\\utils\\facebook\\__init__.py", "repo_name": "stas/appengine-for-facebook", "stars": 8, "license": "None", "language": "python", "size": 485}
{"docstring": "\"\"\"Hashes arguments by joining key=value pairs, appending a secret, and then taking the MD5 hex digest.\"\"\"\n# @author: houyr\n# fix for UnicodeEncodeError\n", "func_signal": "def _hash_args(self, args, secret=None):\n", "code": "hasher = hashlib.md5(''.join(['%s=%s' % (isinstance(x, unicode) and x.encode(\"utf-8\") or x, isinstance(args[x], unicode) and args[x].encode(\"utf-8\") or args[x]) for x in sorted(args.keys())]))\nif secret:\n    hasher.update(secret)\nelif self.secret:\n    hasher.update(self.secret)\nelse:\n    hasher.update(self.secret_key)\nreturn hasher.hexdigest()", "path": "app\\utils\\facebook\\__init__.py", "repo_name": "stas/appengine-for-facebook", "stars": 8, "license": "None", "language": "python", "size": 485}
{"docstring": "\"\"\"Facebook API call. See http://developers.facebook.com/documentation.php?v=1.0&method=auth.createToken\"\"\"\n", "func_signal": "def createToken(self):\n", "code": "token = self._client('%s.createToken' % self._name)\nself._client.auth_token = token\nreturn token", "path": "app\\utils\\facebook\\__init__.py", "repo_name": "stas/appengine-for-facebook", "stars": 8, "license": "None", "language": "python", "size": 485}
{"docstring": "\"\"\"Facebook API call. See http://developers.facebook.com/documentation.php?v=1.0&method=friends.get\"\"\"\n", "func_signal": "def get(self, **kwargs):\n", "code": "if not kwargs.get('flid') and self._client._friends:\n    return self._client._friends\nreturn super(FriendsProxy, self).get(**kwargs)", "path": "app\\utils\\facebook\\__init__.py", "repo_name": "stas/appengine-for-facebook", "stars": 8, "license": "None", "language": "python", "size": 485}
{"docstring": "\"\"\"Adds 'session_key' and 'call_id' to args, which are used for API calls that need sessions.\"\"\"\n", "func_signal": "def _add_session_args(self, args=None):\n", "code": "if args is None:\n    args = {}\n\nif not self.session_key:\n    return args\n    #some calls don't need a session anymore. this might be better done in the markup\n    #raise RuntimeError('Session key not set. Make sure auth.getSession has been called.')\n\nargs['session_key'] = self.session_key\nargs['call_id'] = str(int(time.time() * 1000))\n\nreturn args", "path": "app\\utils\\facebook\\__init__.py", "repo_name": "stas/appengine-for-facebook", "stars": 8, "license": "None", "language": "python", "size": 485}
{"docstring": "\"\"\"\nReturns the URL that the user should be redirected to in order to login.\n\nnext -- the URL that Facebook should redirect to after login\n\n\"\"\"\n", "func_signal": "def get_login_url(self, next=None, popup=False, canvas=True):\n", "code": "args = {'api_key': self.api_key, 'v': '1.0'}\n\nif next is not None:\n    args['next'] = next\n\nif canvas is True:\n    args['canvas'] = 1\n\nif popup is True:\n    args['popup'] = 1\n\nif self.auth_token is not None:\n    args['auth_token'] = self.auth_token\n\nreturn self.get_url('login', **args)", "path": "app\\utils\\facebook\\__init__.py", "repo_name": "stas/appengine-for-facebook", "stars": 8, "license": "None", "language": "python", "size": 485}
{"docstring": "\"\"\"Adds to args parameters that are necessary for every call to the API.\"\"\"\n", "func_signal": "def _build_post_args(self, method, args=None):\n", "code": "if args is None:\n    args = {}\n\nfor arg in args.items():\n    if type(arg[1]) == list:\n        args[arg[0]] = ','.join(str(a) for a in arg[1])\n    elif type(arg[1]) == unicode:\n        args[arg[0]] = arg[1].encode(\"UTF-8\")\n    elif type(arg[1]) == bool:\n        args[arg[0]] = str(arg[1]).lower()\n\nargs['method'] = method\nargs['api_key'] = self.api_key\nargs['v'] = '1.0'\nargs['format'] = RESPONSE_FORMAT\nargs['sig'] = self._hash_args(args)\n\nreturn args", "path": "app\\utils\\facebook\\__init__.py", "repo_name": "stas/appengine-for-facebook", "stars": 8, "license": "None", "language": "python", "size": 485}
{"docstring": "\"\"\"\nHash an email address in a format suitable for Facebook Connect.\n\n\"\"\"\n", "func_signal": "def hash_email(self, email):\n", "code": "email = email.lower().strip()\nreturn \"%s_%s\" % (\n    struct.unpack(\"I\", struct.pack(\"i\", binascii.crc32(email)))[0],\n    hashlib.md5(email).hexdigest(),\n)", "path": "app\\utils\\facebook\\__init__.py", "repo_name": "stas/appengine-for-facebook", "stars": 8, "license": "None", "language": "python", "size": 485}
{"docstring": "\"\"\"Facebook API call. See http://developers.facebook.com/documentation.php?v=1.0&method=photos.upload\n\nsize -- an optional size (width, height) to resize the image to before uploading. Resizes by default\n        to Facebook's maximum display width of 604.\n\"\"\"\n", "func_signal": "def upload(self, image, aid=None, caption=None, size=(604, 1024), filename=None, callback=None):\n", "code": "args = {}\n\nif aid is not None:\n    args['aid'] = aid\n\nif caption is not None:\n    args['caption'] = caption\n\nargs = self._client._build_post_args('facebook.photos.upload', self._client._add_session_args(args))\n\ntry:\n    import cStringIO as StringIO\nexcept ImportError:\n    import StringIO\n\n# check for a filename specified...if the user is passing binary data in\n# image then a filename will be specified\nif filename is None:\n    try:\n        import Image\n    except ImportError:\n        data = StringIO.StringIO(open(image, 'rb').read())\n    else:\n        img = Image.open(image)\n        if size:\n            img.thumbnail(size, Image.ANTIALIAS)\n        data = StringIO.StringIO()\n        img.save(data, img.format)\nelse:\n    # there was a filename specified, which indicates that image was not\n    # the path to an image file but rather the binary data of a file\n    data = StringIO.StringIO(image)\n    image = filename\n\ncontent_type, body = self.__encode_multipart_formdata(list(args.iteritems()), [(image, data)])\nurlinfo = urlparse.urlsplit(self._client.facebook_url)\ntry:\n    content_length = len(body)\n    chunk_size = 4096\n\n    h = httplib.HTTPConnection(urlinfo[1])\n    h.putrequest('POST', urlinfo[2])\n    h.putheader('Content-Type', content_type)\n    h.putheader('Content-Length', str(content_length))\n    h.putheader('MIME-Version', '1.0')\n    h.putheader('User-Agent', 'PyFacebook Client Library')\n    h.endheaders()\n\n    if callback:\n        count = 0\n        while len(body) > 0:\n            if len(body) < chunk_size:\n                data = body\n                body = ''\n            else:\n                data = body[0:chunk_size]\n                body = body[chunk_size:]\n\n            h.send(data)\n            count += 1\n            callback(count, chunk_size, content_length)\n    else:\n        h.send(body)\n\n    response = h.getresponse()\n\n    if response.status != 200:\n        raise Exception('Error uploading photo: Facebook returned HTTP %s (%s)' % (response.status, response.reason))\n    response = response.read()\nexcept:\n    # sending the photo failed, perhaps we are using GAE\n    try:\n        from google.appengine.api import urlfetch\n\n        try:\n            response = urlread(url=self._client.facebook_url,data=body,headers={'POST':urlinfo[2],'Content-Type':content_type,'MIME-Version':'1.0'})\n        except urllib2.URLError:\n            raise Exception('Error uploading photo: Facebook returned %s' % (response))\n    except ImportError:\n        # could not import from google.appengine.api, so we are not running in GAE\n        raise Exception('Error uploading photo.')\n\nreturn self._client._parse_response(response, 'facebook.photos.upload')", "path": "app\\utils\\facebook\\__init__.py", "repo_name": "stas/appengine-for-facebook", "stars": 8, "license": "None", "language": "python", "size": 485}
{"docstring": "\"\"\"\nValidate parameters passed to an internal Facebook app from Facebook.\n\n\"\"\"\n", "func_signal": "def validate_signature(self, post, prefix='fb_sig', timeout=None):\n", "code": "args = post.copy()\n\nif prefix not in args:\n    return None\n\ndel args[prefix]\n\nif timeout and '%s_time' % prefix in post and time.time() - float(post['%s_time' % prefix]) > timeout:\n    return None\n\nargs = dict([(key[len(prefix + '_'):], value) for key, value in args.items() if key.startswith(prefix)])\n\nhash = self._hash_args(args)\n\nif hash == post[prefix]:\n    return args\nelse:\n    return None", "path": "app\\utils\\facebook\\__init__.py", "repo_name": "stas/appengine-for-facebook", "stars": 8, "license": "None", "language": "python", "size": 485}
{"docstring": "\"\"\"\nInitializes a new Facebook object which provides wrappers for the Facebook API.\n\nIf this is a desktop application, the next couple of steps you might want to take are:\n\nfacebook.auth.createToken() # create an auth token\nfacebook.login()            # show a browser window\nwait_login()                # somehow wait for the user to log in\nfacebook.auth.getSession()  # get a session key\n\nFor web apps, if you are passed an auth_token from Facebook, pass that in as a named parameter.\nThen call:\n\nfacebook.auth.getSession()\n\n\"\"\"\n", "func_signal": "def __init__(self, api_key, secret_key, auth_token=None, app_name=None, callback_path=None, internal=None, proxy=None, facebook_url=None, facebook_secure_url=None):\n", "code": "self.api_key = api_key\nself.secret_key = secret_key\nself.session_key = None\nself.session_key_expires = None\nself.auth_token = auth_token\nself.secret = None\nself.uid = None\nself.page_id = None\nself.in_canvas = False\nself.in_iframe = False\nself.is_session_from_cookie = False\nself.in_profile_tab = False\nself.added = False\nself.app_name = app_name\nself.callback_path = callback_path\nself.internal = internal\nself._friends = None\nself.locale = 'en_US'\nself.profile_update_time = None\nself.ext_perms = None\nself.proxy = proxy\nif facebook_url is None:\n    self.facebook_url = FACEBOOK_URL\nelse:\n    self.facebook_url = facebook_url\nif facebook_secure_url is None:\n    self.facebook_secure_url = FACEBOOK_SECURE_URL\nelse:\n    self.facebook_secure_url = facebook_secure_url\n\nfor namespace in METHODS:\n    self.__dict__[namespace] = eval('%sProxy(self, \\'%s\\')' % (namespace.title(), 'facebook.%s' % namespace))", "path": "app\\utils\\facebook\\__init__.py", "repo_name": "stas/appengine-for-facebook", "stars": 8, "license": "None", "language": "python", "size": 485}
{"docstring": "\"\"\"\nValidate parameters passed by cookies, namely facebookconnect or js api.\n\"\"\"\n\n", "func_signal": "def validate_cookie_signature(self, cookies):\n", "code": "api_key = self.api_key\nif api_key not in cookies:\n    return None\n\nprefix = api_key + \"_\"\n       \nparams = {} \nvals = ''\nfor k in sorted(cookies):\n    if k.startswith(prefix):\n        key = k.replace(prefix,\"\")\n        value = cookies[k]\n        params[key] = value\n        vals += '%s=%s' % (key, value)\n        \nhasher = hashlib.md5(vals)\n\nhasher.update(self.secret_key)\ndigest = hasher.hexdigest()\nif digest == cookies[api_key]:\n    params['is_session_from_cookie'] = True\n    return params\nelse:\n    return False", "path": "app\\utils\\facebook\\__init__.py", "repo_name": "stas/appengine-for-facebook", "stars": 8, "license": "None", "language": "python", "size": 485}
{"docstring": "\"\"\"Open a web browser telling the user to grant an extended permission.\"\"\"\n", "func_signal": "def request_extended_permission(self, ext_perm, popup=False):\n", "code": "import webbrowser\nwebbrowser.open(self.get_ext_perm_url(ext_perm, popup=popup))", "path": "app\\utils\\facebook\\__init__.py", "repo_name": "stas/appengine-for-facebook", "stars": 8, "license": "None", "language": "python", "size": 485}
{"docstring": "\"\"\"Parses an XML response node from Facebook.\"\"\"\n", "func_signal": "def _parse_response_item(self, node):\n", "code": "if node.nodeType == node.DOCUMENT_NODE and \\\n    node.childNodes[0].hasAttributes() and \\\n    node.childNodes[0].hasAttribute('list') and \\\n    node.childNodes[0].getAttribute('list') == \"true\":\n    return {node.childNodes[0].nodeName: self._parse_response_list(node.childNodes[0])}\nelif node.nodeType == node.ELEMENT_NODE and \\\n    node.hasAttributes() and \\\n    node.hasAttribute('list') and \\\n    node.getAttribute('list')==\"true\":\n    return self._parse_response_list(node)\nelif len(filter(lambda x: x.nodeType == x.ELEMENT_NODE, node.childNodes)) > 0:\n    return self._parse_response_dict(node)\nelse:\n    return ''.join(node.data for node in node.childNodes if node.nodeType == node.TEXT_NODE)", "path": "app\\utils\\facebook\\__init__.py", "repo_name": "stas/appengine-for-facebook", "stars": 8, "license": "None", "language": "python", "size": 485}
{"docstring": "\"\"\"Make a call to Facebook's REST server.\"\"\"\n# for Django templates, if this object is called without any arguments\n# return the object itself\n", "func_signal": "def __call__(self, method=None, args=None, secure=False):\n", "code": "if method is None:\n    return self\n\n# __init__ hard-codes into en_US\nif args is not None and not args.has_key('locale'):\n    args['locale'] = self.locale\n\n# @author: houyr\n# fix for bug of UnicodeEncodeError\npost_data = self.unicode_urlencode(self._build_post_args(method, args))\n\nif self.proxy:\n    proxy_handler = urllib2.ProxyHandler(self.proxy)\n    opener = urllib2.build_opener(proxy_handler)\n    if secure:\n        response = opener.open(self.facebook_secure_url, post_data).read()\n    else:\n        response = opener.open(self.facebook_url, post_data).read()\nelse:\n    if secure:\n        response = urlread(self.facebook_secure_url, post_data)\n    else:\n        response = urlread(self.facebook_url, post_data)\n\nreturn self._parse_response(response, method)", "path": "app\\utils\\facebook\\__init__.py", "repo_name": "stas/appengine-for-facebook", "stars": 8, "license": "None", "language": "python", "size": 485}
{"docstring": "\"\"\"Parses an XML list response node from Facebook.\"\"\"\n", "func_signal": "def _parse_response_list(self, node):\n", "code": "result = []\nfor item in filter(lambda x: x.nodeType == x.ELEMENT_NODE, node.childNodes):\n    result.append(self._parse_response_item(item))\nreturn result", "path": "app\\utils\\facebook\\__init__.py", "repo_name": "stas/appengine-for-facebook", "stars": 8, "license": "None", "language": "python", "size": 485}
{"docstring": "\"\"\"Facebook API call. See http://developers.facebook.com/documentation.php?v=1.0&method=auth.getSession\"\"\"\n", "func_signal": "def getSession(self):\n", "code": "args = {}\ntry:\n    args['auth_token'] = self._client.auth_token\nexcept AttributeError:\n    raise RuntimeError('Client does not have auth_token set.')\nresult = self._client('%s.getSession' % self._name, args)\nself._client.session_key = result['session_key']\nself._client.uid = result['uid']\nself._client.secret = result.get('secret')\nself._client.session_key_expires = result['expires']\nreturn result", "path": "app\\utils\\facebook\\__init__.py", "repo_name": "stas/appengine-for-facebook", "stars": 8, "license": "None", "language": "python", "size": 485}
{"docstring": "\"\"\"Write the given data to this stream.\n\nIf callback is given, we call it when all of the buffered write\ndata has been successfully written to the stream. If there was\npreviously buffered write data and an old write callback, that\ncallback is simply overwritten with this new callback.\n\"\"\"\n", "func_signal": "def write(self, data, callback=None):\n", "code": "self._check_closed()\nself._write_buffer += data\nself._add_io_state(self.io_loop.WRITE)\nself._write_callback = callback", "path": "tornado\\iostream.py", "repo_name": "grippy/Twitfaced", "stars": 9, "license": "None", "language": "python", "size": 224}
{"docstring": "\"\"\"Sets the default locale, used in get_closest_locale().\n\nThe default locale is assumed to be the language used for all strings\nin the system. The translations loaded from disk are mappings from\nthe default locale to the destination locale. Consequently, you don't\nneed to create a translation file for the default locale.\n\"\"\"\n", "func_signal": "def set_default_locale(code):\n", "code": "global _default_locale\nglobal _supported_locales\n_default_locale = code\n_supported_locales = frozenset(_translations.keys() + [_default_locale])", "path": "tornado\\locale.py", "repo_name": "grippy/Twitfaced", "stars": 9, "license": "None", "language": "python", "size": 224}
{"docstring": "\"\"\"Restarts the process automatically when a module is modified.\n\nWe run on the I/O loop, and restarting is a destructive operation,\nso will terminate any pending requests.\n\"\"\"\n", "func_signal": "def start(io_loop=None, check_time=500):\n", "code": "io_loop = io_loop or ioloop.IOLoop.instance()\nmodify_times = {}\ncallback = functools.partial(_reload_on_update, io_loop, modify_times)\nscheduler = ioloop.PeriodicCallback(callback, check_time, io_loop=io_loop)\nscheduler.start()", "path": "tornado\\autoreload.py", "repo_name": "grippy/Twitfaced", "stars": 9, "license": "None", "language": "python", "size": 224}
{"docstring": "\"\"\"Returns a global IOLoop instance.\n\nMost single-threaded applications have a single, global IOLoop.\nUse this method instead of passing around IOLoop instances \nthroughout your code.\n\nA common pattern for classes that depend on IOLoops is to use\na default argument to enable programs with multiple IOLoops\nbut not require the argument for simpler applications:\n\n    class MyClass(object):\n        def __init__(self, io_loop=None):\n            self.io_loop = io_loop or IOLoop.instance()\n\"\"\"\n", "func_signal": "def instance(cls):\n", "code": "if not hasattr(cls, \"_instance\"):\n    cls._instance = cls()\nreturn cls._instance", "path": "tornado\\ioloop.py", "repo_name": "grippy/Twitfaced", "stars": 9, "license": "None", "language": "python", "size": 224}
{"docstring": "\"\"\"Call callback when we read the given delimiter.\"\"\"\n", "func_signal": "def read_until(self, delimiter, callback):\n", "code": "assert not self._read_callback, \"Already reading\"\nloc = self._read_buffer.find(delimiter)\nif loc != -1:\n    callback(self._consume(loc + len(delimiter)))\n    return\nself._check_closed()\nself._read_delimiter = delimiter\nself._read_callback = callback\nself._add_io_state(self.io_loop.READ)", "path": "tornado\\iostream.py", "repo_name": "grippy/Twitfaced", "stars": 9, "license": "None", "language": "python", "size": 224}
{"docstring": "\"\"\"Formats the given date as a day of week.\n\nExample: \"Monday, January 22\". You can remove the day of week with\ndow=False.\n\"\"\"\n", "func_signal": "def format_day(self, date, gmt_offset=0, dow=True):\n", "code": "local_date = date - datetime.timedelta(minutes=gmt_offset)\n_ = self.translate\nif dow:\n    return _(\"%(weekday)s, %(month_name)s %(day)s\") % {\n        \"month_name\": self._months[local_date.month - 1],\n        \"weekday\": self._weekdays[local_date.weekday()],\n        \"day\": str(local_date.day),\n    }\nelse:\n    return _(\"%(month_name)s %(day)s\") % {\n        \"month_name\": self._months[local_date.month - 1],\n        \"day\": str(local_date.day),\n    }", "path": "tornado\\locale.py", "repo_name": "grippy/Twitfaced", "stars": 9, "license": "None", "language": "python", "size": 224}
{"docstring": "\"\"\"Returns a comma-separated list for the given list of parts.\n\nThe format is, e.g., \"A, B and C\", \"A and B\" or just \"A\" for lists\nof size 1.\n\"\"\"\n", "func_signal": "def list(self, parts):\n", "code": "_ = self.translate\nif len(parts) == 0: return \"\"\nif len(parts) == 1: return parts[0]\ncomma = u' \\u0648 ' if self.code.startswith(\"fa\") else u\", \"\nreturn _(\"%(commas)s and %(last)s\") % {\n    \"commas\": comma.join(parts[:-1]),\n    \"last\": parts[len(parts) - 1],\n}", "path": "tornado\\locale.py", "repo_name": "grippy/Twitfaced", "stars": 9, "license": "None", "language": "python", "size": 224}
{"docstring": "\"\"\"Calls the given callback at the time deadline from the I/O loop.\"\"\"\n", "func_signal": "def add_timeout(self, deadline, callback):\n", "code": "timeout = _Timeout(deadline, callback)\nbisect.insort(self._timeouts, timeout)\nreturn timeout", "path": "tornado\\ioloop.py", "repo_name": "grippy/Twitfaced", "stars": 9, "license": "None", "language": "python", "size": 224}
{"docstring": "\"\"\"Closes this database connection.\"\"\"\n", "func_signal": "def close(self):\n", "code": "if self._db is not None:\n    self._db.close()\n    self._db = None", "path": "tornado\\database.py", "repo_name": "grippy/Twitfaced", "stars": 9, "license": "None", "language": "python", "size": 224}
{"docstring": "\"\"\"Returns a row list for the given query and parameters.\"\"\"\n", "func_signal": "def query(self, query, *parameters):\n", "code": "cursor = self._cursor()\ntry:\n    self._execute(cursor, query, parameters)\n    column_names = [d[0] for d in cursor.description]\n    return [Row(itertools.izip(column_names, row)) for row in cursor]\nfinally:\n    cursor.close()", "path": "tornado\\database.py", "repo_name": "grippy/Twitfaced", "stars": 9, "license": "None", "language": "python", "size": 224}
{"docstring": "\"\"\"Returns a comma-separated number for the given integer.\"\"\"\n", "func_signal": "def friendly_number(self, value):\n", "code": "if self.code not in (\"en\", \"en_US\"):\n    return str(value)\nvalue = str(value)\nparts = []\nwhile value:\n    parts.append(value[-3:])\n    value = value[:-3]\nreturn \",\".join(reversed(parts))", "path": "tornado\\locale.py", "repo_name": "grippy/Twitfaced", "stars": 9, "license": "None", "language": "python", "size": 224}
{"docstring": "\"\"\"Closes the existing database connection and re-opens it.\"\"\"\n", "func_signal": "def reconnect(self):\n", "code": "self.close()\nself._db = MySQLdb.connect(**self._db_args)\nself._db.autocommit(True)", "path": "tornado\\database.py", "repo_name": "grippy/Twitfaced", "stars": 9, "license": "None", "language": "python", "size": 224}
{"docstring": "\"\"\"Stop the loop after the current event loop iteration is complete.\"\"\"\n", "func_signal": "def stop(self):\n", "code": "self._running = False\nself._wake()", "path": "tornado\\ioloop.py", "repo_name": "grippy/Twitfaced", "stars": 9, "license": "None", "language": "python", "size": 224}
{"docstring": "\"\"\"Returns an iterator for the given query and parameters.\"\"\"\n", "func_signal": "def iter(self, query, *parameters):\n", "code": "if self._db is None: self.reconnect()\ncursor = MySQLdb.cursors.SSCursor(self._db)\ntry:\n    self._execute(cursor, query, parameters)\n    column_names = [d[0] for d in cursor.description]\n    for row in cursor:\n        yield Row(zip(column_names, row))\nfinally:\n    cursor.close()", "path": "tornado\\database.py", "repo_name": "grippy/Twitfaced", "stars": 9, "license": "None", "language": "python", "size": 224}
{"docstring": "\"\"\"Loads translations from CSV files in a directory.\n\nTranslations are strings with optional Python-style named placeholders\n(e.g., \"My name is %(name)s\") and their associated translations.\n\nThe directory should have translation files of the form LOCALE.csv,\ne.g. es_GT.csv. The CSV files should have two or three columns: string,\ntranslation, and an optional plural indicator. Plural indicators should\nbe one of \"plural\" or \"singular\". A given string can have both singular\nand plural forms. For example \"%(name)s liked this\" may have a\ndifferent verb conjugation depending on whether %(name)s is one\nname or a list of names. There should be two rows in the CSV file for\nthat string, one with plural indicator \"singular\", and one \"plural\".\nFor strings with no verbs that would change on translation, simply\nuse \"unknown\" or the empty string (or don't include the column at all).\n\nExample translation es_LA.csv:\n\n    \"I love you\",\"Te amo\"\n    \"%(name)s liked this\",\"A %(name)s les gust\\xf3 esto\",\"plural\"\n    \"%(name)s liked this\",\"A %(name)s le gust\\xf3 esto\",\"singular\"\n\n\"\"\"\n", "func_signal": "def load_translations(directory):\n", "code": "global _translations\nglobal _supported_locales\n_translations = {}\nfor path in os.listdir(directory):\n    if not path.endswith(\".csv\"): continue\n    locale, extension = path.split(\".\")\n    if locale not in LOCALE_NAMES:\n        logging.error(\"Unrecognized locale %r (path: %s)\", locale,\n                      os.path.join(directory, path))\n        continue\n    f = open(os.path.join(directory, path), \"r\")\n    _translations[locale] = {}\n    for i, row in enumerate(csv.reader(f)):\n        if not row or len(row) < 2: continue\n        row = [c.decode(\"utf-8\").strip() for c in row]\n        english, translation = row[:2]\n        if len(row) > 2:\n            plural = row[2] or \"unknown\"\n        else:\n            plural = \"unknown\"\n        if plural not in (\"plural\", \"singular\", \"unknown\"):\n            logging.error(\"Unrecognized plural indicator %r in %s line %d\",\n                          plural, path, i + 1)\n            continue\n        _translations[locale].setdefault(plural, {})[english] = translation\n    f.close()\n_supported_locales = frozenset(_translations.keys() + [_default_locale])\nlogging.info(\"Supported locales: %s\", sorted(_supported_locales))", "path": "tornado\\locale.py", "repo_name": "grippy/Twitfaced", "stars": 9, "license": "None", "language": "python", "size": 224}
{"docstring": "\"\"\"Returns the first row returned for the given query.\"\"\"\n", "func_signal": "def get(self, query, *parameters):\n", "code": "rows = self.query(query, *parameters)\nif not rows:\n    return None\nelif len(rows) > 1:\n    raise Exception(\"Multiple rows returned for Database.get() query\")\nelse:\n    return rows[0]", "path": "tornado\\database.py", "repo_name": "grippy/Twitfaced", "stars": 9, "license": "None", "language": "python", "size": 224}
{"docstring": "\"\"\"Returns the Locale for the given locale code.\n\nIf it is not supported, we raise an exception.\n\"\"\"\n", "func_signal": "def get(cls, code):\n", "code": "if not hasattr(cls, \"_cache\"):\n    cls._cache = {}\nif code not in cls._cache:\n    assert code in _supported_locales\n    translations = _translations.get(code, {})\n    cls._cache[code] = Locale(code, translations)\nreturn cls._cache[code]", "path": "tornado\\locale.py", "repo_name": "grippy/Twitfaced", "stars": 9, "license": "None", "language": "python", "size": 224}
{"docstring": "\"\"\"Executes an HTTPRequest, calling callback with an HTTPResponse.\n\nIf an error occurs during the fetch, the HTTPResponse given to the\ncallback has a non-None error attribute that contains the exception\nencountered during the request. You can call response.reraise() to\nthrow the exception (if any) in the callback.\n\"\"\"\n", "func_signal": "def fetch(self, request, callback, **kwargs):\n", "code": "if not isinstance(request, HTTPRequest):\n   request = HTTPRequest(url=request, **kwargs)\nself._requests.append((request, callback))\nself._add_perform_callback()", "path": "tornado\\httpclient.py", "repo_name": "grippy/Twitfaced", "stars": 9, "license": "None", "language": "python", "size": 224}
{"docstring": "\"\"\"Executes the given query against all the given param sequences.\n\nWe return the lastrowid from the query.\n\"\"\"\n", "func_signal": "def executemany(self, query, parameters):\n", "code": "cursor = self._cursor()\ntry:\n    cursor.executemany(query, parameters)\n    return cursor.lastrowid\nfinally:\n    cursor.close()", "path": "tornado\\database.py", "repo_name": "grippy/Twitfaced", "stars": 9, "license": "None", "language": "python", "size": 224}
{"docstring": "\"\"\"Calls the given callback on the next I/O loop iteration.\"\"\"\n", "func_signal": "def add_callback(self, callback):\n", "code": "self._callbacks.add(callback)\nself._wake()", "path": "tornado\\ioloop.py", "repo_name": "grippy/Twitfaced", "stars": 9, "license": "None", "language": "python", "size": 224}
{"docstring": "\"\"\"\nReturns the scanner's current row value and advances to the next\nrow in the table.  When there are no more rows in the table, or a key\ngreater-than-or-equal-to the scanner's specified stopRow is reached,\nan empty list is returned.\n\n@param id id of a scanner returned by scannerOpen\n@return a TRowResult containing the current row and a map of the columns to TCells.\n@throws IllegalArgument if ScannerID is invalid\n@throws NotFound when the scanner reaches the end\n\nParameters:\n - id\n\"\"\"\n", "func_signal": "def scannerGet(self, id):\n", "code": "self.send_scannerGet(id)\nreturn self.recv_scannerGet()", "path": "aggregator\\db\\hbase\\Hbase.py", "repo_name": "andreisavu/feedaggregator", "stars": 9, "license": "bsd-2-clause", "language": "python", "size": 297}
{"docstring": "\"\"\"\nDelete all cells that match the passed row and column and whose\ntimestamp is equal-to or older than the passed timestamp.\n\n@param tableName name of table\n@param row Row to update\n@param column name of column whose value is to be deleted\n@param timestamp timestamp\n\nParameters:\n - tableName\n - row\n - column\n - timestamp\n\"\"\"\n", "func_signal": "def deleteAllTs(self, tableName, row, column, timestamp):\n", "code": "self.send_deleteAllTs(tableName, row, column, timestamp)\nself.recv_deleteAllTs()", "path": "aggregator\\db\\hbase\\Hbase.py", "repo_name": "andreisavu/feedaggregator", "stars": 9, "license": "bsd-2-clause", "language": "python", "size": 297}
{"docstring": "\"\"\"\nCloses the server-state associated with an open scanner.\n\n@param id id of a scanner returned by scannerOpen\n@throws IllegalArgument if ScannerID is invalid\n\nParameters:\n - id\n\"\"\"\n", "func_signal": "def scannerClose(self, id):\n", "code": "self.send_scannerClose(id)\nself.recv_scannerClose()", "path": "aggregator\\db\\hbase\\Hbase.py", "repo_name": "andreisavu/feedaggregator", "stars": 9, "license": "bsd-2-clause", "language": "python", "size": 297}
{"docstring": "\"\"\"\nApply a series of batches (each a series of mutations on a single row)\nin a single transaction.  If an exception is thrown, then the\ntransaction is aborted.  Default current timestamp is used, and\nall entries will have an identical timestamp.\n\n@param tableName name of table\n@param rowBatches list of row batches\n\nParameters:\n - tableName\n - rowBatches\n\"\"\"\n", "func_signal": "def mutateRows(self, tableName, rowBatches):\n", "code": "self.send_mutateRows(tableName, rowBatches)\nself.recv_mutateRows()", "path": "aggregator\\db\\hbase\\Hbase.py", "repo_name": "andreisavu/feedaggregator", "stars": 9, "license": "bsd-2-clause", "language": "python", "size": 297}
{"docstring": "\"\"\"\nDisables a table (takes it off-line) If it is being served, the master\nwill tell the servers to stop serving it.\n@param tableName name of the table\n\nParameters:\n - tableName\n\"\"\"\n", "func_signal": "def disableTable(self, tableName):\n", "code": "self.send_disableTable(tableName)\nself.recv_disableTable()", "path": "aggregator\\db\\hbase\\Hbase.py", "repo_name": "andreisavu/feedaggregator", "stars": 9, "license": "bsd-2-clause", "language": "python", "size": 297}
{"docstring": "\"\"\"\nApply a series of mutations (updates/deletes) to a row in a\nsingle transaction.  If an exception is thrown, then the\ntransaction is aborted.  Default current timestamp is used, and\nall entries will have an identical timestamp.\n\n@param tableName name of table\n@param row row key\n@param mutations list of mutation commands\n\nParameters:\n - tableName\n - row\n - mutations\n\"\"\"\n", "func_signal": "def mutateRow(self, tableName, row, mutations):\n", "code": "self.send_mutateRow(tableName, row, mutations)\nself.recv_mutateRow()", "path": "aggregator\\db\\hbase\\Hbase.py", "repo_name": "andreisavu/feedaggregator", "stars": 9, "license": "bsd-2-clause", "language": "python", "size": 297}
{"docstring": "\"\"\"\nReturns a bytestring version of 's', encoded as specified in 'encoding'.\n\"\"\"\n", "func_signal": "def smart_str(s, encoding='utf-8', errors='replace'):\n", "code": "if not isinstance(s, basestring):\n    try:\n        return str(s)\n    except UnicodeEncodeError:\n        return unicode(s).encode(encoding, errors)\nelif isinstance(s, unicode):\n    return s.encode(encoding, errors)\nelif s and encoding != 'utf-8':\n    return s.decode('utf-8', errors).encode(encoding, errors)\nelse:\n    return s", "path": "aggregator\\util.py", "repo_name": "andreisavu/feedaggregator", "stars": 9, "license": "bsd-2-clause", "language": "python", "size": 297}
{"docstring": "\"\"\"\nGet a scanner on the current table starting at the specified row and\nending at the last row in the table.  Return the specified columns.\n\n@param columns columns to scan. If column name is a column family, all\ncolumns of the specified column family are returned.  Its also possible\nto pass a regex in the column qualifier.\n@param tableName name of table\n@param startRow starting row in table to scan.  send \"\" (empty string) to\n                start at the first row.\n\n@return scanner id to be used with other scanner procedures\n\nParameters:\n - tableName\n - startRow\n - columns\n\"\"\"\n", "func_signal": "def scannerOpen(self, tableName, startRow, columns):\n", "code": "self.send_scannerOpen(tableName, startRow, columns)\nreturn self.recv_scannerOpen()", "path": "aggregator\\db\\hbase\\Hbase.py", "repo_name": "andreisavu/feedaggregator", "stars": 9, "license": "bsd-2-clause", "language": "python", "size": 297}
{"docstring": "\"\"\"\nGet the specified number of versions for the specified table,\nrow, and column.  Only versions less than or equal to the specified\ntimestamp will be returned.\n\n@param tableName name of table\n@param row row key\n@param column column name\n@param timestamp timestamp\n@param numVersions number of versions to retrieve\n@return list of cells for specified row/column\n\nParameters:\n - tableName\n - row\n - column\n - timestamp\n - numVersions\n\"\"\"\n", "func_signal": "def getVerTs(self, tableName, row, column, timestamp, numVersions):\n", "code": "self.send_getVerTs(tableName, row, column, timestamp, numVersions)\nreturn self.recv_getVerTs()", "path": "aggregator\\db\\hbase\\Hbase.py", "repo_name": "andreisavu/feedaggregator", "stars": 9, "license": "bsd-2-clause", "language": "python", "size": 297}
{"docstring": "\"\"\"\nGet the specified number of versions for the specified table,\nrow, and column.\n\n@param tableName name of table\n@param row row key\n@param column column name\n@param numVersions number of versions to retrieve\n@return list of cells for specified row/column\n\nParameters:\n - tableName\n - row\n - column\n - numVersions\n\"\"\"\n", "func_signal": "def getVer(self, tableName, row, column, numVersions):\n", "code": "self.send_getVer(tableName, row, column, numVersions)\nreturn self.recv_getVer()", "path": "aggregator\\db\\hbase\\Hbase.py", "repo_name": "andreisavu/feedaggregator", "stars": 9, "license": "bsd-2-clause", "language": "python", "size": 297}
{"docstring": "\"\"\"\nDeletes a table\n@param tableName name of table to delete\n@throws IOError if table doesn't exist on server or there was some other\nproblem\n\nParameters:\n - tableName\n\"\"\"\n", "func_signal": "def deleteTable(self, tableName):\n", "code": "self.send_deleteTable(tableName)\nself.recv_deleteTable()", "path": "aggregator\\db\\hbase\\Hbase.py", "repo_name": "andreisavu/feedaggregator", "stars": 9, "license": "bsd-2-clause", "language": "python", "size": 297}
{"docstring": "\"\"\"\nList all the userspace tables.\n@return - returns a list of names\n\"\"\"\n", "func_signal": "def getTableNames(self, ):\n", "code": "self.send_getTableNames()\nreturn self.recv_getTableNames()", "path": "aggregator\\db\\hbase\\Hbase.py", "repo_name": "andreisavu/feedaggregator", "stars": 9, "license": "bsd-2-clause", "language": "python", "size": 297}
{"docstring": "\"\"\"\nGet all the data for the specified table and row at the specified\ntimestamp. Returns an empty list if the row does not exist.\n\n@param tableName of table\n@param row row key\n@param timestamp timestamp\n@return TRowResult containing the row and map of columns to TCells\n\nParameters:\n - tableName\n - row\n - timestamp\n\"\"\"\n", "func_signal": "def getRowTs(self, tableName, row, timestamp):\n", "code": "self.send_getRowTs(tableName, row, timestamp)\nreturn self.recv_getRowTs()", "path": "aggregator\\db\\hbase\\Hbase.py", "repo_name": "andreisavu/feedaggregator", "stars": 9, "license": "bsd-2-clause", "language": "python", "size": 297}
{"docstring": "\"\"\"\nParameters:\n - tableNameOrRegionName\n\"\"\"\n", "func_signal": "def compact(self, tableNameOrRegionName):\n", "code": "self.send_compact(tableNameOrRegionName)\nself.recv_compact()", "path": "aggregator\\db\\hbase\\Hbase.py", "repo_name": "andreisavu/feedaggregator", "stars": 9, "license": "bsd-2-clause", "language": "python", "size": 297}
{"docstring": "\"\"\"\nApply a series of mutations (updates/deletes) to a row in a\nsingle transaction.  If an exception is thrown, then the\ntransaction is aborted.  The specified timestamp is used, and\nall entries will have an identical timestamp.\n\n@param tableName name of table\n@param row row key\n@param mutations list of mutation commands\n@param timestamp timestamp\n\nParameters:\n - tableName\n - row\n - mutations\n - timestamp\n\"\"\"\n", "func_signal": "def mutateRowTs(self, tableName, row, mutations, timestamp):\n", "code": "self.send_mutateRowTs(tableName, row, mutations, timestamp)\nself.recv_mutateRowTs()", "path": "aggregator\\db\\hbase\\Hbase.py", "repo_name": "andreisavu/feedaggregator", "stars": 9, "license": "bsd-2-clause", "language": "python", "size": 297}
{"docstring": "\"\"\"\nApply a series of batches (each a series of mutations on a single row)\nin a single transaction.  If an exception is thrown, then the\ntransaction is aborted.  The specified timestamp is used, and\nall entries will have an identical timestamp.\n\n@param tableName name of table\n@param rowBatches list of row batches\n@param timestamp timestamp\n\nParameters:\n - tableName\n - rowBatches\n - timestamp\n\"\"\"\n", "func_signal": "def mutateRowsTs(self, tableName, rowBatches, timestamp):\n", "code": "self.send_mutateRowsTs(tableName, rowBatches, timestamp)\nself.recv_mutateRowsTs()", "path": "aggregator\\db\\hbase\\Hbase.py", "repo_name": "andreisavu/feedaggregator", "stars": 9, "license": "bsd-2-clause", "language": "python", "size": 297}
{"docstring": "\"\"\"\nParameters:\n - tableNameOrRegionName\n\"\"\"\n", "func_signal": "def majorCompact(self, tableNameOrRegionName):\n", "code": "self.send_majorCompact(tableNameOrRegionName)\nself.recv_majorCompact()", "path": "aggregator\\db\\hbase\\Hbase.py", "repo_name": "andreisavu/feedaggregator", "stars": 9, "license": "bsd-2-clause", "language": "python", "size": 297}
{"docstring": "\"\"\"\nList the regions associated with a table.\n@param tableName table name\n@return list of region descriptors\n\nParameters:\n - tableName\n\"\"\"\n", "func_signal": "def getTableRegions(self, tableName):\n", "code": "self.send_getTableRegions(tableName)\nreturn self.recv_getTableRegions()", "path": "aggregator\\db\\hbase\\Hbase.py", "repo_name": "andreisavu/feedaggregator", "stars": 9, "license": "bsd-2-clause", "language": "python", "size": 297}
{"docstring": "\"\"\"\nCompletely delete the row's cells.\n\n@param tableName name of table\n@param row key of the row to be completely deleted.\n\nParameters:\n - tableName\n - row\n\"\"\"\n", "func_signal": "def deleteAllRow(self, tableName, row):\n", "code": "self.send_deleteAllRow(tableName, row)\nself.recv_deleteAllRow()", "path": "aggregator\\db\\hbase\\Hbase.py", "repo_name": "andreisavu/feedaggregator", "stars": 9, "license": "bsd-2-clause", "language": "python", "size": 297}
{"docstring": "\"\"\"\nAtomically increment the column value specified.  Returns the next value post increment.\n@param tableName name of table\n@param row row to increment\n@param column name of column\n@param value amount to increment by\n\nParameters:\n - tableName\n - row\n - column\n - value\n\"\"\"\n", "func_signal": "def atomicIncrement(self, tableName, row, column, value):\n", "code": "self.send_atomicIncrement(tableName, row, column, value)\nreturn self.recv_atomicIncrement()", "path": "aggregator\\db\\hbase\\Hbase.py", "repo_name": "andreisavu/feedaggregator", "stars": 9, "license": "bsd-2-clause", "language": "python", "size": 297}
{"docstring": "\"\"\"Constructor for SingleServerIRCBot objects.\n\nArguments:\n\n    server_list -- A list of tuples (server, port) that\n                   defines which servers the bot should try to\n                   connect to.\n\n    nickname -- The bot's nickname.\n\n    realname -- The bot's realname.\n\n    reconnection_interval -- How long the bot should wait\n                             before trying to reconnect.\n\n    dcc_connections -- A list of initiated/accepted DCC\n    connections.\n\"\"\"\n\n", "func_signal": "def __init__(self, server_list, nickname, realname, reconnection_interval=60):\n", "code": "SimpleIRCClient.__init__(self)\nself.channels = IRCDict()\nself.server_list = server_list\nif not reconnection_interval or reconnection_interval < 0:\n    reconnection_interval = 2**31\nself.reconnection_interval = reconnection_interval\n\nself._nickname = nickname\nself._realname = realname\nfor i in [\"disconnect\", \"join\", \"kick\", \"mode\",\n          \"namreply\", \"nick\", \"part\", \"quit\"]:\n    self.connection.add_global_handler(i,\n                                       getattr(self, \"_on_\" + i),\n                                       -10)", "path": "ircbot.py", "repo_name": "python-irclib/python-irclib", "stars": 8, "license": "lgpl-2.1", "language": "python", "size": 181}
{"docstring": "\"\"\"Send a WHOWAS command.\"\"\"\n", "func_signal": "def whowas(self, nick, max=\"\", server=\"\"):\n", "code": "self.send_raw(\"WHOWAS %s%s%s\" % (nick,\n                                 max and (\" \" + max),\n                                 server and (\" \" + server)))", "path": "irclib.py", "repo_name": "python-irclib/python-irclib", "stars": 8, "license": "lgpl-2.1", "language": "python", "size": 181}
{"docstring": "\"\"\"Set mode on the channel.\n\nArguments:\n\n    mode -- The mode (a single-character string).\n\n    value -- Value\n\"\"\"\n", "func_signal": "def set_mode(self, mode, value=None):\n", "code": "if mode == \"o\":\n    self.operdict[value] = 1\nelif mode == \"v\":\n    self.voiceddict[value] = 1\nelse:\n    self.modes[mode] = value", "path": "ircbot.py", "repo_name": "python-irclib/python-irclib", "stars": 8, "license": "lgpl-2.1", "language": "python", "size": 181}
{"docstring": "\"\"\"[Internal]\"\"\"\n", "func_signal": "def _on_disconnect(self, c, e):\n", "code": "self.channels = IRCDict()\nself.connection.execute_delayed(self.reconnection_interval,\n                                self._connected_checker)", "path": "ircbot.py", "repo_name": "python-irclib/python-irclib", "stars": 8, "license": "lgpl-2.1", "language": "python", "size": 181}
{"docstring": "\"\"\"Get the user part of a nickmask.\n\n(The source of an Event is a nickmask.)\n\"\"\"\n", "func_signal": "def nm_to_u(s):\n", "code": "s = s.split(\"!\")[1]\nreturn s.split(\"@\")[0]", "path": "irclib.py", "repo_name": "python-irclib/python-irclib", "stars": 8, "license": "lgpl-2.1", "language": "python", "size": 181}
{"docstring": "\"\"\"Close the connection.\n\nThis method closes the connection permanently; after it has\nbeen called, the object is unusable.\n\"\"\"\n\n", "func_signal": "def close(self):\n", "code": "self.disconnect(\"Closing object\")\nself.irclibobj._remove_connection(self)", "path": "irclib.py", "repo_name": "python-irclib/python-irclib", "stars": 8, "license": "lgpl-2.1", "language": "python", "size": 181}
{"docstring": "\"\"\"[Internal]\"\"\"\n", "func_signal": "def _parse_modes(mode_string, unary_modes=\"\"):\n", "code": "modes = []\narg_count = 0\n\n# State variable.\nsign = \"\"\n\na = mode_string.split()\nif len(a) == 0:\n    return []\nelse:\n    mode_part, args = a[0], a[1:]\n\nif mode_part[0] not in \"+-\":\n    return []\nfor ch in mode_part:\n    if ch in \"+-\":\n        sign = ch\n    elif ch == \" \":\n        collecting_arguments = 1\n    elif ch in unary_modes:\n        if len(args) >= arg_count + 1:\n            modes.append([sign, ch, args[arg_count]])\n            arg_count = arg_count + 1\n        else:\n            modes.append([sign, ch, None])\n    else:\n        modes.append([sign, ch, None])\nreturn modes", "path": "irclib.py", "repo_name": "python-irclib/python-irclib", "stars": 8, "license": "lgpl-2.1", "language": "python", "size": 181}
{"docstring": "\"\"\"[Internal]\"\"\"\n", "func_signal": "def _connect(self):\n", "code": "password = None\nif len(self.server_list[0]) > 2:\n    password = self.server_list[0][2]\ntry:\n    self.connect(self.server_list[0][0],\n                 self.server_list[0][1],\n                 self._nickname,\n                 password,\n                 ircname=self._realname)\nexcept ServerConnectionError:\n    pass", "path": "ircbot.py", "repo_name": "python-irclib/python-irclib", "stars": 8, "license": "lgpl-2.1", "language": "python", "size": 181}
{"docstring": "\"\"\"[Internal]\"\"\"\n", "func_signal": "def _handle_event(self, connection, event):\n", "code": "h = self.handlers\nfor handler in h.get(\"all_events\", []) + h.get(event.eventtype(), []):\n    if handler[1](connection, event) == \"NO MORE\":\n        return", "path": "irclib.py", "repo_name": "python-irclib/python-irclib", "stars": 8, "license": "lgpl-2.1", "language": "python", "size": 181}
{"docstring": "\"\"\"Called when there is more data to read on connection sockets.\n\nArguments:\n\n    sockets -- A list of socket objects.\n\nSee documentation for IRC.__init__.\n\"\"\"\n", "func_signal": "def process_data(self, sockets):\n", "code": "for s in sockets:\n    for c in self.connections:\n        if s == c._get_socket():\n            c.process_data()", "path": "irclib.py", "repo_name": "python-irclib/python-irclib", "stars": 8, "license": "lgpl-2.1", "language": "python", "size": 181}
{"docstring": "\"\"\"[Internal]\"\"\"\n", "func_signal": "def _on_quit(self, c, e):\n", "code": "nick = nm_to_n(e.source())\nfor ch in self.channels.values():\n    if ch.has_user(nick):\n        ch.remove_user(nick)", "path": "ircbot.py", "repo_name": "python-irclib/python-irclib", "stars": 8, "license": "lgpl-2.1", "language": "python", "size": 181}
{"docstring": "\"\"\"Send a LIST command.\"\"\"\n", "func_signal": "def list(self, channels=None, server=\"\"):\n", "code": "command = \"LIST\"\nif channels:\n    command = command + \" \" + \",\".join(channels)\nif server:\n    command = command + \" \" + server\nself.send_raw(command)", "path": "irclib.py", "repo_name": "python-irclib/python-irclib", "stars": 8, "license": "lgpl-2.1", "language": "python", "size": 181}
{"docstring": "\"\"\"Disconnects all connections.\"\"\"\n", "func_signal": "def disconnect_all(self, message=\"\"):\n", "code": "for c in self.connections:\n    c.disconnect(message)", "path": "irclib.py", "repo_name": "python-irclib/python-irclib", "stars": 8, "license": "lgpl-2.1", "language": "python", "size": 181}
{"docstring": "\"\"\"Constructor of Event objects.\n\nArguments:\n\n    eventtype -- A string describing the event.\n\n    source -- The originator of the event (a nick mask or a server).\n\n    target -- The target of the event (a nick or a channel).\n\n    arguments -- Any event specific arguments.\n\"\"\"\n", "func_signal": "def __init__(self, eventtype, source, target, arguments=None):\n", "code": "self._eventtype = eventtype\nself._source = source\nself._target = target\nif arguments:\n    self._arguments = arguments\nelse:\n    self._arguments = []", "path": "irclib.py", "repo_name": "python-irclib/python-irclib", "stars": 8, "license": "lgpl-2.1", "language": "python", "size": 181}
{"docstring": "\"\"\"Start the bot.\"\"\"\n", "func_signal": "def start(self):\n", "code": "self._connect()\nSimpleIRCClient.start(self)", "path": "ircbot.py", "repo_name": "python-irclib/python-irclib", "stars": 8, "license": "lgpl-2.1", "language": "python", "size": 181}
{"docstring": "\"\"\"[Internal]\"\"\"\n", "func_signal": "def _on_mode(self, c, e):\n", "code": "modes = parse_channel_modes(\" \".join(e.arguments()))\nt = e.target()\nif is_channel(t):\n    ch = self.channels[t]\n    for mode in modes:\n        if mode[0] == \"+\":\n            f = ch.set_mode\n        else:\n            f = ch.clear_mode\n        f(mode[1], mode[2])\nelse:\n    # Mode on self... XXX\n    pass", "path": "ircbot.py", "repo_name": "python-irclib/python-irclib", "stars": 8, "license": "lgpl-2.1", "language": "python", "size": 181}
{"docstring": "\"\"\"[Internal]\"\"\"\n\n# e.arguments()[0] == \"@\" for secret channels,\n#                     \"*\" for private channels,\n#                     \"=\" for others (public channels)\n# e.arguments()[1] == channel\n# e.arguments()[2] == nick list\n\n", "func_signal": "def _on_namreply(self, c, e):\n", "code": "ch = e.arguments()[1]\nfor nick in e.arguments()[2].split():\n    if nick[0] == \"@\":\n        nick = nick[1:]\n        self.channels[ch].set_mode(\"o\", nick)\n    elif nick[0] == \"+\":\n        nick = nick[1:]\n        self.channels[ch].set_mode(\"v\", nick)\n    self.channels[ch].add_user(nick)", "path": "ircbot.py", "repo_name": "python-irclib/python-irclib", "stars": 8, "license": "lgpl-2.1", "language": "python", "size": 181}
{"docstring": "\"\"\"Called when a timeout notification is due.\n\nSee documentation for IRC.__init__.\n\"\"\"\n", "func_signal": "def process_timeout(self):\n", "code": "t = time.time()\nwhile self.delayed_commands:\n    if t >= self.delayed_commands[0][0]:\n        self.delayed_commands[0][1](*self.delayed_commands[0][2])\n        del self.delayed_commands[0]\n    else:\n        break", "path": "irclib.py", "repo_name": "python-irclib/python-irclib", "stars": 8, "license": "lgpl-2.1", "language": "python", "size": 181}
{"docstring": "\"\"\"Let the bot die.\n\nArguments:\n\n    msg -- Quit message.\n\"\"\"\n\n", "func_signal": "def die(self, msg=\"Bye, cruel world!\"):\n", "code": "self.connection.disconnect(msg)\nsys.exit(0)", "path": "ircbot.py", "repo_name": "python-irclib/python-irclib", "stars": 8, "license": "lgpl-2.1", "language": "python", "size": 181}
{"docstring": "\"\"\"Send a LINKS command.\"\"\"\n", "func_signal": "def links(self, remote_server=\"\", server_mask=\"\"):\n", "code": "command = \"LINKS\"\nif remote_server:\n    command = command + \" \" + remote_server\nif server_mask:\n    command = command + \" \" + server_mask\nself.send_raw(command)", "path": "irclib.py", "repo_name": "python-irclib/python-irclib", "stars": 8, "license": "lgpl-2.1", "language": "python", "size": 181}
{"docstring": "\"\"\"Returns a list of possible alternative spellings of 'text', as\n('word', score, weight) triples, where 'word' is the suggested\nword, 'score' is the score that was assigned to the word using\n:meth:`SpellChecker.add_field` or :meth:`SpellChecker.add_scored_words`,\nand 'weight' is the score the word received in the search for the\noriginal word's ngrams.\n\nYou must add words to the dictionary (using add_field, add_words,\nand/or add_scored_words) before you can use this.\n\nThis is a lower-level method, in case an expert user needs access to\nthe raw scores, for example to implement a custom suggestion ranking\nalgorithm. Most people will want to call :meth:`~SpellChecker.suggest`\ninstead, which simply returns the top N valued words.\n\n:param text: The word to check.\n:rtype: list\n\"\"\"\n\n", "func_signal": "def suggestions_and_scores(self, text, weighting=None):\n", "code": "if weighting is None:\n    weighting = TF_IDF()\n\ngrams = defaultdict(list)\nfor size in xrange(self.mingram, self.maxgram + 1):\n    key = \"gram%s\" % size\n    nga = analysis.NgramAnalyzer(size)\n    for t in nga(text):\n        grams[key].append(t.text)\n\nqueries = []\nfor size in xrange(self.mingram, min(self.maxgram + 1, len(text))):\n    key = \"gram%s\" % size\n    gramlist = grams[key]\n    queries.append(query.Term(\"start%s\" % size, gramlist[0],\n                              boost=self.booststart))\n    queries.append(query.Term(\"end%s\" % size, gramlist[-1],\n                              boost=self.boostend))\n    for gram in gramlist:\n        queries.append(query.Term(key, gram))\n\nq = query.Or(queries)\nix = self.index()\ns = ix.searcher(weighting=weighting)\ntry:\n    result = s.search(q)\n    return [(fs[\"word\"], fs[\"score\"], result.score(i))\n            for i, fs in enumerate(result)\n            if fs[\"word\"] != text]\nfinally:\n    s.close()", "path": "src\\whoosh\\spelling.py", "repo_name": "soad241/whoosh", "stars": 9, "license": "apache-2.0", "language": "python", "size": 476}
{"docstring": "# Do an initial check for NullQuery.\n", "func_signal": "def normalize(self):\n", "code": "subqueries = [q for q in self.subqueries if q is not NullQuery]\n\nif not subqueries:\n    return NullQuery\n\n# Normalize the subqueries and eliminate duplicate terms.\nsubqs = []\nseenterms = set()\nfor s in subqueries:\n    s = s.normalize()\n    if s is NullQuery:\n        continue\n\n    if isinstance(s, Term):\n        term = (s.fieldname, s.text)\n        if term in seenterms:\n            continue\n        seenterms.add(term)\n\n    if isinstance(s, self.__class__):\n        subqs += s.subqueries\n    else:\n        subqs.append(s)\n\nif not subqs:\n    return NullQuery\nif len(subqs) == 1:\n    return subqs[0]\n\nreturn self.__class__(subqs, boost=self.boost)", "path": "src\\whoosh\\query.py", "repo_name": "soad241/whoosh", "stars": 9, "license": "apache-2.0", "language": "python", "size": 476}
{"docstring": "\"\"\"Returns the version number of Whoosh as a string.\n\n:param build: Whether to include the build number in the string.\n:param extra: Whether to include alpha/beta/rc etc. tags. Only\n    checked if build is True.\n:rtype: str\n\"\"\"\n\n", "func_signal": "def versionstring(build=True, extra=True):\n", "code": "if build:\n    first = 3\nelse:\n    first = 2\n\ns = \".\".join(str(n) for n in __version__[:first])\nif build and extra:\n    s += \"\".join(str(n) for n in __version__[3:])\n\nreturn s", "path": "src\\whoosh\\__init__.py", "repo_name": "soad241/whoosh", "stars": 9, "license": "apache-2.0", "language": "python", "size": 476}
{"docstring": "\"\"\"\n:param query: A :class:`Query` object. The results of this query\n    are *excluded* from the parent query.\n:param boost: Boost is meaningless for excluded documents but this\n    keyword argument is accepted for the sake of a consistent interface.\n\"\"\"\n\n", "func_signal": "def __init__(self, query, boost=1.0):\n", "code": "self.query = query\nself.boost = boost", "path": "src\\whoosh\\query.py", "repo_name": "soad241/whoosh", "stars": 9, "license": "apache-2.0", "language": "python", "size": 476}
{"docstring": "\"\"\"Returns a set of all terms in this query tree.\n\nThis method simply operates on the query itself, without reference to\nan index (unlike existing_terms()), so it will *not* add terms that\nrequire an index to compute, such as Prefix and Wildcard.\n\n>>> q = And([Term(\"content\", u\"render\"), Term(\"path\", u\"/a/b\")])\n>>> q.all_terms()\nset([(\"content\", u\"render\"), (\"path\", u\"/a/b\")])\n\n:param phrases: Whether to add words found in Phrase queries.\n:rtype: set\n\"\"\"\n\n", "func_signal": "def all_terms(self, termset=None, phrases=True):\n", "code": "if termset is None:\n    termset = set()\nself._all_terms(termset, phrases=phrases)\nreturn termset", "path": "src\\whoosh\\query.py", "repo_name": "soad241/whoosh", "stars": 9, "license": "apache-2.0", "language": "python", "size": 476}
{"docstring": "# If there are no wildcard characters in this \"wildcard\", turn it into\n# a simple Term.\n", "func_signal": "def normalize(self):\n", "code": "text = self.text\nif text == \"*\":\n    return Every(boost=self.boost)\nif \"*\" not in text and \"?\" not in text:\n    # If no wildcard chars, convert to a normal term.\n    return Term(self.fieldname, self.text, boost=self.boost)\nelif (\"?\" not in text\n      and text.endswith(\"*\")\n      and text.find(\"*\") == len(text) - 1\n      and (len(text) < 2 or text[-2] != \"\\\\\")):\n    # If the only wildcard char is an asterisk at the end, convert to a\n    # Prefix query.\n    return Prefix(self.fieldname, self.text[:-1], boost=self.boost)\nelse:\n    return self", "path": "src\\whoosh\\query.py", "repo_name": "soad241/whoosh", "stars": 9, "license": "apache-2.0", "language": "python", "size": 476}
{"docstring": "# Each sub-scorer of the intersection represents a word in the\n# phrase. The positions of each word is therefore the value of the\n# current posting for each sub-scorer.\n", "func_signal": "def _poses(self):\n", "code": "return [scorer.value_as(\"positions\")\n        for scorer in self.intersection.scorers]", "path": "src\\whoosh\\query.py", "repo_name": "soad241/whoosh", "stars": 9, "license": "apache-2.0", "language": "python", "size": 476}
{"docstring": "# Skip excluded document numbers\n", "func_signal": "def _find(self):\n", "code": "id = self.id\nlimit = self.limit\nexclude = self.exclude\nwhile id < limit and id in exclude:\n    id += 1\nif id >= limit:\n    self.id = None\nelse:\n    self.id = id", "path": "src\\whoosh\\query.py", "repo_name": "soad241/whoosh", "stars": 9, "license": "apache-2.0", "language": "python", "size": 476}
{"docstring": "\"\"\"Adds the terms in a field from another index to the backend\ndictionary. This method calls add_scored_words() and uses each term's\nfrequency as the score. As a result, more common words will be\nsuggested before rare words. If you want to calculate the scores\ndifferently, use add_scored_words() directly.\n\n:param ix: The index.Index object from which to add terms.\n:param fieldname: The field name (or number) of a field in the source\n    index. All the indexed terms from this field will be added to the\n    dictionary.\n\"\"\"\n\n", "func_signal": "def add_field(self, ix, fieldname):\n", "code": "r = ix.reader()\ntry:\n    self.add_scored_words((w, freq)\n                          for w, _, freq in r.iter_field(fieldname))\nfinally:\n    r.close()", "path": "src\\whoosh\\spelling.py", "repo_name": "soad241/whoosh", "stars": 9, "license": "apache-2.0", "language": "python", "size": 476}
{"docstring": "\"\"\"\n:param requiredquery: Documents matching this query are returned.\n:param optionalquery: If a document matches this query as well as\n    'requiredquery', the score from this query is added to the\n    document score from 'requiredquery'.\n\"\"\"\n\n# The superclass CompoundQuery expects the subqueries to be\n# in a sequence in self.subqueries\n", "func_signal": "def __init__(self, requiredquery, optionalquery, boost=1.0):\n", "code": "self.subqueries = (requiredquery, optionalquery)\nself.boost = boost", "path": "src\\whoosh\\query.py", "repo_name": "soad241/whoosh", "stars": 9, "license": "apache-2.0", "language": "python", "size": 476}
{"docstring": "\"\"\"\n:param fieldname: the field to search.\n:param words: a list of words (unicode strings) in the phrase.\n:param slop: the number of words allowed between each \"word\" in the\n    phrase; the default of 1 means the phrase must match exactly.\n:param boost: a boost factor that to apply to the raw score of\n    documents matched by this query.\n\"\"\"\n\n", "func_signal": "def __init__(self, fieldname, words, slop=1, boost=1.0):\n", "code": "self.fieldname = fieldname\nself.words = words\nself.slop = slop\nself.boost = boost", "path": "src\\whoosh\\query.py", "repo_name": "soad241/whoosh", "stars": 9, "license": "apache-2.0", "language": "python", "size": 476}
{"docstring": "# Creates a schema given this object's mingram and maxgram attributes.\n\n", "func_signal": "def _schema(self):\n", "code": "from fields import Schema, FieldType, Frequency, ID, STORED\nfrom analysis import SimpleAnalyzer\n\nidtype = ID()\nfreqtype = FieldType(format=Frequency(SimpleAnalyzer()))\n\nfls = [(\"word\", STORED), (\"score\", STORED)]\nfor size in xrange(self.mingram, self.maxgram + 1):\n    fls.extend([(\"start%s\" % size, idtype),\n                (\"end%s\" % size, idtype),\n                (\"gram%s\" % size, freqtype)])\n\nreturn Schema(**dict(fls))", "path": "src\\whoosh\\spelling.py", "repo_name": "soad241/whoosh", "stars": 9, "license": "apache-2.0", "language": "python", "size": 476}
{"docstring": "# Use a term vector for the current document to get the positions\n# of the words in the phrase\n", "func_signal": "def _poses(self):\n", "code": "docnum = self.intersection.id\nfieldnum = self.fieldnum\nif not self.reader.has_vector(docnum, fieldnum):\n    raise QueryError(\"Phrase query: document %s field %r has no vector\")\nvreader = self.reader.vector(docnum, fieldnum)\n# The vector is in sorted order, so grab the positions lists in\n# sorted order and put them in a dictionary\nposes = {}\nfor word in self.sortedwords:\n    vreader.skip_to(word)\n    assert vreader.id == word\n    if vreader.id != word:\n        # Since the term index and term vector can potentially use\n        # different analyzers, it's possible that the words in the\n        # term index might not match the words in the vector.\n        raise QueryError(\"Phrase query: %r in term index but not in vector (possible analyzer mismatch)\" % word)\n    poses[word] = vreader.value_as(\"positions\")\n# Now put the position lists in phrase order\nposes = [poses[word] for word in self.words]\nreturn poses", "path": "src\\whoosh\\query.py", "repo_name": "soad241/whoosh", "stars": 9, "license": "apache-2.0", "language": "python", "size": 476}
{"docstring": "# Returns a BitVector where the positions are docnums\n# and True means the docnum is banned from the results.\n# 'sourcevector' is the incoming exclude_docs. This\n# function makes a copy of it and adds the documents\n# from notqueries\n\n", "func_signal": "def _not_vector(searcher, notqueries, sourcevector):\n", "code": "if sourcevector is None:\n    nvector = BitVector(searcher.reader().doc_count_all())\nelse:\n    nvector = sourcevector.copy()\n\nfor nquery in notqueries:\n    nvector.set_from(nquery.docs(searcher))\n\nreturn nvector", "path": "src\\whoosh\\query.py", "repo_name": "soad241/whoosh", "stars": 9, "license": "apache-2.0", "language": "python", "size": 476}
{"docstring": "\"\"\"Returns a list of suggested alternative spellings of 'text'. You\nmust add words to the dictionary (using add_field, add_words, and/or\nadd_scored_words) before you can use this.\n\n:param text: The word to check.\n:param number: The maximum number of suggestions to return.\n:param usescores: Use the per-word score to influence the suggestions.\n:rtype: list\n\"\"\"\n\n", "func_signal": "def suggest(self, text, number=3, usescores=False):\n", "code": "if usescores:\n    def keyfn(a):\n        return 0 - (1 / distance(text, a[0])) * a[1]\nelse:\n    def keyfn(a):\n        return distance(text, a[0])\n\nsuggestions = self.suggestions_and_scores(text)\nsuggestions.sort(key=keyfn)\nreturn [word for word, _, _ in suggestions[:number]]", "path": "src\\whoosh\\spelling.py", "repo_name": "soad241/whoosh", "stars": 9, "license": "apache-2.0", "language": "python", "size": 476}
{"docstring": "\"\"\"Returns the backend index of this object (instantiating it if it\ndidn't already exist).\n\"\"\"\n\n", "func_signal": "def index(self, create=False):\n", "code": "import index\nif create or not self._index:\n    create = create or not index.exists(self.storage, indexname=self.indexname)\n    if create:\n        self._index = self.storage.create_index(self._schema(), self.indexname)\n    else:\n        self._index = self.storage.open_index(self.indexname)\nreturn self._index", "path": "src\\whoosh\\spelling.py", "repo_name": "soad241/whoosh", "stars": 9, "license": "apache-2.0", "language": "python", "size": 476}
{"docstring": "\"\"\"\n:param positive: query to INCLUDE.\n:param negative: query whose matches should be EXCLUDED.\n:param boost: boost factor that should be applied to the raw score of\n    results matched by this query.\n\"\"\"\n\n", "func_signal": "def __init__(self, positive, negative, boost=1.0):\n", "code": "self.positive = positive\nself.negative = negative\nself.boost = boost", "path": "src\\whoosh\\query.py", "repo_name": "soad241/whoosh", "stars": 9, "license": "apache-2.0", "language": "python", "size": 476}
{"docstring": "\"\"\"Returns an iterator of docnums matching this query.\n\n>>> searcher = my_index.searcher()\n>>> list(my_query.docs(searcher))\n[10, 34, 78, 103]\n\n:param searcher: A :class:`whoosh.searching.Searcher` object.\n:param exclude_docs: A :class:`~whoosh.support.bitvector.BitVector`\n    of document numbers to exclude from the results, or None to not\n    exclude any documents.\n\"\"\"\n\n", "func_signal": "def docs(self, searcher, exclude_docs=None):\n", "code": "try:\n    return self.scorer(searcher).all_ids()\nexcept TermNotFound:\n    return []", "path": "src\\whoosh\\query.py", "repo_name": "soad241/whoosh", "stars": 9, "license": "apache-2.0", "language": "python", "size": 476}
{"docstring": "\"\"\"\n:param scoredquery: The query that is scored. Only documents that also\n    appear in the second query ('requiredquery') are scored.\n:param requiredquery: Only documents that match both 'scoredquery' and\n    'requiredquery' are returned, but this query does not\n    contribute to the scoring.\n\"\"\"\n\n# The superclass CompoundQuery expects the subqueries to be\n# in a sequence in self.subqueries\n", "func_signal": "def __init__(self, scoredquery, requiredquery, boost=1.0):\n", "code": "self.subqueries = (scoredquery, requiredquery)\nself.boost = boost", "path": "src\\whoosh\\query.py", "repo_name": "soad241/whoosh", "stars": 9, "license": "apache-2.0", "language": "python", "size": 476}
{"docstring": "\"\"\"\n:param fieldname: The field to search in.\n:param text: A glob to search for. May contain ? and/or * wildcard\n    characters. Note that matching a wildcard expression that starts\n    with a wildcard is very inefficent, since the query must test every\n    term in the field.\n:param boost: A boost factor that should be applied to the raw score of\n    results matched by this query.\n\"\"\"\n\n", "func_signal": "def __init__(self, fieldname, text, boost=1.0):\n", "code": "self.fieldname = fieldname\nself.text = text\nself.boost = boost\n\nself.expression = re.compile(fnmatch.translate(text))\n\n# Get the \"prefix\" -- the substring before the first wildcard.\nqm = text.find(\"?\")\nst = text.find(\"*\")\nif qm < 0 and st < 0:\n    self.prefix = \"\"\nelif qm < 0:\n    self.prefix = text[:st]\nelif st < 0:\n    self.prefix = text[:qm]\nelse:\n    self.prefix = text[:min(st, qm)]", "path": "src\\whoosh\\query.py", "repo_name": "soad241/whoosh", "stars": 9, "license": "apache-2.0", "language": "python", "size": 476}
{"docstring": "\"\"\"Make the module's controller override another module's controllers.\n\nGiven a <frontend> element, create the sub elements necessary to\nmake the module's controller(s) override the controller(s) of the\nmodule to which self.superclass belongs.\n\nArgs:\n    elem: An lxml.etree._Element object mapping to a <frontend>\n          element.\n\nReturn:\n    An lxml.etree._Element object.\n\n\"\"\"\n", "func_signal": "def _add_override(self, elem):\n", "code": "substrings = self.superclass.split(\"_\")\nsuper_module = substrings[1].lower()\nsuper_prefix = \"_\".join(substrings[:2])\n\nrouters = find_or_create(elem, \"routers\")\nif routers.find(super_module) is not None:\n    return # Bail (assume that an override already exists).\n\nsuper_module = etree.SubElement(routers, super_module)\nargs = etree.SubElement(super_module, \"args\")\nmodules = etree.SubElement(args, \"modules\")\ngroup = etree.SubElement(modules, self.module.name.lower())\ngroup.set(\"before\", super_prefix)\ngroup.text = \"_\".join((self.module.namespace, self.module.name))\nreturn elem", "path": "magetool\\commands\\controller.py", "repo_name": "jhckragh/magetool", "stars": 8, "license": "bsd-2-clause", "language": "python", "size": 214}
{"docstring": "\"\"\"Make Mage aware that the module supplies a layout file.\"\"\"\n", "func_signal": "def register(self, name):\n", "code": "config = self.get_config()\nfrontend = find_or_create(config, \"frontend\")\nlayout = find_or_create(frontend, \"layout\")\nupdates = find_or_create(layout, \"updates\")\n# Only update the file if the following element doesn't exist.\n# This way we avoid inadvertently creating duplicate layout\n# updates.\ngroup = updates.find(self.module.name.lower())\nif group is None:\n    group = etree.SubElement(updates, self.module.name.lower())\n    file_ = etree.SubElement(group, \"file\")\n    file_.text = self._format_name(name)\n    self.put_config(config)", "path": "magetool\\commands\\layout.py", "repo_name": "jhckragh/magetool", "stars": 8, "license": "bsd-2-clause", "language": "python", "size": 214}
{"docstring": "\"\"\"Initialize the module.\"\"\"\n", "func_signal": "def __init__(self):\n", "code": "self._configure()\nself._check_case_convention()\nself._files_created = []", "path": "magetool\\commands\\module.py", "repo_name": "jhckragh/magetool", "stars": 8, "license": "bsd-2-clause", "language": "python", "size": 214}
{"docstring": "\"\"\"Infer the global class's superclass if none is supplied.\"\"\"\n", "func_signal": "def _infer_super(self, superclass):\n", "code": "if superclass is None:\n    end = \"Template\" if self.type == \"block\" else \"Abstract\"\n    superclass = \"Mage_Core_%s_%s\" % (self.type.capitalize(), end)\nreturn superclass", "path": "magetool\\libraries\\globalclass.py", "repo_name": "jhckragh/magetool", "stars": 8, "license": "bsd-2-clause", "language": "python", "size": 214}
{"docstring": "\"\"\"See _fill_template in magetool.libraries.cls.\"\"\"\n", "func_signal": "def _fill_template(self, name, superclass):\n", "code": "end = name.lower().split(\"_\")[-1]\nid_field_name = self.id_field_name or end + \"_id\"\ntemplate = Template(self.template)\ntemplate = template.substitute(namespace=self.module.namespace,\n                               module_name=self.module.name,\n                               name=name,\n                               superclass=superclass,\n                               group=self.module.name.lower(),\n                               name_lower=name.lower(),\n                               end=end,\n                               id_field_name=id_field_name)\nreturn template", "path": "magetool\\commands\\model.py", "repo_name": "jhckragh/magetool", "stars": 8, "license": "bsd-2-clause", "language": "python", "size": 214}
{"docstring": "\"\"\"Tell Mage that the module has one or more self.type global\nclasses.\n\n\"\"\"\n", "func_signal": "def register(self):\n", "code": "tag = self.module.name.lower()\nif not self.config.xpath(self.xpath + \"/\" + tag):\n    group = etree.SubElement(self.type_elem, self.module.name.lower())\n    class_ = etree.SubElement(group, \"class\")\n    class_.text = \"%s_%s_%s\" % (self.module.namespace,\n                                self.module.name,\n                                self.type.capitalize())\n    self.put_config(self.config)", "path": "magetool\\libraries\\globalclass.py", "repo_name": "jhckragh/magetool", "stars": 8, "license": "bsd-2-clause", "language": "python", "size": 214}
{"docstring": "\"\"\"Tell Mage that the module has one or more resource models.\"\"\"\n", "func_signal": "def _register_resource(self, name):\n", "code": "GlobalClass.register(self)\ntag = self.module.name.lower()\ngroup = self.config.xpath(self.xpath + \"/\" + tag)[0]\ngroup_mysql4 = group.tag + \"_mysql4\"\nresource_model = find_or_create(group, \"resourceModel\")\nresource_model.text = group_mysql4\ngroup_mysql4 = find_or_create(self.type_elem, group_mysql4)\nclass_ = find_or_create(group_mysql4, \"class\")\nclass_.text = \"%s_%s_%s_Mysql4\" % (self.module.namespace,\n                                   self.module.name,\n                                   self.type.capitalize())\nentities = find_or_create(group_mysql4, \"entities\")\nname_lower = find_or_create(entities, name.lower())\ntable = find_or_create(name_lower, \"table\")\ntable.text = self.table or group.tag + \"_\" + name_lower.tag\nself.put_config(self.config)", "path": "magetool\\commands\\model.py", "repo_name": "jhckragh/magetool", "stars": 8, "license": "bsd-2-clause", "language": "python", "size": 214}
{"docstring": "\"\"\"Format a name according to the naming convention for controllers.\n\nArgs:\n    name: The name of the controller, with or without the\n          'Controller' suffix, e.g., 'IndexController' or\n          'Tracking'.\n\n\"\"\"\n", "func_signal": "def _format_name(self, name):\n", "code": "suffix = \"controller\"\nif name.lower().endswith(suffix):\n    name = name[:-len(suffix)]\nsubstrings = name.split(\"_\")\nsubstrings[-1] = substrings[-1].capitalize() + \"Controller\"\nreturn \"_\".join(substrings)", "path": "magetool\\commands\\controller.py", "repo_name": "jhckragh/magetool", "stars": 8, "license": "bsd-2-clause", "language": "python", "size": 214}
{"docstring": "\"\"\"Tell Mage that this global class overrides self.superclass.\"\"\"\n", "func_signal": "def _override(self):\n", "code": "substrings = self.superclass.split(\"_\")\ntags = {\"module\": substrings[1].lower(),\n        \"name\": \"_\".join(substrings[3:]).lower()} # e.g., product_view\nelems = \"/rewrite/\".join((tags[\"module\"], tags[\"name\"]))\nif not self.config.xpath(self.xpath + \"/\" + elems):\n    module = find_or_create(self.type_elem, tags[\"module\"])\n    rewrite = find_or_create(module, \"rewrite\")\n    name = etree.SubElement(rewrite, tags[\"name\"])\n    name.text = \"%s_%s_%s_%s\" % (self.module.namespace,\n                                 self.module.name,\n                                 self.type.capitalize(),\n                                 self.name)\n    self.put_config(self.config)", "path": "magetool\\libraries\\globalclass.py", "repo_name": "jhckragh/magetool", "stars": 8, "license": "bsd-2-clause", "language": "python", "size": 214}
{"docstring": "\"\"\"Create the model and its resource.\"\"\"\n", "func_signal": "def create(self, name):\n", "code": "self._register_resource(name)\nGlobalClass.create(self, name)\nself.template = resource_template\nname = \"Mysql4_\" + name # We prepend this string so _create_class()\n                        # will create a directory called \"Mysql4\"\n                        # in the Model/ directory.\nself._create_class(name, \"Mage_Core_Model_Mysql4_Abstract\")", "path": "magetool\\commands\\model.py", "repo_name": "jhckragh/magetool", "stars": 8, "license": "bsd-2-clause", "language": "python", "size": 214}
{"docstring": "\"\"\"Create the global class.\n\nDispatch requests to create an empty global class and update\nthe module's configuration file.\n\nArgs:\n    name: Name of the global class, e.g., \"Product\" or\n          \"ActivePoll\".\n\n\"\"\"\n", "func_signal": "def create(self, name):\n", "code": "self.name = name\nself._create_class(name, self.superclass)\nif self.override:\n    self._override()\nelse:\n    self.register()", "path": "magetool\\libraries\\globalclass.py", "repo_name": "jhckragh/magetool", "stars": 8, "license": "bsd-2-clause", "language": "python", "size": 214}
{"docstring": "\"\"\"Add a <routers> element with associated sub elements to elem.\n\nArgs:\n    elem: An lxml.etree._Element object mapping to a <frontend>\n          element.\n\nReturn:\n    An lxml.etree._Element object.\n\n\"\"\"\n", "func_signal": "def _add_route(self, elem):\n", "code": "route = elem.xpath(\"/config/%s/routers/%s\"\n                   % (elem.tag, self.module.name.lower()))\nif route:\n    return # Bail (assume that a route already exists).\n\nrouters = find_or_create(elem, \"routers\")\ngroup = etree.SubElement(routers, self.module.name.lower())\nuse = etree.SubElement(group, \"use\")\nuse.text = self.router\nargs = etree.SubElement(group, \"args\")\nmodule = etree.SubElement(args, \"module\")\nmodule.text = \"%s_%s\" % (self.module.namespace, self.module.name)\nfront_name = etree.SubElement(args, \"frontName\")\nfront_name.text = self.front_name\nreturn elem", "path": "magetool\\commands\\controller.py", "repo_name": "jhckragh/magetool", "stars": 8, "license": "bsd-2-clause", "language": "python", "size": 214}
{"docstring": "\"\"\"Create the controller.\n\nDispatch requests to create an empty controller class and\nupdate the module's configuration file.\n\n\"\"\"\n", "func_signal": "def create(self, name):\n", "code": "self._create_class(self._format_name(name), self.superclass)\nself.register()", "path": "magetool\\commands\\controller.py", "repo_name": "jhckragh/magetool", "stars": 8, "license": "bsd-2-clause", "language": "python", "size": 214}
{"docstring": "\"\"\"Initialize the global class, e.g., by storing run-time\narguments and by retrieving and preparing the module's\nconfiguration file.\n\nArgs:\n    superclass: Full name of the global class's superclass,\n                e.g., \"Mage_Rss_Block_Abstract\".\n    override: Whether this global class should override its\n              superclass.\n\n\"\"\"\n", "func_signal": "def __init__(self, superclass=None, override=False):\n", "code": "Class.__init__(self)\nself.superclass = self._infer_super(superclass)\nself.override = override\nself.type_tag = self.type + \"s\"\nself.config = self.get_config()\nself._prepare_config()\nself.xpath = \"/config/global/\" + self.type_tag\nself.type_elem = self.config.xpath(self.xpath)[0]", "path": "magetool\\libraries\\globalclass.py", "repo_name": "jhckragh/magetool", "stars": 8, "license": "bsd-2-clause", "language": "python", "size": 214}
{"docstring": "\"\"\"Create a directory structure, a configuration file, and an\nactivation file for the module, using the name parameter as the\nmodule's name.\n\n\"\"\"\n", "func_signal": "def create(self, name):\n", "code": "if not name[0].isupper():\n    warn(self.NAME_CASE_WARNING)\nself.name = name\nself._mkdir(self.name)\nfor directory in settings.directories:\n    self._mkdir(os.path.join(self.name, directory))\nself._create_config()\nself._create_regfile()\nself._print_feedback()", "path": "magetool\\commands\\module.py", "repo_name": "jhckragh/magetool", "stars": 8, "license": "bsd-2-clause", "language": "python", "size": 214}
{"docstring": "\"\"\"Read and parse a module configuration file, returning the root\nelement of the file.\n\n\"\"\"\n", "func_signal": "def get_config(self):\n", "code": "parser = etree.XMLParser(remove_blank_text=True)\nsource = open(self.module.cfg_path)\nconfig = etree.parse(source, parser).getroot()\nsource.close()\nreturn config", "path": "magetool\\libraries\\command.py", "repo_name": "jhckragh/magetool", "stars": 8, "license": "bsd-2-clause", "language": "python", "size": 214}
{"docstring": "\"\"\"Configure the module by guessing its code pool, namespace, and\nname.\n\n\"\"\"\n", "func_signal": "def _configure(self):\n", "code": "self.path = self._guess_module_path(os.getcwd())\ndirs = self.path.split(os.sep)\nself.name = dirs.pop() if dirs[self.CODE_DIR_INDEX] != \"code\" else None\nself.code_pool, self.namespace = dirs[-2:]\nself.app_path = os.sep.join(dirs[:self.CODE_DIR_INDEX])\nself.cfg_path = os.path.join(self.path, \"etc\", \"config.xml\")", "path": "magetool\\commands\\module.py", "repo_name": "jhckragh/magetool", "stars": 8, "license": "bsd-2-clause", "language": "python", "size": 214}
{"docstring": "\"\"\"Create the layout XML file and update the module's configuration\nfile accordingly.\n\n\"\"\"\n", "func_signal": "def create(self, name):\n", "code": "name = self._format_name(name)\nlayout_dir = os.path.join(self.module.app_path, \"design\", \"frontend\",\n                          \"default\", \"default\", \"layout\")\ndest = open(os.path.join(layout_dir, name), \"w\")\ndest.write(self.template)\ndest.close()\nself.register(name)", "path": "magetool\\commands\\layout.py", "repo_name": "jhckragh/magetool", "stars": 8, "license": "bsd-2-clause", "language": "python", "size": 214}
{"docstring": "\"\"\"Import the template file for the class. (We assume that the\ncommand's template file is named after the command's type.)\n\n\"\"\"\n", "func_signal": "def _get_template(self):\n", "code": "template = __import__(\"magetool.templates.\" + self.type,\n                      globals(), locals(), [\"magetool.templates\"])\nreturn template.string", "path": "magetool\\libraries\\command.py", "repo_name": "jhckragh/magetool", "stars": 8, "license": "bsd-2-clause", "language": "python", "size": 214}
{"docstring": "\"\"\"Create a file to register the module with Mage. This file makes\nMage scan the module's etc/ directory.\n\n\"\"\"\n", "func_signal": "def _create_regfile(self):\n", "code": "template = Template(regfile).substitute(namespace=self.namespace,\n                                        module_name=self.name,\n                                        code_pool=self.code_pool)\nparseString(template) # Syntax check\nself._write(os.path.join(self.app_path, \"etc\", \"modules\",\n                        \"%s_%s.xml\" % (self.namespace, self.name)),\n           template)", "path": "magetool\\commands\\module.py", "repo_name": "jhckragh/magetool", "stars": 8, "license": "bsd-2-clause", "language": "python", "size": 214}
{"docstring": "\"\"\"Unpack the page at `path' onto this system\"\"\"\n", "func_signal": "def cmd_unpack(self, path, wiki_slug=None, page_slug=None, force=False):\n", "code": "import conf; conf.configure_django('www.settings')\nfrom www.wiki.pack import unpack\nunpack(path, wiki_slug=wiki_slug, page_slug=page_slug, force=force)", "path": "commands.py", "repo_name": "mixerlabs/wiki", "stars": 11, "license": "None", "language": "python", "size": 136}
{"docstring": "# Cache host instances.  There's only a few and they're immutable.\n", "func_signal": "def host(self):\n", "code": "if self.wiki.host_id not in host_cache:\n    host_cache[self.wiki.host_id] = self.wiki.host.cls.obj()\nreturn host_cache[self.wiki.host_id]", "path": "models.py", "repo_name": "mixerlabs/wiki", "stars": 11, "license": "None", "language": "python", "size": 136}
{"docstring": "\"\"\"Return the category (a string) from a tag (also a string).\"\"\"\n", "func_signal": "def category_of_tag(tag):\n", "code": "label, category = tag.split(':', 1)\nassert label == 'category'\nreturn category", "path": "models.py", "repo_name": "mixerlabs/wiki", "stars": 11, "license": "None", "language": "python", "size": 136}
{"docstring": "# Deleting model 'WikiHost'\n", "func_signal": "def backwards(self, orm):\n", "code": "        db.delete_table('wiki_wikihost')\n# Deleting model 'Attachment'\n        db.delete_table('wiki_attachment')\n# Deleting model 'PageVersion'\n        db.delete_table('wiki_pageversion')\n# Deleting model 'Page'\n        db.delete_table('wiki_page')\n# Deleting model 'Wiki'\n        db.delete_table('wiki_wiki')\n# Deleting unique_together for [wiki, slug] on Page.\n        db.delete_unique('wiki_page', ['wiki_id', 'slug'])\n# Deleting unique_together for [page, version] on PageVersion.\n        db.delete_unique('wiki_pageversion', ['page_id', 'version'])\n# Deleting unique_together for [page, name] on Attachment.\n        db.delete_unique('wiki_attachment', ['page_id', 'name'])", "path": "migrations\\0001_initial.py", "repo_name": "mixerlabs/wiki", "stars": 11, "license": "None", "language": "python", "size": 136}
{"docstring": "\"\"\"Return the category of the given object according to\ndjango-tagging. If it does not have one, return the default as\ndefined in settings.\"\"\"\n", "func_signal": "def category_of_object(obj):\n", "code": "tags = Tag.objects.get_for_object(obj)\nreturn category_of_tag(tags[0].name) if len(tags) > 0 else None", "path": "models.py", "repo_name": "mixerlabs/wiki", "stars": 11, "license": "None", "language": "python", "size": 136}
{"docstring": "# Circular dependency\n", "func_signal": "def each_link(self):\n", "code": "from www.wiki.linkgraph import each_link\nfrom www.wiki.render import render_page_to_stream\nreturn each_link(render_page_to_stream(self.current_version))", "path": "models.py", "repo_name": "mixerlabs/wiki", "stars": 11, "license": "None", "language": "python", "size": 136}
{"docstring": "\"\"\"Updates the global link graph for the given (source) page that\nwas just edited\"\"\"\n# Circular dependency\n", "func_signal": "def update_linkgraph_for_page(page):\n", "code": "from . import render\nif not page.current_version:\n    return\n\n# Oy, this is pretty terrible: creoleparser doesn't give us any\n# access to the creole parsetree (nor does it really have one,\n# only kinda), so the best we can do is read the outgoing genshi\n# stream, and look for links there.\nstream = render.render_page_to_stream(page.current_version)\nsrc    = page.get_absolute_url()\ndsts   = set(each_link(stream))\n\n# Now update the link graph so that the current set of destination\n# reflects `dsts'\n\n# Delete old objects, then find only new links, and insert them.\nEdge.objects.filter(src=src).exclude(dst__in=dsts).delete()\n\nold = set(e.dst for e in Edge.objects.filter(src=src))\nnew = dsts - old\nfor dst in new:\n    Edge.objects.create_safe(src=src, dst=dst)", "path": "linkgraph.py", "repo_name": "mixerlabs/wiki", "stars": 11, "license": "None", "language": "python", "size": 136}
{"docstring": "\"\"\"Initialize post-{save,delete} hooks for maintaining the wiki\nlink graph.\"\"\"\n", "func_signal": "def initialize():\n", "code": "post_save.connect(post_save_handler, sender=Page, weak=False)\npost_delete.connect(post_delete_handler, sender=Page, weak=False)", "path": "linkgraph.py", "repo_name": "mixerlabs/wiki", "stars": 11, "license": "None", "language": "python", "size": 136}
{"docstring": "\"\"\"Return a fresh duplicate of this version. Good for creating\nnew versions. Doesn't include edit-specific things like user &\nIP.\"\"\"\n", "func_signal": "def dup(self):\n", "code": "new       = PageVersion(page=self.page)\nnew.title = self.title\nnew.body  = self.body\n\nreturn new", "path": "models.py", "repo_name": "mixerlabs/wiki", "stars": 11, "license": "None", "language": "python", "size": 136}
{"docstring": "\"\"\"Pack the addressed wiki page to `path'\"\"\"\n", "func_signal": "def cmd_pack(self, wiki_slug, page_slug, path):\n", "code": "import conf; conf.configure_django('www.settings')\nfrom www.wiki.models import Page\nfrom www.wiki.pack   import pack\n\npage = Page.objects.get(wiki__slug=wiki_slug, slug=page_slug)\npack(page, path)", "path": "commands.py", "repo_name": "mixerlabs/wiki", "stars": 11, "license": "None", "language": "python", "size": 136}
{"docstring": "\"\"\"Return categories for a wiki. Results are cached for an hour.\nWhen changing categories of pages, on should call \n'refresh_categories' to invalidate the cache.\"\"\"\n", "func_signal": "def get_categories(self):\n", "code": "return cache_key_('wiki_categories', 60*60, lambda: self.slug,\n                  self._get_categories)", "path": "models.py", "repo_name": "mixerlabs/wiki", "stars": 11, "license": "None", "language": "python", "size": 136}
{"docstring": "\"\"\"Iterate over all visible pages in this wiki.\"\"\"\n", "func_signal": "def each_page(self):\n", "code": "for page in self.all_pages:\n    # set page.wiki to potentially save a DB query\n    page.wiki = self\n    yield page", "path": "models.py", "repo_name": "mixerlabs/wiki", "stars": 11, "license": "None", "language": "python", "size": 136}
{"docstring": "\"\"\"The *category* of the wiki, is one of the strings in the\nCATEGORIES setting.\"\"\"\n", "func_signal": "def category_fget(self):\n", "code": "if self._new_category is not None:\n    return self._new_category\nelse:\n    return category_of_object(self)", "path": "models.py", "repo_name": "mixerlabs/wiki", "stars": 11, "license": "None", "language": "python", "size": 136}
{"docstring": "\"\"\" Rather than deleting the page version, mark it as hidden.  If it's\nthe latest version, create a version from lilb that reverts it. \"\"\"\n", "func_signal": "def possum_eat(self):\n", "code": "self.is_hidden_change = True\nself.save()\nif self.page.current_version != self:\n    return\n# Create a followup version as lilb that duplicates the latest\n# non-hidden version, or empty (delete) if they are all hidden\nlast_good_vsn = self\nwhile last_good_vsn and last_good_vsn.is_hidden_change:\n    last_good_vsn = last_good_vsn.previous\nif last_good_vsn:\n    revert_vsn = last_good_vsn.dup()\nelse:\n    revert_vsn = PageVersion(page=self.page, title=self.title, body='')\nrevert_vsn.edit_user = User.objects.get(username='lilb')\nrevert_vsn.is_hidden_change = True\nrevert_vsn.save()", "path": "models.py", "repo_name": "mixerlabs/wiki", "stars": 11, "license": "None", "language": "python", "size": 136}
{"docstring": "# Adding field 'Page.modified_on'\n", "func_signal": "def forwards(self, orm):\n", "code": "        db.add_column('wiki_page', 'modified_on',\n            models.DateTimeField(auto_now_add=True, default='2009-2-1'),\n            keep_default=False)", "path": "migrations\\0002_add_wiki_modified_on.py", "repo_name": "mixerlabs/wiki", "stars": 11, "license": "None", "language": "python", "size": 136}
{"docstring": "\"\"\"An iterator of pages related to `page' for the current link\ngraph.\"\"\"\n# Circular dependency\n", "func_signal": "def each_related_page(page):\n", "code": "from . import views\nfor edge in Edge.objects.filter(dst=page.get_absolute_url()):\n    # Skip external links.\n    if not edge.src or edge.src[0] != '/':\n        continue\n\n    # This is a bit roundabout. We may improve on this by storing\n    # foreignkeys directly into the link graph.\n    try:\n        view, _, d = resolve(edge.src)\n        if view != views.widget:\n            continue\n    except Resolver404:\n        continue\n\n    try:\n        yield Page.objects.get(wiki__slug=d['wiki_slug'], slug=d['slug'])\n    except Page.DoesNotExist:\n        continue", "path": "linkgraph.py", "repo_name": "mixerlabs/wiki", "stars": 11, "license": "None", "language": "python", "size": 136}
{"docstring": "\"\"\"An iterator for parsing out links from the given genshi stream.\"\"\"\n", "func_signal": "def each_link(stream):\n", "code": "for kind, meta, _ in stream:\n    if kind != 'START':\n        continue\n\n    qname, attrs = meta\n    if qname != 'a':\n        continue\n\n    for key, value in attrs:\n        if key != 'href':\n            continue\n\n        yield value\n        break", "path": "linkgraph.py", "repo_name": "mixerlabs/wiki", "stars": 11, "license": "None", "language": "python", "size": 136}
{"docstring": "# Force page save since self references.\n", "func_signal": "def save(self, force_insert=False, force_update=False):\n", "code": "if self.page.pk is None:\n    self.page.save()\n\n# On first save, set previous and version based on newest version\nif self.pk is None:\n    try:\n        self.previous = self.page.pageversion_set.latest('version')\n        self.version  = self.previous.version + 1\n    except PageVersion.DoesNotExist:\n        self.previous = None\n        self.version  = 1\n\nsuper(PageVersion, self).save(force_insert, force_update)\n\n# After save, we update the category, since it needs a saved\n# object (pk) and invalidate the category cache for the wiki.\nif self._new_category is not None:\n    tag = tag_of_category(self._new_category)\n    Tag.objects.update_tags(self, tag)\n    Tag.objects.update_tags(self.page, tag)\n    self.page.wiki.refresh_categories()\n\nself.page.version_post_save(self)\n\n# Send the notifications after any de-normalization\nself.notify_followers_post_save()", "path": "models.py", "repo_name": "mixerlabs/wiki", "stars": 11, "license": "None", "language": "python", "size": 136}
{"docstring": "# show the last five entries\n", "func_signal": "def history(self, request):\n", "code": "history = self.page.pageversion_set.all().order_by('-created_at')\ntry:\n    paginator = Paginator(history, 10).page(request.GET.get('page', 1))\nexcept InvalidPage:\n    raise Http404\n\ncontext = {\n    'page'      : self.page,\n    'paginator' : paginator,\n    'widget'    : self,\n}\n\nreturn render_to_response('wiki/history.html', context, \n                          base_opts={'tab': www.common.tabs.PAGES})", "path": "widget.py", "repo_name": "mixerlabs/wiki", "stars": 11, "license": "None", "language": "python", "size": 136}
{"docstring": "\"\"\"Return a query set for visible pages in this\nwiki. (`page_set' will return *all* pages, we actually want to\nfilter out invisible ones). TODO(Django1.1): supports the\nnotion of an auto manager, and so with it, we can have\n`page_set' do the right thing.\"\"\"\n# Include current_version because that's usually accessed next.\n", "func_signal": "def all_pages(self):\n", "code": "return self.page_set.filter(is_ephemeral=False, is_deleted=False)\\\n    .exclude(current_version=None).select_related('current_version')", "path": "models.py", "repo_name": "mixerlabs/wiki", "stars": 11, "license": "None", "language": "python", "size": 136}
{"docstring": "\"\"\" Preprocess input text as MuteXt does\n\n    When working on words, MuteXt splits an input string\n    on 'words' and removes all non-alphanumeric characters.\n    It is not clear how the MuteXt system splits words. If they\n    split on non-alphanumeric (i.e., '\\b'), they would miss many \n    of the patterns they report to identify (e.g. G-142-A). We \n    therefore simplisticly split on whitespace here.\n\"\"\"\n", "func_signal": "def _preprocess_words(self,raw_text):\n", "code": "return [self._replace_regex.sub('',word)\\\n        for word in raw_text.split()]", "path": "DistAnnot\\mutation_finder.py", "repo_name": "JudoWill/pyMutF", "stars": 9, "license": "None", "language": "python", "size": 27672}
{"docstring": "\"\"\" Perform precision increasing post-processing steps\n\n    Remove false positives indicated by:\n      -> mutant and wild-type residues being identical (e.g. A42A)\n\n\"\"\"\n\n", "func_signal": "def _post_process(self,mutations):\n", "code": "for mutation in mutations.keys():\n    if mutation.WtResidue == mutation.MutResidue:\n        del mutations[mutation]", "path": "DistAnnot\\mutation_finder.py", "repo_name": "JudoWill/pyMutF", "stars": 9, "license": "None", "language": "python", "size": 27672}
{"docstring": "\"\"\" build the sentence-level regex patterns\n\n    These patterns match an xN followed by a mutant residue\n    mention within ten words\n    (e.g. 'we mutated Ser42 to glycine')\n    The wt residue can be a one- or three-letter abbreviation, and the\n    mt residue can be a three-letter abbreviation or full name.\n\"\"\"\n", "func_signal": "def _build_string_regex_patterns(self):\n", "code": "return [\n    r''.join([r'(^|\\s)',self.single_wt_res_match,\\\n              self.position_match,r'\\s(\\w+\\s){,9}',\\\n              self.triple_mut_res_match,r'(\\s|$)']),\\\n    r''.join([r'(^|\\s)',self.single_wt_res_match,\\\n              self.position_match,r'\\s(\\w+\\s){,9}',\\\n              self.full_mut_res_match,r'(\\s|$)']),\\\n    r''.join([r'(^|\\s)',self.triple_wt_res_match,\\\n              self.position_match,r'\\s(\\w+\\s){,9}',\\\n              self.triple_mut_res_match,r'(\\s|$)']),\\\n    r''.join([r'(^|\\s)',self.triple_wt_res_match,\\\n              self.position_match,r'\\s(\\w+\\s){,9}',\\\n              self.full_mut_res_match,r'(\\s|$)'])\n    ]", "path": "DistAnnot\\mutation_finder.py", "repo_name": "JudoWill/pyMutF", "stars": 9, "license": "None", "language": "python", "size": 27672}
{"docstring": "\"\"\" Initialize the object \n\n    regular_expressions: an interative set of regular expressions to\n        be applied for extracting mutations. These are in the \n        default python syntax (i.e., perl regular expressions), with \n        the single exception being that regular expressions which\n        should be performed in a case sensitive manner should be \n        followed by the string '[CASE_SENSITIVE]', with no spaces \n        between it and the regular expression. \n        This can be a list, a file, or any other object which supports\n        iteration. For an example, you should refer to the regex.txt\n        file in the MutationFinder directory.\n\n\"\"\"\n", "func_signal": "def __init__(self,regular_expressions):\n", "code": "MutationExtractor.__init__(self)\nself._regular_expressions = []\n\nfor regular_expression in regular_expressions:\n    if regular_expression.endswith('[CASE_SENSITIVE]'):\n        self._regular_expressions.append(\\\n         compile(regular_expression[:regular_expression.rindex('[')]))\n    else:\n        self._regular_expressions.append(\\\n         compile(regular_expression,IGNORECASE))", "path": "DistAnnot\\mutation_finder.py", "repo_name": "JudoWill/pyMutF", "stars": 9, "license": "None", "language": "python", "size": 27672}
{"docstring": "\"\"\"Must be passed a QuerySet!!!\"\"\"\n", "func_signal": "def render(self, name, value, attrs=None):\n", "code": "output = super(AutoCompleteTagInputLarge, self).render(name, value, attrs)\n\nqset = self.attrs['queryset']\nmapping = self.attrs['mapping']\njson_data = MakeList(mapping, qset, os.path.join(settings.STATIC_FILE_ROOT,\n                                     'data', 'gene_names.json'))\n#json_data = reverse('gene_list')\nreturn output +  MakeString(name, json_data)", "path": "DistAnnot\\Annot\\widgets.py", "repo_name": "JudoWill/pyMutF", "stars": 9, "license": "None", "language": "python", "size": 27672}
{"docstring": "# Adding model 'MutationAnnot'\n", "func_signal": "def forwards(self, orm):\n", "code": "        db.create_table('Annot_mutationannot', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('User', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['auth.User'])),\n            ('Mutation', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['Interaction.Mutation'])),\n        ))\n        db.send_create_signal('Annot', ['MutationAnnot'])\n\n        # Adding model 'InteractionEffectAnnot'\n        db.create_table('Annot_interactioneffectannot', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('User', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['auth.User'])),\n            ('InteractionEffect', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['Interaction.InteractionEffect'])),\n        ))\n        db.send_create_signal('Annot', ['InteractionEffectAnnot'])", "path": "DistAnnot\\Annot\\migrations\\0001_initial.py", "repo_name": "JudoWill/pyMutF", "stars": 9, "license": "None", "language": "python", "size": 27672}
{"docstring": "# Adding field 'Article.PMCXML'\n", "func_signal": "def forwards(self, orm):\n", "code": "        db.add_column('Interaction_article', 'PMCXML', self.gf('django.db.models.fields.XMLField')(default=None, null=True), keep_default=False)\n\n        # Adding field 'Article.PubMedXML'\n        db.add_column('Interaction_article', 'PubMedXML', self.gf('django.db.models.fields.XMLField')(default=None, null=True), keep_default=False)\n\n        # Adding field 'Article.HasMut'\n        db.add_column('Interaction_article', 'HasMut', self.gf('django.db.models.fields.NullBooleanField')(default=None, null=True, blank=True), keep_default=False)", "path": "DistAnnot\\Interaction\\migrations\\0016_auto__add_field_article_PMCXML__add_field_article_PubMedXML__add_field.py", "repo_name": "JudoWill/pyMutF", "stars": 9, "license": "None", "language": "python", "size": 27672}
{"docstring": "\"\"\" Preprocess input text as MuteXt does\n\n    When working on sentences, MuteXt splits on sentence\n    breaks and removes all non-alphanumeric characters.\n\"\"\"\n", "func_signal": "def _preprocess_sentences(self,raw_text):\n", "code": "return [self._replace_regex.sub('',sentence).strip()\\\n        for sentence in raw_text.split('.')]", "path": "DistAnnot\\mutation_finder.py", "repo_name": "JudoWill/pyMutF", "stars": 9, "license": "None", "language": "python", "size": 27672}
{"docstring": "\"\"\" Strip the path off a filepath to get a filename\"\"\"\n", "func_signal": "def filename_from_filepath(filepath):\n", "code": "try:\n    return filepath[filepath.rindex('/')+1:]\nexcept ValueError:\n    return filepath", "path": "DistAnnot\\mutation_finder.py", "repo_name": "JudoWill/pyMutF", "stars": 9, "license": "None", "language": "python", "size": 27672}
{"docstring": "# Adding model 'GeneAnnot'\n", "func_signal": "def forwards(self, orm):\n", "code": "        db.create_table('Annot_geneannot', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('Annotation', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['Interaction.GeneAnnotation'])),\n            ('User', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['auth.User'])),\n        ))\n        db.send_create_signal('Annot', ['GeneAnnot'])", "path": "DistAnnot\\Annot\\migrations\\0003_auto__add_geneannot.py", "repo_name": "JudoWill/pyMutF", "stars": 9, "license": "None", "language": "python", "size": 27672}
{"docstring": "\"\"\" An iterator for extracting mutations from lines\n\n    lines: an iterable object where each item is a tab-delimited\n        string where the first field is a unique identifier and the\n        remaining fields comprise a single text source; most commonly\n        this will be a file or list\n    mutation_extractor: the MutationExtractor object to be used for the\n        extraction process\n\"\"\"\n", "func_signal": "def extract_mutations_from_lines(lines,mutation_extractor):\n", "code": "for line in lines:\n    fields = line.strip().split('\\t')\n    try:\n        # the data before the first tab is the text source identifier\n        identifier = fields[0]\n        # the data after the first tab (including any subsequent tabs)\n        # is the text\n        text = '\\t'.join(fields[1:])\n        yield identifier, \\\n         extract_mutations_from_string(text,mutation_extractor)\n    except IndexError:\n        # Ignore blank lines\n        pass", "path": "DistAnnot\\mutation_finder.py", "repo_name": "JudoWill/pyMutF", "stars": 9, "license": "None", "language": "python", "size": 27672}
{"docstring": "\"\"\" Extract point mutations mentions from raw_text and return them in a dict\n     \n     raw_text: the text from which mutations should be extracted\n\n     IT IS NOT POSSIBLE TO STORE SPANS WHEN EXTRACTING MUTATIONS WITH\n      BaselineMutationExtractor. Because MuteXt splits on sentences and\n      words, and removes alphanumeric characters from within words, the\n      mappings to original character-offsets get complicated. \n     MutationFinder does, however, return spans.\n\n\"\"\"\n", "func_signal": "def __call__(self,raw_text):\n", "code": "result = {}\n\n# Apply patterns which work on the word level and attempt to\n# find xNy matches\nfor regex in self._word_regexs:\n    for word in self._preprocess_words(raw_text):\n        for m in regex.finditer(word):\n            current_mutation = \\\n              PointMutation(m.group('pos'),m.group('wt_res'),\\\n                        m.group('mut_res'))\n\n            try:\n                result[current_mutation] += 1\n            except KeyError:\n                result[current_mutation] = 1\n\n# Apply patterns which work on the sentence level and attempt\n# to find a mutant residue up to ten words ahead of a xN match\nfor regex in self._string_regexs:\n    for sentence in self._preprocess_sentences(raw_text):\n        for m in regex.finditer(sentence):\n            current_mutation = \\\n              PointMutation(m.group('pos'),m.group('wt_res'),\\\n                        m.group('mut_res'))\n\n            try:\n                result[current_mutation] += 1\n            except KeyError:\n                result[current_mutation] = 1\nreturn result", "path": "DistAnnot\\mutation_finder.py", "repo_name": "JudoWill/pyMutF", "stars": 9, "license": "None", "language": "python", "size": 27672}
{"docstring": "# Deleting model 'MutationAnnot'\n", "func_signal": "def backwards(self, orm):\n", "code": "        db.delete_table('Annot_mutationannot')\n\n        # Deleting model 'InteractionEffectAnnot'\n        db.delete_table('Annot_interactioneffectannot')", "path": "DistAnnot\\Annot\\migrations\\0001_initial.py", "repo_name": "JudoWill/pyMutF", "stars": 9, "license": "None", "language": "python", "size": 27672}
{"docstring": "\"\"\" Initialize the object \"\"\"\n", "func_signal": "def __init__(self):\n", "code": "MutationExtractor.__init__(self)\nword_regex_patterns = self._build_word_regex_patterns()\nstring_regex_patterns = self._build_string_regex_patterns()\n\nself._word_regexs = []\nself._string_regexs = []\nself._replace_regex = compile('[^a-zA-Z0-9\\s]')\n\n# Compile the regular expressions\nfor regex_pattern in word_regex_patterns:\n    self._word_regexs.append(compile(regex_pattern))\nfor regex_pattern in string_regex_patterns:\n    self._string_regexs.append(compile(regex_pattern))", "path": "DistAnnot\\mutation_finder.py", "repo_name": "JudoWill/pyMutF", "stars": 9, "license": "None", "language": "python", "size": 27672}
{"docstring": "# Adding M2M table for field Interactions on 'Sentence'\n", "func_signal": "def backwards(self, orm):\n", "code": "        db.create_table('Interaction_sentence_Interactions', (\n            ('id', models.AutoField(verbose_name='ID', primary_key=True, auto_created=True)),\n            ('sentence', models.ForeignKey(orm['Interaction.sentence'], null=False)),\n            ('interaction', models.ForeignKey(orm['Interaction.interaction'], null=False))\n        ))\n        db.create_unique('Interaction_sentence_Interactions', ['sentence_id', 'interaction_id'])", "path": "DistAnnot\\Interaction\\migrations\\0019_auto.py", "repo_name": "JudoWill/pyMutF", "stars": 9, "license": "None", "language": "python", "size": 27672}
{"docstring": "\"\"\" Initalize the object and call the base class init \n\n    Position: the sequence position or start position of the mutation\n        (castable to an int)\n    WtResidue: the wild-type (pre-mutation) residue identity (a string)\n    MutReside: the mutant (post-mutation) residue identity (a string)\n\n    Residues identities are validated to ensure that they are within \n     the canonical set of amino acid residues are normalized to their\n     one-letter abbreviations.\n\"\"\"\n", "func_signal": "def __init__(self,Position,WtResidue,MutResidue):\n", "code": "self.__wt_residue = self._normalize_residue_identity(WtResidue)\nself.__mut_residue = self._normalize_residue_identity(MutResidue)\nMutation.__init__(self,Position=Position)", "path": "DistAnnot\\mutation_finder.py", "repo_name": "JudoWill/pyMutF", "stars": 9, "license": "None", "language": "python", "size": 27672}
{"docstring": "\"\"\" Extract point mutations mentions from raw_text and return them in a dict\n     \n     raw_text: a string of text\n\n    The result of this method is a dict mapping PointMutation objects to\n     a list of spans where they were identified. Spans are presented in the     \n     form of character-offsets in text. If counts for each mention are \n     required instead of spans, apply len() to each value to convert the \n     list of spans to a count. \n\n    Example result:\n     raw_text: 'We constructed A42G and L22G, and crystalized A42G.'\n     result = {PointMutation(42,'A','G'):[(15,19),(46,50)], \n               PointMutation(22,'L','G'):[(24,28)]}\n\n     Note that the spans won't necessarily be in increasing order, due \n      to the order of processing regular expressions.\n\n\n\"\"\"\n", "func_signal": "def __call__(self,raw_text):\n", "code": "result = {}\nfor regular_expression in self._regular_expressions:\n    for m in regular_expression.finditer(raw_text):\n        current_mutation = \\\n          PointMutation(m.group('pos'),m.group('wt_res'),\\\n                    m.group('mut_res'))\n        # The span of the mutation is calcluated as the min\n        # start span of the three components and the max end span\n        # of the three components -- these are then packed up as \n        # a tuple.\n        span = min(m.span('wt_res')[0],\\\n                   m.span('pos')[0],\\\n                   m.span('mut_res')[0]),\\\n               max(m.span('wt_res')[1],\\\n                   m.span('pos')[1],\\\n                   m.span('mut_res')[1])\n        try:\n            result[current_mutation].append(span) \n        except KeyError:\n            result[current_mutation] = [span]\n\nself._post_process(result)\nreturn result", "path": "DistAnnot\\mutation_finder.py", "repo_name": "JudoWill/pyMutF", "stars": 9, "license": "None", "language": "python", "size": 27672}
{"docstring": "\"\"\" Build the word-level reqex patterns\n\n    These patterns match wNm format mutations using either \n        one-letter abbreviations OR three-letter abbreviations, but\n        not a mixture. \n        (e.g. A42G and Ala42Gly will match, but not A42Gly)\n\n\"\"\"\n", "func_signal": "def _build_word_regex_patterns(self):\n", "code": "return [\n    r''.join(['^',self.single_wt_res_match,\\\n              self.position_match,\\\n              self.single_mut_res_match,'$']),\\\n    r''.join(['^',self.triple_wt_res_match,\\\n              self.position_match,\\\n              self.triple_mut_res_match,'$']),\\\n    ]", "path": "DistAnnot\\mutation_finder.py", "repo_name": "JudoWill/pyMutF", "stars": 9, "license": "None", "language": "python", "size": 27672}
{"docstring": "# Deleting field 'Article.PMCXML'\n", "func_signal": "def backwards(self, orm):\n", "code": "        db.delete_column('Interaction_article', 'PMCXML')\n\n        # Deleting field 'Article.PubMedXML'\n        db.delete_column('Interaction_article', 'PubMedXML')\n\n        # Deleting field 'Article.HasMut'\n        db.delete_column('Interaction_article', 'HasMut')", "path": "DistAnnot\\Interaction\\migrations\\0016_auto__add_field_article_PMCXML__add_field_article_PubMedXML__add_field.py", "repo_name": "JudoWill/pyMutF", "stars": 9, "license": "None", "language": "python", "size": 27672}
{"docstring": "\"\"\"Must be passed a QuerySet!!!\"\"\"\n", "func_signal": "def render(self, name, value, attrs=None):\n", "code": "output = super(AutoCompleteTagInput, self).render(name, value, attrs)\n\nqset = self.attrs['queryset']\nmapping = self.attrs['mapping']\njson_data = MakeList(mapping, qset, os.path.join(settings.STATIC_FILE_ROOT,\n                                     'data', 'gene_names.json'))\n#json_data = reverse('gene_list')\nreturn output +  MakeString(name, json_data)", "path": "DistAnnot\\Annot\\widgets.py", "repo_name": "JudoWill/pyMutF", "stars": 9, "license": "None", "language": "python", "size": 27672}
{"docstring": "'''\nI should be called every once in a while when there is significant\nthread churn.\n'''\n", "func_signal": "def _cleanup(self):\n", "code": "with self._lock:\n    clean = set(self._responses) - set(t.ident for t in threading.enumerate())\n    for tid in clean:\n        del self._responses[tid]\nreturn len(clean)", "path": "embedded.py", "repo_name": "josiahcarlson/yogatable", "stars": 11, "license": "other", "language": "python", "size": 164}
{"docstring": "\"\"\"Begins serving a GET request\"\"\"\n\n# Check for query string in URL\n", "func_signal": "def do_GET(self):\n", "code": "qspos = self.path.find('?')\nif qspos>=0:\n    self.body = cgi.parse_qs(self.path[qspos+1:], keep_blank_values=1)\n    self.path = self.path[:qspos]\n\nself.handle_data()", "path": "thirdparty\\recipe_440665_1.py", "repo_name": "josiahcarlson/yogatable", "stars": 11, "license": "other", "language": "python", "size": 164}
{"docstring": "\"\"\"Collect the data arriving on the connection\"\"\"\n", "func_signal": "def collect_incoming_data(self,data):\n", "code": "if not data:\n    self.ac_in_buffer = \"\"\n    return\nself.incoming.append(data)", "path": "thirdparty\\recipe_440665_1.py", "repo_name": "josiahcarlson/yogatable", "stars": 11, "license": "other", "language": "python", "size": 164}
{"docstring": "'''\nShutdown all processors when they've completed all currently\noutstanding queries.\n'''\n", "func_signal": "def shutdown_when_done(self, wait=True):\n", "code": "self._shutting_down = True\nsent = set()\nwhile self._processors:\n    with self._lock:\n        known = set(self._processors)\n        to_kill = known - sent\n        for name in to_kill:\n            x = self._outgoing_queues.get(name, None)\n            if x:\n                x.put((None, None, '_quit', (), {}))\n        sent = known\n    if not wait:\n        break\n    time.sleep(.001)", "path": "embedded.py", "repo_name": "josiahcarlson/yogatable", "stars": 11, "license": "other", "language": "python", "size": 164}
{"docstring": "'''\nStart up a queue_processor process for each table.\n'''\n", "func_signal": "def _get_or_setup_command_queue(self, table_name):\n", "code": "if self._shutting_down:\n    return\nwith self._lock:\n    if table_name not in self._outgoing_queues:\n        self._outgoing_queues[table_name] = multiprocessing.Queue()\n    if table_name not in self._processors or not self._processors[table_name].is_alive():\n        self._processors[table_name] = p = multiprocessing.Process(\n            target=processor.queue_processor,\n            args=(self._config.table_config(table_name),\n                  table_name,\n                  self._outgoing_queues[table_name],\n                  self._incoming_responses))\n        p.daemon = True\n        p.start()\nreturn self._outgoing_queues[table_name]", "path": "embedded.py", "repo_name": "josiahcarlson/yogatable", "stars": 11, "license": "other", "language": "python", "size": 164}
{"docstring": "'''\nExecute the provided command on the given table named table_name.\n\nThis function ensures that the table queue_processor() is running, that\nthere exists a queue for this thread to wait on, and that the thread that\nroutes responses is running, and finally, it waits for the response\nitself.\n'''\n# set up all of the necessary processors/queues\n", "func_signal": "def _execute(self, table_name, operation, args, kwargs):\n", "code": "outgoing = self._get_or_setup_command_queue(table_name)\nincoming = self._get_or_setup_response()\nself._setup_response_router_if_necessary()\n# get a counter so that we know which command is being executed\nif not hasattr(self._local, 'counter'):\n    self._local.counter = 0\nself._local.counter += 1\n# get the processing started\noutgoing.put((whoami(), self._local.counter, operation, args, kwargs))\nwhile 1:\n    # wait for the response\n    me, id, response = incoming.get()\n    if id != self._local.counter:\n        # ignore responses not directed at me\n        continue\n    break\n\nexceptions.check_response(response)\nassert response['response'] == 'ok'\nreturn response['value']", "path": "embedded.py", "repo_name": "josiahcarlson/yogatable", "stars": 11, "license": "other", "language": "python", "size": 164}
{"docstring": "'''\nWe want to generate monotonically increasing time-based sequences, as\nclose to reality as possible, modulo system precision, and someone\nchanging the clock on the system.\n\nIEEE 754 FP doubles can only support precision of .238 microseconds with\nthe current magnitude of unix time, which gets us around 4 million rows\nper second... hopefully that's enough.\n\nWe use this sequence as a method of defining insertion/update order for\nthe rows in our data table, so that indexing operations can merely walk an\nindex over the time column to discover those rows that need to be indexed.\n\nWe'll be returning these values as integers to bypass float precision\nissues during packing/unpacking.\n'''\n", "func_signal": "def _time_seq(_lt=[time.time(), 0], CAN_USE_CLOCK=CAN_USE_CLOCK):\n", "code": "shift = float(2**22)\nif CAN_USE_CLOCK:\n    # This will get us the best possible real time resolution.  Roughly\n    # 1.2 microseconds resolution on a 2.4 ghz core 2 duo.\n    now = time.time()\n    clk = time.clock()\n    while 1:\n        yield int(shift * (now + time.clock() - clk))\nelse:\n    # This will give us up to 4096 insertions per millisecond, which\n    # should be sufficient for platforms with a shoddy time.clock().\n    # This also ends up being the resolution of IEEE 754 FP doubles with\n    # the current magnitude of unix time.\n    lt = _lt[0]\n    i = _lt[1]\n    while 1:\n        t = time.time()\n        if t != lt:\n            i = 0\n            lt = _lt[0] = t\n        yield int(shift * t + i)\n        i += 1\n        _lt[1] = i", "path": "lib\\om.py", "repo_name": "josiahcarlson/yogatable", "stars": 11, "license": "other", "language": "python", "size": 164}
{"docstring": "'''\nThis method is run as a thread, and routes the responses from the\nqueue_processor() processes to the requesting thread's queue.\n'''\n", "func_signal": "def _route_responses(self):\n", "code": "passes = 0\nEmpty = Queue.Empty\nwhile (not self._shutting_down) or self._processors:\n    passes += 1\n    if not passes % (self._config.THREAD_CLEANUP_RATE * (len(self._responses) or 1)):\n        # handle thread cleanup every once in a while\n        self._cleanup()\n    # get a response\n    try:\n        response = self._incoming_responses.get(timeout=1)\n    except Empty:\n        continue\n    # find it's destination\n    rqueue = self._responses.get(response[0])\n    if rqueue:\n        # We'll only bother to route messages to destinations that\n        # currently exist.\n        rqueue.put(response)\n    elif response[0] == response[1] == None:\n        response = response[2]\n        resp = response.get('response')\n        table_name = response['table_name']\n        if resp == 'indexes':\n            # handle index updates\n            self._known_indexes[table_name] = response['value']\n        elif resp == 'quit':\n            self._processors.pop(table_name)\nself._response_router = None", "path": "embedded.py", "repo_name": "josiahcarlson/yogatable", "stars": 11, "license": "other", "language": "python", "size": 164}
{"docstring": "\"\"\"Called when a POST request body has been read\"\"\"\n", "func_signal": "def handle_post_data(self):\n", "code": "self.rfile = cStringIO.StringIO(''.join(popall(self.incoming)))\nself.rfile.seek(0)\nself.do_POST()", "path": "thirdparty\\recipe_440665_1.py", "repo_name": "josiahcarlson/yogatable", "stars": 11, "license": "other", "language": "python", "size": 164}
{"docstring": "\"\"\"Class to override\"\"\"\n", "func_signal": "def handle_data(self):\n", "code": "if self.use_favicon and self.path == '/favicon.ico':\n    self.send_response(200)\n    self.send_header(\"Content-type\", 'text/html')\n    self.send_header(\"Content-Length\", len(favicon))\n    self.end_headers()\n    self.log_request(self.code, len(favicon))\n    self.outgoing.append(favicon)\n    self.outgoing.append(None)\n    return\n\nf = self.send_head()\nif f:\n    # do some special things with file objects so that we don't have\n    # to read them all into memory at the same time...may leave a\n    # file handle open for longer than is really desired, but it does\n    # make it able to handle files of unlimited size.\n    try:\n        size = os.fstat(f.fileno())[6]\n    except AttributeError:\n        size = f.len\n    self.update_b(size)\n    self.log_request(self.code, size)\n    self.outgoing.append(f)\nelse:\n    self.log_request(self.code)\n\n# signal the end of this request\nself.outgoing.append(None)", "path": "thirdparty\\recipe_440665_1.py", "repo_name": "josiahcarlson/yogatable", "stars": 11, "license": "other", "language": "python", "size": 164}
{"docstring": "\"\"\"Begins serving a POST request. The request data must be readable\non a file-like object called self.rfile\"\"\"\n", "func_signal": "def do_POST(self):\n", "code": "ctype, pdict = cgi.parse_header(self.headers.getheader('content-type'))\nlength = int(self.headers.getheader('content-length'))\nif ctype == 'multipart/form-data':\n    self.body = cgi.parse_multipart(self.rfile, pdict)\nelif ctype == 'application/x-www-form-urlencoded':\n    qs = self.rfile.read(length)\n    self.body = cgi.parse_qs(qs, keep_blank_values=1)\nelse:\n    self.body = {}\n#self.handle_post_body()\nself.handle_data()", "path": "thirdparty\\recipe_440665_1.py", "repo_name": "josiahcarlson/yogatable", "stars": 11, "license": "other", "language": "python", "size": 164}
{"docstring": "\"\"\"Send the blank line ending the MIME headers, send the buffered\nresponse and headers on the connection\"\"\"\n", "func_signal": "def end_headers(self):\n", "code": "if self.request_version != 'HTTP/0.9':\n    self.outgoing.append(\"\\r\\n\")", "path": "thirdparty\\recipe_440665_1.py", "repo_name": "josiahcarlson/yogatable", "stars": 11, "license": "other", "language": "python", "size": 164}
{"docstring": "'''\nKill all processor subprocesses.\n'''\n", "func_signal": "def shutdown_with_kill(self):\n", "code": "self._shutting_down = True\nwhile self._processors:\n    with self._lock:\n        for n,p in self._processors.items():\n            p.terminate()\n            if not p.is_alive():\n                del self._processors[n]\n    time.sleep(.001)", "path": "embedded.py", "repo_name": "josiahcarlson/yogatable", "stars": 11, "license": "other", "language": "python", "size": 164}
{"docstring": "# Bind parm list to corresponding args, or single parm to list of args\n", "func_signal": "def __init__(self, parms=(), args=(), outer=None):\n", "code": "self.outer = outer\nif isa(parms, Symbol):\n    self.update({parms:list(args)})\nelse:\n    if len(args) != len(parms):\n        raise TypeError('expected %r, given %r, '\n                        % (parms, args))\n    self.update(zip(parms,args))", "path": "thirdparty\\lispy.py", "repo_name": "josiahcarlson/yogatable", "stars": 11, "license": "other", "language": "python", "size": 164}
{"docstring": "'''\nRather than having a bunch of threads waiting on passed queues, we'll\ngo ahead and create a designated listening thread that will handle the\nrouting to the proper result queue.\n'''\n", "func_signal": "def _setup_response_router_if_necessary(self):\n", "code": "if self._shutting_down:\n    return\nwith self._lock:\n    if self._response_router is None:\n        self._response_router = threading.Thread(target=self._route_responses)\n        self._response_router.setDaemon(1)\n        self._response_router.start()", "path": "embedded.py", "repo_name": "josiahcarlson/yogatable", "stars": 11, "license": "other", "language": "python", "size": 164}
{"docstring": "\"\"\"Prepare to read the request body\"\"\"\n", "func_signal": "def prepare_POST(self):\n", "code": "bytesToRead = int(self.headers.getheader('content-length'))\n# set terminator to length (will read bytesToRead bytes)\nself.set_terminator(bytesToRead)\nself.incoming.clear()\n# control will be passed to a new found_terminator\nself.found_terminator = self.handle_post_data", "path": "thirdparty\\recipe_440665_1.py", "repo_name": "josiahcarlson/yogatable", "stars": 11, "license": "other", "language": "python", "size": 164}
{"docstring": "'''\nCreate a queue for every thread.  Workloads with large thread churn\nshould occasionally call the _cleanup() function above.\n'''\n", "func_signal": "def _get_or_setup_response(self):\n", "code": "if self._shutting_down:\n    return\ntid = whoami()\nwith self._lock:\n    if tid not in self._responses:\n        self._responses[tid] = Queue.Queue()\nreturn self._responses[tid]", "path": "embedded.py", "repo_name": "josiahcarlson/yogatable", "stars": 11, "license": "other", "language": "python", "size": 164}
{"docstring": "\"\"\"Called when the http request line and headers have been received\"\"\"\n# prepare attributes needed in parse_request()\n", "func_signal": "def handle_request_line(self):\n", "code": "self.rfile = cStringIO.StringIO(''.join(popall(self.incoming)))\nself.rfile.seek(0)\nself.raw_requestline = self.rfile.readline()\nself.parse_request()\n\nif self.command in ['GET','HEAD']:\n    # if method is GET or HEAD, call do_GET or do_HEAD and finish\n    method = \"do_\"+self.command\n    if hasattr(self,method):\n        getattr(self,method)()\nelif self.command==\"POST\":\n    # if method is POST, call prepare_POST, don't finish yet\n    self.prepare_POST()\nelse:\n    self.send_error(501, \"Unsupported method (%s)\" %self.command)", "path": "thirdparty\\recipe_440665_1.py", "repo_name": "josiahcarlson/yogatable", "stars": 11, "license": "other", "language": "python", "size": 164}
{"docstring": "# some of this is borrowed from test_table.py\n", "func_signal": "def test_missing_indexes(self):\n", "code": "self.db.test.add_index('col1', 'col2', '-col3', 'col4')\n# drop non-existing index\nself.db.test.drop_index('col1', 'col2')\n# drop existing index\nself.db.test.drop_index('col1', 'col2', '-col3', 'col4')\n# verify that a number cannot be a prefix to a column\nself.assertRaises(exceptions.ColumnException, lambda:self.db.test.add_index('col1', '5col'))\nself.assertRaises(exceptions.ColumnException, lambda:self.db.test.add_index('col1', '+5col'))\n# verify that you cannot create an empty index\nself.assertRaises(exceptions.IndexWarning, lambda:self.db.test.add_index())", "path": "tests\\test_embedded.py", "repo_name": "josiahcarlson/yogatable", "stars": 11, "license": "other", "language": "python", "size": 164}
{"docstring": "#Preallocate the list to save memory resizing.\n", "func_signal": "def popall(deq):\n", "code": "r = len(deq)*[None]\nfor i in xrange(len(r)):\n    r[i] = deq.popleft()\nreturn r", "path": "thirdparty\\recipe_440665_1.py", "repo_name": "josiahcarlson/yogatable", "stars": 11, "license": "other", "language": "python", "size": 164}
{"docstring": "\"\"\"run the dialog, and if reload was pressed run synaptic\"\"\"\n", "func_signal": "def run(self):\n", "code": "res = self.dialog.run()\nself.dialog.hide()\nif res == gtk.RESPONSE_YES:\n    self.parent.set_sensitive(False)\n    lock = thread.allocate_lock()\n    lock.acquire()\n    t = thread.start_new_thread(self.update_cache,\n                               (self.parent.window.xid, lock))\n    while lock.locked():\n        while gtk.events_pending():\n            gtk.main_iteration()\n            time.sleep(0.05)\n    self.parent.set_sensitive(True)\nreturn res", "path": "src\\thirdsoft.py", "repo_name": "tualatrix/ubuntu-tweak-old", "stars": 9, "license": "gpl-2.0", "language": "python", "size": 2604}
{"docstring": "# FIXME: If the module hasn't load yet! \n", "func_signal": "def on_child_page_update(self, widget, module, action):\n", "code": "if module in self.modules:\n    getattr(self.modules[module], action)()", "path": "src\\mainwindow.py", "repo_name": "tualatrix/ubuntu-tweak-old", "stars": 9, "license": "gpl-2.0", "language": "python", "size": 2604}
{"docstring": "'''The model is icon, title and the list reference'''\n", "func_signal": "def __create_model(self):\n", "code": "model = gtk.ListStore(\n            gtk.gdk.Pixbuf,\n            gobject.TYPE_STRING,\n            gobject.TYPE_STRING)\n\nreturn model", "path": "src\\filetype.py", "repo_name": "tualatrix/ubuntu-tweak-old", "stars": 9, "license": "gpl-2.0", "language": "python", "size": 2604}
{"docstring": "\"\"\"Open the file with gedit\"\"\"\n", "func_signal": "def open(self, source):\n", "code": "if source[-1] == \"root\":\n    cmd = [\"gksu\", \"-m\", _(\"Enter your password to perform the administrative tasks\") , \"gedit\"]\n    cmd.extend(source[:-1])\n    subprocess.call(cmd)\nelse:\n    cmd = [\"gedit\"]\n    cmd.extend(source)\n    subprocess.call(cmd)", "path": "src\\ScriptWorker.py", "repo_name": "tualatrix/ubuntu-tweak-old", "stars": 9, "license": "gpl-2.0", "language": "python", "size": 2604}
{"docstring": "# TODO: need support Mint and other distro based on Ubuntu.\n", "func_signal": "def parse_codename():\n", "code": "if GnomeVersion.distributor == 'Ubuntu':\n    data = open('/etc/lsb-release').read()\n    dict = {}\n    for line in data.split('\\n'):\n        try:\n            key, value = line.split('=')\n            dict[key] = value\n        except:\n            pass\n    return dict['DISTRIB_CODENAME']\nreturn None", "path": "src\\common\\systeminfo.py", "repo_name": "tualatrix/ubuntu-tweak-old", "stars": 9, "license": "gpl-2.0", "language": "python", "size": 2604}
{"docstring": "# TODO: need support Mint and other distro based on Ubuntu.\n", "func_signal": "def parse_distro():\n", "code": "if GnomeVersion.distributor == 'Ubuntu':\n    return file('/etc/issue.net').readline()[:-1]\nreturn GnomeVersion.distributor", "path": "src\\common\\systeminfo.py", "repo_name": "tualatrix/ubuntu-tweak-old", "stars": 9, "license": "gpl-2.0", "language": "python", "size": 2604}
{"docstring": "'''Prepare widget'''\n", "func_signal": "def __init__ (self, parent = None, key = 0, mods = 0, label = None):\n", "code": "super (KeyGrabber, self).__init__ ()\n\nself.main_window = parent\nself.key = key\nself.mods = mods\n\nself.label = label\n\nself.connect (\"clicked\", self.begin_key_grab)\nself.set_label ()", "path": "src\\common\\widgets\\widgets.py", "repo_name": "tualatrix/ubuntu-tweak-old", "stars": 9, "license": "gpl-2.0", "language": "python", "size": 2604}
{"docstring": "# FIXME: Need to combin with page update\n", "func_signal": "def on_child_page_call(self, widget, target, action, params):\n", "code": "if target == 'mainwindow':\n    getattr(self, action)(**params)", "path": "src\\mainwindow.py", "repo_name": "tualatrix/ubuntu-tweak-old", "stars": 9, "license": "gpl-2.0", "language": "python", "size": 2604}
{"docstring": "\"\"\"start synaptic to update the package cache\"\"\"\n", "func_signal": "def update_cache(self, window_id, lock):\n", "code": "try:\n    apt_pkg.PkgSystemUnLock()\nexcept SystemError:\n    pass\ncmd = []\nif os.getuid() != 0:\n    cmd = ['/usr/bin/gksu',\n           '--desktop', '/usr/share/applications/synaptic.desktop',\n           '--']\n\ncmd += ['/usr/sbin/synaptic', '--hide-main-window',\n       '--non-interactive',\n       '--parent-window-id', '%s' % (window_id),\n       '--update-at-startup']\nsubprocess.call(cmd)\nlock.release()", "path": "src\\thirdsoft.py", "repo_name": "tualatrix/ubuntu-tweak-old", "stars": 9, "license": "gpl-2.0", "language": "python", "size": 2604}
{"docstring": "\"\"\"\nCreate the notebook with welcome page.\nthe remain page will be created when request.\n\"\"\"\n", "func_signal": "def create_notebook(self):\n", "code": "notebook = gtk.Notebook()\nnotebook.set_scrollable(True)\nnotebook.set_show_tabs(False)\n\npage = MODULES[WELCOME][MODULE_FUNC]\nnotebook.append_page(page())\nnotebook.append_page(Wait())\n\nreturn notebook", "path": "src\\mainwindow.py", "repo_name": "tualatrix/ubuntu-tweak-old", "stars": 9, "license": "gpl-2.0", "language": "python", "size": 2604}
{"docstring": "'''The model is icon, title and the list reference'''\n", "func_signal": "def __create_model(self):\n", "code": "model = gtk.ListStore(\n            gobject.TYPE_STRING,\n            gtk.gdk.Pixbuf,\n            gobject.TYPE_STRING,\n            gtk.gdk.Pixbuf,\n            gobject.TYPE_STRING,\n            )\n\nreturn model", "path": "src\\filetype.py", "repo_name": "tualatrix/ubuntu-tweak-old", "stars": 9, "license": "gpl-2.0", "language": "python", "size": 2604}
{"docstring": "\"\"\"If the previous setting is none, then select the add edge\"\"\"\n", "func_signal": "def combo_box_changed_cb(self, widget, edge):\n", "code": "if widget.previous:\n    self.change_edge(widget, edge)\nelse:\n    self.add_edge(widget, edge)", "path": "src\\compiz.py", "repo_name": "tualatrix/ubuntu-tweak-old", "stars": 9, "license": "gpl-2.0", "language": "python", "size": 2604}
{"docstring": "\"\"\"Move the file or folder with necessary notice\"\"\"\n", "func_signal": "def move(self, source, dest):\n", "code": "dest = os.path.join(dest, os.path.basename(source))\nself.do_move(source, dest)", "path": "src\\ScriptWorker.py", "repo_name": "tualatrix/ubuntu-tweak-old", "stars": 9, "license": "gpl-2.0", "language": "python", "size": 2604}
{"docstring": "\"\"\"Link the file or folder with necessary notice\"\"\"\n", "func_signal": "def link(self, source, dest):\n", "code": "dest = os.path.join(dest, os.path.basename(source))\nself.do_link(source, dest)", "path": "src\\ScriptWorker.py", "repo_name": "tualatrix/ubuntu-tweak-old", "stars": 9, "license": "gpl-2.0", "language": "python", "size": 2604}
{"docstring": "\"\"\"\nFormats the value like a 'human-readable' file size (i.e. 13 KB, 4.1 MB,\n102 bytes, etc).\n\"\"\"\n", "func_signal": "def filesizeformat(bytes):\n", "code": "try:\n    bytes = float(bytes)\nexcept TypeError:\n    return \"0 bytes\"\n\nif bytes < 1024:\n    return ngettext(\"%(size)d byte\", \"%(size)d bytes\", bytes) % {'size': bytes}\nif bytes < 1024 * 1024:\n    return _(\"%.1f KB\") % (bytes / 1024)\nif bytes < 1024 * 1024 * 1024:\n    return _(\"%.1f MB\") % (bytes / (1024 * 1024))\nreturn _(\"%.1f GB\") % (bytes / (1024 * 1024 * 1024))", "path": "src\\common\\misc.py", "repo_name": "tualatrix/ubuntu-tweak-old", "stars": 9, "license": "gpl-2.0", "language": "python", "size": 2604}
{"docstring": "\"\"\"Copy the file or folder with necessary notice\"\"\"\n", "func_signal": "def copy(self, source, dest):\n", "code": "dest = os.path.join(dest, os.path.basename(source))\nself.do_copy(source, dest)", "path": "src\\ScriptWorker.py", "repo_name": "tualatrix/ubuntu-tweak-old", "stars": 9, "license": "gpl-2.0", "language": "python", "size": 2604}
{"docstring": "'''If the source is coming from internal, then move it, or copy it.'''\n", "func_signal": "def on_drag_data_received(self, treeview, context, x, y, selection, info, etime):\n", "code": "source = selection.data\n\nif source:\n    try:\n        path, position = treeview.get_dest_row_at_pos(x, y)\n        iter = self.model.get_iter(path)\n    except:\n        try:\n            iter = self.model.get_iter_first()\n        except:\n            iter = self.model.append(None)\n\n    target = self.model.get_value(iter, DIR_PATH)\n\n    if context.get_source_widget() is self:\n        file_action = 'move'\n        dir_action = 'move'\n    else:\n        file_action = 'copy'\n        dir_action = 'copytree'\n\n    if '\\r\\n' in source:\n        file_list = source.split('\\r\\n')\n        for file in file_list:\n            if file:\n                self.file_operate(file, dir_action, file_action, target)\n    else:\n        self.file_operate(source, dir_action, file_action, target)\n\n    self.update_model()\n    context.finish(True, False)\nelse:\n    context.finish(False, False)", "path": "src\\common\\widgets\\treeviews.py", "repo_name": "tualatrix/ubuntu-tweak-old", "stars": 9, "license": "gpl-2.0", "language": "python", "size": 2604}
{"docstring": "\"\"\"Browser the folder as root\"\"\"\n", "func_signal": "def browse(self, source):\n", "code": "if source[-1] == \"root\":\n    cmd = [\"gksu\", \"-m\", _(\"Enter your password to perform the administrative tasks\") , \"nautilus\"]\n    cmd.extend(source[:-1])\n    subprocess.call(cmd)\nelse:\n    cmd = [\"nautilus\"]\n    cmd.extend(source)\n    subprocess.call(cmd)", "path": "src\\ScriptWorker.py", "repo_name": "tualatrix/ubuntu-tweak-old", "stars": 9, "license": "gpl-2.0", "language": "python", "size": 2604}
{"docstring": "'''if init is true, force to update, or it will update only once'''\n", "func_signal": "def update_apt_cache(init = False):\n", "code": "global cache\n\nif init:\n    try:\n        cache\n    except NameError:\n        pass\n    else:\n        return\n\napt_pkg.init()\ncache = apt.Cache(apt.progress.OpTextProgress())", "path": "src\\common\\package.py", "repo_name": "tualatrix/ubuntu-tweak-old", "stars": 9, "license": "gpl-2.0", "language": "python", "size": 2604}
{"docstring": "# for performance reasons\n", "func_signal": "def parse(self, filename):\n", "code": "content = self.content\n\nif not os.path.isfile(filename):\n    return\n\n# parse file\ntry:\n    file(filename, 'r')\nexcept IOError:\n    return\n\nfor line in file(filename,'r'):\n    line = line.strip()\n    # empty line\n    if not line:\n        continue\n    # comment\n    elif line[0] == '#':\n        continue\n    # key\n    else:\n        index = line.find(\"=\")\n        key = line[0:index].strip()\n        value = line[index+1:].strip()\n        if self.hasKey(key):\n            continue\n        else:\n            content[key] = value\n\nself.filename = filename", "path": "src\\common\\inifile.py", "repo_name": "tualatrix/ubuntu-tweak-old", "stars": 9, "license": "gpl-2.0", "language": "python", "size": 2604}
{"docstring": "\"\"\"\nTree walking helper function.\n\"\"\"\n", "func_signal": "def __walk_tree(self, handlers, ast, code):\n", "code": "for k, v in ast:\n    if handlers.has_key(k):\n        if k == 'right_paren':\n            code = \"%s)\" % (\n                code\n            )\n        elif k == 'left_paren':\n            code = \"%s(\" % (\n                code\n            )\n        elif k == 'comment':\n            \n            # Comments in Twig have {# not {*\n            code = \"%s{#%s#}\" % (\n                code,\n                v[2:len(v) - 2]\n            )\n        else:\n            code = handlers[k](v, code)\n        \nreturn code", "path": "smartytotwig\\tree_walker.py", "repo_name": "freshrichard/smartytotwig", "stars": 13, "license": "None", "language": "python", "size": 109}
{"docstring": "\"\"\"\nA modifier statement:\n\nfoo|bar:a:b:c\n\"\"\"\n        \n# Walking the expression that starts a\n# modifier statement.\n", "func_signal": "def modifier(self, ast, code):\n", "code": "code = self.__walk_tree (\n    {\n        'symbol': self.symbol,\n        'array': self.array,\n        'string': self.string,\n        'variable_string': self.variable_string,\n        'modifier_right': self.modifier_right\n    },\n    ast,\n    code\n)\n\nreturn code", "path": "smartytotwig\\tree_walker.py", "repo_name": "freshrichard/smartytotwig", "stars": 13, "license": "None", "language": "python", "size": 109}
{"docstring": "\"\"\"\nTest several different types of if statements.\n\"\"\"\n# Test an if statement (no else or elseif)\n", "func_signal": "def test_if_statement(self):\n", "code": "ast = smartytotwig.parse_string(\"{if !foo or foo.bar or foo|bar:foo['hello']}\\nfoo\\n{/if}\")\ntree_walker = TreeWalker(ast)\nself.assertEqual(tree_walker.code, \"{% if not foo or foo.bar or foo|bar(foo['hello']) %}\\nfoo\\n{% endif %}\")\n\n# Test an an if with an else and a single logical operation.\nast = smartytotwig.parse_string(\"{if foo}\\nbar\\n{else}\\nfoo{/if}\")\ntree_walker = TreeWalker(ast)\nself.assertEqual(tree_walker.code, \"{% if foo %}\\nbar\\n{% else %}\\nfoo{% endif %}\")\n\n# Test an an if with an else and an elseif and two logical operations.\nast = smartytotwig.parse_string(\"{if foo and awesome.string|banana:\\\"foo\\\\\\\" $a\\\"}\\nbar\\n{elseif awesome.sauce[1] and blue and 'hello'}\\nfoo{/if}\")\ntree_walker = TreeWalker(ast)\nself.assertEqual(tree_walker.code, \"{% if foo and awesome.string|banana(\\\"foo\\\\\\\" %s\\\"|format(a)) %}\\nbar\\n{% elseif awesome.sauce[1] and blue and \\'hello\\' %}\\nfoo{% endif %}\")\n\n# Test an if with an elseif and else clause.\nast = smartytotwig.parse_string(\"{if foo|bar:3 or !foo[3]}\\nbar\\n{elseif awesome.sauce[1] and blue and 'hello'}\\nfoo\\n{else}bar{/if}\")\ntree_walker = TreeWalker(ast)\nself.assertEqual(tree_walker.code, \"{% if foo|bar(3) or not foo[3] %}\\nbar\\n{% elseif awesome.sauce[1] and blue and 'hello' %}\\nfoo\\n{% else %}bar{% endif %}\")\n\n# Test an an if statement with parenthesis.\nast = smartytotwig.parse_string(\"{if (foo and bar) or foo and (bar or (foo and bar))}\\nbar\\n{else}\\nfoo{/if}\")\ntree_walker = TreeWalker(ast)\nself.assertEqual(tree_walker.code, \"{% if (foo and bar) or foo and (bar or (foo and bar)) %}\\nbar\\n{% else %}\\nfoo{% endif %}\")\n\n# Test an an elseif statement with parenthesis.\nast = smartytotwig.parse_string(\"{if foo}\\nbar\\n{elseif (foo and bar) or foo and (bar or (foo and bar))}\\nfoo{/if}\")\ntree_walker = TreeWalker(ast)\nself.assertEqual(tree_walker.code, \"{% if foo %}\\nbar\\n{% elseif (foo and bar) or foo and (bar or (foo and bar)) %}\\nfoo{% endif %}\")", "path": "tests\\test_smarty_grammar.py", "repo_name": "freshrichard/smartytotwig", "stars": 13, "license": "None", "language": "python", "size": 109}
{"docstring": "\"\"\"\neq, == opeartor in Smarty.\n\"\"\"\n", "func_signal": "def equals_operator(self, ast, code):\n", "code": "code = '%s == ' % (\n    code\n)\n\nreturn code", "path": "smartytotwig\\tree_walker.py", "repo_name": "freshrichard/smartytotwig", "stars": 13, "license": "None", "language": "python", "size": 109}
{"docstring": "\"\"\"\n&&, and operator in Smarty.\n\"\"\"\n", "func_signal": "def and_operator(self, ast, code):\n", "code": "code = '%s and ' % (\n    code\n)\n\nreturn code", "path": "smartytotwig\\tree_walker.py", "repo_name": "freshrichard/smartytotwig", "stars": 13, "license": "None", "language": "python", "size": 109}
{"docstring": "\"\"\"\nTest Smarty's function? statement:\n\n{foo arg=bar}\n\"\"\"\n# Test a a simple function statement.\n", "func_signal": "def test_function_statement(self):\n", "code": "ast = smartytotwig.parse_string(\"{foo arg1=bar arg2=3}\")\ntree_walker = TreeWalker(ast)\nself.assertEqual(tree_walker.code, \"{{['arg1': bar, 'arg2': 3]|foo}}\")\n\n# Test a a simple function statement with object and array arguments.\nast = smartytotwig.parse_string(\"{foo arg1=bar[1] arg2=foo.bar.foo arg3=foo.bar[3] arg4=foo.bar.awesome[3] }\")\ntree_walker = TreeWalker(ast)\nself.assertEqual(tree_walker.code, \"{{['arg1': bar[1], 'arg2': foo.bar.foo, 'arg3': foo.bar[3], 'arg4': foo.bar.awesome[3]]|foo}}\")\n\n# Test a function statement with modifiers in in the parameters.\nast = smartytotwig.parse_string(\"{foo arg1=bar[1]|modifier arg2=foo.bar.foo arg3=foo.bar[3]|modifier:array[0]:\\\"hello $foo \\\" arg4=foo.bar.awesome[3]|modifier2:7:'hello':\\\"hello\\\":\\\"`$apple.banana`\\\"}\")\ntree_walker = TreeWalker(ast)\nself.assertEqual(tree_walker.code, \"{{[\\'arg1\\': bar[1]|modifier, \\'arg2\\': foo.bar.foo, \\'arg3\\': foo.bar[3]|modifier(array[0], \\\"hello %s \\\"|format(foo)), \\'arg4\\': foo.bar.awesome[3]|modifier2(7, \\'hello\\', \\\"hello\\\", \\\"%s\\\"|format(apple.banana))]|foo}}\")", "path": "tests\\test_smarty_grammar.py", "repo_name": "freshrichard/smartytotwig", "stars": 13, "license": "None", "language": "python", "size": 109}
{"docstring": "\"\"\"\n< operator in smarty.\n\"\"\"\n", "func_signal": "def lt_operator(self, ast, code):\n", "code": "code = '%s < ' % (\n    code\n)\n\nreturn code", "path": "smartytotwig\\tree_walker.py", "repo_name": "freshrichard/smartytotwig", "stars": 13, "license": "None", "language": "python", "size": 109}
{"docstring": "\"\"\"\nA literal block in smarty, we can just\ndrop the {literal} tags because Twig\nis less ambiguous.\n\"\"\"\n", "func_signal": "def literal(self, ast, code):\n", "code": "literal_string = ast\nliteral_string = literal_string.replace('{/literal}', '')\nliteral_string = literal_string.replace('{literal}', '')\n\ncode = \"%s%s\" % (\n    code,\n    literal_string\n)\n\nreturn code", "path": "smartytotwig\\tree_walker.py", "repo_name": "freshrichard/smartytotwig", "stars": 13, "license": "None", "language": "python", "size": 109}
{"docstring": "\"\"\"\nA symbol (variable) in Smarty, e.g,\n\n!foobar\nfoo_bar\nfoo3\n\"\"\"\n\n# Assume no $ on the symbol.\n", "func_signal": "def symbol(self, ast, code):\n", "code": "variable = ast[0]\n\n# Is there a ! operator.\nif len(ast[0]) > 0:\n    if ast[0][0] == 'not_operator':\n        code = \"%snot \" % (\n            code\n        )\n    elif ast[0][0] == 'at_operator':\n      pass # Nom nom, at operators are not supported in Twig\n\n# Maybe there was a $ on the symbol?.\nif len(ast) > 1:\n    variable = ast[len(ast) - 1]\n    \ncode = \"%s%s\" % (code, variable)    \n\nreturn code", "path": "smartytotwig\\tree_walker.py", "repo_name": "freshrichard/smartytotwig", "stars": 13, "license": "None", "language": "python", "size": 109}
{"docstring": "\"\"\"\nThe right-hand side of the modifier\nstatement:\n\nbar:a:b:c\n\"\"\"\n", "func_signal": "def modifier_right(self, ast, code):\n", "code": "code = \"%s|\" % code\n        \ncode = self.__walk_tree (\n    {\n        'symbol': self.symbol,\n        'string': self.string,\n        'variable_string': self.variable_string\n    },\n    ast,\n    code\n)\n\n# We must have parameters being passed\n# in to the modifier.\nif len(ast) > 1:\n    code = \"%s(\" % code\n    i = 0\n    for k, v in ast[1:]:\n        code = self.expression(v, code)\n        \n        # Put commas in if needed.\n        i += 1\n        if not i == len(ast) - 1:\n            code = \"%s, \" % code\n            \n    code = \"%s)\" % code\n\nreturn code", "path": "smartytotwig\\tree_walker.py", "repo_name": "freshrichard/smartytotwig", "stars": 13, "license": "None", "language": "python", "size": 109}
{"docstring": "\"\"\"\n> operator in smarty.\n\"\"\"\n", "func_signal": "def gt_operator(self, ast, code):\n", "code": "code = '%s > ' % (\n    code\n)\n\nreturn code", "path": "smartytotwig\\tree_walker.py", "repo_name": "freshrichard/smartytotwig", "stars": 13, "license": "None", "language": "python", "size": 109}
{"docstring": "\"\"\"\n>= operator in Smarty.\n\"\"\"\n", "func_signal": "def gte_operator(self, ast, code):\n", "code": "code = '%s >= ' % (\n    code\n)\n\nreturn code", "path": "smartytotwig\\tree_walker.py", "repo_name": "freshrichard/smartytotwig", "stars": 13, "license": "None", "language": "python", "size": 109}
{"docstring": "\"\"\"\nRaw content, e.g.,\n\n<html>\n    <body>\n        <b>Hey</b>\n    </body>\n</html>\n\"\"\"\n", "func_signal": "def content(self, ast, code):\n", "code": "code = \"%s%s\" % (\n    code,\n    ast\n)\n\nreturn code", "path": "smartytotwig\\tree_walker.py", "repo_name": "freshrichard/smartytotwig", "stars": 13, "license": "None", "language": "python", "size": 109}
{"docstring": "\"\"\"\nA top level expression in Smarty that statements\nare built out of mostly expressions and/or symbols (which are\nencompassed in the expression type.\n\"\"\"\n# Evaluate the different types of expressions.\n", "func_signal": "def expression(self, ast, code):\n", "code": "expression = self.__walk_tree (\n    {\n        'symbol': self.symbol,\n        'string': self.string,\n        'variable_string': self.variable_string,\n        'object_dereference': self.object_dereference,\n        'array': self.array,\n        'modifier': self.modifier\n    },\n    ast,\n    \"\"\n)\n\n# Should we perform any replacements?\nfor k, v in self.replacements.items():\n    if re.match(k, expression):\n        expression = v\n        break\n            \ncode = \"%s%s\" % (\n    code,\n    expression\n)\n\nreturn code", "path": "smartytotwig\\tree_walker.py", "repo_name": "freshrichard/smartytotwig", "stars": 13, "license": "None", "language": "python", "size": 109}
{"docstring": "\"\"\"\nThe entry-point for the parser.\ncontains a set of top-level smarty\nstatements.\n\"\"\"\n\n", "func_signal": "def smarty_language(self, ast, code):\n", "code": "code = self.__walk_tree (\n    {\n        'if_statement': self.if_statement,\n        'content': self.content,\n        'print_statement': self.print_statement,\n        'for_statement': self.for_statement,\n        'function_statement': self.function_statement,\n        'comment': self.function_statement,\n        'literal': self.literal\n    },\n    ast,\n    code\n)\n    \nreturn code", "path": "smartytotwig\\tree_walker.py", "repo_name": "freshrichard/smartytotwig", "stars": 13, "license": "None", "language": "python", "size": 109}
{"docstring": "\"\"\"\nOperators in smarty.\n\"\"\"\n\n# Evaluate the different types of expressions.\n", "func_signal": "def operator(self, ast, code):\n", "code": "code = self.__walk_tree (\n    {\n        'and_operator': self.and_operator,\n        'equals_operator': self.equals_operator,\n        'gte_operator': self.gte_operator,\n        'lte_operator': self.lte_operator,\n        'lt_operator': self.lt_operator,\n        'gt_operator': self.gt_operator,\n        'ne_operator': self.ne_operator,\n        'or_operator': self.or_operator\n    },\n    ast,\n    code\n)\n    \nreturn code", "path": "smartytotwig\\tree_walker.py", "repo_name": "freshrichard/smartytotwig", "stars": 13, "license": "None", "language": "python", "size": 109}
{"docstring": "\"\"\"\nTest Smarty's for statement:\n\n{foreach foo}\n{foreachelse}\n{/foreach}\n\"\"\"\n# Test a a simple foreach statement.\n", "func_signal": "def test_for_statement(self):\n", "code": "ast = smartytotwig.parse_string(\"{foreach item=bar from=foo }{/foreach}\")\ntree_walker = TreeWalker(ast)\nself.assertEqual(tree_walker.code, \"{% for bar in foo %}{% endfor %}\")\n\n# Test a more complex foreach statement.\nast = smartytotwig.parse_string(\"{foreach item='bar'    name=snuh key=\\\"foobar\\\" from=foo[5].bar[2]|hello:\\\"world\\\":\\\" $hey \\\" }bar{/foreach}\")\ntree_walker = TreeWalker(ast)\nself.assertEqual(tree_walker.code, \"{% for bar in foo[5].bar[2]|hello(\\\"world\\\", \\\" %s \\\"|format(hey)) %}bar{% endfor %}\")\n\n# Test a for statement with a foreachelse clause.\nast = smartytotwig.parse_string(\"{foreach item='bar'    name=snuh key=\\\"foobar\\\" from=foo.bar[2]|hello:\\\"world\\\":\\\" $hey \\\" }bar{foreachelse}{if !foo}bar{/if}hello{/foreach}\")\ntree_walker = TreeWalker(ast)\nself.assertEqual(tree_walker.code, \"{% for bar in foo.bar[2]|hello(\\\"world\\\", \\\" %s \\\"|format(hey)) %}bar{% else %}{% if not foo %}bar{% endif %}hello{% endfor %}\")", "path": "tests\\test_smarty_grammar.py", "repo_name": "freshrichard/smartytotwig", "stars": 13, "license": "None", "language": "python", "size": 109}
{"docstring": "\"\"\"\n||, or, operator in Smarty.\n\"\"\"\n", "func_signal": "def or_operator(self, ast, code):\n", "code": "code = '%s or ' % (\n    code\n)\n\nreturn code", "path": "smartytotwig\\tree_walker.py", "repo_name": "freshrichard/smartytotwig", "stars": 13, "license": "None", "language": "python", "size": 109}
{"docstring": "\"\"\"\nThe else part of an if statement.\n\"\"\"\n     \n", "func_signal": "def else_statement(self, ast, code):\n", "code": "code = \"%s{%s else %s}\" % (\n    code,\n    '%',\n    '%'\n)\n\n# The content inside the if statement.\ncode = self.__walk_tree (\n    {\n        'smarty_language': self.smarty_language,\n    },\n    ast,\n    code\n)\nreturn code", "path": "smartytotwig\\tree_walker.py", "repo_name": "freshrichard/smartytotwig", "stars": 13, "license": "None", "language": "python", "size": 109}
{"docstring": "\"\"\"\nA print statement in smarty includes:\n\n{foo}\n{foo|bar:parameter}\n\"\"\"\n                \n# Walking the expression that starts a\n# modifier statement.\n", "func_signal": "def print_statement(self, ast, code):\n", "code": "expression = self.__walk_tree (\n    {\n        'expression': self.expression,\n    },\n    ast,\n    \"\"\n)\n\n# Perform any keyword replacements if found.        \nif self.keywords.has_key(expression):\n    return \"%s%s\" % (\n        code,\n        self.keywords[expression]\n    )\n\ncode = \"%s{{%s}}\" % (\n    code,\n    expression\n)\n        \nreturn code", "path": "smartytotwig\\tree_walker.py", "repo_name": "freshrichard/smartytotwig", "stars": 13, "license": "None", "language": "python", "size": 109}
{"docstring": "\"\"\"Match the wildcard: in a symbol\"\"\"\n\n", "func_signal": "def matchAny(self, input):\n", "code": "self._state.errorRecovery = False\nself.input.consume()", "path": "orderlyjson\\antlr3\\recognizers.py", "repo_name": "kroo/py-orderly-json", "stars": 14, "license": "None", "language": "python", "size": 3530}
{"docstring": "\"\"\"Set the char stream and reset the lexer\"\"\"\n", "func_signal": "def setCharStream(self, input):\n", "code": "self.input = None\nself.reset()\nself.input = input", "path": "orderlyjson\\antlr3\\recognizers.py", "repo_name": "kroo/py-orderly-json", "stars": 14, "license": "None", "language": "python", "size": 3530}
{"docstring": "\"\"\"\nWhat error message should be generated for the various\nexception types?\n\nNot very object-oriented code, but I like having all error message\ngeneration within one method rather than spread among all of the\nexception classes. This also makes it much easier for the exception\nhandling because the exception classes do not have to have pointers back\nto this object to access utility routines and so on. Also, changing\nthe message for an exception type would be difficult because you\nwould have to subclassing exception, but then somehow get ANTLR\nto make those kinds of exception objects instead of the default.\nThis looks weird, but trust me--it makes the most sense in terms\nof flexibility.\n\nFor grammar debugging, you will want to override this to add\nmore information such as the stack frame with\ngetRuleInvocationStack(e, this.getClass().getName()) and,\nfor no viable alts, the decision description and state etc...\n\nOverride this to change the message generated for one or more\nexception types.\n\"\"\"\n\n", "func_signal": "def getErrorMessage(self, e, tokenNames):\n", "code": "if isinstance(e, UnwantedTokenException):\n    tokenName = \"<unknown>\"\n    if e.expecting == EOF:\n        tokenName = \"EOF\"\n\n    else:\n        tokenName = self.tokenNames[e.expecting]\n\n    msg = \"extraneous input %s expecting %s\" % (\n        self.getTokenErrorDisplay(e.getUnexpectedToken()),\n        tokenName\n        )\n\nelif isinstance(e, MissingTokenException):\n    tokenName = \"<unknown>\"\n    if e.expecting == EOF:\n        tokenName = \"EOF\"\n\n    else:\n        tokenName = self.tokenNames[e.expecting]\n\n    msg = \"missing %s at %s\" % (\n        tokenName, self.getTokenErrorDisplay(e.token)\n        )\n\nelif isinstance(e, MismatchedTokenException):\n    tokenName = \"<unknown>\"\n    if e.expecting == EOF:\n        tokenName = \"EOF\"\n    else:\n        tokenName = self.tokenNames[e.expecting]\n\n    msg = \"mismatched input \" \\\n          + self.getTokenErrorDisplay(e.token) \\\n          + \" expecting \" \\\n          + tokenName\n\nelif isinstance(e, MismatchedTreeNodeException):\n    tokenName = \"<unknown>\"\n    if e.expecting == EOF:\n        tokenName = \"EOF\"\n    else:\n        tokenName = self.tokenNames[e.expecting]\n\n    msg = \"mismatched tree node: %s expecting %s\" \\\n          % (e.node, tokenName)\n\nelif isinstance(e, NoViableAltException):\n    msg = \"no viable alternative at input \" \\\n          + self.getTokenErrorDisplay(e.token)\n\nelif isinstance(e, EarlyExitException):\n    msg = \"required (...)+ loop did not match anything at input \" \\\n          + self.getTokenErrorDisplay(e.token)\n\nelif isinstance(e, MismatchedSetException):\n    msg = \"mismatched input \" \\\n          + self.getTokenErrorDisplay(e.token) \\\n          + \" expecting set \" \\\n          + repr(e.expecting)\n\nelif isinstance(e, MismatchedNotSetException):\n    msg = \"mismatched input \" \\\n          + self.getTokenErrorDisplay(e.token) \\\n          + \" expecting set \" \\\n          + repr(e.expecting)\n\nelif isinstance(e, FailedPredicateException):\n    msg = \"rule \" \\\n          + e.ruleName \\\n          + \" failed predicate: {\" \\\n          + e.predicateText \\\n          + \"}?\"\n\nelse:\n    msg = str(e)\n\nreturn msg", "path": "orderlyjson\\antlr3\\recognizers.py", "repo_name": "kroo/py-orderly-json", "stars": 14, "license": "None", "language": "python", "size": 3530}
{"docstring": "\"\"\"Report a recognition problem.\n\nThis method sets errorRecovery to indicate the parser is recovering\nnot parsing.  Once in recovery mode, no errors are generated.\nTo get out of recovery mode, the parser must successfully match\na token (after a resync).  So it will go:\n\n1. error occurs\n2. enter recovery mode, report error\n3. consume until token found in resynch set\n4. try to resume parsing\n5. next match() will reset errorRecovery mode\n\nIf you override, make sure to update syntaxErrors if you care about\nthat.\n\n\"\"\"\n\n# if we've already reported an error and have not matched a token\n# yet successfully, don't report any errors.\n", "func_signal": "def reportError(self, e):\n", "code": "if self._state.errorRecovery:\n    return\n\nself._state.syntaxErrors += 1 # don't count spurious\nself._state.errorRecovery = True\n\nself.displayRecognitionError(self.tokenNames, e)", "path": "orderlyjson\\antlr3\\recognizers.py", "repo_name": "kroo/py-orderly-json", "stars": 14, "license": "None", "language": "python", "size": 3530}
{"docstring": "\"\"\"\nConsume tokens until one matches the given token or token set\n\ntokenTypes can be a single token type or a set of token types\n\n\"\"\"\n\n", "func_signal": "def consumeUntil(self, input, tokenTypes):\n", "code": "if not isinstance(tokenTypes, (set, frozenset)):\n    tokenTypes = frozenset([tokenTypes])\n\nttype = input.LA(1)\nwhile ttype != EOF and ttype not in tokenTypes:\n    input.consume()\n    ttype = input.LA(1)", "path": "orderlyjson\\antlr3\\recognizers.py", "repo_name": "kroo/py-orderly-json", "stars": 14, "license": "None", "language": "python", "size": 3530}
{"docstring": "\"\"\"\nGet the ith token from the current position 1..n where k=1 is the\nfirst symbol of lookahead.\n\"\"\"\n\n", "func_signal": "def LT(self, k):\n", "code": "if self.p == -1:\n    self.fillBuffer()\n\nif k == 0:\n    return None\n\nif k < 0:\n    return self.LB(-k)\n        \ni = self.p\nn = 1\n# find k good tokens\nwhile n < k:\n    # skip off-channel tokens\n    i = self.skipOffTokenChannels(i+1) # leave p on valid token\n    n += 1\n\ntry:\n    return self.tokens[i]\nexcept IndexError:\n    return EOF_TOKEN", "path": "orderlyjson\\antlr3\\streams.py", "repo_name": "kroo/py-orderly-json", "stars": 14, "license": "None", "language": "python", "size": 3530}
{"docstring": "\"\"\"validates the json string with an orderly definition\"\"\"\n", "func_signal": "def validate(json_object, orderly_object):\n", "code": "if type(orderly_object) is str:\n  orderly_object = parse(orderly_object)\njsonschema.validate(json_object, orderly_object)", "path": "orderlyjson\\__init__.py", "repo_name": "kroo/py-orderly-json", "stars": 14, "license": "None", "language": "python", "size": 3530}
{"docstring": "\"\"\"\nReset the stream so that it's in the same state it was\nwhen the object was created *except* the data array is not\ntouched.\n\"\"\"\n\n", "func_signal": "def reset(self):\n", "code": "self.p = 0\nself.line = 1\nself.charPositionInLine = 0\nself._markers = [ ]", "path": "orderlyjson\\antlr3\\streams.py", "repo_name": "kroo/py-orderly-json", "stars": 14, "license": "None", "language": "python", "size": 3530}
{"docstring": "\"\"\"\nGiven a starting index, return the index of the first on-channel\ntoken.\n\"\"\"\n\n", "func_signal": "def skipOffTokenChannels(self, i):\n", "code": "try:\n    while self.tokens[i].channel != self.channel:\n        i += 1\nexcept IndexError:\n    # hit the end of token stream\n    pass\n\nreturn i", "path": "orderlyjson\\antlr3\\streams.py", "repo_name": "kroo/py-orderly-json", "stars": 14, "license": "None", "language": "python", "size": 3530}
{"docstring": "\"\"\"Reset this token stream by setting its token source.\"\"\"\n\n", "func_signal": "def setTokenSource(self, tokenSource):\n", "code": "self.tokenSource = tokenSource\nself.tokens = []\nself.p = -1\nself.channel = DEFAULT_CHANNEL", "path": "orderlyjson\\antlr3\\streams.py", "repo_name": "kroo/py-orderly-json", "stars": 14, "license": "None", "language": "python", "size": 3530}
{"docstring": "\"\"\"\nRecover from an error found on the input stream.  This is\nfor NoViableAlt and mismatched symbol exceptions.  If you enable\nsingle token insertion and deletion, this will usually not\nhandle mismatched symbol exceptions but there could be a mismatched\ntoken that the match() routine could not recover from.\n\"\"\"\n\n# PROBLEM? what if input stream is not the same as last time\n# perhaps make lastErrorIndex a member of input\n", "func_signal": "def recover(self, input, re):\n", "code": "if self._state.lastErrorIndex == input.index():\n    # uh oh, another error at same token index; must be a case\n    # where LT(1) is in the recovery token set so nothing is\n    # consumed; consume a single token so at least to prevent\n    # an infinite loop; this is a failsafe.\n    input.consume()\n\nself._state.lastErrorIndex = input.index()\nfollowSet = self.computeErrorRecoverySet()\n\nself.beginResync()\nself.consumeUntil(input, followSet)\nself.endResync()", "path": "orderlyjson\\antlr3\\recognizers.py", "repo_name": "kroo/py-orderly-json", "stars": 14, "license": "None", "language": "python", "size": 3530}
{"docstring": "\"\"\"Look backwards k tokens on-channel tokens\"\"\"\n\n", "func_signal": "def LB(self, k):\n", "code": "if self.p == -1:\n    self.fillBuffer()\n\nif k == 0:\n    return None\n\nif self.p - k < 0:\n    return None\n\ni = self.p\nn = 1\n# find k good tokens looking backwards\nwhile n <= k:\n    # skip off-channel tokens\n    i = self.skipOffTokenChannelsReverse(i-1) # leave p on valid token\n    n += 1\n\nif i < 0:\n    return None\n    \nreturn self.tokens[i]", "path": "orderlyjson\\antlr3\\streams.py", "repo_name": "kroo/py-orderly-json", "stars": 14, "license": "None", "language": "python", "size": 3530}
{"docstring": "\"\"\"Return next token or raise StopIteration.\n\nNote that this will raise StopIteration when hitting the EOF token,\nso EOF will not be part of the iteration.\n\n\"\"\"\n\n", "func_signal": "def next(self):\n", "code": "token = self.nextToken()\nif token is None or token.type == EOF:\n    raise StopIteration\nreturn token", "path": "orderlyjson\\antlr3\\recognizers.py", "repo_name": "kroo/py-orderly-json", "stars": 14, "license": "None", "language": "python", "size": 3530}
{"docstring": "\"\"\"Attempt to recover from a single missing or extra token.\n\nEXTRA TOKEN\n\nLA(1) is not what we are looking for.  If LA(2) has the right token,\nhowever, then assume LA(1) is some extra spurious token.  Delete it\nand LA(2) as if we were doing a normal match(), which advances the\ninput.\n\nMISSING TOKEN\n\nIf current token is consistent with what could come after\nttype then it is ok to 'insert' the missing token, else throw\nexception For example, Input 'i=(3;' is clearly missing the\n')'.  When the parser returns from the nested call to expr, it\nwill have call chain:\n\n  stat -> expr -> atom\n\nand it will be trying to match the ')' at this point in the\nderivation:\n\n     => ID '=' '(' INT ')' ('+' atom)* ';'\n                        ^\nmatch() will see that ';' doesn't match ')' and report a\nmismatched token error.  To recover, it sees that LA(1)==';'\nis in the set of tokens that can follow the ')' token\nreference in rule atom.  It can assume that you forgot the ')'.\n\"\"\"\n\n", "func_signal": "def recoverFromMismatchedToken(self, input, ttype, follow):\n", "code": "e = None\n\n# if next token is what we are looking for then \"delete\" this token\nif self.mismatchIsUnwantedToken(input, ttype):\n    e = UnwantedTokenException(ttype, input)\n\n    self.beginResync()\n    input.consume() # simply delete extra token\n    self.endResync()\n\n    # report after consuming so AW sees the token in the exception\n    self.reportError(e)\n\n    # we want to return the token we're actually matching\n    matchedSymbol = self.getCurrentInputSymbol(input)\n\n    # move past ttype token as if all were ok\n    input.consume()\n    return matchedSymbol\n\n# can't recover with single token deletion, try insertion\nif self.mismatchIsMissingToken(input, follow):\n    inserted = self.getMissingSymbol(input, e, ttype, follow)\n    e = MissingTokenException(ttype, input, inserted)\n\n    # report after inserting so AW sees the token in the exception\n    self.reportError(e)\n    return inserted\n\n# even that didn't work; must throw the exception\ne = MismatchedTokenException(ttype, input)\nraise e", "path": "orderlyjson\\antlr3\\recognizers.py", "repo_name": "kroo/py-orderly-json", "stars": 14, "license": "None", "language": "python", "size": 3530}
{"docstring": "\"\"\"Not currently used\"\"\"\n\n", "func_signal": "def recoverFromMismatchedSet(self, input, e, follow):\n", "code": "if self.mismatchIsMissingToken(input, follow):\n    self.reportError(e)\n    # we don't know how to conjure up a token for sets yet\n    return self.getMissingSymbol(input, e, INVALID_TOKEN_TYPE, follow)\n\n# TODO do single token deletion like above for Token mismatch\nraise e", "path": "orderlyjson\\antlr3\\recognizers.py", "repo_name": "kroo/py-orderly-json", "stars": 14, "license": "None", "language": "python", "size": 3530}
{"docstring": "\"\"\"\nHas this rule already parsed input at the current index in the\ninput stream?  Return the stop token index or MEMO_RULE_UNKNOWN.\nIf we attempted but failed to parse properly before, return\nMEMO_RULE_FAILED.\n\nThis method has a side-effect: if we have seen this input for\nthis rule and successfully parsed before, then seek ahead to\n1 past the stop token matched for this rule last time.\n\"\"\"\n\n", "func_signal": "def alreadyParsedRule(self, input, ruleIndex):\n", "code": "stopIndex = self.getRuleMemoization(ruleIndex, input.index())\nif stopIndex == self.MEMO_RULE_UNKNOWN:\n    return False\n\nif stopIndex == self.MEMO_RULE_FAILED:\n    raise BacktrackingFailed\n\nelse:\n    input.seek(stopIndex + 1)\n\nreturn True", "path": "orderlyjson\\antlr3\\recognizers.py", "repo_name": "kroo/py-orderly-json", "stars": 14, "license": "None", "language": "python", "size": 3530}
{"docstring": "\"\"\"\nRollback the instruction stream for a program so that\nthe indicated instruction (via instructionIndex) is no\nlonger in the stream.  UNTESTED!\n\"\"\"\n\n", "func_signal": "def rollback(self, *args):\n", "code": "if len(args) == 2:\n    programName = args[0]\n    instructionIndex = args[1]\nelif len(args) == 1:\n    programName = self.DEFAULT_PROGRAM_NAME\n    instructionIndex = args[0]\nelse:\n    raise TypeError(\"Invalid arguments\")\n\np = self.programs.get(programName, None)\nif p is not None:\n    self.programs[programName] = (\n        p[self.MIN_TOKEN_INDEX:instructionIndex])", "path": "orderlyjson\\antlr3\\streams.py", "repo_name": "kroo/py-orderly-json", "stars": 14, "license": "None", "language": "python", "size": 3530}
{"docstring": "\"\"\"\nA more general version of getRuleInvocationStack where you can\npass in, for example, a RecognitionException to get it's rule\nstack trace.  This routine is shared with all recognizers, hence,\nstatic.\n\nTODO: move to a utility class or something; weird having lexer call\nthis\n\"\"\"\n\n# mmmhhh,... perhaps look at the first argument\n# (f_locals[co_varnames[0]]?) and test if it's a (sub)class of\n# requested recognizer...\n\n", "func_signal": "def _getRuleInvocationStack(cls, module):\n", "code": "rules = []\nfor frame in reversed(inspect.stack()):\n    code = frame[0].f_code\n    codeMod = inspect.getmodule(code)\n    if codeMod is None:\n        continue\n\n    # skip frames not in requested module\n    if codeMod.__name__ != module:\n        continue\n\n    # skip some unwanted names\n    if code.co_name in ('nextToken', '<module>'):\n        continue\n\n    rules.append(code.co_name)\n\nreturn rules", "path": "orderlyjson\\antlr3\\recognizers.py", "repo_name": "kroo/py-orderly-json", "stars": 14, "license": "None", "language": "python", "size": 3530}
{"docstring": "\"\"\"\nThe standard method called to automatically emit a token at the\noutermost lexical rule.  The token object should point into the\nchar buffer start..stop.  If there is a text override in 'text',\nuse that to set the token's text.  Override this method to emit\ncustom Token objects.\n\nIf you are building trees, then you should also override\nParser or TreeParser.getMissingSymbol().\n\"\"\"\n\n", "func_signal": "def emit(self, token=None):\n", "code": "if token is None:\n    token = CommonToken(\n        input=self.input,\n        type=self._state.type,\n        channel=self._state.channel,\n        start=self._state.tokenStartCharIndex,\n        stop=self.getCharIndex()-1\n        )\n    token.line = self._state.tokenStartLine\n    token.text = self._state.text\n    token.charPositionInLine = self._state.tokenStartCharPositionInLine\n\nself._state.token = token\n\nreturn token", "path": "orderlyjson\\antlr3\\recognizers.py", "repo_name": "kroo/py-orderly-json", "stars": 14, "license": "None", "language": "python", "size": 3530}
{"docstring": "# Track the set of token types that can follow any rule invocation.\n# Stack grows upwards.\n", "func_signal": "def __init__(self):\n", "code": "self.following = []\n\n# This is true when we see an error and before having successfully\n# matched a token.  Prevents generation of more than one error message\n# per error.\nself.errorRecovery = False\n\n# The index into the input stream where the last error occurred.\n# This is used to prevent infinite loops where an error is found\n# but no token is consumed during recovery...another error is found,\n# ad naseum.  This is a failsafe mechanism to guarantee that at least\n# one token/tree node is consumed for two errors.\nself.lastErrorIndex = -1\n\n# If 0, no backtracking is going on.  Safe to exec actions etc...\n# If >0 then it's the level of backtracking.\nself.backtracking = 0\n\n# An array[size num rules] of Map<Integer,Integer> that tracks\n# the stop token index for each rule.  ruleMemo[ruleIndex] is\n# the memoization table for ruleIndex.  For key ruleStartIndex, you\n# get back the stop token for associated rule or MEMO_RULE_FAILED.\n#\n# This is only used if rule memoization is on (which it is by default).\nself.ruleMemo = None\n\n## Did the recognizer encounter a syntax error?  Track how many.\nself.syntaxErrors = 0\n\n\n# LEXER FIELDS (must be in same state object to avoid casting\n# constantly in generated code and Lexer object) :(", "path": "orderlyjson\\antlr3\\recognizers.py", "repo_name": "kroo/py-orderly-json", "stars": 14, "license": "None", "language": "python", "size": 3530}
{"docstring": "\"\"\"Query for an attribute description\n\"\"\"\n", "func_signal": "def get(self, name, default=None):\n", "code": "try:\n    attrs = self._v_attrs\nexcept AttributeError:\n    attrs = self._v_attrs = {}\nattr = attrs.get(name)\nif attr is None:\n    for iface in self.__iro__:\n        attr = iface.direct(name)\n        if attr is not None:\n            attrs[name] = attr\n            break\n\nif attr is None:\n    return default\nelse:\n    return attr", "path": "zope\\interface\\interface.py", "repo_name": "straup/gae-termextractor", "stars": 9, "license": "None", "language": "python", "size": 676}
{"docstring": "\"\"\"Attaches a tagged value to an interface at definition time.\"\"\"\n", "func_signal": "def taggedValue(key, value):\n", "code": "f_locals = sys._getframe(1).f_locals\ntagged_values = f_locals.setdefault(TAGGED_DATA, {})\ntagged_values[key] = value\nreturn _decorator_non_return", "path": "zope\\interface\\interface.py", "repo_name": "straup/gae-termextractor", "stars": 9, "license": "None", "language": "python", "size": 676}
{"docstring": "\"\"\"Does the specification extend the given interface?\n\nTest whether an interface in the specification extends the\ngiven interface\n\nExamples::\n\n  >>> from zope.interface import Interface\n  >>> from zope.interface.declarations import Declaration\n  >>> class I1(Interface): pass\n  ...\n  >>> class I2(I1): pass\n  ...\n  >>> class I3(Interface): pass\n  ...\n  >>> class I4(I3): pass\n  ...\n  >>> spec = Declaration()\n  >>> int(spec.extends(Interface))\n  1\n  >>> spec = Declaration(I2)\n  >>> int(spec.extends(Interface))\n  1\n  >>> int(spec.extends(I1))\n  1\n  >>> int(spec.extends(I2))\n  1\n  >>> int(spec.extends(I3))\n  0\n  >>> int(spec.extends(I4))\n  0\n  >>> I2.extends(I2)\n  0\n  >>> I2.extends(I2, False)\n  1\n  >>> I2.extends(I2, strict=False)\n  1\n\n\"\"\"\n", "func_signal": "def extends(self, interface, strict=True):\n", "code": "return ((interface in self._implied)\n        and\n        ((not strict) or (self != interface))\n        )", "path": "zope\\interface\\interface.py", "repo_name": "straup/gae-termextractor", "stars": 9, "license": "None", "language": "python", "size": 676}
{"docstring": "# Register ourselves as a dependent of our old bases\n", "func_signal": "def __setBases(self, bases):\n", "code": "for b in self.__bases__:\n    b.unsubscribe(self)\n\n# Register ourselves as a dependent of our bases\nself.__dict__['__bases__'] = bases\nfor b in bases:\n    b.subscribe(self)\n\nself.changed(self)", "path": "zope\\interface\\interface.py", "repo_name": "straup/gae-termextractor", "stars": 9, "license": "None", "language": "python", "size": 676}
{"docstring": "\"\"\"Is the interface implemented by an object\n\n  >>> from zope.interface import *\n  >>> class I1(Interface):\n  ...     pass\n  >>> class C(object):\n  ...     implements(I1)\n  >>> c = C()\n  >>> class X(object):\n  ...     pass\n  >>> x = X()\n  >>> I1.providedBy(x)\n  False\n  >>> I1.providedBy(C)\n  False\n  >>> I1.providedBy(c)\n  True\n  >>> directlyProvides(x, I1)\n  >>> I1.providedBy(x)\n  True\n  >>> directlyProvides(C, I1)\n  >>> I1.providedBy(C)\n  True\n\n\"\"\"\n", "func_signal": "def providedBy(self, ob):\n", "code": "spec = providedBy(ob)\nreturn self in spec._implied", "path": "zope\\interface\\interface.py", "repo_name": "straup/gae-termextractor", "stars": 9, "license": "None", "language": "python", "size": 676}
{"docstring": "\"\"\"Test whether the specification is implemented by a class or factory.\nRaise TypeError if argument is neither a class nor a callable.\"\"\"\n", "func_signal": "def implementedBy(self, cls):\n", "code": "spec = implementedBy(cls)\nreturn self in spec._implied", "path": "zope\\interface\\interface.py", "repo_name": "straup/gae-termextractor", "stars": 9, "license": "None", "language": "python", "size": 676}
{"docstring": "\"\"\"Return an iterator for the interfaces in the specification\n\nfor example::\n\n  >>> from zope.interface import Interface\n  >>> class I1(Interface): pass\n  ...\n  >>> class I2(I1): pass\n  ...\n  >>> class I3(Interface): pass\n  ...\n  >>> class I4(I3): pass\n  ...\n  >>> spec = Specification((I2, I3))\n  >>> spec = Specification((I4, spec))\n  >>> i = spec.interfaces()\n  >>> i.next().getName()\n  'I4'\n  >>> i.next().getName()\n  'I2'\n  >>> i.next().getName()\n  'I3'\n  >>> list(i)\n  []\n\"\"\"\n", "func_signal": "def interfaces(self):\n", "code": "seen = {}\nfor base in self.__bases__:\n    for interface in base.interfaces():\n        if interface not in seen:\n            seen[interface] = 1\n            yield interface", "path": "zope\\interface\\interface.py", "repo_name": "straup/gae-termextractor", "stars": 9, "license": "None", "language": "python", "size": 676}
{"docstring": "\"\"\"See interfaces.ITermExtractor\"\"\"\n", "func_signal": "def extract(self, taggedTerms):\n", "code": "terms = {}\n# Phase 1: A little state machine is used to build simple and\n# composite terms.\nmultiterm = []\nstate = SEARCH\nwhile taggedTerms:\n    term, tag, norm = taggedTerms.pop(0)\n    if state == SEARCH and tag.startswith('N'):\n        state = NOUN\n        _add(term, norm, multiterm, terms)\n    elif state == SEARCH and tag == 'JJ' and term[0].isupper():\n        state = NOUN\n        _add(term, norm, multiterm, terms)\n    elif state == NOUN and tag.startswith('N'):\n        _add(term, norm, multiterm, terms)\n    elif state == NOUN and not tag.startswith('N'):\n        state = SEARCH\n        if len(multiterm) > 1:\n            word = ' '.join([word for word, norm in multiterm])\n            terms.setdefault(word, 0)\n            terms[word] += 1\n        multiterm = []\n# Phase 2: Only select the terms that fulfill the filter criteria.\n# Also create the term strength.\nreturn [\n    (word, occur, len(word.split()))\n    for word, occur in terms.items()\n    if self.filter(word, occur, len(word.split()))]", "path": "topia\\termextract\\extract.py", "repo_name": "straup/gae-termextractor", "stars": 9, "license": "None", "language": "python", "size": 676}
{"docstring": "\"\"\"Adapt an object to the reciever\n\"\"\"\n", "func_signal": "def __adapt__(self, obj):\n", "code": "if self.providedBy(obj):\n    return obj\n\nfor hook in adapter_hooks:\n    adapter = hook(self, obj)\n    if adapter is not None:\n        return adapter", "path": "zope\\interface\\interface.py", "repo_name": "straup/gae-termextractor", "stars": 9, "license": "None", "language": "python", "size": 676}
{"docstring": "\"\"\"We, or something we depend on, have changed\n\"\"\"\n\n", "func_signal": "def changed(self, originally_changed):\n", "code": "implied = self._implied\nimplied.clear()\n\nancestors = ro(self)\n\ntry:\n    if Interface not in ancestors:\n        ancestors.append(Interface)\nexcept NameError:\n    pass # defining Interface itself\n\nself.__sro__ = tuple(ancestors)\nself.__iro__ = tuple([ancestor for ancestor in ancestors\n                      if isinstance(ancestor, InterfaceClass)\n                      ])\n\nfor ancestor in ancestors:\n    # We directly imply our ancestors:\n    implied[ancestor] = ()\n\n# Now, advise our dependents of change:\nfor dependent in self.dependents.keys():\n    dependent.changed(originally_changed)", "path": "zope\\interface\\interface.py", "repo_name": "straup/gae-termextractor", "stars": 9, "license": "None", "language": "python", "size": 676}
{"docstring": "\"\"\"Declare interfaces provided by a module\n\nThis function is used in a module definition.\n\nThe arguments are one or more interfaces or interface specifications\n(``IDeclaration`` objects).\n\nThe given interfaces (including the interfaces in the specifications) are\nused to create the module's direct-object interface specification.  An\nerror will be raised if the module already has an interface specification.\nIn other words, it is an error to call this function more than once in a\nmodule definition.\n\nThis function is provided for convenience. It provides a more convenient\nway to call directlyProvides. For example::\n\n  moduleImplements(I1)\n\nis equivalent to::\n\n  directlyProvides(sys.modules[__name__], I1)\n\"\"\"\n", "func_signal": "def moduleProvides(*interfaces):\n", "code": "frame = sys._getframe(1)\nlocals = frame.f_locals\n\n# Try to make sure we were called from a class def\nif (locals is not frame.f_globals) or ('__name__' not in locals):\n    raise TypeError(\n        \"moduleProvides can only be used from a module definition.\")\n\nif '__provides__' in locals:\n    raise TypeError(\n        \"moduleProvides can only be used once in a module definition.\")\n\nlocals[\"__provides__\"] = Provides(ModuleType,\n                                  *_normalizeargs(interfaces))", "path": "zope\\interface\\declarations.py", "repo_name": "straup/gae-termextractor", "stars": 9, "license": "None", "language": "python", "size": 676}
{"docstring": "\"\"\"See interfaces.ITermExtractor\"\"\"\n", "func_signal": "def __call__(self, text):\n", "code": "terms = self.tagger(text)\nreturn self.extract(terms)", "path": "topia\\termextract\\extract.py", "repo_name": "straup/gae-termextractor", "stars": 9, "license": "None", "language": "python", "size": 676}
{"docstring": "\"\"\"Cache instance declarations\n\n  Instance declarations are shared among instances that have the same\n  declaration. The declarations are cached in a weak value dictionary.\n\n  (Note that, in the examples below, we are going to make assertions about\n   the size of the weakvalue dictionary.  For the assertions to be\n   meaningful, we need to force garbage collection to make sure garbage\n   objects are, indeed, removed from the system. Depending on how Python\n   is run, we may need to make multiple calls to be sure.  We provide a\n   collect function to help with this:\n\n   >>> import gc\n   >>> def collect():\n   ...     for i in range(4):\n   ...         gc.collect()\n\n  )\n\n  >>> collect()\n  >>> before = len(InstanceDeclarations)\n\n  >>> class C(object):\n  ...    pass\n\n  >>> from zope.interface import Interface\n  >>> class I(Interface):\n  ...    pass\n\n  >>> c1 = C()\n  >>> c2 = C()\n\n  >>> len(InstanceDeclarations) == before\n  1\n\n  >>> directlyProvides(c1, I)\n  >>> len(InstanceDeclarations) == before + 1\n  1\n\n  >>> directlyProvides(c2, I)\n  >>> len(InstanceDeclarations) == before + 1\n  1\n\n  >>> del c1\n  >>> collect()\n  >>> len(InstanceDeclarations) == before + 1\n  1\n\n  >>> del c2\n  >>> collect()\n  >>> len(InstanceDeclarations) == before\n  1\n  \"\"\"\n\n", "func_signal": "def Provides(*interfaces):\n", "code": "spec = InstanceDeclarations.get(interfaces)\nif spec is None:\n    spec = ProvidesClass(*interfaces)\n    InstanceDeclarations[interfaces] = spec\n\nreturn spec", "path": "zope\\interface\\declarations.py", "repo_name": "straup/gae-termextractor", "stars": 9, "license": "None", "language": "python", "size": 676}
{"docstring": "\"\"\"Declare additional interfaces implemented for instances of a class\n\n  The arguments after the class are one or more interfaces or\n  interface specifications (``IDeclaration`` objects).\n\n  The interfaces given (including the interfaces in the specifications)\n  are added to any interfaces previously declared.\n\n  Consider the following example:\n\n    >>> from zope.interface import Interface\n    >>> class I1(Interface): pass\n    ...\n    >>> class I2(Interface): pass\n    ...\n    >>> class I3(Interface): pass\n    ...\n    >>> class I4(Interface): pass\n    ...\n    >>> class I5(Interface): pass\n    ...\n    >>> class A(object):\n    ...   implements(I3)\n    >>> class B(object):\n    ...   implements(I4)\n    >>> class C(A, B):\n    ...   pass\n    >>> classImplements(C, I1, I2)\n    >>> [i.getName() for i in implementedBy(C)]\n    ['I1', 'I2', 'I3', 'I4']\n    >>> classImplements(C, I5)\n    >>> [i.getName() for i in implementedBy(C)]\n    ['I1', 'I2', 'I5', 'I3', 'I4']\n\n  Instances of ``C`` provide ``I1``, ``I2``, ``I5``, and whatever\n  interfaces instances of ``A`` and ``B`` provide.\n  \"\"\"\n\n", "func_signal": "def classImplements(cls, *interfaces):\n", "code": "spec = implementedBy(cls)\nspec.declared += tuple(_normalizeargs(interfaces))\n\n# compute the bases\nbases = []\nseen = {}\nfor b in spec.declared:\n    if b not in seen:\n        seen[b] = 1\n        bases.append(b)\n\nif spec.inherit is not None:\n\n    for c in spec.inherit.__bases__:\n        b = implementedBy(c)\n        if b not in seen:\n            seen[b] = 1\n            bases.append(b)\n\nspec.__bases__ = tuple(bases)", "path": "zope\\interface\\declarations.py", "repo_name": "straup/gae-termextractor", "stars": 9, "license": "None", "language": "python", "size": 676}
{"docstring": "\"\"\"Add two specifications or a specification and an interface\n\nExamples:\n\n  >>> from zope.interface import Interface\n  >>> class I1(Interface): pass\n  ...\n  >>> class I2(I1): pass\n  ...\n  >>> class I3(Interface): pass\n  ...\n  >>> class I4(I3): pass\n  ...\n  >>> spec = Declaration()\n  >>> [iface.getName() for iface in spec]\n  []\n  >>> [iface.getName() for iface in spec+I1]\n  ['I1']\n  >>> [iface.getName() for iface in I1+spec]\n  ['I1']\n  >>> spec2 = spec\n  >>> spec += I1\n  >>> [iface.getName() for iface in spec]\n  ['I1']\n  >>> [iface.getName() for iface in spec2]\n  []\n  >>> spec2 += Declaration(I3, I4)\n  >>> [iface.getName() for iface in spec2]\n  ['I3', 'I4']\n  >>> [iface.getName() for iface in spec+spec2]\n  ['I1', 'I3', 'I4']\n  >>> [iface.getName() for iface in spec2+spec]\n  ['I3', 'I4', 'I1']\n\n\"\"\"\n\n", "func_signal": "def __add__(self, other):\n", "code": "seen = {}\nresult = []\nfor i in self.interfaces():\n    if i not in seen:\n        seen[i] = 1\n        result.append(i)\nfor i in other.interfaces():\n    if i not in seen:\n        seen[i] = 1\n        result.append(i)\n\nreturn Declaration(*result)", "path": "zope\\interface\\declarations.py", "repo_name": "straup/gae-termextractor", "stars": 9, "license": "None", "language": "python", "size": 676}
{"docstring": "\"\"\"Get an object specification for an object\n\nFor example:\n\n  >>> from zope.interface import Interface\n  >>> class IFoo(Interface): pass\n  ...\n  >>> class IFooFactory(Interface): pass\n  ...\n  >>> class C(object):\n  ...   implements(IFoo)\n  ...   classProvides(IFooFactory)\n  >>> [i.getName() for i in C.__providedBy__]\n  ['IFooFactory']\n  >>> [i.getName() for i in C().__providedBy__]\n  ['IFoo']\n\n\"\"\"\n\n# Get an ObjectSpecification bound to either an instance or a class,\n# depending on how we were accessed.\n\n", "func_signal": "def __get__(self, inst, cls):\n", "code": "if inst is None:\n    return getObjectSpecification(cls)\n\nprovides = getattr(inst, '__provides__', None)\nif provides is not None:\n    return provides\n\nreturn implementedBy(cls)", "path": "zope\\interface\\declarations.py", "repo_name": "straup/gae-termextractor", "stars": 9, "license": "None", "language": "python", "size": 676}
{"docstring": "\"\"\"Return the attribute description for the given name.\"\"\"\n", "func_signal": "def getDescriptionFor(self, name):\n", "code": "r = self.get(name)\nif r is not None:\n    return r\n\nraise KeyError(name)", "path": "zope\\interface\\interface.py", "repo_name": "straup/gae-termextractor", "stars": 9, "license": "None", "language": "python", "size": 676}
{"docstring": "\"\"\"Return the interfaces directly provided by the given object\n\nThe value returned is an ``IDeclaration``.\n\"\"\"\n", "func_signal": "def directlyProvidedBy(object):\n", "code": "provides = getattr(object, \"__provides__\", None)\nif (provides is None # no spec\n    or\n    # We might have gotten the implements spec, as an\n    # optimization. If so, it's like having only one base, that we\n    # lop off to exclude class-supplied declarations:\n    isinstance(provides, Implements)\n    ):\n    return _empty\n\n# Strip off the class part of the spec:\nreturn Declaration(provides.__bases__[:-1])", "path": "zope\\interface\\declarations.py", "repo_name": "straup/gae-termextractor", "stars": 9, "license": "None", "language": "python", "size": 676}
{"docstring": "\"\"\"Adapt an object to the interface\n\"\"\"\n", "func_signal": "def __call__(self, obj, alternate=_marker):\n", "code": "conform = getattr(obj, '__conform__', None)\nif conform is not None:\n    adapter = self._call_conform(conform)\n    if adapter is not None:\n        return adapter\n\nadapter = self.__adapt__(obj)\n\nif adapter is not None:\n    return adapter\nelif alternate is not _marker:\n    return alternate\nelse:\n    raise TypeError(\"Could not adapt\", obj, self)", "path": "zope\\interface\\interface.py", "repo_name": "straup/gae-termextractor", "stars": 9, "license": "None", "language": "python", "size": 676}
{"docstring": "\"\"\"\nThis removes a directly provided interface from an object.\nConsider the following two interfaces:\n\n  >>> from zope.interface import Interface\n  >>> class I1(Interface): pass\n  ...\n  >>> class I2(Interface): pass\n  ...\n\n``I1`` is provided through the class, ``I2`` is directly provided\nby the object:\n\n  >>> class C(object):\n  ...    implements(I1)\n  >>> c = C()\n  >>> alsoProvides(c, I2)\n  >>> I2.providedBy(c)\n  True\n\nRemove I2 from c again:\n  \n  >>> noLongerProvides(c, I2)\n  >>> I2.providedBy(c)\n  False\n\nRemoving an interface that is provided through the class is not possible:\n\n  >>> noLongerProvides(c, I1)\n  Traceback (most recent call last):\n  ...\n  ValueError: Can only remove directly provided interfaces.\n\n\"\"\"\n", "func_signal": "def noLongerProvides(object, interface):\n", "code": "directlyProvides(object, directlyProvidedBy(object)-interface)\nif interface.providedBy(object):\n    raise ValueError(\"Can only remove directly provided interfaces.\")", "path": "zope\\interface\\declarations.py", "repo_name": "straup/gae-termextractor", "stars": 9, "license": "None", "language": "python", "size": 676}
{"docstring": "\"\"\"Expand Makefile-style variables -- \"${foo}\" or \"$(foo)\" -- in\n'string' according to 'vars' (a dictionary mapping variable names to\nvalues).  Variables not present in 'vars' are silently expanded to the\nempty string.  The variable values in 'vars' should not contain further\nvariable expansions; if 'vars' is the output of 'parse_makefile()',\nyou're fine.  Returns a variable-expanded version of 's'.\n\"\"\"\n\n# This algorithm does multiple expansion, so if vars['foo'] contains\n# \"${bar}\", it will expand ${foo} to ${bar}, and then expand\n# ${bar}... and so forth.  This is fine as long as 'vars' comes from\n# 'parse_makefile()', which takes care of such expansions eagerly,\n# according to make's variable expansion semantics.\n\n", "func_signal": "def expand_makefile_vars(s, vars):\n", "code": "while 1:\n    m = _findvar1_rx.search(s) or _findvar2_rx.search(s)\n    if m:\n        (beg, end) = m.span()\n        s = s[0:beg] + vars.get(m.group(1)) + s[end:]\n    else:\n        break\nreturn s", "path": "Lib\\distutils\\sysconfig.py", "repo_name": "pajju/IncPy", "stars": 11, "license": "other", "language": "python", "size": 13897}
{"docstring": "# Strangely, this *is* supposed to test that overlapping\n# elements are allowed.  HTMLParser is more geared toward\n# lexing the input that parsing the structure.\n", "func_signal": "def test_bad_nesting(self):\n", "code": "self._run_check(\"<a><b></a></b>\", [\n    (\"starttag\", \"a\", []),\n    (\"starttag\", \"b\", []),\n    (\"endtag\", \"a\"),\n    (\"endtag\", \"b\"),\n    ])", "path": "Lib\\test\\test_htmlparser.py", "repo_name": "pajju/IncPy", "stars": 11, "license": "other", "language": "python", "size": 13897}
{"docstring": "\"\"\"Return full pathname of installed pyconfig.h file.\"\"\"\n", "func_signal": "def get_config_h_filename():\n", "code": "if python_build:\n    if os.name == \"nt\":\n        inc_dir = os.path.join(project_base, \"PC\")\n    else:\n        inc_dir = project_base\nelse:\n    inc_dir = get_python_inc(plat_specific=1)\nif get_python_version() < '2.2':\n    config_h = 'config.h'\nelse:\n    # The name of the config.h file changed in 2.2\n    config_h = 'pyconfig.h'\nreturn os.path.join(inc_dir, config_h)", "path": "Lib\\distutils\\sysconfig.py", "repo_name": "pajju/IncPy", "stars": 11, "license": "other", "language": "python", "size": 13897}
{"docstring": "\"\"\"Initialize the module as appropriate for NT\"\"\"\n", "func_signal": "def _init_nt():\n", "code": "g = {}\n# set basic install directories\ng['LIBDEST'] = get_python_lib(plat_specific=0, standard_lib=1)\ng['BINLIBDEST'] = get_python_lib(plat_specific=1, standard_lib=1)\n\n# XXX hmmm.. a normal install puts include files here\ng['INCLUDEPY'] = get_python_inc(plat_specific=0)\n\ng['SO'] = '.pyd'\ng['EXE'] = \".exe\"\ng['VERSION'] = get_python_version().replace(\".\", \"\")\ng['BINDIR'] = os.path.dirname(os.path.abspath(sys.executable))\n\nglobal _config_vars\n_config_vars = g", "path": "Lib\\distutils\\sysconfig.py", "repo_name": "pajju/IncPy", "stars": 11, "license": "other", "language": "python", "size": 13897}
{"docstring": "\"\"\"'nice' representation of an object\"\"\"\n", "func_signal": "def nice(s):\n", "code": "if type(s) is StringType: return repr(s)\nelse: return str(s)", "path": "Lib\\plat-mac\\aetypes.py", "repo_name": "pajju/IncPy", "stars": 11, "license": "other", "language": "python", "size": 13897}
{"docstring": "# This is an implementation detail, not an interface requirement\n", "func_signal": "def test_len(self):\n", "code": "from test.test_iterlen import len\nfor s in ('hello', tuple('hello'), list('hello'), xrange(5)):\n    self.assertEqual(len(reversed(s)), len(s))\n    r = reversed(s)\n    list(r)\n    self.assertEqual(len(r), 0)\nclass SeqWithWeirdLen:\n    called = False\n    def __len__(self):\n        if not self.called:\n            self.called = True\n            return 10\n        raise ZeroDivisionError\n    def __getitem__(self, index):\n        return index\nr = reversed(SeqWithWeirdLen())\nself.assertRaises(ZeroDivisionError, len, r)", "path": "Lib\\test\\test_enumerate.py", "repo_name": "pajju/IncPy", "stars": 11, "license": "other", "language": "python", "size": 13897}
{"docstring": "\"\"\"Return the directory containing installed Python header files.\n\nIf 'plat_specific' is false (the default), this is the path to the\nnon-platform-specific header files, i.e. Python.h and so on;\notherwise, this is the path to platform-specific header files\n(namely pyconfig.h).\n\nIf 'prefix' is supplied, use it instead of sys.prefix or\nsys.exec_prefix -- i.e., ignore 'plat_specific'.\n\"\"\"\n", "func_signal": "def get_python_inc(plat_specific=0, prefix=None):\n", "code": "if prefix is None:\n    prefix = plat_specific and EXEC_PREFIX or PREFIX\nif os.name == \"posix\":\n    if python_build:\n        base = os.path.dirname(os.path.abspath(sys.executable))\n        if plat_specific:\n            inc_dir = base\n        else:\n            inc_dir = os.path.join(base, \"Include\")\n            if not os.path.exists(inc_dir):\n                inc_dir = os.path.join(os.path.dirname(base), \"Include\")\n        return inc_dir\n    return os.path.join(prefix, \"include\", \"python\" + get_python_version())\nelif os.name == \"nt\":\n    return os.path.join(prefix, \"include\")\nelif os.name == \"mac\":\n    if plat_specific:\n        return os.path.join(prefix, \"Mac\", \"Include\")\n    else:\n        return os.path.join(prefix, \"Include\")\nelif os.name == \"os2\":\n    return os.path.join(prefix, \"Include\")\nelse:\n    raise DistutilsPlatformError(\n        \"I don't know where Python installs its C header files \"\n        \"on platform '%s'\" % os.name)", "path": "Lib\\distutils\\sysconfig.py", "repo_name": "pajju/IncPy", "stars": 11, "license": "other", "language": "python", "size": 13897}
{"docstring": "# Tests an implementation detail where tuple is reused\n# whenever nothing else holds a reference to it\n", "func_signal": "def test_tuple_reuse(self):\n", "code": "self.assertEqual(len(set(map(id, list(enumerate(self.seq))))), len(self.seq))\nself.assertEqual(len(set(map(id, enumerate(self.seq)))), min(1,len(self.seq)))", "path": "Lib\\test\\test_enumerate.py", "repo_name": "pajju/IncPy", "stars": 11, "license": "other", "language": "python", "size": 13897}
{"docstring": "# Normalize the list of events so that buffer artefacts don't\n# separate runs of contiguous characters.\n", "func_signal": "def get_events(self):\n", "code": "L = []\nprevtype = None\nfor event in self.events:\n    type = event[0]\n    if type == prevtype == \"data\":\n        L[-1] = (\"data\", L[-1][1] + event[1])\n    else:\n        L.append(event)\n    prevtype = type\nself.events = L\nreturn L", "path": "Lib\\test\\test_htmlparser.py", "repo_name": "pajju/IncPy", "stars": 11, "license": "other", "language": "python", "size": 13897}
{"docstring": "\"\"\"Do any platform-specific customization of a CCompiler instance.\n\nMainly needed on Unix, so we can plug in the information that\nvaries across Unices and is stored in Python's Makefile.\n\"\"\"\n", "func_signal": "def customize_compiler(compiler):\n", "code": "if compiler.compiler_type == \"unix\":\n    (cc, cxx, opt, cflags, ccshared, ldshared, so_ext) = \\\n        get_config_vars('CC', 'CXX', 'OPT', 'CFLAGS',\n                        'CCSHARED', 'LDSHARED', 'SO')\n\n    if 'CC' in os.environ:\n        cc = os.environ['CC']\n    if 'CXX' in os.environ:\n        cxx = os.environ['CXX']\n    if 'LDSHARED' in os.environ:\n        ldshared = os.environ['LDSHARED']\n    if 'CPP' in os.environ:\n        cpp = os.environ['CPP']\n    else:\n        cpp = cc + \" -E\"           # not always\n    if 'LDFLAGS' in os.environ:\n        ldshared = ldshared + ' ' + os.environ['LDFLAGS']\n    if 'CFLAGS' in os.environ:\n        cflags = opt + ' ' + os.environ['CFLAGS']\n        ldshared = ldshared + ' ' + os.environ['CFLAGS']\n    if 'CPPFLAGS' in os.environ:\n        cpp = cpp + ' ' + os.environ['CPPFLAGS']\n        cflags = cflags + ' ' + os.environ['CPPFLAGS']\n        ldshared = ldshared + ' ' + os.environ['CPPFLAGS']\n\n    cc_cmd = cc + ' ' + cflags\n    compiler.set_executables(\n        preprocessor=cpp,\n        compiler=cc_cmd,\n        compiler_so=cc_cmd + ' ' + ccshared,\n        compiler_cxx=cxx,\n        linker_so=ldshared,\n        linker_exe=cc)\n\n    compiler.shared_lib_extension = so_ext", "path": "Lib\\distutils\\sysconfig.py", "repo_name": "pajju/IncPy", "stars": 11, "license": "other", "language": "python", "size": 13897}
{"docstring": "# this bug was never in reversed, it was in\n# PyObject_CallMethod, and reversed_new calls that sometimes.\n", "func_signal": "def test_bug1229429(self):\n", "code": "if not hasattr(sys, \"getrefcount\"):\n    return\ndef f():\n    pass\nr = f.__reversed__ = object()\nrc = sys.getrefcount(r)\nfor i in range(10):\n    try:\n        reversed(f)\n    except TypeError:\n        pass\n    else:\n        self.fail(\"non-callable __reversed__ didn't raise!\")\nself.assertEqual(rc, sys.getrefcount(r))", "path": "Lib\\test\\test_enumerate.py", "repo_name": "pajju/IncPy", "stars": 11, "license": "other", "language": "python", "size": 13897}
{"docstring": "\"\"\"Initialize the module as appropriate for OS/2\"\"\"\n", "func_signal": "def _init_os2():\n", "code": "g = {}\n# set basic install directories\ng['LIBDEST'] = get_python_lib(plat_specific=0, standard_lib=1)\ng['BINLIBDEST'] = get_python_lib(plat_specific=1, standard_lib=1)\n\n# XXX hmmm.. a normal install puts include files here\ng['INCLUDEPY'] = get_python_inc(plat_specific=0)\n\ng['SO'] = '.pyd'\ng['EXE'] = \".exe\"\n\nglobal _config_vars\n_config_vars = g", "path": "Lib\\distutils\\sysconfig.py", "repo_name": "pajju/IncPy", "stars": 11, "license": "other", "language": "python", "size": 13897}
{"docstring": "# Test the interning machinery.\n", "func_signal": "def test(self):\n", "code": "p = expat.ParserCreate()\nL = []\ndef collector(name, *args):\n    L.append(name)\np.StartElementHandler = collector\np.EndElementHandler = collector\np.Parse(\"<e> <e/> <e></e> </e>\", 1)\ntag = L[0]\nself.assertEquals(len(L), 6)\nfor entry in L:\n    # L should have the same string repeated over and over.\n    self.assertTrue(tag is entry)", "path": "Lib\\test\\test_pyexpat.py", "repo_name": "pajju/IncPy", "stars": 11, "license": "other", "language": "python", "size": 13897}
{"docstring": "# http://python.org/sf/1296433\n#\n", "func_signal": "def test_parse_only_xml_data(self):\n", "code": "xml = \"<?xml version='1.0' encoding='iso8859'?><s>%s</s>\" % ('a' * 1025)\n# this one doesn't crash\n#xml = \"<?xml version='1.0'?><s>%s</s>\" % ('a' * 10000)\n\nclass SpecificException(Exception):\n    pass\n\ndef handler(text):\n    raise SpecificException\n\nparser = expat.ParserCreate()\nparser.CharacterDataHandler = handler\n\nself.assertRaises(Exception, parser.Parse, xml)", "path": "Lib\\test\\test_pyexpat.py", "repo_name": "pajju/IncPy", "stars": 11, "license": "other", "language": "python", "size": 13897}
{"docstring": "# XXX This test exposes more detail of Expat's text chunking than we\n# XXX like, but it tests what we need to concisely.\n", "func_signal": "def test1(self):\n", "code": "self.setHandlers([\"StartElementHandler\"])\nself.parser.Parse(\"<a>1<b buffer-text='no'/>2\\n3<c buffer-text='yes'/>4\\n5</a>\", 1)\nself.assertEquals(self.stuff,\n                  [\"<a>\", \"1\", \"<b>\", \"2\", \"\\n\", \"3\", \"<c>\", \"4\\n5\"],\n                  \"buffering control not reacting as expected\")", "path": "Lib\\test\\test_pyexpat.py", "repo_name": "pajju/IncPy", "stars": 11, "license": "other", "language": "python", "size": 13897}
{"docstring": "\"\"\"With no arguments, return a dictionary of all configuration\nvariables relevant for the current platform.  Generally this includes\neverything needed to build extensions and install both pure modules and\nextensions.  On Unix, this means every variable defined in Python's\ninstalled Makefile; on Windows and Mac OS it's a much smaller set.\n\nWith arguments, return a list of values that result from looking up\neach argument in the configuration variable dictionary.\n\"\"\"\n", "func_signal": "def get_config_vars(*args):\n", "code": "global _config_vars\nif _config_vars is None:\n    func = globals().get(\"_init_\" + os.name)\n    if func:\n        func()\n    else:\n        _config_vars = {}\n\n    # Normalized versions of prefix and exec_prefix are handy to have;\n    # in fact, these are the standard versions used most places in the\n    # Distutils.\n    _config_vars['prefix'] = PREFIX\n    _config_vars['exec_prefix'] = EXEC_PREFIX\n\n    if sys.platform == 'darwin':\n        kernel_version = os.uname()[2] # Kernel version (8.4.3)\n        major_version = int(kernel_version.split('.')[0])\n\n        if major_version < 8:\n            # On Mac OS X before 10.4, check if -arch and -isysroot\n            # are in CFLAGS or LDFLAGS and remove them if they are.\n            # This is needed when building extensions on a 10.3 system\n            # using a universal build of python.\n            for key in ('LDFLAGS', 'BASECFLAGS',\n                    # a number of derived variables. These need to be\n                    # patched up as well.\n                    'CFLAGS', 'PY_CFLAGS', 'BLDSHARED'):\n                flags = _config_vars[key]\n                flags = re.sub('-arch\\s+\\w+\\s', ' ', flags)\n                flags = re.sub('-isysroot [^ \\t]*', ' ', flags)\n                _config_vars[key] = flags\n\n        else:\n\n            # Allow the user to override the architecture flags using\n            # an environment variable.\n            # NOTE: This name was introduced by Apple in OSX 10.5 and\n            # is used by several scripting languages distributed with\n            # that OS release.\n\n            if 'ARCHFLAGS' in os.environ:\n                arch = os.environ['ARCHFLAGS']\n                for key in ('LDFLAGS', 'BASECFLAGS',\n                    # a number of derived variables. These need to be\n                    # patched up as well.\n                    'CFLAGS', 'PY_CFLAGS', 'BLDSHARED'):\n\n                    flags = _config_vars[key]\n                    flags = re.sub('-arch\\s+\\w+\\s', ' ', flags)\n                    flags = flags + ' ' + arch\n                    _config_vars[key] = flags\n\n            # If we're on OSX 10.5 or later and the user tries to\n            # compiles an extension using an SDK that is not present\n            # on the current machine it is better to not use an SDK\n            # than to fail.\n            #\n            # The major usecase for this is users using a Python.org\n            # binary installer  on OSX 10.6: that installer uses\n            # the 10.4u SDK, but that SDK is not installed by default\n            # when you install Xcode.\n            #\n            m = re.search('-isysroot\\s+(\\S+)', _config_vars['CFLAGS'])\n            if m is not None:\n                sdk = m.group(1)\n                if not os.path.exists(sdk):\n                    for key in ('LDFLAGS', 'BASECFLAGS',\n                         # a number of derived variables. These need to be\n                         # patched up as well.\n                        'CFLAGS', 'PY_CFLAGS', 'BLDSHARED'):\n\n                        flags = _config_vars[key]\n                        flags = re.sub('-isysroot\\s+\\S+(\\s|$)', ' ', flags)\n                        _config_vars[key] = flags\n\nif args:\n    vals = []\n    for name in args:\n        vals.append(_config_vars.get(name))\n    return vals\nelse:\n    return _config_vars", "path": "Lib\\distutils\\sysconfig.py", "repo_name": "pajju/IncPy", "stars": 11, "license": "other", "language": "python", "size": 13897}
{"docstring": "# Try the parse again, this time producing Unicode output\n", "func_signal": "def test_unicode(self):\n", "code": "out = self.Outputter()\nparser = expat.ParserCreate(namespace_separator='!')\nparser.returns_unicode = 1\nfor name in self.handler_names:\n    setattr(parser, name, getattr(out, name))\n\nparser.Parse(data, 1)\n\nop = out.out\nself.assertEquals(op[0], 'PI: u\\'xml-stylesheet\\' u\\'href=\"stylesheet.css\"\\'')\nself.assertEquals(op[1], \"Comment: u' comment data '\")\nself.assertEquals(op[2], \"Notation declared: (u'notation', None, u'notation.jpeg', None)\")\nself.assertEquals(op[3], \"Unparsed entity decl: (u'unparsed_entity', None, u'entity.file', None, u'notation')\")\nself.assertEquals(op[4], \"Start element: u'root' {u'attr1': u'value1', u'attr2': u'value2\\\\u1f40'}\")\nself.assertEquals(op[5], \"NS decl: u'myns' u'http://www.python.org/namespace'\")\nself.assertEquals(op[6], \"Start element: u'http://www.python.org/namespace!subelement' {}\")\nself.assertEquals(op[7], \"Character data: u'Contents of subelements'\")\nself.assertEquals(op[8], \"End element: u'http://www.python.org/namespace!subelement'\")\nself.assertEquals(op[9], \"End of NS decl: u'myns'\")\nself.assertEquals(op[10], \"Start element: u'sub2' {}\")\nself.assertEquals(op[11], 'Start of CDATA section')\nself.assertEquals(op[12], \"Character data: u'contents of CDATA section'\")\nself.assertEquals(op[13], 'End of CDATA section')\nself.assertEquals(op[14], \"End element: u'sub2'\")\nself.assertEquals(op[15], \"External entity ref: (None, u'entity.file', None)\")\nself.assertEquals(op[16], \"End element: u'root'\")", "path": "Lib\\test\\test_pyexpat.py", "repo_name": "pajju/IncPy", "stars": 11, "license": "other", "language": "python", "size": 13897}
{"docstring": "# Try parsing a file\n", "func_signal": "def test_parse_file(self):\n", "code": "out = self.Outputter()\nparser = expat.ParserCreate(namespace_separator='!')\nparser.returns_unicode = 1\nfor name in self.handler_names:\n    setattr(parser, name, getattr(out, name))\nfile = StringIO.StringIO(data)\n\nparser.ParseFile(file)\n\nop = out.out\nself.assertEquals(op[0], 'PI: u\\'xml-stylesheet\\' u\\'href=\"stylesheet.css\"\\'')\nself.assertEquals(op[1], \"Comment: u' comment data '\")\nself.assertEquals(op[2], \"Notation declared: (u'notation', None, u'notation.jpeg', None)\")\nself.assertEquals(op[3], \"Unparsed entity decl: (u'unparsed_entity', None, u'entity.file', None, u'notation')\")\nself.assertEquals(op[4], \"Start element: u'root' {u'attr1': u'value1', u'attr2': u'value2\\\\u1f40'}\")\nself.assertEquals(op[5], \"NS decl: u'myns' u'http://www.python.org/namespace'\")\nself.assertEquals(op[6], \"Start element: u'http://www.python.org/namespace!subelement' {}\")\nself.assertEquals(op[7], \"Character data: u'Contents of subelements'\")\nself.assertEquals(op[8], \"End element: u'http://www.python.org/namespace!subelement'\")\nself.assertEquals(op[9], \"End of NS decl: u'myns'\")\nself.assertEquals(op[10], \"Start element: u'sub2' {}\")\nself.assertEquals(op[11], 'Start of CDATA section')\nself.assertEquals(op[12], \"Character data: u'contents of CDATA section'\")\nself.assertEquals(op[13], 'End of CDATA section')\nself.assertEquals(op[14], \"End element: u'sub2'\")\nself.assertEquals(op[15], \"External entity ref: (None, u'entity.file', None)\")\nself.assertEquals(op[16], \"End element: u'root'\")", "path": "Lib\\test\\test_pyexpat.py", "repo_name": "pajju/IncPy", "stars": 11, "license": "other", "language": "python", "size": 13897}
{"docstring": "\"\"\"Get an as-yet undefined attribute value.\n\nThis calls the get() function that was passed to the\nconstructor.  The result is stored as an instance variable so\nthat the next time the same attribute is requested,\n__getattr__() won't be invoked.\n\nIf the get() function raises an exception, this is simply\npassed on -- exceptions are not cached.\n\n\"\"\"\n", "func_signal": "def __getattr__(self, name):\n", "code": "attribute = self._get_(name)\nself.__dict__[name] = attribute\nreturn attribute", "path": "Lib\\Bastion.py", "repo_name": "pajju/IncPy", "stars": 11, "license": "other", "language": "python", "size": 13897}
{"docstring": "\"\"\"Initialize the module as appropriate for Macintosh systems\"\"\"\n", "func_signal": "def _init_mac():\n", "code": "g = {}\n# set basic install directories\ng['LIBDEST'] = get_python_lib(plat_specific=0, standard_lib=1)\ng['BINLIBDEST'] = get_python_lib(plat_specific=1, standard_lib=1)\n\n# XXX hmmm.. a normal install puts include files here\ng['INCLUDEPY'] = get_python_inc(plat_specific=0)\n\nimport MacOS\nif not hasattr(MacOS, 'runtimemodel'):\n    g['SO'] = '.ppc.slb'\nelse:\n    g['SO'] = '.%s.slb' % MacOS.runtimemodel\n\n# XXX are these used anywhere?\ng['install_lib'] = os.path.join(EXEC_PREFIX, \"Lib\")\ng['install_platlib'] = os.path.join(EXEC_PREFIX, \"Mac\", \"Lib\")\n\n# These are used by the extension module build\ng['srcdir'] = ':'\nglobal _config_vars\n_config_vars = g", "path": "Lib\\distutils\\sysconfig.py", "repo_name": "pajju/IncPy", "stars": 11, "license": "other", "language": "python", "size": 13897}
{"docstring": "\"\"\"Determine the common suffix of two strings.\n\nArgs:\n  text1: First string.\n  text2: Second string.\n\nReturns:\n  The number of characters common to the end of each string.\n\"\"\"\n# Quick check for common null cases.\n", "func_signal": "def diff_commonSuffix(self, text1, text2):\n", "code": "if not text1 or not text2 or text1[-1] != text2[-1]:\n  return 0\n# Binary search.\n# Performance analysis: http://neil.fraser.name/news/2007/10/09/\npointermin = 0\npointermax = min(len(text1), len(text2))\npointermid = pointermax\npointerend = 0\nwhile pointermin < pointermid:\n  if (text1[-pointermid:len(text1) - pointerend] ==\n      text2[-pointermid:len(text2) - pointerend]):\n    pointermin = pointermid\n    pointerend = pointermin\n  else:\n    pointermax = pointermid\n  pointermid = int((pointermax - pointermin) / 2 + pointermin)\nreturn pointermid", "path": "diffviewer\\diff_match_patch.py", "repo_name": "skyronic/patchbin", "stars": 9, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"Compute and return the destination text (all equalities and insertions).\n\nArgs:\n  diffs: Array of diff tuples.\n\nReturns:\n  Destination text.\n\"\"\"\n", "func_signal": "def diff_text2(self, diffs):\n", "code": "text = []\nfor (op, data) in diffs:\n  if op != self.DIFF_DELETE:\n    text.append(data)\nreturn \"\".join(text)", "path": "diffviewer\\diff_match_patch.py", "repo_name": "skyronic/patchbin", "stars": 9, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"Convert a diff array into a pretty HTML report.\n\nArgs:\n  diffs: Array of diff tuples.\n\nReturns:\n  HTML representation.\n\"\"\"\n", "func_signal": "def diff_prettyHtml(self, diffs):\n", "code": "html = []\ni = 0\nfor (op, data) in diffs:\n  text = (data.replace(\"&\", \"&amp;\").replace(\"<\", \"&lt;\")\n             .replace(\">\", \"&gt;\").replace(\"\\n\", \"&para;<BR>\"))\n  if op == self.DIFF_INSERT:\n    html.append(\"<INS STYLE=\\\"background:#E6FFE6;\\\" TITLE=\\\"i=%i\\\">%s</INS>\"\n        % (i, text))\n  elif op == self.DIFF_DELETE:\n    html.append(\"<DEL STYLE=\\\"background:#FFE6E6;\\\" TITLE=\\\"i=%i\\\">%s</DEL>\"\n        % (i, text))\n  elif op == self.DIFF_EQUAL:\n    html.append(\"<SPAN TITLE=\\\"i=%i\\\">%s</SPAN>\" % (i, text))\n  if op != self.DIFF_DELETE:\n    i += len(data)\nreturn \"\".join(html)", "path": "diffviewer\\diff_match_patch.py", "repo_name": "skyronic/patchbin", "stars": 9, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"Inits a diff_match_patch object with default settings.\nRedefine these in your program to override the defaults.\n\"\"\"\n\n# Number of seconds to map a diff before giving up (0 for infinity).\n", "func_signal": "def __init__(self):\n", "code": "self.Diff_Timeout = 1.0\n# Cost of an empty edit operation in terms of edit characters.\nself.Diff_EditCost = 4\n# The size beyond which the double-ended diff activates.\n# Double-ending is twice as fast, but less accurate.\nself.Diff_DualThreshold = 32\n# At what point is no match declared (0.0 = perfection, 1.0 = very loose).\nself.Match_Threshold = 0.5\n# How far to search for a match (0 = exact location, 1000+ = broad match).\n# A match this many characters away from the expected location will add\n# 1.0 to the score (0.0 is a perfect match).\nself.Match_Distance = 1000\n# When deleting a large block of text (over ~64 characters), how close does\n# the contents have to match the expected contents. (0.0 = perfection,\n# 1.0 = very loose).  Note that Match_Threshold controls how closely the\n# end points of a delete need to match.\nself.Patch_DeleteThreshold = 0.5\n# Chunk size for context length.\nself.Patch_Margin = 4\n\n# How many bits in a number?\n# Python has no maximum, thus to disable patch splitting set to 0.\n# However to avoid long patches in certain pathological cases, use 32.\n# Multiple short patches (using native ints) are much faster than long ones.\nself.Match_MaxBits = 32", "path": "diffviewer\\diff_match_patch.py", "repo_name": "skyronic/patchbin", "stars": 9, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"Explore the intersection points between the two texts.\n\nArgs:\n  text1: Old string to be diffed.\n  text2: New string to be diffed.\n\nReturns:\n  Array of diff tuples or None if no diff available.\n\"\"\"\n\n# Unlike in most languages, Python counts time in seconds.\n", "func_signal": "def diff_map(self, text1, text2):\n", "code": "s_end = time.time() + self.Diff_Timeout  # Don't run for too long.\n# Cache the text lengths to prevent multiple calls.\ntext1_length = len(text1)\ntext2_length = len(text2)\nmax_d = text1_length + text2_length - 1\ndoubleEnd = self.Diff_DualThreshold * 2 < max_d\nv_map1 = []\nv_map2 = []\nv1 = {}\nv2 = {}\nv1[1] = 0\nv2[1] = 0\nfootsteps = {}\ndone = False\n# If the total number of characters is odd, then the front path will\n# collide with the reverse path.\nfront = (text1_length + text2_length) % 2\nfor d in xrange(max_d):\n  # Bail out if timeout reached.\n  if self.Diff_Timeout > 0 and time.time() > s_end:\n    return None\n\n  # Walk the front path one step.\n  v_map1.append({})\n  for k in xrange(-d, d + 1, 2):\n    if k == -d or k != d and v1[k - 1] < v1[k + 1]:\n      x = v1[k + 1]\n    else:\n      x = v1[k - 1] + 1\n    y = x - k\n    if doubleEnd:\n      footstep = (x, y)\n      if front and footstep in footsteps:\n        done = True\n      if not front:\n        footsteps[footstep] = d\n\n    while (not done and x < text1_length and y < text2_length and\n           text1[x] == text2[y]):\n      x += 1\n      y += 1\n      if doubleEnd:\n        footstep = (x, y)\n        if front and footstep in footsteps:\n          done = True\n        if not front:\n          footsteps[footstep] = d\n\n    v1[k] = x\n    v_map1[d][(x, y)] = True\n    if x == text1_length and y == text2_length:\n      # Reached the end in single-path mode.\n      return self.diff_path1(v_map1, text1, text2)\n    elif done:\n      # Front path ran over reverse path.\n      v_map2 = v_map2[:footsteps[footstep] + 1]\n      a = self.diff_path1(v_map1, text1[:x], text2[:y])\n      b = self.diff_path2(v_map2, text1[x:], text2[y:])\n      return a + b\n\n  if doubleEnd:\n    # Walk the reverse path one step.\n    v_map2.append({})\n    for k in xrange(-d, d + 1, 2):\n      if k == -d or k != d and v2[k - 1] < v2[k + 1]:\n        x = v2[k + 1]\n      else:\n        x = v2[k - 1] + 1\n      y = x - k\n      footstep = (text1_length - x, text2_length - y)\n      if not front and footstep in footsteps:\n        done = True\n      if front:\n        footsteps[footstep] = d\n      while (not done and x < text1_length and y < text2_length and\n             text1[-x - 1] == text2[-y - 1]):\n        x += 1\n        y += 1\n        footstep = (text1_length - x, text2_length - y)\n        if not front and footstep in footsteps:\n          done = True\n        if front:\n          footsteps[footstep] = d\n\n      v2[k] = x\n      v_map2[d][(x, y)] = True\n      if done:\n        # Reverse path ran over front path.\n        v_map1 = v_map1[:footsteps[footstep] + 1]\n        a = self.diff_path1(v_map1, text1[:text1_length - x],\n                            text2[:text2_length - y])\n        b = self.diff_path2(v_map2, text1[text1_length - x:],\n                            text2[text2_length - y:])\n        return a + b\n\n# Number of diffs equals number of characters, no commonality at all.\nreturn None", "path": "diffviewer\\diff_match_patch.py", "repo_name": "skyronic/patchbin", "stars": 9, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"Find the differences between two texts.  Assumes that the texts do not\n  have any common prefix or suffix.\n\nArgs:\n  text1: Old string to be diffed.\n  text2: New string to be diffed.\n  checklines: Speedup flag.  If false, then don't run a line-level diff\n    first to identify the changed areas.\n    If true, then run a faster, slightly less optimal diff.\n\nReturns:\n  Array of changes.\n\"\"\"\n", "func_signal": "def diff_compute(self, text1, text2, checklines):\n", "code": "if not text1:\n  # Just add some text (speedup)\n  return [(self.DIFF_INSERT, text2)]\n\nif not text2:\n  # Just delete some text (speedup)\n  return [(self.DIFF_DELETE, text1)]\n\nif len(text1) > len(text2):\n  (longtext, shorttext) = (text1, text2)\nelse:\n  (shorttext, longtext) = (text1, text2)\ni = longtext.find(shorttext)\nif i != -1:\n  # Shorter text is inside the longer text (speedup)\n  diffs = [(self.DIFF_INSERT, longtext[:i]), (self.DIFF_EQUAL, shorttext),\n           (self.DIFF_INSERT, longtext[i + len(shorttext):])]\n  # Swap insertions for deletions if diff is reversed.\n  if len(text1) > len(text2):\n    diffs[0] = (self.DIFF_DELETE, diffs[0][1])\n    diffs[2] = (self.DIFF_DELETE, diffs[2][1])\n  return diffs\nlongtext = shorttext = None  # Garbage collect.\n\n# Check to see if the problem can be split in two.\nhm = self.diff_halfMatch(text1, text2)\nif hm:\n  # A half-match was found, sort out the return data.\n  (text1_a, text1_b, text2_a, text2_b, mid_common) = hm\n  # Send both pairs off for separate processing.\n  diffs_a = self.diff_main(text1_a, text2_a, checklines)\n  diffs_b = self.diff_main(text1_b, text2_b, checklines)\n  # Merge the results.\n  return diffs_a + [(self.DIFF_EQUAL, mid_common)] + diffs_b\n\n# Perform a real diff.\nif checklines and (len(text1) < 100 or len(text2) < 100):\n  checklines = False  # Too trivial for the overhead.\nif checklines:\n  # Scan the text on a line-by-line basis first.\n  (text1, text2, linearray) = self.diff_linesToChars(text1, text2)\n\ndiffs = self.diff_map(text1, text2)\nif not diffs:  # No acceptable result.\n  diffs = [(self.DIFF_DELETE, text1), (self.DIFF_INSERT, text2)]\nif checklines:\n  # Convert the diff back to original text.\n  self.diff_charsToLines(diffs, linearray)\n  # Eliminate freak matches (e.g. blank lines)\n  self.diff_cleanupSemantic(diffs)\n\n  # Rediff any replacement blocks, this time character-by-character.\n  # Add a dummy entry at the end.\n  diffs.append((self.DIFF_EQUAL, ''))\n  pointer = 0\n  count_delete = 0\n  count_insert = 0\n  text_delete = ''\n  text_insert = ''\n  while pointer < len(diffs):\n    if diffs[pointer][0] == self.DIFF_INSERT:\n      count_insert += 1\n      text_insert += diffs[pointer][1]\n    elif diffs[pointer][0] == self.DIFF_DELETE:\n      count_delete += 1\n      text_delete += diffs[pointer][1]\n    elif diffs[pointer][0] == self.DIFF_EQUAL:\n      # Upon reaching an equality, check for prior redundancies.\n      if count_delete >= 1 and count_insert >= 1:\n        # Delete the offending records and add the merged ones.\n        a = self.diff_main(text_delete, text_insert, False)\n        diffs[pointer - count_delete - count_insert : pointer] = a\n        pointer = pointer - count_delete - count_insert + len(a)\n      count_insert = 0\n      count_delete = 0\n      text_delete = ''\n      text_insert = ''\n\n    pointer += 1\n\n  diffs.pop()  # Remove the dummy entry at the end.\nreturn diffs", "path": "diffviewer\\diff_match_patch.py", "repo_name": "skyronic/patchbin", "stars": 9, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"Work from the middle back to the start to determine the path.\n\nArgs:\n  v_map: Array of paths.\n  text1: Old string fragment to be diffed.\n  text2: New string fragment to be diffed.\n\nReturns:\n  Array of diff tuples.\n\"\"\"\n", "func_signal": "def diff_path1(self, v_map, text1, text2):\n", "code": "path = []\nx = len(text1)\ny = len(text2)\nlast_op = None\nfor d in xrange(len(v_map) - 2, -1, -1):\n  while True:\n    if (x - 1, y) in v_map[d]:\n      x -= 1\n      if last_op == self.DIFF_DELETE:\n        path[0] = (self.DIFF_DELETE, text1[x] + path[0][1])\n      else:\n        path[:0] = [(self.DIFF_DELETE, text1[x])]\n      last_op = self.DIFF_DELETE\n      break\n    elif (x, y - 1) in v_map[d]:\n      y -= 1\n      if last_op == self.DIFF_INSERT:\n        path[0] = (self.DIFF_INSERT, text2[y] + path[0][1])\n      else:\n        path[:0] = [(self.DIFF_INSERT, text2[y])]\n      last_op = self.DIFF_INSERT\n      break\n    else:\n      x -= 1\n      y -= 1\n      assert text1[x] == text2[y], (\"No diagonal.  \" +\n          \"Can't happen. (diff_path1)\")\n      if last_op == self.DIFF_EQUAL:\n        path[0] = (self.DIFF_EQUAL, text1[x] + path[0][1])\n      else:\n        path[:0] = [(self.DIFF_EQUAL, text1[x])]\n      last_op = self.DIFF_EQUAL\nreturn path", "path": "diffviewer\\diff_match_patch.py", "repo_name": "skyronic/patchbin", "stars": 9, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"Crush the diff into an encoded string which describes the operations\nrequired to transform text1 into text2.\nE.g. =3\\t-2\\t+ing  -> Keep 3 chars, delete 2 chars, insert 'ing'.\nOperations are tab-separated.  Inserted text is escaped using %xx notation.\n\nArgs:\n  diffs: Array of diff tuples.\n\nReturns:\n  Delta text.\n\"\"\"\n", "func_signal": "def diff_toDelta(self, diffs):\n", "code": "text = []\nfor (op, data) in diffs:\n  if op == self.DIFF_INSERT:\n    # High ascii will raise UnicodeDecodeError.  Use Unicode instead.\n    data = data.encode(\"utf-8\")\n    text.append(\"+\" + urllib.quote(data, \"!~*'();/?:@&=+$,# \"))\n  elif op == self.DIFF_DELETE:\n    text.append(\"-%d\" % len(data))\n  elif op == self.DIFF_EQUAL:\n    text.append(\"=%d\" % len(data))\nreturn \"\\t\".join(text)", "path": "diffviewer\\diff_match_patch.py", "repo_name": "skyronic/patchbin", "stars": 9, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"Reduce the number of edits by eliminating semantically trivial\nequalities.\n\nArgs:\n  diffs: Array of diff tuples.\n\"\"\"\n", "func_signal": "def diff_cleanupSemantic(self, diffs):\n", "code": "changes = False\nequalities = []  # Stack of indices where equalities are found.\nlastequality = None  # Always equal to equalities[-1][1]\npointer = 0  # Index of current position.\nlength_changes1 = 0  # Number of chars that changed prior to the equality.\nlength_changes2 = 0  # Number of chars that changed after the equality.\nwhile pointer < len(diffs):\n  if diffs[pointer][0] == self.DIFF_EQUAL:  # equality found\n    equalities.append(pointer)\n    length_changes1 = length_changes2\n    length_changes2 = 0\n    lastequality = diffs[pointer][1]\n  else:  # an insertion or deletion\n    length_changes2 += len(diffs[pointer][1])\n    if (lastequality != None and (len(lastequality) <= length_changes1) and\n        (len(lastequality) <= length_changes2)):\n      # Duplicate record\n      diffs.insert(equalities[-1], (self.DIFF_DELETE, lastequality))\n      # Change second copy to insert.\n      diffs[equalities[-1] + 1] = (self.DIFF_INSERT,\n          diffs[equalities[-1] + 1][1])\n      # Throw away the equality we just deleted.\n      equalities.pop()\n      # Throw away the previous equality (it needs to be reevaluated).\n      if len(equalities) != 0:\n        equalities.pop()\n      if len(equalities):\n        pointer = equalities[-1]\n      else:\n        pointer = -1\n      length_changes1 = 0  # Reset the counters.\n      length_changes2 = 0\n      lastequality = None\n      changes = True\n  pointer += 1\n\nif changes:\n  self.diff_cleanupMerge(diffs)\n\nself.diff_cleanupSemanticLossless(diffs)", "path": "diffviewer\\diff_match_patch.py", "repo_name": "skyronic/patchbin", "stars": 9, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"Merge a set of patches onto the text.  Return a patched text, as well\nas a list of true/false values indicating which patches were applied.\n\nArgs:\n  patches: Array of patch objects.\n  text: Old text.\n\nReturns:\n  Two element Array, containing the new text and an array of boolean values.\n\"\"\"\n", "func_signal": "def patch_apply(self, patches, text):\n", "code": "if not patches:\n  return (text, [])\n\n# Deep copy the patches so that no changes are made to originals.\npatches = self.patch_deepCopy(patches)\n\nnullPadding = self.patch_addPadding(patches)\ntext = nullPadding + text + nullPadding\nself.patch_splitMax(patches)\n\n# delta keeps track of the offset between the expected and actual location\n# of the previous patch.  If there are patches expected at positions 10 and\n# 20, but the first patch was found at 12, delta is 2 and the second patch\n# has an effective expected position of 22.\ndelta = 0\nresults = []\nfor patch in patches:\n  expected_loc = patch.start2 + delta\n  text1 = self.diff_text1(patch.diffs)\n  end_loc = -1\n  if len(text1) > self.Match_MaxBits:\n    # patch_splitMax will only provide an oversized pattern in the case of\n    # a monster delete.\n    start_loc = self.match_main(text, text1[:self.Match_MaxBits],\n                                expected_loc)\n    if start_loc != -1:\n      end_loc = self.match_main(text, text1[-self.Match_MaxBits:],\n          expected_loc + len(text1) - self.Match_MaxBits)\n      if end_loc == -1 or start_loc >= end_loc:\n        # Can't find valid trailing context.  Drop this patch.\n        start_loc = -1\n  else:\n    start_loc = self.match_main(text, text1, expected_loc)\n  if start_loc == -1:\n    # No match found.  :(\n    results.append(False)\n    # Subtract the delta for this failed patch from subsequent patches.\n    delta -= patch.length2 - patch.length1\n  else:\n    # Found a match.  :)\n    results.append(True)\n    delta = start_loc - expected_loc\n    if end_loc == -1:\n      text2 = text[start_loc : start_loc + len(text1)]\n    else:\n      text2 = text[start_loc : end_loc + self.Match_MaxBits]\n    if text1 == text2:\n      # Perfect match, just shove the replacement text in.\n      text = (text[:start_loc] + self.diff_text2(patch.diffs) +\n                  text[start_loc + len(text1):])\n    else:\n      # Imperfect match.\n      # Run a diff to get a framework of equivalent indices.\n      diffs = self.diff_main(text1, text2, False)\n      if (len(text1) > self.Match_MaxBits and\n          self.diff_levenshtein(diffs) / float(len(text1)) >\n          self.Patch_DeleteThreshold):\n        # The end points match, but the content is unacceptably bad.\n        results[-1] = False\n      else:\n        self.diff_cleanupSemanticLossless(diffs)\n        index1 = 0\n        for (op, data) in patch.diffs:\n          if op != self.DIFF_EQUAL:\n            index2 = self.diff_xIndex(diffs, index1)\n          if op == self.DIFF_INSERT:  # Insertion\n            text = text[:start_loc + index2] + data + text[start_loc +\n                                                           index2:]\n          elif op == self.DIFF_DELETE:  # Deletion\n            text = text[:start_loc + index2] + text[start_loc +\n                self.diff_xIndex(diffs, index1 + len(data)):]\n          if op != self.DIFF_DELETE:\n            index1 += len(data)\n# Strip the padding off.\ntext = text[len(nullPadding):-len(nullPadding)]\nreturn (text, results)", "path": "diffviewer\\diff_match_patch.py", "repo_name": "skyronic/patchbin", "stars": 9, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"Find the differences between two texts.  Simplifies the problem by\n  stripping any common prefix or suffix off the texts before diffing.\n\nArgs:\n  text1: Old string to be diffed.\n  text2: New string to be diffed.\n  checklines: Optional speedup flag.  If present and false, then don't run\n    a line-level diff first to identify the changed areas.\n    Defaults to true, which does a faster, slightly less optimal diff.\n\nReturns:\n  Array of changes.\n\"\"\"\n\n# Check for equality (speedup)\n", "func_signal": "def diff_main(self, text1, text2, checklines=True):\n", "code": "if text1 == text2:\n  return [(self.DIFF_EQUAL, text1)]\n\n# Trim off common prefix (speedup)\ncommonlength = self.diff_commonPrefix(text1, text2)\ncommonprefix = text1[:commonlength]\ntext1 = text1[commonlength:]\ntext2 = text2[commonlength:]\n\n# Trim off common suffix (speedup)\ncommonlength = self.diff_commonSuffix(text1, text2)\nif commonlength == 0:\n  commonsuffix = ''\nelse:\n  commonsuffix = text1[-commonlength:]\n  text1 = text1[:-commonlength]\n  text2 = text2[:-commonlength]\n\n# Compute the diff on the middle block\ndiffs = self.diff_compute(text1, text2, checklines)\n\n# Restore the prefix and suffix\nif commonprefix:\n  diffs[:0] = [(self.DIFF_EQUAL, commonprefix)]\nif commonsuffix:\n  diffs.append((self.DIFF_EQUAL, commonsuffix))\nself.diff_cleanupMerge(diffs)\nreturn diffs", "path": "diffviewer\\diff_match_patch.py", "repo_name": "skyronic/patchbin", "stars": 9, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"Reorder and merge like edit sections.  Merge equalities.\nAny edit section can move as long as it doesn't cross an equality.\n\nArgs:\n  diffs: Array of diff tuples.\n\"\"\"\n", "func_signal": "def diff_cleanupMerge(self, diffs):\n", "code": "diffs.append((self.DIFF_EQUAL, ''))  # Add a dummy entry at the end.\npointer = 0\ncount_delete = 0\ncount_insert = 0\ntext_delete = ''\ntext_insert = ''\nwhile pointer < len(diffs):\n  if diffs[pointer][0] == self.DIFF_INSERT:\n    count_insert += 1\n    text_insert += diffs[pointer][1]\n    pointer += 1\n  elif diffs[pointer][0] == self.DIFF_DELETE:\n    count_delete += 1\n    text_delete += diffs[pointer][1]\n    pointer += 1\n  elif diffs[pointer][0] == self.DIFF_EQUAL:\n    # Upon reaching an equality, check for prior redundancies.\n    if count_delete != 0 or count_insert != 0:\n      if count_delete != 0 and count_insert != 0:\n        # Factor out any common prefixies.\n        commonlength = self.diff_commonPrefix(text_insert, text_delete)\n        if commonlength != 0:\n          x = pointer - count_delete - count_insert - 1\n          if x >= 0 and diffs[x][0] == self.DIFF_EQUAL:\n            diffs[x] = (diffs[x][0], diffs[x][1] +\n                        text_insert[:commonlength])\n          else:\n            diffs.insert(0, (self.DIFF_EQUAL, text_insert[:commonlength]))\n            pointer += 1\n          text_insert = text_insert[commonlength:]\n          text_delete = text_delete[commonlength:]\n        # Factor out any common suffixies.\n        commonlength = self.diff_commonSuffix(text_insert, text_delete)\n        if commonlength != 0:\n          diffs[pointer] = (diffs[pointer][0], text_insert[-commonlength:] +\n              diffs[pointer][1])\n          text_insert = text_insert[:-commonlength]\n          text_delete = text_delete[:-commonlength]\n      # Delete the offending records and add the merged ones.\n      if count_delete == 0:\n        diffs[pointer - count_insert : pointer] = [\n            (self.DIFF_INSERT, text_insert)]\n      elif count_insert == 0:\n        diffs[pointer - count_delete : pointer] = [\n            (self.DIFF_DELETE, text_delete)]\n      else:\n        diffs[pointer - count_delete - count_insert : pointer] = [\n            (self.DIFF_DELETE, text_delete),\n            (self.DIFF_INSERT, text_insert)]\n      pointer = pointer - count_delete - count_insert + 1\n      if count_delete != 0:\n        pointer += 1\n      if count_insert != 0:\n        pointer += 1\n    elif pointer != 0 and diffs[pointer - 1][0] == self.DIFF_EQUAL:\n      # Merge this equality with the previous one.\n      diffs[pointer - 1] = (diffs[pointer - 1][0],\n                            diffs[pointer - 1][1] + diffs[pointer][1])\n      del diffs[pointer]\n    else:\n      pointer += 1\n\n    count_insert = 0\n    count_delete = 0\n    text_delete = ''\n    text_insert = ''\n\nif diffs[-1][1] == '':\n  diffs.pop()  # Remove the dummy entry at the end.\n\n# Second pass: look for single edits surrounded on both sides by equalities\n# which can be shifted sideways to eliminate an equality.\n# e.g: A<ins>BA</ins>C -> <ins>AB</ins>AC\nchanges = False\npointer = 1\n# Intentionally ignore the first and last element (don't need checking).\nwhile pointer < len(diffs) - 1:\n  if (diffs[pointer - 1][0] == self.DIFF_EQUAL and\n      diffs[pointer + 1][0] == self.DIFF_EQUAL):\n    # This is a single edit surrounded by equalities.\n    if diffs[pointer][1].endswith(diffs[pointer - 1][1]):\n      # Shift the edit over the previous equality.\n      diffs[pointer] = (diffs[pointer][0],\n          diffs[pointer - 1][1] +\n          diffs[pointer][1][:-len(diffs[pointer - 1][1])])\n      diffs[pointer + 1] = (diffs[pointer + 1][0],\n                            diffs[pointer - 1][1] + diffs[pointer + 1][1])\n      del diffs[pointer - 1]\n      changes = True\n    elif diffs[pointer][1].startswith(diffs[pointer + 1][1]):\n      # Shift the edit over the next equality.\n      diffs[pointer - 1] = (diffs[pointer - 1][0],\n                            diffs[pointer - 1][1] + diffs[pointer + 1][1])\n      diffs[pointer] = (diffs[pointer][0],\n          diffs[pointer][1][len(diffs[pointer + 1][1]):] +\n          diffs[pointer + 1][1])\n      del diffs[pointer + 1]\n      changes = True\n  pointer += 1\n\n# If shifts were made, the diff needs reordering and another shift sweep.\nif changes:\n  self.diff_cleanupMerge(diffs)", "path": "diffviewer\\diff_match_patch.py", "repo_name": "skyronic/patchbin", "stars": 9, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"Determine the common prefix of two strings.\n\nArgs:\n  text1: First string.\n  text2: Second string.\n\nReturns:\n  The number of characters common to the start of each string.\n\"\"\"\n# Quick check for common null cases.\n", "func_signal": "def diff_commonPrefix(self, text1, text2):\n", "code": "if not text1 or not text2 or text1[0] != text2[0]:\n  return 0\n# Binary search.\n# Performance analysis: http://neil.fraser.name/news/2007/10/09/\npointermin = 0\npointermax = min(len(text1), len(text2))\npointermid = pointermax\npointerstart = 0\nwhile pointermin < pointermid:\n  if text1[pointerstart:pointermid] == text2[pointerstart:pointermid]:\n    pointermin = pointermid\n    pointerstart = pointermin\n  else:\n    pointermax = pointermid\n  pointermid = int((pointermax - pointermin) / 2 + pointermin)\nreturn pointermid", "path": "diffviewer\\diff_match_patch.py", "repo_name": "skyronic/patchbin", "stars": 9, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"loc is a location in text1, compute and return the equivalent location\nin text2.  e.g. \"The cat\" vs \"The big cat\", 1->1, 5->8\n\nArgs:\n  diffs: Array of diff tuples.\n  loc: Location within text1.\n\nReturns:\n  Location within text2.\n\"\"\"\n", "func_signal": "def diff_xIndex(self, diffs, loc):\n", "code": "chars1 = 0\nchars2 = 0\nlast_chars1 = 0\nlast_chars2 = 0\nfor x in xrange(len(diffs)):\n  (op, text) = diffs[x]\n  if op != self.DIFF_INSERT:  # Equality or deletion.\n    chars1 += len(text)\n  if op != self.DIFF_DELETE:  # Equality or insertion.\n    chars2 += len(text)\n  if chars1 > loc:  # Overshot the location.\n    break\n  last_chars1 = chars1\n  last_chars2 = chars2\n\nif len(diffs) != x and diffs[x][0] == self.DIFF_DELETE:\n  # The location was deleted.\n  return last_chars2\n# Add the remaining len(character).\nreturn last_chars2 + (loc - last_chars1)", "path": "diffviewer\\diff_match_patch.py", "repo_name": "skyronic/patchbin", "stars": 9, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"Compute the Levenshtein distance; the number of inserted, deleted or\nsubstituted characters.\n\nArgs:\n  diffs: Array of diff tuples.\n\nReturns:\n  Number of changes.\n\"\"\"\n", "func_signal": "def diff_levenshtein(self, diffs):\n", "code": "levenshtein = 0\ninsertions = 0\ndeletions = 0\nfor (op, data) in diffs:\n  if op == self.DIFF_INSERT:\n    insertions += len(data)\n  elif op == self.DIFF_DELETE:\n    deletions += len(data)\n  elif op == self.DIFF_EQUAL:\n    # A deletion and an insertion is one substitution.\n    levenshtein += max(insertions, deletions)\n    insertions = 0\n    deletions = 0\nlevenshtein += max(insertions, deletions)\nreturn levenshtein", "path": "diffviewer\\diff_match_patch.py", "repo_name": "skyronic/patchbin", "stars": 9, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"Work from the middle back to the end to determine the path.\n\nArgs:\n  v_map: Array of paths.\n  text1: Old string fragment to be diffed.\n  text2: New string fragment to be diffed.\n\nReturns:\n  Array of diff tuples.\n\"\"\"\n", "func_signal": "def diff_path2(self, v_map, text1, text2):\n", "code": "path = []\nx = len(text1)\ny = len(text2)\nlast_op = None\nfor d in xrange(len(v_map) - 2, -1, -1):\n  while True:\n    if (x - 1, y) in v_map[d]:\n      x -= 1\n      if last_op == self.DIFF_DELETE:\n        path[-1] = (self.DIFF_DELETE, path[-1][1] + text1[-x - 1])\n      else:\n        path.append((self.DIFF_DELETE, text1[-x - 1]))\n      last_op = self.DIFF_DELETE\n      break\n    elif (x, y - 1) in v_map[d]:\n      y -= 1\n      if last_op == self.DIFF_INSERT:\n        path[-1] = (self.DIFF_INSERT, path[-1][1] + text2[-y - 1])\n      else:\n        path.append((self.DIFF_INSERT, text2[-y - 1]))\n      last_op = self.DIFF_INSERT\n      break\n    else:\n      x -= 1\n      y -= 1\n      assert text1[-x - 1] == text2[-y - 1], (\"No diagonal.  \" +\n          \"Can't happen. (diff_path2)\")\n      if last_op == self.DIFF_EQUAL:\n        path[-1] = (self.DIFF_EQUAL, path[-1][1] + text1[-x - 1])\n      else:\n        path.append((self.DIFF_EQUAL, text1[-x - 1]))\n      last_op = self.DIFF_EQUAL\nreturn path", "path": "diffviewer\\diff_match_patch.py", "repo_name": "skyronic/patchbin", "stars": 9, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"Look through the patches and break up any which are longer than the\nmaximum limit of the match algorithm.\n\nArgs:\n  patches: Array of patch objects.\n\"\"\"\n", "func_signal": "def patch_splitMax(self, patches):\n", "code": "if self.Match_MaxBits == 0:\n  return\nfor x in xrange(len(patches)):\n  if patches[x].length1 > self.Match_MaxBits:\n    bigpatch = patches[x]\n    # Remove the big old patch.\n    del patches[x]\n    x -= 1\n    patch_size = self.Match_MaxBits\n    start1 = bigpatch.start1\n    start2 = bigpatch.start2\n    precontext = ''\n    while len(bigpatch.diffs) != 0:\n      # Create one of several smaller patches.\n      patch = patch_obj()\n      empty = True\n      patch.start1 = start1 - len(precontext)\n      patch.start2 = start2 - len(precontext)\n      if precontext:\n        patch.length1 = patch.length2 = len(precontext)\n        patch.diffs.append((self.DIFF_EQUAL, precontext))\n\n      while (len(bigpatch.diffs) != 0 and\n             patch.length1 < patch_size - self.Patch_Margin):\n        (diff_type, diff_text) = bigpatch.diffs[0]\n        if diff_type == self.DIFF_INSERT:\n          # Insertions are harmless.\n          patch.length2 += len(diff_text)\n          start2 += len(diff_text)\n          patch.diffs.append(bigpatch.diffs.pop(0))\n          empty = False\n        elif (diff_type == self.DIFF_DELETE and len(patch.diffs) == 1 and\n            patch.diffs[0][0] == self.DIFF_EQUAL and\n            len(diff_text) > 2 * patch_size):\n          # This is a large deletion.  Let it pass in one chunk.\n          patch.length1 += len(diff_text)\n          start1 += len(diff_text)\n          empty = False\n          patch.diffs.append((diff_type, diff_text))\n          del bigpatch.diffs[0]\n        else:\n          # Deletion or equality.  Only take as much as we can stomach.\n          diff_text = diff_text[:patch_size - patch.length1 -\n                                self.Patch_Margin]\n          patch.length1 += len(diff_text)\n          start1 += len(diff_text)\n          if diff_type == self.DIFF_EQUAL:\n            patch.length2 += len(diff_text)\n            start2 += len(diff_text)\n          else:\n            empty = False\n\n          patch.diffs.append((diff_type, diff_text))\n          if diff_text == bigpatch.diffs[0][1]:\n            del bigpatch.diffs[0]\n          else:\n            bigpatch.diffs[0] = (bigpatch.diffs[0][0],\n                                 bigpatch.diffs[0][1][len(diff_text):])\n\n      # Compute the head context for the next patch.\n      precontext = self.diff_text2(patch.diffs)\n      precontext = precontext[-self.Patch_Margin:]\n      # Append the end context for this patch.\n      postcontext = self.diff_text1(bigpatch.diffs)[:self.Patch_Margin]\n      if postcontext:\n        patch.length1 += len(postcontext)\n        patch.length2 += len(postcontext)\n        if len(patch.diffs) != 0 and patch.diffs[-1][0] == self.DIFF_EQUAL:\n          patch.diffs[-1] = (self.DIFF_EQUAL, patch.diffs[-1][1] +\n                             postcontext)\n        else:\n          patch.diffs.append((self.DIFF_EQUAL, postcontext))\n\n      if not empty:\n        x += 1\n        patches.insert(x, patch)", "path": "diffviewer\\diff_match_patch.py", "repo_name": "skyronic/patchbin", "stars": 9, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"Do the two texts share a substring which is at least half the length of\nthe longer text?\n\nArgs:\n  text1: First string.\n  text2: Second string.\n\nReturns:\n  Five element Array, containing the prefix of text1, the suffix of text1,\n  the prefix of text2, the suffix of text2 and the common middle.  Or None\n  if there was no match.\n\"\"\"\n", "func_signal": "def diff_halfMatch(self, text1, text2):\n", "code": "if len(text1) > len(text2):\n  (longtext, shorttext) = (text1, text2)\nelse:\n  (shorttext, longtext) = (text1, text2)\nif len(longtext) < 10 or len(shorttext) < 1:\n  return None  # Pointless.\n\ndef diff_halfMatchI(longtext, shorttext, i):\n  \"\"\"Does a substring of shorttext exist within longtext such that the\n  substring is at least half the length of longtext?\n  Closure, but does not reference any external variables.\n\n  Args:\n    longtext: Longer string.\n    shorttext: Shorter string.\n    i: Start index of quarter length substring within longtext.\n\n  Returns:\n    Five element Array, containing the prefix of longtext, the suffix of\n    longtext, the prefix of shorttext, the suffix of shorttext and the\n    common middle.  Or None if there was no match.\n  \"\"\"\n  seed = longtext[i:i + len(longtext) / 4]\n  best_common = ''\n  j = shorttext.find(seed)\n  while j != -1:\n    prefixLength = self.diff_commonPrefix(longtext[i:], shorttext[j:])\n    suffixLength = self.diff_commonSuffix(longtext[:i], shorttext[:j])\n    if len(best_common) < suffixLength + prefixLength:\n      best_common = (shorttext[j - suffixLength:j] +\n          shorttext[j:j + prefixLength])\n      best_longtext_a = longtext[:i - suffixLength]\n      best_longtext_b = longtext[i + prefixLength:]\n      best_shorttext_a = shorttext[:j - suffixLength]\n      best_shorttext_b = shorttext[j + prefixLength:]\n    j = shorttext.find(seed, j + 1)\n\n  if len(best_common) >= len(longtext) / 2:\n    return (best_longtext_a, best_longtext_b,\n            best_shorttext_a, best_shorttext_b, best_common)\n  else:\n    return None\n\n# First check if the second quarter is the seed for a half-match.\nhm1 = diff_halfMatchI(longtext, shorttext, (len(longtext) + 3) / 4)\n# Check again based on the third quarter.\nhm2 = diff_halfMatchI(longtext, shorttext, (len(longtext) + 1) / 2)\nif not hm1 and not hm2:\n  return None\nelif not hm2:\n  hm = hm1\nelif not hm1:\n  hm = hm2\nelse:\n  # Both matched.  Select the longest.\n  if len(hm1[4]) > len(hm2[4]):\n    hm = hm1\n  else:\n    hm = hm2\n\n# A half-match was found, sort out the return data.\nif len(text1) > len(text2):\n  (text1_a, text1_b, text2_a, text2_b, mid_common) = hm\nelse:\n  (text2_a, text2_b, text1_a, text1_b, mid_common) = hm\nreturn (text1_a, text1_b, text2_a, text2_b, mid_common)", "path": "diffviewer\\diff_match_patch.py", "repo_name": "skyronic/patchbin", "stars": 9, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"Given an array of patches, return another array that is identical.\n\nArgs:\n  patches: Array of patch objects.\n\nReturns:\n  Array of patch objects.\n\"\"\"\n", "func_signal": "def patch_deepCopy(self, patches):\n", "code": "patchesCopy = []\nfor patch in patches:\n  patchCopy = patch_obj()\n  # No need to deep copy the tuples since they are immutable.\n  patchCopy.diffs = patch.diffs[:]\n  patchCopy.start1 = patch.start1\n  patchCopy.start2 = patch.start2\n  patchCopy.length1 = patch.length1\n  patchCopy.length2 = patch.length2\n  patchesCopy.append(patchCopy)\nreturn patchesCopy", "path": "diffviewer\\diff_match_patch.py", "repo_name": "skyronic/patchbin", "stars": 9, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"Rehydrate the text in a diff from a string of line hashes to real lines\nof text.\n\nArgs:\n  diffs: Array of diff tuples.\n  lineArray: Array of unique strings.\n\"\"\"\n", "func_signal": "def diff_charsToLines(self, diffs, lineArray):\n", "code": "for x in xrange(len(diffs)):\n  text = []\n  for char in diffs[x][1]:\n    text.append(lineArray[ord(char)])\n  diffs[x] = (diffs[x][0], \"\".join(text))", "path": "diffviewer\\diff_match_patch.py", "repo_name": "skyronic/patchbin", "stars": 9, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"Used in a call to re.sub to replace HTML, XML, and numeric\nentities with the appropriate Unicode characters. If HTML\nentities are being converted, any unrecognized entities are\nescaped.\"\"\"\n", "func_signal": "def _convertEntities(self, match):\n", "code": "x = match.group(1)\nif self.convertHTMLEntities and x in name2codepoint:\n    return unichr(name2codepoint[x])\nelif x in self.XML_ENTITIES_TO_SPECIAL_CHARS:\n    if self.convertXMLEntities:\n        return self.XML_ENTITIES_TO_SPECIAL_CHARS[x]\n    else:\n        return u'&%s;' % x\nelif len(x) > 0 and x[0] == '#':\n    # Handle numeric entities\n    if len(x) > 1 and x[1] == 'x':\n        return unichr(int(x[2:], 16))\n    else:\n        return unichr(int(x[1:]))\n\nelif self.escapeUnrecognizedEntities:\n    return u'&amp;%s;' % x\nelse:\n    return u'&%s;' % x", "path": "BeautifulSoup.py", "repo_name": "ptarjan/relmeauth", "stars": 15, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Extract all children.\"\"\"\n", "func_signal": "def clear(self):\n", "code": "for child in self.contents[:]:\n    child.extract()", "path": "BeautifulSoup.py", "repo_name": "ptarjan/relmeauth", "stars": 15, "license": "None", "language": "python", "size": 104}
{"docstring": "'''Given a string and its encoding, decodes the string into Unicode.\n%encoding is a string recognized by encodings.aliases'''\n\n# strip Byte Order Mark (if present)\n", "func_signal": "def _toUnicode(self, data, encoding):\n", "code": "if (len(data) >= 4) and (data[:2] == '\\xfe\\xff') \\\n       and (data[2:4] != '\\x00\\x00'):\n    encoding = 'utf-16be'\n    data = data[2:]\nelif (len(data) >= 4) and (data[:2] == '\\xff\\xfe') \\\n         and (data[2:4] != '\\x00\\x00'):\n    encoding = 'utf-16le'\n    data = data[2:]\nelif data[:3] == '\\xef\\xbb\\xbf':\n    encoding = 'utf-8'\n    data = data[3:]\nelif data[:4] == '\\x00\\x00\\xfe\\xff':\n    encoding = 'utf-32be'\n    data = data[4:]\nelif data[:4] == '\\xff\\xfe\\x00\\x00':\n    encoding = 'utf-32le'\n    data = data[4:]\nnewdata = unicode(data, encoding)\nreturn newdata", "path": "BeautifulSoup.py", "repo_name": "ptarjan/relmeauth", "stars": 15, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Encodes an object to a string in some encoding, or to Unicode.\n.\"\"\"\n", "func_signal": "def toEncoding(self, s, encoding=None):\n", "code": "if isinstance(s, unicode):\n    if encoding:\n        s = s.encode(encoding)\nelif isinstance(s, str):\n    if encoding:\n        s = s.encode(encoding)\n    else:\n        s = unicode(s)\nelse:\n    if encoding:\n        s  = self.toEncoding(str(s), encoding)\n    else:\n        s = unicode(s)\nreturn s", "path": "BeautifulSoup.py", "repo_name": "ptarjan/relmeauth", "stars": 15, "license": "None", "language": "python", "size": 104}
{"docstring": "#print \"Matching %s against %s\" % (markup, matchAgainst)\n", "func_signal": "def _matches(self, markup, matchAgainst):\n", "code": "result = False\nif matchAgainst is True:\n    result = markup is not None\nelif callable(matchAgainst):\n    result = matchAgainst(markup)\nelse:\n    #Custom match methods take the tag as an argument, but all\n    #other ways of matching match the tag name as a string.\n    if isinstance(markup, Tag):\n        markup = markup.name\n    if markup and not isinstance(markup, basestring):\n        markup = unicode(markup)\n    #Now we know that chunk is either a string, or None.\n    if hasattr(matchAgainst, 'match'):\n        # It's a regexp object.\n        result = markup and matchAgainst.search(markup)\n    elif hasattr(matchAgainst, '__iter__'): # list-like\n        result = markup in matchAgainst\n    elif hasattr(matchAgainst, 'items'):\n        result = markup.has_key(matchAgainst)\n    elif matchAgainst and isinstance(markup, basestring):\n        if isinstance(markup, unicode):\n            matchAgainst = unicode(matchAgainst)\n        else:\n            matchAgainst = str(matchAgainst)\n\n    if not result:\n        result = matchAgainst == markup\nreturn result", "path": "BeautifulSoup.py", "repo_name": "ptarjan/relmeauth", "stars": 15, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Handle a processing instruction as a ProcessingInstruction\nobject, possibly one with a %SOUP-ENCODING% slot into which an\nencoding will be plugged later.\"\"\"\n", "func_signal": "def handle_pi(self, text):\n", "code": "if text[:3] == \"xml\":\n    text = u\"xml version='1.0' encoding='%SOUP-ENCODING%'\"\nself._toStringSubclass(text, ProcessingInstruction)", "path": "BeautifulSoup.py", "repo_name": "ptarjan/relmeauth", "stars": 15, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Initializes a map representation of this tag's attributes,\nif not already initialized.\"\"\"\n", "func_signal": "def _getAttrMap(self):\n", "code": "if not getattr(self, 'attrMap'):\n    self.attrMap = {}\n    for (key, value) in self.attrs:\n        self.attrMap[key] = value\nreturn self.attrMap", "path": "BeautifulSoup.py", "repo_name": "ptarjan/relmeauth", "stars": 15, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Returns true iff the given string is the name of a\nself-closing tag according to this parser.\"\"\"\n", "func_signal": "def isSelfClosingTag(self, name):\n", "code": "return self.SELF_CLOSING_TAGS.has_key(name) \\\n       or self.instanceSelfClosingTags.has_key(name)", "path": "BeautifulSoup.py", "repo_name": "ptarjan/relmeauth", "stars": 15, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Handle entity references as data, possibly converting known\nHTML and/or XML entity references to the corresponding Unicode\ncharacters.\"\"\"\n", "func_signal": "def handle_entityref(self, ref):\n", "code": "data = None\nif self.convertHTMLEntities:\n    try:\n        data = unichr(name2codepoint[ref])\n    except KeyError:\n        pass\n\nif not data and self.convertXMLEntities:\n        data = self.XML_ENTITIES_TO_SPECIAL_CHARS.get(ref)\n\nif not data and self.convertHTMLEntities and \\\n    not self.XML_ENTITIES_TO_SPECIAL_CHARS.get(ref):\n        # TODO: We've got a problem here. We're told this is\n        # an entity reference, but it's not an XML entity\n        # reference or an HTML entity reference. Nonetheless,\n        # the logical thing to do is to pass it through as an\n        # unrecognized entity reference.\n        #\n        # Except: when the input is \"&carol;\" this function\n        # will be called with input \"carol\". When the input is\n        # \"AT&T\", this function will be called with input\n        # \"T\". We have no way of knowing whether a semicolon\n        # was present originally, so we don't know whether\n        # this is an unknown entity or just a misplaced\n        # ampersand.\n        #\n        # The more common case is a misplaced ampersand, so I\n        # escape the ampersand and omit the trailing semicolon.\n        data = \"&amp;%s\" % ref\nif not data:\n    # This case is different from the one above, because we\n    # haven't already gone through a supposedly comprehensive\n    # mapping of entities to Unicode characters. We might not\n    # have gone through any mapping at all. So the chances are\n    # very high that this is a real entity, and not a\n    # misplaced ampersand.\n    data = \"&%s;\" % ref\nself.handle_data(data)", "path": "BeautifulSoup.py", "repo_name": "ptarjan/relmeauth", "stars": 15, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Given a document, tries to detect its XML encoding.\"\"\"\n", "func_signal": "def _detectEncoding(self, xml_data, isHTML=False):\n", "code": "xml_encoding = sniffed_xml_encoding = None\ntry:\n    if xml_data[:4] == '\\x4c\\x6f\\xa7\\x94':\n        # EBCDIC\n        xml_data = self._ebcdic_to_ascii(xml_data)\n    elif xml_data[:4] == '\\x00\\x3c\\x00\\x3f':\n        # UTF-16BE\n        sniffed_xml_encoding = 'utf-16be'\n        xml_data = unicode(xml_data, 'utf-16be').encode('utf-8')\n    elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xfe\\xff') \\\n             and (xml_data[2:4] != '\\x00\\x00'):\n        # UTF-16BE with BOM\n        sniffed_xml_encoding = 'utf-16be'\n        xml_data = unicode(xml_data[2:], 'utf-16be').encode('utf-8')\n    elif xml_data[:4] == '\\x3c\\x00\\x3f\\x00':\n        # UTF-16LE\n        sniffed_xml_encoding = 'utf-16le'\n        xml_data = unicode(xml_data, 'utf-16le').encode('utf-8')\n    elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xff\\xfe') and \\\n             (xml_data[2:4] != '\\x00\\x00'):\n        # UTF-16LE with BOM\n        sniffed_xml_encoding = 'utf-16le'\n        xml_data = unicode(xml_data[2:], 'utf-16le').encode('utf-8')\n    elif xml_data[:4] == '\\x00\\x00\\x00\\x3c':\n        # UTF-32BE\n        sniffed_xml_encoding = 'utf-32be'\n        xml_data = unicode(xml_data, 'utf-32be').encode('utf-8')\n    elif xml_data[:4] == '\\x3c\\x00\\x00\\x00':\n        # UTF-32LE\n        sniffed_xml_encoding = 'utf-32le'\n        xml_data = unicode(xml_data, 'utf-32le').encode('utf-8')\n    elif xml_data[:4] == '\\x00\\x00\\xfe\\xff':\n        # UTF-32BE with BOM\n        sniffed_xml_encoding = 'utf-32be'\n        xml_data = unicode(xml_data[4:], 'utf-32be').encode('utf-8')\n    elif xml_data[:4] == '\\xff\\xfe\\x00\\x00':\n        # UTF-32LE with BOM\n        sniffed_xml_encoding = 'utf-32le'\n        xml_data = unicode(xml_data[4:], 'utf-32le').encode('utf-8')\n    elif xml_data[:3] == '\\xef\\xbb\\xbf':\n        # UTF-8 with BOM\n        sniffed_xml_encoding = 'utf-8'\n        xml_data = unicode(xml_data[3:], 'utf-8').encode('utf-8')\n    else:\n        sniffed_xml_encoding = 'ascii'\n        pass\nexcept:\n    xml_encoding_match = None\nxml_encoding_match = re.compile(\n    '^<\\?.*encoding=[\\'\"](.*?)[\\'\"].*\\?>').match(xml_data)\nif not xml_encoding_match and isHTML:\n    regexp = re.compile('<\\s*meta[^>]+charset=([^>]*?)[;\\'\">]', re.I)\n    xml_encoding_match = regexp.search(xml_data)\nif xml_encoding_match is not None:\n    xml_encoding = xml_encoding_match.groups()[0].lower()\n    if isHTML:\n        self.declaredHTMLEncoding = xml_encoding\n    if sniffed_xml_encoding and \\\n       (xml_encoding in ('iso-10646-ucs-2', 'ucs-2', 'csunicode',\n                         'iso-10646-ucs-4', 'ucs-4', 'csucs4',\n                         'utf-16', 'utf-32', 'utf_16', 'utf_32',\n                         'utf16', 'u16')):\n        xml_encoding = sniffed_xml_encoding\nreturn xml_data, xml_encoding, sniffed_xml_encoding", "path": "BeautifulSoup.py", "repo_name": "ptarjan/relmeauth", "stars": 15, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Create a new NavigableString.\n\nWhen unpickling a NavigableString, this method is called with\nthe string in DEFAULT_OUTPUT_ENCODING. That encoding needs to be\npassed in to the superclass's __new__ or the superclass won't know\nhow to handle non-ASCII characters.\n\"\"\"\n", "func_signal": "def __new__(cls, value):\n", "code": "if isinstance(value, unicode):\n    return unicode.__new__(cls, value)\nreturn unicode.__new__(cls, value, DEFAULT_OUTPUT_ENCODING)", "path": "BeautifulSoup.py", "repo_name": "ptarjan/relmeauth", "stars": 15, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Sets up the initial relations between this element and\nother elements.\"\"\"\n", "func_signal": "def setup(self, parent=None, previous=None):\n", "code": "self.parent = parent\nself.previous = previous\nself.next = None\nself.previousSibling = None\nself.nextSibling = None\nif self.parent and self.parent.contents:\n    self.previousSibling = self.parent.contents[-1]\n    self.previousSibling.nextSibling = self", "path": "BeautifulSoup.py", "repo_name": "ptarjan/relmeauth", "stars": 15, "license": "None", "language": "python", "size": 104}
{"docstring": "#print \"Push\", tag.name\n", "func_signal": "def pushTag(self, tag):\n", "code": "if self.currentTag:\n    self.currentTag.contents.append(tag)\nself.tagStack.append(tag)\nself.currentTag = self.tagStack[-1]", "path": "BeautifulSoup.py", "repo_name": "ptarjan/relmeauth", "stars": 15, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Adds a certain piece of text to the tree as a NavigableString\nsubclass.\"\"\"\n", "func_signal": "def _toStringSubclass(self, text, subclass):\n", "code": "self.endData()\nself.handle_data(text)\nself.endData(subclass)", "path": "BeautifulSoup.py", "repo_name": "ptarjan/relmeauth", "stars": 15, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Returns the parents of this Tag that match the given\ncriteria.\"\"\"\n\n", "func_signal": "def findParents(self, name=None, attrs={}, limit=None, **kwargs):\n", "code": "return self._findAll(name, attrs, None, limit, self.parentGenerator,\n                     **kwargs)", "path": "BeautifulSoup.py", "repo_name": "ptarjan/relmeauth", "stars": 15, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Setting tag[key] sets the value of the 'key' attribute for the\ntag.\"\"\"\n", "func_signal": "def __setitem__(self, key, value):\n", "code": "self._getAttrMap()\nself.attrMap[key] = value\nfound = False\nfor i in range(0, len(self.attrs)):\n    if self.attrs[i][0] == key:\n        self.attrs[i] = (key, value)\n        found = True\nif not found:\n    self.attrs.append((key, value))\nself._getAttrMap()[key] = value", "path": "BeautifulSoup.py", "repo_name": "ptarjan/relmeauth", "stars": 15, "license": "None", "language": "python", "size": 104}
{"docstring": "# Convert the document to Unicode.\n", "func_signal": "def _feed(self, inDocumentEncoding=None, isHTML=False):\n", "code": "markup = self.markup\nif isinstance(markup, unicode):\n    if not hasattr(self, 'originalEncoding'):\n        self.originalEncoding = None\nelse:\n    dammit = UnicodeDammit\\\n             (markup, [self.fromEncoding, inDocumentEncoding],\n              smartQuotesTo=self.smartQuotesTo, isHTML=isHTML)\n    markup = dammit.unicode\n    self.originalEncoding = dammit.originalEncoding\n    self.declaredHTMLEncoding = dammit.declaredHTMLEncoding\nif markup:\n    if self.markupMassage:\n        if not hasattr(self.markupMassage, \"__iter__\"):\n            self.markupMassage = self.MARKUP_MASSAGE\n        for fix, m in self.markupMassage:\n            markup = fix.sub(m, markup)\n        # TODO: We get rid of markupMassage so that the\n        # soup object can be deepcopied later on. Some\n        # Python installations can't copy regexes. If anyone\n        # was relying on the existence of markupMassage, this\n        # might cause problems.\n        del(self.markupMassage)\nself.reset()\n\nSGMLParser.feed(self, markup)\n# Close out any unfinished strings and close all the open tags.\nself.endData()\nwhile self.currentTag.name != self.ROOT_TAG_NAME:\n    self.popTag()", "path": "BeautifulSoup.py", "repo_name": "ptarjan/relmeauth", "stars": 15, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Returns the closest sibling to this Tag that matches the\ngiven criteria and appears before this Tag in the document.\"\"\"\n", "func_signal": "def findPreviousSibling(self, name=None, attrs={}, text=None, **kwargs):\n", "code": "return self._findOne(self.findPreviousSiblings, name, attrs, text,\n                     **kwargs)", "path": "BeautifulSoup.py", "repo_name": "ptarjan/relmeauth", "stars": 15, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"This method routes method call requests to either the SGMLParser\nsuperclass or the Tag superclass, depending on the method name.\"\"\"\n#print \"__getattr__ called on %s.%s\" % (self.__class__, methodName)\n\n", "func_signal": "def __getattr__(self, methodName):\n", "code": "if methodName.startswith('start_') or methodName.startswith('end_') \\\n       or methodName.startswith('do_'):\n    return SGMLParser.__getattr__(self, methodName)\nelif not methodName.startswith('__'):\n    return Tag.__getattr__(self, methodName)\nelse:\n    raise AttributeError", "path": "BeautifulSoup.py", "repo_name": "ptarjan/relmeauth", "stars": 15, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Pops the tag stack up to and including the most recent\ninstance of the given tag. If inclusivePop is false, pops the tag\nstack up to but *not* including the most recent instqance of\nthe given tag.\"\"\"\n#print \"Popping to %s\" % name\n", "func_signal": "def _popToTag(self, name, inclusivePop=True):\n", "code": "if name == self.ROOT_TAG_NAME:\n    return\n\nnumPops = 0\nmostRecentTag = None\nfor i in range(len(self.tagStack)-1, 0, -1):\n    if name == self.tagStack[i].name:\n        numPops = len(self.tagStack)-i\n        break\nif not inclusivePop:\n    numPops = numPops - 1\n\nfor i in range(0, numPops):\n    mostRecentTag = self.popTag()\nreturn mostRecentTag", "path": "BeautifulSoup.py", "repo_name": "ptarjan/relmeauth", "stars": 15, "license": "None", "language": "python", "size": 104}
{"docstring": "# override internal declaration handler to handle CDATA blocks\n", "func_signal": "def parse_declaration(self, i):\n", "code": "if _debug: sys.stderr.write('entering parse_declaration\\n')\nif self.rawdata[i:i+9] == '<![CDATA[':\n    k = self.rawdata.find(']]>', i)\n    if k == -1: k = len(self.rawdata)\n    self.handle_data(_xmlescape(self.rawdata[i+9:k]), 0)\n    return k+3\nelse:\n    k = self.rawdata.find('>', i)\n    return k+1", "path": "feedparser.py", "repo_name": "directeur/sugoi", "stars": 8, "license": "None", "language": "python", "size": 120}
{"docstring": "'''Parse a string according to the OnBlog 8-bit date format'''\n", "func_signal": "def _parse_date_onblog(dateString):\n", "code": "m = _korean_onblog_date_re.match(dateString)\nif not m: return\nw3dtfdate = '%(year)s-%(month)s-%(day)sT%(hour)s:%(minute)s:%(second)s%(zonediff)s' % \\\n            {'year': m.group(1), 'month': m.group(2), 'day': m.group(3),\\\n             'hour': m.group(4), 'minute': m.group(5), 'second': m.group(6),\\\n             'zonediff': '+09:00'}\nif _debug: sys.stderr.write('OnBlog date parsed as: %s\\n' % w3dtfdate)\nreturn _parse_date_w3dtf(w3dtfdate)", "path": "feedparser.py", "repo_name": "directeur/sugoi", "stars": 8, "license": "None", "language": "python", "size": 120}
{"docstring": "# called for each entity reference, e.g. for '&copy;', ref will be 'copy'\n", "func_signal": "def handle_entityref(self, ref):\n", "code": "if not self.elementstack: return\nif _debug: sys.stderr.write('entering handle_entityref with %s\\n' % ref)\nif ref in ('lt', 'gt', 'quot', 'amp', 'apos'):\n    text = '&%s;' % ref\nelif ref in self.entities.keys():\n    text = self.entities[ref]\n    if text.startswith('&#') and text.endswith(';'):\n        return self.handle_entityref(text)\nelse:\n    try: name2codepoint[ref]\n    except KeyError: text = '&%s;' % ref\n    else: text = unichr(name2codepoint[ref]).encode('utf-8')\nself.elementstack[-1][2].append(text)", "path": "feedparser.py", "repo_name": "directeur/sugoi", "stars": 8, "license": "None", "language": "python", "size": 120}
{"docstring": "\"\"\"parse a date in yyyy/mm/dd hh:mm:ss TTT format\"\"\"\n# Fri, 2006/09/15 08:19:53 EDT\n", "func_signal": "def _parse_date_perforce(aDateString):\n", "code": "_my_date_pattern = re.compile( \\\n\tr'(\\w{,3}), (\\d{,4})/(\\d{,2})/(\\d{2}) (\\d{,2}):(\\d{2}):(\\d{2}) (\\w{,3})')\n\ndow, year, month, day, hour, minute, second, tz = \\\n\t_my_date_pattern.search(aDateString).groups()\nmonths = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\ndateString = \"%s, %s %s %s %s:%s:%s %s\" % (dow, day, months[int(month) - 1], year, hour, minute, second, tz)\ntm = rfc822.parsedate_tz(dateString)\nif tm:\n\treturn time.gmtime(rfc822.mktime_tz(tm))", "path": "feedparser.py", "repo_name": "directeur/sugoi", "stars": 8, "license": "None", "language": "python", "size": 120}
{"docstring": "'''Strips DOCTYPE from XML document, returns (rss_version, stripped_data)\n\nrss_version may be 'rss091n' or None\nstripped_data is the same XML document, minus the DOCTYPE\n'''\n", "func_signal": "def _stripDoctype(data):\n", "code": "start = re.search('<\\w',data)\nstart = start and start.start() or -1\nhead,data = data[:start+1], data[start+1:]\n\nentity_pattern = re.compile(r'^\\s*<!ENTITY([^>]*?)>', re.MULTILINE)\nentity_results=entity_pattern.findall(head)\nhead = entity_pattern.sub('', head)\ndoctype_pattern = re.compile(r'^\\s*<!DOCTYPE([^>]*?)>', re.MULTILINE)\ndoctype_results = doctype_pattern.findall(head)\ndoctype = doctype_results and doctype_results[0] or ''\nif doctype.lower().count('netscape'):\n    version = 'rss091n'\nelse:\n    version = None\n\n# only allow in 'safe' inline entity definitions\nreplacement=''\nif len(doctype_results)==1 and entity_results:\n   safe_pattern=re.compile('\\s+(\\w+)\\s+\"(&#\\w+;|[^&\"]*)\"')\n   safe_entities=filter(lambda e: safe_pattern.match(e),entity_results)\n   if safe_entities:\n       replacement='<!DOCTYPE feed [\\n  <!ENTITY %s>\\n]>' % '>\\n  <!ENTITY '.join(safe_entities)\ndata = doctype_pattern.sub(replacement, head) + data\n\nreturn version, data, dict(replacement and safe_pattern.findall(replacement))", "path": "feedparser.py", "repo_name": "directeur/sugoi", "stars": 8, "license": "None", "language": "python", "size": 120}
{"docstring": "# called for each character reference, e.g. for '&#160;', ref will be '160'\n", "func_signal": "def handle_charref(self, ref):\n", "code": "if not self.elementstack: return\nref = ref.lower()\nif ref in ('34', '38', '39', '60', '62', 'x22', 'x26', 'x27', 'x3c', 'x3e'):\n    text = '&#%s;' % ref\nelse:\n    if ref[0] == 'x':\n        c = int(ref[1:], 16)\n    else:\n        c = int(ref)\n    text = unichr(c).encode('utf-8')\nself.elementstack[-1][2].append(text)", "path": "feedparser.py", "repo_name": "directeur/sugoi", "stars": 8, "license": "None", "language": "python", "size": 120}
{"docstring": "'''Parse a string according to the MS SQL date format'''\n", "func_signal": "def _parse_date_mssql(dateString):\n", "code": "m = _mssql_date_re.match(dateString)\nif not m: return\nw3dtfdate = '%(year)s-%(month)s-%(day)sT%(hour)s:%(minute)s:%(second)s%(zonediff)s' % \\\n            {'year': m.group(1), 'month': m.group(2), 'day': m.group(3),\\\n             'hour': m.group(4), 'minute': m.group(5), 'second': m.group(6),\\\n             'zonediff': '+09:00'}\nif _debug: sys.stderr.write('MS SQL date parsed as: %s\\n' % w3dtfdate)\nreturn _parse_date_w3dtf(w3dtfdate)", "path": "feedparser.py", "repo_name": "directeur/sugoi", "stars": 8, "license": "None", "language": "python", "size": 120}
{"docstring": "'''Parse a string according to a Greek 8-bit date format.'''\n", "func_signal": "def _parse_date_greek(dateString):\n", "code": "m = _greek_date_format_re.match(dateString)\nif not m: return\ntry:\n    wday = _greek_wdays[m.group(1)]\n    month = _greek_months[m.group(3)]\nexcept:\n    return\nrfc822date = '%(wday)s, %(day)s %(month)s %(year)s %(hour)s:%(minute)s:%(second)s %(zonediff)s' % \\\n             {'wday': wday, 'day': m.group(2), 'month': month, 'year': m.group(4),\\\n              'hour': m.group(5), 'minute': m.group(6), 'second': m.group(7),\\\n              'zonediff': m.group(8)}\nif _debug: sys.stderr.write('Greek date parsed as: %s\\n' % rfc822date)\nreturn _parse_date_rfc822(rfc822date)", "path": "feedparser.py", "repo_name": "directeur/sugoi", "stars": 8, "license": "None", "language": "python", "size": 120}
{"docstring": "'''Get the character encoding of the XML document\n\nhttp_headers is a dictionary\nxml_data is a raw string (not Unicode)\n\nThis is so much trickier than it sounds, it's not even funny.\nAccording to RFC 3023 ('XML Media Types'), if the HTTP Content-Type\nis application/xml, application/*+xml,\napplication/xml-external-parsed-entity, or application/xml-dtd,\nthe encoding given in the charset parameter of the HTTP Content-Type\ntakes precedence over the encoding given in the XML prefix within the\ndocument, and defaults to 'utf-8' if neither are specified.  But, if\nthe HTTP Content-Type is text/xml, text/*+xml, or\ntext/xml-external-parsed-entity, the encoding given in the XML prefix\nwithin the document is ALWAYS IGNORED and only the encoding given in\nthe charset parameter of the HTTP Content-Type header should be\nrespected, and it defaults to 'us-ascii' if not specified.\n\nFurthermore, discussion on the atom-syntax mailing list with the\nauthor of RFC 3023 leads me to the conclusion that any document\nserved with a Content-Type of text/* and no charset parameter\nmust be treated as us-ascii.  (We now do this.)  And also that it\nmust always be flagged as non-well-formed.  (We now do this too.)\n\nIf Content-Type is unspecified (input was local file or non-HTTP source)\nor unrecognized (server just got it totally wrong), then go by the\nencoding given in the XML prefix of the document and default to\n'iso-8859-1' as per the HTTP specification (RFC 2616).\n\nThen, assuming we didn't find a character encoding in the HTTP headers\n(and the HTTP Content-type allowed us to look in the body), we need\nto sniff the first few bytes of the XML data and try to determine\nwhether the encoding is ASCII-compatible.  Section F of the XML\nspecification shows the way here:\nhttp://www.w3.org/TR/REC-xml/#sec-guessing-no-ext-info\n\nIf the sniffed encoding is not ASCII-compatible, we need to make it\nASCII compatible so that we can sniff further into the XML declaration\nto find the encoding attribute, which will tell us the true encoding.\n\nOf course, none of this guarantees that we will be able to parse the\nfeed in the declared character encoding (assuming it was declared\ncorrectly, which many are not).  CJKCodecs and iconv_codec help a lot;\nyou should definitely install them if you can.\nhttp://cjkpython.i18n.org/\n'''\n\n", "func_signal": "def _getCharacterEncoding(http_headers, xml_data):\n", "code": "def _parseHTTPContentType(content_type):\n    '''takes HTTP Content-Type header and returns (content type, charset)\n\n    If no charset is specified, returns (content type, '')\n    If no content type is specified, returns ('', '')\n    Both return parameters are guaranteed to be lowercase strings\n    '''\n    content_type = content_type or ''\n    content_type, params = cgi.parse_header(content_type)\n    return content_type, params.get('charset', '').replace(\"'\", '')\n\nsniffed_xml_encoding = ''\nxml_encoding = ''\ntrue_encoding = ''\nhttp_content_type, http_encoding = _parseHTTPContentType(http_headers.get('content-type'))\n# Must sniff for non-ASCII-compatible character encodings before\n# searching for XML declaration.  This heuristic is defined in\n# section F of the XML specification:\n# http://www.w3.org/TR/REC-xml/#sec-guessing-no-ext-info\ntry:\n    if xml_data[:4] == '\\x4c\\x6f\\xa7\\x94':\n        # EBCDIC\n        xml_data = _ebcdic_to_ascii(xml_data)\n    elif xml_data[:4] == '\\x00\\x3c\\x00\\x3f':\n        # UTF-16BE\n        sniffed_xml_encoding = 'utf-16be'\n        xml_data = unicode(xml_data, 'utf-16be').encode('utf-8')\n    elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xfe\\xff') and (xml_data[2:4] != '\\x00\\x00'):\n        # UTF-16BE with BOM\n        sniffed_xml_encoding = 'utf-16be'\n        xml_data = unicode(xml_data[2:], 'utf-16be').encode('utf-8')\n    elif xml_data[:4] == '\\x3c\\x00\\x3f\\x00':\n        # UTF-16LE\n        sniffed_xml_encoding = 'utf-16le'\n        xml_data = unicode(xml_data, 'utf-16le').encode('utf-8')\n    elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xff\\xfe') and (xml_data[2:4] != '\\x00\\x00'):\n        # UTF-16LE with BOM\n        sniffed_xml_encoding = 'utf-16le'\n        xml_data = unicode(xml_data[2:], 'utf-16le').encode('utf-8')\n    elif xml_data[:4] == '\\x00\\x00\\x00\\x3c':\n        # UTF-32BE\n        sniffed_xml_encoding = 'utf-32be'\n        xml_data = unicode(xml_data, 'utf-32be').encode('utf-8')\n    elif xml_data[:4] == '\\x3c\\x00\\x00\\x00':\n        # UTF-32LE\n        sniffed_xml_encoding = 'utf-32le'\n        xml_data = unicode(xml_data, 'utf-32le').encode('utf-8')\n    elif xml_data[:4] == '\\x00\\x00\\xfe\\xff':\n        # UTF-32BE with BOM\n        sniffed_xml_encoding = 'utf-32be'\n        xml_data = unicode(xml_data[4:], 'utf-32be').encode('utf-8')\n    elif xml_data[:4] == '\\xff\\xfe\\x00\\x00':\n        # UTF-32LE with BOM\n        sniffed_xml_encoding = 'utf-32le'\n        xml_data = unicode(xml_data[4:], 'utf-32le').encode('utf-8')\n    elif xml_data[:3] == '\\xef\\xbb\\xbf':\n        # UTF-8 with BOM\n        sniffed_xml_encoding = 'utf-8'\n        xml_data = unicode(xml_data[3:], 'utf-8').encode('utf-8')\n    else:\n        # ASCII-compatible\n        pass\n    xml_encoding_match = re.compile('^<\\?.*encoding=[\\'\"](.*?)[\\'\"].*\\?>').match(xml_data)\nexcept:\n    xml_encoding_match = None\nif xml_encoding_match:\n    xml_encoding = xml_encoding_match.groups()[0].lower()\n    if sniffed_xml_encoding and (xml_encoding in ('iso-10646-ucs-2', 'ucs-2', 'csunicode', 'iso-10646-ucs-4', 'ucs-4', 'csucs4', 'utf-16', 'utf-32', 'utf_16', 'utf_32', 'utf16', 'u16')):\n        xml_encoding = sniffed_xml_encoding\nacceptable_content_type = 0\napplication_content_types = ('application/xml', 'application/xml-dtd', 'application/xml-external-parsed-entity')\ntext_content_types = ('text/xml', 'text/xml-external-parsed-entity')\nif (http_content_type in application_content_types) or \\\n   (http_content_type.startswith('application/') and http_content_type.endswith('+xml')):\n    acceptable_content_type = 1\n    true_encoding = http_encoding or xml_encoding or 'utf-8'\nelif (http_content_type in text_content_types) or \\\n     (http_content_type.startswith('text/')) and http_content_type.endswith('+xml'):\n    acceptable_content_type = 1\n    true_encoding = http_encoding or 'us-ascii'\nelif http_content_type.startswith('text/'):\n    true_encoding = http_encoding or 'us-ascii'\nelif http_headers and (not http_headers.has_key('content-type')):\n    true_encoding = xml_encoding or 'iso-8859-1'\nelse:\n    true_encoding = xml_encoding or 'utf-8'\n# some feeds claim to be gb2312 but are actually gb18030.\n# apparently MSIE and Firefox both do the following switch:\nif true_encoding.lower() == 'gb2312':\n    true_encoding = 'gb18030'\nreturn true_encoding, http_encoding, xml_encoding, sniffed_xml_encoding, acceptable_content_type", "path": "feedparser.py", "repo_name": "directeur/sugoi", "stars": 8, "license": "None", "language": "python", "size": 120}
{"docstring": "# Check if\n# - server requires digest auth, AND\n# - we tried (unsuccessfully) with basic auth, AND\n# - we're using Python 2.3.3 or later (digest auth is irreparably broken in earlier versions)\n# If all conditions hold, parse authentication information\n# out of the Authorization header we sent the first time\n# (for the username and password) and the WWW-Authenticate\n# header the server sent back (for the realm) and retry\n# the request with the appropriate digest auth headers instead.\n# This evil genius hack has been brought to you by Aaron Swartz.\n", "func_signal": "def http_error_401(self, req, fp, code, msg, headers):\n", "code": "host = urlparse.urlparse(req.get_full_url())[1]\ntry:\n    assert sys.version.split()[0] >= '2.3.3'\n    assert base64 != None\n    user, passw = base64.decodestring(req.headers['Authorization'].split(' ')[1]).split(':')\n    realm = re.findall('realm=\"([^\"]*)\"', headers['WWW-Authenticate'])[0]\n    self.add_password(realm, host, user, passw)\n    retry = self.http_error_auth_reqed('www-authenticate', host, req, headers)\n    self.reset_retry_count()\n    return retry\nexcept:\n    return self.http_error_default(req, fp, code, msg, headers)", "path": "feedparser.py", "repo_name": "directeur/sugoi", "stars": 8, "license": "None", "language": "python", "size": 120}
{"docstring": "# called for each character reference, e.g. for '&#160;', ref will be '160'\n# Reconstruct the original character reference.\n", "func_signal": "def handle_charref(self, ref):\n", "code": "if ref.startswith('x'):\n    value = unichr(int(ref[1:],16))\nelse:\n    value = unichr(int(ref))\n\nif value in _cp1252.keys():\n    self.pieces.append('&#%s;' % hex(ord(_cp1252[value]))[1:])\nelse:\n    self.pieces.append('&#%(ref)s;' % locals())", "path": "feedparser.py", "repo_name": "directeur/sugoi", "stars": 8, "license": "None", "language": "python", "size": 120}
{"docstring": "# called for each entity reference, e.g. for '&copy;', ref will be 'copy'\n# Reconstruct the original entity reference.\n", "func_signal": "def handle_entityref(self, ref):\n", "code": "if name2codepoint.has_key(ref):\n    self.pieces.append('&%(ref)s;' % locals())\nelse:\n    self.pieces.append('&amp;%(ref)s' % locals())", "path": "feedparser.py", "repo_name": "directeur/sugoi", "stars": 8, "license": "None", "language": "python", "size": 120}
{"docstring": "# called for each block of plain text, i.e. outside of any tag and\n# not containing any character or entity references\n# Store the original text verbatim.\n", "func_signal": "def handle_data(self, text):\n", "code": "if _debug: sys.stderr.write('_BaseHTMLProcessor, handle_text, text=%s\\n' % text)\nself.pieces.append(text)", "path": "feedparser.py", "repo_name": "directeur/sugoi", "stars": 8, "license": "None", "language": "python", "size": 120}
{"docstring": "\"\"\"URL, filename, or string --> stream\n\nThis function lets you define parsers that take any input source\n(URL, pathname to local or network file, or actual data as a string)\nand deal with it in a uniform manner.  Returned object is guaranteed\nto have all the basic stdio read methods (read, readline, readlines).\nJust .close() the object when you're done with it.\n\nIf the etag argument is supplied, it will be used as the value of an\nIf-None-Match request header.\n\nIf the modified argument is supplied, it can be a tuple of 9 integers\n(as returned by gmtime() in the standard Python time module) or a date\nstring in any format supported by feedparser. Regardless, it MUST\nbe in GMT (Greenwich Mean Time). It will be reformatted into an\nRFC 1123-compliant date and used as the value of an If-Modified-Since\nrequest header.\n\nIf the agent argument is supplied, it will be used as the value of a\nUser-Agent request header.\n\nIf the referrer argument is supplied, it will be used as the value of a\nReferer[sic] request header.\n\nIf handlers is supplied, it is a list of handlers used to build a\nurllib2 opener.\n\"\"\"\n\n", "func_signal": "def _open_resource(url_file_stream_or_string, etag, modified, agent, referrer, handlers):\n", "code": "if hasattr(url_file_stream_or_string, 'read'):\n    return url_file_stream_or_string\n\nif url_file_stream_or_string == '-':\n    return sys.stdin\n\nif urlparse.urlparse(url_file_stream_or_string)[0] in ('http', 'https', 'ftp'):\n    if not agent:\n        agent = USER_AGENT\n    # test for inline user:password for basic auth\n    auth = None\n    if base64:\n        urltype, rest = urllib.splittype(url_file_stream_or_string)\n        realhost, rest = urllib.splithost(rest)\n        if realhost:\n            user_passwd, realhost = urllib.splituser(realhost)\n            if user_passwd:\n                url_file_stream_or_string = '%s://%s%s' % (urltype, realhost, rest)\n                auth = base64.encodestring(user_passwd).strip()\n\n    # iri support\n    try:\n        if isinstance(url_file_stream_or_string,unicode):\n            url_file_stream_or_string = url_file_stream_or_string.encode('idna')\n        else:\n            url_file_stream_or_string = url_file_stream_or_string.decode('utf-8').encode('idna')\n    except:\n        pass\n\n    # try to open with urllib2 (to use optional headers)\n    request = urllib2.Request(url_file_stream_or_string)\n    request.add_header('User-Agent', agent)\n    if etag:\n        request.add_header('If-None-Match', etag)\n    if type(modified) == type(''):\n        modified = _parse_date(modified)\n    if modified:\n        # format into an RFC 1123-compliant timestamp. We can't use\n        # time.strftime() since the %a and %b directives can be affected\n        # by the current locale, but RFC 2616 states that dates must be\n        # in English.\n        short_weekdays = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n        months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n        request.add_header('If-Modified-Since', '%s, %02d %s %04d %02d:%02d:%02d GMT' % (short_weekdays[modified[6]], modified[2], months[modified[1] - 1], modified[0], modified[3], modified[4], modified[5]))\n    if referrer:\n        request.add_header('Referer', referrer)\n    if gzip and zlib:\n        request.add_header('Accept-encoding', 'gzip, deflate')\n    elif gzip:\n        request.add_header('Accept-encoding', 'gzip')\n    elif zlib:\n        request.add_header('Accept-encoding', 'deflate')\n    else:\n        request.add_header('Accept-encoding', '')\n    if auth:\n        request.add_header('Authorization', 'Basic %s' % auth)\n    if ACCEPT_HEADER:\n        request.add_header('Accept', ACCEPT_HEADER)\n    request.add_header('A-IM', 'feed') # RFC 3229 support\n    opener = apply(urllib2.build_opener, tuple([_FeedURLHandler()] + handlers))\n    opener.addheaders = [] # RMK - must clear so we only send our custom User-Agent\n    try:\n        return opener.open(request)\n    finally:\n        opener.close() # JohnD\n\n# try to open with native open function (if url_file_stream_or_string is a filename)\ntry:\n    return open(url_file_stream_or_string)\nexcept:\n    pass\n\n# treat url_file_stream_or_string as string\nreturn _StringIO(str(url_file_stream_or_string))", "path": "feedparser.py", "repo_name": "directeur/sugoi", "stars": 8, "license": "None", "language": "python", "size": 120}
{"docstring": "# called for each block of plain text, i.e. outside of any tag and\n# not containing any character or entity references\n", "func_signal": "def handle_data(self, text, escape=1):\n", "code": "if not self.elementstack: return\nif escape and self.contentparams.get('type') == 'application/xhtml+xml':\n    text = _xmlescape(text)\nself.elementstack[-1][2].append(text)", "path": "feedparser.py", "repo_name": "directeur/sugoi", "stars": 8, "license": "None", "language": "python", "size": 120}
{"docstring": "# called for each end tag, e.g. for </pre>, tag will be 'pre'\n# Reconstruct the original end tag.\n", "func_signal": "def unknown_endtag(self, tag):\n", "code": "if tag not in self.elements_no_end_tag:\n    self.pieces.append(\"</%(tag)s>\" % locals())", "path": "feedparser.py", "repo_name": "directeur/sugoi", "stars": 8, "license": "None", "language": "python", "size": 120}
{"docstring": "# disallow urls\n", "func_signal": "def sanitize_style(self, style):\n", "code": "style=re.compile('url\\s*\\(\\s*[^\\s)]+?\\s*\\)\\s*').sub(' ',style)\n\n# gauntlet\nif not re.match(\"\"\"^([:,;#%.\\sa-zA-Z0-9!]|\\w-\\w|'[\\s\\w]+'|\"[\\s\\w]+\"|\\([\\d,\\s]+\\))*$\"\"\", style): return ''\nif not re.match(\"^(\\s*[-\\w]+\\s*:\\s*[^:;]*(;|$))*$\", style): return ''\n\nclean = []\nfor prop,value in re.findall(\"([-\\w]+)\\s*:\\s*([^:;]*)\",style):\n  if not value: continue\n  if prop.lower() in self.acceptable_css_properties:\n      clean.append(prop + ': ' + value + ';')\n  elif prop.split('-')[0].lower() in ['background','border','margin','padding']:\n      for keyword in value.split():\n          if not keyword in self.acceptable_css_keywords and \\\n              not self.valid_css_values.match(keyword):\n              break\n      else:\n          clean.append(prop + ': ' + value + ';')\n  elif self.svgOK and prop.lower() in self.acceptable_svg_properties:\n      clean.append(prop + ': ' + value + ';')\n\nreturn ' '.join(clean)", "path": "feedparser.py", "repo_name": "directeur/sugoi", "stars": 8, "license": "None", "language": "python", "size": 120}
{"docstring": "'''Changes an XML data stream on the fly to specify a new encoding\n\ndata is a raw sequence of bytes (not Unicode) that is presumed to be in %encoding already\nencoding is a string recognized by encodings.aliases\n'''\n", "func_signal": "def _toUTF8(data, encoding):\n", "code": "if _debug: sys.stderr.write('entering _toUTF8, trying encoding %s\\n' % encoding)\n# strip Byte Order Mark (if present)\nif (len(data) >= 4) and (data[:2] == '\\xfe\\xff') and (data[2:4] != '\\x00\\x00'):\n    if _debug:\n        sys.stderr.write('stripping BOM\\n')\n        if encoding != 'utf-16be':\n            sys.stderr.write('trying utf-16be instead\\n')\n    encoding = 'utf-16be'\n    data = data[2:]\nelif (len(data) >= 4) and (data[:2] == '\\xff\\xfe') and (data[2:4] != '\\x00\\x00'):\n    if _debug:\n        sys.stderr.write('stripping BOM\\n')\n        if encoding != 'utf-16le':\n            sys.stderr.write('trying utf-16le instead\\n')\n    encoding = 'utf-16le'\n    data = data[2:]\nelif data[:3] == '\\xef\\xbb\\xbf':\n    if _debug:\n        sys.stderr.write('stripping BOM\\n')\n        if encoding != 'utf-8':\n            sys.stderr.write('trying utf-8 instead\\n')\n    encoding = 'utf-8'\n    data = data[3:]\nelif data[:4] == '\\x00\\x00\\xfe\\xff':\n    if _debug:\n        sys.stderr.write('stripping BOM\\n')\n        if encoding != 'utf-32be':\n            sys.stderr.write('trying utf-32be instead\\n')\n    encoding = 'utf-32be'\n    data = data[4:]\nelif data[:4] == '\\xff\\xfe\\x00\\x00':\n    if _debug:\n        sys.stderr.write('stripping BOM\\n')\n        if encoding != 'utf-32le':\n            sys.stderr.write('trying utf-32le instead\\n')\n    encoding = 'utf-32le'\n    data = data[4:]\nnewdata = unicode(data, encoding)\nif _debug: sys.stderr.write('successfully converted %s data to unicode\\n' % encoding)\ndeclmatch = re.compile('^<\\?xml[^>]*?>')\nnewdecl = '''<?xml version='1.0' encoding='utf-8'?>'''\nif declmatch.search(newdata):\n    newdata = declmatch.sub(newdecl, newdata)\nelse:\n    newdata = newdecl + u'\\n' + newdata\nreturn newdata.encode('utf-8')", "path": "feedparser.py", "repo_name": "directeur/sugoi", "stars": 8, "license": "None", "language": "python", "size": 120}
{"docstring": "'''Parse a string according to the Nate 8-bit date format'''\n", "func_signal": "def _parse_date_nate(dateString):\n", "code": "m = _korean_nate_date_re.match(dateString)\nif not m: return\nhour = int(m.group(5))\nampm = m.group(4)\nif (ampm == _korean_pm):\n    hour += 12\nhour = str(hour)\nif len(hour) == 1:\n    hour = '0' + hour\nw3dtfdate = '%(year)s-%(month)s-%(day)sT%(hour)s:%(minute)s:%(second)s%(zonediff)s' % \\\n            {'year': m.group(1), 'month': m.group(2), 'day': m.group(3),\\\n             'hour': hour, 'minute': m.group(6), 'second': m.group(7),\\\n             'zonediff': '+09:00'}\nif _debug: sys.stderr.write('Nate date parsed as: %s\\n' % w3dtfdate)\nreturn _parse_date_w3dtf(w3dtfdate)", "path": "feedparser.py", "repo_name": "directeur/sugoi", "stars": 8, "license": "None", "language": "python", "size": 120}
{"docstring": "'''Parse a string according to a Hungarian 8-bit date format.'''\n", "func_signal": "def _parse_date_hungarian(dateString):\n", "code": "m = _hungarian_date_format_re.match(dateString)\nif not m: return\ntry:\n    month = _hungarian_months[m.group(2)]\n    day = m.group(3)\n    if len(day) == 1:\n        day = '0' + day\n    hour = m.group(4)\n    if len(hour) == 1:\n        hour = '0' + hour\nexcept:\n    return\nw3dtfdate = '%(year)s-%(month)s-%(day)sT%(hour)s:%(minute)s%(zonediff)s' % \\\n            {'year': m.group(1), 'month': month, 'day': day,\\\n             'hour': hour, 'minute': m.group(5),\\\n             'zonediff': m.group(6)}\nif _debug: sys.stderr.write('Hungarian date parsed as: %s\\n' % w3dtfdate)\nreturn _parse_date_w3dtf(w3dtfdate)", "path": "feedparser.py", "repo_name": "directeur/sugoi", "stars": 8, "license": "None", "language": "python", "size": 120}
