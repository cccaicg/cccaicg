{"docstring": "\"\"\"\nGet/set the value of this select (the selected option).\n\nIf this is a multi-select, this is a set-like object that\nrepresents all the selected options.\n\"\"\"\n", "func_signal": "def _value__get(self):\n", "code": "if self.multiple:\n    return MultipleSelectOptions(self)\nfor el in _options_xpath(self):\n    if el.get('selected') is not None:\n        return _value_from_option(el)\nreturn None", "path": "alfajor\\browsers\\_lxml.py", "repo_name": "idealistdev/alfajor", "stars": 37, "license": "other", "language": "python", "size": 292}
{"docstring": "# self._contexts is a list of tuples, [0] is the context key\n", "func_signal": "def stopContext(self, context):\n", "code": "if self._contexts and context == self._contexts[-1][0]:\n    key, managers = self._contexts.pop(-1)\n    for manager, declaration in managers:\n        manager.destroy()\n        declaration.proxy._instance = None\n        declaration.proxy._factory = None", "path": "alfajor\\runners\\nose.py", "repo_name": "idealistdev/alfajor", "stars": 37, "license": "other", "language": "python", "size": 292}
{"docstring": "# accept additional request headers?  (e.g. user agent)\n", "func_signal": "def __init__(self, base_url=None):\n", "code": "self._base_url = base_url\nself.reset()", "path": "alfajor\\browsers\\network.py", "repo_name": "idealistdev/alfajor", "stars": 37, "license": "other", "language": "python", "size": 292}
{"docstring": "# FIXME: should del be allowed at all?\n", "func_signal": "def _value__del(self):\n", "code": "if self.multiple:\n    self.value.clear()\nelse:\n    self.value = None", "path": "alfajor\\browsers\\_lxml.py", "repo_name": "idealistdev/alfajor", "stars": 37, "license": "other", "language": "python", "size": 292}
{"docstring": "\"\"\"Get the selenium instance for this test if one exists.\n\nOtherwise return None.\n\"\"\"\n", "func_signal": "def _getSelenium(self):\n", "code": "assert self._contexts\ncontexts, managers = self._contexts[-1]\nfor manager, declaration in managers:\n    instance = declaration.proxy._instance\n    if hasattr(instance, 'selenium'):\n        return instance.selenium\nreturn None", "path": "alfajor\\runners\\nose.py", "repo_name": "idealistdev/alfajor", "stars": 37, "license": "other", "language": "python", "size": 292}
{"docstring": "\"\"\"Transform *values* into a sequence of ('name', ['values']) pairs.\n\nFor use by form.fill().  Collapses repeats of a given name into a single\nlist of values.  (And non-repeated names as a list of one value.)\n\n:param values: a mapping or sequence of name/value pairs.\n\n:param with_prefix: optional, a string that all form fields should\n  start with.  If a supplied field name does not start with this\n  prefix, it will be prepended.\n\n\"\"\"\n", "func_signal": "def _group_key_value_pairs(values, with_prefix=''):\n", "code": "grouped = defaultdict(list)\ntransformed_keys = []\nfor key, value in to_pairs(values):\n    if with_prefix and not key.startswith(with_prefix):\n        key = with_prefix + key\n    grouped[key].append(value)\n    if key not in transformed_keys:\n        transformed_keys.append(key)\nreturn [(key, grouped[key]) for key in transformed_keys]", "path": "alfajor\\browsers\\_lxml.py", "repo_name": "idealistdev/alfajor", "stars": 37, "license": "other", "language": "python", "size": 292}
{"docstring": "# accept additional request headers?  (e.g. user agent)\n", "func_signal": "def __init__(self, wsgi_app, base_url=None):\n", "code": "self._wsgi_app = wsgi_app\nself._base_url = base_url\nself._referrer = None\nself._request_environ = None\nself._cookie_jar = CookieJar()\nself._charset = 'utf-8'\nself.status_code = 0\nself.status = ''\nself.response = None\nself.headers = ()", "path": "alfajor\\browsers\\wsgi.py", "repo_name": "idealistdev/alfajor", "stars": 37, "license": "other", "language": "python", "size": 292}
{"docstring": "\"\"\"Return name, value pairs of form data as a browser would submit.\"\"\"\n", "func_signal": "def form_values(self):\n", "code": "results = []\nfor name, elements in self.inputs.iteritems():\n    if not name:\n        continue\n    if elements[0].tag == 'input':\n        type = elements[0].type\n    else:\n        type = elements[0].tag\n    if type in ('submit', 'image', 'reset'):\n        continue\n    for el in elements:\n        value = el.value\n        if getattr(el, 'checkable', False):\n            if not el.checked:\n                continue\n            # emulate browser behavior for valueless checkboxes\n            results.append((name, value or 'on'))\n            continue\n        elif type == 'select':\n            if value is None:\n                # this won't be reached unless the first option is\n                # <option/>\n                options = el.cssselect('> option')\n                if options:\n                    results.append((name, u''))\n                continue\n            elif el.multiple:\n                for v in value:\n                    results.append((name, v))\n                continue\n        elif type == 'file':\n            if value:\n                mimetype = mimetypes.guess_type(value)[0] \\\n                        or 'application/octet-stream'\n                results.append((name, (value, mimetype)))\n                continue\n        results.append((name, value or u''))\nreturn results", "path": "alfajor\\browsers\\_lxml.py", "repo_name": "idealistdev/alfajor", "stars": 37, "license": "other", "language": "python", "size": 292}
{"docstring": "\"\"\"Return an environ to request *url*, including cookies.\"\"\"\n", "func_signal": "def _create_environ(self, url, method, data, refer, content_type=None):\n", "code": "environ_args = dict(self._wsgi_server, method=method)\nbase_url = self._referrer if refer else self._base_url\nenviron_args.update(self._canonicalize_url(url, base_url))\nenviron_args.update(self._prep_input(method, data, content_type))\nenviron = create_environ(**environ_args)\nif refer and self._referrer:\n    environ['HTTP_REFERER'] = self._referrer\nenviron.setdefault('REMOTE_ADDR', '127.0.0.1')\nself._cookie_jar.export_to_environ(environ)\nreturn environ", "path": "alfajor\\browsers\\wsgi.py", "repo_name": "idealistdev/alfajor", "stars": 37, "license": "other", "language": "python", "size": 292}
{"docstring": "\"\"\"An LXML tree of the :attr:`response` content.\"\"\"\n# TODO: document decision to use 'fromstring' (means dom may\n# be what the remote sent, may not.)\n", "func_signal": "def document(self):\n", "code": "if self.response is None:\n    return None\nreturn html_from_string(self.response, parser=self._lxml_parser)", "path": "alfajor\\browsers\\_lxml.py", "repo_name": "idealistdev/alfajor", "stars": 37, "license": "other", "language": "python", "size": 292}
{"docstring": "\"\"\"The text content of the tag and its children.\n\nThis property overrides the text_content() method of regular\nlxml.html elements.  Similar, but acts usable as an\nattribute or as a method call and normalizes all whitespace\nas single spaces.\n\n\"\"\"\n", "func_signal": "def text_content(self):\n", "code": "text = u' '.join(_collect_string_content(self).split())\nreturn callable_unicode(text)", "path": "alfajor\\browsers\\_lxml.py", "repo_name": "idealistdev/alfajor", "stars": 37, "license": "other", "language": "python", "size": 292}
{"docstring": "\"\"\"True if *needle* exists anywhere in the response content.\"\"\"\n# TODO: make this normalize whitespace?  something like split\n# *needle* on whitespace, build a regex of r'\\s+'-separated\n# bits.  this could be a fallback to a simple containment\n# test.\n", "func_signal": "def __contains__(self, needle):\n", "code": "document = self.document\nif document is None:\n    return False\nreturn needle in document.text_content", "path": "alfajor\\browsers\\_lxml.py", "repo_name": "idealistdev/alfajor", "stars": 37, "license": "other", "language": "python", "size": 292}
{"docstring": "\"\"\"Return encoded and packed POST data.\"\"\"\n", "func_signal": "def _prep_input(self, method, data, content_type):\n", "code": "if data is None or method != 'POST':\n    prepped = {\n        'input_stream': None,\n        'content_length': None,\n        'content_type': None,\n        }\n    if method == 'GET' and data:\n        qs = MultiDict()\n        for key, value in to_pairs(data):\n            qs.setlistdefault(key).append(value)\n        prepped['query_string'] = url_encode(qs)\n    return prepped\nelse:\n    payload = url_encode(MultiDict(to_pairs(data)))\n    content_type = 'application/x-www-form-urlencoded'\n    return {\n        'input_stream': StringIO(payload),\n        'content_length': len(payload),\n        'content_type': content_type\n        }", "path": "alfajor\\browsers\\wsgi.py", "repo_name": "idealistdev/alfajor", "stars": 37, "license": "other", "language": "python", "size": 292}
{"docstring": "\"\"\"\nAll the possible values this select can have (the ``value``\nattribute of all the ``<option>`` elements.\n\"\"\"\n", "func_signal": "def value_options(self):\n", "code": "options = []\nfor el in _options_xpath(self):\n    options.append(_value_from_option(el))\nreturn options", "path": "alfajor\\browsers\\_lxml.py", "repo_name": "idealistdev/alfajor", "stars": 37, "license": "other", "language": "python", "size": 292}
{"docstring": "\"\"\"Retrieve elements by integer index, id or CSS select query.\"\"\"\n", "func_signal": "def __getitem__(self, key):\n", "code": "if not isinstance(key, basestring):\n    return super(DOMElement, self).__getitem__(key)\n# '#foo'?  (and not '#foo li')\nif _single_id_selector.match(key):\n    try:\n        return self.get_element_by_id(key[1:])\n    except KeyError:\n        label = 'Document' if self.tag == 'html' else 'Fragment'\n        raise AssertionError(\"%s contains no element with \"\n                             \"id %r\" % (label, key))\n# 'li #foo'?  (and not 'li #foo li')\nelif _single_id_selector.search(key):\n    elements = self.cssselect(key)\n    if len(elements) != 1:\n        label = 'Document' if self.tag == 'html' else 'Fragment'\n        raise AssertionError(\"%s contains %s elements matching \"\n                             \"id %s!\" % (label, len(elements), key))\n    return elements[0]\nelse:\n    elements = self.cssselect(key)\n    if not elements:\n        label = 'Document' if self.tag == 'html' else 'Fragment'\n        raise AssertionError(\"%s contains no elements matching \"\n                             \"css selector %r\" % (label, key))\n    return elements", "path": "alfajor\\browsers\\_lxml.py", "repo_name": "idealistdev/alfajor", "stars": 37, "license": "other", "language": "python", "size": 292}
{"docstring": "\"\"\"An excerpt of the HTML of this element (without its children).\"\"\"\n", "func_signal": "def __str__(self):\n", "code": "clone = self.makeelement(self.tag, self.attrib, self.nsmap)\nif self.text_content:\n    clone.text = u'...'\nvalue = self.get('value', '')\nif len(value) > 32:\n    clone.attrib['value'] = value + u'...'\nhtml = tostring(clone)\nreturn fill(html, 79, subsequent_indent='    ')", "path": "alfajor\\browsers\\_lxml.py", "repo_name": "idealistdev/alfajor", "stars": 37, "license": "other", "language": "python", "size": 292}
{"docstring": "\"\"\"True if the element or its children contains *needle*.\n\n:param needle: may be an document element, integer index or a\nCSS select query.\n\nIf *needle* is a document element, only immediate decedent\nelements are considered.\n\n\"\"\"\n", "func_signal": "def __contains__(self, needle):\n", "code": "if not isinstance(needle, (int, basestring)):\n    return super(DOMElement, self).__contains__(needle)\ntry:\n    self[needle]\nexcept (AssertionError, IndexError):\n    return False\nelse:\n    return True", "path": "alfajor\\browsers\\_lxml.py", "repo_name": "idealistdev/alfajor", "stars": 37, "license": "other", "language": "python", "size": 292}
{"docstring": "\"\"\"Submit the form's values.\n\nEquivalent to hitting 'return' in a browser form: the data is\nsubmitted without the submit button's key/value pair.\n\n\"\"\"\n\n", "func_signal": "def submit(self, wait_for=None, timeout=0):\n", "code": "\n\"\"\"Fill fields of the form from *values*.\n\n:param values: a mapping or sequence of name/value pairs of form data.\n  If a sequence is provided, the sequence order will be respected when\n  filling fields with the exception of disjoint pairs in a checkbox\n  group, which will be set all at once.\n\n:param with_prefix: optional, a string that all form fields should\n  start with.  If a supplied field name does not start with this\n  prefix, it will be prepended.\n\n\"\"\"\ngrouped = _group_key_value_pairs(values, with_prefix)\nfields = self.fields\nfor name, field_values in grouped:\n    if len(field_values) == 1:\n        value = field_values[0]\n    else:\n        value = field_values\n    fields[name] = value", "path": "alfajor\\browsers\\_lxml.py", "repo_name": "idealistdev/alfajor", "stars": 37, "license": "other", "language": "python", "size": 292}
{"docstring": "\"\"\"The HTML of this element and a dump of its fields.\"\"\"\n", "func_signal": "def __str__(self):\n", "code": "lines = [DOMElement.__str__(self).rstrip('</form>').rstrip('...')]\nfields = self.fields\nfor field_name in sorted(fields.keys()):\n    lines.append('* %s = %s' % (field_name, fields[field_name]))\nreturn '\\n'.join(lines)", "path": "alfajor\\browsers\\_lxml.py", "repo_name": "idealistdev/alfajor", "stars": 37, "license": "other", "language": "python", "size": 292}
{"docstring": "# TODO: process type=submit|reset|button?\n", "func_signal": "def click(self, wait_for=None, timeout=0):\n", "code": "for element in self.iterancestors():\n    if element.tag == 'form':\n        break\nelse:\n    # Not in a form: clicking does nothing.\n    return\npairs = []\nname = self.attrib.get('name', False)\nif name:\n    pairs.append((name, self.attrib.get('value', '')))\nreturn element.submit(_extra_values=pairs)", "path": "alfajor\\browsers\\wsgi.py", "repo_name": "idealistdev/alfajor", "stars": 37, "license": "other", "language": "python", "size": 292}
{"docstring": "\"\"\"\nReturns the root sympy directory and set the global value\nindicating whether the system is case sensitive or not.\n\"\"\"\n", "func_signal": "def get_sympy_dir():\n", "code": "global sys_case_insensitive\n\nthis_file = os.path.abspath(__file__)\nsympy_dir = os.path.join(os.path.dirname(this_file), \"..\", \"..\")\nsympy_dir = os.path.normpath(sympy_dir)\nsys_case_insensitive = (os.path.isdir(sympy_dir) and\n                    os.path.isdir(sympy_dir.lower()) and\n                    os.path.isdir(sympy_dir.upper()))\nreturn sys_normcase(sympy_dir)", "path": "tutorial_sphinx\\runtests.py", "repo_name": "certik/scipy-2013-tutorial", "stars": 33, "license": "other", "language": "python", "size": 28591}
{"docstring": "\"\"\"\nRun the examples in ``test``, and display the results using the\nwriter function ``out``.\n\nThe examples are run in the namespace ``test.globs``.  If\n``clear_globs`` is true (the default), then this namespace will\nbe cleared after the test runs, to help with garbage\ncollection.  If you would like to examine the namespace after\nthe test completes, then use ``clear_globs=False``.\n\n``compileflags`` gives the set of flags that should be used by\nthe Python compiler when running the examples.  If not\nspecified, then it will default to the set of future-import\nflags that apply to ``globs``.\n\nThe output of each example is checked using\n``SymPyDocTestRunner.check_output``, and the results are\nformatted by the ``SymPyDocTestRunner.report_*`` methods.\n\"\"\"\n", "func_signal": "def run(self, test, compileflags=None, out=None, clear_globs=True):\n", "code": "self.test = test\n\nif compileflags is None:\n    compileflags = pdoctest._extract_future_flags(test.globs)\n\nsave_stdout = sys.stdout\nif out is None:\n    out = save_stdout.write\nsys.stdout = self._fakeout\n\n# Patch pdb.set_trace to restore sys.stdout during interactive\n# debugging (so it's not still redirected to self._fakeout).\n# Note that the interactive output will go to *our*\n# save_stdout, even if that's not the real sys.stdout; this\n# allows us to write test cases for the set_trace behavior.\nsave_set_trace = pdb.set_trace\nself.debugger = pdoctest._OutputRedirectingPdb(save_stdout)\nself.debugger.reset()\npdb.set_trace = self.debugger.set_trace\n\n# Patch linecache.getlines, so we can see the example's source\n# when we're inside the debugger.\nself.save_linecache_getlines = pdoctest.linecache.getlines\nlinecache.getlines = self.__patched_linecache_getlines\n\ntry:\n    return self.__run(test, compileflags, out)\nfinally:\n    sys.stdout = save_stdout\n    pdb.set_trace = save_set_trace\n    linecache.getlines = self.save_linecache_getlines\n    if clear_globs:\n        test.globs.clear()", "path": "tutorial_sphinx\\runtests.py", "repo_name": "certik/scipy-2013-tutorial", "stars": 33, "license": "other", "language": "python", "size": 28591}
{"docstring": "\"\"\"Grab signature (if given) and summary\"\"\"\n", "func_signal": "def _parse_summary(self):\n", "code": "if self._is_at_section():\n    return\n\nsummary = self._doc.read_to_next_empty_line()\nsummary_str = \" \".join([s.strip() for s in summary]).strip()\nif re.compile('^([\\w., ]+=)?\\s*[\\w\\.]+\\(.*\\)$').match(summary_str):\n    self['Signature'] = summary_str\n    if not self._is_at_section():\n        self['Summary'] = self._doc.read_to_next_empty_line()\nelse:\n    self['Summary'] = summary\n\nif not self._is_at_section():\n    self['Extended Summary'] = self._read_to_next_section()", "path": "tutorial_sphinx\\ext\\docscrape.py", "repo_name": "certik/scipy-2013-tutorial", "stars": 33, "license": "other", "language": "python", "size": 28591}
{"docstring": "\"\"\"\nReturns the list of \\*.py files (default) from which docstrings\nwill be tested which are at or below directory ``dir``. By default,\nonly those that have an __init__.py in their parent directory\nand do not start with ``test_`` will be included.\n\"\"\"\n", "func_signal": "def get_test_files(self, dir, pat='*.py', init_only=True):\n", "code": "def importable(x):\n    \"\"\"\n    Checks if given pathname x is an importable module by checking for\n    __init__.py file.\n\n    Returns True/False.\n\n    Currently we only test if the __init__.py file exists in the\n    directory with the file \"x\" (in theory we should also test all the\n    parent dirs).\n    \"\"\"\n    init_py = os.path.join(os.path.dirname(x), \"__init__.py\")\n    return os.path.exists(init_py)\n\ndir = os.path.join(self._root_dir, convert_to_native_paths([dir])[0])\n\ng = []\nfor path, folders, files in os.walk(dir):\n    g.extend([os.path.join(path, f) for f in files\n              if not f.startswith('test_') and fnmatch(f, pat)])\nif init_only:\n    # skip files that are not importable (i.e. missing __init__.py)\n    g = [x for x in g if importable(x)]\n\nreturn [sys_normcase(gi) for gi in g]", "path": "tutorial_sphinx\\runtests.py", "repo_name": "certik/scipy-2013-tutorial", "stars": 33, "license": "other", "language": "python", "size": 28591}
{"docstring": "\"\"\"Return all members of an object as (name, value) pairs sorted by name.\n\nOptionally, only return members that satisfy a given predicate.\n\"\"\"\n\n", "func_signal": "def inspect_getmembers(object, predicate=None):\n", "code": "results = []\nfor key in dir(object):\n    try:\n        value = getattr(object, key)\n    except AttributeError:\n        continue\n    if not predicate or predicate(value):\n        results.append((key, value))\nresults.sort()\nreturn results", "path": "tutorial_sphinx\\ext\\docscrape.py", "repo_name": "certik/scipy-2013-tutorial", "stars": 33, "license": "other", "language": "python", "size": 28591}
{"docstring": "# the first line contains \"******\", remove it:\n", "func_signal": "def doctest_fail(self, name, error_msg):\n", "code": "error_msg = \"\\n\".join(error_msg.split(\"\\n\")[1:])\nself._failed_doctest.append((name, error_msg))\nself.write(\"F\", \"Red\")\nself._active_file_error = True", "path": "tutorial_sphinx\\runtests.py", "repo_name": "certik/scipy-2013-tutorial", "stars": 33, "license": "other", "language": "python", "size": 28591}
{"docstring": "\"\"\"\nHack sys.path to import correct (local) sympy.\n\"\"\"\n", "func_signal": "def path_hack():\n", "code": "this_file = os.path.abspath(__file__)\nsympy_dir = os.path.join(os.path.dirname(this_file), \"..\")\nsympy_dir = os.path.normpath(sympy_dir)\nsys.path.insert(0, sympy_dir)", "path": "tutorial_sphinx\\get_sympy.py", "repo_name": "certik/scipy-2013-tutorial", "stars": 33, "license": "other", "language": "python", "size": 28591}
{"docstring": "\"\"\"\nAdd the given number of space characters to the beginning of\nevery non-blank line in ``s``, and return the result.\nIf the string ``s`` is Unicode, it is encoded using the stdout\nencoding and the ``backslashreplace`` error handler.\n\"\"\"\n# After a 2to3 run the below code is bogus, so wrap it with a version check\n", "func_signal": "def _indent(s, indent=4):\n", "code": "if sys.version_info[0] < 3:\n    if isinstance(s, unicode):\n        s = s.encode(pdoctest._encoding, 'backslashreplace')\n# This regexp matches the start of non-blank lines:\nreturn re.sub('(?m)^(?!$)', indent*' ', s)", "path": "tutorial_sphinx\\runtests.py", "repo_name": "certik/scipy-2013-tutorial", "stars": 33, "license": "other", "language": "python", "size": 28591}
{"docstring": "\"\"\"\nRuns doctests in all \\*.py files in the sympy directory which match\nany of the given strings in ``paths`` or all tests if paths=[].\n\nNotes:\n\n- Paths can be entered in native system format or in unix,\n  forward-slash format.\n- Files that are on the blacklist can be tested by providing\n  their path; they are only excluded if no paths are given.\n\nExamples\n========\n\n>>> import sympy\n\nRun all tests:\n\n>>> sympy.doctest() # doctest: +SKIP\n\nRun one file:\n\n>>> sympy.doctest(\"sympy/core/basic.py\") # doctest: +SKIP\n>>> sympy.doctest(\"polynomial.rst\") # doctest: +SKIP\n\nRun all tests in sympy/functions/ and some particular file:\n\n>>> sympy.doctest(\"/functions\", \"basic.py\") # doctest: +SKIP\n\nRun any file having polynomial in its name, doc/src/modules/polynomial.rst,\nsympy/functions/special/polynomials.py, and sympy/polys/polynomial.py:\n\n>>> sympy.doctest(\"polynomial\") # doctest: +SKIP\n\nThe ``subprocess`` and ``verbose`` options are the same as with the function\n``test()``.  See the docstring of that function for more information.\n\"\"\"\n", "func_signal": "def doctest(*paths, **kwargs):\n", "code": "subprocess = kwargs.pop(\"subprocess\", True)\nif subprocess:\n    ret = run_in_subprocess_with_hash_randomization(\"_doctest\",\n        function_args=paths, function_kwargs=kwargs)\n    if ret is not False:\n        return not bool(ret)\nreturn not bool(_doctest(*paths, **kwargs))", "path": "tutorial_sphinx\\runtests.py", "repo_name": "certik/scipy-2013-tutorial", "stars": 33, "license": "other", "language": "python", "size": 28591}
{"docstring": "\"\"\"\nParameters\n----------\ndata : str\n   String with lines separated by '\\n'.\n\n\"\"\"\n", "func_signal": "def __init__(self, data):\n", "code": "if isinstance(data, list):\n    self._str = data\nelse:\n    self._str = data.split('\\n')  # store string as list of lines\n\nself.reset()", "path": "tutorial_sphinx\\ext\\docscrape.py", "repo_name": "certik/scipy-2013-tutorial", "stars": 33, "license": "other", "language": "python", "size": 28591}
{"docstring": "\"\"\"\n.. index: default\n   :refguide: something, else, and more\n\n\"\"\"\n", "func_signal": "def _parse_index(self, section, content):\n", "code": "def strip_each_in(lst):\n    return [s.strip() for s in lst]\n\nout = {}\nsection = section.split('::')\nif len(section) > 1:\n    out['default'] = strip_each_in(section[1].split(','))[0]\nfor line in content:\n    line = line.split(':')\n    if len(line) > 2:\n        out[line[1]] = strip_each_in(line[2].split(','))\nreturn out", "path": "tutorial_sphinx\\ext\\docscrape.py", "repo_name": "certik/scipy-2013-tutorial", "stars": 33, "license": "other", "language": "python", "size": 28591}
{"docstring": "\"\"\"\nfunc_name : Descriptive text\n    continued text\nanother_func_name : Descriptive text\nfunc_name1, func_name2, :meth:`func_name`, func_name3\n\n\"\"\"\n", "func_signal": "def _parse_see_also(self, content):\n", "code": "items = []\n\ndef parse_item_name(text):\n    \"\"\"Match ':role:`name`' or 'name'\"\"\"\n    m = self._name_rgx.match(text)\n    if m:\n        g = m.groups()\n        if g[1] is None:\n            return g[3], None\n        else:\n            return g[2], g[1]\n    raise ValueError(\"%s is not a item name\" % text)\n\ndef push_item(name, rest):\n    if not name:\n        return\n    name, role = parse_item_name(name)\n    items.append((name, list(rest), role))\n    del rest[:]\n\ncurrent_func = None\nrest = []\n\nfor line in content:\n    if not line.strip():\n        continue\n\n    m = self._name_rgx.match(line)\n    if m and line[m.end():].strip().startswith(':'):\n        push_item(current_func, rest)\n        current_func, line = line[:m.end()], line[m.end():]\n        rest = [line.split(':', 1)[1].strip()]\n        if not rest[0]:\n            rest = []\n    elif not line.startswith(' '):\n        push_item(current_func, rest)\n        current_func = None\n        if ',' in line:\n            for func in line.split(','):\n                if func.strip():\n                    push_item(func, [])\n        elif line.strip():\n            current_func = line\n    elif current_func is not None:\n        rest.append(line.strip())\npush_item(current_func, rest)\nreturn items", "path": "tutorial_sphinx\\ext\\docscrape.py", "repo_name": "certik/scipy-2013-tutorial", "stars": 33, "license": "other", "language": "python", "size": 28591}
{"docstring": "\"\"\"\nReturn a DocTest for the given object, if it defines a docstring;\notherwise, return None.\n\"\"\"\n# Extract the object's docstring.  If it doesn't have one,\n# then return None (no test for this object).\n", "func_signal": "def _get_test(self, obj, name, module, globs, source_lines):\n", "code": "if isinstance(obj, basestring):\n    docstring = obj\nelse:\n    try:\n        if obj.__doc__ is None:\n            docstring = ''\n        else:\n            docstring = obj.__doc__\n            if not isinstance(docstring, basestring):\n                docstring = str(docstring)\n    except (TypeError, AttributeError):\n        docstring = ''\n\n# Find the docstring's location in the file.\nlineno = self._find_lineno(obj, source_lines)\n\nif lineno is None:\n    # if None, then _find_lineno couldn't find the docstring.\n    # But IT IS STILL THERE.  Likely it was decorated or something\n    # (i.e., @property docstrings have lineno == None)\n    # TODO: Write our own _find_lineno that is smarter in this regard\n    # Until then, just give it a dummy lineno.  This is just used for\n    # sorting the tests, so the only bad effect is that they will appear\n    # last instead of the order that they really are in the file.\n    # lineno is also used to report the offending line of a failing\n    # doctest, which is another reason to fix this.  See issue 1947.\n    lineno = 0\n\n# Don't bother if the docstring is empty.\nif self._exclude_empty and not docstring:\n    return None\n\n# Return a DocTest for this object.\nif module is None:\n    filename = None\nelse:\n    filename = getattr(module, '__file__', module.__name__)\n    if filename[-4:] in (\".pyc\", \".pyo\"):\n        filename = filename[:-1]\nreturn self._parser.get_doctest(docstring, globs, name,\n                                filename, lineno)", "path": "tutorial_sphinx\\runtests.py", "repo_name": "certik/scipy-2013-tutorial", "stars": 33, "license": "other", "language": "python", "size": 28591}
{"docstring": "\"\"\"\nConverts a list of '/' separated paths into a list of\nnative (os.sep separated) paths and converts to lowercase\nif the system is case insensitive.\n\"\"\"\n", "func_signal": "def convert_to_native_paths(lst):\n", "code": "newlst = []\nfor i, rv in enumerate(lst):\n    rv = os.path.join(*rv.split(\"/\"))\n    # on windows the slash after the colon is dropped\n    if sys.platform == \"win32\":\n        pos = rv.find(':')\n        if pos != -1:\n            if rv[pos+1] != '\\\\':\n                rv = rv[:pos+1] + '\\\\' + rv[pos+1:]\n    newlst.append(sys_normcase(rv))\nreturn newlst", "path": "tutorial_sphinx\\runtests.py", "repo_name": "certik/scipy-2013-tutorial", "stars": 33, "license": "other", "language": "python", "size": 28591}
{"docstring": "\"\"\"\nReturns the list of test_*.py (default) files at or below directory\n``dir`` relative to the sympy home directory.\n\"\"\"\n", "func_signal": "def get_test_files(self, dir, pat = 'test_*.py'):\n", "code": "dir = os.path.join(self._root_dir, convert_to_native_paths([dir])[0])\n\ng = []\nfor path, folders, files in os.walk(dir):\n    g.extend([os.path.join(path, f) for f in files if fnmatch(f, pat)])\n\nreturn [sys_normcase(gi) for gi in g]", "path": "tutorial_sphinx\\runtests.py", "repo_name": "certik/scipy-2013-tutorial", "stars": 33, "license": "other", "language": "python", "size": 28591}
{"docstring": "\"\"\"\nRun tests in the specified test_*.py files.\n\nTests in a particular test_*.py file are run if any of the given strings\nin ``paths`` matches a part of the test file's path. If ``paths=[]``,\ntests in all test_*.py files are run.\n\nNotes:\n\n- If sort=False, tests are run in random order (not default).\n- Paths can be entered in native system format or in unix,\n  forward-slash format.\n\n**Explanation of test results**\n\n======  ===============================================================\nOutput  Meaning\n======  ===============================================================\n.       passed\nF       failed\nX       XPassed (expected to fail but passed)\nf       XFAILed (expected to fail and indeed failed)\ns       skipped\nw       slow\nT       timeout (e.g., when ``--timeout`` is used)\nK       KeyboardInterrupt (when running the slow tests with ``--slow``,\n        you can interrupt one of them without killing the test runner)\n======  ===============================================================\n\n\nColors have no additional meaning and are used just to facilitate\ninterpreting the output.\n\nExamples\n========\n\n>>> import sympy\n\nRun all tests:\n\n>>> sympy.test()    # doctest: +SKIP\n\nRun one file:\n\n>>> sympy.test(\"sympy/core/tests/test_basic.py\")    # doctest: +SKIP\n>>> sympy.test(\"_basic\")    # doctest: +SKIP\n\nRun all tests in sympy/functions/ and some particular file:\n\n>>> sympy.test(\"sympy/core/tests/test_basic.py\",\n...        \"sympy/functions\")    # doctest: +SKIP\n\nRun all tests in sympy/core and sympy/utilities:\n\n>>> sympy.test(\"/core\", \"/util\")    # doctest: +SKIP\n\nRun specific test from a file:\n\n>>> sympy.test(\"sympy/core/tests/test_basic.py\",\n...        kw=\"test_equality\")    # doctest: +SKIP\n\nRun specific test from any file:\n\n>>> sympy.test(kw=\"subs\")    # doctest: +SKIP\n\nRun the tests with verbose mode on:\n\n>>> sympy.test(verbose=True)    # doctest: +SKIP\n\nDon't sort the test output:\n\n>>> sympy.test(sort=False)    # doctest: +SKIP\n\nTurn on post-mortem pdb:\n\n>>> sympy.test(pdb=True)    # doctest: +SKIP\n\nTurn off colors:\n\n>>> sympy.test(colors=False)    # doctest: +SKIP\n\nForce colors, even when the output is not to a terminal (this is useful,\ne.g., if you are piping to ``less -r`` and you still want colors)\n\n>>> sympy.test(force_colors=False)    # doctest: +SKIP\n\nThe traceback verboseness can be set to \"short\" or \"no\" (default is\n\"short\")\n\n>>> sympy.test(tb='no')    # doctest: +SKIP\n\nYou can disable running the tests in a separate subprocess using\n``subprocess=False``.  This is done to support seeding hash randomization,\nwhich is enabled by default in the Python versions where it is supported.\nIf subprocess=False, hash randomization is enabled/disabled according to\nwhether it has been enabled or not in the calling Python process.\nHowever, even if it is enabled, the seed cannot be printed unless it is\ncalled from a new Python process.\n\nHash randomization was added in the minor Python versions 2.6.8, 2.7.3,\n3.1.5, and 3.2.3, and is enabled by default in all Python versions after\nand including 3.3.0.\n\nIf hash randomization is not supported ``subprocess=False`` is used\nautomatically.\n\n>>> sympy.test(subprocess=False)     # doctest: +SKIP\n\nTo set the hash randomization seed, set the environment variable\n``PYTHONHASHSEED`` before running the tests.  This can be done from within\nPython using\n\n>>> import os\n>>> os.environ['PYTHONHASHSEED'] = 42 # doctest: +SKIP\n\nOr from the command line using\n\n$ PYTHONHASHSEED=42 ./bin/test\n\nIf the seed is not set, a random seed will be chosen.\n\nNote that to reproduce the same hash values, you must use both the same as\nwell as the same architecture (32-bit vs. 64-bit).\n\n\"\"\"\n", "func_signal": "def test(*paths, **kwargs):\n", "code": "subprocess = kwargs.pop(\"subprocess\", True)\nif subprocess:\n    ret = run_in_subprocess_with_hash_randomization(\"_test\",\n        function_args=paths, function_kwargs=kwargs)\n    if ret is not False:\n        return not bool(ret)\nreturn not bool(_test(*paths, **kwargs))", "path": "tutorial_sphinx\\runtests.py", "repo_name": "certik/scipy-2013-tutorial", "stars": 33, "license": "other", "language": "python", "size": 28591}
{"docstring": "\"\"\"\nReturn true if the object is a user-defined generator function.\n\nGenerator function objects provides same attributes as functions.\n\nSee isfunction.__doc__ for attributes listing.\n\nAdapted from Python 2.6.\n\"\"\"\n", "func_signal": "def isgeneratorfunction(object):\n", "code": "CO_GENERATOR = 0x20\nif (inspect.isfunction(object) or inspect.ismethod(object)) and \\\n    object.func_code.co_flags & CO_GENERATOR:\n    return True\nreturn False", "path": "tutorial_sphinx\\runtests.py", "repo_name": "certik/scipy-2013-tutorial", "stars": 33, "license": "other", "language": "python", "size": 28591}
{"docstring": "\"\"\"\nInternal function that actually runs the tests.\n\nAll keyword arguments from ``test()`` are passed to this function except for\n``subprocess``.\n\nReturns 0 if tests passed and 1 if they failed.  See the docstring of\n``test()`` for more information.\n\"\"\"\n", "func_signal": "def _test(*paths, **kwargs):\n", "code": "verbose = kwargs.get(\"verbose\", False)\ntb = kwargs.get(\"tb\", \"short\")\nkw = kwargs.get(\"kw\", \"\")\npost_mortem = kwargs.get(\"pdb\", False)\ncolors = kwargs.get(\"colors\", True)\nforce_colors = kwargs.get(\"force_colors\", False)\nsort = kwargs.get(\"sort\", True)\nseed = kwargs.get(\"seed\", None)\nif seed is None:\n    seed = random.randrange(100000000)\ntimeout = kwargs.get(\"timeout\", False)\nslow = kwargs.get(\"slow\", False)\nr = PyTestReporter(verbose=verbose, tb=tb, colors=colors, force_colors=force_colors)\nt = SymPyTests(r, kw, post_mortem, seed)\n\n# Disable warnings for external modules\nimport sympy.external\nsympy.external.importtools.WARN_OLD_VERSION = False\nsympy.external.importtools.WARN_NOT_INSTALLED = False\n\ntest_files = t.get_test_files('sympy')\n\nif len(paths) == 0:\n    t._testfiles.extend(test_files)\nelse:\n    paths = convert_to_native_paths(paths)\n    matched = []\n    for f in test_files:\n        basename = os.path.basename(f)\n        for p in paths:\n            if p in f or fnmatch(basename, p):\n                matched.append(f)\n                break\n    t._testfiles.extend(matched)\n\nreturn int(not t.test(sort=sort, timeout=timeout, slow=slow))", "path": "tutorial_sphinx\\runtests.py", "repo_name": "certik/scipy-2013-tutorial", "stars": 33, "license": "other", "language": "python", "size": 28591}
{"docstring": "\"\"\"\nDoes the keyword expression self._kw match \"x\"? Returns True/False.\n\nAlways returns True if self._kw is \"\".\n\"\"\"\n", "func_signal": "def matches(self, x):\n", "code": "if self._kw == \"\":\n    return True\nreturn x.__name__.find(self._kw) != -1", "path": "tutorial_sphinx\\runtests.py", "repo_name": "certik/scipy-2013-tutorial", "stars": 33, "license": "other", "language": "python", "size": 28591}
{"docstring": "\"\"\"\nPrints a text on the screen.\n\nIt uses sys.stdout.write(), so no readline library is necessary.\n\nParameters\n==========\n\ncolor : choose from the colors below, \"\" means default color\nalign : \"left\"/\"right\", \"left\" is a normal print, \"right\" is aligned on\n        the right-hand side of the screen, filled with spaces if necessary\nwidth : the screen width\n\n\"\"\"\n", "func_signal": "def write(self, text, color=\"\", align=\"left\", width=None, force_colors=False):\n", "code": "color_templates = (\n    (\"Black\"       , \"0;30\"),\n    (\"Red\"         , \"0;31\"),\n    (\"Green\"       , \"0;32\"),\n    (\"Brown\"       , \"0;33\"),\n    (\"Blue\"        , \"0;34\"),\n    (\"Purple\"      , \"0;35\"),\n    (\"Cyan\"        , \"0;36\"),\n    (\"LightGray\"   , \"0;37\"),\n    (\"DarkGray\"    , \"1;30\"),\n    (\"LightRed\"    , \"1;31\"),\n    (\"LightGreen\"  , \"1;32\"),\n    (\"Yellow\"      , \"1;33\"),\n    (\"LightBlue\"   , \"1;34\"),\n    (\"LightPurple\" , \"1;35\"),\n    (\"LightCyan\"   , \"1;36\"),\n    (\"White\"       , \"1;37\"),  )\n\ncolors = {}\n\nfor name, value in color_templates:\n    colors[name] = value\nc_normal = '\\033[0m'\nc_color = '\\033[%sm'\n\nif width is None:\n    width = self.terminal_width\n\nif align == \"right\":\n    if self._write_pos+len(text) > width:\n        # we don't fit on the current line, create a new line\n        self.write(\"\\n\")\n    self.write(\" \"*(width-self._write_pos-len(text)))\n\nif not self._force_colors and hasattr(sys.stdout, 'isatty') and not sys.stdout.isatty():\n    # the stdout is not a terminal, this for example happens if the\n    # output is piped to less, e.g. \"bin/test | less\". In this case,\n    # the terminal control sequences would be printed verbatim, so\n    # don't use any colors.\n    color = \"\"\nelif sys.platform == \"win32\":\n    # Windows consoles don't support ANSI escape sequences\n    color = \"\"\nelif not self._colors:\n    color = \"\"\n\nif self._line_wrap:\n    if text[0] != \"\\n\":\n        sys.stdout.write(\"\\n\")\n\n# Avoid UnicodeEncodeError when printing out test failures\nif IS_PYTHON_3 and IS_WINDOWS:\n    text = text.encode('raw_unicode_escape').decode('utf8', 'ignore')\nelif IS_PYTHON_3 and not sys.stdout.encoding.lower().startswith('utf'):\n    text = text.encode(sys.stdout.encoding, 'backslashreplace').decode(sys.stdout.encoding)\n\nif color == \"\":\n    sys.stdout.write(text)\nelse:\n    sys.stdout.write(\"%s%s%s\" % (c_color % colors[color], text, c_normal))\nsys.stdout.flush()\nl = text.rfind(\"\\n\")\nif l == -1:\n    self._write_pos += len(text)\nelse:\n    self._write_pos = len(text)-l-1\nself._line_wrap = self._write_pos >= width\nself._write_pos %= width", "path": "tutorial_sphinx\\runtests.py", "repo_name": "certik/scipy-2013-tutorial", "stars": 33, "license": "other", "language": "python", "size": 28591}
{"docstring": "\"\"\"tc qdisc add dev lo root handle 1: netem delay 10ms\"\"\"\n", "func_signal": "def test_unpack_add_netem(self):\n", "code": "data = \"\\x4c\\x00\\x00\\x00\\x24\\x00\\x05\\x06\\x07\\x1b\\xcc\\x4d\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\xff\\xff\\xff\\xff\\x00\\x00\\x00\\x00\\x0a\\x00\\x01\\x00\\x6e\\x65\\x74\\x65\\x6d\\x00\\x00\\x00\\x1c\\x00\\x02\\x00\\x5a\\x62\\x02\\x00\\xe8\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\nmsg = Message.unpack(data)\nself.assertEqual(msg.type, RTM_NEWQDISC)\nself.assertEqual(msg.flags, 0x605)\n\nst = msg.service_template\nself.assertAlmostEqual(st.tcm_handle, 0x10000)\nself.assertAlmostEqual(st.tcm_parent, 0xffffffff)\n\nattr, data = NetemOptions.unpack(data[-28:])\nself.assertEqual(attr.data.latency, nl_us2ticks(10*1000))", "path": "tests\\netlink\\test_unpack.py", "repo_name": "praus/shapy", "stars": 60, "license": "mit", "language": "python", "size": 813}
{"docstring": "\"\"\"\nCompletely resets this instance and all associated rules in\nunderlying OS so you can reuse this instance.\n\"\"\"\n", "func_signal": "def reset_all(self):\n", "code": "self.teardown_all()\nself.__setup()", "path": "shapy\\emulation\\shaper.py", "repo_name": "praus/shapy", "stars": 60, "license": "mit", "language": "python", "size": 813}
{"docstring": "# root HTB qdiscs on IFB egress\n", "func_signal": "def __setup(self):\n", "code": "shapy.IFB.modprobe()\nself.ifb_up.add( shapy.HTBQdisc('1:', default_class='1ff') )\nself.ifb_down.add( shapy.HTBQdisc('1:', default_class='1ff') )\n\n# add ingress qdiscs to all real interfaces (such as eth or lo) so\n# we can redirect traffic to the IFB devices for the actual shaping\nfor i in settings.EMU_INTERFACES:\n    interface = shapy.Interface(i)\n    prioq = shapy.PRIOQdisc('1:')\n    ingressq = shapy.IngressQdisc()\n    \n    # Exclude specified ports from shaping altogether\n    for port in settings.EMU_NOSHAPE_PORTS:\n        prioq.add(shapy.FlowFilter('sport %s' % port, '1:1ff',\n                                   mask=0xffff, prio=1))\n        ingressq.add(shapy.FlowFilter('dport %s' % port, '1:1ff',\n                                      mask=0xffff, prio=1))\n    \n    interface.add(prioq)    \n    interface.add_ingress(ingressq)", "path": "shapy\\emulation\\shaper.py", "repo_name": "praus/shapy", "stars": 60, "license": "mit", "language": "python", "size": 813}
{"docstring": "\"\"\"\ntc filter add dev lo parent 1: protocol ip prio 1 u32 \\\nmatch ip sport 8000 0xffff flowid 1:5\n\"\"\"\n", "func_signal": "def test_unpack_tcp_filter(self):\n", "code": "data = \"\\\\\\0\\0\\0,\\0\\5\\6!\\201\\333M\\0\\0\\0\\0\\0\\0\\0\\0\\1\\0\\0\\0\\0\\0\\0\\0\\0\\0\\1\\0\\10\\0\\1\\0\\10\\0\\1\\0u32\\0000\\0\\2\\0\\10\\0\\1\\0\\5\\0\\1\\0$\\0\\5\\0\\1\\0\\1\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\377\\377\\0\\0\\37@\\0\\0\\24\\0\\0\\0\\0\\0\\0\\0\"\n#data = \"\\\\\\0\\0\\0,\\0\\5\\6\\352\\210\\333M\\0\\0\\0\\0\\0\\0\\0\\0\\1\\0\\0\\0\\0\\0\\0\\0\\0\\0\\1\\0\\10\\0\\1\\0\\10\\0\\1\\0u32\\0000\\0\\2\\0\\10\\0\\1\\0\\5\\0\\1\\0$\\0\\5\\0\\1\\0\\1\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\377\\377\\0\\0\\37@\\24\\0\\0\\0\\0\\0\\0\\0\"\nmsg = Message.unpack(data)\nst = msg.service_template\nattr = st.attributes\n\ntc_u32_sel = Struct(\"BBBHHhhI\")\ntc_u32_key = Struct(\"IIii\")\nnested = list(unpack_attrs(attr[1].payload))\nsel = tc_u32_sel.unpack(nested[1].payload[:16])\nkey = tc_u32_key.unpack(nested[1].payload[16:])\n#print sel, key\n#print hex_list(sel), hex_list(key)", "path": "tests\\netlink\\test_unpack.py", "repo_name": "praus/shapy", "stars": 60, "license": "mit", "language": "python", "size": 813}
{"docstring": "\"\"\"\nDeletes ingress qdisc on a interface designated by self.if_index\nand returns the resulting ACK message.\n\"\"\"\n", "func_signal": "def delete_ingress_qdisc(self):\n", "code": "if_index = getattr(self, 'if_index', self.interface.if_index)\ntm = tcmsg(socket.AF_UNSPEC, if_index, 0, TC_H_INGRESS, 0)\nreturn self._del_qdisc(tm)", "path": "tests\\__init__.py", "repo_name": "praus/shapy", "stars": 60, "license": "mit", "language": "python", "size": 813}
{"docstring": "\"\"\"\nshaping_conf format:\n{ (ip1, ...): {\n    download: int [kbps/kbit],\n    upload: int [kbps/kbit],\n    delay: int [ms]}\n    }\n}\nEach IP will have its own separate HTB class.\n\"\"\"\n\n", "func_signal": "def set_shaping(self, shaping_conf):\n", "code": "for node_ips in shaping_conf:\n    # if the the user accidentally passes a string instead of one-item\n    # list, we will do the conversion\n    if isinstance(node_ips, str):\n        node_ips = (str,)\n    if not isinstance(node_ips, (list, tuple)):\n        logger.warning(\"IPs must be specified in a list or tuple, skipping!\")\n        continue\n    \n    shaping_params = shaping_conf[node_ips]\n    upload = shaping_params.get('upload', 2**32-1)\n    download = shaping_params.get('download', 2**32-1)\n    delay = shaping_params.get('delay', 0)\n    jitter = shaping_params.get('jitter', 0)\n    \n    for ip in node_ips:\n        utils.validate_ip(ip)\n        \n        if ip in self.ip_handles:\n            logger.info(\"{0} is already shaped, skipping.\".format(ip))\n            continue\n        logger.info(\"Configuring {0} -> U:{1}{units} D:{2}{units} delay:{3}ms jitter:\u00b1{4}ms\"\\\n                    .format(ip, upload, download, delay, jitter, units=settings.UNITS))\n        # upload shaping for the given set of IPs\n        self.__shape_upload(ip, upload, delay, jitter)\n        # download shaping\n        self.__shape_download(ip, download, delay, jitter)\n\n# finally, we need to actually run those rules we just created\nfor i in settings.EMU_INTERFACES:\n    logger.info(\"Setting up shaping/emulation on interface {0}:\".format(i))\n    shapy.Interface(i).set_shaping()\nlogger.info(\"Setting up IFB devices:\")\nself.ifb_up.set_shaping()\nself.ifb_down.set_shaping()", "path": "shapy\\emulation\\shaper.py", "repo_name": "praus/shapy", "stars": 60, "license": "mit", "language": "python", "size": 813}
{"docstring": "\"\"\"\ntc -s class show dev lo classid 1:1\n\"\"\"\n", "func_signal": "def test_unpack_get_stats(self):\n", "code": "data1 = \"\\x24\\x00\\x00\\x00\\x2a\\x00\\x01\\x03\\x22\\x08\\xdc\\x4d\\x00\\x00\\x00\\x00\"\ndata2 = \"\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n\nnlmsghdr = Struct(\"IHHII\")\ntcmsg = Struct(\"BxxxiIII\")\n\nnlmsghdr.unpack(data1)\ntcmsg.unpack(data2)", "path": "tests\\netlink\\test_unpack.py", "repo_name": "praus/shapy", "stars": 60, "license": "mit", "language": "python", "size": 813}
{"docstring": "\"\"\"\nrate, ceil, mtu: bytes\n\"\"\"\n", "func_signal": "def __init__(self, rate, ceil=0, mtu=1600, quantum=0, prio=0):\n", "code": "if not ceil: ceil = rate\nr = self.tc_ratespec.pack(3, 0, -1, 0, rate)\nc = self.tc_ratespec.pack(3, 0, -1, 0, ceil)\nhz = os.sysconf('SC_CLK_TCK')\nbuffer = tc_calc_xmittime(rate, (rate / hz) + mtu)\ncbuffer = tc_calc_xmittime(ceil, (rate / hz) + mtu)\nt = self.tc_htb_opt.pack(buffer, cbuffer, quantum, 0, prio)\ndata = r + c + t\nAttr.__init__(self, TCA_HTB_PARMS, data)", "path": "shapy\\framework\\netlink\\htb.py", "repo_name": "praus/shapy", "stars": 60, "license": "mit", "language": "python", "size": 813}
{"docstring": "\"\"\"\nSelector for u32 filter, value is IP address for example.\noffset is 12 (0xc) for source IP, 16 for destination IP.\n\nThis is rather crude, underlying kernel infrastructure supports multiple\nkeys, but since there was no need for such a functionality, it is left\nout. See iproute2 sources for example how to implement it.\n\"\"\"\n", "func_signal": "def __init__(self, val, offset, mask=0xffffffff, offmask=0):\n", "code": "flags = TC_U32_TERMINAL\nnkeys = 1\nsel = self.tc_u32_sel.pack(flags, 0, nkeys, 0, 0, 0, 0, 0)\nkey = self.tc_u32_key.pack(mask, val, offset, offmask)\ndata = sel+key\nsuper(u32_selector, self).__init__(TCA_U32_SEL, data)", "path": "shapy\\framework\\netlink\\filter.py", "repo_name": "praus/shapy", "stars": 60, "license": "mit", "language": "python", "size": 813}
{"docstring": "\"\"\"Parses a list of all interfaces reported by `ip link`\"\"\"\n", "func_signal": "def scan_interfaces():\n", "code": "import subprocess\nimport re\nifcs = []\nout = subprocess.check_output([\"ip\", \"link\"]).split('\\n')\nfor line in out:\n    m = re.match(\"^[0-9]+:[ ]([a-z0-9]+):\", line)\n    if m:\n        ifcs.append(m.group(1))\nreturn ifcs", "path": "examples\\teardown.py", "repo_name": "praus/shapy", "stars": 60, "license": "mit", "language": "python", "size": 813}
{"docstring": "\"\"\"\nICMP packets has to be sent by a process owned by the root, /bin/ping has\nsetuid so I'm using this to avoid juggling with root privileges.\nThis is blocking.\n\"\"\"\n", "func_signal": "def ping(host_from, host_to, count=4, interval=0.2):\n", "code": "assert interval>=0.2, \"Interval cannot be less than 0.2 seconds\"\ncmd = 'ping -c {count} -I {0} -i {interval} {1}'\\\n                .format(host_from, host_to, interval=interval, count=count)\np = subprocess.Popen(shlex.split(cmd), stdout=subprocess.PIPE,\n                        stderr=subprocess.PIPE)\nstdout, stderr = p.communicate()\nif p.returncode == 0:\n    return float(stdout.splitlines()[-1].split('/')[4])\nraise PingError(p.returncode)", "path": "tests\\utils.py", "repo_name": "praus/shapy", "stars": 60, "license": "mit", "language": "python", "size": 813}
{"docstring": "\"\"\"\nThis method does not properly reset Shaper object for further use.\nIt just purges external configuration. Use reset_all() if you intend\nto reuse this instance.\n\"\"\"\n", "func_signal": "def teardown_all(self):\n", "code": "for i in settings.EMU_INTERFACES:\n    logger.info(\"Tearing down %s\" % i)\n    shapy.Interface(i).teardown()\nlogger.info(\"Tearing down IFBs\")\nshapy.IFB.teardown()", "path": "shapy\\emulation\\shaper.py", "repo_name": "praus/shapy", "stars": 60, "license": "mit", "language": "python", "size": 813}
{"docstring": "\"\"\"Creates rules on IFB device itself.\"\"\"\n# filter on the IFB device itself to redirect traffic to a HTB class\n", "func_signal": "def __shape_ifb(self, ifb, ip, qhandle, rate, nhandle, delay, jitter):\n", "code": "ifb.root.add( shapy.FlowFilter(ip, '1:%s' % qhandle) )\nhtbq = shapy.HTBClass('1:%s' % qhandle, rate=rate, ceil=rate)\nif delay or jitter:\n    htbq.add(shapy.NetemDelayQdisc('%s:' % nhandle, delay, jitter))\nifb.root.add( htbq )", "path": "shapy\\emulation\\shaper.py", "repo_name": "praus/shapy", "stars": 60, "license": "mit", "language": "python", "size": 813}
{"docstring": "\"\"\"\nReturns sent/received data for the given IP. This is based on\ntc statistics (tc -s) of HTB classes on the respective IFB devices.\n\"\"\"\n", "func_signal": "def get_traffic(self, ip):\n", "code": "if not hasattr(self, 'ip_handles'):\n    return None, None\n\ndef get_htb_class_send(stats):\n    try:\n        return int(re.search(r\"Sent ([0-9]+) \", stats).group(1))\n    except:\n        return None, None\n\nhandle = self.ip_handles[ip]\nup = executor.get_command('TCStats', interface=self.ifb_up, handle=handle)\ndown = executor.get_command('TCStats', interface=self.ifb_down, handle=handle)\nup_stats = executor.run(up, sudo=False)\ndown_stats = executor.run(down, sudo=False)\n\nreturn get_htb_class_send(up_stats), get_htb_class_send(down_stats)", "path": "shapy\\emulation\\shaper.py", "repo_name": "praus/shapy", "stars": 60, "license": "mit", "language": "python", "size": 813}
{"docstring": "# stores the relationship between IP and it's policing HTB classes\n", "func_signal": "def __init__(self, **kwargs):\n", "code": "self.ip_handles = {}\nself.__setup()", "path": "shapy\\emulation\\shaper.py", "repo_name": "praus/shapy", "stars": 60, "license": "mit", "language": "python", "size": 813}
{"docstring": "\"\"\"\ntc filter add dev lo parent 1: protocol ip prio 3 \\\nu32 match ip dst 127.0.0.4 flowid 1:1 \\\naction mirred egress redirect dev ifb0\n\nfilter parent 1: protocol ip pref 3 u32 \nfilter parent 1: protocol ip pref 3 u32 fh 800: ht divisor 1 \nfilter parent 1: protocol ip pref 3 u32 fh 800::800 order 2048 key ht 800 bkt 0 flowid 1:1 \n  match 7f000004/ffffffff at 16\n    action order 1: mirred (Egress Redirect to device ifb0) stolen\n    index 2 ref 1 bind 1\n\"\"\"\n", "func_signal": "def test_unpack_add_redirect_filter(self):\n", "code": "data = \"\\x94\\x00\\x00\\x00\\x2c\\x00\\x05\\x06\\x4f\\x21\\xda\\x4d\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x08\\x00\\x03\\x00\\x08\\x00\\x01\\x00\\x75\\x33\\x32\\x00\\x68\\x00\\x02\\x00\\x08\\x00\\x01\\x00\\x01\\x00\\x01\\x00\\x38\\x00\\x07\\x00\\x34\\x00\\x01\\x00\\x0b\\x00\\x01\\x00\\x6d\\x69\\x72\\x72\\x65\\x64\\x00\\x00\\x24\\x00\\x02\\x00\\x20\\x00\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x05\\x00\\x00\\x00\\x24\\x00\\x05\\x00\\x01\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\x7f\\x00\\x00\\x04\\x10\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\nmsg = Message.unpack(data)\nst = msg.service_template\nattr = st.attributes\n\ntc_u32_sel = Struct(\"BBBHHhhI\")\ntc_u32_key = Struct(\"IIii\")\nfilter_opt = list(unpack_attrs(attr[1].payload)) # TCA_KIND, TCA_OPTIONS\naction = list(unpack_attrs(filter_opt[1].payload)) # TCA_EGRESS_REDIR\nmirred = list(unpack_attrs(action[0].payload)) # TCA_ACT_KIND, TCA_ACT_OPTIONS\nmirred_parms = list(unpack_attrs(mirred[1].payload))\n\ntc_mirred = Struct(\"IIiiiiI\")\n#print tc_mirred.unpack(mirred_parms[0].payload)", "path": "tests\\netlink\\test_unpack.py", "repo_name": "praus/shapy", "stars": 60, "license": "mit", "language": "python", "size": 813}
{"docstring": "\"\"\"Creates and sends a qdisc-creating message and service template.\"\"\"\n", "func_signal": "def make_msg(self, attrs):\n", "code": "tcm = tcmsg(socket.AF_UNSPEC, self.if_index, self.qhandle, TC_H_ROOT, 0,\n           attrs)\nmsg = Message(type=RTM_NEWQDISC,\n              flags=NLM_F_EXCL | NLM_F_CREATE | NLM_F_REQUEST | NLM_F_ACK,\n              service_template=tcm)\nself.conn.send(msg)\nself.check_ack(self.conn.recv())", "path": "tests\\netlink\\test_qdiscs.py", "repo_name": "praus/shapy", "stars": 60, "license": "mit", "language": "python", "size": 813}
{"docstring": "\"\"\"\nrtab[pkt_len>>cell_log] = pkt_xmit_time\n\ncell - The cell size determines he granularity of packet transmission time\ncalculations. Has a sensible default.\n\n\"\"\"\n# http://kerneltrap.org/mailarchive/linux-netdev/2009/11/2/6259456/thread\n", "func_signal": "def tc_calc_rtable(rate, cell_log, mtu):\n", "code": "rtab = []\nbps = rate\n\nif mtu == 0:\n    mtu = 2047\n\nif cell_log < 0:\n    cell_log = 0\n    while (mtu >> cell_log) > 255:\n        cell_log += 1\n\nfor i in range(0, 256):\n    size = (i + 1) << cell_log\n    rtab.append(tc_calc_xmittime(bps, size))\n    \nreturn rtab;", "path": "shapy\\framework\\netlink\\htb.py", "repo_name": "praus/shapy", "stars": 60, "license": "mit", "language": "python", "size": 813}
{"docstring": "\"\"\"Sets up upload shaping (not really policing) and emulation\nfor the given _ip_.\"\"\"\n# egress filter to redirect to IFB, upload -> src <ip> & ifb1\n", "func_signal": "def __shape_upload(self, ip, rate, delay, jitter):\n", "code": "for i in settings.EMU_INTERFACES:\n    f = shapy.RedirectFilter('src %s' % ip, self.ifb_up)\n    shapy.Interface(i).root.add(f)\nqh = self.ifb_up.qhandles.next()\nnh = self.ifb_up.nhandles.next()\nself.ip_handles.update({ip: qh})\nself.__shape_ifb(self.ifb_up, 'src %s' % ip, qh, rate, nh, delay, jitter)", "path": "shapy\\emulation\\shaper.py", "repo_name": "praus/shapy", "stars": 60, "license": "mit", "language": "python", "size": 813}
{"docstring": "\"\"\"\nTCA_HTB_INIT is composed from TCA_HTB_PARMS, TCA_HTB_CTAB, TCA_HTB_RTAB\n\ntc class add dev lo parent 1: classid 1:1 htb rate 534\n\"\"\"\n", "func_signal": "def test_unpack_htb_class(self):\n", "code": "this_dir = os.path.dirname(os.path.realpath(__file__))\nwith open(os.path.join(this_dir, 'htb_add_class.data'), 'rb') as f:\n    data = f.read()\nmsg = Message.unpack(data)\nself.assertEqual(msg.type, RTM_NEWTCLASS)\nself.assertEqual(msg.flags, 0x605)\n\nst = msg.service_template\nself.assertEqual(st.tcm_handle, 0x10001)\nself.assertEqual(st.tcm_parent, 0x10000)\n\ninit = tuple(st.unpack_attrs(data[36:]))[1]\nattrs = list(st.unpack_attrs(init.payload))\nself.assertEqual(len(attrs), 3)\n\nfrom shapy.framework.netlink.htb import tc_calc_rtable\nself.assertItemsEqual(tc_calc_rtable(66, -1, 1600),\n                      struct.unpack(\"256I\", attrs[2].payload),\n                      \"Rate table (rtab) calculation is wrong.\")", "path": "tests\\netlink\\test_unpack.py", "repo_name": "praus/shapy", "stars": 60, "license": "mit", "language": "python", "size": 813}
{"docstring": "\"\"\"Given a document, tries to detect its XML encoding.\"\"\"\n", "func_signal": "def _detectEncoding(self, xml_data):\n", "code": "xml_encoding = sniffed_xml_encoding = None\ntry:\n    if xml_data[:4] == '\\x4c\\x6f\\xa7\\x94':\n        # EBCDIC\n        xml_data = self._ebcdic_to_ascii(xml_data)\n    elif xml_data[:4] == '\\x00\\x3c\\x00\\x3f':\n        # UTF-16BE\n        sniffed_xml_encoding = 'utf-16be'\n        xml_data = unicode(xml_data, 'utf-16be').encode('utf-8')\n    elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xfe\\xff') \\\n             and (xml_data[2:4] != '\\x00\\x00'):\n        # UTF-16BE with BOM\n        sniffed_xml_encoding = 'utf-16be'\n        xml_data = unicode(xml_data[2:], 'utf-16be').encode('utf-8')\n    elif xml_data[:4] == '\\x3c\\x00\\x3f\\x00':\n        # UTF-16LE\n        sniffed_xml_encoding = 'utf-16le'\n        xml_data = unicode(xml_data, 'utf-16le').encode('utf-8')\n    elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xff\\xfe') and \\\n             (xml_data[2:4] != '\\x00\\x00'):\n        # UTF-16LE with BOM\n        sniffed_xml_encoding = 'utf-16le'\n        xml_data = unicode(xml_data[2:], 'utf-16le').encode('utf-8')\n    elif xml_data[:4] == '\\x00\\x00\\x00\\x3c':\n        # UTF-32BE\n        sniffed_xml_encoding = 'utf-32be'\n        xml_data = unicode(xml_data, 'utf-32be').encode('utf-8')\n    elif xml_data[:4] == '\\x3c\\x00\\x00\\x00':\n        # UTF-32LE\n        sniffed_xml_encoding = 'utf-32le'\n        xml_data = unicode(xml_data, 'utf-32le').encode('utf-8')\n    elif xml_data[:4] == '\\x00\\x00\\xfe\\xff':\n        # UTF-32BE with BOM\n        sniffed_xml_encoding = 'utf-32be'\n        xml_data = unicode(xml_data[4:], 'utf-32be').encode('utf-8')\n    elif xml_data[:4] == '\\xff\\xfe\\x00\\x00':\n        # UTF-32LE with BOM\n        sniffed_xml_encoding = 'utf-32le'\n        xml_data = unicode(xml_data[4:], 'utf-32le').encode('utf-8')\n    elif xml_data[:3] == '\\xef\\xbb\\xbf':\n        # UTF-8 with BOM\n        sniffed_xml_encoding = 'utf-8'\n        xml_data = unicode(xml_data[3:], 'utf-8').encode('utf-8')\n    else:\n        sniffed_xml_encoding = 'ascii'\n        pass\n    xml_encoding_match = re.compile \\\n                         ('^<\\?.*encoding=[\\'\"](.*?)[\\'\"].*\\?>')\\\n                         .match(xml_data)\nexcept:\n    xml_encoding_match = None\nif xml_encoding_match:\n    xml_encoding = xml_encoding_match.groups()[0].lower()\n    if sniffed_xml_encoding and \\\n       (xml_encoding in ('iso-10646-ucs-2', 'ucs-2', 'csunicode',\n                         'iso-10646-ucs-4', 'ucs-4', 'csucs4',\n                         'utf-16', 'utf-32', 'utf_16', 'utf_32',\n                         'utf16', 'u16')):\n        xml_encoding = sniffed_xml_encoding\nreturn xml_data, xml_encoding, sniffed_xml_encoding", "path": "core\\thirdparties\\BeautifulSoup.py", "repo_name": "evilsocket/altair", "stars": 48, "license": "gpl-3.0", "language": "python", "size": 184}
{"docstring": "\"\"\"Returns true iff this tag has the same name, the same attributes,\nand the same contents (recursively) as the given tag.\n\nNOTE: right now this will return false if two tags have the\nsame attributes in a different order. Should this be fixed?\"\"\"\n", "func_signal": "def __eq__(self, other):\n", "code": "if not hasattr(other, 'name') or not hasattr(other, 'attrs') or not hasattr(other, 'contents') or self.name != other.name or self.attrs != other.attrs or len(self) != len(other):\n    return False\nfor i in range(0, len(self.contents)):\n    if self.contents[i] != other.contents[i]:\n        return False\nreturn True", "path": "core\\thirdparties\\BeautifulSoup.py", "repo_name": "evilsocket/altair", "stars": 48, "license": "gpl-3.0", "language": "python", "size": 184}
{"docstring": "#print \"End tag %s\" % name\n", "func_signal": "def unknown_endtag(self, name):\n", "code": "if self.quoteStack and self.quoteStack[-1] != name:\n    #This is not a real end tag.\n    #print \"</%s> is not real!\" % name\n    self.handle_data('</%s>' % name)\n    return\nself.endData()\nself._popToTag(name)\nif self.quoteStack and self.quoteStack[-1] == name:\n    self.quoteStack.pop()\n    self.literal = (len(self.quoteStack) > 0)", "path": "core\\thirdparties\\BeautifulSoup.py", "repo_name": "evilsocket/altair", "stars": 48, "license": "gpl-3.0", "language": "python", "size": 184}
{"docstring": "# check for a valid extension\n", "func_signal": "def parse( self, request ):\n", "code": "if self.config.AllowedExtensions != None:\n\t( root, ext ) = os.path.splitext( request.url.path )\n\tif ext[1:] not in self.config.AllowedExtensions and ext != '':\n\t\tself.ed.warning( \"Skipping page with unallowed extension '%s' .\" % request.url.path )\n\t\tself.parsed.append( request )\n\t\treturn\n# check directory depth\nif self.config.MaxDirectoryDepth != None:\n\tif len(request.url.path.split('/')) + 1 > self.config.MaxDirectoryDepth:\n\t\tself.ed.warning( \"Max directory depth exceeded '%s' .\" % request.url.path )\n\t\tself.parsed.append( request )\n\t\treturn\n# if enabled, delay the crawl process\nif self.config.CrawlDelayEnabled != None and self.config.CrawlDelayEnabled == True:\n\tself.ed.warning( \"Delaying crawling process of %d ms ...\" % self.config.CrawlDelay )\n\ttime.sleep( self.config.CrawlDelay / 1000.0 )\n\ntry:\n\t# set user-agent if specified\n\tif self.config.UserAgent != None:\n\t\trequest.setHeader( 'User-Agent', self.config.UserAgent )\n\t# set proxy if specified\n\tif self.config.ProxyEnabled != None and self.config.ProxyEnabled == True:\n\t\tself.ed.status( \"Setting request proxy to %s:%d .\" % ( self.config.ProxyServer, self.config.ProxyPort ) )\n\t\trequest.setProxy( self.config.ProxyServer, self.config.ProxyPort )\n\n\tresponse = request.fetch()\n\t# fix broken html\n\tresponse = re.sub( \"href\\s*=\\s*([^\\\"'\\s>]+)\" , r'href=\"\\1\"', response )\n\tresponse = re.sub( \"src\\s*=\\s*([^\\\"'\\s>]+)\" , r'src=\"\\1\"', response )\n\tresponse = re.sub( \"action\\s*=\\s*([^\\\"'\\s>]+)\" , r'action=\"\\1\"', response )\n\tresponse = re.sub( \"method\\s*=\\s*([^\\\"'\\s>]+)\" , r'method=\"\\1\"', response )\n\tresponse = re.sub( \"name\\s*=\\s*([^\\\"'\\s>]+)\" , r'name=\"\\1\"', response )\n\tresponse = re.sub( \"value\\s*=\\s*([^\\\"'\\s>]+)\" , r'value=\"\\1\"', response )\n\t\n\tresponse = BeautifulSoup(response).prettify()\n\t\t\t\t\n\tself.current = request.url\n\t\n\tself.ed.parsing( request.url )\n\t\n\tself.feed( response )\n\tself.close()\n\t\n\t# custom parsing\n\tpages = re.findall( 'window\\.open\\s*\\(\\s*[\\'\"]([^\\'\"]+)',  response )\n\tif pages != None:\n\t\tfor page in pages:\n\t\t\turl = Url( page, default_netloc = self.domain, default_path = self.root.path )\n\t\t\tif url.netloc == self.domain and url.scheme == self.scheme:\n\t\t\t\treq = GetRequest( url )\n\t\t\t\tif req not in self.requests:\n\t\t\t\t\tself.requests.append( req )\nexcept HTTPError as e:\t\n\tself.ed.warning( \"%s (%s)\" % (request.url.get(),e) )\nexcept Exception as e:\n\tself.ed.warning( e )\nfinally:\n\tself.parsed.append( request )\n\tif request.redirect != None:\n\t\turl = Url( request.redirect, default_netloc = self.domain, default_path = self.root.path )\n\t\tself.parsed.append( GetRequest(url) )\n\nfor req in self.requests:\n\tif req not in self.parsed:\n\t\tself.parse( req )", "path": "core\\html.py", "repo_name": "evilsocket/altair", "stars": 48, "license": "gpl-3.0", "language": "python", "size": 184}
{"docstring": "\"\"\"Returns the parents of this Tag that match the given\ncriteria.\"\"\"\n\n", "func_signal": "def findParents(self, name=None, attrs={}, limit=None, **kwargs):\n", "code": "return self._findAll(name, attrs, None, limit, self.parentGenerator,\n                     **kwargs)", "path": "core\\thirdparties\\BeautifulSoup.py", "repo_name": "evilsocket/altair", "stars": 48, "license": "gpl-3.0", "language": "python", "size": 184}
{"docstring": "\"\"\"Pops the tag stack up to and including the most recent\ninstance of the given tag. If inclusivePop is false, pops the tag\nstack up to but *not* including the most recent instqance of\nthe given tag.\"\"\"\n#print \"Popping to %s\" % name\n", "func_signal": "def _popToTag(self, name, inclusivePop=True):\n", "code": "if name == self.ROOT_TAG_NAME:\n    return            \n\nnumPops = 0\nmostRecentTag = None\nfor i in range(len(self.tagStack)-1, 0, -1):\n    if name == self.tagStack[i].name:\n        numPops = len(self.tagStack)-i\n        break\nif not inclusivePop:\n    numPops = numPops - 1\n\nfor i in range(0, numPops):\n    mostRecentTag = self.popTag()\nreturn mostRecentTag", "path": "core\\thirdparties\\BeautifulSoup.py", "repo_name": "evilsocket/altair", "stars": 48, "license": "gpl-3.0", "language": "python", "size": 184}
{"docstring": "\"\"\"Returns true iff the given string is the name of a\nself-closing tag according to this parser.\"\"\"\n", "func_signal": "def isSelfClosingTag(self, name):\n", "code": "return self.SELF_CLOSING_TAGS.has_key(name) \\\n       or self.instanceSelfClosingTags.has_key(name)", "path": "core\\thirdparties\\BeautifulSoup.py", "repo_name": "evilsocket/altair", "stars": 48, "license": "gpl-3.0", "language": "python", "size": 184}
{"docstring": "\"\"\"Initializes a map representation of this tag's attributes,\nif not already initialized.\"\"\"\n", "func_signal": "def _getAttrMap(self):\n", "code": "if not getattr(self, 'attrMap'):\n    self.attrMap = {}\n    for (key, value) in self.attrs:\n        self.attrMap[key] = value \nreturn self.attrMap", "path": "core\\thirdparties\\BeautifulSoup.py", "repo_name": "evilsocket/altair", "stars": 48, "license": "gpl-3.0", "language": "python", "size": 184}
{"docstring": "\"\"\"Returns the closest parent of this Tag that matches the given\ncriteria.\"\"\"\n# NOTE: We can't use _findOne because findParents takes a different\n# set of arguments.\n", "func_signal": "def findParent(self, name=None, attrs={}, **kwargs):\n", "code": "r = None\nl = self.findParents(name, attrs, 1)\nif l:\n    r = l[0]\nreturn r", "path": "core\\thirdparties\\BeautifulSoup.py", "repo_name": "evilsocket/altair", "stars": 48, "license": "gpl-3.0", "language": "python", "size": 184}
{"docstring": "\"\"\"Handle a processing instruction as a ProcessingInstruction\nobject, possibly one with a %SOUP-ENCODING% slot into which an\nencoding will be plugged later.\"\"\"\n", "func_signal": "def handle_pi(self, text):\n", "code": "if text[:3] == \"xml\":\n    text = \"xml version='1.0' encoding='%SOUP-ENCODING%'\"\nself._toStringSubclass(text, ProcessingInstruction)", "path": "core\\thirdparties\\BeautifulSoup.py", "repo_name": "evilsocket/altair", "stars": 48, "license": "gpl-3.0", "language": "python", "size": 184}
{"docstring": "\"\"\"Convenience method that works with all 2.x versions of Python\nto determine whether or not something is listlike.\"\"\"\n", "func_signal": "def isList(l):\n", "code": "return hasattr(l, '__iter__') \\\n       or (type(l) in (types.ListType, types.TupleType))", "path": "core\\thirdparties\\BeautifulSoup.py", "repo_name": "evilsocket/altair", "stars": 48, "license": "gpl-3.0", "language": "python", "size": 184}
{"docstring": "#print \"Getattr %s.%s\" % (self.__class__, tag)\n", "func_signal": "def __getattr__(self, tag):\n", "code": "if len(tag) > 3 and tag.rfind('Tag') == len(tag)-3:\n    return self.find(tag[:-3])\nelif tag.find('__') != 0:\n    return self.find(tag)", "path": "core\\thirdparties\\BeautifulSoup.py", "repo_name": "evilsocket/altair", "stars": 48, "license": "gpl-3.0", "language": "python", "size": 184}
{"docstring": "\"\"\"Returns the closest sibling to this Tag that matches the\ngiven criteria and appears before this Tag in the document.\"\"\"\n", "func_signal": "def findPreviousSibling(self, name=None, attrs={}, text=None, **kwargs):\n", "code": "return self._findOne(self.findPreviousSiblings, name, attrs, text,\n                     **kwargs)", "path": "core\\thirdparties\\BeautifulSoup.py", "repo_name": "evilsocket/altair", "stars": 48, "license": "gpl-3.0", "language": "python", "size": 184}
{"docstring": "\"\"\"Encodes an object to a string in some encoding, or to Unicode.\n.\"\"\"\n", "func_signal": "def toEncoding(self, s, encoding=None):\n", "code": "if isinstance(s, unicode):\n    if encoding:\n        s = s.encode(encoding)\nelif isinstance(s, str):\n    if encoding:\n        s = s.encode(encoding)\n    else:\n        s = unicode(s)\nelse:\n    if encoding:\n        s  = self.toEncoding(str(s), encoding)\n    else:\n        s = unicode(s)\nreturn s", "path": "core\\thirdparties\\BeautifulSoup.py", "repo_name": "evilsocket/altair", "stars": 48, "license": "gpl-3.0", "language": "python", "size": 184}
{"docstring": "\"\"\"Handle entity references as data, possibly converting known\nHTML entity references to the corresponding Unicode\ncharacters.\"\"\"\n", "func_signal": "def handle_entityref(self, ref):\n", "code": "data = None\nif self.convertEntities == self.HTML_ENTITIES or \\\n       (self.convertEntities == self.XML_ENTITIES and \\\n        self.XML_ENTITY_LIST.get(ref)):\n    try:\n        data = unichr(name2codepoint[ref])\n    except KeyError:\n        pass\nif not data:\n    data = '&%s;' % ref\nself.handle_data(data)", "path": "core\\thirdparties\\BeautifulSoup.py", "repo_name": "evilsocket/altair", "stars": 48, "license": "gpl-3.0", "language": "python", "size": 184}
{"docstring": "\"\"\"Treat a bogus SGML declaration as raw data. Treat a CDATA\ndeclaration as a CData object.\"\"\"\n", "func_signal": "def parse_declaration(self, i):\n", "code": "j = None\nif self.rawdata[i:i+9] == '<![CDATA[':\n     k = self.rawdata.find(']]>', i)\n     if k == -1:\n         k = len(self.rawdata)\n     data = self.rawdata[i+9:k]\n     j = k+3\n     self._toStringSubclass(data, CData)\nelse:\n    try:\n        j = SGMLParser.parse_declaration(self, i)\n    except SGMLParseError:\n        toHandle = self.rawdata[i:]\n        self.handle_data(toHandle)\n        j = i + len(toHandle)\nreturn j", "path": "core\\thirdparties\\BeautifulSoup.py", "repo_name": "evilsocket/altair", "stars": 48, "license": "gpl-3.0", "language": "python", "size": 184}
{"docstring": "\"\"\"Returns the closest sibling to this Tag that matches the\ngiven criteria and appears after this Tag in the document.\"\"\"\n", "func_signal": "def findNextSibling(self, name=None, attrs={}, text=None, **kwargs):\n", "code": "return self._findOne(self.findNextSiblings, name, attrs, text,\n                     **kwargs)", "path": "core\\thirdparties\\BeautifulSoup.py", "repo_name": "evilsocket/altair", "stars": 48, "license": "gpl-3.0", "language": "python", "size": 184}
{"docstring": "#print \"Push\", tag.name\n", "func_signal": "def pushTag(self, tag):\n", "code": "if self.currentTag:\n    self.currentTag.append(tag)\nself.tagStack.append(tag)\nself.currentTag = self.tagStack[-1]", "path": "core\\thirdparties\\BeautifulSoup.py", "repo_name": "evilsocket/altair", "stars": 48, "license": "gpl-3.0", "language": "python", "size": 184}
{"docstring": "\"\"\"Turns a list of maps, lists, or scalars into a single map.\nUsed to build the SELF_CLOSING_TAGS, NESTABLE_TAGS, and\nNESTING_RESET_TAGS maps out of lists and partial maps.\"\"\"\n", "func_signal": "def buildTagMap(default, *args):\n", "code": "built = {}\nfor portion in args:\n    if hasattr(portion, 'items'):\n        #It's a map. Merge it.\n        for k,v in portion.items():\n            built[k] = v\n    elif isList(portion):\n        #It's a list. Map each item to the default.\n        for k in portion:\n            built[k] = default\n    else:\n        #It's a scalar. Map it to the default.\n        built[portion] = default\nreturn built", "path": "core\\thirdparties\\BeautifulSoup.py", "repo_name": "evilsocket/altair", "stars": 48, "license": "gpl-3.0", "language": "python", "size": 184}
{"docstring": "\"\"\"Changes a MS smart quote character to an XML or HTML\nentity.\"\"\"\n", "func_signal": "def _subMSChar(self, orig):\n", "code": "sub = self.MS_CHARS.get(orig)\nif type(sub) == types.TupleType:\n    if self.smartQuotesTo == 'xml':\n        sub = '&#x%s;' % sub[1]\n    else:\n        sub = '&%s;' % sub[0]\nreturn sub", "path": "core\\thirdparties\\BeautifulSoup.py", "repo_name": "evilsocket/altair", "stars": 48, "license": "gpl-3.0", "language": "python", "size": 184}
{"docstring": "\"\"\"Replaces class=\"foo bar\" with class=[\"foo\", \"bar\"]\n\nModifies its input in place.\n\"\"\"\n", "func_signal": "def _replace_cdata_list_attribute_values(self, tag_name, attrs):\n", "code": "if self.cdata_list_attributes:\n    universal = self.cdata_list_attributes.get('*', [])\n    tag_specific = self.cdata_list_attributes.get(\n        tag_name.lower(), [])\n    for cdata_list_attr in itertools.chain(universal, tag_specific):\n        if cdata_list_attr in dict(attrs):\n            # Basically, we have a \"class\" attribute whose\n            # value is a whitespace-separated list of CSS\n            # classes. Split it into a list.\n            value = attrs[cdata_list_attr]\n            if isinstance(value, basestring):\n                values = whitespace_re.split(value)\n            else:\n                # html5lib sometimes calls setAttributes twice\n                # for the same tag when rearranging the parse\n                # tree. On the second call the attribute value\n                # here is already a list.  If this happens,\n                # leave the value alone rather than trying to\n                # split it again.\n                values = value\n            attrs[cdata_list_attr] = values\nreturn attrs", "path": "bs4\\builder\\__init__.py", "repo_name": "nirvanatikku/stack-ed", "stars": 33, "license": "None", "language": "python", "size": 976}
{"docstring": "\"\"\"Wraps a given text to a maximum line length and returns it.\n\nWe turn lines that only contain whitespace into empty lines.  We keep\nnew lines and tabs (e.g., we do not treat tabs as spaces).\n\nArgs:\n  text:             text to wrap\n  length:           maximum length of a line, includes indentation\n                    if this is None then use GetHelpWidth()\n  indent:           indent for all but first line\n  firstline_indent: indent for first line; if None, fall back to indent\n  tabs:             replacement for tabs\n\nReturns:\n  wrapped text\n\nRaises:\n  FlagsError: if indent not shorter than length\n  FlagsError: if firstline_indent not shorter than length\n\"\"\"\n# Get defaults where callee used None\n", "func_signal": "def TextWrap(text, length=None, indent='', firstline_indent=None, tabs='    '):\n", "code": "if length is None:\n  length = GetHelpWidth()\nif indent is None:\n  indent = ''\nif len(indent) >= length:\n  raise FlagsError('Indent must be shorter than length')\n# In line we will be holding the current line which is to be started\n# with indent (or firstline_indent if available) and then appended\n# with words.\nif firstline_indent is None:\n  firstline_indent = ''\n  line = indent\nelse:\n  line = firstline_indent\n  if len(firstline_indent) >= length:\n    raise FlagsError('First line indent must be shorter than length')\n\n# If the callee does not care about tabs we simply convert them to\n# spaces If callee wanted tabs to be single space then we do that\n# already here.\nif not tabs or tabs == ' ':\n  text = text.replace('\\t', ' ')\nelse:\n  tabs_are_whitespace = not tabs.strip()\n\nline_regex = re.compile('([ ]*)(\\t*)([^ \\t]+)', re.MULTILINE)\n\n# Split the text into lines and the lines with the regex above. The\n# resulting lines are collected in result[]. For each split we get the\n# spaces, the tabs and the next non white space (e.g. next word).\nresult = []\nfor text_line in text.splitlines():\n  # Store result length so we can find out whether processing the next\n  # line gave any new content\n  old_result_len = len(result)\n  # Process next line with line_regex. For optimization we do an rstrip().\n  # - process tabs (changes either line or word, see below)\n  # - process word (first try to squeeze on line, then wrap or force wrap)\n  # Spaces found on the line are ignored, they get added while wrapping as\n  # needed.\n  for spaces, current_tabs, word in line_regex.findall(text_line.rstrip()):\n    # If tabs weren't converted to spaces, handle them now\n    if current_tabs:\n      # If the last thing we added was a space anyway then drop\n      # it. But let's not get rid of the indentation.\n      if (((result and line != indent) or\n           (not result and line != firstline_indent)) and line[-1] == ' '):\n        line = line[:-1]\n      # Add the tabs, if that means adding whitespace, just add it at\n      # the line, the rstrip() code while shorten the line down if\n      # necessary\n      if tabs_are_whitespace:\n        line += tabs * len(current_tabs)\n      else:\n        # if not all tab replacement is whitespace we prepend it to the word\n        word = tabs * len(current_tabs) + word\n    # Handle the case where word cannot be squeezed onto current last line\n    if len(line) + len(word) > length and len(indent) + len(word) <= length:\n      result.append(line.rstrip())\n      line = indent + word\n      word = ''\n      # No space left on line or can we append a space?\n      if len(line) + 1 >= length:\n        result.append(line.rstrip())\n        line = indent\n      else:\n        line += ' '\n    # Add word and shorten it up to allowed line length. Restart next\n    # line with indent and repeat, or add a space if we're done (word\n    # finished) This deals with words that cannot fit on one line\n    # (e.g. indent + word longer than allowed line length).\n    while len(line) + len(word) >= length:\n      line += word\n      result.append(line[:length])\n      word = line[length:]\n      line = indent\n    # Default case, simply append the word and a space\n    if word:\n      line += word + ' '\n  # End of input line. If we have content we finish the line. If the\n  # current line is just the indent but we had content in during this\n  # original line then we need to add an empty line.\n  if (result and line != indent) or (not result and line != firstline_indent):\n    result.append(line.rstrip())\n  elif len(result) == old_result_len:\n    result.append('')\n  line = indent\n\nreturn '\\n'.join(result)", "path": "gflags.py", "repo_name": "nirvanatikku/stack-ed", "stars": 33, "license": "None", "language": "python", "size": 976}
{"docstring": "\"\"\"Converts value to a python string or, if necessary, unicode-string.\"\"\"\n", "func_signal": "def _StrOrUnicode(value):\n", "code": "try:\n  return str(value)\nexcept UnicodeEncodeError:\n  return unicode(value)", "path": "gflags.py", "repo_name": "nirvanatikku/stack-ed", "stars": 33, "license": "None", "language": "python", "size": 976}
{"docstring": "\"\"\"Parse a media-range into its component parts.\n\nCarves up a media range and returns a tuple of the (type, subtype,\nparams) where 'params' is a dictionary of all the parameters for the media\nrange.  For example, the media range 'application/*;q=0.5' would get parsed\ninto:\n\n   ('application', '*', {'q', '0.5'})\n\nIn addition this function also guarantees that there is a value for 'q'\nin the params dictionary, filling it in with a proper default if\nnecessary.\n\"\"\"\n", "func_signal": "def parse_media_range(range):\n", "code": "(type, subtype, params) = parse_mime_type(range)\nif not params.has_key('q') or not params['q'] or \\\n        not float(params['q']) or float(params['q']) > 1\\\n        or float(params['q']) < 0:\n    params['q'] = '1'\n\nreturn (type, subtype, params)", "path": "apiclient\\mimeparse.py", "repo_name": "nirvanatikku/stack-ed", "stars": 33, "license": "None", "language": "python", "size": 976}
{"docstring": "\"\"\"Returns the module that's calling into this module.\n\nWe generally use this function to get the name of the module calling a\nDEFINE_foo... function.\n\"\"\"\n# Walk down the stack to find the first globals dict that's not ours.\n", "func_signal": "def _GetCallingModuleObjectAndName():\n", "code": "for depth in range(1, sys.getrecursionlimit()):\n  if not sys._getframe(depth).f_globals is globals():\n    globals_for_frame = sys._getframe(depth).f_globals\n    module, module_name = _GetModuleObjectAndName(globals_for_frame)\n    if module_name is not None:\n      return module, module_name\nraise AssertionError(\"No module was found\")", "path": "gflags.py", "repo_name": "nirvanatikku/stack-ed", "stars": 33, "license": "None", "language": "python", "size": 976}
{"docstring": "\"\"\"Removes a common space prefix from the lines of a multiline text.\n\nIf the first line does not start with a space, it is left as it is and\nonly in the remaining lines a common space prefix is being searched\nfor. That means the first line will stay untouched. This is especially\nuseful to turn doc strings into help texts. This is because some\npeople prefer to have the doc comment start already after the\napostrophe and then align the following lines while others have the\napostrophes on a separate line.\n\nThe function also drops trailing empty lines and ignores empty lines\nfollowing the initial content line while calculating the initial\ncommon whitespace.\n\nArgs:\n  text: text to work on\n\nReturns:\n  the resulting text\n\"\"\"\n", "func_signal": "def CutCommonSpacePrefix(text):\n", "code": "text_lines = text.splitlines()\n# Drop trailing empty lines\nwhile text_lines and not text_lines[-1]:\n  text_lines = text_lines[:-1]\nif text_lines:\n  # We got some content, is the first line starting with a space?\n  if text_lines[0] and text_lines[0][0].isspace():\n    text_first_line = []\n  else:\n    text_first_line = [text_lines.pop(0)]\n  # Calculate length of common leading whitespace (only over content lines)\n  common_prefix = os.path.commonprefix([line for line in text_lines if line])\n  space_prefix_len = len(common_prefix) - len(common_prefix.lstrip())\n  # If we have a common space prefix, drop it from all lines\n  if space_prefix_len:\n    for index in xrange(len(text_lines)):\n      if text_lines[index]:\n        text_lines[index] = text_lines[index][space_prefix_len:]\n  return '\\n'.join(text_first_line + text_lines)\nreturn ''", "path": "gflags.py", "repo_name": "nirvanatikku/stack-ed", "stars": 33, "license": "None", "language": "python", "size": 976}
{"docstring": "\"\"\"Parses one or more arguments with the installed parser.\n\nArgs:\n  arguments: a single argument or a list of arguments (typically a\n    list of default values); a single argument is converted\n    internally into a list containing one item.\n\"\"\"\n", "func_signal": "def Parse(self, arguments):\n", "code": "if not isinstance(arguments, list):\n  # Default value may be a list of values.  Most other arguments\n  # will not be, so convert them into a single-item list to make\n  # processing simpler below.\n  arguments = [arguments]\n\nif self.present:\n  # keep a backup reference to list of previously supplied option values\n  values = self.value\nelse:\n  # \"erase\" the defaults with an empty list\n  values = []\n\nfor item in arguments:\n  # have Flag superclass parse argument, overwriting self.value reference\n  Flag.Parse(self, item)  # also increments self.present\n  values.append(self.value)\n\n# put list of option values back in the 'value' attribute\nself.value = values", "path": "gflags.py", "repo_name": "nirvanatikku/stack-ed", "stars": 33, "license": "None", "language": "python", "size": 976}
{"docstring": "\"\"\"Ensure that flag is not None during program execution.\n\nRegisters a flag validator, which will follow usual validator\nrules.\nArgs:\n  flag_name: string, name of the flag\n  flag_values: FlagValues\nRaises:\n  AttributeError: if flag_name is not registered as a valid flag name.\n\"\"\"\n", "func_signal": "def MarkFlagAsRequired(flag_name, flag_values=FLAGS):\n", "code": "RegisterValidator(flag_name,\n                  lambda value: value is not None,\n                  message='Flag --%s must be specified.' % flag_name,\n                  flag_values=flag_values)", "path": "gflags.py", "repo_name": "nirvanatikku/stack-ed", "stars": 33, "license": "None", "language": "python", "size": 976}
{"docstring": "\"\"\"Return the quality ('q') of a mime-type against a list of media-ranges.\n\nReturns the quality 'q' of a mime-type when compared against the\nmedia-ranges in ranges. For example:\n\n>>> quality('text/html','text/*;q=0.3, text/html;q=0.7,\n              text/html;level=1, text/html;level=2;q=0.4, */*;q=0.5')\n0.7\n\n\"\"\"\n", "func_signal": "def quality(mime_type, ranges):\n", "code": "parsed_ranges = [parse_media_range(r) for r in ranges.split(',')]\n\nreturn quality_parsed(mime_type, parsed_ranges)", "path": "apiclient\\mimeparse.py", "repo_name": "nirvanatikku/stack-ed", "stars": 33, "license": "None", "language": "python", "size": 976}
{"docstring": "\"\"\"Checks whether a Flag object is registered under some name.\n\nNote: this is non trivial: in addition to its normal name, a flag\nmay have a short name too.  In self.FlagDict(), both the normal and\nthe short name are mapped to the same flag object.  E.g., calling\nonly \"del FLAGS.short_name\" is not unregistering the corresponding\nFlag object (it is still registered under the longer name).\n\nArgs:\n  flag_obj: A Flag object.\n\nReturns:\n  A boolean: True iff flag_obj is registered under some name.\n\"\"\"\n", "func_signal": "def _FlagIsRegistered(self, flag_obj):\n", "code": "flag_dict = self.FlagDict()\n# Check whether flag_obj is registered under its long name.\nname = flag_obj.name\nif flag_dict.get(name, None) == flag_obj:\n  return True\n# Check whether flag_obj is registered under its short name.\nshort_name = flag_obj.short_name\nif (short_name is not None and\n    flag_dict.get(short_name, None) == flag_obj):\n  return True\n# The flag cannot be registered under any other name, so we do not\n# need to do a full search through the values of self.FlagDict().\nreturn False", "path": "gflags.py", "repo_name": "nirvanatikku/stack-ed", "stars": 33, "license": "None", "language": "python", "size": 976}
{"docstring": "\"\"\"Returns the list of key flags for a module.\n\nArgs:\n  module: A module object or a module name (a string)\n\nReturns:\n  A new list of Flag objects.  Caller may update this list as he\n  wishes: none of those changes will affect the internals of this\n  FlagValue object.\n\"\"\"\n", "func_signal": "def _GetKeyFlagsForModule(self, module):\n", "code": "if not isinstance(module, str):\n  module = module.__name__\n\n# Any flag is a key flag for the module that defined it.  NOTE:\n# key_flags is a fresh list: we can update it without affecting the\n# internals of this FlagValues object.\nkey_flags = self._GetFlagsDefinedByModule(module)\n\n# Take into account flags explicitly declared as key for a module.\nfor flag in self.KeyFlagsByModuleDict().get(module, []):\n  if flag not in key_flags:\n    key_flags.append(flag)\nreturn key_flags", "path": "gflags.py", "repo_name": "nirvanatikku/stack-ed", "stars": 33, "license": "None", "language": "python", "size": 976}
{"docstring": "\"\"\"Remove flags that were previously appended from another FlagValues.\n\nArgs:\n  flag_values: registry containing flags to remove.\n\"\"\"\n", "func_signal": "def RemoveFlagValues(self, flag_values):\n", "code": "for flag_name in flag_values.FlagDict():\n  self.__delattr__(flag_name)", "path": "gflags.py", "repo_name": "nirvanatikku/stack-ed", "stars": 33, "license": "None", "language": "python", "size": 976}
{"docstring": "\"\"\"Generates a help string for a given module.\"\"\"\n", "func_signal": "def __RenderOurModuleFlags(self, module, output_lines, prefix=\"\"):\n", "code": "flags = self._GetFlagsDefinedByModule(module)\nif flags:\n  self.__RenderModuleFlags(module, flags, output_lines, prefix)", "path": "gflags.py", "repo_name": "nirvanatikku/stack-ed", "stars": 33, "license": "None", "language": "python", "size": 976}
{"docstring": "\"\"\"Records the module that defines a specific flag.\n\nArgs:\n  module_id: An int, the ID of the Python module.\n  flag: A Flag object, a flag that is key to the module.\n\"\"\"\n", "func_signal": "def _RegisterFlagByModuleId(self, module_id, flag):\n", "code": "flags_by_module_id = self.FlagsByModuleIdDict()\nflags_by_module_id.setdefault(module_id, []).append(flag)", "path": "gflags.py", "repo_name": "nirvanatikku/stack-ed", "stars": 33, "license": "None", "language": "python", "size": 976}
{"docstring": "\"\"\"Returns an instance of the argument parser cls.\n\nThis method overrides behavior of the __new__ methods in\nall subclasses of ArgumentParser (inclusive). If an instance\nfor mcs with the same set of arguments exists, this instance is\nreturned, otherwise a new instance is created.\n\nIf any keyword arguments are defined, or the values in args\nare not hashable, this method always returns a new instance of\ncls.\n\nArgs:\n  args: Positional initializer arguments.\n  kwargs: Initializer keyword arguments.\n\nReturns:\n  An instance of cls, shared or new.\n\"\"\"\n", "func_signal": "def __call__(mcs, *args, **kwargs):\n", "code": "if kwargs:\n  return type.__call__(mcs, *args, **kwargs)\nelse:\n  instances = mcs._instances\n  key = (mcs,) + tuple(args)\n  try:\n    return instances[key]\n  except KeyError:\n    # No cache entry for key exists, create a new one.\n    return instances.setdefault(key, type.__call__(mcs, *args))\n  except TypeError:\n    # An object in args cannot be hashed, always return\n    # a new instance.\n    return type.__call__(mcs, *args)", "path": "gflags.py", "repo_name": "nirvanatikku/stack-ed", "stars": 33, "license": "None", "language": "python", "size": 976}
{"docstring": "\"\"\"Writes a simple XML element.\n\nArgs:\n  outfile: File object we write the XML element to.\n  name: A string, the name of XML element.\n  value: A Python object, whose string representation will be used\n    as the value of the XML element.\n  indent: A string, prepended to each line of generated output.\n\"\"\"\n", "func_signal": "def _WriteSimpleXMLElement(outfile, name, value, indent):\n", "code": "value_str = _StrOrUnicode(value)\nif isinstance(value, bool):\n  # Display boolean values as the C++ flag library does: no caps.\n  value_str = value_str.lower()\nsafe_value_str = _MakeXMLSafe(value_str)\noutfile.write('%s<%s>%s</%s>\\n' % (indent, name, safe_value_str, name))", "path": "gflags.py", "repo_name": "nirvanatikku/stack-ed", "stars": 33, "license": "None", "language": "python", "size": 976}
{"docstring": "\"\"\"Return the name of the module defining this flag, or default.\n\nArgs:\n  flagname: Name of the flag to lookup.\n  default: Value to return if flagname is not defined. Defaults\n      to None.\n\nReturns:\n  The name of the module which registered the flag with this name.\n  If no such module exists (i.e. no flag with this name exists),\n  we return default.\n\"\"\"\n", "func_signal": "def FindModuleDefiningFlag(self, flagname, default=None):\n", "code": "for module, flags in self.FlagsByModuleDict().iteritems():\n  for flag in flags:\n    if flag.name == flagname or flag.short_name == flagname:\n      return module\nreturn default", "path": "gflags.py", "repo_name": "nirvanatikku/stack-ed", "stars": 33, "license": "None", "language": "python", "size": 976}
{"docstring": "\"\"\"Describe the key flags of a module.\n\nArgs:\n  module: A module object or a module name (a string).\n\nReturns:\n  string describing the key flags of a module.\n\"\"\"\n", "func_signal": "def ModuleHelp(self, module):\n", "code": "helplist = []\nself.__RenderOurModuleKeyFlags(module, helplist)\nreturn '\\n'.join(helplist)", "path": "gflags.py", "repo_name": "nirvanatikku/stack-ed", "stars": 33, "license": "None", "language": "python", "size": 976}
{"docstring": "\"\"\"Records the module that defines a specific flag.\n\nWe keep track of which flag is defined by which module so that we\ncan later sort the flags by module.\n\nArgs:\n  module_name: A string, the name of a Python module.\n  flag: A Flag object, a flag that is key to the module.\n\"\"\"\n", "func_signal": "def _RegisterFlagByModule(self, module_name, flag):\n", "code": "flags_by_module = self.FlagsByModuleDict()\nflags_by_module.setdefault(module_name, []).append(flag)", "path": "gflags.py", "repo_name": "nirvanatikku/stack-ed", "stars": 33, "license": "None", "language": "python", "size": 976}
{"docstring": "\"\"\"Registers a flag whose value can be any string.\"\"\"\n", "func_signal": "def DEFINE_string(name, default, help, flag_values=FLAGS, **args):\n", "code": "parser = ArgumentParser()\nserializer = ArgumentSerializer()\nDEFINE(parser, name, default, help, flag_values, serializer, **args)", "path": "gflags.py", "repo_name": "nirvanatikku/stack-ed", "stars": 33, "license": "None", "language": "python", "size": 976}
{"docstring": "\"\"\"Returns the index position of the first occurrence of value\n\n>>> L = SortedList([5, 5, -18, -1, 3, 4, 7, 8, 22, 99, 2, 1, 3])\n>>> L.index(5)\n7\n>>> L.index(0)\nTraceback (most recent call last):\n...\nValueError: SortedList.index(x): x not in list\n>>> L.index(99)\n12\n>>> L = SortedList([\"ABC\", \"X\", \"abc\", \"Abc\"], lambda x: x.lower())\n>>> print(L)\n['ABC', 'abc', 'Abc', 'X']\n>>> L.index(\"x\")\n3\n>>> L.index(\"abc\")\n0\n\"\"\"\n", "func_signal": "def index(self, value):\n", "code": "key, index = self.__bisect_left(value)\nif (index < len(self.__list) and\n    self.__key(self.__list[index]) == key):\n    return index\nraise ValueError(\"{0}.index(x): x not in list\".format(\n                 self.__class__.__name__))", "path": "Programming_in_Python3\\Examples\\SortedListDelegate.py", "repo_name": "BaneZhang/python", "stars": 35, "license": "None", "language": "python", "size": 500}
{"docstring": "'''from_roman should fail with repeated pairs of numerals'''\n", "func_signal": "def test_repeated_pairs(self):\n", "code": "for s in ('CMCM', 'CDCD', 'XCXC', 'XLXL', 'IXIX', 'IVIV'):\n    self.assertRaises(roman9.InvalidRomanNumeralError, roman9.from_roman, s)", "path": "Dive_Into_Python_3\\romantest9.py", "repo_name": "BaneZhang/python", "stars": 35, "license": "None", "language": "python", "size": 500}
{"docstring": "'''to_roman should give known result with known input'''\n", "func_signal": "def test_to_roman_known_values(self):\n", "code": "for integer, numeral in self.known_values:\n    result = roman9.to_roman(integer)\n    self.assertEqual(numeral, result)", "path": "Dive_Into_Python_3\\romantest9.py", "repo_name": "BaneZhang/python", "stars": 35, "license": "None", "language": "python", "size": 500}
{"docstring": "\"\"\"Removes the first occurrence of value from the list\n\n>>> L = SortedList([-18, -1, 3, 4, 5, 5, 7, 8, 22, 99])\n>>> print(L)\n[-18, -1, 3, 4, 5, 5, 7, 8, 22, 99]\n>>> L.remove(20)\nTraceback (most recent call last):\n...\nValueError: SortedList.remove(x): x not in list\n>>> L.remove(5)\n>>> L.remove(-18)\n>>> L.remove(99)\n>>> print(L)\n[-1, 3, 4, 5, 7, 8, 22]\n>>> L = SortedList([\"ABC\", \"X\", \"abc\", \"Abc\"], lambda x: x.lower())\n>>> print(L)\n['ABC', 'abc', 'Abc', 'X']\n>>> L.remove(\"Abca\")\nTraceback (most recent call last):\n...\nValueError: SortedList.remove(x): x not in list\n>>> print(L)\n['ABC', 'abc', 'Abc', 'X']\n>>> L.remove(\"Abc\")\n>>> print(L)\n['ABC', 'abc', 'X']\n>>> L.remove(\"ABC\")\n>>> print(L)\n['abc', 'X']\n>>> L.remove(\"X\")\n>>> print(L)\n['abc']\n>>> L.remove(\"abc\")\n>>> print(L)\n[]\n\"\"\"\n", "func_signal": "def remove(self, value):\n", "code": "key, index = self.__bisect_left(value)\nwhile (index < len(self.__list) and\n        self.__key(self.__list[index]) == key):\n    if self.__list[index] == value:\n        del self.__list[index]\n        return\n    index += 1\nraise ValueError(\"{0}.remove(x): x not in list\".format(\n                    self.__class__.__name__))", "path": "Programming_in_Python3\\Examples\\SortedListDelegate.py", "repo_name": "BaneZhang/python", "stars": 35, "license": "None", "language": "python", "size": 500}
{"docstring": "\"\"\"Counts every occurrence of value in the list\n\n>>> L = SortedList([5, 5, -18, -1, 3, 4, 5, 5, 7, 8, 22, 99])\n>>> L.count(5)\n4\n>>> L.count(99)\n1\n>>> L.count(-17)\n0\n>>> L = SortedList([\"ABC\", \"X\", \"abc\", \"Abc\"], lambda x: x.lower())\n>>> L.count(\"abc\")\n3\n\"\"\"\n", "func_signal": "def count(self, value):\n", "code": "count = 0\nkey, index = self.__bisect_left(value)\nwhile (index < len(self.__list) and\n       self.__key(self.__list[index]) == key):\n    index += 1\n    count += 1\nreturn count", "path": "Programming_in_Python3\\Examples\\SortedListDelegate.py", "repo_name": "BaneZhang/python", "stars": 35, "license": "None", "language": "python", "size": 500}
{"docstring": "\"\"\"Returns value's key and its index position in the list\n(or where value belongs if it isn't in the list)\n\"\"\"\n", "func_signal": "def __bisect_left(self, value):\n", "code": "key = self.__key(value)\nleft, right = 0, len(self.__list)\nwhile left < right:\n    middle = (left + right) // 2\n    if self.__key(self.__list[middle]) < key:\n        left = middle + 1\n    else:\n        right = middle\nreturn key, left", "path": "Programming_in_Python3\\Examples\\SortedListDelegate.py", "repo_name": "BaneZhang/python", "stars": 35, "license": "None", "language": "python", "size": 500}
{"docstring": "\"\"\"Creates a SortedList that orders using < on the items,\nor on the results of using the given key function\n\n>>> L = SortedList()\n>>> print(L)\n[]\n>>> L = SortedList((5, 8, -1, 3, 4, 22))\n>>> print(L)\n[-1, 3, 4, 5, 8, 22]\n>>> L = SortedList({9, 8, 7, 6, -1, -2})\n>>> print(L)\n[-2, -1, 6, 7, 8, 9]\n>>> L = SortedList([-5, 4, -3, 8, -2, 16, -1, 0, -3, 8])\n>>> print(L)\n[-5, -3, -3, -2, -1, 0, 4, 8, 8, 16]\n>>> L2 = SortedList(L)\n>>> print(L2)\n[-5, -3, -3, -2, -1, 0, 4, 8, 8, 16]\n>>> L = SortedList((\"the\", \"quick\", \"brown\", \"fox\", \"jumped\"))\n>>> print(L)\n['brown', 'fox', 'jumped', 'quick', 'the']\n\"\"\"\n", "func_signal": "def __init__(self, sequence=None, key=None):\n", "code": "self.__key = key or _identity\nassert hasattr(self.__key, \"__call__\")\nif sequence is None:\n    self.__list = []\nelif (isinstance(sequence, SortedList) and\n      sequence.key == self.__key):\n    self.__list = sequence.__list[:]\nelse:\n    self.__list = sorted(list(sequence), key=self.__key)", "path": "Programming_in_Python3\\Examples\\SortedListDelegate.py", "repo_name": "BaneZhang/python", "stars": 35, "license": "None", "language": "python", "size": 500}
{"docstring": "\"\"\"A 2D cartesian coordinate\n\n>>> point = Point()\n>>> point\nPoint(0, 0)\n\"\"\"\n", "func_signal": "def __init__(self, x=0, y=0):\n", "code": "self.x = x\nself.y = y", "path": "Programming_in_Python3\\Examples\\Shape.py", "repo_name": "BaneZhang/python", "stars": 35, "license": "None", "language": "python", "size": 500}
{"docstring": "'''convert Roman numeral to integer'''\n", "func_signal": "def from_roman(s):\n", "code": "if not isinstance(s, str):\n    raise InvalidRomanNumeralError('Input must be a string')\nif not s:\n    raise InvalidRomanNumeralError('Input can not be blank')\nif not roman_numeral_pattern.search(s):\n    raise InvalidRomanNumeralError('Invalid Roman numeral: {0}'.format(s))\n\nresult = 0\nindex = 0\nfor numeral, integer in roman_numeral_map:\n    while s[index : index + len(numeral)] == numeral:\n        result += integer\n        index += len(numeral)\nreturn result", "path": "Dive_Into_Python_3\\roman9.py", "repo_name": "BaneZhang/python", "stars": 35, "license": "None", "language": "python", "size": 500}
{"docstring": "\"\"\"Removes every occurrence of value from the list\n\nReturns the number of occurrences removed (which could be 0).\n>>> L = SortedList([5, 5, -18, -1, 3, 4, 5, 5, 7, 8, 22, 99])\n>>> L.add(5)\n>>> L.add(5)\n>>> print(L)\n[-18, -1, 3, 4, 5, 5, 5, 5, 5, 5, 7, 8, 22, 99]\n>>> L.remove_every(-3)\n0\n>>> L.remove_every(7)\n1\n>>> L.remove_every(5)\n6\n>>> print(L)\n[-18, -1, 3, 4, 8, 22, 99]\n>>> L = SortedList([\"ABC\", \"X\", \"abc\", \"Abc\"], lambda x: x.lower())\n>>> L.remove_every(\"abc\")\n3\n\"\"\"\n", "func_signal": "def remove_every(self, value):\n", "code": "count = 0\nkey, index = self.__bisect_left(value)\nwhile (index < len(self.__list) and\n       self.__key(self.__list[index]) == key):\n    del self.__list[index]\n    count += 1\nreturn count", "path": "Programming_in_Python3\\Examples\\SortedListDelegate.py", "repo_name": "BaneZhang/python", "stars": 35, "license": "None", "language": "python", "size": 500}
{"docstring": "'''to_roman should give known result with known input'''\n", "func_signal": "def test_to_roman_known_values(self):\n", "code": "for integer, numeral in self.known_values:\n    result = roman2.to_roman(integer)\n    self.assertEqual(numeral, result)", "path": "Dive_Into_Python_3\\romantest2.py", "repo_name": "BaneZhang/python", "stars": 35, "license": "None", "language": "python", "size": 500}
{"docstring": "'''from_roman should fail with malformed antecedents'''\n", "func_signal": "def test_malformed_antecedents(self):\n", "code": "for s in ('IIMXCC', 'VX', 'DCM', 'CMM', 'IXIV',\n          'MCMC', 'XCX', 'IVI', 'LM', 'LD', 'LC'):\n    self.assertRaises(roman9.InvalidRomanNumeralError, roman9.from_roman, s)", "path": "Dive_Into_Python_3\\romantest9.py", "repo_name": "BaneZhang/python", "stars": 35, "license": "None", "language": "python", "size": 500}
{"docstring": "\"\"\"\n>>> filename = os.path.dirname(__file__)\n>>> filename = os.path.join(filename, \"data/iradio-initial.pls\")\n>>> with open(filename, \"rt\", encoding=\"utf8\") as fh:\n...     d = dict_from_key_values_regex(fh)\n>>> for key in sorted(d.keys())[-4:]:\n...     print(\"{0}: {1}\".format(key, d[key]))\ntitle6: Virgin Xtreme (Broadband)\ntitle7: Virgin Classic Rock (Modem)\ntitle8: Virgin Classic Rock (Broadband)\ntitle9: CBC Radio One (Canada)\n>>> d[\"file13\"]\n'http://media.hiof.no/streams/m3u/nrk-petre-172.ogg.m3u'\n>>> d[\"genre15\"]\n''\n>>> len(d.keys())\n54\n\"\"\"\n", "func_signal": "def dict_from_key_values_regex(file, lowercase_keys=False):\n", "code": "INI_HEADER = re.compile(r\"^\\[[^]]+\\]$\")\nKEY_VALUE_RE = re.compile(r\"^(?P<key>\\w+)\\s*=\\s*(?P<value>.*)$\")\n\nkey_values = {}\nfor lino, line in enumerate(file, start=1):\n    line = line.strip()\n    if not line or line.startswith(\"#\"):\n        continue\n    key_value = KEY_VALUE_RE.match(line)\n    if key_value:\n        key = key_value.group(\"key\")\n        if lowercase_keys:\n            key = key.lower()\n        key_values[key] = key_value.group(\"value\")\n    else:\n        ini_header = INI_HEADER.match(line)\n        if not ini_header:\n            print(\"Failed to parse line {0}: {1}\".format(lino,\n                                                         line))\nreturn key_values", "path": "Programming_in_Python3\\Examples\\ReadKeyValue.py", "repo_name": "BaneZhang/python", "stars": 35, "license": "None", "language": "python", "size": 500}
{"docstring": "'''from_roman(to_roman(n))==n for all n'''\n", "func_signal": "def test_roundtrip(self):\n", "code": "for integer in range(1, 5000):\n    numeral = roman9.to_roman(integer)\n    result = roman9.from_roman(numeral)\n    self.assertEqual(integer, result)", "path": "Dive_Into_Python_3\\romantest9.py", "repo_name": "BaneZhang/python", "stars": 35, "license": "None", "language": "python", "size": 500}
{"docstring": "\"\"\"\n>>> filename = os.path.dirname(__file__)\n>>> filename = os.path.join(filename, \"data/iradio-initial.pls\")\n>>> with open(filename, \"rt\", encoding=\"utf8\") as fh:\n...     d = dict_from_key_values_pyparsing(fh)\n>>> for key in sorted(d.keys())[-4:]:\n...     print(\"{0}: {1}\".format(key, d[key]))\ntitle6: Virgin Xtreme (Broadband)\ntitle7: Virgin Classic Rock (Modem)\ntitle8: Virgin Classic Rock (Broadband)\ntitle9: CBC Radio One (Canada)\n>>> d[\"file13\"]\n'http://media.hiof.no/streams/m3u/nrk-petre-172.ogg.m3u'\n>>> d[\"genre15\"]\n''\n>>> len(d.keys())\n54\n\"\"\"\n", "func_signal": "def dict_from_key_values_pyparsing(file, lowercase_keys=False):\n", "code": "def accumulate(tokens):\n    key, value = tokens\n    key = key.lower() if lowercase_keys else key\n    key_values[key] = value\n\nkey_values = {}\nleft_bracket, right_bracket, equals = map(Suppress, \"[]=\")\nini_header = left_bracket + CharsNotIn(\"]\") + right_bracket\nkey_value = Word(alphanums) + equals + restOfLine\nkey_value.setParseAction(accumulate)\ncomment = \"#\" + restOfLine\nparser = OneOrMore(ini_header | key_value)\nparser.ignore(comment)\ntry:\n    parser.parseFile(file)\nexcept ParseException as err:\n    print(\"parse error: {0}\".format(err))\n    return {}\nreturn key_values", "path": "Programming_in_Python3\\Examples\\ReadKeyValue.py", "repo_name": "BaneZhang/python", "stars": 35, "license": "None", "language": "python", "size": 500}
{"docstring": "\"\"\"A Circle\n\n>>> circle = Circle(2)\n>>> circle\nCircle(2, 0, 0)\n\"\"\"\n", "func_signal": "def __init__(self, radius, x=0, y=0):\n", "code": "super().__init__(x, y)\nself.radius = radius", "path": "Programming_in_Python3\\Examples\\Shape.py", "repo_name": "BaneZhang/python", "stars": 35, "license": "None", "language": "python", "size": 500}
{"docstring": "\"\"\"Adds a value to the list (duplicates are allowed)\n\n>>> L = SortedList((5, 8, -1, 3, 4, 22))\n>>> print(L)\n[-1, 3, 4, 5, 8, 22]\n>>> L.add(5)\n>>> L.add(5)\n>>> L.add(7)\n>>> L.add(-18)\n>>> L.add(99)\n>>> print(L)\n[-18, -1, 3, 4, 5, 5, 5, 7, 8, 22, 99]\n\"\"\"\n", "func_signal": "def add(self, value):\n", "code": "index = self.__bisect_left(value)[1]\nif index == len(self.__list):\n    self.__list.append(value)\nelse:\n    self.__list.insert(index, value)", "path": "Programming_in_Python3\\Examples\\SortedListDelegate.py", "repo_name": "BaneZhang/python", "stars": 35, "license": "None", "language": "python", "size": 500}
{"docstring": "\"\"\"\n>>> filename = os.path.dirname(__file__)\n>>> filename = os.path.join(filename, \"data/iradio-initial.pls\")\n>>> with open(filename, \"rt\", encoding=\"utf8\") as fh:\n...     d = dict_from_key_values_ply(fh)\n>>> for key in sorted(d.keys())[-4:]:\n...     print(\"{0}: {1}\".format(key, d[key]))\ntitle6: Virgin Xtreme (Broadband)\ntitle7: Virgin Classic Rock (Modem)\ntitle8: Virgin Classic Rock (Broadband)\ntitle9: CBC Radio One (Canada)\n>>> d[\"file13\"]\n'http://media.hiof.no/streams/m3u/nrk-petre-172.ogg.m3u'\n>>> d[\"genre15\"]\n''\n>>> len(d.keys())\n54\n\"\"\"\n", "func_signal": "def dict_from_key_values_ply(file, lowercase_keys=False):\n", "code": "tokens = (\"INI_HEADER\", \"COMMENT\", \"KEY\", \"VALUE\")\n\nt_ignore_INI_HEADER = r\"\\[[^]]+\\]\"\nt_ignore_COMMENT = r\"\\#.*\"\n\ndef t_KEY(t):\n    r\"\\w+\"\n    if lowercase_keys:\n        t.value = t.value.lower()\n    return t\n\ndef t_VALUE(t):\n    r\"=.*\"\n    t.value = t.value[1:].strip()\n    return t\n\ndef t_newline(t):\n    r\"\\n+\"\n    t.lexer.lineno += len(t.value)\n\ndef t_error(t):\n    line = t.value.lstrip()\n    i = line.find(\"\\n\")\n    line = line if i == -1 else line[:i]\n    print(\"Failed to parse line {0}: {1}\".format(t.lineno + 1,\n                                                 line))\n\nkey_values = {}\nlexer = ply.lex.lex()\nlexer.input(file.read())\nkey = None\nfor token in lexer:\n    if token.type == \"KEY\":\n        key = token.value\n    elif token.type == \"VALUE\":\n        if key is None:\n            print(\"Failed to parse: value '{0}' without key\"\n                  .format(token.value))\n        else:\n            key_values[key] = token.value\n            key = None\nreturn key_values", "path": "Programming_in_Python3\\Examples\\ReadKeyValue.py", "repo_name": "BaneZhang/python", "stars": 35, "license": "None", "language": "python", "size": 500}
{"docstring": "'''from_roman should give known result with known input'''\n", "func_signal": "def test_from_roman_known_values(self):\n", "code": "for integer, numeral in self.known_values:\n    result = roman9.from_roman(numeral)\n    self.assertEqual(integer, result)", "path": "Dive_Into_Python_3\\romantest9.py", "repo_name": "BaneZhang/python", "stars": 35, "license": "None", "language": "python", "size": 500}
{"docstring": "\"\"\"Returns True if value is in the list; otherwise returns False\n\n>>> L = SortedList([5, 5, -18, -1, 3, 4, 7, 8, 22, 99, 2, 1, 3])\n>>> 5 in L\nTrue\n>>> 0 in L\nFalse\n>>> 99 in L\nTrue\n>>> L = SortedList([\"ABC\", \"X\", \"Abc\"], lambda x: x.lower())\n>>> \"abc\" in L\nTrue\n>>> \"x\" in L\nTrue\n>>> \"ZZ\" in L\nFalse\n\"\"\"\n", "func_signal": "def __contains__(self, value):\n", "code": "key, index = self.__bisect_left(value)\nreturn (index < len(self.__list) and\n        self.__key(self.__list[index]) == key)", "path": "Programming_in_Python3\\Examples\\SortedListDelegate.py", "repo_name": "BaneZhang/python", "stars": 35, "license": "None", "language": "python", "size": 500}
{"docstring": "\"\"\"\nLogin to Django admin CRUD using ``username`` and ``password``.\n\nAlso login to Django test client.\n\"\"\"\n", "func_signal": "def login_to_admin(self, username, password):\n", "code": "self.go200('/admin/')\n\nself.formvalue(1, 'username', username)\nself.formvalue(1, 'password', password)\n\nself.submit200()\nself.notfind('<input type=\"hidden\" name=\"this_is_the_login_form\" ' \\\n             'value=\"1\" />')\n\nself.client.login(username=username, password=password)", "path": "tddspry\\django\\cases.py", "repo_name": "playpauseandstop/tddspry", "stars": 38, "license": "bsd-3-clause", "language": "python", "size": 2082}
{"docstring": "\"\"\"\nWrap text to work with Twill.\n\"\"\"\n", "func_signal": "def text_to_twill(self, text):\n", "code": "headers_msg = 'Content: text-plain; encoding=utf-8\\n'\nheaders_msg = StringIO(headers_msg)\nheaders = httplib.HTTPMessage(headers_msg)\n\nstatus_code = 200\nurl = 'text://'\n\nio_response = StringIO(text)\nurllib_response = addinfourl(io_response,\n                             headers,\n                             url,\n                             status_code)\nurllib_response._headers = headers\nurllib_response._url = url\nurllib_response.msg = u'OK'\nurllib_response.seek = urllib_response.fp.seek\n\nself.get_browser()._browser._factory.set_response(urllib_response)\nself.get_browser().result = ResultWrapper(status_code, url, text)\n\nself._apply_xhtml()", "path": "tddspry\\django\\cases.py", "repo_name": "playpauseandstop/tddspry", "stars": 38, "license": "bsd-3-clause", "language": "python", "size": 2082}
{"docstring": "\"\"\"\nTwill used regexp for searching content on web-page. Use ``flat=True``\nto search content on web-page by standart Python ``not what in html``\nexpression.\n\nIf this expression was not True (was found on page) it's raises\n``TwillAssertionError`` as in ``twill.commands.notfind`` method.\n\nYou could escape ``what`` text by standart ``django.utils.html.escape``\nfunction if call method with ``escape=True``, like::\n\n    self.notfind('Text with \"quotes\"', escape=True)\n\n\"\"\"\n", "func_signal": "def notfind(self, what, flags='', flat=False, escape=False):\n", "code": "if escape:\n    what = real_escape(what)\n\nif not flat:\n    return self._notfind(what, flags)\n\nhtml = self.get_browser().get_html()\nif what in html:\n    raise TwillAssertionError('Match to %r' % what)\n\nreturn True", "path": "tddspry\\django\\cases.py", "repo_name": "playpauseandstop/tddspry", "stars": 38, "license": "bsd-3-clause", "language": "python", "size": 2082}
{"docstring": "\"\"\"\nTest that number of all ``model_or_manager`` objects equals to given\n``mixed`` value.\n\nYou can put ``mixed`` argument as ``tuple`` or ``list`` and\n``assert_count`` checks all of its values.\n\nMethod supports ``using`` keyword, so you can test count objects not\nonly in default database.\n\"\"\"\n", "func_signal": "def assert_count(self, model_or_manager, mixed, **kwargs):\n", "code": "manager = self._get_manager(model_or_manager)\nmanager, kwargs = self._process_using(manager, kwargs)\n\ncounter = manager.count()\n\ntry:\n    manager_from_mixed = self._get_manager(mixed)\n    manager_from_mixed, kwargs = \\\n        self._process_using(manager_from_mixed, kwargs)\n\n    number = manager_from_mixed.count()\nexcept:\n    number = mixed\n\nif isinstance(number, (list, tuple)):\n    equaled = False\n    numbers = number\n\n    for number in numbers:\n        if number == counter:\n            equaled = True\n            break\n\n    if not equaled:\n        assert False, '%r model has %d instance(s), not %s.' % \\\n                      (manager.model.__name__, counter, numbers)\nelse:\n    self.assert_equal(counter,\n                      number,\n                      '%r model has %d instance(s), not %d.' % \\\n                      (manager.model.__name__, counter, number))", "path": "tddspry\\django\\cases.py", "repo_name": "playpauseandstop/tddspry", "stars": 38, "license": "bsd-3-clause", "language": "python", "size": 2082}
{"docstring": "\"\"\"\nHelper tries to delete ``instance`` directly or all objects from\n``model`` or ``manager`` and checks that its correctly deleted.\n\"\"\"\n", "func_signal": "def assert_delete(self, mixed, **kwargs):\n", "code": "instance, pk = self._get_instance_and_pk(mixed)\nmanager = self._get_manager(mixed)\nmanager, kwargs = self._process_using(manager, kwargs)\nold_counter = manager.count()\n\nif pk:\n    diff = 1\n    instance.delete()\n    message = 'Could not delete only one %r instance. New counter is '\\\n              '%d, when old counter is %d.'\nelse:\n    diff = manager.count()\n    manager.all().delete()\n    message = 'Could not delete all instances of %r model. New ' \\\n              'counter is %d, when old counter is %d.'\n\nnew_counter = manager.count()\nmessage = message % (manager.model.__name__, new_counter, old_counter)\n\nself.assert_equal(new_counter + diff, old_counter, message)\n\nif pk:\n    try:\n        manager.get(pk=pk)\n    except manager.model.DoesNotExist:\n        pass\n    else:\n        assert False, 'Could not delete %r instance with %d pk.' % \\\n                       (manager.model.__name__, pk)", "path": "tddspry\\django\\cases.py", "repo_name": "playpauseandstop/tddspry", "stars": 38, "license": "bsd-3-clause", "language": "python", "size": 2082}
{"docstring": "\"\"\"\nHelper filters ``model_or_manager`` instance by ``query`` or\n``**kwargs`` lookup and check for empty ``QuerySet``.\n\nMethod support ``using`` keyword so you can test unread models not only\nfor default database.\n\"\"\"\n", "func_signal": "def assert_not_read(self, model_or_manager, query_=None, **kwargs):\n", "code": "manager = self._get_manager(model_or_manager)\nmanager, kwargs = self._process_using(manager, kwargs)\n\nif query_:\n    queryset = manager.filter(query_)\nelse:\n    queryset = manager.filter(**kwargs)\n\ncount = queryset.count()\n\nif count != 0:\n    assert False, '%d %s objects exist by %r lookup.' % \\\n                  (count, manager.model.__name__, query_ or kwargs)\n\nreturn True", "path": "tddspry\\django\\cases.py", "repo_name": "playpauseandstop/tddspry", "stars": 38, "license": "bsd-3-clause", "language": "python", "size": 2082}
{"docstring": "\"\"\"\nHelper tries to create new ``instance`` for ``model_or_manager`` class\nwith given ``**kwargs`` and checks that ``instance`` really created.\n\nMethod supports ``using`` keyword, so you can test create process\nnot only in default database.\n\n.. note:: If your model contains ``using`` field, use::\n\n       self.assert_create(model.objects.using('database'),\n                          ...\n                          using='value')\n\n   instead of given ``using`` as keyword argument, cause::\n\n       self.assert_create(model,\n                          ...\n                          using='value')\n\n   will create new ``model`` object in ``default`` database.\n\nMethod returns created ``instance`` if any.\n\"\"\"\n", "func_signal": "def assert_create(self, model_or_manager, **kwargs):\n", "code": "manager = self._get_manager(model_or_manager)\nmanager, kwargs = self._process_using(manager, kwargs)\n\nold_counter = manager.count()\n\ninstance = manager.create(**kwargs)\nnew_counter = manager.count()\n\nself.assert_equal(new_counter - 1,\n                  old_counter,\n                  'Could not create only one new %r instance. ' \\\n                  'New counter is %d, when old counter is %d.' % \\\n                  (manager.model.__name__, new_counter, old_counter))\n\nreturn instance", "path": "tddspry\\django\\cases.py", "repo_name": "playpauseandstop/tddspry", "stars": 38, "license": "bsd-3-clause", "language": "python", "size": 2082}
{"docstring": "\"\"\"\nShortcut to get fast access to twill glocals dict.\n\"\"\"\n", "func_signal": "def twill_glocals(self):\n", "code": "_, glocals = get_twill_glocals()\nreturn glocals", "path": "tddspry\\django\\cases.py", "repo_name": "playpauseandstop/tddspry", "stars": 38, "license": "bsd-3-clause", "language": "python", "size": 2082}
{"docstring": "\"\"\"\nTwill used regexp for searching content on web-page. Use ``flat=True``\nto search content on web-page by standart Python ``what in html``\nexpression.\n\nIf this expression was not True (was not found on page) it's raises\n``TwillAssertionError`` as in ``twill.commands.find`` method.\n\nSpecify ``count`` to test that ``what`` occurs ``count`` times in the\ncontent of the web-page.\n\nYou could escape ``what`` text by standart ``django.utils.html.escape``\nfunction if call method with ``escape=True``, like::\n\n    self.find('Text with \"quotes\"', escape=True)\n\n\"\"\"\n", "func_signal": "def find(self, what, flags='', flat=False, count=None, escape=False):\n", "code": "if escape:\n    what = real_escape(what)\n\nif not flat and not count:\n    return self._find(what, flags)\n\nhtml = self.get_browser().get_html()\nreal_count = html.count(what)\n\nif count is not None and count != real_count:\n    raise TwillAssertionError('Matched to %r %d times, not %d ' \\\n                              'times.' % (what, real_count, count))\nelif real_count == 0:\n    raise TwillAssertionError('No match to %r' % what)\n\nreturn True", "path": "tddspry\\django\\cases.py", "repo_name": "playpauseandstop/tddspry", "stars": 38, "license": "bsd-3-clause", "language": "python", "size": 2082}
{"docstring": "\"\"\"\nHelper reverses ``url`` if possible and auto-prepends ``SITE`` to\nit if ``prepend=True``.\n\"\"\"\n", "func_signal": "def build_url(self, url, args=None, kwargs=None, prepend=False):\n", "code": "if hasattr(url, 'get_absolute_url'):\n    url = url.get_absolute_url()\n\nif url.startswith(SITE):\n    return url\n\ntry:\n    url = reverse(url, args=args or [], kwargs=kwargs or {})\nexcept NoReverseMatch:\n    pass\n\nif not prepend:\n    return url\n\nreturn SITE + url.lstrip('/')", "path": "tddspry\\django\\cases.py", "repo_name": "playpauseandstop/tddspry", "stars": 38, "license": "bsd-3-clause", "language": "python", "size": 2082}
{"docstring": "\"\"\"\nTest that number of all ``model_or_manager`` objects not equals to\ngiven ``mixed`` value.\n\nYou can put ``mixed`` argument as ``tuple`` or ``list`` and\n``assert_count`` checks all of its values.\n\nMethod supports ``using`` keyword, so you can test count objects not\nonly in default database.\n\"\"\"\n", "func_signal": "def assert_not_count(self, model_or_manager, mixed, **kwargs):\n", "code": "manager = self._get_manager(model_or_manager)\nmanager, kwargs = self._process_using(manager, kwargs)\n\ncounter = manager.count()\n\ntry:\n    manager_from_mixed = self._get_manager(mixed)\n    manager_from_mixed, kwargs = \\\n        self._process_using(manager_from_mixed, kwargs)\n\n    number = manager_from_mixed.count()\nexcept:\n    number = mixed\n\nif isinstance(number, (list, tuple)):\n    equaled = False\n    numbers = number\n\n    for number in numbers:\n        if number == counter:\n            equaled = True\n            break\n\n    if equaled:\n        assert False, '%r model has %d instance(s), but should not.' %\\\n                      (manager.model.__name__, counter)\nelse:\n    self.assert_not_equal(counter,\n                          number,\n                          '%r model has %d instance(s), but should ' \\\n                          'not.' % (manager.model.__name__, number))", "path": "tddspry\\django\\cases.py", "repo_name": "playpauseandstop/tddspry", "stars": 38, "license": "bsd-3-clause", "language": "python", "size": 2082}
{"docstring": "\"\"\"\nReturns human-readable version of your **tddspry** installation.\n\"\"\"\n", "func_signal": "def get_version():\n", "code": "intjoin = lambda data, sep=None: (sep or '.').join(map(str, data))\n\nif VERSION[-1] is not None:\n    if isinstance(VERSION[-1], int):\n        version = intjoin(VERSION)\n    else:\n        version = '%s-%s' % (intjoin(VERSION[:-1]), VERSION[-1])\nelse:\n    version = intjoin(VERSION[:-1])\n\nreturn version", "path": "tddspry\\__init__.py", "repo_name": "playpauseandstop/tddspry", "stars": 38, "license": "bsd-3-clause", "language": "python", "size": 2082}
{"docstring": "\"\"\"\nSpecify ``flags`` supported by ``re`` module in text format, like ``'i'``\nor ``'s'`` instead of ``re.I`` or ``re.S``. If multiple ``flags`` specified\nthey would connected with ``|`` (unary OR), e.g. ``'iu'`` would be\nprocessed as ``re.I | re.U``.\n\nAll supported flags are: ``'i', 'l', 'm', 's', 'u', 'x'``.\n\"\"\"\n", "func_signal": "def process_re_flags(flags=None):\n", "code": "if flags:\n    old_flags = flags\n    flags = None\n\n    for flag in old_flags:\n        flag = flag.lower()\n\n        if not flag in RE_FLAGS:\n            continue\n\n        flag = RE_FLAGS[flag]\n\n        if flags is None:\n            flags = flag\n        else:\n            flags |= flag\n\nreturn flags or 0", "path": "tddspry\\utils.py", "repo_name": "playpauseandstop/tddspry", "stars": 38, "license": "bsd-3-clause", "language": "python", "size": 2082}
{"docstring": "\"\"\"\nWrap Django response to work with Twill.\n\"\"\"\n", "func_signal": "def response_to_twill(self, response):\n", "code": "path = response.request.get('PATH_INFO')\nurl = path and SITE + path.lstrip('/') or path\n\nheaders_msg = '\\n'.join('%s: %s' % (k, v) for k, v in response.items())\nheaders_msg = StringIO(headers_msg)\nheaders = httplib.HTTPMessage(headers_msg)\n\nio_response = StringIO(response.content)\nurllib_response = addinfourl(io_response,\n                             headers,\n                             url,\n                             response.status_code)\nurllib_response._headers = headers\nurllib_response._url = url\nurllib_response.msg = u'OK'\nurllib_response.seek = urllib_response.fp.seek\n\nself.get_browser()._browser._set_response(urllib_response, False)\nself.get_browser().result = ResultWrapper(response.status_code,\n                                          url,\n                                          response.content)\n\nself._apply_xhtml()", "path": "tddspry\\django\\cases.py", "repo_name": "playpauseandstop/tddspry", "stars": 38, "license": "bsd-3-clause", "language": "python", "size": 2082}
{"docstring": "\"\"\"\nEnable ``DEBUG`` mode for Django project (set ``settings.DEBUG`` var\nto ``True``).\n\nTo disable use ``TestCase.disable_debug`` method.\n\"\"\"\n", "func_signal": "def enable_debug(self, flag=True):\n", "code": "self.old_DEBUG = settings.DEBUG\nsettings.DEBUG = flag", "path": "tddspry\\django\\cases.py", "repo_name": "playpauseandstop/tddspry", "stars": 38, "license": "bsd-3-clause", "language": "python", "size": 2082}
{"docstring": "\"\"\"\nAssert that current URL matches the given regexp.\n\nIf ``regexp`` is set function appends '$' to the regexp end if needed.\n\"\"\"\n", "func_signal": "def url(self, url, args=None, kwargs=None, regexp=True):\n", "code": "should_be = self.build_url(url, args, kwargs, True)\n\nif regexp and should_be[-1] != '$':\n    should_be += '$'\n\nreturn self._url(should_be)", "path": "tddspry\\django\\cases.py", "repo_name": "playpauseandstop/tddspry", "stars": 38, "license": "bsd-3-clause", "language": "python", "size": 2082}
{"docstring": "\"\"\"\nUtility function to check if model of manager has ``using`` field and\nif not use it as arg for ``manager.using`` function.\n\"\"\"\n", "func_signal": "def _process_using(self, manager, kwargs):\n", "code": "if 'using' in kwargs:\n    model = manager.model\n\n    all_fields = model._meta.fields + model._meta.many_to_many\n    found = False\n\n    for field in all_fields:\n        if field.name == 'using':\n            found = True\n            break\n\n    if not found:\n        using = kwargs.pop('using')\n        return (manager.using(using), kwargs)\n\nreturn (manager, kwargs)", "path": "tddspry\\django\\cases.py", "repo_name": "playpauseandstop/tddspry", "stars": 38, "license": "bsd-3-clause", "language": "python", "size": 2082}
{"docstring": "\"\"\"\nSet form with ``formid`` as last used by twill. This method useful if\nyou got ``TwillException`` with message: ``\"more than one form;\nyou must select one (use 'fv') before submitting\"``.\n\nMethod supports both of numerical and string form ID.\n\"\"\"\n", "func_signal": "def activate_form(self, formid):\n", "code": "browser = self.get_browser()\nform = browser.get_form(formid)\nbrowser._browser.form = form", "path": "tddspry\\django\\cases.py", "repo_name": "playpauseandstop/tddspry", "stars": 38, "license": "bsd-3-clause", "language": "python", "size": 2082}
{"docstring": "\"\"\"\nGo to url and check that response code is 200.\n\"\"\"\n", "func_signal": "def go200(self, url, args=None, kwargs=None, check_links=False):\n", "code": "self.go(url, args, kwargs)\nself.code(200)\n\nif check_links:\n    self.check_links()", "path": "tddspry\\django\\cases.py", "repo_name": "playpauseandstop/tddspry", "stars": 38, "license": "bsd-3-clause", "language": "python", "size": 2082}
{"docstring": "\"\"\"\nLogout from current Django session.\n\nAlso logout from Django test client.\n\"\"\"\n", "func_signal": "def logout(self, url=None):\n", "code": "self.go200(url or settings.LOGOUT_URL)\nself.client.logout()", "path": "tddspry\\django\\cases.py", "repo_name": "playpauseandstop/tddspry", "stars": 38, "license": "bsd-3-clause", "language": "python", "size": 2082}
{"docstring": "\"\"\"Returns the first python token from the given text.\n\n    >>> python_lookahead = Parser('').python_lookahead\n    >>> python_lookahead('for i in range(10):')\n    'for'\n    >>> python_lookahead('else:')\n    'else'\n    >>> python_lookahead(' x = 1')\n    ' '\n\"\"\"\n", "func_signal": "def python_lookahead(self, text):\n", "code": "readline = iter([text]).next\ntokens = tokenize.generate_tokens(readline)\nreturn tokens.next()[1]", "path": "app\\web\\template.py", "repo_name": "jmhobbs/OpenPhotoBooth", "stars": 33, "license": "None", "language": "python", "size": 1353}
{"docstring": "# parse the text\n", "func_signal": "def generate_code(text, filename):\n", "code": "rootnode = Parser(text, filename).parse()\n        \n# generate python code from the parse tree\ncode = rootnode.emit(indent=\"\").strip()\nreturn safestr(code)", "path": "app\\web\\template.py", "repo_name": "jmhobbs/OpenPhotoBooth", "stars": 33, "license": "None", "language": "python", "size": 1353}
{"docstring": "\"\"\"\nRuns a WSGI-compatible `func` using FCGI, SCGI, or a simple web server,\nas appropriate based on context and `sys.argv`.\n\"\"\"\n\n", "func_signal": "def runwsgi(func):\n", "code": "if os.environ.has_key('SERVER_SOFTWARE'): # cgi\n    os.environ['FCGI_FORCE_CGI'] = 'Y'\n\nif (os.environ.has_key('PHP_FCGI_CHILDREN') #lighttpd fastcgi\n  or os.environ.has_key('SERVER_SOFTWARE')):\n    return runfcgi(func, None)\n\nif 'fcgi' in sys.argv or 'fastcgi' in sys.argv:\n    args = sys.argv[1:]\n    if 'fastcgi' in args: args.remove('fastcgi')\n    elif 'fcgi' in args: args.remove('fcgi')\n    if args:\n        return runfcgi(func, validaddr(args[0]))\n    else:\n        return runfcgi(func, None)\n\nif 'scgi' in sys.argv:\n    args = sys.argv[1:]\n    args.remove('scgi')\n    if args:\n        return runscgi(func, validaddr(args[0]))\n    else:\n        return runscgi(func)\n\nreturn httpserver.runsimple(func, validip(listget(sys.argv, 1, '')))", "path": "app\\web\\wsgi.py", "repo_name": "jmhobbs/OpenPhotoBooth", "stars": 33, "license": "None", "language": "python", "size": 1353}
{"docstring": "\"\"\"Selects the specified form.\"\"\"\n", "func_signal": "def select_form(self, name=None, predicate=None, index=0):\n", "code": "forms = self.get_forms()\n\nif name is not None:\n    forms = [f for f in forms if f.name == name]\nif predicate:\n    forms = [f for f in forms if predicate(f)]\n    \nif forms:\n    self.form = forms[index]\n    return self.form\nelse:\n    raise BrowserError(\"No form selected.\")", "path": "app\\web\\browser.py", "repo_name": "jmhobbs/OpenPhotoBooth", "stars": 33, "license": "None", "language": "python", "size": 1353}
{"docstring": "\"\"\"Compiles templates to python code.\"\"\"\n", "func_signal": "def compile_templates(root):\n", "code": "re_start = re_compile('^', re.M)\n\nfor dirpath, dirnames, filenames in os.walk(root):\n    filenames = [f for f in filenames if not f.startswith('.') and not f.endswith('~') and not f.startswith('__init__.py')]\n\n    for d in dirnames[:]:\n        if d.startswith('.'):\n            dirnames.remove(d) # don't visit this dir\n\n    out = open(os.path.join(dirpath, '__init__.py'), 'w')\n    out.write('from web.template import CompiledTemplate, ForLoop\\n\\n')\n    if dirnames:\n        out.write(\"import \" + \", \".join(dirnames))\n\n    for f in filenames:\n        path = os.path.join(dirpath, f)\n\n        if '.' in f:\n            name, _ = f.split('.', 1)\n        else:\n            name = f\n            \n        text = open(path).read()\n        text = Template.normalize_text(text)\n        code = Template.generate_code(text, path)\n        code = re_start.sub('    ', code)\n                    \n        _gen = '' + \\\n        '\\ndef %s():' + \\\n        '\\n    loop = ForLoop()' + \\\n        '\\n    _dummy  = CompiledTemplate(lambda: None, \"dummy\")' + \\\n        '\\n    join_ = _dummy._join' + \\\n        '\\n    escape_ = _dummy._escape' + \\\n        '\\n' + \\\n        '\\n%s' + \\\n        '\\n    return __template__'\n        \n        gen_code = _gen % (name, code)\n        out.write(gen_code)\n        out.write('\\n\\n')\n        out.write('%s = CompiledTemplate(%s(), %s)\\n\\n' % (name, name, repr(path)))\n\n        # create template to make sure it compiles\n        t = Template(open(path).read(), path)\n    out.close()", "path": "app\\web\\template.py", "repo_name": "jmhobbs/OpenPhotoBooth", "stars": 33, "license": "None", "language": "python", "size": 1353}
{"docstring": "\"\"\"submits the currently selected form.\"\"\"\n", "func_signal": "def submit(self, **kw):\n", "code": "if self.form is None:\n    raise BrowserError(\"No form selected.\")\nreq = self.form.click(**kw)\nreturn self.do_request(req)", "path": "app\\web\\browser.py", "repo_name": "jmhobbs/OpenPhotoBooth", "stars": 33, "license": "None", "language": "python", "size": 1353}
{"docstring": "# quick hack to check if the program is running in dev mode.\n", "func_signal": "def _is_dev_mode():\n", "code": "if os.environ.has_key('SERVER_SOFTWARE') \\\n    or os.environ.has_key('PHP_FCGI_CHILDREN') \\\n    or 'fcgi' in sys.argv or 'fastcgi' in sys.argv \\\n    or 'mod_wsgi' in sys.argv:\n        return False\nreturn True", "path": "app\\web\\wsgi.py", "repo_name": "jmhobbs/OpenPhotoBooth", "stars": 33, "license": "None", "language": "python", "size": 1353}
{"docstring": "\"\"\"Normalizes template text by correcting \\r\\n, tabs and BOM chars.\"\"\"\n", "func_signal": "def normalize_text(text):\n", "code": "text = text.replace('\\r\\n', '\\n').replace('\\r', '\\n').expandtabs()\nif not text.endswith('\\n'):\n    text += '\\n'\n\n# ignore BOM chars at the begining of template\nBOM = '\\xef\\xbb\\xbf'\nif isinstance(text, str) and text.startswith(BOM):\n    text = text[len(BOM):]\n\n# support fort \\$ for backward-compatibility \ntext = text.replace(r'\\$', '$$')\nreturn text", "path": "app\\web\\template.py", "repo_name": "jmhobbs/OpenPhotoBooth", "stars": 33, "license": "None", "language": "python", "size": 1353}
{"docstring": "\"\"\"Returns all links in the document.\"\"\"\n", "func_signal": "def get_links(self, text=None, text_regex=None, url=None, url_regex=None, predicate=None):\n", "code": "return self._filter_links(self._get_links(),\n    text=text, text_regex=text_regex, url=url, url_regex=url_regex, predicate=predicate)", "path": "app\\web\\browser.py", "repo_name": "jmhobbs/OpenPhotoBooth", "stars": 33, "license": "None", "language": "python", "size": 1353}
{"docstring": "\"\"\"Returns content of e or the current document as plain text.\"\"\"\n", "func_signal": "def get_text(self, e=None):\n", "code": "e = e or self.get_soup()\nreturn ''.join([htmlunquote(c) for c in e.recursiveChildGenerator() if isinstance(c, unicode)])", "path": "app\\web\\browser.py", "repo_name": "jmhobbs/OpenPhotoBooth", "stars": 33, "license": "None", "language": "python", "size": 1353}
{"docstring": "\"\"\"Reads a python expression from the text and returns the expression and remaining text.\n\nexpr -> simple_expr | paren_expr\nsimple_expr -> id extended_expr\nextended_expr -> attr_access | paren_expr extended_expr | ''\nattr_access -> dot id extended_expr\nparen_expr -> [ tokens ] | ( tokens ) | { tokens }\n     \n    >>> read_expr = Parser('').read_expr\n    >>> read_expr(\"name\")\n    ($name, '')\n    >>> read_expr(\"a.b and c\")\n    ($a.b, ' and c')\n    >>> read_expr(\"a. b\")\n    ($a, '. b')\n    >>> read_expr(\"name</h1>\")\n    ($name, '</h1>')\n    >>> read_expr(\"(limit)ing\")\n    ($(limit), 'ing')\n    >>> read_expr('a[1, 2][:3].f(1+2, \"weird string[).\", 3 + 4) done.')\n    ($a[1, 2][:3].f(1+2, \"weird string[).\", 3 + 4), ' done.')\n\"\"\"\n", "func_signal": "def read_expr(self, text, escape=True):\n", "code": "def simple_expr():\n    identifier()\n    extended_expr()\n\ndef identifier():\n    tokens.next()\n\ndef extended_expr():\n    lookahead = tokens.lookahead()\n    if lookahead is None:\n        return\n    elif lookahead.value == '.':\n        attr_access()\n    elif lookahead.value in parens:\n        paren_expr()\n        extended_expr()\n    else:\n        return\n\ndef attr_access():\n    from token import NAME # python token constants\n    dot = tokens.lookahead()\n    if tokens.lookahead2().type == NAME:\n        tokens.next() # consume dot\n        identifier()\n        extended_expr()\n\ndef paren_expr():\n    begin = tokens.next().value\n    end = parens[begin]\n    while True:\n        if tokens.lookahead().value in parens:\n            paren_expr()\n        else:\n            t = tokens.next()\n            if t.value == end:\n                break\n    return\n\nparens = {\n    \"(\": \")\",\n    \"[\": \"]\",\n    \"{\": \"}\"\n}\n\ndef get_tokens(text):\n    \"\"\"tokenize text using python tokenizer.\n    Python tokenizer ignores spaces, but they might be important in some cases. \n    This function introduces dummy space tokens when it identifies any ignored space.\n    Each token is a storage object containing type, value, begin and end.\n    \"\"\"\n    readline = iter([text]).next\n    end = None\n    for t in tokenize.generate_tokens(readline):\n        t = storage(type=t[0], value=t[1], begin=t[2], end=t[3])\n        if end is not None and end != t.begin:\n            _, x1 = end\n            _, x2 = t.begin\n            yield storage(type=-1, value=text[x1:x2], begin=end, end=t.begin)\n        end = t.end\n        yield t\n        \nclass BetterIter:\n    \"\"\"Iterator like object with 2 support for 2 look aheads.\"\"\"\n    def __init__(self, items):\n        self.iteritems = iter(items)\n        self.items = []\n        self.position = 0\n        self.current_item = None\n    \n    def lookahead(self):\n        if len(self.items) <= self.position:\n            self.items.append(self._next())\n        return self.items[self.position]\n\n    def _next(self):\n        try:\n            return self.iteritems.next()\n        except StopIteration:\n            return None\n        \n    def lookahead2(self):\n        if len(self.items) <= self.position+1:\n            self.items.append(self._next())\n        return self.items[self.position+1]\n            \n    def next(self):\n        self.current_item = self.lookahead()\n        self.position += 1\n        return self.current_item\n\ntokens = BetterIter(get_tokens(text))\n        \nif tokens.lookahead().value in parens:\n    paren_expr()\nelse:\n    simple_expr()\nrow, col = tokens.current_item.end\nreturn ExpressionNode(text[:col], escape=escape), text[col:]", "path": "app\\web\\template.py", "repo_name": "jmhobbs/OpenPhotoBooth", "stars": 33, "license": "None", "language": "python", "size": 1353}
{"docstring": "\"\"\"Makes a test suite from doctests.\"\"\"\n", "func_signal": "def doctest_suite(module_names):\n", "code": "import doctest\nsuite = TestSuite()\nfor mod in load_modules(module_names):\n    suite.addTest(doctest.DocTestSuite(mod))\nreturn suite", "path": "app\\web\\test.py", "repo_name": "jmhobbs/OpenPhotoBooth", "stars": 33, "license": "None", "language": "python", "size": 1353}
{"docstring": "\"\"\"Makes a suite from a module.\"\"\"\n", "func_signal": "def module_suite(module, classnames=None):\n", "code": "if classnames:\n    return unittest.TestLoader().loadTestsFromNames(classnames, module)\nelif hasattr(module, 'suite'):\n    return module.suite()\nelse:\n    return unittest.TestLoader().loadTestsFromModule(module)", "path": "app\\web\\test.py", "repo_name": "jmhobbs/OpenPhotoBooth", "stars": 33, "license": "None", "language": "python", "size": 1353}
{"docstring": "\"\"\"Runs a WSGI function as an SCGI server.\"\"\"\n", "func_signal": "def runscgi(func, addr=('localhost', 4000)):\n", "code": "import flup.server.scgi as flups\nreturn flups.WSGIServer(func, bindAddress=addr).run()", "path": "app\\web\\wsgi.py", "repo_name": "jmhobbs/OpenPhotoBooth", "stars": 33, "license": "None", "language": "python", "size": 1353}
{"docstring": "\"\"\"Returns all forms in the current document.\nThe returned form objects implement the ClientForm.HTMLForm interface.\n\"\"\"\n", "func_signal": "def get_forms(self):\n", "code": "if self._forms is None:\n    import ClientForm\n    self._forms = ClientForm.ParseResponse(self.get_response(), backwards_compat=False)\nreturn self._forms", "path": "app\\web\\browser.py", "repo_name": "jmhobbs/OpenPhotoBooth", "stars": 33, "license": "None", "language": "python", "size": 1353}
{"docstring": "\"\"\"Opens the specified url.\"\"\"\n", "func_signal": "def open(self, url, data=None, headers={}):\n", "code": "url = urllib.basejoin(self.url, url)\nreq = urllib2.Request(url, data, headers)\nreturn self.do_request(req)", "path": "app\\web\\browser.py", "repo_name": "jmhobbs/OpenPhotoBooth", "stars": 33, "license": "None", "language": "python", "size": 1353}
{"docstring": "\"\"\"Runs a WSGI function as a FastCGI server.\"\"\"\n", "func_signal": "def runfcgi(func, addr=('localhost', 8000)):\n", "code": "import flup.server.fcgi as flups\nreturn flups.WSGIServer(func, multiplexed=True, bindAddress=addr).run()", "path": "app\\web\\wsgi.py", "repo_name": "jmhobbs/OpenPhotoBooth", "stars": 33, "license": "None", "language": "python", "size": 1353}
{"docstring": "\"\"\"Creates a suite from multiple modules.\"\"\"\n", "func_signal": "def suite(module_names):\n", "code": "suite = TestSuite()\nfor mod in load_modules(module_names):\n    suite.addTest(module_suite(mod))\nreturn suite", "path": "app\\web\\test.py", "repo_name": "jmhobbs/OpenPhotoBooth", "stars": 33, "license": "None", "language": "python", "size": 1353}
{"docstring": "\"\"\"Returns beautiful soup of the current document.\"\"\"\n", "func_signal": "def get_soup(self):\n", "code": "import BeautifulSoup\nreturn BeautifulSoup.BeautifulSoup(self.data)", "path": "app\\web\\browser.py", "repo_name": "jmhobbs/OpenPhotoBooth", "stars": 33, "license": "None", "language": "python", "size": 1353}
{"docstring": "\"\"\"Opens the current page in real web browser.\"\"\"\n", "func_signal": "def show(self):\n", "code": "f = open('page.html', 'w')\nf.write(self.data)\nf.close()\n\nimport webbrowser, os\nurl = 'file://' + os.path.abspath('page.html')\nwebbrowser.open(url)", "path": "app\\web\\browser.py", "repo_name": "jmhobbs/OpenPhotoBooth", "stars": 33, "license": "None", "language": "python", "size": 1353}
{"docstring": "\"\"\"add multiple engines to our cluster\"\"\"\n", "func_signal": "def add_engines(self, n=1, block=True):\n", "code": "self.engines.extend(add_engines(n))\nif block:\n    self.wait_on_engines()", "path": "IPython\\parallel\\tests\\clienttest.py", "repo_name": "jupyter-attic/ipython-py3k", "stars": 34, "license": "other", "language": "python", "size": 79217}
{"docstring": "\"\"\"Enhanced version of the builtin reload function.\n\nsuperreload remembers objects previously in the module, and\n\n- upgrades the class dictionary of every old class in the module\n- upgrades the code object of every old function and method\n- clears the module's namespace before reloading\n\n\"\"\"\n\n# collect old objects in the module\n", "func_signal": "def superreload(module, reload=reload, old_objects={}):\n", "code": "for name, obj in list(module.__dict__.items()):\n    if not hasattr(obj, '__module__') or obj.__module__ != module.__name__:\n        continue\n    key = (module.__name__, name)\n    try:\n        old_objects.setdefault(key, []).append(weakref.ref(obj))\n    except TypeError:\n        # weakref doesn't work for all types;\n        # create strong references for 'important' cases\n        if isinstance(obj, type):\n            old_objects.setdefault(key, []).append(StrongRef(obj))\n\n# reload module\ntry:\n    # clear namespace first from old cruft\n    old_name = module.__name__\n    module.__dict__.clear()\n    module.__dict__['__name__'] = old_name\nexcept (TypeError, AttributeError, KeyError):\n    pass\nmodule = reload(module)\n\n# iterate over all objects and update functions & classes\nfor name, new_obj in list(module.__dict__.items()):\n    key = (module.__name__, name)\n    if key not in old_objects: continue\n\n    new_refs = []\n    for old_ref in old_objects[key]:\n        old_obj = old_ref()\n        if old_obj is None: continue\n        new_refs.append(old_ref)\n        update_generic(old_obj, new_obj)\n\n    if new_refs:\n        old_objects[key] = new_refs\n    else:\n        del old_objects[key]\n\nreturn module", "path": "IPython\\quarantine\\ipy_autoreload.py", "repo_name": "jupyter-attic/ipython-py3k", "stars": 34, "license": "other", "language": "python", "size": 79217}
{"docstring": "\"\"\"test tracking messages\"\"\"\n", "func_signal": "def test_tracking(self):\n", "code": "a,b = self.create_bound_pair(zmq.PAIR, zmq.PAIR)\ns = self.session\nstream = ZMQStream(a)\nmsg = s.send(a, 'hello', track=False)\nself.assertTrue(msg['tracker'] is None)\nmsg = s.send(a, 'hello', track=True)\nself.assertTrue(isinstance(msg['tracker'], zmq.MessageTracker))\nM = zmq.Message(b'hi there', track=True)\nmsg = s.send(a, 'hello', buffers=[M], track=True)\nt = msg['tracker']\nself.assertTrue(isinstance(t, zmq.MessageTracker))\nself.assertRaises(zmq.NotDone, t.wait, .1)\ndel M\nt.wait(1) # this will raise", "path": "IPython\\zmq\\tests\\test_session.py", "repo_name": "jupyter-attic/ipython-py3k", "stars": 34, "license": "other", "language": "python", "size": 79217}
{"docstring": "\"\"\"Clean up the junk left around by the build process\"\"\"\n", "func_signal": "def cleanup():\n", "code": "if \"develop\" not in sys.argv:\n    try:\n        shutil.rmtree('ipython.egg-info')\n    except:\n        try:\n            os.unlink('ipython.egg-info')\n        except:\n            pass", "path": "setup.py", "repo_name": "jupyter-attic/ipython-py3k", "stars": 34, "license": "other", "language": "python", "size": 79217}
{"docstring": "\"\"\"Upgrade the code object of a function\"\"\"\n", "func_signal": "def update_function(old, new):\n", "code": "for name in ['func_code', 'func_defaults', 'func_doc',\n             'func_closure', 'func_globals', 'func_dict']:\n    try:\n        setattr(old, name, getattr(new, name))\n    except (AttributeError, TypeError):\n        pass", "path": "IPython\\quarantine\\ipy_autoreload.py", "repo_name": "jupyter-attic/ipython-py3k", "stars": 34, "license": "other", "language": "python", "size": 79217}
{"docstring": "\"\"\"message format\"\"\"\n", "func_signal": "def test_msg(self):\n", "code": "msg = self.session.msg('execute')\nthekeys = set('header msg_id parent_header msg_type content'.split())\ns = set(msg.keys())\nself.assertEquals(s, thekeys)\nself.assertTrue(isinstance(msg['content'],dict))\nself.assertTrue(isinstance(msg['header'],dict))\nself.assertTrue(isinstance(msg['parent_header'],dict))\nself.assertEquals(msg['msg_type'], 'execute')", "path": "IPython\\zmq\\tests\\test_session.py", "repo_name": "jupyter-attic/ipython-py3k", "stars": 34, "license": "other", "language": "python", "size": 79217}
{"docstring": "\"\"\"initialization arguments for Session\"\"\"\n", "func_signal": "def test_args(self):\n", "code": "s = self.session\nself.assertTrue(s.pack is ss.default_packer)\nself.assertTrue(s.unpack is ss.default_unpacker)\nself.assertEquals(s.username, os.environ.get('USER', 'username'))\n\ns = ss.Session()\nself.assertEquals(s.username, os.environ.get('USER', 'username'))\n\nself.assertRaises(TypeError, ss.Session, pack='hi')\nself.assertRaises(TypeError, ss.Session, unpack='hi')\nu = str(uuid.uuid4())\ns = ss.Session(username='carrot', session=u)\nself.assertEquals(s.session, u)\nself.assertEquals(s.username, 'carrot')", "path": "IPython\\zmq\\tests\\test_session.py", "repo_name": "jupyter-attic/ipython-py3k", "stars": 34, "license": "other", "language": "python", "size": 79217}
{"docstring": "\"\"\"wait for our engines to connect.\"\"\"\n", "func_signal": "def wait_on_engines(self, timeout=5):\n", "code": "n = len(self.engines)+self.base_engine_count\ntic = time.time()\nwhile time.time()-tic < timeout and len(self.client.ids) < n:\n    time.sleep(0.1)\n\nassert not len(self.client.ids) < n, \"waiting for engines timed out\"", "path": "IPython\\parallel\\tests\\clienttest.py", "repo_name": "jupyter-attic/ipython-py3k", "stars": 34, "license": "other", "language": "python", "size": 79217}
{"docstring": "\"\"\"this will segfault\"\"\"\n", "func_signal": "def segfault():\n", "code": "import ctypes\nctypes.memset(-1,0,1)", "path": "IPython\\parallel\\tests\\clienttest.py", "repo_name": "jupyter-attic/ipython-py3k", "stars": 34, "license": "other", "language": "python", "size": 79217}
{"docstring": "\"\"\"dir2(obj) -> list of strings\n\nExtended version of the Python builtin dir(), which does a few extra\nchecks, and supports common objects with unusual internals that confuse\ndir(), such as Traits and PyCrust.\n\nThis version is guaranteed to return only a list of true strings, whereas\ndir() returns anything that objects inject into themselves, even if they\nare later not really valid for attribute access (many extension libraries\nhave such bugs).\n\"\"\"\n\n# Start building the attribute list via dir(), and then complete it\n# with a few extra special-purpose calls.\n", "func_signal": "def dir2(obj):\n", "code": "words = dir(obj)\n\nif hasattr(obj,'__class__'):\n    words.append('__class__')\n    words.extend(get_class_members(obj.__class__))\n#if '__base__' in words: 1/0\n\n# Some libraries (such as traits) may introduce duplicates, we want to\n# track and clean this up if it happens\nmay_have_dupes = False\n\n# this is the 'dir' function for objects with Enthought's traits\nif hasattr(obj, 'trait_names'):\n    try:\n        words.extend(obj.trait_names())\n        may_have_dupes = True\n    except TypeError:\n        # This will happen if `obj` is a class and not an instance.\n        pass\n    except AttributeError:\n        # `obj` lied to hasatter (e.g. Pyro), ignore\n        pass\n\n# Support for PyCrust-style _getAttributeNames magic method.\nif hasattr(obj, '_getAttributeNames'):\n    try:\n        words.extend(obj._getAttributeNames())\n        may_have_dupes = True\n    except TypeError:\n        # `obj` is a class and not an instance.  Ignore\n        # this error.\n        pass\n    except AttributeError:\n        # `obj` lied to hasatter (e.g. Pyro), ignore\n        pass\n\nif may_have_dupes:\n    # eliminate possible duplicates, as some traits may also\n    # appear as normal attributes in the dir() call.\n    words = list(set(words))\n    words.sort()\n\n# filter out non-string attributes which may be stuffed by dir() calls\n# and poor coding in third-party modules\nreturn [w for w in words if isinstance(w, str)]", "path": "IPython\\utils\\dir2.py", "repo_name": "jupyter-attic/ipython-py3k", "stars": 34, "license": "other", "language": "python", "size": 79217}
{"docstring": "\"\"\"skip a test if some names are not importable\"\"\"\n", "func_signal": "def skip_without(*names):\n", "code": "@decorator\ndef skip_without_names(f, *args, **kwargs):\n    \"\"\"decorator to skip tests in the absence of numpy.\"\"\"\n    for name in names:\n        try:\n            __import__(name)\n        except ImportError:\n            raise SkipTest\n    return f(*args, **kwargs)\nreturn skip_without_names", "path": "IPython\\parallel\\tests\\clienttest.py", "repo_name": "jupyter-attic/ipython-py3k", "stars": 34, "license": "other", "language": "python", "size": 79217}
{"docstring": "\"\"\"connect a client with my Context, and track its sockets for cleanup\"\"\"\n", "func_signal": "def connect_client(self):\n", "code": "c = Client(profile='iptest', context=self.context)\nfor name in [n for n in dir(c) if n.endswith('socket')]:\n    s = getattr(c, name)\n    s.setsockopt(zmq.LINGER, 0)\n    self.sockets.append(s)\nreturn c", "path": "IPython\\parallel\\tests\\clienttest.py", "repo_name": "jupyter-attic/ipython-py3k", "stars": 34, "license": "other", "language": "python", "size": 79217}
{"docstring": "\"\"\"Check whether some modules need to be reloaded.\"\"\"\n\n", "func_signal": "def check(self, check_all=False):\n", "code": "if check_all or self.check_all:\n    modules = list(sys.modules.keys())\nelse:\n    modules = list(self.modules.keys())\n\nfor modname in modules:\n    m = sys.modules.get(modname, None)\n\n    if modname in self.skip_modules:\n        continue\n\n    if not hasattr(m, '__file__'):\n        continue\n\n    if m.__name__ == '__main__':\n        # we cannot reload(__main__)\n        continue\n\n    filename = m.__file__\n    dirname = os.path.dirname(filename)\n    path, ext = os.path.splitext(filename)\n\n    if ext.lower() == '.py':\n        ext = PY_COMPILED_EXT\n        filename = os.path.join(dirname, path + PY_COMPILED_EXT)\n\n    if ext != PY_COMPILED_EXT:\n        continue\n\n    try:\n        pymtime = os.stat(filename[:-1]).st_mtime\n        if pymtime <= os.stat(filename).st_mtime:\n            continue\n        if self.failed.get(filename[:-1], None) == pymtime:\n            continue\n    except OSError:\n        continue\n\n    try:\n        superreload(m, reload, self.old_objects)\n        if filename[:-1] in self.failed:\n            del self.failed[filename[:-1]]\n    except:\n        print(\"[autoreload of %s failed: %s]\" % (\n                modname, traceback.format_exc(1)), file=sys.stderr)\n        self.failed[filename[:-1]] = pymtime", "path": "IPython\\quarantine\\ipy_autoreload.py", "repo_name": "jupyter-attic/ipython-py3k", "stars": 34, "license": "other", "language": "python", "size": 79217}
{"docstring": "\"\"\"test that messages receive unique ids\"\"\"\n", "func_signal": "def test_unique_msg_ids(self):\n", "code": "ids = set()\nfor i in range(2**12):\n    h = self.session.msg_header('test')\n    msg_id = h['msg_id']\n    self.assertTrue(msg_id not in ids)\n    ids.add(msg_id)", "path": "IPython\\zmq\\tests\\test_session.py", "repo_name": "jupyter-attic/ipython-py3k", "stars": 34, "license": "other", "language": "python", "size": 79217}
{"docstring": "\"\"\"Replace get/set/del functions of a property\"\"\"\n", "func_signal": "def update_property(old, new):\n", "code": "update_generic(old.fdel, new.fdel)\nupdate_generic(old.fget, new.fget)\nupdate_generic(old.fset, new.fset)", "path": "IPython\\quarantine\\ipy_autoreload.py", "repo_name": "jupyter-attic/ipython-py3k", "stars": 34, "license": "other", "language": "python", "size": 79217}
{"docstring": "\"\"\"scrub the front for zmq IDENTITIES\"\"\"\n", "func_signal": "def test_feed_identities(self):\n", "code": "theids = \"engine client other\".split()\ncontent = dict(code='whoda',stuff=object())\nthemsg = self.session.msg('execute',content=content)\npmsg = theids", "path": "IPython\\zmq\\tests\\test_session.py", "repo_name": "jupyter-attic/ipython-py3k", "stars": 34, "license": "other", "language": "python", "size": 79217}
{"docstring": "# self.client.clear(block=True)\n# close fds:\n", "func_signal": "def tearDown(self):\n", "code": "for e in [e for e in launchers if e.poll() is not None]:\n    launchers.remove(e)\n\n# allow flushing of incoming messages to prevent crash on socket close\nself.client.wait(timeout=2)\n# time.sleep(2)\nself.client.spin()\nself.client.close()\nBaseZMQTestCase.tearDown(self)\n# this will be redundant when pyzmq merges PR #88\n# self.context.term()\n# print tempfile.TemporaryFile().fileno(),\n# sys.stdout.flush()", "path": "IPython\\parallel\\tests\\clienttest.py", "repo_name": "jupyter-attic/ipython-py3k", "stars": 34, "license": "other", "language": "python", "size": 79217}
{"docstring": "\"\"\"sleep for a time\"\"\"\n", "func_signal": "def wait(n):\n", "code": "import time\ntime.sleep(n)\nreturn n", "path": "IPython\\parallel\\tests\\clienttest.py", "repo_name": "jupyter-attic/ipython-py3k", "stars": 34, "license": "other", "language": "python", "size": 79217}
{"docstring": "\"\"\"%aimport => Import modules for automatic reloading.\n\n%aimport\nList modules to automatically import and not to import.\n\n%aimport foo\nImport module 'foo' and mark it to be autoreloaded for %autoreload 1\n\n%aimport -foo\nMark module 'foo' to not be autoreloaded for %autoreload 1\n\n\"\"\"\n\n", "func_signal": "def aimport_f(self, parameter_s=''):\n", "code": "modname = parameter_s\nif not modname:\n    to_reload = list(reloader.modules.keys())\n    to_reload.sort()\n    to_skip = list(reloader.skip_modules.keys())\n    to_skip.sort()\n    if reloader.check_all:\n        print(\"Modules to reload:\\nall-expect-skipped\")\n    else:\n        print(\"Modules to reload:\\n%s\" % ' '.join(to_reload))\n    print(\"\\nModules to skip:\\n%s\" % ' '.join(to_skip))\nelif modname.startswith('-'):\n    modname = modname[1:]\n    try: del reloader.modules[modname]\n    except KeyError: pass\n    reloader.skip_modules[modname] = True\nelse:\n    try: del reloader.skip_modules[modname]\n    except KeyError: pass\n    reloader.modules[modname] = True\n\n    # Inject module to user namespace; handle also submodules properly\n    __import__(modname)\n    basename = modname.split('.')[0]\n    mod = sys.modules[basename]\n    ip.push({basename: mod})", "path": "IPython\\quarantine\\ipy_autoreload.py", "repo_name": "jupyter-attic/ipython-py3k", "stars": 34, "license": "other", "language": "python", "size": 79217}
{"docstring": "\"\"\" Add executables in all specified dirs to alias table\n \nUsage:\n\n%rehashdir c:/bin;c:/tools\n  - Add all executables under c:/bin and c:/tools to alias table, in \n  order to make them directly executable from any directory.\n    \n  Without arguments, add all executables in current directory.\n  \n\"\"\"\n\n# most of the code copied from Magic.magic_rehashx\n\n", "func_signal": "def rehashdir_f(self,arg):\n", "code": "def isjunk(fname):\n    junk = ['*~']\n    for j in junk:\n        if fnmatch.fnmatch(fname, j):\n            return True\n    return False\n\ncreated = []\nif not arg:\n    arg = '.'\npath = list(map(os.path.abspath,arg.split(';')))\nalias_table = self.shell.alias_manager.alias_table\n    \nif os.name == 'posix':\n    isexec = lambda fname:os.path.isfile(fname) and \\\n             os.access(fname,os.X_OK)\nelse:\n\n    try:\n        winext = os.environ['pathext'].replace(';','|').replace('.','')\n    except KeyError:\n        winext = 'exe|com|bat|py'\n    if 'py' not in winext:\n        winext += '|py'\n        \n    execre = re.compile(r'(.*)\\.(%s)$' % winext,re.IGNORECASE)\n    isexec = lambda fname:os.path.isfile(fname) and execre.match(fname)\nsavedir = os.getcwd()\ntry:\n    # write the whole loop for posix/Windows so we don't have an if in\n    # the innermost part\n    if os.name == 'posix':\n        for pdir in path:\n            os.chdir(pdir)\n            for ff in os.listdir(pdir):\n                if isexec(ff) and not isjunk(ff):\n                    # each entry in the alias table must be (N,name),\n                    # where N is the number of positional arguments of the\n                    # alias.\n                    src,tgt = os.path.splitext(ff)[0], os.path.abspath(ff)\n                    created.append(src)                        \n                    alias_table[src] = (0,tgt)\n    else:\n        for pdir in path:\n            os.chdir(pdir)\n            for ff in os.listdir(pdir):\n                if isexec(ff) and not isjunk(ff):\n                    src, tgt = execre.sub(r'\\1',ff), os.path.abspath(ff)\n                    src = src.lower()\n                    created.append(src)                                                \n                    alias_table[src] = (0,tgt)\n    # Make sure the alias table doesn't contain keywords or builtins\n    self.shell.alias_table_validate()\n    # Call again init_auto_alias() so we get 'rm -i' and other\n    # modified aliases since %rehashx will probably clobber them\n    # self.shell.init_auto_alias()\nfinally:\n    os.chdir(savedir)\nreturn created", "path": "IPython\\quarantine\\ipy_rehashdir.py", "repo_name": "jupyter-attic/ipython-py3k", "stars": 34, "license": "other", "language": "python", "size": 79217}
{"docstring": "\"\"\"Insert Row\n\n:param row_data:\n    A dictionary containing row data.\n:return:\n    A row dictionary for the inserted row.\n\"\"\"\n", "func_signal": "def insert_row(self, row_data):\n", "code": "entry = self.gd_client.InsertRow(row_data, **self.keys)\nif not isinstance(entry, gdata.spreadsheet.SpreadsheetsList):\n    raise WorksheetException(\"Row insert failed: '{0}'\".format(entry))\nif self.entries:\n    self.entries.append(entry)\nreturn self._row_to_dict(entry)", "path": "google_spreadsheet\\api.py", "repo_name": "yoavaviram/python-google-spreadsheet", "stars": 63, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"Test Delete Row By ID.\n\nFirst gets all rows, than inserts a new row, finally deletes the new\nrow by ID.\n\"\"\"\n", "func_signal": "def test_delete_by_id(self):\n", "code": "rows = self.sheet.get_rows()\nnum_rows = len(rows)\nnew_row = rows[0]\nnew_row = self.sheet.insert_row(new_row)\ninsert_rows = self.sheet.get_rows()\nassert_equals(len(insert_rows), num_rows + 1)\nself.sheet._flush_cache()\ninsert_rows = self.sheet.get_rows()\nassert_equals(len(insert_rows), num_rows + 1)\nself.sheet.delete_row(new_row)\ndelete_rows = self.sheet.get_rows()\nassert_equals(len(delete_rows), num_rows)\nassert_equals(delete_rows[-1], rows[-1])\nself.sheet._flush_cache()\ndelete_rows = self.sheet.get_rows()\nassert_equals(len(delete_rows), num_rows)\nassert_equals(delete_rows[-1], rows[-1])", "path": "tests.py", "repo_name": "yoavaviram/python-google-spreadsheet", "stars": 63, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"Initialise a Spreadsheet API wrapper.\n\n:param email:\n    A string representing a google login email.\n:param password:\n    A string representing a google login password.\n:param source:\n    A string representing source (much like a user agent).\n\"\"\"\n", "func_signal": "def __init__(self, email, password, source):\n", "code": "self.email = email\nself.password = password\nself.source = source", "path": "google_spreadsheet\\api.py", "repo_name": "yoavaviram/python-google-spreadsheet", "stars": 63, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"Turn a row of values into a dictionary.\n:param row:\n    A row element.\n:return:\n    A dictionary with rows.\n\"\"\"\n", "func_signal": "def _row_to_dict(self, row):\n", "code": "result = dict([(key, row.custom[key].text) for key in row.custom])\nresult[ID_FIELD] = row.id.text.split('/')[-1]\nreturn result", "path": "google_spreadsheet\\api.py", "repo_name": "yoavaviram/python-google-spreadsheet", "stars": 63, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"Set Up.\n\nInitialize the Amazon API wrapper. The following values:\n\n* GOOGLE_SPREADSHEET_USER\n* GOOGLE_SPREADSHEET_PASSWORD\n* GOOGLE_SPREADSHEET_SOURCE\n* GOOGLE_SPREADSHEET_KEY\n* GOOGLE_WORKSHEET_KEY\n* COLUMN_NAME\n* COLUMN_UNIQUE_VALUE\n\nAre imported from a custom file named: 'test_settings.py'\n\"\"\"\n", "func_signal": "def setUp(self):\n", "code": "self.spreadsheet = SpreadsheetAPI(GOOGLE_SPREADSHEET_USER,\n    GOOGLE_SPREADSHEET_PASSWORD, GOOGLE_SPREADSHEET_SOURCE)", "path": "tests.py", "repo_name": "yoavaviram/python-google-spreadsheet", "stars": 63, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"Delete Row (By ID).\n\nRequires that the given row dictionary contains an ID_FIELD.\n:param row:\n    A row dictionary to delete.\n\"\"\"\n", "func_signal": "def delete_row(self, row):\n", "code": "try:\n    id = row[ID_FIELD]\nexcept KeyError:\n    raise WorksheetException(\"Row does not contain '{0}' field. \"\n                        \"Please delete by index.\".format(ID_FIELD))\nentry = self._get_row_entry_by_id(id)\nself.gd_client.DeleteRow(entry)\nfor i, e in enumerate(self.entries):\n    if e.id.text == entry.id.text:\n        del self.entries[i]", "path": "google_spreadsheet\\api.py", "repo_name": "yoavaviram/python-google-spreadsheet", "stars": 63, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"Test Get  Worksheet.\n\nTests the get worksheet method by calling it and testing that a\nresult was returned.\n\"\"\"\n", "func_signal": "def test_get_worksheet(self):\n", "code": "sheet = self.spreadsheet.get_worksheet(GOOGLE_SPREADSHEET_KEY,\n    GOOGLE_WORKSHEET_KEY)\nassert_true(sheet)", "path": "tests.py", "repo_name": "yoavaviram/python-google-spreadsheet", "stars": 63, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"Make Query.\n\n A utility method to construct a query.\n\n:return:\n    A :class:`~,gdata.spreadsheet.service.ListQuery` or None.\n\"\"\"\n", "func_signal": "def _make_query(self, query=None, order_by=None, reverse=None):\n", "code": "if query or order_by or reverse:\n    q = gdata.spreadsheet.service.ListQuery()\n    if query:\n        q.sq = query\n    if order_by:\n        q.orderby = order_by\n    if reverse:\n        q.reverse = reverse\n    return q\nelse:\n    return None", "path": "google_spreadsheet\\api.py", "repo_name": "yoavaviram/python-google-spreadsheet", "stars": 63, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"Initialize a `gdata` client.\n\n:returns:\n    A gdata client.\n\"\"\"\n", "func_signal": "def _get_client(self):\n", "code": "gd_client = gdata.spreadsheet.service.SpreadsheetsService()\ngd_client.email = self.email\ngd_client.password = self.password\ngd_client.source = self.source\ngd_client.ProgrammaticLogin()\nreturn gd_client", "path": "google_spreadsheet\\api.py", "repo_name": "yoavaviram/python-google-spreadsheet", "stars": 63, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"Get Row Entries.\n\n:return:\n    A rows entry.\n\"\"\"\n", "func_signal": "def _get_row_entries(self, query=None):\n", "code": "if not self.entries:\n    self.entries = self.gd_client.GetListFeed(\n        query=query, **self.keys).entry\nreturn self.entries", "path": "google_spreadsheet\\api.py", "repo_name": "yoavaviram/python-google-spreadsheet", "stars": 63, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"List Spreadsheets.\n\n:return:\n    A list with information about the spreadsheets available\n\"\"\"\n", "func_signal": "def list_spreadsheets(self):\n", "code": "sheets = self._get_client().GetSpreadsheetsFeed()\nreturn map(lambda e: (e.title.text, e.id.text.rsplit('/', 1)[1]),\n    sheets.entry)", "path": "google_spreadsheet\\api.py", "repo_name": "yoavaviram/python-google-spreadsheet", "stars": 63, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"Test Sort.\n\nSort ascending and descending.\n\"\"\"\n", "func_signal": "def test_sort(self):\n", "code": "rows = self.sheet.get_rows(\n    order_by='column:{0}'.format(COLUMN_NAME), reverse='false')\nassert_true(rows)", "path": "tests.py", "repo_name": "yoavaviram/python-google-spreadsheet", "stars": 63, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"Test Update Rows By Index.\n\nFirst gets all rows, than updates last row.\n\"\"\"\n", "func_signal": "def test_update_row_by_index(self):\n", "code": "rows = self.sheet.get_rows()\nrow_index = len(rows) - 1\nnew_row = rows[0]\nrow = self.sheet.update_row_by_index(row_index, new_row)\ndel row['__rowid__']\ndel new_row['__rowid__']\nassert_equals(row, new_row)", "path": "tests.py", "repo_name": "yoavaviram/python-google-spreadsheet", "stars": 63, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"Test List Spreadsheets.\n\nTests the list spreadsheets method by calling it and testing that at\nleast one result was returned.\n\"\"\"\n", "func_signal": "def test_list_spreadsheets(self):\n", "code": "sheets = self.spreadsheet.list_spreadsheets()\nassert_true(len(sheets))", "path": "tests.py", "repo_name": "yoavaviram/python-google-spreadsheet", "stars": 63, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"Test Get Rows.\n\"\"\"\n", "func_signal": "def test_get_rows(self):\n", "code": "rows = self.sheet.get_rows()\nassert_true(len(rows))", "path": "tests.py", "repo_name": "yoavaviram/python-google-spreadsheet", "stars": 63, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"Delete Row By Index\n\n:param index:\n    A row index. Index is relative to the returned result set, not to\n    the original spreadsheet.\n\"\"\"\n", "func_signal": "def delete_row_by_index(self, index):\n", "code": "entry = self._get_row_entries(self.query)[index]\nself.gd_client.DeleteRow(entry)\ndel self.entries[index]", "path": "google_spreadsheet\\api.py", "repo_name": "yoavaviram/python-google-spreadsheet", "stars": 63, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"Tests deleting of all rows in the sheet\n\"\"\"\n# first retrieve rows and store in memory to re-add after test\n", "func_signal": "def test_delete_all_rows(self):\n", "code": "rows = self.sheet.get_rows()\nself.sheet.delete_all_rows()\nassert_equals(len(self.sheet.get_rows()), 0)\n# add back the rows that were there so the other tests still pass\nfor row in rows:\n  self.sheet.insert_row(row)", "path": "tests.py", "repo_name": "yoavaviram/python-google-spreadsheet", "stars": 63, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"Test Filter.\n\nTests filter in memory.\n\"\"\"\n", "func_signal": "def test_filter(self):\n", "code": "filtered_rows = self.sheet.get_rows(\n    filter_func=lambda row: row[COLUMN_NAME] == unicode(\n        COLUMN_UNIQUE_VALUE))\nassert_equals(1, len(filtered_rows))", "path": "tests.py", "repo_name": "yoavaviram/python-google-spreadsheet", "stars": 63, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"Update Row (By ID).\n\nOnly the fields supplied will be updated.\n:param row_data:\n    A dictionary containing row data. The row will be updated according\n    to the value in the ID_FIELD.\n:return:\n    The updated row.\n\"\"\"\n", "func_signal": "def update_row(self, row_data):\n", "code": "try:\n    id = row_data[ID_FIELD]\nexcept KeyError:\n    raise WorksheetException(\"Row does not contain '{0}' field. \"\n                        \"Please update by index.\".format(ID_FIELD))\nentry = self._get_row_entry_by_id(id)\nnew_row = self._row_to_dict(entry)\nnew_row.update(row_data)\nentry = self.gd_client.UpdateRow(entry, new_row)\nif not isinstance(entry, gdata.spreadsheet.SpreadsheetsList):\n    raise WorksheetException(\"Row update failed: '{0}'\".format(entry))\nfor i, e in enumerate(self.entries):\n    if e.id.text == entry.id.text:\n        self.entries[i] = entry\nreturn self._row_to_dict(entry)", "path": "google_spreadsheet\\api.py", "repo_name": "yoavaviram/python-google-spreadsheet", "stars": 63, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"Delete All Rows\n\"\"\"\n", "func_signal": "def delete_all_rows(self):\n", "code": "entries = self._get_row_entries(self.query)\nfor entry in entries:\n    self.gd_client.DeleteRow(entry)\nself._flush_cache()", "path": "google_spreadsheet\\api.py", "repo_name": "yoavaviram/python-google-spreadsheet", "stars": 63, "license": "other", "language": "python", "size": 139}
{"docstring": "\"\"\"Process a <star> AIML element.\n\nOptional attribute elements:\n    index: Which \"*\" character in the current pattern should\n    be matched?\n\n<star> elements return the text fragment matched by the \"*\"\ncharacter in the current input pattern.  For example, if the\ninput \"Hello Tom Smith, how are you?\" matched the pattern\n\"HELLO * HOW ARE YOU\", then a <star> element in the template\nwould evaluate to \"Tom Smith\".\n\n\"\"\"\n", "func_signal": "def _processStar(self, elem, sessionID):\n", "code": "try: index = int(elem[1]['index'])\nexcept KeyError: index = 1\n# fetch the user's last input\ninputStack = self.getPredicate(self._inputStack, sessionID)\ninput = self._subbers['normal'].sub(inputStack[-1])\n# fetch the Kernel's last response (for 'that' context)\noutputHistory = self.getPredicate(self._outputHistory, sessionID)\ntry: that = self._subbers['normal'].sub(outputHistory[-1])\nexcept: that = \"\" # there might not be any output yet\ntopic = self.getPredicate(\"topic\", sessionID)\nresponse = self._brain.star(\"star\", input, that, topic, index)\nreturn response", "path": "aiml\\Kernel.py", "repo_name": "xdlinux/Talkbot", "stars": 56, "license": "None", "language": "python", "size": 137}
{"docstring": "\"\"\"Process a <template> AIML element.\n\n<template> elements recursively process their contents, and\nreturn the results.  <template> is the root node of any AIML\nresponse tree.\n\n\"\"\"\n", "func_signal": "def _processTemplate(self,elem, sessionID):\n", "code": "response = \"\"\nfor e in elem[2:]:\n    response += self._processElement(e, sessionID)\nreturn response", "path": "aiml\\Kernel.py", "repo_name": "xdlinux/Talkbot", "stars": 56, "license": "None", "language": "python", "size": 137}
{"docstring": "\"\"\"Translate text, returns the modified text.\"\"\"\n", "func_signal": "def sub(self, text):\n", "code": "if self._regexIsDirty:\n    self._update_regex()\nreturn self._regex.sub(self, text)", "path": "aiml\\WordSub.py", "repo_name": "xdlinux/Talkbot", "stars": 56, "license": "None", "language": "python", "size": 137}
{"docstring": "\"\"\"Return a copy of the session data dictionary for the\nspecified session.\n\nIf no sessionID is specified, return a dictionary containing\n*all* of the individual session dictionaries.\n\n\"\"\"\n", "func_signal": "def getSessionData(self, sessionID = None):\n", "code": "s = None\nif sessionID is not None:\n    try: s = self._sessions[sessionID]\n    except KeyError: s = {}\nelse:\n    s = self._sessions\nreturn copy.deepcopy(s)", "path": "aiml\\Kernel.py", "repo_name": "xdlinux/Talkbot", "stars": 56, "license": "None", "language": "python", "size": 137}
{"docstring": "\"\"\"Process an <li> AIML element.\n\nOptional attribute elements:\n    name: the name of a predicate to query.\n    value: the value to check that predicate for.\n\n<li> elements process their contents recursively and return\nthe results. They can only appear inside <condition> and\n<random> elements.  See _processCondition() and\n_processRandom() for details of their usage.\n \n\"\"\"\n", "func_signal": "def _processLi(self,elem, sessionID):\n", "code": "response = \"\"\nfor e in elem[2:]:\n    response += self._processElement(e, sessionID)\nreturn response", "path": "aiml\\Kernel.py", "repo_name": "xdlinux/Talkbot", "stars": 56, "license": "None", "language": "python", "size": 137}
{"docstring": "\"\"\"Initialize the object, and populate it with the entries in\nthe defaults dictionary.\n\n\"\"\"\n", "func_signal": "def __init__(self, defaults = {}):\n", "code": "self._regex = None\nself._regexIsDirty = True\nfor k,v in defaults.items():\n    self[k] = v", "path": "aiml\\WordSub.py", "repo_name": "xdlinux/Talkbot", "stars": 56, "license": "None", "language": "python", "size": 137}
{"docstring": "\"\"\"Process an <sr> AIML element.\n\n<sr> elements are shortcuts for <srai><star/></srai>.\n\n\"\"\"\n", "func_signal": "def _processSr(self,elem,sessionID):\n", "code": "star = self._processElement(['star',{}], sessionID)\nresponse = self._respond(star, sessionID)\nreturn response", "path": "aiml\\Kernel.py", "repo_name": "xdlinux/Talkbot", "stars": 56, "license": "None", "language": "python", "size": 137}
{"docstring": "\"\"\"Build re object based on the keys of the current\ndictionary.\n\n\"\"\"\n", "func_signal": "def _update_regex(self):\n", "code": "self._regex = re.compile(\"|\".join(map(self._wordToRegex, self.keys())))\nself._regexIsDirty = False", "path": "aiml\\WordSub.py", "repo_name": "xdlinux/Talkbot", "stars": 56, "license": "None", "language": "python", "size": 137}
{"docstring": "\"\"\"Process a <thatstar> AIML element.\n\nOptional element attributes:\n    index: Specifies which \"*\" in the <that> pattern to match.\n\n<thatstar> elements are similar to <star> elements, except\nthat where <star/> returns the portion of the input string\nmatched by a \"*\" character in the pattern, <thatstar/> returns\nthe portion of the previous input string that was matched by a\n\"*\" in the current category's <that> pattern.\n\n\"\"\"\n", "func_signal": "def _processThatstar(self, elem, sessionID):\n", "code": "try: index = int(elem[1]['index'])\nexcept KeyError: index = 1\n# fetch the user's last input\ninputStack = self.getPredicate(self._inputStack, sessionID)\ninput = self._subbers['normal'].sub(inputStack[-1])\n# fetch the Kernel's last response (for 'that' context)\noutputHistory = self.getPredicate(self._outputHistory, sessionID)\ntry: that = self._subbers['normal'].sub(outputHistory[-1])\nexcept: that = \"\" # there might not be any output yet\ntopic = self.getPredicate(\"topic\", sessionID)\nresponse = self._brain.star(\"thatstar\", input, that, topic, index)\nreturn response", "path": "aiml\\Kernel.py", "repo_name": "xdlinux/Talkbot", "stars": 56, "license": "None", "language": "python", "size": 137}
{"docstring": "\"\"\"Process a <sentence> AIML element.\n\n<sentence> elements process their contents recursively, and\nthen capitalize the first letter of the results.\n\n\"\"\"\n", "func_signal": "def _processSentence(self,elem, sessionID):\n", "code": "response = \"\"\nfor e in elem[2:]:\n    response += self._processElement(e, sessionID)\ntry:\n    response = response.strip()\n    words = string.split(response, \" \", 1)\n    words[0] = string.capitalize(words[0])\n    response = string.join(words)\n    return response\nexcept IndexError: # response was empty\n    return \"\"", "path": "aiml\\Kernel.py", "repo_name": "xdlinux/Talkbot", "stars": 56, "license": "None", "language": "python", "size": 137}
{"docstring": "\"\"\"Process a <person> AIML element.\n\n<person> elements process their contents recursively, and then\nconvert all pronouns in the results from 1st person to 2nd\nperson, and vice versa.  This subsitution is handled by the\naiml.WordSub module.\n\nIf the <person> tag is used atomically (e.g. <person/>), it is\na shortcut for <person><star/></person>.\n\n\"\"\"\n", "func_signal": "def _processPerson(self,elem, sessionID):\n", "code": "response = \"\"\nfor e in elem[2:]:\n    response += self._processElement(e, sessionID)\nif len(elem[2:]) == 0:  # atomic <person/> = <person><star/></person>\n    response = self._processElement(['star',{}], sessionID)    \nreturn self._subbers['person'].sub(response)", "path": "aiml\\Kernel.py", "repo_name": "xdlinux/Talkbot", "stars": 56, "license": "None", "language": "python", "size": 137}
{"docstring": "\"\"\"Process a <gender> AIML element.\n\n<gender> elements process their contents, and then swap the\ngender of any third-person singular pronouns in the result.\nThis subsitution is handled by the aiml.WordSub module.\n\n\"\"\"\n", "func_signal": "def _processGender(self,elem, sessionID):\n", "code": "response = \"\"\nfor e in elem[2:]:\n    response += self._processElement(e, sessionID)\nreturn self._subbers['gender'].sub(response)", "path": "aiml\\Kernel.py", "repo_name": "xdlinux/Talkbot", "stars": 56, "license": "None", "language": "python", "size": 137}
{"docstring": "\"\"\"Process a <learn> AIML element.\n\n<learn> elements process their contents recursively, and then\ntreat the result as an AIML file to open and learn.\n\n\"\"\"\n", "func_signal": "def _processLearn(self, elem, sessionID):\n", "code": "filename = \"\"\nfor e in elem[2:]:\n    filename += self._processElement(e, sessionID)\nself.learn(filename)\nreturn \"\"", "path": "aiml\\Kernel.py", "repo_name": "xdlinux/Talkbot", "stars": 56, "license": "None", "language": "python", "size": 137}
{"docstring": "\"\"\"Process a <topicstar> AIML element.\n\nOptional element attributes:\n    index: Specifies which \"*\" in the <topic> pattern to match.\n\n<topicstar> elements are similar to <star> elements, except\nthat where <star/> returns the portion of the input string\nmatched by a \"*\" character in the pattern, <topicstar/>\nreturns the portion of current topic string that was matched\nby a \"*\" in the current category's <topic> pattern.\n\n\"\"\"\n", "func_signal": "def _processTopicstar(self, elem, sessionID):\n", "code": "try: index = int(elem[1]['index'])\nexcept KeyError: index = 1\n# fetch the user's last input\ninputStack = self.getPredicate(self._inputStack, sessionID)\ninput = self._subbers['normal'].sub(inputStack[-1])\n# fetch the Kernel's last response (for 'that' context)\noutputHistory = self.getPredicate(self._outputHistory, sessionID)\ntry: that = self._subbers['normal'].sub(outputHistory[-1])\nexcept: that = \"\" # there might not be any output yet\ntopic = self.getPredicate(\"topic\", sessionID)\nresponse = self._brain.star(\"topicstar\", input, that, topic, index)\nreturn response", "path": "aiml\\Kernel.py", "repo_name": "xdlinux/Talkbot", "stars": 56, "license": "None", "language": "python", "size": 137}
{"docstring": "\"\"\"Process a <srai> AIML element.\n\n<srai> elements recursively process their contents, and then\npass the results right back into the AIML interpreter as a new\npiece of input.  The results of this new input string are\nreturned.\n\n\"\"\"\n", "func_signal": "def _processSrai(self,elem, sessionID):\n", "code": "newInput = \"\"\nfor e in elem[2:]:\n    newInput += self._processElement(e, sessionID)\nreturn self._respond(newInput, sessionID)", "path": "aiml\\Kernel.py", "repo_name": "xdlinux/Talkbot", "stars": 56, "license": "None", "language": "python", "size": 137}
{"docstring": "\"\"\"Process an AIML element.\n\nThe first item of the elem list is the name of the element's\nXML tag.  The second item is a dictionary containing any\nattributes passed to that tag, and their values.  Any further\nitems in the list are the elements enclosed by the current\nelement's begin and end tags; they are handled by each\nelement's handler function.\n\n\"\"\"\n", "func_signal": "def _processElement(self,elem, sessionID):\n", "code": "try:\n    handlerFunc = self._elementProcessors[elem[0]]\nexcept:\n    # Oops -- there's no handler function for this element\n    # type!\n    if self._verboseMode:\n        err = \"WARNING: No handler found for <%s> element\\n\" % elem[0].encode(self._textEncoding, 'replace')\n        sys.stderr.write(err)\n    return \"\"\nreturn handlerFunc(elem, sessionID)", "path": "aiml\\Kernel.py", "repo_name": "xdlinux/Talkbot", "stars": 56, "license": "None", "language": "python", "size": 137}
{"docstring": "\"\"\"Reset the brain to its initial state.\n\nThis is essentially equivilant to:\n    del(kern)\n    kern = aiml.Kernel()\n\n\"\"\"\n", "func_signal": "def resetBrain(self):\n", "code": "del(self._brain)\nself.__init__()", "path": "aiml\\Kernel.py", "repo_name": "xdlinux/Talkbot", "stars": 56, "license": "None", "language": "python", "size": 137}
{"docstring": "\"\"\"Process a <think> AIML element.\n\n<think> elements process their contents recursively, and then\ndiscard the results and return the empty string.  They're\nuseful for setting predicates and learning AIML files without\ngenerating any output.\n\n\"\"\"\n", "func_signal": "def _processThink(self,elem, sessionID):\n", "code": "for e in elem[2:]:\n    self._processElement(e, sessionID)\nreturn \"\"", "path": "aiml\\Kernel.py", "repo_name": "xdlinux/Talkbot", "stars": 56, "license": "None", "language": "python", "size": 137}
{"docstring": "\"\"\"Process a <person2> AIML element.\n\n<person2> elements process their contents recursively, and then\nconvert all pronouns in the results from 1st person to 3rd\nperson, and vice versa.  This subsitution is handled by the\naiml.WordSub module.\n\nIf the <person2> tag is used atomically (e.g. <person2/>), it is\na shortcut for <person2><star/></person2>.\n\n\"\"\"\n", "func_signal": "def _processPerson2(self,elem, sessionID):\n", "code": "response = \"\"\nfor e in elem[2:]:\n    response += self._processElement(e, sessionID)\nif len(elem[2:]) == 0:  # atomic <person2/> = <person2><star/></person2>\n    response = self._processElement(['star',{}], sessionID)\nreturn self._subbers['person2'].sub(response)", "path": "aiml\\Kernel.py", "repo_name": "xdlinux/Talkbot", "stars": 56, "license": "None", "language": "python", "size": 137}
{"docstring": "\"\"\"Create a new session with the specified ID string.\"\"\"\n", "func_signal": "def _addSession(self, sessionID):\n", "code": "if self._sessions.has_key(sessionID):\n    return\n# Create the session.\nself._sessions[sessionID] = {\n    # Initialize the special reserved predicates\n    self._inputHistory: [],\n    self._outputHistory: [],\n    self._inputStack: []\n}", "path": "aiml\\Kernel.py", "repo_name": "xdlinux/Talkbot", "stars": 56, "license": "None", "language": "python", "size": 137}
{"docstring": "\"\"\"\nexec_stmt ::= expr exprlist DUP_TOP EXEC_STMT\nexec_stmt ::= expr exprlist EXEC_STMT\n\"\"\"\n", "func_signal": "def n_exec_stmt(self, node):\n", "code": "self.write(self.indent, 'exec ')\nself.preorder(node[0])\nif node[1][0] != NONE:\n    sep = ' in '\n    for subnode in node[1]:\n        self.write(sep); sep = \", \"\n        self.preorder(subnode)\nself.print_()\nself.prune() # stop recursing", "path": "decompyle\\decompyle\\Walker.py", "repo_name": "devyn/unholy", "stars": 60, "license": "other", "language": "python", "size": 472}
{"docstring": "\"\"\"Find globals in this statement.\"\"\"\n", "func_signal": "def find_globals(node, globals):\n", "code": "for n in node:\n    if isinstance(n, AST):\n        #if n != 'stmt': # skip nested statements\n            globals = find_globals(n, globals)\n    elif n.type in __globals_tokens__:\n        globals[n.pattr] = None\nreturn globals", "path": "decompyle\\decompyle\\Walker.py", "repo_name": "devyn/unholy", "stars": 60, "license": "other", "language": "python", "size": 472}
{"docstring": "\"\"\"\nRemove recursive references to allow garbage\ncollector to collect this object.\n\"\"\"\n", "func_signal": "def cleanup(self):\n", "code": "for dict in (self.rule2func, self.rules, self.rule2name, self.first):\n\tfor i in dict.keys():\n\t\tdict[i] = None\nfor i in dir(self):\n\tsetattr(self, i, None)", "path": "decompyle\\decompyle\\new-Parser.py", "repo_name": "devyn/unholy", "stars": 60, "license": "other", "language": "python", "size": 472}
{"docstring": "\"\"\"Simple test program to disassemble a file.\"\"\"\n", "func_signal": "def _test():\n", "code": "if sys.argv[1:]:\n    if sys.argv[2:]:\n        sys.stderr.write(\"usage: python dis.py [-|file]\\n\")\n        sys.exit(2)\n    fn = sys.argv[1]\n    if not fn or fn == \"-\":\n        fn = None\nelse:\n    fn = None\nif fn is None:\n    f = sys.stdin\nelse:\n    f = open(fn)\nsource = f.read()\nif fn is not None:\n    f.close()\nelse:\n    fn = \"<stdin>\"\ncode = compile(source, fn, \"exec\")\ndis(code)", "path": "decompyle\\decompyle\\dis_25.py", "repo_name": "devyn/unholy", "stars": 60, "license": "other", "language": "python", "size": 472}
{"docstring": "\"\"\"\nRemove recursive references to allow garbage\ncollector to collect this object.\n\"\"\"\n", "func_signal": "def cleanup(self):\n", "code": "for dict in (self.rule2func, self.rules, self.rule2name, self.first):\n\tfor i in dict.keys():\n\t\tdict[i] = None\nfor i in dir(self):\n\tsetattr(self, i, None)", "path": "decompyle\\decompyle\\Parser.py", "repo_name": "devyn/unholy", "stars": 60, "license": "other", "language": "python", "size": 472}
{"docstring": "#\n#  Special handling for opcodes that take a variable number\n#  of arguments -- we add a new rule for each:\n#\n#\texpr ::= {expr}^n BUILD_LIST_n\n#\texpr ::= {expr}^n BUILD_TUPLE_n\n#\texpr ::= {expr}^n BUILD_SLICE_n\n#\tunpack_list ::= UNPACK_LIST {expr}^n\n#\tunpack ::= UNPACK_TUPLE {expr}^n\n#\tunpack ::= UNPACK_SEQEUENE {expr}^n\n#\tmkfunc ::= {expr}^n LOAD_CONST MAKE_FUNCTION_n\n#\tmkfunc ::= {expr}^n load_closure LOAD_CONST MAKE_FUNCTION_n\n#\texpr ::= expr {expr}^n CALL_FUNCTION_n\n#\texpr ::= expr {expr}^n CALL_FUNCTION_VAR_n POP_TOP\n#\texpr ::= expr {expr}^n CALL_FUNCTION_VAR_KW_n POP_TOP\n#\texpr ::= expr {expr}^n CALL_FUNCTION_KW_n POP_TOP\n#\n", "func_signal": "def parse(tokens, customize):\n", "code": "global p\nfor k, v in customize.items():\n\t# avoid adding the same rule twice to this parser\n\tif p.customized.has_key(k):\n\t\tcontinue\n\tp.customized[k] = None\n\n\t#nop = lambda self, args: None\n\top = k[:string.rfind(k, '_')]\n\tif op in ('BUILD_LIST', 'BUILD_TUPLE'):\n\t\trule = 'build_list ::= ' + 'expr '*v + k\n\telif op == 'BUILD_SLICE':\n\t\trule = 'expr ::= ' + 'expr '*v + k\n\telif op in ('UNPACK_TUPLE', 'UNPACK_SEQUENCE'):\n\t\trule = 'unpack ::= ' + k + ' designator'*v\n\telif op == 'UNPACK_LIST':\n\t\trule = 'unpack_list ::= ' + k + ' designator'*v\n\telif op == 'DUP_TOPX':\n\t\t# no need to add a rule\n\t\tcontinue\n\t\t#rule = 'dup_topx ::= ' + 'expr '*v + k\n\telif op == 'MAKE_FUNCTION':\n\t\tp.addRule('mklambda ::= %s LOAD_LAMBDA %s' %\n\t\t\t  ('expr '*v, k), nop)\n\t\trule = 'mkfunc ::= %s LOAD_CONST %s' % ('expr '*v, k)\n\telif op == 'MAKE_CLOSURE':\n\t\tp.addRule('mklambda ::= %s load_closure LOAD_LAMBDA %s' %\n\t\t\t  ('expr '*v, k), nop)\n\t\trule = 'mkfunc ::= %s load_closure LOAD_CONST %s' % ('expr '*v, k)\n\telif op in ('CALL_FUNCTION', 'CALL_FUNCTION_VAR',\n\t\t    'CALL_FUNCTION_VAR_KW', 'CALL_FUNCTION_KW'):\n\t\tna = (v & 0xff)           # positional parameters\n\t\tnk = (v >> 8) & 0xff      # keyword parameters\n\t\t# number of apply equiv arguments:\n\t\tnak = ( len(op)-len('CALL_FUNCTION') ) / 3\n\t\trule = 'expr ::= expr ' + 'expr '*na + 'kwarg '*nk \\\n\t\t       + 'expr ' * nak + k\n\telse:\n\t\traise 'unknown customize token %s' % k\n\tp.addRule(rule, nop)\nast = p.parse(tokens)\n#p.cleanup()\nreturn ast", "path": "decompyle\\decompyle\\Parser.py", "repo_name": "devyn/unholy", "stars": 60, "license": "other", "language": "python", "size": 472}
{"docstring": "'''\nexpr ::= load_closure mklambda\nexpr ::= mklambda\nexpr ::= SET_LINENO\nexpr ::= LOAD_FAST\nexpr ::= LOAD_NAME\nexpr ::= LOAD_CONST\nexpr ::= LOAD_GLOBAL\nexpr ::= LOAD_DEREF\nexpr ::= LOAD_LOCALS\nexpr ::= expr LOAD_ATTR\nexpr ::= binary_expr\nexpr ::= build_list\n\nbinary_expr ::= expr expr binary_op\nbinary_op ::= BINARY_ADD\nbinary_op ::= BINARY_SUBTRACT\nbinary_op ::= BINARY_MULTIPLY\nbinary_op ::= BINARY_DIVIDE\nbinary_op ::= BINARY_TRUE_DIVIDE\nbinary_op ::= BINARY_FLOOR_DIVIDE\nbinary_op ::= BINARY_MODULO\nbinary_op ::= BINARY_LSHIFT\nbinary_op ::= BINARY_RSHIFT\nbinary_op ::= BINARY_AND\nbinary_op ::= BINARY_OR\nbinary_op ::= BINARY_XOR\nbinary_op ::= BINARY_POWER\n\nexpr ::= binary_subscr\nbinary_subscr ::= expr expr BINARY_SUBSCR\nexpr ::= expr expr DUP_TOPX_2 BINARY_SUBSCR\nexpr ::= cmp\nexpr ::= expr UNARY_POSITIVE\nexpr ::= expr UNARY_NEGATIVE\nexpr ::= expr UNARY_CONVERT\nexpr ::= expr UNARY_INVERT\nexpr ::= expr UNARY_NOT\nexpr ::= mapexpr\nexpr ::= expr SLICE+0\nexpr ::= expr expr SLICE+1\nexpr ::= expr expr SLICE+2\nexpr ::= expr expr expr SLICE+3\nexpr ::= expr DUP_TOP SLICE+0\nexpr ::= expr expr DUP_TOPX_2 SLICE+1\nexpr ::= expr expr DUP_TOPX_2 SLICE+2\nexpr ::= expr expr expr DUP_TOPX_3 SLICE+3\nexpr ::= and\nexpr ::= or\nor   ::= expr JUMP_IF_TRUE  POP_TOP expr COME_FROM\nand  ::= expr JUMP_IF_FALSE POP_TOP expr COME_FROM\n\ncmp ::= cmp_list\ncmp ::= compare\ncompare ::= expr expr COMPARE_OP\ncmp_list ::= expr cmp_list1 ROT_TWO POP_TOP\n\t\tCOME_FROM\ncmp_list1 ::= expr DUP_TOP ROT_THREE\n\t\tCOMPARE_OP JUMP_IF_FALSE POP_TOP\n\t\tcmp_list1 COME_FROM\ncmp_list1 ::= expr DUP_TOP ROT_THREE\n\t\tCOMPARE_OP JUMP_IF_FALSE POP_TOP\n\t\tcmp_list2 COME_FROM\ncmp_list2 ::= expr COMPARE_OP JUMP_FORWARD\nmapexpr ::= BUILD_MAP kvlist\n\nkvlist ::= kvlist kv\nkvlist ::=\n\nkv ::= DUP_TOP expr ROT_TWO expr STORE_SUBSCR\nkv ::= DUP_TOP expr expr ROT_THREE STORE_SUBSCR\n\nexprlist ::= exprlist expr\nexprlist ::= expr\n\nnullexprlist ::=\n'''\n\n", "func_signal": "def p_expr(self, args):\n", "code": "\ncollect = ('stmts', 'exprlist', 'kvlist')\n\nif nt in collect and len(args) > 1:\n\t#\n\t#  Collect iterated thingies together.\n\t#\n\trv = args[0]\n\trv.append(args[1])\nelse:\n\trv = GenericASTBuilder.nonterminal(self, nt, args)\nreturn rv", "path": "decompyle\\decompyle\\new-Parser.py", "repo_name": "devyn/unholy", "stars": 60, "license": "other", "language": "python", "size": 472}
{"docstring": "\"\"\"Detect all offsets in a byte code which are jump targets.\n\nReturn the list of offsets.\n\n\"\"\"\n", "func_signal": "def findlabels(code):\n", "code": "labels = []\nn = len(code)\ni = 0\nwhile i < n:\n\tc = code[i]\n\top = ord(c)\n\ti = i+1\n\tif op >= HAVE_ARGUMENT:\n\t\toparg = ord(code[i]) + ord(code[i+1])*256\n\t\ti = i+2\n\t\tlabel = -1\n\t\tif op in hasjrel:\n\t\t\tlabel = i+oparg\n\t\telif op in hasjabs:\n\t\t\tlabel = oparg\n\t\tif label >= 0:\n\t\t\tif label not in labels:\n\t\t\t\tlabels.append(label)\nreturn labels", "path": "decompyle\\decompyle\\dis_16.py", "repo_name": "devyn/unholy", "stars": 60, "license": "other", "language": "python", "size": 472}
{"docstring": "\"\"\"convert AST to source code\"\"\"\n\n# if code would be empty, append 'pass'\n", "func_signal": "def gen_source(self, ast, customize, isLambda=0):\n", "code": "if len(ast) == 0:\n    self.print_(self.indent, 'pass')\nelse:\n    self.customize(customize)\n    self.print_(self.traverse(ast, isLambda=isLambda))", "path": "decompyle\\decompyle\\Walker.py", "repo_name": "devyn/unholy", "stars": 60, "license": "other", "language": "python", "size": 472}
{"docstring": "\"\"\"Detect all offsets in a byte code which are jump targets.\n\nReturn the list of offsets.\n\n\"\"\"\n", "func_signal": "def findlabels(code):\n", "code": "labels = []\nn = len(code)\ni = 0\nwhile i < n:\n    c = code[i]\n    op = ord(c)\n    i = i+1\n    if op >= HAVE_ARGUMENT:\n        oparg = ord(code[i]) + ord(code[i+1])*256\n        i = i+2\n        label = -1\n        if op in hasjrel:\n            label = i+oparg\n        elif op in hasjabs:\n            label = oparg\n        if label >= 0:\n            if label not in labels:\n                labels.append(label)\nreturn labels", "path": "decompyle\\decompyle\\dis_25.py", "repo_name": "devyn/unholy", "stars": 60, "license": "other", "language": "python", "size": 472}
{"docstring": "#\n#  Special handling for opcodes that take a variable number\n#  of arguments -- we add a new rule for each:\n#\n#\texpr ::= {expr}^n BUILD_LIST_n\n#\texpr ::= {expr}^n BUILD_TUPLE_n\n#\texpr ::= {expr}^n BUILD_SLICE_n\n#\tunpack_list ::= UNPACK_LIST {expr}^n\n#\tunpack ::= UNPACK_TUPLE {expr}^n\n#\tunpack ::= UNPACK_SEQEUENE {expr}^n\n#\tmkfunc ::= {expr}^n LOAD_CONST MAKE_FUNCTION_n\n#\tmkfunc ::= {expr}^n load_closure LOAD_CONST MAKE_FUNCTION_n\n#\texpr ::= expr {expr}^n CALL_FUNCTION_n\n#\texpr ::= expr {expr}^n CALL_FUNCTION_VAR_n POP_TOP\n#\texpr ::= expr {expr}^n CALL_FUNCTION_VAR_KW_n POP_TOP\n#\texpr ::= expr {expr}^n CALL_FUNCTION_KW_n POP_TOP\n#\n", "func_signal": "def parse(tokens, customize):\n", "code": "global p\nfor k, v in customize.items():\n\t# avoid adding the same rule twice to this parser\n\tif p.customized.has_key(k):\n\t\tcontinue\n\tp.customized[k] = None\n\n\t#nop = lambda self, args: None\n\top = k[:string.rfind(k, '_')]\n\tif op in ('BUILD_LIST', 'BUILD_TUPLE'):\n\t\trule = 'build_list ::= ' + 'expr '*v + k\n\telif op == 'BUILD_SLICE':\n\t\trule = 'expr ::= ' + 'expr '*v + k\n\telif op in ('UNPACK_TUPLE', 'UNPACK_SEQUENCE'):\n\t\trule = 'unpack ::= ' + k + ' designator'*v\n\telif op == 'UNPACK_LIST':\n\t\trule = 'unpack_list ::= ' + k + ' designator'*v\n\telif op == 'DUP_TOPX':\n\t\t# no need to add a rule\n\t\tcontinue\n\t\t#rule = 'dup_topx ::= ' + 'expr '*v + k\n\telif op == 'MAKE_FUNCTION':\n\t\tp.addRule('mklambda ::= %s LOAD_LAMBDA %s' %\n\t\t\t  ('expr '*v, k), nop)\n\t\trule = 'mkfunc ::= %s LOAD_CONST %s' % ('expr '*v, k)\n\telif op == 'MAKE_CLOSURE':\n\t\tp.addRule('mklambda ::= %s load_closure LOAD_LAMBDA %s' %\n\t\t\t  ('expr '*v, k), nop)\n\t\trule = 'mkfunc ::= %s load_closure LOAD_CONST %s' % ('expr '*v, k)\n\telif op in ('CALL_FUNCTION', 'CALL_FUNCTION_VAR',\n\t\t    'CALL_FUNCTION_VAR_KW', 'CALL_FUNCTION_KW'):\n\t\tna = (v & 0xff)           # positional parameters\n\t\tnk = (v >> 8) & 0xff      # keyword parameters\n\t\t# number of apply equiv arguments:\n\t\tnak = ( len(op)-len('CALL_FUNCTION') ) / 3\n\t\trule = 'expr ::= expr ' + 'expr '*na + 'kwarg '*nk \\\n\t\t       + 'expr ' * nak + k\n\telse:\n\t\traise 'unknown customize token %s' % k\n\tp.addRule(rule, nop)\nast = p.parse(tokens)\n#p.cleanup()\nreturn ast", "path": "decompyle\\decompyle\\new-Parser.py", "repo_name": "devyn/unholy", "stars": 60, "license": "other", "language": "python", "size": 472}
{"docstring": "\"\"\"Simple test program to disassemble a file.\"\"\"\n", "func_signal": "def _test():\n", "code": "if sys.argv[1:]:\n\tif sys.argv[2:]:\n\t\tsys.stderr.write(\"usage: python dis.py [-|file]\\n\")\n\t\tsys.exit(2)\n\tfn = sys.argv[1]\n\tif not fn or fn == \"-\":\n\t\tfn = None\nelse:\n\tfn = None\nif not fn:\n\tf = sys.stdin\nelse:\n\tf = open(fn)\nsource = f.read()\nif fn:\n\tf.close()\nelse:\n\tfn = \"<stdin>\"\ncode = compile(source, fn, \"exec\")\ndis(code)", "path": "decompyle\\decompyle\\dis_16.py", "repo_name": "devyn/unholy", "stars": 60, "license": "other", "language": "python", "size": 472}
{"docstring": "# class definition ('class X(A,B,C):')\n", "func_signal": "def n_classdef(self, node):\n", "code": "assert node[0].pattr == node[-1][-1].pattr\nself.write(self.indent, 'class ', str(node[-1][-1].pattr))\nif node[1] != BUILD_TUPLE_0: # this is a subclass\n    self.preorder(node[1]) # output superclasses\nself.print_(':')\n\n# class body\nself.indentMore()\nself.build_class(node[-4][-2].attr)\nself.indentLess()\nself.prune()", "path": "decompyle\\decompyle\\Walker.py", "repo_name": "devyn/unholy", "stars": 60, "license": "other", "language": "python", "size": 472}
{"docstring": "'''\nexpr ::= load_closure mklambda\nexpr ::= mklambda\nexpr ::= SET_LINENO\nexpr ::= LOAD_FAST\nexpr ::= LOAD_NAME\nexpr ::= LOAD_CONST\nexpr ::= LOAD_GLOBAL\nexpr ::= LOAD_DEREF\nexpr ::= LOAD_LOCALS\nexpr ::= expr LOAD_ATTR\nexpr ::= binary_expr\nexpr ::= build_list\n\nbinary_expr ::= expr expr binary_op\nbinary_op ::= BINARY_ADD\nbinary_op ::= BINARY_SUBTRACT\nbinary_op ::= BINARY_MULTIPLY\nbinary_op ::= BINARY_DIVIDE\nbinary_op ::= BINARY_TRUE_DIVIDE\nbinary_op ::= BINARY_FLOOR_DIVIDE\nbinary_op ::= BINARY_MODULO\nbinary_op ::= BINARY_LSHIFT\nbinary_op ::= BINARY_RSHIFT\nbinary_op ::= BINARY_AND\nbinary_op ::= BINARY_OR\nbinary_op ::= BINARY_XOR\nbinary_op ::= BINARY_POWER\n\nexpr ::= binary_subscr\nbinary_subscr ::= expr expr BINARY_SUBSCR\nexpr ::= expr expr DUP_TOPX_2 BINARY_SUBSCR\nexpr ::= cmp\nexpr ::= expr UNARY_POSITIVE\nexpr ::= expr UNARY_NEGATIVE\nexpr ::= expr UNARY_CONVERT\nexpr ::= expr UNARY_INVERT\nexpr ::= expr UNARY_NOT\nexpr ::= mapexpr\nexpr ::= expr SLICE+0\nexpr ::= expr expr SLICE+1\nexpr ::= expr expr SLICE+2\nexpr ::= expr expr expr SLICE+3\nexpr ::= expr DUP_TOP SLICE+0\nexpr ::= expr expr DUP_TOPX_2 SLICE+1\nexpr ::= expr expr DUP_TOPX_2 SLICE+2\nexpr ::= expr expr expr DUP_TOPX_3 SLICE+3\nexpr ::= and\nexpr ::= and2\nexpr ::= or\nor   ::= expr JUMP_IF_TRUE  POP_TOP expr COME_FROM\nand  ::= expr JUMP_IF_FALSE POP_TOP expr COME_FROM\nand2 ::= _jump JUMP_IF_FALSE POP_TOP COME_FROM expr COME_FROM\n\ncmp ::= cmp_list\ncmp ::= compare\ncompare ::= expr expr COMPARE_OP\ncmp_list ::= expr cmp_list1 ROT_TWO POP_TOP\n\t\tCOME_FROM\ncmp_list1 ::= expr DUP_TOP ROT_THREE\n\t\tCOMPARE_OP JUMP_IF_FALSE POP_TOP\n\t\tcmp_list1 COME_FROM\ncmp_list1 ::= expr DUP_TOP ROT_THREE\n\t\tCOMPARE_OP JUMP_IF_FALSE POP_TOP\n\t\tcmp_list2 COME_FROM\ncmp_list2 ::= expr COMPARE_OP JUMP_FORWARD\nmapexpr ::= BUILD_MAP kvlist\n\nkvlist ::= kvlist kv\nkvlist ::= kvlist kv2\nkvlist ::=\n\nkv ::= DUP_TOP expr ROT_TWO expr STORE_SUBSCR\nkv2 ::= DUP_TOP expr expr ROT_THREE STORE_SUBSCR\n\nexprlist ::= exprlist expr\nexprlist ::= expr\n\nnullexprlist ::=\n'''\n\n", "func_signal": "def p_expr(self, args):\n", "code": "\ncollect = ('stmts', 'exprlist', 'kvlist')\n\nif nt in collect and len(args) > 1:\n\t#\n\t#  Collect iterated thingies together.\n\t#\n\trv = args[0]\n\trv.append(args[1])\nelse:\n\trv = GenericASTBuilder.nonterminal(self, nt, args)\nreturn rv", "path": "decompyle\\decompyle\\Parser.py", "repo_name": "devyn/unholy", "stars": 60, "license": "other", "language": "python", "size": 472}
{"docstring": "\"\"\"This has to be present\"\"\"\n\n", "func_signal": "def Doc_Test():\n", "code": "\ndef __init__(self):\n\t\"\"\"__init__: This has to be present\"\"\"\n\tself.a = 1\n\n\tdef XXX22():\n\t\t\"\"\"XXX22: This has to be present\"\"\"\n\t\tpass\n\ndef XXX11():\n\t\"\"\"XXX22: This has to be present\"\"\"\n\tpass\n\ndef XXX12():\n\tfoo = \"\"\"XXX22: This has to be present\"\"\"\n\tpass\n\ndef XXX13():\n\tpass", "path": "decompyle\\test\\test_docstring.py", "repo_name": "devyn/unholy", "stars": 60, "license": "other", "language": "python", "size": 472}
{"docstring": "\"\"\"\nSpecial handling for opcodes that take a variable number\nof arguments -- we add a new entry for each in TABLE_R.\n\"\"\"\n", "func_signal": "def customize(self, customize):\n", "code": "for k, v in customize.items():\n   if TABLE_R.has_key(k):\n      continue\n   op = k[ :k.rfind('_') ]\n   if op == 'CALL_FUNCTION':\tTABLE_R[k] = ('%c(%C)', 0, (1,-1,', '))\n   elif op in ('CALL_FUNCTION_VAR',\n               'CALL_FUNCTION_VAR_KW', 'CALL_FUNCTION_KW'):\n      if v == 0:\n         str = '%c(%C' # '%C' is a dummy here ...\n         p2 = (0, 0, None) # .. because of this\n      else:\n         str = '%c(%C, '\n         p2 = (1,-2, ', ')\n      if op == 'CALL_FUNCTION_VAR':\n         str += '*%c)'\n         entry = (str, 0, p2, -2)\n      elif op == 'CALL_FUNCTION_KW':\n         str += '**%c)'\n         entry = (str, 0, p2, -2)\n      else:\n         str += '*%c, **%c)'\n         if p2[2]: p2 = (1, -3, ', ')\n         entry = (str, 0, p2, -3, -2)\n      TABLE_R[k] = entry\n   ## handled by n_mapexpr:\n   ##if op == 'BUILD_SLICE':\tTABLE_R[k] = ('%C'    ,    (0,-1,':'))\n   ## handled by n_build_list:\n   ##if   op == 'BUILD_LIST':\tTABLE_R[k] = ('[%C]'  ,    (0,-1,', '))\n   ##elif op == 'BUILD_TUPLE':\tTABLE_R[k] = ('(%C%,)',    (0,-1,', '))", "path": "decompyle\\decompyle\\Walker.py", "repo_name": "devyn/unholy", "stars": 60, "license": "other", "language": "python", "size": 472}
{"docstring": "\"\"\"Dump class definition, doc string and class body.\"\"\"\n\n", "func_signal": "def build_class(self, code):\n", "code": "assert type(code) == CodeType\ncode = Code(code, self.scanner)\n#assert isinstance(code, Code)\n\nindent = self.indent\n#self.print_(indent, '#flags:\\t', int(code.co_flags))\nast = self.build_ast(code._tokens, code._customize)\ncode._tokens = None # save memory\nassert ast == 'stmts'\n\n# if docstring exists, dump it\nif code.co_consts[0] != None \\\n   and ast[0] == ASSIGN_DOC_STRING(code.co_consts[0]):\n    #print '\\n\\n>>-->>doc string set\\n\\n'\n    self.print_docstring(indent, code.co_consts[0])\n    del ast[0]\n\n# the function defining a class normally returns locals(); we\n# don't want this to show up in the source, thus remove the node\nif ast[-1] == RETURN_LOCALS:\n    ast.pop() # remove last node\n\nfor g in find_globals(ast, {}).keys():\n   self.print_(indent, 'global ', g)\nself.gen_source(ast, code._customize)\ncode._tokens = None; code._customize = None # save memory", "path": "decompyle\\decompyle\\Walker.py", "repo_name": "devyn/unholy", "stars": 60, "license": "other", "language": "python", "size": 472}
{"docstring": "\"\"\"\nprettyprint a mapexpr\n'mapexpr' is something like k = {'a': 1, 'b': 42 }\"\n\"\"\"\n# \" <- emacs happy\n", "func_signal": "def n_mapexpr(self, node):\n", "code": "assert node[-1] == 'kvlist'\nnode = node[-1] # goto kvlist\n\nself.indentMore(INDENT_PER_LEVEL)\nline_seperator = ',\\n' + self.indent\nsep = INDENT_PER_LEVEL[:-1]\nself.write('{')\nfor kv in node:\n    assert kv in ('kv', 'kv2')\n    # kv ::= DUP_TOP expr ROT_TWO expr STORE_SUBSCR\n    # kv2 ::= DUP_TOP expr expr ROT_THREE STORE_SUBSCR\n    if kv == 'kv':\n        name = self.traverse(kv[-2], indent='');\n        value = self.traverse(kv[1], indent=self.indent+(len(name)+2)*' ')\n    else:\n        name = self.traverse(kv[1], indent='');\n        value = self.traverse(kv[-3], indent=self.indent+(len(name)+2)*' ')\n    self.write(sep, name, ': ', value)\n    sep = line_seperator\nself.write('}')\nself.indentLess(INDENT_PER_LEVEL)\nself.prune()", "path": "decompyle\\decompyle\\Walker.py", "repo_name": "devyn/unholy", "stars": 60, "license": "other", "language": "python", "size": 472}
{"docstring": "\"\"\"\nIf the name of the formal parameter starts with dot,\nit's a tuple parameter, like this:\n#          def MyFunc(xx, (a,b,c), yy):\n#                  print a, b*2, c*42\nIn byte-code, the whole tuple is assigned to parameter '.1' and\nthen the tuple gets unpacked to 'a', 'b' and 'c'.\n\nSince identifiers starting with a dot are illegal in Python,\nwe can search for the byte-code equivalent to '(a,b,c) = .1'\n\"\"\"\n", "func_signal": "def get_tuple_parameter(self, ast, name):\n", "code": "assert ast == 'stmts'\nfor i in range(len(ast)):\n    # search for an assign-statement\n    assert ast[i] == 'stmt'\n    node = ast[i][0]\n    if node == 'assign' \\\n       and node[0] == ASSIGN_TUPLE_PARAM(name):\n        # okay, this assigns '.n' to something\n        del ast[i]\n        # walk lhs; this\n        # returns a tuple of identifiers as used\n        # within the function definition\n        assert node[1] == 'designator'\n        # if lhs is not a UNPACK_TUPLE (or equiv.),\n        # add parenteses to make this a tuple\n        if node[1][0] not in ('unpack', 'unpack_list'):\n            return '(' + self.traverse(node[1], indent='') + ')'\n        return self.traverse(node[1], indent='')\nraise \"Can't find tuple parameter\" % name", "path": "decompyle\\decompyle\\Walker.py", "repo_name": "devyn/unholy", "stars": 60, "license": "other", "language": "python", "size": 472}
{"docstring": "\"\"\"Find the offsets in a byte code which are start of lines in the source.\n\nGenerate pairs (offset, lineno) as described in Python/compile.c.\n\n\"\"\"\n", "func_signal": "def findlinestarts(code):\n", "code": "byte_increments = [ord(c) for c in code.co_lnotab[0::2]]\nline_increments = [ord(c) for c in code.co_lnotab[1::2]]\n\nlastlineno = None\nlineno = code.co_firstlineno\naddr = 0\nfor byte_incr, line_incr in zip(byte_increments, line_increments):\n    if byte_incr:\n        if lineno != lastlineno:\n            yield (addr, lineno)\n            lastlineno = lineno\n        addr += byte_incr\n    lineno += line_incr\nif lineno != lastlineno:\n    yield (addr, lineno)", "path": "decompyle\\decompyle\\dis_25.py", "repo_name": "devyn/unholy", "stars": 60, "license": "other", "language": "python", "size": 472}
{"docstring": "\"\"\"Returns the raw ammount ever received by this wallet.\"\"\"\n", "func_signal": "def total_received(self, minconf=settings.BITCOIN_MINIMUM_CONFIRMATIONS):\n", "code": "if settings.BITCOIN_TRANSACTION_SIGNALING:\n    if minconf == settings.BITCOIN_MINIMUM_CONFIRMATIONS:\n        s = self.addresses.aggregate(models.Sum(\"least_received_confirmed\"))['least_received_confirmed__sum'] or Decimal(0)\n    elif minconf == 0:\n        s = self.addresses.aggregate(models.Sum(\"least_received\"))['least_received__sum'] or Decimal(0)\n    else:\n        s = sum([a.received(minconf=minconf) for a in self.addresses.all()])\nelse:\n    s = sum([a.received(minconf=minconf) for a in self.addresses.all()])\nrt = self.received_transactions.aggregate(models.Sum(\"amount\"))['amount__sum'] or Decimal(0)\nreturn (s + rt)", "path": "hitstarter\\hitstarter\\apps\\django_bitcoin\\models.py", "repo_name": "Miserlou/HitStarter", "stars": 32, "license": "None", "language": "python", "size": 1330}
{"docstring": "#if len(bitcoinpayments)!=len(addresses_shares):\n#    raise Exception(\"\")\n", "func_signal": "def withdraw_all(cls, bitcoinpayments, addresses_shares):\n", "code": "amounts_all=Payment.calculate_amounts(bitcoinpayments, addresses_shares)\nfor bp in bitcoinpayments:\n    am=bp.withdraw_amounts(addresses_shares)\n    bp.withdraw_addresses=\",\".join(addresses_shares.keys())\n    bp.withdraw_proportions=\",\".join(\n        [str(x) for x in addresses_shares.values()])\n    bp.withdraw_amounts=\",\".join(\n        [str(x) for x in am])\n    bp.withdrawn_at=datetime.datetime.now()\n    bp.withdrawn_total=sum(am)\n    bp.save()\nfor i, share in enumerate(addresses_shares.keys()):\n    bitcoind.send(share, amounts_all[i])\nreturn True", "path": "hitstarter\\hitstarter\\apps\\django_bitcoin\\models.py", "repo_name": "Miserlou/HitStarter", "stars": 32, "license": "None", "language": "python", "size": 1330}
{"docstring": "\"\"\"Adds (XORs) the round key to the state.\"\"\"\n", "func_signal": "def addRoundKey(self, state, roundKey):\n", "code": "for i in range(16):\n    state[i] ^= roundKey[i]\nreturn state", "path": "hitstarter\\hitstarter\\apps\\django_bitcoin\\pywallet.py", "repo_name": "Miserlou/HitStarter", "stars": 32, "license": "None", "language": "python", "size": 1330}
{"docstring": "'''\nDisplay a bitcoin address with plus the link to its blockexplorer page.\n'''\n# note: i disapprove including somewhat unnecessary depencies such as this, especially since blockexplorer is  unreliable service\n", "func_signal": "def show_addr(address, arg):\n", "code": "link =\"<a href='http://blockexplorer.com/%s/'>%s</a>\"\nif arg == 'long':\n    return link % (address, address)\nelse:\n    return link % (address, address[:8])", "path": "hitstarter\\hitstarter\\apps\\django_bitcoin\\templatetags\\currency_conversions.py", "repo_name": "Miserlou/HitStarter", "stars": 32, "license": "None", "language": "python", "size": 1330}
{"docstring": "\"\"\"Galois multiplication of 8 bit characters a and b.\"\"\"\n", "func_signal": "def galois_multiplication(self, a, b):\n", "code": "p = 0\nfor counter in range(8):\n    if b & 1: p ^= a\n    hi_bit_set = a & 0x80\n    a <<= 1\n    # keep a 8 bit\n    a &= 0xFF\n    if hi_bit_set:\n        a ^= 0x1b\n    b >>= 1\nreturn p", "path": "hitstarter\\hitstarter\\apps\\django_bitcoin\\pywallet.py", "repo_name": "Miserlou/HitStarter", "stars": 32, "license": "None", "language": "python", "size": 1330}
{"docstring": "# iterate over the 4 columns\n", "func_signal": "def mixColumns(self, state, isInv):\n", "code": "for i in range(4):\n    # construct one column by slicing over the 4 rows\n    column = state[i:i+16:4]\n    # apply the mixColumn on one column\n    column = self.mixColumn(column, isInv)\n    # put the values back into the state\n    state[i:i+16:4] = column\n\nreturn state", "path": "hitstarter\\hitstarter\\apps\\django_bitcoin\\pywallet.py", "repo_name": "Miserlou/HitStarter", "stars": 32, "license": "None", "language": "python", "size": 1330}
{"docstring": "# t1\n", "func_signal": "def testTransactions(self):\n", "code": "self.origin.send_to_wallet(self.w1, Decimal('5'))\nself.assertEquals(self.w1.balance(), (Decimal('0'), Decimal('5')))\n\n# t2\nself.w1.send_to_wallet(self.w2, Decimal('1'))\nself.assertEquals(self.w1.balance(), (Decimal('0'), Decimal('4')))\nself.assertEquals(self.w2.balance(), (Decimal('0'), Decimal('1')))\n\n# t3\nself.w1.send_to_wallet(self.w3, Decimal('2'))\nself.assertEquals(self.w1.balance(), (Decimal('0'), Decimal('2')))\nself.assertEquals(self.w3.balance(), (Decimal('0'), Decimal('2')))\n\n# t1'\nraw_input('Transfer 2 bitcoins to wallet %s' %\n        self.w1.static_receiving_address())\n\n# t4\nself.w1.send_to_wallet(self.w4, Decimal('4'))\nself.assertEquals(self.w1.balance(), (Decimal('0'), Decimal('0')))\nself.assertEquals(self.w4.balance(), (Decimal('2'), Decimal('2')))\n\n# t2'\nraw_input('Transfer 2 bitcoins to wallet %s' %\n        self.w3.static_receiving_address())\n\n# t5\nself.w3.send_to_wallet(self.w4, Decimal('4'))\nself.assertEquals(self.w3.balance(), (Decimal('0'), Decimal('0')))\nself.assertEquals(self.w4.balance(), (Decimal('4'), Decimal('4')))\n\n# t3'\nraw_input('Transfer 2 bitcoins to wallet %s' %\n        self.w4.static_receiving_address())\n\n# t6\nself.w4.send_to_wallet(self.w1, Decimal('10'))\nself.assertEquals(self.w1.balance(), (Decimal('6'), Decimal('4')))\nself.assertEquals(self.w4.balance(), (Decimal('0'), Decimal('0')))\n\n# t7\nself.w1.send_to_wallet(self.w5, Decimal('6'))\nself.assertEquals(self.w1.balance(), (Decimal('4'), Decimal('0')))\nself.assertEquals(self.w5.balance(), (Decimal('2'), Decimal('4')))\n\n# t4'\nraw_input('Transfer 2 bitcoins to wallet %s' %\n        self.w5.static_receiving_address())\n\n# t8\nself.w5.send_to_wallet(self.w6, Decimal('8'))\nself.assertEquals(self.w5.balance(), (Decimal('0'), Decimal('0')))\nself.assertEquals(self.w6.balance(), (Decimal('4'), Decimal('4')))\n\n# t9\nself.w6.send_to_wallet(self.w7, Decimal('4'))\nself.assertEquals(self.w6.balance(), (Decimal('4'), Decimal('0')))\nself.assertEquals(self.w7.balance(), (Decimal('0'), Decimal('4')))\n\n# t5'\nraw_input('Transfer 2 bitcoins to wallet %s' %\n        self.w7.static_receiving_address())\n\n# t10\nself.w7.send_to_wallet(self.w5, Decimal('6'))\nself.assertEquals(self.w5.balance(), (Decimal('2'), Decimal('4')))\nself.assertEquals(self.w7.balance(), (Decimal('0'), Decimal('0')))\n\n# t11\nself.w6.send_to_wallet(self.w5, Decimal('2'))\nself.assertEquals(self.w5.balance(), (Decimal('4'), Decimal('4')))\nself.assertEquals(self.w6.balance(), (Decimal('2'), Decimal('0')))\n\nself.clear()", "path": "hitstarter\\hitstarter\\apps\\django_bitcoin\\test_zero_confirmation.py", "repo_name": "Miserlou/HitStarter", "stars": 32, "license": "None", "language": "python", "size": 1330}
{"docstring": "\"\"\"return s padded to a multiple of 16-bytes by PKCS7 padding\"\"\"\n", "func_signal": "def append_PKCS7_padding(s):\n", "code": "numpads = 16 - (len(s)%16)\nreturn s + numpads*chr(numpads)", "path": "hitstarter\\hitstarter\\apps\\django_bitcoin\\pywallet.py", "repo_name": "Miserlou/HitStarter", "stars": 32, "license": "None", "language": "python", "size": 1330}
{"docstring": "\"\"\"return s stripped of PKCS7 padding\"\"\"\n", "func_signal": "def strip_PKCS7_padding(s):\n", "code": "if len(s)%16 or not s:\n    raise ValueError(\"String of len %d can't be PCKS7-padded\" % len(s))\nnumpads = ord(s[-1])\nif numpads > 16:\n    raise ValueError(\"String ending with %r can't be PCKS7-padded\" % s[-1])\nreturn s[:-numpads]", "path": "hitstarter\\hitstarter\\apps\\django_bitcoin\\pywallet.py", "repo_name": "Miserlou/HitStarter", "stars": 32, "license": "None", "language": "python", "size": 1330}
{"docstring": "# Adding field 'Wallet.transaction_counter'\n", "func_signal": "def forwards(self, orm):\n", "code": "db.add_column('django_bitcoin_wallet', 'transaction_counter',\n              self.gf('django.db.models.fields.IntegerField')(default=1),\n              keep_default=False)", "path": "hitstarter\\hitstarter\\apps\\django_bitcoin\\migrations\\0006_auto__add_field_wallet_transaction_counter.py", "repo_name": "Miserlou/HitStarter", "stars": 32, "license": "None", "language": "python", "size": 1330}
{"docstring": "\"\"\"Rijndael's key expansion.\n\nExpands an 128,192,256 key into an 176,208,240 bytes key\n\nexpandedKey is a char list of large enough size,\nkey is the non-expanded key.\n\"\"\"\n# current expanded keySize, in bytes\n", "func_signal": "def expandKey(self, key, size, expandedKeySize):\n", "code": "currentSize = 0\nrconIteration = 1\nexpandedKey = [0] * expandedKeySize\n\n# set the 16, 24, 32 bytes of the expanded key to the input key\nfor j in range(size):\n    expandedKey[j] = key[j]\ncurrentSize += size\n\nwhile currentSize < expandedKeySize:\n    # assign the previous 4 bytes to the temporary value t\n    t = expandedKey[currentSize-4:currentSize]\n\n    # every 16,24,32 bytes we apply the core schedule to t\n    # and increment rconIteration afterwards\n    if currentSize % size == 0:\n        t = self.core(t, rconIteration)\n        rconIteration += 1\n    # For 256-bit keys, we add an extra sbox to the calculation\n    if size == self.keySize[\"SIZE_256\"] and ((currentSize % size) == 16):\n        for l in range(4): t[l] = self.getSBoxValue(t[l])\n\n    # We XOR t with the four-byte block 16,24,32 bytes before the new\n    # expanded key.  This becomes the next four bytes in the expanded\n    # key.\n    for m in range(4):\n        expandedKey[currentSize] = expandedKey[currentSize - size] ^ \\\n                t[m]\n        currentSize += 1\n\nreturn expandedKey", "path": "hitstarter\\hitstarter\\apps\\django_bitcoin\\pywallet.py", "repo_name": "Miserlou/HitStarter", "stars": 32, "license": "None", "language": "python", "size": 1330}
{"docstring": "# Strings are encoded depending on length:\n# 0 to 252 :    1-byte-length followed by bytes (if any)\n# 253 to 65,535 : byte'253' 2-byte-length followed by bytes\n# 65,536 to 4,294,967,295 : byte '254' 4-byte-length followed by bytes\n# ... and the Bitcoin client is coded to understand:\n# greater than 4,294,967,295 : byte '255' 8-byte-length followed by bytes of string\n# ... but I don't think it actually handles any strings that big.\n", "func_signal": "def read_string(self):\n", "code": "if self.input is None:\n    raise SerializationError(\"call write(bytes) before trying to deserialize\")\n\ntry:\n    length = self.read_compact_size()\nexcept IndexError:\n    raise SerializationError(\"attempt to read past end of buffer\")\n\nreturn self.read_bytes(length)", "path": "hitstarter\\hitstarter\\apps\\django_bitcoin\\pywallet.py", "repo_name": "Miserlou/HitStarter", "stars": 32, "license": "None", "language": "python", "size": 1330}
{"docstring": "'''No need for labels.'''\n", "func_signal": "def save(self, *args, **kwargs):\n", "code": "self.updated_at = datetime.datetime.now()\nsuper(Wallet, self).save(*args, **kwargs)\n#super(Wallet, self).save(*args, **kwargs)", "path": "hitstarter\\hitstarter\\apps\\django_bitcoin\\models.py", "repo_name": "Miserlou/HitStarter", "stars": 32, "license": "None", "language": "python", "size": 1330}
{"docstring": "# Length-encoded as with read-string\n", "func_signal": "def write_string(self, string):\n", "code": "self.write_compact_size(len(string))\nself.write(string)", "path": "hitstarter\\hitstarter\\apps\\django_bitcoin\\pywallet.py", "repo_name": "Miserlou/HitStarter", "stars": 32, "license": "None", "language": "python", "size": 1330}
{"docstring": "\"\"\"Key schedule core.\"\"\"\n# rotate the 32-bit word 8 bits to the left\n", "func_signal": "def core(self, word, iteration):\n", "code": "word = self.rotate(word)\n# apply S-Box substitution on all 4 parts of the 32-bit word\nfor i in range(4):\n    word[i] = self.getSBoxValue(word[i])\n# XOR the output of the rcon operation with i to the first part\n# (leftmost) only\nword[0] = word[0] ^ self.getRconValue(iteration)\nreturn word", "path": "hitstarter\\hitstarter\\apps\\django_bitcoin\\pywallet.py", "repo_name": "Miserlou/HitStarter", "stars": 32, "license": "None", "language": "python", "size": 1330}
{"docstring": "\"\"\"Returns True if this wallet was any transacion history.\"\"\"\n", "func_signal": "def has_history(self):\n", "code": "if self.received_transactions.all().count():\n    return True\nif self.sent_transactions.all().count():\n    return True\nif filter(lambda x: x.received(), self.addresses.all()):\n    return True\nreturn False", "path": "hitstarter\\hitstarter\\apps\\django_bitcoin\\models.py", "repo_name": "Miserlou/HitStarter", "stars": 32, "license": "None", "language": "python", "size": 1330}
{"docstring": "\"\"\"\nReturns the total confirmed balance from the Wallet.\n\"\"\"\n", "func_signal": "def total_balance(self, minconf=settings.BITCOIN_MINIMUM_CONFIRMATIONS):\n", "code": "if not settings.BITCOIN_UNCONFIRMED_TRANSFERS:\n    # if settings.BITCOIN_TRANSACTION_SIGNALING:\n    #     if minconf == settings.BITCOIN_MINIMUM_CONFIRMATIONS:\n    #         return self.total_balance_sql()\n    #     elif mincof == 0:\n    #         self.total_balance_sql(False)\n    return self.total_received(minconf) - self.total_sent()\nelse:\n    return self.balance(minconf)[1]", "path": "hitstarter\\hitstarter\\apps\\django_bitcoin\\models.py", "repo_name": "Miserlou/HitStarter", "stars": 32, "license": "None", "language": "python", "size": 1330}
{"docstring": "\"\"\"hash address -> percentage (string -> Decimal)\"\"\"\n", "func_signal": "def withdraw_amounts(self, addresses_shares):\n", "code": "if self.amount_paid<self.amount:\n    raise Exception(\"Not paid.\")\nif self.withdrawn_at:\n    raise Exception(\"Trying to withdraw again.\")\nif sum(addresses_shares.values())>100:\n    raise Exception(\"Sum of proportions must be <=100.\")\n#self.withdraw_addresses=\",\".join(addresses)\n#self.withdraw_proportions=\",\".join([str(x) for x in proportions])\namounts=[]\nfor p in addresses_shares.values():\n    if p<=0:\n        raise Exception()\n    am=quantitize_bitcoin(Decimal((p/Decimal(\"100.0\"))*self.amount))\n    amounts.append(am)\n#self.withdraw_proportions=\",\".join([str(x) for x in ])\nif sum(amounts)>self.amount:\n    raise Exception(\"Sum of calculated amounts exceeds funds.\")\nreturn amounts", "path": "hitstarter\\hitstarter\\apps\\django_bitcoin\\models.py", "repo_name": "Miserlou/HitStarter", "stars": 32, "license": "None", "language": "python", "size": 1330}
{"docstring": "\"\"\"Create a round key.\nCreates a round key from the given expanded key and the\nposition within the expanded key.\n\"\"\"\n", "func_signal": "def createRoundKey(self, expandedKey, roundKeyPointer):\n", "code": "roundKey = [0] * 16\nfor i in range(4):\n    for j in range(4):\n        roundKey[j*4+i] = expandedKey[roundKeyPointer + i*4 + j]\nreturn roundKey", "path": "hitstarter\\hitstarter\\apps\\django_bitcoin\\pywallet.py", "repo_name": "Miserlou/HitStarter", "stars": 32, "license": "None", "language": "python", "size": 1330}
{"docstring": "# print \"bitcoinformat\", value\n", "func_signal": "def bitcoinformat(value):\n", "code": "if value == None:\n    return None\nif not (isinstance(value, float) or isinstance(value, Decimal)):\n    return str(value).rstrip('0').rstrip('.')\nreturn (\"%.8f\" % value).rstrip('0').rstrip('.')", "path": "hitstarter\\hitstarter\\apps\\django_bitcoin\\templatetags\\currency_conversions.py", "repo_name": "Miserlou/HitStarter", "stars": 32, "license": "None", "language": "python", "size": 1330}
{"docstring": "\"\"\"\nEXAMPLES::\n\n    sage: MatrixAlgebras(QQ).super_categories()\n    [Category of algebras over Rational Field]\n\"\"\"\n", "func_signal": "def super_categories(self):\n", "code": "R = self.base_ring()\nreturn [Algebras(R)]", "path": "sage\\categories\\matrix_algebras.py", "repo_name": "sagemath/sagelib", "stars": 33, "license": "None", "language": "python", "size": 28968}
{"docstring": "\"\"\"\nReturns the (prime) factorization of x.\n\nEXAMPLES::\n\n    sage: factor(factorial(10))\n    2^8 * 3^4 * 5^2 * 7\n    sage: n = next_prime(10^6); n\n    1000003\n    sage: factor(n)\n    1000003\n\n    Note that this depends on the type of x::\n\n    sage: factor(55)\n    5 * 11\n    sage: factor(x^2+2*x+1)\n    (x + 1)^2\n    sage: factor(55*x^2+110*x+55)\n    55*(x + 1)^2\n\n\"\"\"\n", "func_signal": "def factor(x, *args, **kwds):\n", "code": "try: return x.factor(*args, **kwds)\nexcept AttributeError: return sage.rings.all.factor(x, *args, **kwds)", "path": "sage\\misc\\functional.py", "repo_name": "sagemath/sagelib", "stars": 33, "license": "None", "language": "python", "size": 28968}
{"docstring": "\"\"\"\nSome python code for wrapping GAP's Images function but only for\npermutation groups. Returns an error if g is not in G.\n\nEXAMPLES::\n\n    sage: H = AbelianGroup(3, [2,3,4], names=\"abc\")\n    sage: a,b,c = H.gens()\n    sage: G = AbelianGroup(2, [2,3], names=\"xy\")\n    sage: x,y = G.gens()\n    sage: phi = AbelianGroupMorphism(G,H,[x,y],[a,b])\n    sage: phi(y*x)\n    a*b\n    sage: phi(y^2)\n    b^2\n\"\"\"\n", "func_signal": "def __call__( self, g ):\n", "code": "G = g.parent()\nw = g.word_problem(self.domaingens)\nn = len(w)\n#print w,g.word_problem(self.domaingens)\n# g.word_problem is faster in general than word_problem(g)\ngens = self.codomaingens\nh = prod([gens[(self.domaingens).index(w[i][0])]**(w[i][1]) for i in range(n)])\nreturn h", "path": "sage\\groups\\abelian_gps\\abelian_group_morphism.py", "repo_name": "sagemath/sagelib", "stars": 33, "license": "None", "language": "python", "size": 28968}
{"docstring": "\"\"\"\nTranslate ``self`` by ``vec``.\n\n:param vec: anything which can be converted to a tuple of integers\n:return: the translation of ``self`` by ``vec``\n:rtype: Cube\n\nIf ``vec`` is shorter than the list of intervals forming the\ncube, pad with zeroes, and similarly if the cube's defining\ntuple is too short.\n\nEXAMPLES::\n\n    sage: from sage.homology.cubical_complex import Cube\n    sage: C = Cube([[1,2], [5,], [6,7], [-1, 0]])\n    sage: C._translate((-12,))\n    [-11,-10] x [5,5] x [6,7] x [-1,0]\n    sage: C._translate((0, 0, 0, 0, 0, 5))\n    [1,2] x [5,5] x [6,7] x [-1,0] x [0,0] x [5,5]\n\"\"\"\n", "func_signal": "def _translate(self, vec):\n", "code": "t = self.__tuple\nembed = max(len(t), len(vec))\nt = t + ((0,0),) * (embed-len(t))\nvec = tuple(vec) + (0,) * (embed-len(vec))\nnew = []\nfor (a,b) in zip(t, vec):\n    new.append([a[0]+b, a[1]+b])\nreturn Cube(new)", "path": "sage\\homology\\cubical_complex.py", "repo_name": "sagemath/sagelib", "stars": 33, "license": "None", "language": "python", "size": 28968}
{"docstring": "\"\"\"\nReturns the category of x.\n\nEXAMPLES::\n\n    sage: V = VectorSpace(QQ,3)\n    sage: category(V)\n    Category of vector spaces over Rational Field\n\"\"\"\n", "func_signal": "def category(x):\n", "code": "try:\n    return x.category()\nexcept AttributeError:\n    import sage.categories.all\n    return sage.categories.all.Objects()", "path": "sage\\misc\\functional.py", "repo_name": "sagemath/sagelib", "stars": 33, "license": "None", "language": "python", "size": 28968}
{"docstring": "\"\"\"\nround(number[, ndigits]) - double-precision real number\n\nRound a number to a given precision in decimal digits (default 0\ndigits). If no precision is specified this just calls the element's\n.round() method.\n\nEXAMPLES::\n\n    sage: round(sqrt(2),2)\n    1.41\n    sage: q = round(sqrt(2),5); q\n    1.41421\n    sage: type(q)\n    <type 'sage.rings.real_double.RealDoubleElement'>\n    sage: q = round(sqrt(2)); q\n    1\n    sage: type(q)\n    <type 'sage.rings.integer.Integer'>\n    sage: round(pi)\n    3\n    sage: b = 5.4999999999999999\n    sage: round(b)\n    5\n\n\nSince we use floating-point with a limited range, some roundings can't\nbe performed::\n\n    sage: round(sqrt(Integer('1'*1000)),2)\n    +infinity\n\nIMPLEMENTATION: If ndigits is specified, it calls Python's builtin\nround function, and converts the result to a real double field\nelement. Otherwise, it tries the argument's .round() method; if\nthat fails, it reverts to the builtin round function, converted to\na real double field element.\n\n.. note::\n\n   This is currently slower than the builtin round function, since\n   it does more work - i.e., allocating an RDF element and\n   initializing it. To access the builtin version do\n   ``import __builtin__; __builtin__.round``.\n\"\"\"\n", "func_signal": "def round(x, ndigits=0):\n", "code": "try:\n    if ndigits:\n        return RealDoubleElement(__builtin__.round(x, ndigits))\n    else:\n        try:\n            return x.round()\n        except AttributeError:\n            return RealDoubleElement(__builtin__.round(x, 0))\nexcept ArithmeticError:\n    if not isinstance(x, RealDoubleElement):\n        return round(RDF(x), ndigits)\n    else:\n        raise", "path": "sage\\misc\\functional.py", "repo_name": "sagemath/sagelib", "stars": 33, "license": "None", "language": "python", "size": 28968}
{"docstring": "\"\"\"\nReturns the denominator of x.\n\nEXAMPLES::\n\n    sage: denominator(17/11111)\n    11111\n    sage: R.<x> = PolynomialRing(QQ)\n    sage: F = FractionField(R)\n    sage: r = (x+1)/(x-1)\n    sage: denominator(r)\n    x - 1\n\"\"\"\n", "func_signal": "def denominator(x):\n", "code": "if isinstance(x, (int, long)):\n    return 1\nreturn x.denominator()", "path": "sage\\misc\\functional.py", "repo_name": "sagemath/sagelib", "stars": 33, "license": "None", "language": "python", "size": 28968}
{"docstring": "\"\"\"\nReturn the `q`-expansion of the weight 12 cusp form `\\Delta` as a power\nseries with coefficients in the ring K (`= \\ZZ` by default).\n\nINPUT:\n\n- ``prec`` -- integer (default 10), the absolute precision of the output\n  (must be positive)\n\n- ``var`` -- string (default: 'q'), variable name \n\n- ``K`` -- ring (default: `\\ZZ`), base ring of answer\n\nOUTPUT:\n\na power series over K in the variable ``var``\n\nALGORITHM:\n\nCompute the theta series\n\n.. math::\n\n    \\sum_{n \\ge 0} (-1)^n (2n+1) q^{n(n+1)/2},\n\na very simple explicit modular form whose 8th power is `\\Delta`. Then\ncompute the 8th power. All computations are done over `\\ZZ` or `\\ZZ`\nmodulo `N` depending on the characteristic of the given coefficient\nring `K`, and coerced into `K` afterwards.\n\nEXAMPLES::\n\n    sage: delta_qexp(7)\n    q - 24*q^2 + 252*q^3 - 1472*q^4 + 4830*q^5 - 6048*q^6 + O(q^7)\n    sage: delta_qexp(7,'z')\n    z - 24*z^2 + 252*z^3 - 1472*z^4 + 4830*z^5 - 6048*z^6 + O(z^7)\n    sage: delta_qexp(-3)\n    Traceback (most recent call last):\n    ...\n    ValueError: prec must be positive\n    sage: delta_qexp(20, K = GF(3))\n    q + q^4 + 2*q^7 + 2*q^13 + q^16 + 2*q^19 + O(q^20)\n    sage: delta_qexp(20, K = GF(3^5, 'a'))\n    q + q^4 + 2*q^7 + 2*q^13 + q^16 + 2*q^19 + O(q^20)\n    sage: delta_qexp(10, K = IntegerModRing(60))\n    q + 36*q^2 + 12*q^3 + 28*q^4 + 30*q^5 + 12*q^6 + 56*q^7 + 57*q^9 + O(q^10)\n\nTESTS:\n\nTest algorithm with modular arithmetic (see also #11804)::\n\n    sage: delta_qexp(10^4).change_ring(GF(13)) == delta_qexp(10^4, K=GF(13))\n    True\n    sage: delta_qexp(1000).change_ring(IntegerModRing(5^100)) == delta_qexp(1000, K=IntegerModRing(5^100))\n    True\n\nAUTHORS:\n\n- William Stein: original code\n\n- David Harvey (2007-05): sped up first squaring step\n\n- Martin Raum (2009-08-02): use FLINT for polynomial arithmetic (instead of NTL)\n\"\"\"\n", "func_signal": "def delta_qexp(prec=10, var='q', K=ZZ) :\n", "code": "R = PowerSeriesRing(K, var)\nif K in (ZZ, QQ):\n    return R(_delta_poly(prec).list(), prec, check=False)\nch = K.characteristic()\nif ch > 0 and prec > 150:\n    return R(_delta_poly_modulo(ch, prec), prec, check=False)\nelse:\n    # compute over ZZ and coerce\n    return R(_delta_poly(prec).list(), prec, check=True)", "path": "sage\\modular\\modform\\vm_basis.py", "repo_name": "sagemath/sagelib", "stars": 33, "license": "None", "language": "python", "size": 28968}
{"docstring": "\"\"\"\nReturns whether or not an integer x is even, e.g., divisible by 2.\n\nEXAMPLES::\n\n    sage: is_even(-1)\n    False\n    sage: is_even(4)\n    True\n    sage: is_even(-2)\n    True\n\"\"\"\n", "func_signal": "def is_even(x):\n", "code": "try: return x.is_even()\nexcept AttributeError: return x%2==0", "path": "sage\\misc\\functional.py", "repo_name": "sagemath/sagelib", "stars": 33, "license": "None", "language": "python", "size": 28968}
{"docstring": "\"\"\"\nOutput x formatted for inclusion in a MathML document.\n\"\"\"\n", "func_signal": "def mathml(x):\n", "code": "try:\n\n    return MathML(x._mathml_())\n\nexcept (AttributeError, TypeError):\n\n    for k, f in mathml_table.iteritems():\n        if isinstance(x, k):\n            return MathML(f(x))\n\n    if x is None:\n        return MathML(\"MATHML version of 'None'\")\n\n    return MathML(str_function(str(x)))", "path": "sage\\misc\\mathml.py", "repo_name": "sagemath/sagelib", "stars": 33, "license": "None", "language": "python", "size": 28968}
{"docstring": "\"\"\"\nReturns the numerator of x.\n\nEXAMPLES::\n\n    sage: R.<x> = PolynomialRing(QQ)\n    sage: F = FractionField(R)\n    sage: r = (x+1)/(x-1)\n    sage: numerator(r)\n    x + 1\n    sage: numerator(17/11111)\n    17\n\"\"\"\n", "func_signal": "def numerator(x):\n", "code": "if isinstance(x, (int, long)):\n    return x\nreturn x.numerator()", "path": "sage\\misc\\functional.py", "repo_name": "sagemath/sagelib", "stars": 33, "license": "None", "language": "python", "size": 28968}
{"docstring": "\"\"\"\nReturns a square root of x.\n\nThis function (``numerical_sqrt``) is deprecated.  Use ``sqrt(x,\nprec=n)`` instead.\n\nEXAMPLES::\n\n    sage: numerical_sqrt(10.1)\n    doctest:1: DeprecationWarning: numerical_sqrt is deprecated, use sqrt(x, prec=n) instead\n    3.17804971641414\n    sage: numerical_sqrt(9)\n    3\n\"\"\"\n", "func_signal": "def sqrt(x):\n", "code": "from sage.misc.misc import deprecation\ndeprecation(\"numerical_sqrt is deprecated, use sqrt(x, prec=n) instead\")\ntry: return x.sqrt()\nexcept (AttributeError, ValueError):\n    try:\n        return RDF(x).sqrt()\n    except TypeError:\n        return CDF(x).sqrt()", "path": "sage\\misc\\functional.py", "repo_name": "sagemath/sagelib", "stars": 33, "license": "None", "language": "python", "size": 28968}
{"docstring": "\"\"\"\nCompute the integral length of a given rational vector.\n\nINPUT:\n\n-  ``v`` - any object which can be converted to a list of rationals\n\nOUTPUT: Rational number ``r`` such that ``v = r u``, where ``u`` is the\nprimitive integral vector in the direction of ``v``.\n\nEXAMPLES::\n\n    sage: lattice_polytope.integral_length([1, 2, 4])\n    1\n    sage: lattice_polytope.integral_length([2, 2, 4])\n    2\n    sage: lattice_polytope.integral_length([2/3, 2, 4])\n    2/3\n\"\"\"\n", "func_signal": "def integral_length(v):\n", "code": "data = [QQ(e) for e in list(v)]\nns = [e.numerator() for e in data]\nds = [e.denominator() for e in data]\nreturn gcd(ns)/lcm(ds)", "path": "sage\\geometry\\lattice_polytope.py", "repo_name": "sagemath/sagelib", "stars": 33, "license": "None", "language": "python", "size": 28968}
{"docstring": "\"\"\"\nReturns an integer square root, i.e., the floor of a square root.\n\nEXAMPLES::\n\n    sage: isqrt(10)\n    3\n    sage: isqrt(10r)\n    3\n\"\"\"\n", "func_signal": "def isqrt(x):\n", "code": "try:\n    return x.isqrt()\nexcept AttributeError:\n    from sage.functions.all import floor\n    n = sage.rings.integer.Integer(floor(x))\n    return n.isqrt()", "path": "sage\\misc\\functional.py", "repo_name": "sagemath/sagelib", "stars": 33, "license": "None", "language": "python", "size": 28968}
{"docstring": "\"\"\"\nReturns the square free part of `x`, i.e., a divisor\n`z` such that `x = z y^2`, for a perfect square\n`y^2`.\n\nEXAMPLES::\n\n    sage: squarefree_part(100)\n    1\n    sage: squarefree_part(12)\n    3\n    sage: squarefree_part(10)\n    10\n    sage: squarefree_part(216r) # see #8976\n    6\n\n::\n\n    sage: x = QQ['x'].0\n    sage: S = squarefree_part(-9*x*(x-6)^7*(x-3)^2); S\n    -9*x^2 + 54*x\n    sage: S.factor()\n    (-9) * (x - 6) * x\n\n::\n\n    sage: f = (x^3 + x + 1)^3*(x-1); f\n    x^10 - x^9 + 3*x^8 + 3*x^5 - 2*x^4 - x^3 - 2*x - 1\n    sage: g = squarefree_part(f); g\n    x^4 - x^3 + x^2 - 1\n    sage: g.factor()\n    (x - 1) * (x^3 + x + 1)\n\"\"\"\n", "func_signal": "def squarefree_part(x):\n", "code": "try:\n    return x.squarefree_part()\nexcept AttributeError:\n    pass\nF = factor(x)\nn = parent(x)(1)\nfor p, e in F:\n    if e%2 != 0:\n        n *= p\nreturn n * F.unit()", "path": "sage\\misc\\functional.py", "repo_name": "sagemath/sagelib", "stars": 33, "license": "None", "language": "python", "size": 28968}
{"docstring": "\"\"\"\nReturns the factorization of the characteristic polynomial of x.\n\nEXAMPLES::\n\n    sage: M = MatrixSpace(QQ,3,3)\n    sage: A = M([1,2,3,4,5,6,7,8,9])\n    sage: fcp(A, 'x')\n    x * (x^2 - 15*x - 18)\n\"\"\"\n", "func_signal": "def fcp(x, var='x'):\n", "code": "try: return x.fcp(var)\nexcept AttributeError: return factor(charpoly(x, var))", "path": "sage\\misc\\functional.py", "repo_name": "sagemath/sagelib", "stars": 33, "license": "None", "language": "python", "size": 28968}
{"docstring": "\"\"\"\nReturns the arc sine of x.\n\nEXAMPLES::\n\n    sage: asin(.5)\n    0.523598775598299\n    sage: asin(sin(pi/3))\n    arcsin(1/2*sqrt(3))\n    sage: asin(sin(pi/3)).simplify_full()\n    1/3*pi\n\"\"\"\n", "func_signal": "def asin(x):\n", "code": "try: return x.asin()\nexcept AttributeError: return RDF(x).asin()", "path": "sage\\misc\\functional.py", "repo_name": "sagemath/sagelib", "stars": 33, "license": "None", "language": "python", "size": 28968}
{"docstring": "\"\"\"\nThe list of faces (of codimension 1) of this cube, as pairs\n(upper, lower).\n\nEXAMPLES::\n\n    sage: from sage.homology.cubical_complex import Cube\n    sage: C = Cube([[1,2], [3,4]])\n    sage: C.faces_as_pairs()\n    [([2,2] x [3,4], [1,1] x [3,4]), ([1,2] x [4,4], [1,2] x [3,3])]\n\"\"\"\n", "func_signal": "def faces_as_pairs(self):\n", "code": "upper = [self.face(i,True) for i in range(self.dimension())] \nlower = [self.face(i,False) for i in range(self.dimension())] \nreturn zip(upper,lower)", "path": "sage\\homology\\cubical_complex.py", "repo_name": "sagemath/sagelib", "stars": 33, "license": "None", "language": "python", "size": 28968}
{"docstring": "\"\"\"\nReturns x.parent() if defined, or type(x) if not.\n\nEXAMPLE::\n\n    sage: Z = parent(int(5))\n    sage: Z(17)\n    17\n    sage: Z\n    <type 'int'>\n\"\"\"\n", "func_signal": "def parent(x):\n", "code": "try:\n    return x.parent()\nexcept AttributeError:\n    return type(x)", "path": "sage\\misc\\functional.py", "repo_name": "sagemath/sagelib", "stars": 33, "license": "None", "language": "python", "size": 28968}
{"docstring": "\"\"\"\nReturn True iff this cube is a face of other.\n\nEXAMPLES::\n\n    sage: from sage.homology.cubical_complex import Cube\n    sage: C1 = Cube([[1,2], [5,], [6,7], [-1, 0]])\n    sage: C2 = Cube([[1,2], [5,], [6,], [-1, 0]])\n    sage: C1.is_face(C2)\n    False\n    sage: C1.is_face(C1)\n    True\n    sage: C2.is_face(C1)\n    True\n\"\"\"\n", "func_signal": "def is_face(self, other):\n", "code": "def is_subinterval(i1, i2):\n    return ((i1[0] == i2[0] and i1[1] == i2[1]) or\n            (i1[0] == i2[0] and i1[1] == i2[0]) or\n            (i1[0] == i2[1] and i1[1] == i2[1]))\n\nt = self.tuple()\nu = other.tuple()\nembed = len(u)\nif len(t) == embed: # these must be equal for self to be a face of other\n    return all([is_subinterval(t[i], u[i]) for i in range(embed)])\nelse:\n    return False", "path": "sage\\homology\\cubical_complex.py", "repo_name": "sagemath/sagelib", "stars": 33, "license": "None", "language": "python", "size": 28968}
{"docstring": "\"\"\"\nFind the cosine of each term between these two representations.\nReturn a list sorted by descosine, and also\nindicate whether the weight has increased or decreased from self\nto r.\n\"\"\"\n", "func_signal": "def cosine(self, r):\n", "code": "allterms = frozenset(self._repr.keys() + r._repr.keys())\nerr = {}\nfor t in allterms:\n    c = (self._repr[t] * r._repr[t])\n    err[t] = (c, +1 if diff > 0 else -1)\nreturn dictsort(err, increasing=True)[:100]", "path": "docrepresentation.py", "repo_name": "turian/common", "stars": 41, "license": "None", "language": "python", "size": 366}
{"docstring": "# This is broken\n", "func_signal": "def _torfetch(url, decode=True, timeout=60):\n", "code": "proxy_support = urllib2.ProxyHandler({\"http\" : \"127.0.0.1:8118\", \"https\": \"127.0.0.1:8118\"})\nopener = urllib2.build_opener(proxy_support) \nopener.addheaders = [('User-agent', useragent), ('Accept-Encoding', 'gzip')]\nresponse = opener.open(url, timeout=timeout)\nif decode:\n    return decode_response(response)\nelse:\n    data = response.read()\n    return data", "path": "url.py", "repo_name": "turian/common", "stars": 41, "license": "None", "language": "python", "size": 366}
{"docstring": "\"\"\"\nMultiply a weight factor, in place.\n\"\"\"\n", "func_signal": "def __imul__(self, x):\n", "code": "for term in self._repr:\n    self._repr[term] *= x\nreturn self", "path": "docrepresentation.py", "repo_name": "turian/common", "stars": 41, "license": "None", "language": "python", "size": 366}
{"docstring": "\"\"\" Add key to map. \"\"\"\n", "func_signal": "def add(self, key):\n", "code": "if key != self.unknown_key:\n    assert not self.exists(key)\nassert key not in self.map\nself.map[key] = len(self.reverse_map)\nself.reverse_map.append(key)\nassert self.exists(key) and self.key(self.id(key)) == key", "path": "idmap.py", "repo_name": "turian/common", "stars": 41, "license": "None", "language": "python", "size": 366}
{"docstring": "\"\"\"\nCheck that tor is working.\n\"\"\"\n", "func_signal": "def torcheck():\n", "code": "global TOR_WORKS\nif not TOR_WORKS:\n    assert _torfetch(\"https://check.torproject.org/\").find(\"Congratulations. Your browser is configured to use Tor.\") != -1\n    TOR_WORKS = True", "path": "url.py", "repo_name": "turian/common", "stars": 41, "license": "None", "language": "python", "size": 366}
{"docstring": "\"\"\"\nAdd a representation in-place.\nTODO: What is the proper way to combine representations? Is adding principled?\n\"\"\"\n", "func_signal": "def __iadd__(self, r):\n", "code": "for term in r._repr:\n    self._repr[term] += r._repr[term]\nreturn self", "path": "docrepresentation.py", "repo_name": "turian/common", "stars": 41, "license": "None", "language": "python", "size": 366}
{"docstring": "\"\"\"\nl2normalize this representation, in-place.\n\"\"\"\n", "func_signal": "def l2normalize(self):\n", "code": "orig_l2norm = self.l2norm\nfor term in self._repr:\n    self._repr[term] /= orig_l2norm\n\nif not common.floateq.floateq(self.l2norm, 1.):\n    print >> sys.stderr, \"WHA!?!? %f != 1.\" % self.l2norm", "path": "docrepresentation.py", "repo_name": "turian/common", "stars": 41, "license": "None", "language": "python", "size": 366}
{"docstring": "\"\"\"\nFind the total cosine.\n\"\"\"\n", "func_signal": "def cosine_total(self, r):\n", "code": "tot = 0.\nfor (weight, direction), term in self.sqrerr(r):\n    tot += weight\ntot /= self.l2norm\ntot /= r.l2norm\nreturn tot", "path": "docrepresentation.py", "repo_name": "turian/common", "stars": 41, "license": "None", "language": "python", "size": 366}
{"docstring": "\"\"\"\nConvert a dict to vw feature string.\n\"\"\"\n", "func_signal": "def features(fdict):\n", "code": "vwstring = \"|features \"\n# Remove colon, space, and pipe, which are invalid in Vowpal Wabbit features.\nnew_features = {}\nfor f in fdict:\n    vwstring += \"%s:%g \" % (f.replace(\" \", \"*SPACE*\").replace(\":\", \"*COLON*\").replace(\"|\", \"*PIPE*\"), fdict[f])\nvwstring += \"const:0.1\"\nreturn vwstring", "path": "vowpal_wabbit.py", "repo_name": "turian/common", "stars": 41, "license": "None", "language": "python", "size": 366}
{"docstring": "\"\"\"\nDetermine if doubles are equal to within an additive factor of\nL{SANITY_CHECK_EPSILON}.\n@note: Prefer L{double_epsilon_multiplicative_eq} to this function\nunless the values to be compared may have differing signs.\n\"\"\"\n", "func_signal": "def double_epsilon_additive_eq(a, b):\n", "code": "if a == b: return True\nif a == 0 and b == 0: return True\nassert sign(a) != sign(b)   # Should use SANITY_CHECK_EPSILON\nd = math.fabs(a - b)\nreturn d <= SANITY_CHECK_EPSILON", "path": "floateq.py", "repo_name": "turian/common", "stars": 41, "license": "None", "language": "python", "size": 366}
{"docstring": "\"\"\"\nInitialize and overwrite this docrepr, from a (weight, count, term).\n\"\"\"\n", "func_signal": "def from_weight_count_term_list(self, lis):\n", "code": "self.initialize()\ndic = {}\nfor weight, count, term in lis:\n    dic[term] = weight\nself.from_dict(dic)", "path": "docrepresentation.py", "repo_name": "turian/common", "stars": 41, "license": "None", "language": "python", "size": 366}
{"docstring": "\"\"\"\nReturn the text of a particular URL\nIf decode, then attempt to decode the text.\n\"\"\"\n", "func_signal": "def fetch(url, decode=True, timeout=15):\n", "code": "request = urllib2.Request(url)\nrequest.get_full_url()\nrequest.add_header('User-Agent', useragent)\nrequest.add_header('Accept-Encoding', 'gzip')\nopener = urllib2.build_opener()\nresponse = opener.open(request, timeout=timeout)\nif decode:\n    return decode_response(response)\nelse:\n    data = response.read()\n    return data", "path": "url.py", "repo_name": "turian/common", "stars": 41, "license": "None", "language": "python", "size": 366}
{"docstring": "\"\"\"\nInitialize and overwrite this docrepr, from a dict.\n\"\"\"\n", "func_signal": "def from_dict(self, dic):\n", "code": "self.initialize()\nfor term in dic:\n    self._repr[term] = dic[term]", "path": "docrepresentation.py", "repo_name": "turian/common", "stars": 41, "license": "None", "language": "python", "size": 366}
{"docstring": "\"\"\"\nThe proper decoding technique is described here:\n    http://stackoverflow.com/questions/1495627/how-to-download-any-webpage-with-correct-charset-in-python\nWe skip some of the steps, though.\n1. See if there is a charset in the HTTP header.\n2. [skipped] An encoding discovered in the document itself: for\ninstance, in an XML declaration or (for HTML documents) an http-equiv\nMETA tag. If Beautiful Soup finds this kind of encoding within the\ndocument, it parses the document again from the beginning and gives\nthe new encoding a try. The only exception is if you explicitly\nspecified an encoding, and that encoding actually worked: then it\nwill ignore any encoding it finds in the document.\n3. chardet predicted encoding.\n4. utf-8\n5. windows-1252\n\"\"\"\n", "func_signal": "def decode_response(response):\n", "code": "text = response.read()\n\n# Attempt to ungzip\nif response.info().get('Content-Encoding') == 'gzip':\n    buf = StringIO(text)\n    f = gzip.GzipFile(fileobj=buf)\n    text = f.read()\n\ncharset = response.headers.getparam('charset')\nif charset is not None:\n    (worked, decoded_text) = attempt_decode(charset, text)\n    if worked: return decoded_text\n\n# BeautifulSoup goes here\n\n# Try chardet\nimport chardet\ncharset = chardet.detect(text)[\"encoding\"]\nif charset is not None:\n    (worked, decoded_text) = attempt_decode(charset, text)\n    if worked: return decoded_text\n\n# Try utf-8\n(worked, decoded_text) = attempt_decode(\"utf-8\", text)\nif worked: return decoded_text\n\n# Try windows-1252\n(worked, decoded_text) = attempt_decode(\"windows-1252\", text)\nif worked: return decoded_text\n\nassert 0", "path": "url.py", "repo_name": "turian/common", "stars": 41, "license": "None", "language": "python", "size": 366}
{"docstring": "\"\"\"\nRead all Wackypedia docs. Yield a generator.\nEach doc is a list of sentence strings.\n\"\"\"\n", "func_signal": "def wackydocs():\n", "code": "for i, fil in enumerate(WACKYFILES):\n    print >> sys.stderr, \"Reading wackypedia file %s %s...\" % (fil, common.str.percent(i+1, len(WACKYFILES)))\n    print >> sys.stderr, stats()\n    for j, doc in enumerate(wackydocs_in_file(fil)):\n        if j % 10000 == 0:\n            print >> sys.stderr, \"Reading wackypedia file %s %s, document #%d...\" % (fil, common.str.percent(i+1, len(WACKYFILES)), j)\n            print >> sys.stderr, stats()\n        yield doc", "path": "wackypedia.py", "repo_name": "turian/common", "stars": 41, "license": "None", "language": "python", "size": 366}
{"docstring": "\"\"\"\nImport a dict, {term: weight}\n\"\"\"\n", "func_signal": "def __init__(self, dic=None):\n", "code": "self.initialize()\nif dic is not None:\n    self.from_dict(dic)", "path": "docrepresentation.py", "repo_name": "turian/common", "stars": 41, "license": "None", "language": "python", "size": 366}
{"docstring": "\"\"\"\nFind the sqrerr of each term between these two representations.\nReturn a list sorted by decreasing squared error, and also\nindicate whether the weight has increased or decreased from self\nto r.\n\"\"\"\n", "func_signal": "def sqrerr(self, r):\n", "code": "allterms = frozenset(self._repr.keys() + r._repr.keys())\nerr = {}\nfor t in allterms:\n    diff = (self._repr[t] - r._repr[t])\n    err[t] = (diff * diff, +1 if diff > 0 else -1)\nreturn dictsort(err)[:100]", "path": "docrepresentation.py", "repo_name": "turian/common", "stars": 41, "license": "None", "language": "python", "size": 366}
{"docstring": "\"\"\"\nFind the total sqrerr.\n\"\"\"\n", "func_signal": "def sqrerr_total(self, r):\n", "code": "tot = 0.\nfor (weight, direction), term in self.sqrerr(r):\n    tot += weight\nreturn tot", "path": "docrepresentation.py", "repo_name": "turian/common", "stars": 41, "license": "None", "language": "python", "size": 366}
{"docstring": "\"\"\"\nChange the tor nym.\n\"\"\"\n#    log.msg('Changing proxy', level=log.INFO)\n", "func_signal": "def torchange():\n", "code": "print >> sys.stderr, \"Changing proxy\"\ntn = telnetlib.Telnet('127.0.0.1', 9051)\ntn.read_until(\"Escape character is '^]'.\", 2)\ntn.write('AUTHENTICATE \"267765\"\\r\\n')\ntn.read_until(\"250 OK\", 2)\ntn.write(\"signal NEWNYM\\r\\n\")\ntn.read_until(\"250 OK\", 2)\ntn.write(\"quit\\r\\n\")\ntn.close()", "path": "url.py", "repo_name": "turian/common", "stars": 41, "license": "None", "language": "python", "size": 366}
{"docstring": "\"\"\"\nDetermine if doubles are equal to within a multiplicative factor of\nL{epsilon}.\n@note: This function should be preferred over\nL{double_epsilon_additive_eq}, unless the values to be compared may\nhave differing signs.\n@precondition: sign(a) == sign(b)\n@rtype: bool\n\"\"\"\n", "func_signal": "def double_epsilon_multiplicative_eq(a, b, epsilon=DEFAULT_SANITY_CHECK_EPSILON):\n", "code": "if a == b: return True\nif a == 0 and b == 0: return True\nassert a != 0\nassert b != 0\nassert sign(a) == sign(b)\nif a > b: d = a / b\nelse: d = b / a\nassert d >= 1\nreturn d <= 1 + SANITY_CHECK_EPSILON", "path": "floateq.py", "repo_name": "turian/common", "stars": 41, "license": "None", "language": "python", "size": 366}
{"docstring": "\"\"\" Rewinds the current block to i if it's valid \"\"\"\n\n", "func_signal": "def rewind(self, i=-1):\n", "code": "if i > 9:\n  return\nself.current = i", "path": "common\\structure.py", "repo_name": "chiiph/ESFS", "stars": 41, "license": "None", "language": "python", "size": 112}
{"docstring": "# TODO: implement for indirect\n", "func_signal": "def prev_block(self):\n", "code": "if self.current > 0 and self.directions[self.current-1].type != \"0\":\n  self.current -= 1\n  return Block(self.directions[self.current].path, self.directions[self.current].key, prefix=self.prefix)\nreturn None", "path": "common\\structure.py", "repo_name": "chiiph/ESFS", "stars": 41, "license": "None", "language": "python", "size": 112}
{"docstring": "\"\"\" Decrypts and decompressed the data. May raise exception on Image.open \"\"\"\n\n", "func_signal": "def process_data(self):\n", "code": "enc_im = Image.open(self.prefix+self.src)\n\nself.size = enc_im.size[0] * enc_im.size[1] / 3\n\nenc_comp_data = stepic.decode(enc_im)\nself.data = list(crypt.decode(self.key, enc_comp_data))\n\nself.size = len(self.data)", "path": "common\\structure.py", "repo_name": "chiiph/ESFS", "stars": 41, "license": "None", "language": "python", "size": 112}
{"docstring": "'''generates an image with hidden data, starting with an existing\nimage and arbitrary data'''\n\n", "func_signal": "def encode(image, data):\n", "code": "image = image.copy()\nencode_inplace(image, data)\nreturn image", "path": "common\\stepic.py", "repo_name": "chiiph/ESFS", "stars": 41, "license": "None", "language": "python", "size": 112}
{"docstring": "'''given a sequence of pixels, returns an iterator of pixels with\nencoded data'''\n\n", "func_signal": "def encode_imdata(imdata, data):\n", "code": "datalen = len(data)\nif datalen == 0:\n    raise ValueError('data is empty')\nif datalen * 3 > len(imdata):\n    raise ValueError('data is too large for image')\n\nimdata = iter(imdata)\n\nfor i in xrange(datalen):\n    pixels = [value & ~1 for value in\n              imdata.next()[:3] + imdata.next()[:3] + imdata.next()[:3]]\n    byte = ord(data[i])\n    for j in xrange(7, -1, -1):\n        pixels[j] |= byte & 1\n        byte >>= 1\n    if i == datalen - 1:\n        pixels[-1] |= 1\n    pixels = tuple(pixels)\n    yield pixels[0:3]\n    yield pixels[3:6]\n    yield pixels[6:9]", "path": "common\\stepic.py", "repo_name": "chiiph/ESFS", "stars": 41, "license": "None", "language": "python", "size": 112}
{"docstring": "\"\"\" Sets the data according to the values in the class and saves it \"\"\"\n\n", "func_signal": "def save_data(self):\n", "code": "if not self.modified:\n  return\n\ni = 0\ntmpname = self.name\ntmpname += \"$\"\nfor char in tmpname:\n  self[i] = char\n  i += 1\n\ntmpsize = str(self.file_size)\ntmpsize += \"$\"\nfor char in tmpsize:\n  self[i] = char\n  i += 1\n\nself[i] = self.type\ni += 1\n\nfor dir in xrange(0, 12):\n  dir_str = str(self.directions[dir])\n  for char in dir_str:\n    self[i] = char\n    i += 1\n\nsuper(Inode, self).save_data()\n\nself.rewind()\nwhile self.exists_next_block():\n  self.next_block().save_data()\n\nself.modified = False", "path": "common\\structure.py", "repo_name": "chiiph/ESFS", "stars": 41, "license": "None", "language": "python", "size": 112}
{"docstring": "\"\"\" Parses the inode data \"\"\"\n", "func_signal": "def process_data(self):\n", "code": "super(Inode, self).process_data()\n\nif self.new:\n  return\n\nself.name = \"\"\ni = 0\n\n# Parsing name\nwhile True:\n  if self.data[i] == \"$\":\n    break\n  self.name += self.data[i]\n  i += 1\n\n# Parsing size\ni += 1\ntmpsize = \"0\"\nwhile True:\n  if self.data[i] == \"$\":\n    break\n  tmpsize += self.data[i]\n  i += 1\n\nself.file_size = int(tmpsize)\n\n# Parsing type\ni += 1\nself.type = self.data[i]\ni += 1\n\n# Parsing 12 directions (10 direct, 2 indirect)\nfor dir in xrange(0, 12):\n  init = i\n  while True:\n    i += 1\n    if self.data[i] == \"$\":\n      end = i\n      break\n    \n  self.directions.append(Direction(''.join(self.data[init:end])))\n  i += 1", "path": "common\\structure.py", "repo_name": "chiiph/ESFS", "stars": 41, "license": "None", "language": "python", "size": 112}
{"docstring": "'''hides data in an image'''\n\n", "func_signal": "def encode_inplace(image, data):\n", "code": "w = image.size[0]\n(x, y) = (0, 0)\nfor pixel in encode_imdata(image.getdata(), data):\n    image.putpixel((x, y), pixel)\n    if x == w - 1:\n        x = 0\n        y += 1\n    else:\n        x += 1", "path": "common\\stepic.py", "repo_name": "chiiph/ESFS", "stars": 41, "license": "None", "language": "python", "size": 112}
{"docstring": "\"\"\" Sets the name truncated to 255 \"\"\"\n\n", "func_signal": "def set_name(self, str):\n", "code": "self.modified = True\nself.name = str[0:254]", "path": "common\\structure.py", "repo_name": "chiiph/ESFS", "stars": 41, "license": "None", "language": "python", "size": 112}
{"docstring": "\"\"\" Parses a string divided by sep chars of the Direction string definition \"\"\"\n\n", "func_signal": "def parse_string(self, sep):\n", "code": "tmp = ''.join(self.src).split(sep)\nself.type = tmp[0][-1]\nif self.type == \"0\":\n  return\nself.path = tmp[1]\nself.size = int(tmp[2])\nself.key = tmp[3]", "path": "common\\structure.py", "repo_name": "chiiph/ESFS", "stars": 41, "license": "None", "language": "python", "size": 112}
{"docstring": "\"\"\" Sets the new size of the file \"\"\"\n\n", "func_signal": "def set_size(self, size):\n", "code": "self.modified = True\nself.file_size = size", "path": "common\\structure.py", "repo_name": "chiiph/ESFS", "stars": 41, "license": "None", "language": "python", "size": 112}
{"docstring": "'''Given a sequence of pixels, returns an iterator of characters\nencoded in the image'''\n\n", "func_signal": "def decode_imdata(imdata):\n", "code": "data = []\nimdata = iter(imdata)\nwhile True:\n    pixels = list(imdata.next()[:3] + imdata.next()[:3] + imdata.next()[:3])\n    byte = 0\n    for c in xrange(7):\n        byte |= pixels[c] & 1\n        byte <<= 1\n    byte |= pixels[7] & 1\n    data.append(chr(byte))\n    if pixels[-1] & 1:\n        break\n\nreturn data", "path": "common\\stepic.py", "repo_name": "chiiph/ESFS", "stars": 41, "license": "None", "language": "python", "size": 112}
{"docstring": "\"\"\" Given a path it returns the file or directory of the last element \"\"\"\n\n", "func_signal": "def get_obj(self, path):\n", "code": "if path in self.last_objs.keys():\n  return self.last_objs[path]\n\npe = path.split('/')[1:]\n\nstep = 0\ncurrent = self.root_dir\nwhole_path = \"\"\n\n# we cycle through the whole path\nfor part in pe:\n  in_cache = False\n  whole_path += \"/\"+part\n  if whole_path in self.last_objs.keys():\n    in_cache = True\n    current = self.last_objs[whole_path]\n  else:\n    # if its a dir, stand on it\n    if current.dirs[part].is_dir():\n      current = Directory(current.dirs[part], self.free, prefix=self.prefix)\n    # if it's not a dir, and it's not the last element, error\n    elif part != pe[-1]:\n      return -errno.ENOENT\n    # else it's a file and the last element of the path\n    else:\n      current = File(current.dirs[part], self.free, prefix=self.prefix)\n    # if it's just a directory in the middle, cycle, and take the new current\n    # as the base\n\n  # save all the objects in a temporal cache\n  if not in_cache:\n    self.last_objs[whole_path] = current\n\nreturn current", "path": "esfs.py", "repo_name": "chiiph/ESFS", "stars": 41, "license": "None", "language": "python", "size": 112}
{"docstring": "\"\"\" Returns True if there is a next block available (only implemented for direct) \"\"\"\n\n# TODO: implement for indirect\n", "func_signal": "def exists_next_block(self):\n", "code": "if self.current < 9 and self.directions[self.current+1].type != \"0\":\n  return True\nreturn False", "path": "common\\structure.py", "repo_name": "chiiph/ESFS", "stars": 41, "license": "None", "language": "python", "size": 112}
{"docstring": "\"\"\" Appends a direction to the direction if index=-1 or replace the one in index \"\"\"\n\n", "func_signal": "def add_direction_str(self, direction, index=-1):\n", "code": "if index == -1 and len(self.directions) < 12:\n  self.directions.append(direction)\nelif index < len(self.directions):\n  self.directions[index] = direction", "path": "common\\structure.py", "repo_name": "chiiph/ESFS", "stars": 41, "license": "None", "language": "python", "size": 112}
{"docstring": "\"\"\" Sets the i'th byte to value \"\"\"\n\n", "func_signal": "def __setitem__(self, i, value):\n", "code": "self.modified = True\nself.data[i] = value", "path": "common\\structure.py", "repo_name": "chiiph/ESFS", "stars": 41, "license": "None", "language": "python", "size": 112}
{"docstring": "\"\"\" Returns the next block available (only implemented for direct) \"\"\"\n\n# TODO: implement for indirect\n", "func_signal": "def next_block(self):\n", "code": "if self.current < 9 and self.directions[self.current+1].type != \"0\":\n  self.current+=1\n  return Block(self.directions[self.current].path, self.directions[self.current].key, prefix=self.prefix)\nreturn None", "path": "common\\structure.py", "repo_name": "chiiph/ESFS", "stars": 41, "license": "None", "language": "python", "size": 112}
{"docstring": "\"\"\" Writes bug from this file beggining in offset \"\"\"\n\n# rewind the block iterator \n# TODO: optimize this\n", "func_signal": "def write(self, buf, offset):\n", "code": "self.inode.rewind()\n\n# find the block where offset is\nfound = False\ncount = 0\nblock = None\nwhile self.inode.exists_next_block() and not found:\n  block = self.inode.next_block()\n  found = count+block.size > offset\n  if not found:\n    count += block.size\n\n# if there's no block found to write on, and the idea is to write\n# in a new block\nif not found and (self.inode.file_size - offset) <= 0:\n  # if there are no free blocks, return\n  if len(self.free) == 0:\n    return\n  # create a new key for encryption\n  byte,key = crypt.GenerateKey()\n  type = 1 # local\n\n  newblock = Block(self.free[0], key, new=True, prefix=self.prefix)\n\n  self.inode.add_direction(type, newblock.src, newblock.size, newblock.key)\n\n  del self.free[0]\n  block = self.inode.next_block()\n\n# if it doesn't exists return nothing\nif block is None:\n  return\n\n# set the new offset\nnewfrom = offset\n# set a new size\nnewsize = len(buf)\nwritten = 0\n\n# while we haven't read the whole size of what we want\n# or we haven't reach the end of the block:\n# write and advance\nwhile newsize > 0 and (newfrom - count) < block.size:\n  # as there is still space left on the block, write that byte\n  block[newfrom - count] = buf[newfrom - offset]\n  newfrom += 1\n  newsize -= 1\n  written += 1\n  if newfrom > self.inode.file_size:\n    # if there's still space left on the block, but it's the last block\n    # and the declared file size is reached, increase the file size\n    self.inode.set_size(self.inode.file_size + 1)\n\n# if we've reach the end of the block and there's more to write\n# write the newbuf from newfrom\nif newsize > 0:\n  written += self.write(buf[newfrom - offset:], newfrom)\n\nreturn written", "path": "common\\file.py", "repo_name": "chiiph/ESFS", "stars": 41, "license": "None", "language": "python", "size": 112}
{"docstring": "# TODO: implement for indirect\n", "func_signal": "def exists_prev_block(self):\n", "code": "if self.current > 0 and self.directions[self.current-1].type != \"0\":\n  return True\nreturn False", "path": "common\\structure.py", "repo_name": "chiiph/ESFS", "stars": 41, "license": "None", "language": "python", "size": 112}
{"docstring": "\"\"\" Saves the block data \"\"\"\n\n", "func_signal": "def save_data(self):\n", "code": "if not self.modified:\n  return\n\nim = Image.open(self.prefix+self.src)\nencoded = crypt.encode(self.key, ''.join(self.data))\n\nenc_im = stepic.encode(im, encoded)\nenc_im.save(self.prefix+self.src)\n\nself.modified = False\nreturn", "path": "common\\structure.py", "repo_name": "chiiph/ESFS", "stars": 41, "license": "None", "language": "python", "size": 112}
{"docstring": "\"\"\"Escape the characters in label which need it.\n@returns: the escaped string\n@rtype: string\"\"\"\n", "func_signal": "def _escapify(label):\n", "code": "text = ''\nfor c in label:\n    if c in _escaped:\n        text += '\\\\' + c\n    elif ord(c) > 0x20 and ord(c) < 0x7F:\n        text += c\n    else:\n        text += '\\\\%03d' % ord(c)\nreturn text", "path": "dns\\name.py", "repo_name": "cykor/VPNCykoGM", "stars": 40, "license": "None", "language": "python", "size": 360}
{"docstring": "\"\"\"If self is a subdomain of origin, return a new name which is self\nrelative to origin.  Otherwise return self.\n@rtype: dns.name.Name object\n\"\"\"\n\n", "func_signal": "def relativize(self, origin):\n", "code": "if not origin is None and self.is_subdomain(origin):\n    return Name(self[: -len(origin)])\nelse:\n    return self", "path": "dns\\name.py", "repo_name": "cykor/VPNCykoGM", "stars": 40, "license": "None", "language": "python", "size": 360}
{"docstring": "\"\"\"Convert name to wire format, possibly compressing it.\n\n@param file: the file where the name is emitted (typically\na cStringIO file).  If None, a string containing the wire name\nwill be returned.\n@type file: file or None\n@param compress: The compression table.  If None (the default) names\nwill not be compressed.\n@type compress: dict\n@param origin: If the name is relative and origin is not None, then\norigin will be appended to it.\n@type origin: dns.name.Name object\n@raises NeedAbsoluteNameOrOrigin: All names in wire format are\nabsolute.  If self is a relative name, then an origin must be supplied;\nif it is missing, then this exception is raised\n\"\"\"\n\n", "func_signal": "def to_wire(self, file = None, compress = None, origin = None):\n", "code": "if file is None:\n    file = cStringIO.StringIO()\n    want_return = True\nelse:\n    want_return = False\n\nif not self.is_absolute():\n    if origin is None or not origin.is_absolute():\n        raise NeedAbsoluteNameOrOrigin\n    labels = list(self.labels)\n    labels.extend(list(origin.labels))\nelse:\n    labels = self.labels\ni = 0\nfor label in labels:\n    n = Name(labels[i:])\n    i += 1\n    if not compress is None:\n        pos = compress.get(n)\n    else:\n        pos = None\n    if not pos is None:\n        value = 0xc000 + pos\n        s = struct.pack('!H', value)\n        file.write(s)\n        break\n    else:\n        if not compress is None and len(n) > 1:\n            pos = file.tell()\n            if pos < 0xc000:\n                compress[n] = pos\n        l = len(label)\n        file.write(chr(l))\n        if l > 0:\n            file.write(label)\nif want_return:\n    return file.getvalue()", "path": "dns\\name.py", "repo_name": "cykor/VPNCykoGM", "stars": 40, "license": "None", "language": "python", "size": 360}
{"docstring": "\"\"\"Compare two names, returning a 3-tuple (relation, order, nlabels).\n\nI{relation} describes the relation ship beween the names,\nand is one of: dns.name.NAMERELN_NONE,\ndns.name.NAMERELN_SUPERDOMAIN, dns.name.NAMERELN_SUBDOMAIN,\ndns.name.NAMERELN_EQUAL, or dns.name.NAMERELN_COMMONANCESTOR\n\nI{order} is < 0 if self < other, > 0 if self > other, and ==\n0 if self == other.  A relative name is always less than an\nabsolute name.  If both names have the same relativity, then\nthe DNSSEC order relation is used to order them.\n\nI{nlabels} is the number of significant labels that the two names\nhave in common.\n\"\"\"\n\n", "func_signal": "def fullcompare(self, other):\n", "code": "sabs = self.is_absolute()\noabs = other.is_absolute()\nif sabs != oabs:\n    if sabs:\n        return (NAMERELN_NONE, 1, 0)\n    else:\n        return (NAMERELN_NONE, -1, 0)\nl1 = len(self.labels)\nl2 = len(other.labels)\nldiff = l1 - l2\nif ldiff < 0:\n    l = l1\nelse:\n    l = l2\n\norder = 0\nnlabels = 0\nnamereln = NAMERELN_NONE\nwhile l > 0:\n    l -= 1\n    l1 -= 1\n    l2 -= 1\n    label1 = self.labels[l1].lower()\n    label2 = other.labels[l2].lower()\n    if label1 < label2:\n        order = -1\n        if nlabels > 0:\n            namereln = NAMERELN_COMMONANCESTOR\n        return (namereln, order, nlabels)\n    elif label1 > label2:\n        order = 1\n        if nlabels > 0:\n            namereln = NAMERELN_COMMONANCESTOR\n        return (namereln, order, nlabels)\n    nlabels += 1\norder = ldiff\nif ldiff < 0:\n    namereln = NAMERELN_SUPERDOMAIN\nelif ldiff > 0:\n    namereln = NAMERELN_SUBDOMAIN\nelse:\n    namereln = NAMERELN_EQUAL\nreturn (namereln, order, nlabels)", "path": "dns\\name.py", "repo_name": "cykor/VPNCykoGM", "stars": 40, "license": "None", "language": "python", "size": 360}
{"docstring": "\"\"\"Look for rdata with the specified name and type in the zone,\nand return an RRset encapsulating it.\n\nThe I{name}, I{rdtype}, and I{covers} parameters may be\nstrings, in which case they will be converted to their proper\ntype.\n\nThis method is less efficient than the similar L{get_rdataset}\nbecause it creates an RRset instead of returning the matching\nrdataset.  It may be more convenient for some uses since it\nreturns an object which binds the owner name to the rdata.\n\nThis method may not be used to create new nodes or rdatasets;\nuse L{find_rdataset} instead.\n\nNone is returned if the name or type are not found.\nUse L{find_rrset} if you want to have KeyError raised instead.\n\n@param name: the owner name to look for\n@type name: DNS.name.Name object or string\n@param rdtype: the rdata type desired\n@type rdtype: int or string\n@param covers: the covered type (defaults to None)\n@type covers: int or string\n@rtype: dns.rrset.RRset object\n\"\"\"\n\n", "func_signal": "def get_rrset(self, name, rdtype, covers=dns.rdatatype.NONE):\n", "code": "try:\n    rrset = self.find_rrset(name, rdtype, covers)\nexcept KeyError:\n    rrset = None\nreturn rrset", "path": "dns\\zone.py", "repo_name": "cykor/VPNCykoGM", "stars": 40, "license": "None", "language": "python", "size": 360}
{"docstring": "\"\"\"Convert a reverse map domain name into textual address form.\n@param name: an IPv4 or IPv6 address in reverse-map form.\n@type name: dns.name.Name object\n@rtype: str\n\"\"\"\n", "func_signal": "def to_address(name):\n", "code": "if name.is_subdomain(ipv4_reverse_domain):\n    name = name.relativize(ipv4_reverse_domain)\n    labels = list(name.labels)\n    labels.reverse()\n    text = '.'.join(labels)\n    # run through inet_aton() to check syntax and make pretty.\n    return dns.ipv4.inet_ntoa(dns.ipv4.inet_aton(text))\nelif name.is_subdomain(ipv6_reverse_domain):\n    name = name.relativize(ipv6_reverse_domain)\n    labels = list(name.labels)\n    labels.reverse()\n    parts = []\n    i = 0\n    l = len(labels)\n    while i < l:\n        parts.append(''.join(labels[i:i+4]))\n        i += 4\n    text = ':'.join(parts)\n    # run through inet_aton() to check syntax and make pretty.\n    return dns.ipv6.inet_ntoa(dns.ipv6.inet_aton(text))\nelse:\n    raise dns.exception.SyntaxError('unknown reverse-map address family')", "path": "dns\\reversename.py", "repo_name": "cykor/VPNCykoGM", "stars": 40, "license": "None", "language": "python", "size": 360}
{"docstring": "\"\"\"Process one line from a DNS master file.\"\"\"\n# Name\n", "func_signal": "def _rr_line(self):\n", "code": "if self.current_origin is None:\n    raise UnknownOrigin\ntoken = self.tok.get(want_leading = True)\nif not token.is_whitespace():\n    self.last_name = dns.name.from_text(token.value, self.current_origin)\nelse:\n    token = self.tok.get()\n    if token.is_eol_or_eof():\n        # treat leading WS followed by EOL/EOF as if they were EOL/EOF.\n        return\n    self.tok.unget(token)\nname = self.last_name\nif not name.is_subdomain(self.zone.origin):\n    self._eat_line()\n    return\nif self.relativize:\n    name = name.relativize(self.zone.origin)\ntoken = self.tok.get()\nif not token.is_identifier():\n    raise dns.exception.SyntaxError\n# TTL\ntry:\n    ttl = dns.ttl.from_text(token.value)\n    token = self.tok.get()\n    if not token.is_identifier():\n        raise dns.exception.SyntaxError\nexcept dns.ttl.BadTTL:\n    ttl = self.ttl\n# Class\ntry:\n    rdclass = dns.rdataclass.from_text(token.value)\n    token = self.tok.get()\n    if not token.is_identifier():\n        raise dns.exception.SyntaxError\nexcept dns.exception.SyntaxError:\n    raise dns.exception.SyntaxError\nexcept:\n    rdclass = self.zone.rdclass\nif rdclass != self.zone.rdclass:\n    raise dns.exception.SyntaxError(\"RR class is not zone's class\")\n# Type\ntry:\n    rdtype = dns.rdatatype.from_text(token.value)\nexcept:\n    raise dns.exception.SyntaxError(\"unknown rdatatype '%s'\" % token.value)\nn = self.zone.nodes.get(name)\nif n is None:\n    n = self.zone.node_factory()\n    self.zone.nodes[name] = n\ntry:\n    rd = dns.rdata.from_text(rdclass, rdtype, self.tok,\n                             self.current_origin, False)\nexcept dns.exception.SyntaxError:\n    # Catch and reraise.\n    (ty, va) = sys.exc_info()[:2]\n    raise va\nexcept:\n    # All exceptions that occur in the processing of rdata\n    # are treated as syntax errors.  This is not strictly\n    # correct, but it is correct almost all of the time.\n    # We convert them to syntax errors so that we can emit\n    # helpful filename:line info.\n    (ty, va) = sys.exc_info()[:2]\n    raise dns.exception.SyntaxError(\"caught exception %s: %s\" % (str(ty), str(va)))\n\nrd.choose_relativity(self.zone.origin, self.relativize)\ncovers = rd.covers()\nrds = n.find_rdataset(rdclass, rdtype, covers, True)\nrds.add(rd, ttl)", "path": "dns\\zone.py", "repo_name": "cykor/VPNCykoGM", "stars": 40, "license": "None", "language": "python", "size": 360}
{"docstring": "\"\"\"Convert unicode text into a Name object.\n\nLables are encoded in IDN ACE form.\n\n@rtype: dns.name.Name object\n\"\"\"\n\n", "func_signal": "def from_unicode(text, origin = root):\n", "code": "if not isinstance(text, unicode):\n    raise ValueError(\"input to from_unicode() must be a unicode string\")\nif not (origin is None or isinstance(origin, Name)):\n    raise ValueError(\"origin must be a Name or None\")\nlabels = []\nlabel = u''\nescaping = False\nedigits = 0\ntotal = 0\nif text == u'@':\n    text = u''\nif text:\n    if text == u'.':\n        return Name([''])\t# no Unicode \"u\" on this constant!\n    for c in text:\n        if escaping:\n            if edigits == 0:\n                if c.isdigit():\n                    total = int(c)\n                    edigits += 1\n                else:\n                    label += c\n                    escaping = False\n            else:\n                if not c.isdigit():\n                    raise BadEscape\n                total *= 10\n                total += int(c)\n                edigits += 1\n                if edigits == 3:\n                    escaping = False\n                    label += chr(total)\n        elif c == u'.' or c == u'\\u3002' or \\\n             c == u'\\uff0e' or c == u'\\uff61':\n            if len(label) == 0:\n                raise EmptyLabel\n            labels.append(encodings.idna.ToASCII(label))\n            label = u''\n        elif c == u'\\\\':\n            escaping = True\n            edigits = 0\n            total = 0\n        else:\n            label += c\n    if escaping:\n        raise BadEscape\n    if len(label) > 0:\n        labels.append(encodings.idna.ToASCII(label))\n    else:\n        labels.append('')\nif (len(labels) == 0 or labels[-1] != '') and not origin is None:\n    labels.extend(list(origin.labels))\nreturn Name(labels)", "path": "dns\\name.py", "repo_name": "cykor/VPNCykoGM", "stars": 40, "license": "None", "language": "python", "size": 360}
{"docstring": "\"\"\"Return a new name which is the concatenation of self and other.\n@rtype: dns.name.Name object\n@raises AbsoluteConcatenation: self is absolute and other is\nnot the empty name\n\"\"\"\n\n", "func_signal": "def concatenate(self, other):\n", "code": "if self.is_absolute() and len(other) > 0:\n    raise AbsoluteConcatenation\nlabels = list(self.labels)\nlabels.extend(list(other.labels))\nreturn Name(labels)", "path": "dns\\name.py", "repo_name": "cykor/VPNCykoGM", "stars": 40, "license": "None", "language": "python", "size": 360}
{"docstring": "\"\"\"Get a node in the zone, possibly creating it.\n\nThis method is like L{find_node}, except it returns None instead\nof raising an exception if the node does not exist and creation\nhas not been requested.\n\n@param name: the name of the node to find\n@type name: dns.name.Name object or string\n@param create: should the node be created if it doesn't exist?\n@type create: bool\n@rtype: dns.node.Node object or None\n\"\"\"\n\n", "func_signal": "def get_node(self, name, create=False):\n", "code": "try:\n    node = self.find_node(name, create)\nexcept KeyError:\n    node = None\nreturn node", "path": "dns\\zone.py", "repo_name": "cykor/VPNCykoGM", "stars": 40, "license": "None", "language": "python", "size": 360}
{"docstring": "\"\"\"Write a zone to a file.\n\n@param f: file or string.  If I{f} is a string, it is treated\nas the name of a file to open.\n@param sorted: if True, the file will be written with the\nnames sorted in DNSSEC order from least to greatest.  Otherwise\nthe names will be written in whatever order they happen to have\nin the zone's dictionary.\n@param relativize: if True, domain names in the output will be\nrelativized to the zone's origin (if possible).\n@type relativize: bool\n@param nl: The end of line string.  If not specified, the\noutput will use the platform's native end-of-line marker (i.e.\nLF on POSIX, CRLF on Windows, CR on Macintosh).\n@type nl: string or None\n\"\"\"\n\n", "func_signal": "def to_file(self, f, sorted=True, relativize=True, nl=None):\n", "code": "if sys.hexversion >= 0x02030000:\n    # allow Unicode filenames\n    str_type = basestring\nelse:\n    str_type = str\nif nl is None:\n    opts = 'w'\nelse:\n    opts = 'wb'\nif isinstance(f, str_type):\n    f = file(f, opts)\n    want_close = True\nelse:\n    want_close = False\ntry:\n    if sorted:\n        names = self.keys()\n        names.sort()\n    else:\n        names = self.iterkeys()\n    for n in names:\n        l = self[n].to_text(n, origin=self.origin,\n                            relativize=relativize)\n        if nl is None:\n            print >> f, l\n        else:\n            f.write(l)\n            f.write(nl)\nfinally:\n    if want_close:\n        f.close()", "path": "dns\\zone.py", "repo_name": "cykor/VPNCykoGM", "stars": 40, "license": "None", "language": "python", "size": 360}
{"docstring": "\"\"\"Initialize a zone object.\n\n@param origin: The origin of the zone.\n@type origin: dns.name.Name object\n@param rdclass: The zone's rdata class; the default is class IN.\n@type rdclass: int\"\"\"\n\n", "func_signal": "def __init__(self, origin, rdclass=dns.rdataclass.IN, relativize=True):\n", "code": "self.rdclass = rdclass\nself.origin = origin\nself.nodes = {}\nself.relativize = relativize", "path": "dns\\zone.py", "repo_name": "cykor/VPNCykoGM", "stars": 40, "license": "None", "language": "python", "size": 360}
{"docstring": "\"\"\"Do some simple checking of the zone's origin.\n\n@raises dns.zone.NoSOA: there is no SOA RR\n@raises dns.zone.NoNS: there is no NS RRset\n@raises KeyError: there is no origin node\n\"\"\"\n", "func_signal": "def check_origin(self):\n", "code": "if self.relativize:\n    name = dns.name.empty\nelse:\n    name = self.origin\nif self.get_rdataset(name, dns.rdatatype.SOA) is None:\n    raise NoSOA\nif self.get_rdataset(name, dns.rdatatype.NS) is None:\n    raise NoNS", "path": "dns\\zone.py", "repo_name": "cykor/VPNCykoGM", "stars": 40, "license": "None", "language": "python", "size": 360}
{"docstring": "\"\"\"Split a name into a prefix and suffix at depth.\n\n@param depth: the number of labels in the suffix\n@type depth: int\n@raises ValueError: the depth was not >= 0 and <= the length of the\nname.\n@returns: the tuple (prefix, suffix)\n@rtype: tuple\n\"\"\"\n\n", "func_signal": "def split(self, depth):\n", "code": "l = len(self.labels)\nif depth == 0:\n    return (self, dns.name.empty)\nelif depth == l:\n    return (dns.name.empty, self)\nelif depth < 0 or depth > l:\n    raise ValueError('depth must be >= 0 and <= the length of the name')\nreturn (Name(self[: -depth]), Name(self[-depth :]))", "path": "dns\\name.py", "repo_name": "cykor/VPNCykoGM", "stars": 40, "license": "None", "language": "python", "size": 360}
{"docstring": "\"\"\"Delete the specified node if it exists.\n\nIt is not an error if the node does not exist.\n\"\"\"\n\n", "func_signal": "def delete_node(self, name):\n", "code": "name = self._validate_name(name)\nif self.nodes.has_key(name):\n    del self.nodes[name]", "path": "dns\\zone.py", "repo_name": "cykor/VPNCykoGM", "stars": 40, "license": "None", "language": "python", "size": 360}
{"docstring": "\"\"\"Convert name to a format suitable for digesting in hashes.\n\nThe name is canonicalized and converted to uncompressed wire format.\n\n@param origin: If the name is relative and origin is not None, then\norigin will be appended to it.\n@type origin: dns.name.Name object\n@raises NeedAbsoluteNameOrOrigin: All names in wire format are\nabsolute.  If self is a relative name, then an origin must be supplied;\nif it is missing, then this exception is raised\n@rtype: string\n\"\"\"\n\n", "func_signal": "def to_digestable(self, origin=None):\n", "code": "if not self.is_absolute():\n    if origin is None or not origin.is_absolute():\n        raise NeedAbsoluteNameOrOrigin\n    labels = list(self.labels)\n    labels.extend(list(origin.labels))\nelse:\n    labels = self.labels\ndlabels = [\"%s%s\" % (chr(len(x)), x.lower()) for x in labels]\nreturn ''.join(dlabels)", "path": "dns\\name.py", "repo_name": "cykor/VPNCykoGM", "stars": 40, "license": "None", "language": "python", "size": 360}
{"docstring": "\"\"\"Delete the rdataset matching I{rdtype} and I{covers}, if it\nexists at the node specified by I{name}.\n\nThe I{name}, I{rdtype}, and I{covers} parameters may be\nstrings, in which case they will be converted to their proper\ntype.\n\nIt is not an error if the node does not exist, or if there is no\nmatching rdataset at the node.\n\nIf the node has no rdatasets after the deletion, it will itself\nbe deleted.\n\n@param name: the owner name to look for\n@type name: DNS.name.Name object or string\n@param rdtype: the rdata type desired\n@type rdtype: int or string\n@param covers: the covered type (defaults to None)\n@type covers: int or string\n\"\"\"\n\n", "func_signal": "def delete_rdataset(self, name, rdtype, covers=dns.rdatatype.NONE):\n", "code": "name = self._validate_name(name)\nif isinstance(rdtype, (str, unicode)):\n    rdtype = dns.rdatatype.from_text(rdtype)\nif isinstance(covers, (str, unicode)):\n    covers = dns.rdatatype.from_text(covers)\nnode = self.get_node(name)\nif not node is None:\n    node.delete_rdataset(self.rdclass, rdtype, covers)\n    if len(node) == 0:\n        self.delete_node(name)", "path": "dns\\zone.py", "repo_name": "cykor/VPNCykoGM", "stars": 40, "license": "None", "language": "python", "size": 360}
{"docstring": "\"\"\"Look for rdata with the specified name and type in the zone,\nand return an RRset encapsulating it.\n\nThe I{name}, I{rdtype}, and I{covers} parameters may be\nstrings, in which case they will be converted to their proper\ntype.\n\nThis method is less efficient than the similar\nL{find_rdataset} because it creates an RRset instead of\nreturning the matching rdataset.  It may be more convenient\nfor some uses since it returns an object which binds the owner\nname to the rdata.\n\nThis method may not be used to create new nodes or rdatasets;\nuse L{find_rdataset} instead.\n\nKeyError is raised if the name or type are not found.\nUse L{get_rrset} if you want to have None returned instead.\n\n@param name: the owner name to look for\n@type name: DNS.name.Name object or string\n@param rdtype: the rdata type desired\n@type rdtype: int or string\n@param covers: the covered type (defaults to None)\n@type covers: int or string\n@raises KeyError: the node or rdata could not be found\n@rtype: dns.rrset.RRset object\n\"\"\"\n\n", "func_signal": "def find_rrset(self, name, rdtype, covers=dns.rdatatype.NONE):\n", "code": "name = self._validate_name(name)\nif isinstance(rdtype, (str, unicode)):\n    rdtype = dns.rdatatype.from_text(rdtype)\nif isinstance(covers, (str, unicode)):\n    covers = dns.rdatatype.from_text(covers)\nrdataset = self.nodes[name].find_rdataset(self.rdclass, rdtype, covers)\nrrset = dns.rrset.RRset(name, self.rdclass, rdtype, covers)\nrrset.update(rdataset)\nreturn rrset", "path": "dns\\zone.py", "repo_name": "cykor/VPNCykoGM", "stars": 40, "license": "None", "language": "python", "size": 360}
{"docstring": "\"\"\"Convert the output of a zone transfer generator into a zone object.\n\n@param xfr: The xfr generator\n@type xfr: generator of dns.message.Message objects\n@param relativize: should names be relativized?  The default is True.\nIt is essential that the relativize setting matches the one specified\nto dns.query.xfr().\n@type relativize: bool\n@param check_origin: should sanity checks of the origin node be done?\nThe default is True.\n@type check_origin: bool\n@raises dns.zone.NoSOA: No SOA RR was found at the zone origin\n@raises dns.zone.NoNS: No NS RRset was found at the zone origin\n@rtype: dns.zone.Zone object\n\"\"\"\n\n", "func_signal": "def from_xfr(xfr, zone_factory=Zone, relativize=True, check_origin=True):\n", "code": "z = None\nfor r in xfr:\n    if z is None:\n        if relativize:\n            origin = r.origin\n        else:\n            origin = r.answer[0].name\n        rdclass = r.answer[0].rdclass\n        z = zone_factory(origin, rdclass, relativize=relativize)\n    for rrset in r.answer:\n        znode = z.nodes.get(rrset.name)\n        if not znode:\n            znode = z.node_factory()\n            z.nodes[rrset.name] = znode\n        zrds = znode.find_rdataset(rrset.rdclass, rrset.rdtype,\n                                   rrset.covers, True)\n        zrds.update_ttl(rrset.ttl)\n        for rd in rrset:\n            rd.choose_relativity(z.origin, relativize)\n            zrds.add(rd)\nif check_origin:\n    z.check_origin()\nreturn z", "path": "dns\\zone.py", "repo_name": "cykor/VPNCykoGM", "stars": 40, "license": "None", "language": "python", "size": 360}
{"docstring": "\"\"\"Convert an IPv4 or IPv6 address in textual form into a Name object whose\nvalue is the reverse-map domain name of the address.\n@param text: an IPv4 or IPv6 address in textual form (e.g. '127.0.0.1',\n'::1')\n@type text: str\n@rtype: dns.name.Name object\n\"\"\"\n", "func_signal": "def from_address(text):\n", "code": "try:\n    parts = list(dns.ipv6.inet_aton(text).encode('hex_codec'))\n    origin = ipv6_reverse_domain\nexcept:\n    parts = ['%d' % ord(byte) for byte in dns.ipv4.inet_aton(text)]\n    origin = ipv4_reverse_domain\nparts.reverse()\nreturn dns.name.from_text('.'.join(parts), origin=origin)", "path": "dns\\reversename.py", "repo_name": "cykor/VPNCykoGM", "stars": 40, "license": "None", "language": "python", "size": 360}
{"docstring": "# match the pattern for this action to the arg strings\n", "func_signal": "def _match_argument(self, action, arg_strings_pattern):\n", "code": "nargs_pattern = self._get_nargs_pattern(action)\nmatch = _re.match(nargs_pattern, arg_strings_pattern)\n\n# raise an exception if we weren't able to find a match\nif match is None:\n    nargs_errors = {\n        None: _('expected one argument'),\n        OPTIONAL: _('expected at most one argument'),\n        ONE_OR_MORE: _('expected at least one argument'),\n    }\n    default = _('expected %s argument(s)') % action.nargs\n    msg = nargs_errors.get(action.nargs, default)\n    raise ArgumentError(action, msg)\n\n# return the number of arguments matched\nreturn len(match.group(1))", "path": "thr\\argparse.py", "repo_name": "clowwindy/PyWebGet", "stars": 60, "license": "other", "language": "python", "size": 1733}
{"docstring": "\"\"\"Returns the first python token from the given text.\n\n    >>> python_lookahead = Parser().python_lookahead\n    >>> python_lookahead('for i in range(10):')\n    'for'\n    >>> python_lookahead('else:')\n    'else'\n    >>> python_lookahead(' x = 1')\n    ' '\n\"\"\"\n", "func_signal": "def python_lookahead(self, text):\n", "code": "readline = iter([text]).next\ntokens = tokenize.generate_tokens(readline)\nreturn tokens.next()[1]", "path": "web\\template.py", "repo_name": "clowwindy/PyWebGet", "stars": 60, "license": "other", "language": "python", "size": 1733}
{"docstring": "\"\"\"Add a global to this rendering instance.\"\"\"\n", "func_signal": "def _add_global(self, obj, name=None):\n", "code": "if 'globals' not in self._keywords: self._keywords['globals'] = {}\nif not name:\n    name = obj.__name__\nself._keywords['globals'][name] = obj", "path": "web\\template.py", "repo_name": "clowwindy/PyWebGet", "stars": 60, "license": "other", "language": "python", "size": 1733}
{"docstring": "\"\"\"Prepare value of __body__ by joining parts.\n\"\"\"\n", "func_signal": "def _prepare_body(self):\n", "code": "if self._parts:\n    value = u\"\".join(self._parts)\n    self._parts[:] = []\n    body = self._d.get('__body__')\n    if body:\n        self._d['__body__'] = body + value\n    else:\n        self._d['__body__'] = value", "path": "web\\template.py", "repo_name": "clowwindy/PyWebGet", "stars": 60, "license": "other", "language": "python", "size": 1733}
{"docstring": "# replace arg strings that are file references\n", "func_signal": "def _parse_known_args(self, arg_strings, namespace):\n", "code": "if self.fromfile_prefix_chars is not None:\n    arg_strings = self._read_args_from_files(arg_strings)\n\n# map all mutually exclusive arguments to the other arguments\n# they can't occur with\naction_conflicts = {}\nfor mutex_group in self._mutually_exclusive_groups:\n    group_actions = mutex_group._group_actions\n    for i, mutex_action in enumerate(mutex_group._group_actions):\n        conflicts = action_conflicts.setdefault(mutex_action, [])\n        conflicts.extend(group_actions[:i])\n        conflicts.extend(group_actions[i + 1:])\n\n# find all option indices, and determine the arg_string_pattern\n# which has an 'O' if there is an option at an index,\n# an 'A' if there is an argument, or a '-' if there is a '--'\noption_string_indices = {}\narg_string_pattern_parts = []\narg_strings_iter = iter(arg_strings)\nfor i, arg_string in enumerate(arg_strings_iter):\n\n    # all args after -- are non-options\n    if arg_string == '--':\n        arg_string_pattern_parts.append('-')\n        for arg_string in arg_strings_iter:\n            arg_string_pattern_parts.append('A')\n\n    # otherwise, add the arg to the arg strings\n    # and note the index if it was an option\n    else:\n        option_tuple = self._parse_optional(arg_string)\n        if option_tuple is None:\n            pattern = 'A'\n        else:\n            option_string_indices[i] = option_tuple\n            pattern = 'O'\n        arg_string_pattern_parts.append(pattern)\n\n# join the pieces together to form the pattern\narg_strings_pattern = ''.join(arg_string_pattern_parts)\n\n# converts arg strings to the appropriate and then takes the action\nseen_actions = set()\nseen_non_default_actions = set()\n\ndef take_action(action, argument_strings, option_string=None):\n    seen_actions.add(action)\n    argument_values = self._get_values(action, argument_strings)\n\n    # error if this argument is not allowed with other previously\n    # seen arguments, assuming that actions that use the default\n    # value don't really count as \"present\"\n    if argument_values is not action.default:\n        seen_non_default_actions.add(action)\n        for conflict_action in action_conflicts.get(action, []):\n            if conflict_action in seen_non_default_actions:\n                msg = _('not allowed with argument %s')\n                action_name = _get_action_name(conflict_action)\n                raise ArgumentError(action, msg % action_name)\n\n    # take the action if we didn't receive a SUPPRESS value\n    # (e.g. from a default)\n    if argument_values is not SUPPRESS:\n        action(self, namespace, argument_values, option_string)\n\n# function to convert arg_strings into an optional action\ndef consume_optional(start_index):\n\n    # get the optional identified at this index\n    option_tuple = option_string_indices[start_index]\n    action, option_string, explicit_arg = option_tuple\n\n    # identify additional optionals in the same arg string\n    # (e.g. -xyz is the same as -x -y -z if no args are required)\n    match_argument = self._match_argument\n    action_tuples = []\n    while True:\n\n        # if we found no optional action, skip it\n        if action is None:\n            extras.append(arg_strings[start_index])\n            return start_index + 1\n\n        # if there is an explicit argument, try to match the\n        # optional's string arguments to only this\n        if explicit_arg is not None:\n            arg_count = match_argument(action, 'A')\n\n            # if the action is a single-dash option and takes no\n            # arguments, try to parse more single-dash options out\n            # of the tail of the option string\n            chars = self.prefix_chars\n            if arg_count == 0 and option_string[1] not in chars:\n                action_tuples.append((action, [], option_string))\n                char = option_string[0]\n                option_string = char + explicit_arg[0]\n                new_explicit_arg = explicit_arg[1:] or None\n                optionals_map = self._option_string_actions\n                if option_string in optionals_map:\n                    action = optionals_map[option_string]\n                    explicit_arg = new_explicit_arg\n                else:\n                    msg = _('ignored explicit argument %r')\n                    raise ArgumentError(action, msg % explicit_arg)\n\n            # if the action expect exactly one argument, we've\n            # successfully matched the option; exit the loop\n            elif arg_count == 1:\n                stop = start_index + 1\n                args = [explicit_arg]\n                action_tuples.append((action, args, option_string))\n                break\n\n            # error if a double-dash option did not use the\n            # explicit argument\n            else:\n                msg = _('ignored explicit argument %r')\n                raise ArgumentError(action, msg % explicit_arg)\n\n        # if there is no explicit argument, try to match the\n        # optional's string arguments with the following strings\n        # if successful, exit the loop\n        else:\n            start = start_index + 1\n            selected_patterns = arg_strings_pattern[start:]\n            arg_count = match_argument(action, selected_patterns)\n            stop = start + arg_count\n            args = arg_strings[start:stop]\n            action_tuples.append((action, args, option_string))\n            break\n\n    # add the Optional to the list and return the index at which\n    # the Optional's string args stopped\n    assert action_tuples\n    for action, args, option_string in action_tuples:\n        take_action(action, args, option_string)\n    return stop\n\n# the list of Positionals left to be parsed; this is modified\n# by consume_positionals()\npositionals = self._get_positional_actions()\n\n# function to convert arg_strings into positional actions\ndef consume_positionals(start_index):\n    # match as many Positionals as possible\n    match_partial = self._match_arguments_partial\n    selected_pattern = arg_strings_pattern[start_index:]\n    arg_counts = match_partial(positionals, selected_pattern)\n\n    # slice off the appropriate arg strings for each Positional\n    # and add the Positional and its args to the list\n    for action, arg_count in zip(positionals, arg_counts):\n        args = arg_strings[start_index: start_index + arg_count]\n        start_index += arg_count\n        take_action(action, args)\n\n    # slice off the Positionals that we just parsed and return the\n    # index at which the Positionals' string args stopped\n    positionals[:] = positionals[len(arg_counts):]\n    return start_index\n\n# consume Positionals and Optionals alternately, until we have\n# passed the last option string\nextras = []\nstart_index = 0\nif option_string_indices:\n    max_option_string_index = max(option_string_indices)\nelse:\n    max_option_string_index = -1\nwhile start_index <= max_option_string_index:\n\n    # consume any Positionals preceding the next option\n    next_option_string_index = min([\n        index\n        for index in option_string_indices\n        if index >= start_index])\n    if start_index != next_option_string_index:\n        positionals_end_index = consume_positionals(start_index)\n\n        # only try to parse the next optional if we didn't consume\n        # the option string during the positionals parsing\n        if positionals_end_index > start_index:\n            start_index = positionals_end_index\n            continue\n        else:\n            start_index = positionals_end_index\n\n    # if we consumed all the positionals we could and we're not\n    # at the index of an option string, there were extra arguments\n    if start_index not in option_string_indices:\n        strings = arg_strings[start_index:next_option_string_index]\n        extras.extend(strings)\n        start_index = next_option_string_index\n\n    # consume the next optional and any arguments for it\n    start_index = consume_optional(start_index)\n\n# consume any positionals following the last Optional\nstop_index = consume_positionals(start_index)\n\n# if we didn't consume all the argument strings, there were extras\nextras.extend(arg_strings[stop_index:])\n\n# if we didn't use all the Positional objects, there were too few\n# arg strings supplied.\nif positionals:\n    self.error(_('too few arguments'))\n\n# make sure all required actions were present\nfor action in self._actions:\n    if action.required:\n        if action not in seen_actions:\n            name = _get_action_name(action)\n            self.error(_('argument %s is required') % name)\n\n# make sure all required groups had one option present\nfor group in self._mutually_exclusive_groups:\n    if group.required:\n        for action in group._group_actions:\n            if action in seen_non_default_actions:\n                break\n\n        # if no actions were used, report the error\n        else:\n            names = [_get_action_name(action)\n                     for action in group._group_actions\n                     if action.help is not SUPPRESS]\n            msg = _('one of the arguments %s is required')\n            self.error(msg % ' '.join(names))\n\n# return the updated namespace and the extra arguments\nreturn namespace, extras", "path": "thr\\argparse.py", "repo_name": "clowwindy/PyWebGet", "stars": 60, "license": "other", "language": "python", "size": 1733}
{"docstring": "\"\"\"Return an SSL.Context from self attributes.\"\"\"\n# See http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/442473\n", "func_signal": "def get_context(self):\n", "code": "c = SSL.Context(SSL.SSLv23_METHOD)\nc.use_privatekey_file(self.private_key)\nif self.certificate_chain:\n    c.load_verify_locations(self.certificate_chain)\nc.use_certificate_file(self.certificate)\nreturn c", "path": "web\\wsgiserver\\ssl_pyopenssl.py", "repo_name": "clowwindy/PyWebGet", "stars": 60, "license": "other", "language": "python", "size": 1733}
{"docstring": "# determine short and long option strings\n", "func_signal": "def _get_optional_kwargs(self, *args, **kwargs):\n", "code": "option_strings = []\nlong_option_strings = []\nfor option_string in args:\n    # error on strings that don't start with an appropriate prefix\n    if not option_string[0] in self.prefix_chars:\n        msg = _('invalid option string %r: '\n                'must start with a character %r')\n        tup = option_string, self.prefix_chars\n        raise ValueError(msg % tup)\n\n    # strings starting with two prefix characters are long options\n    option_strings.append(option_string)\n    if option_string[0] in self.prefix_chars:\n        if len(option_string) > 1:\n            if option_string[1] in self.prefix_chars:\n                long_option_strings.append(option_string)\n\n# infer destination, '--foo-bar' -> 'foo_bar' and '-x' -> 'x'\ndest = kwargs.pop('dest', None)\nif dest is None:\n    if long_option_strings:\n        dest_option_string = long_option_strings[0]\n    else:\n        dest_option_string = option_strings[0]\n    dest = dest_option_string.lstrip(self.prefix_chars)\n    if not dest:\n        msg = _('dest= is required for options like %r')\n        raise ValueError(msg % option_string)\n    dest = dest.replace('-', '_')\n\n# return the updated keyword arguments\nreturn dict(kwargs, dest=dest, option_strings=option_strings)", "path": "thr\\argparse.py", "repo_name": "clowwindy/PyWebGet", "stars": 60, "license": "other", "language": "python", "size": 1733}
{"docstring": "# if it's an empty string, it was meant to be a positional\n", "func_signal": "def _parse_optional(self, arg_string):\n", "code": "if not arg_string:\n    return None\n\n# if it doesn't start with a prefix, it was meant to be positional\nif not arg_string[0] in self.prefix_chars:\n    return None\n\n# if the option string is present in the parser, return the action\nif arg_string in self._option_string_actions:\n    action = self._option_string_actions[arg_string]\n    return action, arg_string, None\n\n# if it's just a single character, it was meant to be positional\nif len(arg_string) == 1:\n    return None\n\n# if the option string before the \"=\" is present, return the action\nif '=' in arg_string:\n    option_string, explicit_arg = arg_string.split('=', 1)\n    if option_string in self._option_string_actions:\n        action = self._option_string_actions[option_string]\n        return action, option_string, explicit_arg\n\n# search through all possible prefixes of the option string\n# and all actions in the parser for possible interpretations\noption_tuples = self._get_option_tuples(arg_string)\n\n# if multiple actions match, the option string was ambiguous\nif len(option_tuples) > 1:\n    options = ', '.join([option_string\n        for action, option_string, explicit_arg in option_tuples])\n    tup = arg_string, options\n    self.error(_('ambiguous option: %s could match %s') % tup)\n\n# if exactly one action matched, this segmentation is good,\n# so return the parsed action\nelif len(option_tuples) == 1:\n    option_tuple, = option_tuples\n    return option_tuple\n\n# if it was not found as an option, but it looks like a negative\n# number, it was meant to be positional\n# unless there are negative-number-like options\nif self._negative_number_matcher.match(arg_string):\n    if not self._has_negative_number_optionals:\n        return None\n\n# if it contains a space, it was meant to be a positional\nif ' ' in arg_string:\n    return None\n\n# it was meant to be an optional but there is no such option\n# in this parser (though it might be a valid option in a subparser)\nreturn None, arg_string, None", "path": "thr\\argparse.py", "repo_name": "clowwindy/PyWebGet", "stars": 60, "license": "other", "language": "python", "size": 1733}
{"docstring": "\"\"\"\nOutputs an `Expires` header for `delta` from now. \n`delta` is a `timedelta` object or a number of seconds.\n\"\"\"\n", "func_signal": "def expires(delta):\n", "code": "if isinstance(delta, (int, long)):\n    delta = datetime.timedelta(seconds=delta)\ndate_obj = datetime.datetime.utcnow() + delta\nweb.header('Expires', net.httpdate(date_obj))", "path": "web\\http.py", "repo_name": "clowwindy/PyWebGet", "stars": 60, "license": "other", "language": "python", "size": 1733}
{"docstring": "# determine function from conflict handler string\n", "func_signal": "def _get_handler(self):\n", "code": "handler_func_name = '_handle_conflict_%s' % self.conflict_handler\ntry:\n    return getattr(self, handler_func_name)\nexcept AttributeError:\n    msg = _('invalid conflict_resolution value: %r')\n    raise ValueError(msg % self.conflict_handler)", "path": "thr\\argparse.py", "repo_name": "clowwindy/PyWebGet", "stars": 60, "license": "other", "language": "python", "size": 1733}
{"docstring": "# the special argument \"-\" means sys.std{in,out}\n", "func_signal": "def __call__(self, string):\n", "code": "if string == '-':\n    if 'r' in self._mode:\n        return _sys.stdin\n    elif 'w' in self._mode:\n        return _sys.stdout\n    else:\n        msg = _('argument \"-\" with mode %r' % self._mode)\n        raise ValueError(msg)\n\n# all other arguments are used as file names\nif self._bufsize:\n    return open(string, self._mode, self._bufsize)\nelse:\n    return open(string, self._mode)", "path": "thr\\argparse.py", "repo_name": "clowwindy/PyWebGet", "stars": 60, "license": "other", "language": "python", "size": 1733}
{"docstring": "# parse the text\n", "func_signal": "def generate_code(text, filename, parser=None):\n", "code": "parser = parser or Parser()\nrootnode = parser.parse(text, filename)\n        \n# generate python code from the parse tree\ncode = rootnode.emit(indent=\"\").strip()\nreturn safestr(code)", "path": "web\\template.py", "repo_name": "clowwindy/PyWebGet", "stars": 60, "license": "other", "language": "python", "size": 1733}
{"docstring": "\"\"\"Reads a python expression from the text and returns the expression and remaining text.\n\nexpr -> simple_expr | paren_expr\nsimple_expr -> id extended_expr\nextended_expr -> attr_access | paren_expr extended_expr | ''\nattr_access -> dot id extended_expr\nparen_expr -> [ tokens ] | ( tokens ) | { tokens }\n     \n    >>> read_expr = Parser().read_expr\n    >>> read_expr(\"name\")\n    ($name, '')\n    >>> read_expr(\"a.b and c\")\n    ($a.b, ' and c')\n    >>> read_expr(\"a. b\")\n    ($a, '. b')\n    >>> read_expr(\"name</h1>\")\n    ($name, '</h1>')\n    >>> read_expr(\"(limit)ing\")\n    ($(limit), 'ing')\n    >>> read_expr('a[1, 2][:3].f(1+2, \"weird string[).\", 3 + 4) done.')\n    ($a[1, 2][:3].f(1+2, \"weird string[).\", 3 + 4), ' done.')\n\"\"\"\n", "func_signal": "def read_expr(self, text, escape=True):\n", "code": "def simple_expr():\n    identifier()\n    extended_expr()\n\ndef identifier():\n    tokens.next()\n\ndef extended_expr():\n    lookahead = tokens.lookahead()\n    if lookahead is None:\n        return\n    elif lookahead.value == '.':\n        attr_access()\n    elif lookahead.value in parens:\n        paren_expr()\n        extended_expr()\n    else:\n        return\n\ndef attr_access():\n    from token import NAME # python token constants\n    dot = tokens.lookahead()\n    if tokens.lookahead2().type == NAME:\n        tokens.next() # consume dot\n        identifier()\n        extended_expr()\n\ndef paren_expr():\n    begin = tokens.next().value\n    end = parens[begin]\n    while True:\n        if tokens.lookahead().value in parens:\n            paren_expr()\n        else:\n            t = tokens.next()\n            if t.value == end:\n                break\n    return\n\nparens = {\n    \"(\": \")\",\n    \"[\": \"]\",\n    \"{\": \"}\"\n}\n\ndef get_tokens(text):\n    \"\"\"tokenize text using python tokenizer.\n    Python tokenizer ignores spaces, but they might be important in some cases. \n    This function introduces dummy space tokens when it identifies any ignored space.\n    Each token is a storage object containing type, value, begin and end.\n    \"\"\"\n    readline = iter([text]).next\n    end = None\n    for t in tokenize.generate_tokens(readline):\n        t = storage(type=t[0], value=t[1], begin=t[2], end=t[3])\n        if end is not None and end != t.begin:\n            _, x1 = end\n            _, x2 = t.begin\n            yield storage(type=-1, value=text[x1:x2], begin=end, end=t.begin)\n        end = t.end\n        yield t\n        \nclass BetterIter:\n    \"\"\"Iterator like object with 2 support for 2 look aheads.\"\"\"\n    def __init__(self, items):\n        self.iteritems = iter(items)\n        self.items = []\n        self.position = 0\n        self.current_item = None\n    \n    def lookahead(self):\n        if len(self.items) <= self.position:\n            self.items.append(self._next())\n        return self.items[self.position]\n\n    def _next(self):\n        try:\n            return self.iteritems.next()\n        except StopIteration:\n            return None\n        \n    def lookahead2(self):\n        if len(self.items) <= self.position+1:\n            self.items.append(self._next())\n        return self.items[self.position+1]\n            \n    def next(self):\n        self.current_item = self.lookahead()\n        self.position += 1\n        return self.current_item\n\ntokens = BetterIter(get_tokens(text))\n        \nif tokens.lookahead().value in parens:\n    paren_expr()\nelse:\n    simple_expr()\nrow, col = tokens.current_item.end\nreturn ExpressionNode(text[:col], escape=escape), text[col:]", "path": "web\\template.py", "repo_name": "clowwindy/PyWebGet", "stars": 60, "license": "other", "language": "python", "size": 1733}
{"docstring": "# for everything but PARSER args, strip out '--'\n", "func_signal": "def _get_values(self, action, arg_strings):\n", "code": "if action.nargs not in [PARSER, REMAINDER]:\n    arg_strings = [s for s in arg_strings if s != '--']\n\n# optional argument produces a default when not present\nif not arg_strings and action.nargs == OPTIONAL:\n    if action.option_strings:\n        value = action.const\n    else:\n        value = action.default\n    if isinstance(value, basestring):\n        value = self._get_value(action, value)\n        self._check_value(action, value)\n\n# when nargs='*' on a positional, if there were no command-line\n# args, use the default if it is anything other than None\nelif (not arg_strings and action.nargs == ZERO_OR_MORE and\n      not action.option_strings):\n    if action.default is not None:\n        value = action.default\n    else:\n        value = arg_strings\n    self._check_value(action, value)\n\n# single argument or optional argument produces a single value\nelif len(arg_strings) == 1 and action.nargs in [None, OPTIONAL]:\n    arg_string, = arg_strings\n    value = self._get_value(action, arg_string)\n    self._check_value(action, value)\n\n# REMAINDER arguments convert all values, checking none\nelif action.nargs == REMAINDER:\n    value = [self._get_value(action, v) for v in arg_strings]\n\n# PARSER arguments convert all values, but check only the first\nelif action.nargs == PARSER:\n    value = [self._get_value(action, v) for v in arg_strings]\n    self._check_value(action, value[0])\n\n# all other types of nargs produce a list\nelse:\n    value = [self._get_value(action, v) for v in arg_strings]\n    for v in value:\n        self._check_value(action, v)\n\n# return the converted value\nreturn value", "path": "thr\\argparse.py", "repo_name": "clowwindy/PyWebGet", "stars": 60, "license": "other", "language": "python", "size": 1733}
{"docstring": "\"\"\"Wrap and return the given socket.\"\"\"\n", "func_signal": "def bind(self, sock):\n", "code": "if self.context is None:\n    self.context = self.get_context()\nconn = SSLConnection(self.context, sock)\nself._environ = self.get_environ()\nreturn conn", "path": "web\\wsgiserver\\ssl_pyopenssl.py", "repo_name": "clowwindy/PyWebGet", "stars": 60, "license": "other", "language": "python", "size": 1733}
{"docstring": "# format the indented section\n", "func_signal": "def format_help(self):\n", "code": "if self.parent is not None:\n    self.formatter._indent()\njoin = self.formatter._join_parts\nfor func, args in self.items:\n    func(*args)\nitem_help = join([func(*args) for func, args in self.items])\nif self.parent is not None:\n    self.formatter._dedent()\n\n# return nothing if the section was empty\nif not item_help:\n    return ''\n\n# add the heading if the section was non-empty\nif self.heading is not SUPPRESS and self.heading is not None:\n    current_indent = self.formatter._current_indent\n    heading = '%*s%s:\\n' % (current_indent, '', self.heading)\nelse:\n    heading = ''\n\n# join the section-initial newline, the heading and the help\nreturn join(['\\n', heading, item_help, '\\n'])", "path": "thr\\argparse.py", "repo_name": "clowwindy/PyWebGet", "stars": 60, "license": "other", "language": "python", "size": 1733}
{"docstring": "# set prog from the existing prefix\n", "func_signal": "def add_parser(self, name, **kwargs):\n", "code": "if kwargs.get('prog') is None:\n    kwargs['prog'] = '%s %s' % (self._prog_prefix, name)\n\n# create a pseudo-action to hold the choice help\nif 'help' in kwargs:\n    help = kwargs.pop('help')\n    choice_action = self._ChoicesPseudoAction(name, help)\n    self._choices_actions.append(choice_action)\n\n# create the parser and add it to the map\nparser = self._parser_class(**kwargs)\nself._name_parser_map[name] = parser\nreturn parser", "path": "thr\\argparse.py", "repo_name": "clowwindy/PyWebGet", "stars": 60, "license": "other", "language": "python", "size": 1733}
{"docstring": "# find all options that conflict with this option\n", "func_signal": "def _check_conflict(self, action):\n", "code": "        confl_optionals = []\n        for option_string in action.option_strings:\n            if option_string in self._option_string_actions:\n                confl_optional = self._option_string_actions[option_string]\n                confl_optionals.append((option_string, confl_optional))\n# resolve any conflicts\n        if confl_optionals:\n            conflict_handler = self._get_handler()\n            conflict_handler(action, confl_optionals)", "path": "thr\\argparse.py", "repo_name": "clowwindy/PyWebGet", "stars": 60, "license": "other", "language": "python", "size": 1733}
{"docstring": "# add any missing keyword arguments by checking the container\n", "func_signal": "def __init__(self, container, title=None, description=None, **kwargs):\n", "code": "update = kwargs.setdefault\nupdate('conflict_handler', container.conflict_handler)\nupdate('prefix_chars', container.prefix_chars)\nupdate('argument_default', container.argument_default)\nsuper_init = super(_ArgumentGroup, self).__init__\nsuper_init(description=description, **kwargs)\n\n# group attributes\nself.title = title\nself._group_actions = []\n\n# share most attributes with the container\nself._registries = container._registries\nself._actions = container._actions\nself._option_string_actions = container._option_string_actions\nself._defaults = container._defaults\nself._has_negative_number_optionals = \\\n    container._has_negative_number_optionals", "path": "thr\\argparse.py", "repo_name": "clowwindy/PyWebGet", "stars": 60, "license": "other", "language": "python", "size": 1733}
{"docstring": "\"\"\"\nImagine you're at `/foo?a=1&b=2`. Then `changequery(a=3)` will return\n`/foo?a=3&b=2` -- the same URL but with the arguments you requested\nchanged.\n\"\"\"\n", "func_signal": "def changequery(query=None, **kw):\n", "code": "if query is None:\n    query = web.rawinput(method='get')\nfor k, v in kw.iteritems():\n    if v is None:\n        query.pop(k, None)\n    else:\n        query[k] = v\nout = web.ctx.path\nif query:\n    out += '?' + urlencode(query, doseq=True)\nreturn out", "path": "web\\http.py", "repo_name": "clowwindy/PyWebGet", "stars": 60, "license": "other", "language": "python", "size": 1733}
{"docstring": "# \u6bcf\u4e2a\u5b50\u9875\u9762\u7684\u4e0b\u8f7d\u94fe\u63a5\u3001fileName\u7684\u6b63\u5219\n", "func_signal": "def __init__(self):\n", "code": "self.rDownUrl = re.compile(\n    '<a href=\\\"(.*)\\\"><font color=\\\"blue\\\">\u70b9\u6b64\u4e0b\u8f7d.*</a>')\nself.rFileName = re.compile(r'[^/]+\\.mp3')  # \u4ece\u4e0a\u9762\u7684DownUrl\u5f97\u5230", "path": "5tpsMp3\\5tpsMp3.py", "repo_name": "ywzhaiqi/DownloadHelpers", "stars": 43, "license": "None", "language": "python", "size": 140}
{"docstring": "\"\"\" url \u662f\u9644\u4ef6\u7684\u4e0b\u8f7d\u94fe\u63a5\n\"\"\"\n", "func_signal": "def _addToDownload(url, fileName=None):\n", "code": "url = url.replace('attachment.php?', 'forum.php?mod=attachment&').replace('&amp;', '&')\n\naddToIDM(url)\n# fileName = trimFileName(fileName)\n# addToIDM(url, DOWN_PATH, fileName)\nprint('  \u6dfb\u52a0\u5230IDM: %s' % fileName)", "path": "itpub.py", "repo_name": "ywzhaiqi/DownloadHelpers", "stars": 43, "license": "None", "language": "python", "size": 140}
{"docstring": "\"\"\"\u04b3\u01f7\u0335\n\"\"\"\n", "func_signal": "def checkWefiler(d):\n", "code": "links = d('a[href*=\"www.wefiler.com/#download\"]').items()\nurls = []\nfor a in links:\n    url = a.attr['href']\n    url.replace('http://http//', 'http//')\n    urls.append(url)\nreturn urls", "path": "weiphone.py", "repo_name": "ywzhaiqi/DownloadHelpers", "stars": 43, "license": "None", "language": "python", "size": 140}
{"docstring": "\"\"\"\n\"\"\"\n", "func_signal": "def download(threadUrl):\n", "code": "d = PyQuery(url=threadUrl, parser='soup')\nlinks = d('a[href^=\"job.php?action=download&aid=\"]')\n\n# \u0221 verify \u05b5\ntmp = d('script:contains(\"var verifyhash =\")').text()\nverify = re.search(r\"var verifyhash = '(.*?)'\", tmp).group(1)\n\ntotal = len(links)\nd.make_links_absolute()\nfor i, e in enumerate(links.items(), start=1):\n    filename = e.text()\n    print('%s/%s %s' % (i, total, filename))\n\n    if not os.path.exists(os.path.join(SAVE_PATH, filename)):\n        params = urlencode(\n            {'check': 1, 'verify': verify, 'nowtime': int(time.time() * 1000)})\n        url = '%s?%s' % (e.attr['href'], params)\n\n        print('  fetch: ' + url)\n        downDoc = PyQuery(url, headers=headers)\n        # 0\u01f5\u0635\u38ec1\u01b6\u0635\n        downUrl = BASE_URL + downDoc('a[href^=\"remotedown.php\"]').eq(1).attr('href')\n        addToIDM(downUrl, SAVE_PATH, filename)\n        time.sleep(1.5)\n\nwefiler_urls = checkWefiler(d)\nif wefiler_urls:\n    print(wefiler_urls)", "path": "weiphone.py", "repo_name": "ywzhaiqi/DownloadHelpers", "stars": 43, "license": "None", "language": "python", "size": 140}
{"docstring": "# urlstrs = '''\n#     http://bbs.weiphone.com/read-htm-tid-1726790.html\n# '''\n\n", "func_signal": "def main():\n", "code": "parser = argparse.ArgumentParser(description='weiphone iPad \u0534 ')\nparser.add_argument('urls', metavar='URL', nargs='+', help='\u0333\u04bb\u04f5\u05b7')\n\nargs = parser.parse_args()\nif args.urls:\n    # urls = re.findall(r'http://[^\\s]+', urlstrs)\n    for url in args.urls:\n        print(url)\n        download(url)", "path": "weiphone.py", "repo_name": "ywzhaiqi/DownloadHelpers", "stars": 43, "license": "None", "language": "python", "size": 140}
{"docstring": "\"\"\" \u89e3\u6790\u4e00\u4e2a\u5206\u9875\u7684 thread\n\"\"\"\n", "func_signal": "def _parseForum(forumUrl, downed=False):\n", "code": "d = pq(forumUrl)\nthreads = []\nitems = d('img[title=\"\u9644\u4ef6\"]').siblings('a.xst').items()\nfor item in items:\n    url = BASE_URL + item.attr('href')\n    name = item.text()\n    threadId = _getThreadId(url)\n\n    thread = {'_id': threadId, 'name': name, 'url': url, 'downed': downed}\n    threads.append(thread)\n\nprint('\u5f97\u5230 %d \u4e2athread' % len(threads))\nreturn threads", "path": "itpub.py", "repo_name": "ywzhaiqi/DownloadHelpers", "stars": 43, "license": "None", "language": "python", "size": 140}
{"docstring": "#\u53d6\u51fa\u8bb0\u5f55\u5217\u8868\n", "func_signal": "def main():\n", "code": "try:\n    with open(TO_JOIN_PATH) as f:\n        videos = json.load(f)\nexcept:\n    print(\"\u6ca1\u6709\u8bb0\u5f55\u5217\u8868\")\n    return\n\nif not videos:\n    return\n\n#\u4e00\u4e2a\u4e2a\u5730\u68c0\u9a8c\u4e0b\u8f7d\u5b8c\u6210\u6ca1\nfor videoPaths in videos:\n    #\u53ea\u8981\u6709\u4e00\u4e2a\u6ca1\u4e0b\u8f7d\u597d\u5c31\u4e0d\u5408\u5e76\n    isFinish = True\n    for path in videoPaths:\n        if not os.path.exists(path):\n            isFinish = False\n            break\n\n    if isFinish:\n        success = joinVideo(videoPaths)\n        if success:\n            videos.remove(videoPaths)\n\n#\u5982\u679c\u8fd8\u6709videos\u5219\u5b58\u5165\u8fdb\u5ea6\u6587\u4ef6\uff0c\u5426\u5219\u5220\u9664\u8fdb\u5ea6\u6587\u4ef6\nif videos:\n    with open(TO_JOIN_PATH, 'w') as f:\n        json.dump(videos, f, indent=4, ensure_ascii=False)\nelse:\n    if os.path.exists(TO_JOIN_PATH):\n        os.remove(TO_JOIN_PATH)", "path": "youku\\youku_join.py", "repo_name": "ywzhaiqi/DownloadHelpers", "stars": 43, "license": "None", "language": "python", "size": 140}
{"docstring": "\"\"\" \u4ece\u5176\u5b83\u7f51\u7ad9\u83b7\u53d6\u94fe\u63a5 \"\"\"\n", "func_signal": "def parse_other_page(url):\n", "code": "print('\u6b63\u5728\u4ece gdajie.com \u83b7\u53d6\u94fe\u63a5')\nurl = url.replace('www.verycd.com', 'www.verycd.gdajie.com')\nd = PyQuery(url)\nemuleFile = d('#emuleFile a[href^=\"ed2k://\"]')\nif emuleFile:\n    items = [(e.attr('href'), e.text()) for e in emuleFile.items()]\n    return items\nelse:\n    #http://verycdfetch.duapp.com/topics/132012/\n    print('\u90fd\u6ca1\u6709\u94fe\u63a5')\n    pass", "path": "verycd\\verycd.py", "repo_name": "ywzhaiqi/DownloadHelpers", "stars": 43, "license": "None", "language": "python", "size": 140}
{"docstring": "\"\"\" \u5408\u5e76\u4e00\u4e2a\u89c6\u9891\n\"\"\"\n", "func_signal": "def joinVideo(videoPaths):\n", "code": "if len(videoPaths) == 0:\n    print(\"videoPaths size=0\")\n    return\n\nallName = videoPaths[0].replace(\"-0001\", \"\")  # \u5408\u5e76\u540e\u7684\u540d\u79f0\nfileExt = os.path.splitext(videoPaths[0])[1]  # \u540e\u7f00\u540d\n\nif fileExt == '.mp4':\n    command = [\"D:\\\u7f51\u7edc\u5de5\u5177\\\u7855\u9f20\\mp4box.exe\"]\n    for videoPath in videoPaths:\n        command.extend([\"-cat\", videoPath])\n    command.extend([\"-new\", allName])\nelif fileExt == '.flv':\n    command = [\"D:\\\u7f51\u7edc\u5de5\u5177\\\u7855\u9f20\\FlvBind.exe\"]\n    command.append(allName)\n    command.extend(videoPaths)\nelse:\n    print('\u683c\u5f0f\u4e0d\u652f\u6301')\n    return False\n\nreturnCode = subprocess.call(command)\nif returnCode == 0:\n    #\u79fb\u9664\u7247\u6bb5\u6587\u4ef6\n    for path in videoPaths:\n        os.remove(path)\n        print(\"\u5220\u9664\u6587\u4ef6: \" + path)\n    return True", "path": "youku\\youku_join.py", "repo_name": "ywzhaiqi/DownloadHelpers", "stars": 43, "license": "None", "language": "python", "size": 140}
{"docstring": "\"\"\" find out the real download link from download page.\neg. we can get the download link 'http://180j-d.ysts8.com:8000/\u4eba\u7269\u7eaa\u5b9e/\u7ae5\u5e74/001.mp3?\n1251746750178x1356330062x1251747362932-3492f04cf54428055a110a176297d95a' from\n'http://www.5tps.com/down/8297_52_1_1.html'\n\"\"\"\n", "func_signal": "def getDownUrl(self, url):\n", "code": "content = getHtml(url, 'gbk')\ndownUrl = self.rDownUrl.search(content).group(1)\n#\u4eceurl\u4e2d\u63d0\u53d6\u51fafileName\nfileName = self.rFileName.search(downUrl).group()\nfileName = fileName.replace('%20', ' ')\n\nreturn (downUrl, fileName)", "path": "5tpsMp3\\5tpsMp3.py", "repo_name": "ywzhaiqi/DownloadHelpers", "stars": 43, "license": "None", "language": "python", "size": 140}
{"docstring": "# http://www.itpub.net/forum.php?mod=viewthread&tid=1596873&extra=page%3D1%26filter%3Dtypeid%26typeid%3D385%26typeid%3D385&page=2\n# http://www.itpub.net/thread-1761255-1-1.html\n", "func_signal": "def _getThreadId(threadUrl):\n", "code": "o = urlparse(threadUrl)\nif 'mod=viewthread&tid=' in o.query:\n    querys = parse_qs(o.query)\n    threadId = querys.get('tid')[0]\n    # pageNum = querys.get('page')\n    # pageNum = pageNum if pageNum else 1  #\u5982\u679c\u6ca1\u6709\u9ed8\u8ba4\u7b2c\u4e00\u9875\nelif '/thread-' in o.path:\n    pathList = o.path.split('-')\n    threadId = pathList[1]\n    # pageNum = pathList[2]\nelse:\n    print('Error: getThreadId from url error, ' + threadUrl)\nreturn int(threadId)", "path": "itpub.py", "repo_name": "ywzhaiqi/DownloadHelpers", "stars": 43, "license": "None", "language": "python", "size": 140}
{"docstring": "\"\"\" return [(url, name)...] \"\"\"\n# \u4e0d\u77e5\u9053\u662f\u5426\u9700\u8981 urllib.parse.quote(videourl)\n", "func_signal": "def parse(self, url, format='high'):\n", "code": "self.url = \"http://www.flvcd.com/parse.php?kw=\" + url + '&format=' + format\nhtml = self.fetchHtml(self.url)\n\n# \u5148\u627e\u51fa\u4e00\u5927\u5757\u533a\u57df\nm = self.rContent.search(html)\ncontent = m.group(1)\n# \u8fdb\u4e00\u6b65\u627e\u51fa\u6240\u6709\u7684name\u3001url\nresult = self.rNameAndUrl.findall(content)\n\nsize = len(result)\nprint('\u89c6\u9891\u5171\u6709 {:d}\u4e2a\u7247\u6bb5, \u4fdd\u5b58\u7684\u4f4d\u7f6e: {}'.format(int(size / 2), SAVE_PATH))\noutList = []\nif size > 0:\n    for i in range(0, size, 2):\n        name = result[i]  # \u6ca1\u6709\u540e\u7f00 e.g. \u300cZEALER \u51fa\u54c1\u300d \u534e\u4e3a\u8363\u8000\u56db\u6838 \u6d4b\u8bc4-0001\n        url = result[i + 1]\n        # \u53d6\u5f97\u540e\u7f00\u540d\n        fileExt = self.rFileExt.search(url).group(1)\n        name += '.' + fileExt\n\n        outList.append((url, name))\n    return outList\nelse:\n    print(\"URL Not Found\")", "path": "flvcd.py", "repo_name": "ywzhaiqi/DownloadHelpers", "stars": 43, "license": "None", "language": "python", "size": 140}
{"docstring": "# \u4e0d\u77e5\u9053\u662f\u5426\u9700\u8981 urllib.parse.quote(videourl)\n", "func_signal": "def parseFlvcd(url, format='hight'):\n", "code": "print('\u76ee\u6807\uff1a{}\uff0c\u6e05\u6670\u5ea6\uff1a{}'.format(url, format))\nurl = \"http://www.flvcd.com/parse.php?kw=\" + url + '&format=' + format\nd = PyQuery(url)\n\nfilename = d('input[name=\"filename\"]').attr('value')\nfilename = re.sub('[\\\\\\|\\:\\*\\\"\\?\\<\\>]', \"_\", filename)\n\nurltxt = d('input[name=\"inf\"]').attr('value')\nurl = urltxt.strip()\naddToIDM(url, SAVE_PATH, filename)\n# for url in urltxt.split('\\r\\n'):\n#     url = url.strip()\n#     if url:\n#         addToIDM(url)", "path": "flvcd.py", "repo_name": "ywzhaiqi/DownloadHelpers", "stars": 43, "license": "None", "language": "python", "size": 140}
{"docstring": "#\u5f3a\u884c\u4e2d\u65ad\u524d\u4fdd\u5b58\u89e3\u6790\u8fdb\u5ea6\n", "func_signal": "def exitApp(signum, frame):\n", "code": "o5tps['start_pos'] = cur_pos\no5tps['down_size'] = o5tps['total_size'] - o5tps['start_pos']\nwith open(progress_file, 'w') as f:\n    json.dump(o5tps, f, sort_keys=True, indent=4)\nprint('\u4fdd\u5b58\u8fdb\u5ea6\u6210\u529f\uff0c\u81ea\u52a8\u9000\u51fa\uff01')\nsys.exit()", "path": "5tpsMp3\\5tpsMp3.py", "repo_name": "ywzhaiqi/DownloadHelpers", "stars": 43, "license": "None", "language": "python", "size": 140}
{"docstring": "# \u5982\u679c url = http://www.itpub.net/thread-1608864-2-1.html \u8fd9\u662f\u7b2c\u4e8c\u9875\n", "func_signal": "def _createNextPageUrl(url):\n", "code": "if '/forum.php?mod=viewthread' in url:\n    if '&page=' in url:\n        bUrl, pageNum = url.split('&page=')\n        pageNum = int(pageNum)\n    else:\n        bUrl = url\n        pageNum = 1\n    nextUrl = '%s&page=%d' % (bUrl, (pageNum + 1))\n# \u6216 http://www.itpub.net/forum.php?mod=viewthread&tid=512296&page=2\nelif '/thread-' in url:\n    m = re.match(r'(.*)-(\\d)-(\\d\\.html)', url)\n    nextNum = int(m.group(2)) + 1\n    nextUrl = '%s-%d-%s' % (m.group(1), nextNum, m.group(3))\nelse:\n    print('\u751f\u6210\u4e0b\u4e00\u9875\u7f51\u5740\u9519\u8bef: ' + url)\n    return\n\nreturn nextUrl", "path": "itpub.py", "repo_name": "ywzhaiqi/DownloadHelpers", "stars": 43, "license": "None", "language": "python", "size": 140}
{"docstring": "\"\"\" \u4e0b\u8f7d\u7cbe\u534e\u4e66\u7c4d \"\"\"\n", "func_signal": "def downDigestBooks(limit=10):\n", "code": "books = db.itpub.find({'$or': [{'downed': {'$exists': False}}, {'downed': False}]}).limit(limit)\nfor book in books:\n    parseAndDownOneBook(book['url'])\n    setOneBookDowned(book['url'])", "path": "itpub.py", "repo_name": "ywzhaiqi/DownloadHelpers", "stars": 43, "license": "None", "language": "python", "size": 140}
{"docstring": "# ken777\u0228.\u6fab\u0237\u052dPDF\u03f5\u0437\n", "func_signal": "def download_from_weiphone():\n", "code": "startURL = 'http://bbs.weiphone.com/read-htm-tid-847578.html'\n\nsys.path.insert(0, \"..\")\nimport weiphone\nweiphone.download(startURL)", "path": "Ken777\\Ken777.py", "repo_name": "ywzhaiqi/DownloadHelpers", "stars": 43, "license": "None", "language": "python", "size": 140}
{"docstring": "#\u68c0\u67e5\u662f\u5426\u6709\u8fdb\u5ea6\u6587\u4ef6\n", "func_signal": "def getProgress():\n", "code": "try:\n    global o5tps\n    f = open(progress_file)\n    o5tps = json.load(f)\n    f.close()\n\n    print('''\u8bfb\u53d6\u8fdb\u5ea6, \u76ee\u6807: {title}, {start_url}\n          \u5171\u6709{total_size:d}\u4e2a,\u672c\u6b21\u5f00\u59cb:\u7b2c{start_pos:d}\u4e2a,\u8fd8\u6709{down_size:d}\u4e2a'''\n          .format(**o5tps))\n    return True\nexcept:\n    return False", "path": "5tpsMp3\\5tpsMp3.py", "repo_name": "ywzhaiqi/DownloadHelpers", "stars": 43, "license": "None", "language": "python", "size": 140}
{"docstring": "#\u5148\u53d6\u51fa\u5b58\u5728\u7684\u5217\u8868\n", "func_signal": "def saveData(allList):\n", "code": "try:\n    with open(TO_JOIN_PATH) as f:\n        videos = json.load(f)\nexcept:\n    videos = []\n\n#\u5f53\u7247\u6bb5\u6570\u91cf>=2\u624d\u9700\u8981\nif len(allList) >= 2:\n    #\u53d6\u5f97\u5f53\u524d\u6240\u6709\u7684\u7247\u6bb5\u5217\u8868\n    filePaths = []\n    for url, name in allList:\n        path = os.path.join(SAVE_PATH, name)\n        filePaths.append(path)\n\n    #\u6dfb\u52a0\u5230videos\n    videos.append(filePaths)\n\n    #\u91cd\u5199\u8bb0\u5f55\u6587\u4ef6\n    with open(TO_JOIN_PATH, \"w\") as f:\n        json.dump(videos, f, indent=4, ensure_ascii=False)\n        print('-' * 40)\n        print(\"\u8981\u5408\u5e76\u7684\u5217\u8868\u4fdd\u5b58\u5230 %s\" % TO_JOIN_PATH)", "path": "flvcd.py", "repo_name": "ywzhaiqi/DownloadHelpers", "stars": 43, "license": "None", "language": "python", "size": 140}
{"docstring": "\"\"\"\nparse out download page from start url.\neg. we can get 'http://www.5tps.com/down/8297_52_1_1.html' from 'http://www.5tps.com/html/8297.html'\n\"\"\"\n", "func_signal": "def parseStartUrl(self, start_url):\n", "code": "out_dict = {'start_url': start_url}\nhtml = getHtml(start_url, 'gbk')\n\n#\u6807\u9898\ntitleLine = r1(r'<TITLE>([^<>]*)</TITLE>', html)\ntitle = titleLine.split(\" \")[1]\nout_dict['title'] = title\n\n#\u5b50\u94fe\u63a5\nrUrl = re.compile(r'href=[\\\"\\'](/down/.*?html)')\nitemUrls = rUrl.findall(html)\n#/down/8297_52_1_1.html --> http://www.5tps.com/down/8297_52_1_1.html\nout_dict['urls'] = [self.BASE_URL + url for url in itemUrls]\n\n#\u5185\u5bb9\u7b80\u4ecb\n# xpath_contetn = './/*[@id='full']/div/div/ul/p/span' #\u5185\u5bb9\u7b80\u4ecb\ncontent = re.search(r'<h4>.+?</p>', html, re.DOTALL).group()\n#\u53bb\u6389html\u6807\u8bb0\nout_dict['content'] = htmlToText(content)\n\nout_dict['total_size'] = len(out_dict['urls'])\nreturn out_dict", "path": "5tpsMp3\\5tpsMp3.py", "repo_name": "ywzhaiqi/DownloadHelpers", "stars": 43, "license": "None", "language": "python", "size": 140}
{"docstring": "\"\"\"\nHandles authentication with Twitter's OAuth authentication platform.\nIf the authenticated user does not yet have a ``User`` registered,\nthey are redirected to the Twitter registration backend.\n\nThis is what you'll get back from Twitter. Note that it includes the\nuser's user_id and screen_name.\n\n{\n    'oauth_token_secret': 'IcJXPiJh8be3BjDWW50uCY31chyhsMHEhqJVsphC3M',\n    'user_id': '120889797',\n    'oauth_token': '120889797-H5zNnM3qE0iFoTTpNEHIz3noL9FKzXiOxwtnyVOD',\n    'screen_name': 'heyismysiteup'\n}\n\n\"\"\"\n", "func_signal": "def authenticate(self, request, **kwargs):\n", "code": "token = oauth.Token(\n    request.session['request_token']['oauth_token'],\n    request.session['request_token']['oauth_token_secret']\n)\nclient = oauth.Client(self.consumer, token)\nresponse, content = client.request(self.access_token_url, 'GET')\nif response['status'] != '200':\n    raise Exception('Invalid response from Twitter.')\n    return (False, None)\nself.access_token = dict(urlparse.parse_qsl(content))\nself.identifier = self.access_token['user_id']\nreturn authenticate(identifier=self.identifier)", "path": "social_registration\\backends\\twitter\\__init__.py", "repo_name": "bryanveloso/hello-social-registration", "stars": 35, "license": "bsd-3-clause", "language": "python", "size": 140}
{"docstring": "\"\"\"\n``authenticate()`` returned ``None``, so the user is new. Let's send\nthem to registration to set a username and password.\n\n\"\"\"\n", "func_signal": "def create_user(self, request, user, **kwargs):\n", "code": "request.session['twitter_profile'] = twitter.Twitter().users.show(id=self.identifier)\nrequest.session['twitter_identifier'] = self.identifier\nreturn redirect('twitter-setup')", "path": "social_registration\\backends\\twitter\\__init__.py", "repo_name": "bryanveloso/hello-social-registration", "stars": 35, "license": "bsd-3-clause", "language": "python", "size": 140}
{"docstring": "# URLs\n", "func_signal": "def __init__(self, *args, **kwargs):\n", "code": "self.access_token_url = 'https://graph.facebook.com/oauth/access_token'\nself.authorize_url = 'https://graph.facebook.com/oauth/authorize'\nself.graph_url = 'https://graph.facebook.com/me'\n\n# Instance Variables\nself.access_token = None\nself.parameters = {\n    'client_id': settings.FACEBOOK_APPLICATION_ID,\n    'scope': 'email,user_birthday,publish_stream'\n}\nself.profile = None\nself.service = 'facebook'\nsuper(AccountBackend, self).__init__(*args, **kwargs)", "path": "social_registration\\backends\\facebook\\__init__.py", "repo_name": "bryanveloso/hello-social-registration", "stars": 35, "license": "bsd-3-clause", "language": "python", "size": 140}
{"docstring": "\"\"\"\n``authenticate()`` returned ``None``, so the user is new. Let's send\nthem to registration to set a username and password.\n\n\"\"\"\n", "func_signal": "def create_user(self, request, user, **kwargs):\n", "code": "request.session['facebook_access_token'] = self.access_token\nrequest.session['facebook_identifier'] = self.profile['id']\nrequest.session['facebook_profile'] = self.profile\nreturn redirect('facebook-setup')", "path": "social_registration\\backends\\facebook\\__init__.py", "repo_name": "bryanveloso/hello-social-registration", "stars": 35, "license": "bsd-3-clause", "language": "python", "size": 140}
{"docstring": "\"\"\"\nHandles the preliminary steps between the site and Twitter's OAuth\nplatform. A request token is sent and validated to receive an\nauthorization URL which the user is then redirected to.\n\n\"\"\"\n", "func_signal": "def prepare(self, request, **kwargs):\n", "code": "client = oauth.Client(self.consumer)\nresponse, content = client.request(self.request_token_url, 'GET')\nif response['status'] != '200':\n    raise Exception('Invalid response from Twitter.')\nrequest.session['request_token'] = dict(urlparse.parse_qsl(content))\nurl = '%s?oauth_token=%s' % (self.authenticate_url, request.session['request_token']['oauth_token'])\nreturn url", "path": "social_registration\\backends\\twitter\\__init__.py", "repo_name": "bryanveloso/hello-social-registration", "stars": 35, "license": "bsd-3-clause", "language": "python", "size": 140}
{"docstring": "\"\"\"\nBreaks the link between a user and a Twitter account they had\npreviously authenticated with.\n\n\"\"\"\n", "func_signal": "def deauthenticate(self, request, **kwargs):\n", "code": "association = Association.objects.get(user=request.user, service=self.service)\nassociation.is_active = False\nreturn True", "path": "social_registration\\backends\\twitter\\__init__.py", "repo_name": "bryanveloso/hello-social-registration", "stars": 35, "license": "bsd-3-clause", "language": "python", "size": 140}
{"docstring": "\"\"\"\nGiven a username and email, create a new user, an accompanying profile\nand a relation to the Twitter user they authenticated with.\n\n\"\"\"\n# Let's alias some of these session variables.\n", "func_signal": "def register(self, request, **kwargs):\n", "code": "identifier = request.session['twitter_identifier']\nprofile = request.session['twitter_profile']\n\nusername, email = kwargs['username'], kwargs['email']\nuser = User.objects.create_user(username, email)\nuser.set_unusable_password()\nuser.save()\n\nassociation = Association(\n    access_token={\n        'oauth_token': request.session['request_token']['oauth_token'],\n        'oauth_token_secret': request.session['request_token']['oauth_token_secret']\n    },\n    avatar=profile['profile_image_url'],\n    identifier=identifier,\n    is_active=True,\n    profile_url=self.profile_url % self.access_token['name'],\n    service='twitter',\n    user=user\n)\nassociation.save()\nsignals.user_registered.send(sender=self.__class__, user=user, request=request)\nreturn user", "path": "social_registration\\backends\\twitter\\__init__.py", "repo_name": "bryanveloso/hello-social-registration", "stars": 35, "license": "bsd-3-clause", "language": "python", "size": 140}
{"docstring": "\"\"\"\n``authenticate()`` worked, so we have an existing user trying to\nconnect, but they don't have an Association yet, so let's create one.\nWe don't need to log them in though since they've already done so.\n\n\"\"\"\n", "func_signal": "def link_user(self, request, user, **kwargs):\n", "code": "profile = twitter.Twitter().users.show(id=self.identifier)\nassociation = Association.get_or_create(\n    access_token={\n        'oauth_token': self.access_token['oauth_token'],\n        'oauth_token_secret': self.access_token['oauth_token_secret']\n    },\n    avatar=profile['profile_image_url'],\n    identifier=self.identifier,\n    is_active=True,\n    profile_url=self.profile_url % self.access_token['name'],\n    service=self.service,\n    user=user\n)\nassociation.save()\nmessages.success(request, 'Your Twitter account has been linked with your Hello! Ranking account.')\nreturn redirect('edit-profile')", "path": "social_registration\\backends\\twitter\\__init__.py", "repo_name": "bryanveloso/hello-social-registration", "stars": 35, "license": "bsd-3-clause", "language": "python", "size": 140}
{"docstring": "\"\"\"\nA wrapper around ``django.contrib.auth.views.logout_then_login()`` that\nincludes an additional call to flush the user's session upon logging out\nbefore redirecting to the login page.\n\n\"\"\"\n", "func_signal": "def logout_then_login(request, login_url=None):\n", "code": "from django.contrib.auth import logout_then_login\n\nif not login_url:\n    login_url = settings.LOGIN_URL\nrequest.session.clear()\nmessages.success(request, 'You have been logged out. You may log in again below.')\nreturn logout_then_login(request, login_url)", "path": "social_registration\\views.py", "repo_name": "bryanveloso/hello-social-registration", "stars": 35, "license": "bsd-3-clause", "language": "python", "size": 140}
{"docstring": "\"\"\"\nHandles a redirection to the returned URL from the given backend for use\nbefore the user authenticates with an external service. For example, an\nOAuth service provider will require this step grant the application a\n\"request token\" needed to proceed further.\n\n\"\"\"\n", "func_signal": "def prepare(request, backend, **kwargs):\n", "code": "backend = get_backend(backend)\nreturn redirect(backend.prepare(request))", "path": "social_registration\\views.py", "repo_name": "bryanveloso/hello-social-registration", "stars": 35, "license": "bsd-3-clause", "language": "python", "size": 140}
{"docstring": "\"\"\"\nHandles a user wishing to deactivate a connection between an\nauthentication backend and their database account.\n\n\"\"\"\n", "func_signal": "def deauthenticate(request, backend, **kwargs):\n", "code": "backend = get_backend(backend)\nsuccess = backend.deauthenticate(request)\nreturn redirect('edit-profile')", "path": "social_registration\\views.py", "repo_name": "bryanveloso/hello-social-registration", "stars": 35, "license": "bsd-3-clause", "language": "python", "size": 140}
{"docstring": "\"\"\"\nGiven a username and email, create a new user, an accompanying profile\nand a relation to the Facebook user they authenticated with.\n\n\"\"\"\n# Let's alias some of these session variables.\n", "func_signal": "def register(self, request, **kwargs):\n", "code": "access_token = request.session['facebook_access_token']\nidentifier = request.session['facebook_identifier']\nprofile = request.session['facebook_profile']\n\nusername, email = kwargs['username'], kwargs['email']\nuser = User.objects.create_user(username, email)\nuser.first_name = profile['first_name']\nuser.last_name = profile['last_name']\nuser.set_unusable_password()\nuser.save()\n\nassociation = Association(\n    access_token=access_token,\n    avatar=request.facebook.graph.get_connections(identifier, 'picture'),\n    identifier=identifier,\n    is_active=True,\n    profile_url=profile['link'],\n    service='facebook',\n    user=user\n)\nassociation.save()\nsignals.user_registered.send(sender=self.__class__, user=user, request=request)\nreturn user", "path": "social_registration\\backends\\facebook\\__init__.py", "repo_name": "bryanveloso/hello-social-registration", "stars": 35, "license": "bsd-3-clause", "language": "python", "size": 140}
{"docstring": "\"\"\"\n``authenticate()`` worked, so we have an existing user trying to\nconnect, but they don't have an Association yet, so let's create one.\nWe don't need to log them in though since they've already done so.\n\n\"\"\"\n", "func_signal": "def link_user(self, request, user, **kwargs):\n", "code": "association = Association.get_or_create(\n    access_token=self.access_token,\n    avatar=request.facebook.graph.get_connections(self.profile['id'], 'picture'),\n    identifier=self.profile['id'],\n    is_active=True,\n    profile_url=self.profile['link'],\n    service=self.service,\n    user=request.user\n)\nassociation.save()\nmessages.success(request, 'Your Facebook account has been linked with your Hello! Ranking account.')\nreturn redirect('edit-profile')", "path": "social_registration\\backends\\facebook\\__init__.py", "repo_name": "bryanveloso/hello-social-registration", "stars": 35, "license": "bsd-3-clause", "language": "python", "size": 140}
{"docstring": "\"\"\"\nHandles authentication with Facebook Platform. If the authenticated\nuser does not yet have a ``User`` registered, they are redirected to\nthe Facebook registration backend.\n\n\"\"\"\n", "func_signal": "def authenticate(self, request, **kwargs):\n", "code": "self.parameters['client_secret'] = settings.FACEBOOK_SECRET_KEY\nself.parameters['code'] = request.GET.get('code')\nself.parameters['redirect_uri'] = request.build_absolute_uri(request.path)\n\nresponse = urlparse.parse_qs(urllib.urlopen('%s?%s' % (self.access_token_url, urllib.urlencode(self.parameters))).read())\nself.access_token = response['access_token'][-1]\nself.profile = json.load(urllib.urlopen('%s?%s' % (self.graph_url, urllib.urlencode({'access_token': self.access_token}))))\nreturn authenticate(identifier=self.profile['id'])", "path": "social_registration\\backends\\facebook\\__init__.py", "repo_name": "bryanveloso/hello-social-registration", "stars": 35, "license": "bsd-3-clause", "language": "python", "size": 140}
{"docstring": "\"\"\"\nAlthough Facebook Platform doesn't use three-legged OAuth process, the\n``authenticate()`` method cannot return anything other than a user. So\nall the logic to grab an authentication code for use in the method is\ndone here.\n\n\"\"\"\n", "func_signal": "def prepare(self, request, **kwargs):\n", "code": "verification_code = request.GET.get('code', None)\nif verification_code is None:\n    self.parameters['redirect_uri'] = request.build_absolute_uri(reverse('facebook-authentication'))\n    return '%s?%s' % (self.authorize_url, urllib.urlencode(self.parameters))\nreturn reverse('facebook-authentication')", "path": "social_registration\\backends\\facebook\\__init__.py", "repo_name": "bryanveloso/hello-social-registration", "stars": 35, "license": "bsd-3-clause", "language": "python", "size": 140}
{"docstring": "\"\"\"\nBreaks the link between a user and a Facebook account they had\npreviously authenticated with.\n\n\"\"\"\n", "func_signal": "def deauthenticate(self, request, **kwargs):\n", "code": "association = Association.objects.get(user=request.user, service=self.service)\nassociation.is_active = False\nreturn True", "path": "social_registration\\backends\\facebook\\__init__.py", "repo_name": "bryanveloso/hello-social-registration", "stars": 35, "license": "bsd-3-clause", "language": "python", "size": 140}
{"docstring": "\"\"\"\n``authenticate()`` worked and the user has an ``Association`` already,\nso let's fetch it and log them in.\n\n\"\"\"\n", "func_signal": "def grant_user(self, request, user, **kwargs):\n", "code": "try:\n    twitter_profile = twitter.Twitter().users.show(id=self.access_token['user_id'])\n    avatar = twitter_profile['profile_image_url']\nexcept:\n    avatar = ''\nassociation = Association.objects.get(identifier=self.access_token['user_id'], service=self.service)\nassociation.access_token = {\n    'oauth_token': self.access_token['oauth_token'],\n    'oauth_token_secret': self.access_token['oauth_token_secret']\n}\nassociation.avatar = avatar\nassociation.save()\nif user.is_active:\n    login(request, user)\n    messages.success(request, 'Welcome back! You have been logged in!')\n    return redirect('site-home')", "path": "social_registration\\backends\\twitter\\__init__.py", "repo_name": "bryanveloso/hello-social-registration", "stars": 35, "license": "bsd-3-clause", "language": "python", "size": 140}
{"docstring": "# URLs\n", "func_signal": "def __init__(self, *args, **kwargs):\n", "code": "self.access_token_url = 'https://api.twitter.com/oauth/access_token'\nself.authenticate_url = 'http://api.twitter.com/oauth/authenticate'\nself.request_token_url = 'https://api.twitter.com/oauth/request_token'\nself.profile_url = 'http://twitter.com/%s'\n\n# Instance Variables\nself.access_token = None\nself.consumer = oauth.Consumer(settings.TWITTER_KEY, settings.TWITTER_SECRET)\nself.identifier = None\nself.service = 'twitter'\nsuper(AccountBackend, self).__init__(*args, **kwargs)", "path": "social_registration\\backends\\twitter\\__init__.py", "repo_name": "bryanveloso/hello-social-registration", "stars": 35, "license": "bsd-3-clause", "language": "python", "size": 140}
{"docstring": "\"\"\"\n``authenticate()`` worked and the user has an ``Association`` already,\nso let's fetch it and log them in.\n\n\"\"\"\n", "func_signal": "def grant_user(self, request, user, **kwargs):\n", "code": "association = Association.objects.get(user=user, service=self.service)\nassociation.access_token = self.access_token\nassociation.avatar = request.facebook.graph.get_connections(self.profile['id'], 'picture')\nassociation.profile_url = self.profile['link']\nassociation.save()\nif user.is_active:\n    login(request, user)\n    messages.success(request, 'Welcome back! You have been logged in!')\n    return redirect('site-home')", "path": "social_registration\\backends\\facebook\\__init__.py", "repo_name": "bryanveloso/hello-social-registration", "stars": 35, "license": "bsd-3-clause", "language": "python", "size": 140}
{"docstring": "\"\"\"\nHandles a user returned from the given backend's authenticate() method.\nDepending on what is returned a user will either be logged in, \"linked\" or\ncreated.\n\n\"\"\"\n", "func_signal": "def authenticate(request, backend, **kwargs):\n", "code": "backend = get_backend(backend)\nuser = backend.authenticate(request)\n\nif user is not None:\n    return backend.grant_user(request, user)\nelif request.user.is_authenticated():\n    return backend.link_user(request, user)\nelse:\n    return backend.create_user(request, user)", "path": "social_registration\\views.py", "repo_name": "bryanveloso/hello-social-registration", "stars": 35, "license": "bsd-3-clause", "language": "python", "size": 140}
{"docstring": "\"\"\"\n``mailbox`` should be a mailbox class; if None, then use default\nMailbox class.\n\"\"\"\n", "func_signal": "def __init__(self, config, mailbox=None):\n", "code": "print(\"System starting...\")\nself.config = config\n# Set up mailbox and logger before launching agent processes so that\n# the agent processes will have access to them.\nif mailbox:\n    mailbox_class = mailbox\nelse:\n    mailbox_class = Mailbox\nself.mailbox = mailbox_class(config)\nself.mailbox_process = Thread(target=self.mailbox.run, name=\"System Mailbox\")\nself.mailbox_process.start()\n# Start the logger thread.\nself.logger = ResultLogger(config)\nself.logger_process = Thread(target=self.logger.run, name=\"System Logger\")\nself.logger_process.start()\n\nself.processes = self.launch_processes()", "path": "paxos\\sim.py", "repo_name": "gdub/python-paxos", "stars": 35, "license": "mit", "language": "python", "size": 136}
{"docstring": "# let's check if it's already patched\n", "func_signal": "def _patch_egg_dir(path):\n", "code": "pkg_info = os.path.join(path, 'EGG-INFO', 'PKG-INFO')\nif os.path.exists(pkg_info):\n    if _same_content(pkg_info, SETUPTOOLS_PKG_INFO):\n        log.warn('%s already patched.', pkg_info)\n        return False\n_rename_path(path)\nos.mkdir(path)\nos.mkdir(os.path.join(path, 'EGG-INFO'))\npkg_info = os.path.join(path, 'EGG-INFO', 'PKG-INFO')\nf = open(pkg_info, 'w')\ntry:\n    f.write(SETUPTOOLS_PKG_INFO)\nfinally:\n    f.close()\nreturn True", "path": "distribute_setup.py", "repo_name": "gdub/python-paxos", "stars": 35, "license": "mit", "language": "python", "size": 136}
{"docstring": "\"\"\"\nStart a Paxos instance.\n\"\"\"\n", "func_signal": "def handle_client_request(self, msg, instance=None):\n", "code": "proposal = self.create_proposal(instance)\nif proposal.instance not in self.instances:\n    self.instances[proposal.instance] = {}\nif proposal.number not in self.instances[proposal.instance]:\n    self.instances[proposal.instance][proposal.number] = \\\n            BasicPaxosProposerProtocol(self, proposal)\nself.instances[proposal.instance][proposal.number].request = msg.value\nself.instances[proposal.instance][proposal.number].handle_client_request(proposal)", "path": "paxos\\__init__.py", "repo_name": "gdub/python-paxos", "stars": 35, "license": "mit", "language": "python", "size": 136}
{"docstring": "\"\"\"\nHandle a received message.  Meant to be overridden in subclasses for\ncustomizing agent's behavior.\n\"\"\"\n", "func_signal": "def handle_message(self, msg):\n", "code": "if isinstance(msg, SystemConfig):\n    self.set_config(msg)\nif msg == 'quit':\n    self.handle_quit()", "path": "paxos\\__init__.py", "repo_name": "gdub/python-paxos", "stars": 35, "license": "mit", "language": "python", "size": 136}
{"docstring": "\"\"\"Will backup the file then patch it\"\"\"\n", "func_signal": "def _patch_file(path, content):\n", "code": "existing_content = open(path).read()\nif existing_content == content:\n    # already patched\n    log.warn('Already patched.')\n    return False\nlog.warn('Patching...')\n_rename_path(path)\nf = open(path, 'w')\ntry:\n    f.write(content)\nfinally:\n    f.close()\nreturn True", "path": "distribute_setup.py", "repo_name": "gdub/python-paxos", "stars": 35, "license": "mit", "language": "python", "size": 136}
{"docstring": "\"\"\"\nStart the system by sending a message to each process containing\nthis system object.\n\"\"\"\n", "func_signal": "def start(self):\n", "code": "for x in range(len(self.processes)):\n    self.mailbox.send(x, self.config)", "path": "paxos\\sim.py", "repo_name": "gdub/python-paxos", "stars": 35, "license": "mit", "language": "python", "size": 136}
{"docstring": "\"\"\"\nLaunch ``number`` number of processes of the given ``agent_class``,\nusing the given Mailbox instance and with process id starting with the\ngiven ``pid`` and incrementing for each process spawned.\n\nReturn the incremented pid value when done, which is meant to be used\nin subsequent calls to this method for a starting pid.\n\"\"\"\n", "func_signal": "def launch_processes(self):\n", "code": "processes = []\nfor pid, agent_class in self.config.process_list():\n    agent = agent_class(pid, self.mailbox, self.logger)\n    p = Process(target=agent.run)\n    p.start()\n    processes.append(p)\nreturn processes", "path": "paxos\\sim.py", "repo_name": "gdub/python-paxos", "stars": 35, "license": "mit", "language": "python", "size": 136}
{"docstring": "\"\"\"\nCheck that each learner process got the same results in the same order.\nReturn False if there is a result list that is not the same.  If all\nresults are the same, then return the list of results.\n\"\"\"\n", "func_signal": "def check_results(self):\n", "code": "results = list(self.results.values())\ncompare_list = results[0]\nresult = True\nfor result_list in results[1:]:\n    if result_list != compare_list:\n        result =  False\nprint(\"Logger results consistent:\", result)\nreturn result", "path": "paxos\\sim.py", "repo_name": "gdub/python-paxos", "stars": 35, "license": "mit", "language": "python", "size": 136}
{"docstring": "\"\"\"\nWait for all mailbox messages to be processed, then send quit messages\nto all processes and join with all processes.  This will block until\nall agents have terminated.\n\"\"\"\n", "func_signal": "def shutdown_agents(self):\n", "code": "print(\"System waiting for mailbox to go inactive...\")\n# Sleep a bit to allow any actions based on timeouts to fire.\n#time.sleep(10)\nself.mailbox.join()\nprint(\"System shutting down agents...\")\nfor x in range(len(self.processes)):\n    self.mailbox.send(x, \"quit\")\nself.join()", "path": "paxos\\sim.py", "repo_name": "gdub/python-paxos", "stars": 35, "license": "mit", "language": "python", "size": 136}
{"docstring": "\"\"\"\nJoin with all processes that have been launched.\n\"\"\"\n", "func_signal": "def join(self):\n", "code": "for process in self.processes:\n    process.join()", "path": "paxos\\sim.py", "repo_name": "gdub/python-paxos", "stars": 35, "license": "mit", "language": "python", "size": 136}
{"docstring": "\"\"\"\nCount the number of consistent and inconsistent instance results,\nexcluding unlearned or missing values.\n\"\"\"\n# Disjoint set: good and bad (consistent and inconsistent) instances.\n", "func_signal": "def calculate_consistency(self):\n", "code": "self.good_instances = 0\nself.bad_instances = 0\n# Disjoint set: empty, incomplete, and complete instances representing\n# no, some, or all learners learned the value.\nself.empty_instances = 0\nself.incomplete_instances = 0\nself.complete_instances = 0\nfor i in self.instances:\n    values = set()\n    num_none = 0\n    for pid in self.pids:\n        value = self.logger.results[pid].get(i)\n        if value is None:\n            num_none += 1\n        else:\n            values.add(value)\n    length = len(values)\n    if length == 0:\n        self.good_instances += 1\n        self.empty_instances += 1\n    elif length == 1:\n        if num_none == 0:\n            self.complete_instances += 1\n        else:\n            self.incomplete_instances += 1\n        self.good_instances += 1\n    else:\n        self.bad_instances += 1\nself.good_instances_percent = float(100) * self.good_instances / len(self.instances)\nself.bad_instances_percent = float(100) * self.bad_instances / len(self.instances)\nself.empty_instances_percent = float(100) * self.empty_instances / len(self.instances)\nself.incomplete_instances_percent = float(100) * self.incomplete_instances / len(self.instances)\nself.complete_instances_percent = float(100) * self.complete_instances / len(self.instances)", "path": "paxos\\sim.py", "repo_name": "gdub/python-paxos", "stars": 35, "license": "mit", "language": "python", "size": 136}
{"docstring": "\"\"\"Install or upgrade setuptools and EasyInstall\"\"\"\n", "func_signal": "def main(argv, version=DEFAULT_VERSION):\n", "code": "tarball = download_setuptools()\n_install(tarball, _build_install_args(argv))", "path": "distribute_setup.py", "repo_name": "gdub/python-paxos", "stars": 35, "license": "mit", "language": "python", "size": 136}
{"docstring": "\"\"\"\nReturn a list of (pid, agent class) two-tuples that get used by\nSystem.launch_processes.\n\"\"\"\n", "func_signal": "def process_list(self):\n", "code": "pid = 0\nfor pid in self.proposer_ids:\n    yield (pid, self.proposer_class)\nfor pid in self.acceptor_ids:\n    yield (pid, self.acceptor_class)\nfor pid in self.learner_ids:\n    yield (pid, self.learner_class)", "path": "paxos\\__init__.py", "repo_name": "gdub/python-paxos", "stars": 35, "license": "mit", "language": "python", "size": 136}
{"docstring": "# extracting the tarball\n", "func_signal": "def _install(tarball, install_args=()):\n", "code": "tmpdir = tempfile.mkdtemp()\nlog.warn('Extracting in %s', tmpdir)\nold_wd = os.getcwd()\ntry:\n    os.chdir(tmpdir)\n    tar = tarfile.open(tarball)\n    _extractall(tar)\n    tar.close()\n\n    # going in the directory\n    subdir = os.path.join(tmpdir, os.listdir(tmpdir)[0])\n    os.chdir(subdir)\n    log.warn('Now working in %s', subdir)\n\n    # installing\n    log.warn('Installing Distribute')\n    if not _python_cmd('setup.py', 'install', *install_args):\n        log.warn('Something went wrong during the installation.')\n        log.warn('See the error message above.')\nfinally:\n    os.chdir(old_wd)", "path": "distribute_setup.py", "repo_name": "gdub/python-paxos", "stars": 35, "license": "mit", "language": "python", "size": 136}
{"docstring": "\"\"\"\nSet this process's sequence step to the number of proposers.\n\"\"\"\n", "func_signal": "def set_config(self, config):\n", "code": "super(Proposer, self).set_config(config)\nif config.proposer_sequence_start:\n    self.sequence = config.proposer_sequence_start\nif config.proposer_sequence_step:\n    self.sequence_step = config.proposer_sequence_step\nelse:\n    self.sequence_step = len(config.proposer_ids)\n\n# instantiate analyzer if dynamic weights enabled after configuration\nself.analyzer = None\nif config.dynamic_weights:\n    self.analyzer = Analyzer(config.acceptor_ids)", "path": "paxos\\__init__.py", "repo_name": "gdub/python-paxos", "stars": 35, "license": "mit", "language": "python", "size": 136}
{"docstring": "\"\"\"\nLoop forever, listening for and handling any messages sent to us.\n\"\"\"\n", "func_signal": "def run(self):\n", "code": "print(\"{}-{} started\".format(self.pid, self.__class__.__name__))\nwhile self.active:\n    msg = self.recv()\n    self.handle_message(msg)\n    #self.message_done()\nprint(\"Process {} shutting down\".format(self.pid))", "path": "paxos\\__init__.py", "repo_name": "gdub/python-paxos", "stars": 35, "license": "mit", "language": "python", "size": 136}
{"docstring": "# extracting the tarball\n", "func_signal": "def _build_egg(egg, tarball, to_dir):\n", "code": "tmpdir = tempfile.mkdtemp()\nlog.warn('Extracting in %s', tmpdir)\nold_wd = os.getcwd()\ntry:\n    os.chdir(tmpdir)\n    tar = tarfile.open(tarball)\n    _extractall(tar)\n    tar.close()\n\n    # going in the directory\n    subdir = os.path.join(tmpdir, os.listdir(tmpdir)[0])\n    os.chdir(subdir)\n    log.warn('Now working in %s', subdir)\n\n    # building an egg\n    log.warn('Building a Distribute egg in %s', to_dir)\n    _python_cmd('setup.py', '-q', 'bdist_egg', '--dist-dir', to_dir)\n\nfinally:\n    os.chdir(old_wd)\n# returning the result\nlog.warn(egg)\nif not os.path.exists(egg):\n    raise IOError('Could not build the egg.')", "path": "distribute_setup.py", "repo_name": "gdub/python-paxos", "stars": 35, "license": "mit", "language": "python", "size": 136}
{"docstring": "\"\"\"\nBlock until all messages have finished processing and we haven't had\nany messages for a while (i.e. active set to False).\n\"\"\"\n", "func_signal": "def join(self):\n", "code": "while self.active:\n    time.sleep(0.5)\n# Don't join funnel queue because there's a good chance that it will\n# never be fully exhausted due to heart beat messages.\n#self.funnel.join()", "path": "paxos\\sim.py", "repo_name": "gdub/python-paxos", "stars": 35, "license": "mit", "language": "python", "size": 136}
{"docstring": "\"\"\"\nCreate a new proposal using this process's current proposal number\nsequence and instance number sequence.  If instance is given, then\nuse it as the instance number instead of using this process's current\ninstance sequence number.\n\"\"\"\n", "func_signal": "def create_proposal(self, instance=None):\n", "code": "if instance:\n    instance_sequence = instance\nelse:\n    instance_sequence = self.instance_sequence\nprint(\"*** Process {} creating proposal with Number {}, Instance {}\"\n      .format(self.pid, self.sequence, instance_sequence))\nproposal = Proposal(self.sequence, instance_sequence, self.pid)\nself.sequence += self.sequence_step\n# Only increment the instance sequence if we weren't given one.\nif instance is None:\n    self.instance_sequence += 1\nreturn proposal", "path": "paxos\\__init__.py", "repo_name": "gdub/python-paxos", "stars": 35, "license": "mit", "language": "python", "size": 136}
{"docstring": "\"\"\"\nBlocking receive of a message destined to this agent process.\n\"\"\"\n", "func_signal": "def recv(self):\n", "code": "msg = self.mailbox.recv(self.pid)\nsource = getattr(msg, 'source', None)\nprint(\"  Process {}-{} received message from {}: {}\".format(\n      self.pid, self.__class__.__name__, source, msg))\nreturn msg", "path": "paxos\\__init__.py", "repo_name": "gdub/python-paxos", "stars": 35, "license": "mit", "language": "python", "size": 136}
{"docstring": "\"\"\"Iterate over all fields of a node, only yielding existing fields.\"\"\"\n", "func_signal": "def iter_fields(node):\n", "code": "if not hasattr(node, '_fields') or not node._fields:\n    return\nfor field in node._fields:\n    try:\n        yield field, getattr(node, field)\n    except AttributeError:\n        pass", "path": "lib\\mako\\_ast_util.py", "repo_name": "gabriel/shrub", "stars": 45, "license": "mit", "language": "python", "size": 441}
{"docstring": "\"\"\"\nA very verbose representation of the node passed.  This is useful for\ndebugging purposes.\n\"\"\"\n", "func_signal": "def dump(node):\n", "code": "def _format(node):\n    if isinstance(node, AST):\n        return '%s(%s)' % (node.__class__.__name__,\n                           ', '.join('%s=%s' % (a, _format(b))\n                                     for a, b in iter_fields(node)))\n    elif isinstance(node, list):\n        return '[%s]' % ', '.join(_format(x) for x in node)\n    return repr(node)\nif not isinstance(node, AST):\n    raise TypeError('expected AST, got %r' % node.__class__.__name__)\nreturn _format(node)", "path": "lib\\mako\\_ast_util.py", "repo_name": "gabriel/shrub", "stars": 45, "license": "mit", "language": "python", "size": 441}
{"docstring": "\"\"\"\nGet the mode for `compile` of a given node.  If the node is not a `mod`\nnode (`Expression`, `Module` etc.) a `TypeError` is thrown.\n\"\"\"\n", "func_signal": "def get_compile_mode(node):\n", "code": "if not isinstance(node, mod):\n    raise TypeError('expected mod node, got %r' % node.__class__.__name__)\nreturn {\n    Expression:     'eval',\n    Interactive:    'single'\n}.get(node.__class__, 'expr')", "path": "lib\\mako\\_ast_util.py", "repo_name": "gabriel/shrub", "stars": 45, "license": "mit", "language": "python", "size": 441}
{"docstring": "\"\"\"Parses ISO 8601 dates into datetime objects\n\nThe timezone is parsed from the date string. However it is quite common to\nhave dates without a timezone (not strictly correct). In this case the\ndefault timezone specified in default_timezone is used. This is UTC by\ndefault.\n\"\"\"\n", "func_signal": "def parse_date(datestring, default_timezone=UTC):\n", "code": "if not isinstance(datestring, basestring):\n    raise ParseError(\"Expecting a string %r\" % datestring)\nm = ISO8601_REGEX.match(datestring)\nif not m:\n    raise ParseError(\"Unable to parse date string %r\" % datestring)\ngroups = m.groupdict()\ntz = parse_timezone(groups[\"timezone\"], default_timezone=default_timezone)\nif groups[\"fraction\"] is None:\n    groups[\"fraction\"] = 0\nelse:\n    groups[\"fraction\"] = int(float(\"0.%s\" % groups[\"fraction\"]) * 1e6)\nreturn datetime(int(groups[\"year\"]), int(groups[\"month\"]), int(groups[\"day\"]),\n    int(groups[\"hour\"]), int(groups[\"minute\"]), int(groups[\"second\"]),\n    int(groups[\"fraction\"]), tz)", "path": "lib\\iso8601\\iso8601.py", "repo_name": "gabriel/shrub", "stars": 45, "license": "mit", "language": "python", "size": 441}
{"docstring": "\"\"\"\nReturn the docstring for the given node or `None` if no docstring can be\nfound.  If the node provided does not accept docstrings a `TypeError`\nwill be raised.\n\"\"\"\n", "func_signal": "def get_docstring(node):\n", "code": "if not isinstance(node, (FunctionDef, ClassDef, Module)):\n    raise TypeError(\"%r can't have docstrings\" % node.__class__.__name__)\nif node.body and isinstance(node.body[0], Str):\n    return node.body[0].s", "path": "lib\\mako\\_ast_util.py", "repo_name": "gabriel/shrub", "stars": 45, "license": "mit", "language": "python", "size": 441}
{"docstring": "# XXX: python 2.6 only\n", "func_signal": "def visit_Repr(self, node):\n", "code": "self.write('`')\nself.visit(node.value)\nself.write('`')", "path": "lib\\mako\\_ast_util.py", "repo_name": "gabriel/shrub", "stars": 45, "license": "mit", "language": "python", "size": 441}
{"docstring": "\"\"\"\nIterate over all nodes.  This is useful if you only want to modify nodes in\nplace and don't care about the context or the order the nodes are returned.\n\"\"\"\n", "func_signal": "def walk(node):\n", "code": "from collections import deque\ntodo = deque([node])\nwhile todo:\n    node = todo.popleft()\n    todo.extend(iter_child_nodes(node))\n    yield node", "path": "lib\\mako\\_ast_util.py", "repo_name": "gabriel/shrub", "stars": 45, "license": "mit", "language": "python", "size": 441}
{"docstring": "\"\"\"return the portion of a filename that is 'relative' to the directories in this lookup.\"\"\"\n", "func_signal": "def __relativeize(self, filename):\n", "code": "filename = posixpath.normpath(filename)\nfor dir in self.directories:\n    if filename[0:len(dir)] == dir:\n        return filename[len(dir):]\nelse:\n    return None", "path": "lib\\mako\\lookup.py", "repo_name": "gabriel/shrub", "stars": 45, "license": "mit", "language": "python", "size": 441}
{"docstring": "\"\"\"Iterate over all child nodes or a node.\"\"\"\n", "func_signal": "def iter_child_nodes(node):\n", "code": "for name, field in iter_fields(node):\n    if isinstance(field, AST):\n        yield field\n    elif isinstance(field, list):\n        for item in field:\n            if isinstance(item, AST):\n                yield item", "path": "lib\\mako\\_ast_util.py", "repo_name": "gabriel/shrub", "stars": 45, "license": "mit", "language": "python", "size": 441}
{"docstring": "\"\"\"\nIncrement the line numbers of all nodes by `n` if they have line number\nattributes.  This is useful to \"move code\" to a different location in a\nfile.\n\"\"\"\n", "func_signal": "def increment_lineno(node, n=1):\n", "code": "for node in zip((node,), walk(node)):\n    if 'lineno' in node._attributes:\n        node.lineno = getattr(node, 'lineno', 0) + n", "path": "lib\\mako\\_ast_util.py", "repo_name": "gabriel/shrub", "stars": 45, "license": "mit", "language": "python", "size": 441}
{"docstring": "# convert into a list of octets\n", "func_signal": "def url_escape(string):\n", "code": "string = string.encode(\"utf8\")\nreturn urllib.quote_plus(string)", "path": "lib\\mako\\filters.py", "repo_name": "gabriel/shrub", "stars": 45, "license": "mit", "language": "python", "size": 441}
{"docstring": "\"\"\"adjust the given uri based on the calling filename.\"\"\"\n\n", "func_signal": "def adjust_uri(self, uri, relativeto):\n", "code": "if uri[0] != '/':\n    if relativeto is not None:\n        return posixpath.join(posixpath.dirname(relativeto), uri)\n    else:\n        return '/' + uri\nelse:\n    return uri", "path": "lib\\mako\\lookup.py", "repo_name": "gabriel/shrub", "stars": 45, "license": "mit", "language": "python", "size": 441}
{"docstring": "\"\"\"render this Template with the given context.  \n\nthe data is written to the context's buffer.\"\"\"\n", "func_signal": "def render_context(self, context, *args, **kwargs):\n", "code": "if getattr(context, '_with_template', None) is None:\n    context._with_template = self\nruntime._render_context(self, self.callable_, context, *args, **kwargs)", "path": "lib\\mako\\template.py", "repo_name": "gabriel/shrub", "stars": 45, "license": "mit", "language": "python", "size": 441}
{"docstring": "\"\"\"Visit a node.\"\"\"\n", "func_signal": "def visit(self, node):\n", "code": "f = self.get_visitor(node)\nif f is not None:\n    return f(node)\nreturn self.generic_visit(node)", "path": "lib\\mako\\_ast_util.py", "repo_name": "gabriel/shrub", "stars": 45, "license": "mit", "language": "python", "size": 441}
{"docstring": "# XXX: Python 2.6 / 3.0 compatibility\n", "func_signal": "def visit_Raise(self, node):\n", "code": "self.newline()\nself.write('raise')\nif hasattr(node, 'exc') and node.exc is not None:\n    self.write(' ')\n    self.visit(node.exc)\n    if node.cause is not None:\n        self.write(' from ')\n        self.visit(node.cause)\nelif hasattr(node, 'type') and node.type is not None:\n    self.visit(node.type)\n    if node.inst is not None:\n        self.write(', ')\n        self.visit(node.inst)\n    if node.tback is not None:\n        self.write(', ')\n        self.visit(node.tback)", "path": "lib\\mako\\_ast_util.py", "repo_name": "gabriel/shrub", "stars": 45, "license": "mit", "language": "python", "size": 441}
{"docstring": "\"\"\"\nSome nodes require a line number and the column offset.  Without that\ninformation the compiler will abort the compilation.  Because it can be\na dull task to add appropriate line numbers and column offsets when\nadding new nodes this function can help.  It copies the line number and\ncolumn offset of the parent node to the child nodes without this\ninformation.\n\nUnlike `copy_location` this works recursive and won't touch nodes that\nalready have a location information.\n\"\"\"\n", "func_signal": "def fix_missing_locations(node):\n", "code": "def _fix(node, lineno, col_offset):\n    if 'lineno' in node._attributes:\n        if not hasattr(node, 'lineno'):\n            node.lineno = lineno\n        else:\n            lineno = node.lineno\n    if 'col_offset' in node._attributes:\n        if not hasattr(node, 'col_offset'):\n            node.col_offset = col_offset\n        else:\n            col_offset = node.col_offset\n    for child in iter_child_nodes(node):\n        _fix(child, lineno, col_offset)\n_fix(node, 1, 0)\nreturn node", "path": "lib\\mako\\_ast_util.py", "repo_name": "gabriel/shrub", "stars": 45, "license": "mit", "language": "python", "size": 441}
{"docstring": "\"\"\"\nThis function can convert a node tree back into python sourcecode.  This\nis useful for debugging purposes, especially if you're dealing with custom\nasts not generated by python itself.\n\nIt could be that the sourcecode is evaluable when the AST itself is not\ncompilable / evaluable.  The reason for this is that the AST contains some\nmore data than regular sourcecode does, which is dropped during\nconversion.\n\nEach level of indentation is replaced with `indent_with`.  Per default this\nparameter is equal to four spaces as suggested by PEP 8, but it might be\nadjusted to match the application's styleguide.\n\"\"\"\n", "func_signal": "def to_source(node, indent_with=' ' * 4):\n", "code": "generator = SourceGenerator(indent_with)\ngenerator.visit(node)\nreturn ''.join(generator.result)", "path": "lib\\mako\\_ast_util.py", "repo_name": "gabriel/shrub", "stars": 45, "license": "mit", "language": "python", "size": 441}
{"docstring": "# XXX: python 2.6 only\n", "func_signal": "def visit_Print(self, node):\n", "code": "self.newline()\nself.write('print ')\nwant_comma = False\nif node.dest is not None:\n    self.write(' >> ')\n    self.visit(node.dest)\n    want_comma = True\nfor value in node.values:\n    if want_comma:\n        self.write(', ')\n    self.visit(value)\n    want_comma = True\nif not node.nl:\n    self.write(',')", "path": "lib\\mako\\_ast_util.py", "repo_name": "gabriel/shrub", "stars": 45, "license": "mit", "language": "python", "size": 441}
{"docstring": "\"\"\"Replace characters with their character references.\n\nReplace characters by their named entity references.\nNon-ASCII characters, if they do not have a named entity reference,\nare replaced by numerical character references.\n\nThe return value is guaranteed to be ASCII.\n\"\"\"\n", "func_signal": "def escape(self, text):\n", "code": "return self.__escapable.sub(self.__escape, unicode(text)\n                            ).encode('ascii')", "path": "lib\\mako\\filters.py", "repo_name": "gabriel/shrub", "stars": 45, "license": "mit", "language": "python", "size": 441}
{"docstring": "\"\"\"An encoding error handler.\n\nThis python `codecs`_ error handler replaces unencodable\ncharacters with HTML entities, or, if no HTML entity exists for\nthe character, XML character references.\n\n>>> u'The cost was \\u20ac12.'.encode('latin1', 'htmlentityreplace')\n'The cost was &euro;12.'\n\"\"\"\n", "func_signal": "def htmlentityreplace_errors(ex):\n", "code": "if isinstance(ex, UnicodeEncodeError):\n    # Handle encoding errors\n    bad_text = ex.object[ex.start:ex.end]\n    text = _html_entities_escaper.escape(bad_text)\n    return (unicode(text), ex.end)\nraise ex", "path": "lib\\mako\\filters.py", "repo_name": "gabriel/shrub", "stars": 45, "license": "mit", "language": "python", "size": 441}
{"docstring": "'''Requires the client to resend the request, passing a one-time\nvalid token as confirmation.\n'''\n", "func_signal": "def confirm(leaf, *va, **params):\n", "code": "req = App.current.request\n\n# Validate confirmation if available\nparams['confirmed'] = False\ntry:\n\tif params['confirm_token'] == req.session['confirm_token']:\n\t\tparams['confirmed'] = True\nexcept (KeyError, TypeError):\n\tpass\n\n# Make sure we don't keep confirm_token in params\ntry: del params['confirm_token']\nexcept: pass\n\n# Call leaf\nrsp = leaf(*va, **params)\n\n# Add confirmation token if still unconfirmed\nif not params['confirmed']:\n\tif not isinstance(req.session, dict):\n\t\treq.session = {}\n\tconfirm_token = smisk.core.uid()\n\treq.session['confirm_token'] = confirm_token\n\tif not isinstance(rsp, dict):\n\t\trsp = {}\n\trsp['confirm_token'] = confirm_token\nelse:\n\t# Remove confirmation tokens\n\ttry: del req.session['confirm_token']\n\texcept: pass\n\ttry: del rsp['confirm_token']\n\texcept: pass\n\n# Return response\nreturn rsp", "path": "lib\\smisk\\mvc\\filters.py", "repo_name": "rsms/smisk", "stars": 45, "license": "mit", "language": "python", "size": 7478}
{"docstring": "'''Parse a qvalue HTTP header'''\n", "func_signal": "def parse_qvalue_header(s, accept_any_equals='*/*', partial_endswith='/*', return_true_if_accepts_charset=None):\n", "code": "vqs = []\nhighqs = []\npartials = []\naccept_any = False\n\nif not partial_endswith:\n  partial_endswith = None\n\nfor part in s.split(','):\n  part = part.strip(' ')\n  p = part.find(';')\n  if p != -1:\n    # todo Find out what the undocumented, but revealed, level= tags in HTTP 1.1 \n    #      really mean and if they exists in reality. As they are not documented,\n    #      we will not implement support for it. [RFC 2616, chapter 14.1 \"Accept\"]\n    pp = part.find('q=', p)\n    if pp != -1:\n      q = int(float(part[pp+2:])*100.0)\n      part = part[:p]\n      if return_true_if_accepts_charset is not None and part == return_true_if_accepts_charset:\n        return (True, True, True, True)\n      vqs.append([part, q])\n      if q == 100:\n        highqs.append(part)\n      if part == accept_any_equals:\n        accept_any = True\n      continue\n  # No qvalue; we use three classes: any (q=0), partial (q=50) and complete (q=100)\n  if return_true_if_accepts_charset is not None and part == return_true_if_accepts_charset:\n    return (True, True, True, True)\n  qual = 100\n  if part == accept_any_equals:\n    qual = 0\n    accept_any = True\n  else:\n    if partial_endswith is not None and part.endswith('/*'):\n      partial = part[:-2]\n      if not partial:\n        continue\n      qual = 50\n      partials.append(partial) # remove last char '*'\n    else:\n      highqs.append(part)\n  vqs.append([part, qual])\n# Order by qvalue\nvqs.sort(lambda a,b: b[1] - a[1])\nreturn vqs, highqs, partials, accept_any", "path": "lib\\smisk\\util\\string.py", "repo_name": "rsms/smisk", "stars": 45, "license": "mit", "language": "python", "size": 7478}
{"docstring": "'''Get value for key.\n'''\n", "func_signal": "def get(self, key):\n", "code": "response.headers.append('X-Pid: %d' % os.getpid())\ntry:\n  return {'value': self.entries[key]}\nexcept KeyError:\n  raise http.NotFound('no value associated with key %r' % key)", "path": "lib\\smisk\\test\\live\\key_value_store_example\\app.py", "repo_name": "rsms/smisk", "stars": 45, "license": "mit", "language": "python", "size": 7478}
{"docstring": "'''Called when someone calls a `HTTPExc` object, wrapping this status.\n\nThis interface is compatible with the callables returned by routers.\nMainly used by `mvc.Application.error()`\n\n:Parameters:\n  app : mvc.Application\n    The calling application\n:rtype: dict\n'''\n", "func_signal": "def service(self, app, *args, **kwargs):\n", "code": "app.response.status = self\nif self.has_body:\n  desc = self.name\n  if args:\n    desc = ', '.join(args)\n  return {'description': desc}", "path": "lib\\smisk\\mvc\\http.py", "repo_name": "rsms/smisk", "stars": 45, "license": "mit", "language": "python", "size": 7478}
{"docstring": "# return (list args, dict params)\n", "func_signal": "def unserialize(cls, file, length=-1, charset=None):\n", "code": "st = eval(file.read(length), {}, {})\nif isinstance(st, dict):\n  return (None, st)\nelif isinstance(st, list):\n  return (st, None)\nelse:\n  return ((st,), None)", "path": "lib\\smisk\\serialization\\python_py.py", "repo_name": "rsms/smisk", "stars": 45, "license": "mit", "language": "python", "size": 7478}
{"docstring": "# return (list args, dict params)\n", "func_signal": "def unserialize(cls, file, length=-1, encoding=None):\n", "code": "params, method_name = loads(file.read(length))\n\n# Override request path with mathodName. i.e. method.name -> /method/name\nif cls.respect_method_name:\n  if method_name is None:\n    raise http.InternalServerError(\n      'respect_method_name is enabled but request did not include methodName')\n  Application.current.request.url.path = '/'+'/'.join(method_name.split('.'))\n\nargs = []\nkwargs = {}\nif len(params) > 0:\n  for o in params:\n    if isinstance(o, dict):\n      kwargs.update(o)\n    else:\n      args.append(o)\n\nreturn (args, kwargs)", "path": "lib\\smisk\\serialization\\xmlrpc.py", "repo_name": "rsms/smisk", "stars": 45, "license": "mit", "language": "python", "size": 7478}
{"docstring": "'''\n:param url:\n  An absolute URL, absolute path or relative path.\n:type  url:\n  URL | string\n:param ref_base_url:\n  Default absolute URL used to expand a path to a full URL.\n  Uses ``smisk.core.request.url`` if not set.\n:type  ref_base_url:\n  URL\n:rtype:\n  URL\n'''\n", "func_signal": "def normalize_url(url, ref_base_url=None):\n", "code": "is_relative_uri = False\nif '/' not in url:\n  is_relative_uri = True\n  u = URL()\n  u.path = url\n  url = u\nelse:\n  url = URL(url) # make a copy so we don't modify the original\n\nif not ref_base_url:\n  if Application.current and Application.current.request:\n    ref_base_url = Application.current.request.url\n  else:\n    ref_base_url = URL()\n\nif url.scheme is None:\n  url.scheme = ref_base_url.scheme\nif url.user is None:\n  url.user = ref_base_url.user\n  if url.user is not None and url.password is None:\n    url.password = ref_base_url.password\nif url.host is None:\n  url.host = ref_base_url.host\nif url.port == 0:\n  url.port = ref_base_url.port\nif is_relative_uri:\n  base_path = ref_base_url.path\n  if not base_path:\n    base_path = '/'\n  elif not base_path.endswith('/'):\n    base_path = base_path + '/'\n  url.path = base_path + url.path\n\nreturn url", "path": "lib\\smisk\\util\\string.py", "repo_name": "rsms/smisk", "stars": 45, "license": "mit", "language": "python", "size": 7478}
{"docstring": "'''Remove entry.\n'''\n", "func_signal": "def delete(self, key):\n", "code": "response.headers.append('X-Pid: %d' % os.getpid())\nif key not in self.entries:\n  raise http.NotFound('no such entry %r' % key)\ndel self.entries[key]", "path": "lib\\smisk\\test\\live\\key_value_store_example\\app.py", "repo_name": "rsms/smisk", "stars": 45, "license": "mit", "language": "python", "size": 7478}
{"docstring": "'''Redirect the requesting client to someplace else.\n'''\n# If one or more entities are defined, add primary keys to params\n", "func_signal": "def redirect_to(url, entity=None, status=http.Found, **params):\n", "code": "if entity is not None:\n  if not isinstance(entity, (list, tuple)):\n    entity = [entity]\n  for ent in entity:\n    for pk in ent.table.primary_key.keys():\n      params[pk] = getattr(ent, pk)\n\n# The url might be a URL or leaf\nif not isinstance(url, basestring):\n  if not url:\n    url = '/'\n  elif isinstance(url, URL):\n    url = str(url)\n  else:\n    # url is probably an leaf\n    url = control.uri_for(url)\n    # Add filename extension if the initial request used it\n    try:\n      ext = Application.current.request.url.path.rsplit('.', 1)[1]\n      url = url + '.' + ext\n    except:\n      pass\n\n# Append any params to url\nif params and url:\n  if not url.endswith('?'):\n    if '?' in url:\n      url = url + '&'\n    else:\n      url = url + '?'\n  url = url + compose_query(params)\n\n# Status3xx.service() will perform further work on this url or \n# path (absolutize it, etc)\nraise status(url)", "path": "lib\\smisk\\mvc\\helpers.py", "repo_name": "rsms/smisk", "stars": 45, "license": "mit", "language": "python", "size": 7478}
{"docstring": "'''Returns stdout as string or None on failure\n'''\n", "func_signal": "def shell_cmd(args, cwd=BASE_DIR):\n", "code": "if not isinstance(args, (list, tuple)):\n\targs = [args]\nps = Popen(args, shell=True, cwd=cwd, stdout=PIPE, stderr=PIPE,\n\t\t\t\t\t close_fds=True)\nstdout, stderr = ps.communicate()\nif ps.returncode != 0:\n\tif stderr:\n\t\tstderr = stderr.strip()\n\traise IOError('Shell command %s failed (exit status %r): %s' %\\\n\t\t(args, ps.returncode, stderr))\nreturn stdout.strip()", "path": "setup.py", "repo_name": "rsms/smisk", "stars": 45, "license": "mit", "language": "python", "size": 7478}
{"docstring": "'''List available entries.\n'''\n", "func_signal": "def __call__(self):\n", "code": "response.headers.append('X-Pid: %d' % os.getpid())\nreturn {'entries': self.entries}", "path": "lib\\smisk\\test\\live\\key_value_store_example\\app.py", "repo_name": "rsms/smisk", "stars": 45, "license": "mit", "language": "python", "size": 7478}
{"docstring": "'''Deconstruct a URI path into standardized tokens.\n\n:param path: A pathname\n:type  path: string\n:rtype: list'''\n", "func_signal": "def tokenize_path(path):\n", "code": "tokens = []\nfor tok in strip_filename_extension(path).split('/'):\n  tok = URL.decode(tok)\n  if len(tok):\n    tokens.append(tok)\nreturn tokens", "path": "lib\\smisk\\util\\string.py", "repo_name": "rsms/smisk", "stars": 45, "license": "mit", "language": "python", "size": 7478}
{"docstring": "# Alignment\n", "func_signal": "def _machine(self):\n", "code": "machine = platform.machine()\nlog.info('checking machine alignment')\nif machine in X86_MACHINES:\n\tlog.debug('non-aligned')\n\tself.macros['SMISK_SYS_NONALIGNED'] = 1\nelse:\n\tlog.debug('aligned')\n# Endianess\nlog.info('checking machine endianess')\ntest = self._run('''\nint main (int argc, const char * argv[]) {\n\tint i = 0x11223344;\n\tchar *p = (char *) &i;\n\tif (*p == 0x44) return 0;\n\treturn 1;\n}\n''')\nif test == 0:\n\tself.macros['SMISK_SYS_LITTLE_ENDIAN'] = 1\n\tlog.debug(\"little endian\")\nelse:\n\tlog.debug(\"big endian\")", "path": "setup.py", "repo_name": "rsms/smisk", "stars": 45, "license": "mit", "language": "python", "size": 7478}
{"docstring": "'''Returns a tuple of 5 objects: bool is_method, list args, string varargs, \nstring varkw, list defaults\n\n:rtype: tuple\n'''\n", "func_signal": "def getargspec(cls, f):\n", "code": "if not isinstance(f, (MethodType, FunctionType)):\n  f = f.__call__\n\nis_method = False\nargs, varargs, varkw, defaults = inspect.getargspec(f)\n\nif isinstance(f, MethodType):\n  # Remove self\n  args = args[1:]\n  is_method = True\n\nreturn is_method, args, varargs, varkw, defaults", "path": "lib\\smisk\\util\\introspect.py", "repo_name": "rsms/smisk", "stars": 45, "license": "mit", "language": "python", "size": 7478}
{"docstring": "'''Unique id in URN form, distinguishing each unique build.\n\nIf building from a repository checkout, the resulting string\nwill have the format \"urn:rcsid:IDENTIFIER\". If building\nfrom source which is not under revision control, the build id \nwill have the format \"urn:utcts:YYYYMMDDHHMMSS\". (UTC timestamp)\n\nIn the special case of a Debian package, there is a suffix of\nthe following format: \":debian:\" package-revision\nExamples of Debianized build ids:\n\turn:rcsid:1d348b83a8a8297267c25a1dc86fa170c6d141df:debian:3\n\turn:utcts:20081030020621:debian:3\n\nDirty repositories generate a urn:rcsid with a trailing \"+\"\nfollowed by a base 16 encoded timestamp.\n\nA \"unique build\" is defined as the sum of all source code bytes\ncombined. A single modification anywhere causes a new unique\ninstance to be \"born\".\n\nThis value can be overridden by exporting the SMISK_BUILD_ID\nenvironment variable, but keep in mind to use a URN av value.\n\n:return: URN\n:rtype: string\n'''\n", "func_signal": "def core_build_id():\n", "code": "global _core_build_id\nif _core_build_id is None:\n\tif 'SMISK_BUILD_ID' in os.environ:\n\t\t_core_build_id = os.environ['SMISK_BUILD_ID']\n\telse:\n\t\t# Maybe under revision control\n\t\ttry:\n\t\t\t_core_build_id = shell_cmd(['git rev-parse master'])\n\t\texcept IOError:\n\t\t\tpass\n\t\tif _core_build_id:\n\t\t\tdirty_extra = ''\n\t\t\tif _core_build_id[-1] == '+':\n\t\t\t\tdirty_extra = '%x' % int(time.time())\n\t\t\t_core_build_id = 'urn:rcsid:%s%s' % (_core_build_id, dirty_extra)\n\t\tif not _core_build_id:\n\t\t\t# Not under revision control\n\t\t\t_core_build_id = time.strftime('urn:utcts:%Y%m%d%H%M%S', time.gmtime())\n\t\tif 'SMISK_BUILD_ID_SUFFIX' in os.environ:\n\t\t\t_core_build_id += os.environ['SMISK_BUILD_ID_SUFFIX']\nreturn _core_build_id", "path": "setup.py", "repo_name": "rsms/smisk", "stars": 45, "license": "mit", "language": "python", "size": 7478}
{"docstring": "'''Remove any file extension from filename.\n\n:rtype: string\n'''\n", "func_signal": "def strip_filename_extension(fn):\n", "code": "try:\n  return fn[:fn.rindex('.')]\nexcept:\n  return fn", "path": "lib\\smisk\\util\\string.py", "repo_name": "rsms/smisk", "stars": 45, "license": "mit", "language": "python", "size": 7478}
{"docstring": "'''Info about a callable.\n\nThe results are cached for efficiency.\n\n:param f:\n:type  f: callable\n:rtype: frozendict\n'''\n", "func_signal": "def callable_info(cls, f):\n", "code": "if not callable(f):\n  return None\n\nif isinstance(f, FunctionType):\n  # in case of ensure_va_kwa\n  try:\n    f = f.wrapped_func\n  except AttributeError:\n    pass\n\ncache_key = callable_cache_key(f)\n\ntry:\n  return cls._info_cache[cache_key]\nexcept KeyError:\n  pass\n\nis_method, args, varargs, varkw, defaults = cls.getargspec(f)\n\n_args = []\nargs_len = len(args)\ndefaults_len = 0\n\nif defaults is not None:\n  defaults_len = len(defaults)\n\nfor i,n in enumerate(args):\n  default_index = i-(args_len-defaults_len)\n  v = Undefined\n  if default_index > -1:\n    v = defaults[default_index]\n  _args.append((n, v))\n\ninfo = frozendict({\n  'name':f.func_name,\n  'args':tuple(_args),\n  'varargs':bool(varargs),\n  'varkw':bool(varkw),\n  'method':is_method\n})\n\ncls._info_cache[cache_key] = info\nreturn info", "path": "lib\\smisk\\util\\introspect.py", "repo_name": "rsms/smisk", "stars": 45, "license": "mit", "language": "python", "size": 7478}
{"docstring": "'''Calculate key unique enought to be used for caching callables.\n'''\n", "func_signal": "def callable_cache_key(node):\n", "code": "if not isinstance(node, (MethodType, FunctionType)):\n  return hash(node.__call__)^hash(node)\nelif isinstance(node, MethodType):\n  return hash(node)^hash(node.im_class)\nreturn node", "path": "lib\\smisk\\util\\cache.py", "repo_name": "rsms/smisk", "stars": 45, "license": "mit", "language": "python", "size": 7478}
{"docstring": "# did the client even try to authenticate?\n", "func_signal": "def filter(self, *va, **kw):\n", "code": "if 'HTTP_AUTHORIZATION' not in App.current.request.env:\n\treturn self.respond_unauthorized(self.require_authentication, *va, **kw)\n\n# not digest auth?\nif not App.current.request.env['HTTP_AUTHORIZATION'].startswith('Digest '):\n\traise http.BadRequest('only Digest authorization is allowed')\n\n# parse\nparams = {}\nrequired = len(self.required)\nfor k, v in [i.split(\"=\", 1) for i in App.current.request.env['HTTP_AUTHORIZATION'][7:].strip().split(',')]:\n\tk = k.strip()\n\tparams[k] = v.strip().replace('\"', '')\n\tif k in self.required:\n\t\trequired -= 1\n\n# missing required parameters?\nif required > 0:\n\traise http.BadRequest('insufficient authorization parameters')\n\n# user exists?\nif params['username'] not in self.users:\n\treturn self.respond_unauthorized(True, *va, **kw)\n\n# build A1 and A2\nA1 = '%s:%s:%s' % (params['username'], self.realm, self.users[params['username']])\nA2 = App.current.request.method + ':' + App.current.request.url.uri\n\n# build expected response\nexpected_response = None\nif 'qop' in params:\n\t# if qop is sent then cnonce and nc MUST be present\n\tif not 'cnonce' in params or not 'nc' in params:\n\t\traise http.BadRequest('cnonce and/or nc authorization parameters missing')\n\t\n\t# only auth type is supported\n\tif params['qop'] != 'auth':\n\t\traise http.BadRequest('unsupported qop ' + params['qop'])\n\t\n\t# build\n\texpected_response = self.KD(self.H(A1), '%s:%s:%s:%s:%s' % (\n\t\tparams['nonce'], params['nc'], params['cnonce'], params['qop'], self.H(A2)))\nelse:\n\t# qop not present (compatibility with RFC 2069)\n\texpected_response = self.KD(self.H(A1), params['nonce'] + ':' + self.H(A2))\n\n# 401 on realm mismatch\nif params['realm'] != self.realm:\n\tlog.debug('auth failure: unexpected realm')\n\treturn self.respond_unauthorized(True, *va, **kw)\n\n# 401 on unexpected response\nif params['response'] != expected_response:\n\tlog.debug('auth failure: unexpected digest response')\n\treturn self.respond_unauthorized(True, *va, **kw)\n\n# authorized -- delegate further down the filter chain\nreturn self.respond_authorized(params['username'], *va, **kw)", "path": "lib\\smisk\\mvc\\filters.py", "repo_name": "rsms/smisk", "stars": 45, "license": "mit", "language": "python", "size": 7478}
{"docstring": "'''Test that all items in `collection1` is contained within `collection2`.\n\nThe order of the contained items does not matter.\n\n:param collection1: Collection 1\n:type  collection1: collection\n:param collection2: Collection 2\n:type  collection2: collection\n:rtype: None\n'''\n", "func_signal": "def assertContains(self, collection1, collection2):\n", "code": "for item in collection1:\n  if item not in collection2:\n    raise AssertionError(u'%r !contains %r' % (collection1, collection2))", "path": "lib\\smisk\\test\\__init__.py", "repo_name": "rsms/smisk", "stars": 45, "license": "mit", "language": "python", "size": 7478}
{"docstring": "\"\"\"Convert Unicode or a string with unknown encoding into ASCII.\"\"\"\n\n", "func_signal": "def to_ascii(x):\n", "code": "if isinstance(x, str):\n    return x.encode('string_escape')\nelif isinstance(x, unicode):\n    return x.encode('unicode_escape')\nelse:\n    raise TypeError", "path": "theory\\model\\mpdutil.py", "repo_name": "ralfonso/theory", "stars": 33, "license": "mit", "language": "python", "size": 465}
{"docstring": "\"\"\"executes the given function, catches all exceptions and converts to a warning.\"\"\"\n", "func_signal": "def warn_exception(func, *args, **kwargs):\n", "code": "try:\n    return func(*args, **kwargs)\nexcept:\n    warn(\"%s('%s') ignored\" % sys.exc_info()[0:2])", "path": "theory\\model\\mpdutil.py", "repo_name": "ralfonso/theory", "stars": 33, "license": "mit", "language": "python", "size": 465}
{"docstring": "\"\"\"If 'key' is present in dict 'kw', coerce its value to type 'type_' if\nnecessary.  If 'flexi_bool' is True, the string '0' is considered false\nwhen coercing to boolean.\n\"\"\"\n\n", "func_signal": "def coerce_kw_type(kw, key, type_, flexi_bool=True):\n", "code": "if key in kw and type(kw[key]) is not type_ and kw[key] is not None:\n    if type_ is bool and flexi_bool:\n        kw[key] = asbool(kw[key])\n    else:\n        kw[key] = type_(kw[key])", "path": "theory\\model\\mpdutil.py", "repo_name": "ralfonso/theory", "stars": 33, "license": "mit", "language": "python", "size": 465}
{"docstring": "\"\"\"Adjust the incoming callable such that a 'self' argument is not required.\"\"\"\n\n", "func_signal": "def unbound_method_to_callable(func_or_cls):\n", "code": "if isinstance(func_or_cls, types.MethodType) and not func_or_cls.im_self:\n    return func_or_cls.im_func\nelse:\n    return func_or_cls", "path": "theory\\model\\mpdutil.py", "repo_name": "ralfonso/theory", "stars": 33, "license": "mit", "language": "python", "size": 465}
{"docstring": "\"\"\"decode a slice object as sent to __getitem__.\n    \ntakes into account the 2.5 __index__() method, basically.\n    \n\"\"\"\n", "func_signal": "def decode_slice(slc):\n", "code": "ret = []\nfor x in slc.start, slc.stop, slc.step:\n    if hasattr(x, '__index__'):\n        x = x.__index__()\n    ret.append(x)\nreturn tuple(ret)", "path": "theory\\model\\mpdutil.py", "repo_name": "ralfonso/theory", "stars": 33, "license": "mit", "language": "python", "size": 465}
{"docstring": "\"\"\"Given an instance or class, guess if it is or is acting as one of\nthe basic collection types: list, set and dict.  If the __emulates__\nproperty is present, return that preferentially.\n\"\"\"\n\n", "func_signal": "def duck_type_collection(specimen, default=None):\n", "code": "if hasattr(specimen, '__emulates__'):\n    # canonicalize set vs sets.Set to a standard: util.Set\n    if (specimen.__emulates__ is not None and\n        issubclass(specimen.__emulates__, set_types)):\n        return Set\n    else:\n        return specimen.__emulates__\n\nisa = isinstance(specimen, type) and issubclass or isinstance\nif isa(specimen, list):\n    return list\nelif isa(specimen, set_types):\n    return Set\nelif isa(specimen, dict):\n    return dict\n\nif hasattr(specimen, 'append'):\n    return list\nelif hasattr(specimen, 'add'):\n    return Set\nelif hasattr(specimen, 'set'):\n    return dict\nelse:\n    return default", "path": "theory\\model\\mpdutil.py", "repo_name": "ralfonso/theory", "stars": 33, "license": "mit", "language": "python", "size": 465}
{"docstring": "\"\"\"Construct a new named symbol.\"\"\"\n", "func_signal": "def __init__(self, name):\n", "code": "assert isinstance(name, str)\nself.name = name", "path": "theory\\model\\mpdutil.py", "repo_name": "ralfonso/theory", "stars": 33, "license": "mit", "language": "python", "size": 465}
{"docstring": "\"\"\"assign a '_creation_order' sequence to the given instance.\n\nThis allows multiple instances to be sorted in order of\ncreation (typically within a single thread; the counter is\nnot particularly threadsafe).\n\n\"\"\"\n", "func_signal": "def set_creation_order(instance):\n", "code": "global _creation_order\ninstance._creation_order = _creation_order\n_creation_order +=1", "path": "theory\\model\\mpdutil.py", "repo_name": "ralfonso/theory", "stars": 33, "license": "mit", "language": "python", "size": 465}
{"docstring": "\"\"\"Returns a dictionary of formatted, introspected function arguments.\n\nA enhanced variant of inspect.formatargspec to support code generation.\n\nfn\n   An inspectable callable\ngrouped\n  Defaults to True; include (parens, around, argument) lists\n\nReturns:\n\nargs\n  Full inspect.formatargspec for fn\nself_arg\n  The name of the first positional argument, varargs[0], or None\n  if the function defines no positional arguments.\napply_pos\n  args, re-written in calling rather than receiving syntax.  Arguments are\n  passed positionally.\napply_kw\n  Like apply_pos, except keyword-ish args are passed as keywords.\n\nExample::\n\n  >>> format_argspec_plus(lambda self, a, b, c=3, **d: 123)\n  {'args': '(self, a, b, c=3, **d)',\n   'self_arg': 'self',\n   'apply_kw': '(self, a, b, c=c, **d)',\n   'apply_pos': '(self, a, b, c, **d)'}\n\n\"\"\"\n", "func_signal": "def format_argspec_plus(fn, grouped=True):\n", "code": "spec = inspect.getargspec(fn)\nargs = inspect.formatargspec(*spec)\nif spec[0]:\n    self_arg = spec[0][0]\nelif spec[1]:\n    self_arg = '%s[0]' % spec[1]\nelse:\n    self_arg = None\napply_pos = inspect.formatargspec(spec[0], spec[1], spec[2])\ndefaulted_vals = spec[3] is not None and spec[0][0-len(spec[3]):] or ()\napply_kw = inspect.formatargspec(spec[0], spec[1], spec[2], defaulted_vals,\n                                 formatvalue=lambda x: '=' + x)\nif grouped:\n    return dict(args=args, self_arg=self_arg,\n                apply_pos=apply_pos, apply_kw=apply_kw)\nelse:\n    return dict(args=args[1:-1], self_arg=self_arg,\n                apply_pos=apply_pos[1:-1], apply_kw=apply_kw[1:-1])", "path": "theory\\model\\mpdutil.py", "repo_name": "ralfonso/theory", "stars": 33, "license": "mit", "language": "python", "size": 465}
{"docstring": "\"\"\"Interpret a single positional array argument as\n*args for the decorated function.\n\n\"\"\"\n\n", "func_signal": "def array_as_starargs_fn_decorator(fn):\n", "code": "def starargs_as_list(*args, **kwargs):\n    if isinstance(args, basestring) or (len(args) == 1 and not isinstance(args[0], tuple)):\n        return fn(*to_list(args[0], []), **kwargs)\n    else:\n        return fn(*args, **kwargs)\nstarargs_as_list.__doc__ = fn.__doc__\nreturn function_named(starargs_as_list, fn.__name__)", "path": "theory\\model\\mpdutil.py", "repo_name": "ralfonso/theory", "stars": 33, "license": "mit", "language": "python", "size": 465}
{"docstring": "\"\"\"Return a function with a given __name__.\n\nWill assign to __name__ and return the original function if possible on\nthe Python implementation, otherwise a new function will be constructed.\n\n\"\"\"\n", "func_signal": "def function_named(fn, name):\n", "code": "try:\n    fn.__name__ = name\nexcept TypeError:\n    fn = new.function(fn.func_code, fn.func_globals, name,\n                      fn.func_defaults, fn.func_closure)\nreturn fn", "path": "theory\\model\\mpdutil.py", "repo_name": "ralfonso/theory", "stars": 33, "license": "mit", "language": "python", "size": 465}
{"docstring": "\"\"\"Layer some of __builtin__.set's binop behavior onto sets.Set.\"\"\"\n\n", "func_signal": "def py24_style_ops():\n", "code": "def _binary_sanity_check(self, other):\n    pass\ndef issubset(self, iterable):\n    other = type(self)(iterable)\n    return sets.Set.issubset(self, other)\ndef __le__(self, other):\n    sets.Set._binary_sanity_check(self, other)\n    return sets.Set.__le__(self, other)\ndef issuperset(self, iterable):\n    other = type(self)(iterable)\n    return sets.Set.issuperset(self, other)\ndef __ge__(self, other):\n    sets.Set._binary_sanity_check(self, other)\n    return sets.Set.__ge__(self, other)\n# lt and gt still require a BaseSet\ndef __lt__(self, other):\n    sets.Set._binary_sanity_check(self, other)\n    return sets.Set.__lt__(self, other)\ndef __gt__(self, other):\n    sets.Set._binary_sanity_check(self, other)\n    return sets.Set.__gt__(self, other)\n\ndef __ior__(self, other):\n    if not isinstance(other, sets.BaseSet):\n        return NotImplemented\n    return sets.Set.__ior__(self, other)\ndef __iand__(self, other):\n    if not isinstance(other, sets.BaseSet):\n        return NotImplemented\n    return sets.Set.__iand__(self, other)\ndef __ixor__(self, other):\n    if not isinstance(other, sets.BaseSet):\n        return NotImplemented\n    return sets.Set.__ixor__(self, other)\ndef __isub__(self, other):\n    if not isinstance(other, sets.BaseSet):\n        return NotImplemented\n    return sets.Set.__isub__(self, other)\nreturn locals()", "path": "theory\\model\\mpdutil.py", "repo_name": "ralfonso/theory", "stars": 33, "license": "mit", "language": "python", "size": 465}
{"docstring": "\"\"\"Ensure basic interface compliance for an instance or dict of callables.\n\nChecks that ``obj`` implements public methods of ``cls`` or has members\nlisted in ``methods``.  If ``required`` is not supplied, implementing at\nleast one interface method is sufficient.  Methods present on ``obj`` that\nare not in the interface are ignored.\n\nIf ``obj`` is a dict and ``dict`` does not meet the interface\nrequirements, the keys of the dictionary are inspected. Keys present in\n``obj`` that are not in the interface will raise TypeErrors.\n\nRaises TypeError if ``obj`` does not meet the interface criteria.\n\nIn all passing cases, an object with callable members is returned.  In the\nsimple case, ``obj`` is returned as-is; if dict processing kicks in then\nan anonymous class is returned.\n\nobj\n  A type, instance, or dictionary of callables.\ncls\n  Optional, a type.  All public methods of cls are considered the\n  interface.  An ``obj`` instance of cls will always pass, ignoring\n  ``required``..\nmethods\n  Optional, a sequence of method names to consider as the interface.\nrequired\n  Optional, a sequence of mandatory implementations. If omitted, an\n  ``obj`` that provides at least one interface method is considered\n  sufficient.  As a convenience, required may be a type, in which case\n  all public methods of the type are required.\n\n\"\"\"\n", "func_signal": "def as_interface(obj, cls=None, methods=None, required=None):\n", "code": "if not cls and not methods:\n    raise TypeError('a class or collection of method names are required')\n\nif isinstance(cls, type) and isinstance(obj, cls):\n    return obj\n\ninterface = Set(methods or [m for m in dir(cls) if not m.startswith('_')])\nimplemented = Set(dir(obj))\n\ncomplies = operator.ge\nif isinstance(required, type):\n    required = interface\nelif not required:\n    required = Set()\n    complies = operator.gt\nelse:\n    required = Set(required)\n\nif complies(implemented.intersection(interface), required):\n    return obj\n\n# No dict duck typing here.\nif not type(obj) is dict:\n    qualifier = complies is operator.gt and 'any of' or 'all of'\n    raise TypeError(\"%r does not implement %s: %s\" % (\n        obj, qualifier, ', '.join(interface)))\n\nclass AnonymousInterface(object):\n    \"\"\"A callable-holding shell.\"\"\"\n\nif cls:\n    AnonymousInterface.__name__ = 'Anonymous' + cls.__name__\nfound = Set()\n\nfor method, impl in dictlike_iteritems(obj):\n    if method not in interface:\n        raise TypeError(\"%r: unknown in this interface\" % method)\n    if not callable(impl):\n        raise TypeError(\"%r=%r is not callable\" % (method, impl))\n    setattr(AnonymousInterface, method, staticmethod(impl))\n    found.add(method)\n\nif complies(found, required):\n    return AnonymousInterface\n\nraise TypeError(\"dictionary does not contain required keys %s\" %\n                ', '.join(required - found))", "path": "theory\\model\\mpdutil.py", "repo_name": "ralfonso/theory", "stars": 33, "license": "mit", "language": "python", "size": 465}
{"docstring": "\"\"\"apply caching to the return value of a function.\"\"\"\n\n", "func_signal": "def cache_decorator(func):\n", "code": "name = '_cached_' + func.__name__\n\ndef do_with_cache(self, *args, **kwargs):\n    try:\n        return getattr(self, name)\n    except AttributeError:\n        value = func(self, *args, **kwargs)\n        setattr(self, name, value)\n        return value\nreturn do_with_cache", "path": "theory\\model\\mpdutil.py", "repo_name": "ralfonso/theory", "stars": 33, "license": "mit", "language": "python", "size": 465}
{"docstring": "\"\"\"Return the full set of inherited kwargs for the given `cls`.\n\nProbes a class's __init__ method, collecting all named arguments.  If the\n__init__ defines a **kwargs catch-all, then the constructor is presumed to\npass along unrecognized keywords to it's base classes, and the collection\nprocess is repeated recursively on each of the bases.\n\"\"\"\n\n", "func_signal": "def get_cls_kwargs(cls):\n", "code": "for c in cls.__mro__:\n    if '__init__' in c.__dict__:\n        stack = Set([c])\n        break\nelse:\n    return []\n\nargs = Set()\nwhile stack:\n    class_ = stack.pop()\n    ctr = class_.__dict__.get('__init__', False)\n    if not ctr or not isinstance(ctr, types.FunctionType):\n        continue\n    names, _, has_kw, _ = inspect.getargspec(ctr)\n    args.update(names)\n    if has_kw:\n        stack.update(class_.__bases__)\nargs.discard('self')\nreturn list(args)", "path": "theory\\model\\mpdutil.py", "repo_name": "ralfonso/theory", "stars": 33, "license": "mit", "language": "python", "size": 465}
{"docstring": "\"\"\"Interpret a single positional array argument as\n*args for the decorated method.\n\n\"\"\"\n\n", "func_signal": "def array_as_starargs_decorator(fn):\n", "code": "def starargs_as_list(self, *args, **kwargs):\n    if isinstance(args, basestring) or (len(args) == 1 and not isinstance(args[0], tuple)):\n        return fn(self, *to_list(args[0], []), **kwargs)\n    else:\n        return fn(self, *args, **kwargs)\nstarargs_as_list.__doc__ = fn.__doc__\nreturn function_named(starargs_as_list, fn.__name__)", "path": "theory\\model\\mpdutil.py", "repo_name": "ralfonso/theory", "stars": 33, "license": "mit", "language": "python", "size": 465}
{"docstring": "\"\"\"format_argspec_plus with considerations for typical __init__ methods\n\nWraps format_argspec_plus with error handling strategies for typical\n__init__ cases::\n\n  object.__init__ -> (self)\n  other unreflectable (usually C) -> (self, *args, **kwargs)\n\n\"\"\"\n", "func_signal": "def format_argspec_init(method, grouped=True):\n", "code": "try:\n    return format_argspec_plus(method, grouped=grouped)\nexcept TypeError:\n    self_arg = 'self'\n    if method is object.__init__:\n        args = grouped and '(self)' or 'self'\n    else:\n        args = (grouped and '(self, *args, **kwargs)'\n                        or 'self, *args, **kwargs')\n    return dict(self_arg='self', args=args, apply_pos=args, apply_kw=args)", "path": "theory\\model\\mpdutil.py", "repo_name": "ralfonso/theory", "stars": 33, "license": "mit", "language": "python", "size": 465}
{"docstring": "\"\"\"inspect.getargspec with considerations for typical __init__ methods\n\nWraps inspect.getargspec with error handling for typical __init__ cases::\n\n  object.__init__ -> (self)\n  other unreflectable (usually C) -> (self, *args, **kwargs)\n\n\"\"\"\n", "func_signal": "def getargspec_init(method):\n", "code": "try:\n    return inspect.getargspec(method)\nexcept TypeError:\n    if method is object.__init__:\n        return (['self'], None, None, None)\n    else:\n        return (['self'], 'args', 'kwargs', None)", "path": "theory\\model\\mpdutil.py", "repo_name": "ralfonso/theory", "stars": 33, "license": "mit", "language": "python", "size": 465}
{"docstring": "\"\"\"Return an unordered sequence of all classes related to cls.\n\nTraverses diamond hierarchies.\n\nFibs slightly: subclasses of builtin types are not returned.  Thus\nclass_hierarchy(class A(object)) returns (A, object), not A plus every\nclass systemwide that derives from object.\n\n\"\"\"\n", "func_signal": "def class_hierarchy(cls):\n", "code": "hier = Set([cls])\nprocess = list(cls.__mro__)\nwhile process:\n    c = process.pop()\n    for b in [_ for _ in c.__bases__ if _ not in hier]:\n        process.append(b)\n        hier.add(b)\n    if c.__module__ == '__builtin__':\n        continue\n    for s in [_ for _ in c.__subclasses__() if _ not in hier]:\n        process.append(s)\n        hier.add(s)\nreturn list(hier)", "path": "theory\\model\\mpdutil.py", "repo_name": "ralfonso/theory", "stars": 33, "license": "mit", "language": "python", "size": 465}
{"docstring": "\"\"\"Return a (key, value) iterator for almost any dict-like object.\"\"\"\n\n", "func_signal": "def dictlike_iteritems(dictlike):\n", "code": "if hasattr(dictlike, 'iteritems'):\n    return dictlike.iteritems()\nelif hasattr(dictlike, 'items'):\n    return iter(dictlike.items())\n\ngetter = getattr(dictlike, '__getitem__', getattr(dictlike, 'get', None))\nif getter is None:\n    raise TypeError(\n        \"Object '%r' is not dict-like\" % dictlike)\n\nif hasattr(dictlike, 'iterkeys'):\n    def iterator():\n        for key in dictlike.iterkeys():\n            yield key, getter(key)\n    return iterator()\nelif hasattr(dictlike, 'keys'):\n    return iter([(key, getter(key)) for key in dictlike.keys()])\nelse:\n    raise TypeError(\n        \"Object '%r' is not dict-like\" % dictlike)", "path": "theory\\model\\mpdutil.py", "repo_name": "ralfonso/theory", "stars": 33, "license": "mit", "language": "python", "size": 465}
{"docstring": "# remove all <text> tags\n# add missing <html><body> tags where needed\n        \n", "func_signal": "def preprocess(self, raw_filename):\n", "code": "_skip_file(self.re_BACK, raw_filename)\nwith open(os.path.join(self._dataset_dir, 'raw', raw_filename), 'r' ) as f:\n    html_string = _remove_text_tag(f.read(), raw_filename)\n    \n    soup = BeautifulSoup(html_string)\n    if (not soup.find('html')) and (not soup.find('body')):\n        # no html no body tag\n        logger.warn('appending body and html tags to %s', raw_filename)\n        html_string = '<html><body>  %s  </body></html>' % html_string\n        \n    elif (not soup.find('html')) or (not soup.find('body')):\n        # really weird case\n        logger.warning('%s has html tag or body tag but not both', raw_filename) \n    else:\n        logger.info('no tag appending on %s', raw_filename)\n    \n    output_filename = raw_filename.replace('.backup','')\n    logger.debug('preprocesing complete: %s ---> %s',raw_filename,output_filename)\n    with open(os.path.join(self._dataset_dir, 'raw', output_filename) ,'w') as output:\n        output.write(html_string)", "path": "src\\dataset_manage.py", "repo_name": "tomazk/Text-Extraction-Evaluation", "stars": 42, "license": "None", "language": "python", "size": 252}
{"docstring": "'''Pickle the internal state'''\n", "func_signal": "def save(self, dataset_name):\n", "code": "pickle_path = os.path.join(self.__pickle_path,'%s.pickle' % dataset_name)\nlogger.info('saving text based results to: %s', pickle_path)\n\nwith open(pickle_path,'wb') as f:\n    pickle.dump( self.__dict__ ,f)", "path": "src\\txtexeval\\evaluation.py", "repo_name": "tomazk/Text-Extraction-Evaluation", "stars": 42, "license": "None", "language": "python", "size": 252}
{"docstring": "'''Set verbose to True if you want the log to appear on stderr'''\n", "func_signal": "def logging_setup(verbose, output_path):\n", "code": "logger = logging.getLogger()\nlogger.setLevel(logging.DEBUG)\nfile = logging.FileHandler(filename = output_path)\nfile.setLevel(logging.INFO)\nfile.setFormatter(logging.Formatter('%(name)-12s: %(levelname)-8s %(message)s'))\nlogger.addHandler(file)\n\nif verbose:\n    console = logging.StreamHandler()\n    console.setLevel(logging.DEBUG)\n    console.setFormatter(logging.Formatter('%(name)-12s: %(levelname)-8s %(message)s'))\n    logger.addHandler(console)", "path": "src\\extract_manage.py", "repo_name": "tomazk/Text-Extraction-Evaluation", "stars": 42, "license": "None", "language": "python", "size": 252}
{"docstring": "'''Return a tuple containing equidistant distribution baskets.'''\n", "func_signal": "def equidistant_count(start, stop, step , list):\n", "code": "limit_list = np.arange(start,stop, step)\ncount = [0] * len(limit_list) \n\nfor value in list:\n    value = float(value)\n    assert start <= value <= stop\n    mark = False\n    for i, low in enumerate(limit_list):\n        up = low + step\n        if i < (len(limit_list)-1) and low <= value < up:\n            count[i] += 1\n            mark =True\n            break\n        elif i == (len(limit_list)-1) and low <= value <=up:\n            count[i] += 1\n            mark  =True\n            break\n    '''\n    if not mark:\n        print len(limit_list)\n        print j\n        print value\n        print type(value)\n        print 0.3 <= value < 0.35\n        raise Exception('something very weird is going on - %s' % str(value))\n    '''\nreturn tuple(count)", "path": "src\\plot_manage.py", "repo_name": "tomazk/Text-Extraction-Evaluation", "stars": 42, "license": "None", "language": "python", "size": 252}
{"docstring": "# lazy init\n", "func_signal": "def extract(self):\n", "code": "cls = self.__class__\nif cls._driver == None:\n    # init firefox web driver\n    cls._driver = webdriver.Firefox()\n\nurl = self.data_instance.get_url_local()\ncls._driver.get(url)\ntime.sleep(2)\ncls._driver.execute_script(self._bookmarklet_source)\n\ntry:\n    # find the node that contains  content\n    # and check if readability managed to extract anything meaningful\n    element = cls._driver.find_element_by_id('readInner')\n    self._check_content_presence()\nexcept NoSuchElementException:\n    raise ContentExtractorError('readability failed to produce the #readInner DOM node')\nelse:\n    return element.text.encode(self.data_instance.raw_encoding, 'ignore')", "path": "src\\txtexeval\\extractor.py", "repo_name": "tomazk/Text-Extraction-Evaluation", "stars": 42, "license": "None", "language": "python", "size": 252}
{"docstring": "'''Return the extractor class given a slug'''\n", "func_signal": "def get_extractor_cls(extractor_slug):\n", "code": "for e in extractor_list:\n    if e.SLUG == extractor_slug: \n        return e", "path": "src\\txtexeval\\extractor.py", "repo_name": "tomazk/Text-Extraction-Evaluation", "stars": 42, "license": "None", "language": "python", "size": 252}
{"docstring": "'''Plot the error case analysis.'''\n# get results\n", "func_signal": "def dataset_contents_plot(dataset_name, img_name):\n", "code": "txt_results = TextBasedResults()\ntxt_results.load(dataset_name)\ntxt_results.print_results()\n\n# package data\nelist = extractor_list_filter(txt_results.text_eval_results.keys())\nextractor_slugs = tuple( [e.SLUG for e in elist] )\npackage = [\n    ('|rel| = 0','#9DFADE', [ txt_results.result_contents(ex).rel_empty for ex in extractor_slugs] ),\n    ('|rel intersect ret| = 0','#3C70A3', [ txt_results.result_contents(ex).rel_ret_empty for ex in extractor_slugs] ),\n    ('|ret| = 0','#5CCBED', [ txt_results.result_contents(ex).ret_empty for ex in extractor_slugs] ),\n    ('mismatch','#A76CF5', [ txt_results.result_contents(ex).missmatch for ex in extractor_slugs] ),\n    ('failed','#C43156', [ txt_results.result_contents(ex).fail for ex in extractor_slugs] ),\n    ('successful','#31C460', [ txt_results.result_contents(ex).succ for ex in extractor_slugs] ),\n]\nnum_of_extractors = len(extractor_slugs)\nind = np.arange(num_of_extractors)  # the x locations for the groups\nwidth = 0.6\n\nfig = plt.gcf()\nfig.legend(      [plt.Rectangle((0, 0), 1, 1, fc=p[1]) for p in package],\n                 [p[0] for p in package],\n                 fancybox = True,\n                 prop = dict(size='x-small'),                     \n)\n\n# with successful instances\nax1 = plt.subplot(121)\nbottom_y = np.zeros(num_of_extractors)\nfor pdata in package:\n    ax1.bar(ind, pdata[2],width,bottom = bottom_y,color=pdata[1], \n            ecolor ='g', linewidth = 0.2, alpha = 0.95)\n    bottom_y += pdata[2]\n\nax2 = plt.subplot(122)\nbottom_y = np.zeros(num_of_extractors)\ndel package[-1]\nfor pdata in package:\n    ax2.bar(ind, pdata[2],width,bottom = bottom_y,color=pdata[1], \n            ecolor ='g', linewidth = 0.2, alpha = 0.95)\n    bottom_y += pdata[2]\n\n# xticks labels\nextractor_names = [ get_extractor_cls(e).NAME for e in extractor_slugs]\nax1.set_xticks(ind+width/2.)\nax1.set_xticklabels(extractor_names, size = 'xx-small', rotation = 'vertical')\nax2.set_xticks(ind+width/2.)\nax2.set_xticklabels(extractor_names, size = 'xx-small', rotation = 'vertical')\n\n# grid settings\nfig.suptitle('Boundary cases')\nax1.grid(True, alpha = 0.5)\nax2.grid(True, alpha = 0.5)\n\n# adjustment\nw,h = fig.get_size_inches()\nfig.set_size_inches( w*1.5, h*1.5)\nfig.subplots_adjust( bottom = 0.2)\n\n# output \nout_path = os.path.join(settings.PATH_LOCAL_DATA, 'plot-output', img_name)\nfig.savefig(out_path,bbox_inches='tight')", "path": "src\\plot_manage.py", "repo_name": "tomazk/Text-Extraction-Evaluation", "stars": 42, "license": "None", "language": "python", "size": 252}
{"docstring": "'''Sys argument parsing trough argparse'''\n", "func_signal": "def parse_args(args):\n", "code": "ex_list = [e.SLUG for e in extractor_list]    \nparser = argparse.ArgumentParser(description = 'Tool for extracting article text from dataset instances')\nparser.add_argument('extractor', choices = ex_list, help = 'extractor slug')\nparser.add_argument('dataset_name', help = 'name of the dataset')\nparser.add_argument('-v','--verbose', action = 'store_true', help = 'print log to console')\nparser.add_argument('-t','--timeout', type=int, default=0, help='wait x seconds between extraction operations')\nparser.add_argument('-rf','--retry_failed', action = 'store_true', help = 'retry to extract text from instances that failed')\nparser.add_argument('-se','--skip_existing', action = 'store_true', help = 'skip all documents that already have their result stored in the database/filesystem')\nreturn parser.parse_args(args)", "path": "src\\extract_manage.py", "repo_name": "tomazk/Text-Extraction-Evaluation", "stars": 42, "license": "None", "language": "python", "size": 252}
{"docstring": "'''Returns bag of words dictionary from a list of word tokens'''\n", "func_signal": "def _bow(word_tokens):\n", "code": "bow = {}\nfor i in word_tokens:\n    if i not in bow:\n        bow[i] = 1\n    else:\n        bow[i] += 1\nreturn bow", "path": "src\\txtexeval\\evaluation.py", "repo_name": "tomazk/Text-Extraction-Evaluation", "stars": 42, "license": "None", "language": "python", "size": 252}
{"docstring": "'''Tokenize dirty text into a normalized list of words'''\n# remove punctuation and replace with whitespace\n", "func_signal": "def _tokenize_text(dirty_text):\n", "code": "table = string.maketrans(string.punctuation, ' '*len(string.punctuation))\ndirty_text =  dirty_text.translate(table)\n# remove any control char\ndirty_text = re_CONTROL.sub(' ', dirty_text)\n# remove any non ascii char to mitigate the troubles of broken encodings\ndirty_text = re_NONASCII.sub('', dirty_text)\n# normalize to lowercase\ndirty_text = dirty_text.lower()\n# remove empty tokens\nreturn filter(lambda w: w != '', re_WS.split(dirty_text))", "path": "src\\txtexeval\\evaluation.py", "repo_name": "tomazk/Text-Extraction-Evaluation", "stars": 42, "license": "None", "language": "python", "size": 252}
{"docstring": "'''\nFactory function that returns an instance of a format class listed in the\ndataset format map.\n'''\n", "func_signal": "def from_document_factory(document, slug):\n", "code": "map_ = dict(dataset_format_map)\ncls = map_[slug]\nreturn cls.from_document(document)", "path": "src\\txtexeval\\evaluation.py", "repo_name": "tomazk/Text-Extraction-Evaluation", "stars": 42, "license": "None", "language": "python", "size": 252}
{"docstring": "'''Get all test modules'''\n", "func_signal": "def test_modules():\n", "code": "modlist = []\nfor mod in os.listdir('.'):\n    if mod.startswith('test_') and mod.endswith(\".py\"):\n        modlist.append(mod[0:-3])\nreturn modlist", "path": "tests\\testsrunner.py", "repo_name": "tomazk/Text-Extraction-Evaluation", "stars": 42, "license": "None", "language": "python", "size": 252}
{"docstring": "# if filename does not match the given regular expr. \n# then raise the skip trigger\n", "func_signal": "def _skip_file(regex, raw_filename):\n", "code": "if not regex.match(raw_filename):\n    logger.debug('skipping file %s', raw_filename)\n    raise SkipTrigger", "path": "src\\dataset_manage.py", "repo_name": "tomazk/Text-Extraction-Evaluation", "stars": 42, "license": "None", "language": "python", "size": 252}
{"docstring": "# rename every unprocessed [number].html to [number].html.backup \n\n", "func_signal": "def create_backups(self, raw_filename):\n", "code": "raw_filename_path = os.path.join(self._dataset_dir, 'raw', raw_filename)\nbackup_path = raw_filename_path + '.backup'\nlogger.debug('renaming %s to %s', raw_filename, raw_filename + '.backup')\nos.rename(raw_filename_path, backup_path)", "path": "src\\dataset_manage.py", "repo_name": "tomazk/Text-Extraction-Evaluation", "stars": 42, "license": "None", "language": "python", "size": 252}
{"docstring": "'''\nPlot the avg precision, recall and F1 score bar chart for the given dataset\nname.\n'''\n# get results\n", "func_signal": "def dataset_stat_plot(dataset_name, img_name):\n", "code": "txt_results = TextBasedResults()\ntxt_results.load(dataset_name)\ntxt_results.print_results()\n\n#package results\nelist = extractor_list_filter(txt_results.text_eval_results.keys())\nextractor_slugs = tuple([e.SLUG for e in elist])\npackaged_data = (\n    ('Precision', [ (txt_results.precision_statistics(e), e) for e in extractor_slugs ] ),\n    ('Recall', [ (txt_results.recall_statistics(e), e) for e in extractor_slugs ] ),\n    ('F1 score', [ (txt_results.f1score_statistics(e), e) for e in extractor_slugs ] ),\n)\n\nbar_color = ('b','c','m')\nfor i,pdata in enumerate(packaged_data):\n\n    # package plotting values \n    num_of_extractors = len(extractor_slugs)\n    ind = np.arange(num_of_extractors)  # the x locations for the groups\n    width = 0.6      # the width of the bars\n    \n    result_list = pdata[1]\n    result_list.sort(key=lambda i: i[0][0])\n    result_list.reverse()\n    \n    avg = [ x[0][0] for x in result_list]\n    stddev = [ x[0][1] for x in result_list]\n    \n    # plot\n    plt.subplot(3,1,i+1)\n    plt.grid(True, alpha = 0.5)\n    \n    rects_avg = plt.bar(ind, avg, width,color=bar_color[i], ecolor ='g' ,\n        yerr = stddev, linewidth = 0.5, alpha = 0.8)\n    \n    # lables and titles\n    extractor_names = [ get_extractor_cls(r[1]).NAME for r in result_list]\n    plt.title(pdata[0])\n    plt.xticks(ind+width/2., extractor_names, size = 'xx-small', rotation = 'vertical')\n    plt.legend(  (rects_avg[0],),\n                 ('avg',),\n                 fancybox = True,\n                 prop = dict(size='x-small'),\n                 loc = 4 # lower right \n    )\n    for rect in rects_avg:\n        height = rect.get_height()\n        plt.text(rect.get_x()+rect.get_width()/2.25,rect.get_height() + 0.01 ,\n             '%1.2f'%height, ha='center', va='bottom', size = 'x-small')\n    \n    \n#subplots adjusting\nplt.subplots_adjust( wspace=0.5, hspace=0.9)\n\n#adjust figure height\nfig = plt.gcf()\nw,h = fig.get_size_inches()\nfig.set_size_inches( w , h*1.6)\n\n# output \nout_path = os.path.join(settings.PATH_LOCAL_DATA, 'plot-output', img_name)\nplt.savefig(out_path)", "path": "src\\plot_manage.py", "repo_name": "tomazk/Text-Extraction-Evaluation", "stars": 42, "license": "None", "language": "python", "size": 252}
{"docstring": "# validate result \n", "func_signal": "def __init__(self, precision, recall, f1_score, id = None):\n", "code": "if math.isinf(precision) and not math.isinf(recall):\n    assert recall == 0\n    assert math.isnan(f1_score)\nelif not math.isinf(precision) and math.isinf(recall):\n    assert precision == 0\n    assert math.isnan(f1_score)\nelif math.isinf(precision) and math.isinf(recall):\n    assert math.isnan(f1_score)\nelif precision == recall == 0:\n    assert math.isinf(f1_score)\nelif not math.isinf(precision) and not math.isinf(recall):\n    assert 0 < precision <= 1\n    assert 0 < recall <= 1\n    assert 0 < f1_score <= 1\n\nself.precision = precision\nself.recall = recall\nself.f1_score = f1_score\nself.id = id", "path": "src\\txtexeval\\evaluation.py", "repo_name": "tomazk/Text-Extraction-Evaluation", "stars": 42, "license": "None", "language": "python", "size": 252}
{"docstring": "# params: BS tag and attribute name\n# return None or attribute value\n# takes care of encoding\n", "func_signal": "def _get_attribute(tag, name):\n", "code": "try: \n    return tag[name].encode('ascii', 'ignore')\nexcept KeyError:\n    return None", "path": "src\\dataset_manage.py", "repo_name": "tomazk/Text-Extraction-Evaluation", "stars": 42, "license": "None", "language": "python", "size": 252}
{"docstring": "# remove URL meta data\n", "func_signal": "def __init__(self, cleaneval_string):\n", "code": "self._text = self.re_URL.sub('', cleaneval_string)\n# remove tag guidelines\nself._text = self.re_TAG.sub('', self._text)", "path": "src\\txtexeval\\evaluation.py", "repo_name": "tomazk/Text-Extraction-Evaluation", "stars": 42, "license": "None", "language": "python", "size": 252}
{"docstring": "'''\nDRY decorator that mitigates the trouble of inserting boilerplate code \ninside the extract method for invoking the private method _content_status.\nWhateverExtractor._content_status is used to check for errors returned in\nthe response content itself.\n'''\n", "func_signal": "def check_content_status(extract):\n", "code": "def wrapper(self):\n    self._content = extract(self)\n    self._content_status()\n    return self._content\nreturn wrapper", "path": "src\\txtexeval\\extractor.py", "repo_name": "tomazk/Text-Extraction-Evaluation", "stars": 42, "license": "None", "language": "python", "size": 252}
{"docstring": "'''Sys argument parsing trough argparse'''\n", "func_signal": "def parse_args(args):\n", "code": "parser = argparse.ArgumentParser(description = 'Tool for for generating evaluation results')\nparser.add_argument('dataset_type', choices = [i[0] for i in dataset_format_map], help = 'dataset type e.g. cleaneval' )\nparser.add_argument('dataset_name', help = 'name of the dataset')\nparser.add_argument('-v','--verbose', action = 'store_true', help = 'print log to console')\nparser.add_argument('-u','--update', choices = [e.SLUG for e in extractor_list], help = 'update the results for a single extractor')\nreturn parser.parse_args(args)", "path": "src\\evaluate_manage.py", "repo_name": "tomazk/Text-Extraction-Evaluation", "stars": 42, "license": "None", "language": "python", "size": 252}
{"docstring": "\"\"\"Internal function to return text from nodes\n\"\"\"\n", "func_signal": "def _getText(nodelist):\n", "code": "rc = \"\"\nfor node in nodelist:\n\tif node.nodeType == node.TEXT_NODE:\n\t\trc = rc + node.data\nreturn rc", "path": "openastromod\\importfile.py", "repo_name": "pascallemazurier/openastro-dev", "stars": 32, "license": "gpl-3.0", "language": "python", "size": 15945}
{"docstring": "#remove timezone\n", "func_signal": "def localToSecondaryProgression(self,dt):\n", "code": "\t\tdt_utc = dt - datetime.timedelta(seconds=float(self.timezone)*float(3600))\n\t\th,m,s = self.decHour(self.hour)\n\t\tdt_new = ephemeris.years_diff(self.year,self.month,self.day,self.hour,\n\t\t\tdt_utc.year,dt_utc.month,dt_utc.day,self.decHourJoin(dt_utc.hour,\n\t\t\tdt_utc.minute,dt_utc.second))\n\t\n\t\tself.sp_year = dt_new.year\n\t\tself.sp_month = dt_new.month\n\t\tself.sp_day = dt_new.day\n\t\tself.sp_hour = self.decHourJoin(dt_new.hour,dt_new.minute,dt_new.second)\n\t\tself.sp_geolon = self.geolon\n\t\tself.sp_geolat = self.geolat\n\t\tself.sp_altitude = self.altitude\n\t\tself.houses_override = [dt_new.year,dt_new.month,dt_new.day,self.hour]\n\n\t\tdprint(\"localToSecondaryProgression: got UTC %s-%s-%s %s:%s:%s\"%(\n\t\t\tdt_new.year,dt_new.month,dt_new.day,dt_new.hour,dt_new.minute,dt_new.second))\n\t\t\t\n\t\tself.type = \"SecondaryProgression\"\n\t\topenAstro.charttype=\"%s (%s-%02d-%02d %02d:%02d)\" % (openAstro.label[\"secondary_progressions\"],dt.year,dt.month,dt.day,dt.hour,dt.minute)\n\t\topenAstro.transit=False\n\t\treturn", "path": "openastro.py", "repo_name": "pascallemazurier/openastro-dev", "stars": 32, "license": "gpl-3.0", "language": "python", "size": 15945}
{"docstring": "#update chart data\n", "func_signal": "def eventDataApply(self, widget):\n", "code": "openAstro.charttype=openAstro.label[\"radix\"]\nopenAstro.type=\"Radix\"\nopenAstro.transit=False\nself.updateChartData()\n\n#update chart\nself.updateChart()", "path": "openastro.py", "repo_name": "pascallemazurier/openastro-dev", "stars": 32, "license": "gpl-3.0", "language": "python", "size": 15945}
{"docstring": "# create a new window\n", "func_signal": "def settingsAspects(self, widget):\n", "code": "self.win_SA = gtk.Dialog()\nself.win_SA.set_icon_from_file(cfg.iconWindow)\nself.win_SA.set_title(_(\"Aspect Settings\"))\nself.win_SA.connect(\"delete_event\", lambda w,e: self.win_SA.destroy())\nself.win_SA.move(150,150)\nself.win_SA.set_border_width(5)\nself.win_SA.set_size_request(600,500)\n\n#create a table\ntable = gtk.Table(len(openAstro.aspects)-3, 6, False)\ntable.set_col_spacings(0)\ntable.set_row_spacings(0)\ntable.set_border_width(10)\n\n#description\nlabel = gtk.Label(_(\"Deg\"))\ntable.attach(label, 1, 2, 0, 1, xoptions=gtk.FILL, xpadding=10)\nlabel = gtk.Label(_(\"Aspect Name\"))\ntable.attach(label, 2, 3, 0, 1, xoptions=gtk.FILL, xpadding=10)\nlabel = gtk.Label(_(\"Visible\\nin Circle\"))\ntable.attach(label, 3, 4, 0, 1, xoptions=gtk.FILL, xpadding=10)\nlabel = gtk.Label(_(\"Visible\\nin Grid\"))\ntable.attach(label, 4, 5, 0, 1, xoptions=gtk.FILL, xpadding=10)\nlabel = gtk.Label(_(\"Orb\"))\ntable.attach(label, 5, 6, 0, 1, xoptions=gtk.FILL, xpadding=10)\t\t\n\ndata = []\nx=1\nfor i in range(len(openAstro.aspects)):\n\t#0=degree, 1=name, 2=color, 3=is_major, 4=orb\n\tdata.append({})\n\tdata[-1]['icon'] = gtk.Image()\n\tfilename=os.path.join(cfg.iconAspects,str(openAstro.aspects[i]['degree'])+'.svg')\n\tdata[-1]['icon'].set_from_file(filename)\n\tdata[-1]['degree'] = openAstro.aspects[i]['degree']\n\tdata[-1]['degree_str'] = gtk.Label(str(openAstro.aspects[i]['degree']))\n\tdata[-1]['name'] = gtk.Entry()\n\tdata[-1]['name'].set_max_length(25)\n\tdata[-1]['name'].set_width_chars(15)\n\tdata[-1]['name'].set_text(openAstro.aspects[i]['name'])\n\ttable.attach(data[-1]['icon'], 0, 1, x, x+1, xoptions=gtk.FILL, xpadding=10)\n\ttable.attach(data[-1]['degree_str'], 1, 2, x, x+1, xoptions=gtk.FILL, xpadding=10)\n\ttable.attach(data[-1]['name'], 2, 3, x, x+1, xoptions=gtk.FILL, xpadding=10)\n\tdata[-1]['visible'] = gtk.CheckButton()\n\tif openAstro.aspects[i]['visible'] is 1:\n\t\tdata[-1]['visible'].set_active(True)\n\ttable.attach(data[-1]['visible'], 3, 4, x, x+1, xoptions=gtk.EXPAND, xpadding=2, ypadding=2)\n\tdata[-1]['visible_grid'] = gtk.CheckButton()\n\tif openAstro.aspects[i]['visible_grid'] is 1:\n\t\tdata[-1]['visible_grid'].set_active(True)\n\ttable.attach(data[-1]['visible_grid'], 4, 5, x, x+1, xoptions=gtk.EXPAND, xpadding=2, ypadding=2)\n\tdata[-1]['orb'] = gtk.Entry(4)\n\tdata[-1]['orb'].set_width_chars(4)\n\tdata[-1]['orb'].set_text(str(openAstro.aspects[i]['orb']))\n\ttable.attach(data[-1]['orb'], 5, 6, x, x+1, xoptions=gtk.FILL, xpadding=10)\t\t\t\t\t\n\tx=x+1\n\n#make the ui layout with ok button\nscrolledwindow = gtk.ScrolledWindow()\nscrolledwindow.set_border_width(5)\nscrolledwindow.set_policy(gtk.POLICY_AUTOMATIC, gtk.POLICY_AUTOMATIC)\nself.win_SA.vbox.pack_start(scrolledwindow, True, True, 0)\nscrolledwindow.add_with_viewport(table)\n\n#ok button\nbutton = gtk.Button(stock=gtk.STOCK_OK)\nbutton.connect(\"clicked\", self.settingsAspectsSubmit, data)\nbutton.set_flags(gtk.CAN_DEFAULT)\t\t\nself.win_SA.action_area.pack_start(button, True, True, 0)\nbutton.grab_default()\t\t\n\n#cancel button\nbutton = gtk.Button(stock=gtk.STOCK_CANCEL)\nbutton.connect(\"clicked\", lambda w: self.win_SA.destroy())\nself.win_SA.action_area.pack_start(button, True, True, 0)\n\nself.win_SA.show_all()\t\t\nreturn", "path": "openastro.py", "repo_name": "pascallemazurier/openastro-dev", "stars": 32, "license": "gpl-3.0", "language": "python", "size": 15945}
{"docstring": "#check if no changes were made\n", "func_signal": "def eventDataSubmit(self, widget):\n", "code": "if self.name.get_text() == openAstro.name and \\\nself.dateY.get_text() == str(openAstro.year_loc) and \\\nself.dateM.get_text() == '%(#)02d' % {'#':openAstro.month_loc} and \\\nself.dateD.get_text() == '%(#)02d' % {'#':openAstro.day_loc} and \\\nself.eH.get_text() == '%(#)02d' % {'#':openAstro.hour_loc} and \\\nself.eM.get_text() == '%(#)02d' % {'#':openAstro.minute_loc} and \\\nself.eS.get_text() == '%(#)02d' % {'#':openAstro.second_loc}:\n\tif self.iconn and \\\n\tself.geoCC.get_text() == openAstro.countrycode and \\\n\tself.geoLoc.get_text() == openAstro.location.partition(',')[0]:\n\t\t#go ahead ;)\t\t\t\t\n\t\tself.window2.destroy()\n\t\treturn\n\n#apply data\nself.eventDataApply( widget )\n\nif self.geoLocFound:\n\tself.window2.destroy()\n\t#update history\n\tdb.addHistory()\n\tself.updateUI()\n\treturn\nelse:\n\treturn", "path": "openastro.py", "repo_name": "pascallemazurier/openastro-dev", "stars": 32, "license": "gpl-3.0", "language": "python", "size": 15945}
{"docstring": "#look if location is known\n", "func_signal": "def getSettingsLocation(self):\n", "code": "if self.astrocfg.has_key('home_location') == False or self.astrocfg.has_key('home_timezonestr') == False:\n\tself.open()\t\t\t\n\tsql='INSERT OR REPLACE INTO astrocfg (name,value) VALUES(\"home_location\",\"\")'\n\tself.cursor.execute(sql)\n\tsql='INSERT OR REPLACE INTO astrocfg (name,value) VALUES(\"home_geolat\",\"\")'\n\tself.cursor.execute(sql)\n\tsql='INSERT OR REPLACE INTO astrocfg (name,value) VALUES(\"home_geolon\",\"\")'\n\tself.cursor.execute(sql)\n\tsql='INSERT OR REPLACE INTO astrocfg (name,value) VALUES(\"home_countrycode\",\"\")'\n\tself.cursor.execute(sql)\n\tsql='INSERT OR REPLACE INTO astrocfg (name,value) VALUES(\"home_timezonestr\",\"\")'\n\tself.cursor.execute(sql)\t\t\t\n\tself.link.commit()\n\tself.close\n\treturn '','','','',''\nelse:\n\treturn self.astrocfg['home_location'],self.astrocfg['home_geolat'],self.astrocfg['home_geolon'],self.astrocfg['home_countrycode'],self.astrocfg['home_timezonestr']", "path": "openastro.py", "repo_name": "pascallemazurier/openastro-dev", "stars": 32, "license": "gpl-3.0", "language": "python", "size": 15945}
{"docstring": "#pie slices\n", "func_signal": "def zodiacSlice( self , num , r , style,  type):\n", "code": "if db.astrocfg[\"houses_system\"] == \"G\":\n\toffset = 360 - self.houses_degree_ut[18]\nelse:\n\toffset = 360 - self.houses_degree_ut[6]\n#check transit\nif self.type == \"Transit\":\n\tdropin=0\nelse:\n\tdropin=self.c1\t\t\nslice = '<path d=\"M' + str(r) + ',' + str(r) + ' L' + str(dropin + self.sliceToX(num,r-dropin,offset)) + ',' + str( dropin + self.sliceToY(num,r-dropin,offset)) + ' A' + str(r-dropin) + ',' + str(r-dropin) + ' 0 0,0 ' + str(dropin + self.sliceToX(num+1,r-dropin,offset)) + ',' + str(dropin + self.sliceToY(num+1,r-dropin,offset)) + ' z\" style=\"' + style + '\"/>'\n#symbols\noffset = offset + 15\n#check transit\nif self.type == \"Transit\":\n\tdropin=54\nelse:\n\tdropin=18+self.c1\nsign = '<g transform=\"translate(-16,-16)\"><use x=\"' + str(dropin + self.sliceToX(num,r-dropin,offset)) + '\" y=\"' + str(dropin + self.sliceToY(num,r-dropin,offset)) + '\" xlink:href=\"#' + type + '\" /></g>\\n'\nreturn slice + '\\n' + sign", "path": "openastro.py", "repo_name": "pascallemazurier/openastro-dev", "stars": 32, "license": "gpl-3.0", "language": "python", "size": 15945}
{"docstring": "#make local time variables from global UTC\n", "func_signal": "def utcToLocal(self):\n", "code": "h, m, s = self.decHour(self.hour)\nutc = datetime.datetime(self.year, self.month, self.day, h, m, s)\ntz = datetime.timedelta(seconds=float(self.timezone)*float(3600))\nloc = utc + tz\nself.year_loc = loc.year\nself.month_loc = loc.month\nself.day_loc = loc.day\nself.hour_loc = loc.hour\nself.minute_loc = loc.minute\nself.second_loc = loc.second\n#print some info\ndprint('utcToLocal: '+str(utc)+' => '+str(loc)+self.decTzStr(self.timezone))", "path": "openastro.py", "repo_name": "pascallemazurier/openastro-dev", "stars": 32, "license": "gpl-3.0", "language": "python", "size": 15945}
{"docstring": "# create a new window\n", "func_signal": "def settingsPlanets(self, obj):\n", "code": "self.win_SP = gtk.Dialog()\nself.win_SP.set_icon_from_file(cfg.iconWindow)\nself.win_SP.set_title(_(\"Planets & Angles Settings\"))\nself.win_SP.connect(\"delete_event\", lambda w,e: self.win_SP.destroy())\nself.win_SP.move(150,150)\nself.win_SP.set_border_width(5)\nself.win_SP.set_size_request(600,600)\n\n#create a table\ntable = gtk.Table(len(openAstro.planets)-3, 4, False)\ntable.set_border_width(10)\ntable.set_col_spacings(0)\ntable.set_row_spacings(0)\n\n#description\ntable.set_row_spacing(0,8)\nlabel = gtk.Label(_(\"Planet Label\"))\ntable.attach(label, 0, 1, 0, 1)\nlabel = gtk.Label(_(\"Symbol\"))\ntable.attach(label, 1, 2, 0, 1, xoptions=gtk.SHRINK, xpadding=10)\nlabel = gtk.Label(_(\"Aspect Line\"))\ntable.attach(label, 2, 3, 0, 1, xoptions=gtk.SHRINK, xpadding=10)\nlabel = gtk.Label(_(\"Aspect Grid\"))\ntable.attach(label, 3, 4, 0, 1, xoptions=gtk.SHRINK, xpadding=10)\n\t\t\t\t\ndata = []\nx=1\nfor i in range(len(openAstro.planets)):\n\t#planets to skip: 11=true node, 13=osc. apogee, 14=earth, 21=intp. apogee, 22=intp. perigee\n\t#angles: 23=Asc, 24=Mc, 25=Ds, 26=Ic\n\t#points: 27=pars fortuna\n\tif i is 11 or i is 13 or i is 14 or i is 21 or i is 22:\n\t\tcontinue\n\t#start of the angles\t\t\t\n\tif i is 23 or i is 27:\n\t\ttable.set_row_spacing(x-1,20)\n\t\ttable.set_row_spacing(x,8)\n\t\tif i is 27:\n\t\t\tlabel = gtk.Label(_(\"Point Label\"))\n\t\telse:\n\t\t\tlabel = gtk.Label(_(\"Angle Label\"))\t\n\t\ttable.attach(label, 0, 1, x, x+1, xoptions=gtk.SHRINK, xpadding=10)\n\t\tlabel = gtk.Label(_(\"Symbol\"))\n\t\ttable.attach(label, 1, 2, x, x+1, xoptions=gtk.SHRINK, xpadding=10)\n\t\tlabel = gtk.Label(_(\"Aspect Line\"))\n\t\ttable.attach(label, 2, 3, x, x+1, xoptions=gtk.SHRINK, xpadding=10)\n\t\tlabel = gtk.Label(_(\"Aspect Grid\"))\n\t\ttable.attach(label, 3, 4, x, x+1, xoptions=gtk.SHRINK, xpadding=10)\t\t\t\t\n\t\tx=x+1\n\tdata.append({})\n\tdata[-1]['id'] = openAstro.planets[i]['id']\n\tdata[-1]['label'] = gtk.Entry()\n\tdata[-1]['label'].set_max_length(25)\n\tdata[-1]['label'].set_width_chars(15)\n\tdata[-1]['label'].set_text(openAstro.planets[i]['label'])\n\t#data[-1]['label'].set_alignment(xalign=0.0, yalign=0.5)\n\ttable.attach(data[-1]['label'], 0, 1, x, x+1, xoptions=gtk.SHRINK, xpadding=10)\n\tdata[-1]['visible'] = gtk.CheckButton()\n\tif openAstro.planets[i]['visible'] is 1:\n\t\tdata[-1]['visible'].set_active(True)\n\ttable.attach(data[-1]['visible'], 1, 2, x, x+1, xoptions=gtk.SHRINK, xpadding=2, ypadding=2)\n\t\n\tdata[-1]['visible_aspect_line'] = gtk.CheckButton()\n\tif openAstro.planets[i]['visible_aspect_line'] is 1:\n\t\tdata[-1]['visible_aspect_line'].set_active(True)\n\ttable.attach(data[-1]['visible_aspect_line'], 2, 3, x, x+1, xoptions=gtk.SHRINK, xpadding=2, ypadding=2)\t\n\t\n\tdata[-1]['visible_aspect_grid'] = gtk.CheckButton()\n\tif openAstro.planets[i]['visible_aspect_grid'] is 1:\n\t\tdata[-1]['visible_aspect_grid'].set_active(True)\n\ttable.attach(data[-1]['visible_aspect_grid'], 3, 4, x, x+1, xoptions=gtk.SHRINK, xpadding=2, ypadding=2)\t\n\tx=x+1\n\n#make the ui layout with ok button\nscrolledwindow = gtk.ScrolledWindow()\nscrolledwindow.set_border_width(5)\nscrolledwindow.set_policy(gtk.POLICY_AUTOMATIC, gtk.POLICY_AUTOMATIC)\nself.win_SP.vbox.pack_start(scrolledwindow, True, True, 0)\nscrolledwindow.add_with_viewport(table)\n\n#ok button\nbutton = gtk.Button(stock=gtk.STOCK_OK)\nbutton.connect(\"clicked\", self.settingsPlanetsSubmit, data)\nbutton.set_flags(gtk.CAN_DEFAULT)\t\t\nself.win_SP.action_area.pack_start(button, True, True, 0)\nbutton.grab_default()\n\n#cancel button\nbutton = gtk.Button(stock=gtk.STOCK_CANCEL)\nbutton.connect(\"clicked\", lambda w: self.win_SP.destroy())\nself.win_SP.action_area.pack_start(button, True, True, 0)\n\nself.win_SP.show_all()\t\t\nreturn", "path": "openastro.py", "repo_name": "pascallemazurier/openastro-dev", "stars": 32, "license": "gpl-3.0", "language": "python", "size": 15945}
{"docstring": "\"\"\"\nexamples:\n@0102  ; Astrolog chart info.\n/qb 6 23 1972  3:00:00 ST -1:00   5:24:00E 43:18:00N\n/zi \"Zinedine Zidane\" \"Marseille\"\n\n@0102  ; Astrolog32 chart info.\n\n; Date is in American format: month day year.\n\n/qb 10 27 1980 10:20:00 ST -1:00  14:39'00E 50:11'00N\n/zi \"Honzik\" \"Brandys nad Labem\"\n\n\"\"\"\n", "func_signal": "def getAstrolog32(filename):\n", "code": "d={}\nh=open(filename)\nf=EncodedFile(h,\"utf-8\",\"latin-1\")\nfor line in f.readlines():\n\tif line[0:3] == \"/qb\":\n\t\ts0=line.strip().split(' ')\n\t\ts=[]\n\t\tfor j in range(len(s0)):\n\t\t\tif s0[j]!='':\n\t\t\t\ts.append(s0[j])\n\t\td['month']=s[1]\n\t\td['day']=s[2]\n\t\td['year']=s[3]\n\t\td['hour'],d['minute'],d['second']=0,0,0\n\t\tfor x in range(len(s[4].split(':'))):\n\t\t\tif x == 0:\n\t\t\t\td['hour'] = s[4].split(':')[0]\n\t\t\tif x == 1:\n\t\t\t\td['minute'] = s[4].split(':')[1]\n\t\t\tif x == 2:\n\t\t\t\td['second'] = s[4].split(':')[2]\n\n\t\t#timezone\n\t\ttz=s[6].split(':')\n\t\td['timezone']=float(tz[0])+float(tz[1])/60.0\n\t\tif float(tz[0]) < 0:\n\t\t\td['timezone']=d['timezone']/-1.0\n\t\t#longitude\n\t\tlon=s[7].split(':')\n\t\tlon.append(lon[-1][-1])\n\t\tlon[-2]=lon[-2][0:2]\n\t\td['longitude']=float(lon[0])+(float(lon[1])/60.0)\n\t\tif len(lon) > 3:\n\t\t\td['longitude']+=float(lon[2])/3600.0\n\t\tif lon[-1] == 'W':\n\t\t\td['longitude'] = d['longitude']/-1.0\n\t\t#latitude\n\t\tlon=s[8].split(':')\n\t\tlon.append(lon[-1][-1])\n\t\tlon[-2]=lon[-2][0:2]\n\t\td['latitude']=float(lon[0])+(float(lon[1])/60.0)\n\t\tif len(lon) > 3:\n\t\t\td['latitude']+=float(lon[2])/3600.0\n\t\tif lon[-1] == 'S':\n\t\t\td['latitude'] = d['latitude']/-1.0\t\t\t\n\t\t\n\tif line[0:3] == \"/zi\":\n\t\ts0=line.strip().split('\"')\n\t\ts=[]\n\t\tfor j in range(len(s0)):\n\t\t\tif s0[j] != '' and s0[j] != ' ':\n\t\t\t\ts.append(s0[j])\n\t\td['name']=s[1]\n\t\td['location']=s[2]\nf.close()\nreturn [d]", "path": "openastromod\\importfile.py", "repo_name": "pascallemazurier/openastro-dev", "stars": 32, "license": "gpl-3.0", "language": "python", "size": 15945}
{"docstring": "#check for duplicate name\t\n", "func_signal": "def eventDataSaveAsk(self, widget):\n", "code": "en = db.getDatabase()\nfor i in range(len(en)):\n\tif en[i][\"name\"] == self.name.get_text():\n\t\tdialog=gtk.Dialog(_('Duplicate'),self.window2,0,(gtk.STOCK_OK, gtk.RESPONSE_DELETE_EVENT))\n\t\tdialog.set_icon_from_file(cfg.iconWindow)\t\t\t\n\t\tdialog.connect(\"response\", lambda w,e: dialog.destroy())\t\t\t\t\n\t\tdialog.connect(\"close\", lambda w,e: dialog.destroy())\n\t\tdialog.vbox.pack_start(gtk.Label(_('There is allready an entry for this name, please choose another')),True,True,0)\n\t\tdialog.show_all()\t\t\t\t\n\t\treturn\n#ask for confirmation\ndialog=gtk.Dialog(_('Question'),self.window2,gtk.DIALOG_DESTROY_WITH_PARENT,(gtk.STOCK_CANCEL, gtk.RESPONSE_REJECT, gtk.STOCK_OK, gtk.RESPONSE_ACCEPT))\ndialog.set_icon_from_file(cfg.iconWindow)\ndialog.connect(\"close\", lambda w,e: dialog.destroy())\ndialog.connect(\"response\",self.eventDataSave)\ndialog.vbox.pack_start(gtk.Label(_('Are you sure you want to save this entry to the database?')),True,True,0)\ndialog.show_all()\t\nreturn", "path": "openastro.py", "repo_name": "pascallemazurier/openastro-dev", "stars": 32, "license": "gpl-3.0", "language": "python", "size": 15945}
{"docstring": "#check for duplicate name without duplicate id\n", "func_signal": "def openDatabaseEditAsk(self, widget):\n", "code": "en = db.getDatabase()\nfor i in range(len(en)):\n\tif en[i][\"name\"] == self.name.get_text() and self.oDE_list[\"id\"] != en[i][\"id\"]:\n\t\tdialog=gtk.Dialog(_('Duplicate'),self.window2,0,(gtk.STOCK_OK, gtk.RESPONSE_DELETE_EVENT))\n\t\tdialog.set_icon_from_file(cfg.iconWindow)\t\t\t\n\t\tdialog.connect(\"response\", lambda w,e: dialog.destroy())\t\t\t\t\n\t\tdialog.connect(\"close\", lambda w,e: dialog.destroy())\n\t\tdialog.vbox.pack_start(gtk.Label(_('There is allready an entry for this name, please choose another')),True,True,0)\n\t\tdialog.show_all()\t\t\t\t\n\t\treturn\n#ask for confirmation\ndialog=gtk.Dialog(_('Question'),self.window2,gtk.DIALOG_DESTROY_WITH_PARENT,(gtk.STOCK_CANCEL, gtk.RESPONSE_REJECT, gtk.STOCK_OK, gtk.RESPONSE_ACCEPT))\ndialog.set_icon_from_file(cfg.iconWindow)\ndialog.connect(\"close\", lambda w,e: dialog.destroy())\ndialog.connect(\"response\",self.openDatabaseEditSave)\ndialog.vbox.pack_start(gtk.Label(_('Are you sure you want to Save?')),True,True,0)\ndialog.show_all()\t\nreturn", "path": "openastro.py", "repo_name": "pascallemazurier/openastro-dev", "stars": 32, "license": "gpl-3.0", "language": "python", "size": 15945}
{"docstring": "#default location\n", "func_signal": "def eventDataNew(self, widget):\n", "code": "openAstro.location=openAstro.home_location\nopenAstro.geolat=float(openAstro.home_geolat)\nopenAstro.geolon=float(openAstro.home_geolon)\nopenAstro.countrycode=openAstro.home_countrycode\n\n#timezone string, example Europe/Amsterdam\nnow = datetime.datetime.now()\nopenAstro.timezone_str = zonetab.nearest_tz(openAstro.geolat,openAstro.geolon,zonetab.timezones())[2]\n#aware datetime object\ndt = zonetab.stdtime(openAstro.timezone_str, now.year, now.month, now.day, now.hour, now.minute, now.second)\n#naive utc datetime object\ndt_utc = dt.replace(tzinfo=None) - dt.utcoffset()\n\n#Default\nopenAstro.name=_(\"New Chart\")\nopenAstro.charttype=openAstro.label[\"radix\"]\nopenAstro.year=dt_utc.year\nopenAstro.month=dt_utc.month\nopenAstro.day=dt_utc.day\nopenAstro.hour=openAstro.decHourJoin(dt_utc.hour,dt_utc.minute,dt_utc.second)\nopenAstro.timezone=float( (dt.utcoffset().days * 24) + (dt.utcoffset().seconds/3600) )\n\n#Make locals\nopenAstro.utcToLocal()\n\n#open editor\nself.eventData(widget, edit=False)\nreturn", "path": "openastro.py", "repo_name": "pascallemazurier/openastro-dev", "stars": 32, "license": "gpl-3.0", "language": "python", "size": 15945}
{"docstring": "# create a new window\n", "func_signal": "def specialSolar(self, widget):\n", "code": "self.win_SS = gtk.Dialog()\nself.win_SS.set_icon_from_file(cfg.iconWindow)\nself.win_SS.set_title(_(\"Select year for Solar Return\"))\nself.win_SS.connect(\"delete_event\", lambda w,e: self.win_SS.destroy())\nself.win_SS.move(150,150)\nself.win_SS.set_border_width(5)\nself.win_SS.set_size_request(400,100)\n\n#create a table\ntable = gtk.Table(2, 1, False)\ntable.set_col_spacings(0)\ntable.set_row_spacings(0)\ntable.set_border_width(10)\n\n#options\ntable.attach(gtk.Label(_(\"Select year for Solar Return\")), 0, 1, 0, 1, xoptions=gtk.SHRINK, yoptions=gtk.SHRINK, xpadding=10)\nentry=gtk.Entry(4)\nentry.set_width_chars(4) \nentry.set_text(str(datetime.datetime.now().year))\ntable.attach(entry, 1, 2, 0, 1, xoptions=gtk.SHRINK, yoptions=gtk.SHRINK, xpadding=10)\n\n#make the ui layout with ok button\nself.win_SS.vbox.pack_start(table, True, True, 0)\n\n#ok button\nbutton = gtk.Button(stock=gtk.STOCK_OK)\nbutton.connect(\"clicked\", self.specialSolarSubmit, entry)\nbutton.set_flags(gtk.CAN_DEFAULT)\t\t\nself.win_SS.action_area.pack_start(button, True, True, 0)\nbutton.grab_default()\t\t\n\n#cancel button\nbutton = gtk.Button(stock=gtk.STOCK_CANCEL)\nbutton.connect(\"clicked\", lambda w: self.win_SS.destroy())\nself.win_SS.action_area.pack_start(button, True, True, 0)\n\nself.win_SS.show_all()\nreturn", "path": "openastro.py", "repo_name": "pascallemazurier/openastro-dev", "stars": 32, "license": "gpl-3.0", "language": "python", "size": 15945}
{"docstring": "# create a new window\n", "func_signal": "def specialSecondaryProgression(self, widget):\n", "code": "self.win_SSP = gtk.Dialog()\nself.win_SSP.set_icon_from_file(cfg.iconWindow)\nself.win_SSP.set_title(_(\"Enter Date\"))\nself.win_SSP.connect(\"delete_event\", lambda w,e: self.win_SSP.destroy())\nself.win_SSP.move(150,150)\nself.win_SSP.set_border_width(5)\nself.win_SSP.set_size_request(350,180)\n\n#create a table\ntable = gtk.Table(1, 4, False)\ntable.set_col_spacings(0)\ntable.set_row_spacings(0)\ntable.set_border_width(10)\n\n#options\ntable.attach(gtk.Label(_(\"Select date for Secondary Progression\")+\":\"), 0, 1, 0, 1, xoptions=gtk.SHRINK, yoptions=gtk.SHRINK, xpadding=10, ypadding=10)\nhbox = gtk.HBox(spacing=4)  # pack_start(child, expand=True, fill=True, padding=0)\nentry={}\n\nhbox.pack_start(gtk.Label(_('Year')+\": \"))\t\nentry['Y']=gtk.Entry(4)\nentry['Y'].set_width_chars(4) \nentry['Y'].set_text(str(datetime.datetime.now().year))\nhbox.pack_start(entry['Y'])\nhbox.pack_start(gtk.Label(_('Month')+\": \"))\t\nentry['M']=gtk.Entry(2)\nentry['M'].set_width_chars(2) \nentry['M'].set_text('%02d'%(datetime.datetime.now().month))\nhbox.pack_start(entry['M'])\nhbox.pack_start(gtk.Label(_('Day')+\": \"))\t\nentry['D']=gtk.Entry(2)\nentry['D'].set_width_chars(2) \nentry['D'].set_text(str(datetime.datetime.now().day))\nhbox.pack_start(entry['D'])\t\ntable.attach(hbox,0,1,1,2, xoptions=gtk.SHRINK, yoptions=gtk.SHRINK, xpadding=10, ypadding=10)\n\nhbox = gtk.HBox(spacing=4)\nhbox.pack_start(gtk.Label(_('Hour')+\": \"))\t\nentry['h']=gtk.Entry(2)\nentry['h'].set_width_chars(2) \nentry['h'].set_text('%02d'%(datetime.datetime.now().hour))\nhbox.pack_start(entry['h'])\nhbox.pack_start(gtk.Label(_('Min')+\": \"))\t\nentry['m']=gtk.Entry(2)\nentry['m'].set_width_chars(2) \nentry['m'].set_text('%02d'%(datetime.datetime.now().minute))\nhbox.pack_start(entry['m'])\ntable.attach(hbox,0,1,2,3, xoptions=gtk.SHRINK, yoptions=gtk.SHRINK, xpadding=10, ypadding=10)\n\n#make the ui layout with ok button\nself.win_SSP.vbox.pack_start(table, True, True, 0)\n\n#ok button\nbutton = gtk.Button(stock=gtk.STOCK_OK)\nbutton.connect(\"clicked\", self.specialSecondaryProgressionSubmit, entry)\nbutton.set_flags(gtk.CAN_DEFAULT)\t\t\nself.win_SSP.action_area.pack_start(button, True, True, 0)\nbutton.grab_default()\t\t\n\n#cancel button\nbutton = gtk.Button(stock=gtk.STOCK_CANCEL)\nbutton.connect(\"clicked\", lambda w: self.win_SSP.destroy())\nself.win_SSP.action_area.pack_start(button, True, True, 0)\n\nself.win_SSP.show_all()\nreturn", "path": "openastro.py", "repo_name": "pascallemazurier/openastro-dev", "stars": 32, "license": "gpl-3.0", "language": "python", "size": 15945}
{"docstring": "#check for internet connection\n", "func_signal": "def updateChartData(self):\n", "code": "if self.iconn:\n\tresult = geoname.search(self.geoLoc.get_text(),self.geoCC.get_text())\n\tif result:\n\t\tself.geoLocFound = True\n\t\tlat=float(result[0]['lat'])\n\t\tlon=float(result[0]['lng'])\n\t\tgid=int(result[0]['geonameId'])\n\t\tcc=result[0]['countryCode']\n\t\ttzstr=result[0]['timezonestr']\n\t\tloc='%s, %s' % (result[0]['name'],result[0]['countryName'])\n\t\tdprint('updateChartData: %s,%s found; %s %s %s' % (\n\t\t\tself.geoLoc.get_text(),self.geoCC.get_text(),lat,lon,loc))\n\telse:\n\t\tself.geoLocFound = False\n\t\t#revert to defaults\n\t\tlat=openAstro.geolat\n\t\tlon=openAstro.geolon\n\t\tloc=openAstro.location\n\t\tcc=openAstro.countrycode\n\t\ttzstr=openAstro.timezonestr\n\t\tgid=openAstro.geonameid\n\t\tdprint('updateChartData: %s,%s not found, reverting to defaults' % (\n\t\t\tself.geoLoc.get_text(),self.geoCC.get_text()) )\n\t\tself.geoLoc.set_text(_('City not found! Try Again.'))\n\t\treturn\nelse:\n\t#using geonames database\n\tself.geoLocFound = True\n\tlat = float(self.GEON_lat)\n\tlon = float(self.GEON_lon)\n\tloc = self.GEON_loc\n\tcc = self.GEON_cc\n\ttzstr = self.GEON_tzstr\n\tgid = self.GEON_id\n\n#calculate timezone\nopenAstro.timezonestr = tzstr\nopenAstro.geonameid = gid\n\n#aware datetime object local time (with timezone info)\ndt = zonetab.stdtime(openAstro.timezonestr, int(self.dateY.get_text()), int(self.dateM.get_text()), int(self.dateD.get_text()), int(self.eH.get_text()), int(self.eM.get_text()), int(self.eS.get_text()))\n\n#naive datetime object UTC\ndt_utc = dt.replace(tzinfo=None) - dt.utcoffset()\n\n#set globals\nopenAstro.year = dt_utc.year\nopenAstro.month = dt_utc.month\nopenAstro.day = dt_utc.day\nopenAstro.hour = openAstro.decHourJoin(dt_utc.hour, dt_utc.minute, dt_utc.second)\nopenAstro.timezone = float( (dt.utcoffset().days * 24) + (dt.utcoffset().seconds / 3600) )\nopenAstro.name = self.name.get_text()\n\n#location\nopenAstro.geolat=lat\nopenAstro.geolon=lon\nopenAstro.location=loc\nopenAstro.countrycode=cc\n\n#update local time\nopenAstro.utcToLocal()\n\n#update labels\nlabelDateStr = str(openAstro.year_loc)+'-%(#1)02d-%(#2)02d' % {'#1':openAstro.month_loc,'#2':openAstro.day_loc}\t\t\nself.labelDate.set_text(labelDateStr)\t\t\nlabelTzStr = '%(#1)02d:%(#2)02d:%(#3)02d' % {'#1':openAstro.hour_loc,'#2':openAstro.minute_loc,'#3':openAstro.second_loc} + openAstro.decTzStr(openAstro.timezone)\t\t\t\t\nself.labelTz.set_text(labelTzStr)\nself.ename.set_text(openAstro.name)\nself.entry2.set_text(' %s: %s\\n %s: %s\\n %s: %s' % ( _('Latitude'),lat,_('Longitude'),lon,_('Location'),loc) )", "path": "openastro.py", "repo_name": "pascallemazurier/openastro-dev", "stars": 32, "license": "gpl-3.0", "language": "python", "size": 15945}
{"docstring": "\"\"\"\n* Stellium: At least four planets linked together in a series of continuous conjunctions.\n    \t* Grand trine: Three trine aspects together.\n* Grand cross: Two pairs of opposing planets squared to each other.\n* T-Square: Two planets in opposition squared to a third. \n* Yod: Two qunicunxes together joined by a sextile. \n\"\"\"\n", "func_signal": "def makePatterns( self ):\n", "code": "conj = {} #0\nopp = {} #10\nsq = {} #5\ntr = {} #6\nqc = {} #9\nsext = {} #3\nfor i in range(len(self.planets)):\n\ta=self.planets_degree_ut[i]\n\tqc[i]={}\n\tsext[i]={}\n\topp[i]={}\n\tsq[i]={}\n\ttr[i]={}\n\tconj[i]={}\n\t#skip some points\n\tn = self.planets[i]['name']\n\tif n == 'earth' or n == 'true node' or n == 'osc. apogee' or n == 'intp. apogee' or n == 'intp. perigee':\n\t\tcontinue\n\tif n == 'Dsc' or n == 'Ic':\n\t\tcontinue\n\tfor j in range(len(self.planets)):\n\t\t#skip some points\n\t\tn = self.planets[j]['name']\n\t\tif n == 'earth' or n == 'true node' or n == 'osc. apogee' or n == 'intp. apogee' or n == 'intp. perigee':\n\t\t\tcontinue\t\n\t\tif n == 'Dsc' or n == 'Ic':\n\t\t\tcontinue\n\t\tb=self.planets_degree_ut[j]\n\t\tdelta=float(self.degreeDiff(a,b))\n\t\t#check for opposition\n\t\txa = float(self.aspects[10]['degree']) - float(self.aspects[10]['orb'])\n\t\txb = float(self.aspects[10]['degree']) + float(self.aspects[10]['orb'])\n\t\tif( xa <= delta <= xb ):\n\t\t\topp[i][j]=True\t\n\t\t#check for conjunction\n\t\txa = float(self.aspects[0]['degree']) - float(self.aspects[0]['orb'])\n\t\txb = float(self.aspects[0]['degree']) + float(self.aspects[0]['orb'])\n\t\tif( xa <= delta <= xb ):\n\t\t\tconj[i][j]=True\t\t\t\t\t\n\t\t#check for squares\n\t\txa = float(self.aspects[5]['degree']) - float(self.aspects[5]['orb'])\n\t\txb = float(self.aspects[5]['degree']) + float(self.aspects[5]['orb'])\n\t\tif( xa <= delta <= xb ):\n\t\t\tsq[i][j]=True\t\t\t\n\t\t#check for qunicunxes\n\t\txa = float(self.aspects[9]['degree']) - float(self.aspects[9]['orb'])\n\t\txb = float(self.aspects[9]['degree']) + float(self.aspects[9]['orb'])\n\t\tif( xa <= delta <= xb ):\n\t\t\tqc[i][j]=True\n\t\t#check for sextiles\n\t\txa = float(self.aspects[3]['degree']) - float(self.aspects[3]['orb'])\n\t\txb = float(self.aspects[3]['degree']) + float(self.aspects[3]['orb'])\n\t\tif( xa <= delta <= xb ):\n\t\t\tsext[i][j]=True\n\t\t\t\t\t\nyot={}\n#check for double qunicunxes\nfor k,v in qc.iteritems():\n\tif len(qc[k]) >= 2:\n\t\t#check for sextile\n\t\tfor l,w in qc[k].iteritems():\n\t\t\tfor m,x in qc[k].iteritems():\n\t\t\t\tif sext[l].has_key(m):\n\t\t\t\t\tif l > m:\n\t\t\t\t\t\tyot['%s,%s,%s' % (k,m,l)] = [k,m,l]\n\t\t\t\t\telse:\n\t\t\t\t\t\tyot['%s,%s,%s' % (k,l,m)] = [k,l,m]\ntsquare={}\n#check for opposition\nfor k,v in opp.iteritems():\n\tif len(opp[k]) >= 1:\n\t\t#check for square\n\t\tfor l,w in opp[k].iteritems():\n\t\t\t\tfor a,b in sq.iteritems():\n\t\t\t\t\tif sq[a].has_key(k) and sq[a].has_key(l):\n\t\t\t\t\t\t#print 'got tsquare %s %s %s' % (a,k,l)\n\t\t\t\t\t\tif k > l:\n\t\t\t\t\t\t\ttsquare['%s,%s,%s' % (a,l,k)] = '%s => %s, %s' % (\n\t\t\t\t\t\t\t\tself.planets[a]['label'],self.planets[l]['label'],self.planets[k]['label'])\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\ttsquare['%s,%s,%s' % (a,k,l)] = '%s => %s, %s' % (\n\t\t\t\t\t\t\t\tself.planets[a]['label'],self.planets[k]['label'],self.planets[l]['label'])\nstellium={}\n#check for 4 continuous conjunctions\t\nfor k,v in conj.iteritems():\n\tif len(conj[k]) >= 1:\n\t\t#first conjunction\n\t\tfor l,m in conj[k].iteritems():\n\t\t\tif len(conj[l]) >= 1:\n\t\t\t\tfor n,o in conj[l].iteritems():\n\t\t\t\t\t#skip 1st conj\n\t\t\t\t\tif n == k:\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tif len(conj[n]) >= 1:\n\t\t\t\t\t\t#third conjunction\n\t\t\t\t\t\tfor p,q in conj[n].iteritems():\n\t\t\t\t\t\t\t#skip first and second conj\n\t\t\t\t\t\t\tif p == k or p == n:\n\t\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\t\tif len(conj[p]) >= 1:\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t#fourth conjunction\n\t\t\t\t\t\t\t\tfor r,s in conj[p].iteritems():\n\t\t\t\t\t\t\t\t\t#skip conj 1,2,3\n\t\t\t\t\t\t\t\t\tif r == k or r == n or r == p:\n\t\t\t\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\tl=[k,n,p,r]\n\t\t\t\t\t\t\t\t\tl.sort()\n\t\t\t\t\t\t\t\t\tstellium['%s %s %s %s' % (l[0],l[1],l[2],l[3])]='%s %s %s %s' % (\n\t\t\t\t\t\t\t\t\t\tself.planets[l[0]]['label'],self.planets[l[1]]['label'],\n\t\t\t\t\t\t\t\t\t\tself.planets[l[2]]['label'],self.planets[l[3]]['label'])\n#print yots\nout='<g transform=\"translate(-30,380)\">'\nif len(yot) >= 1:\n\ty=0\n\tfor k,v in yot.iteritems():\n\t\tout += '<text y=\"%s\" style=\"fill:#000; font-size: 12px;\">%s</text>\\n' % (y,_(\"Yot\"))\n\t\t\n\t\t#first planet symbol\n\t\tout += '<g transform=\"translate(20,%s)\">' % (y)\n\t\tout += '<use transform=\"scale(0.4)\" x=\"0\" y=\"-20\" xlink:href=\"#%s\" /></g>\\n' % (\n\t\t\tself.planets[yot[k][0]]['name'])\n\t\t\n\t\t#second planet symbol\n\t\tout += '<g transform=\"translate(30,%s)\">'  % (y)\n\t\tout += '<use transform=\"scale(0.4)\" x=\"0\" y=\"-20\" xlink:href=\"#%s\" /></g>\\n' % (\n\t\t\tself.planets[yot[k][1]]['name'])\n\n\t\t#third planet symbol\n\t\tout += '<g transform=\"translate(40,%s)\">'  % (y)\n\t\tout += '<use transform=\"scale(0.4)\" x=\"0\" y=\"-20\" xlink:href=\"#%s\" /></g>\\n' % (\n\t\t\tself.planets[yot[k][2]]['name'])\n\t\t\n\t\ty=y+14\n#finalize\nout += '</g>'\t\t\n#return out\nreturn ''", "path": "openastro.py", "repo_name": "pascallemazurier/openastro-dev", "stars": 32, "license": "gpl-3.0", "language": "python", "size": 15945}
{"docstring": "#get name from selection\n", "func_signal": "def openDatabaseDel(self, widget):\n", "code": "model = self.win_OD_selection.get_selected()[0]\niter = self.win_OD_selection.get_selected()[1]\nfor i in range(len(self.DB)):\n\tif self.DB[i][\"id\"] == model.get_value(iter,3):\n\t\tself.ODDlist = self.DB[i]\nname = self.ODDlist[\"name\"]\ndialog=gtk.Dialog(_('Question'),self.win_OD,gtk.DIALOG_DESTROY_WITH_PARENT,(gtk.STOCK_CANCEL, gtk.RESPONSE_REJECT, gtk.STOCK_OK, gtk.RESPONSE_ACCEPT))\ndialog.connect(\"close\", lambda w,e: dialog.destroy())\ndialog.connect(\"response\",self.openDatabaseDelDo)\ndialog.vbox.pack_start(gtk.Label(_('Are you sure you want to delete')+' '+name+'?'),True,True,0)\ndialog.show_all()\nreturn", "path": "openastro.py", "repo_name": "pascallemazurier/openastro-dev", "stars": 32, "license": "gpl-3.0", "language": "python", "size": 15945}
{"docstring": "# create a new window\n", "func_signal": "def settingsLabel(self, obj):\n", "code": "self.win_SL = gtk.Dialog()\nself.win_SL.set_icon_from_file(cfg.iconWindow)\nself.win_SL.set_title(_(\"Label Settings\"))\nself.win_SL.connect(\"delete_event\", lambda w,e: self.win_SL.destroy())\nself.win_SL.move(150,150)\nself.win_SL.set_border_width(5)\nself.win_SL.set_size_request(600,600)\n\n#create a table\ntable = gtk.Table(len(openAstro.label), 3, False)\ntable.set_border_width(10)\ntable.set_col_spacings(0)\ntable.set_row_spacings(0)\n\n#description\ntable.set_row_spacing(0,8)\nlabel = gtk.Label(_(\"Label\"))\ntable.attach(label, 0, 1, 0, 1)\nlabel = gtk.Label(_(\"Value\"))\ntable.attach(label, 1, 2, 0, 1, xoptions=gtk.SHRINK, xpadding=10)\n\t\t\t\t\nself.SLdata = []\nx=1\nkeys = openAstro.label.keys()\nkeys.sort()", "path": "openastro.py", "repo_name": "pascallemazurier/openastro-dev", "stars": 32, "license": "gpl-3.0", "language": "python", "size": 15945}
{"docstring": "#check for zoom level\n", "func_signal": "def zoom(self, action, current):\n", "code": "if current.get_name() == 'z80':\n\topenAstro.zoom=0.8\nelif current.get_name() == 'z150':\n\topenAstro.zoom=1.5\nelif current.get_name() == 'z200':\n\topenAstro.zoom=2\nelse:\n\topenAstro.zoom=1\n\n#redraw svg\nopenAstro.makeSVG()\nself.draw.queue_draw()\nself.draw.setSVG(self.tempfilename)\nreturn", "path": "openastro.py", "repo_name": "pascallemazurier/openastro-dev", "stars": 32, "license": "gpl-3.0", "language": "python", "size": 15945}
{"docstring": "# DT[NUM=?n,RUL=409] -> NP \"'s\" NP[NUM=?n]\n# DT[NUM=?n,RUL=409] -> NP \"'\" NP[NUM=?n]\n\n", "func_signal": "def EvaluatePossessiveDeterminer(tree):\n", "code": "owner_application = Evaluate(tree[0])\ndef ApplyPossessee(noun_application, verb_application):\n  owned_ref = Referent()\n  def ApplyPossessor(owner_ref):\n    return DRS([], GetPossessionConditions(owner_ref, owned_ref))\n  requirements = noun_application(owned_ref)\n  requirements += owner_application(ApplyPossessor)\n  cond = ResolutionCondition(owned_ref, requirements, 'presuppose')\n  return DRS([], [cond]) + verb_application(owned_ref)\nreturn ApplyPossessee", "path": "src\\drt\\rules.py", "repo_name": "max99x/crystal", "stars": 34, "license": "None", "language": "python", "size": 4963}
{"docstring": "# S[TYP=ynq,RUL=8] -> AuxV[...,TYP=be] NP[...] PRED\n", "func_signal": "def EvaluatePredicateYesNoQuestion(tree):\n", "code": "tree = tree.copy(deep=True)\n\ntree[0].node = tree[0].node.copy()\ntree[0].node[cfg_parser._TYPE_FEATURE] = 'CV_991'\n\nvp_node = nltk.grammar.FeatStructNonterminal(tree[0].node)\nvp_node[cfg_parser._TYPE_FEATURE] = 'VP'\n\ntree[2:3] = [nltk.tree.Tree(vp_node, [tree[0], tree[2]])]\ndel tree[:1]\n\nreturn BooleanQuestionDRS(EvaluateSentence(tree))", "path": "src\\drt\\rules.py", "repo_name": "max99x/crystal", "stars": 34, "license": "None", "language": "python", "size": 4963}
{"docstring": "# S[TYP=ynq,RUL=7,+sealed] -> S[TYP=dcl,-sealed] Pnct[TYP=qst]\n", "func_signal": "def ConvertToQuestion(tree):\n", "code": "sentence = Evaluate(tree[0])\nif not isinstance(sentence, QuestionDRS):\n  sentence = BooleanQuestionDRS(sentence)\nreturn sentence", "path": "src\\drt\\rules.py", "repo_name": "max99x/crystal", "stars": 34, "license": "None", "language": "python", "size": 4963}
{"docstring": "# NP[NUM=sg,PER=3,RUL=311] -> DT[NUM=sg] AJP PN[NUM=sg]\n", "func_signal": "def EvaluateDeterminedProperNameWithAdjective(tree):\n", "code": "adjective_application = Evaluate(tree[1])\nproper_noun_application = Evaluate(tree[2])\ndef ApplyAdjectiveAndVerb(verb_application):\n  return proper_noun_application(\n    lambda ref: adjective_application(ref) + verb_application(ref))\nreturn ApplyAdjectiveAndVerb", "path": "src\\drt\\rules.py", "repo_name": "max99x/crystal", "stars": 34, "license": "None", "language": "python", "size": 4963}
{"docstring": "# Q[+sbj,RUL=904] -> 'whose' Noun\n", "func_signal": "def EvaluatePossessiveProformWithNoun(tree=None):\n", "code": "noun_application = Evaluate(tree[1])\ndef AssignOwner(verb_application):\n  owner_ref = Referent()\n  owned_ref = Referent()\n  own_conds = GetPossessionConditions(owner_ref, owned_ref)\n  \n  drs = noun_application(owned_ref) + verb_application(owned_ref)\n  drs += DRS([owned_ref, owner_ref], own_conds)\n  return SubjectQuestionDRS(drs, owner_ref)\nreturn AssignOwner", "path": "src\\drt\\rules.py", "repo_name": "max99x/crystal", "stars": 34, "license": "None", "language": "python", "size": 4963}
{"docstring": "# PN[NUM=?n,SEX=?s,RUL=601] -> Ttl[NUM=?n,SEX=?s] PrpN[NUM=?n,SEX=?s]\n", "func_signal": "def EvaluateCompoundProperName(tree):\n", "code": "tree = tree.copy(deep=True)\ntree[1][0] = tree[0][0] + ' ' + tree[1][0]\ntree.pop(0)\nreturn EvaluateSimpleProperName(tree)", "path": "src\\drt\\rules.py", "repo_name": "max99x/crystal", "stars": 34, "license": "None", "language": "python", "size": 4963}
{"docstring": "# Noun -> Noun Noun\n", "func_signal": "def EvaluateNounPhraseWithNounModifier(tree):\n", "code": "modifier_noun = Evaluate(tree[0])\nmain_noun = Evaluate(tree[1])\ndef ApplyModifier(ref):\n  modifier_ref = Referent()\n  modification_condition = PredicateCondition('_modify', modifier_ref, ref)\n  modification_drs = DRS([modifier_ref], [modification_condition])\n  return modification_drs + main_noun(ref) + modifier_noun(modifier_ref)\nreturn ApplyModifier", "path": "src\\drt\\rules.py", "repo_name": "max99x/crystal", "stars": 34, "license": "None", "language": "python", "size": 4963}
{"docstring": "# Q[+sbj,RUL=905] -> 'whose' AJP Noun\n", "func_signal": "def EvaluatePossessiveProformWithNounAndAdjective(tree=None):\n", "code": "adjective_application = Evaluate(tree[1])\nnoun_application = Evaluate(tree[2])\ndef AssignOwner(verb_application):\n  owner_ref = Referent()\n  owned_ref = Referent()\n  own_conds = GetPossessionConditions(owner_ref, owned_ref)\n  \n  drs = noun_application(owned_ref)\n  drs += adjective_application(owned_ref)\n  drs += verb_application(owned_ref)\n  drs += DRS([owned_ref, owner_ref], own_conds)\n  return SubjectQuestionDRS(drs, owner_ref)\nreturn AssignOwner", "path": "src\\drt\\rules.py", "repo_name": "max99x/crystal", "stars": 34, "license": "None", "language": "python", "size": 4963}
{"docstring": "# DT[NUM=?n,RUL=406] -> Det[NUM=?n,TYP=neg]\n", "func_signal": "def EvaluateNegativeDeterminer(unused_tree=None):\n", "code": "def CreateNegator(noun_application, verb_application):\n  ref = Referent()\n  antecedent = DRS([ref]) + noun_application(ref)\n  consequent = DRS([], [NegationCondition(verb_application(ref))])\n  return DRS([], [ImplicationCondition(antecedent, consequent)])\nreturn CreateNegator", "path": "src\\drt\\rules.py", "repo_name": "max99x/crystal", "stars": 34, "license": "None", "language": "python", "size": 4963}
{"docstring": "# Q[+sbj,RUL=902] -> 'what' Noun\n", "func_signal": "def EvaluateProformWithNoun(tree=None):\n", "code": "noun_application = Evaluate(tree[1])\nreturn MakeQuestionDeterminer(noun_application)", "path": "src\\drt\\rules.py", "repo_name": "max99x/crystal", "stars": 34, "license": "None", "language": "python", "size": 4963}
{"docstring": "# NP[NUM=?n,PER=3,RUL=308] -> DT[NUM=?n] Noun[NUM=?n]\n", "func_signal": "def EvaluateDeterminedNoun(tree):\n", "code": "ref_generator = Evaluate(tree[0])\nnoun_application = Evaluate(tree[1])\ndef ApplyVerb(verb_application):\n  return ref_generator(noun_application, verb_application)\nreturn ApplyVerb", "path": "src\\drt\\rules.py", "repo_name": "max99x/crystal", "stars": 34, "license": "None", "language": "python", "size": 4963}
{"docstring": "# Prover9 treats u-z as vars, not consts as we want them.\n", "func_signal": "def __init__(self, type=SINGULAR_TYPE):\n", "code": "assert type not in ('u', 'v', 'w', 'x', 'y', 'z')\nself.type = type\nself.index = self.__class__.ref_index\nself.__class__.ref_index += 1", "path": "src\\drt\\drs.py", "repo_name": "max99x/crystal", "stars": 34, "license": "None", "language": "python", "size": 4963}
{"docstring": "# NP[NUM=ms,PER=3,RUL=319] -> AJP Noun[NUM=pl]\n# NP[NUM=ms,PER=3,RUL=320] -> AJP Noun[NUM=ms]\n", "func_signal": "def EvaluateUndeterminedNounWithAdjective(tree):\n", "code": "ref_generator = EvaluateIndefiniteArticle()\nadjective_application = Evaluate(tree[0])\nnoun_application = Evaluate(tree[1])\ncompound_application = lambda ref: (adjective_application(ref) +\n                                    noun_application(ref))\ndef ApplyVerb(verb_application):\n  return ref_generator(compound_application, verb_application)\nreturn ApplyVerb", "path": "src\\drt\\rules.py", "repo_name": "max99x/crystal", "stars": 34, "license": "None", "language": "python", "size": 4963}
{"docstring": "# NP[NUM=pl,PER=3,RUL=306] -> Noun[NUM=pl]\n# NP[NUM=ms,PER=3,RUL=307] -> Noun[NUM=ms]\n", "func_signal": "def EvaluateUndeterminedNoun(tree):\n", "code": "ref_generator = EvaluateIndefiniteArticle()\nnoun_application = Evaluate(tree[0])\ndef ApplyVerb(verb_application):\n  return ref_generator(noun_application, verb_application)\nreturn ApplyVerb", "path": "src\\drt\\rules.py", "repo_name": "max99x/crystal", "stars": 34, "license": "None", "language": "python", "size": 4963}
{"docstring": "# NP[NUM=?n,CASE=obj,PER=?p,RUL=318] -> Pro[NUM=?n,CASE=poss_pred,PER=?p]\n", "func_signal": "def EvaluatePossessivePredicatePronoun(tree):\n", "code": "presuppose_pronoun = EvaluatePronoun(tree)\ndef PresupposeOwnedObject(verb_application):\n  owned_ref = Referent()\n  def ApplyOwnage(owner_ref):\n    return DRS([], [GetPossessionConditions(owner_ref, owned_ref)])\n  type = 'pronoun-poss_main'\n  cond = ResolutionCondition(owned_ref, presuppose_pronoun(ApplyOwnage), type)\n  return DRS([], [cond]) + verb_application(owned_ref)\nreturn PresupposeOwnedObject", "path": "src\\drt\\rules.py", "repo_name": "max99x/crystal", "stars": 34, "license": "None", "language": "python", "size": 4963}
{"docstring": "# S[TYP=whq,RUL=12] -> Q AuxV[...] NP[...] VPQ[...]\n", "func_signal": "def EvaluateObjectQuestion(tree):\n", "code": "tree = tree.copy(deep=False)\ntree[0] = [tree[0]]\nreturn EvaluateParticledObjectQuestion(tree)", "path": "src\\drt\\rules.py", "repo_name": "max99x/crystal", "stars": 34, "license": "None", "language": "python", "size": 4963}
{"docstring": "# S[TYP=ynq,RUL=10] -> AuxV[...,TYP=do] NP[...] VP[TENS=i]\n", "func_signal": "def EvaluateGenericYesNoQuestion(tree):\n", "code": "tree = tree.copy()\ndel tree[:1]\nreturn BooleanQuestionDRS(EvaluateSentence(tree))", "path": "src\\drt\\rules.py", "repo_name": "max99x/crystal", "stars": 34, "license": "None", "language": "python", "size": 4963}
{"docstring": "# Q[+sbj,RUL=903] -> 'what' AJP Noun\n", "func_signal": "def EvaluateProformWithNounAndAdjective(tree=None):\n", "code": "adjective_application = Evaluate(tree[1])\nnoun_application = Evaluate(tree[2])\ndef ApplyAdjectiveAndNound(ref):\n  return adjective_application(ref) + noun_application(ref)\nreturn MakeQuestionDeterminer(ApplyAdjectiveAndNound)", "path": "src\\drt\\rules.py", "repo_name": "max99x/crystal", "stars": 34, "license": "None", "language": "python", "size": 4963}
{"docstring": "# S[TYP=whq,RUL=13] -> QM[...] AuxV[...] NP[...] VPQ[...]\n\n", "func_signal": "def EvaluateParticledObjectQuestion(tree):\n", "code": "tree = tree.copy(deep=True)\nobject_tree = tree[0]\nsubject_tree = tree[2]\nvp_tree = tree[3]\n\ntarget_index = vp_tree.node['TRGT'] - 1\nvp_tree[target_index:target_index] = object_tree\n\nsubject = Evaluate(subject_tree)\nvp = Evaluate(vp_tree)\n\ndrs = subject(vp)\nif not isinstance(drs, SubjectQuestionDRS):\n  target_ref = None\n  for child in drs.Walk():\n    if isinstance(child, SubjectQuestionDRS):\n      if target_ref:\n        raise EvaluatorError('Multiple questions in a single sentence.')\n      else:\n        target_ref = child.target\n  if not target_ref:\n    raise EvaluatorError('Question lost during VP construction.')\n  drs = SubjectQuestionDRS(drs, target_ref)\n\nreturn drs", "path": "src\\drt\\rules.py", "repo_name": "max99x/crystal", "stars": 34, "license": "None", "language": "python", "size": 4963}
{"docstring": "# S[TYP=dcl,RUL=2] -> Cond S[TYP=dcl] Pnct[TYP=com] S[TYP=dcl]\n# S[TYP=dcl,RUL=3] -> Cond S[TYP=dcl] Then S[TYP=dcl]\n", "func_signal": "def EvaluateConditionalSentence(tree):\n", "code": "antecedent = Evaluate(tree[1])\nif _strict_mode and not IsFragmentConsistent(antecedent):\n  raise EvaluatorError('An antecedent failed consistency check.')\n\nconsequent = Evaluate(tree[3])\nif _strict_mode and not IsFragmentConsistent(consequent, antecedent):\n  raise EvaluatorError('A consequent failed consistency check.')\n\nreturn DRS([], [ImplicationCondition(antecedent, consequent)])", "path": "src\\drt\\rules.py", "repo_name": "max99x/crystal", "stars": 34, "license": "None", "language": "python", "size": 4963}
{"docstring": "\"\"\"Shutdown the socket unix socket server ensuring the unix socket is\nremoved.\n\n\"\"\"\n", "func_signal": "def shutdown(self):\n", "code": "err = None\nSocketServer.shutdown(self)", "path": "whizzer\\server.py", "repo_name": "teburd/whizzer", "stars": 36, "license": "mit", "language": "python", "size": 676}
{"docstring": "\"\"\"Connector is successful, return the socket.\"\"\"\n", "func_signal": "def _connected(self, watcher, events):\n", "code": "self.connected = True\nself._finish()\nself.deferred.callback(self.sock)", "path": "whizzer\\client.py", "repo_name": "teburd/whizzer", "stars": 36, "license": "mit", "language": "python", "size": 676}
{"docstring": "\"\"\"Callback performed when the transport is closed.\"\"\"\n", "func_signal": "def closed(self, reason):\n", "code": "self.server.remove_connection(self)\nself.protocol.connection_lost(reason)\nif not isinstance(reason, ConnectionClosed):\n    logger.warn(\"connection closed, reason: %s\" % str(reason))\nelse:\n    logger.info(\"connection closed\")", "path": "whizzer\\server.py", "repo_name": "teburd/whizzer", "stars": 36, "license": "mit", "language": "python", "size": 676}
{"docstring": "\"\"\"Connect failed.\"\"\"\n", "func_signal": "def _connect_failed(self, reason):\n", "code": "self.connector = None\nself.connect_deferred.errback(reason)", "path": "whizzer\\client.py", "repo_name": "teburd/whizzer", "stars": 36, "license": "mit", "language": "python", "size": 676}
{"docstring": "\"\"\"Called by the connections themselves when they have been closed.\"\"\"\n", "func_signal": "def remove_connection(self, connection):\n", "code": "if not self._closing:\n    self.connections.remove(connection)\n    logger.debug(\"removed connection\")", "path": "whizzer\\server.py", "repo_name": "teburd/whizzer", "stars": 36, "license": "mit", "language": "python", "size": 676}
{"docstring": "\"\"\"Run in the client after the fork.\"\"\"\n", "func_signal": "def server_main(loop, path):\n", "code": "loop.fork()\nlogger.debug('forked function')\nsigintwatcher = pyev.Signal(signal.SIGINT, loop, lambda watcher, events: logger.info('interrupt ignored'))\nsigintwatcher.start()\nsigtermwatcher = pyev.Signal(signal.SIGTERM, loop, server_stop)\nsigtermwatcher.start()\nadder = AdderService()\ndispatcher = ObjectDispatch(adder)\npickle_factory = PickleProtocolFactory(dispatcher)\npickle_server = UnixServer(loop, pickle_factory, path)\npickle_server.start()\nmsgpack_factory = MsgPackProtocolFactory(dispatcher)\nmsgpack_server = UnixServer(loop, msgpack_factory, path + '_mp')\nmsgpack_server.start()\n\nlogger.debug('running server loop')\n\nimport cProfile\ncProfile.runctx('loop.loop()', None, {'loop':loop}, 'server_profile')\n\nlogger.debug('server unlooped')", "path": "examples\\forked.py", "repo_name": "teburd/whizzer", "stars": 36, "license": "mit", "language": "python", "size": 676}
{"docstring": "\"\"\"Called externally when the transport is ready.\"\"\"\n", "func_signal": "def make_connection(self, transport, address):\n", "code": "self.connected = True\nself.transport = transport\nself.connection_made(address)", "path": "whizzer\\protocol.py", "repo_name": "teburd/whizzer", "stars": 36, "license": "mit", "language": "python", "size": 676}
{"docstring": "\"\"\"Start the socket server.\n       \nThe socket server will begin accepting incoming connections.\n\n\"\"\"\n", "func_signal": "def start(self):\n", "code": "if self._shutdown:\n    raise ShutdownError()\n\nself.read_watcher.start()\nlogger.info(\"server started listening on {}\".format(self.address))", "path": "whizzer\\server.py", "repo_name": "teburd/whizzer", "stars": 36, "license": "mit", "language": "python", "size": 676}
{"docstring": "\"\"\"Finalize the connector.\"\"\"\n", "func_signal": "def _finish(self):\n", "code": "self.connect_watcher.stop()\nself.timeout_watcher.stop()", "path": "whizzer\\client.py", "repo_name": "teburd/whizzer", "stars": 36, "license": "mit", "language": "python", "size": 676}
{"docstring": "# setup some blocking sockets to test the transport with\n", "func_signal": "def setUp(self):\n", "code": "self.sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\nself.sock.bind(fpath + \"/test_sock\")\nself.sock.listen(1)\nself.csock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\nself.csock.connect(fpath + \"/test_sock\")\nself.ssock, self.saddr = self.sock.accept()\nself.data = [] \nself.reason = None", "path": "whizzer\\test\\test_transport.py", "repo_name": "teburd/whizzer", "stars": 36, "license": "mit", "language": "python", "size": 676}
{"docstring": "\"\"\"Start watching the socket for it to be writtable.\"\"\"\n", "func_signal": "def _connect(self, sock, addr, timeout):\n", "code": "if self.connection:\n    raise SocketClientConnectedError()\n\nif self.connector:\n    raise SocketClientConnectingError()\n\nself.connect_deferred = Deferred(self.loop)\nself.sock = sock\nself.addr = addr\nself.connector = Connector(self.loop, sock, addr, timeout)\nself.connector.deferred.add_callback(self._connected)\nself.connector.deferred.add_errback(self._connect_failed)\nself.connector.start()\n\nreturn self.connect_deferred", "path": "whizzer\\client.py", "repo_name": "teburd/whizzer", "stars": 36, "license": "mit", "language": "python", "size": 676}
{"docstring": "\"\"\"Cancel a connector from completing.\"\"\"\n", "func_signal": "def cancel(self):\n", "code": "if self.started and not self.connected and not self.timedout:\n    self.connect_watcher.stop()\n    self.timeout_watcher.stop()", "path": "whizzer\\client.py", "repo_name": "teburd/whizzer", "stars": 36, "license": "mit", "language": "python", "size": 676}
{"docstring": "\"\"\"Stop the socket server.\n\nThe socket server will stop accepting incoming connections.\n\nThe connections already made will continue to exist.\n\n\"\"\"\n", "func_signal": "def stop(self):\n", "code": "if self._shutdown:\n    raise ShutdownError()\n\nself.read_watcher.stop()\nlogger.info(\"server stopped listening on {}\".format(self.address))", "path": "whizzer\\server.py", "repo_name": "teburd/whizzer", "stars": 36, "license": "mit", "language": "python", "size": 676}
{"docstring": "\"\"\"Socket server listens on a given socket for incoming connections.\nWhen a new connection is available it accepts it and creates a new\nConnection and Protocol to handle reading and writting data.\n\nloop -- pyev loop\nfactory -- protocol factory (object with build(loop) method that returns a protocol object)\nsock -- socket to listen on\n\n\"\"\"\n", "func_signal": "def __init__(self, loop, factory, sock, address):\n", "code": "self.loop = loop\nself.factory = factory\nself.sock = sock\nself.address = address\nself.connections = set()\nself._closing = False\nself._shutdown = False\nself.interrupt_watcher = pyev.Signal(signal.SIGINT, self.loop, self._interrupt)\nself.interrupt_watcher.start()\nself.read_watcher = pyev.Io(self.sock, pyev.EV_READ, self.loop, self._readable)", "path": "whizzer\\server.py", "repo_name": "teburd/whizzer", "stars": 36, "license": "mit", "language": "python", "size": 676}
{"docstring": "\"\"\"When the socket is writtable, the socket is ready to be used.\"\"\"\n", "func_signal": "def _connected(self, sock):\n", "code": "logger.debug('socket connected, building protocol')\nself.protocol = self.factory.build(self.loop)\nself.connection = Connection(self.loop, self.sock, self.addr,\n    self.protocol, self) \nself.connector = None\nself.connect_deferred.callback(self.protocol)", "path": "whizzer\\client.py", "repo_name": "teburd/whizzer", "stars": 36, "license": "mit", "language": "python", "size": 676}
{"docstring": "\"\"\"Shutdown the socket server.\n\nThe socket server will stop accepting incoming connections.\n\nAll connections will be dropped.\n\n\"\"\"\n", "func_signal": "def shutdown(self, reason = ConnectionClosed()):\n", "code": "if self._shutdown:\n    raise ShutdownError()\n\nself.stop()\n       \nself._closing = True\nfor connection in self.connections:\n    connection.close()\nself.connections = set()\nself._shutdown = True\nif isinstance(reason, ConnectionClosed):\n    logger.info(\"server shutdown\")\nelse:\n    logger.warn(\"server shutdown, reason %s\" % str(reason))", "path": "whizzer\\server.py", "repo_name": "teburd/whizzer", "stars": 36, "license": "mit", "language": "python", "size": 676}
{"docstring": "\"\"\"Called by the pyev watcher (self.read_watcher) whenever the socket\nis readable.\n   \nThis means either the socket has been closed or there is a new\nclient connection waiting.\n\n\"\"\"\n", "func_signal": "def _readable(self, watcher, events):\n", "code": "protocol = self.factory.build(self.loop)\ntry:\n    sock, address = self.sock.accept()\n    connection = Connection(self.loop, sock, address, protocol, self)\n    self.connections.add(connection)\n    connection.make_connection()\n    logger.debug(\"added connection\")\nexcept IOError as e:\n    self.shutdown(e)", "path": "whizzer\\server.py", "repo_name": "teburd/whizzer", "stars": 36, "license": "mit", "language": "python", "size": 676}
{"docstring": "\"\"\"Disconnect from a socket.\"\"\"\n", "func_signal": "def _disconnect(self):\n", "code": "if self.connection:\n    self.connection.close()\n    self.connection = None", "path": "whizzer\\client.py", "repo_name": "teburd/whizzer", "stars": 36, "license": "mit", "language": "python", "size": 676}
{"docstring": "\"\"\"Create a server connection.\"\"\"\n", "func_signal": "def __init__(self, loop, sock, address, protocol, server):\n", "code": "self.loop = loop\nself.sock = sock\nself.address = address\nself.protocol = protocol\nself.server = server\nself.transport = SocketTransport(self.loop, self.sock, self.protocol.data, self.closed)", "path": "whizzer\\server.py", "repo_name": "teburd/whizzer", "stars": 36, "license": "mit", "language": "python", "size": 676}
{"docstring": "\"\"\"Create a client connection.\"\"\"\n", "func_signal": "def __init__(self, loop, sock, addr, protocol, client):\n", "code": "self.loop = loop\nself.sock = sock\nself.addr = addr\nself.protocol = protocol\nself.client = client\nlogger.debug('making transport')\nself.transport = SocketTransport(self.loop, self.sock,\n                                 self.protocol.data, self.closed)\nlogger.debug('protocol.make_connection')\nself.protocol.make_connection(self.transport, self.addr)\nlogger.debug('transport.start()')\nself.transport.start()\nlogger.debug('transport started')", "path": "whizzer\\client.py", "repo_name": "teburd/whizzer", "stars": 36, "license": "mit", "language": "python", "size": 676}
{"docstring": "\"\"\"Initializes ctx using env.\"\"\"\n", "func_signal": "def load(self, env):\n", "code": "ctx = web.ctx\nctx.clear()\nctx.status = '200 OK'\nctx.headers = []\nctx.output = ''\nctx.environ = ctx.env = env\nctx.host = env.get('HTTP_HOST')\n\nif env.get('wsgi.url_scheme') in ['http', 'https']:\n    ctx.protocol = env['wsgi.url_scheme']\nelif env.get('HTTPS', '').lower() in ['on', 'true', '1']:\n    ctx.protocol = 'https'\nelse:\n    ctx.protocol = 'http'\nctx.homedomain = ctx.protocol + '://' + env.get('HTTP_HOST', '[unknown]')\nctx.homepath = os.environ.get('REAL_SCRIPT_NAME', env.get('SCRIPT_NAME', ''))\nctx.home = ctx.homedomain + ctx.homepath\n#@@ home is changed when the request is handled to a sub-application.\n#@@ but the real home is required for doing absolute redirects.\nctx.realhome = ctx.home\nctx.ip = env.get('REMOTE_ADDR')\nctx.method = env.get('REQUEST_METHOD')\nctx.path = env.get('PATH_INFO')\n# http://trac.lighttpd.net/trac/ticket/406 requires:\nif env.get('SERVER_SOFTWARE', '').startswith('lighttpd/'):\n    ctx.path = lstrips(env.get('REQUEST_URI').split('?')[0], ctx.homepath)\n    # Apache and CherryPy webservers unquote the url but lighttpd doesn't. \n    # unquote explicitly for lighttpd to make ctx.path uniform across all servers.\n    ctx.path = urllib.unquote(ctx.path)\n\nif env.get('QUERY_STRING'):\n    ctx.query = '?' + env.get('QUERY_STRING', '')\nelse:\n    ctx.query = ''\n\nctx.fullpath = ctx.path + ctx.query\n\nfor k, v in ctx.iteritems():\n    if isinstance(v, str):\n        ctx[k] = safeunicode(v)\n\n# status must always be str\nctx.status = '200 OK'\n\nctx.app_stack = []", "path": "lib\\web\\application.py", "repo_name": "localprojects/Change-By-Us", "stars": 55, "license": "other", "language": "python", "size": 45388}
{"docstring": "# do db commit and release the connection if pooling is enabled.            \n", "func_signal": "def commit(unload=True):\n", "code": "ctx.db.commit()\nif unload and self.has_pooling:\n    self._unload_context(self._ctx)", "path": "lib\\web\\db.py", "repo_name": "localprojects/Change-By-Us", "stars": 55, "license": "other", "language": "python", "size": 45388}
{"docstring": "# firebird doesn't support using clause\n", "func_signal": "def delete(self, table, where=None, using=None, vars=None, _test=False):\n", "code": "using=None\nreturn DB.delete(self, table, where, using, vars, _test)", "path": "lib\\web\\db.py", "repo_name": "localprojects/Change-By-Us", "stars": 55, "license": "other", "language": "python", "size": 45388}
{"docstring": "\"\"\"\nConverts a load hook into an application processor.\n\n    >>> app = auto_application()\n    >>> def f(): \"something done before handling request\"\n    ...\n    >>> app.add_processor(loadhook(f))\n\"\"\"\n", "func_signal": "def loadhook(h):\n", "code": "def processor(handler):\n    h()\n    return handler()\n    \nreturn processor", "path": "lib\\web\\application.py", "repo_name": "localprojects/Change-By-Us", "stars": 55, "license": "other", "language": "python", "size": 45388}
{"docstring": "\"\"\"loadhook to reload mapping and fvars.\"\"\"\n", "func_signal": "def reload_mapping():\n", "code": "mod = __import__(module_name)\nmapping = getattr(mod, mapping_name, None)\nif mapping:\n    self.fvars = mod.__dict__\n    self.mapping = mapping", "path": "lib\\web\\application.py", "repo_name": "localprojects/Change-By-Us", "stars": 55, "license": "other", "language": "python", "size": 45388}
{"docstring": "\"\"\"\n    >>> _sqllist([1, 2, 3])\n    <sql: '(1, 2, 3)'>\n\"\"\"\n", "func_signal": "def _sqllist(values):\n", "code": "items = []\nitems.append('(')\nfor i, v in enumerate(values):\n    if i != 0:\n        items.append(', ')\n    items.append(sqlparam(v))\nitems.append(')')\nreturn SQLQuery(items)", "path": "lib\\web\\db.py", "repo_name": "localprojects/Change-By-Us", "stars": 55, "license": "other", "language": "python", "size": 45388}
{"docstring": "\"\"\"\n`left is a SQL clause like `tablename.arg = ` \nand `lst` is a list of values. Returns a reparam-style\npair featuring the SQL that ORs together the clause\nfor each item in the lst.\n\n    >>> sqlors('foo = ', [])\n    <sql: '1=2'>\n    >>> sqlors('foo = ', [1])\n    <sql: 'foo = 1'>\n    >>> sqlors('foo = ', 1)\n    <sql: 'foo = 1'>\n    >>> sqlors('foo = ', [1,2,3])\n    <sql: '(foo = 1 OR foo = 2 OR foo = 3 OR 1=2)'>\n\"\"\"\n", "func_signal": "def sqlors(left, lst):\n", "code": "if isinstance(lst, iters):\n    lst = list(lst)\n    ln = len(lst)\n    if ln == 0:\n        return SQLQuery(\"1=2\")\n    if ln == 1:\n        lst = lst[0]\n\nif isinstance(lst, iters):\n    return SQLQuery(['('] + \n      sum([[left, sqlparam(x), ' OR '] for x in lst], []) +\n      ['1=2)']\n    )\nelse:\n    return left + sqlparam(lst)", "path": "lib\\web\\db.py", "repo_name": "localprojects/Change-By-Us", "stars": 55, "license": "other", "language": "python", "size": 45388}
{"docstring": "\"\"\"\nReturns a method that takes one argument and calls the method named prefix+arg,\ncalling `notfound()` if there isn't one. Example:\n\n    urls = ('/prefs/(.*)', 'prefs')\n\n    class prefs:\n        GET = autodelegate('GET_')\n        def GET_password(self): pass\n        def GET_privacy(self): pass\n\n`GET_password` would get called for `/prefs/password` while `GET_privacy` for \n`GET_privacy` gets called for `/prefs/privacy`.\n\nIf a user visits `/prefs/password/change` then `GET_password(self, '/change')`\nis called.\n\"\"\"\n", "func_signal": "def autodelegate(prefix=''):\n", "code": "def internal(self, arg):\n    if '/' in arg:\n        first, rest = arg.split('/', 1)\n        func = prefix + first\n        args = ['/' + rest]\n    else:\n        func = prefix + arg\n        args = []\n    \n    if hasattr(self, func):\n        try:\n            return getattr(self, func)(*args)\n        except TypeError:\n            raise web.notfound()\n    else:\n        raise web.notfound()\nreturn internal", "path": "lib\\web\\application.py", "repo_name": "localprojects/Change-By-Us", "stars": 55, "license": "other", "language": "python", "size": 45388}
{"docstring": "\"\"\"Deletes request to sub application `app` rooted at the directory `dir`.\nThe home, homepath, path and fullpath values in web.ctx are updated to mimic request\nto the subapp and are restored after it is handled. \n\n@@Any issues with when used with yield?\n\"\"\"\n", "func_signal": "def _delegate_sub_application(self, dir, app):\n", "code": "web.ctx._oldctx = web.storage(web.ctx)\nweb.ctx.home += dir\nweb.ctx.homepath += dir\nweb.ctx.path = web.ctx.path[len(dir):]\nweb.ctx.fullpath = web.ctx.fullpath[len(dir):]\nreturn app.handle_with_processors()", "path": "lib\\web\\application.py", "repo_name": "localprojects/Change-By-Us", "stars": 55, "license": "other", "language": "python", "size": 45388}
{"docstring": "\"\"\"\nConverts an unload hook into an application processor.\n\n    >>> app = auto_application()\n    >>> def f(): \"something done after handling request\"\n    ...\n    >>> app.add_processor(unloadhook(f))    \n\"\"\"\n", "func_signal": "def unloadhook(h):\n", "code": "def processor(handler):\n    try:\n        result = handler()\n        is_generator = result and hasattr(result, 'next')\n    except:\n        # run the hook even when handler raises some exception\n        h()\n        raise\n\n    if is_generator:\n        return wrap(result)\n    else:\n        h()\n        return result\n        \ndef wrap(result):\n    def next():\n        try:\n            return result.next()\n        except:\n            # call the hook at the and of iterator\n            h()\n            raise\n\n    result = iter(result)\n    while True:\n        yield next()\n        \nreturn processor", "path": "lib\\web\\application.py", "repo_name": "localprojects/Change-By-Us", "stars": 55, "license": "other", "language": "python", "size": 45388}
{"docstring": "# Upgrade operations go here. Don't create your own engine; bind migrate_engine\n# to your metadata\n\n", "func_signal": "def upgrade(migrate_engine):\n", "code": "meta = MetaData(migrate_engine)\nneed = Table('project_need', meta, autoload=True)\n\n# Replace the column referencing a place object with one that just has a\n# string representing a place.\ncreate_column(Column('address', String(256)), need)\ndrop_column('address_id', need)\n\nplace = Table('project_place', meta, autoload=True)\nplace.drop()", "path": "giveaminute\\migrations\\versions\\003_Change_address_to_a_simple_string_field.py", "repo_name": "localprojects/Change-By-Us", "stars": 55, "license": "other", "language": "python", "size": 45388}
{"docstring": "\"\"\"\nReturns the query part of the sql query.\n    >>> q = SQLQuery([\"SELECT * FROM test WHERE name=\", SQLParam('joe')])\n    >>> q.query()\n    'SELECT * FROM test WHERE name=%s'\n    >>> q.query(paramstyle='qmark')\n    'SELECT * FROM test WHERE name=?'\n\"\"\"\n", "func_signal": "def query(self, paramstyle=None):\n", "code": "s = []\nfor x in self.items:\n    if isinstance(x, SQLParam):\n        x = x.get_marker(paramstyle)\n        s.append(safestr(x))\n    else:\n        x = safestr(x)\n        # automatically escape % characters in the query\n        # For backward compatability, ignore escaping when the query looks already escaped\n        if paramstyle in ['format', 'pyformat']:\n            if '%' in x and '%%' not in x:\n                x = x.replace('%', '%%')\n        s.append(x)\nreturn \"\".join(s)", "path": "lib\\web\\db.py", "repo_name": "localprojects/Change-By-Us", "stars": 55, "license": "other", "language": "python", "size": 45388}
{"docstring": "# Check out http://webpy.org/cookbook/testing_with_paste_and_nose for\n# more about testing with Paste.\n", "func_signal": "def should_not_allow_anonymous_user_to_create_user(self):\n", "code": "response = self.app.post('/admin/user/add',\n    params={\n        'f_name': 'John',\n        'l_name': 'Smith',\n        'email': 'jsmith@example.com',\n        'password': 'password',\n        'role': '1',\n        'affiliation': '',\n    },\n    status=303)\ndb = main.sessionDB()\n\n# Check to see if the user was created even though the response\n# returned a redirect (303).\nresults = db.query(\"select user_id from user where email = 'jsmith@example.com'\")\nassert_equal(len(results), 0)", "path": "tests\\integrationtests\\admin.py", "repo_name": "localprojects/Change-By-Us", "stars": 55, "license": "other", "language": "python", "size": 45388}
{"docstring": "\"\"\"find name of the module name from fvars.\"\"\"\n", "func_signal": "def modname(fvars):\n", "code": "file, name = fvars.get('__file__'), fvars.get('__name__')\nif file is None or name is None:\n    return None\n\nif name == '__main__':\n    # Since the __main__ module can't be reloaded, the module has \n    # to be imported using its file name.                    \n    name = main_module_name()\nreturn name", "path": "lib\\web\\application.py", "repo_name": "localprojects/Change-By-Us", "stars": 55, "license": "other", "language": "python", "size": 45388}
{"docstring": "\"\"\"Query postgres to find names of all sequences used in this database.\"\"\"\n", "func_signal": "def _get_all_sequences(self):\n", "code": "if self._sequences is None:\n    q = \"SELECT c.relname FROM pg_class c WHERE c.relkind = 'S'\"\n    self._sequences = set([c.relname for c in self.query(q)])\nreturn self._sequences", "path": "lib\\web\\db.py", "repo_name": "localprojects/Change-By-Us", "stars": 55, "license": "other", "language": "python", "size": 45388}
{"docstring": "\"\"\"\nJoins multiple queries.\n\n>>> SQLQuery.join(['a', 'b'], ', ')\n<sql: 'a, b'>\n\nOptinally, prefix and suffix arguments can be provided.\n\n>>> SQLQuery.join(['a', 'b'], ', ', prefix='(', suffix=')')\n<sql: '(a, b)'>\n\nIf target argument is provided, the items are appended to target instead of creating a new SQLQuery.\n\"\"\"\n", "func_signal": "def join(items, sep=' ', prefix=None, suffix=None, target=None):\n", "code": "if target is None:\n    target = SQLQuery()\n\ntarget_items = target.items\n\nif prefix:\n    target_items.append(prefix)\n\nfor i, item in enumerate(items):\n    if i != 0:\n        target_items.append(sep)\n    if isinstance(item, SQLQuery):\n        target_items.extend(item.items)\n    else:\n        target_items.append(item)\n\nif suffix:\n    target_items.append(suffix)\nreturn target", "path": "lib\\web\\db.py", "repo_name": "localprojects/Change-By-Us", "stars": 55, "license": "other", "language": "python", "size": 45388}
{"docstring": "# Operations to reverse the above upgrade go here.\n\n", "func_signal": "def downgrade(migrate_engine):\n", "code": "meta = MetaData(migrate_engine)\nneed = Table('project_need', meta, autoload=True)\n\n# Recreate the project place table\nplace = Table('project_place', meta,\n    Column('id', Integer, primary_key=True),\n    Column('name', String(256)),\n    Column('street', String(256)),\n    Column('city', String(256)),\n)\nplace.create()\n\n# Bring back the address_id column\ncreate_column(Column('address_id', Integer, ForeignKey('project_place.id')), need)\ndrop_column('address', need)", "path": "giveaminute\\migrations\\versions\\003_Change_address_to_a_simple_string_field.py", "repo_name": "localprojects/Change-By-Us", "stars": 55, "license": "other", "language": "python", "size": 45388}
{"docstring": "# do db rollback and release the connection if pooling is enabled.\n", "func_signal": "def rollback():\n", "code": "ctx.db.rollback()\nif self.has_pooling:\n    self._unload_context(self._ctx)", "path": "lib\\web\\db.py", "repo_name": "localprojects/Change-By-Us", "stars": 55, "license": "other", "language": "python", "size": 45388}
{"docstring": "\"\"\"Import the first available driver or preferred driver.\n\"\"\"\n", "func_signal": "def import_driver(drivers, preferred=None):\n", "code": "if preferred:\n    drivers = [preferred]\n\nfor d in drivers:\n    try:\n        return __import__(d, None, None, ['x'])\n    except ImportError:\n        pass\nraise ImportError(\"Unable to import \" + \" or \".join(drivers))", "path": "lib\\web\\db.py", "repo_name": "localprojects/Change-By-Us", "stars": 55, "license": "other", "language": "python", "size": 45388}
{"docstring": "\"\"\"Creates a database.\n\"\"\"\n# some DB implementaions take optional paramater `driver` to use a specific driver modue\n# but it should not be passed to connect\n", "func_signal": "def __init__(self, db_module, keywords):\n", "code": "keywords.pop('driver', None)\n\nself.db_module = db_module\nself.keywords = keywords\n\nself._ctx = threadeddict()\n# flag to enable/disable printing queries\nself.printing = config.get('debug_sql', config.get('debug', False))\nself.supports_multiple_insert = False\n\ntry:\n    import DBUtils\n    # enable pooling if DBUtils module is available.\n    self.has_pooling = True\nexcept ImportError:\n    self.has_pooling = False\n    \n# Pooling can be disabled by passing pooling=False in the keywords.\nself.has_pooling = self.keywords.pop('pooling', True) and self.has_pooling", "path": "lib\\web\\db.py", "repo_name": "localprojects/Change-By-Us", "stars": 55, "license": "other", "language": "python", "size": 45388}
{"docstring": "\"\"\"Turn the list of bits -> data, into a string\"\"\"\n", "func_signal": "def __BitList_to_String(self, data):\n", "code": "result = []\npos = 0\nc = 0\nwhile pos < len(data):\n\tc += data[pos] << (7 - (pos % 8))\n\tif (pos % 8) == 7:\n\t\tresult.append(c)\n\t\tc = 0\n\tpos += 1\n\nif _pythonMajorVersion < 3:\n\treturn ''.join([ chr(c) for c in result ])\nelse:\n\treturn bytes(result)", "path": "pyDes.py", "repo_name": "maxklein/AppSalesGraph", "stars": 42, "license": "None", "language": "python", "size": 1152}
{"docstring": "\"\"\"Turn the string data, into a list of bits (1, 0)'s\"\"\"\n", "func_signal": "def __String_to_BitList(self, data):\n", "code": "if _pythonMajorVersion < 3:\n\t# Turn the strings into integers. Python 3 uses a bytes\n\t# class, which already has this behaviour.\n\tdata = [ord(c) for c in data]\nl = len(data) * 8\nresult = [0] * l\npos = 0\nfor ch in data:\n\ti = 7\n\twhile i >= 0:\n\t\tif ch & (1 << i) != 0:\n\t\t\tresult[pos] = 1\n\t\telse:\n\t\t\tresult[pos] = 0\n\t\tpos += 1\n\t\ti -= 1\n\nreturn result", "path": "pyDes.py", "repo_name": "maxklein/AppSalesGraph", "stars": 42, "license": "None", "language": "python", "size": 1152}
{"docstring": "\"\"\"Will set the crypting key for this object. Must be 8 bytes.\"\"\"\n", "func_signal": "def setKey(self, key):\n", "code": "_baseDes.setKey(self, key)\nself.__create_sub_keys()", "path": "pyDes.py", "repo_name": "maxklein/AppSalesGraph", "stars": 42, "license": "None", "language": "python", "size": 1152}
{"docstring": "# Only accept byte strings or ascii unicode values, otherwise\n# there is no way to correctly decode the data into bytes.\n", "func_signal": "def _guardAgainstUnicode(self, data):\n", "code": "if _pythonMajorVersion < 3:\n\tif isinstance(data, unicode):\n\t\traise ValueError(\"pyDes can only work with bytes, not Unicode strings.\")\nelse:\n\tif isinstance(data, str):\n\t\t# Only accept ascii unicode values.\n\t\ttry:\n\t\t\treturn data.encode('ascii')\n\t\texcept UnicodeEncodeError:\n\t\t\tpass\n\t\traise ValueError(\"pyDes can only work with encoded strings, not Unicode.\")\nreturn data", "path": "pyDes.py", "repo_name": "maxklein/AppSalesGraph", "stars": 42, "license": "None", "language": "python", "size": 1152}
{"docstring": "\"\"\"Create the 16 subkeys K[1] to K[16] from the given key\"\"\"\n", "func_signal": "def __create_sub_keys(self):\n", "code": "key = self.__permutate(des.__pc1, self.__String_to_BitList(self.getKey()))\ni = 0\n# Split into Left and Right sections\nself.L = key[:28]\nself.R = key[28:]\nwhile i < 16:\n\tj = 0\n\t# Perform circular left shifts\n\twhile j < des.__left_rotations[i]:\n\t\tself.L.append(self.L[0])\n\t\tdel self.L[0]\n\n\t\tself.R.append(self.R[0])\n\t\tdel self.R[0]\n\n\t\tj += 1\n\n\t# Create one of the 16 subkeys through pc2 permutation\n\tself.Kn[i] = self.__permutate(des.__pc2, self.L + self.R)\n\n\ti += 1", "path": "pyDes.py", "repo_name": "maxklein/AppSalesGraph", "stars": 42, "license": "None", "language": "python", "size": 1152}
{"docstring": "\"\"\"setPadding() -> bytes of length 1. Padding character.\"\"\"\n", "func_signal": "def setPadding(self, pad):\n", "code": "_baseDes.setPadding(self, pad)\nfor key in (self.__key1, self.__key2, self.__key3):\n\tkey.setPadding(pad)", "path": "pyDes.py", "repo_name": "maxklein/AppSalesGraph", "stars": 42, "license": "None", "language": "python", "size": 1152}
{"docstring": "\"\"\"Crypt the data in blocks, running it through des_crypt()\"\"\"\n\n# Error check the data\n", "func_signal": "def crypt(self, data, crypt_type):\n", "code": "if not data:\n\treturn ''\nif len(data) % self.block_size != 0:\n\tif crypt_type == des.DECRYPT: # Decryption must work on 8 byte blocks\n\t\traise ValueError(\"Invalid data length, data must be a multiple of \" + str(self.block_size) + \" bytes\\n.\")\n\tif not self.getPadding():\n\t\traise ValueError(\"Invalid data length, data must be a multiple of \" + str(self.block_size) + \" bytes\\n. Try setting the optional padding character\")\n\telse:\n\t\tdata += (self.block_size - (len(data) % self.block_size)) * self.getPadding()\n\t# print \"Len of data: %f\" % (len(data) / self.block_size)\n\nif self.getMode() == CBC:\n\tif self.getIV():\n\t\tiv = self.__String_to_BitList(self.getIV())\n\telse:\n\t\traise ValueError(\"For CBC mode, you must supply the Initial Value (IV) for ciphering\")\n\n# Split the data into blocks, crypting each one seperately\ni = 0\ndict = {}\nresult = []\n#cached = 0\n#lines = 0\nwhile i < len(data):\n\t# Test code for caching encryption results\n\t#lines += 1\n\t#if dict.has_key(data[i:i+8]):\n\t\t#print \"Cached result for: %s\" % data[i:i+8]\n\t#\tcached += 1\n\t#\tresult.append(dict[data[i:i+8]])\n\t#\ti += 8\n\t#\tcontinue\n\t\t\n\tblock = self.__String_to_BitList(data[i:i+8])\n\n\t# Xor with IV if using CBC mode\n\tif self.getMode() == CBC:\n\t\tif crypt_type == des.ENCRYPT:\n\t\t\tblock = list(map(lambda x, y: x ^ y, block, iv))\n\t\t\t#j = 0\n\t\t\t#while j < len(block):\n\t\t\t#\tblock[j] = block[j] ^ iv[j]\n\t\t\t#\tj += 1\n\n\t\tprocessed_block = self.__des_crypt(block, crypt_type)\n\n\t\tif crypt_type == des.DECRYPT:\n\t\t\tprocessed_block = list(map(lambda x, y: x ^ y, processed_block, iv))\n\t\t\t#j = 0\n\t\t\t#while j < len(processed_block):\n\t\t\t#\tprocessed_block[j] = processed_block[j] ^ iv[j]\n\t\t\t#\tj += 1\n\t\t\tiv = block\n\t\telse:\n\t\t\tiv = processed_block\n\telse:\n\t\tprocessed_block = self.__des_crypt(block, crypt_type)\n\n\n\t# Add the resulting crypted block to our list\n\t#d = self.__BitList_to_String(processed_block)\n\t#result.append(d)\n\tresult.append(self.__BitList_to_String(processed_block))\n\t#dict[data[i:i+8]] = d\n\ti += 8\n\n# print \"Lines: %d, cached: %d\" % (lines, cached)\n\n# Return the full crypted string\nif _pythonMajorVersion < 3:\n\treturn ''.join(result)\nelse:\n\treturn bytes.fromhex('').join(result)", "path": "pyDes.py", "repo_name": "maxklein/AppSalesGraph", "stars": 42, "license": "None", "language": "python", "size": 1152}
{"docstring": "\"\"\"Will set the Initial Value, used in conjunction with CBC mode\"\"\"\n", "func_signal": "def setIV(self, IV):\n", "code": "if not IV or len(IV) != self.block_size:\n\traise ValueError(\"Invalid Initial Value (IV), must be a multiple of \" + str(self.block_size) + \" bytes\")\nIV = self._guardAgainstUnicode(IV)\nself._iv = IV", "path": "pyDes.py", "repo_name": "maxklein/AppSalesGraph", "stars": 42, "license": "None", "language": "python", "size": 1152}
{"docstring": "\"\"\"encrypt(data, [pad], [padmode]) -> bytes\n\ndata : Bytes to be encrypted\npad  : Optional argument for encryption padding. Must only be one byte\npadmode : Optional argument for overriding the padding mode.\n\nThe data must be a multiple of 8 bytes and will be encrypted\nwith the already specified key. Data does not have to be a\nmultiple of 8 bytes if the padding character is supplied, or\nthe padmode is set to PAD_PKCS5, as bytes will then added to\nensure the be padded data is a multiple of 8 bytes.\n\"\"\"\n", "func_signal": "def encrypt(self, data, pad=None, padmode=None):\n", "code": "data = self._guardAgainstUnicode(data)\nif pad is not None:\n\tpad = self._guardAgainstUnicode(pad)\ndata = self._padData(data, pad, padmode)\nreturn self.crypt(data, des.ENCRYPT)", "path": "pyDes.py", "repo_name": "maxklein/AppSalesGraph", "stars": 42, "license": "None", "language": "python", "size": 1152}
{"docstring": "\"\"\"setPadding() -> bytes of length 1. Padding character.\"\"\"\n", "func_signal": "def setPadding(self, pad):\n", "code": "if pad is not None:\n\tpad = self._guardAgainstUnicode(pad)\nself._padding = pad", "path": "pyDes.py", "repo_name": "maxklein/AppSalesGraph", "stars": 42, "license": "None", "language": "python", "size": 1152}
{"docstring": "\"\"\"Will set the crypting key for this object. Either 16 or 24 bytes long.\"\"\"\n", "func_signal": "def setKey(self, key):\n", "code": "self.key_size = 24  # Use DES-EDE3 mode\nif len(key) != self.key_size:\n\tif len(key) == 16: # Use DES-EDE2 mode\n\t\tself.key_size = 16\n\telse:\n\t\traise ValueError(\"Invalid triple DES key size. Key must be either 16 or 24 bytes long\")\nif self.getMode() == CBC:\n\tif not self.getIV():\n\t\t# Use the first 8 bytes of the key\n\t\tself.setIV(key[:self.block_size])\n\tif len(self.getIV()) != self.block_size:\n\t\traise ValueError(\"Invalid IV, must be 8 bytes in length\")\nself.__key1 = des(key[:8], self._mode, self._iv,\n\t\t  self._padding, self._padmode)\nself.__key2 = des(key[8:16], self._mode, self._iv,\n\t\t  self._padding, self._padmode)\nif self.key_size == 16:\n\tself.__key3 = self.__key1\nelse:\n\tself.__key3 = des(key[16:], self._mode, self._iv,\n\t\t\t  self._padding, self._padmode)\n_baseDes.setKey(self, key)", "path": "pyDes.py", "repo_name": "maxklein/AppSalesGraph", "stars": 42, "license": "None", "language": "python", "size": 1152}
{"docstring": "\"\"\"Crypt the block of data through DES bit-manipulation\"\"\"\n", "func_signal": "def __des_crypt(self, block, crypt_type):\n", "code": "block = self.__permutate(des.__ip, block)\nself.L = block[:32]\nself.R = block[32:]\n\n# Encryption starts from Kn[1] through to Kn[16]\nif crypt_type == des.ENCRYPT:\n\titeration = 0\n\titeration_adjustment = 1\n# Decryption starts from Kn[16] down to Kn[1]\nelse:\n\titeration = 15\n\titeration_adjustment = -1\n\ni = 0\nwhile i < 16:\n\t# Make a copy of R[i-1], this will later become L[i]\n\ttempR = self.R[:]\n\n\t# Permutate R[i - 1] to start creating R[i]\n\tself.R = self.__permutate(des.__expansion_table, self.R)\n\n\t# Exclusive or R[i - 1] with K[i], create B[1] to B[8] whilst here\n\tself.R = list(map(lambda x, y: x ^ y, self.R, self.Kn[iteration]))\n\tB = [self.R[:6], self.R[6:12], self.R[12:18], self.R[18:24], self.R[24:30], self.R[30:36], self.R[36:42], self.R[42:]]\n\t# Optimization: Replaced below commented code with above\n\t#j = 0\n\t#B = []\n\t#while j < len(self.R):\n\t#\tself.R[j] = self.R[j] ^ self.Kn[iteration][j]\n\t#\tj += 1\n\t#\tif j % 6 == 0:\n\t#\t\tB.append(self.R[j-6:j])\n\n\t# Permutate B[1] to B[8] using the S-Boxes\n\tj = 0\n\tBn = [0] * 32\n\tpos = 0\n\twhile j < 8:\n\t\t# Work out the offsets\n\t\tm = (B[j][0] << 1) + B[j][5]\n\t\tn = (B[j][1] << 3) + (B[j][2] << 2) + (B[j][3] << 1) + B[j][4]\n\n\t\t# Find the permutation value\n\t\tv = des.__sbox[j][(m << 4) + n]\n\n\t\t# Turn value into bits, add it to result: Bn\n\t\tBn[pos] = (v & 8) >> 3\n\t\tBn[pos + 1] = (v & 4) >> 2\n\t\tBn[pos + 2] = (v & 2) >> 1\n\t\tBn[pos + 3] = v & 1\n\n\t\tpos += 4\n\t\tj += 1\n\n\t# Permutate the concatination of B[1] to B[8] (Bn)\n\tself.R = self.__permutate(des.__p, Bn)\n\n\t# Xor with L[i - 1]\n\tself.R = list(map(lambda x, y: x ^ y, self.R, self.L))\n\t# Optimization: This now replaces the below commented code\n\t#j = 0\n\t#while j < len(self.R):\n\t#\tself.R[j] = self.R[j] ^ self.L[j]\n\t#\tj += 1\n\n\t# L[i] becomes R[i - 1]\n\tself.L = tempR\n\n\ti += 1\n\titeration += iteration_adjustment\n\n# Final permutation of R[16]L[16]\nself.final = self.__permutate(des.__fp, self.R + self.L)\nreturn self.final", "path": "pyDes.py", "repo_name": "maxklein/AppSalesGraph", "stars": 42, "license": "None", "language": "python", "size": 1152}
{"docstring": "\"\"\"decrypt(data, [pad], [padmode]) -> bytes\n\ndata : bytes to be encrypted\npad  : Optional argument for decryption padding. Must only be one byte\npadmode : Optional argument for overriding the padding mode.\n\nThe data must be a multiple of 8 bytes and will be decrypted\nwith the already specified key. In PAD_NORMAL mode, if the\noptional padding character is supplied, then the un-encrypted\ndata will have the padding characters removed from the end of\nthe bytes. This pad removal only occurs on the last 8 bytes of\nthe data (last data block). In PAD_PKCS5 mode, the special\npadding end markers will be removed from the data after\ndecrypting, no pad character is required for PAD_PKCS5.\n\"\"\"\n", "func_signal": "def decrypt(self, data, pad=None, padmode=None):\n", "code": "ENCRYPT = des.ENCRYPT\nDECRYPT = des.DECRYPT\ndata = self._guardAgainstUnicode(data)\nif pad is not None:\n\tpad = self._guardAgainstUnicode(pad)\nif self.getMode() == CBC:\n\tself.__key1.setIV(self.getIV())\n\tself.__key2.setIV(self.getIV())\n\tself.__key3.setIV(self.getIV())\n\ti = 0\n\tresult = []\n\twhile i < len(data):\n\t\tiv = data[i:i+8]\n\t\tblock = self.__key3.crypt(iv,    DECRYPT)\n\t\tblock = self.__key2.crypt(block, ENCRYPT)\n\t\tblock = self.__key1.crypt(block, DECRYPT)\n\t\tself.__key1.setIV(iv)\n\t\tself.__key2.setIV(iv)\n\t\tself.__key3.setIV(iv)\n\t\tresult.append(block)\n\t\ti += 8\n\tif _pythonMajorVersion < 3:\n\t\tdata = ''.join(result)\n\telse:\n\t\tdata = bytes.fromhex('').join(result)\nelse:\n\tdata = self.__key3.crypt(data, DECRYPT)\n\tdata = self.__key2.crypt(data, ENCRYPT)\n\tdata = self.__key1.crypt(data, DECRYPT)\nreturn self._unpadData(data, pad, padmode)", "path": "pyDes.py", "repo_name": "maxklein/AppSalesGraph", "stars": 42, "license": "None", "language": "python", "size": 1152}
{"docstring": "\"\"\"Sets the type of padding mode, pyDes.PAD_NORMAL or pyDes.PAD_PKCS5\"\"\"\n", "func_signal": "def setPadMode(self, mode):\n", "code": "_baseDes.setPadMode(self, mode)\nfor key in (self.__key1, self.__key2, self.__key3):\n\tkey.setPadMode(mode)", "path": "pyDes.py", "repo_name": "maxklein/AppSalesGraph", "stars": 42, "license": "None", "language": "python", "size": 1152}
{"docstring": "\"\"\"Will set the crypting key for this object.\"\"\"\n", "func_signal": "def setKey(self, key):\n", "code": "key = self._guardAgainstUnicode(key)\nself.__key = key", "path": "pyDes.py", "repo_name": "maxklein/AppSalesGraph", "stars": 42, "license": "None", "language": "python", "size": 1152}
{"docstring": "# Pad data depending on the mode\n", "func_signal": "def _padData(self, data, pad, padmode):\n", "code": "if padmode is None:\n\t# Get the default padding mode.\n\tpadmode = self.getPadMode()\nif pad and padmode == PAD_PKCS5:\n\traise ValueError(\"Cannot use a pad character with PAD_PKCS5\")\n\nif padmode == PAD_NORMAL:\n\tif len(data) % self.block_size == 0:\n\t\t# No padding required.\n\t\treturn data\n\n\tif not pad:\n\t\t# Get the default padding.\n\t\tpad = self.getPadding()\n\tif not pad:\n\t\traise ValueError(\"Data must be a multiple of \" + str(self.block_size) + \" bytes in length. Use padmode=PAD_PKCS5 or set the pad character.\")\n\tdata += (self.block_size - (len(data) % self.block_size)) * pad\n\nelif padmode == PAD_PKCS5:\n\tpad_len = 8 - (len(data) % self.block_size)\n\tif _pythonMajorVersion < 3:\n\t\tdata += pad_len * chr(pad_len)\n\telse:\n\t\tdata += bytes([pad_len] * pad_len)\n\nreturn data", "path": "pyDes.py", "repo_name": "maxklein/AppSalesGraph", "stars": 42, "license": "None", "language": "python", "size": 1152}
{"docstring": "\"\"\"Will set the Initial Value, used in conjunction with CBC mode\"\"\"\n", "func_signal": "def setIV(self, IV):\n", "code": "_baseDes.setIV(self, IV)\nfor key in (self.__key1, self.__key2, self.__key3):\n\tkey.setIV(IV)", "path": "pyDes.py", "repo_name": "maxklein/AppSalesGraph", "stars": 42, "license": "None", "language": "python", "size": 1152}
{"docstring": "\"\"\"encrypt(data, [pad], [padmode]) -> bytes\n\ndata : bytes to be encrypted\npad  : Optional argument for encryption padding. Must only be one byte\npadmode : Optional argument for overriding the padding mode.\n\nThe data must be a multiple of 8 bytes and will be encrypted\nwith the already specified key. Data does not have to be a\nmultiple of 8 bytes if the padding character is supplied, or\nthe padmode is set to PAD_PKCS5, as bytes will then added to\nensure the be padded data is a multiple of 8 bytes.\n\"\"\"\n", "func_signal": "def encrypt(self, data, pad=None, padmode=None):\n", "code": "ENCRYPT = des.ENCRYPT\nDECRYPT = des.DECRYPT\ndata = self._guardAgainstUnicode(data)\nif pad is not None:\n\tpad = self._guardAgainstUnicode(pad)\n# Pad the data accordingly.\ndata = self._padData(data, pad, padmode)\nif self.getMode() == CBC:\n\tself.__key1.setIV(self.getIV())\n\tself.__key2.setIV(self.getIV())\n\tself.__key3.setIV(self.getIV())\n\ti = 0\n\tresult = []\n\twhile i < len(data):\n\t\tblock = self.__key1.crypt(data[i:i+8], ENCRYPT)\n\t\tblock = self.__key2.crypt(block, DECRYPT)\n\t\tblock = self.__key3.crypt(block, ENCRYPT)\n\t\tself.__key1.setIV(block)\n\t\tself.__key2.setIV(block)\n\t\tself.__key3.setIV(block)\n\t\tresult.append(block)\n\t\ti += 8\n\tif _pythonMajorVersion < 3:\n\t\treturn ''.join(result)\n\telse:\n\t\treturn bytes.fromhex('').join(result)\nelse:\n\tdata = self.__key1.crypt(data, ENCRYPT)\n\tdata = self.__key2.crypt(data, DECRYPT)\n\treturn self.__key3.crypt(data, ENCRYPT)", "path": "pyDes.py", "repo_name": "maxklein/AppSalesGraph", "stars": 42, "license": "None", "language": "python", "size": 1152}
{"docstring": "\"\"\"Sets the type of crypting mode, pyDes.ECB or pyDes.CBC\"\"\"\n", "func_signal": "def setMode(self, mode):\n", "code": "_baseDes.setMode(self, mode)\nfor key in (self.__key1, self.__key2, self.__key3):\n\tkey.setMode(mode)", "path": "pyDes.py", "repo_name": "maxklein/AppSalesGraph", "stars": 42, "license": "None", "language": "python", "size": 1152}
{"docstring": "# Sanity checking of arguments.\n", "func_signal": "def __init__(self, key, mode=ECB, IV=None, pad=None, padmode=PAD_NORMAL):\n", "code": "if len(key) != 8:\n\traise ValueError(\"Invalid DES key size. Key must be exactly 8 bytes long.\")\n_baseDes.__init__(self, mode, IV, pad, padmode)\nself.key_size = 8\n\nself.L = []\nself.R = []\nself.Kn = [ [0] * 48 ] * 16\t# 16 48-bit keys (K1 - K16)\nself.final = []\n\nself.setKey(key)", "path": "pyDes.py", "repo_name": "maxklein/AppSalesGraph", "stars": 42, "license": "None", "language": "python", "size": 1152}
{"docstring": "\"\"\"\n\"\"\"\n", "func_signal": "def get_tasks(self, url=None, name=None, queue_names=None):\n", "code": "tasks = []\nstub = self.get_task_queue_stub()\n\nfor queue_name in queue_names or self.get_task_queue_names():\n    tasks.extend(stub.GetTasks(queue_name))\n\nif url is not None:\n    tasks = [t for t in tasks if t['url'] == url]\n\nif name is not None:\n    tasks = [t for t in tasks if t['name'] == name]\n\nfor task in tasks:\n    params = {}\n    decoded_body = base64.b64decode(task['body'])\n    \n    if decoded_body:\n        # urlparse.parse_qs doesn't seem to be in Python 2.5...\n        params = dict([item.split('=', 2) for item in decoded_body.split('&')])\n    \n    task.update({\n        'decoded_body': decoded_body,\n        'params': params,\n    })\n    \n    # These lines have to remain commented out as (for some reason) the strptime() call\n    # throws a SystemError: Parent module 'gaetestbed' not loaded\n    # This looks to be an issue with NoseGAE's sandboxing (--without-sandbox doesn't throw the error)\n    #\n    #if task.get('eta'):\n    #    task['eta_datetime'] = datetime.strptime(task['eta'], \"%Y/%m/%d %H:%M:%S\")\n    #    task['eta_date'] = task['eta_datetime'].date()\n    #    task['eta_time'] = task['eta_datetime'].time()\n    #\n    #else:\n    #    task.update({\n    #        'eta_datetime': None,\n    #        'eta_date':     None,\n    #        'eta_time':     None,\n    #    })\n\nreturn tasks", "path": "gaetestbed\\taskqueue.py", "repo_name": "jgeewax/gaetestbed", "stars": 40, "license": "other", "language": "python", "size": 289}
{"docstring": "\"\"\"\nAsserts that a response from the test web server (using `get` or `post)\nreturns a 301 or 302 status. \n\nThis assertion would fail if you expect the page to redirect and instead\nthe server tells the browser that there was a 500 error, or some other\nnon-redirecting status code.\n\nFor example::\n\n    import unittest\n    \n    from gaetestbed import WebTestCase\n    \n    from my_handlers.some_handler import application\n    \n    class MyTestCase(WebTestCase, unittest.TestCase):\n        APPLICATION = application\n        \n        def test_redirects(self):\n            response = self.get('/page_that_redirects/')\n            self.assertRedirects(response)\n        \n        def test_redirects_but_errors(self):\n            response = self.get('/page_with_exception/')\n            \n            # This would fail if the page throws an exception:\n            self.assertRedirects(response)\n\"\"\"\n", "func_signal": "def assertRedirects(self, response, to=None):\n", "code": "error = 'Response did not redirect (status code was %i).' % response.status_int\nself.assertTrue(response.status_int in (301, 302), error)\nif to is not None:\n    error = 'Response redirected, but went to %s instead of %s' % (\n        response.location, to\n    )\n    self.assertEqual(response.location, 'http://localhost%s' % to, error)", "path": "gaetestbed\\web.py", "repo_name": "jgeewax/gaetestbed", "stars": 40, "license": "other", "language": "python", "size": 289}
{"docstring": "# Check that the cache starts empty\n", "func_signal": "def test_memcache(self):\n", "code": "self.assertMemcacheItems(0)\n\n# Add something to the cache, check that it was added\nmemcache.set(key='test_item', value='test_content')\nself.assertMemcacheItems(1)\n\n# Clear the cache, check that it's now empty\nself.clear_memcache()\nself.assertMemcacheItems(0)", "path": "gaetestbed\\memcache.py", "repo_name": "jgeewax/gaetestbed", "stars": 40, "license": "other", "language": "python", "size": 289}
{"docstring": "\"\"\"\nAssert that an `iterable` is of a given length.\n\nThis is useful when you don't care about the type of the iterable, \nbut just care that it is a certain length. If the item provided\ndoesn't have the expected length, or the length cannot be determined,\nthe test will fail.\n\nThis will first try to call ``item.count()`` (assuming it's a ``QuerySet``)\nand then try ``len(item)``.\n\nLet's take a look at an example::\n    \n    class TestCase(BaseTestCase, unittest.TestCase):\n        def test_length(self):\n            # This will call len('asdf')\n            self.assertLength('asdf', 4)\n            \n            # This will use .count()\n            self.assertLength(models.MyModel.all(), 0)\n            \n            # These will use len()\n            self.assertLength([0, 1, 2], 3)\n            self.assertLength((), 0)\n            \n            # This will fail\n            self.assertLength(7, 0)\n\"\"\"\n", "func_signal": "def assertLength(self, iterable, count):\n", "code": "length = None\n\nif length is None:\n    try: length = iterable.count()\n    except: pass\n\nif length is None:\n    try: length = len(iterable)\n    except: pass\n\nif length is None:\n    self.fail(\"Unable to get length for object %s\" % type(iterable))\n\nelse:\n    self.assertEqual(length, count)", "path": "gaetestbed\\base.py", "repo_name": "jgeewax/gaetestbed", "stars": 40, "license": "other", "language": "python", "size": 289}
{"docstring": "\"\"\"\nThis method is called at the start of each test case.\n\nAs noted above, if your test case needs to call ``setUp``, make\nsure to call ``super()``! Otherwise the Data Store might not be\nset up correctly.\n\"\"\"\n", "func_signal": "def setUp(self):\n", "code": "super(DataStoreTestCase, self).setUp()\nself.clear_datastore()", "path": "gaetestbed\\datastore.py", "repo_name": "jgeewax/gaetestbed", "stars": 40, "license": "other", "language": "python", "size": 289}
{"docstring": "# Add something to the Data Store\n", "func_signal": "def test_clear_datastore(self):\n", "code": "models.MyModel(field=\"value\").put()\nself.assertLength(models.MyModel.all(), 1)\n\n# And then empty the Data Store\nself.clear_datastore()\nself.assertLength(models.MyModel.all(), 0)", "path": "gaetestbed\\datastore.py", "repo_name": "jgeewax/gaetestbed", "stars": 40, "license": "other", "language": "python", "size": 289}
{"docstring": "\"\"\"\nThe number of queries executed so far in the test.\n\nThis method will keep track of the number of queries on a per-test\nbasis. Since the Data Store is cleared out after each test, the number\nof queries resets to zero after each test.\n\nIf you care how many queries a certain block of code executes, take a look\nat how to use ``max_queries()`` along with the ``with`` statement.\n\nExample::\n\n    import unittest\n    \n    from gaetestbed import DataStoreTestCase\n    \n    class MyTestCase(DataStoreTestCase, unittest.TestCase):\n        def test_clear_datastore(self):\n            # No queries have been run yet\n            self.assertEqual(self.query_count, 0)\n            \n            # Run one query to count the number of models\n            self.assertLength(models.MyModel.all(), 1)\n            \n            # Check that one query was run\n            self.assertEqual(self.query_count, 1)\n\"\"\"\n", "func_signal": "def query_count(self):\n", "code": "count = 0\nqueries = self._get_datastore_stub().QueryHistory()\nfor n in queries.itervalues():\n    count += n\nreturn count", "path": "gaetestbed\\datastore.py", "repo_name": "jgeewax/gaetestbed", "stars": 40, "license": "other", "language": "python", "size": 289}
{"docstring": "\"\"\"\n\"\"\"\n", "func_signal": "def clear_task_queue(self):\n", "code": "stub = self.get_task_queue_stub()\nfor name in self.get_task_queue_names():\n    stub.FlushQueue(name)", "path": "gaetestbed\\taskqueue.py", "repo_name": "jgeewax/gaetestbed", "stars": 40, "license": "other", "language": "python", "size": 289}
{"docstring": "\"\"\"\nUpdates the mail stub with a hook that intercepts messages as they're being\nlogged.\n\nThis grabs the mail stub from the App Engine API proxy and overwrites the\n``_GenerateLog`` method. It simply grabs the message that would've been logged\nas sent, and adds it to the list of sent messages. You can retrieve the sent\nmessages that are intercepted with the ``get_sent_messages`` helper method.\n\"\"\"\n", "func_signal": "def _set_mail_stub(self):\n", "code": "test_case = self\nclass MailStub(mail_stub.MailServiceStub):\n    def _GenerateLog(self, method, message, log, *args, **kwargs):\n        test_case._sent_messages.append(message)\n        return super(MailStub, self)._GenerateLog(method, message, log, *args, **kwargs)\n\nif 'mail' in apiproxy_stub_map.apiproxy._APIProxyStubMap__stub_map:\n    del apiproxy_stub_map.apiproxy._APIProxyStubMap__stub_map['mail']\n\napiproxy_stub_map.apiproxy.RegisterStub('mail', MailStub())", "path": "gaetestbed\\mail.py", "repo_name": "jgeewax/gaetestbed", "stars": 40, "license": "other", "language": "python", "size": 289}
{"docstring": "\"\"\"\n\"\"\"\n", "func_signal": "def setUp(self):\n", "code": "super(TaskQueueTestCase, self).setUp()\nself.clear_task_queue()", "path": "gaetestbed\\taskqueue.py", "repo_name": "jgeewax/gaetestbed", "stars": 40, "license": "other", "language": "python", "size": 289}
{"docstring": "# Nothing has been added to the cache\n", "func_signal": "def test_memcache(self):\n", "code": "self.assertMemcacheItems(0)\n\n# Add something to the cache\nmemcache.set(key='test_item', value='test_content')\n\n# Test that the item was added\nself.assertMemcacheItems(1)\n\n# Remove that key\nmemcache.delete('test_item')\n\n# Test that the cache has zero items\nself.assertMemcacheHits(0)", "path": "gaetestbed\\memcache.py", "repo_name": "jgeewax/gaetestbed", "stars": 40, "license": "other", "language": "python", "size": 289}
{"docstring": "\"\"\"\nAsserts that an e-mail with various details would be sent by App Engine.\n\nWith all parameters blank, this method will fail only if the sent messages\nlist is completely empty. That is, without any parameters, this only asserts\nthat some Email message was somehow sent via the Mail API.\n\nIf any parameters are provided, they are *anded* together such that all the\nrequirements must be met by a particular message in order for the assertion to \npass. That is, if you specify both a recipient (``to``) and a sender (``sender``),\na single message must match those requirements. If you send one message with\nthe correct recipient and a different sender, and another with the correct sender\nand incorrect recipient, this assertion will fail.\n\nFor example, the following assertion would fail::\n\n    import unittest\n\n    from gaetestbed import MailTestCase\n    \n    from google.appengine.api import mail\n    \n    class MyTestCase(MailTestCase, unittest.TestCase):\n        def test_email_sent(self):\n            mail.send_mail(\n                to      = 'correct_recipient@example.org',\n                sender  = 'wrong_sender@example.org',\n                subject = 'Test E-mail',\n                body    = 'This is a test e-mail',\n            )\n            \n            mail.send_mail(\n                to      = 'wrong_recipient@example.org',\n                sender  = 'correct_sender@example.org',\n                subject = 'Test E-mail',\n                body    = 'This is a test e-mail',\n            )\n            \n            # This will FAIL\n            self.assertEmailSent(\n                to     = 'correct_recipient@example.org',\n                sender = 'correct_sender@example.org',\n            )\n\nAll of the fields are an exact match *except* the ``body`` and ``html``\nfields where the check is whether or not the body *contains* the body specified::\n\n    import unittest\n\n    from gaetestbed import MailTestCase\n    \n    from google.appengine.api import mail\n    \n    class MyTestCase(MailTestCase, unittest.TestCase):\n        def test_email_sent(self):\n            mail.send_mail(\n                to      = 'receiver@example.org',\n                sender  = 'sender@example.org',\n                subject = 'Test E-mail',\n                body    = 'This is a test e-mail',\n            )\n            \n            self.assertEmailSent(to='receiver@example.org')\n            self.assertEmailSent(sender='sender@example.org')\n            self.assertEmailSent(subject='Test E-mail')\n            \n            # This will pass because the body contains the string 'test'\n            self.assertEmailSent(body='test')\n\"\"\"\n", "func_signal": "def assertEmailSent(self, to=None, sender=None, subject=None, body=None, html=None):\n", "code": "messages = self.get_sent_messages(\n    to = to,\n    sender = sender,\n    subject = subject,\n    body = body,\n    html = html,\n)\n\nif not messages:\n    failure_message = \"Expected e-mail message sent.\"\n    \n    details = self._get_email_detail_string(to, sender, subject, body, html)\n    if details:\n        failure_message += ' Arguments expected: %s' % details\n    \n    self.fail(failure_message)", "path": "gaetestbed\\mail.py", "repo_name": "jgeewax/gaetestbed", "stars": 40, "license": "other", "language": "python", "size": 289}
{"docstring": "\"\"\"\nReturns a list of ``mail.EmailMessage`` that would've been sent via App\nEngine's Mail API.\n\nAs part of the sandboxing of this test case, any messages are stored only\non a per-test basis. That is, the following assert at the start of each \ntest should always pass::\n\n    self.assertEqual(self.get_sent_messages(), [])\n\nThis gives you back a list of messages that would've been sent inside your\ntest. Each item is a ``mail.EmailMessage`` meaning you can check the various\nfields as part of your testing::\n\n    import unittest\n\n    from gaetestbed import MailTestCase\n    \n    from google.appengine.api import mail\n    \n    class MyTestCase(MailTestCase, unittest.TestCase):\n        def test_email_sent(self):\n            mail.EmailMessage(\n                to = 'test@example.org',\n                subject = 'Test E-mail',\n                sender = 'me@example.org',\n                body = 'This is a test e-mail',\n            ).send()\n            \n            # assertLength is defined in the BaseTestCase\n            self.assertLength(self.get_sent_messages(), 1)\n            \n            # Grab a particular message\n            message = self.get_sent_messages()[0]\n            \n            # Check that the to field is set appropriately\n            self.assertEqual(message.to, 'test@example.org')\n\nYou can also use the same arguments available for ``assertEmailSent()`` in order\nto filter the messages returned. That is, if you specify a ``to`` parameter, the\nonly messages returned would be those that match that recipient::\n\n    import unittest\n    \n    from gaetestbed import MailTestCase\n    \n    from google.appengine.api import mail\n    \n    class MyTestCase(MailTestCase, unittest.TestCase): \n        def test_get_emails(self):\n            mail.EmailMessage(\n                to = 'test@example.org',\n                subject = 'Test E-mail',\n                sender = 'me@example.org',\n                body = 'This is a test e-mail',\n            ).send()\n            \n            # Check that without any parameters, all the messages are returned unfiltered\n            self.assertLength(self.get_sent_messages(), 1)\n            \n            # Since the to address is different, this will return no messages\n            self.assertLength(self.get_sent_messages(to='other@example.org'), 0)\n\nAs with the ``assertEmailSent()`` method, all the filters are anded together such that any\nmessage returned will match *ALL* of the parameters, not just a subset.\n\"\"\"\n", "func_signal": "def get_sent_messages(self, to=None, sender=None, subject=None, body=None, html=None):\n", "code": "messages = self._sent_messages\n\nif to:\n    messages = [m for m in messages if to in m.to_list()]\n\nif sender:\n    messages = [m for m in messages if sender == m.sender()]\n\nif subject:\n    messages = [m for m in messages if subject == m.subject()]\n\nif body:\n    messages = [m for m in messages if body in m.textbody()]\n\nif html:\n    messages = [m for m in messages if html in m.htmlbody()]\n\nreturn messages", "path": "gaetestbed\\mail.py", "repo_name": "jgeewax/gaetestbed", "stars": 40, "license": "other", "language": "python", "size": 289}
{"docstring": "# Nothing has been put in the cache, or retrieved from the cache\n", "func_signal": "def test_memcache(self):\n", "code": "self.assertMemcacheItems(0)\nself.assertMemcacheHits(0)\n\n# Add something to the cache\nmemcache.set(key='test_item', value='test_content')\n\n# One item added, no hits yet\nself.assertMemcacheItems(1)\nself.assertMemcacheHits(0)\n\n# Grab it from the cache:\nitem = memcache.get('test_item')\n\n# One item, one hit\nself.assertMemcacheItems(1)\nself.assertMemcacheHits(1)", "path": "gaetestbed\\memcache.py", "repo_name": "jgeewax/gaetestbed", "stars": 40, "license": "other", "language": "python", "size": 289}
{"docstring": "\"\"\"\n\"\"\"\n", "func_signal": "def assertTasksInQueue(self, n=None, url=None, name=None, queue_names=None):\n", "code": "tasks = self.get_tasks(url=url, name=name, queue_names=queue_names)\n\nif n is None:\n    self.assertNotEqual(len(tasks), 0)\nelse:\n    self.assertLength(tasks, n)", "path": "gaetestbed\\taskqueue.py", "repo_name": "jgeewax/gaetestbed", "stars": 40, "license": "other", "language": "python", "size": 289}
{"docstring": "# Nothing has retrieved from the cache\n", "func_signal": "def test_memcache(self):\n", "code": "self.assertMemcacheHits(0)\n\n# Add something to the cache\nmemcache.set(key='test_item', value='test_content')\n\n# Test still no hits\nself.assertMemcacheHits(0)\n\n# Grab it from the cache:\nitem = memcache.get('test_item')\n\n# Assert that there was a cache hit\nself.assertMemcacheHits(1)\n\n# Grab something that doesn't exist\nmemcache.get('bad_key')\n\n# Assert that still one hit\nself.assertMemcacheHits(1)", "path": "gaetestbed\\memcache.py", "repo_name": "jgeewax/gaetestbed", "stars": 40, "license": "other", "language": "python", "size": 289}
{"docstring": "\"\"\"\nThis method is called at the start of each test case.\n\nIf you need to use this method for your own test set up, make sure\nthat you call ``super()``. If not, the mail hook will not get configured\ncorrectly and the mail assertions will fail when they shouldn't.\n\nHere is an example of how to properly override the ``setUp`` method::\n\n    import unittest\n    \n    from gaetestbed import MailTestCase\n    \n    class MyTestCase(MailTestCase, unittest.TestCase):\n        def setUp(self):\n            super(MyTestCase, self).setUp()\n            # Do anything else you need here\n\"\"\"\n", "func_signal": "def setUp(self):\n", "code": "super(MailTestCase, self).setUp()\nself._set_mail_stub()\nself.clear_sent_messages()", "path": "gaetestbed\\mail.py", "repo_name": "jgeewax/gaetestbed", "stars": 40, "license": "other", "language": "python", "size": 289}
{"docstring": "\"\"\"\nAsserts that a response from the test web server (using `get` or\n`post`) returns a 200 OK status code. \n\nThis assertion would fail if you expect a standard page to be returned\nand instead the server tells the browser to redirect elsewhere.\n\nFor example::\n\n    import unittest\n    \n    from gaetestbed import WebTestCase\n    \n    from my_handlers.some_handler import application\n    \n    class MyTestCase(WebTestCase, unittest.TestCase):\n        APPLICATION = application\n        \n        def test_ok(self):\n            response = self.get('/')\n            self.assertOK(response)\n        \n        def test_ok_with_redirect(self):\n            response = self.get('/page_that_redirects/')\n            \n            # This would fail if the page redirects:\n            self.assertOK(response)\n\"\"\"\n", "func_signal": "def assertOK(self, response):\n", "code": "error = 'Response did not return a 200 OK (status code was %i)' % response.status_int\nreturn self.assertEqual(response.status_int, 200, error)", "path": "gaetestbed\\web.py", "repo_name": "jgeewax/gaetestbed", "stars": 40, "license": "other", "language": "python", "size": 289}
{"docstring": "\"\"\"\nThis method is called at the start of each test case.\n\nIf you need to use this method for your own test set up, make sure\nthat you call ``super()``. If not, the cache may not be emptied\nproperly and your tests might not be properly sandboxed.\n\nHere is an example of how to properly override the ``setUp`` method::\n\n    import unittest\n    \n    from gaetestbed import MemcacheTestCase\n    \n    class MyTestCase(MemcacheTestCase, unittest.TestCase):\n        def setUp(self):\n            super(MyTestCase, self).setUp()\n            # Do anything else you need here\n\"\"\"\n", "func_signal": "def setUp(self):\n", "code": "super(MemcacheTestCase, self).setUp()\nself.clear_memcache()", "path": "gaetestbed\\memcache.py", "repo_name": "jgeewax/gaetestbed", "stars": 40, "license": "other", "language": "python", "size": 289}
{"docstring": "\"\"\"\nAsserts that an e-mail with various details was not sent.\n\nWith all parameters blank, this method will fail if any messages were sent. This\nis roughly equivalent to::\n\n    self.assertLength(self.get_sent_messages(), 0)\n\nIn short, this method behaves as exactly the opposite of ``self.assertEmailSent()``.\nAny place that assertion would pass, this should fail.\n\"\"\"\n", "func_signal": "def assertEmailNotSent(self, to=None, sender=None, subject=None, body=None, html=None):\n", "code": "messages = self.get_sent_messages(\n    to = to,\n    sender = sender,\n    subject = subject,\n    body = body,\n    html = html,\n)\n\nif messages:\n    failure_message = \"Unexpected e-mail message sent.\"\n    \n    details = self._get_email_detail_string(to, sender, subject, body, html)\n    if details:\n        failure_message += ' Arguments expected: %s' % details\n    \n    self.fail(failure_message)", "path": "gaetestbed\\mail.py", "repo_name": "jgeewax/gaetestbed", "stars": 40, "license": "other", "language": "python", "size": 289}
{"docstring": "\"\"\" Finished downloading \"\"\"\n", "func_signal": "def endDownload():\n", "code": "Hellanzb.ht.rate = 0\nsessionStartTime = None\nsessionReadBytes = 0\nfor nsf in Hellanzb.nsfs:\n    sessionReadBytes += nsf.sessionReadBytes\n    if nsf.fillServerPriority == 0:\n        sessionStartTime = nsf.sessionStartTime\n    nsf.endDownload()\n\nHellanzb.downloading = False\nHellanzb.totalSpeed = 0\nHellanzb.scroller.currentLog = None\n\nscrollEnd()\n\nHellanzb.downloadScannerID.cancel()\nHellanzb.totalArchivesDownloaded += 1\nwriteStateXML()\n\nif not len(Hellanzb.queue.currentNZBs()):\n    # END\n    return\n\ncurrentNZB = Hellanzb.queue.currentNZBs()[0]\ndownloadTime = time.time() - currentNZB.downloadStartTime\nspeed = sessionReadBytes / 1024.0 / downloadTime\ninfo('Transferred %s in %s at %.1fKB/s (%s)' % \\\n     (prettySize(sessionReadBytes), prettyElapsed(downloadTime), speed,\n      currentNZB.archiveName))\nif not currentNZB.isParRecovery:\n    currentNZB.downloadTime = downloadTime\nelse:\n    currentNZB.downloadTime += downloadTime\n# END", "path": "Hellanzb\\Daemon.py", "repo_name": "pjenvey/hellanzb", "stars": 43, "license": "other", "language": "python", "size": 1470}
{"docstring": "\"\"\" Set the rarPassword on the specified NZB or NZB archive \"\"\"\n", "func_signal": "def setRarPassword(nzbId, rarPassword):\n", "code": "try:\n    nzbId = int(nzbId)\nexcept:\n    debug('Invalid ID: ' + str(nzbId))\n    return False\n\n# Find the nzbId in the queued list, processing list, or currently downloading nzb\nfound = None\nfor collection in (Hellanzb.queue.currentNZBs(), Hellanzb.postProcessors,\n                   Hellanzb.nzbQueue):\n    for nzbOrArchive in collection:\n        if nzbOrArchive.id == nzbId:\n            found = nzbOrArchive\n            break\n\nif found:\n    found.rarPassword = rarPassword\n    writeStateXML()\n    return True\n\nreturn False", "path": "Hellanzb\\Daemon.py", "repo_name": "pjenvey/hellanzb", "stars": 43, "license": "other", "language": "python", "size": 1470}
{"docstring": "\"\"\"\nGet the Message-IDs for all new news posted to any of the given\ngroups since the specified date - in seconds since the epoch, GMT -\noptionally restricted to the given distributions.  gotNewNews() is\ncalled on success, getNewNewsFailed() on failure.\n\nOne invocation of this function may result in multiple invocations\nof gotNewNews()/getNewNewsFailed().\n\"\"\"\n", "func_signal": "def fetchNewNews(self, groups, date, distributions = ''):\n", "code": "date, timeStr = time.strftime('%y%m%d %H%M%S', time.gmtime(date)).split()\nline = 'NEWNEWS %%s %s %s %s' % (date, timeStr, distributions)\ngroupPart = ''\nwhile len(groups) and len(line) + len(groupPart) + len(groups[-1]) + 1 < NNTPClient.MAX_COMMAND_LENGTH:\n    group = groups.pop()\n    groupPart = groupPart + ',' + group\n\nself.sendLine(line % (groupPart,))\nself._newState(self._stateNewNews, self.getNewNewsFailed)\n\nif len(groups):\n    self.fetchNewNews(groups, date, distributions)", "path": "Hellanzb\\NZBLeecher\\nntp.py", "repo_name": "pjenvey/hellanzb", "stars": 43, "license": "other", "language": "python", "size": 1470}
{"docstring": "\"\"\" Pause the current download \"\"\"\n", "func_signal": "def pauseCurrent():\n", "code": "Hellanzb.downloadPaused = True\n\nfor nsf in Hellanzb.nsfs:\n    for client in nsf.clients:\n        client.transport.stopReading()\n\ninfo('Pausing downloader')\nreturn True", "path": "Hellanzb\\Daemon.py", "repo_name": "pjenvey/hellanzb", "stars": 43, "license": "other", "language": "python", "size": 1470}
{"docstring": "\"\"\" Disconnect antiIdle==0 factories when there's nothing left to download \"\"\"\n", "func_signal": "def disconnectUnAntiIdleFactories():\n", "code": "if len(Hellanzb.queue.nzbs):\n    return\n\n# Nothing left to download. Immediately disconnect antiIdleTimeout factories\nfor nsf in Hellanzb.nsfs:\n    debug('Empty NZB queue: disconnecting %s (antiIdle is 0)' % nsf.serverPoolName)\n    if nsf.antiIdleTimeout == 0:\n        for client in nsf.clients:\n            client.transport.loseConnection()\n\n            # NOTE: WEIRD: after pool-coop branch, I have to force this to prevent\n            # fetchNextNZBSegment from re-calling the fetch loop (it gets called\n            # twice. the parseNZB->beginDownload->fetchNext call is made before\n            # the client gets to call connectionLost). or has this problem always\n            # existed??? See r403\n            client.isLoggedIn = False\n\n            client.deactivate()", "path": "Hellanzb\\Daemon.py", "repo_name": "pjenvey/hellanzb", "stars": 43, "license": "other", "language": "python", "size": 1470}
{"docstring": "\"\"\" Call the post processor via twisted \"\"\"\n", "func_signal": "def postProcess(options, isQueueDaemon=False):\n", "code": "from Hellanzb.Core import shutdown\nif not os.path.isdir(options.postProcessDir):\n    error('Unable to process, not a directory: ' + options.postProcessDir)\n    shutdown()\n    return\n\nif not os.access(options.postProcessDir, os.R_OK):\n    error('Unable to process, no read access to directory: ' + options.postProcessDir)\n    shutdown()\n    return\n\nrarPassword = None\nif options.rarPassword:\n    rarPassword = options.rarPassword\n\n# UNIX: realpath\n# FIXME: I don't recall why realpath is even necessary\ndirName = os.path.realpath(options.postProcessDir)\narchive = PostProcessorUtil.Archive(dirName, rarPassword=rarPassword)\ntroll = Hellanzb.PostProcessor.PostProcessor(archive, background=False)\n\nreactor.callLater(0, info, '')\nreactor.callLater(0, info, 'Starting post processor')\nreactor.callLater(0, reactor.callInThread, troll.run)\nif isQueueDaemon:\n    reactor.callLater(0, writeStateXML)", "path": "Hellanzb\\Daemon.py", "repo_name": "pjenvey/hellanzb", "stars": 43, "license": "other", "language": "python", "size": 1470}
{"docstring": "\"\"\"\nRequest a list of the groups it is recommended a new user subscribe to.\ngotSubscriptions() is called on success, getSubscriptionsFailed() on\nfailure\n\"\"\"\n", "func_signal": "def fetchSubscriptions(self):\n", "code": "self.sendLine('LIST SUBSCRIPTIONS')\nself._newState(self._stateSubscriptions, self.getSubscriptionsFailed)", "path": "Hellanzb\\NZBLeecher\\nntp.py", "repo_name": "pjenvey/hellanzb", "stars": 43, "license": "other", "language": "python", "size": 1470}
{"docstring": "\"\"\"\nAttempt to post an article with the specified text to the server.  'text'\nmust consist of both head and body data, as specified by RFC 850.  If the\narticle is posted successfully, postedOk() is called, otherwise postFailed()\nis called.\n\"\"\"\n", "func_signal": "def postArticle(self, text):\n", "code": "self.sendLine('POST')\nself._newState(None, self.postFailed, self._headerPost)\nself._postText.append(text)", "path": "Hellanzb\\NZBLeecher\\nntp.py", "repo_name": "pjenvey/hellanzb", "stars": 43, "license": "other", "language": "python", "size": 1470}
{"docstring": "\"\"\" Immediately begin (force) downloading recovery blocks (only the nzb.neededBlocks\namount) for the specified NZB \"\"\"\n", "func_signal": "def forceNZBParRecover(nzb):\n", "code": "nzb.isParRecovery = True\n\nif not len(Hellanzb.nzbQueue) and not len(Hellanzb.queue.currentNZBs()):\n    new = os.path.join(Hellanzb.CURRENT_DIR, os.path.basename(nzb.nzbFileName))\n    move(nzb.nzbFileName, new)\n    nzb.nzbFileName = new\n\n    # FIXME: Would be nice to include the number of needed recovery blocks in the\n    # growl notification this triggers\n    if Hellanzb.downloadScannerID is not None and \\\n            not Hellanzb.downloadScannerID.cancelled and \\\n            not Hellanzb.downloadScannerID.called:\n        Hellanzb.downloadScannerID.cancel()\n\n    nzb.destDir = Hellanzb.WORKING_DIR\n    parseNZB(nzb, 'Downloading recovery pars')\nelse:\n    Hellanzb.nzbQueue.insert(0, nzb)\n    forceNZB(nzb.nzbFileName, 'Forcing par recovery download')", "path": "Hellanzb\\Daemon.py", "repo_name": "pjenvey/hellanzb", "stars": 43, "license": "other", "language": "python", "size": 1470}
{"docstring": "\"\"\"\nRequest the overview format from the server.  gotOverview() is called\non success, getOverviewFailed() on failure\n\"\"\"\n", "func_signal": "def fetchOverview(self):\n", "code": "self.sendLine('LIST OVERVIEW.FMT')\nself._newState(self._stateOverview, self.getOverviewFailed)", "path": "Hellanzb\\NZBLeecher\\nntp.py", "repo_name": "pjenvey/hellanzb", "stars": 43, "license": "other", "language": "python", "size": 1470}
{"docstring": "\"\"\" Ensure that all the required directories exist and are writable, otherwise attempt to\ncreate them \"\"\"\n", "func_signal": "def ensureDaemonDirs():\n", "code": "dirNames = {}\nfor arg in dir(Hellanzb):\n    if arg.endswith(\"_DIR\") and arg == arg.upper():\n        dirName = getattr(Hellanzb, arg)\n        if dirName is None:\n            raise FatalError('Required directory not defined in config file: Hellanzb.' + arg)\n        dirNames[arg] = dirName\n        \nensureDirs(dirNames)\n\nif hasattr(Hellanzb, 'QUEUE_LIST'):\n    if not hasattr(Hellanzb, 'STATE_XML_FILE'):\n        Hellanzb.STATE_XML_FILE = Hellanzb.QUEUE_LIST\nif not hasattr(Hellanzb, 'STATE_XML_FILE'):\n    raise FatalError('Hellanzb.STATE_XML_FILE not defined in config file')\nelif os.path.isfile(Hellanzb.STATE_XML_FILE) and not os.access(Hellanzb.STATE_XML_FILE, os.W_OK):\n    raise FatalError('hellanzb does not have write access to the Hellanzb.STATE_XML_FILE file')", "path": "Hellanzb\\Daemon.py", "repo_name": "pjenvey/hellanzb", "stars": 43, "license": "other", "language": "python", "size": 1470}
{"docstring": "\"\"\"\nFetch all new articles from the given groups since the\ngiven date and dump them into the given storage.  groups\nis a list of group names.  date is an integer or floating\npoint representing seconds since the epoch (GMT).  storage is\nany object that implements the NewsStorage interface.\n\"\"\"\n", "func_signal": "def __init__(self, groups, date, storage):\n", "code": "NNTPClient.__init__(self)\nself.groups, self.date, self.storage = groups, date, storage", "path": "Hellanzb\\NZBLeecher\\nntp.py", "repo_name": "pjenvey/hellanzb", "stars": 43, "license": "other", "language": "python", "size": 1470}
{"docstring": "\"\"\" Change the MAX_RATE value. Return the new value \"\"\"\n", "func_signal": "def maxRate(rate):\n", "code": "if rate == 'None' or rate is None:\n    rate = 0\nelse:\n    try:\n        rate = int(rate)\n    except:\n        return getRate()\n\nif rate < 0:\n    rate = 0\n    \ninfo('Resetting MAX_RATE to: ' + str(rate) + 'KB/s')\n\nrate = rate * 1024\n    \nrestartCheckRead = False\nif rate == 0:\n    if Hellanzb.ht.unthrottleReadsID is not None and \\\n            not Hellanzb.ht.unthrottleReadsID.cancelled and \\\n            not Hellanzb.ht.unthrottleReadsID.called:\n        Hellanzb.ht.unthrottleReadsID.cancel()\n\n    if Hellanzb.ht.checkReadBandwidthID is not None and \\\n        not Hellanzb.ht.checkReadBandwidthID.cancelled:\n        Hellanzb.ht.checkReadBandwidthID.cancel()\n    Hellanzb.ht.unthrottleReads()\nelif Hellanzb.ht.readLimit == 0 and rate > 0:\n    restartCheckRead = True\n    \nHellanzb.ht.readLimit = rate\n\nif restartCheckRead:\n    Hellanzb.ht.readThisSecond = 0 # nobody's been resetting this value\n    reactor.callLater(1, Hellanzb.ht.checkReadBandwidth)\nreturn getRate()", "path": "Hellanzb\\Daemon.py", "repo_name": "pjenvey/hellanzb", "stars": 43, "license": "other", "language": "python", "size": 1470}
{"docstring": "\"\"\" Pickup left off Post Processors that were cancelled via CTRL-C \"\"\"\n# FIXME: with the new queue, could kill the processing dir sym links (for windows)\n", "func_signal": "def resumePostProcessors():\n", "code": "from Hellanzb.NZBLeecher.NZBModel import NZB\nfor archiveDirName in os.listdir(Hellanzb.PROCESSING_DIR):\n    if archiveDirName[0] == '.':\n        continue\n    \n    archive = NZB.fromStateXML('processing', archiveDirName)\n    troll = PostProcessor.PostProcessor(archive)\n\n    info('Resuming post processor: ' + archiveName(archiveDirName))\n    troll.start()", "path": "Hellanzb\\Daemon.py", "repo_name": "pjenvey/hellanzb", "stars": 43, "license": "other", "language": "python", "size": 1470}
{"docstring": "\"\"\"\nGet the names of all new groups created/added to the server since\nthe specified date - in seconds since the ecpoh, GMT - optionally\nrestricted to the given distributions.  gotNewGroups() is called\non success, getNewGroupsFailed() on failure.\n\"\"\"\n", "func_signal": "def fetchNewGroups(self, date, distributions):\n", "code": "date, timeStr = time.strftime('%y%m%d %H%M%S', time.gmtime(date)).split()\nself.sendLine('NEWGROUPS %s %s %s' % (date, timeStr, distributions))\nself._newState(self._stateNewGroups, self.getNewGroupsFailed)", "path": "Hellanzb\\NZBLeecher\\nntp.py", "repo_name": "pjenvey/hellanzb", "stars": 43, "license": "other", "language": "python", "size": 1470}
{"docstring": "\"\"\" Hand-off from the downloader -- make a dir for the NZB with its contents, then post\nprocess it in a separate thread\"\"\"\n", "func_signal": "def handleNZBDone(nzb):\n", "code": "disconnectUnAntiIdleFactories()\n\nif nzb.downloadStartTime:\n    downloadAndDecodeTime = time.time() - nzb.downloadStartTime\n    if not nzb.isParRecovery:\n        nzb.downloadAndDecodeTime = downloadAndDecodeTime\n    else:\n        nzb.downloadAndDecodeTime += downloadAndDecodeTime\n\n# Make our new directory, minus the .nzb\nprocessingDir = os.path.join(Hellanzb.PROCESSING_DIR, nzb.archiveName)\n\n# Move our nzb contents to their new location for post processing\nhellaRename(processingDir)\n    \nmove(Hellanzb.WORKING_DIR, processingDir)\nnzb.destDir = processingDir\nnzb.archiveDir = processingDir\n\nnzbFileName = os.path.join(processingDir, os.path.basename(nzb.nzbFileName))\n# We may have downloaded an NZB file of the same name:\n# http://hellanzb.com/trac/hellanzb/ticket/425\nhellaRename(nzbFileName)\nmove(nzb.nzbFileName, nzbFileName)\nnzb.nzbFileName = nzbFileName\n\nos.mkdir(Hellanzb.WORKING_DIR)\n\n# The list of skipped pars is maintained in the state XML as only the subjects of the\n# nzbFiles. PostProcessor only knows to look at the NZB.skippedParSubjects list,\n# created here\nnzb.skippedParSubjects = nzb.getSkippedParSubjects()\n\n# Finally unarchive/process the directory in another thread, and continue\n# nzbing\ntroll = PostProcessor.PostProcessor(nzb)\n\n# Give NZBLeecher some time (another reactor loop) to killHistory() & scrollEnd()\n# without any logging interference from PostProcessor\nreactor.callLater(0, troll.start)\nreactor.callLater(0, writeStateXML)\nreactor.callLater(0, scanQueueDir)", "path": "Hellanzb\\Daemon.py", "repo_name": "pjenvey/hellanzb", "stars": 43, "license": "other", "language": "python", "size": 1470}
{"docstring": "\"\"\"\nGet the body for the specified article (or the currently selected\narticle if index is '') from the server.  gotBody() is called on\nsuccess, getBodyFailed() on failure\n\"\"\"\n", "func_signal": "def fetchBody(self, index = ''):\n", "code": "self.sendLine('BODY %s' % (index,))\nself._newState(self._stateBody, self.getBodyFailed)", "path": "Hellanzb\\NZBLeecher\\nntp.py", "repo_name": "pjenvey/hellanzb", "stars": 43, "license": "other", "language": "python", "size": 1470}
{"docstring": "\"\"\" Initialize the download. Notify the downloaders to begin their work, etc \"\"\"\n# BEGIN\n", "func_signal": "def beginDownload(nzb=None):\n", "code": "Hellanzb.loggedIdleMessage = False\nwriteStateXML()\nnow = time.time()\nif nzb:\n    nzb.downloadStartTime = now\n\n# The scroll level will flood the console with constantly updating\n# statistics -- the logging system can interrupt this scroll\n# temporarily (after scrollBegin)\nscrollBegin()\n\n# Scan the queue dir intermittently during downloading. Reset the scanner delayed call\n# if it's already going\nif Hellanzb.downloadScannerID is not None and \\\n        not Hellanzb.downloadScannerID.cancelled and \\\n        not Hellanzb.downloadScannerID.called:\n    Hellanzb.downloadScannerID.cancel()\nHellanzb.downloadScannerID = reactor.callLater(5, scanQueueDir, False, True)\n\nfor nsf in Hellanzb.nsfs:\n    nsf.beginDownload()\n\nHellanzb.downloading = True", "path": "Hellanzb\\Daemon.py", "repo_name": "pjenvey/hellanzb", "stars": 43, "license": "other", "language": "python", "size": 1470}
{"docstring": "\"\"\" Clear the queue -- optionally clear what's currently being downloaded (cancel it) \"\"\"\n", "func_signal": "def clearCurrent(andCancel):\n", "code": "info('Clearing queue')\ndequeueNZBs([nzb.id for nzb in Hellanzb.nzbQueue], quiet=True)\n\nif andCancel:\n    cancelCurrent()\n\nreturn True", "path": "Hellanzb\\Daemon.py", "repo_name": "pjenvey/hellanzb", "stars": 43, "license": "other", "language": "python", "size": 1470}
{"docstring": "\"\"\"\nGet group information for the specified group from the server.  gotGroup()\nis called on success, getGroupFailed() on failure.\n\"\"\"\n", "func_signal": "def fetchGroup(self, group):\n", "code": "self.sendLine('GROUP %s' % (group,))\nself._newState(None, self.getGroupFailed, self._headerGroup)", "path": "Hellanzb\\NZBLeecher\\nntp.py", "repo_name": "pjenvey/hellanzb", "stars": 43, "license": "other", "language": "python", "size": 1470}
{"docstring": "# generate a sorted dict of fields corresponding to the Field model\n# for the Ad instance\n", "func_signal": "def fields_for_ad(instance):\n", "code": "fields_dict = SortedDict()\nfields = field_list(instance)\n# this really, really should be refactored\nfor field in fields:\n    if field.field_type == Field.BOOLEAN_FIELD:\n        fields_dict[field.name] = forms.BooleanField(label=field.label, required=False, help_text=field.help_text)\n    elif field.field_type == Field.CHAR_FIELD:\n        widget = forms.TextInput\n        fields_dict[field.name] = forms.CharField(label=field.label, required=field.required, max_length=field.max_length, help_text=field.help_text, widget=widget)\n    elif field.field_type == Field.DATE_FIELD:\n        fields_dict[field.name] = forms.DateField(label=field.label, required=field.required, help_text=field.help_text)\n    elif field.field_type == Field.DATETIME_FIELD:\n        fields_dict[field.name] = forms.DateTimeField(label=field.label, required=field.required, help_text=field.help_text)\n    elif field.field_type == Field.EMAIL_FIELD:\n        fields_dict[field.name] = forms.EmailField(label=field.label, required=field.required, help_text=field.help_text)\n    elif field.field_type == Field.FLOAT_FIELD:\n        fields_dict[field.name] = forms.FloatField(label=field.label, required=field.required, help_text=field.help_text)\n    elif field.field_type == Field.INTEGER_FIELD:\n        fields_dict[field.name] = forms.IntegerField(label=field.label, required=field.required, help_text=field.help_text)\n    elif field.field_type == Field.TIME_FIELD:\n        fields_dict[field.name] = forms.TimeField(label=field.label, required=field.required, help_text=field.help_text)\n    elif field.field_type == Field.URL_FIELD:\n        fields_dict[field.name] = forms.URLField(label=field.label, required=field.required, help_text=field.help_text)\n    elif field.field_type == Field.SELECT_FIELD:\n        options = field.options.split(',')\n        fields_dict[field.name] = forms.ChoiceField(label=field.label, required=field.required, help_text=field.help_text, choices=zip(options, options))\n    elif field.field_type == Field.TEXT_FIELD:\n        if field.enable_wysiwyg:\n            widget = TinyMCEWidget\n            field_type = TinyMCEField\n        else:\n            widget = forms.Textarea\n            field_type = forms.CharField\n\n        fields_dict[field.name] = field_type(label=field.label,\n                                             required=field.required,\n                                             help_text=field.help_text,\n                                             max_length=field.max_length,\n                                             widget=widget)\n    else:\n        raise NotImplementedError(u'Unknown field type \"%s\"' % field.get_field_type_display())\n\nreturn fields_dict", "path": "classifieds\\utils.py", "repo_name": "saebyn/django-classifieds", "stars": 57, "license": "bsd-3-clause", "language": "python", "size": 975}
{"docstring": "\"\"\"\nReturns a new QuerySet containing only items with at least\none attribute that matches the user's keywords.\n\"\"\"\n", "func_signal": "def filter(self, qs):\n", "code": "if not self.is_empty():\n    # Create a set of all field value IDs\n    allAdIDs = set()\n\n    if 'title' in self.fieldNames:\n        ad_title_qs = Ad.objects.filter(title__search=self.cleaned_data[\"keywords\"])\n        allAdIDs |= set([val.pk for val in ad_title_qs])\n\n    fvs = set(FieldValue.objects.filter(value__search=self.cleaned_data[\"keywords\"]))\n\n    # Join the current set with this set\n    allAdIDs |= set([val.ad.pk for val in fvs])\n\n    return qs.filter(pk__in=list(allAdIDs))\nelse:\n    return qs", "path": "classifieds\\search.py", "repo_name": "saebyn/django-classifieds", "stars": 57, "license": "bsd-3-clause", "language": "python", "size": 975}
{"docstring": "\"\"\"\nThis creates a MultiForm from the given field list and,\noptionally, a response.  NOTE: It DOES NOT remove\nthe fields it uses from fieldsLeft.\n\"\"\"\n", "func_signal": "def create(fields, fieldsLeft, response=None):\n", "code": "inits = {\"keywords\": [\"\"]}  # ,\"criteria\":[]}\nif response != None:\n    inits.update(response)\n\ninits[\"keywords\"] = inits[\"keywords\"][0]\n\nx = MultiForm(inits)\nx.fieldNames = fieldsLeft\n#x.fields[\"criteria\"].choices=[(field.name,field.label)\n#  for field in fields if field.name in fieldsLeft]\nreturn x", "path": "classifieds\\search.py", "repo_name": "saebyn/django-classifieds", "stars": 57, "license": "bsd-3-clause", "language": "python", "size": 975}
{"docstring": "\"\"\" Delete all tags except for legal ones \"\"\"\n", "func_signal": "def handle_starttag(self, tag, attrs):\n", "code": "if tag in self.valid_tags:\n    self.result = self.result + '<' + tag\n    for k, v in attrs:\n        if string.lower(k[0:2]) != 'on' and \\\n           string.lower(v[0:10]) != 'javascript':\n            self.result = '%s %s=\"%s\"' % (self.result, k, v)\n\n    endTag = '</%s>' % tag\n    self.endTagList.insert(0, endTag)\n    self.result = self.result + '>'", "path": "classifieds\\utils.py", "repo_name": "saebyn/django-classifieds", "stars": 57, "license": "bsd-3-clause", "language": "python", "size": 975}
{"docstring": "\"\"\"\nCreates a new ZipCodeForm if the given fieldsLeft list contains\n'zip_code'.  Pass a dictionary (i.e. from response.GET\nor response.POST) that contains a 'zip_code' key if you want\nto initialize this form.\n\"\"\"\n", "func_signal": "def create(fields, fieldsLeft, response=None):\n", "code": "if 'zip_code' in fieldsLeft:\n    fieldsLeft.remove('zip_code')\n    if response != None:\n        return ZipCodeForm({'zip_code': response['zip_code'][0],\n                            'zip_range': response['zip_range'][0]})\n    else:\n        return ZipCodeForm()\nelse:\n    return None", "path": "classifieds\\search.py", "repo_name": "saebyn/django-classifieds", "stars": 57, "license": "bsd-3-clause", "language": "python", "size": 975}
{"docstring": "# clear payment\n", "func_signal": "def complete(self, amount=0.0):\n", "code": "if self.amount != amount:\n    return False\n\nself.paid = True\nself.paid_on = datetime.datetime.now()\nself.save()\n\n# update ad\nself.ad.expires_on += datetime.timedelta(days=payment.pricing.length)\nself.ad.created_on = datetime.datetime.now()\nself.ad.active = True\nself.ad.save()\n\n# send email for payment\n# 1. render context to email template\nemail_template = loader.get_template('classifieds/email/payment.txt')\ncontext = Context({'payment': self})\nemail_contents = email_template.render(context)\n\n# 2. send email\nsend_mail(_('Your payment has been processed.'),\n          email_contents, settings.FROM_EMAIL,\n          [self.ad.user.email], fail_silently=False)", "path": "classifieds\\models.py", "repo_name": "saebyn/django-classifieds", "stars": 57, "license": "bsd-3-clause", "language": "python", "size": 975}
{"docstring": "\"\"\"Validates max_length and min_length. Returns a Unicode object.\"\"\"\n", "func_signal": "def clean(self, value):\n", "code": "if value in EMPTY_VALUES:\n    return u''\n\nstripped_value = re.sub(r'<.*?>', '', value)\nstripped_value = string.replace(stripped_value, '&nbsp;', ' ')\nstripped_value = string.replace(stripped_value, '&lt;', '<')\nstripped_value = string.replace(stripped_value, '&gt;', '>')\nstripped_value = string.replace(stripped_value, '&amp;', '&')\nstripped_value = string.replace(stripped_value, '\\n', '')\nstripped_value = string.replace(stripped_value, '\\r', '')\n\nvalue_length = len(stripped_value)\nvalue_length -= 1\nif self.max_length is not None and value_length > self.max_length:\n    raise forms.ValidationError(self.error_messages['max_length'] % {'max': self.max_length, 'length': value_length})\nif self.min_length is not None and value_length < self.min_length:\n    raise forms.ValidationError(self.error_messages['min_length'] % {'min': self.min_length, 'length': value_length})\n\nreturn value", "path": "classifieds\\utils.py", "repo_name": "saebyn/django-classifieds", "stars": 57, "license": "bsd-3-clause", "language": "python", "size": 975}
{"docstring": "# Deleting model 'Subcategory'\n", "func_signal": "def forwards(self, orm):\n", "code": "        db.delete_table('classifieds_subcategory')\n\n        # Deleting model 'SiteSetting'\n        db.delete_table('classifieds_sitesetting')\n\n        # Changing field 'Payment.paid_on'\n        db.alter_column('classifieds_payment', 'paid_on', self.gf('django.db.models.fields.DateTimeField')(null=True))", "path": "classifieds\\migrations\\0002_auto__del_subcategory__del_sitesetting__chg_field_payment_paid_on.py", "repo_name": "saebyn/django-classifieds", "stars": 57, "license": "bsd-3-clause", "language": "python", "size": 975}
{"docstring": "# find the ad, if available\n", "func_signal": "def view(request, pk):\n", "code": "ad = get_object_or_404(Ad, pk=pk, active=True)\n\n# only show an expired ad if this user owns it\nif ad.expires_on < datetime.datetime.now() and ad.user != request.user:\n    raise Http404\n\nreturn render_category_page(request, ad.category, 'view.html', {'ad': ad})", "path": "classifieds\\views\\browse.py", "repo_name": "saebyn/django-classifieds", "stars": 57, "license": "bsd-3-clause", "language": "python", "size": 975}
{"docstring": "# Deleting model 'ImageFormat'\n", "func_signal": "def backwards(self, orm):\n", "code": "        db.delete_table('classifieds_imageformat')\n\n        # Deleting model 'Category'\n        db.delete_table('classifieds_category')\n\n        # Removing M2M table for field images_allowed_formats on 'Category'\n        db.delete_table('classifieds_category_images_allowed_formats')\n\n        # Deleting model 'Subcategory'\n        db.delete_table('classifieds_subcategory')\n\n        # Deleting model 'Field'\n        db.delete_table('classifieds_field')\n\n        # Deleting model 'Ad'\n        db.delete_table('classifieds_ad')\n\n        # Deleting model 'AdImage'\n        db.delete_table('classifieds_adimage')\n\n        # Deleting model 'FieldValue'\n        db.delete_table('classifieds_fieldvalue')\n\n        # Deleting model 'Pricing'\n        db.delete_table('classifieds_pricing')\n\n        # Deleting model 'PricingOptions'\n        db.delete_table('classifieds_pricingoptions')\n\n        # Deleting model 'ZipCode'\n        db.delete_table('classifieds_zipcode')\n\n        # Deleting model 'SiteSetting'\n        db.delete_table('classifieds_sitesetting')\n\n        # Deleting model 'Payment'\n        db.delete_table('classifieds_payment')\n\n        # Removing M2M table for field options on 'Payment'\n        db.delete_table('classifieds_payment_options')\n\n        # Deleting model 'UserProfile'\n        db.delete_table('classifieds_userprofile')", "path": "classifieds\\migrations\\0001_initial.py", "repo_name": "saebyn/django-classifieds", "stars": 57, "license": "bsd-3-clause", "language": "python", "size": 975}
{"docstring": "# reset the search params, if present\n", "func_signal": "def search_in_category(request, slug):\n", "code": "try:\n    del request.session['search']\nexcept KeyError:\n    pass\n\nreturn search_results(request, slug)", "path": "classifieds\\views\\browse.py", "repo_name": "saebyn/django-classifieds", "stars": 57, "license": "bsd-3-clause", "language": "python", "size": 975}
{"docstring": "# Adding field 'AdImage.thumb_photo'\n", "func_signal": "def backwards(self, orm):\n", "code": "        db.add_column('classifieds_adimage', 'thumb_photo', self.gf('django.db.models.fields.files.ImageField')(default='', max_length=100, blank=True), keep_default=False)\n\n        # Changing field 'AdImage.full_photo'\n        db.alter_column('classifieds_adimage', 'full_photo', self.gf('django.db.models.fields.files.ImageField')(max_length=100))", "path": "classifieds\\migrations\\0003_auto__del_field_adimage_thumb_photo__chg_field_adimage_full_photo.py", "repo_name": "saebyn/django-classifieds", "stars": 57, "license": "bsd-3-clause", "language": "python", "size": 975}
{"docstring": "# Deleting field 'AdImage.thumb_photo'\n", "func_signal": "def forwards(self, orm):\n", "code": "        db.delete_column('classifieds_adimage', 'thumb_photo')\n\n        # Changing field 'AdImage.full_photo'\n        db.alter_column('classifieds_adimage', 'full_photo', self.gf('sorl.thumbnail.fields.ImageField')(max_length=100))", "path": "classifieds\\migrations\\0003_auto__del_field_adimage_thumb_photo__chg_field_adimage_full_photo.py", "repo_name": "saebyn/django-classifieds", "stars": 57, "license": "bsd-3-clause", "language": "python", "size": 975}
{"docstring": "# Adding model 'ImageFormat'\n", "func_signal": "def forwards(self, orm):\n", "code": "        db.create_table('classifieds_imageformat', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('format', self.gf('django.db.models.fields.CharField')(max_length=10)),\n        ))\n        db.send_create_signal('classifieds', ['ImageFormat'])\n\n        # Adding model 'Category'\n        db.create_table('classifieds_category', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('site', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['sites.Site'])),\n            ('template_prefix', self.gf('django.db.models.fields.CharField')(max_length=200, blank=True)),\n            ('name', self.gf('django.db.models.fields.CharField')(max_length=200)),\n            ('slug', self.gf('django.db.models.fields.SlugField')(max_length=50, db_index=True)),\n            ('enable_contact_form_upload', self.gf('django.db.models.fields.BooleanField')(default=False)),\n            ('contact_form_upload_max_size', self.gf('django.db.models.fields.IntegerField')(default=1048576)),\n            ('contact_form_upload_file_extensions', self.gf('django.db.models.fields.CharField')(default='txt,doc,odf,pdf', max_length=200)),\n            ('images_max_count', self.gf('django.db.models.fields.IntegerField')(default=0)),\n            ('images_max_width', self.gf('django.db.models.fields.IntegerField')(default=1024)),\n            ('images_max_height', self.gf('django.db.models.fields.IntegerField')(default=1024)),\n            ('images_max_size', self.gf('django.db.models.fields.IntegerField')(default=1048576)),\n            ('description', self.gf('django.db.models.fields.TextField')(default='')),\n            ('sortby_fields', self.gf('django.db.models.fields.CharField')(max_length=200, blank=True)),\n            ('sort_order', self.gf('django.db.models.fields.PositiveIntegerField')(default=0)),\n        ))\n        db.send_create_signal('classifieds', ['Category'])\n\n        # Adding M2M table for field images_allowed_formats on 'Category'\n        db.create_table('classifieds_category_images_allowed_formats', (\n            ('id', models.AutoField(verbose_name='ID', primary_key=True, auto_created=True)),\n            ('category', models.ForeignKey(orm['classifieds.category'], null=False)),\n            ('imageformat', models.ForeignKey(orm['classifieds.imageformat'], null=False))\n        ))\n        db.create_unique('classifieds_category_images_allowed_formats', ['category_id', 'imageformat_id'])\n\n        # Adding model 'Subcategory'\n        db.create_table('classifieds_subcategory', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('name', self.gf('django.db.models.fields.CharField')(max_length=200)),\n            ('slug', self.gf('django.db.models.fields.SlugField')(max_length=50, db_index=True)),\n            ('category', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['classifieds.Category'])),\n        ))\n        db.send_create_signal('classifieds', ['Subcategory'])\n\n        # Adding model 'Field'\n        db.create_table('classifieds_field', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('category', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['classifieds.Category'], null=True, blank=True)),\n            ('name', self.gf('django.db.models.fields.CharField')(max_length=100)),\n            ('label', self.gf('django.db.models.fields.CharField')(max_length=200)),\n            ('field_type', self.gf('django.db.models.fields.IntegerField')()),\n            ('help_text', self.gf('django.db.models.fields.TextField')(blank=True)),\n            ('max_length', self.gf('django.db.models.fields.IntegerField')(null=True, blank=True)),\n            ('enable_counter', self.gf('django.db.models.fields.BooleanField')(default=False)),\n            ('enable_wysiwyg', self.gf('django.db.models.fields.BooleanField')(default=False)),\n            ('required', self.gf('django.db.models.fields.BooleanField')(default=False)),\n            ('options', self.gf('django.db.models.fields.TextField')(blank=True)),\n        ))\n        db.send_create_signal('classifieds', ['Field'])\n\n        # Adding model 'Ad'\n        db.create_table('classifieds_ad', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('category', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['classifieds.Category'])),\n            ('user', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['auth.User'])),\n            ('created_on', self.gf('django.db.models.fields.DateTimeField')(auto_now_add=True, blank=True)),\n            ('expires_on', self.gf('django.db.models.fields.DateTimeField')()),\n            ('active', self.gf('django.db.models.fields.BooleanField')(default=False)),\n            ('title', self.gf('django.db.models.fields.CharField')(max_length=255)),\n        ))\n        db.send_create_signal('classifieds', ['Ad'])\n\n        # Adding model 'AdImage'\n        db.create_table('classifieds_adimage', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('ad', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['classifieds.Ad'])),\n            ('full_photo', self.gf('django.db.models.fields.files.ImageField')(max_length=100, blank=True)),\n            ('thumb_photo', self.gf('django.db.models.fields.files.ImageField')(max_length=100, blank=True)),\n        ))\n        db.send_create_signal('classifieds', ['AdImage'])\n\n        # Adding model 'FieldValue'\n        db.create_table('classifieds_fieldvalue', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('field', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['classifieds.Field'])),\n            ('ad', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['classifieds.Ad'])),\n            ('value', self.gf('django.db.models.fields.TextField')()),\n        ))\n        db.send_create_signal('classifieds', ['FieldValue'])\n\n        # Adding model 'Pricing'\n        db.create_table('classifieds_pricing', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('length', self.gf('django.db.models.fields.IntegerField')()),\n            ('price', self.gf('django.db.models.fields.DecimalField')(max_digits=9, decimal_places=2)),\n        ))\n        db.send_create_signal('classifieds', ['Pricing'])\n\n        # Adding model 'PricingOptions'\n        db.create_table('classifieds_pricingoptions', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('name', self.gf('django.db.models.fields.IntegerField')()),\n            ('price', self.gf('django.db.models.fields.DecimalField')(max_digits=9, decimal_places=2)),\n        ))\n        db.send_create_signal('classifieds', ['PricingOptions'])\n\n        # Adding model 'ZipCode'\n        db.create_table('classifieds_zipcode', (\n            ('zipcode', self.gf('django.db.models.fields.IntegerField')(primary_key=True)),\n            ('latitude', self.gf('django.db.models.fields.FloatField')()),\n            ('longitude', self.gf('django.db.models.fields.FloatField')()),\n            ('city', self.gf('django.db.models.fields.CharField')(max_length=30)),\n            ('state', self.gf('django.db.models.fields.CharField')(max_length=2)),\n        ))\n        db.send_create_signal('classifieds', ['ZipCode'])\n\n        # Adding model 'SiteSetting'\n        db.create_table('classifieds_sitesetting', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('site', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['sites.Site'])),\n            ('name', self.gf('django.db.models.fields.CharField')(max_length=100)),\n            ('description', self.gf('django.db.models.fields.CharField')(max_length=200)),\n            ('value', self.gf('django.db.models.fields.CharField')(max_length=200)),\n        ))\n        db.send_create_signal('classifieds', ['SiteSetting'])\n\n        # Adding model 'Payment'\n        db.create_table('classifieds_payment', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('ad', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['classifieds.Ad'])),\n            ('paid', self.gf('django.db.models.fields.BooleanField')(default=False)),\n            ('paid_on', self.gf('django.db.models.fields.DateTimeField')()),\n            ('amount', self.gf('django.db.models.fields.DecimalField')(max_digits=9, decimal_places=2)),\n            ('pricing', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['classifieds.Pricing'])),\n        ))\n        db.send_create_signal('classifieds', ['Payment'])\n\n        # Adding M2M table for field options on 'Payment'\n        db.create_table('classifieds_payment_options', (\n            ('id', models.AutoField(verbose_name='ID', primary_key=True, auto_created=True)),\n            ('payment', models.ForeignKey(orm['classifieds.payment'], null=False)),\n            ('pricingoptions', models.ForeignKey(orm['classifieds.pricingoptions'], null=False))\n        ))\n        db.create_unique('classifieds_payment_options', ['payment_id', 'pricingoptions_id'])\n\n        # Adding model 'UserProfile'\n        db.create_table('classifieds_userprofile', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('user', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['auth.User'], unique=True)),\n            ('receives_new_posting_notices', self.gf('django.db.models.fields.BooleanField')(default=False)),\n            ('receives_newsletter', self.gf('django.db.models.fields.BooleanField')(default=False)),\n            ('address', self.gf('django.db.models.fields.CharField')(max_length=100, blank=True)),\n            ('city', self.gf('django.db.models.fields.CharField')(max_length=100, blank=True)),\n            ('state', self.gf('django.contrib.localflavor.us.models.USStateField')(max_length=2, blank=True)),\n            ('zipcode', self.gf('django.db.models.fields.CharField')(max_length=10, blank=True)),\n            ('phone', self.gf('django.contrib.localflavor.us.models.PhoneNumberField')(default='', max_length=20, blank=True)),\n        ))\n        db.send_create_signal('classifieds', ['UserProfile'])", "path": "classifieds\\migrations\\0001_initial.py", "repo_name": "saebyn/django-classifieds", "stars": 57, "license": "bsd-3-clause", "language": "python", "size": 975}
{"docstring": "\"\"\"\nReturns a new QuerySet with only items in the given zip code.\nRemember to validate this form before calling this function.\n\"\"\"\n", "func_signal": "def filter(self, qs):\n", "code": "if not self.is_empty():\n    zip_code = self.cleaned_data['zip_code']\n    radius = self.cleaned_data['zip_range']\n    zipcodeObj = ZipCode.objects.get(zipcode=zip_code)\n    zipcodes = [zipcode.zipcode for zipcode in zipcodeOb.nearby(radius)]\n\n    fvs = FieldValue.objects.filter(field__name=\"zip_code\")\n    fvs = fvs.filter(value__in=list(zipcodes))\n\n    validAds = [fv.ad.pk for fv in fvs]\n\n    return qs.filter(pk__in=validAds)\nelse:\n    return qs", "path": "classifieds\\search.py", "repo_name": "saebyn/django-classifieds", "stars": 57, "license": "bsd-3-clause", "language": "python", "size": 975}
{"docstring": "# filter search results\n", "func_signal": "def filter(self, qs):\n", "code": "if not self.is_empty():\n    allAdIDs = set()\n\n    for field in self.fields.keys():\n        if field in self.data and \\\n           self.data[field] != \"\" and \\\n           self.data[field] != [] and \\\n           self.data[field] != ['']:\n            if type(self.data[field]) == type([]):\n                fvs = set(FieldValue.objects.filter(field__name=field,\n                                                    value__in=self.data[field]))\n            else:\n                fvs = set(FieldValue.objects.filter(field__name=field,\n                                                    value=self.data[field]))\n\n            # Join the current set with this set\n            allAdIDs |= set([val.ad.pk for val in fvs])\n\n    return qs.filter(pk__in=list(allAdIDs))\n\nreturn qs", "path": "classifieds\\search.py", "repo_name": "saebyn/django-classifieds", "stars": 57, "license": "bsd-3-clause", "language": "python", "size": 975}
{"docstring": "\"\"\" Strip illegal HTML tags from string s \"\"\"\n", "func_signal": "def strip(s):\n", "code": "parser = StrippingParser()\nparser.feed(s)\nparser.close()\nparser.cleanup()\nreturn parser.result", "path": "classifieds\\utils.py", "repo_name": "saebyn/django-classifieds", "stars": 57, "license": "bsd-3-clause", "language": "python", "size": 975}
{"docstring": "\"\"\"\nList the categories available and send the user to the create_in_category\nview.\n\"\"\"\n", "func_signal": "def select_category(request):\n", "code": "return render_to_response('classifieds/category_choice.html',\n                          {'categories': Category.objects.all(),\n                           'type': 'create'},\n                          context_instance=RequestContext(request))", "path": "classifieds\\views\\create.py", "repo_name": "saebyn/django-classifieds", "stars": 57, "license": "bsd-3-clause", "language": "python", "size": 975}
{"docstring": "# Adding model 'Subcategory'\n", "func_signal": "def backwards(self, orm):\n", "code": "        db.create_table('classifieds_subcategory', (\n            ('category', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['classifieds.Category'])),\n            ('slug', self.gf('django.db.models.fields.SlugField')(max_length=50, db_index=True)),\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('name', self.gf('django.db.models.fields.CharField')(max_length=200)),\n        ))\n        db.send_create_signal('classifieds', ['Subcategory'])\n\n        # Adding model 'SiteSetting'\n        db.create_table('classifieds_sitesetting', (\n            ('description', self.gf('django.db.models.fields.CharField')(max_length=200)),\n            ('site', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['sites.Site'])),\n            ('value', self.gf('django.db.models.fields.CharField')(max_length=200)),\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('name', self.gf('django.db.models.fields.CharField')(max_length=100)),\n        ))\n        db.send_create_signal('classifieds', ['SiteSetting'])\n\n        # User chose to not deal with backwards NULL issues for 'Payment.paid_on'\n        raise RuntimeError(\"Cannot reverse this migration. 'Payment.paid_on' and its values cannot be restored.\")", "path": "classifieds\\migrations\\0002_auto__del_subcategory__del_sitesetting__chg_field_payment_paid_on.py", "repo_name": "saebyn/django-classifieds", "stars": 57, "license": "bsd-3-clause", "language": "python", "size": 975}
{"docstring": "# validate category slug\n", "func_signal": "def create_in_category(request, slug):\n", "code": "category = get_object_or_404(Category, slug=slug)\n\nad = Ad.objects.create(category=category, user=request.user,\n                       expires_on=datetime.datetime.now(), active=False)\nad.save()\nreturn redirect('classifieds_create_ad_edit', pk=ad.pk)", "path": "classifieds\\views\\create.py", "repo_name": "saebyn/django-classifieds", "stars": 57, "license": "bsd-3-clause", "language": "python", "size": 975}
{"docstring": "\"\"\"Closes this Web Socket.\n\nOnce the close handshake is successful the socket will be closed.\n\"\"\"\n", "func_signal": "def close(self):\n", "code": "if self.client_terminated and self._waiting:\n    tornado.ioloop.IOLoop.instance().remove_timeout(self._waiting)\n    self.stream.close()\nelse:\n    self.stream.write(\"\\xff\\x00\")\n    self._waiting = tornado.ioloop.IOLoop.instance().add_timeout(\n                        time.time() + 5, self._abort)", "path": "pkg\\tornado-1.2.1\\tornado\\websocket.py", "repo_name": "dongyi/tornado-chatroom", "stars": 39, "license": "None", "language": "python", "size": 768}
{"docstring": "\"\"\"Process any requests that were completed by the last\ncall to multi.socket_action.\n\"\"\"\n", "func_signal": "def _finish_pending_requests(self):\n", "code": "while True:\n    num_q, ok_list, err_list = self._multi.info_read()\n    for curl in ok_list:\n        self._finish(curl)\n    for curl, errnum, errmsg in err_list:\n        self._finish(curl, errnum, errmsg)\n    if num_q == 0:\n        break\nself._process_queue()", "path": "pkg\\tornado-1.2.1\\tornado\\httpclient.py", "repo_name": "dongyi/tornado-chatroom", "stars": 39, "license": "None", "language": "python", "size": 768}
{"docstring": "# header_line as returned by curl includes the end-of-line characters.\n", "func_signal": "def _curl_header_callback(headers, header_line):\n", "code": "header_line = header_line.strip()\nif header_line.startswith(\"HTTP/\"):\n    headers.clear()\n    return\nif not header_line:\n    return\nheaders.parse_line(header_line)", "path": "pkg\\tornado-1.2.1\\tornado\\httpclient.py", "repo_name": "dongyi/tornado-chatroom", "stars": 39, "license": "None", "language": "python", "size": 768}
{"docstring": "\"\"\"Executes an HTTPRequest, calling callback with an HTTPResponse.\n\nIf an error occurs during the fetch, the HTTPResponse given to the\ncallback has a non-None error attribute that contains the exception\nencountered during the request. You can call response.rethrow() to\nthrow the exception (if any) in the callback.\n\"\"\"\n", "func_signal": "def fetch(self, request, callback, **kwargs):\n", "code": "if not isinstance(request, HTTPRequest):\n    request = HTTPRequest(url=request, **kwargs)\nself._requests.append((request, stack_context.wrap(callback)))\nself._process_queue()\nself._set_timeout(0)", "path": "pkg\\tornado-1.2.1\\tornado\\httpclient.py", "repo_name": "dongyi/tornado-chatroom", "stars": 39, "license": "None", "language": "python", "size": 768}
{"docstring": "# implementation detail:  the full context stack at this point\n# is ['application', 'library', 'application'].  The 'library'\n# context was not removed, but is no longer innermost so\n# the application context takes precedence.\n", "func_signal": "def final_callback():\n", "code": "self.assertEqual(self.active_contexts[-1], 'application')\nself.stop()", "path": "pkg\\tornado-1.2.1\\tornado\\test\\stack_context_test.py", "repo_name": "dongyi/tornado-chatroom", "stars": 39, "license": "None", "language": "python", "size": 768}
{"docstring": "\"\"\"Instantly aborts the WebSocket connection by closing the socket\"\"\"\n", "func_signal": "def _abort(self):\n", "code": "self.client_terminated = True\nself.stream.close()", "path": "pkg\\tornado-1.2.1\\tornado\\websocket.py", "repo_name": "dongyi/tornado-chatroom", "stars": 39, "license": "None", "language": "python", "size": 768}
{"docstring": "# Ensure that url-encoded question marks are handled properly\n", "func_signal": "def test_question_mark(self):\n", "code": "self.assertEqual(json_decode(self.fetch('/%3F').body),\n                 dict(path='?', args={}))\nself.assertEqual(json_decode(self.fetch('/%3F?%3F=%3F').body),\n                 dict(path='?', args={'?': ['?']}))", "path": "pkg\\tornado-1.2.1\\tornado\\test\\web_test.py", "repo_name": "dongyi/tornado-chatroom", "stars": 39, "license": "None", "language": "python", "size": 768}
{"docstring": "# capture the caller's context before introducing our own\n", "func_signal": "def library_function(callback):\n", "code": "callback = wrap(callback)\nwith StackContext(functools.partial(self.context, 'library')):\n    self.io_loop.add_callback(\n      functools.partial(library_inner_callback, callback))", "path": "pkg\\tornado-1.2.1\\tornado\\test\\stack_context_test.py", "repo_name": "dongyi/tornado-chatroom", "stars": 39, "license": "None", "language": "python", "size": 768}
{"docstring": "\"\"\"Sends the given message to the client of this Web Socket.\"\"\"\n", "func_signal": "def write_message(self, message):\n", "code": "if isinstance(message, dict):\n    message = tornado.escape.json_encode(message)\nif isinstance(message, unicode):\n    message = message.encode(\"utf-8\")\nassert isinstance(message, str)\nself.stream.write(\"\\x00\" + message + \"\\xff\")", "path": "pkg\\tornado-1.2.1\\tornado\\websocket.py", "repo_name": "dongyi/tornado-chatroom", "stars": 39, "license": "None", "language": "python", "size": 768}
{"docstring": "\"\"\"Called by libcurl when it wants to change the file descriptors\nit cares about.\n\"\"\"\n", "func_signal": "def _handle_socket(self, event, fd, multi, data):\n", "code": "event_map = {\n    pycurl.POLL_NONE: ioloop.IOLoop.NONE,\n    pycurl.POLL_IN: ioloop.IOLoop.READ,\n    pycurl.POLL_OUT: ioloop.IOLoop.WRITE,\n    pycurl.POLL_INOUT: ioloop.IOLoop.READ | ioloop.IOLoop.WRITE\n}\nif event == pycurl.POLL_REMOVE:\n    self.io_loop.remove_handler(fd)\n    del self._fds[fd]\nelse:\n    ioloop_event = event_map[event]\n    if fd not in self._fds:\n        self._fds[fd] = ioloop_event\n        self.io_loop.add_handler(fd, self._handle_events,\n                                 ioloop_event)\n    else:\n        self._fds[fd] = ioloop_event\n        self.io_loop.update_handler(fd, ioloop_event)", "path": "pkg\\tornado-1.2.1\\tornado\\httpclient.py", "repo_name": "dongyi/tornado-chatroom", "stars": 39, "license": "None", "language": "python", "size": 768}
{"docstring": "\"\"\"Verifies all invariant- and required headers\n\nIf a header is missing or have an incorrect value ValueError will be\nraised\n\"\"\"\n", "func_signal": "def _handle_websocket_headers(self):\n", "code": "headers = self.request.headers\nfields = (\"Origin\", \"Host\", \"Sec-Websocket-Key1\",\n          \"Sec-Websocket-Key2\")\nif headers.get(\"Upgrade\", '').lower() != \"websocket\" or \\\n   headers.get(\"Connection\", '').lower() != \"upgrade\" or \\\n   not all(map(lambda f: self.request.headers.get(f), fields)):\n    raise ValueError(\"Missing/Invalid WebSocket headers\")", "path": "pkg\\tornado-1.2.1\\tornado\\websocket.py", "repo_name": "dongyi/tornado-chatroom", "stars": 39, "license": "None", "language": "python", "size": 768}
{"docstring": "\"\"\"Starts the mock S3 server on the given port at the given path.\"\"\"\n", "func_signal": "def start(port, root_directory=\"/tmp/s3\", bucket_depth=0):\n", "code": "application = S3Application(root_directory, bucket_depth)\nhttp_server = httpserver.HTTPServer(application)\nhttp_server.listen(port)\nioloop.IOLoop.instance().start()", "path": "pkg\\tornado-1.2.1\\tornado\\s3server.py", "repo_name": "dongyi/tornado-chatroom", "stars": 39, "license": "None", "language": "python", "size": 768}
{"docstring": "\"\"\"Processes the key headers and calculates their key value.\n\nRaises ValueError when feed invalid key.\"\"\"\n", "func_signal": "def _calculate_part(self, key):\n", "code": "number, spaces = filter(str.isdigit, key), filter(str.isspace, key)\ntry:\n    key_number = int(number) / len(spaces)\nexcept (ValueError, ZeroDivisionError):\n    raise ValueError\nreturn struct.pack(\">I\", key_number)", "path": "pkg\\tornado-1.2.1\\tornado\\websocket.py", "repo_name": "dongyi/tornado-chatroom", "stars": 39, "license": "None", "language": "python", "size": 768}
{"docstring": "# don't call super.__init__\n", "func_signal": "def __init__(self):\n", "code": "self._cookies = {}\nself.application = _O(settings=dict(cookie_secret='0123456789'))", "path": "pkg\\tornado-1.2.1\\tornado\\test\\web_test.py", "repo_name": "dongyi/tornado-chatroom", "stars": 39, "license": "None", "language": "python", "size": 768}
{"docstring": "\"\"\"Called by libcurl to schedule a timeout.\"\"\"\n", "func_signal": "def _set_timeout(self, msecs):\n", "code": "if self._timeout is not None:\n    self.io_loop.remove_timeout(self._timeout)\nself._timeout = self.io_loop.add_timeout(\n    time.time() + msecs/1000.0, self._handle_timeout)", "path": "pkg\\tornado-1.2.1\\tornado\\httpclient.py", "repo_name": "dongyi/tornado-chatroom", "stars": 39, "license": "None", "language": "python", "size": 768}
{"docstring": "\"\"\"Destroys this http client, freeing any file descriptors used.\nNot needed in normal use, but may be helpful in unittests that\ncreate and destroy http clients.  No other methods may be called\non the AsyncHTTPClient after close().\n\"\"\"\n", "func_signal": "def close(self):\n", "code": "del AsyncHTTPClient._ASYNC_CLIENTS[self.io_loop]\nself._force_timeout_callback.stop()\nfor curl in self._curls:\n    curl.close()\nself._multi.close()\nself._closed = True", "path": "pkg\\tornado-1.2.1\\tornado\\httpclient.py", "repo_name": "dongyi/tornado-chatroom", "stars": 39, "license": "None", "language": "python", "size": 768}
{"docstring": "\"\"\"Created by Fredrik Lundh (http://effbot.org/zone/re-sub.htm#unescape-html)\"\"\"\n", "func_signal": "def unescape_html(text):\n", "code": "def fixup(m):\n    text = m.group(0)\n    if text[:2] == \"&#\":\n        # character reference\n        try:\n            if text[:3] == \"&#x\":\n                return unichr(int(text[3:-1], 16))\n            else:\n                return unichr(int(text[2:-1]))\n        except ValueError:\n            pass\n    else:\n        # named entity\n        try:\n            text = unichr(htmlentitydefs.name2codepoint[text[1:-1]])\n        except KeyError:\n            pass\n    return text # leave as is\nreturn re.sub(\"&#?\\w+;\", fixup, text)", "path": "weibopy\\utils.py", "repo_name": "dongyi/tornado-chatroom", "stars": 39, "license": "None", "language": "python", "size": 768}
{"docstring": "# Closed client connection\n", "func_signal": "def on_new_messages(self, messages):\n", "code": "if self.request.connection.stream.closed():\n    return\nself.finish(dict(messages=messages))", "path": "pkg\\tornado-1.2.1\\demos\\chat\\chatdemo.py", "repo_name": "dongyi/tornado-chatroom", "stars": 39, "license": "None", "language": "python", "size": 768}
{"docstring": "\"\"\"Generates the challange response that's needed in the handshake\n\nThe challenge parameter should be the raw bytes as sent from the\nclient.\n\"\"\"\n", "func_signal": "def challenge_response(self, challenge):\n", "code": "key_1 = self.request.headers.get(\"Sec-Websocket-Key1\")\nkey_2 = self.request.headers.get(\"Sec-Websocket-Key2\")\ntry:\n    part_1 = self._calculate_part(key_1)\n    part_2 = self._calculate_part(key_2)\nexcept ValueError:\n    raise ValueError(\"Invalid Keys/Challenge\")\nreturn self._generate_challenge_response(part_1, part_2, challenge)", "path": "pkg\\tornado-1.2.1\\tornado\\websocket.py", "repo_name": "dongyi/tornado-chatroom", "stars": 39, "license": "None", "language": "python", "size": 768}
{"docstring": "# written by Michael Norton (http://docondev.blogspot.com/)\n", "func_signal": "def convert_to_utf8_str(arg):\n", "code": "if isinstance(arg, unicode):\n    arg = arg.encode('utf-8')\nelif not isinstance(arg, str):\n    arg = str(arg)\nreturn arg", "path": "weibopy\\utils.py", "repo_name": "dongyi/tornado-chatroom", "stars": 39, "license": "None", "language": "python", "size": 768}
{"docstring": "\"\"\"\nTest if the list on the stack is empty. We do not pop the\nlist off the stack and the result is pushed onto the stack.\n\n>>> s = SECD()\n>>> s.load_program([NULL], [[], 999])\n>>> s.execute_opcode()\n>>> s.dump_registers()\nS: address = 13 value: [1, [], 999]\nE: address = 3 value: []\nC: address = 7 value: 7\nD: address = 4 value: []\n\n\n>>> s = SECD()\n>>> s.load_program([NULL], [[1, 2, 3], 999])\n>>> s.execute_opcode()\n>>> s.dump_registers()\nS: address = 19 value: [0, [1, 2, 3], 999]\nE: address = 3 value: []\nC: address = 7 value: 7\nD: address = 4 value: []\n\"\"\"\n\n", "func_signal": "def opcode_NULL(self):\n", "code": "assert self.get_int(self.car(self.registers['C'])) == NULL\n\nvalue = self.memory[self.car(self.registers['S'])]\nassert value[0] == TAG_NONTERMINAL\n\nresult = self.get_new_address()\nself.set_int(result, int(value[1] == 0 and value[2] == 0))\nself.push_stack('S', result)\n\nself.registers['C'] = self.cdr(self.registers['C'])", "path": "secd.py", "repo_name": "carlohamalainen/pysecd", "stars": 40, "license": "None", "language": "python", "size": 276}
{"docstring": "\"\"\"\nTest if the number on the stack is zero. We do not pop the\nnumber off the stack, and the result is pushed onto the stack.\n\n>>> s = SECD()\n>>> s.load_program([ZEROP], [0, 999])\n>>> s.execute_opcode()\n>>> s.dump_registers()\nS: address = 13 value: [1, 0, 999]\nE: address = 3 value: []\nC: address = 7 value: 7\nD: address = 4 value: []\n\n>>> s = SECD()\n>>> s.load_program([ZEROP], [2, 999])\n>>> s.execute_opcode()\n>>> s.dump_registers()\nS: address = 13 value: [0, 2, 999]\nE: address = 3 value: []\nC: address = 7 value: 7\nD: address = 4 value: []\n\n\"\"\"\n\n", "func_signal": "def opcode_ZEROP(self):\n", "code": "assert self.get_int(self.car(self.registers['C'])) == ZEROP\n\nvalue = self.memory[self.car(self.registers['S'])]\nassert value[0] == TAG_INTEGER\n\nresult = self.get_new_address()\nself.set_int(result, int(value[1] == 0))\nself.push_stack('S', result)\n\nself.registers['C'] = self.cdr(self.registers['C'])", "path": "secd.py", "repo_name": "carlohamalainen/pysecd", "stars": 40, "license": "None", "language": "python", "size": 276}
{"docstring": "\"\"\"\nInteger subtraction; arguments are taken from the stack.\n\n>>> s = SECD()\n>>> s.load_program([SUB], [100, 42])\n>>> s.execute_opcode()\n>>> s.dump_registers()\nS: address = 13 value: [58]\nE: address = 3 value: []\nC: address = 7 value: 7\nD: address = 4 value: []\n\"\"\"\n\n", "func_signal": "def opcode_SUB(self):\n", "code": "assert self.get_int(self.car(self.registers['C'])) == SUB\n\nval1 = self.get_int(self.car(self.registers['S']))\nself.pop_stack('S')\n\nval2 = self.get_int(self.car(self.registers['S']))\nself.pop_stack('S')\n\nresult = self.get_new_address()\nself.set_int(result, val1 - val2)\nself.push_stack('S', result)\n\nself.registers['C'] = self.cdr(self.registers['C'])", "path": "secd.py", "repo_name": "carlohamalainen/pysecd", "stars": 40, "license": "None", "language": "python", "size": 276}
{"docstring": "\"\"\"\nRefer to K1991 p. 160.\n\nCons onto the front of the environment register a cell\nwith car = nil. Later the car of the new cell will be\nreset by RAP to point to the list of closures.\n\n>>> s = SECD()\n>>> s.load_program([DUM], [])\n>>> s.execute_opcode()\n>>> s.dump_registers()\nS: address = 2 value: []\nE: address = 8 value: ['NIL_PTR0']\nC: address = 7 value: 7\nD: address = 4 value: []\n\n>>> s = SECD()\n>>> s.load_program([DUM], [])\n>>> s.store_py_list(s.registers['E'], [[99, 999]])\n>>> s.execute_opcode()\n>>> s.dump_registers()\nS: address = 2 value: []\nE: address = 14 value: ['NIL_PTR0', [99, 999]]\nC: address = 7 value: 7\nD: address = 4 value: []\n\n\"\"\"\n\n", "func_signal": "def opcode_DUM(self):\n", "code": "assert self.get_int(self.car(self.registers['C'])) == DUM\n\nnew_cell = self.get_new_address()\nself.set_nonterminal(new_cell, 0, self.registers['E'])\nself.registers['E'] = new_cell\n\nself.registers['C'] = self.cdr(self.registers['C'])", "path": "secd.py", "repo_name": "carlohamalainen/pysecd", "stars": 40, "license": "None", "language": "python", "size": 276}
{"docstring": "\"\"\"\nGiven the Python list x, store it in the machine's memory\nat 'address' as a linked list. This function uses a basic\nrecursive definition so it will fail if x is too large (we\nwill hit Python's recursion limit).\n\n>>> m = SECD()\n>>> new_cell = m.get_new_address()\n\n>>> m.store_py_list(new_cell, [])\n>>> m.get_value(new_cell)\n[]\n\n>>> m.store_py_list(new_cell, [1])\n>>> m.get_value(new_cell)\n[1]\n\n>>> m.store_py_list(new_cell, [1, 2, 3])\n>>> m.get_value(new_cell)\n[1, 2, 3]\n\n>>> m.store_py_list(new_cell, [1, 2, []])\n>>> m.get_value(new_cell)\n[1, 2, []]\n\n>>> m.store_py_list(new_cell, [[1, 2], [3], [[[4]]], 5])\n>>> m.get_value(new_cell)\n[[1, 2], [3], [[[4]]], 5]\n\n>>> m.store_py_list(new_cell, [[], [[], []], [[[[ [], [], [[]] ]]]]])\n>>> m.get_value(new_cell)\n[[], [[], []], [[[[[], [], [[]]]]]]]\n\n\"\"\"\n\n", "func_signal": "def store_py_list(self, address, x):\n", "code": "if x == []:\n    self.memory[address] = (TAG_NONTERMINAL, 0, 0)\nelif type(x[0]) == int or (type(x[0]) == str and x[0] in OP_CODES):\n    car_address = self.get_new_address()\n    cdr_address = self.get_new_address()\n\n    self.set_int(car_address, x[0])\n    self.store_py_list(cdr_address, x[1:])\n\n    self.set_nonterminal(address, car_address, cdr_address)\nelif type(x[0]) == list:\n    car_address = self.get_new_address()\n    cdr_address = self.get_new_address()\n\n    self.store_py_list(car_address, x[0])\n    self.store_py_list(cdr_address, x[1:])\n\n    self.set_nonterminal(address, car_address, cdr_address)\nelse:\n    assert False, 'Unknown element type: %s' % (str(type(x[0])))", "path": "secd.py", "repo_name": "carlohamalainen/pysecd", "stars": 40, "license": "None", "language": "python", "size": 276}
{"docstring": "\"\"\"\nLoad a constant onto the stack. The constant expression\nis whatever follows LDC in C, so it may be an arbitrary\ns-expression.\n\n>>> s = SECD()\n>>> s.load_program([LDC, 3], [18, 19])\n>>> s.execute_opcode()\n>>> s.dump_registers()\nS: address = 14 value: [3, 18, 19]\nE: address = 3 value: []\nC: address = 9 value: 9\nD: address = 4 value: []\n\n>>> s = SECD()\n>>> s.load_program([LDC, [3, 4, [18]]], [1])\n>>> s.execute_opcode()\n>>> s.dump_registers()\nS: address = 20 value: [[3, 4, [18]], 1]\nE: address = 3 value: []\nC: address = 9 value: 9\nD: address = 4 value: []\n\"\"\"\n\n", "func_signal": "def opcode_LDC(self):\n", "code": "assert self.get_int(self.car(self.registers['C'])) == LDC\n\nself.push_stack('S', self.car(self.cdr(self.registers['C'])))\n\nself.registers['C'] = self.cdr(self.registers['C']) # skip LDC\nself.registers['C'] = self.cdr(self.registers['C']) # skip the constant expression", "path": "secd.py", "repo_name": "carlohamalainen/pysecd", "stars": 40, "license": "None", "language": "python", "size": 276}
{"docstring": "\"\"\"\nBuilds a closure for the code immediately after LDF. The\nfunction's parameters are to be found on the top of the\nstack. Note that LDF doesn't start the execution of the\nfunction - this happens when an appropriate AP opcode is\nexecuted.\n\n>>> s = SECD()\n>>> s.load_program([LDC, [3, 4], LDF, [LD, [1, 2], LD, [1, 1], ADD, RTN], AP, WRITEI, STOP,], [500])\n>>> s.store_py_list(s.registers['E'], [[99, 999]]) # pretend that this is the enclosing environment\n>>> s.dump_registers()\nS: address = 2 value: [500]\nE: address = 3 value: [[99, 999]]\nC: address = 5 value: 5\nD: address = 4 value: []\n\nPush [3, 4] onto the stack (these are the function's parameters):\n\n>>> s.execute_opcode()\n>>> s.dump_registers()\nS: address = 52 value: [[3, 4], 500]\nE: address = 3 value: [[99, 999]]\nC: address = 9 value: 9\nD: address = 4 value: []\n\nRun LDF, which pushes the code portion onto the stack and\nmoves C to the code after the function:\n\n>>> s.execute_opcode()\n>>> s.dump_registers()\nS: address = 53 value: [[['LD', [1, 2], 'LD', [1, 1], 'ADD', 'RTN'], [[99, 999]]], [3, 4], 500]\nE: address = 3 value: [[99, 999]]\nC: address = 17 value: 17\nD: address = 4 value: []\n\n>>> s.get_value(s.registers['C'])\n['AP', 'WRITEI', 'STOP']\n\nExecute AP, which saves a copy of the program counter, environment, and stack:\n\n>>> s.execute_opcode()\n>>> s.dump_registers()\nS: address = 60 value: []\nE: address = 61 value: [[3, 4], [99, 999]]\nC: address = 16 value: 16\nD: address = 59 value: [['WRITEI', 'STOP'], [[99, 999]], [500]]\n\nNow we can execute the function itself:\n\n>>> s.get_value(s.registers['C'])\n['LD', [1, 2], 'LD', [1, 1], 'ADD', 'RTN']\n\n>>> s.execute_opcode() # LD\n>>> s.execute_opcode() # LD\n>>> s.execute_opcode() # ADD\n>>> s.execute_opcode() # RTN\n\n>>> s.execute_opcode() # WRITEI\n7\n\nNow an example of a nested call. Evaluates (9*5) + (3+4) with the\nmultiplication happening during the LDF for the addition.\n\n>>> mul_5_9 = [LDC, [5, 9], LDF, [LD, [1, 2], LD, [1, 1], MUL, RTN], AP]\n>>> add_3_4 = [LDC, [3, 4], LDF, [LD, [1, 2], LD, [1, 1], ADD] + mul_5_9 + [ADD, RTN], AP, WRITEI, STOP]\n>>> s = SECD()\n>>> s.load_program(add_3_4, [500])\n>>> s.store_py_list(s.registers['E'], [[99, 999]]) # pretend that this is the enclosing environment\n\nRegisters before the LDFs are executed:\n>>> s.dump_registers()\nS: address = 2 value: [500]\nE: address = 3 value: [[99, 999]]\nC: address = 5 value: 5\nD: address = 4 value: []\n\n>>> for _ in range(17): s.execute_opcode()\n52\n<BLANKLINE>\nMACHINE HALTED!\n<BLANKLINE>\n\nNote that the original registers are preserved after the calls:\n\n>>> s.dump_registers()\nS: address = 2 value: [500]\nE: address = 3 value: [[99, 999]]\nC: address = 77 value: 77\nD: address = 4 value: []\n\n\"\"\"\n\n", "func_signal": "def opcode_LDF(self):\n", "code": "assert self.get_int(self.car(self.registers['C'])) == LDF\n\n# Make a note of the start of the original E list:\nE_head = self.registers['E']\n\n# The code after the LDF (the function itself):\ncode = self.car(self.cdr(self.registers['C']))\n\n# The closure consists of code and E_head:\nnew_cell_0 = self.get_new_address()\nnew_cell_1 = self.get_new_address()\nnew_cell_2 = self.get_new_address()\nnew_cell_3 = self.get_new_address()\n\n# Push the closure onto the stack:\nself.set_nonterminal(new_cell_0, new_cell_1, self.registers['S'])\nself.set_nonterminal(new_cell_1, code,       new_cell_2)\nself.set_nonterminal(new_cell_2, E_head,     new_cell_3)\nself.set_nonterminal(new_cell_3, 0, 0)\nself.registers['S'] = new_cell_0\n\nself.registers['C'] = self.cdr(self.registers['C']) # skip LDF\nself.registers['C'] = self.cdr(self.registers['C']) # skip the code", "path": "secd.py", "repo_name": "carlohamalainen/pysecd", "stars": 40, "license": "None", "language": "python", "size": 276}
{"docstring": "\"\"\"\nInitialise the C register with 'code' and the stack S with 'stack'.\n\n>>> s = SECD()\n>>> s.load_program([ADD], [100, 42])\n>>> s.get_value(s.registers['C'])\n['ADD']\n>>> s.get_value(s.registers['S'])\n[100, 42]\n\"\"\"\n\n", "func_signal": "def load_program(self, code, stack=[]):\n", "code": "program = self.get_new_address()\nself.store_py_list(program, code)\nself.registers['C'] = program\n\nself.store_py_list(self.registers['S'], stack)\nself.running = True", "path": "secd.py", "repo_name": "carlohamalainen/pysecd", "stars": 40, "license": "None", "language": "python", "size": 276}
{"docstring": "\"\"\"\nAddress register value of a nonterminal cell.\n\n>>> m = SECD()\n\n>>> new_cell = m.get_new_address()\n>>> m.set_int(new_cell, 123)\n>>> m.push_stack('S', new_cell)\n\n>>> m.car(m.registers['S'])\n5\n\n>>> m.get_value(5)\n123\n\"\"\"\n\n", "func_signal": "def car(self, address):\n", "code": "assert self.memory[address][0] == TAG_NONTERMINAL\nreturn self.memory[address][1]", "path": "secd.py", "repo_name": "carlohamalainen/pysecd", "stars": 40, "license": "None", "language": "python", "size": 276}
{"docstring": "\"\"\"\nFind the j-th element of the i-th sublist of vlist. Typically\nij will be a parameter to a function and vlist will be the\nenvironment register E, containing a list of sublists.\n\nK1991 p. 149. There is a typo in the first line: replace\neach occurence of 'x' with 'ij'.\n\nij    = dotted pair, e.g. 1.3 = [1, 3]\nvlist = memory location of environment list\n\n>>> s = SECD()\n>>> vlist = s.get_new_address()\n>>> s.store_py_list(vlist, [[8,], [4, [2, 2]], [1, 2, 3],])\n\nfor (i, j) in [(3, 3)]: # [(1, 1), (2, 1), (2, 2), (3, 1), (3, 2), (3, 3),]:\n\n>>> ij = s.get_new_address()\n>>> s.store_py_list(ij, [1, 1])\n>>> s.get_value(s.locate(ij, vlist))\n8\n\n>>> ij = s.get_new_address()\n>>> s.store_py_list(ij, [2, 1])\n>>> s.get_value(s.locate(ij, vlist))\n4\n\n>>> ij = s.get_new_address()\n>>> s.store_py_list(ij, [2, 2])\n>>> s.get_value(s.locate(ij, vlist))\n[2, 2]\n\n>>> ij = s.get_new_address()\n>>> s.store_py_list(ij, [3, 1])\n>>> s.get_value(s.locate(ij, vlist))\n1\n\n>>> ij = s.get_new_address()\n>>> s.store_py_list(ij, [3, 2])\n>>> s.get_value(s.locate(ij, vlist))\n2\n\n>>> ij = s.get_new_address()\n>>> s.store_py_list(ij, [3, 3])\n>>> s.get_value(s.locate(ij, vlist))\n3\n\n\"\"\"\n\n", "func_signal": "def locate(self, ij, vlist):\n", "code": "def loc(s, y, z):\n    assert type(y) == int\n    assert y >= 1\n\n    if y == 1:\n        return s.car(z)\n    else:\n        return loc(s, y - 1, s.cdr(z))\n\nreturn loc(self, self.get_int(self.car(self.cdr(ij))), loc(self, self.get_int(self.car(ij)), vlist))", "path": "secd.py", "repo_name": "carlohamalainen/pysecd", "stars": 40, "license": "None", "language": "python", "size": 276}
{"docstring": "\"\"\"\nLoad the value of a variable onto the stack. The cdr of C is a\npair [i, j] which specifies that we want the j-th element of\nthe i-th sublist of the environment E. For further examples\nsee opcode_LDF().\n\n>>> s = SECD()\n>>> s.store_py_list(s.registers['E'], [[8,], [4, [2, 2]], [1, 2, 3],])\n>>> s.load_program([LD, [1, 1], LD, [2, 1], LD, [2, 2], LD, [3, 1], LD, [3, 2], LD, [3, 3],], [])\n>>> s.dump_registers()\nS: address = 2 value: []\nE: address = 3 value: [[8], [4, [2, 2]], [1, 2, 3]]\nC: address = 27 value: 27\nD: address = 4 value: []\n\n>>> s.execute_opcode()\n>>> s.get_value(s.registers['S'])\n[8]\n\n>>> s.execute_opcode()\n>>> s.get_value(s.registers['S'])\n[4, 8]\n\n>>> s.execute_opcode()\n>>> s.get_value(s.registers['S'])\n[[2, 2], 4, 8]\n\n>>> s.execute_opcode()\n>>> s.get_value(s.registers['S'])\n[1, [2, 2], 4, 8]\n\n>>> s.execute_opcode()\n>>> s.get_value(s.registers['S'])\n[2, 1, [2, 2], 4, 8]\n\n>>> s.execute_opcode()\n>>> s.get_value(s.registers['S'])\n[3, 2, 1, [2, 2], 4, 8]\n\n\"\"\"\n\n", "func_signal": "def opcode_LD(self):\n", "code": "assert self.get_int(self.car(self.registers['C'])) == LD\n\nij = self.car(self.cdr(self.registers['C']))\n\nself.push_stack('S', self.locate(ij, self.registers['E']))\n\nself.registers['C'] = self.cdr(self.registers['C']) # LD\nself.registers['C'] = self.cdr(self.registers['C']) # ij", "path": "secd.py", "repo_name": "carlohamalainen/pysecd", "stars": 40, "license": "None", "language": "python", "size": 276}
{"docstring": "\"\"\"\nGet the integer value of a memory cell.\n\n>>> m = SECD()\n>>> new_cell = m.get_new_address()\n>>> m.set_int(new_cell, 123)\n>>> m.get_int(new_cell)\n123\n\"\"\"\n\n", "func_signal": "def get_int(self, address):\n", "code": "assert self.memory[address][0] == TAG_INTEGER\nreturn self.memory[address][1]", "path": "secd.py", "repo_name": "carlohamalainen/pysecd", "stars": 40, "license": "None", "language": "python", "size": 276}
{"docstring": "\"\"\"\nPush an empty list (nil) onto the stack. See also CONS.\n\n>>> s = SECD()\n>>> s.load_program([NIL], [18, 19])\n>>> s.get_value(s.registers['S'])\n[18, 19]\n>>> s.execute_opcode()\n>>> s.dump_registers()\nS: address = 13 value: [[], 18, 19]\nE: address = 3 value: []\nC: address = 7 value: 7\nD: address = 4 value: []\n\"\"\"\n\n", "func_signal": "def opcode_NIL(self):\n", "code": "assert self.get_int(self.car(self.registers['C'])) == NIL\n\nnew_cell = self.get_new_address()\nself.set_nonterminal(new_cell, 0, 0)\nself.push_stack('S', new_cell)\n\nself.registers['C'] = self.cdr(self.registers['C'])", "path": "secd.py", "repo_name": "carlohamalainen/pysecd", "stars": 40, "license": "None", "language": "python", "size": 276}
{"docstring": "\"\"\"\nTake the car of the list on the stack.\n\n>>> s = SECD()\n>>> s.load_program([CAR], [[1, 2, 3]])\n>>> s.dump_registers()\nS: address = 2 value: [[1, 2, 3]]\nE: address = 3 value: []\nC: address = 5 value: 5\nD: address = 4 value: []\n\n>>> s.execute_opcode()\n>>> s.dump_registers()\nS: address = 16 value: [1]\nE: address = 3 value: []\nC: address = 7 value: 7\nD: address = 4 value: []\n\n>>> s = SECD()\n>>> s.load_program([CDR, CAR], [[1, 2, 3]])\n>>> s.execute_opcode()\n>>> s.execute_opcode()\n>>> s.dump_registers()\nS: address = 18 value: [2]\nE: address = 3 value: []\nC: address = 9 value: 9\nD: address = 4 value: []\n\n>>> s = SECD()\n>>> s.load_program([CDR, CDR, CAR], [[1, 2, 3]])\n>>> s.execute_opcode()\n>>> s.execute_opcode()\n>>> s.execute_opcode()\n>>> s.dump_registers()\nS: address = 20 value: [3]\nE: address = 3 value: []\nC: address = 11 value: 11\nD: address = 4 value: []\n\n\"\"\"\n\n", "func_signal": "def opcode_CAR(self):\n", "code": "assert self.get_int(self.car(self.registers['C'])) == CAR\n\ncar_value = self.car(self.car(self.registers['S']))\nself.pop_stack('S')\nself.push_stack('S', car_value)\n\nself.registers['C'] = self.cdr(self.registers['C'])", "path": "secd.py", "repo_name": "carlohamalainen/pysecd", "stars": 40, "license": "None", "language": "python", "size": 276}
{"docstring": "\"\"\"\nProduce a dotty (graphviz) graph representing the linked structure at\n'address'. See draw_graphs() for some examples and associated PNG plots.\n\n>>> m = SECD()\n>>> new_cell = m.get_new_address()\n\n>>> m.store_py_list(new_cell, [])\n>>> m.graph_at_address(new_cell).to_string().replace('\\\\n', '')\n'digraph graphname {rankdir=LR;node5 [shape=record, label=\"<f0> 5|<f1> nil|<f2> nil\"];}'\n\n>>> m.store_py_list(new_cell, [1])\n>>> m.graph_at_address(new_cell).to_string().replace('\\\\n', '')\n'digraph graphname {rankdir=LR;node5 [shape=record, label=\"<f0> 5|<f1> car 6|<f2> cdr 7\"];node5:f1 -> node6:f0;node5:f2 -> node7:f0;node6 [shape=record, label=\"<f0> 6|<f1> 1\"];node7 [shape=record, label=\"<f0> 7|<f1> nil|<f2> nil\"];}'\n\n\n>>> m.store_py_list(new_cell, [1, 2, 3])\n>>> m.graph_at_address(new_cell).to_string().replace('\\\\n', '')\n'digraph graphname {rankdir=LR;node5 [shape=record, label=\"<f0> 5|<f1> car 8|<f2> cdr 9\"];node5:f1 -> node8:f0;node5:f2 -> node9:f0;node8 [shape=record, label=\"<f0> 8|<f1> 1\"];node9 [shape=record, label=\"<f0> 9|<f1> car 10|<f2> cdr 11\"];node9:f1 -> node10:f0;node9:f2 -> node11:f0;node10 [shape=record, label=\"<f0> 10|<f1> 2\"];node11 [shape=record, label=\"<f0> 11|<f1> car 12|<f2> cdr 13\"];node11:f1 -> node12:f0;node11:f2 -> node13:f0;node12 [shape=record, label=\"<f0> 12|<f1> 3\"];node13 [shape=record, label=\"<f0> 13|<f1> nil|<f2> nil\"];}'\n\n\n>>> m.store_py_list(new_cell, [1, 2, []])\n>>> m.graph_at_address(new_cell).to_string().replace('\\\\n', '')\n'digraph graphname {rankdir=LR;node5 [shape=record, label=\"<f0> 5|<f1> car 14|<f2> cdr 15\"];node5:f1 -> node14:f0;node5:f2 -> node15:f0;node14 [shape=record, label=\"<f0> 14|<f1> 1\"];node15 [shape=record, label=\"<f0> 15|<f1> car 16|<f2> cdr 17\"];node15:f1 -> node16:f0;node15:f2 -> node17:f0;node16 [shape=record, label=\"<f0> 16|<f1> 2\"];node17 [shape=record, label=\"<f0> 17|<f1> car 18|<f2> cdr 19\"];node17:f1 -> node18:f0;node17:f2 -> node19:f0;node18 [shape=record, label=\"<f0> 18|<f1> nil|<f2> nil\"];node19 [shape=record, label=\"<f0> 19|<f1> nil|<f2> nil\"];}'\n\n\n>>> m.store_py_list(new_cell, [[1, 2], [3], [[[4]]], 5])\n>>> m.graph_at_address(new_cell).to_string().replace('\\\\n', '')\n'digraph graphname {rankdir=LR;node5 [shape=record, label=\"<f0> 5|<f1> car 20|<f2> cdr 21\"];node5:f1 -> node20:f0;node5:f2 -> node21:f0;node20 [shape=record, label=\"<f0> 20|<f1> car 22|<f2> cdr 23\"];node20:f1 -> node22:f0;node20:f2 -> node23:f0;node22 [shape=record, label=\"<f0> 22|<f1> 1\"];node23 [shape=record, label=\"<f0> 23|<f1> car 24|<f2> cdr 25\"];node23:f1 -> node24:f0;node23:f2 -> node25:f0;node24 [shape=record, label=\"<f0> 24|<f1> 2\"];node25 [shape=record, label=\"<f0> 25|<f1> nil|<f2> nil\"];node21 [shape=record, label=\"<f0> 21|<f1> car 26|<f2> cdr 27\"];node21:f1 -> node26:f0;node21:f2 -> node27:f0;node26 [shape=record, label=\"<f0> 26|<f1> car 28|<f2> cdr 29\"];node26:f1 -> node28:f0;node26:f2 -> node29:f0;node28 [shape=record, label=\"<f0> 28|<f1> 3\"];node29 [shape=record, label=\"<f0> 29|<f1> nil|<f2> nil\"];node27 [shape=record, label=\"<f0> 27|<f1> car 30|<f2> cdr 31\"];node27:f1 -> node30:f0;node27:f2 -> node31:f0;node30 [shape=record, label=\"<f0> 30|<f1> car 32|<f2> cdr 33\"];node30:f1 -> node32:f0;node30:f2 -> node33:f0;node32 [shape=record, label=\"<f0> 32|<f1> car 34|<f2> cdr 35\"];node32:f1 -> node34:f0;node32:f2 -> node35:f0;node34 [shape=record, label=\"<f0> 34|<f1> car 36|<f2> cdr 37\"];node34:f1 -> node36:f0;node34:f2 -> node37:f0;node36 [shape=record, label=\"<f0> 36|<f1> 4\"];node37 [shape=record, label=\"<f0> 37|<f1> nil|<f2> nil\"];node35 [shape=record, label=\"<f0> 35|<f1> nil|<f2> nil\"];node33 [shape=record, label=\"<f0> 33|<f1> nil|<f2> nil\"];node31 [shape=record, label=\"<f0> 31|<f1> car 38|<f2> cdr 39\"];node31:f1 -> node38:f0;node31:f2 -> node39:f0;node38 [shape=record, label=\"<f0> 38|<f1> 5\"];node39 [shape=record, label=\"<f0> 39|<f1> nil|<f2> nil\"];}'\n\n\n\"\"\"\n\n", "func_signal": "def graph_at_address(self, address):\n", "code": "self.seen_by_graph_at_address = {} # avoid infinite loops\ngraph = pydot.Dot('graphname', graph_type='digraph', rankdir='LR')\nself._graph_at_address(address, graph)\nreturn graph", "path": "secd.py", "repo_name": "carlohamalainen/pysecd", "stars": 40, "license": "None", "language": "python", "size": 276}
{"docstring": "\"\"\"\nInteger addition; arguments are taken from the stack.\n\n>>> s = SECD()\n>>> s.load_program([ADD], [100, 42])\n>>> s.execute_opcode()\n>>> s.dump_registers()\nS: address = 13 value: [142]\nE: address = 3 value: []\nC: address = 7 value: 7\nD: address = 4 value: []\n\"\"\"\n\n", "func_signal": "def opcode_ADD(self):\n", "code": "assert self.get_int(self.car(self.registers['C'])) == ADD\n\nval1 = self.get_int(self.car(self.registers['S']))\nself.pop_stack('S')\n\nval2 = self.get_int(self.car(self.registers['S']))\nself.pop_stack('S')\n\nresult = self.get_new_address()\nself.set_int(result, val1 + val2)\nself.push_stack('S', result)\n\nself.registers['C'] = self.cdr(self.registers['C'])", "path": "secd.py", "repo_name": "carlohamalainen/pysecd", "stars": 40, "license": "None", "language": "python", "size": 276}
{"docstring": "# Memory of the machine. A 'None' indicates an unused cell. Note that\n# 0 is never used because that corresponds to nil.\n", "func_signal": "def __init__(self):\n", "code": "self.memory = [None] + [None]*MAX_ADDRESS\nself.max_used_address = 1\n\n# By default WRITEI and WRITEC write to stdout.\nself.output_stream = sys.stdout\nself.input_stream  = sys.stdin\n\nself.debug = False\n\n# Registers:\nself.registers = {}\n\n# The main stack:\nself.registers['S'] = self.get_new_address()\nself.set_nonterminal(self.registers['S'], 0, 0)\n\n# The program counter; points to a memory location:\nself.registers['C'] = -1 # initialised later\n\n# The environment stack:\nself.registers['E'] = self.get_new_address()\nself.set_nonterminal(self.registers['E'], 0, 0)\n\n# The dump stack:\nself.registers['D'] = self.get_new_address()\nself.set_nonterminal(self.registers['D'], 0, 0)\n\nassert self.max_used_address < MAX_ADDRESS", "path": "secd.py", "repo_name": "carlohamalainen/pysecd", "stars": 40, "license": "None", "language": "python", "size": 276}
{"docstring": "\"\"\"\nReturn the address of an unused memory cell.\n\n>>> m = SECD()\n>>> m.get_new_address()\n5\n>>> m.get_new_address()\n6\n\"\"\"\n\n", "func_signal": "def get_new_address(self):\n", "code": "self.max_used_address += 1\nassert self.max_used_address < MAX_ADDRESS, 'Error, out of memory.'\nreturn self.max_used_address", "path": "secd.py", "repo_name": "carlohamalainen/pysecd", "stars": 40, "license": "None", "language": "python", "size": 276}
{"docstring": "\"\"\"\nRead an integer from the console.\n\nThis doctest relies on stdin being '42'. Note the trailing\nwhitespace after the '?' as well (it is part of the prompt).\n\n>>> s = SECD()\n>>> s.load_program([READI, WRITEI], [97])\n>>> s.execute_opcode()\n? \n>>> s.execute_opcode()\n42\n\"\"\"\n\n", "func_signal": "def opcode_READI(self):\n", "code": "i = int(raw_input('? '))\n\nnew_cell = self.get_new_address()\nself.set_int(new_cell, i)\nself.push_stack('S', new_cell)\n\nself.registers['C'] = self.cdr(self.registers['C'])", "path": "secd.py", "repo_name": "carlohamalainen/pysecd", "stars": 40, "license": "None", "language": "python", "size": 276}
{"docstring": "\"\"\"\nTest if the number on the stack is less than zero. We do not\npop the number off the stack, and the result is pushed onto\nthe stack.\n\n>>> s = SECD()\n>>> s.load_program([LT0P], [-3, 999])\n>>> s.execute_opcode()\n>>> s.dump_registers()\nS: address = 13 value: [1, -3, 999]\nE: address = 3 value: []\nC: address = 7 value: 7\nD: address = 4 value: []\n\n>>> s = SECD()\n>>> s.load_program([LT0P], [2, 999])\n>>> s.execute_opcode()\n>>> s.dump_registers()\nS: address = 13 value: [0, 2, 999]\nE: address = 3 value: []\nC: address = 7 value: 7\nD: address = 4 value: []\n\n\"\"\"\n\n", "func_signal": "def opcode_LT0P(self):\n", "code": "assert self.get_int(self.car(self.registers['C'])) == LT0P\n\nvalue = self.memory[self.car(self.registers['S'])]\nassert value[0] == TAG_INTEGER\n\nresult = self.get_new_address()\nself.set_int(result, int(value[1] < 0))\nself.push_stack('S', result)\n\nself.registers['C'] = self.cdr(self.registers['C'])", "path": "secd.py", "repo_name": "carlohamalainen/pysecd", "stars": 40, "license": "None", "language": "python", "size": 276}
{"docstring": "'''\nRecreate the heightfield engine with new initial parameters, this is\nrequired for heightfield engines such as Tessendorf as lookup tables\nare generated upon creation based on input paramters\n'''\n", "func_signal": "def resetHeightfield(self):\n", "code": "del self.heightfield\nself.heightfield = Tessendorf(  self.tileSize,\n                                self.waveHeight, \n                                self.wind,\n                                self.length,\n                                self.period)\nself.surface.setHeightfield( self.heightfield)", "path": "source\\water.py", "repo_name": "pabennett/WaterCaustics", "stars": 42, "license": "None", "language": "python", "size": 5768}
{"docstring": "\"\"\"\nUpdate the camera's position and orientation through a physics system\nupdate. This function should be called in the main render loop with the\ntime delta passed as an argument.\ndt      :   Time delta since last call\n\"\"\"\n", "func_signal": "def update(self, dt):\n", "code": "dt = 0.03 if dt < 0.03 else dt\ndt = 1.0 if dt > 1.0 else dt\n\nself.angularVelocity -= self.angularVelocity * (dt * self.damping)\nself.moveVelocity -= self.moveVelocity * (dt * self.damping)\n\nself.orient(*(self.angularVelocity * dt).values())\nself.position += self.moveVelocity * dt\n\nself.updateViewMatrix()", "path": "source\\camera.py", "repo_name": "pabennett/WaterCaustics", "stars": 42, "license": "None", "language": "python", "size": 5768}
{"docstring": "''' Utility Function for generating numpy arrays '''\n", "func_signal": "def np2DArray(initialiser, rows, columns, dtype=np.float32):\n", "code": "rows = int(rows)\ncolumns = int(columns)\nreturn np.array([[initialiser for i in range(columns)] for j in range(rows)])", "path": "source\\utilities.py", "repo_name": "pabennett/WaterCaustics", "stars": 42, "license": "None", "language": "python", "size": 5768}
{"docstring": "\"\"\" \nPerform right hand multiplication of this quaternion with the input.\nThe result is:\nresult = this * other\n\"\"\"\n", "func_signal": "def __mul__(self, other):\n", "code": "xn = (self.w * other.x) + (self.x * other.w) + (self.y * other.z) - (self.z * other.y)\nyn = (self.w * other.y) + (self.y * other.w) + (self.z * other.x) - (self.x * other.z)\nzn = (self.w * other.z) + (self.z * other.w) + (self.x * other.y) - (self.y * other.x)\nwn = (self.w * other.w) - (self.x * other.x) - (self.y * other.y) - (self.z * other.z)\nreturn Quaternion(wn, xn, yn,zn)", "path": "source\\quaternion.py", "repo_name": "pabennett/WaterCaustics", "stars": 42, "license": "None", "language": "python", "size": 5768}
{"docstring": "\"\"\"\nApply the given yaw, pitch and roll to the current camera orientation.\nThis takes effect immediately and bypasses the physics system.\nyaw     :   The angle in degrees by which the camera should rotate\n            horizontally\npitch   :   The angle in degrees by which the camera should rotate\n            vertically\nroll    :   The angle in degrees by which the camera should rotate about\n            its view axis  \nadditive:   If true, apply the yaw pitch and roll to the current \n            orientation. Otherwise create a new orientation from these\n            values.\n\"\"\"\n", "func_signal": "def orient(self, yaw, pitch, roll, additive=True):\n", "code": "rotation = Quaternion()\n\nif not additive:\n    self.orientation = Quaternion()\n\n\"\"\" \n    Apply the y-axis delta to the quaternion.\n    Pitch causes the camera to rotate around the CAMERA's X-axis.\n    We need to right multiply the quaternions because the pitch\n    rotation is about the CAMERA X-Axis \n\"\"\"\nif(pitch != 0):\n    rotation.setRotationDeg(WORLD_XAXIS, pitch)\n    self.orientation = self.orientation * rotation\n\n\"\"\"\n    Apply the x-axis delta to the quaternion.\n    Yaw causes the camera to rotate around the WORLD's Y-axis.\n    We need to left multiply the quaternions because the yaw\n    rotation is about the WORLD Y-Axis.\n\"\"\"\nif(yaw != 0):\n    rotation.setRotationDeg(WORLD_YAXIS, yaw)\n    self.orientation = rotation * self.orientation\n\n\"\"\"\n    Apply the z-axis delta to the quaternion.\n    Roll causes the camera to rotate around the camera's Z-axis\n    We need to right multiply the quaternions because the roll\n    rotation is about the CAMERA Z-Axis\n\"\"\"\nif(roll != 0):\n    if(self.flightMode):\n        #rotation.setRotationDeg(self.flightForward, roll)\n        rotation.setRotationDeg(WORLD_ZAXIS, roll)\n    else:\n        #rotation.setRotationDeg(self.forward, roll)\n        rotation.setRotationDeg(WORLD_ZAXIS, roll)\n    self.orientation = self.orientation * rotation\n    \nself.updateViewMatrix()", "path": "source\\camera.py", "repo_name": "pabennett/WaterCaustics", "stars": 42, "license": "None", "language": "python", "size": 5768}
{"docstring": "'''\nGenerate vertices for GL_POINT drawing\n\nThe vertices are of the format:\n[v0x, v0y, v0z, t0x, t0y]\n[v1x, v1y, v1z, t1x, t1y]\n          ...\n[v7x, v7y, v7z, t7x, t7y]\n\n'''\n", "func_signal": "def Pointfield2D(dimension=64, scale=1.0):\n", "code": "N = dimension\n\nvertexSize = ctypes.sizeof(GLfloat) * 8                  \nvertices = (GLfloat * (N * N * 8))(*range(N * N * 8)) \n\n\n#indices = np2DArray(0, N, N, GLshort)\n           \n# Populate the initial positions\nfor i in range(N):\n    for j in range(N):\n        idx = (i * N + j) * 8  \n        # # Index\n        # indices[i][j] = i * N + j\n        # Position X\n        vertices[idx] = ((j-N/2.0) * scale) / (N / 2.)\n        # Position Y                        \n        vertices[idx + 1] = ((i-N/2.0) * scale) / (N / 2.)\n        # Position Z\n        vertices[idx + 2] = -1.0\n        # Normal X\n        vertices[idx] = 0.0\n        # Normal Y                        \n        vertices[idx + 1] = 1.0\n        # Normal Z\n        vertices[idx + 2] = 0.0\n        # # Texture X\n        vertices[idx + 3] = i/float(N)\n        # # Texture Y                        \n        vertices[idx + 4] = j/float(N)\n        \nindices = (GLshort * (N * N))(*range(N * N))   \n\nreturn vertices, indices, vertexSize", "path": "source\\utilities.py", "repo_name": "pabennett/WaterCaustics", "stars": 42, "license": "None", "language": "python", "size": 5768}
{"docstring": "\"\"\"\nPosition the camera at the given world x, y, z coords\n\"\"\"\n", "func_signal": "def setpos(self, x, y, z):\n", "code": "self.position = Vector3(x,y,z)\nself.positionUpdateViewMatrix()", "path": "source\\camera.py", "repo_name": "pabennett/WaterCaustics", "stars": 42, "license": "None", "language": "python", "size": 5768}
{"docstring": "\"\"\"\nGet the inverse projection matrix for converting from clip space back to\ncamera space in the vertex shader.\n\"\"\"\n", "func_signal": "def getInverseProjection(self):\n", "code": "return self.projection.inverse_perspective( self.width, \n                                            self.height,\n                                            self.FOV, \n                                            self.fzNear,\n                                            self.fzFar).cvalues()", "path": "source\\camera.py", "repo_name": "pabennett/WaterCaustics", "stars": 42, "license": "None", "language": "python", "size": 5768}
{"docstring": "\"\"\"\nGenerate a view matrix from the camera orientation and position and \nupdate the camera axis information.\n    \nOpenGL Matrix Format (Column Major):\n\n+-----+-----+-----+-----+\n| m0  | m4  | m8  | m12 | \n+-----+-----+-----+-----+\n| m1  | m5  | m9  | m13 |\n+-----+-----+-----+-----+\n| m2  | m6  | m10 | m14 |\n+-----+-----+-----+-----+\n| m3  | m7  | m11 | m15 |\n+-----+-----+-----+-----+\n\nView Matrix Format (OpenGL column major notation):\n\n+-----+-----+-----+-----+    x = x-axis scale and rotation vector3\n| x0  | y0  | z0  | 0   |    y = y-axis scale and rotation vector3\n+-----+-----+-----+-----+    z = z-axis scale and rotation vector3\n| x1  | y1  | z1  | 0   |    t = translation vector3\n+-----+-----+-----+-----+\n| x2  | y2  | z2  | 0   |     \n+-----+-----+-----+-----+           \n| t0  | t1  | t2  | 1   |            \n+-----+-----+-----+-----+            \n                                 \nt = Vector3(-dot(XAXIS,eye),-dot(YAXIS,eye),-dot(ZAXIS,eye))\n\n\"\"\"\n\n# Reconstruct the view matrix.\n", "func_signal": "def updateViewMatrix(self):\n", "code": "self.orientation.normalise()\nself.view = self.orientation.matrix()\n        \n# Load the axis vectors\nself.xAxis = Vector3(self.view[0], self.view[4], self.view[8])\nself.yAxis = Vector3(self.view[1], self.view[5], self.view[9])\nself.zAxis = Vector3(self.view[2], self.view[6], self.view[10])\n\n# Apply translation component.\nself.view[12] = -(self.xAxis.dot(self.position))\nself.view[13] = -(self.yAxis.dot(self.position))\nself.view[14] = -(self.zAxis.dot(self.position))\n\n        \n# Determine the 'forwards' direction (where we are looking).\nself.flightForward = -self.zAxis\nself.forward = WORLD_YAXIS.cross(self.xAxis)\nself.forward = self.forward.normalise()\n\n# Reconstruct the MVP matrix.\n# Projection is row major, view is row major\n# OpenGL uses column major operator on left            Ie: P*V*M*V1 = V2\n# This is the same as row major operator on the right. Ie: V1*M*V*P = V2\nself.MVP = self.view * self.projection", "path": "source\\camera.py", "repo_name": "pabennett/WaterCaustics", "stars": 42, "license": "None", "language": "python", "size": 5768}
{"docstring": "\"\"\" \nReturn a 16 element array representing a matrix formed from this \nquaternion.\n\"\"\"\n\n", "func_signal": "def matrix(self):\n", "code": "qy2 = self.y * self.y\nqx2 = self.x * self.x\nqz2 = self.z * self.z\n\nqxqy = self.x * self.y\nqxqz = self.x * self.z\n\nqyqx = self.y * self.x\nqyqz = self.y * self.z\n\nqzqx = self.z * self.x\nqzqy = self.z * self.y\n\nqwqx = self.w * self.x\nqwqz = self.w * self.z\nqwqy = self.w * self.y\n\nreturn Matrix16(    1.0 - (2.0 * qy2) - (2.0 * qz2),    \\\n                    2.0 * (qxqy - qwqz),                \\\n                    2.0 * (qxqz + qwqy),                \\\n                    0.0,                                \\\n                    2.0 * (qxqy + qwqz),                \\\n                    1.0 - (2.0 * qx2) - (2.0 * qz2),    \\\n                    2.0 * (qyqz - qwqx),                \\\n                    0.0,                                \\\n                    2.0 * (qxqz - qwqy),                \\\n                    2.0 * (qyqz + qwqx),                \\\n                    1.0 - (2.0 * qx2) - (2.0 * qy2),    \\\n                    0.0,                                \\\n                    0.0,                                \\\n                    0.0,                                \\\n                    0.0,                                \\\n                    1.0,                                \\\n                )", "path": "source\\quaternion.py", "repo_name": "pabennett/WaterCaustics", "stars": 42, "license": "None", "language": "python", "size": 5768}
{"docstring": "'''\nIf deltaTime is not zero, perform an ocean surface update for time T.\nThis update will run heightmap, diplacement and normal generation\nroutines and then passes the updated values into the vertex array.\n'''\n", "func_signal": "def update(self, dt):\n", "code": "if dt > 0.0 and self.heightfield:\n    self.time += dt\n    self.heightfield.update(self.time, self.verts, self.v0)\n    # Update the vertex VBO\n    glBindBuffer(GL_ARRAY_BUFFER, self.vertVBO)\n    \n    glBufferData(GL_ARRAY_BUFFER, \n                 self.verts.size*4, \n                 np.ctypeslib.as_ctypes(self.verts),\n                 GL_STATIC_DRAW)", "path": "source\\surface.py", "repo_name": "pabennett/WaterCaustics", "stars": 42, "license": "None", "language": "python", "size": 5768}
{"docstring": "\"\"\" \nCreate an instance of the camera class with the following parameters:\nwidth   : The width of the viewport\nheight  : The height of the viewport\nvFOV    : The desired vertical field of view in degrees\nfzNear  : The near clipping plane of the view frustrum in world units \n          (Minimum view distance) \nfzFar   : The far clipping plane of the view frustrum in world units \n          (Maximum view distance)\n\"\"\"\n\n# Parameters\n", "func_signal": "def __init__(self, width, height, vFOV, fzNear, fzFar):\n", "code": "self.width = width if width > 0 else 1\nself.height = height if height > 0 else 1\nself.aspect = self.width / float(self.height)\nself.FOV = vFOV\nself.fzNear = fzNear\nself.fzFar = fzFar\nself.flightMode = True\nself.damping = 2.8\n\n# Vectors\nself.position = Vector3(0.0, 0.0, 0.0)\nself.xAxis = Vector3(1.0, 0.0, 0.0)\nself.yAxis = Vector3(0.0, 1.0, 0.0)\nself.zAxis = Vector3(0.0, 0.0, 1.0)\nself.flightForward = Vector3(0.0, 0.0, 0.0)\nself.forward = Vector3(0.0, 0.0, 0.0)\nself.moveVelocity = Vector3(0.0, 0.0, 0.0)\nself.angularVelocity = Vector3(0.0, 0.0, 0.0)\n\n# Quaternions\nself.orientation = Quaternion()\n\n# Matrices\nself.projection = Matrix16.perspective(self.FOV, \n                                     self.aspect, \n                                     self.fzNear, \n                                     self.fzFar)\nself.view = Matrix16()\nself.MVP = Matrix16()", "path": "source\\camera.py", "repo_name": "pabennett/WaterCaustics", "stars": 42, "license": "None", "language": "python", "size": 5768}
{"docstring": "'''\nGenerate vertices and indices for drawing a skybox\n'''\n", "func_signal": "def SkyboxVerts():\n", "code": "vertexSize = ctypes.sizeof(GLfloat) * 3\nvertices = (GLfloat * 24) (\n    -1.0,   1.0,    1.0,\n    -1.0,   -1.0,   1.0,\n    1.0,    -1.0,   1.0,\n    1.0,    1.0,    1.0,\n    -1.0,   1.0,    -1.0,\n    -1.0,   -1.0,   -1.0,\n    1.0,    -1.0,   -1.0,\n    1.0,    1.0,    -1.0)\n    \nindices = (GLshort * 24) (\n    0,  1,  2,  3,\n    3,  2,  6,  7,\n    7,  6,  5,  4,\n    4,  5,  1,  0,  \n    0,  3,  7,  4,  \n    1,  2,  6,  5)\n    \nreturn vertices, indices, vertexSize", "path": "source\\utilities.py", "repo_name": "pabennett/WaterCaustics", "stars": 42, "license": "None", "language": "python", "size": 5768}
{"docstring": "''' Utility Function for generating numpy array of vertices '''\n", "func_signal": "def np3DArray(initialiser, points, rows, columns, dtype=np.float32):\n", "code": "points = int(points)\nrows = int(rows)\ncolumns = int(columns)\nreturn np.array([[[initialiser for i in range(points)]  \\\n                               for j in range(columns)] \\\n                               for k in range(rows)], \\\n                               dtype=dtype)", "path": "source\\utilities.py", "repo_name": "pabennett/WaterCaustics", "stars": 42, "license": "None", "language": "python", "size": 5768}
{"docstring": "'''\nGenerate vertices and indices for drawing a fullscreen quad.\n\nThe vertices are of the format:\n[v0x, v0y, v0z, t0x, t0y]\n[v1x, v1y, v1z, t1x, t1y]\n          ...\n[v7x, v7y, v7z, t7x, t7y]\n\n'''\n\n# The vertices are of the format:\n", "func_signal": "def fullscreenQuad():\n", "code": "vertexSize = ctypes.sizeof(GLfloat) * 5\nvertices = (GLfloat * 40)(\n    1.0,    -1.0,   0.0,    1.0,    0.0,\n    1.0,    1.0,    0.0,    1.0,    1.0,\n    -1.0,   1.0,    0.0,    0.0,    1.0,\n    -1.0,   -1.0,   0.0,    0.0,    0.0,\n    1.0,    -1.0,   0.0,    1.0,    0.0,\n    1.0,    1.0,    0.0,    1.0,    1.0,\n    -1.0,   1.0,    0.0,    0.0,    1.0,\n    -1.0,   -1.0,   0.0,    0.0,    0.0)\n        \nindices = (GLshort * 12)(\n    0,  1,  2,\n    2,  3,  0,\n    4,  6,  5,\n    6,  4,  7)\n    \n\nreturn vertices, indices, vertexSize", "path": "source\\utilities.py", "repo_name": "pabennett/WaterCaustics", "stars": 42, "license": "None", "language": "python", "size": 5768}
{"docstring": "# Add a [label,time_to_live] list\n", "func_signal": "def updateConsole(self, string):\n", "code": "self.textBuffer.append(string)\nif (len(self.textBuffer)>self.bufferLength):\n    self.textBuffer = self.textBuffer[1:]\nself.label.text = ''.join([str(x) + '\\n' for x in self.textBuffer])", "path": "source\\console.py", "repo_name": "pabennett/WaterCaustics", "stars": 42, "license": "None", "language": "python", "size": 5768}
{"docstring": "'''\nPerform initial setup for this object's vertex array object, which\nstores the vertex VBO and indices VBO. This is used to draw the surface\ngeometry for computing caustics in the shader.\n'''\n# Vertex Array Object for Position and Normal VBOs\n", "func_signal": "def setupVAO(self):\n", "code": "self.VAO = GLuint()\nglGenVertexArrays(1,ctypes.pointer(self.VAO))\nglBindVertexArray(self.VAO)\n\n# Vertex Buffer Objects (Positions Normals and Indices)\nself.vertVBO = GLuint()\nself.indexVBO = GLuint()\nglGenBuffers(1, ctypes.pointer(self.vertVBO))\nglGenBuffers(1, ctypes.pointer(self.indexVBO))\n        \nindicesGL = np.ctypeslib.as_ctypes(self.surface.indices)\nvertsGL = np.ctypeslib.as_ctypes(self.surface.verts)\nvertexSize = ctypes.sizeof(GLfloat) * 8\noffsetNormals = ctypes.sizeof(GLfloat) * 3\noffsetTexture = ctypes.sizeof(GLfloat) * 6\n\n# Set up vertices VBO (associated with VAO)\nglBindBuffer(GL_ARRAY_BUFFER, self.vertVBO)      \nglBufferData(GL_ARRAY_BUFFER, ctypes.sizeof(vertsGL), vertsGL, GL_STATIC_DRAW)\n# Positions\nglEnableVertexAttribArray(self.positionHandle) \nglVertexAttribPointer(self.positionHandle, 3, GL_FLOAT, GL_FALSE, vertexSize, 0)\n# Normals\nglEnableVertexAttribArray(self.normalHandle) \nglVertexAttribPointer(self.normalHandle, 3, GL_FLOAT, GL_FALSE, vertexSize, offsetNormals)\n\n# Set up indices VBO (associated with VAO)\n# Indices\nglBindBuffer(GL_ELEMENT_ARRAY_BUFFER, self.indexVBO)      \nglBufferData(GL_ELEMENT_ARRAY_BUFFER, ctypes.sizeof(indicesGL), indicesGL, GL_STATIC_DRAW)\n\nglBindVertexArray(0)", "path": "source\\caustics.py", "repo_name": "pabennett/WaterCaustics", "stars": 42, "license": "None", "language": "python", "size": 5768}
{"docstring": "\"\"\"\nPlot the numpy array as an intensity chart and save the figure as an image\n\"\"\"\n", "func_signal": "def np2DArrayToImage(array, name=\"figure.png\"):\n", "code": "import matplotlib\nmatplotlib.use('wxagg')\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n\nfig = plt.figure()\n\nplt.imshow(array, cmap=cm.jet)\n\nplt.savefig(name)", "path": "source\\utilities.py", "repo_name": "pabennett/WaterCaustics", "stars": 42, "license": "None", "language": "python", "size": 5768}
{"docstring": "# Keep grabbing frames until the frameGrabber has captured a full period\n", "func_signal": "def frameGrabberLoop():\n", "code": "print(\"Starting frame grabber, output files will be stored in \" + \n      kFrameGrabPath)\nwhile True:\n    try:\n        res = renderer.frameGrab(kTimeStep, directory=kFrameGrabPath)\n        if res:\n            break\n    except IOError:\n        print(\"The supplied path for saving frames was not valid or is unavailable\")\n        break", "path": "main.py", "repo_name": "pabennett/WaterCaustics", "stars": 42, "license": "None", "language": "python", "size": 5768}
{"docstring": "'''\nGenerate a 2D surface mesh with the given dimensions, scale and offset.\n\n      1   2   3   4  \n    +---+---+---+---+   Size in Quads: NxN where N is the dimension\n  1 |   |   |   |   |   Quad size (world space) = scale\n    +---+---+---+---+   \n  2 |   |   |   |   |  \n    +---+---+---+---+       \n  3 |   |   |   |   |           \n    +---+---+---+---+       \n  4 |   |   |   |   |  \n    +---+---+---+---+     \n'''\n", "func_signal": "def Mesh2DSurface(dimension=64, scale=1.0):\n", "code": "N = dimension               # Dimension - should be power of 2\n\nN1 = N+1                    # Vertex grid has additional row and\n                            # column for tiling purposes\n                                                             \n# Vertex arrays are 3-dimensional have have the following structure:\n# [[[v0x,v0y,v0z,n0x,n0y,n0z],[v1x,v1y,v1z,n1x,n1y,n1z]],\n#  [[v2x,v2y,v2z,n2x,n2y,n2z],[v3x,v3y,v3z,n3x,n3y,n3z]],\n#  [[v4x,v4y,v4z,n4x,n4y,n4z],[v5x,v5y,v5z,n5x,n5y,n5z]]]\nverts = np3DArray(0.0, 8, N1, N1, GLfloat)\n# Indicies are grouped per quad (6 indices for each quad)\n# The mesh is composed of NxN quads\nindices = np3DArray(0,6,N,N,dtype='u4')\n\n# Initialise the surface mesh\n# Populate the index array\nfor i in range(N):\n    for j in range(N):\n        idx = i * N1 + j\n        indices[i][j][0] = idx\n        indices[i][j][1] = idx + N1\n        indices[i][j][2] = idx + 1\n        indices[i][j][3] = idx + 1\n        indices[i][j][4] = idx + N1\n        indices[i][j][5] = idx + N1 + 1\n        \n# Populate the initial positions and normals\nfor i in range(N+1):\n    for j in range(N+1):\n        # Position X\n        verts[i][j][0] = j * scale\n        # Position Y                        \n        verts[i][j][1] = 0.0\n        # Position Z\n        verts[i][j][2] = i * scale\n        # # Normal X\n        verts[i][j][3] = 0.0\n        # # Normal Y                        \n        verts[i][j][4] = 1.0\n        # # Normal Z\n        verts[i][j][5] = 0.0 \n        # # Texture X\n        verts[i][j][6] = i/float(N)\n        # # Texture Y                        \n        verts[i][j][7] = j/float(N)  \nreturn verts, indices", "path": "source\\utilities.py", "repo_name": "pabennett/WaterCaustics", "stars": 42, "license": "None", "language": "python", "size": 5768}
{"docstring": "''' Alias for :attr:`cookies` (deprecated). '''\n", "func_signal": "def COOKIES(self):\n", "code": "depr('BaseRequest.COOKIES was renamed to BaseRequest.cookies (lowercase).')\nreturn self.cookies", "path": "lib\\bottle.py", "repo_name": "andrewnelder/hobo", "stars": 48, "license": "other", "language": "python", "size": 913}
{"docstring": "\"\"\" The values of :attr:`forms` and :attr:`files` combined into a single\n    :class:`FormsDict`. Values are either strings (form values) or\n    instances of :class:`cgi.FieldStorage` (file uploads).\n\"\"\"\n", "func_signal": "def POST(self):\n", "code": "post = FormsDict()\nsafe_env = {'QUERY_STRING':''} # Build a safe environment for cgi\nfor key in ('REQUEST_METHOD', 'CONTENT_TYPE', 'CONTENT_LENGTH'):\n    if key in self.environ: safe_env[key] = self.environ[key]\nif NCTextIOWrapper:\n    fb = NCTextIOWrapper(self.body, encoding='ISO-8859-1', newline='\\n')\nelse:\n    fb = self.body\ndata = cgi.FieldStorage(fp=fb, environ=safe_env, keep_blank_values=True)\nfor item in (data.list or [])[:self.MAX_PARAMS]:\n    post[item.name] = item if item.filename else item.value\nreturn post", "path": "lib\\bottle.py", "repo_name": "andrewnelder/hobo", "stars": 48, "license": "other", "language": "python", "size": 913}
{"docstring": "\"\"\" Cookies parsed into a :class:`FormsDict`. Signed cookies are NOT\n    decoded. Use :meth:`get_cookie` if you expect signed cookies. \"\"\"\n", "func_signal": "def cookies(self):\n", "code": "cookies = SimpleCookie(self.environ.get('HTTP_COOKIE',''))\ncookies = list(cookies.values())[:self.MAX_PARAMS]\nreturn FormsDict((c.key, c.value) for c in cookies)", "path": "lib\\bottle.py", "repo_name": "andrewnelder/hobo", "stars": 48, "license": "other", "language": "python", "size": 913}
{"docstring": "\"\"\" Return the content of a cookie. To read a `Signed Cookie`, the\n    `secret` must match the one used to create the cookie (see\n    :meth:`BaseResponse.set_cookie`). If anything goes wrong (missing\n    cookie or wrong signature), return a default value. \"\"\"\n", "func_signal": "def get_cookie(self, key, default=None, secret=None):\n", "code": "value = self.cookies.get(key)\nif secret and value:\n    dec = cookie_decode(value, secret) # (key, value) tuple or None\n    return dec[1] if dec and dec[0] == key else default\nreturn value or default", "path": "lib\\bottle.py", "repo_name": "andrewnelder/hobo", "stars": 48, "license": "other", "language": "python", "size": 913}
{"docstring": "''' Returns a copy of self. '''\n", "func_signal": "def copy(self):\n", "code": "copy = Response()\ncopy.status = self.status\ncopy._headers = dict((k, v[:]) for (k, v) in self._headers.items())\nreturn copy", "path": "lib\\bottle.py", "repo_name": "andrewnelder/hobo", "stars": 48, "license": "other", "language": "python", "size": 913}
{"docstring": "\"\"\" The bottle WSGI-interface. \"\"\"\n", "func_signal": "def wsgi(self, environ, start_response):\n", "code": "try:\n    out = self._cast(self._handle(environ))\n    # rfc2616 section 4.3\n    if response._status_code in (100, 101, 204, 304)\\\n    or request.method == 'HEAD':\n        if hasattr(out, 'close'): out.close()\n        out = []\n    if isinstance(response._status_line, unicode):\n      response._status_line = str(response._status_line)\n    start_response(response._status_line, list(response.iter_headers()))\n    return out\nexcept (KeyboardInterrupt, SystemExit, MemoryError):\n    raise\nexcept Exception:\n    if not self.catchall: raise\n    err = '<h1>Critical error while processing request: %s</h1>' \\\n          % html_escape(environ.get('PATH_INFO', '/'))\n    if DEBUG:\n        err += '<h2>Error:</h2>\\n<pre>\\n%s\\n</pre>\\n' \\\n               '<h2>Traceback:</h2>\\n<pre>\\n%s\\n</pre>\\n' \\\n               % (html_escape(repr(_e())), html_escape(format_exc(10)))\n    environ['wsgi.errors'].write(err)\n    headers = [('Content-Type', 'text/html; charset=UTF-8')]\n    start_response('500 INTERNAL SERVER ERROR', headers)\n    return [tob(err)]", "path": "lib\\bottle.py", "repo_name": "andrewnelder/hobo", "stars": 48, "license": "other", "language": "python", "size": 913}
{"docstring": "\"\"\" Render the template using keyword arguments as local variables. \"\"\"\n", "func_signal": "def render(self, *args, **kwargs):\n", "code": "for dictarg in args: kwargs.update(dictarg)\nstdout = []\nself.execute(stdout, kwargs)\nreturn ''.join(stdout)", "path": "lib\\bottle.py", "repo_name": "andrewnelder/hobo", "stars": 48, "license": "other", "language": "python", "size": 913}
{"docstring": "''' Return the most recent value for a key.\n\n    :param default: The default value to be returned if the key is not\n           present or the type conversion fails.\n    :param index: An index for the list of available values.\n    :param type: If defined, this callable is used to cast the value\n            into a specific type. Exception are suppressed and result in\n            the default value to be returned.\n'''\n", "func_signal": "def get(self, key, default=None, index=-1, type=None):\n", "code": "try:\n    val = self.dict[key][index]\n    return type(val) if type else val\nexcept Exception:\n    pass\nreturn default", "path": "lib\\bottle.py", "repo_name": "andrewnelder/hobo", "stars": 48, "license": "other", "language": "python", "size": 913}
{"docstring": "# TODO: Documentation and tests\n", "func_signal": "def decorator(callback):\n", "code": "if isinstance(callback, basestring): callback = load(callback)\nfor rule in makelist(path) or yieldroutes(callback):\n    for verb in makelist(method):\n        verb = verb.upper()\n        route = Route(self, rule, verb, callback, name=name,\n                      plugins=plugins, skiplist=skiplist, **config)\n        self.add_route(route)\nreturn callback", "path": "lib\\bottle.py", "repo_name": "andrewnelder/hobo", "stars": 48, "license": "other", "language": "python", "size": 913}
{"docstring": "''' The :attr:`query_string` parsed into a :class:`FormsDict`. These\n    values are sometimes called \"URL arguments\" or \"GET parameters\", but\n    not to be confused with \"URL wildcards\" as they are provided by the\n    :class:`Router`. '''\n", "func_signal": "def query(self):\n", "code": "pairs = parse_qsl(self.query_string, keep_blank_values=True)\nget = self.environ['bottle.get'] = FormsDict()\nfor key, value in pairs[:self.MAX_PARAMS]:\n    get[key] = value\nreturn get", "path": "lib\\bottle.py", "repo_name": "andrewnelder/hobo", "stars": 48, "license": "other", "language": "python", "size": 913}
{"docstring": "''' Create a new response header, replacing any previously defined\n    headers with the same name. '''\n", "func_signal": "def set_header(self, name, value, append=False):\n", "code": "if append:\n    self.add_header(name, value)\nelse:\n    self._headers[_hkey(name)] = [str(value)]", "path": "lib\\bottle.py", "repo_name": "andrewnelder/hobo", "stars": 48, "license": "other", "language": "python", "size": 913}
{"docstring": "''' Add a route object, but do not change the :data:`Route.app`\n    attribute.'''\n", "func_signal": "def add_route(self, route):\n", "code": "self.routes.append(route)\nself.router.add(route.rule, route.method, route, name=route.name)\nif DEBUG: route.prepare()", "path": "lib\\bottle.py", "repo_name": "andrewnelder/hobo", "stars": 48, "license": "other", "language": "python", "size": 913}
{"docstring": "''' This reads or sets the global settings stored in class.settings. '''\n", "func_signal": "def global_config(cls, key, *args):\n", "code": "if args:\n    cls.settings = cls.settings.copy() # Make settings local to class\n    cls.settings[key] = args[0]\nelse:\n    return cls.settings[key]", "path": "lib\\bottle.py", "repo_name": "andrewnelder/hobo", "stars": 48, "license": "other", "language": "python", "size": 913}
{"docstring": "''' True if the request was triggered by a XMLHttpRequest. This only\n    works with JavaScript libraries that support the `X-Requested-With`\n    header (most of the popular libraries do). '''\n", "func_signal": "def is_xhr(self):\n", "code": "requested_with = self.environ.get('HTTP_X_REQUESTED_WITH','')\nreturn requested_with.lower() == 'xmlhttprequest'", "path": "lib\\bottle.py", "repo_name": "andrewnelder/hobo", "stars": 48, "license": "other", "language": "python", "size": 913}
{"docstring": "''' Delete a cookie. Be sure to use the same `domain` and `path`\n    settings as used to create the cookie. '''\n", "func_signal": "def delete_cookie(self, key, **kwargs):\n", "code": "kwargs['max_age'] = -1\nkwargs['expires'] = 0\nself.set_cookie(key, '', **kwargs)", "path": "lib\\bottle.py", "repo_name": "andrewnelder/hobo", "stars": 48, "license": "other", "language": "python", "size": 913}
{"docstring": "\"\"\" (deprecated) Execute the first matching route callback and return\n    the result. :exc:`HTTPResponse` exceptions are catched and returned.\n    If :attr:`Bottle.catchall` is true, other exceptions are catched as\n    well and returned as :exc:`HTTPError` instances (500).\n\"\"\"\n", "func_signal": "def handle(self, path, method='GET'):\n", "code": "depr(\"This method will change semantics in 0.10. Try to avoid it.\")\nif isinstance(path, dict):\n    return self._handle(path)\nreturn self._handle({'PATH_INFO': path, 'REQUEST_METHOD': method.upper()})", "path": "lib\\bottle.py", "repo_name": "andrewnelder/hobo", "stars": 48, "license": "other", "language": "python", "size": 913}
{"docstring": "''' Shift path segments from :attr:`path` to :attr:`script_name` and\n    vice versa.\n\n   :param shift: The number of path segments to shift. May be negative\n                 to change the shift direction. (default: 1)\n'''\n", "func_signal": "def path_shift(self, shift=1):\n", "code": "script = self.environ.get('SCRIPT_NAME','/')\nself['SCRIPT_NAME'], self['PATH_INFO'] = path_shift(script, self.path, shift)", "path": "lib\\bottle.py", "repo_name": "andrewnelder/hobo", "stars": 48, "license": "other", "language": "python", "size": 913}
{"docstring": "\"\"\" Return a string that matches a named route \"\"\"\n", "func_signal": "def get_url(self, routename, **kargs):\n", "code": "scriptname = request.environ.get('SCRIPT_NAME', '').strip('/') + '/'\nlocation = self.router.build(routename, **kargs).lstrip('/')\nreturn urljoin(urljoin('/', scriptname), location)", "path": "lib\\bottle.py", "repo_name": "andrewnelder/hobo", "stars": 48, "license": "other", "language": "python", "size": 913}
{"docstring": "\"\"\" Load a bottle application from a module and make sure that the import\n    does not affect the current default application, but returns a separate\n    application object. See :func:`load` for the target parameter. \"\"\"\n", "func_signal": "def load_app(target):\n", "code": "global NORUN; NORUN, nr_old = True, NORUN\ntry:\n    tmp = default_app.push() # Create a new \"default application\"\n    rv = load(target) # Import the target module\n    return rv if callable(rv) else tmp\nfinally:\n    default_app.remove(tmp) # Remove the temporary added default application\n    NORUN = nr_old", "path": "lib\\bottle.py", "repo_name": "andrewnelder/hobo", "stars": 48, "license": "other", "language": "python", "size": 913}
{"docstring": "''' If the ``Content-Type`` header is ``application/json``, this\n    property holds the parsed content of the request body. Only requests\n    smaller than :attr:`MEMFILE_MAX` are processed to avoid memory\n    exhaustion. '''\n", "func_signal": "def json(self):\n", "code": "if 'application/json' in self.environ.get('CONTENT_TYPE', '') \\\nand 0 < self.content_length < self.MEMFILE_MAX:\n    return json_loads(self.body.read(self.MEMFILE_MAX))\nreturn None", "path": "lib\\bottle.py", "repo_name": "andrewnelder/hobo", "stars": 48, "license": "other", "language": "python", "size": 913}
{"docstring": "# TODO test updated time\n", "func_signal": "def testTags(self):\n", "code": "self.body['uri'] = 'http://foobar.com/%s.html'\\\n                   % sys._getframe().f_code.co_name\nself.body['tags'] = '[\"foo\",\"bar\"]'\nresponse, content = self._post()\nself.assert_( response['status'] == '200' )\nself.assert_( self.body['uri'] in content )\nself.assert_( '[\"foo\", \"bar\"]' in content )", "path": "dammit\\webtests.py", "repo_name": "harryf/urldammit", "stars": 39, "license": "None", "language": "python", "size": 285}
{"docstring": "\"\"\"\nCreation time - can only be set once\n\n>>> u = URI()\n>>> u.created = \"foo\"\nTraceback (most recent call last):\nTypeError: Expecting datetime.datetime not <type 'str'>\n>>> u.created = datetime.datetime.now()\n>>> type(u.created)\n<type 'datetime.datetime'>\n>>> u.created = datetime.datetime.now()\nTraceback (most recent call last):\nAttributeError: property 'created' is immutable\n\"\"\"\n", "func_signal": "def created():\n", "code": "def fget(self):\n    return self._created\n\ndef fset(self, time):\n    \n    if self._created:\n        raise AttributeError(\n            \"property 'created' is immutable\"\n            )\n    \n    if not isinstance(time, datetime.datetime):\n        raise TypeError(\n            \"Expecting datetime.datetime not %s\" % type(time)\n            )\n    \n    self._created = time", "path": "dammit\\uri.py", "repo_name": "harryf/urldammit", "stars": 39, "license": "None", "language": "python", "size": 285}
{"docstring": "\"\"\"\nConstruct an response as JSON\n>>> pack_response('')\n'{}'\n>>> class TestUri: uri = 'http://www.google.com'\n>>> pack_response(TestUri())\n'{\"uri\": \"http://www.google.com\"}'\n>>> class TestUri: status = 200\n>>> pack_response(TestUri())\n'{\"status\": \"200\"}'\n>>> class TestUri: created = datetime.datetime(2009,1,1)\n>>> pack_response(TestUri())\n'{\"created\": \"2009-01-01 00:00:00\"}'\n>>> class TestUri: updated = datetime.datetime(2009,1,1)\n>>> pack_response(TestUri())\n'{\"updated\": \"2009-01-01 00:00:00\"}'\n>>> class TestUri: location = 'http://www.google.com'\n>>> pack_response(TestUri())\n'{\"location\": \"http://www.google.com\"}'\n>>> class TestUri: tags = ['foo','bar']\n>>> pack_response(TestUri())\n'{\"tags\": [\"foo\", \"bar\"]}'\n>>> class TestUri: pairs = {'foo':'bar'}\n>>> pack_response(TestUri())\n'{\"pairs\": {\"foo\": \"bar\"}}'\n\"\"\"\n", "func_signal": "def pack_response(u):\n", "code": "keys = ('uri','status','created','updated',\n        'location','tags','pairs')\n\nd = {}\nfor key in keys:\n    try:\n        v = getattr(u, key)\n        if is_scalar(v) or type(v) == datetime.datetime:\n            d[key] = str(v)\n        else:\n            d[key] = v\n    except:\n        pass\n\ntry:\n    return simplejson.dumps(d)\nexcept:\n    logging.error(\"Can't dump '%s'\" % d)\n    return ''", "path": "dammit\\request.py", "repo_name": "harryf/urldammit", "stars": 39, "license": "None", "language": "python", "size": 285}
{"docstring": "\"\"\"\nCurrent HTTP status attached to this URI\n>>> u = GuardedURI()\n>>> u.status = 200\n>>> u.status\n200\n>>> u.status = 404\nTraceback (most recent call last):\nValueError: Status 404 not supported\n\"\"\"\n", "func_signal": "def status():\n", "code": "def fget(self):\n    return self._status\n\ndef fset(self, code):\n    \n    code = int(code)\n    \n    if not code == 200:\n        raise ValueError(\"Status %s not supported\" % code)\n\n    self._status = code", "path": "dammit\\uri.py", "repo_name": "harryf/urldammit", "stars": 39, "license": "None", "language": "python", "size": 285}
{"docstring": "\"\"\"\n>>> decode_string('foo')\nu'foo'\n>>> decode_string('foo%20bar')\nu'foo bar'\n>>> decode_string('f\u00f6\u00f6%20b\u00e4r')\nu'f\\\\xf6\\\\xf6 b\\\\xe4r'\n\"\"\"\n", "func_signal": "def decode_string(s):\n", "code": "try:\n    s = s.decode('utf-8')\nexcept:\n    if not type(s) in (str, unicode):\n        s = unicode(s)\n    s = unicode(non_ascii_pattern.sub('', s))\nreturn urldecode(s)", "path": "dammit\\request.py", "repo_name": "harryf/urldammit", "stars": 39, "license": "None", "language": "python", "size": 285}
{"docstring": "\"\"\"\nWhether the tags have been changed on this instance\n\n>>> u = URI()\n>>> u.status = 200\n>>> u.uri = \"http://local.ch\"\n>>> u.tags_updated == False\nTrue\n>>> u.tags = ['foo','bar']\n>>> u.tags_updated == True\nTrue\n\"\"\"\n", "func_signal": "def tags_updated():\n", "code": "def fget(self):\n    return self.meta.get('tags_updated', False)", "path": "dammit\\uri.py", "repo_name": "harryf/urldammit", "stars": 39, "license": "None", "language": "python", "size": 285}
{"docstring": "\"\"\"\nSetup default values for configuration\n\"\"\"\n", "func_signal": "def _default_config(self, config):\n", "code": "if not config: config = {}\nconfig['db_host'] = config.get('db_host', 'http://localhost:5984')\nconfig['db_name'] = config.get('db_name', 'urldammit')\nreturn config", "path": "dammit\\db_couch.py", "repo_name": "harryf/urldammit", "stars": 39, "license": "None", "language": "python", "size": 285}
{"docstring": "\"\"\"\nLoad data into the URI from a database\n\n>>> xdata = {'uri':'http://local.ch','status':'200'}\n>>> u = URI.load(xdata)\n>>> u.uri == 'http://local.ch'\nTrue\n>>> u.status == 200\nTrue\n\"\"\"\n", "func_signal": "def load(cls, data):\n", "code": "u = URI()\nitems = (slot for slot in u.__slots__ \\\n         if slot not in ('_id',))\ncasts = {\n    '_status': int,\n    '_tags': list,\n    '_pairs': dict,\n    '_created': lambda x: x,\n    '_updated': lambda x: x,\n    '_meta': dict,\n    }\n\nfor item in items:\n    cast = casts.get(item, str)\n    try:\n        setattr(u, item, cast(data[item[1:]]))\n    except:\n        setattr(u, item, None)\n\nu._id = URI.hash(u.uri)\n\nreturn u", "path": "dammit\\uri.py", "repo_name": "harryf/urldammit", "stars": 39, "license": "None", "language": "python", "size": 285}
{"docstring": "\"\"\"\nKey value pairs to associate with the URI.\nKeys must obey =~ /^[a-zA-Z0-9]{1,20}$/\nValues must be strings, not be longer than 100 bytes\n>>> u = URI()\n>>> u.pairs = \"foo\"\nTraceback (most recent call last):\nAttributeError: Can only modify pairs while status is 200 (not None)\n>>> u.status = 200\n>>> u.pairs = \"foo\"\nTraceback (most recent call last):\nTypeError: pairs must be a dictionary not <type 'str'>\n>>> u.pairs = {1:'x'}\nTraceback (most recent call last):\nTypeError: Key '1' must be a string not <type 'str'>\n>>> u.pairs = {'1+1':'x'}\nTraceback (most recent call last):\nValueError: Invalid key '1+1'\n>>> u.pairs = {'x':1}\nTraceback (most recent call last):\nTypeError: Value for key 'x' must be a string not <type 'int'>\n>>> u.pairs = {'x':'1'}\n>>> u.pairs['x']\n'1'\n\"\"\"\n", "func_signal": "def pairs():\n", "code": "def fget(self):\n    return self._pairs\n\ndef fset(self, mapping):\n    if self._status != 200:\n        raise AttributeError(\n            \"Can only modify pairs while status is 200 (not %s)\" %\\\n            self._status\n            )\n\n    if not isinstance(mapping, dict):\n        raise TypeError(\n            \"pairs must be a dictionary not %s\" % type(mapping)\n            )\n\n    for k, v in mapping.items():\n        \n        if not type(k) in (unicode, str):\n            raise TypeError(\n                \"Key '%s' must be a string not %s\" % (k, type(v))\n                )\n        \n        if not URI.validpairkey.match(k):\n            raise ValueError(\"Invalid key '%s'\" % k)\n        \n        if not type(v) in (unicode, str):\n            raise TypeError(\n                \"Value for key '%s' must be a string not %s\" % (k, type(v))\n                )\n        \n        if len(v) > constants.URI_PAIR_VALUE_LEN:\n            raise ValueError(\n                \"Value for key '%s' too large at %s bytes\" % (k, len(v))\n                )\n                             \n    self._pairs = mapping\n    self.meta['pairs_updated'] = True", "path": "dammit\\uri.py", "repo_name": "harryf/urldammit", "stars": 39, "license": "None", "language": "python", "size": 285}
{"docstring": "\"\"\"\nthe current location of this uri - nothing\nif it's new but the new uri if it's been\nmoved\n>>> u = URI()\n>>> u.status = 200\n>>> u.location = \"http://local.ch/new\"\nTraceback (most recent call last):\nAttributeError: Cannot set property location unless status is 301 (not 200)\n>>> u.status = 301\n>>> u.location = \"http://local.ch/new\"\n>>> u.location == \"http://local.ch/new\"\nTrue\n>>> u.location = \"\".join(\"a\" for x in range(constants.URI_LEN + 1))\nTraceback (most recent call last):\nAttributeError: location is too long\n\"\"\"\n", "func_signal": "def location():\n", "code": "def fget(self):\n    return self._location\n\ndef fset(self, locn):\n    if not self._status == 301:\n        raise AttributeError(\n            \"Cannot set property location unless status is 301 (not %s)\" %\\\n            self._status\n            )\n    if len(locn) > constants.URI_LOCATION_LEN:\n        raise AttributeError(\"location is too long\")\n    self._location = str(locn)", "path": "dammit\\uri.py", "repo_name": "harryf/urldammit", "stars": 39, "license": "None", "language": "python", "size": 285}
{"docstring": "\"\"\"\nThe URI we want to track\n\n>>> u = URI()\n>>> u.uri = \"http://local.ch\"\n>>> u.id == URI.hash(u.uri)\nTrue\n>>> u.uri = \"http://local.ch/new\"\nTraceback (most recent call last):\nAttributeError: property 'uri' is immutable\n>>> u = URI()\n>>> u.uri = \"\".join(\"a\" for x in range(constants.URI_LEN + 1))\nTraceback (most recent call last):\nAttributeError: uri is too long\n\"\"\"\n", "func_signal": "def uri():\n", "code": "def fget(self):\n    return self._uri\n\ndef fset(self, url):\n    url = str(url)\n    if self._uri:\n        raise AttributeError(\"property 'uri' is immutable\")\n    if len(url) > constants.URI_LEN:\n        raise AttributeError(\"uri is too long\")\n    self._uri = url\n    self._id = URI.hash(url)", "path": "dammit\\uri.py", "repo_name": "harryf/urldammit", "stars": 39, "license": "None", "language": "python", "size": 285}
{"docstring": "\"\"\"\nSHA-1 hash of the URI - read only - set via setting uri property\n\n>>> u = URI()\n>>> u.id == None\nTrue\n\"\"\"\n", "func_signal": "def id():\n", "code": "def fget(self):\n    return self._id", "path": "dammit\\uri.py", "repo_name": "harryf/urldammit", "stars": 39, "license": "None", "language": "python", "size": 285}
{"docstring": "\"\"\"\nTime of last change\n\n>>> u = URI()\n>>> u.updated = \"foo\"\nTraceback (most recent call last):\nTypeError: Expecting datetime.datetime not <type 'str'>\n>>> u.updated = datetime.datetime.now()\n>>> type(u.updated)\n<type 'datetime.datetime'>\n\"\"\"\n", "func_signal": "def updated():\n", "code": "def fget(self):\n    return self._updated\n\ndef fset(self, time):\n    if not isinstance(time, datetime.datetime):\n        raise TypeError(\"Expecting datetime.datetime not %s\" % type(time))\n    self._updated = time", "path": "dammit\\uri.py", "repo_name": "harryf/urldammit", "stars": 39, "license": "None", "language": "python", "size": 285}
{"docstring": "\"\"\"\nFor storing meta info e.g. _rev for couchdb\n\n>>> u = URI()\n>>> u.meta['foo'] = 'bar'\n>>> print u.meta['foo']\nbar\n\"\"\"\n", "func_signal": "def meta():\n", "code": "def fget(self):\n    if not self._meta:\n        self._meta = {}\n    return self._meta\n\ndef fset(self, mapping):\n    if not isinstance(mapping, dict):\n        raise TypeError(\"meta is type dict not %s\" % type(mapping))\n    self._meta = mapping", "path": "dammit\\uri.py", "repo_name": "harryf/urldammit", "stars": 39, "license": "None", "language": "python", "size": 285}
{"docstring": "# test the direct response to a POST\n# without following the redirect\n", "func_signal": "def test303(self):\n", "code": "self.body['uri'] = 'http://foobar.com/%s.html'\\\n                   % sys._getframe().f_code.co_name\nself.http.follow_redirects = False\nresponse, content = self._post()\nself.assert_( response['status'] == '303' )\nself.assert_( content == 'None' )", "path": "dammit\\webtests.py", "repo_name": "harryf/urldammit", "stars": 39, "license": "None", "language": "python", "size": 285}
{"docstring": "\"\"\"\nCouchdb doesn't directly support storing of hashes\nbut can be simulated via a list of key, value pairs\nWe have to translate between the two.\n\nThis takes a normal python dict and expands it to\n'couch-ready' form\n\n>>> d = {'a':1, 'b': 2}\n>>> o = str(expand_dict(d))\n>>> o.find(\"{'k': 'a', 'v': 1}\") != -1\nTrue\n\"\"\"\n", "func_signal": "def expand_dict(d):\n", "code": "pairs = []\nif not type(d) == dict:\n    return pairs\nfor k, v in d.items():\n    pairs.append({'k':k, 'v':v})\nreturn pairs", "path": "dammit\\db_couch.py", "repo_name": "harryf/urldammit", "stars": 39, "license": "None", "language": "python", "size": 285}
{"docstring": "# with empty value in status field, record is deleted\n", "func_signal": "def testDeleteViaPost(self):\n", "code": "self.body['uri'] = 'http://foobar.com/%s.html'\\\n                % sys._getframe().f_code.co_name\nresponse, content = self._post()\nself.assert_( response['status'] == '200' )\nself.assert_( self.body['uri'] in content )\n\nuri = response['content-location']\n\nself._init_http()\nself.body['delete'] = 'true'\nresponse, content = self._post()\nself.assert_( response['status'] == '204' )\n\nself._init_http()\nresponse, content = self.http.request(\n    uri, 'GET'\n    )\nself.assert_( response['status'] == '404' )", "path": "dammit\\webtests.py", "repo_name": "harryf/urldammit", "stars": 39, "license": "None", "language": "python", "size": 285}
{"docstring": "# TODO test updated time\n", "func_signal": "def testPOST(self):\n", "code": "self.body['uri'] = 'http://foobar.com/%s.html'\\\n                   % sys._getframe().f_code.co_name\nresponse, content = self._post()\nself.assert_(response['status'] == '200')\nself.assert_( self.body['uri'] in content )", "path": "dammit\\webtests.py", "repo_name": "harryf/urldammit", "stars": 39, "license": "None", "language": "python", "size": 285}
{"docstring": "\"\"\"\nList of tags associated with this URI.\nEach tag must obey =~ /^[a-zA-Z0-9]{1,20}$/\n\n>>> u = URI()\n>>> u.tags = \"foo\"\nTraceback (most recent call last):\nAttributeError: Can only modify tags while status is 200 (not None)\n>>> u.status = 200\n>>> u.tags = \"foo\"\nTraceback (most recent call last):\nTypeError: tags must be a list not <type 'str'>\n>>> u.tags = ['foo', 'bar']\n>>> u.tags\n['foo', 'bar']\n>>> u.tags = ['1+1']\nTraceback (most recent call last):\nValueError: Invalid tag '1+1'\n\"\"\"\n", "func_signal": "def tags():\n", "code": "def fget(self):\n    return self._tags\n\ndef fset(self, keywords):\n    if self._status != 200:\n        \n        raise AttributeError(\n            \"Can only modify tags while status is 200 (not %s)\" %\\\n            self._status\n            )\n    \n    if not isinstance(keywords, list):\n        raise TypeError(\n            \"tags must be a list not %s\" % type(keywords)\n            )\n    \n    for tag in keywords:\n        if not type(tag) in (unicode, str):\n            raise TypeError(\n                \"Tag '%s' must be a string not %s\" % (tag, type(tag))\n                )\n        \n        if not URI.validtag.match(tag):\n            raise ValueError(\"Invalid tag '%s'\" % tag)\n        \n    \n    self._tags = keywords\n    self.meta['tags_updated'] = True", "path": "dammit\\uri.py", "repo_name": "harryf/urldammit", "stars": 39, "license": "None", "language": "python", "size": 285}
{"docstring": "\"\"\"\nCurrent HTTP status attached to this URI\n\n>>> u = URI()\n>>> u.status = 404\nTraceback (most recent call last):\nValueError: New URIs must begin at status '200' not '404'\n>>> u.status = 200\n>>> u.status\n200\n>>> u.status = 500\nTraceback (most recent call last):\nValueError: Status 500 not supported\n>>> u.status = 404\n>>> u.status\n404\n>>> u.status = 200\n>>> u.status = 301\n>>> u.status\n301\n>>> u.status = 404\nTraceback (most recent call last):\nValueError: Current status is '301' - cannot change to '404'\n\"\"\"\n", "func_signal": "def status():\n", "code": "def fget(self):\n    return self._status\n\nstatuses = {\n    200:(200, 301, 404),\n    301:(301,),\n    404:(200, 301, 404)\n    }\n\ndef fset(self, code):\n    \n    code = int(code)\n    \n    if not code in statuses:\n        raise ValueError(\"Status %s not supported\" % code)\n\n    if self._status in statuses:\n        if not code in statuses[self._status]:\n            raise ValueError(\n                \"Current status is '%s' - cannot change to '%s'\" %\\\n                (self._status, code)\n                )\n\n    if not self._status and code != 200:\n        raise ValueError(\n            \"New URIs must begin at status '200' not '%s'\" % code\n            )\n    \n    self._status = code", "path": "dammit\\uri.py", "repo_name": "harryf/urldammit", "stars": 39, "license": "None", "language": "python", "size": 285}
{"docstring": "\"\"\"builds a xml node from book info to be added to the removal_list node\"\"\"\n", "func_signal": "def _slim_book_node(doc, asin, cde_content_type):\n", "code": "book_node = doc.createElement('meta_data')\nqxml.add_child(book_node, 'ASIN', asin)\nqxml.add_child(book_node, 'cde_contenttype', cde_content_type)\nreturn book_node", "path": "src\\handlers\\sync_metadata.py", "repo_name": "pwr/KSP", "stars": 45, "license": "other", "language": "python", "size": 923}
{"docstring": "\"\"\"reload a single book, unconditionally\"\"\"\n\n# logging.debug(\"reloading book %s\", uuid)\n", "func_signal": "def reload(uuid):\n", "code": "with _db_connect() as db:\n\tdb.row_factory = sqlite3.Row\n\tc = db.cursor()\n\tc.execute(\"select id, uuid, title, path, pubdate, timestamp, last_modified from books where uuid = ?\", ( uuid, ))\n\trow = c.fetchone()\n\tif not row:\n\t\tlogging.warn(\"book %s missing\", uuid)\n\t\treturn None\n\n\tbook_id = row['id']\n\tbook_dict = _book_dict(row)\n\tbooks = { book_id: book_dict }\n\n\t_update_book_list(c, books, 'authors',\n\t\t\t\"select books_authors_link.book, authors.name from books_authors_link, authors\"\n\t\t\t\t\" where books_authors_link.book = ? and books_authors_link.author = authors.id\", ( book_id, ))\n\t_update_book_list(c, books, 'publishers',\n\t\t\t\"select books_publishers_link.book, publishers.name from books_publishers_link, publishers\"\n\t\t\t\t\" where books_publishers_link.book = ? and books_publishers_link.publisher = publishers.id\", ( book_id, ))\n\t_update_book_list(c, books, 'languages',\n\t\t\t\"select books_languages_link.book, languages.lang_code from books_languages_link, languages\"\n\t\t\t\t\" where books_languages_link.book = ? and books_languages_link.lang_code = languages.id\", ( book_id, ))\n\n\tformats_q = ','.join( ('?',) * len(features.supported_formats) )\n\t_update_book_map(c, books, 'files',\n\t\t\t\"select book, format, name from data where book = ? and format in (%s)\" % formats_q, ( book_id, ) + tuple(features.supported_formats))\n\n\treturn book_dict", "path": "src\\calibre\\db.py", "repo_name": "pwr/KSP", "stars": 45, "license": "other", "language": "python", "size": 923}
{"docstring": "# logging.debug('\"%s\", %s', query, params)\n", "func_signal": "def _update_book_map(cursor, books_map, field, query, params = ()):\n", "code": "for row in cursor.execute(query, params):\n\tbook = books_map[row[0]]\n\tbook[field][row[1]] = row[2] if len(row) == 3 else row[2:]", "path": "src\\calibre\\db.py", "repo_name": "pwr/KSP", "stars": 45, "license": "other", "language": "python", "size": 923}
{"docstring": "\"\"\"reads the book GUID from a MOBI file\"\"\"\n", "func_signal": "def read_guid(path):\n", "code": "if not os.path.isfile(path):\n\treturn None\n\ntry:\n\twith open(path, 'rb', 4 * 1024) as mobi:\n\t\tmobi.seek(0x3C)\n\t\tif mobi.read(8) != b'BOOKMOBI':\n\t\t\tlogging.warn(\"%s: not a MOBI PDB?\", path)\n\t\t\treturn None\n\t\tmobi.seek(0x4C)\n\t\tpdb_records, = struct.unpack('!H', mobi.read(2))\n\t\t# logging.debug(\"%s: %d PDB records\", path, pdb_records)\n\t\tmobi.seek(pdb_records * 8 + 2 + 0x10, 1)\n\t\tif mobi.read(7) != b'MOBI\\x00\\x00\\x00':\n\t\t\tlogging.debug(\"%s: MOBI header not found\", path)\n\t\t\treturn None\n\t\theader_length = mobi.read(1)[0]\n\t\tif header_length not in (0xE8, 0xF8): # MOBI7 (regular mobi), MOBI8 (aka KF8)\n\t\t\tlogging.debug(\"%s: MOBI content type %d not supported\", path, hex(mobi_type))\n\t\t\treturn None\n\t\tmobi.seek(8, 1)\n\t\treturn mobi.read(4)\nexcept:\n\tlogging.warn(\"failed to read GUID from %s\", path)", "path": "src\\formats\\mobi.py", "repo_name": "pwr/KSP", "stars": 45, "license": "other", "language": "python", "size": 923}
{"docstring": "\"\"\"builds a xml node from book info to be added to the add_update_list node\"\"\"\n", "func_signal": "def _book_node(doc, book):\n", "code": "book_node = doc.createElement('meta_data')\n\n# we keep the same tag order as in Amazon's proper responses\nqxml.add_child(book_node, 'ASIN', book.asin)\nqxml.add_child(book_node, 'title', book.title)\n\nauthors_node = qxml.add_child(book_node, 'authors')\nfor author in book.authors:\n\tqxml.add_child(authors_node, 'author', author)\n\npublishers_node = qxml.add_child(book_node, 'publishers')\nfor publisher in book.publishers:\n\tqxml.add_child(publishers_node, 'publisher', publisher)\nqxml.add_child(book_node, 'publication_date', date_iso(book.last_modified))\n\nqxml.add_child(book_node, 'cde_contenttype', book.cde_content_type)\nqxml.add_child(book_node, 'content_type', book.content_type)\n\nif book.languages:\n\tqxml.add_child(book_node, 'content_language', book.languages[0])\n\nreturn book_node", "path": "src\\handlers\\sync_metadata.py", "repo_name": "pwr/KSP", "stars": 45, "license": "other", "language": "python", "size": 923}
{"docstring": "\"\"\"proxy a request to the originally intended server, returns a http response\"\"\"\n", "func_signal": "def call_upstream(self, request, device):\n", "code": "if device.is_provisional(): # not yet allowed access upstream\n\tlogging.warn(\"device %s may not connect to upstream (provisional)\")\n\treturn None\n\nupstream_host = self._upstream_host(request, device)\nconn = device.connections.get(upstream_host)\nif not conn:\n\t# it's very unlikely a device will shoot two requests to the same service at the same time, just after KSP started\n\t# so we should be reasonable safe creating the connection without locking\n\tconn = HTTPSConnection(upstream_host, context = device.ssl_context(upstream_host))\n\tconn.last_call = 0\n\tconn._lock = RLock()\n\tlogging.info(\"created upstream connection to %s for %s\", upstream_host, device)\n\tconn = device.connections.setdefault(upstream_host, conn) # just in case...\nwith conn._lock:\n\t# uuuuugly... but this way we make sure device requests don't step on each other's toes too much\n\tdel request.headers['Host']\n\trequest.headers['Host'] = conn.host\n\n\t# yeah, let's not leave these around\n\tdel request.headers['X-Forwarded-For']\n\tdel request.headers['X-Forwarded-Host']\n\tdel request.headers['X-Forwarded-Server']\n\n\t# we check the connection state because otherwise we might mess up another request in process\n\tif conn.sock:\n\t\tif conn._HTTPConnection__state != _CS_IDLE:\n\t\t\traise Exception(\"acquired connection but it's not idle!\", upstream_host, str(device))\n\t\tif request.started_at - conn.last_call > _IDLE: # avoid socket timeouts\n\t\t\tconn.close()\n\n\t# finally, actually call upstream\n\tresponse = self._call_upstream(conn, request)\n\tresponse = wrap_response(response)\n\nhttp_debug(\"got response %s\", response)\nreturn response", "path": "src\\handlers\\upstream.py", "repo_name": "pwr/KSP", "stars": 45, "license": "other", "language": "python", "size": 923}
{"docstring": "# we ignore requests not targeted to our service\n", "func_signal": "def ignore_request(self):\n", "code": "if self.request_version != self.protocol_version: # all kindle requests SHOULD be HTTP/1.1\n\treturn True\nif config.server_path_prefix:\n\tif not self.path.startswith(config.server_path_prefix):\n\t\tlogging.warn(\"expected path prefix %s\", config.server_path_prefix)\n\t\treturn True\nreturn False", "path": "src\\server\\http_handler.py", "repo_name": "pwr/KSP", "stars": 45, "license": "other", "language": "python", "size": 923}
{"docstring": "\"\"\"\nbuilds a BookResponse object for downloading the book contents\n\"\"\"\n", "func_signal": "def book_response(self, asin, device, range_header):\n", "code": "book = calibre.book(asin, True)\nif not book:\n\tlogging.warn(\"device %s tried to download book %s, but it is not in the library (anymore?)\", device, asin)\n\treturn None\nif not book.file_path:\n\tlogging.warn(\"device %s tried to download book %s, but it has no file available\", device, asin)\n\treturn None\n\nbytes_range = _range(range_header, book.file_size)\nreturn _BookResponse(book, bytes_range)", "path": "src\\handlers\\download_content.py", "repo_name": "pwr/KSP", "stars": 45, "license": "other", "language": "python", "size": 923}
{"docstring": "# all requests handled by the same Handler instance come from the same device, always\n", "func_signal": "def _detect_device(self):\n", "code": "device = self.last_device\nif device:\n\tlogging.debug(\"%s last_device = %s\", id(self), device)\nif not device:\n\t# look for a device record, will be automatically created if the features is enabled\n\tdevice = devices.detect(request.client_ip(self), cookie = request.xfsn(self),\n\t\t\t\t\t\t\tkind = request.guess_client(self), serial = request.get_device_serial(self))\n\tif not device:\n\t\tlogging.error(\"failed to identify device for %s\", self.requestline)\n\t\treturn None, 403\n\tif self.last_device != device:\n\t\tlogging.debug(\"identified device %s\", device)\n\tif not device.is_provisional():\n\t\tself.last_device = device\n\t\tlogging.debug(\"guessed device %s\", device)\nif device.context_failed(): # failed to create a proper SSL context\n\tlogging.warn(\"denying access to unregistered device %s\", device)\n\treturn None, 401\nreturn device, None", "path": "src\\server\\http_handler.py", "repo_name": "pwr/KSP", "stars": 45, "license": "other", "language": "python", "size": 923}
{"docstring": "# book_ids is NOT a complete list of books on device\n", "func_signal": "def _process_sidecar_upload(device, book_ids, book_nodes, default_timestamp):\n", "code": "for asin in book_ids:\n\tbook = calibre.book(asin)\n\tif book:\n\t\tbook.mark_on_device(device)\n\nfor x_book in book_nodes:\n\tasin = x_book.getAttribute('key')\n\tbook = calibre.book(asin)\n\tif not book:\n\t\tlogging.warn(\"sidecar upload for unknown book %s\", asin)\n\t\tcontinue\n\n\tfor kind in ('last_read', 'bookmark', 'highlight', 'note'):\n\t\tfor x_item in qxml.iter_children(x_book, kind):\n\t\t\ttimestamp = x_item.getAttribute('timestamp') or default_timestamp\n\t\t\tbegin = x_item.getAttribute('begin') or None\n\t\t\tend = x_item.getAttribute('end') or begin\n\t\t\tpos = x_item.getAttribute('pos') or x_item.getAttribute('location') or None\n\t\t\tstate = x_item.getAttribute('state') or None\n\t\t\ttext = qxml.get_text(x_item) if kind == 'note' else None\n\n\t\t\tif kind == 'last_read':\n\t\t\t\tannotations.set_last_read(device, asin, timestamp, begin, pos, state)\n\t\t\telse:\n\t\t\t\taction = x_item.getAttribute('action')\n\t\t\t\tif action == 'create':\n\t\t\t\t\tannotations.create(device, asin, kind, timestamp, begin, end, pos, state, text)\n\t\t\t\telif action == 'delete':\n\t\t\t\t\tannotations.delete(device, asin, kind, timestamp, begin, end)\n\t\t\t\telif action == 'modify':\n\t\t\t\t\tannotations.modify(device, asin, kind, timestamp, begin, end, text)\n\t\t\t\telse:\n\t\t\t\t\tlogging.error(\"unknown sidecar action %s: %s\", action, x_item)", "path": "src\\handlers\\sidecar.py", "repo_name": "pwr/KSP", "stars": 45, "license": "other", "language": "python", "size": 923}
{"docstring": "# logging.debug(\"## %s\", self.requestline)\n\n", "func_signal": "def handle_call(self, device):\n", "code": "request.read_body_and_length(self)\nhttp_debug(\"%s\", self)\n\n# and finally got to the part where we handle the request\nhandler = self.server.find_handler(self)\nresponse = None\nif handler:\n\ttry:\n\t\tresponse = handler.call(self, device)\n\texcept ExceptionResponse as er:\n\t\tresponse = er.response\nif response is None:\n\tlogging.warn(\"not found (%s) %s\", self.headers.get('Host'), self.requestline)\n\treturn 404\nif type(response) == int: # returned just a status code\n\treturn response\n\nself.close_connection = response.will_close == True # will_close is a tristate...\n\nhttp_debug(\"replying with %s\", response)\nself.send_response_only(response.status, response.reason)\n\nheader_strings = [ k + ': ' + str(v) for k, v in response.headers.items() ]\nself.wfile.write(bytes('\\r\\n'.join(header_strings), 'latin1'))\nself.wfile.write(b'\\r\\n\\r\\n')\n\nbyte_count = response.write_to(self.wfile) # writes the body\nself.log_request(response.status, byte_count)", "path": "src\\server\\http_handler.py", "repo_name": "pwr/KSP", "stars": 45, "license": "other", "language": "python", "size": 923}
{"docstring": "\"\"\"reads the CDE_TYPE record from the MOBI file, and checks the MOBI has an expected asin\"\"\"\n", "func_signal": "def read_cde_type(path, asin):\n", "code": "if not os.path.isfile(path):\n\treturn None\n\n# logging.debug(\"checking %s\", path)\nasin_113 = None\nasin_504 = None\ncde_type = None\n\nwith open(path, 'rb', 16 * 1024) as mobi:\n\tmobi.seek(0x3C)\n\tif mobi.read(8) != b'BOOKMOBI':\n\t\tlogging.warn(\"%s: not a MOBI PDB?\", path)\n\t\treturn None\n\tmobi.seek(0x4C)\n\tpdb_records, = struct.unpack('!H', mobi.read(2))\n\t# logging.debug(\"%s: %d PDB records\", path, pdb_records)\n\tmobi.seek(pdb_records * 8 + 2 + 0x10, 1)\n\tif mobi.read(7) != b'MOBI\\x00\\x00\\x00':\n\t\tlogging.debug(\"%s: MOBI header not found\", path)\n\t\treturn None\n\theader_length = mobi.read(1)[0]\n\tif header_length not in (0xE8, 0xF8): # MOBI7 (regular mobi), MOBI7 + MOBI8 (aka KF8) composite\n\t\tlogging.debug(\"%s: MOBI content type %d not supported\", path, hex(mobi_type))\n\t\treturn None\n\tmobi.seek(header_length - 8, 1)\n\tif mobi.read(4) != b'EXTH':\n\t\tlogging.debug(\"%s: EXTH header not found\", path)\n\t\treturn None\n\n\texth_len, exth_record_count = struct.unpack('!II', mobi.read(8))\n\t# logging.debug(\"%s: %d EXTH records in %d bytes\", path, exth_record_count, exth_len)\n\twhile exth_record_count > 0:\n\t\trec_head = mobi.read(8)\n\t\trec_type, rec_length = struct.unpack('!II', rec_head)\n\t\tif rec_type == 0 or rec_length < 8:\n\t\t\tlogging.warn(\"%s: damaged EXTH header\", path)\n\t\t\treturn None\n\t\tif rec_length > 8:\n\t\t\trec_value = mobi.read(rec_length - 8)\n\t\t\tif rec_type == 113:\n\t\t\t\tasin_113 = str(rec_value, 'ascii')\n\t\t\telif rec_type == 504:\n\t\t\t\tasin_504 = str(rec_value, 'ascii')\n\t\t\telif rec_type == 501:\n\t\t\t\tcde_type = str(rec_value, 'ascii')\n\t\texth_record_count -= 1\n\nif asin_504 and asin_504 != asin_113:\n\tlogging.warn(\"%s: ASIN-504 %s different from ASIN-113 %s\", path, asin_504, asin_113)\nif not asin_113:\n\tlogging.warn(\"%s: ASIN-113 record not found\", path)\n\treturn None\nif asin_113 != asin:\n\tlogging.warn(\"%s: ASIN-113 %s does not match expected ASIN %s\", path, asin_113, asin)\n\treturn None\nif not cde_type:\n\tlogging.warn(\"%s: no CDE_TYPE record\", path)\n\treturn None\nif cde_type not in ( 'EBOK', 'PDOC' ):\n\tlogging.warn(\"%s: unexpected CDE_TYPE '%s'\", cde_type)\n\treturn None\n# logging.debug(\"%s: confirmed ASIN %s with CDE TYPE %s\", path, asin, cde_type)\nreturn cde_type", "path": "src\\formats\\mobi.py", "repo_name": "pwr/KSP", "stars": 45, "license": "other", "language": "python", "size": 923}
{"docstring": "\"\"\"returns an ISO date format from a funny calibre date format\"\"\"\n", "func_signal": "def _clean_timeformat(text):\n", "code": "if text.endswith(\"+00:00\"):\n\ttext = text[:-6] + \"+0000\"\nif text[10] == \" \":\n\ttext = text.replace(\" \", \"T\", 1)\nreturn text", "path": "src\\calibre\\db.py", "repo_name": "pwr/KSP", "stars": 45, "license": "other", "language": "python", "size": 923}
{"docstring": "# determine text encoding\n", "func_signal": "def dump_exth(self):\n", "code": "codepage = self.hdr['codepage']\ncodec = 'windows-1252'\ncodec_map = {\n        1252 : 'windows-1252',\n        65001: 'utf-8',\n        }\nif codepage in list(codec_map.keys()):\n    codec = codec_map[codepage]\nif self.exth == '':\n    return\nextheader = self.exth\nid_map_strings = {\n        1 : 'Drm Server Id',\n        2 : 'Drm Commerce Id',\n        3 : 'Drm Ebookbase Book Id',\n        100 : 'Creator',\n        101 : 'Publisher',\n        102 : 'Imprint',\n        103 : 'Description',\n        104 : 'ISBN',\n        105 : 'Subject',\n        106 : 'Published',\n        107 : 'Review',\n        108 : 'Contributor',\n        109 : 'Rights',\n        110 : 'SubjectCode',\n        111 : 'Type',\n        112 : 'Source',\n        113 : 'ASIN',\n        114 : 'versionNumber',\n        117 : 'Adult',\n        118 : 'Price',\n        119 : 'Currency',\n        122 : 'fixed-layout',\n        123 : 'book-type',\n        124 : 'orientation-lock',\n        126 : 'original-resolution',\n        127 : 'zero-gutter',\n        128 : 'zero-margin',\n        129 : 'K8(129)_Masthead/Cover_Image',\n        132 : 'RegionMagnification',\n        200 : 'DictShortName',\n        208 : 'Watermark',\n        501 : 'CDE Type',\n        502 : 'last_update_time',\n        503 : 'Updated Title',\n}\nid_map_values = {\n        115 : 'sample',\n        116 : 'StartOffset',\n        121 : 'K8(121)_Boundary_Section',\n        125 : 'K8(125)_Count_of_Resources_Fonts_Images',\n        131 : 'K8(131)_Unidentified_Count',\n        201 : 'CoverOffset',\n        202 : 'ThumbOffset',\n        203 : 'Fake Cover',\n        204 : 'Creator Software',\n        205 : 'Creator Major Version',\n        206 : 'Creator Minor Version',\n        207 : 'Creator Build Number',\n        401 : 'Clipping Limit',\n        402 : 'Publisher Limit',\n        404 : 'Text to Speech Disabled',\n}\nid_map_hexstrings = {\n        209 : 'Tamper Proof Keys (hex)',\n        300 : 'Font Signature (hex)',\n}\n_length, num_items = struct.unpack('>LL', extheader[4:12])\nextheader = extheader[12:]\npos = 0\nfor _ in range(num_items):\n    id, size = struct.unpack('>LL', extheader[pos:pos+8])\n    content = extheader[pos + 8: pos + size]\n    if id in list(id_map_strings.keys()):\n        name = id_map_strings[id]\n        print('    Key: \"%s\"\\n        Value: \"%s\"' % (name, str(content, codec).encode(\"utf-8\")))\n    elif id in list(id_map_values.keys()):\n        name = id_map_values[id]\n        if size == 9:\n            value, = struct.unpack('B',content)\n            print('    Key: \"%s\"\\n        Value: 0x%01x' % (name, value))\n        elif size == 10:\n            value, = struct.unpack('>H',content)\n            print('    Key: \"%s\"\\n        Value: 0x%02x' % (name, value))\n        elif size == 12:\n            value, = struct.unpack('>L',content)\n            print('    Key: \"%s\"\\n        Value: 0x%04x' % (name, value))\n        else:\n            print(\"Error: Value for %s has unexpected size of %s\" % (name, size))\n    elif id in list(id_map_hexstrings.keys()):\n        name = id_map_hexstrings[id]\n        print('    Key: \"%s\"\\n        Value: 0x%s' % (name, str(binascii.hexlify(content), 'ascii')))\n    else:\n        print(\"Warning: Unknown metadata with id %s found\" % id)\n        name = str(id) + ' (hex)'\n        print('    Key: \"%s\"\\n        Value: 0x%s' % (name, str(binascii.hexlify(content), 'ascii')))\n    pos += size\nreturn", "path": "tools\\DumpMobiHeader.py", "repo_name": "pwr/KSP", "stars": 45, "license": "other", "language": "python", "size": 923}
{"docstring": "\"\"\"\nreload the books map from the calibre library, but only if the library db file has changed since the last load\nif no changes were detected, returns None\n\"\"\"\n", "func_signal": "def reload_all():\n", "code": "global last_update\ndb_last_modified = os.path.getmtime(_db_path)\nif db_last_modified <= last_update:\n\treturn None\n\n# last_update_string = time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime(last_update))\n\nbooks = {}\nload_start = time.time()\nwith _db_connect() as db:\n\tdb.row_factory = sqlite3.Row\n\tc = db.cursor()\n\tfor row in c.execute(\"select id, uuid, title, path, pubdate, timestamp, last_modified from books\"):\n\t\t# logging.debug(\"  %s\", row)\n\t\tbooks[row['id']] = _book_dict(row)\n\n\t_update_book_list(c, books, 'authors',\n\t\t\t\"select books_authors_link.book, authors.sort from books_authors_link, authors\"\n\t\t\t\t\" where books_authors_link.author = authors.id\")\n\t_update_book_list(c, books, 'publishers',\n\t\t\t\"select books_publishers_link.book, publishers.name from books_publishers_link, publishers\"\n\t\t\t\t\" where books_publishers_link.publisher = publishers.id\")\n\t_update_book_list(c, books, 'languages',\n\t\t\t\"select books_languages_link.book, languages.lang_code from books_languages_link, languages\"\n\t\t\t\t\" where books_languages_link.lang_code = languages.id\")\n\n\tformats_q = ','.join( ['?'] * len(features.supported_formats) )\n\t_update_book_map(c, books, 'files',\n\t\t\t\"select book, format, name from data where format in (%s)\" % formats_q, tuple(features.supported_formats))\n\nlast_update = db_last_modified\n\nload_duration = time.time() - load_start\nlogging.info(\"loaded %d book records in %.3f seconds (%s updated %s)\", len(books), load_duration, _db_path, time.ctime(db_last_modified))\n\nreturn books", "path": "src\\calibre\\db.py", "repo_name": "pwr/KSP", "stars": 45, "license": "other", "language": "python", "size": 923}
{"docstring": "\"\"\"returns True if this handler can process the given request\"\"\"\n", "func_signal": "def accept(self, request):\n", "code": "if type(self.expected_path) == str:\n\tif not _path_matches(request.path, self.expected_path):\n\t\treturn False\nelse:\n\tif not any(( _path_matches(request.path, p) for p in self.expected_path )):\n\t\treturn False\nif self.expected_command and request.command != self.expected_command:\n\treturn False\nreturn True", "path": "src\\handlers\\dummy.py", "repo_name": "pwr/KSP", "stars": 45, "license": "other", "language": "python", "size": 923}
{"docstring": "\"\"\"\ngets the Book for a given asin, optionally updating it from the calibre db first\n\"\"\"\n", "func_signal": "def book(asin, refresh = False):\n", "code": "if not asin:\n\tlogging.warn(\"tried to find book with no asin\")\n\treturn None\nuuid = asin\nbook = _books.get(asin)\nif book:\n\tuuid = book.uuid\n\tasin = book.asin\nif refresh or not book:\n\tbook = _book_refresh(uuid, asin, book, None, 0)\nreturn book", "path": "src\\calibre\\__init__.py", "repo_name": "pwr/KSP", "stars": 45, "license": "other", "language": "python", "size": 923}
{"docstring": "\"\"\"converts calibre date time to a posix timestamp\"\"\"\n", "func_signal": "def _parse_timestamp(text):\n", "code": "if text.endswith('+00:00'):\n\ttext = text[:-6]\nif text.endswith('+0000'):\n\ttext = text[:-5]\nif len(text) > 10 and text[10] == ' ':\n\ttext = text[:10] + 'T' + text[11:]\n\ntry:\n\tif len(text) == 19:\n\t\tt = time.strptime(text, \"%Y-%m-%dT%H:%M:%S\")\n\telse:\n\t\tt = time.strptime(text, \"%Y-%m-%dT%H:%M:%S.%f\")\nexcept:\n\t# failed to parse...\n\tlogging.exception('parsing %s', text)\n\treturn 0\n\ntry:\n\treturn time.mktime(t)\nexcept:\n\tlogging.exception('timestamp %s', t)\n\t# some funny dates like 1969-??-?? will throw this...\n\treturn 0", "path": "src\\calibre\\db.py", "repo_name": "pwr/KSP", "stars": 45, "license": "other", "language": "python", "size": 923}
{"docstring": "\"\"\"\ngets the current books map, optionally updating it from the calibre db first\n\"\"\"\n", "func_signal": "def books(refresh = False):\n", "code": "if refresh:\n\tstarted_at = time.time()\n\tbook_dicts = _db.reload_all()\n\tif book_dicts:\n\t\t# we use the timestamp to mark which books we've updated this round\n\t\t# so that we know which book have disappeared from the library since the last update\n\t\ttimestamp = _db.last_update\n\n\t\twhile book_dicts:\n\t\t\tbook_id, book_dict = book_dicts.popitem()\n\t\t\tuuid = book_dict['uuid']\n\t\t\tasin = uuid\n\t\t\tbook = _books.get(asin)\n\t\t\tif not book:\n\t\t\t\tasin2 = asin.replace('-', '')\n\t\t\t\tbook = _books.get(asin2)\n\t\t\t\tif book:\n\t\t\t\t\tasin = book.asin\n\t\t\t_book_refresh(uuid, asin, book, book_dict, timestamp) # adds it to _books as well\n\n\t\tfor book in list(_books.values()):\n\t\t\tif book._timestamp != timestamp: # removed from the library\n\t\t\t\tlogging.warn(\"book was in library but missing after refresh: %s\", book)\n\t\t\t\t_books.pop(book.asin, None)\n\n\t\tload_duration = time.time() - started_at\n\t\tlogging.info(\"Calibre Library loaded and processed in %d:%02.3f minutes, at an average %.3f seconds/book\",\n\t\t\t\tload_duration // 60, load_duration % 60, load_duration / len(_books))\n\nreturn _books", "path": "src\\calibre\\__init__.py", "repo_name": "pwr/KSP", "stars": 45, "license": "other", "language": "python", "size": 923}
{"docstring": "# first 16 bytes are not part of the official mobiheader\n# but we will treat it as such\n# so section 0 is 16 (decimal) + self.length in total == 0x108 bytes for Mobi 8 headers\n", "func_signal": "def __init__(self, header, start):\n", "code": "self.header = header\nself.start = start\nself.version, = struct.unpack_from('>L', self.header, 0x24)\nself.length, = struct.unpack_from('>L',self.header, 0x14)\nprint(\"Header Version is: 0x%0x\" % self.version)\nprint(\"Header start position is: 0x%0x\" % self.start)\nprint(\"Header Length is: 0x%0x\" % self.length)\n# if self.length != 0xf8:\n#     print \"Error: Unexpected Header Length: 0x%0x\" % self.length\nself.hdr = {}\nself.extra = self.header[self.length+16:]\n# set it up for the proper header version\nif self.version < 8:\n    self.mobi_header_sorted_keys = HdrParser.mobi6_header_sorted_keys\n    self.mobi_header = HdrParser.mobi6_header\nelse:\n    self.mobi_header_sorted_keys = HdrParser.mobi8_header_sorted_keys\n    self.mobi_header = HdrParser.mobi8_header\n\n# parse the header information\nfor key in self.mobi_header_sorted_keys:\n    (pos, format, tot_len) = self.mobi_header[key]\n    if pos < (self.length + 16):\n        val, = struct.unpack_from(format, self.header, pos)\n        self.hdr[key] = val\nself.exth = ''\nif self.hdr['exth_flags'] & 0x40:\n    exth_offset = self.length + 16\n    self.exth = self.header[exth_offset:]\n    self.extra = self.header[self.length+ 16: exth_offset]", "path": "tools\\DumpMobiHeader.py", "repo_name": "pwr/KSP", "stars": 45, "license": "other", "language": "python", "size": 923}
{"docstring": "\"\"\"Instantiate plugin pmod with id.\"\"\"\n", "func_signal": "def _instantiate_one(self, id, pname):\n", "code": "clazz = self._find_plugin_class(pname)\nif None == clazz:\n    return\ntry:\n    logger.debug(\"  Instantiating plugin '%s' as '%s'\" % (pname, id))\n    inst = clazz(self.__proto_version,\n                 self.__from_client_q,\n                 self.__from_server_q)\n    inst.init(self.__config.argstr[id])\n    self.__instances[id] = inst\nexcept Exception as e:\n    logger.error(\"Failed to instantiate '%s': %s\" % (id, str(e)))", "path": "mc3p\\plugins.py", "repo_name": "mmcgill/mc3p", "stars": 32, "license": "gpl-2.0", "language": "python", "size": 491}
{"docstring": "\"\"\"Replacement for __import__/imp.load_source().\n\nWhen loading 'foo.py', imp.load_source() uses a pre-compiled\nfile ('foo.pyc' or 'foo.pyo') if its timestamp is not older than\nthat of 'foo.py'. Unfortunately, the timestamps have a resolution\nof seconds on most platforms, so updates made to 'foo.py' within\na second of the imp.load_source() call may or may not be reflected\nin the loaded module -- the behavior is non-deterministic.\n\nThis load_source() replacement deletes a pre-compiled\nfile before calling imp.load_source() if the pre-compiled file's\ntimestamp is less than or equal to the timestamp of path.\n\"\"\"\n", "func_signal": "def load_source(name, path):\n", "code": "if os.path.exists(path):\n    for ending in ('c', 'o'):\n        compiled_path = path+ending\n        if os.path.exists(compiled_path) and \\\n           os.path.getmtime(compiled_path) <= os.path.getmtime(path):\n            os.unlink(compiled_path)\nmod = __import__(name)\nfor p in name.split('.')[1:]:\n    mod = getattr(mod, p)\nreturn reload(mod)", "path": "test_plugins.py", "repo_name": "mmcgill/mc3p", "stars": 32, "license": "gpl-2.0", "language": "python", "size": 491}
{"docstring": "# Map of plugin name to module.\n", "func_signal": "def __init__(self, config, cli_proxy, srv_proxy):\n", "code": "self.__plugins = {}\n\n# Map of instance ID to MC3Plugin instance.\nself.__instances = {}\n\n# True when a successful client-server handshake has completed.\nself.__session_active = False\n\n# Holds the protocol version number after successful handshake.\nself.__proto_version = 0\n\n# Stores handshake messages before handshake has completed,\n# so they can be fed to plugins after initialization.\nself.__msgbuf = []\n\n# For asynchronously injecting messages from the client or server.\nself.__from_client_q = multiprocessing.Queue()\nself.__from_server_q = multiprocessing.Queue()\n\n# Plugin configuration.\nself.__config = config", "path": "mc3p\\plugins.py", "repo_name": "mmcgill/mc3p", "stars": 32, "license": "gpl-2.0", "language": "python", "size": 491}
{"docstring": "\"\"\"Filter msg via the appropriate message handler(s).\n\nReturns True to forward msg on, False to drop it.\nModifications to msg are passed on to the recipient.\n\"\"\"\n", "func_signal": "def filter(self, msg, source):\n", "code": "msgtype = msg['msgtype']\ntry:\n    if not self.default_handler(msg, source):\n        return False\nexcept:\n    logger.error('Error in default handler of plugin %s:\\n%s' % \\\n                 (self.__class__.__name__, traceback.format_exc()))\n    return True\n\ntry:\n    if msgtype in self.__hdlrs:\n        return self.__hdlrs[msgtype](self, msg, source)\n    else:\n        return True\nexcept:\n    hdlr = self.__hdlrs[msgtype]\n    logger.error('Error in handler %s of plugin %s: %s' % \\\n                 (hdlr.__name__, self.__class__.__name__,\n                  traceback.format_exc()))\n    return True", "path": "mc3p\\plugins.py", "repo_name": "mmcgill/mc3p", "stars": 32, "license": "gpl-2.0", "language": "python", "size": 491}
{"docstring": "\"\"\"One-off used to define login message.\n   The login message must be parsed before we know which protocol\n   version is in use, but its format differs across protocol versions.\n   We build a Parsem out of (name,min_version,max_version,Parsem) quads.\n   We assume the first field of the message is an int containing the\n   protocol version. For the remaining fields, min_version and max_version\n   (inclusive) define the range of versions in which the field is present.\n   \"\"\"\n", "func_signal": "def defloginmsg(tuples):\n", "code": "def parse(stream):\n    msg = {'msgtype': 0x01}\n    proto_version = parse_int(stream)\n    msg['proto_version'] = proto_version\n    for (name,parsem,min,max) in map(with_defaults, tuples):\n        if min <= proto_version <= max:\n            msg[name] = parsem.parse(stream)\n    return msg\ndef emit(msg):\n    proto_version = msg['proto_version']\n    pairs = ((name,parsem) for (name,parsem,x,y) in map(with_defaults, tuples)\n                           if x <= proto_version <= y)\n    return ''.join([emit_unsigned_byte(0x01),\n                    emit_int(msg['proto_version']),\n                    ''.join([parsem.emit(msg[name]) for (name,parsem) in pairs])])\nreturn Parsem(parse, emit)", "path": "mc3p\\parsing.py", "repo_name": "mmcgill/mc3p", "stars": 32, "license": "gpl-2.0", "language": "python", "size": 491}
{"docstring": "\"\"\"Parse a single packet out of stream, and return it.\"\"\"\n# read Packet ID\n", "func_signal": "def parse_packet(stream, msg_spec, side):\n", "code": "msgtype = parse_unsigned_byte(stream)\nif not msg_spec[msgtype]:\n    raise UnsupportedPacketException(msgtype)\nlogger.debug(\"%s trying to parse message type %x\" % (side, msgtype))\nmsg_parser = msg_spec[msgtype]\nmsg = msg_parser.parse(stream)\nmsg['raw_bytes'] = stream.packet_finished()\nreturn Message(msg)", "path": "mc3p\\proxy.py", "repo_name": "mmcgill/mc3p", "stars": 32, "license": "gpl-2.0", "language": "python", "size": 491}
{"docstring": "\"\"\"Return a total ordering of instance ids for this msgtype.\"\"\"\n", "func_signal": "def ordering(self, msgtype):\n", "code": "if not msgtype in self.__orderings:\n    return self.ids\nelse:\n    o = list(self.__orderings[msgtype])\n    for id in self.__ids:\n        if not id in o:\n            o.append(id)\n    return o", "path": "mc3p\\plugins.py", "repo_name": "mmcgill/mc3p", "stars": 32, "license": "gpl-2.0", "language": "python", "size": 491}
{"docstring": "\"\"\"Initialize the stream.\"\"\"\n", "func_signal": "def __init__(self):\n", "code": "self.buf = \"\"\nself.i = 0\nself.tot_bytes = 0\nself.wasted_bytes = 0", "path": "mc3p\\util.py", "repo_name": "mmcgill/mc3p", "stars": 32, "license": "gpl-2.0", "language": "python", "size": 491}
{"docstring": "\"\"\"Load or reload plugin pname.\"\"\"\n", "func_signal": "def _load_plugin(self, pname):\n", "code": "try:\n    logger.debug('  Loading %s' % pname)\n    mod = __import__(pname)\n    for p in pname.split('.')[1:]:\n        mod = getattr(mod, p)\n    self.__plugins[pname] = reload(mod)\nexcept Exception as e:\n    logger.error(\"Plugin %s failed to load: %s\" % (pname, str(e)))\n    return", "path": "mc3p\\plugins.py", "repo_name": "mmcgill/mc3p", "stars": 32, "license": "gpl-2.0", "language": "python", "size": 491}
{"docstring": "\"\"\"Read all available bytes, and process as many packets as possible.\n\"\"\"\n", "func_signal": "def handle_read(self):\n", "code": "t = time()\nif self.last_report + 5 < t and self.stream.tot_bytes > 0:\n    self.last_report = t\n    logger.debug(\"%s: total/wasted bytes is %d/%d (%f wasted)\" % (\n         self.side, self.stream.tot_bytes, self.stream.wasted_bytes,\n         100 * float(self.stream.wasted_bytes) / self.stream.tot_bytes))\nself.stream.append(self.recv(4092))\n\nif self.out_of_sync:\n    data = self.stream.read(len(self.stream))\n    self.stream.packet_finished()\n    if self.other_side:\n        self.other_side.send(data)\n    return\n\ntry:\n    packet = parse_packet(self.stream, self.msg_spec, self.side)\n    while packet != None:\n        if packet['msgtype'] == 0x01 and self.side == 'client':\n            # Determine which protocol message definitions to use.\n            proto_version = packet['proto_version']\n            logger.info('Client requests protocol version %d' % proto_version)\n            if not proto_version in messages.protocol:\n                logger.error(\"Unsupported protocol version %d\" % proto_version)\n                self.handle_close()\n                return\n            self.msg_spec, self.other_side.msg_spec = messages.protocol[proto_version]\n        forward = True\n        if self.plugin_mgr:\n            forwarding = self.plugin_mgr.filter(packet, self.side)\n            if forwarding and packet.modified:\n                packet['raw_bytes'] = self.msg_spec[packet['msgtype']].parse(packet)\n        if forwarding and self.other_side:\n            self.other_side.send(packet['raw_bytes'])\n        # Since we know we're at a message boundary, we can inject\n        # any messages in the queue.\n        msgbytes = self.plugin_mgr.next_injected_msg_from(self.side)\n        while self.other_side and msgbytes is not None:\n            self.other_side.send(msgbytes)\n            msgbytes = self.plugin_mgr.next_injected_msg_from(self.side)\n\n        # Attempt to parse the next packet.\n        packet = parse_packet(self.stream,self.msg_spec, self.side)\nexcept PartialPacketException:\n    pass # Not all data for the current packet is available.\nexcept Exception:\n    logger.error(\"MinecraftProxy for %s caught exception, out of sync\" % self.side)\n    logger.error(traceback.format_exc())\n    logger.debug(\"Current stream buffer: %s\" % repr(self.stream.buf))\n    self.out_of_sync = True\n    self.stream.reset()", "path": "mc3p\\proxy.py", "repo_name": "mmcgill/mc3p", "stars": 32, "license": "gpl-2.0", "language": "python", "size": 491}
{"docstring": "\"\"\"Send msg to the server asynchronously.\"\"\"\n", "func_signal": "def to_server(self, msg):\n", "code": "msgbytes = self.__encode_msg('client', msg)\nif msgbytes:\n    self.__to_server.put(msgbytes)", "path": "mc3p\\plugins.py", "repo_name": "mmcgill/mc3p", "stars": 32, "license": "gpl-2.0", "language": "python", "size": 491}
{"docstring": "\"\"\"Return the Queue containing source's messages to be injected.\"\"\"\n", "func_signal": "def next_injected_msg_from(self, source):\n", "code": "if source == 'client':\n    q = self.__from_client_q\nelif source == 'server':\n    q = self.__from_server_q\nelse:\n    raise Exception('Unrecognized source ' + source)\ntry:\n    return q.get(block=False)\nexcept Queue.Empty:\n    return None", "path": "mc3p\\plugins.py", "repo_name": "mmcgill/mc3p", "stars": 32, "license": "gpl-2.0", "language": "python", "size": 491}
{"docstring": "\"\"\"Build a Parsem for a message out of (name,Parsem) pairs.\"\"\"\n", "func_signal": "def defmsg(msgtype, name, pairs):\n", "code": "def parse(stream):\n    msg = {'msgtype': msgtype}\n    for (name,parsem) in pairs:\n        msg[name] = parsem.parse(stream)\n    return msg\ndef emit(msg):\n    return ''.join([emit_unsigned_byte(msgtype),\n                    ''.join([parsem.emit(msg[name]) for (name,parsem) in pairs])])\nreturn Parsem(parse,emit,name)", "path": "mc3p\\parsing.py", "repo_name": "mmcgill/mc3p", "stars": 32, "license": "gpl-2.0", "language": "python", "size": 491}
{"docstring": "\"\"\"Call shutdown handler.\"\"\"\n", "func_signal": "def handle_close(self):\n", "code": "logger.info(\"%s socket closed.\", self.side)\nself.close()\nif self.other_side is not None:\n    logger.info(\"shutting down other side\")\n    self.other_side.other_side = None\n    self.other_side.close()\n    self.other_side = None\n    logger.info(\"shutting down plugin manager\")\n    self.plugin_mgr.destroy()", "path": "mc3p\\proxy.py", "repo_name": "mmcgill/mc3p", "stars": 32, "license": "gpl-2.0", "language": "python", "size": 491}
{"docstring": "\"\"\"Return the subclass of MC3Plugin in pmod.\"\"\"\n", "func_signal": "def _find_plugin_class(self, pname):\n", "code": "pmod = self.__plugins[pname]\nclass_check = lambda c: \\\n    c != MC3Plugin and isinstance(c, type) and issubclass(c, MC3Plugin)\nclasses = filter(class_check, pmod.__dict__.values())\nif len(classes) == 0:\n    logger.error(\"Plugin '%s' does not contain \" % pname + \\\n                 \"a subclass of MC3Plugin\")\n    return None\nelif len(classes) > 1:\n    logger.error((\"Plugin '%s' contains multiple subclasses \" + \\\n                  \"of MC3Plugin: %s\") % \\\n                 (pname, ', '.join([c.__name__ for c in classes])))\nelse:\n    return classes[0]", "path": "mc3p\\plugins.py", "repo_name": "mmcgill/mc3p", "stars": 32, "license": "gpl-2.0", "language": "python", "size": 491}
{"docstring": "\"\"\"Listen on port for client connection, return resulting socket.\"\"\"\n", "func_signal": "def wait_for_client(port):\n", "code": "srvsock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\nsrvsock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\nsrvsock.bind( (\"\", port) )\nsrvsock.listen(1)\nlogger.info(\"mitm_listener bound to %d\" % port)\n(sock, addr) = srvsock.accept()\nsrvsock.close()\nlogger.info(\"mitm_listener accepted connection from %s\" % repr(addr))\nreturn sock", "path": "mc3p\\proxy.py", "repo_name": "mmcgill/mc3p", "stars": 32, "license": "gpl-2.0", "language": "python", "size": 491}
{"docstring": "\"\"\"Free plugin resources.\nOverride in subclass.\"\"\"\n\n", "func_signal": "def destroy(self):\n", "code": "\n\"\"\"Internal cleanup, do not override.\"\"\"\nself.__to_client.close()\nself.__to_server.close()\nself.destroy()", "path": "mc3p\\plugins.py", "repo_name": "mmcgill/mc3p", "stars": 32, "license": "gpl-2.0", "language": "python", "size": 491}
{"docstring": "\"\"\"Proxies one side of a client-server connection.\n\nMinecraftProxy instances are created in pairs that have references to\none another. Since a client initiates a connection, the client side of\nthe pair is always created first, with other_side = None. The creator\nof the client proxy is then responsible for connecting to the server\nand creating a server proxy with other_side=client. Finally, the\nproxy creator should do client_proxy.other_side = server_proxy.\n\"\"\"\n", "func_signal": "def __init__(self, src_sock, other_side=None):\n", "code": "asyncore.dispatcher_with_send.__init__(self, src_sock)\nself.plugin_mgr = None\nself.other_side = other_side\nif other_side == None:\n    self.side = 'client'\n    self.msg_spec = messages.protocol[0][0]\nelse:\n    self.side = 'server'\n    self.msg_spec = messages.protocol[0][1]\n    self.other_side.other_side = self\nself.stream = Stream()\nself.last_report = 0\nself.msg_queue = []\nself.out_of_sync = False", "path": "mc3p\\proxy.py", "repo_name": "mmcgill/mc3p", "stars": 32, "license": "gpl-2.0", "language": "python", "size": 491}
{"docstring": "\"\"\"Configure logging. Can safely be called multiple times.\"\"\"\n", "func_signal": "def config_logging(logfile=None):\n", "code": "global logging_configured\nif not logging_configured:\n    dir = os.path.dirname(os.path.abspath(__file__))\n    if not logfile:\n        logfile = os.path.join(dir, 'logging.conf')\n    if not os.path.exists(logfile):\n        write_default_logging_file(logfile)\n    logging.config.fileConfig(logfile)\n    logging_configured = True", "path": "mc3p\\util.py", "repo_name": "mmcgill/mc3p", "stars": 32, "license": "gpl-2.0", "language": "python", "size": 491}
{"docstring": "\"\"\"Send msg to the client asynchronously.\"\"\"\n", "func_signal": "def to_client(self, msg):\n", "code": "msgbytes = self.__encode_msg('server', msg)\nif msgbytes:\n    self.__to_client.put(msgbytes)", "path": "mc3p\\plugins.py", "repo_name": "mmcgill/mc3p", "stars": 32, "license": "gpl-2.0", "language": "python", "size": 491}
{"docstring": "\"\"\"Like scoreatpercentile but can take and return array of percentiles.\n\nParameters\n----------\na: array\n    data\npcts: sequence of percentile values\n    percentile or percentiles to find score at\n\nReturns\n-------\nscores: array\n    array of scores at requested percentiles\n\"\"\"\n", "func_signal": "def percentiles(a, pcts):\n", "code": "try:\n    scores = np.zeros(len(pcts))\nexcept TypeError:\n    pcts = [pcts]\n    scores = np.zeros(1)\nfor i, p in enumerate(pcts):\n    scores[i] = stats.scoreatpercentile(a, p)\nreturn scores", "path": "utils.py", "repo_name": "mwaskom/Psych216", "stars": 37, "license": "None", "language": "python", "size": 3268}
{"docstring": "\"\"\"Grab current axis and label it.\"\"\"\n", "func_signal": "def axlabel(xlabel, ylabel, **kwargs):\n", "code": "ax = plt.gca()\nax.set_xlabel(xlabel, **kwargs)\nax.set_ylabel(ylabel, **kwargs)", "path": "seaborn\\utils.py", "repo_name": "mwaskom/Psych216", "stars": 37, "license": "None", "language": "python", "size": 3268}
{"docstring": "\"\"\"Set the axis style.\n\nParameters\n----------\nstyle : darkgrid | whitegrid | nogrid\n    Style of axis background.\n\n\"\"\"\n", "func_signal": "def axes_style(style):\n", "code": "grid_params = {\"axes.grid\": True,\n               \"axes.axisbelow\": True}\n\nif style == \"darkgrid\":\n    grid_params.update({\"axes.facecolor\": \"#EAEAF2\",\n                        \"axes.linewidth\": 0,\n                        \"grid.color\": \"w\",\n                        \"grid.linestyle\": \"-\",\n                        \"grid.linewidth\": 1.5})\n    _blank_ticks(grid_params)\n    mpl.rcParams.update(grid_params)\n\nelif style == \"whitegrid\":\n    grid_params.update({\"axes.facecolor\": \"white\",\n                        \"axes.linewidth\": 1,\n                        \"grid.color\": \"#222222\",\n                        \"grid.linestyle\": \":\",\n                        \"grid.linewidth\": .8})\n    _restore_ticks(grid_params)\n    mpl.rcParams.update(grid_params)\n\nelif style == \"nogrid\":\n    params = {\"axes.grid\": False,\n              \"axes.facecolor\": \"white\",\n              \"axes.linewidth\": 1}\n    _restore_ticks(params)\n    mpl.rcParams.update(params)\n\nelse:\n    raise ValueError(\"Style %s not recognized\" % style)", "path": "seaborn\\rcmod.py", "repo_name": "mwaskom/Psych216", "stars": 37, "license": "None", "language": "python", "size": 3268}
{"docstring": "\"\"\"Plot each original data point discretely.\"\"\"\n", "func_signal": "def _plot_obs_points(ax, x, data, color, **kwargs):\n", "code": "if isinstance(color, list):\n    for i, obs in enumerate(data):\n        ax.plot(x, obs, \"o\", color=color[i], alpha=0.8, markersize=4)\nelse:\n    ax.plot(x, data.T, \"o\", color=color, alpha=0.5, markersize=4)", "path": "seaborn\\plotobjs.py", "repo_name": "mwaskom/Psych216", "stars": 37, "license": "None", "language": "python", "size": 3268}
{"docstring": "\"\"\"Resample an array with replacement and calculate a summary stat.\n\nParameters\n----------\na: array\n    data to resample\nn_boot: int\n    number of resamples\nstat_func: callable\n    function to call on each resampled dataset\n\n\"\"\"\n", "func_signal": "def bootstrap(a, n_boot=10000, stat_func=np.mean):\n", "code": "boot_dist = np.zeros(n_boot)\nn = len(a)\nfor i in xrange(n_boot):\n    sample = a[np.random.randint(0, n, n)]\n    boot_dist[i] = stat_func(sample)\nreturn boot_dist", "path": "utils.py", "repo_name": "mwaskom/Psych216", "stars": 37, "license": "None", "language": "python", "size": 3268}
{"docstring": "\"\"\"Upsample over time and plot a KDE of the bootstrap distribution.\"\"\"\n", "func_signal": "def _ts_kde(ax, x, data, color, **kwargs):\n", "code": "kde_data = []\ny_min, y_max = moss.percentiles(data, [1, 99])\ny_vals = np.linspace(y_min, y_max, 100)\nupsampler = interpolate.interp1d(x, data)\ndata_upsample = upsampler(np.linspace(x.min(), x.max(), 100))\nfor pt_data in data_upsample.T:\n    pt_kde = stats.kde.gaussian_kde(pt_data)\n    kde_data.append(pt_kde(y_vals))\nkde_data = np.transpose(kde_data)\nrgb = mpl.colors.ColorConverter().to_rgb(color)\nimg = np.zeros((kde_data.shape[0], kde_data.shape[1], 4))\nimg[:, :, :3] = rgb\nkde_data /= kde_data.max(axis=0)\nkde_data[kde_data > 1] = 1\nimg[:, :, 3] = kde_data\nax.imshow(img, interpolation=\"spline16\", zorder=1,\n          extent=(x.min(), x.max(), y_min, y_max),\n          aspect=\"auto\", origin=\"lower\")", "path": "seaborn\\plotobjs.py", "repo_name": "mwaskom/Psych216", "stars": 37, "license": "None", "language": "python", "size": 3268}
{"docstring": "\"\"\"Return arguments to plt.bar for pmf-like histogram of an array.\n\nParameters\n----------\na: array-like\n    array to make histogram of\nbins: int\n    number of bins\n\nReturns\n-------\nx: array\n    left x position of bars\nh: array\n    height of bars\nw: float\n    width of bars\n\n\"\"\"\n", "func_signal": "def pmf_hist(a, bins=10):\n", "code": "n, x = np.histogram(a, bins)\nh = n / n.sum()\nw = x[1] - x[0]\nreturn x[:-1], h, w", "path": "seaborn\\utils.py", "repo_name": "mwaskom/Psych216", "stars": 37, "license": "None", "language": "python", "size": 3268}
{"docstring": "\"\"\"Reset x and y ticks in a param dict to matplotlib defaults.\"\"\"\n", "func_signal": "def _restore_ticks(params):\n", "code": "for axis in [\"x\", \"y\"]:\n    for step, size in zip([\"major\", \"minor\"], [4, 2]):\n        params[\"%stick.%s.size\" % (axis, step)] = size", "path": "seaborn\\rcmod.py", "repo_name": "mwaskom/Psych216", "stars": 37, "license": "None", "language": "python", "size": 3268}
{"docstring": "\"\"\"Return a fully saturated color with the same hue.\n\nParameters\n----------\ncolor :  matplotlib color\n    hex, rgb-tuple, or html color name\nspace : hsv | hls\n    intermediate color space to max saturation channel\n\nReturns\n-------\nnew_color : rgb tuple\n    saturated color code in RGB tuple representation\n\n\"\"\"\n# Get rgb tuple rep\n", "func_signal": "def saturate(color, space=\"hsv\"):\n", "code": "rgb = mplcol.colorConverter.to_rgb(color)\n\n# Get the parameters to map in and out of hue-based space\nsat_chan, map_in, map_out = _hue_space_params(space)\n\n# Map into the space, desaturate, map back out and return\ninter_rep = list(map_in(*rgb))\ninter_rep[sat_chan] = 1\nnew_color = map_out(*inter_rep)\nreturn new_color", "path": "seaborn\\utils.py", "repo_name": "mwaskom/Psych216", "stars": 37, "license": "None", "language": "python", "size": 3268}
{"docstring": "\"\"\"Set some visual parameters based on intended context.\n\nCurrently just changes font sizes\n\nParameters\n----------\ncontext: notebook | talk | paper\n    Intended context for resulting figures.\n\n\"\"\"\n", "func_signal": "def context_setting(context):\n", "code": "if context == \"talk\":\n    params = {\"axes.labelsize\": 17,\n              \"axes.titlesize\": 19,\n              \"xtick.labelsize\": 16,\n              \"ytick.labelsize\": 16,\n              \"legend.fontsize\": 14,\n              }\n\nelif context == \"notebook\":\n    params = {\"axes.labelsize\": 11,\n              \"axes.titlesize\": 12,\n              \"xtick.labelsize\": 10,\n              \"ytick.labelsize\": 10,\n              }\n\nelif context == \"poster\":\n    params = {\"axes.labelsize\": 18,\n              \"axes.titlesize\": 22,\n              \"xtick.labelsize\": 16,\n              \"ytick.labelsize\": 16,\n              }\n\nelif context == \"paper\":\n    params = {\"axes.labelsize\": 10,\n              \"axes.titlesize\": 13,\n              \"xtick.labelsize\": 10,\n              \"ytick.labelsize\": 10,\n              }\n\nelse:\n    raise ValueError(\"Context %s is not recognized\" % context)\n\nmpl.rcParams.update(params)", "path": "seaborn\\rcmod.py", "repo_name": "mwaskom/Psych216", "stars": 37, "license": "None", "language": "python", "size": 3268}
{"docstring": "\"\"\"Establish support for a kernel density estimate.\"\"\"\n", "func_signal": "def _kde_support(a, kde, npts, thresh=1e-4):\n", "code": "min = a.min()\nmax = a.max()\nrange = max - min\nx = np.linspace(min - range, max + range, npts * 2)\ny = kde(x)\nmask = y > y.max() * thresh\nreturn x[mask]", "path": "seaborn\\plotobjs.py", "repo_name": "mwaskom/Psych216", "stars": 37, "license": "None", "language": "python", "size": 3268}
{"docstring": "\"\"\"Decrease the saturation channel of a color by some percent.\n\nParameters\n----------\ncolor : matplotlib color\n    hex, rgb-tuple, or html color name\npct : float\n    saturation channel of color will be multiplied by this value\nspace : hsv | hls\n    intermediate color space to max saturation channel\n\nReturns\n-------\nnew_color : rgb tuple\n    desaturated color code in RGB tuple representation\n\n\"\"\"\n# Check inputs\n", "func_signal": "def desaturate(color, pct, space=\"hsv\"):\n", "code": "if not 0 < pct < 1:\n    raise ValueError(\"Pct must be between 0 and 1\")\n\n# Get rgb tuple rep\nrgb = mplcol.colorConverter.to_rgb(color)\n\n# Get the parameters to map in and out of hue-based space\nsat_chan, map_in, map_out = _hue_space_params(space)\n\n# Map into the space, desaturate, map back out and return\ninter_rep = list(map_in(*rgb))\ninter_rep[sat_chan] *= pct\nnew_color = map_out(*inter_rep)\nreturn new_color", "path": "seaborn\\utils.py", "repo_name": "mwaskom/Psych216", "stars": 37, "license": "None", "language": "python", "size": 3268}
{"docstring": "\"\"\"Get a set of evenly spaced colors in HLS hue space.\n\nParameters\n----------\n\nn_colors : int\n    number of colors in the palette\nh : float\n    first hue\nl : float\n    lightness\ns : float\n    saturation\n\nReturns\n-------\npalette : list of tuples\n    color palette\n\n\"\"\"\n", "func_signal": "def hls_palette(n_colors=6, h=.01, l=.6, s=.65):\n", "code": "hues = np.linspace(0, 1, n_colors + 1)[:-1]\nhues += h\nhues -= hues.astype(int)\npalette = [colorsys.hls_to_rgb(h_i, l, s) for h_i in hues]\nreturn palette", "path": "seaborn\\utils.py", "repo_name": "mwaskom/Psych216", "stars": 37, "license": "None", "language": "python", "size": 3268}
{"docstring": "\"\"\"Set new RC params in one step.\"\"\"\n", "func_signal": "def set(context=\"notebook\", style=\"darkgrid\", palette=\"deep\"):\n", "code": "context_setting(context)\naxes_style(style)\nset_color_palette(palette)\nparams = {\"figure.figsize\": (8, 5.5),\n          \"lines.linewidth\": 1.4,\n          \"patch.linewidth\": .3}\nmpl.rcParams.update(params)", "path": "seaborn\\rcmod.py", "repo_name": "mwaskom/Psych216", "stars": 37, "license": "None", "language": "python", "size": 3268}
{"docstring": "\"\"\"Return matplotlib color codes for a given palette.\n\nParameters\n----------\nname: None or string\n    Name of palette or None to return current color list\nn_colors : int\n    number of colors in the palette\ndesat : float\n    desaturation factor for each color\nh : float\n    first hue\nl : float\n    lightness\ns : float\n    saturation\n\nReturns\n-------\npalette : list of colors\n    color palette\n\n\"\"\"\n", "func_signal": "def color_palette(name=None, n_colors=8, desat=None, h=.01, l=.6, s=.65):\n", "code": "if name is None:\n    return mpl.rcParams[\"axes.color_cycle\"]\n\npalettes = dict(\n    default=[\"b\", \"g\", \"r\", \"c\", \"m\", \"y\", \"k\"],\n    pastel=[\"#92C6FF\", \"#97F0AA\", \"#FF9F9A\", \"#D0BBFF\", \"#FFFEA3\"],\n    bright=[\"#003FFF\", \"#03ED3A\", \"#E8000B\", \"#00D7FF\", \"#FFB400\"],\n    muted=[\"#4878CF\", \"#6ACC65\", \"#D65F5F\", \"#B47CC7\", \"#C4AD66\"],\n    deep=[\"#4C72B0\", \"#55A868\", \"#C44E52\", \"#8172B2\", \"#CCB974\"],\n    dark=[\"#001C7F\", \"#017517\", \"#8C0900\", \"#7600A1\", \"#007364\"],\n    colorblind=[\"#0072B2\", \"#009E73\", \"#D55E00\", \"#F0E442\",\n                \"#CC79A7\", \"#56B4E9\", \"#E69F00\"],\n)\n\nif name == \"hls\":\n    palette = hls_palette(n_colors, h, l, s)\nelse:\n    try:\n        palette =  palettes[name]\n    except KeyError:\n        bins = np.linspace(0, 1, n_colors + 2)[1:-1]\n        cmap = getattr(mpl.cm, name)\n        palette = map(tuple, cmap(bins)[:, :3])\n    except KeyError:\n        raise ValueError(\"%s is not a valid palette name\" % name)\n\nif desat is not None:\n    palette = [desaturate(c, desat) for c in palette]\n\nreturn palette", "path": "seaborn\\utils.py", "repo_name": "mwaskom/Psych216", "stars": 37, "license": "None", "language": "python", "size": 3268}
{"docstring": "\"\"\"Plot translucent error bands around the central tendancy.\"\"\"\n", "func_signal": "def _plot_ci_band(ax, x, ci, color, **kwargs):\n", "code": "low, high = ci\nax.fill_between(x, low, high, color=color, alpha=0.2)", "path": "seaborn\\plotobjs.py", "repo_name": "mwaskom/Psych216", "stars": 37, "license": "None", "language": "python", "size": 3268}
{"docstring": "\"\"\"Plot datapoints in an array as sticks on an axis.\"\"\"\n", "func_signal": "def rugplot(a, height=None, axis=\"x\", ax=None, **kwargs):\n", "code": "if ax is None:\n    ax = plt.subplot(111)\nother_axis = dict(x=\"y\", y=\"x\")[axis]\nmin, max = getattr(ax, \"get_%slim\" % other_axis)()\nif height is None:\n    range = max - min\n    height = range * .05\nif axis == \"x\":\n    ax.plot([a, a], [min, min + height], **kwargs)\nelse:\n    ax.plot([min, min + height], [a, a], **kwargs)\nreturn ax", "path": "seaborn\\plotobjs.py", "repo_name": "mwaskom/Psych216", "stars": 37, "license": "None", "language": "python", "size": 3268}
{"docstring": "\"\"\"Plot a trace for each observation in the original data.\"\"\"\n", "func_signal": "def _plot_obs_traces(ax, x, data, ci, color, **kwargs):\n", "code": "if isinstance(color, list):\n    for i, obs in enumerate(data):\n        ax.plot(x, obs, color=color[i], alpha=0.5)\nelse:\n    ax.plot(x, data.T, color=color, alpha=0.2)", "path": "seaborn\\plotobjs.py", "repo_name": "mwaskom/Psych216", "stars": 37, "license": "None", "language": "python", "size": 3268}
{"docstring": "\"\"\"Turn off x and y ticks in a param dict (but not labels).\"\"\"\n", "func_signal": "def _blank_ticks(params):\n", "code": "for axis in [\"x\", \"y\"]:\n    for step in [\"major\", \"minor\"]:\n        params[\"%stick.%s.size\" % (axis, step)] = 0", "path": "seaborn\\rcmod.py", "repo_name": "mwaskom/Psych216", "stars": 37, "license": "None", "language": "python", "size": 3268}
{"docstring": "\"\"\"Get parameters to go in and out of hue-based color space.\"\"\"\n", "func_signal": "def _hue_space_params(space):\n", "code": "try:\n    sat_chan = dict(hsv=1, hls=2)[space]\nexcept KeyError:\n    raise ValueError(space + \" is not a valid space value\")\n\n# Get the right function to map into a space with a\n# saturation channel\nmap_in = getattr(colorsys, \"rgb_to_\" + space)\nmap_out = getattr(colorsys, space + \"_to_rgb\")\n\nreturn sat_chan, map_in, map_out", "path": "seaborn\\utils.py", "repo_name": "mwaskom/Psych216", "stars": 37, "license": "None", "language": "python", "size": 3268}
{"docstring": "\"\"\"Similar to `_force_list` but always succeeds and never drops data.\"\"\"\n", "func_signal": "def _to_list(value):\n", "code": "if value is None:\n    return []\nif isinstance(value, basestring):\n    return [value]\ntry:\n    return list(value)\nexcept TypeError:\n    return [value]", "path": "fungiform\\utils.py", "repo_name": "mitsuhiko/fungiform", "stars": 44, "license": "other", "language": "python", "size": 138}
{"docstring": "\"\"\"Joins a name.\"\"\"\n", "func_signal": "def make_name(parent, child):\n", "code": "if parent is None:\n    result = child\nelse:\n    result = '%s.%s' % (parent, child)\n\n# try to return a ascii only bytestring if possible\ntry:\n    return str(result)\nexcept UnicodeError:\n    return unicode(result)", "path": "fungiform\\utils.py", "repo_name": "mitsuhiko/fungiform", "stars": 44, "license": "other", "language": "python", "size": 138}
{"docstring": "\"\"\"A helper to fill the dict passed with the items passed as keyword\narguments if they are not yet in the dict.  If the dict passed was\n`None` a new dict is created and returned.\n\nThis can be used to prepopulate initial dicts in overriden constructors:\n\n    class MyForm(forms.Form):\n        foo = forms.TextField()\n        bar = forms.TextField()\n\n        def __init__(self, initial=None):\n            forms.Form.__init__(self, forms.fill_dict(initial,\n                foo=\"nothing\",\n                bar=\"nothing\"\n            ))\n\"\"\"\n", "func_signal": "def fill_dict(_dict, **kwargs):\n", "code": "if _dict is None:\n    return kwargs\nfor key, value in kwargs.iteritems():\n    if key not in _dict:\n        _dict[key] = value\nreturn _dict", "path": "fungiform\\utils.py", "repo_name": "mitsuhiko/fungiform", "stars": 44, "license": "other", "language": "python", "size": 138}
{"docstring": "\"\"\"Convert a datetime object to the local timezone.\"\"\"\n", "func_signal": "def to_timezone(datetime, tzinfo=UTC):\n", "code": "if datetime.tzinfo is None:\n    datetime = datetime.replace(tzinfo=UTC)\nreturn tzinfo.normalize(datetime.astimezone(tzinfo))", "path": "fungiform\\utils.py", "repo_name": "mitsuhiko/fungiform", "stars": 44, "license": "other", "language": "python", "size": 138}
{"docstring": "\"\"\"A handy helper function that recreates the full URL for the current\nrequest.\n\n:param environ: the WSGI environment to get the current URL from.\n\"\"\"\n", "func_signal": "def get_current_url(environ):\n", "code": "tmp = [environ['wsgi.url_scheme'], '://', get_host(environ)]\ncat = tmp.append\ncat(urllib.quote(environ.get('SCRIPT_NAME', '').rstrip('/')))\ncat(urllib.quote('/' + environ.get('PATH_INFO', '').lstrip('/')))\nqs = environ.get('QUERY_STRING')\nif qs:\n    cat('?' + qs)\nreturn ''.join(tmp)", "path": "fungiform\\utils.py", "repo_name": "mitsuhiko/fungiform", "stars": 44, "license": "other", "language": "python", "size": 138}
{"docstring": "\"\"\"If the value is not a dict, raise an exception.\"\"\"\n", "func_signal": "def _force_dict(value):\n", "code": "if value is None or not isinstance(value, dict):\n    return {}\nreturn value", "path": "fungiform\\utils.py", "repo_name": "mitsuhiko/fungiform", "stars": 44, "license": "other", "language": "python", "size": 138}
{"docstring": "# compare both type and value\n", "func_signal": "def assertEqual(self, first, second, msg=None):\n", "code": "self.assertEq(first, second, msg=msg)\nself.assertEq(type(first), type(second))", "path": "fungiform\\tests\\utils.py", "repo_name": "mitsuhiko/fungiform", "stars": 44, "license": "other", "language": "python", "size": 138}
{"docstring": "\"\"\"Parses a string into a date object.\"\"\"\n# shortcut: string as None or \"today\" returns the current date.\n", "func_signal": "def parse_date(string, date_formats=None):\n", "code": "if string is None or string.lower() in ('today',):\n    return date.today()\n\ndef convert(format):\n    \"\"\"Helper that parses the string.\"\"\"\n    return date(*strptime(string, format)[:3])\n\n# first of all try the ISO 8601 format.\ntry:\n    return convert(u'%Y-%m-%d')\nexcept ValueError:\n    pass\n\n# now try various types of date\nfor fmt in date_formats or DATE_FORMATS:\n    try:\n        return convert(fmt)\n    except ValueError:\n        pass\n\nraise ValueError('invalid date format')", "path": "fungiform\\utils.py", "repo_name": "mitsuhiko/fungiform", "stars": 44, "license": "other", "language": "python", "size": 138}
{"docstring": "\"\"\"Replace special characters \"&\", '\"', \"<\" and \">\" to HTML-safe sequences.\n\nThere is a special handling for `None` which escapes to an empty string.\n\n:param s: the string to escape.\n:param quote: set to true to also escape double quotes.\n\"\"\"\n", "func_signal": "def escape(s):\n", "code": "if s is None:\n    return ''\nelif hasattr(s, '__html__'):\n    return s.__html__()\nelif not isinstance(s, basestring):\n    s = unicode(s)\nreturn Markup(s.replace(u'&', u'&amp;').replace(u'<', u'&lt;')\n               .replace(u'>', u'&gt;').replace(u'\"', u'&#34;'))", "path": "fungiform\\utils.py", "repo_name": "mitsuhiko/fungiform", "stars": 44, "license": "other", "language": "python", "size": 138}
{"docstring": "\"\"\"Return the timezone for the given identifier.\"\"\"\n", "func_signal": "def get_timezone(tzinfo=None):\n", "code": "if tzinfo is None:\n    return UTC\nif isinstance(tzinfo, basestring):\n    return timezone(tzinfo)\n# Return the object itself: it's probably a tzinfo object\nreturn tzinfo", "path": "fungiform\\utils.py", "repo_name": "mitsuhiko/fungiform", "stars": 44, "license": "other", "language": "python", "size": 138}
{"docstring": "\"\"\"Helper for various string-wrapped functions.\"\"\"\n", "func_signal": "def _escape_argspec(obj, iterable):\n", "code": "for key, value in iterable:\n    if hasattr(value, '__html__') or isinstance(value, basestring):\n        obj[key] = escape(value)\nreturn obj", "path": "fungiform\\utils.py", "repo_name": "mitsuhiko/fungiform", "stars": 44, "license": "other", "language": "python", "size": 138}
{"docstring": "\"\"\"A helper that groups an ``(key, value)`` iterable by key and\naccumultates the values in a list.  Used to support webob like dicts in\nthe form data decoder.\n\"\"\"\n", "func_signal": "def _iter_key_grouped(iterable):\n", "code": "last_key = None\nbuffered = []\nfor key, value in sorted(iterable, key=lambda x: x[0]):\n    if key != last_key and buffered:\n        yield last_key, buffered\n        buffered = []\n    last_key = key\n    buffered.append(value)\nif buffered:\n    yield last_key, buffered", "path": "fungiform\\utils.py", "repo_name": "mitsuhiko/fungiform", "stars": 44, "license": "other", "language": "python", "size": 138}
{"docstring": "\"\"\"Parses a string into a datetime object.  Per default a conversion\nfrom the local timezone to UTC is performed but returned as naive\ndatetime object (that is tzinfo being None).  If tzinfo is not used,\nthe string is expected in UTC.\n\nThe return value is **always** a naive datetime object in UTC.  This\nfunction should be considered of a lenient counterpart of\n`format_system_datetime`.\n\"\"\"\n# shortcut: string as None or \"now\" returns the current timestamp.\n", "func_signal": "def parse_datetime(string, tzinfo=None, date_formats=None, time_formats=None):\n", "code": "if string is None or string.lower() in ('now',):\n    return datetime.utcnow().replace(microsecond=0)\n\ndef convert(format):\n    \"\"\"Helper that parses the string and convers the timezone.\"\"\"\n    rv = datetime(*strptime(string, format)[:7])\n    if tzinfo:\n        rv = to_utc(rv, tzinfo)\n    return rv.replace(microsecond=0)\n\n# first of all try the following format because this is the format\n# Texpress will output by default for any date time string in the\n# administration panel.\ntry:\n    return convert(u'%Y-%m-%d %H:%M')\nexcept ValueError:\n    pass\n\nif not time_formats:\n    time_formats = TIME_FORMATS\n# no go with time only, and current day\nfor fmt in time_formats:\n    try:\n        val = convert(fmt)\n    except ValueError:\n        continue\n    return to_utc(datetime.utcnow().replace(hour=val.hour,\n                  minute=val.minute, second=val.second, microsecond=0),\n                  tzinfo=tzinfo)\n\n# now try various types of date + time strings\ndef combined():\n    for t_fmt in time_formats:\n        for d_fmt in date_formats:\n            yield t_fmt + ' ' + d_fmt\n            yield d_fmt + ' ' + t_fmt\n\nif not date_formats:\n    date_formats = DATE_FORMATS\nfor fmt in combined():\n    try:\n        return convert(fmt)\n    except ValueError:\n        pass\n\nraise ValueError('invalid date format')", "path": "fungiform\\utils.py", "repo_name": "mitsuhiko/fungiform", "stars": 44, "license": "other", "language": "python", "size": 138}
{"docstring": "\"\"\"If the value is not a list, make it one.\"\"\"\n", "func_signal": "def _force_list(value):\n", "code": "if value is None:\n    return []\ntry:\n    if isinstance(value, basestring):\n        raise TypeError()\n    return list(value)\nexcept TypeError:\n    return [value]", "path": "fungiform\\utils.py", "repo_name": "mitsuhiko/fungiform", "stars": 44, "license": "other", "language": "python", "size": 138}
{"docstring": "\"\"\"Set all the fields on obj with data if changed.\"\"\"\n", "func_signal": "def set_fields(obj, data, *fields):\n", "code": "for field in fields:\n    value = data[field]\n    if getattr(obj, field) != value:\n        setattr(obj, field, value)", "path": "fungiform\\utils.py", "repo_name": "mitsuhiko/fungiform", "stars": 44, "license": "other", "language": "python", "size": 138}
{"docstring": "\"\"\"Convert a value to unicode, None means empty string.\"\"\"\n", "func_signal": "def _to_string(value):\n", "code": "if value is None:\n    return u''\nreturn unicode(value)", "path": "fungiform\\utils.py", "repo_name": "mitsuhiko/fungiform", "stars": 44, "license": "other", "language": "python", "size": 138}
{"docstring": "\"\"\"Decodes the flat dictionary d into a nested structure.\n\n>>> decode_form_data({'foo': 'bar'})\n{'foo': 'bar'}\n>>> decode_form_data({'foo.0': 'bar', 'foo.1': 'baz'})\n{'foo': ['bar', 'baz']}\n>>> data = decode_form_data({'foo.bar': '1', 'foo.baz': '2'})\n>>> data == {'foo': {'bar': '1', 'baz': '2'}}\nTrue\n\nMore complex mappings work too:\n\n>>> decode_form_data({'foo.bar.0': 'baz', 'foo.bar.1': 'buzz'})\n{'foo': {'bar': ['baz', 'buzz']}}\n>>> decode_form_data({'foo.0.bar': '23', 'foo.1.baz': '42'})\n{'foo': [{'bar': '23'}, {'baz': '42'}]}\n>>> decode_form_data({'foo.0.0': '23', 'foo.0.1': '42'})\n{'foo': [['23', '42']]}\n>>> decode_form_data({'foo': ['23', '42']})\n{'foo': ['23', '42']}\n\n_missing items in lists are ignored for convenience reasons:\n\n>>> decode_form_data({'foo.42': 'a', 'foo.82': 'b'})\n{'foo': ['a', 'b']}\n\nThis can be used for help client side DOM processing (inserting and\ndeleting rows in dynamic forms).\n\nIt also supports werkzeug's and django's multi dicts (or a dict like\nobject that provides an `iterlists`):\n\n>>> class MultiDict(dict):\n...     def iterlists(self):\n...         for key, values in dict.iteritems(self):\n...             yield key, list(values)\n>>> decode_form_data(MultiDict({\"foo\": ['1', '2']}))\n{'foo': ['1', '2']}\n>>> decode_form_data(MultiDict({\"foo.0\": '1', \"foo.1\": '2'}))\n{'foo': ['1', '2']}\n\nThose two submission ways can also be used combined:\n\n>>> decode_form_data(MultiDict({\"foo\": ['1'], \"foo.0\": '2', \"foo.1\": '3'}))\n{'foo': ['1', '2', '3']}\n\nThis function will never raise exceptions except for argument errors\nbut the recovery behavior for invalid form data is undefined.\n\"\"\"\n", "func_signal": "def decode_form_data(data):\n", "code": "list_marker = object()\nvalue_marker = object()\n\nif hasattr(data, 'iterlists'):\n    listiter = data.iterlists()\nelse:\n    if type(data) is dict:\n        listiter = data.iteritems()\n    else:\n        listiter = _iter_key_grouped(data.items())\n    listiter = ((k, not isinstance(v, (list, tuple)) and [v] or v)\n                for k, v in listiter)\n\ndef _split_key(name):\n    result = name.split('.')\n    for idx, part in enumerate(result):\n        if part.isdigit():\n            result[idx] = int(part)\n    return result\n\ndef _enter_container(container, key):\n    if key not in container:\n        return container.setdefault(key, {list_marker: False})\n    return container[key]\n\ndef _convert(container):\n    if value_marker in container:\n        force_list = False\n        values = container.pop(value_marker)\n        if container.pop(list_marker):\n            force_list = True\n            values.extend(_convert(x[1]) for x in\n                          sorted(container.items()))\n        if not force_list and len(values) == 1:\n            values = values[0]\n        return values\n    elif container.pop(list_marker):\n        return [_convert(x[1]) for x in sorted(container.items())]\n    return dict((k, _convert(v)) for k, v in container.iteritems())\n\nresult = {list_marker: False}\nfor key, values in listiter:\n    parts = _split_key(key)\n    if not parts:\n        continue\n    container = result\n    for part in parts:\n        last_container = container\n        container = _enter_container(container, part)\n        last_container[list_marker] = isinstance(part, (int, long))\n    container[value_marker] = values\n\nreturn _convert(result)", "path": "fungiform\\utils.py", "repo_name": "mitsuhiko/fungiform", "stars": 44, "license": "other", "language": "python", "size": 138}
{"docstring": "\"\"\"Return the real host for the given WSGI environment.  This takes care\nof the `X-Forwarded-Host` header.\n\n:param environ: the WSGI environment to get the host of.\n\"\"\"\n", "func_signal": "def get_host(environ):\n", "code": "if 'HTTP_X_FORWARDED_HOST' in environ:\n    return environ['HTTP_X_FORWARDED_HOST']\nelif 'HTTP_HOST' in environ:\n    return environ['HTTP_HOST']\nresult = environ['SERVER_NAME']\nif (environ['wsgi.url_scheme'], environ['SERVER_PORT']) not \\\n   in (('https', '443'), ('http', '80')):\n    result += ':' + environ['SERVER_PORT']\nreturn result", "path": "fungiform\\utils.py", "repo_name": "mitsuhiko/fungiform", "stars": 44, "license": "other", "language": "python", "size": 138}
{"docstring": "\"\"\"Make something unicode if it's not a string or respond to `__html__`\"\"\"\n", "func_signal": "def soft_unicode(s):\n", "code": "if hasattr(s, '__html__'):\n    return s\nelif not isinstance(s, basestring):\n    return unicode(s)\nreturn s", "path": "fungiform\\utils.py", "repo_name": "mitsuhiko/fungiform", "stars": 44, "license": "other", "language": "python", "size": 138}
{"docstring": "\"\"\"Formats a system datetime.\n\n(Format: YYYY-MM-DD hh:mm and in the user timezone\nif tzinfo is provided)\n\"\"\"\n", "func_signal": "def format_system_datetime(datetime=None, tzinfo=None):\n", "code": "if tzinfo:\n    datetime = to_timezone(datetime, tzinfo)\nreturn u'%d-%02d-%02d %02d:%02d' % (\n    datetime.year,\n    datetime.month,\n    datetime.day,\n    datetime.hour,\n    datetime.minute\n)", "path": "fungiform\\utils.py", "repo_name": "mitsuhiko/fungiform", "stars": 44, "license": "other", "language": "python", "size": 138}
{"docstring": "\"\"\"Format the complete environment $VARIABLE setting string.\"\"\"\n\n", "func_signal": "def stringify_env_var(var):\n", "code": "key = result = '$%s' % var\nfor value, behaviour, sep in env.get(key, []):\n    if behaviour == 'append':\n        result = result + sep + '\"' + value + '\"'\n    elif behaviour == 'prepend':\n        result = '\"' + value + '\"' + sep + result\n    else:\n        result = '\"' + value + '\"'\nreturn \"%s=%s\" % (var, result)", "path": "bolt\\fabcode.py", "repo_name": "tav/bolt-python", "stars": 33, "license": "other", "language": "python", "size": 165}
{"docstring": "\"\"\"Handle the bolt command line call.\"\"\"\n\n", "func_signal": "def main(argv=None):\n", "code": "if argv is None:\n    argv = sys.argv[1:]\n\nop = OptionParser(\n    usage=\"bolt <command-1> <command-2> ... [options]\",\n    )\n\nop.add_option(\n    '-v', '--version', action='store_true', default=False,\n    help=\"show program's version number and exit\"\n    )\n\nop.add_option(\n    '-f', dest='file', default=\"Boltfile\",\n    help=\"set the name or path of the bolt file [Boltfile]\"\n    )\n\nop.add_option(\n    '-d', dest='defaults_file', default=_rc_path(),\n    help=\"set the path of the defaults file [~/.bolt.yaml]\"\n    )\n\nop.add_option(\n    '-i',  dest='identity', action='append', default=None,\n    help=\"path to SSH private key file(s) -- may be repeated\"\n    )\n\nop.add_option(\n    '--hide', metavar='LEVELS',\n    help=\"comma-separated list of output levels to hide\"\n    )\n\nop.add_option(\n    '--show', metavar='LEVELS',\n    help=\"comma-separated list of output levels to show\"\n    )\n\nop.add_option(\n    '--disable', metavar='HOOKS',\n    help=\"comma-separated list of hooks to disable\"\n    )\n\nop.add_option(\n    '--enable', metavar='HOOKS',\n    help=\"comma-separated list of hooks to enable\"\n    )\n\nop.add_option(\n    '--list', action='store_true', default=False,\n    help=\"show the list of available tasks and exit\"\n    )\n\nop.add_option(\n    '--no-pty', action='store_true', default=False,\n    help=\"do not use pseudo-terminal in run/sudo\"\n    )\n\noptions, args = op.parse_args(argv)\nsetup_defaults(options.defaults_file)\n\n# Load the Boltfile.\nrunner = init_task_runner(options.file, getcwd())\n\n# Autocompletion support.\nautocomplete_items = runner.tasks.keys()\nif 'autocomplete' in env:\n    autocomplete_items += env.autocomplete\n\nautocomplete(op, ListCompleter(autocomplete_items))\n\nif options.version:\n    print(\"bolt %s\" % __version__)\n    sys.exit()\n\nif options.no_pty:\n    env.always_use_pty = False\n\nif options.identity:\n    env.key_filename = options.identity\n\nsplit_string = lambda s: filter(None, map(str.strip, s.split(',')))\n\n# Handle output levels.\nif options.show:\n    for level in split_string(options.show):\n        output[level] = True\n\nif options.hide:\n    for level in split_string(options.hide):\n        output[level] = False\n\nif output.debug:\n    print(\"Using Boltfile: %s\" % runner.path)\n\n# Handle hooks related options.\nif options.disable:\n    for hook in split_string(options.disable):\n        DISABLED_HOOKS.append(hook)\n\nif options.enable:\n    for hook in split_string(options.enable):\n        ENABLED_HOOKS.append(hook)\n\nif options.list:\n    print('\\n'.join(sorted(runner.tasks)))\n    sys.exit()\n\ntasks = []\nidx = 0\n\n# Parse command line arguments.\nfor task in args:\n\n    # Initialise variables.\n    _args = []\n    _kwargs = {}\n    _ctx = None\n\n    # Handle +env flags.\n    if task.startswith('+'):\n        if ':' in task:\n            name, value = task[1:].split(':', 1)\n            env[name] = value\n        else:\n            env[task[1:]] = True\n        continue\n\n    # Handle @context specifiers.\n    if task.startswith('@'):\n        if not idx:\n            continue\n        ctx = (task[1:],)\n        existing = tasks[idx-1][3]\n        if existing:\n            new = list(existing)\n            new.extend(ctx)\n            ctx = tuple(new)\n        tasks[idx-1][3] = ctx\n        continue\n\n    # Handle tasks with parameters.\n    if ':' in task:\n        task, argstr = task.split(':', 1)\n        for pair in _escape_split(',', argstr):\n            k, _, v = pair.partition('=')\n            if _:\n                _kwargs[k] = v\n            else:\n                _args.append(k)\n\n    idx += 1\n    task_name = task.replace('_', '-')\n\n    if task_name not in runner.tasks:\n        abort(\"Task not found:\\n\\n%s\" % indent(task))\n\n    tasks.append([task_name, _args, _kwargs, _ctx])\n\nif not tasks:\n    runner.display_listing()\n    sys.exit()\n\nrunner.run(tasks)", "path": "bolt\\core.py", "repo_name": "tav/bolt-python", "stars": 33, "license": "other", "language": "python", "size": 165}
{"docstring": "\"\"\"Print the given warning ``msg``.\"\"\"\n\n", "func_signal": "def warn(msg):\n", "code": "if output.warnings:\n    msg = \"\\nWarning: %s\\n\" % msg\n    if env.colors:\n        print >> sys.stderr, env.color_settings['warn'](msg)\n    else:\n        print >> sys.stderr, msg", "path": "bolt\\fabcode.py", "repo_name": "tav/bolt-python", "stars": 33, "license": "other", "language": "python", "size": 165}
{"docstring": "\"\"\"Execute the various tasks given in the spec list.\"\"\"\n\n", "func_signal": "def run(self, spec):\n", "code": "try:\n\n    if output.debug:\n        names = \", \".join(info[0] for info in spec)\n        print(\"Tasks to run: %s\" % names)\n\n    call_hooks('commands.before', self.tasks, spec)\n\n    # Initialise the default stage if none are given as the first task.\n    if 'stages' in env:\n        if spec[0][0] not in env.stages:\n            self.execute_task(env.stages[0], (), {}, None)\n        else:\n            self.execute_task(*spec.pop(0))\n\n    # Load the config YAML file if specified.\n    if env.config_file:\n        config_path = realpath(expanduser(env.config_file))\n        config_path = join(self.directory, config_path)\n        config_file = open(config_path, 'rb')\n        config = load_yaml(config_file.read())\n        if not config:\n            env.config = AttrDict()\n        elif not isinstance(config, dict):\n            abort(\"Invalid config file found at %s\" % config_path)\n        else:\n            env.config = AttrDict(config)\n        config_file.close()\n\n    call_hooks('config.loaded')\n\n    # Execute the tasks in order.\n    for info in spec:\n        self.execute_task(*info)\n\n    if output.status:\n        msg = \"\\nDone.\"\n        if env.colors:\n            msg = env.color_settings['finish'](msg)\n        print(msg)\n\nexcept SystemExit:\n    raise\nexcept KeyboardInterrupt:\n    if output.status:\n        msg = \"\\nStopped.\"\n        if env.colors:\n            msg = env.color_settings['finish'](msg)\n        print >> sys.stderr, msg\n    sys.exit(1)\nexcept:\n    sys.excepthook(*sys.exc_info())\n    sys.exit(1)\nfinally:\n    call_hooks('commands.after')\n    disconnect_all()", "path": "bolt\\core.py", "repo_name": "tav/bolt-python", "stars": 33, "license": "other", "language": "python", "size": 165}
{"docstring": "\"\"\"run the sudoed command on remote hosts\"\"\"\n\n", "func_signal": "def builtin_sudo(spec, arg):\n", "code": "return sudo(\n    arg, spec.shell, spec.pty, spec.combine_stderr, None, spec.dir,\n    spec.format\n    )", "path": "bolt\\core.py", "repo_name": "tav/bolt-python", "stars": 33, "license": "other", "language": "python", "size": 165}
{"docstring": "\"\"\"Run a command on the local system.\"\"\"\n\n", "func_signal": "def local(command, capture=False, dir=None, format=True):\n", "code": "if format:\n    command = command.format(**env)\n\ngiven_command = command\nwrapped_command = _prefix_env_vars(_prefix_commands(command, 'local', dir))\nprefix = \"[localhost]\"\nif env.colors:\n    prefix = env.color_settings['host_prefix'](prefix)\n\nif output.debug:\n    print(\"%s local: %s\" % (prefix, wrapped_command))\nelif output.running:\n    print(\"%s local: %s\" % (prefix, given_command))\n\ndev_null = None\nif capture:\n    out_stream = PIPE\n    err_stream = PIPE\nelse:\n    dev_null = open(devnull, 'w+')\n    out_stream = None if output.stdout else dev_null\n    err_stream = None if output.stderr else dev_null\ntry:\n    cmd_arg = wrapped_command if win32 else [wrapped_command]\n    p = Popen(cmd_arg, shell=True, stdout=out_stream, stderr=err_stream)\n    (stdout, stderr) = p.communicate()\nfinally:\n    if dev_null is not None:\n        dev_null.close()\n\nout = AttrString(stdout.strip() if stdout else \"\")\nerr = AttrString(stderr.strip() if stderr else \"\")\nout.failed = False\nout.return_code = p.returncode\nout.stderr = err\n\nif p.returncode != 0:\n    out.failed = True\n    msg = \"local() encountered an error (return code %s) while executing '%s'\" % (p.returncode, command)\n    _handle_failure(message=msg)\n\nout.succeeded = not out.failed\nreturn out", "path": "bolt\\fabcode.py", "repo_name": "tav/bolt-python", "stars": 33, "license": "other", "language": "python", "size": 165}
{"docstring": "\"\"\"Return the ``text`` indented by the given number of ``spaces``.\"\"\"\n\n", "func_signal": "def indent(text, spaces=4, strip=False):\n", "code": "if not hasattr(text, 'splitlines'):\n    text = '\\n'.join(text)\nif strip:\n    text = dedent(text)\nprefix = ' ' * spaces\noutput = '\\n'.join(prefix + line for line in text.splitlines())\noutput = output.strip()\noutput = prefix + output\nreturn output", "path": "bolt\\fabcode.py", "repo_name": "tav/bolt-python", "stars": 33, "license": "other", "language": "python", "size": 165}
{"docstring": "\"\"\"Print the given ``msg`` and exit with status code 1.\"\"\"\n\n", "func_signal": "def abort(msg):\n", "code": "if output.aborts:\n    if env.colors:\n        abort_color = env.color_settings['abort']\n        print >> sys.stderr, abort_color(\"\\nFatal error: \" + str(msg))\n        print >> sys.stderr, abort_color(\"\\nAborting.\")\n    else:\n        print >> sys.stderr, \"\\nFatal error: \" + str(msg)\n        print >> sys.stderr, \"\\nAborting.\"\n\nsys.exit(1)", "path": "bolt\\fabcode.py", "repo_name": "tav/bolt-python", "stars": 33, "license": "other", "language": "python", "size": 165}
{"docstring": "\"\"\"Return ``env.sudo_prefix`` with ``user`` inserted if necessary.\"\"\"\n\n", "func_signal": "def _sudo_prefix(user):\n", "code": "prefix = env.sudo_prefix % env.sudo_prompt\nif user is not None:\n    if str(user).isdigit():\n        user = \"#%s\" % user\n    return \"%s -u \\\"%s\\\" \" % (prefix, user)\nreturn prefix", "path": "bolt\\fabcode.py", "repo_name": "tav/bolt-python", "stars": 33, "license": "other", "language": "python", "size": 165}
{"docstring": "\"\"\"Prompt for and return a password.\"\"\"\n\n", "func_signal": "def prompt_for_password(prompt=None, no_colon=False, stream=None, user=None):\n", "code": "stream = stream or sys.stderr\nif user:\n    default = \"[%s] Login password for user %s\" % (env.host_string, user)\nelse:\n    default = \"[%s] Login password\" % env.host_string\npassword_prompt = prompt if (prompt is not None) else default\nif not no_colon:\n    password_prompt += \": \"\nnew_password = getpass(password_prompt, stream)\nattempts = 1\nwhile not new_password:\n    attempts += 1\n    if attempts > 3:\n        abort(\"Too many login attempts.\")\n    new_password = getpass(password_prompt, stream)\nreturn new_password", "path": "bolt\\fabcode.py", "repo_name": "tav/bolt-python", "stars": 33, "license": "other", "language": "python", "size": 165}
{"docstring": "\"\"\"Decorate a callable as being a task.\"\"\"\n\n", "func_signal": "def task(*args, **kwargs):\n", "code": "display = kwargs.get('display', 1)\nif args:\n    if hasattr(args[0], '__call__'):\n        func = args[0]\n        func.__task__ = 1\n        if not display:\n            func.__hide__ = 1\n        return func\n    ctx = args\n    if len(ctx) == 1 and not isinstance(ctx[0], basestring):\n        ctx = tuple(args[0])\nelse:\n    ctx = ()\n\ndef __task(__func):\n    __func.__ctx__ = ctx\n    __func.__task__ = 1\n    if not display:\n        __func.__hide__ = 1\n    return __func\n\nreturn __task", "path": "bolt\\core.py", "repo_name": "tav/bolt-python", "stars": 33, "license": "other", "language": "python", "size": 165}
{"docstring": "\"\"\"Execute the given task.\"\"\"\n\n", "func_signal": "def execute_task(self, name, args, kwargs, ctx):\n", "code": "task = self.tasks[name]\nenv.command = name\nif output.running:\n    msg = \"running task: %s\" % name\n    prefix = '[system] '\n    if env.colors:\n        prefix = env.color_settings['prefix'](prefix)\n    print(prefix + msg)\n\nif not ctx:\n    ctx = getattr(task, '__ctx__', None)\n\nif ctx:\n    with settings(ctx=ctx):\n        task(*args, **kwargs)\n    return\n\ntask(*args, **kwargs)", "path": "bolt\\core.py", "repo_name": "tav/bolt-python", "stars": 33, "license": "other", "language": "python", "size": 165}
{"docstring": "\"\"\"run the command in parallel locally for each host\"\"\"\n\n", "func_signal": "def multilocal(spec, arg):\n", "code": "def run_local():\n    return local(arg, capture=0, dir=spec.dir, format=spec.format)\n\nenv().multirun(\n    run_local, spec.shell, spec.pty, spec.combine_stderr, spec.dir,\n    spec.format, quiet_exit=1\n    )", "path": "bolt\\core.py", "repo_name": "tav/bolt-python", "stars": 33, "license": "other", "language": "python", "size": 165}
{"docstring": "\"\"\"Apply the given host string to the env dict.\"\"\"\n\n", "func_signal": "def interpret_host_string(host_string):\n", "code": "username, hostname, port = normalize(host_string)\nenv.host_string = host_string\nenv.host = hostname\nenv.user = username\nenv.port = port\nreturn username, hostname, port", "path": "bolt\\fabcode.py", "repo_name": "tav/bolt-python", "stars": 33, "license": "other", "language": "python", "size": 165}
{"docstring": "\"\"\"Initialise ``env`` and ``output`` to default values.\"\"\"\n\n", "func_signal": "def setup_defaults(path=None):\n", "code": "env.update({\n    'again_prompt': 'Sorry, try again.',\n    'always_use_pty': True,\n    'colors': False,\n    'color_settings': {\n        'abort': yellow,\n        'error': yellow,\n        'finish': cyan,\n        'host_prefix': green,\n        'prefix': red,\n        'prompt': blue,\n        'task': red,\n        'warn': yellow\n        },\n    'combine_stderr': True,\n    'command': None,\n    'command_prefixes': [],\n    'config_file': None,\n    'ctx': (),\n    'cwd': '',\n    'disable_known_hosts': False,\n    'echo_stdin': True,\n    'hook': None,\n    'host': None,\n    'host_string': None,\n    'key_filename': None,\n    'lcwd': '',\n    'multirun_child_timeout': 10,\n    'multirun_pool_size': 10,\n    'no_agent': False,\n    'no_keys': False,\n    'output_prefix': True,\n    'password': None,\n    'passwords': {},\n    'port': None,\n    'reject_unknown_hosts': False,\n    'shell': '/bin/bash -l -c',\n    'shell_history_file': '~/.bolt-shell-history',\n    'sudo_prefix': \"sudo -S -p '%s' \",\n    'sudo_prompt': 'sudo password:',\n    'use_shell': True,\n    'user': _get_system_username(),\n    'warn_only': False\n    })\n\noutput.update({\n    'aborts': True,\n    'debug': False,\n    'running': True,\n    'status': True,\n    'stderr': True,\n    'stdout': True,\n    'user': True,\n    'warnings': True,\n    }, {\n    'everything': ['output', 'running', 'user', 'warnings'],\n    'output': [ 'stderr', 'stdout']\n    })\n\n# Load defaults from a YAML file.\nif path and exists(path):\n    fileobj = open(path, 'rb')\n    mapping = load_yaml(fileobj.read())\n    if not isinstance(mapping, dict):\n        abort(\n            \"Got a %r value when loading %r. Mapping expected.\" %\n            (type(mapping), path)\n            )\n    env.update(mapping)", "path": "bolt\\core.py", "repo_name": "tav/bolt-python", "stars": 33, "license": "other", "language": "python", "size": 165}
{"docstring": "\"\"\"Split string, allowing for escaping of the separator.\"\"\"\n\n", "func_signal": "def _escape_split(sep, argstr):\n", "code": "escaped_sep = r'\\%s' % sep\nif escaped_sep not in argstr:\n    return argstr.split(sep)\n\nbefore, _, after = argstr.partition(escaped_sep)\nstartlist = before.split(sep)\nunfinished = startlist[-1]\nstartlist = startlist[:-1]\nendlist = _escape_split(sep, after)\nunfinished += sep + endlist[0]\nreturn startlist + [unfinished] + endlist[1:]", "path": "bolt\\fabcode.py", "repo_name": "tav/bolt-python", "stars": 33, "license": "other", "language": "python", "size": 165}
{"docstring": "\"\"\"Return the platform-specific path for $HOME/.bolt.yaml.\"\"\"\n\n", "func_signal": "def _rc_path(rc_file='.bolt.yaml'):\n", "code": "if not win32:\n    return expanduser(\"~/\" + rc_file)\nelse:\n    from win32com.shell.shell import SHGetSpecialFolderPath\n    from win32com.shell.shellcon import CSIDL_PROFILE\n    return \"%s/%s\" % (SHGetSpecialFolderPath(0,CSIDL_PROFILE), rc_file)", "path": "bolt\\fabcode.py", "repo_name": "tav/bolt-python", "stars": 33, "license": "other", "language": "python", "size": 165}
{"docstring": "\"\"\"run the sudoed command in parallel on the various hosts\"\"\"\n\n", "func_signal": "def multisudo(spec, arg):\n", "code": "def run_sudo():\n    return sudo(\n        arg, spec.shell, spec.pty, spec.combine_stderr, None, spec.dir,\n        spec.format\n        )\n\nenv().multirun(\n    run_sudo, spec.shell, spec.pty, spec.combine_stderr, spec.dir,\n    spec.format, quiet_exit=1\n    )", "path": "bolt\\core.py", "repo_name": "tav/bolt-python", "stars": 33, "license": "other", "language": "python", "size": 165}
{"docstring": "\"\"\"Return the response to a yes/no question.\"\"\"\n\n", "func_signal": "def confirm(question, default_for_empty_response=True):\n", "code": "if default_for_empty_response:\n    suffix = \"Y/n\"\nelse:\n    suffix = \"y/N\"\n\nwhile 1:\n    response = prompt(\"%s [%s] \" % (question, suffix)).lower()\n    if not response:\n        return default_for_empty_response\n    if response in ('y', 'yes'):\n        return True\n    if response in ('n', 'no'):\n        return False\n    print(\"Please specify '(y)es' or '(n)o'.\")", "path": "bolt\\fabcode.py", "repo_name": "tav/bolt-python", "stars": 33, "license": "other", "language": "python", "size": 165}
{"docstring": "\"\"\"Normalize a ``host_string``, returning the explicit host, user, port.\"\"\"\n\n", "func_signal": "def normalize(host_string, omit_port=False):\n", "code": "if not host_string:\n    return ('', '') if omit_port else ('', '', '')\n\nr = HOST_REGEX.match(host_string).groupdict()\nuser = r['user'] or env.get('user')\nhost = r['host']\nport = r['port'] or '22'\nif omit_port:\n    return user, host\n\nreturn user, host, port", "path": "bolt\\fabcode.py", "repo_name": "tav/bolt-python", "stars": 33, "license": "other", "language": "python", "size": 165}
{"docstring": "\"\"\" Calculates the parameters to be sent when the given function is uploaded\n\n    Parameters:\n      - fn : The function for which the upload parameters are generated.\n\n    Returns:\n      None, if an error occured. Otherwise a description of the function that\n      can be sent to the BinCrowd server.\n\"\"\"\n\n", "func_signal": "def get_regular_function_upload_params(fn, min_edges=0):\n", "code": "name = get_function_name(fn.startEA)\n\np = proxyGraph(fn.startEA)\ne = extract_edge_tuples_from_graph(p)\n\nif not e or len(e) < min_edges:\n    debug_print(\"0x%X: '%s' was not uploaded because it is too small.\" % (fn.startEA, name))\n    return None\n\nedges = edges_array_to_dict(e)\nprime = calculate_prime_product(p)\nnumber_of_nodes = len(p.get_nodes())\n\n#repeatable/non-repeatable\nfn2 = idaapi.get_func(fn.startEA)\ndescription = idaapi.get_func_cmt(fn2, True) or idaapi.get_func_cmt(fn2, False) or ''\n\nif description:\n    description = idaapi.idb2scr(description).decode(\"iso-8859-1\")\n\ninf = idaapi.get_inf_structure()\nprocessor = get_processor_name(inf)\n\n(local_variables, arguments) = get_frame_information(fn.startEA)\n\nstackFrame = (local_variables, arguments)\n\n# Handle optional parameters.\nfunctionInformation = {\n            'base_address'      : idaapi.get_imagebase(),\n            'rva'               : fn.startEA - idaapi.get_imagebase(),\n            'processor'         : processor,\n            'language'          : idaapi.get_compiler_name(inf.cc.id),\n            'number_of_nodes'   : \"%d\" % number_of_nodes\n            }\n\nreturn {'name'                  : name,\n        'description'           : description,\n        'prime_product'         : '%d' % prime,\n        'edges'                 : edges,\n        'function_information'  : functionInformation,\n        'stack_frame'           : stackFrame\n}", "path": "bincrowd.py", "repo_name": "zynamics/bincrowd-plugin-ida", "stars": 41, "license": "None", "language": "python", "size": 199}
{"docstring": "\"\"\" Fills the global imported_functions variable.\n\"\"\"\n", "func_signal": "def fill_imported_functions():\n", "code": "for import_index in xrange(idaapi.get_import_module_qty()):\n    imported_functions.append([])\n    idaapi.enum_import_names(import_index, imported_functions_callback)", "path": "bincrowd.py", "repo_name": "zynamics/bincrowd-plugin-ida", "stars": 41, "license": "None", "language": "python", "size": 199}
{"docstring": "\"\"\" Calculates the prime product and the number of outgoing calls for a control\n    flow graph node.\n\n    Parameters:\n      - node : The node for which the values are calculated.\n\n    Returns:\n      A pair of the prime product of the node and the number of outgoing calls.\n\"\"\"\n", "func_signal": "def calculate_node_values(node):\n", "code": "start = node.startEA\nend = node.endEA\n\ncalls = 0\nmnemonics = []\n\nwhile start < end:\n    if is_call(start):\n        calls = calls + 1\n\n    mnemonics.append(idaapi.ua_mnem(start))\n    start = idaapi.next_head(start, end)\n\nreturn (get_prime(mnemonics), calls)", "path": "bincrowd.py", "repo_name": "zynamics/bincrowd-plugin-ida", "stars": 41, "license": "None", "language": "python", "size": 199}
{"docstring": "\"\"\" Searches through an array of imported functions for the imported function at ea.\n\n    Parameters:\n      - imported_functions : A list of list of imported_functions where each element\n                             of the list contains the imported functions of the imported\n                             library with the same index.\n      - ea                 : The ea to search for.\n\n    Returns:\n      Returns a pair of module name and imported function name if an imported function\n      was found at the given effective address. If there is no imported function at that\n      address, None is returned.\n\"\"\"\n", "func_signal": "def get_imported_function(ea):\n", "code": "fill_imported_functions_if_necessary()\n\nfor index in xrange(len(imported_functions)):\n    functions = imported_functions[index]\n\n    for f_ea, name in functions:\n        if ea == f_ea:\n            return (idaapi.get_import_module_name(index), name)\n\nreturn None", "path": "bincrowd.py", "repo_name": "zynamics/bincrowd-plugin-ida", "stars": 41, "license": "None", "language": "python", "size": 199}
{"docstring": "\"\"\" Determines whether the instruction at the given ea is a call instruction.\n\n    Parameters:\n      - ea : The ea of the instruction to check.\n\n    Returns:\n      True, if the instruction at ea is a call instruction. False, otherwise.\n\"\"\"\n", "func_signal": "def is_call(ea):\n", "code": "xref = idaapi.xrefblk_t()\n\nif xref.first_from(ea, idaapi.XREF_FAR):\n    if is_call_reference(xref):\n        return True\n    while xref.next_from():\n        if is_call_reference(xref):\n            return True\n\nreturn False", "path": "bincrowd.py", "repo_name": "zynamics/bincrowd-plugin-ida", "stars": 41, "license": "None", "language": "python", "size": 199}
{"docstring": "\"\"\" Returns an argument map for a function. This map can be sent to the BinCrowd\n    server to describe the function.\n\n    Parameters:\n      - ea                   : The ea of the function to be described.\n      - skip_small_functions : A flag that says whether small functions should be skipped.\n\n    Returns:\n      A pair of (import, function description) where 'import' is True if\n      the function at 'ea' is an imported function. 'function description'\n      is a map that describes the regular function. If there is no function\n      at the given address, None is returned.\n\"\"\"\n", "func_signal": "def get_download_params(ea, skip_small_functions):\n", "code": "imported_function = get_imported_function(ea)\n\nif imported_function:\n    return (True, get_import_function_download_params(*imported_function))\n\nfn = idaapi.get_func(ea)\n\nif fn:\n    params = get_regular_function_download_params(fn, skip_small_functions)\n    return (False, params) if params else None\n\nreturn None", "path": "bincrowd.py", "repo_name": "zynamics/bincrowd-plugin-ida", "stars": 41, "license": "None", "language": "python", "size": 199}
{"docstring": "\"\"\" Calculates the prime product of a graph.\n\n    Parameters:\n      - graph : The graph to calculate the prime product for.\n\n    Returns:\n      The prime product of the graph.\n\"\"\"\n", "func_signal": "def calculate_prime_product(graph):\n", "code": "prime = 1\n\nfor node in graph.get_nodes():\n    prime = (prime * node.prime_product) % 2**64\n\nreturn prime", "path": "bincrowd.py", "repo_name": "zynamics/bincrowd-plugin-ida", "stars": 41, "license": "None", "language": "python", "size": 199}
{"docstring": "\"\"\" Analyzes the stack frame of the function at the given ea for information\n    that is important to BinCrowd.\n\n    Parameters:\n      - ea : The ea of the function whose stack frame should be analyzed.\n\n    Returns:\n      A pair of local local variables and arguments found in the stack frame.\n\"\"\"\n", "func_signal": "def get_frame_information(ea):\n", "code": "local_variables = [ ]\narguments = [ ]\ncurrent = local_variables\n\nframe = idc.GetFrame(ea)\n\nif frame == None:\n    return [[], []]\n\nfirst = start = idc.GetFirstMember(frame)\nend = idc.GetLastMember(frame)\n\n# There are some really screwed up frames of millions and billions of bytes.\n# We need an upper limit, otherwise we'll loop forever.\n#\n# TODO: Find a better way to loop through the frame.\nwhile start <= end and start <= first + 10000:\n    size = idc.GetMemberSize(frame, start)\n\n    if size == None:\n        start = start + 1\n        continue\n\n    name = idc.GetMemberName(frame, start)\n    description = idc.GetMemberComment(frame, start, True) \\\n        or idc.GetMemberComment(frame, start, False) or '' #repeatable/non-repeatable\n\n    description = idaapi.idb2scr(description).decode(\"iso-8859-1\")\n\n    debug_print(\"%s: %d\" % (name, size))\n\n    start += size\n\n    if name in [\" r\", \" s\"]:\n        # Skip return address and base pointer\n        current = arguments\n        continue\n\n    current.append({'name' : name, 'description' : description, 'size' : size})\n\nreturn (local_variables, arguments)", "path": "bincrowd.py", "repo_name": "zynamics/bincrowd-plugin-ida", "stars": 41, "license": "None", "language": "python", "size": 199}
{"docstring": "\"\"\" build formatted strings of results and store in self.list \"\"\"\n", "func_signal": "def formatresults(results, currentNodeCount, currentEdgeCount):\n", "code": "strlist = []\nfor r in results:\n    degree          = r['match_degree']\n    file            = r['file']\n    name            = r['name']\n    description     = r['description']\n    numberOfNodes   = r['number_of_nodes']\n    numberOfEdges   = r['number_of_edges']\n    owner           = r['owner']\n    strlist.append([MATCHDEGREE_STRINGS[degree], file, name, description, \"%d (%d)\" % (numberOfNodes, numberOfNodes - currentNodeCount), \"%d (%d)\" % (numberOfEdges, numberOfEdges - currentEdgeCount), owner])\nreturn strlist", "path": "bincrowd.py", "repo_name": "zynamics/bincrowd-plugin-ida", "stars": 41, "license": "None", "language": "python", "size": 199}
{"docstring": "\"\"\" Gets the name of the function at address ea and demangles it.\n\n    Parameters:\n      - ea : The ea of the function.\n\n    Returns:\n      The demangled name of the function.\n\"\"\"\n", "func_signal": "def get_demangled_name(ea):\n", "code": "name = Demangle(idc.GetFunctionName(ea), idc.GetLongPrm(INF_SHORT_DN))\nif not name:\n    name = idc.GetFunctionName(ea)\n\n# The Demangle function returns stuff like FooFunction(x,x,x,x) in IDA 5.6.\n# If you upload such a function name and download it again you get an error\n# because names with parentheses are invalid.\nfirst_parens = name.find(\"(\")\nif first_parens != -1:\n    name = name[0:first_parens]\n\nreturn name", "path": "bincrowd.py", "repo_name": "zynamics/bincrowd-plugin-ida", "stars": 41, "license": "None", "language": "python", "size": 199}
{"docstring": "\"\"\" Cleans weird characters from downloaded strings.\n\n    Parameters:\n      - results : The results received from the BinCrowd server.\n\n    Returns:\n      Nothing.\n\"\"\"\n", "func_signal": "def clean_results(results):\n", "code": "for single_result in results:\n    for k, v in single_result.items():\n        if type(v) == type(u\"\"):\n            single_result[k] = idaapi.scr2idb(v.encode(\"iso-8859-1\", \"ignore\"))\n        if type(v) == type([]):\n            clean_results(v[0])\n            clean_results(v[1])", "path": "bincrowd.py", "repo_name": "zynamics/bincrowd-plugin-ida", "stars": 41, "license": "None", "language": "python", "size": 199}
{"docstring": "\"\"\" Reads the BinCrowd IDA plugin configuration file.\n\n    Returns:\n      A tuple of (url, username, password, proxy) which describes the location\n      of the BinCrowd server and how to access it. If anything goes wrong,\n      (None, None, None, None) is returned.\n\"\"\"\n", "func_signal": "def read_config_file(configuration_file):\n", "code": "debug_print(\"Reading configuration file\")\n\ntry:\n    config_file = open(configuration_file, \"r\")\n    lines = config_file.readlines()\n    config_file.close()\n\n    if len(lines) < 3:\n        return (None, None, None, None)\n\n    url = lines[0].rstrip(\"\\r\\n\")\n    username = lines[1].rstrip(\"\\r\\n\")\n    password = lines[2].rstrip(\"\\r\\n\")\n    if len(lines) > 3:\n        proxy = lines[3].rstrip(\"\\r\\n\")\n    else:\n        proxy = None\n\n    return (url, username, password, proxy)\nexcept:\n    return (None, None, None, None)", "path": "bincrowd.py", "repo_name": "zynamics/bincrowd-plugin-ida", "stars": 41, "license": "None", "language": "python", "size": 199}
{"docstring": "\"\"\" Returns an argument map for a regular function. This map can be sent to the BinCrowd\n    server to describe the regular function.\n\n    Parameters:\n      - fn                   : The function to be described.\n      - skip_small_functions : A flag that says whether small functions should be skipped.\n\n    Returns:\n      A map that describes the regular function.\n\"\"\"\n", "func_signal": "def get_regular_function_download_params(fn, skip_small_functions):\n", "code": "p = proxyGraph(fn.startEA)\ne = extract_edge_tuples_from_graph(p)\n\nif skip_small_functions and len(e) < 10:\n    debug_print(\"Function %s is too small\" % get_function_name(fn.startEA))\n    return None\n\nreturn {'prime_product' : '%d' % calculate_prime_product(p), 'edges' : edges_array_to_dict(e) }", "path": "bincrowd.py", "repo_name": "zynamics/bincrowd-plugin-ida", "stars": 41, "license": "None", "language": "python", "size": 199}
{"docstring": "\"\"\" Takes a list of edges and converts them into a list of maps that\n    can be sent to the BinCrowd server.\n\n    Parameters:\n      - A list of edges.\n\n    Returns:\n      A list of the same cardinality where every edge of the input map\n      is described in a way that can be understood by the BinCrowd server.\n\"\"\"\n", "func_signal": "def edges_array_to_dict(edges):\n", "code": "output_list = []\nfor (indegree_source, outdegree_source, indegree_target, outdegree_target, topological_order_source, topological_order_target, source_prime, source_call_num, target_prime, target_call_num) in edges:\n    output_list.append(\n           {'indegree_source'            : indegree_source,\n            'outdegree_source'           : outdegree_source,\n            'indegree_target'            : indegree_target,\n            'outdegree_target'           : outdegree_target,\n            'topological_order_source'   : topological_order_source,\n            'topological_order_target'   : topological_order_target,\n            'source_prime'               : \"%d\" % source_prime,\n            'source_call_num'            : source_call_num,\n            'target_prime'               : \"%d\" % target_prime,\n            'target_call_num'            : target_call_num\n        })\n\nreturn output_list", "path": "bincrowd.py", "repo_name": "zynamics/bincrowd-plugin-ida", "stars": 41, "license": "None", "language": "python", "size": 199}
{"docstring": "\"\"\" Callback function for enumerating all imported functions of a module.\n\"\"\"\n", "func_signal": "def imported_functions_callback(ea, name, ord):\n", "code": "imported_functions[-1].append((ea, name))\n\nreturn 1", "path": "bincrowd.py", "repo_name": "zynamics/bincrowd-plugin-ida", "stars": 41, "license": "None", "language": "python", "size": 199}
{"docstring": "\"\"\" Determines the processor name for which the disassembled binary was generated.\n\n    Parameters:\n      - inf : The inf structure of the IDB file.\n\n    Returns:\n      The processor name for which the IDB was generated.\n\"\"\"\n", "func_signal": "def get_processor_name(inf):\n", "code": "null_idx = inf.procName.find(chr(0))\nif null_idx > 0:\n    return inf.procName[:null_idx]\nelse:\n    return inf.procName", "path": "bincrowd.py", "repo_name": "zynamics/bincrowd-plugin-ida", "stars": 41, "license": "None", "language": "python", "size": 199}
{"docstring": "\"\"\" Finds the number of nodes and edges for the function at the given ea.\n\n    Parameters:\n      - ea : The ea of the function whose graph data should be returned.\n\n    Returns:\n      A pair of (node count, edge count) that describes the graph of the function.\n      If that function is an imported function, (0, 0) is returned.\n\"\"\"\n", "func_signal": "def get_graph_data(ea):\n", "code": "if get_imported_function(ea):\n    return (0, 0)\nelse:\n    fn = idaapi.get_func(ea)\n\n    if not fn:\n        raise \"Internal Error: No function at the given ea\"\n\n    p = proxyGraph(fn.startEA)\n\n    return (len(p.get_nodes()), len(p.get_edges()))", "path": "bincrowd.py", "repo_name": "zynamics/bincrowd-plugin-ida", "stars": 41, "license": "None", "language": "python", "size": 199}
{"docstring": "\"\"\" Takes a list of (ea, edge_count, downloaded function result) triples and converts\n    that list into a list of equal cardinality that is suitable for displaying to the\n    user.\n\n    Parameters:\n      - zipped_overview : The input list.\n\n    Returns:\n      The cleaned up version of the input list.\n\"\"\"\n", "func_signal": "def get_information_all_functions(zipped_overview):\n", "code": "result_list = []\n\nfor (ea, edge_count, result) in zipped_overview:\n    result_list.append([ea, result['h'], result['m'], result['l'], edge_count])\n\nreturn sorted(result_list, lambda x, y : y[4] - x[4])", "path": "bincrowd.py", "repo_name": "zynamics/bincrowd-plugin-ida", "stars": 41, "license": "None", "language": "python", "size": 199}
{"docstring": "\"\"\"Finds the root node of a view. Note that this function is a bit imprecise\n    but it should do the trick for most views.\"\"\"\n", "func_signal": "def find_root(self, nodes):\n", "code": "for node in nodes:\n    if len(node.parents) == 0:\n        return node\nreturn nodes[0]", "path": "bincrowd.py", "repo_name": "zynamics/bincrowd-plugin-ida", "stars": 41, "license": "None", "language": "python", "size": 199}
{"docstring": "\"\"\" Assigns downloaded information to the given regular function.\n\n    Parameters:\n      - information : The downloaded function information for one function.\n      - fn          : The function to which the information will be assigned.\n\"\"\"\n", "func_signal": "def set_normal_information(information, fn):\n", "code": "name        = information['name']\ndescription = information['description']\nidc.MakeName(fn.startEA, name)\nif description:\n    idaapi.set_func_cmt(fn, description, True)\n\n(idb_lv, idb_args) = get_frame_information(fn.startEA)\n(local_variables, arguments) = information['stack_frame']\n\n# If the number of downloaded local variables and arguments are the same\n# as in the current IDB file then we rename the local variables and arguments\n# too.\n\nif (len(idb_lv) == len(local_variables) and len(idb_args) == len(arguments)):\n\n    total = local_variables + arguments\n    index = 0\n    frame = idc.GetFrame(fn.startEA)\n\n    if frame != None:\n        start = idc.GetFirstMember(frame)\n        end = idc.GetLastMember(frame)\n\n        # The second check is important for stack frames ending in \" r\" or \" s\"\n        while start <= end and index < len(total):\n            size = idc.GetMemberSize(frame, start)\n\n            if size == None:\n                start = start + 1\n                continue\n\n            name = total[index]['name']\n\n            if name in [\" r\", \" s\"]:\n                # Skip return address and base pointer\n                start += size\n                continue\n\n            idc.SetMemberName(frame, start, name)\n            idc.SetMemberComment(frame, start, total[index]['description'], True)\n\n            index = index + 1\n            start += size", "path": "bincrowd.py", "repo_name": "zynamics/bincrowd-plugin-ida", "stars": 41, "license": "None", "language": "python", "size": 199}
{"docstring": "\"\"\"\n\tLike get_calls_in_function, but returns list with instructions prepended by function EA in which they are\n\"\"\"\n", "func_signal": "def get_calls_in_function_ext( ea ):\n", "code": "calls = get_calls_in_function( ea )\nfor call in calls:\n\tcall.insert(0, ea)\nreturn calls", "path": "x86_RE_lib.py", "repo_name": "zynamics/rtti-helper-scripts", "stars": 34, "license": "None", "language": "python", "size": 131}
{"docstring": "\"\"\"\n\tRetrieve a list of locations that branch to ea\n\"\"\"\n", "func_signal": "def get_crefs_to( ea ):\n", "code": "ret = []\nxrf = get_first_cref_to( ea )\nif xrf != BADADDR:\n\tret.append( xrf )\nxrf = get_next_cref_to( ea, xrf )\nwhile xrf != BADADDR:\n\tret.append( xrf )\n\txrf = get_next_cref_to( ea, xrf )\nreturn ret", "path": "x86_RE_lib.py", "repo_name": "zynamics/rtti-helper-scripts", "stars": 34, "license": "None", "language": "python", "size": 131}
{"docstring": "\"\"\"\n\tReturns the difference between ESP upon entry and end of the basic block\n\"\"\"\n", "func_signal": "def get_basic_block_stack_delta( address ):\n", "code": "delta = 0\nblk = get_basic_block( address )\nif blk[-1][1] == \"call\":\n\tdelta = 4\nif blk[-1][1] == \"retn\":\n\tdelta = -4\nfunc = get_func( blk[0][0] )\nif blk[0][0] == blk[-1][0]:\n\treturn get_sp_delta( func, blk[0][0] )\nelse:\n\tspdelta = get_spd( func, blk[0][0] ) - get_spd( func, blk[-1][0] )\n\tspdelta = spdelta + delta\n\treturn spdelta", "path": "x86_RE_lib.py", "repo_name": "zynamics/rtti-helper-scripts", "stars": 34, "license": "None", "language": "python", "size": 131}
{"docstring": "\"\"\"\n\tSimple function to generate a flowgraph from an address (forwards)\n\"\"\"\n", "func_signal": "def create_flowgraph_from( address ):\n", "code": "flowgraph = vcg_Graph.vcgGraph()\nworklist = [ get_basic_block( address ) ]\nflowgraph.Add_Node( \"%x\" % worklist[0][0][0] )\t\nwhile len( worklist ) != 0:\n\tcurrent_block = worklist.pop(0)\n\tif current_block[-1][1] != \"call\":\n\t\tnextblocks = get_crefs_from( current_block[-1][0] )\n\telse:\n\t\tnextblocks = get_short_crefs_from( current_block[-1][0] )\n\tfor blockaddr in nextblocks:\n\t\tblock = get_basic_block( blockaddr )\n\t\tif not flowgraph.has_node( \"%x\" % block[0][0] ):\n\t\t\tnewnode = flowgraph.Add_Node( \"%x\" % block[0][0] )\n\t\t\tworklist.append( block )\n\t\tflowgraph.Add_Link( \"%x\" % current_block[0][0], \"%x\" % block[0][0] )\nreturn flowgraph", "path": "x86_RE_lib.py", "repo_name": "zynamics/rtti-helper-scripts", "stars": 34, "license": "None", "language": "python", "size": 131}
{"docstring": "\"\"\"\n\tRetrieves a list of locations that are referred to from ea (data only)\n\"\"\"\n", "func_signal": "def get_drefs_from( ea ):\n", "code": "ret = []\nxrf = get_first_dref_from( ea )\nif xrf != BADADDR:\n\tret.append( xrf )\nxrf = get_next_dref_from( ea, xrf )\nwhile xrf != BADADDR:\n\tret.append( xrf )\n\txrf = get_next_dref_from( ea, xrf )\nreturn ret", "path": "x86_RE_lib.py", "repo_name": "zynamics/rtti-helper-scripts", "stars": 34, "license": "None", "language": "python", "size": 131}
{"docstring": "\"\"\"\n\tRetrieves a list of locations that are referring ea (data only)\n\"\"\"\n", "func_signal": "def get_drefs_to( ea ):\n", "code": "ret = []\nxrf = get_first_dref_to( ea )\nif xrf != BADADDR:\n\tret.append( xrf )\nxrf = get_next_dref_to( ea, xrf )\nwhile xrf != BADADDR:\n\tret.append( xrf )\n\txrf = get_next_dref_to( ea, xrf )\nreturn ret", "path": "x86_RE_lib.py", "repo_name": "zynamics/rtti-helper-scripts", "stars": 34, "license": "None", "language": "python", "size": 131}
{"docstring": "\"\"\"\" Get basic block upper bound\n\nWhile the current instruction is not referenced from anywhere and the preceding instruction is not\nreferencing anywhere else, step backwards. Return the first address at which the above conditions\nare no longer true.\n\"\"\"\n", "func_signal": "def get_basic_block_begin_from_ea( ea ):\n", "code": "oldea = 0\nwhile get_first_fcref_to( ea ) == BADADDR and get_first_fcref_from( get_first_cref_to( ea ) ) == BADADDR and ea != BADADDR:\n\toldea = ea\n\tea = get_first_cref_to( ea )\nif ea == BADADDR:\n\treturn oldea\nreturn ea", "path": "x86_RE_lib.py", "repo_name": "zynamics/rtti-helper-scripts", "stars": 34, "license": "None", "language": "python", "size": 131}
{"docstring": "\"\"\"\n\tA basic block will be a list of lists that contain all the instructions\n\tin this particular basic block.\n\t[ \n\t\t[ firstaddress, mnem, op1, op2, op3 ]\n\t\t...\n\t\t[ lastaddress, mnem, op1, op2, op3 ]\n\t]\n\"\"\"\n", "func_signal": "def get_basic_block( ea ):\n", "code": "begin = get_basic_block_begin_from_ea( ea )\nrealbegin = begin\nend = get_basic_block_end_from_ea( ea )\nret = []\nwhile begin <= end and begin >= realbegin:\n\tret.append( get_disasm_line( begin ) )\n\tif get_first_cref_from( begin ) <= begin:\n\t\tbreak\n\tbegin = get_first_cref_from( begin )\nreturn ret", "path": "x86_RE_lib.py", "repo_name": "zynamics/rtti-helper-scripts", "stars": 34, "license": "None", "language": "python", "size": 131}
{"docstring": "\"\"\"\n\tRetrieves a list of locations that \n\"\"\"\n", "func_signal": "def get_short_crefs_from( ea ):\n", "code": "ret = []\nxrf = get_first_cref_from( ea )\nxrf2 = get_first_fcref_from( ea )\nif xrf != BADADDR and xrf != xrf2:\n\tret.append( xrf )\nxrf = get_next_cref_from( ea, xrf )\nwhile xrf != BADADDR and xrf != xrf2:\n\tret.append( xrf )\n\txrf = get_next_cref_from( ea, xrf )\nreturn ret", "path": "x86_RE_lib.py", "repo_name": "zynamics/rtti-helper-scripts", "stars": 34, "license": "None", "language": "python", "size": 131}
{"docstring": "\"\"\"\n\tReturns list of begin/end tuples for vtables found in the executable\n\tA table is considered a vtable if:\n\t\tit consists of at least 1 pointers to functions\n\t\tit's offset is written to a register in the form [reg]\n\"\"\"\n", "func_signal": "def find_vtables_aggressive( firstaddr = 0, lastaddr = 0x7FFFFFFF ):\n", "code": "valid_reg_strings = [ \"[eax\", \"[ebx\", \"[ecx\", \"[edx\", \"[esi\", \"[edi\",\\\n\t\"[ebp\" ]\nif firstaddr == 0:\n\tstartaddr = nextaddr( firstaddr)\nelse:\n\tstartaddr = firstaddr\nvtables = []\nwhile startaddr != BADADDR:\n\t#\n\t#   Check if the offset is written \n\t#\n\txrefs = get_drefs_to( startaddr )\n\tis_written_to_beginning = 0\n\tfor xref in xrefs:\n\t\tline = get_disasm_line( xref )\n\t\tif len( line ) >= 3:\n\t\t\tfor reg in valid_reg_strings:\n\t\t\t\tif line[2].find( reg ) != -1:\n\t\t\t\t\tis_written_to_beginning = 1\n\t#\n\t#   Check if \n\t#\n\ti = 0\n\tif is_written_to_beginning == 1:\n\t\twhile get_first_dref_from( startaddr + (4 * (i+1))) != BADADDR:\n\t\t\tea = get_first_dref_from( startaddr + (4*i))\n\t\t\tfunc = get_func( ea )\n\t\t\ttry:\n\t\t\t\tif func.startEA != ea:\n\t\t\t\t\tbreak\n\t\t\texcept( AttributeError ):\n\t\t\t\tbreak;\n\t\t\ti = i + 1\n\t\t\tif len( get_drefs_to( startaddr + ( 4 * (i)))) != 0:\n\t\t\t\tbreak;\n\tif i > 0:\n\t\tvtables.append( [ startaddr, startaddr + (4*i) ] )\n\tif i > 0:\n\t\tstartaddr = startaddr + i*4\n\telif get_item_size( startaddr ) != 0:\n\t\tstartaddr = startaddr + get_item_size( startaddr )\n\telse:\n\t\tstartaddr = startaddr + 1\n\tif nextaddr( startaddr ) == BADADDR:\n\t\tbreak\n\tif startaddr >= lastaddr:\n\t\tbreak\nreturn vtables", "path": "x86_RE_lib.py", "repo_name": "zynamics/rtti-helper-scripts", "stars": 34, "license": "None", "language": "python", "size": 131}
{"docstring": "\"\"\" Returns a list [ int address, string mnem, string op1, string op2, string op3 ]\n\n\"\"\"\n", "func_signal": "def get_disasm_line( ea ):\n", "code": "op1 = ua_outop2( ea, 0, 0 )\t\nop2 = ua_outop2( ea, 1, 0 )\nop3 = ua_outop2( ea, 2, 0 )\nif op1 == None:\n\top1 = \"\"\nelse:\n\top1 = idaline_to_string( op1 )\nif op2 == None:\n\top2 = \"\"\nelse:\n\top2 = idaline_to_string( op2 )\nif op3 == None:\n\top3 = \"\"\nelse:\n\top3 = idaline_to_string( op3 )\nret = [ ea, ua_mnem( ea ), op1, op2, op3 ]\nreturn ret", "path": "x86_RE_lib.py", "repo_name": "zynamics/rtti-helper-scripts", "stars": 34, "license": "None", "language": "python", "size": 131}
{"docstring": "\"\"\"\n\tWalks a reachgraph upwards, inlining every function on the path. \n\n\tAllright, describe the algorithm first before writing shit down\n\n\t1. Retrieve first flowgraph and node\n\t2. Remove all that is not before node\n\t3. Scan upwards. Notice stack access in each basic block\n\t\t3a.\tIf you run into a call, and the target of the call is in the\n\t\t\treachgraph, inline it\n\t\t3b. \tIf you run into the beginning of the function, add the return\n\t\t\tnodes of all parents in the reachgraph to the graph\n\"\"\"\n", "func_signal": "def build_ibb_graph_from( ea_source, sourcenode, reachgraph ):\n", "code": "flowgraph = create_flowgraph_from( 0x4423D0 )\nadd_disasm_lines_to_flowgraph( flowgraph )\nflowgraph.write_VCG_File(\"C:\\\\test.vcg\")", "path": "x86_RE_lib.py", "repo_name": "zynamics/rtti-helper-scripts", "stars": 34, "license": "None", "language": "python", "size": 131}
{"docstring": "\"\"\"\n\tReturns a list with call instructions in a given function\n\"\"\"\n", "func_signal": "def get_calls_in_function( ea ):\n", "code": "callist = []\nflowgraph = create_flowgraph_from( ea )\nfor x in flowgraph.nodes.items():\n\tname = x[0]\n\tblock = get_basic_block( string.atol( name, 16 ))\n\tfor instruction in block:\n\t\tif instruction[ 1 ] == \"call\":\n\t\t\tcallist.append( instruction )\nreturn callist", "path": "x86_RE_lib.py", "repo_name": "zynamics/rtti-helper-scripts", "stars": 34, "license": "None", "language": "python", "size": 131}
{"docstring": "\"\"\"\n\"\"\"\n\n", "func_signal": "def get_pushes_before_call( callea, n ):\n", "code": "\nx = get_spd( get_func( callea ), callea ) + (n*4)\nblock = get_basic_block( callea )\nblock.reverse()\nfor insn2 in block:\n\tif get_spd( get_func( callea ), insn2[0]) == x:\n\t\treturn insn2\nreturn None", "path": "x86_RE_lib.py", "repo_name": "zynamics/rtti-helper-scripts", "stars": 34, "license": "None", "language": "python", "size": 131}
{"docstring": "\"\"\"\n\tRetrieve list of locations that ea branches to \n\"\"\"\n", "func_signal": "def get_far_crefs_from( ea ):\n", "code": "ret = []\nxrf = get_first_fcref_from( ea )\nif xrf != BADADDR:\n\tret.append( xrf )\nxrf = get_next_fcref_from( ea, xrf )\nwhile xrf != BADADDR:\n\tret.append( xrf )\n\txrf = get_next_fcref_from( ea, xrf )\nreturn ret", "path": "x86_RE_lib.py", "repo_name": "zynamics/rtti-helper-scripts", "stars": 34, "license": "None", "language": "python", "size": 131}
{"docstring": "\"\"\"\n\tRetrieve a list of locations that refer to ea using a non-call\n\"\"\"\n", "func_signal": "def get_short_crefs_to( ea ):\n", "code": "ret = []\nxrf = get_first_cref_to( ea )\nxrf2 = get_first_fcref_to( ea )\nif xrf != BADADDR and xrf != xrf2:\n\tret.append( xrf )\nxrf = get_next_cref_to( ea, xrf )\nwhile xrf != BADADDR and xrf != xrf2:\n\tret.append( xrf )\n\txrf = get_next_cref_to( ea, xrf )\nreturn ret", "path": "x86_RE_lib.py", "repo_name": "zynamics/rtti-helper-scripts", "stars": 34, "license": "None", "language": "python", "size": 131}
{"docstring": "\"\"\"\n\tRetrieve a list of locations that branch to ea\n\"\"\"\n", "func_signal": "def get_noncall_crefs_to( ea ):\n", "code": "ret = []\nxrf = get_first_cref_to( ea )\nif xrf != BADADDR:\n\tif ua_mnem( xrf ) != \"call\":\n\t\tret.append( xrf )\nelse:\n\tif ea not in get_far_crefs_from( xrf ):\n\t\tret.append( xrf )\nxrf = get_next_cref_to( ea, xrf )\nwhile xrf != BADADDR:\n\tif ua_mnem( xrf ) != \"call\":\n\t\tret.append( xrf )\n\txrf = get_next_cref_to( ea, xrf )\nreturn ret", "path": "x86_RE_lib.py", "repo_name": "zynamics/rtti-helper-scripts", "stars": 34, "license": "None", "language": "python", "size": 131}
{"docstring": "\"\"\"\tReturns either \"END\", \"\", or the new register to track at the end of this block\n\t\nThis code returns eiter \"END\" if the register is fatally overwritten, \"\" if the register is dereferenced\nor the new register in other cases\n\"\"\"\n", "func_signal": "def get_target_reg( self ):\n", "code": "if len( self.lines ) > 0:\n\tif self.reg == \"eax\" and self.lines[-1][1] == \"call\":\n# We have a call that overwrites EAX\n\t\treturn \"END\"\n\tif self.lines[-1][2] == self.reg and self.lines[-1][1] not in neutral_mnem:\n# We have a non-neutral instruction that writes to the register we're tracking\n\t\treturn \"END\"\n\telif self.lines[-1][2].find( self.reg ) != -1:\n# We have memory access to the location this register is pointing to or an operation on itself\n\t\treturn \"\"\n\telse:\n# If the target is a register, return this register\n\t\tif self.lines[-1][2] in x86_registers:\n\t\t\treturn self.lines[-1][2]\n\t\telse:\n\t\t\treturn \"\"\nelse:\n\treturn \"\"", "path": "x86_RE_lib.py", "repo_name": "zynamics/rtti-helper-scripts", "stars": 34, "license": "None", "language": "python", "size": 131}
{"docstring": "#\n# Start out by getting the functions the basic blocks are in respectively\n#\n", "func_signal": "def build_flowgraph_from_to( ea_source, ea_target ):\n", "code": "source_func = get_func_ea_from_ea( ea_source )\ntarget_func = get_func_ea_from_ea( ea_target )\n#\n# Because it's easier, construct a stack delta graph\n#", "path": "x86_RE_lib.py", "repo_name": "zynamics/rtti-helper-scripts", "stars": 34, "license": "None", "language": "python", "size": 131}
{"docstring": "\"\"\"\n\tTakes an IDA Pro disassembly line and removes all the formatting info\n\tfrom it to make it a \"regular\" string.\n\"\"\"\n", "func_signal": "def idaline_to_string( idaline ):\n", "code": "i = 0\nnew = \"\"\nwhile i < len(idaline):\n\tif idaline[i] == '\\x01' or idaline[i] == '\\x02':\n\t\ti = i + 1\n\telse:\n\t\tnew += idaline[i]\n\ti = i + 1\nreturn new", "path": "x86_RE_lib.py", "repo_name": "zynamics/rtti-helper-scripts", "stars": 34, "license": "None", "language": "python", "size": 131}
{"docstring": "\"\"\"saves dict to couchdb\"\"\"\n", "func_signal": "def save(self, db, data, callback=None):\n", "code": "if not callback: callback = self._generic_cb\n# FIXME: should this look for _rev also?\nif '_id' in data: \n    self.get(db).set(data['_id'], data, callback)\nelse: \n    self.get(db).set(data, callback)", "path": "tornado_addons\\cushion.py", "repo_name": "nod/tornado_addons", "stars": 38, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"gets called when we class decorate\"\"\"\n", "func_signal": "def __call__(self, _handler):\n", "code": "name = self.name or _handler.__name__\nself._routes.append(tornado.web.url(self._uri, _handler, name=name))\nreturn _handler", "path": "tornado_addons\\route.py", "repo_name": "nod/tornado_addons", "stars": 38, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nAttempt to get a connection to the specified database but don't add it\nto our pool if it's there.  You should use open(..) if you intend to\nuse the database in the near future.\n\"\"\"\n", "func_signal": "def exists(self, dbname, callback):\n", "code": "if dbname in self:\n    # short circuit the whole mess if it's in \"us\"\n    callback(True)\n    return\n\ncallback_ = callback\n\ndef cb_(db):\n    if db.error: callback_(False)\n    else: callback_(True)\n\nself._server.get(\n    name=dbname,\n    callback=cb_,\n    create=False )", "path": "tornado_addons\\cushion.py", "repo_name": "nod/tornado_addons", "stars": 38, "license": "None", "language": "python", "size": 128}
{"docstring": "# just blow away our test database using standard trombi fare\n", "func_signal": "def tearDown(self):\n", "code": "self.cushion._server.delete(self.dbname, self.stop)\nself.wait()", "path": "tests\\test_cushion.py", "repo_name": "nod/tornado_addons", "stars": 38, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nOpen a connection to a specific database instance.  If the database\ndoesn't exist, an exception will be thrown unless create=True\n\"\"\"\n", "func_signal": "def open(self, dbname, callback, create=False):\n", "code": "if dbname in self:\n    callback(self.get(dbname))\nelse:\n    def cb_wrapper(db):\n        self._cb_add_db(db)\n        callback(db)\n    self._server.get(\n        name=dbname,\n        callback=cb_wrapper,\n        create=create )", "path": "tornado_addons\\cushion.py", "repo_name": "nod/tornado_addons", "stars": 38, "license": "None", "language": "python", "size": 128}
{"docstring": "# note, this creates and deletes a bogus db\n", "func_signal": "def test_db_not_exists(self):\n", "code": "bogus_db = 'test_db_not_exists_' + str(randint(10,99))\nself.cushion.exists(bogus_db, self.stop)\nis_there = self.wait()\nself.assertTrue( not is_there )", "path": "tests\\test_cushion.py", "repo_name": "nod/tornado_addons", "stars": 38, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nRetrieve a particular document from couchdb.\n\n  x = yield self.db_one(key, cb, dbname)\n\nParameters:\ndb <-   name of the db to hit.  If this db isn't in our cushion, we'll\n        block until we get that connection.\nkey <-  the key of our document, a string.\ncallback <- None or a function to call upon completion.\n**  any other remaining kwargs will be passed through to cushion's .one\n    call, which passes them to trombi.\n\"\"\"\n", "func_signal": "def db_one(self, key, callback, db=None, **kwargs):\n", "code": "logging.debug(\"------------- couch 1 -------------\")\n\n# default to the account db\nif not db: db = self.db_default\n\ncush = self.cushion\n# if the db's not open, we're going to open the db with the callback\n# being the same way we were called\ncush.one(db, key, callback, **kwargs)", "path": "tornado_addons\\cushion.py", "repo_name": "nod/tornado_addons", "stars": 38, "license": "None", "language": "python", "size": 128}
{"docstring": "# note, this creates and deletes a bogus db\n", "func_signal": "def test_db_open_with_callback(self):\n", "code": "bogus_db = 'test_db_trash_' + str(randint(10,99))\nself.cushion.create(bogus_db, self.stop)\nself.wait()\nself.cushion.open(bogus_db, self.stop)\nself.wait()\nself.assertTrue(bogus_db in self.cushion)\nself.cushion._server.delete(self.dbname, self.stop)\nself.wait()", "path": "tests\\test_cushion.py", "repo_name": "nod/tornado_addons", "stars": 38, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nA generic callback for yielded async calls that just captures all args\nand kwargs then continues execution.\n\nNotes about retval\n------------------\nIf a single value is returned into the callback, that value is returned\nas the value of a yield expression.\n\ni.e.: x = yield http.fetch(uri, self.mycb)\n\nThe response from the fetch will be returned to x.\n\nIf more than one value is returned, but no kwargs, the retval is the\nargs tuple.  If there are kwargs but no args, then retval is kwargs.\nIf there are both args and kwargs, retval = (args, kwargs).  If none,\nretval is None.\n\nIt's a little gross but works for a large majority of the cases.\n\"\"\"\n", "func_signal": "def yield_cb(self, *args, **ka):\n", "code": "if args and ka:\n    self._yield_continue((args, ka))\nelif ka and not args:\n    self._yield_continue(ka)\nelif args and not ka:\n    if len(args) == 1:\n        # flatten it\n        self._yield_continue(args[0])\n    else:\n        self._yield_continue(args)\nelse:\n    self._yield_continue()", "path": "tornado_addons\\async_yield.py", "repo_name": "nod/tornado_addons", "stars": 38, "license": "None", "language": "python", "size": 128}
{"docstring": "# default to the account database\n", "func_signal": "def db_save(self, data, callback=None, db=None, ignore_cb=False):\n", "code": "if not db: db = self.db_default\n\ncallback = self._db_cb_get(callback, ignore_cb)\n\ncush = self.cushion\n# if the db's not open, we're going to open the db with the callback\n# being the same way we were called\nif db not in cush: # db's not ready...\n    cush.open( db, lambda *a: self.db_save(data, db, callback))\nelse:\n    cush.save(db, data, callback)", "path": "tornado_addons\\cushion.py", "repo_name": "nod/tornado_addons", "stars": 38, "license": "None", "language": "python", "size": 128}
{"docstring": "# note, this creates and deletes a bogus db\n", "func_signal": "def test_db_exists_make_one(self):\n", "code": "bogus_db = 'test_db_exists_' + str(randint(10,99))\nself.cushion.create(bogus_db, self.stop)\nself.wait()\nself.cushion.exists(bogus_db, self.stop)\nis_there = self.wait()\nself.assertTrue( is_there )", "path": "tests\\test_cushion.py", "repo_name": "nod/tornado_addons", "stars": 38, "license": "None", "language": "python", "size": 128}
{"docstring": "# we should never have a callback AND ignore_cb\n", "func_signal": "def _db_cb_get(self, callback=None, ignore_cb=False):\n", "code": "assert(bool(callback) ^ bool(ignore_cb)) # logical xor\n\nif ignore_cb: callback = self.db_ignored_cb\nreturn callback", "path": "tornado_addons\\cushion.py", "repo_name": "nod/tornado_addons", "stars": 38, "license": "None", "language": "python", "size": 128}
{"docstring": "# This test does quite a bit.  First, create 4 test records.\n# Then, create a view that will emit those records and insert that into\n# the db.  Finally, call our cushion.view object and compare results.\n\n", "func_signal": "def test_view(self):\n", "code": "self._save_some_data({'foo': 1, 'bar': 'a'})\nself._save_some_data({'foo': 2, 'bar': 'a'})\nself._save_some_data({'foo': 3, 'bar': 'b'})\nself._save_some_data({'foo': 4, 'bar': 'b'})\n\nfake_map = \"\"\" function (doc) { emit(doc['bar'], doc); } \"\"\"\n\n# we're going to use python-couchdb's dynamic view loader stuff here\nfrom couchdb.design import ViewDefinition\nfrom couchdb.client import Server\nglobal baseurl\ncdb = Server(baseurl)\ncouchdb = cdb[self.dbname]\n\nview_defn = ViewDefinition(\n    'test', 'view',\n    map_fun = fake_map,\n    language = 'javascript' )\nview_defn.sync(couchdb)\n\nself.cushion.view(self.dbname, 'test/view', self.stop, key='b')\nrecords = self.wait()\n\nself.assertTrue(len(records) == 2)\n\n# OPTIMIZE: do more to ensure we're getting back what we want", "path": "tests\\test_cushion.py", "repo_name": "nod/tornado_addons", "stars": 38, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nRemove doc from database.\n\ndata requires an _id and _rev or an exception is thrown.\n\"\"\"\n", "func_signal": "def delete(self, db, data, callback=None):\n", "code": "if not callback: callback = self._generic_cb\nif '_id' in data and '_rev' in data:\n    self.get(db).delete(data, callback)\nelse: raise CushionException(\n        \"record missing _id and _rev, can't delete\"\n        )", "path": "tornado_addons\\cushion.py", "repo_name": "nod/tornado_addons", "stars": 38, "license": "None", "language": "python", "size": 128}
{"docstring": "# note, this creates and deletes a bogus db\n", "func_signal": "def test_db_exists(self):\n", "code": "self.cushion.exists(self.dbname, self.stop)\nis_there = self.wait()\nself.assertTrue( is_there )", "path": "tests\\test_cushion.py", "repo_name": "nod/tornado_addons", "stars": 38, "license": "None", "language": "python", "size": 128}
{"docstring": "# check for a bogus database first\n", "func_signal": "def test_db_get(self):\n", "code": "self.assertRaises(\n    CushionDBNotReady,\n    self.cushion.get, 'bogus-not-there' )\n\n# now check for a good one\nself.assertTrue(\n    isinstance(\n        self.cushion.get(self.dbname),\n        trombi.Database\n        )\n    )", "path": "tests\\test_cushion.py", "repo_name": "nod/tornado_addons", "stars": 38, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nConvenience method to fetch the results of a view from a specific\ndatabase.\nParameters\n==========\ndb -> db name as str\nresource -> string of the resource 'designDocName/resourceName'\n    or '/resourceName' to hit the special view '_alldocs'\ncb -> function ptr to callback\nka -> keyword arguments\n\"\"\"\n", "func_signal": "def view(self, db, resource, cb, **ka):\n", "code": "des, res = resource.split('/')\n# note, this is calling the .view method on a trombi Database obj\nself.get(db).view(des, res, cb, **ka)", "path": "tornado_addons\\cushion.py", "repo_name": "nod/tornado_addons", "stars": 38, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nAttempt to create a database. If it exists, an exception will be\nthrown.\n\"\"\"\n", "func_signal": "def create(self, dbname, callback):\n", "code": "self._server.create(\n    name=dbname,\n    callback=callback )", "path": "tornado_addons\\cushion.py", "repo_name": "nod/tornado_addons", "stars": 38, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nsee comments for db_one\n\"\"\"\n", "func_signal": "def db_view(self, resource, callback, db=None, **kwargs):\n", "code": "logging.debug(\"------------- couch * -------------\")\n\n# default to the account db\nif not db: db = self.db_default\n\ncush = self.cushion\n# if the db's not open, we're going to open the db with the callback\n# being the same way we were called\nif db not in cush: # db's not ready...\n    # open the db then call ourselves once it's ready to go\n    cush.open(\n        db,\n        lambda *a: self.db_view(\n            resource, db, callback, **kwargs )\n        )\nelse:\n    cush.view(db, resource, callback, **kwargs)", "path": "tornado_addons\\cushion.py", "repo_name": "nod/tornado_addons", "stars": 38, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nConvenience method to fetch one object by id from the specified\ndatabase.\n\nParameters\n==========\ndb -> db name as str\n_id -> key of document to fetch as str\ncb -> function ptr to callback\nka -> keyword arguments\n\"\"\"\n", "func_signal": "def one(self, db, _id, cb, **ka):\n", "code": "def _cb(doc):\n    cb(doc.raw() if doc else None)\n# note, this is calling the .get method on a trombi Database obj\nself.get(db).get(_id, _cb, **ka)", "path": "tornado_addons\\cushion.py", "repo_name": "nod/tornado_addons", "stars": 38, "license": "None", "language": "python", "size": 128}
{"docstring": "'''Get the wallclock time for this status message.\n\nUsed to calculate relative_created_at.  Defaults to the time\nthe object was instantiated.\n\nReturns:\n  Whatever the status instance believes the current time to be,\n  in seconds since the epoch.\n'''\n", "func_signal": "def GetNow(self):\n", "code": "if self._now is None:\n  self._now = time.mktime(time.gmtime())\nreturn self._now", "path": "twitter.py", "repo_name": "steffentchr/twitterengine", "stars": 33, "license": "other", "language": "python", "size": 155}
{"docstring": "'''Create a new instance based on a JSON dict.\n\nArgs:\n  data: A JSON dict, as converted from the JSON in the twitter API\nReturns:\n  A twitter.DirectMessage instance\n'''\n", "func_signal": "def NewFromJsonDict(data):\n", "code": "return DirectMessage(created_at=data.get('created_at', None),\n                     recipient_id=data.get('recipient_id', None),\n                     sender_id=data.get('sender_id', None),\n                     text=data.get('text', None),\n                     sender_screen_name=data.get('sender_screen_name', None),\n                     id=data.get('id', None),\n                     recipient_screen_name=data.get('recipient_screen_name', None))", "path": "twitter.py", "repo_name": "steffentchr/twitterengine", "stars": 33, "license": "other", "language": "python", "size": 155}
{"docstring": "'''Returns a list of the direct messages sent to the authenticating user.\n\nThe twitter.Api instance must be authenticated.\n\nArgs:\n  since:\n    Narrows the returned results to just those statuses created\n    after the specified HTTP-formatted date. [optional]\n\nReturns:\n  A sequence of twitter.DirectMessage instances\n'''\n", "func_signal": "def GetDirectMessages(self, since=None):\n", "code": "url = 'http://twitter.com/direct_messages.json'\nif not self._username:\n  raise TwitterError(\"The twitter.Api instance must be authenticated.\")\nparameters = {}\nif since:\n  parameters['since'] = since\njson = self._FetchUrl(url, parameters=parameters)\ndata = simplejson.loads(json)\nreturn [DirectMessage.NewFromJsonDict(x) for x in data]", "path": "twitter.py", "repo_name": "steffentchr/twitterengine", "stars": 33, "license": "other", "language": "python", "size": 155}
{"docstring": "'''Befriends the user specified in the user parameter as the authenticating user.\n\nThe twitter.Api instance must be authenticated.\n\nArgs:\n  The ID or screen name of the user to befriend.\nReturns:\n  A twitter.User instance representing the befriended user.\n'''\n", "func_signal": "def CreateFriendship(self, user):\n", "code": "url = 'http://twitter.com/friendships/create/%s.json' % user\njson = self._FetchUrl(url, post_data={})\ndata = simplejson.loads(json)\nreturn User.NewFromJsonDict(data)", "path": "twitter.py", "repo_name": "steffentchr/twitterengine", "stars": 33, "license": "other", "language": "python", "size": 155}
{"docstring": "'''Return a string in key=value&key=value form\n\nValues are assumed to be encoded in the format specified by self._encoding,\nand are subsequently URL encoded.\n\nArgs:\n  post_data:\n    A dict of (key, value) tuples, where value is encoded as\n    specified by self._encoding\nReturns:\n  A URL-encoded string in \"key=value&key=value\" form\n'''\n", "func_signal": "def _EncodePostData(self, post_data):\n", "code": "if post_data is None:\n  return None\nelse:\n  return urllib.urlencode(dict([(k, self._Encode(v)) for k, v in post_data.items()]))", "path": "twitter.py", "repo_name": "steffentchr/twitterengine", "stars": 33, "license": "other", "language": "python", "size": 155}
{"docstring": "'''Post a twitter direct message from the authenticated user\n\nThe twitter.Api instance must be authenticated.\n\nArgs:\n  user: The ID or screen name of the recipient user.\n  text: The message text to be posted.  Must be less than 140 characters.\n\nReturns:\n  A twitter.DirectMessage instance representing the message posted\n'''\n", "func_signal": "def PostDirectMessage(self, user, text):\n", "code": "if not self._username:\n  raise TwitterError(\"The twitter.Api instance must be authenticated.\")\nurl = 'http://twitter.com/direct_messages/new.json'\ndata = {'text': text, 'user': user}\njson = self._FetchUrl(url, post_data=data)\ndata = simplejson.loads(json)\nreturn DirectMessage.NewFromJsonDict(data)", "path": "twitter.py", "repo_name": "steffentchr/twitterengine", "stars": 33, "license": "other", "language": "python", "size": 155}
{"docstring": "'''Fetch the sequnce of public twitter.Status message for all users.\n\nArgs:\n  since_id:\n    Returns only public statuses with an ID greater than (that is,\n    more recent than) the specified ID. [Optional]\n\nReturns:\n  An sequence of twitter.Status instances, one for each message\n'''\n", "func_signal": "def GetPublicTimeline(self, since_id=None):\n", "code": "parameters = {}\nif since_id:\n  parameters['since_id'] = since_id\nurl = 'http://twitter.com/statuses/public_timeline.json'\njson = self._FetchUrl(url,  parameters=parameters)\ndata = simplejson.loads(json)\nreturn [Status.NewFromJsonDict(x) for x in data]", "path": "twitter.py", "repo_name": "steffentchr/twitterengine", "stars": 33, "license": "other", "language": "python", "size": 155}
{"docstring": "'''Fetch the sequence of twitter.Status messages for a user's friends\n\nThe twitter.Api instance must be authenticated if the user is private.\n\nArgs:\n  user:\n    Specifies the ID or screen name of the user for whom to return\n    the friends_timeline.  If unspecified, the username and password\n    must be set in the twitter.Api instance.  [optional]\n  since:\n    Narrows the returned results to just those statuses created\n    after the specified HTTP-formatted date. [optional]\n\nReturns:\n  A sequence of twitter.Status instances, one for each message\n'''\n", "func_signal": "def GetFriendsTimeline(self, user=None, since=None):\n", "code": "if user:\n  url = 'http://twitter.com/statuses/friends_timeline/%s.json' % user\nelif not user and not self._username:\n  raise TwitterError(\"User must be specified if API is not authenticated.\")\nelse:\n  url = 'http://twitter.com/statuses/friends_timeline.json'\nparameters = {}\nif since:\n  parameters['since'] = since\njson = self._FetchUrl(url, parameters=parameters)\ndata = simplejson.loads(json)\nreturn [Status.NewFromJsonDict(x) for x in data]", "path": "twitter.py", "repo_name": "steffentchr/twitterengine", "stars": 33, "license": "other", "language": "python", "size": 155}
{"docstring": "'''Create a new instance based on a JSON dict.\n\nArgs:\n  data: A JSON dict, as converted from the JSON in the twitter API\nReturns:\n  A twitter.Status instance\n'''\n", "func_signal": "def NewFromJsonDict(data):\n", "code": "if 'user' in data:\n  user = User.NewFromJsonDict(data['user'])\nelse:\n  user = None\nreturn Status(created_at=data.get('created_at', None),\n              id=data.get('id', None),\n              text=data.get('text', None),\n              from_user=data.get('from_user', None),\n              from_user_id=data.get('from_user_id', None),\n              source=data.get('source', None),\n              profile_image_url=data.get('profile_image_url', None),\n              user=user)", "path": "twitter.py", "repo_name": "steffentchr/twitterengine", "stars": 33, "license": "other", "language": "python", "size": 155}
{"docstring": "'''Get a human redable string representing the posting time\n\nReturns:\n  A human readable string representing the posting time\n'''\n", "func_signal": "def GetRelativeCreatedAt(self):\n", "code": "fudge = 1.25\ndelta  = int(self.now) - int(self.created_at_in_seconds)\n\nif delta < (1 * fudge):\n  return 'about a second ago'\nelif delta < (60 * (1/fudge)):\n  return 'about %d seconds ago' % (delta)\nelif delta < (60 * fudge):\n  return 'about a minute ago'\nelif delta < (60 * 60 * (1/fudge)):\n  return 'about %d minutes ago' % (delta / 60)\nelif delta < (60 * 60 * fudge):\n  return 'about an hour ago'\nelif delta < (60 * 60 * 24 * (1/fudge)):\n  return 'about %d hours ago' % (delta / (60 * 60))\nelif delta < (60 * 60 * 24 * fudge):\n  return 'about a day ago'\nelse:\n  return 'about %d days ago' % (delta / (60 * 60 * 24))", "path": "twitter.py", "repo_name": "steffentchr/twitterengine", "stars": 33, "license": "other", "language": "python", "size": 155}
{"docstring": "'''Fetch the sequence of twitter.User instances featured on twitter.com\n\nThe twitter.Api instance must be authenticated.\n\nReturns:\n  A sequence of twitter.User instances\n'''\n", "func_signal": "def GetFeatured(self):\n", "code": "url = 'http://twitter.com/statuses/featured.json'\njson = self._FetchUrl(url)\ndata = simplejson.loads(json)\nreturn [User.NewFromJsonDict(x) for x in data]", "path": "twitter.py", "repo_name": "steffentchr/twitterengine", "stars": 33, "license": "other", "language": "python", "size": 155}
{"docstring": "'''Set the X-Twitter HTTP headers that will be sent to the server.\n\nArgs:\n  client:\n     The client name as a string.  Will be sent to the server as\n     the 'X-Twitter-Client' header.\n  url:\n     The URL of the meta.xml as a string.  Will be sent to the server\n     as the 'X-Twitter-Client-URL' header.\n  version:\n     The client version as a string.  Will be sent to the server\n     as the 'X-Twitter-Client-Version' header.\n'''\n", "func_signal": "def SetXTwitterHeaders(self, client, url, version):\n", "code": "self._request_headers['X-Twitter-Client'] = client\nself._request_headers['X-Twitter-Client-URL'] = url\nself._request_headers['X-Twitter-Client-Version'] = version", "path": "twitter.py", "repo_name": "steffentchr/twitterengine", "stars": 33, "license": "other", "language": "python", "size": 155}
{"docstring": "'''Create a new instance based on a JSON dict.\n\nArgs:\n  data: A JSON dict, as converted from the JSON in the twitter API\nReturns:\n  A twitter.User instance\n'''\n", "func_signal": "def NewFromJsonDict(data):\n", "code": "if 'status' in data:\n  status = Status.NewFromJsonDict(data['status'])\nelse:\n  status = None\nreturn User(id=data.get('id', None),\n            name=data.get('name', None),\n            screen_name=data.get('screen_name', None),\n            location=data.get('location', None),\n            description=data.get('description', None),\n            profile_image_url=data.get('profile_image_url', None),\n            url=data.get('url', None),\n            status=status)", "path": "twitter.py", "repo_name": "steffentchr/twitterengine", "stars": 33, "license": "other", "language": "python", "size": 155}
{"docstring": "'''A dict representation of this twitter.User instance.\n\nThe return value uses the same key names as the JSON representation.\n\nReturn:\n  A dict representing this twitter.User instance\n'''\n", "func_signal": "def AsDict(self):\n", "code": "data = {}\nif self.id:\n  data['id'] = self.id\nif self.name:\n  data['name'] = self.name\nif self.screen_name:\n  data['screen_name'] = self.screen_name\nif self.location:\n  data['location'] = self.location\nif self.description:\n  data['description'] = self.description\nif self.profile_image_url:\n  data['profile_image_url'] = self.profile_image_url\nif self.url:\n  data['url'] = self.url\nif self.status:\n  data['status'] = self.status.AsDict()\nreturn data", "path": "twitter.py", "repo_name": "steffentchr/twitterengine", "stars": 33, "license": "other", "language": "python", "size": 155}
{"docstring": "'''Returns a single user.\n\nThe twitter.Api instance must be authenticated.\n\nArgs:\n  user: The username or id of the user to retrieve.\n\nReturns:\n  A twitter.User instance representing that user\n'''\n", "func_signal": "def GetUser(self, user):\n", "code": "url = 'http://twitter.com/users/show/%s.json' % user\njson = self._FetchUrl(url)\ndata = simplejson.loads(json)\n\nreturn User.NewFromJsonDict(data)", "path": "twitter.py", "repo_name": "steffentchr/twitterengine", "stars": 33, "license": "other", "language": "python", "size": 155}
{"docstring": "'''Return a string in key=value&key=value form\n\nValues of None are not included in the output string.\n\nArgs:\n  parameters:\n    A dict of (key, value) tuples, where value is encoded as\n    specified by self._encoding\nReturns:\n  A URL-encoded string in \"key=value&key=value\" form\n'''\n", "func_signal": "def _EncodeParameters(self, parameters):\n", "code": "if parameters is None:\n  return None\nelse:\n  return urllib.urlencode(dict([(k, self._Encode(v)) for k, v in parameters.items() if v is not None]))", "path": "twitter.py", "repo_name": "steffentchr/twitterengine", "stars": 33, "license": "other", "language": "python", "size": 155}
{"docstring": "'''Fetch the public twitter.Status messages hits for search terms.\n\nUses the Twitter Search API:\nhttp://twitter.com/api\n\nArgs:\nterms: a String of search terms separated by spaces\nReturns public statuses containing those terms\nfrom_user: the username of the sender\nto_user: the username of the recipient\nhashtag: containing a #tag\nsince_id: only newer tweets since the specified id\nlang: the language desired (i.e. en for english)\n\nReturns:\nA sequence of twitter.Status instances, one for each message\n'''\n\n", "func_signal": "def Search(self, terms=None, from_user=None, to_user=None, hashtag=None, since_id=None, lang=None):\n", "code": "url = 'http://search.twitter.com/search.json'\n\nquery = []\nif terms:\n  query.append(terms)\nif from_user:\n  query.append(\"from%%3A%s\" % from_user)\nif to_user:\n  query.append(\"to%%3A%s\" % to_user)\nif hashtag:\n  query.append(\"%%23%s\" % hashtag)\n\nif not query:\n  return None\n\n# the query is the first of several possible parameters\nparams = {'q': \"+\".join(query)}\nif since_id:\n  params['since_id'] = since_id\nif lang:\n  params['lang'] = lang\n\njson = self._FetchUrl(url,  parameters=params)\ndata = simplejson.loads(json)\n# data contains: results, (all parameters used),\n# results_per_page, query, total, max_id, and page\nreturn [Status.NewFromJsonDict(x) for x in data['results']]", "path": "twitter.py", "repo_name": "steffentchr/twitterengine", "stars": 33, "license": "other", "language": "python", "size": 155}
{"docstring": "'''Post a twitter status message from the authenticated user.\n\nThe twitter.Api instance must be authenticated.\n\nArgs:\n  text: The message text to be posted.  Must be less than 140 characters.\n\nReturns:\n  A twitter.Status instance representing the message posted\n'''\n", "func_signal": "def PostUpdate(self, text):\n", "code": "if not self._username:\n  raise TwitterError(\"The twitter.Api instance must be authenticated.\")\nif len(text) > 140:\n  raise TwitterError(\"Text must be less than or equal to 140 characters.\")\nurl = 'http://twitter.com/statuses/update.json'\ndata = {'status': text}\njson = self._FetchUrl(url, post_data=data)\ndata = simplejson.loads(json)\nreturn Status.NewFromJsonDict(data)", "path": "twitter.py", "repo_name": "steffentchr/twitterengine", "stars": 33, "license": "other", "language": "python", "size": 155}
{"docstring": "'''Fetch the sequence of public twitter.Status messages for a single user.\n\nThe twitter.Api instance must be authenticated if the user is private.\n\nArgs:\n  user:\n    either the username (short_name) or id of the user to retrieve.  If\n    not specified, then the current authenticated user is used. [optional]\n  count: the number of status messages to retrieve [optional]\n  since:\n    Narrows the returned results to just those statuses created\n    after the specified HTTP-formatted date. [optional]\n\nReturns:\n  A sequence of twitter.Status instances, one for each message up to count\n'''\n", "func_signal": "def GetUserTimeline(self, user=None, count=None, since=None):\n", "code": "try:\n  if count:\n    int(count)\nexcept:\n  raise TwitterError(\"Count must be an integer\")\nparameters = {}\nif count:\n  parameters['count'] = count\nif since:\n  parameters['since'] = since\nif user:\n  url = 'http://twitter.com/statuses/user_timeline/%s.json' % user\nelif not user and not self._username:\n  raise TwitterError(\"User must be specified if API is not authenticated.\")\nelse:\n  url = 'http://twitter.com/statuses/user_timeline.json'\njson = self._FetchUrl(url, parameters=parameters)\ndata = simplejson.loads(json)\nreturn [Status.NewFromJsonDict(x) for x in data]", "path": "twitter.py", "repo_name": "steffentchr/twitterengine", "stars": 33, "license": "other", "language": "python", "size": 155}
{"docstring": "'''Discontinues friendship with the user specified in the user parameter.\n\nThe twitter.Api instance must be authenticated.\n\nArgs:\n  The ID or screen name of the user  with whom to discontinue friendship.\nReturns:\n  A twitter.User instance representing the discontinued friend.\n'''\n", "func_signal": "def DestroyFriendship(self, user):\n", "code": "url = 'http://twitter.com/friendships/destroy/%s.json' % user\njson = self._FetchUrl(url, post_data={})\ndata = simplejson.loads(json)\nreturn User.NewFromJsonDict(data)", "path": "twitter.py", "repo_name": "steffentchr/twitterengine", "stars": 33, "license": "other", "language": "python", "size": 155}
{"docstring": "\"\"\"Object mapper starts off with empty value.\"\"\"\n", "func_signal": "def __init__(self):\n", "code": "self.value = None\nself.seen = set()", "path": "go\\appcfg\\google\\appengine\\api\\yaml_object.py", "repo_name": "yinqiwen/snova-gae", "stars": 42, "license": "None", "language": "python", "size": 13786}
{"docstring": "\"\"\"\nParse the first YAML document in a stream\nand produce the corresponding representation tree.\n\"\"\"\n", "func_signal": "def compose(stream, Loader=Loader):\n", "code": "loader = Loader(stream)\nif loader.check_node():\n    return loader.get_node()", "path": "go\\appcfg\\yaml\\__init__.py", "repo_name": "yinqiwen/snova-gae", "stars": 42, "license": "None", "language": "python", "size": 13786}
{"docstring": "\"\"\"Object sequencer starts off with empty value.\"\"\"\n", "func_signal": "def __init__(self):\n", "code": "self.value = []\nself.constructor = None", "path": "go\\appcfg\\google\\appengine\\api\\yaml_object.py", "repo_name": "yinqiwen/snova-gae", "stars": 42, "license": "None", "language": "python", "size": 13786}
{"docstring": "\"\"\"\nParse the first YAML document in a stream\nand produce the corresponding Python object.\n\"\"\"\n", "func_signal": "def load(stream, Loader=Loader):\n", "code": "loader = Loader(stream)\nif loader.check_data():\n    return loader.get_data()", "path": "go\\appcfg\\yaml\\__init__.py", "repo_name": "yinqiwen/snova-gae", "stars": 42, "license": "None", "language": "python", "size": 13786}
{"docstring": "# Check the type of the next event.\n", "func_signal": "def check_event(self, *choices):\n", "code": "if self.current_event is None:\n    if self.state:\n        self.current_event = self.state()\nif self.current_event is not None:\n    if not choices:\n        return True\n    for choice in choices:\n        if isinstance(self.current_event, choice):\n            return True\nreturn False", "path": "go\\appcfg\\yaml\\parser.py", "repo_name": "yinqiwen/snova-gae", "stars": 42, "license": "None", "language": "python", "size": 13786}
{"docstring": "\"\"\"Build object from stream.\n\nHandles the basic case of loading a single object from a stream.\n\nArgs:\n  default_class: Class that is instantiated upon the detection of a new\n    document.  An instance of this class will act as the document itself.\n  stream: String document or open file object to process as per the\n    yaml.parse method.  Any object that implements a 'read()' method which\n    returns a string document will work with the YAML parser.\n  loader_class: Used for dependency injection.\n\"\"\"\n", "func_signal": "def BuildSingleObject(default_class, stream, loader=yaml.loader.SafeLoader):\n", "code": "definitions = BuildObjects(default_class, stream, loader)\n\nif len(definitions) < 1:\n  raise yaml_errors.EmptyConfigurationFile()\nif len(definitions) > 1:\n  raise yaml_errors.MultipleConfigurationFile()\nreturn definitions[0]", "path": "go\\appcfg\\google\\appengine\\api\\yaml_object.py", "repo_name": "yinqiwen/snova-gae", "stars": 42, "license": "None", "language": "python", "size": 13786}
{"docstring": "\"\"\"\nParse all YAML documents in a stream\nand produce corresponding Python objects.\n\"\"\"\n", "func_signal": "def load_all(stream, Loader=Loader):\n", "code": "loader = Loader(stream)\nwhile loader.check_data():\n    yield loader.get_data()", "path": "go\\appcfg\\yaml\\__init__.py", "repo_name": "yinqiwen/snova-gae", "stars": 42, "license": "None", "language": "python", "size": 13786}
{"docstring": "# Get the next event.\n", "func_signal": "def peek_event(self):\n", "code": "if self.current_event is None:\n    if self.state:\n        self.current_event = self.state()\nreturn self.current_event", "path": "go\\appcfg\\yaml\\parser.py", "repo_name": "yinqiwen/snova-gae", "stars": 42, "license": "None", "language": "python", "size": 13786}
{"docstring": "\"\"\"Append a value to a sequence.\n\nArgs:\n  subject: _ObjectSequence that is receiving new value.\n  value: Value that is being appended to sequence.\n\"\"\"\n", "func_signal": "def AppendTo(self, subject, value):\n", "code": "if isinstance(value, _ObjectMapper):\n\n  value.set_value(subject.constructor())\n  subject.value.append(value.value)\nelse:\n\n  subject.value.append(value)", "path": "go\\appcfg\\google\\appengine\\api\\yaml_object.py", "repo_name": "yinqiwen/snova-gae", "stars": 42, "license": "None", "language": "python", "size": 13786}
{"docstring": "\"\"\"\nConvert a Python object to a representation node.\n\"\"\"\n", "func_signal": "def to_yaml(cls, dumper, data):\n", "code": "return dumper.represent_yaml_object(cls.yaml_tag, data, cls,\n        flow_style=cls.yaml_flow_style)", "path": "go\\appcfg\\yaml\\__init__.py", "repo_name": "yinqiwen/snova-gae", "stars": 42, "license": "None", "language": "python", "size": 13786}
{"docstring": "# Write BOM if needed.\n", "func_signal": "def write_stream_start(self):\n", "code": "if self.encoding and self.encoding.startswith('utf-16'):\n    self.stream.write(u'\\xFF\\xFE'.encode(self.encoding))", "path": "go\\appcfg\\yaml\\emitter.py", "repo_name": "yinqiwen/snova-gae", "stars": 42, "license": "None", "language": "python", "size": 13786}
{"docstring": "\"\"\"\nParse all YAML documents in a stream\nand produce corresponsing representation trees.\n\"\"\"\n", "func_signal": "def compose_all(stream, Loader=Loader):\n", "code": "loader = Loader(stream)\nwhile loader.check_node():\n    yield loader.get_node()", "path": "go\\appcfg\\yaml\\__init__.py", "repo_name": "yinqiwen/snova-gae", "stars": 42, "license": "None", "language": "python", "size": 13786}
{"docstring": "\"\"\"\nAdd a path based resolver for the given tag.\nA path is a list of keys that forms a path\nto a node in the representation tree.\nKeys can be string values, integers, or None.\n\"\"\"\n", "func_signal": "def add_path_resolver(tag, path, kind=None, Loader=Loader, Dumper=Dumper):\n", "code": "Loader.add_path_resolver(tag, path, kind)\nDumper.add_path_resolver(tag, path, kind)", "path": "go\\appcfg\\yaml\\__init__.py", "repo_name": "yinqiwen/snova-gae", "stars": 42, "license": "None", "language": "python", "size": 13786}
{"docstring": "# Get the next event and proceed further.\n", "func_signal": "def get_event(self):\n", "code": "if self.current_event is None:\n    if self.state:\n        self.current_event = self.state()\nvalue = self.current_event\nself.current_event = None\nreturn value", "path": "go\\appcfg\\yaml\\parser.py", "repo_name": "yinqiwen/snova-gae", "stars": 42, "license": "None", "language": "python", "size": 13786}
{"docstring": "\"\"\"\nScan a YAML stream and produce scanning tokens.\n\"\"\"\n", "func_signal": "def scan(stream, Loader=Loader):\n", "code": "loader = Loader(stream)\nwhile loader.check_token():\n    yield loader.get_token()", "path": "go\\appcfg\\yaml\\__init__.py", "repo_name": "yinqiwen/snova-gae", "stars": 42, "license": "None", "language": "python", "size": 13786}
{"docstring": "\"\"\"New instance of object mapper for opening map scope.\n\nArgs:\n  top_value: Parent of nested object.\n\nReturns:\n  New instance of object mapper.\n\"\"\"\n", "func_signal": "def BuildMapping(self, top_value):\n", "code": "result = _ObjectMapper()\n\n\nif isinstance(top_value, self.default_class):\n  result.value = top_value\nreturn result", "path": "go\\appcfg\\google\\appengine\\api\\yaml_object.py", "repo_name": "yinqiwen/snova-gae", "stars": 42, "license": "None", "language": "python", "size": 13786}
{"docstring": "\"\"\"Get the ultimate type of a repeated validator.\n\nLooks for an instance of validation.Repeated, returning its constructor.\n\nArgs:\n  attribute: Repeated validator attribute to find type for.\n\nReturns:\n  The expected class of of the Type validator, otherwise object.\n\"\"\"\n", "func_signal": "def _GetRepeated(self, attribute):\n", "code": "if isinstance(attribute, validation.Optional):\n  attribute = attribute.validator\nif isinstance(attribute, validation.Repeated):\n  return attribute.constructor\nreturn object", "path": "go\\appcfg\\google\\appengine\\api\\yaml_object.py", "repo_name": "yinqiwen/snova-gae", "stars": 42, "license": "None", "language": "python", "size": 13786}
{"docstring": "\"\"\"\nParse a YAML stream and produce parsing events.\n\"\"\"\n", "func_signal": "def parse(stream, Loader=Loader):\n", "code": "loader = Loader(stream)\nwhile loader.check_event():\n    yield loader.get_event()", "path": "go\\appcfg\\yaml\\__init__.py", "repo_name": "yinqiwen/snova-gae", "stars": 42, "license": "None", "language": "python", "size": 13786}
{"docstring": "# Parse any extra document end indicators.\n", "func_signal": "def parse_document_start(self):\n", "code": "        while self.check_token(DocumentEndToken):\n            self.get_token()\n# Parse an explicit document.\n        if not self.check_token(StreamEndToken):\n            token = self.peek_token()\n            start_mark = token.start_mark\n            version, tags = self.process_directives()\n            if not self.check_token(DocumentStartToken):\n                raise ParserError(None, None,\n                        \"expected '<document start>', but found %r\"\n                        % self.peek_token().id,\n                        self.peek_token().start_mark)\n            token = self.get_token()\n            end_mark = token.end_mark\n            event = DocumentStartEvent(start_mark, end_mark,\n                    explicit=True, version=version, tags=tags)\n            self.states.append(self.parse_document_end)\n            self.state = self.parse_document_content\n        else:\n            # Parse the end of the stream.\n            token = self.get_token()\n            event = StreamEndEvent(token.start_mark, token.end_mark)\n            assert not self.states\n            assert not self.marks\n            self.state = None\n        return event", "path": "go\\appcfg\\yaml\\parser.py", "repo_name": "yinqiwen/snova-gae", "stars": 42, "license": "None", "language": "python", "size": 13786}
{"docstring": "\"\"\"Build objects from stream.\n\nHandles the basic case of loading all the objects from a stream.\n\nArgs:\n  default_class: Class that is instantiated upon the detection of a new\n    document.  An instance of this class will act as the document itself.\n  stream: String document or open file object to process as per the\n    yaml.parse method.  Any object that implements a 'read()' method which\n    returns a string document will work with the YAML parser.\n  loader_class: Used for dependency injection.\n\nReturns:\n  List of default_class instances parsed from the stream.\n\"\"\"\n", "func_signal": "def BuildObjects(default_class, stream, loader=yaml.loader.SafeLoader):\n", "code": "builder = ObjectBuilder(default_class)\nhandler = yaml_builder.BuilderHandler(builder)\nlistener = yaml_listener.EventListener(handler)\n\nlistener.Parse(stream, loader)\nreturn handler.GetResults()", "path": "go\\appcfg\\google\\appengine\\api\\yaml_object.py", "repo_name": "yinqiwen/snova-gae", "stars": 42, "license": "None", "language": "python", "size": 13786}
{"docstring": "\"\"\"Expand all nodes and resolve all variables.\n\"\"\"\n", "func_signal": "def eval(self, ctx):\n", "code": "if self.normalize:\n    return self.walk_to_buffer(execute_callable, self.compile(ctx), append_sent, ctx=ctx)\nelse:\n    return self.walk_to_buffer(execute_callable, self, append_sent, ctx=ctx)", "path": "pt\\dom.py", "repo_name": "dogada/PyT", "stars": 32, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"Append sent values into buf.\"\"\"\n", "func_signal": "def append_sent(buf):\n", "code": "while True:\n    buf.append((yield))", "path": "pt\\utils.py", "repo_name": "dogada/PyT", "stars": 32, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"Takes pairs (value, predicate), evaluates predicates and return value for matched predicate.\nIf all predicates returned False, return None. If predicate is callable (can't be evaluated at\ncompile time), then return function that will evaluate such predicate and all next predicates\nat runtime.\n>>> Maybe(\"No\", False, \"Yes\", True)\n'Yes'\n>>> enum = Enum(range(3))\n>>> [(item, Maybe(\"first\", i==0, \"last\", enum.last(i))) for i, item in enum]\n[(0, 'first'), (1, None), (2, 'last')]\n>>> from pt.demo.env import div, Template, Maybe, S\n>>> doc = Template() [div(cls=Maybe(\"active\", S(\"user\", None)))]\n>>> doc.render({'user': 'tester'})\nu'<div class=\"active\"></div>'\n>>> doc.render({})                  # no class attribute at all\nu'<div></div>'\n\"\"\"\n", "func_signal": "def Maybe(*args):\n", "code": "assert len(args) % 2 == 0, \"Pairs (value, predicate) are expected.\"\n_runtime = []\nfor value, predicate in zip(args[::2], args[1:][::2]):\n    if _runtime or callable(predicate):\n        _runtime.append((value, predicate))\n    elif predicate: # no runtime predicates so far\n        return value\nif _runtime:\n    return lambda ctx: _runtime_maybe(ctx, _runtime)", "path": "pt\\ctx.py", "repo_name": "dogada/PyT", "stars": 32, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"Join sent strings and append into buf.\"\"\"\n", "func_signal": "def join_sent(buf):\n", "code": "prev = []\ntry:\n    while True:\n        obj = (yield)\n        if isinstance(obj, basestring):\n            prev.append(obj)\n        else:\n            if prev:\n                buf.append(u\"\".join(prev))\n                prev = []\n            buf.append(obj)\nfinally:                            # handle GeneratorExit\n    if prev:\n        buf.append(u\"\".join(prev))", "path": "pt\\utils.py", "repo_name": "dogada/PyT", "stars": 32, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"Enable debug for template object ot class.\"\"\"\n", "func_signal": "def enable_debug(template, indent_step=u' '*2):\n", "code": "template.indent_step = indent_step\ntemplate.visitors[Element] = indent_visit_element", "path": "pt\\dom.py", "repo_name": "dogada/PyT", "stars": 32, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"Replaces all callables in self.iterator() with corresponding values from params,\nthe rest items converts to unicode.\n\"\"\"\n", "func_signal": "def eval_list(self, param_list):\n", "code": "params = iter(param_list)\nfor item in self.iterator():\n    if callable(item):\n        yield unicode(params.next())\n    else:\n        yield unicode(item)", "path": "pt\\dom.py", "repo_name": "dogada/PyT", "stars": 32, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"Return 2 functions for testing template in cached and cgi modes.\"\"\"\n", "func_signal": "def cached_and_cgi(name, template_func, render):\n", "code": "_template = template_func()\ndef test_cached():\n    # reuse early created template\n    render(_template)\ntest_cached.__doc__ = \"test_%s\" % name\ndef test_cgi():\n    # create new template on each call\n    render(template_func())\ntest_cgi.__doc__ = \"test_%s_cgi\" % name\nreturn test_cached, test_cgi", "path": "pt\\benchmark\\__init__.py", "repo_name": "dogada/PyT", "stars": 32, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"Apply read stage transformations to template nodes.\n\"\"\"\n", "func_signal": "def read(self, ctx):\n", "code": "return self.walk_to_buffer(self._read_visitor(), self,\\\n                           append_sent, ctx=None)", "path": "pt\\dom.py", "repo_name": "dogada/PyT", "stars": 32, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"Make template from nodes, rememember it in a closure and return renderer func.\n\"\"\"\n", "func_signal": "def render_many_list(nodes, data_source, normalize=True, indent_step=False):\n", "code": "t = Template(normalize=normalize, indent_step=indent_step) [nodes]\ndef renderer(ctx):\n    return t.render_many_list(data_source(ctx))\nreturn renderer", "path": "pt\\dom.py", "repo_name": "dogada/PyT", "stars": 32, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"Return new node subclass with new attrs and same content.\nFor 'class' attribute use 'cls' name, for example div(cls=\"main\") or pass it as\ndiv(**{'class': \"main\"}). If 'cls' and 'class' used together error is raised.\n\"\"\"\n", "func_signal": "def __call__(self, **attrs):\n", "code": "if 'cls' in attrs:\n    assert 'class' not in attrs, \"'cls' and 'class' attribute names used together.\"\n    attrs['class'] = attrs.pop('cls')\nreturn self.__class__(self.name, attrs, self.content)", "path": "pt\\dom.py", "repo_name": "dogada/PyT", "stars": 32, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"Extend this template overwriting some blocks and properties and return new template.\n\"\"\"\n", "func_signal": "def extend(self, blocks=[], **kwargs):\n", "code": "assert all(isinstance(b, Block) for b in blocks), \"Only instances of Block are accepted.\"\nassert all(b.name for b in blocks), \"All extending blocks must have names.\"\nnew_blocks = dict((b.name, b) for b in blocks)\nassert len(blocks) == len(new_blocks), \"All blocks should have unique names.\"\n\ndef copy(obj, blocks=None):\n    if isinstance(obj, Node):\n        if type(obj) == Block:\n            if obj.name in new_blocks:\n                block = copy(new_blocks.pop(obj.name), _find_blocks(obj))\n            else:\n                block = obj.__class__(**copy(obj.__dict__, blocks))\n                # change parent only for redefined blocks and their child blocks\n                if blocks is not None: \n                    block.parent = blocks.pop(obj.name, None)\n            return block\n        return obj.__class__(**copy(obj.__dict__, blocks))\n    elif isinstance(obj, dict):\n        return dict((k, copy(v, blocks)) for k,v in obj.iteritems())\n    elif hasattr(obj, '__iter__'):\n        return obj.__class__(copy(i, blocks) for i in obj)\n    else:\n        return obj\nchild = copy(self)\nassert not new_blocks, \"Unknown blocks: %s\" % new_blocks.keys()\nfor k,v in kwargs.iteritems():\n    setattr(child, k, v)\nreturn child", "path": "pt\\dom.py", "repo_name": "dogada/PyT", "stars": 32, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"Make template from nodes, rememember it in a closure and return renderer func.\n\"\"\"\n", "func_signal": "def render_list(nodes, data_source, normalize=True, indent_step=False):\n", "code": "t = Template(normalize=normalize, indent_step=indent_step) [nodes]\ndef renderer(ctx):\n    return t.render_list(data_source(ctx))\nreturn renderer", "path": "pt\\dom.py", "repo_name": "dogada/PyT", "stars": 32, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"Return all blocks inside obj (including obj itself if it's block). \"\"\"\n", "func_signal": "def _find_blocks(obj, blocks={}):\n", "code": "if type(obj) == Block:\n    assert obj.name not in blocks, \"Duplicated block name '%s' in %r\" % (obj.name, blocks.keys())\n    blocks[obj.name] = obj\n\nif isinstance(obj, Block):\n    _find_blocks(obj.content, blocks)\nelif isinstance(obj, Node):\n    _find_blocks(obj.__dict__, blocks)\nelif isinstance(obj, dict):\n    [_find_blocks(v, blocks) for v in obj.itervalues()]\nelif hasattr(obj, '__iter__'):\n    [_find_blocks(i, blocks) for i in obj]\nreturn blocks", "path": "pt\\dom.py", "repo_name": "dogada/PyT", "stars": 32, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"Join string sequences in the stream into single unicode string.\n\"\"\"\n", "func_signal": "def join_strings(stream):\n", "code": "buf = []\nfor i in stream:\n    if isinstance(i, basestring):\n        buf.append(i)\n    else:\n        if buf:\n            yield u\"\".join(buf)\n            buf = []\n        yield i\nif buf:\n    yield u\"\".join(buf)", "path": "pt\\utils.py", "repo_name": "dogada/PyT", "stars": 32, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"Normalize strings, resolve other issues that don't require ctx.\n\"\"\"\n", "func_signal": "def compile(self, ctx):\n", "code": "cache = self._get_cache()\nkey = self.cache_key(ctx)\nif key not in cache:\n    cache[key] = self.walk_to_buffer(self._read_visitor(), self, join_sent, ctx=None)\nreturn cache[key]", "path": "pt\\dom.py", "repo_name": "dogada/PyT", "stars": 32, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"Return invoker of a function with given kwargs and ctx.\n\"\"\"\n", "func_signal": "def Function(func, *args, **kwargs):\n", "code": "def func_invoker(ctx):\n    return func(*resolve_args(ctx, args), **resolve_kwargs(ctx, kwargs))\nreturn func_invoker", "path": "pt\\ctx.py", "repo_name": "dogada/PyT", "stars": 32, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"Faster version of classical walker.\nPowered by closure of important params and emit generator.\n>>> fast_walk = make_walker(execute_callable, {}, None, {})\n>>> fast_walk.__name__, fast_walk.emit_gen, fast_walk.ctx\n('default_walker', None, {})\n>>> debug_walk = make_walker(execute_callable, {}, None, {}, indent_step=u'  ')\n>>> debug_walk.__name__, debug_walk.emit_gen, debug_walk.ctx, debug_walk.indent\n('indent_walker', None, {}, 0)\n\"\"\"\n", "func_signal": "def make_walker(callable_visitor, visitors, emit_gen, ctx, indent_step=u''):\n", "code": "walker = None                       # will hold reference to actual worker\ndef default_walker(obj):            # no spaces between xml elements\n    try:\n        visitor = visitors[type(obj)]\n    except:                         # KeyError\n        if hasattr(obj, '__iter__'):\n            # iterate Block, Template and subclasess, tuple, list, generator, etc\n            for i in obj:\n                walker(i)\n        elif callable(obj):\n            callable_visitor(walker, obj)\n        elif obj is not None:                               # int, string, etc convert to unicode\n            emit_gen.send(unicode(obj))\n    else:\n        visitor(walker, obj)\n\ndef indent_walker(obj):             # indent xml elements\n    if isinstance(obj, Node) and not isinstance(obj, Block):\n        if indent_walker.indent:\n            emit_gen.send(u'\\n')\n            emit_gen.send(indent_step*indent_walker.indent)\n    default_walker(obj)\nindent_walker.indent = 0\n\nwalker = indent_walker if indent_step else default_walker \nwalker.emit_gen = emit_gen\nwalker.ctx = ctx\nwalker.blocks = []                  # parent blocks stack for Super resolving\nreturn walker", "path": "pt\\dom.py", "repo_name": "dogada/PyT", "stars": 32, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"Return gettext caller marked with preprocess flag.\n\"\"\"\n", "func_signal": "def Trans(message, _gettext=i18n_stub.ugettext, **params):\n", "code": "if params:\n    # 2 stages: on preprocessor stage translate message, on runtime stage - apply params\n    def gettext_params(ctx):\n        translated_message = _gettext(message)\n        return lambda ctx: translated_message % resolve_kwargs(ctx, params)\n    return preprocess(gettext_params)\nelse:\n    return preprocess(lambda ctx: _gettext(message))", "path": "pt\\demo\\env.py", "repo_name": "dogada/PyT", "stars": 32, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"Check predicates at runtime and return matched value or None.\"\"\"\n", "func_signal": "def _runtime_maybe(ctx, variants):\n", "code": "for value, predicate in variants:\n    if callable(predicate):\n        if predicate(ctx):\n            return value\n    elif predicate:\n        return value", "path": "pt\\ctx.py", "repo_name": "dogada/PyT", "stars": 32, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"Dummy ugettext impl.\n>>> set_locale('en')\n>>> ugettext(u'Home')\nu'Home'\n>>> ugettext(u'New feature')  # untranslated text\nu'New feature'\n>>> set_locale('ru')\n>>> ugettext(u'Home')\nu'\\u041d\\u0430\\u0447\\u0430\\u043b\\u043e'\n>>> ugettext(u'Hello, %(user)s!', user=u'\\u0421\\u0442\\u0430\\u0441')\nu'\\u041f\\u0440\\u0438\\u0432\\u0435\\u0442, \\u0421\\u0442\\u0430\\u0441!'\n>>> set_locale('fr')                   # locale without transaltion\n>>> print ugettext(u'Home')\nHome\n\"\"\"\n", "func_signal": "def ugettext(message, **params):\n", "code": "_trans = _translations.get(locale, {}).get(message, message)\nreturn _trans % params if params else _trans", "path": "pt\\demo\\i18n_stub.py", "repo_name": "dogada/PyT", "stars": 32, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"URL, filename, or string --> stream\n\nThis function lets you define parsers that take any input source\n(URL, pathname to local or network file, or actual data as a string)\nand deal with it in a uniform manner.  Returned object is guaranteed\nto have all the basic stdio read methods (read, readline, readlines).\nJust .close() the object when you're done with it.\n\nIf the etag argument is supplied, it will be used as the value of an\nIf-None-Match request header.\n\nIf the modified argument is supplied, it can be a tuple of 9 integers\n(as returned by gmtime() in the standard Python time module) or a date\nstring in any format supported by feedparser. Regardless, it MUST\nbe in GMT (Greenwich Mean Time). It will be reformatted into an\nRFC 1123-compliant date and used as the value of an If-Modified-Since\nrequest header.\n\nIf the agent argument is supplied, it will be used as the value of a\nUser-Agent request header.\n\nIf the referrer argument is supplied, it will be used as the value of a\nReferer[sic] request header.\n\nIf handlers is supplied, it is a list of handlers used to build a\nurllib2 opener.\n\nif request_headers is supplied it is a dictionary of HTTP request headers\nthat will override the values generated by FeedParser.\n\"\"\"\n\n", "func_signal": "def _open_resource(url_file_stream_or_string, etag, modified, agent, referrer, handlers, request_headers):\n", "code": "if hasattr(url_file_stream_or_string, 'read'):\n    return url_file_stream_or_string\n\nif url_file_stream_or_string == '-':\n    return sys.stdin\n\nif urlparse.urlparse(url_file_stream_or_string)[0] in ('http', 'https', 'ftp', 'file', 'feed'):\n    # Deal with the feed URI scheme\n    if url_file_stream_or_string.startswith('feed:http'):\n        url_file_stream_or_string = url_file_stream_or_string[5:]\n    elif url_file_stream_or_string.startswith('feed:'):\n        url_file_stream_or_string = 'http:' + url_file_stream_or_string[5:]\n    if not agent:\n        agent = USER_AGENT\n    # test for inline user:password for basic auth\n    auth = None\n    if base64:\n        urltype, rest = urllib.splittype(url_file_stream_or_string)\n        realhost, rest = urllib.splithost(rest)\n        if realhost:\n            user_passwd, realhost = urllib.splituser(realhost)\n            if user_passwd:\n                url_file_stream_or_string = '%s://%s%s' % (urltype, realhost, rest)\n                auth = base64.standard_b64encode(user_passwd).strip()\n\n    # iri support\n    try:\n        if isinstance(url_file_stream_or_string,unicode):\n            url_file_stream_or_string = url_file_stream_or_string.encode('idna').decode('utf-8')\n        else:\n            url_file_stream_or_string = url_file_stream_or_string.decode('utf-8').encode('idna').decode('utf-8')\n    except:\n        pass\n\n    # try to open with urllib2 (to use optional headers)\n    request = _build_urllib2_request(url_file_stream_or_string, agent, etag, modified, referrer, auth, request_headers)\n    opener = apply(urllib2.build_opener, tuple(handlers + [_FeedURLHandler()]))\n    opener.addheaders = [] # RMK - must clear so we only send our custom User-Agent\n    try:\n        return opener.open(request)\n    finally:\n        opener.close() # JohnD\n\n# try to open with native open function (if url_file_stream_or_string is a filename)\ntry:\n    return open(url_file_stream_or_string, 'rb')\nexcept:\n    pass\n\n# treat url_file_stream_or_string as string\nreturn _StringIO(str(url_file_stream_or_string))", "path": "rss\\feedparser.py", "repo_name": "ifduyue/wet", "stars": 51, "license": "None", "language": "python", "size": 273}
{"docstring": "# called for each entity reference, e.g. for '&copy;', ref will be 'copy'\n", "func_signal": "def handle_entityref(self, ref):\n", "code": "if not self.elementstack: return\nif _debug: sys.stderr.write('entering handle_entityref with %s\\n' % ref)\nif ref in ('lt', 'gt', 'quot', 'amp', 'apos'):\n    text = '&%s;' % ref\nelif ref in self.entities.keys():\n    text = self.entities[ref]\n    if text.startswith('&#') and text.endswith(';'):\n        return self.handle_entityref(text)\nelse:\n    try: name2codepoint[ref]\n    except KeyError: text = '&%s;' % ref\n    else: text = unichr(name2codepoint[ref]).encode('utf-8')\nself.elementstack[-1][2].append(text)", "path": "rss\\feedparser.py", "repo_name": "ifduyue/wet", "stars": 51, "license": "None", "language": "python", "size": 273}
{"docstring": "'''Parse a string according to the Nate 8-bit date format'''\n", "func_signal": "def _parse_date_nate(dateString):\n", "code": "m = _korean_nate_date_re.match(dateString)\nif not m: return\nhour = int(m.group(5))\nampm = m.group(4)\nif (ampm == _korean_pm):\n    hour += 12\nhour = str(hour)\nif len(hour) == 1:\n    hour = '0' + hour\nw3dtfdate = '%(year)s-%(month)s-%(day)sT%(hour)s:%(minute)s:%(second)s%(zonediff)s' % \\\n            {'year': m.group(1), 'month': m.group(2), 'day': m.group(3),\\\n             'hour': hour, 'minute': m.group(6), 'second': m.group(7),\\\n             'zonediff': '+09:00'}\nif _debug: sys.stderr.write('Nate date parsed as: %s\\n' % w3dtfdate)\nreturn _parse_date_w3dtf(w3dtfdate)", "path": "rss\\feedparser.py", "repo_name": "ifduyue/wet", "stars": 51, "license": "None", "language": "python", "size": 273}
{"docstring": "# disallow urls\n", "func_signal": "def sanitize_style(self, style):\n", "code": "style=re.compile('url\\s*\\(\\s*[^\\s)]+?\\s*\\)\\s*').sub(' ',style)\n\n# gauntlet\nif not re.match(\"\"\"^([:,;#%.\\sa-zA-Z0-9!]|\\w-\\w|'[\\s\\w]+'|\"[\\s\\w]+\"|\\([\\d,\\s]+\\))*$\"\"\", style): return ''\n# This replaced a regexp that used re.match and was prone to pathological back-tracking.\nif re.sub(\"\\s*[-\\w]+\\s*:\\s*[^:;]*;?\", '', style).strip(): return ''\n\nclean = []\nfor prop,value in re.findall(\"([-\\w]+)\\s*:\\s*([^:;]*)\",style):\n  if not value: continue\n  if prop.lower() in self.acceptable_css_properties:\n      clean.append(prop + ': ' + value + ';')\n  elif prop.split('-')[0].lower() in ['background','border','margin','padding']:\n      for keyword in value.split():\n          if not keyword in self.acceptable_css_keywords and \\\n              not self.valid_css_values.match(keyword):\n              break\n      else:\n          clean.append(prop + ': ' + value + ';')\n  elif self.svgOK and prop.lower() in self.acceptable_svg_properties:\n      clean.append(prop + ': ' + value + ';')\n\nreturn ' '.join(clean)", "path": "rss\\feedparser.py", "repo_name": "ifduyue/wet", "stars": 51, "license": "None", "language": "python", "size": 273}
{"docstring": "# called for each entity reference, e.g. for '&copy;', ref will be 'copy'\n# Reconstruct the original entity reference.\n", "func_signal": "def handle_entityref(self, ref):\n", "code": "if name2codepoint.has_key(ref):\n    self.pieces.append('&%(ref)s;' % locals())\nelse:\n    self.pieces.append('&amp;%(ref)s' % locals())", "path": "rss\\feedparser.py", "repo_name": "ifduyue/wet", "stars": 51, "license": "None", "language": "python", "size": 273}
{"docstring": "'''Strips DOCTYPE from XML document, returns (rss_version, stripped_data)\n\nrss_version may be 'rss091n' or None\nstripped_data is the same XML document, minus the DOCTYPE\n'''\n", "func_signal": "def _stripDoctype(data):\n", "code": "start = re.search(_s2bytes('<\\w'), data)\nstart = start and start.start() or -1\nhead,data = data[:start+1], data[start+1:]\n\nentity_pattern = re.compile(_s2bytes(r'^\\s*<!ENTITY([^>]*?)>'), re.MULTILINE)\nentity_results=entity_pattern.findall(head)\nhead = entity_pattern.sub(_s2bytes(''), head)\ndoctype_pattern = re.compile(_s2bytes(r'^\\s*<!DOCTYPE([^>]*?)>'), re.MULTILINE)\ndoctype_results = doctype_pattern.findall(head)\ndoctype = doctype_results and doctype_results[0] or _s2bytes('')\nif doctype.lower().count(_s2bytes('netscape')):\n    version = 'rss091n'\nelse:\n    version = None\n\n# only allow in 'safe' inline entity definitions\nreplacement=_s2bytes('')\nif len(doctype_results)==1 and entity_results:\n   safe_pattern=re.compile(_s2bytes('\\s+(\\w+)\\s+\"(&#\\w+;|[^&\"]*)\"'))\n   safe_entities=filter(lambda e: safe_pattern.match(e),entity_results)\n   if safe_entities:\n       replacement=_s2bytes('<!DOCTYPE feed [\\n  <!ENTITY') + _s2bytes('>\\n  <!ENTITY ').join(safe_entities) + _s2bytes('>\\n]>')\ndata = doctype_pattern.sub(replacement, head) + data\n\nreturn version, data, dict(replacement and [(k.decode('utf-8'), v.decode('utf-8')) for k, v in safe_pattern.findall(replacement)])", "path": "rss\\feedparser.py", "repo_name": "ifduyue/wet", "stars": 51, "license": "None", "language": "python", "size": 273}
{"docstring": "# called for each character reference, e.g. for '&#160;', ref will be '160'\n", "func_signal": "def handle_charref(self, ref):\n", "code": "if not self.elementstack: return\nref = ref.lower()\nif ref in ('34', '38', '39', '60', '62', 'x22', 'x26', 'x27', 'x3c', 'x3e'):\n    text = '&#%s;' % ref\nelse:\n    if ref[0] == 'x':\n        c = int(ref[1:], 16)\n    else:\n        c = int(ref)\n    text = unichr(c).encode('utf-8')\nself.elementstack[-1][2].append(text)", "path": "rss\\feedparser.py", "repo_name": "ifduyue/wet", "stars": 51, "license": "None", "language": "python", "size": 273}
{"docstring": "'''Parse a string according to the MS SQL date format'''\n", "func_signal": "def _parse_date_mssql(dateString):\n", "code": "m = _mssql_date_re.match(dateString)\nif not m: return\nw3dtfdate = '%(year)s-%(month)s-%(day)sT%(hour)s:%(minute)s:%(second)s%(zonediff)s' % \\\n            {'year': m.group(1), 'month': m.group(2), 'day': m.group(3),\\\n             'hour': m.group(4), 'minute': m.group(5), 'second': m.group(6),\\\n             'zonediff': '+09:00'}\nif _debug: sys.stderr.write('MS SQL date parsed as: %s\\n' % w3dtfdate)\nreturn _parse_date_w3dtf(w3dtfdate)", "path": "rss\\feedparser.py", "repo_name": "ifduyue/wet", "stars": 51, "license": "None", "language": "python", "size": 273}
{"docstring": "# called for each end tag, e.g. for </pre>, tag will be 'pre'\n# Reconstruct the original end tag.\n", "func_signal": "def unknown_endtag(self, tag):\n", "code": "if tag not in self.elements_no_end_tag:\n    self.pieces.append(\"</%(tag)s>\" % locals())", "path": "rss\\feedparser.py", "repo_name": "ifduyue/wet", "stars": 51, "license": "None", "language": "python", "size": 273}
{"docstring": "# bail if ACCEPTABLE_URI_SCHEMES is empty\n", "func_signal": "def _makeSafeAbsoluteURI(base, rel=None):\n", "code": "if not ACCEPTABLE_URI_SCHEMES:\n    return _urljoin(base, rel or u'')\nif not base:\n    return rel or u''\nif not rel:\n    scheme = urlparse.urlparse(base)[0]\n    if not scheme or scheme in ACCEPTABLE_URI_SCHEMES:\n        return base\n    return u''\nuri = _urljoin(base, rel)\nif uri.strip().split(':', 1)[0] not in ACCEPTABLE_URI_SCHEMES:\n    return u''\nreturn uri", "path": "rss\\feedparser.py", "repo_name": "ifduyue/wet", "stars": 51, "license": "None", "language": "python", "size": 273}
{"docstring": "\"\"\"parse a date in yyyy/mm/dd hh:mm:ss TTT format\"\"\"\n# Fri, 2006/09/15 08:19:53 EDT\n", "func_signal": "def _parse_date_perforce(aDateString):\n", "code": "_my_date_pattern = re.compile( \\\n\tr'(\\w{,3}), (\\d{,4})/(\\d{,2})/(\\d{2}) (\\d{,2}):(\\d{2}):(\\d{2}) (\\w{,3})')\n\ndow, year, month, day, hour, minute, second, tz = \\\n\t_my_date_pattern.search(aDateString).groups()\nmonths = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\ndateString = \"%s, %s %s %s %s:%s:%s %s\" % (dow, day, months[int(month) - 1], year, hour, minute, second, tz)\ntm = rfc822.parsedate_tz(dateString)\nif tm:\n\treturn time.gmtime(rfc822.mktime_tz(tm))", "path": "rss\\feedparser.py", "repo_name": "ifduyue/wet", "stars": 51, "license": "None", "language": "python", "size": 273}
{"docstring": "'''Get the character encoding of the XML document\n\nhttp_headers is a dictionary\nxml_data is a raw string (not Unicode)\n\nThis is so much trickier than it sounds, it's not even funny.\nAccording to RFC 3023 ('XML Media Types'), if the HTTP Content-Type\nis application/xml, application/*+xml,\napplication/xml-external-parsed-entity, or application/xml-dtd,\nthe encoding given in the charset parameter of the HTTP Content-Type\ntakes precedence over the encoding given in the XML prefix within the\ndocument, and defaults to 'utf-8' if neither are specified.  But, if\nthe HTTP Content-Type is text/xml, text/*+xml, or\ntext/xml-external-parsed-entity, the encoding given in the XML prefix\nwithin the document is ALWAYS IGNORED and only the encoding given in\nthe charset parameter of the HTTP Content-Type header should be\nrespected, and it defaults to 'us-ascii' if not specified.\n\nFurthermore, discussion on the atom-syntax mailing list with the\nauthor of RFC 3023 leads me to the conclusion that any document\nserved with a Content-Type of text/* and no charset parameter\nmust be treated as us-ascii.  (We now do this.)  And also that it\nmust always be flagged as non-well-formed.  (We now do this too.)\n\nIf Content-Type is unspecified (input was local file or non-HTTP source)\nor unrecognized (server just got it totally wrong), then go by the\nencoding given in the XML prefix of the document and default to\n'iso-8859-1' as per the HTTP specification (RFC 2616).\n\nThen, assuming we didn't find a character encoding in the HTTP headers\n(and the HTTP Content-type allowed us to look in the body), we need\nto sniff the first few bytes of the XML data and try to determine\nwhether the encoding is ASCII-compatible.  Section F of the XML\nspecification shows the way here:\nhttp://www.w3.org/TR/REC-xml/#sec-guessing-no-ext-info\n\nIf the sniffed encoding is not ASCII-compatible, we need to make it\nASCII compatible so that we can sniff further into the XML declaration\nto find the encoding attribute, which will tell us the true encoding.\n\nOf course, none of this guarantees that we will be able to parse the\nfeed in the declared character encoding (assuming it was declared\ncorrectly, which many are not).  CJKCodecs and iconv_codec help a lot;\nyou should definitely install them if you can.\nhttp://cjkpython.i18n.org/\n'''\n\n", "func_signal": "def _getCharacterEncoding(http_headers, xml_data):\n", "code": "def _parseHTTPContentType(content_type):\n    '''takes HTTP Content-Type header and returns (content type, charset)\n\n    If no charset is specified, returns (content type, '')\n    If no content type is specified, returns ('', '')\n    Both return parameters are guaranteed to be lowercase strings\n    '''\n    content_type = content_type or ''\n    content_type, params = cgi.parse_header(content_type)\n    return content_type, params.get('charset', '').replace(\"'\", '')\n\nsniffed_xml_encoding = ''\nxml_encoding = ''\ntrue_encoding = ''\nhttp_content_type, http_encoding = _parseHTTPContentType(http_headers.get('content-type', http_headers.get('Content-type')))\n# Must sniff for non-ASCII-compatible character encodings before\n# searching for XML declaration.  This heuristic is defined in\n# section F of the XML specification:\n# http://www.w3.org/TR/REC-xml/#sec-guessing-no-ext-info\ntry:\n    if xml_data[:4] == _l2bytes([0x4c, 0x6f, 0xa7, 0x94]):\n        # EBCDIC\n        xml_data = _ebcdic_to_ascii(xml_data)\n    elif xml_data[:4] == _l2bytes([0x00, 0x3c, 0x00, 0x3f]):\n        # UTF-16BE\n        sniffed_xml_encoding = 'utf-16be'\n        xml_data = unicode(xml_data, 'utf-16be').encode('utf-8')\n    elif (len(xml_data) >= 4) and (xml_data[:2] == _l2bytes([0xfe, 0xff])) and (xml_data[2:4] != _l2bytes([0x00, 0x00])):\n        # UTF-16BE with BOM\n        sniffed_xml_encoding = 'utf-16be'\n        xml_data = unicode(xml_data[2:], 'utf-16be').encode('utf-8')\n    elif xml_data[:4] == _l2bytes([0x3c, 0x00, 0x3f, 0x00]):\n        # UTF-16LE\n        sniffed_xml_encoding = 'utf-16le'\n        xml_data = unicode(xml_data, 'utf-16le').encode('utf-8')\n    elif (len(xml_data) >= 4) and (xml_data[:2] == _l2bytes([0xff, 0xfe])) and (xml_data[2:4] != _l2bytes([0x00, 0x00])):\n        # UTF-16LE with BOM\n        sniffed_xml_encoding = 'utf-16le'\n        xml_data = unicode(xml_data[2:], 'utf-16le').encode('utf-8')\n    elif xml_data[:4] == _l2bytes([0x00, 0x00, 0x00, 0x3c]):\n        # UTF-32BE\n        sniffed_xml_encoding = 'utf-32be'\n        xml_data = unicode(xml_data, 'utf-32be').encode('utf-8')\n    elif xml_data[:4] == _l2bytes([0x3c, 0x00, 0x00, 0x00]):\n        # UTF-32LE\n        sniffed_xml_encoding = 'utf-32le'\n        xml_data = unicode(xml_data, 'utf-32le').encode('utf-8')\n    elif xml_data[:4] == _l2bytes([0x00, 0x00, 0xfe, 0xff]):\n        # UTF-32BE with BOM\n        sniffed_xml_encoding = 'utf-32be'\n        xml_data = unicode(xml_data[4:], 'utf-32be').encode('utf-8')\n    elif xml_data[:4] == _l2bytes([0xff, 0xfe, 0x00, 0x00]):\n        # UTF-32LE with BOM\n        sniffed_xml_encoding = 'utf-32le'\n        xml_data = unicode(xml_data[4:], 'utf-32le').encode('utf-8')\n    elif xml_data[:3] == _l2bytes([0xef, 0xbb, 0xbf]):\n        # UTF-8 with BOM\n        sniffed_xml_encoding = 'utf-8'\n        xml_data = unicode(xml_data[3:], 'utf-8').encode('utf-8')\n    else:\n        # ASCII-compatible\n        pass\n    xml_encoding_match = re.compile(_s2bytes('^<\\?.*encoding=[\\'\"](.*?)[\\'\"].*\\?>')).match(xml_data)\nexcept:\n    xml_encoding_match = None\nif xml_encoding_match:\n    xml_encoding = xml_encoding_match.groups()[0].decode('utf-8').lower()\n    if sniffed_xml_encoding and (xml_encoding in ('iso-10646-ucs-2', 'ucs-2', 'csunicode', 'iso-10646-ucs-4', 'ucs-4', 'csucs4', 'utf-16', 'utf-32', 'utf_16', 'utf_32', 'utf16', 'u16')):\n        xml_encoding = sniffed_xml_encoding\nacceptable_content_type = 0\napplication_content_types = ('application/xml', 'application/xml-dtd', 'application/xml-external-parsed-entity')\ntext_content_types = ('text/xml', 'text/xml-external-parsed-entity')\nif (http_content_type in application_content_types) or \\\n   (http_content_type.startswith('application/') and http_content_type.endswith('+xml')):\n    acceptable_content_type = 1\n    true_encoding = http_encoding or xml_encoding or 'utf-8'\nelif (http_content_type in text_content_types) or \\\n     (http_content_type.startswith('text/')) and http_content_type.endswith('+xml'):\n    acceptable_content_type = 1\n    true_encoding = http_encoding or 'us-ascii'\nelif http_content_type.startswith('text/'):\n    true_encoding = http_encoding or 'us-ascii'\nelif http_headers and (not (http_headers.has_key('content-type') or http_headers.has_key('Content-type'))):\n    true_encoding = xml_encoding or 'iso-8859-1'\nelse:\n    true_encoding = xml_encoding or 'utf-8'\n# some feeds claim to be gb2312 but are actually gb18030.\n# apparently MSIE and Firefox both do the following switch:\nif true_encoding.lower() == 'gb2312':\n    true_encoding = 'gb18030'\nreturn true_encoding, http_encoding, xml_encoding, sniffed_xml_encoding, acceptable_content_type", "path": "rss\\feedparser.py", "repo_name": "ifduyue/wet", "stars": 51, "license": "None", "language": "python", "size": 273}
{"docstring": "# called for each block of plain text, i.e. outside of any tag and\n# not containing any character or entity references\n# Store the original text verbatim.\n", "func_signal": "def handle_data(self, text):\n", "code": "if _debug: sys.stderr.write('_BaseHTMLProcessor, handle_data, text=%s\\n' % text)\nself.pieces.append(text)", "path": "rss\\feedparser.py", "repo_name": "ifduyue/wet", "stars": 51, "license": "None", "language": "python", "size": 273}
{"docstring": "'''Parse an RFC822, RFC1123, RFC2822, or asctime-style date'''\n", "func_signal": "def _parse_date_rfc822(dateString):\n", "code": "data = dateString.split()\nif data[0][-1] in (',', '.') or data[0].lower() in rfc822._daynames:\n    del data[0]\nif len(data) == 4:\n    s = data[3]\n    i = s.find('+')\n    if i > 0:\n        data[3:] = [s[:i], s[i+1:]]\n    else:\n        data.append('')\n    dateString = \" \".join(data)\n# Account for the Etc/GMT timezone by stripping 'Etc/'\nelif len(data) == 5 and data[4].lower().startswith('etc/'):\n    data[4] = data[4][4:]\n    dateString = \" \".join(data)\nif len(data) < 5:\n    dateString += ' 00:00:00 GMT'\ntm = rfc822.parsedate_tz(dateString)\nif tm:\n    return time.gmtime(rfc822.mktime_tz(tm))", "path": "rss\\feedparser.py", "repo_name": "ifduyue/wet", "stars": 51, "license": "None", "language": "python", "size": 273}
{"docstring": "# Convert a list of ints to bytes if the interpreter is Python 3\n", "func_signal": "def _l2bytes(l):\n", "code": "try:\n  if bytes is not str:\n    # In Python 2.6 and above, this call won't raise an exception\n    # but it will return bytes([65]) as '[65]' instead of 'A'\n    return bytes(l)\n  raise NameError\nexcept NameError:\n  return ''.join(map(chr, l))", "path": "rss\\feedparser.py", "repo_name": "ifduyue/wet", "stars": 51, "license": "None", "language": "python", "size": 273}
{"docstring": "# called for each character reference, e.g. for '&#160;', ref will be '160'\n# Reconstruct the original character reference.\n", "func_signal": "def handle_charref(self, ref):\n", "code": "if ref.startswith('x'):\n    value = unichr(int(ref[1:],16))\nelse:\n    value = unichr(int(ref))\n\nif value in _cp1252.keys():\n    self.pieces.append('&#%s;' % hex(ord(_cp1252[value]))[1:])\nelse:\n    self.pieces.append('&#%(ref)s;' % locals())", "path": "rss\\feedparser.py", "repo_name": "ifduyue/wet", "stars": 51, "license": "None", "language": "python", "size": 273}
{"docstring": "# called for each block of plain text, i.e. outside of any tag and\n# not containing any character or entity references\n", "func_signal": "def handle_data(self, text, escape=1):\n", "code": "if not self.elementstack: return\nif escape and self.contentparams.get('type') == 'application/xhtml+xml':\n    text = _xmlescape(text)\nself.elementstack[-1][2].append(text)", "path": "rss\\feedparser.py", "repo_name": "ifduyue/wet", "stars": 51, "license": "None", "language": "python", "size": 273}
{"docstring": "# override internal declaration handler to handle CDATA blocks\n", "func_signal": "def parse_declaration(self, i):\n", "code": "if _debug: sys.stderr.write('entering parse_declaration\\n')\nif self.rawdata[i:i+9] == '<![CDATA[':\n    k = self.rawdata.find(']]>', i)\n    if k == -1:\n        # CDATA block began but didn't finish\n        k = len(self.rawdata)\n        return k\n    self.handle_data(_xmlescape(self.rawdata[i+9:k]), 0)\n    return k+3\nelse:\n    k = self.rawdata.find('>', i)\n    if k >= 0:\n        return k+1\n    else:\n        # We have an incomplete CDATA block.\n        return k", "path": "rss\\feedparser.py", "repo_name": "ifduyue/wet", "stars": 51, "license": "None", "language": "python", "size": 273}
{"docstring": "# Convert a UTF-8 str to bytes if the interpreter is Python 3\n", "func_signal": "def _s2bytes(s):\n", "code": "try:\n  return bytes(s, 'utf8')\nexcept (NameError, TypeError):\n  # In Python 2.5 and below, bytes doesn't exist (NameError)\n  # In Python 2.6 and above, bytes and str are the same (TypeError)\n  return s", "path": "rss\\feedparser.py", "repo_name": "ifduyue/wet", "stars": 51, "license": "None", "language": "python", "size": 273}
{"docstring": "'''Parse a string according to the OnBlog 8-bit date format'''\n", "func_signal": "def _parse_date_onblog(dateString):\n", "code": "m = _korean_onblog_date_re.match(dateString)\nif not m: return\nw3dtfdate = '%(year)s-%(month)s-%(day)sT%(hour)s:%(minute)s:%(second)s%(zonediff)s' % \\\n            {'year': m.group(1), 'month': m.group(2), 'day': m.group(3),\\\n             'hour': m.group(4), 'minute': m.group(5), 'second': m.group(6),\\\n             'zonediff': '+09:00'}\nif _debug: sys.stderr.write('OnBlog date parsed as: %s\\n' % w3dtfdate)\nreturn _parse_date_w3dtf(w3dtfdate)", "path": "rss\\feedparser.py", "repo_name": "ifduyue/wet", "stars": 51, "license": "None", "language": "python", "size": 273}
{"docstring": "\"\"\"return the argument declarations of this FunctionDecl as a printable\nlist.\"\"\"\n\n", "func_signal": "def get_argument_expressions(self, include_defaults=True):\n", "code": "namedecls = []\ndefaults = [d for d in self.defaults]\nkwargs = self.kwargs\nvarargs = self.varargs\nargnames = [f for f in self.argnames]\nargnames.reverse()\nfor arg in argnames:\n    default = None\n    if kwargs:\n        arg = \"**\" + arg\n        kwargs = False\n    elif varargs:\n        arg = \"*\" + arg\n        varargs = False\n    else:\n        default = len(defaults) and defaults.pop() or None\n    if include_defaults and default:\n        namedecls.insert(0, \"%s=%s\" %\n                    (arg,\n                    pyparser.ExpressionGenerator(default).value()\n                    )\n                )\n    else:\n        namedecls.insert(0, arg)\nreturn namedecls", "path": "libs\\mako\\ast.py", "repo_name": "droot/gae-boilerplate", "stars": 40, "license": "None", "language": "python", "size": 264}
{"docstring": "\"\"\"Place a new :class:`.Template` object into this\n:class:`.TemplateLookup`, based on the given string of\n``text``.\n\n\"\"\"\n", "func_signal": "def put_string(self, uri, text):\n", "code": "self._collection[uri] = Template(\n                            text,\n                            lookup=self,\n                            uri=uri,\n                            **self.template_args)", "path": "libs\\mako\\lookup.py", "repo_name": "droot/gae-boilerplate", "stars": 40, "license": "None", "language": "python", "size": 264}
{"docstring": "# indentation counter\n", "func_signal": "def __init__(self, stream):\n", "code": "self.indent = 0\n\n# a stack storing information about why we incremented\n# the indentation counter, to help us determine if we\n# should decrement it\nself.indent_detail = []\n\n# the string of whitespace multiplied by the indent\n# counter to produce a line\nself.indentstring = \"    \"\n\n# the stream we are writing to\nself.stream = stream\n\n# a list of lines that represents a buffered \"block\" of code,\n# which can be later printed relative to an indent level\nself.line_buffer = []\n\nself.in_indent_lines = False\n\nself._reset_multi_line_flags()", "path": "libs\\mako\\pygen.py", "repo_name": "droot/gae-boilerplate", "stars": 40, "license": "None", "language": "python", "size": 264}
{"docstring": "\"\"\"return true if the given line is an 'unindentor',\nrelative to the last 'indent' event received.\n\n\"\"\"\n\n# no indentation detail has been pushed on; return False\n", "func_signal": "def _is_unindentor(self, line):\n", "code": "if len(self.indent_detail) == 0:\n    return False\n\nindentor = self.indent_detail[-1]\n\n# the last indent keyword we grabbed is not a\n# compound statement keyword; return False\nif indentor is None:\n    return False\n\n# if the current line doesnt have one of the \"unindentor\" keywords,\n# return False\nmatch = re.match(r\"^\\s*(else|elif|except|finally).*\\:\", line)\nif not match:\n    return False\n\n# whitespace matches up, we have a compound indentor,\n# and this line has an unindentor, this\n# is probably good enough\nreturn True\n\n# should we decide that its not good enough, heres\n# more stuff to check.\n#keyword = match.group(1)\n\n# match the original indent keyword\n#for crit in [\n#   (r'if|elif', r'else|elif'),\n#   (r'try', r'except|finally|else'),\n#   (r'while|for', r'else'),\n#]:\n#   if re.match(crit[0], indentor) and re.match(crit[1], keyword):\n#        return True\n\n#return False", "path": "libs\\mako\\pygen.py", "repo_name": "droot/gae-boilerplate", "stars": 40, "license": "None", "language": "python", "size": 264}
{"docstring": "# flip around the visiting of Assign so the expression gets\n            # evaluated first, in the case of a clause like \"x=x+5\" (x\n            # is undeclared)\n\n", "func_signal": "def visit_Assign(self, node):\n", "code": "            self.visit(node.value)\n            in_a = self.in_assign_targets\n            self.in_assign_targets = True\n            for n in node.targets:\n                self.visit(n)\n            self.in_assign_targets = in_a", "path": "libs\\mako\\pyparser.py", "repo_name": "droot/gae-boilerplate", "stars": 40, "license": "None", "language": "python", "size": 264}
{"docstring": "\"\"\"print a line of python, indenting it according to the current\nindent level.\n\nthis also adjusts the indentation counter according to the\ncontent of the line.\n\n\"\"\"\n\n", "func_signal": "def writeline(self, line):\n", "code": "if not self.in_indent_lines:\n    self._flush_adjusted_lines()\n    self.in_indent_lines = True\n\nif (line is None or\n    re.match(r\"^\\s*#\",line) or\n    re.match(r\"^\\s*$\", line)\n    ):\n    hastext = False\nelse:\n    hastext = True\n\nis_comment = line and len(line) and line[0] == '#'\n\n# see if this line should decrease the indentation level\nif (not is_comment and\n    (not hastext or self._is_unindentor(line))\n    ):\n\n    if self.indent > 0:\n        self.indent -=1\n        # if the indent_detail stack is empty, the user\n        # probably put extra closures - the resulting\n        # module wont compile.\n        if len(self.indent_detail) == 0:\n            raise exceptions.SyntaxException(\n                            \"Too many whitespace closures\")\n        self.indent_detail.pop()\n\nif line is None:\n    return\n\n# write the line\nself.stream.write(self._indent_line(line) + \"\\n\")\n\n# see if this line should increase the indentation level.\n# note that a line can both decrase (before printing) and\n# then increase (after printing) the indentation level.\n\nif re.search(r\":[ \\t]*(?:#.*)?$\", line):\n    # increment indentation count, and also\n    # keep track of what the keyword was that indented us,\n    # if it is a python compound statement keyword\n    # where we might have to look for an \"unindent\" keyword\n    match = re.match(r\"^\\s*(if|try|elif|while|for|with)\", line)\n    if match:\n        # its a \"compound\" keyword, so we will check for \"unindentors\"\n        indentor = match.group(1)\n        self.indent +=1\n        self.indent_detail.append(indentor)\n    else:\n        indentor = None\n        # its not a \"compound\" keyword.  but lets also\n        # test for valid Python keywords that might be indenting us,\n        # else assume its a non-indenting line\n        m2 = re.match(r\"^\\s*(def|class|else|elif|except|finally)\",\n                      line)\n        if m2:\n            self.indent += 1\n            self.indent_detail.append(indentor)", "path": "libs\\mako\\pygen.py", "repo_name": "droot/gae-boilerplate", "stars": 40, "license": "None", "language": "python", "size": 264}
{"docstring": "\"\"\"Adjust the given ``uri`` based on the given relative URI.\"\"\"\n\n", "func_signal": "def adjust_uri(self, uri, relativeto):\n", "code": "key = (uri, relativeto)\nif key in self._uri_cache:\n    return self._uri_cache[key]\n\nif uri[0] != '/':\n    if relativeto is not None:\n        v = self._uri_cache[key] = posixpath.join(\n                                    posixpath.dirname(relativeto), uri)\n    else:\n        v = self._uri_cache[key] = '/' + uri\nelse:\n    v = self._uri_cache[key] = uri\nreturn v", "path": "libs\\mako\\lookup.py", "repo_name": "droot/gae-boilerplate", "stars": 40, "license": "None", "language": "python", "size": 264}
{"docstring": "\"\"\"print a line or lines of python which already contain indentation.\n\nThe indentation of the total block of lines will be adjusted to that of\nthe current indent level.\"\"\"\n", "func_signal": "def write_indented_block(self, block):\n", "code": "self.in_indent_lines = False\nfor l in re.split(r'\\r?\\n', block):\n    self.line_buffer.append(l)", "path": "libs\\mako\\pygen.py", "repo_name": "droot/gae-boilerplate", "stars": 40, "license": "None", "language": "python", "size": 264}
{"docstring": "\"\"\"return true if the given line is part of a multi-line block,\nvia backslash or triple-quote.\"\"\"\n\n# we are only looking for explicitly joined lines here, not\n# implicit ones (i.e. brackets, braces etc.).  this is just to\n# guard against the possibility of modifying the space inside of\n# a literal multiline string with unfortunately placed\n# whitespace\n\n", "func_signal": "def _in_multi_line(self, line):\n", "code": "current_state = (self.backslashed or self.triplequoted)\n\nif re.search(r\"\\\\$\", line):\n    self.backslashed = True\nelse:\n    self.backslashed = False\n\ntriples = len(re.findall(r\"\\\"\\\"\\\"|\\'\\'\\'\", line))\nif triples == 1 or triples % 2 != 0:\n    self.triplequoted = not self.triplequoted\n\nreturn current_state", "path": "libs\\mako\\pygen.py", "repo_name": "droot/gae-boilerplate", "stars": 40, "license": "None", "language": "python", "size": 264}
{"docstring": "# flip around the visiting of Assign so the expression gets\n            # evaluated first, in the case of a clause like \"x=x+5\" (x\n            # is undeclared)\n", "func_signal": "def visitAssign(self, node, *args):\n", "code": "            self.visit(node.expr, *args)\n            for n in node.nodes:\n                self.visit(n, *args)", "path": "libs\\mako\\pyparser.py", "repo_name": "droot/gae-boilerplate", "stars": 40, "license": "None", "language": "python", "size": 264}
{"docstring": "\"\"\"remove the left-whitespace margin of a block of Python code.\"\"\"\n\n", "func_signal": "def adjust_whitespace(text):\n", "code": "state = [False, False]\n(backslashed, triplequoted) = (0, 1)\n\ndef in_multi_line(line):\n    start_state = (state[backslashed] or state[triplequoted])\n\n    if re.search(r\"\\\\$\", line):\n        state[backslashed] = True\n    else:\n        state[backslashed] = False\n\n    def match(reg, t):\n        m = re.match(reg, t)\n        if m:\n            return m, t[len(m.group(0)):]\n        else:\n            return None, t\n\n    while line:\n        if state[triplequoted]:\n            m, line = match(r\"%s\" % state[triplequoted], line)\n            if m:\n                state[triplequoted] = False\n            else:\n                m, line = match(r\".*?(?=%s|$)\" % state[triplequoted], line)\n        else:\n            m, line = match(r'#', line)\n            if m:\n                return start_state\n\n            m, line = match(r\"\\\"\\\"\\\"|\\'\\'\\'\", line)\n            if m:\n                state[triplequoted] = m.group(0)\n                continue\n\n            m, line = match(r\".*?(?=\\\"\\\"\\\"|\\'\\'\\'|#|$)\", line)\n\n    return start_state\n\ndef _indent_line(line, stripspace = ''):\n    return re.sub(r\"^%s\" % stripspace, '', line)\n\nlines = []\nstripspace = None\n\nfor line in re.split(r'\\r?\\n', text):\n    if in_multi_line(line):\n        lines.append(line)\n    else:\n        line = line.expandtabs()\n        if stripspace is None and re.search(r\"^[ \\t]*[^# \\t]\", line):\n            stripspace = re.match(r\"^([ \\t]*)\", line).group(1)\n        lines.append(_indent_line(line, stripspace))\nreturn \"\\n\".join(lines)", "path": "libs\\mako\\pygen.py", "repo_name": "droot/gae-boilerplate", "stars": 40, "license": "None", "language": "python", "size": 264}
{"docstring": "\"\"\"Convert the given ``filename`` to a URI relative to\n   this :class:`.TemplateCollection`.\"\"\"\n\n", "func_signal": "def filename_to_uri(self, filename):\n", "code": "try:\n    return self._uri_cache[filename]\nexcept KeyError:\n    value = self._relativeize(filename)\n    self._uri_cache[filename] = value\n    return value", "path": "libs\\mako\\lookup.py", "repo_name": "droot/gae-boilerplate", "stars": 40, "license": "None", "language": "python", "size": 264}
{"docstring": "\"\"\"print a series of lines of python.\"\"\"\n", "func_signal": "def writelines(self, *lines):\n", "code": "for line in lines:\n    self.writeline(line)", "path": "libs\\mako\\pygen.py", "repo_name": "droot/gae-boilerplate", "stars": 40, "license": "None", "language": "python", "size": 264}
{"docstring": "# flip around visit\n\n", "func_signal": "def visit_For(self, node):\n", "code": "            self.visit(node.iter)\n            self.visit(node.target)\n            for statement in node.body:\n                self.visit(statement)\n            for statement in node.orelse:\n                self.visit(statement)", "path": "libs\\mako\\pyparser.py", "repo_name": "droot/gae-boilerplate", "stars": 40, "license": "None", "language": "python", "size": 264}
{"docstring": "\"\"\"Return a :class:`.Template` object corresponding to the given\n``uri``.\n\n.. note:: The ``relativeto`` argument is not supported here at the moment.\n\n\"\"\"\n\n", "func_signal": "def get_template(self, uri):\n", "code": "try:\n    if self.filesystem_checks:\n        return self._check(uri, self._collection[uri])\n    else:\n        return self._collection[uri]\nexcept KeyError:\n    u = re.sub(r'^\\/+', '', uri)\n    for dir in self.directories:\n        srcfile = posixpath.normpath(posixpath.join(dir, u))\n        if os.path.isfile(srcfile):\n            return self._load(srcfile, uri)\n    else:\n        raise exceptions.TopLevelLookupException(\n                            \"Cant locate template for uri %r\" % uri)", "path": "libs\\mako\\lookup.py", "repo_name": "droot/gae-boilerplate", "stars": 40, "license": "None", "language": "python", "size": 264}
{"docstring": "\"\"\"Find a unicode representation of self.error\"\"\"\n", "func_signal": "def _init_message(self):\n", "code": "try:\n    self.message = unicode(self.error)\nexcept UnicodeError:\n    try:\n        self.message = str(self.error)\n    except UnicodeEncodeError:\n        # Fallback to args as neither unicode nor\n        # str(Exception(u'\\xe6')) work in Python < 2.6\n        self.message = self.error.args[0]\nif not isinstance(self.message, unicode):\n    self.message = unicode(self.message, 'ascii', 'replace')", "path": "libs\\mako\\exceptions.py", "repo_name": "droot/gae-boilerplate", "stars": 40, "license": "None", "language": "python", "size": 264}
{"docstring": "\"\"\"indent the given line according to the current indent level.\n\nstripspace is a string of space that will be truncated from the\nstart of the line before indenting.\"\"\"\n\n", "func_signal": "def _indent_line(self, line, stripspace=''):\n", "code": "return re.sub(r\"^%s\" % stripspace, self.indentstring\n              * self.indent, line)", "path": "libs\\mako\\pygen.py", "repo_name": "droot/gae-boilerplate", "stars": 40, "license": "None", "language": "python", "size": 264}
{"docstring": "\"\"\"Return ``True`` if this :class:`.TemplateLookup` is\ncapable of returning a :class:`.Template` object for the\ngiven ``uri``.\n\n:param uri: String URI of the template to be resolved.\n\n\"\"\"\n", "func_signal": "def has_template(self, uri):\n", "code": "try:\n    self.get_template(uri)\n    return True\nexcept exceptions.TemplateLookupException:\n    return False", "path": "libs\\mako\\lookup.py", "repo_name": "droot/gae-boilerplate", "stars": 40, "license": "None", "language": "python", "size": 264}
{"docstring": "# flip around visit\n\n", "func_signal": "def visitFor(self, node, *args):\n", "code": "            self.visit(node.list, *args)\n            self.visit(node.assign, *args)\n            self.visit(node.body, *args)", "path": "libs\\mako\\pyparser.py", "repo_name": "droot/gae-boilerplate", "stars": 40, "license": "None", "language": "python", "size": 264}
{"docstring": "\"\"\"format a traceback from sys.exc_info() into 7-item tuples,\ncontaining the regular four traceback tuple items, plus the original\ntemplate filename, the line number adjusted relative to the template\nsource, and code line from that line number of the template.\"\"\"\n\n", "func_signal": "def _init(self, trcback):\n", "code": "import mako.template\nmods = {}\nrawrecords = traceback.extract_tb(trcback)\nnew_trcback = []\nfor filename, lineno, function, line in rawrecords:\n    if not line:\n        line = ''\n    try:\n        (line_map, template_lines) = mods[filename]\n    except KeyError:\n        try:\n            info = mako.template._get_module_info(filename)\n            module_source = info.code\n            template_source = info.source\n            template_filename = info.template_filename or filename\n        except KeyError:\n            # A normal .py file (not a Template)\n            if not util.py3k:\n                try:\n                    fp = open(filename, 'rb')\n                    encoding = util.parse_encoding(fp)\n                    fp.close()\n                except IOError:\n                    encoding = None\n                if encoding:\n                    line = line.decode(encoding)\n                else:\n                    line = line.decode('ascii', 'replace')\n            new_trcback.append((filename, lineno, function, line,\n                                    None, None, None, None))\n            continue\n\n        template_ln = module_ln = 1\n        line_map = {}\n        for line in module_source.split(\"\\n\"):\n            match = re.match(r'\\s*# SOURCE LINE (\\d+)', line)\n            if match:\n                template_ln = int(match.group(1))\n            module_ln += 1\n            line_map[module_ln] = template_ln\n        template_lines = [line for line in\n                            template_source.split(\"\\n\")]\n        mods[filename] = (line_map, template_lines)\n\n    template_ln = line_map[lineno]\n    if template_ln <= len(template_lines):\n        template_line = template_lines[template_ln - 1]\n    else:\n        template_line = None\n    new_trcback.append((filename, lineno, function,\n                        line, template_filename, template_ln,\n                        template_line, template_source))\nif not self.source:\n    for l in range(len(new_trcback)-1, 0, -1):\n        if new_trcback[l][5]:\n            self.source = new_trcback[l][7]\n            self.lineno = new_trcback[l][5]\n            break\n    else:\n        if new_trcback:\n            try:\n                # A normal .py file (not a Template)\n                fp = open(new_trcback[-1][0], 'rb')\n                encoding = util.parse_encoding(fp)\n                fp.seek(0)\n                self.source = fp.read()\n                fp.close()\n                if encoding:\n                    self.source = self.source.decode(encoding)\n            except IOError:\n                self.source = ''\n            self.lineno = new_trcback[-1][1]\nreturn new_trcback", "path": "libs\\mako\\exceptions.py", "repo_name": "droot/gae-boilerplate", "stars": 40, "license": "None", "language": "python", "size": 264}
{"docstring": "\"\"\"Given a list of document lines starting with a list item,\n   finds the end of the list, breaks it up, and recursively\n   processes each list item and the remainder of the text file.\n\n   @param parent_elem: A dom element to which the content will be added\n   @param lines: a list of lines\n   @param inList: a level\n   @returns: None\"\"\"\n\n", "func_signal": "def _processList(self, parent_elem, lines, inList, listexpr, tag):\n", "code": "ul = self.doc.createElement(tag)  # ul might actually be '<ol>'\nparent_elem.appendChild(ul)\n\nlooseList = 0\n\n# Make a list of list items\nitems = []\nitem = -1\n\ni = 0  # a counter to keep track of where we are\n\nfor line in lines: \n\n    loose = 0\n    if not line.strip():\n        # If we see a blank line, this _might_ be the end of the list\n        i += 1\n        loose = 1\n\n        # Find the next non-blank line\n        for j in range(i, len(lines)):\n            if lines[j].strip():\n                next = lines[j]\n                break\n        else:\n            # There is no more text => end of the list\n            break\n\n        # Check if the next non-blank line is still a part of the list\n        if ( RE.regExp['ul'].match(next) or\n             RE.regExp['ol'].match(next) or \n             RE.regExp['tabbed'].match(next) ):\n            # get rid of any white space in the line\n            items[item].append(line.strip())\n            looseList = loose or looseList\n            continue\n        else:\n            break # found end of the list\n\n    # Now we need to detect list items (at the current level)\n    # while also detabing child elements if necessary\n\n    for expr in ['ul', 'ol', 'tabbed']:\n\n        m = RE.regExp[expr].match(line)\n        if m:\n            if expr in ['ul', 'ol']:  # We are looking at a new item\n                #if m.group(1) :\n                # Removed the check to allow for a blank line\n                # at the beginning of the list item\n                items.append([m.group(1)])\n                item += 1\n            elif expr == 'tabbed':  # This line needs to be detabbed\n                items[item].append(m.group(4)) #after the 'tab'\n\n            i += 1\n            break\n    else:\n        items[item].append(line)  # Just regular continuation\n        i += 1 # added on 2006.02.25\nelse:\n    i += 1\n\n# Add the dom elements\nfor item in items:\n    li = self.doc.createElement(\"li\")\n    ul.appendChild(li)\n\n    self._processSection(li, item, inList + 1, looseList = looseList)\n\n# Process the remaining part of the section\n\nself._processSection(parent_elem, lines[i:], inList)", "path": "markdown\\markdown.py", "repo_name": "lethain/lifeflow", "stars": 43, "license": "mit", "language": "python", "size": 512}
{"docstring": "\"\"\"Determines if a block should be replaced with an <:wHR>\"\"\"\n", "func_signal": "def _isLine(self, block):\n", "code": "if block.startswith(\"    \"): return 0  # a code block\ntext = \"\".join([x for x in block if not x.isspace()])\nif len(text) <= 2:\n    return 0\nfor pattern in ['isline1', 'isline2', 'isline3']:\n    m = RE.regExp[pattern].match(text)\n    if (m and m.group(1)):\n        return 1\nelse:\n    return 0", "path": "markdown\\markdown.py", "repo_name": "lethain/lifeflow", "stars": 43, "license": "mit", "language": "python", "size": 512}
{"docstring": "\"\"\"Given a list of document lines starting with a quote finds\n   the end of the quote, unindents it and recursively\n   processes the body of the quote and the remainder of the\n   text file.\n\n   @param parent_elem: DOM element to which the content will be added\n   @param lines: a list of lines\n   @param inList: a level\n   @returns: None \"\"\"\n\n", "func_signal": "def _processQuote(self, parent_elem, lines, inList):\n", "code": "dequoted = []\ni = 0\nblank_line = False # allow one blank line between paragraphs\nfor line in lines:\n    m = RE.regExp['quoted'].match(line)\n    if m:\n        dequoted.append(m.group(1))\n        i += 1\n        blank_line = False\n    elif not blank_line and line.strip() != '':\n        dequoted.append(line)\n        i += 1\n    elif not blank_line and line.strip() == '':\n        dequoted.append(line)\n        i += 1\n        blank_line = True\n    else:\n        break\n\nblockquote = self.doc.createElement('blockquote')\nparent_elem.appendChild(blockquote)\n\nself._processSection(blockquote, dequoted, inList)\nself._processSection(parent_elem, lines[i:], inList)", "path": "markdown\\markdown.py", "repo_name": "lethain/lifeflow", "stars": 43, "license": "mit", "language": "python", "size": 512}
{"docstring": "\"\"\"Return the document in XHTML format.\n\n@returns: A serialized XHTML body.\"\"\"\n\n", "func_signal": "def convert (self, source = None):\n", "code": "if source is not None: #Allow blank string\n    self.source = source\n\nif not self.source:\n    return u\"\"\n\ntry:\n    self.source = unicode(self.source)\nexcept UnicodeDecodeError:\n    message(CRITICAL, 'UnicodeDecodeError: Markdown only accepts unicode or ascii  input.')\n    return u\"\"\n\nfor pp in self.textPreprocessors:\n    self.source = pp.run(self.source)\n\ndoc = self._transform()\nxml = doc.toxml()\n\n\n# Return everything but the top level tag\n\nif self.stripTopLevelTags:\n    xml = xml.strip()[23:-7] + \"\\n\"\n\nfor pp in self.textPostprocessors:\n    xml = pp.run(xml)\n\nreturn (self.docType + xml).strip()", "path": "markdown\\markdown.py", "repo_name": "lethain/lifeflow", "stars": 43, "license": "mit", "language": "python", "size": 512}
{"docstring": "''' Report info about instance. Markdown always returns unicode. '''\n", "func_signal": "def __str__(self):\n", "code": "if self.source is None:\n    status = 'in which no source text has been assinged.'\nelse:\n    status = 'which contains %d chars and %d line(s) of source.'%\\\n             (len(self.source), self.source.count('\\n')+1)\nreturn 'An instance of \"%s\" %s'% (self.__class__, status)", "path": "markdown\\markdown.py", "repo_name": "lethain/lifeflow", "stars": 43, "license": "mit", "language": "python", "size": 512}
{"docstring": "\"\"\" Extracts paragraph properties from a style element. \"\"\"\n\n", "func_signal": "def extractParagraphProperties (self, style, parent=None) :\n", "code": "paraProps = ParagraphProps()\n\nname = style.getAttribute(\"style:name\")\n\nif name.startswith(\"Heading_20_\") :\n    level = name[11:]\n    try :\n        level = int(level)\n        paraProps.setHeading(level)\n    except :\n        level = 0\n\nif name == \"Title\" :\n    paraProps.setTitle(True)\n\nparaPropEl = style.getElementsByTagName(\"style:paragraph-properties\")\nif paraPropEl :\n    paraPropEl = paraPropEl[0]\n    leftMargin = paraPropEl.getAttribute(\"fo:margin-left\")\n    if leftMargin :\n        try :\n            leftMargin = float(leftMargin[:-2])\n            if leftMargin > 0.01 :\n                paraProps.setIndented(True)\n        except :\n            pass\n\ntextProps = self.extractTextProperties(style)\nif textProps.fixed :\n    paraProps.setCode(True)\n\nreturn paraProps", "path": "markdown\\odt2txt.py", "repo_name": "lethain/lifeflow", "stars": 43, "license": "mit", "language": "python", "size": 512}
{"docstring": "\"\"\" A utility function to break a list of lines upon the\n    first line that satisfied a condition.  The condition\n    argument should be a predicate function.\n    \"\"\"\n\n", "func_signal": "def _linesUntil(self, lines, condition):\n", "code": "i = -1\nfor line in lines:\n    i += 1\n    if condition(line): break\nelse:\n    i += 1\nreturn lines[:i], lines[i:]", "path": "markdown\\markdown.py", "repo_name": "lethain/lifeflow", "stars": 43, "license": "mit", "language": "python", "size": 512}
{"docstring": "\"\"\" Removes quotes from around a string \"\"\"\n", "func_signal": "def dequote(string):\n", "code": "if ( ( string.startswith('\"') and string.endswith('\"'))\n     or (string.startswith(\"'\") and string.endswith(\"'\")) ):\n    return string[1:-1]\nelse:\n    return string", "path": "markdown\\markdown.py", "repo_name": "lethain/lifeflow", "stars": 43, "license": "mit", "language": "python", "size": 512}
{"docstring": "\"\"\"Given a list of document lines starting with a code block\n   finds the end of the block, puts it into the dom verbatim\n   wrapped in (\"<pre><code>\") and recursively processes the\n   the remainder of the text file.\n\n   @param parent_elem: DOM element to which the content will be added\n   @param lines: a list of lines\n   @param inList: a level\n   @returns: None\"\"\"\n\n", "func_signal": "def _processCodeBlock(self, parent_elem, lines, inList):\n", "code": "detabbed, theRest = self.blockGuru.detectTabbed(lines)\n\npre = self.doc.createElement('pre')\ncode = self.doc.createElement('code')\nparent_elem.appendChild(pre)\npre.appendChild(code)\ntext = \"\\n\".join(detabbed).rstrip()+\"\\n\"\n#text = text.replace(\"&\", \"&amp;\")\ncode.appendChild(self.doc.createTextNode(text))\nself._processSection(parent_elem, theRest, inList)", "path": "markdown\\markdown.py", "repo_name": "lethain/lifeflow", "stars": 43, "license": "mit", "language": "python", "size": 512}
{"docstring": "\"\"\" Extracts text properties from a style element. \"\"\"\n\n", "func_signal": "def extractTextProperties (self, style, parent=None) :\n", "code": "textProps = TextProps()\n\nif parent :\n    parentProp = self.textStyles.get(parent, None)\n    if parentProp :\n        textProp = parentProp\n    \ntextPropEl = style.getElementsByTagName(\"style:text-properties\")\nif not textPropEl : return textProps\n\ntextPropEl = textPropEl[0]\n\nitalic = textPropEl.getAttribute(\"fo:font-style\")\nbold = textPropEl.getAttribute(\"fo:font-weight\")\n\ntextProps.setItalic(italic)\ntextProps.setBold(bold)\n\nif textPropEl.getAttribute(\"style:font-name\") in self.fixedFonts :\n    textProps.setFixed(True)\n\nreturn textProps", "path": "markdown\\odt2txt.py", "repo_name": "lethain/lifeflow", "stars": 43, "license": "mit", "language": "python", "size": 512}
{"docstring": "\"\"\"Transforms the Markdown text into a XHTML body document\n\n   @returns: A NanoDom Document \"\"\"\n\n# Setup the document\n\n", "func_signal": "def _transform(self):\n", "code": "self.doc = Document()\nself.top_element = self.doc.createElement(\"span\")\nself.top_element.appendChild(self.doc.createTextNode('\\n'))\nself.top_element.setAttribute('class', 'markdown')\nself.doc.appendChild(self.top_element)\n\n# Fixup the source text\ntext = self.source\ntext = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\ntext += \"\\n\\n\"\ntext = text.expandtabs(TAB_LENGTH)\n\n# Split into lines and run the preprocessors that will work with\n# self.lines\n\nself.lines = text.split(\"\\n\")\n\n# Run the pre-processors on the lines\nfor prep in self.preprocessors :\n    self.lines = prep.run(self.lines)\n\n# Create a NanoDom tree from the lines and attach it to Document\n\n\nbuffer = []\nfor line in self.lines:\n    if line.startswith(\"#\"):\n        self._processSection(self.top_element, buffer)\n        buffer = [line]\n    else:\n        buffer.append(line)\nself._processSection(self.top_element, buffer)\n\n#self._processSection(self.top_element, self.lines)\n\n# Not sure why I put this in but let's leave it for now.\nself.top_element.appendChild(self.doc.createTextNode('\\n'))\n\n# Run the post-processors\nfor postprocessor in self.postprocessors:\n    postprocessor.run(self.doc)\n\nreturn self.doc", "path": "markdown\\markdown.py", "repo_name": "lethain/lifeflow", "stars": 43, "license": "mit", "language": "python", "size": 512}
{"docstring": "\"\"\" Returns a list of descendants that pass the test function \"\"\"\n", "func_signal": "def find(self, test, depth=0):\n", "code": "matched_nodes = []\nfor child in self.childNodes:\n    if test(child):\n        matched_nodes.append(child)\n    if child.type == \"element\":\n        matched_nodes += child.find(test, depth+1)\nreturn matched_nodes", "path": "markdown\\markdown.py", "repo_name": "lethain/lifeflow", "stars": 43, "license": "mit", "language": "python", "size": 512}
{"docstring": "''' Basic html escaping '''\n", "func_signal": "def escape(self, html):\n", "code": "html = html.replace('&', '&amp;')\nhtml = html.replace('<', '&lt;')\nhtml = html.replace('>', '&gt;')\nreturn html.replace('\"', '&quot;')", "path": "markdown\\markdown.py", "repo_name": "lethain/lifeflow", "stars": 43, "license": "mit", "language": "python", "size": 512}
{"docstring": "\"\"\" Removes extra blank lines from code blocks. \"\"\"\n\n", "func_signal": "def compressCodeBlocks(self, text) :\n", "code": "lines = text.split(\"\\n\")\nbuffer = \"\"\nnumLines = len(lines)\nfor i in range(numLines) :\n    \n    if (lines[i].strip() or i == numLines-1  or i == 0 or\n        not ( lines[i-1].startswith(\"    \")\n              and lines[i+1].startswith(\"    \") ) ):\n        buffer += \"\\n\" + lines[i]\n\nreturn buffer", "path": "markdown\\odt2txt.py", "repo_name": "lethain/lifeflow", "stars": 43, "license": "mit", "language": "python", "size": 512}
{"docstring": "\"\"\" Extracts necessary font information from a font-declaration\n    element.\n    \"\"\"\n", "func_signal": "def processFontDeclarations (self, fontDecl) :\n", "code": "for fontFace in fontDecl.getElementsByTagName(\"style:font-face\") :\n    if fontFace.getAttribute(\"style:font-pitch\") == \"fixed\" :\n        self.fixedFonts.append(fontFace.getAttribute(\"style:name\"))", "path": "markdown\\odt2txt.py", "repo_name": "lethain/lifeflow", "stars": 43, "license": "mit", "language": "python", "size": 512}
{"docstring": "# note that the key is a tuple of the number of arguments,\n# and the name of the reference before the first space.\n# for example [refer year name] would be (2, u\"refer\")\n# and [absurd] would be (0, u\"absurd\")\n# the value is a function that accepts\n# entry, str, and then N additional parameters where\n# N is equal to the number of args specified in the\n# tuple\n\n# [this is my author bio][author]\n", "func_signal": "def make_syntax():\n", "code": "def author(entry, str):\n    authors = entry.authors.all()\n    if len(authors) == 1:\n        return str % authors[0].get_absolute_url()\n    else:\n        return str % u\"/author/\"\n\n# [this is the lifeflow tag ][tag lifeflow]\ndef tag(entry, str, slug):\n    t = lifeflow.models.Tag.objects.get(slug=slug)\n    return str % t.get_absolute_url()\n\n# [this is the comment with primary key 123][comment 123]\ndef comment(entry, str, pk):\n    c = lifeflow.models.Comment.objects.get(pk=int(pk))\n    return str % c.get_absolute_url()\n\n# [this is the project with slug magic-wand][project magic-wand]\ndef project(entry, str, slug):\n    p = lifeflow.models.Project.objects.get(slug=slug)\n    return str % p.get_absolute_url()\n\n\n# [remember my previous entry?][previous]\ndef previous(entry, str):\n    if entry.__class__.__name__ == \"Entry\":\n        prev = entry.get_previous_article()\n        if prev is None:\n            return None\n        return str % prev.get_absolute_url()\n\n\n# [Update: I clarified this in the next entry!][next]\ndef next(entry, str):\n    if entry.__class__.__name__ == \"Entry\":\n        nxt = entry.get_next_article()\n        if nxt is None:\n            return None\n        return str % nxt.get_absolute_url()\n\n\n# [Check out the first entry in this series][series 1]\n# [or the second entry!][series 2]\ndef series_number(entry, str, nth):\n    try:\n        nth = int(nth)\n        if nth > 0:\n            nth = nth - 1\n    except ValueError:\n        return None\n    series = entry.series.all()[0]\n    if series:\n        try:\n            e = series.entry_set.all().order_by('pub_date')[nth]\n            return str % e.get_absolute_url() \n        except IndexError:\n            return None\n\n\n# [Remember the Two-Faced Django series?][series two_faced 1]\n# [Well, I wrote that too! Go me.][series jet-survival 3]\ndef series_slug_number(entry, str, slug, nth):\n    try:\n        nth = int(nth)\n        if nth > 0:\n            nth = nth - 1\n    except ValueError:\n        return None\n    try:\n        series = lifeflow.models.Series.objects.get(slug=slug)\n    except lifeflow.models.Series.DoesNotExist:\n        return None\n    try:\n        e = series.entry_set.all()[nth]\n        return str % e.get_absolute_url() \n    except IndexError:\n        return None\n\n\n# [and check out this code!][file the_name]\n# ![ a picture that I really like][file my_pic]\n# ![ and you can abreviate it][f my_pic]\n# [this way too][f my_code]\ndef file(entry, str, name):\n    try:\n        resource = lifeflow.models.Resource.objects.get(markdown_id=name)\n        return str % resource.get_relative_url()\n    except lifeflow.models.Resource.DoesNotExist:\n        return None\n\n\n# [I like markdown][history md]\n# [and talk about why the lucky stiff occasionally][history why]\n# [but history is long... so...][h why]\n# [and a link to my svn][h svn_lethain]\ndef history(entry, str, name):\n    pass\n\nsyntax = {}\nsyntax[(0, u\"previous\")] = previous\nsyntax[(0, u\"next\")] = next\nsyntax[(0, u\"author\")] = author\nsyntax[(1, u\"file\")] = file\nsyntax[(1, u\"f\")] = file\nsyntax[(1, u\"tag\")] = tag\nsyntax[(1, u\"comment\")] = comment\nsyntax[(1, u\"project\")] = project\nsyntax[(1, u\"series\")] = series_number\nsyntax[(2, u\"series\")] = series_slug_number\n\nreturn syntax", "path": "markdown\\mdx_lifeflow.py", "repo_name": "lethain/lifeflow", "stars": 43, "license": "mit", "language": "python", "size": 512}
{"docstring": "\"\"\" Loads an ODT file. \"\"\"\n\n", "func_signal": "def load(self, filepath) :\n", "code": "zip = zipfile.ZipFile(filepath)\n\nstyles_doc = xml.dom.minidom.parseString(zip.read(\"styles.xml\"))\nself.processFontDeclarations(styles_doc.getElementsByTagName(\n    \"office:font-face-decls\")[0])\nself.processStyles(styles_doc.getElementsByTagName(\"style:style\"))\nself.processListStyles(styles_doc.getElementsByTagName(\n    \"text:list-style\"))\n\nself.content = xml.dom.minidom.parseString(zip.read(\"content.xml\"))\nself.processFontDeclarations(self.content.getElementsByTagName(\n    \"office:font-face-decls\")[0])\nself.processStyles(self.content.getElementsByTagName(\"style:style\"))\nself.processListStyles(self.content.getElementsByTagName(\n    \"text:list-style\"))", "path": "markdown\\odt2txt.py", "repo_name": "lethain/lifeflow", "stars": 43, "license": "mit", "language": "python", "size": 512}
{"docstring": "\"\"\" Converts the document to a string. \"\"\"\n", "func_signal": "def toString (self) :\n", "code": "body = self.content.getElementsByTagName(\"office:body\")[0]\ntext = self.content.getElementsByTagName(\"office:text\")[0]\n\nbuffer = u\"\"\n\n\nparagraphs = [el for el in text.childNodes\n              if el.tagName in [\"text:p\", \"text:h\",\n                                \"text:list\"]]\n\nfor paragraph in paragraphs :\n    if paragraph.tagName == \"text:list\" :\n        text = self.listToString(paragraph)\n    else :\n        text = self.paragraphToString(paragraph)\n    if text :\n        buffer += text + \"\\n\\n\"\n\nif self.footnotes :\n\n    buffer += \"--------\\n\\n\"\n    for cite, body in self.footnotes :\n        buffer += \"[^%s]: %s\\n\\n\" % (cite, body)\n\n\nreturn self.compressCodeBlocks(buffer)", "path": "markdown\\odt2txt.py", "repo_name": "lethain/lifeflow", "stars": 43, "license": "mit", "language": "python", "size": 512}
{"docstring": "# if tag has already been built, ignore\n", "func_signal": "def process_dynamic(self, ref):\n", "code": "if self.tags.has_key(ref):\n    return None\nparts = ref.split(u\" \")\nname = parts[0]\nargs = parts[1:]\nlength = len(args)\nformat = u\"[%s]: %s\" % (ref, u\"%s\")\ntry:\n    func = self.syntax[(length, name)]\n    result = func(self.entry, format, *args)\n    self.tags[ref] = True\n    return result\nexcept KeyError:\n    self.tags[ref] = False\n    to_return = None", "path": "markdown\\mdx_lifeflow.py", "repo_name": "lethain/lifeflow", "stars": 43, "license": "mit", "language": "python", "size": 512}
{"docstring": "\"\"\" Runs through \"style\" elements extracting necessary information.\n    \"\"\"\n\n", "func_signal": "def processStyles(self, styleElements) :\n", "code": "for style in styleElements :\n\n    name = style.getAttribute(\"style:name\")\n\n    if name == \"Standard\" : continue\n\n    family = style.getAttribute(\"style:family\")\n    parent = style.getAttribute(\"style:parent-style-name\")\n\n    if family == \"text\" : \n        self.textStyles[name] = self.extractTextProperties(style,\n                                                           parent)\n\n    elif family == \"paragraph\":\n        self.paragraphStyles[name] = (\n                         self.extractParagraphProperties(style,\n                                                         parent))", "path": "markdown\\odt2txt.py", "repo_name": "lethain/lifeflow", "stars": 43, "license": "mit", "language": "python", "size": 512}
{"docstring": "\"\"\"Find the offsets in a byte code which are start of lines in the source.\n\nGenerate pairs (offset, lineno) as described in Python/compile.c.\n\nCTB -- swiped from Python 2.5, module 'dis', so that earlier versions\nof Python could use the function, too.\n\"\"\"\n", "func_signal": "def findlinestarts(code):\n", "code": "byte_increments = [ord(c) for c in code.co_lnotab[0::2]]\nline_increments = [ord(c) for c in code.co_lnotab[1::2]]\n\nlastlineno = None\nlineno = code.co_firstlineno\naddr = 0\nfor byte_incr, line_incr in zip(byte_increments, line_increments):\n    if byte_incr:\n        if lineno != lastlineno:\n            yield (addr, lineno)\n            lastlineno = lineno\n        addr += byte_incr\n    lineno += line_incr\nif lineno != lastlineno:\n    yield (addr, lineno)", "path": "figleaf\\internals.py", "repo_name": "ctb/figleaf", "stars": 33, "license": "None", "language": "python", "size": 188}
{"docstring": "# transfer common-block code coverage -- if no sections are set,\n# this will be all of the code coverage info.\n\n# update our internal section dictionary with the (filename, line_no)\n# pairs from the section coverage as well.\n", "func_signal": "def update(self, trace_obj):\n", "code": "self.update_coverage(self.common, trace_obj.common.getlines())\n\nfor section_name, section_d in trace_obj.sections.items():\n    section_dict = self.sections.get(section_name, {})\n    self.update_coverage(section_dict, section_d.getlines())\n    self.sections[section_name] = section_dict", "path": "figleaf\\internals.py", "repo_name": "ctb/figleaf", "stars": 33, "license": "None", "language": "python", "size": 188}
{"docstring": "\"\"\"\nwipe out coverage info\n\"\"\"\n", "func_signal": "def clear(self):\n", "code": "self.common = self.c = Collector(self.exclude_prefix,\n                                 self.include_only_prefix)\nself.sections = {}\nself.section_name = None", "path": "figleaf\\internals.py", "repo_name": "ctb/figleaf", "stars": 33, "license": "None", "language": "python", "size": 188}
{"docstring": "\"\"\"\nAnnotate a single file with covered/uncovered lines, returning statistics.\n\"\"\"\n", "func_signal": "def annotate_file_cov(fp, line_info, coverage_info):\n", "code": "n_covered = n_lines = 0\noutput = []\n\nfor i, line in enumerate(fp):\n    is_covered = False\n    is_line = False\n\n    i += 1\n\n    if i in coverage_info:\n        is_covered = True\n        prefix = '+'\n\n        n_covered += 1\n        n_lines += 1\n    elif i in line_info:\n        prefix = '-'\n        is_line = True\n\n        n_lines += 1\n    else:\n        prefix = '0'\n\n    line = line.rstrip()\n    output.append(prefix + ' ' + line)\n\nreturn (n_covered, n_lines, output)", "path": "figleaf\\annotate_cover.py", "repo_name": "ctb/figleaf", "stars": 33, "license": "None", "language": "python", "size": 188}
{"docstring": "\"\"\"\nReturn a dictionary of sets containing section coverage information for\na specific file.  Dict keys are sections, and the dict values are\nsets containing (integer) line numbers.\n\"\"\"\n", "func_signal": "def gather_sections(self, file):\n", "code": "sections = {}\nfor section_name, c in self.sections.items():\n    s = set()\n    for filename in c.keys():\n        if filename == file:\n            lines = c[filename]\n            s.update(lines)\n    sections[section_name] = s\nreturn sections", "path": "figleaf\\internals.py", "repo_name": "ctb/figleaf", "stars": 33, "license": "None", "language": "python", "size": 188}
{"docstring": "\"\"\"\nCount 'interesting' lines of Python in a code object, where\n'interesting' is defined as 'lines that could possibly be\nexecuted'.\n\nThis is done by dissassembling the code objecte and returning\nline numbers.\n\"\"\"\n\n# clean up weird end-of-file issues\n\n", "func_signal": "def get_interesting_lines(code):\n", "code": "lines = set([ l for (o, l) in findlinestarts(code) ])\nfor const in code.co_consts:\n    if type(const) == types.CodeType:\n        lines.update(get_interesting_lines(const))\n\nreturn lines", "path": "figleaf\\internals.py", "repo_name": "ctb/figleaf", "stars": 33, "license": "None", "language": "python", "size": 188}
{"docstring": "\"\"\"\nFinalize: stop recording coverage info, save & exit.\n\"\"\"\n", "func_signal": "def finalize(self, result):\n", "code": "figleaf.stop()\n\nfigleaf.write_coverage(self.figleaf_file, append=False)", "path": "figleaf\\nose_plugin.py", "repo_name": "ctb/figleaf", "stars": 33, "license": "None", "language": "python", "size": 188}
{"docstring": "\"\"\"\nglobal trace function like g0, but only records files starting with\n'self.incl'.\n\"\"\"\n", "func_signal": "def g2(self, f, e, a):\n", "code": "if e == 'call':\n    incl = self.incl\n    if incl and f.f_code.co_filename.startswith(incl):\n        return self.t", "path": "figleaf\\internals.py", "repo_name": "ctb/figleaf", "stars": 33, "license": "None", "language": "python", "size": 188}
{"docstring": "\"\"\"\nglobal trace function like g0, but ignores files starting with\n'self.excl'.\n\"\"\"\n", "func_signal": "def g1(self, f, e, a):\n", "code": "if e == 'call':\n    path = f.f_globals.get('__file__')\n    if path is None:\n        path = f.f_code.co_filename\n\n    excl = self.excl\n    if excl and path.startswith(excl):\n        return\n\n    return self.t", "path": "figleaf\\internals.py", "repo_name": "ctb/figleaf", "stars": 33, "license": "None", "language": "python", "size": 188}
{"docstring": "\"\"\"\nglobal trace function, no exclude/include info.\n\nf == frame, e == event, a == arg        .\n\"\"\"\n", "func_signal": "def g0(self, f, e, a):\n", "code": "if e == 'call':\n    return self.t", "path": "figleaf\\internals.py", "repo_name": "ctb/figleaf", "stars": 33, "license": "None", "language": "python", "size": 188}
{"docstring": "\"\"\"\nMake sure that we can annotate files with DOS EOL characters in them.\n\"\"\"\n", "func_signal": "def test_dos_eol():\n", "code": "import figleaf, figleaf.annotate_html\n\nfigleaf.start()\nexecfile(os.path.join(thisdir, 'tst_dos_eol.py'))\nfigleaf.stop()\n\ncoverage = figleaf.get_data().gather_files()\n\ntmpdir = tempfile.mkdtemp('.figleaf')\n\ntry:\n    figleaf.annotate_html.report_as_html(coverage, tmpdir, [], {})\nfinally:\n    files = glob.glob('%s/*' % (tmpdir,))\n    for f in files:\n        os.unlink(f)\n    os.rmdir(tmpdir)", "path": "tests\\test_simple.py", "repo_name": "ctb/figleaf", "stars": 33, "license": "None", "language": "python", "size": 188}
{"docstring": "\"\"\"\nSetuptools entry point for annotate-sections; see setup.py\n\"\"\"\n", "func_signal": "def main():\n", "code": "import sys\nfrom optparse import OptionParser\n\n#### OPTIONS\n\nparser = OptionParser()\n\nparser.add_option('-c', '--coverage', nargs=1, action=\"store\",\n                  dest=\"coverage_file\", \n                  help = 'load coverage info from this file',\n                  default='.figleaf_sections')\n\n####\n\n(options, args) = parser.parse_args(sys.argv[1:])\ncoverage_file = options.coverage_file\n\nfigleaf.load_pickled_coverage(open(coverage_file, 'rb'))\ndata = internals.CoverageData(figleaf._t)\nfull_cov = data.gather_files()\n\nfor filename in args:\n    annotate_file_with_sections(filename, data, full_cov)", "path": "figleaf\\annotate_sections.py", "repo_name": "ctb/figleaf", "stars": 33, "license": "None", "language": "python", "size": 188}
{"docstring": "\"\"\"\nlocal trace function.\n\"\"\"\n", "func_signal": "def t(self, f, e, a):\n", "code": "if e is 'line':\n    filename = f.f_code.co_filename\n    if filename is self.latest:\n        self.latest_lines.add(f.f_lineno)\n    else:\n        if filename in self.c:\n            latest_lines = self.c[filename]\n            latest_lines.add(f.f_lineno)\n        else:\n            latest_lines = set([f.f_lineno])\n            self.c[filename] = latest_lines\n        self.latest = filename\n        self.latest_lines = latest_lines\nreturn self.t", "path": "figleaf\\internals.py", "repo_name": "ctb/figleaf", "stars": 33, "license": "None", "language": "python", "size": 188}
{"docstring": "\"\"\"\nConfigure: enable plugin?  And if so, where should the coverage\ninfo be placed?\n\"\"\"\n", "func_signal": "def configure(self, options, config):\n", "code": "self.conf = config\n\n# enable?\nif hasattr(options, self.enableOpt):\n    self.enabled = getattr(options, self.enableOpt)\n\n### save coverage file name, if given.\nif options.figleaf_file:\n    self.figleaf_file = options.figleaf_file\nelse:\n    self.figleaf_file = DEFAULT_COVERAGE_FILE\n\nif self.enabled and figleaf is None:\n    raise Exception(\"You must install figleaf 0.6.1 before you can use the figleafsections plugin! See http://darcs.idyll.org/~t/projects/figleaf/doc/\")", "path": "figleaf\\nose_plugin.py", "repo_name": "ctb/figleaf", "stars": 33, "license": "None", "language": "python", "size": 188}
{"docstring": "\"\"\"Add -- only works on common-block coverage, not sections.\"\"\"\n", "func_signal": "def __add__(self, other):\n", "code": "assert isinstance(other, CoverageData)\n\nme = dict(self.common)\nfor k, v in other.common.items():\n    if k in me:\n        me[k].update(v)         # add to existing coverage data\n    else:\n        me[k] = v\n\ncombined = CoverageData()\ncombined.common = me\nreturn combined", "path": "figleaf\\internals.py", "repo_name": "ctb/figleaf", "stars": 33, "license": "None", "language": "python", "size": 188}
{"docstring": "\"\"\"\nMake sure that we can parse files with '#' at the very end.\n\"\"\"\n", "func_signal": "def test_end_comment():\n", "code": "import figleaf\n\nfilename = os.path.join(thisdir, 'tst_end_comment.py')\nfigleaf.get_lines(open(filename))", "path": "tests\\test_simple.py", "repo_name": "ctb/figleaf", "stars": 33, "license": "None", "language": "python", "size": 188}
{"docstring": "\"\"\"Subtract -- only works on common-block coverage, not sections.\"\"\"\n", "func_signal": "def __sub__(self, other):\n", "code": "assert isinstance(other, CoverageData)\n\nme = dict(self.common)\nfor k, other_cov in other.common.items():\n    if k in me:\n        me_cov = me.pop(k)\n        diff = me_cov - other_cov\n        if diff:\n            me[k] = diff        # subtract from existing coverage data\n\ndiff = CoverageData()\ndiff.common = me\nreturn diff", "path": "figleaf\\internals.py", "repo_name": "ctb/figleaf", "stars": 33, "license": "None", "language": "python", "size": 188}
{"docstring": "\"\"\"\nA test generator function for many independent test cases.\n\nThis function yields tests that run coverage analysis on a bunch\nof little test files & reports mismatches against presumed \"good\" results.\n\nNote that we have to keep track of the version # here because Python\ncan change what lines are traced between versions, so we have to have\na set of good results for each Python version supported.\n\"\"\"\n", "func_signal": "def test_source_generator():\n", "code": "for filename in os.listdir(testdir):\n    if filename.startswith('tst') and filename.endswith('.py'):\n        yield compare_coverage, filename", "path": "tests\\test_regress\\__init__.py", "repo_name": "ctb/figleaf", "stars": 33, "license": "None", "language": "python", "size": 188}
{"docstring": "\"\"\"\nCommand-line function to do a basic 'coverage'-style annotation.\n\nSetuptools entry-point for figleaf2cov; see setup.py\n\"\"\"\n\n", "func_signal": "def main():\n", "code": "import sys\nimport logging\nfrom optparse import OptionParser\n\n###\n\noption_parser = OptionParser()\n\noption_parser.add_option('-x', '--exclude-patterns', action=\"store\",\n                         dest=\"exclude_patterns_file\",\n                         help=\"file containing regexp patterns to exclude\")\n\noption_parser.add_option('-f', '--files-list', action=\"store\",\n                         dest=\"files_list\",\n                         help=\"file containing filenames to report on\")\n\noption_parser.add_option('-q', '--quiet', action='store_true',\n                         dest='quiet',\n     help=\"file containig regexp patterns of files to exclude from report\")\n\noption_parser.add_option('-D', '--debug', action='store_true',\n                         dest='debug',\n                         help='Show all debugging messages')\n\noption_parser.add_option('-z', '--no-zero', action='store_true',\n                         dest='no_zero',\n                         help='Omit files with zero lines covered.')\n\n(options, args) = option_parser.parse_args()\n\nlogger.setLevel(logging.INFO)\n\nif options.quiet:\n    logger.setLevel(logging.ERROR)\n\nif options.debug:\n    logger.setLevel(logging.DEBUG)\n\n### load\n\nif not args:\n    args = ['.figleaf']\n\ncoverage = {}\nfor filename in args:\n    logger.info(\"loading coverage info from '%s'\\n\" % (filename,))\n    d = figleaf.read_coverage(filename)\n    coverage = figleaf.combine_coverage(coverage, d)\n\nfiles_list = {}\nif options.files_list:\n    files_list = annotate.read_files_list(options.files_list)\n\nif not coverage:\n    logger.warning('EXITING -- no coverage info!\\n')\n    sys.exit(-1)\n\nexclude = []\nif options.exclude_patterns_file:\n    exclude = annotate.read_exclude_patterns(options.exclude_patterns_file)\n\nreport_as_cover(coverage, exclude, files_list,\n                include_zero=not options.no_zero)", "path": "figleaf\\annotate_cover.py", "repo_name": "ctb/figleaf", "stars": 33, "license": "None", "language": "python", "size": 188}
{"docstring": "\"\"\"\nStart recording.\n\"\"\"\n", "func_signal": "def start(self):\n", "code": "if not self.started:\n    self._enable()", "path": "figleaf\\internals.py", "repo_name": "ctb/figleaf", "stars": 33, "license": "None", "language": "python", "size": 188}
{"docstring": "# adds an instance to a sorted index\n", "func_signal": "def add_to_index(self, schema, pk, index, score):\n", "code": "if isinstance(score, datetime.datetime):\n    score = score.strftime('%s.%m')\nself.conn.zadd(self._get_index_key(schema, index), pk, float(score))", "path": "sentry\\db\\backends\\redis.py", "repo_name": "dcramer/sentry-old", "stars": 41, "license": "bsd-3-clause", "language": "python", "size": 2610}
{"docstring": "# lists relations in a sorted index for base instance\n# XXX: this is O(n)+1, ugh\n", "func_signal": "def list_relations(self, from_schema, from_pk, to_schema, offset=0, limit=-1, desc=False):\n", "code": "if limit > 0:\n    end = offset+limit\nelse:\n    end = limit\n    \npk_set = self.conn.zrange(self._get_relation_key(from_schema, from_pk, to_schema), start=offset, end=end, desc=desc)\n\nreturn [(pk, self.conn.hgetall(self._get_data_key(to_schema, pk))) for pk in pk_set]", "path": "sentry\\db\\backends\\redis.py", "repo_name": "dcramer/sentry-old", "stars": 41, "license": "bsd-3-clause", "language": "python", "size": 2610}
{"docstring": "# adds a relation to a sorted index for base instance\n", "func_signal": "def add_relation(self, from_schema, from_pk, to_schema, to_pk, score):\n", "code": "if isinstance(score, datetime.datetime):\n    score = score.strftime('%s.%m')\nself.conn.zadd(self._get_relation_key(from_schema, from_pk, to_schema), to_pk, float(score))", "path": "sentry\\db\\backends\\redis.py", "repo_name": "dcramer/sentry-old", "stars": 41, "license": "bsd-3-clause", "language": "python", "size": 2610}
{"docstring": "\"\"\"\nSaves a new event to the datastore.\n\"\"\"\n", "func_signal": "def store(self, event_type, tags, data, date, time_spent, event_id, **kwargs):\n", "code": "module, class_name = event_type.rsplit('.', 1)\n\nhandler = getattr(__import__(module, {}, {}, [class_name], -1), class_name)()\n\nevent_hash = hashlib.md5('|'.join(k or '' for k in handler.get_event_hash(**data[handler.interface]))).hexdigest()\n\nevent = Event.objects.create(\n    pk=event_id,\n    type=event_type,\n    hash=event_hash,\n    date=date,\n    time_spent=time_spent,\n    tags=tags,\n)\nevent.set_meta(**data)\n\nevent_message = handler.to_string(data[handler.interface])\n\ngroup, created = Group.objects.get_or_create(\n    type=event_type,\n    hash=event_hash,\n    defaults={\n        'count': 1,\n        'time_spent': time_spent or 0,\n        'tags': tags,\n        'message': event_message,\n    }\n)\nif not created:\n    group.incr('count')\n    if time_spent:\n        group.incr('time_spent', time_spent)\n\ngroup.update(last_seen=event.date, score=group.get_score())\n\ngroup.add_relation(event, date.strftime('%s.%m'))\n\n# TODO: we need to manually add indexes per sort+filter value pair\n\nreturn event, group", "path": "sentry\\client\\base.py", "repo_name": "dcramer/sentry-old", "stars": 41, "license": "bsd-3-clause", "language": "python", "size": 2610}
{"docstring": "\"\"\"\nAttaches threads to the process on the first web request if \nthey're not already present.\n\"\"\"\n", "func_signal": "def init_threads():\n", "code": "if not hasattr(app, 'cleaner'):\n    app.cleaner = Cleaner(app)\n    app.cleaner.start()", "path": "sentry\\__init__.py", "repo_name": "dcramer/sentry-old", "stars": 41, "license": "bsd-3-clause", "language": "python", "size": 2610}
{"docstring": "# We iterate through each frame looking for a deterministic culprit\n# When one is found, we mark it as last \"best guess\" (best_guess) and then\n# check it against SENTRY_EXCLUDE_PATHS. If it isnt listed, then we\n# use this option. If nothing is found, we use the \"best guess\".\n", "func_signal": "def _get_culprit(self, traceback):\n", "code": "def contains(iterator, value):\n    for k in iterator:\n        if value.startswith(k):\n            return True\n    return False\n\nif app.config['INCLUDE_PATHS']:\n    modules = app.config['INCLUDE_PATHS']\nelse:\n    modules = []\n\nbest_guess = None\nfor tb in self._iter_tb(traceback):\n    frame = tb.tb_frame\n    try:\n        culprit = '.'.join([frame.f_globals['__name__'], frame.f_code.co_name])\n    except:\n        continue\n    if contains(modules, culprit):\n        if not (contains(app.config['EXCLUDE_PATHS'], culprit) and best_guess):\n            best_guess = culprit\n    elif best_guess:\n        break\n    \nreturn best_guess", "path": "sentry\\events.py", "repo_name": "dcramer/sentry-old", "stars": 41, "license": "bsd-3-clause", "language": "python", "size": 2610}
{"docstring": "# Import views/templatetags to ensure registration\n", "func_signal": "def run(self):\n", "code": "import sentry.collector.views\n\nupgrade()\napp.wsgi_app = WSGIErrorMiddleware(app.wsgi_app)\nif self.debug:\n    app.run(host=self.host, port=self.port, debug=self.debug)\nelse:\n    wsgi.server(eventlet.listen((self.host, self.port)), app)", "path": "sentry\\collector\\scripts\\runner.py", "repo_name": "dcramer/sentry-old", "stars": 41, "license": "bsd-3-clause", "language": "python", "size": 2610}
{"docstring": "# Import views/templatetags to ensure registration\n", "func_signal": "def run(self):\n", "code": "import sentry.web.templatetags\nimport sentry.web.views\n\nupgrade()\napp.wsgi_app = WSGIErrorMiddleware(app.wsgi_app)\n\nif self.debug:\n    app.run(host=self.host, port=self.port, debug=self.debug)\nelse:\n    wsgi.server(eventlet.listen((self.host, self.port)), app)", "path": "sentry\\web\\scripts\\runner.py", "repo_name": "dcramer/sentry-old", "stars": 41, "license": "bsd-3-clause", "language": "python", "size": 2610}
{"docstring": "\"\"\"\nReturns BASE64 ( HMAC-SHA1 (key, data) )\n\"\"\"\n", "func_signal": "def get_mac_signature(key, data, timestamp, nonce):\n", "code": "hashed = hmac.new(str(key), '%s %s %s' % (timestamp, nonce, data), hashlib.sha1)\nreturn binascii.b2a_base64(hashed.digest())[:-1]", "path": "sentry\\utils\\api.py", "repo_name": "dcramer/sentry-old", "stars": 41, "license": "bsd-3-clause", "language": "python", "size": 2610}
{"docstring": "\"\"\"\nTruncates a string after a certain number of chars.\n\nArgument: Number of chars to truncate after.\n\"\"\"\n", "func_signal": "def truncatechars(value, arg):\n", "code": "try:\n    length = int(arg)\nexcept ValueError: # Invalid literal for int().\n    return value # Fail silently.\nif len(value) > length:\n    return value[:length] + '...'\nreturn value", "path": "sentry\\web\\templatetags.py", "repo_name": "dcramer/sentry-old", "stars": 41, "license": "bsd-3-clause", "language": "python", "size": 2610}
{"docstring": "# generates a pk and sets the values\n", "func_signal": "def add(self, schema, **values):\n", "code": "pk = self.generate_key(schema)\ntable = model_map[schema]\nquery = table.insert(id=pk, **values)\nself.engine.execute(query)\nreturn pk", "path": "sentry\\db\\backends\\sqlalchemy\\backend.py", "repo_name": "dcramer/sentry-old", "stars": 41, "license": "bsd-3-clause", "language": "python", "size": 2610}
{"docstring": "# generates a pk and sets the values\n", "func_signal": "def add(self, schema, **values):\n", "code": "pk = self.generate_key(schema)\nif values:\n    self.set(schema, pk, **values)\nreturn pk", "path": "sentry\\db\\backends\\redis.py", "repo_name": "dcramer/sentry-old", "stars": 41, "license": "bsd-3-clause", "language": "python", "size": 2610}
{"docstring": "# lists relations in a sorted index for base instance\n# XXX: this is O(n)+1, ugh\n", "func_signal": "def list_relations(self, from_schema, from_pk, to_schema, offset=0, limit=-1, desc=False):\n", "code": "if limit > 0:\n    end = offset+limit\nelse:\n    end = limit\n    \npk_set = self.conn.zrange(self._get_relation_key(from_schema, from_pk, to_schema), start=offset, end=end, desc=desc)\n\nreturn [(pk, self.conn.hgetall(self._get_data_key(to_schema, pk))) for pk in pk_set]", "path": "sentry\\db\\backends\\sqlalchemy\\backend.py", "repo_name": "dcramer/sentry-old", "stars": 41, "license": "bsd-3-clause", "language": "python", "size": 2610}
{"docstring": "\"\"\"\nReturns context_lines before and after lineno from file.\nReturns (pre_context_lineno, pre_context, context_line, post_context).\n\"\"\"\n", "func_signal": "def _get_lines_from_file(self, filename, lineno, context_lines, loader=None, module_name=None):\n", "code": "source = None\nif loader is not None and hasattr(loader, \"get_source\"):\n    source = loader.get_source(module_name)\n    if source is not None:\n        source = source.splitlines()\nif source is None:\n    try:\n        f = open(filename)\n        try:\n            source = f.readlines()\n        finally:\n            f.close()\n    except (OSError, IOError):\n        pass\nif source is None:\n    return None, [], None, []\n\nencoding = 'ascii'\nfor line in source[:2]:\n    # File coding may be specified. Match pattern from PEP-263\n    # (http://www.python.org/dev/peps/pep-0263/)\n    match = re.search(r'coding[:=]\\s*([-\\w.]+)', line)\n    if match:\n        encoding = match.group(1)\n        break\nsource = [unicode(sline, encoding, 'replace') for sline in source]\n\nlower_bound = max(0, lineno - context_lines)\nupper_bound = lineno + context_lines\n\npre_context = [line.strip('\\n') for line in source[lower_bound:lineno]]\ncontext_line = source[lineno].strip('\\n')\npost_context = [line.strip('\\n') for line in source[lineno+1:upper_bound]]\n\nreturn lower_bound, pre_context, context_line, post_context", "path": "sentry\\events.py", "repo_name": "dcramer/sentry-old", "stars": 41, "license": "bsd-3-clause", "language": "python", "size": 2610}
{"docstring": "# tags and culprit are special cased and not stored with the\n# default metadata\n", "func_signal": "def capture(self, **kwargs):\n", "code": "return {\n    'culprit': None,\n    'tags': self.get_tags(**kwargs),\n    self.interface: self.get_data(**kwargs),\n}", "path": "sentry\\events.py", "repo_name": "dcramer/sentry-old", "stars": 41, "license": "bsd-3-clause", "language": "python", "size": 2610}
{"docstring": "# adds an instance to a sorted index\n", "func_signal": "def add_to_index(self, schema, pk, index, score):\n", "code": "if isinstance(score, datetime.datetime):\n    score = score.strftime('%s.%m')\nself.conn.zadd(self._get_index_key(schema, index), pk, float(score))", "path": "sentry\\db\\backends\\sqlalchemy\\backend.py", "repo_name": "dcramer/sentry-old", "stars": 41, "license": "bsd-3-clause", "language": "python", "size": 2610}
{"docstring": "# adds a relation to a sorted index for base instance\n", "func_signal": "def add_relation(self, from_schema, from_pk, to_schema, to_pk, score):\n", "code": "if isinstance(score, datetime.datetime):\n    score = score.strftime('%s.%m')\nself.conn.zadd(self._get_relation_key(from_schema, from_pk, to_schema), to_pk, float(score))", "path": "sentry\\db\\backends\\sqlalchemy\\backend.py", "repo_name": "dcramer/sentry-old", "stars": 41, "license": "bsd-3-clause", "language": "python", "size": 2610}
{"docstring": "# redis is so blazing fast that we have to artificially inflate dates\n# or tests wont pass :)\n", "func_signal": "def test_create(self):\n", "code": "now = datetime.datetime.now()\n\nevent, group = app.client.store(\n    'sentry.events.Message',\n    tags=(\n        ('server', 'foo.bar'),\n        ('culprit', 'foo.bar.zoo.baz'),\n    ),\n    date=now,\n    time_spent=53,\n    data={\n        'sentry.interfaces.Message': {\n            'message': 'hello world'\n        }\n    },\n    event_id='foobar',\n)\ngroup_id = group.pk\n\nself.assertTrue(group.pk)\nself.assertEquals(group.type, 'sentry.events.Message')\nself.assertEquals(group.time_spent, 53)\nself.assertEquals(group.count, 1)\nself.assertEquals(len(group.tags), 2)\n\ntag = group.tags[0]\n\nself.assertEquals(tag[0], 'server')\nself.assertEquals(tag[1], 'foo.bar')\n\ntag = group.tags[1]\n\nself.assertEquals(tag[0], 'culprit')\nself.assertEquals(tag[1], 'foo.bar.zoo.baz')\n\nevents = group.get_relations(Event)\n\nself.assertEquals(len(events), 1)\n\nevent = events[0]\n\nself.assertEquals(event.time_spent, group.time_spent)\nself.assertEquals(event.type, group.type)\nself.assertEquals(event.date, group.last_seen)\nself.assertEquals(len(event.tags), 2)\n\ntag = event.tags[0]\n\nself.assertEquals(tag[0], 'server')\nself.assertEquals(tag[1], 'foo.bar')\n\ntag = event.tags[1]\n\nself.assertEquals(tag[0], 'culprit')\nself.assertEquals(tag[1], 'foo.bar.zoo.baz')\n\nevent, group = app.client.store(\n    'sentry.events.Message',\n    tags=(\n        ('server', 'foo.bar'),\n    ),\n    date=now + datetime.timedelta(seconds=1),\n    time_spent=100,\n    data={\n        'sentry.interfaces.Message': {\n            'message': 'hello world',\n        },\n    },\n    event_id='foobar2',\n)\n\nself.assertEquals(group.pk, group_id)\nself.assertEquals(group.count, 2)\nself.assertEquals(group.time_spent, 153)\nself.assertEquals(len(group.tags), 2)\n\ntag = group.tags[0]\n\nself.assertEquals(tag[0], 'server')\nself.assertEquals(tag[1], 'foo.bar')\n\ntag = group.tags[1]\n\nself.assertEquals(tag[0], 'culprit')\nself.assertEquals(tag[1], 'foo.bar.zoo.baz')\n\nevents = group.get_relations(Event, desc=False)\n\nself.assertEquals(len(events), 2)\n\nevent = events[1]\n\nself.assertEquals(event.time_spent, 100)\nself.assertEquals(event.type, group.type)\nself.assertEquals(group.last_seen, event.date)\nself.assertEquals(len(event.tags), 1)\n\ntag = event.tags[0]\n\nself.assertEquals(tag[0], 'server')\nself.assertEquals(tag[1], 'foo.bar')\n\ntags = Tag.objects.order_by('-count')\n\nself.assertEquals(len(tags), 2, tags)\n\ngroups = Group.objects.all()\n\nself.assertEquals(len(groups), 1)\n\nevent, group = app.client.store(\n    'sentry.events.Message',\n    tags=(\n        ('server', 'foo.bar'),\n    ),\n    date=now + datetime.timedelta(seconds=1),\n    time_spent=100,\n    data={\n        'sentry.interfaces.Message': {\n            'message': 'hello world 2',\n        },\n    },\n    event_id='foobar2',\n)\n\nself.assertNotEquals(group.pk, group_id)\nself.assertEquals(group.count, 1)\nself.assertEquals(group.time_spent, 100)\nself.assertEquals(len(group.tags), 1)\n\ntag = group.tags[0]\n\nself.assertEquals(tag[0], 'server')\nself.assertEquals(tag[1], 'foo.bar')\n\nevents = group.get_relations(Event, desc=False)\n\nself.assertEquals(len(events), 1)\n\nevent = events[0]\n\nself.assertEquals(event.time_spent, 100)\nself.assertEquals(event.type, group.type)\nself.assertEquals(group.last_seen, event.date)\nself.assertEquals(len(event.tags), 1)\n\ntag = event.tags[0]\n\nself.assertEquals(tag[0], 'server')\nself.assertEquals(tag[1], 'foo.bar')\n\ntags = Tag.objects.order_by('-count')\n\nself.assertEquals(len(tags), 2, tags)\n\ntag = tags[0]\n\nself.assertEquals(tag.key, 'server')\nself.assertEquals(tag.value, 'foo.bar')\nself.assertEquals(tag.count, 2)\n\ntag = tags[1]\n\nself.assertEquals(tag.key, 'culprit')\nself.assertEquals(tag.value, 'foo.bar.zoo.baz')\nself.assertEquals(tag.count, 1)\n\ngroups = Group.objects.all()\n\nself.assertEquals(len(groups), 2)", "path": "tests\\test_events.py", "repo_name": "dcramer/sentry-old", "stars": 41, "license": "bsd-3-clause", "language": "python", "size": 2610}
{"docstring": "# Adding model 'RedmineIssue'\n", "func_signal": "def forwards(self, orm):\n", "code": "        db.create_table('sentry_redmine_redmineissue', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('group', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['sentry.GroupedMessage'])),\n            ('issue_id', self.gf('django.db.models.fields.PositiveIntegerField')()),\n        ))\n        db.send_create_signal('sentry_redmine', ['RedmineIssue'])", "path": "sentry\\plugins\\sentry_redmine\\migrations\\0001_initial.py", "repo_name": "dcramer/sentry-old", "stars": 41, "license": "bsd-3-clause", "language": "python", "size": 2610}
{"docstring": "\"\"\"\nTakes two datetime objects and returns the time between d and now\nas a nicely formatted string, e.g. \"10 minutes\".  If d occurs after now,\nthen \"0 minutes\" is returned.\n\nUnits used are years, months, weeks, days, hours, and minutes.\nSeconds and microseconds are ignored.  Up to two adjacent units will be\ndisplayed.  For example, \"2 weeks, 3 days\" and \"1 year, 3 months\" are\npossible outputs, but \"2 weeks, 3 hours\" and \"1 year, 5 days\" are not.\n\nAdapted from http://blog.natbat.co.uk/archive/2003/Jun/14/time_since\n\"\"\"\n", "func_signal": "def timesince(d, now=None):\n", "code": "if not d:\n    return 'Never'\n\nif d < datetime.datetime.now() - datetime.timedelta(days=5):\n    return d.date()\n\nchunks = (\n  (60 * 60 * 24 * 365, lambda n: ngettext('year', 'years', n)),\n  (60 * 60 * 24 * 30, lambda n: ngettext('month', 'months', n)),\n  (60 * 60 * 24 * 7, lambda n : ngettext('week', 'weeks', n)),\n  (60 * 60 * 24, lambda n : ngettext('day', 'days', n)),\n  (60 * 60, lambda n: ngettext('hour', 'hours', n)),\n  (60, lambda n: ngettext('minute', 'minutes', n))\n)\n# Convert datetime.date to datetime.datetime for comparison.\nif not isinstance(d, datetime.datetime):\n    d = datetime.datetime(d.year, d.month, d.day)\nif now and not isinstance(now, datetime.datetime):\n    now = datetime.datetime(now.year, now.month, now.day)\n\nif not now:\n    if d.tzinfo:\n        now = datetime.datetime.now(d.tzinfo)\n    else:\n        now = datetime.datetime.now()\n\n# ignore microsecond part of 'd' since we removed it from 'now'\ndelta = now - (d - datetime.timedelta(0, 0, d.microsecond))\nsince = delta.days * 24 * 60 * 60 + delta.seconds\nif since <= 0:\n    # d is in the future compared to now, stop processing.\n    return d.date()\nfor i, (seconds, name) in enumerate(chunks):\n    count = since // seconds\n    if count != 0:\n        break\ns = gettext('%(number)d %(type)s', number=count, type=name(count))\n\nif s == '0 minutes':\n    return 'Just now'\nif s == '1 day':\n    return 'Yesterday'\nreturn s + ' ago'", "path": "sentry\\web\\templatetags.py", "repo_name": "dcramer/sentry-old", "stars": 41, "license": "bsd-3-clause", "language": "python", "size": 2610}
{"docstring": "# prepare the graphs as when the source is generated, but without\n# actually generating the source.\n", "func_signal": "def generate_graphs_for_llinterp(self, db=None):\n", "code": "if db is None:\n    db = self.build_database()\ngraphs = db.all_graphs()\ndb.gctransformer.prepare_inline_helpers(graphs)\nfor node in db.containerlist:\n    if hasattr(node, 'funcgens'):\n        for funcgen in node.funcgens:\n            funcgen.patch_graph(copy_graph=False)\nreturn db", "path": "translator\\c\\genc.py", "repo_name": "diffoperator/Sypy", "stars": 45, "license": "None", "language": "python", "size": 9544}
{"docstring": "# For FuncNode instances, use the python source filename (relative to\n# the top directory):\n", "func_signal": "def getbasecfilefornode(self, node, basecname):\n", "code": "def invent_nice_name(g):\n    # Lookup the filename from the function.\n    # However, not all FunctionGraph objs actually have a \"func\":\n    if hasattr(g, 'func'):\n        if g.filename.endswith('.py'):\n            localpath = py.path.local(g.filename)\n            pypkgpath = localpath.pypkgpath()\n            if pypkgpath:\n                relpypath =  localpath.relto(pypkgpath)\n                return relpypath.replace('.py', '.c')\n    return None\nif hasattr(node.obj, 'graph'):\n    name = invent_nice_name(node.obj.graph)\n    if name is not None:\n        return name\nelif node._funccodegen_owner is not None:\n    name = invent_nice_name(node._funccodegen_owner.graph)\n    if name is not None:\n        return \"data_\" + name\nreturn basecname", "path": "translator\\c\\genc.py", "repo_name": "diffoperator/Sypy", "stars": 45, "license": "None", "language": "python", "size": 9544}
{"docstring": "# assume that the file and stream objects are only visible in the\n# thread that runs __del__, so no race condition should be possible\n", "func_signal": "def __del__(self):\n", "code": "self.clear_all_weakrefs()\nif self.stream is not None:\n    self.enqueue_for_destruction(self.space, W_File.destructor,\n                                 'close() method of ')", "path": "module\\_file\\interp_file.py", "repo_name": "diffoperator/Sypy", "stars": 45, "license": "None", "language": "python", "size": 9544}
{"docstring": "\"\"\"\nLookup a field which ends with ``suffix`` following the rpython struct\ninheritance hierarchy (i.e., looking both at ``val`` and\n``val['*_super']``, recursively.\n\"\"\"\n", "func_signal": "def lookup(val, suffix):\n", "code": "try:\n    return find_field_with_suffix(val, suffix)\nexcept KeyError:\n    baseobj = find_field_with_suffix(val, '_super')\n    return lookup(baseobj, suffix)", "path": "tool\\gdb_pypy.py", "repo_name": "diffoperator/Sypy", "stars": 45, "license": "None", "language": "python", "size": 9544}
{"docstring": "# in the RPython program, 'del os.environ[key]' is redirected here\n", "func_signal": "def delitem(self, obj, key):\n", "code": "if r_getenv(key) is None:\n    raise KeyError\nr_unsetenv(key)", "path": "rpython\\module\\ll_os_environ.py", "repo_name": "diffoperator/Sypy", "stars": 45, "license": "None", "language": "python", "size": 9544}
{"docstring": "\"\"\"Return self.stream or raise an app-level ValueError if missing\n(i.e. if the file is closed).\"\"\"\n", "func_signal": "def getstream(self):\n", "code": "self.check_closed()\nreturn self.stream", "path": "module\\_file\\interp_file.py", "repo_name": "diffoperator/Sypy", "stars": 45, "license": "None", "language": "python", "size": 9544}
{"docstring": "# XXX check that the entrypoint has the correct\n# signature:  list-of-strings -> int\n", "func_signal": "def getentrypointptr(self):\n", "code": "bk = self.translator.annotator.bookkeeper\nreturn getfunctionptr(bk.getdesc(self.entrypoint).getuniquegraph())", "path": "translator\\c\\genc.py", "repo_name": "diffoperator/Sypy", "stars": 45, "license": "None", "language": "python", "size": 9544}
{"docstring": "# in the RPython program reads of 'os.environ[key]' are redirected here\n", "func_signal": "def getitem(self, obj, key):\n", "code": "result = r_getenv(key)\nif result is None:\n    raise KeyError\nreturn result", "path": "rpython\\module\\ll_os_environ.py", "repo_name": "diffoperator/Sypy", "stars": 45, "license": "None", "language": "python", "size": 9544}
{"docstring": "# See above\n", "func_signal": "def test_byref_pointerpointer(self):\n", "code": "from ctypes import c_short, c_uint, c_int, c_long, pointer, POINTER, byref\n\nLPLPINT = POINTER(POINTER(c_int))\nLPLPINT.from_param(byref(pointer(c_int(42))))\n\nraises(TypeError, LPLPINT.from_param, byref(pointer(c_short(22))))\nif c_int != c_long:\n    raises(TypeError, LPLPINT.from_param, byref(pointer(c_long(22))))\nraises(TypeError, LPLPINT.from_param, byref(pointer(c_uint(22))))", "path": "module\\test_lib_pypy\\ctypes_tests\\test_parameters.py", "repo_name": "diffoperator/Sypy", "stars": 45, "license": "None", "language": "python", "size": 9544}
{"docstring": "\"\"\"NOT_RPYTHON\"\"\"\n# must remove the interp-level name '__missing__' after it has\n# been used...  otherwise, some code is not happy about seeing\n# this code object twice\n", "func_signal": "def setup_after_space_initialization(self):\n", "code": "space = self.space\nspace.getattr(self, space.wrap('defaultdict'))  # force importing\nspace.delattr(self, space.wrap('__missing__'))", "path": "module\\_collections\\__init__.py", "repo_name": "diffoperator/Sypy", "stars": 45, "license": "None", "language": "python", "size": 9544}
{"docstring": "# we need a concrete gcpolicy to do this\n", "func_signal": "def collect_compilation_info(self, db):\n", "code": "self.merge_eci(db.gcpolicy.compilation_info())\n\nall = []\nfor node in self.db.globalcontainers():\n    eci = node.compilation_info()\n    if eci:\n        all.append(eci)\nself.merge_eci(*all)", "path": "translator\\c\\genc.py", "repo_name": "diffoperator/Sypy", "stars": 45, "license": "None", "language": "python", "size": 9544}
{"docstring": "#assert self._module\n", "func_signal": "def cleanup(self):\n", "code": "if isinstance(self._module, isolate.Isolate):\n    isolate.close_isolate(self._module)", "path": "translator\\c\\genc.py", "repo_name": "diffoperator/Sypy", "stars": 45, "license": "None", "language": "python", "size": 9544}
{"docstring": "# when .argtypes is not set, calling a function with a certain\n# set of parameters should not prevent another call with\n# another set.\n", "func_signal": "def test_multiple_signature(self):\n", "code": "from ctypes import CDLL, byref\nimport conftest\ndll = CDLL(str(conftest.sofile))\nfunc = dll._testfunc_p_p\n\n# This is call has too many arguments\nassert func(None, 1) == 0\n\n# This one is normal\nassert func(None) == 0", "path": "module\\test_lib_pypy\\ctypes_tests\\test_parameters.py", "repo_name": "diffoperator/Sypy", "stars": 45, "license": "None", "language": "python", "size": 9544}
{"docstring": "# dependency injection, for tests\n", "func_signal": "def __init__(self, gdb=None):\n", "code": "if gdb is None:\n    import gdb\nself.gdb = gdb\nCommand.__init__(self, \"rpy_type\", self.gdb.COMMAND_NONE)", "path": "tool\\gdb_pypy.py", "repo_name": "diffoperator/Sypy", "stars": 45, "license": "None", "language": "python", "size": 9544}
{"docstring": "\"\"\"Flush otherfile before reading from self.\"\"\"\n", "func_signal": "def _when_reading_first_flush(self, otherfile):\n", "code": "self.stream = streamio.CallbackReadFilter(self.stream,\n                                          otherfile._try_to_flush)", "path": "module\\_file\\interp_file.py", "repo_name": "diffoperator/Sypy", "stars": 45, "license": "None", "language": "python", "size": 9544}
{"docstring": "# generate the start-up code and put it into a function\n", "func_signal": "def gen_startupcode(f, database):\n", "code": "print >> f, 'char *RPython_StartupCode(void) {'\nprint >> f, '\\tchar *error = NULL;'\nfor line in database.gcpolicy.gc_startup_code():\n    print >> f,\"\\t\" + line\n\n# put float infinities in global constants, we should not have so many of them for now to make\n# a table+loop preferable\nfor dest, value in database.late_initializations:\n    print >> f, \"\\t%s = %s;\" % (dest, value)\n\nfirsttime = True\nfor node in database.containerlist:\n    lines = list(node.startupcode())\n    if lines:\n        if firsttime:\n            firsttime = False\n        else:\n            print >> f, '\\tif (error) return error;'\n        for line in lines:\n            print >> f, '\\t'+line\nprint >> f, '\\treturn error;'\nprint >> f, '}'", "path": "translator\\c\\genc.py", "repo_name": "diffoperator/Sypy", "stars": 45, "license": "None", "language": "python", "size": 9544}
{"docstring": "\"\"\"__exit__(*excinfo) -> None. Closes the file.\"\"\"\n", "func_signal": "def file__exit__(self, __args__):\n", "code": "self.space.call_method(self, \"close\")\n# can't return close() value\nreturn None", "path": "module\\_file\\interp_file.py", "repo_name": "diffoperator/Sypy", "stars": 45, "license": "None", "language": "python", "size": 9544}
{"docstring": "\"\"\"readinto() -> Undocumented.  Don't use this; it may go away.\"\"\"\n# XXX not the most efficient solution as it doesn't avoid the copying\n", "func_signal": "def file_readinto(self, w_rwbuffer):\n", "code": "space = self.space\nrwbuffer = space.rwbuffer_w(w_rwbuffer)\nw_data = self.file_read(rwbuffer.getlength())\ndata = space.str_w(w_data)\nrwbuffer.setslice(0, data)\nreturn space.wrap(len(data))", "path": "module\\_file\\interp_file.py", "repo_name": "diffoperator/Sypy", "stars": 45, "license": "None", "language": "python", "size": 9544}
{"docstring": "# The from_param class method of POINTER(typ) classes accepts what is\n# returned by byref(obj), it type(obj) == typ\n", "func_signal": "def test_byref_pointer(self):\n", "code": "from ctypes import c_short, c_uint, c_int, c_long, pointer, POINTER, byref\nLPINT = POINTER(c_int)\n\nLPINT.from_param(byref(c_int(42)))\n\nraises(TypeError, LPINT.from_param, byref(c_short(22)))\nif c_int != c_long:\n    raises(TypeError, LPINT.from_param, byref(c_long(22)))\nraises(TypeError, LPINT.from_param, byref(c_uint(22)))", "path": "module\\test_lib_pypy\\ctypes_tests\\test_parameters.py", "repo_name": "diffoperator/Sypy", "stars": 45, "license": "None", "language": "python", "size": 9544}
{"docstring": "#\n# All declarations\n#\n", "func_signal": "def gen_readable_parts_of_main_c_file(f, database, preimplementationlines=[]):\n", "code": "print >> f\ngen_structdef(f, database)\nprint >> f\ngen_forwarddecl(f, database)\n\n#\n# Implementation of functions and global structures and arrays\n#\nprint >> f\nprint >> f, '/***********************************************************/'\nprint >> f, '/***  Implementations                                    ***/'\nprint >> f\nprint >> f, '#define PYPY_FILE_NAME \"%s\"' % os.path.basename(f.name)\nfor line in preimplementationlines:\n    print >> f, line\nprint >> f, '#include \"src/g_include.h\"'\nprint >> f\nblank = True\ngraphs = database.all_graphs()\ndatabase.gctransformer.prepare_inline_helpers(graphs)\nfor node in database.globalcontainers():\n    if blank:\n        print >> f\n        blank = False\n    for line in node.implementation():\n        print >> f, line\n        blank = True", "path": "translator\\c\\genc.py", "repo_name": "diffoperator/Sypy", "stars": 45, "license": "None", "language": "python", "size": 9544}
{"docstring": "\"\"\"Parse a textual representation of patches and return a list of patch\nobjects.\n\nArgs:\n  textline: Text representation of patches.\n\nReturns:\n  Array of Patch objects.\n\nRaises:\n  ValueError: If invalid input.\n\"\"\"\n", "func_signal": "def patch_fromText(self, textline):\n", "code": "if type(textline) == unicode:\n  # Patches should be composed of a subset of ascii chars, Unicode not\n  # required.  If this encode raises UnicodeEncodeError, patch is invalid.\n  textline = textline.encode(\"ascii\")\npatches = []\nif not textline:\n  return patches\ntext = textline.split('\\n')\nwhile len(text) != 0:\n  m = re.match(\"^@@ -(\\d+),?(\\d*) \\+(\\d+),?(\\d*) @@$\", text[0])\n  if not m:\n    raise ValueError(\"Invalid patch string: \" + text[0])\n  patch = patch_obj()\n  patches.append(patch)\n  patch.start1 = int(m.group(1))\n  if m.group(2) == '':\n    patch.start1 -= 1\n    patch.length1 = 1\n  elif m.group(2) == '0':\n    patch.length1 = 0\n  else:\n    patch.start1 -= 1\n    patch.length1 = int(m.group(2))\n\n  patch.start2 = int(m.group(3))\n  if m.group(4) == '':\n    patch.start2 -= 1\n    patch.length2 = 1\n  elif m.group(4) == '0':\n    patch.length2 = 0\n  else:\n    patch.start2 -= 1\n    patch.length2 = int(m.group(4))\n\n  del text[0]\n\n  while len(text) != 0:\n    if text[0]:\n      sign = text[0][0]\n    else:\n      sign = ''\n    line = urllib.unquote(text[0][1:])\n    line = line.decode(\"utf-8\")\n    if sign == '+':\n      # Insertion.\n      patch.diffs.append((self.DIFF_INSERT, line))\n    elif sign == '-':\n      # Deletion.\n      patch.diffs.append((self.DIFF_DELETE, line))\n    elif sign == ' ':\n      # Minor equality.\n      patch.diffs.append((self.DIFF_EQUAL, line))\n    elif sign == '@':\n      # Start of next patch.\n      break\n    elif sign == '':\n      # Blank line?  Whatever.\n      pass\n    else:\n      # WTF?\n      raise ValueError(\"Invalid patch mode: '%s'\\n%s\" % (sign, line))\n    del text[0]\nreturn patches", "path": "python2\\diff_match_patch.py", "repo_name": "rdt1/hardcore", "stars": 34, "license": "apache-2.0", "language": "python", "size": 2571}
{"docstring": "\"\"\"Reorder and merge like edit sections.  Merge equalities.\nAny edit section can move as long as it doesn't cross an equality.\n\nArgs:\n  diffs: Array of diff tuples.\n\"\"\"\n", "func_signal": "def diff_cleanupMerge(self, diffs):\n", "code": "diffs.append((self.DIFF_EQUAL, ''))  # Add a dummy entry at the end.\npointer = 0\ncount_delete = 0\ncount_insert = 0\ntext_delete = ''\ntext_insert = ''\nwhile pointer < len(diffs):\n  if diffs[pointer][0] == self.DIFF_INSERT:\n    count_insert += 1\n    text_insert += diffs[pointer][1]\n    pointer += 1\n  elif diffs[pointer][0] == self.DIFF_DELETE:\n    count_delete += 1\n    text_delete += diffs[pointer][1]\n    pointer += 1\n  elif diffs[pointer][0] == self.DIFF_EQUAL:\n    # Upon reaching an equality, check for prior redundancies.\n    if count_delete + count_insert > 1:\n      if count_delete != 0 and count_insert != 0:\n        # Factor out any common prefixies.\n        commonlength = self.diff_commonPrefix(text_insert, text_delete)\n        if commonlength != 0:\n          x = pointer - count_delete - count_insert - 1\n          if x >= 0 and diffs[x][0] == self.DIFF_EQUAL:\n            diffs[x] = (diffs[x][0], diffs[x][1] +\n                        text_insert[:commonlength])\n          else:\n            diffs.insert(0, (self.DIFF_EQUAL, text_insert[:commonlength]))\n            pointer += 1\n          text_insert = text_insert[commonlength:]\n          text_delete = text_delete[commonlength:]\n        # Factor out any common suffixies.\n        commonlength = self.diff_commonSuffix(text_insert, text_delete)\n        if commonlength != 0:\n          diffs[pointer] = (diffs[pointer][0], text_insert[-commonlength:] +\n              diffs[pointer][1])\n          text_insert = text_insert[:-commonlength]\n          text_delete = text_delete[:-commonlength]\n      # Delete the offending records and add the merged ones.\n      if count_delete == 0:\n        diffs[pointer - count_insert : pointer] = [\n            (self.DIFF_INSERT, text_insert)]\n      elif count_insert == 0:\n        diffs[pointer - count_delete : pointer] = [\n            (self.DIFF_DELETE, text_delete)]\n      else:\n        diffs[pointer - count_delete - count_insert : pointer] = [\n            (self.DIFF_DELETE, text_delete),\n            (self.DIFF_INSERT, text_insert)]\n      pointer = pointer - count_delete - count_insert + 1\n      if count_delete != 0:\n        pointer += 1\n      if count_insert != 0:\n        pointer += 1\n    elif pointer != 0 and diffs[pointer - 1][0] == self.DIFF_EQUAL:\n      # Merge this equality with the previous one.\n      diffs[pointer - 1] = (diffs[pointer - 1][0],\n                            diffs[pointer - 1][1] + diffs[pointer][1])\n      del diffs[pointer]\n    else:\n      pointer += 1\n\n    count_insert = 0\n    count_delete = 0\n    text_delete = ''\n    text_insert = ''\n\nif diffs[-1][1] == '':\n  diffs.pop()  # Remove the dummy entry at the end.\n\n# Second pass: look for single edits surrounded on both sides by equalities\n# which can be shifted sideways to eliminate an equality.\n# e.g: A<ins>BA</ins>C -> <ins>AB</ins>AC\nchanges = False\npointer = 1\n# Intentionally ignore the first and last element (don't need checking).\nwhile pointer < len(diffs) - 1:\n  if (diffs[pointer - 1][0] == self.DIFF_EQUAL and\n      diffs[pointer + 1][0] == self.DIFF_EQUAL):\n    # This is a single edit surrounded by equalities.\n    if diffs[pointer][1].endswith(diffs[pointer - 1][1]):\n      # Shift the edit over the previous equality.\n      diffs[pointer] = (diffs[pointer][0],\n          diffs[pointer - 1][1] +\n          diffs[pointer][1][:-len(diffs[pointer - 1][1])])\n      diffs[pointer + 1] = (diffs[pointer + 1][0],\n                            diffs[pointer - 1][1] + diffs[pointer + 1][1])\n      del diffs[pointer - 1]\n      changes = True\n    elif diffs[pointer][1].startswith(diffs[pointer + 1][1]):\n      # Shift the edit over the next equality.\n      diffs[pointer - 1] = (diffs[pointer - 1][0],\n                            diffs[pointer - 1][1] + diffs[pointer + 1][1])\n      diffs[pointer] = (diffs[pointer][0],\n          diffs[pointer][1][len(diffs[pointer + 1][1]):] +\n          diffs[pointer + 1][1])\n      del diffs[pointer + 1]\n      changes = True\n  pointer += 1\n\n# If shifts were made, the diff needs reordering and another shift sweep.\nif changes:\n  self.diff_cleanupMerge(diffs)", "path": "python2\\diff_match_patch.py", "repo_name": "rdt1/hardcore", "stars": 34, "license": "apache-2.0", "language": "python", "size": 2571}
{"docstring": "\"\"\"Compute and return the destination text (all equalities and insertions).\n\nArgs:\n  diffs: Array of diff tuples.\n\nReturns:\n  Destination text.\n\"\"\"\n", "func_signal": "def diff_text2(self, diffs):\n", "code": "text = []\nfor (op, data) in diffs:\n  if op != self.DIFF_DELETE:\n    text.append(data)\nreturn \"\".join(text)", "path": "python2\\diff_match_patch.py", "repo_name": "rdt1/hardcore", "stars": 34, "license": "apache-2.0", "language": "python", "size": 2571}
{"docstring": "\"\"\"Locate the best instance of 'pattern' in 'text' near 'loc' using the\nBitap algorithm.\n\nArgs:\n  text: The text to search.\n  pattern: The pattern to search for.\n  loc: The location to search around.\n\nReturns:\n  Best match index or -1.\n\"\"\"\n# Python doesn't have a maxint limit, so ignore this check.\n#if self.Match_MaxBits != 0 and len(pattern) > self.Match_MaxBits:\n#  raise ValueError(\"Pattern too long for this application.\")\n\n# Initialise the alphabet.\n", "func_signal": "def match_bitap(self, text, pattern, loc):\n", "code": "s = self.match_alphabet(pattern)\n\ndef match_bitapScore(e, x):\n  \"\"\"Compute and return the score for a match with e errors and x location.\n  Accesses loc and pattern through being a closure.\n\n  Args:\n    e: Number of errors in match.\n    x: Location of match.\n\n  Returns:\n    Overall score for match (0.0 = good, 1.0 = bad).\n  \"\"\"\n  accuracy = float(e) / len(pattern)\n  proximity = abs(loc - x)\n  if not self.Match_Distance:\n    # Dodge divide by zero error.\n    return proximity and 1.0 or accuracy\n  return accuracy + (proximity / float(self.Match_Distance))\n\n# Highest score beyond which we give up.\nscore_threshold = self.Match_Threshold\n# Is there a nearby exact match? (speedup)\nbest_loc = text.find(pattern, loc)\nif best_loc != -1:\n  score_threshold = min(match_bitapScore(0, best_loc), score_threshold)\n  # What about in the other direction? (speedup)\n  best_loc = text.rfind(pattern, loc + len(pattern))\n  if best_loc != -1:\n    score_threshold = min(match_bitapScore(0, best_loc), score_threshold)\n\n# Initialise the bit arrays.\nmatchmask = 1 << (len(pattern) - 1)\nbest_loc = -1\n\nbin_max = len(pattern) + len(text)\n# Empty initialization added to appease pychecker.\nlast_rd = None\nfor d in xrange(len(pattern)):\n  # Scan for the best match each iteration allows for one more error.\n  # Run a binary search to determine how far from 'loc' we can stray at\n  # this error level.\n  bin_min = 0\n  bin_mid = bin_max\n  while bin_min < bin_mid:\n    if match_bitapScore(d, loc + bin_mid) <= score_threshold:\n      bin_min = bin_mid\n    else:\n      bin_max = bin_mid\n    bin_mid = (bin_max - bin_min) // 2 + bin_min\n\n  # Use the result from this iteration as the maximum for the next.\n  bin_max = bin_mid\n  start = max(1, loc - bin_mid + 1)\n  finish = min(loc + bin_mid, len(text)) + len(pattern)\n\n  rd = [0] * (finish + 2)\n  rd[finish + 1] = (1 << d) - 1\n  for j in xrange(finish, start - 1, -1):\n    if len(text) <= j - 1:\n      # Out of range.\n      charMatch = 0\n    else:\n      charMatch = s.get(text[j - 1], 0)\n    if d == 0:  # First pass: exact match.\n      rd[j] = ((rd[j + 1] << 1) | 1) & charMatch\n    else:  # Subsequent passes: fuzzy match.\n      rd[j] = (((rd[j + 1] << 1) | 1) & charMatch) | (\n          ((last_rd[j + 1] | last_rd[j]) << 1) | 1) | last_rd[j + 1]\n    if rd[j] & matchmask:\n      score = match_bitapScore(d, j - 1)\n      # This match will almost certainly be better than any existing match.\n      # But check anyway.\n      if score <= score_threshold:\n        # Told you so.\n        score_threshold = score\n        best_loc = j - 1\n        if best_loc > loc:\n          # When passing loc, don't exceed our current distance from loc.\n          start = max(1, 2 * loc - best_loc)\n        else:\n          # Already passed loc, downhill from here on in.\n          break\n  # No hope for a (better) match at greater error levels.\n  if match_bitapScore(d + 1, loc) > score_threshold:\n    break\n  last_rd = rd\nreturn best_loc", "path": "python2\\diff_match_patch.py", "repo_name": "rdt1/hardcore", "stars": 34, "license": "apache-2.0", "language": "python", "size": 2571}
{"docstring": "\"\"\"loc is a location in text1, compute and return the equivalent location\nin text2.  e.g. \"The cat\" vs \"The big cat\", 1->1, 5->8\n\nArgs:\n  diffs: Array of diff tuples.\n  loc: Location within text1.\n\nReturns:\n  Location within text2.\n\"\"\"\n", "func_signal": "def diff_xIndex(self, diffs, loc):\n", "code": "chars1 = 0\nchars2 = 0\nlast_chars1 = 0\nlast_chars2 = 0\nfor x in xrange(len(diffs)):\n  (op, text) = diffs[x]\n  if op != self.DIFF_INSERT:  # Equality or deletion.\n    chars1 += len(text)\n  if op != self.DIFF_DELETE:  # Equality or insertion.\n    chars2 += len(text)\n  if chars1 > loc:  # Overshot the location.\n    break\n  last_chars1 = chars1\n  last_chars2 = chars2\n\nif len(diffs) != x and diffs[x][0] == self.DIFF_DELETE:\n  # The location was deleted.\n  return last_chars2\n# Add the remaining len(character).\nreturn last_chars2 + (loc - last_chars1)", "path": "python2\\diff_match_patch.py", "repo_name": "rdt1/hardcore", "stars": 34, "license": "apache-2.0", "language": "python", "size": 2571}
{"docstring": "\"\"\"Find the differences between two texts.  Assumes that the texts do not\n  have any common prefix or suffix.\n\nArgs:\n  text1: Old string to be diffed.\n  text2: New string to be diffed.\n  checklines: Speedup flag.  If false, then don't run a line-level diff\n    first to identify the changed areas.\n    If true, then run a faster, slightly less optimal diff.\n  deadline: Time when the diff should be complete by.\n\nReturns:\n  Array of changes.\n\"\"\"\n", "func_signal": "def diff_compute(self, text1, text2, checklines, deadline):\n", "code": "if not text1:\n  # Just add some text (speedup).\n  return [(self.DIFF_INSERT, text2)]\n\nif not text2:\n  # Just delete some text (speedup).\n  return [(self.DIFF_DELETE, text1)]\n\nif len(text1) > len(text2):\n  (longtext, shorttext) = (text1, text2)\nelse:\n  (shorttext, longtext) = (text1, text2)\ni = longtext.find(shorttext)\nif i != -1:\n  # Shorter text is inside the longer text (speedup).\n  diffs = [(self.DIFF_INSERT, longtext[:i]), (self.DIFF_EQUAL, shorttext),\n           (self.DIFF_INSERT, longtext[i + len(shorttext):])]\n  # Swap insertions for deletions if diff is reversed.\n  if len(text1) > len(text2):\n    diffs[0] = (self.DIFF_DELETE, diffs[0][1])\n    diffs[2] = (self.DIFF_DELETE, diffs[2][1])\n  return diffs\n\nif len(shorttext) == 1:\n  # Single character string.\n  # After the previous speedup, the character can't be an equality.\n  return [(self.DIFF_DELETE, text1), (self.DIFF_INSERT, text2)]\n\n# Check to see if the problem can be split in two.\nhm = self.diff_halfMatch(text1, text2)\nif hm:\n  # A half-match was found, sort out the return data.\n  (text1_a, text1_b, text2_a, text2_b, mid_common) = hm\n  # Send both pairs off for separate processing.\n  diffs_a = self.diff_main(text1_a, text2_a, checklines, deadline)\n  diffs_b = self.diff_main(text1_b, text2_b, checklines, deadline)\n  # Merge the results.\n  return diffs_a + [(self.DIFF_EQUAL, mid_common)] + diffs_b\n\nif checklines and len(text1) > 100 and len(text2) > 100:\n  return self.diff_lineMode(text1, text2, deadline)\n\nreturn self.diff_bisect(text1, text2, deadline)", "path": "python2\\diff_match_patch.py", "repo_name": "rdt1/hardcore", "stars": 34, "license": "apache-2.0", "language": "python", "size": 2571}
{"docstring": "\"\"\"Compute a list of patches to turn text1 into text2.\nUse diffs if provided, otherwise compute it ourselves.\nThere are four ways to call this function, depending on what data is\navailable to the caller:\nMethod 1:\na = text1, b = text2\nMethod 2:\na = diffs\nMethod 3 (optimal):\na = text1, b = diffs\nMethod 4 (deprecated, use method 3):\na = text1, b = text2, c = diffs\n\nArgs:\n  a: text1 (methods 1,3,4) or Array of diff tuples for text1 to\n      text2 (method 2).\n  b: text2 (methods 1,4) or Array of diff tuples for text1 to\n      text2 (method 3) or undefined (method 2).\n  c: Array of diff tuples for text1 to text2 (method 4) or\n      undefined (methods 1,2,3).\n\nReturns:\n  Array of Patch objects.\n\"\"\"\n", "func_signal": "def patch_make(self, a, b=None, c=None):\n", "code": "text1 = None\ndiffs = None\n# Note that texts may arrive as 'str' or 'unicode'.\nif isinstance(a, basestring) and isinstance(b, basestring) and c is None:\n  # Method 1: text1, text2\n  # Compute diffs from text1 and text2.\n  text1 = a\n  diffs = self.diff_main(text1, b, True)\n  if len(diffs) > 2:\n    self.diff_cleanupSemantic(diffs)\n    self.diff_cleanupEfficiency(diffs)\nelif isinstance(a, list) and b is None and c is None:\n  # Method 2: diffs\n  # Compute text1 from diffs.\n  diffs = a\n  text1 = self.diff_text1(diffs)\nelif isinstance(a, basestring) and isinstance(b, list) and c is None:\n  # Method 3: text1, diffs\n  text1 = a\n  diffs = b\nelif (isinstance(a, basestring) and isinstance(b, basestring) and\n      isinstance(c, list)):\n  # Method 4: text1, text2, diffs\n  # text2 is not used.\n  text1 = a\n  diffs = c\nelse:\n  raise ValueError(\"Unknown call format to patch_make.\")\n\nif not diffs:\n  return []  # Get rid of the None case.\npatches = []\npatch = patch_obj()\nchar_count1 = 0  # Number of characters into the text1 string.\nchar_count2 = 0  # Number of characters into the text2 string.\nprepatch_text = text1  # Recreate the patches to determine context info.\npostpatch_text = text1\nfor x in xrange(len(diffs)):\n  (diff_type, diff_text) = diffs[x]\n  if len(patch.diffs) == 0 and diff_type != self.DIFF_EQUAL:\n    # A new patch starts here.\n    patch.start1 = char_count1\n    patch.start2 = char_count2\n  if diff_type == self.DIFF_INSERT:\n    # Insertion\n    patch.diffs.append(diffs[x])\n    patch.length2 += len(diff_text)\n    postpatch_text = (postpatch_text[:char_count2] + diff_text +\n                      postpatch_text[char_count2:])\n  elif diff_type == self.DIFF_DELETE:\n    # Deletion.\n    patch.length1 += len(diff_text)\n    patch.diffs.append(diffs[x])\n    postpatch_text = (postpatch_text[:char_count2] +\n                      postpatch_text[char_count2 + len(diff_text):])\n  elif (diff_type == self.DIFF_EQUAL and\n        len(diff_text) <= 2 * self.Patch_Margin and\n        len(patch.diffs) != 0 and len(diffs) != x + 1):\n    # Small equality inside a patch.\n    patch.diffs.append(diffs[x])\n    patch.length1 += len(diff_text)\n    patch.length2 += len(diff_text)\n\n  if (diff_type == self.DIFF_EQUAL and\n      len(diff_text) >= 2 * self.Patch_Margin):\n    # Time for a new patch.\n    if len(patch.diffs) != 0:\n      self.patch_addContext(patch, prepatch_text)\n      patches.append(patch)\n      patch = patch_obj()\n      # Unlike Unidiff, our patch lists have a rolling context.\n      # http://code.google.com/p/google-diff-match-patch/wiki/Unidiff\n      # Update prepatch text & pos to reflect the application of the\n      # just completed patch.\n      prepatch_text = postpatch_text\n      char_count1 = char_count2\n\n  # Update the current character count.\n  if diff_type != self.DIFF_INSERT:\n    char_count1 += len(diff_text)\n  if diff_type != self.DIFF_DELETE:\n    char_count2 += len(diff_text)\n\n# Pick up the leftover patch if not empty.\nif len(patch.diffs) != 0:\n  self.patch_addContext(patch, prepatch_text)\n  patches.append(patch)\nreturn patches", "path": "python2\\diff_match_patch.py", "repo_name": "rdt1/hardcore", "stars": 34, "license": "apache-2.0", "language": "python", "size": 2571}
{"docstring": "\"\"\"Inits a diff_match_patch object with default settings.\nRedefine these in your program to override the defaults.\n\"\"\"\n\n# Number of seconds to map a diff before giving up (0 for infinity).\n", "func_signal": "def __init__(self):\n", "code": "self.Diff_Timeout = 1.0\n# Cost of an empty edit operation in terms of edit characters.\nself.Diff_EditCost = 4\n# At what point is no match declared (0.0 = perfection, 1.0 = very loose).\nself.Match_Threshold = 0.5\n# How far to search for a match (0 = exact location, 1000+ = broad match).\n# A match this many characters away from the expected location will add\n# 1.0 to the score (0.0 is a perfect match).\nself.Match_Distance = 1000\n# When deleting a large block of text (over ~64 characters), how close do\n# the contents have to be to match the expected contents. (0.0 = perfection,\n# 1.0 = very loose).  Note that Match_Threshold controls how closely the\n# end points of a delete need to match.\nself.Patch_DeleteThreshold = 0.5\n# Chunk size for context length.\nself.Patch_Margin = 4\n\n# The number of bits in an int.\n# Python has no maximum, thus to disable patch splitting set to 0.\n# However to avoid long patches in certain pathological cases, use 32.\n# Multiple short patches (using native ints) are much faster than long ones.\nself.Match_MaxBits = 32", "path": "python2\\diff_match_patch.py", "repo_name": "rdt1/hardcore", "stars": 34, "license": "apache-2.0", "language": "python", "size": 2571}
{"docstring": "\"\"\"Reduce the number of edits by eliminating operationally trivial\nequalities.\n\nArgs:\n  diffs: Array of diff tuples.\n\"\"\"\n", "func_signal": "def diff_cleanupEfficiency(self, diffs):\n", "code": "changes = False\nequalities = []  # Stack of indices where equalities are found.\nlastequality = None  # Always equal to diffs[equalities[-1]][1]\npointer = 0  # Index of current position.\npre_ins = False  # Is there an insertion operation before the last equality.\npre_del = False  # Is there a deletion operation before the last equality.\npost_ins = False  # Is there an insertion operation after the last equality.\npost_del = False  # Is there a deletion operation after the last equality.\nwhile pointer < len(diffs):\n  if diffs[pointer][0] == self.DIFF_EQUAL:  # Equality found.\n    if (len(diffs[pointer][1]) < self.Diff_EditCost and\n        (post_ins or post_del)):\n      # Candidate found.\n      equalities.append(pointer)\n      pre_ins = post_ins\n      pre_del = post_del\n      lastequality = diffs[pointer][1]\n    else:\n      # Not a candidate, and can never become one.\n      equalities = []\n      lastequality = None\n\n    post_ins = post_del = False\n  else:  # An insertion or deletion.\n    if diffs[pointer][0] == self.DIFF_DELETE:\n      post_del = True\n    else:\n      post_ins = True\n\n    # Five types to be split:\n    # <ins>A</ins><del>B</del>XY<ins>C</ins><del>D</del>\n    # <ins>A</ins>X<ins>C</ins><del>D</del>\n    # <ins>A</ins><del>B</del>X<ins>C</ins>\n    # <ins>A</del>X<ins>C</ins><del>D</del>\n    # <ins>A</ins><del>B</del>X<del>C</del>\n\n    if lastequality and ((pre_ins and pre_del and post_ins and post_del) or\n                         ((len(lastequality) < self.Diff_EditCost / 2) and\n                          (pre_ins + pre_del + post_ins + post_del) == 3)):\n      # Duplicate record.\n      diffs.insert(equalities[-1], (self.DIFF_DELETE, lastequality))\n      # Change second copy to insert.\n      diffs[equalities[-1] + 1] = (self.DIFF_INSERT,\n          diffs[equalities[-1] + 1][1])\n      equalities.pop()  # Throw away the equality we just deleted.\n      lastequality = None\n      if pre_ins and pre_del:\n        # No changes made which could affect previous entry, keep going.\n        post_ins = post_del = True\n        equalities = []\n      else:\n        if len(equalities):\n          equalities.pop()  # Throw away the previous equality.\n        if len(equalities):\n          pointer = equalities[-1]\n        else:\n          pointer = -1\n        post_ins = post_del = False\n      changes = True\n  pointer += 1\n\nif changes:\n  self.diff_cleanupMerge(diffs)", "path": "python2\\diff_match_patch.py", "repo_name": "rdt1/hardcore", "stars": 34, "license": "apache-2.0", "language": "python", "size": 2571}
{"docstring": "\"\"\"Given the original text1, and an encoded string which describes the\noperations required to transform text1 into text2, compute the full diff.\n\nArgs:\n  text1: Source string for the diff.\n  delta: Delta text.\n\nReturns:\n  Array of diff tuples.\n\nRaises:\n  ValueError: If invalid input.\n\"\"\"\n", "func_signal": "def diff_fromDelta(self, text1, delta):\n", "code": "if type(delta) == unicode:\n  # Deltas should be composed of a subset of ascii chars, Unicode not\n  # required.  If this encode raises UnicodeEncodeError, delta is invalid.\n  delta = delta.encode(\"ascii\")\ndiffs = []\npointer = 0  # Cursor in text1\ntokens = delta.split(\"\\t\")\nfor token in tokens:\n  if token == \"\":\n    # Blank tokens are ok (from a trailing \\t).\n    continue\n  # Each token begins with a one character parameter which specifies the\n  # operation of this token (delete, insert, equality).\n  param = token[1:]\n  if token[0] == \"+\":\n    param = urllib.unquote(param).decode(\"utf-8\")\n    diffs.append((self.DIFF_INSERT, param))\n  elif token[0] == \"-\" or token[0] == \"=\":\n    try:\n      n = int(param)\n    except ValueError:\n      raise ValueError(\"Invalid number in diff_fromDelta: \" + param)\n    if n < 0:\n      raise ValueError(\"Negative number in diff_fromDelta: \" + param)\n    text = text1[pointer : pointer + n]\n    pointer += n\n    if token[0] == \"=\":\n      diffs.append((self.DIFF_EQUAL, text))\n    else:\n      diffs.append((self.DIFF_DELETE, text))\n  else:\n    # Anything else is an error.\n    raise ValueError(\"Invalid diff operation in diff_fromDelta: \" +\n        token[0])\nif pointer != len(text1):\n  raise ValueError(\n      \"Delta length (%d) does not equal source text length (%d).\" %\n     (pointer, len(text1)))\nreturn diffs", "path": "python2\\diff_match_patch.py", "repo_name": "rdt1/hardcore", "stars": 34, "license": "apache-2.0", "language": "python", "size": 2571}
{"docstring": "\"\"\"Compute and return the source text (all equalities and deletions).\n\nArgs:\n  diffs: Array of diff tuples.\n\nReturns:\n  Source text.\n\"\"\"\n", "func_signal": "def diff_text1(self, diffs):\n", "code": "text = []\nfor (op, data) in diffs:\n  if op != self.DIFF_INSERT:\n    text.append(data)\nreturn \"\".join(text)", "path": "python2\\diff_match_patch.py", "repo_name": "rdt1/hardcore", "stars": 34, "license": "apache-2.0", "language": "python", "size": 2571}
{"docstring": "\"\"\"Find the differences between two texts.  Simplifies the problem by\n  stripping any common prefix or suffix off the texts before diffing.\n\nArgs:\n  text1: Old string to be diffed.\n  text2: New string to be diffed.\n  checklines: Optional speedup flag.  If present and false, then don't run\n    a line-level diff first to identify the changed areas.\n    Defaults to true, which does a faster, slightly less optimal diff.\n  deadline: Optional time when the diff should be complete by.  Used\n    internally for recursive calls.  Users should set DiffTimeout instead.\n\nReturns:\n  Array of changes.\n\"\"\"\n# Set a deadline by which time the diff must be complete.\n", "func_signal": "def diff_main(self, text1, text2, checklines=True, deadline=None):\n", "code": "if deadline == None:\n  # Unlike in most languages, Python counts time in seconds.\n  if self.Diff_Timeout <= 0:\n    deadline = sys.maxint\n  else:\n    deadline = time.time() + self.Diff_Timeout\n\n# Check for null inputs.\nif text1 == None or text2 == None:\n  raise ValueError(\"Null inputs. (diff_main)\")\n\n# Check for equality (speedup).\nif text1 == text2:\n  if text1:\n    return [(self.DIFF_EQUAL, text1)]\n  return []\n\n# Trim off common prefix (speedup).\ncommonlength = self.diff_commonPrefix(text1, text2)\ncommonprefix = text1[:commonlength]\ntext1 = text1[commonlength:]\ntext2 = text2[commonlength:]\n\n# Trim off common suffix (speedup).\ncommonlength = self.diff_commonSuffix(text1, text2)\nif commonlength == 0:\n  commonsuffix = ''\nelse:\n  commonsuffix = text1[-commonlength:]\n  text1 = text1[:-commonlength]\n  text2 = text2[:-commonlength]\n\n# Compute the diff on the middle block.\ndiffs = self.diff_compute(text1, text2, checklines, deadline)\n\n# Restore the prefix and suffix.\nif commonprefix:\n  diffs[:0] = [(self.DIFF_EQUAL, commonprefix)]\nif commonsuffix:\n  diffs.append((self.DIFF_EQUAL, commonsuffix))\nself.diff_cleanupMerge(diffs)\nreturn diffs", "path": "python2\\diff_match_patch.py", "repo_name": "rdt1/hardcore", "stars": 34, "license": "apache-2.0", "language": "python", "size": 2571}
{"docstring": "\"\"\"Locate the best instance of 'pattern' in 'text' near 'loc'.\n\nArgs:\n  text: The text to search.\n  pattern: The pattern to search for.\n  loc: The location to search around.\n\nReturns:\n  Best match index or -1.\n\"\"\"\n# Check for null inputs.\n", "func_signal": "def match_main(self, text, pattern, loc):\n", "code": "if text == None or pattern == None:\n  raise ValueError(\"Null inputs. (match_main)\")\n\nloc = max(0, min(loc, len(text)))\nif text == pattern:\n  # Shortcut (potentially not guaranteed by the algorithm)\n  return 0\nelif not text:\n  # Nothing to match.\n  return -1\nelif text[loc:loc + len(pattern)] == pattern:\n  # Perfect match at the perfect spot!  (Includes case of null pattern)\n  return loc\nelse:\n  # Do a fuzzy compare.\n  match = self.match_bitap(text, pattern, loc)\n  return match", "path": "python2\\diff_match_patch.py", "repo_name": "rdt1/hardcore", "stars": 34, "license": "apache-2.0", "language": "python", "size": 2571}
{"docstring": "\"\"\"Crush the diff into an encoded string which describes the operations\nrequired to transform text1 into text2.\nE.g. =3\\t-2\\t+ing  -> Keep 3 chars, delete 2 chars, insert 'ing'.\nOperations are tab-separated.  Inserted text is escaped using %xx notation.\n\nArgs:\n  diffs: Array of diff tuples.\n\nReturns:\n  Delta text.\n\"\"\"\n", "func_signal": "def diff_toDelta(self, diffs):\n", "code": "text = []\nfor (op, data) in diffs:\n  if op == self.DIFF_INSERT:\n    # High ascii will raise UnicodeDecodeError.  Use Unicode instead.\n    data = data.encode(\"utf-8\")\n    text.append(\"+\" + urllib.quote(data, \"!~*'();/?:@&=+$,# \"))\n  elif op == self.DIFF_DELETE:\n    text.append(\"-%d\" % len(data))\n  elif op == self.DIFF_EQUAL:\n    text.append(\"=%d\" % len(data))\nreturn \"\\t\".join(text)", "path": "python2\\diff_match_patch.py", "repo_name": "rdt1/hardcore", "stars": 34, "license": "apache-2.0", "language": "python", "size": 2571}
{"docstring": "\"\"\"Increase the context until it is unique,\nbut don't let the pattern expand beyond Match_MaxBits.\n\nArgs:\n  patch: The patch to grow.\n  text: Source text.\n\"\"\"\n", "func_signal": "def patch_addContext(self, patch, text):\n", "code": "if len(text) == 0:\n  return\npattern = text[patch.start2 : patch.start2 + patch.length1]\npadding = 0\n\n# Look for the first and last matches of pattern in text.  If two different\n# matches are found, increase the pattern length.\nwhile (text.find(pattern) != text.rfind(pattern) and (self.Match_MaxBits ==\n    0 or len(pattern) < self.Match_MaxBits - self.Patch_Margin -\n    self.Patch_Margin)):\n  padding += self.Patch_Margin\n  pattern = text[max(0, patch.start2 - padding) :\n                 patch.start2 + patch.length1 + padding]\n# Add one chunk for good luck.\npadding += self.Patch_Margin\n\n# Add the prefix.\nprefix = text[max(0, patch.start2 - padding) : patch.start2]\nif prefix:\n  patch.diffs[:0] = [(self.DIFF_EQUAL, prefix)]\n# Add the suffix.\nsuffix = text[patch.start2 + patch.length1 :\n              patch.start2 + patch.length1 + padding]\nif suffix:\n  patch.diffs.append((self.DIFF_EQUAL, suffix))\n\n# Roll back the start points.\npatch.start1 -= len(prefix)\npatch.start2 -= len(prefix)\n# Extend lengths.\npatch.length1 += len(prefix) + len(suffix)\npatch.length2 += len(prefix) + len(suffix)", "path": "python2\\diff_match_patch.py", "repo_name": "rdt1/hardcore", "stars": 34, "license": "apache-2.0", "language": "python", "size": 2571}
{"docstring": "\"\"\"Rehydrate the text in a diff from a string of line hashes to real lines\nof text.\n\nArgs:\n  diffs: Array of diff tuples.\n  lineArray: Array of unique strings.\n\"\"\"\n", "func_signal": "def diff_charsToLines(self, diffs, lineArray):\n", "code": "for x in xrange(len(diffs)):\n  text = []\n  for char in diffs[x][1]:\n    text.append(lineArray[ord(char)])\n  diffs[x] = (diffs[x][0], \"\".join(text))", "path": "python2\\diff_match_patch.py", "repo_name": "rdt1/hardcore", "stars": 34, "license": "apache-2.0", "language": "python", "size": 2571}
{"docstring": "\"\"\"Compute the Levenshtein distance; the number of inserted, deleted or\nsubstituted characters.\n\nArgs:\n  diffs: Array of diff tuples.\n\nReturns:\n  Number of changes.\n\"\"\"\n", "func_signal": "def diff_levenshtein(self, diffs):\n", "code": "levenshtein = 0\ninsertions = 0\ndeletions = 0\nfor (op, data) in diffs:\n  if op == self.DIFF_INSERT:\n    insertions += len(data)\n  elif op == self.DIFF_DELETE:\n    deletions += len(data)\n  elif op == self.DIFF_EQUAL:\n    # A deletion and an insertion is one substitution.\n    levenshtein += max(insertions, deletions)\n    insertions = 0\n    deletions = 0\nlevenshtein += max(insertions, deletions)\nreturn levenshtein", "path": "python2\\diff_match_patch.py", "repo_name": "rdt1/hardcore", "stars": 34, "license": "apache-2.0", "language": "python", "size": 2571}
{"docstring": "\"\"\"Look through the patches and break up any which are longer than the\nmaximum limit of the match algorithm.\nIntended to be called only from within patch_apply.\n\nArgs:\n  patches: Array of Patch objects.\n\"\"\"\n", "func_signal": "def patch_splitMax(self, patches):\n", "code": "patch_size = self.Match_MaxBits\nif patch_size == 0:\n  # Python has the option of not splitting strings due to its ability\n  # to handle integers of arbitrary precision.\n  return\nfor x in xrange(len(patches)):\n  if patches[x].length1 <= patch_size:\n    continue\n  bigpatch = patches[x]\n  # Remove the big old patch.\n  del patches[x]\n  x -= 1\n  start1 = bigpatch.start1\n  start2 = bigpatch.start2\n  precontext = ''\n  while len(bigpatch.diffs) != 0:\n    # Create one of several smaller patches.\n    patch = patch_obj()\n    empty = True\n    patch.start1 = start1 - len(precontext)\n    patch.start2 = start2 - len(precontext)\n    if precontext:\n      patch.length1 = patch.length2 = len(precontext)\n      patch.diffs.append((self.DIFF_EQUAL, precontext))\n\n    while (len(bigpatch.diffs) != 0 and\n           patch.length1 < patch_size - self.Patch_Margin):\n      (diff_type, diff_text) = bigpatch.diffs[0]\n      if diff_type == self.DIFF_INSERT:\n        # Insertions are harmless.\n        patch.length2 += len(diff_text)\n        start2 += len(diff_text)\n        patch.diffs.append(bigpatch.diffs.pop(0))\n        empty = False\n      elif (diff_type == self.DIFF_DELETE and len(patch.diffs) == 1 and\n          patch.diffs[0][0] == self.DIFF_EQUAL and\n          len(diff_text) > 2 * patch_size):\n        # This is a large deletion.  Let it pass in one chunk.\n        patch.length1 += len(diff_text)\n        start1 += len(diff_text)\n        empty = False\n        patch.diffs.append((diff_type, diff_text))\n        del bigpatch.diffs[0]\n      else:\n        # Deletion or equality.  Only take as much as we can stomach.\n        diff_text = diff_text[:patch_size - patch.length1 -\n                              self.Patch_Margin]\n        patch.length1 += len(diff_text)\n        start1 += len(diff_text)\n        if diff_type == self.DIFF_EQUAL:\n          patch.length2 += len(diff_text)\n          start2 += len(diff_text)\n        else:\n          empty = False\n\n        patch.diffs.append((diff_type, diff_text))\n        if diff_text == bigpatch.diffs[0][1]:\n          del bigpatch.diffs[0]\n        else:\n          bigpatch.diffs[0] = (bigpatch.diffs[0][0],\n                               bigpatch.diffs[0][1][len(diff_text):])\n\n    # Compute the head context for the next patch.\n    precontext = self.diff_text2(patch.diffs)\n    precontext = precontext[-self.Patch_Margin:]\n    # Append the end context for this patch.\n    postcontext = self.diff_text1(bigpatch.diffs)[:self.Patch_Margin]\n    if postcontext:\n      patch.length1 += len(postcontext)\n      patch.length2 += len(postcontext)\n      if len(patch.diffs) != 0 and patch.diffs[-1][0] == self.DIFF_EQUAL:\n        patch.diffs[-1] = (self.DIFF_EQUAL, patch.diffs[-1][1] +\n                           postcontext)\n      else:\n        patch.diffs.append((self.DIFF_EQUAL, postcontext))\n\n    if not empty:\n      x += 1\n      patches.insert(x, patch)", "path": "python2\\diff_match_patch.py", "repo_name": "rdt1/hardcore", "stars": 34, "license": "apache-2.0", "language": "python", "size": 2571}
{"docstring": "\"\"\"Take a list of patches and return a textual representation.\n\nArgs:\n  patches: Array of Patch objects.\n\nReturns:\n  Text representation of patches.\n\"\"\"\n", "func_signal": "def patch_toText(self, patches):\n", "code": "text = []\nfor patch in patches:\n  text.append(str(patch))\nreturn \"\".join(text)", "path": "python2\\diff_match_patch.py", "repo_name": "rdt1/hardcore", "stars": 34, "license": "apache-2.0", "language": "python", "size": 2571}
{"docstring": "\"\"\"Determine the common prefix of two strings.\n\nArgs:\n  text1: First string.\n  text2: Second string.\n\nReturns:\n  The number of characters common to the start of each string.\n\"\"\"\n# Quick check for common null cases.\n", "func_signal": "def diff_commonPrefix(self, text1, text2):\n", "code": "if not text1 or not text2 or text1[0] != text2[0]:\n  return 0\n# Binary search.\n# Performance analysis: http://neil.fraser.name/news/2007/10/09/\npointermin = 0\npointermax = min(len(text1), len(text2))\npointermid = pointermax\npointerstart = 0\nwhile pointermin < pointermid:\n  if text1[pointerstart:pointermid] == text2[pointerstart:pointermid]:\n    pointermin = pointermid\n    pointerstart = pointermin\n  else:\n    pointermax = pointermid\n  pointermid = (pointermax - pointermin) // 2 + pointermin\nreturn pointermid", "path": "python2\\diff_match_patch.py", "repo_name": "rdt1/hardcore", "stars": 34, "license": "apache-2.0", "language": "python", "size": 2571}
{"docstring": "\"\"\"docstring for testIsAdmin_project_user\"\"\"\n", "func_signal": "def testIsAdmin_project_user(self):\n", "code": "r1 = ProjectUserRoles.objects.create(\n        user=self.kim, group=self.guest, project=self.marsProj)\nr2 = ProjectUserRoles.objects.create(\n        user=self.jack, group=self.developer, project=self.marsProj)\nr3 = ProjectUserRoles.objects.create(\n        user=self.chloe, group=self.admin, project=self.marsProj)\n\nself.assertTrue(self.marsProj.isAdmin(self.harryxu))\nself.assertTrue(self.marsProj.isAdmin(self.chloe))\n\nself.assertFalse(self.marsProj.isAdmin(self.kim))\nself.assertFalse(self.marsProj.isAdmin(self.jack))\n\nr1.delete()\nr2.delete()", "path": "gitube\\apps\\project\\tests\\__init__.py", "repo_name": "harryxu/gitube", "stars": 36, "license": "None", "language": "python", "size": 1887}
{"docstring": "\"\"\"docstring for pubkeySaveHandler\"\"\"\n", "func_signal": "def pubkeySaveHandler(sender, instance, **kwargs):\n", "code": "username = instance.user.username\nnewKey = instance.key\nssh.writeKey(authorized_keys, username, newKey)\n\nfor key in keysTobeRemove:\n    ssh.removeKey(authorized_keys, username, key)\n    keysTobeRemove.remove(key)", "path": "gitube\\apps\\sshkey\\models.py", "repo_name": "harryxu/gitube", "stars": 36, "license": "None", "language": "python", "size": 1887}
{"docstring": "\"\"\"Test user can/not view repo.\"\"\"\n# add chloe to flex repo as a developer\n", "func_signal": "def testCanRead_repo_user(self):\n", "code": "r = RepositoryUserRoles.objects.create(\n        user = self.chloe,\n        group = self.developer,\n        repo = self.flexRepo)\n\nself.assertTrue(self.flexRepo.canRead(self.harryxu))\nself.assertTrue(self.flexRepo.canRead(self.chloe))\n# Neither sarah is owner nor a user in flex repo.\nself.assertFalse(self.flexRepo.canRead(self.sarah))\n\nr.delete()", "path": "gitube\\apps\\project\\tests\\__init__.py", "repo_name": "harryxu/gitube", "stars": 36, "license": "None", "language": "python", "size": 1887}
{"docstring": "\"\"\"docstring for preSaveHandler\"\"\"\n", "func_signal": "def preSaveHandler(sender, instance, **kwargs):\n", "code": "if instance.id is not None and instance.id > 0:\n    oldKey = sender.objects.filter(pk=instance.pk).get().key\n    keysTobeRemove.append(oldKey)", "path": "gitube\\apps\\sshkey\\models.py", "repo_name": "harryxu/gitube", "stars": 36, "license": "None", "language": "python", "size": 1887}
{"docstring": "\"\"\"Create a new project\"\"\"\n", "func_signal": "def createProject(request):\n", "code": "if request.method == 'GET':\n    form = forms.ProjectFrom()\nelse:\n    project = models.Project()\n    project.owner = request.user\n    form = forms.ProjectFrom(request.POST, instance=project)\n    if form.is_valid():\n        form.save()\n        return redirect(project)\n\nreturn render_to_response('project/project_form.html', RequestContext(request, {\n    'form': form,\n}))", "path": "gitube\\apps\\project\\views.py", "repo_name": "harryxu/gitube", "stars": 36, "license": "None", "language": "python", "size": 1887}
{"docstring": "\"\"\"docstring for create\"\"\"\n", "func_signal": "def create(request):\n", "code": "if request.method == 'POST':\n    key = models.SSHKey()\n    key.user = request.user\n    form = forms.KeyForm(request.POST, instance=key)\n    if form.is_valid():\n        form.save()\n        return redirect('public_keys_home')\nelse:\n    form = forms.KeyForm()\nreturn render_to_response('sshkey/form.html',\n        RequestContext(request, {\n            'form': form, \n            'action': 'Create'}))", "path": "gitube\\apps\\sshkey\\views.py", "repo_name": "harryxu/gitube", "stars": 36, "license": "None", "language": "python", "size": 1887}
{"docstring": "\"\"\"docstring for remove\"\"\"\n", "func_signal": "def removeProjectMember(request, pslug, userId):\n", "code": "project = get_object_or_404(models.Project, slug=pslug)\nif not project.isAdmin(request.user):\n    raise Http404\nuser = get_object_or_404(User, pk=userId)\npurs = get_object_or_404(models.ProjectUserRoles, project=project, user=user)\npurs.delete()\nreturn redirect('list_project_members', pslug=project.slug)", "path": "gitube\\apps\\project\\views.py", "repo_name": "harryxu/gitube", "stars": 36, "license": "None", "language": "python", "size": 1887}
{"docstring": "\"\"\"docstring for testCanRead_project_user\"\"\"\n", "func_signal": "def testCanRead_project_user(self):\n", "code": "r1 = ProjectUserRoles.objects.create(\n        user=self.kim, group=self.guest, project=self.marsProj)\nr2 = ProjectUserRoles.objects.create(\n        user=self.jack, group=self.developer, project=self.marsProj)\n\nself.assertTrue(self.marsProj.canRead(self.kim))\nself.assertTrue(self.marsProj.canRead(self.jack))\nself.assertTrue(self.flexRepo.canRead(self.kim))\nself.assertTrue(self.flexRepo.canRead(self.jack))\n\nself.assertFalse(self.marsProj.canRead(self.chloe))\nself.assertFalse(self.flexRepo.canRead(self.chloe))\n\nr1.delete()\nr2.delete()", "path": "gitube\\apps\\project\\tests\\__init__.py", "repo_name": "harryxu/gitube", "stars": 36, "license": "None", "language": "python", "size": 1887}
{"docstring": "\"\"\"docstring for edit\"\"\"\n", "func_signal": "def edit(request, id):\n", "code": "key = get_object_or_404(models.SSHKey, pk=id, user=request.user)\nif request.method == 'POST':\n    form = forms.KeyForm(request.POST, instance=key)\n    if form.is_valid():\n        form.save()\n        return redirect('public_keys_home')\nelse:\n    form = forms.KeyForm(instance=key)\nreturn render_to_response('sshkey/form.html',\n        RequestContext(request, {\n            'form': form, \n            'action': 'Edit'}))", "path": "gitube\\apps\\sshkey\\views.py", "repo_name": "harryxu/gitube", "stars": 36, "license": "None", "language": "python", "size": 1887}
{"docstring": "\"\"\" Access controll \"\"\"\n", "func_signal": "def haveAccess(config, user, mode, path):\n", "code": "logging.debug('check access for %(user)r as %(mode)r on %(path)r'\n                % {'user':user, 'mode':mode, 'path':path})\n\ntry:\n    myuser = User.objects.get(username=user)\nexcept User.DoesNotExist:\n    logging.debug('User \"%(user)r\" not found' % {'user':user})\n    return None\n\nbasename, ext = os.path.splitext(path)\nif ext == '.git':\n    path = basename\n\npathHash = hashlib.sha1(path).hexdigest()\ntry:\n    repo = Repository.objects.get(path_hash=pathHash)\nexcept Repository.DoesNotExist:\n    logging.debug('Repo %(path)r not found, hashed: %(hashed)r'\n            % {'path':path, 'hashed':pathHash})\n    return None\n\nif mode != 'readonly' and mode != 'writable':\n    return None\nif mode == 'readonly' and not repo.canRead(myuser):\n    return None\nelif mode == 'writable' and not repo.canPush(myuser):\n    return None\n\nbasename, ext = os.path.splitext(path)\nif ext == '.git':\n    path = basename\n\nprefix = getattr(settings, 'REPO_BASE_PATH', 'repositories')\nreturn (prefix, path)", "path": "gitube\\tools\\access.py", "repo_name": "harryxu/gitube", "stars": 36, "license": "None", "language": "python", "size": 1887}
{"docstring": "\"\"\"docstring for createRepo\"\"\"\n", "func_signal": "def createRepository(request, pslug):\n", "code": "project = get_object_or_404(models.Project, slug=pslug)\nif not project.isAdmin(request.user):\n    raise Http404\nif request.method == 'GET':\n    form = forms.RepositoryForm()\nelse:\n    repo = models.Repository()\n    repo.project = project\n    form = forms.RepositoryForm(request.POST, instance=repo)\n    if form.is_valid():\n        form.save()\n        return redirect(repo)\n\nreturn render_to_response('project/repository_form.html', RequestContext(request, {\n            'form': form,\n            'project': project,\n        }))", "path": "gitube\\apps\\project\\views.py", "repo_name": "harryxu/gitube", "stars": 36, "license": "None", "language": "python", "size": 1887}
{"docstring": "\"\"\"docstring for index\"\"\"\n", "func_signal": "def index(request):\n", "code": "keys = models.SSHKey.objects.filter(user=request.user)\nreturn render_to_response('sshkey/home.html',\n        RequestContext(request, {'keys':keys}))", "path": "gitube\\apps\\sshkey\\views.py", "repo_name": "harryxu/gitube", "stars": 36, "license": "None", "language": "python", "size": 1887}
{"docstring": "\"\"\"docstring for addProjectMember\"\"\"\n", "func_signal": "def addProjectMember(request, pslug):\n", "code": "project = get_object_or_404(models.Project, slug=pslug)\nif not project.isAdmin(request.user):\n    raise Http404\n\nif request.method == 'POST':\n    form = forms.ProjectMemberForm(request.POST)\n    if form.is_valid():\n        form.save()\n        return redirect('list_project_members', pslug=project.slug)\nelse:\n    form = forms.ProjectMemberForm(initial={'project':project.id})\n\nreturn render_to_response('project/member_add_form.html',\n    RequestContext(request, {'form':form, 'project':project}))", "path": "gitube\\apps\\project\\views.py", "repo_name": "harryxu/gitube", "stars": 36, "license": "None", "language": "python", "size": 1887}
{"docstring": "\"\"\"docstring for start\"\"\"\n", "func_signal": "def home(request):\n", "code": "viewData = {}\nif request.user.is_authenticated():\n    viewData['projects'] = models.Project.objects.filter(owner=request.user)\n    viewData['projectRoles'] = models.ProjectUserRoles.objects.filter(user=request.user)\nreturn render_to_response('home.html', \n        RequestContext(request, viewData))", "path": "gitube\\apps\\project\\views.py", "repo_name": "harryxu/gitube", "stars": 36, "license": "None", "language": "python", "size": 1887}
{"docstring": "\"\"\"docstring for viewProject\"\"\"\n", "func_signal": "def viewProject(request, pslug):\n", "code": "project = get_object_or_404(models.Project, slug=pslug)\nif not project.canRead(request.user):\n    raise Http404\nrepos = project.repository_set.all()\n    \nreturn render_to_response('project/view_project.html', RequestContext(request, {\n    'project':project,\n    'repositories':repos\n}))", "path": "gitube\\apps\\project\\views.py", "repo_name": "harryxu/gitube", "stars": 36, "license": "None", "language": "python", "size": 1887}
{"docstring": "\"\"\"docstring for pubkeyRemoveHandler\"\"\"\n", "func_signal": "def pubkeyRemoveHandler(sender, instance, **kwargs):\n", "code": "if instance.id is not None and instance.id > 0:\n    ssh.removeKey(authorized_keys, instance.user.username, instance.key)", "path": "gitube\\apps\\sshkey\\models.py", "repo_name": "harryxu/gitube", "stars": 36, "license": "None", "language": "python", "size": 1887}
{"docstring": "\"\"\"Test user can/not edit repo.\"\"\"\n", "func_signal": "def testIsAdmin_repo_user(self):\n", "code": "r1 = RepositoryUserRoles.objects.create(\n        user = self.chloe, group = self.admin, repo = self.flexRepo)\nr2 = RepositoryUserRoles.objects.create(\n        user = self.sarah, group = self.developer, repo = self.flexRepo)\n\nself.assertTrue(self.flexRepo.isAdmin(self.harryxu))\nself.assertTrue(self.flexRepo.isAdmin(self.chloe))\nself.assertFalse(self.flexRepo.isAdmin(self.sarah))\nself.assertFalse(self.flexRepo.isAdmin(self.clark))\n\nr1.delete()\nr2.delete()", "path": "gitube\\apps\\project\\tests\\__init__.py", "repo_name": "harryxu/gitube", "stars": 36, "license": "None", "language": "python", "size": 1887}
{"docstring": "\"\"\"docstring for viewRepository\"\"\"\n", "func_signal": "def viewRepository(request, pslug, rslug):\n", "code": "project = get_object_or_404(models.Project , slug=pslug)\nrepo = get_object_or_404(models.Repository, slug=rslug, project=project)\nreturn render_to_response('project/view_repository.html',\n        RequestContext(request, {'repository': repo}))", "path": "gitube\\apps\\project\\views.py", "repo_name": "harryxu/gitube", "stars": 36, "license": "None", "language": "python", "size": 1887}
{"docstring": "\"\"\"docstring for index\"\"\"\n", "func_signal": "def index(request):\n", "code": "return render_to_response('account/home.html', \n        RequestContext(request, {}))", "path": "gitube\\apps\\account\\views.py", "repo_name": "harryxu/gitube", "stars": 36, "license": "None", "language": "python", "size": 1887}
{"docstring": "\"\"\"docstring for delete\"\"\"\n", "func_signal": "def delete(request, id):\n", "code": "key = get_object_or_404(models.SSHKey, pk=id, user=request.user)\nkey.delete()\nreturn redirect('public_keys_home')", "path": "gitube\\apps\\sshkey\\views.py", "repo_name": "harryxu/gitube", "stars": 36, "license": "None", "language": "python", "size": 1887}
{"docstring": "# These choices show the periodicity very well\n# Better choices are a = 16,807 m = 2**31 -1 c = 0\n# Or m = 2**32 a = 1,664,525 c = 1,013,904,223\n", "func_signal": "def lcg(x0,n):\n", "code": "a = 23\nm = 197\nc = 0\n\nrnd = zeros((n))\n\nrnd[0] = mod(a*x0 + c,m)\n\nfor i in range(1,n):\n    rnd[i] = mod(a*rnd[i-1]+c,m)\n    \nreturn rnd", "path": "14 MCMC\\lcg.py", "repo_name": "tback/MLBook_source", "stars": 41, "license": "None", "language": "python", "size": 159}
{"docstring": "\"\"\" Run AND and XOR logic functions\"\"\"\n\n", "func_signal": "def logic(self):\n", "code": "a = array([[0,0,0],[0,1,0],[1,0,0],[1,1,1]])\nb = array([[0,0,0],[0,1,1],[1,0,1],[1,1,0]])\n\np = self.pcn(a[:,0:2],a[:,2:])\np.pcntrain(a[:,0:2],a[:,2:],0.25,10)\np.confmat(a[:,0:2],a[:,2:])\n\nq = self.pcn(a[:,0:2],b[:,2:])\nq.pcntrain(a[:,0:2],b[:,2:],0.25,10)\nq.confmat(a[:,0:2],b[:,2:])", "path": "2 Linear\\pcn.py", "repo_name": "tback/MLBook_source", "stars": 41, "license": "None", "language": "python", "size": 159}
{"docstring": "# Scale fitness by total fitness\n", "func_signal": "def fps(self,population,fitness):\n", "code": "\t\tfitness = fitness/sum(fitness)\n\t\tfitness = 10*fitness/fitness.max()\n\t\t\n\t\t# Put repeated copies of each string in according to fitness\n\t\t# Deal with strings with very low fitness\n\t\tj=0\n\t\twhile round(fitness[j])<1:\n\t\t\tj = j+1\n\t\t\n\t\tnewPopulation = kron(ones((round(fitness[j]),1)),population[j,:])\n# Add multiple copies of strings into the newPopulation\n\t\tfor i in range(j+1,self.populationSize):\n\t\t\tif round(fitness[i])>=1:\n\t\t\t\tnewPopulation = concatenate((newPopulation,kron(ones((round(fitness[i]),1)),population[i,:])),axis=0)\n# Shuffle the order (note that there are still too many)\n\t\tindices = range(shape(newPopulation)[0])\n\t\trandom.shuffle(indices)\n\t\tnewPopulation = newPopulation[indices[:self.populationSize],:]\n\t\treturn newPopulation", "path": "12 Evolutionary\\ga.py", "repo_name": "tback/MLBook_source", "stars": 41, "license": "None", "language": "python", "size": 159}
{"docstring": "# Make the swiss roll dataset\n", "func_signal": "def swissroll():\n", "code": "N = 1000\nnoise = 0.05\n\nt = 3*math.pi/2 * (1 + 2*random.rand(1,N))\nh = 21 * random.rand(1,N)\ndata = concatenate((t*cos(t),h,t*sin(t))) + noise*random.randn(3,N)\t\nreturn transpose(data), squeeze(t)", "path": "10 Dimension Reduction\\lle.py", "repo_name": "tback/MLBook_source", "stars": 41, "license": "None", "language": "python", "size": 159}
{"docstring": "\"\"\" Run the network forward \"\"\"\n\n", "func_signal": "def pcnfwd(self,inputs):\n", "code": "outputs =  dot(inputs,self.weights)\n\n# Threshold the outputs\nreturn where(outputs>0,1,0)", "path": "2 Linear\\pcn.py", "repo_name": "tback/MLBook_source", "stars": 41, "license": "None", "language": "python", "size": 159}
{"docstring": "\"\"\" Constructor \"\"\"\n# Set up network size\n", "func_signal": "def __init__(self,inputs,targets):\n", "code": "if ndim(inputs)>1:\n\tself.nIn = shape(inputs)[1]\nelse: \n\tself.nIn = 1\n\t\nif ndim(targets)>1:\n\tself.nOut = shape(targets)[1]\nelse:\n\tself.nOut = 1\n\nself.nData = shape(inputs)[0]\n\t\n# Initialise network\nself.weights = random.rand(self.nIn+1,self.nOut)*0.1-0.05", "path": "2 Linear\\pcn_logic_eg.py", "repo_name": "tback/MLBook_source", "stars": 41, "license": "None", "language": "python", "size": 159}
{"docstring": "# Single point crossover\n", "func_signal": "def spCrossover(self,population):\n", "code": "newPopulation = zeros(shape(population))\ncrossoverPoint = random.randint(0,self.stringLength,self.populationSize)\nfor i in range(0,self.populationSize,2):\n\tnewPopulation[i,:crossoverPoint[i]] = population[i,:crossoverPoint[i]]\n\tnewPopulation[i+1,:crossoverPoint[i]] = population[i+1,:crossoverPoint[i]]\n\tnewPopulation[i,crossoverPoint[i]:] = population[i+1,crossoverPoint[i]:]\n\tnewPopulation[i+1,crossoverPoint[i]:] = population[i,crossoverPoint[i]:]\nreturn newPopulation", "path": "12 Evolutionary\\ga.py", "repo_name": "tback/MLBook_source", "stars": 41, "license": "None", "language": "python", "size": 159}
{"docstring": "\"\"\" Constructor \"\"\"\n# Set up network size\n", "func_signal": "def __init__(self,inputs,targets,nhidden,beta=1,momentum=0.9,outtype='logistic'):\n", "code": "self.nin = shape(inputs)[1]\nself.nout = shape(targets)[1]\nself.ndata = shape(inputs)[0]\nself.nhidden = nhidden\n\nself.beta = beta\nself.momentum = momentum\nself.outtype = outtype\n    \n# Initialise network\nself.weights1 = (random.rand(self.nin+1,self.nhidden)-0.5)*2/sqrt(self.nin)\nself.weights2 = (random.rand(self.nhidden+1,self.nout)-0.5)*2/sqrt(self.nhidden)", "path": "3 MLP\\mlp.py", "repo_name": "tback/MLBook_source", "stars": 41, "license": "None", "language": "python", "size": 159}
{"docstring": "\"\"\" Constructor\"\"\"\n", "func_signal": "def __init__(self,stringLength,fitnessFunction,nEpochs,populationSize=100,mutationProb=-1,crossover='un',nElite=4,tournament=True):\n", "code": "self.stringLength = stringLength\n\n# Population size should be even\nif mod(populationSize,2)==0:\n\tself.populationSize = populationSize\nelse:\n\tself.populationSize = populationSize+1\n\nif mutationProb < 0:\n\t self.mutationProb = 1/stringLength\nelse:\n\t self.mutationProb = mutationProb\n\t \t  \nself.nEpochs = nEpochs\n\nself.fitnessFunction = fitnessFunction\n\nself.crossover = crossover\nself.nElite = nElite\nself.tournment = tournament\n\nself.population = random.rand(self.populationSize,self.stringLength)\nself.population = where(self.population<0.5,0,1)", "path": "12 Evolutionary\\ga.py", "repo_name": "tback/MLBook_source", "stars": 41, "license": "None", "language": "python", "size": 159}
{"docstring": "# Make the swiss roll dataset\n", "func_signal": "def swissroll():\n", "code": "N = 1000\nnoise = 0.05\n\nt = 3.*math.pi/2 * (1. + 2.*random.rand(1,N))\nh = 21. * random.rand(1,N)\ndata = concatenate((t*cos(t),h,t*sin(t))) + noise*random.randn(3,N)\t\nreturn transpose(data), squeeze(t)", "path": "10 Dimension Reduction\\isomap.py", "repo_name": "tback/MLBook_source", "stars": 41, "license": "None", "language": "python", "size": 159}
{"docstring": "# Preprocess data (won't work if (0,0,...0) is in data)\n", "func_signal": "def kmeanstrain(self,data):\n", "code": "normalisers = sqrt(sum(data**2,axis=1))*ones((1,shape(data)[0]))\ndata = transpose(transpose(data)/normalisers)\n\nfor i in range(self.nEpochs):\n    for j in range(self.nData):\n        activation = sum(self.weights*transpose(data[j:j+1,:]),axis=0)\n        winner = argmax(activation)\n        self.weights[:,winner] += self.eta * data[j,:] - self.weights[:,winner]", "path": "9 Unsupervised\\kmeansnet.py", "repo_name": "tback/MLBook_source", "stars": 41, "license": "None", "language": "python", "size": 159}
{"docstring": "# Centre data\n", "func_signal": "def lda(data,labels,redDim):\n", "code": "    data -= data.mean(axis=0)\n    nData = shape(data)[0]\n    nDim = shape(data)[1]\n    \n    Sw = zeros((nDim,nDim))\n    Sb = zeros((nDim,nDim))\n    \n    C = cov(transpose(data))\n    \n    # Loop over classes\n    classes = unique(labels)\n    for i in range(len(classes)):\n        # Find relevant datapoints\n        indices = squeeze(where(labels==classes[i]))\n        d = squeeze(data[indices,:])\n        classcov = cov(transpose(d))\n        Sw += float(shape(indices)[0])/nData * classcov\n        \n    Sb = C - Sw\n    # Now solve for W\n    # Compute eigenvalues, eigenvectors and sort into order\n    #evals,evecs = linalg.eig(dot(linalg.pinv(Sw),sqrt(Sb)))\n    evals,evecs = la.eig(Sw,Sb)\n    indices = argsort(evals)\n    indices = indices[::-1]\n    evecs = evecs[:,indices]\n    evals = evals[indices]\n    w = evecs[:,:redDim]\n    #print evals, w\n    \n    newData = dot(data,w)\n    return newData,w", "path": "10 Dimension Reduction\\lda.py", "repo_name": "tback/MLBook_source", "stars": 41, "license": "None", "language": "python", "size": 159}
{"docstring": "\"\"\" Constructor \"\"\"\n\n", "func_signal": "def __init__(self):\n", "code": "\nfid = open(filename,\"r\")\ndata = []\nd = []\nfor line in fid.readlines():\n\td.append(line.strip())\nfor d1 in d:\n\tdata.append(d1.split(\",\"))\nfid.close()\n\nself.featureNames = data[0]\nself.featureNames = self.featureNames[:-1]\ndata = data[1:]\nself.classes = []\nfor d in range(len(data)):\n\tself.classes.append(data[d][-1])\n\tdata[d] = data[d][:-1]\n\nreturn data,self.classes,self.featureNames", "path": "6 Trees\\dtree.py", "repo_name": "tback/MLBook_source", "stars": 41, "license": "None", "language": "python", "size": 159}
{"docstring": "\"\"\" Run the network forward \"\"\"\n\n", "func_signal": "def pcnfwd(self,inputs):\n", "code": "outputs =  dot(inputs,self.weights)\n\n# Threshold the outputs\nreturn where(outputs>0,1,0)", "path": "2 Linear\\pcn_logic_eg.py", "repo_name": "tback/MLBook_source", "stars": 41, "license": "None", "language": "python", "size": 159}
{"docstring": "\"\"\" Constructor \"\"\"\n\n", "func_signal": "def __init__(self):\n", "code": "\nfid = open(filename,\"r\")\ndata = []\nd = []\nfor line in fid.readlines():\n    d.append(line.strip())\nfor d1 in d:\n    data.append(d1.split(\",\"))\nfid.close()\n\nself.featureNames = data[0]\nself.featureNames = self.featureNames[:-1]\ndata = data[1:]\nself.classes = []\nfor d in range(len(data)):\n    self.classes.append(data[d][-1])\n    data[d] = data[d][:-1]\n\nreturn data,self.classes,self.featureNames", "path": "7 Committee\\dtw.py", "repo_name": "tback/MLBook_source", "stars": 41, "license": "None", "language": "python", "size": 159}
{"docstring": "# Mutation\n", "func_signal": "def mutate(self,population):\n", "code": "whereMutate = random.rand(shape(population)[0],shape(population)[1])\npopulation[where(whereMutate < self.mutationProb)] = 1 - population[where(whereMutate < self.mutationProb)]\nreturn population", "path": "12 Evolutionary\\ga.py", "repo_name": "tback/MLBook_source", "stars": 41, "license": "None", "language": "python", "size": 159}
{"docstring": "# Add the inputs that match the bias node\n", "func_signal": "def pcntrain(self,inputs,targets,eta,nIterations):\n", "code": "\t\tinputs = concatenate((inputs,-ones((self.nData,1))),axis=1)\n\t\t# Training\n\t\tchange = range(self.nData)\n\n\t\tfor n in range(nIterations):\n\t\t\t\n\t\t\tself.outputs = self.pcnfwd(inputs);\n\t\t\tself.weights += eta*dot(transpose(inputs),targets-self.outputs)\n\t\t\n\t\t\t# Randomise order of inputs\n\t\t\trandom.shuffle(change)\n\t\t\tinputs = inputs[change,:]\n\t\t\ttargets = targets[change,:]\n\t\t\t\n\t\t#return self.weights", "path": "2 Linear\\pcn.py", "repo_name": "tback/MLBook_source", "stars": 41, "license": "None", "language": "python", "size": 159}
{"docstring": "\"\"\" The main function, which recursively constructs the tree\"\"\"\n\n", "func_signal": "def make_tree(self,data,classes,featureNames,maxlevel=-1,level=0):\n", "code": "nData = len(data)\nnFeatures = len(data[0])\n\ntry: \n\tself.featureNames\nexcept:\n\tself.featureNames = featureNames\n\t\n# List the possible classes\nnewClasses = []\nfor aclass in classes:\n\tif newClasses.count(aclass)==0:\n\t\tnewClasses.append(aclass)\n\n# Compute the default class (and total entropy)\nfrequency = zeros(len(newClasses))\n\ntotalEntropy = 0\ntotalGini = 0\nindex = 0\nfor aclass in newClasses:\n\tfrequency[index] = classes.count(aclass)\n\ttotalEntropy += self.calc_entropy(float(frequency[index])/nData)\n\ttotalGini += (float(frequency[index])/nData)**2\n\n\tindex += 1\n\ntotalGini = 1 - totalGini\ndefault = classes[argmax(frequency)]\n\nif nData==0 or nFeatures == 0 or (maxlevel>=0 and level>maxlevel):\n\t# Have reached an empty branch\n\treturn default\nelif classes.count(classes[0]) == nData:\n\t# Only 1 class remains\n\treturn classes[0]\nelse:\n\n\t# Choose which feature is best\t\n\tgain = zeros(nFeatures)\n\tggain = zeros(nFeatures)\n\tfor feature in range(nFeatures):\n\t\tg,gg = self.calc_info_gain(data,classes,feature)\n\t\tgain[feature] = totalEntropy - g\n\t\tggain[feature] = totalGini - gg\n\n\tbestFeature = argmax(gain)\n\ttree = {featureNames[bestFeature]:{}}\n\n\t# List the values that bestFeature can take\n\tvalues = []\n\tfor datapoint in data:\n\t\tif values.count(datapoint[bestFeature])==0:\n\t\t\tvalues.append(datapoint[bestFeature])\n\n\tfor value in values:\n\t\t# Find the datapoints with each feature value\n\t\tnewData = []\n\t\tnewClasses = []\n\t\tindex = 0\n\t\tfor datapoint in data:\n\t\t\tif datapoint[bestFeature]==value:\n\t\t\t\tif bestFeature==0:\n\t\t\t\t\tnewdatapoint = datapoint[1:]\n\t\t\t\t\tnewNames = featureNames[1:]\n\t\t\t\telif bestFeature==nFeatures:\n\t\t\t\t\tnewdatapoint = datapoint[:-1]\n\t\t\t\t\tnewNames = featureNames[:-1]\n\t\t\t\telse:\n\t\t\t\t\tnewdatapoint = datapoint[:bestFeature]\n\t\t\t\t\tnewdatapoint.extend(datapoint[bestFeature+1:])\n\t\t\t\t\tnewNames = featureNames[:bestFeature]\n\t\t\t\t\tnewNames.extend(featureNames[bestFeature+1:])\n\t\t\t\tnewData.append(newdatapoint)\n\t\t\t\tnewClasses.append(classes[index])\n\t\t\tindex += 1\n\n\t\t# Now recurse to the next level\t\n\t\tsubtree = self.make_tree(newData,newClasses,newNames,maxlevel,level+1)\n\n\t\t# And on returning, add the subtree on to the tree\n\t\ttree[featureNames[bestFeature]][value] = subtree\n\n\treturn tree", "path": "6 Trees\\dtree.py", "repo_name": "tback/MLBook_source", "stars": 41, "license": "None", "language": "python", "size": 159}
{"docstring": "# Uniform crossover\n", "func_signal": "def uniformCrossover(self,population):\n", "code": "newPopulation = zeros(shape(population))\nwhich = random.rand(self.populationSize,self.stringLength)\nwhich1 = which>=0.5\nfor i in range(0,self.populationSize,2):\n\tnewPopulation[i,:] = population[i,:]*which1[i,:] + population[i+1,:]*(1-which1[i,:])\n\tnewPopulation[i+1,:] = population[i,:]*(1-which1[i,:]) + population[i+1,:]*which1[i,:]\nreturn newPopulation", "path": "12 Evolutionary\\ga.py", "repo_name": "tback/MLBook_source", "stars": 41, "license": "None", "language": "python", "size": 159}
{"docstring": "# Find the minimum and maximum values for each feature\n", "func_signal": "def kmeanstrain(self,data,maxIterations=10):\n", "code": "\t\tminima = data.min(axis=0)\n\t\tmaxima = data.max(axis=0)\n\t\n\t\t# Pick the centre locations randomly\n\t\tself.centres = random.rand(self.k,self.nDim)*(maxima-minima)+minima\n\t\toldCentres = random.rand(self.k,self.nDim)*(maxima-minima)+minima\n\t\n\t\tcount = 0\n\t\t#print centres\n\t\twhile sum(sum(oldCentres-self.centres))!= 0 and count<maxIterations:\n\t\n\t\t\toldCentres = self.centres.copy()\n\t\t\tcount += 1\n\t\n\t\t\t# Compute distances\n\t\t\tdistances = ones((1,self.nData))*sum((data-self.centres[0,:])**2,axis=1)\n\t\t\tfor j in range(self.k-1):\n\t\t\t\tdistances = append(distances,ones((1,self.nData))*sum((data-self.centres[j+1,:])**2,axis=1),axis=0)\n\t\n\t\t\t# Identify the closest cluster\n\t\t\tcluster = distances.argmin(axis=0)\n\t\t\tcluster = transpose(cluster*ones((1,self.nData)))\n\t\n\t\t\t# Update the cluster centres\t\n\t\t\tfor j in range(self.k):\n\t\t\t\tthisCluster = where(cluster==j,1,0)\n\t\t\t\tif sum(thisCluster)>0:\n\t\t\t\t\tself.centres[j,:] = sum(data*thisCluster,axis=0)/sum(thisCluster)\n\t\t\t#plot(data[:,0],data[:,1],'kx')\n\t\t\t#plot(centres[:,0],centres[:,1],'ro')\n\t\treturn self.centres", "path": "9 Unsupervised\\kmeans.py", "repo_name": "tback/MLBook_source", "stars": 41, "license": "None", "language": "python", "size": 159}
{"docstring": "\"\"\"key = (variable,value), value = [has_success,has_failure]\"\"\"\n", "func_signal": "def _init_result_cache(self):\n", "code": "result_cache = {}\nfor var_name in self._variables:\n    result_cache[var_name] = {}\nreturn result_cache", "path": "constraint-0.4.0\\fd.py", "repo_name": "CS480EmployeeScheduling/Employee-Scheduler", "stars": 42, "license": "None", "language": "python", "size": 474}
{"docstring": "\"\"\"returns the variable having the largest domain.\n(or one of such variables if there is a tie)\n\"\"\"\n", "func_signal": "def findLargestDomain(self, domains):\n", "code": "domlist = [(dom.size(), variable) for variable, dom in domains.items()\n                                  if dom.size() > 1]\ndomlist.sort()\nreturn domlist[-1][1]", "path": "constraint-0.4.0\\distributors.py", "repo_name": "CS480EmployeeScheduling/Employee-Scheduler", "stars": 42, "license": "None", "language": "python", "size": 474}
{"docstring": "\"\"\"make sure we don't change the default behaviour\nfor loadTestsFromModule() and loadTestsFromTestCase\n\"\"\"\n", "func_signal": "def test_collect_everything(self):\n", "code": "testsuite = self.loader.loadTestsFromModule(self.module)\nself.assertEqual(len(testsuite._tests), 2)\nsuite1, suite2 = testsuite._tests\nself.assertEqual(len(suite1._tests) + len(suite2._tests), 4)", "path": "logilab-common-0.57.2\\test\\unittest_testlib.py", "repo_name": "CS480EmployeeScheduling/Employee-Scheduler", "stars": 42, "license": "None", "language": "python", "size": 474}
{"docstring": "\"\"\" method to implement in concrete class\n\ntake self.nb_subspaces copy of the original domains as argument\ndistribute the domains and return each modified domain\n\"\"\"\n", "func_signal": "def _distribute(self, *args):\n", "code": "raise NotImplementedError(\"Use a concrete implementation of \"\n                          \"the Distributor interface\")", "path": "constraint-0.4.0\\distributors.py", "repo_name": "CS480EmployeeScheduling/Employee-Scheduler", "stars": 42, "license": "None", "language": "python", "size": 474}
{"docstring": "\"\"\"tests xml is valid\"\"\"\n", "func_signal": "def test_xml_valid(self):\n", "code": "valid = \"\"\"<root>\n<hello />\n<world>Logilab</world>\n</root>\"\"\"\ninvalid = \"\"\"<root><h2> </root>\"\"\"\nself.tc.assertXMLStringWellFormed(valid)\nself.assertRaises(AssertionError, self.tc.assertXMLStringWellFormed, invalid)\ninvalid = \"\"\"<root><h2 </h2> </root>\"\"\"\nself.assertRaises(AssertionError, self.tc.assertXMLStringWellFormed, invalid)", "path": "logilab-common-0.57.2\\test\\unittest_testlib.py", "repo_name": "CS480EmployeeScheduling/Employee-Scheduler", "stars": 42, "license": "None", "language": "python", "size": 474}
{"docstring": "\"\"\"narrowing algorithm for the constraint\"\"\"\n", "func_signal": "def narrow(self, domains):\n", "code": "variables = [(domains[variable].size(), variable, domains[variable])\n             for variable in self._variables]\n\nvariables.sort()\n# if a domain has a size of 1,\n# then the value must be removed from the other domains\nfor size, var, dom in variables:\n    if dom.size() == 1:\n        for _siz, _var, _dom in variables:\n            if _var != var:\n                try:\n                    _dom.removeValue(dom.getValues()[0])\n                except KeyError:\n                    # we ignore errors caused by the removal of\n                    # non existing values\n                    pass\n\n# if there are less values than variables, the constraint fails\nvalues = {}\nfor size, var, dom in variables:\n    for val in dom:\n        values[val] = 0\nif len(values) < len(variables):\n    raise ConsistencyFailure()\n    \n# the constraint is entailed if all domains have a size of 1\nfor variable in variables:\n    if variable[2].size() != 1:\n        return 0\nreturn 1", "path": "constraint-0.4.0\\fd.py", "repo_name": "CS480EmployeeScheduling/Employee-Scheduler", "stars": 42, "license": "None", "language": "python", "size": 474}
{"docstring": "\"\"\"tests TestCase.assertDictEqual\"\"\"\n", "func_signal": "def test_dict_equals(self):\n", "code": "d1 = {'a' : 1, 'b' : 2}\nd2 = {'a' : 1, 'b' : 3}\nd3 = dict(d1)\nself.assertRaises(AssertionError, self.tc.assertDictEqual, d1, d2)\nself.tc.assertDictEqual(d1, d3)\nself.tc.assertDictEqual(d3, d1)\nself.tc.assertDictEqual(d1, d1)", "path": "logilab-common-0.57.2\\test\\unittest_testlib.py", "repo_name": "CS480EmployeeScheduling/Employee-Scheduler", "stars": 42, "license": "None", "language": "python", "size": 474}
{"docstring": "\"\"\"tests TestCase.assertListEqual\"\"\"\n", "func_signal": "def test_list_equals(self):\n", "code": "l1 = range(10)\nl2 = range(5)\nl3 = range(10)\nself.assertRaises(AssertionError, self.tc.assertListEqual, l1, l2)\nself.tc.assertListEqual(l1, l1)\nself.tc.assertListEqual(l1, l3)\nself.tc.assertListEqual(l3, l1)", "path": "logilab-common-0.57.2\\test\\unittest_testlib.py", "repo_name": "CS480EmployeeScheduling/Employee-Scheduler", "stars": 42, "license": "None", "language": "python", "size": 474}
{"docstring": "\"\"\"returns the variable having the smallest domain.\n(or one of such varibles if there is a tie)\n\"\"\"\n", "func_signal": "def findSmallestDomain(self, domains):\n", "code": "domlist = [(dom.size(), variable ) for variable, dom in domains.items()\n                                   if dom.size() > 1]\ndomlist.sort()\nreturn domlist[0][1]", "path": "constraint-0.4.0\\distributors.py", "repo_name": "CS480EmployeeScheduling/Employee-Scheduler", "stars": 42, "license": "None", "language": "python", "size": 474}
{"docstring": "\"\"\"do the minimal job and let concrete class distribute variables\n\"\"\"\n", "func_signal": "def distribute(self, domains, verbose=0):\n", "code": "self.verbose = verbose\nreplicas = []\nfor i in range(self.nb_subdomains(domains)):\n    replicas.append(make_new_domains(domains))\nmodified_domains = self._distribute(*replicas)\nfor domain in modified_domains:\n    domain.resetFlags()\nreturn replicas", "path": "constraint-0.4.0\\distributors.py", "repo_name": "CS480EmployeeScheduling/Employee-Scheduler", "stars": 42, "license": "None", "language": "python", "size": 474}
{"docstring": "\"\"\" should throw a ValueError exception\n\"\"\"\n", "func_signal": "def test_require_version_exception(self):\n", "code": "def func() :\n    pass\ncompare = ('2.5.a', '2.a', 'azerty')\nfor version in compare:\n    decorator = require_version(version)\n    self.assertRaises(ValueError, decorator, func)", "path": "logilab-common-0.57.2\\test\\unittest_testlib.py", "repo_name": "CS480EmployeeScheduling/Employee-Scheduler", "stars": 42, "license": "None", "language": "python", "size": 474}
{"docstring": "\"\"\"See AbstractDistributor\"\"\"\n", "func_signal": "def nb_subdomains(self, domains):\n", "code": "self.__to_split = self.findSmallestDomain(domains)\nif self.nb_subspaces:\n    return min(self.nb_subspaces, domains[self.__to_split].size())\nelse:\n    return domains[self.__to_split].size()", "path": "constraint-0.4.0\\distributors.py", "repo_name": "CS480EmployeeScheduling/Employee-Scheduler", "stars": 42, "license": "None", "language": "python", "size": 474}
{"docstring": "\"\"\"return a shallow copy of dict of domains passed in argument\"\"\"\n", "func_signal": "def make_new_domains(domains):\n", "code": "domain = {}\nfor key, value in domains.items():\n    domain[key] = value.copy()\nreturn domain", "path": "constraint-0.4.0\\distributors.py", "repo_name": "CS480EmployeeScheduling/Employee-Scheduler", "stars": 42, "license": "None", "language": "python", "size": 474}
{"docstring": "\"\"\"return an xml rpc server on <url>, using user / password if specified\n\"\"\"\n", "func_signal": "def connect(url, user=None, passwd=None, encoding='ISO-8859-1'):\n", "code": "if user or passwd:\n    assert user and passwd is not None\n    if url.startswith('https://'):\n        transport = BasicAuthSafeTransport(user, passwd, encoding)\n    else:\n        transport = BasicAuthTransport(user, passwd, encoding)\nelse:\n    transport = None\nserver = xmlrpclib.ServerProxy(url, transport, encoding=encoding)\nreturn server", "path": "logilab-common-0.57.2\\xmlrpcutils.py", "repo_name": "CS480EmployeeScheduling/Employee-Scheduler", "stars": 42, "license": "None", "language": "python", "size": 474}
{"docstring": "\"\"\" should return the same function\n\"\"\"\n", "func_signal": "def test_require_module_good(self):\n", "code": "def func() :\n    pass\nmodule = 'sys'\ndecorator = require_module(module)\nself.assertEqual(func, decorator(func), 'module %s exists : function \\\n    return by the decorator should be the same.' % module)", "path": "logilab-common-0.57.2\\test\\unittest_testlib.py", "repo_name": "CS480EmployeeScheduling/Employee-Scheduler", "stars": 42, "license": "None", "language": "python", "size": 474}
{"docstring": "\"\"\"Remove value of domain and check for consistency\"\"\"\n##         print \"removing\", value, \"from\", self._values.keys()\n", "func_signal": "def removeValue(self, value):\n", "code": "if self._cow:\n    self.setValues(self._values)\ndel self._values[value]\nself._valueRemoved()", "path": "constraint-0.4.0\\fd.py", "repo_name": "CS480EmployeeScheduling/Employee-Scheduler", "stars": 42, "license": "None", "language": "python", "size": 474}
{"docstring": "\"\"\"variables is a list of variables which appear in the formula\nformula is a python expression that will be evaluated as a boolean\"\"\"\n", "func_signal": "def __init__(self, variables, formula, type='fd.Expression'):\n", "code": "AbstractConstraint.__init__(self, variables)\nself.formula = formula\nself.type = type\ntry:\n    self.filterFunc = Expression._FILTER_CACHE[formula]\nexcept KeyError:\n    self.filterFunc = eval('lambda %s: %s' % \\\n                                (','.join(variables), formula), {}, {})\n    Expression._FILTER_CACHE[formula] = self.filterFunc", "path": "constraint-0.4.0\\fd.py", "repo_name": "CS480EmployeeScheduling/Employee-Scheduler", "stars": 42, "license": "None", "language": "python", "size": 474}
{"docstring": "\"\"\"issue XML-RPC request\"\"\"\n", "func_signal": "def request(self, host, handler, request_body, verbose=0):\n", "code": "h = self.make_connection(host)\nh.putrequest(\"POST\", handler)\n# required by XML-RPC\nh.putheader(\"User-Agent\", self.user_agent)\nh.putheader(\"Content-Type\", \"text/xml\")\nh.putheader(\"Content-Length\", str(len(request_body)))\nh.putheader(\"Host\", host)\nh.putheader(\"Connection\", \"close\")\n# basic auth\nif self.username is not None and self.password is not None:\n    h.putheader(\"AUTHORIZATION\", \"Basic %s\" % encodestring(\n        \"%s:%s\" % (self.username, self.password)).replace(\"\\012\", \"\"))\nh.endheaders()\n# send body\nif request_body:\n    h.send(request_body)\n# get and check reply\nerrcode, errmsg, headers = h.getreply()\nif errcode != 200:\n    raise ProtocolError(host + handler, errcode, errmsg, headers)\nfile = h.getfile()", "path": "logilab-common-0.57.2\\xmlrpcutils.py", "repo_name": "CS480EmployeeScheduling/Employee-Scheduler", "stars": 42, "license": "None", "language": "python", "size": 474}
{"docstring": "\"\"\" should return a different function : skipping test\n\"\"\"\n", "func_signal": "def test_require_version_bad(self):\n", "code": "def func() :\n    pass\nsys.version_info = (2, 5, 5, 'final', 4)\ncurrent = sys.version_info[:3]\ncompare = ('2.5.6', '2.6', '2.6.5')\nfor version in compare:\n    decorator = require_version(version)\n    self.assertNotEqual(func, decorator(func), '%s >= %s : function \\\n         return by the decorator should NOT be the same.'\n         % ('.'.join([str(element) for element in current]), version))", "path": "logilab-common-0.57.2\\test\\unittest_testlib.py", "repo_name": "CS480EmployeeScheduling/Employee-Scheduler", "stars": 42, "license": "None", "language": "python", "size": 474}
{"docstring": "\"\"\"generic narrowing algorithm for n-ary expressions\"\"\"\n", "func_signal": "def narrow(self, domains):\n", "code": "maybe_entailed = 1\nffunc = self.filterFunc\nresult_cache = self._init_result_cache()\nfor kwargs in self._assign_values(domains):\n    if maybe_entailed:\n        for var, val in kwargs.iteritems():\n            if val not in result_cache[var]:\n                break\n        else:\n            continue\n    if ffunc(**kwargs):\n        for var, val in kwargs.items():\n            result_cache[var][val] = 1\n    else:\n        maybe_entailed = 0\n\ntry:\n    for var, keep in result_cache.iteritems():\n        domain = domains[var]\n        domain.removeValues([val for val in domain if val not in keep])\n        \nexcept ConsistencyFailure:\n    raise ConsistencyFailure('Inconsistency while applying %s' % \\\n                             repr(self))\nexcept KeyError:\n    # There are no more value in result_cache\n    pass\n\nreturn maybe_entailed", "path": "constraint-0.4.0\\fd.py", "repo_name": "CS480EmployeeScheduling/Employee-Scheduler", "stars": 42, "license": "None", "language": "python", "size": 474}
{"docstring": "\"\"\"Encodes a Python object into a JSON-encoded string.\n\nIf 'strict' is set to True, then only strictly-conforming JSON\noutput will be produced.  Note that this means that some types\nof values may not be convertable and will result in a\nJSONEncodeError exception.\n\nIf 'compactly' is set to True, then the resulting string will\nhave all extraneous white space removed; if False then the\nstring will be \"pretty printed\" with whitespace and indentation\nadded to make it more readable.\n\nIf 'escape_unicode' is set to True, then all non-ASCII characters\nwill be represented as a unicode escape sequence; if False then\nthe actual real unicode character will be inserted.\n\nIf no encoding is specified (encoding=None) then the output will\neither be a Python string (if entirely ASCII) or a Python unicode\nstring type.\n\nHowever if an encoding name is given then the returned value will\nbe a python string which is the byte sequence encoding the JSON\nvalue.  As the default/recommended encoding for JSON is UTF-8,\nyou should almost always pass in encoding='utf8'.\n\n\"\"\"\n", "func_signal": "def encode( obj, strict=False, compactly=True, escape_unicode=False, encoding=None ):\n", "code": "import sys\nencoder = None # Custom codec encoding function\nbom = None  # Byte order mark to prepend to final output\ncdk = None  # Codec to use\nif encoding is not None:\n    import codecs\n    try:\n        cdk = codecs.lookup(encoding)\n    except LookupError:\n        cdk = None\n\n    if cdk:\n        pass\n    elif not cdk:\n        # No built-in codec was found, see if it is something we\n        # can do ourself.\n        encoding = encoding.lower()\n        if encoding.startswith('utf-32') or encoding.startswith('utf32') \\\n               or encoding.startswith('ucs4') \\\n               or encoding.startswith('ucs-4'):\n            # Python doesn't natively have a UTF-32 codec, but JSON\n            # requires that it be supported.  So we must decode these\n            # manually.\n            if encoding.endswith('le'):\n                encoder = utf32le_encode\n            elif encoding.endswith('be'):\n                encoder = utf32be_encode\n            else:\n                encoder = utf32be_encode\n                bom = codecs.BOM_UTF32_BE\n        elif encoding.startswith('ucs2') or encoding.startswith('ucs-2'):\n            # Python has no UCS-2, but we can simulate with\n            # UTF-16.  We just need to force us to not try to\n            # encode anything past the BMP.\n            encoding = 'utf-16'\n            if not escape_unicode and not callable(escape_unicode):\n               escape_unicode = lambda c: (0xD800 <= ord(c) <= 0xDFFF) or ord(c) >= 0x10000\n        else:\n            raise JSONEncodeError('this python has no codec for this character encoding',encoding)\n\nif not escape_unicode and not callable(escape_unicode):\n    if encoding and encoding.startswith('utf'):\n        # All UTF-x encodings can do the whole Unicode repertoire, so\n        # do nothing special.\n        pass\n    else:\n        # Even though we don't want to escape all unicode chars,\n        # the encoding being used may force us to do so anyway.\n        # We must pass in a function which says which characters\n        # the encoding can handle and which it can't.\n        def in_repertoire( c, encoding_func ):\n            try:\n                x = encoding_func( c, errors='strict' )\n            except UnicodeError:\n                return False\n            return True\n        if encoder:\n            escape_unicode = lambda c: not in_repertoire(c, encoder)\n        elif cdk:\n            escape_unicode = lambda c: not in_repertoire(c, cdk[0])\n        else:\n            pass # Let the JSON object deal with it\n\nj = JSON( strict=strict, compactly=compactly, escape_unicode=escape_unicode )\n\nunitxt = j.encode( obj )\nif encoder:\n    txt = encoder( unitxt )\nelif encoding is not None:\n    txt = unitxt.encode( encoding )\nelse:\n    txt = unitxt\nif bom:\n    txt = bom + txt\nreturn txt", "path": "demjson.py", "repo_name": "bgolub/blog", "stars": 43, "license": "mit", "language": "python", "size": 3749}
{"docstring": "\"\"\"Try to return the Nan, Infinity, and -Infinity float values.\n\nThis is unnecessarily complex because there is no standard\nplatform- independent way to do this in Python as the language\n(opposed to some implementation of it) doesn't discuss\nnon-numbers.  We try various strategies from the best to the\nworst.\n\nIf this Python interpreter uses the IEEE 754 floating point\nstandard then the returned values will probably be real instances\nof the 'float' type.  Otherwise a custom class object is returned\nwhich will attempt to simulate the correct behavior as much as\npossible.\n\n\"\"\"\n", "func_signal": "def _nonnumber_float_constants():\n", "code": "try:\n    # First, try (mostly portable) float constructor.  Works under\n    # Linux x86 (gcc) and some Unices.\n    nan = float('nan')\n    inf = float('inf')\n    neginf = float('-inf')\nexcept ValueError:\n    try:\n        # Try the AIX (PowerPC) float constructors\n        nan = float('NaNQ')\n        inf = float('INF')\n        neginf = float('-INF')\n    except ValueError:\n        try:\n            # Next, try binary unpacking.  Should work under\n            # platforms using IEEE 754 floating point.\n            import struct, sys\n            xnan = '7ff8000000000000'.decode('hex')  # Quiet NaN\n            xinf = '7ff0000000000000'.decode('hex')\n            xcheck = 'bdc145651592979d'.decode('hex') # -3.14159e-11\n            # Could use float.__getformat__, but it is a new python feature,\n            # so we use sys.byteorder.\n            if sys.byteorder == 'big':\n                nan = struct.unpack('d', xnan)[0]\n                inf = struct.unpack('d', xinf)[0]\n                check = struct.unpack('d', xcheck)[0]\n            else:\n                nan = struct.unpack('d', xnan[::-1])[0]\n                inf = struct.unpack('d', xinf[::-1])[0]\n                check = struct.unpack('d', xcheck[::-1])[0]\n            neginf = - inf\n            if check != -3.14159e-11:\n                raise ValueError('Unpacking raw IEEE 754 floats does not work')\n        except (ValueError, TypeError):\n            # Punt, make some fake classes to simulate.  These are\n            # not perfect though.  For instance nan * 1.0 == nan,\n            # as expected, but 1.0 * nan == 0.0, which is wrong.\n            class nan(float):\n                \"\"\"An approximation of the NaN (not a number) floating point number.\"\"\"\n                def __repr__(self): return 'nan'\n                def __str__(self): return 'nan'\n                def __add__(self,x): return self\n                def __radd__(self,x): return self\n                def __sub__(self,x): return self\n                def __rsub__(self,x): return self\n                def __mul__(self,x): return self\n                def __rmul__(self,x): return self\n                def __div__(self,x): return self\n                def __rdiv__(self,x): return self\n                def __divmod__(self,x): return (self,self)\n                def __rdivmod__(self,x): return (self,self)\n                def __mod__(self,x): return self\n                def __rmod__(self,x): return self\n                def __pow__(self,exp): return self\n                def __rpow__(self,exp): return self\n                def __neg__(self): return self\n                def __pos__(self): return self\n                def __abs__(self): return self\n                def __lt__(self,x): return False\n                def __le__(self,x): return False\n                def __eq__(self,x): return False\n                def __neq__(self,x): return True\n                def __ge__(self,x): return False\n                def __gt__(self,x): return False\n                def __complex__(self,*a): raise NotImplementedError('NaN can not be converted to a complex')\n            if decimal:\n                nan = decimal.Decimal('NaN')\n            else:\n                nan = nan()\n            class inf(float):\n                \"\"\"An approximation of the +Infinity floating point number.\"\"\"\n                def __repr__(self): return 'inf'\n                def __str__(self): return 'inf'\n                def __add__(self,x): return self\n                def __radd__(self,x): return self\n                def __sub__(self,x): return self\n                def __rsub__(self,x): return self\n                def __mul__(self,x):\n                    if x is neginf or x < 0:\n                        return neginf\n                    elif x == 0:\n                        return nan\n                    else:\n                        return self\n                def __rmul__(self,x): return self.__mul__(x)\n                def __div__(self,x):\n                    if x == 0:\n                        raise ZeroDivisionError('float division')\n                    elif x < 0:\n                        return neginf\n                    else:\n                        return self\n                def __rdiv__(self,x):\n                    if x is inf or x is neginf or x is nan:\n                        return nan\n                    return 0.0\n                def __divmod__(self,x):\n                    if x == 0:\n                        raise ZeroDivisionError('float divmod()')\n                    elif x < 0:\n                        return (nan,nan)\n                    else:\n                        return (self,self)\n                def __rdivmod__(self,x):\n                    if x is inf or x is neginf or x is nan:\n                        return (nan, nan)\n                    return (0.0, x)\n                def __mod__(self,x):\n                    if x == 0:\n                        raise ZeroDivisionError('float modulo')\n                    else:\n                        return nan\n                def __rmod__(self,x):\n                    if x is inf or x is neginf or x is nan:\n                        return nan\n                    return x\n                def __pow__(self, exp):\n                    if exp == 0:\n                        return 1.0\n                    else:\n                        return self\n                def __rpow__(self, x):\n                    if -1 < x < 1: return 0.0\n                    elif x == 1.0: return 1.0\n                    elif x is nan or x is neginf or x < 0:\n                        return nan\n                    else:\n                        return self\n                def __neg__(self): return neginf\n                def __pos__(self): return self\n                def __abs__(self): return self\n                def __lt__(self,x): return False\n                def __le__(self,x):\n                    if x is self:\n                        return True\n                    else:\n                        return False\n                def __eq__(self,x):\n                    if x is self:\n                        return True\n                    else:\n                        return False\n                def __neq__(self,x):\n                    if x is self:\n                        return False\n                    else:\n                        return True\n                def __ge__(self,x): return True\n                def __gt__(self,x): return True\n                def __complex__(self,*a): raise NotImplementedError('Infinity can not be converted to a complex')\n            if decimal:\n                inf = decimal.Decimal('Infinity')\n            else:\n                inf = inf()\n            class neginf(float):\n                \"\"\"An approximation of the -Infinity floating point number.\"\"\"\n                def __repr__(self): return '-inf'\n                def __str__(self): return '-inf'\n                def __add__(self,x): return self\n                def __radd__(self,x): return self\n                def __sub__(self,x): return self\n                def __rsub__(self,x): return self\n                def __mul__(self,x):\n                    if x is self or x < 0:\n                        return inf\n                    elif x == 0:\n                        return nan\n                    else:\n                        return self\n                def __rmul__(self,x): return self.__mul__(self)\n                def __div__(self,x):\n                    if x == 0:\n                        raise ZeroDivisionError('float division')\n                    elif x < 0:\n                        return inf\n                    else:\n                        return self\n                def __rdiv__(self,x):\n                    if x is inf or x is neginf or x is nan:\n                        return nan\n                    return -0.0\n                def __divmod__(self,x):\n                    if x == 0:\n                        raise ZeroDivisionError('float divmod()')\n                    elif x < 0:\n                        return (nan,nan)\n                    else:\n                        return (self,self)\n                def __rdivmod__(self,x):\n                    if x is inf or x is neginf or x is nan:\n                        return (nan, nan)\n                    return (-0.0, x)\n                def __mod__(self,x):\n                    if x == 0:\n                        raise ZeroDivisionError('float modulo')\n                    else:\n                        return nan\n                def __rmod__(self,x):\n                    if x is inf or x is neginf or x is nan:\n                        return nan\n                    return x\n                def __pow__(self,exp):\n                    if exp == 0:\n                        return 1.0\n                    else:\n                        return self\n                def __rpow__(self, x):\n                    if x is nan or x is inf or x is inf:\n                        return nan\n                    return 0.0\n                def __neg__(self): return inf\n                def __pos__(self): return self\n                def __abs__(self): return inf\n                def __lt__(self,x): return True\n                def __le__(self,x): return True\n                def __eq__(self,x):\n                    if x is self:\n                        return True\n                    else:\n                        return False\n                def __neq__(self,x):\n                    if x is self:\n                        return False\n                    else:\n                        return True\n                def __ge__(self,x):\n                    if x is self:\n                        return True\n                    else:\n                        return False\n                def __gt__(self,x): return False\n                def __complex__(self,*a): raise NotImplementedError('-Infinity can not be converted to a complex')\n            if decimal:\n                neginf = decimal.Decimal('-Infinity')\n            else:\n                neginf = neginf(0)\nreturn nan, inf, neginf", "path": "demjson.py", "repo_name": "bgolub/blog", "stars": 43, "license": "mit", "language": "python", "size": 3749}
{"docstring": "\"\"\"Is the object of a Python string type?\"\"\"\n", "func_signal": "def isstringtype( obj ):\n", "code": "if isinstance(obj, basestring):\n    return True\n# Must also check for some other pseudo-string types\nimport types, UserString\nreturn isinstance(obj, types.StringTypes) \\\n       or isinstance(obj, UserString.UserString) \\\n       or isinstance(obj, UserString.MutableString)", "path": "demjson.py", "repo_name": "bgolub/blog", "stars": 43, "license": "mit", "language": "python", "size": 3749}
{"docstring": "\"\"\"Determines if the given character is considered as white space.\n\nNote that Javscript is much more permissive on what it considers\nto be whitespace than does JSON.\n\nRef. ECMAScript section 7.2\n\n\"\"\"\n", "func_signal": "def isws(self, c):\n", "code": "if not self._allow_unicode_whitespace:\n    return c in ' \\t\\n\\r'\nelse:\n    if not isinstance(c,unicode):\n        c = unicode(c)\n    if c in u' \\t\\n\\r\\f\\v':\n        return True\n    import unicodedata\n    return unicodedata.category(c) == 'Zs'", "path": "demjson.py", "repo_name": "bgolub/blog", "stars": 43, "license": "mit", "language": "python", "size": 3749}
{"docstring": "\"\"\"Decodes a JSON-encoded string into a Python object.\n\nIf 'strict' is set to True, then those strings that are not\nentirely strictly conforming to JSON will result in a\nJSONDecodeError exception.\n\nThe input string can be either a python string or a python unicode\nstring.  If it is already a unicode string, then it is assumed\nthat no character set decoding is required.\n\nHowever, if you pass in a non-Unicode text string (i.e., a python\ntype 'str') then an attempt will be made to auto-detect and decode\nthe character encoding.  This will be successful if the input was\nencoded in any of UTF-8, UTF-16 (BE or LE), or UTF-32 (BE or LE),\nand of course plain ASCII works too.\n\nNote though that if you know the character encoding, then you\nshould convert to a unicode string yourself, or pass it the name\nof the 'encoding' to avoid the guessing made by the auto\ndetection, as with\n\n    python_object = demjson.decode( input_bytes, encoding='utf8' )\n\nOptional keywords arguments must be of the form\n    allow_xxxx=True/False\nor\n    prevent_xxxx=True/False\nwhere each will allow or prevent the specific behavior, after the\nevaluation of the 'strict' argument.  For example, if strict=True\nthen by also passing 'allow_comments=True' then comments will be\nallowed.  If strict=False then prevent_comments=True will allow\neverything except comments.\n\n\"\"\"\n# Initialize the JSON object\n", "func_signal": "def decode( txt, strict=False, encoding=None, **kw ):\n", "code": "j = JSON( strict=strict )\nfor keyword, value in kw.items():\n    if keyword.startswith('allow_'):\n        behavior = keyword[6:]\n        allow = bool(value)\n    elif keyword.startswith('prevent_'):\n        behavior = keyword[8:]\n        allow = not bool(value)\n    else:\n        raise ValueError('unknown keyword argument', keyword)\n    if allow:\n        j.allow(behavior)\n    else:\n        j.prevent(behavior)\n\n# Convert the input string into unicode if needed.\nif isinstance(txt,unicode):\n    unitxt = txt\nelse:\n    if encoding is None:\n        unitxt = auto_unicode_decode( txt )\n    else:\n        cdk = None # codec\n        decoder = None\n        import codecs\n        try:\n            cdk = codecs.lookup(encoding)\n        except LookupError:\n            encoding = encoding.lower()\n            decoder = None\n            if encoding.startswith('utf-32') \\\n                   or encoding.startswith('ucs4') \\\n                   or encoding.startswith('ucs-4'):\n                # Python doesn't natively have a UTF-32 codec, but JSON\n                # requires that it be supported.  So we must decode these\n                # manually.\n                if encoding.endswith('le'):\n                    decoder = utf32le_decode\n                elif encoding.endswith('be'):\n                    decoder = utf32be_decode\n                else:\n                    if txt.startswith( codecs.BOM_UTF32_BE ):\n                        decoder = utf32be_decode\n                        txt = txt[4:]\n                    elif txt.startswith( codecs.BOM_UTF32_LE ):\n                        decoder = utf32le_decode\n                        txt = txt[4:]\n                    else:\n                        if encoding.startswith('ucs'):\n                            raise JSONDecodeError('UCS-4 encoded string must start with a BOM')\n                        decoder = utf32be_decode # Default BE for UTF, per unicode spec\n            elif encoding.startswith('ucs2') or encoding.startswith('ucs-2'):\n                # Python has no UCS-2, but we can simulate with\n                # UTF-16.  We just need to force us to not try to\n                # encode anything past the BMP.\n                encoding = 'utf-16'\n\n        if decoder:\n            unitxt = decoder(txt)\n        elif encoding:\n            unitxt = txt.decode(encoding)\n        else:\n            raise JSONDecodeError('this python has no codec for this character encoding',encoding)\n\n    # Check that the decoding seems sane.  Per RFC 4627 section 3:\n    #    \"Since the first two characters of a JSON text will\n    #    always be ASCII characters [RFC0020], ...\"\n    #\n    # This check is probably not necessary, but it allows us to\n    # raise a suitably descriptive error rather than an obscure\n    # syntax error later on.\n    #\n    # Note that the RFC requirements of two ASCII characters seems\n    # to be an incorrect statement as a JSON string literal may\n    # have as it's first character any unicode character.  Thus\n    # the first two characters will always be ASCII, unless the\n    # first character is a quotation mark.  And in non-strict\n    # mode we can also have a few other characters too.\n    if len(unitxt) > 2:\n        first, second = unitxt[:2]\n        if first in '\"\\'':\n            pass # second can be anything inside string literal\n        else:\n            if ((ord(first) < 0x20 or ord(first) > 0x7f) or \\\n                (ord(second) < 0x20 or ord(second) > 0x7f)) and \\\n                (not j.isws(first) and not j.isws(second)):\n                # Found non-printable ascii, must check unicode\n                # categories to see if the character is legal.\n                # Only whitespace, line and paragraph separators,\n                # and format control chars are legal here.\n                import unicodedata\n                catfirst = unicodedata.category(unicode(first))\n                catsecond = unicodedata.category(unicode(second))\n                if catfirst not in ('Zs','Zl','Zp','Cf') or \\\n                       catsecond not in ('Zs','Zl','Zp','Cf'):\n                    raise JSONDecodeError('the decoded string is gibberish, is the encoding correct?',encoding)\n# Now ready to do the actual decoding\nobj = j.decode( unitxt )\nreturn obj", "path": "demjson.py", "repo_name": "bgolub/blog", "stars": 43, "license": "mit", "language": "python", "size": 3749}
{"docstring": "\"\"\"Skips all whitespace, including comments and unicode whitespace\n\nTakes a string and a starting index, and returns the index of the\nnext non-whitespace character.\n\nIf skip_comments is True and not running in strict JSON mode, then\ncomments will be skipped over just like whitespace.\n\n\"\"\"\n", "func_signal": "def skipws_any(self, txt, i=0, imax=None, skip_comments=True):\n", "code": "if imax is None:\n    imax = len(txt)\nwhile i < imax:\n    if txt[i] == '/':\n        cmt, i = self.skip_comment(txt, i)\n    if i < imax and self.isws(txt[i]):\n        i += 1\n    else:\n        break\nreturn i", "path": "demjson.py", "repo_name": "bgolub/blog", "stars": 43, "license": "mit", "language": "python", "size": 3749}
{"docstring": "\"\"\"Takes a pair of unicode surrogates and returns the equivalent unicode character.\n\nThe input pair must be a surrogate pair, with c1 in the range\nU+D800 to U+DBFF and c2 in the range U+DC00 to U+DFFF.\n\n\"\"\"\n", "func_signal": "def surrogate_pair_as_unicode( c1, c2 ):\n", "code": "n1, n2 = ord(c1), ord(c2)\nif n1 < 0xD800 or n1 > 0xDBFF or n2 < 0xDC00 or n2 > 0xDFFF:\n    raise JSONDecodeError('illegal Unicode surrogate pair',(c1,c2))\na = n1 - 0xD800\nb = n2 - 0xDC00\nv = (a << 10) | b\nv += 0x10000\nreturn unichr(v)", "path": "demjson.py", "repo_name": "bgolub/blog", "stars": 43, "license": "mit", "language": "python", "size": 3749}
{"docstring": "\"\"\"Encodes just dictionaries, lists, or sequences.\n\nBasically handles any python type for which iter() can create\nan iterator object.\n\nThis method is not intended to be called directly.  Use the\nencode() method instead.\n\n\"\"\"\n#print 'encode_complex_helper(chunklist=%r, obj=%r, nest_level=%r)'%(chunklist,obj,nest_level)\n", "func_signal": "def encode_composite(self, chunklist, obj, nest_level):\n", "code": "try:\n    # Is it a dictionary or UserDict?  Try iterkeys method first.\n    it = obj.iterkeys()\nexcept AttributeError:\n    try:\n        # Is it a sequence?  Try to make an iterator for it.\n        it = iter(obj)\n    except TypeError:\n        it = None\nif it is not None:\n    # Does it look like a dictionary?  Check for a minimal dict or\n    # UserDict interface.\n    isdict = hasattr(obj, '__getitem__') and hasattr(obj, 'keys')\n    compactly = self._encode_compactly\n    if isdict:\n        chunklist.append('{')\n        if compactly:\n            dictcolon = ':'\n        else:\n            dictcolon = ' : '\n    else:\n        chunklist.append('[')\n    #print nest_level, 'opening sequence:', repr(chunklist)\n    if not compactly:\n        indent0 = '  ' * nest_level\n        indent = '  ' * (nest_level+1)\n        chunklist.append(' ')\n    sequence_chunks = []  # use this to allow sorting afterwards if dict\n    try: # while not StopIteration\n        numitems = 0\n        while True:\n            obj2 = it.next()\n            if obj2 is obj:\n                raise JSONEncodeError('trying to encode an infinite sequence',obj)\n            if isdict and not isstringtype(obj2):\n                # Check JSON restrictions on key types\n                if isnumbertype(obj2):\n                    if not self._allow_nonstring_keys:\n                        raise JSONEncodeError('object properties (dictionary keys) must be strings in strict JSON',obj2)\n                else:\n                    raise JSONEncodeError('object properties (dictionary keys) can only be strings or numbers in ECMAScript',obj2)\n\n            # Encode this item in the sequence and put into item_chunks\n            item_chunks = []\n            self.encode_helper( item_chunks, obj2, nest_level=nest_level+1 )\n            if isdict:\n                item_chunks.append(dictcolon)\n                obj3 = obj[obj2]\n                self.encode_helper(item_chunks, obj3, nest_level=nest_level+2)\n\n            #print nest_level, numitems, 'item:', repr(obj2)\n            #print nest_level, numitems, 'sequence_chunks:', repr(sequence_chunks)\n            #print nest_level, numitems, 'item_chunks:', repr(item_chunks)\n            #extend_list_with_sep(sequence_chunks, item_chunks)\n            sequence_chunks.append(item_chunks)\n            #print nest_level, numitems, 'new sequence_chunks:', repr(sequence_chunks)\n            numitems += 1\n    except StopIteration:\n        pass\n\n    if isdict and self._sort_dictionary_keys:\n        sequence_chunks.sort()  # Note sorts by JSON repr, not original Python object\n    if compactly:\n        sep = ','\n    else:\n        sep = ',\\n' + indent\n\n    #print nest_level, 'closing sequence'\n    #print nest_level, 'chunklist:', repr(chunklist)\n    #print nest_level, 'sequence_chunks:', repr(sequence_chunks)\n    extend_and_flatten_list_with_sep( chunklist, sequence_chunks, sep )\n    #print nest_level, 'new chunklist:', repr(chunklist)\n\n    if not compactly:\n        if numitems > 1:\n            chunklist.append('\\n' + indent0)\n        else:\n            chunklist.append(' ')\n    if isdict:\n        chunklist.append('}')\n    else:\n        chunklist.append(']')\nelse: # Can't create an iterator for the object\n    json2 = self.encode_default( obj, nest_level=nest_level )\n    chunklist.append( json2 )", "path": "demjson.py", "repo_name": "bgolub/blog", "stars": 43, "license": "mit", "language": "python", "size": 3749}
{"docstring": "\"\"\"Takes a single unicode character and returns a sequence of surrogate pairs.\n\nThe output of this function is a tuple consisting of one or two unicode\ncharacters, such that if the input character is outside the BMP range\nthen the output is a two-character surrogate pair representing that character.\n\nIf the input character is inside the BMP then the output tuple will have\njust a single character...the same one.\n\n\"\"\"\n", "func_signal": "def unicode_as_surrogate_pair( c ):\n", "code": "n = ord(c)\nif n < 0x10000:\n    return (unichr(n),)  # in BMP, surrogate pair not required\nv = n - 0x10000\nvh = (v >> 10) & 0x3ff   # highest 10 bits\nvl = v & 0x3ff  # lowest 10 bits\nw1 = 0xD800 | vh\nw2 = 0xDC00 | vl\nreturn (unichr(w1), unichr(w2))", "path": "demjson.py", "repo_name": "bgolub/blog", "stars": 43, "license": "mit", "language": "python", "size": 3749}
{"docstring": "\"\"\"Intermediate-level JSON decoder for composite literal types (array and object).\n\nTakes text and a starting index, and returns either a Python list or\ndictionary and the index of the next unparsed character.\n\n\"\"\"\n", "func_signal": "def decode_composite(self, txt, i=0, imax=None):\n", "code": "if imax is None:\n    imax = len(txt)\ni = self.skipws(txt, i, imax)\nstarti = i\nif i >= imax or txt[i] not in '{[':\n    raise JSONDecodeError('composite object must start with \"[\" or \"{\"',txt[i:])\nif txt[i] == '[':\n    isdict = False\n    closer = ']'\n    obj = []\nelse:\n    isdict = True\n    closer = '}'\n    obj = {}\ni += 1 # skip opener\ni = self.skipws(txt, i, imax)\n\nif i < imax and txt[i] == closer:\n    # empty composite\n    i += 1\n    done = True\nelse:\n    saw_value = False   # set to false at beginning and after commas\n    done = False\n    while i < imax:\n        i = self.skipws(txt, i, imax)\n        if i < imax and (txt[i] == ',' or txt[i] == closer):\n            c = txt[i]\n            i += 1\n            if c == ',':\n                if not saw_value:\n                    # no preceeding value, an elided (omitted) element\n                    if isdict:\n                        raise JSONDecodeError('can not omit elements of an object (dictionary)')\n                    if self._allow_omitted_array_elements:\n                        if self._allow_undefined_values:\n                            obj.append( undefined )\n                        else:\n                            obj.append( None )\n                    else:\n                        raise JSONDecodeError('strict JSON does not permit omitted array (list) elements',txt[i:])\n                saw_value = False\n                continue\n            else: # c == closer\n                if not saw_value and not self._allow_trailing_comma_in_literal:\n                    if isdict:\n                        raise JSONDecodeError('strict JSON does not allow a final comma in an object (dictionary) literal',txt[i-2:])\n                    else:\n                        raise JSONDecodeError('strict JSON does not allow a final comma in an array (list) literal',txt[i-2:])\n                done = True\n                break\n\n        # Decode the item\n        if isdict and self._allow_nonstring_keys:\n            r = self.decodeobj(txt, i, identifier_as_string=True)\n        else:\n            r = self.decodeobj(txt, i, identifier_as_string=False)\n        if r:\n            if saw_value:\n                # two values without a separating comma\n                raise JSONDecodeError('values must be separated by a comma', txt[i:r[1]])\n            saw_value = True\n            i = self.skipws(txt, r[1], imax)\n            if isdict:\n                key = r[0]  # Ref 11.1.5\n                if not isstringtype(key):\n                    if isnumbertype(key):\n                        if not self._allow_nonstring_keys:\n                            raise JSONDecodeError('strict JSON only permits string literals as object properties (dictionary keys)',txt[starti:])\n                    else:\n                        raise JSONDecodeError('object properties (dictionary keys) must be either string literals or numbers',txt[starti:])\n                if i >= imax or txt[i] != ':':\n                    raise JSONDecodeError('object property (dictionary key) has no value, expected \":\"',txt[starti:])\n                i += 1\n                i = self.skipws(txt, i, imax)\n                rval = self.decodeobj(txt, i)\n                if rval:\n                    i = self.skipws(txt, rval[1], imax)\n                    obj[key] = rval[0]\n                else:\n                    raise JSONDecodeError('object property (dictionary key) has no value',txt[starti:])\n            else: # list\n                obj.append( r[0] )\n        else: # not r\n            if isdict:\n                raise JSONDecodeError('expected a value, or \"}\"',txt[i:])\n            elif not self._allow_omitted_array_elements:\n                raise JSONDecodeError('expected a value or \"]\"',txt[i:])\n            else:\n                raise JSONDecodeError('expected a value, \",\" or \"]\"',txt[i:])\n    # end while\nif not done:\n    if isdict:\n        raise JSONDecodeError('object literal (dictionary) is not terminated',txt[starti:])\n    else:\n        raise JSONDecodeError('array literal (list) is not terminated',txt[starti:])\nreturn obj, i", "path": "demjson.py", "repo_name": "bgolub/blog", "stars": 43, "license": "mit", "language": "python", "size": 3749}
{"docstring": "\"\"\"Returns a tuple (significant_digits, max_exponent) for the float type.\n\"\"\"\n", "func_signal": "def determine_float_precision():\n", "code": "import math\n# Just count the digits in pi.  The last two decimal digits\n# may only be partial digits, so discount for them.\nwhole, frac = repr(math.pi).split('.')\nsigdigits = len(whole) + len(frac) - 2\n\n# This is a simple binary search.  We find the largest exponent\n# that the float() type can handle without going infinite or\n# raising errors.\nmaxexp = None\nminv = 0; maxv = 1000\nwhile True:\n    if minv+1 == maxv:\n        maxexp = minv - 1\n        break\n    elif maxv < minv:\n        maxexp = None\n        break\n    m = (minv + maxv) // 2\n    try:\n        f = repr(float( '1e+%d' % m ))\n    except ValueError:\n        f = None\n    else:\n        if not f or f[0] < '0' or f[0] > '9':\n            f = None\n    if not f:\n        # infinite\n        maxv = m\n    else:\n        minv = m\nreturn sigdigits, maxexp", "path": "demjson.py", "repo_name": "bgolub/blog", "stars": 43, "license": "mit", "language": "python", "size": 3749}
{"docstring": "\"\"\"Skips whitespace.\n\"\"\"\n", "func_signal": "def skipws(self, txt, i=0, imax=None, skip_comments=True):\n", "code": "if not self._allow_comments and not self._allow_unicode_whitespace:\n    if imax is None:\n        imax = len(txt)\n    while i < imax and txt[i] in ' \\r\\n\\t':\n        i += 1\n    return i\nelse:\n    return self.skipws_any(txt, i, imax, skip_comments)", "path": "demjson.py", "repo_name": "bgolub/blog", "stars": 43, "license": "mit", "language": "python", "size": 3749}
{"docstring": "#print 'encode_helper(chunklist=%r, obj=%r, nest_level=%r)'%(chunklist,obj,nest_level)\n", "func_signal": "def encode_helper(self, chunklist, obj, nest_level):\n", "code": "if hasattr(obj, 'json_equivalent'):\n    json = self.encode_equivalent( obj, nest_level=nest_level )\n    if json is not None:\n        chunklist.append( json )\n        return\nif obj is None:\n    chunklist.append( self.encode_null() )\nelif obj is undefined:\n    if self._allow_undefined_values:\n        chunklist.append( self.encode_undefined() )\n    else:\n        raise JSONEncodeError('strict JSON does not permit \"undefined\" values')\nelif isinstance(obj, bool):\n    chunklist.append( self.encode_boolean(obj) )\nelif isinstance(obj, (int,long,float,complex)) or \\\n         (decimal and isinstance(obj, decimal.Decimal)):\n    chunklist.append( self.encode_number(obj) )\nelif isinstance(obj, basestring) or isstringtype(obj):\n    chunklist.append( self.encode_string(obj) )\nelse:\n    self.encode_composite(chunklist, obj, nest_level)", "path": "demjson.py", "repo_name": "bgolub/blog", "stars": 43, "license": "mit", "language": "python", "size": 3749}
{"docstring": "\"\"\"Is the object of a Python number type (excluding complex)?\"\"\"\n", "func_signal": "def isnumbertype( obj ):\n", "code": "return isinstance(obj, (int,long,float)) \\\n       and not isinstance(obj, bool) \\\n       or obj is nan or obj is inf or obj is neginf", "path": "demjson.py", "repo_name": "bgolub/blog", "stars": 43, "license": "mit", "language": "python", "size": 3749}
{"docstring": "\"\"\"Encodes a Unicode string into a UTF-32BE encoded byte string.\"\"\"\n", "func_signal": "def utf32be_encode( obj, errors='strict' ):\n", "code": "import struct\ntry:\n    import cStringIO as sio\nexcept ImportError:\n    import StringIO as sio\nf = sio.StringIO()\nwrite = f.write\npack = struct.pack\nfor c in obj:\n    n = ord(c)\n    if 0xD800 <= n <= 0xDFFF: # surrogate codepoints are prohibited by UTF-32\n        if errors == 'ignore':\n            continue\n        elif errors == 'replace':\n            n = ord('?')\n        else:\n            cname = 'U+%04X'%n\n            raise UnicodeError('UTF-32 can not encode surrogate characters',cname)\n    write( pack('>L', n) )\nreturn f.getvalue()", "path": "demjson.py", "repo_name": "bgolub/blog", "stars": 43, "license": "mit", "language": "python", "size": 3749}
{"docstring": "\"\"\"Decodes a UTF-32BE byte string into a Unicode string.\"\"\"\n", "func_signal": "def utf32be_decode( obj, errors='strict' ):\n", "code": "if len(obj) % 4 != 0:\n    raise UnicodeError('UTF-32 decode error, data length not a multiple of 4 bytes')\nimport struct\nunpack = struct.unpack\nchars = []\ni = 0\nfor i in range(0, len(obj), 4):\n    seq = obj[i:i+4]\n    n = unpack('>L',seq)[0]\n    chars.append( unichr(n) )\nreturn u''.join( chars )", "path": "demjson.py", "repo_name": "bgolub/blog", "stars": 43, "license": "mit", "language": "python", "size": 3749}
{"docstring": "\"\"\"Creates a JSON encoder/decoder object.\n\nIf 'strict' is set to True, then only strictly-conforming JSON\noutput will be produced.  Note that this means that some types\nof values may not be convertable and will result in a\nJSONEncodeError exception.\n\nIf 'compactly' is set to True, then the resulting string will\nhave all extraneous white space removed; if False then the\nstring will be \"pretty printed\" with whitespace and indentation\nadded to make it more readable.\n\nIf 'escape_unicode' is set to True, then all non-ASCII characters\nwill be represented as a unicode escape sequence; if False then\nthe actual real unicode character will be inserted if possible.\n\nThe 'escape_unicode' can also be a function, which when called\nwith a single argument of a unicode character will return True\nif the character should be escaped or False if it should not.\n\nIf you wish to extend the encoding to ba able to handle\nadditional types, you should subclass this class and override\nthe encode_default() method.\n\n\"\"\"\n", "func_signal": "def __init__(self, strict=False, compactly=True, escape_unicode=False):\n", "code": "import sys\nself._set_strictness(strict)\nself._encode_compactly = compactly\ntry:\n    # see if we were passed a predicate function\n    b = escape_unicode(u'A')\n    self._encode_unicode_as_escapes = escape_unicode\nexcept (ValueError, NameError, TypeError):\n    # Just set to True or False.  We could use lambda x:True\n    # to make it more consistent (always a function), but it\n    # will be too slow, so we'll make explicit tests later.\n    self._encode_unicode_as_escapes = bool(escape_unicode)\nself._sort_dictionary_keys = True\n\n# The following is a boolean map of the first 256 characters\n# which will quickly tell us which of those characters never\n# need to be escaped.\n\nself._asciiencodable = [32 <= c < 128 and not self._rev_escapes.has_key(chr(c))\n                      for c in range(0,255)]", "path": "demjson.py", "repo_name": "bgolub/blog", "stars": 43, "license": "mit", "language": "python", "size": 3749}
{"docstring": "\"\"\"Skips an ECMAScript comment, either // or /* style.\n\nThe contents of the comment are returned as a string, as well\nas the index of the character immediately after the comment.\n\n\"\"\"\n", "func_signal": "def skip_comment(self, txt, i=0):\n", "code": "if i+1 >= len(txt) or txt[i] != '/' or txt[i+1] not in '/*':\n    return None, i\nif not self._allow_comments:\n    raise JSONDecodeError('comments are not allowed in strict JSON',txt[i:])\nmultiline = (txt[i+1] == '*')\nistart = i\ni += 2\nwhile i < len(txt):\n    if multiline:\n        if txt[i] == '*' and i+1 < len(txt) and txt[i+1] == '/':\n            j = i+2\n            break\n        elif txt[i] == '/' and i+1 < len(txt) and txt[i+1] == '*':\n            raise JSONDecodeError('multiline /* */ comments may not nest',txt[istart:i+1])\n    else:\n        if self.islineterm(txt[i]):\n            j = i  # line terminator is not part of comment\n            break\n    i += 1\n\nif i >= len(txt):\n    if not multiline:\n        j = len(txt)  # // comment terminated by end of file is okay\n    else:\n        raise JSONDecodeError('comment was never terminated',txt[istart:])\nreturn txt[istart:j], j", "path": "demjson.py", "repo_name": "bgolub/blog", "stars": 43, "license": "mit", "language": "python", "size": 3749}
{"docstring": "\"\"\"Changes the strictness behavior.\n\nPass True to be very strict about JSON syntax, or False to be looser.\n\"\"\"\n", "func_signal": "def _set_strictness(self, strict):\n", "code": "self._allow_any_type_at_start = not strict\nself._allow_all_numeric_signs = not strict\nself._allow_comments = not strict\nself._allow_control_char_in_string = not strict\nself._allow_hex_numbers = not strict\nself._allow_initial_decimal_point = not strict\nself._allow_js_string_escapes = not strict\nself._allow_non_numbers = not strict\nself._allow_nonescape_characters = not strict  # \"\\z\" -> \"z\"\nself._allow_nonstring_keys = not strict\nself._allow_omitted_array_elements = not strict\nself._allow_single_quoted_strings = not strict\nself._allow_trailing_comma_in_literal = not strict\nself._allow_undefined_values = not strict\nself._allow_unicode_format_control_chars = not strict\nself._allow_unicode_whitespace = not strict\n# Always disable this by default\nself._allow_octal_numbers = False", "path": "demjson.py", "repo_name": "bgolub/blog", "stars": 43, "license": "mit", "language": "python", "size": 3749}
{"docstring": "\"\"\"Decodes a JSON-endoded string into a Python object.\"\"\"\n", "func_signal": "def decode(self, txt):\n", "code": "if self._allow_unicode_format_control_chars:\n    txt = self.strip_format_control_chars(txt)\nr = self.decodeobj(txt, 0, only_object_or_array=not self._allow_any_type_at_start)\nif not r:\n    raise JSONDecodeError('can not decode value',txt)\nelse:\n    obj, i = r\n    i = self.skipws(txt, i)\n    if i < len(txt):\n        raise JSONDecodeError('unexpected or extra text',txt[i:])\nreturn obj", "path": "demjson.py", "repo_name": "bgolub/blog", "stars": 43, "license": "mit", "language": "python", "size": 3749}
{"docstring": "'''\nTranslates addresses to import names through a dictionary lookup.\n\n@type iaddr: address\n@param iaddr: Address of import\n\n@return: name (if successful) or same argument (on failure)  '''\n\n", "func_signal": "def _find_import_name(self, iaddr):\n", "code": "for k in self.import_dict.keys():\n    if self.import_dict[k] == iaddr:\n        name = k\n        \nif name:\n    return name\nelse:\n    return iaddr", "path": "milf.py", "repo_name": "carlosgprado/MILF", "stars": 55, "license": "None", "language": "python", "size": 740}
{"docstring": "'''\nCreates a downgraph of xrefs FROM this function.\nCalling it recursively allow us to get infinite depth.\n\n@type ea: ()\n@param ea: address of ROOT NODE\n\n@rtype: dictionary\n@return: Dictionary of function ea's and child *addresses* { ea : [c1_ea, c2_ea, ...] } '''\n\n", "func_signal": "def graph_down(self, ea, graph = {}, path = set([])):\n", "code": "graph[ea] = list()    # Create a new entry on the graph dictionary {node: [child1, child2, ...], ...}\npath.add(ea)        # This is a set, therefore the add() method\n\n# Iterate through all function instructions and take only call instructions\nfor x in [x for x in FuncItems(ea) if is_call_insn(x)]:        # Take the call elements\n    for xref in XrefsFrom(x, XREF_FAR):                                   \n        if not xref.iscode:\n            continue\n                \n        if xref.to not in path:        # Eliminates recursions\n            graph[ea].append(xref.to)\n            self.graph_down(xref.to, graph, path)\n            \nreturn graph", "path": "milf.py", "repo_name": "carlosgprado/MILF", "stars": 55, "license": "None", "language": "python", "size": 740}
{"docstring": "'''\nEnumerate all switches in downgraph\nShamelessly copied from Aaron Portnoy :)\n\n@type graph: graph\n@param graph: Complex structure. See connect_graph()\n\n@rtype: dictionary\n@return: dictionary { address : [cmp_mnem, disasm] } '''\n\n", "func_signal": "def enum_switches(self, graph):\n", "code": "switch_dict = dict()\njmpList = ['jmp', 'jz', 'jnz', 'jg', 'jl', 'ja', 'jb']\n        \n# Extract a *list* of nodes from the graph data structure\ngraph_list = [x['node'] for x in graph.itervalues()]\n         \nfor func_start in graph_list:\n    # if the function end isn't defined (probably a library call) then skip it\n    func_end = FindFuncEnd(func_start)\n    if func_end == 0xFFFFFFFF:\n        continue\n\n    for instr in FuncItems(func_start):\n        # check for switch jump                    \n        if GetMnem(instr) in jmpList:\n            # step backwards and find the cmp for the # of cases (if possible)\n            prev_instruction = PrevHead(instr, 0)\n\n            count = 5\n            while count > 0:\n                if GetMnem(prev_instruction) == 'cmp':\n                    # get comparison number, plus for for case 0\n                    cmp_mnem = GetDisasm(prev_instruction)\n                    switch_dict[instr] = [cmp_mnem, GetDisasm(instr)]\n                    break\n                \n            prev_instruction = PrevHead(prev_instruction, 0)    \n            count -= 1\n\n\nreturn switch_dict", "path": "milf.py", "repo_name": "carlosgprado/MILF", "stars": 55, "license": "None", "language": "python", "size": 740}
{"docstring": "'''\nSimple algorithm to reduce small loops in trace files.\nLoops of the type a -> b -> a ... are identified.\n@return: two-dimensional list (int addr, str comment), where the comment \n         indicate the number of times the loop occurred or empty string if none. '''\n\n", "func_signal": "def _find_trace_loops(self, bb_addr):\n", "code": "idx     = 0\nloop     = 0\n\nwhile idx < len(bb_addr):\n    # Implement a simple logic here\n    pass", "path": "milf.py", "repo_name": "carlosgprado/MILF", "stars": 55, "license": "None", "language": "python", "size": 740}
{"docstring": "'''\nFinds interesting imported functions and the nodes that call them. \nVery handy in locating user inputs.\n\n@attention: There are imports called through a thunk and directly.\n@rtype: Dictionary (of lists)\n@return: Dictionary containing *the address of the functions* \n         calling the imports,\n         {fn_call_ea: [idata1_ea, idata2_ea, ...], ...}\n          \n@todo: IIRC this needs some review '''\n\n", "func_signal": "def _find_import_callers(self, regexp):\n", "code": "importCallers = dict()\nimportPattern = re.compile(regexp, re.IGNORECASE)\n\nfor imp_name, idata_ea in self.import_dict.iteritems():\n    # This dict has the *IAT names* (i.e. __imp_ReadFile, within the .idata section)\n    if importPattern.match(imp_name):\n        for import_caller in XrefsTo(idata_ea, True):\n            import_caller_addr = import_caller.frm\n            import_caller_fn = get_func(import_caller_addr)\n            \n            if import_caller_fn:\n                \n                # Check if caller is a THUNK\n                if (import_caller_fn.flags & idaapi.FUNC_THUNK) != 0:\n                    # It IS a thunk\n                    for thunk_caller in XrefsTo(import_caller_addr, True):\n                        thunk_caller_fn = get_func(thunk_caller.frm)\n                        import_caller_ea = thunk_caller_fn.startEA\n                        if importCallers.has_key(import_caller_ea):\n                            # Remove nasty duplicates\n                            if idata_ea in importCallers[import_caller_ea]:\n                                continue\n                            else:\n                                importCallers[import_caller_ea].append(idata_ea)\n                        else:\n                            importCallers[import_caller_ea] = [idata_ea]\n                            \n                else:\n                    # It is NOT a thunk, no need for recursion                    \n                    import_caller_ea = import_caller_fn.startEA\n                    \n                    if importCallers.has_key(import_caller_ea):\n                        # Remove nasty duplicates\n                        if idata_ea in importCallers[import_caller_ea]:\n                            continue\n                        else:\n                            importCallers[import_caller_ea].append(idata_ea)\n                    else:\n                        importCallers[import_caller_ea] = [idata_ea]\n\n            else:\n                #import_caller_fn is None\n                pass\n            \n            \nreturn importCallers", "path": "milf.py", "repo_name": "carlosgprado/MILF", "stars": 55, "license": "None", "language": "python", "size": 740}
{"docstring": "'''\nTriggered when a menu command is selected through the menu of hotkey\n@return: None '''\n\n", "func_signal": "def OnCommand(self, cmd_id):\n", "code": "if cmd_id == self.cmd_close:\n    self.Close()\n    return", "path": "milf.py", "repo_name": "carlosgprado/MILF", "stars": 55, "license": "None", "language": "python", "size": 740}
{"docstring": "'''\nInternal method. See show_path() for an example wrapper.\nIt paints a *list* of functions with some color.\n\n@type graph: List\n@param graph: List of nodes_ea\n\n@type color: hex\n@param color: (optional) color to paint the functions  '''\n\n", "func_signal": "def _colorize_graph(self, node_list, color = 0x2020c0):\n", "code": "for x in node_list:\n    SetColor(x, CIC_FUNC, color)\n\nreturn True", "path": "milf.py", "repo_name": "carlosgprado/MILF", "stars": 55, "license": "None", "language": "python", "size": 740}
{"docstring": "'''\nConvenience method.\nSet color back to white for selected graph.\n\n@type graph: List\n@param graph: List of nodes\n\n@note: Call with \"all\" string to reset the whole module. '''\n\n", "func_signal": "def reset_colorize_graph(self, c_graph):\n", "code": "WHITE = 0xffffff\n\nif c_graph == 'all':\n    for function in Functions():\n        SetColor(function, CIC_FUNC, WHITE)\nelse:\n    self._colorize_graph(c_graph, WHITE)\n    \nreturn True", "path": "milf.py", "repo_name": "carlosgprado/MILF", "stars": 55, "license": "None", "language": "python", "size": 740}
{"docstring": "'''\nThis is in case I would like to add, for example,\na confirmation dialog in the future\n'''\n", "func_signal": "def DelEntry(self, line_no):\n", "code": "self.DelLine(line_no)\n\nreturn True", "path": "milf.py", "repo_name": "carlosgprado/MILF", "stars": 55, "license": "None", "language": "python", "size": 740}
{"docstring": "''' initialization of the main class '''\n", "func_signal": "def __init__(self, debug = False, nx_support = True):\n", "code": "self.import_dict = dict()\nself.debug = debug\nself.nx_support = nx_support\n\n            \n# Create a dictionary with all imports.\n# It populates self.import_dict\nself._enum_all_imports()", "path": "milf.py", "repo_name": "carlosgprado/MILF", "stars": 55, "license": "None", "language": "python", "size": 740}
{"docstring": "'''\nConvenience function. It colors all switches in a graph.\n@type graph: dictionary\n@param graph: Complex data structure. See connect_graph()\n\n@type color: hex\n@param color:(optional) color to mark the switches\n \n@return: True '''\n\n", "func_signal": "def mark_switches(self, graph, color = 0x20c020):\n", "code": "switches = self.enum_switches(graph)\nfor sw in switches.keys():\n    SetColor(sw, CIC_ITEM, color)\n\n\nreturn True", "path": "milf.py", "repo_name": "carlosgprado/MILF", "stars": 55, "license": "None", "language": "python", "size": 740}
{"docstring": "'''Menus are better than no GUI at all *sigh*'''\n\n", "func_signal": "def AddMenuElements(self):\n", "code": "idaapi.add_menu_item(\"Edit/Plugins/\", \"MILF: Select Origin Basic Block\", \"\", 0, self.MilfMarkOriginBB, ())\nidaapi.add_menu_item(\"Edit/Plugins/\", \"MILF: Select Destination Basic Block\", \"\", 0, self.MilfMarkDestBB, ())\nidaapi.add_menu_item(\"Edit/Plugins/\", \"MILF: Connect Blocks!\", \"\", 0, self.MilfConnectBlocks, ())\nidaapi.add_menu_item(\"Edit/Plugins/\", \"MILF: Most referenced functions\", \"\", 0, self.MilfMostReferenced, ())\nidaapi.add_menu_item(\"Edit/Plugins/\", \"MILF: Connect Graph\", \"Ctrl+F8\", 0, self.MilfConnGraph, ())\nidaapi.add_menu_item(\"Edit/Plugins/\", \"MILF: Mark dangerous functions\", \"Ctrl+F9\", 0, self.MilfMarkDangerous, ())\nidaapi.add_menu_item(\"Edit/Plugins/\", \"MILF: Mark immediate compares\", \"Ctrl+F10\", 0, self.MarkImmCompares, ())\nidaapi.add_menu_item(\"Edit/Plugins/\", \"MILF: Locate allocs\", \"Ctrl+F11\", 0, self.MilfLocateAllocs, ())\nidaapi.add_menu_item(\"Edit/Plugins/\", \"MILF: Locate network IO\", \"Ctrl+F12\", 0, self.MilfLocateNetIO, ())\nidaapi.add_menu_item(\"Edit/Plugins/\", \"MILF: Mark dangerous size params\", \"\", 0, self.MilfMarkDangerousSize, ())\nidaapi.add_menu_item(\"Edit/Plugins/\", \"MILF: Reset all markings\", \"\", 0, self.MilfResetMarkings, ())\nidaapi.add_menu_item(\"Edit/Plugins/\", \"MILF: Export function addresses to disk\", \"\", 0, self.MilfExportFunctions, ())\nidaapi.add_menu_item(\"Edit/Plugins/\", \"MILF: Export function addresses (and arguments info) to disk\", \"\", 0, self.MilfExportFunctionsAdvanced, ())\nidaapi.add_menu_item(\"Edit/Plugins/\", \"MILF: Import function addresses from file\", \"\", 0, self.MilfImportFunctions, ())\nidaapi.add_menu_item(\"Edit/Plugins/\", \"MILF: Import basic blocks from file\", \"\", 0, self.MilfImportBasicBlocks, ())", "path": "milf.py", "repo_name": "carlosgprado/MILF", "stars": 55, "license": "None", "language": "python", "size": 740}
{"docstring": "'''\nWrapper to connect_graph(). This allows origin to be an import.\nEx. Graph between \"recv\" and \"WriteFile\"\n\n@type origin: string\n@param origin: Function NAME\n\n@type destination: string\n@param destination: Function NAME\n\n@note: This returns several \"connect graphs\", one for every function \n       calling the \"origin\" import. Indexed by address.\n       See connect_graph() for graph type definition.\n       \n@rtype:  dictionary\n@return: Complex data {imp_caller1_ea : connect_graph1, ...} '''\n\n", "func_signal": "def connect_graph_import(self, origin, destination):\n", "code": "graph_dict = dict()\nimport_callers_dict = self._find_import_callers(origin)\n\nfor imp_caller_addr in import_callers_dict.keys():\n    # imp_caller_addr is the address within the function, where \n    # the actual call instruction is located, not the ea (beginning) \n    imp_caller_name = GetFunctionName(imp_caller_addr)\n    imp_caller_ea = LocByName(imp_caller_name)\n    graph_dict[imp_caller_ea] = self.connect_graph(imp_caller_name, destination)\n    \nreturn graph_dict", "path": "milf.py", "repo_name": "carlosgprado/MILF", "stars": 55, "license": "None", "language": "python", "size": 740}
{"docstring": "'''\nTranslates an ea to a function/import name.\n\n@type ea: address\n@param ea: address to lookup\n\n@return: function/import name (on success) or same argument (on failure) '''\n\n", "func_signal": "def _translate_ea_name(self, ea):\n", "code": "t = GetFunctionName(ea)\nif not t:\n    if SegName(ea) == '.idata':\n        # The address is inside the imports section\n        t = self._find_import_name(ea)\n        if not t:\n            t = ea\n    else:\n        t = ea\n        \nreturn t", "path": "milf.py", "repo_name": "carlosgprado/MILF", "stars": 55, "license": "None", "language": "python", "size": 740}
{"docstring": "''' Callback for double-click '''\n\n", "func_signal": "def OnSelectLine(self, n):\n", "code": "trace_addr = int(self.items[n][1], 16)\nSetColor(trace_addr, CIC_ITEM, 0x3db43d)\nJump(trace_addr)", "path": "milf.py", "repo_name": "carlosgprado/MILF", "stars": 55, "license": "None", "language": "python", "size": 740}
{"docstring": "'''\nAnalogous to graph_down().\nTo set the \"root\" block, call with CurrentBlockLayer = [bb_src_ea] \n\n@rtype: set\n@return: set containing upgraph blocks '''\n\n", "func_signal": "def _aux_calc_down_set(self, f, CurrentBlockLayer, DownGraphBlockSet = set([])):\n", "code": "self.FuncFlowChart = FlowChart(f)\nself.CurrentBlockLayer = CurrentBlockLayer\nself.NextBlockLayer = list()\n\n# Iterate through all basic blocks and get the egress connections.\nfor bb in self.CurrentBlockLayer:                    # bb: address\n    block = self._aux_lookup_ea_bb(f, bb)\n    for enode in block.succs():                        # enode: basic block type\n        if enode.startEA not in DownGraphBlockSet:    # Eliminates recursions\n            self.NextBlockLayer.append(enode.startEA)\n            DownGraphBlockSet.add(enode.startEA)\n            self._aux_calc_down_set(f, self.NextBlockLayer, DownGraphBlockSet)\n        \nreturn DownGraphBlockSet", "path": "milf.py", "repo_name": "carlosgprado/MILF", "stars": 55, "license": "None", "language": "python", "size": 740}
{"docstring": "'''\nTakes a graph, { node: [child1, child2, ...], ...}\nand lookup as many function names as possible.\n\n@type graph: dictionary\n@param graph: dictionary of function ea's and \"child\" nodes { ea : [c1_ea, c2_ea, ...] }\n\n@rtype: dictionary\n@return: same dictionary but names instead of ea's (where possible) '''\n\n", "func_signal": "def translate_graph(self, graph):\n", "code": "translated_graph = dict()\n\n# This loop translates the dict keys (nodes)\nfor node in graph.keys():\n    translated_key = self._translate_ea_name(node)\n    translated_graph[translated_key] = list()\n    # This loop translates the dict values (children)\n    for child in graph[node]: # traverses a list\n        translated_graph[translated_key].append(self._translate_ea_name(child))\n            \n    \nreturn translated_graph", "path": "milf.py", "repo_name": "carlosgprado/MILF", "stars": 55, "license": "None", "language": "python", "size": 740}
{"docstring": "'''\nColorizes a path.\nOriginally though to be useful to visualize \"connect graphs\".\n\n@type origin: string\n@param origin: Function NAME\n\n@type destination: string\n@param destination: Function NAME\n\n@rtype: dictionary\n@return: Complex struct. See connect_graph() '''\n\n", "func_signal": "def show_path(self, origin, destination, color = 0x2020c0):\n", "code": "conn_graph = self.connect_graph(origin, destination)\n\n# The connection graph is a complex data structure, but\n# _colorize_graph() argument is a list of nodes\ngraph_list = [x['node'] for x in conn_graph.itervalues()]\n\nself._colorize_graph(graph_list, color)\n\nreturn conn_graph", "path": "milf.py", "repo_name": "carlosgprado/MILF", "stars": 55, "license": "None", "language": "python", "size": 740}
{"docstring": "'''@todo: this algorithm is a bit clumsy. Get back to it.'''\n\n", "func_signal": "def OnRefresh(self):\n", "code": "self.Clear()\nidNode = dict() # { node_ea : node_id }\n\nfor x in self.graph.itervalues():\n    # First, add all nodes and populate the idNode list\n    node_ea = x['node']\n    idNode[node_ea] = self.AddNode(node_ea)\n\nfor node_ea, x in self.graph.iteritems():\n    # Link the node with parents and children\n    # These 'children' elements are *all* references from the node,\n    # not just the ones belonging to the connected graph.\n    for c in x['children']:\n        try:\n            self.AddEdge(idNode[node_ea], idNode[c])\n        except:\n            continue\n        \n    for p in x['parents']:\n        try:\n            self.AddEdge(idNode[p], idNode[node_ea])\n        except:\n            continue\n\n# Calculate a handy reverse dictionary { node_id: node_ea}\nself.AddrNode = dict()\nfor ea,id in idNode.iteritems():\n    self.AddrNode[id] = ea\n            \nreturn True", "path": "milf.py", "repo_name": "carlosgprado/MILF", "stars": 55, "license": "None", "language": "python", "size": 740}
{"docstring": "'''\nReturns a basic block object given an address\n\n@type f: func_t object\n@param f: represents the function of interest\n\n@type ea: address\n@param ea: address of the basic block\n\n@rtype: Basic Block Object\n@return: well... a basic block object :) '''\n\n", "func_signal": "def _aux_lookup_ea_bb(self, f, ea):\n", "code": "self.f = f\nself.ea = ea\nself.FlowChart = FlowChart(f)\n\nfor bb in self.FlowChart:\n    if bb.startEA == self.ea:\n        return bb\n    \nreturn False", "path": "milf.py", "repo_name": "carlosgprado/MILF", "stars": 55, "license": "None", "language": "python", "size": 740}
{"docstring": "'''Returns a single user by email address.\n\nArgs:\n  email: The email of the user to retrieve.\nReturns:\n  A twitter.User instance representing that user\n'''\n", "func_signal": "def GetUserByEmail(self, email):\n", "code": "url = 'http://twitter.com/users/show.json?email=%s' % email\njson = self._FetchUrl(url)\ndata = simplejson.loads(json)\nself._CheckForTwitterError(data)\nreturn User.NewFromJsonDict(data)", "path": "twitter.py", "repo_name": "yasulab/SimpleTwitterBot", "stars": 34, "license": "None", "language": "python", "size": 436}
{"docstring": "'''Befriends the user specified in the user parameter as the authenticating user.\n\nThe twitter.Api instance must be authenticated.\n\nArgs:\n  The ID or screen name of the user to befriend.\nReturns:\n  A twitter.User instance representing the befriended user.\n'''\n", "func_signal": "def CreateFriendship(self, user):\n", "code": "url = 'http://twitter.com/friendships/create/%s.json' % user\njson = self._FetchUrl(url, post_data={})\ndata = simplejson.loads(json)\nself._CheckForTwitterError(data)\nreturn User.NewFromJsonDict(data)", "path": "twitter.py", "repo_name": "yasulab/SimpleTwitterBot", "stars": 34, "license": "None", "language": "python", "size": 436}
{"docstring": "'''Returns a single status message.\n\nThe twitter.Api instance must be authenticated if the status message is private.\n\nArgs:\n  id: The numerical ID of the status you're trying to retrieve.\n\nReturns:\n  A twitter.Status instance representing that status message\n'''\n", "func_signal": "def GetStatus(self, id):\n", "code": "try:\n  if id:\n    long(id)\nexcept:\n  raise TwitterError(\"id must be an long integer\")\nurl = 'http://twitter.com/statuses/show/%s.json' % id\njson = self._FetchUrl(url)\ndata = simplejson.loads(json)\nself._CheckForTwitterError(data)\nreturn Status.NewFromJsonDict(data)", "path": "twitter.py", "repo_name": "yasulab/SimpleTwitterBot", "stars": 34, "license": "None", "language": "python", "size": 436}
{"docstring": "'''A dict representation of this twitter.Status instance.\n\nThe return value uses the same key names as the JSON representation.\n\nReturn:\n  A dict representing this twitter.Status instance\n'''\n", "func_signal": "def AsDict(self):\n", "code": "data = {}\nif self.created_at:\n  data['created_at'] = self.created_at\nif self.favorited:\n  data['favorited'] = self.favorited\nif self.id:\n  data['id'] = self.id\nif self.text:\n  data['text'] = self.text\nif self.user:\n  data['user'] = self.user.AsDict()\nif self.in_reply_to_screen_name:\n  data['in_reply_to_screen_name'] = self.in_reply_to_screen_name\nif self.in_reply_to_user_id:\n  data['in_reply_to_user_id'] = self.in_reply_to_user_id\nif self.in_reply_to_status_id:\n  data['in_reply_to_status_id'] = self.in_reply_to_status_id\nif self.truncated is not None:\n  data['truncated'] = self.truncated\nif self.favorited is not None:\n  data['favorited'] = self.favorited\nif self.source:\n  data['source'] = self.source\nreturn data", "path": "twitter.py", "repo_name": "yasulab/SimpleTwitterBot", "stars": 34, "license": "None", "language": "python", "size": 436}
{"docstring": "'''Create a new instance based on a JSON dict.\n\nArgs:\n  data: A JSON dict, as converted from the JSON in the twitter API\nReturns:\n  A twitter.User instance\n'''\n", "func_signal": "def NewFromJsonDict(data):\n", "code": "if 'status' in data:\n  status = Status.NewFromJsonDict(data['status'])\nelse:\n  status = None\nreturn User(id=data.get('id', None),\n            name=data.get('name', None),\n            screen_name=data.get('screen_name', None),\n            location=data.get('location', None),\n            description=data.get('description', None),\n            statuses_count=data.get('statuses_count', None),\n            followers_count=data.get('followers_count', None),\n            favourites_count=data.get('favourites_count', None),\n            friends_count=data.get('friends_count', None),\n            profile_image_url=data.get('profile_image_url', None),\n            profile_background_tile = data.get('profile_background_tile', None),\n            profile_background_image_url = data.get('profile_background_image_url', None),\n            profile_sidebar_fill_color = data.get('profile_sidebar_fill_color', None),\n            profile_background_color = data.get('profile_background_color', None),\n            profile_link_color = data.get('profile_link_color', None),\n            profile_text_color = data.get('profile_text_color', None),\n            protected = data.get('protected', None),\n            utc_offset = data.get('utc_offset', None),\n            time_zone = data.get('time_zone', None),\n            url=data.get('url', None),\n            status=status)", "path": "twitter.py", "repo_name": "yasulab/SimpleTwitterBot", "stars": 34, "license": "None", "language": "python", "size": 436}
{"docstring": "'''Post a twitter direct message from the authenticated user\n\nThe twitter.Api instance must be authenticated.\n\nArgs:\n  user: The ID or screen name of the recipient user.\n  text: The message text to be posted.  Must be less than 140 characters.\n\nReturns:\n  A twitter.DirectMessage instance representing the message posted\n'''\n", "func_signal": "def PostDirectMessage(self, user, text):\n", "code": "if not self._username:\n  raise TwitterError(\"The twitter.Api instance must be authenticated.\")\nurl = 'http://twitter.com/direct_messages/new.json'\ndata = {'text': text, 'user': user}\njson = self._FetchUrl(url, post_data=data)\ndata = simplejson.loads(json)\nself._CheckForTwitterError(data)\nreturn DirectMessage.NewFromJsonDict(data)", "path": "twitter.py", "repo_name": "yasulab/SimpleTwitterBot", "stars": 34, "license": "None", "language": "python", "size": 436}
{"docstring": "'''Destroys the direct message specified in the required ID parameter.\n\nThe twitter.Api instance must be authenticated, and the\nauthenticating user must be the recipient of the specified direct\nmessage.\n\nArgs:\n  id: The id of the direct message to be destroyed\n\nReturns:\n  A twitter.DirectMessage instance representing the message destroyed\n'''\n", "func_signal": "def DestroyDirectMessage(self, id):\n", "code": "url = 'http://twitter.com/direct_messages/destroy/%s.json' % id\njson = self._FetchUrl(url, post_data={})\ndata = simplejson.loads(json)\nself._CheckForTwitterError(data)\nreturn DirectMessage.NewFromJsonDict(data)", "path": "twitter.py", "repo_name": "yasulab/SimpleTwitterBot", "stars": 34, "license": "None", "language": "python", "size": 436}
{"docstring": "'''Fetch the sequence of twitter.User instances, one for each follower\n\nThe twitter.Api instance must be authenticated.\n\nReturns:\n  A sequence of twitter.User instances, one for each follower\n'''\n", "func_signal": "def GetFollowers(self, page=None):\n", "code": "if not self._username:\n  raise TwitterError(\"twitter.Api instance must be authenticated\")\nurl = 'http://twitter.com/statuses/followers.json'\nparameters = {}\nif page:\n  parameters['page'] = page\njson = self._FetchUrl(url, parameters=parameters)\ndata = simplejson.loads(json)\nself._CheckForTwitterError(data)\nreturn [User.NewFromJsonDict(x) for x in data]", "path": "twitter.py", "repo_name": "yasulab/SimpleTwitterBot", "stars": 34, "license": "None", "language": "python", "size": 436}
{"docstring": "'''Returns a single user.\n\nThe twitter.Api instance must be authenticated.\n\nArgs:\n  user: The username or id of the user to retrieve.\n\nReturns:\n  A twitter.User instance representing that user\n'''\n", "func_signal": "def GetUser(self, user):\n", "code": "url = 'http://twitter.com/users/show/%s.json' % user\njson = self._FetchUrl(url)\ndata = simplejson.loads(json)\nself._CheckForTwitterError(data)\nreturn User.NewFromJsonDict(data)", "path": "twitter.py", "repo_name": "yasulab/SimpleTwitterBot", "stars": 34, "license": "None", "language": "python", "size": 436}
{"docstring": "'''Clear the username and password for this instance\n'''\n", "func_signal": "def ClearCredentials(self):\n", "code": "self._username = None\nself._password = None", "path": "twitter.py", "repo_name": "yasulab/SimpleTwitterBot", "stars": 34, "license": "None", "language": "python", "size": 436}
{"docstring": "\"\"\"Return the Python representation of ``s`` (a ``str`` or ``unicode``\ninstance containing a JSON document)\n\n\"\"\"\n", "func_signal": "def decode(self, s, _w=WHITESPACE.match):\n", "code": "obj, end = self.raw_decode(s, idx=_w(s, 0).end())\nend = _w(s, end).end()\nif end != len(s):\n    raise ValueError(errmsg(\"Extra data\", s, end, len(s)))\nreturn obj", "path": "python-twitter-0.6\\simplejson\\decoder.py", "repo_name": "yasulab/SimpleTwitterBot", "stars": 34, "license": "None", "language": "python", "size": 436}
{"docstring": "'''Fetch the sequnce of public twitter.Status message for all users.\n\nArgs:\n  since_id:\n    Returns only public statuses with an ID greater than (that is,\n    more recent than) the specified ID. [Optional]\n\nReturns:\n  An sequence of twitter.Status instances, one for each message\n'''\n", "func_signal": "def GetPublicTimeline(self, since_id=None):\n", "code": "parameters = {}\nif since_id:\n  parameters['since_id'] = since_id\nurl = 'http://twitter.com/statuses/public_timeline.json'\njson = self._FetchUrl(url,  parameters=parameters)\ndata = simplejson.loads(json)\nself._CheckForTwitterError(data)\nreturn [Status.NewFromJsonDict(x) for x in data]", "path": "twitter.py", "repo_name": "yasulab/SimpleTwitterBot", "stars": 34, "license": "None", "language": "python", "size": 436}
{"docstring": "'''Fetch the sequence of twitter.User instances featured on twitter.com\n\nThe twitter.Api instance must be authenticated.\n\nReturns:\n  A sequence of twitter.User instances\n'''\n", "func_signal": "def GetFeatured(self):\n", "code": "url = 'http://twitter.com/statuses/featured.json'\njson = self._FetchUrl(url)\ndata = simplejson.loads(json)\nself._CheckForTwitterError(data)\nreturn [User.NewFromJsonDict(x) for x in data]", "path": "twitter.py", "repo_name": "yasulab/SimpleTwitterBot", "stars": 34, "license": "None", "language": "python", "size": 436}
{"docstring": "'''Return a string in key=value&key=value form\n\nValues of None are not included in the output string.\n\nArgs:\n  parameters:\n    A dict of (key, value) tuples, where value is encoded as\n    specified by self._encoding\nReturns:\n  A URL-encoded string in \"key=value&key=value\" form\n'''\n", "func_signal": "def _EncodeParameters(self, parameters):\n", "code": "if parameters is None:\n  return None\nelse:\n  return urllib.urlencode(dict([(k, self._Encode(v)) for k, v in parameters.items() if v is not None]))", "path": "twitter.py", "repo_name": "yasulab/SimpleTwitterBot", "stars": 34, "license": "None", "language": "python", "size": 436}
{"docstring": "'''Post a twitter status message from the authenticated user.\n\nThe twitter.Api instance must be authenticated.\n\nArgs:\n  status:\n    The message text to be posted.  Must be less than or equal to\n    140 characters.\n  in_reply_to_status_id:\n    The ID of an existing status that the status to be posted is\n    in reply to.  This implicitly sets the in_reply_to_user_id\n    attribute of the resulting status to the user ID of the\n    message being replied to.  Invalid/missing status IDs will be\n    ignored. [Optional]\nReturns:\n  A twitter.Status instance representing the message posted.\n'''\n", "func_signal": "def PostUpdate(self, status, in_reply_to_status_id=None):\n", "code": "if not self._username:\n  raise TwitterError(\"The twitter.Api instance must be authenticated.\")\n\nurl = 'http://twitter.com/statuses/update.json'\n\nif len(status) > CHARACTER_LIMIT:\n  raise TwitterError(\"Text must be less than or equal to %d characters. \"\n                     \"Consider using PostUpdates.\" % CHARACTER_LIMIT)\n\ndata = {'status': status}\nif in_reply_to_status_id:\n  data['in_reply_to_status_id'] = in_reply_to_status_id\njson = self._FetchUrl(url, post_data=data)\ndata = simplejson.loads(json)\nself._CheckForTwitterError(data)\nreturn Status.NewFromJsonDict(data)", "path": "twitter.py", "repo_name": "yasulab/SimpleTwitterBot", "stars": 34, "license": "None", "language": "python", "size": 436}
{"docstring": "'''Create a new instance based on a JSON dict.\n\nArgs:\n  data: A JSON dict, as converted from the JSON in the twitter API\nReturns:\n  A twitter.DirectMessage instance\n'''\n", "func_signal": "def NewFromJsonDict(data):\n", "code": "return DirectMessage(created_at=data.get('created_at', None),\n                     recipient_id=data.get('recipient_id', None),\n                     sender_id=data.get('sender_id', None),\n                     text=data.get('text', None),\n                     sender_screen_name=data.get('sender_screen_name', None),\n                     id=data.get('id', None),\n                     recipient_screen_name=data.get('recipient_screen_name', None))", "path": "twitter.py", "repo_name": "yasulab/SimpleTwitterBot", "stars": 34, "license": "None", "language": "python", "size": 436}
{"docstring": "'''Get a human redable string representing the posting time\n\nReturns:\n  A human readable string representing the posting time\n'''\n", "func_signal": "def GetRelativeCreatedAt(self):\n", "code": "fudge = 1.25\ndelta  = long(self.now) - long(self.created_at_in_seconds)\n\nif delta < (1 * fudge):\n  return 'about a second ago'\nelif delta < (60 * (1/fudge)):\n  return 'about %d seconds ago' % (delta)\nelif delta < (60 * fudge):\n  return 'about a minute ago'\nelif delta < (60 * 60 * (1/fudge)):\n  return 'about %d minutes ago' % (delta / 60)\nelif delta < (60 * 60 * fudge):\n  return 'about an hour ago'\nelif delta < (60 * 60 * 24 * (1/fudge)):\n  return 'about %d hours ago' % (delta / (60 * 60))\nelif delta < (60 * 60 * 24 * fudge):\n  return 'about a day ago'\nelse:\n  return 'about %d days ago' % (delta / (60 * 60 * 24))", "path": "twitter.py", "repo_name": "yasulab/SimpleTwitterBot", "stars": 34, "license": "None", "language": "python", "size": 436}
{"docstring": "# Note that this function is called from _speedups\n", "func_signal": "def errmsg(msg, doc, pos, end=None):\n", "code": "lineno, colno = linecol(doc, pos)\nif end is None:\n    return '%s: line %d column %d (char %d)' % (msg, lineno, colno, pos)\nendlineno, endcolno = linecol(doc, end)\nreturn '%s: line %d column %d - line %d column %d (char %d - %d)' % (\n    msg, lineno, colno, endlineno, endcolno, pos, end)", "path": "python-twitter-0.6\\simplejson\\decoder.py", "repo_name": "yasulab/SimpleTwitterBot", "stars": 34, "license": "None", "language": "python", "size": 436}
{"docstring": "'''Destroys the status specified by the required ID parameter.\n\nThe twitter.Api instance must be authenticated and thee\nauthenticating user must be the author of the specified status.\n\nArgs:\n  id: The numerical ID of the status you're trying to destroy.\n\nReturns:\n  A twitter.Status instance representing the destroyed status message\n'''\n", "func_signal": "def DestroyStatus(self, id):\n", "code": "try:\n  if id:\n    long(id)\nexcept:\n  raise TwitterError(\"id must be an integer\")\nurl = 'http://twitter.com/statuses/destroy/%s.json' % id\njson = self._FetchUrl(url, post_data={})\ndata = simplejson.loads(json)\nself._CheckForTwitterError(data)\nreturn Status.NewFromJsonDict(data)", "path": "twitter.py", "repo_name": "yasulab/SimpleTwitterBot", "stars": 34, "license": "None", "language": "python", "size": 436}
{"docstring": "'''Returns a list of the direct messages sent to the authenticating user.\n\nThe twitter.Api instance must be authenticated.\n\nArgs:\n  since:\n    Narrows the returned results to just those statuses created\n    after the specified HTTP-formatted date. [optional]\n  since_id:\n    Returns only public statuses with an ID greater than (that is,\n    more recent than) the specified ID. [Optional]\n\nReturns:\n  A sequence of twitter.DirectMessage instances\n'''\n", "func_signal": "def GetDirectMessages(self, since=None, since_id=None, page=None):\n", "code": "url = 'http://twitter.com/direct_messages.json'\nif not self._username:\n  raise TwitterError(\"The twitter.Api instance must be authenticated.\")\nparameters = {}\nif since:\n  parameters['since'] = since\nif since_id:\n  parameters['since_id'] = since_id\nif page:\n  parameters['page'] = page \njson = self._FetchUrl(url, parameters=parameters)\ndata = simplejson.loads(json)\nself._CheckForTwitterError(data)\nreturn [DirectMessage.NewFromJsonDict(x) for x in data]", "path": "twitter.py", "repo_name": "yasulab/SimpleTwitterBot", "stars": 34, "license": "None", "language": "python", "size": 436}
{"docstring": "# simulate Charlottetown's somewhat complicated cases:\n        # \n        # For all categories except Parks, send email to  not_parks1@city.com  and  not_parks2@city.com\n        # with a Cc: to not_parks_cc@city.com  and  the ward councillor.\n        #\n        # For the category of Parks, send email to:  parks@city.com\n        # with a Cc: to  parks_cc@city.com  and  the ward councillor   \n        #\n", "func_signal": "def test(self):\n", "code": "        parks_category = ReportCategory.objects.get(name_en='Lights Malfunctioning in Park')\n        not_parks_category = ReportCategory.objects.get(name_en='Damaged Curb')\n        parks_category_class = ReportCategoryClass.objects.get(name_en='Parks')\n        \n        # always CC the councillor\n        EmailRule( is_cc=True, rule=EmailRule.TO_COUNCILLOR, city = self.test_city ).save()\n        \n        # parks rules\n        EmailRule(rule=EmailRule.MATCHING_CATEGORY_CLASS, city =self.test_city, category_class = parks_category_class, email='parks@city.com' ).save()\n        EmailRule(rule=EmailRule.MATCHING_CATEGORY_CLASS, city =self.test_city, category_class = parks_category_class, email='parks_cc@city.com', is_cc=True ).save()\n# not parks rules\n        EmailRule( rule=EmailRule.NOT_MATCHING_CATEGORY_CLASS, city = self.test_city, category_class = parks_category_class, email='not_parks1@city.com' ).save()\n        EmailRule( rule=EmailRule.NOT_MATCHING_CATEGORY_CLASS, city = self.test_city, category_class = parks_category_class, email='not_parks2@city.com' ).save()\n        EmailRule( rule=EmailRule.NOT_MATCHING_CATEGORY_CLASS, city = self.test_city, category_class = parks_category_class, email='not_parks_cc@city.com', is_cc=True ).save()\n        \n        parks_report = Report(ward=self.test_ward,category = parks_category )\n        self.failUnlessEqual( self.test_ward.get_emails(parks_report), ([ u'parks@city.com' ],[u\"councillor_email@testward1.com\", u'parks_cc@city.com'] ))\n        \n        not_parks_report = Report(ward=self.test_ward,category = not_parks_category )\n        self.failUnlessEqual( self.test_ward.get_emails(not_parks_report), ([ u'not_parks1@city.com',u'not_parks2@city.com' ], [u\"councillor_email@testward1.com\",u'not_parks_cc@city.com'] ))", "path": "mainapp\\tests\\emailrules.py", "repo_name": "visiblegovernment/django-fixmystreet", "stars": 54, "license": "None", "language": "python", "size": 348830}
{"docstring": "\"\"\"\nTell whoever cares that there's been an update to this report.\n -  If it's the first update, tell city officials\n -  Anything after that, tell subscribers\n\"\"\"\n", "func_signal": "def notify(self):\n", "code": "if self.first_update:\n    self.notify_on_new()\nelse:\n    self.notify_on_update()", "path": "mainapp\\models.py", "repo_name": "visiblegovernment/django-fixmystreet", "stars": 54, "license": "None", "language": "python", "size": 348830}
{"docstring": "# these are from the fixtures file.\n", "func_signal": "def setUp(self):\n", "code": "self.test_categoryclass = ReportCategoryClass.objects.get(name_en='Parks')\nself.test_category = ReportCategory.objects.get(name_en='Broken or Damaged Equipment/Play Structures')\nself.not_test_category = ReportCategory.objects.get(name_en='Damaged Curb')\n\nself.test_city = City.objects.get(name='TestCityWithoutEmail')\nself.test_ward = Ward.objects.get(name = 'WardInCityWithNo311Email')\nself.test_report = Report(ward=self.test_ward,category=self.test_category)", "path": "mainapp\\tests\\emailrules.py", "repo_name": "visiblegovernment/django-fixmystreet", "stars": 54, "license": "None", "language": "python", "size": 348830}
{"docstring": "# check that default values are already filled in.\n", "func_signal": "def test_subscribe_form(self):\n", "code": "c = Client()\nr = c.login(username='user2',password='user2')\nr = c.get( '/reports/4/subscribers' )\nself.assertEquals( r.status_code, 200 )\nself.assertContains(r,\"user2@test.com\")\nc2 = Client()\nr = c2.get( '/reports/4/subscribers' )\nself.assertEquals( r.status_code, 200 )\nself.assertNotContains(r,\"user2@test.com\")", "path": "mainapp\\tests\\account.py", "repo_name": "visiblegovernment/django-fixmystreet", "stars": 54, "license": "None", "language": "python", "size": 348830}
{"docstring": "'''\nReplaces the responder in Collection.__init__ with responders, which\nmaybe a list of responders or None. In the case of None, default\nresponders are allocated to the colelction.\n\nSee Collection.__init__ for more details\n'''\n", "func_signal": "def __init__(self, queryset, responders=None, **kwargs):\n", "code": "if responders is None:\n    responders = {\n        'json'  : JSONResponder(),\n        'xml'   :XMLResponder(),\n    }\nself.responders = {}\nfor k, r in responders.items():\n    Collection.__init__(self, queryset, r, **kwargs)\n    self.responders[k] = self.responder", "path": "mainapp\\views\\mobile\\mobile.py", "repo_name": "visiblegovernment/django-fixmystreet", "stars": 54, "license": "None", "language": "python", "size": 348830}
{"docstring": "\"\"\" \n    user 2 has:\n    -- created report 3\n    -- updated report 1\n    -- not subscribed to any report\n\"\"\"\n \n", "func_signal": "def test_user2(self):\n", "code": "c = Client()\nr = c.login(username='user2',password='user2')\nself.assertEqual(r,True)\nr = c.get('/accounts/home/',follow=True)\nself.assertEqual(r.status_code,200)\n\nself.assertEqual(len(r.context['reports']),2)\n\n# check report 3\nself.assertEqual(r.context['reports'][0].id,3)\nself.assertEqual(r.context['reports'][0].is_reporter,True)\nself.assertEqual(r.context['reports'][0].is_updater,False)\n\n# check report 1\nself.assertEqual(r.context['reports'][1].id,1)\nself.assertEqual(r.context['reports'][1].is_reporter,False)\nself.assertEqual(r.context['reports'][1].is_updater,True)", "path": "mainapp\\tests\\account.py", "repo_name": "visiblegovernment/django-fixmystreet", "stars": 54, "license": "None", "language": "python", "size": 348830}
{"docstring": "# starting conditions        \n", "func_signal": "def test_socialuth_registration_w_noemail(self):\n", "code": "self.assertEquals(User.objects.filter(first_name=FNAME).count(),0)\n\nc = Client()\nresponse = self._do_social_auth(c,SOCIAL_COMPLETE_URL_NO_EMAIL)\n\n# calling the same URL twice doesn't make two sets.\nself._do_social_auth(c, SOCIAL_COMPLETE_URL_NO_EMAIL)\n\n# complete the registration.\nself._register(c)\nself._activate()\nself._login(c)", "path": "mainapp\\tests\\account.py", "repo_name": "visiblegovernment/django-fixmystreet", "stars": 54, "license": "None", "language": "python", "size": 348830}
{"docstring": "\"\"\" \n    user 1 has:\n    -- created reports 1 and 2\n    -- updated reports 2 and 3\n    -- subscribed to report 4\n\"\"\"\n \n", "func_signal": "def test_user1(self):\n", "code": "c = Client()\nr = c.login(username='user1',password='user1')\nself.assertEqual(r,True)\nr = c.get('/accounts/home/',follow=True)\nself.assertEqual(r.status_code,200)\nself.assertEqual(len(r.context['reports']),4)\n# check report 4\nself.assertEqual(r.context['reports'][0].id,4)\nself.assertEqual(r.context['reports'][0].is_reporter,False)\nself.assertEqual(r.context['reports'][0].is_updater,False)\n# check report 3\nself.assertEqual(r.context['reports'][1].id,3)\nself.assertEqual(r.context['reports'][1].is_reporter,False)\nself.assertEqual(r.context['reports'][1].is_updater,True)\n\n# check report 2\nself.assertEqual(r.context['reports'][2].id,2)\nself.assertEqual(r.context['reports'][2].is_reporter,True)\nself.assertEqual(r.context['reports'][2].is_updater,True)\n# check report 1\nself.assertEqual(r.context['reports'][3].id,1)\nself.assertEqual(r.context['reports'][3].is_reporter,True)\nself.assertEqual(r.context['reports'][3].is_updater,False)", "path": "mainapp\\tests\\account.py", "repo_name": "visiblegovernment/django-fixmystreet", "stars": 54, "license": "None", "language": "python", "size": 348830}
{"docstring": "# check that default values are already filled in.\n", "func_signal": "def test_report_form(self):\n", "code": "c = Client()\nr = c.login(username='user1',password='user1')\nself.assertEquals(r, True)\nurl = '/reports/?lat=%s;lon=%s' % (CREATE_PARAMS['lat'],CREATE_PARAMS['lon'] )\nr = c.get( url )\nself.assertEquals( r.status_code, 200 )\nself.assertContains(r,\"Clark Kent\")\nself.assertContains(r,\"user1@test.com\")\nself.assertContains(r,\"555-111-1111\")\n# check that default values are not filled in\n# for a second, anonymous user (problem in the field)\nc2 = Client()\nr = c2.get( url )\nself.assertEquals( r.status_code, 200 )\nself.assertNotContains(r,\"Clark Kent\")\nself.assertNotContains(r,\"user1@test.com\")\nself.assertNotContains(r,\"555-111-1111\")", "path": "mainapp\\tests\\account.py", "repo_name": "visiblegovernment/django-fixmystreet", "stars": 54, "license": "None", "language": "python", "size": 348830}
{"docstring": "#top problems\n", "func_signal": "def home( request, city, error_msg, disambiguate ):\n", "code": "top_problems = Report.objects.filter(ward__city=city,is_fixed=False).annotate(subscriber_count=Count('reportsubscriber' ) ).filter(subscriber_count__gte=1).order_by('-subscriber_count')[:10]\nreports_with_photos = Report.objects.filter(is_confirmed=True, ward__city=city).exclude(photo='').order_by(\"-created_at\")[:3]\nrecent_reports = Report.objects.filter(is_confirmed=True, ward__city=city).order_by(\"-created_at\")[:5]\n    \nreturn render_to_response(\"cities/home.html\",\n            {\"report_counts\": City.objects.filter(id=city.id).annotate(ReportTotalCounters('ward__report','10 years'))[0],\n             \"cities\": City.objects.all(),\n             'city':city,\n             'top_problems': top_problems,\n             \"reports_with_photos\": reports_with_photos,\n             \"recent_reports\": recent_reports , \n             'error_msg': error_msg,\n             'disambiguate':disambiguate },\n            context_instance=RequestContext(request))", "path": "mainapp\\views\\cities.py", "repo_name": "visiblegovernment/django-fixmystreet", "stars": 54, "license": "None", "language": "python", "size": 348830}
{"docstring": "# send to the city immediately.           \n", "func_signal": "def notify_on_new(self):\n", "code": "subject = render_to_string(\"emails/send_report_to_city/subject.txt\", {'update': self })\nmessage = render_to_string(\"emails/send_report_to_city/message.txt\", { 'update': self, 'SITE_URL':settings.SITE_URL })\n\nto_email_addrs,cc_email_addrs = self.report.ward.get_emails(self.report)\nemail_msg = CCEmailMessage(subject,message,settings.EMAIL_FROM_USER, \n                to_email_addrs, cc_email_addrs,headers = {'Reply-To': self.email })\nif self.report.photo:\n    email_msg.attach_file( self.report.photo.file.name )\n\nemail_msg.send()\n\n# update report to show time sent to city.\nself.report.sent_at=dt.now()\nemail_addr_str = \"\"\nfor email in to_email_addrs:\n    if email_addr_str != \"\":\n        email_addr_str += \", \"\n    email_addr_str += email\n    \nself.report.email_sent_to = email_addr_str\nself.report.save()", "path": "mainapp\\models.py", "repo_name": "visiblegovernment/django-fixmystreet", "stars": 54, "license": "None", "language": "python", "size": 348830}
{"docstring": "# is this report too young to be counted in this time span?\n", "func_signal": "def add_report(self,report):\n", "code": "        if self.get_report_age(report).days < self.day:\n            return\n\n        # how long was this report open for?\n        open_time = self.get_open_time(report).days\n\n        if self.status == CountReportsWithStatusOnDay.OPEN:\n            # was this report open on the given day?\n            if open_time >= self.day:\n                self.count +=1\n        else:\n            # looking to count fixed reports only\n            if report.is_fixed:\n                # was this report fixed by the given day?\n                if open_time < self.day:\n                    self.count +=1", "path": "mainapp\\management\\commands\\stats.py", "repo_name": "visiblegovernment/django-fixmystreet", "stars": 54, "license": "None", "language": "python", "size": 348830}
{"docstring": "# In case of \"/rss/beats/0613/foo/bar/baz/\", or other such clutter,\n# check that bits has only one member.\n", "func_signal": "def get_object(self, bits):\n", "code": "if len(bits) != 1:\n    raise ObjectDoesNotExist        \nreturn Report.objects.get(id=bits[0])", "path": "mainapp\\feeds.py", "repo_name": "visiblegovernment/django-fixmystreet", "stars": 54, "license": "None", "language": "python", "size": 348830}
{"docstring": "\"\"\"Returns user, might be logged in\"\"\"\n", "func_signal": "def auth_complete(self, *args, **kwargs):\n", "code": "response = { 'email': self.data.get('email',''),\n             'username': self.data.get('username',''),\n             'first_name': self.data.get('first_name',''),\n             'last_name': self.data.get('last_name','')\n            }\n\nif self.data.get('uid',None):\n    response['id'] = self.data.get('uid')\n    \nkwargs.update({'response': response, DummyBackend.name: True})\nreturn authenticate(*args, **kwargs)", "path": "mainapp\\tests\\testsocial_auth\\dummy_socialauth.py", "repo_name": "visiblegovernment/django-fixmystreet", "stars": 54, "license": "None", "language": "python", "size": 348830}
{"docstring": "\"\"\"\n    Run through our regular report/submit/confirm/update-is-fixed/confirm\n    lifecycle to make sure there's no issues, and the right\n    emails are being sent.\n\"\"\"\n", "func_signal": "def test_create_basecase(self):\n", "code": "response = self.c.post('/reports/', LOCAL_PARAMS, follow=True )\nself.assertEquals( response.status_code, 200 )\nself.assertEquals(response.template[0].name, 'reports/show.html')\nself.assertEqual(Report.objects.filter(title=LOCAL_PARAMS['title']).count(), 1 )\n\n# a confirmation email should be sent to the user\nself.assertEquals(len(mail.outbox), 1)\nself.assertEquals(mail.outbox[0].to, [u'testuser@hotmail.com'])\n\n#test confirmation link\nconfirm_url = self._get_confirm_url(mail.outbox[0])\nresponse = self.c.get(confirm_url, follow=True)\nself.assertEquals( response.status_code, 200 )\n\n#now there should be two emails in our outbox\nself.assertEquals(len(mail.outbox), 2)\nself.assertEquals(mail.outbox[1].to, [u'example_city_email@yahoo.ca'])\n\n#now submit a 'fixed' update.\nreport = Report.objects.get(title=LOCAL_PARAMS['title'])\nself.assertEquals( ReportUpdate.objects.filter(report=report).count(),1)\nupdate_url = report.get_absolute_url() + \"/updates/\"\nresponse = self.c.post(update_url,UPDATE_PARAMS, follow=True)\nself.assertEquals( response.status_code, 200 )\nself.assertEquals( ReportUpdate.objects.filter(report=report).count(),2)\n  \n# we should have sent another confirmation link\nself.assertEquals(len(mail.outbox), 3)\nself.assertEquals(mail.outbox[2].to, [u'testuser@hotmail.com'])\n\n#confirm the update\nconfirm_url = self._get_confirm_url(mail.outbox[2])\nresponse = self.c.get(confirm_url, follow=True)\nself.assertEquals( response.status_code, 200 )\n\n#I guess we send an email to the user to let them know they've confirmed.\n#seems redundant.\n\nself.assertEquals(len(mail.outbox), 4)\nself.assertEquals(mail.outbox[3].to, [u'testuser@hotmail.com'])", "path": "mainapp\\tests\\mobile.py", "repo_name": "visiblegovernment/django-fixmystreet", "stars": 54, "license": "None", "language": "python", "size": 348830}
{"docstring": "''' As above, but user has email field set --\n    should show up user model, and registraton form.\n'''\n# starting conditions\n", "func_signal": "def test_socialuth_registration_w_email(self):\n", "code": "self.assertEquals(User.objects.filter(email=EMAIL).count(),0)\n\nc = Client()\nresponse = self._do_social_auth(c, SOCIAL_COMPLETE_URL_W_EMAIL)\n\n# check that our user model has the email.\nself.assertEquals(User.objects.filter(email=EMAIL,first_name=FNAME,last_name=LNAME,is_active=False).count(),1)\n\n# check that email is in the form\nself.assertContains( response, EMAIL )\n\n# check that calling the same URL twice doesn't make \n# two profiles.        \n\nself._do_social_auth(c, SOCIAL_COMPLETE_URL_W_EMAIL)\n\n# complete registration and get going.\n\nself._register(c)\nself._activate()\nself._login(c)", "path": "mainapp\\tests\\account.py", "repo_name": "visiblegovernment/django-fixmystreet", "stars": 54, "license": "None", "language": "python", "size": 348830}
{"docstring": "# check that default values are already filled in.\n", "func_signal": "def test_update_form(self):\n", "code": "c = Client()\nr = c.login(username='user1',password='user1')\nurl = '/reports/4' \nr = c.get( url )\nself.assertEquals( r.status_code, 200 )\nself.assertContains(r,\"Clark Kent\")\nself.assertContains(r,\"user1@test.com\")\nself.assertContains(r,\"555-111-1111\")\n\n# check that default values are NOT already filled in.\n# for a second client (problem in the field)\nc2 = Client()\nr = c2.get( url )\nself.assertEquals( r.status_code, 200 )\nself.assertNotContains(r,\"Clark Kent\")\nself.assertNotContains(r,\"user1@test.com\")\nself.assertNotContains(r,\"555-111-1111\")", "path": "mainapp\\tests\\account.py", "repo_name": "visiblegovernment/django-fixmystreet", "stars": 54, "license": "None", "language": "python", "size": 348830}
{"docstring": "# expect format <city>_<province-abbrev>.fixmystreet.ca\n", "func_signal": "def _parse_jurisdiction(self,jurisdiction):\n", "code": "match = re.match(r\"(\\w+)_(\\w+)\\.fixmystreet\\.ca\",jurisdiction)\nif not match:\n    return None\ncity = get_object_or_404(City,name__iexact=match.group(1),province__abbrev__iexact=match.group(2))\nreturn city", "path": "mainapp\\views\\mobile\\open311v2.py", "repo_name": "visiblegovernment/django-fixmystreet", "stars": 54, "license": "None", "language": "python", "size": 348830}
{"docstring": "# does this update require confirmation?\n", "func_signal": "def save(self):\n", "code": "if not self.is_confirmed:\n    self.get_confirmation()\n    super(ReportUpdate,self).save()\nelse:\n    # update parent report\n    if not self.created_at: \n        self.created_at = datetime.datetime.now()\n    if self.is_fixed:\n        self.report.is_fixed = True\n        self.report.fixed_at = self.created_at   \n    # we track a last updated time in the report to make statistics \n    # (such as on the front page) easier.  \n    if not self.first_update:\n        self.report.updated_at = self.created_at\n    else:\n        self.report.updated_at = self.report.created_at\n        self.report.is_confirmed = True\n    super(ReportUpdate,self).save()\n    self.report.save()", "path": "mainapp\\models.py", "repo_name": "visiblegovernment/django-fixmystreet", "stars": 54, "license": "None", "language": "python", "size": 348830}
{"docstring": "\"\"\" a commonly used computation \"\"\"\n", "func_signal": "def get_fix_time(self,report):\n", "code": "if (not report.is_fixed) or (report.fixed_at == None):\n    raise Exception(\"report is not fixed\")\nreturn( report.fixed_at - report.created_at )", "path": "mainapp\\management\\commands\\stats.py", "repo_name": "visiblegovernment/django-fixmystreet", "stars": 54, "license": "None", "language": "python", "size": 348830}
{"docstring": "\"\"\"Method called by ``nonblocking`` to run via LSF.\"\"\"\n", "func_signal": "def _lsf(self, ex, *args, **kwargs):\n", "code": "if not(isinstance(ex,Execution)):\n    raise ValueError(\"First argument to a program must be an Execution.\")\n\nif kwargs.has_key('stdout'):\n    stdout = kwargs['stdout']\n    kwargs.pop('stdout')\n    load_stdout = False\nelse:\n    stdout = unique_filename_in(ex.working_directory)\n    load_stdout = True\n\nif kwargs.has_key('stderr'):\n    stderr = kwargs['stderr']\n    kwargs.pop('stderr')\n    load_stderr = False\nelse:\n    stderr = unique_filename_in(ex.working_directory)\n    load_stderr = True\n\nd = self.gen_args(*args, **kwargs)\n\n# Jacques Rougemont figured out the following syntax that works in\n# both bash and tcsh.\nremote_cmd = \" \".join(d[\"arguments\"])\nremote_cmd += \" > \"+stdout\nremote_cmd = \" ( \"+remote_cmd+\" ) >& \"+stderr\ncmds = [\"bsub\",\"-cwd\",ex.remote_working_directory,\"-o\",\"/dev/null\",\n        \"-e\",\"/dev/null\",\"-K\",\"-r\",remote_cmd]\nclass Future(object):\n    def __init__(self):\n        self.program_output = None\n        self.return_value = None\n    def wait(self):\n        v.wait()\n        ex.report(self.program_output)\n        return self.return_value\nf = Future()\nv = threading.Event()\ndef g():\n    try:\n        nullout = open(os.path.devnull, 'w')\n        sp = subprocess.Popen(cmds, bufsize=-1, stdout=nullout, \n                              stderr=nullout)\n        return_code = sp.wait()\n        while not(os.path.exists(os.path.join(ex.working_directory,\n                                              stdout))):\n            pass # We need to wait until the files actually show up\n        if load_stdout:\n            with open(os.path.join(ex.working_directory,stdout), 'r') as fo:\n                stdout_value = fo.readlines()\n        else:\n            stdout_value = None\n\n        while not(os.path.exists(os.path.join(ex.working_directory,stderr))):\n            pass # We need to wait until the files actually show up\n        if load_stderr:\n            with open(os.path.join(ex.working_directory,stderr), 'r') as fe:\n                stderr_value = fe.readlines()\n        else:\n            stderr_value = None\n\n        f.program_output = ProgramOutput(return_code, sp.pid,\n                                         cmds, stdout_value, stderr_value)\n        if return_code == 0:\n            z = d[\"return_value\"]\n            if callable(z):\n                f.return_value = z(f.program_output)\n            else:\n                f.return_value = z\n        v.set()\n    except:\n        f.return_value = None\n        v.set()\n        raise\na = threading.Thread(target=g)\na.start()\nreturn(f)", "path": "bein\\__init__.py", "repo_name": "madhadron/bein", "stars": 39, "license": "gpl-3.0", "language": "python", "size": 2003}
{"docstring": "\"\"\"Make the string *alias* an alias for *fileid* in the repository.\n\nAn alias can be used in place of an integer file ID in \nall methods that take a file ID.\n\"\"\"\n", "func_signal": "def add_alias(self, fileid, alias):\n", "code": "self.db.execute(\"\"\"insert into file_alias(alias,file) values (?,?)\"\"\",\n                (alias, self.resolve_alias(fileid)))\nself.db.commit()", "path": "bein\\__init__.py", "repo_name": "madhadron/bein", "stars": 39, "license": "gpl-3.0", "language": "python", "size": 2003}
{"docstring": "\"\"\"Remove the file association from *file_or_alias* to *associated_to*.\n\nBoth fields can be either an integer or an alias string.\n\"\"\"\n", "func_signal": "def delete_file_association(self, file_or_alias, associated_to):\n", "code": "src = self.resolve_alias(file_or_alias)\ndst = self.resolve_alias(associated_to)\nself.db.execute(\"\"\"delete from file_association where fileid=? and associated_to=?\"\"\", (src,dst))\nself.db.commit()", "path": "bein\\__init__.py", "repo_name": "madhadron/bein", "stars": 39, "license": "gpl-3.0", "language": "python", "size": 2003}
{"docstring": "\"\"\"Deprecated.  Use nonblocking(via=\"lsf\") instead.\"\"\"\n", "func_signal": "def lsf(self, ex, *args, **kwargs):\n", "code": "raise DeprecationWarning(\"Use nonblocking(via='lsf') instead.\")\nreturn self._lsf(ex, *args, **kwargs)", "path": "bein\\__init__.py", "repo_name": "madhadron/bein", "stars": 39, "license": "gpl-3.0", "language": "python", "size": 2003}
{"docstring": "# Make the filename in the repository match this association\n", "func_signal": "def _associate_file(self, thisid, targetid, template):\n", "code": "raw_name = self.db.execute(\"\"\"select repository_name from file where id=?\"\"\", \n                           (targetid,)).fetchone()[0]\nnew_target_name = template % raw_name\nself._rename_in_repository(thisid, new_target_name)\nself.associate_file(thisid, targetid, template)", "path": "bein\\__init__.py", "repo_name": "madhadron/bein", "stars": 39, "license": "gpl-3.0", "language": "python", "size": 2003}
{"docstring": "\"\"\"Add a file association from *file_or_alias* to *associate_to*.\n\nWhen the file *associate_to* is used in an execution, \n*file_or_alias* is also used, and named according to *template*. \n*template* should be a string containing ``%s``, which will be\nreplaced with the name *associate_to* is copied to.  So if\n*associate_to* is copied to *X* in the working directory, and\nthe template is ``\"%s.idx\"``, then `file_or_alias` is copied \nto *X*``.idx``.\n\"\"\"\n", "func_signal": "def associate_file(self, file_or_alias, associate_to, template):\n", "code": "src = self.resolve_alias(file_or_alias)\ndst = self.resolve_alias(associate_to)\nif template.find(\"%s\") == -1:\n    raise ValueError(\"Template of a file association must contain exactly one %s.\")\nelse:\n    self.db.execute(\"\"\"insert into file_association(fileid,associated_to,template) values (?,?,?)\"\"\", (src, dst, template))\n    self.db.commit()", "path": "bein\\__init__.py", "repo_name": "madhadron/bein", "stars": 39, "license": "gpl-3.0", "language": "python", "size": 2003}
{"docstring": "\"\"\"Write an execution to the MiniLIMS.\n\nThe overall Execution object is recorded in the execution\ntable.  Each program in it is entered as a row in the program\ntable, including their stdout, stderr, etc.  Each argument to\nthe program gets a row in the argument table.  All files which\nwere used in the execution from the MiniLIMS repository are\nadded to the execution_use table.  Any files which were added\nto the repository are copied to the repository and entered in\nthe file table.\n\"\"\"\n", "func_signal": "def write(self, ex, description = \"\", exception_string=None):\n", "code": "self.db.execute(\"\"\"insert into execution\n                   (started_at, finished_at, working_directory,\n                    description, exception) \n                   values (?,?,?,?,?)\"\"\",\n                (ex.started_at, ex.finished_at, ex.working_directory, \n                 description, exception_string))\nexid = self.db.execute(\"select last_insert_rowid()\").fetchone()[0]\n\n# Write all the programs\nfor i,p in enumerate(ex.programs):\n    if p.stdout == None:\n        stdout_value = \"\"\n    else:\n        stdout_value = \"\".join(p.stdout)\n    if p.stderr == None:\n        stderr_value = \"\"\n    else:\n        stderr_value = \"\".join(p.stderr)\n\n    self.db.execute(\"\"\"insert into program(pos,execution,pid,\n                                           return_code,stdout,stderr)\n                       values (?,?,?,?,?,?)\"\"\",\n                    (i, exid, p.pid, p.return_code,\n                     stdout_value, stderr_value))\n    for j,a in enumerate(p.arguments):\n        self.db.execute(\"\"\"insert into argument(pos,program,execution,\n                           argument) values (?,?,?,?)\"\"\",\n                        (j,i,exid,a))\n\n# Write the files\n\n# This section is rather complicated due to the necessity of\n# handling hierarchies of associations correctly.  The\n# algorithm is roughly as follows:\n\n# remaining = files which have yet to be inserted\n# removed = those files already processed\n# while True:\n#     these = all files to be processed this round,\n#             defined as those whose associate_to_file field\n#             (field 3 of the tuple) is in removed.\n#     for file in these:\n#         insert the file\n#         add any alias\n#         associate the file, renaming it so association namings are\n#             preserved even in the repository\n\nfileids = {}\nremoved = [(None,)]\nremaining = ex.files\nwhile remaining != []:\n    these = [k for k in ex.files if k[3] in [x[0] for x in removed]]\n\n    for (filename,description,associate_to_id,associate_to_filename,\n         template,alias) in these:\n\n        fileids[filename] = self._insert_file(ex, exid, filename, description)\n\n        if alias != None:\n            self.add_alias(fileids[filename], alias)\n\n        if associate_to_id != None or associate_to_filename != None:\n            if template == None:\n                raise ValueError(\"Must provide a template for an association.\")\n            elif template == \"%s\":\n                raise ValueError(\"Template must be more than just %s\")\n            elif template.index(\"%s\") == -1:\n                raise ValueError(\"Template must contain %s\")\n            elif associate_to_id != None:\n                self._associate_file(fileids[filename], associate_to_id, template)\n            else:\n                self._associate_file(fileids[filename], fileids[associate_to_filename], template)\n\n    [remaining.remove(t) for t in these]\n    removed.extend(these)\n\n\nfor used_file in set(ex.used_files):\n    self.db.execute(\"\"\"insert into execution_use(execution,file) \n                       values (?,?)\"\"\", (exid,used_file))\nself.db.commit()\nreturn exid", "path": "bein\\__init__.py", "repo_name": "madhadron/bein", "stars": 39, "license": "gpl-3.0", "language": "python", "size": 2003}
{"docstring": "\"\"\"Find all files associated to *file_or_alias*.\n\nReturn a list of ``(fileid, template)`` of all files associated \nto *file_or_alias*.\n\"\"\"\n", "func_signal": "def associated_files_of(self, file_or_alias):\n", "code": "f = self.resolve_alias(file_or_alias)\nreturn self.db.execute(\"\"\"select fileid,template from file_association where associated_to = ?\"\"\", (f,)).fetchall()", "path": "bein\\__init__.py", "repo_name": "madhadron/bein", "stars": 39, "license": "gpl-3.0", "language": "python", "size": 2003}
{"docstring": "\"\"\"Delete a file from the MiniLIMS repository.\n\nThis function should only be called from SQLite3, not from Python.\n\"\"\"\n", "func_signal": "def _delete_repository_file(self,filename):\n", "code": "os.remove(os.path.join(self.file_path,filename))\nreturn None", "path": "bein\\__init__.py", "repo_name": "madhadron/bein", "stars": 39, "license": "gpl-3.0", "language": "python", "size": 2003}
{"docstring": "\"\"\"Copy a file src into the MiniLIMS repository.\n\nsrc can be a fairly arbitrary path, either from the CWD, or\nusing .. and other such shortcuts.  This function should only\nbe called from SQLite3, not Python.\n\"\"\"\n", "func_signal": "def _copy_file_to_repository(self,src):\n", "code": "filename = unique_filename_in(self.file_path)\nshutil.copyfile(src,os.path.abspath(os.path.join(self.file_path,filename)))\nreturn filename", "path": "bein\\__init__.py", "repo_name": "madhadron/bein", "stars": 39, "license": "gpl-3.0", "language": "python", "size": 2003}
{"docstring": "\"\"\"Resolve an alias to an integer file id.\n\nIf an integer is passed to ``resolve_alias``, it is returned as is,\nso this method can be used without worry any time any alias\nmight have to be resolved.        \n\"\"\"\n", "func_signal": "def resolve_alias(self, alias):\n", "code": "if isinstance(alias, int):\n    x = self.db.execute(\"select id from file where id=?\", (alias,)).fetchall()\n    if x != [] and x != None:\n        return alias\n    else:\n        raise ValueError(\"No such file with id %d\" % alias)\nelif isinstance(alias, str):\n    x = self.db.execute(\"select file from file_alias where alias=?\", (alias,)).fetchone()\n    if x == None:\n        raise ValueError(\"No such file alias: \" + alias)\n    else:\n        return x[0]", "path": "bein\\__init__.py", "repo_name": "madhadron/bein", "stars": 39, "license": "gpl-3.0", "language": "python", "size": 2003}
{"docstring": "\"\"\"Create an ``Execution`` connected to the given MiniLIMS object.\n\n``execution`` is a ``contextmanager``, so it can be used in a ``with``\nstatement, as in::\n\n    with execution(mylims) as ex:\n        touch('boris')\n\nIt creates a temporary directory where the execution will work,\nsets up the ``Execution`` object, then runs the body of the\n``with`` statement.  After the body finished, or if it fails and\nthrows an exception, ``execution`` writes the ``Execution`` to the\nMiniLIMS repository and deletes the temporary directory after all\nis finished.\n\nThe ``Execution`` has field ``id`` set to ``None`` during the\n``with`` block, but afterwards ``id`` is set to the execution ID\nit ran as.  For example::\n\n    with execution(mylims) as ex:\n        pass\n\n    print ex.id\n\nwill print the execution ID the ``with`` block ran as.\n\nOn some clusters, such as VITAL-IT in Lausanne, the path to the\ncurrent directory is different on worker nodes where batch jobs\nrun than on the nodes from which jobs are submitted.  For\ninstance, if you are working in /scratch/abc on your local node,\nthe worker nodes might mount the same directory as\n/nfs/boris/scratch/abc.  In this case, running programs via LSF\nwould not work correctly.\n\nIf this is the case, you can pass the equivalent directory on\nworker nodes as *remote_working_directory*.  In the example above,\nan execution may create a directory lK4321fdr21 in /scratch/abc.\nOn the worker node, it would be /nfs/boris/scratch/abc/lK4321fd21,\nso you pass /nfs/boris/scratch/abc as *remote_working_directory*.\n\"\"\"\n", "func_signal": "def execution(lims = None, description=\"\", remote_working_directory=None):\n", "code": "execution_dir = unique_filename_in(os.getcwd())\nos.mkdir(os.path.join(os.getcwd(), execution_dir))\nex = Execution(lims,os.path.join(os.getcwd(), execution_dir))\nif remote_working_directory == None:\n    ex.remote_working_directory = ex.working_directory\nelse:\n    ex.remote_working_directory = os.path.join(remote_working_directory,\n                                               execution_dir)\nos.chdir(os.path.join(os.getcwd(), execution_dir))\nexception_string = None\ntry:\n    yield ex\nexcept:\n    (exc_type, exc_value, exc_traceback) = sys.exc_info()\n    exception_string = ''.join(traceback.format_exception(exc_type, exc_value,\n                                                        exc_traceback))\n    raise\nfinally:\n    ex.finish()\n    try:\n        if lims != None:\n            ex.id = lims.write(ex, description, exception_string)\n    finally:\n        os.chdir(\"..\")\n        shutil.rmtree(ex.working_directory, ignore_errors=True)\n        cleaned_up = True\n    assert(cleaned_up)", "path": "bein\\__init__.py", "repo_name": "madhadron/bein", "stars": 39, "license": "gpl-3.0", "language": "python", "size": 2003}
{"docstring": "\"\"\"Sets up a new MiniLIMS database.\n\"\"\"\n", "func_signal": "def initialize_database(self, db):\n", "code": "self.db.execute(\"\"\"\nCREATE TABLE if not exists execution ( \n     id integer primary key, \n     started_at integer not null, \n     finished_at integer default null,\n     working_directory text not null, \n     description text not null default '',\n     exception text default null\n)\"\"\")\nself.db.execute(\"\"\"\nCREATE TABLE if not exists program (\n       pos integer,\n       execution integer references execution(id),\n       pid integer not null,\n       stdin text default null,\n       return_code integer not null,\n       stdout text default null,\n       stderr text default null,\n       primary key (pos,execution)\n)\"\"\")\nself.db.execute(\"\"\"\nCREATE TABLE if not exists argument (\n       pos integer,\n       program integer references program(pos),\n       execution integer references program(execution),\n       argument text not null,\n       primary key (pos,program,execution)\n)\"\"\")\nself.db.execute(\"\"\"\nCREATE TABLE if not exists file ( \n       id integer primary key autoincrement, \n       external_name text, \n       repository_name text,\n       created timestamp default current_timestamp, \n       description text not null default '',\n       origin text not null default 'execution', \n       origin_value integer default null\n)\"\"\")\nself.db.execute(\"\"\"\nCREATE TABLE if not exists execution_use (\n       execution integer references execution(id),\n       file integer references file(id)\n)\"\"\")\nself.db.execute(\"\"\"\nCREATE TABLE if not exists file_alias (\n       alias text primary key,\n       file integer references file(id)\n)\"\"\")\nself.db.execute(\"\"\"\nCREATE TABLE if not exists file_association (\n       id integer primary key,\n       fileid integer references file(id) not null,\n       associated_to integer references file(id) not null,\n       template text not null\n)\"\"\")\nself.db.execute(\"\"\"\nCREATE TRIGGER if not exists prevent_repository_name_change BEFORE UPDATE ON file\nFOR EACH ROW WHEN (OLD.repository_name != NEW.repository_name) BEGIN\n     SELECT RAISE(FAIL, 'Cannot change the repository name of a file.');\nEND\"\"\")\nself.db.execute(\"\"\"\nCREATE VIEW if not exists file_direct_immutability AS \nSELECT file.id as id, count(execution) > 0 as immutable \nfrom file left join execution_use \non file.id = execution_use.file\ngroup by file.id\n\"\"\")\nself.db.execute(\"\"\"\ncreate view if not exists all_associations as\nselect file.id as id, file_association.associated_to as target\nfrom file inner join file_association\non file.id = file_association.fileid\nunion all\nselect file.id as id, file.id as target\nfrom file\norder by id asc\n\"\"\")\nself.db.execute(\"\"\"\ncreate view if not exists file_immutability as\nselect aa.id as id, max(fdi.immutable) as immutable\nfrom all_associations as aa left join file_direct_immutability as fdi\non aa.target = fdi.id\ngroup by aa.id\n\"\"\")\nself.db.execute(\"\"\"\nCREATE VIEW if not exists execution_outputs AS\nselect execution.id as execution, file.id as file \nfrom execution left join file \non execution.id = file.origin_value\nand file.origin='execution'\n\"\"\")\nself.db.execute(\"\"\"\nCREATE VIEW if not exists execution_immutability AS\nSELECT eo.execution as id, ifnull(max(fi.immutable),0) as immutable from\nexecution_outputs as eo left join file_immutability as fi\non eo.file = fi.id\ngroup by id\n\"\"\")\nself.db.execute(\"\"\"\nCREATE TRIGGER if not exists prevent_file_delete BEFORE DELETE ON file \nFOR EACH ROW WHEN \n    (SELECT immutable FROM file_immutability WHERE id = OLD.id) = 1\nBEGIN\n    SELECT RAISE(FAIL, 'File is immutable; cannot delete it.'); \nEND\n\"\"\")\nself.db.execute(\"\"\"\nCREATE TRIGGER if not exists prevent_argument_delete BEFORE DELETE ON argument\nFOR EACH ROW WHEN \n    (SELECT immutable FROM execution_immutability WHERE id = OLD.execution) = 1 \nBEGIN \n    SELECT RAISE(FAIL, 'Execution is immutable; cannot delete argument.'); \nEND\t   \n\"\"\")\nself.db.execute(\"\"\"\nCREATE TRIGGER if not exists prevent_argument_update BEFORE UPDATE ON argument\nFOR EACH ROW WHEN\n    (SELECT immutable FROM execution_immutability WHERE id = OLD.execution) = 1 \nBEGIN\n    SELECT RAISE(FAIL, 'Execution is immutable; cannot update command arguments.'); \nEND\n\"\"\")\nself.db.execute(\"\"\"\nCREATE TRIGGER if not exists prevent_command_delete BEFORE DELETE ON program\nFOR EACH ROW WHEN \n    (SELECT immutable FROM execution_immutability WHERE id = OLD.execution) = 1 \nBEGIN\n    SELECT RAISE(FAIL, 'Execution is immutable; cannot delete command.'); \nEND\n\"\"\")\nself.db.execute(\"\"\"\nCREATE TRIGGER if not exists prevent_command_update BEFORE UPDATE ON program\nFOR EACH ROW WHEN \n    (SELECT immutable FROM execution_immutability WHERE id = OLD.execution) = 1\nBEGIN\n    SELECT RAISE(FAIL, 'Execution is immutable; cannot update commands.'); \nEND\n\"\"\")\nself.db.execute(\"\"\"\nCREATE TRIGGER if not exists prevent_execution_delete BEFORE DELETE ON execution \nFOR EACH ROW WHEN\n    (SELECT immutable FROM execution_immutability WHERE id = OLD.id) = 1\nBEGIN\n    SELECT RAISE(FAIL, 'Execution is immutable; cannot delete.'); \nEND\n\"\"\")\nself.db.execute(\"\"\"\nCREATE TRIGGER if not exists prevent_execution_update BEFORE UPDATE ON execution\nFOR EACH ROW WHEN\n    (SELECT immutable FROM execution_immutability WHERE id = OLD.id) = 1 AND \n    (OLD.id != NEW.id OR OLD.started_at != NEW.started_at OR OLD.finished_at != NEW.finished_at\n     OR OLD.temp_dir != NEW.temp_dir) \nBEGIN\n    SELECT RAISE(FAIL, 'Execution is immutable; cannot update anything but description.'); \nEND\n\"\"\")\nself.db.execute(\"\"\"\nCREATE TRIGGER if not exists prevent_immutable_file_update BEFORE UPDATE on file \nFOR EACH ROW WHEN \n    (SELECT immutable FROM file_immutability WHERE id = old.id) = 1 AND \n    (OLD.id != NEW.id OR OLD.external_name != NEW.external_name OR \n     OLD.repository_name != NEW.repository_name OR \n     OLD.created != NEW.created OR OLD.origin != NEW.origin OR \n     OLD.origin_value != NEW.origin_value) \nBEGIN \n    SELECT RAISE(FAIL, 'File is immutable; cannot update except description.'); \nEND\n\"\"\")\nself.db.execute(\"\"\"\nCREATE TABLE if not exists memopad (\n    call_hash integer primary key,\n    filename text unique not null,\n    call text,\n    inserted timestamp default current_timestamp\n)\"\"\")\nself.db.commit()", "path": "bein\\__init__.py", "repo_name": "madhadron/bein", "stars": 39, "license": "gpl-3.0", "language": "python", "size": 2003}
{"docstring": "\"\"\"Write *file_or_alias* from the MiniLIMS repository to *dst*.\n\n*dst* can be either a directory, in which case the file will\nhave its repository name in the new directory, or can specify\na filename, in which case the file will be copied to that\nfilename.\nAssociated files will also be copied if *with_associated=True*.\n\"\"\"\n", "func_signal": "def export_file(self, file_or_alias, dst, with_associated=False):\n", "code": "src = self.path_to_file(file_or_alias)\nshutil.copy(src, dst) \nif with_associated:\n    if os.path.isdir(dst):\n        dst = os.path.join(dst, self.fetch_file(file_or_alias)['repository_name'])\n    for association in self.fetch_file(file_or_alias)['associations']:  #association = (id,template)\n        fileid = association[0]\n        template = association[1][2:] #removes %s\n        dst = dst + template\n        shutil.copy(src,dst)", "path": "bein\\__init__.py", "repo_name": "madhadron/bein", "stars": 39, "license": "gpl-3.0", "language": "python", "size": 2003}
{"docstring": "\"\"\"Run a program, but return a Future object instead of blocking.\n\nLike __call__, nonblocking takes an Execution as an extra,\ninitial argument before the arguments to the decorated\nfunction.  However, instead of blocking, it starts the program\nin a separate thread, and returns an object which lets the\nuser choose when to wait for the program by calling its wait()\nmethod.  When wait() is called, the thread blocks, and the\nprogram is recorded in the execution and its value returned as\nif the use had called __call__ directory.  Thus,\n\nwith execution(lims) as ex:\n    f = touch(\"boris\")\n\nis exactly equivalent to\n\nwith execution(lims) as ex:\n    a = touch.nonblocking(\"boris\")\n    f = a.wait()\n\nAll the methods are named as _method, with the same arguments\nas ``nonblocking``.  That is, the ``via=\"local\"`` method is\nimplemented by ``_local``, the ``via=\"lsf\"`` method by\n``_lsf``, etc.  When writing a new method, name it in the same\nway, and add a condition to the ``if`` statement in\n``nonblocking``.\n\nIf you need to pass a keyword argument ``via`` to your\nprogram, you will need to call one of the hidden methods\n(``_local`` or ``_lsf``) directly.\n\"\"\"\n", "func_signal": "def nonblocking(self, ex, *args, **kwargs):\n", "code": "if not(isinstance(ex,Execution)):\n    raise ValueError(\"First argument to a program must be an Execution.\")\nelif ex.id != None:\n    raise SyntaxError(\"Program being called on an execution that has already terminated.\")\n\nif kwargs.has_key('via'):\n    via = kwargs['via']\n    kwargs.pop('via')\nelse:\n    via = 'local'\n\nif via == 'local':\n    return self._local(ex, *args, **kwargs)\nelif via == 'lsf':\n    return self._lsf(ex, *args, **kwargs)", "path": "bein\\__init__.py", "repo_name": "madhadron/bein", "stars": 39, "license": "gpl-3.0", "language": "python", "size": 2003}
{"docstring": "\"\"\"Returns a dictionary describing the given file.\"\"\"\n", "func_signal": "def fetch_file(self, id_or_alias):\n", "code": "fileid = self.resolve_alias(id_or_alias)\nfields = self.db.execute(\"\"\"select external_name, repository_name,\n                            created, description, origin, origin_value\n                            from file where id=?\"\"\", \n                         (fileid,)).fetchone()\nif fields == None:\n    raise ValueError(\"No such file \" + str(id_or_alias) + \" in MiniLIMS.\")\nelse:\n    [external_name, repository_name, created, description,\n     origin_type, origin_value] = fields\nif origin_type == 'copy':\n    origin = ('copy',origin_value)\nelif origin_type == 'execution':\n    origin = ('execution',origin_value)\nelif origin_type == 'import':\n    origin = 'import'\naliases = [a for (a,) in \n           self.db.execute(\"select alias from file_alias where file=?\",\n                           (fileid,))]\nassociations = self.db.execute(\"\"\"select fileid,template from file_association where\n                                  associated_to=?\"\"\", (fileid,)).fetchall()\nassociated_to = self.db.execute(\"\"\"select associated_to,template from file_association\n                                   where fileid=?\"\"\", (fileid,)).fetchall()\nimmutable = self.db.execute(\"select immutable from file_immutability where id=?\",\n                            (fileid,)).fetchone()[0]\nreturn {'external_name': external_name,\n        'repository_name': repository_name,\n        'created': created,\n        'description': description,\n        'origin': origin,\n        'aliases': aliases,\n        'associations': associations,\n        'associated_to': associated_to,\n        'immutable': immutable == 1}", "path": "bein\\__init__.py", "repo_name": "madhadron/bein", "stars": 39, "license": "gpl-3.0", "language": "python", "size": 2003}
{"docstring": "\"\"\"Delete the alias *alias* from the repository.\n\nThe file itself is untouched.  This only affects the alias.\n\"\"\"\n", "func_signal": "def delete_alias(self, alias):\n", "code": "self.db.execute(\"\"\"delete from file_alias where alias = ?\"\"\", (alias,))\nself.db.commit()", "path": "bein\\__init__.py", "repo_name": "madhadron/bein", "stars": 39, "license": "gpl-3.0", "language": "python", "size": 2003}
{"docstring": "\"\"\"Return a random filename unique in the given path.\n\nThe filename returned is twenty alphanumeric characters which are\nnot already serving as a filename in *path*.  If *path* is\nomitted, it defaults to the current working directory.\n\"\"\"\n", "func_signal": "def unique_filename_in(path=None):\n", "code": "if path == None:\n    path = os.getcwd()\ndef random_string():\n    return \"\".join([random.choice(string.letters + string.digits) \n                    for x in range(20)])\nwhile True:\n    filename = random_string()\n    files = [f for f in os.listdir(path) if f.startswith(filename)]\n    if files == []:\n        break\nreturn filename", "path": "bein\\__init__.py", "repo_name": "madhadron/bein", "stars": 39, "license": "gpl-3.0", "language": "python", "size": 2003}
{"docstring": "\"\"\"Fetch the path to *id_or_alias* in the attached LIMS.\"\"\"\n", "func_signal": "def path_to_file(self, id_or_alias):\n", "code": "if self.lims == None:\n    raise ValueError(\"Cannot use path_to_file; no attached LIMS.\")\nelse:\n    fileid = self.lims.resolve_alias(id_or_alias)\n    self.used_files.append(fileid)\n    return self.lims.path_to_file(fileid)", "path": "bein\\__init__.py", "repo_name": "madhadron/bein", "stars": 39, "license": "gpl-3.0", "language": "python", "size": 2003}
{"docstring": "\"\"\"Returns a dictionary of all the data corresponding to the given execution id.\"\"\"\n", "func_signal": "def fetch_execution(self, exid):\n", "code": "def fetch_program(exid, progid):\n    fields = self.db.execute(\"\"\"select pid,return_code,stdout,stderr\n                                from program where execution=? and pos=?\"\"\",\n                             (exid, progid)).fetchone()\n    if fields == None:\n        raise ValueError(\"No such program: execution %d, program %d\" % (exid, progid))\n    else:\n        [pid, return_code, stdout, stderr] = fields\n    arguments = [a for (a,) in self.db.execute(\"\"\"select argument from argument\n                                                  where execution=? and program=?\n                                                  order by pos asc\"\"\", (exid,progid))]\n    return {'pid': pid,\n            'return_code': return_code,\n            'stdout': stdout,\n            'stderr': stderr,\n            'arguments': arguments}\nexfields = self.db.execute(\"\"\"select started_at, finished_at, working_directory,\n                                   description, exception from execution\n                            where id=?\"\"\", (exid,)).fetchone()\nif exfields == None:\n    raise ValueError(\"No such execution with id %d\" % (exid,))\nelse:\n    (started_at,finished_at,working_directory,\n     description, exception) = exfields\nprogids = [a for (a,) in self.db.execute(\"\"\"select pos from program where execution=?\n                                          order by pos asc\"\"\", (exid,))]\nprogs = [fetch_program(exid,i) for i in progids]\nadded_files = [a for (a,) in self.db.execute(\"\"\"select id from file where\n                                                origin='execution' and origin_value=?\"\"\",\n                                             (exid,))]\nused_files = [a for (a,) in self.db.execute(\"\"\"select file from execution_use\n                                               where execution=?\"\"\", (exid,))]\nimmutability = self.db.execute(\"\"\"select immutable from execution_immutability\n    where id=?\"\"\", (exid,)).fetchone()[0]\n    \nreturn {'started_at': started_at,\n        'finished_at': finished_at,\n        'working_directory': working_directory,\n        'description': description,\n        'exception_string': exception,\n        'programs': progs,\n        'added_files': added_files,\n        'used_files': used_files,\n        'immutable': immutability == 1}", "path": "bein\\__init__.py", "repo_name": "madhadron/bein", "stars": 39, "license": "gpl-3.0", "language": "python", "size": 2003}
{"docstring": "\"\"\"Converts the diff output to include an svn-style \"Index:\" line as well\nas record the hashes of the files, so we can upload them along with our\ndiff.\"\"\"\n# Special used by git to indicate \"no such content\".\n", "func_signal": "def PostProcessDiff(self, gitdiff):\n", "code": "NULL_HASH = \"0\"*40\n\ndef IsFileNew(filename):\n  return filename in self.hashes and self.hashes[filename][0] is None\n\ndef AddSubversionPropertyChange(filename):\n  \"\"\"Add svn's property change information into the patch if given file is\n  new file.\n\n  We use Subversion's auto-props setting to retrieve its property.\n  See http://svnbook.red-bean.com/en/1.1/ch07.html#svn-ch-7-sect-1.3.2 for\n  Subversion's [auto-props] setting.\n  \"\"\"\n  if self.options.emulate_svn_auto_props and IsFileNew(filename):\n    svnprops = GetSubversionPropertyChanges(filename)\n    if svnprops:\n      svndiff.append(\"\\n\" + svnprops + \"\\n\")\n\nsvndiff = []\nfilecount = 0\nfilename = None\nfor line in gitdiff.splitlines():\n  match = re.match(r\"diff --git a/(.*) b/(.*)$\", line)\n  if match:\n    # Add auto property here for previously seen file.\n    if filename is not None:\n      AddSubversionPropertyChange(filename)\n    filecount += 1\n    # Intentionally use the \"after\" filename so we can show renames.\n    filename = match.group(2)\n    svndiff.append(\"Index: %s\\n\" % filename)\n    if match.group(1) != match.group(2):\n      self.renames[match.group(2)] = match.group(1)\n  else:\n    # The \"index\" line in a git diff looks like this (long hashes elided):\n    #   index 82c0d44..b2cee3f 100755\n    # We want to save the left hash, as that identifies the base file.\n    match = re.match(r\"index (\\w+)\\.\\.(\\w+)\", line)\n    if match:\n      before, after = (match.group(1), match.group(2))\n      if before == NULL_HASH:\n        before = None\n      if after == NULL_HASH:\n        after = None\n      self.hashes[filename] = (before, after)\n  svndiff.append(line + \"\\n\")\nif not filecount:\n  ErrorExit(\"No valid patches found in output from git diff\")\n# Add auto property for the last seen file.\nassert filename is not None\nAddSubversionPropertyChange(filename)\nreturn \"\".join(svndiff)", "path": "tools\\rietveld\\upload.py", "repo_name": "springpad/springpad", "stars": 43, "license": "None", "language": "python", "size": 246}
{"docstring": "\"\"\"Encode form fields for multipart/form-data.\n\nArgs:\n  fields: A sequence of (name, value) elements for regular form fields.\n  files: A sequence of (name, filename, value) elements for data to be\n         uploaded as files.\nReturns:\n  (content_type, body) ready for httplib.HTTP instance.\n\nSource:\n  http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/146306\n\"\"\"\n", "func_signal": "def EncodeMultipartFormData(fields, files):\n", "code": "BOUNDARY = '-M-A-G-I-C---B-O-U-N-D-A-R-Y-'\nCRLF = '\\r\\n'\nlines = []\nfor (key, value) in fields:\n  lines.append('--' + BOUNDARY)\n  lines.append('Content-Disposition: form-data; name=\"%s\"' % key)\n  lines.append('')\n  if isinstance(value, unicode):\n    value = value.encode('utf-8')\n  lines.append(value)\nfor (key, filename, value) in files:\n  lines.append('--' + BOUNDARY)\n  lines.append('Content-Disposition: form-data; name=\"%s\"; filename=\"%s\"' %\n           (key, filename))\n  lines.append('Content-Type: %s' % GetContentType(filename))\n  lines.append('')\n  if isinstance(value, unicode):\n    value = value.encode('utf-8')\n  lines.append(value)\nlines.append('--' + BOUNDARY + '--')\nlines.append('')\nbody = CRLF.join(lines)\ncontent_type = 'multipart/form-data; boundary=%s' % BOUNDARY\nreturn content_type, body", "path": "tools\\rietveld\\upload.py", "repo_name": "springpad/springpad", "stars": 43, "license": "None", "language": "python", "size": 246}
{"docstring": "\"\"\"Return a list of files unknown to the VCS.\"\"\"\n", "func_signal": "def GetUnknownFiles(self):\n", "code": "args = []\nstatus = RunShell([\"hg\", \"status\", \"--rev\", self.base_rev, \"-u\", \".\"],\n    silent_ok=True)\nunknown_files = []\nfor line in status.splitlines():\n  st, fn = line.split(\" \", 1)\n  if st == \"?\":\n    unknown_files.append(fn)\nreturn unknown_files", "path": "tools\\rietveld\\upload.py", "repo_name": "springpad/springpad", "stars": 43, "license": "None", "language": "python", "size": 246}
{"docstring": "\"\"\"Creates a new urllib request.\"\"\"\n", "func_signal": "def _CreateRequest(self, url, data=None):\n", "code": "logging.debug(\"Creating request for: '%s' with payload:\\n%s\", url, data)\nreq = urllib2.Request(url, data=data)\nif self.host_override:\n  req.add_header(\"Host\", self.host_override)\nfor key, value in self.extra_headers.iteritems():\n  req.add_header(key, value)\nreturn req", "path": "tools\\rietveld\\upload.py", "repo_name": "springpad/springpad", "stars": 43, "license": "None", "language": "python", "size": 246}
{"docstring": "\"\"\"Parse the given property value which comes from [auto-props] section and\nreturns a list whose element is a (svn_prop_key, svn_prop_value) pair.\n\nSee the following doctest for example.\n\n>>> ParseSubversionPropertyValues('svn:eol-style=LF')\n[('svn:eol-style', 'LF')]\n>>> ParseSubversionPropertyValues('svn:mime-type=image/jpeg')\n[('svn:mime-type', 'image/jpeg')]\n>>> ParseSubversionPropertyValues('svn:eol-style=LF;svn:executable')\n[('svn:eol-style', 'LF'), ('svn:executable', '*')]\n\"\"\"\n", "func_signal": "def ParseSubversionPropertyValues(props):\n", "code": "key_value_pairs = []\nfor prop in props.split(\";\"):\n  key_value = prop.split(\"=\")\n  assert len(key_value) <= 2\n  if len(key_value) == 1:\n    # If value is not given, use '*' as a Subversion's convention.\n    key_value_pairs.append((key_value[0], \"*\"))\n  else:\n    key_value_pairs.append((key_value[0], key_value[1]))\nreturn key_value_pairs", "path": "tools\\rietveld\\upload.py", "repo_name": "springpad/springpad", "stars": 43, "license": "None", "language": "python", "size": 246}
{"docstring": "\"\"\"Save the cookie jar after authentication.\"\"\"\n", "func_signal": "def _Authenticate(self):\n", "code": "super(HttpRpcServer, self)._Authenticate()\nif self.save_cookies:\n  StatusUpdate(\"Saving authentication cookies to %s\" % self.cookie_file)\n  self.cookie_jar.save()", "path": "tools\\rietveld\\upload.py", "repo_name": "springpad/springpad", "stars": 43, "license": "None", "language": "python", "size": 246}
{"docstring": "\"\"\"Validate a reviewer -- either a nickname or an email addres.\n\nArgs:\n  reviewer: A nickname or an email address.\n\nCalls ErrorExit() if it is an invalid email address.\n\"\"\"\n", "func_signal": "def CheckReviewer(reviewer):\n", "code": "if \"@\" not in reviewer:\n  return  # Assume nickname\nparts = reviewer.split(\"@\")\nif len(parts) > 2:\n  ErrorExit(\"Invalid email address: %r\" % reviewer)\nassert len(parts) == 2\nif \".\" not in parts[1]:\n  ErrorExit(\"Invalid email address: %r\" % reviewer)", "path": "tools\\rietveld\\upload.py", "repo_name": "springpad/springpad", "stars": 43, "license": "None", "language": "python", "size": 246}
{"docstring": "\"\"\"Return the current diff as a string.\n\nArgs:\n  args: Extra arguments to pass to the diff command.\n\"\"\"\n", "func_signal": "def GenerateDiff(self, args):\n", "code": "raise NotImplementedError(\n    \"abstract method -- subclass %s must override\" % self.__class__)", "path": "tools\\rietveld\\upload.py", "repo_name": "springpad/springpad", "stars": 43, "license": "None", "language": "python", "size": 246}
{"docstring": "\"\"\"Return a list of files unknown to the VCS.\"\"\"\n", "func_signal": "def GetUnknownFiles(self):\n", "code": "raise NotImplementedError(\n    \"abstract method -- subclass %s must override\" % self.__class__)", "path": "tools\\rietveld\\upload.py", "repo_name": "springpad/springpad", "stars": 43, "license": "None", "language": "python", "size": 246}
{"docstring": "\"\"\"Returns base URL for current diff.\n\nArgs:\n  required: If true, exits if the url can't be guessed, otherwise None is\n    returned.\n\"\"\"\n", "func_signal": "def _GuessBase(self, required):\n", "code": "info = RunShell([\"svn\", \"info\"])\nfor line in info.splitlines():\n  if line.startswith(\"URL: \"):\n    url = line.split()[1]\n    scheme, netloc, path, params, query, fragment = urlparse.urlparse(url)\n    username, netloc = urllib.splituser(netloc)\n    if username:\n      logging.info(\"Removed username from base URL\")\n    guess = \"\"\n    if netloc == \"svn.python.org\" and scheme == \"svn+ssh\":\n      path = \"projects\" + path\n      scheme = \"http\"\n      guess = \"Python \"\n    elif netloc.endswith(\".googlecode.com\"):\n      scheme = \"http\"\n      guess = \"Google Code \"\n    path = path + \"/\"\n    base = urlparse.urlunparse((scheme, netloc, path, params,\n                                query, fragment))\n    logging.info(\"Guessed %sbase = %s\", guess, base)\n    return base\nif required:\n  ErrorExit(\"Can't find URL in output from svn info\")\nreturn None", "path": "tools\\rietveld\\upload.py", "repo_name": "springpad/springpad", "stars": 43, "license": "None", "language": "python", "size": 246}
{"docstring": "\"\"\"Return a Subversion's 'Property changes on ...' string, which is used in\nthe patch file.\n\nArgs:\n  filename: filename whose property might be set by [auto-props] config.\n\nReturns:\n  A string like 'Property changes on |filename| ...' if given |filename|\n    matches any entries in [auto-props] section. None, otherwise.\n\"\"\"\n", "func_signal": "def GetSubversionPropertyChanges(filename):\n", "code": "global svn_auto_props_map\nif svn_auto_props_map is None:\n  svn_auto_props_map = LoadSubversionAutoProperties()\n\nall_props = []\nfor file_pattern, props in svn_auto_props_map.items():\n  if fnmatch.fnmatch(filename, file_pattern):\n    all_props.extend(props)\nif all_props:\n  return FormatSubversionPropertyChanges(filename, all_props)\nreturn None", "path": "tools\\rietveld\\upload.py", "repo_name": "springpad/springpad", "stars": 43, "license": "None", "language": "python", "size": 246}
{"docstring": "# -du generates a unified diff, which is nearly svn format\n", "func_signal": "def GenerateMergeDiff(diffData, args):\n", "code": "diffData.file_body = self.RunPerforceCommand(\n    [\"diff\", \"-du\", diffData.filename] + args)\ndiffData.base_rev = self.GetBaseRevision(diffData.filename)\ndiffData.prefix = \"\"\n\n# We have to replace p4's file status output (the lines starting\n# with +++ or ---) to match svn's diff format\nlines = diffData.file_body.splitlines()\nfirst_good_line = 0\nwhile (first_good_line < len(lines) and\n      not lines[first_good_line].startswith(\"@@\")):\n  first_good_line += 1\ndiffData.file_body = \"\\n\".join(lines[first_good_line:])\nreturn diffData", "path": "tools\\rietveld\\upload.py", "repo_name": "springpad/springpad", "stars": 43, "license": "None", "language": "python", "size": 246}
{"docstring": "\"\"\"Returns the contents of a file.\"\"\"\n", "func_signal": "def ReadFile(self, filename):\n", "code": "file = open(filename, 'rb')\nresult = \"\"\ntry:\n  result = file.read()\nfinally:\n  file.close()\nreturn result", "path": "tools\\rietveld\\upload.py", "repo_name": "springpad/springpad", "stars": 43, "license": "None", "language": "python", "size": 246}
{"docstring": "# \"hg status\" and \"hg cat\" both take a path relative to the current subdir\n# rather than to the repo root, but \"hg diff\" has given us the full path\n# to the repo root.\n", "func_signal": "def GetBaseFile(self, filename):\n", "code": "base_content = \"\"\nnew_content = None\nis_binary = False\noldrelpath = relpath = self._GetRelPath(filename)\n# \"hg status -C\" returns two lines for moved/copied files, one otherwise\nout = RunShell([\"hg\", \"status\", \"-C\", \"--rev\", self.base_rev, relpath])\nout = out.splitlines()\n# HACK: strip error message about missing file/directory if it isn't in\n# the working copy\nif out[0].startswith('%s: ' % relpath):\n  out = out[1:]\nstatus, _ = out[0].split(' ', 1)\nif len(out) > 1 and status == \"A\":\n  # Moved/copied => considered as modified, use old filename to\n  # retrieve base contents\n  oldrelpath = out[1].strip()\n  status = \"M\"\nif \":\" in self.base_rev:\n  base_rev = self.base_rev.split(\":\", 1)[0]\nelse:\n  base_rev = self.base_rev\nif status != \"A\":\n  base_content = RunShell([\"hg\", \"cat\", \"-r\", base_rev, oldrelpath],\n    silent_ok=True)\n  is_binary = \"\\0\" in base_content  # Mercurial's heuristic\nif status != \"R\":\n  new_content = open(relpath, \"rb\").read()\n  is_binary = is_binary or \"\\0\" in new_content\nif is_binary and base_content:\n  # Fetch again without converting newlines\n  base_content = RunShell([\"hg\", \"cat\", \"-r\", base_rev, oldrelpath],\n    silent_ok=True, universal_newlines=False)\nif not is_binary or not self.IsImage(relpath):\n  new_content = None\nreturn base_content, new_content, is_binary, status", "path": "tools\\rietveld\\upload.py", "repo_name": "springpad/springpad", "stars": 43, "license": "None", "language": "python", "size": 246}
{"docstring": "\"\"\"Returns Subversion's 'Property changes on ...' strings using given filename\nand properties.\n\nArgs:\n  filename: filename\n  props: A list whose element is a (svn_prop_key, svn_prop_value) pair.\n\nReturns:\n  A string which can be used in the patch file for Subversion.\n\nSee the following doctest for example.\n\n>>> print FormatSubversionPropertyChanges('foo.cc', [('svn:eol-style', 'LF')])\nProperty changes on: foo.cc\n___________________________________________________________________\nAdded: svn:eol-style\n   + LF\n<BLANKLINE>\n\"\"\"\n", "func_signal": "def FormatSubversionPropertyChanges(filename, props):\n", "code": "prop_changes_lines = [\n  \"Property changes on: %s\" % filename,\n  \"___________________________________________________________________\"]\nfor key, value in props:\n  prop_changes_lines.append(\"Added: \" + key)\n  prop_changes_lines.append(\"   + \" + value)\nreturn \"\\n\".join(prop_changes_lines) + \"\\n\"", "path": "tools\\rietveld\\upload.py", "repo_name": "springpad/springpad", "stars": 43, "license": "None", "language": "python", "size": 246}
{"docstring": "\"\"\"Get the content of the upstream version of a file.\n\nReturns:\n  A tuple (base_content, new_content, is_binary, status)\n    base_content: The contents of the base file.\n    new_content: For text files, this is empty.  For binary files, this is\n      the contents of the new file, since the diff output won't contain\n      information to reconstruct the current file.\n    is_binary: True iff the file is binary.\n    status: The status of the file.\n\"\"\"\n\n", "func_signal": "def GetBaseFile(self, filename):\n", "code": "raise NotImplementedError(\n    \"abstract method -- subclass %s must override\" % self.__class__)", "path": "tools\\rietveld\\upload.py", "repo_name": "springpad/springpad", "stars": 43, "license": "None", "language": "python", "size": 246}
{"docstring": "\"\"\"Returns true if the guessed mimetyped isnt't in text group.\"\"\"\n", "func_signal": "def IsBinary(self, filename):\n", "code": "mimetype = mimetypes.guess_type(filename)[0]\nif not mimetype:\n  return False  # e.g. README, \"real\" binaries usually have an extension\n# special case for text files which don't start with text/\nif mimetype in TEXT_MIMETYPES:\n  return False\nreturn not mimetype.startswith(\"text/\")", "path": "tools\\rietveld\\upload.py", "repo_name": "springpad/springpad", "stars": 43, "license": "None", "language": "python", "size": 246}
{"docstring": "\"\"\"Helper that calls GetBase file for each file in the patch.\n\nReturns:\n  A dictionary that maps from filename to GetBaseFile's tuple.  Filenames\n  are retrieved based on lines that start with \"Index:\" or\n  \"Property changes on:\".\n\"\"\"\n", "func_signal": "def GetBaseFiles(self, diff):\n", "code": "files = {}\nfor line in diff.splitlines(True):\n  if line.startswith('Index:') or line.startswith('Property changes on:'):\n    unused, filename = line.split(':', 1)\n    # On Windows if a file has property changes its filename uses '\\'\n    # instead of '/'.\n    filename = filename.strip().replace('\\\\', '/')\n    files[filename] = self.GetBaseFile(filename)\nreturn files", "path": "tools\\rietveld\\upload.py", "repo_name": "springpad/springpad", "stars": 43, "license": "None", "language": "python", "size": 246}
{"docstring": "\"\"\"Collapses SVN keywords.\"\"\"\n# svn cat translates keywords but svn diff doesn't. As a result of this\n# behavior patching.PatchChunks() fails with a chunk mismatch error.\n# This part was originally written by the Review Board development team\n# who had the same problem (http://reviews.review-board.org/r/276/).\n# Mapping of keywords to known aliases\n", "func_signal": "def _CollapseKeywords(self, content, keyword_str):\n", "code": "svn_keywords = {\n  # Standard keywords\n  'Date':                ['Date', 'LastChangedDate'],\n  'Revision':            ['Revision', 'LastChangedRevision', 'Rev'],\n  'Author':              ['Author', 'LastChangedBy'],\n  'HeadURL':             ['HeadURL', 'URL'],\n  'Id':                  ['Id'],\n\n  # Aliases\n  'LastChangedDate':     ['LastChangedDate', 'Date'],\n  'LastChangedRevision': ['LastChangedRevision', 'Rev', 'Revision'],\n  'LastChangedBy':       ['LastChangedBy', 'Author'],\n  'URL':                 ['URL', 'HeadURL'],\n}\n\ndef repl(m):\n   if m.group(2):\n     return \"$%s::%s$\" % (m.group(1), \" \" * len(m.group(3)))\n   return \"$%s$\" % m.group(1)\nkeywords = [keyword\n            for name in keyword_str.split(\" \")\n            for keyword in svn_keywords.get(name, [])]\nreturn re.sub(r\"\\$(%s):(:?)([^\\$]+)\\$\" % '|'.join(keywords), repl, content)", "path": "tools\\rietveld\\upload.py", "repo_name": "springpad/springpad", "stars": 43, "license": "None", "language": "python", "size": 246}
{"docstring": "\"\"\"Splits a patch into separate pieces for each file.\n\nArgs:\n  data: A string containing the output of svn diff.\n\nReturns:\n  A list of 2-tuple (filename, text) where text is the svn diff output\n    pertaining to filename.\n\"\"\"\n", "func_signal": "def SplitPatch(data):\n", "code": "patches = []\nfilename = None\ndiff = []\nfor line in data.splitlines(True):\n  new_filename = None\n  if line.startswith('Index:'):\n    unused, new_filename = line.split(':', 1)\n    new_filename = new_filename.strip()\n  elif line.startswith('Property changes on:'):\n    unused, temp_filename = line.split(':', 1)\n    # When a file is modified, paths use '/' between directories, however\n    # when a property is modified '\\' is used on Windows.  Make them the same\n    # otherwise the file shows up twice.\n    temp_filename = temp_filename.strip().replace('\\\\', '/')\n    if temp_filename != filename:\n      # File has property changes but no modifications, create a new diff.\n      new_filename = temp_filename\n  if new_filename:\n    if filename and diff:\n      patches.append((filename, ''.join(diff)))\n    filename = new_filename\n    diff = [line]\n    continue\n  if diff is not None:\n    diff.append(line)\nif filename and diff:\n  patches.append((filename, ''.join(diff)))\nreturn patches", "path": "tools\\rietveld\\upload.py", "repo_name": "springpad/springpad", "stars": 43, "license": "None", "language": "python", "size": 246}
{"docstring": "''' Optimized PATH_INFO matcher. '''\n", "func_signal": "def _match_path(self, environ):\n", "code": "path = environ['PATH_INFO'] or '/'\n# Assume we are in a warm state. Search compiled rules first.\nmatch = self.static.get(path)\nif match: return match, {}\nfor combined, rules in self.dynamic:\n    match = combined.match(path)\n    if not match: continue\n    gpat, match = rules[match.lastindex - 1]\n    return match, gpat(path).groupdict() if gpat else {}\n# Lazy-check if we are really in a warm state. If yes, stop here.\nif self.static or self.dynamic or not self.routes: return None, {}\n# Cold state: We have not compiled any rules yet. Do so and try again.\nif not environ.get('wsgi.run_once'):\n    self._compile()\n    return self._match_path(environ)\n# For run_once (CGI) environments, don't compile. Just check one by one.\nepath = path.replace(':','\\\\:') # Turn path into its own static rule.\nmatch = self.routes.get(epath) # This returns static rule only.\nif match: return match, {}\nfor rule in self.rules:\n    #: Skip static routes to reduce re.compile() calls.\n    if rule.count(':') < rule.count('\\\\:'): continue\n    match = self._compile_pattern(rule).match(path)\n    if match: return self.routes[rule], match.groupdict()\nreturn None, {}", "path": "bottle.py", "repo_name": "iurisilvio/bottle-chat", "stars": 63, "license": "None", "language": "python", "size": 154}
{"docstring": "''' Translate header field name to CGI/WSGI environ key. '''\n", "func_signal": "def _ekey(self, key):\n", "code": "key = key.replace('-','_').upper()\nif key in self.cgikeys:\n    return key\nreturn 'HTTP_' + key", "path": "bottle.py", "repo_name": "iurisilvio/bottle-chat", "stars": 63, "license": "None", "language": "python", "size": 154}
{"docstring": "''' An instance of :class:`HeaderDict`, a case-insensitive dict-like\n    view on the response headers. '''\n", "func_signal": "def headers(self):\n", "code": "self.__dict__['headers'] = hdict = HeaderDict()\nhdict.dict = self._headers\nreturn hdict", "path": "bottle.py", "repo_name": "iurisilvio/bottle-chat", "stars": 63, "license": "None", "language": "python", "size": 154}
{"docstring": "''' Make sure that other installed plugins don't affect the same\n    keyword argument.'''\n", "func_signal": "def setup(self, app):\n", "code": "for other in app.plugins:\n    if not isinstance(other, BeakerPlugin): continue\n    if other.keyword == self.keyword:\n        raise PluginError(\"Found another beaker session plugin \"\\\n        \"with conflicting settings (non-unique keyword).\")", "path": "chat.py", "repo_name": "iurisilvio/bottle-chat", "stars": 63, "license": "None", "language": "python", "size": 154}
{"docstring": "''' Reset all routes (force plugins to be re-applied) and clear all\n    caches. If an ID is given, only that specific route is affected. '''\n", "func_signal": "def reset(self, id=None):\n", "code": "if id is None: self.ccache.clear()\nelse: self.ccache.pop(id, None)\nif DEBUG:\n    for route in self.routes:\n        if route['id'] not in self.ccache:\n            self.ccache[route['id']] = self._build_callback(route)", "path": "bottle.py", "repo_name": "iurisilvio/bottle-chat", "stars": 63, "license": "None", "language": "python", "size": 154}
{"docstring": "''' Create a new cookie or replace an old one. If the `secret` parameter is\n    set, create a `Signed Cookie` (described below).\n\n    :param key: the name of the cookie.\n    :param value: the value of the cookie.\n    :param secret: a signature key required for signed cookies.\n\n    Additionally, this method accepts all RFC 2109 attributes that are\n    supported by :class:`cookie.Morsel`, including:\n\n    :param max_age: maximum age in seconds. (default: None)\n    :param expires: a datetime object or UNIX timestamp. (default: None)\n    :param domain: the domain that is allowed to read the cookie.\n      (default: current domain)\n    :param path: limits the cookie to a given path (default: ``/``)\n    :param secure: limit the cookie to HTTPS connections (default: off).\n    :param httponly: prevents client-side javascript to read this cookie\n      (default: off, requires Python 2.6 or newer).\n\n    If neither `expires` nor `max_age` is set (default), the cookie will\n    expire at the end of the browser session (as soon as the browser\n    window is closed).\n\n    Signed cookies may store any pickle-able object and are\n    cryptographically signed to prevent manipulation. Keep in mind that\n    cookies are limited to 4kb in most browsers.\n\n    Warning: Signed cookies are not encrypted (the client can still see\n    the content) and not copy-protected (the client can restore an old\n    cookie). The main intention is to make pickling and unpickling\n    save, not to store secret information at client side.\n'''\n", "func_signal": "def set_cookie(self, key, value, secret=None, **options):\n", "code": "if not self._cookies:\n    self._cookies = SimpleCookie()\n\nif secret:\n    value = touni(cookie_encode((key, value), secret))\nelif not isinstance(value, basestring):\n    raise TypeError('Secret key missing for non-string Cookie.')\n\nself._cookies[key] = value\nfor k, v in options.iteritems():\n    self._cookies[key][k.replace('_', '-')] = v", "path": "bottle.py", "repo_name": "iurisilvio/bottle-chat", "stars": 63, "license": "None", "language": "python", "size": 154}
{"docstring": "''' Shift path fragments from PATH_INFO to SCRIPT_NAME and vice versa.\n\n    :return: The modified paths.\n    :param script_name: The SCRIPT_NAME path.\n    :param script_name: The PATH_INFO path.\n    :param shift: The number of path fragments to shift. May be negative to\n      change the shift direction. (default: 1)\n'''\n", "func_signal": "def path_shift(script_name, path_info, shift=1):\n", "code": "if shift == 0: return script_name, path_info\npathlist = path_info.strip('/').split('/')\nscriptlist = script_name.strip('/').split('/')\nif pathlist and pathlist[0] == '': pathlist = []\nif scriptlist and scriptlist[0] == '': scriptlist = []\nif shift > 0 and shift <= len(pathlist):\n    moved = pathlist[:shift]\n    scriptlist = scriptlist + moved\n    pathlist = pathlist[shift:]\nelif shift < 0 and shift >= -len(scriptlist):\n    moved = scriptlist[shift:]\n    pathlist = moved + pathlist\n    scriptlist = scriptlist[:shift]\nelse:\n    empty = 'SCRIPT_NAME' if shift < 0 else 'PATH_INFO'\n    raise AssertionError(\"Cannot shift. Nothing left from %s\" % empty)\nnew_script_name = '/' + '/'.join(scriptlist)\nnew_path_info = '/' + '/'.join(pathlist)\nif path_info.endswith('/') and pathlist: new_path_info += '/'\nreturn new_script_name, new_path_info", "path": "bottle.py", "repo_name": "iurisilvio/bottle-chat", "stars": 63, "license": "None", "language": "python", "size": 154}
{"docstring": "'''\nGet a rendered template as a string iterator.\nYou can use a name, a filename or a template string as first parameter.\nTemplate rendering arguments can be passed as dictionaries\nor directly (as keyword arguments).\n'''\n", "func_signal": "def template(*args, **kwargs):\n", "code": "tpl = args[0] if args else None\ntemplate_adapter = kwargs.pop('template_adapter', SimpleTemplate)\nif tpl not in TEMPLATES or DEBUG:\n    settings = kwargs.pop('template_settings', {})\n    lookup = kwargs.pop('template_lookup', TEMPLATE_PATH)\n    if isinstance(tpl, template_adapter):\n        TEMPLATES[tpl] = tpl\n        if settings: TEMPLATES[tpl].prepare(**settings)\n    elif \"\\n\" in tpl or \"{\" in tpl or \"%\" in tpl or '$' in tpl:\n        TEMPLATES[tpl] = template_adapter(source=tpl, lookup=lookup, **settings)\n    else:\n        TEMPLATES[tpl] = template_adapter(name=tpl, lookup=lookup, **settings)\nif not TEMPLATES[tpl]:\n    abort(500, 'Template (%s) not found' % tpl)\nfor dictarg in args[1:]: kwargs.update(dictarg)\nreturn TEMPLATES[tpl].render(kwargs)", "path": "bottle.py", "repo_name": "iurisilvio/bottle-chat", "stars": 63, "license": "None", "language": "python", "size": 154}
{"docstring": "\"\"\" Create a new template.\nIf the source parameter (str or buffer) is missing, the name argument\nis used to guess a template filename. Subclasses can assume that\nself.source and/or self.filename are set. Both are strings.\nThe lookup, encoding and settings parameters are stored as instance\nvariables.\nThe lookup parameter stores a list containing directory paths.\nThe encoding parameter should be used to decode byte strings or files.\nThe settings parameter contains a dict for engine-specific settings.\n\"\"\"\n", "func_signal": "def __init__(self, source=None, name=None, lookup=[], encoding='utf8', **settings):\n", "code": "self.name = name\nself.source = source.read() if hasattr(source, 'read') else source\nself.filename = source.filename if hasattr(source, 'filename') else None\nself.lookup = map(os.path.abspath, lookup)\nself.encoding = encoding\nself.settings = self.settings.copy() # Copy from class variable\nself.settings.update(settings) # Apply\nif not self.source and self.name:\n    self.filename = self.search(self.name, self.lookup)\n    if not self.filename:\n        raise TemplateError('Template %s not found.' % repr(name))\nif not self.source and not self.filename:\n    raise TemplateError('No template specified.')\nself.prepare(**self.settings)", "path": "bottle.py", "repo_name": "iurisilvio/bottle-chat", "stars": 63, "license": "None", "language": "python", "size": 154}
{"docstring": "\"\"\" Form values parsed from an `url-encoded` or `multipart/form-data`\n    encoded POST or PUT request body. The result is retuned as a\n    :class:`MultiDict`. All keys and values are strings. File uploads\n    are stored separately in :attr:`files`. \"\"\"\n", "func_signal": "def forms(self):\n", "code": "forms = MultiDict()\nfor name, item in self.POST.iterallitems():\n    if not hasattr(item, 'filename'):\n        forms[name] = item\nreturn forms", "path": "bottle.py", "repo_name": "iurisilvio/bottle-chat", "stars": 63, "license": "None", "language": "python", "size": 154}
{"docstring": "\"\"\" Decorator: Register an output handler for a HTTP error code\"\"\"\n", "func_signal": "def error(self, code=500):\n", "code": "def wrapper(handler):\n    self.error_handler[int(code)] = handler\n    return handler\nreturn wrapper", "path": "bottle.py", "repo_name": "iurisilvio/bottle-chat", "stars": 63, "license": "None", "language": "python", "size": 154}
{"docstring": "\"\"\" Cookies parsed into a dictionary. Signed cookies are NOT decoded.\n    Use :meth:`get_cookie` if you expect signed cookies. \"\"\"\n", "func_signal": "def cookies(self):\n", "code": "raw_dict = SimpleCookie(self.environ.get('HTTP_COOKIE',''))\ncookies = {}\nfor cookie in raw_dict.itervalues():\n    cookies[cookie.key] = cookie.value\nreturn cookies", "path": "bottle.py", "repo_name": "iurisilvio/bottle-chat", "stars": 63, "license": "None", "language": "python", "size": 154}
{"docstring": "''' Uninstall plugins. Pass an instance to remove a specific plugin.\n    Pass a type object to remove all plugins that match that type.\n    Subclasses are not removed. Pass a string to remove all plugins with\n    a matching ``name`` attribute. Pass ``True`` to remove all plugins.\n    The list of affected plugins is returned. '''\n", "func_signal": "def uninstall(self, plugin):\n", "code": "removed, remove = [], plugin\nfor i, plugin in list(enumerate(self.plugins))[::-1]:\n    if remove is True or remove is plugin or remove is type(plugin) \\\n    or getattr(plugin, 'name', True) == remove:\n        removed.append(plugin)\n        del self.plugins[i]\n        if hasattr(plugin, 'close'): plugin.close()\nif removed: self.reset()\nreturn removed", "path": "bottle.py", "repo_name": "iurisilvio/bottle-chat", "stars": 63, "license": "None", "language": "python", "size": 154}
{"docstring": "''' Return a regular expression with named groups for each wildcard. '''\n", "func_signal": "def _compile_pattern(self, rule):\n", "code": "out = ''\nfor i, part in enumerate(self.syntax.split(rule)):\n    if i%3 == 0:   out += re.escape(part.replace('\\\\:',':'))\n    elif i%3 == 1: out += '(?P<%s>' % part if part else '(?:'\n    else:          out += '%s)' % (part or '[^/]+')\nreturn re.compile('^%s$'%out)", "path": "bottle.py", "repo_name": "iurisilvio/bottle-chat", "stars": 63, "license": "None", "language": "python", "size": 154}
{"docstring": "''' Create a new response header, replacing any previously defined\n    headers with the same name. This equals ``response[name] = value``.\n\n    :param append: Do not delete previously defined headers. This can\n                   result in two (or more) headers having the same name.\n'''\n", "func_signal": "def set_header(self, name, value, append=False):\n", "code": "if append:\n    self._headers.setdefault(_hkey(name), []).append(str(value))\nelse:\n    self._headers[_hkey(name)] = [str(value)]", "path": "bottle.py", "repo_name": "iurisilvio/bottle-chat", "stars": 63, "license": "None", "language": "python", "size": 154}
{"docstring": "\"\"\" Parse rfc2617 HTTP authentication header string (basic) and return (user,pass) tuple or None\"\"\"\n", "func_signal": "def parse_auth(header):\n", "code": "try:\n    method, data = header.split(None, 1)\n    if method.lower() == 'basic':\n        #TODO: Add 2to3 save base64[encode/decode] functions.\n        user, pwd = touni(base64.b64decode(tob(data))).split(':',1)\n        return user, pwd\nexcept (KeyError, ValueError):\n    return None", "path": "bottle.py", "repo_name": "iurisilvio/bottle-chat", "stars": 63, "license": "None", "language": "python", "size": 154}
{"docstring": "''' Close the application and all installed plugins. '''\n", "func_signal": "def close(self):\n", "code": "for plugin in self.plugins:\n    if hasattr(plugin, 'close'): plugin.close()\nself.stopped = True", "path": "bottle.py", "repo_name": "iurisilvio/bottle-chat", "stars": 63, "license": "None", "language": "python", "size": 154}
{"docstring": "\"\"\" Aborts execution and causes a 303 redirect. \"\"\"\n", "func_signal": "def redirect(url, code=303):\n", "code": "location = urljoin(request.url, url)\nraise HTTPResponse(\"\", status=code, header=dict(Location=location))", "path": "bottle.py", "repo_name": "iurisilvio/bottle-chat", "stars": 63, "license": "None", "language": "python", "size": 154}
{"docstring": "\"\"\" Open a file in a safe way and return :exc:`HTTPResponse` with status\n    code 200, 305, 401 or 404. Set Content-Type, Content-Encoding,\n    Content-Length and Last-Modified header. Obey If-Modified-Since header\n    and HEAD requests.\n\"\"\"\n", "func_signal": "def static_file(filename, root, mimetype='auto', download=False):\n", "code": "root = os.path.abspath(root) + os.sep\nfilename = os.path.abspath(os.path.join(root, filename.strip('/\\\\')))\nheader = dict()\n\nif not filename.startswith(root):\n    return HTTPError(403, \"Access denied.\")\nif not os.path.exists(filename) or not os.path.isfile(filename):\n    return HTTPError(404, \"File does not exist.\")\nif not os.access(filename, os.R_OK):\n    return HTTPError(403, \"You do not have permission to access this file.\")\n\nif mimetype == 'auto':\n    mimetype, encoding = mimetypes.guess_type(filename)\n    if mimetype: header['Content-Type'] = mimetype\n    if encoding: header['Content-Encoding'] = encoding\nelif mimetype:\n    header['Content-Type'] = mimetype\n\nif download:\n    download = os.path.basename(filename if download == True else download)\n    header['Content-Disposition'] = 'attachment; filename=\"%s\"' % download\n\nstats = os.stat(filename)\nheader['Content-Length'] = stats.st_size\nlm = time.strftime(\"%a, %d %b %Y %H:%M:%S GMT\", time.gmtime(stats.st_mtime))\nheader['Last-Modified'] = lm\n\nims = request.environ.get('HTTP_IF_MODIFIED_SINCE')\nif ims:\n    ims = parse_date(ims.split(\";\")[0].strip())\nif ims is not None and ims >= int(stats.st_mtime):\n    header['Date'] = time.strftime(\"%a, %d %b %Y %H:%M:%S GMT\", time.gmtime())\n    return HTTPResponse(status=304, header=header)\n\nbody = '' if request.method == 'HEAD' else open(filename, 'rb')\nreturn HTTPResponse(body, header=header)", "path": "bottle.py", "repo_name": "iurisilvio/bottle-chat", "stars": 63, "license": "None", "language": "python", "size": 154}
{"docstring": "''' This reads or sets the global settings stored in class.settings. '''\n", "func_signal": "def global_config(cls, key, *args):\n", "code": "if args:\n    cls.settings[key] = args[0]\nelse:\n    return cls.settings[key]", "path": "bottle.py", "repo_name": "iurisilvio/bottle-chat", "stars": 63, "license": "None", "language": "python", "size": 154}
{"docstring": "# Make sure textareas escape properly and render properly\n", "func_signal": "def test_textarea(self):\n", "code": "f = DummyField('hi<>bye')\nself.assertEqual(TextArea()(f), '<textarea id=\"\" name=\"f\">hi&lt;&gt;bye</textarea>')", "path": "tests\\widgets.py", "repo_name": "clones/wtforms", "stars": 32, "license": "other", "language": "python", "size": 1639}
{"docstring": "\"\"\"Returns a form field for a ``db.StringProperty``.\"\"\"\n", "func_signal": "def convert_StringProperty(model, prop, kwargs):\n", "code": "if prop.multiline:\n    kwargs['validators'].append(validators.length(max=500))\n    return f.TextAreaField(**kwargs)\nelse:\n    return get_TextField(kwargs)", "path": "wtforms\\ext\\appengine\\db.py", "repo_name": "clones/wtforms", "stars": 32, "license": "other", "language": "python", "size": 1639}
{"docstring": "# ListWidget just expects an iterable of field-like objects as its\n# 'field' so that is what we will give it\n", "func_signal": "def test(self):\n", "code": "field = DummyField([DummyField(x, label='l' + x) for x in ['foo', 'bar']], id='hai')\n\nself.assertEqual(ListWidget()(field), u'<ul id=\"hai\"><li>lfoo: foo</li><li>lbar: bar</li></ul>')\n\nw = ListWidget(html_tag='ol', prefix_label=False)\nself.assertEqual(w(field), u'<ol id=\"hai\"><li>foo lfoo</li><li>bar lbar</li></ol>')", "path": "tests\\widgets.py", "repo_name": "clones/wtforms", "stars": 32, "license": "other", "language": "python", "size": 1639}
{"docstring": "\"\"\"\nReturns an ``IntegerField``, applying the ``db.IntegerProperty`` range\nlimits.\n\"\"\"\n", "func_signal": "def get_IntegerField(kwargs):\n", "code": "v = validators.NumberRange(min=-0x8000000000000000, max=0x7fffffffffffffff)\nkwargs['validators'].append(v)\nreturn f.IntegerField(**kwargs)", "path": "wtforms\\ext\\appengine\\db.py", "repo_name": "clones/wtforms", "stars": 32, "license": "other", "language": "python", "size": 1639}
{"docstring": "\"\"\"Returns a form field for a ``db.LinkProperty``.\"\"\"\n", "func_signal": "def convert_LinkProperty(model, prop, kwargs):\n", "code": "kwargs['validators'].append(validators.url())\nreturn get_TextField(kwargs)", "path": "wtforms\\ext\\appengine\\db.py", "repo_name": "clones/wtforms", "stars": 32, "license": "other", "language": "python", "size": 1639}
{"docstring": "\"\"\"Returns a form field for a ``db.EmailProperty``.\"\"\"\n", "func_signal": "def convert_EmailProperty(model, prop, kwargs):\n", "code": "kwargs['validators'].append(validators.email())\nreturn get_TextField(kwargs)", "path": "wtforms\\ext\\appengine\\db.py", "repo_name": "clones/wtforms", "stars": 32, "license": "other", "language": "python", "size": 1639}
{"docstring": "\"\"\"Returns a form field for a ``db.ReferenceProperty``.\"\"\"\n", "func_signal": "def convert_ReferenceProperty(model, prop, kwargs):\n", "code": "kwargs['reference_class'] = prop.reference_class\nreturn ReferencePropertyField(**kwargs)", "path": "wtforms\\ext\\appengine\\db.py", "repo_name": "clones/wtforms", "stars": 32, "license": "other", "language": "python", "size": 1639}
{"docstring": "\"\"\"\nYield indices of any keys with given prefix.\n\nformdata must be an object which will produce keys when iterated.  For\nexample, if field 'foo' contains keys 'foo-0-bar', 'foo-1-baz', then\nthe numbers 0 and 1 will be yielded, but not neccesarily in order.\n\"\"\"\n", "func_signal": "def _extract_indices(self, prefix, formdata):\n", "code": "offset = len(prefix) + 1\nfor k in formdata:\n    if k.startswith(prefix):\n        k = k[offset:].split('-', 1)[0]\n        if k.isdigit():\n            yield int(k)", "path": "wtforms\\fields.py", "repo_name": "clones/wtforms", "stars": 32, "license": "other", "language": "python", "size": 1639}
{"docstring": "\"\"\"Tests that the validators support lazy translation strings for messages.\"\"\"\n\n", "func_signal": "def test_lazy_proxy(self):\n", "code": "class ReallyLazyProxy(object):\n    def __unicode__(self):\n        raise Exception('Translator function called during form declaration: it should be called at response time.')\n    __str__ = __unicode__\n\nmessage = ReallyLazyProxy()\nself.assertRaises(Exception, str, message)\nself.assertRaises(Exception, unicode, message)\nself.assert_(equal_to('fieldname', message=message))\nself.assert_(length(min=1, message=message))\nself.assert_(NumberRange(1,5, message=message))\nself.assert_(required(message=message))\nself.assert_(regexp('.+', message=message))\nself.assert_(email(message=message))\nself.assert_(ip_address(message=message))\nself.assert_(url(message=message))", "path": "tests\\validators.py", "repo_name": "clones/wtforms", "stars": 32, "license": "other", "language": "python", "size": 1639}
{"docstring": "\"\"\"\nProcess data received over the wire from a form.\n\nThis will be called during form construction with data supplied\nthrough the `formdata` argument.\n\n:param valuelist: A list of strings to process.\n\"\"\"\n", "func_signal": "def process_formdata(self, valuelist):\n", "code": "if valuelist:\n    self.data = valuelist[0]", "path": "wtforms\\fields.py", "repo_name": "clones/wtforms", "stars": 32, "license": "other", "language": "python", "size": 1639}
{"docstring": "\"\"\" Removes the last entry from the list and returns it. \"\"\"\n", "func_signal": "def pop_entry(self):\n", "code": "entry = self.entries.pop()\nself.last_index -= 1\nreturn entry", "path": "wtforms\\fields.py", "repo_name": "clones/wtforms", "stars": 32, "license": "other", "language": "python", "size": 1639}
{"docstring": "\"\"\"\nGenerate a dictionary of fields for a given Django model.\n\nSee `model_form` docstring for description of parameters.\n\"\"\"\n", "func_signal": "def model_fields(model, only=None, exclude=None, field_args=None, converter=None):\n", "code": "converter = converter or ModelConverter()\nfield_args = field_args or {}\n\nmodel_fields = ((f.attname, f) for f in model._meta.fields)\nif only:\n    model_fields = (x for x in model_fields if x[0] in only)\nelif exclude:\n    model_fields = (x for x in model_fields if x[0] not in exclude)\n\nfield_dict = {}\nfor name, model_field in model_fields:\n    field = converter.convert(model, model_field, field_args.get(name))\n    if field is not None:\n        field_dict[name] = field\n\nreturn field_dict", "path": "wtforms\\ext\\django\\orm.py", "repo_name": "clones/wtforms", "stars": 32, "license": "other", "language": "python", "size": 1639}
{"docstring": "\"\"\"\nCreate a wtforms Form for a given Django model class::\n\n    from wtforms.ext.django.orm import model_form\n    from myproject.myapp.models import User\n    UserForm = model_form(User)\n\n:param model:\n    A Django ORM model class\n:param base_class:\n    Base form class to extend from. Must be a ``wtforms.Form`` subclass.\n:param only:\n    An optional iterable with the property names that should be included in\n    the form. Only these properties will have fields.\n:param exclude:\n    An optional iterable with the property names that should be excluded\n    from the form. All other properties will have fields.\n:param field_args:\n    An optional dictionary of field names mapping to keyword arguments used\n    to construct each field object.\n:param converter:\n    A converter to generate the fields based on the model properties. If\n    not set, ``ModelConverter`` is used.\n\"\"\"\n", "func_signal": "def model_form(model, base_class=Form, only=None, exclude=None, field_args=None, converter=None):\n", "code": "field_dict = model_fields(model, only, exclude, field_args, converter)\nreturn type(model._meta.object_name + 'Form', (base_class, ), field_dict)", "path": "wtforms\\ext\\django\\orm.py", "repo_name": "clones/wtforms", "stars": 32, "license": "other", "language": "python", "size": 1639}
{"docstring": "\"\"\"Returns a form field for a ``db.TimeProperty``.\"\"\"\n", "func_signal": "def convert_TimeProperty(model, prop, kwargs):\n", "code": "if prop.auto_now or prop.auto_now_add:\n    return None\n\nreturn f.DateTimeField(format='%H-%M-%S', **kwargs)", "path": "wtforms\\ext\\appengine\\db.py", "repo_name": "clones/wtforms", "stars": 32, "license": "other", "language": "python", "size": 1639}
{"docstring": "\"\"\"Returns a form field for a ``db.DateProperty``.\"\"\"\n", "func_signal": "def convert_DateProperty(model, prop, kwargs):\n", "code": "if prop.auto_now or prop.auto_now_add:\n    return None\n\nreturn f.DateField(format='%Y-%m-%d', **kwargs)", "path": "wtforms\\ext\\appengine\\db.py", "repo_name": "clones/wtforms", "stars": 32, "license": "other", "language": "python", "size": 1639}
{"docstring": "\"\"\"\nGenerate a dictionary of fields for a given SQLAlchemy model.\n\nSee `model_form` docstring for description of parameters.\n\"\"\"\n", "func_signal": "def model_fields(model, only=None, exclude=None, field_args=None, converter=None):\n", "code": "if not hasattr(model, '_sa_class_manager'):\n    raise TypeError('model must be a sqlalchemy mapped model')\n\nmapper = model._sa_class_manager.mapper\nconverter = converter or ModelConverter()\nfield_args = field_args or {}\n\nproperties = ((p.key, p) for p in mapper.iterate_properties)\nif only:\n    properties = (x for x in properties if x[0] in only)\nelif exclude:\n    properties = (x for x in properties if x[0] not in exclude)\n\nfield_dict = {}\nfor name, prop in properties:\n    field = converter.convert(model, mapper, prop, field_args.get(name))\n    if field is not None:\n        field_dict[name] = field\n\nreturn field_dict", "path": "wtforms\\ext\\sqlalchemy\\orm.py", "repo_name": "clones/wtforms", "stars": 32, "license": "other", "language": "python", "size": 1639}
{"docstring": "\"\"\"\nCreate a wtforms Form for a given SQLAlchemy model class::\n\n    from wtforms.ext.sqlalchemy.orm import model_form\n    from myapp.models import User\n    UserForm = model_form(User)\n\n:param model:\n    A SQLAlchemy mapped model class.\n:param base_class:\n    Base form class to extend from. Must be a ``wtforms.Form`` subclass.\n:param only:\n    An optional iterable with the property names that should be included in\n    the form. Only these properties will have fields.\n:param exclude:\n    An optional iterable with the property names that should be excluded\n    from the form. All other properties will have fields.\n:param field_args:\n    An optional dictionary of field names mapping to keyword arguments used\n    to construct each field object.\n:param converter:\n    A converter to generate the fields based on the model properties. If\n    not set, ``ModelConverter`` is used.\n\"\"\"\n", "func_signal": "def model_form(model, base_class=Form, only=None, exclude=None, field_args=None, converter=None):\n", "code": "field_dict = model_fields(model, only, exclude, field_args, converter)\nreturn type(model.__name__ + 'Form', (base_class, ), field_dict)", "path": "wtforms\\ext\\sqlalchemy\\orm.py", "repo_name": "clones/wtforms", "stars": 32, "license": "other", "language": "python", "size": 1639}
{"docstring": "\"\"\"Returns a form field for a ``db.DateTimeProperty``.\"\"\"\n", "func_signal": "def convert_DateTimeProperty(model, prop, kwargs):\n", "code": "if prop.auto_now or prop.auto_now_add:\n    return None\n\nreturn f.DateTimeField(format='%Y-%m-%d %H-%M-%S', **kwargs)", "path": "wtforms\\ext\\appengine\\db.py", "repo_name": "clones/wtforms", "stars": 32, "license": "other", "language": "python", "size": 1639}
{"docstring": "\"\"\"\nReturns a form field for a single model property.\n\n:param model:\n    The ``db.Model`` class that contains the property.\n:param prop:\n    The model property: a ``db.Property`` instance.\n:param field_args:\n    Optional keyword arguments to construct the field.\n\"\"\"\n", "func_signal": "def convert(self, model, prop, field_args):\n", "code": "kwargs = {\n    'label': prop.name.replace('_', ' ').title(),\n    'default': prop.default_value(),\n    'validators': [],\n}\nif field_args:\n    kwargs.update(field_args)\n\nif prop.required:\n    kwargs['validators'].append(validators.required())\n\nif prop.choices:\n    # Use choices in a select field.\n    kwargs['choices'] = [(v, v) for v in prop.choices]\n    return f.SelectField(**kwargs)\nelse:\n    converter = self.converters.get(type(prop).__name__, None)\n    if converter is not None:\n        return converter(model, prop, kwargs)", "path": "wtforms\\ext\\appengine\\db.py", "repo_name": "clones/wtforms", "stars": 32, "license": "other", "language": "python", "size": 1639}
{"docstring": "\"\"\"\nReturns a ``TextField``, applying the ``db.StringProperty`` length limit\nof 500 bytes.\n\"\"\"\n", "func_signal": "def get_TextField(kwargs):\n", "code": "kwargs['validators'].append(validators.length(max=500))\nreturn f.TextField(**kwargs)", "path": "wtforms\\ext\\appengine\\db.py", "repo_name": "clones/wtforms", "stars": 32, "license": "other", "language": "python", "size": 1639}
{"docstring": "\"\"\"Hashes a file using OpenSubtitle's method.\n\n> In natural language it calculates: size + 64bit chksum of the first and\n> last 64k (even if they overlap because the file is smaller than 128k).\n\nA slightly more Pythonic version of the Python solution on..\nhttp://trac.opensubtitles.org/projects/opensubtitles/wiki/HashSourceCodes\n\"\"\"\n", "func_signal": "def opensubtitleHashFile(name):\n", "code": "longlongformat = 'q'\nbytesize = struct.calcsize(longlongformat)\n\nf = open(name, \"rb\")\n\nfilesize = os.path.getsize(name)\nfhash = filesize\n\nif filesize < 65536 * 2:\n    raise ValueError(\"File size must be larger than %s bytes (is %s)\" % (65536*2, filesize))\n\nfor x in range(65536/bytesize):\n    buf = f.read(bytesize)\n    (l_value,) = struct.unpack(longlongformat, buf)\n    fhash += l_value\n    fhash = fhash & 0xFFFFFFFFFFFFFFFF # to remain as 64bit number\n\n\nf.seek(max(0,filesize-65536),0)\nfor x in range(65536/bytesize):\n    buf = f.read(bytesize)\n    (l_value,)= struct.unpack(longlongformat, buf)\n    fhash += l_value\n    fhash = fhash & 0xFFFFFFFFFFFFFFFF\n\nf.close()\nreturn  \"%016x\" % fhash", "path": "tmdb.py", "repo_name": "dbr/themoviedb", "stars": 33, "license": "unlicense", "language": "python", "size": 360}
{"docstring": "\"\"\"Returns movie info by its TheMovieDb or IMDb ID.\nAn IMDb ID has to start with 'tt'.\nReturns a Movie instance\n\"\"\"\n", "func_signal": "def getMovieInfo(self, id):\n", "code": "if str(id).startswith('tt'):\n    imdb_url = config['urls']['movie.imdbLookup'] % (id)\n    etree = XmlHandler(imdb_url).getEt()\n    id = etree.find(\"movies/movie/id\").text\nurl = config['urls']['movie.getInfo'] % (id)\netree = XmlHandler(url).getEt()\nmoviesTree = etree.find(\"movies\").findall(\"movie\")\n\nif len(moviesTree) == 0:\n    raise TmdNoResults(\"No results for id %s\" % id)\n\nreturn self._parseMovie(moviesTree[0])", "path": "tmdb.py", "repo_name": "dbr/themoviedb", "stars": 33, "license": "unlicense", "language": "python", "size": 360}
{"docstring": "\"\"\"Takes an elementtree Element ('image') and stores the url,\nalong with the type, id and size.\n\nIs a list containing each image as a dictionary (which includes the\nvarious sizes)\n\nFor example:\n<image type=\"poster\" size=\"original\" url=\"http://images.themoviedb.org/posters/4181/67926_sin-city-02-color_122_207lo.jpg\" id=\"4181\"/>\n\n..becomes:\nimages[0] = {'id':4181', 'type': 'poster', 'original': 'http://images.themov...'}\n\"\"\"\n", "func_signal": "def set(self, image_et):\n", "code": "_type = image_et.get(\"type\")\n_id = image_et.get(\"id\")\nsize = image_et.get(\"size\")\nurl = image_et.get(\"url\")\n\ncur = self.find_by('id', _id)\nif len(cur) == 0:\n    nimg = Image(_id = _id, _type = _type, size = size, url = url)\n    self.append(nimg)\nelif len(cur) == 1:\n    cur[0][size] = url\nelse:\n    raise ValueError(\"Found more than one poster with id %s, this should never happen\" % (_id))", "path": "tmdb.py", "repo_name": "dbr/themoviedb", "stars": 33, "license": "unlicense", "language": "python", "size": 360}
{"docstring": "\"\"\"Takes an elementtree Element ('studio') and stores the url,\nusing the name as the dict key.\n\nFor example:\n       <studio url=\"http://www.themoviedb.org/encyclopedia/company/20\" name=\"Miramax Films\"/>\n\n..becomes:\nstudios['name'] = 'http://www.themoviedb.org/encyclopedia/company/20'\n\"\"\"\n", "func_signal": "def set(self, studio_et):\n", "code": "name = studio_et.get(\"name\")\nurl = studio_et.get(\"url\")\nself[name] = url", "path": "tmdb.py", "repo_name": "dbr/themoviedb", "stars": 33, "license": "unlicense", "language": "python", "size": 360}
{"docstring": "\"\"\"Checks you can get the director of a film\n\"\"\"\n", "func_signal": "def test_get_director():\n", "code": "mid = tmdb.search(\"Inglourious Basterds\")[0]['id']\nmovie = tmdb.getMovieInfo(mid)\n\nassert len(movie['cast']['director']) == 1\nassert movie['cast']['director'][0]['name'] == \"Quentin Tarantino\"", "path": "test_tmdb.py", "repo_name": "dbr/themoviedb", "stars": 33, "license": "unlicense", "language": "python", "size": 360}
{"docstring": "\"\"\"Tests tmdb.search() wrapper works correctly\n\"\"\"\n", "func_signal": "def test_search_wrapper():\n", "code": "r = tmdb.search(\"The Matrix\")\nassert isinstance(r, tmdb.SearchResults)", "path": "test_tmdb.py", "repo_name": "dbr/themoviedb", "stars": 33, "license": "unlicense", "language": "python", "size": 360}
{"docstring": "\"\"\"Takes an elementtree Element ('country') and stores the url,\nusing the name and code as the dict key.\n\nFor example:\n       <country url=\"http://www.themoviedb.org/encyclopedia/country/223\" name=\"United States of America\" code=\"US\"/>\n\n..becomes:\ncountries['code']['name'] = 'http://www.themoviedb.org/encyclopedia/country/223'\n\"\"\"\n", "func_signal": "def set(self, country_et):\n", "code": "code = country_et.get(\"code\")\nname = country_et.get(\"name\")\nurl = country_et.get(\"url\")\nself.setdefault(code, {})[name] = url", "path": "tmdb.py", "repo_name": "dbr/themoviedb", "stars": 33, "license": "unlicense", "language": "python", "size": 360}
{"docstring": "\"\"\"Get info from first search result\n\"\"\"\n", "func_signal": "def test_info_from_search():\n", "code": "t = tmdb.MovieDb()\nresults = t.search(\"Fight Club\")\nfirst_result = results[0]\ninfo = first_result.info()\nassert info['name'] == \"Fight Club\"", "path": "test_tmdb.py", "repo_name": "dbr/themoviedb", "stars": 33, "license": "unlicense", "language": "python", "size": 360}
{"docstring": "\"\"\"Tests searching by file hash\n\"\"\"\n\n", "func_signal": "def test_mediagetinfo():\n", "code": "import nose\nraise nose.SkipTest(\"Finding reliable hash to test with is difficult..\")\n\nt = tmdb.MovieDb()\nfilms = t.mediaGetInfo(hash = '907172e7fe51ba57', size = 742086656)\nfilm = films[0]\nprint(film['name'])\nassert film['name'] == 'Sin City'", "path": "test_tmdb.py", "repo_name": "dbr/themoviedb", "stars": 33, "license": "unlicense", "language": "python", "size": 360}
{"docstring": "\"\"\"Convenience wrapper for MovieDb.mediaGetInfo - so you can do..\n\n>>> import tmdb\n>>> tmdb.mediaGetInfo('907172e7fe51ba57', size = 742086656)[0]  #doctest: +SKIP\n<MovieResult: 'Sin City' (2005-03-31)>\n\nNote that I have not found this API method reliable (even finding\na reliable test-case has proved difficult)\n\"\"\"\n", "func_signal": "def mediaGetInfo(hash, size):\n", "code": "mdb = MovieDb()\nreturn mdb.mediaGetInfo(hash, size)", "path": "tmdb.py", "repo_name": "dbr/themoviedb", "stars": 33, "license": "unlicense", "language": "python", "size": 360}
{"docstring": "\"\"\"Takes an elementtree Element ('category') and stores the url,\nusing the type and name as the dict key.\n\nFor example:\n       <category type=\"genre\" url=\"http://themoviedb.org/encyclopedia/category/80\" name=\"Crime\"/>\n\n..becomes:\ncategories['genre']['Crime'] = 'http://themoviedb.org/encyclopedia/category/80'\n\"\"\"\n", "func_signal": "def set(self, category_et):\n", "code": "_type = category_et.get(\"type\")\nname = category_et.get(\"name\")\nurl = category_et.get(\"url\")\nself.setdefault(_type, {})[name] = url\nself[_type][name] = url", "path": "tmdb.py", "repo_name": "dbr/themoviedb", "stars": 33, "license": "unlicense", "language": "python", "size": 360}
{"docstring": "\"\"\"Used to retrieve specific information about a movie but instead of\npassing a TMDb ID, you pass a file hash and filesize in bytes\n\"\"\"\n", "func_signal": "def mediaGetInfo(self, hash, size):\n", "code": "url = config['urls']['media.getInfo'] % (hash, size)\netree = XmlHandler(url).getEt()\nmoviesTree = etree.find(\"movies\").findall(\"movie\")\nif len(moviesTree) == 0:\n    raise TmdNoResults(\"No results for hash %s\" % hash)\n\nreturn [self._parseMovie(x) for x in moviesTree]", "path": "tmdb.py", "repo_name": "dbr/themoviedb", "stars": 33, "license": "unlicense", "language": "python", "size": 360}
{"docstring": "\"\"\"Generates several tests posters/backdrops\n\nUses test generator to prevent multiple requests for the movie info\n\"\"\"\n", "func_signal": "def test_artwork_generator():\n", "code": "filmId = tmdb.MovieDb().search(\"Fight Club\")[0]['id']\nfilm = tmdb.MovieDb().getMovieInfo(filmId)\n\ndef test_poster_urls():\n    \"\"\"Checks posters are valid looking URLs\n    \"\"\"\n    for poster in film['images'].posters:\n        for key, value in poster.items():\n            if key not in ['id', 'type']:\n                assert value.startswith(\"http://\")\n\nyield test_poster_urls\n\ndef test_backdrop_urls():\n    \"\"\"Checks backdrop images are valid looking URLs\n    \"\"\"\n    for poster in film['images'].posters:\n        for key, value in poster.items():\n            if key not in ['id', 'type']:\n                assert value.startswith(\"http://\")\n\nyield test_backdrop_urls\n\ndef test_artwork_repr():\n    \"\"\"Checks artwork repr looks sensible\n    \"\"\"\n    poster_repr = repr(film['images'].posters[0])\n    assert poster_repr.startswith(\"<Image (poster for ID\")\n\n    backdrop_repr = repr(film['images'].backdrops[0])\n    assert backdrop_repr.startswith(\"<Image (backdrop for ID\")\n\nyield test_artwork_repr\n\ndef test_posters():\n    \"\"\"Check retrieving largest artwork\n    \"\"\"\n    assert len(film['images'].posters) > 1\n    assert len(film['images'].backdrops) > 1\n    print(film['images'].posters[0]['cover'])\n    url = film['images'].posters[0]['cover']\n    assert url.startswith('http://')\n    assert url.endswith('.jpg')\n\nyield test_posters", "path": "test_tmdb.py", "repo_name": "dbr/themoviedb", "stars": 33, "license": "unlicense", "language": "python", "size": 360}
{"docstring": "\"\"\"Convenience wrapper for MovieDb.search - so you can do..\n\n>>> import tmdb\n>>> tmdb.search(\"Inglourious Basterds\")\n<Search results: [<MovieResult: 'Inglourious Basterds' (2009-08-20)>]>\n\"\"\"\n", "func_signal": "def search(name):\n", "code": "mdb = MovieDb()\nreturn mdb.search(name)", "path": "tmdb.py", "repo_name": "dbr/themoviedb", "stars": 33, "license": "unlicense", "language": "python", "size": 360}
{"docstring": "\"\"\"Gets a movie ID via search helper, then calls getMovieInfo using this\n\"\"\"\n", "func_signal": "def test_search_to_info():\n", "code": "sr = tmdb.search(\"fight club\")[0]\nmovie = tmdb.getMovieInfo(sr['id'])\nassert sr['name'] == movie['name']", "path": "test_tmdb.py", "repo_name": "dbr/themoviedb", "stars": 33, "license": "unlicense", "language": "python", "size": 360}
{"docstring": "\"\"\"Checks that a movie can be retrieved via its IMDb ID\n\"\"\"\n", "func_signal": "def test_get_movie_from_imdb():\n", "code": "movie = tmdb.getMovieInfo('tt0079023')\n\nassert len(movie['cast']['director']) == 2\nassert movie['cast']['director'][0]['name'] == \"Jean-Marie Straub\"\nprint(repr(movie['cast']['director'][1]['name']))\nassert movie['cast']['director'][1]['name'] == u\"Dani\\xe8le Huillet\"", "path": "test_tmdb.py", "repo_name": "dbr/themoviedb", "stars": 33, "license": "unlicense", "language": "python", "size": 360}
{"docstring": "\"\"\"Check SearchResults are usable\n\"\"\"\n", "func_signal": "def test_search_results():\n", "code": "t = tmdb.MovieDb()\nresults = t.search(\"Fight Club\")\nfirst_result = results[0]\n\nassert isinstance(first_result, tmdb.MovieResult)\n\nprint(first_result['name'])\nassert first_result['name'] == 'Fight Club'\n\nprint(first_result['released'])\nassert first_result['released'] == '1999-10-14'\n\nprint(first_result['imdb_id'])\nassert first_result['imdb_id'] == 'tt0137523'", "path": "test_tmdb.py", "repo_name": "dbr/themoviedb", "stars": 33, "license": "unlicense", "language": "python", "size": 360}
{"docstring": "\"\"\"Simple test search\n\"\"\"\n", "func_signal": "def test_simple_search():\n", "code": "t = tmdb.MovieDb()\nsr = t.search(\"Fight Club\")\nassert isinstance(sr, tmdb.SearchResults)\nassert sr[0]['name'] == \"Fight Club\"", "path": "test_tmdb.py", "repo_name": "dbr/themoviedb", "stars": 33, "license": "unlicense", "language": "python", "size": 360}
{"docstring": "\"\"\"Tests actors have thumbnails\n\"\"\"\n", "func_signal": "def test_castthumbnails():\n", "code": "t = tmdb.MovieDb()\nfilm = t.getMovieInfo(950)\nassert 'thumb' in film['cast']['actor'][0]\nassert film['cast']['actor'][0]['thumb'].startswith('http://')", "path": "test_tmdb.py", "repo_name": "dbr/themoviedb", "stars": 33, "license": "unlicense", "language": "python", "size": 360}
{"docstring": "\"\"\"Performs a MovieDb.getMovieInfo search on the current id, returns\na Movie object\n\"\"\"\n", "func_signal": "def info(self):\n", "code": "cur_id = self['id']\ninfo = MovieDb().getMovieInfo(cur_id)\nreturn info", "path": "tmdb.py", "repo_name": "dbr/themoviedb", "stars": 33, "license": "unlicense", "language": "python", "size": 360}
{"docstring": "\"\"\"str(BEREnumerated(n)) should give known result with known input\"\"\"\n", "func_signal": "def testToBEREnumeratedKnownValues(self):\n", "code": "for integer, encoded in self.knownValues:\n    result = pureber.BEREnumerated(integer)\n    result = str(result)\n    result = map(ord, result)\n    assert encoded==result", "path": "ldaptor\\test\\test_pureber.py", "repo_name": "antong/ldaptor", "stars": 39, "license": "lgpl-2.1", "language": "python", "size": 1668}
{"docstring": "\"\"\"BERInteger(encoded=\"...\") should give known result with known input\"\"\"\n", "func_signal": "def testFromBERIntegerKnownValues(self):\n", "code": "for integer, encoded in self.knownValues:\n    m=s(*encoded)\n    result, bytes = pureber.berDecodeObject(pureber.BERDecoderContext(), m)\n    self.assertEquals(bytes, len(m))\n    assert isinstance(result, pureber.BERInteger)\n    result = result.value\n    assert integer==result", "path": "ldaptor\\test\\test_pureber.py", "repo_name": "antong/ldaptor", "stars": 39, "license": "lgpl-2.1", "language": "python", "size": 1668}
{"docstring": "\"\"\"BEROctetString(encoded=BEROctetString(n*'x')).value==n*'x' for some values of n\"\"\"\n", "func_signal": "def testSanity(self):\n", "code": "for n in 0,1,2,3,4,5,6,100,126,127,128,129,1000,2000:\n    encoded = str(pureber.BEROctetString(n*'x'))\n    result, bytes = pureber.berDecodeObject(pureber.BERDecoderContext(), encoded)\n    self.assertEquals(bytes, len(encoded))\n    assert isinstance(result, pureber.BEROctetString)\n    result = result.value\n    assert n*'x'==result", "path": "ldaptor\\test\\test_pureber.py", "repo_name": "antong/ldaptor", "stars": 39, "license": "lgpl-2.1", "language": "python", "size": 1668}
{"docstring": "\"\"\"Join all members of list to a string. Integer members are chr()ed\"\"\"\n", "func_signal": "def s(*l):\n", "code": "r=''\nfor e in l:\n    if isinstance(e, types.IntType):\n        e=chr(e)\n    r=r+str(e)\nreturn r", "path": "ldaptor\\test\\test_pureber.py", "repo_name": "antong/ldaptor", "stars": 39, "license": "lgpl-2.1", "language": "python", "size": 1668}
{"docstring": "\"\"\"BERSequence(encoded=\"...\") with too short input should throw BERExceptionInsufficientData\"\"\"\n", "func_signal": "def testPartialBERSequenceEncodings(self):\n", "code": "m=str(pureber.BERSequence([pureber.BERInteger(2)]))\nassert len(m)==5\n\nself.assertRaises(pureber.BERExceptionInsufficientData, pureber.berDecodeObject, pureber.BERDecoderContext(), m[:4])\nself.assertRaises(pureber.BERExceptionInsufficientData, pureber.berDecodeObject, pureber.BERDecoderContext(), m[:3])\nself.assertRaises(pureber.BERExceptionInsufficientData, pureber.berDecodeObject, pureber.BERDecoderContext(), m[:2])\nself.assertRaises(pureber.BERExceptionInsufficientData, pureber.berDecodeObject, pureber.BERDecoderContext(), m[:1])\nself.assertEquals((None, 0), pureber.berDecodeObject(pureber.BERDecoderContext(), ''))", "path": "ldaptor\\test\\test_pureber.py", "repo_name": "antong/ldaptor", "stars": 39, "license": "lgpl-2.1", "language": "python", "size": 1668}
{"docstring": "\"\"\"Like os.path.split, only does all the splits at once.\"\"\"\n", "func_signal": "def splitpath(path):\n", "code": "l=[]\nwhile path:\n    head,tail=os.path.split(path)\n    l.insert(0, tail)\n    path=head\nreturn l", "path": "admin\\doctest-all.py", "repo_name": "antong/ldaptor", "stars": 39, "license": "lgpl-2.1", "language": "python", "size": 1668}
{"docstring": "\"\"\"BER objects do not equal BER objects with different type or content\"\"\"\n", "func_signal": "def testBERBaseInEquality(self):\n", "code": "for i in xrange(len(self.valuesToTest)):\n    for j in xrange(len(self.valuesToTest)):\n        if i!=j:\n            i_class, i_args = self.valuesToTest[i]\n            j_class, j_args = self.valuesToTest[j]\n            x=i_class(*i_args)\n            y=j_class(*j_args)\n            assert x!=y", "path": "ldaptor\\test\\test_pureber.py", "repo_name": "antong/ldaptor", "stars": 39, "license": "lgpl-2.1", "language": "python", "size": 1668}
{"docstring": "\"\"\"BERInteger(encoded=BERInteger(n)).value==n for -1000..1000\"\"\"\n", "func_signal": "def testSanity(self):\n", "code": "for n in range(-1000, 1001, 10):\n    encoded = str(pureber.BERInteger(n))\n    result, bytes = pureber.berDecodeObject(pureber.BERDecoderContext(), encoded)\n    self.assertEquals(bytes, len(encoded))\n    assert isinstance(result, pureber.BERInteger)\n    result = result.value\n    assert n==result", "path": "ldaptor\\test\\test_pureber.py", "repo_name": "antong/ldaptor", "stars": 39, "license": "lgpl-2.1", "language": "python", "size": 1668}
{"docstring": "\"\"\"BEROctetString(encoded=\"...\") should give known result with known input\"\"\"\n", "func_signal": "def testFromBEROctetStringKnownValues(self):\n", "code": "for st, encoded in self.knownValues:\n    m=s(*encoded)\n    result, bytes = pureber.berDecodeObject(pureber.BERDecoderContext(), m)\n    self.assertEquals(bytes, len(m))\n    assert isinstance(result, pureber.BEROctetString)\n    result = str(result)\n    result = map(ord, result)\n    assert encoded==result", "path": "ldaptor\\test\\test_pureber.py", "repo_name": "antong/ldaptor", "stars": 39, "license": "lgpl-2.1", "language": "python", "size": 1668}
{"docstring": "\"\"\"str(BERInteger(n)) should give known result with known input\"\"\"\n", "func_signal": "def testToBERIntegerKnownValues(self):\n", "code": "for integer, encoded in self.knownValues:\n    result = pureber.BERInteger(integer)\n    result = str(result)\n    result = map(ord, result)\n    assert encoded==result", "path": "ldaptor\\test\\test_pureber.py", "repo_name": "antong/ldaptor", "stars": 39, "license": "lgpl-2.1", "language": "python", "size": 1668}
{"docstring": "\"\"\"BERBoolean(encoded=\"...\") with too short input should throw BERExceptionInsufficientData\"\"\"\n", "func_signal": "def testPartialBERBooleanEncodings(self):\n", "code": "m=str(pureber.BERBoolean(42))\nassert len(m)==3\nself.assertRaises(pureber.BERExceptionInsufficientData, pureber.berDecodeObject, pureber.BERDecoderContext(), m[:2])\nself.assertRaises(pureber.BERExceptionInsufficientData, pureber.berDecodeObject, pureber.BERDecoderContext(), m[:1])\nself.assertEquals((None, 0), pureber.berDecodeObject(pureber.BERDecoderContext(), ''))", "path": "ldaptor\\test\\test_pureber.py", "repo_name": "antong/ldaptor", "stars": 39, "license": "lgpl-2.1", "language": "python", "size": 1668}
{"docstring": "\"\"\"BERNull(encoded=\"...\") should give known result with known input\"\"\"\n", "func_signal": "def testFromBERNullKnownValues(self):\n", "code": "encoded=[0x05, 0x00]\nm=s(*encoded)\nresult, bytes = pureber.berDecodeObject(pureber.BERDecoderContext(), m)\nself.assertEquals(bytes, len(m))\nassert isinstance(result, pureber.BERNull)\nassert 0x05==result.tag", "path": "ldaptor\\test\\test_pureber.py", "repo_name": "antong/ldaptor", "stars": 39, "license": "lgpl-2.1", "language": "python", "size": 1668}
{"docstring": "\"\"\"BERNull(encoded=\"...\") with too short input should throw BERExceptionInsufficientData\"\"\"\n", "func_signal": "def testPartialBERNullEncodings(self):\n", "code": "m=str(pureber.BERNull())\nassert len(m)==2\nself.assertRaises(pureber.BERExceptionInsufficientData, pureber.berDecodeObject, pureber.BERDecoderContext(), m[:1])\nself.assertEquals((None, 0), pureber.berDecodeObject(pureber.BERDecoderContext(), ''))", "path": "ldaptor\\test\\test_pureber.py", "repo_name": "antong/ldaptor", "stars": 39, "license": "lgpl-2.1", "language": "python", "size": 1668}
{"docstring": "\"\"\"BERSequence(encoded=\"...\") should give known result with known input\"\"\"\n", "func_signal": "def testFromBERSequenceKnownValues(self):\n", "code": "for content, encoded in self.knownValues:\n    m=s(*encoded)\n    result, bytes = pureber.berDecodeObject(pureber.BERDecoderContext(), m)\n    self.assertEquals(bytes, len(m))\n    assert isinstance(result, pureber.BERSequence)\n    result = result.data\n    assert len(content)==len(result)\n    for i in xrange(len(content)):\n        assert content[i]==result[i]\n    assert content==result", "path": "ldaptor\\test\\test_pureber.py", "repo_name": "antong/ldaptor", "stars": 39, "license": "lgpl-2.1", "language": "python", "size": 1668}
{"docstring": "\"\"\"str(BERSequence(x)) should give known result with known input\"\"\"\n", "func_signal": "def testToBERSequenceKnownValues(self):\n", "code": "for content, encoded in self.knownValues:\n    result = pureber.BERSequence(content)\n    result = str(result)\n    result = map(ord, result)\n    assert encoded==result", "path": "ldaptor\\test\\test_pureber.py", "repo_name": "antong/ldaptor", "stars": 39, "license": "lgpl-2.1", "language": "python", "size": 1668}
{"docstring": "\"\"\"BEROctetString(encoded=\"...\") with too short input should throw BERExceptionInsufficientData\"\"\"\n", "func_signal": "def testPartialBEROctetStringEncodings(self):\n", "code": "m=str(pureber.BEROctetString(\"x\"))\nassert len(m)==3\nself.assertRaises(pureber.BERExceptionInsufficientData, pureber.berDecodeObject, pureber.BERDecoderContext(), m[:2])\nself.assertRaises(pureber.BERExceptionInsufficientData, pureber.berDecodeObject, pureber.BERDecoderContext(), m[:1])\nself.assertEquals((None, 0), pureber.berDecodeObject(pureber.BERDecoderContext(), ''))", "path": "ldaptor\\test\\test_pureber.py", "repo_name": "antong/ldaptor", "stars": 39, "license": "lgpl-2.1", "language": "python", "size": 1668}
{"docstring": "\"\"\"BER objects equal BER objects with same type and content\"\"\"\n", "func_signal": "def testBERBaseEquality(self):\n", "code": "for class_, args in self.valuesToTest:\n    x=class_(*args)\n    y=class_(*args)\n    assert x==x\n    assert x==y", "path": "ldaptor\\test\\test_pureber.py", "repo_name": "antong/ldaptor", "stars": 39, "license": "lgpl-2.1", "language": "python", "size": 1668}
{"docstring": "\"\"\"BERInteger(encoded=\"...\") with too short input should throw BERExceptionInsufficientData\"\"\"\n", "func_signal": "def testPartialBERIntegerEncodings(self):\n", "code": "m=str(pureber.BERInteger(42))\nassert len(m)==3\nself.assertRaises(pureber.BERExceptionInsufficientData, pureber.berDecodeObject, pureber.BERDecoderContext(), m[:2])\nself.assertRaises(pureber.BERExceptionInsufficientData, pureber.berDecodeObject, pureber.BERDecoderContext(), m[:1])\nself.assertEquals((None, 0), pureber.berDecodeObject(pureber.BERDecoderContext(), ''))", "path": "ldaptor\\test\\test_pureber.py", "repo_name": "antong/ldaptor", "stars": 39, "license": "lgpl-2.1", "language": "python", "size": 1668}
{"docstring": "\"\"\"BERBoolean(encoded=\"...\") should give known result with known input\"\"\"\n", "func_signal": "def testFromBERBooleanKnownValues(self):\n", "code": "for integer, encoded, canon in self.knownValues:\n    m=s(*encoded)\n    result, bytes = pureber.berDecodeObject(pureber.BERDecoderContext(), m)\n    self.assertEquals(bytes, len(m))\n    assert isinstance(result, pureber.BERBoolean)\n    result = result.value\n    assert result==canon", "path": "ldaptor\\test\\test_pureber.py", "repo_name": "antong/ldaptor", "stars": 39, "license": "lgpl-2.1", "language": "python", "size": 1668}
{"docstring": "\"\"\"BEREnumerated(encoded=\"...\") should give known result with known input\"\"\"\n", "func_signal": "def testFromBEREnumeratedKnownValues(self):\n", "code": "for integer, encoded in self.knownValues:\n    m=s(*encoded)\n    result, bytes = pureber.berDecodeObject(pureber.BERDecoderContext(), m)\n    self.assertEquals(bytes, len(m))\n    assert isinstance(result, pureber.BEREnumerated)\n    result = result.value\n    assert integer==result", "path": "ldaptor\\test\\test_pureber.py", "repo_name": "antong/ldaptor", "stars": 39, "license": "lgpl-2.1", "language": "python", "size": 1668}
{"docstring": "\"\"\"This can simulate the streaming calls through the net rule ports for testing\"\"\"\n", "func_signal": "def s(self, port, action, **kwargs):\n", "code": "if action == 'progressive':\n    for bytes in self.progressive(**kwargs):\n        yield bytes\nelif action == 'stream_ts':\n    for bytes in self.stream_ts(**kwargs):\n        yield bytes\nelse:\n    raise cherrypy.HTTPError(400, message=\"Invalid action specified\")", "path": "dripls\\main.py", "repo_name": "hulu/dripls", "stars": 39, "license": "None", "language": "python", "size": 107643}
{"docstring": "\"\"\" Replace a segment from the original playlist with a new segment \"\"\"\n\n", "func_signal": "def switch_segment(playlist_contents, old_segment, new_segment):\n", "code": "new_contents = playlist_contents.replace(old_segment, new_segment)\nreturn new_contents", "path": "dripls\\httpls_client.py", "repo_name": "hulu/dripls", "stars": 39, "license": "None", "language": "python", "size": 107643}
{"docstring": "\"\"\"Cache the segment and call the external shaper script\"\"\"\n\n", "func_signal": "def shape_hls_segment(segment, rule_action, mock_shape_segment=False, shape_session = {}):\n", "code": "sid = hashlib.sha224(conf.data.provider.normalize_segment_url(segment[\"url\"])).hexdigest()\n(traffic_limit, traffic_loss, cache_segment) = parse_net_rule_action(rule_action)\n\nport = get_shape_port_for(traffic_limit, traffic_loss, shape_session, mock_shape_segment)\n\nif cache_segment:\n    #cache the file if it hasn't been cached already\n    s_filename = \"{0}playlists/{1}.ts\".format(shaper_store_path, sid)\n    if (not os.path.exists(s_filename)):\n        logging.debug(\"Fetching segment {0} \".format(segment[\"url\"]))\n        segment_content = urllib2.urlopen(segment[\"url\"]).read()\n        logging.debug(\"Done\")\n\n        with open(s_filename, \"wb+\") as segment_file:\n            segment_file.write(segment_content)\n            segment_file.close()\n\n        with open(\"{0}.meta\".format(s_filename), \"wb+\") as metadata_file:\n            metadata_file.write(segment[\"url\"])\n            metadata_file.close()\n    else: \n        logging.debug(\"Segment cached {0} \".format(segment[\"url\"]))\n\n\n    #return the final url\n    return conf.common.get_final_url(\"s/{0}/playlists/{1}.ts\".format(port, sid), \"\" )\nelse:\n    return conf.common.get_final_url(\"s/{0}/stream.ts?url={1}\".format(port, urllib.quote_plus(segment[\"url\"])), \"\")", "path": "dripls\\shaper.py", "repo_name": "hulu/dripls", "stars": 39, "license": "None", "language": "python", "size": 107643}
{"docstring": "\"\"\" Pull a variant playlist's content \"\"\"\n\n", "func_signal": "def pull_variant_playlist(url):\n", "code": "variant_playlist = {}\nplaylist_response = urllib2.urlopen(url)\nvariant_playlist[\"url\"] = url\nvariant_playlist[\"content\"] = playlist_response.read()\nvariant_playlist[\"segments\"] = {}\n\ntry:\n    segment_counts = {}\n\n    ext = \"\"\n    for line in variant_playlist[\"content\"].splitlines():\n        if line.startswith(\"#EXT\"):\n            # replace key with fullpath key\n            if line.startswith(\"#EXT-X-KEY:\"):\n                orig_line = line\n                line = rewrite_ext_key(url, orig_line)\n\n                variant_playlist[\"key_ext\"] = line\n                variant_playlist[\"content\"] = variant_playlist[\"content\"].replace(orig_line, line)\n                \n        else:\n            type = conf.data.provider.get_segment_type(line)            \n       \n            if not \"segment\" in segment_counts:\n                segment_counts[\"segment\"] = 0\n            else:\n                segment_counts[\"segment\"] += 1\n  \n            if not type in segment_counts:\n                segment_counts[type] = 0\n            else:\n                segment_counts[type] += 1\n\n            variant_playlist[\"segments\"][segment_counts[\"segment\"]] = { \n               \"url\": urlparse.urljoin(url, line), \n               \"original_url\": line, \n               \"segment\": segment_counts[\"segment\"], \n               \"{0}_segment\".format(type): segment_counts[type],\n               \"type\":type,\n               \"ext\":ext\n            }\nexcept:\n    raise RuntimeError(\"Unable to parse playlist content at url: {0}\".format(url) )\n\nreturn variant_playlist", "path": "dripls\\httpls_client.py", "repo_name": "hulu/dripls", "stars": 39, "license": "None", "language": "python", "size": 107643}
{"docstring": "#shape port \n", "func_signal": "def get_shape_port_for(traffic_limit, traffic_loss, shape_session, mock_shape_segment = False):\n", "code": "key = \"{0}.{1}\".format(traffic_limit, traffic_loss) \nif shape_session.has_key(key):\n    return shape_session[key]\nelse:\n    port = get_next_shape_port()\n    call_ext_shape_port(port, traffic_limit, traffic_loss, mock_shape_segment)\n    shape_session[key] = port\n\n    return port", "path": "dripls\\shaper.py", "repo_name": "hulu/dripls", "stars": 39, "license": "None", "language": "python", "size": 107643}
{"docstring": "\"\"\"\n\nCall the external port shaper script to make sure that the desired rules are set for the port\n\nWarning: If executing user is not in sudoers, the operation will fail\n\n\"\"\"\n\n", "func_signal": "def call_ext_shape_port(port, traffic_limit, traffic_loss, mock_shape_segment):\n", "code": "shape_cmd = \"sudo {0} {1} {2} {3}\".format(conf.shaper_path, port, traffic_limit, traffic_loss) \nlogging.info(\"External shape call : {0} {1}\".format(port, shape_cmd))\n\nif not mock_shape_segment:\n    # execute non-interactive\n    p = subprocess.Popen([\"/usr/bin/sudo\", \"-n\", conf.shaper_path, str(port), str(traffic_limit), str(traffic_loss)], stdin=subprocess.PIPE)\n    p.wait()\n\n    if p.returncode != 0:\n        raise SystemError('Executing {0} failed with {1}'.format(conf.shaper_path, p.returncode))", "path": "dripls\\shaper.py", "repo_name": "hulu/dripls", "stars": 39, "license": "None", "language": "python", "size": 107643}
{"docstring": "\"\"\" Store all arguments recieved on the url as the associated tag \"\"\"\n", "func_signal": "def store_tag(self, cid, r, tag, kwargs):\n", "code": "tag_args = conf.data.provider.get_tag_kwargs(kwargs)\ntag_args[\"cid\"] = cid\n\nif r:\n    tag_args[\"r\"] = r\n\nwith open(\"{0}/playlists/tag_{1}\".format(shaper.shaper_store_path, tag), \"w\") as pf:\n    pf.write(\"{0}\".format(urllib.urlencode(tag_args)))", "path": "dripls\\main.py", "repo_name": "hulu/dripls", "stars": 39, "license": "None", "language": "python", "size": 107643}
{"docstring": "\"\"\"\n\nCheck if any rule matches the current segment. Generate the playlist/segment possible \nrule permutation and test for rule hit. Work from more specific rules to more generic rules.\nIf a rule is hit, no further checks will be made.\n\n\"\"\"\n\n", "func_signal": "def hls_segment_rule_match(rules, playlist, segment):\n", "code": "non_cdn_bandwidth_key = \"{0}k\".format(int(playlist[\"bandwidth\"]) / 1000)\ncdn_bandwidth_key = \"{0}.{1}\".format(playlist[\"cdn\"], non_cdn_bandwidth_key) \ncdn_wildcard_key = \"{0}.*\".format(playlist[\"cdn\"]) \nwildcard_key = \"*\"\n\nfor bandwidth_key in (cdn_bandwidth_key,non_cdn_bandwidth_key, cdn_wildcard_key, wildcard_key ):\n    check_rules = []\n\n    # playlist match \n    if (segment[\"type\"] == \"vplaylist\"):\n        if bandwidth_key in rules: \n            return rules[bandwidth_key]\n\n        if \"*\" in rules:\n            return rules[\"*\"]\n\n        continue\n\n    # handle case of specific type rule\n    segment_type = \"{0}_segment\".format(segment[\"type\"])\n    check_rules.append(\"{0}.{1}{2}\".format(bandwidth_key, segment[\"type\"][0], segment[segment_type])) \n    check_rules.append(\"{0}.{1}*\".format(bandwidth_key, segment[\"type\"][0])) \n\n    # handle case of general segment rule\n    check_rules.append(\"{0}.s{1}\".format(bandwidth_key, segment[\"segment\"])) \n    check_rules.append(\"{0}.s*\".format(bandwidth_key))\n    check_rules.append(\"{0}.*\".format(bandwidth_key))\n\n    for rule in check_rules:\n        if rule in rules: \n            rule_action = rules[rule].lower() \n\n            logging.debug(\"matched rule : {0} in segment: {1} \".format(rule, segment[\"url\"]))\n\n            return rule_action", "path": "dripls\\shaper.py", "repo_name": "hulu/dripls", "stars": 39, "license": "None", "language": "python", "size": 107643}
{"docstring": "\"\"\" Cache and rewrite a m3u8 \"\"\"\n\n", "func_signal": "def cache_info(self, cid=None, r=None, tag=None, **kwargs):\n", "code": "info = self.cache_stream(cid, r, tag, kwargs)\n\nreturn json.dumps(info, sort_keys=True, indent=4)", "path": "dripls\\main.py", "repo_name": "hulu/dripls", "stars": 39, "license": "None", "language": "python", "size": 107643}
{"docstring": "\"\"\" Endpoint for shaping a progressive stream\"\"\"\n", "func_signal": "def progressive(self, url, r=None, **kwargs):\n", "code": "stream_url = url\nif not kwargs.has_key(\"from_dripls\"):\n    # if we have already come from dripls, we will set from_dripls so we can bypass the rule matching since we are \n    # running through a net traffic rule\n    if r:\n        stream_url = self._handle_progressive_rules(stream_url, r, cherrypy.serving.request)\n\nfor bytes in self._stream_url(cherrypy.serving.request, stream_url):\n    yield bytes", "path": "dripls\\main.py", "repo_name": "hulu/dripls", "stars": 39, "license": "None", "language": "python", "size": 107643}
{"docstring": "\"\"\"Returns shaped m3u8 playlist\n\nProcess and shape a m3u8 playlist based on a set of rules \n\n\"\"\"\n\n", "func_signal": "def hls_cache_and_shape(master_playlist, seeded_content_id, rules, master_playlist_url = ''):\n", "code": "shape_info = {}\nshape_info[\"id\"] = seeded_content_id\nshape_info[\"variants\"] = {}\nshape_port_session = {}\n\nvariant_playlists = httpls_client.get_variant_playlist_urls(master_playlist, master_playlist_url)\n\nfor bitrate in variant_playlists.iterkeys():\n    for alt in variant_playlists[bitrate].iterkeys():\n        variant_playlist_desc = variant_playlists[bitrate][alt]\n        variant_playlist = httpls_client.pull_variant_playlist( variant_playlist_desc[\"url\"])\n\n        # perform rewrite on the variant playlist url to local url or a rule matched url \n        seg_rewrite_url = hls_segment_rule_rewrite(rules, variant_playlist_desc, variant_playlist_desc, shape_port_session)\n        local_rewrite_url = conf.common.get_final_url(\"playlist.m3u8\",\"p=m_{0}_{1}_{2}\".format(seeded_content_id, bitrate, alt))\n        shape_info[\"variants\"][\"{0}_{1}\".format(bitrate, alt)] = \"{0}_{1}_{2}\".format(seeded_content_id, bitrate, alt)\n        master_playlist = httpls_client.switch_segment( master_playlist, variant_playlist_desc[\"original_url\"], seg_rewrite_url if seg_rewrite_url else local_rewrite_url )\n\n        # don't process a playlist if it hit a rule (ie has been errored out)\n        if seg_rewrite_url:\n            shape_info[\"{0}.{1}\".format(bitrate, alt)] = seg_rewrite_url\n            continue\n\n        # perform rule rewrite on segments within the variant playlist  \n        for s in variant_playlist[\"segments\"].iterkeys():\n            seg_rewrite_url = hls_segment_rule_rewrite(rules, variant_playlist_desc, variant_playlist[\"segments\"][s], shape_port_session)\n\n            # rewrite local to full url playlist\n            if variant_playlist[\"segments\"][s][\"original_url\"] != variant_playlist[\"segments\"][s][\"url\"]:\n                variant_playlist[\"content\"] = httpls_client.switch_segment(variant_playlist[\"content\"], variant_playlist[\"segments\"][s][\"original_url\"], variant_playlist[\"segments\"][s][\"url\"])\n\n            # replace segment with shaped url\n            if seg_rewrite_url:\n                variant_playlist[\"content\"] = httpls_client.switch_segment(variant_playlist[\"content\"], variant_playlist[\"segments\"][s][\"url\"], seg_rewrite_url)\n                variant_playlist[\"segments\"][s][\"url\"] = seg_rewrite_url\n                shape_info[\"{0}.{1}.s{2}\".format( bitrate, alt, s)] = seg_rewrite_url\n\n        httpls_client.store_playlist(variant_playlist[\"content\"], shaper_store_path + \"playlists/m_{0}_{1}_{2}.m3u8\".format(seeded_content_id, bitrate, alt))\n\n\nshape_port_session = {}\nhttpls_client.store_playlist(master_playlist, shaper_store_path + \"playlists/m_{0}.m3u8\".format(seeded_content_id))\nreturn shape_info", "path": "dripls\\shaper.py", "repo_name": "hulu/dripls", "stars": 39, "license": "None", "language": "python", "size": 107643}
{"docstring": "\"\"\"\nSet local environment\n\"\"\"\n", "func_signal": "def dev():\n", "code": "env.hosts = ['localhost']\nenv.path = '/tmp/{0}'.format(env.project_name)\nenv.env = 'dev'\n\n# The proxy behind which dripls is running\nenv.proxy_host = 'http://localhost:8080'", "path": "dripls\\deploy\\fabfile.py", "repo_name": "hulu/dripls", "stars": 39, "license": "None", "language": "python", "size": 107643}
{"docstring": "\"\"\"\n\nGiven a set of rules and a segment in a playlist, find out whether the \nsegment is matched in any of the rules and if so perform the rule action\n\nPossible rule actions are e - raise HTTP error, net - traffic shape\n\n\"\"\"\n# perform rule matching\n", "func_signal": "def hls_segment_rule_rewrite(rules, playlist, segment, shape_session, mock_shape_segment=False):\n", "code": "rule_action = hls_segment_rule_match(rules,playlist, segment)\n\n# no rule match    \nif not rule_action:\n    return None\n\n# generate error pages if a match found  \nif rule_action.startswith(\"e\"):\n    return generate_status(rule_action[1:]) \n\nif rule_action.startswith(\"net\"):\n    return shape_hls_segment(segment, rule_action, mock_shape_segment=mock_shape_segment, shape_session = shape_session)\n\nraise ValueError( \"Cannot match action against appropriate set of actions : {0}\".format(rule_action))", "path": "dripls\\shaper.py", "repo_name": "hulu/dripls", "stars": 39, "license": "None", "language": "python", "size": 107643}
{"docstring": "\"\"\" Extract variant playlists from the main playlist \"\"\"\n\n", "func_signal": "def get_variant_playlist_urls(main_playlist, master_playlist_url = '' ):\n", "code": "variant_uris = {}\nbandwidth = None\next = \"\"\n\ntry:\n    for line in main_playlist.splitlines():\n        line = line.strip()\n        if len(line) > 0:\n           if line.startswith(\"#EXT\"):\n               ext = line\n               for arg in line.split(\",\"):\n                   if arg.strip().startswith(\"BANDWIDTH\"):\n                      bandwidth = arg.split(\"=\")[1].strip()\n           else:\n               if not bandwidth:\n                   continue\n\n               if not bandwidth in variant_uris: \n                   variant_uris[str(bandwidth)] = {}\n           \n               variant_uris[str(bandwidth)][conf.data.provider.get_cdn_from_playlist_url(line)] = {\n                   \"url\": urlparse.urljoin(master_playlist_url, line), \n                   \"original_url\" : line, \n                   \"type\":\"vplaylist\", \n                   \"bandwidth\":bandwidth, \n                   \"cdn\" : conf.data.provider.get_cdn_from_playlist_url(line), \n                   \"ext\":ext}\nexcept:\n    raise ValueError(\"Unable to parse master playlist's content\")\n\nreturn variant_uris", "path": "dripls\\httpls_client.py", "repo_name": "hulu/dripls", "stars": 39, "license": "None", "language": "python", "size": 107643}
{"docstring": "\"\"\" \n\nParse the rule action. Net rule consists of two parts: bandwidth, and packet loss. \nFormat is net<speed>.loss<%packetloss>. Speed is assumed to be in kbs and loss is\nin percentages.\n\n\"\"\"\n\n# Default to traffic limit exceeding any practical bandwith limitations\n# Useful for scenarios where only packet loss is provided \n", "func_signal": "def parse_net_rule_action(rule_action):\n", "code": "traffic_limit = 100000\ncache = True\ntraffic_loss = 0\nfor netrule in rule_action.split(\".\"):\n    if netrule.startswith(\"netcache\"):\n        traffic_limit = int(netrule[8:])\n        cache = True\n    elif netrule.startswith(\"net\"):\n        traffic_limit = int(netrule[3:])\n        cache = False\n\n\n    if netrule.startswith(\"loss\"):\n        traffic_loss = int(netrule[4:].replace(\"%\",\"\"))\n\nreturn (traffic_limit, traffic_loss, cache)", "path": "dripls\\shaper.py", "repo_name": "hulu/dripls", "stars": 39, "license": "None", "language": "python", "size": 107643}
{"docstring": "\"\"\" Stream back a cached playlist \"\"\"\n\n", "func_signal": "def playlist_m3u8(self, p=None, **kwargs):\n", "code": "with open(\"{0}/playlists/{1}.m3u8\".format(shaper.shaper_store_path,p), \"r\") as pf:\n    playlist_content = pf.read()\n\ncherrypy.response.headers['Content-Type'] = \"application/vnd.apple.mpegurl\"\ncherrypy.response.headers['Content-Disposition'] = \"inline; filename={0}.m3u8\".format(p)\ncherrypy.response.headers['Last-Modified'] = httputil.HTTPDate()\n\nreturn playlist_content", "path": "dripls\\main.py", "repo_name": "hulu/dripls", "stars": 39, "license": "None", "language": "python", "size": 107643}
{"docstring": "\"\"\" Update a previously shaped segment on the fly \"\"\"\n\n", "func_signal": "def updatesegment(self, url, new_action):\n", "code": "shaper.update_shaped_segment(url, new_action)\n\nreturn \"OK\"", "path": "dripls\\main.py", "repo_name": "hulu/dripls", "stars": 39, "license": "None", "language": "python", "size": 107643}
{"docstring": "\"\"\" Perform the actual caching and shaping  of the stream \"\"\"\n\n\n", "func_signal": "def cache_stream(self, cid=None, r=None, tag=None, kwargs=None):\n", "code": "seeded_content_id = conf.common.get_seeded_cid(cid)\nmaster_playlist_url = conf.data.provider.master_m3u8_url(cid, kwargs)\nmaster_playlist = conf.data.provider.pull_master_m3u8(cid, kwargs)\nvarient_playlists = httpls_client.get_variant_playlist_urls(master_playlist, master_playlist_url)\n\nrules = shaper.parse_hls_rules(r, varient_playlists)\n\ninfo = shaper.hls_cache_and_shape(master_playlist, seeded_content_id, rules, master_playlist_url)\ninfo[\"url\"] = conf.common.get_final_url(\"playlist.m3u8\",\"p=m_{0}\".format(seeded_content_id))\n\n# if we have a tag, store\nif tag:\n    self.store_tag(cid, r, tag, kwargs)\n\nreturn info", "path": "dripls\\main.py", "repo_name": "hulu/dripls", "stars": 39, "license": "None", "language": "python", "size": 107643}
{"docstring": "\"\"\" Stream a ts from original location \"\"\"\n\n", "func_signal": "def stream_ts(self, p=None, **kwargs):\n", "code": "serving_request = cherrypy.serving.request\nif serving_request.protocol >= (1, 1):\n    r = httputil.get_ranges(serving_request.headers.get('Range'), sys.maxint)\n\n    # TODO : do something with range request if need be\n       \nfor bytes in self._stream_url(serving_request, kwargs.get(\"url\")):\n    yield bytes", "path": "dripls\\main.py", "repo_name": "hulu/dripls", "stars": 39, "license": "None", "language": "python", "size": 107643}
{"docstring": "\"\"\" Do the shaping off of a pre-made tag \"\"\"\n\n", "func_signal": "def tag_m3u8(self, tag=None, **kwargs):\n", "code": "with open(\"{0}/playlists/tag_{1}\".format(shaper.shaper_store_path, tag) , \"r\") as pf: \n    tag_qs = pf.read()\n\n#add old keys\ntag_args = urlparse.parse_qs(tag_qs)\nfor key in tag_args:\n    tag_args[key] = tag_args[key][0]\n\n#add any new kes that we might have gotten(some might override old keys)\nfor key in kwargs:\n    tag_args[key] = kwargs[key]\n  \n#run master with the tag args\nreturn self.master_m3u8(**tag_args);", "path": "dripls\\main.py", "repo_name": "hulu/dripls", "stars": 39, "license": "None", "language": "python", "size": 107643}
{"docstring": "\"\"\"generate(bits:int, randfunc:callable, progress_func:callable)\n\nGenerate an ElGamal key of length 'bits', using 'randfunc' to get\nrandom data and 'progress_func', if present, to display\nthe progress of the key generation.\n\"\"\"\n", "func_signal": "def generate(bits, randfunc, progress_func=None):\n", "code": "obj=ElGamalobj()\n# Generate prime p\nif progress_func:\n    progress_func('p\\n')\nobj.p=bignum(getPrime(bits, randfunc))\n# Generate random number g\nif progress_func:\n    progress_func('g\\n')\nsize=bits-1-(ord(randfunc(1)) & 63) # g will be from 1--64 bits smaller than p\nif size<1:\n    size=bits-1\nwhile (1):\n    obj.g=bignum(getPrime(size, randfunc))\n    if obj.g < obj.p:\n        break\n    size=(size+1) % bits\n    if size==0:\n        size=4\n# Generate random number x\nif progress_func:\n    progress_func('x\\n')\nwhile (1):\n    size=bits-1-ord(randfunc(1)) # x will be from 1 to 256 bits smaller than p\n    if size>2:\n        break\nwhile (1):\n    obj.x=bignum(getPrime(size, randfunc))\n    if obj.x < obj.p:\n        break\n    size = (size+1) % bits\n    if size==0:\n        size=4\nif progress_func:\n    progress_func('y\\n')\nobj.y = pow(obj.g, obj.x, obj.p)\nreturn obj", "path": "crx-appengine-packager\\Crypto\\PublicKey\\ElGamal.py", "repo_name": "kurrik/chrome-extensions", "stars": 57, "license": "bsd-3-clause", "language": "python", "size": 1209}
{"docstring": "\"\"\"Returns true if argument OID resides deeper in the OID tree\"\"\"\n", "func_signal": "def isPrefixOf(self, value):\n", "code": "l = len(self._value)\nif l <= len(value):\n    if self._value[:l] == value[:l]:\n        return 1\nreturn 0", "path": "crx-appengine-packager\\pyasn1\\type\\univ.py", "repo_name": "kurrik/chrome-extensions", "stars": 57, "license": "bsd-3-clause", "language": "python", "size": 1209}
{"docstring": "\"\"\" Converts a long into the bit string. \"\"\"\n", "func_signal": "def toBitString_(self, num):\n", "code": "buf = ''\nwhile num > 1:\n  buf = str(num & 1) + buf\n  num = num >> 1\nbuf = str(num) + buf\nreturn buf", "path": "crx-appengine-packager\\crx.py", "repo_name": "kurrik/chrome-extensions", "stars": 57, "license": "bsd-3-clause", "language": "python", "size": 1209}
{"docstring": "\"\"\"Return a Boolean denoting whether the object contains\nprivate components.\"\"\"\n", "func_signal": "def has_private(self):\n", "code": "if hasattr(self, 'x'):\n    return 1\nelse:\n    return 0", "path": "crx-appengine-packager\\Crypto\\PublicKey\\ElGamal.py", "repo_name": "kurrik/chrome-extensions", "stars": 57, "license": "bsd-3-clause", "language": "python", "size": 1209}
{"docstring": "# Adds a bit of noise to the random pool, by adding in the\n# current time and CPU usage of this process.\n# The difference from the previous call to _noise() is taken\n# in an effort to estimate the entropy.\n", "func_signal": "def _noise(self):\n", "code": "t=time.time()\ndelta = (t - self._lastcounter)/self._ticksize*1e6\nself._lastcounter = t\nself._addBytes(long_to_bytes(long(1000*time.time())))\nself._addBytes(long_to_bytes(long(1000*time.clock())))\nself._addBytes(long_to_bytes(long(1000*time.time())))\nself._addBytes(long_to_bytes(long(delta)))\n\n# Reduce delta to a maximum of 8 bits so we don't add too much\n# entropy as a result of this call.\ndelta=delta % 0xff\nreturn int(delta)", "path": "crx-appengine-packager\\Crypto\\Util\\randpool.py", "repo_name": "kurrik/chrome-extensions", "stars": 57, "license": "bsd-3-clause", "language": "python", "size": 1209}
{"docstring": "\"\"\"On unpickling a key object, the key data is converted to the big\nnumber representation being used, whether that is Python long\nintegers, MPZ objects, or whatever.\"\"\"\n", "func_signal": "def __setstate__(self, d):\n", "code": "for key in self.keydata:\n    if d.has_key(key): self.__dict__[key]=bignum(d[key])", "path": "crx-appengine-packager\\Crypto\\build\\lib.macosx-10.5-i386-2.5\\Crypto\\PublicKey\\pubkey.py", "repo_name": "kurrik/chrome-extensions", "stars": 57, "license": "bsd-3-clause", "language": "python", "size": 1209}
{"docstring": "\"\"\"getPrime(N:int, randfunc:callable):long\nReturn a random N-bit prime number.\n\"\"\"\n\n", "func_signal": "def getPrime(N, randfunc):\n", "code": "number=getRandomNumber(N, randfunc) | 1\nwhile (not isPrime(number)):\n    number=number+2\nreturn number", "path": "crx-appengine-packager\\Crypto\\Util\\number.py", "repo_name": "kurrik/chrome-extensions", "stars": 57, "license": "bsd-3-clause", "language": "python", "size": 1209}
{"docstring": "\"\"\"GCD(x:long, y:long): long\nReturn the GCD of x and y.\n\"\"\"\n", "func_signal": "def GCD(x,y):\n", "code": "x = abs(x) ; y = abs(y)\nwhile x > 0:\n    x, y = y % x, x\nreturn y", "path": "crx-appengine-packager\\Crypto\\Util\\number.py", "repo_name": "kurrik/chrome-extensions", "stars": 57, "license": "bsd-3-clause", "language": "python", "size": 1209}
{"docstring": "\"\"\" Packages a zip file into a CRX, given a signing key. \"\"\"\n# Obtain the hash of the zip file contents\n", "func_signal": "def package(self, zip_string, key):\n", "code": "zip_hash = hashlib.sha1(zip_string).digest()\n\n# Get the SHA1 AlgorithmIdentifier\nsha1identifier = univ.ObjectIdentifier('1.3.14.3.2.26')\nsha1info = univ.Sequence()\nsha1info.setComponentByPosition(0, sha1identifier)\nsha1info.setComponentByPosition(1, univ.Null(''))\n\n# Get the DigestInfo sequence, composed of the SHA1 id and the zip hash\ndigestinfo = univ.Sequence()\ndigestinfo.setComponentByPosition(0, sha1info)\ndigestinfo.setComponentByPosition(1, univ.OctetString(zip_hash))\n\n# Encode the sequence into ASN.1\ndigest = encoder.encode(digestinfo)\n\n# Pad the hash\npaddinglength = 128 - 3 - len(digest)\npaddedhexstr = \"0001%s00%s\" % (paddinglength * 'ff', digest.encode('hex'))\n\n# Calculate the signature\nsignature_bytes = key.getRSAKey().sign(paddedhexstr.decode('hex'), \"\")[0]\nsignature = ('%X' % signature_bytes).decode('hex')\n\n# Get the public key\npublickey = key.getRSAPublicKey()\n\n# Write the actual CRX contents\ncrx_buffer = StringIO.StringIO(\"wb\")\ncrx_buffer.write(\"Cr24\")  # Extension file magic number, from the CRX focs\ncrx_buffer.write(struct.pack('iii', 2, len(publickey), len(signature)))\ncrx_buffer.write(publickey)\ncrx_buffer.write(signature)\ncrx_buffer.write(zip_string)\ncrx_file = crx_buffer.getvalue()\nreturn crx_file", "path": "crx-appengine-packager\\crx.py", "repo_name": "kurrik/chrome-extensions", "stars": 57, "license": "bsd-3-clause", "language": "python", "size": 1209}
{"docstring": "\"\"\" Returns a signing key from the data store or creates one if it doesn't\nalready exist. \"\"\"\n# See if there's already a key in the datastore\n", "func_signal": "def getOrCreate():\n", "code": "key = SigningKey.get_by_key_name('signingkey')\nif not key:\n  # Create one if not\n  rsakey = RSA.generate(1024, os.urandom)\n  key = SigningKey(key_name='signingkey', blob=pickle.dumps(rsakey))\n  # Store it for use later\n  key.put()\nreturn key", "path": "crx-appengine-packager\\crx.py", "repo_name": "kurrik/chrome-extensions", "stars": 57, "license": "bsd-3-clause", "language": "python", "size": 1209}
{"docstring": "\"\"\"To keep key objects platform-independent, the key data is\nconverted to standard Python long integers before being\nwritten out.  It will then be reconverted as necessary on\nrestoration.\"\"\"\n", "func_signal": "def __getstate__(self):\n", "code": "d=self.__dict__\nfor key in self.keydata:\n    if d.has_key(key): d[key]=long(d[key])\nreturn d", "path": "crx-appengine-packager\\Crypto\\build\\lib.macosx-10.5-i386-2.5\\Crypto\\PublicKey\\pubkey.py", "repo_name": "kurrik/chrome-extensions", "stars": 57, "license": "bsd-3-clause", "language": "python", "size": 1209}
{"docstring": "\"\"\"verify(M:string|long, signature:tuple) : bool\nVerify that the signature is valid for the message M;\nreturns true if the signature checks out.\n\"\"\"\n", "func_signal": "def verify (self, M, signature):\n", "code": "if isinstance(M, types.StringType): M=bytes_to_long(M)\nreturn self._verify(M, signature)", "path": "crx-appengine-packager\\Crypto\\build\\lib.macosx-10.5-i386-2.5\\Crypto\\PublicKey\\pubkey.py", "repo_name": "kurrik/chrome-extensions", "stars": 57, "license": "bsd-3-clause", "language": "python", "size": 1209}
{"docstring": "\"\"\"get_bytes(N:int) : string\nReturn N bytes of random data.\n\"\"\"\n\n", "func_signal": "def get_bytes (self, N):\n", "code": "s=''\ni, pool = self._getPos, self._randpool\nh=self._hash.new()\ndsize = self._hash.digest_size\nnum = N\nwhile num > 0:\n    h.update( self._randpool[i:i+dsize] )\n    s = s + h.digest()\n    num = num - dsize\n    i = (i + dsize) % self.bytes\n    if i<dsize:\n        self.stir()\n        i=self._getPos\n\nself._getPos = i\nself._updateEntropyEstimate(- 8*N)\nreturn s[:N]", "path": "crx-appengine-packager\\Crypto\\Util\\randpool.py", "repo_name": "kurrik/chrome-extensions", "stars": 57, "license": "bsd-3-clause", "language": "python", "size": 1209}
{"docstring": "\"\"\" Gets an ASN.1-encoded form of this RSA key's public key. \"\"\"\n# Get a RSAPublicKey structure\n", "func_signal": "def getRSAPublicKey(self):\n", "code": "pkinfo = univ.Sequence()\nrsakey = self.getRSAKey()\npkinfo.setComponentByPosition(0, univ.Integer(rsakey.n))\npkinfo.setComponentByPosition(1, univ.Integer(rsakey.e))\n\n# Encode the public key info as a bit string\npklong = long(encoder.encode(pkinfo).encode('hex'), 16)\npkbitstring = univ.BitString(\"'00%s'B\" % self.toBitString_(pklong))\n\n# Get the rsaEncryption identifier:\nidrsaencryption = univ.ObjectIdentifier('1.2.840.113549.1.1.1')\n\n# Get the AlgorithmIdentifier for rsaEncryption\nidinfo = univ.Sequence()\nidinfo.setComponentByPosition(0, idrsaencryption)\nidinfo.setComponentByPosition(1, univ.Null(''))\n\n# Get the SubjectPublicKeyInfo structure\npublickeyinfo = univ.Sequence()\npublickeyinfo.setComponentByPosition(0, idinfo)\npublickeyinfo.setComponentByPosition(1, pkbitstring)\n\n# Encode the public key structure\npublickey = encoder.encode(publickeyinfo)\nreturn publickey", "path": "crx-appengine-packager\\crx.py", "repo_name": "kurrik/chrome-extensions", "stars": 57, "license": "bsd-3-clause", "language": "python", "size": 1209}
{"docstring": "\"\"\"unblind(M : string|long, B : string|long) : string|long\nUnblind message M using blinding factor B.\n\"\"\"\n", "func_signal": "def unblind(self, M, B):\n", "code": "wasString=0\nif isinstance(M, types.StringType):\n    M=bytes_to_long(M) ; wasString=1\nif isinstance(B, types.StringType): B=bytes_to_long(B)\nunblindedmessage=self._unblind(M, B)\nif wasString: return long_to_bytes(unblindedmessage)\nelse: return unblindedmessage", "path": "crx-appengine-packager\\Crypto\\build\\lib.macosx-10.5-i386-2.5\\Crypto\\PublicKey\\pubkey.py", "repo_name": "kurrik/chrome-extensions", "stars": 57, "license": "bsd-3-clause", "language": "python", "size": 1209}
{"docstring": "\"\"\"decrypt(ciphertext:tuple|string|long): string\nDecrypt 'ciphertext' using this key.\n\"\"\"\n", "func_signal": "def decrypt(self, ciphertext):\n", "code": "wasString=0\nif not isinstance(ciphertext, types.TupleType):\n    ciphertext=(ciphertext,)\nif isinstance(ciphertext[0], types.StringType):\n    ciphertext=tuple(map(bytes_to_long, ciphertext)) ; wasString=1\nplaintext=self._decrypt(ciphertext)\nif wasString: return long_to_bytes(plaintext)\nelse: return plaintext", "path": "crx-appengine-packager\\Crypto\\build\\lib.macosx-10.5-i386-2.5\\Crypto\\PublicKey\\pubkey.py", "repo_name": "kurrik/chrome-extensions", "stars": 57, "license": "bsd-3-clause", "language": "python", "size": 1209}
{"docstring": "\"\"\"Tuple of numerics -> dotted string OID converter\"\"\"\n", "func_signal": "def prettyOut(self, value):\n", "code": "r = []\nfor subOid in value:\n    r.append(str(subOid))\n    if r[-1] and r[-1][-1] == 'L':\n        r[-1][-1] = r[-1][:-1]\nreturn string.join(r, '.')", "path": "crx-appengine-packager\\pyasn1\\type\\univ.py", "repo_name": "kurrik/chrome-extensions", "stars": 57, "license": "bsd-3-clause", "language": "python", "size": 1209}
{"docstring": "\"\"\"encrypt(plaintext:string|long, K:string|long) : tuple\nEncrypt the string or integer plaintext.  K is a random\nparameter required by some algorithms.\n\"\"\"\n", "func_signal": "def encrypt(self, plaintext, K):\n", "code": "wasString=0\nif isinstance(plaintext, types.StringType):\n    plaintext=bytes_to_long(plaintext) ; wasString=1\nif isinstance(K, types.StringType):\n    K=bytes_to_long(K)\nciphertext=self._encrypt(plaintext, K)\nif wasString: return tuple(map(long_to_bytes, ciphertext))\nelse: return ciphertext", "path": "crx-appengine-packager\\Crypto\\build\\lib.macosx-10.5-i386-2.5\\Crypto\\PublicKey\\pubkey.py", "repo_name": "kurrik/chrome-extensions", "stars": 57, "license": "bsd-3-clause", "language": "python", "size": 1209}
{"docstring": "\"\"\"add_event(s:string)\nAdd an event to the random pool.  The current time is stored\nbetween calls and used to estimate the entropy.  The optional\n's' parameter is a string that will also be XORed into the pool.\nReturns the estimated number of additional bits of entropy gain.\n\"\"\"\n", "func_signal": "def add_event(self, s=''):\n", "code": "event = time.time()*1000\ndelta = self._noise()\ns = (s + long_to_bytes(event) +\n     4*chr(0xaa) + long_to_bytes(delta) )\nself._addBytes(s)\nif event==self._event1 and event==self._event2:\n    # If events are coming too closely together, assume there's\n    # no effective entropy being added.\n    bits=0\nelse:\n    # Count the number of bits in delta, and assume that's the entropy.\n    bits=0\n    while delta:\n        delta, bits = delta>>1, bits+1\n    if bits>8: bits=8\n\nself._event1, self._event2 = event, self._event1\n\nself._updateEntropyEstimate(bits)\nreturn bits", "path": "crx-appengine-packager\\Crypto\\Util\\randpool.py", "repo_name": "kurrik/chrome-extensions", "stars": 57, "license": "bsd-3-clause", "language": "python", "size": 1209}
{"docstring": "\"\"\"blind(M : string|long, B : string|long) : string|long\nBlind message M using blinding factor B.\n\"\"\"\n", "func_signal": "def blind(self, M, B):\n", "code": "wasString=0\nif isinstance(M, types.StringType):\n    M=bytes_to_long(M) ; wasString=1\nif isinstance(B, types.StringType): B=bytes_to_long(B)\nblindedmessage=self._blind(M, B)\nif wasString: return long_to_bytes(blindedmessage)\nelse: return blindedmessage", "path": "crx-appengine-packager\\Crypto\\build\\lib.macosx-10.5-i386-2.5\\Crypto\\PublicKey\\pubkey.py", "repo_name": "kurrik/chrome-extensions", "stars": 57, "license": "bsd-3-clause", "language": "python", "size": 1209}
{"docstring": "\"\"\"Get the robots segment of the head\"\"\"\n", "func_signal": "def get_robots_meta(self,head):\n", "code": "\n\"\"\"get the base link meta of the head\"\"\"\ntry:\n    r=re.compile(\"<base +href *= *[\\\"']?([^<>'\\\"]+)[\\\"']?\",re.M|re.I)\n    matches=r.findall(head)\n    self[\"base\"]=matches.pop()\nexcept:\n    pass", "path": "hyer\\document.py", "repo_name": "xurenlu/hyer", "stars": 38, "license": "other", "language": "python", "size": 1973}
{"docstring": "\"\"\"\nhandle signals\n\"\"\"\n", "func_signal": "def handler(signum, frame):\n", "code": "sig_exit()\nif signum == 3:\n    sig_exit()\nif signum == 2:\n    sig_exit()\nif signum == 9:\n    sig_exit()\n    return None", "path": "newspider.py", "repo_name": "xurenlu/hyer", "stars": 38, "license": "other", "language": "python", "size": 1973}
{"docstring": "''' '''\n", "func_signal": "def __init__(self,agent='Mozilla/Firefox 3.1'):\n", "code": "self.rules={}\nself.agent=agent", "path": "hyer\\rules_monster.py", "repo_name": "xurenlu/hyer", "stars": 38, "license": "other", "language": "python", "size": 1973}
{"docstring": "\"\"\"\u9e1f\u67aa\u6362\u70ae\u4e86,\u7528chardet\u6765\u63a2\u6d4b\u5f53\u524d\u6587\u6863\u7684encoding ,\u5e76\u81ea\u52a8\u6362\u4e3aUTF-8\"\"\"\n", "func_signal": "def get_charset(self,data,url):\n", "code": "charset=GET_CHARSET_METHOD(data,url)\nif charset==None :\n    return \n\ncharset=charset.upper()\nif charset==\"ASCII\":\n    self[\"charset\"]=\"UTF-8\"\n    pass\nelif charset!=\"UTF-8\":\n    if charset in [\"UTF-8\",\"UTF8\",\"UTF-16\",\"GBK\",\"GB2312\",\"GB18030\"]:\n        self[\"content\"]=data.decode(charset,\"ignore\").encode(\"UTF-8\")\n        self[\"charset\"]=\"UTF-8\"\nelse:\n    self[\"charset\"]=charset", "path": "hyer\\document.py", "repo_name": "xurenlu/hyer", "stars": 38, "license": "other", "language": "python", "size": 1973}
{"docstring": "\"\"\"get the html between <body> and </body> tags\"\"\"\n", "func_signal": "def get_body_data(self,html):\n", "code": "try:\n    r=re.compile(\"<body.*?>(.*?)</body>\",re.M|re.S)\n    matches=r.findall(html)\n    self[\"body\"]=matches.pop()\nexcept:\n    pass", "path": "hyer\\document.py", "repo_name": "xurenlu/hyer", "stars": 38, "license": "other", "language": "python", "size": 1973}
{"docstring": "\"\"\"return all links in the html content\nhave not been validated \"\"\"\n", "func_signal": "def scan_links(self,content):\n", "code": "hrefs=[]\nfor r in self.href_regexs:\n    reg=re.compile(r)\n    links=reg.findall(content)\n    hrefs.extend([l[0] for l in links if l !=\"\"])\nuniq_hrefs=list(set(hrefs))\nself['links']=uniq_hrefs", "path": "hyer\\document.py", "repo_name": "xurenlu/hyer", "stars": 38, "license": "other", "language": "python", "size": 1973}
{"docstring": "\"\"\"return <head>*</head> data in the html content\"\"\"\n", "func_signal": "def get_head_data(self,content):\n", "code": "try:\n    r=re.compile(r'<head[^>]*>(.*?)<\\/head>',re.MULTILINE|re.S)\n    heads=r.findall(content)\n    last_head=heads.pop()\n    self[\"head_data\"]=last_head\n    return last_head\nexcept:\n    pass", "path": "hyer\\document.py", "repo_name": "xurenlu/hyer", "stars": 38, "license": "other", "language": "python", "size": 1973}
{"docstring": "\"\"\"\nCount the number of html tags and the length of pure text information in a\nsoup entity.\nReturn: {'self': (tag density, number of tags, length of pure text, soup object), \n'child': list of count dics for child entities }\n\"\"\"\n", "func_signal": "def calcDensity(soup):\n", "code": "uni = unicode(soup)\nif isinstance(soup, BeautifulSoup.NavigableString):\n\tif uni.startswith(\"<!--\"):\n\t\treturn {'self': (0.0, 0, 0, soup)}\n\treturn {'self': (0.0, 0, len(uni), soup)}\nif soup.name in (\"script\", \"style\"):\n\treturn {'self': (1.0, 0, 0, BeautifulSoup.BeautifulSoup(\"\"))}\ncountTagNo = 1   # This is the current tag.\ncountTextLen = 0\ndicList = []\nfor content in soup.contents:\n\tdic = calcDensity(content)\n\tdicList.append(dic)\n\ttagNo, textLen = dic['self'][1:3]\n\tcountTagNo += tagNo\n\tcountTextLen += textLen\ndensity = countTextLen != 0 and float(countTagNo) / countTextLen or 1.0\nreturn {'self': (density, countTagNo, countTextLen, soup), 'child': dicList}", "path": "hyer\\vendor\\TextExtract.py", "repo_name": "xurenlu/hyer", "stars": 38, "license": "other", "language": "python", "size": 1973}
{"docstring": "\"\"\"get the charset of document from the HTML head segment \"\"\"\n", "func_signal": "def get_charset_meta(self,head):\n", "code": "try:\n    r=re.compile(\"<meta +http\\-equiv*=[\\\"']?Content-Type[\\\"']? *content=[\\\"']?([^<>'\\\"]+)[\\\"']?\",re.M|re.S)\n    matches=r.findall(head)\n    charset_str=matches.pop().split(\"=\").pop()\n    self[\"charset\"]=charset_str\n    return charset_str\nexcept:\n    pass", "path": "hyer\\document.py", "repo_name": "xurenlu/hyer", "stars": 38, "license": "other", "language": "python", "size": 1973}
{"docstring": "\"\"\"remove html tags\"\"\"\n", "func_signal": "def _html2text(self,text):\n", "code": "try:\n    r=re.compile(\"<script[^>]*>.*?</script>\",re.I|re.S)\n    text=r.sub(\"\",text)\nexcept:\n    pass\n\ntry:\n    r=re.compile(\"<style[^>]*>.*?</style>\",re.M|re.I|re.S)\n    text=r.sub(\"\",text)\nexcept:\n    pass\n\n\ntry:\n    r=re.compile(\"<.*?>\",re.M|re.S)\n    text=r.sub('',text)\n    return text\nexcept:\n    pass", "path": "hyer\\document.py", "repo_name": "xurenlu/hyer", "stars": 38, "license": "other", "language": "python", "size": 1973}
{"docstring": "\"\"\"return if the page is mostly text\"\"\"\n", "func_signal": "def is_text_page(self,html):\n", "code": "html_len=len(self[\"content\"])\nr=re.compile(\"<a[^>]*>.*?</a>\",re.M|re.I|re.S)\nunlinked_text=r.sub(\"\",self[\"content\"])\nunlinked_text=self._html2text(unlinked_text)\nwords=len(unlinked_text)\nreturn words/html_len", "path": "hyer\\document.py", "repo_name": "xurenlu/hyer", "stars": 38, "license": "other", "language": "python", "size": 1973}
{"docstring": "\"\"\" \u5f97\u5230\u6307\u5b9aURL\u7684\u7f16\u7801,\u5e76\u4e14\u5c06\u4e4b\u7f13\u5b58\u8d77\u6765,\n\u518d\u9047\u5230\u6709\u6307\u5b9a\u76ee\u5f55\u7684URL\u7684\u7f16\u7801\u67e5\u8be2\u8bf7\u6c42,\u5c31\u76f4\u63a5\u8fd4\u56de;\n\u8fd9\u4f1a\u5bfc\u51fa\u5185\u5b58\u6162\u6162\u589e\u5927,\u4f46\u5f71\u54cd\u4e0d\u5927\n\u540c\u65f6\u4e3a\u4e86\u8fdb\u4e00\u6b65\u5730\u8ba9\u7a0b\u5e8f\u5feb,\u6211\u4eec\u4e0d\u9700\u8981\u63a2\u6d4b\u6574\u4e2a\u7f51\u9875\u7684\u7f16\u7801 ,\n\u63a2\u6d4b\u524d1K\u5c31\u8db3\u591f\u4e86,\u5982\u679c\u662fsohu\u7684\u7f51\u9875(200K+),\u80fd\u5feb200\u500d\u5462.\n\"\"\"\n", "func_signal": "def _get_charset(data,url):\n", "code": "dir=hyer.urlfunc.get_dir(url)\nif CHARSETS_PER_DIR.has_key(dir):\n    return  CHARSETS_PER_DIR[dir]\nelse:\n    charset=chardet.detect(data[:2048])[\"encoding\"]\n    CHARSETS_PER_DIR[dir] = charset\n    return charset", "path": "hyer\\document.py", "repo_name": "xurenlu/hyer", "stars": 38, "license": "other", "language": "python", "size": 1973}
{"docstring": "\"\"\"\nGet the longest html part with tag density smaller than threshold according \nto density dictionary.\nReturn: (soup object, max text length with small enough tag density)\n\"\"\"\n", "func_signal": "def getMainText(countDic, threshold):\n", "code": "dens, tagNo, textLen, soup = countDic['self']\nif dens <= threshold:\n\tmaxSoup = soup\n\tmaxTextLen = textLen\nelse:\n\tmaxSoup = BeautifulSoup.BeautifulSoup(\"\")\n\tmaxTextLen = 0\nif countDic.has_key('child'):\n\tfor childDic in countDic['child']:\n\t\tsoup, textLen = getMainText(childDic, threshold)\n\t\tif textLen > maxTextLen:\n\t\t\tmaxSoup = soup\n\t\t\tmaxTextLen = textLen\nreturn (maxSoup, maxTextLen)", "path": "hyer\\vendor\\TextExtract.py", "repo_name": "xurenlu/hyer", "stars": 38, "license": "other", "language": "python", "size": 1973}
{"docstring": "\"\"\"return the document type:hub,text,pic\nhub:document with many links ,little text and pics\ntext:a document with lots of words\npics:a document focus on pictures\"\"\"\n", "func_signal": "def parse_document_type(self,html):\n", "code": "pics=0\nlinks=0\nwords=0\ntry:\n    r=re.compile(\"<img +\",re.M|re.S)\n    matches=r.findall(html)\t\n    pics=len(matches)\nexcept:\n    pass\ntext=self[\"body\"]\nlinks=len(self[\"links\"])\n#\u5f97\u5230\u6240\u6709\u7684\u975e\u94fe\u63a5\u6587\u5b57\"\nwords=len(text)\t\t\ntry:\n    r=re.compile(\"<a[^>]*>.*?</a>\",re.M|re.I|re.S)\n    unlinked_text=r.sub(\"\",text)\n    unlinked_text=self._html2text(unlinked_text)\n    words=len(unlinked_text)\nexcept :\n    pass\nself[\"links_count\"]=links\nself[\"pics_count\"]=pics\nself[\"words_count\"]=words\nself[\"doc_type\"]=self.TYPE_TEXT\nself[\"doc_rate_pic\"]=r1=words/(pics+1)\nself[\"doc_rate_link\"]=r2=words/(links+1)\nif r1 < self.PIC_RATE:\n    self[\"doc_type\"]=self.TYPE_PIC\nif r2 < self.HUB_RATE:\n    self[\"doc_type\"]=self.TYPE_HUB", "path": "hyer\\document.py", "repo_name": "xurenlu/hyer", "stars": 38, "license": "other", "language": "python", "size": 1973}
{"docstring": "\"\"\"\nhandle pid files\n\"\"\"\n", "func_signal": "def handle_pid():\n", "code": "pid=os.getpid()\npidfile= \"/tmp/run.generic.pid\"\ntry:\n    lastpid=int(open(pidfile).read())\nexcept:\n    lastpid=0\n    pass\ntry:\n    if lastpid>0:\n        os.kill(lastpid,9)\nexcept:\n    pass\nfp=open(pidfile,\"w\")\nfp.write(str(pid))\nfp.close()", "path": "newspider.py", "repo_name": "xurenlu/hyer", "stars": 38, "license": "other", "language": "python", "size": 1973}
{"docstring": "\"\"\"return the text without html tags\"\"\"\n", "func_signal": "def textualize(self,body):\n", "code": "text=body\nself[\"text\"]=self._html2text(text)", "path": "hyer\\document.py", "repo_name": "xurenlu/hyer", "stars": 38, "license": "other", "language": "python", "size": 1973}
{"docstring": "\"\"\" \n@param content:the HTML content\n@param uri:the URI of the html like file:///temp/os.html or http://www.162cm.com/index.html \"\"\"\n", "func_signal": "def __init__(self,content,uri='',task='default'):\n", "code": "Document.__init__(self,content,uri,task)\nself[\"URI\"]=uri\nself[\"html\"]=content\nself[\"links\"]=[]\nself[\"base\"]=\"\"\nself[\"keywords\"]=\"\"\nself[\"description\"]=\"\"\nself[\"charset\"]=\"\"\nself[\"head_data\"]=\"\"\nself[\"title\"]=\"\"\nself[\"body\"]=\"\"\nself[\"content\"]=content\n\nself.get_charset(self[\"content\"],self[\"URI\"])\nself.scan_links(self[\"content\"])\nself.get_head_data(self[\"content\"])\nself.get_title_data(self[\"head_data\"])\nself.get_keywords_meta(self[\"head_data\"])\nself.get_base_meta(self[\"head_data\"])\nself.get_description_meta(self[\"head_data\"])\nself.get_body_data(self[\"content\"])\nself.textualize(self[\"body\"])\nself.parse_document_type(self[\"body\"])", "path": "hyer\\document.py", "repo_name": "xurenlu/hyer", "stars": 38, "license": "other", "language": "python", "size": 1973}
{"docstring": "\"\"\"return <title>*</title> data in <head>*</head> section\nArguments:\n    -head\tthe <head>*</head> section content\"\"\"\n", "func_signal": "def get_title_data(self,head):\n", "code": "try:\n    r=re.compile(r'<title[^>]*>(.*?)<\\/title>',re.MULTILINE|re.S)\n    titles=r.findall(head)\n    title=titles.pop()\n    self[\"title\"]=title\n    return title\nexcept:\n    pass", "path": "hyer\\document.py", "repo_name": "xurenlu/hyer", "stars": 38, "license": "other", "language": "python", "size": 1973}
{"docstring": "\"\"\" return description meta data in the head content\"\"\"\n", "func_signal": "def get_description_meta(self,head):\n", "code": "try:\n    r=re.compile(\"<meta +name *=[\\\"']?description[\\\"']? *content=[\\\"']?([^<>'\\\"]+)[\\\"']?\",re.M|re.S)\n    matches=r.findall(head)\n    self[\"description\"]=matches.pop()\nexcept:\n    pass", "path": "hyer\\document.py", "repo_name": "xurenlu/hyer", "stars": 38, "license": "other", "language": "python", "size": 1973}
{"docstring": "\"\"\" \n@param content:the HTML content\n@param uri:the URI of the html like file:///temp/os.html or http://www.162cm.com/index.html \"\"\"\n", "func_signal": "def __init__(self,content,uri=\"\",task=\"default\"):\n", "code": "Document.__init__(self,content,uri,task)\nself[\"URI\"]=uri\nself[\"html\"]=content\nself[\"links\"]=[]\nself[\"base\"]=\"\"\nself[\"keywords\"]=\"\"\nself[\"description\"]=\"\"\nself[\"charset\"]=\"\"\nself[\"head_data\"]=\"\"\nself[\"title\"]=\"\"\nself[\"body\"]=\"\"\nself[\"content\"]=content\t\nself.get_charset(self[\"content\"],self[\"URI\"])\nself.scan_links(self[\"content\"])\n#self.parse_document_type(self[\"body\"])", "path": "hyer\\document.py", "repo_name": "xurenlu/hyer", "stars": 38, "license": "other", "language": "python", "size": 1973}
{"docstring": "\"\"\"\nReturns a list of :class:`ResultURL` instances related to the search and ordered by decreasing weight. This will be cached on the instance.\n\n:param threshhold: The earliest datetime that a :class:`Click` can have been made on a related :class:`ResultURL` in order to be included in the weighted results (or ``None`` to include all :class:`Click`\\ s and :class:`ResultURL`\\ s).\n\n\"\"\"\n", "func_signal": "def get_weighted_results(self, threshhold=None):\n", "code": "if not hasattr(self, '_weighted_results'):\n\tresult_qs = self.result_urls.all()\n\t\n\tif threshhold is not None:\n\t\tresult_qs = result_qs.filter(counts__datetime__gte=threshhold)\n\t\n\tresults = [result for result in result_qs]\n\t\n\tresults.sort(cmp=lambda x,y: cmp(y.weight, x.weight))\n\t\n\tself._weighted_results = results\n\nreturn self._weighted_results", "path": "philo\\contrib\\sobol\\models.py", "repo_name": "ithinksw/philo", "stars": 49, "license": "isc", "language": "python", "size": 3245}
{"docstring": "# Potential bottleneck?\n", "func_signal": "def get_location_querysets(self):\n", "code": "location_map = {}\nlocations = Event.objects.values_list('location_content_type', 'location_pk')\n\nfor ct, pk in locations:\n\tlocation_map.setdefault(ct, []).append(pk)\n\nlocation_cts = ContentType.objects.in_bulk(location_map.keys())\nlocation_querysets = {}\n\nfor ct_pk, pks in location_map.items():\n\tct = location_cts[ct_pk]\n\tlocation_querysets[ct] = ct.model_class()._default_manager.filter(pk__in=pks)\n\nreturn location_querysets", "path": "philo\\contrib\\julian\\models.py", "repo_name": "ithinksw/philo", "stars": 49, "license": "isc", "language": "python", "size": 3245}
{"docstring": "\"\"\"\nRenders :attr:`results_page` with a context containing an instance of :attr:`search_form`. If the form was submitted and was valid, then one of two things has happened:\n\n* A search has been initiated. In this case, a list of search instances will be added to the context as ``searches``. If :attr:`enable_ajax_api` is enabled, each instance will have an ``ajax_api_url`` attribute containing the url needed to make an AJAX request for the search results.\n* A link has been chosen. In this case, corresponding :class:`Search`, :class:`ResultURL`, and :class:`Click` instances will be created and the user will be redirected to the link's actual url.\n\n\"\"\"\n", "func_signal": "def results_view(self, request, extra_context=None):\n", "code": "results = None\n\ncontext = self.get_context()\ncontext.update(extra_context or {})\n\nif SEARCH_ARG_GET_KEY in request.GET:\n\tform = self.search_form(request.GET)\n\t\n\tif form.is_valid():\n\t\tsearch_string = request.GET[SEARCH_ARG_GET_KEY].lower()\n\t\turl = request.GET.get(URL_REDIRECT_GET_KEY)\n\t\thash = request.GET.get(HASH_REDIRECT_GET_KEY)\n\t\t\n\t\tif url and hash:\n\t\t\tif check_redirect_hash(hash, search_string, url):\n\t\t\t\t# Create the necessary models\n\t\t\t\tsearch = Search.objects.get_or_create(string=search_string)[0]\n\t\t\t\tresult_url = search.result_urls.get_or_create(url=url)[0]\n\t\t\t\tresult_url.clicks.create(datetime=datetime.datetime.now())\n\t\t\t\treturn HttpResponseRedirect(url)\n\t\t\telse:\n\t\t\t\tmessages.add_message(request, messages.INFO, \"The link you followed had been tampered with. Here are all the results for your search term instead!\")\n\t\t\t\t# TODO: Should search_string be escaped here?\n\t\t\t\treturn HttpResponseRedirect(\"%s?%s=%s\" % (request.path, SEARCH_ARG_GET_KEY, search_string))\n\t\t\n\t\tsearch_instances = []\n\t\tfor slug in self.searches:\n\t\t\tif slug in registry:\n\t\t\t\tsearch_instance = get_search_instance(slug, search_string)\n\t\t\t\tsearch_instances.append(search_instance)\n\t\t\t\n\t\t\t\tif self.enable_ajax_api:\n\t\t\t\t\tsearch_instance.ajax_api_url = \"%s?%s=%s\" % (self.reverse('ajax_api_view', kwargs={'slug': slug}, node=request.node), SEARCH_ARG_GET_KEY, search_string)\n\t\t\n\t\tif eventlet and not self.enable_ajax_api:\n\t\t\tpool = eventlet.GreenPool()\n\t\t\tfor instance in search_instances:\n\t\t\t\tpool.spawn_n(lambda x: x.results, search_instance)\n\t\t\tpool.waitall()\n\t\t\n\t\tcontext.update({\n\t\t\t'searches': search_instances,\n\t\t\t'favored_results': []\n\t\t})\n\t\t\n\t\ttry:\n\t\t\tsearch = Search.objects.get(string=search_string)\n\t\texcept Search.DoesNotExist:\n\t\t\tpass\n\t\telse:\n\t\t\tcontext['favored_results'] = [r.url for r in search.get_favored_results()]\nelse:\n\tform = SearchForm()\n\ncontext.update({\n\t'form': form\n})\nreturn self.results_page.render_to_response(request, extra_context=context)", "path": "philo\\contrib\\sobol\\models.py", "repo_name": "ithinksw/philo", "stars": 49, "license": "isc", "language": "python", "size": 3245}
{"docstring": "\"\"\"\nCalculates, caches, and returns the weight of the :class:`ResultURL`.\n\n:param threshhold: The datetime limit before which :class:`Click`\\ s will not contribute to the weight of the :class:`ResultURL`.\n\n\"\"\"\n", "func_signal": "def get_weight(self, threshhold=None):\n", "code": "if not hasattr(self, '_weight'):\n\tclicks = self.clicks.all()\n\t\n\tif threshhold is not None:\n\t\tclicks = clicks.filter(datetime__gte=threshhold)\n\t\n\tself._weight = sum([click.weight for click in clicks])\n\nreturn self._weight", "path": "philo\\contrib\\sobol\\models.py", "repo_name": "ithinksw/philo", "stars": 49, "license": "isc", "language": "python", "size": 3245}
{"docstring": "# Removing unique constraint on 'Template', fields ['slug', 'parent']\n", "func_signal": "def backwards(self, orm):\n", "code": "        db.delete_unique('philo_template', ['slug', 'parent_id'])\n\n        # Removing unique constraint on 'Node', fields ['slug', 'parent']\n        db.delete_unique('philo_node', ['slug', 'parent_id'])", "path": "philo\\migrations\\0015_auto__add_unique_node_slug_parent__add_unique_template_slug_parent.py", "repo_name": "ithinksw/philo", "stars": 49, "license": "isc", "language": "python", "size": 3245}
{"docstring": "# Deleting field 'Redirect.target_node'\n", "func_signal": "def backwards(self, orm):\n", "code": "        db.delete_column('philo_redirect', 'target_node_id')\n\n        # Deleting field 'Redirect.url_or_subpath'\n        db.delete_column('philo_redirect', 'url_or_subpath')\n\n        # Deleting field 'Redirect.reversing_parameters'\n        db.delete_column('philo_redirect', 'reversing_parameters_json')", "path": "philo\\migrations\\0010_auto__add_field_redirect_target_node__add_field_redirect_url_or_subpat.py", "repo_name": "ithinksw/philo", "stars": 49, "license": "isc", "language": "python", "size": 3245}
{"docstring": "\"\"\"Returns a :class:`Node` instance at ``path`` (relative to the current site) or ``None``.\"\"\"\n", "func_signal": "def get_node(path):\n", "code": "try:\n\tcurrent_site = Site.objects.get_current()\nexcept Site.DoesNotExist:\n\tcurrent_site = None\n\ntrailing_slash = False\nif path[-1] == '/':\n\ttrailing_slash = True\n\ntry:\n\tnode, subpath = Node.objects.get_with_path(path, root=getattr(current_site, 'root_node', None), absolute_result=False)\nexcept Node.DoesNotExist:\n\treturn None\n\nif subpath is None:\n\tsubpath = \"\"\nsubpath = \"/\" + subpath\n\nif trailing_slash and subpath[-1] != \"/\":\n\tsubpath += \"/\"\n\nnode._path = path\nnode._subpath = subpath\n\nreturn node", "path": "philo\\middleware.py", "repo_name": "ithinksw/philo", "stars": 49, "license": "isc", "language": "python", "size": 3245}
{"docstring": "# Adding field 'Redirect.target_node'\n", "func_signal": "def forwards(self, orm):\n", "code": "        db.add_column('philo_redirect', 'target_node', self.gf('django.db.models.fields.related.ForeignKey')(blank=True, related_name='philo_redirect_related', null=True, to=orm['philo.Node']), keep_default=False)\n\n        # Adding field 'Redirect.url_or_subpath'\n        db.add_column('philo_redirect', 'url_or_subpath', self.gf('django.db.models.fields.CharField')(default='', max_length=200, blank=True), keep_default=False)\n\n        # Adding field 'Redirect.reversing_parameters'\n        db.add_column('philo_redirect', 'reversing_parameters', self.gf('philo.models.fields.JSONField')(default='null', blank=True), keep_default=False)", "path": "philo\\migrations\\0010_auto__add_field_redirect_target_node__add_field_redirect_url_or_subpat.py", "repo_name": "ithinksw/philo", "stars": 49, "license": "isc", "language": "python", "size": 3245}
{"docstring": "# Adding model 'Location'\n", "func_signal": "def forwards(self, orm):\n", "code": "        db.create_table('julian_location', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('name', self.gf('django.db.models.fields.CharField')(max_length=255)),\n            ('slug', self.gf('django.db.models.fields.SlugField')(unique=True, max_length=255, db_index=True)),\n        ))\n        db.send_create_signal('julian', ['Location'])\n\n        # Adding model 'Event'\n        db.create_table('julian_event', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('start_date', self.gf('django.db.models.fields.DateField')()),\n            ('start_time', self.gf('django.db.models.fields.TimeField')(null=True, blank=True)),\n            ('end_date', self.gf('django.db.models.fields.DateField')()),\n            ('end_time', self.gf('django.db.models.fields.TimeField')(null=True, blank=True)),\n            ('name', self.gf('django.db.models.fields.CharField')(max_length=255)),\n            ('slug', self.gf('django.db.models.fields.SlugField')(max_length=255, db_index=True)),\n            ('location_content_type', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['contenttypes.ContentType'], null=True, blank=True)),\n            ('location_pk', self.gf('django.db.models.fields.TextField')(blank=True)),\n            ('description', self.gf('philo.models.fields.TemplateField')()),\n            ('parent_event', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['julian.Event'], null=True, blank=True)),\n            ('owner', self.gf('django.db.models.fields.related.ForeignKey')(related_name='owned_events', to=orm['auth.User'])),\n            ('created', self.gf('django.db.models.fields.DateTimeField')(auto_now_add=True, blank=True)),\n            ('last_modified', self.gf('django.db.models.fields.DateTimeField')(auto_now=True, blank=True)),\n            ('site', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['sites.Site'])),\n        ))\n        db.send_create_signal('julian', ['Event'])\n\n        # Adding unique constraint on 'Event', fields ['site', 'created']\n        db.create_unique('julian_event', ['site_id', 'created'])\n\n        # Adding M2M table for field tags on 'Event'\n        db.create_table('julian_event_tags', (\n            ('id', models.AutoField(verbose_name='ID', primary_key=True, auto_created=True)),\n            ('event', models.ForeignKey(orm['julian.event'], null=False)),\n            ('tag', models.ForeignKey(orm['philo.tag'], null=False))\n        ))\n        db.create_unique('julian_event_tags', ['event_id', 'tag_id'])\n\n        # Adding model 'Calendar'\n        db.create_table('julian_calendar', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('name', self.gf('django.db.models.fields.CharField')(max_length=100)),\n            ('slug', self.gf('django.db.models.fields.SlugField')(max_length=100, db_index=True)),\n            ('description', self.gf('django.db.models.fields.TextField')(blank=True)),\n            ('site', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['sites.Site'])),\n            ('language', self.gf('django.db.models.fields.CharField')(default='en', max_length=5)),\n        ))\n        db.send_create_signal('julian', ['Calendar'])\n\n        # Adding unique constraint on 'Calendar', fields ['name', 'site', 'language']\n        db.create_unique('julian_calendar', ['name', 'site_id', 'language'])\n\n        # Adding M2M table for field events on 'Calendar'\n        db.create_table('julian_calendar_events', (\n            ('id', models.AutoField(verbose_name='ID', primary_key=True, auto_created=True)),\n            ('calendar', models.ForeignKey(orm['julian.calendar'], null=False)),\n            ('event', models.ForeignKey(orm['julian.event'], null=False))\n        ))\n        db.create_unique('julian_calendar_events', ['calendar_id', 'event_id'])\n\n        # Adding model 'CalendarView'\n        db.create_table('julian_calendarview', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('feed_type', self.gf('django.db.models.fields.CharField')(default='text/calendar', max_length=50)),\n            ('feed_suffix', self.gf('django.db.models.fields.CharField')(default='feed', max_length=255)),\n            ('feeds_enabled', self.gf('django.db.models.fields.BooleanField')(default=True)),\n            ('feed_length', self.gf('django.db.models.fields.PositiveIntegerField')(default=15, null=True, blank=True)),\n            ('item_title_template', self.gf('django.db.models.fields.related.ForeignKey')(blank=True, related_name='julian_calendarview_title_related', null=True, to=orm['philo.Template'])),\n            ('item_description_template', self.gf('django.db.models.fields.related.ForeignKey')(blank=True, related_name='julian_calendarview_description_related', null=True, to=orm['philo.Template'])),\n            ('calendar', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['julian.Calendar'])),\n            ('index_page', self.gf('django.db.models.fields.related.ForeignKey')(related_name='calendar_index_related', to=orm['philo.Page'])),\n            ('event_detail_page', self.gf('django.db.models.fields.related.ForeignKey')(related_name='calendar_detail_related', to=orm['philo.Page'])),\n            ('timespan_page', self.gf('django.db.models.fields.related.ForeignKey')(blank=True, related_name='calendar_timespan_related', null=True, to=orm['philo.Page'])),\n            ('tag_page', self.gf('django.db.models.fields.related.ForeignKey')(blank=True, related_name='calendar_tag_related', null=True, to=orm['philo.Page'])),\n            ('location_page', self.gf('django.db.models.fields.related.ForeignKey')(blank=True, related_name='calendar_location_related', null=True, to=orm['philo.Page'])),\n            ('owner_page', self.gf('django.db.models.fields.related.ForeignKey')(blank=True, related_name='calendar_owner_related', null=True, to=orm['philo.Page'])),\n            ('tag_archive_page', self.gf('django.db.models.fields.related.ForeignKey')(blank=True, related_name='calendar_tag_archive_related', null=True, to=orm['philo.Page'])),\n            ('location_archive_page', self.gf('django.db.models.fields.related.ForeignKey')(blank=True, related_name='calendar_location_archive_related', null=True, to=orm['philo.Page'])),\n            ('owner_archive_page', self.gf('django.db.models.fields.related.ForeignKey')(blank=True, related_name='calendar_owner_archive_related', null=True, to=orm['philo.Page'])),\n            ('tag_permalink_base', self.gf('django.db.models.fields.CharField')(default='tags', max_length=30)),\n            ('owner_permalink_base', self.gf('django.db.models.fields.CharField')(default='owners', max_length=30)),\n            ('location_permalink_base', self.gf('django.db.models.fields.CharField')(default='locations', max_length=30)),\n            ('events_per_page', self.gf('django.db.models.fields.PositiveIntegerField')(null=True, blank=True)),\n        ))\n        db.send_create_signal('julian', ['CalendarView'])", "path": "philo\\contrib\\julian\\migrations\\0001_initial.py", "repo_name": "ithinksw/philo", "stars": 49, "license": "isc", "language": "python", "size": 3245}
{"docstring": "\"\"\"\n:param root: Only return the path since this object.\n:param pathsep: The path separator to use when constructing an instance's path\n:param field: The field to pull path information from for each ancestor.\n:param memoize: Whether to use memoized results. Since, in most cases, the ancestors of a TreeEntity will not change over the course of an instance's lifetime, this defaults to ``True``.\n:returns: A string representation of an object's path.\n\n\"\"\"\n\n", "func_signal": "def get_path(self, root=None, pathsep='/', field='pk', memoize=True):\n", "code": "if root == self:\n\treturn ''\n\nparent_id = getattr(self, \"%s_id\" % self._mptt_meta.parent_attr)\nif getattr(root, 'pk', None) == parent_id:\n\treturn getattr(self, field, '?')\n\nif root is not None and not self.is_descendant_of(root):\n\traise AncestorDoesNotExist(root)\n\nif memoize:\n\tmemo_args = (parent_id, getattr(root, 'pk', None), pathsep, getattr(self, field, '?'))\n\ttry:\n\t\treturn self._path_memo[memo_args]\n\texcept AttributeError:\n\t\tself._path_memo = {}\n\texcept KeyError:\n\t\tpass\n\nqs = self.get_ancestors(include_self=True)\n\nif root is not None:\n\tqs = qs.filter(**{'%s__gt' % self._mptt_meta.level_attr: root.get_level()})\n\npath = pathsep.join([getattr(parent, field, '?') for parent in qs])\n\nif memoize:\n\tself._path_memo[memo_args] = path\n\nreturn path", "path": "philo\\models\\base.py", "repo_name": "ithinksw/philo", "stars": 49, "license": "isc", "language": "python", "size": 3245}
{"docstring": "\"\"\"Given a value and a value class, sets up self.value appropriately.\"\"\"\n", "func_signal": "def set_value(self, value, value_class=JSONValue):\n", "code": "if isinstance(self.value, value_class):\n\tval = self.value\nelse:\n\tif isinstance(self.value, models.Model):\n\t\tself.value.delete()\n\tval = value_class()\n\nval.set_value(value)\nval.save()\n\nself.value = val\nself.save()", "path": "philo\\models\\base.py", "repo_name": "ithinksw/philo", "stars": 49, "license": "isc", "language": "python", "size": 3245}
{"docstring": "\"\"\"\nReturns a JSON object containing the following variables:\n\nsearch\n\tContains the slug for the search.\nresults\n\tContains the results of :meth:`.Result.get_context` for each result.\nrendered\n\tContains the results of :meth:`.Result.render` for each result.\nhasMoreResults\n\t``True`` or ``False`` whether the search has more results according to :meth:`BaseSearch.has_more_results`\nmoreResultsURL\n\tContains ``None`` or a querystring which, once accessed, will note the :class:`Click` and redirect the user to a page containing more results.\n\n\"\"\"\n", "func_signal": "def ajax_api_view(self, request, slug, extra_context=None):\n", "code": "search_string = request.GET.get(SEARCH_ARG_GET_KEY)\n\nif not request.is_ajax() or not self.enable_ajax_api or slug not in registry or slug not in self.searches or search_string is None:\n\traise Http404\n\nsearch_instance = get_search_instance(slug, search_string)\n\nreturn HttpResponse(json.dumps({\n\t'search': search_instance.slug,\n\t'results': [result.get_context() for result in search_instance.results],\n\t'hasMoreResults': search_instance.has_more_results,\n\t'moreResultsURL': search_instance.more_results_url,\n}), mimetype=\"application/json\")", "path": "philo\\contrib\\sobol\\models.py", "repo_name": "ithinksw/philo", "stars": 49, "license": "isc", "language": "python", "size": 3245}
{"docstring": "\"\"\"\nIf ``absolute_result`` is ``True``, returns the object at ``path`` (starting at ``root``) or raises an :class:`~django.core.exceptions.ObjectDoesNotExist` exception. Otherwise, returns a tuple containing the deepest object found along ``path`` (or ``root`` if no deeper object is found) and the remainder of the path after that object as a string (or None if there is no remaining path).\n\n.. note:: If you are looking for something with an exact path, it is faster to use absolute_result=True, unless the path depth is over ~40, in which case the high cost of the absolute query may make a binary search (i.e. non-absolute) faster.\n\n.. note:: SQLite allows max of 64 tables in one join. That means the binary search will only work on paths with a max depth of 127 and the absolute fetch will only work to a max depth of (surprise!) 63. Larger depths could be handled, but since the common use case will not have a tree structure that deep, they are not.\n\n:param path: The path of the object\n:param root: The object which will be considered the root of the search\n:param absolute_result: Whether to return an absolute result or do a binary search\n:param pathsep: The path separator used in ``path``\n:param field: The field on the model which should be queried for ``path`` segment matching.\n:returns: An instance if ``absolute_result`` is ``True`` or an (instance, remaining_path) tuple otherwise.\n:raises django.core.exceptions.ObjectDoesNotExist: if no object can be found matching the input parameters.\n\n\"\"\"\n\n", "func_signal": "def get_with_path(self, path, root=None, absolute_result=True, pathsep='/', field='pk'):\n", "code": "segments = path.split(pathsep)\n\n# Clean out blank segments. Handles multiple consecutive pathseps.\nwhile True:\n\ttry:\n\t\tsegments.remove('')\n\texcept ValueError:\n\t\tbreak\n\n# Special-case a lack of segments. No queries necessary.\nif not segments:\n\tif root is not None:\n\t\tif absolute_result:\n\t\t\treturn root\n\t\treturn root, None\n\telse:\n\t\traise self.model.DoesNotExist('%s matching query does not exist.' % self.model._meta.object_name)\n\ndef make_query_kwargs(segments, root):\n\tkwargs = {}\n\tprefix = \"\"\n\trevsegs = list(segments)\n\trevsegs.reverse()\n\t\n\tfor segment in revsegs:\n\t\tkwargs[\"%s%s__exact\" % (prefix, field)] = segment\n\t\tprefix += \"parent__\"\n\t\n\tif prefix:\n\t\tkwargs[prefix[:-2]] = root\n\t\n\treturn kwargs\n\ndef find_obj(segments, depth, deepest_found=None):\n\tif deepest_found is None:\n\t\tdeepest_level = 0\n\telif root is None:\n\t\tdeepest_level = deepest_found.get_level() + 1\n\telse:\n\t\tdeepest_level = deepest_found.get_level() - root.get_level()\n\ttry:\n\t\tobj = self.get(**make_query_kwargs(segments[deepest_level:depth], deepest_found or root))\n\texcept self.model.DoesNotExist:\n\t\tif not deepest_level and depth > 1:\n\t\t\t# make sure there's a root node...\n\t\t\tdepth = 1\n\t\telse:\n\t\t\t# Try finding one with half the path since the deepest find.\n\t\t\tdepth = (deepest_level + depth)/2\n\t\t\n\t\tif deepest_level == depth:\n\t\t\t# This should happen if nothing is found with any part of the given path.\n\t\t\tif root is not None and deepest_found is None:\n\t\t\t\treturn root, pathsep.join(segments)\n\t\t\traise\n\t\t\n\t\treturn find_obj(segments, depth, deepest_found)\n\telse:\n\t\t# Yay! Found one!\n\t\tif root is None:\n\t\t\tdeepest_level = obj.get_level() + 1\n\t\telse:\n\t\t\tdeepest_level = obj.get_level() - root.get_level()\n\t\t\n\t\t# Could there be a deeper one?\n\t\tif obj.is_leaf_node():\n\t\t\treturn obj, pathsep.join(segments[deepest_level:]) or None\n\t\t\n\t\tdepth += (len(segments) - depth)/2 or len(segments) - depth\n\t\t\n\t\tif depth > deepest_level + obj.get_descendant_count():\n\t\t\tdepth = deepest_level + obj.get_descendant_count()\n\t\t\n\t\tif deepest_level == depth:\n\t\t\treturn obj, pathsep.join(segments[deepest_level:]) or None\n\t\t\n\t\ttry:\n\t\t\treturn find_obj(segments, depth, obj)\n\t\texcept self.model.DoesNotExist:\n\t\t\t# Then this was the deepest.\n\t\t\treturn obj, pathsep.join(segments[deepest_level:])\n\nif absolute_result:\n\treturn self.get(**make_query_kwargs(segments, root))\n\n# Try a modified binary search algorithm. Feed the root in so that query complexity\n# can be reduced. It might be possible to weight the search towards the beginning\n# of the path, since short paths are more likely, but how far forward? It would\n# need to shift depending on len(segments) - perhaps logarithmically?\nreturn find_obj(segments, len(segments)/2 or len(segments))", "path": "philo\\models\\base.py", "repo_name": "ithinksw/philo", "stars": 49, "license": "isc", "language": "python", "size": 3245}
{"docstring": "# Perhaps timespans should be done with GET parameters? Or two /-separated\n# date slugs? (e.g. 2010-02-1/2010-02-2) or a start and duration?\n# (e.g. 2010-02-01/week/ or ?d=2010-02-01&l=week)\n", "func_signal": "def urlpatterns(self):\n", "code": "urlpatterns = self.feed_patterns(r'^', 'get_all_events', 'index_page', 'index') + \\\n\tself.timespan_patterns(r'^(?P<year>\\d{4})', 'year') + \\\n\tself.timespan_patterns(r'^(?P<year>\\d{4})/(?P<month>\\d{2})', 'month') + \\\n\tself.timespan_patterns(r'^(?P<year>\\d{4})/(?P<month>\\d{2})/(?P<day>\\d{2})', 'day') + \\\n\tself.feed_patterns(r'^%s/(?P<username>[^/]+)' % self.owner_permalink_base, 'get_events_by_owner', 'owner_page', 'events_by_user') + \\\n\tself.feed_patterns(r'^%s/(?P<app_label>\\w+)/(?P<model>\\w+)/(?P<pk>[^/]+)' % self.location_permalink_base, 'get_events_by_location', 'location_page', 'events_by_location') + \\\n\tself.feed_patterns(r'^%s/(?P<tag_slugs>[-\\w]+[-+/\\w]*)' % self.tag_permalink_base, 'get_events_by_tag', 'tag_page', 'events_by_tag') + \\\n\tpatterns('',\n\t\turl(r'(?P<year>\\d{4})/(?P<month>\\d{2})/(?P<day>\\d{2})/(?P<slug>[\\w-]+)$', self.event_detail_view, name=\"event_detail\"),\n\t)\n\t\n\t# Some sort of shortcut for a location would be useful. This could be on a per-calendar\n\t# or per-calendar-view basis.\n\t#url(r'^%s/(?P<slug>[\\w-]+)' % self.location_permalink_base, ...)\n\nif self.tag_archive_page_id:\n\turlpatterns += patterns('',\n\t\turl(r'^%s$' % self.tag_permalink_base, self.tag_archive_view, name='tag_archive')\n\t)\n\nif self.owner_archive_page_id:\n\turlpatterns += patterns('',\n\t\turl(r'^%s$' % self.owner_permalink_base, self.owner_archive_view, name='owner_archive')\n\t)\n\nif self.location_archive_page_id:\n\turlpatterns += patterns('',\n\t\turl(r'^%s$' % self.location_permalink_base, self.location_archive_view, name='location_archive')\n\t)\nreturn urlpatterns", "path": "philo\\contrib\\julian\\models.py", "repo_name": "ithinksw/philo", "stars": 49, "license": "isc", "language": "python", "size": 3245}
{"docstring": "# Value must be a queryset. Watch out for ModelMultipleChoiceField;\n# it returns its value as a list if empty.\n\n", "func_signal": "def set_value(self, value):\n", "code": "self.content_type = ContentType.objects.get_for_model(value.model)\n\n# Before we can fiddle with the many-to-many to foreignkeyvalues, we need\n# a pk.\nif self.pk is None:\n\tself.save()\n\nobject_ids = value.values_list('id', flat=True)\n\n# These lines shouldn't be necessary; however, if object_ids is an EmptyQuerySet,\n# the code (specifically the object_id__in query) won't work without them. Unclear why...\n# TODO: is this still the case?\nif not object_ids:\n\tself.values.all().delete()\nelse:\n\tself.values.exclude(object_id__in=object_ids, content_type=self.content_type).delete()\n\t\n\tcurrent_ids = self.object_ids\n\t\n\tfor object_id in object_ids:\n\t\tif object_id in current_ids:\n\t\t\tcontinue\n\t\tself.values.create(content_type=self.content_type, object_id=object_id)", "path": "philo\\models\\base.py", "repo_name": "ithinksw/philo", "stars": 49, "license": "isc", "language": "python", "size": 3245}
{"docstring": "\"\"\"\nReturns a :class:`.TreeAttributeMapper` or :class:`.AttributeMapper` which can be used to retrieve related :class:`Attribute`\\ s' values directly. If an :class:`Attribute` with a given key is not related to the :class:`Entity`, then the mapper will check the parent's attributes.\n\nExample::\n\n\t>>> attr = entity.attribute_set.get(key='spam')\n\tDoesNotExist: Attribute matching query does not exist.\n\t>>> attr = entity.parent.attribute_set.get(key='spam')\n\t>>> attr.value.value\n\tu'eggs'\n\t>>> entity.attributes['spam']\n\tu'eggs'\n\n\"\"\"\n", "func_signal": "def get_attribute_mapper(self, mapper=None):\n", "code": "if mapper is None:\n\tif getattr(self, \"%s_id\" % self._mptt_meta.parent_attr):\n\t\tmapper = TreeAttributeMapper\n\telse:\n\t\tmapper = AttributeMapper\nreturn super(TreeEntity, self).get_attribute_mapper(mapper)", "path": "philo\\models\\base.py", "repo_name": "ithinksw/philo", "stars": 49, "license": "isc", "language": "python", "size": 3245}
{"docstring": "\"\"\"\nCalculates the set of most-favored results based on their weight. Evenly-weighted results will be grouped together and either added or excluded as a group.\n\n:param error: An arbitrary number; higher values will cause this method to be more reticent about adding new items to the favored results.\n:param threshhold: Will be passed directly into :meth:`get_weighted_results`\n\n\"\"\"\n", "func_signal": "def get_favored_results(self, error=5, threshhold=None):\n", "code": "if not hasattr(self, '_favored_results'):\n\tresults = self.get_weighted_results(threshhold)\n\t\n\tgrouped_results = SortedDict()\n\t\n\tfor result in results:\n\t\tgrouped_results.setdefault(result.weight, []).append(result)\n\t\n\tself._favored_results = []\n\t\n\tfor value, subresults in grouped_results.items():\n\t\tcost = error * sum([(value - result.weight)**2 for result in self._favored_results])\n\t\tif value > cost:\n\t\t\tself._favored_results += subresults\n\t\telse:\n\t\t\tbreak\n\tif len(self._favored_results) == len(results):\n\t\tself._favored_results = []\nreturn self._favored_results", "path": "philo\\contrib\\sobol\\models.py", "repo_name": "ithinksw/philo", "stars": 49, "license": "isc", "language": "python", "size": 3245}
{"docstring": "# Removing unique constraint on 'Calendar', fields ['name', 'site', 'language']\n", "func_signal": "def backwards(self, orm):\n", "code": "        db.delete_unique('julian_calendar', ['name', 'site_id', 'language'])\n\n        # Removing unique constraint on 'Event', fields ['site', 'created']\n        db.delete_unique('julian_event', ['site_id', 'created'])\n\n        # Deleting model 'Location'\n        db.delete_table('julian_location')\n\n        # Deleting model 'Event'\n        db.delete_table('julian_event')\n\n        # Removing M2M table for field tags on 'Event'\n        db.delete_table('julian_event_tags')\n\n        # Deleting model 'Calendar'\n        db.delete_table('julian_calendar')\n\n        # Removing M2M table for field events on 'Calendar'\n        db.delete_table('julian_calendar_events')\n\n        # Deleting model 'CalendarView'\n        db.delete_table('julian_calendarview')", "path": "philo\\contrib\\julian\\migrations\\0001_initial.py", "repo_name": "ithinksw/philo", "stars": 49, "license": "isc", "language": "python", "size": 3245}
{"docstring": "# Adding unique constraint on 'Node', fields ['slug', 'parent']\n", "func_signal": "def forwards(self, orm):\n", "code": "        db.create_unique('philo_node', ['slug', 'parent_id'])\n\n        # Adding unique constraint on 'Template', fields ['slug', 'parent']\n        db.create_unique('philo_template', ['slug', 'parent_id'])", "path": "philo\\migrations\\0015_auto__add_unique_node_slug_parent__add_unique_template_slug_parent.py", "repo_name": "ithinksw/philo", "stars": 49, "license": "isc", "language": "python", "size": 3245}
{"docstring": "# What datastructure should locations be?\n", "func_signal": "def location_archive_view(self, request, extra_context=None):\n", "code": "locations = self.get_location_querysets()\ncontext = self.get_context()\ncontext.update(extra_context or {})\ncontext.update({\n\t'locations': locations\n})\nreturn self.location_archive_page.render_to_response(request, extra_context=context)", "path": "philo\\contrib\\julian\\models.py", "repo_name": "ithinksw/philo", "stars": 49, "license": "isc", "language": "python", "size": 3245}
{"docstring": "\"\"\" Build a classifier and test it \n\n\tArgs:\n\t\tclf: classifier object\n\t\ttestFile: test metrics file\n\t\toutFile: submission file\n\n\"\"\"\n", "func_signal": "def testAndOutput(self, clf=None, testFile='', orderFile=None, outfile='sub.csv'):\n", "code": "tf_ = pd.read_csv(testFile)\n\nif orderFile is not None:\n\ttf_['proba'] = np.loadtxt(orderFile,delimiter=',')\n\ntest = self.scaler.transform(np.array(tf_)[:,self.indices])\nX, y = self.shuffle()\nclf.fit(X,y)\ny_ =  clf.predict_proba(test)\nnp.savetxt(outfile,y_[:,1],delimiter=',')", "path": "classifier.py", "repo_name": "nmkridler/moby", "stars": 55, "license": "None", "language": "python", "size": 508}
{"docstring": "\"\"\"\"\"\"\n", "func_signal": "def __init__(self, dataDir=''):\n", "code": "self.dataDir = dataDir\nself.test = range(1,54504)\nself.nTest = 54503", "path": "fileio.py", "repo_name": "nmkridler/moby", "stars": 55, "license": "None", "language": "python", "size": 508}
{"docstring": "\"\"\" Read the csv file and populate lists \"\"\"\n", "func_signal": "def Load(self):\n", "code": "file = open(self.fileName, 'r')\nself.hdr = file.readline().split('\\n')[0].split(',')\n\nfor line in file.readlines():\n\ttokens = line.split('\\n')[0].split(',')\n\tif int(tokens[1]) == 0:\n\t\tself.h0.append(tokens[0])\n\telse:\n\t\tself.h1.append(tokens[0])\nfile.close()\nself.numH1 = len(self.h1)\nself.numH0 = len(self.h0)", "path": "fileio.py", "repo_name": "nmkridler/moby", "stars": 55, "license": "None", "language": "python", "size": 508}
{"docstring": "\"\"\" Template matching\n\n\tPerform template matching for a list of templates\n\n\tArgs:\n\t\tP: 2-d numpy array image\n\t\ttmpl: templateManager object\n\n\tReturns:\n\t\tList of correlations, x and y pixel locations of the max \n\"\"\"\n", "func_signal": "def templateMetrics(P, tmpl):\n", "code": "maxs, xs, ys = [], [], []\nfor k in range(tmpl.size):\n\tmf, y, x  = matchTemplate(P,tmpl.templates[k])\n\tmaxs.append(mf)\n\txs.append(x)\n\tys.append(y)\nreturn maxs + xs + ys", "path": "metrics.py", "repo_name": "nmkridler/moby", "stars": 55, "license": "None", "language": "python", "size": 508}
{"docstring": "\"\"\" Oops metrics\n\n\tThis was supposed to be the centroid of the frequency slices\n\tas defined in timeMetrics. Fortunate typo that has power\n\tin discrimination.\n\n\tArgs:\n\t\tP: 2-d numpy array image\n\t\tb: time bins\n\t\tmaxM: max frequency slice to consider\n\n\tReturns:\n\t\tA list containing the statistics\n\"\"\"\n", "func_signal": "def oopsMetrics(P, b,maxM=50):\n", "code": "cf_ = [np.sum(P[i,:]*b)/np.sum(P[:,i]) for i in range(maxM)]\nreturn cf_", "path": "metrics.py", "repo_name": "nmkridler/moby", "stars": 55, "license": "None", "language": "python", "size": 508}
{"docstring": "\"\"\" Enhance the constrast vertically (along frequency dimension)\n\n\tCut off extreme values and demean the image\n\tUtilize numpy convolve to get the mean at a given pixel\n\tRemove local mean with inner exclusion region\n\n\tArgs:\n\t\tP: 2-d numpy array image\n\t\tinner: inner exclusion region \n\t\touter: length of the window\n\t\tmaxM: size of the output image in the y-dimension\n\t\tnorm: boolean to cut off extreme values\n\n\tReturns:\n\t\tQ: 2-d numpy contrast enhanced vertically\n\"\"\"\n", "func_signal": "def slidingWindowV(P,inner=3,outer=64,maxM = 50,norm=True):\n", "code": "Q = P.copy()\nm, n = Q.shape\nif norm:\n\tmval, sval = np.mean(Q[:maxM,:]), np.std(Q[:maxM,:])\n\tfact_ = 1.5\n\tQ[Q > mval + fact_*sval] = mval + fact_*sval\n\tQ[Q < mval - fact_*sval] = mval - fact_*sval\nwInner = np.ones(inner)\nwOuter = np.ones(outer)\nfor i in range(n):\n\tQ[:,i] = Q[:,i] - (np.convolve(Q[:,i],wOuter,'same') - np.convolve(Q[:,i],wInner,'same'))/(outer - inner)\nreturn Q[:maxM,:]", "path": "metrics.py", "repo_name": "nmkridler/moby", "stars": 55, "license": "None", "language": "python", "size": 508}
{"docstring": "\"\"\"Spectrogram\"\"\"\n", "func_signal": "def PlotSpecgram(P, freqs, bins):\n", "code": "Z = np.flipud(P)\n\nxextent = 0, np.amax(bins)\nxmin, xmax = xextent\nextent = xmin, xmax, freqs[0], freqs[-1]\n\nim = pl.imshow(Z, extent=extent)\npl.axis('auto')\npl.xlim([0.0, bins[-1]])\npl.ylim([0, 400])", "path": "plotting.py", "repo_name": "nmkridler/moby", "stars": 55, "license": "None", "language": "python", "size": 508}
{"docstring": "\"\"\"\"\"\"\n", "func_signal": "def __init__(self, trainFile='', orderFile=None, useCols=None):\n", "code": "self.trainFile = trainFile\nself.orderFile = orderFile\nself.indices = useCols\nself.loadTrain()\nself.m, self.n = self.train.shape\nself.scaler = StandardScaler()\nself.scaler.fit(self.train)", "path": "classifier.py", "repo_name": "nmkridler/moby", "stars": 55, "license": "None", "language": "python", "size": 508}
{"docstring": "\"\"\" Compute a bunch of metrics\n\n\tPerform template matching and time stats\n\n\tArgs:\n\t\tP: 2-d numpy array\n\t\ttmpl: templateManager object\n\t\tbins: time bins\n\t\tmaxT: maximum frequency slice for time stats\n\n\tReturns:\n\t\tList of metrics\n\"\"\"\n", "func_signal": "def computeMetrics(P, tmpl, bins, maxT):\n", "code": "Q = slidingWindowV(P,inner=3,maxM=40)\nW = slidingWindowH(P,inner=3,outer=32,maxM=60)\nout = templateMetrics(Q, tmpl)\t\nout += templateMetrics(W, tmpl)\t\nout += timeMetrics(P,bins,maxM=maxT)\nout += oopsMetrics(P,bins)\nout += highFreqMetrics(P,bins)\nreturn out", "path": "metrics.py", "repo_name": "nmkridler/moby", "stars": 55, "license": "None", "language": "python", "size": 508}
{"docstring": "\"\"\" Grab an H0 sample\n\n\tArgs:\n\t\tindex: index of file to read\n\t\tparams: dictionary containing specgram params\n\n\tReturns:\n\t\tSpectrogram and freq/time bins\n\"\"\"\n", "func_signal": "def H0Sample(self, index=None, params=None):\n", "code": "if index == None:\n\tindex = random.randint(0,self.numH0-1)\ns = ReadAIFF(self.dataDir+self.h0[index])\nP, freqs, bins = mlab.specgram(s, **params)\nreturn P, freqs, bins", "path": "fileio.py", "repo_name": "nmkridler/moby", "stars": 55, "license": "None", "language": "python", "size": 508}
{"docstring": "\"\"\"\"\"\"\n", "func_signal": "def __init__(self, fileName='', dataDir=''):\n", "code": "self.fileName = fileName\nself.dataDir = dataDir\nself.h1 = []\nself.h0 = []\nself.Load()", "path": "fileio.py", "repo_name": "nmkridler/moby", "stars": 55, "license": "None", "language": "python", "size": 508}
{"docstring": "\"\"\"Density plot\"\"\"\n", "func_signal": "def PlotDensity(prediction, labelStr, minval=None, maxval=None):\n", "code": "if minval != None and maxval != None:\n\tprediction = (prediction - minval)/(maxval - minval)\n\ndensity = gaussian_kde(prediction)\nxs = np.linspace(0,1,200)\ndensity.covariance_factor = lambda : .1\ndensity._compute_covariance()\npl.plot(xs, density(xs), lw=3, label=labelStr)\npl.legend(loc=\"upper right\")", "path": "plotting.py", "repo_name": "nmkridler/moby", "stars": 55, "license": "None", "language": "python", "size": 508}
{"docstring": "\"\"\" Enhance the constrast horizontally (along temporal dimension)\n\n\tCut off extreme values and demean the image\n\tUtilize numpy convolve to get the mean at a given pixel\n\tRemove local mean with inner exclusion region\n\n\tArgs:\n\t\tP: 2-d numpy array image\n\t\tinner: inner exclusion region \n\t\touter: length of the window\n\t\tmaxM: size of the output image in the y-dimension\n\t\tnorm: boolean to cut off extreme values\n\n\tReturns:\n\t\tQ: 2-d numpy contrast enhanced vertically\n\"\"\"\n", "func_signal": "def slidingWindowH(P,inner=3,outer=32,maxM=50,norm=True):\n", "code": "Q = P.copy()\nm, n = Q.shape\nif norm:\n\tmval, sval = np.mean(Q[:maxM,:]), np.std(Q[:maxM,:])\n\tfact_ = 1.5\n\tQ[Q > mval + fact_*sval] = mval + fact_*sval\n\tQ[Q < mval - fact_*sval] = mval - fact_*sval\nwInner = np.ones(inner)\nwOuter = np.ones(outer)\nfor i in range(maxM):\n\tQ[i,:] = Q[i,:] - (np.convolve(Q[i,:],wOuter,'same') - np.convolve(Q[i,:],wInner,'same'))/(outer - inner)\nreturn Q[:maxM,:]", "path": "metrics.py", "repo_name": "nmkridler/moby", "stars": 55, "license": "None", "language": "python", "size": 508}
{"docstring": "\"\"\" High frequency template matching\n\n\tApply horizontal contrast enhancement and\n\tlook for strong vertical features in the image.\n\tCut out the lower frequencies\n\n\tArgs:\n\t\tP: 2-d numpy array image\n\t\ttmpl: 2-d numpy array template image\n\n\tReturns:\n\t\tMaximum correlation as a list\n\"\"\"\n", "func_signal": "def highFreqTemplate(P, tmpl):\n", "code": "Q = slidingWindowH(P,inner=7,maxM=50,norm=True)[38:,:]\t\nmf = cv2.matchTemplate(Q.astype('Float32'), tmpl, cv2.TM_CCOEFF_NORMED)\nreturn [mf.max()]", "path": "metrics.py", "repo_name": "nmkridler/moby", "stars": 55, "license": "None", "language": "python", "size": 508}
{"docstring": "\"\"\" Calculate the auc at different iterations \"\"\"\n", "func_signal": "def getAuc(clf,X,y,n_estimators):\n", "code": "test_auc = np.zeros((n_estimators,), dtype=np.float64)\nfor i, y_pred in enumerate(clf.staged_decision_function(X)):\n\tif i % 20 == 0:\n\t\tfpr, tpr, thresholds = roc_curve(y,y_pred)\n\t\ttest_auc[i] = auc(fpr,tpr)\npl.plot((np.arange(test_auc.shape[0]) + 1)[::20], test_auc[::20],'-',lw=2)", "path": "classifier.py", "repo_name": "nmkridler/moby", "stars": 55, "license": "None", "language": "python", "size": 508}
{"docstring": "\"\"\" Shuffle and scale\n\n\tArgs:\n\t\tseed: random seed\n\n\tReturns:\n\t\tshuffled and scaled data\n\"\"\"\n", "func_signal": "def shuffle(self, seed=0):\n", "code": "self.p = range(self.m)\nrandom.seed(seed)\nrandom.shuffle(self.p)\nreturn self.scaler.transform(self.train[self.p,:]), self.truth[self.p]", "path": "classifier.py", "repo_name": "nmkridler/moby", "stars": 55, "license": "None", "language": "python", "size": 508}
{"docstring": "\"\"\" Read AIFF and convert to numpy array\n\n\tArgs:\n\t\tfile: string\n\t\t\tfile to read\n\n\tReturns:\n\t\tnumpy array containing whale audio clip\n\"\"\"\n", "func_signal": "def ReadAIFF(file):\n", "code": "s = aifc.open(file,'r')\nnFrames = s.getnframes()\nstrSig = s.readframes(nFrames)\nreturn np.fromstring(strSig, np.short).byteswap()", "path": "fileio.py", "repo_name": "nmkridler/moby", "stars": 55, "license": "None", "language": "python", "size": 508}
{"docstring": "\"\"\" Build a header\n\n\tBuild the header for the metrics\n\n\tArgs:\n\t\ttmpl: templateManager object\n\n\tReturns:\n\t\theader string as csv\n\"\"\"\n", "func_signal": "def buildHeader(tmpl,maxT=60):\n", "code": "hdr_ = []\nprefix_ = ['max','xLoc','yLoc']\nfor p_ in prefix_:\n\tfor i in range(tmpl.size):\n\t\thdr_.append(p_+'_%07d'%tmpl.info[i]['file'])\nfor p_ in prefix_:\n\tfor i in range(tmpl.size):\n\t\thdr_.append(p_+'H_%07d'%tmpl.info[i]['file'])\n\n# Add time metrics\nfor i in range(maxT):\n\thdr_ += ['centTime_%04d'%i]\nfor i in range(maxT):\n\thdr_ += ['bwTime_%04d'%i]\nfor i in range(maxT):\n\thdr_ += ['skewTime_%04d'%i]\nfor i in range(maxT):\n\thdr_ += ['tvTime_%04d'%i]\n\n# Add time metrics\nfor i in range(50):\n\thdr_ += ['centOops_%04d'%i]\n\n# Add high frequency metrics\nhdr_ += ['CentStd','AvgBwd','hfCent','hfBwd']\nhdr_ += ['hfMax','hfMax2','hfMax3']\nreturn ','.join(hdr_)", "path": "metrics.py", "repo_name": "nmkridler/moby", "stars": 55, "license": "None", "language": "python", "size": 508}
{"docstring": "\"\"\" Calculate statistics for a range of frequency slices\n\n\tCalculate centroid, width, skew, and total variation\n\t\tlet x = P[i,:], and t = time bins\n\t\tcentroid = sum(x*t)/sum(x)\n\t\twidth = sqrt(sum(x*(t-centroid)^2)/sum(x))\n\t\tskew = scipy.stats.skew\n\t\ttotal variation = sum(abs(x_i+1 - x_i))\n\n\tArgs:\n\t\tP: 2-d numpy array image\n\t\tb: time bins \n\n\tReturns:\n\t\tA list containing the statistics\n\n\"\"\"\n", "func_signal": "def timeMetrics(P, b,maxM=50):\n", "code": "m, n = P.shape\ncf_ = [np.sum(P[i,:]*b)/np.sum(P[i,:]) for i in range(maxM)]\nbw_ = [np.sum(P[i,:]*(b - cf_[i])*(b - cf_[i]))/np.sum(P[i,:]) for i in range(maxM)]\nsk_ = [skew(P[i,:]) for i in range(maxM)]\ntv_ = [np.sum(np.abs(P[i,1:] - P[i,:-1])) for i in range(maxM)]\nreturn cf_ + bw_ + sk_ + tv_", "path": "metrics.py", "repo_name": "nmkridler/moby", "stars": 55, "license": "None", "language": "python", "size": 508}
{"docstring": "\"\"\" Enhance the constrast\n\n\tCut off extreme values and demean the image\n\tUtilize scipy convolve2d to get the mean at a given pixel\n\tRemove local mean with inner exclusion region\n\n\tArgs:\n\t\tP: 2-d numpy array image\n\t\tinX: inner exclusion region in the x-dimension\n\t\toutX: length of the window in the x-dimension\n\t\tinY: inner exclusion region in the y-dimension\n\t\toutY: length of the window in the y-dimension\n\t\tmaxM: size of the output image in the y-dimension\n\t\tnorm: boolean to cut off extreme values\n\n\tReturns:\n\t\tQ: 2-d numpy contrast enhanced\n\"\"\"\n", "func_signal": "def slidingWindow(P,inX=3,outX=32,inY=3,outY=64,maxM=50,norm=True):\n", "code": "Q = P.copy()\nm, n = Q.shape\nif norm:\n\tmval, sval = np.mean(Q[:maxM,:]), np.std(Q[:maxM,:])\n\tfact_ = 1.5\n\tQ[Q > mval + fact_*sval] = mval + fact_*sval\n\tQ[Q < mval - fact_*sval] = mval - fact_*sval\nwInner = np.ones((inY,inX))\nwOuter = np.ones((outY,outX))\nQ = Q - (convolve2d(Q,wOuter,'same') - convolve2d(Q,wInner,'same'))/(wOuter.size - wInner.size)\nreturn Q[:maxM,:]", "path": "metrics.py", "repo_name": "nmkridler/moby", "stars": 55, "license": "None", "language": "python", "size": 508}
{"docstring": "\"\"\"Shortcut for the GetTransactionDetails method.\n    \nUse the TRANSACTIONID from DoAuthorization, DoDirectPayment or\nDoExpressCheckoutPayment for the ``transactionid``.\n\"\"\"\n", "func_signal": "def get_transaction_details(self, transactionid):\n", "code": "args = locals()\ndel args['self']\nreturn self._call('GetTransactionDetails', **args)", "path": "paypal\\interface.py", "repo_name": "patcoll/paypal-python", "stars": 56, "license": "None", "language": "python", "size": 101}
{"docstring": "\"\"\"Shortcut for the AddressVerify method.\n    \n``email``::\n    Email address of a PayPal member to verify.\n    Maximum string length: 255 single-byte characters\n    Input mask: ?@?.??\n``street``::\n    First line of the billing or shipping postal address to verify.\n    \n    To pass verification, the value of Street must match the first three\n    single-byte characters of a postal address on file for the PayPal member.\n    \n    Maximum string length: 35 single-byte characters.\n    Alphanumeric plus - , . \u2018 # \\\n    Whitespace and case of input value are ignored.\n``zip``::\n    Postal code to verify.\n    \n    To pass verification, the value of Zip mustmatch the first five\n    single-byte characters of the postal code of the verified postal\n    address for the verified PayPal member.\n    \n    Maximumstring length: 16 single-byte characters.\n    Whitespace and case of input value are ignored.\n\"\"\"\n", "func_signal": "def address_verify(self, email, street, zip):\n", "code": "args = locals()\ndel args['self']\nreturn self._call('AddressVerify', **args)", "path": "paypal\\interface.py", "repo_name": "patcoll/paypal-python", "stars": 56, "license": "None", "language": "python", "size": 101}
{"docstring": "\"\"\"Shortcut for the DoAuthorization method.\n    \nUse the TRANSACTIONID from DoExpressCheckoutPayment for the\n``transactionid``. The latest version of the API does not support the\ncreation of an Order from `DoDirectPayment`.\n    \nThe `amt` should be the same as passed to `DoExpressCheckoutPayment`.\n    \nFlow for a payment involving a `DoAuthorization` call::\n    \n     1. One or many calls to `SetExpressCheckout` with pertinent order\n        details, returns `TOKEN`\n     1. `DoExpressCheckoutPayment` with `TOKEN`, `PAYMENTACTION` set to\n        Order, `AMT` set to the amount of the transaction, returns\n        `TRANSACTIONID`\n     1. `DoAuthorization` with `TRANSACTIONID` and `AMT` set to the\n        amount of the transaction.\n     1. `DoCapture` with the `AUTHORIZATIONID` (the `TRANSACTIONID`\n        returned by `DoAuthorization`)\n    \n\"\"\"\n", "func_signal": "def do_authorization(self, transactionid, amt):\n", "code": "args = locals()\ndel args['self']\nreturn self._call('DoAuthorization', **args)", "path": "paypal\\interface.py", "repo_name": "patcoll/paypal-python", "stars": 56, "license": "None", "language": "python", "size": 101}
{"docstring": "\"\"\"\nMake sure response exception handling is working as intended by\nforcing some bad values.\n\"\"\"\n", "func_signal": "def test_exception_handling(self):\n", "code": "new_details = self.credit_card\n# Set an invalid credit card number.\nnew_details['acct'] = '123'\n# Make sure this raises an exception.\nself.assertRaises(PayPalAPIResponseError, interface.do_direct_payment, \n                  'Sale', **new_details)", "path": "tests\\t_direct_payment.py", "repo_name": "patcoll/paypal-python", "stars": 56, "license": "None", "language": "python", "size": 101}
{"docstring": "\"\"\"Shortcut for the DoCapture method.\n    \nUse the TRANSACTIONID from DoAuthorization, DoDirectPayment or\nDoExpressCheckoutPayment for the ``authorizationid``.\n    \nThe `amt` should be the same as the authorized transaction.\n\"\"\"\n", "func_signal": "def do_capture(self, authorizationid, amt, completetype='Complete', **kwargs):\n", "code": "kwargs.update(locals())\ndel kwargs['self']\nreturn self._call('DoCapture', **kwargs)", "path": "paypal\\interface.py", "repo_name": "patcoll/paypal-python", "stars": 56, "license": "None", "language": "python", "size": 101}
{"docstring": "\"\"\"Submit token, get redirect url for client.\"\"\"\n", "func_signal": "def generate_express_checkout_redirect_url(self, token):\n", "code": "url_vars = (self.config.PAYPAL_URL_BASE, token)\nreturn \"%s?cmd=_express-checkout&token=%s\" % url_vars", "path": "paypal\\interface.py", "repo_name": "patcoll/paypal-python", "stars": 56, "license": "None", "language": "python", "size": 101}
{"docstring": "\"\"\"\nTests a four-step checkout process involving the following flow::\n\n    One or more calls to `SetExpressCheckout`.\n    --- User goes to PayPal, logs in, and confirms shipping, taxes,\n        and total amount. ---\n    A call to `GetExpressCheckoutDetails`.\n    A call to `DoExpressCheckoutPayment`.\n    A call to `DoAuthorization`.\n    A call to `DoCapture`.\n\"\"\"\n", "func_signal": "def test_authorize_and_delayed_capture(self):\n", "code": "setexp = interface.set_express_checkout(amt='10.00', returnurl=self.returnurl, \\\n             cancelurl=self.cancelurl, paymentaction='Order', \\\n             email=api_details.EMAIL_PERSONAL)\nself.assertTrue(setexp.success)\n# print(setexp)\n# getexp = get_express_checkout_details(token=setexp.token)\n# print(getexp)", "path": "tests\\t_express_checkout.py", "repo_name": "patcoll/paypal-python", "stars": 56, "license": "None", "language": "python", "size": 101}
{"docstring": "\"\"\"https://www.sandbox.paypal.com/webscr \n    ?cmd=_cart\n    &upload=1\n\"\"\"\n", "func_signal": "def generate_cart_upload_redirect_url(self, **kwargs):\n", "code": "required_vals = ('business', 'item_name_1', 'amount_1', 'quantity_1')\nself._check_required(required_vals, **kwargs)\nurl = \"%s?cmd=_cart&upload=1\" % self.config.PAYPAL_URL_BASE\nadditional = self._encode_utf8(**kwargs)\nadditional = urllib.urlencode(additional)\nreturn url + \"&\" + additional", "path": "paypal\\interface.py", "repo_name": "patcoll/paypal-python", "stars": 56, "license": "None", "language": "python", "size": 101}
{"docstring": "\"\"\"\nConstructor, which passes all config directives to the config class\nvia kwargs. For example:\n\n    paypal = PayPalInterface(API_USERNAME='somevalue')\n    \nOptionally, you may pass a 'config' kwarg to provide your own\nPayPalConfig object.\n\"\"\"\n", "func_signal": "def __init__(self , config=None, **kwargs):\n", "code": "if config:\n    # User provided their own PayPalConfig object.\n    self.config = config\nelse:\n    # Take the kwargs and stuff them in a new PayPalConfig object.\n    self.config = PayPalConfig(**kwargs)", "path": "paypal\\interface.py", "repo_name": "patcoll/paypal-python", "stars": 56, "license": "None", "language": "python", "size": 101}
{"docstring": "# authorize payment\n", "func_signal": "def test_authorize_and_delayed_capture(self):\n", "code": "auth = interface.do_direct_payment('Authorization', **self.credit_card)\nself.assertTrue(auth.success)\nself.assertEqual(auth.AMT, self.credit_card['amt'])\n\n# capture payment\ncaptured = interface.do_capture(auth.TRANSACTIONID, auth.AMT)\nself.assertTrue(captured.success)\nself.assertEqual(auth.TRANSACTIONID, captured.PARENTTRANSACTIONID)\nself.assertEqual(captured.PAYMENTSTATUS.upper(), 'COMPLETED')\nself.assertEqual(captured.REASONCODE.upper(), 'NONE')", "path": "tests\\t_direct_payment.py", "repo_name": "patcoll/paypal-python", "stars": 56, "license": "None", "language": "python", "size": 101}
{"docstring": "\"\"\"Shortcut for the SetExpressCheckout method.\n    JV did not like the original method. found it limiting.\n\"\"\"\n", "func_signal": "def set_express_checkout(self, token='', **kwargs):\n", "code": "kwargs.update(locals())\ndel kwargs['self']\nself._check_required(('amt',), **kwargs)\nreturn self._call('SetExpressCheckout', **kwargs)", "path": "paypal\\interface.py", "repo_name": "patcoll/paypal-python", "stars": 56, "license": "None", "language": "python", "size": 101}
{"docstring": "# authorize payment\n", "func_signal": "def test_authorize_and_void(self):\n", "code": "auth = interface.do_direct_payment('Authorization', **self.credit_card)\nself.assertTrue(auth.success)\nself.assertEqual(auth.AMT, self.credit_card['amt'])\n\n# void payment\nnote = 'Voided the authorization.'\nvoid = interface.do_void(auth.TRANSACTIONID, note)\nself.assertTrue(void.success)\nself.assertEqual(auth.TRANSACTIONID, void.AUTHORIZATIONID)\n\ndetails = interface.get_transaction_details(auth.TRANSACTIONID)\nself.assertTrue(details.success)\nself.assertEqual(details.PAYMENTSTATUS.upper(), 'VOIDED')", "path": "tests\\t_direct_payment.py", "repo_name": "patcoll/paypal-python", "stars": 56, "license": "None", "language": "python", "size": 101}
{"docstring": "\"\"\"\nUTF8 encodes all of the NVP values.\n\"\"\"\n", "func_signal": "def _encode_utf8(self, **kwargs):\n", "code": "unencoded_pairs = kwargs\nfor i in unencoded_pairs.keys():\n    if isinstance(unencoded_pairs[i], types.UnicodeType):\n        unencoded_pairs[i] = unencoded_pairs[i].encode('utf-8')\nreturn unencoded_pairs", "path": "paypal\\interface.py", "repo_name": "patcoll/paypal-python", "stars": 56, "license": "None", "language": "python", "size": 101}
{"docstring": "\"\"\"Shortcut for the DoDirectPayment method.\n    \n``paymentaction`` could be 'Authorization' or 'Sale'\n    \nTo issue a Sale immediately::\n    \n    charge = {\n        'amt': '10.00',\n        'creditcardtype': 'Visa',\n        'acct': '4812177017895760',\n        'expdate': '012010',\n        'cvv2': '962',\n        'firstname': 'John',\n        'lastname': 'Doe',\n        'street': '1 Main St',\n        'city': 'San Jose',\n        'state': 'CA',\n        'zip': '95131',\n        'countrycode': 'US',\n        'currencycode': 'USD',\n    }\n    direct_payment(\"Sale\", **charge)\n    \nOr, since \"Sale\" is the default:\n    \n    direct_payment(**charge)\n    \nTo issue an Authorization, simply pass \"Authorization\" instead of \"Sale\".\n    \nYou may also explicitly set ``paymentaction`` as a keyword argument:\n    \n    ...\n    direct_payment(paymentaction=\"Sale\", **charge)\n\"\"\"\n", "func_signal": "def do_direct_payment(self, paymentaction=\"Sale\", **kwargs):\n", "code": "kwargs.update(locals())\ndel kwargs['self']\nreturn self._call('DoDirectPayment', **kwargs)", "path": "paypal\\interface.py", "repo_name": "patcoll/paypal-python", "stars": 56, "license": "None", "language": "python", "size": 101}
{"docstring": "\"\"\"\nChecks kwargs for the values specified in 'requires', which is a tuple\nof strings. These strings are the NVP names of the required values.\n\"\"\"\n", "func_signal": "def _check_required(self, requires, **kwargs):\n", "code": "for req in requires:\n    # PayPal api is never mixed-case.\n    if req.lower() not in kwargs and req.upper() not in kwargs:\n        raise PayPalError('missing required : %s' % req)", "path": "paypal\\interface.py", "repo_name": "patcoll/paypal-python", "stars": 56, "license": "None", "language": "python", "size": 101}
{"docstring": "\"\"\"Shortcut for the DoVoid method.\n    \nUse the TRANSACTIONID from DoAuthorization, DoDirectPayment or\nDoExpressCheckoutPayment for the ``authorizationid``.\n\"\"\"\n", "func_signal": "def do_void(self, authorizationid, note=''):\n", "code": "args = locals()\ndel args['self']\nreturn self._call('DoVoid', **args)", "path": "paypal\\interface.py", "repo_name": "patcoll/paypal-python", "stars": 56, "license": "None", "language": "python", "size": 101}
{"docstring": "\"\"\"\nHandles the retrieval of attributes that don't exist on the object\nalready. This is used to get API response values.\n\"\"\"\n# PayPal response names are always uppercase.\n", "func_signal": "def __getattr__(self, key):\n", "code": "key = key.upper()\ntry:\n    value = self.raw[key]\n    if len(value) == 1:\n        return value[0]\n    return value\nexcept KeyError:\n    if self.config.KEY_ERROR:\n        raise AttributeError(self)\n    else:\n        return None", "path": "paypal\\response.py", "repo_name": "patcoll/paypal-python", "stars": 56, "license": "None", "language": "python", "size": 101}
{"docstring": "\"\"\"\nquery_string is the response from the API, in NVP format. This is\nparseable by urlparse.parse_qs(), which sticks it into the self.raw\ndict for retrieval by the user.\n\"\"\"\n# A dict of NVP values. Don't access this directly, use\n# PayPalResponse.attribname instead. See self.__getattr__().\n", "func_signal": "def __init__(self, query_string, config):\n", "code": "self.raw = parse_qs(query_string)\nself.config = config", "path": "paypal\\response.py", "repo_name": "patcoll/paypal-python", "stars": 56, "license": "None", "language": "python", "size": 101}
{"docstring": "\"\"\"Shortcut for the DoExpressCheckoutPayment method.\n\n    Required\n        *METHOD\n        *TOKEN\n        PAYMENTACTION\n        PAYERID\n        AMT\n        \n    Optional\n        RETURNFMFDETAILS\n        GIFTMESSAGE\n        GIFTRECEIPTENABLE\n        GIFTWRAPNAME\n        GIFTWRAPAMOUNT\n        BUYERMARKETINGEMAIL\n        SURVEYQUESTION\n        SURVEYCHOICESELECTED\n        CURRENCYCODE\n        ITEMAMT\n        SHIPPINGAMT\n        INSURANCEAMT\n        HANDLINGAMT\n        TAXAMT\n\n    Optional + USEFUL\n        INVNUM - invoice number\n        \n\"\"\"\n", "func_signal": "def do_express_checkout_payment(self, token, **kwargs):\n", "code": "kwargs.update(locals())\ndel kwargs['self']\nself._check_required(('paymentaction', 'payerid'), **kwargs)\nreturn self._call('DoExpressCheckoutPayment', **kwargs)", "path": "paypal\\interface.py", "repo_name": "patcoll/paypal-python", "stars": 56, "license": "None", "language": "python", "size": 101}
{"docstring": "\"\"\"\nChecks for the presence of errors in the response. Returns True if\nall is well, False otherwise.\n\"\"\"\n", "func_signal": "def success(self):\n", "code": "return self.ack.upper() in (self.config.ACK_SUCCESS, \n                            self.config.ACK_SUCCESS_WITH_WARNING)", "path": "paypal\\response.py", "repo_name": "patcoll/paypal-python", "stars": 56, "license": "None", "language": "python", "size": 101}
{"docstring": "# a helpful error message for debugging\n", "func_signal": "def __init__(self, message, response=None):\n", "code": "self.message = message\n\n# if available, the associate response object\nself.response = response", "path": "flask_rauth.py", "repo_name": "joelverhagen/flask-rauth", "stars": 53, "license": "other", "language": "python", "size": 243}
{"docstring": "'''\nReturns the consumer_key for this object. The following method is used\nto determine what the consumer_key is:\n\n1. A `static_consumer_key`, set by passing a `consumer_key` to the\n    constructor.\n2. The `consumer_key` set in the config of an app passed to the\n    constructor. The application config key is based on the name\n    passed to the constructor. See :func:`init_app` for more\n    information.\n3. The `consumer_key` set in the config of the Flask `current_app`.\n'''\n", "func_signal": "def consumer_key(self):\n", "code": "if self.static_consumer_key is not None:\n    # if a consumer key was provided in the constructor, default to that\n    return self.static_consumer_key\nelif self.app is not None and self._consumer_key_config() in self.app.config:\n    # if an app was provided in the constructor, search its config first\n    return self.app.config[self._consumer_key_config()]\n\n# otherwise, search in the current_app config\nreturn current_app.config.get(self._consumer_key_config(), None)", "path": "flask_rauth.py", "repo_name": "joelverhagen/flask-rauth", "stars": 53, "license": "other", "language": "python", "size": 243}
{"docstring": "'''\nMake a request using an `access_token` obtained via the\n    :func:`authorized_handler`.\n\nIf no access_token is provided and a\n:func:`RauthServiceMixin.tokengetter` **was** provided, the\n:func:`RauthServiceMixin.tokengetter` will be called.\n\n:param method: Same as :func:`rauth.OAuth2Service.request`.\n:param url: Same as :func:`rauth.OAuth2Service.request`, except when a\n    `base_url` was provided to the constructor, in which case the URL\n    should be any valid endpoint after being :func:`urljoin` ed with\n    the `base_url`.\n:param access_token: The `access_token` required to make requests\n    against this service.\n:param kwargs: Any `kwargs` that can be passed to\n    :func:`OAuth2Service.request`.\n'''\n", "func_signal": "def request(self, method, url, access_token=None, **kwargs):\n", "code": "url = self._expand_url(url)\n\nif access_token is None and self.tokengetter_f is not None:\n    access_token = self.tokengetter_f()\n\n# add in the access_token\nif 'params' not in kwargs:\n    kwargs['params'] = {'access_token': access_token}\nelif 'access_token' not in kwargs['params']:\n    # TODO: handle if the user sends bytes -> properly append 'access_token'\n    kwargs['params']['access_token'] = access_token\n\n# call the parent implementation\nreturn RauthResponse(OAuth2Service.request(self, method, url, **kwargs))", "path": "flask_rauth.py", "repo_name": "joelverhagen/flask-rauth", "stars": 53, "license": "other", "language": "python", "size": 243}
{"docstring": "'''\nThe handler should expect two arguments: `response` and `oauth_token`.\nBy default, a `POST` request is used to fetch the access token. If you\nneed to send a `GET` request, use the\n``authorized_handler(method='GET')`` to do so.\n\nIf `response` is ``None`` then the user *most-likely* denied access\n    to his/her information. Since OAuth 1.0a does not specify a\n    standard query parameter to specify that the user denied the\n    authorization, you will need to figure out how the endpoint that\n    your are interacting with delineates this edge-case.\n'''\n", "func_signal": "def authorized_handler(self, method='POST'):\n", "code": "def create_authorized_handler(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        resp = oauth_token = None\n        if 'oauth_verifier' in request.args:\n            resp = RauthResponse(self.get_access_token(\n                method=method,\n                data={'oauth_verifier': request.args['oauth_verifier']},\n                **session.pop(self._session_key('request_token'), {}))\n            )\n\n            if resp.status != 200:\n                raise RauthException('An error occurred during OAuth 1.0a authorization', resp)\n\n            oauth_token = (resp.content.get('oauth_token'), resp.content.get('oauth_token_secret'))\n\n        return f(*((resp, oauth_token) + args), **kwargs)\n    return decorated\nreturn create_authorized_handler", "path": "flask_rauth.py", "repo_name": "joelverhagen/flask-rauth", "stars": 53, "license": "other", "language": "python", "size": 243}
{"docstring": "'''\nCalled after authorization. After this function finished handling,\nthe tokengetter from above is used to retrieve the 2-tuple containing the\noauth_token and oauth_token_secret.\n\nBecause reauthorization often changes any previous\noauth_token/oauth_token_secret values, then we must update them in the\ndatabase.\n\nIf the application redirected back after denying, the `resp` passed\nto the function will be `None`. Unfortunately, OAuth 1.0a (the version\nthat Twitter, LinkedIn, etc use) does not specify exactly what should\nhappen when the user denies access. In the case of Twitter, a query\nparameter `denied=(some hash)` is appended to the redirect URL.\n'''\n", "func_signal": "def authorized(resp, oauth_token):\n", "code": "next_url = request.args.get('next') or url_for('index')\n\n# check for the Twitter-specific \"access_denied\" indicator\nif resp is None and 'denied' in request.args:\n    flash(u'You denied the request to sign in.')\n    return redirect(next_url)\n\n# pull out the nicely parsed response content.\ncontent = resp.content\n\nuser = User.query.filter_by(name=content['screen_name']).first()\n\n# this if the first time signing in for this user\nif user is None:\n    user = User(content['screen_name'])\n    db_session.add(user)\n\n# we now update the oauth_token and oauth_token_secret\n# this involves destructuring the 2-tuple that is passed back from the\n#   Twitter API, so it can be easily stored in the SQL database\nuser.oauth_token = oauth_token[0]\nuser.oauth_secret = oauth_token[1]\ndb_session.commit()\n\nsession['user_id'] = user.id\nflash('You were signed in')\nreturn redirect(next_url)", "path": "example\\tweet.py", "repo_name": "joelverhagen/flask-rauth", "stars": 53, "license": "other", "language": "python", "size": 243}
{"docstring": "'''\nThe tokengetter decorator used to provide a function that will return\nthe required token before making a request.\n'''\n", "func_signal": "def tokengetter(self, f):\n", "code": "self.tokengetter_f = f\nreturn f", "path": "flask_rauth.py", "repo_name": "joelverhagen/flask-rauth", "stars": 53, "license": "other", "language": "python", "size": 243}
{"docstring": "# the original response\n", "func_signal": "def __init__(self, resp):\n", "code": "self.response = resp.response\n\nself._cached_content = None", "path": "flask_rauth.py", "repo_name": "joelverhagen/flask-rauth", "stars": 53, "license": "other", "language": "python", "size": 243}
{"docstring": "'''\nMake a request using an `oflyUserid` obtained via the\n    :func:`authorized_handler`.\n'''\n", "func_signal": "def request(self, method, url, oflyUserid=None, **kwargs):\n", "code": "url = self._expand_url(url)\n\nif oflyUserid is None and self.tokengetter_f is not None:\n    oflyUserid = self.tokengetter_f()\n\n# add in the access_token\nif 'params' not in kwargs:\n    kwargs['params'] = {'oflyUserid': oflyUserid}\nelif 'oflyUserid' not in kwargs['params']:\n    # TODO: handle if the user sends bytes -> properly append 'oflyUserid'\n    kwargs['params']['oflyUserid'] = oflyUserid\n\n# call the parent implementation\nreturn RauthResponse(OflyService.request(self, method, url, **kwargs))", "path": "flask_rauth.py", "repo_name": "joelverhagen/flask-rauth", "stars": 53, "license": "other", "language": "python", "size": 243}
{"docstring": "'''\nBegins the OAuth 1.0a authorization process for this service.\n\n:param callback: The **required** absolute URL that will be\n    redirected to by the OAuth 1.0 endpoint after authorization is\n    complete.\n:param request_params: Query parameters to be passed to the request,\n    token endpoint, in addition to the `callback`. One common example\n    is `scope`.\n'''\n# fetch the request_token (token and secret 2-tuple) and convert it to a dict\n", "func_signal": "def authorize(self, callback, **request_params):\n", "code": "request_token = self.get_request_token(oauth_callback=callback, **request_params)\nrequest_token = {'request_token': request_token[0], 'request_token_secret': request_token[1]}\n\n# save the request_token in the session\nsession[self._session_key('request_token')] = request_token\n\n# pass the token and any user-provided parameters\nreturn redirect(self.get_authorize_url(request_token['request_token']))", "path": "flask_rauth.py", "repo_name": "joelverhagen/flask-rauth", "stars": 53, "license": "other", "language": "python", "size": 243}
{"docstring": "'''\nInitializes the application with this object as an extension.\n\nThis simply ensures that there are `config` entries for keys generated\nby :func:`_consumer_key_config` and :func:`_consumer_secret_config`,\ni.e. ``(NAME)_CONSUMER_KEY`` and ``(NAME)_CONSUMER_SECRET``.\n\n:param app: A Flask application object.\n'''\n# the name attribute will be set by a rauth service\n", "func_signal": "def init_app(self, app):\n", "code": "app.config.setdefault(self._consumer_key_config())\napp.config.setdefault(self._consumer_secret_config())", "path": "flask_rauth.py", "repo_name": "joelverhagen/flask-rauth", "stars": 53, "license": "other", "language": "python", "size": 243}
{"docstring": "# prepend the base base_url, if we have it\n", "func_signal": "def _expand_url(self, url):\n", "code": "if self.base_url is not None:\n    url = urljoin(self.base_url, url)\nreturn url", "path": "flask_rauth.py", "repo_name": "joelverhagen/flask-rauth", "stars": 53, "license": "other", "language": "python", "size": 243}
{"docstring": "'''\nInspects a :class:`requests.Response` object and returns the content in a\nmore usable form. The following parsing checks are done:\n\n1. JSON, using the `json` attribute.\n2. XML, using :func:`get_etree`.\n3. RSS or Atom, using :mod:`feedparser`, if available.\n4. Query string, using :func:`parse_utf8_qsl`.\n5. If all else fails the plain-text `content` is returned.\n\n:param resp: A `requests.Response` object.\n'''\n", "func_signal": "def parse_response(resp):\n", "code": "if resp.json is not None:\n    return resp.json\n\nct, _ = parse_options_header(resp.headers.get('content-type'))\n\nif ct in ('application/xml', 'text/xml'):\n    etree = get_etree()\n    if etree is not None:\n        return etree.fromstring(resp.content)\n\nif ct in ('application/atom+xml', 'application/rss+xml'):\n    try:\n        import feedparser\n        return feedparser.parse(resp.content)\n    except:\n        pass\n\nif isinstance(resp.content, basestring):\n    return parse_utf8_qsl(resp.content)\n\nreturn resp.content", "path": "flask_rauth.py", "repo_name": "joelverhagen/flask-rauth", "stars": 53, "license": "other", "language": "python", "size": 243}
{"docstring": "'''\nThis is used by the API to look for the auth token and secret that are used\nfor Twitter API calls. If you don't want to store this in the database,\nconsider putting it into the session instead.\n\nSince the Twitter API is OAuth 1.0a, the `tokengetter` must return a\n2-tuple: (oauth_token, oauth_secret).\n'''\n", "func_signal": "def get_twitter_token():\n", "code": "user = g.user\nif user is not None:\n    return user.oauth_token, user.oauth_secret", "path": "example\\tweet.py", "repo_name": "joelverhagen/flask-rauth", "stars": 53, "license": "other", "language": "python", "size": 243}
{"docstring": "'''\nCalls the remote twitter API to create a new status update.\n'''\n", "func_signal": "def tweet():\n", "code": "if g.user is None:\n    return redirect(url_for('login', next=request.url))\nstatus = request.form['tweet']\nif not status:\n    return redirect(url_for('index'))\nresp = twitter.post('statuses/update.json', data={\n    'status': status\n})\nif resp.status == 403:\n    flash('Your tweet was too long.')\nelif resp.status == 401:\n    flash('Authorization error with Twitter.')\nelse:\n    flash('Successfully tweeted your tweet (ID: #%s)' % resp.content['id'])\nreturn redirect(url_for('index'))", "path": "example\\tweet.py", "repo_name": "joelverhagen/flask-rauth", "stars": 53, "license": "other", "language": "python", "size": 243}
{"docstring": "'''\nCalling into `authorize` will cause the OAuth 1.0a machinery to kick\nin. If all has worked out as expected or if the user denied access to\nhis/her information, the remote application will redirect back to the callback URL\nprovided.\n\nInt our case, the 'authorized/' route handles the interaction after the redirect.\n'''\n", "func_signal": "def login():\n", "code": "return twitter.authorize(callback=url_for('authorized',\n    _external=True,\n    next=request.args.get('next') or request.referrer or None))", "path": "example\\tweet.py", "repo_name": "joelverhagen/flask-rauth", "stars": 53, "license": "other", "language": "python", "size": 243}
{"docstring": "'''\nThe decorator to assign a function that will be called after\nauthorization is complete. By default, a `POST` request is used to\nfetch the access token. If you need to send a `GET` request, use the\n``authorized_handler(method='GET')`` to do so.\n\nIt should be a route that takes two parameters: `response` and\n`access_token`.\n\nIf `response` is ``access_denied``, then the user denied access to\n    his/her information.\n'''\n", "func_signal": "def authorized_handler(self, method='POST'):\n", "code": "def create_authorized_handler(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        resp = access_token = None\n        if 'error' in request.args and request.args['error'] == ACCESS_DENIED:\n            resp = ACCESS_DENIED\n        elif 'code' in request.args:\n            resp = RauthResponse(self.get_access_token(method=method, data={\n                'code': request.args['code'],\n                'redirect_uri': session.pop(self._session_key('redirect_uri'), None)\n            }))\n\n            if resp.status != 200:\n                raise RauthException('An error occurred while getting the OAuth 2.0 access_token', resp)\n\n            access_token = resp.content.get('access_token')\n\n        return f(*((resp, access_token) + args), **kwargs)\n    return decorated\nreturn create_authorized_handler", "path": "flask_rauth.py", "repo_name": "joelverhagen/flask-rauth", "stars": 53, "license": "other", "language": "python", "size": 243}
{"docstring": "'''\nThe handler should expect two arguments: `response` and `oflyUserid`.\nThe `method` parameter is unused.\n\nIf `response` is ``access_denied``, then the user denied access to\n    his/her information.\n'''\n", "func_signal": "def authorized_handler(self, method='POST'):\n", "code": "def create_authorized_handler(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        resp = oflyUserid = None\n        if 'oflyUserid' in request.args:\n            if request.args['oflyUserid'] == 'no-grant':\n                resp = ACCESS_DENIED\n            else:\n                resp = {\n                    'oflyUserid': request.args['oflyUserid'],\n                    'oflyAppId': request.args.get('oflyAppId'),\n                    'oflyUserEmail': request.args.get('oflyUserEmail')\n                }\n\n                oflyUserid = request.args['oflyUserid']\n\n        return f(*((resp, oflyUserid) + args), **kwargs)\n    return decorated\nreturn create_authorized_handler", "path": "flask_rauth.py", "repo_name": "joelverhagen/flask-rauth", "stars": 53, "license": "other", "language": "python", "size": 243}
{"docstring": "'''\nThe content associated with the response. The content is parsed into a\nmore useful format, if possible, using :func:`parse_response`.\n\nThe content is cached, so that :func:`parse_response` is only run once.\n'''\n", "func_signal": "def content(self):\n", "code": "if self._cached_content is None:\n    # the parsed content from the server\n    self._cached_content = parse_response(self.response)\nreturn self._cached_content", "path": "flask_rauth.py", "repo_name": "joelverhagen/flask-rauth", "stars": 53, "license": "other", "language": "python", "size": 243}
{"docstring": "'''\nMake a request using an `oauth_token` obtained via the\n    :func:`authorized_handler`.\n'''\n", "func_signal": "def request(self, method, url, oauth_token=None, **kwargs):\n", "code": "url = self._expand_url(url)\n\nif oauth_token is None and self.tokengetter_f is not None:\n    oauth_token = self.tokengetter_f()\n\n# take apart the 2-tuple\nif oauth_token is not None:\n    oauth_token, oauth_token_secret = oauth_token\nelse:\n    oauth_token_secret = None\n\n# call the parent implementation\nreturn RauthResponse(OAuth1Service.request(self, method, url, access_token=oauth_token, access_token_secret=oauth_token_secret, **kwargs))", "path": "flask_rauth.py", "repo_name": "joelverhagen/flask-rauth", "stars": 53, "license": "other", "language": "python", "size": 243}
{"docstring": "'''\nBegins the OAuth 2.0 authorization process for this service.\n\n:param callback: The **required** absolute URL that will be\n    redirected to by the OAuth 2.0 endpoint after authorization is\n    complete.\n:param authorize_params: Query parameters to be passed to authorization,\n    prompt, addition to the `redirect_uri`. One common example is\n    `scope`.\n'''\n# save the redirect_uri in the session\n", "func_signal": "def authorize(self, callback, **authorize_params):\n", "code": "session[self._session_key('redirect_uri')] = callback\n\nreturn redirect(self.get_authorize_url(redirect_uri=callback, **authorize_params))", "path": "flask_rauth.py", "repo_name": "joelverhagen/flask-rauth", "stars": 53, "license": "other", "language": "python", "size": 243}
{"docstring": "\"\"\"Compute the size of a varint value.\"\"\"\n", "func_signal": "def _VarintSize(value):\n", "code": "if value <= 0x7f: return 1\nif value <= 0x3fff: return 2\nif value <= 0x1fffff: return 3\nif value <= 0xfffffff: return 4\nif value <= 0x7ffffffff: return 5\nif value <= 0x3ffffffffff: return 6\nif value <= 0x1ffffffffffff: return 7\nif value <= 0xffffffffffffff: return 8\nif value <= 0x7fffffffffffffff: return 9\nreturn 10", "path": "libs\\proto\\google\\protobuf\\internal\\encoder.py", "repo_name": "zku/Diablo-III-Protocol-Simulator", "stars": 39, "license": "None", "language": "python", "size": 334}
{"docstring": "\"\"\"Like _SimpleSizer except for a fixed-size field.  The input is the size\nof one value.\"\"\"\n\n", "func_signal": "def _FixedSizer(value_size):\n", "code": "def SpecificSizer(field_number, is_repeated, is_packed):\n  tag_size = _TagSize(field_number)\n  if is_packed:\n    local_VarintSize = _VarintSize\n    def PackedFieldSize(value):\n      result = len(value) * value_size\n      return result + local_VarintSize(result) + tag_size\n    return PackedFieldSize\n  elif is_repeated:\n    element_size = value_size + tag_size\n    def RepeatedFieldSize(value):\n      return len(value) * element_size\n    return RepeatedFieldSize\n  else:\n    field_size = value_size + tag_size\n    def FieldSize(value):\n      return field_size\n    return FieldSize\n\nreturn SpecificSizer", "path": "libs\\proto\\google\\protobuf\\internal\\encoder.py", "repo_name": "zku/Diablo-III-Protocol-Simulator", "stars": 39, "license": "None", "language": "python", "size": 334}
{"docstring": "\"\"\"Compares the current instance with another one.\"\"\"\n", "func_signal": "def __eq__(self, other):\n", "code": "if self is other:\n  return True\n# Special case for the same type which should be common and fast.\nif isinstance(other, self.__class__):\n  return other._values == self._values\n# We are presumably comparing against some other sequence type.\nreturn other == self._values", "path": "libs\\proto\\google\\protobuf\\internal\\containers.py", "repo_name": "zku/Diablo-III-Protocol-Simulator", "stars": 39, "license": "None", "language": "python", "size": 334}
{"docstring": "\"\"\"Deletes the subset of items from between the specified indices.\"\"\"\n", "func_signal": "def __delslice__(self, start, stop):\n", "code": "del self._values[start:stop]\nself._message_listener.Modified()", "path": "libs\\proto\\google\\protobuf\\internal\\containers.py", "repo_name": "zku/Diablo-III-Protocol-Simulator", "stars": 39, "license": "None", "language": "python", "size": 334}
{"docstring": "\"\"\"Appends the contents of another repeated field of the same type to this\none. We do not check the types of the individual fields.\n\"\"\"\n", "func_signal": "def MergeFrom(self, other):\n", "code": "self._values.extend(other._values)\nself._message_listener.Modified()", "path": "libs\\proto\\google\\protobuf\\internal\\containers.py", "repo_name": "zku/Diablo-III-Protocol-Simulator", "stars": 39, "license": "None", "language": "python", "size": 334}
{"docstring": "\"\"\"A sizer which uses the function compute_value_size to compute the size of\neach value.  Typically compute_value_size is _VarintSize.\"\"\"\n\n", "func_signal": "def _SimpleSizer(compute_value_size):\n", "code": "def SpecificSizer(field_number, is_repeated, is_packed):\n  tag_size = _TagSize(field_number)\n  if is_packed:\n    local_VarintSize = _VarintSize\n    def PackedFieldSize(value):\n      result = 0\n      for element in value:\n        result += compute_value_size(element)\n      return result + local_VarintSize(result) + tag_size\n    return PackedFieldSize\n  elif is_repeated:\n    def RepeatedFieldSize(value):\n      result = tag_size * len(value)\n      for element in value:\n        result += compute_value_size(element)\n      return result\n    return RepeatedFieldSize\n  else:\n    def FieldSize(value):\n      return tag_size + compute_value_size(value)\n    return FieldSize\n\nreturn SpecificSizer", "path": "libs\\proto\\google\\protobuf\\internal\\encoder.py", "repo_name": "zku/Diablo-III-Protocol-Simulator", "stars": 39, "license": "None", "language": "python", "size": 334}
{"docstring": "\"\"\"Return an encoder for a basic varint value (does not include tag).\"\"\"\n\n", "func_signal": "def _VarintEncoder():\n", "code": "local_chr = chr\ndef EncodeVarint(write, value):\n  bits = value & 0x7f\n  value >>= 7\n  while value:\n    write(local_chr(0x80|bits))\n    bits = value & 0x7f\n    value >>= 7\n  return write(local_chr(bits))\n\nreturn EncodeVarint", "path": "libs\\proto\\google\\protobuf\\internal\\encoder.py", "repo_name": "zku/Diablo-III-Protocol-Simulator", "stars": 39, "license": "None", "language": "python", "size": 334}
{"docstring": "\"\"\"Sets the subset of items from between the specified indices.\"\"\"\n", "func_signal": "def __setslice__(self, start, stop, values):\n", "code": "new_values = []\nfor value in values:\n  self._type_checker.CheckValue(value)\n  new_values.append(value)\nself._values[start:stop] = new_values\nself._message_listener.Modified()", "path": "libs\\proto\\google\\protobuf\\internal\\containers.py", "repo_name": "zku/Diablo-III-Protocol-Simulator", "stars": 39, "license": "None", "language": "python", "size": 334}
{"docstring": "\"\"\"Compute the size of a signed varint value.\"\"\"\n", "func_signal": "def _SignedVarintSize(value):\n", "code": "if value < 0: return 10\nif value <= 0x7f: return 1\nif value <= 0x3fff: return 2\nif value <= 0x1fffff: return 3\nif value <= 0xfffffff: return 4\nif value <= 0x7ffffffff: return 5\nif value <= 0x3ffffffffff: return 6\nif value <= 0x1ffffffffffff: return 7\nif value <= 0xffffffffffffff: return 8\nif value <= 0x7fffffffffffffff: return 9\nreturn 10", "path": "libs\\proto\\google\\protobuf\\internal\\encoder.py", "repo_name": "zku/Diablo-III-Protocol-Simulator", "stars": 39, "license": "None", "language": "python", "size": 334}
{"docstring": "\"\"\"Encode the given integer as a varint and return the bytes.  This is only\ncalled at startup time so it doesn't need to be fast.\"\"\"\n\n", "func_signal": "def _VarintBytes(value):\n", "code": "pieces = []\n_EncodeVarint(pieces.append, value)\nreturn \"\".join(pieces)", "path": "libs\\proto\\google\\protobuf\\internal\\encoder.py", "repo_name": "zku/Diablo-III-Protocol-Simulator", "stars": 39, "license": "None", "language": "python", "size": 334}
{"docstring": "\"\"\"Return an encoder for a basic signed varint value (does not include\ntag).\"\"\"\n\n", "func_signal": "def _SignedVarintEncoder():\n", "code": "local_chr = chr\ndef EncodeSignedVarint(write, value):\n  if value < 0:\n    value += (1 << 64)\n  bits = value & 0x7f\n  value >>= 7\n  while value:\n    write(local_chr(0x80|bits))\n    bits = value & 0x7f\n    value >>= 7\n  return write(local_chr(bits))\n\nreturn EncodeSignedVarint", "path": "libs\\proto\\google\\protobuf\\internal\\encoder.py", "repo_name": "zku/Diablo-III-Protocol-Simulator", "stars": 39, "license": "None", "language": "python", "size": 334}
{"docstring": "\"\"\"Copies the content of the specified message into the current message.\n\nThe method clears the current message and then merges the specified\nmessage using MergeFrom.\n\nArgs:\n  other_msg: Message to copy into the current one.\n\"\"\"\n", "func_signal": "def CopyFrom(self, other_msg):\n", "code": "if self is other_msg:\n  return\nself.Clear()\nself.MergeFrom(other_msg)", "path": "libs\\proto\\google\\protobuf\\message.py", "repo_name": "zku/Diablo-III-Protocol-Simulator", "stars": 39, "license": "None", "language": "python", "size": 334}
{"docstring": "\"\"\"Like SimpleEncoder but additionally invokes modify_value on every value\nbefore passing it to encode_value.  Usually modify_value is ZigZagEncode.\"\"\"\n\n", "func_signal": "def _ModifiedEncoder(wire_type, encode_value, compute_value_size, modify_value):\n", "code": "def SpecificEncoder(field_number, is_repeated, is_packed):\n  if is_packed:\n    tag_bytes = TagBytes(field_number, wire_format.WIRETYPE_LENGTH_DELIMITED)\n    local_EncodeVarint = _EncodeVarint\n    def EncodePackedField(write, value):\n      write(tag_bytes)\n      size = 0\n      for element in value:\n        size += compute_value_size(modify_value(element))\n      local_EncodeVarint(write, size)\n      for element in value:\n        encode_value(write, modify_value(element))\n    return EncodePackedField\n  elif is_repeated:\n    tag_bytes = TagBytes(field_number, wire_type)\n    def EncodeRepeatedField(write, value):\n      for element in value:\n        write(tag_bytes)\n        encode_value(write, modify_value(element))\n    return EncodeRepeatedField\n  else:\n    tag_bytes = TagBytes(field_number, wire_type)\n    def EncodeField(write, value):\n      write(tag_bytes)\n      return encode_value(write, modify_value(value))\n    return EncodeField\n\nreturn SpecificEncoder", "path": "libs\\proto\\google\\protobuf\\internal\\encoder.py", "repo_name": "zku/Diablo-III-Protocol-Simulator", "stars": 39, "license": "None", "language": "python", "size": 334}
{"docstring": "\"\"\"Encoder for extensions of MessageSet.\n\nThe message set message looks like this:\n  message MessageSet {\n    repeated group Item = 1 {\n      required int32 type_id = 2;\n      required string message = 3;\n    }\n  }\n\"\"\"\n", "func_signal": "def MessageSetItemEncoder(field_number):\n", "code": "start_bytes = \"\".join([\n    TagBytes(1, wire_format.WIRETYPE_START_GROUP),\n    TagBytes(2, wire_format.WIRETYPE_VARINT),\n    _VarintBytes(field_number),\n    TagBytes(3, wire_format.WIRETYPE_LENGTH_DELIMITED)])\nend_bytes = TagBytes(1, wire_format.WIRETYPE_END_GROUP)\nlocal_EncodeVarint = _EncodeVarint\n\ndef EncodeField(write, value):\n  write(start_bytes)\n  local_EncodeVarint(write, value.ByteSize())\n  value._InternalSerialize(write)\n  return write(end_bytes)\n\nreturn EncodeField", "path": "libs\\proto\\google\\protobuf\\internal\\encoder.py", "repo_name": "zku/Diablo-III-Protocol-Simulator", "stars": 39, "license": "None", "language": "python", "size": 334}
{"docstring": "\"\"\"\nNote that we pass in a descriptor instead of the generated directly,\nsince at the time we construct a _RepeatedCompositeFieldContainer we\nhaven't yet necessarily initialized the type that will be contained in the\ncontainer.\n\nArgs:\n  message_listener: A MessageListener implementation.\n    The RepeatedCompositeFieldContainer will call this object's\n    Modified() method when it is modified.\n  message_descriptor: A Descriptor instance describing the protocol type\n    that should be present in this container.  We'll use the\n    _concrete_class field of this descriptor when the client calls add().\n\"\"\"\n", "func_signal": "def __init__(self, message_listener, message_descriptor):\n", "code": "super(RepeatedCompositeFieldContainer, self).__init__(message_listener)\nself._message_descriptor = message_descriptor", "path": "libs\\proto\\google\\protobuf\\internal\\containers.py", "repo_name": "zku/Diablo-III-Protocol-Simulator", "stars": 39, "license": "None", "language": "python", "size": 334}
{"docstring": "\"\"\"Returns an encoder for a boolean field.\"\"\"\n\n", "func_signal": "def BoolEncoder(field_number, is_repeated, is_packed):\n", "code": "false_byte = chr(0)\ntrue_byte = chr(1)\nif is_packed:\n  tag_bytes = TagBytes(field_number, wire_format.WIRETYPE_LENGTH_DELIMITED)\n  local_EncodeVarint = _EncodeVarint\n  def EncodePackedField(write, value):\n    write(tag_bytes)\n    local_EncodeVarint(write, len(value))\n    for element in value:\n      if element:\n        write(true_byte)\n      else:\n        write(false_byte)\n  return EncodePackedField\nelif is_repeated:\n  tag_bytes = TagBytes(field_number, wire_format.WIRETYPE_VARINT)\n  def EncodeRepeatedField(write, value):\n    for element in value:\n      write(tag_bytes)\n      if element:\n        write(true_byte)\n      else:\n        write(false_byte)\n  return EncodeRepeatedField\nelse:\n  tag_bytes = TagBytes(field_number, wire_format.WIRETYPE_VARINT)\n  def EncodeField(write, value):\n    write(tag_bytes)\n    if value:\n      return write(true_byte)\n    return write(false_byte)\n  return EncodeField", "path": "libs\\proto\\google\\protobuf\\internal\\encoder.py", "repo_name": "zku/Diablo-III-Protocol-Simulator", "stars": 39, "license": "None", "language": "python", "size": 334}
{"docstring": "\"\"\"Sets the item on the specified position.\"\"\"\n", "func_signal": "def __setitem__(self, key, value):\n", "code": "self._type_checker.CheckValue(value)\nself._values[key] = value\nself._message_listener.Modified()", "path": "libs\\proto\\google\\protobuf\\internal\\containers.py", "repo_name": "zku/Diablo-III-Protocol-Simulator", "stars": 39, "license": "None", "language": "python", "size": 334}
{"docstring": "\"\"\"Compares the current instance with another one.\"\"\"\n", "func_signal": "def __eq__(self, other):\n", "code": "if self is other:\n  return True\nif not isinstance(other, self.__class__):\n  raise TypeError('Can only compare repeated composite fields against '\n                  'other repeated composite fields.')\nreturn self._values == other._values", "path": "libs\\proto\\google\\protobuf\\internal\\containers.py", "repo_name": "zku/Diablo-III-Protocol-Simulator", "stars": 39, "license": "None", "language": "python", "size": 334}
{"docstring": "\"\"\"Returns a sizer for a message field.\"\"\"\n\n", "func_signal": "def MessageSizer(field_number, is_repeated, is_packed):\n", "code": "tag_size = _TagSize(field_number)\nlocal_VarintSize = _VarintSize\nassert not is_packed\nif is_repeated:\n  def RepeatedFieldSize(value):\n    result = tag_size * len(value)\n    for element in value:\n      l = element.ByteSize()\n      result += local_VarintSize(l) + l\n    return result\n  return RepeatedFieldSize\nelse:\n  def FieldSize(value):\n    l = value.ByteSize()\n    return tag_size + local_VarintSize(l) + l\n  return FieldSize", "path": "libs\\proto\\google\\protobuf\\internal\\encoder.py", "repo_name": "zku/Diablo-III-Protocol-Simulator", "stars": 39, "license": "None", "language": "python", "size": 334}
{"docstring": "\"\"\"Returns a sizer for a string field.\"\"\"\n\n", "func_signal": "def StringSizer(field_number, is_repeated, is_packed):\n", "code": "tag_size = _TagSize(field_number)\nlocal_VarintSize = _VarintSize\nlocal_len = len\nassert not is_packed\nif is_repeated:\n  def RepeatedFieldSize(value):\n    result = tag_size * len(value)\n    for element in value:\n      l = local_len(element.encode('utf-8'))\n      result += local_VarintSize(l) + l\n    return result\n  return RepeatedFieldSize\nelse:\n  def FieldSize(value):\n    l = local_len(value.encode('utf-8'))\n    return tag_size + local_VarintSize(l) + l\n  return FieldSize", "path": "libs\\proto\\google\\protobuf\\internal\\encoder.py", "repo_name": "zku/Diablo-III-Protocol-Simulator", "stars": 39, "license": "None", "language": "python", "size": 334}
{"docstring": "\"\"\"\nThe integer index of the last matched capturing group, or `None`\nif no group was matched at all.  For example, the expressions\n``(a)b``, ``((a)(b))``, and ``((ab))`` will have ``lastindex == 1``\nif applied to the string ``'ab'``, while the expression ``(a)(b)``\nwill have ``lastindex == 2``, if applied to the same string.\n\"\"\"\n", "func_signal": "def lastindex(self):\n", "code": "m = (None, -1)\nfor idx, (start, end) in enumerate(self.spans[1:]):\n    if end > m[1]:\n        m = (idx + 1, end)\nreturn m[0]", "path": "ponyguruma\\_highlevel.py", "repo_name": "mitsuhiko/ponyguruma", "stars": 32, "license": "None", "language": "python", "size": 177}
{"docstring": "\"\"\"\nSplit string by the occurrences of the pattern.  If capturing\nparentheses are used in pattern, then the text of all groups\nin the pattern are also returned as part of the resulting list.\nIf `maxsplit` is nonzero, at most maxsplit splits occur, and the\nremainder of the string is returned as the final element of the list.\n\nUnless there are captured groups in the pattern the delimiter is not\npart of the returned list.  Groups will appear in the result as a\ntuple of the groups, even if only one group is present.  If you set\n`flat` to `True` the tuples will be merged into the list so that all\ngroups become part of the result as strings.\n\"\"\"\n", "func_signal": "def split(self, string, maxsplit=0, pos=0, endpos=-1, flat=False):\n", "code": "result = []\nstartstring = string[:pos]\nn = 0\npush_match = (flag and result.append or result.extend)\nwhile 1:\n    state = regexp_match(self, string, pos, endpos, False)\n    if state is None:\n        break\n    n += 1\n    m = Match(state)\n    result.append(string[pos:m.start()])\n    if len(m):\n        push_match(m.groups)\n    pos = m.end()\n    if n == maxsplit:\n        break\nresult.append(string[pos:])\nresult[0] = startstring + result[0]\nreturn result", "path": "ponyguruma\\_highlevel.py", "repo_name": "mitsuhiko/ponyguruma", "stars": 32, "license": "None", "language": "python", "size": 177}
{"docstring": "\"\"\"\nIf zero or more characters at the beginning of `string` match\nthe regular expression pattern, return a corresponding `Match`\ninstance.  Return `None` if the string does not match the pattern;\nnote that this is different from a zero-length match.\n\nIf pos is given it will start matching at exactly that position,\nif endpos is a positive integer it will stop matching at that\nposition.\n\n**Note:** If you want to locate a match anywhere in `string`\nyou should use `search` instead.\n\"\"\"\n", "func_signal": "def match(self, string, pos=0, endpos=-1):\n", "code": "state = regexp_match(self, string, pos, endpos, True)\nif state is not None:\n    return Match(state)", "path": "ponyguruma\\_highlevel.py", "repo_name": "mitsuhiko/ponyguruma", "stars": 32, "license": "None", "language": "python", "size": 177}
{"docstring": "\"\"\"Reset the scanner.\"\"\"\n", "func_signal": "def reset(self):\n", "code": "self.pos = 0\nself.old_pos = None\nself.end = len(self.string)\nself.match = None", "path": "ponyguruma\\_highlevel.py", "repo_name": "mitsuhiko/ponyguruma", "stars": 32, "license": "None", "language": "python", "size": 177}
{"docstring": "\"\"\"Escape all non-alphanumeric characters in pattern.\"\"\"\n", "func_signal": "def escape(pattern):\n", "code": "s = list(pattern)\nfor i, c in enumerate(s):\n    if not ('a' <= c <= 'z' or 'A' <= c <= 'Z' or '0' <= c <= '9'):\n        if c in _special_escapes:\n            s[i] = _special_escapes[c]\n        else:\n            s[i] = '\\\\' + c\nreturn type(pattern)().join(s)", "path": "ponyguruma\\_highlevel.py", "repo_name": "mitsuhiko/ponyguruma", "stars": 32, "license": "None", "language": "python", "size": 177}
{"docstring": "\"\"\"\nThe span of a single group.  Group can be a string if it's a\nnamed group, otherwise an integer.  If you omit the value the\nspan of the whole match is returned.\n\"\"\"\n", "func_signal": "def span(self, group=0):\n", "code": "if isinstance(group, basestring):\n    group = self.groupnames[group]\nreturn self.spans[group]", "path": "ponyguruma\\_highlevel.py", "repo_name": "mitsuhiko/ponyguruma", "stars": 32, "license": "None", "language": "python", "size": 177}
{"docstring": "\"\"\"Add a new string to the string that is scanned.\"\"\"\n", "func_signal": "def feed(self, string):\n", "code": "self.string += string\nself.end = len(self.string)", "path": "ponyguruma\\_highlevel.py", "repo_name": "mitsuhiko/ponyguruma", "stars": 32, "license": "None", "language": "python", "size": 177}
{"docstring": "\"\"\"\nGet the next character as string or `None` if end is reached.\n\"\"\"\n", "func_signal": "def getch(self):\n", "code": "rv = self.match(r'(?:.|\\n)')\nif rv is not None:\n    return rv.group()", "path": "ponyguruma\\_highlevel.py", "repo_name": "mitsuhiko/ponyguruma", "stars": 32, "license": "None", "language": "python", "size": 177}
{"docstring": "\"\"\"\nExpand a template string.\n\"\"\"\n", "func_signal": "def expand(self, template):\n", "code": "def handle(match):\n    numeric, named = match.groups\n    if numeric:\n        return self.group(int(numeric))\n    return self.group(named)\nreturn _repl_re.sub(handle, template)", "path": "ponyguruma\\_highlevel.py", "repo_name": "mitsuhiko/ponyguruma", "stars": 32, "license": "None", "language": "python", "size": 177}
{"docstring": "\"\"\"\nThis returns the value that `match` would return, without advancing the scan\npointer.  Also the match register is not updated.\n\"\"\"\n", "func_signal": "def check(self, regexp):\n", "code": "if regexp in self._cache:\n    re = self._cache[regexp]\nelse:\n    re = self._cache[regexp] = Regexp(regexp)\nreturn re.match(self.string, self.pos)", "path": "ponyguruma\\_highlevel.py", "repo_name": "mitsuhiko/ponyguruma", "stars": 32, "license": "None", "language": "python", "size": 177}
{"docstring": "\"\"\"\nGo one position back. Only one is allowed.\n\"\"\"\n", "func_signal": "def rewind(self):\n", "code": "if self.old_pos is None:\n    if self.pos == 0:\n        raise RuntimeError('Cannot rewind beyond start position')\n    raise RuntimeError('Cannot rewind more than one position back')\nself.pos = self.old_pos\nself.old_pos = None", "path": "ponyguruma\\_highlevel.py", "repo_name": "mitsuhiko/ponyguruma", "stars": 32, "license": "None", "language": "python", "size": 177}
{"docstring": "\"\"\"\nReturn an iterator yielding `Match` instances over all\nnon-overlapping matches for the pattern in string.  Empty matches\nare included in the result unless they touch the beginning of\nanother match.\n\"\"\"\n", "func_signal": "def find(self, string, pos=0, endpos=-1):\n", "code": "while 1:\n    state = regexp_match(self, string, pos, endpos, False)\n    if state is None:\n        return\n    m = Match(state)\n    pos = m.end()\n    yield m", "path": "ponyguruma\\_highlevel.py", "repo_name": "mitsuhiko/ponyguruma", "stars": 32, "license": "None", "language": "python", "size": 177}
{"docstring": "\"\"\"\nThe name of the last matched capturing group, or `None` if the\ngroup didn't have a name, or if no group was matched at all.\n\"\"\"\n", "func_signal": "def lastgroup(self):\n", "code": "g = self.lastindex\nif g is not None:\n    for name, index in self.groupnames.iteritems():\n        if index == g:\n            return name", "path": "ponyguruma\\_highlevel.py", "repo_name": "mitsuhiko/ponyguruma", "stars": 32, "license": "None", "language": "python", "size": 177}
{"docstring": "\"\"\"\nWorks like `scan` but scans not only at the beginning but it skips until the\npattern matches.  If the pattern does not match the return value is `None`\nand neither the pointer no the match register is updated.  Otherwise the\nreturn value is the string skipped and the match register points to the\nused match object.\n\"\"\"\n", "func_signal": "def search(self, regexp):\n", "code": "if regexp in self._cache:\n    re = self._cache[regexp]\nelse:\n    re = self._cache[regexp] = Regexp(regexp)\nmatch = re.search(regexp, self.pos)\nif match is not None:\n    self.old_pos = start = self.pos\n    self.pos = end = match.end()\n    self.match = match\n    return self.string[start:end]", "path": "ponyguruma\\_highlevel.py", "repo_name": "mitsuhiko/ponyguruma", "stars": 32, "license": "None", "language": "python", "size": 177}
{"docstring": "\"\"\"\nScan through `string` looking for a location where the regular\nexpression pattern produces a match, and return a corresponding\n`Match` instance.  Return `None` if no position in the string\nmatches the pattern; note that this is different from finding a\nzero-length match at some point in the string.\n\nThe pos and endpos parameters can be used to limit the search range.\n\"\"\"\n", "func_signal": "def search(self, string, pos=0, endpos=-1):\n", "code": "state = regexp_match(self, string, pos, endpos, False)\nif state is not None:\n    return Match(state)", "path": "ponyguruma\\_highlevel.py", "repo_name": "mitsuhiko/ponyguruma", "stars": 32, "license": "None", "language": "python", "size": 177}
{"docstring": "\"\"\"\nPerform the same operation as `sub()`, but return a tuple\n``(new_string, number_of_subs_made)``.\n\"\"\"\n", "func_signal": "def subn(self, repl, string, count=0, pos=0, endpos=-1):\n", "code": "new = [string[:pos]]\nif not callable(repl):\n    if '\\\\' in repl:\n        repl = lambda m, r=repl: m.expand(r)\n    else:\n        repl = lambda m, r=repl: r\nif endpos == -1:\n    endpos = len(string)\nn = skipped = 0\nwhile 1:\n    state = regexp_match(self, string, pos, endpos, False)\n    if state is None:\n        break\n    n += 1\n    m = Match(state)\n    startpos = m.start()\n    new.append(string[pos - skipped:startpos])\n    pos = m.end()\n    if startpos == pos:\n        skipped = 1\n    else:\n        skipped = 0\n    new.append(repl(m))\n    if n == count or pos - skipped >= endpos:\n        break\n    pos += skipped\nnew.append(string[pos:])\nreturn ''.join(new), n", "path": "ponyguruma\\_highlevel.py", "repo_name": "mitsuhiko/ponyguruma", "stars": 32, "license": "None", "language": "python", "size": 177}
{"docstring": "\"\"\"\nLike find but yields the string value of the matches.\n\"\"\"\n", "func_signal": "def findstrings(self, string, pos=0, endpos=-1):\n", "code": "for match in self.find(string, pos, endpos):\n    yield match.group()", "path": "ponyguruma\\_highlevel.py", "repo_name": "mitsuhiko/ponyguruma", "stars": 32, "license": "None", "language": "python", "size": 177}
{"docstring": "\"\"\"\nA dict of all named groups with their corresponding values.\n\"\"\"\n", "func_signal": "def groupdict(self):\n", "code": "d = {}\nfor key in self.groupnames:\n    d[key] = self.group(key)\nreturn d", "path": "ponyguruma\\_highlevel.py", "repo_name": "mitsuhiko/ponyguruma", "stars": 32, "license": "None", "language": "python", "size": 177}
{"docstring": "\"\"\"\nReturn the value of a single group.\n\"\"\"\n", "func_signal": "def group(self, group=0):\n", "code": "if isinstance(group, basestring):\n    group = self.groupnames[group]\nreturn match_extract_group(self.state, group)", "path": "ponyguruma\\_highlevel.py", "repo_name": "mitsuhiko/ponyguruma", "stars": 32, "license": "None", "language": "python", "size": 177}
{"docstring": "\"\"\"\nTries to match with pattern at the current position. If there's a match, the\nscanner advances the scan pointer and returns the match object.  Otherwise,\nthe return value is `None` and the position is unchanged.\n\"\"\"\n", "func_signal": "def scan(self, regexp):\n", "code": "match = self.check(regexp)\nif match is not None:\n    self.old_pos = self.pos\n    self.pos = match.end()\n    self.match = match\n    return match", "path": "ponyguruma\\_highlevel.py", "repo_name": "mitsuhiko/ponyguruma", "stars": 32, "license": "None", "language": "python", "size": 177}
{"docstring": "\"\"\"PHP header detected\"\"\"\n", "func_signal": "def probe_x_powered_by(self):\n", "code": "rv = self.make_request(self.config.url)\nreturn rv.headers.get('x-powered-by', '').startswith('PHP/')", "path": "libprobe.py", "repo_name": "mitsuhiko/probe", "stars": 47, "license": "other", "language": "python", "size": 346}
{"docstring": "\"\"\"The missing slash redirection was detected\"\"\"\n", "func_signal": "def probe_redirect_missing_slash(self):\n", "code": "if self.config.url_with_slash is None:\n    return False\nrv = self.make_request(self.config.url_with_slash.rstrip('/'))\nif rv.status != 301:\n    return False\nreturn 'content-length' in rv.headers and \\\n       rv.headers.get('location', '') \\\n            .startswith(('http://', 'https://')) and \\\n       '<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\">' in rv.body and \\\n       '<h1>Redirecting...</h1>' in rv.body", "path": "libprobe.py", "repo_name": "mitsuhiko/probe", "stars": 47, "license": "other", "language": "python", "size": 346}
{"docstring": "\"\"\"The default redirecting behavior is there\"\"\"\n", "func_signal": "def probe_redirects(self):\n", "code": "rv = self.make_request(self.config.missing_url.rstrip('/'))\nreturn rv.status in (301, 302) and rv.body == '' and \\\n    rv.headers.get('location', '').startswith(('http://', 'https://'))", "path": "libprobe.py", "repo_name": "mitsuhiko/probe", "stars": 47, "license": "other", "language": "python", "size": 346}
{"docstring": "\"\"\"The default form rendering naming was detected\"\"\"\n", "func_signal": "def probe_form_rendering(self):\n", "code": "if self.config.form_url is None:\n    return 0.0\nrv = self.make_request(self.config.form_url)\nif rv.status != 200:\n    return 0.0\nfound_label = found_input = False\ninputs_found = set()\n\nfor match in _input_re.finditer(rv.body):\n    attrs = parse_html_attributes(match.group())\n    if 'id' in attrs and 'name' in attrs and \\\n       attrs['id'] == 'id_' + attrs['name']:\n        inputs_found.add(attrs['id'])\n        found_input = True\n\nfor match in _label_re.finditer(rv.body):\n    attrs = parse_html_attributes(match.group())\n    if 'for' in attrs and attrs['for'] in inputs_found:\n        found_input = True\n        break\n\nreturn (found_label + found_input) * 0.25", "path": "libprobe.py", "repo_name": "mitsuhiko/probe", "stars": 47, "license": "other", "language": "python", "size": 346}
{"docstring": "\"\"\"An admin interface was detected\"\"\"\n", "func_signal": "def probe_admin(self):\n", "code": "rv = self.make_request('admin/')\nreturn rv.status == 200 and 'Django' in rv.body", "path": "libprobe.py", "repo_name": "mitsuhiko/probe", "stars": 47, "license": "other", "language": "python", "size": 346}
{"docstring": "\"\"\"A new CSRF token was found\"\"\"\n", "func_signal": "def probe_csrf_cookie(self):\n", "code": "for url in self.config.url, self.config.form_url:\n    if url is None:\n        continue\n    rv = self.make_request(url)\n    if rv.status != 200:\n        continue\n    cookie = rv.headers.get('set-cookie')\n    if cookie is not None and \\\n       'csrftoken' in cookie:\n        return True\nreturn False", "path": "libprobe.py", "repo_name": "mitsuhiko/probe", "stars": 47, "license": "other", "language": "python", "size": 346}
{"docstring": "\"\"\"The pyramid 'don't care about slashes' behavior was detected\"\"\"\n", "func_signal": "def probe_slash_behavior(self):\n", "code": "if self.config.url_without_slash is None:\n    return False\nrv1 = self.make_request(self.config.url_without_slash)\nif rv1.status != 200:\n    return False\nrv2 = self.make_request(self.config.url_without_slash + '/')\nif rv2.status != 200:\n    return False\nreturn difflib.get_close_matches(rv1.body, [rv2.body], 1, 0.8) != []", "path": "libprobe.py", "repo_name": "mitsuhiko/probe", "stars": 47, "license": "other", "language": "python", "size": 346}
{"docstring": "\"\"\"The status reason has the correct capitalization\"\"\"\n", "func_signal": "def probe_reason_capitalization(self):\n", "code": "rv = self.make_request(self.config.missing_url)\nreturn rv.status == 404 and rv.reason == 'NOT FOUND'", "path": "libprobe.py", "repo_name": "mitsuhiko/probe", "stars": 47, "license": "other", "language": "python", "size": 346}
{"docstring": "\"\"\"Connection close behavior detected\"\"\"\n", "func_signal": "def probe_connection_close(self):\n", "code": "rv = self.make_request(self.config.url)\nreturn rv.headers.get('connection') == 'close'", "path": "libprobe.py", "repo_name": "mitsuhiko/probe", "stars": 47, "license": "other", "language": "python", "size": 346}
{"docstring": "\"\"\"PHP easter egg detected\"\"\"\n", "func_signal": "def probe_easter_egg(self):\n", "code": "rv = self.make_request(self.config.url, query={'': 'PHPB8B5F2A0-3C92-11d3-A3A9-4C7B08C10000'})\nreturn \"PHP Credits\" in rv.body", "path": "libprobe.py", "repo_name": "mitsuhiko/probe", "stars": 47, "license": "other", "language": "python", "size": 346}
{"docstring": "\"\"\"The correct default charset was detected\"\"\"\n", "func_signal": "def probe_content_type(self):\n", "code": "rv = self.make_request(self.config.url)\nreturn rv.headers.get('content-type') == 'text/html; charset=utf-8'", "path": "libprobe.py", "repo_name": "mitsuhiko/probe", "stars": 47, "license": "other", "language": "python", "size": 346}
{"docstring": "\"\"\"The CSRF middleware was detected\"\"\"\n", "func_signal": "def probe_csrf_middleware(self):\n", "code": "if self.config.form_url is None:\n    return\nrv = self.make_request(self.config.form_url)\nreturn \"<input type='hidden' name='csrfmiddlewaretoken'\" in rv.body", "path": "libprobe.py", "repo_name": "mitsuhiko/probe", "stars": 47, "license": "other", "language": "python", "size": 346}
{"docstring": "\"\"\"The correct default charset was detected\"\"\"\n", "func_signal": "def probe_content_type(self):\n", "code": "rv = self.make_request(self.config.url)\nreturn rv.headers.get('content-type') == 'text/html; charset=UTF-8'", "path": "libprobe.py", "repo_name": "mitsuhiko/probe", "stars": 47, "license": "other", "language": "python", "size": 346}
{"docstring": "\"\"\"A debug 404 page was found\"\"\"\n", "func_signal": "def probe_debug_404(self):\n", "code": "rv = self.make_request(self.config.missing_url)\nreturn '<h1>Page not found <span>(404)</span></h1>' in rv.body and \\\n       (\"You're seeing this error because you have \"\n        \"<code>DEBUG = True</code>\") in rv.body", "path": "libprobe.py", "repo_name": "mitsuhiko/probe", "stars": 47, "license": "other", "language": "python", "size": 346}
{"docstring": "\"\"\"The correct default charset was detected\"\"\"\n", "func_signal": "def probe_content_type(self):\n", "code": "rv = self.make_request(self.config.url)\nreturn rv.headers.get('content-type') == 'text/html; charset=utf-8'", "path": "libprobe.py", "repo_name": "mitsuhiko/probe", "stars": 47, "license": "other", "language": "python", "size": 346}
{"docstring": "\"\"\"404 page has a content length\"\"\"\n", "func_signal": "def probe_404_content_length(self):\n", "code": "rv = self.make_request(self.config.missing_url)\nreturn 'content-length' in rv.headers", "path": "libprobe.py", "repo_name": "mitsuhiko/probe", "stars": 47, "license": "other", "language": "python", "size": 346}
{"docstring": "\"\"\"Admin interface CSS files were found\"\"\"\n", "func_signal": "def probe_admin_media(self):\n", "code": "for url in 'media/admin/css/login.css', 'adminmedia/admin/css/login.css':\n    rv = self.make_request(url)\n    if rv.status == 200:\n        break\nelse:\n    scheme, netloc, path = urlparse.urlsplit(self.config.url)[:3]\n    path = posixpath.join(path, 'media/admin/css/login.css')\n    if netloc.startswith('www.'):\n        netloc = netloc[4:]\n    if scheme == 'https':\n        scheme = 'http'\n    url = urlparse.urlunsplit((scheme, 'media.' + netloc, path, '', ''))\n    rv = self.make_request(url)\nif rv.status == 200:\n    return 'base.css' in rv.body\nreturn False", "path": "libprobe.py", "repo_name": "mitsuhiko/probe", "stars": 47, "license": "other", "language": "python", "size": 346}
{"docstring": "\"\"\"Something that looks like a securecookie was detected\"\"\"\n", "func_signal": "def probe_securecookie(self):\n", "code": "rv = self.make_request(self.config.url)\nsetcookie = rv.headers.get('set-cookie')\nif setcookie is None:\n    return False\nif not '; Path=' in setcookie:\n    return False\ncookie = SimpleCookie(setcookie)\nfor key, morsel in cookie.items():\n    if not '?' in morsel.value:\n        continue\n    try:\n        morsel.value.split('?')[0].decode('base64')\n        return True\n    except Exception:\n        return False\nreturn False", "path": "libprobe.py", "repo_name": "mitsuhiko/probe", "stars": 47, "license": "other", "language": "python", "size": 346}
{"docstring": "\"\"\"The status reason has the correct capitalization\"\"\"\n", "func_signal": "def probe_reason_capitalization(self):\n", "code": "rv = self.make_request(self.config.missing_url + '/')\nreturn rv.status == 404 and rv.reason == 'NOT FOUND'", "path": "libprobe.py", "repo_name": "mitsuhiko/probe", "stars": 47, "license": "other", "language": "python", "size": 346}
{"docstring": "\"\"\"The status reason has the correct capitalization\"\"\"\n", "func_signal": "def probe_reason_capitalization(self):\n", "code": "rv = self.make_request(self.config.missing_url)\nreturn rv.status == 404 and rv.reason == 'Not Found'", "path": "libprobe.py", "repo_name": "mitsuhiko/probe", "stars": 47, "license": "other", "language": "python", "size": 346}
{"docstring": "\"\"\"Formats a date according to the given format.\"\"\"\n", "func_signal": "def date(value, arg=None):\n", "code": "if not value:\n    return u''\nif arg is None:\n    arg = settings.DATE_FORMAT\ntry:\n    return formats.date_format(value, arg)\nexcept AttributeError:\n    try:\n        return format(value, arg)\n    except AttributeError:\n        return ''", "path": "crate\\web\\utils\\helpers.py", "repo_name": "crateio/crate.web", "stars": 46, "license": "bsd-2-clause", "language": "python", "size": 1653}
{"docstring": "# Adding field 'List.slug'\n", "func_signal": "def forwards(self, orm):\n", "code": "db.add_column('lists_list', 'slug',\n              self.gf('django.db.models.fields.SlugField')(max_length=50, null=True),\n              keep_default=False)\n\n# Adding unique constraint on 'List', fields ['user', 'slug']\ndb.create_unique('lists_list', ['user_id', 'slug'])", "path": "crate\\web\\lists\\migrations\\0002_auto__add_field_list_slug__add_unique_list_user_slug.py", "repo_name": "crateio/crate.web", "stars": 46, "license": "bsd-2-clause", "language": "python", "size": 1653}
{"docstring": "\"\"\"Return user entry with same email address as one returned on details.\"\"\"\n", "func_signal": "def associate_by_email(details, *args, **kwargs):\n", "code": "email = details.get('email')\n\nwarn_setting('SOCIAL_AUTH_ASSOCIATE_BY_MAIL', 'associate_by_email')\n\nif email and setting('SOCIAL_AUTH_ASSOCIATE_BY_MAIL', True):\n    # try to associate accounts registered with the same email address,\n    # only if it's a single object. AuthException is raised if multiple\n    # objects are returned\n    try:\n        address = EmailAddress.objects.filter(email=email, verified=True).select_related(\"user\").get()\n        return {\"user\": address.user}\n    except MultipleObjectsReturned:\n        raise AuthException(kwargs['backend'], 'Not unique email address.')\n    except EmailAddress.DoesNotExist:\n        pass", "path": "crate\\web\\social_auth\\pipeline\\associate.py", "repo_name": "crateio/crate.web", "stars": 46, "license": "bsd-2-clause", "language": "python", "size": 1653}
{"docstring": "# Removing index on 'Package', fields ['deleted']\n", "func_signal": "def backwards(self, orm):\n", "code": "db.delete_index('packages_package', ['deleted'])\n\n# Removing index on 'Release', fields ['deleted']\ndb.delete_index('packages_release', ['deleted'])", "path": "crate\\web\\packages\\migrations\\0013_auto.py", "repo_name": "crateio/crate.web", "stars": 46, "license": "bsd-2-clause", "language": "python", "size": 1653}
{"docstring": "\"\"\"\nPaginate the results, if needed.\n\"\"\"\n", "func_signal": "def paginate_results(self, results, page_size):\n", "code": "paginator = self.get_paginator(results, page_size, allow_empty_first_page=self.get_allow_empty())\npage = self.kwargs.get(\"page\") or self.request.GET.get(\"page\") or 1\ntry:\n    page_number = int(page)\nexcept ValueError:\n    if page == \"last\":\n        page_number = paginator.num_pages\n    else:\n        raise Http404(_(u\"Page is not 'last', nor can it be converted to an int.\"))\ntry:\n    page = paginator.page(page_number)\n    return (paginator, page, page.object_list, page.has_other_pages())\nexcept InvalidPage:\n    raise Http404(_(u\"Invalid page (%(page_number)s)\") % {\n                        \"page_number\": page_number\n    })", "path": "crate\\web\\search\\views.py", "repo_name": "crateio/crate.web", "stars": 46, "license": "bsd-2-clause", "language": "python", "size": 1653}
{"docstring": "\"\"\"\nReturns the keyword arguments for instanciating the form.\n\"\"\"\n", "func_signal": "def get_form_kwargs(self):\n", "code": "kwargs = {\n    \"initial\": self.get_initial(),\n    \"searchqueryset\": self.get_searchqueryset(),\n    \"load_all\": self.get_load_all(),\n}\nif \"q\" in self.request.GET:\n    kwargs.update({\n        \"data\": self.request.GET,\n    })\nreturn kwargs", "path": "crate\\web\\search\\views.py", "repo_name": "crateio/crate.web", "stars": 46, "license": "bsd-2-clause", "language": "python", "size": 1653}
{"docstring": "\"\"\"\nOnly save the search if we're on the first page.\nThis will prevent an excessive number of duplicates for what is\nessentially the same search.\n\"\"\"\n", "func_signal": "def save_search(self, page, query, results):\n", "code": "if query and page.number == 1:\n    # Save the search.\n    saved_search = SavedSearch(\n        search_key=self.search_key,\n        user_query=query,\n        result_count=len(results)\n    )\n\n    if hasattr(results, 'query'):\n        query_seen = results.query.build_query()\n\n        if isinstance(query_seen, basestring):\n            saved_search.full_query = query_seen\n\n    if self.request.user.is_authenticated():\n        saved_search.user = self.request.user\n\n    saved_search.save()", "path": "crate\\web\\search\\views.py", "repo_name": "crateio/crate.web", "stars": 46, "license": "bsd-2-clause", "language": "python", "size": 1653}
{"docstring": "# Adding model 'ReadTheDocsPackageSlug'\n", "func_signal": "def forwards(self, orm):\n", "code": "db.create_table('packages_readthedocspackageslug', (\n    ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n    ('package', self.gf('django.db.models.fields.related.OneToOneField')(related_name='readthedocs_slug', unique=True, to=orm['packages.Package'])),\n    ('slug', self.gf('django.db.models.fields.CharField')(unique=True, max_length=150)),\n))\ndb.send_create_signal('packages', ['ReadTheDocsPackageSlug'])", "path": "crate\\web\\packages\\migrations\\0008_auto__add_readthedocspackageslug.py", "repo_name": "crateio/crate.web", "stars": 46, "license": "bsd-2-clause", "language": "python", "size": 1653}
{"docstring": "# Deleting field 'Release.deleted'\n", "func_signal": "def forwards(self, orm):\n", "code": "db.delete_column('packages_release', 'deleted')\n\n# Deleting field 'Package.deleted'\ndb.delete_column('packages_package', 'deleted')", "path": "crate\\web\\packages\\migrations\\0015_auto__del_field_release_deleted__del_field_package_deleted.py", "repo_name": "crateio/crate.web", "stars": 46, "license": "bsd-2-clause", "language": "python", "size": 1653}
{"docstring": "# Adding field 'List.description'\n", "func_signal": "def forwards(self, orm):\n", "code": "db.add_column('lists_list', 'description',\n              self.gf('django.db.models.fields.CharField')(default='', max_length=250, blank=True),\n              keep_default=False)", "path": "crate\\web\\lists\\migrations\\0005_auto__add_field_list_description.py", "repo_name": "crateio/crate.web", "stars": 46, "license": "bsd-2-clause", "language": "python", "size": 1653}
{"docstring": "# Adding field 'Release.frequency'\n", "func_signal": "def forwards(self, orm):\n", "code": "db.add_column('packages_release', 'frequency',\n              self.gf('django.db.models.fields.CharField')(default='hourly', max_length=25),\n              keep_default=False)", "path": "crate\\web\\packages\\migrations\\0003_auto__add_field_release_frequency.py", "repo_name": "crateio/crate.web", "stars": 46, "license": "bsd-2-clause", "language": "python", "size": 1653}
{"docstring": "# Adding index on 'Release', fields ['deleted']\n", "func_signal": "def forwards(self, orm):\n", "code": "db.create_index('packages_release', ['deleted'])\n\n# Adding index on 'Package', fields ['deleted']\ndb.create_index('packages_package', ['deleted'])", "path": "crate\\web\\packages\\migrations\\0013_auto.py", "repo_name": "crateio/crate.web", "stars": 46, "license": "bsd-2-clause", "language": "python", "size": 1653}
{"docstring": "# Removing unique constraint on 'List', fields ['user', 'slug']\n", "func_signal": "def backwards(self, orm):\n", "code": "db.delete_unique('lists_list', ['user_id', 'slug'])\n\n# Deleting field 'List.slug'\ndb.delete_column('lists_list', 'slug')", "path": "crate\\web\\lists\\migrations\\0002_auto__add_field_list_slug__add_unique_list_user_slug.py", "repo_name": "crateio/crate.web", "stars": 46, "license": "bsd-2-clause", "language": "python", "size": 1653}
{"docstring": "# Adding field 'ReleaseFile.hidden'\n", "func_signal": "def forwards(self, orm):\n", "code": "db.add_column('packages_releasefile', 'hidden',\n              self.gf('django.db.models.fields.BooleanField')(default=False),\n              keep_default=False)", "path": "crate\\web\\packages\\migrations\\0019_auto__add_field_releasefile_hidden.py", "repo_name": "crateio/crate.web", "stars": 46, "license": "bsd-2-clause", "language": "python", "size": 1653}
{"docstring": "# Adding field 'Release.show_install_command'\n", "func_signal": "def forwards(self, orm):\n", "code": "db.add_column('packages_release', 'show_install_command',\n              self.gf('django.db.models.fields.BooleanField')(default=True),\n              keep_default=False)", "path": "crate\\web\\packages\\migrations\\0020_auto__add_field_release_show_install_command.py", "repo_name": "crateio/crate.web", "stars": 46, "license": "bsd-2-clause", "language": "python", "size": 1653}
{"docstring": "# Adding field 'Release.deleted'\n", "func_signal": "def backwards(self, orm):\n", "code": "db.add_column('packages_release', 'deleted',\n              self.gf('django.db.models.fields.BooleanField')(default=False, db_index=True),\n              keep_default=False)\n\n# Adding field 'Package.deleted'\ndb.add_column('packages_package', 'deleted',\n              self.gf('django.db.models.fields.BooleanField')(default=False, db_index=True),\n              keep_default=False)", "path": "crate\\web\\packages\\migrations\\0015_auto__del_field_release_deleted__del_field_package_deleted.py", "repo_name": "crateio/crate.web", "stars": 46, "license": "bsd-2-clause", "language": "python", "size": 1653}
{"docstring": "\"\"\"\nTracks property changes on a model instance.\n\nThe changed list of properties is refreshed on model initialization\nand save.\n\n>>> @track_data('name')\n>>> class Post(models.Model):\n>>>     name = models.CharField(...)\n>>>\n>>>     @classmethod\n>>>     def post_save(cls, sender, instance, created, **kwargs):\n>>>         if instance.has_changed('name'):\n>>>             print \"Hooray!\"\n\"\"\"\n\n", "func_signal": "def track_data(*fields):\n", "code": "UNSAVED = dict()\n\ndef _store(self):\n    \"Updates a local copy of attributes values\"\n    if self.id:\n        self.__data = dict((f, getattr(self, f)) for f in fields)\n    else:\n        self.__data = UNSAVED\n\ndef inner(cls):\n    # contains a local copy of the previous values of attributes\n    cls.__data = {}\n\n    def has_changed(self, field):\n        \"Returns ``True`` if ``field`` has changed since initialization.\"\n        if self.__data is UNSAVED:\n            return False\n        return self.__data.get(field) != getattr(self, field)\n    cls.has_changed = has_changed\n\n    def old_value(self, field):\n        \"Returns the previous value of ``field``\"\n        return self.__data.get(field)\n    cls.old_value = old_value\n\n    def whats_changed(self):\n        \"Returns a list of changed attributes.\"\n        changed = {}\n        if self.__data is UNSAVED:\n            return changed\n        for k, v in self.__data.iteritems():\n            if v != getattr(self, k):\n                changed[k] = v\n        return changed\n    cls.whats_changed = whats_changed\n\n    # Ensure we are updating local attributes on model init\n    def _post_init(sender, instance, **kwargs):\n        _store(instance)\n    post_init.connect(_post_init, sender=cls, weak=False)\n\n    # Ensure we are updating local attributes on model save\n    def save(self, *args, **kwargs):\n        save._original(self, *args, **kwargs)\n        _store(self)\n    save._original = cls.save\n    cls.save = save\n    return cls\nreturn inner", "path": "crate\\web\\packages\\utils\\datatools.py", "repo_name": "crateio/crate.web", "stars": 46, "license": "bsd-2-clause", "language": "python", "size": 1653}
{"docstring": "# Use a custom queryset if provided; this is required for subclasses\n# like DateDetailView\n", "func_signal": "def get_object(self, queryset=None):\n", "code": "if queryset is None:\n    queryset = self.get_queryset()\n\n# Next, try looking up by primary key.\npk = self.kwargs.get(self.pk_url_kwarg, None)\nslug = self.kwargs.get(self.slug_url_kwarg, None)\nif pk is not None:\n    queryset = queryset.filter(pk=pk)\n\n# Next, try looking up by slug.\nelif slug is not None:\n    slug_field = self.get_slug_field()\n    queryset = queryset.filter(**{slug_field: slug})\n\n# If none of those are defined, it's an error.\nelse:\n    raise AttributeError(u\"Generic detail view %s must be called with \"\n                         u\"either an object pk or a slug.\"\n                         % self.__class__.__name__)\n\ntry:\n    obj = queryset.get()\nexcept ObjectDoesNotExist:\n    try:\n        queryset = self.get_queryset()\n        queryset = queryset.filter(normalized_name=re.sub('[^A-Za-z0-9.]+', '-', slug).lower())\n        obj = queryset.get()\n    except ObjectDoesNotExist:\n        raise Http404(_(u\"No %(verbose_name)s found matching the query\") %\n                  {'verbose_name': queryset.model._meta.verbose_name})\n\nreturn obj", "path": "crate\\web\\packages\\simple\\views.py", "repo_name": "crateio/crate.web", "stars": 46, "license": "bsd-2-clause", "language": "python", "size": 1653}
{"docstring": "# Adding model 'Event'\n", "func_signal": "def forwards(self, orm):\n", "code": "db.create_table('history_event', (\n    ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n    ('created', self.gf('model_utils.fields.AutoCreatedField')(default=datetime.datetime.now)),\n    ('modified', self.gf('model_utils.fields.AutoLastModifiedField')(default=datetime.datetime.now)),\n    ('package', self.gf('django.db.models.fields.SlugField')(max_length=150)),\n    ('version', self.gf('django.db.models.fields.CharField')(max_length=512)),\n    ('action', self.gf('django.db.models.fields.CharField')(max_length=25)),\n    ('data', self.gf('jsonfield.fields.JSONField')()),\n))\ndb.send_create_signal('history', ['Event'])", "path": "crate\\web\\history\\migrations\\0001_initial.py", "repo_name": "crateio/crate.web", "stars": 46, "license": "bsd-2-clause", "language": "python", "size": 1653}
{"docstring": "\"\"\"\nGet the number of items to paginate by, or ``None`` for no pagination.\n\"\"\"\n", "func_signal": "def get_paginate_by(self):\n", "code": "if self.paginate_by is None:\n    return getattr(settings, \"HAYSTACK_SEARCH_RESULTS_PER_PAGE\", 20)\nreturn self.paginate_by", "path": "crate\\web\\search\\views.py", "repo_name": "crateio/crate.web", "stars": 46, "license": "bsd-2-clause", "language": "python", "size": 1653}
{"docstring": "\"\"\"Handle a processing instruction as a ProcessingInstruction\nobject, possibly one with a %SOUP-ENCODING% slot into which an\nencoding will be plugged later.\"\"\"\n", "func_signal": "def handle_pi(self, text):\n", "code": "if text[:3] == \"xml\":\n    text = u\"xml version='1.0' encoding='%SOUP-ENCODING%'\"\nself._toStringSubclass(text, ProcessingInstruction)", "path": "download.py", "repo_name": "abhirama/coursera-download", "stars": 45, "license": "None", "language": "python", "size": 277}
{"docstring": "\"\"\"Returns the closest parent of this Tag that matches the given\ncriteria.\"\"\"\n# NOTE: We can't use _findOne because findParents takes a different\n# set of arguments.\n", "func_signal": "def findParent(self, name=None, attrs={}, **kwargs):\n", "code": "r = None\nl = self.findParents(name, attrs, 1)\nif l:\n    r = l[0]\nreturn r", "path": "download.py", "repo_name": "abhirama/coursera-download", "stars": 45, "license": "None", "language": "python", "size": 277}
{"docstring": "\"\"\"Used in a call to re.sub to replace HTML, XML, and numeric\nentities with the appropriate Unicode characters. If HTML\nentities are being converted, any unrecognized entities are\nescaped.\"\"\"\n", "func_signal": "def _convertEntities(self, match):\n", "code": "x = match.group(1)\nif self.convertHTMLEntities and x in name2codepoint:\n    return unichr(name2codepoint[x])\nelif x in self.XML_ENTITIES_TO_SPECIAL_CHARS:\n    if self.convertXMLEntities:\n        return self.XML_ENTITIES_TO_SPECIAL_CHARS[x]\n    else:\n        return u'&%s;' % x\nelif len(x) > 0 and x[0] == '#':\n    # Handle numeric entities\n    if len(x) > 1 and x[1] == 'x':\n        return unichr(int(x[2:], 16))\n    else:\n        return unichr(int(x[1:]))\n\nelif self.escapeUnrecognizedEntities:\n    return u'&amp;%s;' % x\nelse:\n    return u'&%s;' % x", "path": "download.py", "repo_name": "abhirama/coursera-download", "stars": 45, "license": "None", "language": "python", "size": 277}
{"docstring": "\"\"\"Beautiful Soup can detect a charset included in a META tag,\ntry to convert the document to that charset, and re-parse the\ndocument from the beginning.\"\"\"\n", "func_signal": "def start_meta(self, attrs):\n", "code": "httpEquiv = None\ncontentType = None\ncontentTypeIndex = None\ntagNeedsEncodingSubstitution = False\n\nfor i in range(0, len(attrs)):\n    key, value = attrs[i]\n    key = key.lower()\n    if key == 'http-equiv':\n        httpEquiv = value\n    elif key == 'content':\n        contentType = value\n        contentTypeIndex = i\n\nif httpEquiv and contentType: # It's an interesting meta tag.\n    match = self.CHARSET_RE.search(contentType)\n    if match:\n        if (self.declaredHTMLEncoding is not None or\n            self.originalEncoding == self.fromEncoding):\n            # An HTML encoding was sniffed while converting\n            # the document to Unicode, or an HTML encoding was\n            # sniffed during a previous pass through the\n            # document, or an encoding was specified\n            # explicitly and it worked. Rewrite the meta tag.\n            def rewrite(match):\n                return match.group(1) + \"%SOUP-ENCODING%\"\n            newAttr = self.CHARSET_RE.sub(rewrite, contentType)\n            attrs[contentTypeIndex] = (attrs[contentTypeIndex][0],\n                                       newAttr)\n            tagNeedsEncodingSubstitution = True\n        else:\n            # This is our first pass through the document.\n            # Go through it again with the encoding information.\n            newCharset = match.group(3)\n            if newCharset and newCharset != self.originalEncoding:\n                self.declaredHTMLEncoding = newCharset\n                self._feed(self.declaredHTMLEncoding)\n                raise StopParsing\n            pass\ntag = self.unknown_starttag(\"meta\", attrs)\nif tag and tagNeedsEncodingSubstitution:\n    tag.containsSubstitutions = True", "path": "download.py", "repo_name": "abhirama/coursera-download", "stars": 45, "license": "None", "language": "python", "size": 277}
{"docstring": "\"\"\"This method routes method call requests to either the SGMLParser\nsuperclass or the Tag superclass, depending on the method name.\"\"\"\n#print \"__getattr__ called on %s.%s\" % (self.__class__, methodName)\n\n", "func_signal": "def __getattr__(self, methodName):\n", "code": "if methodName.startswith('start_') or methodName.startswith('end_') \\\n       or methodName.startswith('do_'):\n    return SGMLParser.__getattr__(self, methodName)\nelif not methodName.startswith('__'):\n    return Tag.__getattr__(self, methodName)\nelse:\n    raise AttributeError", "path": "download.py", "repo_name": "abhirama/coursera-download", "stars": 45, "license": "None", "language": "python", "size": 277}
{"docstring": "#print \"Matching %s against %s\" % (markup, matchAgainst)\n", "func_signal": "def _matches(self, markup, matchAgainst):\n", "code": "result = False\nif matchAgainst is True:\n    result = markup is not None\nelif callable(matchAgainst):\n    result = matchAgainst(markup)\nelse:\n    #Custom match methods take the tag as an argument, but all\n    #other ways of matching match the tag name as a string.\n    if isinstance(markup, Tag):\n        markup = markup.name\n    if markup and not isinstance(markup, basestring):\n        markup = unicode(markup)\n    #Now we know that chunk is either a string, or None.\n    if hasattr(matchAgainst, 'match'):\n        # It's a regexp object.\n        result = markup and matchAgainst.search(markup)\n    elif hasattr(matchAgainst, '__iter__'): # list-like\n        result = markup in matchAgainst\n    elif hasattr(matchAgainst, 'items'):\n        result = markup.has_key(matchAgainst)\n    elif matchAgainst and isinstance(markup, basestring):\n        if isinstance(markup, unicode):\n            matchAgainst = unicode(matchAgainst)\n        else:\n            matchAgainst = str(matchAgainst)\n\n    if not result:\n        result = matchAgainst == markup\nreturn result", "path": "download.py", "repo_name": "abhirama/coursera-download", "stars": 45, "license": "None", "language": "python", "size": 277}
{"docstring": "\"\"\"Create a new NavigableString.\n\nWhen unpickling a NavigableString, this method is called with\nthe string in DEFAULT_OUTPUT_ENCODING. That encoding needs to be\npassed in to the superclass's __new__ or the superclass won't know\nhow to handle non-ASCII characters.\n\"\"\"\n", "func_signal": "def __new__(cls, value):\n", "code": "if isinstance(value, unicode):\n    return unicode.__new__(cls, value)\nreturn unicode.__new__(cls, value, DEFAULT_OUTPUT_ENCODING)", "path": "download.py", "repo_name": "abhirama/coursera-download", "stars": 45, "license": "None", "language": "python", "size": 277}
{"docstring": "\"\"\"Extract all children.\"\"\"\n", "func_signal": "def clear(self):\n", "code": "for child in self.contents[:]:\n    child.extract()", "path": "download.py", "repo_name": "abhirama/coursera-download", "stars": 45, "license": "None", "language": "python", "size": 277}
{"docstring": "\"\"\"Initializes a map representation of this tag's attributes,\nif not already initialized.\"\"\"\n", "func_signal": "def _getAttrMap(self):\n", "code": "if not getattr(self, 'attrMap'):\n    self.attrMap = {}\n    for (key, value) in self.attrs:\n        self.attrMap[key] = value\nreturn self.attrMap", "path": "download.py", "repo_name": "abhirama/coursera-download", "stars": 45, "license": "None", "language": "python", "size": 277}
{"docstring": "\"\"\"Turns a list of maps, lists, or scalars into a single map.\nUsed to build the SELF_CLOSING_TAGS, NESTABLE_TAGS, and\nNESTING_RESET_TAGS maps out of lists and partial maps.\"\"\"\n", "func_signal": "def buildTagMap(default, *args):\n", "code": "built = {}\nfor portion in args:\n    if hasattr(portion, 'items'):\n        #It's a map. Merge it.\n        for k,v in portion.items():\n            built[k] = v\n    elif hasattr(portion, '__iter__'): # is a list\n        #It's a list. Map each item to the default.\n        for k in portion:\n            built[k] = default\n    else:\n        #It's a scalar. Map it to the default.\n        built[portion] = default\nreturn built", "path": "download.py", "repo_name": "abhirama/coursera-download", "stars": 45, "license": "None", "language": "python", "size": 277}
{"docstring": "\"\"\"Returns the closest sibling to this Tag that matches the\ngiven criteria and appears after this Tag in the document.\"\"\"\n", "func_signal": "def findNextSibling(self, name=None, attrs={}, text=None, **kwargs):\n", "code": "return self._findOne(self.findNextSiblings, name, attrs, text,\n                     **kwargs)", "path": "download.py", "repo_name": "abhirama/coursera-download", "stars": 45, "license": "None", "language": "python", "size": 277}
{"docstring": "\"\"\"Returns true iff this tag has the same name, the same attributes,\nand the same contents (recursively) as the given tag.\n\nNOTE: right now this will return false if two tags have the\nsame attributes in a different order. Should this be fixed?\"\"\"\n", "func_signal": "def __eq__(self, other):\n", "code": "if other is self:\n    return True\nif not hasattr(other, 'name') or not hasattr(other, 'attrs') or not hasattr(other, 'contents') or self.name != other.name or self.attrs != other.attrs or len(self) != len(other):\n    return False\nfor i in range(0, len(self.contents)):\n    if self.contents[i] != other.contents[i]:\n        return False\nreturn True", "path": "download.py", "repo_name": "abhirama/coursera-download", "stars": 45, "license": "None", "language": "python", "size": 277}
{"docstring": "#print contentDispositionString\n", "func_signal": "def extractFileName(contentDispositionString):\n", "code": "pattern = 'attachment; filename=\"(.*?)\"'\nm = re.search(pattern, contentDispositionString)\ntry:\n    return m.group(1)\nexcept Exception:\n    return ''", "path": "download.py", "repo_name": "abhirama/coursera-download", "stars": 45, "license": "None", "language": "python", "size": 277}
{"docstring": "\"\"\"Treat a bogus SGML declaration as raw data. Treat a CDATA\ndeclaration as a CData object.\"\"\"\n", "func_signal": "def parse_declaration(self, i):\n", "code": "j = None\nif self.rawdata[i:i+9] == '<![CDATA[':\n     k = self.rawdata.find(']]>', i)\n     if k == -1:\n         k = len(self.rawdata)\n     data = self.rawdata[i+9:k]\n     j = k+3\n     self._toStringSubclass(data, CData)\nelse:\n    try:\n        j = SGMLParser.parse_declaration(self, i)\n    except SGMLParseError:\n        toHandle = self.rawdata[i:]\n        self.handle_data(toHandle)\n        j = i + len(toHandle)\nreturn j", "path": "download.py", "repo_name": "abhirama/coursera-download", "stars": 45, "license": "None", "language": "python", "size": 277}
{"docstring": "\"\"\"Sets up the initial relations between this element and\nother elements.\"\"\"\n", "func_signal": "def setup(self, parent=None, previous=None):\n", "code": "self.parent = parent\nself.previous = previous\nself.next = None\nself.previousSibling = None\nself.nextSibling = None\nif self.parent and self.parent.contents:\n    self.previousSibling = self.parent.contents[-1]\n    self.previousSibling.nextSibling = self", "path": "download.py", "repo_name": "abhirama/coursera-download", "stars": 45, "license": "None", "language": "python", "size": 277}
{"docstring": "\"\"\"Returns the closest sibling to this Tag that matches the\ngiven criteria and appears before this Tag in the document.\"\"\"\n", "func_signal": "def findPreviousSibling(self, name=None, attrs={}, text=None, **kwargs):\n", "code": "return self._findOne(self.findPreviousSiblings, name, attrs, text,\n                     **kwargs)", "path": "download.py", "repo_name": "abhirama/coursera-download", "stars": 45, "license": "None", "language": "python", "size": 277}
{"docstring": "\"\"\"Destructively rips this element out of the tree.\"\"\"\n", "func_signal": "def extract(self):\n", "code": "if self.parent:\n    try:\n        del self.parent.contents[self.parent.index(self)]\n    except ValueError:\n        pass\n\n#Find the two elements that would be next to each other if\n#this element (and any children) hadn't been parsed. Connect\n#the two.\nlastChild = self._lastRecursiveChild()\nnextElement = lastChild.next\n\nif self.previous:\n    self.previous.next = nextElement\nif nextElement:\n    nextElement.previous = self.previous\nself.previous = None\nlastChild.next = None\n\nself.parent = None\nif self.previousSibling:\n    self.previousSibling.nextSibling = self.nextSibling\nif self.nextSibling:\n    self.nextSibling.previousSibling = self.previousSibling\nself.previousSibling = self.nextSibling = None\nreturn self", "path": "download.py", "repo_name": "abhirama/coursera-download", "stars": 45, "license": "None", "language": "python", "size": 277}
{"docstring": "\"\"\"Changes a MS smart quote character to an XML or HTML\nentity.\"\"\"\n", "func_signal": "def _subMSChar(self, orig):\n", "code": "sub = self.MS_CHARS.get(orig)\nif isinstance(sub, tuple):\n    if self.smartQuotesTo == 'xml':\n        sub = '&#x%s;' % sub[1]\n    else:\n        sub = '&%s;' % sub[0]\nreturn sub", "path": "download.py", "repo_name": "abhirama/coursera-download", "stars": 45, "license": "None", "language": "python", "size": 277}
{"docstring": "\"\"\"This method fixes a bug in Python's SGMLParser.\"\"\"\n", "func_signal": "def convert_charref(self, name):\n", "code": "try:\n    n = int(name)\nexcept ValueError:\n    return\nif not 0 <= n <= 127 : # ASCII ends at 127, not 255\n    return\nreturn self.convert_codepoint(n)", "path": "download.py", "repo_name": "abhirama/coursera-download", "stars": 45, "license": "None", "language": "python", "size": 277}
{"docstring": "\"\"\"Returns the parents of this Tag that match the given\ncriteria.\"\"\"\n\n", "func_signal": "def findParents(self, name=None, attrs={}, limit=None, **kwargs):\n", "code": "return self._findAll(name, attrs, None, limit, self.parentGenerator,\n                     **kwargs)", "path": "download.py", "repo_name": "abhirama/coursera-download", "stars": 45, "license": "None", "language": "python", "size": 277}
{"docstring": "\"\"\"\nThe ``cleanupregistration`` management command properly\ndeletes expired accounts.\n\n\"\"\"\n", "func_signal": "def test_management_command(self):\n", "code": "new_user = RegistrationProfile.objects.create_inactive_user(site=Site.objects.get_current(),\n                                                            **self.user_info)\nexpired_user = RegistrationProfile.objects.create_inactive_user(site=Site.objects.get_current(),\n                                                                username='bob',\n                                                                password='secret',\n                                                                email='bob@example.com')\nexpired_user.date_joined -= datetime.timedelta(days=settings.ACCOUNT_ACTIVATION_DAYS + 1)\nexpired_user.save()\n\nmanagement.call_command('cleanupregistration')\nself.assertEqual(RegistrationProfile.objects.count(), 1)\nself.assertRaises(User.DoesNotExist, User.objects.get, username='bob')", "path": "registration\\tests\\models.py", "repo_name": "yuvipanda/wtfimb", "stars": 36, "license": "None", "language": "python", "size": 5995}
{"docstring": "\"\"\"\nGiven an an activation key, look up and activate the user\naccount corresponding to that key (if possible).\n\nAfter successful activation, the signal\n``registration.signals.user_activated`` will be sent, with the\nnewly activated ``User`` as the keyword argument ``user`` and\nthe class of this backend as the sender.\n\n\"\"\"\n", "func_signal": "def activate(self, request, activation_key):\n", "code": "activated = RegistrationProfile.objects.activate_user(activation_key)\nif activated:\n    signals.user_activated.send(sender=self.__class__,\n                                user=activated,\n                                request=request)\nreturn activated", "path": "registration\\backends\\default\\__init__.py", "repo_name": "yuvipanda/wtfimb", "stars": 36, "license": "None", "language": "python", "size": 5995}
{"docstring": "\"\"\"\nCreating a registration profile for a user populates the\nprofile with the correct user and a SHA1 hash to use as\nactivation key.\n\n\"\"\"\n", "func_signal": "def test_profile_creation(self):\n", "code": "new_user = User.objects.create_user(**self.user_info)\nprofile = RegistrationProfile.objects.create_profile(new_user)\n\nself.assertEqual(RegistrationProfile.objects.count(), 1)\nself.assertEqual(profile.user.id, new_user.id)\nself.failUnless(re.match('^[a-f0-9]{40}$', profile.activation_key))\nself.assertEqual(unicode(profile),\n                 \"Registration information for alice\")", "path": "registration\\tests\\models.py", "repo_name": "yuvipanda/wtfimb", "stars": 36, "license": "None", "language": "python", "size": 5995}
{"docstring": "\"\"\"\nVerifiy that the values entered into the two password fields\nmatch. Note that an error here will end up in\n``non_field_errors()`` because it doesn't apply to a single\nfield.\n\n\"\"\"\n", "func_signal": "def clean(self):\n", "code": "if 'password1' in self.cleaned_data and 'password2' in self.cleaned_data:\n    if self.cleaned_data['password1'] != self.cleaned_data['password2']:\n        raise forms.ValidationError(_(\"The two password fields didn't match.\"))\nreturn self.cleaned_data", "path": "registration\\forms.py", "repo_name": "yuvipanda/wtfimb", "stars": 36, "license": "None", "language": "python", "size": 5995}
{"docstring": "\"\"\"\nReturns context variables required by apps that use django-authopenid.\n\"\"\"\n", "func_signal": "def authopenid(request):\n", "code": "if hasattr(request, 'openid'):\n    openid = request.openid\nelse:\n    openid = None\n    \nif hasattr(request, 'openids'):\n    openids = request.openids\nelse:\n    openids = []\n    \nif hasattr(request, 'associated_openids'):\n    associated_openids = request.associated_openids\nelse:\n    associated_openids = []\n    \nreturn {\n    \"openid\": openid,\n    \"openids\": openids,\n    \"associated_openids\": associated_openids,\n    \"signin_with_openid\": (openid is not None),\n    \"has_openids\": (len(associated_openids) > 0)\n}", "path": "django_authopenid\\context_processors.py", "repo_name": "yuvipanda/wtfimb", "stars": 36, "license": "None", "language": "python", "size": 5995}
{"docstring": "\"\"\"\nGiven a username, email address and password, register a new\nuser account, which will initially be inactive.\n\nAlong with the new ``User`` object, a new\n``registration.models.RegistrationProfile`` will be created,\ntied to that ``User``, containing the activation key which\nwill be used for this account.\n\nAn email will be sent to the supplied email address; this\nemail should contain an activation link. The email will be\nrendered using two templates. See the documentation for\n``RegistrationProfile.send_activation_email()`` for\ninformation about these templates and the contexts provided to\nthem.\n\nAfter the ``User`` and ``RegistrationProfile`` are created and\nthe activation email is sent, the signal\n``registration.signals.user_registered`` will be sent, with\nthe new ``User`` as the keyword argument ``user`` and the\nclass of this backend as the sender.\n\n\"\"\"\n", "func_signal": "def register(self, request, **kwargs):\n", "code": "username, email, password = kwargs['username'], kwargs['email'], kwargs['password1']\nif Site._meta.installed:\n    site = Site.objects.get_current()\nelse:\n    site = RequestSite(request)\nnew_user = RegistrationProfile.objects.create_inactive_user(username, email,\n                                                            password, site)\nsignals.user_registered.send(sender=self.__class__,\n                             user=new_user,\n                             request=request)\nreturn new_user", "path": "registration\\backends\\default\\__init__.py", "repo_name": "yuvipanda/wtfimb", "stars": 36, "license": "None", "language": "python", "size": 5995}
{"docstring": "\"\"\"Return the absolute name of the module to be imported.\"\"\"\n", "func_signal": "def _resolve_name(name, package, level):\n", "code": "if not hasattr(package, 'rindex'):\n    raise ValueError(\"'package' not set to a string\")\ndot = len(package)\nfor x in xrange(level, 1, -1):\n    try:\n        dot = package.rindex('.', 0, dot)\n    except ValueError:\n        raise ValueError(\"attempted relative import beyond top-level \"\n                          \"package\")\nreturn \"%s.%s\" % (package[:dot], name)", "path": "django_authopenid\\utils\\importlib.py", "repo_name": "yuvipanda/wtfimb", "stars": 36, "license": "None", "language": "python", "size": 5995}
{"docstring": "\"\"\"Find the best match for a given mime_type against \n   a list of media_ranges that have already been \n   parsed by parse_media_range(). Returns the \n   'q' quality parameter of the best match, 0 if no\n   match was found. This function bahaves the same as quality()\n   except that 'parsed_ranges' must be a list of\n   parsed media ranges. \"\"\"\n", "func_signal": "def quality_parsed(mime_type, parsed_ranges):\n", "code": "best_fitness = -1 \nbest_match = \"\"\nbest_fit_q = 0\n(target_type, target_subtype, target_params) =\\\n        parse_media_range(mime_type)\nfor (type, subtype, params) in parsed_ranges:\n    param_matches = reduce(lambda x, y: x+y, [1 for (key, value) in \\\n            target_params.iteritems() if key != 'q' and \\\n            params.has_key(key) and value == params[key]], 0)\n    if (type == target_type or type == '*' or target_type == '*') and \\\n            (subtype == target_subtype or subtype == '*' or target_subtype == '*'):\n        fitness = (type == target_type) and 100 or 0\n        fitness += (subtype == target_subtype) and 10 or 0\n        fitness += param_matches\n        if fitness > best_fitness:\n            best_fitness = fitness\n            best_fit_q = params['q']\n        \nreturn float(best_fit_q)", "path": "django_authopenid\\utils\\mimeparse.py", "repo_name": "yuvipanda/wtfimb", "stars": 36, "license": "None", "language": "python", "size": 5995}
{"docstring": "# Deleting model 'Route'\n", "func_signal": "def backwards(self, orm):\n", "code": "        db.delete_table('routes_route')\n\n        # Deleting model 'RouteStage'\n        db.delete_table('routes_routestage')", "path": "routes\\migrations\\0001_initial.py", "repo_name": "yuvipanda/wtfimb", "stars": 36, "license": "None", "language": "python", "size": 5995}
{"docstring": "\"\"\"\nBy default, creating a new user sends an activation email.\n\n\"\"\"\n", "func_signal": "def test_user_creation_email(self):\n", "code": "new_user = RegistrationProfile.objects.create_inactive_user(site=Site.objects.get_current(),\n                                                            **self.user_info)\nself.assertEqual(len(mail.outbox), 1)", "path": "registration\\tests\\models.py", "repo_name": "yuvipanda/wtfimb", "stars": 36, "license": "None", "language": "python", "size": 5995}
{"docstring": "\"\"\"\nAttempting to activate with a non-existent key (i.e., one not\nassociated with any account) fails.\n\n\"\"\"\n# Due to the way activation keys are constructed during\n# registration, this will never be a valid key.\n", "func_signal": "def test_activation_nonexistent_key(self):\n", "code": "invalid_key = sha_constructor('foo').hexdigest()\nself.failIf(RegistrationProfile.objects.activate_user(invalid_key))", "path": "registration\\tests\\models.py", "repo_name": "yuvipanda/wtfimb", "stars": 36, "license": "None", "language": "python", "size": 5995}
{"docstring": "\"\"\"\nValidate that the username is alphanumeric and is not already\nin use.\n\n\"\"\"\n", "func_signal": "def clean_username(self):\n", "code": "try:\n    user = User.objects.get(username__iexact=self.cleaned_data['username'])\nexcept User.DoesNotExist:\n    return self.cleaned_data['username']\nraise forms.ValidationError(_(\"A user with that username already exists.\"))", "path": "registration\\forms.py", "repo_name": "yuvipanda/wtfimb", "stars": 36, "license": "None", "language": "python", "size": 5995}
{"docstring": "\"\"\"\n``RegistrationProfile.send_activation_email`` sends an\nemail.\n\n\"\"\"\n", "func_signal": "def test_activation_email(self):\n", "code": "new_user = User.objects.create_user(**self.user_info)\nprofile = RegistrationProfile.objects.create_profile(new_user)\nprofile.send_activation_email(Site.objects.get_current())\nself.assertEqual(len(mail.outbox), 1)\nself.assertEqual(mail.outbox[0].to, [self.user_info['email']])", "path": "registration\\tests\\models.py", "repo_name": "yuvipanda/wtfimb", "stars": 36, "license": "None", "language": "python", "size": 5995}
{"docstring": "\"\"\"Takes a list of supported mime-types and finds the best\nmatch for all the media-ranges listed in header. The value of\nheader must be a string that conforms to the format of the \nHTTP Accept: header. The value of 'supported' is a list of\nmime-types.\n\n>>> best_match(['application/xbel+xml', 'text/xml'], 'text/*;q=0.5,*/*; q=0.1')\n'text/xml'\n\"\"\"\n", "func_signal": "def best_match(supported, header):\n", "code": "parsed_header = [parse_media_range(r) for r in header.split(\",\")]\nweighted_matches = [(quality_parsed(mime_type, parsed_header), mime_type)\\\n        for mime_type in supported]\nweighted_matches.sort()\nreturn weighted_matches[-1][0] and weighted_matches[-1][1] or ''", "path": "django_authopenid\\utils\\mimeparse.py", "repo_name": "yuvipanda/wtfimb", "stars": 36, "license": "None", "language": "python", "size": 5995}
{"docstring": "\"\"\"\nPassing ``send_email=False`` when creating a new user will not\nsend an activation email.\n\n\"\"\"\n", "func_signal": "def test_user_creation_no_email(self):\n", "code": "new_user = RegistrationProfile.objects.create_inactive_user(site=Site.objects.get_current(),\n                                                            send_email=False,\n                                                            **self.user_info)\nself.assertEqual(len(mail.outbox), 0)", "path": "registration\\tests\\models.py", "repo_name": "yuvipanda/wtfimb", "stars": 36, "license": "None", "language": "python", "size": 5995}
{"docstring": "\"\"\"\nAttempting to activate outside the permitted window does not\nactivate the account.\n\n\"\"\"\n", "func_signal": "def test_expired_activation(self):\n", "code": "new_user = RegistrationProfile.objects.create_inactive_user(site=Site.objects.get_current(),\n                                                            **self.user_info)\nnew_user.date_joined -= datetime.timedelta(days=settings.ACCOUNT_ACTIVATION_DAYS + 1)\nnew_user.save()\n\nprofile = RegistrationProfile.objects.get(user=new_user)\nactivated = RegistrationProfile.objects.activate_user(profile.activation_key)\n\nself.failIf(isinstance(activated, User))\nself.failIf(activated)\n\nnew_user = User.objects.get(username='alice')\nself.failIf(new_user.is_active)\n\nprofile = RegistrationProfile.objects.get(user=new_user)\nself.assertNotEqual(profile.activation_key, RegistrationProfile.ACTIVATED)", "path": "registration\\tests\\models.py", "repo_name": "yuvipanda/wtfimb", "stars": 36, "license": "None", "language": "python", "size": 5995}
{"docstring": "\"\"\"\n``RegistrationProfile.activation_key_expired()`` is ``True``\noutside the activation window.\n\n\"\"\"\n", "func_signal": "def test_expired_account(self):\n", "code": "new_user = RegistrationProfile.objects.create_inactive_user(site=Site.objects.get_current(),\n                                                            **self.user_info)\nnew_user.date_joined -= datetime.timedelta(days=settings.ACCOUNT_ACTIVATION_DAYS + 1)\nnew_user.save()\nprofile = RegistrationProfile.objects.get(user=new_user)\nself.failUnless(profile.activation_key_expired())", "path": "registration\\tests\\models.py", "repo_name": "yuvipanda/wtfimb", "stars": 36, "license": "None", "language": "python", "size": 5995}
{"docstring": "\"\"\"\nActivating a user within the permitted window makes the\naccount active, and resets the activation key.\n\n\"\"\"\n", "func_signal": "def test_valid_activation(self):\n", "code": "new_user = RegistrationProfile.objects.create_inactive_user(site=Site.objects.get_current(),\n                                                            **self.user_info)\nprofile = RegistrationProfile.objects.get(user=new_user)\nactivated = RegistrationProfile.objects.activate_user(profile.activation_key)\n\nself.failUnless(isinstance(activated, User))\nself.assertEqual(activated.id, new_user.id)\nself.failUnless(activated.is_active)\n\nprofile = RegistrationProfile.objects.get(user=new_user)\nself.assertEqual(profile.activation_key, RegistrationProfile.ACTIVATED)", "path": "registration\\tests\\models.py", "repo_name": "yuvipanda/wtfimb", "stars": 36, "license": "None", "language": "python", "size": 5995}
{"docstring": "\"\"\"\nCheck the supplied email address against a list of known free\nwebmail domains.\n\n\"\"\"\n", "func_signal": "def clean_email(self):\n", "code": "email_domain = self.cleaned_data['email'].split('@')[1]\nif email_domain in self.bad_domains:\n    raise forms.ValidationError(_(\"Registration using free email addresses is prohibited. Please supply a different email address.\"))\nreturn self.cleaned_data['email']", "path": "registration\\forms.py", "repo_name": "yuvipanda/wtfimb", "stars": 36, "license": "None", "language": "python", "size": 5995}
{"docstring": "\"\"\"\nValidate that the supplied email address is unique for the\nsite.\n\n\"\"\"\n", "func_signal": "def clean_email(self):\n", "code": "if User.objects.filter(email__iexact=self.cleaned_data['email']):\n    raise forms.ValidationError(_(\"This email address is already in use. Please supply a different email address.\"))\nreturn self.cleaned_data['email']", "path": "registration\\forms.py", "repo_name": "yuvipanda/wtfimb", "stars": 36, "license": "None", "language": "python", "size": 5995}
{"docstring": "\"\"\"Returns a new copy of a C{ParseResults} object.\"\"\"\n", "func_signal": "def copy( self ):\n", "code": "ret = ParseResults( self.__toklist )\nret.__tokdict = self.__tokdict.copy()\nret.__parent = self.__parent\nret.__accumNames.update( self.__accumNames )\nret.__name = self.__name\nreturn ret", "path": "src\\pyparsing.py", "repo_name": "greghaskins/pyparsing", "stars": 36, "license": "None", "language": "python", "size": 2001}
{"docstring": "\"\"\"Helper to create a validating parse action to be used with start tags created\n   with C{makeXMLTags} or C{makeHTMLTags}. Use C{withAttribute} to qualify a starting tag\n   with a required attribute value, to avoid false matches on common tags such as\n   C{<TD>} or C{<DIV>}.\n\n   Call C{withAttribute} with a series of attribute names and values. Specify the list\n   of filter attributes names and values as:\n    - keyword arguments, as in C{(align=\"right\")}, or\n    - as an explicit dict with C{**} operator, when an attribute name is also a Python\n      reserved word, as in C{**{\"class\":\"Customer\", \"align\":\"right\"}}\n    - a list of name-value tuples, as in ( (\"ns1:class\", \"Customer\"), (\"ns2:align\",\"right\") )\n   For attribute names with a namespace prefix, you must use the second form.  Attribute\n   names are matched insensitive to upper/lower case.\n\n   To verify that the attribute exists, but without specifying a value, pass\n   C{withAttribute.ANY_VALUE} as the value.\n   \"\"\"\n", "func_signal": "def withAttribute(*args,**attrDict):\n", "code": "if args:\n    attrs = args[:]\nelse:\n    attrs = attrDict.items()\nattrs = [(k,v) for k,v in attrs]\ndef pa(s,l,tokens):\n    for attrName,attrValue in attrs:\n        if attrName not in tokens:\n            raise ParseException(s,l,\"no matching attribute \" + attrName)\n        if attrValue != withAttribute.ANY_VALUE and tokens[attrName] != attrValue:\n            raise ParseException(s,l,\"attribute '%s' has value '%s', must be '%s'\" %\n                                        (attrName, tokens[attrName], attrValue))\nreturn pa", "path": "src\\pyparsing.py", "repo_name": "greghaskins/pyparsing", "stars": 36, "license": "None", "language": "python", "size": 2001}
{"docstring": "\"\"\"Implementation of ^ operator - returns C{Or}\"\"\"\n", "func_signal": "def __xor__(self, other ):\n", "code": "if isinstance( other, basestring ):\n    other = Literal( other )\nif not isinstance( other, ParserElement ):\n    warnings.warn(\"Cannot combine element of type %s with ParserElement\" % type(other),\n            SyntaxWarning, stacklevel=2)\n    return None\nreturn Or( [ self, other ] )", "path": "src\\pyparsing.py", "repo_name": "greghaskins/pyparsing", "stars": 36, "license": "None", "language": "python", "size": 2001}
{"docstring": "\"\"\"Scan the input string for expression matches.  Each match will return the\n   matching tokens, start location, and end location.  May be called with optional\n   C{maxMatches} argument, to clip scanning after 'n' matches are found.  If\n   C{overlap} is specified, then overlapping matches will be reported.\n\n   Note that the start and end locations are reported relative to the string\n   being parsed.  See L{I{parseString}<parseString>} for more information on parsing\n   strings with embedded tabs.\"\"\"\n", "func_signal": "def scanString( self, instring, maxMatches=_MAX_INT, overlap=False ):\n", "code": "if not self.streamlined:\n    self.streamline()\nfor e in self.ignoreExprs:\n    e.streamline()\n\nif not self.keepTabs:\n    instring = _ustr(instring).expandtabs()\ninstrlen = len(instring)\nloc = 0\npreparseFn = self.preParse\nparseFn = self._parse\nParserElement.resetCache()\nmatches = 0\ntry:\n    while loc <= instrlen and matches < maxMatches:\n        try:\n            preloc = preparseFn( instring, loc )\n            nextLoc,tokens = parseFn( instring, preloc, callPreParse=False )\n        except ParseException:\n            loc = preloc+1\n        else:\n            if nextLoc > loc:\n                matches += 1\n                yield tokens, preloc, nextLoc\n                if overlap:\n                    nextloc = preparseFn( instring, loc )\n                    if nextloc > loc:\n                        loc = nextLoc\n                    else:\n                        loc += 1\n                else:\n                    loc = nextLoc\n            else:\n                loc = preloc+1\nexcept ParseBaseException:\n    if ParserElement.verbose_stacktrace:\n        raise\n    else:\n        # catch and re-raise exception from here, clears out pyparsing internal stack trace\n        exc = sys.exc_info()[1]\n        raise exc", "path": "src\\pyparsing.py", "repo_name": "greghaskins/pyparsing", "stars": 36, "license": "None", "language": "python", "size": 2001}
{"docstring": "\"\"\"Helper method for defining parse actions that require matching at a specific\n   column in the input text.\n\"\"\"\n", "func_signal": "def matchOnlyAtCol(n):\n", "code": "def verifyCol(strg,locn,toks):\n    if col(locn,strg) != n:\n        raise ParseException(strg,locn,\"matched token not at column %d\" % n)\nreturn verifyCol", "path": "src\\pyparsing.py", "repo_name": "greghaskins/pyparsing", "stars": 36, "license": "None", "language": "python", "size": 2001}
{"docstring": "\"\"\"Implementation of - operator, returns C{And} with error stop\"\"\"\n", "func_signal": "def __sub__(self, other):\n", "code": "if isinstance( other, basestring ):\n    other = Literal( other )\nif not isinstance( other, ParserElement ):\n    warnings.warn(\"Cannot combine element of type %s with ParserElement\" % type(other),\n            SyntaxWarning, stacklevel=2)\n    return None\nreturn And( [ self, And._ErrorStop(), other ] )", "path": "src\\pyparsing.py", "repo_name": "greghaskins/pyparsing", "stars": 36, "license": "None", "language": "python", "size": 2001}
{"docstring": "\"\"\"Enable display of debugging messages while doing pattern matching.\"\"\"\n", "func_signal": "def setDebugActions( self, startAction, successAction, exceptionAction ):\n", "code": "self.debugActions = (startAction or _defaultStartDebugAction,\n                     successAction or _defaultSuccessDebugAction,\n                     exceptionAction or _defaultExceptionDebugAction)\nself.debug = True\nreturn self", "path": "src\\pyparsing.py", "repo_name": "greghaskins/pyparsing", "stars": 36, "license": "None", "language": "python", "size": 2001}
{"docstring": "\"\"\"Helper method for defining space-delimited indentation blocks, such as\n   those used to define block statements in Python source code.\n\n   Parameters:\n    - blockStatementExpr - expression defining syntax of statement that\n        is repeated within the indented block\n    - indentStack - list created by caller to manage indentation stack\n        (multiple statementWithIndentedBlock expressions within a single grammar\n        should share a common indentStack)\n    - indent - boolean indicating whether block must be indented beyond the\n        the current level; set to False for block of left-most statements\n        (default=True)\n\n   A valid block must contain at least one C{blockStatement}.\n\"\"\"\n", "func_signal": "def indentedBlock(blockStatementExpr, indentStack, indent=True):\n", "code": "def checkPeerIndent(s,l,t):\n    if l >= len(s): return\n    curCol = col(l,s)\n    if curCol != indentStack[-1]:\n        if curCol > indentStack[-1]:\n            raise ParseFatalException(s,l,\"illegal nesting\")\n        raise ParseException(s,l,\"not a peer entry\")\n\ndef checkSubIndent(s,l,t):\n    curCol = col(l,s)\n    if curCol > indentStack[-1]:\n        indentStack.append( curCol )\n    else:\n        raise ParseException(s,l,\"not a subentry\")\n\ndef checkUnindent(s,l,t):\n    if l >= len(s): return\n    curCol = col(l,s)\n    if not(indentStack and curCol < indentStack[-1] and curCol <= indentStack[-2]):\n        raise ParseException(s,l,\"not an unindent\")\n    indentStack.pop()\n\nNL = OneOrMore(LineEnd().setWhitespaceChars(\"\\t \").suppress())\nINDENT = Empty() + Empty().setParseAction(checkSubIndent)\nPEER   = Empty().setParseAction(checkPeerIndent)\nUNDENT = Empty().setParseAction(checkUnindent)\nif indent:\n    smExpr = Group( Optional(NL) +\n        #~ FollowedBy(blockStatementExpr) +\n        INDENT + (OneOrMore( PEER + Group(blockStatementExpr) + Optional(NL) )) + UNDENT)\nelse:\n    smExpr = Group( Optional(NL) +\n        (OneOrMore( PEER + Group(blockStatementExpr) + Optional(NL) )) )\nblockStatementExpr.ignore(_bslash + LineEnd())\nreturn smExpr", "path": "src\\pyparsing.py", "repo_name": "greghaskins/pyparsing", "stars": 36, "license": "None", "language": "python", "size": 2001}
{"docstring": "\"\"\"Make a copy of this C{ParserElement}.  Useful for defining different parse actions\n   for the same parsing pattern, using copies of the original parse element.\"\"\"\n", "func_signal": "def copy( self ):\n", "code": "cpy = copy.copy( self )\ncpy.parseAction = self.parseAction[:]\ncpy.ignoreExprs = self.ignoreExprs[:]\nif self.copyDefaultWhiteChars:\n    cpy.whiteChars = ParserElement.DEFAULT_WHITE_CHARS\nreturn cpy", "path": "src\\pyparsing.py", "repo_name": "greghaskins/pyparsing", "stars": 36, "license": "None", "language": "python", "size": 2001}
{"docstring": "#~ nonlocal limit\n", "func_signal": "def wrapper(*args):\n", "code": "while 1:\n    try:\n        return func(*args[limit:])\n    except TypeError:\n        if limit:\n            limit -= 1\n            continue\n        raise", "path": "src\\pyparsing.py", "repo_name": "greghaskins/pyparsing", "stars": 36, "license": "None", "language": "python", "size": 2001}
{"docstring": "# must be at least one\n", "func_signal": "def parseImpl( self, instring, loc, doActions=True ):\n", "code": "loc, tokens = self.expr._parse( instring, loc, doActions, callPreParse=False )\ntry:\n    hasIgnoreExprs = ( len(self.ignoreExprs) > 0 )\n    while 1:\n        if hasIgnoreExprs:\n            preloc = self._skipIgnorables( instring, loc )\n        else:\n            preloc = loc\n        loc, tmptokens = self.expr._parse( instring, preloc, doActions )\n        if tmptokens or tmptokens.keys():\n            tokens += tmptokens\nexcept (ParseException,IndexError):\n    pass\n\nreturn loc, tokens", "path": "src\\pyparsing.py", "repo_name": "greghaskins/pyparsing", "stars": 36, "license": "None", "language": "python", "size": 2001}
{"docstring": "\"\"\"supported attributes by name are:\n    - lineno - returns the line number of the exception text\n    - col - returns the column number of the exception text\n    - line - returns the line containing the exception text\n\"\"\"\n", "func_signal": "def __getattr__( self, aname ):\n", "code": "if( aname == \"lineno\" ):\n    return lineno( self.loc, self.pstr )\nelif( aname in (\"col\", \"column\") ):\n    return col( self.loc, self.pstr )\nelif( aname == \"line\" ):\n    return line( self.loc, self.pstr )\nelse:\n    raise AttributeError(aname)", "path": "src\\pyparsing.py", "repo_name": "greghaskins/pyparsing", "stars": 36, "license": "None", "language": "python", "size": 2001}
{"docstring": "\"\"\"Enable display of debugging messages while doing pattern matching.\n   Set C{flag} to True to enable, False to disable.\"\"\"\n", "func_signal": "def setDebug( self, flag=True ):\n", "code": "if flag:\n    self.setDebugActions( _defaultStartDebugAction, _defaultSuccessDebugAction, _defaultExceptionDebugAction )\nelse:\n    self.debug = False\nreturn self", "path": "src\\pyparsing.py", "repo_name": "greghaskins/pyparsing", "stars": 36, "license": "None", "language": "python", "size": 2001}
{"docstring": "\"\"\"Execute the parse expression with the given string.\n   This is the main interface to the client code, once the complete\n   expression has been built.\n\n   If you want the grammar to require that the entire input string be\n   successfully parsed, then set C{parseAll} to True (equivalent to ending\n   the grammar with C{StringEnd()}).\n\n   Note: C{parseString} implicitly calls C{expandtabs()} on the input string,\n   in order to report proper column numbers in parse actions.\n   If the input string contains tabs and\n   the grammar uses parse actions that use the C{loc} argument to index into the\n   string being parsed, you can ensure you have a consistent view of the input\n   string by:\n    - calling C{parseWithTabs} on your grammar before calling C{parseString}\n      (see L{I{parseWithTabs}<parseWithTabs>})\n    - define your parse action using the full C{(s,loc,toks)} signature, and\n      reference the input string using the parse action's C{s} argument\n    - explictly expand the tabs in your input string before calling\n      C{parseString}\n\"\"\"\n", "func_signal": "def parseString( self, instring, parseAll=False ):\n", "code": "ParserElement.resetCache()\nif not self.streamlined:\n    self.streamline()\n    #~ self.saveAsList = True\nfor e in self.ignoreExprs:\n    e.streamline()\nif not self.keepTabs:\n    instring = instring.expandtabs()\ntry:\n    loc, tokens = self._parse( instring, 0 )\n    if parseAll:\n        loc = self.preParse( instring, loc )\n        se = Empty() + StringEnd()\n        se._parse( instring, loc )\nexcept ParseBaseException:\n    if ParserElement.verbose_stacktrace:\n        raise\n    else:\n        # catch and re-raise exception from here, clears out pyparsing internal stack trace\n        exc = sys.exc_info()[1]\n        raise exc\nelse:\n    return tokens", "path": "src\\pyparsing.py", "repo_name": "greghaskins/pyparsing", "stars": 36, "license": "None", "language": "python", "size": 2001}
{"docstring": "\"\"\"Helper to define an expression that is indirectly defined from\n   the tokens matched in a previous expression, that is, it looks\n   for a 'repeat' of a previous expression.  For example::\n       first = Word(nums)\n       second = matchPreviousLiteral(first)\n       matchExpr = first + \":\" + second\n   will match C{\"1:1\"}, but not C{\"1:2\"}.  Because this matches a\n   previous literal, will also match the leading C{\"1:1\"} in C{\"1:10\"}.\n   If this is not desired, use C{matchPreviousExpr}.\n   Do *not* use with packrat parsing enabled.\n\"\"\"\n", "func_signal": "def matchPreviousLiteral(expr):\n", "code": "rep = Forward()\ndef copyTokenToRepeater(s,l,t):\n    if t:\n        if len(t) == 1:\n            rep << t[0]\n        else:\n            # flatten t tokens\n            tflat = _flatten(t.asList())\n            rep << And( [ Literal(tt) for tt in tflat ] )\n    else:\n        rep << Empty()\nexpr.addParseAction(copyTokenToRepeater, callDuringTry=True)\nreturn rep", "path": "src\\pyparsing.py", "repo_name": "greghaskins/pyparsing", "stars": 36, "license": "None", "language": "python", "size": 2001}
{"docstring": "\"\"\"Define name for referencing matching tokens as a nested attribute\n   of the returned parse results.\n   NOTE: this returns a *copy* of the original C{ParserElement} object;\n   this is so that the client can define a basic element, such as an\n   integer, and reference it in multiple places with different names.\n   \n   You can also set results names using the abbreviated syntax,\n   C{expr(\"name\")} in place of C{expr.setResultsName(\"name\")} - \n   see L{I{__call__}<__call__>}.\n\"\"\"\n", "func_signal": "def setResultsName( self, name, listAllMatches=False ):\n", "code": "newself = self.copy()\nif name.endswith(\"*\"):\n    name = name[:-1]\n    listAllMatches=True\nnewself.resultsName = name\nnewself.modalResults = not listAllMatches\nreturn newself", "path": "src\\pyparsing.py", "repo_name": "greghaskins/pyparsing", "stars": 36, "license": "None", "language": "python", "size": 2001}
{"docstring": "\"\"\"Implementation of + operator when left operand is not a C{ParserElement}\"\"\"\n", "func_signal": "def __radd__(self, other ):\n", "code": "if isinstance( other, basestring ):\n    other = Literal( other )\nif not isinstance( other, ParserElement ):\n    warnings.warn(\"Cannot combine element of type %s with ParserElement\" % type(other),\n            SyntaxWarning, stacklevel=2)\n    return None\nreturn other + self", "path": "src\\pyparsing.py", "repo_name": "greghaskins/pyparsing", "stars": 36, "license": "None", "language": "python", "size": 2001}
{"docstring": "\"\"\"Extension to C{scanString}, to modify matching text with modified tokens that may\n   be returned from a parse action.  To use C{transformString}, define a grammar and\n   attach a parse action to it that modifies the returned token list.\n   Invoking C{transformString()} on a target string will then scan for matches,\n   and replace the matched text patterns according to the logic in the parse\n   action.  C{transformString()} returns the resulting transformed string.\"\"\"\n", "func_signal": "def transformString( self, instring ):\n", "code": "out = []\nlastE = 0\n# force preservation of <TAB>s, to minimize unwanted transformation of string, and to\n# keep string locs straight between transformString and scanString\nself.keepTabs = True\ntry:\n    for t,s,e in self.scanString( instring ):\n        out.append( instring[lastE:s] )\n        if t:\n            if isinstance(t,ParseResults):\n                out += t.asList()\n            elif isinstance(t,list):\n                out += t\n            else:\n                out.append(t)\n        lastE = e\n    out.append(instring[lastE:])\n    out = [o for o in out if o]\n    return \"\".join(map(_ustr,_flatten(out)))\nexcept ParseBaseException:\n    if ParserElement.verbose_stacktrace:\n        raise\n    else:\n        # catch and re-raise exception from here, clears out pyparsing internal stack trace\n        exc = sys.exc_info()[1]\n        raise exc", "path": "src\\pyparsing.py", "repo_name": "greghaskins/pyparsing", "stars": 36, "license": "None", "language": "python", "size": 2001}
{"docstring": "\"\"\"Returns the line of text containing loc within a string, counting newlines as line separators.\n   \"\"\"\n", "func_signal": "def line( loc, strg ):\n", "code": "lastCR = strg.rfind(\"\\n\", 0, loc)\nnextCR = strg.find(\"\\n\", loc)\nif nextCR >= 0:\n    return strg[lastCR+1:nextCR]\nelse:\n    return strg[lastCR+1:]", "path": "src\\pyparsing.py", "repo_name": "greghaskins/pyparsing", "stars": 36, "license": "None", "language": "python", "size": 2001}
{"docstring": "\"\"\"Implementation of ^ operator when left operand is not a C{ParserElement}\"\"\"\n", "func_signal": "def __rxor__(self, other ):\n", "code": "if isinstance( other, basestring ):\n    other = Literal( other )\nif not isinstance( other, ParserElement ):\n    warnings.warn(\"Cannot combine element of type %s with ParserElement\" % type(other),\n            SyntaxWarning, stacklevel=2)\n    return None\nreturn other ^ self", "path": "src\\pyparsing.py", "repo_name": "greghaskins/pyparsing", "stars": 36, "license": "None", "language": "python", "size": 2001}
{"docstring": "\"\"\"Initializes the list.\nThe iterable will be turned into EmDocuments as well.\n\nArgs:\n  emdocument_class: The class for the custom EmDocument\n  iterable: The iterable of dictionary or the EmDocument objects\n\"\"\"\n", "func_signal": "def __init__(self, emdocument_class, iterable=None):\n", "code": "self.emdocument_class = emdocument_class\nnew_list = self._standardizeList(_valueOrList(iterable))\nlist.__init__(self, new_list)", "path": "riakkit\\commons\\properties.py", "repo_name": "shuhaowu/riakkit", "stars": 36, "license": "lgpl-3.0", "language": "python", "size": 881}
{"docstring": "\"\"\"A generator that goes through riak_link\"\"\"\n", "func_signal": "def run(self):\n", "code": "for link in self.riak_links:\n  yield self.cls.load(link.get())", "path": "riakkit\\queries.py", "repo_name": "shuhaowu/riakkit", "stars": 36, "license": "lgpl-3.0", "language": "python", "size": 881}
{"docstring": "\"\"\"The default validation function.\n\nThis validation function goes through the validators and checks each one.\nAny subclass should do:\n\nreturn BaseProperty.validate(self, value) and your_result\n\nwhere your_result is your validation result.\n\nArgs:\n  value: The value to be validated\n\nReturns:\n  True if validation pass, False otherwise.\n\"\"\"\n\n", "func_signal": "def validate(self, value):\n", "code": "if callable(self.validators):\n  return self.validators(value)\nelif type(self.validators) in (tuple, list):\n  for validator in self.validators:\n    if not validator(value):\n      return False\n\nreturn True", "path": "riakkit\\commons\\properties.py", "repo_name": "shuhaowu/riakkit", "stars": 36, "license": "lgpl-3.0", "language": "python", "size": 881}
{"docstring": "\"\"\"Converts the value from the database back to an app friendly value (a\nvalue that standardize() will consider as valid).\n\nNote that the BaseProperty's convertFromDb will change the value from None\n(if that's how it was in the database) to the default value, if the default\nvalue is not None. This is done so that any attributes that's added after\nthe object is saved that has a default value wouldn't be None.\n\nMust be able to handle None.\n\nArgs:\n  value: The value to be converted\n\"\"\"\n", "func_signal": "def convertFromDb(self, value):\n", "code": "default = self.defaultValue()\nif value is None and default is not None:\n  value = default\nreturn self._processValue(value, self.backwardprocessors)", "path": "riakkit\\commons\\properties.py", "repo_name": "shuhaowu/riakkit", "stars": 36, "license": "lgpl-3.0", "language": "python", "size": 881}
{"docstring": "\"\"\"Generates a random string from a-zA-Z0-9 using os.urandom.\n\nCould be a way to generate the keys. May collide more, not certain though.\n\nArgs:\n  n: The length of the string.\nReturns:\n  a n-length string of random characters\n\"\"\"\n", "func_signal": "def rndstr(n):\n", "code": "t = \"\"\nwhile n > 0:\n  i = ord(os.urandom(1))\n  while i >= 248:\n    i = ord(os.urandom(1))\n  i %= 62\n  t += _p[i]\n  n -= 1\nreturn t", "path": "riakkit\\commons\\__init__.py", "repo_name": "shuhaowu/riakkit", "stars": 36, "license": "lgpl-3.0", "language": "python", "size": 881}
{"docstring": "\"\"\"Reloads the object from the database.\n\nThis grabs the most recent version of the object from the database and\nupdates the document accordingly. The data will change if the object\nfrom the database has been changed at one point.\n\nThis only works if the object has been saved at least once before.\n\nReturns:\n  self for OOP.\n\nRaises:\n  NotFoundError: if the object hasn't been saved before.\n\"\"\"\n", "func_signal": "def reload(self, r=None, vtag=None):\n", "code": "if self._obj:\n  self._obj.reload(r=r, vtag=vtag)\n  if not self._obj.exists():\n    self._deleted()\n  else:\n    self.saved = True\n    self.deleted = False\n    self.deserialize(self._obj.get_data())\n    self.setIndexes(self._getIndexesFromRiakObj(self._obj))\n    self.setLinks(self._getLinksFromRiakObj(self._obj))\nelse:\n  raise NotFoundError(\"Object not saved!\")", "path": "riakkit\\document.py", "repo_name": "shuhaowu/riakkit", "stars": 36, "license": "lgpl-3.0", "language": "python", "size": 881}
{"docstring": "\"\"\"Searches through the bucket with some query text.\n\nThe bucket must have search installed via search-cmd install BUCKETNAME. The\nclass must have been marked to be  with cls. = True.\n\nArgs:\n  querytext: The query text as outlined in the python-riak documentations.\n  bucket: The bucket to search. Leave default for the default bucket.\n\nReturns:\n  A MapReduceQuery object. Similar to the RiakMapReduce object.\"\"\"\n", "func_signal": "def search(cls, querytext, bucket=None):\n", "code": "query_obj = cls.client.search(cls.bucket_name[0] if bucket is None else bucket, querytext)\nreturn MapReduceQuery(cls, query_obj)", "path": "riakkit\\document.py", "repo_name": "shuhaowu/riakkit", "stars": 36, "license": "lgpl-3.0", "language": "python", "size": 881}
{"docstring": "# Since there's no collection_names, no ensuring that saving d will save m.\n# TODO: Fix this?\n", "func_signal": "def test_emdocumentWithReference(self):\n", "code": "m = SearchableModel(intprop=1337).save()\n\ne = EmDocumentWithRef(ref=m)\nd = TestEmDocumentWithRef(em=e)\nd.save()\n\nd.reload()\nself.assertEquals(m, d.em.ref)\n\nd.delete()\nm.delete()", "path": "test_all.py", "repo_name": "shuhaowu/riakkit", "stars": 36, "license": "lgpl-3.0", "language": "python", "size": 881}
{"docstring": "\"\"\"Used in __new__ while getting attributes of class objects that's about to\nbe created.\n\nThis also looks up the ladder of the parents of this class object.\n\nArgs:\n  name: Name of the attribute\n  attrs: The attributes from the __new__ methods.\n  parents: The class objects that's the parent of this class from the __new__.\n\nReturns:\n  None if the attributes is not found from the attrs nor the parents.\n  Otherwise the first one that's found, from attrs to BFS'ed parents.\n\"\"\"\n", "func_signal": "def getProperty(name, attrs, parents):\n", "code": "parents = walkParents(parents)\nvalue = attrs.get(name, None)\ni = 0\nlength = len(parents)\nwhile value is None:\n  if i == length:\n    return None\n  value = getattr(parents[i], name, None)\n  i += 1\n\nreturn value", "path": "riakkit\\commons\\__init__.py", "repo_name": "shuhaowu/riakkit", "stars": 36, "license": "lgpl-3.0", "language": "python", "size": 881}
{"docstring": "\"\"\"Returns the default specified or now.\"\"\"\n", "func_signal": "def defaultValue(self):\n", "code": "if callable(self.default):\n  return self.default()\n\nreturn self.default or datetime.datetime.fromtimestamp(time.time())", "path": "riakkit\\commons\\properties.py", "repo_name": "shuhaowu/riakkit", "stars": 36, "license": "lgpl-3.0", "language": "python", "size": 881}
{"docstring": "\"\"\"Similar to get, but does not raise error if not found. A new (unsaved)\ndocument will be created.\n\nArgs:\n  Everything: The same as get.\n  For this method. The bucket argument only has effect on the get operation.\n  **kwargs: Additional kwargs to merge data into the document.\n\"\"\"\n", "func_signal": "def getOrNew(cls, key, cached=True, r=None, bucket=None, **kwargs):\n", "code": "try:\n  d = cls.load(key, cached, r, bucket)\n  d.mergeData(kwargs)\n  return d\nexcept NotFoundError:\n  return cls(key=key, **kwargs)", "path": "riakkit\\document.py", "repo_name": "shuhaowu/riakkit", "stars": 36, "license": "lgpl-3.0", "language": "python", "size": 881}
{"docstring": "\"\"\"It's kind of like a deep copy, but it only make copies of lists, tuples,\nand dictionaries (and other primitive types). Other complex object such as\nones you created are kept as references.\n\nArg:\n  obj: Any object.\n\"\"\"\n", "func_signal": "def mediocreCopy(obj):\n", "code": "if isinstance(obj, list): # TODO: Sets\n  return [mediocreCopy(i) for i in obj]\nif isinstance(obj, tuple):\n  return tuple(mediocreCopy(i) for i in obj)\nif isinstance(obj, dict):\n  return dict(mediocreCopy(i) for i in obj.iteritems())\nreturn obj", "path": "riakkit\\commons\\__init__.py", "repo_name": "shuhaowu/riakkit", "stars": 36, "license": "lgpl-3.0", "language": "python", "size": 881}
{"docstring": "\"\"\"Initializes a Reference Property\n\nYou can set it up so that riakkit automatically link back from\nthe reference_class like GAE's ReferenceProperty. Except everything here\nis a list.\n\nArgs:\n  reference_class: The classes that should be added to this LinkedDocuments.\n                   i.e. Documents have to be objects of that class to pass\n                   validation. None if you wish to allow any Document class.\n  collection_name: The collection name for the reference_class. Works the\n                   same way as GAE's collection_name for their\n                   ReferenceProperty. See the README file at the repository\n                   for detailed tutorial.\n  strict: If true, the remote object must exist. Otherwise it doesn't have to.\n\"\"\"\n", "func_signal": "def __init__(self, reference_class, collection_name=None, required=False, strict=True):\n", "code": "BaseProperty.__init__(self, required=required)\nif not reference_class._clsType:\n  raise TypeError(\"Reference property cannot be constructed with class '%s'\" % reference_class.__name__)\n\nself.clstype = reference_class._clsType\nself.reference_class = reference_class\nself.collection_name = collection_name\nself.is_reference_back = False\nself.strict = strict", "path": "riakkit\\commons\\properties.py", "repo_name": "shuhaowu/riakkit", "stars": 36, "license": "lgpl-3.0", "language": "python", "size": 881}
{"docstring": "\"\"\"The default value for this type.\n\nThe default value is the value after being standardize()'d. Whenever an\nattribute is not present in the data when saved, this value will be\nsubstituted in.\n\nReturns:\n  The default value for this type.\n\"\"\"\n", "func_signal": "def defaultValue(self):\n", "code": "if callable(self.default):\n  return self.default()\n\nreturn self.default", "path": "riakkit\\commons\\properties.py", "repo_name": "shuhaowu/riakkit", "stars": 36, "license": "lgpl-3.0", "language": "python", "size": 881}
{"docstring": "\"\"\"Saves the document into the database.\n\nThis will save the object to the database. All linked objects will be saved\nas well.\n\nArgs:\n  w: W value\n  dw: DW value\n  endpoint: See if this is an endpoint. i.e. It will save the documents\n            that's modified while modifying this one. Default: False\n  bucket: Save to a specific bucket. Default is the default bucket. Only\n          has an effect if the document is new.\n\"\"\"\n", "func_signal": "def save(self, w=None, dw=None, endpoint=False, bucket=None):\n", "code": "dataToBeSaved = self.serialize()\nuniquesToBeDeleted = []\nothersToBeSaved = []\n\n# Process uniques\nfor name in self._uniques:\n  if self._data.get(name, None) is None:\n    if self._obj: # TODO: could be somehow refactored, as this condition is always true?\n      originalValue = self._obj.get_data().get(name, None)\n      if originalValue is not None:\n        uniquesToBeDeleted.append((self._meta[name].unique_bucket, originalValue))\n  else:\n    changed = False\n    if self._obj:\n      originalValue = self._obj.get_data().get(name, None)\n      if self._data[name] != originalValue and originalValue is not None:\n        uniquesToBeDeleted.append((self._meta[name].unique_bucket, originalValue))\n        changed = True\n    else:\n      changed = True\n\n    if changed and self._meta[name].unique_bucket.get(dataToBeSaved[name]).exists():\n      raise IntegrityError(\n        field=name,\n        message=\"'%s' already exists for '%s'!\" % (self._data[name], name)\n      )\n\n# Process references\nfor name in self._references:\n  currentDocsKeys = None\n  strict = self._meta[name].strict\n  colname = self._meta[name].collection_name\n\n  if colname:\n    currentDocsKeys = set()\n    if isinstance(self._meta[name], ReferenceProperty):\n      docs = [getattr(self, name)]\n    else:\n      docs = getattr(self, name)\n\n    for doc in docs: # These are foreign documents\n      if doc is None or (not strict and not doc.__class__.exists(doc.key)):\n        continue\n\n      currentDocsKeys.add(doc.key)\n\n      currentList = getattr(doc, colname, [])\n      found = False # Linear search algorithm. Maybe binary search??\n      for d in currentList:\n        if d.key == self.key:\n          found = True\n          break\n      if not found:\n        currentList.append(self)\n        doc._data[colname] = currentList\n        othersToBeSaved.append((doc, False))\n\n\n  colname = colname or self._meta[name].is_reference_back\n\n  if colname:\n    if self._obj:\n      originalValues = self._obj.get_data().get(name, [])\n      if not isinstance(originalValues, list):\n        originalValues = [originalValues]\n    else:\n      originalValues = []\n\n    if currentDocsKeys is None:\n      currentDocsKeys = set()\n      for d in self._data[name]:\n        if d is None:\n          continue\n        else:\n          currentDocsKeys.add(getattr(d, \"key\", d))\n\n    for dockey in originalValues:\n      if dockey is None:\n        continue\n\n      # This means that this specific document is not in the current version,\n      # but last version. Hence it needs to be cleaned from the last version.\n      if dockey not in currentDocsKeys:\n        try:\n          doc = self._meta[name].reference_class.load(dockey, True)\n        except NotFoundError: # TODO: Another hackjob? This is _probably_ due to we're back deleting the reference.\n          continue\n        if doc._meta[colname].deleteReference(doc, self):\n          othersToBeSaved.append((doc, True)) # CODE-REVIEW: For some reason i feel this won't work for some cases.\n\n\nif self._obj:\n  self._obj.set_data(dataToBeSaved)\nelse:\n  bucket = self.buckets.get(bucket, self.bucket)\n  self._obj = bucket.new(self.key, dataToBeSaved)\n\nself._obj.set_links(self.links(True), True)\nself._obj.set_indexes(self.indexes())\nself.key = self._obj.get_key()\n\nself._obj.store(w=w, dw=dw)\nfor name in self._uniques:\n  if self._data[name] and not self._meta[name].unique_bucket.get(self._data[name]).exists():\n    obj = self._meta[name].unique_bucket.new(self._data[name], {\"key\" : self.key})\n    obj.store(w=w, dw=dw)\n\nfor bucket, key in uniquesToBeDeleted:\n  bucket.get(key).delete()\n\nself.saved = True\nself.deleted = False\n\nif not endpoint: # CODE-REVIEW: Total hackjob. This gotta be redone\n  for doc, end in othersToBeSaved:\n    doc.save(w, dw, end)\n\nreturn self", "path": "riakkit\\document.py", "repo_name": "shuhaowu/riakkit", "stars": 36, "license": "lgpl-3.0", "language": "python", "size": 881}
{"docstring": "\"\"\"Checks if a value exists in the db if the unique flag is turned on.\n\nArgs:\n  value: The value to check. No processing will be done here.\n\nReturns:\n  True/False if it exist or not. None if the unique flag is not on.\n\"\"\"\n", "func_signal": "def hasValue(self, value):\n", "code": "if self.unique:\n  return self.unique_bucket.get(value).exists()\nreturn None", "path": "riakkit\\commons\\properties.py", "repo_name": "shuhaowu/riakkit", "stars": 36, "license": "lgpl-3.0", "language": "python", "size": 881}
{"docstring": "\"\"\"Gets all the links.\n\nArgs:\n  riakLinks: Defaults to False. If True, it will return a list of RiakLinks\n\nReturns:\n  A set of (document, tag) or [RiakLink, RiakLink]\"\"\"\n", "func_signal": "def links(self, riakLinks=False):\n", "code": "if riakLinks:\n  return [RiakLink(self.bucket_name[0], d.key, t) for d, t in self._links]\nreturn copy(self._links)", "path": "riakkit\\document.py", "repo_name": "shuhaowu/riakkit", "stars": 36, "license": "lgpl-3.0", "language": "python", "size": 881}
{"docstring": "\"\"\"Returns a generator that goes through each document that's searched.\"\"\"\n", "func_signal": "def run(self):\n", "code": "for doc in self.result[u\"docs\"]:\n  yield self.loadDoc(doc)", "path": "riakkit\\queries.py", "repo_name": "shuhaowu/riakkit", "stars": 36, "license": "lgpl-3.0", "language": "python", "size": 881}
{"docstring": "\"\"\"Construct a Document based object given a RiakObject.\n\nArgs:\n  riak_obj: The RiakObject that the document is suppose to build from.\n  cached: Reload the object or not if it's found in the pool of objects.\n  r: R value\n  bucket: The bucket to grab from. Defaults to the default bucket.\n\nReturns:\n  A Document object (whichever subclass this was called from).\n\"\"\"\n\n", "func_signal": "def load(cls, robj, cached=False, r=None, bucket=None):\n", "code": "if isinstance(robj, RiakObject):\n  key = robj.get_key()\nelse:\n  key = robj\n\ntry:\n  doc = cls.instances[key]\nexcept KeyError:\n  bucket = cls.buckets.get(bucket, cls.bucket)\n  robj = bucket.get(key, r)\n  if not robj.exists():\n    raise NotFoundError(\"%s not found!\" % key)\n\n  # This is done before so that deserialize won't recurse\n  # infinitely with collection_name. This wouldn't cause an problem as\n  # deserialize calls for the loading of the referenced document\n  # from cache, which load this document from cache, and it see that it\n  # exists, finish loading the referenced document, then come back and finish\n  # loading this document.\n\n  doc = cls(key, saved=True)\n  doc._obj = robj\n  cls.instances[key] = doc\n  doc.reload()\nelse:\n  if not cached:\n    doc.reload()\n\nreturn doc", "path": "riakkit\\document.py", "repo_name": "shuhaowu/riakkit", "stars": 36, "license": "lgpl-3.0", "language": "python", "size": 881}
{"docstring": "\"\"\"Gets the raw data that's contained in the RiakObject.\n\nIf default is not specified, AttributeError will be raised if the attribute\ndoesn't exist\n\nIf the object is not saved. NotFoundError will be raised\n\nArgs:\n  name: The name of the attribute.\n  default: The default to return if not available. Defaults to some garbage, which is DocumentMetaclass\n\nReturns:\n  The value or default.\n\nRaises:\n  AttributeError if default is not specified and attribute not found\n  NotFoundError if default not specified and object not found.\n\"\"\"\n\n", "func_signal": "def getRawData(self, name, default=DocumentMetaclass):\n", "code": "if self._obj:\n  data = self._obj.get_data()\n  if default == DocumentMetaclass:\n    if name not in data:\n      self._attrError(name)\n    else:\n      return data[name]\n  else:\n    return data.get(name, default)\nelse:\n  if default == DocumentMetaclass:\n    raise NotFoundError(\"%s is not loaded!\" % self.key)\n  else:\n    return default", "path": "riakkit\\document.py", "repo_name": "shuhaowu/riakkit", "stars": 36, "license": "lgpl-3.0", "language": "python", "size": 881}
{"docstring": "\"\"\"Invoke the command. Return exit code for program.\"\"\"\n", "func_signal": "def invoke(cls, args):\n", "code": "if len(args) < 1:\n    raise Exception(\"Missing virtual machine argument\")\nvm = VirtualMachine.find(args.pop(0))\nif len(args) < 1:\n    raise Exception(\"Missing hard disk filenames\")\nfor hd in args:\n    hd = cls.harddisk(hd)\n    verboseMsg(\"Attaching %s\" % hd)\n    vm.attachMedium(hd)\nreturn 0", "path": "utils\\pyvbox.py", "repo_name": "von/pyVBox", "stars": 38, "license": "None", "language": "python", "size": 245}
{"docstring": "\"\"\"Invoke the command. Return exit code for program.\"\"\"\n", "func_signal": "def invoke(cls, args):\n", "code": "if len(args) == 0:\n    raise Exception(\"Missing virtual machine name argument\");\nfor name in args:\n    vm = VirtualMachine.find(name)\n    verboseMsg(\"Ejecting %s\" % vm)\n    vm.eject()\nreturn 0", "path": "utils\\pyvbox.py", "repo_name": "von/pyVBox", "stars": 38, "license": "None", "language": "python", "size": 245}
{"docstring": "\"\"\"Invoke the command. Return exit code for program.\"\"\"\n", "func_signal": "def invoke(cls, args):\n", "code": "if len(args) < 1:\n    raise Exception(\"Missing size argument\")\nsize = cls.string_to_size(args.pop(0))\nif len(args) < 1:\n    raise Exception(\"Missing path argument\")\npath = args.pop(0)\nverboseMsg(\"Creating Hard Disk at %s with size %d\" % (path, size))\nHardDisk.createWithStorage(path, size)\nreturn 0", "path": "utils\\pyvbox.py", "repo_name": "von/pyVBox", "stars": 38, "license": "None", "language": "python", "size": 245}
{"docstring": "\"\"\"Invoke the command. Return exit code for program.\"\"\"\n", "func_signal": "def invoke(cls, args):\n", "code": "mode = \"gui\" # or \"vrdp\"\nif len(args) < 1:\n    raise Exception(\"Missing virtual machine filename argument\")\nvm = VirtualMachine.open(args.pop(0))\natexit.register(vm.eject)\nif not vm.isRegistered():\n    vm.register()\nfor hd in args:\n    hd = HardDisk.find(hd)\n    vm.attachMedium(hd)\nverboseMsg(\"Starting VM in %s mode\" % mode)\nvm.powerOn(type=mode)\n# Wait until machine is running or we have a race condition\n# where it still might be down when we call waitUntilDown()\nvm.waitUntilRunning()\nverboseMsg(\"VM started. Waiting until power down...\")\nvm.waitUntilDown()\nverboseMsg(\"VM powered down.\")\n# Let atexit clean up\nreturn 0", "path": "utils\\pyvbox.py", "repo_name": "von/pyVBox", "stars": 38, "license": "None", "language": "python", "size": 245}
{"docstring": "\"\"\"Is a hard disk at the given path already registered?\"\"\"\n", "func_signal": "def isRegistered(cls, path):\n", "code": "try:\n    with VirtualBoxException.ExceptionHandler():\n        cls.find(path)\nexcept VirtualBoxException.VirtualBoxObjectNotFoundException:\n    return False\nreturn True", "path": "pyVBox\\HardDisk.py", "repo_name": "von/pyVBox", "stars": 38, "license": "None", "language": "python", "size": 245}
{"docstring": "\"\"\"Invoke the command. Return exit code for program.\"\"\"\n", "func_signal": "def invoke(cls, args):\n", "code": "if len(args) < 1:\n    raise Exception(\"Missing virtual machine argument\")\nvm = VirtualMachine.find(args.pop(0))\nif len(args) < 1:\n    raise Exception(\"Missing target directory argument\")\ntargetDir = args.pop(0)\nverboseMsg(\"Backing up %s to %s\" % (vm, targetDir))\nif vm.isRunning():\n    verboseMsg(\"Pausing VM...\")\n    # Must wait until paused or will have race condition for lock\n    # on disks.\n    vm.pause(wait=True)\n    atexit.register(vm.resume)\n# Todo: Backup settings file in some way.\n# Todo: Want to back up devices than hard drives?\ndisks = vm.getHardDrives()\nfor disk in disks:\n    targetFilename = os.path.join(targetDir, disk.basename())\n    # Todo: Need to resolve file already existing here.\n    verboseMsg(\"Backing up disk %s to %s (%d bytes)\" % (disk,\n                                                        targetFilename,\n                                                        disk.size))\n    progress = disk.clone(targetFilename, wait=False)\n    show_progress(progress)\n    # Remove newly created clone from registry\n    clone = HardDisk.find(targetFilename)\n    clone.close()", "path": "utils\\pyvbox.py", "repo_name": "von/pyVBox", "stars": 38, "license": "None", "language": "python", "size": 245}
{"docstring": "\"\"\"Invoke the command. Return exit code for program.\"\"\"\n", "func_signal": "def invoke(cls, args):\n", "code": "if len(args) == 0:\n    raise Exception(\"Missing virtual machine name argument\");\nvm = VirtualMachine.find(args.pop(0))\nvm.resume()\nreturn 0", "path": "utils\\pyvbox.py", "repo_name": "von/pyVBox", "stars": 38, "license": "None", "language": "python", "size": 245}
{"docstring": "\"\"\"Return a StorageController around the given IStorageController instance\"\"\"\n", "func_signal": "def __init__(self, storageController):\n", "code": "assert(storageController is not None)\nself._wrappedInstance = storageController", "path": "pyVBox\\StorageController.py", "repo_name": "von/pyVBox", "stars": 38, "license": "None", "language": "python", "size": 245}
{"docstring": "\"\"\"Test StorageController attributes\"\"\"\n", "func_signal": "def testStorageController(self):\n", "code": "machine = VirtualMachine.open(self.testVMpath)\ncontrollers = machine.getStorageControllers()\n# Should be an IDE controller and a SATA controller\nself.assertTrue(len(controllers) == 2)\nfor controller in controllers:\n    self.assertNotEqual(None, controller.name)\n    self.assertNotEqual(None, controller.bus)\n    self.assertNotEqual(None, controller.controllerType)\n    self.assertNotEqual(None, controller.instance)\n    self.assertNotEqual(None, controller.maxDevicesPerPortCount)\n    self.assertNotEqual(None, controller.maxPortCount)\n    self.assertNotEqual(None, controller.minPortCount)\n    self.assertNotEqual(None, controller.portCount)\n    self.assertNotEqual(None, str(controller))", "path": "test\\StorageControllerTests.py", "repo_name": "von/pyVBox", "stars": 38, "license": "None", "language": "python", "size": 245}
{"docstring": "\"\"\"Invoke the command. Return exit code for program.\"\"\"\n", "func_signal": "def invoke(cls, args):\n", "code": "if len(args) < 1:\n    raise Exception(\"Missing source VM name argument\")\nsrcVM = VirtualMachine.find(args.pop(0))\nif len(args) < 1:\n    raise Exception(\"Missing target VM name argument\")\ntargetName = args.pop(0)\nmessage(\"Cloning %s to %s\" % (srcVM, targetName))\ncloneVM = srcVM.clone(targetName)\n# Now clone and attach disks\ndisks = srcVM.getHardDrives()\nfor disk in disks:\n    # Generate new HD filename by prefixing new VM name.\n    # Not the greatest, but not sure what the best way is.\n    targetFilename = os.path.join(disk.dirname(),\n                                  \"%s-%s\" % (targetName, disk.name))\n    # Todo: Need to resolve file already existing here.\n    message(\"Cloning disk %s to %s (%d bytes)\"\n            % (disk,\n               os.path.basename(targetFilename),\n               disk.size))\n    progress = disk.clone(targetFilename, wait=False)\n    show_progress(progress)\n    cloneHD = HardDisk.find(targetFilename)\n    message(\"Attaching %s to %s\" % (cloneHD, cloneVM))\n    cloneVM.attachMedium(cloneHD)\nreturn 0", "path": "utils\\pyvbox.py", "repo_name": "von/pyVBox", "stars": 38, "license": "None", "language": "python", "size": 245}
{"docstring": "\"\"\"Invoke the command. Return exit code for program.\"\"\"\n", "func_signal": "def invoke(cls, args):\n", "code": "if len(args) == 0:\n    raise Exception(\"Missing VM name\")\nvm = VirtualMachine.find(args.pop(0))\nsnapshot = vm.getCurrentSnapshot()\nprogress = vm.deleteSnapshot(snapshot, wait=False)\nshow_progress(progress)", "path": "utils\\pyvbox.py", "repo_name": "von/pyVBox", "stars": 38, "license": "None", "language": "python", "size": 245}
{"docstring": "\"\"\"Load a harddisk described by string.\n\nWill open disk if needed.\"\"\"\n# TODO: Should also support string being UUID\n", "func_signal": "def harddisk(cls, string):\n", "code": "if HardDisk.isRegistered(string):\n    hd = HardDisk.find(string)\nelse:\n    hd = HardDisk.open(string)\nreturn hd", "path": "utils\\pyvbox.py", "repo_name": "von/pyVBox", "stars": 38, "license": "None", "language": "python", "size": 245}
{"docstring": "\"\"\"Invoke the command. Return exit code for program.\"\"\"\n", "func_signal": "def invoke(cls, args):\n", "code": "if len(args) == 0:\n    raise Exception(\"Missing virtual machine filename argument\");\nfor filename in args:\n    vm = VirtualMachine.open(args.pop(0))\n    if vm.isRegistered():\n        errorMsg(\"VM \\\"%s\\\" is already registered.\" % vm)\n    else:\n        verboseMsg(\"Registering VM %s\" % vm)\n        vm.register()\nreturn 0", "path": "utils\\pyvbox.py", "repo_name": "von/pyVBox", "stars": 38, "license": "None", "language": "python", "size": 245}
{"docstring": "\"\"\"Invoke the command. Return exit code for program.\"\"\"\n", "func_signal": "def invoke(cls, args):\n", "code": "if len(args) == 0:\n    raise Exception(\"Missing VM name\")\nvm = VirtualMachine.find(args.pop(0))\nif len(args) == 0:\n    raise Exception(\"Missing snapshot name\")\nname = args.pop(0)\ndescription = None\nif len(args) > 0:\n    description = args.pop(0)\nvm.takeSnapshot(name, description)", "path": "utils\\pyvbox.py", "repo_name": "von/pyVBox", "stars": 38, "license": "None", "language": "python", "size": 245}
{"docstring": "\"\"\"Given a string representing a size return size as long.\n\nSize is in MB by default. Supported suffixes: MB, GB, TB, PB\"\"\"\n", "func_signal": "def string_to_size(cls, string):\n", "code": "import re\n\nmatch = re.match(\"(\\d+)(\\w\\w)?\\Z\", string)\nif not match:\n    raise Exception(\"Could not parse size \\\"%s\\\"\" % string)\nnumber = long(match.group(1))\nsuffix = match.group(2)\nif suffix:\n    suffix = suffix.lower()\n    suffixes = {\n        \"mb\" : 1,\n        \"gb\" : 1024,\n        \"tb\" : 1048576,\n        \"pb\" : 1073741824,\n        }\n    if not suffixes.has_key(suffix):\n        raise Exception(\"Unrecognized suffix \\\"%s\\\"\" % suffix)\n    number *= suffixes[suffix]\nreturn number", "path": "utils\\pyvbox.py", "repo_name": "von/pyVBox", "stars": 38, "license": "None", "language": "python", "size": 245}
{"docstring": "\"\"\"Waits until the task is done (including all sub-operations).\n\nTimeout is in milliseconds, specify None for an indefinite wait.\"\"\"\n", "func_signal": "def waitForCompletion(self, timeout = None):\n", "code": "if timeout is None:\n    timeout = self.WaitIndefinite\nwith VirtualBoxException.ExceptionHandler():\n    self._wrappedInstance.waitForCompletion(timeout)\nif (((not self.completed) and (timeout == self.WaitIndefinite)) or\n    (self.completed and (self.resultCode != 0))):\n    # TODO: This is not the right exception to return.\n    raise VirtualBoxException.VirtualBoxException(\n        \"Task %s did not complete: %s (%d)\" % \n        (self.description,\n         self.errorInfo.text,\n         self.resultCode))", "path": "pyVBox\\Progress.py", "repo_name": "von/pyVBox", "stars": 38, "license": "None", "language": "python", "size": 245}
{"docstring": "\"\"\"Invoke the command. Return exit code for program.\"\"\"\n", "func_signal": "def invoke(cls, args):\n", "code": "if len(args) == 0:\n    raise Exception(\"Missing virtual machine name argument\");\nvm = VirtualMachine.find(args.pop(0))\nvm.pause()\nreturn 0", "path": "utils\\pyvbox.py", "repo_name": "von/pyVBox", "stars": 38, "license": "None", "language": "python", "size": 245}
{"docstring": "\"\"\"Invoke the command. Return exit code for program.\"\"\"\n", "func_signal": "def invoke(cls, args):\n", "code": "mode = \"gui\"\nif len(args) == 0:\n    raise Exception(\"Missing virtual machine name argument\");\nvm = VirtualMachine.find(args.pop(0))\nvm.powerOn(type=mode)", "path": "utils\\pyvbox.py", "repo_name": "von/pyVBox", "stars": 38, "license": "None", "language": "python", "size": 245}
{"docstring": "\"\"\"Invoke the command. Return exit code for program.\"\"\"\n", "func_signal": "def invoke(cls, args):\n", "code": "if len(args) == 0:\n    raise Exception(\"Missing virtual machine name argument\");\nfor filename in args:\n    vm = VirtualMachine.find(args.pop(0))\n    if vm.isRegistered():\n        verboseMsg(\"Unregistering VM %s\" % vm)\n        vm.unregister()\n    else:\n        errorMsg(\"VM \\\"%s\\\" is not registered.\" % vm)\nreturn 0", "path": "utils\\pyvbox.py", "repo_name": "von/pyVBox", "stars": 38, "license": "None", "language": "python", "size": 245}
{"docstring": "\"\"\"Invoke the command. Return exit code for program.\"\"\"\n", "func_signal": "def invoke(cls, args):\n", "code": "if len(args) < 1:\n    raise Exception(\"Missing source path argument\")\nsrcHD = cls.harddisk(args.pop(0))\nif len(args) < 1:\n    raise Exception(\"Missing target path argument\")\ntargetPath = args.pop(0)\nverboseMsg(\"Cloning %s to %s\" % (srcHD, targetPath))\nprogress = srcHD.clone(targetPath, wait=False)\nshow_progress(progress)\nreturn 0", "path": "utils\\pyvbox.py", "repo_name": "von/pyVBox", "stars": 38, "license": "None", "language": "python", "size": 245}
{"docstring": "\"\"\"\nSearches for the given text in the given set of files.\n\"\"\"\n", "func_signal": "def search_files_for_text(filenames, text, restrict_types=[], ignore_types=[], nested_types=[]):\n", "code": "anns = __filenames_to_annotations(filenames)\nreturn search_anns_for_text(anns, text, restrict_types=restrict_types, ignore_types=ignore_types, nested_types=nested_types)", "path": "server\\src\\search.py", "repo_name": "nlplab/stav", "stars": 34, "license": "other", "language": "python", "size": 16162}
{"docstring": "\"\"\"\nSearches for annotated spans that match in string content but\ndisagree in type in given Annotations objects.\n\"\"\"\n\n# treat None and empty list uniformly\n", "func_signal": "def eq_text_neq_type_spans(ann_objs, restrict_types=[], ignore_types=[], nested_types=[]):\n", "code": "restrict_types = [] if restrict_types is None else restrict_types\nignore_types   = [] if ignore_types is None else ignore_types\nnested_types   = [] if nested_types is None else nested_types\n\n# TODO: nested_types constraints not applied\n\nmatches = SearchMatchSet(\"Text marked with different types\")\n\ntext_type_ann_map = _get_text_type_ann_map(ann_objs, restrict_types, ignore_types, nested_types)\n\nfor text in text_type_ann_map:\n    if len(text_type_ann_map[text]) < 2:\n        # all matching texts have same type, OK\n        continue\n\n    types = text_type_ann_map[text].keys()\n    # avoiding any() etc. to be compatible with python 2.4\n    if restrict_types != [] and len([t for t in types if t in restrict_types]) == 0:\n        # Does not involve any of the types restricted do\n        continue\n\n    # debugging\n    #print >> sys.stderr, \"Text marked with %d different types:\\t%s\\t: %s\" % (len(text_type_ann_map[text]), text, \", \".join([\"%s (%d occ.)\" % (type, len(text_type_ann_map[text][type])) for type in text_type_ann_map[text]]))\n    for type in text_type_ann_map[text]:\n        for ann_obj, ann in text_type_ann_map[text][type]:\n            # debugging\n            #print >> sys.stderr, \"\\t%s %s\" % (ann.source_id, ann)\n            matches.add_match(ann_obj, ann)\n\nreturn matches", "path": "server\\src\\search.py", "repo_name": "nlplab/stav", "stars": 34, "license": "other", "language": "python", "size": 16162}
{"docstring": "# similar to _fill_type_configuration, but for types for which\n# full annotation configuration was not found but some visual\n# configuration can be filled.\n\n# TODO: duplicates parts of _fill_type_configuration; combine?\n", "func_signal": "def _fill_visual_configuration(types, project_conf):\n", "code": "items = []\nfor _type in types:\n    item = {}\n    item['name'] = project_conf.preferred_display_form(_type)\n    item['type'] = _type\n    item['unused'] = True\n    item['labels'] = project_conf.get_labels_by_type(_type)\n\n    drawing_conf = project_conf.get_drawing_config_by_type(_type) \n    # not sure if this is a good default, but let's try\n    if drawing_conf is None:\n        drawing_conf = project_conf.get_drawing_config_by_type(VISUAL_SPAN_DEFAULT)\n    if drawing_conf is None:\n        drawing_conf = {}\n    # just plug in everything found, whether for a span or arc\n    for k in chain(SPAN_DRAWING_ATTRIBUTES, ARC_DRAWING_ATTRIBUTES):\n        if k in drawing_conf:\n            item[k] = drawing_conf[k]\n\n    # TODO: anything else?\n\n    items.append(item)\n\nreturn items", "path": "server\\src\\document.py", "repo_name": "nlplab/stav", "stars": 34, "license": "other", "language": "python", "size": 16162}
{"docstring": "# inverse of real_directory\n", "func_signal": "def relative_directory(directory):\n", "code": "assert isabs(directory), 'directory \"%s\" is not absolute' % directory\nassert directory.startswith(DATA_DIR), 'directory \"%s\" not under DATA_DIR'\nreturn directory[len(DATA_DIR):]", "path": "server\\src\\document.py", "repo_name": "nlplab/stav", "stars": 34, "license": "other", "language": "python", "size": 16162}
{"docstring": "\"\"\"\nSearches for inconsistent annotations in given Annotations\nobjects.  Returns a list of SearchMatchSet objects, one for each\nchecked criterion that generated matches for the search.\n\"\"\"\n\n", "func_signal": "def check_consistency(ann_objs, restrict_types=[], ignore_types=[], nested_types=[]):\n", "code": "match_sets = []\n\nprint >> sys.stderr, \"NOTE: TEMPORARILY SWITCHING OFF TYPE AGREEMENT CHECKING!\"", "path": "server\\src\\search.py", "repo_name": "nlplab/stav", "stars": 34, "license": "other", "language": "python", "size": 16162}
{"docstring": "#TODO: DOC!\n\n# pointing at directory instead of document?\n", "func_signal": "def _document_json_dict(document):\n", "code": "if isdir(document):\n    raise IsDirectoryError(document)\n\nj_dic = {}\n_enrich_json_with_base(j_dic)\n\n#TODO: We don't check if the files exist, let's be more error friendly\n# Read in the textual data to make it ready to push\n_enrich_json_with_text(j_dic, document + '.' + TEXT_FILE_SUFFIX)\n\nwith TextAnnotations(document) as ann_obj:\n    _enrich_json_with_data(j_dic, ann_obj)\n\nreturn j_dic", "path": "server\\src\\document.py", "repo_name": "nlplab/stav", "stars": 34, "license": "other", "language": "python", "size": 16162}
{"docstring": "\"\"\"\nSearches the given Annotations objects for Event annotations\nmatching the given specification. Returns a SearchMatchSet object.\n\"\"\"\n\n", "func_signal": "def search_anns_for_event(ann_objs, trigger_text, args, restrict_types=[], ignore_types=[]):\n", "code": "global REPORT_SEARCH_TIMINGS\nif REPORT_SEARCH_TIMINGS:\n    process_start = datetime.now()\n\n# treat None and empty list uniformly\nrestrict_types = [] if restrict_types is None else restrict_types\nignore_types   = [] if ignore_types is None else ignore_types\n\n# TODO: include args in description\ndescription = \"Event triggered by text containing '%s'\" % trigger_text\nif restrict_types != []:\n    description = description + ' (of type %s)' % (\",\".join(restrict_types))\nmatches = SearchMatchSet(description)\n\nfor ann_obj in ann_objs:\n    # collect per-document (ann_obj) for sorting\n    ann_matches = []\n\n    for e in ann_obj.get_events():\n        if e.type in ignore_types:\n            continue\n        if restrict_types != [] and e.type not in restrict_types:\n            continue\n\n        try:\n            t_ann = ann_obj.get_ann_by_id(e.trigger)\n        except:\n            # TODO: specific exception\n            Messager.error('Failed to retrieve trigger annotation %s, skipping event %s in search' % (e.trigger, e.id))\n\n        # TODO: make options for \"text included\" vs. \"text matches\"\n        if (trigger_text != None and trigger_text != \"\" and \n            trigger_text != DEFAULT_EMPTY_STRING and \n            trigger_text not in t_ann.text):\n            continue\n\n        # argument constraints\n        missing_match = False\n        for arg in args:\n            for s in ('role', 'type', 'text'):\n                assert s in arg, \"Error: missing mandatory field '%s' in event search\" % s\n            found_match = False\n            for role, aid in e.args:\n                if arg['role'] is not None and arg['role'] != '' and arg['role'] != role:\n                    # mismatch on role\n                    continue\n                arg_ent = ann_obj.get_ann_by_id(aid)\n                if (arg['type'] is not None and arg['type'] != '' and \n                    arg['type'] != arg_ent.type):\n                    # mismatch on type\n                    continue\n                if (arg['text'] is not None and arg['text'] != '' and\n                    arg['text'] not in arg_ent.get_text()):\n                    # mismatch on text\n                    continue\n                found_match = True\n                break\n            if not found_match:\n                missing_match = True\n                break\n        if missing_match:\n            continue\n\n        ann_matches.append((t_ann, e))\n\n    # sort by trigger start offset\n    ann_matches.sort(lambda a,b: cmp((a[0].start,-a[0].end),(b[0].start,-b[0].end)))\n\n    # add to overall collection\n    for t_obj, e in ann_matches:\n        matches.add_match(ann_obj, e)\n\n# sort by document name for output\nmatches.sort_matches()\n\nif REPORT_SEARCH_TIMINGS:\n    process_delta = datetime.now() - process_start\n    print >> stderr, \"search_anns_for_event: processed in\", str(process_delta.seconds)+\".\"+str(process_delta.microseconds/10000), \"seconds\"\n\nreturn matches", "path": "server\\src\\search.py", "repo_name": "nlplab/stav", "stars": 34, "license": "other", "language": "python", "size": 16162}
{"docstring": "# TODO: Make the names here and the ones in the Annotations object conform\n# This is the from offset\n", "func_signal": "def _enrich_json_with_base(j_dic):\n", "code": "j_dic['offset'] = 0\nj_dic['entities'] = []\nj_dic['events'] = []\nj_dic['relations'] = []\nj_dic['triggers'] = []\nj_dic['modifications'] = []\nj_dic['attributes'] = []\nj_dic['equivs'] = []\nj_dic['comments'] = []", "path": "server\\src\\document.py", "repo_name": "nlplab/stav", "stars": 34, "license": "other", "language": "python", "size": 16162}
{"docstring": "\"\"\"\nSearches the given Annotations objects for relation annotations\nmatching the given specification. Returns a SearchMatchSet object.\n\"\"\"\n\n", "func_signal": "def search_anns_for_relation(ann_objs, arg1, arg1type, arg2, arg2type, restrict_types=[], ignore_types=[]):\n", "code": "global REPORT_SEARCH_TIMINGS\nif REPORT_SEARCH_TIMINGS:\n    process_start = datetime.now()\n\n# treat None and empty list uniformly\nrestrict_types = [] if restrict_types is None else restrict_types\nignore_types   = [] if ignore_types is None else ignore_types\n\n# TODO: include args in description\ndescription = \"Relations\"\nif restrict_types != []:\n    description = description + ' (of type %s)' % (\",\".join(restrict_types))\nmatches = SearchMatchSet(description)\n\nfor ann_obj in ann_objs:\n    # collect per-document (ann_obj) for sorting\n    ann_matches = []\n    \n    # binary relations and equivs need to be treated separately due\n    # to different structure (not a great design there)\n    for r in ann_obj.get_relations():\n        if r.type in ignore_types:\n            continue\n        if restrict_types != [] and r.type not in restrict_types:\n            continue\n\n        # argument constraints\n        if arg1 is not None or arg1type is not None:\n            arg1ent = ann_obj.get_ann_by_id(r.arg1)\n            if arg1 is not None and arg1 not in arg1ent.get_text():\n                continue\n            if arg1type is not None and arg1type != arg1ent.type:\n                continue\n        if arg2 is not None or arg2type is not None:\n            arg2ent = ann_obj.get_ann_by_id(r.arg2)\n            if arg2 is not None and arg2 not in arg2ent.get_text():\n                continue\n            if arg2type is not None and arg2type != arg2.type:\n                continue\n            \n        ann_matches.append(r)\n\n    for r in ann_obj.get_equivs():\n        if r.type in ignore_types:\n            continue\n        if restrict_types != [] and r.type not in restrict_types:\n            continue\n\n        # argument constraints. This differs from that for non-equiv\n        # for relations as equivs are symmetric, so the arg1-arg2\n        # distinction can be ignored.\n\n        # TODO: this can match the same thing twice, which most\n        # likely isn't what a user expects: for example, having\n        # 'Protein' for both arg1type and arg2type can still match\n        # an equiv between 'Protein' and 'Gene'.\n        match_found = False\n        for arg, argtype in ((arg1, arg1type), (arg2, arg2type)):\n            match_found = False\n            for aeid in r.entities:\n                argent = ann_obj.get_ann_by_id(aeid)\n                if arg is not None and arg not in argent.get_text():\n                    continue\n                if argtype is not None and argtype != argent.type:\n                    continue\n                match_found = True\n                break\n            if not match_found:\n                break\n        if not match_found:\n            continue\n\n        ann_matches.append(r)\n\n    # TODO: sort, e.g. by offset of participant occurring first\n    #ann_matches.sort(lambda a,b: cmp(???))\n\n    # add to overall collection\n    for r in ann_matches:\n        matches.add_match(ann_obj, r)\n\n# sort by document name for output\nmatches.sort_matches()\n\nif REPORT_SEARCH_TIMINGS:\n    process_delta = datetime.now() - process_start\n    print >> stderr, \"search_anns_for_relation: processed in\", str(process_delta.seconds)+\".\"+str(process_delta.microseconds/10000), \"seconds\"\n\nreturn matches", "path": "server\\src\\search.py", "repo_name": "nlplab/stav", "stars": 34, "license": "other", "language": "python", "size": 16162}
{"docstring": "\"\"\"\nSearches for the given text in textbound annotations in the given\nset of files.\n\"\"\"\n", "func_signal": "def search_files_for_textbound(filenames, text, restrict_types=[], ignore_types=[], nested_types=[]):\n", "code": "anns = __filenames_to_annotations(filenames)\nreturn search_anns_for_textbound(anns, text, restrict_types=restrict_types, ignore_types=ignore_types, nested_types=nested_types)", "path": "server\\src\\search.py", "repo_name": "nlplab/stav", "stars": 34, "license": "other", "language": "python", "size": 16162}
{"docstring": "\"\"\"\nHelper function for search. Given annotations, returns a\ndict-of-dicts, outer key annotation text, inner type, values\nannotation objects.\n\"\"\"\n\n# treat None and empty list uniformly\n", "func_signal": "def _get_text_type_ann_map(ann_objs, restrict_types=[], ignore_types=[], nested_types=[]):\n", "code": "restrict_types = [] if restrict_types is None else restrict_types\nignore_types   = [] if ignore_types is None else ignore_types\nnested_types   = [] if nested_types is None else nested_types\n\ntext_type_ann_map = {}\nfor ann_obj in ann_objs:\n    for t in ann_obj.get_textbounds():\n        if t.type in ignore_types:\n            continue\n\n        if t.text not in text_type_ann_map:\n            text_type_ann_map[t.text] = {}\n        if t.type not in text_type_ann_map[t.text]:\n            text_type_ann_map[t.text][t.type] = []\n        text_type_ann_map[t.text][t.type].append((ann_obj,t))\n\nreturn text_type_ann_map", "path": "server\\src\\search.py", "repo_name": "nlplab/stav", "stars": 34, "license": "other", "language": "python", "size": 16162}
{"docstring": "\"\"\"\nSearches for the given text in the document texts of the given\nAnnotations objects.  Returns a SearchMatchSet object.\n\"\"\"\n\n", "func_signal": "def search_anns_for_text(ann_objs, text, restrict_types=[], ignore_types=[], nested_types=[]):\n", "code": "global REPORT_SEARCH_TIMINGS\nif REPORT_SEARCH_TIMINGS:\n    process_start = datetime.now()\n\n# treat None and empty list uniformly\nrestrict_types = [] if restrict_types is None else restrict_types\nignore_types   = [] if ignore_types is None else ignore_types\nnested_types   = [] if nested_types is None else nested_types\n\ndescription = \"Text matching '%s'\" % text\nif restrict_types != []:\n    description = description + ' (embedded in %s)' % (\",\".join(restrict_types))\nif ignore_types != []:\n    description = description + ' (not embedded in %s)' % \",\".join(ignore_types)    \nmatches = SearchMatchSet(description)\n\nfor ann_obj in ann_objs:\n    doctext = ann_obj.get_document_text()\n\n    for m in re.finditer(r'\\b('+text+r')\\b', doctext):\n        # only need to care about embedding annotations if there's\n        # some annotation-based restriction\n        #if restrict_types == [] and ignore_types == []:\n        # TODO: _extremely_ naive and slow way to find embedding\n        # annotations.  Use some reasonable data structure\n        # instead.\n        embedding = []\n        # if there are no type restrictions, we can skip this bit\n        if restrict_types != [] or ignore_types != []:\n            for t in ann_obj.get_textbounds():\n                if t.start <= m.start() and t.end >= m.end():\n                    embedding.append(t)\n\n        # Note interpretation of ignore_types here: if the text span\n        # is embedded in one or more of the ignore_types, the match is\n        # ignored.\n        if len([e for e in embedding if e.type in ignore_types]) != 0:\n            continue\n\n        if restrict_types != [] and len([e for e in embedding if e.type in restrict_types]) == 0:\n            continue\n\n        # TODO: need a clean, standard way of identifying a text span\n        # that does not involve an annotation; this is a bit of a hack\n        tm = TextMatch(m.start(), m.end(), m.group())\n        matches.add_match(ann_obj, tm)\n\nif REPORT_SEARCH_TIMINGS:\n    process_delta = datetime.now() - process_start\n    print >> stderr, \"search_anns_for_text: processed in\", str(process_delta.seconds)+\".\"+str(process_delta.microseconds/10000), \"seconds\"\n\nreturn matches", "path": "server\\src\\search.py", "repo_name": "nlplab/stav", "stars": 34, "license": "other", "language": "python", "size": 16162}
{"docstring": "\"\"\"\nSearches for the given text in the Textbound annotations in the\ngiven Annotations objects.  Returns a SearchMatchSet object.\n\"\"\"\n\n", "func_signal": "def search_anns_for_textbound(ann_objs, text, restrict_types=[], ignore_types=[], nested_types=[]):\n", "code": "global REPORT_SEARCH_TIMINGS\nif REPORT_SEARCH_TIMINGS:\n    process_start = datetime.now()\n\n# treat None and empty list uniformly\nrestrict_types = [] if restrict_types is None else restrict_types\nignore_types   = [] if ignore_types is None else ignore_types\nnested_types   = [] if nested_types is None else nested_types\n\ndescription = \"Textbounds containing text '%s'\" % text\nif restrict_types != []:\n    description = description + ' (of type %s)' % (\",\".join(restrict_types))\nif nested_types != []:\n    description = description + ' (nesting annotation of type %s)' % (\",\".join(nested_types))\nmatches = SearchMatchSet(description)\n\nfor ann_obj in ann_objs:\n    # collect per-document (ann_obj) for sorting\n    ann_matches = []\n    for t in ann_obj.get_textbounds():\n        if t.type in ignore_types:\n            continue\n        if restrict_types != [] and t.type not in restrict_types:\n            continue\n        # TODO: make options for \"text included\" vs. \"text matches\"\n        if (text != None and text != \"\" and \n            text != DEFAULT_EMPTY_STRING and text not in t.text):\n            continue\n        if nested_types != []:\n            # TODO: massively inefficient\n            nested = [x for x in ann_obj.get_textbounds() if x != t and x.start >= t.start and x.end <= t.end]\n            if len([x for x in nested if x.type in nested_types]) == 0:\n                continue\n\n        ann_matches.append(t)\n\n    # sort by start offset\n    ann_matches.sort(lambda a,b: cmp((a.start,-a.end),(b.start,-b.end)))\n\n    # add to overall collection\n    for t in ann_matches:\n        matches.add_match(ann_obj, t)    \n\n# sort by document name for output\nmatches.sort_matches()\n\nif REPORT_SEARCH_TIMINGS:\n    process_delta = datetime.now() - process_start\n    print >> stderr, \"search_anns_for_textbound: processed in\", str(process_delta.seconds)+\".\"+str(process_delta.microseconds/10000), \"seconds\"\n\nreturn matches", "path": "server\\src\\search.py", "repo_name": "nlplab/stav", "stars": 34, "license": "other", "language": "python", "size": 16162}
{"docstring": "\"\"\"\nSearches for inconsistent annotations in the given set of files.\n\"\"\"\n", "func_signal": "def check_files_consistency(filenames, restrict_types=[], ignore_types=[], nested_types=[]):\n", "code": "anns = __filenames_to_annotations(filenames)\nreturn check_consistency(anns, restrict_types=restrict_types, ignore_types=ignore_types, nested_types=nested_types)", "path": "server\\src\\search.py", "repo_name": "nlplab/stav", "stars": 34, "license": "other", "language": "python", "size": 16162}
{"docstring": "\"\"\"\nHelper function for search. Given annotations, returns a dict\nmapping offsets in text into the set of annotations spanning each\noffset.\n\"\"\"\n\n# treat None and empty list uniformly\n", "func_signal": "def _get_offset_ann_map(ann_objs, restrict_types=[], ignore_types=[]):\n", "code": "restrict_types = [] if restrict_types is None else restrict_types\nignore_types   = [] if ignore_types is None else ignore_types\n\noffset_ann_map = {}\nfor ann_obj in ann_objs:\n    for t in ann_obj.get_textbounds():\n        if t.type in ignore_types:\n            continue\n        if restrict_types != [] and t.type not in restrict_types:\n            continue\n\n        for o in range(t.start, t.end):\n            if o not in offset_ann_map:\n                offset_ann_map[o] = set()\n            offset_ann_map[o].add(t)\n\nreturn offset_ann_map", "path": "server\\src\\search.py", "repo_name": "nlplab/stav", "stars": 34, "license": "other", "language": "python", "size": 16162}
{"docstring": "# We collect trigger ids to be able to link the textbound later on\n", "func_signal": "def _enrich_json_with_data(j_dic, ann_obj):\n", "code": "trigger_ids = set()\nfor event_ann in ann_obj.get_events():\n    trigger_ids.add(event_ann.trigger)\n    j_dic['events'].append(\n            [unicode(event_ann.id), unicode(event_ann.trigger), event_ann.args]\n            )\n\nfor rel_ann in ann_obj.get_relations():\n    j_dic['relations'].append(\n        [unicode(rel_ann.id), unicode(rel_ann.type), rel_ann.arg1, rel_ann.arg2]\n        )\n\nfor tb_ann in ann_obj.get_textbounds():\n    j_tb = [unicode(tb_ann.id), tb_ann.type, tb_ann.start, tb_ann.end]\n\n    # If we spotted it in the previous pass as a trigger for an\n    # event or if the type is known to be an event type, we add it\n    # as a json trigger.\n    # TODO: proper handling of disconnected triggers. Currently\n    # these will be erroneously passed as 'entities'\n    if unicode(tb_ann.id) in trigger_ids:\n        j_dic['triggers'].append(j_tb)\n    else: \n        j_dic['entities'].append(j_tb)\n\nfor eq_ann in ann_obj.get_equivs():\n    j_dic['equivs'].append(\n            (['*', eq_ann.type]\n                + [e for e in eq_ann.entities])\n            )\n\nfor att_ann in ann_obj.get_attributes():\n    j_dic['attributes'].append(\n            [unicode(att_ann.id), att_ann.type, att_ann.target, att_ann.value]\n            )\n\nfor com_ann in ann_obj.get_oneline_comments():\n    j_dic['comments'].append(\n            [com_ann.target, com_ann.type, com_ann.tail.strip()]\n            )\n\nif ann_obj.failed_lines:\n    error_msg = 'Unable to parse the following line(s):\\n%s' % (\n            '\\n'.join(\n            [('%s: %s' % (\n                        # The line number is off by one\n                        unicode(line_num + 1),\n                        unicode(ann_obj[line_num])\n                        )).strip()\n             for line_num in ann_obj.failed_lines])\n            )\n    Messager.error(error_msg, duration=len(ann_obj.failed_lines) * 3)\n\nj_dic['mtime'] = ann_obj.ann_mtime\nj_dic['ctime'] = ann_obj.ann_ctime\n\n# (no verification in visualizer, assume everything is OK.)\n\n# Attach the source files for the annotations and text\nfrom os.path import splitext\nfrom annotation import TEXT_FILE_SUFFIX\nann_files = [splitext(p)[1][1:] for p in ann_obj._input_files]\nann_files.append(TEXT_FILE_SUFFIX)\nann_files = [p for p in set(ann_files)]\nann_files.sort()\nj_dic['source_files'] = ann_files", "path": "server\\src\\document.py", "repo_name": "nlplab/stav", "stars": 34, "license": "other", "language": "python", "size": 16162}
{"docstring": "\"\"\"\nGiven a directory, returns Annotations objects for contained files.\n\"\"\"\n# TODO: put this shared functionality in a more reasonable place\n", "func_signal": "def __directory_to_annotations(directory):\n", "code": "from document import real_directory,_listdir\nfrom os.path import join as path_join\n\nreal_dir = real_directory(directory)\n# Get the document names\nbase_names = [fn[0:-4] for fn in _listdir(real_dir) if fn.endswith('txt')]\n\nfilenames = [path_join(real_dir, bn) for bn in base_names]\n\nreturn __filenames_to_annotations(filenames)", "path": "server\\src\\search.py", "repo_name": "nlplab/stav", "stars": 34, "license": "other", "language": "python", "size": 16162}
{"docstring": "\"\"\"\nHelper, sentence-splits and returns a mapping from character\noffsets to sentence number.\n\"\"\"\n", "func_signal": "def _get_offset_sentence_map(s):\n", "code": "from ssplit import en_sentence_boundary_gen\n\nm = {} # TODO: why is this a dict and not an array?\nsprev, snum = 0, 1 # note: sentences indexed from 1\nfor sstart, send in en_sentence_boundary_gen(s):\n    # if there are extra newlines (i.e. more than one) in between\n    # the previous end and the current start, those need to be\n    # added to the sentence number\n    snum += max(0,len([nl for nl in s[sprev:sstart] if nl == \"\\n\"]) - 1)\n    for o in range(sprev, send):\n        m[o] = snum\n    sprev = send\n    snum += 1\nreturn m", "path": "server\\src\\search.py", "repo_name": "nlplab/stav", "stars": 34, "license": "other", "language": "python", "size": 16162}
{"docstring": "\"\"\"\nHelper, sentence-splits and tokenizes, returns array comparable to\nwhat you would get from re.split(r'(\\s+)', s).\n\"\"\"\n", "func_signal": "def _split_and_tokenize(s):\n", "code": "from ssplit import en_sentence_boundary_gen\nfrom tokenise import en_token_boundary_gen\n\ntokens = []\n\nsprev = 0\nfor sstart, send in en_sentence_boundary_gen(s):\n    if sprev != sstart:\n        # between-sentence space\n        tokens.append(s[sprev:sstart])\n    stext = s[sstart:send]\n    tprev = sstart\n    for tstart, tend in en_token_boundary_gen(stext):\n        if tprev != tstart:\n            # between-token space\n            tokens.append(s[sstart+tprev:sstart+tstart])\n        tokens.append(s[sstart+tstart:sstart+tend])\n        tprev = tend\n    sprev = send\n\nif sprev != len(s):\n    # document-final space\n    tokens.append(s[sprev:])\n\nassert \"\".join(tokens) == s, \"INTERNAL ERROR\\n'%s'\\n'%s'\" % (\"\".join(tokens),s)\n\nreturn tokens", "path": "server\\src\\search.py", "repo_name": "nlplab/stav", "stars": 34, "license": "other", "language": "python", "size": 16162}
{"docstring": "\"\"\"\nGiven matches to a search (a SearchMatchSet), formats the results\nfor the client, returning a dictionary with the results in the\nexpected format.\n\"\"\"\n# decided to give filename only, remove this bit if the decision\n# sticks\n#     from document import relative_directory\n", "func_signal": "def format_results(matches):\n", "code": "from os.path import basename\n\n# the search response format is built similarly to that of the\n# directory listing.\n\nresponse = {}\n\n# fill in header for search result browser\nresponse['header'] = [('Document', 'string'), \n                      ('Annotation', 'string')]\n\n# determine which additional fields can be shown; depends on the\n# type of the results\n\n# TODO: this is much uglier than necessary, revise\ninclude_type = True\ntry:\n    for ann_obj, ann in matches.get_matches():\n        ann.type\nexcept AttributeError:\n    include_type = False\n\ninclude_text = True\ntry:\n    for ann_obj, ann in matches.get_matches():\n        ann.text\nexcept AttributeError:\n    include_text = False\n\ninclude_trigger_text = True\ntry:\n    for ann_obj, ann in matches.get_matches():\n        ann.trigger\nexcept AttributeError:\n    include_trigger_text = False\n\nif include_type:\n    response['header'].append(('Type', 'string'))\n\nif include_text:\n    response['header'].append(('Text', 'string'))\n\nif include_trigger_text:\n    response['header'].append(('Trigger text', 'string'))\n\n# fill in content\nitems = []\nfor ann_obj, ann in matches.get_matches():\n    # First value (\"a\") signals that the item points to a specific\n    # annotation, not a collection (directory) or document.\n    # second entry is non-listed \"pointer\" to annotation\n    fn = basename(ann_obj.get_document())\n    items.append([\"a\", { 'focus' : [ann.reference_id()] }, fn, ann.reference_text()])\n    if include_type:\n        items[-1].append(ann.type)\n    if include_text:\n        items[-1].append(ann.text)\n    if include_trigger_text:\n        try:\n            items[-1].append(ann_obj.get_ann_by_id(ann.trigger).text)\n        except:\n            # TODO: specific exception\n            items[-1].append(\"(ERROR)\")\nresponse['items'] = items\nreturn response", "path": "server\\src\\search.py", "repo_name": "nlplab/stav", "stars": 34, "license": "other", "language": "python", "size": 16162}
{"docstring": "# called for each entity reference, e.g. for '&copy;', ref will be 'copy'\n", "func_signal": "def handle_entityref(self, ref):\n", "code": "if not self.elementstack: return\nif _debug: sys.stderr.write('entering handle_entityref with %s\\n' % ref)\nif ref in ('lt', 'gt', 'quot', 'amp', 'apos'):\n    text = '&%s;' % ref\nelse:\n    # entity resolution graciously donated by Aaron Swartz\n    def name2cp(k):\n        import htmlentitydefs\n        if hasattr(htmlentitydefs, 'name2codepoint'): # requires Python 2.3\n            return htmlentitydefs.name2codepoint[k]\n        k = htmlentitydefs.entitydefs[k]\n        if k.startswith('&#') and k.endswith(';'):\n            return int(k[2:-1]) # not in latin-1\n        return ord(k)\n    try: name2cp(ref)\n    except KeyError: text = '&%s;' % ref\n    else: text = unichr(name2cp(ref)).encode('utf-8')\nself.elementstack[-1][2].append(text)", "path": "canto\\feedparser_builtin.py", "repo_name": "themoken/Canto", "stars": 47, "license": "gpl-2.0", "language": "python", "size": 1022}
{"docstring": "'''Parse a string according to the MS SQL date format'''\n", "func_signal": "def _parse_date_mssql(dateString):\n", "code": "m = _mssql_date_re.match(dateString)\nif not m: return\nw3dtfdate = '%(year)s-%(month)s-%(day)sT%(hour)s:%(minute)s:%(second)s%(zonediff)s' % \\\n            {'year': m.group(1), 'month': m.group(2), 'day': m.group(3),\\\n             'hour': m.group(4), 'minute': m.group(5), 'second': m.group(6),\\\n             'zonediff': '+09:00'}\nif _debug: sys.stderr.write('MS SQL date parsed as: %s\\n' % w3dtfdate)\nreturn _parse_date_w3dtf(w3dtfdate)", "path": "canto\\feedparser_builtin.py", "repo_name": "themoken/Canto", "stars": 47, "license": "gpl-2.0", "language": "python", "size": 1022}
{"docstring": "'''Parse a string according to the OnBlog 8-bit date format'''\n", "func_signal": "def _parse_date_onblog(dateString):\n", "code": "m = _korean_onblog_date_re.match(dateString)\nif not m: return\nw3dtfdate = '%(year)s-%(month)s-%(day)sT%(hour)s:%(minute)s:%(second)s%(zonediff)s' % \\\n            {'year': m.group(1), 'month': m.group(2), 'day': m.group(3),\\\n             'hour': m.group(4), 'minute': m.group(5), 'second': m.group(6),\\\n             'zonediff': '+09:00'}\nif _debug: sys.stderr.write('OnBlog date parsed as: %s\\n' % w3dtfdate)\nreturn _parse_date_w3dtf(w3dtfdate)", "path": "canto\\feedparser_builtin.py", "repo_name": "themoken/Canto", "stars": 47, "license": "gpl-2.0", "language": "python", "size": 1022}
{"docstring": "'''Parse a string according to a Hungarian 8-bit date format.'''\n", "func_signal": "def _parse_date_hungarian(dateString):\n", "code": "m = _hungarian_date_format_re.match(dateString)\nif not m: return\ntry:\n    month = _hungarian_months[m.group(2)]\n    day = m.group(3)\n    if len(day) == 1:\n        day = '0' + day\n    hour = m.group(4)\n    if len(hour) == 1:\n        hour = '0' + hour\nexcept:\n    return\nw3dtfdate = '%(year)s-%(month)s-%(day)sT%(hour)s:%(minute)s%(zonediff)s' % \\\n            {'year': m.group(1), 'month': month, 'day': day,\\\n             'hour': hour, 'minute': m.group(5),\\\n             'zonediff': m.group(6)}\nif _debug: sys.stderr.write('Hungarian date parsed as: %s\\n' % w3dtfdate)\nreturn _parse_date_w3dtf(w3dtfdate)", "path": "canto\\feedparser_builtin.py", "repo_name": "themoken/Canto", "stars": 47, "license": "gpl-2.0", "language": "python", "size": 1022}
{"docstring": "# Don't unselect before the function so that the function can still\n# ascertain what item is selected.\n", "func_signal": "def dec(self, *args):\n", "code": "oldsel = self.sel\n\nr = fn(self, *args)\n\nif oldsel:\n    oldsel[\"item\"].unselect()\n    if self.cfg.unselect_hook:\n        self.cfg.unselect_hook(oldsel[\"tag\"], oldsel[\"item\"])\n\nif self.sel:\n    self.sel[\"item\"].select()\n    if self.cfg.select_hook:\n        self.cfg.select_hook(self.sel[\"tag\"], self.sel[\"item\"])\n\nif \"change_tag\" in self.cfg.triggers and\\\n    oldsel and self.sel and \\\n    oldsel[\"tag\"] != self.sel[\"tag\"]:\n        self.change_tag_override = 1\nreturn r", "path": "canto\\gui.py", "repo_name": "themoken/Canto", "stars": 47, "license": "gpl-2.0", "language": "python", "size": 1022}
{"docstring": "# uninstaller has to try one method or the other, not both.\n# if neither is set, then uninstaller gets to choose.\n", "func_signal": "def finalize_options(self):\n", "code": "if self.force_log and self.force_guess:\n\traise distutils.errors.DistutilsOptionError(\n\t_('choose either the `force-log\\' or `force-guess\\' option.')\n\t)\n\n# do some validation\nif (self.install_log is not None) and not(os.path.exists(self.install_log)):\n\traise distutils.errors.DistutilsOptionError(\n\t_('the `install-log\\' option must point to an existing file.')\n\t)", "path": "uninstall.py", "repo_name": "themoken/Canto", "stars": 47, "license": "gpl-2.0", "language": "python", "size": 1022}
{"docstring": "# utility method to be called by descendants\n", "func_signal": "def normalize_attrs(self, attrs):\n", "code": "attrs = [(k.lower(), v) for k, v in attrs]\nattrs = [(k, k in ('rel', 'type') and v.lower() or v) for k, v in attrs]\nreturn attrs", "path": "canto\\feedparser_builtin.py", "repo_name": "themoken/Canto", "stars": 47, "license": "gpl-2.0", "language": "python", "size": 1022}
{"docstring": "# Check if\n# - server requires digest auth, AND\n# - we tried (unsuccessfully) with basic auth, AND\n# - we're using Python 2.3.3 or later (digest auth is irreparably broken in earlier versions)\n# If all conditions hold, parse authentication information\n# out of the Authorization header we sent the first time\n# (for the username and password) and the WWW-Authenticate\n# header the server sent back (for the realm) and retry\n# the request with the appropriate digest auth headers instead.\n# This evil genius hack has been brought to you by Aaron Swartz.\n", "func_signal": "def http_error_401(self, req, fp, code, msg, headers):\n", "code": "host = urlparse.urlparse(req.get_full_url())[1]\ntry:\n    assert sys.version.split()[0] >= '2.3.3'\n    assert base64 != None\n    user, passw = base64.decodestring(req.headers['Authorization'].split(' ')[1]).split(':')\n    realm = re.findall('realm=\"([^\"]*)\"', headers['WWW-Authenticate'])[0]\n    self.add_password(realm, host, user, passw)\n    retry = self.http_error_auth_reqed('www-authenticate', host, req, headers)\n    self.reset_retry_count()\n    return retry\nexcept:\n    return self.http_error_default(req, fp, code, msg, headers)", "path": "canto\\feedparser_builtin.py", "repo_name": "themoken/Canto", "stars": 47, "license": "gpl-2.0", "language": "python", "size": 1022}
{"docstring": "# called for each start tag\n# attrs is a list of (attr, value) tuples\n# e.g. for <pre class='screen'>, tag='pre', attrs=[('class', 'screen')]\n", "func_signal": "def unknown_starttag(self, tag, attrs):\n", "code": "if _debug: sys.stderr.write('_BaseHTMLProcessor, unknown_starttag, tag=%s\\n' % tag)\nuattrs = []\n# thanks to Kevin Marks for this breathtaking hack to deal with (valid) high-bit attribute values in UTF-8 feeds\nfor key, value in attrs:\n    if type(value) != type(u''):\n        value = unicode(value, self.encoding, errors='replace')\n    uattrs.append((unicode(key, self.encoding), value))\nstrattrs = u''.join([u' %s=\"%s\"' % (key, value) for key, value in uattrs]).encode(self.encoding)\nif tag in self.elements_no_end_tag:\n    self.pieces.append('<%(tag)s%(strattrs)s />' % locals())\nelse:\n    self.pieces.append('<%(tag)s%(strattrs)s>' % locals())", "path": "canto\\feedparser_builtin.py", "repo_name": "themoken/Canto", "stars": 47, "license": "gpl-2.0", "language": "python", "size": 1022}
{"docstring": "'''Return the Time Zone Designator as an offset in seconds from UTC.'''\n", "func_signal": "def __extract_tzd(m):\n", "code": "if not m:\n    return 0\ntzd = m.group('tzd')\nif not tzd:\n    return 0\nif tzd == 'Z':\n    return 0\nhours = int(m.group('tzdhours'))\nminutes = m.group('tzdminutes')\nif minutes:\n    minutes = int(minutes)\nelse:\n    minutes = 0\noffset = (hours*60 + minutes) * 60\nif tzd[0] == '+':\n    return -offset\nreturn offset", "path": "canto\\feedparser_builtin.py", "repo_name": "themoken/Canto", "stars": 47, "license": "gpl-2.0", "language": "python", "size": 1022}
{"docstring": "'''Parse an RFC822, RFC1123, RFC2822, or asctime-style date'''\n", "func_signal": "def _parse_date_rfc822(dateString):\n", "code": "data = dateString.split()\nif data[0][-1] in (',', '.') or data[0].lower() in rfc822._daynames:\n    del data[0]\nif len(data) == 4:\n    s = data[3]\n    i = s.find('+')\n    if i > 0:\n        data[3:] = [s[:i], s[i+1:]]\n    else:\n        data.append('')\n    dateString = \" \".join(data)\nif len(data) < 5:\n    dateString += ' 00:00:00 GMT'\ntm = rfc822.parsedate_tz(dateString)\nif tm:\n    return time.gmtime(rfc822.mktime_tz(tm))", "path": "canto\\feedparser_builtin.py", "repo_name": "themoken/Canto", "stars": 47, "license": "gpl-2.0", "language": "python", "size": 1022}
{"docstring": "\"\"\"URL, filename, or string --> stream\n\nThis function lets you define parsers that take any input source\n(URL, pathname to local or network file, or actual data as a string)\nand deal with it in a uniform manner.  Returned object is guaranteed\nto have all the basic stdio read methods (read, readline, readlines).\nJust .close() the object when you're done with it.\n\nIf the etag argument is supplied, it will be used as the value of an\nIf-None-Match request header.\n\nIf the modified argument is supplied, it must be a tuple of 9 integers\nas returned by gmtime() in the standard Python time module. This MUST\nbe in GMT (Greenwich Mean Time). The formatted date/time will be used\nas the value of an If-Modified-Since request header.\n\nIf the agent argument is supplied, it will be used as the value of a\nUser-Agent request header.\n\nIf the referrer argument is supplied, it will be used as the value of a\nReferer[sic] request header.\n\nIf handlers is supplied, it is a list of handlers used to build a\nurllib2 opener.\n\"\"\"\n\n", "func_signal": "def _open_resource(url_file_stream_or_string, etag, modified, agent, referrer, handlers):\n", "code": "if hasattr(url_file_stream_or_string, 'read'):\n    return url_file_stream_or_string\n\nif url_file_stream_or_string == '-':\n    return sys.stdin\n\nif urlparse.urlparse(url_file_stream_or_string)[0] in ('http', 'https', 'ftp'):\n    if not agent:\n        agent = USER_AGENT\n    # test for inline user:password for basic auth\n    auth = None\n    if base64:\n        urltype, rest = urllib.splittype(url_file_stream_or_string)\n        realhost, rest = urllib.splithost(rest)\n        if realhost:\n            user_passwd, realhost = urllib.splituser(realhost)\n            if user_passwd:\n                url_file_stream_or_string = '%s://%s%s' % (urltype, realhost, rest)\n                auth = base64.encodestring(user_passwd).strip()\n    # try to open with urllib2 (to use optional headers)\n    request = urllib2.Request(url_file_stream_or_string)\n    request.add_header('User-Agent', agent)\n    if etag:\n        request.add_header('If-None-Match', etag)\n    if modified:\n        # format into an RFC 1123-compliant timestamp. We can't use\n        # time.strftime() since the %a and %b directives can be affected\n        # by the current locale, but RFC 2616 states that dates must be\n        # in English.\n        short_weekdays = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n        months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n        request.add_header('If-Modified-Since', '%s, %02d %s %04d %02d:%02d:%02d GMT' % (short_weekdays[modified[6]], modified[2], months[modified[1] - 1], modified[0], modified[3], modified[4], modified[5]))\n    if referrer:\n        request.add_header('Referer', referrer)\n    if gzip and zlib:\n        request.add_header('Accept-encoding', 'gzip, deflate')\n    elif gzip:\n        request.add_header('Accept-encoding', 'gzip')\n    elif zlib:\n        request.add_header('Accept-encoding', 'deflate')\n    else:\n        request.add_header('Accept-encoding', '')\n    if auth:\n        request.add_header('Authorization', 'Basic %s' % auth)\n    if ACCEPT_HEADER:\n        request.add_header('Accept', ACCEPT_HEADER)\n    request.add_header('A-IM', 'feed') # RFC 3229 support\n    opener = apply(urllib2.build_opener, tuple(handlers + [_FeedURLHandler()]))\n    opener.addheaders = [] # RMK - must clear so we only send our custom User-Agent\n    try:\n        return opener.open(request)\n    finally:\n        opener.close() # JohnD\n\n# try to open with native open function (if url_file_stream_or_string is a filename)\ntry:\n    return open(url_file_stream_or_string)\nexcept:\n    pass\n\n# treat url_file_stream_or_string as string\nreturn _StringIO(str(url_file_stream_or_string))", "path": "canto\\feedparser_builtin.py", "repo_name": "themoken/Canto", "stars": 47, "license": "gpl-2.0", "language": "python", "size": 1022}
{"docstring": "'''Parse a string according to the Nate 8-bit date format'''\n", "func_signal": "def _parse_date_nate(dateString):\n", "code": "m = _korean_nate_date_re.match(dateString)\nif not m: return\nhour = int(m.group(5))\nampm = m.group(4)\nif (ampm == _korean_pm):\n    hour += 12\nhour = str(hour)\nif len(hour) == 1:\n    hour = '0' + hour\nw3dtfdate = '%(year)s-%(month)s-%(day)sT%(hour)s:%(minute)s:%(second)s%(zonediff)s' % \\\n            {'year': m.group(1), 'month': m.group(2), 'day': m.group(3),\\\n             'hour': hour, 'minute': m.group(6), 'second': m.group(7),\\\n             'zonediff': '+09:00'}\nif _debug: sys.stderr.write('Nate date parsed as: %s\\n' % w3dtfdate)\nreturn _parse_date_w3dtf(w3dtfdate)", "path": "canto\\feedparser_builtin.py", "repo_name": "themoken/Canto", "stars": 47, "license": "gpl-2.0", "language": "python", "size": 1022}
{"docstring": "# called for each block of plain text, i.e. outside of any tag and\n# not containing any character or entity references\n# Store the original text verbatim.\n", "func_signal": "def handle_data(self, text):\n", "code": "if _debug: sys.stderr.write('_BaseHTMLProcessor, handle_text, text=%s\\n' % text)\nself.pieces.append(text)", "path": "canto\\feedparser_builtin.py", "repo_name": "themoken/Canto", "stars": 47, "license": "gpl-2.0", "language": "python", "size": 1022}
{"docstring": "# called for each block of plain text, i.e. outside of any tag and\n# not containing any character or entity references\n", "func_signal": "def handle_data(self, text, escape=1):\n", "code": "if not self.elementstack: return\nif escape and self.contentparams.get('type') == 'application/xhtml+xml':\n    text = _xmlescape(text)\nself.elementstack[-1][2].append(text)", "path": "canto\\feedparser_builtin.py", "repo_name": "themoken/Canto", "stars": 47, "license": "gpl-2.0", "language": "python", "size": 1022}
{"docstring": "'''Parse a string according to a Greek 8-bit date format.'''\n", "func_signal": "def _parse_date_greek(dateString):\n", "code": "m = _greek_date_format_re.match(dateString)\nif not m: return\ntry:\n    wday = _greek_wdays[m.group(1)]\n    month = _greek_months[m.group(3)]\nexcept:\n    return\nrfc822date = '%(wday)s, %(day)s %(month)s %(year)s %(hour)s:%(minute)s:%(second)s %(zonediff)s' % \\\n             {'wday': wday, 'day': m.group(2), 'month': month, 'year': m.group(4),\\\n              'hour': m.group(5), 'minute': m.group(6), 'second': m.group(7),\\\n              'zonediff': m.group(8)}\nif _debug: sys.stderr.write('Greek date parsed as: %s\\n' % rfc822date)\nreturn _parse_date_rfc822(rfc822date)", "path": "canto\\feedparser_builtin.py", "repo_name": "themoken/Canto", "stars": 47, "license": "gpl-2.0", "language": "python", "size": 1022}
{"docstring": "# Get new self.cfg.{height, width}\n", "func_signal": "def refresh(self, restart = False):\n", "code": "if not restart:\n    try:\n        curses.endwin()\n    except:\n        pass\n\n    self.cfg.stdscr.touchwin()\n    self.cfg.stdscr.refresh()\n    self.cfg.stdscr.keypad(1)\n\nself.cfg.height, self.cfg.width = self.cfg.stdscr.getmaxyx()\n\n# If there's a resize hook, execute it.\nif self.cfg.resize_hook:\n    self.cfg.resize_hook(self.cfg)\n\n# Make sure we've got a minimum for columns and reader_lines\nself.cfg.columns = max(self.cfg.columns, 1)\nself.cfg.reader_lines = max(self.cfg.reader_lines, 3)\n\n# Adjust gui_height to compensate for the message at the bottom.\nself.cfg.gui_height = self.cfg.height - self.cfg.msg_height\nself.cfg.gui_width = self.cfg.width\n\n# Now we interpret the reader_orientation setting from the config to\n# shape the reader area, and adjust other height / width settings\n# accordingly.\n\n# XXX This logic could be cleaned up...\n\nif self.cfg.reader_orientation == \"top\":\n    self.cfg.gui_height -= self.cfg.reader_lines\n    self.cfg.gui_top = self.cfg.reader_lines\nelif self.cfg.reader_orientation == \"bottom\":\n    self.cfg.gui_height -= self.cfg.reader_lines\nelif self.cfg.reader_orientation == \"left\":\n    self.cfg.gui_width -= self.cfg.reader_lines\n    self.cfg.gui_right = self.cfg.reader_lines\nelif self.cfg.reader_orientation == \"right\":\n    self.cfg.gui_width -= self.cfg.reader_lines\n\n# Create the message window. This could arguably be crammed into the cfg\n# class itself, however, for logging and messaging, the cfg class is\n# basically just a glorified way to do away with globals =P\n\nself.cfg.msg = curses.newwin(self.cfg.msg_height,\\\n        self.cfg.width, self.cfg.height - self.cfg.msg_height, 0)\nself.cfg.msg.bkgdset(curses.color_pair(1))\nself.cfg.msg.erase()\nself.cfg.msg.refresh()\n\n# Perform the main update update.\nself.gui.refresh()\nself.gui.draw_elements()", "path": "canto\\main.py", "repo_name": "themoken/Canto", "stars": 47, "license": "gpl-2.0", "language": "python", "size": 1022}
{"docstring": "# We have this try except because under no circumstances\n# should the HTML parser crash the application. Better\n# handling is done per case in the handler itself so that\n# bad HTML doesn't necessarily lead to garbage output.\n\n", "func_signal": "def convert(self, s):\n", "code": "try:\n    self.feed(s)\nexcept:\n    pass\nr = self.result\nl = self.links\nself.reset()\nreturn (r,l)", "path": "canto\\canto_html.py", "repo_name": "themoken/Canto", "stars": 47, "license": "gpl-2.0", "language": "python", "size": 1022}
{"docstring": "# called for each character reference, e.g. for '&#160;', ref will be '160'\n", "func_signal": "def handle_charref(self, ref):\n", "code": "if not self.elementstack: return\nref = ref.lower()\nif ref in ('34', '38', '39', '60', '62', 'x22', 'x26', 'x27', 'x3c', 'x3e'):\n    text = '&#%s;' % ref\nelse:\n    if ref[0] == 'x':\n        c = int(ref[1:], 16)\n    else:\n        c = int(ref)\n    text = unichr(c).encode('utf-8')\nself.elementstack[-1][2].append(text)", "path": "canto\\feedparser_builtin.py", "repo_name": "themoken/Canto", "stars": 47, "license": "gpl-2.0", "language": "python", "size": 1022}
{"docstring": "# Hooray for Python introspection.\n", "func_signal": "def new_func(self, *args):\n", "code": "base = getattr(self, func.func_name + \"_base\", None)\npre = getattr(self, \"pre_\" + func.func_name, [])\npost = getattr(self, \"post_\" + func.func_name, [])\nr = None\n\n# Base function to set dict[\"content\"]\nif base:\n    base(*args)\n\n# Pre hooks\nfor f in pre:\n    f(*args)\n\n# The actual expected call\nr = func(self, *args)\n\n# Post hooks\nfor f in post:\n    f(*args)\n\nreturn r", "path": "canto\\interface_draw.py", "repo_name": "themoken/Canto", "stars": 47, "license": "gpl-2.0", "language": "python", "size": 1022}
{"docstring": "\"\"\"Set the blend method of the given group of layers\n\nIf more than one layer is supplied, they will be encapsulated.\n\nKeyword arguments:\nlayers -- list of layers\nblend_method -- blend method to give the layers\nis_end -- set to True if layers are at the end of a canvas\n\nReturns: list of layers\n\"\"\"\n", "func_signal": "def op_set_blend(self, layers, blend_method, is_end=False):\n", "code": "if layers == []:\n    return layers\nif blend_method == \"composite\":\n    return layers\n\nlayer = layers[0]\nif len(layers) > 1 or self.get_param(layers[0], \"amount\") != 1.0:\n    layer = self.op_encapsulate(layers)[0]\n\nlayer = deepcopy(layer)\n\nself.set_param(layer, \"blend_method\", sif.blend_methods[blend_method])\n\nreturn [layer]", "path": "synfig_output.py", "repo_name": "nikitakit/svg2sif", "stars": 32, "license": "None", "language": "python", "size": 150}
{"docstring": "\"\"\"Calculate angle (in radians) of a tangent given two points\"\"\"\n", "func_signal": "def _calc_angle(self, p1x, p1y, p2x, p2y):\n", "code": "dx = p2x-p1x\ndy = p2y-p1y\nif dx > 0 and dy > 0:\n    ag = math.pi + math.atan(dy/dx)\nelif dx > 0 and dy < 0:\n    ag = math.pi + math.atan(dy/dx)\nelif dx < 0 and dy < 0:\n    ag = math.atan(dy/dx)\nelif dx < 0 and dy > 0:\n    ag = 2*math.pi + math.atan(dy/dx)\nelif dx == 0 and dy > 0:\n    ag = -1*math.pi/2\nelif dx == 0 and dy < 0:\n    ag = math.pi/2\nelif dx == 0 and dy == 0:\n    ag = 0\nelif dx < 0 and dy == 0:\n    ag = 0\nelif dx > 0 and dy == 0:\n    ag = math.pi\n\nreturn (ag*180)/math.pi", "path": "synfig_output.py", "repo_name": "nikitakit/svg2sif", "stars": 32, "license": "None", "language": "python", "size": 150}
{"docstring": "\"\"\"Build an empty layer\"\"\"\n", "func_signal": "def build_layer(self, layer_type, desc, canvas=None, active=True, version=\"auto\"):\n", "code": "if canvas is None:\n    layer = self.root_canvas.makeelement(\"layer\")\nelse:\n    layer = etree.SubElement(canvas, \"layer\")\n\nlayer.set(\"type\", layer_type)\nlayer.set(\"desc\", desc)\nif active:\n    layer.set(\"active\", \"true\")\nelse:\n    layer.set(\"active\", \"false\")\n\nif version == \"auto\":\n    version = sif.defaultLayerVersion(layer_type)\n\nif type(version) == float:\n    version = str(version)\n\nlayer.set(\"version\", version)\n\nreturn layer", "path": "synfig_output.py", "repo_name": "nikitakit/svg2sif", "stars": 32, "license": "None", "language": "python", "size": 150}
{"docstring": "\"\"\"Convert SVG coordinate [x, y] to Synfig units\"\"\"\n", "func_signal": "def coor_svg2sif(self, vector):\n", "code": "x = vector[0]\ny = self.height - vector[1]\n\nx -= self.width/2.0\ny -= self.height/2.0\nx /= sif.kux\ny /= sif.kux\n\nreturn [x, y]", "path": "synfig_output.py", "repo_name": "nikitakit/svg2sif", "stars": 32, "license": "None", "language": "python", "size": 150}
{"docstring": "\"\"\"Convert Synfig coordinate [x, y] to SVG units\"\"\"\n", "func_signal": "def coor_sif2svg(self, vector):\n", "code": "x = vector[0] * sif.kux + self.width/2.0\ny = vector[1] * sif.kux + self.height/2.0\n\ny = self.height - y\n\nassert self.coor_svg2sif([x, y]) == vector, \"sif to svg coordinate conversion error\"\n\nreturn [x, y]", "path": "synfig_output.py", "repo_name": "nikitakit/svg2sif", "stars": 32, "license": "None", "language": "python", "size": 150}
{"docstring": "\"\"\"Scan a list for coordinate pairs and convert them to Synfig units\"\"\"\n# If list has two numerical elements,\n# treat it as a coordinate pair\n", "func_signal": "def list_coor_svg2sif(self, l):\n", "code": "if type(l) == list and len(l) == 2:\n    if type(l[0]) == int or type(l[0]) == float:\n        if type(l[1]) == int or type(l[1]) == float:\n            l_sif = self.coor_svg2sif(l)\n            l[0] = l_sif[0]\n            l[1] = l_sif[1]\n            return\n\n# Otherwise recursively iterate over the list\nfor x in l:\n    if type(x) == list:\n        self.list_coor_svg2sif(x)", "path": "synfig_output.py", "repo_name": "nikitakit/svg2sif", "stars": 32, "license": "None", "language": "python", "size": 150}
{"docstring": "\"\"\"Scan a list for coordinate pairs and convert them to SVG units\"\"\"\n# If list has two numerical elements,\n# treat it as a coordinate pair\n", "func_signal": "def list_coor_sif2svg(self, l):\n", "code": "if type(l) == list and len(l) == 2:\n    if type(l[0]) == int or type(l[0]) == float:\n        if type(l[1]) == int or type(l[1]) == float:\n            l_sif = self.coor_sif2svg(l)\n            l[0] = l_sif[0]\n            l[1] = l_sif[1]\n            return\n\n# Otherwise recursively iterate over the list\nfor x in l:\n    if type(x) == list:\n        self.list_coor_sif2svg(x)", "path": "synfig_output.py", "repo_name": "nikitakit/svg2sif", "stars": 32, "license": "None", "language": "python", "size": 150}
{"docstring": "\"\"\"Create a new layer\n\nKeyword arguments:\nlayer_type -- layer type string used internally by Synfig\ndesc -- layer description\nparams -- a dictionary of parameter names and their values\nguids -- a dictionary of parameter types and their guids (optional)\nactive -- set to False to create a hidden layer\n\"\"\"\n", "func_signal": "def create_layer(self, layer_type, desc, params={}, guids={}, canvas=None, active=True, version=\"auto\"):\n", "code": "layer = self.build_layer(layer_type, desc, canvas, active, version)\ndefault_layer_params = sif.defaultLayerParams(layer_type)\n\nfor param_name in default_layer_params.keys():\n    param_type = default_layer_params[param_name][0]\n    if param_name in params.keys():\n        param_value = params[param_name]\n    else:\n        param_value = default_layer_params[param_name][1]\n\n    if param_name in guids.keys():\n        param_guid = guids[param_name]\n    else:\n        param_guid = None\n\n    if param_value is not None:\n        self.build_param(layer, param_name, param_value, param_type, guid=param_guid)\n\nreturn layer", "path": "synfig_output.py", "repo_name": "nikitakit/svg2sif", "stars": 32, "license": "None", "language": "python", "size": 150}
{"docstring": "\"\"\"Return a list Synfig layers that represent the gradient with the given id\"\"\"\n", "func_signal": "def convert_url(self, url_id, mtx, d):\n", "code": "gradient = d.get_gradient(url_id)\nif gradient is None:\n    # Patterns and other URLs not supported\n    return [None]\n\nif gradient[\"type\"] == \"linear\":\n    layer = d.create_layer(\"linear_gradient\", url_id,\n                         d.gradient_to_params(gradient),\n                         guids={\"gradient\" : gradient[\"stops_guid\"]}  )\n\nif gradient[\"type\"] == \"radial\":\n    layer = d.create_layer(\"radial_gradient\", url_id,\n                         d.gradient_to_params(gradient),\n                         guids={\"gradient\" : gradient[\"stops_guid\"]}  )\n\nreturn d.op_transform([layer], simpletransform.composeTransform(mtx, gradient[\"mtx\"]))", "path": "synfig_output.py", "repo_name": "nikitakit/svg2sif", "stars": 32, "license": "None", "language": "python", "size": 150}
{"docstring": "\"\"\"Convert an SVG node to a list of Synfig layers\"\"\"\n# Parse tags that don't draw any layers\n", "func_signal": "def convert_node(self, node, d):\n", "code": "if node.tag == addNS(\"namedview\", \"sodipodi\"):\n    return []\nelif node.tag == addNS(\"defs\", \"svg\"):\n    self.parse_defs(node, d)\n    return []\nelif node.tag == addNS(\"metadata\", \"svg\"):\n    return []\nelif node.tag not in [\n    addNS(\"g\", \"svg\"),\n    addNS(\"a\", \"svg\"),\n    addNS(\"switch\", \"svg\"),\n    addNS(\"path\", \"svg\")]:\n    # An unsupported element\n    return []\n\nlayers = []\nif node.tag == addNS(\"g\", \"svg\"):\n    for subnode in node:\n        layers += self.convert_node(subnode, d)\n    if node.get(addNS(\"groupmode\", \"inkscape\")) == \"layer\":\n        name = node.get(addNS(\"label\", \"inkscape\"), \"Inline Canvas\")\n        layers = d.op_encapsulate(layers, name=name)\n\nelif (node.tag == addNS(\"a\", \"svg\")\n      or node.tag == addNS(\"switch\", \"svg\")):\n    # Treat anchor and switch as a group\n    for subnode in node:\n        layers += self.convert_node(subnode, d)\nelif node.tag == addNS(\"path\", \"svg\"):\n    layers = self.convert_path(node, d)\n\nstyle = extract_style(node)\nif \"filter\" in style.keys() and style[\"filter\"].startswith(\"url\"):\n    filter_id = style[\"filter\"][5:].split(\")\")[0]\n    layers = d.op_filter(layers, filter_id)\n\nopacity = extract_opacity(style, \"opacity\")\nif opacity != 1.0:\n    layers = d.op_fade(layers, opacity)\n\nreturn layers", "path": "synfig_output.py", "repo_name": "nikitakit/svg2sif", "stars": 32, "license": "None", "language": "python", "size": 150}
{"docstring": "\"\"\"Set layer parameters\n\nKeyword arguments:\nlayer -- the layer to set the parameter for\nparams -- a dictionary of parameter names and their values\nguids -- a dictionary of parameter types and their guids (optional)\n\"\"\"\n", "func_signal": "def set_params(self, layer, params={}, guids={}, modify_linked=False):\n", "code": "for param_name in params.keys():\n    if param_name in guids.keys():\n        self.set_param(layer, param_name, params[param_name], guid=guids[param_name], modify_linked=modify_linked)\n    else:\n        self.set_param(layer, param_name, params[param_name], modify_linked=modify_linked)", "path": "synfig_output.py", "repo_name": "nikitakit/svg2sif", "stars": 32, "license": "None", "language": "python", "size": 150}
{"docstring": "\"\"\"Gaussian blur the given layers by the given x and y amounts\n\nKeyword arguments:\nlayers -- list of layers\nx -- x-amount of blur\ny -- x-amount of blur\nis_end -- set to True if layers are at the end of a canvas\n\nReturns: list of layers\n\"\"\"\n", "func_signal": "def op_blur(self, layers, x, y, name=\"Blur\", is_end=False):\n", "code": "blur = self.create_layer(\"blur\", name, params={\n        \"blend_method\" : sif.blend_methods[\"straight\"],\n        \"size\" : [x, y]\n        })\n\nif is_end:\n    return layers + [blur]\nelse:\n    return self.op_encapsulate(layers + [blur])", "path": "synfig_output.py", "repo_name": "nikitakit/svg2sif", "stars": 32, "license": "None", "language": "python", "size": 150}
{"docstring": "#return simplestyle.parseStyle(node.get(\"style\"))\n\n# Work around a simplestyle bug in older verions of Inkscape\n# that leaves spaces at the beginning and end of values\n", "func_signal": "def extract_style(node, style_attrib=\"style\"):\n", "code": "s = node.get(style_attrib)\nif s is None:\n    return {}\nelse:\n    return dict([[x.strip() for x in i.split(\":\")] for i in s.split(\";\") if len(i)])", "path": "synfig_output.py", "repo_name": "nikitakit/svg2sif", "stars": 32, "license": "None", "language": "python", "size": 150}
{"docstring": "\"\"\"Convert an SVG path node to a list of Synfig layers\"\"\"\n", "func_signal": "def convert_path(self, node, d):\n", "code": "layers = []\n\nnode_id = node.get(\"id\", str(id(node)))\nstyle = extract_style(node)\nmtx = simpletransform.parseTransform(node.get(\"transform\"))\n\nblines = path_to_bline_list(node.get(\"d\"), node.get(addNS(\"nodetypes\", \"sodipodi\")), mtx)\nfor bline in blines:\n    d.bline_coor_svg2sif(bline)\n    bline_guid = d.new_guid()\n\n    if style.setdefault(\"fill\", \"#000000\")  != \"none\":\n        if style[\"fill\"].startswith(\"url\"):\n            # Set the color to black, so we can later overlay\n            # the shape with a gradient or pattern\n            color = [0, 0, 0, 1]\n        else:\n            color = extract_color(style, \"fill\", \"fill-opacity\")\n\n        layer = d.create_layer(\"region\", node_id, {\n                \"bline\": bline,\n                \"color\": color,\n                \"winding_style\": 1 if style.setdefault(\"fill-rule\", \"nonzero\") == \"evenodd\" else 0,\n                }, guids={\n                \"bline\":bline_guid\n                }   )\n\n        if style[\"fill\"].startswith(\"url\"):\n            color_layer = self.convert_url(style[\"fill\"][5:].split(\")\")[0], mtx, d)[0]\n            layer = d.op_color([layer], overlay=color_layer)[0]\n            layer = d.op_fade([layer], extract_opacity(style, \"fill-opacity\"))[0]\n\n        layers.append(layer)\n\n    if style.setdefault(\"stroke\", \"none\")  != \"none\":\n        if style[\"stroke\"].startswith(\"url\"):\n            # Set the color to black, so we can later overlay\n            # the shape with a gradient or pattern\n            color = [0, 0, 0, 1]\n        else:\n            color = extract_color(style, \"stroke\", \"stroke-opacity\")\n\n        layer = d.create_layer(\"outline\", node_id, {\n                \"bline\": bline,\n                \"color\": color,\n                \"width\": extract_width(style, \"stroke-width\", mtx),\n                \"sharp_cusps\": True if style.setdefault(\"stroke-linejoin\", \"miter\") == \"miter\" else False,\n                \"round_tip[0]\": False if style.setdefault(\"stroke-linecap\", \"butt\") == \"butt\" else True,\n                \"round_tip[1]\": False if style.setdefault(\"stroke-linecap\", \"butt\") == \"butt\" else True\n                }, guids={\n                \"bline\":bline_guid\n                }   )\n\n        if style[\"stroke\"].startswith(\"url\"):\n            color_layer = self.convert_url(style[\"stroke\"][5:].split(\")\")[0], mtx, d)[0]\n            layer = d.op_color([layer], overlay=color_layer)[0]\n            layer = d.op_fade([layer], extract_opacity(style, \"stroke-opacity\"))[0]\n\n        layers.append(layer)\n\nreturn layers", "path": "synfig_output.py", "repo_name": "nikitakit/svg2sif", "stars": 32, "license": "None", "language": "python", "size": 150}
{"docstring": "\"\"\"Apply a matrix transformation to the given layers\n\nKeyword arguments:\nlayers -- list of layers\nmtx -- transformation matrix\nname -- name of the Transform layer that is added\nis_end -- set to True if layers are at the end of a canvas\n\nReturns: list of layers\n\"\"\"\n", "func_signal": "def op_transform(self, layers, mtx, name=\"Transform\", is_end=False):\n", "code": "if layers == []:\n    return layers\nif mtx is None or mtx == [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]]:\n    return layers\n\nsrc_tl = [100, 100]\nsrc_br = [200, 200]\n\ndest_tl = [100, 100]\ndest_tr = [200, 100]\ndest_br = [200, 200]\ndest_bl = [100, 200]\n\nsimpletransform.applyTransformToPoint(mtx, dest_tl)\nsimpletransform.applyTransformToPoint(mtx, dest_tr)\nsimpletransform.applyTransformToPoint(mtx, dest_br)\nsimpletransform.applyTransformToPoint(mtx, dest_bl)\n\nwarp = self.create_layer(\"warp\", name, params={\n    \"src_tl\": self.coor_svg2sif(src_tl),\n    \"src_br\": self.coor_svg2sif(src_br),\n    \"dest_tl\": self.coor_svg2sif(dest_tl),\n    \"dest_tr\": self.coor_svg2sif(dest_tr),\n    \"dest_br\": self.coor_svg2sif(dest_br),\n    \"dest_bl\": self.coor_svg2sif(dest_bl)\n    } )\n\nif is_end:\n    return layers + [warp]\nelse:\n    return self.op_encapsulate(layers + [warp])", "path": "synfig_output.py", "repo_name": "nikitakit/svg2sif", "stars": 32, "license": "None", "language": "python", "size": 150}
{"docstring": "\"\"\"Transform gradient to a list of parameters to pass to a Synfig layer\"\"\"\n# Create a copy of the gradient\n", "func_signal": "def gradient_to_params(self, gradient):\n", "code": "g = gradient.copy()\n\n# Set synfig-only attribs\nif g[\"spreadMethod\"] == \"repeat\":\n    g[\"loop\"] = True\nelif g[\"spreadMethod\"] == \"reflect\":\n    g[\"loop\"] = True\n    # Reflect the gradient\n    # Original: 0.0 [A . B . C] 1.0\n    # New:      0.0 [A . B . C . B . A] 1.0\n    #           (with gradient size doubled)\n    new_stops = {}\n\n    # reflect the stops\n    for pos in g[\"stops\"]:\n        val = g[\"stops\"][pos]\n        if pos == 1.0:\n            new_stops[pos/2.0] = val\n        else:\n            new_stops[pos/2.0] = val\n            new_stops[1 - pos/2.0] = val\n    g[\"stops\"] = new_stops\n\n    # double the gradient size\n    if g[\"type\"] == \"linear\":\n        g[\"p2\"] = [ g[\"p1\"][0]+2.0*(g[\"p2\"][0]-g[\"p1\"][0]),\n                    g[\"p1\"][1]+2.0*(g[\"p2\"][1]-g[\"p1\"][1]) ]\n    if g[\"type\"] == \"radial\":\n        g[\"radius\"]= 2.0*g[\"radius\"]\n\n# Rename \"stops\" to \"gradient\"\ng[\"gradient\"] = g[\"stops\"]\n\n# Convert coordinates\nif g[\"type\"] == \"linear\":\n    g[\"p1\"] = self.coor_svg2sif(g[\"p1\"])\n    g[\"p2\"] = self.coor_svg2sif(g[\"p2\"])\n\nif g[\"type\"] == \"radial\":\n    g[\"center\"] = self.coor_svg2sif(g[\"center\"])\n    g[\"radius\"] = self.distance_svg2sif(g[\"radius\"])\n\n# Delete extra attribs\nremoved_attribs = [\"type\",\n                   \"stops\",\n                   \"stops_guid\",\n                   \"mtx\",\n                   \"focus\",\n                   \"spreadMethod\"]\nfor x in removed_attribs:\n    if x in g.keys():\n        del g[x]\nreturn g", "path": "synfig_output.py", "repo_name": "nikitakit/svg2sif", "stars": 32, "license": "None", "language": "python", "size": 150}
{"docstring": "\"\"\"Encapsulate the given layers\n\nKeyword arguments:\nlayers -- list of layers\nname -- Name of the PasteCanvas layer that is created\nis_end -- set to True if layers are at the end of a canvas\n\nReturns: list of one layer\n\"\"\"\n\n", "func_signal": "def op_encapsulate(self, layers, name=\"Inline Canvas\", is_end=False):\n", "code": "if layers == []:\n    return layers\n\nlayer = self.create_layer(\"PasteCanvas\", name, params={\"canvas\":layers})\nreturn [layer]", "path": "synfig_output.py", "repo_name": "nikitakit/svg2sif", "stars": 32, "license": "None", "language": "python", "size": 150}
{"docstring": "\"\"\"Increase the opacity of the given layers by a certain amount\n\nKeyword arguments:\nlayers -- list of layers\nopacity -- the opacity to apply (float between 0.0 to 1.0)\nname -- name of the Transform layer that is added\nis_end -- set to True if layers are at the end of a canvas\n\nReturns: list of layers\n\"\"\"\n# If there is blending involved, first encapsulate the layers\n", "func_signal": "def op_fade(self, layers, opacity, is_end=False):\n", "code": "for layer in layers:\n    if self.get_param(layer, \"blend_method\") != sif.blend_methods[\"composite\"]:\n        return self.op_fade(self.op_encapsulate(layers), opacity, is_end)\n\n# Otherwise, set their amount\nfor layer in layers:\n    amount = self.get_param(layer, \"amount\")\n    self.set_param(layer, \"amount\", amount*opacity)\n\nreturn layers", "path": "synfig_output.py", "repo_name": "nikitakit/svg2sif", "stars": 32, "license": "None", "language": "python", "size": 150}
{"docstring": "# Prepare the document for exporting\n", "func_signal": "def effect(self):\n", "code": "SynfigPrep.effect(self)\n\nsvg = self.document.getroot()\nwidth = get_dimension(svg.get(\"width\", 1024))\nheight = get_dimension(svg.get(\"height\", 768))\n\ntitle = svg.xpath(\"svg:title\", namespaces=NSS)\nif len(title) == 1:\n    name = title[0].text\nelse:\n    name = svg.get(addNS(\"docname\", \"sodipodi\"), \"Synfig Animation 1\")\n\nd = SynfigDocument(width, height, name)\n\nlayers = []\nfor node in svg.iterchildren():\n    layers += self.convert_node(node, d)\n\nroot_canvas = d.get_root_canvas()\nfor layer in layers:\n    root_canvas.append(layer)\n\nd.get_root_tree().write(sys.stdout)", "path": "synfig_output.py", "repo_name": "nikitakit/svg2sif", "stars": 32, "license": "None", "language": "python", "size": 150}
{"docstring": "\"\"\"Update the viewbox to match document width and height\"\"\"\n", "func_signal": "def _update_viewbox(self):\n", "code": "attr_viewbox = \"%f %f %f %f\" % (\n     -self.width/2.0/sif.kux,\n      self.height/2.0/sif.kux,\n      self.width/2.0/sif.kux,\n     -self.height/2.0/sif.kux\n     )\nself.root_canvas.set(\"view-box\", attr_viewbox)", "path": "synfig_output.py", "repo_name": "nikitakit/svg2sif", "stars": 32, "license": "None", "language": "python", "size": 150}
{"docstring": "# raw format:\n# table, fieldname, ??? (flags?), datatype\n# ['groupmap', 'gid', '\\013\\000\\000', '\\003', '\\013B\\000']\n# MySQL returns\n# ['gid', 'groupmap', 'long', 11, 'pri notnull auto_inc mkey']\n\n", "func_signal": "def fields (self):\n", "code": "result = []\nfor field in self._fields:\n    flag_list = []\n    flag_value = struct.unpack(\n        'H', field[decode_field_pos['flags']])[0]\n\n    for value, name in decode_flag.items():\n        if 0 < value & flag_value:\n            flag_list.append(name)\n\n    self._flags = flag_list\n\n    type = field[decode_field_pos['type']]\n\n    result.append(\n        [field[decode_field_pos['table']],\n         field[decode_field_pos['name']],\n         decode_type_names[ord(type)],\n         string.join(flag_list)])\n\nreturn result", "path": "gogreen\\coromysql.py", "repo_name": "slideinc/gogreen", "stars": 59, "license": "bsd-3-clause", "language": "python", "size": 323}
{"docstring": "'''return the smallest request priority that is not full. a full\nrequest priority is one that has created all of its worker threads\n'''\n", "func_signal": "def least_prio(self):\n", "code": "for prio in xrange(len(self.sizes)):\n    if self._wcount(prio) < self.sizes[prio]:\n        break\nreturn prio", "path": "gogreen\\corowork.py", "repo_name": "slideinc/gogreen", "stars": 59, "license": "bsd-3-clause", "language": "python", "size": 323}
{"docstring": "#\n# track average request time per second for a given period\n#\n", "func_signal": "def _local_request(self, elapse, name, current):\n", "code": "record = self._local.setdefault(name, [])\n\nif not record or record[-1]['timestamp'] != current:\n    record.append({'timestamp': current, 'elapse': 0, 'count': 0})\n\nrecord[-1]['count']  += 1\nrecord[-1]['elapse'] += int(elapse * 1000000)\n#\n# clear old entries\n#\nwhile len(record) > self._depth:\n    del(record[0])", "path": "gogreen\\statistics.py", "repo_name": "slideinc/gogreen", "stars": 59, "license": "bsd-3-clause", "language": "python", "size": 323}
{"docstring": "'''profile_clear\n\nReset the threads context switch counter and the execution time\ncounters.\n'''\n", "func_signal": "def profile_clear(self, children = False):\n", "code": "self._resume_count = 0\nself._total_time   = 0\nself._long_time    = 0\n\nif not children:\n    return None\n\nfor child in self.child_list():\n    child.clear(children = children)", "path": "gogreen\\coro.py", "repo_name": "slideinc/gogreen", "stars": 59, "license": "bsd-3-clause", "language": "python", "size": 323}
{"docstring": "'''join\n\nWait, no longer then timeout, until this thread exits.\n\nresults:\n  True  - successfully joined\n  False - join failed due to timeout\n  None  - join failed because coroutine is not joinable.\n'''\n", "func_signal": "def join(self, timeout = None):\n", "code": "waiter = coroutine_cond()\nif self._joinees is not None:\n    self._joinees.append(waiter)\n    result = waiter.wait(timeout)\n    result = ((result is None and [False]) or [True])[0]\nelse:\n    result = None\n\nreturn result", "path": "gogreen\\coro.py", "repo_name": "slideinc/gogreen", "stars": 59, "license": "bsd-3-clause", "language": "python", "size": 323}
{"docstring": "'''child_drop\n\nchildren unregister from their parent when they are destroyed\n'''\n", "func_signal": "def child_drop(self, child):\n", "code": "o = self._children.pop(child.thread_id(), None)\nif o is not None:\n    self._cwaiter.wake_all()", "path": "gogreen\\coro.py", "repo_name": "slideinc/gogreen", "stars": 59, "license": "bsd-3-clause", "language": "python", "size": 323}
{"docstring": "# http://www.python.org/peps/pep-0249.html\n# (name, type_code, display_size, internal_size, precision, scale,\n#  null_ok). The first two items (name and type_code) are mandatory.\n", "func_signal": "def describe(self):\n", "code": "return tuple(map(\n    lambda x: (x[4], x[9], None, None, None, None, None),\n    self._fields))", "path": "gogreen\\coromysql.py", "repo_name": "slideinc/gogreen", "stars": 59, "license": "bsd-3-clause", "language": "python", "size": 323}
{"docstring": "'''make a request for this work server. this method can take a prio\nkwarg:  when it is present, it specifies the maximum priority slot\nthis request is made in. defaults to the first slot, 0.\n\nrequests are put into the lowest possible slot until that slot is\n\"full,\" here full meaning # of worker threads < size for priority slot.\n'''\n", "func_signal": "def request(self, *args, **kwargs):\n", "code": "if self.stop:\n    raise QueueError('Sevrer has been shutdown.')\n\n# find the priority to queue this request in to\nmax_prio = min(kwargs.get('prio', 0), len(self.sizes))\nfor prio in xrange(max_prio+1):\n    if self.requests[prio].waiters():\n        break\n    self.workers[prio] = filter(\n        lambda w: w.isAlive(), self.workers[prio])\n\n    # if we got here it means nothing is waiting for something\n    # to do at this prio. check to see if we can spawn.\n    room = max(self.sizes[prio] - len(self.workers[prio]), 0)\n    if not room:\n        continue\n    room -= len(filter(lambda i: i[1] == prio, self.klist))\n    if room > 0:\n        self.klist.append((self.work, prio))\n        self.waiter.wake_all()\n        break\n\nself.requests[prio].push((args, kwargs, time.time()))", "path": "gogreen\\corowork.py", "repo_name": "slideinc/gogreen", "stars": 59, "license": "bsd-3-clause", "language": "python", "size": 323}
{"docstring": "'''profile\n\nEnable/Disable the collection of the context switch counter and\nthe execution time counter. Optionally cascade to child threads\n'''\n", "func_signal": "def profile(self, status, children = False):\n", "code": "self._profile = status\n\nif not children:\n    return None\n\nfor child in self.child_list():\n    child.profile(status, children = children)", "path": "gogreen\\coro.py", "repo_name": "slideinc/gogreen", "stars": 59, "license": "bsd-3-clause", "language": "python", "size": 323}
{"docstring": "#\n# Perform a non-blocking poll on the socket to determine if there\n# is an error/read readiness event waiting. If there is, then\n# attempt to read into the internal connection buffer and handle\n# the error occordingly.\n#\n# In normal operation we only expect readiness in the uncommon\n# error case, normally there should be nothing (e.g. FIN) there.\n#\n", "func_signal": "def _check(self):\n", "code": "p = select.poll()\np.register(self.socket, CHECK_MASK)\n\nif not p.poll(0.0):\n    return None\n\ntry:\n    self.recv()\nexcept NetworkError:\n    raise NetworkError(\n            CR_SERVER_GONE_ERROR,\n            'MySQL server has gone away')", "path": "gogreen\\coromysql.py", "repo_name": "slideinc/gogreen", "stars": 59, "license": "bsd-3-clause", "language": "python", "size": 323}
{"docstring": "#\n# track average request time per second for a given period\n#\n", "func_signal": "def _global_request(self, elapse, current):\n", "code": "if self._global[-1]['timestamp'] != current:\n    self._global.append({'timestamp': current,'elapse': 0,'count': 0})\n\nself._global[-1]['count']  += 1\nself._global[-1]['elapse'] += int(elapse * 1000000)\n#\n# clear old entries\n#\nwhile len(self._global) > self._depth:\n    del(self._global[0])", "path": "gogreen\\statistics.py", "repo_name": "slideinc/gogreen", "stars": 59, "license": "bsd-3-clause", "language": "python", "size": 323}
{"docstring": "'''child_join\n\nchildren register with their parent at initialization time\n'''\n", "func_signal": "def child_join(self, child):\n", "code": "tid = child.thread_id()\nself._children[tid] = tid", "path": "gogreen\\coro.py", "repo_name": "slideinc/gogreen", "stars": 59, "license": "bsd-3-clause", "language": "python", "size": 323}
{"docstring": "#\n# dump all events/threads, I'm using this to shut down, so\n# if the event_loop exits, I want to startup another eventloop\n# thread set to perform shutdown functions.\n#\n", "func_signal": "def reset_event_loop():\n", "code": "global the_event_list\nglobal the_event_poll\nglobal the_loop_count\nglobal pending\n\nthe_event_list = event_list()\nthe_event_poll = event_poll()\nthe_loop_count = 0\n\npending = {}\n\nreturn None", "path": "gogreen\\coro.py", "repo_name": "slideinc/gogreen", "stars": 59, "license": "bsd-3-clause", "language": "python", "size": 323}
{"docstring": "'''preemptive_locked\n\nDecorator for functions which need to execute w/ preemption disabled.\n'''\n", "func_signal": "def preemptive_locked():\n", "code": "def function(method):\n    def disabled(obj, *args, **kwargs):\n        obj.preemptable_disable()\n        try:\n            return method(obj, *args, **kwargs)\n        finally:\n            obj.preemptable_enable()\n\n    return disabled\nreturn function", "path": "gogreen\\coro.py", "repo_name": "slideinc/gogreen", "stars": 59, "license": "bsd-3-clause", "language": "python", "size": 323}
{"docstring": "# try to re-use a server port if possible\n", "func_signal": "def set_reuse_addr (self):\n", "code": "try:\n    self.socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\nexcept:\n    pass", "path": "gogreen\\coro.py", "repo_name": "slideinc/gogreen", "stars": 59, "license": "bsd-3-clause", "language": "python", "size": 323}
{"docstring": "#\n# add self to timeout event list\n#\n", "func_signal": "def Yield(self, timeout = None, arg = None):\n", "code": "if timeout is not None:\n    triple = the_event_list.insert_event(self, timeout, arg)\n#\n# in debug mode record stack\n#\nif self._trace:\n    self._where = get_callstack()\n#\n# release control\n#\nresult = MAIN_EVENT_LOOP.switch()  ##coroutine.main(())\n#\n# remove self from timeout event list.\n#\nif timeout is not None:\n    the_event_list.remove_event(triple)\n\nreturn result", "path": "gogreen\\coro.py", "repo_name": "slideinc/gogreen", "stars": 59, "license": "bsd-3-clause", "language": "python", "size": 323}
{"docstring": "# 3-byte length, one-byte packet number, followed by packet data\n", "func_signal": "def unpacket (p):\n", "code": "a,b,c,s = map (ord, p[:4])\nl = a | (b << 8) | (c << 16)\n# s is a sequence number\n\nreturn l, s", "path": "gogreen\\coromysql.py", "repo_name": "slideinc/gogreen", "stars": 59, "license": "bsd-3-clause", "language": "python", "size": 323}
{"docstring": "\"\"\"Timeout semantics for __init__():\n   If you do not pass in a keyword arg for timeout (or\n   pass in a value of None), then the socket is blocking.\n\"\"\"\n\n", "func_signal": "def __init__ (self, *args, **kwargs):\n", "code": "self.socket = kwargs.get('sock', args and args[0] or None)\nif self.socket:\n    self.socket.setblocking(0)\n    self.set_fileno()\n\nself.family = kwargs.get('family', socket.AF_UNSPEC)\n\nme = current_thread()\nif me is not None:\n    default = me.get_socket_timeout()\nelse:\n    default = None\n\nif default is None:\n    default = getdefaulttimeout()\n\nself._timeout         = kwargs.get('timeout', default)\nself._connect_timeout = kwargs.get('connect_timeout', default)\n\nself._wake   = []\nself._closed = 0\n#\n# coro poll threads waiting for sig.\nself._waits  = {None: 0}", "path": "gogreen\\coro.py", "repo_name": "slideinc/gogreen", "stars": 59, "license": "bsd-3-clause", "language": "python", "size": 323}
{"docstring": "'''reconnect_cmds\n\nAssign the number of DB commands which will have reconnect retry\nenabled and reset the counter of commands executed towards this\ncounter back to 0. To set the actual number of retries use\nreconnect_retry_set()\n\nThe value is the number of commands for which retry will be\nenabled. (default: 0) The value None indicates no limit.\n\nNOTE: The reason for the existence of this method is that once\n      a transaction has begun auto-reconnect can lead to results\n      that are inconsistent. If auto-reconnect is desired with a\n      connection then it should only be enabled for the first\n      command.\n'''\n", "func_signal": "def reconnect_cmds(self, value):\n", "code": "self._reconnect_cmds = value\nself._reconnect_exec = 0", "path": "gogreen\\coromysql.py", "repo_name": "slideinc/gogreen", "stars": 59, "license": "bsd-3-clause", "language": "python", "size": 323}
{"docstring": "'''trace\n\nEnable/Disable the recording of the threads execution stack during\nthe most recent context switch.\n'''\n#\n# to allow orthogonal components to enable/disable trace, the\n# variable is implemented as a counter.\n#\n", "func_signal": "def trace(self, status):\n", "code": "self._trace += (-1,1)[int(bool(status))]\nself._trace  = max(self._trace, 0)", "path": "gogreen\\coro.py", "repo_name": "slideinc/gogreen", "stars": 59, "license": "bsd-3-clause", "language": "python", "size": 323}
{"docstring": "\"\"\"\n.. versionadded:: 0.2.3\n\nGet infos about an quest\n\n| ``Example:``\n::\n\n    get_quest('eu',25)\n\"\"\"\n", "func_signal": "def get_quest(self,region,questid,lastmodified=None,lang=None):\n", "code": "if not int(questid):\n    raise ValueError('Quest id must be a integer')\nreturn self._get_data(region,datatypes['quest']['path'] % (questid),None,lastmodified,lang)", "path": "scripts\\wowapi\\wowapi\\api.py", "repo_name": "Aldriana/ShadowCraft-Engine", "stars": 37, "license": "lgpl-3.0", "language": "python", "size": 1257}
{"docstring": "# Call this before calling any of the assassination_dps functions\n# directly.  If you're just calling get_dps, you can ignore this as it\n# happens automatically; however, if you're going to pull a damage\n# breakdown or other sub-result, make sure to call this, as it\n# initializes many values that are needed to perform the calculations.\n\n", "func_signal": "def init_assassination(self):\n", "code": "if not self.settings.is_assassination_rogue():\n    raise InputNotModeledException(_('You must specify an assassination cycle to match your assassination spec.'))\nif self.stats.mh.type != 'dagger' or self.stats.oh.type != 'dagger':\n    raise InputNotModeledException(_('Assassination modeling requires daggers in both hands'))\n\nself.set_constants()\n\nself.envenom_energy_cost = self.get_net_energy_cost('envenom')\nself.envenom_energy_cost *= self.stats.gear_buffs.rogue_t13_2pc_cost_multiplier()\n\nself.base_energy_regen = 10\n\nvendetta_duration = 20 + 10 * self.glyphs.vendetta\nvendetta_duration += self.stats.gear_buffs.rogue_t13_4pc * 9\nvendetta_uptime = vendetta_duration / (120 + self.settings.response_time)\nvendetta_multiplier = .3 - .05 * self.glyphs.vendetta\nself.vendetta_mult = 1 + vendetta_multiplier * vendetta_uptime\n\nshadow_blades_duration = self.get_shadow_blades_duration()\nvendetta_shadow_blades_overlap = min(shadow_blades_duration, vendetta_duration)\nvendetta_uptime_during_shadow_blades = .5 * vendetta_shadow_blades_overlap / shadow_blades_duration\nself.shadow_blades_vendetta_mult = 1 + vendetta_multiplier * vendetta_uptime_during_shadow_blades\n\nshadow_blades_spacing = 180 + self.settings.response_time\nautoattack_duration = shadow_blades_spacing - shadow_blades_duration\nautoattack_vendetta_overlap = shadow_blades_spacing * vendetta_uptime - shadow_blades_duration * vendetta_uptime_during_shadow_blades\nself.autoattack_vendetta_mult = 1 + vendetta_multiplier * autoattack_vendetta_overlap / autoattack_duration", "path": "shadowcraft\\calcs\\rogue\\Aldriana\\__init__.py", "repo_name": "Aldriana/ShadowCraft-Engine", "stars": 37, "license": "lgpl-3.0", "language": "python", "size": 1257}
{"docstring": "# If someone tries to access a talent not initialized (the talent\n# string was shorter than 6) we return False\n", "func_signal": "def __getattr__(self, name):\n", "code": "if name in self.allowed_talents:\n    return False\nobject.__getattribute__(self, name)", "path": "shadowcraft\\objects\\talents.py", "repo_name": "Aldriana/ShadowCraft-Engine", "stars": 37, "license": "lgpl-3.0", "language": "python", "size": 1257}
{"docstring": "# General setup that we'll use in all 3 cycles.\n", "func_signal": "def set_constants(self):\n", "code": "self.bonus_energy_regen = 0\nif self.settings.tricks_on_cooldown and not self.glyphs.tricks_of_the_trade:\n    self.bonus_energy_regen -= 15. / (30 + self.settings.response_time)\nif self.settings.shiv_interval != 0:\n    self.bonus_energy_regen -= 20. / self.settings.shiv_interval\nif self.race.arcane_torrent:\n    self.bonus_energy_regen += 15. / (120 + self.settings.response_time)\n\nself.set_openers()\n\nself.base_stats = {\n    'agi': self.stats.agi + self.buffs.buff_agi() + self.race.racial_agi,\n    'ap': self.stats.ap + 2 * self.level - 30,\n    'crit': self.stats.crit,\n    'haste': self.stats.haste,\n    'mastery': self.stats.mastery + self.buffs.buff_mast()\n}\n\nfor boost in self.race.get_racial_stat_boosts():\n    if boost['stat'] in self.base_stats:\n        self.base_stats[boost['stat']] += boost['value'] * boost['duration'] * 1.0 / (boost['cooldown'] + self.settings.response_time)\n\nif getattr(self.stats.gear_buffs, 'synapse_springs'):\n    self.stats.gear_buffs.activated_boosts['synapse_springs']['stat'] = 'agi'\nfor proc in self.stats.procs.get_all_procs_for_stat('highest'):\n    if 'agi' in proc.stats:\n        proc.stat = 'agi'\n\nfor stat in self.base_stats:\n    for boost in self.stats.gear_buffs.get_all_activated_boosts_for_stat(stat):\n        if boost['cooldown'] is not None:\n            self.base_stats[stat] += (boost['value'] * boost['duration']) * 1.0 / (boost['cooldown'] + self.settings.response_time)\n        else:\n            self.base_stats[stat] += (boost['value'] * boost['duration']) * 1.0 / self.settings.duration\n\nself.agi_multiplier = self.buffs.stat_multiplier() * self.stats.gear_buffs.leather_specialization_multiplier()\n\nself.base_strength = self.stats.str + self.buffs.buff_str() + self.race.racial_str\nself.base_strength *= self.buffs.stat_multiplier()\n\nself.relentless_strikes_energy_return_per_cp = .20 * 25\n\nself.base_speed_multiplier = 1.4 * self.buffs.melee_haste_multiplier() * self.get_heroism_haste_multiplier()\nif self.race.berserking:\n    self.base_speed_multiplier *= (1 + .2 * 10. / (180 + self.settings.response_time))\nif self.race.time_is_money:\n    self.base_speed_multiplier *= 1.01\n\nself.strike_hit_chance = self.one_hand_melee_hit_chance()\nself.poison_hit_chance = self.melee_spells_hit_chance()\nself.base_rupture_energy_cost = self.get_net_energy_cost('rupture')\nself.base_rupture_energy_cost *= self.stats.gear_buffs.rogue_t13_2pc_cost_multiplier()\nself.base_eviscerate_energy_cost = self.get_net_energy_cost('eviscerate')\nself.base_eviscerate_energy_cost *= self.stats.gear_buffs.rogue_t13_2pc_cost_multiplier()\n\nif self.stats.procs.heroic_matrix_restabilizer or self.stats.procs.matrix_restabilizer:\n    self.set_matrix_restabilizer_stat(self.base_stats)", "path": "shadowcraft\\calcs\\rogue\\Aldriana\\__init__.py", "repo_name": "Aldriana/ShadowCraft-Engine", "stars": 37, "license": "lgpl-3.0", "language": "python", "size": 1257}
{"docstring": "\"\"\"\n.. versionadded:: 0.3.0\n\nGet infos about an recipe\n\n| ``Example:``\n::\n\n    get_recipe('eu',33994)\n\"\"\"\n", "func_signal": "def get_recipe(self,region,recipeid,lastmodified=None,lang=None):\n", "code": "if not int(recipeid):\n    raise ValueError('Recipe id must be a integer')\nreturn self._get_data(region,datatypes['recipe']['path'] % (recipeid),None,lastmodified,lang)", "path": "scripts\\wowapi\\wowapi\\api.py", "repo_name": "Aldriana/ShadowCraft-Engine", "stars": 37, "license": "lgpl-3.0", "language": "python", "size": 1257}
{"docstring": "# TODO: Include damaging proc hits in figuring out how often everything else procs.\n", "func_signal": "def get_procs_per_second(self, proc, attacks_per_second, crit_rates):\n", "code": "if getattr(proc, 'mh_only', False):\n    procs_per_second = self.get_mh_procs_per_second(proc, attacks_per_second, crit_rates)\nelif getattr(proc, 'oh_only', False):\n    procs_per_second = self.get_oh_procs_per_second(proc, attacks_per_second, crit_rates)\nelse:\n    procs_per_second = self.get_mh_procs_per_second(proc, attacks_per_second, crit_rates)\n    procs_per_second += self.get_oh_procs_per_second(proc, attacks_per_second, crit_rates)\n    procs_per_second += self.get_other_procs_per_second(proc, attacks_per_second, crit_rates)\n\nreturn procs_per_second", "path": "shadowcraft\\calcs\\rogue\\Aldriana\\__init__.py", "repo_name": "Aldriana/ShadowCraft-Engine", "stars": 37, "license": "lgpl-3.0", "language": "python", "size": 1257}
{"docstring": "# Any proc we haven't assigned a value to, we don't have.\n", "func_signal": "def __getattr__(self, proc):\n", "code": "if proc in self.allowed_procs:\n    return False\nobject.__getattribute__(self, proc)", "path": "shadowcraft\\objects\\procs.py", "repo_name": "Aldriana/ShadowCraft-Engine", "stars": 37, "license": "lgpl-3.0", "language": "python", "size": 1257}
{"docstring": "# Builds a phony 'poison' proc object to count triggers through the proc\n# methods. Removes first poison hit.\n", "func_signal": "def get_poison_counts(self, attacks_per_second):\n", "code": "poison = procs.Proc(**proc_data.allowed_procs['rogue_poison'])\nmh_hits_per_second = self.get_mh_procs_per_second(poison, attacks_per_second, None)\noh_hits_per_second = self.get_oh_procs_per_second(poison, attacks_per_second, None)\ntotal_hits_per_second = mh_hits_per_second + oh_hits_per_second\nif self.settings.dmg_poison == 'dp':\n    poison_base_proc_rate = .3\nelif self.settings.dmg_poison == 'wp':\n    poison_base_proc_rate = .3\n\nif self.settings.is_assassination_rogue():\n    poison_base_proc_rate += .2\n    poison_envenom_proc_rate = poison_base_proc_rate + .15\n    envenom_uptime = min(sum([(1 / self.strike_hit_chance + cps) * attacks_per_second['envenom'][cps] for cps in xrange(1, 6)]), 1)\n    avg_poison_proc_rate = poison_base_proc_rate * (1 - envenom_uptime) + poison_envenom_proc_rate * envenom_uptime\nelse:\n    avg_poison_proc_rate = poison_base_proc_rate\n\nif self.settings.dmg_poison == 'dp':\n    poison_procs = avg_poison_proc_rate * total_hits_per_second - 1 / self.settings.duration\n    attacks_per_second['deadly_instant_poison'] = poison_procs * self.poison_hit_chance\n    attacks_per_second['deadly_poison'] = 1. / 3 * (1 - total_hits_per_second / self.settings.duration)\nelif self.settings.dmg_poison == 'wp':\n    attacks_per_second['wound_poison'] = total_hits_per_second * avg_poison_proc_rate * self.poison_hit_chance", "path": "shadowcraft\\calcs\\rogue\\Aldriana\\__init__.py", "repo_name": "Aldriana/ShadowCraft-Engine", "stars": 37, "license": "lgpl-3.0", "language": "python", "size": 1257}
{"docstring": "\"\"\"\nGet infos about an item\n\n| ``Example:``\n::\n\n    get_item('eu',25)\n\"\"\"\n", "func_signal": "def get_item(self,region,itemid,lastmodified=None,lang=None):\n", "code": "if not int(itemid):\n    raise ValueError('Itemid must be a integer')\nreturn self._get_data(region,datatypes['item']['path'] % (itemid),None,lastmodified,lang)", "path": "scripts\\wowapi\\wowapi\\api.py", "repo_name": "Aldriana/ShadowCraft-Engine", "stars": 37, "license": "lgpl-3.0", "language": "python", "size": 1257}
{"docstring": "# This only deals with the SS/RvS damage increase.\n", "func_signal": "def get_rogue_t13_legendary_combat_multiplier(self):\n", "code": "if self.stats.gear_buffs.rogue_t13_legendary or self.stats.procs.jaws_of_retribution or self.stats.procs.maw_of_oblivion or self.stats.procs.fangs_of_the_father:\n    return 1.45\nelse:\n    return 1.", "path": "shadowcraft\\calcs\\rogue\\Aldriana\\__init__.py", "repo_name": "Aldriana/ShadowCraft-Engine", "stars": 37, "license": "lgpl-3.0", "language": "python", "size": 1257}
{"docstring": "# TODO: Needs better implementation, should be usable for now\n", "func_signal": "def get_self_healing(self, dps_breakdown=None):\n", "code": "if dps_breakdown is None:\n    dps_breakdown = self.get_dps_breakdown()\nhealing_breakdown = {\n    #'leeching': 0,\n    'recuperate': 0, #if we ever allow recup weaving\n    #'shiv_effect': 0 #if we ever allow shiv weaving (only with lp)\n}\nhealing_sum = 0\nif self.settings.utl_poison == 'lp':\n    if self.settings.shiv_interval > 0:\n        healing_breakdown['shiv_effect'] = .05 * self.stats.get_max_health() * (1./self.settings.shiv_interval)\n    healing_breakdown['leeching'] = 0\n    for key in dps_breakdown:\n        if key in self.melee_attacks:\n            healing_breakdown['leeching'] += dps_breakdown[key]*.1\nfor entry in healing_breakdown:\n    healing_sum += healing_breakdown[entry]\nreturn healing_sum, healing_breakdown", "path": "shadowcraft\\calcs\\rogue\\Aldriana\\__init__.py", "repo_name": "Aldriana/ShadowCraft-Engine", "stars": 37, "license": "lgpl-3.0", "language": "python", "size": 1257}
{"docstring": "# This function will install gettext as the _() function and use the\n# language specified. It will fall back to code strings if given a not supported\n# language. Note that the 'local' value only makes sense when not running from\n# the hosted online version.\n", "func_signal": "def set_language(language):\n", "code": "if language == 'local':\n    # Setting up a list of locales in your machine and asign them to the _() function\n    languages_list = []\n\n    default_local_language, encoding = locale.getdefaultlocale()\n    if (default_local_language):\n        languages_list = [default_local_language]\n\n    gnu_lang = os.environ.get('LANGUAGE', None)\n    if (gnu_lang):\n        languages_list += gnu_lang.split(\":\")\n\n    gettext.translation(TRANSLATION_DOMAIN, LOCALE_DIR, fallback=True, languages=languages_list).install(unicode=True)\n\nelse:\n    gettext.translation(TRANSLATION_DOMAIN, LOCALE_DIR, fallback=True, languages=[language]).install(unicode=True)", "path": "shadowcraft\\core\\i18n.py", "repo_name": "Aldriana/ShadowCraft-Engine", "stars": 37, "license": "lgpl-3.0", "language": "python", "size": 1257}
{"docstring": "# TODO: Crit cap\n#\n# TODO: Hit/Exp procs\n\n", "func_signal": "def compute_damage(self, attack_counts_function):\n", "code": "current_stats = {\n    'agi': self.base_stats['agi'] * self.agi_multiplier,\n    'ap': self.base_stats['ap'],\n    'crit': self.base_stats['crit'],\n    'haste': self.base_stats['haste'],\n    'mastery': self.base_stats['mastery']\n}\nself.current_variables = {}\n\nactive_procs = []\ndamage_procs = []\nweapon_damage_procs = []\n\nself.setup_unique_procs()\n\nfor proc_info in self.stats.procs.get_all_procs_for_stat():\n    if proc_info.stat in current_stats and not proc_info.is_ppm():\n        active_procs.append(proc_info)\n    elif proc_info.stat in ('spell_damage', 'physical_damage'):\n        damage_procs.append(proc_info)\n    elif proc_info.stat == 'extra_weapon_damage':\n        weapon_damage_procs.append(proc_info)\n\nwindsong_enchants = []\nweapon_enchants = set([])\nfor hand, enchant in [(x, y) for x in ('mh', 'oh') for y in ('landslide', 'hurricane', 'avalanche', 'windsong', 'dancing_steel', 'elemental_force')]:\n    proc = getattr(getattr(self.stats, hand), enchant)\n    if proc:\n        setattr(proc, '_'.join((hand, 'only')), True)\n        if proc.stat in current_stats:\n            active_procs.append(proc)\n        elif enchant in ('avalanche', 'elemental_force'):\n            damage_procs.append(proc)\n        elif enchant == 'windsong':\n            windsong_enchants.append(proc)\n        elif proc.stat == 'highest' and 'agi' in proc.stats:\n            proc.stat = 'agi'\n            active_procs.append(proc)\n\n        if enchant not in weapon_enchants and enchant in ('hurricane', 'avalanche'):\n            weapon_enchants.add(enchant)\n            spell_component = copy.copy(proc)\n            delattr(spell_component, '_'.join((hand, 'only')))\n            spell_component.behaviour_toggle = 'spell'\n            if enchant == 'hurricane':\n                # This would heavily overestimate Hurricane by ignoring the refresh mechanic.\n                # active_procs.append(spell_component)\n                pass\n            elif enchant == 'avalanche':\n                damage_procs.append(spell_component)\n\nattacks_per_second, crit_rates = attack_counts_function(current_stats)\n\nfor _loop in range(20):\n    current_stats = {\n        'agi': self.base_stats['agi'],\n        'ap': self.base_stats['ap'],\n        'crit': self.base_stats['crit'],\n        'haste': self.base_stats['haste'],\n        'mastery': self.base_stats['mastery']\n    }\n\n    self.update_crit_rates_for_4pc_t11(attacks_per_second, crit_rates)\n\n    for proc in damage_procs:\n        if not proc.icd:\n            self.update_with_damaging_proc(proc, attacks_per_second, crit_rates)\n\n    for proc in active_procs:\n        if not proc.icd:\n            self.set_uptime(proc, attacks_per_second, crit_rates)\n            current_stats[proc.stat] += proc.uptime * proc.value\n\n    if windsong_enchants:\n        proc = windsong_enchants[0]\n        stats = proc.stats\n        effective_ppm_multiplier = len(windsong_enchants) * 1.0 / len(stats)\n        proc.ppm *= effective_ppm_multiplier\n        self.set_uptime(proc, attacks_per_second, crit_rates)\n        proc.ppm /= effective_ppm_multiplier\n        for stat in stats:\n            current_stats[stat] += proc.uptime * proc.value\n\n    current_stats['agi'] *= self.agi_multiplier\n    for stat in ('crit', 'haste', 'mastery'):\n        current_stats[stat] *= self.get_4pc_t12_multiplier()\n\n    old_attacks_per_second = attacks_per_second\n    attacks_per_second, crit_rates = attack_counts_function(current_stats)\n\n    if self.are_close_enough(old_attacks_per_second, attacks_per_second):\n        break\n\nfor proc in active_procs:\n    if proc.icd:\n        self.set_uptime(proc, attacks_per_second, crit_rates)\n        if proc.stat == 'agi':\n            current_stats[proc.stat] += proc.uptime * proc.value * self.agi_multiplier\n        elif proc.stat in ('crit', 'haste', 'mastery'):\n            current_stats[proc.stat] += proc.uptime * proc.value * self.get_4pc_t12_multiplier()\n        else:\n            current_stats[proc.stat] += proc.uptime * proc.value\n\nattacks_per_second, crit_rates = attack_counts_function(current_stats)\n\nself.update_crit_rates_for_4pc_t11(attacks_per_second, crit_rates)\n\nfor proc in damage_procs:\n    self.update_with_damaging_proc(proc, attacks_per_second, crit_rates)\n\nfor proc in weapon_damage_procs:\n    self.set_uptime(proc, attacks_per_second, crit_rates)\n\ndamage_breakdown = self.get_damage_breakdown(current_stats, attacks_per_second, crit_rates, damage_procs)\n\n# Discard the crit component.\nfor key in damage_breakdown:\n    damage_breakdown[key] = damage_breakdown[key][0]\n\nreturn damage_breakdown", "path": "shadowcraft\\calcs\\rogue\\Aldriana\\__init__.py", "repo_name": "Aldriana/ShadowCraft-Engine", "stars": 37, "license": "lgpl-3.0", "language": "python", "size": 1257}
{"docstring": "# TODO\n", "func_signal": "def get_talents(self):\n", "code": "talents = '000000'\n#for each row\n\nreturn (talents)", "path": "test_ui\\testing_ui.py", "repo_name": "Aldriana/ShadowCraft-Engine", "stars": 37, "license": "lgpl-3.0", "language": "python", "size": 1257}
{"docstring": "# We need to set these behaviours before calling any other method.\n# The stage 3 will very likely need a different set of behaviours\n# once we figure the whole thing.\n", "func_signal": "def setup_unique_procs(self):\n", "code": "for proc in ('jaws_of_retribution', 'maw_of_oblivion', 'fangs_of_the_father'):\n    if getattr(self.stats.procs, proc):\n        if self.talents.is_assassination_rogue():\n            spec = 'assassination'\n        elif self.talents.is_combat_rogue():\n            spec = 'combat'\n        elif self.talents.is_subtlety_rogue():\n            spec = 'subtlety'\n        getattr(self.stats.procs, proc).behaviour_toggle = spec\n\n# Tie Nokaled to the MH (equipping it in the OH, as a rogue, is unlikely)\nfor i in ('', 'heroic_', 'lfr_'):\n    proc = getattr(self.stats.procs, ''.join((i, 'nokaled_the_elements_of_death')))\n    if proc:\n        setattr(proc, 'mh_only', True)", "path": "shadowcraft\\calcs\\rogue\\Aldriana\\__init__.py", "repo_name": "Aldriana/ShadowCraft-Engine", "stars": 37, "license": "lgpl-3.0", "language": "python", "size": 1257}
{"docstring": "# Appends the keys passed in args to attacks_per_second. This includes\n# autoattack, autoattack_hits, shadow_blades, main_gauche and poisons.\n# If no args passed, it'll attempt to append all of them.\n", "func_signal": "def update_with_autoattack_passives(self, attacks_per_second, *args, **kwargs):\n", "code": "if not args or 'swings' in args or 'mh_autoattack' not in attacks_per_second or 'oh_autoattack' not in attacks_per_second:\n    attacks_per_second['mh_autoattacks'] = kwargs['attack_speed_multiplier'] / self.stats.mh.speed\n    attacks_per_second['oh_autoattacks'] = kwargs['attack_speed_multiplier'] / self.stats.oh.speed\nif self.swing_reset_spacing is not None:\n    attacks_per_second['mh_autoattacks'] *= (1 - max((1 - .5 * self.stats.mh.speed / kwargs['attack_speed_multiplier']), 0) / self.swing_reset_spacing)\n    attacks_per_second['oh_autoattacks'] *= (1 - max((1 - .5 * self.stats.oh.speed / kwargs['attack_speed_multiplier']), 0) / self.swing_reset_spacing)\nif (not args or 'shadow_blades' in args):\n    if 'shadow_blades_uptime' not in kwargs:\n        kwargs['shadow_blades_uptime'] = self.get_shadow_blades_uptime()\n    self.update_with_shadow_blades(attacks_per_second, kwargs['shadow_blades_uptime'])\nif not args or 'autoattack_hits' in args:\n    attacks_per_second['mh_autoattack_hits'] = attacks_per_second['mh_autoattacks'] * self.dual_wield_mh_hit_chance()\n    attacks_per_second['oh_autoattack_hits'] = attacks_per_second['oh_autoattacks'] * self.dual_wield_oh_hit_chance()\nif not args or 'poisons' in args:\n    self.get_poison_counts(attacks_per_second)\nif self.settings.is_combat_rogue() and (not args or 'main_gauche' in args):\n    if 'main_gauche_proc_rate' in kwargs:\n        main_gauche_proc_rate = kwargs['main_gauche_proc_rate']\n    elif 'current_stats' in kwargs:\n        main_gauche_proc_rate = self.combat_mastery_conversion * self.stats.get_mastery_from_rating(kwargs['current_stats']['mastery']) * self.one_hand_melee_hit_chance()\n    attacks_per_second['main_gauche'] = main_gauche_proc_rate * (attacks_per_second['mh_autoattack_hits'] + attacks_per_second['mh_shadow_blade'])", "path": "shadowcraft\\calcs\\rogue\\Aldriana\\__init__.py", "repo_name": "Aldriana/ShadowCraft-Engine", "stars": 37, "license": "lgpl-3.0", "language": "python", "size": 1257}
{"docstring": "# This could be merged with __setattr__; its sole purpose is\n# to clearly surface the parameters passed with the behaviours.\n", "func_signal": "def _set_behaviour(self, icd, trigger, proc_chance=False, ppm=False, on_crit=False, on_procced_strikes=True, real_ppm=False):\n", "code": "self.proc_chance = proc_chance\nself.trigger = trigger\nself.icd = icd\nself.on_crit = on_crit\nself.ppm = ppm\nself.real_ppm = real_ppm\nself.on_procced_strikes = on_procced_strikes  # Main Gauche and its kin", "path": "shadowcraft\\objects\\procs.py", "repo_name": "Aldriana/ShadowCraft-Engine", "stars": 37, "license": "lgpl-3.0", "language": "python", "size": 1257}
{"docstring": "# bugged\n", "func_signal": "def calculate(self):\n", "code": "if not self.initializing:\n    gear_stats = self.gear_page.get_stats()\n    my_stats = stats.Stats(**gear_stats)\n    my_talents = talents.Talents(talent_string='311113') #(*self.talents_page.get_talents() )\n    #my_talents = '311113'\n    my_glyphs = glyphs.Glyphs('rogue', *self.talents_page.get_glyphs()) #.RogueGlyphs(*self.talents_page.get_glyphs())\n    my_buffs = buffs.Buffs(*self.buffs_page.current_buffs)\n    my_race = race.Race(self.settings_page.get_race())\n    test_settings = settings.Settings(self.settings_page.get_cycle(), response_time = self.settings_page.get_response_time())\n\n    self.error_area.SetLabel(\"\")\n    try:\n        calculator = AldrianasRogueDamageCalculator(my_stats, my_talents, my_glyphs, my_buffs, my_race, test_settings)\n        dps = calculator.get_dps()\n        ep_values = calculator.get_ep()\n        dps_breakdown = calculator.get_dps_breakdown()\n\n    except exceptions.InvalidInputException as e:\n        self.error_area.SetLabel(str(e))\n    \n    self.dps.SetValue(str(dps))\n    self.ep_box.SetValue(self.pretty_print(ep_values))\n    self.dps_breakdown.SetValue(self.pretty_print(dps_breakdown))\n    for stat in GearPage.stats:\n        tc = getattr(self, stat)\n        tc.SetValue(str(gear_stats[stat]))", "path": "test_ui\\testing_ui.py", "repo_name": "Aldriana/ShadowCraft-Engine", "stars": 37, "license": "lgpl-3.0", "language": "python", "size": 1257}
{"docstring": "\"\"\"\nReturns all auctions of a realms\n\n| ``Example:``\n::\n\n    get_auction('eu','Doomhammer')\n\"\"\"\n", "func_signal": "def get_auctions(self,region,realm,lastmodified=None,lang=None):\n", "code": "data = self._get_data(region,datatypes['auction']['path'] % (quote(realm)),None,lastmodified,lang)\nrequest = Request(data['data']['files'][0]['url'], None, {'Accept-Encoding': 'gzip'})\n\nreturn {'lastmodified': data['lastmodified'],'data':self._decode_response(self._do_request(request))}", "path": "scripts\\wowapi\\wowapi\\api.py", "repo_name": "Aldriana/ShadowCraft-Engine", "stars": 37, "license": "lgpl-3.0", "language": "python", "size": 1257}
{"docstring": "# Computes the combined probabilites of getting an additional cp from\n# each of the items in probs.\n", "func_signal": "def get_cp_per_cpg(self, base_cp_per_cpg=1, *probs):\n", "code": "cp_per_cpg = {base_cp_per_cpg: 1}\nfor prob in probs:\n    if prob == 0:\n        continue\n    new_cp_per_cpg = {}\n    for cp in cp_per_cpg:\n        new_cp_per_cpg.setdefault(cp, 0)\n        new_cp_per_cpg.setdefault(cp + 1, 0)\n        new_cp_per_cpg[cp] += cp_per_cpg[cp] * (1 - prob)\n        new_cp_per_cpg[cp + 1] += cp_per_cpg[cp] * prob\n    cp_per_cpg = new_cp_per_cpg\nreturn cp_per_cpg", "path": "shadowcraft\\calcs\\rogue\\Aldriana\\__init__.py", "repo_name": "Aldriana/ShadowCraft-Engine", "stars": 37, "license": "lgpl-3.0", "language": "python", "size": 1257}
{"docstring": "\"\"\"\nSwitching the anonymous_access flag should change the location of the \nrepository\n\"\"\"\n", "func_signal": "def testMovePrivateToPublic(self):\n", "code": "src_repo = SourceRepository.objects.get(name=\"private-repo\")\nsrc_repo.anonymous_access = True\nsrc_repo.save()\nself.assertEquals(src_repo.repo_path, os.path.abspath(os.path.join(PUBLIC_REPO_DIR, 'private-repo')))\nself.assertTrue(os.path.exists(src_repo.repo_path))", "path": "repositories\\tests.py", "repo_name": "washingtontimes/django-repositories", "stars": 39, "license": "apache-2.0", "language": "python", "size": 367}
{"docstring": "\"\"\"\nDeleting the repository should remove the repository\n\"\"\"\n", "func_signal": "def testDeletePublicRemovesRepo(self):\n", "code": "src_repo = SourceRepository.objects.get(anonymous_access=True)\nrepo_path = src_repo.repo_path\nsrc_repo.delete()\nself.assertFalse(os.path.exists(repo_path))", "path": "repositories\\tests.py", "repo_name": "washingtontimes/django-repositories", "stars": 39, "license": "apache-2.0", "language": "python", "size": 367}
{"docstring": "\"\"\"\nIs this user an owner of the project?\n\n:param userobj: The :class:`User` to test\n:type userobj: :class:`User`\n:returns: ``True`` if the :class:`User` is an owner\n:rtype: ``boolean``\n\"\"\"\n", "func_signal": "def user_is_owner(self, userobj):\n", "code": "if settings.USE_OBJECT_PERMS:\n    return userobj.has_object_perm(self, self.perms.owner)\nelse:\n    try:\n        repousr = self.repositoryuser_set.get(user=userobj)\n        return repousr.permission > 4\n    except:\n        pass\n\n    for group in self.repositorygroup_set.filter(permission__gte=5):\n        if userobj in group.group.user_set.all():\n            return True\n\nreturn False", "path": "repositories\\models.py", "repo_name": "washingtontimes/django-repositories", "stars": 39, "license": "apache-2.0", "language": "python", "size": 367}
{"docstring": "\"\"\"\nDeletes the source repository before it deletes the record.\n\"\"\"\n# Delete the source repository here\n", "func_signal": "def delete(self):\n", "code": "self._vcs.delete()\nsuper(SourceRepository, self).delete()", "path": "repositories\\models.py", "repo_name": "washingtontimes/django-repositories", "stars": 39, "license": "apache-2.0", "language": "python", "size": 367}
{"docstring": "\"\"\"\nDoes this user have write access to this repository?\n\n:param userobj: The :class:`User` to test\n:type userobj: :class:`User`\n:returns: ``True`` if the :class:`User` can write to this repository\n:rtype: ``boolean``\n\"\"\"\n", "func_signal": "def user_can_write(self, userobj):\n", "code": "if settings.USE_OBJECT_PERMS:\n    return userobj.has_object_perm(self, self.perms.write)\nelse:\n    try:\n        repousr = self.repositoryuser_set.get(user=userobj)\n        return repousr.permission > 2\n    except:\n        pass\n\n    for group in self.repositorygroup_set.filter(permission__gte=3):\n        if userobj in group.group.user_set.all():\n            return True\n\nreturn False", "path": "repositories\\models.py", "repo_name": "washingtontimes/django-repositories", "stars": 39, "license": "apache-2.0", "language": "python", "size": 367}
{"docstring": "\"\"\"\nA base class to handle Version Control System functions\n\nname = name of the repository\nanonymous_access = Is it public?\ntemplate = The name of the template to use\n\"\"\"\n", "func_signal": "def __init__(self, name, anonymous_access, template=None):\n", "code": "self.public = anonymous_access\nself.name = name\nself.template = template\nself.config = self.get_config()\nself._update_path() # Sets self.path and url", "path": "repositories\\vcs\\base.py", "repo_name": "washingtontimes/django-repositories", "stars": 39, "license": "apache-2.0", "language": "python", "size": 367}
{"docstring": "\"\"\"\nDelete the source repository here\n\"\"\"\n", "func_signal": "def delete(self):\n", "code": "if self.exists():\n    shutil.rmtree(self.path)", "path": "repositories\\vcs\\base.py", "repo_name": "washingtontimes/django-repositories", "stars": 39, "license": "apache-2.0", "language": "python", "size": 367}
{"docstring": "\"\"\"\nGet the users with write access, including users that are in a\n:class:`RepositoryGroup` with write access.\n\n:rtype: ``list`` of :class:`User`\n\"\"\"\n", "func_signal": "def members(self):\n", "code": "users = self.repositoryuser_set.filter(permission=3).select_related()\ngroups = self.repositorygroup_set.filter(permission=3).select_related()\n\nuser_set = set([i.user for i in users])\nfor group in groups:\n    user_set.update(set(group.user_set.all()))\nreturn list(user_set)", "path": "repositories\\models.py", "repo_name": "washingtontimes/django-repositories", "stars": 39, "license": "apache-2.0", "language": "python", "size": 367}
{"docstring": "\"\"\"\nDeleting the repository should remove the repository\n\"\"\"\n", "func_signal": "def testDeletePrivateRemovesRepo(self):\n", "code": "src_repo = SourceRepository.objects.get(anonymous_access=False)\nrepo_path = src_repo.repo_path\nsrc_repo.delete()\nself.assertFalse(os.path.exists(repo_path))", "path": "repositories\\tests.py", "repo_name": "washingtontimes/django-repositories", "stars": 39, "license": "apache-2.0", "language": "python", "size": 367}
{"docstring": "\"\"\"\nWhen a :class:`SourceRepository` object is instantiated and the object\nhas selected a version control system, it sets up the ``self._vcs`` \nattribute to the :class:`VCSClass` implementation, else set it to ``None``\n\"\"\"\n", "func_signal": "def __init__(self, *args, **kwargs):\n", "code": "super(SourceRepository, self).__init__(*args, **kwargs)\n# Select the appropriate VCS class from the VCS dictionary and instantiate it\nif self.vc_system:\n    VCSClass = VCS[self.vc_system]\n    self._vcs = VCSClass(self.name, self.anonymous_access, self.repo_template)\nelse:\n    self._vcs = None", "path": "repositories\\models.py", "repo_name": "washingtontimes/django-repositories", "stars": 39, "license": "apache-2.0", "language": "python", "size": 367}
{"docstring": "\"\"\"\nMove a repository from private to public\n\"\"\"\n", "func_signal": "def make_public(self):\n", "code": "dest = os.path.abspath(os.path.join(self.config['public_path'], self.name))\nsource = self.path\nshutil.move(source, dest)\nself.public = True\nself._update_path()", "path": "repositories\\vcs\\base.py", "repo_name": "washingtontimes/django-repositories", "stars": 39, "license": "apache-2.0", "language": "python", "size": 367}
{"docstring": "\"\"\"\nCreate a setup.py file and print it to standard out based on repository settings\n\nYou can pass in a string/unicode for the name of the project, and int for\nthe id of the project, or an instance of the model.\n\"\"\"\n", "func_signal": "def create_setup_script(sourcerepo):\n", "code": "if isinstance(sourcerepo, (str,unicode)):\n    repo = SourceRepository.objects.get(name=sourcerepo)\nelif isinstance(sourcerepo, int):\n    repo = SourceRepository.objects.get(pk=sourcerepo)\nelif isinstance(sourcerepo, SourceRepository):\n    repo = sourcerepo\ntpl = loader.get_template('setup_py.txt')\n\nsite = Site.objects.get_current()\nauthors = repo.owners()\ntry:\n    version = repo.metadata_set.get(key=VERSION_KEY).value\nexcept:\n    version = '0.0.1'\ndescription = repo.summary or ''\nlong_description = repo.description or ''\ncontext = Context({\n    'name': repo.name,\n    'version': version,\n    'description': mkdn2rest(description),\n    'long_description': mkdn2rest(long_description),\n    'author': \", \".join([author.get_full_name() for author in authors]),\n    'author_email': \", \".join([author.email for author in authors]),\n    'url': \"%s%s\" % (site.domain,repo.get_absolute_url()),\n    'classifications': [md.key for md in repo.metadata_set.all() if md.key != VERSION_KEY],\n})\nreturn tpl.render(context)", "path": "repositories\\utils.py", "repo_name": "washingtontimes/django-repositories", "stars": 39, "license": "apache-2.0", "language": "python", "size": 367}
{"docstring": "\"\"\"\nMove a repository from public to private\n\"\"\"\n", "func_signal": "def make_private(self):\n", "code": "source = self.path\ndest = os.path.abspath(os.path.join(self.config['private_path'], self.name))\nshutil.move(source, dest)\nself.public = False\nself._update_path()", "path": "repositories\\vcs\\base.py", "repo_name": "washingtontimes/django-repositories", "stars": 39, "license": "apache-2.0", "language": "python", "size": 367}
{"docstring": "\"\"\"\nUpdate a remote repository when it receives a repository_changed signal and\nit has one or more remote repositories.\n\n:param sender: The :class:`SourceRepository` that changed\n:type sender: :class:`SourceRepository`\n:param current_rev: The current revision\n:type current_rev: ``string``\n:param previous_rev: The previous revision found. There could be multiple\n                     revisions in between, depending on the checking frequency\n:type previous_rev: ``string``\n\"\"\"\n", "func_signal": "def update_remote(sender, current_rev, previous_rev, **kwargs):\n", "code": "for remote_repo in sender.remotesourcerepository_set.all():\n    sender._vcs.update_remote(name=remote_repo.name, branch=remote_repo.branch)", "path": "repositories\\models.py", "repo_name": "washingtontimes/django-repositories", "stars": 39, "license": "apache-2.0", "language": "python", "size": 367}
{"docstring": "\"\"\"\nSwitching the anonymous_access flag should change the location of the \nrepository\n\"\"\"\n", "func_signal": "def testMovePrivateToPublic(self):\n", "code": "src_repo = SourceRepository.objects.get(name=\"public-repo\")\nsrc_repo.anonymous_access = False\nsrc_repo.save()\nself.assertEquals(src_repo.repo_path, os.path.abspath(os.path.join(PRIVATE_REPO_DIR, 'public-repo')))\nself.assertTrue(os.path.exists(src_repo.repo_path))", "path": "repositories\\tests.py", "repo_name": "washingtontimes/django-repositories", "stars": 39, "license": "apache-2.0", "language": "python", "size": 367}
{"docstring": "\"\"\"\nReturn a list of source repositories which the user has at least the\ngiven permission level\n\n:param user: The :class:`User` for which to find :class:`SourceRepository` instances.\n:type user: :class:`User`\n:param permission: The type of permission, one of :data:`repositories.models.PERM_CHOICES`\\ . **Default:** 3 (read/write)\n:type permission: `integer`\n:returns: A :class:`QuerySet` of :class:`SourceRepository`\n:rtype: :class:`QuerySet`\n\"\"\"\n", "func_signal": "def get_for_user(self, user, permission=3):\n", "code": "if settings.USE_OBJECT_PERMS:\n    return user.get_objects_with_perms(SourceRepository, permission)\nproject_ids = []\nif permission == 1:\n    # We have to add in every Public source repository in only this case\n    project_ids.extend([x[0] for x in SourceRepository.objects.filter(anonymous_access=True).values_list('id')])\nproject_ids.extend([x[0] for x in RepositoryUser.objects.filter(user=user, permission__gte=permission).values_list('source_repository_id')])\n\ngroup_ids = [x[0] for x in user.groups.all().values_list('id')]\nrepo_ids = [x[0] for x in RepositoryGroup.objects.filter(\n                        permission__gte=permission, \n                        group__id__in=group_ids).values_list('source_repository_id')]\nproject_ids.extend(repo_ids)\nreturn super(SourceRepositoryManager, self).get_query_set().filter(pk__in=project_ids).order_by('name')", "path": "repositories\\models.py", "repo_name": "washingtontimes/django-repositories", "stars": 39, "license": "apache-2.0", "language": "python", "size": 367}
{"docstring": "\"\"\"\nGet a list of folders that contain the templates for first import.\n\"\"\"\n", "func_signal": "def get_repo_template_choices():\n", "code": "rtmpls=os.listdir(os.path.abspath(os.path.join(os.path.dirname(__file__), 'repo-templates')))\nreturn [(item,item) for item in rtmpls if item[0] != '.']", "path": "repositories\\settings.py", "repo_name": "washingtontimes/django-repositories", "stars": 39, "license": "apache-2.0", "language": "python", "size": 367}
{"docstring": "\"\"\"\nMove a repository from private to public. \n\nCalls the instantiated version control object to do the low-level work,\nand then updates the :attr:`repo_path` and :attr:`repo_url`\n\n:returns: Nothing\n\"\"\"\n", "func_signal": "def move_to_public(self):\n", "code": "self._vcs.make_public()\nself.repo_path = self._vcs.path\nself.repo_url = self._vcs.url", "path": "repositories\\models.py", "repo_name": "washingtontimes/django-repositories", "stars": 39, "license": "apache-2.0", "language": "python", "size": 367}
{"docstring": "\"\"\"\nMove a repository from public to private\n\nCalls the instantiated version control object to do the low-level work,\nand then updates the :attr:`repo_path` and :attr:`repo_url`\n\n:returns: Nothing\n\"\"\"\n", "func_signal": "def move_to_private(self):\n", "code": "self._vcs.make_private()\nself.repo_path = self._vcs.path\nself.repo_url = self._vcs.url", "path": "repositories\\models.py", "repo_name": "washingtontimes/django-repositories", "stars": 39, "license": "apache-2.0", "language": "python", "size": 367}
{"docstring": "\"\"\"\nCreating a new SourceRepository object should create a new repository\n\"\"\"\n", "func_signal": "def testNewPublicRepo(self):\n", "code": "src_repo = SourceRepository.objects.get(name=\"public-repo\")\nself.assertEquals(src_repo.repo_path, os.path.abspath(os.path.join(PUBLIC_REPO_DIR, 'public-repo')))\nself.assertTrue(os.path.exists(src_repo.repo_path))", "path": "repositories\\tests.py", "repo_name": "washingtontimes/django-repositories", "stars": 39, "license": "apache-2.0", "language": "python", "size": 367}
{"docstring": "\"\"\"Pops the tag stack up to and including the most recent\ninstance of the given tag. If inclusivePop is false, pops the tag\nstack up to but *not* including the most recent instqance of\nthe given tag.\"\"\"\n#print \"Popping to %s\" % name\n", "func_signal": "def _popToTag(self, name, inclusivePop=True):\n", "code": "if name == self.ROOT_TAG_NAME:\n    return\n\nnumPops = 0\nmostRecentTag = None\nfor i in range(len(self.tagStack)-1, 0, -1):\n    if name == self.tagStack[i].name:\n        numPops = len(self.tagStack)-i\n        break\nif not inclusivePop:\n    numPops = numPops - 1\n\nfor i in range(0, numPops):\n    mostRecentTag = self.popTag()\nreturn mostRecentTag", "path": "demos\\BeautifulSoup.py", "repo_name": "okoye/PyV8", "stars": 55, "license": "None", "language": "python", "size": 5500}
{"docstring": "\"\"\"Returns true iff the given string is the name of a\nself-closing tag according to this parser.\"\"\"\n", "func_signal": "def isSelfClosingTag(self, name):\n", "code": "return self.SELF_CLOSING_TAGS.has_key(name) \\\n       or self.instanceSelfClosingTags.has_key(name)", "path": "demos\\BeautifulSoup.py", "repo_name": "okoye/PyV8", "stars": 55, "license": "None", "language": "python", "size": 5500}
{"docstring": "\"\"\"Treat a bogus SGML declaration as raw data. Treat a CDATA\ndeclaration as a CData object.\"\"\"\n", "func_signal": "def parse_declaration(self, i):\n", "code": "j = None\nif self.rawdata[i:i+9] == '<![CDATA[':\n     k = self.rawdata.find(']]>', i)\n     if k == -1:\n         k = len(self.rawdata)\n     data = self.rawdata[i+9:k]\n     j = k+3\n     self._toStringSubclass(data, CData)\nelse:\n    try:\n        j = HTMLParser.parse_declaration(self, i)\n    except HTMLParseError:\n        toHandle = self.rawdata[i:]\n        self.handle_data(toHandle)\n        j = i + len(toHandle)\nreturn j", "path": "demos\\BeautifulSoup.py", "repo_name": "okoye/PyV8", "stars": 55, "license": "None", "language": "python", "size": 5500}
{"docstring": "# we use preemption scheduler to switch between threads\n# so, just comment the JSUnlocker\n#\n# with JSUnlocker() as unlocker:\n", "func_signal": "def add(self, value):\n", "code": "time.sleep(0.1)\n\nself.result.append(value)", "path": "PyV8.py", "repo_name": "okoye/PyV8", "stars": 55, "license": "None", "language": "python", "size": 5500}
{"docstring": "\"\"\"Turns a list of maps, lists, or scalars into a single map.\nUsed to build the SELF_CLOSING_TAGS, NESTABLE_TAGS, and\nNESTING_RESET_TAGS maps out of lists and partial maps.\"\"\"\n", "func_signal": "def buildTagMap(default, *args):\n", "code": "built = {}\nfor portion in args:\n    if hasattr(portion, 'items'):\n        #It's a map. Merge it.\n        for k,v in portion.items():\n            built[k] = v\n    elif isList(portion) and not isString(portion):\n        #It's a list. Map each item to the default.\n        for k in portion:\n            built[k] = default\n    else:\n        #It's a scalar. Map it to the default.\n        built[portion] = default\nreturn built", "path": "demos\\BeautifulSoup.py", "repo_name": "okoye/PyV8", "stars": 55, "license": "None", "language": "python", "size": 5500}
{"docstring": "#print \"Push\", tag.name\n", "func_signal": "def pushTag(self, tag):\n", "code": "if self.currentTag:\n    self.currentTag.contents.append(tag)\nself.tagStack.append(tag)\nself.currentTag = self.tagStack[-1]", "path": "demos\\BeautifulSoup.py", "repo_name": "okoye/PyV8", "stars": 55, "license": "None", "language": "python", "size": 5500}
{"docstring": "\"\"\"Sets up the initial relations between this element and\nother elements.\"\"\"\n", "func_signal": "def setup(self, parent=None, previous=None):\n", "code": "self.parent = parent\nself.previous = previous\nself.next = None\nself.previousSibling = None\nself.nextSibling = None\nif self.parent and self.parent.contents:\n    self.previousSibling = self.parent.contents[-1]\n    self.previousSibling.nextSibling = self", "path": "demos\\BeautifulSoup.py", "repo_name": "okoye/PyV8", "stars": 55, "license": "None", "language": "python", "size": 5500}
{"docstring": "\"\"\"Initializes a map representation of this tag's attributes,\nif not already initialized.\"\"\"\n", "func_signal": "def _getAttrMap(self):\n", "code": "if not getattr(self, 'attrMap'):\n    self.attrMap = {}\n    for (key, value) in self.attrs:\n        self.attrMap[key] = value\nreturn self.attrMap", "path": "demos\\BeautifulSoup.py", "repo_name": "okoye/PyV8", "stars": 55, "license": "None", "language": "python", "size": 5500}
{"docstring": "\"\"\"Recursively destroys the contents of this tree.\"\"\"\n", "func_signal": "def decompose(self):\n", "code": "contents = [i for i in self.contents]\nfor i in contents:\n    if isinstance(i, Tag):\n        i.decompose()\n    else:\n        i.extract()\nself.extract()", "path": "demos\\BeautifulSoup.py", "repo_name": "okoye/PyV8", "stars": 55, "license": "None", "language": "python", "size": 5500}
{"docstring": "#print \"End tag %s\" % name\n", "func_signal": "def unknown_endtag(self, name):\n", "code": "if self.quoteStack and self.quoteStack[-1] != name:\n    #This is not a real end tag.\n    #print \"</%s> is not real!\" % name\n    self.handle_data('</%s>' % name)\n    return\nself.endData()\nself._popToTag(name)\nif self.quoteStack and self.quoteStack[-1] == name:\n    self.quoteStack.pop()\n    self.literal = (len(self.quoteStack) > 0)", "path": "demos\\BeautifulSoup.py", "repo_name": "okoye/PyV8", "stars": 55, "license": "None", "language": "python", "size": 5500}
{"docstring": "\"\"\"Returns the closest parent of this Tag that matches the given\ncriteria.\"\"\"\n# NOTE: We can't use _findOne because findParents takes a different\n# set of arguments.\n", "func_signal": "def findParent(self, name=None, attrs={}, **kwargs):\n", "code": "r = None\nl = self.findParents(name, attrs, 1)\nif l:\n    r = l[0]\nreturn r", "path": "demos\\BeautifulSoup.py", "repo_name": "okoye/PyV8", "stars": 55, "license": "None", "language": "python", "size": 5500}
{"docstring": "\"\"\"Handle a processing instruction as a ProcessingInstruction\nobject, possibly one with a %SOUP-ENCODING% slot into which an\nencoding will be plugged later.\"\"\"\n", "func_signal": "def handle_pi(self, text):\n", "code": "if text[:3] == \"xml\":\n    text = u\"xml version='1.0' encoding='%SOUP-ENCODING%'\"\nself._toStringSubclass(text, ProcessingInstruction)", "path": "demos\\BeautifulSoup.py", "repo_name": "okoye/PyV8", "stars": 55, "license": "None", "language": "python", "size": 5500}
{"docstring": "\"\"\"Handle entity references as data, possibly converting known\nHTML and/or XML entity references to the corresponding Unicode\ncharacters.\"\"\"\n", "func_signal": "def handle_entityref(self, ref):\n", "code": "data = None\nif self.soup.convertHTMLEntities:\n    try:\n        data = unichr(name2codepoint[ref])\n    except KeyError:\n        pass\n\nif not data and self.soup.convertXMLEntities:\n        data = self.soup.XML_ENTITIES_TO_SPECIAL_CHARS.get(ref)\n\nif not data and self.soup.convertHTMLEntities and \\\n    not self.soup.XML_ENTITIES_TO_SPECIAL_CHARS.get(ref):\n        # TODO: We've got a problem here. We're told this is\n        # an entity reference, but it's not an XML entity\n        # reference or an HTML entity reference. Nonetheless,\n        # the logical thing to do is to pass it through as an\n        # unrecognized entity reference.\n        #\n        # Except: when the input is \"&carol;\" this function\n        # will be called with input \"carol\". When the input is\n        # \"AT&T\", this function will be called with input\n        # \"T\". We have no way of knowing whether a semicolon\n        # was present originally, so we don't know whether\n        # this is an unknown entity or just a misplaced\n        # ampersand.\n        #\n        # The more common case is a misplaced ampersand, so I\n        # escape the ampersand and omit the trailing semicolon.\n        data = \"&amp;%s\" % ref\nif not data:\n    # This case is different from the one above, because we\n    # haven't already gone through a supposedly comprehensive\n    # mapping of entities to Unicode characters. We might not\n    # have gone through any mapping at all. So the chances are\n    # very high that this is a real entity, and not a\n    # misplaced ampersand.\n    data = \"&%s;\" % ref\nself.handle_data(data)", "path": "demos\\BeautifulSoup.py", "repo_name": "okoye/PyV8", "stars": 55, "license": "None", "language": "python", "size": 5500}
{"docstring": "\"\"\"Given a document, tries to detect its XML encoding.\"\"\"\n", "func_signal": "def _detectEncoding(self, xml_data, isHTML=False):\n", "code": "xml_encoding = sniffed_xml_encoding = None\ntry:\n    if xml_data[:4] == '\\x4c\\x6f\\xa7\\x94':\n        # EBCDIC\n        xml_data = self._ebcdic_to_ascii(xml_data)\n    elif xml_data[:4] == '\\x00\\x3c\\x00\\x3f':\n        # UTF-16BE\n        sniffed_xml_encoding = 'utf-16be'\n        xml_data = unicode(xml_data, 'utf-16be').encode('utf-8')\n    elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xfe\\xff') \\\n             and (xml_data[2:4] != '\\x00\\x00'):\n        # UTF-16BE with BOM\n        sniffed_xml_encoding = 'utf-16be'\n        xml_data = unicode(xml_data[2:], 'utf-16be').encode('utf-8')\n    elif xml_data[:4] == '\\x3c\\x00\\x3f\\x00':\n        # UTF-16LE\n        sniffed_xml_encoding = 'utf-16le'\n        xml_data = unicode(xml_data, 'utf-16le').encode('utf-8')\n    elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xff\\xfe') and \\\n             (xml_data[2:4] != '\\x00\\x00'):\n        # UTF-16LE with BOM\n        sniffed_xml_encoding = 'utf-16le'\n        xml_data = unicode(xml_data[2:], 'utf-16le').encode('utf-8')\n    elif xml_data[:4] == '\\x00\\x00\\x00\\x3c':\n        # UTF-32BE\n        sniffed_xml_encoding = 'utf-32be'\n        xml_data = unicode(xml_data, 'utf-32be').encode('utf-8')\n    elif xml_data[:4] == '\\x3c\\x00\\x00\\x00':\n        # UTF-32LE\n        sniffed_xml_encoding = 'utf-32le'\n        xml_data = unicode(xml_data, 'utf-32le').encode('utf-8')\n    elif xml_data[:4] == '\\x00\\x00\\xfe\\xff':\n        # UTF-32BE with BOM\n        sniffed_xml_encoding = 'utf-32be'\n        xml_data = unicode(xml_data[4:], 'utf-32be').encode('utf-8')\n    elif xml_data[:4] == '\\xff\\xfe\\x00\\x00':\n        # UTF-32LE with BOM\n        sniffed_xml_encoding = 'utf-32le'\n        xml_data = unicode(xml_data[4:], 'utf-32le').encode('utf-8')\n    elif xml_data[:3] == '\\xef\\xbb\\xbf':\n        # UTF-8 with BOM\n        sniffed_xml_encoding = 'utf-8'\n        xml_data = unicode(xml_data[3:], 'utf-8').encode('utf-8')\n    else:\n        sniffed_xml_encoding = 'ascii'\n        pass\nexcept:\n    xml_encoding_match = None\nxml_encoding_re = '^<\\?.*encoding=[\\'\"](.*?)[\\'\"].*\\?>'.encode()\nxml_encoding_match = re.compile(xml_encoding_re).match(xml_data)\nif not xml_encoding_match and isHTML:\n    meta_re = '<\\s*meta[^>]+charset=([^>]*?)[;\\'\">]'.encode()\n    regexp = re.compile(meta_re, re.I)\n    xml_encoding_match = regexp.search(xml_data)\nif xml_encoding_match is not None:\n    xml_encoding = xml_encoding_match.groups()[0].decode(\n        'ascii').lower()\n    if isHTML:\n        self.declaredHTMLEncoding = xml_encoding\n    if sniffed_xml_encoding and \\\n       (xml_encoding in ('iso-10646-ucs-2', 'ucs-2', 'csunicode',\n                         'iso-10646-ucs-4', 'ucs-4', 'csucs4',\n                         'utf-16', 'utf-32', 'utf_16', 'utf_32',\n                         'utf16', 'u16')):\n        xml_encoding = sniffed_xml_encoding\nreturn xml_data, xml_encoding, sniffed_xml_encoding", "path": "demos\\BeautifulSoup.py", "repo_name": "okoye/PyV8", "stars": 55, "license": "None", "language": "python", "size": 5500}
{"docstring": "\"\"\"Convenience method that works with all 2.x versions of Python\nto determine whether or not something is listlike.\"\"\"\n", "func_signal": "def isList(l):\n", "code": "return ((hasattr(l, '__iter__') and not isString(l))\n        or (type(l) in (types.ListType, types.TupleType)))", "path": "demos\\BeautifulSoup.py", "repo_name": "okoye/PyV8", "stars": 55, "license": "None", "language": "python", "size": 5500}
{"docstring": "\"\"\"Beautiful Soup can detect a charset included in a META tag,\ntry to convert the document to that charset, and re-parse the\ndocument from the beginning.\"\"\"\n", "func_signal": "def extractCharsetFromMeta(self, attrs):\n", "code": "httpEquiv = None\ncontentType = None\ncontentTypeIndex = None\ntagNeedsEncodingSubstitution = False\n\nfor i in range(0, len(attrs)):\n    key, value = attrs[i]\n    key = key.lower()\n    if key == 'http-equiv':\n        httpEquiv = value\n    elif key == 'content':\n        contentType = value\n        contentTypeIndex = i\n\nif httpEquiv and contentType: # It's an interesting meta tag.\n    match = self.CHARSET_RE.search(contentType)\n    if match:\n        if (self.declaredHTMLEncoding is not None or\n            self.originalEncoding == self.fromEncoding):\n            # An HTML encoding was sniffed while converting\n            # the document to Unicode, or an HTML encoding was\n            # sniffed during a previous pass through the\n            # document, or an encoding was specified\n            # explicitly and it worked. Rewrite the meta tag.\n            def rewrite(match):\n                return match.group(1) + \"%SOUP-ENCODING%\"\n            newAttr = self.CHARSET_RE.sub(rewrite, contentType)\n            attrs[contentTypeIndex] = (attrs[contentTypeIndex][0],\n                                       newAttr)\n            tagNeedsEncodingSubstitution = True\n        else:\n            # This is our first pass through the document.\n            # Go through it again with the encoding information.\n            newCharset = match.group(3)\n            if newCharset and newCharset != self.originalEncoding:\n                self.declaredHTMLEncoding = newCharset\n                self._feed(self.declaredHTMLEncoding)\n                raise StopParsing\n            pass\ntag = self.unknown_starttag(\"meta\", attrs)\nif tag and tagNeedsEncodingSubstitution:\n    tag.containsSubstitutions = True", "path": "demos\\BeautifulSoup.py", "repo_name": "okoye/PyV8", "stars": 55, "license": "None", "language": "python", "size": 5500}
{"docstring": "\"\"\"Destructively rips this element out of the tree.\"\"\"\n", "func_signal": "def extract(self):\n", "code": "if self.parent:\n    try:\n        self.parent.contents.remove(self)\n    except ValueError:\n        pass\n\n#Find the two elements that would be next to each other if\n#this element (and any children) hadn't been parsed. Connect\n#the two.\nlastChild = self._lastRecursiveChild()\nnextElement = lastChild.next\n\nif self.previous:\n    self.previous.next = nextElement\nif nextElement:\n    nextElement.previous = self.previous\nself.previous = None\nlastChild.next = None\n\nself.parent = None\nif self.previousSibling:\n    self.previousSibling.nextSibling = self.nextSibling\nif self.nextSibling:\n    self.nextSibling.previousSibling = self.previousSibling\nself.previousSibling = self.nextSibling = None\nreturn self", "path": "demos\\BeautifulSoup.py", "repo_name": "okoye/PyV8", "stars": 55, "license": "None", "language": "python", "size": 5500}
{"docstring": "\"\"\"Returns the closest sibling to this Tag that matches the\ngiven criteria and appears after this Tag in the document.\"\"\"\n", "func_signal": "def findNextSibling(self, name=None, attrs={}, text=None, **kwargs):\n", "code": "return self._findOne(self.findNextSiblings, name, attrs, text,\n                     **kwargs)", "path": "demos\\BeautifulSoup.py", "repo_name": "okoye/PyV8", "stars": 55, "license": "None", "language": "python", "size": 5500}
{"docstring": "# Create an environment\n", "func_signal": "def _testMultiContext(self):\n", "code": "with JSContext() as ctxt0:\n    ctxt0.securityToken = \"password\"\n\n    global0 = ctxt0.locals\n    global0.custom = 1234\n\n    self.assertEquals(1234, int(global0.custom))\n\n    # Create an independent environment\n    with JSContext() as ctxt1:\n        ctxt1.securityToken = ctxt0.securityToken\n\n        global1 = ctxt1.locals\n        global1.custom = 1234\n\n        with ctxt0:\n            self.assertEquals(1234, int(global0.custom))\n        self.assertEquals(1234, int(global1.custom))\n\n        # Now create a new context with the old global\n        with JSContext(global1) as ctxt2:\n            ctxt2.securityToken = ctxt1.securityToken\n\n            with ctxt1:\n                self.assertEquals(1234, int(global1.custom))\n                \n            self.assertEquals(1234, int(global2.custom))", "path": "PyV8.py", "repo_name": "okoye/PyV8", "stars": 55, "license": "None", "language": "python", "size": 5500}
{"docstring": "\"\"\"Loads a specific URL from the history list\"\"\"\n", "func_signal": "def go(self, num_or_url):\n", "code": "try:\n    off = int(num_or_url)\n\n    self.pos += off\n    self.pos = min(max(0, self.pos), len(self.urls)-1)\n\n    self._win.open(self.urls[self.pos])\nexcept ValueError:\n    self._win.open(num_or_url)", "path": "demos\\browser.py", "repo_name": "okoye/PyV8", "stars": 55, "license": "None", "language": "python", "size": 5500}
{"docstring": "'''Returns a single status message.\n\nThe twitter.Api instance must be authenticated if the status message is private.\n\nArgs:\n  id: The numerical ID of the status you're trying to retrieve.\n\nReturns:\n  A twitter.Status instance representing that status message\n'''\n", "func_signal": "def GetStatus(self, id):\n", "code": "try:\n  if id:\n    int(id)\nexcept:\n  raise TwitterError(\"id must be an integer\")\nurl = 'http://twitter.com/statuses/show/%s.json' % id\njson = self._FetchUrl(url)\ndata = simplejson.loads(json)\nreturn Status.NewFromJsonDict(data)", "path": "twitter.py", "repo_name": "adafruit/Tweet-a-Watt", "stars": 32, "license": "None", "language": "python", "size": 105}
{"docstring": "'''Fetch the sequence of twitter.User instances featured on twitter.com\n\nThe twitter.Api instance must be authenticated.\n\nReturns:\n  A sequence of twitter.User instances\n'''\n", "func_signal": "def GetFeatured(self):\n", "code": "url = 'http://twitter.com/statuses/featured.json'\njson = self._FetchUrl(url)\ndata = simplejson.loads(json)\nreturn [User.NewFromJsonDict(x) for x in data]", "path": "twitter.py", "repo_name": "adafruit/Tweet-a-Watt", "stars": 32, "license": "None", "language": "python", "size": 105}
{"docstring": "'''Get a sequence of status messages representing the 20 most recent\nreplies (status updates prefixed with @username) to the authenticating\nuser.\n\nReturns:\n  A sequence of twitter.Status instances, one for each reply to the user.\n'''\n", "func_signal": "def GetReplies(self):\n", "code": "url = 'http://twitter.com/statuses/replies.json'\nif not self._username:\n  raise TwitterError(\"The twitter.Api instance must be authenticated.\")\njson = self._FetchUrl(url)\ndata = simplejson.loads(json)\nreturn [Status.NewFromJsonDict(x) for x in data]", "path": "twitter.py", "repo_name": "adafruit/Tweet-a-Watt", "stars": 32, "license": "None", "language": "python", "size": 105}
{"docstring": "'''Create a new instance based on a JSON dict.\n\nArgs:\n  data: A JSON dict, as converted from the JSON in the twitter API\nReturns:\n  A twitter.DirectMessage instance\n'''\n", "func_signal": "def NewFromJsonDict(data):\n", "code": "return DirectMessage(created_at=data.get('created_at', None),\n                     recipient_id=data.get('recipient_id', None),\n                     sender_id=data.get('sender_id', None),\n                     text=data.get('text', None),\n                     sender_screen_name=data.get('sender_screen_name', None),\n                     id=data.get('id', None),\n                     recipient_screen_name=data.get('recipient_screen_name', None))", "path": "twitter.py", "repo_name": "adafruit/Tweet-a-Watt", "stars": 32, "license": "None", "language": "python", "size": 105}
{"docstring": "# Break url into consituent parts\n", "func_signal": "def _BuildUrl(self, url, path_elements=None, extra_params=None):\n", "code": "(scheme, netloc, path, params, query, fragment) = urlparse.urlparse(url)\n\n# Add any additional path elements to the path\nif path_elements:\n  # Filter out the path elements that have a value of None\n  p = [i for i in path_elements if i]\n  if not path.endswith('/'):\n    path += '/'\n  path += '/'.join(p)\n\n# Add any additional query parameters to the query string\nif extra_params and len(extra_params) > 0:\n  extra_query = self._EncodeParameters(extra_params)\n  # Add it to the existing query\n  if query:\n    query += '&' + extra_query\n  else:\n    query = extra_query\n\n# Return the rebuilt URL\nreturn urlparse.urlunparse((scheme, netloc, path, params, query, fragment))", "path": "twitter.py", "repo_name": "adafruit/Tweet-a-Watt", "stars": 32, "license": "None", "language": "python", "size": 105}
{"docstring": "'''Fetch the sequnce of public twitter.Status message for all users.\n\nArgs:\n  since_id:\n    Returns only public statuses with an ID greater than (that is,\n    more recent than) the specified ID. [Optional]\n\nReturns:\n  An sequence of twitter.Status instances, one for each message\n'''\n", "func_signal": "def GetPublicTimeline(self, since_id=None):\n", "code": "parameters = {}\nif since_id:\n  parameters['since_id'] = since_id\nurl = 'http://twitter.com/statuses/public_timeline.json'\njson = self._FetchUrl(url,  parameters=parameters)\ndata = simplejson.loads(json)\nreturn [Status.NewFromJsonDict(x) for x in data]", "path": "twitter.py", "repo_name": "adafruit/Tweet-a-Watt", "stars": 32, "license": "None", "language": "python", "size": 105}
{"docstring": "'''Clear the username and password for this instance\n'''\n", "func_signal": "def ClearCredentials(self):\n", "code": "self._username = None\nself._password = None", "path": "twitter.py", "repo_name": "adafruit/Tweet-a-Watt", "stars": 32, "license": "None", "language": "python", "size": 105}
{"docstring": "'''Destroys the status specified by the required ID parameter.\n\nThe twitter.Api instance must be authenticated and thee\nauthenticating user must be the author of the specified status.\n\nArgs:\n  id: The numerical ID of the status you're trying to destroy.\n\nReturns:\n  A twitter.Status instance representing the destroyed status message\n'''\n", "func_signal": "def DestroyStatus(self, id):\n", "code": "try:\n  if id:\n    int(id)\nexcept:\n  raise TwitterError(\"id must be an integer\")\nurl = 'http://twitter.com/statuses/destroy/%s.json' % id\njson = self._FetchUrl(url, post_data={})\ndata = simplejson.loads(json)\nreturn Status.NewFromJsonDict(data)", "path": "twitter.py", "repo_name": "adafruit/Tweet-a-Watt", "stars": 32, "license": "None", "language": "python", "size": 105}
{"docstring": "'''Return a string in key=value&key=value form\n\nValues are assumed to be encoded in the format specified by self._encoding,\nand are subsequently URL encoded.\n\nArgs:\n  post_data:\n    A dict of (key, value) tuples, where value is encoded as\n    specified by self._encoding\nReturns:\n  A URL-encoded string in \"key=value&key=value\" form\n'''\n", "func_signal": "def _EncodePostData(self, post_data):\n", "code": "if post_data is None:\n  return None\nelse:\n  return urllib.urlencode(dict([(k, self._Encode(v)) for k, v in post_data.items()]))", "path": "twitter.py", "repo_name": "adafruit/Tweet-a-Watt", "stars": 32, "license": "None", "language": "python", "size": 105}
{"docstring": "'''Attempt to find the username in a cross-platform fashion.'''\n", "func_signal": "def _GetUsername(self):\n", "code": "return os.getenv('USER') or \\\n    os.getenv('LOGNAME') or \\\n    os.getenv('USERNAME') or \\\n    os.getlogin() or \\\n    'nobody'", "path": "twitter.py", "repo_name": "adafruit/Tweet-a-Watt", "stars": 32, "license": "None", "language": "python", "size": 105}
{"docstring": "'''Create a new instance based on a JSON dict.\n\nArgs:\n  data: A JSON dict, as converted from the JSON in the twitter API\nReturns:\n  A twitter.User instance\n'''\n", "func_signal": "def NewFromJsonDict(data):\n", "code": "if 'status' in data:\n  status = Status.NewFromJsonDict(data['status'])\nelse:\n  status = None\nreturn User(id=data.get('id', None),\n            name=data.get('name', None),\n            screen_name=data.get('screen_name', None),\n            location=data.get('location', None),\n            description=data.get('description', None),\n            profile_image_url=data.get('profile_image_url', None),\n            url=data.get('url', None),\n            status=status)", "path": "twitter.py", "repo_name": "adafruit/Tweet-a-Watt", "stars": 32, "license": "None", "language": "python", "size": 105}
{"docstring": "'''Returns a list of the direct messages sent to the authenticating user.\n\nThe twitter.Api instance must be authenticated.\n\nArgs:\n  since:\n    Narrows the returned results to just those statuses created\n    after the specified HTTP-formatted date. [optional]\n\nReturns:\n  A sequence of twitter.DirectMessage instances\n'''\n", "func_signal": "def GetDirectMessages(self, since=None):\n", "code": "url = 'http://twitter.com/direct_messages.json'\nif not self._username:\n  raise TwitterError(\"The twitter.Api instance must be authenticated.\")\nparameters = {}\nif since:\n  parameters['since'] = since\njson = self._FetchUrl(url, parameters=parameters)\ndata = simplejson.loads(json)\nreturn [DirectMessage.NewFromJsonDict(x) for x in data]", "path": "twitter.py", "repo_name": "adafruit/Tweet-a-Watt", "stars": 32, "license": "None", "language": "python", "size": 105}
{"docstring": "'''Get the wallclock time for this status message.\n\nUsed to calculate relative_created_at.  Defaults to the time\nthe object was instantiated.\n\nReturns:\n  Whatever the status instance believes the current time to be,\n  in seconds since the epoch.\n'''\n", "func_signal": "def GetNow(self):\n", "code": "if self._now is None:\n  self._now = time.mktime(time.gmtime())\nreturn self._now", "path": "twitter.py", "repo_name": "adafruit/Tweet-a-Watt", "stars": 32, "license": "None", "language": "python", "size": 105}
{"docstring": "'''Post a twitter status message from the authenticated user.\n\nThe twitter.Api instance must be authenticated.\n\nArgs:\n  text: The message text to be posted.  Must be less than 140 characters.\n\nReturns:\n  A twitter.Status instance representing the message posted\n'''\n", "func_signal": "def PostUpdate(self, text):\n", "code": "if not self._username:\n  raise TwitterError(\"The twitter.Api instance must be authenticated.\")\nif len(text) > 140:\n  raise TwitterError(\"Text must be less than or equal to 140 characters.\")\nurl = 'http://twitter.com/statuses/update.json'\ndata = {'status': text}\njson = self._FetchUrl(url, post_data=data)\ndata = simplejson.loads(json)\nreturn Status.NewFromJsonDict(data)", "path": "twitter.py", "repo_name": "adafruit/Tweet-a-Watt", "stars": 32, "license": "None", "language": "python", "size": 105}
{"docstring": "'''Befriends the user specified in the user parameter as the authenticating user.\n\nThe twitter.Api instance must be authenticated.\n\nArgs:\n  The ID or screen name of the user to befriend.\nReturns:\n  A twitter.User instance representing the befriended user.\n'''\n", "func_signal": "def CreateFriendship(self, user):\n", "code": "url = 'http://twitter.com/friendships/create/%s.json' % user\njson = self._FetchUrl(url, post_data={})\ndata = simplejson.loads(json)\nreturn User.NewFromJsonDict(data)", "path": "twitter.py", "repo_name": "adafruit/Tweet-a-Watt", "stars": 32, "license": "None", "language": "python", "size": 105}
{"docstring": "'''A dict representation of this twitter.DirectMessage instance.\n\nThe return value uses the same key names as the JSON representation.\n\nReturn:\n  A dict representing this twitter.DirectMessage instance\n'''\n", "func_signal": "def AsDict(self):\n", "code": "data = {}\nif self.id:\n  data['id'] = self.id\nif self.created_at:\n  data['created_at'] = self.created_at\nif self.sender_id:\n  data['sender_id'] = self.sender_id\nif self.sender_screen_name:\n  data['sender_screen_name'] = self.sender_screen_name\nif self.recipient_id:\n  data['recipient_id'] = self.recipient_id\nif self.recipient_screen_name:\n  data['recipient_screen_name'] = self.recipient_screen_name\nif self.text:\n  data['text'] = self.text\nreturn data", "path": "twitter.py", "repo_name": "adafruit/Tweet-a-Watt", "stars": 32, "license": "None", "language": "python", "size": 105}
{"docstring": "'''Fetch the sequence of twitter.User instances, one for each friend.\n\nArgs:\n  user: the username or id of the user whose friends you are fetching.  If\n  not specified, defaults to the authenticated user. [optional]\n\nThe twitter.Api instance must be authenticated.\n\nReturns:\n  A sequence of twitter.User instances, one for each friend\n'''\n", "func_signal": "def GetFriends(self, user=None):\n", "code": "if not self._username:\n  raise TwitterError(\"twitter.Api instance must be authenticated\")\nif user:\n  url = 'http://twitter.com/statuses/friends/%s.json' % user\nelse:\n  url = 'http://twitter.com/statuses/friends.json'\njson = self._FetchUrl(url)\ndata = simplejson.loads(json)\nreturn [User.NewFromJsonDict(x) for x in data]", "path": "twitter.py", "repo_name": "adafruit/Tweet-a-Watt", "stars": 32, "license": "None", "language": "python", "size": 105}
{"docstring": "'''Returns a single user.\n\nThe twitter.Api instance must be authenticated.\n\nArgs:\n  user: The username or id of the user to retrieve.\n\nReturns:\n  A twitter.User instance representing that user\n'''\n", "func_signal": "def GetUser(self, user):\n", "code": "url = 'http://twitter.com/users/show/%s.json' % user\njson = self._FetchUrl(url)\ndata = simplejson.loads(json)\nreturn User.NewFromJsonDict(data)", "path": "twitter.py", "repo_name": "adafruit/Tweet-a-Watt", "stars": 32, "license": "None", "language": "python", "size": 105}
{"docstring": "'''Get a human redable string representing the posting time\n\nReturns:\n  A human readable string representing the posting time\n'''\n", "func_signal": "def GetRelativeCreatedAt(self):\n", "code": "fudge = 1.25\ndelta  = int(self.now) - int(self.created_at_in_seconds)\n\nif delta < (1 * fudge):\n  return 'about a second ago'\nelif delta < (60 * (1/fudge)):\n  return 'about %d seconds ago' % (delta)\nelif delta < (60 * fudge):\n  return 'about a minute ago'\nelif delta < (60 * 60 * (1/fudge)):\n  return 'about %d minutes ago' % (delta / 60)\nelif delta < (60 * 60 * fudge):\n  return 'about an hour ago'\nelif delta < (60 * 60 * 24 * (1/fudge)):\n  return 'about %d hours ago' % (delta / (60 * 60))\nelif delta < (60 * 60 * 24 * fudge):\n  return 'about a day ago'\nelse:\n  return 'about %d days ago' % (delta / (60 * 60 * 24))", "path": "twitter.py", "repo_name": "adafruit/Tweet-a-Watt", "stars": 32, "license": "None", "language": "python", "size": 105}
{"docstring": "'''Set the X-Twitter HTTP headers that will be sent to the server.\n\nArgs:\n  client:\n     The client name as a string.  Will be sent to the server as\n     the 'X-Twitter-Client' header.\n  url:\n     The URL of the meta.xml as a string.  Will be sent to the server\n     as the 'X-Twitter-Client-URL' header.\n  version:\n     The client version as a string.  Will be sent to the server\n     as the 'X-Twitter-Client-Version' header.\n'''\n", "func_signal": "def SetXTwitterHeaders(self, client, url, version):\n", "code": "self._request_headers['X-Twitter-Client'] = client\nself._request_headers['X-Twitter-Client-URL'] = url\nself._request_headers['X-Twitter-Client-Version'] = version", "path": "twitter.py", "repo_name": "adafruit/Tweet-a-Watt", "stars": 32, "license": "None", "language": "python", "size": 105}
{"docstring": "\"\"\"\nTarBlock initializer - just store data\n\"\"\"\n", "func_signal": "def __init__(self, index, data):\n", "code": "self.index = index\nself.data = data", "path": "duplicity\\diffdir.py", "repo_name": "hcarvalhoalves/duplicity", "stars": 50, "license": "gpl-2.0", "language": "python", "size": 1116}
{"docstring": "\"\"\"\nConvert signature tar file object open for reading into path iter\n\"\"\"\n", "func_signal": "def sigtar2path_iter(sigtarobj):\n", "code": "tf = tarfile.TarFile(\"Arbitrary Name\", \"r\", sigtarobj)\ntf.debug = 2\nfor tarinfo in tf:\n    for prefix in [\"signature/\", \"snapshot/\", \"deleted/\"]:\n        if tarinfo.name.startswith(prefix):\n            # strip prefix and from name and set it to difftype\n            name, difftype = tarinfo.name[len(prefix):], prefix[:-1]\n            break\n    else:\n        raise DiffDirException(\"Bad tarinfo name %s\" % (tarinfo.name,))\n\n    index = tuple(name.split(\"/\"))\n    if not index[-1]:\n        index = index[:-1] # deal with trailing /, \"\"\n\n    ropath = ROPath(index)\n    ropath.difftype = difftype\n    if difftype == \"signature\" or difftype == \"snapshot\":\n        ropath.init_from_tarinfo(tarinfo)\n        if ropath.isreg():\n            ropath.setfileobj(tf.extractfile(tarinfo))\n    yield ropath\nsigtarobj.close()", "path": "duplicity\\diffdir.py", "repo_name": "hcarvalhoalves/duplicity", "stars": 50, "license": "gpl-2.0", "language": "python", "size": 1116}
{"docstring": "\"\"\"\nGet more tarblocks\n\nIf processing val above would produce more than one TarBlock,\nget the rest of them by calling process_continue.\n\"\"\"\n", "func_signal": "def process_continued(self, size):\n", "code": "assert self.process_waiting\nXXX # Override in subclass @UndefinedVariable", "path": "duplicity\\diffdir.py", "repo_name": "hcarvalhoalves/duplicity", "stars": 50, "license": "gpl-2.0", "language": "python", "size": 1116}
{"docstring": "\"\"\"\nWrite block_iter to filename, path, or file object\n\"\"\"\n", "func_signal": "def write_block_iter(block_iter, out_obj):\n", "code": "if isinstance(out_obj, Path):\n    fp = open(out_obj.name, \"wb\")\nelif type(out_obj) is types.StringType:\n    fp = open(out_obj, \"wb\")\nelse:\n    fp = out_obj\nfor block in block_iter:\n    fp.write(block.data)\nfp.write(block_iter.get_footer())\nassert not fp.close()\nif isinstance(out_obj, Path):\n    out_obj.setdata()", "path": "duplicity\\diffdir.py", "repo_name": "hcarvalhoalves/duplicity", "stars": 50, "license": "gpl-2.0", "language": "python", "size": 1116}
{"docstring": "\"\"\"\nReturn pair (next data block, boolean last data block)\n\"\"\"\n", "func_signal": "def get_data_block(self, fp, max_size):\n", "code": "read_size = min(64*1024, max(max_size, 512))\nbuf = fp.read(read_size)\nif len(buf) < read_size:\n    if fp.close():\n        raise DiffDirException(\"Error closing file\")\n    return (buf, True)\nelse:\n    return (buf, False)", "path": "duplicity\\diffdir.py", "repo_name": "hcarvalhoalves/duplicity", "stars": 50, "license": "gpl-2.0", "language": "python", "size": 1116}
{"docstring": "\"\"\"\nLook at delta path and log delta.  Add stats if new_path is set\n\"\"\"\n", "func_signal": "def log_delta_path(delta_path, new_path = None, stats = None):\n", "code": "if delta_path.difftype == \"snapshot\":\n    if new_path and stats:\n        stats.add_new_file(new_path)\n    log.Info(_(\"A %s\") %\n             (delta_path.get_relative_path(),),\n             log.InfoCode.diff_file_new,\n             util.escape(delta_path.get_relative_path()))\nelse:\n    if new_path and stats:\n        stats.add_changed_file(new_path)\n    log.Info(_(\"M %s\") %\n             (delta_path.get_relative_path(),),\n             log.InfoCode.diff_file_changed,\n             util.escape(delta_path.get_relative_path()))", "path": "duplicity\\diffdir.py", "repo_name": "hcarvalhoalves/duplicity", "stars": 50, "license": "gpl-2.0", "language": "python", "size": 1116}
{"docstring": "\"\"\"\nReturn next block, no bigger than size, and update offset\n\"\"\"\n", "func_signal": "def next(self, size = 1024 * 1024):\n", "code": "if self.process_waiting:\n    result = self.process_continued(size)\nelse:\n    # Below a StopIteration exception will just be passed upwards\n    result = self.process(self.input_iter.next(), size)\nblock_number = self.process_next_vol_number\nself.offset += len(result.data)\nself.previous_index = result.index\nself.previous_block = block_number\nif self.remember_next:\n    self.remember_value = result.index\n    self.remember_block = block_number\n    self.remember_next = False\nreturn result", "path": "duplicity\\diffdir.py", "repo_name": "hcarvalhoalves/duplicity", "stars": 50, "license": "gpl-2.0", "language": "python", "size": 1116}
{"docstring": "\"\"\"\nCollate two iterators.\n\nThe elements yielded by each iterator must be have an index\nvariable, and this function returns pairs (elem1, elem2), (elem1,\nNone), or (None, elem2) two elements in a pair will have the same\nindex, and earlier indicies are yielded later than later indicies.\n\"\"\"\n", "func_signal": "def collate2iters(riter1, riter2):\n", "code": "relem1, relem2 = None, None\nwhile 1:\n    if not relem1:\n        try:\n            relem1 = riter1.next()\n        except StopIteration:\n            if relem2:\n                yield (None, relem2)\n            for relem2 in riter2:\n                yield (None, relem2)\n            break\n        index1 = relem1.index\n    if not relem2:\n        try:\n            relem2 = riter2.next()\n        except StopIteration:\n            if relem1:\n                yield (relem1, None)\n            for relem1 in riter1:\n                yield (relem1, None)\n            break\n        index2 = relem2.index\n\n    if index1 < index2:\n        yield (relem1, None)\n        relem1 = None\n    elif index1 == index2:\n        yield (relem1, relem2)\n        relem1, relem2 = None, None\n    else:\n        # index2 is less\n        yield (None, relem2)\n        relem2 = None", "path": "duplicity\\diffdir.py", "repo_name": "hcarvalhoalves/duplicity", "stars": 50, "license": "gpl-2.0", "language": "python", "size": 1116}
{"docstring": "\"\"\"Delete all files in filename list\"\"\"\n", "func_signal": "def delete(self, filename_list, raise_errors=False):\n", "code": "import types\nimport ubuntuone.couch.auth as auth\nassert type(filename_list) is not types.StringType\nfor filename in filename_list:\n    remote_full = self.meta_base + self.quote(filename)", "path": "duplicity\\backends\\u1backend.py", "repo_name": "hcarvalhoalves/duplicity", "stars": 50, "license": "gpl-2.0", "language": "python", "size": 1116}
{"docstring": "\"\"\"\nCalled by get_delta_iter, report error in getting delta\n\"\"\"\n", "func_signal": "def delta_iter_error_handler(exc, new_path, sig_path, sig_tar = None):\n", "code": "if new_path:\n    index_string = new_path.get_relative_path()\nelif sig_path:\n    index_string = sig_path.get_relative_path()\nelse:\n    assert 0, \"Both new and sig are None for some reason\"\nlog.Warn(_(\"Error %s getting delta for %s\") % (str(exc), index_string))\nreturn None", "path": "duplicity\\diffdir.py", "repo_name": "hcarvalhoalves/duplicity", "stars": 50, "license": "gpl-2.0", "language": "python", "size": 1116}
{"docstring": "\"\"\"\nTurn next value of input_iter into a TarBlock\n\"\"\"\n", "func_signal": "def process(self, val, size):\n", "code": "assert not self.process_waiting\nXXX # Override in subclass @UndefinedVariable", "path": "duplicity\\diffdir.py", "repo_name": "hcarvalhoalves/duplicity", "stars": 50, "license": "gpl-2.0", "language": "python", "size": 1116}
{"docstring": "\"\"\"\nProduce new iterator by combining the iterators in path_iter_list\n\nThis new iter will iterate every path that is in path_iter_list in\norder of increasing index.  If multiple iterators in\npath_iter_list yield paths with the same index, combine_path_iters\nwill discard all paths but the one yielded by the last path_iter.\n\nThis is used to combine signature iters, as the output will be a\nfull up-to-date signature iter.\n\"\"\"\n", "func_signal": "def combine_path_iters(path_iter_list):\n", "code": "path_iter_list = path_iter_list[:] # copy before destructive reverse\npath_iter_list.reverse()\n\ndef get_triple(iter_index):\n    \"\"\"\n    Represent the next element as a triple, to help sorting\n    \"\"\"\n    try:\n        path = path_iter_list[iter_index].next()\n    except StopIteration:\n        return None\n    return (path.index, iter_index, path)\n\ndef refresh_triple_list(triple_list):\n    \"\"\"\n    Update all elements with path_index same as first element\n    \"\"\"\n    path_index = triple_list[0][0]\n    iter_index = 0\n    while iter_index < len(triple_list):\n        old_triple = triple_list[iter_index]\n        if old_triple[0] == path_index:\n            new_triple = get_triple(old_triple[1])\n            if new_triple:\n                triple_list[iter_index] = new_triple\n                iter_index += 1\n            else:\n                del triple_list[iter_index]\n        else:\n            break # assumed triple_list sorted, so can exit now\n\ntriple_list = filter(lambda x: x, map(get_triple,\n                                      range(len(path_iter_list))))\nwhile triple_list:\n    triple_list.sort()\n    yield triple_list[0][2]\n    refresh_triple_list(triple_list)", "path": "duplicity\\diffdir.py", "repo_name": "hcarvalhoalves/duplicity", "stars": 50, "license": "gpl-2.0", "language": "python", "size": 1116}
{"docstring": "\"\"\"\nGet a fake tarblock from delta_ropath\n\"\"\"\n", "func_signal": "def process(self, delta_ropath, size):\n", "code": "ti = delta_ropath.get_tarinfo()\nindex = delta_ropath.index\n\n# Return blocks of deleted files or fileless snapshots\nif not delta_ropath.type or not delta_ropath.fileobj:\n    return self.tarinfo2tarblock(index, ti)\n\nif stats:\n    # Since we don't read the source files, we can't analyze them.\n    # Best we can do is count them raw.\n    stats.SourceFiles += 1\n    stats.SourceFileSize += delta_ropath.getsize()\n    log.Progress(None, stats.SourceFileSize)\nreturn self.tarinfo2tarblock(index, ti)", "path": "duplicity\\diffdir.py", "repo_name": "hcarvalhoalves/duplicity", "stars": 50, "license": "gpl-2.0", "language": "python", "size": 1116}
{"docstring": "\"\"\"Run duplicity backup to default directory\"\"\"\n", "func_signal": "def backup(self, type, input_dir, options = []):\n", "code": "options = options[:]\nif type == \"full\":\n    options.insert(0, 'full')\nargs = [input_dir, \"'%s'\" % backend_url]\nself.run_duplicity(args, options)", "path": "testing\\badupload.py", "repo_name": "hcarvalhoalves/duplicity", "stars": 50, "license": "gpl-2.0", "language": "python", "size": 1116}
{"docstring": "\"\"\"\nReturn new delta_path which, when read, writes sig to sig_fileobj,\nif sigTarFile is not None\n\"\"\"\n", "func_signal": "def get_delta_path(new_path, sig_path, sigTarFile = None):\n", "code": "assert new_path\nif sigTarFile:\n    ti = new_path.get_tarinfo()\n    index = new_path.index\ndelta_path = new_path.get_ropath()\nlog.Debug(_(\"Getting delta of %s and %s\") % (new_path, sig_path))\n\ndef callback(sig_string):\n    \"\"\"\n    Callback activated when FileWithSignature read to end\n    \"\"\"\n    ti.size = len(sig_string)\n    ti.name = \"signature/\" + \"/\".join(index)\n    sigTarFile.addfile(ti, cStringIO.StringIO(sig_string))\n\nif new_path.isreg() and sig_path and sig_path.difftype == \"signature\":\n    delta_path.difftype = \"diff\"\n    old_sigfp = sig_path.open(\"rb\")\n    newfp = FileWithReadCounter(new_path.open(\"rb\"))\n    if sigTarFile:\n        newfp = FileWithSignature(newfp, callback,\n                                  new_path.getsize())\n    delta_path.setfileobj(librsync.DeltaFile(old_sigfp, newfp))\nelse:\n    delta_path.difftype = \"snapshot\"\n    if sigTarFile:\n        ti.name = \"snapshot/\" + \"/\".join(index)\n    if not new_path.isreg():\n        if sigTarFile:\n            sigTarFile.addfile(ti)\n        if stats:\n            stats.SourceFileSize += delta_path.getsize()\n    else:\n        newfp = FileWithReadCounter(new_path.open(\"rb\"))\n        if sigTarFile:\n            newfp = FileWithSignature(newfp, callback,\n                                      new_path.getsize())\n        delta_path.setfileobj(newfp)\nnew_path.copy_attribs(delta_path)\ndelta_path.stat.st_size = new_path.stat.st_size\nreturn delta_path", "path": "duplicity\\diffdir.py", "repo_name": "hcarvalhoalves/duplicity", "stars": 50, "license": "gpl-2.0", "language": "python", "size": 1116}
{"docstring": "# GIO requires a dbus session bus which can start the gvfs daemons\n# when required.  So we make sure that such a bus exists and that our\n# environment points to it.\n", "func_signal": "def ensure_dbus():\n", "code": "import atexit\nimport os\nimport subprocess\nimport signal\nif 'DBUS_SESSION_BUS_ADDRESS' not in os.environ:\n    output = subprocess.Popen(['dbus-launch'], stdout=subprocess.PIPE).communicate()[0]\n    lines = output.split('\\n')\n    for line in lines:\n        parts = line.split('=', 1)\n        if len(parts) == 2:\n            if parts[0] == 'DBUS_SESSION_BUS_PID': # cleanup at end\n                atexit.register(os.kill, int(parts[1]), signal.SIGTERM)\n            os.environ[parts[0]] = parts[1]", "path": "duplicity\\backends\\u1backend.py", "repo_name": "hcarvalhoalves/duplicity", "stars": 50, "license": "gpl-2.0", "language": "python", "size": 1116}
{"docstring": "\"\"\"\nReturn associated signature TarBlock from path\n\nHere size is just ignored --- let's hope a signature isn't too\nbig.  Also signatures are stored in multiple volumes so it\ndoesn't matter.\n\"\"\"\n", "func_signal": "def process(self, path, size):\n", "code": "ti = path.get_tarinfo()\nif path.isreg():\n    sfp = librsync.SigFile(path.open(\"rb\"),\n                           get_block_size(path.getsize()))\n    sigbuf = sfp.read()\n    sfp.close()\n    ti.name = \"signature/\" + \"/\".join(path.index)\n    return self.tarinfo2tarblock(path.index, ti, sigbuf)\nelse:\n    ti.name = \"snapshot/\" + \"/\".join(path.index)\n    return self.tarinfo2tarblock(path.index, ti)", "path": "duplicity\\diffdir.py", "repo_name": "hcarvalhoalves/duplicity", "stars": 50, "license": "gpl-2.0", "language": "python", "size": 1116}
{"docstring": "\"\"\"\nProduce tarblock diff given dirsig_fileobj_list and pathiter\n\ndirsig_fileobj_list should either be a tar fileobj or a list of\nthose, sorted so the most recent is last.\n\"\"\"\n", "func_signal": "def DirDelta(path_iter, dirsig_fileobj_list):\n", "code": "global stats\nstats = statistics.StatsDeltaProcess()\nif type(dirsig_fileobj_list) is types.ListType:\n    sig_iter = combine_path_iters(map(sigtar2path_iter,\n                                      dirsig_fileobj_list))\nelse:\n    sig_iter = sigtar2path_iter(dirsig_fileobj_list)\ndelta_iter = get_delta_iter(path_iter, sig_iter)\nif globals.dry_run:\n    return DummyBlockIter(delta_iter)\nelse:\n    return DeltaTarBlockIter(delta_iter)", "path": "duplicity\\diffdir.py", "repo_name": "hcarvalhoalves/duplicity", "stars": 50, "license": "gpl-2.0", "language": "python", "size": 1116}
{"docstring": "\"\"\"\nGenerate delta iter from new Path iter and sig Path iter.\n\nFor each delta path of regular file type, path.difftype with be\nset to \"snapshot\", \"diff\".  sig_iter will probably iterate ROPaths\ninstead of Paths.\n\nIf sig_fileobj is not None, will also write signatures to sig_fileobj.\n\"\"\"\n", "func_signal": "def get_delta_iter(new_iter, sig_iter, sig_fileobj=None):\n", "code": "collated = collate2iters(new_iter, sig_iter)\nif sig_fileobj:\n    sigTarFile = tarfile.TarFile(\"arbitrary\", \"w\", sig_fileobj)\nelse:\n    sigTarFile = None\nfor new_path, sig_path in collated:\n    log.Debug(_(\"Comparing %s and %s\") % (new_path and new_path.index,\n                                          sig_path and sig_path.index))\n    if not new_path or not new_path.type:\n        # file doesn't exist\n        if sig_path and sig_path.exists():\n            # but signature says it did\n            log.Info(_(\"D %s\") %\n                     (sig_path.get_relative_path(),),\n                     log.InfoCode.diff_file_deleted,\n                     util.escape(sig_path.get_relative_path()))\n            if sigTarFile:\n                ti = ROPath(sig_path.index).get_tarinfo()\n                ti.name = \"deleted/\" + \"/\".join(sig_path.index)\n                sigTarFile.addfile(ti)\n            stats.add_deleted_file()\n            yield ROPath(sig_path.index)\n    elif not sig_path or new_path != sig_path:\n        # Must calculate new signature and create delta\n        delta_path = robust.check_common_error(delta_iter_error_handler,\n                                               get_delta_path,\n                                               (new_path, sig_path, sigTarFile))\n        if delta_path:\n            # log and collect stats\n            log_delta_path(delta_path, new_path, stats)\n            yield delta_path\n        else:\n            # if not, an error must have occurred\n            stats.Errors += 1\n    else:\n        stats.add_unchanged_file(new_path)\nstats.close()\nif sigTarFile:\n    sigTarFile.close()", "path": "duplicity\\diffdir.py", "repo_name": "hcarvalhoalves/duplicity", "stars": 50, "license": "gpl-2.0", "language": "python", "size": 1116}
{"docstring": "\"\"\"Get file and put in local_path (Path object)\"\"\"\n", "func_signal": "def get(self, filename, local_path, raise_errors=False):\n", "code": "import json\nimport ubuntuone.couch.auth as auth\nremote_full = self.meta_base + self.quote(filename)\nanswer = auth.request(remote_full)\nself.handle_error(raise_errors, 'get', answer, remote_full, filename)\nnode = json.loads(answer[1])\n\nremote_full = self.content_base + self.quote(node.get('content_path'))\nanswer = auth.request(remote_full)\nself.handle_error(raise_errors, 'get', answer, remote_full, filename)\nf = open(local_path.name, 'wb')\nf.write(answer[1])\nlocal_path.setdata()", "path": "duplicity\\backends\\u1backend.py", "repo_name": "hcarvalhoalves/duplicity", "stars": 50, "license": "gpl-2.0", "language": "python", "size": 1116}
{"docstring": "\"\"\"\\\nUpdate the set with the intersection of itself and other iterables.\n\"\"\"\n", "func_signal": "def intersection_update(self, *args):\n", "code": "common = set()\ncommon.update(args)\nfor item in self.keys():\n    if item not in common:\n        self.remove(item)", "path": "fuzz\\iset.py", "repo_name": "ezod/fuzzpy", "stars": 36, "license": "other", "language": "python", "size": 437}
{"docstring": "\"\"\"\\\nReturn the difference of the set with other iterables.\n\"\"\"\n", "func_signal": "def difference(self, *args):\n", "code": "result = self.copy()\nresult.difference_update(*args)\nreturn result", "path": "fuzz\\iset.py", "repo_name": "ezod/fuzzpy", "stars": 36, "license": "other", "language": "python", "size": 437}
{"docstring": "\"\"\"\\\nReturn a fuzzy set of edges with tail and/or head optionally\nspecified.\n\n@param tail: The tail vertex constraint (optional).\n@type tail: C{object}\n@param head: The head vertex constraint (optional).\n@type head: C{object}\n@return: The fuzzy set of edges specified.\n@rtype: L{FuzzySet}\n\"\"\"\n", "func_signal": "def edges(self, tail=None, head=None):\n", "code": "if (tail is not None and not tail in self.vertices()) \\\nor (head is not None and not head in self.vertices()):\n    raise KeyError('specified tail/head must be in vertex set')\neset = set([edge.index for edge in self._E \\\n    if (tail is None or edge.index.tail == tail) \\\n    and (head is None or edge.index.head == head)])\nif not self.directed:\n    eset |= set([edge.index for edge in self._E \\\n        if (tail is None or edge.index.head == tail) \\\n        and (head is None or edge.index.tail == head)])\nreturn eset", "path": "fuzz\\fgraph.py", "repo_name": "ezod/fuzzpy", "stars": 36, "license": "other", "language": "python", "size": 437}
{"docstring": "\"\"\"\\\nReturn the weight of an edge. Returns the inverse of the membership\ndegree for a fuzzy graph.\n\n@param tail: The tail vertex.\n@type tail: C{object}\n@param head: The head vertex.\n@type head: C{object}\n@return: The weight of the edge from tail to head.\n@rtype: C{float}\n\"\"\"\n", "func_signal": "def weight(self, tail, head):\n", "code": "if tail == head:\n    return 0.0\ntry:\n    return 1.0 / self.mu(tail, head)\nexcept ZeroDivisionError:\n    return float('inf')", "path": "fuzz\\fgraph.py", "repo_name": "ezod/fuzzpy", "stars": 36, "license": "other", "language": "python", "size": 437}
{"docstring": "\"\"\"\\\nAssign an item by key. Normally, new items are added via add() and\nexisting items modified via object reference; this is included for\ncompleteness.\n\n@param key: The index of the item to assign.\n@type key: C{object}\n@param item: The item to assign.\n@type item: C{object}\n\"\"\"\n", "func_signal": "def __setitem__(self, key, item):\n", "code": "if not item.index == key:\n    raise ValueError('key does not match item index attribute')\nif key in self:\n    self.remove(key)\nset.add(self, item)", "path": "fuzz\\iset.py", "repo_name": "ezod/fuzzpy", "stars": 36, "license": "other", "language": "python", "size": 437}
{"docstring": "\"\"\"\\\nAdd an item to the set. Uses a copy since IndexedMembers have mutable\nproperties.\n\n@param item: The item to add.\n@type item: L{IndexedMember}\n\"\"\"\n", "func_signal": "def add(self, item, *args, **kwargs):\n", "code": "if not isinstance(item, self._itemcls):\n    item = self._itemcls(item, *args, **kwargs)\nset.add(self, copy(item))", "path": "fuzz\\iset.py", "repo_name": "ezod/fuzzpy", "stars": 36, "license": "other", "language": "python", "size": 437}
{"docstring": "\"\"\"\\\nNormalize the fuzzy graph by normalizing its vertex and edge sets.\n\"\"\"\n", "func_signal": "def normalize(self):\n", "code": "self._V.normalize()\nself._E.normalize()", "path": "fuzz\\fgraph.py", "repo_name": "ezod/fuzzpy", "stars": 36, "license": "other", "language": "python", "size": 437}
{"docstring": "\"\"\"\\\nCheck that the other argument to a binary operation is also a fuzzy\ngraph, raising a TypeError otherwise.\n\n@param other: The other argument.\n@type other: L{FuzzyGraph}\n\"\"\"\n", "func_signal": "def _binary_sanity_check(other):\n", "code": "if not isinstance(other, FuzzyGraph):\n    raise TypeError('operation only permitted between fuzzy graphs')", "path": "fuzz\\fgraph.py", "repo_name": "ezod/fuzzpy", "stars": 36, "license": "other", "language": "python", "size": 437}
{"docstring": "\"\"\"\\\nConstructor.\n\n@param index: The index object (immutable).\n@type index: C{object}\n\"\"\"\n", "func_signal": "def __init__(self, index):\n", "code": "if not hasattr(type(index), '__hash__') \\\nor not hasattr(type(index), '__eq__'):\n    raise TypeError('index object must be immutable')\nself._index = index", "path": "fuzz\\iset.py", "repo_name": "ezod/fuzzpy", "stars": 36, "license": "other", "language": "python", "size": 437}
{"docstring": "\"\"\"\\\nConstruct a fuzzy graph from optional iterables.\n\n@param viter: The iterable for the vertex set (optional).\n@type viter: C{object}\n@param eiter: The iterable for the edge set (optional).\n@type eiter: C{object}\n@param directed: Defines the graph as directed or undirected.\n@type directed: C{bool}\n\"\"\"\n", "func_signal": "def __init__(self, viter=None, eiter=None, directed=True):\n", "code": "super(FuzzyGraph, self).__init__(viter=viter, eiter=eiter,\n    directed=directed)", "path": "fuzz\\fgraph.py", "repo_name": "ezod/fuzzpy", "stars": 36, "license": "other", "language": "python", "size": 437}
{"docstring": "\"\"\"\\\nVisualization plugin factory method.\n\nReturns a new instance of the appropriate visualization plugin.\nIf no 'plugin' argument is specified as the preferred visualization\nbackend, the first plugin that supports visualization for 'obj's\nclass name will be used as the backend.\n\n@param obj: Object to draw\n@type obj: Object\n@param plugin: Name of the plugin to use\n@type plugin: C{str}\n@returns: The return value of the plugin's visualize() method\n@rtype: C{tuple} (format, payload)\n\"\"\"\n\n# Pick a supported plugin if none is specified\n", "func_signal": "def create_backend(obj, plugin=None, *args, **kwargs):\n", "code": "if None == plugin:\n    try:\n        plugin = VisManager.get_supported_plugins(obj.__class__)[0]\n    except IndexError:\n        raise ImportError((\"Unable to load any plugin to handle the \"\n                           \"specified object type\"))\n\nplugin_mod = __import__(\"visplugins.%s\" % plugin, globals(), locals(),\n        fromlist=[plugin])\n\n# Extract plugin class name\nplugin_name = getattr(plugin_mod, 'VIS_PLUGIN')\n\nreturn getattr(plugin_mod, plugin_name)\\\n    (obj=obj, args=args, kwargs=kwargs)", "path": "fuzz\\visualization.py", "repo_name": "ezod/fuzzpy", "stars": 36, "license": "other", "language": "python", "size": 437}
{"docstring": "\"\"\"\\\nConstructor.\n\n@param iterable: The iterable to intialize the set with.\n@type iterable: C{iterable}\n\"\"\"\n", "func_signal": "def __init__(self, iterable=set()):\n", "code": "super(IndexedSet, self).__init__()\nfor item in iterable:\n    self.add(item)", "path": "fuzz\\iset.py", "repo_name": "ezod/fuzzpy", "stars": 36, "license": "other", "language": "python", "size": 437}
{"docstring": "\"\"\"\\\nReturns a list of plugins supported by the current system.\n\nIf L{datatype} is specified, try to find supported plugins that can\nbe used to represent the specified datatype. Otherwise, return a list\nof all plugins that can run on that system.\n\nIf any type of exception is raised during the C{is_supported()} call,\nthe plugin will B{not} be included in the resulting list.\n\n@param datatype: fuzzpy datatype to look for supported plugins.\n@type datatype: C{str}\n@rtype: C{list}\n@return: list of plugins that can run in the current environment\n\"\"\"\n", "func_signal": "def get_supported_plugins(datatype=None):\n", "code": "supported = []\n\nfor plugin in visplugins.__all__:\n    # Try to import the plugin\n    try:\n        plugin_mod = __import__(\"visplugins.%s\" % plugin,\n                globals(), locals(), fromlist=[plugin])\n    except ImportError as ex:\n        warnings.warn(ex)\n        continue\n        \n    # Extract plugin class name (or raise AttributeError)\n    if not getattr(plugin_mod, 'VIS_PLUGIN'):\n        raise AttributeError((\"Plugin %s is missing VIS_PLUGIN \"\n                              \"property\") % plugin)\n    plugin_class = getattr(plugin_mod, plugin_mod.VIS_PLUGIN)\n    \n    if (getattr(plugin_class, 'is_supported')() == True) and \\\n        (datatype in [None] + getattr(plugin_mod, 'VIS_TYPES')):\n        supported.append(plugin)\n\nreturn supported", "path": "fuzz\\visualization.py", "repo_name": "ezod/fuzzpy", "stars": 36, "license": "other", "language": "python", "size": 437}
{"docstring": "\"\"\"\\\nAdd an edge to the fuzzy graph.\n\n@param edge: The edge to add.\n@type edge: L{FuzzyElement} of L{GraphEdge}\n@param mu: The membership degree of the edge (optional).\n@type mu: C{float}\n\"\"\"\n", "func_signal": "def add_edge(self, edge, mu=1.0):\n", "code": "if not isinstance(edge, FuzzyElement):\n    edge = FuzzyElement(edge, mu)\ntry:\n    if not isinstance(edge.index, GraphEdge):\n        raise TypeError('edge must be a GraphEdge')\nexcept AttributeError:\n    Graph.add_edge(self, edge)\nif not edge.index.tail in self.vertices() \\\nor not edge.index.head in self.vertices():\n    raise KeyError('tail and head must be in vertex set')\nif edge.index in self.edges():\n    raise ValueError('edge already exists')\nself._E.add(edge)", "path": "fuzz\\fgraph.py", "repo_name": "ezod/fuzzpy", "stars": 36, "license": "other", "language": "python", "size": 437}
{"docstring": "\"\"\"\\\nAlpha cut function. Returns the crisp graph for which both vertex and\nedge membership values meet or exceed the alpha value.\n\n@param alpha: The alpha value for the cut in [0, 1].\n@type alpha: C{float}\n@return: The crisp graph result of the alpha cut.\n@rtype: L{Graph}\n\"\"\"\n", "func_signal": "def alpha(self, alpha):\n", "code": "Va = self._V.alpha(alpha)\nEa = set()\nfor edge in self._E.alpha(alpha):\n    if edge.tail in Va and edge.head in Va:\n        Ea.add(edge)\nreturn Graph(viter=Va, eiter=Ea, directed=self.directed)", "path": "fuzz\\fgraph.py", "repo_name": "ezod/fuzzpy", "stars": 36, "license": "other", "language": "python", "size": 437}
{"docstring": "\"\"\"\\\nUpdate the set with the union of itself and other iterables.\n\"\"\"\n", "func_signal": "def update(self, *args):\n", "code": "for arg in args:\n    for item in arg:\n        self.add(item)", "path": "fuzz\\iset.py", "repo_name": "ezod/fuzzpy", "stars": 36, "license": "other", "language": "python", "size": 437}
{"docstring": "\"\"\"\\\nUpdate the set with the symmetric difference of itself and other\niterables.\n\"\"\"\n", "func_signal": "def symmetric_difference_update(self, *args):\n", "code": "common = set()\ncommon.update(args)\nfor item in common:\n    try:\n        self.remove(item)\n    except KeyError:\n        self.add(item)", "path": "fuzz\\iset.py", "repo_name": "ezod/fuzzpy", "stars": 36, "license": "other", "language": "python", "size": 437}
{"docstring": "\"\"\"\\\nReturn the membership degree of a vertex or edge.\n\n@param tail: The vertex or tail vertex.\n@type tail: C{object}\n@param head: The head vertex.\n@type head: C{object}\n@return: The membership degree of the vertex or edge from tail to head.\n@rtype: C{float}\n\"\"\"\n", "func_signal": "def mu(self, tail, head=None):\n", "code": "if head is None:\n    return self._V.mu(tail)\nelif self.directed:\n    return self._E.mu(GraphEdge((tail, head)))\nelse:\n    return max(self._E.mu(GraphEdge((tail, head))),\n               self._E.mu(GraphEdge((head, tail))))", "path": "fuzz\\fgraph.py", "repo_name": "ezod/fuzzpy", "stars": 36, "license": "other", "language": "python", "size": 437}
{"docstring": "\"\"\"\\\nReturn the symmetric difference of the set with other iterables.\n\"\"\"\n", "func_signal": "def symmetric_difference(self, *args):\n", "code": "result = self.copy()\nresult.symmetric_difference_update(*args)\nreturn result", "path": "fuzz\\iset.py", "repo_name": "ezod/fuzzpy", "stars": 36, "license": "other", "language": "python", "size": 437}
{"docstring": "\"\"\"\\\nUpdate the set with the difference of itself and other iterables.\n\"\"\"\n", "func_signal": "def difference_update(self, *args):\n", "code": "common = set()\ncommon.update(args)\nfor item in common:\n    self.discard(item)", "path": "fuzz\\iset.py", "repo_name": "ezod/fuzzpy", "stars": 36, "license": "other", "language": "python", "size": 437}
{"docstring": "# Q objects can be negated\n", "func_signal": "def test_q_negated(self):\n", "code": "self.assertQuerysetEqual(\n    Article.objects.filter(Q(pk=self.a1) | ~Q(pk=self.a2)), [\n        'Hello',\n        'Hello and goodbye'\n    ],\n    attrgetter(\"headline\")\n)\n\n# Does not work on MongoDB:\n#self.assertQuerysetEqual(\n#    Article.objects.filter(~Q(pk=self.a1) & ~Q(pk=self.a2)), [\n#        'Hello and goodbye'\n#    ],\n#    attrgetter(\"headline\"),\n#)\n\n# This allows for more complex queries than filter() and exclude()\n# alone would allow\nself.assertQuerysetEqual(\n    Article.objects.filter(Q(pk=self.a1) & (~Q(pk=self.a2) | Q(pk=self.a3))), [\n        'Hello'\n    ],\n    attrgetter(\"headline\"),\n)", "path": "tests\\or_lookups\\tests.py", "repo_name": "aparo/django-mongodb-engine", "stars": 55, "license": "None", "language": "python", "size": 350}
{"docstring": "# Q arg objects are ANDed\n", "func_signal": "def test_q_and(self):\n", "code": "self.assertQuerysetEqual(\n    Article.objects.filter(Q(headline__startswith='Hello'), Q(headline__contains='bye')), [\n        'Hello and goodbye'\n    ],\n    attrgetter(\"headline\")\n)\n# Q arg AND order is irrelevant\nself.assertQuerysetEqual(\n    Article.objects.filter(Q(headline__contains='bye'), headline__startswith='Hello'), [\n        'Hello and goodbye'\n    ],\n    attrgetter(\"headline\"),\n)\n\nself.assertQuerysetEqual(\n    Article.objects.filter(Q(headline__startswith='Hello') & Q(headline__startswith='Goodbye')),\n    []\n)", "path": "tests\\or_lookups\\tests.py", "repo_name": "aparo/django-mongodb-engine", "stars": 55, "license": "None", "language": "python", "size": 350}
{"docstring": "\"\"\"Creates the collection for model. Mostly used for capped collections.\n\n:param model: The model that should be created.\n:param \\*args: Extra args not used in this engine.\n:param \\*\\*kwargs: Extra kwargs not used in this engine.\n\nExample\n\n    >>> class TestFieldModel(Task):\n    ...\n    ...     class MongoMeta:\n    ...         capped = True\n    ...         collection_max = 100000\n    ...         collection_size = 10\n\"\"\"\n", "func_signal": "def sql_create_model(self, model, *args, **kwargs):\n", "code": "opts = model._meta\nkwargs = {}\nkwargs[\"capped\"] = getattr(opts, \"capped\", False)\nif hasattr(opts, \"collection_max\") and opts.collection_max:\n    kwargs[\"max\"] = opts.collection_max\nif hasattr(opts, \"collection_size\") and opts.collection_size:\n    kwargs[\"size\"] = opts.collection_size\ncol = Collection(self.connection.db_connection, model._meta.db_table, **kwargs)\nreturn [], {}", "path": "django_mongodb_engine\\creation.py", "repo_name": "aparo/django-mongodb-engine", "stars": 55, "license": "None", "language": "python", "size": 350}
{"docstring": "# You can shorten this syntax with code like the following,  which is\n# especially useful if building the query in stages:\n", "func_signal": "def test_stages(self):\n", "code": "articles = Article.objects.all()\nself.assertQuerysetEqual(\n    articles.filter(headline__startswith='Hello') & articles.filter(headline__startswith='Goodbye'),\n    []\n)\nself.assertQuerysetEqual(\n    articles.filter(headline__startswith='Hello') & articles.filter(headline__contains='bye'), [\n        'Hello and goodbye'\n    ],\n    attrgetter(\"headline\")\n)", "path": "tests\\or_lookups\\tests.py", "repo_name": "aparo/django-mongodb-engine", "stars": 55, "license": "None", "language": "python", "size": 350}
{"docstring": "\"\"\"\nReturns the db_connection instance\n (a :class:`pymongo.database.Database`)\n\"\"\"\n", "func_signal": "def db_connection(self):\n", "code": "self._connect()\nreturn self._db_connection", "path": "django_mongodb_engine\\base.py", "repo_name": "aparo/django-mongodb-engine", "stars": 55, "license": "None", "language": "python", "size": 350}
{"docstring": "\"\"\"\nDoes a raw MongoDB update. `spec_or_q` is either a MongoDB filter\ndict or a :class:`~django.db.models.query_utils.Q` instance that selects\nthe records to update. `update_dict` is a MongoDB style update document\ncontaining either a new document or atomic modifiers such as ``$inc``.\n\nKeyword arguments will be passed to :meth:`pymongo.Collection.update`.\n\"\"\"\n", "func_signal": "def raw_update(self, spec_or_q, update_dict, **kwargs):\n", "code": "queryset = self.get_query_set()\nif isinstance(spec_or_q, dict):\n    queryset.query.where.add(RawQuery(spec_or_q), AND)\nelse:\n    queryset = queryset.filter(spec_or_q)\ndummy_field = self.model._meta.pk.column\nreturn queryset.update(**{dummy_field: RawSpec(update_dict, kwargs)})", "path": "django_mongodb_engine\\contrib\\__init__.py", "repo_name": "aparo/django-mongodb-engine", "stars": 55, "license": "None", "language": "python", "size": 350}
{"docstring": "# This is a 1:1 copy of EmbeddedModelField's doctest\n", "func_signal": "def test_field_docstring(self):\n", "code": "bob = Customer(\n    name='Bob', last_name='Laxley',\n    address=Address(street='Behind the Mountains 23',\n                    postal_code=1337, city='Blurginson')\n)\nself.assertEqual(bob.address.postal_code, 1337)\nbob.save()\nbob_from_db = Customer.objects.get(name='Bob')\nself.assertEqual(bob.address.city, 'Blurginson')", "path": "tests\\embedded\\tests.py", "repo_name": "aparo/django-mongodb-engine", "stars": 55, "license": "None", "language": "python", "size": 350}
{"docstring": "\"\"\"\nReturns your text, enclosed in ANSI graphics codes.\n\nDepends on the keyword arguments 'fg' and 'bg', and the contents of\nthe opts tuple/list.\n\nReturns the RESET code if no parameters are given.\n\nValid colors:\n    'black', 'red', 'green', 'yellow', 'blue', 'magenta', 'cyan', 'white'\n\nValid options:\n    'bold'\n    'underscore'\n    'blink'\n    'reverse'\n    'conceal'\n    'noreset' - string will not be auto-terminated with the RESET code\n\nExamples:\n    colorize('hello', fg='red', bg='blue', opts=('blink',))\n    colorize()\n    colorize('goodbye', opts=('underscore',))\n    print colorize('first line', fg='red', opts=('noreset',))\n    print 'this should be red too'\n    print colorize('and so should this')\n    print 'this should not be red'\n\"\"\"\n", "func_signal": "def colorize(text='', opts=(), **kwargs):\n", "code": "color_names = ('black', 'red', 'green', 'yellow',\n               'blue', 'magenta', 'cyan', 'white')\nforeground = dict([(color_names[x], '3%s' % x) for x in range(8)])\nbackground = dict([(color_names[x], '4%s' % x) for x in range(8)])\n\nRESET = '0'\nopt_dict = {'bold': '1',\n            'underscore': '4',\n            'blink': '5',\n            'reverse': '7',\n            'conceal': '8'}\n\ntext = str(text)\ncode_list = []\nif text == '' and len(opts) == 1 and opts[0] == 'reset':\n    return '\\x1b[%sm' % RESET\nfor k, v in kwargs.iteritems():\n    if k == 'fg':\n        code_list.append(foreground[v])\n    elif k == 'bg':\n        code_list.append(background[v])\nfor o in opts:\n    if o in opt_dict:\n        code_list.append(opt_dict[o])\nif 'noreset' not in opts:\n    text = text + '\\x1b[%sm' % RESET\nreturn ('\\x1b[%sm' % ';'.join(code_list)) + text", "path": "docs\\_ext\\literals_to_xrefs.py", "repo_name": "aparo/django-mongodb-engine", "stars": 55, "license": "None", "language": "python", "size": 350}
{"docstring": "\"\"\"Create Indexes for field in model. Returns an empty List. (Django Compatibility)\n\n:param model: The model containing field\n:param f: The field to create indexes to.\n:param \\*\\*kwargs: Extra kwargs not used in this engine.\n\"\"\"\n\n", "func_signal": "def sql_indexes_for_field(self, model, field, **kwargs):\n", "code": "if field.db_index:\n    kwargs = {}\n    opts = model._meta\n    col = getattr(self.connection.db_connection, opts.db_table)\n    descending = getattr(opts, \"descending_indexes\", [])\n    direction =  (field.attname in descending and -1) or 1\n    kwargs[\"unique\"] = field.unique\n    col.ensure_index([(field.name, direction)], **kwargs)\nreturn []", "path": "django_mongodb_engine\\creation.py", "repo_name": "aparo/django-mongodb-engine", "stars": 55, "license": "None", "language": "python", "size": 350}
{"docstring": "\"\"\"Add the aggregate to the nominated query.\n\nThis method is used to convert the generic Aggregate definition into a\nbackend-specific definition.\n\n * query is the backend-specific query instance to which the aggregate\n   is to be added.\n * col is a column reference describing the subject field\n   of the aggregate. It can be an alias, or a tuple describing\n   a table and column name.\n * source is the underlying field or aggregate definition for\n   the column reference. If the aggregate is not an ordinal or\n   computed type, this reference is used to determine the coerced\n   output type of the aggregate.\n * is_summary is a boolean that is set True if the aggregate is a\n   summary value rather than an annotation.\n\"\"\"\n", "func_signal": "def add_to_query(self, query, alias, col, source, is_summary):\n", "code": "self.alias = alias\nself.field = self.source = source\n\nif self.valid_field_types and not self.source.get_internal_type() in self.valid_field_types:\n    raise RuntimeError()\nquery.aggregates[alias] = self", "path": "django_mongodb_engine\\contrib\\aggregations.py", "repo_name": "aparo/django-mongodb-engine", "stars": 55, "license": "None", "language": "python", "size": 350}
{"docstring": "# TODO: How do I find out which host/port/name the test DB has?\n", "func_signal": "def get_pymongo_collection(collection):\n", "code": "connection = Connection(settings.DATABASES['default']['HOST'],\n                        int(settings.DATABASES['default']['PORT']))\ndatabase = connection['test_test']\nreturn database[collection]", "path": "tests\\embedded\\utils.py", "repo_name": "aparo/django-mongodb-engine", "stars": 55, "license": "None", "language": "python", "size": 350}
{"docstring": "\"\"\"Create indexes for fields in group that belong to model.\n    This method is used to do compound indexes.\n\n:param model: The model containing the fields inside group.\n:param group: A ``dict`` containing the fields map to index.\n:param \\*\\*kwargs: Extra kwargs not used in this engine.\n\n\nExample\n\n    >>> class TestFieldModel(Task):\n    ...\n    ...     class MongoMeta:\n    ...         index_together = [{\n    ...             'fields' : [ ('title', False), 'mlist']\n    ...             }]\n    ...\n\"\"\"\n", "func_signal": "def index_fields_group(self, model, group, **kwargs):\n", "code": "if not isinstance(group, dict):\n    raise TypeError(\"Indexes group has to be instance of dict\")\n\nfields = group.pop(\"fields\")\n\nif not isinstance(fields, (list, tuple)):\n    raise TypeError(\"index_together fields has to be instance of list\")\n\nopts = model._meta\ncol = getattr(self.connection.db_connection, opts.db_table)\nchecked_fields = []\nmodel_fields = [ f.name for f in opts.local_fields]\n\nfor field in fields:\n    field_name = field\n    direction = 1\n    if isinstance(field, (tuple, list)):\n        field_name = field[0]\n        direction = (field[1] and 1) or -1\n    if not field_name in model_fields:\n        from django.db.models.fields import FieldDoesNotExist\n        raise FieldDoesNotExist('%s has no field named %r' % (opts.object_name, field_name))\n    checked_fields.append((field_name, direction))\ncol.ensure_index(checked_fields, **group)", "path": "django_mongodb_engine\\creation.py", "repo_name": "aparo/django-mongodb-engine", "stars": 55, "license": "None", "language": "python", "size": 350}
{"docstring": "# LegacyLegacyModelField should behave like EmbeddedLegacyModelField for\n# \"new-style\" data sets\n", "func_signal": "def test_legacy_field(self):\n", "code": "LegacyModel.objects.create(legacy=EmbeddedModel(charfield='blah'))\nself.assertEqual(LegacyModel.objects.get().legacy.charfield, u'blah')\n\n# LegacyLegacyModelField should keep the embedded model's 'id' if the data\n# set contains it. To add one, we have to do a manual update here:\nfrom utils import get_pymongo_collection\ncollection = get_pymongo_collection('embedded_legacymodel')\ncollection.update({}, {'$set' : {'legacy._id' : 42}}, safe=True)\nself.assertEqual(LegacyModel.objects.get().legacy.id, 42)\n\n# If the data record contains '_app' or '_model', they should be\n# stripped out so the resulting model instance is not populated with them.\ncollection.update({}, {'$set' : {'legacy._model' : 'a', 'legacy._app' : 'b'}}, safe=True)\nself.assertFalse(hasattr(LegacyModel.objects.get().legacy, '_model'))\nself.assertFalse(hasattr(LegacyModel.objects.get().legacy, '_app'))", "path": "tests\\embedded\\tests.py", "repo_name": "aparo/django-mongodb-engine", "stars": 55, "license": "None", "language": "python", "size": 350}
{"docstring": "\"\"\"\nremove leading underscore from folders in in the output folder.\n\n:todo: should only affect html built\n\"\"\"\n", "func_signal": "def move_private_folders(app, e):\n", "code": "def join(dir):\n    return os.path.join(app.builder.outdir, dir)\n\nfor item in os.listdir(app.builder.outdir):\n    if item.startswith('_') and os.path.isdir(join(item)):\n        shutil.move(join(item), join(item[1:]))", "path": "docs\\_ext\\django_mongodb_engine_docs.py", "repo_name": "aparo/django-mongodb-engine", "stars": 55, "license": "None", "language": "python", "size": 350}
{"docstring": "\"\"\"\nHandles aggregate/count queries\n\"\"\"\n", "func_signal": "def execute_sql(self, result_type=MULTI):\n", "code": "aggregations = self.query.aggregate_select.items()\n\nif len(aggregations) == 1 and isinstance(aggregations[0][1], sqlaggregates.Count):\n    # Ne need for full-featured aggregation processing if we only\n    # want to count() -- let djangotoolbox's simple Count aggregation\n    # implementation handle this case.\n    return super(SQLCompiler, self).execute_sql(result_type)\n\nfrom .contrib import aggregations as aggregations_module\nsqlagg, reduce, finalize, order, initial = [], [], [], [], {}\nquery = self.build_query()\n\n# First aggregations implementation\n# THIS CAN/WILL BE IMPROVED!!!\nfor alias, aggregate in aggregations:\n    if isinstance(aggregate, sqlaggregates.Aggregate):\n        if isinstance(aggregate, sqlaggregates.Count):\n            order.append(None)\n            # Needed to keep the iteration order which is important in the returned value.\n            sqlagg.append(self.get_count())\n            continue\n\n        aggregate_class = getattr(aggregations_module, aggregate.__class__.__name__)\n        # aggregation availability has been checked in check_aggregate_support in base.py\n\n        field = aggregate.source.name if aggregate.source else '_id'\n        if alias is None:\n            alias = '_id__%s' % cls_name\n        aggregate = aggregate_class(field, **aggregate.extra)\n        aggregate.add_to_query(self.query, alias, aggregate.col, aggregate.source,\n                               aggregate.extra.get(\"is_summary\", False))\n\n    order.append(aggregate.alias) # just to keep the right order\n    initial_, reduce_, finalize_ = aggregate.as_query(query)\n    initial.update(initial_)\n    reduce.append(reduce_)\n    finalize.append(finalize_)\n\ncursor = query.collection.group(None,\n                    query.db_query,\n                    initial,\n                    reduce=\"function(doc, out){ %s }\" % \"; \".join(reduce),\n                    finalize=\"function(out){ %s }\" % \"; \".join(finalize))\n\nret = []\nfor alias in order:\n    result = cursor[0][alias] if alias else sqlagg.pop(0)\n    if result_type is MULTI:\n        result = [result]\n    ret.append(result)\nreturn ret", "path": "django_mongodb_engine\\compiler.py", "repo_name": "aparo/django-mongodb-engine", "stars": 55, "license": "None", "language": "python", "size": 350}
{"docstring": "\"\"\"Make sure that a mongodb model appears on a mongodb database\"\"\"\n\n", "func_signal": "def allow_syncdb(self, db, model):\n", "code": "if db in self.mongodb_databases:\n    return self.is_managed(model)\nelif self.is_managed(model):\n    return db in self.mongodb_databases\n\nreturn None", "path": "django_mongodb_engine\\router.py", "repo_name": "aparo/django-mongodb-engine", "stars": 55, "license": "None", "language": "python", "size": 350}
{"docstring": "# Try some arg queries with operations other than filter.\n", "func_signal": "def test_other_arg_queries(self):\n", "code": "self.assertEqual(\n    Article.objects.get(Q(headline__startswith='Hello'),\n                        Q(headline__contains='bye')).headline,\n    'Hello and goodbye'\n)\n\nself.assertEqual(\n    Article.objects.filter(Q(headline__startswith='Hello') | Q(headline__contains='bye')).count(),\n    3\n)\n\nself.assertQuerysetEqual(\n    Article.objects.filter(Q(headline__startswith='Hello'), Q(headline__contains='bye')).values(), [\n        {\"headline\": \"Hello and goodbye\", \"id\": self.a3, \"pub_date\": datetime(2005, 11, 29)},\n    ],\n    lambda o: o,\n)\n\nself.assertEqual(\n    Article.objects.filter(Q(headline__startswith='Hello')).in_bulk([self.a1, self.a2]),\n    {self.a1: Article.objects.get(pk=self.a1)}\n)", "path": "tests\\or_lookups\\tests.py", "repo_name": "aparo/django-mongodb-engine", "stars": 55, "license": "None", "language": "python", "size": 350}
{"docstring": "\"\"\"\nReturns a list of SQL statements that have to be executed to drop\nall `tables`. No SQL in MongoDB, so just drop all tables here and\nreturn an empty list.\n\"\"\"\n", "func_signal": "def sql_flush(self, style, tables, sequence_list):\n", "code": "tables = self.connection.db_connection.collection_names()\nfor table in tables:\n    if table.startswith('system.'):\n        # no do not system collections\n        continue\n    self.connection.db_connection.drop_collection(table)\nreturn []", "path": "django_mongodb_engine\\base.py", "repo_name": "aparo/django-mongodb-engine", "stars": 55, "license": "None", "language": "python", "size": 350}
{"docstring": "\"\"\"Make sure that a model is valid for a database provider\"\"\"\n", "func_signal": "def valid_for_db_engine(self, driver, model):\n", "code": "if driver != 'mongodb':\n    return False\nreturn self.is_managed(model)", "path": "django_mongodb_engine\\router.py", "repo_name": "aparo/django-mongodb-engine", "stars": 55, "license": "None", "language": "python", "size": 350}
{"docstring": "# The 'complex_filter' method supports framework features such as\n# 'limit_choices_to' which normally take a single dictionary of lookup\n# arguments but need to support arbitrary queries via Q objects too.\n", "func_signal": "def test_complex_filter(self):\n", "code": "self.assertQuerysetEqual(\n    Article.objects.complex_filter({'pk': self.a1}), [\n        'Hello'\n    ],\n    attrgetter(\"headline\"),\n)\n\nself.assertQuerysetEqual(\n    Article.objects.complex_filter(Q(pk=self.a1) | Q(pk=self.a2)), [\n        'Hello',\n        'Goodbye'\n    ],\n    attrgetter(\"headline\"),\n)", "path": "tests\\or_lookups\\tests.py", "repo_name": "aparo/django-mongodb-engine", "stars": 55, "license": "None", "language": "python", "size": 350}
{"docstring": "\"\"\"Mark state as critical.\"\"\"\n", "func_signal": "def crit(self, message=None):\n", "code": "self._status = max(self._status, CRITICAL)\nif message is not None:\n  self._messages[CRITICAL].append(message)\nreturn self", "path": "checklib\\src\\greplin\\nagios.py", "repo_name": "Cue/greplin-nagios-utils", "stars": 39, "license": "apache-2.0", "language": "python", "size": 170}
{"docstring": "\"\"\"Create an error message.\"\"\"\n", "func_signal": "def message(self, name, value):\n", "code": "if self.check(value) == CRITICAL:\n  return '%s: %.9g%s > %.9g%s' % (name, value, self.unit, self.critLevel, self.unit)\nelif self.check(value) == WARNING:\n  return '%s: %.9g%s > %.9g%s' % (name, value, self.unit, self.warnLevel, self.unit)", "path": "checklib\\src\\greplin\\nagios.py", "repo_name": "Cue/greplin-nagios-utils", "stars": 39, "license": "apache-2.0", "language": "python", "size": 170}
{"docstring": "\"\"\"Gets an http page, but times out if it's too slow.\"\"\"\n", "func_signal": "def wgetWithTimeout(host, port, path, timeout, secure = False):\n", "code": "start = time.time()\ntry:\n  if secure:\n    conn = httplib.HTTPSConnection(host, port, timeout=timeout)\n  else:\n    conn = httplib.HTTPConnection(host, port, timeout=timeout)\n  conn.request('GET', path)\n  body = conn.getresponse().read()\n  return time.time() - start, body\n\nexcept (socket.gaierror, socket.error):\n  output(\"CRIT: Could not connect to %s\" % host)\n  exit(CRITICAL)\n\nexcept socket.timeout:\n  output(\"CRIT: Timed out after %s seconds\" % timeout)\n  exit(CRITICAL)", "path": "checklib\\src\\greplin\\nagios.py", "repo_name": "Cue/greplin-nagios-utils", "stars": 39, "license": "apache-2.0", "language": "python", "size": 170}
{"docstring": "\"\"\"Mark state as warning.\"\"\"\n", "func_signal": "def warn(self, message=None):\n", "code": "self._status = max(self._status, WARNING)\nif message is not None:\n  self._messages[WARNING].append(message)\nreturn self", "path": "checklib\\src\\greplin\\nagios.py", "repo_name": "Cue/greplin-nagios-utils", "stars": 39, "license": "apache-2.0", "language": "python", "size": 170}
{"docstring": "\"\"\"Runs the check.\"\"\"\n", "func_signal": "def check(argv):\n", "code": "_ = parseArgs('check_slow.py', ('NAME', str), argv=argv)\ntime.sleep(5)\n\n(ResponseBuilder().addRule('harrypotter', Maximum(42, 108), 69)).finish()", "path": "checkserver\\testchecks\\check_slow.py", "repo_name": "Cue/greplin-nagios-utils", "stars": 39, "license": "apache-2.0", "language": "python", "size": 170}
{"docstring": "\"\"\"Adds an alert rule and associated performance data.\"\"\"\n", "func_signal": "def addRule(self, name, rule, value):\n", "code": "status = rule.check(value)\nif status:\n  ruleStatus = rule.check(value)\n  self._status = max(self._status, ruleStatus)\n  self._messages[ruleStatus].append(rule.message(name, value))\nself._stats.append(rule.format(name, value))\nreturn self", "path": "checklib\\src\\greplin\\nagios.py", "repo_name": "Cue/greplin-nagios-utils", "stars": 39, "license": "apache-2.0", "language": "python", "size": 170}
{"docstring": "\"\"\"Adds a stat from a sequential key lookup.\"\"\"\n", "func_signal": "def addStatLookup(self, name, data, *keys, **kw):\n", "code": "value = lookup(data, *keys, **kw)\nreturn self.addValue(name, str(value) + kw.get('suffix', ''))", "path": "checklib\\src\\greplin\\nagios.py", "repo_name": "Cue/greplin-nagios-utils", "stars": 39, "license": "apache-2.0", "language": "python", "size": 170}
{"docstring": "\"\"\"Adds a child for each child of the given dict.\"\"\"\n", "func_signal": "def addStatChildren(self, name, data, *keys, **kw):\n", "code": "values = lookup(data, *keys, **kw)\nif values:\n  for childName, value in values.items():\n    self.addValue(name % childName, str(value) + kw.get('suffix', ''))\nreturn self", "path": "checklib\\src\\greplin\\nagios.py", "repo_name": "Cue/greplin-nagios-utils", "stars": 39, "license": "apache-2.0", "language": "python", "size": 170}
{"docstring": "\"\"\"Checks if the given value is under the minimums.\"\"\"\n", "func_signal": "def check(self, value):\n", "code": "if value < self.critLevel:\n  return CRITICAL\nelif value < self.warnLevel:\n  return WARNING\nelse:\n  return OK", "path": "checklib\\src\\greplin\\nagios.py", "repo_name": "Cue/greplin-nagios-utils", "stars": 39, "license": "apache-2.0", "language": "python", "size": 170}
{"docstring": "\"\"\"Adds a value to be tracked.\"\"\"\n", "func_signal": "def addValue(self, name, value):\n", "code": "self._stats.append(\"'%s'=%s;;;;;\" % (name, str(value)))\nreturn self", "path": "checklib\\src\\greplin\\nagios.py", "repo_name": "Cue/greplin-nagios-utils", "stars": 39, "license": "apache-2.0", "language": "python", "size": 170}
{"docstring": "\"\"\"Successively looks up each key, returning the default keyword arg if a dead end is reached.\"\"\"\n", "func_signal": "def lookup(source, *keys, **kw):\n", "code": "fallback = kw.get('default')\ntry:\n  for key in keys:\n    source = source[key]\n  return source\nexcept (KeyError, AttributeError, TypeError):\n  return fallback", "path": "checklib\\src\\greplin\\nagios.py", "repo_name": "Cue/greplin-nagios-utils", "stars": 39, "license": "apache-2.0", "language": "python", "size": 170}
{"docstring": "\"\"\"Mark state as critical on the given condition.\"\"\"\n", "func_signal": "def critIf(self, condition, message=None):\n", "code": "if condition:\n  self.crit(message)\nreturn self", "path": "checklib\\src\\greplin\\nagios.py", "repo_name": "Cue/greplin-nagios-utils", "stars": 39, "license": "apache-2.0", "language": "python", "size": 170}
{"docstring": "\"\"\"Checks if the given value exceeds the maximums.\"\"\"\n", "func_signal": "def check(self, value):\n", "code": "if value > self.critLevel:\n  return CRITICAL\nelif value > self.warnLevel:\n  return WARNING\nelse:\n  return OK", "path": "checklib\\src\\greplin\\nagios.py", "repo_name": "Cue/greplin-nagios-utils", "stars": 39, "license": "apache-2.0", "language": "python", "size": 170}
{"docstring": "\"\"\"Runs the check.\"\"\"\n", "func_signal": "def check(argv):\n", "code": "_ = parseArgs('check_fast.py', ('NAME', str), argv=argv) / 0 # Badness!\n\n\n(ResponseBuilder().addRule('seven', Maximum(8, 11), 7)).finish()", "path": "checkserver\\testchecks\\check_error.py", "repo_name": "Cue/greplin-nagios-utils", "stars": 39, "license": "apache-2.0", "language": "python", "size": 170}
{"docstring": "\"\"\"Builds the response, prints it, and exits.\"\"\"\n", "func_signal": "def finish(self):\n", "code": "status = STATUS_NAME[self._status]\nmessages = self._messages[UNKNOWN] + self._messages[CRITICAL] + self._messages[WARNING] + self._messages[OK]\nif messages:\n  status += ': ' + (', '.join(messages))\nif self._stats:\n  status += '|' + self.build()\n\noutput(status)\nsys.exit(self._status)", "path": "checklib\\src\\greplin\\nagios.py", "repo_name": "Cue/greplin-nagios-utils", "stars": 39, "license": "apache-2.0", "language": "python", "size": 170}
{"docstring": "\"\"\"Warn on a given condition.\"\"\"\n", "func_signal": "def warnIf(self, condition, message=None):\n", "code": "if condition:\n  self.warn(message)\nreturn self", "path": "checklib\\src\\greplin\\nagios.py", "repo_name": "Cue/greplin-nagios-utils", "stars": 39, "license": "apache-2.0", "language": "python", "size": 170}
{"docstring": "\"\"\"Create an error message.\"\"\"\n", "func_signal": "def message(self, name, value):\n", "code": "if self.check(value) == CRITICAL:\n  return ('%s: %.9g%s < %.9g%s') % (name, value, self.unit, self.critLevel, self.unit)\nelif self.check(value) == WARNING:\n  return ('%s: %.9g%s < %.9g%s') % (name, value, self.unit, self.warnLevel, self.unit)", "path": "checklib\\src\\greplin\\nagios.py", "repo_name": "Cue/greplin-nagios-utils", "stars": 39, "license": "apache-2.0", "language": "python", "size": 170}
{"docstring": "\"\"\"Mark state as unknown on the given condition.\"\"\"\n", "func_signal": "def unknownIf(self, condition, message=None):\n", "code": "if condition:\n  self.unknown(message)\nreturn self", "path": "checklib\\src\\greplin\\nagios.py", "repo_name": "Cue/greplin-nagios-utils", "stars": 39, "license": "apache-2.0", "language": "python", "size": 170}
{"docstring": "\"\"\"Mark state as unknown.\"\"\"\n", "func_signal": "def unknown(self, message=None):\n", "code": "self._status = max(self._status, UNKNOWN)\nif message is not None:\n  self._messages[UNKNOWN].append(message)\nreturn self", "path": "checklib\\src\\greplin\\nagios.py", "repo_name": "Cue/greplin-nagios-utils", "stars": 39, "license": "apache-2.0", "language": "python", "size": 170}
{"docstring": "\"\"\"Send output to output stream.\"\"\"\n", "func_signal": "def output(msg):\n", "code": "GLOBAL_CONFIG.outfile.write(msg)\nGLOBAL_CONFIG.outfile.write('\\n')", "path": "checklib\\src\\greplin\\nagios.py", "repo_name": "Cue/greplin-nagios-utils", "stars": 39, "license": "apache-2.0", "language": "python", "size": 170}
{"docstring": "\"\"\"\nUsed to generate random key/secret pairings. Use this after you've\nadded the other data in place of save(). \n\nc = Consumer()\nc.name = \"My consumer\" \nc.description = \"An app that makes ponies from the API.\"\nc.user = some_user_object\nc.generate_random_codes()\n\"\"\"\n", "func_signal": "def generate_random_codes(self):\n", "code": "key = User.objects.make_random_password(length=KEY_SIZE)\nsecret = generate_random(SECRET_SIZE)\n\nwhile Consumer.objects.filter(key__exact=key, secret__exact=secret).count():\n    secret = generate_random(SECRET_SIZE)\n\nself.key = key\nself.secret = secret\nself.save()", "path": "piston\\models.py", "repo_name": "proffalken/edison", "stars": 42, "license": "bsd-3-clause", "language": "python", "size": 1021}
{"docstring": "\"\"\"Return whether the given ``value`` can be used as a JSON-P callback.\"\"\"\n\n", "func_signal": "def is_valid_jsonp_callback_value(value):\n", "code": "for identifier in value.split(u'.'):\n    while '[' in identifier:\n        if not has_valid_array_index(identifier):\n            return False\n        identifier = replace_array_index(u'', identifier)\n    if not is_valid_javascript_identifier(identifier):\n        return False\n\nreturn True", "path": "piston\\validate_jsonp.py", "repo_name": "proffalken/edison", "stars": 42, "license": "bsd-3-clause", "language": "python", "size": 1021}
{"docstring": "\"\"\"Parses the URL and rebuilds it to be scheme://host/path.\"\"\"\n", "func_signal": "def get_normalized_http_url(self):\n", "code": "parts = urlparse.urlparse(self.http_url)\nscheme, netloc, path = parts[:3]\n# Exclude default port numbers.\nif scheme == 'http' and netloc[-3:] == ':80':\n    netloc = netloc[:-3]\nelif scheme == 'https' and netloc[-4:] == ':443':\n    netloc = netloc[:-4]\nreturn '%s://%s%s' % (scheme, netloc, path)", "path": "piston\\oauth.py", "repo_name": "proffalken/edison", "stars": 42, "license": "bsd-3-clause", "language": "python", "size": 1021}
{"docstring": "\"\"\" Returns a token from something like:\noauth_token_secret=xxx&oauth_token=xxx\n\"\"\"\n", "func_signal": "def from_string(s):\n", "code": "params = cgi.parse_qs(s, keep_blank_values=False)\nkey = params['oauth_token'][0]\nsecret = params['oauth_token_secret'][0]\ntoken = OAuthToken(key, secret)\ntry:\n    token.callback_confirmed = params['oauth_callback_confirmed'][0]\nexcept KeyError:\n    pass # 1.0, no callback confirmed.\nreturn token", "path": "piston\\oauth.py", "repo_name": "proffalken/edison", "stars": 42, "license": "bsd-3-clause", "language": "python", "size": 1021}
{"docstring": "\"\"\"Verify that timestamp is recentish.\"\"\"\n", "func_signal": "def _check_timestamp(self, timestamp):\n", "code": "timestamp = int(timestamp)\nnow = int(time.time())\nlapsed = now - timestamp\nif lapsed > self.timestamp_threshold:\n    raise OAuthError('Expired timestamp: given %d and now %s has a '\n        'greater difference than threshold %d' %\n        (timestamp, now, self.timestamp_threshold))", "path": "piston\\oauth.py", "repo_name": "proffalken/edison", "stars": 42, "license": "bsd-3-clause", "language": "python", "size": 1021}
{"docstring": "# Check signature...\n", "func_signal": "def get_csrf_signature(key, token):\n", "code": "try:\n    import hashlib # 2.5\n    hashed = hmac.new(key, token, hashlib.sha1)\nexcept:\n    import sha # deprecated\n    hashed = hmac.new(key, token, sha)\n\n# calculate the digest base 64\nreturn base64.b64encode(hashed.digest())", "path": "piston\\forms.py", "repo_name": "proffalken/edison", "stars": 42, "license": "bsd-3-clause", "language": "python", "size": 1021}
{"docstring": "\"\"\"Set the signature parameter to the result of build_signature.\"\"\"\n# Set the signature method.\n", "func_signal": "def sign_request(self, signature_method, consumer, token):\n", "code": "self.set_parameter('oauth_signature_method',\n    signature_method.get_name())\n# Set the signature.\nself.set_parameter('oauth_signature',\n    self.build_signature(signature_method, consumer, token))", "path": "piston\\oauth.py", "repo_name": "proffalken/edison", "stars": 42, "license": "bsd-3-clause", "language": "python", "size": 1021}
{"docstring": "\"\"\"Serialize as post data for a POST request.\"\"\"\n", "func_signal": "def to_postdata(self):\n", "code": "return '&'.join(['%s=%s' % (escape(str(k)), escape(str(v))) \\\n    for k, v in self.parameters.iteritems()])", "path": "piston\\oauth.py", "repo_name": "proffalken/edison", "stars": 42, "license": "bsd-3-clause", "language": "python", "size": 1021}
{"docstring": "\"\"\"Return whether the given ``id`` is a valid Javascript identifier.\"\"\"\n\n", "func_signal": "def is_valid_javascript_identifier(identifier, escape=r'\\u', ucd_cat=category):\n", "code": "if not identifier:\n    return False\n\nif not isinstance(identifier, unicode):\n    try:\n        identifier = unicode(identifier, 'utf-8')\n    except UnicodeDecodeError:\n        return False\n\nif escape in identifier:\n\n    new = []; add_char = new.append\n    split_id = identifier.split(escape)\n    add_char(split_id.pop(0))\n\n    for segment in split_id:\n        if len(segment) < 4:\n            return False\n        try:\n            add_char(unichr(int('0x' + segment[:4], 16)))\n        except Exception:\n            return False\n        add_char(segment[4:])\n        \n    identifier = u''.join(new)\n\nif is_reserved_js_word(identifier):\n    return False\n\nfirst_char = identifier[0]\n\nif not ((first_char in valid_jsid_chars) or\n        (ucd_cat(first_char) in valid_jsid_categories_start)):\n    return False\n\nfor char in identifier[1:]:\n    if not ((char in valid_jsid_chars) or\n            (ucd_cat(char) in valid_jsid_categories)):\n        return False\n\nreturn True", "path": "piston\\validate_jsonp.py", "repo_name": "proffalken/edison", "stars": 42, "license": "bsd-3-clause", "language": "python", "size": 1021}
{"docstring": "\"\"\"Processes a request_token request and returns the\nrequest token on success.\n\"\"\"\n", "func_signal": "def fetch_request_token(self, oauth_request):\n", "code": "try:\n    # Get the request token for authorization.\n    token = self._get_token(oauth_request, 'request')\nexcept OAuthError:\n    # No token required for the initial token request.\n    version = self._get_version(oauth_request)\n    consumer = self._get_consumer(oauth_request)\n    try:\n        callback = self.get_callback(oauth_request)\n    except OAuthError:\n        callback = None # 1.0, no callback specified.\n    self._check_signature(oauth_request, consumer, None)\n    # Fetch a new token.\n    token = self.data_store.fetch_request_token(consumer, callback)\nreturn token", "path": "piston\\oauth.py", "repo_name": "proffalken/edison", "stars": 42, "license": "bsd-3-clause", "language": "python", "size": 1021}
{"docstring": "\"\"\"Turn Authorization: header into parameters.\"\"\"\n", "func_signal": "def _split_header(header):\n", "code": "params = {}\nparts = header.split(',')\nfor param in parts:\n    # Ignore realm parameter.\n    if param.find('realm') > -1:\n        continue\n    # Remove whitespace.\n    param = param.strip()\n    # Split key-value.\n    param_parts = param.split('=', 1)\n    # Remove quotes and unescape the value.\n    params[param_parts[0]] = urllib.unquote(param_parts[1].strip('\\\"'))\nreturn params", "path": "piston\\oauth.py", "repo_name": "proffalken/edison", "stars": 42, "license": "bsd-3-clause", "language": "python", "size": 1021}
{"docstring": "\"\"\"Verify the correct version request for this server.\"\"\"\n", "func_signal": "def _get_version(self, oauth_request):\n", "code": "try:\n    version = oauth_request.get_parameter('oauth_version')\nexcept:\n    version = VERSION\nif version and version != self.version:\n    raise OAuthError('OAuth version %s not supported.' % str(version))\nreturn version", "path": "piston\\oauth.py", "repo_name": "proffalken/edison", "stars": 42, "license": "bsd-3-clause", "language": "python", "size": 1021}
{"docstring": "\"\"\"Processes an access_token request and returns the\naccess token on success.\n\"\"\"\n", "func_signal": "def fetch_access_token(self, oauth_request):\n", "code": "version = self._get_version(oauth_request)\nconsumer = self._get_consumer(oauth_request)\nverifier = self._get_verifier(oauth_request)\n# Get the request token.\ntoken = self._get_token(oauth_request, 'request')\nself._check_signature(oauth_request, consumer, token)\nnew_token = self.data_store.fetch_access_token(consumer, token, verifier)\nreturn new_token", "path": "piston\\oauth.py", "repo_name": "proffalken/edison", "stars": 42, "license": "bsd-3-clause", "language": "python", "size": 1021}
{"docstring": "\"\"\"Convert unicode to utf-8.\"\"\"\n", "func_signal": "def _utf8_str(s):\n", "code": "if isinstance(s, unicode):\n    return s.encode(\"utf-8\")\nelse:\n    return str(s)", "path": "piston\\oauth.py", "repo_name": "proffalken/edison", "stars": 42, "license": "bsd-3-clause", "language": "python", "size": 1021}
{"docstring": "\"\"\"\nGets an emitter, returns the class and a content-type.\n\"\"\"\n", "func_signal": "def get(cls, format):\n", "code": "if cls.EMITTERS.has_key(format):\n    return cls.EMITTERS.get(format)\n\nraise ValueError(\"No emitters found for type %s\" % format)", "path": "piston\\emitters.py", "repo_name": "proffalken/edison", "stars": 42, "license": "bsd-3-clause", "language": "python", "size": 1021}
{"docstring": "\"\"\"Serialize as a header for an HTTPAuth request.\"\"\"\n", "func_signal": "def to_header(self, realm=''):\n", "code": "auth_header = 'OAuth realm=\"%s\"' % realm\n# Add the oauth parameters.\nif self.parameters:\n    for k, v in self.parameters.iteritems():\n        if k[:6] == 'oauth_':\n            auth_header += ', %s=\"%s\"' % (k, escape(str(v)))\nreturn {'Authorization': auth_header}", "path": "piston\\oauth.py", "repo_name": "proffalken/edison", "stars": 42, "license": "bsd-3-clause", "language": "python", "size": 1021}
{"docstring": "\"\"\"Turn URL string into parameters.\"\"\"\n", "func_signal": "def _split_url_string(param_str):\n", "code": "parameters = cgi.parse_qs(param_str, keep_blank_values=False)\nfor k, v in parameters.iteritems():\n    parameters[k] = urllib.unquote(v[0])\nreturn parameters", "path": "piston\\oauth.py", "repo_name": "proffalken/edison", "stars": 42, "license": "bsd-3-clause", "language": "python", "size": 1021}
{"docstring": "\"\"\"Builds the base signature string.\"\"\"\n", "func_signal": "def build_signature(self, oauth_request, consumer, token):\n", "code": "key, raw = self.build_signature_base_string(oauth_request, consumer,\n    token)\n\n# HMAC object.\ntry:\n    import hashlib # 2.5\n    hashed = hmac.new(key, raw, hashlib.sha1)\nexcept:\n    import sha # Deprecated\n    hashed = hmac.new(key, raw, sha)\n\n# Calculate the digest base 64.\nreturn binascii.b2a_base64(hashed.digest())[:-1]", "path": "piston\\oauth.py", "repo_name": "proffalken/edison", "stars": 42, "license": "bsd-3-clause", "language": "python", "size": 1021}
{"docstring": "\"\"\"Verifies an api call and checks all the parameters.\"\"\"\n# -> consumer and token\n", "func_signal": "def verify_request(self, oauth_request):\n", "code": "version = self._get_version(oauth_request)\nconsumer = self._get_consumer(oauth_request)\n# Get the access token.\ntoken = self._get_token(oauth_request, 'access')\nself._check_signature(oauth_request, consumer, token)\nparameters = oauth_request.get_nonoauth_parameters()\nreturn consumer, token, parameters", "path": "piston\\oauth.py", "repo_name": "proffalken/edison", "stars": 42, "license": "bsd-3-clause", "language": "python", "size": 1021}
{"docstring": "\"\"\"Verify that the nonce is uniqueish.\"\"\"\n", "func_signal": "def _check_nonce(self, consumer, token, nonce):\n", "code": "nonce = self.data_store.lookup_nonce(consumer, token, nonce)\nif nonce:\n    raise OAuthError('Nonce already used: %s' % str(nonce))", "path": "piston\\oauth.py", "repo_name": "proffalken/edison", "stars": 42, "license": "bsd-3-clause", "language": "python", "size": 1021}
{"docstring": "\"\"\"\nsupers = \"\"\nif self.supers:\n    supers = \" extends \" + \" \".join(self.supers)\nimplements = \"\"\nif self.implements:\n    implements = \" implements \" + \" \".join(self.implements)\n\"\"\"\n", "func_signal": "def __repr__(self):\n", "code": "return \"%s %s%s%s\" % (\" \".join(self.access), self.desc, supers,\n        implements)\n#return '\\n'.join([repr(m) for m in self.methods])", "path": "apkil\\api.py", "repo_name": "kelwin/apkil", "stars": 58, "license": "None", "language": "python", "size": 80961}
{"docstring": "\"\"\"\n    Return all fields of a specific class\n\n    :param class_name: the class name\n    :type class_name: string\n\n    :rtype: a list with :class:`EncodedField` objects\n\"\"\"\n", "func_signal": "def get_fields_class(self, class_name) :\n", "code": "l = []\nfor i in self.classes.class_def :\n    for j in i.get_fields() :\n        if class_name == j.get_class_name() :\n            l.append( j )\n\nreturn l", "path": "androguard\\core\\bytecodes\\dvm.py", "repo_name": "kelwin/apkil", "stars": 58, "license": "None", "language": "python", "size": 80961}
{"docstring": "\"\"\"\n    Get a particular instruction by using the address\n\n    :param off: address of the instruction\n    :type off: int\n\n    :rtype: an :class:`Instruction` object\n\"\"\"\n", "func_signal": "def get_ins_off(self, off) :\n", "code": "idx = 0\nfor i in self.get_instructions() :\n    if idx == off :\n        return i\n    idx += i.get_length()\nreturn None", "path": "androguard\\core\\bytecodes\\dvm.py", "repo_name": "kelwin/apkil", "stars": 58, "license": "None", "language": "python", "size": 80961}
{"docstring": "\"\"\"\n    Create XREF for this object\n\n    :param python_export (boolean): export xref in each method\n\"\"\"\n", "func_signal": "def create_xref(self, python_export=True) :\n", "code": "gvm = self.CM.get_gvmanalysis()\n\nfor _class in self.get_classes() :\n    for method in _class.get_methods() :\n        method.XREFfrom = XREF()\n        method.XREFto = XREF()\n\n        key = \"%s %s %s\" % (method.get_class_name(), method.get_name(), method.get_descriptor())\n\n        if key in gvm.nodes :\n            for i in gvm.G.predecessors( gvm.nodes[ key ].id ) :\n                xref = gvm.nodes_id[ i ]\n                xref_meth = self.get_method_descriptor( xref.class_name, xref.method_name, xref.descriptor)\n                if xref_meth != None :\n                    name = bytecode.FormatClassToPython( xref_meth.get_class_name() ) + \"__\" + \\\n                    bytecode.FormatNameToPython( xref_meth.get_name() ) + \"__\" + \\\n                    bytecode.FormatDescriptorToPython( xref_meth.get_descriptor() )\n\n                    if python_export == True :\n                        setattr( method.XREFfrom, name, xref_meth )\n                    method.XREFfrom.add( xref_meth, xref.edges[ gvm.nodes[ key ] ] )\n\n            for i in gvm.G.successors( gvm.nodes[ key ].id ) :\n                xref = gvm.nodes_id[ i ]\n                xref_meth = self.get_method_descriptor( xref.class_name, xref.method_name, xref.descriptor)\n                if xref_meth != None :\n                    name = bytecode.FormatClassToPython( xref_meth.get_class_name() ) + \"__\" + \\\n                    bytecode.FormatNameToPython( xref_meth.get_name() ) + \"__\" + \\\n                    bytecode.FormatDescriptorToPython( xref_meth.get_descriptor() )\n                    \n                    if python_export == True :\n                        setattr( method.XREFto, name, xref_meth )\n                    method.XREFto.add( xref_meth, gvm.nodes[ key ].edges[ xref ] )", "path": "androguard\\core\\bytecodes\\dvm.py", "repo_name": "kelwin/apkil", "stars": 58, "license": "None", "language": "python", "size": 80961}
{"docstring": "\"\"\"\n    Return the access flags string of the class\n\n    :rtype: string\n\"\"\"\n", "func_signal": "def get_access_flags_string(self) :\n", "code": "if self.access_flags_string == None :\n    self.access_flags_string = get_access_flags_string( self.get_access_flags() )\n\n    if self.access_flags_string == \"\" :\n        self.access_flags_string = \"0x%x\" % self.get_access_flags()\nreturn self.access_flags_string", "path": "androguard\\core\\bytecodes\\dvm.py", "repo_name": "kelwin/apkil", "stars": 58, "license": "None", "language": "python", "size": 80961}
{"docstring": "\"\"\"\n    Get the position of an instruction by using the address\n\n    :param off: address of the instruction\n    :type off: int\n\n    :rtype: int\n\"\"\"\n", "func_signal": "def off_to_pos(self, off) :\n", "code": "idx = 0\nnb = 0\nfor i in self.get_instructions() :\n    if idx == off :\n        return nb\n    nb += 1\n    idx += i.get_length()\nreturn -1", "path": "androguard\\core\\bytecodes\\dvm.py", "repo_name": "kelwin/apkil", "stars": 58, "license": "None", "language": "python", "size": 80961}
{"docstring": "\"\"\"\n    Return a list all methods which corresponds to the regexp\n\n    :param name: the name of the method (a python regexp)\n\n    :rtype: a list with all :class:`EncodedMethod` objects\n\"\"\"\n", "func_signal": "def get_method(self, name) :\n", "code": "prog = re.compile(name)\nl = []\nfor i in self.classes.class_def :\n    for j in i.get_methods() :\n        if prog.match( j.get_name() ) :\n            l.append( j )\nreturn l", "path": "androguard\\core\\bytecodes\\dvm.py", "repo_name": "kelwin/apkil", "stars": 58, "license": "None", "language": "python", "size": 80961}
{"docstring": "\"\"\"\n    Return the descriptor\n\n    :rtype: string\n\"\"\"\n", "func_signal": "def get_descriptor(self) :\n", "code": "proto = self.get_proto()\nreturn proto[0] + proto[1]", "path": "androguard\\core\\bytecodes\\dvm.py", "repo_name": "kelwin/apkil", "stars": 58, "license": "None", "language": "python", "size": 80961}
{"docstring": "\"\"\"\n    Display (with a pretty print) this object\n\n    :param m_a: :class:`MethodAnalysis` object\n\"\"\"\n", "func_signal": "def pretty_show(self, m_a) :\n", "code": "bytecode.PrettyShow( m_a.basic_blocks.gets(), self.notes )\nbytecode.PrettyShowEx( m_a.exceptions.gets() )", "path": "androguard\\core\\bytecodes\\dvm.py", "repo_name": "kelwin/apkil", "stars": 58, "license": "None", "language": "python", "size": 80961}
{"docstring": "#print name, start_ea\n        \n", "func_signal": "def __init__(self, name, start_ea, instructions, information) :\n", "code": "self.name = name\nself.start_ea = start_ea\nself.information = information\nself.basic_blocks = []\nself.instructions = instructions\n\nr = {}\nidx = 0\nfor i in instructions :\n    r[ i[0] ] = idx\n    idx += 1\n\nfor i in information[0] :\n    try :\n        start = r[i[0]]\n        end = r[i[1]] + 1\n        self.basic_blocks.append( BasicBlock( instructions[start:end] ) )\n    except KeyError :\n        pass", "path": "androguard\\core\\binaries\\idapipe.py", "repo_name": "kelwin/apkil", "stars": 58, "license": "None", "language": "python", "size": 80961}
{"docstring": "\"\"\"\n  Retrieve the type of a descriptor (e.g : I)\n\"\"\"\n", "func_signal": "def get_type(atype, size=None):\n", "code": "if atype.startswith('java.lang'):\n    atype = atype.replace('java.lang.', '')\nres = TYPE_DESCRIPTOR.get(atype.lstrip('java.lang'))\nif res is None:\n    if atype[0] == 'L':\n        res = atype[1:-1].replace('/', '.')\n    elif atype[0] == '[':\n        if size is None:\n            res = '%s[]' % get_type(atype[1:])\n        else:\n            res = '%s[%s]' % (get_type(atype[1:]), size)\n    else:\n        res = atype\nreturn res", "path": "androguard\\core\\bytecodes\\dvm.py", "repo_name": "kelwin/apkil", "stars": 58, "license": "None", "language": "python", "size": 80961}
{"docstring": "\"\"\"\n    :param cm: a ClassManager object\n    :type cm: :class:`ClassManager` object\n    :param size: the total size of the buffer\n    :type size: int\n    :param insn: a raw buffer where are the instructions\n    :type insn: string\n    :param idx: a start address in the buffer\n    :type idx: int\n\n    :rtype: a generator of :class:`Instruction` objects\n\"\"\"\n", "func_signal": "def get_instructions(self, cm, size, insn, idx) :\n", "code": "self.odex = cm.get_odex_format()\n\nmax_idx = size * calcsize( '=H' )\n# Get instructions\nwhile idx < max_idx :\n  obj = None\n  classic_instruction = True\n\n  op_value = unpack( '=B', insn[idx] )[0]\n\n  #print \"%x %x\" % (op_value, idx)\n\n  #payload instructions or extented/optimized instructions\n  if (op_value == 0x00 or op_value == 0xff) and ((idx + 2) < max_idx) :\n    op_value = unpack( '=H', insn[idx:idx+2] )[0]\n\n    # payload instructions ?\n    if op_value in DALVIK_OPCODES_PAYLOAD :\n      obj = get_instruction_payload( op_value, insn[idx:] )\n      classic_instruction = False\n\n    elif op_value in DALVIK_OPCODES_EXTENDED_WIDTH :\n      obj = get_extented_instruction( cm, op_value, insn[idx:] )\n      classic_instruction = False\n\n    # optimized instructions ?\n    elif self.odex and (op_value in DALVIK_OPCODES_OPTIMIZED) :\n      obj = get_optimized_instruction( cm, op_value, insn[idx:] )\n      classic_instruction = False\n  \n  # classical instructions\n  if classic_instruction :\n    op_value = unpack( '=B', insn[idx] )[0]\n    obj = get_instruction( cm, op_value, insn[idx:], self.odex)\n\n  # emit instruction\n  yield obj\n  idx = idx + obj.get_length()", "path": "androguard\\core\\bytecodes\\dvm.py", "repo_name": "kelwin/apkil", "stars": 58, "license": "None", "language": "python", "size": 80961}
{"docstring": "\"\"\"\n    Set the instructions\n\n    :param instructions: the list of instructions\n    :type instructions: a list of :class:`Instruction`\n\"\"\"\n", "func_signal": "def set_instructions(self, instructions) :\n", "code": "if self.code == None :\n  return []\nreturn self.code.get_bc().set_instructions(instructions)", "path": "androguard\\core\\bytecodes\\dvm.py", "repo_name": "kelwin/apkil", "stars": 58, "license": "None", "language": "python", "size": 80961}
{"docstring": "\"\"\"\n    Return all methods of a specific class\n\n    :param class_name: the class name\n    :type class_name: string\n\n    :rtype: a list with :class:`EncodedMethod` objects\n\"\"\"\n", "func_signal": "def get_methods_class(self, class_name) :\n", "code": "l = []\nfor i in self.classes.class_def :\n    for j in i.get_methods() :\n        if class_name == j.get_class_name() :\n            l.append( j )\n\nreturn l", "path": "androguard\\core\\bytecodes\\dvm.py", "repo_name": "kelwin/apkil", "stars": 58, "license": "None", "language": "python", "size": 80961}
{"docstring": "\"\"\"\n    Display the basic information about the method\n\"\"\"\n", "func_signal": "def show_info(self) :\n", "code": "bytecode._PrintSubBanner(\"Method Information\") \nbytecode._PrintDefault(\"%s->%s%s [access_flags=%s]\\n\" % ( self.get_class_name(), self.get_name(), self.get_descriptor(), self.get_access_flags_string() ))", "path": "androguard\\core\\bytecodes\\dvm.py", "repo_name": "kelwin/apkil", "stars": 58, "license": "None", "language": "python", "size": 80961}
{"docstring": "# no exceptions !\n", "func_signal": "def determineException(vm, m) :\n", "code": "if m.get_code().get_tries_size() <= 0 :\n    return []\n\nh_off = {}\n\nhandler_catch_list = m.get_code().get_handlers()\n\nfor try_item in m.get_code().get_tries() :\n    offset_handler = try_item.get_handler_off() + handler_catch_list.get_off()\n    if offset_handler in h_off :\n      h_off[ offset_handler ].append( [ try_item ] )\n    else :\n      h_off[ offset_handler ] = []\n      h_off[ offset_handler ].append( [ try_item ] )\n\n#print m.get_name(), \"\\t HANDLER_CATCH_LIST SIZE\", handler_catch_list.size, handler_catch_list.get_offset()\nfor handler_catch in handler_catch_list.get_list() :\n    if handler_catch.get_off() not in h_off :\n        continue\n\n    for i in h_off[ handler_catch.get_off() ] :\n      i.append( handler_catch )\n\nexceptions = []\n#print m.get_name(), h_off\nfor i in h_off :\n  for value in h_off[ i ] :\n    try_value = value[0]\n\n    z = [ try_value.get_start_addr() * 2, (try_value.get_start_addr() * 2) + (try_value.get_insn_count() * 2) - 1 ]\n\n    handler_catch = value[1]\n    if handler_catch.get_size() <= 0 :\n        z.append( [ \"any\", handler_catch.get_catch_all_addr() * 2 ] )\n\n    for handler in handler_catch.get_handlers() :\n        z.append( [ vm.get_cm_type( handler.get_type_idx() ), handler.get_addr() * 2 ] )\n\n    exceptions.append( z )\n\n#print m.get_name(), exceptions \nreturn exceptions", "path": "androguard\\core\\bytecodes\\dvm.py", "repo_name": "kelwin/apkil", "stars": 58, "license": "None", "language": "python", "size": 80961}
{"docstring": "\"\"\"\n    Return the dex (with the modifications) into raw format (fix checksums)\n\n    :rtype: string\n\"\"\"\n", "func_signal": "def save(self) :\n", "code": "l = []\nh = {}\ns = {}\nh_r = {}\n\nidx = 0\nfor i in self.map_list.get_obj() :\n  length = 0\n\n  if isinstance(i, list) :\n    for j in i :\n      if isinstance(j, AnnotationsDirectoryItem) :\n        if idx % 4 != 0 :\n          idx = idx + (idx % 4)\n\n      l.append( j )\n\n      c_length = j.get_length()\n      h[ j ] = idx + length\n      h_r[ idx + length ] = j\n      s[ idx + length ] = c_length\n\n      length += c_length\n\n  else :\n    if isinstance(i, MapList) :\n      if idx % 4 != 0 :\n        idx = idx + (idx % 4)\n\n    l.append( i )\n    h[ i ] = idx\n    h_r[ idx ] = i\n\n    length = i.get_length()\n\n    s[ idx ] = length\n\n  idx += length\n\nself.header.file_size = idx\n\nlast_idx = 0\nfor i in l :\n  idx = h[ i ]\n  i.set_off( h[ i ] )", "path": "androguard\\core\\bytecodes\\dvm.py", "repo_name": "kelwin/apkil", "stars": 58, "license": "None", "language": "python", "size": 80961}
{"docstring": "\"\"\"\n    Return all fields of this class\n\n    :rtype: a list of :class:`EncodedField` objects\n\"\"\"\n", "func_signal": "def get_fields(self) :\n", "code": "if self.class_data_item != None :\n    return self.class_data_item.get_fields()\nreturn []", "path": "androguard\\core\\bytecodes\\dvm.py", "repo_name": "kelwin/apkil", "stars": 58, "license": "None", "language": "python", "size": 80961}
{"docstring": "\"\"\"\n  Return the debug object associated to this method\n\n  :rtype: :class:`DebugInfoItem`\n\"\"\"\n", "func_signal": "def get_debug(self) :\n", "code": "if self.code == None :\n    return None\nreturn self.code.get_debug()", "path": "androguard\\core\\bytecodes\\dvm.py", "repo_name": "kelwin/apkil", "stars": 58, "license": "None", "language": "python", "size": 80961}
{"docstring": "\"\"\"\n  Return all field objects\n\n  :rtype: a list of :class:`EncodedField` objects\n\"\"\"\n", "func_signal": "def get_fields(self) :\n", "code": "l = []\nfor i in self.classes.class_def :\n    for j in i.get_fields() :\n        l.append( j )\nreturn l", "path": "androguard\\core\\bytecodes\\dvm.py", "repo_name": "kelwin/apkil", "stars": 58, "license": "None", "language": "python", "size": 80961}
{"docstring": "# stop ssdp server\n", "func_signal": "def stopService(self):\n", "code": "self.ssdp.leaveGroup(UpnpBase.SSDP_ADDR, interface=self.ip_addr)\nself.ssdp.stopListening()\n\nService.stopService(self)", "path": "airpnp\\device_discovery.py", "repo_name": "provegard/airpnp", "stars": 42, "license": "bsd-3-clause", "language": "python", "size": 309}
{"docstring": "# Deferred.result\n", "func_signal": "def test_is_playing_without_uri(self):\n", "code": "playing = self.avcp.is_playing().result\nself.assertEqual(playing, False)", "path": "test\\test_bridge.py", "repo_name": "provegard/airpnp", "stars": 42, "license": "bsd-3-clause", "language": "python", "size": 309}
{"docstring": "\"\"\"Handle a bye-bye message from a device, or lack of renewal.\"\"\"\n", "func_signal": "def _device_expired(self, udn):\n", "code": "if udn in self._devices:\n    builder = self._builders.pop(udn, None)\n    if builder:\n        builder.cancel()\n    mgr = self._devices.pop(udn)\n    log.msg('Device %s expired or said goodbye' % (mgr.device, ), ll=2)\n    mgr.stop()\n    self.on_device_removed(mgr.device)", "path": "airpnp\\device_discovery.py", "repo_name": "provegard/airpnp", "stars": 42, "license": "bsd-3-clause", "language": "python", "size": 309}
{"docstring": "\"\"\"Handle error that occurred when building a device.\"\"\"\n", "func_signal": "def _device_error(self, fail, udn):\n", "code": "if not fail.check(defer.CancelledError):\n    del self._builders[udn]\n    if fail.check(DeviceRejectedError):\n        device = fail.value.device\n        log.msg('Adding device %s to ignore list, because %s' %\n                (device, fail.getErrorMessage()), ll=2)\n        self._ignored.append(udn)\n    elif fail.check(error.Error):\n        if hasattr(fail, 'url'):\n            msg = \"%s when fetching %s\" % (str(fail.value), fail.url)\n        else:\n            msg = str(fail.value)\n        log.msg('Adding UDN %s to ignore list, because %s' % (udn, msg), ll=2)\n        self._ignored.append(udn)\n    else:\n        log.err(fail, \"Failed to build Device with UDN %s\" % (udn, ))", "path": "airpnp\\device_discovery.py", "repo_name": "provegard/airpnp", "stars": 42, "license": "bsd-3-clause", "language": "python", "size": 309}
{"docstring": "# Setup mock\n", "func_signal": "def test_soap_response(self, pageMock):\n", "code": "pageMock.return_value = defer.succeed(SoapMessage('urn:schemas-upnp-org:service:ConnectionManager:1',\n                                                  'GetCurrentConnectionIDsResponse').tostring())\n\n# Given\nmsg = SoapMessage('urn:schemas-upnp-org:service:ConnectionManager:1', 'GetCurrentConnectionIDs')\n\n# When\nd = send_soap_message_deferred('http://www.dummy.com', msg)\n\n# Then\nresponse = d.result\nself.assertEqual(response.__class__, SoapMessage)\nself.assertEqual(response.get_header(),\n                 '\"urn:schemas-upnp-org:service:ConnectionManager:1#GetCurrentConnectionIDsResponse\"')", "path": "test\\test_util.py", "repo_name": "provegard/airpnp", "stars": 42, "license": "bsd-3-clause", "language": "python", "size": 309}
{"docstring": "\"\"\"Initialize a device builder.\n\nArguments:\nsoap_sender -- passed to the created Device object\nfilter_     -- optional callable that receives the created device to\n               determine if the builder should continue with service\n               initialization. Should return a tuple of (bool, string),\n               where the bool is the continue flag, and the string is\n               a reason in case the continue flag is False.\n\n\"\"\"\n", "func_signal": "def __init__(self, soap_sender, filter_=None):\n", "code": "self._filter = filter_\nself._soap_sender = soap_sender", "path": "airpnp\\device_builder.py", "repo_name": "provegard/airpnp", "stars": 42, "license": "bsd-3-clause", "language": "python", "size": 309}
{"docstring": "\"\"\" Parse npt time formatted string and return in second.\n    S+(.sss)\n    H+:MM:SS(.sss)\n\"\"\"\n# S+(.sss)\n", "func_signal": "def parse_npt(npt_time):\n", "code": "m = nptsecref.match(npt_time)\nif m:\n    return float(npt_time)\n\n# H+:MM:SS(.sss)\nm = npthmsref.match(npt_time)\nif not m:\n    raise ValueError('invalid npt-time: %s' % npt_time)\n\nhour = int(m.group('hour'))\nmin = int(m.group('min'))\nsec = int(m.group('sec'))\n\nif not ((0 <= min <= 59) and (0 <= sec <= 59)):\n    raise ValueError('invalid npt-time: %s' % npt_time)\n\nsec = float((hour * 60 + min) * 60) + sec\n\nmsec = m.group('msec')\nif msec:\n    sec += float('0.' + msec)\n\nreturn sec", "path": "airpnp\\upnp.py", "repo_name": "provegard/airpnp", "stars": 42, "license": "bsd-3-clause", "language": "python", "size": 309}
{"docstring": "\"\"\"Handle a notification message from a device.\"\"\"\n", "func_signal": "def _handle_notify(self, umessage):\n", "code": "udn = umessage.get_udn()\nif not udn in self._ignored:\n    nts = umessage.get_notification_sub_type()\n    if nts == 'ssdp:alive':\n        self._handle_response(umessage)\n    elif nts == 'ssdp:byebye':\n        self._device_expired(umessage.get_udn())", "path": "airpnp\\device_discovery.py", "repo_name": "provegard/airpnp", "stars": 42, "license": "bsd-3-clause", "language": "python", "size": 309}
{"docstring": "\"\"\"Build a Device object from a remote location.\n\nArguments:\nlocation -- the HTTP URL where the root device XML can be found\n\nReturn a Deferred which will callback when the Device object is ready.\nThe caller must add an errback to handle errors raised during the build\nprocess.\n\n\"\"\"\n", "func_signal": "def build(self, location):\n", "code": "d = defer.succeed(location)\n\n# get the device XML\nd.addCallback(client.getPage, timeout=5)\n\n# parse it to an element\nd.addCallback(ET.fromstring)\n\n# create a new Device object\nd.addCallback(Device, location)\n\n# check if the device passes the filter\nd.addCallback(self._check_filter)\n\n# initialize services\nd.addCallback(self._init_services)\n\n# error handling for the service initialization\nd.addErrback(reraise_with_url, location)\n\n# make sure the Device object is returned\nd.addCallback(self._get_device)\n\nreturn d", "path": "airpnp\\device_builder.py", "repo_name": "provegard/airpnp", "stars": 42, "license": "bsd-3-clause", "language": "python", "size": 309}
{"docstring": "\"\"\"Handle completion of device building.\"\"\"\n", "func_signal": "def _device_finished(self, device, umessage):\n", "code": "mgr = DeviceManager(device, self._device_expired)\nself._devices[device.UDN] = mgr\n\n# Start the device container timer\nmgr.touch(umessage)\n\n# Publish the device\nself.on_device_found(device)", "path": "airpnp\\device_discovery.py", "repo_name": "provegard/airpnp", "stars": 42, "license": "bsd-3-clause", "language": "python", "size": 309}
{"docstring": "# the device must have an approved device type\n", "func_signal": "def _is_device_interesting(self, device):\n", "code": "compat = [d for d in self._dev_types \n          if are_service_types_compatible(d, device.deviceType)]\nif self._dev_types and not compat:\n    reason = \"device type %s is not recognized\" % (device.deviceType, )\n    return False, reason\n\n# the device must contain all required services\nmissing = []\nfor rs in self._req_services:\n    matches = [s.serviceType for s in device\n               if are_service_types_compatible(rs, s.serviceType)]\n    if not matches:\n        missing.append(rs)\nif missing:\n    reason = \"services %s are missing\" % (missing, )\n    return False, reason\n\n# passed all tests, device is interesting\nreturn True, None", "path": "airpnp\\device_discovery.py", "repo_name": "provegard/airpnp", "stars": 42, "license": "bsd-3-clause", "language": "python", "size": 309}
{"docstring": "# DeferredList errbacks with a FirstError failure, from which we can\n# get the real failure.\n", "func_signal": "def late_error(self, fail, request):\n", "code": "if isinstance(fail, defer.FirstError):\n    fail = fail.subFailure\nlog.err(fail, \"AirPlay request handling failed.\")\nif not request._disconnected:\n    try:\n        # must set content-length to avoid chunked encoding\n        request.setHeader('content-length', 0)\n        request.setResponseCode(501)\n        request.finish()\n    except:\n        log.err(None, \"Failed to write error response for AirPlay request.\")", "path": "airpnp\\airplayserver.py", "repo_name": "provegard/airpnp", "stars": 42, "license": "bsd-3-clause", "language": "python", "size": 309}
{"docstring": "# Setup mock\n", "func_signal": "def test_soap_error_on_500_response(self, pageMock):\n", "code": "f = failure.Failure(error.Error(http.INTERNAL_SERVER_ERROR, 'Internal Error', \n                                SoapError(501, 'Action Failed').tostring()))\npageMock.return_value = defer.fail(f)\n\n# Given\nmsg = SoapMessage('urn:schemas-upnp-org:service:ConnectionManager:1', 'GetCurrentConnectionIDs')\n\n# When\nd = send_soap_message_deferred('http://www.dummy.com', msg)\n\n# Then\nresponse = d.result\nself.assertEqual(response.__class__, SoapError)\nself.assertEqual(response.code, '501')", "path": "test\\test_util.py", "repo_name": "provegard/airpnp", "stars": 42, "license": "bsd-3-clause", "language": "python", "size": 309}
{"docstring": "# Setup mock\n", "func_signal": "def test_unrecognized_error_is_reraised(self, pageMock):\n", "code": "f = failure.Failure(error.Error(http.NOT_FOUND, 'Not Found', 'Not Found'))\npageMock.return_value = defer.fail(f)\n\n# Given\nmsg = SoapMessage('urn:schemas-upnp-org:service:ConnectionManager:1', 'GetCurrentConnectionIDs')\n\n# When\nd = send_soap_message_deferred('http://www.dummy.com', msg)\n\n# Then\nf = d.result\nself.assertTrue(f.check(error.Error))", "path": "test\\test_util.py", "repo_name": "provegard/airpnp", "stars": 42, "license": "bsd-3-clause", "language": "python", "size": 309}
{"docstring": "# Setup AVTransport service\n", "func_signal": "def setUp(self):\n", "code": "self.avtransport = mock.Mock()\nself.avtransport.serviceId = 'urn:upnp-org:serviceId:AVTransport'\nself.avtransport.serviceType = 'urn:schemas-upnp-org:service:AVTransport:1'\n\n# Setup ConnectionManager service\nself.connmgr = mock.Mock()\nself.connmgr.serviceId = 'urn:upnp-org:serviceId:ConnectionManager'\nself.connmgr.serviceType = 'urn:schemas-upnp-org:service:ConnectionManager:1'\n\ndef gsbyid(id):\n    all = [self.avtransport, self.conngr]\n    match = [s for s in all if s.serviceId == id]\n    if len(match) == 1:\n        return match[0]\n    else:\n        raise ValueError(\"Unknown or ambiguous id: \" + id)\ndevice = mock.MagicMock()\ndevice.__getitem__ = mock.Mock(side_effect=gsbyid)\ndevice.__iter__.return_value = [self.avtransport, self.connmgr]\nself.avcp = AVControlPoint(device, None, \"127.0.0.1\")\n\n# mock away instance ID business since these methods check for \n# attributes that will be auto-added by the mock service.\nself.avcp.allocate_instance_id = mock.Mock(return_value=\"0\")\nself.avcp.release_instance_id = mock.Mock()\n\n# suppress logging\nself.avcp.msg = lambda *args: None", "path": "test\\test_bridge.py", "repo_name": "provegard/airpnp", "stars": 42, "license": "bsd-3-clause", "language": "python", "size": 309}
{"docstring": "# Given\n", "func_signal": "def test_request_headers(self, pageMock):\n", "code": "msg = SoapMessage('urn:schemas-upnp-org:service:ConnectionManager:1', 'GetCurrentConnectionIDs')\n\n# When\nsend_soap_message_deferred('http://www.dummy.com', msg)\n\n# Then\nheaders = pageMock.call_args[1]['headers']\nself.assertEqual(headers['Content-Type'], 'text/xml; charset=\"utf-8\"')\nself.assertEqual(headers['Soapaction'],\n                 '\"urn:schemas-upnp-org:service:ConnectionManager:1#GetCurrentConnectionIDs\"')", "path": "test\\test_util.py", "repo_name": "provegard/airpnp", "stars": 42, "license": "bsd-3-clause", "language": "python", "size": 309}
{"docstring": "\"\"\"Handle response to M-SEARCH message.\"\"\"\n", "func_signal": "def _handle_response(self, umessage):\n", "code": "udn = umessage.get_udn()\nif udn and not udn in self._ignored:\n    mgr = self._devices.get(udn)\n    if mgr:\n        mgr.touch(umessage)\n    elif not udn in self._builders:\n        self._build_device(umessage)", "path": "airpnp\\device_discovery.py", "repo_name": "provegard/airpnp", "stars": 42, "license": "bsd-3-clause", "language": "python", "size": 309}
{"docstring": "\"\"\" Parse duration formatted string and return in second.\n    ['+'|'-']H+:MM:SS[.F0+|.F0/F1]\n\"\"\"\n", "func_signal": "def parse_duration(text):\n", "code": "m = durationref.match(text)\nif m == None:\n    raise ValueError('invalid format')\n\nhour = int(m.group('hour'))\nminute = int(m.group('minute'))\nsecond = int(m.group('second'))\nif minute >= 60 or second >= 60:\n    raise ValueError('invalid format')\n\nmsec = m.group('msec')\nif msec:\n    msec = float('0.' + msec)\nelse:\n    F1 = m.group('F1')\n    if F1:\n        F1 = float(F1)\n        if F1 == 0:\n            raise ValueError('invalid format')\n        F0 = float(m.group('F0'))\n        if F0 >= F1:\n            raise ValueError('invalid format')\n        msec = F0 / F1\n    else:\n        msec = 0\n\na = text[0] == '-' and -1 or 1\nreturn a * (((hour * 60 + minute) * 60) + second + msec)", "path": "airpnp\\upnp.py", "repo_name": "provegard/airpnp", "stars": 42, "license": "bsd-3-clause", "language": "python", "size": 309}
{"docstring": "\"\"\"Start or reset the device timer based on UPnP HTTP headers.\n\nIf the expire timer hasn't been started before, it will be when this\nmethod is called. Otherwise, the timer will be reset. The timer time\nis taken from the \"max-age\" directive of the \"CACHE-CONTROL\" header.\n\nArguments:\nheaders -- dictionary of UPnP HTTP headers\n\n\"\"\"\n", "func_signal": "def touch(self, umessage):\n", "code": "seconds = get_max_age(umessage.headers) # TODO\nif seconds:\n    udn = umessage.get_udn()\n    timer = self._expire_timer\n    if timer and timer.active():\n        timer.reset(seconds)\n    else:\n        newtimer = reactor.callLater(seconds, self._device_expired,\n                                     udn)\n        self._expire_timer = newtimer", "path": "airpnp\\device_discovery.py", "repo_name": "provegard/airpnp", "stars": 42, "license": "bsd-3-clause", "language": "python", "size": 309}
{"docstring": "\"\"\"Start building a device if it seems to be a proper one.\"\"\"\n", "func_signal": "def _build_device(self, umessage):\n", "code": "acttype = umessage.get_type()\ncompat = [s for s in self._sn_types \n          if are_service_types_compatible(s, acttype)]\nif compat:\n    udn = umessage.get_udn()\n    builder = DeviceBuilder(self._send_soap_message,\n                            self._is_device_interesting)\n    d = builder.build(umessage.get_location())\n    \n    d.addCallback(self._device_finished, umessage)\n    d.addErrback(self._device_error, udn)\n\n    log.msg(\"Starting build of device with UDN = %s\" % (udn, ), ll=3)\n    self._builders[udn] = d", "path": "airpnp\\device_discovery.py", "repo_name": "provegard/airpnp", "stars": 42, "license": "bsd-3-clause", "language": "python", "size": 309}
