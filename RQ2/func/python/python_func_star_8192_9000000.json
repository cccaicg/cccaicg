{"docstring": "\"\"\"Get the project root directory.\n\nWe require that all commands are run from the project root, i.e. the\ndirectory that contains setup.py, setup.cfg, and versioneer.py .\n\"\"\"\n", "func_signal": "def get_root():\n", "code": "root = os.path.realpath(os.path.abspath(os.getcwd()))\nsetup_py = os.path.join(root, \"setup.py\")\nversioneer_py = os.path.join(root, \"versioneer.py\")\nif not (os.path.exists(setup_py) or os.path.exists(versioneer_py)):\n    # allow 'python path/to/setup.py COMMAND'\n    root = os.path.dirname(os.path.realpath(os.path.abspath(sys.argv[0])))\n    setup_py = os.path.join(root, \"setup.py\")\n    versioneer_py = os.path.join(root, \"versioneer.py\")\nif not (os.path.exists(setup_py) or os.path.exists(versioneer_py)):\n    err = (\"Versioneer was unable to run the project root directory. \"\n           \"Versioneer requires setup.py to be executed from \"\n           \"its immediate directory (like 'python setup.py COMMAND'), \"\n           \"or in a way that lets it use sys.argv[0] to find the root \"\n           \"(like 'python path/to/setup.py COMMAND').\")\n    raise VersioneerBadRootError(err)\ntry:\n    # Certain runtime workflows (setup.py install/develop in a setuptools\n    # tree) execute all dependencies in a single python process, so\n    # \"versioneer\" may be imported multiple times, and python's shared\n    # module-import table will cache the first one. So we can't use\n    # os.path.dirname(__file__), as that will find whichever\n    # versioneer.py was first imported, even in later projects.\n    me = os.path.realpath(os.path.abspath(__file__))\n    me_dir = os.path.normcase(os.path.splitext(me)[0])\n    vsr_dir = os.path.normcase(os.path.splitext(versioneer_py)[0])\n    if me_dir != vsr_dir:\n        print(\"Warning: build in %s is using versioneer.py from %s\"\n              % (os.path.dirname(me), versioneer_py))\nexcept NameError:\n    pass\nreturn root", "path": "magic-wormhole/versioneer.py", "commit_date": "2018-11-23 00:00:00", "repo_name": "magic-wormhole/magic-wormhole", "stars": 17697, "license": "mit", "language": "python", "size": 3237}
{"docstring": "# first, pull out all the relays, we'll connect to them later\n", "func_signal": "def _use_hints(self, hints):\n", "code": "relays = []\ndirect = defaultdict(list)\nfor h in hints:\n    if isinstance(h, RelayV1Hint):\n        relays.append(h)\n    else:\n        direct[h.priority].append(h)\ndelay = 0.0\nmade_direct = False\npriorities = sorted(set(direct.keys()), reverse=True)\nfor p in priorities:\n    for h in direct[p]:\n        if isinstance(h, TorTCPV1Hint) and not self._tor:\n            continue\n        self._schedule_connection(delay, h, is_relay=False)\n        made_direct = True\n        # Make all direct connections immediately. Later, we'll change\n        # the add_candidate() function to look at the priority when\n        # deciding whether to accept a successful connection or not,\n        # and it can wait for more options if it sees a higher-priority\n        # one still running. But if we bail on that, we might consider\n        # putting an inter-direct-hint delay here to influence the\n        # process.\n        # delay += 1.0\n\nif made_direct and not self._no_listen:\n    # Prefer direct connections by stalling relay connections by a\n    # few seconds. We don't wait until direct connections have\n    # failed, because many direct hints will be to unused\n    # local-network IP address, which won't answer, and can take the\n    # full 30s TCP timeout to fail.\n    #\n    # If we didn't make any direct connections, or we're using\n    # --no-listen, then we're probably going to have to use the\n    # relay, so don't delay it at all.\n    delay += self.RELAY_DELAY\n\n# It might be nice to wire this so that a failure in the direct hints\n# causes the relay hints to be used right away (fast failover). But\n# none of our current use cases would take advantage of that: if we\n# have any viable direct hints, then they're either going to succeed\n# quickly or hang for a long time.\nfor r in relays:\n    for h in r.hints:\n        self._schedule_connection(delay, h, is_relay=True)\n# TODO:\n# if not contenders:\n#    raise TransitError(\"No contenders for connection\")", "path": "magic-wormhole/src/wormhole/_dilation/connector.py", "commit_date": "2020-04-12 00:00:00", "repo_name": "magic-wormhole/magic-wormhole", "stars": 17697, "license": "mit", "language": "python", "size": 3237}
{"docstring": "\"\"\"Store f in HANDLERS[vcs][method].\"\"\"\n", "func_signal": "def decorate(f):\n", "code": "if vcs not in HANDLERS:\n    HANDLERS[vcs] = {}\nHANDLERS[vcs][method] = f\nreturn f", "path": "magic-wormhole/versioneer.py", "commit_date": "2018-11-23 00:00:00", "repo_name": "magic-wormhole/magic-wormhole", "stars": 17697, "license": "mit", "language": "python", "size": 3237}
{"docstring": "\"\"\"Get the project version from whatever source is available.\n\nReturns dict with two keys: 'version' and 'full'.\n\"\"\"\n", "func_signal": "def get_versions(verbose=False):\n", "code": "if \"versioneer\" in sys.modules:\n    # see the discussion in cmdclass.py:get_cmdclass()\n    del sys.modules[\"versioneer\"]\n\nroot = get_root()\ncfg = get_config_from_root(root)\n\nassert cfg.VCS is not None, \"please set [versioneer]VCS= in setup.cfg\"\nhandlers = HANDLERS.get(cfg.VCS)\nassert handlers, \"unrecognized VCS '%s'\" % cfg.VCS\nverbose = verbose or cfg.verbose\nassert cfg.versionfile_source is not None, \\\n    \"please set versioneer.versionfile_source\"\nassert cfg.tag_prefix is not None, \"please set versioneer.tag_prefix\"\n\nversionfile_abs = os.path.join(root, cfg.versionfile_source)\n\n# extract version from first of: _version.py, VCS command (e.g. 'git\n# describe'), parentdir. This is meant to work for developers using a\n# source checkout, for users of a tarball created by 'setup.py sdist',\n# and for users of a tarball/zipball created by 'git archive' or github's\n# download-from-tag feature or the equivalent in other VCSes.\n\nget_keywords_f = handlers.get(\"get_keywords\")\nfrom_keywords_f = handlers.get(\"keywords\")\nif get_keywords_f and from_keywords_f:\n    try:\n        keywords = get_keywords_f(versionfile_abs)\n        ver = from_keywords_f(keywords, cfg.tag_prefix, verbose)\n        if verbose:\n            print(\"got version from expanded keyword %s\" % ver)\n        return ver\n    except NotThisMethod:\n        pass\n\ntry:\n    ver = versions_from_file(versionfile_abs)\n    if verbose:\n        print(\"got version from file %s %s\" % (versionfile_abs, ver))\n    return ver\nexcept NotThisMethod:\n    pass\n\nfrom_vcs_f = handlers.get(\"pieces_from_vcs\")\nif from_vcs_f:\n    try:\n        pieces = from_vcs_f(cfg.tag_prefix, root, verbose)\n        ver = render(pieces, cfg.style)\n        if verbose:\n            print(\"got version from VCS %s\" % ver)\n        return ver\n    except NotThisMethod:\n        pass\n\ntry:\n    if cfg.parentdir_prefix:\n        ver = versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n        if verbose:\n            print(\"got version from parentdir %s\" % ver)\n        return ver\nexcept NotThisMethod:\n    pass\n\nif verbose:\n    print(\"unable to compute version\")\n\nreturn {\"version\": \"0+unknown\", \"full-revisionid\": None,\n        \"dirty\": None, \"error\": \"unable to compute version\",\n        \"date\": None}", "path": "magic-wormhole/versioneer.py", "commit_date": "2018-11-23 00:00:00", "repo_name": "magic-wormhole/magic-wormhole", "stars": 17697, "license": "mit", "language": "python", "size": 3237}
{"docstring": "# set up a server that serves web/ at the root, plus a /data.json built\n# from {timeline}. Quit when it fetches /done .\n", "func_signal": "def web():\n", "code": "from twisted.web import resource, static, server\nfrom twisted.internet import reactor, endpoints\nep = endpoints.serverFromString(reactor, \"tcp:8066:interface=127.0.0.1\")\nroot = static.File(web_root)\nroot.putChild(\"data.json\", static.Data(json.dumps(data).encode(\"utf-8\"),\n                                       \"application/json\"))\nroot.putChild(\"lib\", static.File(lib_root))\nclass Shutdown(resource.Resource):\n    def render_GET(self, request):\n        #print(\"timeline ready, server shutting down\")\n        #reactor.stop()\n        return \"shutting down\"\nroot.putChild(\"done\", Shutdown())\nsite = server.Site(root)\nep.listen(site)\nimport webbrowser\ndef launch_browser():\n    webbrowser.open(\"http://localhost:%d/timeline.html\" % 8066)\n    print(\"browser opened, waiting for shutdown\")\nreactor.callLater(0, launch_browser)\nreactor.run()", "path": "magic-wormhole/misc/dump-timing.py", "commit_date": "2018-01-01 00:00:00", "repo_name": "magic-wormhole/magic-wormhole", "stars": 17697, "license": "mit", "language": "python", "size": 3237}
{"docstring": "\"\"\"TAG-DISTANCE-gHEX[-dirty].\n\nLike 'git describe --tags --dirty --always -long'.\nThe distance/hash is unconditional.\n\nExceptions:\n1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n\"\"\"\n", "func_signal": "def render_git_describe_long(pieces):\n", "code": "if pieces[\"closest-tag\"]:\n    rendered = pieces[\"closest-tag\"]\n    rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])\nelse:\n    # exception #1\n    rendered = pieces[\"short\"]\nif pieces[\"dirty\"]:\n    rendered += \"-dirty\"\nreturn rendered", "path": "magic-wormhole/versioneer.py", "commit_date": "2018-11-23 00:00:00", "repo_name": "magic-wormhole/magic-wormhole", "stars": 17697, "license": "mit", "language": "python", "size": 3237}
{"docstring": "\"\"\"Validate the contents of setup.py against Versioneer's expectations.\"\"\"\n", "func_signal": "def scan_setup_py():\n", "code": "found = set()\nsetters = False\nerrors = 0\nwith open(\"setup.py\", \"r\") as f:\n    for line in f.readlines():\n        if \"import versioneer\" in line:\n            found.add(\"import\")\n        if \"versioneer.get_cmdclass()\" in line:\n            found.add(\"cmdclass\")\n        if \"versioneer.get_version()\" in line:\n            found.add(\"get_version\")\n        if \"versioneer.VCS\" in line:\n            setters = True\n        if \"versioneer.versionfile_source\" in line:\n            setters = True\nif len(found) != 3:\n    print(\"\")\n    print(\"Your setup.py appears to be missing some important items\")\n    print(\"(but I might be wrong). Please make sure it has something\")\n    print(\"roughly like the following:\")\n    print(\"\")\n    print(\" import versioneer\")\n    print(\" setup( version=versioneer.get_version(),\")\n    print(\"        cmdclass=versioneer.get_cmdclass(),  ...)\")\n    print(\"\")\n    errors += 1\nif setters:\n    print(\"You should remove lines like 'versioneer.VCS = ' and\")\n    print(\"'versioneer.versionfile_source = ' . This configuration\")\n    print(\"now lives in setup.cfg, and should be removed from setup.py\")\n    print(\"\")\n    errors += 1\nreturn errors", "path": "magic-wormhole/versioneer.py", "commit_date": "2018-11-23 00:00:00", "repo_name": "magic-wormhole/magic-wormhole", "stars": 17697, "license": "mit", "language": "python", "size": 3237}
{"docstring": "\"\"\"Get the custom setuptools/distutils subclasses used by Versioneer.\"\"\"\n", "func_signal": "def get_cmdclass():\n", "code": "if \"versioneer\" in sys.modules:\n    del sys.modules[\"versioneer\"]\n    # this fixes the \"python setup.py develop\" case (also 'install' and\n    # 'easy_install .'), in which subdependencies of the main project are\n    # built (using setup.py bdist_egg) in the same python process. Assume\n    # a main project A and a dependency B, which use different versions\n    # of Versioneer. A's setup.py imports A's Versioneer, leaving it in\n    # sys.modules by the time B's setup.py is executed, causing B to run\n    # with the wrong versioneer. Setuptools wraps the sub-dep builds in a\n    # sandbox that restores sys.modules to it's pre-build state, so the\n    # parent is protected against the child's \"import versioneer\". By\n    # removing ourselves from sys.modules here, before the child build\n    # happens, we protect the child from the parent's versioneer too.\n    # Also see https://github.com/warner/python-versioneer/issues/52\n\ncmds = {}\n\n# we add \"version\" to both distutils and setuptools\nfrom distutils.core import Command\n\nclass cmd_version(Command):\n    description = \"report generated version string\"\n    user_options = []\n    boolean_options = []\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        vers = get_versions(verbose=True)\n        print(\"Version: %s\" % vers[\"version\"])\n        print(\" full-revisionid: %s\" % vers.get(\"full-revisionid\"))\n        print(\" dirty: %s\" % vers.get(\"dirty\"))\n        print(\" date: %s\" % vers.get(\"date\"))\n        if vers[\"error\"]:\n            print(\" error: %s\" % vers[\"error\"])\ncmds[\"version\"] = cmd_version\n\n# we override \"build_py\" in both distutils and setuptools\n#\n# most invocation pathways end up running build_py:\n#  distutils/build -> build_py\n#  distutils/install -> distutils/build ->..\n#  setuptools/bdist_wheel -> distutils/install ->..\n#  setuptools/bdist_egg -> distutils/install_lib -> build_py\n#  setuptools/install -> bdist_egg ->..\n#  setuptools/develop -> ?\n#  pip install:\n#   copies source tree to a tempdir before running egg_info/etc\n#   if .git isn't copied too, 'git describe' will fail\n#   then does setup.py bdist_wheel, or sometimes setup.py install\n#  setup.py egg_info -> ?\n\n# we override different \"build_py\" commands for both environments\nif \"setuptools\" in sys.modules:\n    from setuptools.command.build_py import build_py as _build_py\nelse:\n    from distutils.command.build_py import build_py as _build_py\n\nclass cmd_build_py(_build_py):\n    def run(self):\n        root = get_root()\n        cfg = get_config_from_root(root)\n        versions = get_versions()\n        _build_py.run(self)\n        # now locate _version.py in the new build/ directory and replace\n        # it with an updated value\n        if cfg.versionfile_build:\n            target_versionfile = os.path.join(self.build_lib,\n                                              cfg.versionfile_build)\n            print(\"UPDATING %s\" % target_versionfile)\n            write_to_version_file(target_versionfile, versions)\ncmds[\"build_py\"] = cmd_build_py\n\nif \"cx_Freeze\" in sys.modules:  # cx_freeze enabled?\n    from cx_Freeze.dist import build_exe as _build_exe\n    # nczeczulin reports that py2exe won't like the pep440-style string\n    # as FILEVERSION, but it can be used for PRODUCTVERSION, e.g.\n    # setup(console=[{\n    #   \"version\": versioneer.get_version().split(\"+\", 1)[0], # FILEVERSION\n    #   \"product_version\": versioneer.get_version(),\n    #   ...\n\n    class cmd_build_exe(_build_exe):\n        def run(self):\n            root = get_root()\n            cfg = get_config_from_root(root)\n            versions = get_versions()\n            target_versionfile = cfg.versionfile_source\n            print(\"UPDATING %s\" % target_versionfile)\n            write_to_version_file(target_versionfile, versions)\n\n            _build_exe.run(self)\n            os.unlink(target_versionfile)\n            with open(cfg.versionfile_source, \"w\") as f:\n                LONG = LONG_VERSION_PY[cfg.VCS]\n                f.write(LONG %\n                        {\"DOLLAR\": \"$\",\n                         \"STYLE\": cfg.style,\n                         \"TAG_PREFIX\": cfg.tag_prefix,\n                         \"PARENTDIR_PREFIX\": cfg.parentdir_prefix,\n                         \"VERSIONFILE_SOURCE\": cfg.versionfile_source,\n                         })\n    cmds[\"build_exe\"] = cmd_build_exe\n    del cmds[\"build_py\"]\n\nif 'py2exe' in sys.modules:  # py2exe enabled?\n    try:\n        from py2exe.distutils_buildexe import py2exe as _py2exe  # py3\n    except ImportError:\n        from py2exe.build_exe import py2exe as _py2exe  # py2\n\n    class cmd_py2exe(_py2exe):\n        def run(self):\n            root = get_root()\n            cfg = get_config_from_root(root)\n            versions = get_versions()\n            target_versionfile = cfg.versionfile_source\n            print(\"UPDATING %s\" % target_versionfile)\n            write_to_version_file(target_versionfile, versions)\n\n            _py2exe.run(self)\n            os.unlink(target_versionfile)\n            with open(cfg.versionfile_source, \"w\") as f:\n                LONG = LONG_VERSION_PY[cfg.VCS]\n                f.write(LONG %\n                        {\"DOLLAR\": \"$\",\n                         \"STYLE\": cfg.style,\n                         \"TAG_PREFIX\": cfg.tag_prefix,\n                         \"PARENTDIR_PREFIX\": cfg.parentdir_prefix,\n                         \"VERSIONFILE_SOURCE\": cfg.versionfile_source,\n                         })\n    cmds[\"py2exe\"] = cmd_py2exe\n\n# we override different \"sdist\" commands for both environments\nif \"setuptools\" in sys.modules:\n    from setuptools.command.sdist import sdist as _sdist\nelse:\n    from distutils.command.sdist import sdist as _sdist\n\nclass cmd_sdist(_sdist):\n    def run(self):\n        versions = get_versions()\n        self._versioneer_generated_versions = versions\n        # unless we update this, the command will keep using the old\n        # version\n        self.distribution.metadata.version = versions[\"version\"]\n        return _sdist.run(self)\n\n    def make_release_tree(self, base_dir, files):\n        root = get_root()\n        cfg = get_config_from_root(root)\n        _sdist.make_release_tree(self, base_dir, files)\n        # now locate _version.py in the new base_dir directory\n        # (remembering that it may be a hardlink) and replace it with an\n        # updated value\n        target_versionfile = os.path.join(base_dir, cfg.versionfile_source)\n        print(\"UPDATING %s\" % target_versionfile)\n        write_to_version_file(target_versionfile,\n                              self._versioneer_generated_versions)\ncmds[\"sdist\"] = cmd_sdist\n\nreturn cmds", "path": "magic-wormhole/versioneer.py", "commit_date": "2018-11-23 00:00:00", "repo_name": "magic-wormhole/magic-wormhole", "stars": 17697, "license": "mit", "language": "python", "size": 3237}
{"docstring": "\"\"\"Get version information from git keywords.\"\"\"\n", "func_signal": "def git_versions_from_keywords(keywords, tag_prefix, verbose):\n", "code": "if not keywords:\n    raise NotThisMethod(\"no keywords at all, weird\")\ndate = keywords.get(\"date\")\nif date is not None:\n    # git-2.2.0 added \"%cI\", which expands to an ISO-8601 -compliant\n    # datestamp. However we prefer \"%ci\" (which expands to an \"ISO-8601\n    # -like\" string, which we must then edit to make compliant), because\n    # it's been around since git-1.5.3, and it's too difficult to\n    # discover which version we're using, or to work around using an\n    # older one.\n    date = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\nrefnames = keywords[\"refnames\"].strip()\nif refnames.startswith(\"$Format\"):\n    if verbose:\n        print(\"keywords are unexpanded, not using\")\n    raise NotThisMethod(\"unexpanded keywords, not a git-archive tarball\")\nrefs = set([r.strip() for r in refnames.strip(\"()\").split(\",\")])\n# starting in git-1.8.3, tags are listed as \"tag: foo-1.0\" instead of\n# just \"foo-1.0\". If we see a \"tag: \" prefix, prefer those.\nTAG = \"tag: \"\ntags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])\nif not tags:\n    # Either we're using git < 1.8.3, or there really are no tags. We use\n    # a heuristic: assume all version tags have a digit. The old git %d\n    # expansion behaves like git log --decorate=short and strips out the\n    # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n    # between branches and tags. By ignoring refnames without digits, we\n    # filter out many common branch names like \"release\" and\n    # \"stabilization\", as well as \"HEAD\" and \"master\".\n    tags = set([r for r in refs if re.search(r'\\d', r)])\n    if verbose:\n        print(\"discarding '%s', no digits\" % \",\".join(refs - tags))\nif verbose:\n    print(\"likely tags: %s\" % \",\".join(sorted(tags)))\nfor ref in sorted(tags):\n    # sorting will prefer e.g. \"2.0\" over \"2.0rc1\"\n    if ref.startswith(tag_prefix):\n        r = ref[len(tag_prefix):]\n        if verbose:\n            print(\"picking %s\" % r)\n        return {\"version\": r,\n                \"full-revisionid\": keywords[\"full\"].strip(),\n                \"dirty\": False, \"error\": None,\n                \"date\": date}\n# no suitable tags, so version is \"0+unknown\", but full hex is still there\nif verbose:\n    print(\"no suitable tags, using unknown + full revision id\")\nreturn {\"version\": \"0+unknown\",\n        \"full-revisionid\": keywords[\"full\"].strip(),\n        \"dirty\": False, \"error\": \"no suitable tags\", \"date\": None}", "path": "magic-wormhole/versioneer.py", "commit_date": "2018-11-23 00:00:00", "repo_name": "magic-wormhole/magic-wormhole", "stars": 17697, "license": "mit", "language": "python", "size": 3237}
{"docstring": "\"\"\"Git-specific installation logic for Versioneer.\n\nFor Git, this means creating/changing .gitattributes to mark _version.py\nfor export-subst keyword substitution.\n\"\"\"\n", "func_signal": "def do_vcs_install(manifest_in, versionfile_source, ipy):\n", "code": "GITS = [\"git\"]\nif sys.platform == \"win32\":\n    GITS = [\"git.cmd\", \"git.exe\"]\nfiles = [manifest_in, versionfile_source]\nif ipy:\n    files.append(ipy)\ntry:\n    me = __file__\n    if me.endswith(\".pyc\") or me.endswith(\".pyo\"):\n        me = os.path.splitext(me)[0] + \".py\"\n    versioneer_file = os.path.relpath(me)\nexcept NameError:\n    versioneer_file = \"versioneer.py\"\nfiles.append(versioneer_file)\npresent = False\ntry:\n    f = open(\".gitattributes\", \"r\")\n    for line in f.readlines():\n        if line.strip().startswith(versionfile_source):\n            if \"export-subst\" in line.strip().split()[1:]:\n                present = True\n    f.close()\nexcept EnvironmentError:\n    pass\nif not present:\n    f = open(\".gitattributes\", \"a+\")\n    f.write(\"%s export-subst\\n\" % versionfile_source)\n    f.close()\n    files.append(\".gitattributes\")\nrun_command(GITS, [\"add\", \"--\"] + files)", "path": "magic-wormhole/versioneer.py", "commit_date": "2018-11-23 00:00:00", "repo_name": "magic-wormhole/magic-wormhole", "stars": 17697, "license": "mit", "language": "python", "size": 3237}
{"docstring": "\"\"\"Extract version information from the given file.\"\"\"\n# the code embedded in _version.py can just fetch the value of these\n# keywords. When used from setup.py, we don't want to import _version.py,\n# so we do it with a regexp instead. This function is not used from\n# _version.py.\n", "func_signal": "def git_get_keywords(versionfile_abs):\n", "code": "keywords = {}\ntry:\n    f = open(versionfile_abs, \"r\")\n    for line in f.readlines():\n        if line.strip().startswith(\"git_refnames =\"):\n            mo = re.search(r'=\\s*\"(.*)\"', line)\n            if mo:\n                keywords[\"refnames\"] = mo.group(1)\n        if line.strip().startswith(\"git_full =\"):\n            mo = re.search(r'=\\s*\"(.*)\"', line)\n            if mo:\n                keywords[\"full\"] = mo.group(1)\n        if line.strip().startswith(\"git_date =\"):\n            mo = re.search(r'=\\s*\"(.*)\"', line)\n            if mo:\n                keywords[\"date\"] = mo.group(1)\n    f.close()\nexcept EnvironmentError:\n    pass\nreturn keywords", "path": "magic-wormhole/versioneer.py", "commit_date": "2018-11-23 00:00:00", "repo_name": "magic-wormhole/magic-wormhole", "stars": 17697, "license": "mit", "language": "python", "size": 3237}
{"docstring": "\"\"\"Store f in HANDLERS[vcs][method].\"\"\"\n", "func_signal": "def decorate(f):\n", "code": "if vcs not in HANDLERS:\n    HANDLERS[vcs] = {}\nHANDLERS[vcs][method] = f\nreturn f", "path": "magic-wormhole/versioneer.py", "commit_date": "2018-11-23 00:00:00", "repo_name": "magic-wormhole/magic-wormhole", "stars": 17697, "license": "mit", "language": "python", "size": 3237}
{"docstring": "\"\"\"Get version from 'git describe' in the root of the source tree.\n\nThis only gets called if the git-archive 'subst' keywords were *not*\nexpanded, and _version.py hasn't already been rewritten with a short\nversion string, meaning we're inside a checked out source tree.\n\"\"\"\n", "func_signal": "def git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n", "code": "GITS = [\"git\"]\nif sys.platform == \"win32\":\n    GITS = [\"git.cmd\", \"git.exe\"]\n\nout, rc = run_command(GITS, [\"rev-parse\", \"--git-dir\"], cwd=root,\n                      hide_stderr=True)\nif rc != 0:\n    if verbose:\n        print(\"Directory %s not under git control\" % root)\n    raise NotThisMethod(\"'git rev-parse --git-dir' returned error\")\n\n# if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n# if there isn't one, this yields HEX[-dirty] (no NUM)\ndescribe_out, rc = run_command(GITS, [\"describe\", \"--tags\", \"--dirty\",\n                                      \"--always\", \"--long\",\n                                      \"--match\", \"%s*\" % tag_prefix],\n                               cwd=root)\n# --long was added in git-1.5.5\nif describe_out is None:\n    raise NotThisMethod(\"'git describe' failed\")\ndescribe_out = describe_out.strip()\nfull_out, rc = run_command(GITS, [\"rev-parse\", \"HEAD\"], cwd=root)\nif full_out is None:\n    raise NotThisMethod(\"'git rev-parse' failed\")\nfull_out = full_out.strip()\n\npieces = {}\npieces[\"long\"] = full_out\npieces[\"short\"] = full_out[:7]  # maybe improved later\npieces[\"error\"] = None\n\n# parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n# TAG might have hyphens.\ngit_describe = describe_out\n\n# look for -dirty suffix\ndirty = git_describe.endswith(\"-dirty\")\npieces[\"dirty\"] = dirty\nif dirty:\n    git_describe = git_describe[:git_describe.rindex(\"-dirty\")]\n\n# now we have TAG-NUM-gHEX or HEX\n\nif \"-\" in git_describe:\n    # TAG-NUM-gHEX\n    mo = re.search(r'^(.+)-(\\d+)-g([0-9a-f]+)$', git_describe)\n    if not mo:\n        # unparseable. Maybe git-describe is misbehaving?\n        pieces[\"error\"] = (\"unable to parse git-describe output: '%s'\"\n                           % describe_out)\n        return pieces\n\n    # tag\n    full_tag = mo.group(1)\n    if not full_tag.startswith(tag_prefix):\n        if verbose:\n            fmt = \"tag '%s' doesn't start with prefix '%s'\"\n            print(fmt % (full_tag, tag_prefix))\n        pieces[\"error\"] = (\"tag '%s' doesn't start with prefix '%s'\"\n                           % (full_tag, tag_prefix))\n        return pieces\n    pieces[\"closest-tag\"] = full_tag[len(tag_prefix):]\n\n    # distance: number of commits since tag\n    pieces[\"distance\"] = int(mo.group(2))\n\n    # commit: short hex revision ID\n    pieces[\"short\"] = mo.group(3)\n\nelse:\n    # HEX: no tags\n    pieces[\"closest-tag\"] = None\n    count_out, rc = run_command(GITS, [\"rev-list\", \"HEAD\", \"--count\"],\n                                cwd=root)\n    pieces[\"distance\"] = int(count_out)  # total number of commits\n\n# commit date: see ISO-8601 comment in git_versions_from_keywords()\ndate = run_command(GITS, [\"show\", \"-s\", \"--format=%ci\", \"HEAD\"],\n                   cwd=root)[0].strip()\npieces[\"date\"] = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n\nreturn pieces", "path": "magic-wormhole/versioneer.py", "commit_date": "2018-11-23 00:00:00", "repo_name": "magic-wormhole/magic-wormhole", "stars": 17697, "license": "mit", "language": "python", "size": 3237}
{"docstring": "# this always happens before received_dilation_message\n", "func_signal": "def got_wormhole_versions(self, their_wormhole_versions):\n", "code": "dilation_version = None\ntheir_dilation_versions = set(their_wormhole_versions.get(\"can-dilate\", []))\nmy_versions = set(DILATION_VERSIONS)\nshared_versions = my_versions.intersection(their_dilation_versions)\nif \"1\" in shared_versions:\n    dilation_version = \"1\"\n\n# dilation_version is the best mutually-compatible version we have\n# with the peer, or None if we have nothing in common\n\nif not dilation_version:  # \"1\" or None\n    # TODO: be more specific about the error. dilation_version==None\n    # means we had no version in common with them, which could either\n    # be because they're so old they don't dilate at all, or because\n    # they're so new that they no longer accommodate our old version\n    self.fail(failure.Failure(OldPeerCannotDilateError()))\n\nself.start()", "path": "magic-wormhole/src/wormhole/_dilation/manager.py", "commit_date": "2020-04-12 00:00:00", "repo_name": "magic-wormhole/magic-wormhole", "stars": 17697, "license": "mit", "language": "python", "size": 3237}
{"docstring": "# start with the odd words\n", "func_signal": "def get_completions(self, prefix, num_words=2):\n", "code": "count = prefix.count(\"-\")\nif count % 2 == 0:\n    words = odd_words_lowercase\nelse:\n    words = even_words_lowercase\nlast_partial_word = prefix.split(\"-\")[-1]\nlp = len(last_partial_word)\ncompletions = set()\nfor word in words:\n    if word.startswith(last_partial_word):\n        if lp == 0:\n            suffix = prefix + word\n        else:\n            suffix = prefix[:-lp] + word\n        # append a hyphen if we expect more words\n        if count + 1 < num_words:\n            suffix += \"-\"\n        completions.add(suffix)\nreturn completions", "path": "magic-wormhole/src/wormhole/_wordlist.py", "commit_date": "2018-04-21 00:00:00", "repo_name": "magic-wormhole/magic-wormhole", "stars": 17697, "license": "mit", "language": "python", "size": 3237}
{"docstring": "# we get list of {id: ID}, with maybe more attributes in the future\n", "func_signal": "def _response_handle_nameplates(self, msg):\n", "code": "nameplates = msg[\"nameplates\"]\nassert isinstance(nameplates, list), type(nameplates)\nnids = set()\nfor n in nameplates:\n    assert isinstance(n, dict), type(n)\n    nameplate_id = n[\"id\"]\n    assert isinstance(nameplate_id, type(\"\")), type(nameplate_id)\n    nids.add(nameplate_id)\n# deliver a set of nameplate ids\nself._L.rx_nameplates(nids)", "path": "magic-wormhole/src/wormhole/_rendezvous.py", "commit_date": "2020-01-17 00:00:00", "repo_name": "magic-wormhole/magic-wormhole", "stars": 17697, "license": "mit", "language": "python", "size": 3237}
{"docstring": "\"\"\"TAG[.post.devDISTANCE] -- No -dirty.\n\nExceptions:\n1: no tags. 0.post.devDISTANCE\n\"\"\"\n", "func_signal": "def render_pep440_pre(pieces):\n", "code": "if pieces[\"closest-tag\"]:\n    rendered = pieces[\"closest-tag\"]\n    if pieces[\"distance\"]:\n        rendered += \".post.dev%d\" % pieces[\"distance\"]\nelse:\n    # exception #1\n    rendered = \"0.post.dev%d\" % pieces[\"distance\"]\nreturn rendered", "path": "magic-wormhole/versioneer.py", "commit_date": "2018-11-23 00:00:00", "repo_name": "magic-wormhole/magic-wormhole", "stars": 17697, "license": "mit", "language": "python", "size": 3237}
{"docstring": "# records with sequence numbers: always ack, ignore old ones\n", "func_signal": "def got_record(self, r):\n", "code": "if isinstance(r, (Open, Data, Close)):\n    self.send_ack(r.seqnum)  # always ack, even for old ones\n    if self._inbound.is_record_old(r):\n        return\n    self._inbound.update_ack_watermark(r.seqnum)\n    if isinstance(r, Open):\n        self._inbound.handle_open(r.scid)\n    elif isinstance(r, Data):\n        self._inbound.handle_data(r.scid, r.data)\n    else:  # isinstance(r, Close)\n        self._inbound.handle_close(r.scid)\n    return\nif isinstance(r, KCM):\n    log.err(UnexpectedKCM())\nelif isinstance(r, Ping):\n    self.handle_ping(r.ping_id)\nelif isinstance(r, Pong):\n    self.handle_pong(r.ping_id)\nelif isinstance(r, Ack):\n    self._outbound.handle_ack(r.resp_seqnum)  # retire queued messages\nelse:\n    log.err(UnknownMessageType(\"{}\".format(r)))", "path": "magic-wormhole/src/wormhole/_dilation/manager.py", "commit_date": "2020-04-12 00:00:00", "repo_name": "magic-wormhole/magic-wormhole", "stars": 17697, "license": "mit", "language": "python", "size": 3237}
{"docstring": "# ClientService.stopService is defined to \"Stop attempting to\n# reconnect and close any existing connections\"\n", "func_signal": "def stop(self):\n", "code": "self._stopping = True  # to catch _initial_connection_failed error\nd = defer.maybeDeferred(self._connector.stopService)\n# ClientService.stopService always fires with None, even if the\n# initial connection failed, so log.err just in case\nd.addErrback(log.err)\nd.addBoth(self._stopped)", "path": "magic-wormhole/src/wormhole/_rendezvous.py", "commit_date": "2020-01-17 00:00:00", "repo_name": "magic-wormhole/magic-wormhole", "stars": 17697, "license": "mit", "language": "python", "size": 3237}
{"docstring": "\"\"\"Main VCS-independent setup function for installing Versioneer.\"\"\"\n", "func_signal": "def do_setup():\n", "code": "root = get_root()\ntry:\n    cfg = get_config_from_root(root)\nexcept (EnvironmentError, configparser.NoSectionError,\n        configparser.NoOptionError) as e:\n    if isinstance(e, (EnvironmentError, configparser.NoSectionError)):\n        print(\"Adding sample versioneer config to setup.cfg\",\n              file=sys.stderr)\n        with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n            f.write(SAMPLE_CONFIG)\n    print(CONFIG_ERROR, file=sys.stderr)\n    return 1\n\nprint(\" creating %s\" % cfg.versionfile_source)\nwith open(cfg.versionfile_source, \"w\") as f:\n    LONG = LONG_VERSION_PY[cfg.VCS]\n    f.write(LONG % {\"DOLLAR\": \"$\",\n                    \"STYLE\": cfg.style,\n                    \"TAG_PREFIX\": cfg.tag_prefix,\n                    \"PARENTDIR_PREFIX\": cfg.parentdir_prefix,\n                    \"VERSIONFILE_SOURCE\": cfg.versionfile_source,\n                    })\n\nipy = os.path.join(os.path.dirname(cfg.versionfile_source),\n                   \"__init__.py\")\nif os.path.exists(ipy):\n    try:\n        with open(ipy, \"r\") as f:\n            old = f.read()\n    except EnvironmentError:\n        old = \"\"\n    if INIT_PY_SNIPPET not in old:\n        print(\" appending to %s\" % ipy)\n        with open(ipy, \"a\") as f:\n            f.write(INIT_PY_SNIPPET)\n    else:\n        print(\" %s unmodified\" % ipy)\nelse:\n    print(\" %s doesn't exist, ok\" % ipy)\n    ipy = None\n\n# Make sure both the top-level \"versioneer.py\" and versionfile_source\n# (PKG/_version.py, used by runtime code) are in MANIFEST.in, so\n# they'll be copied into source distributions. Pip won't be able to\n# install the package without this.\nmanifest_in = os.path.join(root, \"MANIFEST.in\")\nsimple_includes = set()\ntry:\n    with open(manifest_in, \"r\") as f:\n        for line in f:\n            if line.startswith(\"include \"):\n                for include in line.split()[1:]:\n                    simple_includes.add(include)\nexcept EnvironmentError:\n    pass\n# That doesn't cover everything MANIFEST.in can do\n# (http://docs.python.org/2/distutils/sourcedist.html#commands), so\n# it might give some false negatives. Appending redundant 'include'\n# lines is safe, though.\nif \"versioneer.py\" not in simple_includes:\n    print(\" appending 'versioneer.py' to MANIFEST.in\")\n    with open(manifest_in, \"a\") as f:\n        f.write(\"include versioneer.py\\n\")\nelse:\n    print(\" 'versioneer.py' already in MANIFEST.in\")\nif cfg.versionfile_source not in simple_includes:\n    print(\" appending versionfile_source ('%s') to MANIFEST.in\" %\n          cfg.versionfile_source)\n    with open(manifest_in, \"a\") as f:\n        f.write(\"include %s\\n\" % cfg.versionfile_source)\nelse:\n    print(\" versionfile_source already in MANIFEST.in\")\n\n# Make VCS-specific changes. For git, this means creating/changing\n# .gitattributes to mark _version.py for export-subst keyword\n# substitution.\ndo_vcs_install(manifest_in, cfg.versionfile_source, ipy)\nreturn 0", "path": "magic-wormhole/versioneer.py", "commit_date": "2018-11-23 00:00:00", "repo_name": "magic-wormhole/magic-wormhole", "stars": 17697, "license": "mit", "language": "python", "size": 3237}
{"docstring": "'''\n    This method captures the frames from the video file provided.\n    This program makes use of openCV library\n'''\n", "func_signal": "def capture_frames(self):\n", "code": "cv2_object = cv2.VideoCapture(self.file_path)\n\nframe_number = 0\nframe_found = 1\n\nwhile frame_found:\n    frame_found, image = cv2_object.read()\n    capture = f'{self.directory}/frame{frame_number}.jpg'\n    cv2.imwrite(capture, image)\n\n    frame_number += 1", "path": "python-mini-projects/projects/Capture_Video_Frames/capture_video_frames.py", "commit_date": "2020-07-10 00:00:00", "repo_name": "Python-World/python-mini-projects", "stars": 13922, "license": "mit", "language": "python", "size": 138465}
{"docstring": "'''Display tasks'''\n", "func_signal": "def tasks(ctx):\n", "code": "if ctx.obj['TASKS']:\n    click.echo('YOUR TASKS\\n**********')\n    #Iterate through all the tasks stored in the context\n    for i, task in ctx.obj['TASKS'].items():\n        click.echo('\u2022 ' + task + ' (ID: ' + i + ')')\n    click.echo('')\nelse:\n    click.echo('No tasks yet! Use ADD to add one.\\n')", "path": "python-mini-projects/projects/Cli_todo/todo.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "Python-World/python-mini-projects", "stars": 13922, "license": "mit", "language": "python", "size": 138465}
{"docstring": "'''\nSplit Given list of files and return generator\n'''\n", "func_signal": "def split(data, count):\n", "code": "for i in range(1, len(data), count):\n    if i + count-1 > len(data):\n        start, end = (i-1, len(data))\n    else:\n        start, end = (i-1, i+count-1)\n    yield data[start:end]", "path": "python-mini-projects/projects/Split_folder_into_subfolders/split_and_copy.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "Python-World/python-mini-projects", "stars": 13922, "license": "mit", "language": "python", "size": 138465}
{"docstring": "'''Simple CLI Todo App'''\n", "func_signal": "def todo(ctx):\n", "code": "ctx.ensure_object(dict)\n#Open todo.txt \u2013 first line contains latest ID, rest contain tasks and IDs\nwith open('./todo.txt') as f:\n    content = f.readlines()\n#Transfer data from todo.txt to the context\nctx.obj['LATEST'] = int(content[:1][0])\nctx.obj['TASKS'] = {en.split('```')[0]:en.split('```')[1][:-1] for en in content[1:]}", "path": "python-mini-projects/projects/Cli_todo/todo.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "Python-World/python-mini-projects", "stars": 13922, "license": "mit", "language": "python", "size": 138465}
{"docstring": "'''Function to change file extenstion to png and save it to User's prefered location '''\n", "func_signal": "def convertToPNG():\n", "code": "global im1\nif im1 is None:\n    tk.messagebox.showerror(\"Error\", \"No File selected\")\nelse:\n    export_file_path = filedialog.asksaveasfilename(defaultextension='.png')\n    im1.save(export_file_path)", "path": "python-mini-projects/projects/Convert_JPEG_to_PNG/converter_GUI.py", "commit_date": "2020-10-17 00:00:00", "repo_name": "Python-World/python-mini-projects", "stars": 13922, "license": "mit", "language": "python", "size": 138465}
{"docstring": "'''Delete a task by ID'''\n#Find task with associated ID\n", "func_signal": "def done(ctx, fin_taskid):\n", "code": "if str(fin_taskid) in ctx.obj['TASKS'].keys():\n    task = ctx.obj['TASKS'][str(fin_taskid)]\n    #Delete task from task list in context\n    del ctx.obj['TASKS'][str(fin_taskid)]\n    click.echo('Finished and removed task \"' + task + '\" with id ' + str(fin_taskid))\n    #Open todo.txt and write current index and tasks with IDs (separated by \" ``` \")\n    if ctx.obj['TASKS']:\n        curr_ind = [str(ctx.obj['LATEST'] + 1)] \n        tasks = [str(i) + '```' + t for (i, t) in ctx.obj['TASKS'].items()]\n        with open('./todo.txt', 'w') as f:\n            f.writelines(['%s\\n' % en for en in curr_ind + tasks])\n    else:\n        #Resets ID tracker to 0 if list is empty\n        with open('./todo.txt', 'w') as f:\n            f.writelines([str(0) + '\\n'])\nelse:\n    click.echo('Error: no task with id ' + str(fin_taskid))", "path": "python-mini-projects/projects/Cli_todo/todo.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "Python-World/python-mini-projects", "stars": 13922, "license": "mit", "language": "python", "size": 138465}
{"docstring": "#Player names input;\n\n", "func_signal": "def names():\n", "code": "p1_name=input(\"\\nEnter NAME of PLAYER 1:\\t\").capitalize()\np2_name=input(\"Enter NAME of PLAYER 2:\\t\").capitalize()\nreturn (p1_name, p2_name)", "path": "python-mini-projects/projects/Tic_tac_toe_with_ai/tic-tac-toe-AI.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "Python-World/python-mini-projects", "stars": 13922, "license": "mit", "language": "python", "size": 138465}
{"docstring": "\"\"\"returns the numeric coded image\"\"\"\n\n# resizing parameters\n# adjust these parameters if the output doesn't fit to the screen\n", "func_signal": "def img_to_ascii(image):\n", "code": "height, width = image.shape\nnew_width = int(width / 20) \nnew_height = int(height / 40)\n\n# resize image to fit the printing screen\nresized_image = cv2.resize(image, (new_width, new_height),)\n\nthresh_image = np.zeros(resized_image.shape)\n\nfor i, threshold in enumerate(threshold_list):\n    # assign corresponding values according to the index of threshold applied\n    thresh_image[resized_image > threshold] = i\nreturn thresh_image", "path": "python-mini-projects/projects/Ascii_art/make_art.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "Python-World/python-mini-projects", "stars": 13922, "license": "mit", "language": "python", "size": 138465}
{"docstring": "'''\nreturn a list of files avialable in given folder\n'''\n", "func_signal": "def get_files(path):\n", "code": "files = glob.glob(f'{path}/*')\nreturn files", "path": "python-mini-projects/projects/Split_folder_into_subfolders/split_and_copy.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "Python-World/python-mini-projects", "stars": 13922, "license": "mit", "language": "python", "size": 138465}
{"docstring": "\"\"\"This function takes a user defined url as input\nand returns the time taken to load that url in seconds.\n\nArgs:\n    url (string): The user defined url.\n\nReturns:\n    time_to_load (float): The time taken to load the website in seconds.\n\"\"\"\n\n", "func_signal": "def get_load_time(url):\n", "code": "if (\"https\" or \"http\") in url:  # Checking for presence of protocols\n    open_this_url = urlopen(url)  # Open the url as entered by the user\nelse:\n    open_this_url = urlopen(\"https://\" + url)  # Adding https to the url\nstart_time = time.time()  # Time stamp before the reading of url starts\nopen_this_url.read()  # Reading the user defined url\nend_time = time.time()  # Time stamp after the reading of the url\nopen_this_url.close()  # Closing the instance of the urlopen object\ntime_to_load = end_time - start_time\n\nreturn time_to_load", "path": "python-mini-projects/projects/Time_to_load_website/time_to_load_website.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "Python-World/python-mini-projects", "stars": 13922, "license": "mit", "language": "python", "size": 138465}
{"docstring": "# N.B. This query may fail with ERROR_INVALID_FUNCTION\n# for some filesystems.\n", "func_signal": "def get_file_security(filename, request=_DEFAULT_SECURITY_INFORMATION):\n", "code": "pSD = PSECURITY_DESCRIPTOR(needs_free=True)\nerror = advapi32.GetNamedSecurityInfoW(filename, SE_FILE_OBJECT, request,\n            ctypes.byref(pSD.pOwner), ctypes.byref(pSD.pGroup),\n            ctypes.byref(pSD.pDacl), ctypes.byref(pSD.pSacl),\n            ctypes.byref(pSD))\nif error != 0:\n    raise ctypes.WinError(error)\nreturn pSD", "path": "python-mini-projects/projects/Get_meta_information_of_images/author_utils.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "Python-World/python-mini-projects", "stars": 13922, "license": "mit", "language": "python", "size": 138465}
{"docstring": "# Should be unreachable, but just in case\n", "func_signal": "def fetch(page_no, verbose=False):\n", "code": "if page_no <= 0:\n    raise ValueError('Number of Pages must be greater than zero')\npage_no = min(page_no, 20)\ni = page_no\nif verbose:\n    print('Fetching Page {}...'.format(i))\ntry:\n    res = requests.get('https://news.ycombinator.com/?p=' + str(i))\n    only_td = SoupStrainer('td')\n    soup = BeautifulSoup(res.content, 'html.parser', parse_only=only_td)\n    tdtitle = soup.find_all('td', attrs={'class': 'title'})\n    tdmetrics = soup.find_all('td', attrs={'class': 'subtext'})\n    with open(os.path.join('HackerNews', 'NewsPage{}.txt'.format(i)), 'w+') as f:\n        f.write('-' * 80)\n        f.write('\\n')\n        f.write('Page {}'.format(i))\n        tdtitle = soup.find_all('td', attrs={'class': 'title'})\n        tdrank = soup.find_all(\n            'td',\n            attrs={\n                'class': 'title',\n                'align': 'right'})\n        tdtitleonly = [t for t in tdtitle if t not in tdrank]\n        tdmetrics = soup.find_all('td', attrs={'class': 'subtext'})\n        tdt = tdtitleonly\n        tdr = tdrank\n        tdm = tdmetrics\n        num_iter = min(len(tdr), len(tdt))\n        for idx in range(num_iter):\n            f.write('\\n' + '-' * 80 + '\\n')\n            rank = tdr[idx].find('span', attrs={'class': 'rank'})\n            titl = tdt[idx].find('a', attrs={'class': 'storylink'})\n            url = titl['href'] if titl and titl['href'].startswith(\n                'https') else 'https://news.ycombinator.com/' + titl['href']\n            site = tdt[idx].find('span', attrs={'class': 'sitestr'})\n            score = tdm[idx].find('span', attrs={'class': 'score'})\n            time = tdm[idx].find('span', attrs={'class': 'age'})\n            author = tdm[idx].find('a', attrs={'class': 'hnuser'})\n            f.write(\n                '\\nArticle Number: ' +\n                rank.text.replace(\n                    '.',\n                    '') if rank else '\\nArticle Number: Could not get article number')\n            f.write(\n                '\\nArticle Title: ' +\n                titl.text if titl else '\\nArticle Title: Could not get article title')\n            f.write(\n                '\\nSource Website: ' +\n                site.text if site else '\\nSource Website: https://news.ycombinator.com')\n            f.write(\n                '\\nSource URL: ' +\n                url if url else '\\nSource URL: No URL found for this article')\n            f.write(\n                '\\nArticle Author: ' +\n                author.text if author else '\\nArticle Author: Could not get article author')\n            f.write(\n                '\\nArticle Score: ' +\n                score.text if score else '\\nArticle Score: Not Scored')\n            f.write(\n                '\\nPosted: ' +\n                time.text if time else '\\nPosted: Could not find when the article was posted')\n            f.write('\\n' + '-' * 80 + '\\n')\nexcept (requests.ConnectionError, requests.packages.urllib3.exceptions.ConnectionError) as e:\n    print('Connection Failed for page {}'.format(i))\nexcept requests.RequestException as e:\n    print(\"Some ambiguous Request Exception occurred. The exception is \" + str(e))", "path": "python-mini-projects/projects/Scrape_Hacker_News/main.py", "commit_date": "2020-10-17 00:00:00", "repo_name": "Python-World/python-mini-projects", "stars": 13922, "license": "mit", "language": "python", "size": 138465}
{"docstring": "'''Add a task'''\n", "func_signal": "def add(ctx, add_task):\n", "code": "if add_task:\n    #Add task to list in context \n    ctx.obj['TASKS'][ctx.obj['LATEST']] = add_task\n    click.echo('Added task \"' + add_task + '\" with ID ' + str(ctx.obj['LATEST']))\n    #Open todo.txt and write current index and tasks with IDs (separated by \" ``` \")\n    curr_ind = [str(ctx.obj['LATEST'] + 1)] \n    tasks = [str(i) + '```' + t for (i, t) in ctx.obj['TASKS'].items()]\n    with open('./todo.txt', 'w') as f:\n        f.writelines(['%s\\n' % en for en in curr_ind + tasks])", "path": "python-mini-projects/projects/Cli_todo/todo.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "Python-World/python-mini-projects", "stars": 13922, "license": "mit", "language": "python", "size": 138465}
{"docstring": "'''main function accept instagram username\nreturn an dictionary object containging profile deatils\n'''\n", "func_signal": "def main(username):\n", "code": "url = \"https://www.instagram.com/{}/?hl=en\".format(username)\npage = requests.get(url)\ntree = html.fromstring(page.content)\ndata = tree.xpath('//meta[starts-with(@name,\"description\")]/@content')\n\nif data:\n    data = tree.xpath('//meta[starts-with(@name,\"description\")]/@content')\n    data = data[0].split(', ')\n    followers = data[0][:-9].strip()\n    following = data[1][:-9].strip()\n    posts = re.findall(r'\\d+[,]*', data[2])[0]\n    name = re.findall(r'name\":\"\\w*[\\s]+\\w*\"', page.text)[-1][7:-1]\n    aboutinfo = re.findall(r'\"description\":\"([^\"]+)\"', page.text)[0]\n    instagram_profile = {\n        'success': True,\n        'profile': {\n            'name': name,\n            'profileurl': url,\n            'username': username,\n            'followers': followers,\n            'following': following,\n            'posts': posts,\n            'aboutinfo': aboutinfo\n        }\n    }\nelse:\n    instagram_profile = {\n        'success': False,\n        'profile': {}\n    }\nreturn instagram_profile", "path": "python-mini-projects/projects/Instagram_profile/InstgramProfile.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "Python-World/python-mini-projects", "stars": 13922, "license": "mit", "language": "python", "size": 138465}
{"docstring": "'''\nThis function copy file from src to dst\nif dst dir is not there it will create new\n'''\n", "func_signal": "def copyfiles(src, dst):\n", "code": "if not os.path.isdir(dst):\n    os.makedirs(dst)\ncopy2(src, dst)", "path": "python-mini-projects/projects/Split_folder_into_subfolders/split_and_copy.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "Python-World/python-mini-projects", "stars": 13922, "license": "mit", "language": "python", "size": 138465}
{"docstring": "\"\"\"prints the coded image with symbols\"\"\"\n\n", "func_signal": "def print_out_ascii(array):\n", "code": "for row in array:\n    for e in row:\n    \t# select symbol based on the type of coding\n        print(symbols_list[int(e) % len(symbols_list)], end=\"\")\n    print()", "path": "python-mini-projects/projects/Ascii_art/make_art.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "Python-World/python-mini-projects", "stars": 13922, "license": "mit", "language": "python", "size": 138465}
{"docstring": "#To check if one of the following patterns are true; then the respective player has won!;\n\n#HORIZONTAL CHECK;\n", "func_signal": "def win_check(board, choice):\n", "code": "return ( \n   ( board[1] == choice and board[2] == choice and board[3] == choice )\nor ( board[4] == choice and board[5] == choice and board[6] == choice )\nor ( board[7] == choice and board[8] == choice and board[9] == choice )\n#VERTICAL CHECK;\nor ( board[1] == choice and board[4] == choice and board[7] == choice )\nor ( board[2] == choice and board[5] == choice and board[8] == choice )\nor ( board[3] == choice and board[6] == choice and board[9] == choice )\n#DIAGONAL CHECK;\nor ( board[1] == choice and board[5] == choice and board[9] == choice )\nor ( board[3] == choice and board[5] == choice and board[7] == choice )  )", "path": "python-mini-projects/projects/Tic_tac_toe_with_ai/tic-tac-toe-AI.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "Python-World/python-mini-projects", "stars": 13922, "license": "mit", "language": "python", "size": 138465}
{"docstring": "#To check if the board is full, then the game is a draw;\n", "func_signal": "def full_board_check(board):\n", "code": "for i in range(1,10):\n    if space_check(board, i):\n        return False\nreturn True", "path": "python-mini-projects/projects/Tic_tac_toe_with_ai/tic-tac-toe-AI.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "Python-World/python-mini-projects", "stars": 13922, "license": "mit", "language": "python", "size": 138465}
{"docstring": "'''\ndownloading the file and saving it\n'''\n", "func_signal": "def download(url, file_name):\n", "code": "with open(file_name, \"wb\") as file:\n    response = get(url)\n    file.write(response.content)", "path": "python-mini-projects/projects/Write_a_script_to_download_a_random_image_from_unsplash_and_set_it_as_wallpaper/background_windows.py", "commit_date": "2020-10-17 00:00:00", "repo_name": "Python-World/python-mini-projects", "stars": 13922, "license": "mit", "language": "python", "size": 138465}
{"docstring": "#To mark/replace the position on the board list;\n", "func_signal": "def place_marker(board, avail, choice, position):\n", "code": "board[position] = choice\navail[position] = ' '", "path": "python-mini-projects/projects/Tic_tac_toe_with_ai/tic-tac-toe-AI.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "Python-World/python-mini-projects", "stars": 13922, "license": "mit", "language": "python", "size": 138465}
{"docstring": "\"\"\"\nAn error in Connection.on_connect should disconnect from the server\nsee for details: https://github.com/andymccurdy/redis-py/issues/368\n\"\"\"\n# this assumes the Redis server being tested against doesn't have\n# 9999 databases ;)\n", "func_signal": "def test_on_connect_error(self):\n", "code": "bad_connection = redis.Redis(db=9999)\n# an error should be raised on connect\nwith pytest.raises(redis.RedisError):\n    bad_connection.info()\npool = bad_connection.connection_pool\nassert len(pool._available_connections) == 1\nassert not pool._available_connections[0]._sock", "path": "redis-py/tests/test_connection_pool.py", "commit_date": "2020-08-15 00:00:00", "repo_name": "redis/redis-py", "stars": 12134, "license": "mit", "language": "python", "size": 8337}
{"docstring": "\"\"\"\nBusyLoadingErrors should raise from Pipelines that execute a\ncommand immediately, like WATCH does.\n\"\"\"\n", "func_signal": "def test_busy_loading_from_pipeline_immediate_command(self, r):\n", "code": "pipe = r.pipeline()\nwith pytest.raises(redis.BusyLoadingError):\n    pipe.immediate_execute_command('DEBUG', 'ERROR',\n                                   'LOADING fake message')\npool = r.connection_pool\nassert not pipe.connection\nassert len(pool._available_connections) == 1\nassert not pool._available_connections[0]._sock", "path": "redis-py/tests/test_connection_pool.py", "commit_date": "2020-08-15 00:00:00", "repo_name": "redis/redis-py", "stars": 12134, "license": "mit", "language": "python", "size": 8337}
{"docstring": "\"\"\"\nA connection owned by a parent is unusable by a child if the parent\n(the owning process) closes the connection.\n\"\"\"\n", "func_signal": "def test_close_connection_in_parent(self, master_host):\n", "code": "conn = Connection(host=master_host)\nconn.send_command('ping')\nassert conn.read_response() == b'PONG'\n\ndef target(conn, ev):\n    ev.wait()\n    # the parent closed the connection. because it also created the\n    # connection, the connection is shutdown and the child\n    # cannot use it.\n    with pytest.raises(ConnectionError):\n        conn.send_command('ping')\n\nev = multiprocessing.Event()\nproc = multiprocessing.Process(target=target, args=(conn, ev))\nproc.start()\n\nconn.disconnect()\nev.set()\n\nproc.join(3)\nassert proc.exitcode == 0", "path": "redis-py/tests/test_multiprocessing.py", "commit_date": "2020-08-06 00:00:00", "repo_name": "redis/redis-py", "stars": 12134, "license": "mit", "language": "python", "size": 8337}
{"docstring": "\"\"\"\nCommands with custom EMPTY_ERROR functionality return their default\nvalues in the pipeline no matter the raise_on_error preference\n\"\"\"\n", "func_signal": "def test_transaction_with_empty_error_command(self, r):\n", "code": "for error_switch in (True, False):\n    with r.pipeline() as pipe:\n        pipe.set('a', 1).mget([]).set('c', 3)\n        result = pipe.execute(raise_on_error=error_switch)\n\n        assert result[0]\n        assert result[1] == []\n        assert result[2]", "path": "redis-py/tests/test_pipeline.py", "commit_date": "2020-08-06 00:00:00", "repo_name": "redis/redis-py", "stars": 12134, "license": "mit", "language": "python", "size": 8337}
{"docstring": "# eventually make this more robust and take optional args from\n# argparse\n", "func_signal": "def get_client(self, **kwargs):\n", "code": "if self._client is None or kwargs:\n    defaults = {\n        'db': 9\n    }\n    defaults.update(kwargs)\n    pool = redis.ConnectionPool(**kwargs)\n    self._client = redis.Redis(connection_pool=pool)\nreturn self._client", "path": "redis-py/benchmarks/base.py", "commit_date": "2020-08-06 00:00:00", "repo_name": "redis/redis-py", "stars": 12134, "license": "mit", "language": "python", "size": 8337}
{"docstring": "\"\"\"\nBusyLoadingErrors should be raised from a pipeline execution\nregardless of the raise_on_error flag.\n\"\"\"\n", "func_signal": "def test_busy_loading_from_pipeline(self, r):\n", "code": "pipe = r.pipeline()\npipe.execute_command('DEBUG', 'ERROR', 'LOADING fake message')\nwith pytest.raises(redis.BusyLoadingError):\n    pipe.execute()\npool = r.connection_pool\nassert not pipe.connection\nassert len(pool._available_connections) == 1\nassert not pool._available_connections[0]._sock", "path": "redis-py/tests/test_connection_pool.py", "commit_date": "2020-08-15 00:00:00", "repo_name": "redis/redis-py", "stars": 12134, "license": "mit", "language": "python", "size": 8337}
{"docstring": "# Put first sentinel 'foo' down\n", "func_signal": "def test_discover_master_sentinel_timeout(cluster, sentinel, master_ip):\n", "code": "cluster.nodes_timeout.add(('foo', 26379))\naddress = sentinel.discover_master('mymaster')\nassert address == (master_ip, 6379)\n# 'bar' is now first sentinel\nassert sentinel.sentinels[0].id == ('bar', 26379)", "path": "redis-py/tests/test_sentinel.py", "commit_date": "2020-08-06 00:00:00", "repo_name": "redis/redis-py", "stars": 12134, "license": "mit", "language": "python", "size": 8337}
{"docstring": "\"\"\"\nPubsub can handle a new subscribe when it's time to check the\nconnection health\n\"\"\"\n", "func_signal": "def test_health_check_in_pubsub_after_subscribed(self, r):\n", "code": "p = r.pubsub()\np.connection = p.connection_pool.get_connection('_')\np.connection.next_health_check = 0\nwith mock.patch.object(p.connection, 'send_command',\n                       wraps=p.connection.send_command) as m:\n    p.subscribe('foo')\n    subscribe_message = wait_for_message(p)\n    assert subscribe_message['type'] == 'subscribe'\n    self.assert_interval_advanced(p.connection)\n    # because we weren't subscribed when sending the subscribe\n    # message to 'foo', the connection's standard check_health ran\n    # prior to subscribing.\n    m.assert_any_call('PING', check_health=False)\n\n    p.connection.next_health_check = 0\n    m.reset_mock()\n\n    p.subscribe('bar')\n    # the second subscribe issues exactly only command (the subscribe)\n    # and the health check is not invoked\n    m.assert_called_once_with('SUBSCRIBE', 'bar', check_health=False)\n\n    # since no message has been read since the health check was\n    # reset, it should still be 0\n    assert p.connection.next_health_check == 0\n\n    subscribe_message = wait_for_message(p)\n    assert subscribe_message['type'] == 'subscribe'\n    assert wait_for_message(p) is None\n    # now that the connection is subscribed, the pubsub health\n    # check should have taken over and include the HEALTH_CHECK_MESSAGE\n    m.assert_any_call('PING', p.HEALTH_CHECK_MESSAGE,\n                      check_health=False)\n    self.assert_interval_advanced(p.connection)", "path": "redis-py/tests/test_connection_pool.py", "commit_date": "2020-08-15 00:00:00", "repo_name": "redis/redis-py", "stars": 12134, "license": "mit", "language": "python", "size": 8337}
{"docstring": "\"\"\"\nAsks sentinel servers for the Redis master's address corresponding\nto the service labeled ``service_name``.\n\nReturns a pair (address, port) or raises MasterNotFoundError if no\nmaster is found.\n\"\"\"\n", "func_signal": "def discover_master(self, service_name):\n", "code": "for sentinel_no, sentinel in enumerate(self.sentinels):\n    try:\n        masters = sentinel.sentinel_masters()\n    except (ConnectionError, TimeoutError):\n        continue\n    state = masters.get(service_name)\n    if state and self.check_master_state(state, service_name):\n        # Put this sentinel at the top of the list\n        self.sentinels[0], self.sentinels[sentinel_no] = (\n            sentinel, self.sentinels[0])\n        return state['ip'], state['port']\nraise MasterNotFoundError(\"No master found for %r\" % (service_name,))", "path": "redis-py/redis/sentinel.py", "commit_date": "2020-08-06 00:00:00", "repo_name": "redis/redis-py", "stars": 12134, "license": "mit", "language": "python", "size": 8337}
{"docstring": "\"\"\"\nAuthenticationError should be raised when the server requires a\npassword but one isn't supplied.\n\"\"\"\n", "func_signal": "def test_connect_no_auth_supplied_when_required(self, r):\n", "code": "with pytest.raises(redis.AuthenticationError):\n    r.execute_command('DEBUG', 'ERROR',\n                      'ERR Client sent AUTH, but no password is set')", "path": "redis-py/tests/test_connection_pool.py", "commit_date": "2020-08-15 00:00:00", "repo_name": "redis/redis-py", "stars": 12134, "license": "mit", "language": "python", "size": 8337}
{"docstring": "\"\"\"\nPolling a pubsub connection that's subscribed will regularly\ncheck the connection's health.\n\"\"\"\n", "func_signal": "def test_health_check_in_pubsub_poll(self, r):\n", "code": "p = r.pubsub()\np.connection = p.connection_pool.get_connection('_')\nwith mock.patch.object(p.connection, 'send_command',\n                       wraps=p.connection.send_command) as m:\n    p.subscribe('foo')\n    subscribe_message = wait_for_message(p)\n    assert subscribe_message['type'] == 'subscribe'\n    self.assert_interval_advanced(p.connection)\n\n    # polling the connection before the health check interval\n    # doesn't result in another health check\n    m.reset_mock()\n    next_health_check = p.connection.next_health_check\n    assert wait_for_message(p) is None\n    assert p.connection.next_health_check == next_health_check\n    m.assert_not_called()\n\n    # reset the health check and poll again\n    # we should not receive a pong message, but the next_health_check\n    # should be advanced\n    p.connection.next_health_check = 0\n    assert wait_for_message(p) is None\n    m.assert_called_with('PING', p.HEALTH_CHECK_MESSAGE,\n                         check_health=False)\n    self.assert_interval_advanced(p.connection)", "path": "redis-py/tests/test_connection_pool.py", "commit_date": "2020-08-15 00:00:00", "repo_name": "redis/redis-py", "stars": 12134, "license": "mit", "language": "python", "size": 8337}
{"docstring": "\"\"\"\nA child process that uses the same pool as its parent isn't affected\nwhen the parent disconnects all connections within the pool.\n\"\"\"\n", "func_signal": "def test_close_pool_in_main(self, max_connections, master_host):\n", "code": "pool = ConnectionPool.from_url('redis://{}'.format(master_host),\n                               max_connections=max_connections)\n\nconn = pool.get_connection('ping')\nassert conn.send_command('ping') is None\nassert conn.read_response() == b'PONG'\n\ndef target(pool, disconnect_event):\n    conn = pool.get_connection('ping')\n    with exit_callback(pool.release, conn):\n        assert conn.send_command('ping') is None\n        assert conn.read_response() == b'PONG'\n        disconnect_event.wait()\n        assert conn.send_command('ping') is None\n        assert conn.read_response() == b'PONG'\n\nev = multiprocessing.Event()\n\nproc = multiprocessing.Process(target=target, args=(pool, ev))\nproc.start()\n\npool.disconnect()\nev.set()\nproc.join(3)\nassert proc.exitcode == 0", "path": "redis-py/tests/test_multiprocessing.py", "commit_date": "2020-08-06 00:00:00", "repo_name": "redis/redis-py", "stars": 12134, "license": "mit", "language": "python", "size": 8337}
{"docstring": "\"\"\"\nCommands with custom EMPTY_ERROR functionality return their default\nvalues in the pipeline no matter the raise_on_error preference\n\"\"\"\n", "func_signal": "def test_pipeline_with_empty_error_command(self, r):\n", "code": "for error_switch in (True, False):\n    with r.pipeline(transaction=False) as pipe:\n        pipe.set('a', 1).mget([]).set('c', 3)\n        result = pipe.execute(raise_on_error=error_switch)\n\n        assert result[0]\n        assert result[1] == []\n        assert result[2]", "path": "redis-py/tests/test_pipeline.py", "commit_date": "2020-08-06 00:00:00", "repo_name": "redis/redis-py", "stars": 12134, "license": "mit", "language": "python", "size": 8337}
{"docstring": "# invoke a command to make sure the connection is entirely setup\n", "func_signal": "def test_arbitrary_command_invokes_health_check(self, r):\n", "code": "r.get('foo')\nr.connection.next_health_check = time.time()\nwith mock.patch.object(r.connection, 'send_command',\n                       wraps=r.connection.send_command) as m:\n    r.get('foo')\n    m.assert_called_with('PING', check_health=False)\n\nself.assert_interval_advanced(r.connection)", "path": "redis-py/tests/test_connection_pool.py", "commit_date": "2020-08-15 00:00:00", "repo_name": "redis/redis-py", "stars": 12134, "license": "mit", "language": "python", "size": 8337}
{"docstring": "\"\"\"\nA connection owned by a parent and closed by a child doesn't\ndestroy the file descriptors so a parent can still use it.\n\"\"\"\n", "func_signal": "def test_close_connection_in_child(self, master_host):\n", "code": "conn = Connection(host=master_host)\nconn.send_command('ping')\nassert conn.read_response() == b'PONG'\n\ndef target(conn):\n    conn.send_command('ping')\n    assert conn.read_response() == b'PONG'\n    conn.disconnect()\n\nproc = multiprocessing.Process(target=target, args=(conn,))\nproc.start()\nproc.join(3)\nassert proc.exitcode == 0\n\n# The connection was created in the parent but disconnected in the\n# child. The child called socket.close() but did not call\n# socket.shutdown() because it wasn't the \"owning\" process.\n# Therefore the connection still works in the parent.\nconn.send_command('ping')\nassert conn.read_response() == b'PONG'", "path": "redis-py/tests/test_multiprocessing.py", "commit_date": "2020-08-06 00:00:00", "repo_name": "redis/redis-py", "stars": 12134, "license": "mit", "language": "python", "size": 8337}
{"docstring": "# get the sha, then clear the cache\n", "func_signal": "def test_script_loading(self, r):\n", "code": "sha = r.script_load(multiply_script)\nr.script_flush()\nassert r.script_exists(sha) == [False]\nr.script_load(multiply_script)\nassert r.script_exists(sha) == [True]", "path": "redis-py/tests/test_scripting.py", "commit_date": "2020-08-06 00:00:00", "repo_name": "redis/redis-py", "stars": 12134, "license": "mit", "language": "python", "size": 8337}
{"docstring": "\"\"\"\nan invalid pipeline command at exec time adds the exception instance\nto the list of returned values\n\"\"\"\n", "func_signal": "def test_exec_error_in_response(self, r):\n", "code": "r['c'] = 'a'\nwith r.pipeline() as pipe:\n    pipe.set('a', 1).set('b', 2).lpush('c', 3).set('d', 4)\n    result = pipe.execute(raise_on_error=False)\n\n    assert result[0]\n    assert r['a'] == b'1'\n    assert result[1]\n    assert r['b'] == b'2'\n\n    # we can't lpush to a key that's a string value, so this should\n    # be a ResponseError exception\n    assert isinstance(result[2], redis.ResponseError)\n    assert r['c'] == b'a'\n\n    # since this isn't a transaction, the other commands after the\n    # error are still executed\n    assert result[3]\n    assert r['d'] == b'4'\n\n    # make sure the pipe was restored to a working state\n    assert pipe.set('z', 'zzz').execute() == [True]\n    assert r['z'] == b'zzz'", "path": "redis-py/tests/test_pipeline.py", "commit_date": "2020-08-06 00:00:00", "repo_name": "redis/redis-py", "stars": 12134, "license": "mit", "language": "python", "size": 8337}
{"docstring": "# Put first sentinel 'foo' down\n", "func_signal": "def test_discover_master_sentinel_down(cluster, sentinel, master_ip):\n", "code": "cluster.nodes_down.add(('foo', 26379))\naddress = sentinel.discover_master('mymaster')\nassert address == (master_ip, 6379)\n# 'bar' is now first sentinel\nassert sentinel.sentinels[0].id == ('bar', 26379)", "path": "redis-py/tests/test_sentinel.py", "commit_date": "2020-08-06 00:00:00", "repo_name": "redis/redis-py", "stars": 12134, "license": "mit", "language": "python", "size": 8337}
{"docstring": "\"\"\"\nWhen out of connections, block until another connection is released\nto the pool\n\"\"\"\n", "func_signal": "def test_connection_pool_blocks_until_conn_available(self, master_host):\n", "code": "connection_kwargs = {'host': master_host}\npool = self.get_pool(max_connections=1, timeout=2,\n                     connection_kwargs=connection_kwargs)\nc1 = pool.get_connection('_')\n\ndef target():\n    time.sleep(0.1)\n    pool.release(c1)\n\nstart = time.time()\nThread(target=target).start()\npool.get_connection('_')\nassert time.time() - start >= 0.1", "path": "redis-py/tests/test_connection_pool.py", "commit_date": "2020-08-15 00:00:00", "repo_name": "redis/redis-py", "stars": 12134, "license": "mit", "language": "python", "size": 8337}
{"docstring": "\"\"\"\nIf Redis raises a LOADING error, the connection should be\ndisconnected and a BusyLoadingError raised\n\"\"\"\n", "func_signal": "def test_busy_loading_disconnects_socket(self, r):\n", "code": "with pytest.raises(redis.BusyLoadingError):\n    r.execute_command('DEBUG', 'ERROR', 'LOADING fake message')\nassert not r.connection._sock", "path": "redis-py/tests/test_connection_pool.py", "commit_date": "2020-08-15 00:00:00", "repo_name": "redis/redis-py", "stars": 12134, "license": "mit", "language": "python", "size": 8337}
{"docstring": "\"\"\"Init gradient arrays corresponding to each weight array.\"\"\"\n", "func_signal": "def init_grad(self):\n", "code": "for key in self._params.keys():\n    if key not in self._grads:\n        self._grads[key] = np.zeros_like(self._params[key])", "path": "MLAlgorithms/mla/neuralnet/parameters.py", "commit_date": "2019-08-25 00:00:00", "repo_name": "rushter/MLAlgorithms", "stars": 10223, "license": "mit", "language": "python", "size": 11924}
{"docstring": "\"\"\"Find optimal value for leaf.\"\"\"\n", "func_signal": "def _calculate_leaf_value(self, targets):\n", "code": "if self.loss is not None:\n    # Gradient boosting\n    self.outcome = self.loss.approximate(targets[\"actual\"], targets[\"y_pred\"])\nelse:\n    # Random Forest\n    if self.regression:\n        # Mean value for regression task\n        self.outcome = np.mean(targets[\"y\"])\n    else:\n        # Probability for classification task\n        self.outcome = np.bincount(targets[\"y\"], minlength=self.n_classes) / targets[\"y\"].shape[0]", "path": "MLAlgorithms/mla/ensemble/tree.py", "commit_date": "2020-09-13 00:00:00", "repo_name": "rushter/MLAlgorithms", "stars": 10223, "license": "mit", "language": "python", "size": 11924}
{"docstring": "\"\"\"Get the assignments for X with GMM clusters.\"\"\"\n", "func_signal": "def _predict(self, X):\n", "code": "if not X.shape:\n    return self.assignments\nlikelihoods = self._get_likelihood(X)\nweighted_likelihoods = self._get_weighted_likelihood(likelihoods)\nassignments = weighted_likelihoods.argmax(axis=1)\nreturn assignments", "path": "MLAlgorithms/mla/gaussian_mixture.py", "commit_date": "2020-08-20 00:00:00", "repo_name": "rushter/MLAlgorithms", "stars": 10223, "license": "mit", "language": "python", "size": 11924}
{"docstring": "\"\"\"Find best feature and value for a split. Greedy algorithm.\"\"\"\n\n# Sample random subset of features\n", "func_signal": "def _find_best_split(self, X, target, n_features):\n", "code": "subset = random.sample(list(range(0, X.shape[1])), n_features)\nmax_gain, max_col, max_val = None, None, None\n\nfor column in subset:\n    split_values = self._find_splits(X[:, column])\n    for value in split_values:\n        if self.loss is None:\n            # Random forest\n            splits = split(X[:, column], target[\"y\"], value)\n            gain = self.criterion(target[\"y\"], splits)\n        else:\n            # Gradient boosting\n            left, right = split_dataset(X, target, column, value, return_X=False)\n            gain = xgb_criterion(target, left, right, self.loss)\n\n        if (max_gain is None) or (gain > max_gain):\n            max_col, max_val, max_gain = column, value, gain\nreturn max_col, max_val, max_gain", "path": "MLAlgorithms/mla/ensemble/tree.py", "commit_date": "2020-09-13 00:00:00", "repo_name": "rushter/MLAlgorithms", "stars": 10223, "license": "mit", "language": "python", "size": 11924}
{"docstring": "\"\"\"Apply regularization to the loss.\"\"\"\n", "func_signal": "def _add_penalty(self, loss, w):\n", "code": "if self.penalty == \"l1\":\n    loss += self.C * np.abs(w[1:]).sum()\nelif self.penalty == \"l2\":\n    loss += (0.5 * self.C) * (w[1:] ** 2).sum()\nreturn loss", "path": "MLAlgorithms/mla/linear_models.py", "commit_date": "2019-08-25 00:00:00", "repo_name": "rushter/MLAlgorithms", "stars": 10223, "license": "mit", "language": "python", "size": 11924}
{"docstring": "\"\"\"Check if the difference of the latest two likelihood is less than the tolerance.\"\"\"\n", "func_signal": "def _is_converged(self):\n", "code": "if (len(self.likelihood) > 1) and (self.likelihood[-1] - self.likelihood[-2] <= self.tolerance):\n    return True\nreturn False", "path": "MLAlgorithms/mla/gaussian_mixture.py", "commit_date": "2020-08-20 00:00:00", "repo_name": "rushter/MLAlgorithms", "stars": 10223, "license": "mit", "language": "python", "size": 11924}
{"docstring": "\"\"\"Returns initialization function by the name.\"\"\"\n", "func_signal": "def get_initializer(name):\n", "code": "try:\n    return globals()[name]\nexcept Exception:\n    raise ValueError(\"Invalid initialization function.\")", "path": "MLAlgorithms/mla/neuralnet/initializations.py", "commit_date": "2019-08-25 00:00:00", "repo_name": "rushter/MLAlgorithms", "stars": 10223, "license": "mit", "language": "python", "size": 11924}
{"docstring": "\"\"\"Predict single row.\"\"\"\n", "func_signal": "def predict_row(self, row):\n", "code": "if not self.is_terminal:\n    if row[self.column_index] < self.threshold:\n        return self.left_child.predict_row(row)\n    else:\n        return self.right_child.predict_row(row)\nreturn self.outcome", "path": "MLAlgorithms/mla/ensemble/tree.py", "commit_date": "2020-09-13 00:00:00", "repo_name": "rushter/MLAlgorithms", "stars": 10223, "license": "mit", "language": "python", "size": 11924}
{"docstring": "\"\"\"Plot contour for 2D data.\"\"\"\n", "func_signal": "def plot(self, data=None, ax=None, holdon=False):\n", "code": "if not (len(self.X.shape) == 2 and self.X.shape[1] == 2):\n    raise AttributeError(\"Only support for visualizing 2D data.\")\n\nif ax is None:\n    _, ax = plt.subplots()\n\nif data is None:\n    data = self.X\n    assignments = self.assignments\nelse:\n    assignments = self.predict(data)\n\nCOLOR = \"bgrcmyk\"\ncmap = lambda assignment: COLOR[int(assignment) % len(COLOR)]\n\n# generate grid\ndelta = 0.025\nmargin = 0.2\nxmax, ymax = self.X.max(axis=0) + margin\nxmin, ymin = self.X.min(axis=0) - margin\naxis_X, axis_Y = np.meshgrid(np.arange(xmin, xmax, delta), np.arange(ymin, ymax, delta))\n\ndef grid_gaussian_pdf(mean, cov):\n    grid_array = np.array(list(zip(axis_X.flatten(), axis_Y.flatten())))\n    return multivariate_normal.pdf(grid_array, mean, cov).reshape(axis_X.shape)\n\n# plot scatters\nif assignments is None:\n    c = None\nelse:\n    c = [cmap(assignment) for assignment in assignments]\nax.scatter(data[:, 0], data[:, 1], c=c)\n\n# plot contours\nfor assignment in range(self.K):\n    ax.contour(\n        axis_X,\n        axis_Y,\n        grid_gaussian_pdf(self.means[assignment], self.covs[assignment]),\n        colors=cmap(assignment),\n    )\n\nif not holdon:\n    plt.show()", "path": "MLAlgorithms/mla/gaussian_mixture.py", "commit_date": "2020-08-20 00:00:00", "repo_name": "rushter/MLAlgorithms", "stars": 10223, "license": "mit", "language": "python", "size": 11924}
{"docstring": "# Generate a random binary classification problem.\n", "func_signal": "def classification():\n", "code": "X, y = make_classification(\n    n_samples=1000, n_features=10, n_informative=10, random_state=1111, n_classes=2, class_sep=2.5, n_redundant=0\n)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1111)\n\nmodel = NaiveBayesClassifier()\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)[:, 1]\n\nprint(\"classification accuracy\", roc_auc_score(y_test, predictions))", "path": "MLAlgorithms/examples/naive_bayes.py", "commit_date": "2019-05-11 00:00:00", "repo_name": "rushter/MLAlgorithms", "stars": 10223, "license": "mit", "language": "python", "size": 11924}
{"docstring": "\"\"\"Maximization (M-step) for Gaussian Mixture.\"\"\"\n", "func_signal": "def _M_step(self):\n", "code": "weights = self.responsibilities.sum(axis=0)\nfor assignment in range(self.K):\n    resp = self.responsibilities[:, assignment][:, np.newaxis]\n    self.means[assignment] = (resp * self.X).sum(axis=0) / resp.sum()\n    self.covs[assignment] = (self.X - self.means[assignment]).T.dot(\n        (self.X - self.means[assignment]) * resp\n    ) / weights[assignment]\nself.weights = weights / weights.sum()", "path": "MLAlgorithms/mla/gaussian_mixture.py", "commit_date": "2020-08-20 00:00:00", "repo_name": "rushter/MLAlgorithms", "stars": 10223, "license": "mit", "language": "python", "size": 11924}
{"docstring": "\"\"\"Increase specific weight by amount of the step parameter.\"\"\"\n", "func_signal": "def step(self, name, step):\n", "code": "self._params[name] += step\n\nif name in self.constraints:\n    self._params[name] = self.constraints[name].clip(self._params[name])", "path": "MLAlgorithms/mla/neuralnet/parameters.py", "commit_date": "2019-08-25 00:00:00", "repo_name": "rushter/MLAlgorithms", "stars": 10223, "license": "mit", "language": "python", "size": 11924}
{"docstring": "\"\"\"Build a decision tree from training set.\n\nParameters\n----------\n\nX : array-like\n    Feature dataset.\ntarget : dictionary or array-like\n    Target values.\nmax_features : int or None\n    The number of features to consider when looking for the best split.\nmin_samples_split : int\n    The minimum number of samples required to split an internal node.\nmax_depth : int\n    Maximum depth of the tree.\nminimum_gain : float, default 0.01\n    Minimum gain required for splitting.\nloss : function, default None\n    Loss function for gradient boosting.\n\"\"\"\n\n", "func_signal": "def train(self, X, target, max_features=None, min_samples_split=10, max_depth=None, minimum_gain=0.01, loss=None):\n", "code": "if not isinstance(target, dict):\n    target = {\"y\": target}\n\n# Loss for gradient boosting\nif loss is not None:\n    self.loss = loss\n\nif not self.regression:\n    self.n_classes = len(np.unique(target['y']))\n\nself._train(X, target, max_features=max_features, min_samples_split=min_samples_split,\n            max_depth=max_depth, minimum_gain=minimum_gain)", "path": "MLAlgorithms/mla/ensemble/tree.py", "commit_date": "2020-09-13 00:00:00", "repo_name": "rushter/MLAlgorithms", "stars": 10223, "license": "mit", "language": "python", "size": 11924}
{"docstring": "\"\"\"Update gradient values.\"\"\"\n", "func_signal": "def update_grad(self, name, value):\n", "code": "self._grads[name] = value\n\nif name in self.regularizers:\n    self._grads[name] += self.regularizers[name](self._params[name])", "path": "MLAlgorithms/mla/neuralnet/parameters.py", "commit_date": "2019-08-25 00:00:00", "repo_name": "rushter/MLAlgorithms", "stars": 10223, "license": "mit", "language": "python", "size": 11924}
{"docstring": "\"\"\"A container for layer's parameters.\n\nParameters\n----------\ninit : str, default 'glorot_uniform'.\n    The name of the weight initialization function.\nscale : float, default 0.5\nbias : float, default 1.0\n    Initial values for bias.\nregularizers : dict\n    Weight regularizers.\n    >>> {'W' : L2()}\nconstraints : dict\n    Weight constraints.\n    >>> {'b' : MaxNorm()}\n\"\"\"\n", "func_signal": "def __init__(self, init=\"glorot_uniform\", scale=0.5, bias=1.0, regularizers=None, constraints=None):\n", "code": "if constraints is None:\n    self.constraints = {}\nelse:\n    self.constraints = constraints\n\nif regularizers is None:\n    self.regularizers = {}\nelse:\n    self.regularizers = regularizers\n\nself.initial_bias = bias\nself.scale = scale\nself.init = get_initializer(init)\n\nself._params = {}\nself._grads = {}", "path": "MLAlgorithms/mla/neuralnet/parameters.py", "commit_date": "2019-08-25 00:00:00", "repo_name": "rushter/MLAlgorithms", "stars": 10223, "license": "mit", "language": "python", "size": 11924}
{"docstring": "\"\"\"Ensure inputs to an estimator are in the expected format.\n\nEnsures X and y are stored as numpy ndarrays by converting from an\narray-like object if necessary. Enables estimators to define whether\nthey require a set of y target values or not with y_required, e.g.\nkmeans clustering requires no target labels and is fit against only X.\n\nParameters\n----------\nX : array-like\n    Feature dataset.\ny : array-like\n    Target values. By default is required, but if y_required = false\n    then may be omitted.\n\"\"\"\n", "func_signal": "def _setup_input(self, X, y=None):\n", "code": "if not isinstance(X, np.ndarray):\n    X = np.array(X)\n\nif X.size == 0:\n    raise ValueError(\"Got an empty matrix.\")\n\nif X.ndim == 1:\n    self.n_samples, self.n_features = 1, X.shape\nelse:\n    self.n_samples, self.n_features = X.shape[0], np.prod(X.shape[1:])\n\nself.X = X\n\nif self.y_required:\n    if y is None:\n        raise ValueError(\"Missed required argument y\")\n\n    if not isinstance(y, np.ndarray):\n        y = np.array(y)\n\n    if y.size == 0:\n        raise ValueError(\"The targets array must be no-empty.\")\n\nself.y = y", "path": "MLAlgorithms/mla/base/base.py", "commit_date": "2020-08-20 00:00:00", "repo_name": "rushter/MLAlgorithms", "stars": 10223, "license": "mit", "language": "python", "size": 11924}
{"docstring": "# Convert values to probability\n", "func_signal": "def f_entropy(p):\n", "code": "p = np.bincount(p) / float(p.shape[0])\n\nep = stats.entropy(p)\nif ep == -float(\"inf\"):\n    return 0.0\nreturn ep", "path": "MLAlgorithms/mla/ensemble/base.py", "commit_date": "2019-08-25 00:00:00", "repo_name": "rushter/MLAlgorithms", "stars": 10223, "license": "mit", "language": "python", "size": 11924}
{"docstring": "# helper function to sample an index from a probability array\n", "func_signal": "def sample(preds, temperature=1.0):\n", "code": "preds = np.asarray(preds).astype(\"float64\")\npreds = np.log(preds) / temperature\nexp_preds = np.exp(preds)\npreds = exp_preds / np.sum(exp_preds)\nprobas = np.random.multinomial(1, preds, 1)\nreturn np.argmax(probas)", "path": "MLAlgorithms/examples/nnet_rnn_text_generation.py", "commit_date": "2019-05-11 00:00:00", "repo_name": "rushter/MLAlgorithms", "stars": 10223, "license": "mit", "language": "python", "size": 11924}
{"docstring": "\"\"\"Set the initial weights, means and covs (with full covariance matrix).\n\nweights: the prior of the clusters (what percentage of data does a cluster have)\nmeans: the mean points of the clusters\ncovs: the covariance matrix of the clusters\n\"\"\"\n", "func_signal": "def _initialize(self):\n", "code": "self.weights = np.ones(self.K)\nif self.init == \"random\":\n    self.means = [self.X[x] for x in random.sample(range(self.n_samples), self.K)]\n    self.covs = [np.cov(self.X.T) for _ in range(self.K)]\n\nelif self.init == \"kmeans\":\n    kmeans = KMeans(K=self.K, max_iters=self.max_iters // 3, init=\"++\")\n    kmeans.fit(self.X)\n    self.assignments = kmeans.predict()\n    self.means = kmeans.centroids\n    self.covs = []\n    for i in np.unique(self.assignments):\n        self.weights[int(i)] = (self.assignments == i).sum()\n        self.covs.append(np.cov(self.X[self.assignments == i].T))\nelse:\n    raise ValueError(\"Unknown type of init parameter\")\nself.weights /= self.weights.sum()", "path": "MLAlgorithms/mla/gaussian_mixture.py", "commit_date": "2020-08-20 00:00:00", "repo_name": "rushter/MLAlgorithms", "stars": 10223, "license": "mit", "language": "python", "size": 11924}
{"docstring": "\"\"\"Find all possible split values.\"\"\"\n", "func_signal": "def _find_splits(self, X):\n", "code": "split_values = set()\n\n# Get unique values in a sorted order\nx_unique = list(np.unique(X))\nfor i in range(1, len(x_unique)):\n    # Find a point between two values\n    average = (x_unique[i - 1] + x_unique[i]) / 2.0\n    split_values.add(average)\n\nreturn list(split_values)", "path": "MLAlgorithms/mla/ensemble/tree.py", "commit_date": "2020-09-13 00:00:00", "repo_name": "rushter/MLAlgorithms", "stars": 10223, "license": "mit", "language": "python", "size": 11924}
{"docstring": "# error is in an iframe so is annoying to read out - get it from the store\n", "func_signal": "def get_error_html(dash_duo, index):\n", "code": "return dash_duo.driver.execute_script(\n    \"return store.getState().error.backEnd[{}].error.html;\".format(index)\n)", "path": "dash/tests/integration/devtools/test_devtools_error_handling.py", "commit_date": "2020-07-06 00:00:00", "repo_name": "plotly/dash", "stars": 20224, "license": "mit", "language": "python", "size": 166114}
{"docstring": "# Start processing\n", "func_signal": "def _get_metadata(metadata_path):\n", "code": "with open(metadata_path) as data_file:\n    json_string = data_file.read()\n    data = json.JSONDecoder(object_pairs_hook=collections.OrderedDict).decode(\n        json_string\n    )\nreturn data", "path": "dash/dash/development/component_loader.py", "commit_date": "2020-07-06 00:00:00", "repo_name": "plotly/dash", "stars": 20224, "license": "mit", "language": "python", "size": 166114}
{"docstring": "\"\"\"Modify the DOM tree by adding new components in the callbacks.\"\"\"\n\n# some components don't exist in the initial render\n", "func_signal": "def test_cbsc002_callbacks_generating_children(dash_duo):\n", "code": "app = dash.Dash(__name__, suppress_callback_exceptions=True)\napp.layout = html.Div(\n    [dcc.Input(id=\"input\", value=\"initial value\"), html.Div(id=\"output\")]\n)\n\n@app.callback(Output(\"output\", \"children\"), [Input(\"input\", \"value\")])\ndef pad_output(input):\n    return html.Div(\n        [\n            dcc.Input(id=\"sub-input-1\", value=\"sub input initial value\"),\n            html.Div(id=\"sub-output-1\"),\n        ]\n    )\n\ncall_count = Value(\"i\", 0)\n\n@app.callback(Output(\"sub-output-1\", \"children\"), [Input(\"sub-input-1\", \"value\")])\ndef update_input(value):\n    call_count.value = call_count.value + 1\n    return value\n\ndash_duo.start_server(app)\n\ndash_duo.wait_for_text_to_equal(\"#sub-output-1\", \"sub input initial value\")\n\nassert call_count.value == 1, \"called once at initial stage\"\n\npad_input, pad_div = dash_duo.dash_innerhtml_dom.select_one(\n    \"#output > div\"\n).contents\n\nassert (\n    pad_input.attrs[\"value\"] == \"sub input initial value\"\n    and pad_input.attrs[\"id\"] == \"sub-input-1\"\n)\nassert pad_input.name == \"input\"\n\nassert (\n    pad_div.text == pad_input.attrs[\"value\"] and pad_div.get(\"id\") == \"sub-output-1\"\n), \"the sub-output-1 content reflects to sub-input-1 value\"\n\ndash_duo.percy_snapshot(name=\"callback-generating-function-1\")\n\npaths = dash_duo.redux_state_paths\nassert paths[\"objs\"] == {}\nassert paths[\"strs\"] == {\n    \"input\": [\"props\", \"children\", 0],\n    \"output\": [\"props\", \"children\", 1],\n    \"sub-input-1\": [\n        \"props\",\n        \"children\",\n        1,\n        \"props\",\n        \"children\",\n        \"props\",\n        \"children\",\n        0,\n    ],\n    \"sub-output-1\": [\n        \"props\",\n        \"children\",\n        1,\n        \"props\",\n        \"children\",\n        \"props\",\n        \"children\",\n        1,\n    ],\n}, \"the paths should include these new output IDs\"\n\n# editing the input should modify the sub output\ndash_duo.find_element(\"#sub-input-1\").send_keys(\"deadbeef\")\n\nassert (\n    dash_duo.find_element(\"#sub-output-1\").text\n    == pad_input.attrs[\"value\"] + \"deadbeef\"\n), \"deadbeef is added\"\n\n# the total updates is initial one + the text input changes\ndash_duo.wait_for_text_to_equal(\n    \"#sub-output-1\", pad_input.attrs[\"value\"] + \"deadbeef\"\n)\n\nassert not dash_duo.redux_state_is_loading, \"loadingMap is empty\"\n\ndash_duo.percy_snapshot(name=\"callback-generating-function-2\")\nassert dash_duo.get_logs() == [], \"console is clean\"", "path": "dash/tests/integration/callbacks/test_basic_callback.py", "commit_date": "2020-12-16 00:00:00", "repo_name": "plotly/dash", "stars": 20224, "license": "mit", "language": "python", "size": 166114}
{"docstring": "\"\"\"Records timing information for a server resource.\n\n:param name: The name of the resource.\n:type name: string\n\n:param duration: The time in seconds to report. Internally, this\n    is rounded to the nearest millisecond.\n:type duration: float or None\n\n:param description: A description of the resource.\n:type description: string or None\n\"\"\"\n", "func_signal": "def record_timing(name, duration=None, description=None):\n", "code": "timing_information = getattr(flask.g, \"timing_information\", {})\n\nif name in timing_information:\n    raise KeyError('Duplicate resource name \"{}\" found.'.format(name))\n\ntiming_information[name] = {\"dur\": round(duration * 1000), \"desc\": description}\n\nsetattr(flask.g, \"timing_information\", timing_information)", "path": "dash/dash/_callback_context.py", "commit_date": "2020-09-03 00:00:00", "repo_name": "plotly/dash", "stars": 20224, "license": "mit", "language": "python", "size": 166114}
{"docstring": "\"\"\"Load React component metadata into a format Dash can parse, then create\nPython class files.\n\nUsage: generate_classes()\n\nKeyword arguments:\nnamespace -- name of the generated Python package (also output dir)\n\nmetadata_path -- a path to a JSON file created by\n[`react-docgen`](https://github.com/reactjs/react-docgen).\n\nReturns:\n\"\"\"\n\n", "func_signal": "def generate_classes(namespace, metadata_path=\"lib/metadata.json\"):\n", "code": "data = _get_metadata(metadata_path)\nimports_path = os.path.join(namespace, \"_imports_.py\")\n\n# Make sure the file doesn't exist, as we use append write\nif os.path.exists(imports_path):\n    os.remove(imports_path)\n\ncomponents = generate_classes_files(namespace, data, generate_class_file)\n\n# Add the __all__ value so we can import * from _imports_\ngenerate_imports(namespace, components)", "path": "dash/dash/development/component_loader.py", "commit_date": "2020-07-06 00:00:00", "repo_name": "plotly/dash", "stars": 20224, "license": "mit", "language": "python", "size": 166114}
{"docstring": "# this one is an error!\n", "func_signal": "def test_cbmi003_some_missing_inputs(dash_duo):\n", "code": "app = dash.Dash(__name__, suppress_callback_exceptions=True)\napp.layout = html.Div(\n    [\n        html.Div(\"Title\", id=\"title\"),\n        html.Button(\"click\", id=\"btn\"),\n        html.Div(id=\"content\"),\n        html.Div(\"output1 init\", id=\"out1\"),\n        html.Div(\"output2 init\", id=\"out2\"),\n    ]\n)\n\n@app.callback(Output(\"content\", \"children\"), [Input(\"btn\", \"n_clicks\")])\ndef content(n):\n    return [html.Div(\"A\", id=\"a\"), html.Div(\"B\", id=\"b\")] if n else \"content init\"\n\n@app.callback(\n    Output(\"out1\", \"children\"), [Input(\"a\", \"children\"), Input(\"title\", \"children\")]\n)\ndef out1(a, title):\n    return a + title\n\n@app.callback(Output(\"out2\", \"children\"), [Input(\"out1\", \"children\")])\ndef out2(out1):\n    return out1 + \" - 2\"\n\ndash_duo.start_server(app)\nwait_for_queue(dash_duo)\n\ndash_duo.wait_for_text_to_equal(\"#content\", \"content init\")\n\nlogs = json.dumps(dash_duo.get_logs())\nassert \"ReferenceError\" in logs\nassert \"The id of this object is `a` and the property is `children`.\" in logs\n\n# after the error we can still use the app - and no more errors\ndash_duo.find_element(\"#btn\").click()\ndash_duo.wait_for_text_to_equal(\"#out2\", \"ATitle - 2\")\nassert not dash_duo.get_logs()", "path": "dash/tests/integration/callbacks/test_missing_inputs.py", "commit_date": "2020-06-15 00:00:00", "repo_name": "plotly/dash", "stars": 20224, "license": "mit", "language": "python", "size": 166114}
{"docstring": "# this variant does NOT get called with empty ALL\n# the second callback handles outputting 0 in that case.\n# if it were to be called throw an error so we'll see it in get_logs\n", "func_signal": "def content_inner(n2):\n", "code": "n1i = len(dash.callback_context.outputs_list[0])\nn1j = len(dash.callback_context.outputs_list[1])\nif not n1i + n1j:\n    raise ValueError(\"should not be called with no outputs!\")\nreturn [n2 or 0] * n1i, [(n2 or 0) + 2] * n1j", "path": "dash/tests/integration/callbacks/test_missing_outputs.py", "commit_date": "2020-12-16 00:00:00", "repo_name": "plotly/dash", "stars": 20224, "license": "mit", "language": "python", "size": 166114}
{"docstring": "# this variant does NOT get called with empty ALL\n# the second callback handles outputting 0 in that case.\n# if it were to be called throw an error so we'll see it in get_logs\n", "func_signal": "def content_inner(n2):\n", "code": "n1 = len(dash.callback_context.outputs_list)\nif not n1:\n    raise ValueError(\"should not be called with no outputs!\")\nreturn [n2 or 0] * n1", "path": "dash/tests/integration/callbacks/test_missing_outputs.py", "commit_date": "2020-12-16 00:00:00", "repo_name": "plotly/dash", "stars": 20224, "license": "mit", "language": "python", "size": 166114}
{"docstring": "# this variant does NOT get called with empty ALL\n# the second callback handles outputting 0 in that case.\n# if it were to be called throw an error so we'll see it in get_logs\n", "func_signal": "def content_inner(n2):\n", "code": "n1 = len(dash.callback_context.outputs_list)\nif not n1:\n    raise ValueError(\"should not be called with no outputs!\")\nreturn [n2 or 0] * n1", "path": "dash/tests/integration/callbacks/test_missing_outputs.py", "commit_date": "2020-12-16 00:00:00", "repo_name": "plotly/dash", "stars": 20224, "license": "mit", "language": "python", "size": 166114}
{"docstring": "\"\"\"Load React component metadata into a format Dash can parse.\n\nUsage: load_components('../../component-suites/lib/metadata.json')\n\nKeyword arguments:\nmetadata_path -- a path to a JSON file created by\n[`react-docgen`](https://github.com/reactjs/react-docgen).\n\nReturns:\ncomponents -- a list of component objects with keys\n`type`, `valid_kwargs`, and `setup`.\n\"\"\"\n\n# Register the component lib for index include.\n", "func_signal": "def load_components(metadata_path, namespace=\"default_namespace\"):\n", "code": "ComponentRegistry.registry.add(namespace)\ncomponents = []\n\ndata = _get_metadata(metadata_path)\n\n# Iterate over each property name (which is a path to the component)\nfor componentPath in data:\n    componentData = data[componentPath]\n\n    # Extract component name from path\n    # e.g. src/components/MyControl.react.js\n    # TODO Make more robust - some folks will write .jsx and others\n    # will be on windows. Unfortunately react-docgen doesn't include\n    # the name of the component atm.\n    name = componentPath.split(\"/\").pop().split(\".\")[0]\n    component = generate_class(\n        name, componentData[\"props\"], componentData[\"description\"], namespace\n    )\n\n    components.append(component)\n\nreturn components", "path": "dash/dash/development/component_loader.py", "commit_date": "2020-07-06 00:00:00", "repo_name": "plotly/dash", "stars": 20224, "license": "mit", "language": "python", "size": 166114}
{"docstring": "# this variant *does* get called with empty ALL, because of the\n# second Output\n# TODO: however it doesn't get *triggered* by the ALL emptying,\n# should it? for now, added content_ids to get proper triggering.\n", "func_signal": "def content_and_output(n2, content_ids):\n", "code": "n1 = len(content_ids)\ncontent = [n2 or 0] * n1\nreturn content, sum(content)", "path": "dash/tests/integration/callbacks/test_missing_outputs.py", "commit_date": "2020-12-16 00:00:00", "repo_name": "plotly/dash", "stars": 20224, "license": "mit", "language": "python", "size": 166114}
{"docstring": "# mostly for cases where no callbacks should fire:\n# just wait until we have the button and the queue is empty\n", "func_signal": "def wait_for_queue(dash_duo):\n", "code": "dash_duo.wait_for_text_to_equal(\"#btn\", \"click\")\nwait.until(lambda: not dash_duo.redux_state_is_loading, 3)", "path": "dash/tests/integration/callbacks/test_missing_inputs.py", "commit_date": "2020-06-15 00:00:00", "repo_name": "plotly/dash", "stars": 20224, "license": "mit", "language": "python", "size": 166114}
{"docstring": "# this variant *does* get called with empty ALL, because of the\n# second Output\n# TODO: however it doesn't get *triggered* by the ALL emptying,\n# should it? for now, added content_ids to get proper triggering.\n", "func_signal": "def content_and_output(n2, content_ids):\n", "code": "n1 = len(content_ids)\ncontent = [n2 or 0] * n1\nreturn content, sum(content)", "path": "dash/tests/integration/callbacks/test_missing_outputs.py", "commit_date": "2020-12-16 00:00:00", "repo_name": "plotly/dash", "stars": 20224, "license": "mit", "language": "python", "size": 166114}
{"docstring": "# if only SOME of the inputs are multi wildcards, even with an output,\n# we DO NOT fire the callback\n", "func_signal": "def test_cbmi008_multi_wildcards_and_simple_all_missing(dash_duo):\n", "code": "app = dash.Dash(__name__, suppress_callback_exceptions=True)\napp.layout = html.Div(\n    [\n        html.Div(\"Title\", id=\"title\"),\n        html.Button(\"click\", id=\"btn\"),\n        html.Div(id=\"content\"),\n        html.Div(\"output1 init\", id=\"out1\"),\n        html.Div(\"output2 init\", id=\"out2\"),\n    ]\n)\n\n@app.callback(Output(\"content\", \"children\"), [Input(\"btn\", \"n_clicks\")])\ndef content(n):\n    out = [html.Div(\"item {}\".format(i), id={\"i\": i}) for i in range(n or 0)]\n    return (out + [html.Div(\"A\", id=\"a\")]) if out else \"content init\"\n\n@app.callback(\n    Output(\"out1\", \"children\"),\n    [Input({\"i\": ALL}, \"children\"), Input(\"a\", \"children\")],\n)\ndef out1(items, a):\n    return a + \" - \" + (\", \".join(items) or \"no items\")\n\n@app.callback(Output(\"out2\", \"children\"), [Input(\"out1\", \"children\")])\ndef out2(out1):\n    return out1 + \" - 2\"\n\ndash_duo.start_server(app)\nwait_for_queue(dash_duo)\n\ndash_duo.wait_for_text_to_equal(\"#content\", \"content init\")\n\nassert dash_duo.find_element(\"#out1\").text == \"output1 init\"\nassert dash_duo.find_element(\"#out2\").text == \"output2 init\"\n\ndash_duo.find_element(\"#btn\").click()\ndash_duo.wait_for_text_to_equal(\"#out1\", \"A - item 0\")\ndash_duo.wait_for_text_to_equal(\"#out2\", \"A - item 0 - 2\")\n\ndash_duo.find_element(\"#btn\").click()\ndash_duo.wait_for_text_to_equal(\"#out1\", \"A - item 0, item 1\")\ndash_duo.wait_for_text_to_equal(\"#out2\", \"A - item 0, item 1 - 2\")\n\nassert not dash_duo.get_logs()", "path": "dash/tests/integration/callbacks/test_missing_inputs.py", "commit_date": "2020-06-15 00:00:00", "repo_name": "plotly/dash", "stars": 20224, "license": "mit", "language": "python", "size": 166114}
{"docstring": "# title exists at the start but State isn't enough to fire the callback\n", "func_signal": "def out1(a, b, c, title):\n", "code": "assert c == \"C\"\nassert title == \"Title\"\nreturn a + b", "path": "dash/tests/integration/callbacks/test_missing_inputs.py", "commit_date": "2020-06-15 00:00:00", "repo_name": "plotly/dash", "stars": 20224, "license": "mit", "language": "python", "size": 166114}
{"docstring": "# if all the inputs are multi wildcards, but there's NO output,\n# we DO NOT fire the callback\n", "func_signal": "def test_cbmi006_all_multi_wildcards_no_outputs(dash_duo):\n", "code": "app = dash.Dash(__name__, suppress_callback_exceptions=True)\napp.layout = html.Div(\n    [\n        html.Div(\"Title\", id=\"title\"),\n        html.Button(\"click\", id=\"btn\"),\n        html.Div(id=\"content\"),\n        html.Div(\"output2 init\", id=\"out2\"),\n    ]\n)\n\n@app.callback(Output(\"content\", \"children\"), [Input(\"btn\", \"n_clicks\")])\ndef content(n):\n    out = [html.Div(\"item {}\".format(i), id={\"i\": i}) for i in range(n or 0)]\n    return (out + [html.Div(\"output1 init\", id=\"out1\")]) if out else \"content init\"\n\n@app.callback(Output(\"out1\", \"children\"), [Input({\"i\": ALL}, \"children\")])\ndef out1(items):\n    return \", \".join(items) or \"no items\"\n\n@app.callback(Output(\"out2\", \"children\"), [Input(\"out1\", \"children\")])\ndef out2(out1):\n    return out1 + \" - 2\"\n\ndash_duo.start_server(app)\nwait_for_queue(dash_duo)\n\ndash_duo.wait_for_text_to_equal(\"#out2\", \"output2 init\")\n\nassert dash_duo.find_element(\"#content\").text == \"content init\"\n\ndash_duo.find_element(\"#btn\").click()\ndash_duo.wait_for_text_to_equal(\"#out1\", \"item 0\")\ndash_duo.wait_for_text_to_equal(\"#out2\", \"item 0 - 2\")\n\ndash_duo.find_element(\"#btn\").click()\ndash_duo.wait_for_text_to_equal(\"#out1\", \"item 0, item 1\")\ndash_duo.wait_for_text_to_equal(\"#out2\", \"item 0, item 1 - 2\")\n\nassert not dash_duo.get_logs()", "path": "dash/tests/integration/callbacks/test_missing_inputs.py", "commit_date": "2020-06-15 00:00:00", "repo_name": "plotly/dash", "stars": 20224, "license": "mit", "language": "python", "size": 166114}
{"docstring": "# This is a funny case, that seems to mostly happen with dcc.Location\n# but in principle could happen in other cases too:\n# A callback chain (in this case the initial hydration) is set to update a\n# value, but after that callback is queued and before it returns, that value\n# is also set explicitly from the front end (in this case Location.pathname,\n# which gets set in its componentDidMount during the render process, and\n# callbacks are delayed until after rendering is finished because of the\n# async table)\n# At one point in the wildcard PR #1103, changing from requestQueue to\n# pendingCallbacks, calling PreventUpdate in the callback would also skip\n# any callbacks that depend on pathname, despite the new front-end-provided\n# value.\n\n", "func_signal": "def test_cbsc007_parallel_updates(refresh, dash_duo):\n", "code": "app = dash.Dash()\n\napp.layout = html.Div(\n    [\n        dcc.Location(id=\"loc\", refresh=refresh),\n        html.Button(\"Update path\", id=\"btn\"),\n        dash_table.DataTable(id=\"t\", columns=[{\"name\": \"a\", \"id\": \"a\"}]),\n        html.Div(id=\"out\"),\n    ]\n)\n\n@app.callback(Output(\"t\", \"data\"), [Input(\"loc\", \"pathname\")])\ndef set_data(path):\n    return [{\"a\": (path or repr(path)) + \":a\"}]\n\n@app.callback(\n    Output(\"out\", \"children\"), [Input(\"loc\", \"pathname\"), Input(\"t\", \"data\")]\n)\ndef set_out(path, data):\n    return json.dumps(data) + \" - \" + (path or repr(path))\n\n@app.callback(Output(\"loc\", \"pathname\"), [Input(\"btn\", \"n_clicks\")])\ndef set_path(n):\n    if not n:\n        raise PreventUpdate\n\n    return \"/{0}\".format(n)\n\ndash_duo.start_server(app)\n\ndash_duo.wait_for_text_to_equal(\"#out\", '[{\"a\": \"/:a\"}] - /')\ndash_duo.find_element(\"#btn\").click()\n# the refresh=True case here is testing that we really do get the right\n# pathname, not the prevented default value from the layout.\ndash_duo.wait_for_text_to_equal(\"#out\", '[{\"a\": \"/1:a\"}] - /1')\nif not refresh:\n    dash_duo.find_element(\"#btn\").click()\n    dash_duo.wait_for_text_to_equal(\"#out\", '[{\"a\": \"/2:a\"}] - /2')", "path": "dash/tests/integration/callbacks/test_basic_callback.py", "commit_date": "2020-12-16 00:00:00", "repo_name": "plotly/dash", "stars": 20224, "license": "mit", "language": "python", "size": 166114}
{"docstring": "# same as above (cbmi006) but multi-output, some outputs present some missing.\n# Again we DO NOT fire the callback\n", "func_signal": "def test_cbmi007_all_multi_wildcards_some_outputs(dash_duo):\n", "code": "app = dash.Dash(__name__, suppress_callback_exceptions=True)\napp.layout = html.Div(\n    [\n        html.Div(\"Title\", id=\"title\"),\n        html.Button(\"click\", id=\"btn\"),\n        html.Div(id=\"content\"),\n        html.Div(\"output2 init\", id=\"out2\"),\n        html.Div(\"output3 init\", id=\"out3\"),\n    ]\n)\n\n@app.callback(Output(\"content\", \"children\"), [Input(\"btn\", \"n_clicks\")])\ndef content(n):\n    out = [html.Div(\"item {}\".format(i), id={\"i\": i}) for i in range(n or 0)]\n    return (out + [html.Div(\"output1 init\", id=\"out1\")]) if out else \"content init\"\n\n@app.callback(\n    [Output(\"out1\", \"children\"), Output(\"out3\", \"children\")],\n    [Input({\"i\": ALL}, \"children\")],\n)\ndef out1(items):\n    out = \", \".join(items) or \"no items\"\n    return out, (out + \" - 3\")\n\n@app.callback(Output(\"out2\", \"children\"), [Input(\"out1\", \"children\")])\ndef out2(out1):\n    return out1 + \" - 2\"\n\ndash_duo.start_server(app)\nwait_for_queue(dash_duo)\n\ndash_duo.wait_for_text_to_equal(\"#out2\", \"output2 init\")\ndash_duo.wait_for_text_to_equal(\"#out3\", \"output3 init\")\n\nassert dash_duo.find_element(\"#content\").text == \"content init\"\n\ndash_duo.find_element(\"#btn\").click()\ndash_duo.wait_for_text_to_equal(\"#out1\", \"item 0\")\ndash_duo.wait_for_text_to_equal(\"#out2\", \"item 0 - 2\")\ndash_duo.wait_for_text_to_equal(\"#out3\", \"item 0 - 3\")\n\ndash_duo.find_element(\"#btn\").click()\ndash_duo.wait_for_text_to_equal(\"#out1\", \"item 0, item 1\")\ndash_duo.wait_for_text_to_equal(\"#out2\", \"item 0, item 1 - 2\")\ndash_duo.wait_for_text_to_equal(\"#out3\", \"item 0, item 1 - 3\")\n\nassert not dash_duo.get_logs()", "path": "dash/tests/integration/callbacks/test_missing_inputs.py", "commit_date": "2020-06-15 00:00:00", "repo_name": "plotly/dash", "stars": 20224, "license": "mit", "language": "python", "size": 166114}
{"docstring": "# if all the inputs are multi wildcards, AND there's an output,\n# we DO fire the callback\n", "func_signal": "def test_cbmi005_all_multi_wildcards_with_output(dash_duo):\n", "code": "app = dash.Dash(__name__, suppress_callback_exceptions=True)\napp.layout = html.Div(\n    [\n        html.Div(\"Title\", id=\"title\"),\n        html.Button(\"click\", id=\"btn\"),\n        html.Div(id=\"content\"),\n        html.Div(\"output1 init\", id=\"out1\"),\n        html.Div(\"output2 init\", id=\"out2\"),\n    ]\n)\n\n@app.callback(Output(\"content\", \"children\"), [Input(\"btn\", \"n_clicks\")])\ndef content(n):\n    out = [html.Div(\"item {}\".format(i), id={\"i\": i}) for i in range(n or 0)]\n    return out or \"content init\"\n\n@app.callback(Output(\"out1\", \"children\"), [Input({\"i\": ALL}, \"children\")])\ndef out1(items):\n    return \", \".join(items) or \"no items\"\n\n@app.callback(Output(\"out2\", \"children\"), [Input(\"out1\", \"children\")])\ndef out2(out1):\n    return out1 + \" - 2\"\n\ndash_duo.start_server(app)\nwait_for_queue(dash_duo)\n\ndash_duo.wait_for_text_to_equal(\"#out2\", \"no items - 2\")\n\nassert dash_duo.find_element(\"#out1\").text == \"no items\"\nassert dash_duo.find_element(\"#content\").text == \"content init\"\n\ndash_duo.find_element(\"#btn\").click()\ndash_duo.wait_for_text_to_equal(\"#out1\", \"item 0\")\ndash_duo.wait_for_text_to_equal(\"#out2\", \"item 0 - 2\")\n\ndash_duo.find_element(\"#btn\").click()\ndash_duo.wait_for_text_to_equal(\"#out1\", \"item 0, item 1\")\ndash_duo.wait_for_text_to_equal(\"#out2\", \"item 0, item 1 - 2\")\n\nassert not dash_duo.get_logs()", "path": "dash/tests/integration/callbacks/test_missing_inputs.py", "commit_date": "2020-06-15 00:00:00", "repo_name": "plotly/dash", "stars": 20224, "license": "mit", "language": "python", "size": 166114}
{"docstring": "# Kind of contrived - MATCH will always be 0. Just want to make sure\n# that this behaves the same as cbmi001\n", "func_signal": "def test_cbmi009_match_wildcards_all_missing(dash_duo):\n", "code": "app = dash.Dash(__name__, suppress_callback_exceptions=True)\napp.layout = html.Div(\n    [\n        html.Div(\"Title\", id={\"i\": 0, \"id\": \"title\"}),\n        html.Button(\"click\", id=\"btn\"),\n        html.Div(id=\"content\"),\n        html.Div(\"output1 init\", id={\"i\": 0, \"id\": \"out1\"}),\n        html.Div(\"output2 init\", id={\"i\": 0, \"id\": \"out2\"}),\n        html.Div(\"output3 init\", id={\"i\": 0, \"id\": \"out3\"}),\n    ]\n)\n\n@app.callback(Output(\"content\", \"children\"), [Input(\"btn\", \"n_clicks\")])\ndef content(n):\n    return (\n        [\n            html.Div(\"A\", id={\"i\": 0, \"id\": \"a\"}),\n            html.Div(\"B\", id={\"i\": 0, \"id\": \"b\"}),\n            html.Div(\"C\", id={\"i\": 0, \"id\": \"c\"}),\n        ]\n        if n\n        else \"content init\"\n    )\n\n@app.callback(\n    Output({\"i\": MATCH, \"id\": \"out1\"}, \"children\"),\n    [\n        Input({\"i\": MATCH, \"id\": \"a\"}, \"children\"),\n        Input({\"i\": MATCH, \"id\": \"b\"}, \"children\"),\n    ],\n    [\n        State({\"i\": MATCH, \"id\": \"c\"}, \"children\"),\n        State({\"i\": MATCH, \"id\": \"title\"}, \"children\"),\n    ],\n)\ndef out1(a, b, c, title):\n    # title exists at the start but State isn't enough to fire the callback\n    assert c == \"C\"\n    assert title == \"Title\"\n    return a + b\n\n@app.callback(\n    Output({\"i\": MATCH, \"id\": \"out2\"}, \"children\"),\n    [Input({\"i\": MATCH, \"id\": \"out1\"}, \"children\")],\n    [State({\"i\": MATCH, \"id\": \"title\"}, \"children\")],\n)\ndef out2(out1, title):\n    return out1 + \" - 2 - \" + title\n\n@app.callback(\n    Output({\"i\": MATCH, \"id\": \"out3\"}, \"children\"),\n    [\n        Input({\"i\": MATCH, \"id\": \"out1\"}, \"children\"),\n        Input({\"i\": MATCH, \"id\": \"title\"}, \"children\"),\n    ],\n)\ndef out3(out1, title):\n    return out1 + \" - 3 - \" + title\n\ndash_duo.start_server(app)\nwait_for_queue(dash_duo)\n\ndef cssid(v):\n    # escaped CSS for object IDs\n    return r\"#\\{\\\"i\\\"\\:0\\,\\\"id\\\"\\:\\\"\" + v + r\"\\\"\\}\"\n\n# out3 fires because it has another Input besides out1\ndash_duo.wait_for_text_to_equal(cssid(\"out3\"), \"output1 init - 3 - Title\")\n\nassert dash_duo.find_element(cssid(\"out1\")).text == \"output1 init\"\n\n# out2 doesn't fire because its only input (out1) is \"prevented\"\n# State items don't matter for this.\nassert dash_duo.find_element(cssid(\"out2\")).text == \"output2 init\"\n\ndash_duo.find_element(\"#btn\").click()\n\n# after a button click all inputs are present so all callbacks fire.\ndash_duo.wait_for_text_to_equal(cssid(\"out1\"), \"AB\")\ndash_duo.wait_for_text_to_equal(cssid(\"out2\"), \"AB - 2 - Title\")\ndash_duo.wait_for_text_to_equal(cssid(\"out3\"), \"AB - 3 - Title\")\n\nassert not dash_duo.get_logs()", "path": "dash/tests/integration/callbacks/test_missing_inputs.py", "commit_date": "2020-06-15 00:00:00", "repo_name": "plotly/dash", "stars": 20224, "license": "mit", "language": "python", "size": 166114}
{"docstring": "# TODO, need more investigation\n# there is rref leak when shutting down, suspect it is because\n# ref as arg is passed to pybind boundary, and the ref is not garbage\n# collected by python when calling shutdown()\n", "func_signal": "def test_trainer_ps_torchscript_functions(self):\n", "code": "import torch.distributed.rpc.api as api\napi._ignore_rref_leak = True\n\nself._test_trainer_ps(create_torchscript_tensor, _run_trainer_torchscript)", "path": "pytorch/torch/testing/_internal/distributed/rpc/dist_autograd_test.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "pytorch/pytorch", "stars": 76254, "license": "other", "language": "python", "size": 1041248}
{"docstring": "# local version\n", "func_signal": "def test_dist_optim(self):\n", "code": "module1 = MyModule()\nmodule2 = MyModule()\nparams = [module1.get_w(), module2.get_w()]\nlocal_optim = optim.SGD(params, lr=0.05)\n\nold_w1 = module1.w.clone().detach()\nold_w2 = module2.w.clone().detach()\n\ng_cpu = torch.Generator()\ng_cpu.manual_seed(0)\nt1 = torch.rand((3, 3), requires_grad=True, generator=g_cpu)\nt2 = torch.rand((3, 3), requires_grad=True, generator=g_cpu)\noutput1 = module1.forward(t2)\noutput2 = module2.forward(output1)\nloss = torch.add(output2, t1).sum()\n\nloss.backward()\nlocal_optim.step()\n\n# distributed version\nowner1 = \"worker%d\" % ((self.rank + 1) % self.world_size)\nowner2 = \"worker%d\" % ((self.rank + 2) % self.world_size)\n\nremote_module1 = rpc.remote(owner1, MyModule)\nremote_module2 = rpc.remote(owner2, MyModule)\nremote_param1 = remote_method(MyModule.get_w, remote_module1)\nremote_param2 = remote_method(MyModule.get_w, remote_module2)\n\nold_w1_remote = remote_param1.to_here()\n\n# sanity check: local and remote initial weights should match\nself.assertEqual(old_w1, remote_param1.to_here())\nself.assertEqual(old_w2, remote_param2.to_here())\n\ndist_optim = DistributedOptimizer(\n    optim.SGD, [remote_param1, remote_param2], lr=0.05\n)\n\nwith dist_autograd.context() as context_id:\n    g_cpu.manual_seed(0)\n    t1 = torch.rand((3, 3), requires_grad=True, generator=g_cpu)\n    t2 = torch.rand((3, 3), requires_grad=True, generator=g_cpu)\n    output1 = rpc_async_method(MyModule.forward, remote_module1, t2)\n    output2 = rpc_async_method(MyModule.forward, remote_module2, output1.wait())\n    loss = torch.add(output2.wait(), t1)\n\n    dist_autograd.backward(context_id, [loss.sum()])\n    dist_optim.step(context_id)\n\n    new_w1 = rpc_async_method(MyModule.get_w, remote_module1).wait()\n    new_w2 = rpc_async_method(MyModule.get_w, remote_module2).wait()\n\n    # ensure optimizer changed weights\n    self.assertNotEqual(old_w1, new_w1)\n    self.assertNotEqual(old_w2, new_w2)\n    # ensure local equals remote\n    self.assertEqual(new_w1, module1.get_w())\n    self.assertEqual(new_w2, module2.get_w())", "path": "pytorch/torch/testing/_internal/distributed/rpc/dist_optimizer_test.py", "commit_date": "2020-09-26 00:00:00", "repo_name": "pytorch/pytorch", "stars": 76254, "license": "other", "language": "python", "size": 1041248}
{"docstring": "# distributed version\n", "func_signal": "def test_dist_optim_exception(self):\n", "code": "owner1 = \"worker%d\" % ((self.rank + 1) % self.world_size)\nowner2 = \"worker%d\" % ((self.rank + 2) % self.world_size)\n\nremote_module1 = rpc.remote(owner1, MyModule)\nremote_module2 = rpc.remote(owner2, MyModule)\nremote_param1 = remote_method(MyModule.get_w, remote_module1)\nremote_param2 = remote_method(MyModule.get_w, remote_module2)\n\ndist_optim = DistributedOptimizer(\n    FailingOptimizer, [remote_param1, remote_param2]\n)\n\nwith dist_autograd.context() as context_id:\n    g_cpu = torch.Generator()\n    g_cpu.manual_seed(0)\n    t1 = torch.rand((3, 3), requires_grad=True, generator=g_cpu)\n    t2 = torch.rand((3, 3), requires_grad=True, generator=g_cpu)\n    output1 = rpc_async_method(MyModule.forward, remote_module1, t2)\n    output2 = rpc_async_method(MyModule.forward, remote_module2, output1.wait())\n    loss = torch.add(output2.wait(), t1).sum()\n\n    dist_autograd.backward(context_id, [loss])\n    with self.assertRaisesRegex(Exception, \"Error running optimizer\"):\n        dist_optim.step(context_id)", "path": "pytorch/torch/testing/_internal/distributed/rpc/dist_optimizer_test.py", "commit_date": "2020-09-26 00:00:00", "repo_name": "pytorch/pytorch", "stars": 76254, "license": "other", "language": "python", "size": 1041248}
{"docstring": "# Run the test in a thread which would never finish.\n", "func_signal": "def test_backward_unused_send_function(self):\n", "code": "t = threading.Thread(\n    target=self._run_test_backward_unused_send_function_in_thread\n)\nt.daemon = True\nt.start()\nt.join(10)  # Wait for 10s.\n\n# Verify thread is still alive (indicating backward hasn't completed yet).\nself.assertTrue(t.is_alive())", "path": "pytorch/torch/testing/_internal/distributed/rpc/dist_autograd_test.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "pytorch/pytorch", "stars": 76254, "license": "other", "language": "python", "size": 1041248}
{"docstring": "\"\"\"\nCall rpc.rpc_async on a method in a remote object.\n\nArgs:\n    method: the method (for example, Class.method)\n    obj_rref (RRef): remote reference to the object\n    args: positional arguments to pass to the method\n    kwargs: keyword arguments to pass to the method\n\nReturns a Future to the method call result.\n\"\"\"\n", "func_signal": "def rpc_async_method(method, obj_rref, *args, **kwargs):\n", "code": "return rpc.rpc_async(\n    obj_rref.owner(),\n    _call_method,\n    args=[method, obj_rref] + list(args),\n    kwargs=kwargs,\n)", "path": "pytorch/torch/testing/_internal/distributed/rpc/dist_optimizer_test.py", "commit_date": "2020-09-26 00:00:00", "repo_name": "pytorch/pytorch", "stars": 76254, "license": "other", "language": "python", "size": 1041248}
{"docstring": "# cannot directly use torch.manual_seed(0) as all threads share the same\n# default generator. The race from multiple RPC threads could mess up\n# the draw order from the default RNG instance, leading to\n# non-deterministic behavior. Hence, create a dedicated RNG here.\n", "func_signal": "def __init__(self):\n", "code": "g_cpu = torch.Generator()\ng_cpu.manual_seed(0)\nself.w = torch.rand((3, 3), requires_grad=True, generator=g_cpu)", "path": "pytorch/torch/testing/_internal/distributed/rpc/dist_optimizer_test.py", "commit_date": "2020-09-26 00:00:00", "repo_name": "pytorch/pytorch", "stars": 76254, "license": "other", "language": "python", "size": 1041248}
{"docstring": "\"\"\"\nThis test simulates the situation where the 'backward' call might throw\nan exception locally which would lead to the autograd context being\ncleaned up if we're using the context manager. As a result, the autograd\ncontext might be cleaned up while some threads are still using the\nautograd context.\n\nIt is fine for the 'backward' call to throw an exception in this test,\nbut the process should not crash.\n\"\"\"\n", "func_signal": "def test_clean_context_during_backward(self):\n", "code": "initialize_pg(self.file_init_method, self.rank, self.world_size)\n\ncontext = dist_autograd._new_context()\ncontext_id = context._context_id()\nDistAutogradTest._test_clean_context_backward_context_id = context_id\n\n# Send the context id to all nodes.\nfor i in range(0, self.world_size):\n    if i != self.rank:\n        rank_distance = (i - self.rank + self.world_size) % self.world_size\n        rpc.rpc_sync(\n            worker_name(i),\n            _set_rpc_done,\n            args=(context_id, rank_distance),\n        )\n\ndist.barrier()\n\n# Verify all context ids have been received.\nself.assertEqual(self.world_size - 1, len(known_context_ids))\n\nt1 = torch.rand((3, 3), requires_grad=True)\nfor i in range(0, 100):\n    dst = self._next_rank()\n    t1 = rpc.rpc_sync(worker_name(dst), torch.add, args=(t1, t1))\n\n# Call MyBackwardFunc as the first op of the backward pass to\n# ensure we release the context early in the backward pass.\nt1 = DistAutogradTest.MyBackwardFunc.apply(t1)\nself.assertEqual(100, len(context._send_functions()))\n\ncontext_id = 100  # dummy context_id\nwith self.assertRaisesRegex(\n    RuntimeError,\n    \"Could not find autograd context with id: {}\".format(context_id),\n):\n    dist_autograd.backward(context_id, [t1.sum()])\n\n# HACK: Killing workers since otherwise the autograd engine gets stuck on\n# other nodes. The proper fix would be addressing:\n# https://github.com/pytorch/pytorch/issues/27643, which would inform\n# other nodes about the failure.\n# The autograd engine gets stuck on other nodes since they're waiting to\n# receive gradients from the node that received an error (and as a\n# result it didn't execute the rest of the graph).\ndist.barrier()\nrpc.shutdown(graceful=False)\nsys.exit(0)", "path": "pytorch/torch/testing/_internal/distributed/rpc/dist_autograd_test.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "pytorch/pytorch", "stars": 76254, "license": "other", "language": "python", "size": 1041248}
{"docstring": "# Run equivalent of _nested_python_udf locally.\n", "func_signal": "def test_backwards_nested_python_udf(self):\n", "code": "t1 = torch.rand((3, 3), requires_grad=True)\nt2 = torch.rand((3, 3), requires_grad=True)\nt3 = t1 * t2\nt4 = t1 + t2\nres = t3 + t4\nloss = torch.chain_matmul(t1, t2, t3, t4, res).sum()\ntorch.autograd.backward([loss])\n\n# Now run distributed autograd.\nwith dist_autograd.context() as context_id:\n    loss = rpc.rpc_sync(\n        worker_name(self._next_rank()),\n        DistAutogradTest._nested_python_udf,\n        args=(t1, t2, self._next_rank()),\n    )\n    dist_autograd.backward(context_id, [loss.sum()])\n\n    grads = dist_autograd.get_gradients(context_id)\n    self.assertEqual(t1.grad, grads[t1])\n    self.assertEqual(t2.grad, grads[t2])", "path": "pytorch/torch/testing/_internal/distributed/rpc/dist_autograd_test.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "pytorch/pytorch", "stars": 76254, "license": "other", "language": "python", "size": 1041248}
{"docstring": "# Run the same code locally and with dist autograd and verify gradients\n# are same.\n", "func_signal": "def test_backward_simple_python_udf(self):\n", "code": "local_grads = None\nt1 = torch.rand((3, 3), requires_grad=True)\nt2 = torch.rand((3, 3), requires_grad=True)\nfor exec_mode in [ExecMode.LOCAL, ExecMode.REMOTE]:\n    with dist_autograd.context() as context_id:\n        ret = self._exec_func(exec_mode, my_py_add, t1, t2)\n        loss = ret.sum()\n        local_grads = self._verify_backwards(\n            exec_mode, [loss], context_id, local_grads, t1, t2\n        )", "path": "pytorch/torch/testing/_internal/distributed/rpc/dist_autograd_test.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "pytorch/pytorch", "stars": 76254, "license": "other", "language": "python", "size": 1041248}
{"docstring": "# Ref: https://nbviewer.jupyter.org/gist/fehiepsi/5ef8e09e61604f10607380467eb82006#Precision-to-scale_tril\n", "func_signal": "def _precision_to_scale_tril(P):\n", "code": "Lf = torch.cholesky(torch.flip(P, (-2, -1)))\nL_inv = torch.transpose(torch.flip(Lf, (-2, -1)), -2, -1)\nL = torch.triangular_solve(torch.eye(P.shape[-1], dtype=P.dtype, device=P.device),\n                           L_inv, upper=False)[0]\nreturn L", "path": "pytorch/torch/distributions/multivariate_normal.py", "commit_date": "2020-12-03 00:00:00", "repo_name": "pytorch/pytorch", "stars": 76254, "license": "other", "language": "python", "size": 1041248}
{"docstring": "# Verify next function is AddBackward0\n", "func_signal": "def _verify_graph_for_rpc_call_exec(self, send_function):\n", "code": "next_funcs = send_function.next_functions\nself.assertEqual(1, len(next_funcs))\nadd_backward_fn = next_funcs[0][0]\nself.assertEqual(\"AddBackward0\", add_backward_fn.name())\n\n# Verify the next two functions are the same recv backward function.\nnext_funcs = add_backward_fn.next_functions\nself.assertEqual(2, len(next_funcs))\nself.assertEqual(\n    \"torch::distributed::autograd::RecvRpcBackward\", next_funcs[0][0].name()\n)\nself.assertEqual(\n    \"torch::distributed::autograd::RecvRpcBackward\", next_funcs[1][0].name()\n)\nself.assertEqual(next_funcs[0][0], next_funcs[1][0])", "path": "pytorch/torch/testing/_internal/distributed/rpc/dist_autograd_test.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "pytorch/pytorch", "stars": 76254, "license": "other", "language": "python", "size": 1041248}
{"docstring": "# Set a short timeout to quickly time out failed RPCs.\n", "func_signal": "def test_backward_node_failure_python_udf(self):\n", "code": "rpc._set_rpc_timeout(5)  # 5 seconds\ninitialize_pg(self.file_init_method, self.rank, self.world_size)\n\nwith dist_autograd.context() as context_id:\n    t1 = torch.rand((3, 3), requires_grad=True)\n    t2 = torch.rand((3, 3), requires_grad=True)\n\n    dst = self._next_rank()\n    res = rpc.rpc_sync(\n        worker_name(dst),\n        my_py_nested_call,\n        args=(t1, t2, dst, self.world_size, 1),\n    )\n\n    dist.barrier()\n\n    # Kill rank 2 (last hop of nested rpc) and verify rank 0 receives an error.\n    if self.rank == 2:\n        return\n\n    store = dist.distributed_c10d._get_default_store()\n    if self.rank == 0:\n        # Wait for rank 2 to die.\n        shutdown_error_regex = self.get_shutdown_error_regex()\n        wait_until_node_failure(2, shutdown_error_regex)\n        # Shutdown sequence is not very well defined and as a result\n        # we might see any error given by get_shutdown_error_regex().\n        with self.assertRaisesRegex(RuntimeError, shutdown_error_regex):\n            # Run backwards, and validate we receive an error since rank 2 is dead.\n            dist_autograd.backward(context_id, [res.sum()])\n\n        # Mark rank 0 is done in the store, since the RPC framework on\n        # some nodes might be broken at this point (listenLoop() in\n        # ProcessGroupAgent might've exited).\n        store.set('test_backward_node_failure_python_udf_rank0_done', \"True\")\n    else:\n        # Wait for backward to finish on rank 0.\n        store.wait(['test_backward_node_failure_python_udf_rank0_done'], timedelta(seconds=10))", "path": "pytorch/torch/testing/_internal/distributed/rpc/dist_autograd_test.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "pytorch/pytorch", "stars": 76254, "license": "other", "language": "python", "size": 1041248}
{"docstring": "# local version\n", "func_signal": "def test_dist_optim_functional(self):\n", "code": "module1 = MyModule()\nmodule2 = MyModule()\nparams = [module1.get_w(), module2.get_w()]\nlocal_optim = optim.Adagrad(params, lr=0.05)\n\nold_w1 = module1.w.clone().detach()\nold_w2 = module2.w.clone().detach()\n\ng_cpu = torch.Generator()\ng_cpu.manual_seed(0)\nt1 = torch.rand((3, 3), requires_grad=True, generator=g_cpu)\nt2 = torch.rand((3, 3), requires_grad=True, generator=g_cpu)\noutput1 = module1.forward(t2)\noutput2 = module2.forward(output1)\nloss = torch.add(output2, t1).sum()\n\nloss.backward()\nlocal_optim.step()\n\n# distributed version\nowner1 = \"worker%d\" % ((self.rank + 1) % self.world_size)\nowner2 = \"worker%d\" % ((self.rank + 2) % self.world_size)\n\nremote_module1 = rpc.remote(owner1, MyModule)\nremote_module2 = rpc.remote(owner2, MyModule)\nremote_param1 = remote_method(MyModule.get_w, remote_module1)\nremote_param2 = remote_method(MyModule.get_w, remote_module2)\n\nold_w1_remote = remote_param1.to_here()\n\n# sanity check: local and remote initial weights should match\nself.assertEqual(old_w1, remote_param1.to_here())\nself.assertEqual(old_w2, remote_param2.to_here())\n\ndist_optim = DistributedOptimizer(\n    optim.Adagrad, [remote_param1, remote_param2], lr=0.05\n)\n\nwith dist_autograd.context() as context_id:\n    g_cpu.manual_seed(0)\n    t1 = torch.rand((3, 3), requires_grad=True, generator=g_cpu)\n    t2 = torch.rand((3, 3), requires_grad=True, generator=g_cpu)\n    output1 = rpc_async_method(MyModule.forward, remote_module1, t2)\n    output2 = rpc_async_method(MyModule.forward, remote_module2, output1.wait())\n    loss = torch.add(output2.wait(), t1)\n\n    dist_autograd.backward(context_id, [loss.sum()])\n    dist_optim.step(context_id)\n\n    new_w1 = rpc_async_method(MyModule.get_w, remote_module1).wait()\n    new_w2 = rpc_async_method(MyModule.get_w, remote_module2).wait()\n\n    # ensure optimizer changed weights\n    self.assertNotEqual(old_w1, new_w1)\n    self.assertNotEqual(old_w2, new_w2)\n    # ensure local equals remote\n    self.assertEqual(new_w1, module1.get_w())\n    self.assertEqual(new_w2, module2.get_w())", "path": "pytorch/torch/testing/_internal/distributed/rpc/dist_optimizer_test.py", "commit_date": "2020-09-26 00:00:00", "repo_name": "pytorch/pytorch", "stars": 76254, "license": "other", "language": "python", "size": 1041248}
{"docstring": "# create autograd function that saves grad pointer as class static\n", "func_signal": "def test_no_grad_copy_sparse(self):\n", "code": "class MyFunc(Function):\n    static_grad_ptr = None\n\n    @staticmethod\n    def forward(ctx, inp):\n        return inp\n\n    @staticmethod\n    def backward(ctx, grad):\n        MyFunc.static_grad_ptr = grad._values().data_ptr()\n        return grad\n\nclass NonContGradFunc(Function):\n    static_grad_ptr = None\n\n    @staticmethod\n    def forward(ctx, inp1, inp2):\n        return inp1 + inp2\n\n    @staticmethod\n    def backward(ctx, grad):\n        # Create a sparse tensor with non-contigous indices and values\n        # and return as grad.\n        v = torch.rand(1, 3)\n        i = torch.ones(1, 1, dtype=torch.long)\n        nv = v.expand(8, 3)\n        ni = i.expand(1, 8)\n        ngrad = torch.sparse.FloatTensor(ni, nv, torch.Size([10, 3]))\n        NonContGradFunc.static_grad_ptr = ngrad._values().data_ptr()\n        return ngrad, ngrad\n\na = torch.randn(10, 3, requires_grad=True)\nb = torch.randn(10, 3, requires_grad=True)\ninput = torch.tensor([1, 2, 4, 5, 4, 3, 2, 9])\noffsets = torch.tensor([0, 4])\nimport torch.nn.functional as F\n\n# test case that should trigger no copy for a.\nwith dist_autograd.context() as context_id:\n    emb_matrix = MyFunc.apply(a)\n    loss = F.embedding_bag(emb_matrix, input, offsets, sparse=True).sum()\n    dist_autograd.backward(context_id, [loss], retain_graph=True)\n    grads = dist_autograd.get_gradients(context_id)\n    p_g = MyFunc.static_grad_ptr\n    p_a = grads[a]._values().data_ptr()\n    # check a uses the same buffer\n    self.assertTrue(p_a == p_g)\n\n    # Run backwards multiple times.\n    for i in range(10):\n        dist_autograd.backward(context_id, [loss], retain_graph=True)\n\n# non-contiguous indices and value, we should trigger a copy.\nwith dist_autograd.context() as context_id:\n    emb_matrix = NonContGradFunc.apply(a, b)\n    loss = F.embedding_bag(emb_matrix, input, offsets, sparse=True).sum()\n    dist_autograd.backward(context_id, [loss], retain_graph=True)\n    grads = dist_autograd.get_gradients(context_id)\n    p_g = NonContGradFunc.static_grad_ptr\n    p_a = grads[a]._values().data_ptr()\n    p_b = grads[b]._values().data_ptr()\n    # check a,b uses different grad buffer\n    self.assertFalse(p_a == p_b)\n    # Verify we cloned both grads.\n    self.assertFalse(p_a == p_g)\n    self.assertFalse(p_b == p_g)\n\n    # Run backwards multiple times to verify accumulation.\n    for i in range(10):\n        dist_autograd.backward(context_id, [loss], retain_graph=True)", "path": "pytorch/torch/testing/_internal/distributed/rpc/dist_autograd_test.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "pytorch/pytorch", "stars": 76254, "license": "other", "language": "python", "size": 1041248}
{"docstring": "'''\nSimilar to test in test_autograd.py.\n'''\n# create autograd function that saves grad pointer as class static\n", "func_signal": "def test_no_grad_copy(self):\n", "code": "class MyFunc(Function):\n    static_grad_ptr = None\n\n    @staticmethod\n    def forward(ctx, inp1, inp2):\n        return inp1 + inp2\n\n    @staticmethod\n    def backward(ctx, grad):\n        MyFunc.static_grad_ptr = grad.data_ptr()\n        return grad, grad\n\nclass MyFuncSingleGrad(Function):\n    static_grad_ptr = None\n\n    @staticmethod\n    def forward(ctx, inp):\n        return inp\n\n    @staticmethod\n    def backward(ctx, grad):\n        MyFuncSingleGrad.static_grad_ptr = grad.data_ptr()\n        return grad\n\nclass NonContGradFunc(Function):\n    @staticmethod\n    def forward(ctx, inp1):\n        ctx.size = inp1.size()\n        return torch.tensor([1.])\n\n    @staticmethod\n    def backward(ctx, grad):\n        return torch.ones(1).expand(ctx.size)\n\na = torch.randn(5, 6, requires_grad=True)\nb = torch.randn(5, 6, requires_grad=True)\n# non-contiguous grad should be copied\nwith dist_autograd.context() as context_id:\n    dist_autograd.backward(context_id, [NonContGradFunc.apply(MyFunc.apply(a, b))])\n    grads = dist_autograd.get_gradients(context_id)\n    self.assertFalse(grads[a].data_ptr() == MyFunc.static_grad_ptr)\n    self.assertFalse(grads[b].data_ptr() == MyFunc.static_grad_ptr)\n\n# test case that should trigger no copy for a\nwith dist_autograd.context() as context_id:\n    dist_autograd.backward(context_id, [MyFuncSingleGrad.apply(a)[1][0]])\n    grads = dist_autograd.get_gradients(context_id)\n    p_g = MyFuncSingleGrad.static_grad_ptr\n    p_a = grads[a].data_ptr()\n    # Verify there was no clone.\n    self.assertTrue(p_a == p_g)\n\n# Test case that should trigger copy for both of a,b. This is\n# different in the distributed autograd case since we hold\n# a reference to all grads in a vector until all accumulation is done.\nwith dist_autograd.context() as context_id:\n    dist_autograd.backward(context_id, [MyFunc.apply(a, b)[1][0]])\n    grads = dist_autograd.get_gradients(context_id)\n    p_g = MyFunc.static_grad_ptr\n    p_a = grads[a].data_ptr()\n    p_b = grads[b].data_ptr()\n    # check a,b uses different grad buffer\n    self.assertFalse(p_a == p_b)\n    # both should be copied.\n    self.assertFalse(grads[a].data_ptr() == MyFunc.static_grad_ptr)\n    self.assertFalse(grads[b].data_ptr() == MyFunc.static_grad_ptr)", "path": "pytorch/torch/testing/_internal/distributed/rpc/dist_autograd_test.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "pytorch/pytorch", "stars": 76254, "license": "other", "language": "python", "size": 1041248}
{"docstring": "# distributed version\n", "func_signal": "def test_dist_optim_exception_on_constructor(self):\n", "code": "owner1 = \"worker%d\" % ((self.rank + 1) % self.world_size)\nowner2 = \"worker%d\" % ((self.rank + 2) % self.world_size)\n\nremote_module1 = rpc.remote(owner1, MyModule)\nremote_module2 = rpc.remote(owner2, MyModule)\nremote_param1 = remote_method(MyModule.get_w, remote_module1)\nremote_param2 = remote_method(MyModule.get_w, remote_module2)\n\nwith self.assertRaisesRegex(Exception, \"Error creating optimizer.\"):\n    dist_optim = DistributedOptimizer(\n        OptimizerFailingOnConstructor, [remote_param1, remote_param2]\n    )", "path": "pytorch/torch/testing/_internal/distributed/rpc/dist_optimizer_test.py", "commit_date": "2020-09-26 00:00:00", "repo_name": "pytorch/pytorch", "stars": 76254, "license": "other", "language": "python", "size": 1041248}
{"docstring": "# Verify max possible id.\n", "func_signal": "def test_autograd_context(self):\n", "code": "max_auto_increment = 281474976710655\nself.assertEqual(\n    max_auto_increment + (self.worker_id << 48), dist_autograd._get_max_id()\n)\n\ncontext_ids = []\nfor i in range(200):\n    with dist_autograd.context() as context_id:\n        self.assertEqual(\n            context_id,\n            dist_autograd._retrieve_context(context_id)._context_id(),\n        )\n        # First 16 bits should be worker_id.\n        self.assertEqual(self.worker_id, context_id >> 48)\n        context_ids.append(context_id)\n\nfor context_id in context_ids:\n    with self.assertRaisesRegex(\n        RuntimeError,\n        \"Could not find autograd context with id: {}\".format(context_id),\n    ):\n        dist_autograd._retrieve_context(context_id)", "path": "pytorch/torch/testing/_internal/distributed/rpc/dist_autograd_test.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "pytorch/pytorch", "stars": 76254, "license": "other", "language": "python", "size": 1041248}
{"docstring": "# create autograd function that saves grad pointer as class static\n", "func_signal": "def test_grad_copy_sparse_indices_extra_ref(self):\n", "code": "class MyFunc(Function):\n    static_grad_ptr = None\n    static_grad_indices_ref = None\n    static_grad_values_ref = None\n\n    @staticmethod\n    def forward(ctx, inp):\n        return inp\n\n    @staticmethod\n    def backward(ctx, grad):\n        MyFunc.static_grad_ptr = grad._values().data_ptr()\n        # indices() and values() return views, so holding onto\n        # references of them would not increment refcount of indices\n        # and values inside the sparse tensor.\n        MyFunc.static_grad_indices_ref = grad._indices()\n        MyFunc.static_grad_values_ref = grad._values()\n        return grad\n\na = torch.randn(10, 3, requires_grad=True)\ninput = torch.tensor([1, 2, 4, 5, 4, 3, 2, 9])\noffsets = torch.tensor([0, 4])\nimport torch.nn.functional as F\n\nwith dist_autograd.context() as context_id:\n    emb_matrix = MyFunc.apply(a)\n    loss = F.embedding_bag(emb_matrix, input, offsets, sparse=True).sum()\n    dist_autograd.backward(context_id, [loss], retain_graph=True)\n    grads = dist_autograd.get_gradients(context_id)\n    p_g = MyFunc.static_grad_ptr\n    p_a = grads[a]._values().data_ptr()\n    self.assertIsNotNone(MyFunc.static_grad_indices_ref)\n    self.assertIsNotNone(MyFunc.static_grad_values_ref)\n    # grad would be stolen, since static_grad_indices_ref and\n    # static_grad_values_ref are holding onto views and don't bump the\n    # refcount.\n    self.assertTrue(p_g == p_a)", "path": "pytorch/torch/testing/_internal/distributed/rpc/dist_autograd_test.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "pytorch/pytorch", "stars": 76254, "license": "other", "language": "python", "size": 1041248}
{"docstring": "# Run the same code locally and with dist autograd and verify gradients\n# are same.\n", "func_signal": "def test_backward_simple_script_call(self):\n", "code": "local_grads = None\nt1 = torch.rand((3, 3), requires_grad=True)\nt2 = torch.rand((3, 3), requires_grad=True)\nfor exec_mode in [\n    ExecMode.LOCAL,\n    ExecMode.RPC_SYNC,\n    ExecMode.RPC_ASYNC,\n    ExecMode.REMOTE,\n]:\n    with dist_autograd.context() as context_id:\n        forward_ret = self._exec_func(exec_mode, my_script_add, t1, t2)\n        loss = forward_ret.sum()\n        ret = self._verify_backwards(\n            exec_mode, [loss], context_id, local_grads, t1, t2\n        )\n        local_grads = ret if ret else local_grads", "path": "pytorch/torch/testing/_internal/distributed/rpc/dist_autograd_test.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "pytorch/pytorch", "stars": 76254, "license": "other", "language": "python", "size": 1041248}
{"docstring": "# Run the same code locally and with dist autograd and verify gradients\n# are same.\n", "func_signal": "def _test_backward_simple(self, dst):\n", "code": "local_grads = None\nt1 = torch.rand((3, 3), requires_grad=True)\nt2 = torch.rand((3, 3), requires_grad=True)\nfor exec_mode in [ExecMode.LOCAL, ExecMode.RPC_SYNC, ExecMode.REMOTE]:\n    with dist_autograd.context() as context_id:\n        ret = self._exec_func_with_dst(\n            dst, exec_mode, torch.add, t1, t2\n        )\n        loss = ret.sum()\n        ret = self._verify_backwards(\n            exec_mode, [loss], context_id, local_grads, t1, t2\n        )\n        local_grads = ret if ret else local_grads", "path": "pytorch/torch/testing/_internal/distributed/rpc/dist_autograd_test.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "pytorch/pytorch", "stars": 76254, "license": "other", "language": "python", "size": 1041248}
{"docstring": "\"\"\"\nInitialization of the ApDisplyInfo object\n:param self: A TuiApSel object\n:type self: TuiApSel\n:param screen: A curses window object\n:type screen: _curses.curses.window\n:param info: A namedtuple of information from pywifiphisher\n:type info: namedtuple\n:return ApDisplayInfo object\n:rtype: ApDisplayInfo\n\"\"\"\n", "func_signal": "def init_display_info(self, screen, info):\n", "code": "position = 1\npage_number = 1\n\n# get window height, length and create a box inside\nmax_window_height, max_window_length = screen.getmaxyx()\nif max_window_height < 14 or max_window_length < 9:\n    box = curses.newwin(max_window_height, max_window_length, 0, 0)\n    self.renew_box = True\nelse:\n    box = curses.newwin(max_window_height - 9, max_window_length - 5,\n                        4, 3)\nbox.box()\n\n# calculate the box's maximum number of row's\nbox_height = box.getmaxyx()[0]\n# subtracting 2 from the height for the border\nmax_row = box_height - 2\nkey = 0\nbox_info = [max_window_height, max_window_length, max_row, key]\n\nap_info = ApDisplayInfo(position, page_number, box, box_info)\n\nself.mac_matcher = info.mac_matcher\n# start finding access points\nself.access_point_finder = recon.AccessPointFinder(\n    info.interface, info.network_manager)\nif info.args.lure10_capture:\n    self.access_point_finder.capture_aps()\nself.access_point_finder.find_all_access_points()\n\nreturn ap_info", "path": "wifiphisher/wifiphisher/common/tui.py", "commit_date": "2019-12-10 00:00:00", "repo_name": "wifiphisher/wifiphisher", "stars": 12591, "license": "gpl-3.0", "language": "python", "size": 12889}
{"docstring": "\"\"\"\nConstruct the class\n:param self: A TuiTemplateSelection object\n:type self: TuiTemplateSelection\n:return None\n:rtype None\n\"\"\"\n\n", "func_signal": "def __init__(self):\n", "code": "self.green_text = None\n# heightlight the phishing scenario\nself.heightlight_text = None\n# record current hightlight template number\nself.heightlight_number = 0\n# store the current page number\nself.page_number = 0\n# store the phishing contents of each scenario\nself.sections = list()\n# map the section to page number\nself.sec_page_map = {}\n# the window size for (y, x)\nself.dimension = [0, 0]", "path": "wifiphisher/wifiphisher/common/tui.py", "commit_date": "2019-12-10 00:00:00", "repo_name": "wifiphisher/wifiphisher", "stars": 12591, "license": "gpl-3.0", "language": "python", "size": 12889}
{"docstring": "\"\"\"\nDisplay the template information to users\n:param self: A TuiTemplateSelection object\n:type self: TuiTemplateSelection\n:param screen: A curses window object\n:type screen: _curses.curses.window\n:param templates: A dictionay map page to PhishingTemplate\n:type templates: dict\n:param template_names: list of template names\n:type template_names: list\n\"\"\"\n\n# setup curses\n", "func_signal": "def display_info(self, screen, templates, template_names):\n", "code": "try:\n    curses.curs_set(0)\nexcept curses.error:\n    pass\nscreen.nodelay(True)\ncurses.init_pair(1, curses.COLOR_GREEN, screen.getbkgd())\n# heightlight the phishing scenarios\ncurses.init_pair(2, curses.COLOR_BLACK, curses.COLOR_CYAN)\n\nself.green_text = curses.color_pair(1) | curses.A_BOLD\nself.heightlight_text = curses.color_pair(2) | curses.A_BOLD\n\n# setup number of templates\nnumber_of_sections = len(templates)\n\n# how many chars for user keying the template number\nscreen.erase()\nwhile True:\n    # display the four default phishing scenarios\n    # catch the exception when screen size is smaller than\n    # the text length\n    row_number = self.display_phishing_scenarios(screen)\n\n    # update the heightlight_number\n    key = screen.getch()\n    self.key_movement(screen, number_of_sections, key)\n    # add two blank lines\n    row_number += 2\n    # display the words of chosen template\n    if key == ord(\"\\n\"):\n        try:\n            screen.addstr(row_number, 3, \"YOU HAVE SELECTED \" +\n                          template_names[self.heightlight_number],\n                          curses.A_BOLD)\n        except curses.error:\n            pass\n        screen.refresh()\n        time.sleep(1)\n        template_name = template_names[self.heightlight_number]\n        template = templates[template_name]\n        return template\n    screen.refresh()", "path": "wifiphisher/wifiphisher/common/tui.py", "commit_date": "2019-12-10 00:00:00", "repo_name": "wifiphisher/wifiphisher", "stars": 12591, "license": "gpl-3.0", "language": "python", "size": 12889}
{"docstring": "\"\"\"\nResize the window if the dimensions have been changed\n\n:param self: A TuiApSel object\n:type self: TuiApSel\n:param screen: A curses window object\n:type screen: _curses.curses.window\n:param ap_info: An ApDisplayInfo object\n:type ap_info: ApDisplayInfo\n\"\"\"\n\n", "func_signal": "def resize_window(self, screen, ap_info):\n", "code": "if screen.getmaxyx() != (ap_info.max_h, ap_info.max_l):\n    ap_info.max_h, ap_info.max_l = screen.getmaxyx()\n    # sanity check for the box size (the height needed is 10 and\n    # the width needed is 6. Just create a box which is same as the\n    # base screen\n    if ap_info.max_h < 10 + 4 or ap_info.max_l < 6 + 3:\n        box = curses.newwin(ap_info.max_h, ap_info.max_l, 0, 0)\n        box.box()\n        ap_info.box = box\n        self.renew_box = True\n        return\n    elif self.renew_box:\n        screen.erase()\n        box = curses.newwin(ap_info.max_h - 9, ap_info.max_l - 5, 4, 3)\n        box.box()\n        ap_info.box = box\n        self.renew_box = False\n\n    ap_info.box.resize(ap_info.max_h - 9, ap_info.max_l - 5)\n    # calculate the box's maximum number of row's\n    box_height = ap_info.box.getmaxyx()[0]\n    # subtracting 2 from the height for the border\n    ap_info.max_row = box_height - 2\n    # reset the page and position to avoid problems\n    ap_info.pos = 1\n    ap_info.page_number = 1", "path": "wifiphisher/wifiphisher/common/tui.py", "commit_date": "2019-12-10 00:00:00", "repo_name": "wifiphisher/wifiphisher", "stars": 12591, "license": "gpl-3.0", "language": "python", "size": 12889}
{"docstring": "\"\"\"\nGet the information from pywifiphisher and print them out\n:param self: A TuiApSel object\n:type self: TuiApSel\n:param screen: A curses window object\n:type screen: _curses.curses.window\n:param info: A namedtuple of information from pywifiphisher\n:type info: namedtuple\n:return AccessPoint object if users type enter\n:rtype AccessPoint if users type enter else None\n\"\"\"\n# setup curses\n# make cursor invisible\n", "func_signal": "def gather_info(self, screen, info):\n", "code": "try:\n    curses.curs_set(0)\nexcept curses.error:\n    pass\n# don't wait for user input\nscreen.nodelay(True)\n# setup the font color\ncurses.init_pair(1, curses.COLOR_BLACK, curses.COLOR_CYAN)\nself.highlight_text = curses.color_pair(1)\nself.normal_text = curses.A_NORMAL\n\n# information regarding access points\nap_info = self.init_display_info(screen, info)\n\n# show information until user presses Esc key\nwhile ap_info.key != 27:\n    # display info will modifiy the key value\n    is_done = self.display_info(screen, ap_info)\n\n    if is_done:\n        # turn off access point discovery and return the result\n        self.access_point_finder.stop_finding_access_points()\n        return self.access_points[ap_info.pos - 1]\n\n# turn off access point discovery\nself.access_point_finder.stop_finding_access_points()", "path": "wifiphisher/wifiphisher/common/tui.py", "commit_date": "2019-12-10 00:00:00", "repo_name": "wifiphisher/wifiphisher", "stars": 12591, "license": "gpl-3.0", "language": "python", "size": 12889}
{"docstring": "\"\"\"\nGet the information from pywifiphisher and print them out\n:param self: A TuiMain object\n:param screen: A curses window object\n:param info: A namedtuple of printing information\n:type self: TuiMain\n:type screen: _curses.curses.window\n:type info: namedtuple\n:return: None\n:rtype: None\n\"\"\"\n\n# setup curses\n", "func_signal": "def gather_info(self, screen, info):\n", "code": "try:\n    curses.curs_set(0)\nexcept curses.error:\n    pass\nscreen.nodelay(True)\ncurses.init_pair(1, curses.COLOR_BLUE, screen.getbkgd())\ncurses.init_pair(2, curses.COLOR_YELLOW, screen.getbkgd())\ncurses.init_pair(3, curses.COLOR_RED, screen.getbkgd())\nself.blue_text = curses.color_pair(1) | curses.A_BOLD\nself.yellow_text = curses.color_pair(2) | curses.A_BOLD\nself.red_text = curses.color_pair(3) | curses.A_BOLD\n\nwhile True:\n    # catch the exception when screen size is smaller than\n    # the text length\n    is_done = self.display_info(screen, info)\n    if is_done:\n        return", "path": "wifiphisher/wifiphisher/common/tui.py", "commit_date": "2019-12-10 00:00:00", "repo_name": "wifiphisher/wifiphisher", "stars": 12591, "license": "gpl-3.0", "language": "python", "size": 12889}
{"docstring": "\"\"\"\nGet all the phishing scenario contents and store them\nin a list\n:param self: A TuiTemplateSelection object\n:param template_names: A list of string\n:param templates: A dictionary\n:type self: TuiTemplateSelection\n:type template_names: list\n:type templates: dict\n:return None\n:rtype: None\n\"\"\"\n\n", "func_signal": "def get_sections(self, template_names, templates):\n", "code": "for name in template_names:\n    phishing_contents = \" - \" + str(templates[name])\n    # total line in the phishing contents\n    lines = phishing_contents.splitlines()\n    # split the line into 15 words per shorter line\n    short_lines = []\n    for line in lines:\n        for short_line in line_splitter(15, line):\n            short_lines.append(short_line)\n    self.sections.append(short_lines)", "path": "wifiphisher/wifiphisher/common/tui.py", "commit_date": "2019-12-10 00:00:00", "repo_name": "wifiphisher/wifiphisher", "stars": 12591, "license": "gpl-3.0", "language": "python", "size": 12889}
{"docstring": "\"\"\"\nSelect a template based on whether the template argument\nis set or not. If the template argument is not set, it will\ninterfactively ask user for a template\n:param self: A TuiTemplateSelection object\n:type self: TuiTemplateSelection\n:param template_argument: The template argument which might\nhave been entered by the user\n:type template_argument: str\n:param template_manager: A TemplateManager object\n:type template_manager: TemplateManager\n:return A PhishingTemplate object\n:rtype: PhishingTemplagte\n:raises  InvalidTemplate in case the template argument entered\nby the user is not available.\n\"\"\"\n# get all available templates\n", "func_signal": "def gather_info(self, template_argument, template_manager):\n", "code": "templates = template_manager.get_templates()\n\n# get all the templates names for display\ntemplate_names = list(templates.keys())\n\n# get all the section contents\nself.get_sections(template_names, templates)\n\n# check if the template argument is set and is correct\nif template_argument and template_argument in templates:\n    # return the template name\n    return templates[template_argument]\nelif template_argument and template_argument not in templates:\n    # in case of an invalid template\n    raise phishingpage.InvalidTemplate\nelse:\n    # prompt interactive phishing scenarios to let user select one\n    template = curses.wrapper(self.display_info, templates,\n                              template_names)\nreturn template", "path": "wifiphisher/wifiphisher/common/tui.py", "commit_date": "2019-12-10 00:00:00", "repo_name": "wifiphisher/wifiphisher", "stars": 12591, "license": "gpl-3.0", "language": "python", "size": 12889}
{"docstring": "\"\"\"\nCheck for key movement and hightlight the corresponding\nphishing scenario\n\n:param self: A TuiTemplateSelection object\n:param number_of_sections: Number of templates\n:param key: The char user keying\n:type self: TuiTemplateSelection\n:type number_of_sections: int\n:type key: str\n:return: None\n:rtype: None\n\"\"\"\n\n", "func_signal": "def key_movement(self, screen, number_of_sections, key):\n", "code": "if key == curses.KEY_DOWN:\n    if self.heightlight_number < number_of_sections - 1:\n        page_number = self.sec_page_map[self.heightlight_number + 1]\n        if page_number > self.page_number:\n            self.page_number += 1\n            screen.erase()\n        self.heightlight_number += 1\nelif key == curses.KEY_UP:\n    if self.heightlight_number > 0:\n        page_number = self.sec_page_map[self.heightlight_number - 1]\n        if page_number < self.page_number:\n            self.page_number -= 1\n            screen.erase()\n        self.heightlight_number -= 1", "path": "wifiphisher/wifiphisher/common/tui.py", "commit_date": "2019-12-10 00:00:00", "repo_name": "wifiphisher/wifiphisher", "stars": 12591, "license": "gpl-3.0", "language": "python", "size": 12889}
{"docstring": "\"\"\"\nPrint the http request on the main terminal\n:param self: A TuiMain object\n:type self: TuiMain\n:param start_row_num: start line to print the http request\ntype start_row_num: int\n:param http_output: string of the http requests\n:type http_output: str\n\"\"\"\n\n", "func_signal": "def print_http_requests(self, screen, start_row_num, http_output):\n", "code": "requests = http_output.splitlines()\nmatch_str = r\"(.*\\s)(request from\\s)(.*)(\\sfor|with\\s)(.*)\"\nfor request in requests:\n    # match the information from the input string\n    match = re.match(match_str, request.decode('utf-8'))\n    if match is None:\n        continue\n\n    # POST or GET\n    request_type = match.group(1)\n    # requst from\n    request_from = match.group(2)\n    # ip address or http address\n    ip_address = match.group(3)\n    # for or with\n    for_or_with = match.group(4)\n    resource = match.group(5)\n\n    start_col = 0\n    screen.addstr(start_row_num, start_col, '[')\n    start_col += 1\n    screen.addstr(start_row_num, start_col, '*', self.yellow_text)\n    start_col += 1\n    screen.addstr(start_row_num, start_col, '] ')\n    start_col += 2\n\n    # concatenate GET or POST\n    screen.addstr(start_row_num, start_col, request_type,\n                  self.yellow_text)\n    start_col += len(request_type)\n\n    # concatenate the word 'request from'\n    screen.addstr(start_row_num, start_col, request_from)\n    start_col += len(request_from)\n\n    # concatenate the ip address\n    screen.addstr(start_row_num, start_col, ip_address,\n                  self.yellow_text)\n    start_col += len(ip_address)\n\n    # concatenate with or for\n    screen.addstr(start_row_num, start_col, for_or_with)\n    start_col += len(for_or_with)\n\n    # resource url\n    screen.addstr(start_row_num, start_col, resource, self.yellow_text)\n\n    start_row_num += 1", "path": "wifiphisher/wifiphisher/common/tui.py", "commit_date": "2019-12-10 00:00:00", "repo_name": "wifiphisher/wifiphisher", "stars": 12591, "license": "gpl-3.0", "language": "python", "size": 12889}
{"docstring": "\"\"\"\nPrint the information of Victims on the terminal\n:param self: A TuiMain object\n:param screen: A curses window object\n:param info: A nameduple of printing information\n:type self: TuiMain\n:type screen: _curses.curses.window\n:type info: namedtuple\n:return True if users have pressed the Esc key\n:rtype: bool\n\"\"\"\n\n# Get accesspoint instance and read victims from file\n", "func_signal": "def display_info(self, screen, info):\n", "code": "accesspoint_instance = accesspoint.AccessPoint.get_instance()\naccesspoint_instance.read_connected_victims_file()\n\nis_done = False\nscreen.erase()\n\n_, max_window_length = screen.getmaxyx()\ntry:\n    # print the basic info on the right top corner\n    screen.addstr(0, max_window_length - 30, \"|\")\n    screen.addstr(1, max_window_length - 30, \"|\")\n    # continue from the \"Wifiphisher\"\n    screen.addstr(1, max_window_length - 29,\n                  \" Wifiphisher \" + info.version, self.blue_text)\n\n    screen.addstr(2, max_window_length - 30,\n                  \"|\" + \" ESSID: \" + info.essid)\n    screen.addstr(3, max_window_length - 30,\n                  \"|\" + \" Channel: \" + info.channel)\n    screen.addstr(4, max_window_length - 30,\n                  \"|\" + \" AP interface: \" + info.ap_iface)\n    screen.addstr(5, max_window_length - 30,\n                  \"|\" + \" Options: [Esc] Quit\")\n    screen.addstr(6, max_window_length - 30, \"|\" + \"_\" * 29)\n\n    # Print the extensions section\n    screen.addstr(1, 0, \"Extensions feed: \", self.blue_text)\nexcept curses.error:\n    pass\n\nif info.em:\n    # start raw number from 2\n    raw_num = 2\n    for client in info.em.get_output()[-5:]:\n        screen.addstr(raw_num, 0, client)\n        raw_num += 1\ntry:\n    # Print the connected victims section\n    screen.addstr(7, 0, \"Connected Victims: \", self.blue_text)\n    victims_instance = victim.Victims.get_instance()\n    vict_dic = victims_instance.get_print_representation()\n    row_counter = 8\n    for key in vict_dic:\n        screen.addstr(row_counter, 0, key, self.red_text)\n        screen.addstr(row_counter, 22, vict_dic[key])\n        row_counter += 1\n    # Print the http request section\n    screen.addstr(13, 0, \"HTTP requests: \", self.blue_text)\n    if os.path.isfile('/tmp/wifiphisher-webserver.tmp'):\n        http_output = check_output(\n            ['tail', '-5', '/tmp/wifiphisher-webserver.tmp'])\n        self.print_http_requests(screen, 14, http_output)\nexcept curses.error:\n    pass\n\n# detect if users have pressed the Esc Key\nif screen.getch() == 27:\n    is_done = True\n\nif info.phishinghttp.terminate and info.args.quitonsuccess:\n    is_done = True\n\nscreen.refresh()\nreturn is_done", "path": "wifiphisher/wifiphisher/common/tui.py", "commit_date": "2019-12-10 00:00:00", "repo_name": "wifiphisher/wifiphisher", "stars": 12591, "license": "gpl-3.0", "language": "python", "size": 12889}
{"docstring": "\"\"\"\nCheck for any key movement and update it's result\n\n:param self: A TuiApSel object\n:type self: TuiApSel\n:param ap_info: ApDisplayInfo object\n:type: ApDisplayInfo\n:return: None\n:rtype: None\n\"\"\"\n\n", "func_signal": "def key_movement(self, ap_info):\n", "code": "key = ap_info.key\npos = ap_info.pos\nmax_row = ap_info.max_row\npage_number = ap_info.page_number\n\n# in case arrow down key has been pressed\nif key == curses.KEY_DOWN:\n    # if next item exists move down, otherwise don't move\n    try:\n        self.access_points[pos]\n    except IndexError:\n        ap_info.key = 0\n        ap_info.pos = pos\n        ap_info.max_row = max_row\n        return\n\n    # if next item is in the next page change page and move\n    # down otherwise just move down)\n    if pos % max_row == 0:\n        pos += 1\n        page_number += 1\n    else:\n        pos += 1\n\n# in case arrow up key has been pressed\nelif key == curses.KEY_UP:\n    # if not the first item\n    if (pos - 1) > 0:\n        # if previous item is in previous page_number, change page\n        # and move up otherwise just move up\n        if (pos - 1) % max_row == 0:\n            pos -= 1\n            page_number -= 1\n        else:\n            pos -= 1\n# update key, position, and page_number\nap_info.key = key\nap_info.pos = pos\nap_info.page_number = page_number", "path": "wifiphisher/wifiphisher/common/tui.py", "commit_date": "2019-12-10 00:00:00", "repo_name": "wifiphisher/wifiphisher", "stars": 12591, "license": "gpl-3.0", "language": "python", "size": 12889}
{"docstring": "\"\"\"\nSplit line to the shorter lines\n:param num_of_words: split the line into the line with lenth equeal\nto num_of_words\n:type num_of_words: int\n:param line: A sentence\n:type line: str\n:return: tuple of shorter lines\n:rtype: tuple\n\"\"\"\n", "func_signal": "def line_splitter(num_of_words, line):\n", "code": "pieces = line.split()\nreturn (\" \".join(pieces[i:i + num_of_words])\n        for i in range(0, len(pieces), num_of_words))", "path": "wifiphisher/wifiphisher/common/tui.py", "commit_date": "2019-12-10 00:00:00", "repo_name": "wifiphisher/wifiphisher", "stars": 12591, "license": "gpl-3.0", "language": "python", "size": 12889}
{"docstring": "\"\"\"\nConstruct the class\n:param self: A TuiMain object\n:type self: TuiMain\n:return: None\n:rtype: None\n\"\"\"\n\n", "func_signal": "def __init__(self):\n", "code": "self.blue_text = None\nself.orange_text = None\nself.yellow_text = None", "path": "wifiphisher/wifiphisher/common/tui.py", "commit_date": "2019-12-10 00:00:00", "repo_name": "wifiphisher/wifiphisher", "stars": 12591, "license": "gpl-3.0", "language": "python", "size": 12889}
{"docstring": "\"\"\"\nConstruct the class\n:param self: ApDisplayInfo\n:param pos: position of the line in the ap selection page\n:param page_number: page number of the ap selection\n:param box: the curses.newwin.box object containing ap information\n:param key: the key user have keyed in\n:param box_info: list of window height, window len, and max row number\n:type self: ApDisplayInfo\n:type pos: int\n:type page_number: int\n:type box: curse.newwin.box\n:type key: str\n:return: None\n:rtype: None\n\"\"\"\n\n", "func_signal": "def __init__(self, pos, page_number, box, box_info):\n", "code": "self.pos = pos\nself.page_number = page_number\nself.box = box\n# list of (max_win_height, max_win_len, max_row, key)\nself._box_info = box_info", "path": "wifiphisher/wifiphisher/common/tui.py", "commit_date": "2019-12-10 00:00:00", "repo_name": "wifiphisher/wifiphisher", "stars": 12591, "license": "gpl-3.0", "language": "python", "size": 12889}
{"docstring": "\"\"\"\nDisplay information in the box window\n\n:param self: A TuiApSel object\n:type self: TuiApSel\n:param screen: A curses window object\n:type screen: _curses.curses.window\n:param ap_info: An ApDisplayInfo object\n:type ap_info: ApDisplayInfo\n:return: None\n:rtype: None\n.. note: The display system is setup like the following:\n\n         ----------------------------------------\n         - (1,3)Options                         -\n         -   (3,5)Header                        -\n         - (4,3)****************************    -\n         -      *       ^                  *    -\n         -      *       |                  *    -\n         -      *       |                  *    -\n         -    < *       |----              *    -\n         -    v *       |   v              *    -\n         -    v *       |   v              *    -\n         -    v *       |   v              *    -\n         -    v *       v   v              *    -\n         -    v ************v***************    -\n         -    v             v      v            -\n         -----v-------------v------v-------------\n              v             v      v\n              v             v      > max_window_length-5\n              v             v\n        max_window_height-9 v\n                            V\n                            v--> box_height-2\n\n\"\"\"\n\n# get the page boundary\n", "func_signal": "def display_access_points(self, screen, ap_info):\n", "code": "page_boundary = list(range(1 + (ap_info.max_row *\n                           (ap_info.page_number - 1)),\n                      ap_info.max_row + 1 +\n                      (ap_info.max_row * (ap_info.page_number - 1))))\n\n# remove previous content and draw border\nap_info.box.erase()\nap_info.box.border(0)\n\n# show the header\nheader_fmt = \"{0:30} {1:16} {2:3} {3:4} {4:9} {5:5} {6:20}\"\nheader = header_fmt.format(\"ESSID\", \"BSSID\", \"CH\", \"PWR\", \"ENCR\",\n                           \"CLIENTS\", \"VENDOR\")\nopt_str = (\"Options:  [Esc] Quit  [Up Arrow] Move Up  \"\n           \"[Down Arrow] Move Down\")\n\ntry:\n    window_l = screen.getmaxyx()[1]\n    screen.addstr(1, 3, display_string(window_l - 3, opt_str))\n    screen.addstr(3, 5, display_string(window_l - 5, header))\nexcept curses.error:\n    return\n\n# show all the items based on their position\nfor item_position in page_boundary:\n    # in case of no access points discovered yet\n    if self.total_ap_number == 0:\n        display_str = \"No access point has been discovered yet!\"\n        try:\n            ap_info.box.addstr(1, 1,\n                               display_string(ap_info.max_l - 1,\n                                              display_str),\n                               self.highlight_text)\n        except curses.error:\n            return\n    # in case of at least one access point\n    else:\n        # get the access point and it's vendor\n        access_point = self.access_points[item_position - 1]\n        vendor = self.mac_matcher.get_vendor_name(\n            access_point.mac_address)\n\n        # the display format for showing access points\n        display_text = ((\n            \"{0:30} {1:17} {2:2} {3:3}% {4:^8} {5:^5}\"\n            \" {6:20}\").format(\n                access_point.name, access_point.mac_address,\n                access_point.channel, access_point.signal_strength,\n                access_point.encryption,\n                access_point.client_count, vendor))\n        # shows whether the access point should be highlighted or not\n        # based on our current position\n        print_row_number = item_position - ap_info.max_row * (\n            ap_info.page_number - 1)\n\n        # bypass the addstr exception\n        try:\n\n            if item_position == ap_info.pos:\n                ap_info.box.addstr(print_row_number, 2,\n                                   display_string(\n                                       ap_info.max_l - 2,\n                                       display_text),\n                                   self.highlight_text)\n            else:\n                ap_info.box.addstr(print_row_number, 2,\n                                   display_string(\n                                       ap_info.max_l - 2,\n                                       display_text), self.normal_text)\n        except curses.error:\n            return\n\n        # stop if it is the last item in page\n        if item_position == self.total_ap_number:\n            break\n\n# update the screen\nscreen.refresh()\nap_info.box.refresh()", "path": "wifiphisher/wifiphisher/common/tui.py", "commit_date": "2019-12-10 00:00:00", "repo_name": "wifiphisher/wifiphisher", "stars": 12591, "license": "gpl-3.0", "language": "python", "size": 12889}
{"docstring": "\"\"\"\nConstruct the class\n:param self: A TuiApSel object\n:type self: TuiApSel\n:return: None\n:rtype: None\n\"\"\"\n\n", "func_signal": "def __init__(self):\n", "code": "self.total_ap_number = 0\nself.access_points = list()\nself.access_point_finder = None\nself.highlight_text = None\nself.normal_text = None\nself.mac_matcher = None\n# when screen becomes too small we'll create a box with\n# the size equal to the screen terminal. We need to renew\n# the box when the screen terminal expands again.\nself.renew_box = False", "path": "wifiphisher/wifiphisher/common/tui.py", "commit_date": "2019-12-10 00:00:00", "repo_name": "wifiphisher/wifiphisher", "stars": 12591, "license": "gpl-3.0", "language": "python", "size": 12889}
{"docstring": "\"\"\"\nUpdate the page number for each section\n:param self: A TuiTemplateSelection object\n:param last_row: The last row of the window\n:type self: TuiTemplateSelection\n:type last_row: int\n:return: None\n:rtype: None\n\"\"\"\n\n", "func_signal": "def update_sec_page_map(self, last_row):\n", "code": "page_number = 0\nrow_number = 0\nself.sec_page_map = {}\nfor number, section in enumerate(self.sections):\n    row_number += len(section)\n    if row_number > last_row:\n        row_number = 0\n        page_number += 1\n    self.sec_page_map[number] = page_number", "path": "wifiphisher/wifiphisher/common/tui.py", "commit_date": "2019-12-10 00:00:00", "repo_name": "wifiphisher/wifiphisher", "stars": 12591, "license": "gpl-3.0", "language": "python", "size": 12889}
{"docstring": "\"\"\"\nDisplay the phishing scenarios\n:param self: A TuiTemplateSelection object\n:type self: TuiTemplateSelection\n:param screen: A curses window object\n:type screen: _curses.curses.window\n:return total row numbers used to display the phishing scenarios\n:rtype: int\n\"\"\"\n\n", "func_signal": "def display_phishing_scenarios(self, screen):\n", "code": "try:\n    max_window_height, max_window_len = screen.getmaxyx()\n    if self.dimension[0] != max_window_height or\\\n            self.dimension[1] != max_window_len:\n        screen.erase()\n    self.dimension[0] = max_window_height\n    self.dimension[1] = max_window_len\n    # add margins for changing the pages\n    self.update_sec_page_map(max_window_height - 20)\n    display_str = \"Options: [Up Arrow] Move Up  [Down Arrow] Move Down\"\n    screen.addstr(0, 0, display_string(max_window_len, display_str))\n    display_str = \"Available Phishing Scenarios:\"\n    screen.addstr(3, 0, display_string(max_window_len, display_str),\n                  curses.A_BOLD)\nexcept curses.error:\n    return 0\n\n# add blank line\nrow_num = 5\nfirst = False\nfor number, short_lines in enumerate(self.sections):\n    try:\n\n        # incase user shrink the window and the heightlight section\n        # is in the next page. for this case, just shift the\n        # heightlight section to the first scenario in the first\n        # page\n        if self.sec_page_map[self.heightlight_number] !=\\\n                self.page_number and not first:\n            # heightlight the first scenario\n            screen.addstr(row_num, 2, short_lines[0],\n                          self.heightlight_text)\n            self.heightlight_number = 0\n            self.page_number = 0\n            first = True\n\n        # display the sections belonged to the current page\n        if self.sec_page_map[number] != self.page_number:\n            continue\n\n        screen.addstr(row_num, 0, str(number + 1), self.green_text)\n\n        # emphasize the phishing scenario\n        if number == self.heightlight_number:\n            screen.addstr(row_num, 2, short_lines[0],\n                          self.heightlight_text)\n        else:\n            screen.addstr(row_num, 2, short_lines[0], curses.A_BOLD)\n        row_num += 1\n        # add 8 spaces to the first line\n        screen.addstr(row_num, 8, short_lines[1])\n        row_num += 1\n        if len(short_lines) > 1:\n            for short_line in short_lines[2:]:\n                screen.addstr(row_num, 0, short_line)\n                row_num += 1\n        # add blank line between phishing scenarios\n        row_num += 1\n    except curses.error:\n        return row_num\n\nreturn row_num", "path": "wifiphisher/wifiphisher/common/tui.py", "commit_date": "2019-12-10 00:00:00", "repo_name": "wifiphisher/wifiphisher", "stars": 12591, "license": "gpl-3.0", "language": "python", "size": 12889}
{"docstring": "\"\"\"\nDisplay the AP informations on the screen\n\n:param self: A TuiApSel object\n:type self: TuiApSel\n:param screen: A curses window object\n:type screen: _curses.curses.window\n:param ap_info: An ApDisplayInfo object\n:type ap_info: ApDisplayInfo\n:return True if ap selection is done\n:rtype: bool\n\"\"\"\n\n", "func_signal": "def display_info(self, screen, ap_info):\n", "code": "is_apsel_end = False\nself.resize_window(screen, ap_info)\n\n# check if any new access points have been discovered\nnew_total_ap_number = len(\n    self.access_point_finder.observed_access_points)\n\nif new_total_ap_number != self.total_ap_number:\n    self.access_points = self.access_point_finder.\\\n        get_sorted_access_points()\n    self.total_ap_number = len(self.access_points)\n\n# display the information to the user\nself.display_access_points(screen, ap_info)\n# check for key movement and store result\nself.key_movement(ap_info)\n\n# ask for a key input (doesn't block)\nap_info.key = screen.getch()\nif ap_info.key == ord(\"\\n\") and self.total_ap_number != 0:\n    # show message and exit\n    screen.addstr(ap_info.max_h - 2, 3, \"YOU HAVE SELECTED \" +\n                  self.access_points[ap_info.pos - 1].name)\n    screen.refresh()\n    time.sleep(1)\n    is_apsel_end = True\nreturn is_apsel_end", "path": "wifiphisher/wifiphisher/common/tui.py", "commit_date": "2019-12-10 00:00:00", "repo_name": "wifiphisher/wifiphisher", "stars": 12591, "license": "gpl-3.0", "language": "python", "size": 12889}
{"docstring": "\"\"\"\nSimple constructor for PandasColumns that expresses existence constraints.\n\nArgs:\n    name (str): Name of the column. This must match up with the column name in the dataframe you\n        expect to receive.\n    non_nullable (Optional[bool]): If true, this column will enforce a constraint that all values in the column\n        ought to be non null values.\n    unique (Optional[bool]): If true, this column will enforce a uniqueness constraint on the column values.\n    ignore_missing_vals (Optional[bool]): A flag that is passed into most constraints. If true, the constraint will\n        only evaluate non-null data. Ignore_missing_vals and non_nullable cannot both be True.\n    is_required (Optional[bool]): Flag indicating the optional/required presence of the column.\n        If the column exists the validate function will validate the column. Default to True.\n\"\"\"\n", "func_signal": "def exists(name, non_nullable=False, unique=False, ignore_missing_vals=False, is_required=None):\n", "code": "return PandasColumn(\n    name=check.str_param(name, \"name\"),\n    constraints=_construct_keyword_constraints(\n        non_nullable=non_nullable, unique=unique, ignore_missing_vals=ignore_missing_vals\n    ),\n    is_required=is_required,\n)", "path": "dagster/python_modules/libraries/dagster-pandas/dagster_pandas/validation.py", "commit_date": "2020-10-28 00:00:00", "repo_name": "dagster-io/dagster", "stars": 9796, "license": "apache-2.0", "language": "python", "size": 928130}
{"docstring": "\"\"\"Synchronously execute a single query against Redshift. Will return a list of rows, where\neach row is a tuple of values, e.g. SELECT 1 will return [(1,)].\n\nArgs:\n    query (str): The query to execute.\n    fetch_results (Optional[bool]): Whether to return the results of executing the query.\n        Defaults to False, in which case the query will be executed without retrieving the\n        results.\n    cursor_factory (Optional[:py:class:`psycopg2.extensions.cursor`]): An alternative\n        cursor_factory; defaults to None. Will be used when constructing the cursor.\n    error_callback (Optional[Callable[[Exception, Cursor, DagsterLogManager], None]]): A\n        callback function, invoked when an exception is encountered during query execution;\n        this is intended to support executing additional queries to provide diagnostic\n        information, e.g. by querying ``stl_load_errors`` using ``pg_last_copy_id()``. If no\n        function is provided, exceptions during query execution will be raised directly.\n\nReturns:\n    Optional[List[Tuple[Any, ...]]]: Results of the query, as a list of tuples, when\n        fetch_results is set. Otherwise return None.\n\"\"\"\n", "func_signal": "def execute_query(self, query, fetch_results=False, cursor_factory=None, error_callback=None):\n", "code": "check.str_param(query, \"query\")\ncheck.bool_param(fetch_results, \"fetch_results\")\ncheck.opt_subclass_param(cursor_factory, \"cursor_factory\", psycopg2.extensions.cursor)\ncheck.opt_callable_param(error_callback, \"error_callback\")\n\nwith self._get_conn() as conn:\n    with self._get_cursor(conn, cursor_factory=cursor_factory) as cursor:\n        try:\n            six.ensure_str(query)\n\n            self.log.info(\"Executing query '{query}'\".format(query=query))\n            cursor.execute(query)\n\n            if fetch_results and cursor.rowcount > 0:\n                return cursor.fetchall()\n            else:\n                self.log.info(\"Empty result from query\")\n\n        except Exception as e:  # pylint: disable=broad-except\n            # If autocommit is disabled or not set (it is disabled by default), Redshift\n            # will be in the middle of a transaction at exception time, and because of\n            # the failure the current transaction will not accept any further queries.\n            #\n            # This conn.commit() call closes the open transaction before handing off\n            # control to the error callback, so that the user can issue additional\n            # queries. Notably, for e.g. pg_last_copy_id() to work, it requires you to\n            # use the same conn/cursor, so you have to do this conn.commit() to ensure\n            # things are in a usable state in the error callback.\n            if not self.autocommit:\n                conn.commit()\n\n            if error_callback is not None:\n                error_callback(e, cursor, self.log)\n            else:\n                raise", "path": "dagster/python_modules/libraries/dagster-aws/dagster_aws/redshift/resources.py", "commit_date": "2020-10-28 00:00:00", "repo_name": "dagster-io/dagster", "stars": 9796, "license": "apache-2.0", "language": "python", "size": 928130}
{"docstring": "\"\"\"\nvalidates that all values in an iterable are unique\nReturns duplicated values as metadata\n\nUsage:\n    As a validation function for a\n    :py:class:'~dagster_pandas.constraints.ColumnAggregateConstraintWithMetadata'\n    or :py:class:'~dagster_pandas.constraints.MultiAggregateConstraintWithMetadata'\nExample:\n    .. code-block:: python\n        aggregate_validator = MultiAggregateConstraintWithMetadata(\n        \"confirms all values are unique\",\n        {'bar': [all_unique_validator]},\n        ConstraintWithMetadataException,\n        raise_or_typecheck=False,\n        )\n        ntype = create_structured_dataframe_type(\n        \"NumericType\",\n        columns_aggregate_validator=aggregate_validator\n        )\n        @solid(output_defs=[OutputDefinition(name='basic_dataframe', dagster_type=ntype)])\n        def create_dataframe(_):\n            yield Output(\n            DataFrame({'foo': [1, 2, 3], 'bar': [9, 10, 10]}), output_name='basic_dataframe',\n        )\n        #will fail with\n        metadata['offending'] == {'bar': {'all_unique_validator': 'a violation'}}\n        metadata['actual'] == {'bar': {'all_unique_validator': [10.0]}}\n\"\"\"\n", "func_signal": "def all_unique_validator(column, ignore_missing_vals=False):\n", "code": "column = pd.Series(column)\nduplicated = column.duplicated()\nif ignore_missing_vals:\n    duplicated = apply_ignore_missing_data_to_mask(duplicated, column)\nreturn not duplicated.any(), {\"actual\": column[duplicated]}", "path": "dagster/python_modules/libraries/dagster-pandas/dagster_pandas/constraints.py", "commit_date": "2020-11-23 00:00:00", "repo_name": "dagster-io/dagster", "stars": 9796, "license": "apache-2.0", "language": "python", "size": 928130}
{"docstring": "\"\"\"\nTest that the read_keys, which are deprecated, do not overlap with\nthe normal loader config_keys.\n\"\"\"\n", "func_signal": "def test_dataframe_loader_config_keys_dont_overlap():\n", "code": "config_keys = set(DataFrameUtilities.keys())\nconfig_keys.add(\"read\")\nread_keys = set(DataFrameReadTypes.keys())\n\nassert len(config_keys.intersection(read_keys)) == 0", "path": "dagster/python_modules/libraries/dagster-dask/dagster_dask_tests/test_data_frame.py", "commit_date": "2020-10-28 00:00:00", "repo_name": "dagster-io/dagster", "stars": 9796, "license": "apache-2.0", "language": "python", "size": 928130}
{"docstring": "# Create first server and query ID\n", "func_signal": "def test_detect_server_restart():\n", "code": "port, server_process = create_server_process()\ntry:\n    api_client = DagsterGrpcClient(port=port)\n    server_id_one = api_client.get_server_id()\n    assert server_id_one\nfinally:\n    interrupt_ipc_subprocess_pid(server_process.pid)\n\nseven.wait_for_process(server_process, timeout=5)\nwith pytest.raises(grpc._channel._InactiveRpcError):  # pylint: disable=protected-access\n    api_client.get_server_id()\n\n# Create second server and query ID\nport, server_process = create_server_process()\ntry:\n    api_client = DagsterGrpcClient(port=port)\n    server_id_two = api_client.get_server_id()\n    assert server_id_two\nfinally:\n    interrupt_ipc_subprocess_pid(server_process.pid)\n\nassert server_id_one != server_id_two", "path": "dagster/python_modules/dagster/dagster_tests/general_tests/grpc_tests/test_ping.py", "commit_date": "2020-11-20 00:00:00", "repo_name": "dagster-io/dagster", "stars": 9796, "license": "apache-2.0", "language": "python", "size": 928130}
{"docstring": "\"\"\"Redshift configuration. See the Redshift documentation for reference:\n\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/connecting-to-cluster.html\n\"\"\"\n\n", "func_signal": "def define_redshift_config():\n", "code": "return {\n    \"host\": Field(StringSource, description=\"Redshift host\", is_required=True),\n    \"port\": Field(\n        IntSource, description=\"Redshift port\", is_required=False, default_value=5439\n    ),\n    \"user\": Field(\n        StringSource, description=\"Username for Redshift connection\", is_required=False,\n    ),\n    \"password\": Field(\n        StringSource, description=\"Password for Redshift connection\", is_required=False,\n    ),\n    \"database\": Field(\n        StringSource,\n        description=\"Name of the default database to use. After login, you can use USE DATABASE\"\n        \" to change the database.\",\n        is_required=False,\n    ),\n    \"schema\": Field(\n        StringSource,\n        description=\"Name of the default schema to use. After login, you can use USE SCHEMA to \"\n        \"change the schema.\",\n        is_required=False,\n    ),\n    \"autocommit\": Field(\n        bool,\n        description=\"None by default, which honors the Redshift parameter AUTOCOMMIT. Set to \"\n        \"True or False to enable or disable autocommit mode in the session, respectively.\",\n        is_required=False,\n    ),\n    \"connect_timeout\": Field(\n        int,\n        description=\"Connection timeout in seconds. 5 seconds by default\",\n        is_required=False,\n        default_value=5,\n    ),\n    \"sslmode\": Field(\n        str,\n        description=\"SSL mode to use. See the Redshift documentation for more information on \"\n        \"usage: https://docs.aws.amazon.com/redshift/latest/mgmt/connecting-ssl-support.html\",\n        is_required=False,\n        default_value=\"require\",\n    ),\n}", "path": "dagster/python_modules/libraries/dagster-aws/dagster_aws/redshift/resources.py", "commit_date": "2020-10-28 00:00:00", "repo_name": "dagster-io/dagster", "stars": 9796, "license": "apache-2.0", "language": "python", "size": 928130}
{"docstring": "\"\"\"PushAdd metrics to the given pushgateway.\n`job` is the job label to be attached to all pushed metrics\n`registry` is an instance of CollectorRegistry\n`grouping_key` please see the pushgateway documentation for details.\n            Defaults to None\n`handler` is an optional function which can be provided to perform\n        requests to the 'gateway'.\n        Defaults to None, in which case an http or https request\n        will be carried out by a default handler.\n        See the 'prometheus_client.push_to_gateway' documentation\n        for implementation requirements.\nThis replaces metrics with the same name, job and grouping_key.\nThis uses the POST HTTP method.\"\"\"\n", "func_signal": "def pushadd_to_gateway(self, job, grouping_key=None, handler=default_handler):\n", "code": "prometheus_client.pushadd_to_gateway(\n    gateway=self.gateway,\n    job=job,\n    registry=self.registry,\n    grouping_key=grouping_key,\n    timeout=self.timeout,\n    handler=handler,\n)", "path": "dagster/python_modules/libraries/dagster-prometheus/dagster_prometheus/resources.py", "commit_date": "2020-11-23 00:00:00", "repo_name": "dagster-io/dagster", "stars": 9796, "license": "apache-2.0", "language": "python", "size": 928130}
{"docstring": "# manually building up this guy\n", "func_signal": "def test_fan_in_manual():\n", "code": "@composite_solid\ndef _target_composite_dsl(str_in, none_in):\n    num = emit_num()\n    return collect([num, str_in, none_in])\n\n# base case works\n_target_composite_manual = CompositeSolidDefinition(\n    name=\"manual_composite\",\n    solid_defs=[emit_num, collect],\n    input_mappings=[\n        InputDefinition(\"str_in\").mapping_to(\"collect\", \"stuff\", 1),\n        InputDefinition(\"none_in\").mapping_to(\"collect\", \"stuff\", 2),\n    ],\n    output_mappings=[OutputDefinition().mapping_from(\"collect\")],\n    dependencies={\n        \"collect\": {\n            \"stuff\": MultiDependencyDefinition(\n                [\n                    DependencyDefinition(\"emit_num\"),\n                    MappedInputPlaceholder,\n                    MappedInputPlaceholder,\n                ]\n            )\n        }\n    },\n)\n\nwith pytest.raises(\n    DagsterInvalidDefinitionError,\n    match=\"index 2 in the MultiDependencyDefinition is not a MappedInputPlaceholder\",\n):\n    _missing_placeholder = CompositeSolidDefinition(\n        name=\"manual_composite\",\n        solid_defs=[emit_num, collect],\n        input_mappings=[\n            InputDefinition(\"str_in\").mapping_to(\"collect\", \"stuff\", 1),\n            InputDefinition(\"none_in\").mapping_to(\"collect\", \"stuff\", 2),\n        ],\n        output_mappings=[OutputDefinition().mapping_from(\"collect\")],\n        dependencies={\n            \"collect\": {\n                \"stuff\": MultiDependencyDefinition(\n                    [DependencyDefinition(\"emit_num\"), MappedInputPlaceholder,]\n                )\n            }\n        },\n    )\n\nwith pytest.raises(DagsterInvalidDefinitionError, match=\"is not a MultiDependencyDefinition\"):\n    _bad_target = CompositeSolidDefinition(\n        name=\"manual_composite\",\n        solid_defs=[emit_num, collect],\n        input_mappings=[\n            InputDefinition(\"str_in\").mapping_to(\"collect\", \"stuff\", 1),\n            InputDefinition(\"none_in\").mapping_to(\"collect\", \"stuff\", 2),\n        ],\n        output_mappings=[OutputDefinition().mapping_from(\"collect\")],\n        dependencies={\"collect\": {\"stuff\": DependencyDefinition(\"emit_num\")}},\n    )\n\nwith pytest.raises(\n    DagsterInvalidDefinitionError, match=\"Unsatisfied MappedInputPlaceholder at index 3\",\n):\n    _missing_placeholder = CompositeSolidDefinition(\n        name=\"manual_composite\",\n        solid_defs=[emit_num, collect],\n        input_mappings=[\n            InputDefinition(\"str_in\").mapping_to(\"collect\", \"stuff\", 1),\n            InputDefinition(\"none_in\").mapping_to(\"collect\", \"stuff\", 2),\n        ],\n        output_mappings=[OutputDefinition().mapping_from(\"collect\")],\n        dependencies={\n            \"collect\": {\n                \"stuff\": MultiDependencyDefinition(\n                    [\n                        DependencyDefinition(\"emit_num\"),\n                        MappedInputPlaceholder,\n                        MappedInputPlaceholder,\n                        MappedInputPlaceholder,\n                    ]\n                )\n            }\n        },\n    )", "path": "dagster/python_modules/dagster/dagster_tests/core_tests/test_multi_dependency.py", "commit_date": "2020-12-04 00:00:00", "repo_name": "dagster-io/dagster", "stars": 9796, "license": "apache-2.0", "language": "python", "size": 928130}
{"docstring": "\"\"\"Delete metrics from the given pushgateway.\n`job` is the job label to be attached to all pushed metrics\n`grouping_key` please see the pushgateway documentation for details.\n            Defaults to None\n`handler` is an optional function which can be provided to perform\n        requests to the 'gateway'.\n        Defaults to None, in which case an http or https request\n        will be carried out by a default handler.\n        See the 'prometheus_client.push_to_gateway' documentation\n        for implementation requirements.\nThis deletes metrics with the given job and grouping_key.\nThis uses the DELETE HTTP method.\"\"\"\n", "func_signal": "def delete_from_gateway(self, job, grouping_key=None, handler=default_handler):\n", "code": "prometheus_client.delete_from_gateway(\n    gateway=self.gateway,\n    job=job,\n    grouping_key=grouping_key,\n    timeout=self.timeout,\n    handler=handler,\n)", "path": "dagster/python_modules/libraries/dagster-prometheus/dagster_prometheus/resources.py", "commit_date": "2020-11-23 00:00:00", "repo_name": "dagster-io/dagster", "stars": 9796, "license": "apache-2.0", "language": "python", "size": 928130}
{"docstring": "\"\"\"\nTest that the to_keys, which are deprecated, do not overlap with\nthe normal materializer config_keys.\n\"\"\"\n", "func_signal": "def test_dataframe_materializer_config_keys_dont_overlap():\n", "code": "config_keys = set(DataFrameUtilities.keys())\nconfig_keys.add(\"to\")\nto_keys = set(DataFrameToTypes.keys())\n\nassert len(config_keys.intersection(to_keys)) == 0", "path": "dagster/python_modules/libraries/dagster-dask/dagster_dask_tests/test_data_frame.py", "commit_date": "2020-10-28 00:00:00", "repo_name": "dagster-io/dagster", "stars": 9796, "license": "apache-2.0", "language": "python", "size": 928130}
{"docstring": "\"\"\"Fake for execute_query; returns [self.QUERY_RESULT]\n\nArgs:\n    query (str): The query to execute.\n    fetch_results (Optional[bool]): Whether to return the results of executing the query.\n        Defaults to False, in which case the query will be executed without retrieving the\n        results.\n    cursor_factory (Optional[:py:class:`psycopg2.extensions.cursor`]): An alternative\n        cursor_factory; defaults to None. Will be used when constructing the cursor.\n    error_callback (Optional[Callable[[Exception, Cursor, DagsterLogManager], None]]): A\n        callback function, invoked when an exception is encountered during query execution;\n        this is intended to support executing additional queries to provide diagnostic\n        information, e.g. by querying ``stl_load_errors`` using ``pg_last_copy_id()``. If no\n        function is provided, exceptions during query execution will be raised directly.\n\nReturns:\n    Optional[List[Tuple[Any, ...]]]: Results of the query, as a list of tuples, when\n        fetch_results is set. Otherwise return None.\n\"\"\"\n", "func_signal": "def execute_query(self, query, fetch_results=False, cursor_factory=None, error_callback=None):\n", "code": "check.str_param(query, \"query\")\ncheck.bool_param(fetch_results, \"fetch_results\")\ncheck.opt_subclass_param(cursor_factory, \"cursor_factory\", psycopg2.extensions.cursor)\ncheck.opt_callable_param(error_callback, \"error_callback\")\n\nself.log.info(\"Executing query '{query}'\".format(query=query))\nif fetch_results:\n    return self.QUERY_RESULT", "path": "dagster/python_modules/libraries/dagster-aws/dagster_aws/redshift/resources.py", "commit_date": "2020-10-28 00:00:00", "repo_name": "dagster-io/dagster", "stars": 9796, "license": "apache-2.0", "language": "python", "size": 928130}
{"docstring": "\"\"\"Used to represent a leading Log4J line that doesn't conform to the regular expressions we\nexpect.\n\"\"\"\n", "func_signal": "def fake_record(line, line_num):\n", "code": "return Log4jRecord(\n    caller_location=\"\",\n    level=\"\",\n    logger=\"\",\n    message=line,\n    num_lines=1,\n    start_line=line_num,\n    thread=\"\",\n    timestamp=\"\",\n)", "path": "dagster/python_modules/libraries/dagster-aws/dagster_aws/utils/mrjob/log4j.py", "commit_date": "2020-08-22 00:00:00", "repo_name": "dagster-io/dagster", "stars": 9796, "license": "apache-2.0", "language": "python", "size": 928130}
{"docstring": "# cant guarantee order\n", "func_signal": "def sum_num(_context, numbers):\n", "code": "assert set(numbers) == set([1, 2, 3])\nreturn sum(numbers)", "path": "dagster/python_modules/dagster/dagster_tests/core_tests/test_multi_dependency.py", "commit_date": "2020-12-04 00:00:00", "repo_name": "dagster-io/dagster", "stars": 9796, "license": "apache-2.0", "language": "python", "size": 928130}
{"docstring": "\"\"\"\ndecorator for column validation functions to make them error on nulls\nUsage:\n    pass decorated functions as column validators to\n    :py:class:'~dagster_pandas.constraints.ColumnConstraintWithMetadata'\n    or :py:class:'~dagster_pandas.constraints.MultiColumnConstraintWithMetadata'\nArgs:\n    func (Callable[[Any], Tuple[bool, dict[str, Union[dict,list, str, set]]]]]):\n        the column validator you want to error on nulls\n\"\"\"\n\n", "func_signal": "def nonnull(func):\n", "code": "@wraps(func)\ndef nvalidator(val):\n    origval = func(val)\n    nval = non_null_validation(val)\n    return origval[0] and nval[0], {}\n\nnvalidator.__doc__ += \" and ensures no values are null\"\n\nreturn nvalidator", "path": "dagster/python_modules/libraries/dagster-pandas/dagster_pandas/constraints.py", "commit_date": "2020-11-23 00:00:00", "repo_name": "dagster-io/dagster", "stars": 9796, "license": "apache-2.0", "language": "python", "size": 928130}
{"docstring": "\"\"\"Parse lines from a hadoop log into log4j records.\n\nYield Log4jRecords.\n\nLines will be converted to unicode, and trailing \\r and \\n will be stripped\nfrom lines.\n\nAlso yields fake records for leading non-log4j lines (trailing non-log4j\nlines are assumed to be part of a multiline message if not pre-filtered).\n\"\"\"\n", "func_signal": "def parse_hadoop_log4j_records(lines):\n", "code": "last_record = None\nline_num = 0\n\nfor line_num, line in enumerate(lines.split(\"\\n\")):\n    # convert from bytes to unicode, if needed, and strip trailing newlines\n    line = six.ensure_str(line).rstrip(\"\\r\\n\")\n\n    m = _HADOOP_LOG4J_LINE_RE.match(line) or _HADOOP_LOG4J_LINE_ALTERNATE_RE.match(line)\n\n    if m:\n        if last_record:\n            last_record = last_record._replace(num_lines=line_num - last_record.start_line)\n            yield last_record\n\n        matches = m.groupdict()\n\n        last_record = Log4jRecord(\n            caller_location=matches.get(\"caller_location\", \"\"),\n            level=matches[\"level\"],\n            logger=matches[\"logger\"],\n            message=matches[\"message\"],\n            num_lines=1,\n            start_line=line_num,\n            thread=matches.get(\"thread\", \"\"),\n            timestamp=matches[\"timestamp\"],\n        )\n    else:\n        # add on to previous record\n        if last_record:\n            last_record = last_record._replace(message=last_record.message + \"\\n\" + line)\n        else:\n            yield Log4jRecord.fake_record(line, line_num)\n\nif last_record:\n    last_record = last_record._replace(num_lines=line_num + 1 - last_record.start_line)\n    yield last_record", "path": "dagster/python_modules/libraries/dagster-aws/dagster_aws/utils/mrjob/log4j.py", "commit_date": "2020-08-22 00:00:00", "repo_name": "dagster-io/dagster", "stars": 9796, "license": "apache-2.0", "language": "python", "size": 928130}
{"docstring": "\"\"\"Push metrics to the given pushgateway.\n`job` is the job label to be attached to all pushed metrics\n`grouping_key` please see the pushgateway documentation for details.\n            Defaults to None\n`handler` is an optional function which can be provided to perform\n        requests to the 'gateway'.\n        Defaults to None, in which case an http or https request\n        will be carried out by a default handler.\n        If not None, the argument must be a function which accepts\n        the following arguments:\n        url, method, timeout, headers, and content\n        May be used to implement additional functionality not\n        supported by the built-in default handler (such as SSL\n        client certicates, and HTTP authentication mechanisms).\n        'url' is the URL for the request, the 'gateway' argument\n        described earlier will form the basis of this URL.\n        'method' is the HTTP method which should be used when\n        carrying out the request.\n        'timeout' requests not successfully completed after this\n        many seconds should be aborted.  If timeout is None, then\n        the handler should not set a timeout.\n        'headers' is a list of (\"header-name\",\"header-value\") tuples\n        which must be passed to the pushgateway in the form of HTTP\n        request headers.\n        The function should raise an exception (e.g. IOError) on\n        failure.\n        'content' is the data which should be used to form the HTTP\n        Message Body.\nThis overwrites all metrics with the same job and grouping_key.\nThis uses the PUT HTTP method.\"\"\"\n", "func_signal": "def push_to_gateway(self, job, grouping_key=None, handler=default_handler):\n", "code": "prometheus_client.push_to_gateway(\n    gateway=self.gateway,\n    job=job,\n    registry=self.registry,\n    grouping_key=grouping_key,\n    timeout=self.timeout,\n    handler=handler,\n)", "path": "dagster/python_modules/libraries/dagster-prometheus/dagster_prometheus/resources.py", "commit_date": "2020-11-23 00:00:00", "repo_name": "dagster-io/dagster", "stars": 9796, "license": "apache-2.0", "language": "python", "size": 928130}
{"docstring": "\"\"\"\nfactory for validators testing if column values are within a range\nArgs:\n    minim(Optional[Comparable]): the low end of the range\n    maxim(Optional[Comparable]): the high end of the range\n    ignore_missing_vals(Optional[bool]): whether to ignore nulls\n\nReturns: a validation function for this constraint\nUsage:\n    pass returned functions as column validators to\n     :py:class:'~dagster_pandas.constraints.ColumnConstraintWithMetadata'\n    or :py:class:'~dagster_pandas.constraints.MultiColumnConstraintWithMetadata'\nExamples:\n    .. code-block:: python\n        in_range_validator = column_range_validation_factory(1, 3, ignore_missing_vals=True)\n        column_validator = MultiColumnConstraintWithMetadata(\n                        \"confirms values are numbers in a range\",\n                        {'foo': [in_range_validator]},\n                        ColumnWithMetadataException,\n                        raise_or_typecheck=False,\n                    )\n        ntype = create_structured_dataframe_type(\n        \"NumericType\",\n        columns_validator=column_validator\n        )\n        @solid(output_defs=[OutputDefinition(name='basic_dataframe', dagster_type=ntype)])\n        def create_dataframe(_):\n            yield Output(\n            DataFrame({'foo': [1, 2, 7], 'bar': [9, 10, 10]}), output_name='basic_dataframe',\n        )\n        #will fail with\n        metadata['offending'] == {'foo': {'in_range_validation_fn': ['row 2']}}\n        metadata['actual'] == {'foo': {'in_range_validation_fn': [7]}}\n\n\"\"\"\n", "func_signal": "def column_range_validation_factory(minim=None, maxim=None, ignore_missing_vals=False):\n", "code": "if minim is None:\n    if isinstance(maxim, datetime):\n        minim = datetime.min\n    else:\n        minim = -1 * (sys.maxsize - 1)\nif maxim is None:\n    if isinstance(minim, datetime):\n        maxim = datetime.max\n    else:\n        maxim = sys.maxsize\n\ndef in_range_validation_fn(x):\n    if ignore_missing_vals and pd.isnull(x):\n        return True, {}\n    return (isinstance(x, (type(minim), type(maxim)))) and (x <= maxim) and (x >= minim), {}\n\nin_range_validation_fn.__doc__ = \"checks whether values are between {} and {}\".format(\n    minim, maxim\n)\nif ignore_missing_vals:\n    in_range_validation_fn.__doc__ += \", ignoring nulls\"\n\nreturn in_range_validation_fn", "path": "dagster/python_modules/libraries/dagster-pandas/dagster_pandas/constraints.py", "commit_date": "2020-11-23 00:00:00", "repo_name": "dagster-io/dagster", "stars": 9796, "license": "apache-2.0", "language": "python", "size": 928130}
{"docstring": "\"\"\"\nfactory for testing if the dtype of a val falls within some allowed set\nArgs:\n    datatypes(Union[set[type], type]): which datatype/datatypes are allowed\n    ignore_missing_vals(Optional[bool]): whether to ignore nulls\n\nReturns: a validation function for this constraint\n\nUsage:\n    pass returned functions as column validators to\n    :py:class:'~dagster_pandas.constraints.ColumnConstraintWithMetadata'\n    or :py:class:'~dagster_pandas.constraints.MultiColumnConstraintWithMetadata'\n\nExamples:\n    .. code-block:: python\n        dtype_is_num_validator = dtype_in_set_validation_factory((int, float, int64, float64))\n        column_validator = MultiColumnConstraintWithMetadata(\n        \"confirms values are numbers in a range\",\n        {'foo': [dtype_is_num_validator]},\n        ColumnWithMetadataException,\n        raise_or_typecheck=False,\n        )\n        ntype = create_structured_dataframe_type(\n        \"NumericType\",\n        columns_validator=column_validator\n        )\n        @solid(output_defs=[OutputDefinition(name='basic_dataframe', dagster_type=ntype)])\n        def create_dataframe(_):\n            yield Output(\n            DataFrame({'foo': [1, 'a', 7], 'bar': [9, 10, 10]}), output_name='basic_dataframe',\n        )\n        #will fail with\n        metadata['offending'] == {'foo': {'categorical_validation_fn': ['row 1']}}\n        metadata['actual'] == {'foo': {'categorical_validation_fn': ['a']}}\n\n\"\"\"\n\n", "func_signal": "def dtype_in_set_validation_factory(datatypes, ignore_missing_vals=False):\n", "code": "def dtype_in_set_validation_fn(x):\n    if ignore_missing_vals and pd.isnull(x):\n        return True, {}\n    return isinstance(x, datatypes), {}\n\ndtype_in_set_validation_fn.__doc__ = \"checks whether values are this type/types: {}\".format(\n    datatypes\n)\nif ignore_missing_vals:\n    dtype_in_set_validation_fn.__doc__ += \", ignoring nulls\"\n\nreturn dtype_in_set_validation_fn", "path": "dagster/python_modules/libraries/dagster-pandas/dagster_pandas/constraints.py", "commit_date": "2020-11-23 00:00:00", "repo_name": "dagster-io/dagster", "stars": 9796, "license": "apache-2.0", "language": "python", "size": 928130}
{"docstring": "\"\"\"\nReady pod right away\n\"\"\"\n", "func_signal": "def test_wait_for_pod_success():\n", "code": "mock_client = create_mocked_client()\n\nsingle_ready_running_pod = _pod_list_for_container_status(_ready_running_status())\n\nmock_client.core_api.list_namespaced_pod.side_effect = [single_ready_running_pod]\n\npod_name = \"a_pod\"\n\nmock_client.wait_for_pod(pod_name=pod_name, namespace=\"namespace\")\n\nassert_logger_calls(\n    mock_client.logger,\n    ['Waiting for pod \"%s\"' % pod_name, 'Pod \"%s\" is ready, done waiting' % pod_name],\n)", "path": "dagster/python_modules/libraries/dagster-k8s/dagster_k8s_tests/unit_tests/test_client.py", "commit_date": "2020-10-28 00:00:00", "repo_name": "dagster-io/dagster", "stars": 9796, "license": "apache-2.0", "language": "python", "size": 928130}
{"docstring": "\"\"\"\nfactory for validators testing if all values are in some set\nArgs:\n    categories(Union[Sequence, set]): the set of allowed values\n    ignore_missing_vals(Optional[bool]): whether to ignore nulls\n\nReturns: a validation function for this constraint\n\nUsage:\n    pass returned functions as column validators to\n    :py:class:'~dagster_pandas.constraints.ColumnConstraintWithMetadata'\n    or :py:class:'~dagster_pandas.constraints.MultiColumnConstraintWithMetadata'\n\nExample:\n    .. code-block:: python\n        categorical_validation_fn = categorical_column_validator_factory([1, 2])\n        column_validator = MultiColumnConstraintWithMetadata(\n                        \"confirms values are numbers in a range\",\n                        {'foo': [categorical_validation_fn]},\n                        ColumnWithMetadataException,\n                        raise_or_typecheck=False,\n                    )\n        ntype = create_structured_dataframe_type(\n        \"NumericType\",\n        columns_validator=column_validator\n        )\n        @solid(output_defs=[OutputDefinition(name='basic_dataframe', dagster_type=ntype)])\n        def create_dataframe(_):\n            yield Output(\n            DataFrame({'foo': [1, 2, 7], 'bar': [9, 10, 10]}), output_name='basic_dataframe',\n        )\n        #will fail with\n        metadata['offending'] == {'foo': {'categorical_validation_fn': ['row 2']}}\n        metadata['actual'] == {'foo': {'categorical_validation_fn': [7]}}\n\n\"\"\"\n\n", "func_signal": "def categorical_column_validator_factory(categories, ignore_missing_vals=False):\n", "code": "categories = set(categories)\n\ndef categorical_validation_fn(x):\n    if ignore_missing_vals and pd.isnull(x):\n        return True, {}\n    return (x in categories), {}\n\ncategorical_validation_fn.__doc__ = \"checks whether values are within this set of values: {}\".format(\n    categories\n)\nif ignore_missing_vals:\n    categorical_validation_fn.__doc__ += \", ignoring nulls\"\n\nreturn categorical_validation_fn", "path": "dagster/python_modules/libraries/dagster-pandas/dagster_pandas/constraints.py", "commit_date": "2020-11-23 00:00:00", "repo_name": "dagster-io/dagster", "stars": 9796, "license": "apache-2.0", "language": "python", "size": 928130}
{"docstring": "\"\"\"\n    specials: A lower-cased list of keys that will not be quoted.\n\"\"\"\n", "func_signal": "def _format_pairs(pairs, specials=(), sep=\"; \"):\n", "code": "vals = []\nfor k, v in pairs:\n    if k.lower() not in specials and _has_special(v):\n        v = ESCAPE.sub(r\"\\\\\\1\", v)\n        v = '\"%s\"' % v\n    vals.append(f\"{k}={v}\")\nreturn sep.join(vals)", "path": "mitmproxy/mitmproxy/net/http/cookies.py", "commit_date": "2020-11-20 00:00:00", "repo_name": "mitmproxy/mitmproxy", "stars": 33844, "license": "mit", "language": "python", "size": 61443}
{"docstring": "\"\"\"\n    Read pairs of lhs=rhs values from Cookie headers.\n\n    off: start offset\n\"\"\"\n", "func_signal": "def _read_cookie_pairs(s, off=0):\n", "code": "pairs = []\n\nwhile True:\n    lhs, off = _read_key(s, off)\n    lhs = lhs.lstrip()\n\n    rhs = \"\"\n    if off < len(s) and s[off] == \"=\":\n        rhs, off = _read_value(s, off + 1, \";\")\n    if rhs or lhs:\n        pairs.append([lhs, rhs])\n\n    off += 1\n\n    if not off < len(s):\n        break\n\nreturn pairs, off", "path": "mitmproxy/mitmproxy/net/http/cookies.py", "commit_date": "2020-11-20 00:00:00", "repo_name": "mitmproxy/mitmproxy", "stars": 33844, "license": "mit", "language": "python", "size": 61443}
{"docstring": "\"\"\"\n    Read until one of the characters in term is reached.\n\"\"\"\n", "func_signal": "def _read_until(s, start, term):\n", "code": "if start == len(s):\n    return \"\", start + 1\nfor i in range(start, len(s)):\n    if s[i] in term:\n        return s[start:i], i\nreturn s[start:i + 1], i + 1", "path": "mitmproxy/mitmproxy/net/http/cookies.py", "commit_date": "2020-11-20 00:00:00", "repo_name": "mitmproxy/mitmproxy", "stars": 33844, "license": "mit", "language": "python", "size": 61443}
{"docstring": "\"\"\"\nassert that data received before a server connection is established\nwill still be forwarded.\n\"\"\"\n", "func_signal": "def test_receive_data_before_server_connected(tctx):\n", "code": "assert (\n        Playbook(tcp.TCPLayer(tctx), hooks=False)\n        << OpenConnection(tctx.server)\n        >> DataReceived(tctx.client, b\"hello!\")\n        >> reply(None, to=-2)\n        << SendData(tctx.server, b\"hello!\")\n)", "path": "mitmproxy/test/mitmproxy/proxy/layers/test_tcp.py", "commit_date": "2020-12-14 00:00:00", "repo_name": "mitmproxy/mitmproxy", "stars": 33844, "license": "mit", "language": "python", "size": 61443}
{"docstring": "\"\"\"\ndata received after the other connection has been half-closed should still be forwarded.\n\"\"\"\n", "func_signal": "def test_receive_data_after_half_close(tctx):\n", "code": "assert (\n        Playbook(tcp.TCPLayer(tctx), hooks=False)\n        << OpenConnection(tctx.server)\n        >> reply(None)\n        >> DataReceived(tctx.client, b\"eof-delimited-request\")\n        << SendData(tctx.server, b\"eof-delimited-request\")\n        >> ConnectionClosed(tctx.client)\n        << CloseConnection(tctx.server, half_close=True)\n        >> DataReceived(tctx.server, b\"i'm late\")\n        << SendData(tctx.client, b\"i'm late\")\n        >> ConnectionClosed(tctx.server)\n        << CloseConnection(tctx.client)\n)", "path": "mitmproxy/test/mitmproxy/proxy/layers/test_tcp.py", "commit_date": "2020-12-14 00:00:00", "repo_name": "mitmproxy/mitmproxy", "stars": 33844, "license": "mit", "language": "python", "size": 61443}
{"docstring": "\"\"\"\n    Remove an addon and all its sub-addons.\n\n    If the addon is not in the chain - that is, if it's managed by a\n    parent addon - it's the parent's responsibility to remove it from\n    its own addons attribute.\n\"\"\"\n", "func_signal": "def remove(self, addon):\n", "code": "for a in traverse([addon]):\n    n = _get_name(a)\n    if n not in self.lookup:\n        raise exceptions.AddonManagerError(\"No such addon: %s\" % n)\n    self.chain = [i for i in self.chain if i is not a]\n    del self.lookup[_get_name(a)]\nself.invoke_addon(addon, \"done\")", "path": "mitmproxy/mitmproxy/addonmanager.py", "commit_date": "2020-12-28 00:00:00", "repo_name": "mitmproxy/mitmproxy", "stars": 33844, "license": "mit", "language": "python", "size": 61443}
{"docstring": "\"\"\"\n    Determines the time when the cookie will be expired.\n\n    Considering both 'expires' and 'max-age' parameters.\n\n    Returns: timestamp of when the cookie will expire.\n             None, if no expiration time is set.\n\"\"\"\n", "func_signal": "def get_expiration_ts(cookie_attrs):\n", "code": "if 'expires' in cookie_attrs:\n    e = email.utils.parsedate_tz(cookie_attrs[\"expires\"])\n    if e:\n        return email.utils.mktime_tz(e)\n\nelif 'max-age' in cookie_attrs:\n    try:\n        max_age = int(cookie_attrs['Max-Age'])\n    except ValueError:\n        pass\n    else:\n        now_ts = time.time()\n        return now_ts + max_age\n\nreturn None", "path": "mitmproxy/mitmproxy/net/http/cookies.py", "commit_date": "2020-11-20 00:00:00", "repo_name": "mitmproxy/mitmproxy", "stars": 33844, "license": "mit", "language": "python", "size": 61443}
{"docstring": "\"\"\"\n    Recursively traverse an addon chain.\n\"\"\"\n", "func_signal": "def traverse(chain):\n", "code": "for a in chain:\n    yield a\n    if hasattr(a, \"addons\"):\n        yield from traverse(a.addons)", "path": "mitmproxy/mitmproxy/addonmanager.py", "commit_date": "2020-12-28 00:00:00", "repo_name": "mitmproxy/mitmproxy", "stars": 33844, "license": "mit", "language": "python", "size": 61443}
{"docstring": "\"\"\"\n    Register an addon, call its load event, and then register all its\n    sub-addons. This should be used by addons that dynamically manage\n    addons.\n\n    If the calling addon is already running, it should follow with\n    running and configure events. Must be called within a current\n    context.\n\"\"\"\n", "func_signal": "def register(self, addon):\n", "code": "for a in traverse([addon]):\n    name = _get_name(a)\n    if name in self.lookup:\n        raise exceptions.AddonManagerError(\n            \"An addon called '%s' already exists.\" % name\n        )\nl = Loader(self.master)\nself.invoke_addon(addon, \"load\", l)\nfor a in traverse([addon]):\n    name = _get_name(a)\n    self.lookup[name] = a\nfor a in traverse([addon]):\n    self.master.commands.collect_commands(a)\nself.master.options.process_deferred()\nreturn addon", "path": "mitmproxy/mitmproxy/addonmanager.py", "commit_date": "2020-12-28 00:00:00", "repo_name": "mitmproxy/mitmproxy", "stars": 33844, "license": "mit", "language": "python", "size": 61443}
{"docstring": "\"\"\"\n    Add addons to the end of the chain, and run their load event.\n    If any addon has sub-addons, they are registered.\n\"\"\"\n", "func_signal": "def add(self, *addons):\n", "code": "for i in addons:\n    self.chain.append(self.register(i))", "path": "mitmproxy/mitmproxy/addonmanager.py", "commit_date": "2020-12-28 00:00:00", "repo_name": "mitmproxy/mitmproxy", "stars": 33844, "license": "mit", "language": "python", "size": 61443}
{"docstring": "\"\"\"\n    start: offset to the first quote of the string to be read\n\n    A sort of loose super-set of the various quoted string specifications.\n\n    RFC6265 disallows backslashes or double quotes within quoted strings.\n    Prior RFCs use backslashes to escape. This leaves us free to apply\n    backslash escaping by default and be compatible with everything.\n\"\"\"\n", "func_signal": "def _read_quoted_string(s, start):\n", "code": "escaping = False\nret = []\n# Skip the first quote\ni = start  # initialize in case the loop doesn't run.\nfor i in range(start + 1, len(s)):\n    if escaping:\n        ret.append(s[i])\n        escaping = False\n    elif s[i] == '\"':\n        break\n    elif s[i] == \"\\\\\":\n        escaping = True\n    else:\n        ret.append(s[i])\nreturn \"\".join(ret), i + 1", "path": "mitmproxy/mitmproxy/net/http/cookies.py", "commit_date": "2020-11-20 00:00:00", "repo_name": "mitmproxy/mitmproxy", "stars": 33844, "license": "mit", "language": "python", "size": 61443}
{"docstring": "\"\"\"\n    Trigger an event across all addons.\n\"\"\"\n", "func_signal": "def trigger(self, name, *args, **kwargs):\n", "code": "for i in self.chain:\n    try:\n        with safecall():\n            self.invoke_addon(i, name, *args, **kwargs)\n    except exceptions.AddonHalt:\n        return", "path": "mitmproxy/mitmproxy/addonmanager.py", "commit_date": "2020-12-28 00:00:00", "repo_name": "mitmproxy/mitmproxy", "stars": 33844, "license": "mit", "language": "python", "size": 61443}
{"docstring": "\"\"\"\nCut off a traceback at the function with the given name.\nThe func_name's frame is excluded.\n\nArgs:\n    tb: traceback object, as returned by sys.exc_info()[2]\n    func_name: function name\n\nReturns:\n    Reduced traceback.\n\"\"\"\n", "func_signal": "def cut_traceback(tb, func_name):\n", "code": "tb_orig = tb\nfor _, _, fname, _ in traceback.extract_tb(tb):\n    tb = tb.tb_next\n    if fname == func_name:\n        break\nreturn tb or tb_orig", "path": "mitmproxy/mitmproxy/addonmanager.py", "commit_date": "2020-12-28 00:00:00", "repo_name": "mitmproxy/mitmproxy", "stars": 33844, "license": "mit", "language": "python", "size": 61443}
{"docstring": "\"\"\"open connection, receive data, send it to peer\"\"\"\n", "func_signal": "def test_simple(tctx):\n", "code": "f = Placeholder(TCPFlow)\n\nassert (\n        Playbook(tcp.TCPLayer(tctx))\n        << tcp.TcpStartHook(f)\n        >> reply()\n        << OpenConnection(tctx.server)\n        >> reply(None)\n        >> DataReceived(tctx.client, b\"hello!\")\n        << tcp.TcpMessageHook(f)\n        >> reply()\n        << SendData(tctx.server, b\"hello!\")\n        >> DataReceived(tctx.server, b\"hi\")\n        << tcp.TcpMessageHook(f)\n        >> reply()\n        << SendData(tctx.client, b\"hi\")\n        >> ConnectionClosed(tctx.server)\n        << CloseConnection(tctx.client, half_close=True)\n        >> ConnectionClosed(tctx.client)\n        << CloseConnection(tctx.server)\n        << tcp.TcpEndHook(f)\n        >> reply()\n        >> ConnectionClosed(tctx.client)\n        << None\n)\nassert len(f().messages) == 2", "path": "mitmproxy/test/mitmproxy/proxy/layers/test_tcp.py", "commit_date": "2020-12-14 00:00:00", "repo_name": "mitmproxy/mitmproxy", "stars": 33844, "license": "mit", "language": "python", "size": 61443}
{"docstring": "\"\"\"\nConverts a list of pairs to a (name, value, attrs) for each cookie.\n\"\"\"\n\n", "func_signal": "def group_cookies(pairs):\n", "code": "if not pairs:\n    return []\n\ncookie_list = []\n\n# First pair is always a new cookie\nname, value = pairs[0]\nattrs = []\n\nfor k, v in pairs[1:]:\n    if k.lower() in _cookie_params:\n        attrs.append((k, v))\n    else:\n        cookie_list.append((name, value, CookieAttrs(attrs)))\n        name, value, attrs = k, v, []\n\ncookie_list.append((name, value, CookieAttrs(attrs)))\nreturn cookie_list", "path": "mitmproxy/mitmproxy/net/http/cookies.py", "commit_date": "2020-11-20 00:00:00", "repo_name": "mitmproxy/mitmproxy", "stars": 33844, "license": "mit", "language": "python", "size": 61443}
{"docstring": "\"\"\"\n    Determines whether a cookie has expired.\n\n    Returns: boolean\n\"\"\"\n\n", "func_signal": "def is_expired(cookie_attrs):\n", "code": "exp_ts = get_expiration_ts(cookie_attrs)\nnow_ts = time.time()\n\n# If no expiration information was provided with the cookie\nif exp_ts is None:\n    return False\nelse:\n    return exp_ts <= now_ts", "path": "mitmproxy/mitmproxy/net/http/cookies.py", "commit_date": "2020-11-20 00:00:00", "repo_name": "mitmproxy/mitmproxy", "stars": 33844, "license": "mit", "language": "python", "size": 61443}
{"docstring": "\"\"\"\nIf there is no server connection yet, establish one,\nbecause the server may send data first.\n\"\"\"\n", "func_signal": "def test_open_connection(tctx):\n", "code": "assert (\n        Playbook(tcp.TCPLayer(tctx, True))\n        << OpenConnection(tctx.server)\n)\n\ntctx.server.state = ConnectionState.OPEN\nassert (\n        Playbook(tcp.TCPLayer(tctx, True))\n        << None\n)", "path": "mitmproxy/test/mitmproxy/proxy/layers/test_tcp.py", "commit_date": "2020-12-14 00:00:00", "repo_name": "mitmproxy/mitmproxy", "stars": 33844, "license": "mit", "language": "python", "size": 61443}
{"docstring": "\"\"\"\nno flow hooks when we set ignore.\n\"\"\"\n\n", "func_signal": "def test_ignore(tctx, ignore):\n", "code": "def no_flow_hooks():\n    assert (\n            Playbook(tcp.TCPLayer(tctx, ignore=ignore), hooks=True)\n            << OpenConnection(tctx.server)\n            >> reply(None)\n            >> DataReceived(tctx.client, b\"hello!\")\n            << SendData(tctx.server, b\"hello!\")\n    )\n\nif ignore:\n    no_flow_hooks()\nelse:\n    with pytest.raises(AssertionError):\n        no_flow_hooks()", "path": "mitmproxy/test/mitmproxy/proxy/layers/test_tcp.py", "commit_date": "2020-12-14 00:00:00", "repo_name": "mitmproxy/mitmproxy", "stars": 33844, "license": "mit", "language": "python", "size": 61443}
{"docstring": "\"\"\"\n    Parse a Cookie header value.\n    Returns a list of (lhs, rhs) tuples.\n\"\"\"\n", "func_signal": "def parse_cookie_header(line):\n", "code": "pairs, off_ = _read_cookie_pairs(line)\nreturn pairs", "path": "mitmproxy/mitmproxy/net/http/cookies.py", "commit_date": "2020-11-20 00:00:00", "repo_name": "mitmproxy/mitmproxy", "stars": 33844, "license": "mit", "language": "python", "size": 61443}
{"docstring": "\"\"\"\n    Invoke an event on an addon and all its children.\n\"\"\"\n", "func_signal": "def invoke_addon(self, addon, name, *args, **kwargs):\n", "code": "if name not in eventsequence.Events:\n    raise exceptions.AddonManagerError(\"Unknown event: %s\" % name)\nfor a in traverse([addon]):\n    func = getattr(a, name, None)\n    if func:\n        if callable(func):\n            func(*args, **kwargs)\n        elif isinstance(func, types.ModuleType):\n            # we gracefully exclude module imports with the same name as hooks.\n            # For example, a user may have \"from mitmproxy import log\" in an addon,\n            # which has the same name as the \"log\" hook. In this particular case,\n            # we end up in an error loop because we \"log\" this error.\n            pass\n        else:\n            raise exceptions.AddonManagerError(\n                f\"Addon handler {name} ({a}) not callable\"\n            )", "path": "mitmproxy/mitmproxy/addonmanager.py", "commit_date": "2020-12-28 00:00:00", "repo_name": "mitmproxy/mitmproxy", "stars": 33844, "license": "mit", "language": "python", "size": 61443}
{"docstring": "\"\"\"Test that noun_chunks raises Value Error for 'de' language if Doc is not parsed.\"\"\"\n", "func_signal": "def test_noun_chunks_is_parsed_de(de_tokenizer):\n", "code": "doc = de_tokenizer(\"Er lag auf seinem\")\nwith pytest.raises(ValueError):\n    list(doc.noun_chunks)", "path": "spaCy/spacy/tests/lang/de/test_noun_chunks.py", "commit_date": "2020-09-29 00:00:00", "repo_name": "explosion/spaCy", "stars": 28396, "license": "mit", "language": "python", "size": 203282}
{"docstring": "\"\"\"#6060: reset norm in split\"\"\"\n", "func_signal": "def test_doc_retokenizer_split_norm(en_vocab):\n", "code": "text = \"The quick brownfoxjumpsoverthe lazy dog w/ white spots\"\ndoc = Doc(en_vocab, words=text.split())\n\n# Set custom norm on the w/ token.\ndoc[5].norm_ = \"with\"\n\n# Retokenize to split out the words in the token at doc[2].\ntoken = doc[2]\nwith doc.retokenize() as retokenizer:\n    retokenizer.split(\n        token,\n        [\"brown\", \"fox\", \"jumps\", \"over\", \"the\"],\n        heads=[(token, idx) for idx in range(5)],\n    )\n\nassert doc[9].text == \"w/\"\nassert doc[9].norm_ == \"with\"\nassert doc[5].text == \"over\"\nassert doc[5].norm_ == \"over\"", "path": "spaCy/spacy/tests/doc/test_retokenize_split.py", "commit_date": "2020-10-05 00:00:00", "repo_name": "explosion/spaCy", "stars": 28396, "license": "mit", "language": "python", "size": 203282}
{"docstring": "\"\"\"Test that the user_data can be preserved (but not by default). \"\"\"\n", "func_signal": "def test_span_as_doc_user_data(doc):\n", "code": "my_key = \"my_info\"\nmy_value = 342\ndoc.user_data[my_key] = my_value\n\nspan = doc[4:10]\nspan_doc_with = span.as_doc(copy_user_data=True)\nspan_doc_without = span.as_doc()\n\nassert doc.user_data.get(my_key, None) is my_value\nassert span_doc_with.user_data.get(my_key, None) is my_value\nassert span_doc_without.user_data.get(my_key, None) is None", "path": "spaCy/spacy/tests/doc/test_span.py", "commit_date": "2020-10-04 00:00:00", "repo_name": "explosion/spaCy", "stars": 28396, "license": "mit", "language": "python", "size": 203282}
{"docstring": "\"\"\"Test span.sentiment property's default averaging behaviour\"\"\"\n", "func_signal": "def test_spans_default_sentiment(en_tokenizer):\n", "code": "text = \"good stuff bad stuff\"\ntokens = en_tokenizer(text)\ntokens.vocab[tokens[0].text].sentiment = 3.0\ntokens.vocab[tokens[2].text].sentiment = -2.0\ndoc = Doc(tokens.vocab, words=[t.text for t in tokens])\nassert doc[:2].sentiment == 3.0 / 2\nassert doc[-2:].sentiment == -2.0 / 2\nassert doc[:-1].sentiment == (3.0 + -2) / 3.0", "path": "spaCy/spacy/tests/doc/test_span.py", "commit_date": "2020-10-04 00:00:00", "repo_name": "explosion/spaCy", "stars": 28396, "license": "mit", "language": "python", "size": 203282}
{"docstring": "# note: three spaces after \"I\"\n", "func_signal": "def test_ja_tokenizer_extra_spaces(ja_tokenizer):\n", "code": "tokens = ja_tokenizer(\"I   like cheese.\")\nassert tokens[1].orth_ == \"  \"", "path": "spaCy/spacy/tests/lang/ja/test_tokenizer.py", "commit_date": "2020-09-29 00:00:00", "repo_name": "explosion/spaCy", "stars": 28396, "license": "mit", "language": "python", "size": 203282}
{"docstring": "\"\"\"Test spans can be hashed.\"\"\"\n", "func_signal": "def test_spans_are_hashable(en_tokenizer):\n", "code": "text = \"good stuff bad stuff\"\ntokens = en_tokenizer(text)\nspan1 = tokens[:2]\nspan2 = tokens[2:4]\nassert hash(span1) != hash(span2)\nspan3 = tokens[0:2]\nassert hash(span3) == hash(span1)", "path": "spaCy/spacy/tests/doc/test_span.py", "commit_date": "2020-10-04 00:00:00", "repo_name": "explosion/spaCy", "stars": 28396, "license": "mit", "language": "python", "size": 203282}
{"docstring": "\"\"\"Test that the regular retokenizer.split raises an error if the orths\ndon't match the original token text. There might still be a method that\nallows this, but for the default use cases, merging and splitting should\nalways conform with spaCy's non-destructive tokenization policy. Otherwise,\nit can lead to very confusing and unexpected results.\n\"\"\"\n", "func_signal": "def test_doc_retokenize_split_orths_mismatch(en_vocab):\n", "code": "doc = Doc(en_vocab, words=[\"LosAngeles\", \"start\", \".\"])\nwith pytest.raises(ValueError):\n    with doc.retokenize() as retokenizer:\n        retokenizer.split(doc[0], [\"L\", \"A\"], [(doc[0], 0), (doc[0], 0)])", "path": "spaCy/spacy/tests/doc/test_retokenize_split.py", "commit_date": "2020-10-05 00:00:00", "repo_name": "explosion/spaCy", "stars": 28396, "license": "mit", "language": "python", "size": 203282}
{"docstring": "\"\"\"Test span's lca matrix generation\"\"\"\n", "func_signal": "def test_spans_lca_matrix(en_tokenizer):\n", "code": "tokens = en_tokenizer(\"the lazy dog slept\")\ndoc = Doc(\n    tokens.vocab,\n    words=[t.text for t in tokens],\n    heads=[2, 2, 3, 3],\n    deps=[\"dep\"] * 4,\n)\nlca = doc[:2].get_lca_matrix()\nassert lca.shape == (2, 2)\nassert lca[0, 0] == 0  # the & the -> the\nassert lca[0, 1] == -1  # the & lazy -> dog (out of span)\nassert lca[1, 0] == -1  # lazy & the -> dog (out of span)\nassert lca[1, 1] == 1  # lazy & lazy -> lazy\n\nlca = doc[1:].get_lca_matrix()\nassert lca.shape == (3, 3)\nassert lca[0, 0] == 0  # lazy & lazy -> lazy\nassert lca[0, 1] == 1  # lazy & dog -> dog\nassert lca[0, 2] == 2  # lazy & slept -> slept\n\nlca = doc[2:].get_lca_matrix()\nassert lca.shape == (2, 2)\nassert lca[0, 0] == 0  # dog & dog -> dog\nassert lca[0, 1] == 1  # dog & slept -> slept\nassert lca[1, 0] == 1  # slept & dog -> slept\nassert lca[1, 1] == 1  # slept & slept -> slept", "path": "spaCy/spacy/tests/doc/test_span.py", "commit_date": "2020-10-04 00:00:00", "repo_name": "explosion/spaCy", "stars": 28396, "license": "mit", "language": "python", "size": 203282}
{"docstring": "\"\"\"Test span.sentiment property's default averaging behaviour\"\"\"\n", "func_signal": "def test_spans_override_sentiment(en_tokenizer):\n", "code": "text = \"good stuff bad stuff\"\ntokens = en_tokenizer(text)\ntokens.vocab[tokens[0].text].sentiment = 3.0\ntokens.vocab[tokens[2].text].sentiment = -2.0\ndoc = Doc(tokens.vocab, words=[t.text for t in tokens])\ndoc.user_span_hooks[\"sentiment\"] = lambda span: 10.0\nassert doc[:2].sentiment == 10.0\nassert doc[-2:].sentiment == 10.0\nassert doc[:-1].sentiment == 10.0", "path": "spaCy/spacy/tests/doc/test_span.py", "commit_date": "2020-10-04 00:00:00", "repo_name": "explosion/spaCy", "stars": 28396, "license": "mit", "language": "python", "size": 203282}
{"docstring": "# Test entity IOB stays consistent after merging\n", "func_signal": "def test_doc_retokenize_spans_entity_split_iob():\n", "code": "words = [\"abc\", \"d\", \"e\"]\ndoc = Doc(Vocab(), words=words)\ndoc.ents = [(doc.vocab.strings.add(\"ent-abcd\"), 0, 2)]\nassert doc[0].ent_iob_ == \"B\"\nassert doc[1].ent_iob_ == \"I\"\nwith doc.retokenize() as retokenizer:\n    retokenizer.split(doc[0], [\"a\", \"b\", \"c\"], [(doc[0], 1), (doc[0], 2), doc[1]])\nassert doc[0].ent_iob_ == \"B\"\nassert doc[1].ent_iob_ == \"I\"\nassert doc[2].ent_iob_ == \"I\"\nassert doc[3].ent_iob_ == \"I\"", "path": "spaCy/spacy/tests/doc/test_retokenize_split.py", "commit_date": "2020-10-05 00:00:00", "repo_name": "explosion/spaCy", "stars": 28396, "license": "mit", "language": "python", "size": 203282}
{"docstring": "# note: three spaces after \"I\"\n", "func_signal": "def test_zh_extra_spaces(zh_tokenizer_char):\n", "code": "tokens = zh_tokenizer_char(\"I   like cheese.\")\nassert tokens[1].orth_ == \"  \"", "path": "spaCy/spacy/tests/lang/zh/test_tokenizer.py", "commit_date": "2020-09-26 00:00:00", "repo_name": "explosion/spaCy", "stars": 28396, "license": "mit", "language": "python", "size": 203282}
{"docstring": "# normalise base exceptions,  e.g. punctuation or currency symbols\n", "func_signal": "def norm(string):\n", "code": "if string in BASE_NORMS:\n    return BASE_NORMS[string]\n# set stem word as norm,  if available,  adapted from:\n# https://github.com/explosion/spaCy/blob/master/spacy/lang/hi/lex_attrs.py\n# https://www.researchgate.net/publication/237261579_Structure_of_Nepali_Grammar\nfor suffix_group in reversed(_stem_suffixes):\n    length = len(suffix_group[0])\n    if len(string) <= length:\n        break\n    for suffix in suffix_group:\n        if string.endswith(suffix):\n            return string[:-length]\nreturn string", "path": "spaCy/spacy/lang/ne/lex_attrs.py", "commit_date": "2020-07-22 00:00:00", "repo_name": "explosion/spaCy", "stars": 28396, "license": "mit", "language": "python", "size": 203282}
{"docstring": "# fmt: off\n", "func_signal": "def test_doc_retokenize_spans_sentence_update_after_split(en_vocab):\n", "code": "words = [\"StewartLee\", \"is\", \"a\", \"stand\", \"up\", \"comedian\", \".\", \"He\",\n         \"lives\", \"in\", \"England\", \"and\", \"loves\", \"JoePasquale\", \".\"]\nheads = [1, 1, 3, 5, 3, 1, 1, 8, 8, 8, 9, 8, 8, 14, 12]\ndeps = [\"nsubj\", \"ROOT\", \"det\", \"amod\", \"prt\", \"attr\", \"punct\", \"nsubj\",\n        \"ROOT\", \"prep\", \"pobj\", \"cc\", \"conj\", \"compound\", \"punct\"]\n# fmt: on\ndoc = Doc(en_vocab, words=words, heads=heads, deps=deps)\nsent1, sent2 = list(doc.sents)\ninit_len = len(sent1)\ninit_len2 = len(sent2)\nwith doc.retokenize() as retokenizer:\n    retokenizer.split(\n        doc[0],\n        [\"Stewart\", \"Lee\"],\n        [(doc[0], 1), doc[1]],\n        attrs={\"dep\": [\"compound\", \"nsubj\"]},\n    )\n    retokenizer.split(\n        doc[13],\n        [\"Joe\", \"Pasquale\"],\n        [(doc[13], 1), doc[12]],\n        attrs={\"dep\": [\"compound\", \"dobj\"]},\n    )\nsent1, sent2 = list(doc.sents)\nassert len(sent1) == init_len + 1\nassert len(sent2) == init_len2 + 1", "path": "spaCy/spacy/tests/doc/test_retokenize_split.py", "commit_date": "2020-10-05 00:00:00", "repo_name": "explosion/spaCy", "stars": 28396, "license": "mit", "language": "python", "size": 203282}
{"docstring": "\"\"\"Test that noun_chunks raises Value Error for 'nb' language if Doc is not parsed.\"\"\"\n", "func_signal": "def test_noun_chunks_is_parsed_nb(nb_tokenizer):\n", "code": "doc = nb_tokenizer(\"Sm\u00f8rsausen brukes bl.a. til\")\nwith pytest.raises(ValueError):\n    list(doc.noun_chunks)", "path": "spaCy/spacy/tests/lang/nb/test_noun_chunks.py", "commit_date": "2020-09-29 00:00:00", "repo_name": "explosion/spaCy", "stars": 28396, "license": "mit", "language": "python", "size": 203282}
{"docstring": "\"\"\"Compute a loss based on a distance between the documents' vectors and\nthe prediction.\n\"\"\"\n# The simplest way to implement this would be to vstack the\n# token.vector values, but that's a bit inefficient, especially on GPU.\n# Instead we fetch the index into the vectors table for each of our tokens,\n# and look them up all at once. This prevents data copying.\n", "func_signal": "def get_vectors_loss(ops, docs, prediction, distance):\n", "code": "ids = ops.flatten([doc.to_array(ID).ravel() for doc in docs])\ntarget = docs[0].vocab.vectors.data[ids]\nd_target, loss = distance(prediction, target)\nreturn loss, d_target", "path": "spaCy/spacy/ml/models/multi_task.py", "commit_date": "2020-12-08 00:00:00", "repo_name": "explosion/spaCy", "stars": 28396, "license": "mit", "language": "python", "size": 203282}
{"docstring": "\"\"\"Test span.sent property\"\"\"\n", "func_signal": "def test_spans_span_sent(doc, doc_not_parsed):\n", "code": "assert len(list(doc.sents))\nassert doc[:2].sent.root.text == \"is\"\nassert doc[:2].sent.text == \"This is a sentence .\"\nassert doc[6:7].sent.root.left_edge.text == \"This\"\n# test on manual sbd\ndoc_not_parsed[0].is_sent_start = True\ndoc_not_parsed[5].is_sent_start = True\nassert doc_not_parsed[1:3].sent == doc_not_parsed[0:5]\nassert doc_not_parsed[10:14].sent == doc_not_parsed[5:]", "path": "spaCy/spacy/tests/doc/test_span.py", "commit_date": "2020-10-04 00:00:00", "repo_name": "explosion/spaCy", "stars": 28396, "license": "mit", "language": "python", "size": 203282}
{"docstring": "# two sentences to test that all matches are within the same sentence\n", "func_signal": "def test_dependency_matcher_precedence_ops(en_vocab, op, num_matches):\n", "code": "doc = Doc(\n    en_vocab,\n    words=[\"a\", \"b\", \"c\", \"d\", \"e\"] * 2,\n    heads=[0, 0, 0, 0, 0, 5, 5, 5, 5, 5],\n    deps=[\"dep\"] * 10,\n)\nmatch_count = 0\nfor text in [\"a\", \"b\", \"c\", \"d\", \"e\"]:\n    pattern = [\n        {\"RIGHT_ID\": \"1\", \"RIGHT_ATTRS\": {\"ORTH\": text}},\n        {\"LEFT_ID\": \"1\", \"REL_OP\": op, \"RIGHT_ID\": \"2\", \"RIGHT_ATTRS\": {}},\n    ]\n    matcher = DependencyMatcher(en_vocab)\n    matcher.add(\"A\", [pattern])\n    matches = matcher(doc)\n    match_count += len(matches)\n    for match in matches:\n        match_id, token_ids = match\n        # token_ids[0] op token_ids[1]\n        if op == \".\":\n            assert token_ids[0] == token_ids[1] - 1\n        elif op == \";\":\n            assert token_ids[0] == token_ids[1] + 1\n        elif op == \".*\":\n            assert token_ids[0] < token_ids[1]\n        elif op == \";*\":\n            assert token_ids[0] > token_ids[1]\n        # all tokens are within the same sentence\n        assert doc[token_ids[0]].sent == doc[token_ids[1]].sent\nassert match_count == num_matches", "path": "spaCy/spacy/tests/matcher/test_dependency_matcher.py", "commit_date": "2020-10-31 00:00:00", "repo_name": "explosion/spaCy", "stars": 28396, "license": "mit", "language": "python", "size": 203282}
{"docstring": "\"\"\"Test that retokenization also sets attributes on the lexeme if they're\nlexical attributes. For example, if a user sets IS_STOP, it should mean that\n\"all tokens with that lexeme\" are marked as a stop word, so the ambiguity\nhere is acceptable. Also see #2390.\n\"\"\"\n", "func_signal": "def test_doc_retokenizer_split_lex_attrs(en_vocab):\n", "code": "assert not Doc(en_vocab, words=[\"Los\"])[0].is_stop\nassert not Doc(en_vocab, words=[\"Angeles\"])[0].is_stop\ndoc = Doc(en_vocab, words=[\"LosAngeles\", \"start\"])\nassert not doc[0].is_stop\nwith doc.retokenize() as retokenizer:\n    attrs = {\"is_stop\": [True, False]}\n    heads = [(doc[0], 1), doc[1]]\n    retokenizer.split(doc[0], [\"Los\", \"Angeles\"], heads, attrs=attrs)\nassert doc[0].is_stop\nassert not doc[1].is_stop", "path": "spaCy/spacy/tests/doc/test_retokenize_split.py", "commit_date": "2020-10-05 00:00:00", "repo_name": "explosion/spaCy", "stars": 28396, "license": "mit", "language": "python", "size": 203282}
{"docstring": "# fmt: off\n", "func_signal": "def doc(en_tokenizer):\n", "code": "text = \"This is a sentence. This is another sentence. And a third.\"\nheads = [1, 1, 3, 1, 1, 6, 6, 8, 6, 6, 12, 12, 12, 12]\ndeps = [\"nsubj\", \"ROOT\", \"det\", \"attr\", \"punct\", \"nsubj\", \"ROOT\", \"det\",\n        \"attr\", \"punct\", \"ROOT\", \"det\", \"npadvmod\", \"punct\"]\n# fmt: on\ntokens = en_tokenizer(text)\nreturn Doc(tokens.vocab, words=[t.text for t in tokens], heads=heads, deps=deps)", "path": "spaCy/spacy/tests/doc/test_span.py", "commit_date": "2020-10-04 00:00:00", "repo_name": "explosion/spaCy", "stars": 28396, "license": "mit", "language": "python", "size": 203282}
{"docstring": "\"\"\"Compute a loss based on a number of characters predicted from the docs.\"\"\"\n", "func_signal": "def get_characters_loss(ops, docs, prediction, nr_char):\n", "code": "target_ids = numpy.vstack([doc.to_utf8_array(nr_char=nr_char) for doc in docs])\ntarget_ids = target_ids.reshape((-1,))\ntarget = ops.asarray(to_categorical(target_ids, n_classes=256), dtype=\"f\")\ntarget = target.reshape((-1, 256 * nr_char))\ndiff = prediction - target\nloss = (diff ** 2).sum()\nd_target = diff / float(prediction.shape[0])\nreturn loss, d_target", "path": "spaCy/spacy/ml/models/multi_task.py", "commit_date": "2020-12-08 00:00:00", "repo_name": "explosion/spaCy", "stars": 28396, "license": "mit", "language": "python", "size": 203282}
{"docstring": "# Some tests need input files to operate on. There are two cases:\n", "func_signal": "def get_data_dir():\n", "code": "if getattr(sys, 'frozen', False):\n    # 1. Frozen: rely on gettemp to find the correct directory both in\n    #    onefile and in onedir modes.\n    return gettemp('data')\nelse:\n    # 2. Not frozen: rely on the filesystem layout of this git repository.\n    return os.path.join(os.path.dirname(os.path.abspath(__file__)),\n                        '..', 'data')", "path": "pyinstaller/tests/functional/modules/pyi_get_datadir.py", "commit_date": "2020-01-08 00:00:00", "repo_name": "pyinstaller/pyinstaller", "stars": 11133, "license": "other", "language": "python", "size": 61798}
{"docstring": "# TODO: look if this can be done more efficient with out the\n# loop, e.g. by not using a list as base at all\n", "func_signal": "def extend(self, other):\n", "code": "for entry in other:\n    self.append(entry)", "path": "pyinstaller/PyInstaller/building/datastruct.py", "commit_date": "2020-01-08 00:00:00", "repo_name": "pyinstaller/pyinstaller", "stars": 11133, "license": "other", "language": "python", "size": 61798}
{"docstring": "\"\"\"\nInitialize this graph node.\n\nParameters\n----------\nidentifier : str\n    Fully-qualified name of this graph node's corresponding module,\n    package, or C extension.\n\"\"\"\n\n", "func_signal": "def __init__(self, identifier):\n", "code": "self.code = None\nself.filename = None\nself.graphident = identifier\nself.identifier = identifier\nself.packagepath = None\nself._deferred_imports = None\nself._global_attr_names = set()\nself._starimported_ignored_module_names = set()\nself._submodule_basename_to_node = dict()", "path": "pyinstaller/PyInstaller/lib/modulegraph/modulegraph.py", "commit_date": "2020-12-19 00:00:00", "repo_name": "pyinstaller/pyinstaller", "stars": 11133, "license": "other", "language": "python", "size": 61798}
{"docstring": "\"\"\"\nInitialize this alias.\n\nParameters\n----------\nname : str\n    Fully-qualified name of the non-existent target module to be\n    created (as an alias of the existing source module).\nnode : Node\n    Graph node of the existing source module being aliased.\n\"\"\"\n", "func_signal": "def __init__(self, name, node):\n", "code": "super(AliasNode, self).__init__(name)\n\n#FIXME: Why only some? Why not *EVERYTHING* except \"graphident\", which\n#must remain equal to \"name\" for lookup purposes? This is, after all,\n#an alias. The idea is for the two nodes to effectively be the same.\n\n# Copy some attributes from this source module into this target alias.\nfor attr_name in (\n    'identifier', 'packagepath',\n    '_global_attr_names', '_starimported_ignored_module_names',\n    '_submodule_basename_to_node'):\n    if hasattr(node, attr_name):\n        setattr(self, attr_name, getattr(node, attr_name))", "path": "pyinstaller/PyInstaller/lib/modulegraph/modulegraph.py", "commit_date": "2020-12-19 00:00:00", "repo_name": "pyinstaller/pyinstaller", "stars": 11133, "license": "other", "language": "python", "size": 61798}
{"docstring": "\"\"\"\nReturn structure like:\n\n{\n    # Translation independent information.\n    # VS_FIXEDFILEINFO - Contains version information about a file. This information is language and code page independent.\n    u'FileVersion':      (1, 2, 3, 4),\n    u'ProductVersion':   (9, 10, 11, 12),\n\n    # PE files might contain several translations of version information.\n    # VS_VERSIONINFO - Depicts the organization of data in a file-version resource. It is the root structure that contains all other file-version information structures.\n    u'translations': {\n        'lang_id1' : {\n            u'Comments':         u'\u65e5\u672c\u8a9e, Unicode \u5bfe\u5fdc.',\n            u'CompanyName':      u'your company.',\n            u'FileDescription':  u'your file desc.',\n            u'FileVersion':      u'1, 2, 3, 4',\n            u'InternalName':     u'your internal name.',\n            u'LegalCopyright':   u'your legal copyright.',\n            u'LegalTrademarks':  u'your legal trademarks.',\n            u'OriginalFilename': u'your original filename.',\n            u'PrivateBuild':     u'5, 6, 7, 8',\n            u'ProductName':      u'your product name',\n            u'ProductVersion':   u'9, 10, 11, 12',\n            u'SpecialBuild':     u'13, 14, 15, 16',\n        },\n\n        'lang_id2' : {\n            ...\n        }\n    }\n}\n\nVersion info can contain multiple languages.\n\"\"\"\n# TODO\n", "func_signal": "def pefile_read_version(filename):\n", "code": "vers = {\n    'FileVersion': (0, 0, 0, 0),\n    'ProductVersion': (0, 0, 0, 0),\n    'translations': {\n        'lang_id1': {\n            'Comments': '',\n            'CompanyName': '',\n            'FileDescription': '',\n            'FileVersion': '',\n            'InternalName': '',\n            'LegalCopyright': '',\n            'LegalTrademarks': '',\n            'OriginalFilename': '',\n            'PrivateBuild': '',\n            'ProductName': '',\n            'ProductVersion': '',\n            'SpecialBuild': '',\n        }\n    }\n}\npe = pefile.PE(filename)\n#ffi = pe.VS_FIXEDFILEINFO\n#vers['FileVersion'] = (ffi.FileVersionMS >> 16, ffi.FileVersionMS & 0xFFFF, ffi.FileVersionLS >> 16, ffi.FileVersionLS & 0xFFFF)\n#vers['ProductVersion'] = (ffi.ProductVersionMS >> 16, ffi.ProductVersionMS & 0xFFFF, ffi.ProductVersionLS >> 16, ffi.ProductVersionLS & 0xFFFF)\n#print(pe.VS_FIXEDFILEINFO.FileVersionMS)\n# TODO Only first available language is used for now.\n#vers = pe.FileInfo[0].StringTable[0].entries\nfrom pprint import pprint\npprint(pe.VS_FIXEDFILEINFO)\nprint(dir(pe.VS_FIXEDFILEINFO))\nprint(repr(pe.VS_FIXEDFILEINFO))\nprint(pe.dump_info())\npe.close()\nreturn vers", "path": "pyinstaller/PyInstaller/utils/win32/versioninfo.py", "commit_date": "2020-12-12 00:00:00", "repo_name": "pyinstaller/pyinstaller", "stars": 11133, "license": "other", "language": "python", "size": 61798}
{"docstring": "\"\"\"Remove duplicates from a list, preserving order\"\"\"\n# Taken from https://stackoverflow.com/questions/480214\n", "func_signal": "def uniq(seq):\n", "code": "seen = set()\nseen_add = seen.add\nreturn [x for x in seq if not (x in seen or seen_add(x))]", "path": "pyinstaller/PyInstaller/lib/modulegraph/modulegraph.py", "commit_date": "2020-12-19 00:00:00", "repo_name": "pyinstaller/pyinstaller", "stars": 11133, "license": "other", "language": "python", "size": 61798}
{"docstring": "# XXX: Can this be implemented using Dot()?\n", "func_signal": "def itergraphreport(self, name='G', flatpackages=()):\n", "code": "nodes = list(map(self.graph.describe_node, self.graph.iterdfs(self)))\ndescribe_edge = self.graph.describe_edge\nedges = deque()\npackagenodes = set()\npackageidents = {}\nnodetoident = {}\ninpackages = {}\nmainedges = set()\n\n# XXX - implement\nflatpackages = dict(flatpackages)\n\ndef nodevisitor(node, data, outgoing, incoming):\n    if not isinstance(data, Node):\n        return {'label': str(node)}\n    #if isinstance(d, (ExcludedModule, MissingModule, BadModule)):\n    #    return None\n    s = '<f0> ' + type(data).__name__\n    for i, v in enumerate(data.infoTuple()[:1], 1):\n        s += '| <f%d> %s' % (i, v)\n    return {'label': s, 'shape': 'record'}\n\n\ndef edgevisitor(edge, data, head, tail):\n    # XXX: This method nonsense, the edge\n    # data is never initialized.\n    if data == 'orphan':\n        return {'style': 'dashed'}\n    elif data == 'pkgref':\n        return {'style': 'dotted'}\n    return {}\n\nyield 'digraph %s {\\ncharset=\"UTF-8\";\\n' % (name,)\nattr = dict(rankdir='LR', concentrate='true')\ncpatt = '%s=\"%s\"'\nfor item in attr.items():\n    yield '\\t%s;\\n' % (cpatt % item,)\n\n# find all packages (subgraphs)\nfor (node, data, outgoing, incoming) in nodes:\n    nodetoident[node] = getattr(data, 'identifier', None)\n    if isinstance(data, Package):\n        packageidents[data.identifier] = node\n        inpackages[node] = set([node])\n        packagenodes.add(node)\n\n# create sets for subgraph, write out descriptions\nfor (node, data, outgoing, incoming) in nodes:\n    # update edges\n    for edge in (describe_edge(e) for e in outgoing):\n        edges.append(edge)\n\n    # describe node\n    yield '\\t\"%s\" [%s];\\n' % (\n        node,\n        ','.join([\n            (cpatt % item) for item in\n            nodevisitor(node, data, outgoing, incoming).items()\n        ]),\n    )\n\n    inside = inpackages.get(node)\n    if inside is None:\n        inside = inpackages[node] = set()\n    ident = nodetoident[node]\n    if ident is None:\n        continue\n    pkgnode = packageidents.get(ident[:ident.rfind('.')])\n    if pkgnode is not None:\n        inside.add(pkgnode)\n\ngraph = []\nsubgraphs = {}\nfor key in packagenodes:\n    subgraphs[key] = []\n\nwhile edges:\n    edge, data, head, tail = edges.popleft()\n    if ((head, tail)) in mainedges:\n        continue\n    mainedges.add((head, tail))\n    tailpkgs = inpackages[tail]\n    common = inpackages[head] & tailpkgs\n    if not common and tailpkgs:\n        usepkgs = sorted(tailpkgs)\n        if len(usepkgs) != 1 or usepkgs[0] != tail:\n            edges.append((edge, data, head, usepkgs[0]))\n            edges.append((edge, 'pkgref', usepkgs[-1], tail))\n            continue\n    if common:\n        common = common.pop()\n        if tail == common:\n            edges.append((edge, data, tail, head))\n        elif head == common:\n            subgraphs[common].append((edge, 'pkgref', head, tail))\n        else:\n            edges.append((edge, data, common, head))\n            edges.append((edge, data, common, tail))\n\n    else:\n        graph.append((edge, data, head, tail))\n\ndef do_graph(edges, tabs):\n    edgestr = tabs + '\"%s\" -> \"%s\" [%s];\\n'\n    # describe edge\n    for (edge, data, head, tail) in edges:\n        attribs = edgevisitor(edge, data, head, tail)\n        yield edgestr % (\n            head,\n            tail,\n            ','.join([(cpatt % item) for item in attribs.items()]),\n        )\n\nfor g, edges in subgraphs.items():\n    yield '\\tsubgraph \"cluster_%s\" {\\n' % (g,)\n    yield '\\t\\tlabel=\"%s\";\\n' % (nodetoident[g],)\n    for s in do_graph(edges, '\\t\\t'):\n        yield s\n    yield '\\t}\\n'\n\nfor s in do_graph(graph, '\\t'):\n    yield s\n\nyield '}\\n'", "path": "pyinstaller/PyInstaller/lib/modulegraph/modulegraph.py", "commit_date": "2020-12-19 00:00:00", "repo_name": "pyinstaller/pyinstaller", "stars": 11133, "license": "other", "language": "python", "size": 61798}
{"docstring": "# This is a hack, but sadly enough the necessary information\n# isn't available otherwise.\n", "func_signal": "def _path_from_importerror(exc, default):\n", "code": "m = re.match(r'^No module named (\\S+)$', str(exc))\nif m is not None:\n    return m.group(1)\n\nreturn default", "path": "pyinstaller/PyInstaller/lib/modulegraph/modulegraph.py", "commit_date": "2020-12-19 00:00:00", "repo_name": "pyinstaller/pyinstaller", "stars": 11133, "license": "other", "language": "python", "size": 61798}
{"docstring": "\"\"\" Overridable.\n    Check to see if the file object self.lib actually has a file\n    we understand.\n\"\"\"\n", "func_signal": "def checkmagic(self):\n", "code": "self.lib.seek(self.start)  # default - magic is at start of file.\nif self.lib.read(len(self.MAGIC)) != self.MAGIC:\n    raise RuntimeError(\"%s is not a valid %s archive file\"\n                       % (self.path, self.__class__.__name__))\nif self.lib.read(len(self.pymagic)) != self.pymagic:\n    print(\"Warning: pyz is from a different Python version\",\n          file=sys.stderr)\nself.lib.read(4)", "path": "pyinstaller/PyInstaller/utils/cliutils/archive_viewer.py", "commit_date": "2020-01-12 00:00:00", "repo_name": "pyinstaller/pyinstaller", "stars": 11133, "license": "other", "language": "python", "size": 61798}
{"docstring": "\"\"\"\nReturn the __path__ for the python package in *fqname*.\n\nThis function uses setuptools metadata to extract information\nabout namespace packages from installed eggs.\n\"\"\"\n", "func_signal": "def _namespace_package_path(fqname, pathnames, path=None):\n", "code": "working_set = pkg_resources.WorkingSet(path)\n\npath = list(pathnames)\n\nfor dist in working_set:\n    if dist.has_metadata('namespace_packages.txt'):\n        namespaces = dist.get_metadata(\n                'namespace_packages.txt').splitlines()\n        if fqname in namespaces:\n            nspath = os.path.join(dist.location, *fqname.split('.'))\n            if nspath not in path:\n                path.append(nspath)\n\nreturn path", "path": "pyinstaller/PyInstaller/lib/modulegraph/modulegraph.py", "commit_date": "2020-12-19 00:00:00", "repo_name": "pyinstaller/pyinstaller", "stars": 11133, "license": "other", "language": "python", "size": 61798}
{"docstring": "\"\"\"\nAlias the source module to the target module with the passed names.\n\nThis method ensures that the next call to findNode() given the target\nmodule name will resolve this alias. This includes importing and adding\na graph node for the source module if needed as well as adding a\nreference from the target to source module.\n\nParameters\n----------\nsrc_module_name : str\n    Fully-qualified name of the existing **source module** (i.e., the\n    module being aliased).\ntrg_module_name : str\n    Fully-qualified name of the non-existent **target module** (i.e.,\n    the alias to be created).\n\"\"\"\n", "func_signal": "def alias_module(self, src_module_name, trg_module_name):\n", "code": "self.msg(3, 'alias_module \"%s\" -> \"%s\"' % (src_module_name, trg_module_name))\n# print('alias_module \"%s\" -> \"%s\"' % (src_module_name, trg_module_name))\nassert isinstance(src_module_name, str), '\"%s\" not a module name.' % str(src_module_name)\nassert isinstance(trg_module_name, str), '\"%s\" not a module name.' % str(trg_module_name)\n\n# If the target module has already been added to the graph as either a\n# non-alias or as a different alias, raise an exception.\ntrg_module = self.findNode(trg_module_name)\nif trg_module is not None and not (\n   isinstance(trg_module, AliasNode) and\n   trg_module.identifier == src_module_name):\n    raise ValueError(\n        'Target module \"%s\" already imported as \"%s\".' % (\n            trg_module_name, trg_module))\n\n# See findNode() for details.\nself.lazynodes[trg_module_name] = Alias(src_module_name)", "path": "pyinstaller/PyInstaller/lib/modulegraph/modulegraph.py", "commit_date": "2020-12-19 00:00:00", "repo_name": "pyinstaller/pyinstaller", "stars": 11133, "license": "other", "language": "python", "size": 61798}
{"docstring": "\"\"\" Convert code object to a .pyc pseudo-file \"\"\"\n", "func_signal": "def _code_to_file(co):\n", "code": "if sys.version_info >= (3, 7):\n    header = imp.get_magic() + (b'\\0' * 12)\nelif sys.version_info >= (3, 4):\n    header = imp.get_magic() + (b'\\0' * 8)\nelse:\n    header = imp.get_magic() + (b'\\0' * 4)\nreturn BytesIO(header + marshal.dumps(co))", "path": "pyinstaller/PyInstaller/lib/modulegraph/modulegraph.py", "commit_date": "2020-12-19 00:00:00", "repo_name": "pyinstaller/pyinstaller", "stars": 11133, "license": "other", "language": "python", "size": 61798}
{"docstring": "'''\nTest the importability of the `gi.repository` subpackage with the passed\nname installed with PyGObject (e.g., `GLib`, corresponding to the\n`gi.repository.GLib` subpackage). Version '1.0' are for PyGObject >=1.0,\n'2.0' for PyGObject >= 2.0 and some other libraries have strange version\n(eg. Champlain)\n'''\n\n# Test the importability of this subpackage.\n", "func_signal": "def test_gi_repository(pyi_builder, repository_name, version):\n", "code": "pyi_builder.test_source('''\n    import gi\n    gi.require_version('{repository_name}', '{version}')\n    from gi.repository import {repository_name}\n    print({repository_name})\n    '''.format(repository_name=repository_name, version=version))", "path": "pyinstaller/tests/functional/test_hooks/test_gi.py", "commit_date": "2020-01-08 00:00:00", "repo_name": "pyinstaller/pyinstaller", "stars": 11133, "license": "other", "language": "python", "size": 61798}
{"docstring": "\"\"\"\nReturns True if rebuild/assemble is required\n\"\"\"\n", "func_signal": "def _check_guts(self, data, last_build):\n", "code": "if len(data) != len(self._GUTS):\n    logger.info(\"Building because %s is bad\", self.tocbasename)\n    return True\nfor attr, func in self._GUTS:\n    if func is None:\n        # no check for this value\n        continue\n    if func(attr, data[attr], getattr(self, attr), last_build):\n        return True\nreturn False", "path": "pyinstaller/PyInstaller/building/datastruct.py", "commit_date": "2020-01-08 00:00:00", "repo_name": "pyinstaller/pyinstaller", "stars": 11133, "license": "other", "language": "python", "size": 61798}
{"docstring": "\"\"\"\nAdd the passed module node to the graph if not already added.\n\nIf that module has a parent module or package with a previously added\nnode, this method also adds a reference from this module node to its\nparent node and adds this module node to its parent node's namespace.\n\nThis high-level method wraps the low-level `addNode()` method, but is\ntypically _only_ called by graph hooks adding runtime module nodes. For\nall other node types, the `import_module()` method should be called.\n\nParameters\n----------\nmodule : BaseModule\n    Graph node of the module to be added.\n\"\"\"\n", "func_signal": "def add_module(self, module):\n", "code": "self.msg(3, 'add_module', module)\n\n# If no node exists for this module, add such a node.\nmodule_added = self.findNode(module.identifier)\nif module_added is None:\n    self.addNode(module)\nelse:\n    assert module == module_added, 'New module %r != previous %r.' % (module, module_added)\n\n# If this module has a previously added parent, reference this module to\n# its parent and add this module to its parent's namespace.\nparent_name, _, module_basename = module.identifier.rpartition('.')\nif parent_name:\n    parent = self.findNode(parent_name)\n    if parent is None:\n        self.msg(4, 'add_module parent not found:', parent_name)\n    else:\n        self.createReference(module, parent)\n        parent.add_submodule(module_basename, module)", "path": "pyinstaller/PyInstaller/lib/modulegraph/modulegraph.py", "commit_date": "2020-12-19 00:00:00", "repo_name": "pyinstaller/pyinstaller", "stars": 11133, "license": "other", "language": "python", "size": 61798}
{"docstring": "\"\"\"\nImport the submodule with the passed name and all parent packages of\nthis module from the previously imported parent package corresponding\nto the passed graph node.\n\nParameters\n----------\npackage : Package\n    Graph node of the previously imported package containing this\n    submodule.\nsubmodule_name : str\n    Name of the submodule to be imported in either qualified (e.g.,\n    `email.mime.base`) or unqualified (e.g., `base`) form.\n\nReturns\n----------\nNode\n    Graph node created for this submodule.\n\nRaises\n----------\nImportError\n    If this submodule is unimportable.\n\"\"\"\n", "func_signal": "def _load_tail(self, package, submodule_name):\n", "code": "self.msgin(4, \"load_tail\", package, submodule_name)\n\nsubmodule = package\nwhile submodule_name:\n    i = submodule_name.find('.')\n    if i < 0:\n        i = len(submodule_name)\n    head, submodule_name = submodule_name[:i], submodule_name[i+1:]\n    mname = \"%s.%s\" % (submodule.identifier, head)\n    submodule = self._safe_import_module(head, mname, submodule)\n\n    if submodule is None:\n        # FIXME: Why do we no longer return a MissingModule instance?\n        # result = self.createNode(MissingModule, mname)\n        self.msgout(4, \"raise ImportError: No module named\", mname)\n        raise ImportError(\"No module named \" + repr(mname))\n\nself.msgout(4, \"load_tail ->\", submodule)\nreturn submodule", "path": "pyinstaller/PyInstaller/lib/modulegraph/modulegraph.py", "commit_date": "2020-12-19 00:00:00", "repo_name": "pyinstaller/pyinstaller", "stars": 11133, "license": "other", "language": "python", "size": 61798}
{"docstring": "\"\"\"\n3-tuple describing the physical location of the module with the passed\nname if this module is physically findable _or_ raise `ImportError`.\n\nThis high-level method wraps the low-level `modulegraph.find_module()`\nfunction with additional support for graph-based module caching.\n\nParameters\n----------\nname : str\n    Fully-qualified name of the Python module to be found.\npath : list\n    List of the absolute paths of all directories to search for this\n    module _or_ `None` if the default path list `self.path` is to be\n    searched.\nparent : Node\n    Package containing this module if this module is a submodule of a\n    package _or_ `None` if this is a top-level module.\n\nReturns\n----------\n(filename, loader)\n    See `modulegraph._find_module()` for details.\n\nRaises\n----------\nImportError\n    If this module is _not_ found.\n\"\"\"\n\n", "func_signal": "def _find_module(self, name, path, parent=None):\n", "code": "if parent is not None:\n    # assert path is not None\n    fullname = parent.identifier + '.' + name\nelse:\n    fullname = name\n\nnode = self.findNode(fullname)\nif node is not None:\n    self.msg(3, \"find_module: already included?\", node)\n    raise ImportError(name)\n\nif path is None:\n    if name in sys.builtin_module_names:\n        return (None, BUILTIN_MODULE)\n\n    path = self.path\n\nreturn self._find_module_path(fullname, name, path)", "path": "pyinstaller/PyInstaller/lib/modulegraph/modulegraph.py", "commit_date": "2020-12-19 00:00:00", "repo_name": "pyinstaller/pyinstaller", "stars": 11133, "license": "other", "language": "python", "size": 61798}
{"docstring": "\"\"\"\nGraph node uniquely identified by the passed fully-qualified module\nname if this module has been added to the graph _or_ `None` otherwise.\n\nIf (in order):\n\n. A namespace package with this identifier exists _and_ the passed\n  `create_nspkg` parameter is `True`, this package will be\n  instantiated and returned.\n. A lazy node with this identifier and:\n  * No dependencies exists, this node will be instantiated and\n    returned.\n  * Dependencies exists, this node and all transitive dependencies of\n    this node be instantiated and this node returned.\n. A non-lazy node with this identifier exists, this node will be\n  returned as is.\n\nParameters\n----------\nname : str\n    Fully-qualified name of the module whose graph node is to be found.\ncreate_nspkg : bool\n    Whether or not to implicitly instantiate namespace packages. If\n    `True` _and_ this name is that of a previously registered namespace\n    package (i.e., in `self.nspackages`) not already added to the\n    graph, this package will be added to the graph. Defaults to `True`.\n\nReturns\n----------\nNode\n    Graph node of this module if added to the graph _or_ `None`\n    otherwise.\n\"\"\"\n\n", "func_signal": "def findNode(self, name, create_nspkg=True):\n", "code": "data = super(ModuleGraph, self).findNode(name)\n\nif data is not None:\n    return data\n\nif name in self.lazynodes:\n    deps = self.lazynodes.pop(name)\n\n    if deps is None:\n        # excluded module\n        m = self.createNode(ExcludedModule, name)\n    elif isinstance(deps, Alias):\n        other = self._safe_import_hook(deps, None, None).pop()\n        m = self.createNode(AliasNode, name, other)\n        self.implyNodeReference(m, other)\n    else:\n        m = self._safe_import_hook(name, None, None).pop()\n        for dep in deps:\n            self.implyNodeReference(m, dep)\n\n    return m\n\nif name in self.nspackages and create_nspkg:\n    # name is a --single-version-externally-managed\n    # namespace package (setuptools/distribute)\n    pathnames = self.nspackages.pop(name)\n    m = self.createNode(NamespacePackage, name)\n\n    # FIXME: The filename must be set to a string to ensure that py2app\n    # works, it is not clear yet why that is. Setting to None would be\n    # cleaner.\n    m.filename = '-'\n    m.packagepath = _namespace_package_path(name, pathnames, self.path)\n\n    # As per comment at top of file, simulate runtime packagepath additions.\n    m.packagepath = m.packagepath + self._package_path_map.get(name, [])\n    return m\n\nreturn None", "path": "pyinstaller/PyInstaller/lib/modulegraph/modulegraph.py", "commit_date": "2020-12-19 00:00:00", "repo_name": "pyinstaller/pyinstaller", "stars": 11133, "license": "other", "language": "python", "size": 61798}
{"docstring": "\"\"\"\nInput is the repr of a tuple of strings, output\nis that tuple.\n\nThis only works with a tuple where the members are\npython identifiers.\n\"\"\"\n", "func_signal": "def _eval_str_tuple(value):\n", "code": "if not (value.startswith('(') and value.endswith(')')):\n    raise ValueError(value)\n\norig_value = value\nvalue = value[1:-1]\n\nresult = []\nwhile value:\n    m = _strs.match(value)\n    if m is None:\n        raise ValueError(orig_value)\n\n    result.append(m.group(1))\n    value = value[len(m.group(0)):]\n\nreturn tuple(result)", "path": "pyinstaller/PyInstaller/lib/modulegraph/modulegraph.py", "commit_date": "2020-12-19 00:00:00", "repo_name": "pyinstaller/pyinstaller", "stars": 11133, "license": "other", "language": "python", "size": 61798}
{"docstring": "\"\"\" Return True iff there is an edge from 'fromnode' to 'tonode' \"\"\"\n", "func_signal": "def hasEdge(self, fromnode, tonode):\n", "code": "fromnode = self.findNode(fromnode)\ntonode = self.findNode(tonode)\n\nreturn self.graph.edge_by_node(fromnode, tonode) is not None", "path": "pyinstaller/PyInstaller/lib/modulegraph/modulegraph.py", "commit_date": "2020-12-19 00:00:00", "repo_name": "pyinstaller/pyinstaller", "stars": 11133, "license": "other", "language": "python", "size": 61798}
{"docstring": "\"\"\"\n:return: XCUIElement horizontal size class\n:rtype: lldb.SBValue\n\"\"\"\n", "func_signal": "def horizontal_size_class(self):\n", "code": "if self._horizontalSizeClass is None:\n    name = \"_horizontalSizeClass\"\n    if self.element.GetIndexOfChildWithName(name) == NOT_FOUND:\n        self._horizontalSizeClass = fb.evaluateExpressionValue(\n            \"(int)[{} horizontalSizeClass]\".format(self.element_value)\n        )\n    else:\n        self._horizontalSizeClass = self.element.GetChildMemberWithName(name)\nreturn self._horizontalSizeClass", "path": "chisel/commands/FBXCTestCommands.py", "commit_date": "2020-11-11 00:00:00", "repo_name": "facebook/chisel", "stars": 9075, "license": "mit", "language": "python", "size": 3545}
{"docstring": "\"\"\"\n:return: XCUIElement children\n:rtype: lldb.SBValue\n\"\"\"\n", "func_signal": "def children(self):\n", "code": "if self._children is None:\n    name = \"_children\"\n    if self.element.GetIndexOfChildWithName(name) == NOT_FOUND:\n        self._children = fb.evaluateExpressionValue(\n            \"(NSArray *)[{} children]\".format(self.element_value)\n        )\n    else:\n        self._children = self.element.GetChildMemberWithName(name)\nreturn self._children", "path": "chisel/commands/FBXCTestCommands.py", "commit_date": "2020-11-11 00:00:00", "repo_name": "facebook/chisel", "stars": 9075, "license": "mit", "language": "python", "size": 3545}
{"docstring": "\"\"\"\n:return: XCUIElement identifier\n:rtype: lldb.SBValue\n\"\"\"\n", "func_signal": "def identifier(self):\n", "code": "if self._identifier is None:\n    name = \"_identifier\"\n    if self.element.GetIndexOfChildWithName(name) == NOT_FOUND:\n        self._identifier = fb.evaluateExpressionValue(\n            \"(NSString *)[{} identifier]\".format(self.element_value)\n        )\n    else:\n        self._identifier = self.element.GetChildMemberWithName(name)\nreturn self._identifier", "path": "chisel/commands/FBXCTestCommands.py", "commit_date": "2020-11-11 00:00:00", "repo_name": "facebook/chisel", "stars": 9075, "license": "mit", "language": "python", "size": 3545}
{"docstring": "\"\"\"\n:return: XCUIElement label summary\n:rtype: str | None\n\"\"\"\n", "func_signal": "def label_summary(self):\n", "code": "if len(self.label_value) == 0:\n    return None\nreturn \"label: '{}'\".format(self.label_value)", "path": "chisel/commands/FBXCTestCommands.py", "commit_date": "2020-11-11 00:00:00", "repo_name": "facebook/chisel", "stars": 9075, "license": "mit", "language": "python", "size": 3545}
{"docstring": "\"\"\"\nImport UIKit framework to the debugger\n\"\"\"\n", "func_signal": "def import_uikit():\n", "code": "global _uikit_imported\nif _uikit_imported:\n    return\n_uikit_imported = True\nfb.evaluateExpressionValue(\"@import UIKit\")", "path": "chisel/commands/FBXCTestCommands.py", "commit_date": "2020-11-11 00:00:00", "repo_name": "facebook/chisel", "stars": 9075, "license": "mit", "language": "python", "size": 3545}
{"docstring": "\"\"\"\n:return: XCUIElement suggested hit points\n:rtype: lldb.SBValue\n\"\"\"\n", "func_signal": "def suggested_hit_points(self):\n", "code": "return fb.evaluateExpressionValue(\n    \"(NSArray *)[{} suggestedHitpoints]\".format(self.element_value)\n)", "path": "chisel/commands/FBXCTestCommands.py", "commit_date": "2020-11-11 00:00:00", "repo_name": "facebook/chisel", "stars": 9075, "license": "mit", "language": "python", "size": 3545}
{"docstring": "\"\"\"\n:return: XCUIElement uniquely identifying Objective-C code value\n:rtype: str\n\"\"\"\n", "func_signal": "def uniquely_identifying_objective_c_code_value(self):\n", "code": "return normalize_array_description(\n    self.uniquely_identifying_objective_c_code.GetObjectDescription()\n)", "path": "chisel/commands/FBXCTestCommands.py", "commit_date": "2020-11-11 00:00:00", "repo_name": "facebook/chisel", "stars": 9075, "license": "mit", "language": "python", "size": 3545}
{"docstring": "\"\"\"\n:return: XCUIElement uniquely identifying Swift code value\n:rtype: str\n\"\"\"\n", "func_signal": "def uniquely_identifying_swift_code_value(self):\n", "code": "return normalize_array_description(\n    self.uniquely_identifying_swift_code.GetObjectDescription()\n)", "path": "chisel/commands/FBXCTestCommands.py", "commit_date": "2020-11-11 00:00:00", "repo_name": "facebook/chisel", "stars": 9075, "license": "mit", "language": "python", "size": 3545}
{"docstring": "\"\"\"\n:return: XCUIElement is selected\n:rtype: lldb.SBValue\n\"\"\"\n", "func_signal": "def selected(self):\n", "code": "if self._selected is None:\n    name = \"_selected\"\n    if self.element.GetIndexOfChildWithName(name) == NOT_FOUND:\n        self._selected = fb.evaluateExpressionValue(\n            \"(BOOL)[{} selected]\".format(self.element_value)\n        )\n    else:\n        self._selected = self.element.GetChildMemberWithName(name)\nreturn self._selected", "path": "chisel/commands/FBXCTestCommands.py", "commit_date": "2020-11-11 00:00:00", "repo_name": "facebook/chisel", "stars": 9075, "license": "mit", "language": "python", "size": 3545}
{"docstring": "\"\"\"\n:return: XCUIElement isMainWindow summary\n:rtype: str | None\n\"\"\"\n", "func_signal": "def is_main_window_summary(self):\n", "code": "if self.is_main_window_value:\n    return \"MainWindow\"\nreturn None", "path": "chisel/commands/FBXCTestCommands.py", "commit_date": "2020-11-11 00:00:00", "repo_name": "facebook/chisel", "stars": 9075, "license": "mit", "language": "python", "size": 3545}
{"docstring": "\"\"\"\n:param lldb.SBValue element: CGRect object\n\"\"\"\n", "func_signal": "def __init__(self, element):\n", "code": "super(CGRect, self).__init__()\n\nself.element = element", "path": "chisel/commands/FBXCTestCommands.py", "commit_date": "2020-11-11 00:00:00", "repo_name": "facebook/chisel", "stars": 9075, "license": "mit", "language": "python", "size": 3545}
{"docstring": "\"\"\"\nGet element traits string from UIAccessibilityTraits (as int)\n\n:param int value: UIAccessibilityTraits (as int)\n:return: UIAccessibilityTraits string\n:rtype: str\n\"\"\"\n", "func_signal": "def name_for_value(cls, value):\n", "code": "if value == 0:\n    return \"None\"\n\ntraits = []\nattributes = cls._attributes_by_value()\nfor k in attributes.keys():\n    if value & k:\n        traits.append(attributes[k])\n\nif len(traits) == 0:\n    return \"Unknown\"\nelse:\n    return \", \".join(traits)", "path": "chisel/commands/FBXCTestCommands.py", "commit_date": "2020-11-11 00:00:00", "repo_name": "facebook/chisel", "stars": 9075, "license": "mit", "language": "python", "size": 3545}
{"docstring": "\"\"\"\n:return: XCUIElement title\n:rtype: lldb.SBValue\n\"\"\"\n", "func_signal": "def title(self):\n", "code": "if self._title is None:\n    name = \"_title\"\n    if self.element.GetIndexOfChildWithName(name) == NOT_FOUND:\n        self._title = fb.evaluateExpressionValue(\n            \"(NSString *)[{} title]\".format(self.element_value)\n        )\n    else:\n        self._title = self.element.GetChildMemberWithName(name)\nreturn self._title", "path": "chisel/commands/FBXCTestCommands.py", "commit_date": "2020-11-11 00:00:00", "repo_name": "facebook/chisel", "stars": 9075, "license": "mit", "language": "python", "size": 3545}
{"docstring": "\"\"\"\nGet user interface size class string from UIUserInterfaceSizeClass (as int)\n\n:param int value: UIAccessibilityTraits (as int)\n:return: UIUserInterfaceSizeClass string\n:rtype: str\n\"\"\"\n", "func_signal": "def name_for_value(cls, value):\n", "code": "if value == cls.Unspecified:\n    return \"Unspecified\"\nelif value == cls.Compact:\n    return \"Compact\"\nelif value == cls.Regular:\n    return \"Regular\"\nelse:\n    return \"Unknown ({:#x})\".format(value)", "path": "chisel/commands/FBXCTestCommands.py", "commit_date": "2020-11-11 00:00:00", "repo_name": "facebook/chisel", "stars": 9075, "license": "mit", "language": "python", "size": 3545}
{"docstring": "\"\"\"\n:return: Hash of all attributes and their values\n:rtype: dict[int, str]\n\"\"\"\n", "func_signal": "def _attributes_by_value(cls):\n", "code": "class_attributes = set(dir(cls)) - set(dir(object))\nreturn dict(\n    [\n        (getattr(cls, n), n)\n        for n in class_attributes\n        if not callable(getattr(cls, n)) and not n.startswith(\"__\")\n    ]\n)", "path": "chisel/commands/FBXCTestCommands.py", "commit_date": "2020-11-11 00:00:00", "repo_name": "facebook/chisel", "stars": 9075, "license": "mit", "language": "python", "size": 3545}
{"docstring": "\"\"\"\n:return: XCUIElement hit point for scrolling\n:rtype: lldb.SBValue\n\"\"\"\n", "func_signal": "def hit_point_for_scrolling(self):\n", "code": "import_uikit()\nreturn fb.evaluateExpressionValue(\n    \"(CGPoint)[{} hitPointForScrolling]\".format(self.element_value)\n)", "path": "chisel/commands/FBXCTestCommands.py", "commit_date": "2020-11-11 00:00:00", "repo_name": "facebook/chisel", "stars": 9075, "license": "mit", "language": "python", "size": 3545}
{"docstring": "\"\"\"\n:return: XCUIElement vertical size class\n:rtype: lldb.SBValue\n\"\"\"\n", "func_signal": "def vertical_size_class(self):\n", "code": "if self._verticalSizeClass is None:\n    name = \"_verticalSizeClass\"\n    if self.element.GetIndexOfChildWithName(name) == NOT_FOUND:\n        self._verticalSizeClass = fb.evaluateExpressionValue(\n            \"(int)[{} verticalSizeClass]\".format(self.element_value)\n        )\n    else:\n        self._verticalSizeClass = self.element.GetChildMemberWithName(name)\nreturn self._verticalSizeClass", "path": "chisel/commands/FBXCTestCommands.py", "commit_date": "2020-11-11 00:00:00", "repo_name": "facebook/chisel", "stars": 9075, "license": "mit", "language": "python", "size": 3545}
{"docstring": "\"\"\"\n:return: XCUIElement uniquely identifying Swift code\n:rtype: lldb.SBValue\n\"\"\"\n", "func_signal": "def uniquely_identifying_swift_code(self):\n", "code": "return fb.evaluateExpressionValue(\n    \"(id)[{} _uniquelyIdentifyingSwiftCode]\".format(self.element_value)\n)", "path": "chisel/commands/FBXCTestCommands.py", "commit_date": "2020-11-11 00:00:00", "repo_name": "facebook/chisel", "stars": 9075, "license": "mit", "language": "python", "size": 3545}
{"docstring": "\"\"\"\n:return: XCUIElement identifier summary\n:rtype: str | None\n\"\"\"\n", "func_signal": "def identifier_summary(self):\n", "code": "if len(self.identifier_value) == 0:\n    return None\nreturn \"identifier: '{}'\".format(self.identifier_value)", "path": "chisel/commands/FBXCTestCommands.py", "commit_date": "2020-11-11 00:00:00", "repo_name": "facebook/chisel", "stars": 9075, "license": "mit", "language": "python", "size": 3545}
{"docstring": "\"\"\"\n:return: XCUIElement uniquely identifying Objective-C code\n:rtype: lldb.SBValue\n\"\"\"\n", "func_signal": "def uniquely_identifying_objective_c_code(self):\n", "code": "return fb.evaluateExpressionValue(\n    \"(id)[{} _uniquelyIdentifyingObjectiveCCode]\".format(self.element_value)\n)", "path": "chisel/commands/FBXCTestCommands.py", "commit_date": "2020-11-11 00:00:00", "repo_name": "facebook/chisel", "stars": 9075, "license": "mit", "language": "python", "size": 3545}
{"docstring": "\"\"\"\nHelper method to both parse and validate phone and its hash.\n\"\"\"\n", "func_signal": "def _parse_phone_and_hash(self, phone, phone_hash):\n", "code": "phone = utils.parse_phone(phone) or self._phone\nif not phone:\n    raise ValueError(\n        'Please make sure to call send_code_request first.'\n    )\n\nphone_hash = phone_hash or self._phone_code_hash.get(phone, None)\nif not phone_hash:\n    raise ValueError('You also need to provide a phone_code_hash.')\n\nreturn phone, phone_hash", "path": "Telethon/telethon/client/auth.py", "commit_date": "2020-10-18 00:00:00", "repo_name": "LonamiWebs/Telethon", "stars": 8822, "license": "mit", "language": "python", "size": 8978}
{"docstring": "\"\"\"\nGet the relative path for the given path from the current\nfile by working around https://bugs.python.org/issue20012.\n\"\"\"\n", "func_signal": "def _rel(self, path):\n", "code": "return os.path.relpath(\n    str(path), self._parent).replace(os.path.sep, '/')", "path": "Telethon/telethon_generator/docswriter.py", "commit_date": "2019-07-17 00:00:00", "repo_name": "LonamiWebs/Telethon", "stars": 8822, "license": "mit", "language": "python", "size": 8978}
{"docstring": "\"\"\"\nHelps to cut boilerplate on async context\nmanagers that offer synchronous variants.\n\"\"\"\n", "func_signal": "def _sync_enter(self):\n", "code": "if hasattr(self, 'loop'):\n    loop = self.loop\nelse:\n    loop = self._client.loop\n\nif loop.is_running():\n    raise RuntimeError(\n        'You must use \"async with\" if the event loop '\n        'is running (i.e. you are inside an \"async def\")'\n    )\n\nreturn loop.run_until_complete(self.__aenter__())", "path": "Telethon/telethon/helpers.py", "commit_date": "2020-09-14 00:00:00", "repo_name": "LonamiWebs/Telethon", "stars": 8822, "license": "mit", "language": "python", "size": 8978}
{"docstring": "\"\"\"\nStrips whitespace from the given text modifying the provided entities.\n\nThis assumes that there are no overlapping entities, that their length\nis greater or equal to one, and that their length is not out of bounds.\n\"\"\"\n", "func_signal": "def strip_text(text, entities):\n", "code": "if not entities:\n    return text.strip()\n\nwhile text and text[-1].isspace():\n    e = entities[-1]\n    if e.offset + e.length == len(text):\n        if e.length == 1:\n            del entities[-1]\n            if not entities:\n                return text.strip()\n        else:\n            e.length -= 1\n    text = text[:-1]\n\nwhile text and text[0].isspace():\n    for i in reversed(range(len(entities))):\n        e = entities[i]\n        if e.offset != 0:\n            e.offset -= 1\n            continue\n\n        if e.length == 1:\n            del entities[0]\n            if not entities:\n                return text.lstrip()\n        else:\n            e.length -= 1\n\n    text = text[1:]\n\nreturn text", "path": "Telethon/telethon/helpers.py", "commit_date": "2020-09-14 00:00:00", "repo_name": "LonamiWebs/Telethon", "stars": 8822, "license": "mit", "language": "python", "size": 8978}
{"docstring": "\"\"\"Writes a title header in the document body,\n   with an optional depth level\n\"\"\"\n", "func_signal": "def write_title(self, title, level=1, id=None):\n", "code": "if id:\n    self.write('<h{lv} id=\"{id}\">{title}</h{lv}>',\n               title=title, lv=level, id=id)\nelse:\n    self.write('<h{lv}>{title}</h{lv}>',\n               title=title, lv=level)", "path": "Telethon/telethon_generator/docswriter.py", "commit_date": "2019-07-17 00:00:00", "repo_name": "LonamiWebs/Telethon", "stars": 8822, "license": "mit", "language": "python", "size": 8978}
{"docstring": "\"\"\"Writes a button with 'text' which can be used\n   to copy 'text_to_copy' to clipboard when it's clicked.\"\"\"\n", "func_signal": "def write_copy_button(self, text, text_to_copy):\n", "code": "self.write_copy_script = True\nself.write('<button onclick=\"cp(\\'{}\\');\">{}</button>'\n           .format(text_to_copy, text))", "path": "Telethon/telethon_generator/docswriter.py", "commit_date": "2019-07-17 00:00:00", "repo_name": "LonamiWebs/Telethon", "stars": 8822, "license": "mit", "language": "python", "size": 8978}
{"docstring": "\"\"\"Ends the whole document. This should be called the last\"\"\"\n", "func_signal": "def end_body(self):\n", "code": "if self.write_copy_script:\n    self.write(\n        '<textarea id=\"c\" class=\"invisible\"></textarea>'\n        '<script>'\n        'function cp(t){'\n        'var c=document.getElementById(\"c\");'\n        'c.value=t;'\n        'c.select();'\n        'try{document.execCommand(\"copy\")}'\n        'catch(e){}}'\n        '</script>'\n    )\n\nself.write('</div>{}</body></html>', self._script)", "path": "Telethon/telethon_generator/docswriter.py", "commit_date": "2019-07-17 00:00:00", "repo_name": "LonamiWebs/Telethon", "stars": 8822, "license": "mit", "language": "python", "size": 8978}
{"docstring": "\"\"\"Returns the arguments properly sorted and ready to plug-in\n   into a Python's method header (i.e., flags and those which\n   can be inferred will go last so they can default =None)\n\"\"\"\n", "func_signal": "def sorted_args(self):\n", "code": "return sorted(self.args,\n              key=lambda x: x.is_flag or x.can_be_inferred)", "path": "Telethon/telethon_generator/parsers/tlobject/tlobject.py", "commit_date": "2019-07-15 00:00:00", "repo_name": "LonamiWebs/Telethon", "stars": 8822, "license": "mit", "language": "python", "size": 8978}
{"docstring": "# Sanity check\n", "func_signal": "def __enter__(self):\n", "code": "self.filename.parent.mkdir(parents=True, exist_ok=True)\nself.handle = self.filename.open('w', encoding='utf-8')\nreturn self", "path": "Telethon/telethon_generator/docswriter.py", "commit_date": "2019-07-17 00:00:00", "repo_name": "LonamiWebs/Telethon", "stars": 8822, "license": "mit", "language": "python", "size": 8978}
{"docstring": "\"\"\"Writes the code for the given 'tlobject' properly\n   formatted with hyperlinks\n\"\"\"\n", "func_signal": "def write_code(self, tlobject):\n", "code": "self.write('<pre>---{}---\\n',\n           'functions' if tlobject.is_function else 'types')\n\n# Write the function or type and its ID\nif tlobject.namespace:\n    self.write(tlobject.namespace)\n    self.write('.')\n\nself.write('{}#{:08x}', tlobject.name, tlobject.id)\n\n# Write all the arguments (or do nothing if there's none)\nfor arg in tlobject.args:\n    self.write(' ')\n    add_link = not arg.generic_definition and not arg.is_generic\n\n    # \"Opening\" modifiers\n    if arg.generic_definition:\n        self.write('{')\n\n    # Argument name\n    self.write(arg.name)\n    self.write(':')\n\n    # \"Opening\" modifiers\n    if arg.is_flag:\n        self.write('flags.{}?', arg.flag_index)\n\n    if arg.is_generic:\n        self.write('!')\n\n    if arg.is_vector:\n        self.write('<a href=\"{}\">Vector</a>&lt;',\n                   self.type_to_path('vector'))\n\n    # Argument type\n    if arg.type:\n        if add_link:\n            self.write('<a href=\"{}\">', self.type_to_path(arg.type))\n        self.write(arg.type)\n        if add_link:\n            self.write('</a>')\n    else:\n        self.write('#')\n\n    # \"Closing\" modifiers\n    if arg.is_vector:\n        self.write('&gt;')\n\n    if arg.generic_definition:\n        self.write('}')\n\n# Now write the resulting type (result from a function/type)\nself.write(' = ')\ngeneric_name = next((arg.name for arg in tlobject.args\n                     if arg.generic_definition), None)\n\nif tlobject.result == generic_name:\n    # Generic results cannot have any link\n    self.write(tlobject.result)\nelse:\n    if re.search('^vector<', tlobject.result, re.IGNORECASE):\n        # Notice that we don't simply make up the \"Vector\" part,\n        # because some requests (as of now, only FutureSalts),\n        # use a lower type name for it (see #81)\n        vector, inner = tlobject.result.split('<')\n        inner = inner.strip('>')\n        self.write('<a href=\"{}\">{}</a>&lt;',\n                   self.type_to_path(vector), vector)\n\n        self.write('<a href=\"{}\">{}</a>&gt;',\n                   self.type_to_path(inner), inner)\n    else:\n        self.write('<a href=\"{}\">{}</a>',\n                   self.type_to_path(tlobject.result), tlobject.result)\n\nself.write('</pre>')", "path": "Telethon/telethon_generator/docswriter.py", "commit_date": "2019-07-17 00:00:00", "repo_name": "LonamiWebs/Telethon", "stars": 8822, "license": "mit", "language": "python", "size": 8978}
{"docstring": "\"\"\"\nCallback called whenever the login or sign up process completes.\n\nReturns the input user parameter.\n\"\"\"\n", "func_signal": "def _on_login(self, user):\n", "code": "self._bot = bool(user.bot)\nself._self_input_peer = utils.get_input_peer(user, allow_self=False)\nself._authorized = True\n\nreturn user", "path": "Telethon/telethon/client/auth.py", "commit_date": "2020-10-18 00:00:00", "repo_name": "LonamiWebs/Telethon", "stars": 8822, "license": "mit", "language": "python", "size": 8978}
{"docstring": "# If there was any column left, finish it before closing the table\n", "func_signal": "def end_table(self):\n", "code": "if self.table_columns_left:\n    self.write('</tr>')\n\nself.write('</table>')", "path": "Telethon/telethon_generator/docswriter.py", "commit_date": "2019-07-17 00:00:00", "repo_name": "LonamiWebs/Telethon", "stars": 8822, "license": "mit", "language": "python", "size": 8978}
{"docstring": "\"\"\"Generates the key data corresponding to the given nonce\"\"\"\n", "func_signal": "def generate_key_data_from_nonce(server_nonce, new_nonce):\n", "code": "server_nonce = server_nonce.to_bytes(16, 'little', signed=True)\nnew_nonce = new_nonce.to_bytes(32, 'little', signed=True)\nhash1 = sha1(new_nonce + server_nonce).digest()\nhash2 = sha1(server_nonce + new_nonce).digest()\nhash3 = sha1(new_nonce + new_nonce).digest()\n\nkey = hash1 + hash2[:12]\niv = hash2[12:20] + hash3 + new_nonce[:4]\nreturn key, iv", "path": "Telethon/telethon/helpers.py", "commit_date": "2020-09-14 00:00:00", "repo_name": "LonamiWebs/Telethon", "stars": 8822, "license": "mit", "language": "python", "size": 8978}
{"docstring": "\"\"\"This will create a new row, or add text to the next column\n   of the previously created, incomplete row, closing it if complete\"\"\"\n", "func_signal": "def add_row(self, text, link=None, bold=False, align=None):\n", "code": "if not self.table_columns_left:\n    # Starting a new row\n    self.write('<tr>')\n    self.table_columns_left = self.table_columns\n\nself.write('<td')\nif align:\n    self.write(' style=\"text-align:{}\"', align)\nself.write('>')\n\nif bold:\n    self.write('<b>')\nif link:\n    self.write('<a href=\"{}\">', self._rel(link))\n\n# Finally write the real table data, the given text\nself.write(text)\n\nif link:\n    self.write('</a>')\nif bold:\n    self.write('</b>')\n\nself.write('</td>')\n\nself.table_columns_left -= 1\nif not self.table_columns_left:\n    self.write('</tr>')", "path": "Telethon/telethon_generator/docswriter.py", "commit_date": "2019-07-17 00:00:00", "repo_name": "LonamiWebs/Telethon", "stars": 8822, "license": "mit", "language": "python", "size": 8978}
{"docstring": "\"\"\"Adds a menu entry, will create it if it doesn't exist yet\"\"\"\n", "func_signal": "def add_menu(self, name, link=None):\n", "code": "if self.menu_began:\n    if self.menu_separator_tag:\n        self.write(self.menu_separator_tag)\nelse:\n    # First time, create the menu tag\n    self.write('<ul class=\"horizontal\">')\n    self.menu_began = True\n\nself.write('<li>')\nif link:\n    self.write('<a href=\"{}\">', self._rel(link))\n\n# Write the real menu entry text\nself.write(name)\n\nif link:\n    self.write('</a>')\nself.write('</li>')", "path": "Telethon/telethon_generator/docswriter.py", "commit_date": "2019-07-17 00:00:00", "repo_name": "LonamiWebs/Telethon", "stars": 8822, "license": "mit", "language": "python", "size": 8978}
{"docstring": "\"\"\"Ends an opened menu\"\"\"\n", "func_signal": "def end_menu(self):\n", "code": "if not self.menu_began:\n    raise RuntimeError('No menu had been started in the first place.')\nself.write('</ul>')", "path": "Telethon/telethon_generator/docswriter.py", "commit_date": "2019-07-17 00:00:00", "repo_name": "LonamiWebs/Telethon", "stars": 8822, "license": "mit", "language": "python", "size": 8978}
{"docstring": "\"\"\"Sets the menu separator.\n   Must be called before adding entries to the menu\n\"\"\"\n", "func_signal": "def set_menu_separator(self, img):\n", "code": "if img:\n    self.menu_separator_tag = '<img src=\"{}\" alt=\"/\" />'.format(\n        self._rel(img))\nelse:\n    self.menu_separator_tag = None", "path": "Telethon/telethon_generator/docswriter.py", "commit_date": "2019-07-17 00:00:00", "repo_name": "LonamiWebs/Telethon", "stars": 8822, "license": "mit", "language": "python", "size": 8978}
{"docstring": "\"\"\"Wrapper around handle.write\"\"\"\n", "func_signal": "def write(self, s, *args, **kwargs):\n", "code": "if args or kwargs:\n    self.handle.write(s.format(*args, **kwargs))\nelse:\n    self.handle.write(s)", "path": "Telethon/telethon_generator/docswriter.py", "commit_date": "2019-07-17 00:00:00", "repo_name": "LonamiWebs/Telethon", "stars": 8822, "license": "mit", "language": "python", "size": 8978}
{"docstring": "\"\"\"\nGenerates an integer sequence starting from 1. If `retries` is\nnot a zero or a positive integer value, the sequence will be\ninfinite, otherwise it will end at `retries + 1`.\n\"\"\"\n\n# We need at least one iteration even if the retries are 0\n# when force_retry is True.\n", "func_signal": "def retry_range(retries, force_retry=True):\n", "code": "if force_retry and not (retries is None or retries < 0):\n    retries += 1\n\nattempt = 0\nwhile attempt != retries:\n    attempt += 1\n    yield attempt", "path": "Telethon/telethon/helpers.py", "commit_date": "2020-09-14 00:00:00", "repo_name": "LonamiWebs/Telethon", "stars": 8822, "license": "mit", "language": "python", "size": 8978}
{"docstring": "# This could be a `utils` method that just ran a few `isinstance` on\n# `utils.get_peer(...)`'s result. However, there are *a lot* of auto\n# casts going on, plenty of calls and temporary short-lived objects.\n#\n# So we just check if a string is in the class name.\n# Still, assert that it's the right type to not return false results.\n", "func_signal": "def _entity_type(entity):\n", "code": "try:\n    if entity.SUBCLASS_OF_ID not in (\n            0x2d45687,  # crc32(b'Peer')\n            0xc91c90b6,  # crc32(b'InputPeer')\n            0xe669bf46,  # crc32(b'InputUser')\n            0x40f202fd,  # crc32(b'InputChannel')\n            0x2da17977,  # crc32(b'User')\n            0xc5af5d94,  # crc32(b'Chat')\n            0x1f4661b9,  # crc32(b'UserFull')\n            0xd49a2697,  # crc32(b'ChatFull')\n    ):\n        raise TypeError('{} does not have any entity type'.format(entity))\nexcept AttributeError:\n    raise TypeError('{} is not a TLObject, cannot determine entity type'.format(entity))\n\nname = entity.__class__.__name__\nif 'User' in name:\n    return _EntityType.USER\nelif 'Chat' in name:\n    return _EntityType.CHAT\nelif 'Channel' in name:\n    return _EntityType.CHANNEL\nelif 'Self' in name:\n    return _EntityType.USER\n\n# 'Empty' in name or not found, we don't care, not a valid entity.\nraise TypeError('{} does not have any entity type'.format(entity))", "path": "Telethon/telethon/helpers.py", "commit_date": "2020-09-14 00:00:00", "repo_name": "LonamiWebs/Telethon", "stars": 8822, "license": "mit", "language": "python", "size": 8978}
{"docstring": "# Should encode with quotetabs=True\n", "func_signal": "def test_quopri_stateless(self):\n", "code": "encoded = codecs.encode(b\"space tab\\teol \\n\", \"quopri-codec\")\nself.assertEqual(encoded, b\"space=20tab=09eol=20\\n\")\n# But should still support unescaped tabs and spaces\nunescaped = b\"space tab eol\\n\"\nself.assertEqual(codecs.decode(unescaped, \"quopri-codec\"), unescaped)", "path": "cpython/Lib/test/test_codecs.py", "commit_date": "2020-10-17 00:00:00", "repo_name": "python/cpython", "stars": 58706, "license": "other", "language": "python", "size": 557565}
{"docstring": "# Test input longer than INT_MAX.\n# Input should contain undecodable bytes before and after\n# the INT_MAX limit.\n", "func_signal": "def test_large_input(self, size):\n", "code": "encoded = (b'01234567' * ((size//8)-1) +\n           b'\\x85\\x86\\xea\\xeb\\xec\\xef\\xfc\\xfd\\xfe\\xff')\nself.assertEqual(len(encoded), size+2)\ndecoded = codecs.code_page_decode(932, encoded, 'surrogateescape', True)\nself.assertEqual(decoded[1], len(encoded))\ndel encoded\nself.assertEqual(len(decoded[0]), decoded[1])\nself.assertEqual(decoded[0][:10], '0123456701')\nself.assertEqual(decoded[0][-20:],\n                 '6701234567'\n                 '\\udc85\\udc86\\udcea\\udceb\\udcec'\n                 '\\udcef\\udcfc\\udcfd\\udcfe\\udcff')", "path": "cpython/Lib/test/test_codecs.py", "commit_date": "2020-10-17 00:00:00", "repo_name": "python/cpython", "stars": 58706, "license": "other", "language": "python", "size": 557565}
{"docstring": "# Issue #8941: insufficient result allocation when decoding into\n# surrogate pairs on UCS-2 builds.\n", "func_signal": "def test_issue8941(self):\n", "code": "encoded = b'\\x00\\x01\\x00\\x00' * 1024\nself.assertEqual('\\U00010000' * 1024,\n                 codecs.utf_32_be_decode(encoded)[0])", "path": "cpython/Lib/test/test_codecs.py", "commit_date": "2020-10-17 00:00:00", "repo_name": "python/cpython", "stars": 58706, "license": "other", "language": "python", "size": 557565}
{"docstring": "# We check all the transform codecs accept memoryview input\n# for encoding and decoding\n# and also that they roundtrip correctly\n", "func_signal": "def test_buffer_api_usage(self):\n", "code": "original = b\"12345\\x80\"\nfor encoding in bytes_transform_encodings:\n    with self.subTest(encoding=encoding):\n        data = original\n        view = memoryview(data)\n        data = codecs.encode(data, encoding)\n        view_encoded = codecs.encode(view, encoding)\n        self.assertEqual(view_encoded, data)\n        view = memoryview(data)\n        data = codecs.decode(data, encoding)\n        self.assertEqual(data, original)\n        view_decoded = codecs.decode(view, encoding)\n        self.assertEqual(view_decoded, data)", "path": "cpython/Lib/test/test_codecs.py", "commit_date": "2020-10-17 00:00:00", "repo_name": "python/cpython", "stars": 58706, "license": "other", "language": "python", "size": 557565}
{"docstring": "# The horizontal scrollbar should be shown/hidden according to\n# the 'wrap' setting: It should only be shown when 'wrap' is\n# set to NONE.\n\n# wrap = NONE -> with horizontal scrolling\n", "func_signal": "def test_horiz_scrollbar(self):\n", "code": "frame = self.make_frame(wrap=NONE)\nself.assertEqual(frame.text.cget('wrap'), NONE)\nself.assertIsNotNone(frame.xscroll)\n\n# wrap != NONE -> no horizontal scrolling\nfor wrap in [CHAR, WORD]:\n    with self.subTest(wrap=wrap):\n        frame = self.make_frame(wrap=wrap)\n        self.assertEqual(frame.text.cget('wrap'), wrap)\n        self.assertIsNone(frame.xscroll)", "path": "cpython/Lib/idlelib/idle_test/test_textview.py", "commit_date": "2019-07-27 00:00:00", "repo_name": "python/cpython", "stars": 58706, "license": "other", "language": "python", "size": 557565}
{"docstring": "# all codecs should be able to encode these\n", "func_signal": "def test_seek(self):\n", "code": "s = \"%s\\n%s\\n\" % (100*\"abc123\", 100*\"def456\")\nfor encoding in all_unicode_encodings:\n    if encoding == \"idna\": # FIXME: See SF bug #1163178\n        continue\n    if encoding in broken_unicode_with_stateful:\n        continue\n    reader = codecs.getreader(encoding)(io.BytesIO(s.encode(encoding)))\n    for t in range(5):\n        # Test that calling seek resets the internal codec state and buffers\n        reader.seek(0, 0)\n        data = reader.read()\n        self.assertEqual(s, data)", "path": "cpython/Lib/test/test_codecs.py", "commit_date": "2020-10-17 00:00:00", "repo_name": "python/cpython", "stars": 58706, "license": "other", "language": "python", "size": 557565}
{"docstring": "# Escape-decoding a unicode string is supported and gives the same\n# result as decoding the equivalent ASCII bytes string.\n", "func_signal": "def test_unicode_escape(self):\n", "code": "self.assertEqual(codecs.unicode_escape_decode(r\"\\u1234\"), (\"\\u1234\", 6))\nself.assertEqual(codecs.unicode_escape_decode(br\"\\u1234\"), (\"\\u1234\", 6))\nself.assertEqual(codecs.raw_unicode_escape_decode(r\"\\u1234\"), (\"\\u1234\", 6))\nself.assertEqual(codecs.raw_unicode_escape_decode(br\"\\u1234\"), (\"\\u1234\", 6))\n\nself.assertRaises(UnicodeDecodeError, codecs.unicode_escape_decode, br\"\\U00110000\")\nself.assertEqual(codecs.unicode_escape_decode(r\"\\U00110000\", \"replace\"), (\"\\ufffd\", 10))\nself.assertEqual(codecs.unicode_escape_decode(r\"\\U00110000\", \"backslashreplace\"),\n                 (r\"\\x5c\\x55\\x30\\x30\\x31\\x31\\x30\\x30\\x30\\x30\", 10))\n\nself.assertRaises(UnicodeDecodeError, codecs.raw_unicode_escape_decode, br\"\\U00110000\")\nself.assertEqual(codecs.raw_unicode_escape_decode(r\"\\U00110000\", \"replace\"), (\"\\ufffd\", 10))\nself.assertEqual(codecs.raw_unicode_escape_decode(r\"\\U00110000\", \"backslashreplace\"),\n                 (r\"\\x5c\\x55\\x30\\x30\\x31\\x31\\x30\\x30\\x30\\x30\", 10))", "path": "cpython/Lib/test/test_codecs.py", "commit_date": "2020-10-17 00:00:00", "repo_name": "python/cpython", "stars": 58706, "license": "other", "language": "python", "size": 557565}
{"docstring": "#[None in cache]\n", "func_signal": "def test_None_in_cache(self):\n", "code": "name = 'using_None'\nwith util.uncache(name):\n    sys.modules[name] = None\n    with self.assertRaises(ImportError) as cm:\n        self.__import__(name)\n    self.assertEqual(cm.exception.name, name)", "path": "cpython/Lib/test/test_importlib/import_/test_caching.py", "commit_date": "2020-12-04 00:00:00", "repo_name": "python/cpython", "stars": 58706, "license": "other", "language": "python", "size": 557565}
{"docstring": "# What does it mean to \"clear\" a document?  Does the\n# documentElement disappear?\n", "func_signal": "def abort(self):\n", "code": "raise NotImplementedError(\n    \"haven't figured out what this means yet\")", "path": "cpython/Lib/xml/dom/xmlbuilder.py", "commit_date": "2020-04-30 00:00:00", "repo_name": "python/cpython", "stars": 58706, "license": "other", "language": "python", "size": 557565}
{"docstring": "# Issue #1813: under Turkish locales, lookup of some codecs failed\n# because 'I' is lowercased as \"\u0131\" (dotless i)\n", "func_signal": "def test_lookup_issue1813(self):\n", "code": "oldlocale = locale.setlocale(locale.LC_CTYPE)\nself.addCleanup(locale.setlocale, locale.LC_CTYPE, oldlocale)\ntry:\n    locale.setlocale(locale.LC_CTYPE, 'tr_TR')\nexcept locale.Error:\n    # Unsupported locale on this system\n    self.skipTest('test needs Turkish locale')\nc = codecs.lookup('ASCII')\nself.assertEqual(c.name, 'ascii')", "path": "cpython/Lib/test/test_codecs.py", "commit_date": "2020-10-17 00:00:00", "repo_name": "python/cpython", "stars": 58706, "license": "other", "language": "python", "size": 557565}
{"docstring": "# Issue6373\n", "func_signal": "def test_latin1(self):\n", "code": "self.assertEqual(\"\\udce4\\udceb\\udcef\\udcf6\\udcfc\".encode(\"latin-1\", \"surrogateescape\"),\n                 b\"\\xe4\\xeb\\xef\\xf6\\xfc\")", "path": "cpython/Lib/test/test_codecs.py", "commit_date": "2020-10-17 00:00:00", "repo_name": "python/cpython", "stars": 58706, "license": "other", "language": "python", "size": 557565}
{"docstring": "# Check hex codec gives a good error for malformed input\n", "func_signal": "def test_custom_hex_error_is_wrapped(self):\n", "code": "msg = \"^decoding with 'hex_codec' codec failed\"\nwith self.assertRaisesRegex(Exception, msg) as failure:\n    codecs.decode(b\"hello\", \"hex_codec\")\nself.assertIsInstance(failure.exception.__cause__,\n                                        type(failure.exception))", "path": "cpython/Lib/test/test_codecs.py", "commit_date": "2020-10-17 00:00:00", "repo_name": "python/cpython", "stars": 58706, "license": "other", "language": "python", "size": 557565}
{"docstring": "# [from cache for fromlist]\n", "func_signal": "def test_using_cache_for_fromlist(self):\n", "code": "with warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\", ImportWarning)\n    with self.create_mock('pkg.__init__', 'pkg.module') as importer:\n        with util.import_state(meta_path=[importer]):\n            module = self.__import__('pkg', fromlist=['module'])\n            self.assertTrue(hasattr(module, 'module'))\n            self.assertEqual(id(module.module),\n                            id(sys.modules['pkg.module']))", "path": "cpython/Lib/test/test_importlib/import_/test_caching.py", "commit_date": "2020-12-04 00:00:00", "repo_name": "python/cpython", "stars": 58706, "license": "other", "language": "python", "size": 557565}
{"docstring": "# [use cache]\n", "func_signal": "def test_using_cache(self):\n", "code": "module_to_use = \"some module found!\"\nwith util.uncache('some_module'):\n    sys.modules['some_module'] = module_to_use\n    module = self.__import__('some_module')\n    self.assertEqual(id(module_to_use), id(module))", "path": "cpython/Lib/test/test_importlib/import_/test_caching.py", "commit_date": "2020-12-04 00:00:00", "repo_name": "python/cpython", "stars": 58706, "license": "other", "language": "python", "size": 557565}
{"docstring": "# The stdlib non-text codecs are now marked so they're\n# pre-emptively skipped by the text model related methods\n# However, third party codecs won't be flagged, so we still make\n# sure the case where an inappropriate output type is produced is\n# handled appropriately\n", "func_signal": "def test_unflagged_non_text_codec_handling(self):\n", "code": "def encode_to_str(*args, **kwds):\n    return \"not bytes!\", 0\ndef decode_to_bytes(*args, **kwds):\n    return b\"not str!\", 0\nself.set_codec(encode_to_str, decode_to_bytes)\n# No input or output type checks on the codecs module functions\nencoded = codecs.encode(None, self.codec_name)\nself.assertEqual(encoded, \"not bytes!\")\ndecoded = codecs.decode(None, self.codec_name)\nself.assertEqual(decoded, b\"not str!\")\n# Text model methods should complain\nfmt = (r\"^{!r} encoder returned 'str' instead of 'bytes'; \"\n       r\"use codecs.encode\\(\\) to encode to arbitrary types$\")\nmsg = fmt.format(self.codec_name)\nwith self.assertRaisesRegex(TypeError, msg):\n    \"str_input\".encode(self.codec_name)\nfmt = (r\"^{!r} decoder returned 'bytes' instead of 'str'; \"\n       r\"use codecs.decode\\(\\) to decode to arbitrary types$\")\nmsg = fmt.format(self.codec_name)\nwith self.assertRaisesRegex(TypeError, msg):\n    b\"bytes input\".decode(self.codec_name)", "path": "cpython/Lib/test/test_codecs.py", "commit_date": "2020-10-17 00:00:00", "repo_name": "python/cpython", "stars": 58706, "license": "other", "language": "python", "size": 557565}
{"docstring": "# Check that getstate() and setstate() handle the state properly\n", "func_signal": "def test_decoder_state(self):\n", "code": "u = \"abc123\"\nfor encoding in all_unicode_encodings:\n    if encoding not in broken_unicode_with_stateful:\n        self.check_state_handling_decode(encoding, u, u.encode(encoding))\n        self.check_state_handling_encode(encoding, u, u.encode(encoding))", "path": "cpython/Lib/test/test_codecs.py", "commit_date": "2020-10-17 00:00:00", "repo_name": "python/cpython", "stars": 58706, "license": "other", "language": "python", "size": 557565}
{"docstring": "# Test input longer than INT_MAX.\n# Input should contain a decodable multi-byte character\n# surrounding INT_MAX\n", "func_signal": "def test_large_utf8_input(self, size):\n", "code": "encoded = (b'0123456\\xed\\x84\\x80' * (size//8))\nself.assertEqual(len(encoded), size // 8 * 10)\ndecoded = codecs.code_page_decode(65001, encoded, 'ignore', True)\nself.assertEqual(decoded[1], len(encoded))\ndel encoded\nself.assertEqual(len(decoded[0]), size)\nself.assertEqual(decoded[0][:10], '0123456\\ud10001')\nself.assertEqual(decoded[0][-11:], '56\\ud1000123456\\ud100')", "path": "cpython/Lib/test/test_codecs.py", "commit_date": "2020-10-17 00:00:00", "repo_name": "python/cpython", "stars": 58706, "license": "other", "language": "python", "size": 557565}
{"docstring": "# Issue #36312: For some code pages (e.g. UTF-7) flags for\n# MultiByteToWideChar() must be set to 0.\n", "func_signal": "def test_code_page_decode_flags(self):\n", "code": "if support.verbose:\n    sys.stdout.write('\\n')\nfor cp in (50220, 50221, 50222, 50225, 50227, 50229,\n           *range(57002, 57011+1), 65000):\n    # On small versions of Windows like Windows IoT\n    # not all codepages are present.\n    # A missing codepage causes an OSError exception\n    # so check for the codepage before decoding\n    if is_code_page_present(cp):\n        self.assertEqual(codecs.code_page_decode(cp, b'abc'), ('abc', 3), f'cp{cp}')\n    else:\n        if support.verbose:\n            print(f\"  skipping cp={cp}\")\nself.assertEqual(codecs.code_page_decode(42, b'abc'),\n                 ('\\uf061\\uf062\\uf063', 3))", "path": "cpython/Lib/test/test_codecs.py", "commit_date": "2020-10-17 00:00:00", "repo_name": "python/cpython", "stars": 58706, "license": "other", "language": "python", "size": 557565}
{"docstring": "# Check str.encode gives a good error message for str -> str codecs\n", "func_signal": "def test_text_to_binary_denylists_text_transforms(self):\n", "code": "msg = (r\"^'rot_13' is not a text encoding; \"\n       r\"use codecs.encode\\(\\) to handle arbitrary codecs\")\nwith self.assertRaisesRegex(LookupError, msg):\n    \"just an example message\".encode(\"rot_13\")", "path": "cpython/Lib/test/test_codecs.py", "commit_date": "2020-10-17 00:00:00", "repo_name": "python/cpython", "stars": 58706, "license": "other", "language": "python", "size": 557565}
{"docstring": "# Most decoders don't accept unicode input\n", "func_signal": "def test_decode_unicode(self):\n", "code": "decoders = [\n    codecs.utf_7_decode,\n    codecs.utf_8_decode,\n    codecs.utf_16_le_decode,\n    codecs.utf_16_be_decode,\n    codecs.utf_16_ex_decode,\n    codecs.utf_32_decode,\n    codecs.utf_32_le_decode,\n    codecs.utf_32_be_decode,\n    codecs.utf_32_ex_decode,\n    codecs.latin_1_decode,\n    codecs.ascii_decode,\n    codecs.charmap_decode,\n]\nif hasattr(codecs, \"mbcs_decode\"):\n    decoders.append(codecs.mbcs_decode)\nfor decoder in decoders:\n    self.assertRaises(TypeError, decoder, \"xxx\")", "path": "cpython/Lib/test/test_codecs.py", "commit_date": "2020-10-17 00:00:00", "repo_name": "python/cpython", "stars": 58706, "license": "other", "language": "python", "size": 557565}
{"docstring": "\"\"\"[deprecated] Aggregate logging outputs from data parallel training.\"\"\"\n", "func_signal": "def aggregate_logging_outputs(self, logging_outputs, criterion):\n", "code": "utils.deprecation_warning(\n    \"The aggregate_logging_outputs API is deprecated. \"\n    \"Please use the reduce_metrics API instead.\"\n)\nwith metrics.aggregate() as agg:\n    self.reduce_metrics(logging_outputs, criterion)\n    return agg.get_smoothed_values()", "path": "fairseq/fairseq/tasks/fairseq_task.py", "commit_date": "2020-12-28 00:00:00", "repo_name": "facebookresearch/fairseq", "stars": 28848, "license": "mit", "language": "python", "size": 25793}
{"docstring": "\"\"\"\ninput size: B x C x T\noutput size: B x C x T\n\"\"\"\n", "func_signal": "def forward(self, input):\n", "code": "B, C, T = input.size()\nH = self.num_heads\n\nweight = self.weight\nif self.weight_softmax:\n    weight = F.softmax(weight, dim=-1)\n\nweight = self.weight_dropout_module(weight)\n# Merge every C/H entries into the batch dimension (C = self.input_size)\n# B x C x T -> (B * C/H) x H x T\n# One can also expand the weight to C x 1 x K by a factor of C/H\n# and do not reshape the input instead, which is slow though\ninput = input.view(-1, H, T)\noutput = F.conv1d(input, weight, padding=self.padding, groups=self.num_heads)\noutput = output.view(B, C, T)\nif self.bias is not None:\n    output = output + self.bias.view(1, -1, 1)\n\nreturn output", "path": "fairseq/fairseq/modules/lightweight_convolution.py", "commit_date": "2020-10-19 00:00:00", "repo_name": "facebookresearch/fairseq", "stars": 28848, "license": "mit", "language": "python", "size": 25793}
{"docstring": "\"\"\"Load a given dataset split.\n\nArgs:\n    split (str): name of the split (e.g., train, valid, test)\n\"\"\"\n", "func_signal": "def load_dataset(self, split, epoch=1, combine=False):\n", "code": "loaded_datasets = []\n\npaths = utils.split_paths(self.args.data)\nassert len(paths) > 0\ndata_path = paths[(epoch - 1) % len(paths)]\nlogger.info(\"data_path\", data_path)\n\nfor k in itertools.count():\n    split_k = split + (str(k) if k > 0 else \"\")\n    path = os.path.join(data_path, split_k)\n    ds = indexed_dataset.make_dataset(\n        path,\n        impl=self.args.dataset_impl,\n        fix_lua_indexing=True,\n        dictionary=self.dictionary,\n    )\n\n    if ds is None:\n        if k > 0:\n            break\n        else:\n            raise FileNotFoundError(\n                \"Dataset not found: {} ({})\".format(split, data_path)\n            )\n\n    with data_utils.numpy_seed(self.seed + k):\n        loaded_datasets.append(\n            BlockPairDataset(\n                ds,\n                self.dictionary,\n                ds.sizes,\n                self.args.tokens_per_sample,\n                break_mode=self.args.break_mode,\n                doc_break_size=1,\n            )\n        )\n\n    logger.info(\n        \"{} {} {} examples\".format(data_path, split_k, len(loaded_datasets[-1]))\n    )\n\n    if not combine:\n        break\n\nif len(loaded_datasets) == 1:\n    dataset = loaded_datasets[0]\n    sizes = dataset.sizes\nelse:\n    dataset = ConcatDataset(loaded_datasets)\n    sizes = np.concatenate([ds.sizes for ds in loaded_datasets])\n\nself.datasets[split] = MaskedLMDataset(\n    dataset=dataset,\n    sizes=sizes,\n    vocab=self.dictionary,\n    pad_idx=self.dictionary.pad(),\n    mask_idx=self.dictionary.mask(),\n    classif_token_idx=self.dictionary.cls(),\n    sep_token_idx=self.dictionary.sep(),\n    shuffle=self.args.shuffle_dataset,\n    seed=self.seed,\n)", "path": "fairseq/fairseq/tasks/legacy_masked_lm.py", "commit_date": "2020-10-19 00:00:00", "repo_name": "facebookresearch/fairseq", "stars": 28848, "license": "mit", "language": "python", "size": 25793}
{"docstring": "\"\"\"Add task-specific arguments to the parser.\"\"\"\n", "func_signal": "def add_args(cls, parser):\n", "code": "dc = getattr(cls, \"__dataclass\", None)\nif dc is not None:\n    gen_parser_from_dataclass(parser, dc())", "path": "fairseq/fairseq/tasks/fairseq_task.py", "commit_date": "2020-12-28 00:00:00", "repo_name": "facebookresearch/fairseq", "stars": 28848, "license": "mit", "language": "python", "size": 25793}
{"docstring": "\"\"\"Turn the convolution filters into band matrices and do matrix multiplication.\nThis is faster when the sequence is short, but less memory efficient.\nThis is not used in the decoder during inference.\n\"\"\"\n", "func_signal": "def _forward_expanded(self, x, incremental_state):\n", "code": "T, B, C = x.size()\nK, H = self.kernel_size, self.num_heads\nR = C // H\nassert R * H == C == self.input_size\n\nweight = self.weight.view(H, K)\nif self.weight_softmax:\n    weight = utils.softmax(weight, dim=1, onnx_trace=self.onnx_trace).type_as(\n        weight\n    )\nweight = weight.view(1, H, K).expand(T * B, H, K).contiguous()\nweight = weight.view(T, B * H, K).transpose(0, 1)\n\nx = x.view(T, B * H, R).transpose(0, 1)\nP = self.padding_l\nif K > T and P == K - 1:\n    weight = weight.narrow(2, K - T, T)\n    K, P = T, T - 1\n# turn the convolution filters into band matrices\nweight_expanded = weight.new_zeros(B * H, T, T + K - 1, requires_grad=False)\nweight_expanded.as_strided((B * H, T, K), (T * (T + K - 1), T + K, 1)).copy_(\n    weight\n)\nweight_expanded = weight_expanded.narrow(2, P, T)\nweight_expanded = self.weight_dropout_module(weight_expanded)\n\noutput = torch.bmm(weight_expanded, x)\noutput = output.transpose(0, 1).contiguous().view(T, B, C)\nreturn output", "path": "fairseq/fairseq/modules/lightweight_convolution.py", "commit_date": "2020-10-19 00:00:00", "repo_name": "facebookresearch/fairseq", "stars": 28848, "license": "mit", "language": "python", "size": 25793}
{"docstring": "\"\"\"Convert a list of 2d frames into a padded 3d tensor\nArgs:\n    frames (list): list of 2d frames of size L[i]*f_dim. Where L[i] is\n        length of i-th frame and f_dim is static dimension of features\nReturns:\n    3d tensor of size len(frames)*len_max*f_dim where len_max is max of L[i]\n\"\"\"\n", "func_signal": "def _collate_frames(self, frames):\n", "code": "len_max = max(frame.size(0) for frame in frames)\nf_dim = frames[0].size(1)\nres = frames[0].new(len(frames), len_max, f_dim).fill_(0.0)\n\nfor i, v in enumerate(frames):\n    res[i, : v.size(0)] = v\n\nreturn res", "path": "fairseq/examples/speech_recognition/data/collaters.py", "commit_date": "2020-10-19 00:00:00", "repo_name": "facebookresearch/fairseq", "stars": 28848, "license": "mit", "language": "python", "size": 25793}
{"docstring": "\"\"\"Build a new model instance.\"\"\"\n# make sure all arguments are present in older models\n", "func_signal": "def build_model(cls, args, task):\n", "code": "base_lm_architecture(args)\n\nif hasattr(args, \"max_target_positions\") and not hasattr(\n    args, \"tokens_per_sample\"\n):\n    args.tokens_per_sample = args.max_target_positions\n\ndecoder = FConvDecoder(\n    dictionary=task.target_dictionary,\n    embed_dim=args.decoder_embed_dim,\n    convolutions=eval(args.decoder_layers),\n    out_embed_dim=args.decoder_embed_dim,\n    attention=eval(args.decoder_attention),\n    dropout=args.dropout,\n    max_positions=args.tokens_per_sample,\n    share_embed=False,\n    positional_embeddings=False,\n    adaptive_softmax_cutoff=(\n        utils.eval_str_list(args.adaptive_softmax_cutoff, type=int)\n        if args.criterion == \"adaptive_loss\"\n        else None\n    ),\n    adaptive_softmax_dropout=args.adaptive_softmax_dropout,\n)\nreturn FConvLanguageModel(decoder)", "path": "fairseq/fairseq/models/fconv_lm.py", "commit_date": "2020-10-19 00:00:00", "repo_name": "facebookresearch/fairseq", "stars": 28848, "license": "mit", "language": "python", "size": 25793}
{"docstring": "\"\"\"Save all training state in a checkpoint file.\"\"\"\n", "func_signal": "def save_checkpoint(self, filename, extra_state):\n", "code": "extra_state['rng_tracker_states'] \\\n    = get_cuda_rng_tracker().get_states()\nsuper().save_checkpoint(filename, extra_state)", "path": "fairseq/fairseq/model_parallel/megatron_trainer.py", "commit_date": "2020-11-05 00:00:00", "repo_name": "facebookresearch/fairseq", "stars": 28848, "license": "mit", "language": "python", "size": 25793}
{"docstring": "\"\"\"Aggregate logging outputs from data parallel training.\"\"\"\n# backward compatibility for tasks that override aggregate_logging_outputs\n", "func_signal": "def reduce_metrics(self, logging_outputs, criterion):\n", "code": "base_func = FairseqTask.aggregate_logging_outputs\nself_func = getattr(self, \"aggregate_logging_outputs\").__func__\nif self_func is not base_func:\n    utils.deprecation_warning(\n        \"Tasks should implement the reduce_metrics API. \"\n        \"Falling back to deprecated aggregate_logging_outputs API.\"\n    )\n    agg_logging_outputs = self.aggregate_logging_outputs(\n        logging_outputs, criterion\n    )\n    for k, v in agg_logging_outputs.items():\n        metrics.log_scalar(k, v)\n    return\n\nif not any(\"ntokens\" in log for log in logging_outputs):\n    warnings.warn(\n        \"ntokens not found in Criterion logging outputs, cannot log wpb or wps\"\n    )\nelse:\n    ntokens = sum(log.get(\"ntokens\", 0) for log in logging_outputs)\n    metrics.log_scalar(\"wpb\", ntokens, priority=180, round=1)\n    metrics.log_speed(\"wps\", ntokens, priority=90, round=1)\n\nif not any(\"nsentences\" in log for log in logging_outputs):\n    warnings.warn(\n        \"nsentences not found in Criterion logging outputs, cannot log bsz\"\n    )\nelse:\n    nsentences = sum(log.get(\"nsentences\", 0) for log in logging_outputs)\n    metrics.log_scalar(\"bsz\", nsentences, priority=190, round=1)\n\ncriterion.__class__.reduce_metrics(logging_outputs)", "path": "fairseq/fairseq/tasks/fairseq_task.py", "commit_date": "2020-12-28 00:00:00", "repo_name": "facebookresearch/fairseq", "stars": 28848, "license": "mit", "language": "python", "size": 25793}
{"docstring": "\"\"\"\nutility function to collate samples into batch for speech recognition.\n\"\"\"\n", "func_signal": "def collate(self, samples):\n", "code": "if len(samples) == 0:\n    return {}\n\n# parse samples into torch tensors\nparsed_samples = []\nfor s in samples:\n    # skip invalid samples\n    if s[\"data\"][self.feature_index] is None:\n        continue\n    source = s[\"data\"][self.feature_index]\n    if isinstance(source, (np.ndarray, np.generic)):\n        source = torch.from_numpy(source)\n    target = s[\"data\"][self.label_index]\n    if isinstance(target, (np.ndarray, np.generic)):\n        target = torch.from_numpy(target).long()\n    elif isinstance(target, list):\n        target = torch.LongTensor(target)\n\n    parsed_sample = {\"id\": s[\"id\"], \"source\": source, \"target\": target}\n    parsed_samples.append(parsed_sample)\nsamples = parsed_samples\n\nid = torch.LongTensor([s[\"id\"] for s in samples])\nframes = self._collate_frames([s[\"source\"] for s in samples])\n# sort samples by descending number of frames\nframes_lengths = torch.LongTensor([s[\"source\"].size(0) for s in samples])\nframes_lengths, sort_order = frames_lengths.sort(descending=True)\nid = id.index_select(0, sort_order)\nframes = frames.index_select(0, sort_order)\n\ntarget = None\ntarget_lengths = None\nprev_output_tokens = None\nif samples[0].get(\"target\", None) is not None:\n    ntokens = sum(len(s[\"target\"]) for s in samples)\n    target = fairseq_data_utils.collate_tokens(\n        [s[\"target\"] for s in samples],\n        self.pad_index,\n        self.eos_index,\n        left_pad=False,\n        move_eos_to_beginning=False,\n    )\n    target = target.index_select(0, sort_order)\n    target_lengths = torch.LongTensor(\n        [s[\"target\"].size(0) for s in samples]\n    ).index_select(0, sort_order)\n    prev_output_tokens = fairseq_data_utils.collate_tokens(\n        [s[\"target\"] for s in samples],\n        self.pad_index,\n        self.eos_index,\n        left_pad=False,\n        move_eos_to_beginning=self.move_eos_to_beginning,\n    )\n    prev_output_tokens = prev_output_tokens.index_select(0, sort_order)\nelse:\n    ntokens = sum(len(s[\"source\"]) for s in samples)\n\nbatch = {\n    \"id\": id,\n    \"ntokens\": ntokens,\n    \"net_input\": {\"src_tokens\": frames, \"src_lengths\": frames_lengths},\n    \"target\": target,\n    \"target_lengths\": target_lengths,\n    \"nsentences\": len(samples),\n}\nif prev_output_tokens is not None:\n    batch[\"net_input\"][\"prev_output_tokens\"] = prev_output_tokens\nreturn batch", "path": "fairseq/examples/speech_recognition/data/collaters.py", "commit_date": "2020-10-19 00:00:00", "repo_name": "facebookresearch/fairseq", "stars": 28848, "license": "mit", "language": "python", "size": 25793}
{"docstring": "\"\"\"Setup the task.\"\"\"\n", "func_signal": "def setup_task(cls, args, **kwargs):\n", "code": "paths = utils.split_paths(args.data)\nassert len(paths) > 0\ndictionary = BertDictionary.load(os.path.join(paths[0], \"dict.txt\"))\nlogger.info(\"dictionary: {} types\".format(len(dictionary)))\n\nreturn cls(args, dictionary)", "path": "fairseq/fairseq/tasks/legacy_masked_lm.py", "commit_date": "2020-10-19 00:00:00", "repo_name": "facebookresearch/fairseq", "stars": 28848, "license": "mit", "language": "python", "size": 25793}
{"docstring": "# Initialize to default options.\n", "func_signal": "def parse_config_yaml(yaml_data):\n", "code": "quantization_options = {\n    \"n_centroids\": {\n        \"Linear\": [\"in_features\", {\"*\": 256}],\n        \"Embedding\": [\"embedding_dim\", {\"*\": 256}],\n    },\n    \"block_sizes\": {\n        \"Linear\": [\"fuzzy_name\", {\"fc\": 8, \"attn\": 4, \"emb\": 4}],\n        \"Embedding\": [\"fuzzy_name\", {\"emb\": 8}],\n    },\n    \"layers_to_quantize\": [\n        \"decoder\\\\.layers\\\\.\\\\d+\\\\.fc[12]\",\n        \"decoder\\\\.embed_tokens\\\\.embeddings\\\\.[012]\\\\.[01]\",\n        \"decoder\\\\.layers\\\\.\\\\d+\\\\.self_attn\\\\.(k_proj|v_proj|q_proj|out_proj)\",\n    ],\n}\n\nif \"n_centroids\" in yaml_data:\n    quantization_options[\"n_centroids\"] = {\n        layer: convert_yaml_to_tuple(layer_data)\n        for layer, layer_data in yaml_data[\"n_centroids\"].items()\n    }\nif \"block_sizes\" in yaml_data:\n    quantization_options[\"block_sizes\"] = {\n        layer: convert_yaml_to_tuple(layer_data)\n        for layer, layer_data in yaml_data[\"block_sizes\"].items()\n    }\nif \"layers_to_quantize\" in yaml_data:\n    quantization_options[\"layers_to_quantize\"] = yaml_data[\"layers_to_quantize\"]\n\nreturn quantization_options", "path": "fairseq/fairseq/modules/quantization/quantization_options.py", "commit_date": "2020-05-01 00:00:00", "repo_name": "facebookresearch/fairseq", "stars": 28848, "license": "mit", "language": "python", "size": 25793}
{"docstring": "# Work out how cleanly we can divide the dataset into bsz parts.\n", "func_signal": "def batchify(data, bsz):\n", "code": "nbatch = data.size(0) // bsz\n# Trim off any extra elements that wouldn't cleanly fit (remainders).\ndata = data.narrow(0, 0, nbatch * bsz)\n# Evenly divide the data across the bsz batches.\ndata = data.view(bsz, -1).contiguous()\nreturn data", "path": "fairseq/examples/truncated_bptt/truncated_bptt_lm_task.py", "commit_date": "2020-12-18 00:00:00", "repo_name": "facebookresearch/fairseq", "stars": 28848, "license": "mit", "language": "python", "size": 25793}
{"docstring": "# train with QuantNoise and evaluate the fully quantized network\n", "func_signal": "def forward(self, input):\n", "code": "p = self.p if self.training else 1\n\n# update parameters every 1000 iterations\nif self.counter % self.update_step == 0:\n    self.scale = None\n    self.zero_point = None\nself.counter += 1\n\n# quantize weight\nweight_quantized, self.scale, self.zero_point = emulate_int(\n    self.weight.detach(),\n    bits=self.bits,\n    method=self.method,\n    scale=self.scale,\n    zero_point=self.zero_point,\n)\n\n# mask to apply noise\nmask = torch.zeros_like(self.weight)\nmask.bernoulli_(1 - p)\nnoise = (weight_quantized - self.weight).masked_fill(mask.bool(), 0)\n\n# using straight-through estimator (STE)\nclamp_low = -self.scale * self.zero_point\nclamp_high = self.scale * (2 ** self.bits - 1 - self.zero_point)\nweight = (\n    torch.clamp(self.weight, clamp_low.item(), clamp_high.item())\n    + noise.detach()\n)\n\n# return output\noutput = F.embedding(\n    input,\n    weight,\n    self.padding_idx,\n    self.max_norm,\n    self.norm_type,\n    self.scale_grad_by_freq,\n    self.sparse,\n)\nreturn output", "path": "fairseq/fairseq/modules/quantization/scalar/modules/qemb.py", "commit_date": "2020-10-19 00:00:00", "repo_name": "facebookresearch/fairseq", "stars": 28848, "license": "mit", "language": "python", "size": 25793}
{"docstring": "\"\"\"Add task-specific arguments to the parser.\"\"\"\n", "func_signal": "def add_args(parser):\n", "code": "parser.add_argument(\n    \"data\",\n    help=\"colon separated path to data directories list, \\\n                    will be iterated upon during epochs in round-robin manner\",\n)\nparser.add_argument(\n    \"--tokens-per-sample\",\n    default=512,\n    type=int,\n    help=\"max number of total tokens over all segments\"\n    \" per sample for BERT dataset\",\n)\nparser.add_argument(\n    \"--break-mode\", default=\"doc\", type=str, help=\"mode for breaking sentence\"\n)\nparser.add_argument(\"--shuffle-dataset\", action=\"store_true\", default=False)", "path": "fairseq/fairseq/tasks/legacy_masked_lm.py", "commit_date": "2020-10-19 00:00:00", "repo_name": "facebookresearch/fairseq", "stars": 28848, "license": "mit", "language": "python", "size": 25793}
{"docstring": "\"\"\"Add model-specific arguments to the parser.\"\"\"\n", "func_signal": "def add_args(parser):\n", "code": "parser.add_argument(\n    \"--dropout\", type=float, metavar=\"D\", help=\"dropout probability\"\n)\nparser.add_argument(\n    \"--decoder-embed-dim\",\n    type=int,\n    metavar=\"N\",\n    help=\"decoder embedding dimension\",\n)\nparser.add_argument(\n    \"--decoder-layers\",\n    type=str,\n    metavar=\"EXPR\",\n    help=\"decoder layers [(dim, kernel_size), ...]\",\n)\nparser.add_argument(\n    \"--decoder-out-embed-dim\",\n    type=int,\n    metavar=\"N\",\n    help=\"decoder output embedding dimension\",\n)\nparser.add_argument(\n    \"--adaptive-softmax-cutoff\",\n    metavar=\"EXPR\",\n    help=\"comma separated list of adaptive softmax cutoff points. \"\n    \"Must be used with adaptive_loss criterion\",\n)\nparser.add_argument(\n    \"--adaptive-softmax-dropout\",\n    type=float,\n    metavar=\"D\",\n    help=\"sets adaptive softmax dropout for the tail projections\",\n)\nparser.add_argument(\n    \"--decoder-attention\",\n    type=str,\n    metavar=\"EXPR\",\n    help=\"decoder attention [True, ...]\",\n)", "path": "fairseq/fairseq/models/fconv_lm.py", "commit_date": "2020-10-19 00:00:00", "repo_name": "facebookresearch/fairseq", "stars": 28848, "license": "mit", "language": "python", "size": 25793}
{"docstring": "\"\"\"Load a given dataset split (e.g., train, valid, test)\"\"\"\n\n# support sharded datasets\n", "func_signal": "def load_dataset(self, split, epoch=1, combine=False, **kwargs):\n", "code": "paths = utils.split_paths(self.cfg.data)\nassert len(paths) > 0\ndata_path = paths[(epoch - 1) % len(paths)]\nsplit_path = os.path.join(data_path, split)\n\n# each element of *data* will be a tensorized line from the original\n# text dataset, similar to ``open(split_path).readlines()``\ndata = data_utils.load_indexed_dataset(\n    split_path, self.dictionary, combine=combine\n)\nif data is None:\n    raise FileNotFoundError(\n        \"Dataset not found: {} ({})\".format(split, split_path)\n    )\n\n# this is similar to ``data.view(-1).split(tokens_per_sample)``\ndata = TokenBlockDataset(\n    data,\n    data.sizes,\n    block_size=self.cfg.tokens_per_sample,\n    pad=None,  # unused\n    eos=None,  # unused\n    break_mode=\"none\",\n)\n\nself.datasets[split] = TruncatedBPTTDataset(\n    data=data,\n    bsz_per_shard=self.cfg.batch_size,\n    shard_id=self.cfg.data_parallel_rank,\n    num_shards=self.cfg.data_parallel_size,\n)", "path": "fairseq/examples/truncated_bptt/truncated_bptt_lm_task.py", "commit_date": "2020-12-18 00:00:00", "repo_name": "facebookresearch/fairseq", "stars": 28848, "license": "mit", "language": "python", "size": 25793}
{"docstring": "\"\"\"The conventional implementation of convolutions.\nUnfolding the input by having a window shifting to the right.\"\"\"\n", "func_signal": "def _forward_unfolded(self, x, incremental_state):\n", "code": "T, B, C = x.size()\nK, H = self.kernel_size, self.num_heads\nR = C // H\nassert R * H == C == self.input_size\n\nweight = self.weight.view(H, K)\nif incremental_state is not None:\n    input_buffer = self._get_input_buffer(incremental_state)\n    if input_buffer is None:\n        input_buffer = x.new()\n    x_unfold = torch.cat([input_buffer, x.unsqueeze(3)], dim=3)\n    if self.kernel_size > 1:\n        self._set_input_buffer(\n            incremental_state, x_unfold[:, :, :, -self.kernel_size + 1 :]\n        )\n    x_unfold = x_unfold.view(T * B * H, R, -1)\nelse:\n    # unfold the input: T x B x C --> T' x B x C x K\n    x_unfold = unfold1d(x, self.kernel_size, self.padding_l, 0)\n    x_unfold = x_unfold.view(T * B * H, R, K)\n\nif self.weight_softmax:\n    weight = utils.softmax(weight, dim=1, onnx_trace=self.onnx_trace).type_as(\n        weight\n    )\n\nif incremental_state is not None:\n    weight = weight[:, -x_unfold.size(2) :]\n    K = weight.size(1)\n\nweight = (\n    weight.view(1, H, K).expand(T * B, H, K).contiguous().view(T * B * H, K, 1)\n)\n\nweight = self.weight_dropout_module(weight)\noutput = torch.bmm(x_unfold, weight)  # T*B*H x R x 1\noutput = output.view(T, B, C)\nreturn output", "path": "fairseq/fairseq/modules/lightweight_convolution.py", "commit_date": "2020-10-19 00:00:00", "repo_name": "facebookresearch/fairseq", "stars": 28848, "license": "mit", "language": "python", "size": 25793}
{"docstring": "\"\"\"Assuming the input, x, of the shape T x B x C and producing an output in the shape T x B x C\nargs:\n    x: Input of shape T x B x C, i.e. (timesteps, batch_size, input_size)\n    incremental_state: A dict to keep the state\n    unfold: unfold the input or not. If not, we use the matrix trick instead\n\"\"\"\n", "func_signal": "def forward(self, x, incremental_state=None, unfold=False):\n", "code": "unfold = unfold or (incremental_state is not None)\n\nif unfold:\n    output = self._forward_unfolded(x, incremental_state)\nelse:\n    output = self._forward_expanded(x, incremental_state)\n\nif self.bias is not None:\n    output = output + self.bias.view(1, 1, -1)\nreturn output", "path": "fairseq/fairseq/modules/lightweight_convolution.py", "commit_date": "2020-10-19 00:00:00", "repo_name": "facebookresearch/fairseq", "stars": 28848, "license": "mit", "language": "python", "size": 25793}
{"docstring": "# use the TransformerDecoder's __init__\n", "func_signal": "def __init__(self, args, dictionary, embed_tokens, no_encoder_attn=False):\n", "code": "super(LevenshteinTransformerDecoder, self).__init__(\n    args, dictionary, embed_tokens, no_encoder_attn=no_encoder_attn\n)\n\nself.dictionary = dictionary\nself.bos = dictionary.bos()\nself.unk = dictionary.unk()\nself.eos = dictionary.eos()\nself.pool_out = Linear(self.output_embed_dim * 2, self.output_embed_dim)\n\nself.label_tau = getattr(args, \"label_tau\", None)", "path": "fairseq/fairseq/models/nat/insertion_transformer.py", "commit_date": "2020-10-19 00:00:00", "repo_name": "facebookresearch/fairseq", "stars": 28848, "license": "mit", "language": "python", "size": 25793}
{"docstring": "# GH#38120 freq should be preserved\n", "func_signal": "def test_factorize_preserves_freq(self):\n", "code": "idx3 = timedelta_range(\"1 day\", periods=4, freq=\"s\")\nexp_arr = np.array([0, 1, 2, 3], dtype=np.intp)\narr, idx = idx3.factorize()\ntm.assert_numpy_array_equal(arr, exp_arr)\ntm.assert_index_equal(idx, idx3)\nassert idx.freq == idx3.freq\n\narr, idx = factorize(idx3)\ntm.assert_numpy_array_equal(arr, exp_arr)\ntm.assert_index_equal(idx, idx3)\nassert idx.freq == idx3.freq", "path": "pandas/pandas/tests/indexes/timedeltas/methods/test_factorize.py", "commit_date": "2020-12-21 00:00:00", "repo_name": "pandas-dev/pandas", "stars": 41435, "license": "bsd-3-clause", "language": "python", "size": 352357}
{"docstring": "# see gh-2184\n", "func_signal": "def test_converters_no_implicit_conv(all_parsers):\n", "code": "parser = all_parsers\ndata = \"\"\"000102,1.2,A\\n001245,2,B\"\"\"\n\nconverters = {0: lambda x: x.strip()}\nresult = parser.read_csv(StringIO(data), header=None, converters=converters)\n\n# Column 0 should not be casted to numeric and should remain as object.\nexpected = DataFrame([[\"000102\", 1.2, \"A\"], [\"001245\", 2, \"B\"]])\ntm.assert_frame_equal(result, expected)", "path": "pandas/pandas/tests/io/parser/test_converters.py", "commit_date": "2020-11-28 00:00:00", "repo_name": "pandas-dev/pandas", "stars": 41435, "license": "bsd-3-clause", "language": "python", "size": 352357}
{"docstring": "\"\"\"\nAttempt to prevent foot-shooting in a helpful way.\n\nParameters\n----------\nterms : Term\n    Terms can contain\n\"\"\"\n", "func_signal": "def _check_ne_builtin_clash(expr):\n", "code": "names = expr.names\noverlap = names & _ne_builtins\n\nif overlap:\n    s = \", \".join(repr(x) for x in overlap)\n    raise NumExprClobberingError(\n        f'Variables in expression \"{expr}\" overlap with builtins: ({s})'\n    )", "path": "pandas/pandas/core/computation/engines.py", "commit_date": "2020-09-09 00:00:00", "repo_name": "pandas-dev/pandas", "stars": 41435, "license": "bsd-3-clause", "language": "python", "size": 352357}
{"docstring": "# GH#35530\n", "func_signal": "def test_tz_convert_readonly():\n", "code": "arr = np.array([0], dtype=np.int64)\narr.setflags(write=False)\nresult = tzconversion.tz_convert_from_utc(arr, UTC)\ntm.assert_numpy_array_equal(result, arr)", "path": "pandas/pandas/tests/tslibs/test_conversion.py", "commit_date": "2020-08-06 00:00:00", "repo_name": "pandas-dev/pandas", "stars": 41435, "license": "bsd-3-clause", "language": "python", "size": 352357}
{"docstring": "# GH#13107\n", "func_signal": "def test_equals2(self):\n", "code": "idx = TimedeltaIndex([\"1 days\", \"2 days\", \"NaT\"])\nassert idx.equals(idx)\nassert idx.equals(idx.copy())\nassert idx.equals(idx.astype(object))\nassert idx.astype(object).equals(idx)\nassert idx.astype(object).equals(idx.astype(object))\nassert not idx.equals(list(idx))\nassert not idx.equals(pd.Series(idx))\n\nidx2 = TimedeltaIndex([\"2 days\", \"1 days\", \"NaT\"])\nassert not idx.equals(idx2)\nassert not idx.equals(idx2.copy())\nassert not idx.equals(idx2.astype(object))\nassert not idx.astype(object).equals(idx2)\nassert not idx.astype(object).equals(idx2.astype(object))\nassert not idx.equals(list(idx2))\nassert not idx.equals(pd.Series(idx2))\n\n# Check that we dont raise OverflowError on comparisons outside the\n#  implementation range\noob = Index([timedelta(days=10 ** 6)] * 3, dtype=object)\nassert not idx.equals(oob)\nassert not idx2.equals(oob)\n\n# FIXME: oob.apply(np.timedelta64) incorrectly overflows\noob2 = Index([np.timedelta64(x) for x in oob], dtype=object)\nassert not idx.equals(oob2)\nassert not idx2.equals(oob2)", "path": "pandas/pandas/tests/indexes/datetimelike_/test_equals.py", "commit_date": "2020-12-19 00:00:00", "repo_name": "pandas-dev/pandas", "stars": 41435, "license": "bsd-3-clause", "language": "python", "size": 352357}
{"docstring": "# GH #10174\n\n# interpolation = linear (default case)\n", "func_signal": "def test_quantile_interpolation_dtype(self):\n", "code": "q = Series([1, 3, 4]).quantile(0.5, interpolation=\"lower\")\nassert q == np.percentile(np.array([1, 3, 4]), 50)\nassert is_integer(q)\n\nq = Series([1, 3, 4]).quantile(0.5, interpolation=\"higher\")\nassert q == np.percentile(np.array([1, 3, 4]), 50)\nassert is_integer(q)", "path": "pandas/pandas/tests/series/methods/test_quantile.py", "commit_date": "2020-10-30 00:00:00", "repo_name": "pandas-dev/pandas", "stars": 41435, "license": "bsd-3-clause", "language": "python", "size": 352357}
{"docstring": "# covers #9694\n", "func_signal": "def test_datetime_timedelta_quantiles(self):\n", "code": "assert pd.isna(Series([], dtype=\"M8[ns]\").quantile(0.5))\nassert pd.isna(Series([], dtype=\"m8[ns]\").quantile(0.5))", "path": "pandas/pandas/tests/series/methods/test_quantile.py", "commit_date": "2020-10-30 00:00:00", "repo_name": "pandas-dev/pandas", "stars": 41435, "license": "bsd-3-clause", "language": "python", "size": 352357}
{"docstring": "# don't want to be able to modify the index stored elsewhere after\n# making a copy\n\n", "func_signal": "def test_copy_index_name_checking(self, datetime_series):\n", "code": "datetime_series.index.name = None\nassert datetime_series.index.name is None\nassert datetime_series is datetime_series\n\ncp = datetime_series.copy()\ncp.index.name = \"foo\"\nassert datetime_series.index.name is None", "path": "pandas/pandas/tests/series/methods/test_copy.py", "commit_date": "2020-10-29 00:00:00", "repo_name": "pandas-dev/pandas", "stars": 41435, "license": "bsd-3-clause", "language": "python", "size": 352357}
{"docstring": "\"\"\"\nRun the engine on the expression.\n\nThis method performs alignment which is necessary no matter what engine\nis being used, thus its implementation is in the base class.\n\nReturns\n-------\nobject\n    The result of the passed expression.\n\"\"\"\n", "func_signal": "def evaluate(self) -> object:\n", "code": "if not self._is_aligned:\n    self.result_type, self.aligned_axes = align_terms(self.expr.terms)\n\n# make sure no names in resolvers and locals/globals clash\nres = self._evaluate()\nreturn reconstruct_object(\n    self.result_type, res, self.aligned_axes, self.expr.terms.return_type\n)", "path": "pandas/pandas/core/computation/engines.py", "commit_date": "2020-09-09 00:00:00", "repo_name": "pandas-dev/pandas", "stars": 41435, "license": "bsd-3-clause", "language": "python", "size": 352357}
{"docstring": "# GH 25851\n# ensure that subclassed datetime works with\n# localize_pydatetime\n", "func_signal": "def test_localize_pydatetime_dt_types(dt, expected):\n", "code": "result = conversion.localize_pydatetime(dt, UTC)\nassert result == expected", "path": "pandas/pandas/tests/tslibs/test_conversion.py", "commit_date": "2020-08-06 00:00:00", "repo_name": "pandas-dev/pandas", "stars": 41435, "license": "bsd-3-clause", "language": "python", "size": 352357}
{"docstring": "# GH#13107\n", "func_signal": "def test_equals2(self, freq):\n", "code": "idx = PeriodIndex([\"2011-01-01\", \"2011-01-02\", \"NaT\"], freq=freq)\nassert idx.equals(idx)\nassert idx.equals(idx.copy())\nassert idx.equals(idx.astype(object))\nassert idx.astype(object).equals(idx)\nassert idx.astype(object).equals(idx.astype(object))\nassert not idx.equals(list(idx))\nassert not idx.equals(pd.Series(idx))\n\nidx2 = PeriodIndex([\"2011-01-01\", \"2011-01-02\", \"NaT\"], freq=\"H\")\nassert not idx.equals(idx2)\nassert not idx.equals(idx2.copy())\nassert not idx.equals(idx2.astype(object))\nassert not idx.astype(object).equals(idx2)\nassert not idx.equals(list(idx2))\nassert not idx.equals(pd.Series(idx2))\n\n# same internal, different tz\nidx3 = PeriodIndex._simple_new(\n    idx._values._simple_new(idx._values.asi8, freq=\"H\")\n)\ntm.assert_numpy_array_equal(idx.asi8, idx3.asi8)\nassert not idx.equals(idx3)\nassert not idx.equals(idx3.copy())\nassert not idx.equals(idx3.astype(object))\nassert not idx.astype(object).equals(idx3)\nassert not idx.equals(list(idx3))\nassert not idx.equals(pd.Series(idx3))", "path": "pandas/pandas/tests/indexes/datetimelike_/test_equals.py", "commit_date": "2020-12-19 00:00:00", "repo_name": "pandas-dev/pandas", "stars": 41435, "license": "bsd-3-clause", "language": "python", "size": 352357}
{"docstring": "# see gh-1835\n", "func_signal": "def test_converter_index_col_bug(all_parsers):\n", "code": "parser = all_parsers\ndata = \"A;B\\n1;2\\n3;4\"\n\nrs = parser.read_csv(\n    StringIO(data), sep=\";\", index_col=\"A\", converters={\"A\": lambda x: x}\n)\n\nxp = DataFrame({\"B\": [2, 4]}, index=Index([1, 3], name=\"A\"))\ntm.assert_frame_equal(rs, xp)", "path": "pandas/pandas/tests/io/parser/test_converters.py", "commit_date": "2020-11-28 00:00:00", "repo_name": "pandas-dev/pandas", "stars": 41435, "license": "bsd-3-clause", "language": "python", "size": 352357}
{"docstring": "# GH 28930\n", "func_signal": "def test_compare_to_int(self, any_nullable_int_dtype, all_compare_operators):\n", "code": "s1 = pd.Series([1, None, 3], dtype=any_nullable_int_dtype)\ns2 = pd.Series([1, None, 3], dtype=\"float\")\n\nmethod = getattr(s1, all_compare_operators)\nresult = method(2)\n\nmethod = getattr(s2, all_compare_operators)\nexpected = method(2).astype(\"boolean\")\nexpected[s2.isna()] = pd.NA\n\nself.assert_series_equal(result, expected)", "path": "pandas/pandas/tests/arrays/integer/test_comparison.py", "commit_date": "2020-05-09 00:00:00", "repo_name": "pandas-dev/pandas", "stars": 41435, "license": "bsd-3-clause", "language": "python", "size": 352357}
{"docstring": "# GH#13107\n", "func_signal": "def test_equals2(self):\n", "code": "idx = DatetimeIndex([\"2011-01-01\", \"2011-01-02\", \"NaT\"])\nassert idx.equals(idx)\nassert idx.equals(idx.copy())\nassert idx.equals(idx.astype(object))\nassert idx.astype(object).equals(idx)\nassert idx.astype(object).equals(idx.astype(object))\nassert not idx.equals(list(idx))\nassert not idx.equals(pd.Series(idx))\n\nidx2 = DatetimeIndex([\"2011-01-01\", \"2011-01-02\", \"NaT\"], tz=\"US/Pacific\")\nassert not idx.equals(idx2)\nassert not idx.equals(idx2.copy())\nassert not idx.equals(idx2.astype(object))\nassert not idx.astype(object).equals(idx2)\nassert not idx.equals(list(idx2))\nassert not idx.equals(pd.Series(idx2))\n\n# same internal, different tz\nidx3 = DatetimeIndex(idx.asi8, tz=\"US/Pacific\")\ntm.assert_numpy_array_equal(idx.asi8, idx3.asi8)\nassert not idx.equals(idx3)\nassert not idx.equals(idx3.copy())\nassert not idx.equals(idx3.astype(object))\nassert not idx.astype(object).equals(idx3)\nassert not idx.equals(list(idx3))\nassert not idx.equals(pd.Series(idx3))\n\n# check that we do not raise when comparing with OutOfBounds objects\noob = Index([datetime(2500, 1, 1)] * 3, dtype=object)\nassert not idx.equals(oob)\nassert not idx2.equals(oob)\nassert not idx3.equals(oob)\n\n# check that we do not raise when comparing with OutOfBounds dt64\noob2 = oob.map(np.datetime64)\nassert not idx.equals(oob2)\nassert not idx2.equals(oob2)\nassert not idx3.equals(oob2)", "path": "pandas/pandas/tests/indexes/datetimelike_/test_equals.py", "commit_date": "2020-12-19 00:00:00", "repo_name": "pandas-dev/pandas", "stars": 41435, "license": "bsd-3-clause", "language": "python", "size": 352357}
{"docstring": "# GH#11794\n# copy of tz-aware\n", "func_signal": "def test_copy_tzaware(self, deep):\n", "code": "expected = Series([Timestamp(\"2012/01/01\", tz=\"UTC\")])\nexpected2 = Series([Timestamp(\"1999/01/01\", tz=\"UTC\")])\n\nser = Series([Timestamp(\"2012/01/01\", tz=\"UTC\")])\n\nif deep is None:\n    ser2 = ser.copy()\nelse:\n    ser2 = ser.copy(deep=deep)\n\nser2[0] = Timestamp(\"1999/01/01\", tz=\"UTC\")\n\n# default deep is True\nif deep is None or deep is True:\n    # Did not modify original Series\n    tm.assert_series_equal(ser2, expected2)\n    tm.assert_series_equal(ser, expected)\nelse:\n    # we DID modify the original Series\n    tm.assert_series_equal(ser2, expected2)\n    tm.assert_series_equal(ser, expected2)", "path": "pandas/pandas/tests/series/methods/test_copy.py", "commit_date": "2020-10-29 00:00:00", "repo_name": "pandas-dev/pandas", "stars": 41435, "license": "bsd-3-clause", "language": "python", "size": 352357}
{"docstring": "# GH 28930\n", "func_signal": "def test_compare_to_string(self, any_nullable_int_dtype):\n", "code": "s = pd.Series([1, None], dtype=any_nullable_int_dtype)\nresult = s == \"a\"\nexpected = pd.Series([False, pd.NA], dtype=\"boolean\")\n\nself.assert_series_equal(result, expected)", "path": "pandas/pandas/tests/arrays/integer/test_comparison.py", "commit_date": "2020-05-09 00:00:00", "repo_name": "pandas-dev/pandas", "stars": 41435, "license": "bsd-3-clause", "language": "python", "size": 352357}
{"docstring": "# GH#29684\n", "func_signal": "def test_ensure_datetime64ns_bigendian():\n", "code": "arr = np.array([np.datetime64(1, \"ms\")], dtype=\">M8[ms]\")\nresult = conversion.ensure_datetime64ns(arr)\n\nexpected = np.array([np.datetime64(1, \"ms\")], dtype=\"M8[ns]\")\ntm.assert_numpy_array_equal(result, expected)", "path": "pandas/pandas/tests/tslibs/test_conversion.py", "commit_date": "2020-08-06 00:00:00", "repo_name": "pandas-dev/pandas", "stars": 41435, "license": "bsd-3-clause", "language": "python", "size": 352357}
{"docstring": "# we choose dtypes so as to make the blocks\n#  a) not perfectly match between right and left\n#  b) appreciably bigger than single columns\n", "func_signal": "def setup(self, op):\n", "code": "n_cols = 2000\nn_rows = 500\n\n# construct dataframe with 2 blocks\narr1 = np.random.randn(n_rows, int(n_cols / 2)).astype(\"f8\")\narr2 = np.random.randn(n_rows, int(n_cols / 2)).astype(\"f4\")\ndf = pd.concat(\n    [pd.DataFrame(arr1), pd.DataFrame(arr2)], axis=1, ignore_index=True\n)\n# should already be the case, but just to be sure\ndf._consolidate_inplace()\n\n# TODO: GH#33198 the setting here shoudlnt need two steps\narr1 = np.random.randn(n_rows, int(n_cols / 4)).astype(\"f8\")\narr2 = np.random.randn(n_rows, int(n_cols / 2)).astype(\"i8\")\narr3 = np.random.randn(n_rows, int(n_cols / 4)).astype(\"f8\")\ndf2 = pd.concat(\n    [pd.DataFrame(arr1), pd.DataFrame(arr2), pd.DataFrame(arr3)],\n    axis=1,\n    ignore_index=True,\n)\n# should already be the case, but just to be sure\ndf2._consolidate_inplace()\n\nself.left = df\nself.right = df2", "path": "pandas/asv_bench/benchmarks/arithmetic.py", "commit_date": "2020-09-22 00:00:00", "repo_name": "pandas-dev/pandas", "stars": 41435, "license": "bsd-3-clause", "language": "python", "size": 352357}
{"docstring": "# GH#31300\n", "func_signal": "def setup(self):\n", "code": "arr = np.arange(10 ** 6)\ndf = DataFrame({\"A\": arr})\nser = df[\"A\"]\n\nself.df = df\nself.ser = ser", "path": "pandas/asv_bench/benchmarks/arithmetic.py", "commit_date": "2020-09-22 00:00:00", "repo_name": "pandas-dev/pandas", "stars": 41435, "license": "bsd-3-clause", "language": "python", "size": 352357}
{"docstring": "# Check that tz_localize behaves the same vectorized and pointwise.\n", "func_signal": "def _compare_local_to_utc(tz_didx, naive_didx):\n", "code": "err1 = err2 = None\ntry:\n    result = tzconversion.tz_localize_to_utc(naive_didx.asi8, tz_didx.tz)\n    err1 = None\nexcept Exception as err:\n    err1 = err\n\ntry:\n    expected = naive_didx.map(lambda x: x.tz_localize(tz_didx.tz)).asi8\nexcept Exception as err:\n    err2 = err\n\nif err1 is not None:\n    assert type(err1) == type(err2)\nelse:\n    assert err2 is None\n    tm.assert_numpy_array_equal(result, expected)", "path": "pandas/pandas/tests/tslibs/test_conversion.py", "commit_date": "2020-08-06 00:00:00", "repo_name": "pandas-dev/pandas", "stars": 41435, "license": "bsd-3-clause", "language": "python", "size": 352357}
{"docstring": "\"\"\"Calculate losses, gradients, and update network weights; called in every training iteration\"\"\"\n# forward\n", "func_signal": "def optimize_parameters(self):\n", "code": "self.forward()      # compute fake images and reconstruction images.\n# G_A and G_B\nself.set_requires_grad([self.netD_A, self.netD_B], False)  # Ds require no gradients when optimizing Gs\nself.optimizer_G.zero_grad()  # set G_A and G_B's gradients to zero\nself.backward_G()             # calculate gradients for G_A and G_B\nself.optimizer_G.step()       # update G_A and G_B's weights\n# D_A and D_B\nself.set_requires_grad([self.netD_A, self.netD_B], True)\nself.optimizer_D.zero_grad()   # set D_A and D_B's gradients to zero\nself.backward_D_A()      # calculate gradients for D_A\nself.backward_D_B()      # calculate graidents for D_B\nself.optimizer_D.step()  # update D_A and D_B's weights", "path": "pytorch-CycleGAN-and-pix2pix/models/cycle_gan_model.py", "commit_date": "2019-02-07 00:00:00", "repo_name": "junyanz/pytorch-CycleGAN-and-pix2pix", "stars": 21628, "license": "other", "language": "python", "size": 8391}
{"docstring": "\"\"\"Load and print networks; create schedulers\n\nParameters:\n    opt (Option class) -- stores all the experiment flags; needs to be a subclass of BaseOptions\n\"\"\"\n", "func_signal": "def setup(self, opt):\n", "code": "if self.isTrain:\n    self.schedulers = [networks.get_scheduler(optimizer, opt) for optimizer in self.optimizers]\nif not self.isTrain or opt.continue_train:\n    load_suffix = 'iter_%d' % opt.load_iter if opt.load_iter > 0 else opt.epoch\n    self.load_networks(load_suffix)\nself.print_networks(opt.verbose)", "path": "pytorch-CycleGAN-and-pix2pix/models/base_model.py", "commit_date": "2020-07-19 00:00:00", "repo_name": "junyanz/pytorch-CycleGAN-and-pix2pix", "stars": 21628, "license": "other", "language": "python", "size": 8391}
{"docstring": "\"\"\"Initialize the ImagePool class\n\nParameters:\n    pool_size (int) -- the size of image buffer, if pool_size=0, no buffer will be created\n\"\"\"\n", "func_signal": "def __init__(self, pool_size):\n", "code": "self.pool_size = pool_size\nif self.pool_size > 0:  # create an empty pool\n    self.num_imgs = 0\n    self.images = []", "path": "pytorch-CycleGAN-and-pix2pix/util/image_pool.py", "commit_date": "2019-01-04 00:00:00", "repo_name": "junyanz/pytorch-CycleGAN-and-pix2pix", "stars": 21628, "license": "other", "language": "python", "size": 8391}
{"docstring": "\"\"\"Run forward pass; called by both functions <optimize_parameters> and <test>.\"\"\"\n", "func_signal": "def forward(self):\n", "code": "self.fake_B = self.netG_A(self.real_A)  # G_A(A)\nself.rec_A = self.netG_B(self.fake_B)   # G_B(G_A(A))\nself.fake_A = self.netG_B(self.real_B)  # G_B(B)\nself.rec_B = self.netG_A(self.fake_A)   # G_A(G_B(B))", "path": "pytorch-CycleGAN-and-pix2pix/models/cycle_gan_model.py", "commit_date": "2019-02-07 00:00:00", "repo_name": "junyanz/pytorch-CycleGAN-and-pix2pix", "stars": 21628, "license": "other", "language": "python", "size": 8391}
{"docstring": "\"\"\"Save all the networks to the disk.\n\nParameters:\n    epoch (int) -- current epoch; used in the file name '%s_net_%s.pth' % (epoch, name)\n\"\"\"\n", "func_signal": "def save_networks(self, epoch):\n", "code": "for name in self.model_names:\n    if isinstance(name, str):\n        save_filename = '%s_net_%s.pth' % (epoch, name)\n        save_path = os.path.join(self.save_dir, save_filename)\n        net = getattr(self, 'net' + name)\n\n        if len(self.gpu_ids) > 0 and torch.cuda.is_available():\n            torch.save(net.module.cpu().state_dict(), save_path)\n            net.cuda(self.gpu_ids[0])\n        else:\n            torch.save(net.cpu().state_dict(), save_path)", "path": "pytorch-CycleGAN-and-pix2pix/models/base_model.py", "commit_date": "2020-07-19 00:00:00", "repo_name": "junyanz/pytorch-CycleGAN-and-pix2pix", "stars": 21628, "license": "other", "language": "python", "size": 8391}
{"docstring": "\"\"\"Add new dataset-specific options, and rewrite default values for existing options.\n\nParameters:\n    parser          -- original option parser\n    is_train (bool) -- whether training phase or test phase. You can use this flag to add training-specific or test-specific options.\n\nReturns:\n    the modified parser.\n\nFor CycleGAN, in addition to GAN losses, we introduce lambda_A, lambda_B, and lambda_identity for the following losses.\nA (source domain), B (target domain).\nGenerators: G_A: A -> B; G_B: B -> A.\nDiscriminators: D_A: G_A(A) vs. B; D_B: G_B(B) vs. A.\nForward cycle loss:  lambda_A * ||G_B(G_A(A)) - A|| (Eqn. (2) in the paper)\nBackward cycle loss: lambda_B * ||G_A(G_B(B)) - B|| (Eqn. (2) in the paper)\nIdentity loss (optional): lambda_identity * (||G_A(B) - B|| * lambda_B + ||G_B(A) - A|| * lambda_A) (Sec 5.2 \"Photo generation from paintings\" in the paper)\nDropout is not used in the original CycleGAN paper.\n\"\"\"\n", "func_signal": "def modify_commandline_options(parser, is_train=True):\n", "code": "parser.set_defaults(no_dropout=True)  # default CycleGAN did not use dropout\nif is_train:\n    parser.add_argument('--lambda_A', type=float, default=10.0, help='weight for cycle loss (A -> B -> A)')\n    parser.add_argument('--lambda_B', type=float, default=10.0, help='weight for cycle loss (B -> A -> B)')\n    parser.add_argument('--lambda_identity', type=float, default=0.5, help='use identity mapping. Setting lambda_identity other than 0 has an effect of scaling the weight of the identity mapping loss. For example, if the weight of the identity loss should be 10 times smaller than the weight of the reconstruction loss, please set lambda_identity = 0.1')\n\nreturn parser", "path": "pytorch-CycleGAN-and-pix2pix/models/cycle_gan_model.py", "commit_date": "2019-02-07 00:00:00", "repo_name": "junyanz/pytorch-CycleGAN-and-pix2pix", "stars": 21628, "license": "other", "language": "python", "size": 8391}
{"docstring": "\"\"\"Load all the networks from the disk.\n\nParameters:\n    epoch (int) -- current epoch; used in the file name '%s_net_%s.pth' % (epoch, name)\n\"\"\"\n", "func_signal": "def load_networks(self, epoch):\n", "code": "for name in self.model_names:\n    if isinstance(name, str):\n        load_filename = '%s_net_%s.pth' % (epoch, name)\n        load_path = os.path.join(self.save_dir, load_filename)\n        net = getattr(self, 'net' + name)\n        if isinstance(net, torch.nn.DataParallel):\n            net = net.module\n        print('loading the model from %s' % load_path)\n        # if you are using PyTorch newer than 0.4 (e.g., built from\n        # GitHub source), you can remove str() on self.device\n        state_dict = torch.load(load_path, map_location=str(self.device))\n        if hasattr(state_dict, '_metadata'):\n            del state_dict._metadata\n\n        # patch InstanceNorm checkpoints prior to 0.4\n        for key in list(state_dict.keys()):  # need to copy keys here because we mutate in loop\n            self.__patch_instance_norm_state_dict(state_dict, net, key.split('.'))\n        net.load_state_dict(state_dict)", "path": "pytorch-CycleGAN-and-pix2pix/models/base_model.py", "commit_date": "2020-07-19 00:00:00", "repo_name": "junyanz/pytorch-CycleGAN-and-pix2pix", "stars": 21628, "license": "other", "language": "python", "size": 8391}
{"docstring": "\"\"\"Initialize the CycleGAN class.\n\nParameters:\n    opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions\n\"\"\"\n", "func_signal": "def __init__(self, opt):\n", "code": "BaseModel.__init__(self, opt)\n# specify the training losses you want to print out. The training/test scripts will call <BaseModel.get_current_losses>\nself.loss_names = ['D_A', 'G_A', 'cycle_A', 'idt_A', 'D_B', 'G_B', 'cycle_B', 'idt_B']\n# specify the images you want to save/display. The training/test scripts will call <BaseModel.get_current_visuals>\nvisual_names_A = ['real_A', 'fake_B', 'rec_A']\nvisual_names_B = ['real_B', 'fake_A', 'rec_B']\nif self.isTrain and self.opt.lambda_identity > 0.0:  # if identity loss is used, we also visualize idt_B=G_A(B) ad idt_A=G_A(B)\n    visual_names_A.append('idt_B')\n    visual_names_B.append('idt_A')\n\nself.visual_names = visual_names_A + visual_names_B  # combine visualizations for A and B\n# specify the models you want to save to the disk. The training/test scripts will call <BaseModel.save_networks> and <BaseModel.load_networks>.\nif self.isTrain:\n    self.model_names = ['G_A', 'G_B', 'D_A', 'D_B']\nelse:  # during test time, only load Gs\n    self.model_names = ['G_A', 'G_B']\n\n# define networks (both Generators and discriminators)\n# The naming is different from those used in the paper.\n# Code (vs. paper): G_A (G), G_B (F), D_A (D_Y), D_B (D_X)\nself.netG_A = networks.define_G(opt.input_nc, opt.output_nc, opt.ngf, opt.netG, opt.norm,\n                                not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids)\nself.netG_B = networks.define_G(opt.output_nc, opt.input_nc, opt.ngf, opt.netG, opt.norm,\n                                not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids)\n\nif self.isTrain:  # define discriminators\n    self.netD_A = networks.define_D(opt.output_nc, opt.ndf, opt.netD,\n                                    opt.n_layers_D, opt.norm, opt.init_type, opt.init_gain, self.gpu_ids)\n    self.netD_B = networks.define_D(opt.input_nc, opt.ndf, opt.netD,\n                                    opt.n_layers_D, opt.norm, opt.init_type, opt.init_gain, self.gpu_ids)\n\nif self.isTrain:\n    if opt.lambda_identity > 0.0:  # only works when input and output images have the same number of channels\n        assert(opt.input_nc == opt.output_nc)\n    self.fake_A_pool = ImagePool(opt.pool_size)  # create image buffer to store previously generated images\n    self.fake_B_pool = ImagePool(opt.pool_size)  # create image buffer to store previously generated images\n    # define loss functions\n    self.criterionGAN = networks.GANLoss(opt.gan_mode).to(self.device)  # define GAN loss.\n    self.criterionCycle = torch.nn.L1Loss()\n    self.criterionIdt = torch.nn.L1Loss()\n    # initialize optimizers; schedulers will be automatically created by function <BaseModel.setup>.\n    self.optimizer_G = torch.optim.Adam(itertools.chain(self.netG_A.parameters(), self.netG_B.parameters()), lr=opt.lr, betas=(opt.beta1, 0.999))\n    self.optimizer_D = torch.optim.Adam(itertools.chain(self.netD_A.parameters(), self.netD_B.parameters()), lr=opt.lr, betas=(opt.beta1, 0.999))\n    self.optimizers.append(self.optimizer_G)\n    self.optimizers.append(self.optimizer_D)", "path": "pytorch-CycleGAN-and-pix2pix/models/cycle_gan_model.py", "commit_date": "2019-02-07 00:00:00", "repo_name": "junyanz/pytorch-CycleGAN-and-pix2pix", "stars": 21628, "license": "other", "language": "python", "size": 8391}
{"docstring": "\"\"\"Calculate the loss for generators G_A and G_B\"\"\"\n", "func_signal": "def backward_G(self):\n", "code": "lambda_idt = self.opt.lambda_identity\nlambda_A = self.opt.lambda_A\nlambda_B = self.opt.lambda_B\n# Identity loss\nif lambda_idt > 0:\n    # G_A should be identity if real_B is fed: ||G_A(B) - B||\n    self.idt_A = self.netG_A(self.real_B)\n    self.loss_idt_A = self.criterionIdt(self.idt_A, self.real_B) * lambda_B * lambda_idt\n    # G_B should be identity if real_A is fed: ||G_B(A) - A||\n    self.idt_B = self.netG_B(self.real_A)\n    self.loss_idt_B = self.criterionIdt(self.idt_B, self.real_A) * lambda_A * lambda_idt\nelse:\n    self.loss_idt_A = 0\n    self.loss_idt_B = 0\n\n# GAN loss D_A(G_A(A))\nself.loss_G_A = self.criterionGAN(self.netD_A(self.fake_B), True)\n# GAN loss D_B(G_B(B))\nself.loss_G_B = self.criterionGAN(self.netD_B(self.fake_A), True)\n# Forward cycle loss || G_B(G_A(A)) - A||\nself.loss_cycle_A = self.criterionCycle(self.rec_A, self.real_A) * lambda_A\n# Backward cycle loss || G_A(G_B(B)) - B||\nself.loss_cycle_B = self.criterionCycle(self.rec_B, self.real_B) * lambda_B\n# combined loss and calculate gradients\nself.loss_G = self.loss_G_A + self.loss_G_B + self.loss_cycle_A + self.loss_cycle_B + self.loss_idt_A + self.loss_idt_B\nself.loss_G.backward()", "path": "pytorch-CycleGAN-and-pix2pix/models/cycle_gan_model.py", "commit_date": "2019-02-07 00:00:00", "repo_name": "junyanz/pytorch-CycleGAN-and-pix2pix", "stars": 21628, "license": "other", "language": "python", "size": 8391}
{"docstring": "\"\"\"Initialize the BaseModel class.\n\nParameters:\n    opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions\n\nWhen creating your custom class, you need to implement your own initialization.\nIn this function, you should first call <BaseModel.__init__(self, opt)>\nThen, you need to define four lists:\n    -- self.loss_names (str list):          specify the training losses that you want to plot and save.\n    -- self.model_names (str list):         define networks used in our training.\n    -- self.visual_names (str list):        specify the images that you want to display and save.\n    -- self.optimizers (optimizer list):    define and initialize optimizers. You can define one optimizer for each network. If two networks are updated at the same time, you can use itertools.chain to group them. See cycle_gan_model.py for an example.\n\"\"\"\n", "func_signal": "def __init__(self, opt):\n", "code": "self.opt = opt\nself.gpu_ids = opt.gpu_ids\nself.isTrain = opt.isTrain\nself.device = torch.device('cuda:{}'.format(self.gpu_ids[0])) if self.gpu_ids else torch.device('cpu')  # get device name: CPU or GPU\nself.save_dir = os.path.join(opt.checkpoints_dir, opt.name)  # save all the checkpoints to save_dir\nif opt.preprocess != 'scale_width':  # with [scale_width], input images might have different sizes, which hurts the performance of cudnn.benchmark.\n    torch.backends.cudnn.benchmark = True\nself.loss_names = []\nself.model_names = []\nself.visual_names = []\nself.optimizers = []\nself.image_paths = []\nself.metric = 0  # used for learning rate policy 'plateau'", "path": "pytorch-CycleGAN-and-pix2pix/models/base_model.py", "commit_date": "2020-07-19 00:00:00", "repo_name": "junyanz/pytorch-CycleGAN-and-pix2pix", "stars": 21628, "license": "other", "language": "python", "size": 8391}
{"docstring": "\"\"\"Initialize the class; save the options in the class\n\nParameters:\n    opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions\n\"\"\"\n", "func_signal": "def __init__(self, opt):\n", "code": "self.opt = opt\nself.root = opt.dataroot", "path": "pytorch-CycleGAN-and-pix2pix/data/base_dataset.py", "commit_date": "2020-02-26 00:00:00", "repo_name": "junyanz/pytorch-CycleGAN-and-pix2pix", "stars": 21628, "license": "other", "language": "python", "size": 8391}
{"docstring": "\"\"\"Return traning losses / errors. train.py will print out these errors on console, and save them to a file\"\"\"\n", "func_signal": "def get_current_losses(self):\n", "code": "errors_ret = OrderedDict()\nfor name in self.loss_names:\n    if isinstance(name, str):\n        errors_ret[name] = float(getattr(self, 'loss_' + name))  # float(...) works for both scalar tensor and float number\nreturn errors_ret", "path": "pytorch-CycleGAN-and-pix2pix/models/base_model.py", "commit_date": "2020-07-19 00:00:00", "repo_name": "junyanz/pytorch-CycleGAN-and-pix2pix", "stars": 21628, "license": "other", "language": "python", "size": 8391}
{"docstring": "\"\"\"Print warning information about image size(only print once)\"\"\"\n", "func_signal": "def __print_size_warning(ow, oh, w, h):\n", "code": "if not hasattr(__print_size_warning, 'has_printed'):\n    print(\"The image size needs to be a multiple of 4. \"\n          \"The loaded image size was (%d, %d), so it was adjusted to \"\n          \"(%d, %d). This adjustment will be done to all images \"\n          \"whose sizes are not multiples of 4\" % (ow, oh, w, h))\n    __print_size_warning.has_printed = True", "path": "pytorch-CycleGAN-and-pix2pix/data/base_dataset.py", "commit_date": "2020-02-26 00:00:00", "repo_name": "junyanz/pytorch-CycleGAN-and-pix2pix", "stars": 21628, "license": "other", "language": "python", "size": 8391}
{"docstring": "\"\"\"Forward function used in test time.\n\nThis function wraps <forward> function in no_grad() so we don't save intermediate steps for backprop\nIt also calls <compute_visuals> to produce additional visualization results\n\"\"\"\n", "func_signal": "def test(self):\n", "code": "with torch.no_grad():\n    self.forward()\n    self.compute_visuals()", "path": "pytorch-CycleGAN-and-pix2pix/models/base_model.py", "commit_date": "2020-07-19 00:00:00", "repo_name": "junyanz/pytorch-CycleGAN-and-pix2pix", "stars": 21628, "license": "other", "language": "python", "size": 8391}
{"docstring": "\"\"\"Make models eval mode during test time\"\"\"\n", "func_signal": "def eval(self):\n", "code": "for name in self.model_names:\n    if isinstance(name, str):\n        net = getattr(self, 'net' + name)\n        net.eval()", "path": "pytorch-CycleGAN-and-pix2pix/models/base_model.py", "commit_date": "2020-07-19 00:00:00", "repo_name": "junyanz/pytorch-CycleGAN-and-pix2pix", "stars": 21628, "license": "other", "language": "python", "size": 8391}
{"docstring": "\"\"\"Return an image from the pool.\n\nParameters:\n    images: the latest generated images from the generator\n\nReturns images from the buffer.\n\nBy 50/100, the buffer will return input images.\nBy 50/100, the buffer will return images previously stored in the buffer,\nand insert the current images to the buffer.\n\"\"\"\n", "func_signal": "def query(self, images):\n", "code": "if self.pool_size == 0:  # if the buffer size is 0, do nothing\n    return images\nreturn_images = []\nfor image in images:\n    image = torch.unsqueeze(image.data, 0)\n    if self.num_imgs < self.pool_size:   # if the buffer is not full; keep inserting current images to the buffer\n        self.num_imgs = self.num_imgs + 1\n        self.images.append(image)\n        return_images.append(image)\n    else:\n        p = random.uniform(0, 1)\n        if p > 0.5:  # by 50% chance, the buffer will return a previously stored image, and insert the current image into the buffer\n            random_id = random.randint(0, self.pool_size - 1)  # randint is inclusive\n            tmp = self.images[random_id].clone()\n            self.images[random_id] = image\n            return_images.append(tmp)\n        else:       # by another 50% chance, the buffer will return the current image\n            return_images.append(image)\nreturn_images = torch.cat(return_images, 0)   # collect all the images and return\nreturn return_images", "path": "pytorch-CycleGAN-and-pix2pix/util/image_pool.py", "commit_date": "2019-01-04 00:00:00", "repo_name": "junyanz/pytorch-CycleGAN-and-pix2pix", "stars": 21628, "license": "other", "language": "python", "size": 8391}
{"docstring": "\"\"\"Calculate GAN loss for the discriminator\n\nParameters:\n    netD (network)      -- the discriminator D\n    real (tensor array) -- real images\n    fake (tensor array) -- images generated by a generator\n\nReturn the discriminator loss.\nWe also call loss_D.backward() to calculate the gradients.\n\"\"\"\n# Real\n", "func_signal": "def backward_D_basic(self, netD, real, fake):\n", "code": "pred_real = netD(real)\nloss_D_real = self.criterionGAN(pred_real, True)\n# Fake\npred_fake = netD(fake.detach())\nloss_D_fake = self.criterionGAN(pred_fake, False)\n# Combined loss and calculate gradients\nloss_D = (loss_D_real + loss_D_fake) * 0.5\nloss_D.backward()\nreturn loss_D", "path": "pytorch-CycleGAN-and-pix2pix/models/cycle_gan_model.py", "commit_date": "2019-02-07 00:00:00", "repo_name": "junyanz/pytorch-CycleGAN-and-pix2pix", "stars": 21628, "license": "other", "language": "python", "size": 8391}
{"docstring": "\"\"\"Calculate GAN loss for discriminator D_B\"\"\"\n", "func_signal": "def backward_D_B(self):\n", "code": "fake_A = self.fake_A_pool.query(self.fake_A)\nself.loss_D_B = self.backward_D_basic(self.netD_B, self.real_A, fake_A)", "path": "pytorch-CycleGAN-and-pix2pix/models/cycle_gan_model.py", "commit_date": "2019-02-07 00:00:00", "repo_name": "junyanz/pytorch-CycleGAN-and-pix2pix", "stars": 21628, "license": "other", "language": "python", "size": 8391}
{"docstring": "\"\"\"Update learning rates for all the networks; called at the end of every epoch\"\"\"\n", "func_signal": "def update_learning_rate(self):\n", "code": "old_lr = self.optimizers[0].param_groups[0]['lr']\nfor scheduler in self.schedulers:\n    if self.opt.lr_policy == 'plateau':\n        scheduler.step(self.metric)\n    else:\n        scheduler.step()\n\nlr = self.optimizers[0].param_groups[0]['lr']\nprint('learning rate %.7f -> %.7f' % (old_lr, lr))", "path": "pytorch-CycleGAN-and-pix2pix/models/base_model.py", "commit_date": "2020-07-19 00:00:00", "repo_name": "junyanz/pytorch-CycleGAN-and-pix2pix", "stars": 21628, "license": "other", "language": "python", "size": 8391}
{"docstring": "\"\"\"Unpack input data from the dataloader and perform necessary pre-processing steps.\n\nParameters:\n    input (dict): include the data itself and its metadata information.\n\nThe option 'direction' can be used to swap domain A and domain B.\n\"\"\"\n", "func_signal": "def set_input(self, input):\n", "code": "AtoB = self.opt.direction == 'AtoB'\nself.real_A = input['A' if AtoB else 'B'].to(self.device)\nself.real_B = input['B' if AtoB else 'A'].to(self.device)\nself.image_paths = input['A_paths' if AtoB else 'B_paths']", "path": "pytorch-CycleGAN-and-pix2pix/models/cycle_gan_model.py", "commit_date": "2019-02-07 00:00:00", "repo_name": "junyanz/pytorch-CycleGAN-and-pix2pix", "stars": 21628, "license": "other", "language": "python", "size": 8391}
{"docstring": "\"\"\"Draw 3-pixel width bounding boxes on the given image array.\ncolor: list of 3 int values for RGB.\n\"\"\"\n", "func_signal": "def draw_box(image, box, color):\n", "code": "y1, x1, y2, x2 = box\nimage[y1:y1 + 2, x1:x2] = color\nimage[y2:y2 + 2, x1:x2] = color\nimage[y1:y2, x1:x1 + 2] = color\nimage[y1:y2, x2:x2 + 2] = color\nreturn image", "path": "Mask_RCNN/mrcnn/visualize.py", "commit_date": "2019-02-03 00:00:00", "repo_name": "matterport/Mask_RCNN", "stars": 23972, "license": "other", "language": "python", "size": 122597}
{"docstring": "\"\"\"Creates random specifications of an image with multiple shapes.\nReturns the background color of the image and a list of shape\nspecifications that can be used to draw the image.\n\"\"\"\n# Pick random background color\n", "func_signal": "def random_image(self, height, width):\n", "code": "bg_color = np.array([random.randint(0, 255) for _ in range(3)])\n# Generate a few random shapes and record their\n# bounding boxes\nshapes = []\nboxes = []\nN = random.randint(1, 4)\nfor _ in range(N):\n    shape, color, dims = self.random_shape(height, width)\n    shapes.append((shape, color, dims))\n    x, y, s = dims\n    boxes.append([y - s, x - s, y + s, x + s])\n# Apply non-max suppression wit 0.3 threshold to avoid\n# shapes covering each other\nkeep_ixs = utils.non_max_suppression(\n    np.array(boxes), np.arange(N), 0.3)\nshapes = [s for i, s in enumerate(shapes) if i in keep_ixs]\nreturn bg_color, shapes", "path": "Mask_RCNN/samples/shapes/shapes.py", "commit_date": "2018-04-07 00:00:00", "repo_name": "matterport/Mask_RCNN", "stars": 23972, "license": "other", "language": "python", "size": 122597}
{"docstring": "\"\"\"Apply the given mask to the image.\n\"\"\"\n", "func_signal": "def apply_mask(image, mask, color, alpha=0.5):\n", "code": "for c in range(3):\n    image[:, :, c] = np.where(mask == 1,\n                              image[:, :, c] *\n                              (1 - alpha) + alpha * color[c] * 255,\n                              image[:, :, c])\nreturn image", "path": "Mask_RCNN/mrcnn/visualize.py", "commit_date": "2019-02-03 00:00:00", "repo_name": "matterport/Mask_RCNN", "stars": 23972, "license": "other", "language": "python", "size": 122597}
{"docstring": "\"\"\"Return the shapes data of the image.\"\"\"\n", "func_signal": "def image_reference(self, image_id):\n", "code": "info = self.image_info[image_id]\nif info[\"source\"] == \"shapes\":\n    return info[\"shapes\"]\nelse:\n    super(self.__class__).image_reference(self, image_id)", "path": "Mask_RCNN/samples/shapes/shapes.py", "commit_date": "2018-04-07 00:00:00", "repo_name": "matterport/Mask_RCNN", "stars": 23972, "license": "other", "language": "python", "size": 122597}
{"docstring": "\"\"\"Load a subset of the nuclei dataset.\n\ndataset_dir: Root directory of the dataset\nsubset: Subset to load. Either the name of the sub-directory,\n        such as stage1_train, stage1_test, ...etc. or, one of:\n        * train: stage1_train excluding validation images\n        * val: validation images from VAL_IMAGE_IDS\n\"\"\"\n# Add classes. We have one class.\n# Naming the dataset nucleus, and the class nucleus\n", "func_signal": "def load_nucleus(self, dataset_dir, subset):\n", "code": "self.add_class(\"nucleus\", 1, \"nucleus\")\n\n# Which subset?\n# \"val\": use hard-coded list above\n# \"train\": use data from stage1_train minus the hard-coded list above\n# else: use the data from the specified sub-directory\nassert subset in [\"train\", \"val\", \"stage1_train\", \"stage1_test\", \"stage2_test\"]\nsubset_dir = \"stage1_train\" if subset in [\"train\", \"val\"] else subset\ndataset_dir = os.path.join(dataset_dir, subset_dir)\nif subset == \"val\":\n    image_ids = VAL_IMAGE_IDS\nelse:\n    # Get image ids from directory names\n    image_ids = next(os.walk(dataset_dir))[1]\n    if subset == \"train\":\n        image_ids = list(set(image_ids) - set(VAL_IMAGE_IDS))\n\n# Add images\nfor image_id in image_ids:\n    self.add_image(\n        \"nucleus\",\n        image_id=image_id,\n        path=os.path.join(dataset_dir, image_id, \"images/{}.png\".format(image_id)))", "path": "Mask_RCNN/samples/nucleus/nucleus.py", "commit_date": "2018-06-06 00:00:00", "repo_name": "matterport/Mask_RCNN", "stars": 23972, "license": "other", "language": "python", "size": 122597}
{"docstring": "\"\"\"Display values in a table format.\ntable: an iterable of rows, and each row is an iterable of values.\n\"\"\"\n", "func_signal": "def display_table(table):\n", "code": "html = \"\"\nfor row in table:\n    row_html = \"\"\n    for col in row:\n        row_html += \"<td>{:40}</td>\".format(str(col))\n    html += \"<tr>\" + row_html + \"</tr>\"\nhtml = \"<table>\" + html + \"</table>\"\nIPython.display.display(IPython.display.HTML(html))", "path": "Mask_RCNN/mrcnn/visualize.py", "commit_date": "2019-02-03 00:00:00", "repo_name": "matterport/Mask_RCNN", "stars": 23972, "license": "other", "language": "python", "size": 122597}
{"docstring": "\"\"\"Run detection on images in the given directory.\"\"\"\n", "func_signal": "def detect(model, dataset_dir, subset):\n", "code": "print(\"Running on {}\".format(dataset_dir))\n\n# Create directory\nif not os.path.exists(RESULTS_DIR):\n    os.makedirs(RESULTS_DIR)\nsubmit_dir = \"submit_{:%Y%m%dT%H%M%S}\".format(datetime.datetime.now())\nsubmit_dir = os.path.join(RESULTS_DIR, submit_dir)\nos.makedirs(submit_dir)\n\n# Read dataset\ndataset = NucleusDataset()\ndataset.load_nucleus(dataset_dir, subset)\ndataset.prepare()\n# Load over images\nsubmission = []\nfor image_id in dataset.image_ids:\n    # Load image and run detection\n    image = dataset.load_image(image_id)\n    # Detect objects\n    r = model.detect([image], verbose=0)[0]\n    # Encode image to RLE. Returns a string of multiple lines\n    source_id = dataset.image_info[image_id][\"id\"]\n    rle = mask_to_rle(source_id, r[\"masks\"], r[\"scores\"])\n    submission.append(rle)\n    # Save image with masks\n    visualize.display_instances(\n        image, r['rois'], r['masks'], r['class_ids'],\n        dataset.class_names, r['scores'],\n        show_bbox=False, show_mask=False,\n        title=\"Predictions\")\n    plt.savefig(\"{}/{}.png\".format(submit_dir, dataset.image_info[image_id][\"id\"]))\n\n# Save to csv file\nsubmission = \"ImageId,EncodedPixels\\n\" + \"\\n\".join(submission)\nfile_path = os.path.join(submit_dir, \"submit.csv\")\nwith open(file_path, \"w\") as f:\n    f.write(submission)\nprint(\"Saved to \", submit_dir)", "path": "Mask_RCNN/samples/nucleus/nucleus.py", "commit_date": "2018-06-06 00:00:00", "repo_name": "matterport/Mask_RCNN", "stars": 23972, "license": "other", "language": "python", "size": 122597}
{"docstring": "\"\"\"Generate instance masks for an image.\n       Returns:\nmasks: A bool array of shape [height, width, instance count] with\n    one mask per instance.\nclass_ids: a 1D array of class IDs of the instance masks.\n\"\"\"\n", "func_signal": "def load_mask(self, image_id):\n", "code": "info = self.image_info[image_id]\n# Get mask directory from image path\nmask_dir = os.path.join(os.path.dirname(os.path.dirname(info['path'])), \"masks\")\n\n# Read mask files from .png image\nmask = []\nfor f in next(os.walk(mask_dir))[2]:\n    if f.endswith(\".png\"):\n        m = skimage.io.imread(os.path.join(mask_dir, f)).astype(np.bool)\n        mask.append(m)\nmask = np.stack(mask, axis=-1)\n# Return mask, and array of class IDs of each instance. Since we have\n# one class ID, we return an array of ones\nreturn mask, np.ones([mask.shape[-1]], dtype=np.int32)", "path": "Mask_RCNN/samples/nucleus/nucleus.py", "commit_date": "2018-06-06 00:00:00", "repo_name": "matterport/Mask_RCNN", "stars": 23972, "license": "other", "language": "python", "size": 122597}
{"docstring": "\"\"\"Generate instance masks for shapes of the given image ID.\n\"\"\"\n", "func_signal": "def load_mask(self, image_id):\n", "code": "info = self.image_info[image_id]\nshapes = info['shapes']\ncount = len(shapes)\nmask = np.zeros([info['height'], info['width'], count], dtype=np.uint8)\nfor i, (shape, _, dims) in enumerate(info['shapes']):\n    mask[:, :, i:i + 1] = self.draw_shape(mask[:, :, i:i + 1].copy(),\n                                          shape, dims, 1)\n# Handle occlusions\nocclusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\nfor i in range(count - 2, -1, -1):\n    mask[:, :, i] = mask[:, :, i] * occlusion\n    occlusion = np.logical_and(\n        occlusion, np.logical_not(mask[:, :, i]))\n# Map class names to class IDs.\nclass_ids = np.array([self.class_names.index(s[0]) for s in shapes])\nreturn mask, class_ids.astype(np.int32)", "path": "Mask_RCNN/samples/shapes/shapes.py", "commit_date": "2018-04-07 00:00:00", "repo_name": "matterport/Mask_RCNN", "stars": 23972, "license": "other", "language": "python", "size": 122597}
{"docstring": "\"\"\"Scans all the weights in the model and returns a list of tuples\nthat contain stats about each weight.\n\"\"\"\n", "func_signal": "def display_weight_stats(model):\n", "code": "layers = model.get_trainable_layers()\ntable = [[\"WEIGHT NAME\", \"SHAPE\", \"MIN\", \"MAX\", \"STD\"]]\nfor l in layers:\n    weight_values = l.get_weights()  # list of Numpy arrays\n    weight_tensors = l.weights  # list of TF tensors\n    for i, w in enumerate(weight_values):\n        weight_name = weight_tensors[i].name\n        # Detect problematic layers. Exclude biases of conv layers.\n        alert = \"\"\n        if w.min() == w.max() and not (l.__class__.__name__ == \"Conv2D\" and i == 1):\n            alert += \"<span style='color:red'>*** dead?</span>\"\n        if np.abs(w.min()) > 1000 or np.abs(w.max()) > 1000:\n            alert += \"<span style='color:red'>*** Overflow?</span>\"\n        # Add row\n        table.append([\n            weight_name + alert,\n            str(w.shape),\n            \"{:+9.4f}\".format(w.min()),\n            \"{:+10.4f}\".format(w.max()),\n            \"{:+9.4f}\".format(w.std()),\n        ])\ndisplay_table(table)", "path": "Mask_RCNN/mrcnn/visualize.py", "commit_date": "2019-02-03 00:00:00", "repo_name": "matterport/Mask_RCNN", "stars": 23972, "license": "other", "language": "python", "size": 122597}
{"docstring": "\"\"\"Train the model.\"\"\"\n# Training dataset.\n", "func_signal": "def train(model, dataset_dir, subset):\n", "code": "dataset_train = NucleusDataset()\ndataset_train.load_nucleus(dataset_dir, subset)\ndataset_train.prepare()\n\n# Validation dataset\ndataset_val = NucleusDataset()\ndataset_val.load_nucleus(dataset_dir, \"val\")\ndataset_val.prepare()\n\n# Image augmentation\n# http://imgaug.readthedocs.io/en/latest/source/augmenters.html\naugmentation = iaa.SomeOf((0, 2), [\n    iaa.Fliplr(0.5),\n    iaa.Flipud(0.5),\n    iaa.OneOf([iaa.Affine(rotate=90),\n               iaa.Affine(rotate=180),\n               iaa.Affine(rotate=270)]),\n    iaa.Multiply((0.8, 1.5)),\n    iaa.GaussianBlur(sigma=(0.0, 5.0))\n])\n\n# *** This training schedule is an example. Update to your needs ***\n\n# If starting from imagenet, train heads only for a bit\n# since they have random weights\nprint(\"Train network heads\")\nmodel.train(dataset_train, dataset_val,\n            learning_rate=config.LEARNING_RATE,\n            epochs=20,\n            augmentation=augmentation,\n            layers='heads')\n\nprint(\"Train all layers\")\nmodel.train(dataset_train, dataset_val,\n            learning_rate=config.LEARNING_RATE,\n            epochs=40,\n            augmentation=augmentation,\n            layers='all')", "path": "Mask_RCNN/samples/nucleus/nucleus.py", "commit_date": "2018-06-06 00:00:00", "repo_name": "matterport/Mask_RCNN", "stars": 23972, "license": "other", "language": "python", "size": 122597}
{"docstring": "\"\"\"Generate an image from the specs of the given image ID.\nTypically this function loads the image from a file, but\nin this case it generates the image on the fly from the\nspecs in image_info.\n\"\"\"\n", "func_signal": "def load_image(self, image_id):\n", "code": "info = self.image_info[image_id]\nbg_color = np.array(info['bg_color']).reshape([1, 1, 3])\nimage = np.ones([info['height'], info['width'], 3], dtype=np.uint8)\nimage = image * bg_color.astype(np.uint8)\nfor shape, color, dims in info['shapes']:\n    image = self.draw_shape(image, shape, dims, color)\nreturn image", "path": "Mask_RCNN/samples/shapes/shapes.py", "commit_date": "2018-04-07 00:00:00", "repo_name": "matterport/Mask_RCNN", "stars": 23972, "license": "other", "language": "python", "size": 122597}
{"docstring": "\"\"\"Draw the precision-recall curve.\n\nAP: Average precision at IoU >= 0.5\nprecisions: list of precision values\nrecalls: list of recall values\n\"\"\"\n# Plot the Precision-Recall curve\n", "func_signal": "def plot_precision_recall(AP, precisions, recalls):\n", "code": "_, ax = plt.subplots(1)\nax.set_title(\"Precision-Recall Curve. AP@50 = {:.3f}\".format(AP))\nax.set_ylim(0, 1.1)\nax.set_xlim(0, 1.1)\n_ = ax.plot(recalls, precisions)", "path": "Mask_RCNN/mrcnn/visualize.py", "commit_date": "2019-02-03 00:00:00", "repo_name": "matterport/Mask_RCNN", "stars": 23972, "license": "other", "language": "python", "size": 122597}
{"docstring": "\"\"\"\nanchors: [n, (y1, x1, y2, x2)] list of anchors in image coordinates.\nproposals: [n, 4] the same anchors but refined to fit objects better.\n\"\"\"\n", "func_signal": "def draw_rois(image, rois, refined_rois, mask, class_ids, class_names, limit=10):\n", "code": "masked_image = image.copy()\n\n# Pick random anchors in case there are too many.\nids = np.arange(rois.shape[0], dtype=np.int32)\nids = np.random.choice(\n    ids, limit, replace=False) if ids.shape[0] > limit else ids\n\nfig, ax = plt.subplots(1, figsize=(12, 12))\nif rois.shape[0] > limit:\n    plt.title(\"Showing {} random ROIs out of {}\".format(\n        len(ids), rois.shape[0]))\nelse:\n    plt.title(\"{} ROIs\".format(len(ids)))\n\n# Show area outside image boundaries.\nax.set_ylim(image.shape[0] + 20, -20)\nax.set_xlim(-50, image.shape[1] + 20)\nax.axis('off')\n\nfor i, id in enumerate(ids):\n    color = np.random.rand(3)\n    class_id = class_ids[id]\n    # ROI\n    y1, x1, y2, x2 = rois[id]\n    p = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2,\n                          edgecolor=color if class_id else \"gray\",\n                          facecolor='none', linestyle=\"dashed\")\n    ax.add_patch(p)\n    # Refined ROI\n    if class_id:\n        ry1, rx1, ry2, rx2 = refined_rois[id]\n        p = patches.Rectangle((rx1, ry1), rx2 - rx1, ry2 - ry1, linewidth=2,\n                              edgecolor=color, facecolor='none')\n        ax.add_patch(p)\n        # Connect the top-left corners of the anchor and proposal for easy visualization\n        ax.add_line(lines.Line2D([x1, rx1], [y1, ry1], color=color))\n\n        # Label\n        label = class_names[class_id]\n        ax.text(rx1, ry1 + 8, \"{}\".format(label),\n                color='w', size=11, backgroundcolor=\"none\")\n\n        # Mask\n        m = utils.unmold_mask(mask[id], rois[id]\n                              [:4].astype(np.int32), image.shape)\n        masked_image = apply_mask(masked_image, m, color)\n\nax.imshow(masked_image)\n\n# Print stats\nprint(\"Positive ROIs: \", class_ids[class_ids > 0].shape[0])\nprint(\"Negative ROIs: \", class_ids[class_ids == 0].shape[0])\nprint(\"Positive Ratio: {:.2f}\".format(\n    class_ids[class_ids > 0].shape[0] / class_ids.shape[0]))", "path": "Mask_RCNN/mrcnn/visualize.py", "commit_date": "2019-02-03 00:00:00", "repo_name": "matterport/Mask_RCNN", "stars": 23972, "license": "other", "language": "python", "size": 122597}
{"docstring": "\"\"\"Decodes an RLE encoded list of space separated\nnumbers and returns a binary mask.\"\"\"\n", "func_signal": "def rle_decode(rle, shape):\n", "code": "rle = list(map(int, rle.split()))\nrle = np.array(rle, dtype=np.int32).reshape([-1, 2])\nrle[:, 1] += rle[:, 0]\nrle -= 1\nmask = np.zeros([shape[0] * shape[1]], np.bool)\nfor s, e in rle:\n    assert 0 <= s < mask.shape[0]\n    assert 1 <= e <= mask.shape[0], \"shape: {}  s {}  e {}\".format(shape, s, e)\n    mask[s:e] = 1\n# Reshape and transpose\nmask = mask.reshape([shape[1], shape[0]]).T\nreturn mask", "path": "Mask_RCNN/samples/nucleus/nucleus.py", "commit_date": "2018-06-06 00:00:00", "repo_name": "matterport/Mask_RCNN", "stars": 23972, "license": "other", "language": "python", "size": 122597}
{"docstring": "\"\"\"Draws a shape from the given specs.\"\"\"\n# Get the center x, y and the size s\n", "func_signal": "def draw_shape(self, image, shape, dims, color):\n", "code": "x, y, s = dims\nif shape == 'square':\n    image = cv2.rectangle(image, (x - s, y - s),\n                          (x + s, y + s), color, -1)\nelif shape == \"circle\":\n    image = cv2.circle(image, (x, y), s, color, -1)\nelif shape == \"triangle\":\n    points = np.array([[(x, y - s),\n                        (x - s / math.sin(math.radians(60)), y + s),\n                        (x + s / math.sin(math.radians(60)), y + s),\n                        ]], dtype=np.int32)\n    image = cv2.fillPoly(image, points, color)\nreturn image", "path": "Mask_RCNN/samples/shapes/shapes.py", "commit_date": "2018-04-07 00:00:00", "repo_name": "matterport/Mask_RCNN", "stars": 23972, "license": "other", "language": "python", "size": 122597}
{"docstring": "\"\"\"Generate the requested number of synthetic images.\ncount: number of images to generate.\nheight, width: the size of the generated images.\n\"\"\"\n# Add classes\n", "func_signal": "def load_shapes(self, count, height, width):\n", "code": "self.add_class(\"shapes\", 1, \"square\")\nself.add_class(\"shapes\", 2, \"circle\")\nself.add_class(\"shapes\", 3, \"triangle\")\n\n# Add images\n# Generate random specifications of images (i.e. color and\n# list of shapes sizes and locations). This is more compact than\n# actual images. Images are generated on the fly in load_image().\nfor i in range(count):\n    bg_color, shapes = self.random_image(height, width)\n    self.add_image(\"shapes\", image_id=i, path=None,\n                   width=width, height=height,\n                   bg_color=bg_color, shapes=shapes)", "path": "Mask_RCNN/samples/shapes/shapes.py", "commit_date": "2018-04-07 00:00:00", "repo_name": "matterport/Mask_RCNN", "stars": 23972, "license": "other", "language": "python", "size": 122597}
{"docstring": "\"\"\"Generates specifications of a random shape that lies within\nthe given height and width boundaries.\nReturns a tuple of three valus:\n* The shape name (square, circle, ...)\n* Shape color: a tuple of 3 values, RGB.\n* Shape dimensions: A tuple of values that define the shape size\n                    and location. Differs per shape type.\n\"\"\"\n# Shape\n", "func_signal": "def random_shape(self, height, width):\n", "code": "shape = random.choice([\"square\", \"circle\", \"triangle\"])\n# Color\ncolor = tuple([random.randint(0, 255) for _ in range(3)])\n# Center x, y\nbuffer = 20\ny = random.randint(buffer, height - buffer - 1)\nx = random.randint(buffer, width - buffer - 1)\n# Size\ns = random.randint(buffer, height // 4)\nreturn shape, color, (x, y, s)", "path": "Mask_RCNN/samples/shapes/shapes.py", "commit_date": "2018-04-07 00:00:00", "repo_name": "matterport/Mask_RCNN", "stars": 23972, "license": "other", "language": "python", "size": 122597}
{"docstring": "\"\"\"Display the given image and the top few class masks.\"\"\"\n", "func_signal": "def display_top_masks(image, mask, class_ids, class_names, limit=4):\n", "code": "to_display = []\ntitles = []\nto_display.append(image)\ntitles.append(\"H x W={}x{}\".format(image.shape[0], image.shape[1]))\n# Pick top prominent classes in this image\nunique_class_ids = np.unique(class_ids)\nmask_area = [np.sum(mask[:, :, np.where(class_ids == i)[0]])\n             for i in unique_class_ids]\ntop_ids = [v[0] for v in sorted(zip(unique_class_ids, mask_area),\n                                key=lambda r: r[1], reverse=True) if v[1] > 0]\n# Generate images and titles\nfor i in range(limit):\n    class_id = top_ids[i] if i < len(top_ids) else -1\n    # Pull masks of instances belonging to the same class.\n    m = mask[:, :, np.where(class_ids == class_id)[0]]\n    m = np.sum(m * np.arange(1, m.shape[-1] + 1), -1)\n    to_display.append(m)\n    titles.append(class_names[class_id] if class_id != -1 else \"-\")\ndisplay_images(to_display, titles=titles, cols=limit + 1, cmap=\"Blues_r\")", "path": "Mask_RCNN/mrcnn/visualize.py", "commit_date": "2019-02-03 00:00:00", "repo_name": "matterport/Mask_RCNN", "stars": 23972, "license": "other", "language": "python", "size": 122597}
{"docstring": "\"\"\"Encodes a mask in Run Length Encoding (RLE).\nReturns a string of space-separated values.\n\"\"\"\n", "func_signal": "def rle_encode(mask):\n", "code": "assert mask.ndim == 2, \"Mask must be of shape [Height, Width]\"\n# Flatten it column wise\nm = mask.T.flatten()\n# Compute gradient. Equals 1 or -1 at transition points\ng = np.diff(np.concatenate([[0], m, [0]]), n=1)\n# 1-based indicies of transition points (where gradient != 0)\nrle = np.where(g != 0)[0].reshape([-1, 2]) + 1\n# Convert second index in each pair to lenth\nrle[:, 1] = rle[:, 1] - rle[:, 0]\nreturn \" \".join(map(str, rle.flatten()))", "path": "Mask_RCNN/samples/nucleus/nucleus.py", "commit_date": "2018-06-06 00:00:00", "repo_name": "matterport/Mask_RCNN", "stars": 23972, "license": "other", "language": "python", "size": 122597}
{"docstring": "\"\"\"Get version information or return default if unable to do so.\"\"\"\n# I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n# __file__, we can work backwards from there to the root. Some\n# py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n# case we can only use expanded keywords.\n\n", "func_signal": "def get_versions():\n", "code": "cfg = get_config()\nverbose = cfg.verbose\n\ntry:\n    return git_versions_from_keywords(get_keywords(), cfg.tag_prefix, verbose)\nexcept NotThisMethod:\n    pass\n\ntry:\n    root = os.path.realpath(__file__)\n    # versionfile_source is the relative path from the top of the source\n    # tree (where the .git directory might live) to this file. Invert\n    # this to find the root from __file__.\n    for i in cfg.versionfile_source.split(\"/\"):\n        root = os.path.dirname(root)\nexcept NameError:\n    return {\n        \"version\": \"0+unknown\",\n        \"full-revisionid\": None,\n        \"dirty\": None,\n        \"error\": \"unable to find root of source tree\",\n        \"date\": None,\n    }\n\ntry:\n    pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)\n    return render(pieces, cfg.style)\nexcept NotThisMethod:\n    pass\n\ntry:\n    if cfg.parentdir_prefix:\n        return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\nexcept NotThisMethod:\n    pass\n\nreturn {\n    \"version\": \"0+unknown\",\n    \"full-revisionid\": None,\n    \"dirty\": None,\n    \"error\": \"unable to compute version\",\n    \"date\": None,\n}", "path": "modin/modin/_version.py", "commit_date": "2020-01-22 00:00:00", "repo_name": "modin-project/modin", "stars": 9371, "license": "apache-2.0", "language": "python", "size": 51945}
{"docstring": "\"\"\"TAG[.postDISTANCE[.dev0]] .\n\nThe \".dev0\" means dirty.\n\nEexceptions:\n1: no tags. 0.postDISTANCE[.dev0]\n\"\"\"\n", "func_signal": "def render_pep440_old(pieces):\n", "code": "if pieces[\"closest-tag\"]:\n    rendered = pieces[\"closest-tag\"]\n    if pieces[\"distance\"] or pieces[\"dirty\"]:\n        rendered += \".post%d\" % pieces[\"distance\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dev0\"\nelse:\n    # exception #1\n    rendered = \"0.post%d\" % pieces[\"distance\"]\n    if pieces[\"dirty\"]:\n        rendered += \".dev0\"\nreturn rendered", "path": "modin/modin/_version.py", "commit_date": "2020-01-22 00:00:00", "repo_name": "modin-project/modin", "stars": 9371, "license": "apache-2.0", "language": "python", "size": 51945}
{"docstring": "# We are not testing when limit is not positive until pandas-27042 gets fixed.\n# We are not testing when axis is over rows until pandas-17399 gets fixed.\n", "func_signal": "def test_fillna(data, method, axis, limit):\n", "code": "if limit > 0 and axis != 1 and axis != \"columns\":\n    modin_df = pd.DataFrame(data)\n    pandas_df = pandas.DataFrame(data)\n\n    try:\n        pandas_result = pandas_df.fillna(0, method=method, axis=axis, limit=limit)\n    except Exception as e:\n        with pytest.raises(type(e)):\n            modin_df.fillna(0, method=method, axis=axis, limit=limit)\n    else:\n        modin_result = modin_df.fillna(0, method=method, axis=axis, limit=limit)\n        df_equals(modin_result, pandas_result)", "path": "modin/modin/pandas/test/dataframe/test_window.py", "commit_date": "2020-09-09 00:00:00", "repo_name": "modin-project/modin", "stars": 9371, "license": "apache-2.0", "language": "python", "size": 51945}
{"docstring": "\"\"\"Build up version string, with post-release \"local version identifier\".\n\nOur goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\nget a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty\n\nExceptions:\n1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n\"\"\"\n", "func_signal": "def render_pep440(pieces):\n", "code": "if pieces[\"closest-tag\"]:\n    rendered = pieces[\"closest-tag\"]\n    if pieces[\"distance\"] or pieces[\"dirty\"]:\n        rendered += plus_or_dot(pieces)\n        rendered += \"%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n        if pieces[\"dirty\"]:\n            rendered += \".dirty\"\nelse:\n    # exception #1\n    rendered = \"0+untagged.%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n    if pieces[\"dirty\"]:\n        rendered += \".dirty\"\nreturn rendered", "path": "modin/modin/_version.py", "commit_date": "2020-01-22 00:00:00", "repo_name": "modin-project/modin", "stars": 9371, "license": "apache-2.0", "language": "python", "size": 51945}
{"docstring": "\"\"\"TAG[.postDISTANCE[.dev0]+gHEX] .\n\nThe \".dev0\" means dirty. Note that .dev0 sorts backwards\n(a dirty tree will appear \"older\" than the corresponding clean one),\nbut you shouldn't be releasing software with -dirty anyways.\n\nExceptions:\n1: no tags. 0.postDISTANCE[.dev0]\n\"\"\"\n", "func_signal": "def render_pep440_post(pieces):\n", "code": "if pieces[\"closest-tag\"]:\n    rendered = pieces[\"closest-tag\"]\n    if pieces[\"distance\"] or pieces[\"dirty\"]:\n        rendered += \".post%d\" % pieces[\"distance\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dev0\"\n        rendered += plus_or_dot(pieces)\n        rendered += \"g%s\" % pieces[\"short\"]\nelse:\n    # exception #1\n    rendered = \"0.post%d\" % pieces[\"distance\"]\n    if pieces[\"dirty\"]:\n        rendered += \".dev0\"\n    rendered += \"+g%s\" % pieces[\"short\"]\nreturn rendered", "path": "modin/modin/_version.py", "commit_date": "2020-01-22 00:00:00", "repo_name": "modin-project/modin", "stars": 9371, "license": "apache-2.0", "language": "python", "size": 51945}
{"docstring": "# make sure that fillna on an empty frame works\n", "func_signal": "def test_fillna_dtype_conversion():\n", "code": "df = pandas.DataFrame(index=range(3), columns=[\"A\", \"B\"], dtype=\"float64\")\nmodin_df = pd.DataFrame(index=range(3), columns=[\"A\", \"B\"], dtype=\"float64\")\ndf_equals(modin_df.fillna(\"nan\"), df.fillna(\"nan\"))\n\nframe_data = {\"A\": [1, np.nan], \"B\": [1.0, 2.0]}\ndf = pandas.DataFrame(frame_data)\nmodin_df = pd.DataFrame(frame_data)\nfor v in [\"\", 1, np.nan, 1.0]:\n    df_equals(modin_df.fillna(v), df.fillna(v))", "path": "modin/modin/pandas/test/dataframe/test_window.py", "commit_date": "2020-09-09 00:00:00", "repo_name": "modin-project/modin", "stars": 9371, "license": "apache-2.0", "language": "python", "size": 51945}
{"docstring": "# This is the project root\n", "func_signal": "def test_line_endings():\n", "code": "rootdir = dirname(dirname(abspath(__file__)))\nfor subdir, dirs, files in os.walk(rootdir):\n    if any(i in subdir for i in [\".git\", \".idea\", \"__pycache__\"]):\n        continue\n    for file in files:\n        filepath = os.path.join(subdir, file)\n        with open(filepath, \"rb+\") as f:\n            file_contents = f.read()\n            new_contents = file_contents.replace(b\"\\r\\n\", b\"\\n\")\n            assert new_contents == file_contents, \"File has CRLF: {}\".format(\n                filepath\n            )", "path": "modin/modin/test/test_headers.py", "commit_date": "2020-09-16 00:00:00", "repo_name": "modin-project/modin", "stars": 9371, "license": "apache-2.0", "language": "python", "size": 51945}
{"docstring": "\"\"\"Store f in HANDLERS[vcs][method].\"\"\"\n", "func_signal": "def decorate(f):\n", "code": "if vcs not in HANDLERS:\n    HANDLERS[vcs] = {}\nHANDLERS[vcs][method] = f\nreturn f", "path": "modin/modin/_version.py", "commit_date": "2020-01-22 00:00:00", "repo_name": "modin-project/modin", "stars": 9371, "license": "apache-2.0", "language": "python", "size": 51945}
{"docstring": "\"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n", "func_signal": "def display_errors(errors_index, img_errors, pred_errors, obs_errors):\n", "code": "n = 0\nnrows = 2\nncols = 3\nfig, ax = plt.subplots(nrows, ncols, sharex=True, sharey=True)\nfor row in range(nrows):\n    for col in range(ncols):\n        error = errors_index[n]\n        ax[row, col].imshow((img_errors[error]).reshape((28, 28)))\n        ax[row, col].set_title(\n            \"Predicted label :{}\\nTrue label :{}\".format(\n                pred_errors[error], obs_errors[error]\n            )\n        )\n        n += 1", "path": "modin/stress_tests/kaggle/kaggle6.py", "commit_date": "2019-03-20 00:00:00", "repo_name": "modin-project/modin", "stars": 9371, "license": "apache-2.0", "language": "python", "size": 51945}
{"docstring": "# infer int64 from float64\n", "func_signal": "def test_fillna_downcast():\n", "code": "frame_data = {\"a\": [1.0, np.nan]}\ndf = pandas.DataFrame(frame_data)\nresult = df.fillna(0, downcast=\"infer\")\nmodin_df = pd.DataFrame(frame_data).fillna(0, downcast=\"infer\")\ndf_equals(modin_df, result)\n\n# infer int64 from float64 when fillna value is a dict\ndf = pandas.DataFrame(frame_data)\nresult = df.fillna({\"a\": 0}, downcast=\"infer\")\nmodin_df = pd.DataFrame(frame_data).fillna({\"a\": 0}, downcast=\"infer\")\ndf_equals(modin_df, result)", "path": "modin/modin/pandas/test/dataframe/test_window.py", "commit_date": "2020-09-09 00:00:00", "repo_name": "modin-project/modin", "stars": 9371, "license": "apache-2.0", "language": "python", "size": 51945}
{"docstring": "\"\"\"Generates a synthetic dataset using the given arguments.\n\nArgs:\n    columns (list): Column names of the result\n    dtypes (list): List of dtypes for the corresponding column\n    size (int): Number of rows for result\n\nReturns:\n    Modin dataframe of synthetic data following arguments.\n\"\"\"\n# Record of files generated for a test\n", "func_signal": "def generate_dataset():\n", "code": "filenames = []\n\ndef _dataset_builder(filename, columns, dtypes, size=DF_SIZE, files_to_remove=[]):\n    # Add the files generated by the script to be removed\n    for file in files_to_remove:\n        filenames.append(\"{}/{}\".format(KAGGLE_DIR_PATH, file))\n\n    # Update filename to include path\n    filename = \"{}/{}\".format(KAGGLE_DIR_PATH, filename)\n\n    # Check that the number of column names is the same as the nubmer of dtypes\n    if len(columns) != len(dtypes):\n        raise ValueError(\"len(columns) != len(dtypes)\")\n\n    # Determine number of rows for synthetic dataset\n    row_size = (\n        create_dataframe(columns, dtypes, 1)\n        .memory_usage(index=False, deep=True)\n        .sum()\n    )\n    result = create_dataframe(columns, dtypes, np.ceil(size / row_size))\n\n    result.to_csv(filename)\n    filenames.append(filename)\n    return result\n\n# Return dataset builder factory\nyield _dataset_builder\n\n# Delete files created\nfor filename in filenames:\n    if os.path.exists(filename):\n        os.remove(filename)", "path": "modin/stress_tests/test_kaggle_ipynb.py", "commit_date": "2020-06-10 00:00:00", "repo_name": "modin-project/modin", "stars": 9371, "license": "apache-2.0", "language": "python", "size": 51945}
{"docstring": "\"\"\"\nReturn True if given object is scalar.\n\nThis method wrks the same as is_scalar method from Pandas but\nit is optimized for Modin frames. For BasePandasDataset objects\nPandas version of is_scalar tries to access missing attribute\ncausing index scan. This tiggers execution for lazy frames and\nwe avoid it by handling BasePandasDataset objects separately.\n\nParameters\n----------\nval: object\n    Object to check.\n\nReturns\n-------\nbool\n    True if given object is scalar and False otherwise.\n\"\"\"\n", "func_signal": "def is_scalar(obj):\n", "code": "from pandas.api.types import is_scalar as pandas_is_scalar\nfrom .base import BasePandasDataset\n\nreturn not isinstance(obj, BasePandasDataset) and pandas_is_scalar(obj)", "path": "modin/modin/pandas/utils.py", "commit_date": "2020-10-16 00:00:00", "repo_name": "modin-project/modin", "stars": 9371, "license": "apache-2.0", "language": "python", "size": 51945}
{"docstring": "\"\"\"\nImplement [METHOD_NAME].\n\nTODO: Add more details for this docstring template.\n\nParameters\n----------\nWhat arguments does this function have.\n[\nPARAMETER_NAME: PARAMETERS TYPES\n    Description.\n]\n\nReturns\n-------\nWhat this returns (if anything)\n\"\"\"\n", "func_signal": "def from_non_pandas(df, index, columns, dtype):\n", "code": "from modin.data_management.factories.dispatcher import EngineDispatcher\n\nnew_qc = EngineDispatcher.from_non_pandas(df, index, columns, dtype)\nif new_qc is not None:\n    from .dataframe import DataFrame\n\n    return DataFrame(query_compiler=new_qc)\nreturn new_qc", "path": "modin/modin/pandas/utils.py", "commit_date": "2020-10-16 00:00:00", "repo_name": "modin-project/modin", "stars": 9371, "license": "apache-2.0", "language": "python", "size": 51945}
{"docstring": "\"\"\"TAG-DISTANCE-gHEX[-dirty].\n\nLike 'git describe --tags --dirty --always -long'.\nThe distance/hash is unconditional.\n\nExceptions:\n1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n\"\"\"\n", "func_signal": "def render_git_describe_long(pieces):\n", "code": "if pieces[\"closest-tag\"]:\n    rendered = pieces[\"closest-tag\"]\n    rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])\nelse:\n    # exception #1\n    rendered = pieces[\"short\"]\nif pieces[\"dirty\"]:\n    rendered += \"-dirty\"\nreturn rendered", "path": "modin/modin/_version.py", "commit_date": "2020-01-22 00:00:00", "repo_name": "modin-project/modin", "stars": 9371, "license": "apache-2.0", "language": "python", "size": 51945}
{"docstring": "# with different dtype\n", "func_signal": "def test_fillna_sanity():\n", "code": "frame_data = [\n    [\"a\", \"a\", np.nan, \"a\"],\n    [\"b\", \"b\", np.nan, \"b\"],\n    [\"c\", \"c\", np.nan, \"c\"],\n]\ndf = pandas.DataFrame(frame_data)\n\nresult = df.fillna({2: \"foo\"})\nmodin_df = pd.DataFrame(frame_data).fillna({2: \"foo\"})\n\ndf_equals(modin_df, result)\n\nmodin_df = pd.DataFrame(df)\ndf.fillna({2: \"foo\"}, inplace=True)\nmodin_df.fillna({2: \"foo\"}, inplace=True)\ndf_equals(modin_df, result)\n\nframe_data = {\n    \"Date\": [pandas.NaT, pandas.Timestamp(\"2014-1-1\")],\n    \"Date2\": [pandas.Timestamp(\"2013-1-1\"), pandas.NaT],\n}\ndf = pandas.DataFrame(frame_data)\nresult = df.fillna(value={\"Date\": df[\"Date2\"]})\nmodin_df = pd.DataFrame(frame_data).fillna(value={\"Date\": df[\"Date2\"]})\ndf_equals(modin_df, result)\n\nframe_data = {\"A\": [pandas.Timestamp(\"2012-11-11 00:00:00+01:00\"), pandas.NaT]}\ndf = pandas.DataFrame(frame_data)\nmodin_df = pd.DataFrame(frame_data)\ndf_equals(modin_df.fillna(method=\"pad\"), df.fillna(method=\"pad\"))\n\nframe_data = {\"A\": [pandas.NaT, pandas.Timestamp(\"2012-11-11 00:00:00+01:00\")]}\ndf = pandas.DataFrame(frame_data)\nmodin_df = pd.DataFrame(frame_data).fillna(method=\"bfill\")\ndf_equals(modin_df, df.fillna(method=\"bfill\"))", "path": "modin/modin/pandas/test/dataframe/test_window.py", "commit_date": "2020-09-09 00:00:00", "repo_name": "modin-project/modin", "stars": 9371, "license": "apache-2.0", "language": "python", "size": 51945}
{"docstring": "\"\"\"Call the given command(s).\"\"\"\n", "func_signal": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n", "code": "assert isinstance(commands, list)\np = None\nfor c in commands:\n    try:\n        dispcmd = str([c] + args)\n        # remember shell=False, so use git.cmd on windows, not just git\n        p = subprocess.Popen(\n            [c] + args,\n            cwd=cwd,\n            env=env,\n            stdout=subprocess.PIPE,\n            stderr=(subprocess.PIPE if hide_stderr else None),\n        )\n        break\n    except EnvironmentError:\n        e = sys.exc_info()[1]\n        if e.errno == errno.ENOENT:\n            continue\n        if verbose:\n            print(\"unable to run %s\" % dispcmd)\n            print(e)\n        return None, None\nelse:\n    if verbose:\n        print(\"unable to find command, tried %s\" % (commands,))\n    return None, None\nstdout = p.communicate()[0].strip()\nif sys.version_info[0] >= 3:\n    stdout = stdout.decode()\nif p.returncode != 0:\n    if verbose:\n        print(\"unable to run %s (error)\" % dispcmd)\n        print(\"stdout was %s\" % stdout)\n    return None, p.returncode\nreturn stdout, p.returncode", "path": "modin/modin/_version.py", "commit_date": "2020-01-22 00:00:00", "repo_name": "modin-project/modin", "stars": 9371, "license": "apache-2.0", "language": "python", "size": 51945}
{"docstring": "\"\"\"Try to determine the version from the parent directory name.\n\nSource tarballs conventionally unpack into a directory that includes both\nthe project name and a version string. We will also support searching up\ntwo directory levels for an appropriately named parent directory\n\"\"\"\n", "func_signal": "def versions_from_parentdir(parentdir_prefix, root, verbose):\n", "code": "rootdirs = []\n\nfor i in range(3):\n    dirname = os.path.basename(root)\n    if dirname.startswith(parentdir_prefix):\n        return {\n            \"version\": dirname[len(parentdir_prefix) :],\n            \"full-revisionid\": None,\n            \"dirty\": False,\n            \"error\": None,\n            \"date\": None,\n        }\n    else:\n        rootdirs.append(root)\n        root = os.path.dirname(root)  # up a level\n\nif verbose:\n    print(\n        \"Tried directories %s but none started with prefix %s\"\n        % (str(rootdirs), parentdir_prefix)\n    )\nraise NotThisMethod(\"rootdir doesn't start with parentdir_prefix\")", "path": "modin/modin/_version.py", "commit_date": "2020-01-22 00:00:00", "repo_name": "modin-project/modin", "stars": 9371, "license": "apache-2.0", "language": "python", "size": 51945}
{"docstring": "\"\"\"\nConvert an Arrow Table to a Modin DataFrame.\n\nParameters\n----------\nat: Arrow Table\n    The Arrow Table to convert from.\n\nReturns\n-------\nDataFrame\n    A new Modin DataFrame object.\n\"\"\"\n", "func_signal": "def from_arrow(at):\n", "code": "from modin.data_management.factories.dispatcher import EngineDispatcher\nfrom .dataframe import DataFrame\n\nreturn DataFrame(query_compiler=EngineDispatcher.from_arrow(at))", "path": "modin/modin/pandas/utils.py", "commit_date": "2020-10-16 00:00:00", "repo_name": "modin-project/modin", "stars": 9371, "license": "apache-2.0", "language": "python", "size": 51945}
{"docstring": "\"\"\"\nConvert a pandas DataFrame to a Modin DataFrame.\n\nParameters\n----------\ndf: pandas.DataFrame\n    The pandas DataFrame to convert.\n\nReturns\n-------\n    A new Modin DataFrame object.\n\"\"\"\n", "func_signal": "def from_pandas(df):\n", "code": "from modin.data_management.factories.dispatcher import EngineDispatcher\nfrom .dataframe import DataFrame\n\nreturn DataFrame(query_compiler=EngineDispatcher.from_pandas(df))", "path": "modin/modin/pandas/utils.py", "commit_date": "2020-10-16 00:00:00", "repo_name": "modin-project/modin", "stars": 9371, "license": "apache-2.0", "language": "python", "size": 51945}
{"docstring": "\"\"\"Get version information from git keywords.\"\"\"\n", "func_signal": "def git_versions_from_keywords(keywords, tag_prefix, verbose):\n", "code": "if not keywords:\n    raise NotThisMethod(\"no keywords at all, weird\")\ndate = keywords.get(\"date\")\nif date is not None:\n    # git-2.2.0 added \"%cI\", which expands to an ISO-8601 -compliant\n    # datestamp. However we prefer \"%ci\" (which expands to an \"ISO-8601\n    # -like\" string, which we must then edit to make compliant), because\n    # it's been around since git-1.5.3, and it's too difficult to\n    # discover which version we're using, or to work around using an\n    # older one.\n    date = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\nrefnames = keywords[\"refnames\"].strip()\nif refnames.startswith(\"$Format\"):\n    if verbose:\n        print(\"keywords are unexpanded, not using\")\n    raise NotThisMethod(\"unexpanded keywords, not a git-archive tarball\")\nrefs = set([r.strip() for r in refnames.strip(\"()\").split(\",\")])\n# starting in git-1.8.3, tags are listed as \"tag: foo-1.0\" instead of\n# just \"foo-1.0\". If we see a \"tag: \" prefix, prefer those.\nTAG = \"tag: \"\ntags = set([r[len(TAG) :] for r in refs if r.startswith(TAG)])\nif not tags:\n    # Either we're using git < 1.8.3, or there really are no tags. We use\n    # a heuristic: assume all version tags have a digit. The old git %d\n    # expansion behaves like git log --decorate=short and strips out the\n    # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n    # between branches and tags. By ignoring refnames without digits, we\n    # filter out many common branch names like \"release\" and\n    # \"stabilization\", as well as \"HEAD\" and \"master\".\n    tags = set([r for r in refs if re.search(r\"\\d\", r)])\n    if verbose:\n        print(\"discarding '%s', no digits\" % \",\".join(refs - tags))\nif verbose:\n    print(\"likely tags: %s\" % \",\".join(sorted(tags)))\nfor ref in sorted(tags):\n    # sorting will prefer e.g. \"2.0\" over \"2.0rc1\"\n    if ref.startswith(tag_prefix):\n        r = ref[len(tag_prefix) :]\n        if verbose:\n            print(\"picking %s\" % r)\n        return {\n            \"version\": r,\n            \"full-revisionid\": keywords[\"full\"].strip(),\n            \"dirty\": False,\n            \"error\": None,\n            \"date\": date,\n        }\n# no suitable tags, so version is \"0+unknown\", but full hex is still there\nif verbose:\n    print(\"no suitable tags, using unknown + full revision id\")\nreturn {\n    \"version\": \"0+unknown\",\n    \"full-revisionid\": keywords[\"full\"].strip(),\n    \"dirty\": False,\n    \"error\": \"no suitable tags\",\n    \"date\": None,\n}", "path": "modin/modin/_version.py", "commit_date": "2020-01-22 00:00:00", "repo_name": "modin-project/modin", "stars": 9371, "license": "apache-2.0", "language": "python", "size": 51945}
{"docstring": "\"\"\"\nBreaks an AudioSegment into chunks that are <chunk_length> milliseconds\nlong.\nif chunk_length is 50 then you'll get a list of 50 millisecond long audio\nsegments back (except the last one, which can be shorter)\n\"\"\"\n", "func_signal": "def make_chunks(audio_segment, chunk_length):\n", "code": "number_of_chunks = ceil(len(audio_segment) / float(chunk_length))\nreturn [audio_segment[i * chunk_length:(i + 1) * chunk_length]\n        for i in range(int(number_of_chunks))]", "path": "pydub/pydub/utils.py", "commit_date": "2020-07-28 00:00:00", "repo_name": "jiaaro/pydub", "stars": 8198, "license": "mit", "language": "python", "size": 36944}
{"docstring": "# Use overlay silence with volume change\n", "func_signal": "def test_overlay_with_gain_change(self):\n", "code": "seg_one = self.seg1[:5000]\nseg_silent = AudioSegment.silent(duration=2000)\nseg_over = seg_one.overlay(seg_silent, gain_during_overlay=-7)\n\n# Manually lower first segment\nseg_one_lower = seg_one - 7\nseg_manual = seg_one_lower[:2000] + seg_one[2000:]\n\nself.assertEqual(len(seg_over), len(seg_manual))\nself.assertAlmostEqual(seg_over.dBFS, seg_manual.dBFS)\nself.assertEqual(len(seg_manual), 5000)\nself.assertEqual(len(seg_over), 5000)", "path": "pydub/test/test.py", "commit_date": "2020-06-02 00:00:00", "repo_name": "jiaaro/pydub", "stars": 8198, "license": "mit", "language": "python", "size": 36944}
{"docstring": "\"\"\"\nArgs:\n    freq: The cutoff frequency for highpass and lowpass filters. For\n        band filters, a list of [low_cutoff, high_cutoff]\n    type: \"lowpass\", \"highpass\", or \"band\"\n    order: nth order butterworth filter (default: 5th order). The\n        attenuation is -6dB/octave beyond the cutoff frequency (for 1st\n        order). A Higher order filter will have more attenuation, each level\n        adding an additional -6dB (so a 3rd order butterworth filter would\n        be -18dB/octave).\n\nReturns:\n    function which can filter a mono audio segment\n\n\"\"\"\n", "func_signal": "def _mk_butter_filter(freq, type, order):\n", "code": "def filter_fn(seg):\n    assert seg.channels == 1\n\n    nyq = 0.5 * seg.frame_rate\n    try:\n        freqs = [f / nyq for f in freq]\n    except TypeError:\n        freqs = freq / nyq\n\n    sos = butter(order, freqs, btype=type, output='sos')\n    y = sosfilt(sos, seg.get_array_of_samples())\n\n    return seg._spawn(y.astype(seg.array_type))\n\nreturn filter_fn", "path": "pydub/pydub/scipy_effects.py", "commit_date": "2017-01-06 00:00:00", "repo_name": "jiaaro/pydub", "stars": 8198, "license": "mit", "language": "python", "size": 36944}
{"docstring": "\"\"\"\nReturn enconder default application for system, either avconv or ffmpeg\n\"\"\"\n", "func_signal": "def get_encoder_name():\n", "code": "if which(\"avconv\"):\n    return \"avconv\"\nelif which(\"ffmpeg\"):\n    return \"ffmpeg\"\nelse:\n    # should raise exception\n    warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n    return \"ffmpeg\"", "path": "pydub/pydub/utils.py", "commit_date": "2020-07-28 00:00:00", "repo_name": "jiaaro/pydub", "stars": 8198, "license": "mit", "language": "python", "size": 36944}
{"docstring": "\"\"\"\ndecorator for adding pydub effects to the AudioSegment objects.\nexample use:\n    @register_pydub_effect\n    def normalize(audio_segment):\n        ...\nor you can specify a name:\n    @register_pydub_effect(\"normalize\")\n    def normalize_audio_segment(audio_segment):\n        ...\n\"\"\"\n", "func_signal": "def register_pydub_effect(fn, name=None):\n", "code": "if isinstance(fn, basestring):\n    name = fn\n    return lambda fn: register_pydub_effect(fn, name)\n\nif name is None:\n    name = fn.__name__\n\nfrom .audio_segment import AudioSegment\nsetattr(AudioSegment, name, fn)\nreturn fn", "path": "pydub/pydub/utils.py", "commit_date": "2020-07-28 00:00:00", "repo_name": "jiaaro/pydub", "stars": 8198, "license": "mit", "language": "python", "size": 36944}
{"docstring": "\"\"\"\nleft_gain - amount of gain to apply to the left channel (in dB)\nright_gain - amount of gain to apply to the right channel (in dB)\n\nnote: mono audio segments will be converted to stereo\n\"\"\"\n", "func_signal": "def apply_gain_stereo(seg, left_gain=0.0, right_gain=0.0):\n", "code": "if seg.channels == 1:\n    left = right = seg\nelif seg.channels == 2:\n    left, right = seg.split_to_mono()\n\nl_mult_factor = db_to_float(left_gain)\nr_mult_factor = db_to_float(right_gain)\n\nleft_data = audioop.mul(left._data, left.sample_width, l_mult_factor)\nleft_data = audioop.tostereo(left_data, left.sample_width, 1, 0)\n\nright_data = audioop.mul(right._data, right.sample_width, r_mult_factor)\nright_data = audioop.tostereo(right_data, right.sample_width, 0, 1)\n\noutput = audioop.add(left_data, right_data, seg.sample_width)\n\nreturn seg._spawn(data=output,\n            overrides={'channels': 2,\n                       'frame_width': 2 * seg.sample_width})", "path": "pydub/pydub/effects.py", "commit_date": "2019-01-22 00:00:00", "repo_name": "jiaaro/pydub", "stars": 8198, "license": "mit", "language": "python", "size": 36944}
{"docstring": "# A Sine wave should not be affected by a HPF 3 octaves lower\n", "func_signal": "def test_highpass_filter_cutoff_frequency(self):\n", "code": "s = Sine(800).to_audio_segment()\nless_bass = s.high_pass_filter(100)\nself.assertAlmostEqual(less_bass.dBFS, s.dBFS, places=0)", "path": "pydub/test/test.py", "commit_date": "2020-06-02 00:00:00", "repo_name": "jiaaro/pydub", "stars": 8198, "license": "mit", "language": "python", "size": 36944}
{"docstring": "\"\"\"\nConverts the input float to db, which represents the equivalent\nto the ratio in power represented by the multiplier passed in.\n\"\"\"\n", "func_signal": "def ratio_to_db(ratio, val2=None, using_amplitude=True):\n", "code": "ratio = float(ratio)\n\n# accept 2 values and use the ratio of val1 to val2\nif val2 is not None:\n    ratio = ratio / val2\n\n# special case for multiply-by-zero (convert to silence)\nif ratio == 0:\n    return -float('inf')\n\nif using_amplitude:\n    return 20 * log(ratio, 10)\nelse:  # using power\n    return 10 * log(ratio, 10)", "path": "pydub/pydub/utils.py", "commit_date": "2020-07-28 00:00:00", "repo_name": "jiaaro/pydub", "stars": 8198, "license": "mit", "language": "python", "size": 36944}
{"docstring": "\"\"\"\nConverts the input db to a float, which represents the equivalent\nratio in power.\n\"\"\"\n", "func_signal": "def db_to_float(db, using_amplitude=True):\n", "code": "db = float(db)\nif using_amplitude:\n    return 10 ** (db / 20)\nelse:  # using power\n    return 10 ** (db / 10)", "path": "pydub/pydub/utils.py", "commit_date": "2020-07-28 00:00:00", "repo_name": "jiaaro/pydub", "stars": 8198, "license": "mit", "language": "python", "size": 36944}
{"docstring": "\"\"\"\navprobe sometimes gives more information on stderr than\non the json output. The information has to be extracted\nfrom stderr of the format of:\n'    Stream #0:0: Audio: flac, 88200 Hz, stereo, s32 (24 bit)'\nor (macOS version):\n'    Stream #0:0: Audio: vorbis'\n'      44100 Hz, stereo, fltp, 320 kb/s'\n\n:type stderr: str\n:rtype: list of dict\n\"\"\"\n", "func_signal": "def get_extra_info(stderr):\n", "code": "extra_info = {}\n\nre_stream = r'(?P<space_start> +)Stream #0[:\\.](?P<stream_id>([0-9]+))(?P<content_0>.+)\\n?(?! *Stream)((?P<space_end> +)(?P<content_1>.+))?'\nfor i in re.finditer(re_stream, stderr):\n    if i.group('space_end') is not None and len(i.group('space_start')) <= len(\n            i.group('space_end')):\n        content_line = ','.join([i.group('content_0'), i.group('content_1')])\n    else:\n        content_line = i.group('content_0')\n    tokens = [x.strip() for x in re.split('[:,]', content_line) if x]\n    extra_info[int(i.group('stream_id'))] = tokens\nreturn extra_info", "path": "pydub/pydub/utils.py", "commit_date": "2020-07-28 00:00:00", "repo_name": "jiaaro/pydub", "stars": 8198, "license": "mit", "language": "python", "size": 36944}
{"docstring": "\"\"\"\nReturn enconder default application for system, either avconv or ffmpeg\n\"\"\"\n", "func_signal": "def get_player_name():\n", "code": "if which(\"avplay\"):\n    return \"avplay\"\nelif which(\"ffplay\"):\n    return \"ffplay\"\nelse:\n    # should raise exception\n    warn(\"Couldn't find ffplay or avplay - defaulting to ffplay, but may not work\", RuntimeWarning)\n    return \"ffplay\"", "path": "pydub/pydub/utils.py", "commit_date": "2020-07-28 00:00:00", "repo_name": "jiaaro/pydub", "stars": 8198, "license": "mit", "language": "python", "size": 36944}
{"docstring": "# infinite\n", "func_signal": "def test_overlay_times(self):\n", "code": "seg_mult = self.seg1[:5000] * self.seg2[:3000]\nseg_over = self.seg1[:5000].overlay(self.seg2[:3000], times=99999999)\nself.assertEqual(len(seg_mult), len(seg_over))\nself.assertEqual(len(seg_over), 5000)\nself.assertTrue(seg_mult._data == seg_over._data)\n\n# no times, no-op\npiece = self.seg2[:1000]\nseg_manual = self.seg1[:4000]\nseg_over = self.seg1[:4000].overlay(piece, times=0)\nself.assertEqual(len(seg_manual), len(seg_over))\nself.assertEqual(len(seg_over), 4000)\nself.assertFalse(seg_mult._data == seg_over._data)\n\n# 1 loop\nseg_manual = self.seg1[:4000].overlay(piece, position=500)\nseg_over = self.seg1[:4000].overlay(piece, times=1)\nself.assertEqual(len(seg_manual), len(seg_over))\nself.assertEqual(len(seg_over), 4000)\nself.assertFalse(seg_mult._data == seg_over._data)\n\n# 2 loops\nseg_manual = self.seg1[:4000].overlay(piece, position=500) \\\n    .overlay(piece, position=1500)\nseg_over = self.seg1[:4000].overlay(piece, times=2)\nself.assertEqual(len(seg_manual), len(seg_over))\nself.assertEqual(len(seg_over), 4000)\nself.assertFalse(seg_mult._data == seg_over._data)\n\n# 3 loops\nseg_manual = self.seg1[:4000].overlay(piece, position=500) \\\n    .overlay(piece, position=1500).overlay(piece, position=2500)\nseg_over = self.seg1[:4000].overlay(piece, times=3)\nself.assertEqual(len(seg_manual), len(seg_over))\nself.assertEqual(len(seg_over), 4000)\nself.assertFalse(seg_mult._data == seg_over._data)\n\n# 4 loops (last will pass end)\nseg_manual = self.seg1[:4000].overlay(piece, position=500) \\\n    .overlay(piece, position=1500).overlay(piece, position=2500) \\\n    .overlay(piece, position=3500)\nseg_over = self.seg1[:4000].overlay(piece, times=4)\nself.assertEqual(len(seg_manual), len(seg_over))\nself.assertEqual(len(seg_over), 4000)\nself.assertFalse(seg_mult._data == seg_over._data)\n\n# 5 loops (last won't happen b/c past end)\nseg_manual = self.seg1[:4000].overlay(piece, position=500) \\\n    .overlay(piece, position=1500).overlay(piece, position=2500) \\\n    .overlay(piece, position=3500).overlay(piece, position=3500)\nseg_over = self.seg1[:4000].overlay(piece, times=5)\nself.assertEqual(len(seg_manual), len(seg_over))\nself.assertEqual(len(seg_over), 4000)\nself.assertFalse(seg_mult._data == seg_over._data)\n\n# ~infinite, same (as 4 and 5 really)\nseg_over = self.seg1[:4000].overlay(piece, times=999999999)\nself.assertEqual(len(seg_manual), len(seg_over))\nself.assertEqual(len(seg_over), 4000)\nself.assertFalse(seg_mult._data == seg_over._data)", "path": "pydub/test/test.py", "commit_date": "2020-06-02 00:00:00", "repo_name": "jiaaro/pydub", "stars": 8198, "license": "mit", "language": "python", "size": 36944}
{"docstring": "\"\"\"\nKeyword Arguments:\n    \n    threshold - default: -20.0\n        Threshold in dBFS. default of -20.0 means -20dB relative to the\n        maximum possible volume. 0dBFS is the maximum possible value so\n        all values for this argument sould be negative.\n\n    ratio - default: 4.0\n        Compression ratio. Audio louder than the threshold will be \n        reduced to 1/ratio the volume. A ratio of 4.0 is equivalent to\n        a setting of 4:1 in a pro-audio compressor like the Waves C1.\n    \n    attack - default: 5.0\n        Attack in milliseconds. How long it should take for the compressor\n        to kick in once the audio has exceeded the threshold.\n\n    release - default: 50.0\n        Release in milliseconds. How long it should take for the compressor\n        to stop compressing after the audio has falled below the threshold.\n\n\nFor an overview of Dynamic Range Compression, and more detailed explanation\nof the related terminology, see: \n\n    http://en.wikipedia.org/wiki/Dynamic_range_compression\n\"\"\"\n\n", "func_signal": "def compress_dynamic_range(seg, threshold=-20.0, ratio=4.0, attack=5.0, release=50.0):\n", "code": "thresh_rms = seg.max_possible_amplitude * db_to_float(threshold)\n\nlook_frames = int(seg.frame_count(ms=attack))\ndef rms_at(frame_i):\n    return seg.get_sample_slice(frame_i - look_frames, frame_i).rms\ndef db_over_threshold(rms):\n    if rms == 0: return 0.0\n    db = ratio_to_db(rms / thresh_rms)\n    return max(db, 0)\n\noutput = []\n\n# amount to reduce the volume of the audio by (in dB)\nattenuation = 0.0\n\nattack_frames = seg.frame_count(ms=attack)\nrelease_frames = seg.frame_count(ms=release)\nfor i in xrange(int(seg.frame_count())):\n    rms_now = rms_at(i)\n    \n    # with a ratio of 4.0 this means the volume will exceed the threshold by\n    # 1/4 the amount (of dB) that it would otherwise\n    max_attenuation = (1 - (1.0 / ratio)) * db_over_threshold(rms_now)\n    \n    attenuation_inc = max_attenuation / attack_frames\n    attenuation_dec = max_attenuation / release_frames\n    \n    if rms_now > thresh_rms and attenuation <= max_attenuation:\n        attenuation += attenuation_inc\n        attenuation = min(attenuation, max_attenuation)\n    else:\n        attenuation -= attenuation_dec\n        attenuation = max(attenuation, 0)\n    \n    frame = seg.get_frame(i)\n    if attenuation != 0.0:\n        frame = audioop.mul(frame,\n                            seg.sample_width,\n                            db_to_float(-attenuation))\n    \n    output.append(frame)\n\nreturn seg._spawn(data=b''.join(output))", "path": "pydub/pydub/effects.py", "commit_date": "2019-01-22 00:00:00", "repo_name": "jiaaro/pydub", "stars": 8198, "license": "mit", "language": "python", "size": 36944}
{"docstring": "\"\"\"Wrapper for os.fsdecode which was introduced in python 3.2 .\"\"\"\n\n", "func_signal": "def fsdecode(filename):\n", "code": "if sys.version_info >= (3, 2):\n    PathLikeTypes = (basestring, bytes)\n    if sys.version_info >= (3, 6):\n        PathLikeTypes += (os.PathLike,)\n    if isinstance(filename, PathLikeTypes):\n        return os.fsdecode(filename)\nelse:\n    if isinstance(filename, bytes):\n        return filename.decode(sys.getfilesystemencoding())\n    if isinstance(filename, basestring):\n        return filename\n\nraise TypeError(\"type {0} not accepted by fsdecode\".format(type(filename)))", "path": "pydub/pydub/utils.py", "commit_date": "2020-07-28 00:00:00", "repo_name": "jiaaro/pydub", "stars": 8198, "license": "mit", "language": "python", "size": 36944}
{"docstring": "# A Sine wave should not be affected by a LPF 3 octaves Higher\n", "func_signal": "def test_lowpass_filter_cutoff_frequency(self):\n", "code": "s = Sine(100).to_audio_segment()\nless_treble = s.low_pass_filter(800)\nself.assertAlmostEqual(less_treble.dBFS, s.dBFS, places=0)", "path": "pydub/test/test.py", "commit_date": "2020-06-02 00:00:00", "repo_name": "jiaaro/pydub", "stars": 8198, "license": "mit", "language": "python", "size": 36944}
{"docstring": "\"\"\"\n    cutoff - Frequency (in Hz) where lower frequency signal will begin to\n        be reduced by 6dB per octave (doubling in frequency) below this point\n\"\"\"\n", "func_signal": "def high_pass_filter(seg, cutoff):\n", "code": "RC = 1.0 / (cutoff * 2 * math.pi)\ndt = 1.0 / seg.frame_rate\n\nalpha = RC / (RC + dt)\n\nminval, maxval = get_min_max_value(seg.sample_width * 8)\n\noriginal = seg.get_array_of_samples()\nfilteredArray = array.array(seg.array_type, original)\n\nframe_count = int(seg.frame_count())\n\nlast_val = [0] * seg.channels\nfor i in range(seg.channels):\n    last_val[i] = filteredArray[i] = original[i]\n\nfor i in range(1, frame_count):\n    for j in range(seg.channels):\n        offset = (i * seg.channels) + j\n        offset_minus_1 = ((i-1) * seg.channels) + j\n\n        last_val[j] = alpha * (last_val[j] + original[offset] - original[offset_minus_1])\n        filteredArray[offset] = int(min(max(last_val[j], minval), maxval))\n\nreturn seg._spawn(data=filteredArray)", "path": "pydub/pydub/effects.py", "commit_date": "2019-01-22 00:00:00", "repo_name": "jiaaro/pydub", "stars": 8198, "license": "mit", "language": "python", "size": 36944}
{"docstring": "\"\"\"\nheadroom is how close to the maximum volume to boost the signal up to (specified in dB)\n\"\"\"\n", "func_signal": "def normalize(seg, headroom=0.1):\n", "code": "peak_sample_val = seg.max\n\n# if the max is 0, this audio segment is silent, and can't be normalized\nif peak_sample_val == 0:\n    return seg\n\ntarget_peak = seg.max_possible_amplitude * db_to_float(-headroom)\n\nneeded_boost = ratio_to_db(target_peak / peak_sample_val)\nreturn seg.apply_gain(needed_boost)", "path": "pydub/pydub/effects.py", "commit_date": "2019-01-22 00:00:00", "repo_name": "jiaaro/pydub", "stars": 8198, "license": "mit", "language": "python", "size": 36944}
{"docstring": "\"\"\"\nMimics behavior of UNIX which command.\n\"\"\"\n# Add .exe program extension for windows support\n", "func_signal": "def which(program):\n", "code": "if os.name == \"nt\" and not program.endswith(\".exe\"):\n    program += \".exe\"\n\nenvdir_list = [os.curdir] + os.environ[\"PATH\"].split(os.pathsep)\n\nfor envdir in envdir_list:\n    program_path = os.path.join(envdir, program)\n    if os.path.isfile(program_path) and os.access(program_path, os.X_OK):\n        return program_path", "path": "pydub/pydub/utils.py", "commit_date": "2020-07-28 00:00:00", "repo_name": "jiaaro/pydub", "stars": 8198, "license": "mit", "language": "python", "size": 36944}
{"docstring": "\"\"\"Return json dictionary with media info(codec, duration, size, bitrate...) from filepath\n\"\"\"\n", "func_signal": "def mediainfo_json(filepath, read_ahead_limit=-1):\n", "code": "prober = get_prober_name()\ncommand_args = [\n    \"-v\", \"info\",\n    \"-show_format\",\n    \"-show_streams\",\n]\ntry:\n    command_args += [fsdecode(filepath)]\n    stdin_parameter = None\n    stdin_data = None\nexcept TypeError:\n    if prober == 'ffprobe':\n        command_args += [\"-read_ahead_limit\", str(read_ahead_limit),\n                         \"cache:pipe:0\"]\n    else:\n        command_args += [\"-\"]\n    stdin_parameter = PIPE\n    file, close_file = _fd_or_path_or_tempfile(filepath, 'rb', tempfile=False)\n    file.seek(0)\n    stdin_data = file.read()\n    if close_file:\n        file.close()\n\ncommand = [prober, '-of', 'json'] + command_args\nres = Popen(command, stdin=stdin_parameter, stdout=PIPE, stderr=PIPE)\noutput, stderr = res.communicate(input=stdin_data)\noutput = output.decode(\"utf-8\", 'ignore')\nstderr = stderr.decode(\"utf-8\", 'ignore')\n\ninfo = json.loads(output)\n\nif not info:\n    # If ffprobe didn't give any information, just return it\n    # (for example, because the file doesn't exist)\n    return info\n\nextra_info = get_extra_info(stderr)\n\naudio_streams = [x for x in info['streams'] if x['codec_type'] == 'audio']\nif len(audio_streams) == 0:\n    return info\n\n# We just operate on the first audio stream in case there are more\nstream = audio_streams[0]\n\ndef set_property(stream, prop, value):\n    if prop not in stream or stream[prop] == 0:\n        stream[prop] = value\n\nfor token in extra_info[stream['index']]:\n    m = re.match(r'([su]([0-9]{1,2})p?) \\(([0-9]{1,2}) bit\\)$', token)\n    m2 = re.match(r'([su]([0-9]{1,2})p?)( \\(default\\))?$', token)\n    if m:\n        set_property(stream, 'sample_fmt', m.group(1))\n        set_property(stream, 'bits_per_sample', int(m.group(2)))\n        set_property(stream, 'bits_per_raw_sample', int(m.group(3)))\n    elif m2:\n        set_property(stream, 'sample_fmt', m2.group(1))\n        set_property(stream, 'bits_per_sample', int(m2.group(2)))\n        set_property(stream, 'bits_per_raw_sample', int(m2.group(2)))\n    elif re.match(r'(flt)p?( \\(default\\))?$', token):\n        set_property(stream, 'sample_fmt', token)\n        set_property(stream, 'bits_per_sample', 32)\n        set_property(stream, 'bits_per_raw_sample', 32)\n    elif re.match(r'(dbl)p?( \\(default\\))?$', token):\n        set_property(stream, 'sample_fmt', token)\n        set_property(stream, 'bits_per_sample', 64)\n        set_property(stream, 'bits_per_raw_sample', 64)\nreturn info", "path": "pydub/pydub/utils.py", "commit_date": "2020-07-28 00:00:00", "repo_name": "jiaaro/pydub", "stars": 8198, "license": "mit", "language": "python", "size": 36944}
{"docstring": "\"\"\"\nchannels- specifies which channel (left or right) to reverse the phase of.\nNote that mono AudioSegments will become stereo.\n\"\"\"\n", "func_signal": "def invert_phase(seg, channels=(1, 1)):\n", "code": "if channels == (1, 1):\n    inverted = audioop.mul(seg._data, seg.sample_width, -1.0)  \n    return seg._spawn(data=inverted)\n\nelse:\n    if seg.channels == 2:\n        left, right = seg.split_to_mono()\n    else:\n        raise Exception(\"Can't implicitly convert an AudioSegment with \" + str(seg.channels) + \" channels to stereo.\")\n        \n    if channels == (1, 0):    \n        left = left.invert_phase()\n    else:\n        right = right.invert_phase()\n    \n    return seg.from_mono_audiosegments(left, right)", "path": "pydub/pydub/effects.py", "commit_date": "2019-01-22 00:00:00", "repo_name": "jiaaro/pydub", "stars": 8198, "license": "mit", "language": "python", "size": 36944}
{"docstring": "# Run testfunc count times in within_seconds seconds (and actually\n# within a little less time so we're sure we're under the limit).\n#\n# Because some services are slow, like IMAP, we can't necessarily\n# run testfunc sequentially and still get to count requests within\n# the required time. So we split the requests across threads.\n\n", "func_signal": "def run_test(testfunc, args, count, within_seconds, parallel):\n", "code": "import requests.exceptions\nfrom multiprocessing import Pool\n\nrestart_fail2ban_service()\n\n# Log.\nprint(testfunc.__name__, \" \".join(str(a) for a in args), \"...\")\n\n# Record the start time so we can know how to evenly space our\n# calls to testfunc.\nstart_time = time.time()\n\nwith Pool(parallel) as p:\n\t# Distribute the requests across the pool.\n\tasyncresults = []\n\tfor i in range(count):\n\t\tar = p.apply_async(testfunc_runner, [i, testfunc] + list(args))\n\t\tasyncresults.append(ar)\n\n\t# Wait for all runs to finish.\n\tp.close()\n\tp.join()\n\n\t# Check for errors.\n\tfor ar in asyncresults:\n\t\ttry:\n\t\t\tar.get()\n\t\texcept IsBlocked:\n\t\t\tprint(\"Test machine prematurely blocked!\")\n\t\t\treturn False\n\n# Did we make enough requests within the limit?\nif (time.time()-start_time) > within_seconds:\n\traise Exception(\"Test failed to make %s requests in %d seconds.\" % (count, within_seconds))\n\n# Wait a moment for the block to be put into place.\ntime.sleep(4)\n\n# The next call should fail.\nprint(\"*\", end=\" \", flush=True)\ntry:\n\ttestfunc(*args)\nexcept IsBlocked:\n\t# Success -- this one is supposed to be refused.\n\tprint(\"blocked [OK]\")\n\treturn True # OK\n\nprint(\"not blocked!\")\nreturn False", "path": "mailinabox/tests/fail2ban.py", "commit_date": "2019-08-31 00:00:00", "repo_name": "mail-in-a-box/mailinabox", "stars": 13160, "license": "cc0-1.0", "language": "python", "size": 3491}
{"docstring": "# A safe way to execute processes.\n# Some processes like apt-get require being given a sane PATH.\n", "func_signal": "def shell(method, cmd_args, env={}, capture_stderr=False, return_bytes=False, trap=False, input=None):\n", "code": "import subprocess\n\nenv.update({ \"PATH\": \"/sbin:/bin:/usr/sbin:/usr/bin\" })\nkwargs = {\n    'env': env,\n    'stderr': None if not capture_stderr else subprocess.STDOUT,\n}\nif method == \"check_output\" and input is not None:\n    kwargs['input'] = input\n\nif not trap:\n    ret = getattr(subprocess, method)(cmd_args, **kwargs)\nelse:\n    try:\n        ret = getattr(subprocess, method)(cmd_args, **kwargs)\n        code = 0\n    except subprocess.CalledProcessError as e:\n        ret = e.output\n        code = e.returncode\nif not return_bytes and isinstance(ret, bytes): ret = ret.decode(\"utf8\")\nif not trap:\n    return ret\nelse:\n    return code, ret", "path": "mailinabox/management/utils.py", "commit_date": "2017-01-15 00:00:00", "repo_name": "mail-in-a-box/mailinabox", "stars": 13160, "license": "cc0-1.0", "language": "python", "size": 3491}
{"docstring": "# Sanitize a domain name so it is safe to use as a file name on disk.\n", "func_signal": "def safe_domain_name(name):\n", "code": "import urllib.parse\nreturn urllib.parse.quote(name, safe='')", "path": "mailinabox/management/utils.py", "commit_date": "2017-01-15 00:00:00", "repo_name": "mail-in-a-box/mailinabox", "stars": 13160, "license": "cc0-1.0", "language": "python", "size": 3491}
{"docstring": "# Block until a service on a given port (bound privately or publicly)\n# is taking connections, with a maximum timeout.\n", "func_signal": "def wait_for_service(port, public, env, timeout):\n", "code": "import socket, time\nstart = time.perf_counter()\nwhile True:\n\ts = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\ts.settimeout(timeout/3)\n\ttry:\n\t\ts.connect((\"127.0.0.1\" if not public else env['PUBLIC_IP'], port))\n\t\treturn True\n\texcept OSError:\n\t\tif time.perf_counter() > start+timeout:\n\t\t\treturn False\n\ttime.sleep(min(timeout/4, 1))", "path": "mailinabox/management/utils.py", "commit_date": "2017-01-15 00:00:00", "repo_name": "mail-in-a-box/mailinabox", "stars": 13160, "license": "cc0-1.0", "language": "python", "size": 3491}
{"docstring": "# We don't have a Python sieve client, so we'll\n# just run the IMAP client and see what happens.\n", "func_signal": "def managesieve_test():\n", "code": "import imaplib\n\ntry:\n\tM = imaplib.IMAP4(hostname, 4190)\nexcept ConnectionRefusedError:\n\t# looks like fail2ban worked\n\traise IsBlocked()\n\ntry:\n\tM.login(\"fakeuser\", \"fakepassword\")\n\traise Exception(\"authentication didn't fail\")\nexcept imaplib.IMAP4.error:\n\t# authentication should fail\n\tpass\nfinally:\n\tM.logout() # shuts down connection, has nothing to do with login()", "path": "mailinabox/tests/fail2ban.py", "commit_date": "2019-08-31 00:00:00", "repo_name": "mail-in-a-box/mailinabox", "stars": 13160, "license": "cc0-1.0", "language": "python", "size": 3491}
{"docstring": "# Google Compute Engine instances install some Python-2-only boto plugins that\n# conflict with boto running under Python 3. Disable boto's default configuration\n# file prior to importing boto so that GCE's plugin is not loaded:\n", "func_signal": "def fix_boto():\n", "code": "import os\nos.environ[\"BOTO_CONFIG\"] = \"/etc/boto3.cfg\"", "path": "mailinabox/management/utils.py", "commit_date": "2017-01-15 00:00:00", "repo_name": "mail-in-a-box/mailinabox", "stars": 13160, "license": "cc0-1.0", "language": "python", "size": 3491}
{"docstring": "# Execute silently, but if there is an error then display the output & exit.\n", "func_signal": "def service_command(service, command, quit=None):\n", "code": "code, ret = shell('check_output', [\"/usr/sbin/service\", service, command], capture_stderr=True, trap=True)\nif code != 0:\n\tprint(ret)\n\tif quit:\n\t\tsys.exit(code)", "path": "mailinabox/management/backup.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "mail-in-a-box/mailinabox", "stars": 13160, "license": "cc0-1.0", "language": "python", "size": 3491}
{"docstring": "# Put domain names in a nice sorted order.\n\n# The nice order will group domain names by DNS zone, i.e. the top-most\n# domain name that we serve that ecompasses a set of subdomains. Map\n# each of the domain names to the zone that contains them. Walk the domains\n# from shortest to longest since zones are always shorter than their\n# subdomains.\n", "func_signal": "def sort_domains(domain_names, env):\n", "code": "zones = { }\nfor domain in sorted(domain_names, key=lambda d : len(d)):\n    for z in zones.values():\n        if domain.endswith(\".\" + z):\n            # We found a parent domain already in the list.\n            zones[domain] = z\n            break\n    else:\n        # 'break' did not occur: there is no parent domain, so it is its\n        # own zone.\n        zones[domain] = domain\n\n# Sort the zones.\nzone_domains = sorted(zones.values(),\n  key = lambda d : (\n    # PRIMARY_HOSTNAME or the zone that contains it is always first.\n    not (d == env['PRIMARY_HOSTNAME'] or env['PRIMARY_HOSTNAME'].endswith(\".\" + d)),\n\n    # Then just dumb lexicographically.\n    d,\n  ))\n\n# Now sort the domain names that fall within each zone.\ndomain_names = sorted(domain_names,\n  key = lambda d : (\n    # First by zone.\n    zone_domains.index(zones[d]),\n\n    # PRIMARY_HOSTNAME is always first within the zone that contains it.\n    d != env['PRIMARY_HOSTNAME'],\n\n    # Followed by any of its subdomains.\n    not d.endswith(\".\" + env['PRIMARY_HOSTNAME']),\n\n    # Then in right-to-left lexicographic order of the .-separated parts of the name.\n    list(reversed(d.split(\".\"))),\n  ))\n\nreturn domain_names", "path": "mailinabox/management/utils.py", "commit_date": "2017-01-15 00:00:00", "repo_name": "mail-in-a-box/mailinabox", "stars": 13160, "license": "cc0-1.0", "language": "python", "size": 3491}
{"docstring": "\"\"\" Relay munin cgi dynazoom requests\n/usr/lib/munin/cgi/munin-cgi-graph is a perl cgi script in the munin package\nthat is responsible for generating binary png images _and_ associated HTTP\nheaders based on parameters in the requesting URL. All output is written\nto stdout which munin_cgi splits into response headers and binary response\ndata.\nmunin-cgi-graph reads environment variables to determine\nwhat it should do. It expects a path to be in the env-var PATH_INFO, and a\nquerystring to be in the env-var QUERY_STRING.\nmunin-cgi-graph has several failure modes. Some write HTTP Status headers and\nothers return nonzero exit codes.\nSituating munin_cgi between the user-agent and munin-cgi-graph enables keeping\nthe cgi script behind mailinabox's auth mechanisms and avoids additional\nsupport infrastructure like spawn-fcgi.\n\"\"\"\n\n", "func_signal": "def munin_cgi(filename):\n", "code": "COMMAND = 'su - munin --preserve-environment --shell=/bin/bash -c /usr/lib/munin/cgi/munin-cgi-graph'\n# su changes user, we use the munin user here\n# --preserve-environment retains the environment, which is where Popen's `env` data is\n# --shell=/bin/bash ensures the shell used is bash\n# -c \"/usr/lib/munin/cgi/munin-cgi-graph\" passes the command to run as munin\n# \"%s\" is a placeholder for where the request's querystring will be added\n\nif filename == \"\":\n\treturn (\"a path must be specified\", 404)\n\nquery_str = request.query_string.decode(\"utf-8\", 'ignore')\n\nenv = {'PATH_INFO': '/%s/' % filename, 'REQUEST_METHOD': 'GET', 'QUERY_STRING': query_str}\ncode, binout = utils.shell('check_output',\n\t\t\t\t\t\t   COMMAND.split(\" \", 5),\n\t\t\t\t\t\t   # Using a maxsplit of 5 keeps the last arguments together\n\t\t\t\t\t\t   env=env,\n\t\t\t\t\t\t   return_bytes=True,\n\t\t\t\t\t\t   trap=True)\n\nif code != 0:\n\t# nonzero returncode indicates error\n\tapp.logger.error(\"munin_cgi: munin-cgi-graph returned nonzero exit code, %s\", code)\n\treturn (\"error processing graph image\", 500)\n\n# /usr/lib/munin/cgi/munin-cgi-graph returns both headers and binary png when successful.\n# A double-Windows-style-newline always indicates the end of HTTP headers.\nheaders, image_bytes = binout.split(b'\\r\\n\\r\\n', 1)\nresponse = make_response(image_bytes)\nfor line in headers.splitlines():\n\tname, value = line.decode(\"utf8\").split(':', 1)\n\tresponse.headers[name] = value\nif 'Status' in response.headers and '404' in response.headers['Status']:\n\tapp.logger.warning(\"munin_cgi: munin-cgi-graph returned 404 status code. PATH_INFO=%s\", env['PATH_INFO'])\nreturn response", "path": "mailinabox/management/daemon.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "mail-in-a-box/mailinabox", "stars": 13160, "license": "cc0-1.0", "language": "python", "size": 3491}
{"docstring": "# Anyone accessing this route is an admin, and we permit them to\n# disable the MFA status for any user if they submit a 'user' form\n# field.\n", "func_signal": "def totp_post_disable():\n", "code": "email = request.form.get('user', request.user_email) # user field if given, otherwise the user making the request\ntry:\n\tresult = disable_mfa(email, request.form.get('mfa-id') or None, env) # convert empty string to None\nexcept ValueError as e:\n\treturn (str(e), 400)\nif result: # success\n\treturn \"OK\"\nelse: # error\n\treturn (\"Invalid user or MFA id.\", 400)", "path": "mailinabox/management/daemon.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "mail-in-a-box/mailinabox", "stars": 13160, "license": "cc0-1.0", "language": "python", "size": 3491}
{"docstring": "# Log in over SSH to restart fail2ban.\n", "func_signal": "def restart_fail2ban_service(final=False):\n", "code": "command = \"sudo fail2ban-client reload\"\nif not final:\n\t# Stop recidive jails during testing.\n\tcommand += \" && sudo fail2ban-client stop recidive\"\nos.system(\"%s \\\"%s\\\"\" % (ssh_command, command))", "path": "mailinabox/tests/fail2ban.py", "commit_date": "2019-08-31 00:00:00", "repo_name": "mail-in-a-box/mailinabox", "stars": 13160, "license": "cc0-1.0", "language": "python", "size": 3491}
{"docstring": "# To keep the attack surface low, we don't allow a remote reboot if one isn't necessary.\n", "func_signal": "def do_reboot():\n", "code": "from status_checks import is_reboot_needed_due_to_package_installation\nif is_reboot_needed_due_to_package_installation():\n\treturn utils.shell(\"check_output\", [\"/sbin/shutdown\", \"-r\", \"now\"], capture_stderr=True)\nelse:\n\treturn \"No reboot is required, so it is not allowed.\"", "path": "mailinabox/management/daemon.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "mail-in-a-box/mailinabox", "stars": 13160, "license": "cc0-1.0", "language": "python", "size": 3491}
{"docstring": "# Anyone accessing this route is an admin, and we permit them to\n# see the MFA status for any user if they submit a 'user' form\n# field. But we don't include provisioning info since a user can\n# only provision for themselves.\n", "func_signal": "def mfa_get_status():\n", "code": "email = request.form.get('user', request.user_email) # user field if given, otherwise the user making the request\ntry:\n\tresp = {\n\t\t\"enabled_mfa\": get_public_mfa_state(email, env)\n\t}\n\tif email == request.user_email:\n\t\tresp.update({\n\t\t\t\"new_mfa\": {\n\t\t\t\t\"totp\": provision_totp(email, env)\n\t\t\t}\n\t\t})\nexcept ValueError as e:\n\treturn (str(e), 400)\nreturn json_response(resp)", "path": "mailinabox/management/daemon.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "mail-in-a-box/mailinabox", "stars": 13160, "license": "cc0-1.0", "language": "python", "size": 3491}
{"docstring": "# Render the control panel. This route does not require user authentication\n# so it must be safe!\n\n", "func_signal": "def index():\n", "code": "no_users_exist = (len(get_mail_users(env)) == 0)\nno_admins_exist = (len(get_admins(env)) == 0)\n\nutils.fix_boto() # must call prior to importing boto\nimport boto.s3\nbackup_s3_hosts = [(r.name, r.endpoint) for r in boto.s3.regions()]\n\nreturn render_template('index.html',\n\thostname=env['PRIMARY_HOSTNAME'],\n\tstorage_root=env['STORAGE_ROOT'],\n\n\tno_users_exist=no_users_exist,\n\tno_admins_exist=no_admins_exist,\n\n\tbackup_s3_hosts=backup_s3_hosts,\n\tcsr_country_codes=csr_country_codes,\n)", "path": "mailinabox/management/daemon.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "mail-in-a-box/mailinabox", "stars": 13160, "license": "cc0-1.0", "language": "python", "size": 3491}
{"docstring": "# Checks administrative access (@authorized_personnel_only) and then just proxies\n# the request to static files.\n", "func_signal": "def munin(filename=\"\"):\n", "code": "if filename == \"\": filename = \"index.html\"\nreturn send_from_directory(\"/var/cache/munin/www\", filename)", "path": "mailinabox/management/daemon.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "mail-in-a-box/mailinabox", "stars": 13160, "license": "cc0-1.0", "language": "python", "size": 3491}
{"docstring": "# Force a full backup when the total size of the increments\n# since the last full backup is greater than half the size\n# of that full backup.\n", "func_signal": "def should_force_full(config, env):\n", "code": "inc_size = 0\nfor bak in backup_status(env)[\"backups\"]:\n\tif not bak[\"full\"]:\n\t\t# Scan through the incremental backups cumulating\n\t\t# size...\n\t\tinc_size += bak[\"size\"]\n\telse:\n\t\t# ...until we reach the most recent full backup.\n\t\t# Return if we should to a full backup, which is based\n\t\t# on the size of the increments relative to the full\n\t\t# backup, as well as the age of the full backup.\n\t\tif inc_size > .5*bak[\"size\"]:\n\t\t\treturn True\n\t\tif dateutil.parser.parse(bak[\"date\"]) + datetime.timedelta(days=config[\"min_age_in_days\"]*10+1) < datetime.datetime.now(dateutil.tz.tzlocal()):\n\t\t\treturn True\n\t\treturn False\nelse:\n\t# If we got here there are no (full) backups, so make one.\n\t# (I love for/else blocks. Here it's just to show off.)\n\treturn True", "path": "mailinabox/management/backup.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "mail-in-a-box/mailinabox", "stars": 13160, "license": "cc0-1.0", "language": "python", "size": 3491}
{"docstring": "# If backups are dissbled, return no status.\n", "func_signal": "def backup_status(env):\n", "code": "config = get_backup_config(env)\nif config[\"target\"] == \"off\":\n\treturn { }\n\n# Query duplicity to get a list of all full and incremental\n# backups available.\n\nbackups = { }\nnow = datetime.datetime.now(dateutil.tz.tzlocal())\nbackup_root = os.path.join(env[\"STORAGE_ROOT\"], 'backup')\nbackup_cache_dir = os.path.join(backup_root, 'cache')\n\ndef reldate(date, ref, clip):\n\tif ref < date: return clip\n\trd = dateutil.relativedelta.relativedelta(ref, date)\n\tif rd.years > 1: return \"%d years, %d months\" % (rd.years, rd.months)\n\tif rd.years == 1: return \"%d year, %d months\" % (rd.years, rd.months)\n\tif rd.months > 1: return \"%d months, %d days\" % (rd.months, rd.days)\n\tif rd.months == 1: return \"%d month, %d days\" % (rd.months, rd.days)\n\tif rd.days >= 7: return \"%d days\" % rd.days\n\tif rd.days > 1: return \"%d days, %d hours\" % (rd.days, rd.hours)\n\tif rd.days == 1: return \"%d day, %d hours\" % (rd.days, rd.hours)\n\treturn \"%d hours, %d minutes\" % (rd.hours, rd.minutes)\n\n# Get duplicity collection status and parse for a list of backups.\ndef parse_line(line):\n\tkeys = line.strip().split()\n\tdate = dateutil.parser.parse(keys[1]).astimezone(dateutil.tz.tzlocal())\n\treturn {\n\t\t\"date\": keys[1],\n\t\t\"date_str\": date.strftime(\"%Y-%m-%d %X\") + \" \" + now.tzname(),\n\t\t\"date_delta\": reldate(date, now, \"the future?\"),\n\t\t\"full\": keys[0] == \"full\",\n\t\t\"size\": 0, # collection-status doesn't give us the size\n\t\t\"volumes\": int(keys[2]), # number of archive volumes for this backup (not really helpful)\n\t}\n\ncode, collection_status = shell('check_output', [\n\t\"/usr/bin/duplicity\",\n\t\"collection-status\",\n\t\"--archive-dir\", backup_cache_dir,\n\t\"--gpg-options\", \"--cipher-algo=AES256\",\n\t\"--log-fd\", \"1\",\n\tconfig[\"target\"],\n\t] + rsync_ssh_options,\n\tget_env(env),\n\ttrap=True)\nif code != 0:\n\t# Command failed. This is likely due to an improperly configured remote\n\t# destination for the backups or the last backup job terminated unexpectedly.\n\traise Exception(\"Something is wrong with the backup: \" + collection_status)\nfor line in collection_status.split('\\n'):\n\tif line.startswith(\" full\") or line.startswith(\" inc\"):\n\t\tbackup = parse_line(line)\n\t\tbackups[backup[\"date\"]] = backup\n\n# Look at the target directly to get the sizes of each of the backups. There is more than one file per backup.\n# Starting with duplicity in Ubuntu 18.04, \"signatures\" files have dates in their\n# filenames that are a few seconds off the backup date and so don't line up\n# with the list of backups we have. Track unmatched files so we know how much other\n# space is used for those.\nunmatched_file_size = 0\nfor fn, size in list_target_files(config):\n\tm = re.match(r\"duplicity-(full|full-signatures|(inc|new-signatures)\\.(?P<incbase>\\d+T\\d+Z)\\.to)\\.(?P<date>\\d+T\\d+Z)\\.\", fn)\n\tif not m: continue # not a part of a current backup chain\n\tkey = m.group(\"date\")\n\tif key in backups:\n\t\tbackups[key][\"size\"] += size\n\telse:\n\t\tunmatched_file_size += size\n\n# Ensure the rows are sorted reverse chronologically.\n# This is relied on by should_force_full() and the next step.\nbackups = sorted(backups.values(), key = lambda b : b[\"date\"], reverse=True)\n\n# Get the average size of incremental backups, the size of the\n# most recent full backup, and the date of the most recent\n# backup and the most recent full backup.\nincremental_count = 0\nincremental_size = 0\nfirst_date = None\nfirst_full_size = None\nfirst_full_date = None\nfor bak in backups:\n\tif first_date is None:\n\t\tfirst_date = dateutil.parser.parse(bak[\"date\"])\n\tif bak[\"full\"]:\n\t\tfirst_full_size = bak[\"size\"]\n\t\tfirst_full_date = dateutil.parser.parse(bak[\"date\"])\n\t\tbreak\n\tincremental_count += 1\n\tincremental_size += bak[\"size\"]\n\n# When will the most recent backup be deleted? It won't be deleted if the next\n# backup is incremental, because the increments rely on all past increments.\n# So first guess how many more incremental backups will occur until the next\n# full backup. That full backup frees up this one to be deleted. But, the backup\n# must also be at least min_age_in_days old too.\ndeleted_in = None\nif incremental_count > 0 and incremental_size > 0 and first_full_size is not None:\n\t# How many days until the next incremental backup? First, the part of\n\t# the algorithm based on increment sizes:\n\test_days_to_next_full = (.5 * first_full_size - incremental_size) / (incremental_size/incremental_count)\n\test_time_of_next_full = first_date + datetime.timedelta(days=est_days_to_next_full)\n\n\t# ...And then the part of the algorithm based on full backup age:\n\test_time_of_next_full = min(est_time_of_next_full, first_full_date + datetime.timedelta(days=config[\"min_age_in_days\"]*10+1))\n\n\t# It still can't be deleted until it's old enough.\n\test_deleted_on = max(est_time_of_next_full, first_date + datetime.timedelta(days=config[\"min_age_in_days\"]))\n\n\tdeleted_in = \"approx. %d days\" % round((est_deleted_on-now).total_seconds()/60/60/24 + .5)\n\n# When will a backup be deleted? Set the deleted_in field of each backup.\nsaw_full = False\nfor bak in backups:\n\tif deleted_in:\n\t\t# The most recent increment in a chain and all of the previous backups\n\t\t# it relies on are deleted at the same time.\n\t\tbak[\"deleted_in\"] = deleted_in\n\tif bak[\"full\"]:\n\t\t# Reset when we get to a full backup. A new chain start *next*.\n\t\tsaw_full = True\n\t\tdeleted_in = None\n\telif saw_full and not deleted_in:\n\t\t# We're now on backups prior to the most recent full backup. These are\n\t\t# free to be deleted as soon as they are min_age_in_days old.\n\t\tdeleted_in = reldate(now, dateutil.parser.parse(bak[\"date\"]) + datetime.timedelta(days=config[\"min_age_in_days\"]), \"on next daily backup\")\n\t\tbak[\"deleted_in\"] = deleted_in\n\nreturn {\n\t\"backups\": backups,\n\t\"unmatched_file_size\": unmatched_file_size,\n}", "path": "mailinabox/management/backup.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "mail-in-a-box/mailinabox", "stars": 13160, "license": "cc0-1.0", "language": "python", "size": 3491}
{"docstring": "# Load settings from a KEY=VALUE file.\n", "func_signal": "def load_env_vars_from_file(fn):\n", "code": "import collections\nenv = collections.OrderedDict()\nfor line in open(fn): env.setdefault(*line.strip().split(\"=\", 1))\nreturn env", "path": "mailinabox/management/utils.py", "commit_date": "2017-01-15 00:00:00", "repo_name": "mail-in-a-box/mailinabox", "stars": 13160, "license": "cc0-1.0", "language": "python", "size": 3491}
{"docstring": "# Computes the size of all files in the path, like the `du` command.\n# Based on http://stackoverflow.com/a/17936789. Takes into account\n# soft and hard links.\n", "func_signal": "def du(path):\n", "code": "total_size = 0\nseen = set()\nfor dirpath, dirnames, filenames in os.walk(path):\n    for f in filenames:\n        fp = os.path.join(dirpath, f)\n        try:\n            stat = os.lstat(fp)\n        except OSError:\n            continue\n        if stat.st_ino in seen:\n            continue\n        seen.add(stat.st_ino)\n        total_size += stat.st_size\nreturn total_size", "path": "mailinabox/management/utils.py", "commit_date": "2017-01-15 00:00:00", "repo_name": "mail-in-a-box/mailinabox", "stars": 13160, "license": "cc0-1.0", "language": "python", "size": 3491}
{"docstring": "# Get the encryption passphrase. secret_key.txt is 2048 random\n# bits base64-encoded and with line breaks every 65 characters.\n# gpg will only take the first line of text, so sanity check that\n# that line is long enough to be a reasonable passphrase. It\n# only needs to be 43 base64-characters to match AES256's key\n# length of 32 bytes.\n", "func_signal": "def get_passphrase(env):\n", "code": "backup_root = os.path.join(env[\"STORAGE_ROOT\"], 'backup')\nwith open(os.path.join(backup_root, 'secret_key.txt')) as f:\n\tpassphrase = f.readline().strip()\nif len(passphrase) < 43: raise Exception(\"secret_key.txt's first line is too short!\")\n\nreturn passphrase", "path": "mailinabox/management/backup.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "mail-in-a-box/mailinabox", "stars": 13160, "license": "cc0-1.0", "language": "python", "size": 3491}
{"docstring": "# Confirm input image is non-square resolution\n", "func_signal": "def test_convert_to_square_resolution(renderer, resources, outpdf):\n", "code": "in_pageinfo = PdfInfo(resources / 'aspect.pdf')\nassert in_pageinfo[0].dpi.x != in_pageinfo[0].dpi.y\n\n# --force-ocr requires means forced conversion to square resolution\ncheck_ocrmypdf(\n    resources / 'aspect.pdf',\n    outpdf,\n    '--force-ocr',\n    '--pdf-renderer',\n    renderer,\n    '--plugin',\n    'tests/plugins/tesseract_cache.py',\n)\n\nout_pageinfo = PdfInfo(outpdf)\n\nin_p0, out_p0 = in_pageinfo[0], out_pageinfo[0]\n\n# Resolution show now be equal\nassert out_p0.dpi.x == out_p0.dpi.y\n\n# Page size should match input page size\nassert isclose(in_p0.width_inches, out_p0.width_inches)\nassert isclose(in_p0.height_inches, out_p0.height_inches)\n\n# Because we rasterized the page to produce a new image, it should occupy\n# the entire page\nout_im_w = out_p0.images[0].width / out_p0.images[0].dpi.x\nout_im_h = out_p0.images[0].height / out_p0.images[0].dpi.y\nassert isclose(out_p0.width_inches, out_im_w)\nassert isclose(out_p0.height_inches, out_im_h)", "path": "OCRmyPDF/tests/test_preprocessing.py", "commit_date": "2020-08-05 00:00:00", "repo_name": "ocrmypdf/OCRmyPDF", "stars": 11541, "license": "mpl-2.0", "language": "python", "size": 66403}
{"docstring": "# Confirm input image is non-square resolution\n", "func_signal": "def test_non_square_resolution(renderer, resources, outpdf):\n", "code": "in_pageinfo = PdfInfo(resources / 'aspect.pdf')\nassert in_pageinfo[0].dpi.x != in_pageinfo[0].dpi.y\n\ncheck_ocrmypdf(\n    resources / 'aspect.pdf',\n    outpdf,\n    '--pdf-renderer',\n    renderer,\n    '--plugin',\n    'tests/plugins/tesseract_cache.py',\n)\n\nout_pageinfo = PdfInfo(outpdf)\n\n# Confirm resolution was kept the same\nassert in_pageinfo[0].dpi == out_pageinfo[0].dpi", "path": "OCRmyPDF/tests/test_preprocessing.py", "commit_date": "2020-08-05 00:00:00", "repo_name": "ocrmypdf/OCRmyPDF", "stars": 11541, "license": "mpl-2.0", "language": "python", "size": 66403}
{"docstring": "# Remove the two arguments that tell ghostscript to create a PDF/A\n# Does not remove the Postscript definition file - not necessary\n# to cause PDF/A creation failure\n", "func_signal": "def run_rig_args(args, **kwargs):\n", "code": "new_args = [\n    arg for arg in args if not arg.startswith('-dPDFA') and not arg.endswith('.ps')\n]\nproc = run_polling_stderr(new_args, **kwargs)\nreturn proc", "path": "OCRmyPDF/tests/plugins/gs_pdfa_failure.py", "commit_date": "2020-12-29 00:00:00", "repo_name": "ocrmypdf/OCRmyPDF", "stars": 11541, "license": "mpl-2.0", "language": "python", "size": 66403}
{"docstring": "# Verify leptonica: check that an incorrect rotated image has poor\n# correlation with reference\n", "func_signal": "def test_monochrome_correlation(resources, outdir):\n", "code": "corr = check_monochrome_correlation(\n    outdir,\n    reference_pdf=resources / 'cardinal.pdf',\n    reference_pageno=1,  # north facing page\n    test_pdf=resources / 'cardinal.pdf',\n    test_pageno=3,  # south facing page\n)\nassert corr < 0.10\ncorr = check_monochrome_correlation(\n    outdir,\n    reference_pdf=resources / 'cardinal.pdf',\n    reference_pageno=2,\n    test_pdf=resources / 'cardinal.pdf',\n    test_pageno=2,\n)\nassert corr > 0.90", "path": "OCRmyPDF/tests/test_rotation.py", "commit_date": "2020-11-07 00:00:00", "repo_name": "ocrmypdf/OCRmyPDF", "stars": 11541, "license": "mpl-2.0", "language": "python", "size": 66403}
{"docstring": "\"\"\"Copy a font from the filename text into pdf_base\"\"\"\n\n", "func_signal": "def _find_font(self, text):\n", "code": "font, font_key = None, None\npossible_font_names = ('/f-0-0', '/F1')\ntry:\n    with pikepdf.open(text) as pdf_text:\n        try:\n            pdf_text_fonts = pdf_text.pages[0].Resources.get('/Font', {})\n        except (AttributeError, IndexError, KeyError):\n            return None, None\n        for f in possible_font_names:\n            pdf_text_font = pdf_text_fonts.get(f, None)\n            if pdf_text_font is not None:\n                font_key = f\n                break\n        if pdf_text_font:\n            font = self.pdf_base.copy_foreign(pdf_text_font)\n        return font, font_key\nexcept (FileNotFoundError, pikepdf.PdfError):\n    # PdfError occurs if a 0-length file is written e.g. due to OCR timeout\n    return None, None", "path": "OCRmyPDF/src/ocrmypdf/_graft.py", "commit_date": "2020-10-24 00:00:00", "repo_name": "ocrmypdf/OCRmyPDF", "stars": 11541, "license": "mpl-2.0", "language": "python", "size": 66403}
{"docstring": "\"\"\"Run a process like ``ocrmypdf.subprocess.run``, and poll stderr.\n\nEvery line of produced by stderr will be forwarded to the callback function.\nThe intended use is monitoring progress of subprocesses that output their\nown progress indicators. In addition, each line will be logged if debug\nlogging is enabled.\n\nRequires stderr to be opened in text mode for ease of handling errors. In\naddition the expected encoding= and errors= arguments should be set. Note\nthat if stdout is already set up, it need not be binary.\n\"\"\"\n", "func_signal": "def run_polling_stderr(args, *, callback, check=False, env=None, **kwargs):\n", "code": "args, env, process_log, text = _fix_process_args(args, env, kwargs)\nassert text, \"Must use text=True\"\n\nproc = Popen(args, env=env, **kwargs)\n\nlines = []\nwhile proc.poll() is None:\n    for msg in iter(proc.stderr.readline, ''):\n        if process_log.isEnabledFor(logging.DEBUG):\n            process_log.debug(msg.strip())\n        callback(msg)\n        lines.append(msg)\nstderr = ''.join(lines)\n\nif check and proc.returncode != 0:\n    raise CalledProcessError(proc.returncode, args, output=None, stderr=stderr)\nreturn CompletedProcess(args, proc.returncode, None, stderr=stderr)", "path": "OCRmyPDF/src/ocrmypdf/subprocess/__init__.py", "commit_date": "2020-12-11 00:00:00", "repo_name": "ocrmypdf/OCRmyPDF", "stars": 11541, "license": "mpl-2.0", "language": "python", "size": 66403}
{"docstring": "# Tesseract 4.x can be multithreaded, and we also run multiple workers. We want\n# to manage how many threads it uses to avoid creating total threads than cores.\n# Performance testing shows we're better off\n# parallelizing ocrmypdf and forcing Tesseract to be single threaded, which we\n# get by setting the envvar OMP_THREAD_LIMIT to 1. But if the page count of the\n# input file is small, then we allow Tesseract to use threads, subject to the\n# constraint: (ocrmypdf workers) * (tesseract threads) <= max_workers.\n# As of Tesseract 4.1, 3 threads is the most effective on a 4 core/8 thread system.\n", "func_signal": "def validate(pdfinfo, options):\n", "code": "if not os.environ.get('OMP_THREAD_LIMIT', '').isnumeric():\n    tess_threads = clamp(options.jobs // len(pdfinfo), 1, 3)\n    os.environ['OMP_THREAD_LIMIT'] = str(tess_threads)\nelse:\n    tess_threads = int(os.environ['OMP_THREAD_LIMIT'])\nlog.debug(\"Using Tesseract OpenMP thread limit %d\", tess_threads)", "path": "OCRmyPDF/src/ocrmypdf/builtin_plugins/tesseract_ocr.py", "commit_date": "2020-10-08 00:00:00", "repo_name": "ocrmypdf/OCRmyPDF", "stars": 11541, "license": "mpl-2.0", "language": "python", "size": 66403}
{"docstring": "# cardinal.pdf contains four copies of an image rotated in each cardinal\n# direction - these ones are \"burned in\" not tagged with /Rotate\n", "func_signal": "def test_autorotate(renderer, resources, outdir):\n", "code": "out = check_ocrmypdf(\n    resources / 'cardinal.pdf',\n    outdir / 'out.pdf',\n    '-r',\n    '-v',\n    '1',\n    '--pdf-renderer',\n    renderer,\n    '--plugin',\n    'tests/plugins/tesseract_cache.py',\n)\nfor n in range(1, 4 + 1):\n    correlation = check_monochrome_correlation(\n        outdir,\n        reference_pdf=resources / 'cardinal.pdf',\n        reference_pageno=1,\n        test_pdf=outdir / 'out.pdf',\n        test_pageno=n,\n    )\n    assert correlation > 0.80", "path": "OCRmyPDF/tests/test_rotation.py", "commit_date": "2020-11-07 00:00:00", "repo_name": "ocrmypdf/OCRmyPDF", "stars": 11541, "license": "mpl-2.0", "language": "python", "size": 66403}
{"docstring": "# This test requires an image that pngquant is capable of converting to\n# to 1bpp - so use an existing 1bpp image, convert up, confirm it can\n# convert down\n", "func_signal": "def test_flate_to_jbig2(resources, outdir):\n", "code": "with Image.open(fspath(resources / 'typewriter.png')) as im:\n    assert im.mode in ('1', 'P')\n    im = im.convert('L')\n    im.save(fspath(outdir / 'type8.png'))\n\ncheck_ocrmypdf(\n    outdir / 'type8.png',\n    outdir / 'out.pdf',\n    '--image-dpi',\n    '100',\n    '--png-quality',\n    '50',\n    '--optimize',\n    '3',\n    '--plugin',\n    'tests/plugins/tesseract_noop.py',\n)\n\npdf = pikepdf.open(outdir / 'out.pdf')\npim = pikepdf.PdfImage(next(iter(pdf.pages[0].images.values())))\nassert pim.filters[0] == '/JBIG2Decode'", "path": "OCRmyPDF/tests/test_optimize.py", "commit_date": "2020-12-30 00:00:00", "repo_name": "ocrmypdf/OCRmyPDF", "stars": 11541, "license": "mpl-2.0", "language": "python", "size": 66403}
{"docstring": "# For multiprocessing we must be able to pickle our information - if\n# this fails then we are probably storing some unpickleabe pikepdf or\n# other external data around\n", "func_signal": "def test_pickle(resources):\n", "code": "filename = resources / 'graph_ocred.pdf'\npdf = pdfinfo.PdfInfo(filename)\npickle.dumps(pdf)", "path": "OCRmyPDF/tests/test_pdfinfo.py", "commit_date": "2020-08-05 00:00:00", "repo_name": "ocrmypdf/OCRmyPDF", "stars": 11541, "license": "mpl-2.0", "language": "python", "size": 66403}
{"docstring": "# Font encoding is specified either by a name of\n# built-in encoding or a dictionary that describes\n# the differences.\n", "func_signal": "def PDFSimpleFont__init__(self, descriptor, widths, spec):\n", "code": "original_PDFSimpleFont_init(self, descriptor, widths, spec)\n# pdfminer is incorrect. If there is no ToUnicode and no Encoding, do not\n# assume Unicode conversion is possible. RM 9.10.2\nif not self.unicode_map and 'Encoding' not in spec:\n    self.cid2unicode = {}\nreturn", "path": "OCRmyPDF/src/ocrmypdf/pdfinfo/layout.py", "commit_date": "2020-12-25 00:00:00", "repo_name": "ocrmypdf/OCRmyPDF", "stars": 11541, "license": "mpl-2.0", "language": "python", "size": 66403}
{"docstring": "\"\"\"Update this page's fonts with a reference to the Glyphless font\"\"\"\n\n", "func_signal": "def _update_page_resources(*, page, font, font_key, procset):\n", "code": "if '/Resources' not in page:\n    page['/Resources'] = pikepdf.Dictionary({})\nresources = page['/Resources']\ntry:\n    fonts = resources['/Font']\nexcept KeyError:\n    fonts = pikepdf.Dictionary({})\nif font_key is not None and font_key not in fonts:\n    fonts[font_key] = font\nresources['/Font'] = fonts\n\n# Reassign /ProcSet to one that just lists everything - ProcSet is\n# obsolete and doesn't matter but recommended for old viewer support\nresources['/ProcSet'] = procset", "path": "OCRmyPDF/src/ocrmypdf/_graft.py", "commit_date": "2020-10-24 00:00:00", "repo_name": "ocrmypdf/OCRmyPDF", "stars": 11541, "license": "mpl-2.0", "language": "python", "size": 66403}
{"docstring": "\"\"\"Wrapper around :py:func:`subprocess.run`\n\nThe main purpose of this wrapper is to log subprocess output in an orderly\nfashion that indentifies the responsible subprocess. An additional\ntask is that this function goes to greater lengths to find possible Windows\nlocations of our dependencies when they are not on the system PATH.\n\nArguments should be identical to ``subprocess.run``, except for following:\n\nArguments:\n    logs_errors_to_stdout: If True, indicates that the process writes its error\n        messages to stdout rather than stderr, so stdout should be logged\n        if there is an error. If False, stderr is logged. Could be used with\n        stderr=STDOUT, stdout=PIPE for example.\n\"\"\"\n", "func_signal": "def run(args, *, env=None, logs_errors_to_stdout=False, **kwargs):\n", "code": "args, env, process_log, _text = _fix_process_args(args, env, kwargs)\n\nstderr = None\nstderr_name = 'stderr' if not logs_errors_to_stdout else 'stdout'\ntry:\n    proc = subprocess_run(args, env=env, **kwargs)\nexcept CalledProcessError as e:\n    stderr = getattr(e, stderr_name, None)\n    raise\nelse:\n    stderr = getattr(proc, stderr_name, None)\nfinally:\n    if process_log.isEnabledFor(logging.DEBUG) and stderr:\n        with suppress(AttributeError, UnicodeDecodeError):\n            stderr = stderr.decode('utf-8', 'replace')\n        if logs_errors_to_stdout:\n            process_log.debug(\"stdout/stderr = %s\", stderr)\n        else:\n            process_log.debug(\"stderr = %s\", stderr)\nreturn proc", "path": "OCRmyPDF/src/ocrmypdf/subprocess/__init__.py", "commit_date": "2020-12-11 00:00:00", "repo_name": "ocrmypdf/OCRmyPDF", "stars": 11541, "license": "mpl-2.0", "language": "python", "size": 66403}
{"docstring": "\"\"\"Check if characters can be combined into a textline\n\nWe consider characters compatible if:\n    - the Unicode mapping is known, and both have the same render mode\n    - the Unicode mapping is unknown but both are part of the same font\n\"\"\"\n# pylint: disable=protected-access\n", "func_signal": "def is_compatible(self, obj):\n", "code": "both_unicode_mapped = isinstance(self._text, str) and isinstance(obj._text, str)\ntry:\n    if both_unicode_mapped:\n        return self.rendermode == obj.rendermode\n    font0, _ = self._text\n    font1, _ = obj._text\n    return font0 == font1 and self.rendermode == obj.rendermode\nexcept (ValueError, AttributeError):\n    return False", "path": "OCRmyPDF/src/ocrmypdf/pdfinfo/layout.py", "commit_date": "2020-12-25 00:00:00", "repo_name": "ocrmypdf/OCRmyPDF", "stars": 11541, "license": "mpl-2.0", "language": "python", "size": 66403}
{"docstring": "# This loop waits to make sure that the file is completely loaded on\n# disk before attempting to read. Docker sometimes will publish the\n# watchdog event before the file is actually fully on disk, causing\n# pikepdf to fail.\n\n", "func_signal": "def wait_for_file_ready(file_path):\n", "code": "retries = 5\nwhile retries:\n    try:\n        pdf = pikepdf.open(file_path)\n    except (FileNotFoundError, pikepdf.PdfError) as e:\n        log.info(f\"File {file_path} is not ready yet\")\n        log.debug(\"Exception was\", exc_info=e)\n        time.sleep(POLL_NEW_FILE_SECONDS)\n        retries -= 1\n    else:\n        pdf.close()\n        return True\n\nreturn False", "path": "OCRmyPDF/misc/watcher.py", "commit_date": "2020-12-27 00:00:00", "repo_name": "ocrmypdf/OCRmyPDF", "stars": 11541, "license": "mpl-2.0", "language": "python", "size": 66403}
{"docstring": "# Ensure the input image does not contain pure white/black\n", "func_signal": "def test_remove_background(resources, outdir):\n", "code": "with Image.open(resources / 'congress.jpg') as im:\n    assert im.getextrema() != ((0, 255), (0, 255), (0, 255))\n\noutput_pdf = check_ocrmypdf(\n    resources / 'congress.jpg',\n    outdir / 'test_remove_bg.pdf',\n    '--remove-background',\n    '--image-dpi',\n    '150',\n    '--plugin',\n    'tests/plugins/tesseract_noop.py',\n)\n\noutput_png = outdir / 'remove_bg.png'\n\nghostscript.rasterize_pdf(\n    output_pdf,\n    output_png,\n    raster_device='png16m',\n    raster_dpi=Resolution(100, 100),\n    pageno=1,\n)\n\n# The output image should contain pure white and black\nwith Image.open(output_png) as im:\n    assert im.getextrema() == ((0, 255), (0, 255), (0, 255))", "path": "OCRmyPDF/tests/test_preprocessing.py", "commit_date": "2020-08-05 00:00:00", "repo_name": "ocrmypdf/OCRmyPDF", "stars": 11541, "license": "mpl-2.0", "language": "python", "size": 66403}
{"docstring": "# Ensure that log messages appear before the progress bar, even when\n# printed after the progress bar updates.\n", "func_signal": "def before_pbar(message):\n", "code": "v = bio.getvalue()\npbar_start_marker = '|#'\nreturn v.index(message) < v.index(pbar_start_marker)", "path": "OCRmyPDF/tests/test_api.py", "commit_date": "2020-08-05 00:00:00", "repo_name": "ocrmypdf/OCRmyPDF", "stars": 11541, "license": "mpl-2.0", "language": "python", "size": 66403}
{"docstring": "\"\"\"This document contains an image that is rotated 90 into place with a\n/Rotate tag and intentionally skewed by altering the transformation matrix.\n\nThis tests for a bug where the combination of preprocessing and a tesseract\ntimeout produced a page whose dimensions did not match the original's.\n\"\"\"\n\n", "func_signal": "def test_rotated_skew_timeout(resources, outpdf):\n", "code": "input_file = resources / 'rotated_skew.pdf'\nin_pageinfo = PdfInfo(input_file)[0]\n\nassert (\n    in_pageinfo.height_pixels < in_pageinfo.width_pixels\n), \"Expected the input page to be landscape\"\nassert in_pageinfo.rotation == 90, \"Expected a rotated page\"\n\nout = check_ocrmypdf(\n    input_file,\n    outpdf,\n    '--pdf-renderer',\n    'hocr',\n    '--deskew',\n    '--tesseract-timeout',\n    '0',\n)\n\nout_pageinfo = PdfInfo(out)[0]\nw, h = out_pageinfo.width_pixels, out_pageinfo.height_pixels\n\nassert h > w, \"Expected the output page to be portrait\"\n\nassert out_pageinfo.rotation == 0, \"Expected no page rotation for output\"\n\nassert (\n    in_pageinfo.width_pixels == h and in_pageinfo.height_pixels == w\n), \"Expected page rotation to be baked in\"", "path": "OCRmyPDF/tests/test_rotation.py", "commit_date": "2020-11-07 00:00:00", "repo_name": "ocrmypdf/OCRmyPDF", "stars": 11541, "license": "mpl-2.0", "language": "python", "size": 66403}
{"docstring": "\"\"\"Save and reload the Pdf.\n\nThis will keep a lid on our memory usage for very large files. Attach\nthe font to page 1 even if page 1 doesn't use it, so we have a way to get it\nback.\n\"\"\"\n\n", "func_signal": "def save_and_reload(self):\n", "code": "page0 = self.pdf_base.pages[0]\n_update_page_resources(\n    page=page0, font=self.font, font_key=self.font_key, procset=self.procset\n)\n\n# We cannot read and write the same file, that will corrupt it\n# but we don't to keep more copies than we need to. Delete intermediates.\n# {interim_count} is the opened file we were updateing\n# {interim_count - 1} can be deleted\n# {interim_count + 1} is the new file will produce and open\nold_file = self.output_file.with_suffix(f'.working{self.interim_count - 1}.pdf')\nif not self.context.options.keep_temporary_files:\n    with suppress(FileNotFoundError):\n        old_file.unlink()\n\nnext_file = self.output_file.with_suffix(\n    f'.working{self.interim_count + 1}.pdf'\n)\nself.pdf_base.save(next_file)\nself.pdf_base.close()\n\nself.pdf_base = pikepdf.open(next_file)\nself.procset = self.pdf_base.pages[0].Resources.ProcSet\nself.font, self.font_key = None, None  # Ensure we reacquire this information\nself.interim_count += 1", "path": "OCRmyPDF/src/ocrmypdf/_graft.py", "commit_date": "2020-10-24 00:00:00", "repo_name": "ocrmypdf/OCRmyPDF", "stars": 11541, "license": "mpl-2.0", "language": "python", "size": 66403}
{"docstring": "# Run with deskew\n", "func_signal": "def test_deskew(resources, outdir):\n", "code": "deskewed_pdf = check_ocrmypdf(\n    resources / 'skew.pdf',\n    outdir / 'skew.pdf',\n    '-d',\n    '--plugin',\n    'tests/plugins/tesseract_noop.py',\n)\n\n# Now render as an image again and use Leptonica to find the skew angle\n# to confirm that it was deskewed\ndeskewed_png = outdir / 'deskewed.png'\n\nghostscript.rasterize_pdf(\n    deskewed_pdf,\n    deskewed_png,\n    raster_device='pngmono',\n    raster_dpi=Resolution(150, 150),\n    pageno=1,\n)\n\npix = Pix.open(deskewed_png)\nskew_angle, _skew_confidence = pix.find_skew()\n\nprint(skew_angle)\nassert -0.5 < skew_angle < 0.5, \"Deskewing failed\"", "path": "OCRmyPDF/tests/test_preprocessing.py", "commit_date": "2020-08-05 00:00:00", "repo_name": "ocrmypdf/OCRmyPDF", "stars": 11541, "license": "mpl-2.0", "language": "python", "size": 66403}
{"docstring": "\"\"\"\nTest enabling remote authentication with the default configuration.\n\"\"\"\n", "func_signal": "def test_remote_auth_disabled(self):\n", "code": "headers = {\n    'HTTP_REMOTE_USER': 'remoteuser1',\n}\n\nself.assertFalse(settings.REMOTE_AUTH_ENABLED)\nself.assertEqual(settings.REMOTE_AUTH_HEADER, 'HTTP_REMOTE_USER')\n\n# Client should not be authenticated\nresponse = self.client.get(reverse('home'), follow=True, **headers)\nself.assertNotIn('_auth_user_id', self.client.session)", "path": "netbox/netbox/netbox/tests/test_authentication.py", "commit_date": "2020-12-01 00:00:00", "repo_name": "netbox-community/netbox", "stars": 14675, "license": "apache-2.0", "language": "python", "size": 86716}
{"docstring": "# Attempt to edit an object without permission\n", "func_signal": "def test_edit_object(self):\n", "code": "        data = {'site': self.sites[0].pk}\n        url = reverse('ipam-api:prefix-detail', kwargs={'pk': self.prefixes[0].pk})\n        response = self.client.patch(url, data, format='json', **self.header)\n        self.assertEqual(response.status_code, 403)\n# Assign object permission\n        obj_perm = ObjectPermission(\n            name='Test permission',\n            constraints={'site__name': 'Site 1'},\n            actions=['change']\n        )\n        obj_perm.save()\n        obj_perm.users.add(self.user)\n        obj_perm.object_types.add(ContentType.objects.get_for_model(Prefix))\n# Attempt to edit a non-permitted object\n        data = {'site': self.sites[0].pk}\n        url = reverse('ipam-api:prefix-detail', kwargs={'pk': self.prefixes[3].pk})\n        response = self.client.patch(url, data, format='json', **self.header)\n        self.assertEqual(response.status_code, 404)\n# Edit a permitted object\n        data['status'] = 'reserved'\n        url = reverse('ipam-api:prefix-detail', kwargs={'pk': self.prefixes[0].pk})\n        response = self.client.patch(url, data, format='json', **self.header)\n        self.assertEqual(response.status_code, 200)\n# Attempt to modify a permitted object to a non-permitted object\n        data['site'] = self.sites[1].pk\n        url = reverse('ipam-api:prefix-detail', kwargs={'pk': self.prefixes[0].pk})\n        response = self.client.patch(url, data, format='json', **self.header)\n        self.assertEqual(response.status_code, 403)", "path": "netbox/netbox/netbox/tests/test_authentication.py", "commit_date": "2020-12-01 00:00:00", "repo_name": "netbox-community/netbox", "stars": 14675, "license": "apache-2.0", "language": "python", "size": 86716}
{"docstring": "\"\"\"\nCreate a test user and token for API calls.\n\"\"\"\n", "func_signal": "def setUp(self):\n", "code": "self.user = User.objects.create(username='testuser')\nself.token = Token.objects.create(user=self.user)\nself.header = {'HTTP_AUTHORIZATION': 'Token {}'.format(self.token.key)}", "path": "netbox/netbox/netbox/tests/test_authentication.py", "commit_date": "2020-12-01 00:00:00", "repo_name": "netbox-community/netbox", "stars": 14675, "license": "apache-2.0", "language": "python", "size": 86716}
{"docstring": "\"\"\"\nFilter the QuerySet to return only objects on which the specified user has been granted the specified\npermission.\n\n:param user: User instance\n:param action: The action which must be permitted (e.g. \"view\" for \"dcim.view_site\"); default is 'view'\n\"\"\"\n# Resolve the full name of the required permission\n", "func_signal": "def restrict(self, user, action='view'):\n", "code": "app_label = self.model._meta.app_label\nmodel_name = self.model._meta.model_name\npermission_required = f'{app_label}.{action}_{model_name}'\n\n# Bypass restriction for superusers and exempt views\nif user.is_superuser or permission_is_exempt(permission_required):\n    qs = self\n\n# User is anonymous or has not been granted the requisite permission\nelif not user.is_authenticated or permission_required not in user.get_all_permissions():\n    qs = self.none()\n\n# Filter the queryset to include only objects with allowed attributes\nelse:\n    attrs = Q()\n    for perm_attrs in user._object_perm_cache[permission_required]:\n        if type(perm_attrs) is list:\n            for p in perm_attrs:\n                attrs |= Q(**p)\n        elif perm_attrs:\n            attrs |= Q(**perm_attrs)\n        else:\n            # Any permission with null constraints grants access to _all_ instances\n            attrs = Q()\n            break\n    qs = self.filter(attrs)\n\nreturn qs", "path": "netbox/netbox/utilities/querysets.py", "commit_date": "2020-08-14 00:00:00", "repo_name": "netbox-community/netbox", "stars": 14675, "license": "apache-2.0", "language": "python", "size": 86716}
{"docstring": "# Attempt to retrieve object without permission\n", "func_signal": "def test_get_object(self):\n", "code": "        url = reverse('ipam-api:prefix-detail', kwargs={'pk': self.prefixes[0].pk})\n        response = self.client.get(url, **self.header)\n        self.assertEqual(response.status_code, 403)\n# Assign object permission\n        obj_perm = ObjectPermission(\n            name='Test permission',\n            constraints={'site__name': 'Site 1'},\n            actions=['view']\n        )\n        obj_perm.save()\n        obj_perm.users.add(self.user)\n        obj_perm.object_types.add(ContentType.objects.get_for_model(Prefix))\n# Retrieve permitted object\n        url = reverse('ipam-api:prefix-detail', kwargs={'pk': self.prefixes[0].pk})\n        response = self.client.get(url, **self.header)\n        self.assertEqual(response.status_code, 200)\n# Attempt to retrieve non-permitted object\n        url = reverse('ipam-api:prefix-detail', kwargs={'pk': self.prefixes[3].pk})\n        response = self.client.get(url, **self.header)\n        self.assertEqual(response.status_code, 404)", "path": "netbox/netbox/netbox/tests/test_authentication.py", "commit_date": "2020-12-01 00:00:00", "repo_name": "netbox-community/netbox", "stars": 14675, "license": "apache-2.0", "language": "python", "size": 86716}
{"docstring": "\"\"\"\nFind Webhook(s) assigned to this instance + action and enqueue them\nto be processed\n\"\"\"\n# Determine whether this type of object supports webhooks\n", "func_signal": "def enqueue_webhooks(instance, user, request_id, action):\n", "code": "app_label = instance._meta.app_label\nmodel_name = instance._meta.model_name\nif model_name not in registry['model_features']['webhooks'].get(app_label, []):\n    return\n\n# Retrieve any applicable Webhooks\ncontent_type = ContentType.objects.get_for_model(instance)\naction_flag = {\n    ObjectChangeActionChoices.ACTION_CREATE: 'type_create',\n    ObjectChangeActionChoices.ACTION_UPDATE: 'type_update',\n    ObjectChangeActionChoices.ACTION_DELETE: 'type_delete',\n}[action]\nwebhooks = Webhook.objects.filter(content_types=content_type, enabled=True, **{action_flag: True})\n\nif webhooks.exists():\n    # Get the Model's API serializer class and serialize the object\n    serializer_class = get_serializer_for_model(instance.__class__)\n    serializer_context = {\n        'request': None,\n    }\n    serializer = serializer_class(instance, context=serializer_context)\n\n    # Enqueue the webhooks\n    webhook_queue = get_queue('default')\n    for webhook in webhooks:\n        webhook_queue.enqueue(\n            \"extras.webhooks_worker.process_webhook\",\n            webhook,\n            serializer.data,\n            instance._meta.model_name,\n            action,\n            str(timezone.now()),\n            user.username,\n            request_id\n        )", "path": "netbox/netbox/extras/webhooks.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "netbox-community/netbox", "stars": 14675, "license": "apache-2.0", "language": "python", "size": 86716}
{"docstring": "\"\"\"\nCheck that creating a virtual machine with a duplicate name fails.\n\"\"\"\n", "func_signal": "def test_unique_name_per_cluster_constraint(self):\n", "code": "data = {\n    'name': 'Virtual Machine 1',\n    'cluster': Cluster.objects.first().pk,\n}\nurl = reverse('virtualization-api:virtualmachine-list')\nself.add_permissions('virtualization.add_virtualmachine')\n\nresponse = self.client.post(url, data, format='json', **self.header)\nself.assertHttpStatus(response, status.HTTP_400_BAD_REQUEST)", "path": "netbox/netbox/virtualization/tests/test_api.py", "commit_date": "2020-11-02 00:00:00", "repo_name": "netbox-community/netbox", "stars": 14675, "license": "apache-2.0", "language": "python", "size": 86716}
{"docstring": "\"\"\"\nGenerate a user-friendly error message in response to a ProtectedError exception.\n\"\"\"\n", "func_signal": "def handle_protectederror(obj_list, request, e):\n", "code": "protected_objects = list(e.protected_objects)\nprotected_count = len(protected_objects) if len(protected_objects) <= 50 else 'More than 50'\nerr_message = f\"Unable to delete <strong>{', '.join(str(obj) for obj in obj_list)}</strong>. \" \\\n              f\"{protected_count} dependent objects were found: \"\n\n# Append dependent objects to error message\ndependent_objects = []\nfor dependent in protected_objects[:50]:\n    if hasattr(dependent, 'get_absolute_url'):\n        dependent_objects.append(f'<a href=\"{dependent.get_absolute_url()}\">{escape(dependent)}</a>')\n    else:\n        dependent_objects.append(str(dependent))\nerr_message += ', '.join(dependent_objects)\n\nmessages.error(request, mark_safe(err_message))", "path": "netbox/netbox/utilities/error_handlers.py", "commit_date": "2020-08-20 00:00:00", "repo_name": "netbox-community/netbox", "stars": 14675, "license": "apache-2.0", "language": "python", "size": 86716}
{"docstring": "\"\"\"\nReplace the stock determine_actions() method to assess object permissions only\nwhen viewing a specific object. This is necessary to support OPTIONS requests\nwith bulk update in place (see #5470).\n\"\"\"\n", "func_signal": "def determine_actions(self, request, view):\n", "code": "actions = {}\nfor method in {'PUT', 'POST'} & set(view.allowed_methods):\n    view.request = clone_request(request, method)\n    try:\n        # Test global permissions\n        if hasattr(view, 'check_permissions'):\n            view.check_permissions(view.request)\n        # Test object permissions (if viewing a specific object)\n        if method == 'PUT' and view.lookup_url_kwarg and hasattr(view, 'get_object'):\n            view.get_object()\n    except (exceptions.APIException, PermissionDenied, Http404):\n        pass\n    else:\n        # If user has appropriate permissions for the view, include\n        # appropriate metadata about the fields that should be supplied.\n        serializer = view.get_serializer()\n        actions[method] = self.get_serializer_info(serializer)\n    finally:\n        view.request = request\n\nreturn actions", "path": "netbox/netbox/netbox/api/metadata.py", "commit_date": "2020-12-16 00:00:00", "repo_name": "netbox-community/netbox", "stars": 14675, "license": "apache-2.0", "language": "python", "size": 86716}
{"docstring": "\"\"\"\nCheck that config context data can be excluded by passing ?exclude=config_context.\n\"\"\"\n", "func_signal": "def test_config_context_excluded(self):\n", "code": "url = reverse('virtualization-api:virtualmachine-list') + '?exclude=config_context'\nself.add_permissions('virtualization.view_virtualmachine')\n\nresponse = self.client.get(url, **self.header)\nself.assertFalse('config_context' in response.data['results'][0])", "path": "netbox/netbox/virtualization/tests/test_api.py", "commit_date": "2020-11-02 00:00:00", "repo_name": "netbox-community/netbox", "stars": 14675, "license": "apache-2.0", "language": "python", "size": 86716}
{"docstring": "\"\"\"Test that a `label` cannot be generated for each generated `name` from `name_pattern` due to invalid `label_pattern` on InterfaceCreateForm\"\"\"\n", "func_signal": "def test_interface_label_count_mismatch(self):\n", "code": "bad_interface_data = {\n    'device': self.device.pk,\n    'name_pattern': 'eth[0-9]',\n    'label_pattern': 'Interface[0-1]',\n    'type': InterfaceTypeChoices.TYPE_100ME_FIXED,\n}\nform = InterfaceCreateForm(bad_interface_data)\n\nself.assertFalse(form.is_valid())\nself.assertIn('label_pattern', form.errors)", "path": "netbox/netbox/dcim/tests/test_forms.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "netbox-community/netbox", "stars": 14675, "license": "apache-2.0", "language": "python", "size": 86716}
{"docstring": "\"\"\"\nTest enabling remote authentication with the default configuration.\n\"\"\"\n", "func_signal": "def test_remote_auth_default_permissions(self):\n", "code": "headers = {\n    'HTTP_REMOTE_USER': 'remoteuser2',\n}\n\nself.assertTrue(settings.REMOTE_AUTH_ENABLED)\nself.assertTrue(settings.REMOTE_AUTH_AUTO_CREATE_USER)\nself.assertEqual(settings.REMOTE_AUTH_HEADER, 'HTTP_REMOTE_USER')\nself.assertEqual(settings.REMOTE_AUTH_DEFAULT_PERMISSIONS, {'dcim.add_site': None, 'dcim.change_site': None})\n\nresponse = self.client.get(reverse('home'), follow=True, **headers)\nself.assertEqual(response.status_code, 200)\n\nnew_user = User.objects.get(username='remoteuser2')\nself.assertEqual(int(self.client.session.get('_auth_user_id')), new_user.pk, msg='Authentication failed')\nself.assertTrue(new_user.has_perms(['dcim.add_site', 'dcim.change_site']))", "path": "netbox/netbox/netbox/tests/test_authentication.py", "commit_date": "2020-12-01 00:00:00", "repo_name": "netbox-community/netbox", "stars": 14675, "license": "apache-2.0", "language": "python", "size": 86716}
{"docstring": "\"\"\"\nCheck that config context data is included by default in the virtual machines list.\n\"\"\"\n", "func_signal": "def test_config_context_included_by_default_in_list_view(self):\n", "code": "virtualmachine = VirtualMachine.objects.first()\nurl = '{}?id={}'.format(reverse('virtualization-api:virtualmachine-list'), virtualmachine.pk)\nself.add_permissions('virtualization.view_virtualmachine')\n\nresponse = self.client.get(url, **self.header)\nself.assertEqual(response.data['results'][0].get('config_context', {}).get('A'), 1)", "path": "netbox/netbox/virtualization/tests/test_api.py", "commit_date": "2020-11-02 00:00:00", "repo_name": "netbox-community/netbox", "stars": 14675, "license": "apache-2.0", "language": "python", "size": 86716}
{"docstring": "# Attempt to delete an object without permission\n", "func_signal": "def test_delete_object(self):\n", "code": "        url = reverse('ipam-api:prefix-detail', kwargs={'pk': self.prefixes[0].pk})\n        response = self.client.delete(url, format='json', **self.header)\n        self.assertEqual(response.status_code, 403)\n# Assign object permission\n        obj_perm = ObjectPermission(\n            name='Test permission',\n            constraints={'site__name': 'Site 1'},\n            actions=['delete']\n        )\n        obj_perm.save()\n        obj_perm.users.add(self.user)\n        obj_perm.object_types.add(ContentType.objects.get_for_model(Prefix))\n# Attempt to delete a non-permitted object\n        url = reverse('ipam-api:prefix-detail', kwargs={'pk': self.prefixes[3].pk})\n        response = self.client.delete(url, format='json', **self.header)\n        self.assertEqual(response.status_code, 404)\n# Delete a permitted object\n        url = reverse('ipam-api:prefix-detail', kwargs={'pk': self.prefixes[0].pk})\n        response = self.client.delete(url, format='json', **self.header)\n        self.assertEqual(response.status_code, 204)", "path": "netbox/netbox/netbox/tests/test_authentication.py", "commit_date": "2020-12-01 00:00:00", "repo_name": "netbox-community/netbox", "stars": 14675, "license": "apache-2.0", "language": "python", "size": 86716}
{"docstring": "\"\"\"\nReturn a cryptographic signature that can be used to verify the authenticity of webhook data.\n\"\"\"\n", "func_signal": "def generate_signature(request_body, secret):\n", "code": "hmac_prep = hmac.new(\n    key=secret.encode('utf8'),\n    msg=request_body,\n    digestmod=hashlib.sha512\n)\nreturn hmac_prep.hexdigest()", "path": "netbox/netbox/extras/webhooks.py", "commit_date": "2020-12-02 00:00:00", "repo_name": "netbox-community/netbox", "stars": 14675, "license": "apache-2.0", "language": "python", "size": 86716}
{"docstring": "\"\"\"Test that a `label` can be generated for each generated `name` from `name_pattern` on InterfaceCreateForm\"\"\"\n", "func_signal": "def test_interface_label_count_valid(self):\n", "code": "interface_data = {\n    'device': self.device.pk,\n    'name_pattern': 'eth[0-9]',\n    'label_pattern': 'Interface[0-9]',\n    'type': InterfaceTypeChoices.TYPE_100ME_FIXED,\n}\nform = InterfaceCreateForm(interface_data)\n\nself.assertTrue(form.is_valid())", "path": "netbox/netbox/dcim/tests/test_forms.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "netbox-community/netbox", "stars": 14675, "license": "apache-2.0", "language": "python", "size": 86716}
{"docstring": "\"\"\"\nTest enabling remote authentication with the default configuration.\n\"\"\"\n", "func_signal": "def test_remote_auth_default_groups(self):\n", "code": "headers = {\n    'HTTP_REMOTE_USER': 'remoteuser2',\n}\n\nself.assertTrue(settings.REMOTE_AUTH_ENABLED)\nself.assertTrue(settings.REMOTE_AUTH_AUTO_CREATE_USER)\nself.assertEqual(settings.REMOTE_AUTH_HEADER, 'HTTP_REMOTE_USER')\nself.assertEqual(settings.REMOTE_AUTH_DEFAULT_GROUPS, ['Group 1', 'Group 2'])\n\n# Create required groups\ngroups = (\n    Group(name='Group 1'),\n    Group(name='Group 2'),\n    Group(name='Group 3'),\n)\nGroup.objects.bulk_create(groups)\n\nresponse = self.client.get(reverse('home'), follow=True, **headers)\nself.assertEqual(response.status_code, 200)\n\nnew_user = User.objects.get(username='remoteuser2')\nself.assertEqual(int(self.client.session.get('_auth_user_id')), new_user.pk, msg='Authentication failed')\nself.assertListEqual(\n    [groups[0], groups[1]],\n    list(new_user.groups.all())\n)", "path": "netbox/netbox/netbox/tests/test_authentication.py", "commit_date": "2020-12-01 00:00:00", "repo_name": "netbox-community/netbox", "stars": 14675, "license": "apache-2.0", "language": "python", "size": 86716}
{"docstring": "\"\"\"\nTest enabling remote authentication with automatic user creation disabled.\n\"\"\"\n", "func_signal": "def test_remote_auth_auto_create(self):\n", "code": "headers = {\n    'HTTP_REMOTE_USER': 'remoteuser2',\n}\n\nself.assertTrue(settings.REMOTE_AUTH_ENABLED)\nself.assertTrue(settings.REMOTE_AUTH_AUTO_CREATE_USER)\nself.assertEqual(settings.REMOTE_AUTH_HEADER, 'HTTP_REMOTE_USER')\n\nresponse = self.client.get(reverse('home'), follow=True, **headers)\nself.assertEqual(response.status_code, 200)\n\n# Local user should have been automatically created\nnew_user = User.objects.get(username='remoteuser2')\nself.assertEqual(int(self.client.session.get('_auth_user_id')), new_user.pk, msg='Authentication failed')", "path": "netbox/netbox/netbox/tests/test_authentication.py", "commit_date": "2020-12-01 00:00:00", "repo_name": "netbox-community/netbox", "stars": 14675, "license": "apache-2.0", "language": "python", "size": 86716}
{"docstring": "\"\"\"\nTest enabling remote authentication with a custom HTTP header.\n\"\"\"\n", "func_signal": "def test_remote_auth_custom_header(self):\n", "code": "headers = {\n    'HTTP_FOO': 'remoteuser1',\n}\n\nself.assertTrue(settings.REMOTE_AUTH_ENABLED)\nself.assertEqual(settings.REMOTE_AUTH_HEADER, 'HTTP_FOO')\n\nresponse = self.client.get(reverse('home'), follow=True, **headers)\nself.assertEqual(response.status_code, 200)\nself.assertEqual(int(self.client.session.get('_auth_user_id')), self.user.pk, msg='Authentication failed')", "path": "netbox/netbox/netbox/tests/test_authentication.py", "commit_date": "2020-12-01 00:00:00", "repo_name": "netbox-community/netbox", "stars": 14675, "license": "apache-2.0", "language": "python", "size": 86716}
{"docstring": "\"\"\"\nTest enabling remote authentication with the default configuration.\n\"\"\"\n", "func_signal": "def test_remote_auth_enabled(self):\n", "code": "headers = {\n    'HTTP_REMOTE_USER': 'remoteuser1',\n}\n\nself.assertTrue(settings.REMOTE_AUTH_ENABLED)\nself.assertEqual(settings.REMOTE_AUTH_HEADER, 'HTTP_REMOTE_USER')\n\nresponse = self.client.get(reverse('home'), follow=True, **headers)\nself.assertEqual(response.status_code, 200)\nself.assertEqual(int(self.client.session.get('_auth_user_id')), self.user.pk, msg='Authentication failed')", "path": "netbox/netbox/netbox/tests/test_authentication.py", "commit_date": "2020-12-01 00:00:00", "repo_name": "netbox-community/netbox", "stars": 14675, "license": "apache-2.0", "language": "python", "size": 86716}
{"docstring": "# NOTE: this is horrible. Any simpler version ?\n", "func_signal": "def test_issue_334():\n", "code": "last_move = None\nlast_move1 = None\n\nlis = [\n    (0.0, 113, 167, 47),\n    (0.32, 138, 159, 47),\n    (0.44, 152, 144, 47),\n    (0.48, 193, 148, 47),\n    (0.6, 193, 148, 47),\n    (0.76, 205, 138, 55),\n    (0.88, 204, 121, 63),\n    (0.92, 190, 31, 127),\n    (1.2, 183, 59, 127),\n    (1.4, 137, 22, 127),\n    (1.52, 137, 22, 127),\n    (1.72, 129, 67, 127),\n    (1.88, 123, 69, 127),\n    (2.04, 131, 123, 63),\n    (2.24, 130, 148, 63),\n    (2.48, 130, 148, 63),\n    (2.8, 138, 180, 63),\n    (3.0, 138, 180, 63),\n    (3.2, 146, 192, 63),\n    (3.28, 105, 91, 151),\n    (3.44, 105, 91, 151),\n    (3.72, 11, 48, 151),\n    (3.96, 5, 78, 151),\n    (4.32, 4, 134, 1),\n    (4.6, 149, 184, 48),\n    (4.8, 145, 188, 48),\n    (5.0, 154, 217, 48),\n    (5.08, 163, 199, 48),\n    (5.2, 163, 199, 48),\n    (5.32, 164, 187, 48),\n    (5.48, 163, 200, 48),\n    (5.76, 163, 200, 48),\n    (5.96, 173, 199, 48),\n    (6.0, 133, 172, 48),\n    (6.04, 128, 165, 48),\n    (6.28, 128, 165, 48),\n    (6.4, 129, 180, 48),\n    (6.52, 133, 166, 48),\n    (6.64, 133, 166, 48),\n    (6.88, 144, 183, 48),\n    (7.0, 153, 174, 48),\n    (7.16, 153, 174, 48),\n    (7.24, 153, 174, 48),\n    (7.28, 253, 65, 104),\n    (7.64, 253, 65, 104),\n    (7.8, 279, 116, 80),\n    (8.0, 290, 105, 80),\n    (8.24, 288, 124, 80),\n    (8.44, 243, 102, 80),\n    (8.56, 243, 102, 80),\n    (8.8, 202, 107, 80),\n    (8.84, 164, 27, 104),\n    (9.0, 164, 27, 104),\n    (9.12, 121, 9, 104),\n    (9.28, 77, 33, 104),\n    (9.32, 52, 23, 104),\n    (9.48, 52, 23, 104),\n    (9.64, 33, 46, 104),\n    (9.8, 93, 49, 104),\n    (9.92, 93, 49, 104),\n    (10.16, 173, 19, 104),\n    (10.2, 226, 173, 48),\n    (10.36, 226, 173, 48),\n    (10.48, 211, 172, 48),\n    (10.64, 208, 162, 48),\n    (10.92, 220, 171, 48),\n]\n\nlis1 = [\n    (0.0, 113, 167, 47),\n    (0.32, 138, 159, 47),\n    (0.44, 152, 144, 47),\n    (0.48, 193, 148, 47),\n    (0.6, 193, 148, 47),\n    (0.76, 205, 138, 55),\n    (0.88, 204, 121, 63),\n    (0.92, 190, 31, 127),\n    (1.2, 183, 59, 127),\n    (1.4, 137, 22, 127),\n    (1.52, 137, 22, 127),\n    (1.72, 129, 67, 127),\n    (1.88, 123, 69, 127),\n    (2.04, 131, 123, 63),\n    (2.24, 130, 148, 63),\n    (2.48, 130, 148, 63),\n    (2.8, 138, 180, 63),\n    (3.0, 138, 180, 63),\n    (3.2, 146, 192, 63),\n    (3.28, 105, 91, 151),\n    (3.44, 105, 91, 151),\n    (3.72, 11, 48, 151),\n    (3.96, 5, 78, 151),\n    (4.32, 4, 134, 1),\n    (4.6, 149, 184, 48),\n    (4.8, 145, 188, 48),\n    (5.0, 154, 217, 48),\n    (5.08, 163, 199, 48),\n    (5.2, 163, 199, 48),\n    (5.32, 164, 187, 48),\n    (5.48, 163, 200, 48),\n    (5.76, 163, 200, 48),\n    (5.96, 173, 199, 48),\n    (6.0, 133, 172, 48),\n    (6.04, 128, 165, 48),\n    (6.28, 128, 165, 48),\n    (6.4, 129, 180, 48),\n    (6.52, 133, 166, 48),\n    (6.64, 133, 166, 48),\n    (6.88, 144, 183, 48),\n    (7.0, 153, 174, 48),\n    (7.16, 153, 174, 48),\n    (7.24, 153, 174, 48),\n    (7.28, 253, 65, 104),\n    (7.64, 253, 65, 104),\n    (7.8, 279, 116, 80),\n    (8.0, 290, 105, 80),\n    (8.24, 288, 124, 80),\n    (8.44, 243, 102, 80),\n    (8.56, 243, 102, 80),\n    (8.8, 202, 107, 80),\n    (8.84, 164, 27, 104),\n    (9.0, 164, 27, 104),\n    (9.12, 121, 9, 104),\n    (9.28, 77, 33, 104),\n    (9.32, 52, 23, 104),\n    (9.48, 52, 23, 104),\n    (9.64, 33, 46, 104),\n    (9.8, 93, 49, 104),\n    (9.92, 93, 49, 104),\n    (10.16, 173, 19, 104),\n    (10.2, 226, 173, 48),\n    (10.36, 226, 173, 48),\n    (10.48, 211, 172, 48),\n    (10.64, 208, 162, 48),\n    (10.92, 220, 171, 48),\n]\n\ndef posi(t):\n    global last_move\n    if len(lis) == 0:\n        return (last_move[1], last_move[2])\n    if t >= lis[0][0]:\n        last_move = item = lis.pop(0)\n        return (item[1], item[2])\n    else:\n        if len(lis) > 0:\n            dura = lis[0][0] - last_move[0]\n            now = t - last_move[0]\n            w = (lis[0][1] - last_move[1]) * (now / dura)\n            h = (lis[0][2] - last_move[2]) * (now / dura)\n            # print t, last_move[1] + w, last_move[2] + h\n            return (last_move[1] + w, last_move[2] + h)\n        return (last_move[1], last_move[2])\n\ndef size(t):\n    global last_move1\n    if len(lis1) == 0:\n        return (last_move1[3], last_move1[3] * 1.33)\n    if t >= lis1[0][0]:\n        last_move1 = item = lis1.pop(0)\n        return (item[3], item[3] * 1.33)\n    else:\n        if len(lis) > 0:\n            dura = lis1[0][0] - last_move1[0]\n            now = t - last_move1[0]\n            s = (lis1[0][3] - last_move1[3]) * (now / dura)\n            nsw = last_move1[3] + s\n            nsh = nsw * 1.33\n            # print t, nsw, nsh\n            return (nsw, nsh)\n        return (last_move1[3], last_move1[3] * 1.33)\n\navatar = VideoFileClip(\"media/big_buck_bunny_432_433.webm\", has_mask=True)\navatar.audio = None\nmaskclip = ImageClip(\"media/afterimage.png\", is_mask=True, transparent=True)\navatar.with_mask(maskclip)  # must set maskclip here..\nconcatenated = concatenate_videoclips([avatar] * 3)\n\ntt = VideoFileClip(\"media/big_buck_bunny_0_30.webm\").subclip(0, 3)\n# TODO: Setting mask here does not work:\n# .with_mask(maskclip).resize(size)])\nfinal = CompositeVideoClip([tt, concatenated.with_position(posi).resize(size)])\nfinal.duration = tt.duration\nfinal.write_videofile(os.path.join(TMP_DIR, \"issue_334.mp4\"), fps=10)", "path": "moviepy/tests/test_issues.py", "commit_date": "2020-10-08 00:00:00", "repo_name": "Zulko/moviepy", "stars": 11574, "license": "mit", "language": "python", "size": 41611}
{"docstring": "\"\"\"\nReturns a valid bitmap list that represents each frame of the clip.\nIf `color_dict` is not specified, then it will use the same `color_dict`\nthat was used to create the clip.\n\"\"\"\n", "func_signal": "def to_bitmap(self, color_dict=None):\n", "code": "color_dict = color_dict or self.color_dict\n\nbitmap = []\nfor frame in self.iter_frames():\n    bitmap.append([])\n    for line in frame:\n        bitmap[-1].append(\"\")\n        for pixel in line:\n            letter = list(color_dict.keys())[\n                list(color_dict.values()).index(tuple(pixel))\n            ]\n            bitmap[-1][-1] += letter\n\nreturn bitmap", "path": "moviepy/moviepy/video/VideoClip.py", "commit_date": "2020-10-09 00:00:00", "repo_name": "Zulko/moviepy", "stars": 11574, "license": "mit", "language": "python", "size": 41611}
{"docstring": "\"\"\"\nReturns an ImageClip made out of the clip's frame at time ``t``,\nwhich can be expressed in seconds (15.35), in (min, sec),\nin (hour, min, sec), or as a string: '01:03:05.35'.\n\"\"\"\n", "func_signal": "def to_ImageClip(self, t=0, with_mask=True, duration=None):\n", "code": "new_clip = ImageClip(self.get_frame(t), is_mask=self.is_mask, duration=duration)\nif with_mask and self.mask is not None:\n    new_clip.mask = self.mask.to_ImageClip(t)\nreturn new_clip", "path": "moviepy/moviepy/video/VideoClip.py", "commit_date": "2020-10-09 00:00:00", "repo_name": "Zulko/moviepy", "stars": 11574, "license": "mit", "language": "python", "size": 41611}
{"docstring": "\"\"\"Place the clip on a colored background.\n\nReturns a clip made of the current clip overlaid on a color\nclip of a possibly bigger size. Can serve to flatten transparent\nclips.\n\nParameters\n-----------\n\nsize\n  Size (width, height) in pixels of the final clip.\n  By default it will be the size of the current clip.\n\ncolor\n  Background color of the final clip ([R,G,B]).\n\npos\n  Position of the clip in the final clip. 'center' is the default\n\ncol_opacity\n  Parameter in 0..1 indicating the opacity of the colored\n  background.\n\n\"\"\"\n", "func_signal": "def on_color(self, size=None, color=(0, 0, 0), pos=None, col_opacity=None):\n", "code": "from .compositing.CompositeVideoClip import CompositeVideoClip\n\nif size is None:\n    size = self.size\nif pos is None:\n    pos = \"center\"\ncolorclip = ColorClip(size, color=color)\n\nif col_opacity is not None:\n    colorclip = ColorClip(\n        size, color=color, duration=self.duration\n    ).with_opacity(col_opacity)\n    result = CompositeVideoClip([colorclip, self.with_position(pos)])\nelse:\n    result = CompositeVideoClip(\n        [self.with_position(pos)], size=size, bg_color=color\n    )\n\nif (\n    isinstance(self, ImageClip)\n    and (not hasattr(pos, \"__call__\"))\n    and ((self.mask is None) or isinstance(self.mask, ImageClip))\n):\n    new_result = result.to_ImageClip()\n    if result.mask is not None:\n        new_result.mask = result.mask.to_ImageClip()\n    return new_result.with_duration(result.duration)\n\nreturn result", "path": "moviepy/moviepy/video/VideoClip.py", "commit_date": "2020-10-09 00:00:00", "repo_name": "Zulko/moviepy", "stars": 11574, "license": "mit", "language": "python", "size": 41611}
{"docstring": "\"\"\"Return a mask a video clip made from the clip.\"\"\"\n", "func_signal": "def to_mask(self, canal=0):\n", "code": "if self.is_mask:\n    return self\nelse:\n    new_clip = self.image_transform(lambda pic: 1.0 * pic[:, :, canal] / 255)\n    new_clip.is_mask = True\n    return new_clip", "path": "moviepy/moviepy/video/VideoClip.py", "commit_date": "2020-10-09 00:00:00", "repo_name": "Zulko/moviepy", "stars": 11574, "license": "mit", "language": "python", "size": 41611}
{"docstring": "\"\"\"\nReturns the result of the blit of the clip's frame at time `t`\non the given `picture`, the position of the clip being given\nby the clip's ``pos`` attribute. Meant for compositing.\n\"\"\"\n", "func_signal": "def blit_on(self, picture, t):\n", "code": "hf, wf = picture.size\n\nct = t - self.start  # clip time\n\n# GET IMAGE AND MASK IF ANY\nimg = self.get_frame(ct).astype(\"uint8\")\nim_img = Image.fromarray(img)\n\nif self.mask is not None:\n    mask = self.mask.get_frame(ct).astype(\"uint8\")\n    im_mask = Image.fromarray(255 * mask).convert(\"L\")\n\n    if im_img.size != im_mask.size:\n        bg_size = (\n            max(im_img.size[0], im_mask.size[0]),\n            max(im_img.size[1], im_mask.size[1]),\n        )\n\n        im_img_bg = Image.new(\"RGB\", bg_size, \"black\")\n        im_img_bg.paste(im_img, (0, 0))\n\n        im_mask_bg = Image.new(\"L\", bg_size, 0)\n        im_mask_bg.paste(im_mask, (0, 0))\n\n        im_img, im_mask = im_img_bg, im_mask_bg\n\nelse:\n    im_mask = None\n\nhi, wi = im_img.size\n# SET POSITION\npos = self.pos(ct)\n\n# preprocess short writings of the position\nif isinstance(pos, str):\n    pos = {\n        \"center\": [\"center\", \"center\"],\n        \"left\": [\"left\", \"center\"],\n        \"right\": [\"right\", \"center\"],\n        \"top\": [\"center\", \"top\"],\n        \"bottom\": [\"center\", \"bottom\"],\n    }[pos]\nelse:\n    pos = list(pos)\n\n# is the position relative (given in % of the clip's size) ?\nif self.relative_pos:\n    for i, dim in enumerate([wf, hf]):\n        if not isinstance(pos[i], str):\n            pos[i] = dim * pos[i]\n\nif isinstance(pos[0], str):\n    D = {\"left\": 0, \"center\": (wf - wi) / 2, \"right\": wf - wi}\n    pos[0] = D[pos[0]]\n\nif isinstance(pos[1], str):\n    D = {\"top\": 0, \"center\": (hf - hi) / 2, \"bottom\": hf - hi}\n    pos[1] = D[pos[1]]\n\npos = map(int, pos)\nreturn blit(im_img, picture, pos, mask=im_mask)", "path": "moviepy/moviepy/video/VideoClip.py", "commit_date": "2020-10-09 00:00:00", "repo_name": "Zulko/moviepy", "stars": 11574, "license": "mit", "language": "python", "size": 41611}
{"docstring": "\"\"\"Save a clip's frame to an image file.\n\nSaves the frame of clip corresponding to time ``t`` in\n'filename'. ``t`` can be expressed in seconds (15.35), in\n(min, sec), in (hour, min, sec), or as a string: '01:03:05.35'.\n\nIf ``with_mask`` is ``True`` the mask is saved in\nthe alpha layer of the picture (only works with PNGs).\n\n\"\"\"\n\n", "func_signal": "def save_frame(self, filename, t=0, with_mask=True):\n", "code": "im = self.get_frame(t)\nif with_mask and self.mask is not None:\n    mask = 255 * self.mask.get_frame(t)\n    im = np.dstack([im, mask]).astype(\"uint8\")\nelse:\n    im = im.astype(\"uint8\")\n\nimsave(filename, im)", "path": "moviepy/moviepy/video/VideoClip.py", "commit_date": "2020-10-09 00:00:00", "repo_name": "Zulko/moviepy", "stars": 11574, "license": "mit", "language": "python", "size": 41611}
{"docstring": "\"\"\"Image-transformation filter.\n\nDoes the same as VideoClip.image_transform, but for ImageClip the\ntranformed clip is computed once and for all at the beginning,\nand not for each 'frame'.\n\"\"\"\n", "func_signal": "def image_transform(self, image_func, apply_to=None):\n", "code": "if apply_to is None:\n    apply_to = []\narr = image_func(self.get_frame(0))\nself.size = arr.shape[:2][::-1]\nself.make_frame = lambda t: arr\nself.img = arr\n\nfor attr in apply_to:\n    a = getattr(self, attr, None)\n    if a is not None:\n        new_a = a.image_transform(image_func)\n        setattr(self, attr, new_a)", "path": "moviepy/moviepy/video/VideoClip.py", "commit_date": "2020-10-09 00:00:00", "repo_name": "Zulko/moviepy", "stars": 11574, "license": "mit", "language": "python", "size": 41611}
{"docstring": "\"\"\"Returns the of all valid entries which contain ``string`` for the\nargument ``arg`` of ``TextClip``, for instance\n\n>>> # Find all the available fonts which contain \"Courier\"\n>>> print(TextClip.search('Courier', 'font'))\n\n\"\"\"\n", "func_signal": "def search(string, arg):\n", "code": "string = string.lower()\nnames_list = TextClip.list(arg)\nreturn [name for name in names_list if string in name.lower()]", "path": "moviepy/moviepy/video/VideoClip.py", "commit_date": "2020-10-09 00:00:00", "repo_name": "Zulko/moviepy", "stars": 11574, "license": "mit", "language": "python", "size": 41611}
{"docstring": "\"\"\"Return a non-mask video clip made from the mask video clip.\"\"\"\n", "func_signal": "def to_RGB(self):\n", "code": "if self.is_mask:\n    new_clip = self.image_transform(\n        lambda pic: np.dstack(3 * [255 * pic]).astype(\"uint8\")\n    )\n    new_clip.is_mask = False\n    return new_clip\nelse:\n    return self", "path": "moviepy/moviepy/video/VideoClip.py", "commit_date": "2020-10-09 00:00:00", "repo_name": "Zulko/moviepy", "stars": 11574, "license": "mit", "language": "python", "size": 41611}
{"docstring": "# ColorClip has no fps attribute.\n", "func_signal": "def test_issue_416():\n", "code": "green = ColorClip((640, 480), color=(0, 255, 0)).with_duration(2)\nvideo1 = concatenate_videoclips([green])\nassert video1.fps is None", "path": "moviepy/tests/test_issues.py", "commit_date": "2020-10-08 00:00:00", "repo_name": "Zulko/moviepy", "stars": 11574, "license": "mit", "language": "python", "size": 41611}
{"docstring": "# failed in python2\n", "func_signal": "def test_issue_417():\n", "code": "cad = \"media/python_logo.png\"\nmyclip = ImageClip(cad).fx(resize, new_size=[1280, 660])\nCompositeVideoClip([myclip], size=(1280, 720))\n# final.with_duration(7).write_videofile(\"test.mp4\", fps=30)", "path": "moviepy/tests/test_issues.py", "commit_date": "2020-10-08 00:00:00", "repo_name": "Zulko/moviepy", "stars": 11574, "license": "mit", "language": "python", "size": 41611}
{"docstring": "\"\"\"General transformation filter.\n\nEquivalent to VideoClip.transform. The result is no more an\nImageClip, it has the class VideoClip (since it may be animated)\n\"\"\"\n", "func_signal": "def transform(self, func, apply_to=None, keep_duration=True):\n", "code": "if apply_to is None:\n    apply_to = []\n# When we use transform on an image clip it may become animated.\n# Therefore the result is not an ImageClip, just a VideoClip.\nnew_clip = VideoClip.transform(\n    self, func, apply_to=apply_to, keep_duration=keep_duration\n)\nnew_clip.__class__ = VideoClip\nreturn new_clip", "path": "moviepy/moviepy/video/VideoClip.py", "commit_date": "2020-10-09 00:00:00", "repo_name": "Zulko/moviepy", "stars": 11574, "license": "mit", "language": "python", "size": 41611}
{"docstring": "\"\"\"\nModifies the images of a clip by replacing the frame\n`get_frame(t)` by another frame,  `image_func(get_frame(t))`\n\"\"\"\n", "func_signal": "def image_transform(self, image_func, apply_to=None):\n", "code": "apply_to = apply_to or []\nreturn self.transform(lambda get_frame, t: image_func(get_frame(t)), apply_to)", "path": "moviepy/moviepy/video/VideoClip.py", "commit_date": "2020-10-09 00:00:00", "repo_name": "Zulko/moviepy", "stars": 11574, "license": "mit", "language": "python", "size": 41611}
{"docstring": "\"\"\"Time-transformation filter.\n\nApplies a transformation to the clip's timeline\n(see Clip.time_transform).\n\nThis method does nothing for ImageClips (but it may affect their\nmasks or their audios). The result is still an ImageClip.\n\"\"\"\n", "func_signal": "def time_transform(self, time_func, apply_to=None, keep_duration=False):\n", "code": "if apply_to is None:\n    apply_to = [\"mask\", \"audio\"]\nfor attr in apply_to:\n    a = getattr(self, attr, None)\n    if a is not None:\n        new_a = a.time_transform(time_func)\n        setattr(self, attr, new_a)", "path": "moviepy/moviepy/video/VideoClip.py", "commit_date": "2020-10-09 00:00:00", "repo_name": "Zulko/moviepy", "stars": 11574, "license": "mit", "language": "python", "size": 41611}
{"docstring": "\"\"\"Add a mask VideoClip to the VideoClip.\n\nReturns a copy of the clip with a completely opaque mask\n(made of ones). This makes computations slower compared to\nhaving a None mask but can be useful in many cases. Choose\n\nSet ``constant_size`` to  `False` for clips with moving\nimage size.\n\"\"\"\n", "func_signal": "def add_mask(self):\n", "code": "if self.has_constant_size:\n    mask = ColorClip(self.size, 1.0, is_mask=True)\n    return self.with_mask(mask.with_duration(self.duration))\nelse:\n\n    def make_frame(t):\n        return np.ones(self.get_frame(t).shape[:2], dtype=float)\n\n    mask = VideoClip(is_mask=True, make_frame=make_frame)\n    return self.with_mask(mask.with_duration(self.duration))", "path": "moviepy/moviepy/video/VideoClip.py", "commit_date": "2020-10-09 00:00:00", "repo_name": "Zulko/moviepy", "stars": 11574, "license": "mit", "language": "python", "size": 41611}
{"docstring": "\"\"\"Change the clip's ``get_frame``.\n\nReturns a copy of the VideoClip instance, with the make_frame\nattribute set to `mf`.\n\"\"\"\n", "func_signal": "def with_make_frame(self, mf):\n", "code": "self.make_frame = mf\nself.size = self.get_frame(0).shape[:2][::-1]", "path": "moviepy/moviepy/video/VideoClip.py", "commit_date": "2020-10-09 00:00:00", "repo_name": "Zulko/moviepy", "stars": 11574, "license": "mit", "language": "python", "size": 41611}
{"docstring": "\"\"\"Set the clip's position in compositions.\n\nSets the position that the clip will have when included\nin compositions. The argument ``pos`` can be either a couple\n``(x,y)`` or a function ``t-> (x,y)``. `x` and `y` mark the\nlocation of the top left corner of the clip, and can be\nof several types.\n\nExamples\n----------\n\n>>> clip.with_position((45,150)) # x=45, y=150\n>>>\n>>> # clip horizontally centered, at the top of the picture\n>>> clip.with_position((\"center\",\"top\"))\n>>>\n>>> # clip is at 40% of the width, 70% of the height:\n>>> clip.with_position((0.4,0.7), relative=True)\n>>>\n>>> # clip's position is horizontally centered, and moving up !\n>>> clip.with_position(lambda t: ('center', 50+t) )\n\n\"\"\"\n", "func_signal": "def with_position(self, pos, relative=False):\n", "code": "self.relative_pos = relative\nif hasattr(pos, \"__call__\"):\n    self.pos = pos\nelse:\n    self.pos = lambda t: pos", "path": "moviepy/moviepy/video/VideoClip.py", "commit_date": "2020-10-09 00:00:00", "repo_name": "Zulko/moviepy", "stars": 11574, "license": "mit", "language": "python", "size": 41611}
{"docstring": "\"\"\"Set the clip's mask.\n\nReturns a copy of the VideoClip with the mask attribute set to\n``mask``, which must be a greyscale (values in 0-1) VideoClip\"\"\"\n", "func_signal": "def with_mask(self, mask):\n", "code": "assert mask is None or mask.is_mask\nself.mask = mask", "path": "moviepy/moviepy/video/VideoClip.py", "commit_date": "2020-10-09 00:00:00", "repo_name": "Zulko/moviepy", "stars": 11574, "license": "mit", "language": "python", "size": 41611}
{"docstring": "\"\"\"Apply a transformation to a part of the clip.\n\nReturns a new clip in which the function ``fun`` (clip->clip)\nhas been applied to the subclip between times `start_time` and `end_time`\n(in seconds).\n\nExamples\n---------\n\n>>> # The scene between times t=3s and t=6s in ``clip`` will be\n>>> # be played twice slower in ``new_clip``\n>>> new_clip = clip.subapply(lambda c:c.speedx(0.5) , 3,6)\n\n\"\"\"\n", "func_signal": "def subfx(self, fx, start_time=0, end_time=None, **kwargs):\n", "code": "left = None if (start_time == 0) else self.subclip(0, start_time)\ncenter = self.subclip(start_time, end_time).fx(fx, **kwargs)\nright = None if (end_time is None) else self.subclip(start_time=end_time)\n\nclips = [clip for clip in [left, center, right] if clip is not None]\n\n# beurk, have to find other solution\nfrom moviepy.video.compositing.concatenate import concatenate_videoclips\n\nreturn concatenate_videoclips(clips).with_start(self.start)", "path": "moviepy/moviepy/video/VideoClip.py", "commit_date": "2020-10-09 00:00:00", "repo_name": "Zulko/moviepy", "stars": 11574, "license": "mit", "language": "python", "size": 41611}
{"docstring": "\"\"\" Function to handle difficult examples\nUpdate on each object \"\"\"\n", "func_signal": "def btnstate(self, item= None):\n", "code": "if not self.canvas.editing():\n    return\n\nitem = self.currentItem()\nif not item: # If not selected Item, take the first one\n    item = self.labelList.item(self.labelList.count()-1)\n\ndifficult = self.diffcButton.isChecked()\n\ntry:\n    shape = self.itemsToShapes[item]\nexcept:\n    pass\n# Checked and Update\ntry:\n    if difficult != shape.difficult:\n        shape.difficult = difficult\n        self.setDirty()\n    else:  # User probably changed item visibility\n        self.canvas.setShapeVisible(shape, item.checkState() == Qt.Checked)\nexcept:\n    pass", "path": "labelImg/labelImg.py", "commit_date": "2020-11-15 00:00:00", "repo_name": "HumanSignal/labelImg", "stars": 21524, "license": "mit", "language": "python", "size": 242833}
{"docstring": "\"\"\"\nMoves a point x,y to within the boundaries of the canvas.\n:return: (x,y,snapped) where snapped is True if x or y were changed, False if not.\n\"\"\"\n", "func_signal": "def snapPointToCanvas(self, x, y):\n", "code": "if x < 0 or x > self.pixmap.width() or y < 0 or y > self.pixmap.height():\n    x = max(x, 0)\n    y = max(y, 0)\n    x = min(x, self.pixmap.width())\n    y = min(y, self.pixmap.height())\n    return x, y, True\n\nreturn x, y, False", "path": "labelImg/libs/canvas.py", "commit_date": "2020-09-12 00:00:00", "repo_name": "HumanSignal/labelImg", "stars": 21524, "license": "mit", "language": "python", "size": 242833}
{"docstring": "'''construct main app and run it'''\n", "func_signal": "def main():\n", "code": "app, _win = get_main_app(sys.argv)\nreturn app.exec_()", "path": "labelImg/labelImg.py", "commit_date": "2020-11-15 00:00:00", "repo_name": "HumanSignal/labelImg", "stars": 21524, "license": "mit", "language": "python", "size": 242833}
{"docstring": "\"\"\"\nStandard boilerplate Qt application code.\nDo everything but app.exec_() -- so that we can test the application in one thread\n\"\"\"\n", "func_signal": "def get_main_app(argv=[]):\n", "code": "app = QApplication(argv)\napp.setApplicationName(__appname__)\napp.setWindowIcon(newIcon(\"app\"))\n# Tzutalin 201705+: Accept extra agruments to change predefined class file\nargparser = argparse.ArgumentParser()\nargparser.add_argument(\"image_dir\", nargs=\"?\")\nargparser.add_argument(\"predefined_classes_file\",\n                       default=os.path.join(os.path.dirname(__file__), \"data\", \"predefined_classes.txt\"),\n                       nargs=\"?\")\nargparser.add_argument(\"save_dir\", nargs=\"?\")\nargs = argparser.parse_args(argv[1:])\n# Usage : labelImg.py image predefClassFile saveDir\nwin = MainWindow(args.image_dir,\n                 args.predefined_classes_file,\n                 args.save_dir)\nwin.show()\nreturn app, win", "path": "labelImg/labelImg.py", "commit_date": "2020-11-15 00:00:00", "repo_name": "HumanSignal/labelImg", "stars": 21524, "license": "mit", "language": "python", "size": 242833}
{"docstring": "# Get the unique labels and add them to the Combobox.\n", "func_signal": "def updateComboBox(self):\n", "code": "itemsTextList = [str(self.labelList.item(i).text()) for i in range(self.labelList.count())]\n\nuniqueTextList = list(set(itemsTextList))\n# Add a null row for showing all the labels\nuniqueTextList.append(\"\")\nuniqueTextList.sort()\n\nself.comboBox.update_items(uniqueTextList)", "path": "labelImg/labelImg.py", "commit_date": "2020-11-15 00:00:00", "repo_name": "HumanSignal/labelImg", "stars": 21524, "license": "mit", "language": "python", "size": 242833}
{"docstring": "\"\"\"Select the first shape created which contains this point.\"\"\"\n", "func_signal": "def selectShapePoint(self, point):\n", "code": "self.deSelectShape()\nif self.selectedVertex():  # A vertex is marked for selection.\n    index, shape = self.hVertex, self.hShape\n    shape.highlightVertex(index, shape.MOVE_VERTEX)\n    self.selectShape(shape)\n    return self.hVertex\nfor shape in reversed(self.shapes):\n    if self.isVisible(shape) and shape.containsPoint(point):\n        self.selectShape(shape)\n        self.calculateOffsets(shape, point)\n        return self.selectedShape\nreturn None", "path": "labelImg/libs/canvas.py", "commit_date": "2020-09-12 00:00:00", "repo_name": "HumanSignal/labelImg", "stars": 21524, "license": "mit", "language": "python", "size": 242833}
{"docstring": "# shapes type:\n# [labbel, [(x1,y1), (x2,y2), (x3,y3), (x4,y4)], color, color, difficult]\n", "func_signal": "def __init__(self, filepath, image, classListPath=None):\n", "code": "self.shapes = []\nself.filepath = filepath\n\nif classListPath is None:\n    dir_path = os.path.dirname(os.path.realpath(self.filepath))\n    self.classListPath = os.path.join(dir_path, \"classes.txt\")\nelse:\n    self.classListPath = classListPath\n\n# print (filepath, self.classListPath)\n\nclassesFile = open(self.classListPath, 'r')\nself.classes = classesFile.read().strip('\\n').split('\\n')\n\n# print (self.classes)\n\nimgSize = [image.height(), image.width(),\n              1 if image.isGrayscale() else 3]\n\nself.imgSize = imgSize\n\nself.verified = False\n# try:\nself.parseYoloFormat()\n# except:\n    # pass", "path": "labelImg/libs/yolo_io.py", "commit_date": "2020-07-02 00:00:00", "repo_name": "HumanSignal/labelImg", "stars": 21524, "license": "mit", "language": "python", "size": 242833}
{"docstring": "# We need at least 4 points here, since the mousePress handler\n# adds an extra one before this handler is called.\n", "func_signal": "def mouseDoubleClickEvent(self, ev):\n", "code": "if self.canCloseShape() and len(self.current) > 3:\n    self.current.popPoint()\n    self.finalise()", "path": "labelImg/libs/canvas.py", "commit_date": "2020-09-12 00:00:00", "repo_name": "HumanSignal/labelImg", "stars": 21524, "license": "mit", "language": "python", "size": 242833}
{"docstring": "# Try to move in one direction, and if it fails in another.\n# Give up if both fail.\n", "func_signal": "def boundedShiftShape(self, shape):\n", "code": "point = shape[0]\noffset = QPointF(2.0, 2.0)\nself.calculateOffsets(shape, point)\nself.prevPoint = point\nif not self.boundedMoveShape(shape, point - offset):\n    self.boundedMoveShape(shape, point + offset)", "path": "labelImg/libs/canvas.py", "commit_date": "2020-09-12 00:00:00", "repo_name": "HumanSignal/labelImg", "stars": 21524, "license": "mit", "language": "python", "size": 242833}
{"docstring": "\"\"\"Figure out the size of the pixmap in order to fit the main widget.\"\"\"\n", "func_signal": "def scaleFitWindow(self):\n", "code": "e = 2.0  # So that no scrollbars are generated.\nw1 = self.centralWidget().width() - e\nh1 = self.centralWidget().height() - e\na1 = w1 / h1\n# Calculate a new scale value based on the pixmap's aspect ratio.\nw2 = self.canvas.pixmap.width() - 0.0\nh2 = self.canvas.pixmap.height() - 0.0\na2 = w2 / h2\nreturn w1 / w2 if a2 >= a1 else h1 / h2", "path": "labelImg/labelImg.py", "commit_date": "2020-11-15 00:00:00", "repo_name": "HumanSignal/labelImg", "stars": 21524, "license": "mit", "language": "python", "size": 242833}
{"docstring": "\"\"\"Update line with last point and current coordinates.\"\"\"\n", "func_signal": "def mouseMoveEvent(self, ev):\n", "code": "pos = self.transformPos(ev.pos())\n\n# Update coordinates in status bar if image is opened\nwindow = self.parent().window()\nif window.filePath is not None:\n    self.parent().window().labelCoordinates.setText(\n        'X: %d; Y: %d' % (pos.x(), pos.y()))\n\n# Polygon drawing.\nif self.drawing():\n    self.overrideCursor(CURSOR_DRAW)\n    if self.current:\n        # Display annotation width and height while drawing\n        currentWidth = abs(self.current[0].x() - pos.x())\n        currentHeight = abs(self.current[0].y() - pos.y())\n        self.parent().window().labelCoordinates.setText(\n                'Width: %d, Height: %d / X: %d; Y: %d' % (currentWidth, currentHeight, pos.x(), pos.y()))\n\n        color = self.drawingLineColor\n        if self.outOfPixmap(pos):\n            # Don't allow the user to draw outside the pixmap.\n            # Clip the coordinates to 0 or max,\n            # if they are outside the range [0, max]\n            size = self.pixmap.size()\n            clipped_x = min(max(0, pos.x()), size.width())\n            clipped_y = min(max(0, pos.y()), size.height())\n            pos = QPointF(clipped_x, clipped_y)\n        elif len(self.current) > 1 and self.closeEnough(pos, self.current[0]):\n            # Attract line to starting point and colorise to alert the\n            # user:\n            pos = self.current[0]\n            color = self.current.line_color\n            self.overrideCursor(CURSOR_POINT)\n            self.current.highlightVertex(0, Shape.NEAR_VERTEX)\n\n        if self.drawSquare:\n            initPos = self.current[0]\n            minX = initPos.x()\n            minY = initPos.y()\n            min_size = min(abs(pos.x() - minX), abs(pos.y() - minY))\n            directionX = -1 if pos.x() - minX < 0 else 1\n            directionY = -1 if pos.y() - minY < 0 else 1\n            self.line[1] = QPointF(minX + directionX * min_size, minY + directionY * min_size)\n        else:\n            self.line[1] = pos\n\n        self.line.line_color = color\n        self.prevPoint = QPointF()\n        self.current.highlightClear()\n    else:\n        self.prevPoint = pos\n    self.repaint()\n    return\n\n# Polygon copy moving.\nif Qt.RightButton & ev.buttons():\n    if self.selectedShapeCopy and self.prevPoint:\n        self.overrideCursor(CURSOR_MOVE)\n        self.boundedMoveShape(self.selectedShapeCopy, pos)\n        self.repaint()\n    elif self.selectedShape:\n        self.selectedShapeCopy = self.selectedShape.copy()\n        self.repaint()\n    return\n\n# Polygon/Vertex moving.\nif Qt.LeftButton & ev.buttons():\n    if self.selectedVertex():\n        self.boundedMoveVertex(pos)\n        self.shapeMoved.emit()\n        self.repaint()\n    elif self.selectedShape and self.prevPoint:\n        self.overrideCursor(CURSOR_MOVE)\n        self.boundedMoveShape(self.selectedShape, pos)\n        self.shapeMoved.emit()\n        self.repaint()\n    else:\n        #pan\n        delta_x = pos.x() - self.pan_initial_pos.x()\n        delta_y = pos.y() - self.pan_initial_pos.y()\n        self.scrollRequest.emit(delta_x, Qt.Horizontal)\n        self.scrollRequest.emit(delta_y, Qt.Vertical)\n        self.update()\n    return\n\n# Just hovering over the canvas, 2 posibilities:\n# - Highlight shapes\n# - Highlight vertex\n# Update shape/vertex fill and tooltip value accordingly.\nself.setToolTip(\"Image\")\nfor shape in reversed([s for s in self.shapes if self.isVisible(s)]):\n    # Look for a nearby vertex to highlight. If that fails,\n    # check if we happen to be inside a shape.\n    index = shape.nearestVertex(pos, self.epsilon)\n    if index is not None:\n        if self.selectedVertex():\n            self.hShape.highlightClear()\n        self.hVertex, self.hShape = index, shape\n        shape.highlightVertex(index, shape.MOVE_VERTEX)\n        self.overrideCursor(CURSOR_POINT)\n        self.setToolTip(\"Click & drag to move point\")\n        self.setStatusTip(self.toolTip())\n        self.update()\n        break\n    elif shape.containsPoint(pos):\n        if self.selectedVertex():\n            self.hShape.highlightClear()\n        self.hVertex, self.hShape = None, shape\n        self.setToolTip(\n            \"Click & drag to move shape '%s'\" % shape.label)\n        self.setStatusTip(self.toolTip())\n        self.overrideCursor(CURSOR_GRAB)\n        self.update()\n        break\nelse:  # Nothing found, clear highlights, reset state.\n    if self.hShape:\n        self.hShape.highlightClear()\n        self.update()\n    self.hVertex, self.hShape = None, None\n    self.overrideCursor(CURSOR_DEFAULT)", "path": "labelImg/libs/canvas.py", "commit_date": "2020-09-12 00:00:00", "repo_name": "HumanSignal/labelImg", "stars": 21524, "license": "mit", "language": "python", "size": 242833}
{"docstring": "\"\"\"Enable/Disable widgets which depend on an opened image.\"\"\"\n", "func_signal": "def toggleActions(self, value=True):\n", "code": "for z in self.actions.zoomActions:\n    z.setEnabled(value)\nfor action in self.actions.onLoadActive:\n    action.setEnabled(value)", "path": "labelImg/labelImg.py", "commit_date": "2020-11-15 00:00:00", "repo_name": "HumanSignal/labelImg", "stars": 21524, "license": "mit", "language": "python", "size": 242833}
{"docstring": "\"\"\"Load the specified file, or the last opened file if None.\"\"\"\n", "func_signal": "def loadFile(self, filePath=None):\n", "code": "self.resetState()\nself.canvas.setEnabled(False)\nif filePath is None:\n    filePath = self.settings.get(SETTING_FILENAME)\n\n# Make sure that filePath is a regular python string, rather than QString\nfilePath = ustr(filePath)\n\n# Fix bug: An  index error after select a directory when open a new file.\nunicodeFilePath = ustr(filePath)\nunicodeFilePath = os.path.abspath(unicodeFilePath)\n# Tzutalin 20160906 : Add file list and dock to move faster\n# Highlight the file item\nif unicodeFilePath and self.fileListWidget.count() > 0:\n    if unicodeFilePath in self.mImgList:\n        index = self.mImgList.index(unicodeFilePath)\n        fileWidgetItem = self.fileListWidget.item(index)\n        fileWidgetItem.setSelected(True)\n    else:\n        self.fileListWidget.clear()\n        self.mImgList.clear()\n\nif unicodeFilePath and os.path.exists(unicodeFilePath):\n    if LabelFile.isLabelFile(unicodeFilePath):\n        try:\n            self.labelFile = LabelFile(unicodeFilePath)\n        except LabelFileError as e:\n            self.errorMessage(u'Error opening file',\n                              (u\"<p><b>%s</b></p>\"\n                               u\"<p>Make sure <i>%s</i> is a valid label file.\")\n                              % (e, unicodeFilePath))\n            self.status(\"Error reading %s\" % unicodeFilePath)\n            return False\n        self.imageData = self.labelFile.imageData\n        self.lineColor = QColor(*self.labelFile.lineColor)\n        self.fillColor = QColor(*self.labelFile.fillColor)\n        self.canvas.verified = self.labelFile.verified\n    else:\n        # Load image:\n        # read data first and store for saving into label file.\n        self.imageData = read(unicodeFilePath, None)\n        self.labelFile = None\n        self.canvas.verified = False\n\n    if isinstance(self.imageData, QImage):\n        image = self.imageData\n    else:\n        image = QImage.fromData(self.imageData)\n    if image.isNull():\n        self.errorMessage(u'Error opening file',\n                          u\"<p>Make sure <i>%s</i> is a valid image file.\" % unicodeFilePath)\n        self.status(\"Error reading %s\" % unicodeFilePath)\n        return False\n    self.status(\"Loaded %s\" % os.path.basename(unicodeFilePath))\n    self.image = image\n    self.filePath = unicodeFilePath\n    self.canvas.loadPixmap(QPixmap.fromImage(image))\n    if self.labelFile:\n        self.loadLabels(self.labelFile.shapes)\n    self.setClean()\n    self.canvas.setEnabled(True)\n    self.adjustScale(initial=True)\n    self.paintCanvas()\n    self.addRecentFile(self.filePath)\n    self.toggleActions(True)\n    self.showBoundingBoxFromAnnotationFile(filePath)\n\n    self.setWindowTitle(__appname__ + ' ' + filePath)\n\n    # Default : select last item if there is at least one item\n    if self.labelList.count():\n        self.labelList.setCurrentItem(self.labelList.item(self.labelList.count()-1))\n        self.labelList.item(self.labelList.count()-1).setSelected(True)\n\n    self.canvas.setFocus(True)\n    return True\nreturn False", "path": "labelImg/labelImg.py", "commit_date": "2020-11-15 00:00:00", "repo_name": "HumanSignal/labelImg", "stars": 21524, "license": "mit", "language": "python", "size": 242833}
{"docstring": "# Proceding prev image without dialog if having any label\n", "func_signal": "def openPrevImg(self, _value=False):\n", "code": "if self.autoSaving.isChecked():\n    if self.defaultSaveDir is not None:\n        if self.dirty is True:\n            self.saveFile()\n    else:\n        self.changeSavedirDialog()\n        return\n\nif not self.mayContinue():\n    return\n\nif len(self.mImgList) <= 0:\n    return\n\nif self.filePath is None:\n    return\n\ncurrIndex = self.mImgList.index(self.filePath)\nif currIndex - 1 >= 0:\n    filename = self.mImgList[currIndex - 1]\n    if filename:\n        self.loadFile(filename)", "path": "labelImg/labelImg.py", "commit_date": "2020-11-15 00:00:00", "repo_name": "HumanSignal/labelImg", "stars": 21524, "license": "mit", "language": "python", "size": 242833}
{"docstring": "# get the current scrollbar positions\n# calculate the percentages ~ coordinates\n", "func_signal": "def zoomRequest(self, delta):\n", "code": "h_bar = self.scrollBars[Qt.Horizontal]\nv_bar = self.scrollBars[Qt.Vertical]\n\n# get the current maximum, to know the difference after zooming\nh_bar_max = h_bar.maximum()\nv_bar_max = v_bar.maximum()\n\n# get the cursor position and canvas size\n# calculate the desired movement from 0 to 1\n# where 0 = move left\n#       1 = move right\n# up and down analogous\ncursor = QCursor()\npos = cursor.pos()\nrelative_pos = QWidget.mapFromGlobal(self, pos)\n\ncursor_x = relative_pos.x()\ncursor_y = relative_pos.y()\n\nw = self.scrollArea.width()\nh = self.scrollArea.height()\n\n# the scaling from 0 to 1 has some padding\n# you don't have to hit the very leftmost pixel for a maximum-left movement\nmargin = 0.1\nmove_x = (cursor_x - margin * w) / (w - 2 * margin * w)\nmove_y = (cursor_y - margin * h) / (h - 2 * margin * h)\n\n# clamp the values from 0 to 1\nmove_x = min(max(move_x, 0), 1)\nmove_y = min(max(move_y, 0), 1)\n\n# zoom in\nunits = delta / (8 * 15)\nscale = 10\nself.addZoom(scale * units)\n\n# get the difference in scrollbar values\n# this is how far we can move\nd_h_bar_max = h_bar.maximum() - h_bar_max\nd_v_bar_max = v_bar.maximum() - v_bar_max\n\n# get the new scrollbar values\nnew_h_bar_value = h_bar.value() + move_x * d_h_bar_max\nnew_v_bar_value = v_bar.value() + move_y * d_v_bar_max\n\nh_bar.setValue(new_h_bar_value)\nv_bar.setValue(new_v_bar_value)", "path": "labelImg/labelImg.py", "commit_date": "2020-11-15 00:00:00", "repo_name": "HumanSignal/labelImg", "stars": 21524, "license": "mit", "language": "python", "size": 242833}
{"docstring": "\"\"\"\nreturns a tuple containing (title, icon_name) of the selected format\n\"\"\"\n", "func_signal": "def getFormatMeta(format):\n", "code": "if format == LabelFileFormat.PASCAL_VOC:\n    return ('&PascalVOC', 'format_voc')\nelif format == LabelFileFormat.YOLO:\n    return ('&YOLO', 'format_yolo')\nelif format == LabelFileFormat.CREATE_ML:\n    return ('&CreateML', 'format_createml')", "path": "labelImg/labelImg.py", "commit_date": "2020-11-15 00:00:00", "repo_name": "HumanSignal/labelImg", "stars": 21524, "license": "mit", "language": "python", "size": 242833}
{"docstring": "# The epsilon does not seem to work too well here.\n", "func_signal": "def scaleFitWidth(self):\n", "code": "w = self.centralWidget().width() - 2.0\nreturn w / self.canvas.pixmap.width()", "path": "labelImg/labelImg.py", "commit_date": "2020-11-15 00:00:00", "repo_name": "HumanSignal/labelImg", "stars": 21524, "license": "mit", "language": "python", "size": 242833}
{"docstring": "# print(self.selectedShape.points)\n", "func_signal": "def moveOnePixel(self, direction):\n", "code": "if direction == 'Left' and not self.moveOutOfBound(QPointF(-1.0, 0)):\n    # print(\"move Left one pixel\")\n    self.selectedShape.points[0] += QPointF(-1.0, 0)\n    self.selectedShape.points[1] += QPointF(-1.0, 0)\n    self.selectedShape.points[2] += QPointF(-1.0, 0)\n    self.selectedShape.points[3] += QPointF(-1.0, 0)\nelif direction == 'Right' and not self.moveOutOfBound(QPointF(1.0, 0)):\n    # print(\"move Right one pixel\")\n    self.selectedShape.points[0] += QPointF(1.0, 0)\n    self.selectedShape.points[1] += QPointF(1.0, 0)\n    self.selectedShape.points[2] += QPointF(1.0, 0)\n    self.selectedShape.points[3] += QPointF(1.0, 0)\nelif direction == 'Up' and not self.moveOutOfBound(QPointF(0, -1.0)):\n    # print(\"move Up one pixel\")\n    self.selectedShape.points[0] += QPointF(0, -1.0)\n    self.selectedShape.points[1] += QPointF(0, -1.0)\n    self.selectedShape.points[2] += QPointF(0, -1.0)\n    self.selectedShape.points[3] += QPointF(0, -1.0)\nelif direction == 'Down' and not self.moveOutOfBound(QPointF(0, 1.0)):\n    # print(\"move Down one pixel\")\n    self.selectedShape.points[0] += QPointF(0, 1.0)\n    self.selectedShape.points[1] += QPointF(0, 1.0)\n    self.selectedShape.points[2] += QPointF(0, 1.0)\n    self.selectedShape.points[3] += QPointF(0, 1.0)\nself.shapeMoved.emit()\nself.repaint()", "path": "labelImg/libs/canvas.py", "commit_date": "2020-09-12 00:00:00", "repo_name": "HumanSignal/labelImg", "stars": 21524, "license": "mit", "language": "python", "size": 242833}
{"docstring": "# Proceding next image without dialog if having any label\n", "func_signal": "def verifyImg(self, _value=False):\n", "code": "if self.filePath is not None:\n    try:\n        self.labelFile.toggleVerify()\n    except AttributeError:\n        # If the labelling file does not exist yet, create if and\n        # re-save it with the verified attribute.\n        self.saveFile()\n        if self.labelFile != None:\n            self.labelFile.toggleVerify()\n        else:\n            return\n\n    self.canvas.verified = self.labelFile.verified\n    self.paintCanvas()\n    self.saveFile()", "path": "labelImg/labelImg.py", "commit_date": "2020-11-15 00:00:00", "repo_name": "HumanSignal/labelImg", "stars": 21524, "license": "mit", "language": "python", "size": 242833}
{"docstring": "# Proceding prev image without dialog if having any label\n", "func_signal": "def openNextImg(self, _value=False):\n", "code": "if self.autoSaving.isChecked():\n    if self.defaultSaveDir is not None:\n        if self.dirty is True:\n            self.saveFile()\n    else:\n        self.changeSavedirDialog()\n        return\n\nif not self.mayContinue():\n    return\n\nif len(self.mImgList) <= 0:\n    return\n\nfilename = None\nif self.filePath is None:\n    filename = self.mImgList[0]\nelse:\n    currIndex = self.mImgList.index(self.filePath)\n    if currIndex + 1 < len(self.mImgList):\n        filename = self.mImgList[currIndex + 1]\n\nif filename:\n    self.loadFile(filename)", "path": "labelImg/labelImg.py", "commit_date": "2020-11-15 00:00:00", "repo_name": "HumanSignal/labelImg", "stars": 21524, "license": "mit", "language": "python", "size": 242833}
{"docstring": "\"\"\"\n:type head: ListNode\n:rtype: ListNode\n\"\"\"\n", "func_signal": "def reverse_list_recursive(head):\n", "code": "if head is None or head.next is None:\n    return head\np = head.next\nhead.next = None\nrevrest = reverse_list_recursive(p)\np.next = head\nreturn revrest", "path": "algorithms/algorithms/linkedlist/reverse.py", "commit_date": "2018-05-29 00:00:00", "repo_name": "keon/algorithms", "stars": 23420, "license": "mit", "language": "python", "size": 1468}
{"docstring": "\"\"\"\n:type input: str\n:rtype: int\n\"\"\"\n", "func_signal": "def length_longest_path(input):\n", "code": "curr_len, max_len = 0, 0    # running length and max length\nstack = []    # keep track of the name length\nfor s in input.split('\\n'):\n    print(\"---------\")\n    print(\"<path>:\", s)\n    depth = s.count('\\t')    # the depth of current dir or file\n    print(\"depth: \", depth)\n    print(\"stack: \", stack)\n    print(\"curlen: \", curr_len)\n    while len(stack) > depth:    # go back to the correct depth\n        curr_len -= stack.pop()\n    stack.append(len(s.strip('\\t'))+1)   # 1 is the length of '/'\n    curr_len += stack[-1]    # increase current length\n    print(\"stack: \", stack)\n    print(\"curlen: \", curr_len)\n    if '.' in s:    # update maxlen only when it is a file\n        max_len = max(max_len, curr_len-1)    # -1 is to minus one '/'\nreturn max_len", "path": "algorithms/algorithms/stack/longest_abs_path.py", "commit_date": "2018-06-08 00:00:00", "repo_name": "keon/algorithms", "stars": 23420, "license": "mit", "language": "python", "size": 1468}
{"docstring": "\"\"\"\n     Updates a node in Binary Index Tree (bit_tree) at given index in bit_tree. The given value 'val' is added to bit_tree[i] and all of its ancestors in tree. \n\"\"\"\n      \n# index in bit_ree[] is 1 more than the index in arr[] \n", "func_signal": "def update_bit(self, bit_tree, i, v):\n", "code": "i += 1\n      \n# Traverse all ancestors and add 'val' \nwhile i <= self.n: \n      \n    # Add 'val' to current node of bit_tree \n    bit_tree[i] += v \n      \n    # Update index to that of parent in update View \n    i += i & (-i)", "path": "algorithms/algorithms/tree/fenwick_tree/fenwick_tree.py", "commit_date": "2020-11-06 00:00:00", "repo_name": "keon/algorithms", "stars": 23420, "license": "mit", "language": "python", "size": 1468}
{"docstring": "\"\"\"\nn: int\nnums: list[object]\ntarget: object\nsum_closure: function, optional\n    Given two elements of nums, return sum of both.\ncompare_closure: function, optional\n    Given one object of nums and target, return -1, 1, or 0.\nsame_closure: function, optional\n    Given two object of nums, return bool.\nreturn: list[list[object]]\n\nNote:\n1. type of sum_closure's return should be same \n   as type of compare_closure's first param\n\"\"\"\n\n", "func_signal": "def n_sum(n, nums, target, **kv):\n", "code": "def sum_closure_default(a, b):\n    return a + b\n\ndef compare_closure_default(num, target):\n    \"\"\" above, below, or right on? \"\"\"\n    if num < target:\n        return -1\n    elif num > target:\n        return 1\n    else:\n        return 0\n\ndef same_closure_default(a, b):\n    return a == b\n\ndef n_sum(n, nums, target):\n    if n == 2:      # want answers with only 2 terms? easy!\n        results = two_sum(nums, target)\n    else:\n        results = []\n        prev_num = None\n        for index, num in enumerate(nums):\n            if prev_num is not None and \\\n               same_closure(prev_num, num):\n                continue\n\n            prev_num = num\n            n_minus1_results = (\n                n_sum(                      # recursive call\n                    n - 1,                  # a\n                    nums[index + 1:],       # b\n                    target - num            # c\n                    )   # x = n_sum( a, b, c )\n                )   # n_minus1_results = x\n\n            n_minus1_results = (\n                append_elem_to_each_list(num, n_minus1_results)\n                )\n            results += n_minus1_results\n    return union(results)\n\ndef two_sum(nums, target):\n    nums.sort()\n    lt = 0\n    rt = len(nums) - 1\n    results = []\n    while lt < rt:\n        sum_ = sum_closure(nums[lt], nums[rt])\n        flag = compare_closure(sum_, target)\n        if flag == -1:\n            lt += 1\n        elif flag == 1:\n            rt -= 1\n        else:\n            results.append(sorted([nums[lt], nums[rt]]))\n            lt += 1\n            rt -= 1\n            while (lt < len(nums) and\n                   same_closure(nums[lt - 1], nums[lt])):\n                lt += 1\n            while (0 <= rt and\n                   same_closure(nums[rt], nums[rt + 1])):\n                rt -= 1\n    return results\n\ndef append_elem_to_each_list(elem, container):\n    results = []\n    for elems in container:\n        elems.append(elem)\n        results.append(sorted(elems))\n    return results\n\ndef union(duplicate_results):\n    results = []\n\n    if len(duplicate_results) != 0:\n        duplicate_results.sort()\n        results.append(duplicate_results[0])\n        for result in duplicate_results[1:]:\n            if results[-1] != result:\n                results.append(result)\n\n    return results\n\nsum_closure = kv.get('sum_closure', sum_closure_default)\nsame_closure = kv.get('same_closure', same_closure_default)\ncompare_closure = kv.get('compare_closure', compare_closure_default)\nnums.sort()\nreturn n_sum(n, nums, target)", "path": "algorithms/algorithms/arrays/n_sum.py", "commit_date": "2018-10-08 00:00:00", "repo_name": "keon/algorithms", "stars": 23420, "license": "mit", "language": "python", "size": 1468}
{"docstring": "\"\"\"\n:type s: str\n:type t: str\n:rtype: bool\n\"\"\"\n", "func_signal": "def is_anagram(s, t):\n", "code": "maps = {}\nmapt = {}\nfor i in s:\n    maps[i] = maps.get(i, 0) + 1\nfor i in t:\n    mapt[i] = mapt.get(i, 0) + 1\nreturn maps == mapt", "path": "algorithms/algorithms/map/is_anagram.py", "commit_date": "2018-08-14 00:00:00", "repo_name": "keon/algorithms", "stars": 23420, "license": "mit", "language": "python", "size": 1468}
{"docstring": "\"\"\"\n:type head: ListNode\n:rtype: ListNode\n\"\"\"\n", "func_signal": "def reverse_list(head):\n", "code": "if not head or not head.next:\n    return head\nprev = None\nwhile head:\n    current = head\n    head = head.next\n    current.next = prev\n    prev = current\nreturn prev", "path": "algorithms/algorithms/linkedlist/reverse.py", "commit_date": "2018-05-29 00:00:00", "repo_name": "keon/algorithms", "stars": 23420, "license": "mit", "language": "python", "size": 1468}
{"docstring": "\"\"\"\n:rtype: int\n\"\"\"\n", "func_signal": "def next(self):\n", "code": "v=self.queue.pop(0)\nret=v.pop(0)\nif v: self.queue.append(v)\nreturn ret", "path": "algorithms/algorithms/queues/zigzagiterator.py", "commit_date": "2018-05-29 00:00:00", "repo_name": "keon/algorithms", "stars": 23420, "license": "mit", "language": "python", "size": 1468}
{"docstring": "''' Function that performs DFS '''\n\n", "func_signal": "def dfs(source,visited,l):\n", "code": "visited[source] = True\nfor child in l[source]:\n    if not visited[child]:\n        dfs(child,visited,l)", "path": "algorithms/algorithms/graph/count_connected_number_of_component.py", "commit_date": "2020-03-07 00:00:00", "repo_name": "keon/algorithms", "stars": 23420, "license": "mit", "language": "python", "size": 1468}
{"docstring": "\"\"\"Calculates the binomial coefficient, C(n,k), with n>=k using recursion\nTime complexity is O(k), so can calculate fairly quickly for large values of k.\n\n>>> recursive_binomial_coefficient(5,0)\n1\n\n>>> recursive_binomial_coefficient(8,2)\n28\n\n>>> recursive_binomial_coefficient(500,300)\n5054949849935535817667719165973249533761635252733275327088189563256013971725761702359997954491403585396607971745777019273390505201262259748208640\n\n\"\"\"\n\n", "func_signal": "def recursive_binomial_coefficient(n,k):\n", "code": "if k>n:\n    raise ValueError('Invalid Inputs, ensure that n >= k')\n    #function is only defined for n>=k\nif k == 0 or n == k:\n    #C(n,0) = C(n,n) = 1, so this is our base case.\n    return 1\nif k > n/2:\n    #C(n,k) = C(n,n-k), so if n/2 is sufficiently small, we can reduce the problem size.\n    return recursive_binomial_coefficient(n,n-k)\nelse:\n    #else, we know C(n,k) = (n/k)C(n-1,k-1), so we can use this to reduce our problem size.\n    return int((n/k)*recursive_binomial_coefficient(n-1,k-1))", "path": "algorithms/algorithms/maths/recursive_binomial_coefficient.py", "commit_date": "2019-12-11 00:00:00", "repo_name": "keon/algorithms", "stars": 23420, "license": "mit", "language": "python", "size": 1468}
{"docstring": "# AND-function\n", "func_signal": "def test_nearest_neighbor(self):\n", "code": "self.assertEqual(nearest_neighbor((1,1), self.trainSetAND), 1)\nself.assertEqual(nearest_neighbor((0,1), self.trainSetAND), 0)\n\n# dark/light color test\nself.assertEqual(nearest_neighbor((31, 242, 164), self.trainSetLight), 'L')\nself.assertEqual(nearest_neighbor((13, 94, 64), self.trainSetLight), 'D')\nself.assertEqual(nearest_neighbor((230, 52, 239), self.trainSetLight), 'L')", "path": "algorithms/tests/test_ml.py", "commit_date": "2018-07-24 00:00:00", "repo_name": "keon/algorithms", "stars": 23420, "license": "mit", "language": "python", "size": 1468}
{"docstring": "\"\"\"\n    iterator: returns a perumation by each call.\n\"\"\"\n", "func_signal": "def permute_iter(elements):\n", "code": "if len(elements) <= 1:\n    yield elements\nelse:\n    for perm in permute_iter(elements[1:]):\n        for i in range(len(elements)):\n            yield perm[:i] + elements[0:1] + perm[i:]", "path": "algorithms/algorithms/backtrack/permute.py", "commit_date": "2019-03-26 00:00:00", "repo_name": "keon/algorithms", "stars": 23420, "license": "mit", "language": "python", "size": 1468}
{"docstring": "\"\"\"\n     Constructs and returns a Binary Indexed Tree for given array of size n. \n\"\"\"\n      \n# Create and initialize bit_ree[] as 0 \n", "func_signal": "def construct(self):\n", "code": "bit_tree = [0]*(self.n+1) \n      \n# Store the actual values in bit_ree[] using update() \nfor i in range(self.n): \n    self.update_bit(bit_tree, i, self.arr[i]) \n\nreturn bit_tree", "path": "algorithms/algorithms/tree/fenwick_tree/fenwick_tree.py", "commit_date": "2020-11-06 00:00:00", "repo_name": "keon/algorithms", "stars": 23420, "license": "mit", "language": "python", "size": 1468}
{"docstring": "\"\"\"\n    returns a list with the permuations.\n\"\"\"\n", "func_signal": "def permute(elements):\n", "code": "if len(elements) <= 1:\n    return [elements]\nelse:\n    tmp = []\n    for perm in permute(elements[1:]):\n        for i in range(len(elements)):\n            tmp.append(perm[:i] + elements[0:1] + perm[i:])\n    return tmp", "path": "algorithms/algorithms/backtrack/permute.py", "commit_date": "2019-03-26 00:00:00", "repo_name": "keon/algorithms", "stars": 23420, "license": "mit", "language": "python", "size": 1468}
{"docstring": "# train set for the AND-function\n", "func_signal": "def setUp(self):\n", "code": "self.trainSetAND = {(0,0) : 0, (0,1) :0, (1,0) : 0, (1,1) : 1} \n\n# train set for light or dark colors\nself.trainSetLight = {(11, 98, 237) : 'L', (3, 39, 96) : 'D', (242, 226, 12) : 'L', (99, 93, 4) : 'D',\n(232, 62, 32) : 'L', (119, 28, 11) : 'D', (25, 214, 47) : 'L', (89, 136, 247) : 'L',\n(21, 34, 63) : 'D', (237, 99, 120) : 'L', (73, 33, 39) : 'D'}", "path": "algorithms/tests/test_ml.py", "commit_date": "2018-07-24 00:00:00", "repo_name": "keon/algorithms", "stars": 23420, "license": "mit", "language": "python", "size": 1468}
{"docstring": "\"\"\"\n:rtype: bool\n\"\"\"\n", "func_signal": "def has_next(self):\n", "code": "if self.queue: return True\nreturn False", "path": "algorithms/algorithms/queues/zigzagiterator.py", "commit_date": "2018-05-29 00:00:00", "repo_name": "keon/algorithms", "stars": 23420, "license": "mit", "language": "python", "size": 1468}
{"docstring": "\"\"\"\nInitialize your data structure here.\n:type v1: List[int]\n:type v2: List[int]\n\"\"\"\n", "func_signal": "def __init__(self, v1, v2):\n", "code": "self.queue=[_ for _ in (v1,v2) if _]\nprint(self.queue)", "path": "algorithms/algorithms/queues/zigzagiterator.py", "commit_date": "2018-05-29 00:00:00", "repo_name": "keon/algorithms", "stars": 23420, "license": "mit", "language": "python", "size": 1468}
{"docstring": "\"\"\"Return True if n is a prime number\nElse return False.\n\"\"\"\n\n", "func_signal": "def prime_check(n):\n", "code": "if n <= 1:\n    return False\nif n == 2 or n == 3:\n    return True\nif n % 2 == 0 or n % 3 == 0:\n    return False\nj = 5\nwhile j * j <= n:\n    if n % j == 0 or n % (j + 2) == 0:\n        return False\n    j += 6\nreturn True", "path": "algorithms/algorithms/maths/prime_check.py", "commit_date": "2018-06-21 00:00:00", "repo_name": "keon/algorithms", "stars": 23420, "license": "mit", "language": "python", "size": 1468}
{"docstring": "\"\"\"\n     Returns sum of arr[0..index]. This function assumes that the array is preprocessed and partial sums of array elements are stored in bit_tree[]. \n\"\"\"\n\n", "func_signal": "def get_sum(self, bit_tree, i):\n", "code": "s = 0\n      \n# index in bit_tree[] is 1 more than the index in arr[] \ni = i+1\n      \n# Traverse ancestors of bit_tree[index] \nwhile i > 0: \n      \n    # Add current element of bit_tree to sum \n    s += bit_tree[i] \n      \n    # Move index to parent node in getSum View \n    i -= i & (-i) \nreturn s", "path": "algorithms/algorithms/tree/fenwick_tree/fenwick_tree.py", "commit_date": "2020-11-06 00:00:00", "repo_name": "keon/algorithms", "stars": 23420, "license": "mit", "language": "python", "size": 1468}
{"docstring": "\"\"\" Insertion Sort\n    Complexity: O(n^2)\n\"\"\"\n\n", "func_signal": "def insertion_sort(arr, simulation=False):\n", "code": "iteration = 0\nif simulation:\n    print(\"iteration\",iteration,\":\",*arr)\n    \nfor i in range(len(arr)):\n    cursor = arr[i]\n    pos = i\n    \n    while pos > 0 and arr[pos - 1] > cursor:\n        # Swap the number down the list\n        arr[pos] = arr[pos - 1]\n        pos = pos - 1\n    # Break and do the final swap\n    arr[pos] = cursor\n    \n    if simulation:\n            iteration = iteration + 1\n            print(\"iteration\",iteration,\":\",*arr)\n\nreturn arr", "path": "algorithms/algorithms/sort/insertion_sort.py", "commit_date": "2018-06-08 00:00:00", "repo_name": "keon/algorithms", "stars": 23420, "license": "mit", "language": "python", "size": 1468}
{"docstring": "''' \nFunction that counts the Connected components on bases of DFS.\nreturn type : int\n'''\n\n", "func_signal": "def count_components(l,size):\n", "code": "count = 0\nvisited = [False]*(size+1)\nfor i in range(1,size+1):\n    if not visited[i]:\n        dfs(i,visited,l)\n        count+=1\nreturn count", "path": "algorithms/algorithms/graph/count_connected_number_of_component.py", "commit_date": "2020-03-07 00:00:00", "repo_name": "keon/algorithms", "stars": 23420, "license": "mit", "language": "python", "size": 1468}
{"docstring": "# type: (UnparsedVersion, Optional[bool]) -> bool\n\n# Determine if prereleases are to be allowed or not.\n", "func_signal": "def contains(self, item, prereleases=None):\n", "code": "if prereleases is None:\n    prereleases = self.prereleases\n\n# Normalize item to a Version or LegacyVersion, this allows us to have\n# a shortcut for ``\"2.0\" in Specifier(\">=2\")\nnormalized_item = self._coerce_version(item)\n\n# Determine if we should be supporting prereleases in this specifier\n# or not, if we do not support prereleases than we can short circuit\n# logic if this version is a prereleases.\nif normalized_item.is_prerelease and not prereleases:\n    return False\n\n# Actually do the comparison to determine if this item is contained\n# within this Specifier or not.\noperator_callable = self._get_operator(self.operator)  # type: CallableOperator\nreturn operator_callable(normalized_item, self.version)", "path": "pip/src/pip/_vendor/packaging/specifiers.py", "commit_date": "2020-11-28 00:00:00", "repo_name": "pypa/pip", "stars": 9178, "license": "mit", "language": "python", "size": 72277}
{"docstring": "# type: () -> bool\n\n# If there is an explicit prereleases set for this, then we'll just\n# blindly use that.\n", "func_signal": "def prereleases(self):\n", "code": "if self._prereleases is not None:\n    return self._prereleases\n\n# Look at all of our specifiers and determine if they are inclusive\n# operators, and if they are if they are including an explicit\n# prerelease.\noperator, version = self._spec\nif operator in [\"==\", \">=\", \"<=\", \"~=\", \"===\"]:\n    # The == specifier can include a trailing .*, if it does we\n    # want to remove before parsing.\n    if operator == \"==\" and version.endswith(\".*\"):\n        version = version[:-2]\n\n    # Parse the version, and if it is a pre-release than this\n    # specifier allows pre-releases.\n    if parse(version).is_prerelease:\n        return True\n\nreturn False", "path": "pip/src/pip/_vendor/packaging/specifiers.py", "commit_date": "2020-11-28 00:00:00", "repo_name": "pypa/pip", "stars": 9178, "license": "mit", "language": "python", "size": 72277}
{"docstring": "# type: (Union[ParsedVersion, str], Optional[bool]) -> bool\n\n# Ensure that our item is a Version or LegacyVersion instance.\n", "func_signal": "def contains(self, item, prereleases=None):\n", "code": "if not isinstance(item, (LegacyVersion, Version)):\n    item = parse(item)\n\n# Determine if we're forcing a prerelease or not, if we're not forcing\n# one for this particular filter call, then we'll use whatever the\n# SpecifierSet thinks for whether or not we should support prereleases.\nif prereleases is None:\n    prereleases = self.prereleases\n\n# We can determine if we're going to allow pre-releases by looking to\n# see if any of the underlying items supports them. If none of them do\n# and this item is a pre-release then we do not allow it and we can\n# short circuit that here.\n# Note: This means that 1.0.dev1 would not be contained in something\n#       like >=1.0.devabc however it would be in >=1.0.debabc,>0.0.dev0\nif not prereleases and item.is_prerelease:\n    return False\n\n# We simply dispatch to the underlying specs here to make sure that the\n# given version is contained within all of them.\n# Note: This use of all() here means that an empty set of specifiers\n#       will always return True, this is an explicit design decision.\nreturn all(s.contains(item, prereleases=prereleases) for s in self._specs)", "path": "pip/src/pip/_vendor/packaging/specifiers.py", "commit_date": "2020-11-28 00:00:00", "repo_name": "pypa/pip", "stars": 9178, "license": "mit", "language": "python", "size": 72277}
{"docstring": "# type: (ParsedVersion, str) -> bool\n\n# We need special logic to handle prefix matching\n", "func_signal": "def _compare_equal(self, prospective, spec):\n", "code": "if spec.endswith(\".*\"):\n    # In the case of prefix matching we want to ignore local segment.\n    prospective = Version(prospective.public)\n    # Split the spec out by dots, and pretend that there is an implicit\n    # dot in between a release segment and a pre-release segment.\n    split_spec = _version_split(spec[:-2])  # Remove the trailing .*\n\n    # Split the prospective version out by dots, and pretend that there\n    # is an implicit dot in between a release segment and a pre-release\n    # segment.\n    split_prospective = _version_split(str(prospective))\n\n    # Shorten the prospective version to be the same length as the spec\n    # so that we can determine if the specifier is a prefix of the\n    # prospective version or not.\n    shortened_prospective = split_prospective[: len(split_spec)]\n\n    # Pad out our two sides with zeros so that they both equal the same\n    # length.\n    padded_spec, padded_prospective = _pad_version(\n        split_spec, shortened_prospective\n    )\n\n    return padded_prospective == padded_spec\nelse:\n    # Convert our spec string into a Version\n    spec_version = Version(spec)\n\n    # If the specifier does not have a local segment, then we want to\n    # act as if the prospective version also does not have a local\n    # segment.\n    if not spec_version.local:\n        prospective = Version(prospective.public)\n\n    return prospective == spec_version", "path": "pip/src/pip/_vendor/packaging/specifiers.py", "commit_date": "2020-11-28 00:00:00", "repo_name": "pypa/pip", "stars": 9178, "license": "mit", "language": "python", "size": 72277}
{"docstring": "# type: (Iterable[UnparsedVersion], Optional[bool]) -> Iterable[UnparsedVersion]\n\n", "func_signal": "def filter(self, iterable, prereleases=None):\n", "code": "yielded = False\nfound_prereleases = []\n\nkw = {\"prereleases\": prereleases if prereleases is not None else True}\n\n# Attempt to iterate over all the values in the iterable and if any of\n# them match, yield them.\nfor version in iterable:\n    parsed_version = self._coerce_version(version)\n\n    if self.contains(parsed_version, **kw):\n        # If our version is a prerelease, and we were not set to allow\n        # prereleases, then we'll store it for later incase nothing\n        # else matches this specifier.\n        if parsed_version.is_prerelease and not (\n            prereleases or self.prereleases\n        ):\n            found_prereleases.append(version)\n        # Either this is not a prerelease, or we should have been\n        # accepting prereleases from the beginning.\n        else:\n            yielded = True\n            yield version\n\n# Now that we've iterated over everything, determine if we've yielded\n# any values, and if we have not and we have any prereleases stored up\n# then we will go ahead and yield the prereleases.\nif not yielded and found_prereleases:\n    for version in found_prereleases:\n        yield version", "path": "pip/src/pip/_vendor/packaging/specifiers.py", "commit_date": "2020-11-28 00:00:00", "repo_name": "pypa/pip", "stars": 9178, "license": "mit", "language": "python", "size": 72277}
{"docstring": "# type: (ParsedVersion, str) -> bool\n\n# Compatible releases have an equivalent combination of >= and ==. That\n# is that ~=2.2 is equivalent to >=2.2,==2.*. This allows us to\n# implement this in terms of the other specifiers instead of\n# implementing it ourselves. The only thing we need to do is construct\n# the other specifiers.\n\n# We want everything but the last item in the version, but we want to\n# ignore post and dev releases and we want to treat the pre-release as\n# it's own separate segment.\n", "func_signal": "def _compare_compatible(self, prospective, spec):\n", "code": "prefix = \".\".join(\n    list(\n        itertools.takewhile(\n            lambda x: (not x.startswith(\"post\") and not x.startswith(\"dev\")),\n            _version_split(spec),\n        )\n    )[:-1]\n)\n\n# Add the prefix notation to the end of our string\nprefix += \".*\"\n\nreturn self._get_operator(\">=\")(prospective, spec) and self._get_operator(\"==\")(\n    prospective, prefix\n)", "path": "pip/src/pip/_vendor/packaging/specifiers.py", "commit_date": "2020-11-28 00:00:00", "repo_name": "pypa/pip", "stars": 9178, "license": "mit", "language": "python", "size": 72277}
{"docstring": "# type: (UnparsedVersion) -> ParsedVersion\n", "func_signal": "def _coerce_version(self, version):\n", "code": "if not isinstance(version, (LegacyVersion, Version)):\n    version = parse(version)\nreturn version", "path": "pip/src/pip/_vendor/packaging/specifiers.py", "commit_date": "2020-11-28 00:00:00", "repo_name": "pypa/pip", "stars": 9178, "license": "mit", "language": "python", "size": 72277}
{"docstring": "# type: (str) -> List[str]\n", "func_signal": "def _version_split(version):\n", "code": "result = []  # type: List[str]\nfor item in version.split(\".\"):\n    match = _prefix_regex.search(item)\n    if match:\n        result.extend(match.groups())\n    else:\n        result.append(item)\nreturn result", "path": "pip/src/pip/_vendor/packaging/specifiers.py", "commit_date": "2020-11-28 00:00:00", "repo_name": "pypa/pip", "stars": 9178, "license": "mit", "language": "python", "size": 72277}
{"docstring": "# type: (ParsedVersion, str) -> bool\n\n# Convert our spec to a Version instance, since we'll want to work with\n# it as a version.\n", "func_signal": "def _compare_greater_than(self, prospective, spec_str):\n", "code": "spec = Version(spec_str)\n\n# Check to see if the prospective version is greater than the spec\n# version. If it's not we can short circuit and just return False now\n# instead of doing extra unneeded work.\nif not prospective > spec:\n    return False\n\n# This special case is here so that, unless the specifier itself\n# includes is a post-release version, that we do not accept\n# post-release versions for the version mentioned in the specifier\n# (e.g. >3.1 should not match 3.0.post0, but should match 3.2.post0).\nif not spec.is_postrelease and prospective.is_postrelease:\n    if Version(prospective.base_version) == Version(spec.base_version):\n        return False\n\n# Ensure that we do not allow a local version of the version mentioned\n# in the specifier, which is technically greater than, to match.\nif prospective.local is not None:\n    if Version(prospective.base_version) == Version(spec.base_version):\n        return False\n\n# If we've gotten to here, it means that prospective version is both\n# greater than the spec version *and* it's not a pre-release of the\n# same version in the spec.\nreturn True", "path": "pip/src/pip/_vendor/packaging/specifiers.py", "commit_date": "2020-11-28 00:00:00", "repo_name": "pypa/pip", "stars": 9178, "license": "mit", "language": "python", "size": 72277}
{"docstring": "# type: (str, Optional[bool]) -> None\n\n# Split on , to break each individual specifier into it's own item, and\n# strip each item to remove leading/trailing whitespace.\n", "func_signal": "def __init__(self, specifiers=\"\", prereleases=None):\n", "code": "split_specifiers = [s.strip() for s in specifiers.split(\",\") if s.strip()]\n\n# Parsed each individual specifier, attempting first to make it a\n# Specifier and falling back to a LegacySpecifier.\nparsed = set()\nfor specifier in split_specifiers:\n    try:\n        parsed.add(Specifier(specifier))\n    except InvalidSpecifier:\n        parsed.add(LegacySpecifier(specifier))\n\n# Turn our parsed specifiers into a frozen set and save them for later.\nself._specs = frozenset(parsed)\n\n# Store our prereleases value so we can use it later to determine if\n# we accept prereleases or not.\nself._prereleases = prereleases", "path": "pip/src/pip/_vendor/packaging/specifiers.py", "commit_date": "2020-11-28 00:00:00", "repo_name": "pypa/pip", "stars": 9178, "license": "mit", "language": "python", "size": 72277}
{"docstring": "# type: (object) -> bool\n", "func_signal": "def __ne__(self, other):\n", "code": "if isinstance(other, string_types):\n    try:\n        other = self.__class__(str(other))\n    except InvalidSpecifier:\n        return NotImplemented\nelif not isinstance(other, self.__class__):\n    return NotImplemented\n\nreturn self._spec != other._spec", "path": "pip/src/pip/_vendor/packaging/specifiers.py", "commit_date": "2020-11-28 00:00:00", "repo_name": "pypa/pip", "stars": 9178, "license": "mit", "language": "python", "size": 72277}
{"docstring": "# type: () -> str\n", "func_signal": "def __repr__(self):\n", "code": "pre = (\n    \", prereleases={0!r}\".format(self.prereleases)\n    if self._prereleases is not None\n    else \"\"\n)\n\nreturn \"<SpecifierSet({0!r}{1})>\".format(str(self), pre)", "path": "pip/src/pip/_vendor/packaging/specifiers.py", "commit_date": "2020-11-28 00:00:00", "repo_name": "pypa/pip", "stars": 9178, "license": "mit", "language": "python", "size": 72277}
{"docstring": "# type: (ParsedVersion, str) -> bool\n\n# Convert our spec to a Version instance, since we'll want to work with\n# it as a version.\n", "func_signal": "def _compare_less_than(self, prospective, spec_str):\n", "code": "spec = Version(spec_str)\n\n# Check to see if the prospective version is less than the spec\n# version. If it's not we can short circuit and just return False now\n# instead of doing extra unneeded work.\nif not prospective < spec:\n    return False\n\n# This special case is here so that, unless the specifier itself\n# includes is a pre-release version, that we do not accept pre-release\n# versions for the version mentioned in the specifier (e.g. <3.1 should\n# not match 3.1.dev0, but should match 3.0.dev0).\nif not spec.is_prerelease and prospective.is_prerelease:\n    if Version(prospective.base_version) == Version(spec.base_version):\n        return False\n\n# If we've gotten to here, it means that prospective version is both\n# less than the spec version *and* it's not a pre-release of the same\n# version in the spec.\nreturn True", "path": "pip/src/pip/_vendor/packaging/specifiers.py", "commit_date": "2020-11-28 00:00:00", "repo_name": "pypa/pip", "stars": 9178, "license": "mit", "language": "python", "size": 72277}
{"docstring": "\"\"\"\nIf a position is provided, move file to that point.\nOtherwise, we'll attempt to record a position for future use.\n\"\"\"\n", "func_signal": "def set_file_position(body, pos):\n", "code": "if pos is not None:\n    rewind_body(body, pos)\nelif getattr(body, \"tell\", None) is not None:\n    try:\n        pos = body.tell()\n    except (IOError, OSError):\n        # This differentiates from None, allowing us to catch\n        # a failed `tell()` later when trying to rewind the body.\n        pos = _FAILEDTELL\n\nreturn pos", "path": "pip/src/pip/_vendor/urllib3/util/request.py", "commit_date": "2020-11-19 00:00:00", "repo_name": "pypa/pip", "stars": 9178, "license": "mit", "language": "python", "size": 72277}
{"docstring": "# type: (str) -> CallableOperator\n", "func_signal": "def _get_operator(self, op):\n", "code": "operator_callable = getattr(\n    self, \"_compare_{0}\".format(self._operators[op])\n)  # type: CallableOperator\nreturn operator_callable", "path": "pip/src/pip/_vendor/packaging/specifiers.py", "commit_date": "2020-11-28 00:00:00", "repo_name": "pypa/pip", "stars": 9178, "license": "mit", "language": "python", "size": 72277}
{"docstring": "# type: (Union[ParsedVersion, str]) -> LegacyVersion\n", "func_signal": "def _coerce_version(self, version):\n", "code": "if not isinstance(version, LegacyVersion):\n    version = LegacyVersion(str(version))\nreturn version", "path": "pip/src/pip/_vendor/packaging/specifiers.py", "commit_date": "2020-11-28 00:00:00", "repo_name": "pypa/pip", "stars": 9178, "license": "mit", "language": "python", "size": 72277}
{"docstring": "# type: (object) -> bool\n", "func_signal": "def __eq__(self, other):\n", "code": "if isinstance(other, (string_types, _IndividualSpecifier)):\n    other = SpecifierSet(str(other))\nelif not isinstance(other, SpecifierSet):\n    return NotImplemented\n\nreturn self._specs == other._specs", "path": "pip/src/pip/_vendor/packaging/specifiers.py", "commit_date": "2020-11-28 00:00:00", "repo_name": "pypa/pip", "stars": 9178, "license": "mit", "language": "python", "size": 72277}
{"docstring": "\"\"\"\nAttempt to rewind body to a certain position.\nPrimarily used for request redirects and retries.\n\n:param body:\n    File-like object that supports seek.\n\n:param int pos:\n    Position to seek to in file.\n\"\"\"\n", "func_signal": "def rewind_body(body, body_pos):\n", "code": "body_seek = getattr(body, \"seek\", None)\nif body_seek is not None and isinstance(body_pos, integer_types):\n    try:\n        body_seek(body_pos)\n    except (IOError, OSError):\n        raise UnrewindableBodyError(\n            \"An error occurred when rewinding request body for redirect/retry.\"\n        )\nelif body_pos is _FAILEDTELL:\n    raise UnrewindableBodyError(\n        \"Unable to record file position for rewinding \"\n        \"request body during a redirect/retry.\"\n    )\nelse:\n    raise ValueError(\n        \"body_pos must be of type integer, instead it was %s.\" % type(body_pos)\n    )", "path": "pip/src/pip/_vendor/urllib3/util/request.py", "commit_date": "2020-11-19 00:00:00", "repo_name": "pypa/pip", "stars": 9178, "license": "mit", "language": "python", "size": 72277}
{"docstring": "# type: (List[str], List[str]) -> Tuple[List[str], List[str]]\n", "func_signal": "def _pad_version(left, right):\n", "code": "left_split, right_split = [], []\n\n# Get the release segment of our versions\nleft_split.append(list(itertools.takewhile(lambda x: x.isdigit(), left)))\nright_split.append(list(itertools.takewhile(lambda x: x.isdigit(), right)))\n\n# Get the rest of our versions\nleft_split.append(left[len(left_split[0]) :])\nright_split.append(right[len(right_split[0]) :])\n\n# Insert our padding\nleft_split.insert(1, [\"0\"] * max(0, len(right_split[0]) - len(left_split[0])))\nright_split.insert(1, [\"0\"] * max(0, len(left_split[0]) - len(right_split[0])))\n\nreturn (list(itertools.chain(*left_split)), list(itertools.chain(*right_split)))", "path": "pip/src/pip/_vendor/packaging/specifiers.py", "commit_date": "2020-11-28 00:00:00", "repo_name": "pypa/pip", "stars": 9178, "license": "mit", "language": "python", "size": 72277}
{"docstring": "# type: (Union[SpecifierSet, str]) -> SpecifierSet\n", "func_signal": "def __and__(self, other):\n", "code": "if isinstance(other, string_types):\n    other = SpecifierSet(other)\nelif not isinstance(other, SpecifierSet):\n    return NotImplemented\n\nspecifier = SpecifierSet()\nspecifier._specs = frozenset(self._specs | other._specs)\n\nif self._prereleases is None and other._prereleases is not None:\n    specifier._prereleases = other._prereleases\nelif self._prereleases is not None and other._prereleases is None:\n    specifier._prereleases = self._prereleases\nelif self._prereleases == other._prereleases:\n    specifier._prereleases = self._prereleases\nelse:\n    raise ValueError(\n        \"Cannot combine SpecifierSets with True and False prerelease \"\n        \"overrides.\"\n    )\n\nreturn specifier", "path": "pip/src/pip/_vendor/packaging/specifiers.py", "commit_date": "2020-11-28 00:00:00", "repo_name": "pypa/pip", "stars": 9178, "license": "mit", "language": "python", "size": 72277}
{"docstring": "\"\"\"Logs an error if no Copyright message appears at the top of the file.\"\"\"\n\n# We'll say it should occur by line 10. Don't forget there's a\n# dummy line at the front.\n", "func_signal": "def CheckForCopyright(filename, lines, error):\n", "code": "for line in range(1, min(len(lines), 11)):\n  if re.search(r'Copyright', lines[line], re.I): break\nelse:                       # means no copyright line was found\n  error(filename, 0, 'legal/copyright', 5,\n        'No copyright message found.  '\n        'You should have a line: \"Copyright [year] <Copyright Owner>\"')", "path": "CppCoreGuidelines/scripts/python/cpplint.py", "commit_date": "2018-01-22 00:00:00", "repo_name": "isocpp/CppCoreGuidelines", "stars": 41185, "license": "other", "language": "python", "size": 27250}
{"docstring": "\"\"\"Logs an error if a source file does not include its header.\"\"\"\n\n# Do not check test files\n", "func_signal": "def CheckHeaderFileIncluded(filename, include_state, error):\n", "code": "fileinfo = FileInfo(filename)\nif Search(_TEST_FILE_SUFFIX, fileinfo.BaseName()):\n  return\n\nfor ext in GetHeaderExtensions():\n    basefilename = filename[0:len(filename) - len(fileinfo.Extension())]\n    headerfile = basefilename + '.' + ext\n    if not os.path.exists(headerfile):\n      continue\n    headername = FileInfo(headerfile).RepositoryName()\n    first_include = None\n    for section_list in include_state.include_list:\n      for f in section_list:\n        if headername in f[0] or f[0] in headername:\n          return\n        if not first_include:\n          first_include = f[1]\n\n    error(filename, first_include, 'build/include', 5,\n          '%s should include its header file %s' % (fileinfo.RepositoryName(),\n                                                    headername))", "path": "CppCoreGuidelines/scripts/python/cpplint.py", "commit_date": "2018-01-22 00:00:00", "repo_name": "isocpp/CppCoreGuidelines", "stars": 41185, "license": "other", "language": "python", "size": 27250}
{"docstring": "\"\"\"Start analyzing function body.\n\nArgs:\n  function_name: The name of the function being tracked.\n\"\"\"\n", "func_signal": "def Begin(self, function_name):\n", "code": "self.in_a_function = True\nself.lines_in_function = 0\nself.current_function = function_name", "path": "CppCoreGuidelines/scripts/python/cpplint.py", "commit_date": "2018-01-22 00:00:00", "repo_name": "isocpp/CppCoreGuidelines", "stars": 41185, "license": "other", "language": "python", "size": 27250}
{"docstring": "\"\"\"Update nesting state with current line.\n\nArgs:\n  filename: The name of the current file.\n  clean_lines: A CleansedLines instance containing the file.\n  linenum: The number of the line to check.\n  error: The function to call with any errors found.\n\"\"\"\n", "func_signal": "def Update(self, filename, clean_lines, linenum, error):\n", "code": "line = clean_lines.elided[linenum]\n\n# Remember top of the previous nesting stack.\n#\n# The stack is always pushed/popped and not modified in place, so\n# we can just do a shallow copy instead of copy.deepcopy.  Using\n# deepcopy would slow down cpplint by ~28%.\nif self.stack:\n  self.previous_stack_top = self.stack[-1]\nelse:\n  self.previous_stack_top = None\n\n# Update pp_stack\nself.UpdatePreprocessor(line)\n\n# Count parentheses.  This is to avoid adding struct arguments to\n# the nesting stack.\nif self.stack:\n  inner_block = self.stack[-1]\n  depth_change = line.count('(') - line.count(')')\n  inner_block.open_parentheses += depth_change\n\n  # Also check if we are starting or ending an inline assembly block.\n  if inner_block.inline_asm in (_NO_ASM, _END_ASM):\n    if (depth_change != 0 and\n        inner_block.open_parentheses == 1 and\n        _MATCH_ASM.match(line)):\n      # Enter assembly block\n      inner_block.inline_asm = _INSIDE_ASM\n    else:\n      # Not entering assembly block.  If previous line was _END_ASM,\n      # we will now shift to _NO_ASM state.\n      inner_block.inline_asm = _NO_ASM\n  elif (inner_block.inline_asm == _INSIDE_ASM and\n        inner_block.open_parentheses == 0):\n    # Exit assembly block\n    inner_block.inline_asm = _END_ASM\n\n# Consume namespace declaration at the beginning of the line.  Do\n# this in a loop so that we catch same line declarations like this:\n#   namespace proto2 { namespace bridge { class MessageSet; } }\nwhile True:\n  # Match start of namespace.  The \"\\b\\s*\" below catches namespace\n  # declarations even if it weren't followed by a whitespace, this\n  # is so that we don't confuse our namespace checker.  The\n  # missing spaces will be flagged by CheckSpacing.\n  namespace_decl_match = Match(r'^\\s*namespace\\b\\s*([:\\w]+)?(.*)$', line)\n  if not namespace_decl_match:\n    break\n\n  new_namespace = _NamespaceInfo(namespace_decl_match.group(1), linenum)\n  self.stack.append(new_namespace)\n\n  line = namespace_decl_match.group(2)\n  if line.find('{') != -1:\n    new_namespace.seen_open_brace = True\n    line = line[line.find('{') + 1:]\n\n# Look for a class declaration in whatever is left of the line\n# after parsing namespaces.  The regexp accounts for decorated classes\n# such as in:\n#   class LOCKABLE API Object {\n#   };\nclass_decl_match = Match(\n    r'^(\\s*(?:template\\s*<[\\w\\s<>,:=]*>\\s*)?'\n    r'(class|struct)\\s+(?:[A-Z_]+\\s+)*(\\w+(?:::\\w+)*))'\n    r'(.*)$', line)\nif (class_decl_match and\n    (not self.stack or self.stack[-1].open_parentheses == 0)):\n  # We do not want to accept classes that are actually template arguments:\n  #   template <class Ignore1,\n  #             class Ignore2 = Default<Args>,\n  #             template <Args> class Ignore3>\n  #   void Function() {};\n  #\n  # To avoid template argument cases, we scan forward and look for\n  # an unmatched '>'.  If we see one, assume we are inside a\n  # template argument list.\n  end_declaration = len(class_decl_match.group(1))\n  if not self.InTemplateArgumentList(clean_lines, linenum, end_declaration):\n    self.stack.append(_ClassInfo(\n        class_decl_match.group(3), class_decl_match.group(2),\n        clean_lines, linenum))\n    line = class_decl_match.group(4)\n\n# If we have not yet seen the opening brace for the innermost block,\n# run checks here.\nif not self.SeenOpenBrace():\n  self.stack[-1].CheckBegin(filename, clean_lines, linenum, error)\n\n# Update access control if we are inside a class/struct\nif self.stack and isinstance(self.stack[-1], _ClassInfo):\n  classinfo = self.stack[-1]\n  access_match = Match(\n      r'^(.*)\\b(public|private|protected|signals)(\\s+(?:slots\\s*)?)?'\n      r':(?:[^:]|$)',\n      line)\n  if access_match:\n    classinfo.access = access_match.group(2)\n\n    # Check that access keywords are indented +1 space.  Skip this\n    # check if the keywords are not preceded by whitespaces.\n    indent = access_match.group(1)\n    if (len(indent) != classinfo.class_indent + 1 and\n        Match(r'^\\s*$', indent)):\n      if classinfo.is_struct:\n        parent = 'struct ' + classinfo.name\n      else:\n        parent = 'class ' + classinfo.name\n      slots = ''\n      if access_match.group(3):\n        slots = access_match.group(3)\n      error(filename, linenum, 'whitespace/indent', 3,\n            '%s%s: should be indented +1 space inside %s' % (\n                access_match.group(2), slots, parent))\n\n# Consume braces or semicolons from what's left of the line\nwhile True:\n  # Match first brace, semicolon, or closed parenthesis.\n  matched = Match(r'^[^{;)}]*([{;)}])(.*)$', line)\n  if not matched:\n    break\n\n  token = matched.group(1)\n  if token == '{':\n    # If namespace or class hasn't seen a opening brace yet, mark\n    # namespace/class head as complete.  Push a new block onto the\n    # stack otherwise.\n    if not self.SeenOpenBrace():\n      self.stack[-1].seen_open_brace = True\n    elif Match(r'^extern\\s*\"[^\"]*\"\\s*\\{', line):\n      self.stack.append(_ExternCInfo(linenum))\n    else:\n      self.stack.append(_BlockInfo(linenum, True))\n      if _MATCH_ASM.match(line):\n        self.stack[-1].inline_asm = _BLOCK_ASM\n\n  elif token == ';' or token == ')':\n    # If we haven't seen an opening brace yet, but we already saw\n    # a semicolon, this is probably a forward declaration.  Pop\n    # the stack for these.\n    #\n    # Similarly, if we haven't seen an opening brace yet, but we\n    # already saw a closing parenthesis, then these are probably\n    # function arguments with extra \"class\" or \"struct\" keywords.\n    # Also pop these stack for these.\n    if not self.SeenOpenBrace():\n      self.stack.pop()\n  else:  # token == '}'\n    # Perform end of block checks and pop the stack.\n    if self.stack:\n      self.stack[-1].CheckEnd(filename, clean_lines, linenum, error)\n      self.stack.pop()\n  line = matched.group(2)", "path": "CppCoreGuidelines/scripts/python/cpplint.py", "commit_date": "2018-01-22 00:00:00", "repo_name": "isocpp/CppCoreGuidelines", "stars": 41185, "license": "other", "language": "python", "size": 27250}
{"docstring": "\"\"\"Check that make_pair's template arguments are deduced.\n\nG++ 4.6 in C++11 mode fails badly if make_pair's template arguments are\nspecified explicitly, and such use isn't intended in any case.\n\nArgs:\n  filename: The name of the current file.\n  clean_lines: A CleansedLines instance containing the file.\n  linenum: The number of the line to check.\n  error: The function to call with any errors found.\n\"\"\"\n", "func_signal": "def CheckMakePairUsesDeduction(filename, clean_lines, linenum, error):\n", "code": "line = clean_lines.elided[linenum]\nmatch = _RE_PATTERN_EXPLICIT_MAKEPAIR.search(line)\nif match:\n  error(filename, linenum, 'build/explicit_make_pair',\n        4,  # 4 = high confidence\n        'For C++11-compatibility, omit template arguments from make_pair'\n        ' OR use pair directly OR if appropriate, construct a pair directly')", "path": "CppCoreGuidelines/scripts/python/cpplint.py", "commit_date": "2018-01-22 00:00:00", "repo_name": "isocpp/CppCoreGuidelines", "stars": 41185, "license": "other", "language": "python", "size": 27250}
{"docstring": "\"\"\"Collapses strings and chars on a line to simple \"\" or '' blocks.\n\nWe nix strings first so we're not fooled by text like '\"http://\"'\n\nArgs:\n  elided: The line being processed.\n\nReturns:\n  The line with collapsed strings.\n\"\"\"\n", "func_signal": "def _CollapseStrings(elided):\n", "code": "if _RE_PATTERN_INCLUDE.match(elided):\n  return elided\n\n# Remove escaped characters first to make quote/single quote collapsing\n# basic.  Things that look like escaped characters shouldn't occur\n# outside of strings and chars.\nelided = _RE_PATTERN_CLEANSE_LINE_ESCAPES.sub('', elided)\n\n# Replace quoted strings and digit separators.  Both single quotes\n# and double quotes are processed in the same loop, otherwise\n# nested quotes wouldn't work.\ncollapsed = ''\nwhile True:\n  # Find the first quote character\n  match = Match(r'^([^\\'\"]*)([\\'\"])(.*)$', elided)\n  if not match:\n    collapsed += elided\n    break\n  head, quote, tail = match.groups()\n\n  if quote == '\"':\n    # Collapse double quoted strings\n    second_quote = tail.find('\"')\n    if second_quote >= 0:\n      collapsed += head + '\"\"'\n      elided = tail[second_quote + 1:]\n    else:\n      # Unmatched double quote, don't bother processing the rest\n      # of the line since this is probably a multiline string.\n      collapsed += elided\n      break\n  else:\n    # Found single quote, check nearby text to eliminate digit separators.\n    #\n    # There is no special handling for floating point here, because\n    # the integer/fractional/exponent parts would all be parsed\n    # correctly as long as there are digits on both sides of the\n    # separator.  So we are fine as long as we don't see something\n    # like \"0.'3\" (gcc 4.9.0 will not allow this literal).\n    if Search(r'\\b(?:0[bBxX]?|[1-9])[0-9a-fA-F]*$', head):\n      match_literal = Match(r'^((?:\\'?[0-9a-zA-Z_])*)(.*)$', \"'\" + tail)\n      collapsed += head + match_literal.group(1).replace(\"'\", '')\n      elided = match_literal.group(2)\n    else:\n      second_quote = tail.find('\\'')\n      if second_quote >= 0:\n        collapsed += head + \"''\"\n        elided = tail[second_quote + 1:]\n      else:\n        # Unmatched single quote\n        collapsed += elided\n        break\n\nreturn collapsed", "path": "CppCoreGuidelines/scripts/python/cpplint.py", "commit_date": "2018-01-22 00:00:00", "repo_name": "isocpp/CppCoreGuidelines", "stars": 41185, "license": "other", "language": "python", "size": 27250}
{"docstring": "\"\"\"Searches a list of filenames and replaces directories in the list with\nall files descending from those directories. Files with extensions not in\nthe valid extensions list are excluded.\n\nArgs:\n  filenames: A list of files or directories\n\nReturns:\n  A list of all files that are members of filenames or descended from a\n  directory in filenames\n\"\"\"\n", "func_signal": "def _ExpandDirectories(filenames):\n", "code": "expanded = set()\nfor filename in filenames:\n    if not os.path.isdir(filename):\n      expanded.add(filename)\n      continue\n\n    for root, _, files in os.walk(filename):\n      for loopfile in files:\n        fullname = os.path.join(root, loopfile)\n        if fullname.startswith('.' + os.path.sep):\n          fullname = fullname[len('.' + os.path.sep):]\n        expanded.add(fullname)\n\nfiltered = []\nfor filename in expanded:\n    if os.path.splitext(filename)[1][1:] in GetAllExtensions():\n        filtered.append(filename)\n\nreturn filtered", "path": "CppCoreGuidelines/scripts/python/cpplint.py", "commit_date": "2018-01-22 00:00:00", "repo_name": "isocpp/CppCoreGuidelines", "stars": 41185, "license": "other", "language": "python", "size": 27250}
{"docstring": "\"\"\"Check if current line contains an out-of-line method definition.\n\nArgs:\n  clean_lines: A CleansedLines instance containing the file.\n  linenum: The number of the line to check.\nReturns:\n  True if current line contains an out-of-line method definition.\n\"\"\"\n# Scan back a few lines for start of current function\n", "func_signal": "def IsOutOfLineMethodDefinition(clean_lines, linenum):\n", "code": "for i in xrange(linenum, max(-1, linenum - 10), -1):\n  if Match(r'^([^()]*\\w+)\\(', clean_lines.elided[i]):\n    return Match(r'^[^()]*\\w+::\\w+\\(', clean_lines.elided[i]) is not None\nreturn False", "path": "CppCoreGuidelines/scripts/python/cpplint.py", "commit_date": "2018-01-22 00:00:00", "repo_name": "isocpp/CppCoreGuidelines", "stars": 41185, "license": "other", "language": "python", "size": 27250}
{"docstring": "\"\"\"Returns the CPP variable that should be used as a header guard.\n\nArgs:\n  filename: The name of a C++ header file.\n\nReturns:\n  The CPP variable that should be used as a header guard in the\n  named file.\n\n\"\"\"\n\n# Restores original filename in case that cpplint is invoked from Emacs's\n# flymake.\n", "func_signal": "def GetHeaderGuardCPPVariable(filename):\n", "code": "filename = re.sub(r'_flymake\\.h$', '.h', filename)\nfilename = re.sub(r'/\\.flymake/([^/]*)$', r'/\\1', filename)\n# Replace 'c++' with 'cpp'.\nfilename = filename.replace('C++', 'cpp').replace('c++', 'cpp')\n\nfileinfo = FileInfo(filename)\nfile_path_from_root = fileinfo.RepositoryName()\nif _root:\n  suffix = os.sep\n  # On Windows using directory separator will leave us with\n  # \"bogus escape error\" unless we properly escape regex.\n  if suffix == '\\\\':\n    suffix += '\\\\'\n  file_path_from_root = re.sub('^' + _root + suffix, '', file_path_from_root)\nreturn re.sub(r'[^a-zA-Z0-9]', '_', file_path_from_root).upper() + '_'", "path": "CppCoreGuidelines/scripts/python/cpplint.py", "commit_date": "2018-01-22 00:00:00", "repo_name": "isocpp/CppCoreGuidelines", "stars": 41185, "license": "other", "language": "python", "size": 27250}
{"docstring": "'''returns the indent depth, 0 means not code in markup'''\n", "func_signal": "def is_code(line, indent_depth = 4):\n", "code": "if line.startswith(' ' * indent_depth):\n    return len(line) - len(line.lstrip(' '))\nreturn 0", "path": "CppCoreGuidelines/scripts/python/md-split.py", "commit_date": "2017-10-11 00:00:00", "repo_name": "isocpp/CppCoreGuidelines", "stars": 41185, "license": "other", "language": "python", "size": 27250}
{"docstring": "\"\"\"Check if these two filenames belong to the same module.\n\nThe concept of a 'module' here is a as follows:\nfoo.h, foo-inl.h, foo.cc, foo_test.cc and foo_unittest.cc belong to the\nsame 'module' if they are in the same directory.\nsome/path/public/xyzzy and some/path/internal/xyzzy are also considered\nto belong to the same module here.\n\nIf the filename_cc contains a longer path than the filename_h, for example,\n'/absolute/path/to/base/sysinfo.cc', and this file would include\n'base/sysinfo.h', this function also produces the prefix needed to open the\nheader. This is used by the caller of this function to more robustly open the\nheader file. We don't have access to the real include paths in this context,\nso we need this guesswork here.\n\nKnown bugs: tools/base/bar.cc and base/bar.h belong to the same module\naccording to this implementation. Because of this, this function gives\nsome false positives. This should be sufficiently rare in practice.\n\nArgs:\n  filename_cc: is the path for the source (e.g. .cc) file\n  filename_h: is the path for the header path\n\nReturns:\n  Tuple with a bool and a string:\n  bool: True if filename_cc and filename_h belong to the same module.\n  string: the additional prefix needed to open the header file.\n\"\"\"\n", "func_signal": "def FilesBelongToSameModule(filename_cc, filename_h):\n", "code": "fileinfo_cc = FileInfo(filename_cc)\nif not fileinfo_cc.Extension().lstrip('.') in GetNonHeaderExtensions():\n  return (False, '')\n\nfileinfo_h = FileInfo(filename_h)\nif not fileinfo_h.Extension().lstrip('.') in GetHeaderExtensions():\n  return (False, '')\n\nfilename_cc = filename_cc[:-(len(fileinfo_cc.Extension()))]\nmatched_test_suffix = Search(_TEST_FILE_SUFFIX, fileinfo_cc.BaseName())\nif matched_test_suffix:\n  filename_cc = filename_cc[:-len(matched_test_suffix.group(1))]\n\nfilename_cc = filename_cc.replace('/public/', '/')\nfilename_cc = filename_cc.replace('/internal/', '/')\n\nfilename_h = filename_h[:-(len(fileinfo_h.Extension()))]\nif filename_h.endswith('-inl'):\n  filename_h = filename_h[:-len('-inl')]\nfilename_h = filename_h.replace('/public/', '/')\nfilename_h = filename_h.replace('/internal/', '/')\n\nfiles_belong_to_same_module = filename_cc.endswith(filename_h)\ncommon_path = ''\nif files_belong_to_same_module:\n  common_path = filename_cc[:-len(filename_h)]\nreturn files_belong_to_same_module, common_path", "path": "CppCoreGuidelines/scripts/python/cpplint.py", "commit_date": "2018-01-22 00:00:00", "repo_name": "isocpp/CppCoreGuidelines", "stars": 41185, "license": "other", "language": "python", "size": 27250}
{"docstring": "\"\"\"Checks for a C-style cast by looking for the pattern.\n\nArgs:\n  filename: The name of the current file.\n  clean_lines: A CleansedLines instance containing the file.\n  linenum: The number of the line to check.\n  cast_type: The string for the C++ cast to recommend.  This is either\n    reinterpret_cast, static_cast, or const_cast, depending.\n  pattern: The regular expression used to find C-style casts.\n  error: The function to call with any errors found.\n\nReturns:\n  True if an error was emitted.\n  False otherwise.\n\"\"\"\n", "func_signal": "def CheckCStyleCast(filename, clean_lines, linenum, cast_type, pattern, error):\n", "code": "line = clean_lines.elided[linenum]\nmatch = Search(pattern, line)\nif not match:\n  return False\n\n# Exclude lines with keywords that tend to look like casts\ncontext = line[0:match.start(1) - 1]\nif Match(r'.*\\b(?:sizeof|alignof|alignas|[_A-Z][_A-Z0-9]*)\\s*$', context):\n  return False\n\n# Try expanding current context to see if we one level of\n# parentheses inside a macro.\nif linenum > 0:\n  for i in xrange(linenum - 1, max(0, linenum - 5), -1):\n    context = clean_lines.elided[i] + context\nif Match(r'.*\\b[_A-Z][_A-Z0-9]*\\s*\\((?:\\([^()]*\\)|[^()])*$', context):\n  return False\n\n# operator++(int) and operator--(int)\nif context.endswith(' operator++') or context.endswith(' operator--'):\n  return False\n\n# A single unnamed argument for a function tends to look like old style cast.\n# If we see those, don't issue warnings for deprecated casts.\nremainder = line[match.end(0):]\nif Match(r'^\\s*(?:;|const\\b|throw\\b|final\\b|override\\b|[=>{),]|->)',\n         remainder):\n  return False\n\n# At this point, all that should be left is actual casts.\nerror(filename, linenum, 'readability/casting', 4,\n      'Using C-style cast.  Use %s<%s>(...) instead' %\n      (cast_type, match.group(1)))\n\nreturn True", "path": "CppCoreGuidelines/scripts/python/cpplint.py", "commit_date": "2018-01-22 00:00:00", "repo_name": "isocpp/CppCoreGuidelines", "stars": 41185, "license": "other", "language": "python", "size": 27250}
{"docstring": "\"\"\"Flag those C++14 features that we restrict.\n\nArgs:\n  filename: The name of the current file.\n  clean_lines: A CleansedLines instance containing the file.\n  linenum: The number of the line to check.\n  error: The function to call with any errors found.\n\"\"\"\n", "func_signal": "def FlagCxx14Features(filename, clean_lines, linenum, error):\n", "code": "line = clean_lines.elided[linenum]\n\ninclude = Match(r'\\s*#\\s*include\\s+[<\"]([^<\"]+)[\">]', line)\n\n# Flag unapproved C++14 headers.\nif include and include.group(1) in ('scoped_allocator', 'shared_mutex'):\n  error(filename, linenum, 'build/c++14', 5,\n        ('<%s> is an unapproved C++14 header.') % include.group(1))", "path": "CppCoreGuidelines/scripts/python/cpplint.py", "commit_date": "2018-01-22 00:00:00", "repo_name": "isocpp/CppCoreGuidelines", "stars": 41185, "license": "other", "language": "python", "size": 27250}
{"docstring": "# Remove well-formed html tags, fixing mistakes by legitimate users\n", "func_signal": "def stripped(line):\n", "code": "sline = TAG_REGEX.sub('', line)\nsline = re.sub('[()\\[\\]#*]', ' ', line)\nreturn sline", "path": "CppCoreGuidelines/scripts/python/md-split.py", "commit_date": "2017-10-11 00:00:00", "repo_name": "isocpp/CppCoreGuidelines", "stars": 41185, "license": "other", "language": "python", "size": 27250}
{"docstring": "\"\"\"Checks that VLOG() is only used for defining a logging level.\n\nFor example, VLOG(2) is correct. VLOG(INFO), VLOG(WARNING), VLOG(ERROR), and\nVLOG(FATAL) are not.\n\nArgs:\n  filename: The name of the current file.\n  clean_lines: A CleansedLines instance containing the file.\n  linenum: The number of the line to check.\n  error: The function to call with any errors found.\n\"\"\"\n", "func_signal": "def CheckVlogArguments(filename, clean_lines, linenum, error):\n", "code": "line = clean_lines.elided[linenum]\nif Search(r'\\bVLOG\\((INFO|ERROR|WARNING|DFATAL|FATAL)\\)', line):\n  error(filename, linenum, 'runtime/vlog', 5,\n        'VLOG() should be used with numeric verbosity level.  '\n        'Use LOG() if you want symbolic severity levels.')", "path": "CppCoreGuidelines/scripts/python/cpplint.py", "commit_date": "2018-01-22 00:00:00", "repo_name": "isocpp/CppCoreGuidelines", "stars": 41185, "license": "other", "language": "python", "size": 27250}
{"docstring": "\"\"\"Checks for the correctness of various spacing issues in the code.\n\nThings we check for: spaces around operators, spaces after\nif/for/while/switch, no spaces around parens in function calls, two\nspaces between code and comment, don't start a block with a blank\nline, don't end a function with a blank line, don't add a blank line\nafter public/protected/private, don't have too many blank lines in a row.\n\nArgs:\n  filename: The name of the current file.\n  clean_lines: A CleansedLines instance containing the file.\n  linenum: The number of the line to check.\n  nesting_state: A NestingState instance which maintains information about\n                 the current stack of nested blocks being parsed.\n  error: The function to call with any errors found.\n\"\"\"\n\n# Don't use \"elided\" lines here, otherwise we can't check commented lines.\n# Don't want to use \"raw\" either, because we don't want to check inside C++11\n# raw strings,\n", "func_signal": "def CheckSpacing(filename, clean_lines, linenum, nesting_state, error):\n", "code": "raw = clean_lines.lines_without_raw_strings\nline = raw[linenum]\n\n# Before nixing comments, check if the line is blank for no good\n# reason.  This includes the first line after a block is opened, and\n# blank lines at the end of a function (ie, right before a line like '}'\n#\n# Skip all the blank line checks if we are immediately inside a\n# namespace body.  In other words, don't issue blank line warnings\n# for this block:\n#   namespace {\n#\n#   }\n#\n# A warning about missing end of namespace comments will be issued instead.\n#\n# Also skip blank line checks for 'extern \"C\"' blocks, which are formatted\n# like namespaces.\nif (IsBlankLine(line) and\n    not nesting_state.InNamespaceBody() and\n    not nesting_state.InExternC()):\n  elided = clean_lines.elided\n  prev_line = elided[linenum - 1]\n  prevbrace = prev_line.rfind('{')\n  # TODO(unknown): Don't complain if line before blank line, and line after,\n  #                both start with alnums and are indented the same amount.\n  #                This ignores whitespace at the start of a namespace block\n  #                because those are not usually indented.\n  if prevbrace != -1 and prev_line[prevbrace:].find('}') == -1:\n    # OK, we have a blank line at the start of a code block.  Before we\n    # complain, we check if it is an exception to the rule: The previous\n    # non-empty line has the parameters of a function header that are indented\n    # 4 spaces (because they did not fit in a 80 column line when placed on\n    # the same line as the function name).  We also check for the case where\n    # the previous line is indented 6 spaces, which may happen when the\n    # initializers of a constructor do not fit into a 80 column line.\n    exception = False\n    if Match(r' {6}\\w', prev_line):  # Initializer list?\n      # We are looking for the opening column of initializer list, which\n      # should be indented 4 spaces to cause 6 space indentation afterwards.\n      search_position = linenum-2\n      while (search_position >= 0\n             and Match(r' {6}\\w', elided[search_position])):\n        search_position -= 1\n      exception = (search_position >= 0\n                   and elided[search_position][:5] == '    :')\n    else:\n      # Search for the function arguments or an initializer list.  We use a\n      # simple heuristic here: If the line is indented 4 spaces; and we have a\n      # closing paren, without the opening paren, followed by an opening brace\n      # or colon (for initializer lists) we assume that it is the last line of\n      # a function header.  If we have a colon indented 4 spaces, it is an\n      # initializer list.\n      exception = (Match(r' {4}\\w[^\\(]*\\)\\s*(const\\s*)?(\\{\\s*$|:)',\n                         prev_line)\n                   or Match(r' {4}:', prev_line))\n\n    if not exception:\n      error(filename, linenum, 'whitespace/blank_line', 2,\n            'Redundant blank line at the start of a code block '\n            'should be deleted.')\n  # Ignore blank lines at the end of a block in a long if-else\n  # chain, like this:\n  #   if (condition1) {\n  #     // Something followed by a blank line\n  #\n  #   } else if (condition2) {\n  #     // Something else\n  #   }\n  if linenum + 1 < clean_lines.NumLines():\n    next_line = raw[linenum + 1]\n    if (next_line\n        and Match(r'\\s*}', next_line)\n        and next_line.find('} else ') == -1):\n      error(filename, linenum, 'whitespace/blank_line', 3,\n            'Redundant blank line at the end of a code block '\n            'should be deleted.')\n\n  matched = Match(r'\\s*(public|protected|private):', prev_line)\n  if matched:\n    error(filename, linenum, 'whitespace/blank_line', 3,\n          'Do not leave a blank line after \"%s:\"' % matched.group(1))\n\n# Next, check comments\nnext_line_start = 0\nif linenum + 1 < clean_lines.NumLines():\n  next_line = raw[linenum + 1]\n  next_line_start = len(next_line) - len(next_line.lstrip())\nCheckComment(line, filename, linenum, next_line_start, error)\n\n# get rid of comments and strings\nline = clean_lines.elided[linenum]\n\n# You shouldn't have spaces before your brackets, except maybe after\n# 'delete []' or 'return []() {};'\nif Search(r'\\w\\s+\\[', line) and not Search(r'(?:delete|return)\\s+\\[', line):\n  error(filename, linenum, 'whitespace/braces', 5,\n        'Extra space before [')\n\n# In range-based for, we wanted spaces before and after the colon, but\n# not around \"::\" tokens that might appear.\nif (Search(r'for *\\(.*[^:]:[^: ]', line) or\n    Search(r'for *\\(.*[^: ]:[^:]', line)):\n  error(filename, linenum, 'whitespace/forcolon', 2,\n        'Missing space around colon in range-based for loop')", "path": "CppCoreGuidelines/scripts/python/cpplint.py", "commit_date": "2018-01-22 00:00:00", "repo_name": "isocpp/CppCoreGuidelines", "stars": 41185, "license": "other", "language": "python", "size": 27250}
{"docstring": "# Look for a bare ':'\n", "func_signal": "def CheckBegin(self, filename, clean_lines, linenum, error):\n", "code": "if Search('(^|[^:]):($|[^:])', clean_lines.elided[linenum]):\n  self.is_derived = True", "path": "CppCoreGuidelines/scripts/python/cpplint.py", "commit_date": "2018-01-22 00:00:00", "repo_name": "isocpp/CppCoreGuidelines", "stars": 41185, "license": "other", "language": "python", "size": 27250}
{"docstring": "\"\"\"Check rules that are applicable to #include lines.\n\nStrings on #include lines are NOT removed from elided line, to make\ncertain tasks easier. However, to prevent false positives, checks\napplicable to #include lines in CheckLanguage must be put here.\n\nArgs:\n  filename: The name of the current file.\n  clean_lines: A CleansedLines instance containing the file.\n  linenum: The number of the line to check.\n  include_state: An _IncludeState instance in which the headers are inserted.\n  error: The function to call with any errors found.\n\"\"\"\n", "func_signal": "def CheckIncludeLine(filename, clean_lines, linenum, include_state, error):\n", "code": "fileinfo = FileInfo(filename)\nline = clean_lines.lines[linenum]\n\n# \"include\" should use the new style \"foo/bar.h\" instead of just \"bar.h\"\n# Only do this check if the included header follows google naming\n# conventions.  If not, assume that it's a 3rd party API that\n# requires special include conventions.\n#\n# We also make an exception for Lua headers, which follow google\n# naming convention but not the include convention.\nmatch = Match(r'#include\\s*\"([^/]+\\.h)\"', line)\nif match and not _THIRD_PARTY_HEADERS_PATTERN.match(match.group(1)):\n  error(filename, linenum, 'build/include_subdir', 4,\n        'Include the directory when naming .h files')\n\n# we shouldn't include a file more than once. actually, there are a\n# handful of instances where doing so is okay, but in general it's\n# not.\nmatch = _RE_PATTERN_INCLUDE.search(line)\nif match:\n  include = match.group(2)\n  is_system = (match.group(1) == '<')\n  duplicate_line = include_state.FindHeader(include)\n  if duplicate_line >= 0:\n    error(filename, linenum, 'build/include', 4,\n          '\"%s\" already included at %s:%s' %\n          (include, filename, duplicate_line))\n    return\n\n  for extension in GetNonHeaderExtensions():\n    if (include.endswith('.' + extension) and\n        os.path.dirname(fileinfo.RepositoryName()) != os.path.dirname(include)):\n      error(filename, linenum, 'build/include', 4,\n            'Do not include .' + extension + ' files from other packages')\n      return\n\n  if not _THIRD_PARTY_HEADERS_PATTERN.match(include):\n    include_state.include_list[-1].append((include, linenum))\n\n    # We want to ensure that headers appear in the right order:\n    # 1) for foo.cc, foo.h  (preferred location)\n    # 2) c system files\n    # 3) cpp system files\n    # 4) for foo.cc, foo.h  (deprecated location)\n    # 5) other google headers\n    #\n    # We classify each include statement as one of those 5 types\n    # using a number of techniques. The include_state object keeps\n    # track of the highest type seen, and complains if we see a\n    # lower type after that.\n    error_message = include_state.CheckNextIncludeOrder(\n        _ClassifyInclude(fileinfo, include, is_system))\n    if error_message:\n      error(filename, linenum, 'build/include_order', 4,\n            '%s. Should be: %s.h, c system, c++ system, other.' %\n            (error_message, fileinfo.BaseName()))\n    canonical_include = include_state.CanonicalizeAlphabeticalOrder(include)\n    if not include_state.IsInAlphabeticalOrder(\n        clean_lines, linenum, canonical_include):\n      error(filename, linenum, 'build/include_alpha', 4,\n            'Include \"%s\" not in alphabetical order' % include)\n    include_state.SetLastHeader(canonical_include)", "path": "CppCoreGuidelines/scripts/python/cpplint.py", "commit_date": "2018-01-22 00:00:00", "repo_name": "isocpp/CppCoreGuidelines", "stars": 41185, "license": "other", "language": "python", "size": 27250}
{"docstring": "\"\"\"Check if expression looks like a type name, returns true if so.\n\nArgs:\n  clean_lines: A CleansedLines instance containing the file.\n  nesting_state: A NestingState instance which maintains information about\n                 the current stack of nested blocks being parsed.\n  expr: The expression to check.\nReturns:\n  True, if token looks like a type.\n\"\"\"\n# Keep only the last token in the expression\n", "func_signal": "def _IsType(clean_lines, nesting_state, expr):\n", "code": "last_word = Match(r'^.*(\\b\\S+)$', expr)\nif last_word:\n  token = last_word.group(1)\nelse:\n  token = expr\n\n# Match native types and stdint types\nif _TYPES.match(token):\n  return True\n\n# Try a bit harder to match templated types.  Walk up the nesting\n# stack until we find something that resembles a typename\n# declaration for what we are looking for.\ntypename_pattern = (r'\\b(?:typename|class|struct)\\s+' + re.escape(token) +\n                    r'\\b')\nblock_index = len(nesting_state.stack) - 1\nwhile block_index >= 0:\n  if isinstance(nesting_state.stack[block_index], _NamespaceInfo):\n    return False\n\n  # Found where the opening brace is.  We want to scan from this\n  # line up to the beginning of the function, minus a few lines.\n  #   template <typename Type1,  // stop scanning here\n  #             ...>\n  #   class C\n  #     : public ... {  // start scanning here\n  last_line = nesting_state.stack[block_index].starting_linenum\n\n  next_block_start = 0\n  if block_index > 0:\n    next_block_start = nesting_state.stack[block_index - 1].starting_linenum\n  first_line = last_line\n  while first_line >= next_block_start:\n    if clean_lines.elided[first_line].find('template') >= 0:\n      break\n    first_line -= 1\n  if first_line < next_block_start:\n    # Didn't find any \"template\" keyword before reaching the next block,\n    # there are probably no template things to check for this block\n    block_index -= 1\n    continue\n\n  # Look for typename in the specified range\n  for i in xrange(first_line, last_line + 1, 1):\n    if Search(typename_pattern, clean_lines.elided[i]):\n      return True\n  block_index -= 1\n\nreturn False", "path": "CppCoreGuidelines/scripts/python/cpplint.py", "commit_date": "2018-01-22 00:00:00", "repo_name": "isocpp/CppCoreGuidelines", "stars": 41185, "license": "other", "language": "python", "size": 27250}
{"docstring": "\"\"\"Get class info on the top of the stack.\n\nReturns:\n  A _ClassInfo object if we are inside a class, or None otherwise.\n\"\"\"\n", "func_signal": "def InnermostClass(self):\n", "code": "for i in range(len(self.stack), 0, -1):\n  classinfo = self.stack[i - 1]\n  if isinstance(classinfo, _ClassInfo):\n    return classinfo\nreturn None", "path": "CppCoreGuidelines/scripts/python/cpplint.py", "commit_date": "2018-01-22 00:00:00", "repo_name": "isocpp/CppCoreGuidelines", "stars": 41185, "license": "other", "language": "python", "size": 27250}
{"docstring": "\"\"\"\napps.get_containing_app_config() should raise an exception if\napps.apps_ready isn't True.\n\"\"\"\n", "func_signal": "def test_get_containing_app_config_apps_not_ready(self):\n", "code": "apps.apps_ready = False\ntry:\n    with self.assertRaisesMessage(AppRegistryNotReady, \"Apps aren't loaded yet\"):\n        apps.get_containing_app_config('foo')\nfinally:\n    apps.apps_ready = True", "path": "django/tests/apps/tests.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "django/django", "stars": 75956, "license": "bsd-3-clause", "language": "python", "size": 247650}
{"docstring": "\"\"\"If path set as class attr, overrides __path__ and __file__.\"\"\"\n", "func_signal": "def test_explicit_path_overrides(self):\n", "code": "class MyAppConfig(AppConfig):\n    path = 'foo'\n\nac = MyAppConfig('label', Stub(__path__=['a'], __file__='b/__init__.py'))\n\nself.assertEqual(ac.path, 'foo')", "path": "django/tests/apps/tests.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "django/django", "stars": 75956, "license": "bsd-3-clause", "language": "python", "size": 247650}
{"docstring": "\"\"\"\nTests when INSTALLED_APPS contains an incorrect app config.\n\"\"\"\n", "func_signal": "def test_bad_app_config(self):\n", "code": "msg = \"'apps.apps.BadConfig' must supply a name attribute.\"\nwith self.assertRaisesMessage(ImproperlyConfigured, msg):\n    with self.settings(INSTALLED_APPS=['apps.apps.BadConfig']):\n        pass", "path": "django/tests/apps/tests.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "django/django", "stars": 75956, "license": "bsd-3-clause", "language": "python", "size": 247650}
{"docstring": "\"\"\"\nLoad an app that specifies a default AppConfig class in __init__ and do\nnot have an apps module.\n\"\"\"\n", "func_signal": "def test_explicit_default_app_config_without_apps(self):\n", "code": "msg = (\n    \"'apps.explicit_default_config_without_apps' defines \"\n    \"default_app_config = 'apps.explicit_default_config_without_apps.\"\n    \"ExplicitDefaultConfigWithoutApps'. However, Django's automatic \"\n    \"detection did not find this configuration. You should move the \"\n    \"default config class to the apps submodule of your application \"\n    \"and, if this module defines several config classes, mark the \"\n    \"default one with default = True.\"\n)\nwith self.assertRaisesMessage(RemovedInDjango41Warning, msg):\n    with self.settings(INSTALLED_APPS=['apps.explicit_default_config_without_apps']):\n        pass\nwith ignore_warnings(category=RemovedInDjango41Warning):\n    with self.settings(INSTALLED_APPS=['apps.explicit_default_config_without_apps']):\n        self.assertIsInstance(\n            apps.get_app_config('explicit_default_config_without_apps'),\n            ExplicitDefaultConfigWithoutApps,\n        )", "path": "django/tests/apps/tests.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "django/django", "stars": 75956, "license": "bsd-3-clause", "language": "python", "size": 247650}
{"docstring": "\"\"\"Load an app that provides two default AppConfig classes.\"\"\"\n", "func_signal": "def test_two_default_configs_app(self):\n", "code": "msg = (\n    \"'apps.two_default_configs_app.apps' declares more than one \"\n    \"default AppConfig: 'TwoConfig', 'TwoConfigBis'.\"\n)\nwith self.assertRaisesMessage(RuntimeError, msg):\n    with self.settings(INSTALLED_APPS=['apps.two_default_configs_app']):\n        pass", "path": "django/tests/apps/tests.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "django/django", "stars": 75956, "license": "bsd-3-clause", "language": "python", "size": 247650}
{"docstring": "\"\"\"\nLoad an app that specifies a default AppConfig class matching the\nautodetected one.\n\"\"\"\n", "func_signal": "def test_explicit_default_app_config_warning(self):\n", "code": "msg = (\n    \"'apps.explicit_default_config_app' defines default_app_config = \"\n    \"'apps.explicit_default_config_app.apps.ExplicitDefaultConfig'. \"\n    \"Django now detects this configuration automatically. You can \"\n    \"remove default_app_config.\"\n)\nwith self.assertRaisesMessage(RemovedInDjango41Warning, msg):\n    with self.settings(INSTALLED_APPS=['apps.explicit_default_config_app']):\n        pass\nwith ignore_warnings(category=RemovedInDjango41Warning):\n    with self.settings(INSTALLED_APPS=['apps.explicit_default_config_app']):\n        self.assertIsInstance(\n            apps.get_app_config('explicit_default_config_app'),\n            ExplicitDefaultConfig,\n        )", "path": "django/tests/apps/tests.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "django/django", "stars": 75956, "license": "bsd-3-clause", "language": "python", "size": 247650}
{"docstring": "\"\"\"\nTest for behavior when two models clash in the app registry.\n\"\"\"\n", "func_signal": "def test_model_clash(self):\n", "code": "new_apps = Apps([\"apps\"])\nmeta_contents = {\n    'app_label': \"apps\",\n    'apps': new_apps,\n}\n\nbody = {}\nbody['Meta'] = type(\"Meta\", (), meta_contents)\nbody['__module__'] = TotallyNormal.__module__\ntype(\"SouthPonies\", (models.Model,), body)\n\n# When __name__ and __module__ match we assume the module\n# was reloaded and issue a warning. This use-case is\n# useful for REPL. Refs #23621.\nbody = {}\nbody['Meta'] = type(\"Meta\", (), meta_contents)\nbody['__module__'] = TotallyNormal.__module__\nmsg = (\n    \"Model 'apps.southponies' was already registered. \"\n    \"Reloading models is not advised as it can lead to inconsistencies, \"\n    \"most notably with related models.\"\n)\nwith self.assertRaisesMessage(RuntimeWarning, msg):\n    type(\"SouthPonies\", (models.Model,), body)\n\n# If it doesn't appear to be a reloaded module then we expect\n# a RuntimeError.\nbody = {}\nbody['Meta'] = type(\"Meta\", (), meta_contents)\nbody['__module__'] = TotallyNormal.__module__ + '.whatever'\nwith self.assertRaisesMessage(RuntimeError, \"Conflicting 'southponies' models in application 'apps':\"):\n    type(\"SouthPonies\", (models.Model,), body)", "path": "django/tests/apps/tests.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "django/django", "stars": 75956, "license": "bsd-3-clause", "language": "python", "size": 247650}
{"docstring": "\"\"\"\nApp discovery should preserve stack traces. Regression test for #22920.\n\"\"\"\n", "func_signal": "def test_import_exception_is_not_masked(self):\n", "code": "with self.assertRaisesMessage(ImportError, \"Oops\"):\n    with self.settings(INSTALLED_APPS=['import_error_package']):\n        pass", "path": "django/tests/apps/tests.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "django/django", "stars": 75956, "license": "bsd-3-clause", "language": "python", "size": 247650}
{"docstring": "\"\"\"\nIf the __path__ attr contains duplicate paths and there is no\n__file__, they duplicates should be deduplicated (#25246).\n\"\"\"\n", "func_signal": "def test_duplicate_dunder_path_no_dunder_file(self):\n", "code": "ac = AppConfig('label', Stub(__path__=['a', 'a']))\nself.assertEqual(ac.path, 'a')", "path": "django/tests/apps/tests.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "django/django", "stars": 75956, "license": "bsd-3-clause", "language": "python", "size": 247650}
{"docstring": "\"\"\"If single element in __path__, use it (in preference to __file__).\"\"\"\n", "func_signal": "def test_dunder_path(self):\n", "code": "ac = AppConfig('label', Stub(__path__=['a'], __file__='b/__init__.py'))\n\nself.assertEqual(ac.path, 'a')", "path": "django/tests/apps/tests.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "django/django", "stars": 75956, "license": "bsd-3-clause", "language": "python", "size": 247650}
{"docstring": "\"\"\"If subclass sets path as class attr, no module attributes needed.\"\"\"\n", "func_signal": "def test_path_set_explicitly(self):\n", "code": "class MyAppConfig(AppConfig):\n    path = 'foo'\n\nac = MyAppConfig('label', Stub())\n\nself.assertEqual(ac.path, 'foo')", "path": "django/tests/apps/tests.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "django/django", "stars": 75956, "license": "bsd-3-clause", "language": "python", "size": 247650}
{"docstring": "\"\"\"\nMultiple locations are ok only if app-config has explicit path.\n\"\"\"\n# Temporarily add two directories to sys.path that both contain\n# components of the \"nsapp\" package.\n", "func_signal": "def test_multiple_paths_explicit_path(self):\n", "code": "with extend_sys_path(self.base_location, self.other_location):\n    with self.settings(INSTALLED_APPS=['nsapp.apps.NSAppConfig']):\n        app_config = apps.get_app_config('nsapp')\n        self.assertEqual(app_config.path, self.app_path)", "path": "django/tests/apps/tests.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "django/django", "stars": 75956, "license": "bsd-3-clause", "language": "python", "size": 247650}
{"docstring": "\"\"\"\nLoad an app that provides two AppConfig classes, one being the default.\n\"\"\"\n", "func_signal": "def test_two_configs_one_default_app(self):\n", "code": "with self.settings(INSTALLED_APPS=['apps.two_configs_one_default_app']):\n    config = apps.get_app_config('two_configs_one_default_app')\nself.assertIsInstance(config, TwoConfig)", "path": "django/tests/apps/tests.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "django/django", "stars": 75956, "license": "bsd-3-clause", "language": "python", "size": 247650}
{"docstring": "\"\"\"\nA Py3.3+ namespace package with multiple locations cannot be an app.\n\n(Because then we wouldn't know where to load its templates, static\nassets, etc. from.)\n\"\"\"\n# Temporarily add two directories to sys.path that both contain\n# components of the \"nsapp\" package.\n", "func_signal": "def test_multiple_paths(self):\n", "code": "with extend_sys_path(self.base_location, self.other_location):\n    with self.assertRaises(ImproperlyConfigured):\n        with self.settings(INSTALLED_APPS=['nsapp']):\n            pass", "path": "django/tests/apps/tests.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "django/django", "stars": 75956, "license": "bsd-3-clause", "language": "python", "size": 247650}
{"docstring": "\"\"\"If the __path__ attr is length>1, use __file__ if set.\"\"\"\n", "func_signal": "def test_multiple_dunder_path_fallback_to_dunder_file(self):\n", "code": "ac = AppConfig('label', Stub(__path__=['a', 'b'], __file__='c/__init__.py'))\n\nself.assertEqual(ac.path, 'c')", "path": "django/tests/apps/tests.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "django/django", "stars": 75956, "license": "bsd-3-clause", "language": "python", "size": 247650}
{"docstring": "\"\"\"\nTests the ready property of the master registry.\n\"\"\"\n# The master app registry is always ready when the tests run.\n", "func_signal": "def test_ready(self):\n", "code": "self.assertIs(apps.ready, True)\n# Non-master app registries are populated in __init__.\nself.assertIs(Apps().ready, True)\n# The condition is set when apps are ready\nself.assertIs(apps.ready_event.is_set(), True)\nself.assertIs(Apps().ready_event.is_set(), True)", "path": "django/tests/apps/tests.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "django/django", "stars": 75956, "license": "bsd-3-clause", "language": "python", "size": 247650}
{"docstring": "\"\"\"If there is no __path__ attr, use __file__.\"\"\"\n", "func_signal": "def test_no_dunder_path_fallback_to_dunder_file(self):\n", "code": "ac = AppConfig('label', Stub(__file__='b/__init__.py'))\n\nself.assertEqual(ac.path, 'b')", "path": "django/tests/apps/tests.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "django/django", "stars": 75956, "license": "bsd-3-clause", "language": "python", "size": 247650}
{"docstring": "\"\"\"Load an app that doesn't provide an AppConfig class.\"\"\"\n", "func_signal": "def test_no_config_app(self):\n", "code": "with self.settings(INSTALLED_APPS=['apps.no_config_app']):\n    config = apps.get_app_config('no_config_app')\nself.assertIsInstance(config, AppConfig)", "path": "django/tests/apps/tests.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "django/django", "stars": 75956, "license": "bsd-3-clause", "language": "python", "size": 247650}
{"docstring": "\"\"\"\nMakes a new model at runtime and ensures it goes into the right place.\n\"\"\"\n", "func_signal": "def test_dynamic_load(self):\n", "code": "old_models = list(apps.get_app_config(\"apps\").get_models())\n# Construct a new model in a new app registry\nbody = {}\nnew_apps = Apps([\"apps\"])\nmeta_contents = {\n    'app_label': \"apps\",\n    'apps': new_apps,\n}\nmeta = type(\"Meta\", (), meta_contents)\nbody['Meta'] = meta\nbody['__module__'] = TotallyNormal.__module__\ntemp_model = type(\"SouthPonies\", (models.Model,), body)\n# Make sure it appeared in the right place!\nself.assertEqual(list(apps.get_app_config(\"apps\").get_models()), old_models)\nwith self.assertRaises(LookupError):\n    apps.get_model(\"apps\", \"SouthPonies\")\nself.assertEqual(new_apps.get_model(\"apps\", \"SouthPonies\"), temp_model)", "path": "django/tests/apps/tests.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "django/django", "stars": 75956, "license": "bsd-3-clause", "language": "python", "size": 247650}
{"docstring": "\"\"\"If the __path__ attr is length>1 and there is no __file__, raise.\"\"\"\n", "func_signal": "def test_multiple_dunder_path_no_dunder_file(self):\n", "code": "with self.assertRaises(ImproperlyConfigured):\n    AppConfig('label', Stub(__path__=['a', 'b']))", "path": "django/tests/apps/tests.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "django/django", "stars": 75956, "license": "bsd-3-clause", "language": "python", "size": 247650}
{"docstring": "# ToDo: Handle situations where share is password protected\n", "func_signal": "def mkdir(self, shareName, pathName, password = None):\n", "code": "pathName = pathName.replace('/', '\\\\')\npathName = ntpath.normpath(pathName)\nif len(pathName) > 0 and pathName[0] == '\\\\':\n    pathName = pathName[1:]\n\ntreeId = self.connectTree(shareName)\n\nfileId = None\ntry:\n    fileId = self.create(treeId, pathName, GENERIC_ALL, FILE_SHARE_READ | FILE_SHARE_WRITE | FILE_SHARE_DELETE,\n                         FILE_DIRECTORY_FILE | FILE_SYNCHRONOUS_IO_NONALERT, FILE_CREATE, 0)\nfinally:\n    if fileId is not None:\n        self.close(treeId, fileId)\n    self.disconnectTree(treeId)\n\nreturn True", "path": "impacket/impacket/smb3.py", "commit_date": "2020-06-09 00:00:00", "repo_name": "fortra/impacket", "stars": 12473, "license": "other", "language": "python", "size": 9110}
{"docstring": "# Let's store some data for later use\n", "func_signal": "def negotiateSession(self, preferredDialect = None, negSessionResponse = None):\n", "code": "self._Connection['ClientSecurityMode'] = SMB2_NEGOTIATE_SIGNING_ENABLED\nif self.RequireMessageSigning is True:\n    self._Connection['ClientSecurityMode'] |= SMB2_NEGOTIATE_SIGNING_REQUIRED\nself._Connection['Capabilities'] = SMB2_GLOBAL_CAP_ENCRYPTION\ncurrentDialect = SMB2_DIALECT_WILDCARD\n\n# Do we have a negSessionPacket already?\nif negSessionResponse is not None:\n    # Yes, let's store the dialect answered back\n    negResp = SMB2Negotiate_Response(negSessionResponse['Data'])\n    currentDialect = negResp['DialectRevision']\n\nif currentDialect == SMB2_DIALECT_WILDCARD:\n    # Still don't know the chosen dialect, let's send our options\n\n    packet = self.SMB_PACKET()\n    packet['Command'] = SMB2_NEGOTIATE\n    negSession = SMB2Negotiate()\n\n    negSession['SecurityMode'] = self._Connection['ClientSecurityMode']\n    negSession['Capabilities'] = self._Connection['Capabilities']\n    negSession['ClientGuid'] = self.ClientGuid\n    if preferredDialect is not None:\n        negSession['Dialects'] = [preferredDialect]\n        if preferredDialect == SMB2_DIALECT_311:\n            # Build the Contexts\n            contextData = SMB311ContextData()\n            contextData['NegotiateContextOffset'] = 64+38+2\n            contextData['NegotiateContextCount'] = 0\n            # Add an SMB2_NEGOTIATE_CONTEXT with ContextType as SMB2_PREAUTH_INTEGRITY_CAPABILITIES\n            # to the negotiate request as specified in section 2.2.3.1:\n            negotiateContext = SMB2NegotiateContext()\n            negotiateContext['ContextType'] = SMB2_PREAUTH_INTEGRITY_CAPABILITIES\n\n            preAuthIntegrityCapabilities = SMB2PreAuthIntegrityCapabilities()\n            preAuthIntegrityCapabilities['HashAlgorithmCount'] = 1\n            preAuthIntegrityCapabilities['SaltLength'] = 32\n            preAuthIntegrityCapabilities['HashAlgorithms'] = b'\\x01\\x00'\n            preAuthIntegrityCapabilities['Salt'] = ''.join([rand.choice(string.ascii_letters) for _ in\n                                                             range(preAuthIntegrityCapabilities['SaltLength'])])\n\n            negotiateContext['Data'] = preAuthIntegrityCapabilities.getData()\n            negotiateContext['DataLength'] = len(negotiateContext['Data'])\n            contextData['NegotiateContextCount'] += 1\n            pad = b'\\xFF' * ((8 - (negotiateContext['DataLength'] % 8)) % 8)\n\n            # Add an SMB2_NEGOTIATE_CONTEXT with ContextType as SMB2_ENCRYPTION_CAPABILITIES\n            # to the negotiate request as specified in section 2.2.3.1 and initialize\n            # the Ciphers field with the ciphers supported by the client in the order of preference.\n\n            negotiateContext2 = SMB2NegotiateContext ()\n            negotiateContext2['ContextType'] = SMB2_ENCRYPTION_CAPABILITIES\n\n            encryptionCapabilities = SMB2EncryptionCapabilities()\n            encryptionCapabilities['CipherCount'] = 1\n            encryptionCapabilities['Ciphers'] = 1\n\n            negotiateContext2['Data'] = encryptionCapabilities.getData()\n            negotiateContext2['DataLength'] = len(negotiateContext2['Data'])\n            contextData['NegotiateContextCount'] += 1\n\n            negSession['ClientStartTime'] = contextData.getData()\n            negSession['Padding'] = b'\\xFF\\xFF'\n            # Subsequent negotiate contexts MUST appear at the first 8-byte aligned offset following the\n            # previous negotiate context.\n            negSession['NegotiateContextList'] = negotiateContext.getData() + pad + negotiateContext2.getData()\n\n            # Do you want to enforce encryption? Uncomment here:\n            #self._Connection['SupportsEncryption'] = True\n\n    else:\n        negSession['Dialects'] = [SMB2_DIALECT_002, SMB2_DIALECT_21, SMB2_DIALECT_30]\n    negSession['DialectCount'] = len(negSession['Dialects'])\n    packet['Data'] = negSession\n\n    packetID = self.sendSMB(packet)\n    ans = self.recvSMB(packetID)\n    if ans.isValidAnswer(STATUS_SUCCESS):\n        negResp = SMB2Negotiate_Response(ans['Data'])\n        if negResp['DialectRevision']  == SMB2_DIALECT_311:\n            self.__UpdateConnectionPreAuthHash(ans.rawData)\n\nself._Connection['MaxTransactSize']   = min(0x100000,negResp['MaxTransactSize'])\nself._Connection['MaxReadSize']       = min(0x100000,negResp['MaxReadSize'])\nself._Connection['MaxWriteSize']      = min(0x100000,negResp['MaxWriteSize'])\nself._Connection['ServerGuid']        = negResp['ServerGuid']\nself._Connection['GSSNegotiateToken'] = negResp['Buffer']\nself._Connection['Dialect']           = negResp['DialectRevision']\nif (negResp['SecurityMode'] & SMB2_NEGOTIATE_SIGNING_REQUIRED) == SMB2_NEGOTIATE_SIGNING_REQUIRED or \\\n        self._Connection['Dialect'] == SMB2_DIALECT_311:\n    self._Connection['RequireSigning'] = True\nif self._Connection['Dialect'] == SMB2_DIALECT_311:\n    # Always Sign\n    self._Connection['RequireSigning'] = True\n\nif (negResp['Capabilities'] & SMB2_GLOBAL_CAP_LEASING) == SMB2_GLOBAL_CAP_LEASING:\n    self._Connection['SupportsFileLeasing'] = True\nif (negResp['Capabilities'] & SMB2_GLOBAL_CAP_LARGE_MTU) == SMB2_GLOBAL_CAP_LARGE_MTU:\n    self._Connection['SupportsMultiCredit'] = True\n\nif self._Connection['Dialect'] >= SMB2_DIALECT_30:\n    # Switching to the right packet format\n    self.SMB_PACKET = SMB3Packet\n    if (negResp['Capabilities'] & SMB2_GLOBAL_CAP_DIRECTORY_LEASING) == SMB2_GLOBAL_CAP_DIRECTORY_LEASING:\n        self._Connection['SupportsDirectoryLeasing'] = True\n    if (negResp['Capabilities'] & SMB2_GLOBAL_CAP_MULTI_CHANNEL) == SMB2_GLOBAL_CAP_MULTI_CHANNEL:\n        self._Connection['SupportsMultiChannel'] = True\n    if (negResp['Capabilities'] & SMB2_GLOBAL_CAP_PERSISTENT_HANDLES) == SMB2_GLOBAL_CAP_PERSISTENT_HANDLES:\n        self._Connection['SupportsPersistentHandles'] = True\n    if (negResp['Capabilities'] & SMB2_GLOBAL_CAP_ENCRYPTION) == SMB2_GLOBAL_CAP_ENCRYPTION:\n        self._Connection['SupportsEncryption'] = True\n\n    self._Connection['ServerCapabilities'] = negResp['Capabilities']\n    self._Connection['ServerSecurityMode'] = negResp['SecurityMode']", "path": "impacket/impacket/smb3.py", "commit_date": "2020-06-09 00:00:00", "repo_name": "fortra/impacket", "stars": 12473, "license": "other", "language": "python", "size": 9110}
{"docstring": "# If TGT or TGS are specified, they are in the form of:\n# TGS['KDC_REP'] = the response from the server\n# TGS['cipher'] = the cipher used\n# TGS['sessionKey'] = the sessionKey\n# If we have hashes, normalize them\n", "func_signal": "def kerberosLogin(self, user, password, domain = '', lmhash = '', nthash = '', aesKey='', kdcHost = '', TGT=None, TGS=None, mutualAuth=False):\n", "code": "if lmhash != '' or nthash != '':\n    if len(lmhash) % 2:     lmhash = '0%s' % lmhash\n    if len(nthash) % 2:     nthash = '0%s' % nthash\n    try: # just in case they were converted already\n        lmhash = a2b_hex(lmhash)\n        nthash = a2b_hex(nthash)\n    except:\n        pass\n\nself.__userName = user\nself.__password = password\nself.__domain   = domain\nself.__lmhash   = lmhash\nself.__nthash   = nthash\nself.__kdc      = kdcHost\nself.__aesKey   = aesKey\nself.__TGT      = TGT\nself.__TGS      = TGS\nself._doKerberos= True\n\nsessionSetup = SMB2SessionSetup()\nif self.RequireMessageSigning is True:\n   sessionSetup['SecurityMode'] = SMB2_NEGOTIATE_SIGNING_REQUIRED\nelse:\n   sessionSetup['SecurityMode'] = SMB2_NEGOTIATE_SIGNING_ENABLED\n\nsessionSetup['Flags'] = 0\n#sessionSetup['Capabilities'] = SMB2_GLOBAL_CAP_LARGE_MTU | SMB2_GLOBAL_CAP_LEASING | SMB2_GLOBAL_CAP_DFS\n\n# Importing down here so pyasn1 is not required if kerberos is not used.\nfrom impacket.krb5.asn1 import AP_REQ, Authenticator, TGS_REP, seq_set\nfrom impacket.krb5.kerberosv5 import getKerberosTGT, getKerberosTGS\nfrom impacket.krb5 import constants\nfrom impacket.krb5.types import Principal, KerberosTime, Ticket\nfrom pyasn1.codec.der import decoder, encoder\nimport datetime\n\n# First of all, we need to get a TGT for the user\nuserName = Principal(user, type=constants.PrincipalNameType.NT_PRINCIPAL.value)\nif TGT is None:\n    if TGS is None:\n        tgt, cipher, oldSessionKey, sessionKey = getKerberosTGT(userName, password, domain, lmhash, nthash, aesKey, kdcHost)\nelse:\n    tgt = TGT['KDC_REP']\n    cipher = TGT['cipher']\n    sessionKey = TGT['sessionKey']\n\n# Save the ticket\n# If you want, for debugging purposes", "path": "impacket/impacket/smb3.py", "commit_date": "2020-06-09 00:00:00", "repo_name": "fortra/impacket", "stars": 12473, "license": "other", "language": "python", "size": 9110}
{"docstring": "# 802.11 Control Frame ACK\n", "func_signal": "def setUp(self):\n", "code": "self.frame_orig=b'\\xd4\\x00\\x00\\x00\\x00\\x08\\x54\\xac\\x2f\\x85\\xb7\\x7f\\xc3\\x9e'\n\nd = Dot11(self.frame_orig)\n\ntype = d.get_type()\nself.assertEqual(type,Dot11Types.DOT11_TYPE_CONTROL)\n\nsubtype = d.get_subtype()\nself.assertEqual(subtype,Dot11Types.DOT11_SUBTYPE_CONTROL_ACKNOWLEDGMENT)\n\ntypesubtype = d.get_type_n_subtype()\nself.assertEqual(typesubtype,Dot11Types.DOT11_TYPE_CONTROL_SUBTYPE_ACKNOWLEDGMENT)\n    \nself.ack = Dot11ControlFrameACK(d.get_body_as_string())\n    \nd.contains(self.ack)", "path": "impacket/tests/dot11/test_FrameControlACK.py", "commit_date": "2018-12-05 00:00:00", "repo_name": "fortra/impacket", "stars": 12473, "license": "other", "language": "python", "size": 9110}
{"docstring": "# Direct connection to ncacn_http service, RPC over HTTP v1\n# No authentication\n", "func_signal": "def test_1(self):\n", "code": "stringbinding = 'ncacn_http:%s' % self.machine\nrpctransport = transport.DCERPCTransportFactory(stringbinding)\n\ndce = rpctransport.get_dce_rpc()\ndce.connect()\ndce.bind(epm.MSRPC_UUID_PORTMAP)\n\nrequest = epm.ept_lookup()\nrequest['inquiry_type'] = epm.RPC_C_EP_ALL_ELTS\nrequest['object'] = NULL\nrequest['Ifid'] = NULL\nrequest['vers_option'] = epm.RPC_C_VERS_ALL\nrequest['max_ents'] = 10\n\nresp = dce.request(request)\ndce.disconnect()\n\n# Reconnecting\ndce.connect()\ndce.bind(epm.MSRPC_UUID_PORTMAP)\n\nresp = dce.request(request)\ndce.disconnect()", "path": "impacket/tests/SMB_RPC/test_rpch.py", "commit_date": "2020-08-19 00:00:00", "repo_name": "fortra/impacket", "stars": 12473, "license": "other", "language": "python", "size": 9110}
{"docstring": "# CONN/C2\n", "func_signal": "def test_5(self):\n", "code": "resp = b'\\x05\\x00\\x14\\x03\\x10\\x00\\x00\\x00\\x2c\\x00\\x00\\x00\\x00\\x00' + \\\n       b'\\x00\\x00\\x00\\x00\\x03\\x00\\x06\\x00\\x00\\x00\\x01\\x00\\x00\\x00' + \\\n       b'\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x02\\x00\\x00\\x00\\xc0\\xd4' + \\\n       b'\\x01\\x00'\n\npacket = rpch.RTSHeader(resp)\npacket.dump()\n\npduData = packet['pduData']\nnumberOfCommands = packet['NumberOfCommands']\n\nserver_cmds = []\nwhile numberOfCommands > 0:\n    numberOfCommands -= 1\n\n    cmd_type = unpack('<L', pduData[:4])[0]\n    cmd = rpch.COMMANDS[cmd_type](pduData)\n    server_cmds.append(cmd)\n    pduData = pduData[len(cmd):]\n\nfor cmd in server_cmds:\n    cmd.dump()\n\nconnectionTimeout = rpch.ConnectionTimeout()\nconnectionTimeout['ConnectionTimeout'] = 120000\n\nself.assertTrue(server_cmds[2].getData() == connectionTimeout.getData())\n\nreceiveWindowSize = rpch.ReceiveWindowSize()\nreceiveWindowSize['ReceiveWindowSize'] = 65536\n\nself.assertTrue(server_cmds[1].getData() == receiveWindowSize.getData())\nself.assertTrue(server_cmds[0].getData() == rpch.Version().getData())", "path": "impacket/tests/SMB_RPC/test_rpch.py", "commit_date": "2020-08-19 00:00:00", "repo_name": "fortra/impacket", "stars": 12473, "license": "other", "language": "python", "size": 9110}
{"docstring": "# ToDo: Handle situations where share is password protected\n", "func_signal": "def rmdir(self, shareName, pathName, password = None):\n", "code": "pathName = pathName.replace('/', '\\\\')\npathName = ntpath.normpath(pathName)\nif len(pathName) > 0 and pathName[0] == '\\\\':\n    pathName = pathName[1:]\n\ntreeId = self.connectTree(shareName)\n\nfileId = None\ntry:\n    fileId = self.create(treeId, pathName, desiredAccess=DELETE | FILE_READ_ATTRIBUTES | SYNCHRONIZE,\n                         shareMode=FILE_SHARE_DELETE | FILE_SHARE_READ | FILE_SHARE_WRITE,\n                         creationOptions=FILE_DIRECTORY_FILE | FILE_OPEN_REPARSE_POINT,\n                         creationDisposition=FILE_OPEN, fileAttributes=0)\n    from impacket import smb\n    delete_req = smb.SMBSetFileDispositionInfo()\n    delete_req['DeletePending'] = True\n    self.setInfo(treeId, fileId, inputBlob=delete_req, fileInfoClass=SMB2_FILE_DISPOSITION_INFO)\nfinally:\n    if fileId is not None:\n        self.close(treeId, fileId)\n    self.disconnectTree(treeId)\n\nreturn True", "path": "impacket/impacket/smb3.py", "commit_date": "2020-06-09 00:00:00", "repo_name": "fortra/impacket", "stars": 12473, "license": "other", "language": "python", "size": 9110}
{"docstring": "#Separate the Scope ID, if present\n", "func_signal": "def __from_string(self, address):\n", "code": "if self.__is_a_scoped_address(address):\n    split_parts = address.split(self.SCOPE_SEPARATOR)\n    address = split_parts[0]\n    if split_parts[1] == \"\":\n        raise Exception(\"Empty scope ID\")\n    self.__scope_id = split_parts[1]\n\n#Expand address if it's in compressed form\nif self.__is_address_in_compressed_form(address):\n    address = self.__expand_compressed_address(address)\n    \n#Insert leading zeroes where needed        \naddress = self.__insert_leading_zeroes(address)\n\n#Sanity check\nif len(address) != self.ADDRESS_TEXT_SIZE:\n    raise Exception('IP6_Address - from_string - address size != ' + str(self.ADDRESS_TEXT_SIZE))\n    \n#Split address into hex groups\nhex_groups = address.split(self.SEPARATOR)\nif len(hex_groups) != self.TOTAL_HEX_GROUPS:\n    raise Exception('IP6_Address - parsed hex groups != ' + str(self.TOTAL_HEX_GROUPS))\n\n#For each hex group, convert it into integer words\noffset = 0\nfor group in hex_groups:\n    if len(group) != self.HEX_GROUP_SIZE:\n        raise Exception('IP6_Address - parsed hex group length != ' + str(self.HEX_GROUP_SIZE))\n    \n    group_as_int = int(group, 16)\n    self.__bytes[offset]     = (group_as_int & 0xFF00) >> 8\n    self.__bytes[offset + 1] = (group_as_int & 0x00FF) \n    offset += 2", "path": "impacket/impacket/IP6_Address.py", "commit_date": "2018-12-06 00:00:00", "repo_name": "fortra/impacket", "stars": 12473, "license": "other", "language": "python", "size": 9110}
{"docstring": "#The internal representation of an IP6 address is a 16-byte array\n", "func_signal": "def __init__(self, address):\n", "code": "self.__bytes = array.array('B', b'\\0' * self.ADDRESS_BYTE_SIZE)\nself.__scope_id = \"\"\n\n#Invoke a constructor based on the type of the argument\nif isinstance(address, string_types):\n    self.__from_string(address)\nelse:\n    self.__from_bytes(address)", "path": "impacket/impacket/IP6_Address.py", "commit_date": "2018-12-06 00:00:00", "repo_name": "fortra/impacket", "stars": 12473, "license": "other", "language": "python", "size": 9110}
{"docstring": "# ToDo: Handle the custom smb_packet situation\n", "func_signal": "def read_andx(self, tid, fid, offset=0, max_size = None, wait_answer=1, smb_packet=None):\n", "code": "if max_size is None:\n    max_size = self._Connection['MaxReadSize']\nreturn self.read(tid, fid, offset, max_size, wait_answer)", "path": "impacket/impacket/smb3.py", "commit_date": "2020-06-09 00:00:00", "repo_name": "fortra/impacket", "stars": 12473, "license": "other", "language": "python", "size": 9110}
{"docstring": "# IMPORTANT NOTE: As you can see, this was coded as a recursive function\n# Hence, you can exhaust the memory pretty easy ( large bytesToWrite )\n# This function should NOT be used for writing directly to files, but another higher\n# level function should be used that will break the writes into smaller pieces\n\n", "func_signal": "def write(self, treeId, fileId, data, offset = 0, bytesToWrite = 0, waitAnswer = True):\n", "code": "if (treeId in self._Session['TreeConnectTable']) is False:\n    raise SessionError(STATUS_INVALID_PARAMETER)\nif (fileId in self._Session['OpenTable']) is False:\n    raise SessionError(STATUS_INVALID_PARAMETER)\n\npacket = self.SMB_PACKET()\npacket['Command'] = SMB2_WRITE\npacket['TreeID']  = treeId\n\nif self._Connection['MaxWriteSize'] < bytesToWrite:\n    maxBytesToWrite = self._Connection['MaxWriteSize']\nelse:\n    maxBytesToWrite = bytesToWrite\n\nif self._Connection['Dialect'] != SMB2_DIALECT_002 and self._Connection['SupportsMultiCredit'] is True:\n    packet['CreditCharge'] = ( 1 + (maxBytesToWrite - 1) // 65536)\nelse:\n    maxBytesToWrite = min(65536,bytesToWrite)\n\nsmbWrite = SMB2Write()\nsmbWrite['FileID'] = fileId\nsmbWrite['Length'] = maxBytesToWrite\nsmbWrite['Offset'] = offset\nsmbWrite['WriteChannelInfoOffset'] = 0\nsmbWrite['Buffer'] = data[:maxBytesToWrite]\npacket['Data'] = smbWrite\n\npacketID = self.sendSMB(packet)\nif waitAnswer is True:\n    ans = self.recvSMB(packetID)\nelse:\n    return maxBytesToWrite\n\nif ans.isValidAnswer(STATUS_SUCCESS):\n    writeResponse = SMB2Write_Response(ans['Data'])\n    bytesWritten = writeResponse['Count']\n    if bytesWritten < bytesToWrite:\n        bytesWritten += self.write(treeId, fileId, data[bytesWritten:], offset+bytesWritten, bytesToWrite-bytesWritten, waitAnswer)\n    return bytesWritten", "path": "impacket/impacket/smb3.py", "commit_date": "2020-06-09 00:00:00", "repo_name": "fortra/impacket", "stars": 12473, "license": "other", "language": "python", "size": 9110}
{"docstring": "# ToDo: Handle situations where share is password protected\n", "func_signal": "def storeFile(self, shareName, path, callback, mode = FILE_OVERWRITE_IF, offset = 0, password = None, shareAccessMode = FILE_SHARE_WRITE):\n", "code": "path = path.replace('/', '\\\\')\npath = ntpath.normpath(path)\nif len(path) > 0 and path[0] == '\\\\':\n    path = path[1:]\n\ntreeId = self.connectTree(shareName)\nfileId = None\ntry:\n    fileId = self.create(treeId, path, FILE_WRITE_DATA, shareAccessMode, FILE_NON_DIRECTORY_FILE, mode, 0)\n    finished = False\n    writeOffset = offset\n    while not finished:\n        data = callback(self._Connection['MaxWriteSize'])\n        if len(data) == 0:\n            break\n        written = self.write(treeId, fileId, data, writeOffset, len(data))\n        writeOffset += written\nfinally:\n    if fileId is not None:\n        self.close(treeId, fileId)\n    self.disconnectTree(treeId)", "path": "impacket/impacket/smb3.py", "commit_date": "2020-06-09 00:00:00", "repo_name": "fortra/impacket", "stars": 12473, "license": "other", "language": "python", "size": 9110}
{"docstring": "# IMPORTANT NOTE: As you can see, this was coded as a recursive function\n# Hence, you can exhaust the memory pretty easy ( large bytesToRead )\n# This function should NOT be used for reading files directly, but another higher\n# level function should be used that will break the read into smaller pieces\n\n", "func_signal": "def read(self, treeId, fileId, offset = 0, bytesToRead = 0, waitAnswer = True):\n", "code": "if (treeId in self._Session['TreeConnectTable']) is False:\n    raise SessionError(STATUS_INVALID_PARAMETER)\nif (fileId in self._Session['OpenTable']) is False:\n    raise SessionError(STATUS_INVALID_PARAMETER)\n\npacket = self.SMB_PACKET()\npacket['Command'] = SMB2_READ\npacket['TreeID']  = treeId\n\nif self._Connection['MaxReadSize'] < bytesToRead:\n    maxBytesToRead = self._Connection['MaxReadSize']\nelse:\n    maxBytesToRead = bytesToRead\n\nif self._Connection['Dialect'] != SMB2_DIALECT_002 and self._Connection['SupportsMultiCredit'] is True:\n    packet['CreditCharge'] = ( 1 + (maxBytesToRead - 1) // 65536)\nelse:\n    maxBytesToRead = min(65536,bytesToRead)\n\nsmbRead = SMB2Read()\nsmbRead['Padding']  = 0x50\nsmbRead['FileID']   = fileId\nsmbRead['Length']   = maxBytesToRead\nsmbRead['Offset']   = offset\npacket['Data'] = smbRead\n\npacketID = self.sendSMB(packet)\nans = self.recvSMB(packetID)\n\nif ans.isValidAnswer(STATUS_SUCCESS):\n    readResponse = SMB2Read_Response(ans['Data'])\n    retData = readResponse['Buffer']\n    if readResponse['DataRemaining'] > 0:\n        retData += self.read(treeId, fileId, offset+len(retData), readResponse['DataRemaining'], waitAnswer)\n    return retData", "path": "impacket/impacket/smb3.py", "commit_date": "2020-06-09 00:00:00", "repo_name": "fortra/impacket", "stars": 12473, "license": "other", "language": "python", "size": 9110}
{"docstring": "# Do we have pubKeyAuth?\n", "func_signal": "def getData(self):\n", "code": "if 'pubKeyAuth' in self.fields:\n    pubKeyAuth = pack('B',0xa3)\n    pubKeyAuth += asn1encode(pack('B', ASN1_OCTET_STRING) +\n                  asn1encode(self['pubKeyAuth']))\nelse:\n    pubKeyAuth = b''\n\nif 'authInfo' in self.fields:\n    authInfo = pack('B',0xa2)\n    authInfo+= asn1encode(pack('B', ASN1_OCTET_STRING) +\n                  asn1encode(self['authInfo']))\nelse: \n    authInfo = b''\n\nif 'NegoData' in self.fields:\n    negoData = pack('B',0xa1) \n    negoData += asn1encode(pack('B', ASN1_SEQUENCE) +\n               asn1encode(pack('B', ASN1_SEQUENCE) + \n               asn1encode(pack('B', 0xa0) + \n               asn1encode(pack('B', ASN1_OCTET_STRING) + \n               asn1encode(self['NegoData'])))))\nelse:\n    negoData = b''\nans = pack('B', ASN1_SEQUENCE)\nans += asn1encode(pack('B',0xa0) + \n       asn1encode(pack('B',0x02) + asn1encode(pack('B',0x02))) +\n       negoData + authInfo + pubKeyAuth)\n\nreturn ans", "path": "impacket/examples/rdp_check.py", "commit_date": "2020-09-08 00:00:00", "repo_name": "fortra/impacket", "stars": 12473, "license": "other", "language": "python", "size": 9110}
{"docstring": "# CONN/A1\n", "func_signal": "def test_2(self):\n", "code": "resp = b'\\x05\\x00\\x14\\x03\\x10\\x00\\x00\\x00\\x4c\\x00\\x00\\x00\\x00\\x00' + \\\n       b'\\x00\\x00\\x00\\x00\\x04\\x00\\x06\\x00\\x00\\x00\\x01\\x00\\x00\\x00' + \\\n       b'\\x03\\x00\\x00\\x00\\xb0\\xf6\\xaf\\x3d\\x77\\x62\\x98\\x07\\x9b\\x21' + \\\n       b'\\x54\\x6e\\xec\\xf4\\x22\\x53\\x03\\x00\\x00\\x00\\x3a\\x24\\x7a\\x37' + \\\n       b'\\x6d\\xc1\\xed\\x2c\\x68\\x5d\\x34\\x35\\x13\\x46\\x43\\x25\\x00\\x00' + \\\n       b'\\x00\\x00\\x00\\x00\\x04\\x00'\n\npacket = rpch.RTSHeader(resp)\npacket.dump()\n\npduData = packet['pduData']\nnumberOfCommands = packet['NumberOfCommands']\n\nself.assertTrue(numberOfCommands == 4)\nself.assertTrue(packet['Flags'] == rpch.RTS_FLAG_NONE)\nself.assertTrue(packet['frag_len'] == 76)\nself.assertTrue(len(pduData) == 56)\n\nserver_cmds = []\nwhile numberOfCommands > 0:\n    numberOfCommands -= 1\n\n    cmd_type = unpack('<L', pduData[:4])[0]\n    cmd = rpch.COMMANDS[cmd_type](pduData)\n    server_cmds.append(cmd)\n    pduData = pduData[len(cmd):]\n\nfor cmd in server_cmds:\n    cmd.dump()\n\nself.assertTrue(server_cmds[0].getData() == rpch.Version().getData())\nreceiveWindowSize = rpch.ReceiveWindowSize()\nreceiveWindowSize['ReceiveWindowSize'] = 262144\n\nself.assertTrue(server_cmds[3].getData() == receiveWindowSize.getData())\n\ncookie = rpch.Cookie()\ncookie['Cookie'] = b'\\xb0\\xf6\\xaf=wb\\x98\\x07\\x9b!Tn\\xec\\xf4\"S'\n\nself.assertTrue(server_cmds[1].getData() == cookie.getData())", "path": "impacket/tests/SMB_RPC/test_rpch.py", "commit_date": "2020-08-19 00:00:00", "repo_name": "fortra/impacket", "stars": 12473, "license": "other", "language": "python", "size": 9110}
{"docstring": "# The idea here is to receive multiple/single commands and create a compound request, and send it\n# Should return the MessageID for later retrieval. Implement compounded related requests.\n\n# If Connection.Dialect is equal to \"3.000\" and if Connection.SupportsMultiChannel or\n# Connection.SupportsPersistentHandles is TRUE, the client MUST set ChannelSequence in the\n# SMB2 header to Session.ChannelSequence\n\n# Check this is not a CANCEL request. If so, don't consume sequence numbers\n", "func_signal": "def sendSMB(self, packet):\n", "code": "if packet['Command'] is not SMB2_CANCEL:\n    packet['MessageID'] = self._Connection['SequenceWindow']\n    self._Connection['SequenceWindow'] += 1\npacket['SessionID'] = self._Session['SessionID']\n\n# Default the credit charge to 1 unless set by the caller\nif ('CreditCharge' in packet.fields) is False:\n    packet['CreditCharge'] = 1\n\n# Standard credit request after negotiating protocol\nif self._Connection['SequenceWindow'] > 3:\n    packet['CreditRequestResponse'] = 127\n\nmessageId = packet['MessageID']\n\nif self._Session['SigningActivated'] is True and self._Connection['SequenceWindow'] > 2:\n    if packet['TreeID'] > 0 and (packet['TreeID'] in self._Session['TreeConnectTable']) is True:\n        if self._Session['TreeConnectTable'][packet['TreeID']]['EncryptData'] is False:\n            packet['Flags'] = SMB2_FLAGS_SIGNED\n            self.signSMB(packet)\n    elif packet['TreeID'] == 0:\n        packet['Flags'] = SMB2_FLAGS_SIGNED\n        self.signSMB(packet)\n\nif packet['Command'] is SMB2_NEGOTIATE:\n    data = packet.getData()\n    self.__UpdateConnectionPreAuthHash(data)\n    self._Session['CalculatePreAuthHash'] = False\n\nif packet['Command'] is SMB2_SESSION_SETUP:\n    self._Session['CalculatePreAuthHash'] = True\n\nif (self._Session['SessionFlags'] & SMB2_SESSION_FLAG_ENCRYPT_DATA) or ( packet['TreeID'] != 0 and self._Session['TreeConnectTable'][packet['TreeID']]['EncryptData'] is True):\n    plainText = packet.getData()\n    transformHeader = SMB2_TRANSFORM_HEADER()\n    transformHeader['Nonce'] = ''.join([rand.choice(string.ascii_letters) for _ in range(11)])\n    transformHeader['OriginalMessageSize'] = len(plainText)\n    transformHeader['EncryptionAlgorithm'] = SMB2_ENCRYPTION_AES128_CCM\n    transformHeader['SessionID'] = self._Session['SessionID']\n    cipher = AES.new(self._Session['EncryptionKey'], AES.MODE_CCM,  b(transformHeader['Nonce']))\n    cipher.update(transformHeader.getData()[20:])\n    cipherText = cipher.encrypt(plainText)\n    transformHeader['Signature'] = cipher.digest()\n    packet = transformHeader.getData() + cipherText\n\n    self._NetBIOSSession.send_packet(packet)\nelse:\n    data = packet.getData()\n    if self._Session['CalculatePreAuthHash'] is True:\n        self.__UpdatePreAuthHash(data)\n\n    self._NetBIOSSession.send_packet(data)\n\nreturn messageId", "path": "impacket/impacket/smb3.py", "commit_date": "2020-06-09 00:00:00", "repo_name": "fortra/impacket", "stars": 12473, "license": "other", "language": "python", "size": 9110}
{"docstring": "# FlowControlAckWithDestination\n", "func_signal": "def test_6(self):\n", "code": "resp = b'\\x05\\x00\\x14\\x03\\x10\\x00\\x00\\x00\\x38\\x00\\x00\\x00\\x00\\x00' + \\\n       b'\\x00\\x00\\x02\\x00\\x02\\x00\\x0d\\x00\\x00\\x00\\x00\\x00\\x00\\x00' + \\\n       b'\\x01\\x00\\x00\\x00\\x92\\x80\\x00\\x00\\x00\\x00\\x01\\x00\\xe3\\x79' + \\\n       b'\\x6e\\x7c\\xbc\\x68\\xa9\\x4d\\xab\\x8d\\x82\\x40\\xa0\\x05\\x72\\x32'\n\npacket = rpch.RTSHeader(resp)\npacket.dump()\n\npduData = packet['pduData']\nnumberOfCommands = packet['NumberOfCommands']\n\nserver_cmds = []\nwhile numberOfCommands > 0:\n    numberOfCommands -= 1\n\n    cmd_type = unpack('<L', pduData[:4])[0]\n    cmd = rpch.COMMANDS[cmd_type](pduData)\n    server_cmds.append(cmd)\n    pduData = pduData[len(cmd):]\n\nfor cmd in server_cmds:\n    cmd.dump()\n\nself.assertTrue(packet['Flags'] == rpch.RTS_FLAG_OTHER_CMD)\n\nack = rpch.Ack()\nack['BytesReceived'] = 32914\nack['AvailableWindow'] = 65536\nack['ChannelCookie'] = rpch.RTSCookie()\nack['ChannelCookie']['Cookie'] = b'\\xe3yn|\\xbch\\xa9M\\xab\\x8d\\x82@\\xa0\\x05r2'\n\nself.assertTrue(server_cmds[1]['Ack'].getData() == ack.getData())", "path": "impacket/tests/SMB_RPC/test_rpch.py", "commit_date": "2020-08-19 00:00:00", "repo_name": "fortra/impacket", "stars": 12473, "license": "other", "language": "python", "size": 9110}
{"docstring": "# First, verify we don't have the packet already\n", "func_signal": "def recvSMB(self, packetID = None):\n", "code": "if packetID in self._Connection['OutstandingResponses']:\n    return self._Connection['OutstandingResponses'].pop(packetID)\n\ndata = self._NetBIOSSession.recv_packet(self._timeout)\n\nif data.get_trailer().startswith(b'\\xfdSMB'):\n    # Packet is encrypted\n    transformHeader = SMB2_TRANSFORM_HEADER(data.get_trailer())\n    cipher = AES.new(self._Session['DecryptionKey'], AES.MODE_CCM,  transformHeader['Nonce'][:11])\n    cipher.update(transformHeader.getData()[20:])\n    plainText = cipher.decrypt(data.get_trailer()[len(SMB2_TRANSFORM_HEADER()):])\n    #cipher.verify(transformHeader['Signature'])\n    packet = SMB2Packet(plainText)\nelse:\n    # In all SMB dialects for a response this field is interpreted as the Status field.\n    # This field can be set to any value. For a list of valid status codes,\n    # see [MS-ERREF] section 2.3.\n    packet = SMB2Packet(data.get_trailer())\n\n# Loop while we receive pending requests\nif packet['Status'] == STATUS_PENDING:\n    status = STATUS_PENDING\n    while status == STATUS_PENDING:\n        data = self._NetBIOSSession.recv_packet(self._timeout)\n        if data.get_trailer().startswith(b'\\xfeSMB'):\n            packet = SMB2Packet(data.get_trailer())\n        else:\n            # Packet is encrypted\n            transformHeader = SMB2_TRANSFORM_HEADER(data.get_trailer())\n            cipher = AES.new(self._Session['DecryptionKey'], AES.MODE_CCM,  transformHeader['Nonce'][:11])\n            cipher.update(transformHeader.getData()[20:])\n            plainText = cipher.decrypt(data.get_trailer()[len(SMB2_TRANSFORM_HEADER()):])\n            #cipher.verify(transformHeader['Signature'])\n            packet = SMB2Packet(plainText)\n        status = packet['Status']\n\nif packet['MessageID'] == packetID or packetID is None:\n    # Let's update the sequenceWindow based on the CreditsCharged\n    # In the SMB 2.0.2 dialect, this field MUST NOT be used and MUST be reserved.\n    # The sender MUST set this to 0, and the receiver MUST ignore it.\n    # In all other dialects, this field indicates the number of credits that this request consumes.\n    if self._Connection['Dialect'] > SMB2_DIALECT_002:\n        self._Connection['SequenceWindow'] += (packet['CreditCharge'] - 1)\n    return packet\nelse:\n    self._Connection['OutstandingResponses'][packet['MessageID']] = packet\n    return self.recvSMB(packetID)", "path": "impacket/impacket/smb3.py", "commit_date": "2020-06-09 00:00:00", "repo_name": "fortra/impacket", "stars": 12473, "license": "other", "language": "python", "size": 9110}
{"docstring": "# 802.11 Management Frame \n#\n", "func_signal": "def setUp(self):\n", "code": "self.rawProbeResponseframe=b'\\x00\\x00\\x18\\x00\\x2e\\x48\\x00\\x00\\x00\\x02\\x85\\x09\\xa0\\x00\\xb0\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x50\\x00\\x3a\\x01\\x00\\x21\\xfe\\x39\\x3f\\x77\\x00\\x1b\\x11\\x32\\x66\\x23\\x00\\x1b\\x11\\x32\\x66\\x23\\x20\\x73\\x7f\\xa0\\x22\\xf8\\x3f\\x01\\x00\\x00\\x64\\x00\\x11\\x04\\x00\\x07\\x66\\x72\\x65\\x65\\x62\\x73\\x64\\x01\\x08\\x82\\x84\\x8b\\x96\\x24\\x30\\x48\\x6c\\x03\\x01\\x06\\x2a\\x01\\x04\\x2f\\x01\\x04\\x32\\x04\\x0c\\x12\\x18\\x60\\xdd\\x75\\x00\\x50\\xf2\\x04\\x10\\x4a\\x00\\x01\\x10\\x10\\x44\\x00\\x01\\x02\\x10\\x41\\x00\\x01\\x00\\x10\\x3b\\x00\\x01\\x03\\x10\\x47\\x00\\x10\\x11\\x4e\\xf7\\x46\\xa9\\xc6\\xfb\\x1d\\x70\\x1b\\x00\\x1b\\x11\\x32\\x66\\x23\\x10\\x21\\x00\\x06\\x44\\x2d\\x4c\\x69\\x6e\\x6b\\x10\\x23\\x00\\x07\\x44\\x49\\x52\\x2d\\x33\\x32\\x30\\x10\\x24\\x00\\x07\\x44\\x49\\x52\\x2d\\x33\\x32\\x30\\x10\\x42\\x00\\x08\\x30\\x30\\x30\\x30\\x30\\x30\\x30\\x30\\x10\\x54\\x00\\x08\\x00\\x06\\x00\\x50\\xf2\\x04\\x00\\x01\\x10\\x11\\x00\\x07\\x44\\x49\\x52\\x2d\\x33\\x32\\x30\\x10\\x08\\x00\\x02\\x00\\x8e\\xdd\\x05\\x00\\x50\\xf2\\x05\\x00\\xdd\\x09\\x00\\x10\\x18\\x02\\x01\\xf0\\x00\\x00\\x00\\xdd\\x18\\x00\\x50\\xf2\\x01\\x01\\x00\\x00\\x50\\xf2\\x02\\x01\\x00\\x00\\x50\\xf2\\x02\\x01\\x00\\x00\\x50\\xf2\\x02\\x00\\x00'\nself.radiotap_decoder = RadioTapDecoder()\nradiotap=self.radiotap_decoder.decode(self.rawProbeResponseframe)\n\nif PY2:\n    self.assertEqual(str(radiotap.__class__), \"impacket.dot11.RadioTap\")\nelse:\n    self.assertEqual(str(radiotap.__class__), \"<class 'impacket.dot11.RadioTap'>\")\n\nself.dot11=radiotap.child()\nif PY2:\n    self.assertEqual(str(self.dot11.__class__), \"impacket.dot11.Dot11\")\nelse:\n    self.assertEqual(str(self.dot11.__class__), \"<class 'impacket.dot11.Dot11'>\")\n\ntype = self.dot11.get_type()\nself.assertEqual(type,Dot11Types.DOT11_TYPE_MANAGEMENT)\n\nsubtype = self.dot11.get_subtype()\nself.assertEqual(subtype,Dot11Types.DOT11_SUBTYPE_MANAGEMENT_PROBE_RESPONSE)\n\ntypesubtype = self.dot11.get_type_n_subtype()\nself.assertEqual(typesubtype,Dot11Types.DOT11_TYPE_MANAGEMENT_SUBTYPE_PROBE_RESPONSE)\n\nself.management_base=self.dot11.child()\nif PY2:\n    self.assertEqual(str(self.management_base.__class__), \"impacket.dot11.Dot11ManagementFrame\")\nelse:\n    self.assertEqual(str(self.management_base.__class__), \"<class 'impacket.dot11.Dot11ManagementFrame'>\")\n\nself.management_probe_response=self.management_base.child()\nif PY2:\n    self.assertEqual(str(self.management_probe_response.__class__), \"impacket.dot11.Dot11ManagementProbeResponse\")\nelse:\n    self.assertEqual(str(self.management_probe_response.__class__), \"<class 'impacket.dot11.Dot11ManagementProbeResponse'>\")", "path": "impacket/tests/dot11/test_FrameManagementProbeResponse.py", "commit_date": "2018-12-04 00:00:00", "repo_name": "fortra/impacket", "stars": 12473, "license": "other", "language": "python", "size": 9110}
{"docstring": "# CONN/B2, IPv4\n", "func_signal": "def test_7(self):\n", "code": "resp = b'\\x05\\x00\\x14\\x03\\x10\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x00\\x00' + \\\n       b'\\x00\\x00\\x08\\x00\\x07\\x00\\x06\\x00\\x00\\x00\\x01\\x00\\x00\\x00' + \\\n       b'\\x03\\x00\\x00\\x00\\x61\\xec\\x8b\\xb3\\x40\\x28\\xa8\\x46\\xba\\xfd' + \\\n       b'\\x90\\xcf\\x6d\\x31\\xdc\\x29\\x03\\x00\\x00\\x00\\x20\\xce\\x94\\x22' + \\\n       b'\\x30\\x83\\x1b\\x45\\x94\\xea\\x0d\\x7e\\x05\\xd2\\xa8\\x5a\\x00\\x00' + \\\n       b'\\x00\\x00\\x00\\x00\\x01\\x00\\x02\\x00\\x00\\x00\\xc0\\xd4\\x01\\x00' + \\\n       b'\\x0c\\x00\\x00\\x00\\xdf\\x28\\xb4\\x20\\x77\\xa4\\x70\\x42\\xb1\\xd1' + \\\n       b'\\x4a\\x03\\x49\\x5f\\x6b\\x7b\\x0b\\x00\\x00\\x00\\x00\\x00\\x00\\x00' + \\\n       b'\\x00\\x00\\x00\\x00\\xc0\\xa8\\x02\\xfe\\x00\\x00\\x00\\x00\\x00\\x00' + \\\n       b'\\x00\\x00'\n\npacket = rpch.RTSHeader(resp)\npacket.dump()\n\npduData = packet['pduData']\nnumberOfCommands = packet['NumberOfCommands']\n\nself.assertTrue(packet['Flags'] == rpch.RTS_FLAG_IN_CHANNEL)\n\nserver_cmds = []\nwhile numberOfCommands > 0:\n    numberOfCommands -= 1\n\n    cmd_type = unpack('<L', pduData[:4])[0]\n    cmd = rpch.COMMANDS[cmd_type](pduData)\n    server_cmds.append(cmd)\n    pduData = pduData[len(cmd):]\n\nfor cmd in server_cmds:\n    cmd.dump()\n\n# TODO: Check ClientAddress. Why is it in the padding?!", "path": "impacket/tests/SMB_RPC/test_rpch.py", "commit_date": "2020-08-19 00:00:00", "repo_name": "fortra/impacket", "stars": 12473, "license": "other", "language": "python", "size": 9110}
{"docstring": "'''\nReturns\n  str(option_string * DropDown Value)\n  e.g.\n  -vvvvv\n'''\n", "func_signal": "def counter(metatdata, value):\n", "code": "if not str(value).isdigit():\n    return None\ncommand = str(metatdata['commands'][0]).strip()\nreturn ' '.join(itertools.repeat(command, int(value)))", "path": "Gooey/gooey/gui/formatters.py", "commit_date": "2019-09-22 00:00:00", "repo_name": "chriskiehl/Gooey", "stars": 20214, "license": "mit", "language": "python", "size": 4269}
{"docstring": "\"\"\"\nTesting that select/deselect behaves as expected\n\"\"\"\n", "func_signal": "def test_optional_radiogroup_click_behavior(self):\n", "code": "testcases = [\n    self.click_scenarios_optional_widget(),\n    self.click_scenarios_required_widget(),\n    self.click_scenarios_initial_selection()\n]\n\nfor testcase in testcases:\n    with self.subTest(testcase['name']):\n        # wire up the parse with our test case options\n        parser = self.mutext_group(testcase['input'])\n\n        with instrumentGooey(parser) as (app, gooeyApp):\n            radioGroup = gooeyApp.configs[0].reifiedWidgets[0]\n\n            for scenario in testcase['scenario']:\n                targetButton = scenario['clickButton']\n\n                event = wx.CommandEvent(wx.wxEVT_LEFT_DOWN, wx.Window.NewControlId())\n                event.SetEventObject(radioGroup.radioButtons[targetButton])\n\n                radioGroup.radioButtons[targetButton].ProcessEvent(event)\n\n                expectedEnabled, expectedDisabled = scenario['postState']\n\n                for index in expectedEnabled:\n                    self.assertEqual(radioGroup.selected, radioGroup.radioButtons[index])\n                    self.assertTrue(radioGroup.widgets[index].IsEnabled())\n\n                for index in expectedDisabled:\n                    self.assertNotEqual(radioGroup.selected, radioGroup.radioButtons[index])\n                    self.assertFalse(radioGroup.widgets[index].IsEnabled())", "path": "Gooey/gooey/tests/test_radiogroup.py", "commit_date": "2020-11-08 00:00:00", "repo_name": "chriskiehl/Gooey", "stars": 20214, "license": "mit", "language": "python", "size": 4269}
{"docstring": "\"\"\"\nSends a gooey-seed-ui request to the client program it retrieve\ndynamically generated defaults with which to seed the UI\n\"\"\"\n# TODO: this needs to apply the same argpase_to_json data cleaning rules\n", "func_signal": "def fetchDynamicProperties(target, encoding):\n", "code": "cmd = '{} {}'.format(target, 'gooey-seed-ui --ignore-gooey')\nproc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\nif proc.returncode != 0:\n    out, _ = proc.communicate()\n    return json.loads(out.decode(encoding))\nelse:\n    # TODO: useful feedback\n    return {}", "path": "Gooey/gooey/gui/seeder.py", "commit_date": "2020-11-08 00:00:00", "repo_name": "chriskiehl/Gooey", "stars": 20214, "license": "mit", "language": "python", "size": 4269}
{"docstring": "\"\"\"Very basic field validation / sanity checking for\nthe time being.\n\nFuture plans are to assert against the options and actions together\nto facilitate checking that certain options like `initial_selection` in\nRadioGroups map to a value which actually exists (rather than exploding\nat runtime with an unhelpful error)\n\nAdditional problems with the current approach is that no feedback is given\nas to WHERE the issue took place (in terms of stacktrace). Which means we should\nprobably explode in GooeyParser proper rather than trying to collect all the errors here.\nIt's not super ideal in that the user will need to run multiple times to\nsee all the issues, but, ultimately probably less annoying that trying to\ndebug which `gooey_option` key had an issue in a large program.\n\nThat said \"better is the enemy of done.\" This is good enough for now. It'll be\na TODO: better validation \n\"\"\"\n", "func_signal": "def validate_gooey_options(action, widget, options):\n", "code": "errors = collect_errors(validators, options)\nif errors:\n    from pprint import pformat\n    raise ValueError(str(action.dest) + str(pformat(errors)))", "path": "Gooey/gooey/python_bindings/argparse_to_json.py", "commit_date": "2020-12-20 00:00:00", "repo_name": "chriskiehl/Gooey", "stars": 20214, "license": "mit", "language": "python", "size": 4269}
{"docstring": "\"\"\"\nOriginally the formatter was dropping '0' due to\nit being interpreted as falsey\n\"\"\"\n", "func_signal": "def testZerosAreReturned(self):\n", "code": "parser = self.makeParser(widget='IntegerField')\nwith instrumentGooey(parser) as (app, gooeyApp):\n    field = gooeyApp.configs[0].reifiedWidgets[0]\n    result = field.getValue()\n    self.assertEqual(result['rawValue'], 0)\n    self.assertIsNotNone(result['cmd'])", "path": "Gooey/gooey/tests/test_numeric_inputs.py", "commit_date": "2020-12-20 00:00:00", "repo_name": "chriskiehl/Gooey", "stars": 20214, "license": "mit", "language": "python", "size": 4269}
{"docstring": "\"\"\"\nTesting that the up/down arrow keys spawn the dropdown\nand cycle through its options wrapping around as needed.\n\"\"\"\n", "func_signal": "def test_arrow_key_selection_cycling(self):\n", "code": "Scenario = namedtuple('Scenario', [\n    'key', 'expectVisible', 'expectedSelection', 'expectedDisplayValue'])\n\nchoices = ['alpha', 'beta']\n# no text entered yet\ninitial = Scenario(None, False, -1, '')\nscenarios = [\n    # cycling down\n    [\n    Scenario(wx.WXK_DOWN, True, -1, ''),\n    Scenario(wx.WXK_DOWN, True, 0, 'alpha'),\n    Scenario(wx.WXK_DOWN, True, 1, 'beta'),\n    # wraps around to top\n    Scenario(wx.WXK_DOWN, True, 0, 'alpha')\n],  # cycling up\n    [\n    Scenario(wx.WXK_UP, True, -1, ''),\n    Scenario(wx.WXK_UP, True, 1, 'beta'),\n    Scenario(wx.WXK_UP, True, 0, 'alpha'),\n    # wraps around to top\n    Scenario(wx.WXK_UP, True, 1, 'beta'),\n]]\n\nfor actions in scenarios:\n    parser = self.make_parser(choices=choices)\n    with instrumentGooey(parser) as (app, gooeyApp):\n        dropdown = gooeyApp.configs[0].reifiedWidgets[0]\n        # sanity check we're starting from our known initial state\n        self.assertEqual(dropdown.model.suggestionsVisible, initial.expectVisible)\n        self.assertEqual(dropdown.model.displayValue, initial.expectedDisplayValue)\n        self.assertEqual(dropdown.model.selectedSuggestion, initial.expectedSelection)\n\n        for action in actions:\n            self.pressButton(dropdown, action.key)\n            self.assertEqual(\n                dropdown.model.suggestionsVisible,\n                dropdown.listbox.IsShown()\n            )\n            self.assertEqual(\n                dropdown.model.displayValue,\n                action.expectedDisplayValue\n            )\n            self.assertEqual(\n                dropdown.model.selectedSuggestion,\n                action.expectedSelection\n            )", "path": "Gooey/gooey/tests/test_filterable_dropdown.py", "commit_date": "2020-11-15 00:00:00", "repo_name": "chriskiehl/Gooey", "stars": 20214, "license": "mit", "language": "python", "size": 4269}
{"docstring": "\"\"\"\n_clean should drop any keys with None values\nand flatten the layout_option kwargs to the root level\n\"\"\"\n", "func_signal": "def test_clean_method(self):\n", "code": "result = options._clean({'a': None, 'b': 123, 'c': 0})\nself.assertEqual(result, {'b': 123, 'c': 0})\n\nresult = options._clean({'root_level': 123, 'layout_options': {\n    'nested': 'hello',\n    'another': 1234\n}})\nself.assertEqual(result, {'root_level': 123, 'nested': 'hello', 'another': 1234})", "path": "Gooey/gooey/tests/test_options.py", "commit_date": "2020-11-29 00:00:00", "repo_name": "chriskiehl/Gooey", "stars": 20214, "license": "mit", "language": "python", "size": 4269}
{"docstring": "\"\"\"\nMerges a set of group defaults with incoming options.\n\nA bunch of ceremony here is to ensure backwards compatibility with the old\nnum_required_cols and num_optional_cols decorator args. They are used as\nthe seed values for the new group defaults which keeps the old behavior\n_mostly_ in tact.\n\nKnown failure points:\n    * Using custom groups / names. No 'positional arguments' group\n      means no required_cols arg being honored\n    * Non-positional args marked as required. It would take group\n      shuffling along the lines of that required to make\n      mutually exclusive groups show in the correct place. In short, not\n      worth the complexity for a legacy feature that's been succeeded by\n      a much more powerful alternative.\n\"\"\"\n", "func_signal": "def handle_option_merge(group_defaults, incoming_options, title):\n", "code": "if title == 'positional arguments':\n    # the argparse default 'required' bucket\n    req_cols = getin(group_defaults, ['legacy', 'required_cols'], 2)\n    new_defaults = assoc(group_defaults, 'columns', req_cols)\n    return merge(new_defaults, incoming_options)\nelse:\n    opt_cols = getin(group_defaults, ['legacy', 'optional_cols'], 2)\n    new_defaults = assoc(group_defaults, 'columns', opt_cols)\n    return merge(new_defaults, incoming_options)", "path": "Gooey/gooey/python_bindings/argparse_to_json.py", "commit_date": "2020-12-20 00:00:00", "repo_name": "chriskiehl/Gooey", "stars": 20214, "license": "mit", "language": "python", "size": 4269}
{"docstring": "'''\n_actions possessing the `required` flag and not implicitly optional\nthrough `nargs` being '*' or '?'\n'''\n", "func_signal": "def is_required(action):\n", "code": "return not isinstance(action, _SubParsersAction) and (\naction.required == True and action.nargs not in ['*', '?'])", "path": "Gooey/gooey/python_bindings/argparse_to_json.py", "commit_date": "2020-12-20 00:00:00", "repo_name": "chriskiehl/Gooey", "stars": 20214, "license": "mit", "language": "python", "size": 4269}
{"docstring": "\"\"\"\nGenerate a powerset of all possible combinations of\nthe header parameters (empty, some present, all present, all combos)\n\"\"\"\n", "func_signal": "def testcases(self):\n", "code": "iterable = product(['show_time_remaining', 'hide_time_remaining_on_complete'], [True, False])\nallCombinations = list(powerset(iterable))\nreturn [{k: v for k,v in args}\n        for args in allCombinations]", "path": "Gooey/gooey/tests/test_time_remaining.py", "commit_date": "2020-11-08 00:00:00", "repo_name": "chriskiehl/Gooey", "stars": 20214, "license": "mit", "language": "python", "size": 4269}
{"docstring": "''' Iterate over name, parser pairs '''\n", "func_signal": "def iter_parsers(parser):\n", "code": "try:\n    return get_subparser(parser._actions).choices.items()\nexcept:\n    return iter([('::gooey/default', parser)])", "path": "Gooey/gooey/python_bindings/argparse_to_json.py", "commit_date": "2020-12-20 00:00:00", "repo_name": "chriskiehl/Gooey", "stars": 20214, "license": "mit", "language": "python", "size": 4269}
{"docstring": "'''\nRecursively extract argument groups and associated actions\nfrom ParserGroup objects\n'''\n", "func_signal": "def extract_groups(action_group, group_defaults):\n", "code": "return {\n    'name': action_group.title,\n    'description': action_group.description,\n    'items': [action for action in action_group._group_actions\n              if not is_help_message(action)],\n    'groups': [extract_groups(group, group_defaults)\n               for group in action_group._action_groups],\n    'options': handle_option_merge(\n        group_defaults,\n        getattr(action_group, 'gooey_options', {}),\n        action_group.title)\n}", "path": "Gooey/gooey/python_bindings/argparse_to_json.py", "commit_date": "2020-12-20 00:00:00", "repo_name": "chriskiehl/Gooey", "stars": 20214, "license": "mit", "language": "python", "size": 4269}
{"docstring": "\"\"\"\nMore sanity checking that the internal use of locals()\ndoes the Right Thing\n\"\"\"\n", "func_signal": "def test_only_provided_arguments_included(self):\n", "code": "option = options.LayoutOptions(label_color='#ffffff')\nself.assertIn('label_color', option)\n\noption = options.LayoutOptions()\nself.assertNotIn('label_color', option)\n\noption = options.TextField(label_color='#ffffff')\nself.assertIn('label_color', option)\n\noption = options.TextField()\nself.assertNotIn('label_color', option)", "path": "Gooey/gooey/tests/test_options.py", "commit_date": "2020-11-29 00:00:00", "repo_name": "chriskiehl/Gooey", "stars": 20214, "license": "mit", "language": "python", "size": 4269}
{"docstring": "\"\"\"coerce a default value to the best appropriate type for\ningestion into wx\"\"\"\n", "func_signal": "def coerce_default(default, widget):\n", "code": "dispatcher = {\n    'Listbox': clean_list_defaults,\n    'Dropdown': safe_string,\n    'Counter': safe_string\n}\n# Issue #321:\n# Defaults for choice types must be coerced to strings\n# to be able to match the stringified `choices` used by `wx.ComboBox`\ncleaned = clean_default(default)\n\n# dispatch to the appropriate cleaning function, or return the value\n# as is if no special handler is present\nreturn dispatcher.get(widget, identity)(cleaned)", "path": "Gooey/gooey/python_bindings/argparse_to_json.py", "commit_date": "2020-12-20 00:00:00", "repo_name": "chriskiehl/Gooey", "stars": 20214, "license": "mit", "language": "python", "size": 4269}
{"docstring": "\"\"\"\nVanilla TextInputs which have nargs options which produce lists (i.e.\nnargs +, *, N, or REMAINDER) need to have their default values transformed\ninto CLI style space-separated entries when they're supplied as a list of values\non the Python side.\n\"\"\"\n", "func_signal": "def textinput_with_nargs_and_list_default(action, widget):\n", "code": "return (\n    widget in {'TextField', 'Textarea', 'PasswordField'}\n    and (isinstance(action.default, list) or isinstance(action.default, tuple))\n    and is_list_based_nargs(action))", "path": "Gooey/gooey/python_bindings/argparse_to_json.py", "commit_date": "2020-12-20 00:00:00", "repo_name": "chriskiehl/Gooey", "stars": 20214, "license": "mit", "language": "python", "size": 4269}
{"docstring": "# argparse stores mutually exclusive groups independently\n# of all other groups. So, they must be manually re-combined\n# with the groups/subgroups to which they were originally declared\n# in order to have them appear in the correct location in the UI.\n#\n# Order is attempted to be preserved by inserting the MutexGroup\n# into the _actions list at the first occurrence of any item\n# where the two groups intersect\n", "func_signal": "def reapply_mutex_groups(mutex_groups, action_groups):\n", "code": "def swap_actions(actions):\n    for mutexgroup in mutex_groups:\n        mutex_actions = mutexgroup._group_actions\n        if contains_actions(mutex_actions, actions):\n            # make a best guess as to where we should store the group\n            targetindex = actions.index(mutexgroup._group_actions[0])\n            # insert the _ArgumentGroup container\n            actions[targetindex] = mutexgroup\n            # remove the duplicated individual actions\n            actions = [action for action in actions\n                    if action not in mutex_actions]\n    return actions\n\nreturn [group.update({'items': swap_actions(group['items'])}) or group\n        for group in action_groups]", "path": "Gooey/gooey/python_bindings/argparse_to_json.py", "commit_date": "2020-12-20 00:00:00", "repo_name": "chriskiehl/Gooey", "stars": 20214, "license": "mit", "language": "python", "size": 4269}
{"docstring": "\"\"\"\nAttempts to safely coalesce the default value down to\na valid JSON type.\n\"\"\"\n", "func_signal": "def clean_default(default):\n", "code": "try:\n    json.dumps(default)\n    return default\nexcept TypeError as e:\n    # see: Issue #377\n    # if is ins't json serializable (i.e. primitive data) there's nothing\n    # useful for Gooey to do with it (since Gooey deals in primitive data\n    # types). So the correct behavior is dropping them. This affects ONLY\n    # gooey, not the client code.\n    return None", "path": "Gooey/gooey/python_bindings/argparse_to_json.py", "commit_date": "2020-12-20 00:00:00", "repo_name": "chriskiehl/Gooey", "stars": 20214, "license": "mit", "language": "python", "size": 4269}
{"docstring": "\"\"\"Sanity check that my docstring wrappers all behave as expected\"\"\"\n", "func_signal": "def test_doc_schenanigans(self):\n", "code": "@options._include_layout_docs\ndef no_self_docstring():\n    pass\n\n@options._include_layout_docs\ndef yes_self_docstring():\n    \"\"\"sup\"\"\"\n    pass\n\n# gets attached to functions even if they don't have a docstring\nself.assertIn(options.LayoutOptions.__doc__, no_self_docstring.__doc__)\n# gets attached to the *end* of existing doc strings\nself.assertTrue(yes_self_docstring.__doc__.startswith('sup'))\nself.assertIn(options.LayoutOptions.__doc__, yes_self_docstring.__doc__)", "path": "Gooey/gooey/tests/test_options.py", "commit_date": "2020-11-29 00:00:00", "repo_name": "chriskiehl/Gooey", "stars": 20214, "license": "mit", "language": "python", "size": 4269}
{"docstring": "\"\"\"\nBasic radio group consisting of two options.\n\"\"\"\n", "func_signal": "def mutext_group(self, options):\n", "code": "parser = GooeyParser()\ngroup = parser.add_mutually_exclusive_group(**options)\ngroup.add_argument(\"-b\", type=str)\ngroup.add_argument(\"-d\", type=str, widget=\"DateChooser\")\nreturn parser", "path": "Gooey/gooey/tests/test_radiogroup.py", "commit_date": "2020-11-08 00:00:00", "repo_name": "chriskiehl/Gooey", "stars": 20214, "license": "mit", "language": "python", "size": 4269}
{"docstring": "\"\"\" actions which are general \"store\" instructions.\ne.g. anything which has an argument style like:\n   $ script.py -f myfilename.txt\n\"\"\"\n", "func_signal": "def is_standard(action):\n", "code": "boolean_actions = (\n    _StoreConstAction, _StoreFalseAction,\n    _StoreTrueAction\n)\nreturn (not action.choices\n        and not isinstance(action.type, argparse.FileType)\n        and not isinstance(action, _CountAction)\n        and not isinstance(action, _HelpAction)\n        and type(action) not in boolean_actions)", "path": "Gooey/gooey/python_bindings/argparse_to_json.py", "commit_date": "2020-12-20 00:00:00", "repo_name": "chriskiehl/Gooey", "stars": 20214, "license": "mit", "language": "python", "size": 4269}
{"docstring": "\"\"\"\nParse a string representing a time span and return the number of seconds.\nValid formats are: 20, 20s, 3m, 2h, 1h20m, 3h30m10s, etc.\n\"\"\"\n", "func_signal": "def parse_timespan(time_str):\n", "code": "if not time_str:\n    raise ValueError(\"Invalid time span format\")\n\nif re.match(r\"^\\d+$\", time_str):\n    # if an int is specified we assume they want seconds\n    return int(time_str)\n\ntimespan_regex = re.compile(r\"((?P<hours>\\d+?)h)?((?P<minutes>\\d+?)m)?((?P<seconds>\\d+?)s)?\")\nparts = timespan_regex.match(time_str)\nif not parts:\n    raise ValueError(\"Invalid time span format. Valid formats: 20, 20s, 3m, 2h, 1h20m, 3h30m10s, etc.\")\nparts = parts.groupdict()\ntime_params = {name: int(value) for name, value in parts.items() if value}\nif not time_params:\n    raise ValueError(\"Invalid time span format. Valid formats: 20, 20s, 3m, 2h, 1h20m, 3h30m10s, etc.\")\nreturn int(timedelta(**time_params).total_seconds())", "path": "locust/locust/util/timespan.py", "commit_date": "2020-08-27 00:00:00", "repo_name": "locustio/locust", "stars": 23293, "license": "mit", "language": "python", "size": 20024}
{"docstring": "\"\"\"\nCheck that RequestStats.latest_total_response_times are pruned when exceeding 20 entries\n\"\"\"\n", "func_signal": "def test_latest_total_response_times_pruned(self):\n", "code": "s = StatsEntry(self.stats, \"/\", \"GET\", use_response_times_cache=True)\nt = int(time.time())\nfor i in reversed(range(2, 30)):\n    s.response_times_cache[t - i] = CachedResponseTimes(response_times={}, num_requests=0)\nself.assertEqual(29, len(s.response_times_cache))\ns.log(17, 1337)\ns.last_request_timestamp -= 1\ns.log(1, 1)\nself.assertEqual(20, len(s.response_times_cache))\nself.assertEqual(\n    CachedResponseTimes(response_times={17: 1}, num_requests=1),\n    s.response_times_cache.popitem(last=True)[1],\n)", "path": "locust/locust/test/test_stats.py", "commit_date": "2020-10-12 00:00:00", "repo_name": "locustio/locust", "stars": 23293, "license": "mit", "language": "python", "size": 20024}
{"docstring": "\"\"\"\nEvent handler that get triggered on every successful request\n\"\"\"\n", "func_signal": "def on_request_success(request_type, name, response_time, response_length):\n", "code": "stats.setdefault(name, {\"content-length\": 0})\nstats[name][\"content-length\"] += response_length", "path": "locust/examples/extend_web_ui/extend.py", "commit_date": "2020-09-29 00:00:00", "repo_name": "locustio/locust", "stars": 23293, "license": "mit", "language": "python", "size": 20024}
{"docstring": "\"\"\"\nReturn a dict containing task execution ratio info\n\"\"\"\n", "func_signal": "def get_task_ratio_dict(tasks, total=False, parent_ratio=1.0):\n", "code": "if hasattr(tasks[0], \"weight\"):\n    divisor = sum(t.weight for t in tasks)\nelse:\n    divisor = len(tasks) / parent_ratio\nratio = {}\nfor task in tasks:\n    ratio.setdefault(task, 0)\n    ratio[task] += task.weight if hasattr(task, \"weight\") else 1\n\n# get percentage\nratio_percent = dict((k, float(v) / divisor) for k, v in ratio.items())\n\ntask_dict = {}\nfor locust, ratio in ratio_percent.items():\n    d = {\"ratio\": ratio}\n    if inspect.isclass(locust):\n        if issubclass(locust, (User, TaskSet)):\n            T = locust.tasks\n        if total:\n            d[\"tasks\"] = get_task_ratio_dict(T, total, ratio)\n        else:\n            d[\"tasks\"] = get_task_ratio_dict(T, total)\n\n    task_dict[locust.__name__] = d\n\nreturn task_dict", "path": "locust/locust/user/inspectuser.py", "commit_date": "2020-08-27 00:00:00", "repo_name": "locustio/locust", "stars": 23293, "license": "mit", "language": "python", "size": 20024}
{"docstring": "\"\"\"\nUsed as a convenience decorator to be able to declare tasks for a User or a TaskSet\ninline in the class. Example::\n\n    class ForumPage(TaskSet):\n        @task(100)\n        def read_thread(self):\n            pass\n\n        @task(7)\n        def create_thread(self):\n            pass\n\"\"\"\n\n", "func_signal": "def task(weight=1):\n", "code": "def decorator_func(func):\n    if func.__name__ in [\"on_stop\", \"on_start\"]:\n        logging.warning(\n            \"You have tagged your on_stop/start function with @task. This will make the method get called both as a task AND on stop/start.\"\n        )  # this is usually not what the user intended\n    func.locust_task_weight = weight\n    return func\n\n\"\"\"\nCheck if task was used without parentheses (not called), like this::\n\n    @task\n    def my_task()\n        pass\n\"\"\"\nif callable(weight):\n    func = weight\n    weight = 1\n    return decorator_func(func)\nelse:\n    return decorator_func", "path": "locust/locust/user/task.py", "commit_date": "2020-10-16 00:00:00", "repo_name": "locustio/locust", "stars": 23293, "license": "mit", "language": "python", "size": 20024}
{"docstring": "\"\"\"\nFunction used by both TaskSetMeta and UserMeta for collecting all declared tasks\non the TaskSet/User class and all its base classes\n\"\"\"\n", "func_signal": "def get_tasks_from_base_classes(bases, class_dict):\n", "code": "new_tasks = []\nfor base in bases:\n    if hasattr(base, \"tasks\") and base.tasks:\n        new_tasks += base.tasks\n\nif \"tasks\" in class_dict and class_dict[\"tasks\"] is not None:\n    tasks = class_dict[\"tasks\"]\n    if isinstance(tasks, dict):\n        tasks = tasks.items()\n\n    for task in tasks:\n        if isinstance(task, tuple):\n            task, count = task\n            for _ in range(count):\n                new_tasks.append(task)\n        else:\n            new_tasks.append(task)\n\nfor item in class_dict.values():\n    if \"locust_task_weight\" in dir(item):\n        for i in range(item.locust_task_weight):\n            new_tasks.append(item)\n\nreturn new_tasks", "path": "locust/locust/user/task.py", "commit_date": "2020-10-16 00:00:00", "repo_name": "locustio/locust", "stars": 23293, "license": "mit", "language": "python", "size": 20024}
{"docstring": "\"\"\"\nMethod that returns the time (in seconds) between the execution of tasks.\n\nExample::\n\n    from locust import TaskSet, between\n    class Tasks(TaskSet):\n        wait_time = between(3, 25)\n\"\"\"\n", "func_signal": "def wait_time(self):\n", "code": "if self.user.wait_time:\n    return self.user.wait_time()\nelif self.min_wait is not None and self.max_wait is not None:\n    return random.randint(self.min_wait, self.max_wait) / 1000.0\nelse:\n    raise MissingWaitTimeError(\n        \"You must define a wait_time method on either the %s or %s class\"\n        % (\n            type(self.user).__name__,\n            type(self).__name__,\n        )\n    )", "path": "locust/locust/user/task.py", "commit_date": "2020-10-16 00:00:00", "repo_name": "locustio/locust", "stars": 23293, "license": "mit", "language": "python", "size": 20024}
{"docstring": "# reset stats\n", "func_signal": "def test_error_grouping_errors_with_memory_addresses(self):\n", "code": "self.stats = RequestStats()\n\nclass Dummy:\n    pass\n\nself.stats.log_error(\"GET\", \"/\", Exception(\"Error caused by %r\" % Dummy()))\nself.assertEqual(1, len(self.stats.errors))", "path": "locust/locust/test/test_stats.py", "commit_date": "2020-10-12 00:00:00", "repo_name": "locustio/locust", "stars": 23293, "license": "mit", "language": "python", "size": 20024}
{"docstring": "\"\"\"Spawn CVS writer and exit loop after first iteration.\"\"\"\n", "func_signal": "def _write_csv_files(environment, stats_base_name, full_history=False):\n", "code": "stats_writer = StatsCSVFileWriter(environment, PERCENTILES_TO_REPORT, stats_base_name, full_history=full_history)\ngreenlet = gevent.spawn(stats_writer)\ngevent.sleep(_TEST_CSV_STATS_INTERVAL_WAIT_SEC)\ngevent.kill(greenlet)\nstats_writer.close_files()", "path": "locust/locust/test/test_stats.py", "commit_date": "2020-10-12 00:00:00", "repo_name": "locustio/locust", "stars": 23293, "license": "mit", "language": "python", "size": 20024}
{"docstring": "\"\"\"\nFunction used by Environment to recursively remove any tasks/TaskSets from a TaskSet/User that\nshouldn't be executed according to the tag options\n\"\"\"\n\n", "func_signal": "def filter_tasks_by_tags(task_holder, tags=None, exclude_tags=None, checked=None):\n", "code": "new_tasks = []\nif checked is None:\n    checked = {}\nfor task in task_holder.tasks:\n    if task in checked:\n        if checked[task]:\n            new_tasks.append(task)\n        continue\n\n    passing = True\n    if hasattr(task, \"tasks\"):\n        filter_tasks_by_tags(task, tags, exclude_tags, checked)\n        passing = len(task.tasks) > 0\n    else:\n        if tags is not None:\n            passing &= \"locust_tag_set\" in dir(task) and len(task.locust_tag_set & tags) > 0\n        if exclude_tags is not None:\n            passing &= \"locust_tag_set\" not in dir(task) or len(task.locust_tag_set & exclude_tags) == 0\n\n    if passing:\n        new_tasks.append(task)\n    checked[task] = passing\n\ntask_holder.tasks = new_tasks", "path": "locust/locust/user/task.py", "commit_date": "2020-10-16 00:00:00", "repo_name": "locustio/locust", "stars": 23293, "license": "mit", "language": "python", "size": 20024}
{"docstring": "\"\"\"\nSerialize a RequestStats instance, then serialize it through a Message,\nand unserialize the whole thing again. This is done \"IRL\" when stats are sent\nfrom workers to master.\n\"\"\"\n", "func_signal": "def test_serialize_through_message(self):\n", "code": "s1 = StatsEntry(self.stats, \"test\", \"GET\")\ns1.log(10, 0)\ns1.log(20, 0)\ns1.log(40, 0)\nu1 = StatsEntry.unserialize(s1.serialize())\n\ndata = Message.unserialize(Message(\"dummy\", s1.serialize(), \"none\").serialize()).data\nu1 = StatsEntry.unserialize(data)\n\nself.assertEqual(20, u1.median_response_time)", "path": "locust/locust/test/test_stats.py", "commit_date": "2020-10-12 00:00:00", "repo_name": "locustio/locust", "stars": 23293, "license": "mit", "language": "python", "size": 20024}
{"docstring": "\"\"\"\nWe need somewhere to store the stats.\n\nOn the master node stats will contain the aggregated sum of all content-lengths,\nwhile on the worker nodes this will be the sum of the content-lengths since the\nlast stats report was sent to the master\n\"\"\"\n", "func_signal": "def locust_init(environment, **kwargs):\n", "code": "if environment.web_ui:\n    # this code is only run on the master node (the web_ui instance doesn't exist on workers)\n    @environment.web_ui.app.route(\"/content-length\")\n    def total_content_length():\n        \"\"\"\n        Add a route to the Locust web app, where we can see the total content-length\n        \"\"\"\n        return \"Total content-length received: %i\" % stats[\"content-length\"]", "path": "locust/examples/events.py", "commit_date": "2020-11-18 00:00:00", "repo_name": "locustio/locust", "stars": 23293, "license": "mit", "language": "python", "size": 20024}
{"docstring": "\"\"\"\nEvent handler that get triggered on click of web UI Reset Stats button\n\"\"\"\n", "func_signal": "def on_reset_stats():\n", "code": "global stats\nstats = {}", "path": "locust/examples/extend_web_ui/extend.py", "commit_date": "2020-09-29 00:00:00", "repo_name": "locustio/locust", "stars": 23293, "license": "mit", "language": "python", "size": 20024}
{"docstring": "\"\"\"\nThis event is triggered on the worker instances every time a stats report is\nto be sent to the locust master. It will allow us to add our extra content-length\ndata to the dict that is being sent, and then we clear the local stats in the worker.\n\"\"\"\n", "func_signal": "def on_report_to_master(client_id, data):\n", "code": "data[\"content-length\"] = stats[\"content-length\"]\nstats[\"content-length\"] = 0", "path": "locust/examples/events.py", "commit_date": "2020-11-18 00:00:00", "repo_name": "locustio/locust", "stars": 23293, "license": "mit", "language": "python", "size": 20024}
{"docstring": "\"\"\"\nMake the running user sleep for a duration defined by the Locust.wait_time\nfunction (or TaskSet.wait_time function if it's been defined).\n\nThe user can also be killed gracefully while it's sleeping, so calling this\nmethod within a task makes it possible for a user to be killed mid-task, even if you've\nset a stop_timeout. If this behaviour is not desired you should make the user wait using\ngevent.sleep() instead.\n\"\"\"\n", "func_signal": "def wait(self):\n", "code": "if self.user._state == LOCUST_STATE_STOPPING:\n    raise StopUser()\nself.user._state = LOCUST_STATE_WAITING\nself._sleep(self.wait_time())\nif self.user._state == LOCUST_STATE_STOPPING:\n    raise StopUser()\nself.user._state = LOCUST_STATE_RUNNING", "path": "locust/locust/user/task.py", "commit_date": "2020-10-16 00:00:00", "repo_name": "locustio/locust", "stars": 23293, "license": "mit", "language": "python", "size": 20024}
{"docstring": "# reset stats\n", "func_signal": "def test_error_grouping(self):\n", "code": "self.stats = RequestStats()\n\nself.stats.log_error(\"GET\", \"/some-path\", Exception(\"Exception!\"))\nself.stats.log_error(\"GET\", \"/some-path\", Exception(\"Exception!\"))\n\nself.assertEqual(1, len(self.stats.errors))\nself.assertEqual(2, list(self.stats.errors.values())[0].occurrences)\n\nself.stats.log_error(\"GET\", \"/some-path\", Exception(\"Another exception!\"))\nself.stats.log_error(\"GET\", \"/some-path\", Exception(\"Another exception!\"))\nself.stats.log_error(\"GET\", \"/some-path\", Exception(\"Third exception!\"))\nself.assertEqual(3, len(self.stats.errors))", "path": "locust/locust/test/test_stats.py", "commit_date": "2020-10-12 00:00:00", "repo_name": "locustio/locust", "stars": 23293, "license": "mit", "language": "python", "size": 20024}
{"docstring": "# check if the function is a method bound to the current locust, and if so, don't pass self as first argument\n", "func_signal": "def execute_task(self, task):\n", "code": "if hasattr(task, \"__self__\") and task.__self__ == self:\n    # task is a bound method on self\n    task()\nelif hasattr(task, \"tasks\") and issubclass(task, TaskSet):\n    # task is another (nested) TaskSet class\n    task(self).run()\nelse:\n    # task is a function\n    task(self)", "path": "locust/locust/user/task.py", "commit_date": "2020-10-16 00:00:00", "repo_name": "locustio/locust", "stars": 23293, "license": "mit", "language": "python", "size": 20024}
{"docstring": "# assume all users arrive at the index page\n", "func_signal": "def on_start(self):\n", "code": "self.index_page()\nself.urls_on_current_page = self.toc_urls", "path": "locust/examples/browse_docs_test.py", "commit_date": "2020-08-27 00:00:00", "repo_name": "locustio/locust", "stars": 23293, "license": "mit", "language": "python", "size": 20024}
{"docstring": "\"\"\"\nAdd a task to the User's task execution queue.\n\n:param task_callable: User task to schedule.\n:param first: Optional keyword argument. If True, the task will be put first in the queue.\n\"\"\"\n", "func_signal": "def schedule_task(self, task_callable, first=False):\n", "code": "if first:\n    self._task_queue.insert(0, task_callable)\nelse:\n    self._task_queue.append(task_callable)", "path": "locust/locust/user/task.py", "commit_date": "2020-10-16 00:00:00", "repo_name": "locustio/locust", "stars": 23293, "license": "mit", "language": "python", "size": 20024}
{"docstring": "\"\"\"\nMemoization decorator with support for timeout.\n\nIf dynamic_timeout is set, the cache timeout is doubled if the cached function\ntakes longer time to run than the timeout time\n\"\"\"\n", "func_signal": "def memoize(timeout, dynamic_timeout=False):\n", "code": "cache = {\"timeout\": timeout}\n\ndef decorator(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        start = time()\n        if (not \"time\" in cache) or (start - cache[\"time\"] > cache[\"timeout\"]):\n            # cache miss\n            cache[\"result\"] = func(*args, **kwargs)\n            cache[\"time\"] = time()\n            if dynamic_timeout and cache[\"time\"] - start > cache[\"timeout\"]:\n                cache[\"timeout\"] *= 2\n        return cache[\"result\"]\n\n    def clear_cache():\n        if \"time\" in cache:\n            del cache[\"time\"]\n        if \"result\" in cache:\n            del cache[\"result\"]\n\n    wrapper.clear_cache = clear_cache\n    return wrapper\n\nreturn decorator", "path": "locust/locust/util/cache.py", "commit_date": "2020-08-27 00:00:00", "repo_name": "locustio/locust", "stars": 23293, "license": "mit", "language": "python", "size": 20024}
{"docstring": "'''\nConvert OpenCV camera calibration matrix to OpenGL projection and model view matrix\n:param proj_mat: OpenCV camera projeciton matrix\n:param width: Image width\n:param height: Image height\n:param near: Z near value\n:param far: Z far value\n:return: OpenGL projection matrix and model view matrix\n'''\n", "func_signal": "def MVP_from_P(proj_mat, width, height, near=0.1, far=10000):\n", "code": "res = cv2.decomposeProjectionMatrix(proj_mat)\nK, Rot, camera_center_homog = res[0], res[1], res[2]\ncamera_center = camera_center_homog[0:3] / camera_center_homog[3]\ntrans = -Rot.dot(camera_center)\nK = K / K[2][2]\n\nextrinsic = np.eye(4)\nextrinsic[:3, :3] = Rot\nextrinsic[:3, 3:4] = trans\naxis_adj = np.eye(4)\naxis_adj[2, 2] = -1\naxis_adj[1, 1] = -1\nmodel_view = np.matmul(axis_adj, extrinsic)\n\nzFar = far\nzNear = near\nprojective = np.zeros([4, 4])\nprojective[:2, :2] = K[:2, :2]\nprojective[:2, 2:3] = -K[:2, 2:3]\nprojective[3, 2] = -1\nprojective[2, 2] = (zNear + zFar)\nprojective[2, 3] = (zNear * zFar)\n\nndc = ortho(0, width, 0, height, zNear, zFar)\n\nperspective = np.matmul(ndc, projective)\n\nreturn perspective, model_view", "path": "pifuhd/lib/render/camera.py", "commit_date": "2020-06-13 00:00:00", "repo_name": "facebookresearch/pifuhd", "stars": 9376, "license": "other", "language": "python", "size": 407}
{"docstring": "'''\nargs:\n    points: [B, 3, N] 3d points in world space\n    images: [B, C, H, W] input images\n    calibs: [B, 3, 4] calibration matrices for each image\n    transforms: [B, 2, 3] image space coordinate transforms\nreturn:\n    [B, C, N] prediction corresponding to the given points\n'''\n", "func_signal": "def forward(self, points, images, calibs, transforms=None):\n", "code": "self.filter(images)\nself.query(points, calibs, transforms)\nreturn self.get_preds()", "path": "pifuhd/lib/model/BasePIFuNet.py", "commit_date": "2020-06-12 00:00:00", "repo_name": "facebookresearch/pifuhd", "stars": 9376, "license": "other", "language": "python", "size": 407}
{"docstring": "# First we draw a scene.\n# Notice the result is stored in the texture buffer.\n", "func_signal": "def display(self):\n", "code": "self.draw()\n\n# Then we return to the default frame buffer since we will display on the screen.\nglBindFramebuffer(GL_FRAMEBUFFER, 0)\n\n# Do the clean-up.\n# glClearColor(0.0, 0.0, 0.0, 0.0)        #Black background\nglClearColor(1.0, 1.0, 1.0, 0.0)        #Black background\nglClear(GL_COLOR_BUFFER_BIT)\n\n# We draw a rectangle which covers the whole screen.\nglUseProgram(self.quad_program)\nglBindBuffer(GL_ARRAY_BUFFER, self.quad_buffer)\n\nsize_of_double = 8\nglEnableVertexAttribArray(0)\nglVertexAttribPointer(0, 2, GL_DOUBLE, GL_FALSE, 4 * size_of_double, None)\nglEnableVertexAttribArray(1)\nglVertexAttribPointer(1, 2, GL_DOUBLE, GL_FALSE, 4 * size_of_double, c_void_p(2 * size_of_double))\n\nglDisable(GL_DEPTH_TEST)\n\n# The stored texture is then mapped to this rectangle.\n# properly assing color buffer texture\nglActiveTexture(GL_TEXTURE0)\nglBindTexture(GL_TEXTURE_2D, self.screen_texture[0])\nglUniform1i(glGetUniformLocation(self.quad_program, 'screenTexture'), 0)\n\nglDrawArrays(GL_TRIANGLES, 0, 6)\n\nglDisableVertexAttribArray(1)\nglDisableVertexAttribArray(0)\n\nglEnable(GL_DEPTH_TEST)\nglBindBuffer(GL_ARRAY_BUFFER, 0)\nglUseProgram(0)\n\nglutSwapBuffers()\nglutPostRedisplay()", "path": "pifuhd/lib/render/gl/render.py", "commit_date": "2020-06-13 00:00:00", "repo_name": "facebookresearch/pifuhd", "stars": 9376, "license": "other", "language": "python", "size": 407}
{"docstring": "# Setup\n", "func_signal": "def set_renderer():\n", "code": "device = torch.device(\"cuda:0\")\ntorch.cuda.set_device(device)\n\n# Initialize an OpenGL perspective camera.\nR, T = look_at_view_transform(2.0, 0, 180) \ncameras = OpenGLOrthographicCameras(device=device, R=R, T=T)\n\nraster_settings = RasterizationSettings(\n    image_size=512, \n    blur_radius=0.0, \n    faces_per_pixel=1, \n    bin_size = None, \n    max_faces_per_bin = None\n)\n\nlights = PointLights(device=device, location=((2.0, 2.0, 2.0),))\n\nrenderer = MeshRenderer(\n    rasterizer=MeshRasterizer(\n        cameras=cameras, \n        raster_settings=raster_settings\n    ),\n    shader=HardPhongShader(\n        device=device, \n        cameras=cameras,\n        lights=lights\n    )\n)\nreturn renderer", "path": "pifuhd/lib/colab_util.py", "commit_date": "2020-09-19 00:00:00", "repo_name": "facebookresearch/pifuhd", "stars": 9376, "license": "other", "language": "python", "size": 407}
{"docstring": "# check if file exists, get full path name\n", "func_signal": "def loadShader(shaderType, shaderFile):\n", "code": "strFilename = findFileOrThrow(shaderFile)\nshaderData = None\nwith open(strFilename, 'r') as f:\n    shaderData = f.read()\n\nshader = glCreateShader(shaderType)\nglShaderSource(shader, shaderData)  # note that this is a simpler function call than in C\n\n# This shader compilation is more explicit than the one used in\n# framework.cpp, which relies on a glutil wrapper function.\n# This is made explicit here mainly to decrease dependence on pyOpenGL\n# utilities and wrappers, which docs caution may change in future versions.\nglCompileShader(shader)\n\nstatus = glGetShaderiv(shader, GL_COMPILE_STATUS)\nif status == GL_FALSE:\n    # Note that getting the error log is much simpler in Python than in C/C++\n    # and does not require explicit handling of the string buffer\n    strInfoLog = glGetShaderInfoLog(shader)\n    strShaderType = \"\"\n    if shaderType is GL_VERTEX_SHADER:\n        strShaderType = \"vertex\"\n    elif shaderType is GL_GEOMETRY_SHADER:\n        strShaderType = \"geometry\"\n    elif shaderType is GL_FRAGMENT_SHADER:\n        strShaderType = \"fragment\"\n\n    print(\"Compilation failure for \" + strShaderType + \" shader:\\n\" + str(strInfoLog))\n\nreturn shader", "path": "pifuhd/lib/render/gl/framework.py", "commit_date": "2020-06-13 00:00:00", "repo_name": "facebookresearch/pifuhd", "stars": 9376, "license": "other", "language": "python", "size": 407}
{"docstring": "# Datasets related\n", "func_signal": "def initialize(self, parser):\n", "code": "g_data = parser.add_argument_group('Data')\ng_data.add_argument('--dataset', type=str, default='renderppl', help='dataset name')\ng_data.add_argument('--dataroot', type=str, default='./data',\n                    help='path to images (data folder)')\n\ng_data.add_argument('--loadSize', type=int, default=512, help='load size of input image')\n\n# Experiment related\ng_exp = parser.add_argument_group('Experiment')\ng_exp.add_argument('--name', type=str, default='',\n                   help='name of the experiment. It decides where to store samples and models')\ng_exp.add_argument('--debug', action='store_true', help='debug mode or not')\ng_exp.add_argument('--mode', type=str, default='inout', help='inout || color')\n\n# Training related\ng_train = parser.add_argument_group('Training')\ng_train.add_argument('--tmp_id', type=int, default=0, help='tmp_id')\ng_train.add_argument('--gpu_id', type=int, default=0, help='gpu id for cuda')\ng_train.add_argument('--batch_size', type=int, default=32, help='input batch size')\ng_train.add_argument('--num_threads', default=1, type=int, help='# sthreads for loading data')\ng_train.add_argument('--serial_batches', action='store_true',\n                     help='if true, takes images in order to make batches, otherwise takes them randomly')\ng_train.add_argument('--pin_memory', action='store_true', help='pin_memory')\ng_train.add_argument('--learning_rate', type=float, default=1e-3, help='adam learning rate')\ng_train.add_argument('--num_iter', type=int, default=30000, help='num iterations to train')\ng_train.add_argument('--freq_plot', type=int, default=100, help='freqency of the error plot')\ng_train.add_argument('--freq_mesh', type=int, default=20000, help='freqency of the save_checkpoints')\ng_train.add_argument('--freq_eval', type=int, default=5000, help='freqency of the save_checkpoints')\ng_train.add_argument('--freq_save_ply', type=int, default=5000, help='freqency of the save ply')\ng_train.add_argument('--freq_save_image', type=int, default=100, help='freqency of the save input image')\ng_train.add_argument('--resume_epoch', type=int, default=-1, help='epoch resuming the training')\ng_train.add_argument('--continue_train', action='store_true', help='continue training: load the latest model')\ng_train.add_argument('--finetune', action='store_true', help='fine tuning netG in training C')\n\n# Testing related\ng_test = parser.add_argument_group('Testing')\ng_test.add_argument('--resolution', type=int, default=512, help='# of grid in mesh reconstruction')\ng_test.add_argument('--no_numel_eval', action='store_true', help='no numerical evaluation')\ng_test.add_argument('--no_mesh_recon', action='store_true', help='no mesh reconstruction')\n\n# Sampling related\ng_sample = parser.add_argument_group('Sampling')\ng_sample.add_argument('--num_sample_inout', type=int, default=6000, help='# of sampling points')\ng_sample.add_argument('--num_sample_surface', type=int, default=0, help='# of sampling points')\ng_sample.add_argument('--num_sample_normal', type=int, default=0, help='# of sampling points')\ng_sample.add_argument('--num_sample_color', type=int, default=0, help='# of sampling points')\ng_sample.add_argument('--num_pts_dic', type=int, default=1, help='# of pts dic you load')\n\ng_sample.add_argument('--crop_type', type=str, default='fullbody', help='Sampling file name.')\ng_sample.add_argument('--uniform_ratio', type=float, default=0.1, help='maximum sigma for sampling')\ng_sample.add_argument('--mask_ratio', type=float, default=0.5, help='maximum sigma for sampling')\ng_sample.add_argument('--sampling_parts', action='store_true', help='Sampling on the fly')\ng_sample.add_argument('--sampling_otf', action='store_true', help='Sampling on the fly')\ng_sample.add_argument('--sampling_mode', type=str, default='sigma_uniform', help='Sampling file name.')\ng_sample.add_argument('--linear_anneal_sigma', action='store_true', help='linear annealing of sigma')\ng_sample.add_argument('--sigma_max', type=float, default=0.0, help='maximum sigma for sampling')\ng_sample.add_argument('--sigma_min', type=float, default=0.0, help='minimum sigma for sampling')\ng_sample.add_argument('--sigma', type=float, default=1.0, help='sigma for sampling')\ng_sample.add_argument('--sigma_surface', type=float, default=1.0, help='sigma for sampling')\n\ng_sample.add_argument('--z_size', type=float, default=200.0, help='z normalization factor')\n\n# Model related\ng_model = parser.add_argument_group('Model')\n# General\ng_model.add_argument('--norm', type=str, default='batch',\n                     help='instance normalization or batch normalization or group normalization')\n\n# Image filter General\ng_model.add_argument('--netG', type=str, default='hgpifu', help='piximp | fanimp | hghpifu')\ng_model.add_argument('--netC', type=str, default='resblkpifu', help='resblkpifu | resblkhpifu')\n\n# hgimp specific\ng_model.add_argument('--num_stack', type=int, default=4, help='# of hourglass')\ng_model.add_argument('--hg_depth', type=int, default=2, help='# of stacked layer of hourglass')\ng_model.add_argument('--hg_down', type=str, default='ave_pool', help='ave pool || conv64 || conv128')\ng_model.add_argument('--hg_dim', type=int, default=256, help='256 | 512')\n\n# Classification General\ng_model.add_argument('--mlp_norm', type=str, default='group', help='normalization for volume branch')\ng_model.add_argument('--mlp_dim', nargs='+', default=[257, 1024, 512, 256, 128, 1], type=int,\n                     help='# of dimensions of mlp. no need to put the first channel')\ng_model.add_argument('--mlp_dim_color', nargs='+', default=[1024, 512, 256, 128, 3], type=int,\n                     help='# of dimensions of mlp. no need to put the first channel')\ng_model.add_argument('--mlp_res_layers', nargs='+', default=[2,3,4], type=int,\n                     help='leyers that has skip connection. use 0 for no residual pass')\ng_model.add_argument('--merge_layer', type=int, default=-1)\n\n# for train\nparser.add_argument('--random_body_chop', action='store_true', help='if random flip')\nparser.add_argument('--random_flip', action='store_true', help='if random flip')\nparser.add_argument('--random_trans', action='store_true', help='if random flip')\nparser.add_argument('--random_scale', action='store_true', help='if random flip')\nparser.add_argument('--random_rotate', action='store_true', help='if random flip')\nparser.add_argument('--random_bg', action='store_true', help='using random background')\n\nparser.add_argument('--schedule', type=int, nargs='+', default=[10, 15],\n                    help='Decrease learning rate at these epochs.')\nparser.add_argument('--gamma', type=float, default=0.1, help='LR is multiplied by gamma on schedule.')\nparser.add_argument('--lambda_nml', type=float, default=0.0, help='weight of normal loss')\nparser.add_argument('--lambda_cmp_l1', type=float, default=0.0, help='weight of normal loss')\nparser.add_argument('--occ_loss_type', type=str, default='mse', help='bce | brock_bce | mse')\nparser.add_argument('--clr_loss_type', type=str, default='mse', help='mse | l1')\nparser.add_argument('--nml_loss_type', type=str, default='mse', help='mse | l1')\nparser.add_argument('--occ_gamma', type=float, default=None, help='weighting term')\nparser.add_argument('--no_finetune', action='store_true', help='fine tuning netG in training C')\n\n# for eval\nparser.add_argument('--val_test_error', action='store_true', help='validate errors of test data')\nparser.add_argument('--val_train_error', action='store_true', help='validate errors of train data')\nparser.add_argument('--gen_test_mesh', action='store_true', help='generate test mesh')\nparser.add_argument('--gen_train_mesh', action='store_true', help='generate train mesh')\nparser.add_argument('--all_mesh', action='store_true', help='generate meshs from all hourglass output')\nparser.add_argument('--num_gen_mesh_test', type=int, default=4,\n                    help='how many meshes to generate during testing')\n\n# path\nparser.add_argument('--load_netG_checkpoint_path', type=str, help='path to save checkpoints')\nparser.add_argument('--load_netC_checkpoint_path', type=str, help='path to save checkpoints')\nparser.add_argument('--checkpoints_path', type=str, default='./checkpoints', help='path to save checkpoints')\nparser.add_argument('--results_path', type=str, default='./results', help='path to save results ply')\nparser.add_argument('--load_checkpoint_path', type=str, help='path to save results ply')\nparser.add_argument('--single', type=str, default='', help='single data for training')\n\n# for single image reconstruction\nparser.add_argument('--mask_path', type=str, help='path for input mask')\nparser.add_argument('--img_path', type=str, help='path for input image')\n\n# for multi resolution\nparser.add_argument('--load_netMR_checkpoint_path', type=str, help='path to save checkpoints')\nparser.add_argument('--loadSizeBig', type=int, default=1024, help='load size of input image')\nparser.add_argument('--loadSizeLocal', type=int, default=512, help='load size of input image')\nparser.add_argument('--train_full_pifu', action='store_true', help='enable end-to-end training')\nparser.add_argument('--num_local', type=int, default=1, help='number of local cropping')\n\n# for normal condition\nparser.add_argument('--load_netFB_checkpoint_path', type=str, help='path to save checkpoints')\nparser.add_argument('--load_netF_checkpoint_path', type=str, help='path to save checkpoints')\nparser.add_argument('--load_netB_checkpoint_path', type=str, help='path to save checkpoints')\nparser.add_argument('--use_aio_normal', action='store_true')\nparser.add_argument('--use_front_normal', action='store_true')\nparser.add_argument('--use_back_normal', action='store_true')\nparser.add_argument('--no_intermediate_loss', action='store_true')\n\n# aug\ngroup_aug = parser.add_argument_group('aug')\ngroup_aug.add_argument('--aug_alstd', type=float, default=0.0, help='augmentation pca lighting alpha std')\ngroup_aug.add_argument('--aug_bri', type=float, default=0.2, help='augmentation brightness')\ngroup_aug.add_argument('--aug_con', type=float, default=0.2, help='augmentation contrast')\ngroup_aug.add_argument('--aug_sat', type=float, default=0.05, help='augmentation saturation')\ngroup_aug.add_argument('--aug_hue', type=float, default=0.05, help='augmentation hue')\ngroup_aug.add_argument('--aug_gry', type=float, default=0.1, help='augmentation gray scale')\ngroup_aug.add_argument('--aug_blur', type=float, default=0.0, help='augmentation blur')\n\n# for reconstruction\nparser.add_argument('--start_id', type=int, default=-1, help='load size of input image')\nparser.add_argument('--end_id', type=int, default=-1, help='load size of input image')\n\n# special tasks\nself.initialized = True\nreturn parser", "path": "pifuhd/lib/options.py", "commit_date": "2020-06-12 00:00:00", "repo_name": "facebookresearch/pifuhd", "stars": 9376, "license": "other", "language": "python", "size": 407}
{"docstring": "# Create a zeroed array with the same type and shape as our vertices i.e., per vertex normal\n", "func_signal": "def compute_normal(vertices, faces):\n", "code": "norm = np.zeros(vertices.shape, dtype=vertices.dtype)\n# Create an indexed view into the vertex array using the array of three indices for triangles\ntris = vertices[faces]\n# Calculate the normal for all the triangles, by taking the cross product of the vectors v1-v0, and v2-v0 in each triangle\nn = np.cross(tris[::, 1] - tris[::, 0], tris[::, 2] - tris[::, 0])\n# n is now an array of normals per triangle. The length of each normal is dependent the vertices,\n# we need to normalize these, so that our next step weights each normal equally.\nnormalize_v3(n)\n# now we have a normalized array of normals, one per triangle, i.e., per triangle normals.\n# But instead of one per triangle (i.e., flat shading), we add to each vertex in that triangle,\n# the triangles' normal. Multiple triangles would then contribute to every vertex, so we need to normalize again afterwards.\n# The cool part, we can actually add the normals through an indexed view of our (zeroed) per vertex normal array\nnorm[faces[:, 0]] += n\nnorm[faces[:, 1]] += n\nnorm[faces[:, 2]] += n\nnormalize_v3(norm)\n\nreturn norm", "path": "pifuhd/lib/render/mesh.py", "commit_date": "2020-06-12 00:00:00", "repo_name": "facebookresearch/pifuhd", "stars": 9376, "license": "other", "language": "python", "size": 407}
{"docstring": "# Focal Length\n# equivalent 50mm\n", "func_signal": "def __init__(self, width=1600, height=1200):\n", "code": "focal = np.sqrt(width * width + height * height)\nself.focal_x = focal\nself.focal_y = focal\n# Principal Point Offset\nself.principal_x = width / 2\nself.principal_y = height / 2\n# Axis Skew\nself.skew = 0\n# Image Size\nself.width = width\nself.height = height\n\nself.near = 1\nself.far = 10\n\n# Camera Center\nself.eye = np.array([0, 0, -3.6])\nself.center = np.array([0, 0, 0])\nself.direction = np.array([0, 0, -1])\nself.right = np.array([1, 0, 0])\nself.up = np.array([0, 1, 0])\n\nself.ortho_ratio = None", "path": "pifuhd/lib/render/camera.py", "commit_date": "2020-06-13 00:00:00", "repo_name": "facebookresearch/pifuhd", "stars": 9376, "license": "other", "language": "python", "size": 407}
{"docstring": "'''\napply a fully convolutional network to images.\nthe resulting feature will be stored.\nargs:\n    images: [B1, C, H, W]\n'''\n", "func_signal": "def filter_global(self, images):\n", "code": "if self.opt.train_full_pifu:\n    self.netG.filter(images)\nelse:\n    with torch.no_grad():\n        self.netG.filter(images)", "path": "pifuhd/lib/model/HGPIFuMRNet.py", "commit_date": "2020-06-12 00:00:00", "repo_name": "facebookresearch/pifuhd", "stars": 9376, "license": "other", "language": "python", "size": 407}
{"docstring": "'''\nCreate a dense grid of given resolution and bounding box\n:param resX: resolution along X axis\n:param resY: resolution along Y axis\n:param resZ: resolution along Z axis\n:param b_min: vec3 (x_min, y_min, z_min) bounding box corner\n:param b_max: vec3 (x_max, y_max, z_max) bounding box corner\n:return: [3, resX, resY, resZ] coordinates of the grid, and transform matrix from mesh index\n'''\n", "func_signal": "def create_grid(resX, resY, resZ, b_min=np.array([-1, -1, -1]), b_max=np.array([1, 1, 1]), transform=None):\n", "code": "coords = np.mgrid[:resX, :resY, :resZ]\ncoords = coords.reshape(3, -1)\ncoords_matrix = np.eye(4)\nlength = b_max - b_min\ncoords_matrix[0, 0] = length[0] / resX\ncoords_matrix[1, 1] = length[1] / resY\ncoords_matrix[2, 2] = length[2] / resZ\ncoords_matrix[0:3, 3] = b_min\ncoords = np.matmul(coords_matrix[:3, :3], coords) + coords_matrix[:3, 3:4]\nif transform is not None:\n    coords = np.matmul(transform[:3, :3], coords) + transform[:3, 3:4]\n    coords_matrix = np.matmul(transform, coords_matrix)\ncoords = coords.reshape(3, resX, resY, resZ)\nreturn coords, coords_matrix", "path": "pifuhd/lib/sdf.py", "commit_date": "2020-06-12 00:00:00", "repo_name": "facebookresearch/pifuhd", "stars": 9376, "license": "other", "language": "python", "size": 407}
{"docstring": "'''\nreturn the loss given the ground truth labels and prediction\n'''\n\n", "func_signal": "def get_error(self):\n", "code": "error = {}\nif self.opt.train_full_pifu:\n    if not self.opt.no_intermediate_loss:\n        error['Err(occ)'] = 0.0\n        for i in range(self.preds_low.size(0)):\n            error['Err(occ)'] += self.criteria['occ'](self.preds_low[i], self.labels, self.gamma, self.w)\n        error['Err(occ)'] /= self.preds_low.size(0)\n\n    error['Err(occ:fine)'] = 0.0\n    for i in range(self.preds_interm.size(0)):\n        error['Err(occ:fine)'] += self.criteria['occ'](self.preds_interm[i], self.labels, self.gamma, self.w)\n    error['Err(occ:fine)'] /= self.preds_interm.size(0)\n\n    if self.nmls is not None and self.labels_nml is not None:\n        error['Err(nml:fine)'] = self.criteria['nml'](self.nmls, self.labels_nml)\nelse:\n    error['Err(occ:fine)'] = 0.0\n    for i in range(self.preds_interm.size(0)):\n        error['Err(occ:fine)'] += self.criteria['occ'](self.preds_interm[i], self.labels, self.gamma, self.w)\n    error['Err(occ:fine)'] /= self.preds_interm.size(0)\n\n    if self.nmls is not None and self.labels_nml is not None:\n        error['Err(nml:fine)'] = self.criteria['nml'](self.nmls, self.labels_nml)\n\nreturn error", "path": "pifuhd/lib/model/HGPIFuMRNet.py", "commit_date": "2020-06-12 00:00:00", "repo_name": "facebookresearch/pifuhd", "stars": 9376, "license": "other", "language": "python", "size": 407}
{"docstring": "'''\nfeature may include multiple view inputs\nargs:\n    feature: [B, C_in, N]\nreturn:\n    [B, C_out, N] prediction\n'''\n", "func_signal": "def forward(self, feature):\n", "code": "y = feature\ntmpy = feature\nphi = None\nfor i, f in enumerate(self.filters):\n    y = f(\n        y if i not in self.res_layers\n        else torch.cat([y, tmpy], 1)\n    )\n    if i != len(self.filters)-1:\n        if self.norm not in ['batch', 'group']:\n            y = F.leaky_relu(y)\n        else:\n            y = F.leaky_relu(self.norms[i](y))         \n    if i == self.merge_layer:\n        phi = y.clone()\n\nif self.last_op is not None:\n    y = self.last_op(y)\n\nreturn y, phi", "path": "pifuhd/lib/model/MLP.py", "commit_date": "2020-06-12 00:00:00", "repo_name": "facebookresearch/pifuhd", "stars": 9376, "license": "other", "language": "python", "size": 407}
{"docstring": "'''\napply a fully convolutional network to images.\nthe resulting feature will be stored.\nargs:\n    images: [B1, B2, C, H, W]\n'''\n", "func_signal": "def filter_local(self, images, rect=None):\n", "code": "nmls = []\ntry:\n    if self.netG.opt.use_front_normal:\n        nmls.append(self.netG.nmlF)\n    if self.netG.opt.use_back_normal:\n        nmls.append(self.netG.nmlB)\nexcept:\n    pass\n\nif len(nmls):\n    nmls = nn.Upsample(size=(self.opt.loadSizeBig,self.opt.loadSizeBig), mode='bilinear', align_corners=True)(torch.cat(nmls,1))\n    \n    # it's kind of damn way.\n    if rect is None:\n        images = torch.cat([images, nmls[:,None].expand(-1,images.size(1),-1,-1,-1)], 2)\n    else:\n        nml = []\n        for i in range(rect.size(0)):\n            for j in range(rect.size(1)):\n                x1, y1, x2, y2 = rect[i,j]\n                tmp = nmls[i,:,y1:y2,x1:x2]\n                nml.append(nmls[i,:,y1:y2,x1:x2])\n        nml = torch.stack(nml, 0).view(*rect.shape[:2],*nml[0].size())\n        images = torch.cat([images, nml], 2)\n\nself.im_feat_list, self.normx = self.image_filter(images.view(-1,*images.size()[2:]))\nif not self.training:\n    self.im_feat_list = [self.im_feat_list[-1]]", "path": "pifuhd/lib/model/HGPIFuMRNet.py", "commit_date": "2020-06-12 00:00:00", "repo_name": "facebookresearch/pifuhd", "stars": 9376, "license": "other", "language": "python", "size": 407}
{"docstring": "# res = np.ones([4, 4], dtype=np.float32)\n", "func_signal": "def ortho(left, right, bottom, top, zNear, zFar):\n", "code": "res = identity()\nres[0][0] = 2 / (right - left)\nres[1][1] = 2 / (top - bottom)\nres[2][2] = - 2 / (zFar - zNear)\nres[3][0] = - (right + left) / (right - left)\nres[3][1] = - (top + bottom) / (top - bottom)\nres[3][2] = - (zFar + zNear) / (zFar - zNear)\nreturn res.T", "path": "pifuhd/lib/render/glm.py", "commit_date": "2020-06-13 00:00:00", "repo_name": "facebookresearch/pifuhd", "stars": 9376, "license": "other", "language": "python", "size": 407}
{"docstring": "'''\nreturn surface normal in 'model' space.\nit computes normal only in the last stack.\nnote that the current implementation use forward difference.\nargs:\n    points: [B1, B2, 3, N] 3d points in world space\n    calibs_local: [B1, B2, 4, 4] calibration matrices for each image\n    calibs_global: [B1, 4, 4] calibration matrices for each image\n    transforms: [B1, 2, 3] image space coordinate transforms\n    labels: [B1, B2, 3, N] ground truth normal\n    delta: perturbation for finite difference\n    fd_type: finite difference type (forward/backward/central) \n'''\n", "func_signal": "def calc_normal(self, points, calib_local, calib_global, transforms=None, labels=None, delta=0.001, fd_type='forward'):\n", "code": "B = calib_local.size(1)\n\nif labels is not None:\n    self.labels_nml = labels.view(-1,*labels.size()[2:])\n\nim_feat = self.im_feat_list[-1].view(-1,B,*self.im_feat_list[-1].size()[1:])\n\nnmls = []\nfor i in range(B):\n    points_sub = points[:,i]\n    pdx = points_sub.clone()\n    pdx[:,0,:] += delta\n    pdy = points_sub.clone()\n    pdy[:,1,:] += delta\n    pdz = points_sub.clone()\n    pdz[:,2,:] += delta\n\n    points_all = torch.stack([points_sub, pdx, pdy, pdz], 3)\n    points_all = points_all.view(*points_sub.size()[:2],-1)\n    xyz = self.projection(points_all, calib_local[:,i], transforms)\n    xy = xyz[:, :2, :]\n\n\n    self.netG.query(points=points_all, calibs=calib_global, update_pred=False)\n    z_feat = self.netG.phi\n    if not self.opt.train_full_pifu:\n        z_feat = z_feat.detach()\n\n    point_local_feat_list = [self.index(im_feat[:,i], xy), z_feat]            \n    point_local_feat = torch.cat(point_local_feat_list, 1)\n    pred = self.mlp(point_local_feat)[0]\n\n    pred = pred.view(*pred.size()[:2],-1,4) # (B, 1, N, 4)\n\n    # divide by delta is omitted since it's normalized anyway\n    dfdx = pred[:,:,:,1] - pred[:,:,:,0]\n    dfdy = pred[:,:,:,2] - pred[:,:,:,0]\n    dfdz = pred[:,:,:,3] - pred[:,:,:,0]\n\n    nml = -torch.cat([dfdx,dfdy,dfdz], 1)\n    nml = F.normalize(nml, dim=1, eps=1e-8)\n\n    nmls.append(nml)\n\nself.nmls = torch.stack(nmls,1).view(-1,3,points.size(3))", "path": "pifuhd/lib/model/HGPIFuMRNet.py", "commit_date": "2020-06-12 00:00:00", "repo_name": "facebookresearch/pifuhd", "stars": 9376, "license": "other", "language": "python", "size": 407}
{"docstring": "''' Normalize a numpy array of 3 component vectors shape=(n,3) '''\n", "func_signal": "def normalize_v3(arr):\n", "code": "lens = np.sqrt(arr[:, 0] ** 2 + arr[:, 1] ** 2 + arr[:, 2] ** 2)\neps = 0.00000001\nlens[lens < eps] = eps\narr[:, 0] /= lens\narr[:, 1] /= lens\narr[:, 2] /= lens\nreturn arr", "path": "pifuhd/lib/render/mesh.py", "commit_date": "2020-06-12 00:00:00", "repo_name": "facebookresearch/pifuhd", "stars": 9376, "license": "other", "language": "python", "size": 407}
{"docstring": "'''\nnormalize depth value\nargs:\n    xyz: [B, 3, N] depth value\n'''\n", "func_signal": "def forward(self, xyz, calibs=None, index_feat=None):\n", "code": "z_feat = xyz[:,2:3,:] * (self.opt.loadSize // 2) / self.opt.z_size\n\nreturn z_feat", "path": "pifuhd/lib/model/DepthNormalizer.py", "commit_date": "2020-06-12 00:00:00", "repo_name": "facebookresearch/pifuhd", "stars": 9376, "license": "other", "language": "python", "size": 407}
{"docstring": "# Keep constant names in C-style convention, for readability\n# when comparing to C(/C++) code.\n", "func_signal": "def findFileOrThrow(strBasename):\n", "code": "if os.path.isfile(strBasename):\n    return strBasename\n\nLOCAL_FILE_DIR = \"data\" + os.sep\nGLOBAL_FILE_DIR = os.path.dirname(os.path.abspath(__file__)) + os.sep + \"data\" + os.sep\n\nstrFilename = LOCAL_FILE_DIR + strBasename\nif os.path.isfile(strFilename):\n    return strFilename\n\nstrFilename = GLOBAL_FILE_DIR + strBasename\nif os.path.isfile(strFilename):\n    return strFilename\n\nraise IOError('Could not find target file ' + strBasename)", "path": "pifuhd/lib/render/gl/framework.py", "commit_date": "2020-06-13 00:00:00", "repo_name": "facebookresearch/pifuhd", "stars": 9376, "license": "other", "language": "python", "size": 407}
{"docstring": "'''\ngiven 3d points, we obtain 2d projection of these given the camera matrices.\nfilter needs to be called beforehand.\nthe prediction is stored to self.preds\nargs:\n    points: [B1, B2, 3, N] 3d points in world space\n    calibs_local: [B1, B2, 4, 4] calibration matrices for each image\n    calibs_global: [B1, 4, 4] calibration matrices for each image\n    transforms: [B1, 2, 3] image space coordinate transforms\n    labels: [B1, B2, C, N] ground truth labels (for supervision only)\nreturn:\n    [B, C, N] prediction\n'''\n", "func_signal": "def query(self, points, calib_local, calib_global=None, transforms=None, labels=None):\n", "code": "if calib_global is not None:\n    B = calib_local.size(1)\nelse:\n    B = 1\n    points = points[:,None]\n    calib_global = calib_local\n    calib_local = calib_local[:,None]\n\nws = []\npreds = []\npreds_interm = []\npreds_low = []\ngammas = []\nnewlabels = []\nfor i in range(B):\n    xyz = self.projection(points[:,i], calib_local[:,i], transforms)\n    \n    xy = xyz[:, :2, :]\n\n    # if the point is outside bounding box, return outside.\n    in_bb = (xyz >= -1) & (xyz <= 1)\n    in_bb = in_bb[:, 0, :] & in_bb[:, 1, :]\n    in_bb = in_bb[:, None, :].detach().float()\n\n    self.netG.query(points=points[:,i], calibs=calib_global)\n    preds_low.append(torch.stack(self.netG.intermediate_preds_list,0))\n\n    if labels is not None:\n        newlabels.append(in_bb * labels[:,i])\n        with torch.no_grad():\n            ws.append(in_bb.size(2) / in_bb.view(in_bb.size(0),-1).sum(1))\n            gammas.append(1 - newlabels[-1].view(newlabels[-1].size(0),-1).sum(1) / in_bb.view(in_bb.size(0),-1).sum(1))\n\n    z_feat = self.netG.phi\n    if not self.opt.train_full_pifu:\n        z_feat = z_feat.detach()\n                \n    intermediate_preds_list = []\n    for j, im_feat in enumerate(self.im_feat_list):\n        point_local_feat_list = [self.index(im_feat.view(-1,B,*im_feat.size()[1:])[:,i], xy), z_feat]\n        point_local_feat = torch.cat(point_local_feat_list, 1)\n        pred = self.mlp(point_local_feat)[0]\n        pred = in_bb * pred\n        intermediate_preds_list.append(pred)\n\n    preds_interm.append(torch.stack(intermediate_preds_list,0))\n    preds.append(intermediate_preds_list[-1])\n\nself.preds = torch.cat(preds,0)\nself.preds_interm = torch.cat(preds_interm, 1) # first dim is for intermediate predictions\nself.preds_low = torch.cat(preds_low, 1) # first dim is for intermediate predictions\n\nif labels is not None:\n    self.w = torch.cat(ws,0)\n    self.gamma = torch.cat(gammas,0)\n    self.labels = torch.cat(newlabels,0)", "path": "pifuhd/lib/model/HGPIFuMRNet.py", "commit_date": "2020-06-12 00:00:00", "repo_name": "facebookresearch/pifuhd", "stars": 9376, "license": "other", "language": "python", "size": 407}
{"docstring": "# initialize parser with basic options\n", "func_signal": "def gather_options(self, args=None):\n", "code": "if not self.initialized:\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser = self.initialize(parser)\n    self.parser = parser\n\nif args is None:\n    return self.parser.parse_args()\nelse:\n    return self.parser.parse_args(args)", "path": "pifuhd/lib/options.py", "commit_date": "2020-06-12 00:00:00", "repo_name": "facebookresearch/pifuhd", "stars": 9376, "license": "other", "language": "python", "size": 407}
{"docstring": "\"\"\"Get TF (scalar) summary.\n\nArgs:\n  sess: A TF Session to be used in making summary.\n  key: A string indicating the name of summary.\n  value: A string indicating the value of summary.\n\nReturns:\n  A TF summary.\n\"\"\"\n", "func_signal": "def get_summary(self, sess, key, value):\n", "code": "self._add_key_if_not_exists(key)\nplaceholder, summary = self._key_to_ph_summary_tuple[key]\nreturn sess.run(summary, {placeholder: value})", "path": "magenta/magenta/models/latent_transfer/common_joint.py", "commit_date": "2020-09-13 00:00:00", "repo_name": "magenta/magenta", "stars": 18879, "license": "apache-2.0", "language": "python", "size": 37838}
{"docstring": "\"\"\"Decode from given latant space vectors `z`.\n\nArgs:\n  z: A numpy array of latent space vectors.\n  batch_size: (Optional) a integer to indication batch size for computation\n      which is useful if the sampling requires lots of GPU memory.\n\nReturns:\n  A numpy array, the dataspace points from decoding.\n\"\"\"\n", "func_signal": "def decode(self, z, batch_size=None):\n", "code": "m = self.m\nbatch_size = batch_size or self.DEFAULT_BATCH_SIZE\nreturn run_with_batch(self.sess, m.x_mean, m.z, z, batch_size)", "path": "magenta/magenta/models/latent_transfer/common_joint.py", "commit_date": "2020-09-13 00:00:00", "repo_name": "magenta/magenta", "stars": 18879, "license": "apache-2.0", "language": "python", "size": 37838}
{"docstring": "\"\"\"Context manager that logs to a section nested one level deeper.\n\nArgs:\n  label: A short name for the section.\n  subsample_factor: Rate at which to subsample logging in this section.\n\nYields:\n  yields to caller.\n\"\"\"\n", "func_signal": "def section(self, label, subsample_factor=None):\n", "code": "new_section = _Section(label, subsample_factor=subsample_factor)\nself.stack[-1].log(new_section)\nself.stack.append(new_section)\nyield\nself.stack.pop()", "path": "magenta/magenta/models/coconet/lib_logging.py", "commit_date": "2020-01-23 00:00:00", "repo_name": "magenta/magenta", "stars": 18879, "license": "apache-2.0", "language": "python", "size": 37838}
{"docstring": "\"\"\"Classify given dataspace points `real_x`.\n\nArgs:\n  real_x: A numpy array of dataspace points.\n  batch_size: (Optional) a integer to indication batch size for computation\n      which is useful if the classification requires lots of GPU memory.\n\nReturns:\n  A numpy array, the prediction from classifier.\n\"\"\"\n", "func_signal": "def classify(self, real_x, batch_size=None):\n", "code": "batch_size = batch_size or self.DEFAULT_BATCH_SIZE\npred = run_with_batch(self.sess_sc09_class, self.sc09_class_scores,\n                      self.sc09_class_x, real_x, batch_size)\npred = np.argmax(pred, axis=-1)\nreturn pred", "path": "magenta/magenta/models/latent_transfer/common_joint.py", "commit_date": "2020-09-13 00:00:00", "repo_name": "magenta/magenta", "stars": 18879, "license": "apache-2.0", "language": "python", "size": 37838}
{"docstring": "\"\"\"Save dataspace instances.\n\nArgs:\n  x: A numpy array of dataspace points.\n  name: A string indicating the name in the saved file.\n  save_dir: A string indicating the directory to put the saved file.\n  x_is_real_x: An boolean indicating whether `x` is already in dataspace. If\n      not, `x` is converted to dataspace before saving\n\"\"\"\n", "func_signal": "def save_data(self, x, name, save_dir, x_is_real_x=False):\n", "code": "real_x = x if x_is_real_x else self.decode(x)\nreal_x = common.post_proc(real_x, self.config)\nbatched_real_x = common.batch_image(real_x)\nsample_file = os.path.join(save_dir, '%s.png' % name)\ncommon.save_image(batched_real_x, sample_file)", "path": "magenta/magenta/models/latent_transfer/common_joint.py", "commit_date": "2020-09-13 00:00:00", "repo_name": "magenta/magenta", "stars": 18879, "license": "apache-2.0", "language": "python", "size": 37838}
{"docstring": "\"\"\"Build the TF graph and heads from pre-trained WaveGAN ckpts.\n\nIt also prepares different graph, session and heads for sampling and\nclassification respectively.\n\"\"\"\n\n# pylint:disable=unused-variable,possibly-unused-variable\n# Reason:\n#   All endpoints are stored as attribute at the end of `_build`.\n#   Pylint cannot infer this case so it emits false alarm of\n#   unused-variable if we do not disable this warning.\n\n# pylint:disable=invalid-name\n# Reason:\n#   Variable useing 'G' in is name to be consistent with WaveGAN's author\n#   has name consider to be invalid by pylint so we disable the warning.\n\n# Dataset (SC09, WaveGAN)'s generator\n", "func_signal": "def build(self):\n", "code": "graph_sc09_gan = tf.Graph()\nwith graph_sc09_gan.as_default():\n  # Use the retrained, Gaussian priored model\n  gen_ckpt_dir = os.path.expanduser(FLAGS.wavegan_gen_ckpt_dir)\n  sess_sc09_gan = tf.Session(graph=graph_sc09_gan)\n  saver_gan = tf.train.import_meta_graph(\n      os.path.join(gen_ckpt_dir, 'infer', 'infer.meta'))\n\n# Dataset (SC09, WaveGAN)'s  classifier (inception)\ngraph_sc09_class = tf.Graph()\nwith graph_sc09_class.as_default():\n  inception_ckpt_dir = os.path.expanduser(FLAGS.wavegan_inception_ckpt_dir)\n  sess_sc09_class = tf.Session(graph=graph_sc09_class)\n  saver_class = tf.train.import_meta_graph(\n      os.path.join(inception_ckpt_dir, 'infer.meta'))\n\n# Dataset B (SC09, WaveGAN)'s Tensor symbols\nsc09_gan_z = graph_sc09_gan.get_tensor_by_name('z:0')\nsc09_gan_G_z = graph_sc09_gan.get_tensor_by_name('G_z:0')[:, :, 0]\n\n# Classification: Tensor symbols\nsc09_class_x = graph_sc09_class.get_tensor_by_name('x:0')\nsc09_class_scores = graph_sc09_class.get_tensor_by_name('scores:0')\n\n# Add all endpoints as object attributes\nfor k, v in locals().items():\n  self.__dict__[k] = v", "path": "magenta/magenta/models/latent_transfer/common_joint.py", "commit_date": "2020-09-13 00:00:00", "repo_name": "magenta/magenta", "stars": 18879, "license": "apache-2.0", "language": "python", "size": 37838}
{"docstring": "\"\"\"Creates a signature def for the SavedModel.\"\"\"\n", "func_signal": "def get_signature_def(model, use_tf_sampling):\n", "code": "if use_tf_sampling:\n  return tf.saved_model.signature_def_utils.predict_signature_def(\n      inputs={\n          'pianorolls': model.inputs['pianorolls'],\n      }, outputs={\n          'predictions': tf.cast(model.samples, tf.bool),\n      })\nreturn tf.saved_model.signature_def_utils.predict_signature_def(\n    inputs={\n        'pianorolls': model.model.pianorolls,\n        'masks': model.model.masks,\n        'lengths': model.model.lengths,\n    }, outputs={\n        'predictions': model.model.predictions\n    })", "path": "magenta/magenta/models/coconet/lib_saved_model.py", "commit_date": "2020-01-23 00:00:00", "repo_name": "magenta/magenta", "stars": 18879, "license": "apache-2.0", "language": "python", "size": 37838}
{"docstring": "\"\"\"Restore the weights of models.\"\"\"\n", "func_signal": "def restore(self):\n", "code": "gen_ckpt_dir = self.gen_ckpt_dir\ngraph_sc09_gan = self.graph_sc09_gan\nsaver_gan = self.saver_gan\nsess_sc09_gan = self.sess_sc09_gan\n\ninception_ckpt_dir = self.inception_ckpt_dir\ngraph_sc09_class = self.graph_sc09_class\nsaver_class = self.saver_class\nsess_sc09_class = self.sess_sc09_class\n\nwith graph_sc09_gan.as_default():\n  saver_gan.restore(\n      sess_sc09_gan,\n      os.path.join(gen_ckpt_dir, 'bridge', 'model.ckpt'))\n\nwith graph_sc09_class.as_default():\n  saver_class.restore(sess_sc09_class,\n                      os.path.join(inception_ckpt_dir, 'best_acc-103005'))\n\n# pylint:enable=unused-variable,possibly-unused-variable\n# pylint:enable=invalid-name", "path": "magenta/magenta/models/latent_transfer/common_joint.py", "commit_date": "2020-09-13 00:00:00", "repo_name": "magenta/magenta", "stars": 18879, "license": "apache-2.0", "language": "python", "size": 37838}
{"docstring": "\"\"\"Pick a batch where instances are sampled from Guassian distributions.\"\"\"\n", "func_signal": "def pick_batch(self, batch_index):\n", "code": "mu, sigma = self.mu, self.sigma\nbatch_mu, batch_sigma = self._np_index_arrs(batch_index, mu, sigma)\nbatch = self._np_sample_from_gaussian(batch_mu, batch_sigma)\nreturn batch", "path": "magenta/magenta/models/latent_transfer/common_joint.py", "commit_date": "2020-09-13 00:00:00", "repo_name": "magenta/magenta", "stars": 18879, "license": "apache-2.0", "language": "python", "size": 37838}
{"docstring": "\"\"\"Load a dataset from a config's name.\n\nThe loaded dataset consists of:\n  - original data (dataset_blob, train_data, train_label),\n  - encoded data from a pretrained model (train_mu, train_sigma), and\n  - index grouped by label (index_grouped_by_label).\n\nArgs:\n  config_name: A string indicating the name of config to parameterize the\n      model that associates with the dataset.\n  exp_uid: A string representing the unique id of experiment to be used in\n      model that associates with the dataset.\n\nReturns:\n  An tuple of abovementioned components in the dataset.\n\"\"\"\n\n", "func_signal": "def load_dataset(config_name, exp_uid):\n", "code": "config = load_config(config_name)\nif config_is_wavegan(config):\n  return load_dataset_wavegan()\n\nmodel_uid = common.get_model_uid(config_name, exp_uid)\n\ndataset = common.load_dataset(config)\ntrain_data = dataset.train_data\nattr_train = dataset.attr_train\npath_train = os.path.join(dataset.basepath, 'encoded', model_uid,\n                          'encoded_train_data.npz')\ntrain = np.load(path_train)\ntrain_mu = train['mu']\ntrain_sigma = train['sigma']\ntrain_label = np.argmax(attr_train, axis=-1)  # from one-hot to label\nindex_grouped_by_label = common.get_index_grouped_by_label(train_label)\n\ntf.logging.info('index_grouped_by_label size: %s',\n                [len(_) for _ in index_grouped_by_label])\n\ntf.logging.info('train loaded from %s', path_train)\ntf.logging.info('train shapes: mu = %s, sigma = %s', train_mu.shape,\n                train_sigma.shape)\ndataset_blob = dataset\nreturn (dataset_blob, train_data, train_label, train_mu, train_sigma,\n        index_grouped_by_label)", "path": "magenta/magenta/models/latent_transfer/common_joint.py", "commit_date": "2020-09-13 00:00:00", "repo_name": "magenta/magenta", "stars": 18879, "license": "apache-2.0", "language": "python", "size": 37838}
{"docstring": "\"\"\"Initialize a Logger instance.\"\"\"\n", "func_signal": "def __init__(self):\n", "code": "self.root = _Section(\"root\", subsample_factor=1)\nself.stack = [self.root]", "path": "magenta/magenta/models/coconet/lib_logging.py", "commit_date": "2020-01-23 00:00:00", "repo_name": "magenta/magenta", "stars": 18879, "license": "apache-2.0", "language": "python", "size": 37838}
{"docstring": "\"\"\"Initialize a Section instance.\n\nArgs:\n  label: A short name for the section.\n  subsample_factor: Rate at which to subsample logging in this section.\n\"\"\"\n", "func_signal": "def __init__(self, label, subsample_factor=None):\n", "code": "self.label = label\nself.subsample_factor = 1 if subsample_factor is None else subsample_factor\nself.items = []\nself.i = 0", "path": "magenta/magenta/models/coconet/lib_logging.py", "commit_date": "2020-01-23 00:00:00", "repo_name": "magenta/magenta", "stars": 18879, "license": "apache-2.0", "language": "python", "size": 37838}
{"docstring": "\"\"\"Decode from given latant space vectors `z`.\n\nArgs:\n  z: A numpy array of latent space vectors.\n  batch_size: (Optional) a integer to indication batch size for computation\n      which is useful if the sampling requires lots of GPU memory.\n\nReturns:\n  A numpy array, the dataspace points from decoding.\n\"\"\"\n", "func_signal": "def decode(self, z, batch_size=None):\n", "code": "batch_size = batch_size or self.DEFAULT_BATCH_SIZE\nreturn run_with_batch(self.sess_sc09_gan, self.sc09_gan_G_z,\n                      self.sc09_gan_z, z, batch_size)", "path": "magenta/magenta/models/latent_transfer/common_joint.py", "commit_date": "2020-09-13 00:00:00", "repo_name": "magenta/magenta", "stars": 18879, "license": "apache-2.0", "language": "python", "size": 37838}
{"docstring": "\"\"\"Save dataspace instances.\n\nArgs:\n  x: A numpy array of dataspace points.\n  name: A string indicating the name in the saved file.\n  save_dir: A string indicating the directory to put the saved file.\n  x_is_real_x: An boolean indicating whether `x` is already in dataspace. If\n      not, `x` is converted to dataspace before saving\n\"\"\"\n", "func_signal": "def save_data(self, x, name, save_dir, x_is_real_x=False):\n", "code": "real_x = x if x_is_real_x else self.decode(x)\nreal_x = real_x.reshape(-1)\nsample_file = os.path.join(save_dir, '%s.wav' % name)\nwavfile.write(sample_file, rate=16000, data=real_x)", "path": "magenta/magenta/models/latent_transfer/common_joint.py", "commit_date": "2020-09-13 00:00:00", "repo_name": "magenta/magenta", "stars": 18879, "license": "apache-2.0", "language": "python", "size": 37838}
{"docstring": "\"\"\"for a given frame, find the next one in a list of frame.\n\nArgs:\n  l_name: the name of file\n  r_list: a list of potential file to be matched\n\nReturns:\n  a match (or False if no match)\n  the frame number of the match\n\"\"\"\n", "func_signal": "def is_match(l_name, r_list):\n", "code": "basename = ntpath.basename(l_name)\nframe_number = int(basename.split('.')[0][1:])\nmatched_name = '{:07d}.jpg'.format(frame_number + 1)\nmatches = [x for x in r_list if matched_name in x]\nif matches:\n  return matches[0], frame_number\nreturn False, 0", "path": "magenta/magenta/video/next_frame_prediction_pix2pix/join_pairs.py", "commit_date": "2019-01-10 00:00:00", "repo_name": "magenta/magenta", "stars": 18879, "license": "apache-2.0", "language": "python", "size": 37838}
{"docstring": "\"\"\"Exports the given model as SavedModel to destination.\"\"\"\n", "func_signal": "def export_saved_model(model, destination, tags, use_tf_sampling):\n", "code": "if model is None or destination is None or not destination:\n  tf.logging.error('No model or destination provided.')\n  return\n\nbuilder = tf.saved_model.builder.SavedModelBuilder(destination)\n\nsignature_def_map = {\n    tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:\n    get_signature_def(model, use_tf_sampling)}\n\nbuilder.add_meta_graph_and_variables(\n    model.sess,\n    tags,\n    signature_def_map=signature_def_map,\n    strip_default_attrs=True)\nbuilder.save()", "path": "magenta/magenta/models/coconet/lib_saved_model.py", "commit_date": "2020-01-23 00:00:00", "repo_name": "magenta/magenta", "stars": 18879, "license": "apache-2.0", "language": "python", "size": 37838}
{"docstring": "\"\"\"Sampling from Guassian distribtuion specified by `mu` and `sigma`.\"\"\"\n", "func_signal": "def _np_sample_from_gaussian(mu, sigma):\n", "code": "assert mu.shape == sigma.shape\nreturn mu + sigma * np.random.randn(*sigma.shape)", "path": "magenta/magenta/models/latent_transfer/common_joint.py", "commit_date": "2020-09-13 00:00:00", "repo_name": "magenta/magenta", "stars": 18879, "license": "apache-2.0", "language": "python", "size": 37838}
{"docstring": "\"\"\"Restore the pretrained model and classifier.\n\nArgs:\n  dataset_blob: The object containts `save_path` used for restoring.\n\"\"\"\n", "func_signal": "def restore(self, dataset_blob):\n", "code": "this_config_is_wavegan = self.this_config_is_wavegan\nm_helper = self.m_helper\nm_classifier_helper = self.m_classifier_helper\n\nif this_config_is_wavegan:\n  m_helper.restore()\n  # We don't need restore the `m_classifier_helper` again since `m_helper`\n  # and `m_classifier_helper` are two identicial objects.\nelse:\n  m_helper.restore_best('vae_saver', dataset_blob.save_path,\n                        'vae_best_%s.ckpt')\n  m_classifier_helper.restore_best(\n      'classifier_saver', dataset_blob.save_path, 'classifier_best_%s.ckpt')", "path": "magenta/magenta/models/latent_transfer/common_joint.py", "commit_date": "2020-09-13 00:00:00", "repo_name": "magenta/magenta", "stars": 18879, "license": "apache-2.0", "language": "python", "size": 37838}
{"docstring": "\"\"\"Add related TF heads for a key if it is not used before.\"\"\"\n", "func_signal": "def _add_key_if_not_exists(self, key):\n", "code": "if key in self._key_to_ph_summary_tuple:\n  return\nplaceholder = tf.placeholder(tf.float32, shape=(), name=key + '_ph')\nsummary = tf.summary.scalar(key, placeholder)\nself._key_to_ph_summary_tuple[key] = (placeholder, summary)", "path": "magenta/magenta/models/latent_transfer/common_joint.py", "commit_date": "2020-09-13 00:00:00", "repo_name": "magenta/magenta", "stars": 18879, "license": "apache-2.0", "language": "python", "size": 37838}
{"docstring": "\"\"\"Build the TF graph and heads for dataspace model.\n\nIt also prepares different graph, session and heads for sampling and\nclassification respectively.\n\"\"\"\n\n", "func_signal": "def build(self):\n", "code": "config_name = self.config_name\nconfig = load_config(config_name)\nexp_uid = self.exp_uid\n\ngraph = tf.Graph()\nwith graph.as_default():\n  sess = tf.Session(graph=graph)\n  m = load_model(model_dataspace.Model, config_name, exp_uid)\n\nself.config = config\nself.graph = graph\nself.sess = sess\nself.m = m", "path": "magenta/magenta/models/latent_transfer/common_joint.py", "commit_date": "2020-09-13 00:00:00", "repo_name": "magenta/magenta", "stars": 18879, "license": "apache-2.0", "language": "python", "size": 37838}
{"docstring": "# don't collapse this message if it has a new direct child\n", "func_signal": "def should_collapse(cls, message):\n", "code": "if hasattr(message, \"child\"):\n    has_new_child = any(child.new for child in message.child.things)\nelse:\n    has_new_child = False\n\nreturn (message.is_collapsed and\n    not message.new and\n    not has_new_child)", "path": "reddit/r2/r2/models/builder.py", "commit_date": "2017-02-21 00:00:00", "repo_name": "reddit-archive/reddit", "stars": 16744, "license": "other", "language": "python", "size": 40093}
{"docstring": "\"\"\"Return a list of CommentTuples in tree insertion order.\n\nAlso add a MissingChildrenTuple to the end of the list if there\nare missing root level comments.\n\n\"\"\"\n\n", "func_signal": "def get_comment_order(self):\n", "code": "with g.stats.get_timer('comment_tree.get.1') as comment_tree_timer:\n    comment_tree = CommentTree.by_link(self.link, comment_tree_timer)\n    sort_name = self.sort.col\n    sorter = get_comment_scores(\n        self.link, sort_name, comment_tree.cids, comment_tree_timer)\n    comment_tree_timer.intermediate('get_scores')\n\nif isinstance(self.sort, operators.shuffled):\n    # randomize the scores of top level comments\n    top_level_ids = comment_tree.tree.get(None, [])\n    top_level_scores = [\n        sorter[comment_id] for comment_id in top_level_ids]\n    shuffle(top_level_scores)\n    for i, comment_id in enumerate(top_level_ids):\n        sorter[comment_id] = top_level_scores[i]\n\nself.timer.intermediate(\"load_storage\")\n\ncomment_tree = self.modify_comment_tree(comment_tree)\nself.timer.intermediate(\"modify_comment_tree\")\n\ninitial_candidates, offset_depth = self.get_initial_candidates(comment_tree)\n\ncomment_tuples = self.get_initial_comment_list(comment_tree)\nif comment_tuples:\n    # some comments have bypassed the sorting/inserting process, remove\n    # them from `initial_candidates` so they won't be inserted again\n    comment_tuple_ids = {\n        comment_tuple.comment_id for comment_tuple in comment_tuples}\n    initial_candidates = [\n        comment_id for comment_id in initial_candidates\n        if comment_id not in comment_tuple_ids\n    ]\n\ncandidates = []\nself.update_candidates(candidates, sorter, initial_candidates)\nself.timer.intermediate(\"pick_candidates\")\n\n# choose which comments to show\nwhile candidates and len(comment_tuples) < self.max_comments:\n    sort_val, comment_id = heapq.heappop(candidates)\n    if comment_id not in comment_tree.cids:\n        continue\n\n    comment_depth = comment_tree.depth[comment_id] - offset_depth\n    if comment_depth >= self.max_depth:\n        continue\n\n    child_ids = comment_tree.tree.get(comment_id, [])\n\n    comment_tuples.append(CommentTuple(\n        comment_id=comment_id,\n        depth=comment_depth,\n        parent_id=comment_tree.parents[comment_id],\n        num_children=comment_tree.num_children[comment_id],\n        child_ids=child_ids,\n    ))\n\n    child_depth = comment_depth + 1\n    if child_depth < self.max_depth:\n        self.update_candidates(candidates, sorter, child_ids)\n\nself.timer.intermediate(\"pick_comments\")\n\n# add all not-selected top level comments to the comment_tuples list\n# so we can make MoreChildren for them later\ntop_level_not_visible = {\n    comment_id for sort_val, comment_id in candidates\n    if comment_tree.depth.get(comment_id, 0) - offset_depth == 0\n}\n\nif top_level_not_visible:\n    num_children_not_visible = sum(\n        1 + comment_tree.num_children[comment_id]\n        for comment_id in top_level_not_visible\n    )\n    comment_tuples.append(MissingChildrenTuple(\n        num_children=num_children_not_visible,\n        child_ids=top_level_not_visible,\n    ))\n\nself.timer.intermediate(\"handle_morechildren\")\nreturn comment_tuples", "path": "reddit/r2/r2/models/builder.py", "commit_date": "2017-02-21 00:00:00", "repo_name": "reddit-archive/reddit", "stars": 16744, "license": "other", "language": "python", "size": 40093}
{"docstring": "\"\"\"Determine if an item meets all criteria to display as controversial.\"\"\"\n\n# A sample-size threshold before posts can be considered controversial\n", "func_signal": "def _is_controversial(self, wrapped):\n", "code": "num_votes = wrapped.upvotes + wrapped.downvotes\nif num_votes < g.live_config['cflag_min_votes']:\n    return False\n\n# If an item falls within a boundary of upvote ratios, it's controversial\n# e.g. 0.4 < x < 0.6\nlower = g.live_config['cflag_lower_bound']\nupper = g.live_config['cflag_upper_bound']\nif lower <= wrapped.upvote_ratio <= upper:\n    return True\n\nreturn False", "path": "reddit/r2/r2/models/builder.py", "commit_date": "2017-02-21 00:00:00", "repo_name": "reddit-archive/reddit", "stars": 16744, "license": "other", "language": "python", "size": 40093}
{"docstring": "\"\"\"Return the preference settings of the logged in user\"\"\"\n", "func_signal": "def GET_prefs(self, fields):\n", "code": "resp = PrefsJsonTemplate(fields).data(c.oauth_user)\nreturn self.api_wrapper(resp)", "path": "reddit/r2/r2/controllers/apiv1/user.py", "commit_date": "2016-06-08 00:00:00", "repo_name": "reddit-archive/reddit", "stars": 16744, "license": "other", "language": "python", "size": 40093}
{"docstring": "# CampaignBuilder has special wrapping logic to operate on\n# PromoTuples and PromoCampaigns. `after` is just a Link, so bypass\n# the special wrapping logic and use the base class.\n", "func_signal": "def valid_after(self, after):\n", "code": "if self.prewrap_fn:\n    after = self.prewrap_fn(after)\nif self.wrap:\n    after = Builder.wrap_items(self, (after,))[0]\nreturn not self.must_skip(after)", "path": "reddit/r2/r2/models/builder.py", "commit_date": "2017-02-21 00:00:00", "repo_name": "reddit-archive/reddit", "stars": 16744, "license": "other", "language": "python", "size": 40093}
{"docstring": "\"\"\"Start the tree from the first ancestor of requested comment.\"\"\"\n", "func_signal": "def get_initial_candidates(self, comment_tree):\n", "code": "path = self.get_path_to_comment(\n    self.comment, self.context, comment_tree)\n\n# get_path_to_comment returns path ordered from ancestor to\n# selected comment\nroot_comment = path[0]\ninitial_candidates = [root_comment]\noffset_depth = comment_tree.depth[root_comment]\nreturn initial_candidates, offset_depth", "path": "reddit/r2/r2/models/builder.py", "commit_date": "2017-02-21 00:00:00", "repo_name": "reddit-archive/reddit", "stars": 16744, "license": "other", "language": "python", "size": 40093}
{"docstring": "\"\"\"A typical example of what we might get out of `safemarkdown()`\"\"\"\n", "func_signal": "def test_benign(self):\n", "code": "testcase = \"\"\"\n    <!-- SC_OFF -->\n    <div class=\"md\"><a href=\"http://zombo.com/\">Welcome</a></div>\n    <!-- SC_ON -->\n\"\"\"\nself.assertFragmentValid(testcase)", "path": "reddit/r2/r2/tests/unit/lib/souptest_test.py", "commit_date": "2015-09-24 00:00:00", "repo_name": "reddit-archive/reddit", "stars": 16744, "license": "other", "language": "python", "size": 40093}
{"docstring": "\"\"\"Build the tree starting from all root level comments.\"\"\"\n", "func_signal": "def get_initial_candidates(self, comment_tree):\n", "code": "initial_candidates = comment_tree.tree.get(None, [])\nif initial_candidates:\n    offset_depth = min(comment_tree.depth[comment_id]\n        for comment_id in initial_candidates)\nelse:\n    offset_depth = 0\nreturn initial_candidates, offset_depth", "path": "reddit/r2/r2/models/builder.py", "commit_date": "2017-02-21 00:00:00", "repo_name": "reddit-archive/reddit", "stars": 16744, "license": "other", "language": "python", "size": 40093}
{"docstring": "\"\"\"Promote the sticky comment, if any.\"\"\"\n", "func_signal": "def get_initial_comment_list(self, comment_tree):\n", "code": "comment_tuples = []\n\nif self.link.sticky_comment_id:\n    root_level_comments = comment_tree.tree.get(None, [])\n    sticky_comment_id = self.link.sticky_comment_id\n    if sticky_comment_id in root_level_comments:\n        comment_tuples.append(CommentTuple(\n            comment_id=sticky_comment_id,\n            depth=0,\n            parent_id=None,\n            num_children=comment_tree.num_children[sticky_comment_id],\n            child_ids=comment_tree.tree.get(sticky_comment_id, []),\n        ))\n    else:\n        g.log.warning(\"Non-top-level sticky comment detected on \"\n                      \"link %r.\", self.link)\nreturn comment_tuples", "path": "reddit/r2/r2/models/builder.py", "commit_date": "2017-02-21 00:00:00", "repo_name": "reddit-archive/reddit", "stars": 16744, "license": "other", "language": "python", "size": 40093}
{"docstring": "# only activate precomputed sorts for links with enough comments.\n# (value of 0 means not active for any value of link.num_comments)\n", "func_signal": "def get_active_sort_orders_for_link(link):\n", "code": "min_comments = g.live_config['precomputed_comment_sort_min_comments']\nif min_comments <= 0 or link.num_comments < min_comments:\n    return set()\n\nactive_sorts = set(g.live_config['precomputed_comment_sorts'])\nif g.live_config['precomputed_comment_suggested_sort']:\n    suggested_sort = link.sort_if_suggested()\n    if suggested_sort:\n        active_sorts.add(suggested_sort)\n\nreturn active_sorts", "path": "reddit/r2/r2/models/builder.py", "commit_date": "2017-02-21 00:00:00", "repo_name": "reddit-archive/reddit", "stars": 16744, "license": "other", "language": "python", "size": 40093}
{"docstring": "\"\"\"Return the path back to top level from comment.\n\nRestrict the path to a maximum of `context` levels deep.\"\"\"\n\n", "func_signal": "def get_path_to_comment(cls, comment, context, comment_tree):\n", "code": "if comment._id not in comment_tree.cids:\n    # the comment isn't in the tree\n    raise InconsistentCommentTreeError\n\ncomment_id = comment._id\npath = []\nwhile comment_id and len(path) <= context:\n    path.append(comment_id)\n    try:\n        comment_id = comment_tree.parents[comment_id]\n    except KeyError:\n        # the comment's parent is missing from the tree. this might\n        # just mean that the child was added to the tree first and\n        # the tree will be correct when the parent is added.\n        raise InconsistentCommentTreeError\n\n# reverse the list so the first element is the most root level comment\npath.reverse()\nreturn path", "path": "reddit/r2/r2/models/builder.py", "commit_date": "2017-02-21 00:00:00", "repo_name": "reddit-archive/reddit", "stars": 16744, "license": "other", "language": "python", "size": 40093}
{"docstring": "\"\"\"Return a breakdown of subreddit karma.\"\"\"\n", "func_signal": "def GET_karma(self):\n", "code": "karmas = c.oauth_user.all_karmas(include_old=False)\nresp = KarmaListJsonTemplate().render(karmas)\nreturn self.api_wrapper(resp.finalize())", "path": "reddit/r2/r2/controllers/apiv1/user.py", "commit_date": "2016-06-08 00:00:00", "repo_name": "reddit-archive/reddit", "stars": 16744, "license": "other", "language": "python", "size": 40093}
{"docstring": "# Messages that have been spammed are still valid afters\n", "func_signal": "def valid_after(self, after):\n", "code": "w = self.convert_items((after,))[0]\nreturn MessageBuilder._viewable_message(self, w)", "path": "reddit/r2/r2/models/builder.py", "commit_date": "2017-02-21 00:00:00", "repo_name": "reddit-archive/reddit", "stars": 16744, "license": "other", "language": "python", "size": 40093}
{"docstring": "\"\"\"Start the tree from the requested children.\"\"\"\n\n", "func_signal": "def get_initial_candidates(self, comment_tree):\n", "code": "children = [\n    comment_id for comment_id in self.children\n    if comment_id in comment_tree.depth\n]\n\nif children:\n    children_depth = min(\n        comment_tree.depth[comment_id] for comment_id in children)\n\n    children = [\n        comment_id for comment_id in children\n        if comment_tree.depth[comment_id] == children_depth\n    ]\n\ninitial_candidates = children\n\n# BUG: current viewing depth isn't considered, so requesting children\n# of a deep comment can return nothing. the fix is to send the current\n# offset_depth along with the MoreChildren request\noffset_depth = 0\n\nreturn initial_candidates, offset_depth", "path": "reddit/r2/r2/models/builder.py", "commit_date": "2017-02-21 00:00:00", "repo_name": "reddit-archive/reddit", "stars": 16744, "license": "other", "language": "python", "size": 40093}
{"docstring": "\"\"\"Stop being friends with a user.\"\"\"\n", "func_signal": "def DELETE_friends(self, friend_rel):\n", "code": "c.user.remove_friend(friend_rel._thing2)\nif c.user.gold:\n    c.user.friend_rels_cache(_update=True)\nresponse.status = 204", "path": "reddit/r2/r2/controllers/apiv1/user.py", "commit_date": "2016-06-08 00:00:00", "repo_name": "reddit-archive/reddit", "stars": 16744, "license": "other", "language": "python", "size": 40093}
{"docstring": "\"\"\"Filter out the comments we don't want to show in QA sort.\n\nQA sort only displays comments that are:\n1. Top-level\n2. Responses from the OP(s)\n3. Within one level of an OP reply\n4. Distinguished\n\nAll ancestors of comments meeting the above rules will also be shown.\nThis ensures the question responded to by OP is shown.\n\n\"\"\"\n\n", "func_signal": "def _get_comment_order(self):\n", "code": "comment_tuples = CommentOrdererBase.get_comment_order(self)\nif not comment_tuples:\n    return comment_tuples\nelif isinstance(comment_tuples[-1], MissingChildrenTuple):\n    missing_children_tuple = comment_tuples.pop()\nelse:\n    missing_children_tuple = None\n\nspecial_responder_ids = self.link.responder_ids\n\n# unfortunately we need to look up all the Comments for QA\ncomment_ids = {ct.comment_id for ct in comment_tuples}\ncomments_by_id = Comment._byID(comment_ids, data=True)\n\n# figure out which comments will be kept (all others are discarded)\nkept_comment_ids = set()\nfor comment_tuple in comment_tuples:\n    if comment_tuple.depth == 0:\n        kept_comment_ids.add(comment_tuple.comment_id)\n        continue\n\n    comment = comments_by_id[comment_tuple.comment_id]\n    parent = comments_by_id[comment.parent_id] if comment.parent_id else None\n\n    if comment.author_id in special_responder_ids:\n        kept_comment_ids.add(comment_tuple.comment_id)\n        continue\n\n    if parent and parent.author_id in special_responder_ids:\n        kept_comment_ids.add(comment_tuple.comment_id)\n        continue\n\n    if hasattr(comment, \"distinguished\") and comment.distinguished != \"no\":\n        kept_comment_ids.add(comment_tuple.comment_id)\n        continue\n\n# add all ancestors to kept_comment_ids\nfor comment_id in sorted(kept_comment_ids):\n    # sort the comments so we start with the most root level comments\n    comment = comments_by_id[comment_id]\n    parent_id = comment.parent_id\n\n    counter = 0\n    while (parent_id and\n                parent_id not in kept_comment_ids and\n                counter < g.max_comment_parent_walk):\n        kept_comment_ids.add(parent_id)\n        counter += 1\n\n        comment = comments_by_id[parent_id]\n        parent_id = comment.parent_id\n\n# remove all comment tuples that aren't in kept_comment_ids\ncomment_tuples = [comment_tuple for comment_tuple in comment_tuples\n    if comment_tuple.comment_id in kept_comment_ids\n]\n\nif missing_children_tuple:\n    comment_tuples.append(missing_children_tuple)\n\nreturn comment_tuples", "path": "reddit/r2/r2/models/builder.py", "commit_date": "2017-02-21 00:00:00", "repo_name": "reddit-archive/reddit", "stars": 16744, "license": "other", "language": "python", "size": 40093}
{"docstring": "# we don't really care about getting detailed timings here, the entire\n# process will be timed by the caller\n", "func_signal": "def write_comment_orders(link):\n", "code": "timer = SimpleSillyStub()\n\nprecomputed_sorts = set()\nfor sort_name in get_active_sort_orders_for_link(link):\n    sort = SORT_OPERATOR_BY_NAME.get(sort_name)\n    if not sort:\n        continue\n\n    if sort_name == \"qa\":\n        QACommentOrderer.write_cache(link, sort, timer)\n    else:\n        CommentOrderer.write_cache(link, sort, timer)\n\n    precomputed_sorts.add(sort_name)\n\nif precomputed_sorts:\n    g.stats.simple_event(\"CommentOrderer.write_comment_orders.write\")\nelse:\n    g.stats.simple_event(\"CommentOrderer.write_comment_orders.noop\")\n\n# replace empty set with None to match the Link._defaults value\nlink.precomputed_sorts = precomputed_sorts or None\nlink._commit()", "path": "reddit/r2/r2/models/builder.py", "commit_date": "2017-02-21 00:00:00", "repo_name": "reddit-archive/reddit", "stars": 16744, "license": "other", "language": "python", "size": 40093}
{"docstring": "\"\"\"whether or not to skip any item regardless of whether the builder\nwas contructed with skip=true\"\"\"\n", "func_signal": "def must_skip(self, item):\n", "code": "user = c.user if c.user_is_loggedin else None\n\nif hasattr(item, \"promoted\") and item.promoted is not None:\n    return False\n\n# can_view_slow only exists for Messages, but checking was_comment\n# is also necessary because items may also be comments that are being\n# viewed from the inbox page where their render class is overridden.\n# This check needs to be done before looking at whether they can view\n# the subreddit, or modmail to/from private subreddits that the user\n# doesn't have access to will be skipped.\nif hasattr(item, 'can_view_slow') and not item.was_comment:\n    return not item.can_view_slow()\n\nif hasattr(item, 'subreddit') and not item.subreddit.can_view(user):\n    return True", "path": "reddit/r2/r2/models/builder.py", "commit_date": "2017-02-21 00:00:00", "repo_name": "reddit-archive/reddit", "stars": 16744, "license": "other", "language": "python", "size": 40093}
{"docstring": "\"\"\"Iterates over the items returned by get_items\"\"\"\n", "func_signal": "def item_iter(self, a):\n", "code": "for i in a[0]:\n    yield i", "path": "reddit/r2/r2/models/builder.py", "commit_date": "2017-02-21 00:00:00", "repo_name": "reddit-archive/reddit", "stars": 16744, "license": "other", "language": "python", "size": 40093}
{"docstring": "# doesn't use the default keep_item because we want to keep\n# things that were voted on, even if they've chosen to hide\n# them in normal listings\n", "func_signal": "def keep_item(self, item):\n", "code": "user = c.user if c.user_is_loggedin else None\n\nif item._spam or item._deleted:\n    return False\n# If checking (wrapped) links, filter out banned subreddits\nelif hasattr(item, 'subreddit') and item.subreddit.spammy():\n    return False\nelif (hasattr(item, 'subreddit') and\n      not c.user_is_admin and\n      not item.subreddit.is_exposed(user)):\n    return False\nelif (self.skip_deleted_authors and\n      getattr(item, \"author\", None) and item.author._deleted):\n    return False\nelif isinstance(item.lookups[0], Subreddit) and not item.is_exposed(user):\n    return False\n\n# show NSFW to API and RSS users unless obey_over18=true\nis_api_or_rss = (c.render_style in API_TYPES\n                 or c.render_style in RSS_TYPES)\nif is_api_or_rss:\n    include_over18 = not c.obey_over18 or c.over18\nelif feature.is_enabled('safe_search'):\n    include_over18 = c.over18\nelse:\n    include_over18 = True\n\nis_nsfw = (item.over_18 or\n    (hasattr(item, 'subreddit') and item.subreddit.over_18))\nif is_nsfw and not include_over18:\n    return False\n\nreturn True", "path": "reddit/r2/r2/models/builder.py", "commit_date": "2017-02-21 00:00:00", "repo_name": "reddit-archive/reddit", "stars": 16744, "license": "other", "language": "python", "size": 40093}
{"docstring": "\"\"\"Load known emojis from a directory, and return dictionary\nof PIL Image objects correspodent to the loaded emojis.\nEach emoji is resized to the CHAR_HEIGHT size.\n\"\"\"\n\n", "func_signal": "def _load_emojilib():\n", "code": "emojilib = {}\nfor filename in glob.glob(\"share/emoji/*.png\"):\n    character = os.path.basename(filename)[:-4]\n    emojilib[character] = \\\n        Image.open(filename).resize((CHAR_HEIGHT, CHAR_HEIGHT))\nreturn emojilib", "path": "wttr.in/lib/fmt/png.py", "commit_date": "2020-04-25 00:00:00", "repo_name": "chubin/wttr.in", "stars": 23351, "license": "apache-2.0", "language": "python", "size": 1742}
{"docstring": "\"\"\"\nRender format `line` using `data`\n\"\"\"\n\n", "func_signal": "def render_line(line, data, query):\n", "code": "def get_local_time_of():\n\n    location = data[\"location\"]\n    geo_data = get_geodata(location)\n\n    city = LocationInfo()\n    city.latitude = geo_data[\"latitude\"]\n    city.longitude = geo_data[\"longitude\"]\n    city.timezone = geo_data[\"timezone\"]\n\n    timezone = city.timezone\n\n    local_tz = pytz.timezone(timezone)\n\n    datetime_day_start = datetime.datetime.now()\\\n            .replace(hour=0, minute=0, second=0, microsecond=0)\n    current_sun = sun(city.observer, date=datetime_day_start)\n\n    local_time_of = lambda x: current_sun[x]\\\n                                .replace(tzinfo=pytz.utc)\\\n                                .astimezone(local_tz)\\\n                                .strftime(\"%H:%M:%S\")\n    return local_time_of\n\ndef render_symbol(match):\n    \"\"\"\n    Render one format symbol from re `match`\n    using `data` from external scope.\n    \"\"\"\n\n    symbol_string = match.group(0)\n    symbol = symbol_string[-1]\n\n    if symbol in FORMAT_SYMBOL:\n        render_function = FORMAT_SYMBOL[symbol]\n        return render_function(data, query)\n    if symbol in FORMAT_SYMBOL_ASTRO and local_time_of is not None:\n        render_function = FORMAT_SYMBOL_ASTRO[symbol]\n        return render_function(data, query, local_time_of)\n\n    return ''\n\ntemplate_regexp = r'%[a-zA-Z]'\nfor template_code in re.findall(template_regexp, line):\n    if template_code.lstrip(\"%\") in FORMAT_SYMBOL_ASTRO:\n        local_time_of = get_local_time_of()\n        break\n\nreturn re.sub(template_regexp, render_symbol, line)", "path": "wttr.in/lib/view/line.py", "commit_date": "2020-11-07 00:00:00", "repo_name": "chubin/wttr.in", "stars": 23351, "license": "apache-2.0", "language": "python", "size": 1742}
{"docstring": "\"\"\"Strips empty spaces from behind and from the right side.\n(from the right side is not yet implemented)\n\"\"\"\n\n", "func_signal": "def _strip_buf(buf):\n", "code": "def empty_line(line):\n    \"Returns True if the line consists from spaces\"\n    return all(x.data == ' ' for x in line)\n\ndef line_len(line):\n    \"Returns len of the line excluding spaces from the right\"\n\n    last_pos = len(line)\n    while last_pos > 0 and line[last_pos-1].data == ' ':\n        last_pos -= 1\n    return last_pos\n\nnumber_of_lines = 0\nfor line in buf[::-1]:\n    if not empty_line(line):\n        break\n    number_of_lines += 1\n\nif number_of_lines:\n    buf = buf[:-number_of_lines]\n\nmax_len = max(line_len(x) for x in buf)\nbuf = [line[:max_len] for line in buf]\n\nreturn buf", "path": "wttr.in/lib/fmt/png.py", "commit_date": "2020-04-25 00:00:00", "repo_name": "chubin/wttr.in", "stars": 23351, "license": "apache-2.0", "language": "python", "size": 1742}
{"docstring": "\"\"\"\nprecipitation (p)\n\"\"\"\n\n", "func_signal": "def render_precipitation(data, query):\n", "code": "answer = data.get('precipMM', '')\nif answer:\n    answer += 'mm'\nreturn answer", "path": "wttr.in/lib/view/line.py", "commit_date": "2020-11-07 00:00:00", "repo_name": "chubin/wttr.in", "stars": 23351, "license": "apache-2.0", "language": "python", "size": 1742}
{"docstring": "\"\"\"Renders rendered pyte buffer `buf` and list of workaround `graphemes`\nto a PNG file, and return its content\n\"\"\"\n\n", "func_signal": "def _gen_term(buf, graphemes, options=None):\n", "code": "if not options:\n    options = {}\n\ncurrent_grapheme = 0\n\nbuf = _strip_buf(buf)\ncols = max(len(x) for x in buf)\nrows = len(buf)\n\nimage = Image.new('RGB', (cols * CHAR_WIDTH, rows * CHAR_HEIGHT))\n\nbuf = buf[-ROWS:]\n\ndraw = ImageDraw.Draw(image)\nfont = {}\nfor cat in FONT_CAT:\n    font[cat] = ImageFont.truetype(FONT_CAT[cat], FONT_SIZE)\n\nemojilib = _load_emojilib()\n\nx_pos = 0\ny_pos = 0\nfor line in buf:\n    x_pos = 0\n    for char in line:\n        current_color = _color_mapping(char.fg)\n        if char.bg != 'default':\n            draw.rectangle(\n                ((x_pos, y_pos),\n                 (x_pos+CHAR_WIDTH, y_pos+CHAR_HEIGHT)),\n                fill=_color_mapping(char.bg))\n\n        if char.data == \"!\":\n            try:\n                data = graphemes[current_grapheme]\n            except IndexError:\n                pass\n            current_grapheme += 1\n        else:\n            data = char.data\n\n        if data:\n            cat = _script_category(data[0])\n            if cat not in font:\n                globals.log(\"Unknown font category: %s\" % cat)\n            if cat == 'Emoji' and emojilib.get(data):\n                image.paste(emojilib.get(data), (x_pos, y_pos))\n            else:\n                draw.text(\n                    (x_pos, y_pos),\n                    data,\n                    font=font.get(cat, font.get('default')),\n                    fill=current_color)\n\n        x_pos += CHAR_WIDTH * constants.WEATHER_SYMBOL_WIDTH_VTE.get(data, 1)\n    y_pos += CHAR_HEIGHT\n\nif 'transparency' in options:\n    transparency = options.get('transparency', '255')\n    try:\n        transparency = int(transparency)\n    except ValueError:\n        transparency = 255\n\n    if transparency < 0:\n        transparency = 0\n\n    if transparency > 255:\n        transparency = 255\n\n    image = image.convert(\"RGBA\")\n    datas = image.getdata()\n\n    new_data = []\n    for item in datas:\n        new_item = tuple(list(item[:3]) + [transparency])\n        new_data.append(new_item)\n\n    image.putdata(new_data)\n\nimg_bytes = io.BytesIO()\nimage.save(img_bytes, format=\"png\")\nreturn img_bytes.getvalue()", "path": "wttr.in/lib/fmt/png.py", "commit_date": "2020-04-25 00:00:00", "repo_name": "chubin/wttr.in", "stars": 23351, "license": "apache-2.0", "language": "python", "size": 1742}
{"docstring": "\"\"\"Convert pyte color to PIL color\n\nReturn: tuple of color values (R,G,B)\n\"\"\"\n\n", "func_signal": "def _color_mapping(color):\n", "code": "if color == 'default':\n    return 'lightgray'\nif color in ['green', 'black', 'cyan', 'blue', 'brown']:\n    return color\ntry:\n    return (\n        int(color[0:2], 16),\n        int(color[2:4], 16),\n        int(color[4:6], 16))\nexcept (ValueError, IndexError):\n    # if we do not know this color and it can not be decoded as RGB,\n    # print it and return it as it is (will be displayed as black)\n    # print color\n    return color\nreturn color", "path": "wttr.in/lib/fmt/png.py", "commit_date": "2020-04-25 00:00:00", "repo_name": "chubin/wttr.in", "stars": 23351, "license": "apache-2.0", "language": "python", "size": 1742}
{"docstring": "\"\"\"\ntemperature (t)\n\"\"\"\n\n", "func_signal": "def render_temperature(data, query):\n", "code": "if query.get('use_imperial', False):\n    temperature = u'%s\u00b0F' % data['temp_F']\nelse:\n    temperature = u'%s\u00b0C' % data['temp_C']\n\nif temperature[0] != '-':\n    temperature = '+' + temperature\n\nreturn temperature", "path": "wttr.in/lib/view/line.py", "commit_date": "2020-11-07 00:00:00", "repo_name": "chubin/wttr.in", "stars": 23351, "license": "apache-2.0", "language": "python", "size": 1742}
{"docstring": "\"\"\"Returns category of a Unicode character\n\nPossible values:\n    default, Cyrillic, Greek, Han, Hiragana\n\"\"\"\n\n", "func_signal": "def _script_category(char):\n", "code": "if char in emoji.UNICODE_EMOJI:\n    return \"Emoji\"\n\ncat = unicodedata2.script_cat(char)[0]\nif char == u'\uff1a':\n    return 'Han'\nif cat in ['Latin', 'Common']:\n    return 'default'\nreturn cat", "path": "wttr.in/lib/fmt/png.py", "commit_date": "2020-04-25 00:00:00", "repo_name": "chubin/wttr.in", "stars": 23351, "license": "apache-2.0", "language": "python", "size": 1742}
{"docstring": "\"\"\"\nhumidity (h)\n\"\"\"\n\n", "func_signal": "def render_humidity(data, query):\n", "code": "humidity = data.get('humidity', '')\nif humidity:\n    humidity += '%'\nreturn humidity", "path": "wttr.in/lib/view/line.py", "commit_date": "2020-11-07 00:00:00", "repo_name": "chubin/wttr.in", "stars": 23351, "license": "apache-2.0", "language": "python", "size": 1742}
{"docstring": "\"\"\"\ncondition_fullname (C)\n\"\"\"\n\n", "func_signal": "def render_condition_fullname(data, query):\n", "code": "found = None\nfor key, val in data.items():\n    if key.startswith('lang_'):\n        found = val\n        break\nif not found:\n    found = data['weatherDesc']\n\ntry:\n    weather_condition = found[0]['value']\nexcept KeyError:\n    weather_condition = ''\n\nreturn weather_condition", "path": "wttr.in/lib/view/line.py", "commit_date": "2020-11-07 00:00:00", "repo_name": "chubin/wttr.in", "stars": 23351, "license": "apache-2.0", "language": "python", "size": 1742}
{"docstring": "\"\"\"\nwind (w)\n\"\"\"\n\n", "func_signal": "def render_wind(data, query):\n", "code": "try:\n    degree = data[\"winddirDegree\"]\nexcept KeyError:\n    degree = \"\"\n\ntry:\n    degree = int(degree)\nexcept ValueError:\n    degree = \"\"\n\nif degree:\n    wind_direction = WIND_DIRECTION[int(((degree+22.5)%360)/45.0)]\nelse:\n    wind_direction = \"\"\n\nif query.get('use_ms_for_wind', False):\n    unit = 'm/s'\n    wind = u'%s%.1f%s' % (wind_direction, float(data['windspeedKmph'])/36.0*10.0, unit)\nelif query.get('use_imperial', False):\n    unit = 'mph'\n    wind = u'%s%s%s' % (wind_direction, data['windspeedMiles'], unit)\nelse:\n    unit = 'km/h'\n    wind = u'%s%s%s' % (wind_direction, data['windspeedKmph'], unit)\n\nreturn wind", "path": "wttr.in/lib/view/line.py", "commit_date": "2020-11-07 00:00:00", "repo_name": "chubin/wttr.in", "stars": 23351, "license": "apache-2.0", "language": "python", "size": 1742}
{"docstring": "\"\"\"Render `text` (terminal sequence) in a PNG file\npaying attention to passed command line `options`.\n\nReturn: file content\n\"\"\"\n\n", "func_signal": "def render_ansi(text, options=None):\n", "code": "screen = pyte.screens.Screen(COLS, ROWS)\nscreen.set_mode(pyte.modes.LNM)\nstream = pyte.Stream(screen)\n\ntext, graphemes = _fix_graphemes(text)\nstream.feed(text)\n\nbuf = sorted(screen.buffer.items(), key=lambda x: x[0])\nbuf = [[x[1] for x in sorted(line[1].items(), key=lambda x: x[0])] for line in buf]\n\nreturn _gen_term(buf, graphemes, options=options)", "path": "wttr.in/lib/fmt/png.py", "commit_date": "2020-04-25 00:00:00", "repo_name": "chubin/wttr.in", "stars": 23351, "license": "apache-2.0", "language": "python", "size": 1742}
{"docstring": "\"\"\"\nFormat information about current weather `data` for `location`\nwith specified in `format_line` format\n\"\"\"\n\n", "func_signal": "def format_weather_data(query, parsed_query, data):\n", "code": "if 'data' not in data:\n    return 'Unknown location; please try ~%s' % parsed_query[\"location\"]\n\nformat_line = parsed_query.get(\"view\", \"\")\nif format_line in PRECONFIGURED_FORMAT:\n    format_line = PRECONFIGURED_FORMAT[format_line]\n\nif format_line == \"j1\":\n    return render_json(data['data'])\nif format_line == \"p1\":\n    return prometheus.render_prometheus(data['data'])\nif format_line[:2] == \"v2\":\n    return v2.main(query, parsed_query, data)\nif format_line[:2] == \"v3\":\n    return v3.main(query, parsed_query, data)\n\ncurrent_condition = data['data']['current_condition'][0]\ncurrent_condition['location'] = parsed_query[\"location\"]\ncurrent_condition['override_location'] = parsed_query[\"override_location_name\"]\noutput = render_line(format_line, current_condition, query)\noutput = output.rstrip(\"\\n\").replace(r\"\\n\", \"\\n\")\nreturn output", "path": "wttr.in/lib/view/line.py", "commit_date": "2020-11-07 00:00:00", "repo_name": "chubin/wttr.in", "stars": 23351, "license": "apache-2.0", "language": "python", "size": 1742}
{"docstring": "\"\"\"\nExtract long graphemes sequences that can't be handled\nby pyte correctly because of the bug pyte#131.\nGraphemes are omited and replaced with placeholders,\nand returned as a list.\n\nReturn:\n    text_without_graphemes, graphemes\n\"\"\"\n\n", "func_signal": "def _fix_graphemes(text):\n", "code": "output = \"\"\ngraphemes = []\n\nfor gra in grapheme.graphemes(text):\n    if len(gra) > 1:\n        character = \"!\"\n        graphemes.append(gra)\n    else:\n        character = gra\n    output += character\n\nreturn output, graphemes", "path": "wttr.in/lib/fmt/png.py", "commit_date": "2020-04-25 00:00:00", "repo_name": "chubin/wttr.in", "stars": 23351, "license": "apache-2.0", "language": "python", "size": 1742}
{"docstring": "\"\"\"\nReturn 1line weather information for `location`\nin format `line_format`\n\"\"\"\n", "func_signal": "def wttr_line(query, parsed_query):\n", "code": "location = parsed_query['location']\nlang = parsed_query['lang']\n\ndata = get_weather_data(location, lang)\noutput = format_weather_data(query, parsed_query, data)\nreturn output", "path": "wttr.in/lib/view/line.py", "commit_date": "2020-11-07 00:00:00", "repo_name": "chubin/wttr.in", "stars": 23351, "license": "apache-2.0", "language": "python", "size": 1742}
{"docstring": "\"\"\"\nFunction for standalone module usage\n\"\"\"\n\n", "func_signal": "def main():\n", "code": "location = sys.argv[1]\nquery = {\n    'line': sys.argv[2],\n    }\nparsed_query = {\n    \"location\": location,\n    \"orig_location\": location,\n    \"language\": \"en\",\n    \"format\": \"v2\",\n    }\n\nsys.stdout.write(wttr_line(query, parsed_query))", "path": "wttr.in/lib/view/line.py", "commit_date": "2020-11-07 00:00:00", "repo_name": "chubin/wttr.in", "stars": 23351, "license": "apache-2.0", "language": "python", "size": 1742}
{"docstring": "\"\"\"\npressure (P)\n\"\"\"\n\n", "func_signal": "def render_pressure(data, query):\n", "code": "answer = data.get('pressure', '')\nif answer:\n    answer += 'hPa'\nreturn answer", "path": "wttr.in/lib/view/line.py", "commit_date": "2020-11-07 00:00:00", "repo_name": "chubin/wttr.in", "stars": 23351, "license": "apache-2.0", "language": "python", "size": 1742}
{"docstring": "\"\"\"moonday(M)\nAn number describing the phase of the moon (days after the New Moon)\n\"\"\"\n", "func_signal": "def render_moonday(_, query):\n", "code": "moon_phase = moon.phase(date=datetime.datetime.today())\nreturn str(int(moon_phase))", "path": "wttr.in/lib/view/line.py", "commit_date": "2020-11-07 00:00:00", "repo_name": "chubin/wttr.in", "stars": 23351, "license": "apache-2.0", "language": "python", "size": 1742}
{"docstring": "\"\"\"moonpahse(m)\nA symbol describing the phase of the moon\n\"\"\"\n", "func_signal": "def render_moonphase(_, query):\n", "code": "moon_phase = moon.phase(date=datetime.datetime.today())\nmoon_index = int(int(32.0*moon_phase/28+2)%32/4)\nreturn MOON_PHASES[moon_index]", "path": "wttr.in/lib/view/line.py", "commit_date": "2020-11-07 00:00:00", "repo_name": "chubin/wttr.in", "stars": 23351, "license": "apache-2.0", "language": "python", "size": 1742}
{"docstring": "\"\"\"Emoji encoded weather condition (c)\n\"\"\"\n\n", "func_signal": "def render_condition(data, query):\n", "code": "weather_condition = WEATHER_SYMBOL[WWO_CODE[data['weatherCode']]]\nspaces = \" \"*(WEATHER_SYMBOL_WIDTH_VTE.get(weather_condition) - 1)\n\nreturn weather_condition + spaces", "path": "wttr.in/lib/view/line.py", "commit_date": "2020-11-07 00:00:00", "repo_name": "chubin/wttr.in", "stars": 23351, "license": "apache-2.0", "language": "python", "size": 1742}
{"docstring": "\"\"\"Merge event records together if they are close enough\n\nThe time elapsed between two consecutive event records returned by this\nfunction is guaranteed to be at least min_rec_duration.\n\nThe duration of each record is also computed. Any record with a duration\ngreater than `max_rec_duration` will see its duration reduce to this value.\nThe duration of the last record can't be computed and is simply set to\n`last_rec_duration`.\n\n:param event_records: Sequence of records in asciicast v2 format\n:param min_rec_duration: Minimum time between two records returned by the\nfunction in milliseconds.\n:param max_rec_duration: Maximum duration of a record in milliseconds\n:param last_rec_duration: Duration of the last record in milliseconds\n:return: Sequence of records with duration\n\"\"\"\n# TODO: itertools.accumulate?\n", "func_signal": "def _group_by_time(event_records, min_rec_duration, max_rec_duration, last_rec_duration):\n", "code": "current_string = ''\ncurrent_time = 0\ndropped_time = 0\n\nif max_rec_duration:\n    max_rec_duration /= 1000\n\nfor event_record in event_records:\n    assert isinstance(event_record, AsciiCastV2Event)\n    # Silently ignoring the duration on input records is a source\n    # of confusion so fail hard if the duration is set\n    assert event_record.duration is None\n    if event_record.event_type != 'o':\n        continue\n\n    time_between_events = event_record.time - (current_time + dropped_time)\n    if time_between_events * 1000 >= min_rec_duration:\n        if max_rec_duration:\n            if max_rec_duration < time_between_events:\n                dropped_time += time_between_events - max_rec_duration\n                time_between_events = max_rec_duration\n        accumulator_event = AsciiCastV2Event(time=current_time,\n                                             event_type='o',\n                                             event_data=current_string,\n                                             duration=time_between_events)\n        yield accumulator_event\n        current_string = ''\n        current_time += time_between_events\n\n    current_string += event_record.event_data\n\naccumulator_event = AsciiCastV2Event(time=current_time,\n                                     event_type='o',\n                                     event_data=current_string,\n                                     duration=last_rec_duration / 1000)\nyield accumulator_event", "path": "termtosvg/termtosvg/term.py", "commit_date": "2019-05-12 00:00:00", "repo_name": "nbedos/termtosvg", "stars": 9687, "license": "bsd-3-clause", "language": "python", "size": 1400}
{"docstring": "\"\"\"Return mapping between the name of a template and the SVG template itself\"\"\"\n", "func_signal": "def default_templates():\n", "code": "templates = {}\nfor template_name in DEFAULT_TEMPLATES_NAMES:\n    pkg_template_path = '{}/{}'.format(PKG_TEMPLATE_PATH, template_name)\n    bstream = pkgutil.get_data(__name__, pkg_template_path)\n    suffix = '.svg'\n    if template_name.endswith(suffix):\n        templates[template_name[:-len(suffix)]] = bstream\n    else:\n        templates[template_name] = bstream\n\nreturn templates", "path": "termtosvg/termtosvg/config.py", "commit_date": "2019-11-16 00:00:00", "repo_name": "nbedos/termtosvg", "stars": 9687, "license": "bsd-3-clause", "language": "python", "size": 1400}
{"docstring": "\"\"\"Return a list of 'rect' tags representing the background of 'screen_line'\n\nIf consecutive cells have the same background color, a single 'rect' tag is\nreturned for all these cells.\nIf a cell background uses default_bg_color, no 'rect' will be generated for\nthis cell since the default background is always displayed.\n\n:param screen_line: Mapping between column numbers and CharacterCells\n:param height: Vertical position of the line on the screen in pixels\n:param cell_height: Height of the a character cell in pixels\n:param cell_width: Width of a character cell in pixels\n\"\"\"\n", "func_signal": "def _render_line_bg_colors(screen_line, height, cell_height, cell_width):\n", "code": "non_default_bg_cells = [(column, cell) for (column, cell)\n                        in sorted(screen_line.items())\n                        if cell.background_color != 'background']\n\nkey = ConsecutiveWithSameAttributes(['background_color'])\nrect_tags = [\n    _make_rect_tag(\n        column,\n        wcswidth(''.join(t[1].text for t in group)),\n        height,\n        cell_width,\n        cell_height,\n        attributes['background_color']\n    ) for (column, attributes), group in groupby(non_default_bg_cells, key)]\n\nreturn rect_tags", "path": "termtosvg/termtosvg/anim.py", "commit_date": "2019-07-06 00:00:00", "repo_name": "nbedos/termtosvg", "stars": 9687, "license": "bsd-3-clause", "language": "python", "size": 1400}
{"docstring": "\"\"\"Yield asciicast v2 records from the file\n\nThe records in the file may themselves be in either asciicast v1 or v2 format (although\nthere must be only one record format version in the file).\nRaise AsciiCastError if a record is invalid\"\"\"\n", "func_signal": "def read_records(filename):\n", "code": "try:\n    with open(filename, 'r') as cast_file:\n        for line in cast_file:\n            yield AsciiCastV2Record.from_json_line(line)\nexcept AsciiCastError:\n    with open(filename, 'r') as cast_file:\n        yield from _read_v1_records(cast_file.read())", "path": "termtosvg/termtosvg/asciicast.py", "commit_date": "2019-06-14 00:00:00", "repo_name": "nbedos/termtosvg", "stars": 9687, "license": "bsd-3-clause", "language": "python", "size": 1400}
{"docstring": "\"\"\"Return a tuple made of the geometry of the screen and a generator of\ninstances of TimedFrame computed from asciicast records\n\nAsciicast records are first coalesced so that the mininum duration between\ntwo frames is at least `min_frame_dur` milliseconds. Events with a duration\ngreater than `max_frame_dur` will see their duration reduced to that value.\n\nThe duration of all frames lasting until the end of the animation\nwill be adjusted so that the last frame of the animation lasts\n`last_frame_dur`\n\n:param records: Terminal session record in Asciicast v2 format\n:param min_frame_dur: Minimum frame duration in milliseconds (integer)\n:param min_frame_dur: Minimum frame duration in milliseconds (integer)\n:param max_frame_dur: Maximum frame duration in milliseconds (None or\ninteger)\n:param last_frame_dur: Duration of the last frame of the animation\n(integer)\n\"\"\"\n", "func_signal": "def timed_frames(records, min_frame_dur=1, max_frame_dur=None, last_frame_dur=1000):\n", "code": "if not isinstance(records, Iterator):\n    records = iter(records)\n\nheader = next(records)\nassert isinstance(header, AsciiCastV2Header)\n\nif not max_frame_dur and header.idle_time_limit:\n    max_frame_dur = int(header.idle_time_limit * 1000)\n\ndef generator():\n    screen = pyte.Screen(header.width, header.height)\n    stream = pyte.Stream(screen)\n    timed_records = _group_by_time(records, min_frame_dur, max_frame_dur,\n                                   last_frame_dur)\n\n    for record_ in timed_records:\n        assert isinstance(record_, AsciiCastV2Event)\n        for char in record_.event_data:\n            stream.feed(char)\n        yield TimedFrame(int(1000 * record_.time),\n                         int(1000 * record_.duration),\n                         _screen_buffer(screen))\n\nreturn (header.width, header.height), generator()", "path": "termtosvg/termtosvg/term.py", "commit_date": "2019-05-12 00:00:00", "repo_name": "nbedos/termtosvg", "stars": 9687, "license": "bsd-3-clause", "language": "python", "size": 1400}
{"docstring": "\"\"\"Create a CharacterCell from a pyte character\"\"\"\n", "func_signal": "def from_pyte(cls, char):\n", "code": "if char.fg == 'default':\n    text_color = 'foreground'\nelse:\n    if char.bold and not str(char.fg).startswith('bright'):\n        named_color = 'bright{}'.format(char.fg)\n    else:\n        named_color = char.fg\n\n    if named_color in NAMED_COLORS:\n        text_color = 'color{}'.format(NAMED_COLORS.index(named_color))\n    elif len(char.fg) == 6:\n        # HEXADECIMAL COLORS\n        # raise ValueError if char.fg is not an hexadecimal number\n        int(char.fg, 16)\n        text_color = '#{}'.format(char.fg)\n    else:\n        raise ValueError('Invalid foreground color: {}'.format(char.fg))\n\nif char.bg == 'default':\n    background_color = 'background'\nelif char.bg in NAMED_COLORS:\n    background_color = 'color{}'.format(NAMED_COLORS.index(char.bg))\nelif len(char.bg) == 6:\n    # Hexadecimal colors\n    # raise ValueError if char.bg is not an hexadecimal number\n    int(char.bg, 16)\n    background_color = '#{}'.format(char.bg)\nelse:\n    raise ValueError('Invalid background color')\n\nif char.reverse:\n    text_color, background_color = background_color, text_color\n\nreturn CharacterCell(char.data, text_color, background_color,\n                     char.bold, char.italics, char.underscore,\n                     char.strikethrough)", "path": "termtosvg/termtosvg/anim.py", "commit_date": "2019-07-06 00:00:00", "repo_name": "nbedos/termtosvg", "stars": 9687, "license": "bsd-3-clause", "language": "python", "size": 1400}
{"docstring": "# Ensure zero width characters in terminal output does not result\n# in Pyte dropping all following data\n# Issue https://github.com/nbedos/termtosvg/issues/89\n\n# test_text = \"e\ud83d\udd75\ufe0f\u200da\"\n", "func_signal": "def test_timed_frames_unprintable_chars(self):\n", "code": "test_text = (b'e' +\n             b'\\xf0\\x9f\\x95\\xb5' +  # sleuth emoji\n             b'\\xef\\xb8\\x8f' +      # variation selector 16\n             b'\\xe2\\x80\\x8d' +      # zero width joiner\n             b'a').decode('utf-8')  # character that should be preserved\n\nrecords = [\n    AsciiCastV2Header(version=2, width=80, height=24, theme=THEME),\n    AsciiCastV2Event(0, 'o', test_text, None),\n]\n_, events = term.timed_frames(records, 1, None, last_frame_dur=1000)\n\nframe = next(events)\ncharacters = ''.join(frame.buffer[0][col].text\n                     for col in frame.buffer[0])\n\n# Ensure data following sleuth emoji wasn't ignored\n# (rstrip() removes blank cursor character at end of line)\nself.assertEqual(characters.rstrip()[-1], test_text[-1])", "path": "termtosvg/termtosvg/tests/test_term.py", "commit_date": "2019-05-12 00:00:00", "repo_name": "nbedos/termtosvg", "stars": 9687, "license": "bsd-3-clause", "language": "python", "size": 1400}
{"docstring": "\"\"\"Resize template based on the number of rows and columns of the terminal\"\"\"\n", "func_signal": "def resize_template(template, geometry, cell_width, cell_height):\n", "code": "def scale(element, template_columns, template_rows, columns, rows):\n    \"\"\"Resize viewbox based on the number of rows and columns of the terminal\"\"\"\n    try:\n        viewbox = element.attrib['viewBox'].replace(',', ' ').split()\n    except KeyError:\n        raise TemplateError('Missing \"viewBox\" for element \"{}\"'.format(element))\n\n    vb_min_x, vb_min_y, vb_width, vb_height = [int(n) for n in viewbox]\n    vb_width += cell_width * (columns - template_columns)\n    vb_height += cell_height * (rows - template_rows)\n    element.attrib['viewBox'] = ' '.join(map(str, (vb_min_x, vb_min_y, vb_width, vb_height)))\n\n    scalable_attributes = {\n        'width': cell_width * (columns - template_columns),\n        'height': cell_height * (rows - template_rows)\n    }\n\n    for attribute, delta in scalable_attributes.items():\n        if attribute in element.attrib:\n            try:\n                element.attrib[attribute] = str(int(element.attrib[attribute]) + delta)\n            except ValueError:\n                raise TemplateError('\"{}\" attribute of {} must be in user units'\n                                    .format(attribute, element))\n    return element\n\ntry:\n    tree = etree.parse(io.BytesIO(template))\n    root = tree.getroot()\nexcept etree.Error as exc:\n    raise TemplateError('Invalid template') from exc\n\n# Extract the screen geometry which is saved in a private data portion of\n# the template\nsettings = root.find('.//{{{}}}defs/{{{}}}template_settings'\n                     .format(SVG_NS, TERMTOSVG_NS))\nif settings is None:\n    raise TemplateError('Missing \"template_settings\" element in definitions')\n\nsvg_geometry = settings.find('{{{}}}screen_geometry[@columns][@rows]'\n                             .format(TERMTOSVG_NS))\nif svg_geometry is None:\n    raise TemplateError('Missing \"screen_geometry\" element in \"template_settings\"')\n\nattributes_err_msg = ('Missing or invalid \"columns\" or \"rows\" attribute '\n                      'for element \"screen_geometry\": expected positive '\n                      'integers')\ntry:\n    template_columns = int(svg_geometry.attrib['columns'])\n    template_rows = int(svg_geometry.attrib['rows'])\nexcept (KeyError, ValueError) as exc:\n    raise TemplateError(attributes_err_msg) from exc\n\n# Update settings with real columns and rows values to preserve the scale\n# in case the animation serves as a template\ncolumns, rows = geometry\nsvg_geometry.attrib['columns'], svg_geometry.attrib['rows'] = (str(columns),\n                                                               str(rows))\n\nif template_rows <= 0 or template_columns <= 0:\n    raise TemplateError(attributes_err_msg)\n\n# Scale the viewBox of the root svg element based on the size of the screen\n# and the size registered in the template\nscale(root, template_columns, template_rows, columns, rows)\n\n# Also scale the viewBox of the svg element with id 'screen'\nscreen = root.find('.//{{{namespace}}}svg[@id=\"screen\"]'\n                   .format(namespace=SVG_NS))\nif screen is None:\n    raise TemplateError('svg element with id \"screen\" not found')\nscale(screen, template_columns, template_rows, columns, rows)\n\nreturn root", "path": "termtosvg/termtosvg/anim.py", "commit_date": "2019-07-06 00:00:00", "repo_name": "nbedos/termtosvg", "stars": 9687, "license": "bsd-3-clause", "language": "python", "size": 1400}
{"docstring": "\"\"\"Raise ValueError if 'screen_geometry' does not conform to <integer>x<integer> format\"\"\"\n", "func_signal": "def validate_geometry(screen_geometry):\n", "code": "columns, rows = [int(value) for value in screen_geometry.lower().split('x')]\nif columns <= 0 or rows <= 0:\n    raise ValueError('Invalid value for screen-geometry option: \"{}\"'.format(screen_geometry))\nreturn columns, rows", "path": "termtosvg/termtosvg/config.py", "commit_date": "2019-11-16 00:00:00", "repo_name": "nbedos/termtosvg", "stars": 9687, "license": "bsd-3-clause", "language": "python", "size": 1400}
{"docstring": "\"\"\"Raise AsciiCastError if line is not a valid asciicast v2 record\"\"\"\n", "func_signal": "def from_json_line(cls, line):\n", "code": "try:\n    json_dict = json.loads(line)\nexcept json.JSONDecodeError as exc:\n    raise AsciiCastError from exc\nif isinstance(json_dict, dict):\n    return AsciiCastV2Header.from_json_line(line)\nif isinstance(json_dict, list):\n    return AsciiCastV2Event.from_json_line(line)\ntruncated_line = line if len(line) < 20 else '{}...'.format(line[:20])\nraise AsciiCastError('Unknown record type: \"{}\"'.format(truncated_line))", "path": "termtosvg/termtosvg/asciicast.py", "commit_date": "2019-06-14 00:00:00", "repo_name": "nbedos/termtosvg", "stars": 9687, "license": "bsd-3-clause", "language": "python", "size": 1400}
{"docstring": "\"\"\"Build SVG text element based on content and style attributes\"\"\"\n", "func_signal": "def _make_text_tag(column, attributes, text, cell_width):\n", "code": "text_tag_attributes = {\n    'x': str(column * cell_width),\n    'textLength': str(wcswidth(text) * cell_width),\n}\nif attributes['bold']:\n    text_tag_attributes['font-weight'] = 'bold'\n\nif attributes['italics']:\n    text_tag_attributes['font-style'] = 'italic'\n\ndecoration = ''\nif attributes['underscore']:\n    decoration = 'underline'\nif attributes['strikethrough']:\n    decoration += ' line-through'\nif decoration:\n    text_tag_attributes['text-decoration'] = decoration\n\nif attributes['color'].startswith('#'):\n    text_tag_attributes['fill'] = attributes['color']\nelse:\n    text_tag_attributes['class'] = attributes['color']\n\ntext_tag = etree.Element('text', text_tag_attributes)\ntext_tag.text = text\nreturn text_tag", "path": "termtosvg/termtosvg/anim.py", "commit_date": "2019-07-06 00:00:00", "repo_name": "nbedos/termtosvg", "stars": 9687, "license": "bsd-3-clause", "language": "python", "size": 1400}
{"docstring": "# Use pipes in lieu of stdin and stdout\n", "func_signal": "def test_record(self):\n", "code": "fd_in_read, fd_in_write = os.pipe()\nfd_out_read, fd_out_write = os.pipe()\n\nlines = 24\ncolumns = 80\n\npid = os.fork()\nif pid == 0:\n    # Child process\n    for line in commands:\n        os.write(fd_in_write, line.encode('utf-8'))\n        time.sleep(0.060)\n    os._exit(0)\n\n# Parent process\nwith term.TerminalMode(fd_in_read):\n    for _ in term.record(['sh'], columns, lines, fd_in_read, fd_out_write):\n        pass\n\nos.waitpid(pid, 0)\nfor fd in fd_in_read, fd_in_write, fd_out_read, fd_out_write:\n    os.close(fd)", "path": "termtosvg/termtosvg/tests/test_term.py", "commit_date": "2019-05-12 00:00:00", "repo_name": "nbedos/termtosvg", "stars": 9687, "license": "bsd-3-clause", "language": "python", "size": 1400}
{"docstring": "\"\"\"Validate an SVG file against SVG 1.1 Document Type Definition\"\"\"\n", "func_signal": "def validate_svg(svg_file):\n", "code": "package = __name__.split('.')[0]\ndtd_bytes = pkgutil.get_data(package, '/data/svg11-flat-20110816.dtd')\nwith io.BytesIO(dtd_bytes) as bstream:\n    dtd = etree.DTD(bstream)\n\ntry:\n    tree = etree.parse(svg_file)\n    for bad in tree.xpath('/svg:svg/svg:defs/termtosvg:template_settings',\n                          namespaces=NAMESPACES):\n        bad.getparent().remove(bad)\n    root = tree.getroot()\n    is_valid = dtd.validate(root)\nexcept etree.Error as exc:\n    raise ValueError('Invalid SVG file') from exc\n\nif not is_valid:\n    reason = dtd.error_log.filter_from_errors()[0]\n    raise ValueError('Invalid SVG file: {}'.format(reason))", "path": "termtosvg/termtosvg/anim.py", "commit_date": "2019-07-06 00:00:00", "repo_name": "nbedos/termtosvg", "stars": 9687, "license": "bsd-3-clause", "language": "python", "size": 1400}
{"docstring": "\"\"\"Return a list of 'text' elements representing the line of the screen\n\nConsecutive characters with the same styling attributes (text color, font\nweight...) are grouped together in a single text element.\n\n:param screen_line: Mapping between column numbers and characters\n:param cell_width: Width of a character cell in pixels\n\"\"\"\n", "func_signal": "def _render_characters(screen_line, cell_width):\n", "code": "line = sorted(screen_line.items())\nkey = ConsecutiveWithSameAttributes(['color', 'bold', 'italics', 'underscore', 'strikethrough'])\ntext_tags = [_make_text_tag(column, attributes, ''.join(c.text for _, c in group), cell_width)\n             for (column, attributes), group in groupby(line, key)]\n\nreturn text_tags", "path": "termtosvg/termtosvg/anim.py", "commit_date": "2019-07-06 00:00:00", "repo_name": "nbedos/termtosvg", "stars": 9687, "license": "bsd-3-clause", "language": "python", "size": 1400}
{"docstring": "\"\"\"Record raw input and output of a process\n\nThis function forks the current process. The child process runs the command\nspecified by 'process_args' which is a session leader and has a controlling\nterminal and is run in the background. The parent process, which runs in\nthe foreground, transmits data between the standard input, output and the\nchild process and logs it. From the user point of view, it appears they are\ncommunicating with the process they intend to record (through their\nterminal emulator) when in fact they communicate with our parent process\nwhich logs all data exchanges with the user\n\nThe implementation of this method is mostly copied from the pty.spawn\nfunction of the CPython standard library. It has been modified in order to\nmake the record function a generator.\nSee https://github.com/python/cpython/blob/master/Lib/pty.py\n\n:param process_args: List of arguments to run the process to be recorded\n:param columns: Initial number of columns of the terminal\n:param lines: Initial number of lines of the terminal\n:param input_fileno: File descriptor of the input data stream\n:param output_fileno: File descriptor of the output data stream\n\"\"\"\n", "func_signal": "def _record(process_args, columns, lines, input_fileno, output_fileno):\n", "code": "pid, master_fd = pty.fork()\nif pid == 0:\n    # Child process - this call never returns\n    os.execlp(process_args[0], *process_args)\n\n# Parent process\n# Set the terminal size for master_fd\nttysize = struct.pack(\"HHHH\", lines, columns, 0, 0)\nfcntl.ioctl(master_fd, termios.TIOCSWINSZ, ttysize)\n\ntry:\n    tty.setraw(input_fileno)\nexcept tty.error:\n    pass\n\nfor data, time in _capture_output(input_fileno, output_fileno, master_fd):\n    yield data, time\n\nos.close(master_fd)\n\n_, child_exit_status = os.waitpid(pid, 0)\nreturn child_exit_status", "path": "termtosvg/termtosvg/term.py", "commit_date": "2019-05-12 00:00:00", "repo_name": "nbedos/termtosvg", "stars": 9687, "license": "bsd-3-clause", "language": "python", "size": 1400}
{"docstring": "# Use pipes in lieu of stdin and stdout\n", "func_signal": "def run_main(args, process_input):\n", "code": "fd_in_read, fd_in_write = os.pipe()\nfd_out_read, fd_out_write = os.pipe()\n\npid = os.fork()\nif pid == 0:\n    # Child process\n    for line in process_input:\n        os.write(fd_in_write, line.encode('utf-8'))\n        time.sleep(0.060)\n    os._exit(0)\n\ntermtosvg.main.main(args, fd_in_read, fd_out_write)\n\nos.waitpid(pid, 0)\nfor fd in fd_in_read, fd_in_write, fd_out_read, fd_out_write:\n    os.close(fd)", "path": "termtosvg/termtosvg/tests/test_main.py", "commit_date": "2019-11-05 00:00:00", "repo_name": "nbedos/termtosvg", "stars": 9687, "license": "bsd-3-clause", "language": "python", "size": 1400}
{"docstring": "\"\"\"Return a group element containing an SVG version of the provided frame.\n\n:param buffer: 2D array of CharacterCells\n:param cell_height: Height of a character cell in pixels\n:param cell_width: Width of a character cell in pixels\n:param definitions: Existing definitions (updated in place)\n:return: A tuple consisting of a group element and new definitions\n\"\"\"\n", "func_signal": "def _render_timed_frame(offset, buffer, cell_height, cell_width, definitions):\n", "code": "frame_group_tag = etree.Element('g')\n\ngroup_definitions = {}\nfor row_number in buffer:\n    if buffer[row_number]:\n        current_definitions = {**definitions, **group_definitions}\n        tags, new_definitions = _render_line(offset,\n                                             row_number,\n                                             buffer[row_number],\n                                             cell_height,\n                                             cell_width,\n                                             current_definitions)\n        for tag in tags:\n            frame_group_tag.append(tag)\n        group_definitions.update(new_definitions)\n\nreturn frame_group_tag, group_definitions", "path": "termtosvg/termtosvg/anim.py", "commit_date": "2019-07-06 00:00:00", "repo_name": "nbedos/termtosvg", "stars": 9687, "license": "bsd-3-clause", "language": "python", "size": 1400}
{"docstring": "\"\"\"Record a process in asciicast v2 format\n\nThe records returned by this method are:\n    - a single header containing configuration information\n    - multiple event records made of data captured from the terminal and\n    timing information (except for record duration which needs to be\n    computed separately)\n\n:param process_args: Arguments required to spawn the process (list of\nstring)\n:param columns: Width of the terminal screen (integer)\n:param lines: Height of the terminal screen (integer)\n:param input_fileno: File descriptor that will be used as the standard\ninput of the process\n:param output_fileno: File descriptor that will be used as the standard\noutput of the process\n\nWhen using `sys.stdout.fileno()` for `output_fileno` there is a risk\nthat the terminal is left in an unusable state if `record` fails. To\nprevent this, `record` should be called inside the `TerminalMode`\ncontext manager.\n\"\"\"\n", "func_signal": "def record(process_args, columns, lines, input_fileno, output_fileno):\n", "code": "yield AsciiCastV2Header(version=2, width=columns, height=lines, theme=None)\n\n# TODO: why start != 0?\nstart = None\nutf8_decoder = codecs.getincrementaldecoder('utf-8')('replace')\nfor data, time in _record(process_args, columns, lines, input_fileno, output_fileno):\n    if start is None:\n        start = time\n\n    yield AsciiCastV2Event(time=(time - start).total_seconds(),\n                           event_type='o',\n                           event_data=utf8_decoder.decode(data),\n                           duration=None)", "path": "termtosvg/termtosvg/term.py", "commit_date": "2019-05-12 00:00:00", "repo_name": "nbedos/termtosvg", "stars": 9687, "license": "bsd-3-clause", "language": "python", "size": 1400}
{"docstring": "\"\"\"Send data from input_fileno to master_fd and send data from master_fd to\noutput_fileno and to the caller\n\nThe implementation of this method is mostly copied from the pty.spawn\nfunction of the CPython standard library. It has been modified in order to\nmake the `record` function a generator.\n\nSee https://github.com/python/cpython/blob/master/Lib/pty.py\n\"\"\"\n", "func_signal": "def _capture_output(input_fileno, output_fileno, master_fd, buffer_size=1024):\n", "code": "rlist = [input_fileno, master_fd]\nxlist = [input_fileno, output_fileno, master_fd]\n\nxfds = []\nwhile not xfds:\n    rfds, _, xfds = select.select(rlist, [], xlist)\n    for fd in rfds:\n        try:\n            data = os.read(fd, buffer_size)\n        except OSError:\n            xfds.append(fd)\n            continue\n\n        if not data:\n            xfds.append(fd)\n            continue\n\n        if fd == input_fileno:\n            write_fileno = master_fd\n        else:\n            write_fileno = output_fileno\n            yield data, datetime.datetime.now()\n\n        while data:\n            n = os.write(write_fileno, data)\n            data = data[n:]", "path": "termtosvg/termtosvg/term.py", "commit_date": "2019-05-12 00:00:00", "repo_name": "nbedos/termtosvg", "stars": 9687, "license": "bsd-3-clause", "language": "python", "size": 1400}
{"docstring": "\"\"\"Ensure hidden cursor don't appear in the buffer return by _redraw_buffer\nEnsure that only one cursor is present in the buffer\"\"\"\n#   '\\u001b[?25h' : display cursor\n#   '\\u001b[?25l' : hide cursor\n", "func_signal": "def test__buffer_hidden_cursor(self):\n", "code": "escape_sequences = [\n    '\\u001b[?25ha',\n    '\\r\\n\\u001b[?25lb',\n    '\\r\\n\\u001b[?25hc',\n]\n\nscreen = pyte.Screen(80, 24)\nstream = pyte.Stream(screen)\n\nexpected_cursors = [\n    ((1, 0), True),\n    ((1, 1), False),\n    ((1, 2), True),\n]\nz = itertools.zip_longest(expected_cursors, escape_sequences)\nfor count, ((cursor_pos, cursor_visible), escape_sequence) in enumerate(z):\n    with self.subTest(case='Hidden cursor - item #{}'.format(count)):\n        stream.feed(escape_sequence)\n        buffer = term._screen_buffer(screen)\n        column, line = cursor_pos\n        if cursor_visible:\n            self.assertEqual(buffer[line][column], CURSOR_CHAR)\n        # Ensure old cursors are deleted\n        for row in buffer:\n            for column in buffer[row]:\n                if buffer[row][column].text == ' ':\n                    self.assertEqual((column, row), cursor_pos)", "path": "termtosvg/termtosvg/tests/test_term.py", "commit_date": "2019-05-12 00:00:00", "repo_name": "nbedos/termtosvg", "stars": 9687, "license": "bsd-3-clause", "language": "python", "size": 1400}
{"docstring": "# Convert model output to target format [batch_id, class_id, x, y, w, h, conf]\n", "func_signal": "def output_to_target(output):\n", "code": "targets = []\nfor i, o in enumerate(output):\n    for *box, conf, cls in o.cpu().numpy():\n        targets.append([i, cls, *list(*xyxy2xywh(np.array(box)[None])), conf])\nreturn np.array(targets)", "path": "yolov3/utils/plots.py", "commit_date": "2020-12-09 00:00:00", "repo_name": "ultralytics/yolov3", "stars": 9921, "license": "agpl-3.0", "language": "python", "size": 10146}
{"docstring": "# Returns the IoU of box1 to box2. box1 is 4, box2 is nx4\n", "func_signal": "def bbox_iou(box1, box2, x1y1x2y2=True, GIoU=False, DIoU=False, CIoU=False, eps=1e-9):\n", "code": "box2 = box2.T\n\n# Get the coordinates of bounding boxes\nif x1y1x2y2:  # x1, y1, x2, y2 = box1\n    b1_x1, b1_y1, b1_x2, b1_y2 = box1[0], box1[1], box1[2], box1[3]\n    b2_x1, b2_y1, b2_x2, b2_y2 = box2[0], box2[1], box2[2], box2[3]\nelse:  # transform from xywh to xyxy\n    b1_x1, b1_x2 = box1[0] - box1[2] / 2, box1[0] + box1[2] / 2\n    b1_y1, b1_y2 = box1[1] - box1[3] / 2, box1[1] + box1[3] / 2\n    b2_x1, b2_x2 = box2[0] - box2[2] / 2, box2[0] + box2[2] / 2\n    b2_y1, b2_y2 = box2[1] - box2[3] / 2, box2[1] + box2[3] / 2\n\n# Intersection area\ninter = (torch.min(b1_x2, b2_x2) - torch.max(b1_x1, b2_x1)).clamp(0) * \\\n        (torch.min(b1_y2, b2_y2) - torch.max(b1_y1, b2_y1)).clamp(0)\n\n# Union Area\nw1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1 + eps\nw2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1 + eps\nunion = w1 * h1 + w2 * h2 - inter + eps\n\niou = inter / union\nif GIoU or DIoU or CIoU:\n    cw = torch.max(b1_x2, b2_x2) - torch.min(b1_x1, b2_x1)  # convex (smallest enclosing box) width\n    ch = torch.max(b1_y2, b2_y2) - torch.min(b1_y1, b2_y1)  # convex height\n    if CIoU or DIoU:  # Distance or Complete IoU https://arxiv.org/abs/1911.08287v1\n        c2 = cw ** 2 + ch ** 2 + eps  # convex diagonal squared\n        rho2 = ((b2_x1 + b2_x2 - b1_x1 - b1_x2) ** 2 +\n                (b2_y1 + b2_y2 - b1_y1 - b1_y2) ** 2) / 4  # center distance squared\n        if DIoU:\n            return iou - rho2 / c2  # DIoU\n        elif CIoU:  # https://github.com/Zzh-tju/DIoU-SSD-pytorch/blob/master/utils/box/box_utils.py#L47\n            v = (4 / math.pi ** 2) * torch.pow(torch.atan(w2 / h2) - torch.atan(w1 / h1), 2)\n            with torch.no_grad():\n                alpha = v / ((1 + eps) - iou + v)\n            return iou - (rho2 / c2 + v * alpha)  # CIoU\n    else:  # GIoU https://arxiv.org/pdf/1902.09630.pdf\n        c_area = cw * ch + eps  # convex area\n        return iou - (c_area - union) / c_area  # GIoU\nelse:\n    return iou  # IoU", "path": "yolov3/utils/general.py", "commit_date": "2020-12-09 00:00:00", "repo_name": "ultralytics/yolov3", "stars": 9921, "license": "agpl-3.0", "language": "python", "size": 10146}
{"docstring": "# Print mutation results to evolve.txt (for use with train.py --evolve)\n", "func_signal": "def print_mutation(hyp, results, yaml_file='hyp_evolved.yaml', bucket=''):\n", "code": "a = '%10s' * len(hyp) % tuple(hyp.keys())  # hyperparam keys\nb = '%10.3g' * len(hyp) % tuple(hyp.values())  # hyperparam values\nc = '%10.4g' * len(results) % results  # results (P, R, mAP@0.5, mAP@0.5:0.95, val_losses x 3)\nprint('\\n%s\\n%s\\nEvolved fitness: %s\\n' % (a, b, c))\n\nif bucket:\n    url = 'gs://%s/evolve.txt' % bucket\n    if gsutil_getsize(url) > (os.path.getsize('evolve.txt') if os.path.exists('evolve.txt') else 0):\n        os.system('gsutil cp %s .' % url)  # download evolve.txt if larger than local\n\nwith open('evolve.txt', 'a') as f:  # append result\n    f.write(c + b + '\\n')\nx = np.unique(np.loadtxt('evolve.txt', ndmin=2), axis=0)  # load unique rows\nx = x[np.argsort(-fitness(x))]  # sort\nnp.savetxt('evolve.txt', x, '%10.3g')  # save sort by fitness\n\n# Save yaml\nfor i, k in enumerate(hyp.keys()):\n    hyp[k] = float(x[0, i + 7])\nwith open(yaml_file, 'w') as f:\n    results = tuple(x[0, :7])\n    c = '%10.4g' * len(results) % results  # results (P, R, mAP@0.5, mAP@0.5:0.95, val_losses x 3)\n    f.write('# Hyperparameter Evolution Results\\n# Generations: %g\\n# Metrics: ' % len(x) + c + '\\n\\n')\n    yaml.dump(hyp, f, sort_keys=False)\n\nif bucket:\n    os.system('gsutil cp evolve.txt %s gs://%s' % (yaml_file, bucket))  # upload", "path": "yolov3/utils/general.py", "commit_date": "2020-12-09 00:00:00", "repo_name": "ultralytics/yolov3", "stars": 9921, "license": "agpl-3.0", "language": "python", "size": 10146}
{"docstring": "# Build targets for compute_loss(), input targets(image,class,x,y,w,h)\n", "func_signal": "def build_targets(p, targets, model):\n", "code": "det = model.module.model[-1] if is_parallel(model) else model.model[-1]  # Detect() module\nna, nt = det.na, targets.shape[0]  # number of anchors, targets\ntcls, tbox, indices, anch = [], [], [], []\ngain = torch.ones(7, device=targets.device)  # normalized to gridspace gain\nai = torch.arange(na, device=targets.device).float().view(na, 1).repeat(1, nt)  # same as .repeat_interleave(nt)\ntargets = torch.cat((targets.repeat(na, 1, 1), ai[:, :, None]), 2)  # append anchor indices\n\ng = 0.5  # bias\noff = torch.tensor([[0, 0],\n                    # [1, 0], [0, 1], [-1, 0], [0, -1],  # j,k,l,m\n                    # [1, 1], [1, -1], [-1, 1], [-1, -1],  # jk,jm,lk,lm\n                    ], device=targets.device).float() * g  # offsets\n\nfor i in range(det.nl):\n    anchors = det.anchors[i]\n    gain[2:6] = torch.tensor(p[i].shape)[[3, 2, 3, 2]]  # xyxy gain\n\n    # Match targets to anchors\n    t = targets * gain\n    if nt:\n        # Matches\n        r = t[:, :, 4:6] / anchors[:, None]  # wh ratio\n        j = torch.max(r, 1. / r).max(2)[0] < model.hyp['anchor_t']  # compare\n        # j = wh_iou(anchors, t[:, 4:6]) > model.hyp['iou_t']  # iou(3,n)=wh_iou(anchors(3,2), gwh(n,2))\n        t = t[j]  # filter\n\n        # Offsets\n        gxy = t[:, 2:4]  # grid xy\n        gxi = gain[[2, 3]] - gxy  # inverse\n        j, k = ((gxy % 1. < g) & (gxy > 1.)).T\n        l, m = ((gxi % 1. < g) & (gxi > 1.)).T\n        j = torch.stack((torch.ones_like(j),))\n        t = t.repeat((off.shape[0], 1, 1))[j]\n        offsets = (torch.zeros_like(gxy)[None] + off[:, None])[j]\n    else:\n        t = targets[0]\n        offsets = 0\n\n    # Define\n    b, c = t[:, :2].long().T  # image, class\n    gxy = t[:, 2:4]  # grid xy\n    gwh = t[:, 4:6]  # grid wh\n    gij = (gxy - offsets).long()\n    gi, gj = gij.T  # grid xy indices\n\n    # Append\n    a = t[:, 6].long()  # anchor indices\n    indices.append((b, a, gj.clamp_(0, gain[3] - 1), gi.clamp_(0, gain[2] - 1)))  # image, anchor, grid indices\n    tbox.append(torch.cat((gxy - gij, gwh), 1))  # box\n    anch.append(anchors[a])  # anchors\n    tcls.append(c)  # class\n\nreturn tcls, tbox, indices, anch", "path": "yolov3/utils/loss.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "ultralytics/yolov3", "stars": 9921, "license": "agpl-3.0", "language": "python", "size": 10146}
{"docstring": "# supports inference from various sources. For height=720, width=1280, RGB images example inputs are:\n#   opencv:     imgs = cv2.imread('image.jpg')[:,:,::-1]  # HWC BGR to RGB x(720,1280,3)\n#   PIL:        imgs = Image.open('image.jpg')  # HWC x(720,1280,3)\n#   numpy:      imgs = np.zeros((720,1280,3))  # HWC\n#   torch:      imgs = torch.zeros(16,3,720,1280)  # BCHW\n#   multiple:   imgs = [Image.open('image1.jpg'), Image.open('image2.jpg'), ...]  # list of images\n\n", "func_signal": "def forward(self, imgs, size=640, augment=False, profile=False):\n", "code": "p = next(self.model.parameters())  # for device and type\nif isinstance(imgs, torch.Tensor):  # torch\n    return self.model(imgs.to(p.device).type_as(p), augment, profile)  # inference\n\n# Pre-process\nif not isinstance(imgs, list):\n    imgs = [imgs]\nshape0, shape1 = [], []  # image and inference shapes\nbatch = range(len(imgs))  # batch size\nfor i in batch:\n    imgs[i] = np.array(imgs[i])  # to numpy\n    if imgs[i].shape[0] < 5:  # image in CHW\n        imgs[i] = imgs[i].transpose((1, 2, 0))  # reverse dataloader .transpose(2, 0, 1)\n    imgs[i] = imgs[i][:, :, :3] if imgs[i].ndim == 3 else np.tile(imgs[i][:, :, None], 3)  # enforce 3ch input\n    s = imgs[i].shape[:2]  # HWC\n    shape0.append(s)  # image shape\n    g = (size / max(s))  # gain\n    shape1.append([y * g for y in s])\nshape1 = [make_divisible(x, int(self.stride.max())) for x in np.stack(shape1, 0).max(0)]  # inference shape\nx = [letterbox(imgs[i], new_shape=shape1, auto=False)[0] for i in batch]  # pad\nx = np.stack(x, 0) if batch[-1] else x[0][None]  # stack\nx = np.ascontiguousarray(x.transpose((0, 3, 1, 2)))  # BHWC to BCHW\nx = torch.from_numpy(x).to(p.device).type_as(p) / 255.  # uint8 to fp16/32\n\n# Inference\nwith torch.no_grad():\n    y = self.model(x, augment, profile)[0]  # forward\ny = non_max_suppression(y, conf_thres=self.conf, iou_thres=self.iou, classes=self.classes)  # NMS\n\n# Post-process\nfor i in batch:\n    scale_coords(shape1, y[i][:, :4], shape0[i])\n\nreturn Detections(imgs, y, self.names)", "path": "yolov3/models/common.py", "commit_date": "2020-12-09 00:00:00", "repo_name": "ultralytics/yolov3", "stars": 9921, "license": "agpl-3.0", "language": "python", "size": 10146}
{"docstring": "# Increment path, i.e. runs/exp --> runs/exp{sep}0, runs/exp{sep}1 etc.\n", "func_signal": "def increment_path(path, exist_ok=True, sep=''):\n", "code": "path = Path(path)  # os-agnostic\nif (path.exists() and exist_ok) or (not path.exists()):\n    return str(path)\nelse:\n    dirs = glob.glob(f\"{path}{sep}*\")  # similar paths\n    matches = [re.search(rf\"%s{sep}(\\d+)\" % path.stem, d) for d in dirs]\n    i = [int(m.groups()[0]) for m in matches if m]  # indices\n    n = max(i) + 1 if i else 2  # increment number\n    return f\"{path}{sep}{n}\"  # update path", "path": "yolov3/utils/general.py", "commit_date": "2020-12-09 00:00:00", "repo_name": "ultralytics/yolov3", "stars": 9921, "license": "agpl-3.0", "language": "python", "size": 10146}
{"docstring": "# Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n", "func_signal": "def xywh2xyxy(x):\n", "code": "y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\ny[:, 0] = x[:, 0] - x[:, 2] / 2  # top left x\ny[:, 1] = x[:, 1] - x[:, 3] / 2  # top left y\ny[:, 2] = x[:, 0] + x[:, 2] / 2  # bottom right x\ny[:, 3] = x[:, 1] + x[:, 3] / 2  # bottom right y\nreturn y", "path": "yolov3/utils/general.py", "commit_date": "2020-12-09 00:00:00", "repo_name": "ultralytics/yolov3", "stars": 9921, "license": "agpl-3.0", "language": "python", "size": 10146}
{"docstring": "# Produces image weights based on class_weights and image contents\n", "func_signal": "def labels_to_image_weights(labels, nc=80, class_weights=np.ones(80)):\n", "code": "class_counts = np.array([np.bincount(x[:, 0].astype(np.int), minlength=nc) for x in labels])\nimage_weights = (class_weights.reshape(1, nc) * class_counts).sum(1)\n# index = random.choices(range(n), weights=image_weights, k=1)  # weight image sample\nreturn image_weights", "path": "yolov3/utils/general.py", "commit_date": "2020-12-09 00:00:00", "repo_name": "ultralytics/yolov3", "stars": 9921, "license": "agpl-3.0", "language": "python", "size": 10146}
{"docstring": "# Clip bounding xyxy bounding boxes to image shape (height, width)\n", "func_signal": "def clip_coords(boxes, img_shape):\n", "code": "boxes[:, 0].clamp_(0, img_shape[1])  # x1\nboxes[:, 1].clamp_(0, img_shape[0])  # y1\nboxes[:, 2].clamp_(0, img_shape[1])  # x2\nboxes[:, 3].clamp_(0, img_shape[0])  # y2", "path": "yolov3/utils/general.py", "commit_date": "2020-12-09 00:00:00", "repo_name": "ultralytics/yolov3", "stars": 9921, "license": "agpl-3.0", "language": "python", "size": 10146}
{"docstring": "# Download dataset if not found locally\n", "func_signal": "def check_dataset(dict):\n", "code": "val, s = dict.get('val'), dict.get('download')\nif val and len(val):\n    val = [Path(x).resolve() for x in (val if isinstance(val, list) else [val])]  # val path\n    if not all(x.exists() for x in val):\n        print('\\nWARNING: Dataset not found, nonexistent paths: %s' % [str(x) for x in val if not x.exists()])\n        if s and len(s):  # download script\n            print('Downloading %s ...' % s)\n            if s.startswith('http') and s.endswith('.zip'):  # URL\n                f = Path(s).name  # filename\n                torch.hub.download_url_to_file(s, f)\n                r = os.system('unzip -q %s -d ../ && rm %s' % (f, f))  # unzip\n            else:  # bash script\n                r = os.system(s)\n            print('Dataset autodownload %s\\n' % ('success' if r == 0 else 'failure'))  # analyze return value\n        else:\n            raise Exception('Dataset not found.')", "path": "yolov3/utils/general.py", "commit_date": "2020-12-09 00:00:00", "repo_name": "ultralytics/yolov3", "stars": 9921, "license": "agpl-3.0", "language": "python", "size": 10146}
{"docstring": "# Convert nx4 boxes from [x1, y1, x2, y2] to [x, y, w, h] where xy1=top-left, xy2=bottom-right\n", "func_signal": "def xyxy2xywh(x):\n", "code": "y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\ny[:, 0] = (x[:, 0] + x[:, 2]) / 2  # x center\ny[:, 1] = (x[:, 1] + x[:, 3]) / 2  # y center\ny[:, 2] = x[:, 2] - x[:, 0]  # width\ny[:, 3] = x[:, 3] - x[:, 1]  # height\nreturn y", "path": "yolov3/utils/general.py", "commit_date": "2020-12-09 00:00:00", "repo_name": "ultralytics/yolov3", "stars": 9921, "license": "agpl-3.0", "language": "python", "size": 10146}
{"docstring": "# applies a second stage classifier to yolo outputs\n", "func_signal": "def apply_classifier(x, model, img, im0):\n", "code": "im0 = [im0] if isinstance(im0, np.ndarray) else im0\nfor i, d in enumerate(x):  # per image\n    if d is not None and len(d):\n        d = d.clone()\n\n        # Reshape and pad cutouts\n        b = xyxy2xywh(d[:, :4])  # boxes\n        b[:, 2:] = b[:, 2:].max(1)[0].unsqueeze(1)  # rectangle to square\n        b[:, 2:] = b[:, 2:] * 1.3 + 30  # pad\n        d[:, :4] = xywh2xyxy(b).long()\n\n        # Rescale boxes from img_size to im0 size\n        scale_coords(img.shape[2:], d[:, :4], im0[i].shape)\n\n        # Classes\n        pred_cls1 = d[:, 5].long()\n        ims = []\n        for j, a in enumerate(d):  # per item\n            cutout = im0[i][int(a[1]):int(a[3]), int(a[0]):int(a[2])]\n            im = cv2.resize(cutout, (224, 224))  # BGR\n            # cv2.imwrite('test%i.jpg' % j, cutout)\n\n            im = im[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n            im = np.ascontiguousarray(im, dtype=np.float32)  # uint8 to float32\n            im /= 255.0  # 0 - 255 to 0.0 - 1.0\n            ims.append(im)\n\n        pred_cls2 = model(torch.Tensor(ims).to(d.device)).argmax(1)  # classifier prediction\n        x[i] = x[i][pred_cls1 == pred_cls2]  # retain matching class detections\n\nreturn x", "path": "yolov3/utils/general.py", "commit_date": "2020-12-09 00:00:00", "repo_name": "ultralytics/yolov3", "stars": 9921, "license": "agpl-3.0", "language": "python", "size": 10146}
{"docstring": "# 2d histogram used in labels.png and evolve.png\n", "func_signal": "def hist2d(x, y, n=100):\n", "code": "xedges, yedges = np.linspace(x.min(), x.max(), n), np.linspace(y.min(), y.max(), n)\nhist, xedges, yedges = np.histogram2d(x, y, (xedges, yedges))\nxidx = np.clip(np.digitize(x, xedges) - 1, 0, hist.shape[0] - 1)\nyidx = np.clip(np.digitize(y, yedges) - 1, 0, hist.shape[1] - 1)\nreturn np.log(hist[xidx, yidx])", "path": "yolov3/utils/plots.py", "commit_date": "2020-12-09 00:00:00", "repo_name": "ultralytics/yolov3", "stars": 9921, "license": "agpl-3.0", "language": "python", "size": 10146}
{"docstring": "# Return path to most recent 'last.pt' in /runs (i.e. to --resume from)\n", "func_signal": "def get_latest_run(search_dir='.'):\n", "code": "last_list = glob.glob(f'{search_dir}/**/last*.pt', recursive=True)\nreturn max(last_list, key=os.path.getctime) if last_list else ''", "path": "yolov3/utils/general.py", "commit_date": "2020-12-09 00:00:00", "repo_name": "ultralytics/yolov3", "stars": 9921, "license": "agpl-3.0", "language": "python", "size": 10146}
{"docstring": "# Plot LR simulating training for full epochs\n", "func_signal": "def plot_lr_scheduler(optimizer, scheduler, epochs=300, save_dir=''):\n", "code": "optimizer, scheduler = copy(optimizer), copy(scheduler)  # do not modify originals\ny = []\nfor _ in range(epochs):\n    scheduler.step()\n    y.append(optimizer.param_groups[0]['lr'])\nplt.plot(y, '.-', label='LR')\nplt.xlabel('epoch')\nplt.ylabel('LR')\nplt.grid()\nplt.xlim(0, epochs)\nplt.ylim(0)\nplt.savefig(Path(save_dir) / 'LR.png', dpi=200)", "path": "yolov3/utils/plots.py", "commit_date": "2020-12-09 00:00:00", "repo_name": "ultralytics/yolov3", "stars": 9921, "license": "agpl-3.0", "language": "python", "size": 10146}
{"docstring": "# Plot image grid with labels\n\n", "func_signal": "def plot_images(images, targets, paths=None, fname='images.jpg', names=None, max_size=640, max_subplots=16):\n", "code": "if isinstance(images, torch.Tensor):\n    images = images.cpu().float().numpy()\nif isinstance(targets, torch.Tensor):\n    targets = targets.cpu().numpy()\n\n# un-normalise\nif np.max(images[0]) <= 1:\n    images *= 255\n\ntl = 3  # line thickness\ntf = max(tl - 1, 1)  # font thickness\nbs, _, h, w = images.shape  # batch size, _, height, width\nbs = min(bs, max_subplots)  # limit plot images\nns = np.ceil(bs ** 0.5)  # number of subplots (square)\n\n# Check if we should resize\nscale_factor = max_size / max(h, w)\nif scale_factor < 1:\n    h = math.ceil(scale_factor * h)\n    w = math.ceil(scale_factor * w)\n\ncolors = color_list()  # list of colors\nmosaic = np.full((int(ns * h), int(ns * w), 3), 255, dtype=np.uint8)  # init\nfor i, img in enumerate(images):\n    if i == max_subplots:  # if last batch has fewer images than we expect\n        break\n\n    block_x = int(w * (i // ns))\n    block_y = int(h * (i % ns))\n\n    img = img.transpose(1, 2, 0)\n    if scale_factor < 1:\n        img = cv2.resize(img, (w, h))\n\n    mosaic[block_y:block_y + h, block_x:block_x + w, :] = img\n    if len(targets) > 0:\n        image_targets = targets[targets[:, 0] == i]\n        boxes = xywh2xyxy(image_targets[:, 2:6]).T\n        classes = image_targets[:, 1].astype('int')\n        labels = image_targets.shape[1] == 6  # labels if no conf column\n        conf = None if labels else image_targets[:, 6]  # check for confidence presence (label vs pred)\n\n        if boxes.shape[1]:\n            if boxes.max() <= 1.01:  # if normalized with tolerance 0.01\n                boxes[[0, 2]] *= w  # scale to pixels\n                boxes[[1, 3]] *= h\n            elif scale_factor < 1:  # absolute coords need scale if image scales\n                boxes *= scale_factor\n        boxes[[0, 2]] += block_x\n        boxes[[1, 3]] += block_y\n        for j, box in enumerate(boxes.T):\n            cls = int(classes[j])\n            color = colors[cls % len(colors)]\n            cls = names[cls] if names else cls\n            if labels or conf[j] > 0.25:  # 0.25 conf thresh\n                label = '%s' % cls if labels else '%s %.1f' % (cls, conf[j])\n                plot_one_box(box, mosaic, label=label, color=color, line_thickness=tl)\n\n    # Draw image filename labels\n    if paths:\n        label = Path(paths[i]).name[:40]  # trim to 40 char\n        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n        cv2.putText(mosaic, label, (block_x + 5, block_y + t_size[1] + 5), 0, tl / 3, [220, 220, 220], thickness=tf,\n                    lineType=cv2.LINE_AA)\n\n    # Image border\n    cv2.rectangle(mosaic, (block_x, block_y), (block_x + w, block_y + h), (255, 255, 255), thickness=3)\n\nif fname:\n    r = min(1280. / max(h, w) / ns, 1.0)  # ratio to limit image size\n    mosaic = cv2.resize(mosaic, (int(ns * w * r), int(ns * h * r)), interpolation=cv2.INTER_AREA)\n    # cv2.imwrite(fname, cv2.cvtColor(mosaic, cv2.COLOR_BGR2RGB))  # cv2 save\n    Image.fromarray(mosaic).save(fname)  # PIL save\nreturn mosaic", "path": "yolov3/utils/plots.py", "commit_date": "2020-12-09 00:00:00", "repo_name": "ultralytics/yolov3", "stars": 9921, "license": "agpl-3.0", "language": "python", "size": 10146}
{"docstring": "# Search for file if not found\n", "func_signal": "def check_file(file):\n", "code": "if os.path.isfile(file) or file == '':\n    return file\nelse:\n    files = glob.glob('./**/' + file, recursive=True)  # find file\n    assert len(files), 'File Not Found: %s' % file  # assert file was found\n    assert len(files) == 1, \"Multiple files match '%s', specify exact path: %s\" % (file, files)  # assert unique\n    return files[0]  # return file", "path": "yolov3/utils/general.py", "commit_date": "2020-12-09 00:00:00", "repo_name": "ultralytics/yolov3", "stars": 9921, "license": "agpl-3.0", "language": "python", "size": 10146}
{"docstring": "# return a list of Detections objects, i.e. 'for result in results.tolist():'\n", "func_signal": "def tolist(self):\n", "code": "x = [Detections([self.imgs[i]], [self.pred[i]], self.names) for i in range(self.n)]\nfor d in x:\n    for k in ['imgs', 'pred', 'xyxy', 'xyxyn', 'xywh', 'xywhn']:\n        setattr(d, k, getattr(d, k)[0])  # pop out of list\nreturn x", "path": "yolov3/models/common.py", "commit_date": "2020-12-09 00:00:00", "repo_name": "ultralytics/yolov3", "stars": 9921, "license": "agpl-3.0", "language": "python", "size": 10146}
{"docstring": "# Suggest 'git pull' if repo is out of date\n", "func_signal": "def check_git_status():\n", "code": "if platform.system() in ['Linux', 'Darwin'] and not os.path.isfile('/.dockerenv'):\n    s = subprocess.check_output('if [ -d .git ]; then git fetch && git status -uno; fi', shell=True).decode('utf-8')\n    if 'Your branch is behind' in s:\n        print(s[s.find('Your branch is behind'):s.find('\\n\\n')] + '\\n')", "path": "yolov3/utils/general.py", "commit_date": "2020-12-09 00:00:00", "repo_name": "ultralytics/yolov3", "stars": 9921, "license": "agpl-3.0", "language": "python", "size": 10146}
{"docstring": "# https://stackoverflow.com/questions/28536191/how-to-filter-smooth-with-scipy-numpy\n", "func_signal": "def butter_lowpass_filtfilt(data, cutoff=1500, fs=50000, order=5):\n", "code": "def butter_lowpass(cutoff, fs, order):\n    nyq = 0.5 * fs\n    normal_cutoff = cutoff / nyq\n    return butter(order, normal_cutoff, btype='low', analog=False)\n\nb, a = butter_lowpass(cutoff, fs, order=order)\nreturn filtfilt(b, a, data)  # forward-backward filter", "path": "yolov3/utils/plots.py", "commit_date": "2020-12-09 00:00:00", "repo_name": "ultralytics/yolov3", "stars": 9921, "license": "agpl-3.0", "language": "python", "size": 10146}
{"docstring": "# Render {\"href\": \"...\", \"label\": \"...\"} as link\n", "func_signal": "def render_cell(value, database):\n", "code": "if not isinstance(value, str):\n    return None\nstripped = value.strip()\nif not stripped.startswith(\"{\") and stripped.endswith(\"}\"):\n    return None\ntry:\n    data = json.loads(value)\nexcept ValueError:\n    return None\nif not isinstance(data, dict):\n    return None\nif set(data.keys()) != {\"href\", \"label\"}:\n    return None\nhref = data[\"href\"]\nif not (\n    href.startswith(\"/\")\n    or href.startswith(\"http://\")\n    or href.startswith(\"https://\")\n):\n    return None\nreturn jinja2.Markup(\n    '<a data-database=\"{database}\" href=\"{href}\">{label}</a>'.format(\n        database=database,\n        href=jinja2.escape(data[\"href\"]),\n        label=jinja2.escape(data[\"label\"] or \"\") or \"&nbsp;\",\n    )\n)", "path": "datasette/tests/plugins/my_plugin_2.py", "commit_date": "2020-10-31 00:00:00", "repo_name": "simonw/datasette", "stars": 8751, "license": "apache-2.0", "language": "python", "size": 6541}
{"docstring": "# Given a request and the metadata configuration for a table, return\n# a dictionary of selected facets, their lists of configs and for each\n# config whether it came from the request or the metadata.\n#\n#   return {type: [\n#       {\"source\": \"metadata\", \"config\": config1},\n#       {\"source\": \"request\", \"config\": config2}]}\n", "func_signal": "def load_facet_configs(request, table_metadata):\n", "code": "facet_configs = {}\ntable_metadata = table_metadata or {}\nmetadata_facets = table_metadata.get(\"facets\", [])\nfor metadata_config in metadata_facets:\n    if isinstance(metadata_config, str):\n        type = \"column\"\n        metadata_config = {\"simple\": metadata_config}\n    else:\n        assert (\n            len(metadata_config.values()) == 1\n        ), \"Metadata config dicts should be {type: config}\"\n        type, metadata_config = metadata_config.items()[0]\n        if isinstance(metadata_config, str):\n            metadata_config = {\"simple\": metadata_config}\n    facet_configs.setdefault(type, []).append(\n        {\"source\": \"metadata\", \"config\": metadata_config}\n    )\nqs_pairs = urllib.parse.parse_qs(request.query_string, keep_blank_values=True)\nfor key, values in qs_pairs.items():\n    if key.startswith(\"_facet\"):\n        # Figure out the facet type\n        if key == \"_facet\":\n            type = \"column\"\n        elif key.startswith(\"_facet_\"):\n            type = key[len(\"_facet_\") :]\n        for value in values:\n            # The value is the config - either JSON or not\n            if value.startswith(\"{\"):\n                config = json.loads(value)\n            else:\n                config = {\"simple\": value}\n            facet_configs.setdefault(type, []).append(\n                {\"source\": \"request\", \"config\": config}\n            )\nreturn facet_configs", "path": "datasette/datasette/facets.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "simonw/datasette", "stars": 8751, "license": "apache-2.0", "language": "python", "size": 6541}
{"docstring": "\"\"\"Render a response as JSON\"\"\"\n", "func_signal": "def json_renderer(args, data, view_name):\n", "code": "status_code = 200\n# Handle the _json= parameter which may modify data[\"rows\"]\njson_cols = []\nif \"_json\" in args:\n    json_cols = args.getlist(\"_json\")\nif json_cols and \"rows\" in data and \"columns\" in data:\n    data[\"rows\"] = convert_specific_columns_to_json(\n        data[\"rows\"], data[\"columns\"], json_cols\n    )\n\n# unless _json_infinity=1 requested, replace infinity with None\nif \"rows\" in data and not value_as_boolean(args.get(\"_json_infinity\", \"0\")):\n    data[\"rows\"] = [remove_infinites(row) for row in data[\"rows\"]]\n\n# Deal with the _shape option\nshape = args.get(\"_shape\", \"arrays\")\n\nnext_url = data.get(\"next_url\")\n\nif shape == \"arrayfirst\":\n    data = [row[0] for row in data[\"rows\"]]\nelif shape in (\"objects\", \"object\", \"array\"):\n    columns = data.get(\"columns\")\n    rows = data.get(\"rows\")\n    if rows and columns:\n        data[\"rows\"] = [dict(zip(columns, row)) for row in rows]\n    if shape == \"object\":\n        error = None\n        if \"primary_keys\" not in data:\n            error = \"_shape=object is only available on tables\"\n        else:\n            pks = data[\"primary_keys\"]\n            if not pks:\n                error = (\n                    \"_shape=object not available for tables with no primary keys\"\n                )\n            else:\n                object_rows = {}\n                for row in data[\"rows\"]:\n                    pk_string = path_from_row_pks(row, pks, not pks)\n                    object_rows[pk_string] = row\n                data = object_rows\n        if error:\n            data = {\"ok\": False, \"error\": error}\n    elif shape == \"array\":\n        data = data[\"rows\"]\n\nelif shape == \"arrays\":\n    pass\nelse:\n    status_code = 400\n    data = {\n        \"ok\": False,\n        \"error\": f\"Invalid _shape: {shape}\",\n        \"status\": 400,\n        \"title\": None,\n    }\n# Handle _nl option for _shape=array\nnl = args.get(\"_nl\", \"\")\nif nl and shape == \"array\":\n    body = \"\\n\".join(json.dumps(item, cls=CustomJSONEncoder) for item in data)\n    content_type = \"text/plain\"\nelse:\n    body = json.dumps(data, cls=CustomJSONEncoder)\n    content_type = \"application/json; charset=utf-8\"\nheaders = {}\nif next_url:\n    headers[\"link\"] = f'<{next_url}>; rel=\"next\"'\nreturn Response(\n    body, status=status_code, headers=headers, content_type=content_type\n)", "path": "datasette/datasette/renderer.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "simonw/datasette", "stars": 8751, "license": "apache-2.0", "language": "python", "size": 6541}
{"docstring": "# This helps unit tests that want to run assertions against the request object:\n", "func_signal": "def extra_template_vars(template, database, table, view_name, request, datasette):\n", "code": "datasette._last_request = request\n\nasync def query_database(sql):\n    first_db = list(datasette.databases.keys())[0]\n    return (await datasette.execute(first_db, sql)).rows[0][0]\n\nasync def inner():\n    return {\n        \"extra_template_vars_from_awaitable\": json.dumps(\n            {\n                \"template\": template,\n                \"scope_path\": request.scope[\"path\"] if request else None,\n                \"awaitable\": True,\n            },\n            default=lambda b: b.decode(\"utf8\"),\n        ),\n        \"query_database\": query_database,\n    }\n\nreturn inner", "path": "datasette/tests/plugins/my_plugin_2.py", "commit_date": "2020-10-31 00:00:00", "repo_name": "simonw/datasette", "stars": 8751, "license": "apache-2.0", "language": "python", "size": 6541}
{"docstring": "\"\"\"Calculate the hash of a database, efficiently.\"\"\"\n", "func_signal": "def inspect_hash(path):\n", "code": "m = hashlib.sha256()\nwith path.open(\"rb\") as fp:\n    while True:\n        data = fp.read(HASH_BLOCK_SIZE)\n        if not data:\n            break\n        m.update(data)\n\nreturn m.hexdigest()", "path": "datasette/datasette/inspect.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "simonw/datasette", "stars": 8751, "license": "apache-2.0", "language": "python", "size": 6541}
{"docstring": "\"\"\"The /-/auth-token endpoint sets the correct cookie\"\"\"\n", "func_signal": "def test_auth_token(app_client):\n", "code": "assert app_client.ds._root_token is not None\npath = f\"/-/auth-token?token={app_client.ds._root_token}\"\nresponse = app_client.get(\n    path,\n    allow_redirects=False,\n)\nassert 302 == response.status\nassert \"/\" == response.headers[\"Location\"]\nassert {\"a\": {\"id\": \"root\"}} == app_client.ds.unsign(\n    response.cookies[\"ds_actor\"], \"actor\"\n)\n# Check that a second with same token fails\nassert app_client.ds._root_token is None\nassert (\n    403\n    == app_client.get(\n        path,\n        allow_redirects=False,\n    ).status\n)", "path": "datasette/tests/test_auth.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "simonw/datasette", "stars": 8751, "license": "apache-2.0", "language": "python", "size": 6541}
{"docstring": "# Without auth only shows three queries\n", "func_signal": "def test_canned_query_permissions_on_database_page(canned_write_client):\n", "code": "query_names = {\n    q[\"name\"] for q in canned_write_client.get(\"/data.json\").json[\"queries\"]\n}\nassert {\n    \"canned_read\",\n    \"add_name\",\n    \"add_name_specify_id\",\n    \"update_name\",\n    \"from_async_hook\",\n    \"from_hook\",\n} == query_names\n\n# With auth shows four\nresponse = canned_write_client.get(\n    \"/data.json\",\n    cookies={\"ds_actor\": canned_write_client.actor_cookie({\"id\": \"root\"})},\n)\nassert 200 == response.status\nquery_names_and_private = sorted(\n    [\n        {\"name\": q[\"name\"], \"private\": q[\"private\"]}\n        for q in response.json[\"queries\"]\n    ],\n    key=lambda q: q[\"name\"],\n)\nassert query_names_and_private == [\n    {\"name\": \"add_name\", \"private\": False},\n    {\"name\": \"add_name_specify_id\", \"private\": False},\n    {\"name\": \"canned_read\", \"private\": False},\n    {\"name\": \"delete_name\", \"private\": True},\n    {\"name\": \"from_async_hook\", \"private\": False},\n    {\"name\": \"from_hook\", \"private\": False},\n    {\"name\": \"update_name\", \"private\": False},\n]", "path": "datasette/tests/test_canned_queries.py", "commit_date": "2020-11-15 00:00:00", "repo_name": "simonw/datasette", "stars": 8751, "license": "apache-2.0", "language": "python", "size": 6541}
{"docstring": "# Copy instead if os.link raises OSError (normally due to different device)\n", "func_signal": "def test_temporary_docker_directory_uses_copy_if_hard_link_fails(mock_link):\n", "code": "mock_link.side_effect = OSError\nwith tempfile.TemporaryDirectory() as td:\n    os.chdir(td)\n    open(\"hello\", \"w\").write(\"world\")\n    # Default usage of this should use symlink\n    with utils.temporary_docker_directory(\n        files=[\"hello\"],\n        name=\"t\",\n        metadata=None,\n        extra_options=None,\n        branch=None,\n        template_dir=None,\n        plugins_dir=None,\n        static=[],\n        install=[],\n        spatialite=False,\n        version_note=None,\n        secret=None,\n    ) as temp_docker:\n        hello = os.path.join(temp_docker, \"hello\")\n        assert \"world\" == open(hello).read()\n        # It should be a copy, not a hard link\n        assert 1 == os.stat(hello).st_nlink", "path": "datasette/tests/test_utils.py", "commit_date": "2020-11-30 00:00:00", "repo_name": "simonw/datasette", "stars": 8751, "license": "apache-2.0", "language": "python", "size": 6541}
{"docstring": "\"\"\"A valid actor cookie sets request.scope['actor']\"\"\"\n", "func_signal": "def test_actor_cookie(app_client):\n", "code": "cookie = app_client.actor_cookie({\"id\": \"test\"})\nresponse = app_client.get(\"/\", cookies={\"ds_actor\": cookie})\nassert {\"id\": \"test\"} == app_client.ds._last_request.scope[\"actor\"]", "path": "datasette/tests/test_auth.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "simonw/datasette", "stars": 8751, "license": "apache-2.0", "language": "python", "size": 6541}
{"docstring": "\"\"\"List views in a database.\"\"\"\n", "func_signal": "def inspect_views(conn):\n", "code": "return [\n    v[0] for v in conn.execute('select name from sqlite_master where type = \"view\"')\n]", "path": "datasette/datasette/inspect.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "simonw/datasette", "stars": 8751, "license": "apache-2.0", "language": "python", "size": 6541}
{"docstring": "# Testing asyncio version of permission_allowed\n", "func_signal": "def permission_allowed(datasette, actor, action):\n", "code": "async def inner():\n    assert 2 == (await datasette.get_database().execute(\"select 1 + 1\")).first()[0]\n    if action == \"this_is_allowed_async\":\n        return True\n    elif action == \"this_is_denied_async\":\n        return False\n\nreturn inner", "path": "datasette/tests/plugins/my_plugin_2.py", "commit_date": "2020-10-31 00:00:00", "repo_name": "simonw/datasette", "stars": 8751, "license": "apache-2.0", "language": "python", "size": 6541}
{"docstring": "# Ensure test_cli.py and test_black.py and test_inspect.py run first before any asyncio code kicks in\n", "func_signal": "def pytest_collection_modifyitems(items):\n", "code": "move_to_front(items, \"test_cli\")\nmove_to_front(items, \"test_black\")\nmove_to_front(items, \"test_inspect_cli\")\nmove_to_front(items, \"test_serve_with_get\")\nmove_to_front(items, \"test_serve_with_get_exit_code_for_error\")\nmove_to_front(items, \"test_inspect_cli_writes_to_file\")\nmove_to_front(items, \"test_spatialite_error_if_attempt_to_open_spatialite\")\nmove_to_front(items, \"test_package\")\nmove_to_front(items, \"test_package_with_port\")", "path": "datasette/tests/conftest.py", "commit_date": "2020-11-30 00:00:00", "repo_name": "simonw/datasette", "stars": 8751, "license": "apache-2.0", "language": "python", "size": 6541}
{"docstring": "# Without _stream should return header + 100 rows:\n", "func_signal": "def test_table_csv_stream(app_client):\n", "code": "response = app_client.get(\"/fixtures/compound_three_primary_keys.csv?_size=max\")\nassert 101 == len([b for b in response.body.split(b\"\\r\\n\") if b])\n# With _stream=1 should return header + 1001 rows\nresponse = app_client.get(\"/fixtures/compound_three_primary_keys.csv?_stream=1\")\nassert 1002 == len([b for b in response.body.split(b\"\\r\\n\") if b])", "path": "datasette/tests/test_csv.py", "commit_date": "2020-12-10 00:00:00", "repo_name": "simonw/datasette", "stars": 8751, "license": "apache-2.0", "language": "python", "size": 6541}
{"docstring": "# Returns the first available database:\n", "func_signal": "def test_get_database_no_argument(datasette):\n", "code": "db = datasette.get_database()\nassert \"fixtures\" == db.name", "path": "datasette/tests/test_internals_datasette.py", "commit_date": "2020-11-24 00:00:00", "repo_name": "simonw/datasette", "stars": 8751, "license": "apache-2.0", "language": "python", "size": 6541}
{"docstring": "\"\"\"Exit (with error message) if ``binary` isn't installed\"\"\"\n", "func_signal": "def fail_if_publish_binary_not_installed(binary, publish_target, install_link):\n", "code": "if not shutil.which(binary):\n    click.secho(\n        \"Publishing to {publish_target} requires {binary} to be installed and configured\".format(\n            publish_target=publish_target, binary=binary\n        ),\n        bg=\"red\",\n        fg=\"white\",\n        bold=True,\n        err=True,\n    )\n    click.echo(\n        f\"Follow the instructions at {install_link}\",\n        err=True,\n    )\n    sys.exit(1)", "path": "datasette/datasette/publish/common.py", "commit_date": "2020-11-15 00:00:00", "repo_name": "simonw/datasette", "stars": 8751, "license": "apache-2.0", "language": "python", "size": 6541}
{"docstring": "\"\"\"List tables and their row counts, excluding uninteresting tables.\"\"\"\n", "func_signal": "def inspect_tables(conn, database_metadata):\n", "code": "tables = {}\ntable_names = [\n    r[\"name\"]\n    for r in conn.execute('select * from sqlite_master where type=\"table\"')\n]\n\nfor table in table_names:\n    table_metadata = database_metadata.get(\"tables\", {}).get(table, {})\n\n    try:\n        count = conn.execute(\n            f\"select count(*) from {escape_sqlite(table)}\"\n        ).fetchone()[0]\n    except sqlite3.OperationalError:\n        # This can happen when running against a FTS virtual table\n        # e.g. \"select count(*) from some_fts;\"\n        count = 0\n\n    column_names = table_columns(conn, table)\n\n    tables[table] = {\n        \"name\": table,\n        \"columns\": column_names,\n        \"primary_keys\": detect_primary_keys(conn, table),\n        \"count\": count,\n        \"hidden\": table_metadata.get(\"hidden\") or False,\n        \"fts_table\": detect_fts(conn, table),\n    }\n\nforeign_keys = get_all_foreign_keys(conn)\nfor table, info in foreign_keys.items():\n    tables[table][\"foreign_keys\"] = info\n\n# Mark tables 'hidden' if they relate to FTS virtual tables\nhidden_tables = [\n    r[\"name\"]\n    for r in conn.execute(\n        \"\"\"\n            select name from sqlite_master\n            where rootpage = 0\n            and sql like '%VIRTUAL TABLE%USING FTS%'\n        \"\"\"\n    )\n]\n\nif detect_spatialite(conn):\n    # Also hide Spatialite internal tables\n    hidden_tables += [\n        \"ElementaryGeometries\",\n        \"SpatialIndex\",\n        \"geometry_columns\",\n        \"spatial_ref_sys\",\n        \"spatialite_history\",\n        \"sql_statements_log\",\n        \"sqlite_sequence\",\n        \"views_geometry_columns\",\n        \"virts_geometry_columns\",\n    ] + [\n        r[\"name\"]\n        for r in conn.execute(\n            \"\"\"\n                select name from sqlite_master\n                where name like \"idx_%\"\n                and type = \"table\"\n            \"\"\"\n        )\n    ]\n\nfor t in tables.keys():\n    for hidden_table in hidden_tables:\n        if t == hidden_table or t.startswith(hidden_table):\n            tables[t][\"hidden\"] = True\n            continue\n\nreturn tables", "path": "datasette/datasette/inspect.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "simonw/datasette", "stars": 8751, "license": "apache-2.0", "language": "python", "size": 6541}
{"docstring": "# These forms embed a csrftoken so they should be served with Vary: Cookie\n", "func_signal": "def test_vary_header(canned_write_client):\n", "code": "assert \"vary\" not in canned_write_client.get(\"/data\").headers\nassert \"Cookie\" == canned_write_client.get(\"/data/update_name\").headers[\"vary\"]", "path": "datasette/tests/test_canned_queries.py", "commit_date": "2020-11-15 00:00:00", "repo_name": "simonw/datasette", "stars": 8751, "license": "apache-2.0", "language": "python", "size": 6541}
{"docstring": "# tracer is a list\n", "func_signal": "def capture_traces(tracer):\n", "code": "task_id = get_task_id()\nif task_id is None:\n    yield\n    return\ntracers[task_id] = tracer\nyield\ndel tracers[task_id]", "path": "datasette/datasette/tracer.py", "commit_date": "2020-12-21 00:00:00", "repo_name": "simonw/datasette", "stars": 8751, "license": "apache-2.0", "language": "python", "size": 6541}
{"docstring": "# Render some debug output in cell with value RENDER_CELL_DEMO\n", "func_signal": "def render_cell(value, column, table, database, datasette):\n", "code": "if value != \"RENDER_CELL_DEMO\":\n    return None\nreturn json.dumps(\n    {\n        \"column\": column,\n        \"table\": table,\n        \"database\": database,\n        \"config\": datasette.plugin_config(\n            \"name-of-plugin\",\n            database=database,\n            table=table,\n        ),\n    }\n)", "path": "datasette/tests/plugins/my_plugin.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "simonw/datasette", "stars": 8751, "license": "apache-2.0", "language": "python", "size": 6541}
{"docstring": "# Mainly for the latest.datasette.io demo\n", "func_signal": "def login_as_root(datasette, request):\n", "code": "if request.method == \"POST\":\n    response = Response.redirect(\"/\")\n    response.set_cookie(\n        \"ds_actor\", datasette.sign({\"a\": {\"id\": \"root\"}}, \"actor\")\n    )\n    return response\nreturn Response.html(\n    \"\"\"\n    <form action=\"{}\" method=\"POST\">\n        <p>\n            <input type=\"hidden\" name=\"csrftoken\" value=\"{}\">\n            <input type=\"submit\" value=\"Sign in as root user\"></p>\n    </form>\n\"\"\".format(\n        request.path, request.scope[\"csrftoken\"]()\n    )\n)", "path": "datasette/tests/plugins/my_plugin.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "simonw/datasette", "stars": 8751, "license": "apache-2.0", "language": "python", "size": 6541}
{"docstring": "# \u767b\u5f55qun.qq.com\n\n# \u8bbf\u95ee\u7f51\u9875\uff0c\u4e3a\u4e86\u83b7\u53d6\u53c2\u6570pt_login_sig\n", "func_signal": "def login_qun_qq_com(self):\n", "code": "login_url = 'http://ui.ptlogin2.qq.com/cgi-bin/login?appid=549000912&s_url=http://qun.qq.com/member.html'\nhtml = get_html(login_url, '')\n# \u5bf9\u8fd4\u56de\u7684cookies\u8fdb\u884c\u8f6c\u5316\u4e3adict\u7c7b\u578b\uff0c\u65b9\u4fbf\u5904\u7406\ncookies_back_dict = dict_from_cookiejar(html.cookies)\npt_login_sig = cookies_back_dict['pt_login_sig']\nself.cookies_merge_dict_in_qun_qq_com.update(cookies_back_dict)\n\n# \u8bbf\u95ee\u7f51\u9875\uff0c\u4e3a\u4e86\u83b7\u53d6\u53c2\u6570ptqrtoken\nqrcode_url = 'https://ptlogin2.qq.com/ptqrshow?appid=549000912&e=2&l=M&s=4&d=72&v=4&t=0.39550762134604156'\nhtml = get_html(qrcode_url, '')\n# \u5bf9\u8fd4\u56de\u7684cookies\u8fdb\u884c\u8f6c\u5316\u4e3adict\u7c7b\u578b\uff0c\u65b9\u4fbf\u5904\u7406\ncookies_back_dict = dict_from_cookiejar(html.cookies)\nqrsig = cookies_back_dict['qrsig']\nptqrtoken = hash33_token(qrsig)\nself.cookies_merge_dict_in_qun_qq_com.update(cookies_back_dict)\n\n# \u5c06\u4e8c\u7ef4\u7801\u663e\u793a\u5230\u56fe\u7247\u6846\nBytesIOObj = BytesIO()\nBytesIOObj.write(html.content)\nqr_code = PIL.Image.open(BytesIOObj)\nimage = PIL.ImageTk.PhotoImage(qr_code)\nimage_label['image'] = image\n\n\n\n# \u5b9e\u65f6\u68c0\u6d4b\u4e8c\u7ef4\u7801\u72b6\u6001\nwhile (True):\n    # \u76ee\u6807\u7f51\u5740\n    target_url = 'http://ptlogin2.qq.com/ptqrlogin?u1=http://qun.qq.com/member.html&' + 'ptqrtoken=' + str(\n        ptqrtoken) + '&ptredirect=1&h=1&t=1&g=1&from_ui=1&ptlang=2052&action=0-0-1499652067577&js_ver=10224&js_type=1&login_sig=' + str(\n        pt_login_sig) + '&pt_uistyle=40&aid=549000912&'\n\n    # \u767b\u5f55\uff0c\u9700\u8981\u5e26\u4e0a\u8bbf\u95eecookies\n    html = get_html(target_url, self.cookies_merge_dict_in_qun_qq_com)\n\n    # \u8fd4\u56de\u7684\u54cd\u5e94\u7801\u4e3a200\u8bf4\u660e\u4e8c\u7ef4\u7801\u6ca1\u8fc7\u671f\n    if (html.status_code):\n        if ('\u4e8c\u7ef4\u7801\u672a\u5931\u6548' in html.text):\n            custom_print(u'(2/3)\u767b\u5f55qun.qq.com\u4e2d\uff0c\u5f53\u524d\u4e8c\u7ef4\u7801\u672a\u5931\u6548\uff0c\u8bf7\u4f60\u626b\u63cf\u4e8c\u7ef4\u7801\u8fdb\u884c\u767b\u5f55')\n        elif ('\u4e8c\u7ef4\u7801\u8ba4\u8bc1' in html.text):\n            custom_print(u'(2/3)\u767b\u5f55qun.qq.com\u4e2d\uff0c\u626b\u63cf\u6210\u529f\uff0c\u6b63\u5728\u8ba4\u8bc1\u4e2d')\n        elif ('\u767b\u5f55\u6210\u529f' in html.text):\n            self.is_login = True\n            custom_print(u'(2/3)\u767b\u5f55qun.qq.com\u4e2d\uff0c\u767b\u5f55\u6210\u529f')\n            break\n        if ('\u4e8c\u7ef4\u7801\u5df2\u7ecf\u5931\u6548' in html.text):\n            custom_print(u'(2/3)\u767b\u5f55qun.qq.com\u4e2d\uff0c\u5f53\u524d\u4e8c\u7ef4\u7801\u5df2\u5931\u6548\uff0c\u8bf7\u91cd\u542f\u672c\u8f6f\u4ef6')\n            exit()\n\n    # \u5ef6\u65f6\n    time.sleep(2)\n\n# \u767b\u5f55\u6210\u529f\u540e\uff0c\u628a\u8fd4\u56de\u7684cookies\u5408\u5e76\u8fdb\u53bb\ncookies_back_dict = dict_from_cookiejar(html.cookies)\nself.cookies_merge_dict_in_qun_qq_com.update(cookies_back_dict)\n# print(u'\u5f53\u524dcookies:{}'.format(cookies_merge_dict))\n\n# \u83b7\u53d6\u6b64\u6b21\u767b\u5f55\u7684qq\u53f7\u7801\nqq_list = re.findall(r'&uin=(.+?)&service', html.text)\nself.qq_number = qq_list[0]\n\n\n# \u767b\u5f55\u6210\u529f\u540e\uff0c\u4f1a\u8fd4\u56de\u4e00\u4e2a\u5730\u5740\uff0c\u9700\u8981\u5bf9\u8be5\u5730\u5740\u8fdb\u884c\u8bbf\u95ee\u4ee5\u4fbf\u83b7\u53d6\u65b0\u7684\u8fd4\u56decookies\nstartIndex = (html.text).find('http')\nendIndex = (html.text).find('pt_3rd_aid=0')\nurl = (html.text)[startIndex:endIndex] + 'pt_3rd_aid=0'\n\n# \u8fd9\u91cc\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u9700\u8981\u7981\u6b62\u91cd\u5b9a\u5411\uff0c\u624d\u80fd\u6b63\u786e\u83b7\u5f97\u8fd4\u56de\u7684cookies\nhtml = get(url, cookies=self.cookies_merge_dict_in_qun_qq_com, allow_redirects=False)\n# \u628a\u8fd4\u56de\u7684cookies\u5408\u5e76\u8fdb\u53bb\ncookies_back_dict = dict_from_cookiejar(html.cookies)\nself.cookies_merge_dict_in_qun_qq_com.update(cookies_back_dict)", "path": "examples-of-web-crawlers/9.\u4e00\u952e\u751f\u6210QQ\u4e2a\u4eba\u5386\u53f2\u62a5\u544a/qq_bot.py", "commit_date": "2019-05-20 00:00:00", "repo_name": "shengqiangzhang/examples-of-web-crawlers", "stars": 13340, "license": "mit", "language": "python", "size": 243919}
{"docstring": "# \u83b7\u53d6\u6210\u4e3a\u597d\u53cb\u7684\u5929\u6570\uff0c\u4ee5\u53ca\u4e0e\u8fd9\u4e2a\u597d\u53cb\u5171\u540c\u7684\u597d\u53cb\u4e2a\u6570\u548c\u7fa4\u804a\u4e2a\u6570\n# bkn\u7531\u53c2\u6570skey\u901a\u8fc7\u53e6\u4e00\u4e2a\u52a0\u5bc6\u51fd\u6570\u5f97\u5230\n", "func_signal": "def qzone_friendship(self, number):\n", "code": "bkn = hash33_bkn(self.cookies_merge_dict_in_qzone_qq_com['p_skey'])\n\n# \u83b7\u53d6\u53c2\u6570qzonetoken\nurllib3.disable_warnings()\ntarget_url = 'https://user.qzone.qq.com/' + self.qq_number\nhtml = get_html(target_url, self.cookies_merge_dict_in_qzone_qq_com)\nqzonetoken = re.findall(r'{ try{return \"(.+?)\";', html.text)\nqzonetoken = qzonetoken[0]\n\n\n# \u83b7\u53d6\u6211\u5728\u610f\u8c01\u6570\u636e\ntarget_url = 'https://user.qzone.qq.com/proxy/domain/r.qzone.qq.com/cgi-bin/friendship/cgi_friendship?activeuin=' + self.qq_number +'&passiveuin=' + str(number) + '&situation=1&isCalendar=1&g_tk=' + str(bkn) + '&qzonetoken=' + str(qzonetoken) + '&g_tk=' + str(bkn)\nurllib3.disable_warnings()\nhtml = get_html(target_url, self.cookies_merge_dict_in_qzone_qq_com)\n\n# \u5904\u7406\u8fd4\u56de\u6570\u636e\nresult_data = (html.text).replace('_Callback(','')\nresult_data = result_data[:len(result_data)-2]\n# \u5c06\u8fd4\u56de\u6570\u636e\u8f6c\u5316\u4e3apython\u5bf9\u8c61\nresult_data = loads(result_data)\n\nprint(result_data)", "path": "examples-of-web-crawlers/9.\u4e00\u952e\u751f\u6210QQ\u4e2a\u4eba\u5386\u53f2\u62a5\u544a/qq_bot.py", "commit_date": "2019-05-20 00:00:00", "repo_name": "shengqiangzhang/examples-of-web-crawlers", "stars": 13340, "license": "mit", "language": "python", "size": 243919}
{"docstring": "# \u83b7\u53d6\u8be5\u8d26\u6237\u7684qb\u503c\n# \u9700\u8981\u63d0\u4ea4\u7684\u6570\u636e\n", "func_signal": "def get_qb(self):\n", "code": "qq_number = str(self.qq_number)\nskey = str(self.cookies_merge_dict_in_qun_qq_com['skey'])\nurl = 'https://api.unipay.qq.com/v1/r/1450000186/wechat_query?cmd=4&pf=vip_m-pay_html5-html5&pfkey=pfkey&from_h5=1&from_https=1&openid=' + qq_number + '&openkey=' + skey + '&session_id=uin&session_type=skey'\n\n# \u8bbe\u7f6e\u8bf7\u6c42\u5934,\u6a21\u62df\u4eba\u5de5\nheader = {\n    'Accept': 'application/json, text/javascript, */*; q=0.01',\n    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36',\n    'Accept-Encoding': 'gzip, deflate',\n    'Referer': 'https://my.pay.qq.com/account/index.shtml',\n    'Origin': 'https://my.pay.qq.com',\n    'Connection': 'close'\n}\n\n# \u5c4f\u853dhttps\u8bc1\u4e66\u8b66\u544a\nurllib3.disable_warnings()\n# \u7f51\u9875\u8bbf\u95ee,get\u65b9\u5f0f\nhtml = get(url, cookies=self.cookies_merge_dict_in_qun_qq_com, headers=header, verify=False)\n\n# \u5c06\u8fd4\u56de\u6570\u636e\u89e3\u6790\u4e3apython\u5bf9\u8c61\nresult = loads(html.text)\n\nqb_value = float(result['qb_balance']) / 10\nreturn qb_value", "path": "examples-of-web-crawlers/9.\u4e00\u952e\u751f\u6210QQ\u4e2a\u4eba\u5386\u53f2\u62a5\u544a/qq_bot.py", "commit_date": "2019-05-20 00:00:00", "repo_name": "shengqiangzhang/examples-of-web-crawlers", "stars": 13340, "license": "mit", "language": "python", "size": 243919}
{"docstring": "# \u767b\u5f55id.qq.com\n\n# \u8bbf\u95ee\u7f51\u9875\uff0c\u4e3a\u4e86\u83b7\u53d6\u53c2\u6570pt_login_sig\n", "func_signal": "def login_id_qq_com(self):\n", "code": "login_url = 'https://xui.ptlogin2.qq.com/cgi-bin/xlogin?pt_disable_pwd=1&appid=1006102&daid=1&style=23&hide_border=1&proxy_url=https://id.qq.com/login/proxy.html&s_url=https://id.qq.com/index.html'\nhtml = get_html(login_url, '')\n# \u5bf9\u8fd4\u56de\u7684cookies\u8fdb\u884c\u8f6c\u5316\u4e3adict\u7c7b\u578b\uff0c\u65b9\u4fbf\u5904\u7406\ncookies_back_dict = dict_from_cookiejar(html.cookies)\npt_login_sig = cookies_back_dict['pt_login_sig']\nself.cookies_merge_dict_in_id_qq_com.update(cookies_back_dict)\n\n# \u8bbf\u95ee\u7f51\u9875\uff0c\u4e3a\u4e86\u83b7\u53d6\u53c2\u6570ptqrtoken\nqrcode_url = 'https://ssl.ptlogin2.qq.com/ptqrshow?appid=1006102&e=2&l=M&s=4&d=72&v=4&t=0.10239549811477189&daid=1&pt_3rd_aid=0'\nhtml = get_html(qrcode_url, '')\n# \u5bf9\u8fd4\u56de\u7684cookies\u8fdb\u884c\u8f6c\u5316\u4e3adict\u7c7b\u578b\uff0c\u65b9\u4fbf\u5904\u7406\ncookies_back_dict = dict_from_cookiejar(html.cookies)\nqrsig = cookies_back_dict['qrsig']\nptqrtoken = hash33_token(qrsig)\nself.cookies_merge_dict_in_id_qq_com.update(cookies_back_dict)\n\n\n# \u5c06\u4e8c\u7ef4\u7801\u663e\u793a\u5230\u56fe\u7247\u6846\nBytesIOObj = BytesIO()\nBytesIOObj.write(html.content)\nqr_code = PIL.Image.open(BytesIOObj)\nimage = PIL.ImageTk.PhotoImage(qr_code)\nimage_label['image'] = image\n\n\n# \u5b9e\u65f6\u68c0\u6d4b\u4e8c\u7ef4\u7801\u72b6\u6001\nwhile (True):\n    # \u76ee\u6807\u7f51\u5740\n    target_url = 'https://ssl.ptlogin2.qq.com/ptqrlogin?u1=https://id.qq.com/index.html&ptqrtoken=' + str(ptqrtoken) + '&ptredirect=1&h=1&t=1&g=1&from_ui=1&ptlang=2052&action=0-0-1556812236254&js_ver=19042519&js_type=1&login_sig=' + str(pt_login_sig) + '&pt_uistyle=40&aid=1006102&daid=1&'\n\n    # \u767b\u5f55\uff0c\u9700\u8981\u5e26\u4e0a\u8bbf\u95eecookies\n    html = get_html(target_url, self.cookies_merge_dict_in_id_qq_com)\n\n    # \u8fd4\u56de\u7684\u54cd\u5e94\u7801\u4e3a200\u8bf4\u660e\u4e8c\u7ef4\u7801\u6ca1\u8fc7\u671f\n    if (html.status_code):\n        if ('\u4e8c\u7ef4\u7801\u672a\u5931\u6548' in html.text):\n            custom_print(u'(1/3)\u767b\u5f55id.qq.com\u4e2d\uff0c\u5f53\u524d\u4e8c\u7ef4\u7801\u672a\u5931\u6548\uff0c\u8bf7\u4f60\u626b\u63cf\u4e8c\u7ef4\u7801\u8fdb\u884c\u767b\u5f55')\n        elif ('\u4e8c\u7ef4\u7801\u8ba4\u8bc1' in html.text):\n            custom_print(u'(1/3)\u767b\u5f55id.qq.com\u4e2d\uff0c\u626b\u63cf\u6210\u529f\uff0c\u6b63\u5728\u8ba4\u8bc1\u4e2d')\n        elif ('\u767b\u5f55\u6210\u529f' in html.text):\n            self.is_login = True\n            custom_print(u'(1/3)\u767b\u5f55id.qq.com\u4e2d\uff0c\u767b\u5f55\u6210\u529f')\n            break\n        if ('\u4e8c\u7ef4\u7801\u5df2\u7ecf\u5931\u6548' in html.text):\n            custom_print(u'(1/3)\u767b\u5f55id.qq.com\u4e2d\uff0c\u5f53\u524d\u4e8c\u7ef4\u7801\u5df2\u5931\u6548\uff0c\u8bf7\u91cd\u542f\u672c\u8f6f\u4ef6')\n            exit()\n\n    # \u5ef6\u65f6\n    time.sleep(2)\n\n# \u767b\u5f55\u6210\u529f\u540e\uff0c\u628a\u8fd4\u56de\u7684cookies\u5408\u5e76\u8fdb\u53bb\nself.cookies_merge_dict_in_id_qq_com = dict_from_cookiejar(html.cookies)\nself.cookies_merge_dict_in_id_qq_com.update(cookies_back_dict)\n# print(u'\u5f53\u524dcookies:{}'.format(cookies_merge_dict))\n\n# \u83b7\u53d6\u6b64\u6b21\u767b\u5f55\u7684qq\u53f7\u7801\nqq_list = re.findall(r'&uin=(.+?)&service', html.text)\nself.qq_number = qq_list[0]\n\n\n# \u767b\u5f55\u6210\u529f\u540e\uff0c\u4f1a\u8fd4\u56de\u4e00\u4e2a\u5730\u5740\uff0c\u9700\u8981\u5bf9\u8be5\u5730\u5740\u8fdb\u884c\u8bbf\u95ee\u4ee5\u4fbf\u83b7\u53d6\u65b0\u7684\u8fd4\u56decookies\nstartIndex = (html.text).find('http')\nendIndex = (html.text).find('pt_3rd_aid=0')\nurl = (html.text)[startIndex:endIndex] + 'pt_3rd_aid=0'\n\n# \u5c4f\u853dhttps\u8bc1\u4e66\u8b66\u544a\nurllib3.disable_warnings()\n\n# \u8fd9\u91cc\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u9700\u8981\u7981\u6b62\u91cd\u5b9a\u5411\uff0c\u624d\u80fd\u6b63\u786e\u83b7\u5f97\u8fd4\u56de\u7684cookies\nhtml = get(url, cookies=self.cookies_merge_dict_in_id_qq_com, allow_redirects=False, verify=False)\n# \u628a\u8fd4\u56de\u7684cookies\u5408\u5e76\u8fdb\u53bb\ncookies_back_dict = dict_from_cookiejar(html.cookies)\nself.cookies_merge_dict_in_id_qq_com.update(cookies_back_dict)", "path": "examples-of-web-crawlers/9.\u4e00\u952e\u751f\u6210QQ\u4e2a\u4eba\u5386\u53f2\u62a5\u544a/qq_bot.py", "commit_date": "2019-05-20 00:00:00", "repo_name": "shengqiangzhang/examples-of-web-crawlers", "stars": 13340, "license": "mit", "language": "python", "size": 243919}
{"docstring": "# \u83b7\u53d6\u6307\u5b9aqq\u7684\u5934\u50cf\uff0csize\u7684\u503c\u53ef\u4e3a40\u3001100\u3001140\uff0c\u9ed8\u8ba4\u4e3a100\n# \u5c4f\u853dhttps\u8bc1\u4e66\u8b66\u544a\n", "func_signal": "def get_profile_picture(self, qq_number, size=100):\n", "code": "urllib3.disable_warnings()\n\n# \u8bbe\u7f6e\u8bf7\u6c42\u5934,\u6a21\u62df\u4eba\u5de5\nheader = {\n    'Accept': 'application/json, text/javascript, */*; q=0.01',\n    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36',\n    'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',\n    'Referer':'http://find.qq.com/'\n}\n\n# \u7f51\u9875\u8bbf\u95ee,get\u65b9\u5f0f\nhtml = get('http://q1.qlogo.cn/g?b=qq&nk=' + str(qq_number) + '&s=' + str(size), headers=header, verify=False)\nreturn html.content", "path": "examples-of-web-crawlers/9.\u4e00\u952e\u751f\u6210QQ\u4e2a\u4eba\u5386\u53f2\u62a5\u544a/qq_bot.py", "commit_date": "2019-05-20 00:00:00", "repo_name": "shengqiangzhang/examples-of-web-crawlers", "stars": 13340, "license": "mit", "language": "python", "size": 243919}
{"docstring": "# \u83b7\u53d6\u6240\u6709qq\u597d\u53cb\u57fa\u672c\u4fe1\u606f\n        # bkn\u7531\u53c2\u6570skey\u901a\u8fc7\u53e6\u4e00\u4e2a\u52a0\u5bc6\u51fd\u6570\u5f97\u5230\n", "func_signal": "def get_all_friends_in_qq(self):\n", "code": "        bkn = hash33_bkn(self.cookies_merge_dict_in_qun_qq_com['skey'])\n        submit_data = {'bkn': bkn}\n        html = post_html('https://qun.qq.com/cgi-bin/qun_mgr/get_friend_list', self.cookies_merge_dict_in_qun_qq_com, submit_data)\n        friend_info = loads(html.text)\n        # print(friend_info)\n        return friend_info['result']", "path": "examples-of-web-crawlers/9.\u4e00\u952e\u751f\u6210QQ\u4e2a\u4eba\u5386\u53f2\u62a5\u544a/qq_bot.py", "commit_date": "2019-05-20 00:00:00", "repo_name": "shengqiangzhang/examples-of-web-crawlers", "stars": 13340, "license": "mit", "language": "python", "size": 243919}
{"docstring": "\"\"\"\u83b7\u53d6\u4e66\u67b6\u4e0a\u6240\u6709\u4e66\"\"\"\n", "func_signal": "def get_bookshelf(userVid, headers):\n", "code": "url = \"https://i.weread.qq.com/shelf/friendCommon\"\nparams = dict(userVid=userVid)\nr = requests.get(url, params=params, headers=headers, verify=False)\nif r.ok:\n    data = r.json()\nelse:\n    raise Exception(r.text)\n\nbooks_finish_read = set() # \u5df2\u8bfb\u5b8c\u7684\u4e66\u7c4d\nbooks_recent_read = set() # \u6700\u8fd1\u9605\u8bfb\u7684\u4e66\u7c4d\nbooks_all = set() # \u4e66\u67b6\u4e0a\u7684\u6240\u6709\u4e66\u7c4d\n\n\nfor book in data['finishReadBooks']:\n    if not book['bookId'].isdigit(): # \u8fc7\u6ee4\u516c\u4f17\u53f7\n        continue\n    b = Book(book['bookId'], book['title'], book['author'], book['cover'], book['intro'], book['category'])\n    books_finish_read.add(b)\nbooks_finish_read = list(books_finish_read)\nbooks_finish_read.sort(key=itemgetter(-1)) # operator.itemgetter(-1)\u6307\u7684\u662f\u83b7\u53d6\u5bf9\u8c61\u7684\u6700\u540e\u4e00\u4e2a\u57df\u7684\u503c\uff0c\u5373\u4ee5category\u8fdb\u884c\u6392\u5e8f\n\n\n\nfor book in data['recentBooks']:\n    if not book['bookId'].isdigit(): # \u8fc7\u6ee4\u516c\u4f17\u53f7\n        continue\n    b = Book(book['bookId'], book['title'], book['author'], book['cover'], book['intro'], book['category'])\n    books_recent_read.add(b)\nbooks_recent_read = list(books_recent_read)\nbooks_recent_read.sort(key=itemgetter(-1)) # operator.itemgetter(-1)\u6307\u7684\u662f\u83b7\u53d6\u5bf9\u8c61\u7684\u6700\u540e\u4e00\u4e2a\u57df\u7684\u503c\uff0c\u5373\u4ee5category\u8fdb\u884c\u6392\u5e8f\n\n\nbooks_all = books_finish_read + books_recent_read\n\nreturn dict(finishReadBooks=books_finish_read, recentBooks=books_recent_read, allBooks=books_all)", "path": "examples-of-web-crawlers/12.\u4e00\u952e\u5bfc\u51fa\u5fae\u4fe1\u8bfb\u4e66\u7684\u4e66\u7c4d\u548c\u7b14\u8bb0/wereader.py", "commit_date": "2020-04-11 00:00:00", "repo_name": "shengqiangzhang/examples-of-web-crawlers", "stars": 13340, "license": "mit", "language": "python", "size": 243919}
{"docstring": "# \u5224\u65ad\u6b64\u6b21\u767b\u5f55\u7684qq\u662f\u5426\u4e3avip\u6216\u8005svip\n# \u9700\u8981\u63d0\u4ea4\u7684\u6570\u636e\n# bkn\u7531\u53c2\u6570skey\u901a\u8fc7\u53e6\u4e00\u4e2a\u52a0\u5bc6\u51fd\u6570\u5f97\u5230\n", "func_signal": "def is_vip_svip(self):\n", "code": "bkn = hash33_bkn(self.cookies_merge_dict_in_qun_qq_com['skey'])\nqq_number = str(self.qq_number)\nskey = str(self.cookies_merge_dict_in_qun_qq_com['skey'])\nurl = 'https://proxy.vip.qq.com/cgi-bin/srfentry.fcgi?bkn=' + str(bkn) + '&ts=&g_tk=' + str(bkn) + '&data={\"11053\":{\"iAppId\":1,\"iKeyType\":1,\"sClientIp\":\"\",\"sSessionKey\":\"' + skey + '\",\"sUin\":\"' + qq_number + '\"}}'\n\n# \u8bbe\u7f6e\u8bf7\u6c42\u5934,\u6a21\u62df\u4eba\u5de5\nheader = {\n    'Accept': 'application/json, text/javascript, */*; q=0.01',\n    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36',\n    'Accept-Encoding': 'gzip, deflate',\n    'Referer': 'https://huifu.qq.com/recovery/index.html?frag=1',\n    'Origin': 'https://huifu.qq.com',\n    'Connection': 'close'\n}\n\n# \u5c4f\u853dhttps\u8bc1\u4e66\u8b66\u544a\nurllib3.disable_warnings()\n# \u7f51\u9875\u8bbf\u95ee,post\u65b9\u5f0f\nhtml = get(url, cookies=self.cookies_merge_dict_in_qun_qq_com, headers=header, verify=False)\n\n# \u5c06\u8fd4\u56de\u6570\u636e\u89e3\u6790\u4e3apython\u5bf9\u8c61\nresult = loads(html.text)\nisSvip = result['11053']['data']['isSvip']\nisVip = result['11053']['data']['isVip']\nreturn {'isSvip':isSvip, 'isVip':isVip}", "path": "examples-of-web-crawlers/9.\u4e00\u952e\u751f\u6210QQ\u4e2a\u4eba\u5386\u53f2\u62a5\u544a/qq_bot.py", "commit_date": "2019-05-20 00:00:00", "repo_name": "shengqiangzhang/examples-of-web-crawlers", "stars": 13340, "license": "mit", "language": "python", "size": 243919}
{"docstring": "\"\"\"\u5224\u65ad\u662f\u5426\u767b\u5f55\u6210\u529f\"\"\"\n", "func_signal": "def login_success(headers):\n", "code": "url = \"https://i.weread.qq.com/user/notebooks\"\nr = requests.get(url,headers=headers,verify=False)\n\nif r.ok:\n    return True\nelse:\n    return False", "path": "examples-of-web-crawlers/12.\u4e00\u952e\u5bfc\u51fa\u5fae\u4fe1\u8bfb\u4e66\u7684\u4e66\u7c4d\u548c\u7b14\u8bb0/wereader.py", "commit_date": "2020-04-11 00:00:00", "repo_name": "shengqiangzhang/examples-of-web-crawlers", "stars": 13340, "license": "mit", "language": "python", "size": 243919}
{"docstring": "# \u83b7\u53d6\u67d0\u4e2aqq\u597d\u53cb\u7684\u8be6\u7ec6\u4fe1\u606f\n# \u9700\u8981\u63d0\u4ea4\u7684\u6570\u636e\n        # bkn\u7531\u53c2\u6570skey\u901a\u8fc7\u53e6\u4e00\u4e2a\u52a0\u5bc6\u51fd\u6570\u5f97\u5230\n", "func_signal": "def get_info_in_qq_friend(self,qq_number):\n", "code": "        bkn = hash33_bkn(self.cookies_merge_dict_in_qun_qq_com['skey'])\n        submit_data = {'keyword':str(qq_number), 'ldw': str(bkn), 'num':'20', 'page':'0', 'sessionid':'0', 'agerg':'0', 'sex':'0', 'firston':'0', 'video':'0', 'country':'1', 'province':'65535', 'city':'0', 'district':'0', 'hcountry':'1', 'hprovince':'0', 'hcity':'0', 'hdistrict':'0', 'online':'0'}\n# \u9700\u8981\u63d0\u4ea4\u7684cookies\n        # cookies = {'uin':self.cookies_merge_dict_in_qun_qq_com['uin'], 'skey':self.cookies_merge_dict_in_qun_qq_com['skey'], 'ptisp':self.cookies_merge_dict_in_qun_qq_com['ptisp'], 'RK':self.cookies_merge_dict_in_qun_qq_com['RK'], 'ptcz':self.cookies_merge_dict_in_qun_qq_com['ptcz']}\n# \u8bbe\u7f6e\u8bf7\u6c42\u5934,\u6a21\u62df\u4eba\u5de5\n        header = {\n            'Accept': 'application/json, text/javascript, */*; q=0.01',\n            'Origin': 'http://find.qq.com',\n            'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36',\n            'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',\n            'Referer':'http://find.qq.com/',\n        }\n# \u5c4f\u853dhttps\u8bc1\u4e66\u8b66\u544a\n        urllib3.disable_warnings()\n        # \u7f51\u9875\u8bbf\u95ee,post\u65b9\u5f0f\n        html = post('http://cgi.find.qq.com/qqfind/buddy/search_v3', data=submit_data, cookies=self.cookies_merge_dict_in_qun_qq_com, headers=header, verify=False)\n# \u5c06\u597d\u53cb\u4fe1\u606f\u89e3\u6790\u4e3apython\u5bf9\u8c61\n        friend_info = loads(html.text)\n        # print(friend_info)\n        return friend_info['result']['buddy']['info_list'][0]", "path": "examples-of-web-crawlers/9.\u4e00\u952e\u751f\u6210QQ\u4e2a\u4eba\u5386\u53f2\u62a5\u544a/qq_bot.py", "commit_date": "2019-05-20 00:00:00", "repo_name": "shengqiangzhang/examples-of-web-crawlers", "stars": 13340, "license": "mit", "language": "python", "size": 243919}
{"docstring": "# \u83b7\u53d6\u5c4f\u5e55 \u5bbd\u3001\u9ad8\n", "func_signal": "def center_window(self, w, h):\n", "code": "ws = self.root.winfo_screenwidth()\nhs = self.root.winfo_screenheight()\n# \u8ba1\u7b97 x, y \u4f4d\u7f6e\nx = (ws / 2) - (w / 2)\ny = (hs / 2) - (h / 2)\nself.root.geometry('%dx%d+%d+%d' % (w, h, x, y))", "path": "examples-of-web-crawlers/9.\u4e00\u952e\u751f\u6210QQ\u4e2a\u4eba\u5386\u53f2\u62a5\u544a/tkinter_gui.py", "commit_date": "2019-05-20 00:00:00", "repo_name": "shengqiangzhang/examples-of-web-crawlers", "stars": 13340, "license": "mit", "language": "python", "size": 243919}
{"docstring": "\"\"\"\u83b7\u53d6\u4e66\u7684\u76ee\u5f55\"\"\"\n", "func_signal": "def get_chapters(bookId, headers):\n", "code": "url = \"https://i.weread.qq.com/book/chapterInfos\"\ndata = '{\"bookIds\":[\"%d\"],\"synckeys\":[0]}' % bookId\n\nr = requests.post(url, data=data, headers=headers, verify=False)\n\nif r.ok:\n    data = r.json()\n    clipboard.copy(json.dumps(data, indent=4, sort_keys=True))\nelse:\n    raise Exception(r.text)\n\nchapters = []\nfor item in data['data'][0]['updated']:\n    if 'anchors' in item:\n        chapters.append((item.get('level', 1), item['title']))\n        for ac in item['anchors']:\n            chapters.append((ac['level'], ac['title']))\n\n    elif 'level' in item:\n        chapters.append((item.get('level', 1), item['title']))\n\n    else:\n        chapters.append((1, item['title']))\n\nreturn chapters", "path": "examples-of-web-crawlers/12.\u4e00\u952e\u5bfc\u51fa\u5fae\u4fe1\u8bfb\u4e66\u7684\u4e66\u7c4d\u548c\u7b14\u8bb0/wereader.py", "commit_date": "2020-04-11 00:00:00", "repo_name": "shengqiangzhang/examples-of-web-crawlers", "stars": 13340, "license": "mit", "language": "python", "size": 243919}
{"docstring": "# \u83b7\u53d6\u6240\u6709\u7fa4\u57fa\u672c\u4fe1\u606f\n        # bkn\u7531\u53c2\u6570skey\u901a\u8fc7\u53e6\u4e00\u4e2a\u52a0\u5bc6\u51fd\u6570\u5f97\u5230\n", "func_signal": "def get_group(self):\n", "code": "        bkn = hash33_bkn(self.cookies_merge_dict_in_qun_qq_com['skey'])\n        submit_data = {'bkn': bkn}\n        html = post_html('https://qun.qq.com/cgi-bin/qun_mgr/get_group_list', self.cookies_merge_dict_in_qun_qq_com, submit_data)\n        group_info = loads(html.text)\n        print(group_info)\n        return group_info['join']", "path": "examples-of-web-crawlers/9.\u4e00\u952e\u751f\u6210QQ\u4e2a\u4eba\u5386\u53f2\u62a5\u544a/qq_bot.py", "commit_date": "2019-05-20 00:00:00", "repo_name": "shengqiangzhang/examples-of-web-crawlers", "stars": 13340, "license": "mit", "language": "python", "size": 243919}
{"docstring": "\"\"\"\u83b7\u53d6\u4e66\u7684\u8be6\u60c5\"\"\"\n", "func_signal": "def get_bookinfo(bookId, headers):\n", "code": "url = \"https://i.weread.qq.com/book/info\"\nparams = dict(bookId=bookId)\nr = requests.get(url, params=params, headers=headers, verify=False)\n\nif r.ok:\n    data = r.json()\nelse:\n    raise Exception(r.text)\nreturn data", "path": "examples-of-web-crawlers/12.\u4e00\u952e\u5bfc\u51fa\u5fae\u4fe1\u8bfb\u4e66\u7684\u4e66\u7c4d\u548c\u7b14\u8bb0/wereader.py", "commit_date": "2020-04-11 00:00:00", "repo_name": "shengqiangzhang/examples-of-web-crawlers", "stars": 13340, "license": "mit", "language": "python", "size": 243919}
{"docstring": "# \u8f93\u51fa\u804a\u5929\u5185\u5bb9\n", "func_signal": "def print_others(msg):\n", "code": "    print(msg.text)\n# \u53ef\u91c7\u7528snownlp\u6216\u8005jieba\u7b49\u8fdb\u884c\u5206\u8bcd\u3001\u60c5\u611f\u5206\u6790\uff0c\u7531\u4e8e\u6253\u5305\u540e\u6587\u4ef6\u4f53\u79ef\u592a\u5927\uff0c\u6545\u6682\u65f6\u4e0d\u91c7\u7528\u8fd9\u79cd\u65b9\u5f0f\n    # \u4ec5\u4ec5\u662f\u76f4\u63a5\u8c03\u7528\u7f51\u7edc\u63a5\u53e3\n    \n    # \u505a\u6781\u5176\u7b80\u5355\u7684\u60c5\u611f\u5206\u6790\n    # \u7ed3\u679c\u4ec5\u4f9b\u53c2\u8003\uff0c\u8bf7\u52ff\u5b8c\u5168\u76f8\u4fe1\n    postData = {'data':msg.text}\n    response = post('https://bosonnlp.com/analysis/sentiment?analysisType=',data=postData)\n    data = response.text\n# \u60c5\u611f\u8bc4\u5206\u6307\u6570(\u8d8a\u63a5\u8fd11\u8868\u793a\u5fc3\u60c5\u8d8a\u597d\uff0c\u8d8a\u63a5\u8fd10\u8868\u793a\u5fc3\u60c5\u8d8a\u5dee)\n    now_mod_rank = (data.split(',')[0]).replace('[[','')\n    print(\"\u6765\u81ea\u5973\u53cb\u7684\u6d88\u606f:%s\\n\u5f53\u524d\u60c5\u611f\u5f97\u5206:%s\\n\u8d8a\u63a5\u8fd11\u8868\u793a\u5fc3\u60c5\u8d8a\u597d\uff0c\u8d8a\u63a5\u8fd10\u8868\u793a\u5fc3\u60c5\u8d8a\u5dee\uff0c\u60c5\u611f\u7ed3\u679c\u4ec5\u4f9b\u53c2\u8003\uff0c\u8bf7\u52ff\u5b8c\u5168\u76f8\u4fe1\uff01\\n\\n\" % (msg.text, now_mod_rank))\n# \u53d1\u9001\u4fe1\u606f\u5230\u6587\u4ef6\u4f20\u8f93\u52a9\u624b\n    mood_message = u\"\u6765\u81ea\u5973\u53cb\u7684\u6d88\u606f:\" + msg.text + \"\\n\u5f53\u524d\u60c5\u611f\u5f97\u5206:\" + now_mod_rank + \"\\n\u8d8a\u63a5\u8fd11\u8868\u793a\u5fc3\u60c5\u8d8a\u597d\uff0c\u8d8a\u63a5\u8fd10\u8868\u793a\u5fc3\u60c5\u8d8a\u5dee\uff0c\u60c5\u611f\u7ed3\u679c\u4ec5\u4f9b\u53c2\u8003\uff0c\u8bf7\u52ff\u5b8c\u5168\u76f8\u4fe1\uff01\\n\\n\"\n    bot.file_helper.send(mood_message)", "path": "examples-of-web-crawlers/4.\u6bcf\u5929\u4e0d\u540c\u65f6\u95f4\u6bb5\u901a\u8fc7\u5fae\u4fe1\u53d1\u6d88\u606f\u63d0\u9192\u5973\u53cb/say_to_lady.py", "commit_date": "2019-03-21 00:00:00", "repo_name": "shengqiangzhang/examples-of-web-crawlers", "stars": 13340, "license": "mit", "language": "python", "size": 243919}
{"docstring": "# \u83b7\u53d6\u5e2e\u522b\u4eba\u7684\u4ee3\u4ed8\n# \u9700\u8981\u63d0\u4ea4\u7684\u6570\u636e\n", "func_signal": "def get_pay_for_another(self):\n", "code": "skey = str(self.cookies_merge_dict_in_qun_qq_com['skey'])\nurl = 'https://pay.qq.com/cgi-bin/personal/account_msg.cgi?p=0.6796416908412624&cmd=1&sck=' + get_sck(skey) + '&type=100&showitem=2&per=100&pageno=1&r=0.3177912609760205'\n\n# \u8bbe\u7f6e\u8bf7\u6c42\u5934,\u6a21\u62df\u4eba\u5de5\nheader = {\n    'Accept': 'application/json, text/javascript, */*; q=0.01',\n    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36',\n    'Accept-Encoding': 'gzip, deflate',\n    'Referer': 'https://pay.qq.com/infocenter/infocenter.shtml?asktype=100',\n    'Connection': 'keep-alive'\n}\n\n# \u5c4f\u853dhttps\u8bc1\u4e66\u8b66\u544a\nurllib3.disable_warnings()\n# \u7f51\u9875\u8bbf\u95ee,get\u65b9\u5f0f\nhtml = get(url, cookies=self.cookies_merge_dict_in_qun_qq_com, headers=header, verify=False)\n\n# \u5c06\u8fd4\u56de\u6570\u636e\u89e3\u6790\u4e3apython\u5bf9\u8c61\nresult = loads(html.text)\n# print(result)\n\nreturn result['resultinfo']['list']", "path": "examples-of-web-crawlers/9.\u4e00\u952e\u751f\u6210QQ\u4e2a\u4eba\u5386\u53f2\u62a5\u544a/qq_bot.py", "commit_date": "2019-05-20 00:00:00", "repo_name": "shengqiangzhang/examples-of-web-crawlers", "stars": 13340, "license": "mit", "language": "python", "size": 243919}
{"docstring": "# \u83b7\u53d6\u8be5\u8d26\u6237\u7684\u8be6\u7ec6\u8d44\u6599\n\n# \u5b58\u50a8\u8fd4\u56de\u6570\u636e\n", "func_signal": "def get_detail_information(self):\n", "code": "result = {}\n\n# \u83b7\u53d6\u57fa\u672c\u4fe1\u606f\n# bkn\u7531\u53c2\u6570skey\u901a\u8fc7\u53e6\u4e00\u4e2a\u52a0\u5bc6\u51fd\u6570\u5f97\u5230\nbkn = hash33_bkn(self.cookies_merge_dict_in_id_qq_com['skey'])\nurl = 'https://id.qq.com/cgi-bin/summary?ldw=' + str(bkn)\n\n# \u8bbe\u7f6e\u8bf7\u6c42\u5934,\u6a21\u62df\u4eba\u5de5\nheader = {\n    'Accept': '*/*',\n    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36',\n    'Accept-Encoding': 'gzip, deflate, br',\n    'accept-language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7',\n    'Referer': 'https://id.qq.com/home/home.html?ver=10049&',\n    'Connection': 'keep-alive'\n}\n\n# \u5c4f\u853dhttps\u8bc1\u4e66\u8b66\u544a\nurllib3.disable_warnings()\n# \u7f51\u9875\u8bbf\u95ee,get\u65b9\u5f0f\nhtml = get(url, cookies=self.cookies_merge_dict_in_id_qq_com, headers=header, verify=False)\n# \u6307\u5b9a\u8fd4\u56de\u6570\u636e\u7f16\u7801\u683c\u5f0f\nhtml.encoding = 'utf-8'\n# \u5c06\u8fd4\u56de\u6570\u636e\u89e3\u6790\u4e3apython\u5bf9\u8c61\uff0c\u5e76\u5b58\u5165result\nresult.update(loads(html.text))\n\n\n\n\n# \u83b7\u53d6\u5728\u7ebf\u5929\u6570\nskey = str(self.cookies_merge_dict_in_id_qq_com['skey'])\ng_tk = str(get_csrf_token(skey))\nurl = 'https://cgi.vip.qq.com/querygrow/get?r=0.8102122812749504&g_tk=' + g_tk\n# \u8bbe\u7f6e\u8bf7\u6c42\u5934,\u6a21\u62df\u4eba\u5de5\nheader = {\n    'Accept': '*/*',\n    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36',\n    'Accept-Encoding': 'gzip, deflate, br',\n    'accept-language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7',\n    'Referer': 'https://id.qq.com/level/mylevel.html?ver=10043&',\n    'Connection': 'keep-alive'\n}\n# \u5c4f\u853dhttps\u8bc1\u4e66\u8b66\u544a\nurllib3.disable_warnings()\n# \u7f51\u9875\u8bbf\u95ee,get\u65b9\u5f0f\nhtml = get(url, cookies=self.cookies_merge_dict_in_id_qq_com, headers=header, verify=False)\n# \u6307\u5b9a\u8fd4\u56de\u6570\u636e\u7f16\u7801\u683c\u5f0f\nhtml.encoding = 'utf-8'\n# \u5c06\u8fd4\u56de\u6570\u636e\u89e3\u6790\u4e3apython\u5bf9\u8c61\uff0c\u5e76\u5b58\u5165result\nresult.update(loads(html.text))\n\n\n\n# \u83b7\u53d6\u66f4\u52a0\u8be6\u7ec6\u7684\u8d44\u6599\nwhile(True):\n    url = 'https://id.qq.com/cgi-bin/userinfo?ldw=' + str(bkn)\n    # \u8bbe\u7f6e\u8bf7\u6c42\u5934,\u6a21\u62df\u4eba\u5de5\n    header = {\n        'Accept': '*/*',\n        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36',\n        'Accept-Encoding': 'gzip, deflate, br',\n        'accept-language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7',\n        'Referer': 'https://id.qq.com/myself/myself.html?ver=10045&',\n        'Connection': 'keep-alive'\n    }\n    # \u5c4f\u853dhttps\u8bc1\u4e66\u8b66\u544a\n    urllib3.disable_warnings()\n    # \u7f51\u9875\u8bbf\u95ee,get\u65b9\u5f0f\n    html = get(url, cookies=self.cookies_merge_dict_in_id_qq_com, headers=header, verify=False)\n    # \u6307\u5b9a\u8fd4\u56de\u6570\u636e\u7f16\u7801\u683c\u5f0f\n    html.encoding = 'utf-8'\n\n    # \u8be5\u7f51\u7ad9\u6709\u65f6\u5019\u4f1a\u8fd4\u56de\u7a7a\u6570\u636e\uff0c\u6240\u4ee5\u8981\u5224\u65ad\u4e00\u4e0b\uff0c\u5982\u679c\u662f\u7a7a\u5219\u91cd\u65b0\u53d1\u5305\u83b7\u53d6\n    if(html.text != ''):\n        # \u5c06\u8fd4\u56de\u6570\u636e\u89e3\u6790\u4e3apython\u5bf9\u8c61\uff0c\u5e76\u5b58\u5165result\n        result.update(loads(html.text))\n\n        # \u8df3\u51fa\u5faa\u73af\n        break\n\n\n# \u6570\u636e\u83b7\u53d6\u5b8c\u6bd5\uff0c\u7b5b\u9009\u51fa\u6211\u4eec\u60f3\u8fd4\u56de\u7684\u7ed3\u679c\ndata = {}\ndata.update({'bind_email':result['bind_email']})\ndata.update({'nickname': result['nick']})\ndata.update({'age': result['age']})\ndata.update({'birthday': str(result['bir_y']) + '/' + str(result['bir_m']) + '/' + str(result['bir_d'])})\ndata.update({'last_contact_friend_count': result['chat_count']})\ndata.update({'friend_count': result['friend_count']})\ndata.update({'group_count': result['group_count']})\ndata.update({'remark_friend_count': result['remark_count']})\ndata.update({'odd_friend_count': result['odd_count']})\ndata.update({'qq_level': result['level']})\ndata.update({'qq_level_rank': str(result['level_rank']) + '/' + str(result['friend_count'])})\ndata.update({'qq_age': result['qq_age']})\ndata.update({'mobile_qq_online_hour': result['iMobileQQOnlineTime']})\ndata.update({'no_hide_online_hour': result['iNoHideOnlineTime']})\ndata.update({'total_active_day': result['iTotalActiveDay']})\nqq_signature = result['ln'].replace('&nbsp;',' ')\n# data.update({'qq_signature': qq_signature})\n\nreturn data", "path": "examples-of-web-crawlers/9.\u4e00\u952e\u751f\u6210QQ\u4e2a\u4eba\u5386\u53f2\u62a5\u544a/qq_bot.py", "commit_date": "2019-05-20 00:00:00", "repo_name": "shengqiangzhang/examples-of-web-crawlers", "stars": 13340, "license": "mit", "language": "python", "size": 243919}
{"docstring": "# \u83b7\u53d6\u4e00\u4e2a\u968f\u673auser_agent\u548cReferer\n", "func_signal": "def get_fund_code():\n", "code": "header = {'User-Agent': random.choice(user_agent_list),\n          'Referer': random.choice(referer_list)\n}\n\n# \u8bbf\u95ee\u7f51\u9875\u63a5\u53e3\nreq = requests.get('http://fund.eastmoney.com/js/fundcode_search.js', timeout=5, headers=header)\n\n# \u83b7\u53d6\u6240\u6709\u57fa\u91d1\u4ee3\u7801\nfund_code = req.content.decode()\nfund_code = fund_code.replace(\"\ufeffvar r = [\",\"\").replace(\"];\",\"\")\n\n# \u6b63\u5219\u6279\u91cf\u63d0\u53d6\nfund_code = re.findall(r\"[\\[](.*?)[\\]]\", fund_code)\n\n# \u5bf9\u6bcf\u884c\u6570\u636e\u8fdb\u884c\u5904\u7406\uff0c\u5e76\u5b58\u50a8\u5230fund_code_list\u5217\u8868\u4e2d\nfund_code_list = []\nfor sub_data in fund_code:\n    data = sub_data.replace(\"\\\"\",\"\").replace(\"'\",\"\")\n    data_list = data.split(\",\")\n    fund_code_list.append(data_list)\n\nreturn fund_code_list", "path": "examples-of-web-crawlers/7.\u722c\u53d6\u5929\u5929\u57fa\u91d1\u7f51\u6240\u6709\u57fa\u91d1\u6570\u636e/main.py", "commit_date": "2019-03-27 00:00:00", "repo_name": "shengqiangzhang/examples-of-web-crawlers", "stars": 13340, "license": "mit", "language": "python", "size": 243919}
{"docstring": "# \u65b0\u5efa\u4e00\u4e2a\u5de5\u4f5c\u7c3f\n", "func_signal": "def write_excel_xls(path, sheet_name_list, value):\n", "code": "workbook = xlwt.Workbook()\n\n# \u83b7\u53d6\u9700\u8981\u5199\u5165\u6570\u636e\u7684\u884c\u6570\nindex = len(value)\n\nfor sheet_name in sheet_name_list:\n\n    # \u5728\u5de5\u4f5c\u7c3f\u4e2d\u65b0\u5efa\u4e00\u4e2a\u8868\u683c\n    sheet = workbook.add_sheet(sheet_name)\n\n    # \u5f80\u8fd9\u4e2a\u5de5\u4f5c\u7c3f\u7684\u8868\u683c\u4e2d\u5199\u5165\u6570\u636e\n    for i in range(0, index):\n        for j in range(0, len(value[i])):\n            sheet.write(i, j, value[i][j])\n\n# \u4fdd\u5b58\u5de5\u4f5c\u7c3f\nworkbook.save(path)", "path": "examples-of-web-crawlers/12.\u4e00\u952e\u5bfc\u51fa\u5fae\u4fe1\u8bfb\u4e66\u7684\u4e66\u7c4d\u548c\u7b14\u8bb0/excel_func.py", "commit_date": "2020-04-11 00:00:00", "repo_name": "shengqiangzhang/examples-of-web-crawlers", "stars": 13340, "license": "mit", "language": "python", "size": 243919}
{"docstring": "# \u83b7\u53d6\u67d0\u4e2a\u7fa4\u7684\u7fa4\u6210\u5458\n        # bkn\u7531\u53c2\u6570skey\u901a\u8fc7\u53e6\u4e00\u4e2a\u52a0\u5bc6\u51fd\u6570\u5f97\u5230\n", "func_signal": "def get_members_in_group(self,group_number):\n", "code": "        bkn = hash33_bkn(self.cookies_merge_dict_in_qun_qq_com['skey'])\n        url = 'http://qinfo.clt.qq.com/cgi-bin/qun_info/get_members_info_v1?friends=1&name=1&gc=' + str(group_number) + '&bkn=' + str(bkn) + '&src=qinfo_v3'\n        html = get_html(url, self.cookies_merge_dict_in_qun_qq_com)\n        group_member = loads(html.text)\n        return group_member", "path": "examples-of-web-crawlers/9.\u4e00\u952e\u751f\u6210QQ\u4e2a\u4eba\u5386\u53f2\u62a5\u544a/qq_bot.py", "commit_date": "2019-05-20 00:00:00", "repo_name": "shengqiangzhang/examples-of-web-crawlers", "stars": 13340, "license": "mit", "language": "python", "size": 243919}
{"docstring": "\"\"\"Return request cookies.\n\nA read-only dictionary-like object.\n\"\"\"\n", "func_signal": "def cookies(self) -> Mapping[str, str]:\n", "code": "raw = self.headers.get(hdrs.COOKIE, \"\")\nparsed = SimpleCookie(raw)  # type: SimpleCookie[str]\nreturn MappingProxyType({key: val.value for key, val in parsed.items()})", "path": "aiohttp/aiohttp/web_request.py", "commit_date": "2020-12-17 00:00:00", "repo_name": "aio-libs/aiohttp", "stars": 14425, "license": "other", "language": "python", "size": 25666}
{"docstring": "\"\"\"Enables automatic chunked transfer encoding.\"\"\"\n", "func_signal": "def enable_chunked_encoding(self) -> None:\n", "code": "self._chunked = True\n\nif hdrs.CONTENT_LENGTH in self._headers:\n    raise RuntimeError(\n        \"You can't enable chunked encoding when \" \"a content length is set\"\n    )", "path": "aiohttp/aiohttp/web_response.py", "commit_date": "2020-11-14 00:00:00", "repo_name": "aio-libs/aiohttp", "stars": 14425, "license": "other", "language": "python", "size": 25666}
{"docstring": "\"\"\"Hostname of the request.\n\nHostname is resolved in this order:\n\n- overridden value by .clone(host=new_host) call.\n- HOST HTTP header\n- socket.getfqdn() value\n\"\"\"\n", "func_signal": "def host(self) -> str:\n", "code": "host = self._message.headers.get(hdrs.HOST)\nif host is not None:\n    return host\nelse:\n    return socket.getfqdn()", "path": "aiohttp/aiohttp/web_request.py", "commit_date": "2020-12-17 00:00:00", "repo_name": "aio-libs/aiohttp", "stars": 14425, "license": "other", "language": "python", "size": 25666}
{"docstring": "# form data (x-www-form-urlencoded)\n", "func_signal": "def _gen_form_urlencoded(self) -> payload.BytesPayload:\n", "code": "data = []\nfor type_options, _, value in self._fields:\n    data.append((type_options[\"name\"], value))\n\ncharset = self._charset if self._charset is not None else \"utf-8\"\n\nif charset == \"utf-8\":\n    content_type = \"application/x-www-form-urlencoded\"\nelse:\n    content_type = \"application/x-www-form-urlencoded; \" \"charset=%s\" % charset\n\nreturn payload.BytesPayload(\n    urlencode(data, doseq=True, encoding=charset).encode(),\n    content_type=content_type,\n)", "path": "aiohttp/aiohttp/formdata.py", "commit_date": "2020-12-17 00:00:00", "repo_name": "aio-libs/aiohttp", "stars": 14425, "license": "other", "language": "python", "size": 25666}
{"docstring": "\"\"\"Result of route resolving.\"\"\"\n", "func_signal": "def match_info(self) -> \"UrlMappingMatchInfo\":\n", "code": "match_info = self._match_info\nassert match_info is not None\nreturn match_info", "path": "aiohttp/aiohttp/web_request.py", "commit_date": "2020-12-17 00:00:00", "repo_name": "aio-libs/aiohttp", "stars": 14425, "license": "other", "language": "python", "size": 25666}
{"docstring": "# We might receive a header like this if we're sitting behind a reverse\n# proxy that blindly appends a forwarded-element without checking\n# the syntax of existing field-values. We should be able to recover\n# the appended element anyway.\n", "func_signal": "def test_single_forwarded_header_injection1() -> None:\n", "code": "header = 'for=_injected;by=\", for=_real'\nreq = make_mocked_request(\"GET\", \"/\", headers=CIMultiDict({\"Forwarded\": header}))\nassert len(req.forwarded) == 2\nassert \"by\" not in req.forwarded[0]\nassert req.forwarded[1][\"for\"] == \"_real\"", "path": "aiohttp/tests/test_web_request.py", "commit_date": "2020-11-25 00:00:00", "repo_name": "aio-libs/aiohttp", "stars": 14425, "license": "other", "language": "python", "size": 25666}
{"docstring": "# Test case for https://github.com/aio-libs/aiohttp/issues/2084\n", "func_signal": "def test_cookie_not_expired_when_added_after_removal(self) -> None:\n", "code": "timestamps = [\n    533588.993,\n    533588.993,\n    533588.993,\n    533588.993,\n    533589.093,\n    533589.093,\n]\n\nloop = mock.Mock()\nloop.time.side_effect = itertools.chain(\n    timestamps, itertools.cycle([timestamps[-1]])\n)\n\nasync def make_jar():\n    return CookieJar(unsafe=True)\n\njar = self.loop.run_until_complete(make_jar())\n# Remove `foo` cookie.\njar.update_cookies(SimpleCookie('foo=\"\"; Max-Age=0'))\n# Set `foo` cookie to `bar`.\njar.update_cookies(SimpleCookie('foo=\"bar\"'))\n\n# Assert that there is a cookie.\nassert len(jar) == 1", "path": "aiohttp/tests/test_cookiejar.py", "commit_date": "2020-11-25 00:00:00", "repo_name": "aio-libs/aiohttp", "stars": 14425, "license": "other", "language": "python", "size": 25666}
{"docstring": "\"\"\"Remote IP of client initiated HTTP request.\n\nThe IP is resolved in this order:\n\n- overridden value by .clone(remote=new_remote) call.\n- peername of opened socket\n\"\"\"\n", "func_signal": "def remote(self) -> Optional[str]:\n", "code": "if isinstance(self._transport_peername, (list, tuple)):\n    return self._transport_peername[0]\nelse:\n    return self._transport_peername", "path": "aiohttp/aiohttp/web_request.py", "commit_date": "2020-12-17 00:00:00", "repo_name": "aio-libs/aiohttp", "stars": 14425, "license": "other", "language": "python", "size": 25666}
{"docstring": "\"\"\"Encode a list of fields using the multipart/form-data MIME format\"\"\"\n", "func_signal": "def _gen_form_data(self) -> multipart.MultipartWriter:\n", "code": "if self._is_processed:\n    raise RuntimeError(\"Form data has been processed already\")\nfor dispparams, headers, value in self._fields:\n    try:\n        if hdrs.CONTENT_TYPE in headers:\n            part = payload.get_payload(\n                value,\n                content_type=headers[hdrs.CONTENT_TYPE],\n                headers=headers,\n                encoding=self._charset,\n            )\n        else:\n            part = payload.get_payload(\n                value, headers=headers, encoding=self._charset\n            )\n    except Exception as exc:\n        raise TypeError(\n            \"Can not serialize value type: %r\\n \"\n            \"headers: %r\\n value: %r\" % (type(value), headers, value)\n        ) from exc\n\n    if dispparams:\n        part.set_content_disposition(\n            \"form-data\", quote_fields=self._quote_fields, **dispparams\n        )\n        # FIXME cgi.FieldStorage doesn't likes body parts with\n        # Content-Length which were sent via chunked transfer encoding\n        assert part.headers is not None\n        part.headers.popall(hdrs.CONTENT_LENGTH, None)\n\n    self._writer.append_payload(part)\n\nself._is_processed = True\nreturn self._writer", "path": "aiohttp/aiohttp/formdata.py", "commit_date": "2020-12-17 00:00:00", "repo_name": "aio-libs/aiohttp", "stars": 14425, "license": "other", "language": "python", "size": 25666}
{"docstring": "\"\"\"A string representing the scheme of the request.\n\nHostname is resolved in this order:\n\n- overridden value by .clone(scheme=new_scheme) call.\n- type of connection to peer: HTTPS if socket is SSL, HTTP otherwise.\n\n'http' or 'https'.\n\"\"\"\n", "func_signal": "def scheme(self) -> str:\n", "code": "if self._transport_sslcontext:\n    return \"https\"\nelse:\n    return \"http\"", "path": "aiohttp/aiohttp/web_request.py", "commit_date": "2020-12-17 00:00:00", "repo_name": "aio-libs/aiohttp", "stars": 14425, "license": "other", "language": "python", "size": 25666}
{"docstring": "# Set up signals through the event loop API.\n\n", "func_signal": "def init_signals(self) -> None:\n", "code": "self.loop.add_signal_handler(\n    signal.SIGQUIT, self.handle_quit, signal.SIGQUIT, None\n)\n\nself.loop.add_signal_handler(\n    signal.SIGTERM, self.handle_exit, signal.SIGTERM, None\n)\n\nself.loop.add_signal_handler(\n    signal.SIGINT, self.handle_quit, signal.SIGINT, None\n)\n\nself.loop.add_signal_handler(\n    signal.SIGWINCH, self.handle_winch, signal.SIGWINCH, None\n)\n\nself.loop.add_signal_handler(\n    signal.SIGUSR1, self.handle_usr1, signal.SIGUSR1, None\n)\n\nself.loop.add_signal_handler(\n    signal.SIGABRT, self.handle_abort, signal.SIGABRT, None\n)\n\n# Don't let SIGTERM and SIGUSR1 disturb active requests\n# by interrupting system calls\nsignal.siginterrupt(signal.SIGTERM, False)\nsignal.siginterrupt(signal.SIGUSR1, False)", "path": "aiohttp/aiohttp/worker.py", "commit_date": "2020-10-27 00:00:00", "repo_name": "aio-libs/aiohttp", "stars": 14425, "license": "other", "language": "python", "size": 25666}
{"docstring": "\"\"\"Return a port that is unused on the current host.\"\"\"\n", "func_signal": "def unused_port() -> int:\n", "code": "with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n    s.bind((\"127.0.0.1\", 0))\n    return s.getsockname()[1]", "path": "aiohttp/aiohttp/test_utils.py", "commit_date": "2020-11-27 00:00:00", "repo_name": "aio-libs/aiohttp", "stars": 14425, "license": "other", "language": "python", "size": 25666}
{"docstring": "# create new event_loop after fork\n", "func_signal": "def init_process(self) -> None:\n", "code": "self.loop = asyncio.new_event_loop()\nasyncio.set_event_loop(self.loop)\n\nsuper().init_process()", "path": "aiohttp/aiohttp/worker.py", "commit_date": "2020-10-27 00:00:00", "repo_name": "aio-libs/aiohttp", "stars": 14425, "license": "other", "language": "python", "size": 25666}
{"docstring": "# This is allowed by the grammar given in RFC 7239\n", "func_signal": "def test_single_forwarded_header_empty_params() -> None:\n", "code": "header = \";For=identifier;;PROTO=https;;;\"\nreq = make_mocked_request(\"GET\", \"/\", headers=CIMultiDict({\"Forwarded\": header}))\nassert req.forwarded[0][\"for\"] == \"identifier\"\nassert req.forwarded[0][\"proto\"] == \"https\"", "path": "aiohttp/tests/test_web_request.py", "commit_date": "2020-11-25 00:00:00", "repo_name": "aio-libs/aiohttp", "stars": 14425, "license": "other", "language": "python", "size": 25666}
{"docstring": "# for backward compatibility\n# web.Server instance\n", "func_signal": "def handler(self) -> Server:\n", "code": "runner = self.runner\nassert runner is not None\nassert runner.server is not None\nreturn runner.server", "path": "aiohttp/aiohttp/test_utils.py", "commit_date": "2020-11-27 00:00:00", "repo_name": "aio-libs/aiohttp", "stars": 14425, "license": "other", "language": "python", "size": 25666}
{"docstring": "# NB: //foo/bar is an absolute URL with foo netloc and /bar path\n", "func_signal": "def test_doubleslashes() -> None:\n", "code": "req = make_mocked_request(\"GET\", \"/bar//foo/\")\nassert \"/bar//foo/\" == req.path", "path": "aiohttp/tests/test_web_request.py", "commit_date": "2020-11-25 00:00:00", "repo_name": "aio-libs/aiohttp", "stars": 14425, "license": "other", "language": "python", "size": 25666}
{"docstring": "# fakes -- dns -> port dict\n", "func_signal": "def __init__(self, fakes):\n", "code": "self._fakes = fakes\nself._resolver = aiohttp.DefaultResolver()", "path": "aiohttp/tests/test_web_functional.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "aio-libs/aiohttp", "stars": 14425, "license": "other", "language": "python", "size": 25666}
{"docstring": "\"\"\"Stop accepting new pipelinig messages and close\nconnection when handlers done processing messages\"\"\"\n", "func_signal": "def close(self) -> None:\n", "code": "self._close = True\nif self._waiter:\n    self._waiter.cancel()", "path": "aiohttp/aiohttp/web_protocol.py", "commit_date": "2020-11-27 00:00:00", "repo_name": "aio-libs/aiohttp", "stars": 14425, "license": "other", "language": "python", "size": 25666}
{"docstring": "# just in case if we have transport close callbacks\n", "func_signal": "def tearDown(self) -> None:\n", "code": "self.loop.stop()\nself.loop.run_forever()\nself.loop.close()\ngc.collect()", "path": "aiohttp/tests/test_proxy.py", "commit_date": "2020-11-25 00:00:00", "repo_name": "aio-libs/aiohttp", "stars": 14425, "license": "other", "language": "python", "size": 25666}
{"docstring": "\"\"\"A tuple containing all parsed Forwarded header(s).\n\nMakes an effort to parse Forwarded headers as specified by RFC 7239:\n\n- It adds one (immutable) dictionary per Forwarded 'field-value', ie\n  per proxy. The element corresponds to the data in the Forwarded\n  field-value added by the first proxy encountered by the client. Each\n  subsequent item corresponds to those added by later proxies.\n- It checks that every value has valid syntax in general as specified\n  in section 4: either a 'token' or a 'quoted-string'.\n- It un-escapes found escape sequences.\n- It does NOT validate 'by' and 'for' contents as specified in section\n  6.\n- It does NOT validate 'host' contents (Host ABNF).\n- It does NOT validate 'proto' contents for valid URI scheme names.\n\nReturns a tuple containing one or more immutable dicts\n\"\"\"\n", "func_signal": "def forwarded(self) -> Tuple[Mapping[str, str], ...]:\n", "code": "elems = []\nfor field_value in self._message.headers.getall(hdrs.FORWARDED, ()):\n    length = len(field_value)\n    pos = 0\n    need_separator = False\n    elem = {}  # type: Dict[str, str]\n    elems.append(types.MappingProxyType(elem))\n    while 0 <= pos < length:\n        match = _FORWARDED_PAIR_RE.match(field_value, pos)\n        if match is not None:  # got a valid forwarded-pair\n            if need_separator:\n                # bad syntax here, skip to next comma\n                pos = field_value.find(\",\", pos)\n            else:\n                name, value, port = match.groups()\n                if value[0] == '\"':\n                    # quoted string: remove quotes and unescape\n                    value = _QUOTED_PAIR_REPLACE_RE.sub(r\"\\1\", value[1:-1])\n                if port:\n                    value += port\n                elem[name.lower()] = value\n                pos += len(match.group(0))\n                need_separator = True\n        elif field_value[pos] == \",\":  # next forwarded-element\n            need_separator = False\n            elem = {}\n            elems.append(types.MappingProxyType(elem))\n            pos += 1\n        elif field_value[pos] == \";\":  # next forwarded-pair\n            need_separator = False\n            pos += 1\n        elif field_value[pos] in \" \\t\":\n            # Allow whitespace even between forwarded-pairs, though\n            # RFC 7239 doesn't. This simplifies code and is in line\n            # with Postel's law.\n            pos += 1\n        else:\n            # bad syntax here, skip to next comma\n            pos = field_value.find(\",\", pos)\nreturn tuple(elems)", "path": "aiohttp/aiohttp/web_request.py", "commit_date": "2020-12-17 00:00:00", "repo_name": "aio-libs/aiohttp", "stars": 14425, "license": "other", "language": "python", "size": 25666}
{"docstring": "\"\"\"Decide what method to use for paging through text.\"\"\"\n", "func_signal": "def pager(generator, color=None):\n", "code": "stdout = _default_text_stdout()\nif not isatty(sys.stdin) or not isatty(stdout):\n    return _nullpager(stdout, generator, color)\npager_cmd = (os.environ.get(\"PAGER\", None) or \"\").strip()\nif pager_cmd:\n    if WIN:\n        return _tempfilepager(generator, pager_cmd, color)\n    return _pipepager(generator, pager_cmd, color)\nif os.environ.get(\"TERM\") in (\"dumb\", \"emacs\"):\n    return _nullpager(stdout, generator, color)\nif WIN or sys.platform.startswith(\"os2\"):\n    return _tempfilepager(generator, \"more <\", color)\nif hasattr(os, \"system\") and os.system(\"(less) 2>/dev/null\") == 0:\n    return _pipepager(generator, \"less\", color)\n\nimport tempfile\n\nfd, filename = tempfile.mkstemp()\nos.close(fd)\ntry:\n    if hasattr(os, \"system\") and os.system(f'more \"{filename}\"') == 0:\n        return _pipepager(generator, \"more\", color)\n    return _nullpager(stdout, generator, color)\nfinally:\n    os.unlink(filename)", "path": "click/src/click/_termui_impl.py", "commit_date": "2020-11-01 00:00:00", "repo_name": "pallets/click", "stars": 14851, "license": "bsd-3-clause", "language": "python", "size": 3152}
{"docstring": "\"\"\"This script prints some colors.  If colorama is installed this will\nalso work on Windows.  It will also automatically remove all ANSI\nstyles if data is piped into a file.\n\nGive it a try!\n\"\"\"\n", "func_signal": "def cli():\n", "code": "for color in all_colors:\n    click.echo(click.style(f\"I am colored {color}\", fg=color))\nfor color in all_colors:\n    click.echo(click.style(f\"I am colored {color} and bold\", fg=color, bold=True))\nfor color in all_colors:\n    click.echo(click.style(f\"I am reverse colored {color}\", fg=color, reverse=True))\n\nclick.echo(click.style(\"I am blinking\", blink=True))\nclick.echo(click.style(\"I am underlined\", underline=True))", "path": "click/examples/colors/colors.py", "commit_date": "2020-04-20 00:00:00", "repo_name": "pallets/click", "stars": 14851, "license": "bsd-3-clause", "language": "python", "size": 3152}
{"docstring": "\"\"\"This script works similar to the Unix `cat` command but it writes\ninto a specific file (which could be the standard output as denoted by\nthe ``-`` sign).\n\n\\b\nCopy stdin to stdout:\n    inout - -\n\n\\b\nCopy foo.txt and bar.txt to stdout:\n    inout foo.txt bar.txt -\n\n\\b\nWrite stdin into the file foo.txt\n    inout - foo.txt\n\"\"\"\n", "func_signal": "def cli(input, output):\n", "code": "for f in input:\n    while True:\n        chunk = f.read(1024)\n        if not chunk:\n            break\n        output.write(chunk)\n        output.flush()", "path": "click/examples/inout/inout.py", "commit_date": "2020-03-06 00:00:00", "repo_name": "pallets/click", "stars": 14851, "license": "bsd-3-clause", "language": "python", "size": 3152}
{"docstring": "\"\"\"Writes a paragraph into the buffer.\"\"\"\n", "func_signal": "def write_paragraph(self):\n", "code": "if self.buffer:\n    self.write(\"\\n\")", "path": "click/src/click/formatting.py", "commit_date": "2020-04-20 00:00:00", "repo_name": "pallets/click", "stars": 14851, "license": "bsd-3-clause", "language": "python", "size": 3152}
{"docstring": "\"\"\"Simply print unformatted text.  This is the ultimate fallback.\"\"\"\n", "func_signal": "def _nullpager(stream, generator, color):\n", "code": "for text in generator:\n    if not color:\n        text = strip_ansi(text)\n    stream.write(text)", "path": "click/src/click/_termui_impl.py", "commit_date": "2020-11-01 00:00:00", "repo_name": "pallets/click", "stars": 14851, "license": "bsd-3-clause", "language": "python", "size": 3152}
{"docstring": "\"\"\"Helpful context manager that writes a paragraph, a heading,\nand the indents.\n\n:param name: the section name that is written as heading.\n\"\"\"\n", "func_signal": "def section(self, name):\n", "code": "self.write_paragraph()\nself.write_heading(name)\nself.indent()\ntry:\n    yield\nfinally:\n    self.dedent()", "path": "click/src/click/formatting.py", "commit_date": "2020-04-20 00:00:00", "repo_name": "pallets/click", "stars": 14851, "license": "bsd-3-clause", "language": "python", "size": 3152}
{"docstring": "\"\"\"Deletes a repository.\n\nThis will throw away the current repository.\n\"\"\"\n", "func_signal": "def delete(repo):\n", "code": "click.echo(f\"Destroying repo {repo.home}\")\nclick.echo(\"Deleted!\")", "path": "click/examples/repo/repo.py", "commit_date": "2020-04-20 00:00:00", "repo_name": "pallets/click", "stars": 14851, "license": "bsd-3-clause", "language": "python", "size": 3152}
{"docstring": "\"\"\"A Group can be built with a list of commands.\"\"\"\n\n", "func_signal": "def test_group_from_list(runner):\n", "code": "@click.command()\ndef sub():\n    click.echo(\"sub\", nl=False)\n\ncli = click.Group(commands=[sub])\nresult = runner.invoke(cli, [\"sub\"])\nassert result.output == \"sub\"", "path": "click/tests/test_basic.py", "commit_date": "2020-10-30 00:00:00", "repo_name": "pallets/click", "stars": 14851, "license": "bsd-3-clause", "language": "python", "size": 3152}
{"docstring": "# The function `getch` will return a bytes object corresponding to\n# the pressed character. Since Windows 10 build 1803, it will also\n# return \\x00 when called a second time after pressing a regular key.\n#\n# `getwch` does not share this probably-bugged behavior. Moreover, it\n# returns a Unicode object by default, which is what we want.\n#\n# Either of these functions will return \\x00 or \\xe0 to indicate\n# a special key, and you need to call the same function again to get\n# the \"rest\" of the code. The fun part is that \\u00e0 is\n# \"latin small letter a with grave\", so if you type that on a French\n# keyboard, you _also_ get a \\xe0.\n# E.g., consider the Up arrow. This returns \\xe0 and then \\x48. The\n# resulting Unicode string reads as \"a with grave\" + \"capital H\".\n# This is indistinguishable from when the user actually types\n# \"a with grave\" and then \"capital H\".\n#\n# When \\xe0 is returned, we assume it's part of a special-key sequence\n# and call `getwch` again, but that means that when the user types\n# the \\u00e0 character, `getchar` doesn't return until a second\n# character is typed.\n# The alternative is returning immediately, but that would mess up\n# cross-platform handling of arrow keys and others that start with\n# \\xe0. Another option is using `getch`, but then we can't reliably\n# read non-ASCII characters, because return values of `getch` are\n# limited to the current 8-bit codepage.\n#\n# Anyway, Click doesn't claim to do this Right(tm), and using `getwch`\n# is doing the right thing in more situations than with `getch`.\n", "func_signal": "def getchar(echo):\n", "code": "if echo:\n    func = msvcrt.getwche\nelse:\n    func = msvcrt.getwch\n\nrv = func()\nif rv in (\"\\x00\", \"\\xe0\"):\n    # \\x00 and \\xe0 are control characters that indicate special key,\n    # see above.\n    rv += func()\n_translate_ch_to_exc(rv)\nreturn rv", "path": "click/src/click/_termui_impl.py", "commit_date": "2020-11-01 00:00:00", "repo_name": "pallets/click", "stars": 14851, "license": "bsd-3-clause", "language": "python", "size": 3152}
{"docstring": "\"\"\"Sets the user credentials.\n\nThis will override the current user config.\n\"\"\"\n", "func_signal": "def setuser(repo, username, email, password):\n", "code": "repo.set_config(\"username\", username)\nrepo.set_config(\"email\", email)\nrepo.set_config(\"password\", \"*\" * len(password))\nclick.echo(\"Changed credentials.\")", "path": "click/examples/repo/repo.py", "commit_date": "2020-04-20 00:00:00", "repo_name": "pallets/click", "stars": 14851, "license": "bsd-3-clause", "language": "python", "size": 3152}
{"docstring": "\"\"\"Return a generator which yields the items added to the bar\nduring construction, and updates the progress bar *after* the\nyielded block returns.\n\"\"\"\n# WARNING: the iterator interface for `ProgressBar` relies on\n# this and only works because this is a simple generator which\n# doesn't create or manage additional state. If this function\n# changes, the impact should be evaluated both against\n# `iter(bar)` and `next(bar)`. `next()` in particular may call\n# `self.generator()` repeatedly, and this must remain safe in\n# order for that interface to work.\n", "func_signal": "def generator(self):\n", "code": "if not self.entered:\n    raise RuntimeError(\"You need to use progress bars in a with block.\")\n\nif self.is_hidden:\n    yield from self.iter\nelse:\n    for rv in self.iter:\n        self.current_item = rv\n        yield rv\n        self.update(1)\n    self.finish()\n    self.render_progress()", "path": "click/src/click/_termui_impl.py", "commit_date": "2020-11-01 00:00:00", "repo_name": "pallets/click", "stars": 14851, "license": "bsd-3-clause", "language": "python", "size": 3152}
{"docstring": "\"\"\"A context manager that increases the indentation.\"\"\"\n", "func_signal": "def indentation(self):\n", "code": "self.indent()\ntry:\n    yield\nfinally:\n    self.dedent()", "path": "click/src/click/formatting.py", "commit_date": "2020-04-20 00:00:00", "repo_name": "pallets/click", "stars": 14851, "license": "bsd-3-clause", "language": "python", "size": 3152}
{"docstring": "\"\"\"Clones a repository.\n\nThis will clone the repository at SRC into the folder DEST.  If DEST\nis not provided this will automatically use the last path component\nof SRC and create that folder.\n\"\"\"\n", "func_signal": "def clone(repo, src, dest, shallow, rev):\n", "code": "if dest is None:\n    dest = posixpath.split(src)[-1] or \".\"\nclick.echo(f\"Cloning repo {src} to {os.path.basename(dest)}\")\nrepo.home = dest\nif shallow:\n    click.echo(\"Making shallow checkout\")\nclick.echo(f\"Checking out revision {rev}\")", "path": "click/examples/repo/repo.py", "commit_date": "2020-04-20 00:00:00", "repo_name": "pallets/click", "stars": 14851, "license": "bsd-3-clause", "language": "python", "size": 3152}
{"docstring": "\"\"\"Update the progress bar by advancing a specified number of\nsteps, and optionally set the ``current_item`` for this new\nposition.\n\n:param n_steps: Number of steps to advance.\n:param current_item: Optional item to set as ``current_item``\n    for the updated position.\n\n.. versionchanged:: 8.0\n    Added the ``current_item`` optional parameter.\n\n.. versionchanged:: 8.0\n    Only render when the number of steps meets the\n    ``update_min_steps`` threshold.\n\"\"\"\n", "func_signal": "def update(self, n_steps, current_item=None):\n", "code": "self._completed_intervals += n_steps\n\nif self._completed_intervals >= self.update_min_steps:\n    self.make_step(self._completed_intervals)\n\n    if current_item is not None:\n        self.current_item = current_item\n\n    self.render_progress()\n    self._completed_intervals = 0", "path": "click/src/click/_termui_impl.py", "commit_date": "2020-11-01 00:00:00", "repo_name": "pallets/click", "stars": 14851, "license": "bsd-3-clause", "language": "python", "size": 3152}
{"docstring": "\"\"\"Given a list of option strings this joins them in the most appropriate\nway and returns them in the form ``(formatted_string,\nany_prefix_is_slash)`` where the second item in the tuple is a flag that\nindicates if any of the option prefixes was a slash.\n\"\"\"\n", "func_signal": "def join_options(options):\n", "code": "rv = []\nany_prefix_is_slash = False\nfor opt in options:\n    prefix = split_opt(opt)[0]\n    if prefix == \"/\":\n        any_prefix_is_slash = True\n    rv.append((len(prefix), opt))\n\nrv.sort(key=lambda x: x[0])\n\nrv = \", \".join(x[1] for x in rv)\nreturn rv, any_prefix_is_slash", "path": "click/src/click/formatting.py", "commit_date": "2020-04-20 00:00:00", "repo_name": "pallets/click", "stars": 14851, "license": "bsd-3-clause", "language": "python", "size": 3152}
{"docstring": "\"\"\"Page through text by invoking a program on a temporary file.\"\"\"\n", "func_signal": "def _tempfilepager(generator, cmd, color):\n", "code": "import tempfile\n\nfilename = tempfile.mktemp()\n# TODO: This never terminates if the passed generator never terminates.\ntext = \"\".join(generator)\nif not color:\n    text = strip_ansi(text)\nencoding = get_best_encoding(sys.stdout)\nwith open_stream(filename, \"wb\")[0] as f:\n    f.write(text.encode(encoding))\ntry:\n    os.system(f'{cmd} \"{filename}\"')\nfinally:\n    os.unlink(filename)", "path": "click/src/click/_termui_impl.py", "commit_date": "2020-11-01 00:00:00", "repo_name": "pallets/click", "stars": 14851, "license": "bsd-3-clause", "language": "python", "size": 3152}
{"docstring": "\"\"\"Copies one or multiple files to a new location.  This copies all\nfiles from SRC to DST.\n\"\"\"\n", "func_signal": "def copy(repo, src, dst, force):\n", "code": "for fn in src:\n    click.echo(f\"Copy from {fn} -> {dst}\")", "path": "click/examples/repo/repo.py", "commit_date": "2020-04-20 00:00:00", "repo_name": "pallets/click", "stars": 14851, "license": "bsd-3-clause", "language": "python", "size": 3152}
{"docstring": "\"\"\"Page through text by feeding it to another program.  Invoking a\npager through this might support colors.\n\"\"\"\n", "func_signal": "def _pipepager(generator, cmd, color):\n", "code": "import subprocess\n\nenv = dict(os.environ)\n\n# If we're piping to less we might support colors under the\n# condition that\ncmd_detail = cmd.rsplit(\"/\", 1)[-1].split()\nif color is None and cmd_detail[0] == \"less\":\n    less_flags = f\"{os.environ.get('LESS', '')}{' '.join(cmd_detail[1:])}\"\n    if not less_flags:\n        env[\"LESS\"] = \"-R\"\n        color = True\n    elif \"r\" in less_flags or \"R\" in less_flags:\n        color = True\n\nc = subprocess.Popen(cmd, shell=True, stdin=subprocess.PIPE, env=env)\nencoding = get_best_encoding(c.stdin)\ntry:\n    for text in generator:\n        if not color:\n            text = strip_ansi(text)\n\n        c.stdin.write(text.encode(encoding, \"replace\"))\nexcept (OSError, KeyboardInterrupt):\n    pass\nelse:\n    c.stdin.close()\n\n# Less doesn't respect ^C, but catches it for its own UI purposes (aborting\n# search or other commands inside less).\n#\n# That means when the user hits ^C, the parent process (click) terminates,\n# but less is still alive, paging the output and messing up the terminal.\n#\n# If the user wants to make the pager exit on ^C, they should set\n# `LESS='-K'`. It's not our decision to make.\nwhile True:\n    try:\n        c.wait()\n    except KeyboardInterrupt:\n        pass\n    else:\n        break", "path": "click/src/click/_termui_impl.py", "commit_date": "2020-11-01 00:00:00", "repo_name": "pallets/click", "stars": 14851, "license": "bsd-3-clause", "language": "python", "size": 3152}
{"docstring": "\"\"\"A Group can be built with a dict of commands.\"\"\"\n\n", "func_signal": "def test_group_commands_dict(runner):\n", "code": "@click.command()\ndef sub():\n    click.echo(\"sub\", nl=False)\n\ncli = click.Group(commands={\"other\": sub})\nresult = runner.invoke(cli, [\"other\"])\nassert result.output == \"sub\"", "path": "click/tests/test_basic.py", "commit_date": "2020-10-30 00:00:00", "repo_name": "pallets/click", "stars": 14851, "license": "bsd-3-clause", "language": "python", "size": 3152}
{"docstring": "\"\"\"Commits outstanding changes.\n\nCommit changes to the given files into the repository.  You will need to\n\"repo push\" to push up your changes to other repositories.\n\nIf a list of files is omitted, all changes reported by \"repo status\"\nwill be committed.\n\"\"\"\n", "func_signal": "def commit(repo, files, message):\n", "code": "if not message:\n    marker = \"# Files to be committed:\"\n    hint = [\"\", \"\", marker, \"#\"]\n    for file in files:\n        hint.append(f\"#   U {file}\")\n    message = click.edit(\"\\n\".join(hint))\n    if message is None:\n        click.echo(\"Aborted!\")\n        return\n    msg = message.split(marker)[0].rstrip()\n    if not msg:\n        click.echo(\"Aborted! Empty commit message\")\n        return\nelse:\n    msg = \"\\n\".join(message)\nclick.echo(f\"Files to be committed: {files}\")\nclick.echo(f\"Commit message:\\n{msg}\")", "path": "click/examples/repo/repo.py", "commit_date": "2020-04-20 00:00:00", "repo_name": "pallets/click", "stars": 14851, "license": "bsd-3-clause", "language": "python", "size": 3152}
{"docstring": "\"\"\"\nRecursively set the chmod for files to 0600 and 0700 for folders.\n\nIt's ok unless we need something more specific.\n\nArgs:\n    target (str): Root file or folder\n\"\"\"\n", "func_signal": "def chmod(target):\n", "code": "assert isinstance(target, str)\nassert os.path.exists(target)\n\nfile_mode = stat.S_IRUSR | stat.S_IWUSR\nfolder_mode = stat.S_IRUSR | stat.S_IWUSR | stat.S_IXUSR\n\n# Remove the immutable attribute recursively if there is one\nremove_immutable_attribute(target)\n\nif os.path.isfile(target):\n    os.chmod(target, file_mode)\n\nelif os.path.isdir(target):\n    # chmod the root item\n    os.chmod(target, folder_mode)\n\n    # chmod recursively in the folder it it's one\n    for root, dirs, files in os.walk(target):\n        for cur_dir in dirs:\n            os.chmod(os.path.join(root, cur_dir), folder_mode)\n        for cur_file in files:\n            os.chmod(os.path.join(root, cur_file), file_mode)\n\nelse:\n    raise ValueError(\"Unsupported file type: {}\".format(target))", "path": "mackup/mackup/utils.py", "commit_date": "2020-03-04 00:00:00", "repo_name": "lra/mackup", "stars": 14064, "license": "gpl-3.0", "language": "python", "size": 2374}
{"docstring": "\"\"\"\nRemove the immutable attribute of the given path.\n\nRemove the immutable attribute of the file or folder located on the given\npath. Also remove the immutable attribute of any file and folder below the\ngiven one, recursively.\n\nArgs:\n    path (str): Path to the file or folder to remove the immutable\n                attribute for, recursively.\n\"\"\"\n# Some files have ACLs, let's remove them recursively\n", "func_signal": "def remove_immutable_attribute(path):\n", "code": "if (platform.system() == constants.PLATFORM_DARWIN) and os.path.isfile(\n    \"/usr/bin/chflags\"\n):\n    subprocess.call([\"/usr/bin/chflags\", \"-R\", \"nouchg\", path])\nelif platform.system() == constants.PLATFORM_LINUX and os.path.isfile(\n    \"/usr/bin/chattr\"\n):\n    subprocess.call([\"/usr/bin/chattr\", \"-R\", \"-f\", \"-i\", path])", "path": "mackup/mackup/utils.py", "commit_date": "2020-03-04 00:00:00", "repo_name": "lra/mackup", "stars": 14064, "license": "gpl-3.0", "language": "python", "size": 2374}
{"docstring": "\"\"\"\nTry to locate the Copy folder.\n\nReturns:\n    (str) Full path to the current Copy folder\n\"\"\"\n", "func_signal": "def get_copy_folder_location():\n", "code": "copy_settings_path = \"Library/Application Support/Copy Agent/config.db\"\ncopy_home = None\n\ncopy_settings = os.path.join(os.environ[\"HOME\"], copy_settings_path)\n\nif os.path.isfile(copy_settings):\n    database = sqlite3.connect(copy_settings)\n    if database:\n        cur = database.cursor()\n        query = \"SELECT value \" \"FROM config2 \" \"WHERE option = 'csmRootPath';\"\n        cur.execute(query)\n        data = cur.fetchone()\n        copy_home = str(data[0])\n        cur.close()\n\nif not copy_home:\n    error(\"Unable to find your Copy install =(\")\n\nreturn copy_home", "path": "mackup/mackup/utils.py", "commit_date": "2020-03-04 00:00:00", "repo_name": "lra/mackup", "stars": 14064, "license": "gpl-3.0", "language": "python", "size": 2374}
{"docstring": "\"\"\"\nCheck if the given path can be synced locally.\n\nCheck if it makes sense to sync the file at the given path on the current\nplatform.\nFor now we don't sync any file in the ~/Library folder on GNU/Linux.\nThere might be other exceptions in the future.\n\nArgs:\n    (str): Path to the file or folder to check. If relative, prepend it\n           with the home folder.\n           'abc' becomes '~/abc'\n           '/def' stays '/def'\n\nReturns:\n    (bool): True if given file can be synced\n\"\"\"\n", "func_signal": "def can_file_be_synced_on_current_platform(path):\n", "code": "can_be_synced = True\n\n# If the given path is relative, prepend home\nfullpath = os.path.join(os.environ[\"HOME\"], path)\n\n# Compute the ~/Library path on OS X\n# End it with a slash because we are looking for this specific folder and\n# not any file/folder named LibrarySomething\nlibrary_path = os.path.join(os.environ[\"HOME\"], \"Library/\")\n\nif platform.system() == constants.PLATFORM_LINUX:\n    if fullpath.startswith(library_path):\n        can_be_synced = False\n\nreturn can_be_synced", "path": "mackup/mackup/utils.py", "commit_date": "2020-03-04 00:00:00", "repo_name": "lra/mackup", "stars": 14064, "license": "gpl-3.0", "language": "python", "size": 2374}
{"docstring": "\"\"\"\nTry to locate the Dropbox folder.\n\nReturns:\n    (str) Full path to the current Dropbox folder\n\"\"\"\n", "func_signal": "def get_dropbox_folder_location():\n", "code": "host_db_path = os.path.join(os.environ[\"HOME\"], \".dropbox/host.db\")\ntry:\n    with open(host_db_path, \"r\") as f_hostdb:\n        data = f_hostdb.read().split()\nexcept IOError:\n    error(\"Unable to find your Dropbox install =(\")\ndropbox_home = base64.b64decode(data[1]).decode()\n\nreturn dropbox_home", "path": "mackup/mackup/utils.py", "commit_date": "2020-03-04 00:00:00", "repo_name": "lra/mackup", "stars": 14064, "license": "gpl-3.0", "language": "python", "size": 2374}
{"docstring": "\"\"\"Mackup Constructor.\"\"\"\n", "func_signal": "def __init__(self):\n", "code": "self._config = config.Config()\n\nself.mackup_folder = self._config.fullpath\nself.temp_folder = tempfile.mkdtemp(prefix=\"mackup_tmp_\")", "path": "mackup/mackup/mackup.py", "commit_date": "2020-03-04 00:00:00", "repo_name": "lra/mackup", "stars": 14064, "license": "gpl-3.0", "language": "python", "size": 2374}
{"docstring": "\"\"\"If the Mackup home folder does not exist, create it.\"\"\"\n", "func_signal": "def create_mackup_home(self):\n", "code": "if not os.path.isdir(self.mackup_folder):\n    if utils.confirm(\n        \"Mackup needs a directory to store your\"\n        \" configuration files\\n\"\n        \"Do you want to create it now? <{}>\".format(self.mackup_folder)\n    ):\n        os.makedirs(self.mackup_folder)\n    else:\n        utils.error(\"Mackup can't do anything without a home =(\")", "path": "mackup/mackup/mackup.py", "commit_date": "2020-03-04 00:00:00", "repo_name": "lra/mackup", "stars": 14064, "license": "gpl-3.0", "language": "python", "size": 2374}
{"docstring": "\"\"\"Check if the current env can be used to back up files.\"\"\"\n", "func_signal": "def check_for_usable_backup_env(self):\n", "code": "self.check_for_usable_environment()\nself.create_mackup_home()", "path": "mackup/mackup/mackup.py", "commit_date": "2020-03-04 00:00:00", "repo_name": "lra/mackup", "stars": 14064, "license": "gpl-3.0", "language": "python", "size": 2374}
{"docstring": "\"\"\"\nTry to locate the iCloud Drive folder.\n\nReturns:\n    (str) Full path to the iCloud Drive folder.\n\"\"\"\n", "func_signal": "def get_icloud_folder_location():\n", "code": "yosemite_icloud_path = \"~/Library/Mobile Documents/com~apple~CloudDocs/\"\n\nicloud_home = os.path.expanduser(yosemite_icloud_path)\n\nif not os.path.isdir(icloud_home):\n    error(\"Unable to find your iCloud Drive =(\")\n\nreturn str(icloud_home)", "path": "mackup/mackup/utils.py", "commit_date": "2020-03-04 00:00:00", "repo_name": "lra/mackup", "stars": 14064, "license": "gpl-3.0", "language": "python", "size": 2374}
{"docstring": "\"\"\"\nCreate a link to a target file or a folder.\n\nFor simplicity sake, both target and link_to must be absolute path and must\ninclude the filename of the file or folder.\nAlso do not include any trailing slash.\n\ne.g. link('/path/to/file', '/path/to/link')\n\nBut not: link('/path/to/file', 'path/to/')\nor link('/path/to/folder/', '/path/to/link')\n\nArgs:\n    target (str): file or folder the link will point to\n    link_to (str): Link to create\n\"\"\"\n", "func_signal": "def link(target, link_to):\n", "code": "assert isinstance(target, str)\nassert os.path.exists(target)\nassert isinstance(link_to, str)\n\n# Create the path to the link if it does not exists\nabs_path = os.path.dirname(os.path.abspath(link_to))\nif not os.path.isdir(abs_path):\n    os.makedirs(abs_path)\n\n# Make sure the file or folder recursively has the good mode\nchmod(target)\n\n# Create the link to target\nos.symlink(target, link_to)", "path": "mackup/mackup/utils.py", "commit_date": "2020-03-04 00:00:00", "repo_name": "lra/mackup", "stars": 14064, "license": "gpl-3.0", "language": "python", "size": 2374}
{"docstring": "\"\"\"Check if the current env is usable and has everything's required.\"\"\"\n# Allow only explicit superuser usage\n", "func_signal": "def check_for_usable_environment(self):\n", "code": "if os.geteuid() == 0 and not utils.CAN_RUN_AS_ROOT:\n    utils.error(\n        \"Running Mackup as superuser can be dangerous.\"\n        \" Don't do it unless you know what you're doing!\"\n        \" Run mackup --help for guidance.\"\n    )\n\n# Do we have a folder to put the Mackup folder ?\nif not os.path.isdir(self._config.path):\n    utils.error(\n        \"Unable to find the storage folder: {}\".format(self._config.path)\n    )\n\n# Is Sublime Text running ?\n# if is_process_running('Sublime Text'):\n#    error(\"Sublime Text is running. It is known to cause problems\"\n#          \" when Sublime Text is running while I backup or restore\"\n#          \" its configuration files. Please close Sublime Text and\"\n#          \" run me again.\")", "path": "mackup/mackup/mackup.py", "commit_date": "2020-03-04 00:00:00", "repo_name": "lra/mackup", "stars": 14064, "license": "gpl-3.0", "language": "python", "size": 2374}
{"docstring": "\"\"\"\nCheck if a process with the given name is running.\n\nArgs:\n    (str): Process name, e.g. \"Sublime Text\"\n\nReturns:\n    (bool): True if the process is running\n\"\"\"\n", "func_signal": "def is_process_running(process_name):\n", "code": "is_running = False\n\n# On systems with pgrep, check if the given process is running\nif os.path.isfile(\"/usr/bin/pgrep\"):\n    dev_null = open(os.devnull, \"wb\")\n    returncode = subprocess.call([\"/usr/bin/pgrep\", process_name], stdout=dev_null)\n    is_running = bool(returncode == 0)\n\nreturn is_running", "path": "mackup/mackup/utils.py", "commit_date": "2020-03-04 00:00:00", "repo_name": "lra/mackup", "stars": 14064, "license": "gpl-3.0", "language": "python", "size": 2374}
{"docstring": "\"\"\"\nThrow an error with the given message and immediately quit.\n\nArgs:\n    message(str): The message to display.\n\"\"\"\n", "func_signal": "def error(message):\n", "code": "fail = \"\\033[91m\"\nend = \"\\033[0m\"\nsys.exit(fail + \"Error: {}\".format(message) + end)", "path": "mackup/mackup/utils.py", "commit_date": "2020-03-04 00:00:00", "repo_name": "lra/mackup", "stars": 14064, "license": "gpl-3.0", "language": "python", "size": 2374}
{"docstring": "\"\"\"\nTry to locate the Google Drive folder.\n\nReturns:\n    (str) Full path to the current Google Drive folder\n\"\"\"\n", "func_signal": "def get_google_drive_folder_location():\n", "code": "gdrive_db_path = \"Library/Application Support/Google/Drive/sync_config.db\"\nyosemite_gdrive_db_path = (\n    \"Library/Application Support/Google/Drive/\" \"user_default/sync_config.db\"\n)\nyosemite_gdrive_db = os.path.join(os.environ[\"HOME\"], yosemite_gdrive_db_path)\nif os.path.isfile(yosemite_gdrive_db):\n    gdrive_db_path = yosemite_gdrive_db\n\ngoogledrive_home = None\n\ngdrive_db = os.path.join(os.environ[\"HOME\"], gdrive_db_path)\nif os.path.isfile(gdrive_db):\n    con = sqlite3.connect(gdrive_db)\n    if con:\n        cur = con.cursor()\n        query = (\n            \"SELECT data_value \"\n            \"FROM data \"\n            \"WHERE entry_key = 'local_sync_root_path';\"\n        )\n        cur.execute(query)\n        data = cur.fetchone()\n        googledrive_home = str(data[0])\n        con.close()\n\nif not googledrive_home:\n    error(\"Unable to find your Google Drive install =(\")\n\nreturn googledrive_home", "path": "mackup/mackup/utils.py", "commit_date": "2020-03-04 00:00:00", "repo_name": "lra/mackup", "stars": 14064, "license": "gpl-3.0", "language": "python", "size": 2374}
{"docstring": "\"\"\"\nCopy a file or a folder (recursively) from src to dst.\n\nFor simplicity sake, both src and dst must be absolute path and must\ninclude the filename of the file or folder.\nAlso do not include any trailing slash.\n\ne.g. copy('/path/to/src_file', '/path/to/dst_file')\nor copy('/path/to/src_folder', '/path/to/dst_folder')\n\nBut not: copy('/path/to/src_file', 'path/to/')\nor copy('/path/to/src_folder/', '/path/to/dst_folder')\n\nArgs:\n    src (str): Source file or folder\n    dst (str): Destination file or folder\n\"\"\"\n", "func_signal": "def copy(src, dst):\n", "code": "assert isinstance(src, str)\nassert os.path.exists(src)\nassert isinstance(dst, str)\n\n# Create the path to the dst file if it does not exists\nabs_path = os.path.dirname(os.path.abspath(dst))\nif not os.path.isdir(abs_path):\n    os.makedirs(abs_path)\n\n# We need to copy a single file\nif os.path.isfile(src):\n    # Copy the src file to dst\n    shutil.copy(src, dst)\n\n# We need to copy a whole folder\nelif os.path.isdir(src):\n    shutil.copytree(src, dst)\n\n# What the heck is this ?\nelse:\n    raise ValueError(\"Unsupported file: {}\".format(src))\n\n# Set the good mode to the file or folder recursively\nchmod(dst)", "path": "mackup/mackup/utils.py", "commit_date": "2020-03-04 00:00:00", "repo_name": "lra/mackup", "stars": 14064, "license": "gpl-3.0", "language": "python", "size": 2374}
{"docstring": "\"\"\"\nRemove the ACL of the file or folder located on the given path.\n\nAlso remove the ACL of any file and folder below the given one,\nrecursively.\n\nArgs:\n    path (str): Path to the file or folder to remove the ACL for,\n                recursively.\n\"\"\"\n# Some files have ACLs, let's remove them recursively\n", "func_signal": "def remove_acl(path):\n", "code": "if platform.system() == constants.PLATFORM_DARWIN and os.path.isfile(\"/bin/chmod\"):\n    subprocess.call([\"/bin/chmod\", \"-R\", \"-N\", path])\nelif (platform.system() == constants.PLATFORM_LINUX) and os.path.isfile(\n    \"/bin/setfacl\"\n):\n    subprocess.call([\"/bin/setfacl\", \"-R\", \"-b\", path])", "path": "mackup/mackup/utils.py", "commit_date": "2020-03-04 00:00:00", "repo_name": "lra/mackup", "stars": 14064, "license": "gpl-3.0", "language": "python", "size": 2374}
{"docstring": "\"\"\"\nAsk the user if he really want something to happen.\n\nArgs:\n    question(str): What can happen\n\nReturns:\n    (boolean): Confirmed or not\n\"\"\"\n", "func_signal": "def confirm(question):\n", "code": "if FORCE_YES:\n    return True\n\nwhile True:\n    answer = input(question + \" <Yes|No>\").lower()\n\n    if answer == \"yes\" or answer == \"y\":\n        confirmed = True\n        break\n    if answer == \"no\" or answer == \"n\":\n        confirmed = False\n        break\n\nreturn confirmed", "path": "mackup/mackup/utils.py", "commit_date": "2020-03-04 00:00:00", "repo_name": "lra/mackup", "stars": 14064, "license": "gpl-3.0", "language": "python", "size": 2374}
{"docstring": "\"\"\"\nDelete the given file, directory or link.\n\nIt Should support undelete later on.\n\nArgs:\n    filepath (str): Absolute full path to a file. e.g. /path/to/file\n\"\"\"\n# Some files have ACLs, let's remove them recursively\n", "func_signal": "def delete(filepath):\n", "code": "remove_acl(filepath)\n\n# Some files have immutable attributes, let's remove them recursively\nremove_immutable_attribute(filepath)\n\n# Finally remove the files and folders\nif os.path.isfile(filepath) or os.path.islink(filepath):\n    os.remove(filepath)\nelif os.path.isdir(filepath):\n    shutil.rmtree(filepath)", "path": "mackup/mackup/utils.py", "commit_date": "2020-03-04 00:00:00", "repo_name": "lra/mackup", "stars": 14064, "license": "gpl-3.0", "language": "python", "size": 2374}
{"docstring": "\"\"\"\nGet the list of applications that should be backed up by Mackup.\n\nIt's the list of allowed apps minus the list of ignored apps.\n\nReturns:\n    (set) List of application names to back up\n\"\"\"\n# Instantiate the app db\n", "func_signal": "def get_apps_to_backup(self):\n", "code": "app_db = appsdb.ApplicationsDatabase()\n\n# If a list of apps to sync is specify, we only allow those\n# Or we allow every supported app by default\napps_to_backup = self._config.apps_to_sync or app_db.get_app_names()\n\n# Remove the specified apps to ignore\nfor app_name in self._config.apps_to_ignore:\n    apps_to_backup.discard(app_name)\n\nreturn apps_to_backup", "path": "mackup/mackup/mackup.py", "commit_date": "2020-03-04 00:00:00", "repo_name": "lra/mackup", "stars": 14064, "license": "gpl-3.0", "language": "python", "size": 2374}
{"docstring": "\"\"\"Check if the current env can be used to restore files.\"\"\"\n", "func_signal": "def check_for_usable_restore_env(self):\n", "code": "self.check_for_usable_environment()\n\nif not os.path.isdir(self.mackup_folder):\n    utils.error(\n        \"Unable to find the Mackup folder: {}\\n\"\n        \"You might want to back up some files or get your\"\n        \" storage directory synced first.\".format(self.mackup_folder)\n    )", "path": "mackup/mackup/mackup.py", "commit_date": "2020-03-04 00:00:00", "repo_name": "lra/mackup", "stars": 14064, "license": "gpl-3.0", "language": "python", "size": 2374}
{"docstring": "\"\"\"\nPerforms a single optimization step.\nArguments:\n    closure (callable, optional) -- a closure that reevaluates the model and returns the loss (default: None)\n\"\"\"\n\n", "func_signal": "def step(self, closure=None):\n", "code": "loss = None\nif closure is not None:\n    loss = closure()\n\nself.zero_hessian()\nself.set_hessian()\n\nfor group in self.param_groups:\n    for p in group['params']:\n        if p.grad is None or p.hess is None:\n            continue\n\n        if self.avg_conv_kernel and p.dim() == 4:\n            p.hess = torch.abs(p.hess).mean(dim=[2, 3], keepdim=True).expand_as(p.hess).clone()\n\n        # Perform correct stepweight decay as in AdamW\n        p.mul_(1 - group['lr'] * group['weight_decay'])\n\n        state = self.state[p]\n\n        # State initialization\n        if len(state) == 1:\n            state['step'] = 0\n            # Exponential moving average of gradient values\n            state['exp_avg'] = torch.zeros_like(p)\n            # Exponential moving average of Hessian diagonal square values\n            state['exp_hessian_diag_sq'] = torch.zeros_like(p)\n\n        exp_avg, exp_hessian_diag_sq = state['exp_avg'], state['exp_hessian_diag_sq']\n        beta1, beta2 = group['betas']\n        state['step'] += 1\n\n        # Decay the first and second moment running average coefficient\n        exp_avg.mul_(beta1).add_(p.grad, alpha=1 - beta1)\n        exp_hessian_diag_sq.mul_(beta2).addcmul_(p.hess, p.hess, value=1 - beta2)\n\n        bias_correction1 = 1 - beta1 ** state['step']\n        bias_correction2 = 1 - beta2 ** state['step']\n\n        k = group['hessian_power']\n        denom = (exp_hessian_diag_sq / bias_correction2).pow_(k / 2).add_(group['eps'])\n\n        # make update\n        step_size = group['lr'] / bias_correction1\n        p.addcdiv_(exp_avg, denom, value=-step_size)\n\nreturn loss", "path": "pytorch-image-models/timm/optim/adahessian.py", "commit_date": "2020-10-10 00:00:00", "repo_name": "huggingface/pytorch-image-models", "stars": 29010, "license": "apache-2.0", "language": "python", "size": 25509}
{"docstring": "\"\"\"\nZeros out the accumalated hessian traces.\n\"\"\"\n\n", "func_signal": "def zero_hessian(self):\n", "code": "for p in self.get_params():\n    if not isinstance(p.hess, float) and self.state[p][\"hessian step\"] % self.update_each == 0:\n        p.hess.zero_()", "path": "pytorch-image-models/timm/optim/adahessian.py", "commit_date": "2020-10-10 00:00:00", "repo_name": "huggingface/pytorch-image-models", "stars": 29010, "license": "apache-2.0", "language": "python", "size": 25509}
{"docstring": "\"\"\" Modified Aligned Xception-41\n\"\"\"\n", "func_signal": "def xception41(pretrained=False, **kwargs):\n", "code": "block_cfg = [\n    # entry flow\n    dict(in_chs=64, out_chs=128, stride=2),\n    dict(in_chs=128, out_chs=256, stride=2),\n    dict(in_chs=256, out_chs=728, stride=2),\n    # middle flow\n    *([dict(in_chs=728, out_chs=728, stride=1)] * 8),\n    # exit flow\n    dict(in_chs=728, out_chs=(728, 1024, 1024), stride=2),\n    dict(in_chs=1024, out_chs=(1536, 1536, 2048), stride=1, no_skip=True, start_with_relu=False),\n]\nmodel_args = dict(block_cfg=block_cfg, norm_kwargs=dict(eps=.001, momentum=.1), **kwargs)\nreturn _xception('xception41', pretrained=pretrained, **model_args)", "path": "pytorch-image-models/timm/models/xception_aligned.py", "commit_date": "2020-10-13 00:00:00", "repo_name": "huggingface/pytorch-image-models", "stars": 29010, "license": "apache-2.0", "language": "python", "size": 25509}
{"docstring": "# download and load the pre-trained model\n", "func_signal": "def convert(mxnet_name, torch_name):\n", "code": "net = gluoncv.model_zoo.get_model(mxnet_name, pretrained=True)\n\n# create corresponding torch model\ntorch_net = create_model(torch_name)\n\nmxp = [(k, v) for k, v in net.collect_params().items() if 'running' not in k]\ntorchp = list(torch_net.named_parameters())\ntorch_params = {}\n\n# convert parameters\n# NOTE: we are relying on the fact that the order of parameters\n# are usually exactly the same between these models, thus no key name mapping\n# is necessary. Asserts will trip if this is not the case.\nfor (tn, tv), (mn, mv) in zip(torchp, mxp):\n    m_split = mn.split('_')\n    t_split = tn.split('.')\n    print(t_split, m_split)\n    print(tv.shape, mv.shape)\n\n    # ensure ordering of BN params match since their sizes are not specific\n    if m_split[-1] == 'gamma':\n        assert t_split[-1] == 'weight'\n    if m_split[-1] == 'beta':\n        assert t_split[-1] == 'bias'\n\n    # ensure shapes match\n    assert all(t == m for t, m in zip(tv.shape, mv.shape))\n\n    torch_tensor = torch.from_numpy(mv.data().asnumpy())\n    torch_params[tn] = torch_tensor\n\n# convert buffers (batch norm running stats)\nmxb = [(k, v) for k, v in net.collect_params().items() if any(x in k for x in ['running_mean', 'running_var'])]\ntorchb = [(k, v) for k, v in torch_net.named_buffers() if 'num_batches' not in k]\nfor (tn, tv), (mn, mv) in zip(torchb, mxb):\n    print(tn, mn)\n    print(tv.shape, mv.shape)\n\n    # ensure ordering of BN params match since their sizes are not specific\n    if 'running_var' in tn:\n        assert 'running_var' in mn\n    if 'running_mean' in tn:\n        assert 'running_mean' in mn\n        \n    torch_tensor = torch.from_numpy(mv.data().asnumpy())\n    torch_params[tn] = torch_tensor\n\ntorch_net.load_state_dict(torch_params)\ntorch_filename = './%s.pth' % torch_name\ntorch.save(torch_net.state_dict(), torch_filename)\nwith open(torch_filename, 'rb') as f:\n    sha_hash = hashlib.sha256(f.read()).hexdigest()\nfinal_filename = os.path.splitext(torch_filename)[0] + '-' + sha_hash[:8] + '.pth'\nos.rename(torch_filename, final_filename)\nprint(\"=> Saved converted model to '{}, SHA256: {}'\".format(final_filename, sha_hash))", "path": "pytorch-image-models/convert/convert_from_mxnet.py", "commit_date": "2019-08-10 00:00:00", "repo_name": "huggingface/pytorch-image-models", "stars": 29010, "license": "apache-2.0", "language": "python", "size": 25509}
{"docstring": "\"\"\"Performs a single optimization step.\n\nArguments:\n    closure (callable, optional): A closure that reevaluates the model\n        and returns the loss.\n\"\"\"\n", "func_signal": "def step(self, closure=None):\n", "code": "loss = None\nif closure is not None:\n    loss = closure()\n\nfor group in self.param_groups:\n    for p in group['params']:\n        if p.grad is None:\n            continue\n        grad = p.grad.data\n        state = self.state[p]\n\n        # State initialization\n        if len(state) == 0:\n            state['step'] = 0\n            state['m_schedule'] = 1.\n            state['exp_avg'] = grad.new().resize_as_(grad).zero_()\n            state['exp_avg_sq'] = grad.new().resize_as_(grad).zero_()\n\n        # Warming momentum schedule\n        m_schedule = state['m_schedule']\n        schedule_decay = group['schedule_decay']\n        exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n        beta1, beta2 = group['betas']\n        eps = group['eps']\n        state['step'] += 1\n        t = state['step']\n\n        if group['weight_decay'] != 0:\n            grad = grad.add(group['weight_decay'], p.data)\n\n        momentum_cache_t = beta1 * \\\n            (1. - 0.5 * (0.96 ** (t * schedule_decay)))\n        momentum_cache_t_1 = beta1 * \\\n            (1. - 0.5 * (0.96 ** ((t + 1) * schedule_decay)))\n        m_schedule_new = m_schedule * momentum_cache_t\n        m_schedule_next = m_schedule * momentum_cache_t * momentum_cache_t_1\n        state['m_schedule'] = m_schedule_new\n\n        # Decay the first and second moment running average coefficient\n        exp_avg.mul_(beta1).add_(1. - beta1, grad)\n        exp_avg_sq.mul_(beta2).addcmul_(1. - beta2, grad, grad)\n        exp_avg_sq_prime = exp_avg_sq / (1. - beta2 ** t)\n        denom = exp_avg_sq_prime.sqrt_().add_(eps)\n\n        p.data.addcdiv_(-group['lr'] * (1. - momentum_cache_t) / (1. - m_schedule_new), grad, denom)\n        p.data.addcdiv_(-group['lr'] * momentum_cache_t_1 / (1. - m_schedule_next), exp_avg, denom)\n\nreturn loss", "path": "pytorch-image-models/timm/optim/nadam.py", "commit_date": "2019-06-20 00:00:00", "repo_name": "huggingface/pytorch-image-models", "stars": 29010, "license": "apache-2.0", "language": "python", "size": 25509}
{"docstring": "\"\"\" A fast collation function optimized for uint8 images (np array or torch) and int64 targets (labels)\"\"\"\n", "func_signal": "def fast_collate(batch):\n", "code": "assert isinstance(batch[0], tuple)\nbatch_size = len(batch)\nif isinstance(batch[0][0], tuple):\n    # This branch 'deinterleaves' and flattens tuples of input tensors into one tensor ordered by position\n    # such that all tuple of position n will end up in a torch.split(tensor, batch_size) in nth position\n    inner_tuple_size = len(batch[0][0])\n    flattened_batch_size = batch_size * inner_tuple_size\n    targets = torch.zeros(flattened_batch_size, dtype=torch.int64)\n    tensor = torch.zeros((flattened_batch_size, *batch[0][0][0].shape), dtype=torch.uint8)\n    for i in range(batch_size):\n        assert len(batch[i][0]) == inner_tuple_size  # all input tensor tuples must be same length\n        for j in range(inner_tuple_size):\n            targets[i + j * batch_size] = batch[i][1]\n            tensor[i + j * batch_size] += torch.from_numpy(batch[i][0][j])\n    return tensor, targets\nelif isinstance(batch[0][0], np.ndarray):\n    targets = torch.tensor([b[1] for b in batch], dtype=torch.int64)\n    assert len(targets) == batch_size\n    tensor = torch.zeros((batch_size, *batch[0][0].shape), dtype=torch.uint8)\n    for i in range(batch_size):\n        tensor[i] += torch.from_numpy(batch[i][0])\n    return tensor, targets\nelif isinstance(batch[0][0], torch.Tensor):\n    targets = torch.tensor([b[1] for b in batch], dtype=torch.int64)\n    assert len(targets) == batch_size\n    tensor = torch.zeros((batch_size, *batch[0][0].shape), dtype=torch.uint8)\n    for i in range(batch_size):\n        tensor[i].copy_(batch[i][0])\n    return tensor, targets\nelse:\n    assert False", "path": "pytorch-image-models/timm/data/loader.py", "commit_date": "2020-08-07 00:00:00", "repo_name": "huggingface/pytorch-image-models", "stars": 29010, "license": "apache-2.0", "language": "python", "size": 25509}
{"docstring": "# my port of Tensorflow adversarially trained Inception V3 from\n# http://download.tensorflow.org/models/adv_inception_v3_2017_08_18.tar.gz\n", "func_signal": "def adv_inception_v3(pretrained=False, **kwargs):\n", "code": "model = _create_inception_v3('adv_inception_v3', pretrained=pretrained, **kwargs)\nreturn model", "path": "pytorch-image-models/timm/models/inception_v3.py", "commit_date": "2020-12-01 00:00:00", "repo_name": "huggingface/pytorch-image-models", "stars": 29010, "license": "apache-2.0", "language": "python", "size": 25509}
{"docstring": "\"\"\"Performs a single optimization step.\n\nArguments:\n    closure (callable, optional): A closure that reevaluates the model\n    and returns the loss.\n\"\"\"\n", "func_signal": "def step(self, closure=None):\n", "code": "loss = None\nif closure is not None:\n    loss = closure()\n\nfor group in self.param_groups:\n    for p in group['params']:\n        if p.grad is None:\n            continue\n        grad = p.grad.data\n        if grad.is_sparse:\n            raise RuntimeError('Sparse gradients are not supported.')\n        amsgrad = group['amsgrad']\n\n        state = self.state[p]\n\n        # State initialization\n        if len(state) == 0:\n            state['step'] = 0\n            # Exponential moving average of gradient values\n            state['exp_avg'] = torch.zeros_like(p.data)\n            # Exponential moving average of squared gradient values\n            state['exp_avg_sq'] = torch.zeros([]).to(state['exp_avg'].device)\n            if amsgrad:\n                # Maintains max of all exp. moving avg. of sq. grad. values\n                state['max_exp_avg_sq'] = torch.zeros([]).to(state['exp_avg'].device)\n\n        exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n        if amsgrad:\n            max_exp_avg_sq = state['max_exp_avg_sq']\n        beta1, beta2 = group['betas']\n\n        state['step'] += 1\n\n        norm = torch.sum(torch.pow(grad, 2))\n\n        if exp_avg_sq == 0:\n            exp_avg_sq.copy_(norm)\n        else:\n            exp_avg_sq.mul_(beta2).add_(1 - beta2, norm)\n\n        if amsgrad:\n            # Maintains the maximum of all 2nd moment running avg. till now\n            torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n            # Use the max. for normalizing running avg. of gradient\n            denom = max_exp_avg_sq.sqrt().add_(group['eps'])\n        else:\n            denom = exp_avg_sq.sqrt().add_(group['eps'])\n\n        grad.div_(denom)\n        if group['weight_decay'] != 0:\n            grad.add_(group['weight_decay'], p.data)\n        if group['grad_averaging']:\n            grad.mul_(1 - beta1)\n        exp_avg.mul_(beta1).add_(grad)\n\n        p.data.add_(-group['lr'], exp_avg)\n\nreturn loss", "path": "pytorch-image-models/timm/optim/nvnovograd.py", "commit_date": "2019-08-29 00:00:00", "repo_name": "huggingface/pytorch-image-models", "stars": 29010, "license": "apache-2.0", "language": "python", "size": 25509}
{"docstring": "# from gluon pretrained models, best performing in terms of accuracy/loss metrics\n# https://gluon-cv.mxnet.io/model_zoo/classification.html\n", "func_signal": "def gluon_inception_v3(pretrained=False, **kwargs):\n", "code": "model = _create_inception_v3('gluon_inception_v3', pretrained=pretrained, **kwargs)\nreturn model", "path": "pytorch-image-models/timm/models/inception_v3.py", "commit_date": "2020-12-01 00:00:00", "repo_name": "huggingface/pytorch-image-models", "stars": 29010, "license": "apache-2.0", "language": "python", "size": 25509}
{"docstring": "# N x 768 x 17 x 17\n", "func_signal": "def forward(self, x):\n", "code": "x = F.avg_pool2d(x, kernel_size=5, stride=3)\n# N x 768 x 5 x 5\nx = self.conv0(x)\n# N x 128 x 5 x 5\nx = self.conv1(x)\n# N x 768 x 1 x 1\n# Adaptive average pooling\nx = F.adaptive_avg_pool2d(x, (1, 1))\n# N x 768 x 1 x 1\nx = torch.flatten(x, 1)\n# N x 768\nx = self.fc(x)\n# N x 1000\nreturn x", "path": "pytorch-image-models/timm/models/inception_v3.py", "commit_date": "2020-12-01 00:00:00", "repo_name": "huggingface/pytorch-image-models", "stars": 29010, "license": "apache-2.0", "language": "python", "size": 25509}
{"docstring": "# my port of Tensorflow SLIM weights (http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz)\n", "func_signal": "def tf_inception_v3(pretrained=False, **kwargs):\n", "code": "model = _create_inception_v3('tf_inception_v3', pretrained=pretrained, **kwargs)\nreturn model", "path": "pytorch-image-models/timm/models/inception_v3.py", "commit_date": "2020-12-01 00:00:00", "repo_name": "huggingface/pytorch-image-models", "stars": 29010, "license": "apache-2.0", "language": "python", "size": 25509}
{"docstring": "# original PyTorch weights, ported from Tensorflow but modified\n", "func_signal": "def inception_v3(pretrained=False, **kwargs):\n", "code": "model = _create_inception_v3('inception_v3', pretrained=pretrained, **kwargs)\nreturn model", "path": "pytorch-image-models/timm/models/inception_v3.py", "commit_date": "2020-12-01 00:00:00", "repo_name": "huggingface/pytorch-image-models", "stars": 29010, "license": "apache-2.0", "language": "python", "size": 25509}
{"docstring": "\"\"\" Set JIT executor to legacy w/ support for op fusion\nThis is hopefully a temporary need in 1.5/1.5.1/1.6 to restore performance due to changes\nin the JIT exectutor. These API are not supported so could change.\n\"\"\"\n#\n", "func_signal": "def set_jit_legacy():\n", "code": "assert hasattr(torch._C, '_jit_set_profiling_executor'), \"Old JIT behavior doesn't exist!\"\ntorch._C._jit_set_profiling_executor(False)\ntorch._C._jit_set_profiling_mode(False)\ntorch._C._jit_override_can_fuse_on_gpu(True)\n#torch._C._jit_set_texpr_fuser_enabled(True)", "path": "pytorch-image-models/timm/utils/jit.py", "commit_date": "2020-09-07 00:00:00", "repo_name": "huggingface/pytorch-image-models", "stars": 29010, "license": "apache-2.0", "language": "python", "size": 25509}
{"docstring": "\"\"\" Modified Aligned Xception-71\n\"\"\"\n", "func_signal": "def xception71(pretrained=False, **kwargs):\n", "code": "block_cfg = [\n    # entry flow\n    dict(in_chs=64, out_chs=128, stride=2),\n    dict(in_chs=128, out_chs=256, stride=1),\n    dict(in_chs=256, out_chs=256, stride=2),\n    dict(in_chs=256, out_chs=728, stride=1),\n    dict(in_chs=728, out_chs=728, stride=2),\n    # middle flow\n    *([dict(in_chs=728, out_chs=728, stride=1)] * 16),\n    # exit flow\n    dict(in_chs=728, out_chs=(728, 1024, 1024), stride=2),\n    dict(in_chs=1024, out_chs=(1536, 1536, 2048), stride=1, no_skip=True, start_with_relu=False),\n]\nmodel_args = dict(block_cfg=block_cfg, norm_kwargs=dict(eps=.001, momentum=.1), **kwargs)\nreturn _xception('xception71', pretrained=pretrained, **model_args)", "path": "pytorch-image-models/timm/models/xception_aligned.py", "commit_date": "2020-10-13 00:00:00", "repo_name": "huggingface/pytorch-image-models", "stars": 29010, "license": "apache-2.0", "language": "python", "size": 25509}
{"docstring": "# N x 3 x 299 x 299\n", "func_signal": "def forward_preaux(self, x):\n", "code": "x = self.Conv2d_1a_3x3(x)\n# N x 32 x 149 x 149\nx = self.Conv2d_2a_3x3(x)\n# N x 32 x 147 x 147\nx = self.Conv2d_2b_3x3(x)\n# N x 64 x 147 x 147\nx = self.Pool1(x)\n# N x 64 x 73 x 73\nx = self.Conv2d_3b_1x1(x)\n# N x 80 x 73 x 73\nx = self.Conv2d_4a_3x3(x)\n# N x 192 x 71 x 71\nx = self.Pool2(x)\n# N x 192 x 35 x 35\nx = self.Mixed_5b(x)\n# N x 256 x 35 x 35\nx = self.Mixed_5c(x)\n# N x 288 x 35 x 35\nx = self.Mixed_5d(x)\n# N x 288 x 35 x 35\nx = self.Mixed_6a(x)\n# N x 768 x 17 x 17\nx = self.Mixed_6b(x)\n# N x 768 x 17 x 17\nx = self.Mixed_6c(x)\n# N x 768 x 17 x 17\nx = self.Mixed_6d(x)\n# N x 768 x 17 x 17\nx = self.Mixed_6e(x)\n# N x 768 x 17 x 17\nreturn x", "path": "pytorch-image-models/timm/models/inception_v3.py", "commit_date": "2020-12-01 00:00:00", "repo_name": "huggingface/pytorch-image-models", "stars": 29010, "license": "apache-2.0", "language": "python", "size": 25509}
{"docstring": "\"\"\"Performs a single optimization step.\n\nArguments:\n    closure (callable, optional): A closure that reevaluates the model\n        and returns the loss.\n\"\"\"\n", "func_signal": "def step(self, closure=None):\n", "code": "loss = None\nif closure is not None:\n    loss = closure()\n\nfor group in self.param_groups:\n    for p in group['params']:\n        if p.grad is None:\n            continue\n\n        # Perform stepweight decay\n        p.data.mul_(1 - group['lr'] * group['weight_decay'])\n\n        # Perform optimization step\n        grad = p.grad.data\n        if grad.is_sparse:\n            raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n        amsgrad = group['amsgrad']\n\n        state = self.state[p]\n\n        # State initialization\n        if len(state) == 0:\n            state['step'] = 0\n            # Exponential moving average of gradient values\n            state['exp_avg'] = torch.zeros_like(p.data)\n            # Exponential moving average of squared gradient values\n            state['exp_avg_sq'] = torch.zeros_like(p.data)\n            if amsgrad:\n                # Maintains max of all exp. moving avg. of sq. grad. values\n                state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n\n        exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n        if amsgrad:\n            max_exp_avg_sq = state['max_exp_avg_sq']\n        beta1, beta2 = group['betas']\n\n        state['step'] += 1\n        bias_correction1 = 1 - beta1 ** state['step']\n        bias_correction2 = 1 - beta2 ** state['step']\n\n        # Decay the first and second moment running average coefficient\n        exp_avg.mul_(beta1).add_(1 - beta1, grad)\n        exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n        if amsgrad:\n            # Maintains the maximum of all 2nd moment running avg. till now\n            torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n            # Use the max. for normalizing running avg. of gradient\n            denom = (max_exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n        else:\n            denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n\n        step_size = group['lr'] / bias_correction1\n\n        p.data.addcdiv_(-step_size, exp_avg, denom)\n\nreturn loss", "path": "pytorch-image-models/timm/optim/adamw.py", "commit_date": "2019-08-28 00:00:00", "repo_name": "huggingface/pytorch-image-models", "stars": 29010, "license": "apache-2.0", "language": "python", "size": 25509}
{"docstring": "\"\"\"\nComputes the Hutchinson approximation of the hessian trace and accumulates it for each trainable parameter.\n\"\"\"\n\n", "func_signal": "def set_hessian(self):\n", "code": "params = []\nfor p in filter(lambda p: p.grad is not None, self.get_params()):\n    if self.state[p][\"hessian step\"] % self.update_each == 0:  # compute the trace only each `update_each` step\n        params.append(p)\n    self.state[p][\"hessian step\"] += 1\n\nif len(params) == 0:\n    return\n\nif self.generator.device != params[0].device:  # hackish way of casting the generator to the right device\n    self.generator = torch.Generator(params[0].device).manual_seed(self.seed)\n\ngrads = [p.grad for p in params]\n\nfor i in range(self.n_samples):\n    # Rademacher distribution {-1.0, 1.0}\n    zs = [torch.randint(0, 2, p.size(), generator=self.generator, device=p.device) * 2.0 - 1.0 for p in params]\n    h_zs = torch.autograd.grad(\n        grads, params, grad_outputs=zs, only_inputs=True, retain_graph=i < self.n_samples - 1)\n    for h_z, z, p in zip(h_zs, zs, params):\n        p.hess += h_z * z / self.n_samples  # approximate the expected values of z*(H@z)", "path": "pytorch-image-models/timm/optim/adahessian.py", "commit_date": "2020-10-10 00:00:00", "repo_name": "huggingface/pytorch-image-models", "stars": 29010, "license": "apache-2.0", "language": "python", "size": 25509}
{"docstring": "#assert id(self.param_groups) == id(self.base_optimizer.param_groups)\n", "func_signal": "def step(self, closure=None):\n", "code": "loss = self.base_optimizer.step(closure)\nfor group in self.param_groups:\n    group['lookahead_step'] += 1\n    if group['lookahead_step'] % group['lookahead_k'] == 0:\n        self.update_slow(group)\nreturn loss", "path": "pytorch-image-models/timm/optim/lookahead.py", "commit_date": "2020-07-27 00:00:00", "repo_name": "huggingface/pytorch-image-models", "stars": 29010, "license": "apache-2.0", "language": "python", "size": 25509}
{"docstring": "# get per stage args for stage and containing blocks, calculate strides to meet target output_stride\n", "func_signal": "def _cfg_to_stage_args(cfg, curr_stride=2, output_stride=32, drop_path_rate=0.):\n", "code": "num_stages = len(cfg['depth'])\nif 'groups' not in cfg:\n    cfg['groups'] = (1,) * num_stages\nif 'down_growth' in cfg and not isinstance(cfg['down_growth'], (list, tuple)):\n    cfg['down_growth'] = (cfg['down_growth'],) * num_stages\nif 'cross_linear' in cfg and not isinstance(cfg['cross_linear'], (list, tuple)):\n    cfg['cross_linear'] = (cfg['cross_linear'],) * num_stages\ncfg['block_dpr'] = [None] * num_stages if not drop_path_rate else \\\n    [x.tolist() for x in torch.linspace(0, drop_path_rate, sum(cfg['depth'])).split(cfg['depth'])]\nstage_strides = []\nstage_dilations = []\nstage_first_dilations = []\ndilation = 1\nfor cfg_stride in cfg['stride']:\n    stage_first_dilations.append(dilation)\n    if curr_stride >= output_stride:\n        dilation *= cfg_stride\n        stride = 1\n    else:\n        stride = cfg_stride\n        curr_stride *= stride\n    stage_strides.append(stride)\n    stage_dilations.append(dilation)\ncfg['stride'] = stage_strides\ncfg['dilation'] = stage_dilations\ncfg['first_dilation'] = stage_first_dilations\nstage_args = [dict(zip(cfg.keys(), values)) for values in zip(*cfg.values())]\nreturn stage_args", "path": "pytorch-image-models/timm/models/cspnet.py", "commit_date": "2020-08-12 00:00:00", "repo_name": "huggingface/pytorch-image-models", "stars": 29010, "license": "apache-2.0", "language": "python", "size": 25509}
{"docstring": "\"\"\" Modified Aligned Xception-65\n\"\"\"\n", "func_signal": "def xception65(pretrained=False, **kwargs):\n", "code": "block_cfg = [\n    # entry flow\n    dict(in_chs=64, out_chs=128, stride=2),\n    dict(in_chs=128, out_chs=256, stride=2),\n    dict(in_chs=256, out_chs=728, stride=2),\n    # middle flow\n    *([dict(in_chs=728, out_chs=728, stride=1)] * 16),\n    # exit flow\n    dict(in_chs=728, out_chs=(728, 1024, 1024), stride=2),\n    dict(in_chs=1024, out_chs=(1536, 1536, 2048), stride=1, no_skip=True, start_with_relu=False),\n]\nmodel_args = dict(block_cfg=block_cfg, norm_kwargs=dict(eps=.001, momentum=.1), **kwargs)\nreturn _xception('xception65', pretrained=pretrained, **model_args)", "path": "pytorch-image-models/timm/models/xception_aligned.py", "commit_date": "2020-10-13 00:00:00", "repo_name": "huggingface/pytorch-image-models", "stars": 29010, "license": "apache-2.0", "language": "python", "size": 25509}
{"docstring": "\"\"\"Retrieves client library object that allow access to Cloud Data Catalog service.\"\"\"\n", "func_signal": "def get_conn(self) -> DataCatalogClient:\n", "code": "if not self._client:\n    self._client = DataCatalogClient(\n        credentials=self._get_credentials(), client_info=self.client_info\n    )\nreturn self._client", "path": "airflow/airflow/providers/google/cloud/hooks/datacatalog.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "apache/airflow", "stars": 33783, "license": "apache-2.0", "language": "python", "size": 265373}
{"docstring": "\"\"\"\nSet dictionary in 'google_cloud_default' connection to contain content\nof the json service account file.\n:return: None\n\"\"\"\n", "func_signal": "def set_dictionary_in_airflow_connection(self):\n", "code": "session = settings.Session()\ntry:\n    conn = session.query(Connection).filter(Connection.conn_id == 'google_cloud_default')[0]\n    extras = conn.extra_dejson\n    with open(self.full_key_path) as path_file:\n        content = json.load(path_file)\n    extras[KEYFILE_DICT_EXTRA] = json.dumps(content)\n    if extras.get(KEYPATH_EXTRA):\n        del extras[KEYPATH_EXTRA]\n    extras[SCOPE_EXTRA] = 'https://www.googleapis.com/auth/cloud-platform'\n    extras[PROJECT_EXTRA] = self.project_extra\n    conn.extra = json.dumps(extras)\n    session.commit()\nexcept BaseException as ex:\n    self.log.error('Airflow DB Session error: %s', str(ex))\n    session.rollback()\n    raise\nfinally:\n    session.close()", "path": "airflow/tests/providers/google/cloud/utils/gcp_authenticator.py", "commit_date": "2020-12-18 00:00:00", "repo_name": "apache/airflow", "stars": 33783, "license": "apache-2.0", "language": "python", "size": 265373}
{"docstring": "\"\"\"Test variable_delete command\"\"\"\n", "func_signal": "def test_variables_delete(self):\n", "code": "variable_command.variables_set(self.parser.parse_args(['variables', 'set', 'foo', 'bar']))\nvariable_command.variables_delete(self.parser.parse_args(['variables', 'delete', 'foo']))\nself.assertRaises(KeyError, Variable.get, \"foo\")", "path": "airflow/tests/cli/commands/test_variable_command.py", "commit_date": "2020-11-15 00:00:00", "repo_name": "apache/airflow", "stars": 33783, "license": "apache-2.0", "language": "python", "size": 265373}
{"docstring": "\"\"\"Test the execute function when the run is successful.\"\"\"\n\n", "func_signal": "def test_execute(self, gcs_mock_hook, adls_one_mock_hook, adls_two_mock_hook):\n", "code": "operator = ADLSToGCSOperator(\n    task_id=TASK_ID,\n    src_adls=ADLS_PATH_1,\n    dest_gcs=GCS_PATH,\n    replace=False,\n    azure_data_lake_conn_id=AZURE_CONN_ID,\n    google_cloud_storage_conn_id=GCS_CONN_ID,\n    google_impersonation_chain=IMPERSONATION_CHAIN,\n)\n\nadls_one_mock_hook.return_value.list.return_value = MOCK_FILES\nadls_two_mock_hook.return_value.list.return_value = MOCK_FILES\n\n# gcs_mock_hook.return_value.upload.side_effect = _assert_upload\nuploaded_files = operator.execute(None)\ngcs_mock_hook.return_value.upload.assert_has_calls(\n    [\n        mock.call(\n            bucket_name='test', filename=mock.ANY, object_name='test/path/PARQUET.parquet', gzip=False\n        ),\n        mock.call(\n            bucket_name='test', filename=mock.ANY, object_name='test/path/TEST3.csv', gzip=False\n        ),\n        mock.call(bucket_name='test', filename=mock.ANY, object_name='test/path/PIC.png', gzip=False),\n        mock.call(bucket_name='test', filename=mock.ANY, object_name='test/TEST1.csv', gzip=False),\n        mock.call(bucket_name='test', filename=mock.ANY, object_name='test/TEST2.csv', gzip=False),\n    ],\n    any_order=True,\n)\n\nadls_one_mock_hook.assert_called_once_with(azure_data_lake_conn_id=AZURE_CONN_ID)\nadls_two_mock_hook.assert_called_once_with(azure_data_lake_conn_id=AZURE_CONN_ID)\ngcs_mock_hook.assert_called_once_with(\n    google_cloud_storage_conn_id=GCS_CONN_ID,\n    delegate_to=None,\n    impersonation_chain=IMPERSONATION_CHAIN,\n)\n\n# we expect MOCK_FILES to be uploaded\nself.assertEqual(sorted(MOCK_FILES), sorted(uploaded_files))", "path": "airflow/tests/providers/google/cloud/transfers/test_adls_to_gcs.py", "commit_date": "2020-11-03 00:00:00", "repo_name": "apache/airflow", "stars": 33783, "license": "apache-2.0", "language": "python", "size": 265373}
{"docstring": "\"\"\"\nTest check selection from vertica into memory and\nafter that inserting into mysql\n\"\"\"\n", "func_signal": "def test_select_insert_transfer(self, *args):\n", "code": "task = VerticaToMySqlOperator(\n    task_id='test_task_id',\n    sql='select a, b, c',\n    mysql_table='test_table',\n    vertica_conn_id='test_vertica_conn_id',\n    mysql_conn_id='test_mysql_conn_id',\n    params={},\n    bulk_load=False,\n    dag=self.dag,\n)\ntask.execute(None)", "path": "airflow/tests/providers/mysql/transfers/test_vertica_to_mysql.py", "commit_date": "2020-09-09 00:00:00", "repo_name": "apache/airflow", "stars": 33783, "license": "apache-2.0", "language": "python", "size": 265373}
{"docstring": "\"\"\"Test variable_set command\"\"\"\n", "func_signal": "def test_variables_set(self):\n", "code": "variable_command.variables_set(self.parser.parse_args(['variables', 'set', 'foo', 'bar']))\nself.assertIsNotNone(Variable.get(\"foo\"))\nself.assertRaises(KeyError, Variable.get, \"foo1\")", "path": "airflow/tests/cli/commands/test_variable_command.py", "commit_date": "2020-11-15 00:00:00", "repo_name": "apache/airflow", "stars": 33783, "license": "apache-2.0", "language": "python", "size": 265373}
{"docstring": "\"\"\"\nSet key path in 'google_cloud_default' connection to point to the full\nkey path\n:return: None\n\"\"\"\n", "func_signal": "def set_key_path_in_airflow_connection(self):\n", "code": "session = settings.Session()\ntry:\n    conn = session.query(Connection).filter(Connection.conn_id == 'google_cloud_default')[0]\n    extras = conn.extra_dejson\n    extras[KEYPATH_EXTRA] = self.full_key_path\n    if extras.get(KEYFILE_DICT_EXTRA):\n        del extras[KEYFILE_DICT_EXTRA]\n    extras[SCOPE_EXTRA] = 'https://www.googleapis.com/auth/cloud-platform'\n    extras[PROJECT_EXTRA] = self.project_extra if self.project_extra else self.project_id\n    conn.extra = json.dumps(extras)\n    session.commit()\nexcept BaseException as ex:\n    self.log.error('Airflow DB Session error: %s', str(ex))\n    session.rollback()\n    raise\nfinally:\n    session.close()", "path": "airflow/tests/providers/google/cloud/utils/gcp_authenticator.py", "commit_date": "2020-12-18 00:00:00", "repo_name": "apache/airflow", "stars": 33783, "license": "apache-2.0", "language": "python", "size": 265373}
{"docstring": "\"\"\"Test storage of various data types\"\"\"\n# Set a dict\n", "func_signal": "def test_variables_set_different_types(self):\n", "code": "variable_command.variables_set(\n    self.parser.parse_args(['variables', 'set', 'dict', '{\"foo\": \"oops\"}'])\n)\n# Set a list\nvariable_command.variables_set(self.parser.parse_args(['variables', 'set', 'list', '[\"oops\"]']))\n# Set str\nvariable_command.variables_set(self.parser.parse_args(['variables', 'set', 'str', 'hello string']))\n# Set int\nvariable_command.variables_set(self.parser.parse_args(['variables', 'set', 'int', '42']))\n# Set float\nvariable_command.variables_set(self.parser.parse_args(['variables', 'set', 'float', '42.0']))\n# Set true\nvariable_command.variables_set(self.parser.parse_args(['variables', 'set', 'true', 'true']))\n# Set false\nvariable_command.variables_set(self.parser.parse_args(['variables', 'set', 'false', 'false']))\n# Set none\nvariable_command.variables_set(self.parser.parse_args(['variables', 'set', 'null', 'null']))\n\n# Export and then import\nvariable_command.variables_export(\n    self.parser.parse_args(['variables', 'export', 'variables_types.json'])\n)\nvariable_command.variables_import(\n    self.parser.parse_args(['variables', 'import', 'variables_types.json'])\n)\n\n# Assert value\nself.assertEqual({'foo': 'oops'}, Variable.get('dict', deserialize_json=True))\nself.assertEqual(['oops'], Variable.get('list', deserialize_json=True))\nself.assertEqual('hello string', Variable.get('str'))  # cannot json.loads(str)\nself.assertEqual(42, Variable.get('int', deserialize_json=True))\nself.assertEqual(42.0, Variable.get('float', deserialize_json=True))\nself.assertEqual(True, Variable.get('true', deserialize_json=True))\nself.assertEqual(False, Variable.get('false', deserialize_json=True))\nself.assertEqual(None, Variable.get('null', deserialize_json=True))\n\nos.remove('variables_types.json')", "path": "airflow/tests/cli/commands/test_variable_command.py", "commit_date": "2020-11-15 00:00:00", "repo_name": "apache/airflow", "stars": 33783, "license": "apache-2.0", "language": "python", "size": 265373}
{"docstring": "\"\"\"Test the execute function when the run is successful.\"\"\"\n\n", "func_signal": "def test_execute_with_gzip(self, gcs_mock_hook, adls_one_mock_hook, adls_two_mock_hook):\n", "code": "operator = ADLSToGCSOperator(\n    task_id=TASK_ID,\n    src_adls=ADLS_PATH_1,\n    dest_gcs=GCS_PATH,\n    replace=False,\n    azure_data_lake_conn_id=AZURE_CONN_ID,\n    google_cloud_storage_conn_id=GCS_CONN_ID,\n    gzip=True,\n)\n\nadls_one_mock_hook.return_value.list.return_value = MOCK_FILES\nadls_two_mock_hook.return_value.list.return_value = MOCK_FILES\n\n# gcs_mock_hook.return_value.upload.side_effect = _assert_upload\nuploaded_files = operator.execute(None)\ngcs_mock_hook.return_value.upload.assert_has_calls(\n    [\n        mock.call(\n            bucket_name='test', filename=mock.ANY, object_name='test/path/PARQUET.parquet', gzip=True\n        ),\n        mock.call(\n            bucket_name='test', filename=mock.ANY, object_name='test/path/TEST3.csv', gzip=True\n        ),\n        mock.call(bucket_name='test', filename=mock.ANY, object_name='test/path/PIC.png', gzip=True),\n        mock.call(bucket_name='test', filename=mock.ANY, object_name='test/TEST1.csv', gzip=True),\n        mock.call(bucket_name='test', filename=mock.ANY, object_name='test/TEST2.csv', gzip=True),\n    ],\n    any_order=True,\n)\n\n# we expect MOCK_FILES to be uploaded\nself.assertEqual(sorted(MOCK_FILES), sorted(uploaded_files))", "path": "airflow/tests/providers/google/cloud/transfers/test_adls_to_gcs.py", "commit_date": "2020-11-03 00:00:00", "repo_name": "apache/airflow", "stars": 33783, "license": "apache-2.0", "language": "python", "size": 265373}
{"docstring": "\"\"\"\nSets full key path - if GCP_CONFIG_DIR points to absolute\n    directory, it tries to find the key in this directory. Otherwise it assumes\n    that Airflow is running from the directory where configuration is checked\n    out next to airflow directory in config directory\n    it tries to find the key folder in the workspace's config\n    directory.\n:param : name of the key file to find.\n\"\"\"\n", "func_signal": "def _set_key_path(self):\n", "code": "if \"GCP_CONFIG_DIR\" in os.environ:\n    gcp_config_dir = os.environ[\"GCP_CONFIG_DIR\"]\nelse:\n    gcp_config_dir = os.path.join(AIRFLOW_MAIN_FOLDER, os.pardir, \"config\")\nif not os.path.isdir(gcp_config_dir):\n    self.log.info(\"The %s is not a directory\", gcp_config_dir)\nkey_dir = os.path.join(gcp_config_dir, \"keys\")\nif not os.path.isdir(key_dir):\n    self.log.error(\"The %s is not a directory\", key_dir)\n    return\nkey_path = os.path.join(key_dir, self.gcp_key)\nif not os.path.isfile(key_path):\n    self.log.error(\"The %s file is missing\", key_path)\nself.full_key_path = key_path", "path": "airflow/tests/providers/google/cloud/utils/gcp_authenticator.py", "commit_date": "2020-12-18 00:00:00", "repo_name": "apache/airflow", "stars": 33783, "license": "apache-2.0", "language": "python", "size": 265373}
{"docstring": "\"\"\"\nChange default authentication to none - which is not existing one.\n\"\"\"\n", "func_signal": "def gcp_revoke_authentication(self):\n", "code": "self._validate_key_set()\nself.log.info(\"Revoking authentication - setting it to none\")\nself.execute_cmd(['gcloud', 'config', 'get-value', 'account', f'--project={self.project_id}'])\nself.execute_cmd(['gcloud', 'config', 'set', 'account', 'none', f'--project={self.project_id}'])", "path": "airflow/tests/providers/google/cloud/utils/gcp_authenticator.py", "commit_date": "2020-12-18 00:00:00", "repo_name": "apache/airflow", "stars": 33783, "license": "apache-2.0", "language": "python", "size": 265373}
{"docstring": "\"\"\"\nReturns Gcp Video Intelligence Service client\n\n:rtype: google.cloud.videointelligence_v1.VideoIntelligenceServiceClient\n\"\"\"\n", "func_signal": "def get_conn(self) -> VideoIntelligenceServiceClient:\n", "code": "if not self._conn:\n    self._conn = VideoIntelligenceServiceClient(\n        credentials=self._get_credentials(), client_info=self.client_info\n    )\nreturn self._conn", "path": "airflow/airflow/providers/google/cloud/hooks/video_intelligence.py", "commit_date": "2020-10-20 00:00:00", "repo_name": "apache/airflow", "stars": 33783, "license": "apache-2.0", "language": "python", "size": 265373}
{"docstring": "\"\"\"Test AdlsToGoogleCloudStorageOperator instance is properly initialized.\"\"\"\n\n", "func_signal": "def test_init(self):\n", "code": "operator = ADLSToGCSOperator(\n    task_id=TASK_ID,\n    src_adls=ADLS_PATH_1,\n    dest_gcs=GCS_PATH,\n    replace=False,\n    azure_data_lake_conn_id=AZURE_CONN_ID,\n    gcp_conn_id=GCS_CONN_ID,\n)\n\nself.assertEqual(operator.task_id, TASK_ID)\nself.assertEqual(operator.src_adls, ADLS_PATH_1)\nself.assertEqual(operator.dest_gcs, GCS_PATH)\nself.assertEqual(operator.replace, False)\nself.assertEqual(operator.gcp_conn_id, GCS_CONN_ID)\nself.assertEqual(operator.azure_data_lake_conn_id, AZURE_CONN_ID)", "path": "airflow/tests/providers/google/cloud/transfers/test_adls_to_gcs.py", "commit_date": "2020-11-03 00:00:00", "repo_name": "apache/airflow", "stars": 33783, "license": "apache-2.0", "language": "python", "size": 265373}
{"docstring": "\"\"\"\nRestore authentication to the original one one.\n\"\"\"\n", "func_signal": "def gcp_restore_authentication(self):\n", "code": "self._validate_key_set()\nif GcpAuthenticator.original_account:\n    self.log.info(\"Restoring original account stored: %s\", GcpAuthenticator.original_account)\n    subprocess.call(\n        [\n            'gcloud',\n            'config',\n            'set',\n            'account',\n            GcpAuthenticator.original_account,\n            f'--project={self.project_id}',\n        ]\n    )\nelse:\n    self.log.info(\"Not restoring the original Google Cloud account: it is not set\")", "path": "airflow/tests/providers/google/cloud/utils/gcp_authenticator.py", "commit_date": "2020-12-18 00:00:00", "repo_name": "apache/airflow", "stars": 33783, "license": "apache-2.0", "language": "python", "size": 265373}
{"docstring": "\"\"\"\nStore authentication as it was originally so it can be restored and revoke\nauthentication.\n\"\"\"\n", "func_signal": "def gcp_store_authentication(self):\n", "code": "self._validate_key_set()\nif not GcpAuthenticator.original_account:\n    GcpAuthenticator.original_account = self.check_output(\n        ['gcloud', 'config', 'get-value', 'account', f'--project={self.project_id}']\n    ).decode('utf-8')\n    self.log.info(\"Storing account: to restore it later %s\", GcpAuthenticator.original_account)", "path": "airflow/tests/providers/google/cloud/utils/gcp_authenticator.py", "commit_date": "2020-12-18 00:00:00", "repo_name": "apache/airflow", "stars": 33783, "license": "apache-2.0", "language": "python", "size": 265373}
{"docstring": "\"\"\"\nAuthenticate with service account specified via key name.\n\"\"\"\n", "func_signal": "def gcp_authenticate(self):\n", "code": "self._validate_key_set()\nself.log.info(\"Setting the Google Cloud key to %s\", self.full_key_path)\n# Checking if we can authenticate using service account credentials provided\nself.execute_cmd(\n    [\n        'gcloud',\n        'auth',\n        'activate-service-account',\n        f'--key-file={self.full_key_path}',\n        f'--project={self.project_id}',\n    ]\n)\nself.set_key_path_in_airflow_connection()", "path": "airflow/tests/providers/google/cloud/utils/gcp_authenticator.py", "commit_date": "2020-12-18 00:00:00", "repo_name": "apache/airflow", "stars": 33783, "license": "apache-2.0", "language": "python", "size": 265373}
{"docstring": "\"\"\"Test variables_import command\"\"\"\n", "func_signal": "def test_variables_import(self):\n", "code": "with self.assertRaisesRegex(SystemExit, r\"Invalid variables file\"):\n    variable_command.variables_import(self.parser.parse_args(['variables', 'import', os.devnull]))", "path": "airflow/tests/cli/commands/test_variable_command.py", "commit_date": "2020-11-15 00:00:00", "repo_name": "apache/airflow", "stars": 33783, "license": "apache-2.0", "language": "python", "size": 265373}
{"docstring": "\"\"\"\nEstablishes a connection depending on the security mode set via config or environment variable.\n:return: a hdfscli InsecureClient or KerberosClient object.\n:rtype: hdfs.InsecureClient or hdfs.ext.kerberos.KerberosClient\n\"\"\"\n", "func_signal": "def get_conn(self) -> Any:\n", "code": "connection = self._find_valid_server()\nif connection is None:\n    raise AirflowWebHDFSHookException(\"Failed to locate the valid server.\")\nreturn connection", "path": "airflow/airflow/providers/apache/hdfs/hooks/webhdfs.py", "commit_date": "2020-12-08 00:00:00", "repo_name": "apache/airflow", "stars": 33783, "license": "apache-2.0", "language": "python", "size": 265373}
{"docstring": "\"\"\"Test isolation of variables\"\"\"\n", "func_signal": "def test_variables_isolation(self):\n", "code": "tmp1 = tempfile.NamedTemporaryFile(delete=True)\ntmp2 = tempfile.NamedTemporaryFile(delete=True)\n\n# First export\nvariable_command.variables_set(self.parser.parse_args(['variables', 'set', 'foo', '{\"foo\":\"bar\"}']))\nvariable_command.variables_set(self.parser.parse_args(['variables', 'set', 'bar', 'original']))\nvariable_command.variables_export(self.parser.parse_args(['variables', 'export', tmp1.name]))\n\nfirst_exp = open(tmp1.name)\n\nvariable_command.variables_set(self.parser.parse_args(['variables', 'set', 'bar', 'updated']))\nvariable_command.variables_set(self.parser.parse_args(['variables', 'set', 'foo', '{\"foo\":\"oops\"}']))\nvariable_command.variables_delete(self.parser.parse_args(['variables', 'delete', 'foo']))\nvariable_command.variables_import(self.parser.parse_args(['variables', 'import', tmp1.name]))\n\nself.assertEqual('original', Variable.get('bar'))\nself.assertEqual('{\\n  \"foo\": \"bar\"\\n}', Variable.get('foo'))\n\n# Second export\nvariable_command.variables_export(self.parser.parse_args(['variables', 'export', tmp2.name]))\n\nsecond_exp = open(tmp2.name)\nself.assertEqual(first_exp.read(), second_exp.read())\n\n# Clean up files\nsecond_exp.close()\nfirst_exp.close()", "path": "airflow/tests/cli/commands/test_variable_command.py", "commit_date": "2020-11-15 00:00:00", "repo_name": "apache/airflow", "stars": 33783, "license": "apache-2.0", "language": "python", "size": 265373}
{"docstring": "\"\"\"\nTest check selection from vertica into temporary file and\nafter that bulk inserting into mysql\n\"\"\"\n", "func_signal": "def test_select_bulk_insert_transfer(self, *args):\n", "code": "task = VerticaToMySqlOperator(\n    task_id='test_task_id',\n    sql='select a, b, c',\n    mysql_table='test_table',\n    vertica_conn_id='test_vertica_conn_id',\n    mysql_conn_id='test_mysql_conn_id',\n    params={},\n    bulk_load=True,\n    dag=self.dag,\n)\ntask.execute(None)", "path": "airflow/tests/providers/mysql/transfers/test_vertica_to_mysql.py", "commit_date": "2020-09-09 00:00:00", "repo_name": "apache/airflow", "stars": 33783, "license": "apache-2.0", "language": "python", "size": 265373}
{"docstring": "\"\"\"Return True if there is a line that the buffer can return, False otherwise.\"\"\"\n", "func_signal": "def yieldable(self):\n", "code": "if self.read_buffer is None:\n    return False\n\nt = _remove_trailing_new_line(self.read_buffer)\nn = _find_furthest_new_line(t)\nif n >= 0:\n    return True\n\n# we have read in entire file and have some unprocessed lines\nif self.read_position == 0 and self.read_buffer is not None:\n    return True\nreturn False", "path": "HanLP/hanlp/utils/file_read_backwards/buffer_work_space.py", "commit_date": "2020-01-01 00:00:00", "repo_name": "hankcs/HanLP", "stars": 31753, "license": "apache-2.0", "language": "python", "size": 72868}
{"docstring": "\"\"\"Whether to use L2 weight decay for `param_name`.\"\"\"\n", "func_signal": "def _do_use_weight_decay(self, param_name):\n", "code": "if self.weight_decay_rate == 0:\n  return False\n\nif self._include_in_weight_decay:\n  for r in self._include_in_weight_decay:\n    if re.search(r, param_name) is not None:\n      return True\n\nif self._exclude_from_weight_decay:\n  for r in self._exclude_from_weight_decay:\n    if re.search(r, param_name) is not None:\n      return False\nreturn True", "path": "HanLP/hanlp/optimizers/adamw/optimization.py", "commit_date": "2020-08-31 00:00:00", "repo_name": "hankcs/HanLP", "stars": 31753, "license": "apache-2.0", "language": "python", "size": 72868}
{"docstring": "\"\"\"Return True if every single line in the file has been returned, False otherwise.\"\"\"\n", "func_signal": "def has_returned_every_line(self):\n", "code": "if self.read_position == 0 and self.read_buffer is None:\n    return True\nreturn False", "path": "HanLP/hanlp/utils/file_read_backwards/buffer_work_space.py", "commit_date": "2020-01-01 00:00:00", "repo_name": "hankcs/HanLP", "stars": 31753, "license": "apache-2.0", "language": "python", "size": 72868}
{"docstring": "\"\"\"Add additional bytes content as read from the read_position.\n\nArgs:\n    content (bytes): data to be added to buffer working BufferWorkSpac.\n    read_position (int): where in the file pointer the data was read from.\n\"\"\"\n", "func_signal": "def add_to_buffer(self, content, read_position):\n", "code": "self.read_position = read_position\nif self.read_buffer is None:\n    self.read_buffer = content\nelse:\n    self.read_buffer = content + self.read_buffer", "path": "HanLP/hanlp/utils/file_read_backwards/buffer_work_space.py", "commit_date": "2020-01-01 00:00:00", "repo_name": "hankcs/HanLP", "stars": 31753, "license": "apache-2.0", "language": "python", "size": 72868}
{"docstring": "\"\"\"Return a line content (with a trailing newline) if there are content. Return '' otherwise.\"\"\"\n\n", "func_signal": "def readline(self):\n", "code": "try:\n    r = next(self.iterator) + os.linesep\n    return r\nexcept StopIteration:\n    return \"\"", "path": "HanLP/hanlp/utils/file_read_backwards/file_read_backwards.py", "commit_date": "2020-01-01 00:00:00", "repo_name": "hankcs/HanLP", "stars": 31753, "license": "apache-2.0", "language": "python", "size": 72868}
{"docstring": "\"\"\"Constructor for FileReadBackwards.\n\nArgs:\n    path: Path to the file to be read\n    encoding (str): Encoding\n    chunk_size (int): How many bytes to read at a time\n\"\"\"\n", "func_signal": "def __init__(self, path, encoding=\"utf-8\", chunk_size=io.DEFAULT_BUFFER_SIZE):\n", "code": "if encoding.lower() not in supported_encodings:\n    error_message = \"{0} encoding was not supported/tested.\".format(encoding)\n    error_message += \"Supported encodings are '{0}'\".format(\",\".join(supported_encodings))\n    raise NotImplementedError(error_message)\n\nself.path = path\nself.encoding = encoding.lower()\nself.chunk_size = chunk_size\nself.iterator = FileReadBackwardsIterator(io.open(self.path, mode=\"rb\"), self.encoding, self.chunk_size)", "path": "HanLP/hanlp/utils/file_read_backwards/file_read_backwards.py", "commit_date": "2020-01-01 00:00:00", "repo_name": "hankcs/HanLP", "stars": 31753, "license": "apache-2.0", "language": "python", "size": 72868}
{"docstring": "\"\"\"\nGenerate filename using current datetime, in 20180102_030405 format\nReturns\n-------\n\n\"\"\"\n", "func_signal": "def now_filename(fmt=\"%y%m%d_%H%M%S\"):\n", "code": "now = datetime.datetime.now()\nreturn now.strftime(fmt)", "path": "HanLP/hanlp/utils/time_util.py", "commit_date": "2020-01-01 00:00:00", "repo_name": "hankcs/HanLP", "stars": 31753, "license": "apache-2.0", "language": "python", "size": 72868}
{"docstring": "# check if a chunk ended between the previous and current word\n# arguments: previous and current chunk tags, previous and current types\n", "func_signal": "def end_of_chunk(prev_tag, tag, prev_type, type_):\n", "code": "return ((prev_tag == \"B\" and tag == \"B\") or\n        (prev_tag == \"B\" and tag == \"O\") or\n        (prev_tag == \"I\" and tag == \"B\") or\n        (prev_tag == \"I\" and tag == \"O\") or\n\n        (prev_tag == \"E\" and tag == \"E\") or\n        (prev_tag == \"E\" and tag == \"I\") or\n        (prev_tag == \"E\" and tag == \"O\") or\n        (prev_tag == \"I\" and tag == \"O\") or\n\n        (prev_tag != \"O\" and prev_tag != \".\" and prev_type != type_) or\n        (prev_tag == \"]\" or prev_tag == \"[\"))", "path": "HanLP/hanlp/metrics/chunking/conlleval.py", "commit_date": "2020-01-01 00:00:00", "repo_name": "hankcs/HanLP", "stars": 31753, "license": "apache-2.0", "language": "python", "size": 72868}
{"docstring": "\"\"\"\nsplit chunk tag into IOBES prefix and chunk_type\ne.g.\nB-PER -> (B, PER)\nO -> (O, None)\n\"\"\"\n", "func_signal": "def split_tag(chunk_tag):\n", "code": "if chunk_tag == 'O':\n    return ('O', None)\nreturn chunk_tag.split('-', maxsplit=1)", "path": "HanLP/hanlp/metrics/chunking/conlleval.py", "commit_date": "2020-01-01 00:00:00", "repo_name": "hankcs/HanLP", "stars": 31753, "license": "apache-2.0", "language": "python", "size": 72868}
{"docstring": "\"\"\"Creates an optimizer from its config with WarmUp custom object.\"\"\"\n", "func_signal": "def from_config(cls, config):\n", "code": "custom_objects = {'WarmUp': WarmUp}\nreturn super(AdamWeightDecay, cls).from_config(\n    config, custom_objects=custom_objects)", "path": "HanLP/hanlp/optimizers/adamw/optimization.py", "commit_date": "2020-08-31 00:00:00", "repo_name": "hankcs/HanLP", "stars": 31753, "license": "apache-2.0", "language": "python", "size": 72868}
{"docstring": "# check if a chunk started between the previous and current word\n# arguments: previous and current chunk tags, previous and current types\n", "func_signal": "def start_of_chunk(prev_tag, tag, prev_type, type_):\n", "code": "chunkStart = ((prev_tag == \"B\" and tag == \"B\") or\n              (prev_tag == \"B\" and tag == \"B\") or\n              (prev_tag == \"I\" and tag == \"B\") or\n              (prev_tag == \"O\" and tag == \"B\") or\n              (prev_tag == \"O\" and tag == \"I\") or\n\n              (prev_tag == \"E\" and tag == \"E\") or\n              (prev_tag == \"E\" and tag == \"I\") or\n              (prev_tag == \"O\" and tag == \"E\") or\n              (prev_tag == \"O\" and tag == \"I\") or\n\n              (tag != \"O\" and tag != \".\" and prev_type != type_) or\n              (tag == \"]\" or tag == \"[\"))\n# corrected 1998-12-22: these chunks are assumed to have length 1\n\n# print(\"startOfChunk?\", prevTag, tag, prevType, type)\n# print(chunkStart)\nreturn chunkStart", "path": "HanLP/hanlp/metrics/chunking/conlleval.py", "commit_date": "2020-01-01 00:00:00", "repo_name": "hankcs/HanLP", "stars": 31753, "license": "apache-2.0", "language": "python", "size": 72868}
{"docstring": "\"\"\"Convention for the data.\n\nWhen read_buffer is not None, it represents contents of the file from `read_position` onwards\n    that has not been processed/returned.\nread_position represents the file pointer position that has been read into read_buffer\n    initialized to be just past the end of file.\n\"\"\"\n", "func_signal": "def __init__(self, fp, chunk_size):\n", "code": "self.fp = fp\nself.read_position = _get_file_size(self.fp)  # set the previously read position to the\nself.read_buffer = None\nself.chunk_size = chunk_size", "path": "HanLP/hanlp/utils/file_read_backwards/buffer_work_space.py", "commit_date": "2020-01-01 00:00:00", "repo_name": "hankcs/HanLP", "stars": 31753, "license": "apache-2.0", "language": "python", "size": 72868}
{"docstring": "\"\"\"\ncompute overall precision, recall and FB1 (default values are 0.0)\nif percent is True, return 100 * original decimal value\n\"\"\"\n", "func_signal": "def calc_metrics(tp, p, t, percent=True):\n", "code": "precision = tp / p if p else 0\nrecall = tp / t if t else 0\nfb1 = 2 * precision * recall / (precision + recall) if precision + recall else 0\nif percent:\n    return 100 * precision, 100 * recall, 100 * fb1\nelse:\n    return precision, recall, fb1", "path": "HanLP/hanlp/metrics/chunking/conlleval.py", "commit_date": "2020-01-01 00:00:00", "repo_name": "hankcs/HanLP", "stars": 31753, "license": "apache-2.0", "language": "python", "size": 72868}
{"docstring": "\"\"\"Creates an optimizer with learning rate schedule.\"\"\"\n# Implements linear decay of the learning rate.\n", "func_signal": "def create_optimizer(init_lr, num_train_steps, num_warmup_steps):\n", "code": "learning_rate_fn = tf.keras.optimizers.schedules.PolynomialDecay(\n    initial_learning_rate=init_lr,\n    decay_steps=num_train_steps,\n    end_learning_rate=0.0)\nif num_warmup_steps:\n  learning_rate_fn = WarmUp(initial_learning_rate=init_lr,\n                            decay_schedule_fn=learning_rate_fn,\n                            warmup_steps=num_warmup_steps)\noptimizer = AdamWeightDecay(\n    learning_rate=learning_rate_fn,\n    weight_decay_rate=0.01,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-6,\n    exclude_from_weight_decay=['layer_norm', 'bias'])\nreturn optimizer", "path": "HanLP/hanlp/optimizers/adamw/optimization.py", "commit_date": "2020-08-31 00:00:00", "repo_name": "hankcs/HanLP", "stars": 31753, "license": "apache-2.0", "language": "python", "size": 72868}
{"docstring": "\"\"\"Closes all opened its file handler and propagates all exceptions on exit.\"\"\"\n", "func_signal": "def __exit__(self, exc_type, exc_val, exc_tb):\n", "code": "self.close()\nreturn False", "path": "HanLP/hanlp/utils/file_read_backwards/file_read_backwards.py", "commit_date": "2020-01-01 00:00:00", "repo_name": "hankcs/HanLP", "stars": 31753, "license": "apache-2.0", "language": "python", "size": 72868}
{"docstring": "\"\"\"Remove a single instance of new line at the end of l if it exists.\n\nReturns:\n    bytestring\n\"\"\"\n# replace only 1 instance of newline\n# match longest line first (hence the reverse=True), we want to match \"\\r\\n\" rather than \"\\n\" if we can\n", "func_signal": "def _remove_trailing_new_line(l):\n", "code": "for n in sorted(new_lines_bytes, key=lambda x: len(x), reverse=True):\n    if l.endswith(n):\n        remove_new_line = slice(None, -len(n))\n        return l[remove_new_line]\nreturn l", "path": "HanLP/hanlp/utils/file_read_backwards/buffer_work_space.py", "commit_date": "2020-01-01 00:00:00", "repo_name": "hankcs/HanLP", "stars": 31753, "license": "apache-2.0", "language": "python", "size": 72868}
{"docstring": "\"\"\"Returns unicode string from the last line until the beginning of file.\n\nGets exhausted if::\n\n    * already reached the beginning of the file on previous iteration\n    * the file got closed\n\nWhen it gets exhausted, it closes the file handler.\n\"\"\"\n# Using binary mode, because some encodings such as \"utf-8\" use variable number of\n# bytes to encode different Unicode points.\n# Without using binary mode, we would probably need to understand each encoding more\n# and do the seek operations to find the proper boundary before issuing read\n", "func_signal": "def next(self):\n", "code": "if self.closed:\n    raise StopIteration\nif self.__buf.has_returned_every_line():\n    self.close()\n    raise StopIteration\nself.__buf.read_until_yieldable()\nr = self.__buf.return_line()\nreturn r.decode(self.encoding)", "path": "HanLP/hanlp/utils/file_read_backwards/file_read_backwards.py", "commit_date": "2020-01-01 00:00:00", "repo_name": "hankcs/HanLP", "stars": 31753, "license": "apache-2.0", "language": "python", "size": 72868}
{"docstring": "\"\"\"Return information on which file pointer position to read from and how many bytes.\n\nArgs:\n    fp\n    past_read_positon (int): The file pointer position that has been read previously\n    chunk_size(int): ideal io chunk_size\n\nReturns:\n    (int, int): The next seek position, how many bytes to read next\n\"\"\"\n", "func_signal": "def _get_what_to_read_next(fp, previously_read_position, chunk_size):\n", "code": "seek_position = max(previously_read_position - chunk_size, 0)\nread_size = chunk_size\n\n# examples: say, our new_lines are potentially \"\\r\\n\", \"\\n\", \"\\r\"\n# find a reading point where it is not \"\\n\", rewind further if necessary\n# if we have \"\\r\\n\" and we read in \"\\n\",\n# the next iteration would treat \"\\r\" as a different new line.\n# Q: why don't I just check if it is b\"\\n\", but use a function ?\n# A: so that we can potentially expand this into generic sets of separators, later on.\nwhile seek_position > 0:\n    fp.seek(seek_position)\n    if _is_partially_read_new_line(fp.read(1)):\n        seek_position -= 1\n        read_size += 1  # as we rewind further, let's make sure we read more to compensate\n    else:\n        break\n\n# take care of special case when we are back to the beginnin of the file\nread_size = min(previously_read_position - seek_position, read_size)\nreturn seek_position, read_size", "path": "HanLP/hanlp/utils/file_read_backwards/buffer_work_space.py", "commit_date": "2020-01-01 00:00:00", "repo_name": "hankcs/HanLP", "stars": 31753, "license": "apache-2.0", "language": "python", "size": 72868}
{"docstring": "\"\"\"Return a new line if it is available.\n\nPrecondition: self.yieldable() must be True\n\"\"\"\n", "func_signal": "def return_line(self):\n", "code": "assert(self.yieldable())\n\nt = _remove_trailing_new_line(self.read_buffer)\ni = _find_furthest_new_line(t)\n\nif i >= 0:\n    l = i + 1\n    after_new_line = slice(l, None)\n    up_to_include_new_line = slice(0, l)\n    r = t[after_new_line]\n    self.read_buffer = t[up_to_include_new_line]\nelse:  # the case where we have read in entire file and at the \"last\" line\n    r = t\n    self.read_buffer = None\nreturn r", "path": "HanLP/hanlp/utils/file_read_backwards/buffer_work_space.py", "commit_date": "2020-01-01 00:00:00", "repo_name": "hankcs/HanLP", "stars": 31753, "license": "apache-2.0", "language": "python", "size": 72868}
{"docstring": "\"\"\"Retrieves the learning rate with the given state.\"\"\"\n", "func_signal": "def _get_lr(self, var_device, var_dtype, apply_state):\n", "code": "if apply_state is None:\n  return self._decayed_lr_t[var_dtype], {}\n\napply_state = apply_state or {}\ncoefficients = apply_state.get((var_device, var_dtype))\nif coefficients is None:\n  coefficients = self._fallback_apply_state(var_device, var_dtype)\n  apply_state[(var_device, var_dtype)] = coefficients\n\nreturn coefficients['lr_t'], dict(apply_state=apply_state)", "path": "HanLP/hanlp/optimizers/adamw/optimization.py", "commit_date": "2020-08-31 00:00:00", "repo_name": "hankcs/HanLP", "stars": 31753, "license": "apache-2.0", "language": "python", "size": 72868}
{"docstring": "# Assert the accuracy of the predicative algorithm (\"gro\u00dfer\" => \"gro\u00df\").\n", "func_signal": "def test_predicative(self):\n", "code": "from pattern.db import Datasheet\ni, n = 0, 0\nfor tag, pred, attr in Datasheet.load(os.path.join(PATH, \"corpora\", \"wordforms-de-celex.csv\")):\n    if tag == \"a\":\n        if de.predicative(attr) == pred:\n            i += 1\n        n += 1\nself.assertTrue(float(i) / n > 0.98)\nprint(\"pattern.de.predicative()\")", "path": "pattern/test/test_de.py", "commit_date": "2017-08-24 00:00:00", "repo_name": "clips/pattern", "stars": 8637, "license": "bsd-3-clause", "language": "python", "size": 52274}
{"docstring": "\"\"\" Annotates the tokens with lemmata for plural nouns and conjugated verbs,\n    where each token is a [word, part-of-speech] list.\n\"\"\"\n", "func_signal": "def find_lemmata(tokens):\n", "code": "for token in tokens:\n    word, pos, lemma = token[0], token[1], token[0]\n    if pos.startswith((\"DT\",)):\n        lemma = singularize(word, pos=\"DT\")\n    if pos.startswith(\"JJ\"):\n        lemma = predicative(word)\n    if pos == \"NNS\":\n        lemma = singularize(word)\n    if pos.startswith((\"VB\", \"MD\")):\n        lemma = conjugate(word, INFINITIVE) or word\n    token.append(lemma.lower())\nreturn tokens", "path": "pattern/pattern/text/it/__init__.py", "commit_date": "2017-08-24 00:00:00", "repo_name": "clips/pattern", "stars": 8637, "license": "bsd-3-clause", "language": "python", "size": 52274}
{"docstring": "#print(\"path:\", path)\n#print(\"data:\", data)\n# Construct a file name in /data from the URL path.\n# For example, path=(\"pages\", \"bio.html\")\n# is mapped to \"/data/pages/bio.html.txt\".\n", "func_signal": "def index(*path, **data):\n", "code": "page = \"/\".join(path)\npage = page if page else \"index.html\"\npage = page.replace(\" \", \"-\")\npage = page + \".txt\"\npage = os.path.join(app.path, \"data\", page) # Absolute paths are safer.\n#print(\"page:\", page)\n\n# If the URL ends in \"?save\", update the page content.\nif \"save\" in data and \"content\" in data:\n    return save(page, src=data[\"content\"])\n# If the URL ends in \"?edit\", show the page editor.\nif \"edit\" in data:\n    return edit(page)\n# If the page does not exist, show the page editor.\nif not os.path.exists(page):\n    return edit(page)\n# Show the page.\nelse:\n    return view(page)", "path": "pattern/examples/08-server/03-wiki/wiki.py", "commit_date": "2017-08-24 00:00:00", "repo_name": "clips/pattern", "stars": 8637, "license": "bsd-3-clause", "language": "python", "size": 52274}
{"docstring": "# Assert the accuracy of the verb lemmatization algorithm.\n# Note: the accuracy is higher (88%) when measured on CELEX word forms\n# (presumably because de.inflect.verbs has high percentage irregular verbs).\n", "func_signal": "def test_find_lemma(self):\n", "code": "i, n = 0, 0\nfor v1, v2 in de.inflect.verbs.inflections.items():\n    if de.inflect.verbs.find_lemma(v1) == v2:\n        i += 1\n    n += 1\nself.assertTrue(float(i) / n > 0.86)\nprint(\"pattern.de.inflect.verbs.find_lemma()\")", "path": "pattern/test/test_de.py", "commit_date": "2017-08-24 00:00:00", "repo_name": "clips/pattern", "stars": 8637, "license": "bsd-3-clause", "language": "python", "size": 52274}
{"docstring": "# Assert [(\"der\", \"DT\"), (\"grosse\", \"JJ\"), (\"Hund\", \"NN\")].\n", "func_signal": "def test_tag(self):\n", "code": "v = de.tag(\"der grosse Hund\")\nself.assertEqual(v, [(\"der\", \"DT\"), (\"grosse\", \"JJ\"), (\"Hund\", \"NN\")])\nprint(\"pattern.de.tag()\")", "path": "pattern/test/test_de.py", "commit_date": "2017-08-24 00:00:00", "repo_name": "clips/pattern", "stars": 8637, "license": "bsd-3-clause", "language": "python", "size": 52274}
{"docstring": "# Assert tense recognition.\n", "func_signal": "def test_tenses(self):\n", "code": "self.assertTrue((de.PRESENT, 3, de.SG) in de.tenses(\"ist\"))\nself.assertTrue(\"2sg\" in de.tenses(\"bist\"))\nprint(\"pattern.de.tenses()\")", "path": "pattern/test/test_de.py", "commit_date": "2017-08-24 00:00:00", "repo_name": "clips/pattern", "stars": 8637, "license": "bsd-3-clause", "language": "python", "size": 52274}
{"docstring": "\"\"\" Yields the semantically opposite synset, for example:\n    synsets(\"death\")[0].antonym => Synset(\"birth\").\n\"\"\"\n", "func_signal": "def antonym(self):\n", "code": "p = [Synset(a.synset()) for l in self._wnsynset.lemmas() for a in l.antonyms()]\nreturn len(p) > 0 and p or None", "path": "pattern/pattern/text/en/wordnet/__init__.py", "commit_date": "2017-08-24 00:00:00", "repo_name": "clips/pattern", "stars": 8637, "license": "bsd-3-clause", "language": "python", "size": 52274}
{"docstring": "# Assert different tenses with different conjugations.\n", "func_signal": "def test_conjugate(self):\n", "code": "for (v1, v2, tense) in (\n  (\"sein\",  \"sein\",     de.INFINITIVE),\n  (\"sein\",  \"bin\",     (de.PRESENT, 1, de.SINGULAR)),\n  (\"sein\",  \"bist\",    (de.PRESENT, 2, de.SINGULAR)),\n  (\"sein\",  \"ist\",     (de.PRESENT, 3, de.SINGULAR)),\n  (\"sein\",  \"sind\",    (de.PRESENT, 1, de.PLURAL)),\n  (\"sein\",  \"seid\",    (de.PRESENT, 2, de.PLURAL)),\n  (\"sein\",  \"sind\",    (de.PRESENT, 3, de.PLURAL)),\n  (\"sein\",  \"seiend\",  (de.PRESENT + de.PARTICIPLE)),\n  (\"sein\",  \"war\",     (de.PAST, 1, de.SINGULAR)),\n  (\"sein\",  \"warst\",   (de.PAST, 2, de.SINGULAR)),\n  (\"sein\",  \"war\",     (de.PAST, 3, de.SINGULAR)),\n  (\"sein\",  \"waren\",   (de.PAST, 1, de.PLURAL)),\n  (\"sein\",  \"wart\",    (de.PAST, 2, de.PLURAL)),\n  (\"sein\",  \"waren\",   (de.PAST, 3, de.PLURAL)),\n  (\"sein\",  \"gewesen\", (de.PAST + de.PARTICIPLE)),\n  (\"sein\",  \"sei\",     (de.PRESENT, 2, de.SINGULAR, de.IMPERATIVE)),\n  (\"sein\",  \"seien\",   (de.PRESENT, 1, de.PLURAL, de.IMPERATIVE)),\n  (\"sein\",  \"seid\",    (de.PRESENT, 2, de.PLURAL, de.IMPERATIVE)),\n  (\"sein\", \"sei\",     (de.PRESENT, 1, de.SINGULAR, de.SUBJUNCTIVE)),\n  (\"sein\", \"seiest\",  (de.PRESENT, 2, de.SINGULAR, de.SUBJUNCTIVE)),\n  (\"sein\", \"sei\",     (de.PRESENT, 3, de.SINGULAR, de.SUBJUNCTIVE)),\n  (\"sein\", \"seien\",   (de.PRESENT, 1, de.PLURAL, de.SUBJUNCTIVE)),\n  (\"sein\", \"seiet\",   (de.PRESENT, 2, de.PLURAL, de.SUBJUNCTIVE)),\n  (\"sein\", \"seien\",   (de.PRESENT, 3, de.PLURAL, de.SUBJUNCTIVE)),\n  (\"sein\", \"w\u00e4re\",    (de.PAST, 1, de.SINGULAR, de.SUBJUNCTIVE)),\n  (\"sein\", \"w\u00e4rest\",  (de.PAST, 2, de.SINGULAR, de.SUBJUNCTIVE)),\n  (\"sein\", \"w\u00e4re\",    (de.PAST, 3, de.SINGULAR, de.SUBJUNCTIVE)),\n  (\"sein\", \"w\u00e4ren\",   (de.PAST, 1, de.PLURAL, de.SUBJUNCTIVE)),\n  (\"sein\", \"w\u00e4ret\",   (de.PAST, 2, de.PLURAL, de.SUBJUNCTIVE)),\n  (\"sein\", \"w\u00e4ren\",   (de.PAST, 3, de.PLURAL, de.SUBJUNCTIVE))):\n    self.assertEqual(de.conjugate(v1, tense), v2)\nprint(\"pattern.de.conjugate()\")", "path": "pattern/test/test_de.py", "commit_date": "2017-08-24 00:00:00", "repo_name": "clips/pattern", "stars": 8637, "license": "bsd-3-clause", "language": "python", "size": 52274}
{"docstring": "\"\"\" Returns a sorted list of keywords in the given string.\n\"\"\"\n", "func_signal": "def keywords(s, top=10, **kwargs):\n", "code": "return parser.find_keywords(s, **dict({\n    \"frequency\": parser.frequency,\n          \"top\": top,\n          \"pos\": (\"NN\",),\n       \"ignore\": (\"rt\",)}, **kwargs))", "path": "pattern/pattern/text/it/__init__.py", "commit_date": "2017-08-24 00:00:00", "repo_name": "clips/pattern", "stars": 8637, "license": "bsd-3-clause", "language": "python", "size": 52274}
{"docstring": "\"\"\" Returns the common ancestor of both synsets.\n    For example synsets(\"cat\")[0].ancestor(synsets(\"dog\")[0]) => Synset(\"carnivore\")\n\"\"\"\n", "func_signal": "def ancestor(synset1, synset2):\n", "code": "h1, h2 = synset1.hypernyms(recursive=True), synset2.hypernyms(recursive=True)\nfor s in h1:\n    if s in h2:\n        return s", "path": "pattern/pattern/text/en/wordnet/__init__.py", "commit_date": "2017-08-24 00:00:00", "repo_name": "clips/pattern", "stars": 8637, "license": "bsd-3-clause", "language": "python", "size": 52274}
{"docstring": "\"\"\" A set of synonyms that share a common meaning.\n\"\"\"\n", "func_signal": "def __init__(self, synset):\n", "code": "if isinstance(synset, WordNetSynset):\n    self._wnsynset = synset\nelif isinstance(synset, Synset):\n    self = self\nelif isinstance(synset, (tuple, int)):\n    if isinstance(synset, int):\n        synset = (synset, \"NN\")\n    offset, pos = synset\n    self._wnsynset = wn._synset_from_pos_and_offset(_pattern2wordnet[pos] if pos in _pattern2wordnet else pos, offset)\nelse:\n    raise NotImplementedError\n\nself._synset = _synset", "path": "pattern/pattern/text/en/wordnet/__init__.py", "commit_date": "2017-08-24 00:00:00", "repo_name": "clips/pattern", "stars": 8637, "license": "bsd-3-clause", "language": "python", "size": 52274}
{"docstring": "\"\"\" Returns a list of Synset objects, one for each word sense.\n    Each word can be understood in different \"senses\", \n    each of which is part of a set of synonyms (= Synset).\n\"\"\"\n", "func_signal": "def synsets(word, pos=NOUN):\n", "code": "word, pos = normalize(word), pos.lower()\ntry:\n    if pos.startswith(NOUN.lower()): # \"NNS\" or \"nn\" will also pass.\n        w = wn.synsets(word, pos = wn.NOUN)\n    elif pos.startswith(VERB.lower()):\n        w = wn.synsets(word, pos = wn.VERB)\n    elif pos.startswith(ADJECTIVE.lower()):\n        w = wn.synsets(word, pos = wn.ADJ)\n    elif pos.startswith(ADVERB.lower()):\n        w = wn.synsets(word, pos = wn.ADV)\n    else:\n        raise TypeError(\"part of speech must be NOUN, VERB, ADJECTIVE or ADVERB, not %s\" % repr(pos))\n    return [Synset(synset) for synset in w]\nexcept KeyError:\n    return []\nreturn []", "path": "pattern/pattern/text/en/wordnet/__init__.py", "commit_date": "2017-08-24 00:00:00", "repo_name": "clips/pattern", "stars": 8637, "license": "bsd-3-clause", "language": "python", "size": 52274}
{"docstring": "\"\"\" Yields the synset that is the semantic parent, for example:\n    synsets(\"train\")[0].hypernym => Synset(\"public transport\").\n\"\"\"\n", "func_signal": "def hypernym(self):\n", "code": "p = self.hypernyms()\nreturn len(p) > 0 and p[0] or None", "path": "pattern/pattern/text/en/wordnet/__init__.py", "commit_date": "2017-08-24 00:00:00", "repo_name": "clips/pattern", "stars": 8637, "license": "bsd-3-clause", "language": "python", "size": 52274}
{"docstring": "\"\"\" Yields the part-of-speech tag (NOUN, VERB, ADJECTIVE or ADVERB).\n\"\"\"\n", "func_signal": "def pos(self):\n", "code": "pos = self._wnsynset.pos()\nif pos == wn.NOUN:\n    return NOUN\nif pos == wn.VERB:\n    return VERB\nif pos == wn.ADJ or pos == wn.ADJ_SAT:\n    return ADJECTIVE\nif pos == wn.ADV:\n    return ADVERB", "path": "pattern/pattern/text/en/wordnet/__init__.py", "commit_date": "2017-08-24 00:00:00", "repo_name": "clips/pattern", "stars": 8637, "license": "bsd-3-clause", "language": "python", "size": 52274}
{"docstring": "# Assert parsed output from the command-line (example from the documentation).\n", "func_signal": "def test_command_line(self):\n", "code": "p = [\"python\", \"-m\", \"pattern.de\", \"-s\", \"Der grosse Hund.\", \"-OTCRL\"]\np = subprocess.Popen(p, stdout=subprocess.PIPE)\np.wait()\nv = p.stdout.read().decode('utf-8')\nv = v.strip()\nself.assertEqual(v, \"Der/DT/B-NP/O/O/der grosse/JJ/I-NP/O/O/gross Hund/NN/I-NP/O/O/hund ././O/O/O/.\")\nprint(\"python -m pattern.de\")", "path": "pattern/test/test_de.py", "commit_date": "2017-08-24 00:00:00", "repo_name": "clips/pattern", "stars": 8637, "license": "bsd-3-clause", "language": "python", "size": 52274}
{"docstring": "\"\"\" Yields a list of synsets that are semantic members/parts of this synset, for example:\n    synsets(\"house\")[0].meronyms() =>\n    [Synset(\"library\"),\n     Synset(\"loft\"),\n     Synset(\"porch\")\n    ]\n\"\"\"\n", "func_signal": "def meronyms(self):\n", "code": "p = self._wnsynset.member_meronyms()\np += self._wnsynset.part_meronyms()\nreturn [Synset(p) for p in p]", "path": "pattern/pattern/text/en/wordnet/__init__.py", "commit_date": "2017-08-24 00:00:00", "repo_name": "clips/pattern", "stars": 8637, "license": "bsd-3-clause", "language": "python", "size": 52274}
{"docstring": "# Assert all inflections of \"sein\".\n", "func_signal": "def test_lexeme(self):\n", "code": "v = de.lexeme(\"sein\")\nself.assertEqual(v, [\n    \"sein\", \"bin\", \"bist\", \"ist\", \"sind\", \"seid\", \"seiend\",\n    \"war\", \"warst\", \"waren\", \"wart\", \"gewesen\",\n    \"sei\", \"seien\", \"seiest\", \"seiet\",\n    \"w\u00e4re\", \"w\u00e4rest\", \"w\u00e4ren\", \"w\u00e4ret\"\n])\nprint(\"pattern.de.inflect.lexeme()\")", "path": "pattern/test/test_de.py", "commit_date": "2017-08-24 00:00:00", "repo_name": "clips/pattern", "stars": 8637, "license": "bsd-3-clause", "language": "python", "size": 52274}
{"docstring": "\"\"\" Yields a list of synsets of which this synset is a member/part, for example:\n    synsets(\"tree\")[0].holonyms() => Synset(\"forest\").\n\"\"\"\n", "func_signal": "def holonyms(self):\n", "code": "p = self._wnsynset.member_holonyms()\np += self._wnsynset.part_holonyms()\nreturn [Synset(p) for p in p]", "path": "pattern/pattern/text/en/wordnet/__init__.py", "commit_date": "2017-08-24 00:00:00", "repo_name": "clips/pattern", "stars": 8637, "license": "bsd-3-clause", "language": "python", "size": 52274}
{"docstring": "\"\"\" Yields a list of semantically more specific synsets, for example:\n    synsets(\"train\")[0].hyponyms() =>\n    [Synset(\"boat train\"),\n     Synset(\"car train\"),\n     Synset(\"freight train\"),\n     Synset(\"hospital train\"),\n     Synset(\"mail train\"),\n     Synset(\"passenger train\"),\n     Synset(\"streamliner\"),\n     Synset(\"subway train\")\n    ]\n\"\"\"\n", "func_signal": "def hyponyms(self, recursive=False, depth=None):\n", "code": "p = [Synset(p) for p in self._wnsynset.hyponyms()]\nif depth is None and recursive is False:\n    return p\nif depth == 0:\n    return []\nif depth is not None:\n    depth -= 1\nif depth is None or depth > 0:\n    [p.extend(s.hyponyms(True, depth)) for s in list(p)]\nreturn p", "path": "pattern/pattern/text/en/wordnet/__init__.py", "commit_date": "2017-08-24 00:00:00", "repo_name": "clips/pattern", "stars": 8637, "license": "bsd-3-clause", "language": "python", "size": 52274}
{"docstring": "# Assert parsed output with Penn Treebank II tags (slash-formatted).\n# 1) \"der gro\u00dfe Hund\" is a noun phrase, \"auf der Matte\" is a prepositional noun phrase.\n", "func_signal": "def test_parse(self):\n", "code": "v = de.parser.parse(\"Der gro\u00dfe Hund sitzt auf der Matte.\")\nself.assertEqual(v,\n    \"Der/DT/B-NP/O gro\u00dfe/JJ/I-NP/O Hund/NN/I-NP/O \" + \\\n    \"sitzt/VB/B-VP/O \" + \\\n    \"auf/IN/B-PP/B-PNP der/DT/B-NP/I-PNP Matte/NN/I-NP/I-PNP ././O/O\"\n)\n# 2) \"gro\u00dfe\" and \"sitzt\" lemmata are \"gro\u00df\" and \"sitzen\".\n# Note how articles are problematic (\"der\" can be male subject but also plural possessive).\nv = de.parser.parse(\"Der gro\u00dfe Hund sitzt auf der Matte.\", lemmata=True)\nself.assertEqual(v,\n    \"Der/DT/B-NP/O/der gro\u00dfe/JJ/I-NP/O/gro\u00df Hund/NN/I-NP/O/hund \" + \\\n    \"sitzt/VB/B-VP/O/sitzen \" + \\\n    \"auf/IN/B-PP/B-PNP/auf der/DT/B-NP/I-PNP/der Matte/NN/I-NP/I-PNP/matte ././O/O/.\"\n)\n# 3) Assert the accuracy of the German tagger.\ni, n = 0, 0\nfor sentence in open(os.path.join(PATH, \"corpora\", \"tagged-de-tiger.txt\")).readlines():\n    sentence = sentence.strip()\n    s1 = [w.split(\"/\") for w in sentence.split(\" \")]\n    s1 = [de.stts2penntreebank(w, pos) for w, pos in s1]\n    s2 = [[w for w, pos in s1]]\n    s2 = de.parse(s2, tokenize=False)\n    s2 = [w.split(\"/\") for w in s2.split(\" \")]\n    for j in range(len(s1)):\n        if s1[j][1] == s2[j][1]:\n            i += 1\n        n += 1\nself.assertTrue(float(i) / n > 0.844)\nprint(\"pattern.de.parse()\")", "path": "pattern/test/test_de.py", "commit_date": "2017-08-24 00:00:00", "repo_name": "clips/pattern", "stars": 8637, "license": "bsd-3-clause", "language": "python", "size": 52274}
{"docstring": "\"\"\"Fast R-CNN blob names.\"\"\"\n# rois blob: holds R regions of interest, each is a 5-tuple\n# (batch_idx, x1, y1, x2, y2) specifying an image batch index and a\n# rectangle (x1, y1, x2, y2)\n", "func_signal": "def get_fast_rcnn_blob_names(is_training=True):\n", "code": "blob_names = ['rois']\nif is_training:\n    # labels_int32 blob: R categorical labels in [0, ..., K] for K\n    # foreground classes plus background\n    blob_names += ['labels_int32']\nif is_training:\n    # bbox_targets blob: R bounding-box regression targets with 4\n    # targets per class\n    blob_names += ['bbox_targets']\n    # bbox_inside_weights blob: At most 4 targets per roi are active\n    # this binary vector sepcifies the subset of active targets\n    blob_names += ['bbox_inside_weights']\n    blob_names += ['bbox_outside_weights']\nif is_training and cfg.MODEL.MASK_ON:\n    # 'mask_rois': RoIs sampled for training the mask prediction branch.\n    # Shape is (#masks, 5) in format (batch_idx, x1, y1, x2, y2).\n    blob_names += ['mask_rois']\n    # 'roi_has_mask': binary labels for the RoIs specified in 'rois'\n    # indicating if each RoI has a mask or not. Note that in some cases\n    # a *bg* RoI will have an all -1 (ignore) mask associated with it in\n    # the case that no fg RoIs can be sampled. Shape is (batchsize).\n    blob_names += ['roi_has_mask_int32']\n    # 'masks_int32' holds binary masks for the RoIs specified in\n    # 'mask_rois'. Shape is (#fg, M * M) where M is the ground truth\n    # mask size.\n    blob_names += ['masks_int32']\nif is_training and cfg.MODEL.KEYPOINTS_ON:\n    # 'keypoint_rois': RoIs sampled for training the keypoint prediction\n    # branch. Shape is (#instances, 5) in format (batch_idx, x1, y1, x2,\n    # y2).\n    blob_names += ['keypoint_rois']\n    # 'keypoint_locations_int32': index of keypoint in\n    # KRCNN.HEATMAP_SIZE**2 sized array. Shape is (#instances). Used in\n    # SoftmaxWithLoss.\n    blob_names += ['keypoint_locations_int32']\n    # 'keypoint_weights': weight assigned to each target in\n    # 'keypoint_locations_int32'. Shape is (#instances). Used in\n    # SoftmaxWithLoss.\n    blob_names += ['keypoint_weights']\n    # 'keypoint_loss_normalizer': optional normalization factor to use if\n    # cfg.KRCNN.NORMALIZE_BY_VISIBLE_KEYPOINTS is False.\n    blob_names += ['keypoint_loss_normalizer']\nif cfg.FPN.FPN_ON and cfg.FPN.MULTILEVEL_ROIS:\n    # Support for FPN multi-level rois without bbox reg isn't\n    # implemented (... and may never be implemented)\n    k_max = cfg.FPN.ROI_MAX_LEVEL\n    k_min = cfg.FPN.ROI_MIN_LEVEL\n    # Same format as rois blob, but one per FPN level\n    for lvl in range(k_min, k_max + 1):\n        blob_names += ['rois_fpn' + str(lvl)]\n    blob_names += ['rois_idx_restore_int32']\n    if is_training:\n        if cfg.MODEL.MASK_ON:\n            for lvl in range(k_min, k_max + 1):\n                blob_names += ['mask_rois_fpn' + str(lvl)]\n            blob_names += ['mask_rois_idx_restore_int32']\n        if cfg.MODEL.KEYPOINTS_ON:\n            for lvl in range(k_min, k_max + 1):\n                blob_names += ['keypoint_rois_fpn' + str(lvl)]\n            blob_names += ['keypoint_rois_idx_restore_int32']\nreturn blob_names", "path": "Detectron/detectron/roi_data/fast_rcnn.py", "commit_date": "2018-05-23 00:00:00", "repo_name": "facebookresearch/Detectron", "stars": 26104, "license": "apache-2.0", "language": "python", "size": 4525}
{"docstring": "''' Returns None if no condition is satisfied '''\n", "func_signal": "def op_filter(**filter_args):\n", "code": "def actual_decorator(f):\n    @wraps(f)\n    def wrapper(op, **params):\n        if not filter_op(op, **filter_args):\n            return None\n        return f(op, **params)\n    return wrapper\nreturn actual_decorator", "path": "Detectron/detectron/utils/model_convert_utils.py", "commit_date": "2018-10-09 00:00:00", "repo_name": "facebookresearch/Detectron", "stars": 26104, "license": "apache-2.0", "language": "python", "size": 4525}
{"docstring": "\"\"\"\nGenerate all types of anchors for all fpn levels/scales/aspect ratios.\nThis function is called only once at the beginning of inference.\n\"\"\"\n", "func_signal": "def _create_cell_anchors():\n", "code": "k_max, k_min = cfg.FPN.RPN_MAX_LEVEL, cfg.FPN.RPN_MIN_LEVEL\nscales_per_octave = cfg.RETINANET.SCALES_PER_OCTAVE\naspect_ratios = cfg.RETINANET.ASPECT_RATIOS\nanchor_scale = cfg.RETINANET.ANCHOR_SCALE\nA = scales_per_octave * len(aspect_ratios)\nanchors = {}\nfor lvl in range(k_min, k_max + 1):\n    # create cell anchors array\n    stride = 2. ** lvl\n    cell_anchors = np.zeros((A, 4))\n    a = 0\n    for octave in range(scales_per_octave):\n        octave_scale = 2 ** (octave / float(scales_per_octave))\n        for aspect in aspect_ratios:\n            anchor_sizes = (stride * octave_scale * anchor_scale, )\n            anchor_aspect_ratios = (aspect, )\n            cell_anchors[a, :] = generate_anchors(\n                stride=stride, sizes=anchor_sizes,\n                aspect_ratios=anchor_aspect_ratios)\n            a += 1\n    anchors[lvl] = cell_anchors\nreturn anchors", "path": "Detectron/detectron/core/test_retinanet.py", "commit_date": "2018-09-25 00:00:00", "repo_name": "facebookresearch/Detectron", "stars": 26104, "license": "apache-2.0", "language": "python", "size": 4525}
{"docstring": "''' Generate an initialization net based on a blob dict '''\n", "func_signal": "def gen_init_net_from_blobs(blobs, blobs_to_use=None, excluded_blobs=None):\n", "code": "ret = caffe2_pb2.NetDef()\nif blobs_to_use is None:\n    blobs_to_use = {x for x in blobs}\nelse:\n    blobs_to_use = copy.deepcopy(blobs_to_use)\nif excluded_blobs is not None:\n    blobs_to_use = [x for x in blobs_to_use if x not in excluded_blobs]\nfor name in blobs_to_use:\n    blob = blobs[name]\n    if isinstance(blob, str):\n        print('Blob {} with type {} is not supported in generating init net,'\n              ' skipped.'.format(name, type(blob)))\n        continue\n    add_tensor(ret, name, blob)\n\nreturn ret", "path": "Detectron/detectron/utils/model_convert_utils.py", "commit_date": "2018-10-09 00:00:00", "repo_name": "facebookresearch/Detectron", "stars": 26104, "license": "apache-2.0", "language": "python", "size": 4525}
{"docstring": "''' model_func(test_image, check_blobs)\n'''\n", "func_signal": "def compare_model(model1_func, model2_func, test_image, check_blobs):\n", "code": "cb1, cb2 = check_blobs, check_blobs\nif isinstance(check_blobs, dict):\n    cb1 = check_blobs.keys()\n    cb2 = check_blobs.values()\nprint('Running the first model...')\nres1 = model1_func(test_image, check_blobs)\nprint('Running the second model...')\nres2 = model2_func(test_image, check_blobs)\nfor idx in range(len(cb1)):\n    print('Checking {} -> {}...'.format(cb1[idx], cb2[idx]))\n    n1, n2 = cb1[idx], cb2[idx]\n    r1 = res1[n1] if n1 in res1 else None\n    r2 = res2[n2] if n2 in res2 else None\n    assert r1 is not None or r2 is None, \\\n        \"Blob {} in model1 is None\".format(n1)\n    assert r2 is not None or r1 is None, \\\n        \"Blob {} in model2 is None\".format(n2)\n    assert r1.shape == r2.shape, \\\n        \"Blob {} and {} shape mismatched: {} vs {}\".format(\n            n1, n2, r1.shape, r2.shape)\n\n    np.testing.assert_array_almost_equal(\n        r1, r2, decimal=3,\n        err_msg='{} and {} not matched. Max diff: {}'.format(\n            n1, n2, np.amax(np.absolute(r1 - r2))))\n\nreturn True", "path": "Detectron/detectron/utils/model_convert_utils.py", "commit_date": "2018-10-09 00:00:00", "repo_name": "facebookresearch/Detectron", "stars": 26104, "license": "apache-2.0", "language": "python", "size": 4525}
{"docstring": "''' Run funcs one by one until func return is not None '''\n", "func_signal": "def op_func_chain(convert_func_list):\n", "code": "assert isinstance(convert_func_list, list)\n\ndef _chain(op):\n    for x in convert_func_list:\n        ret = x(op)\n        if ret is not None:\n            return ret\n    return None\n\nreturn _chain", "path": "Detectron/detectron/utils/model_convert_utils.py", "commit_date": "2018-10-09 00:00:00", "repo_name": "facebookresearch/Detectron", "stars": 26104, "license": "apache-2.0", "language": "python", "size": 4525}
{"docstring": "\"\"\"Check that our cython implementation of bounding box IoU overlap\nmatches the COCO API implementation.\n\"\"\"\n", "func_signal": "def test_cython_bbox_iou_against_coco_api_bbox_iou(self):\n", "code": "def _do_test(b1, b2):\n    # Compute IoU overlap with the cython implementation\n    cython_iou = box_utils.bbox_overlaps(b1, b2)\n    # Compute IoU overlap with the COCO API implementation\n    # (requires converting boxes from xyxy to xywh format)\n    xywh_b1 = box_utils.xyxy_to_xywh(b1)\n    xywh_b2 = box_utils.xyxy_to_xywh(b2)\n    not_crowd = [int(False)] * b2.shape[0]\n    coco_ious = COCOmask.iou(xywh_b1, xywh_b2, not_crowd)\n    # IoUs should be similar\n    np.testing.assert_array_almost_equal(\n        cython_iou, coco_ious, decimal=5\n    )\n\n# Test small boxes\nb1 = random_boxes([10, 10, 20, 20], 5, 10)\nb2 = random_boxes([10, 10, 20, 20], 5, 10)\n_do_test(b1, b2)\n\n# Test bigger boxes\nb1 = random_boxes([10, 10, 110, 20], 20, 10)\nb2 = random_boxes([10, 10, 110, 20], 20, 10)\n_do_test(b1, b2)", "path": "Detectron/detectron/tests/test_bbox_transform.py", "commit_date": "2018-05-07 00:00:00", "repo_name": "facebookresearch/Detectron", "stars": 26104, "license": "apache-2.0", "language": "python", "size": 4525}
{"docstring": "\"\"\"Add blobs needed for training Fast R-CNN style models.\"\"\"\n# Sample training RoIs from each image and append them to the blob lists\n", "func_signal": "def add_fast_rcnn_blobs(blobs, im_scales, roidb):\n", "code": "for im_i, entry in enumerate(roidb):\n    frcn_blobs = _sample_rois(entry, im_scales[im_i], im_i)\n    for k, v in frcn_blobs.items():\n        blobs[k].append(v)\n# Concat the training blob lists into tensors\nfor k, v in blobs.items():\n    if isinstance(v, list) and len(v) > 0:\n        blobs[k] = np.concatenate(v)\n# Add FPN multilevel training RoIs, if configured\nif cfg.FPN.FPN_ON and cfg.FPN.MULTILEVEL_ROIS:\n    _add_multilevel_rois(blobs)\n\n# Perform any final work and validity checks after the collating blobs for\n# all minibatch images\nvalid = True\nif cfg.MODEL.KEYPOINTS_ON:\n    valid = keypoint_rcnn_roi_data.finalize_keypoint_minibatch(blobs, valid)\n\nreturn valid", "path": "Detectron/detectron/roi_data/fast_rcnn.py", "commit_date": "2018-05-23 00:00:00", "repo_name": "facebookresearch/Detectron", "stars": 26104, "license": "apache-2.0", "language": "python", "size": 4525}
{"docstring": "\"\"\"Import custom ops.\"\"\"\n", "func_signal": "def import_custom_ops():\n", "code": "custom_ops_lib = envu.get_custom_ops_lib()\ndyndep.InitOpsLibrary(custom_ops_lib)", "path": "Detectron/detectron/utils/c2.py", "commit_date": "2018-10-01 00:00:00", "repo_name": "facebookresearch/Detectron", "stars": 26104, "license": "apache-2.0", "language": "python", "size": 4525}
{"docstring": "\"\"\"By default training RoIs are added for a single feature map level only.\nWhen using FPN, the RoIs must be distributed over different FPN levels\naccording the level assignment heuristic (see: modeling.FPN.\nmap_rois_to_fpn_levels).\n\"\"\"\n", "func_signal": "def _add_multilevel_rois(blobs):\n", "code": "lvl_min = cfg.FPN.ROI_MIN_LEVEL\nlvl_max = cfg.FPN.ROI_MAX_LEVEL\n\ndef _distribute_rois_over_fpn_levels(rois_blob_name):\n    \"\"\"Distribute rois over the different FPN levels.\"\"\"\n    # Get target level for each roi\n    # Recall blob rois are in (batch_idx, x1, y1, x2, y2) format, hence take\n    # the box coordinates from columns 1:5\n    target_lvls = fpn.map_rois_to_fpn_levels(\n        blobs[rois_blob_name][:, 1:5], lvl_min, lvl_max\n    )\n    # Add per FPN level roi blobs named like: <rois_blob_name>_fpn<lvl>\n    fpn.add_multilevel_roi_blobs(\n        blobs, rois_blob_name, blobs[rois_blob_name], target_lvls, lvl_min,\n        lvl_max\n    )\n\n_distribute_rois_over_fpn_levels('rois')\nif cfg.MODEL.MASK_ON:\n    _distribute_rois_over_fpn_levels('mask_rois')\nif cfg.MODEL.KEYPOINTS_ON:\n    _distribute_rois_over_fpn_levels('keypoint_rois')", "path": "Detectron/detectron/roi_data/fast_rcnn.py", "commit_date": "2018-05-23 00:00:00", "repo_name": "facebookresearch/Detectron", "stars": 26104, "license": "apache-2.0", "language": "python", "size": 4525}
{"docstring": "''' Get blobs in 'blob_names' in the default workspace,\n    get all blobs if blob_names is None '''\n", "func_signal": "def get_ws_blobs(blob_names=None):\n", "code": "blobs = {}\nif blob_names is None:\n    blob_names = workspace.Blobs()\nblobs = {x: workspace.FetchBlob(x) for x in blob_names}\n\nreturn blobs", "path": "Detectron/detectron/utils/model_convert_utils.py", "commit_date": "2018-10-09 00:00:00", "repo_name": "facebookresearch/Detectron", "stars": 26104, "license": "apache-2.0", "language": "python", "size": 4525}
{"docstring": "\"\"\"Import Detectron ops.\"\"\"\n", "func_signal": "def import_detectron_ops():\n", "code": "detectron_ops_lib = envu.get_detectron_ops_lib()\ndyndep.InitOpsLibrary(detectron_ops_lib)", "path": "Detectron/detectron/utils/c2.py", "commit_date": "2018-10-01 00:00:00", "repo_name": "facebookresearch/Detectron", "stars": 26104, "license": "apache-2.0", "language": "python", "size": 4525}
{"docstring": "\"\"\"Returns a new Net from the given Net (`net`) that includes only the ops\nafter removing the first `prefix_len` number of ops. The new Net is thus a\nsuffix of `net`. Blobs listed in `outputs` are registered as external output\nblobs.\n\"\"\"\n", "func_signal": "def SuffixNet(name, net, prefix_len, outputs):\n", "code": "outputs = BlobReferenceList(outputs)\nfor output in outputs:\n    assert net.BlobIsDefined(output)\nnew_net = net.Clone(name)\n\ndel new_net.Proto().op[:]\ndel new_net.Proto().external_input[:]\ndel new_net.Proto().external_output[:]\n\n# Add suffix ops\nnew_net.Proto().op.extend(net.Proto().op[prefix_len:])\n# Add external input blobs\n# Treat any undefined blobs as external inputs\ninput_names = [\n    i for op in new_net.Proto().op for i in op.input\n    if not new_net.BlobIsDefined(i)]\nnew_net.Proto().external_input.extend(input_names)\n# Add external output blobs\noutput_names = [str(o) for o in outputs]\nnew_net.Proto().external_output.extend(output_names)\nreturn new_net, [new_net.GetBlobRef(o) for o in output_names]", "path": "Detectron/detectron/utils/c2.py", "commit_date": "2018-10-01 00:00:00", "repo_name": "facebookresearch/Detectron", "stars": 26104, "license": "apache-2.0", "language": "python", "size": 4525}
{"docstring": "\"\"\"Create a CPU device scope.\"\"\"\n", "func_signal": "def CpuScope():\n", "code": "cpu_dev = core.DeviceOption(caffe2_pb2.CPU)\nwith core.DeviceScope(cpu_dev):\n    yield", "path": "Detectron/detectron/utils/c2.py", "commit_date": "2018-10-01 00:00:00", "repo_name": "facebookresearch/Detectron", "stars": 26104, "license": "apache-2.0", "language": "python", "size": 4525}
{"docstring": "\"\"\"Create a name scope for GPU device `gpu_id`.\"\"\"\n", "func_signal": "def GpuNameScope(gpu_id):\n", "code": "with core.NameScope('gpu_{:d}'.format(gpu_id)):\n    yield", "path": "Detectron/detectron/utils/c2.py", "commit_date": "2018-10-01 00:00:00", "repo_name": "facebookresearch/Detectron", "stars": 26104, "license": "apache-2.0", "language": "python", "size": 4525}
{"docstring": "\"\"\"Simulate the process of reading a ground-truth box from a dataset,\nmake predictions from proposals, convert the predictions back to the\ndataset format, and then use the COCO API to compute IoU overlap between\nthe gt box and the predictions. These should have IoU of 1.\n\"\"\"\n", "func_signal": "def test_bbox_dataset_to_prediction_roundtrip(self):\n", "code": "weights = (5, 5, 10, 10)\n# 1/ \"read\" a box from a dataset in the default (x1, y1, w, h) format\ngt_xywh_box = [10, 20, 100, 150]\n# 2/ convert it to our internal (x1, y1, x2, y2) format\ngt_xyxy_box = box_utils.xywh_to_xyxy(gt_xywh_box)\n# 3/ consider nearby proposal boxes\nprop_xyxy_boxes = random_boxes(gt_xyxy_box, 10, 10)\n# 4/ compute proposal-to-gt transformation deltas\ndeltas = box_utils.bbox_transform_inv(\n    prop_xyxy_boxes, np.array([gt_xyxy_box]), weights=weights\n)\n# 5/ use deltas to transform proposals to xyxy predicted box\npred_xyxy_boxes = box_utils.bbox_transform(\n    prop_xyxy_boxes, deltas, weights=weights\n)\n# 6/ convert xyxy predicted box to xywh predicted box\npred_xywh_boxes = box_utils.xyxy_to_xywh(pred_xyxy_boxes)\n# 7/ use COCO API to compute IoU\nnot_crowd = [int(False)] * pred_xywh_boxes.shape[0]\nious = COCOmask.iou(pred_xywh_boxes, np.array([gt_xywh_box]), not_crowd)\nnp.testing.assert_array_almost_equal(ious, np.ones(ious.shape))", "path": "Detectron/detectron/tests/test_bbox_transform.py", "commit_date": "2018-05-07 00:00:00", "repo_name": "facebookresearch/Detectron", "stars": 26104, "license": "apache-2.0", "language": "python", "size": 4525}
{"docstring": "\"\"\"Create a CUDA device scope for GPU device `gpu_id`.\"\"\"\n", "func_signal": "def CudaScope(gpu_id):\n", "code": "gpu_dev = CudaDevice(gpu_id)\nwith core.DeviceScope(gpu_dev):\n    yield", "path": "Detectron/detectron/utils/c2.py", "commit_date": "2018-10-01 00:00:00", "repo_name": "facebookresearch/Detectron", "stars": 26104, "license": "apache-2.0", "language": "python", "size": 4525}
{"docstring": "\"\"\"Bounding-box regression targets are stored in a compact form in the\nroidb.\n\nThis function expands those targets into the 4-of-4*K representation used\nby the network (i.e. only one class has non-zero targets). The loss weights\nare similarly expanded.\n\nReturns:\n    bbox_target_data (ndarray): N x 4K blob of regression targets\n    bbox_inside_weights (ndarray): N x 4K blob of loss weights\n\"\"\"\n", "func_signal": "def _expand_bbox_targets(bbox_target_data):\n", "code": "num_bbox_reg_classes = cfg.MODEL.NUM_CLASSES\nif cfg.MODEL.CLS_AGNOSTIC_BBOX_REG:\n    num_bbox_reg_classes = 2  # bg and fg\n\nclss = bbox_target_data[:, 0]\nbbox_targets = blob_utils.zeros((clss.size, 4 * num_bbox_reg_classes))\nbbox_inside_weights = blob_utils.zeros(bbox_targets.shape)\ninds = np.where(clss > 0)[0]\nfor ind in inds:\n    cls = int(clss[ind])\n    start = 4 * cls\n    end = start + 4\n    bbox_targets[ind, start:end] = bbox_target_data[ind, 1:]\n    bbox_inside_weights[ind, start:end] = (1.0, 1.0, 1.0, 1.0)\nreturn bbox_targets, bbox_inside_weights", "path": "Detectron/detectron/roi_data/fast_rcnn.py", "commit_date": "2018-05-23 00:00:00", "repo_name": "facebookresearch/Detectron", "stars": 26104, "license": "apache-2.0", "language": "python", "size": 4525}
{"docstring": "# Run until we hit a fixed point\n", "func_signal": "def fuse_affine(net, params, ignore_failure):\n", "code": "removed_tensors = []\nwhile True:\n    (next_net, next_params, removed_tensors) = \\\n        fuse_first_affine(net, params, removed_tensors)\n    if len(next_net.op) == len(net.op):\n        if (\n            any(op.type == \"AffineChannel\" for op in next_net.op) and\n            not ignore_failure\n        ):\n            raise Exception(\n                \"Model contains AffineChannel op after fusion: %s\", next_net)\n        return (next_net, next_params, removed_tensors)\n    net, params, removed_tensors = (next_net, next_params, removed_tensors)", "path": "Detectron/detectron/utils/model_convert_utils.py", "commit_date": "2018-10-09 00:00:00", "repo_name": "facebookresearch/Detectron", "stars": 26104, "license": "apache-2.0", "language": "python", "size": 4525}
{"docstring": "\"\"\"Generate a random sample of RoIs comprising foreground and background\nexamples.\n\"\"\"\n", "func_signal": "def _sample_rois(roidb, im_scale, batch_idx):\n", "code": "rois_per_image = int(cfg.TRAIN.BATCH_SIZE_PER_IM)\nfg_rois_per_image = int(np.round(cfg.TRAIN.FG_FRACTION * rois_per_image))\nmax_overlaps = roidb['max_overlaps']\n\n# Select foreground RoIs as those with >= FG_THRESH overlap\nfg_inds = np.where(max_overlaps >= cfg.TRAIN.FG_THRESH)[0]\n# Guard against the case when an image has fewer than fg_rois_per_image\n# foreground RoIs\nfg_rois_per_this_image = np.minimum(fg_rois_per_image, fg_inds.size)\n# Sample foreground regions without replacement\nif fg_inds.size > 0:\n    fg_inds = npr.choice(\n        fg_inds, size=fg_rois_per_this_image, replace=False\n    )\n\n# Select background RoIs as those within [BG_THRESH_LO, BG_THRESH_HI)\nbg_inds = np.where(\n    (max_overlaps < cfg.TRAIN.BG_THRESH_HI) &\n    (max_overlaps >= cfg.TRAIN.BG_THRESH_LO)\n)[0]\n# Compute number of background RoIs to take from this image (guarding\n# against there being fewer than desired)\nbg_rois_per_this_image = rois_per_image - fg_rois_per_this_image\nbg_rois_per_this_image = np.minimum(bg_rois_per_this_image, bg_inds.size)\n# Sample foreground regions without replacement\nif bg_inds.size > 0:\n    bg_inds = npr.choice(\n        bg_inds, size=bg_rois_per_this_image, replace=False\n    )\n\n# The indices that we're selecting (both fg and bg)\nkeep_inds = np.append(fg_inds, bg_inds)\n# Label is the class each RoI has max overlap with\nsampled_labels = roidb['max_classes'][keep_inds]\nsampled_labels[fg_rois_per_this_image:] = 0  # Label bg RoIs with class 0\nsampled_boxes = roidb['boxes'][keep_inds]\n\nbbox_targets, bbox_inside_weights = _expand_bbox_targets(\n    roidb['bbox_targets'][keep_inds, :]\n)\nbbox_outside_weights = np.array(\n    bbox_inside_weights > 0, dtype=bbox_inside_weights.dtype\n)\n\n# Scale rois and format as (batch_idx, x1, y1, x2, y2)\nsampled_rois = sampled_boxes * im_scale\nrepeated_batch_idx = batch_idx * blob_utils.ones((sampled_rois.shape[0], 1))\nsampled_rois = np.hstack((repeated_batch_idx, sampled_rois))\n\n# Base Fast R-CNN blobs\nblob_dict = dict(\n    labels_int32=sampled_labels.astype(np.int32, copy=False),\n    rois=sampled_rois,\n    bbox_targets=bbox_targets,\n    bbox_inside_weights=bbox_inside_weights,\n    bbox_outside_weights=bbox_outside_weights\n)\n\n# Optionally add Mask R-CNN blobs\nif cfg.MODEL.MASK_ON:\n    mask_rcnn_roi_data.add_mask_rcnn_blobs(\n        blob_dict, sampled_boxes, roidb, im_scale, batch_idx\n    )\n\n# Optionally add Keypoint R-CNN blobs\nif cfg.MODEL.KEYPOINTS_ON:\n    keypoint_rcnn_roi_data.add_keypoint_rcnn_blobs(\n        blob_dict, roidb, fg_rois_per_image, fg_inds, im_scale, batch_idx\n    )\n\nreturn blob_dict", "path": "Detectron/detectron/roi_data/fast_rcnn.py", "commit_date": "2018-05-23 00:00:00", "repo_name": "facebookresearch/Detectron", "stars": 26104, "license": "apache-2.0", "language": "python", "size": 4525}
{"docstring": "\"\"\"\nTree structure and array storage:\n\nTree index:\n     0         -> storing priority sum\n    / \\\n  1     2\n / \\   / \\\n3   4 5   6    -> storing priority for transitions\n\nArray type for storing:\n[0,1,2,3,4,5,6]\n\"\"\"\n", "func_signal": "def _retrieve(self, lower_bound, parent_idx=0):\n", "code": "left_child_idx = 2 * parent_idx + 1\nright_child_idx = left_child_idx + 1\n\nif left_child_idx >= len(self.tree):  # end search when no more child\n    return parent_idx\n\nif self.tree[left_child_idx] == self.tree[right_child_idx]:\n    return self._retrieve(lower_bound, np.random.choice([left_child_idx, right_child_idx]))\nif lower_bound <= self.tree[left_child_idx]:  # downward search, always search for a higher priority node\n    return self._retrieve(lower_bound, left_child_idx)\nelse:\n    return self._retrieve(lower_bound - self.tree[left_child_idx], right_child_idx)", "path": "Reinforcement-learning-with-tensorflow/experiments/Solve_LunarLander/DuelingDQNPrioritizedReplay.py", "commit_date": "2017-05-06 00:00:00", "repo_name": "MorvanZhou/Reinforcement-learning-with-tensorflow", "stars": 8551, "license": "mit", "language": "python", "size": 438}
{"docstring": "# ------------------ build evaluate_net ------------------\n", "func_signal": "def _build_net(self):\n", "code": "self.s = tf.placeholder(tf.float32, [None, self.n_features], name='s')  # input\nself.q_target = tf.placeholder(tf.float32, [None, self.n_actions], name='Q_target')  # for calculating loss\nwith tf.variable_scope('eval_net'):\n    # c_names(collections_names) are the collections to store variables\n    c_names, n_l1, w_initializer, b_initializer = \\\n        ['eval_net_params', tf.GraphKeys.GLOBAL_VARIABLES], 10, \\\n        tf.random_normal_initializer(0., 0.3), tf.constant_initializer(0.1)  # config of layers\n\n    # first layer. collections is used later when assign to target net\n    with tf.variable_scope('l1'):\n        w1 = tf.get_variable('w1', [self.n_features, n_l1], initializer=w_initializer, collections=c_names)\n        b1 = tf.get_variable('b1', [1, n_l1], initializer=b_initializer, collections=c_names)\n        l1 = tf.nn.relu(tf.matmul(self.s, w1) + b1)\n\n    # second layer. collections is used later when assign to target net\n    with tf.variable_scope('l2'):\n        w2 = tf.get_variable('w2', [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)\n        b2 = tf.get_variable('b2', [1, self.n_actions], initializer=b_initializer, collections=c_names)\n        self.q_eval = tf.matmul(l1, w2) + b2\n\nwith tf.variable_scope('loss'):\n    self.loss = tf.reduce_mean(tf.squared_difference(self.q_target, self.q_eval))\nwith tf.variable_scope('train'):\n    self._train_op = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss)\n\n# ------------------ build target_net ------------------\nself.s_ = tf.placeholder(tf.float32, [None, self.n_features], name='s_')    # input\nwith tf.variable_scope('target_net'):\n    # c_names(collections_names) are the collections to store variables\n    c_names = ['target_net_params', tf.GraphKeys.GLOBAL_VARIABLES]\n\n    # first layer. collections is used later when assign to target net\n    with tf.variable_scope('l1'):\n        w1 = tf.get_variable('w1', [self.n_features, n_l1], initializer=w_initializer, collections=c_names)\n        b1 = tf.get_variable('b1', [1, n_l1], initializer=b_initializer, collections=c_names)\n        l1 = tf.nn.relu(tf.matmul(self.s_, w1) + b1)\n\n    # second layer. collections is used later when assign to target net\n    with tf.variable_scope('l2'):\n        w2 = tf.get_variable('w2', [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)\n        b2 = tf.get_variable('b2', [1, self.n_actions], initializer=b_initializer, collections=c_names)\n        self.q_next = tf.matmul(l1, w2) + b2", "path": "Reinforcement-learning-with-tensorflow/contents/6_OpenAI_gym/RL_brain.py", "commit_date": "2017-10-28 00:00:00", "repo_name": "MorvanZhou/Reinforcement-learning-with-tensorflow", "stars": 8551, "license": "mit", "language": "python", "size": 438}
{"docstring": "# check to replace target parameters\n", "func_signal": "def learn(self):\n", "code": "if self.learn_step_counter % self.replace_target_iter == 0:\n    self.sess.run(self.target_replace_op)\n\n# sample batch memory from all memory\ntop = self.memory_size if self.memory_counter > self.memory_size else self.memory_counter\nsample_index = np.random.choice(top, size=self.batch_size)\nbatch_memory = self.memory[sample_index, :]\n\nbs, ba, br, bs_ = batch_memory[:, :self.n_s], batch_memory[:, self.n_s], \\\n    batch_memory[:, self.n_s + 1], batch_memory[:, -self.n_s:]\nself.sess.run(self.dqn_train, feed_dict={self.tfs: bs, self.tfa: ba, self.tfr: br, self.tfs_: bs_})\nif self.learn_step_counter % 1000 == 0:     # delay training in order to stay curious\n    self.sess.run(self.dyn_train, feed_dict={self.tfs: bs, self.tfa: ba, self.tfs_: bs_})\nself.learn_step_counter += 1", "path": "Reinforcement-learning-with-tensorflow/contents/Curiosity_Model/Curiosity.py", "commit_date": "2019-01-21 00:00:00", "repo_name": "MorvanZhou/Reinforcement-learning-with-tensorflow", "stars": 8551, "license": "mit", "language": "python", "size": 438}
{"docstring": "# This is how environment be updated\n", "func_signal": "def update_env(S, episode, step_counter):\n", "code": "env_list = ['-']*(N_STATES-1) + ['T']   # '---------T' our environment\nif S == 'terminal':\n    interaction = 'Episode %s: total_steps = %s' % (episode+1, step_counter)\n    print('\\r{}'.format(interaction), end='')\n    time.sleep(2)\n    print('\\r                                ', end='')\nelse:\n    env_list[S] = 'o'\n    interaction = ''.join(env_list)\n    print('\\r{}'.format(interaction), end='')\n    time.sleep(FRESH_TIME)", "path": "Reinforcement-learning-with-tensorflow/contents/1_command_line_reinforcement_learning/treasure_on_right.py", "commit_date": "2018-01-27 00:00:00", "repo_name": "MorvanZhou/Reinforcement-learning-with-tensorflow", "stars": 8551, "license": "mit", "language": "python", "size": 438}
{"docstring": "# This is how to choose an action\n", "func_signal": "def choose_action(state, q_table):\n", "code": "state_actions = q_table.iloc[state, :]\nif (np.random.uniform() > EPSILON) or ((state_actions == 0).all()):  # act non-greedy or state-action have no value\n    action_name = np.random.choice(ACTIONS)\nelse:   # act greedy\n    action_name = state_actions.idxmax()    # replace argmax to idxmax as argmax means a different function in newer version of pandas\nreturn action_name", "path": "Reinforcement-learning-with-tensorflow/contents/1_command_line_reinforcement_learning/treasure_on_right.py", "commit_date": "2018-01-27 00:00:00", "repo_name": "MorvanZhou/Reinforcement-learning-with-tensorflow", "stars": 8551, "license": "mit", "language": "python", "size": 438}
{"docstring": "# to have batch dimension when feed into tf placeholder\n", "func_signal": "def choose_action(self, observation):\n", "code": "observation = observation[np.newaxis, :]\n\nif np.random.uniform() < self.epsilon:\n    # forward feed the observation and get q value for every actions\n    actions_value = self.sess.run(self.q_eval, feed_dict={self.s: observation})\n    action = np.argmax(actions_value)\nelse:\n    action = np.random.randint(0, self.n_actions)\nreturn action", "path": "Reinforcement-learning-with-tensorflow/contents/6_OpenAI_gym/RL_brain.py", "commit_date": "2017-10-28 00:00:00", "repo_name": "MorvanZhou/Reinforcement-learning-with-tensorflow", "stars": 8551, "license": "mit", "language": "python", "size": 438}
{"docstring": "# discount and normalize episode reward\n", "func_signal": "def learn(self):\n", "code": "discounted_ep_rs_norm = self._discount_and_norm_rewards()\n\n# train on episode\nself.sess.run(self.train_op, feed_dict={\n     self.tf_obs: np.vstack(self.ep_obs),  # shape=[None, n_obs]\n     self.tf_acts: np.array(self.ep_as),  # shape=[None, ]\n     self.tf_vt: discounted_ep_rs_norm,  # shape=[None, ]\n})\n\nself.ep_obs, self.ep_as, self.ep_rs = [], [], []    # empty episode data\nreturn discounted_ep_rs_norm", "path": "Reinforcement-learning-with-tensorflow/contents/7_Policy_gradient_softmax/RL_brain.py", "commit_date": "2017-05-06 00:00:00", "repo_name": "MorvanZhou/Reinforcement-learning-with-tensorflow", "stars": 8551, "license": "mit", "language": "python", "size": 438}
{"docstring": "\"\"\"change the sum of priority value in all parent nodes\"\"\"\n", "func_signal": "def _propagate_change(self, tree_idx, change):\n", "code": "parent_idx = (tree_idx - 1) // 2\nself.tree[parent_idx] += change\nif parent_idx != 0:\n    self._propagate_change(parent_idx, change)", "path": "Reinforcement-learning-with-tensorflow/experiments/Solve_LunarLander/DuelingDQNPrioritizedReplay.py", "commit_date": "2017-05-06 00:00:00", "repo_name": "MorvanZhou/Reinforcement-learning-with-tensorflow", "stars": 8551, "license": "mit", "language": "python", "size": 438}
{"docstring": "# check to replace target parameters\n", "func_signal": "def learn(self):\n", "code": "if self.learn_step_counter % self.replace_target_iter == 0:\n    self.sess.run(self.replace_target_op)\n    print('\\ntarget_params_replaced\\n')\n\n# sample batch memory from all memory\nif self.memory_counter > self.memory_size:\n    sample_index = np.random.choice(self.memory_size, size=self.batch_size)\nelse:\n    sample_index = np.random.choice(self.memory_counter, size=self.batch_size)\nbatch_memory = self.memory[sample_index, :]\n\nq_next, q_eval = self.sess.run(\n    [self.q_next, self.q_eval],\n    feed_dict={\n        self.s_: batch_memory[:, -self.n_features:],  # fixed params\n        self.s: batch_memory[:, :self.n_features],  # newest params\n    })\n\n# change q_target w.r.t q_eval's action\nq_target = q_eval.copy()\n\nbatch_index = np.arange(self.batch_size, dtype=np.int32)\neval_act_index = batch_memory[:, self.n_features].astype(int)\nreward = batch_memory[:, self.n_features + 1]\n\nq_target[batch_index, eval_act_index] = reward + self.gamma * np.max(q_next, axis=1)\n\n\"\"\"\nFor example in this batch I have 2 samples and 3 actions:\nq_eval =\n[[1, 2, 3],\n [4, 5, 6]]\n\nq_target = q_eval =\n[[1, 2, 3],\n [4, 5, 6]]\n\nThen change q_target with the real q_target value w.r.t the q_eval's action.\nFor example in:\n    sample 0, I took action 0, and the max q_target value is -1;\n    sample 1, I took action 2, and the max q_target value is -2:\nq_target =\n[[-1, 2, 3],\n [4, 5, -2]]\n\nSo the (q_target - q_eval) becomes:\n[[(-1)-(1), 0, 0],\n [0, 0, (-2)-(6)]]\n\nWe then backpropagate this error w.r.t the corresponding action to network,\nleave other action as error=0 cause we didn't choose it.\n\"\"\"\n\n# train eval network\n_, self.cost = self.sess.run([self._train_op, self.loss],\n                             feed_dict={self.s: batch_memory[:, :self.n_features],\n                                        self.q_target: q_target})\nself.cost_his.append(self.cost)\n\n# increasing epsilon\nself.epsilon = self.epsilon + self.epsilon_increment if self.epsilon < self.epsilon_max else self.epsilon_max\nself.learn_step_counter += 1", "path": "Reinforcement-learning-with-tensorflow/contents/6_OpenAI_gym/RL_brain.py", "commit_date": "2017-10-28 00:00:00", "repo_name": "MorvanZhou/Reinforcement-learning-with-tensorflow", "stars": 8551, "license": "mit", "language": "python", "size": 438}
{"docstring": "# discount episode rewards\n", "func_signal": "def _discount_and_norm_rewards(self):\n", "code": "discounted_ep_rs = np.zeros_like(self.ep_rs)\nrunning_add = 0\nfor t in reversed(range(0, len(self.ep_rs))):\n    running_add = running_add * self.gamma + self.ep_rs[t]\n    discounted_ep_rs[t] = running_add\n\n# normalize episode rewards\ndiscounted_ep_rs -= np.mean(discounted_ep_rs)\ndiscounted_ep_rs /= np.std(discounted_ep_rs)\nreturn discounted_ep_rs", "path": "Reinforcement-learning-with-tensorflow/contents/7_Policy_gradient_softmax/RL_brain.py", "commit_date": "2017-05-06 00:00:00", "repo_name": "MorvanZhou/Reinforcement-learning-with-tensorflow", "stars": 8551, "license": "mit", "language": "python", "size": 438}
{"docstring": "\"\"\"change the sum of priority value in all parent nodes\"\"\"\n", "func_signal": "def _propagate_change(self, tree_idx, change):\n", "code": "parent_idx = (tree_idx - 1) // 2\nself.tree[parent_idx] += change\nif parent_idx != 0:\n    self._propagate_change(parent_idx, change)", "path": "Reinforcement-learning-with-tensorflow/experiments/Solve_BipedalWalker/DDPG.py", "commit_date": "2017-07-21 00:00:00", "repo_name": "MorvanZhou/Reinforcement-learning-with-tensorflow", "stars": 8551, "license": "mit", "language": "python", "size": 438}
{"docstring": "# to have batch dimension when feed into tf placeholder\n", "func_signal": "def choose_action(self, observation):\n", "code": "s = observation[np.newaxis, :]\n\nif np.random.uniform() < self.epsilon:\n    # forward feed the observation and get q value for every actions\n    actions_value = self.sess.run(self.q, feed_dict={self.tfs: s})\n    action = np.argmax(actions_value)\nelse:\n    action = np.random.randint(0, self.n_a)\nreturn action", "path": "Reinforcement-learning-with-tensorflow/contents/Curiosity_Model/Random_Network_Distillation.py", "commit_date": "2019-01-21 00:00:00", "repo_name": "MorvanZhou/Reinforcement-learning-with-tensorflow", "stars": 8551, "license": "mit", "language": "python", "size": 438}
{"docstring": "# to have batch dimension when feed into tf placeholder\n", "func_signal": "def choose_action(self, observation):\n", "code": "s = observation[np.newaxis, :]\n\nif np.random.uniform() < self.epsilon:\n    # forward feed the observation and get q value for every actions\n    actions_value = self.sess.run(self.q, feed_dict={self.tfs: s})\n    action = np.argmax(actions_value)\nelse:\n    action = np.random.randint(0, self.n_a)\nreturn action", "path": "Reinforcement-learning-with-tensorflow/contents/Curiosity_Model/Curiosity.py", "commit_date": "2019-01-21 00:00:00", "repo_name": "MorvanZhou/Reinforcement-learning-with-tensorflow", "stars": 8551, "license": "mit", "language": "python", "size": 438}
{"docstring": "# check to replace target parameters\n", "func_signal": "def learn(self):\n", "code": "if self.learn_step_counter % self.replace_target_iter == 0:\n    self.sess.run(self.target_replace_op)\n\n# sample batch memory from all memory\ntop = self.memory_size if self.memory_counter > self.memory_size else self.memory_counter\nsample_index = np.random.choice(top, size=self.batch_size)\nbatch_memory = self.memory[sample_index, :]\n\nbs, ba, br, bs_ = batch_memory[:, :self.n_s], batch_memory[:, self.n_s], \\\n    batch_memory[:, self.n_s + 1], batch_memory[:, -self.n_s:]\nself.sess.run(self.dqn_train, feed_dict={self.tfs: bs, self.tfa: ba, self.tfr: br, self.tfs_: bs_})\nif self.learn_step_counter % 100 == 0:     # delay training in order to stay curious\n    self.sess.run(self.pred_train, feed_dict={self.tfs_: bs_})\nself.learn_step_counter += 1", "path": "Reinforcement-learning-with-tensorflow/contents/Curiosity_Model/Random_Network_Distillation.py", "commit_date": "2019-01-21 00:00:00", "repo_name": "MorvanZhou/Reinforcement-learning-with-tensorflow", "stars": 8551, "license": "mit", "language": "python", "size": 438}
{"docstring": "# main part of RL loop\n", "func_signal": "def rl():\n", "code": "q_table = build_q_table(N_STATES, ACTIONS)\nfor episode in range(MAX_EPISODES):\n    step_counter = 0\n    S = 0\n    is_terminated = False\n    update_env(S, episode, step_counter)\n    while not is_terminated:\n\n        A = choose_action(S, q_table)\n        S_, R = get_env_feedback(S, A)  # take action & get next state and reward\n        q_predict = q_table.loc[S, A]\n        if S_ != 'terminal':\n            q_target = R + GAMMA * q_table.iloc[S_, :].max()   # next state is not terminal\n        else:\n            q_target = R     # next state is terminal\n            is_terminated = True    # terminate this episode\n\n        q_table.loc[S, A] += ALPHA * (q_target - q_predict)  # update\n        S = S_  # move to next state\n\n        update_env(S, episode, step_counter+1)\n        step_counter += 1\nreturn q_table", "path": "Reinforcement-learning-with-tensorflow/contents/1_command_line_reinforcement_learning/treasure_on_right.py", "commit_date": "2018-01-27 00:00:00", "repo_name": "MorvanZhou/Reinforcement-learning-with-tensorflow", "stars": 8551, "license": "mit", "language": "python", "size": 438}
{"docstring": "# set work's ip:port\n", "func_signal": "def work(job_name, task_index, global_ep, lock, r_queue, global_running_r):\n", "code": "cluster = tf.train.ClusterSpec({\n    \"ps\": ['localhost:2220', 'localhost:2221',],\n    \"worker\": ['localhost:2222', 'localhost:2223', 'localhost:2224', 'localhost:2225',]\n})\nserver = tf.train.Server(cluster, job_name=job_name, task_index=task_index)\nif job_name == 'ps':\n    print('Start Parameter Sever: ', task_index)\n    server.join()\nelse:\n    t1 = time.time()\n    env = gym.make('CartPole-v0').unwrapped\n    print('Start Worker: ', task_index)\n    with tf.device(tf.train.replica_device_setter(\n            worker_device=\"/job:worker/task:%d\" % task_index,\n            cluster=cluster)):\n        opt_a = tf.train.RMSPropOptimizer(LR_A, name='opt_a')\n        opt_c = tf.train.RMSPropOptimizer(LR_C, name='opt_c')\n        global_net = ACNet('global_net')\n\n    local_net = ACNet('local_ac%d' % task_index, opt_a, opt_c, global_net)\n    # set training steps\n    hooks = [tf.train.StopAtStepHook(last_step=100000)]\n    with tf.train.MonitoredTrainingSession(master=server.target,\n                                           is_chief=True,\n                                           hooks=hooks,) as sess:\n        print('Start Worker Session: ', task_index)\n        local_net.sess = sess\n        total_step = 1\n        buffer_s, buffer_a, buffer_r = [], [], []\n        while (not sess.should_stop()) and (global_ep.value < 1000):\n            s = env.reset()\n            ep_r = 0\n            while True:\n                # if task_index:\n                #     env.render()\n                a = local_net.choose_action(s)\n                s_, r, done, info = env.step(a)\n                if done: r = -5.\n                ep_r += r\n                buffer_s.append(s)\n                buffer_a.append(a)\n                buffer_r.append(r)\n\n                if total_step % UPDATE_GLOBAL_ITER == 0 or done:  # update global and assign to local net\n                    if done:\n                        v_s_ = 0  # terminal\n                    else:\n                        v_s_ = sess.run(local_net.v, {local_net.s: s_[np.newaxis, :]})[0, 0]\n                    buffer_v_target = []\n                    for r in buffer_r[::-1]:  # reverse buffer r\n                        v_s_ = r + GAMMA * v_s_\n                        buffer_v_target.append(v_s_)\n                    buffer_v_target.reverse()\n\n                    buffer_s, buffer_a, buffer_v_target = np.vstack(buffer_s), np.array(buffer_a), np.vstack(\n                        buffer_v_target)\n                    feed_dict = {\n                        local_net.s: buffer_s,\n                        local_net.a_his: buffer_a,\n                        local_net.v_target: buffer_v_target,\n                    }\n                    local_net.update_global(feed_dict)\n                    buffer_s, buffer_a, buffer_r = [], [], []\n                    local_net.pull_global()\n                s = s_\n                total_step += 1\n                if done:\n                    if r_queue.empty():  # record running episode reward\n                        global_running_r.value = ep_r\n                    else:\n                        global_running_r.value = .99 * global_running_r.value + 0.01 * ep_r\n                    r_queue.put(global_running_r.value)\n\n                    print(\n                        \"Task: %i\" % task_index,\n                        \"| Ep: %i\" % global_ep.value,\n                        \"| Ep_r: %i\" % global_running_r.value,\n                        \"| Global_step: %i\" % sess.run(local_net.global_step),\n                    )\n                    with lock:\n                        global_ep.value += 1\n                    break\n\n    print('Worker Done: ', task_index, time.time()-t1)", "path": "Reinforcement-learning-with-tensorflow/contents/10_A3C/A3C_distributed_tf.py", "commit_date": "2018-01-18 00:00:00", "repo_name": "MorvanZhou/Reinforcement-learning-with-tensorflow", "stars": 8551, "license": "mit", "language": "python", "size": 438}
{"docstring": "# action = (node1 angular v, node2 angular v)\n", "func_signal": "def step(self, action):\n", "code": "action = np.clip(action, *self.action_bound)\nself.arm_info[:, 1] += action * self.dt\nself.arm_info[:, 1] %= np.pi * 2\n\narm1rad = self.arm_info[0, 1]\narm2rad = self.arm_info[1, 1]\narm1dx_dy = np.array([self.arm_info[0, 0] * np.cos(arm1rad), self.arm_info[0, 0] * np.sin(arm1rad)])\narm2dx_dy = np.array([self.arm_info[1, 0] * np.cos(arm2rad), self.arm_info[1, 0] * np.sin(arm2rad)])\nself.arm_info[0, 2:4] = self.center_coord + arm1dx_dy  # (x1, y1)\nself.arm_info[1, 2:4] = self.arm_info[0, 2:4] + arm2dx_dy  # (x2, y2)\n\ns, arm2_distance = self._get_state()\nr = self._r_func(arm2_distance)\n\nreturn s, r, self.get_point", "path": "Reinforcement-learning-with-tensorflow/experiments/Robot_arm/arm_env.py", "commit_date": "2017-05-06 00:00:00", "repo_name": "MorvanZhou/Reinforcement-learning-with-tensorflow", "stars": 8551, "license": "mit", "language": "python", "size": 438}
{"docstring": "# return the distance (dx, dy) between arm finger point with blue point\n", "func_signal": "def _get_state(self):\n", "code": "arm_end = self.arm_info[:, 2:4]\nt_arms = np.ravel(arm_end - self.point_info)\ncenter_dis = (self.center_coord - self.point_info)/200\nin_point = 1 if self.grab_counter > 0 else 0\nreturn np.hstack([in_point, t_arms/200, center_dis,\n                  # arm1_distance_p, arm1_distance_b,\n                  ]), t_arms[-2:]", "path": "Reinforcement-learning-with-tensorflow/experiments/Robot_arm/arm_env.py", "commit_date": "2017-05-06 00:00:00", "repo_name": "MorvanZhou/Reinforcement-learning-with-tensorflow", "stars": 8551, "license": "mit", "language": "python", "size": 438}
{"docstring": "\"\"\"\nTree structure and array storage:\n\nTree index:\n     0         -> storing priority sum\n    / \\\n  1     2\n / \\   / \\\n3   4 5   6    -> storing priority for transitions\n\nArray type for storing:\n[0,1,2,3,4,5,6]\n\"\"\"\n", "func_signal": "def _retrieve(self, lower_bound, parent_idx=0):\n", "code": "left_child_idx = 2 * parent_idx + 1\nright_child_idx = left_child_idx + 1\n\nif left_child_idx >= len(self.tree):  # end search when no more child\n    return parent_idx\n\nif self.tree[left_child_idx] == self.tree[right_child_idx]:\n    return self._retrieve(lower_bound, np.random.choice([left_child_idx, right_child_idx]))\nif lower_bound <= self.tree[left_child_idx]:  # downward search, always search for a higher priority node\n    return self._retrieve(lower_bound, left_child_idx)\nelse:\n    return self._retrieve(lower_bound - self.tree[left_child_idx], right_child_idx)", "path": "Reinforcement-learning-with-tensorflow/experiments/Solve_BipedalWalker/DDPG.py", "commit_date": "2017-07-21 00:00:00", "repo_name": "MorvanZhou/Reinforcement-learning-with-tensorflow", "stars": 8551, "license": "mit", "language": "python", "size": 438}
{"docstring": "# node1 (l, d_rad, x, y),\n# node2 (l, d_rad, x, y)\n", "func_signal": "def __init__(self, mode='easy'):\n", "code": "self.mode = mode\nself.arm_info = np.zeros((2, 4))\nself.arm_info[0, 0] = self.arm1l\nself.arm_info[1, 0] = self.arm2l\nself.point_info = np.array([250, 303])\nself.point_info_init = self.point_info.copy()\nself.center_coord = np.array(self.viewer_xy)/2", "path": "Reinforcement-learning-with-tensorflow/experiments/Robot_arm/arm_env.py", "commit_date": "2017-05-06 00:00:00", "repo_name": "MorvanZhou/Reinforcement-learning-with-tensorflow", "stars": 8551, "license": "mit", "language": "python", "size": 438}
{"docstring": "# pylint: disable=invalid-name\n", "func_signal": "def __init__(self, name):\n", "code": "self._h = self.HEIGHT  # top and bottom box edges + text\nself._w = len(name) + 2  # right and left bottom edges + text", "path": "dvc/dvc/dagascii.py", "commit_date": "2020-06-18 00:00:00", "repo_name": "iterative/dvc", "stars": 12952, "license": "apache-2.0", "language": "python", "size": 19502}
{"docstring": "\"\"\"Parses text from .py file into Python structure.\"\"\"\n", "func_signal": "def parse_py(text, path):\n", "code": "with reraise(SyntaxError, PythonFileCorruptedError(path)):\n    tree = ast.parse(text, filename=path)\n\nresult = _ast_tree_to_dict(tree)\nreturn result", "path": "dvc/dvc/utils/serialize/_py.py", "commit_date": "2020-09-24 00:00:00", "repo_name": "iterative/dvc", "stars": 12952, "license": "apache-2.0", "language": "python", "size": 19502}
{"docstring": "\"\"\"Print a text on ASCII canvas.\n\nArgs:\n    x (int): x coordinate where the text should start.\n    y (int): y coordinate where the text should start.\n    text (str): string that should be printed.\n\"\"\"\n", "func_signal": "def text(self, x, y, text):\n", "code": "for i, char in enumerate(text):\n    self.point(x + i, y, char)", "path": "dvc/dvc/dagascii.py", "commit_date": "2020-06-18 00:00:00", "repo_name": "iterative/dvc", "stars": 12952, "license": "apache-2.0", "language": "python", "size": 19502}
{"docstring": "\"\"\"\nRenames an output file and modifies the stage associated\nto reflect the change on the pipeline.\n\nIf the output has the same name as its stage, it would\nalso rename the corresponding DVC-file.\n\nE.g.\n      Having: (hello, hello.dvc)\n\n      $ dvc move hello greetings\n\n      Result: (greeting, greeting.dvc)\n\nIt only works with outputs generated by `add` or `import`,\nalso known as data sources.\n\"\"\"\n", "func_signal": "def move(self, from_path, to_path):\n", "code": "import dvc.output as Output\nfrom dvc.stage import Stage\n\nfrom ..dvcfile import DVC_FILE_SUFFIX, Dvcfile\n\nfrom_out = Output.loads_from(Stage(self), [from_path])[0]\nassert from_out.scheme == \"local\"\n\nto_path = _expand_target_path(from_path, to_path)\n\nouts = self.find_outs_by_path(from_out.fspath)\nassert len(outs) == 1\nout = outs[0]\nstage = out.stage\n\nif not stage.is_data_source:\n    raise MoveNotDataSourceError(stage.addressing)\n\nstage_name = os.path.splitext(os.path.basename(stage.path))[0]\nfrom_name = os.path.basename(from_out.fspath)\nif stage_name == from_name:\n    os.unlink(stage.path)\n\n    stage.path = os.path.join(\n        os.path.dirname(to_path),\n        os.path.basename(to_path) + DVC_FILE_SUFFIX,\n    )\n\n    stage.wdir = os.path.abspath(\n        os.path.join(os.curdir, os.path.dirname(to_path))\n    )\n\nto_path = os.path.relpath(to_path, stage.wdir)\n\nto_out = Output.loads_from(stage, [to_path], out.use_cache, out.metric)[0]\n\nout.move(to_out)\nstage.save()\n\nDvcfile(self, stage.path).dump(stage)", "path": "dvc/dvc/repo/move.py", "commit_date": "2020-08-17 00:00:00", "repo_name": "iterative/dvc", "stars": 12952, "license": "apache-2.0", "language": "python", "size": 19502}
{"docstring": "# patching to speedup tests\n", "func_signal": "def test_with(tmp_dir, dvc, mocker):\n", "code": "mocker.patch(\"dvc.lock.DEFAULT_TIMEOUT\", 0.01)\n\nlockfile = tmp_dir / dvc.tmp_dir / \"lock\"\nwith Lock(lockfile):\n    with pytest.raises(LockError), Lock(lockfile):\n        pass", "path": "dvc/tests/func/test_lock.py", "commit_date": "2020-10-19 00:00:00", "repo_name": "iterative/dvc", "stars": 12952, "license": "apache-2.0", "language": "python", "size": 19502}
{"docstring": "\"\"\"Parses ast trees to dict.\n\n:param tree: ast.Tree\n:param only_self_params: get only self params from class __init__ function\n:param lineno: add params line number (needed for update)\n:return:\n\"\"\"\n", "func_signal": "def _ast_tree_to_dict(tree, only_self_params=False, lineno=False):\n", "code": "result = {}\nfor _body in tree.body:\n    try:\n        if isinstance(_body, (ast.Assign, ast.AnnAssign)):\n            result.update(\n                _ast_assign_to_dict(_body, only_self_params, lineno)\n            )\n        elif isinstance(_body, ast.ClassDef):\n            result.update(\n                {_body.name: _ast_tree_to_dict(_body, lineno=lineno)}\n            )\n        elif (\n            isinstance(_body, ast.FunctionDef) and _body.name == \"__init__\"\n        ):\n            result.update(\n                _ast_tree_to_dict(\n                    _body, only_self_params=True, lineno=lineno\n                )\n            )\n    except ValueError:\n        continue\n    except AttributeError:\n        continue\nreturn result", "path": "dvc/dvc/utils/serialize/_py.py", "commit_date": "2020-09-24 00:00:00", "repo_name": "iterative/dvc", "stars": 12952, "license": "apache-2.0", "language": "python", "size": 19502}
{"docstring": "\"\"\"Create a point on ASCII canvas.\n\nArgs:\n    x (int): x coordinate. Should be >= 0 and < number of columns in\n        the canvas.\n    y (int): y coordinate. Should be >= 0 an < number of lines in the\n        canvas.\n    char (str): character to place in the specified point on the\n        canvas.\n\"\"\"\n", "func_signal": "def point(self, x, y, char):\n", "code": "assert len(char) == 1\nassert x >= 0\nassert x < self.cols\nassert y >= 0\nassert y < self.lines\n\nself.canvas[y][x] = char", "path": "dvc/dvc/dagascii.py", "commit_date": "2020-06-18 00:00:00", "repo_name": "iterative/dvc", "stars": 12952, "license": "apache-2.0", "language": "python", "size": 19502}
{"docstring": "# dict, so that we could delete from it easily\n", "func_signal": "def test_get_summary():\n", "code": "stats = OrderedDict(\n    [\n        (\"fetched\", 3),\n        (\"added\", [\"file1\", \"file2\", \"file3\"]),\n        (\"deleted\", [\"file4\", \"file5\"]),\n        (\"modified\", [\"file6\", \"file7\"]),\n    ]\n)\n\nassert get_summary(stats.items()) == (\n    \"3 files fetched, \"\n    \"3 files added, \"\n    \"2 files deleted \"\n    \"and \"\n    \"2 files modified\"\n)\n\ndel stats[\"fetched\"]\ndel stats[\"deleted\"][1]\nassert (\n    get_summary(stats.items())\n    == \"3 files added, 1 file deleted and 2 files modified\"\n)\n\ndel stats[\"deleted\"][0]\nassert get_summary(stats.items()) == \"3 files added and 2 files modified\"\n\ndel stats[\"modified\"]\nassert get_summary(stats.items()) == \"3 files added\"\n\nassert get_summary([]) == \"\"\nassert get_summary([(\"x\", 0), (\"y\", [])]) == \"\"\nassert get_summary([(\"x\", 1), (\"y\", [])]) == \"1 file x\"", "path": "dvc/tests/unit/utils/test_humanize.py", "commit_date": "2020-05-05 00:00:00", "repo_name": "iterative/dvc", "stars": 12952, "license": "apache-2.0", "language": "python", "size": 19502}
{"docstring": "# patching to speedup tests\n", "func_signal": "def test_cli(tmp_dir, dvc, mocker, caplog):\n", "code": "mocker.patch(\"dvc.lock.DEFAULT_TIMEOUT\", 0.01)\n\nexpected_error_msg = (\n    \"Unable to acquire lock. Most likely another DVC process is \"\n    \"running or was terminated abruptly. Check the page \"\n    \"<https://dvc.org/doc/user-guide/troubleshooting#lock-issue> \"\n    \"for other possible reasons and to learn how to resolve this.\"\n)\nwith Lock(tmp_dir / dvc.tmp_dir / \"lock\"):\n    assert main([\"add\", \"foo\"]) == 1\nassert expected_error_msg in caplog.text", "path": "dvc/tests/func/test_lock.py", "commit_date": "2020-10-19 00:00:00", "repo_name": "iterative/dvc", "stars": 12952, "license": "apache-2.0", "language": "python", "size": 19502}
{"docstring": "# optimization: we don't support overriding a list\n", "func_signal": "def __deepcopy__(self, _):\n", "code": "new = CtxList([])\nnew.data = self.data[:]  # shortcircuting __setitem__\nreturn new", "path": "dvc/dvc/parsing/context.py", "commit_date": "2020-12-28 00:00:00", "repo_name": "iterative/dvc", "stars": 12952, "license": "apache-2.0", "language": "python", "size": 19502}
{"docstring": "#\n# Just a reminder about naming conventions:\n# +------------X\n# |\n# |\n# |\n# |\n# Y\n#\n\n", "func_signal": "def _build_sugiyama_layout(vertexes, edges):\n", "code": "vertexes = {v: Vertex(f\" {v} \") for v in vertexes}\n# NOTE: reverting edges to correctly orientate the graph\nedges = [Edge(vertexes[e], vertexes[s]) for s, e in edges]\nvertexes = vertexes.values()\ngraph = Graph(vertexes, edges)\n\nfor vertex in vertexes:\n    vertex.view = VertexViewer(vertex.data)\n\n# NOTE: determine min box length to create the best layout\nminw = min(v.view.w for v in vertexes)\n\nfor edge in edges:\n    edge.view = EdgeViewer()\n\nsug = SugiyamaLayout(graph.C[0])\ngraph = graph.C[0]\nroots = list(filter(lambda x: len(x.e_in()) == 0, graph.sV))\n\nsug.init_all(roots=roots, optimize=True)\n\nsug.yspace = VertexViewer.HEIGHT\nsug.xspace = minw\nsug.route_edge = route_with_lines\n\nsug.draw()\n\nreturn sug", "path": "dvc/dvc/dagascii.py", "commit_date": "2020-06-18 00:00:00", "repo_name": "iterative/dvc", "stars": 12952, "license": "apache-2.0", "language": "python", "size": 19502}
{"docstring": "\"\"\"Parses text into dict for update params.\"\"\"\n", "func_signal": "def parse_py_for_update(text, path):\n", "code": "with reraise(SyntaxError, PythonFileCorruptedError(path)):\n    tree = ast.parse(text, filename=path)\n\nresult = _ast_tree_to_dict(tree)\nresult.update({_PARAMS_KEY: _ast_tree_to_dict(tree, lineno=True)})\nresult.update({_PARAMS_TEXT_KEY: text})\nreturn result", "path": "dvc/dvc/utils/serialize/_py.py", "commit_date": "2020-09-24 00:00:00", "repo_name": "iterative/dvc", "stars": 12952, "license": "apache-2.0", "language": "python", "size": 19502}
{"docstring": "\"\"\"Create a line on ASCII canvas.\n\nArgs:\n    x0 (int): x coordinate where the line should start.\n    y0 (int): y coordinate where the line should start.\n    x1 (int): x coordinate where the line should end.\n    y1 (int): y coordinate where the line should end.\n    char (str): character to draw the line with.\n\"\"\"\n# pylint: disable=too-many-arguments, too-many-branches\n", "func_signal": "def line(self, x0, y0, x1, y1, char):\n", "code": "if x0 > x1:\n    x1, x0 = x0, x1\n    y1, y0 = y0, y1\n\ndx = x1 - x0\ndy = y1 - y0\n\nif dx == 0 and dy == 0:\n    self.point(x0, y0, char)\nelif abs(dx) >= abs(dy):\n    for x in range(x0, x1 + 1):\n        if dx == 0:\n            y = y0\n        else:\n            y = y0 + int(round((x - x0) * dy / float(dx)))\n        self.point(x, y, char)\nelif y0 < y1:\n    for y in range(y0, y1 + 1):\n        if dy == 0:\n            x = x0\n        else:\n            x = x0 + int(round((y - y0) * dx / float(dy)))\n        self.point(x, y, char)\nelse:\n    for y in range(y1, y0 + 1):\n        if dy == 0:\n            x = x0\n        else:\n            x = x1 + int(round((y - y1) * dx / float(dy)))\n        self.point(x, y, char)", "path": "dvc/dvc/dagascii.py", "commit_date": "2020-06-18 00:00:00", "repo_name": "iterative/dvc", "stars": 12952, "license": "apache-2.0", "language": "python", "size": 19502}
{"docstring": "\"\"\"Recursively resolves interpolation and returns resolved data.\n\nArgs:\n    src: Data (str/list/dict etc.) to resolve\n    unwrap: Unwrap CtxDict/CtxList/Value to it's original data if\n            inside `src`. Defaults to True.\n\n>>> c = Context({\"three\": 3})\n>>> c.resolve({\"lst\": [1, 2, \"${three}\"]})\n{'lst': [1, 2, 3]}\n\"\"\"\n", "func_signal": "def resolve(self, src, unwrap=True, skip_interpolation_checks=False):\n", "code": "func = recurse(self.resolve_str)\nreturn func(src, unwrap, skip_interpolation_checks)", "path": "dvc/dvc/parsing/context.py", "commit_date": "2020-12-28 00:00:00", "repo_name": "iterative/dvc", "stars": 12952, "license": "apache-2.0", "language": "python", "size": 19502}
{"docstring": "\"\"\"Validate schema on each serialization.\"\"\"\n", "func_signal": "def to_pipeline_file(stage):\n", "code": "e = _to_pipeline_file(stage)\nassert len(Schema(e)) == 1\nreturn e", "path": "dvc/tests/unit/stage/test_serialize_pipeline_file.py", "commit_date": "2020-06-09 00:00:00", "repo_name": "iterative/dvc", "stars": 12952, "license": "apache-2.0", "language": "python", "size": 19502}
{"docstring": "# NOTE: check that `dvc root` is not blocked with dvc lock\n", "func_signal": "def test_root_locked(tmp_dir, dvc, caplog):\n", "code": "with dvc.lock:\n    assert main([\"root\"]) == 0\nassert \".\\n\" in caplog.text", "path": "dvc/tests/func/test_root.py", "commit_date": "2020-12-30 00:00:00", "repo_name": "iterative/dvc", "stars": 12952, "license": "apache-2.0", "language": "python", "size": 19502}
{"docstring": "\"\"\"\nTop level mutable dict, with some helpers to create context and track\n\"\"\"\n", "func_signal": "def __init__(self, *args, **kwargs):\n", "code": "super().__init__(*args, **kwargs)\nself._track = False\nself._tracked_data = defaultdict(dict)\nself.imports = {}\nself._reserved_keys = {}", "path": "dvc/dvc/parsing/context.py", "commit_date": "2020-12-28 00:00:00", "repo_name": "iterative/dvc", "stars": 12952, "license": "apache-2.0", "language": "python", "size": 19502}
{"docstring": "\"\"\"Recursively apply changes from src to dest.\n\nPreserves dest type and hidden info in dest structure,\nlike ruamel.yaml leaves when parses files. This includes comments,\nordering and line foldings.\n\nUsed in Stage load/dump cycle to preserve comments and custom formatting.\n\"\"\"\n", "func_signal": "def apply_diff(src, dest):\n", "code": "Seq = (list, tuple)\nContainer = (Mapping, list, tuple)\n\ndef is_same_type(a, b):\n    return any(\n        isinstance(a, t) and isinstance(b, t)\n        for t in [str, Mapping, Seq, bool]\n    )\n\nif isinstance(src, Mapping) and isinstance(dest, Mapping):\n    for key, value in src.items():\n        if isinstance(value, Container) and is_same_type(\n            value, dest.get(key)\n        ):\n            apply_diff(value, dest[key])\n        elif key not in dest or value != dest[key]:\n            dest[key] = value\n    for key in set(dest) - set(src):\n        del dest[key]\nelif isinstance(src, Seq) and isinstance(dest, Seq):\n    if len(src) != len(dest):\n        dest[:] = src\n    else:\n        for i, value in enumerate(src):\n            if isinstance(value, Container) and is_same_type(\n                value, dest[i]\n            ):\n                apply_diff(value, dest[i])\n            elif value != dest[i]:\n                dest[i] = value\nelse:\n    raise AssertionError(\n        \"Can't apply diff from {} to {}\".format(\n            src.__class__.__name__, dest.__class__.__name__\n        )\n    )", "path": "dvc/dvc/utils/collections.py", "commit_date": "2020-08-10 00:00:00", "repo_name": "iterative/dvc", "stars": 12952, "license": "apache-2.0", "language": "python", "size": 19502}
{"docstring": "\"\"\"Draws ASCII canvas on the screen.\"\"\"\n", "func_signal": "def draw(self):\n", "code": "lines = map(\"\".join, self.canvas)\njoined_lines = os.linesep.join(lines)\nreturn joined_lines", "path": "dvc/dvc/dagascii.py", "commit_date": "2020-06-18 00:00:00", "repo_name": "iterative/dvc", "stars": 12952, "license": "apache-2.0", "language": "python", "size": 19502}
{"docstring": "\"\"\"Generate a graph by using the given stages on the given directory\n\nThe nodes of the graph are the stage's path relative to the root.\n\nEdges are created when the output of one stage is used as a\ndependency in other stage.\n\nThe direction of the edges goes from the stage to its dependency:\n\nFor example, running the following:\n\n    $ dvc run -o A \"echo A > A\"\n    $ dvc run -d A -o B \"echo B > B\"\n    $ dvc run -d B -o C \"echo C > C\"\n\nWill create the following graph:\n\n       ancestors <--\n                   |\n        C.dvc -> B.dvc -> A.dvc\n        |          |\n        |          --> descendants\n        |\n        ------- pipeline ------>\n                   |\n                   v\n      (weakly connected components)\n\nArgs:\n    stages (list): used to build a graph from\n\nRaises:\n    OutputDuplicationError: two outputs with the same path\n    StagePathAsOutputError: stage inside an output directory\n    OverlappingOutputPathsError: output inside output directory\n    CyclicGraphError: resulting graph has cycles\n\"\"\"\n", "func_signal": "def build_graph(stages, outs_trie=None):\n", "code": "import networkx as nx\n\nfrom dvc.exceptions import StagePathAsOutputError\n\nfrom ..path_info import PathInfo\nfrom .trie import build_outs_trie\n\nG = nx.DiGraph()\n\n# Use trie to efficiently find overlapping outs and deps\nouts_trie = outs_trie or build_outs_trie(stages)\n\nfor stage in stages:\n    out = outs_trie.shortest_prefix(PathInfo(stage.path).parts).value\n    if out:\n        raise StagePathAsOutputError(stage, str(out))\n\n# Building graph\nG.add_nodes_from(stages)\nfor stage in stages:\n    for dep in stage.deps:\n        if dep.path_info is None:\n            continue\n\n        dep_key = dep.path_info.parts\n        overlapping = [n.value for n in outs_trie.prefixes(dep_key)]\n        if outs_trie.has_subtrie(dep_key):\n            overlapping.extend(outs_trie.values(prefix=dep_key))\n\n        G.add_edges_from((stage, out.stage) for out in overlapping)\ncheck_acyclic(G)\n\nreturn G", "path": "dvc/dvc/repo/graph.py", "commit_date": "2020-12-15 00:00:00", "repo_name": "iterative/dvc", "stars": 12952, "license": "apache-2.0", "language": "python", "size": 19502}
{"docstring": "\"\"\"Unicode strings written to specified files\"\"\"\n", "func_signal": "def test_unicode_string_io_for_specified_file():\n", "code": "for _ in tqdm(range(3), file=WriteTypeChecker(expected_type=type(u''))):\n    pass", "path": "tqdm/tests/tests_tqdm.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "tqdm/tqdm", "stars": 27123, "license": "other", "language": "python", "size": 5953}
{"docstring": "\"\"\"Replaces internal `message_id`'s text with `s`.\"\"\"\n", "func_signal": "def write(self, s):\n", "code": "if not s:\n    s = \"...\"\ns = s.replace('\\r', '').strip()\nif s == self.text:\n    return  # avoid duplicate message Bot error\nself.text = s\ntry:\n    future = self.submit(\n        self.session.post, self.API + '%s/editMessageText' % self.token,\n        data=dict(text='`' + s + '`', chat_id=self.chat_id,\n                  message_id=self.message_id, parse_mode='MarkdownV2'))\nexcept Exception as e:\n    tqdm_auto.write(str(e))\nelse:\n    return future", "path": "tqdm/tqdm/contrib/telegram.py", "commit_date": "2020-12-25 00:00:00", "repo_name": "tqdm/tqdm", "stars": 27123, "license": "other", "language": "python", "size": 5953}
{"docstring": "\"\"\"Test unpause\"\"\"\n", "func_signal": "def test_unpause():\n", "code": "timer = DiscreteTimer()\nwith closing(StringIO()) as our_file:\n    t = trange(10, file=our_file, leave=True, mininterval=0)\n    cpu_timify(t, timer)\n    timer.sleep(0.01)\n    t.update()\n    timer.sleep(0.01)\n    t.update()\n    timer.sleep(0.1)  # longer wait time\n    t.unpause()\n    timer.sleep(0.01)\n    t.update()\n    timer.sleep(0.01)\n    t.update()\n    t.close()\n    r_before = progressbar_rate(get_bar(our_file.getvalue(), 2))\n    r_after = progressbar_rate(get_bar(our_file.getvalue(), 3))\nassert r_before == r_after", "path": "tqdm/tests/tests_tqdm.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "tqdm/tqdm", "stars": 27123, "license": "other", "language": "python", "size": 5953}
{"docstring": "\"\"\"Test write messages\"\"\"\n", "func_signal": "def test_write():\n", "code": "s = \"Hello world\"\nwith closing(StringIO()) as our_file:\n    # Change format to keep only left part w/o bar and it/s rate\n    t1 = tqdm(total=10, file=our_file, desc='pos0 bar',\n              bar_format='{l_bar}', mininterval=0, miniters=1)\n    t2 = trange(10, file=our_file, desc='pos1 bar', bar_format='{l_bar}',\n                mininterval=0, miniters=1)\n    t3 = tqdm(total=10, file=our_file, desc='pos2 bar',\n              bar_format='{l_bar}', mininterval=0, miniters=1)\n    t1.update()\n    t2.update()\n    t3.update()\n    before = our_file.getvalue()\n\n    # Write msg and see if bars are correctly redrawn below the msg\n    t1.write(s, file=our_file)  # call as an instance method\n    tqdm.write(s, file=our_file)  # call as a class method\n    after = our_file.getvalue()\n\n    t1.close()\n    t2.close()\n    t3.close()\n\n    before_squashed = squash_ctrlchars(before)\n    after_squashed = squash_ctrlchars(after)\n\n    assert after_squashed == [s, s] + before_squashed\n\n# Check that no bar clearing if different file\nwith closing(StringIO()) as our_file_bar:\n    with closing(StringIO()) as our_file_write:\n        t1 = tqdm(total=10, file=our_file_bar, desc='pos0 bar',\n                  bar_format='{l_bar}', mininterval=0, miniters=1)\n\n        t1.update()\n        before_bar = our_file_bar.getvalue()\n\n        tqdm.write(s, file=our_file_write)\n\n        after_bar = our_file_bar.getvalue()\n        t1.close()\n\n        assert before_bar == after_bar\n\n# Test stdout/stderr anti-mixup strategy\n# Backup stdout/stderr\nstde = sys.stderr\nstdo = sys.stdout\n# Mock stdout/stderr\nwith closing(StringIO()) as our_stderr:\n    with closing(StringIO()) as our_stdout:\n        sys.stderr = our_stderr\n        sys.stdout = our_stdout\n        t1 = tqdm(total=10, file=sys.stderr, desc='pos0 bar',\n                  bar_format='{l_bar}', mininterval=0, miniters=1)\n\n        t1.update()\n        before_err = sys.stderr.getvalue()\n        before_out = sys.stdout.getvalue()\n\n        tqdm.write(s, file=sys.stdout)\n        after_err = sys.stderr.getvalue()\n        after_out = sys.stdout.getvalue()\n\n        t1.close()\n\n        assert before_err == '\\rpos0 bar:   0%|\\rpos0 bar:  10%|'\n        assert before_out == ''\n        after_err_res = [m[0] for m in RE_pos.findall(after_err)]\n        exres = ['\\rpos0 bar:   0%|',\n                 '\\rpos0 bar:  10%|',\n                 '\\r               ',\n                 '\\r\\rpos0 bar:  10%|']\n        pos_line_diff(after_err_res, exres)\n        assert after_out == s + '\\n'\n# Restore stdout and stderr\nsys.stderr = stde\nsys.stdout = stdo", "path": "tqdm/tests/tests_tqdm.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "tqdm/tqdm", "stars": 27123, "license": "other", "language": "python", "size": 5953}
{"docstring": "\"\"\"Test csv iterator\"\"\"\n# Create a test csv pseudo file\n", "func_signal": "def test_iterate_over_csv_rows():\n", "code": "with closing(StringIO()) as test_csv_file:\n    writer = csv.writer(test_csv_file)\n    for _ in _range(3):\n        writer.writerow(['test'] * 3)\n    test_csv_file.seek(0)\n\n    # Test that nothing fails if we iterate over rows\n    reader = csv.DictReader(test_csv_file, fieldnames=('row1', 'row2', 'row3'))\n    with closing(StringIO()) as our_file:\n        for _ in tqdm(reader, file=our_file):\n            pass", "path": "tqdm/tests/tests_tqdm.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "tqdm/tqdm", "stars": 27123, "license": "other", "language": "python", "size": 5953}
{"docstring": "\"\"\"Test output to arbitrary file-like objects\"\"\"\n", "func_signal": "def test_file_output():\n", "code": "with closing(StringIO()) as our_file:\n    for i in tqdm(_range(3), file=our_file):\n        if i == 1:\n            our_file.seek(0)\n            assert '0/3' in our_file.read()", "path": "tqdm/tests/tests_tqdm.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "tqdm/tqdm", "stars": 27123, "license": "other", "language": "python", "size": 5953}
{"docstring": "\"\"\"Test float totals\"\"\"\n", "func_signal": "def test_float_progress():\n", "code": "with closing(StringIO()) as our_file:\n    with trange(10, total=9.6, file=our_file) as t:\n        with catch_warnings(record=True) as w:\n            simplefilter(\"always\", category=TqdmWarning)\n            for i in t:\n                if i < 9:\n                    assert not w\n            assert w\n            assert \"clamping frac\" in str(w[-1].message)", "path": "tqdm/tests/tests_tqdm.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "tqdm/tqdm", "stars": 27123, "license": "other", "language": "python", "size": 5953}
{"docstring": "\"\"\"Test clearing bar display\"\"\"\n", "func_signal": "def test_clear():\n", "code": "with closing(StringIO()) as our_file:\n    t1 = tqdm(total=10, file=our_file, desc='pos0 bar', bar_format='{l_bar}')\n    t2 = trange(10, file=our_file, desc='pos1 bar', bar_format='{l_bar}')\n    before = squash_ctrlchars(our_file.getvalue())\n    t2.clear()\n    t1.clear()\n    after = squash_ctrlchars(our_file.getvalue())\n    t1.close()\n    t2.close()\n    assert before == ['pos0 bar:   0%|', 'pos1 bar:   0%|']\n    assert after == ['', '']", "path": "tqdm/tests/tests_tqdm.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "tqdm/tqdm", "stars": 27123, "license": "other", "language": "python", "size": 5953}
{"docstring": "\"\"\"Test comparison functions\"\"\"\n", "func_signal": "def test_cmp():\n", "code": "with closing(StringIO()) as our_file:\n    t0 = tqdm(total=10, file=our_file)\n    t1 = tqdm(total=10, file=our_file)\n    t2 = tqdm(total=10, file=our_file)\n\n    assert t0 < t1\n    assert t2 >= t0\n    assert t0 <= t2\n\n    t3 = tqdm(total=10, file=our_file)\n    t4 = tqdm(total=10, file=our_file)\n    t5 = tqdm(total=10, file=our_file)\n    t5.close()\n    t6 = tqdm(total=10, file=our_file)\n\n    assert t3 != t4\n    assert t3 > t2\n    assert t5 == t6\n    t6.close()\n    t4.close()\n    t3.close()\n    t2.close()\n    t1.close()\n    t0.close()", "path": "tqdm/tests/tests_tqdm.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "tqdm/tqdm", "stars": 27123, "license": "other", "language": "python", "size": 5953}
{"docstring": "\"\"\"Test autodisable will not disable on TTY\"\"\"\n", "func_signal": "def test_autodisable_enable():\n", "code": "with closing(StringIO()) as our_file:\n    setattr(our_file, \"isatty\", lambda: True)\n    with tqdm(total=10, disable=None, file=our_file) as t:\n        t.update()\n    assert our_file.getvalue() != ''", "path": "tqdm/tests/tests_tqdm.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "tqdm/tqdm", "stars": 27123, "license": "other", "language": "python", "size": 5953}
{"docstring": "\"\"\"Test disable\"\"\"\n", "func_signal": "def test_disable():\n", "code": "with closing(StringIO()) as our_file:\n    for _ in tqdm(_range(3), file=our_file, disable=True):\n        pass\n    assert our_file.getvalue() == ''\n\nwith closing(StringIO()) as our_file:\n    progressbar = tqdm(total=3, file=our_file, miniters=1, disable=True)\n    progressbar.update(3)\n    progressbar.close()\n    assert our_file.getvalue() == ''", "path": "tqdm/tests/tests_tqdm.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "tqdm/tqdm", "stars": 27123, "license": "other", "language": "python", "size": 5953}
{"docstring": "\"\"\"\nParameters\n----------\ntoken  : str, required. Telegram token\n    [default: ${TQDM_TELEGRAM_TOKEN}].\nchat_id  : str, required. Telegram chat ID\n    [default: ${TQDM_TELEGRAM_CHAT_ID}].\n\nSee `tqdm.auto.tqdm.__init__` for other parameters.\n\"\"\"\n", "func_signal": "def __init__(self, *args, **kwargs):\n", "code": "kwargs = kwargs.copy()\nself.tgio = TelegramIO(kwargs.pop('token', getenv('TQDM_TELEGRAM_TOKEN')),\n                       kwargs.pop('chat_id', getenv('TQDM_TELEGRAM_CHAT_ID')))\nsuper(tqdm_telegram, self).__init__(*args, **kwargs)", "path": "tqdm/tqdm/contrib/telegram.py", "commit_date": "2020-12-25 00:00:00", "repo_name": "tqdm/tqdm", "stars": 27123, "license": "other", "language": "python", "size": 5953}
{"docstring": "\"\"\"Test exponential weighted average\"\"\"\n", "func_signal": "def test_ema():\n", "code": "ema = EMA(0.01)\nassert round(ema(10), 2) == 10\nassert round(ema(1), 2) == 5.48\nassert round(ema(), 2) == 5.48\nassert round(ema(1), 2) == 3.97\nassert round(ema(1), 2) == 3.22", "path": "tqdm/tests/tests_tqdm.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "tqdm/tqdm", "stars": 27123, "license": "other", "language": "python", "size": 5953}
{"docstring": "\"\"\"Check that the RLock has not been constructed.\"\"\"\n", "func_signal": "def _rlock_creation_target():\n", "code": "from unittest.mock import patch\nimport multiprocessing as mp\n\n# Patch the RLock class/method but use the original implementation\nwith patch('multiprocessing.RLock', wraps=mp.RLock) as rlock_mock:\n    # Importing the module should not create a lock\n    from tqdm import tqdm\n    assert rlock_mock.call_count == 0\n    # Creating a progress bar should initialize the lock\n    with closing(StringIO()) as our_file:\n        with tqdm(file=our_file) as _:  # NOQA\n            pass\n    assert rlock_mock.call_count == 1\n    # Creating a progress bar again should reuse the lock\n    with closing(StringIO()) as our_file:\n        with tqdm(file=our_file) as _:  # NOQA\n            pass\n    assert rlock_mock.call_count == 1", "path": "tqdm/tests/tests_tqdm.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "tqdm/tqdm", "stars": 27123, "license": "other", "language": "python", "size": 5953}
{"docstring": "\"\"\"Test postfix\"\"\"\n", "func_signal": "def test_postfix():\n", "code": "postfix = {'float': 0.321034, 'gen': 543, 'str': 'h', 'lst': [2]}\npostfix_order = (('w', 'w'), ('a', 0))  # no need for OrderedDict\nexpected = ['float=0.321', 'gen=543', 'lst=[2]', 'str=h']\nexpected_order = ['w=w', 'a=0', 'float=0.321', 'gen=543', 'lst=[2]', 'str=h']\n\n# Test postfix set at init\nwith closing(StringIO()) as our_file:\n    with tqdm(total=10, file=our_file, desc='pos0 bar',\n              bar_format='{r_bar}', postfix=postfix) as t1:\n        t1.refresh()\n        out = our_file.getvalue()\n\n# Test postfix set after init\nwith closing(StringIO()) as our_file:\n    with trange(10, file=our_file, desc='pos1 bar', bar_format='{r_bar}',\n                postfix=None) as t2:\n        t2.set_postfix(**postfix)\n        t2.refresh()\n        out2 = our_file.getvalue()\n\n# Order of items in dict may change, so need a loop to check per item\nfor res in expected:\n    assert res in out\n    assert res in out2\n\n# Test postfix (with ordered dict and no refresh) set after init\nwith closing(StringIO()) as our_file:\n    with trange(10, file=our_file, desc='pos2 bar', bar_format='{r_bar}',\n                postfix=None) as t3:\n        t3.set_postfix(postfix_order, False, **postfix)\n        t3.refresh()  # explicit external refresh\n        out3 = our_file.getvalue()\n\nout3 = out3[1:-1].split(', ')[3:]\nassert out3 == expected_order\n\n# Test postfix (with ordered dict and refresh) set after init\nwith closing(StringIO()) as our_file:\n    with trange(10, file=our_file, desc='pos2 bar',\n                bar_format='{r_bar}', postfix=None) as t4:\n        t4.set_postfix(postfix_order, True, **postfix)\n        t4.refresh()  # double refresh\n        out4 = our_file.getvalue()\n\nassert out4.count('\\r') > out3.count('\\r')\nassert out4.count(\", \".join(expected_order)) == 2\n\n# Test setting postfix string directly\nwith closing(StringIO()) as our_file:\n    with trange(10, file=our_file, desc='pos2 bar', bar_format='{r_bar}',\n                postfix=None) as t5:\n        t5.set_postfix_str(\"Hello\", False)\n        t5.set_postfix_str(\"World\")\n        out5 = our_file.getvalue()\n\nassert \"Hello\" not in out5\nout5 = out5[1:-1].split(', ')[3:]\nassert out5 == [\"World\"]", "path": "tqdm/tests/tests_tqdm.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "tqdm/tqdm", "stars": 27123, "license": "other", "language": "python", "size": 5953}
{"docstring": "\"\"\"Test advance len (numpy array shape)\"\"\"\n", "func_signal": "def test_len():\n", "code": "np = importorskip(\"numpy\")\nwith closing(StringIO()) as f:\n    with tqdm(np.zeros((3, 4)), file=f) as t:\n        assert len(t) == 3", "path": "tqdm/tests/tests_tqdm.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "tqdm/tqdm", "stars": 27123, "license": "other", "language": "python", "size": 5953}
{"docstring": "\"\"\"Test Bar.__format__ spec\"\"\"\n", "func_signal": "def test_bar_formatspec():\n", "code": "assert \"{0:5a}\".format(Bar(0.3)) == \"#5   \"\nassert \"{0:2}\".format(Bar(0.5, charset=\" .oO0\")) == \"0 \"\nassert \"{0:2a}\".format(Bar(0.5, charset=\" .oO0\")) == \"# \"\nassert \"{0:-6a}\".format(Bar(0.5, 10)) == '##  '\nassert \"{0:2b}\".format(Bar(0.5, 10)) == '  '", "path": "tqdm/tests/tests_tqdm.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "tqdm/tqdm", "stars": 27123, "license": "other", "language": "python", "size": 5953}
{"docstring": "\"\"\"Test write_bytes argument with and without `file`\"\"\"\n# specified file (and bytes)\n", "func_signal": "def test_write_bytes():\n", "code": "for _ in tqdm(range(3), file=WriteTypeChecker(expected_type=type(b'')),\n              write_bytes=True):\n    pass\n# unspecified file (and unicode)\nstderr = sys.stderr\ntry:\n    sys.stderr = WriteTypeChecker(expected_type=type(u''))\n    for _ in tqdm(range(3), write_bytes=False):\n        pass\nfinally:\n    sys.stderr = stderr", "path": "tqdm/tests/tests_tqdm.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "tqdm/tqdm", "stars": 27123, "license": "other", "language": "python", "size": 5953}
{"docstring": "\"\"\"Test that importing tqdm does not create multiprocessing objects.\"\"\"\n", "func_signal": "def test_rlock_creation():\n", "code": "import multiprocessing as mp\nif sys.version_info < (3, 3):\n    skip(\"unittest.mock is a 3.3+ feature\")\n\n# Use 'spawn' instead of 'fork' so that the process does not inherit any\n# globals that have been constructed by running other tests\nctx = mp.get_context('spawn')\nwith ctx.Pool(1) as pool:\n    # The pool will propagate the error if the target method fails\n    pool.apply(_rlock_creation_target)", "path": "tqdm/tests/tests_tqdm.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "tqdm/tqdm", "stars": 27123, "license": "other", "language": "python", "size": 5953}
{"docstring": "\"\"\"Test miniters\"\"\"\n", "func_signal": "def test_min_iters():\n", "code": "with closing(StringIO()) as our_file:\n    for _ in tqdm(_range(3), file=our_file, leave=True, mininterval=0, miniters=2):\n        pass\n\n    out = our_file.getvalue()\n    assert '| 0/3 ' in out\n    assert '| 1/3 ' not in out\n    assert '| 2/3 ' in out\n    assert '| 3/3 ' in out\n\nwith closing(StringIO()) as our_file:\n    for _ in tqdm(_range(3), file=our_file, leave=True, mininterval=0, miniters=1):\n        pass\n\n    out = our_file.getvalue()\n    assert '| 0/3 ' in out\n    assert '| 1/3 ' in out\n    assert '| 2/3 ' in out\n    assert '| 3/3 ' in out", "path": "tqdm/tests/tests_tqdm.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "tqdm/tqdm", "stars": 27123, "license": "other", "language": "python", "size": 5953}
{"docstring": "\"\"\"\u6d4b\u8bd5\u987a\u5e8f\u67e5\u627e\"\"\"\n", "func_signal": "def test_seq_search(self):\n", "code": "self.assertEqual(0, seq_search(self.data1, 35))\nself.assertEqual(2, seq_search(self.data1, 12))\nself.assertEqual(6, seq_search(self.data1, 81))\nself.assertEqual(7, seq_search(self.data1, 40))\nself.assertEqual(-1, seq_search(self.data1, 99))\nself.assertEqual(-1, seq_search(self.data1, 7))", "path": "Python-100-Days/Day16-20/code/test_example01.py", "commit_date": "2019-02-12 00:00:00", "repo_name": "jackfrued/Python-100-Days", "stars": 148093, "license": "None", "language": "python", "size": 321278}
{"docstring": "# \u6a21\u62df\u4e0b\u8f7d\u4efb\u52a1\u9700\u8981\u82b1\u8d3910\u79d2\u949f\u65f6\u95f4\n", "func_signal": "def download():\n", "code": "time.sleep(10)\ntkinter.messagebox.showinfo('\u63d0\u793a', '\u4e0b\u8f7d\u5b8c\u6210!')", "path": "Python-100-Days/Day01-15/code/Day13/singlethread2.py", "commit_date": "2019-06-05 00:00:00", "repo_name": "jackfrued/Python-100-Days", "stars": 148093, "license": "None", "language": "python", "size": 321278}
{"docstring": "\"\"\"\u521d\u59cb\u5316\u65b9\u6cd5\"\"\"\n", "func_signal": "def __init__(self, width=0, height=0):\n", "code": "self.__width = width\nself.__height = height", "path": "Python-100-Days/Day01-15/code/Day08/rect.py", "commit_date": "2019-06-05 00:00:00", "repo_name": "jackfrued/Python-100-Days", "stars": 148093, "license": "None", "language": "python", "size": 321278}
{"docstring": "\"\"\"\u6d4b\u8bd5\u4e8c\u5206\u67e5\u627e\"\"\"\n", "func_signal": "def test_bin_search(self):\n", "code": "self.assertEqual(1, bin_search(self.data2, 35))\nself.assertEqual(0, bin_search(self.data2, 12))\nself.assertEqual(6, bin_search(self.data2, 81))\nself.assertEqual(2, bin_search(self.data2, 40))\nself.assertEqual(7, bin_search(self.data2, 97))\nself.assertEqual(-1, bin_search(self.data2, 7))\nself.assertEqual(-1, bin_search(self.data2, 99))", "path": "Python-100-Days/Day16-20/code/test_example01.py", "commit_date": "2019-02-12 00:00:00", "repo_name": "jackfrued/Python-100-Days", "stars": 148093, "license": "None", "language": "python", "size": 321278}
{"docstring": "# \u83b7\u5f97\u9501\u540e\u4ee3\u7801\u624d\u80fd\u7ee7\u7eed\u6267\u884c\n", "func_signal": "def deposit(self, money):\n", "code": "self._lock.acquire()\ntry:\n    new_balance = self._balance + money\n    time.sleep(0.01)\n    self._balance = new_balance\nfinally:\n    # \u64cd\u4f5c\u5b8c\u6210\u540e\u4e00\u5b9a\u8981\u8bb0\u7740\u91ca\u653e\u9501\n    self._lock.release()", "path": "Python-100-Days/Day01-15/code/Day13/multithread6.py", "commit_date": "2019-06-05 00:00:00", "repo_name": "jackfrued/Python-100-Days", "stars": 148093, "license": "None", "language": "python", "size": 321278}
{"docstring": "\"\"\"\u8f93\u5165\u7269\u54c1\u4fe1\u606f\"\"\"\n", "func_signal": "def input_thing():\n", "code": "name_str, price_str, weight_str = input().split()\nreturn name_str, int(price_str), int(weight_str)", "path": "Python-100-Days/Day16-20/code/example04.py", "commit_date": "2019-02-12 00:00:00", "repo_name": "jackfrued/Python-100-Days", "stars": 148093, "license": "None", "language": "python", "size": 321278}
{"docstring": "# \u6a21\u62df\u4e0b\u8f7d\u4efb\u52a1\u9700\u8981\u82b1\u8d3910\u79d2\u949f\u65f6\u95f4\n", "func_signal": "def run(self):\n", "code": "time.sleep(10)\ntkinter.messagebox.showinfo('\u63d0\u793a', '\u4e0b\u8f7d\u5b8c\u6210!')\n# \u542f\u7528\u4e0b\u8f7d\u6309\u94ae\nbutton1.config(state=tkinter.NORMAL)", "path": "Python-100-Days/Day01-15/code/Day13/multithread4.py", "commit_date": "2019-06-05 00:00:00", "repo_name": "jackfrued/Python-100-Days", "stars": 148093, "license": "None", "language": "python", "size": 321278}
{"docstring": "# os.system('clear')\n", "func_signal": "def print_board(board):\n", "code": "for row in board:\n    for col in row:\n        print(str(col).center(4), end='')\n    print()", "path": "Python-100-Days/Day16-20/code/example05.py", "commit_date": "2019-02-12 00:00:00", "repo_name": "jackfrued/Python-100-Days", "stars": 148093, "license": "None", "language": "python", "size": 321278}
{"docstring": "\"\"\"\u6590\u6ce2\u62c9\u5207\u6570\"\"\"\n", "func_signal": "def fib(num, results={}):\n", "code": "assert num > 0\nif num in (1, 2):\n    return 1\ntry:\n    return results[num]\nexcept KeyError:\n    results[num] = fib(num - 1) + fib(num - 2)\n    return results[num]", "path": "Python-100-Days/Day16-20/code/example03.py", "commit_date": "2019-02-12 00:00:00", "repo_name": "jackfrued/Python-100-Days", "stars": 148093, "license": "None", "language": "python", "size": 321278}
{"docstring": "# \u901a\u8fc7sys.argv\u83b7\u53d6\u547d\u4ee4\u884c\u53c2\u6570\n", "func_signal": "def main():\n", "code": "if len(sys.argv) > 1:\n    # \u7b2c\u4e00\u4e2a\u547d\u4ee4\u884c\u53c2\u6570\u662f\u7a0b\u5e8f\u672c\u8eab\u6240\u4ee5\u4ece\u7b2c\u4e8c\u4e2a\u5f00\u59cb\u53d6\n    for index in range(1, len(sys.argv)):\n        try:\n            # \u901a\u8fc7subprocess\u6a21\u5757\u7684call\u51fd\u6570\u542f\u52a8\u5b50\u8fdb\u7a0b\n            status = subprocess.call(sys.argv[index])\n        except FileNotFoundError:\n            print('\u4e0d\u80fd\u6267\u884c%s\u547d\u4ee4' % sys.argv[index])\nelse:\n    print('\u8bf7\u4f7f\u7528\u547d\u4ee4\u884c\u53c2\u6570\u6307\u5b9a\u8981\u6267\u884c\u7684\u8fdb\u7a0b')", "path": "Python-100-Days/Day01-15/code/Day13/multiprocess3.py", "commit_date": "2019-06-05 00:00:00", "repo_name": "jackfrued/Python-100-Days", "stars": 148093, "license": "None", "language": "python", "size": 321278}
{"docstring": "\"\"\"\u4e3b\u51fd\u6570\"\"\"\n# for val in fib3(20):\n#     print(val)\n# gen = fib3(20)\n# for _ in range(10):\n#     print(next(gen))\n", "func_signal": "def main():\n", "code": "for num in range(1, 121):\n    with timer():\n        print(f'{num}: {fib(num)}')\n# print(fac(5))\n# print(fac(-5))", "path": "Python-100-Days/Day16-20/code/example03.py", "commit_date": "2019-02-12 00:00:00", "repo_name": "jackfrued/Python-100-Days", "stars": 148093, "license": "None", "language": "python", "size": 321278}
{"docstring": "# \u7981\u7528\u4e0b\u8f7d\u6309\u94ae\n", "func_signal": "def download():\n", "code": "button1.config(state=tkinter.DISABLED)\n# \u901a\u8fc7daemon\u53c2\u6570\u5c06\u7ebf\u7a0b\u8bbe\u7f6e\u4e3a\u5b88\u62a4\u7ebf\u7a0b(\u4e3b\u7a0b\u5e8f\u9000\u51fa\u5c31\u4e0d\u518d\u4fdd\u7559\u6267\u884c)\nDownloadTaskHandler(daemon=True).start()", "path": "Python-100-Days/Day01-15/code/Day13/multithread4.py", "commit_date": "2019-06-05 00:00:00", "repo_name": "jackfrued/Python-100-Days", "stars": 148093, "license": "None", "language": "python", "size": 321278}
{"docstring": "\"\"\"\u4e3b\u51fd\u6570\uff08\u7a0b\u5e8f\u5165\u53e3\uff09\"\"\"\n", "func_signal": "def main():\n", "code": "num = 6\nstyles = ['r-.', 'g-*', 'b-o', 'y-x', 'c-^', 'm-+', 'k-d']\nlegends = ['\u5bf9\u6570', '\u7ebf\u6027', '\u7ebf\u6027\u5bf9\u6570', '\u5e73\u65b9', '\u7acb\u65b9', '\u51e0\u4f55\u7ea7\u6570', '\u9636\u4e58']\nx_data = [x for x in range(1, num + 1)]\ny_data1 = [log2(y) for y in range(1, num + 1)]\ny_data2 = [y for y in range(1, num + 1)]\ny_data3 = [y * log2(y) for y in range(1, num + 1)]\ny_data4 = [y ** 2 for y in range(1, num + 1)]\ny_data5 = [y ** 3 for y in range(1, num + 1)]\ny_data6 = [3 ** y for y in range(1, num + 1)]\ny_data7 = [factorial(y) for y in range(1, num + 1)]\ny_datas = [y_data1, y_data2, y_data3, y_data4, y_data5, y_data6, y_data7]\nfor index, y_data in enumerate(y_datas):\n    pyplot.plot(x_data, y_data, styles[index])\npyplot.legend(legends)\npyplot.xticks(numpy.arange(1, 7, step=1))\npyplot.yticks(numpy.arange(0, 751, step=50))\npyplot.show()", "path": "Python-100-Days/Day16-20/code/example01.py", "commit_date": "2019-05-03 00:00:00", "repo_name": "jackfrued/Python-100-Days", "stars": 148093, "license": "None", "language": "python", "size": 321278}
{"docstring": "# 1.\u521b\u5efa\u5957\u63a5\u5b57\u5bf9\u8c61\u5e76\u6307\u5b9a\u4f7f\u7528\u54ea\u79cd\u4f20\u8f93\u670d\u52a1\n# family=AF_INET - IPv4\u5730\u5740\n# family=AF_INET6 - IPv6\u5730\u5740\n# type=SOCK_STREAM - TCP\u5957\u63a5\u5b57\n# type=SOCK_DGRAM - UDP\u5957\u63a5\u5b57\n# type=SOCK_RAW - \u539f\u59cb\u5957\u63a5\u5b57\n", "func_signal": "def main():\n", "code": "server = socket(family=AF_INET, type=SOCK_STREAM)\n# 2.\u7ed1\u5b9aIP\u5730\u5740\u548c\u7aef\u53e3(\u533a\u5206\u4e0d\u540c\u7684\u670d\u52a1)\nserver.bind(('192.168.1.2', 6789))\n# 3.\u5f00\u542f\u76d1\u542c - \u76d1\u542c\u5ba2\u6237\u7aef\u8fde\u63a5\u5230\u670d\u52a1\u5668\nserver.listen(512)\nprint('\u670d\u52a1\u5668\u542f\u52a8\u5f00\u59cb\u76d1\u542c...')\n# 4.\u901a\u8fc7\u5faa\u73af\u63a5\u6536\u5ba2\u6237\u7aef\u7684\u8fde\u63a5\u5e76\u4f5c\u51fa\u76f8\u5e94\u7684\u5904\u7406(\u63d0\u4f9b\u670d\u52a1)\nwhile True:\n    # accept\u65b9\u6cd5\u662f\u4e00\u4e2a\u963b\u585e\u65b9\u6cd5\u5982\u679c\u6ca1\u6709\u5ba2\u6237\u7aef\u8fde\u63a5\u5230\u670d\u52a1\u5668\n    # \u8fd9\u4e2a\u65b9\u6cd5\u5c31\u4f1a\u963b\u585e\u4ee3\u7801\u4e0d\u4f1a\u5411\u4e0b\u6267\u884c\n    # accept\u65b9\u6cd5\u8fd4\u56de\u5143\u7ec4\u5176\u4e2d\u7684\u7b2c\u4e00\u4e2a\u5143\u7d20\u662f\u5ba2\u6237\u7aef\u5bf9\u8c61\n    # \u7b2c\u4e8c\u4e2a\u5143\u7d20\u662f\u5ba2\u6237\u7aef\u7684\u5730\u5740(\u7531IP\u548c\u7aef\u53e3\u4e24\u90e8\u5206\u6784\u6210)\n    client, addr = server.accept()\n    print(str(addr) + '\u8fde\u63a5\u5230\u4e86\u670d\u52a1\u5668.')\n    # 5.\u53d1\u9001\u6570\u636e\n    client.send(str(datetime.now()).encode('utf-8'))\n    # 6.\u65ad\u5f00\u8fde\u63a5\n    client.close()", "path": "Python-100-Days/Day01-15/code/Day14/timeserver.py", "commit_date": "2019-06-05 00:00:00", "repo_name": "jackfrued/Python-100-Days", "stars": 148093, "license": "None", "language": "python", "size": 321278}
{"docstring": "# \u901a\u8fc7requests\u6a21\u5757\u7684get\u51fd\u6570\u83b7\u53d6\u7f51\u7edc\u8d44\u6e90\n", "func_signal": "def main():\n", "code": "resp = requests.get(\n    'http://api.tianapi.com/meinv/?key=772a81a51ae5c780251b1f98ea431b84&num=10')\n# \u5c06\u670d\u52a1\u5668\u8fd4\u56de\u7684JSON\u683c\u5f0f\u7684\u6570\u636e\u89e3\u6790\u4e3a\u5b57\u5178\ndata_model = resp.json()\nfor mm_dict in data_model['newslist']:\n    url = mm_dict['picUrl']\n    # \u901a\u8fc7\u591a\u7ebf\u7a0b\u7684\u65b9\u5f0f\u5b9e\u73b0\u56fe\u7247\u4e0b\u8f7d\n    DownloadHanlder(url).start()", "path": "Python-100-Days/Day01-15/code/Day14/mmdownloader.py", "commit_date": "2019-06-05 00:00:00", "repo_name": "jackfrued/Python-100-Days", "stars": 148093, "license": "None", "language": "python", "size": 321278}
{"docstring": "# \u7528range\u521b\u5efa\u6570\u503c\u5217\u8868\n", "func_signal": "def main():\n", "code": "list1 = list(range(1, 11))\nprint(list1)\n# \u751f\u6210\u8868\u8fbe\u5f0f\nlist2 = [x * x for x in range(1, 11)]\nprint(list2)\nlist3 = [m + n for m in 'ABCDEFG' for n in '12345']\nprint(list3)\nprint(len(list3))\n# \u751f\u6210\u5668(\u8282\u7701\u7a7a\u95f4\u4f46\u751f\u6210\u4e0b\u4e00\u4e2a\u5143\u7d20\u65f6\u9700\u8981\u82b1\u8d39\u65f6\u95f4)\ngen = (m + n for m in 'ABCDEFG' for n in '12345')\nprint(gen)\nfor elem in gen:\n    print(elem, end=' ')\nprint()\ngen = fib(20)\nprint(gen)\nfor elem in gen:\n    print(elem, end=' ')\nprint()", "path": "Python-100-Days/Day01-15/code/Day07/list3.py", "commit_date": "2019-06-05 00:00:00", "repo_name": "jackfrued/Python-100-Days", "stars": 148093, "license": "None", "language": "python", "size": 321278}
{"docstring": "# \u5148\u83b7\u53d6\u9501\u624d\u80fd\u6267\u884c\u540e\u7eed\u7684\u4ee3\u7801\n", "func_signal": "def deposit(self, money):\n", "code": "self._lock.acquire()\ntry:\n    new_balance = self._balance + money\n    sleep(0.01)\n    self._balance = new_balance\nfinally:\n    # \u8fd9\u6bb5\u4ee3\u7801\u653e\u5728finally\u4e2d\u4fdd\u8bc1\u91ca\u653e\u9501\u7684\u64cd\u4f5c\u4e00\u5b9a\u8981\u6267\u884c\n    self._lock.release()", "path": "Python-100-Days/Day01-15/code/Day13/multithread5.py", "commit_date": "2019-06-05 00:00:00", "repo_name": "jackfrued/Python-100-Days", "stars": 148093, "license": "None", "language": "python", "size": 321278}
{"docstring": "\"\"\"\u7ed8\u5236\u4e94\u89d2\u661f\"\"\"\n", "func_signal": "def draw_star(x, y, radius):\n", "code": "turtle.setpos(x, y)\npos1 = turtle.pos()\nturtle.circle(-radius, 72)\npos2 = turtle.pos()\nturtle.circle(-radius, 72)\npos3 = turtle.pos()\nturtle.circle(-radius, 72)\npos4 = turtle.pos()\nturtle.circle(-radius, 72)\npos5 = turtle.pos()\nturtle.color('yellow', 'yellow')\nturtle.begin_fill()\nturtle.goto(pos3)\nturtle.goto(pos1)\nturtle.goto(pos4)\nturtle.goto(pos2)\nturtle.goto(pos5)\nturtle.end_fill()", "path": "Python-100-Days/Day01-15/code/Day01/flag.py", "commit_date": "2019-09-25 00:00:00", "repo_name": "jackfrued/Python-100-Days", "stars": 148093, "license": "None", "language": "python", "size": 321278}
{"docstring": "# \u521b\u5efa\u6b63\u5219\u8868\u8fbe\u5f0f\u5bf9\u8c61 \u4f7f\u7528\u4e86\u524d\u77bb\u548c\u56de\u987e\u6765\u4fdd\u8bc1\u624b\u673a\u53f7\u524d\u540e\u4e0d\u5e94\u8be5\u51fa\u73b0\u6570\u5b57\n", "func_signal": "def main():\n", "code": "pattern = re.compile(r'(?<=\\D)(1[38]\\d{9}|14[57]\\d{8}|15[0-35-9]\\d{8}|17[678]\\d{8})(?=\\D)')\nsentence = '''\n\u91cd\u8981\u7684\u4e8b\u60c5\u8bf48130123456789\u904d\uff0c\u6211\u7684\u624b\u673a\u53f7\u662f13512346789\u8fd9\u4e2a\u9753\u53f7\uff0c\n\u4e0d\u662f15600998765\uff0c\u4e5f\u662f110\u6216119\uff0c\u738b\u5927\u9524\u7684\u624b\u673a\u53f7\u624d\u662f15600998765\u3002\n'''\n# \u67e5\u627e\u6240\u6709\u5339\u914d\u5e76\u4fdd\u5b58\u5230\u4e00\u4e2a\u5217\u8868\u4e2d\nmylist = re.findall(pattern, sentence)\nprint(mylist)\nprint('--------\u534e\u4e3d\u7684\u5206\u9694\u7ebf--------')\n# \u901a\u8fc7\u8fed\u4ee3\u5668\u53d6\u51fa\u5339\u914d\u5bf9\u8c61\u5e76\u83b7\u5f97\u5339\u914d\u7684\u5185\u5bb9\nfor temp in pattern.finditer(sentence):\n    print(temp.group())\nprint('--------\u534e\u4e3d\u7684\u5206\u9694\u7ebf--------')\n# \u901a\u8fc7search\u51fd\u6570\u6307\u5b9a\u641c\u7d22\u4f4d\u7f6e\u627e\u51fa\u6240\u6709\u5339\u914d\nm = pattern.search(sentence)\nwhile m:\n    print(m.group())\n    m = pattern.search(sentence, m.end())", "path": "Python-100-Days/Day01-15/code/Day12/test4.py", "commit_date": "2019-06-05 00:00:00", "repo_name": "jackfrued/Python-100-Days", "stars": 148093, "license": "None", "language": "python", "size": 321278}
{"docstring": "# \u5f53\u591a\u4e2a\u7ebf\u7a0b\u540c\u65f6\u8bbf\u95ee\u4e00\u4e2a\u8d44\u6e90\u7684\u65f6\u5019 \u5c31\u6709\u53ef\u80fd\u56e0\u4e3a\u7ade\u4e89\u8d44\u6e90\u5bfc\u81f4\u8d44\u6e90\u7684\u72b6\u6001\u9519\u8bef\n# \u88ab\u591a\u4e2a\u7ebf\u7a0b\u8bbf\u95ee\u7684\u8d44\u6e90\u6211\u4eec\u901a\u5e38\u79f0\u4e4b\u4e3a\u4e34\u754c\u8d44\u6e90 \u5bf9\u4e34\u754c\u8d44\u6e90\u7684\u8bbf\u95ee\u9700\u8981\u52a0\u4e0a\u4fdd\u62a4\n", "func_signal": "def deposit(self, money):\n", "code": "if money > 0:\n    self._lock.acquire()\n    try:\n        new_balance = self._balance + money\n        time.sleep(0.01)\n        self._balance = new_balance\n    finally:\n        self._lock.release()", "path": "Python-100-Days/Day01-15/code/Day13/test2.py", "commit_date": "2019-06-05 00:00:00", "repo_name": "jackfrued/Python-100-Days", "stars": 148093, "license": "None", "language": "python", "size": 321278}
{"docstring": "\"\"\"Instead of querying for a list of servers, set a link to a\nspeedtest mini server\n\"\"\"\n\n", "func_signal": "def set_mini_server(self, server):\n", "code": "urlparts = urlparse(server)\n\nname, ext = os.path.splitext(urlparts[2])\nif ext:\n    url = os.path.dirname(server)\nelse:\n    url = server\n\nrequest = build_request(url)\nuh, e = catch_request(request, opener=self._opener)\nif e:\n    raise SpeedtestMiniConnectFailure('Failed to connect to %s' %\n                                      server)\nelse:\n    text = uh.read()\n    uh.close()\n\nextension = re.findall('upload_?[Ee]xtension: \"([^\"]+)\"',\n                       text.decode())\nif not extension:\n    for ext in ['php', 'asp', 'aspx', 'jsp']:\n        try:\n            f = self._opener.open(\n                '%s/speedtest/upload.%s' % (url, ext)\n            )\n        except Exception:\n            pass\n        else:\n            data = f.read().strip().decode()\n            if (f.code == 200 and\n                    len(data.splitlines()) == 1 and\n                    re.match('size=[0-9]', data)):\n                extension = [ext]\n                break\nif not urlparts or not extension:\n    raise InvalidSpeedtestMiniServer('Invalid Speedtest Mini Server: '\n                                     '%s' % server)\n\nself.servers = [{\n    'sponsor': 'Speedtest Mini',\n    'name': urlparts[1],\n    'd': 0,\n    'url': '%s/speedtest/upload.%s' % (url.rstrip('/'), extension[0]),\n    'latency': 0,\n    'id': 0\n}]\n\nreturn self.servers", "path": "speedtest-cli/speedtest.py", "commit_date": "2019-08-22 00:00:00", "repo_name": "sivel/speedtest-cli", "stars": 13209, "license": "apache-2.0", "language": "python", "size": 312}
{"docstring": "\"\"\"Wrapper function for py3 to print, with a utf-8 encoded stdout\"\"\"\n", "func_signal": "def print_(*args, **kwargs):\n", "code": "if kwargs.get('file') == sys.stderr:\n    kwargs['file'] = _py3_utf8_stderr\nelse:\n    kwargs['file'] = kwargs.get('file', _py3_utf8_stdout)\n_py3_print(*args, **kwargs)", "path": "speedtest-cli/speedtest.py", "commit_date": "2019-08-22 00:00:00", "repo_name": "sivel/speedtest-cli", "stars": 13209, "license": "apache-2.0", "language": "python", "size": 312}
{"docstring": "\"\"\"Helper function print a string with various features\"\"\"\n\n", "func_signal": "def printer(string, quiet=False, debug=False, error=False, **kwargs):\n", "code": "if debug and not DEBUG:\n    return\n\nif debug:\n    if sys.stdout.isatty():\n        out = '\\033[1;30mDEBUG: %s\\033[0m' % string\n    else:\n        out = 'DEBUG: %s' % string\nelse:\n    out = string\n\nif error:\n    kwargs['file'] = sys.stderr\n\nif not quiet:\n    print_(out, **kwargs)", "path": "speedtest-cli/speedtest.py", "commit_date": "2019-08-22 00:00:00", "repo_name": "sivel/speedtest-cli", "stars": 13209, "license": "apache-2.0", "language": "python", "size": 312}
{"docstring": "\"\"\"Return dictionary of result data\"\"\"\n\n", "func_signal": "def dict(self):\n", "code": "return {\n    'download': self.download,\n    'upload': self.upload,\n    'ping': self.ping,\n    'server': self.server,\n    'timestamp': self.timestamp,\n    'bytes_sent': self.bytes_sent,\n    'bytes_received': self.bytes_received,\n    'share': self._share,\n    'client': self.client,\n}", "path": "speedtest-cli/speedtest.py", "commit_date": "2019-08-22 00:00:00", "repo_name": "sivel/speedtest-cli", "stars": 13209, "license": "apache-2.0", "language": "python", "size": 312}
{"docstring": "\"\"\"Connect to the host and port specified in __init__.\"\"\"\n", "func_signal": "def connect(self):\n", "code": "try:\n    self.sock = socket.create_connection(\n        (self.host, self.port),\n        self.timeout,\n        self.source_address\n    )\nexcept (AttributeError, TypeError):\n    self.sock = create_connection(\n        (self.host, self.port),\n        self.timeout,\n        self.source_address\n    )\n\nif self._tunnel_host:\n    self._tunnel()", "path": "speedtest-cli/speedtest.py", "commit_date": "2019-08-22 00:00:00", "repo_name": "sivel/speedtest-cli", "stars": 13209, "license": "apache-2.0", "language": "python", "size": 312}
{"docstring": "\"\"\"Test upload speed against speedtest.net\n\nA ``threads`` value of ``None`` will fall back to those dictated\nby the speedtest.net configuration\n\"\"\"\n\n", "func_signal": "def upload(self, callback=do_nothing, pre_allocate=True, threads=None):\n", "code": "sizes = []\n\nfor size in self.config['sizes']['upload']:\n    for _ in range(0, self.config['counts']['upload']):\n        sizes.append(size)\n\n# request_count = len(sizes)\nrequest_count = self.config['upload_max']\n\nrequests = []\nfor i, size in enumerate(sizes):\n    # We set ``0`` for ``start`` and handle setting the actual\n    # ``start`` in ``HTTPUploader`` to get better measurements\n    data = HTTPUploaderData(\n        size,\n        0,\n        self.config['length']['upload'],\n        shutdown_event=self._shutdown_event\n    )\n    if pre_allocate:\n        data.pre_allocate()\n\n    headers = {'Content-length': size}\n    requests.append(\n        (\n            build_request(self.best['url'], data, secure=self._secure,\n                          headers=headers),\n            size\n        )\n    )\n\nmax_threads = threads or self.config['threads']['upload']\nin_flight = {'threads': 0}\n\ndef producer(q, requests, request_count):\n    for i, request in enumerate(requests[:request_count]):\n        thread = HTTPUploader(\n            i,\n            request[0],\n            start,\n            request[1],\n            self.config['length']['upload'],\n            opener=self._opener,\n            shutdown_event=self._shutdown_event\n        )\n        while in_flight['threads'] >= max_threads:\n            timeit.time.sleep(0.001)\n        thread.start()\n        q.put(thread, True)\n        in_flight['threads'] += 1\n        callback(i, request_count, start=True)\n\nfinished = []\n\ndef consumer(q, request_count):\n    _is_alive = thread_is_alive\n    while len(finished) < request_count:\n        thread = q.get(True)\n        while _is_alive(thread):\n            thread.join(timeout=0.001)\n        in_flight['threads'] -= 1\n        finished.append(thread.result)\n        callback(thread.i, request_count, end=True)\n\nq = Queue(threads or self.config['threads']['upload'])\nprod_thread = threading.Thread(target=producer,\n                               args=(q, requests, request_count))\ncons_thread = threading.Thread(target=consumer,\n                               args=(q, request_count))\nstart = timeit.default_timer()\nprod_thread.start()\ncons_thread.start()\n_is_alive = thread_is_alive\nwhile _is_alive(prod_thread):\n    prod_thread.join(timeout=0.1)\nwhile _is_alive(cons_thread):\n    cons_thread.join(timeout=0.1)\n\nstop = timeit.default_timer()\nself.results.bytes_sent = sum(finished)\nself.results.upload = (\n    (self.results.bytes_sent / (stop - start)) * 8.0\n)\nreturn self.results.upload", "path": "speedtest-cli/speedtest.py", "commit_date": "2019-08-22 00:00:00", "repo_name": "sivel/speedtest-cli", "stars": 13209, "license": "apache-2.0", "language": "python", "size": 312}
{"docstring": "\"\"\"Built in callback function used by Thread classes for printing\nstatus\n\"\"\"\n", "func_signal": "def print_dots(shutdown_event):\n", "code": "def inner(current, total, start=False, end=False):\n    if shutdown_event.isSet():\n        return\n\n    sys.stdout.write('.')\n    if current + 1 == total and end is True:\n        sys.stdout.write('\\n')\n    sys.stdout.flush()\nreturn inner", "path": "speedtest-cli/speedtest.py", "commit_date": "2019-08-22 00:00:00", "repo_name": "sivel/speedtest-cli", "stars": 13209, "license": "apache-2.0", "language": "python", "size": 312}
{"docstring": "# response doesn't support tell() and read(), required by\n# GzipFile\n", "func_signal": "def __init__(self, response):\n", "code": "if not gzip:\n    raise SpeedtestHTTPError('HTTP response body is gzip encoded, '\n                             'but gzip support is not available')\nIO = BytesIO or StringIO\nself.io = IO()\nwhile 1:\n    chunk = response.read(1024)\n    if len(chunk) == 0:\n        break\n    self.io.write(chunk)\nself.io.seek(0)\ngzip.GzipFile.__init__(self, mode='rb', fileobj=self.io)", "path": "speedtest-cli/speedtest.py", "commit_date": "2019-08-22 00:00:00", "repo_name": "sivel/speedtest-cli", "stars": 13209, "license": "apache-2.0", "language": "python", "size": 312}
{"docstring": "\"\"\"Perform a speedtest.net \"ping\" to determine which speedtest.net\nserver has the lowest latency\n\"\"\"\n\n", "func_signal": "def get_best_server(self, servers=None):\n", "code": "if not servers:\n    if not self.closest:\n        servers = self.get_closest_servers()\n    servers = self.closest\n\nif self._source_address:\n    source_address_tuple = (self._source_address, 0)\nelse:\n    source_address_tuple = None\n\nuser_agent = build_user_agent()\n\nresults = {}\nfor server in servers:\n    cum = []\n    url = os.path.dirname(server['url'])\n    stamp = int(timeit.time.time() * 1000)\n    latency_url = '%s/latency.txt?x=%s' % (url, stamp)\n    for i in range(0, 3):\n        this_latency_url = '%s.%s' % (latency_url, i)\n        printer('%s %s' % ('GET', this_latency_url),\n                debug=True)\n        urlparts = urlparse(latency_url)\n        try:\n            if urlparts[0] == 'https':\n                h = SpeedtestHTTPSConnection(\n                    urlparts[1],\n                    source_address=source_address_tuple\n                )\n            else:\n                h = SpeedtestHTTPConnection(\n                    urlparts[1],\n                    source_address=source_address_tuple\n                )\n            headers = {'User-Agent': user_agent}\n            path = '%s?%s' % (urlparts[2], urlparts[4])\n            start = timeit.default_timer()\n            h.request(\"GET\", path, headers=headers)\n            r = h.getresponse()\n            total = (timeit.default_timer() - start)\n        except HTTP_ERRORS:\n            e = get_exception()\n            printer('ERROR: %r' % e, debug=True)\n            cum.append(3600)\n            continue\n\n        text = r.read(9)\n        if int(r.status) == 200 and text == 'test=test'.encode():\n            cum.append(total)\n        else:\n            cum.append(3600)\n        h.close()\n\n    avg = round((sum(cum) / 6) * 1000.0, 3)\n    results[avg] = server\n\ntry:\n    fastest = sorted(results.keys())[0]\nexcept IndexError:\n    raise SpeedtestBestServerFailure('Unable to connect to servers to '\n                                     'test latency.')\nbest = results[fastest]\nbest['latency'] = fastest\n\nself.results.ping = fastest\nself.results.server = best\n\nself._best.update(best)\nprinter('Best Server:\\n%r' % best, debug=True)\nreturn best", "path": "speedtest-cli/speedtest.py", "commit_date": "2019-08-22 00:00:00", "repo_name": "sivel/speedtest-cli", "stars": 13209, "license": "apache-2.0", "language": "python", "size": 312}
{"docstring": "\"\"\"Return data in JSON format\"\"\"\n\n", "func_signal": "def json(self, pretty=False):\n", "code": "kwargs = {}\nif pretty:\n    kwargs.update({\n        'indent': 4,\n        'sort_keys': True\n    })\nreturn json.dumps(self.dict(), **kwargs)", "path": "speedtest-cli/speedtest.py", "commit_date": "2019-08-22 00:00:00", "repo_name": "sivel/speedtest-cli", "stars": 13209, "license": "apache-2.0", "language": "python", "size": 312}
{"docstring": "\"\"\"Helper function to return either a Gzip reader if\n``Content-Encoding`` is ``gzip`` otherwise the response itself\n\n\"\"\"\n\n", "func_signal": "def get_response_stream(response):\n", "code": "try:\n    getheader = response.headers.getheader\nexcept AttributeError:\n    getheader = response.getheader\n\nif getheader('content-encoding') == 'gzip':\n    return GzipDecodedResponse(response)\n\nreturn response", "path": "speedtest-cli/speedtest.py", "commit_date": "2019-08-22 00:00:00", "repo_name": "sivel/speedtest-cli", "stars": 13209, "license": "apache-2.0", "language": "python", "size": 312}
{"docstring": "\"\"\"Check if an argument was provided that depends on a module that may\nnot be part of the Python standard library.\n\nIf such an argument is supplied, and the module does not exist, exit\nwith an error stating which module is missing.\n\"\"\"\n", "func_signal": "def validate_optional_args(args):\n", "code": "optional_args = {\n    'json': ('json/simplejson python module', json),\n    'secure': ('SSL support', HTTPSConnection),\n}\n\nfor arg, info in optional_args.items():\n    if getattr(args, arg, False) and info[1] is None:\n        raise SystemExit('%s is not installed. --%s is '\n                         'unavailable' % (info[0], arg))", "path": "speedtest-cli/speedtest.py", "commit_date": "2019-08-22 00:00:00", "repo_name": "sivel/speedtest-cli", "stars": 13209, "license": "apache-2.0", "language": "python", "size": 312}
{"docstring": "\"\"\"Function to handle building and parsing of command line arguments\"\"\"\n", "func_signal": "def parse_args():\n", "code": "description = (\n    'Command line interface for testing internet bandwidth using '\n    'speedtest.net.\\n'\n    '------------------------------------------------------------'\n    '--------------\\n'\n    'https://github.com/sivel/speedtest-cli')\n\nparser = ArgParser(description=description)\n# Give optparse.OptionParser an `add_argument` method for\n# compatibility with argparse.ArgumentParser\ntry:\n    parser.add_argument = parser.add_option\nexcept AttributeError:\n    pass\nparser.add_argument('--no-download', dest='download', default=True,\n                    action='store_const', const=False,\n                    help='Do not perform download test')\nparser.add_argument('--no-upload', dest='upload', default=True,\n                    action='store_const', const=False,\n                    help='Do not perform upload test')\nparser.add_argument('--single', default=False, action='store_true',\n                    help='Only use a single connection instead of '\n                         'multiple. This simulates a typical file '\n                         'transfer.')\nparser.add_argument('--bytes', dest='units', action='store_const',\n                    const=('byte', 8), default=('bit', 1),\n                    help='Display values in bytes instead of bits. Does '\n                         'not affect the image generated by --share, nor '\n                         'output from --json or --csv')\nparser.add_argument('--share', action='store_true',\n                    help='Generate and provide a URL to the speedtest.net '\n                         'share results image, not displayed with --csv')\nparser.add_argument('--simple', action='store_true', default=False,\n                    help='Suppress verbose output, only show basic '\n                         'information')\nparser.add_argument('--csv', action='store_true', default=False,\n                    help='Suppress verbose output, only show basic '\n                         'information in CSV format. Speeds listed in '\n                         'bit/s and not affected by --bytes')\nparser.add_argument('--csv-delimiter', default=',', type=PARSER_TYPE_STR,\n                    help='Single character delimiter to use in CSV '\n                         'output. Default \",\"')\nparser.add_argument('--csv-header', action='store_true', default=False,\n                    help='Print CSV headers')\nparser.add_argument('--json', action='store_true', default=False,\n                    help='Suppress verbose output, only show basic '\n                         'information in JSON format. Speeds listed in '\n                         'bit/s and not affected by --bytes')\nparser.add_argument('--list', action='store_true',\n                    help='Display a list of speedtest.net servers '\n                         'sorted by distance')\nparser.add_argument('--server', type=PARSER_TYPE_INT, action='append',\n                    help='Specify a server ID to test against. Can be '\n                         'supplied multiple times')\nparser.add_argument('--exclude', type=PARSER_TYPE_INT, action='append',\n                    help='Exclude a server from selection. Can be '\n                         'supplied multiple times')\nparser.add_argument('--mini', help='URL of the Speedtest Mini server')\nparser.add_argument('--source', help='Source IP address to bind to')\nparser.add_argument('--timeout', default=10, type=PARSER_TYPE_FLOAT,\n                    help='HTTP timeout in seconds. Default 10')\nparser.add_argument('--secure', action='store_true',\n                    help='Use HTTPS instead of HTTP when communicating '\n                         'with speedtest.net operated servers')\nparser.add_argument('--no-pre-allocate', dest='pre_allocate',\n                    action='store_const', default=True, const=False,\n                    help='Do not pre allocate upload data. Pre allocation '\n                         'is enabled by default to improve upload '\n                         'performance. To support systems with '\n                         'insufficient memory, use this option to avoid a '\n                         'MemoryError')\nparser.add_argument('--version', action='store_true',\n                    help='Show the version number and exit')\nparser.add_argument('--debug', action='store_true',\n                    help=ARG_SUPPRESS, default=ARG_SUPPRESS)\n\noptions = parser.parse_args()\nif isinstance(options, tuple):\n    args = options[0]\nelse:\n    args = options\nreturn args", "path": "speedtest-cli/speedtest.py", "commit_date": "2019-08-22 00:00:00", "repo_name": "sivel/speedtest-cli", "stars": 13209, "license": "apache-2.0", "language": "python", "size": 312}
{"docstring": "\"\"\"Limit servers to the closest speedtest.net servers based on\ngeographic distance\n\"\"\"\n\n", "func_signal": "def get_closest_servers(self, limit=5):\n", "code": "if not self.servers:\n    self.get_servers()\n\nfor d in sorted(self.servers.keys()):\n    for s in self.servers[d]:\n        self.closest.append(s)\n        if len(self.closest) == limit:\n            break\n    else:\n        continue\n    break\n\nprinter('Closest Servers:\\n%r' % self.closest, debug=True)\nreturn self.closest", "path": "speedtest-cli/speedtest.py", "commit_date": "2019-08-22 00:00:00", "repo_name": "sivel/speedtest-cli", "stars": 13209, "license": "apache-2.0", "language": "python", "size": 312}
{"docstring": "\"\"\"Return data in CSV format\"\"\"\n\n", "func_signal": "def csv(self, delimiter=','):\n", "code": "data = self.dict()\nout = StringIO()\nwriter = csv.writer(out, delimiter=delimiter, lineterminator='')\nrow = [data['server']['id'], data['server']['sponsor'],\n       data['server']['name'], data['timestamp'],\n       data['server']['d'], data['ping'], data['download'],\n       data['upload'], self._share or '', self.client['ip']]\nwriter.writerow([to_utf8(v) for v in row])\nreturn out.getvalue()", "path": "speedtest-cli/speedtest.py", "commit_date": "2019-08-22 00:00:00", "repo_name": "sivel/speedtest-cli", "stars": 13209, "license": "apache-2.0", "language": "python", "size": 312}
{"docstring": "\"\"\"Build a Mozilla/5.0 compatible User-Agent string\"\"\"\n\n", "func_signal": "def build_user_agent():\n", "code": "ua_tuple = (\n    'Mozilla/5.0',\n    '(%s; U; %s; en-us)' % (platform.platform(),\n                            platform.architecture()[0]),\n    'Python/%s' % platform.python_version(),\n    '(KHTML, like Gecko)',\n    'speedtest-cli/%s' % __version__\n)\nuser_agent = ' '.join(ua_tuple)\nprinter('User-Agent: %s' % user_agent, debug=True)\nreturn user_agent", "path": "speedtest-cli/speedtest.py", "commit_date": "2019-08-22 00:00:00", "repo_name": "sivel/speedtest-cli", "stars": 13209, "license": "apache-2.0", "language": "python", "size": 312}
{"docstring": "\"\"\"Return CSV Headers\"\"\"\n\n", "func_signal": "def csv_header(delimiter=','):\n", "code": "row = ['Server ID', 'Sponsor', 'Server Name', 'Timestamp', 'Distance',\n       'Ping', 'Download', 'Upload', 'Share', 'IP Address']\nout = StringIO()\nwriter = csv.writer(out, delimiter=delimiter, lineterminator='')\nwriter.writerow([to_utf8(v) for v in row])\nreturn out.getvalue()", "path": "speedtest-cli/speedtest.py", "commit_date": "2019-08-22 00:00:00", "repo_name": "sivel/speedtest-cli", "stars": 13209, "license": "apache-2.0", "language": "python", "size": 312}
{"docstring": "\"\"\"POST data to the speedtest.net API to obtain a share results\nlink\n\"\"\"\n\n", "func_signal": "def share(self):\n", "code": "if self._share:\n    return self._share\n\ndownload = int(round(self.download / 1000.0, 0))\nping = int(round(self.ping, 0))\nupload = int(round(self.upload / 1000.0, 0))\n\n# Build the request to send results back to speedtest.net\n# We use a list instead of a dict because the API expects parameters\n# in a certain order\napi_data = [\n    'recommendedserverid=%s' % self.server['id'],\n    'ping=%s' % ping,\n    'screenresolution=',\n    'promo=',\n    'download=%s' % download,\n    'screendpi=',\n    'upload=%s' % upload,\n    'testmethod=http',\n    'hash=%s' % md5(('%s-%s-%s-%s' %\n                     (ping, upload, download, '297aae72'))\n                    .encode()).hexdigest(),\n    'touchscreen=none',\n    'startmode=pingselect',\n    'accuracy=1',\n    'bytesreceived=%s' % self.bytes_received,\n    'bytessent=%s' % self.bytes_sent,\n    'serverid=%s' % self.server['id'],\n]\n\nheaders = {'Referer': 'http://c.speedtest.net/flash/speedtest.swf'}\nrequest = build_request('://www.speedtest.net/api/api.php',\n                        data='&'.join(api_data).encode(),\n                        headers=headers, secure=self._secure)\nf, e = catch_request(request, opener=self._opener)\nif e:\n    raise ShareResultsConnectFailure(e)\n\nresponse = f.read()\ncode = f.code\nf.close()\n\nif int(code) != 200:\n    raise ShareResultsSubmitFailure('Could not submit results to '\n                                    'speedtest.net')\n\nqsargs = parse_qs(response.decode())\nresultid = qsargs.get('resultid')\nif not resultid or len(resultid) != 1:\n    raise ShareResultsSubmitFailure('Could not submit results to '\n                                    'speedtest.net')\n\nself._share = 'http://www.speedtest.net/result/%s.png' % resultid[0]\n\nreturn self._share", "path": "speedtest-cli/speedtest.py", "commit_date": "2019-08-22 00:00:00", "repo_name": "sivel/speedtest-cli", "stars": 13209, "license": "apache-2.0", "language": "python", "size": 312}
{"docstring": "\"\"\"Determine distance between 2 sets of [lat,lon] in km\"\"\"\n\n", "func_signal": "def distance(origin, destination):\n", "code": "lat1, lon1 = origin\nlat2, lon2 = destination\nradius = 6371  # km\n\ndlat = math.radians(lat2 - lat1)\ndlon = math.radians(lon2 - lon1)\na = (math.sin(dlat / 2) * math.sin(dlat / 2) +\n     math.cos(math.radians(lat1)) *\n     math.cos(math.radians(lat2)) * math.sin(dlon / 2) *\n     math.sin(dlon / 2))\nc = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\nd = radius * c\n\nreturn d", "path": "speedtest-cli/speedtest.py", "commit_date": "2019-08-22 00:00:00", "repo_name": "sivel/speedtest-cli", "stars": 13209, "license": "apache-2.0", "language": "python", "size": 312}
{"docstring": "\"\"\"Print the version\"\"\"\n\n", "func_signal": "def version():\n", "code": "printer('speedtest-cli %s' % __version__)\nprinter('Python %s' % sys.version.replace('\\n', ''))\nsys.exit(0)", "path": "speedtest-cli/speedtest.py", "commit_date": "2019-08-22 00:00:00", "repo_name": "sivel/speedtest-cli", "stars": 13209, "license": "apache-2.0", "language": "python", "size": 312}
{"docstring": "\"\"\"\nAn object to facilitate agent training and evaluation.\n\nParameters\n----------\nagent : :class:`AgentBase` instance\n    The agent to train.\nenv : ``gym.wrappers`` or ``gym.envs`` instance\n    The environment to run the agent on.\n\"\"\"\n", "func_signal": "def __init__(self, agent, env):\n", "code": "self.env = env\nself.agent = agent\nself.rewards = {\"total\": [], \"smooth_total\": [], \"n_steps\": [], \"duration\": []}", "path": "numpy-ml/numpy_ml/rl_models/trainer.py", "commit_date": "2019-08-12 00:00:00", "repo_name": "ddbourgin/numpy-ml", "stars": 14486, "license": "gpl-3.0", "language": "python", "size": 10514}
{"docstring": "\"\"\"\nCreate `n_trees`-worth of bootstrapped samples from the training data\nand use each to fit a separate decision tree.\n\"\"\"\n", "func_signal": "def fit(self, X, Y):\n", "code": "self.trees = []\nfor _ in range(self.n_trees):\n    X_samp, Y_samp = bootstrap_sample(X, Y)\n    tree = DecisionTree(\n        n_feats=self.n_feats,\n        max_depth=self.max_depth,\n        criterion=self.criterion,\n        classifier=self.classifier,\n    )\n    tree.fit(X_samp, Y_samp)\n    self.trees.append(tree)", "path": "numpy-ml/numpy_ml/trees/rf.py", "commit_date": "2019-08-13 00:00:00", "repo_name": "ddbourgin/numpy-ml", "stars": 14486, "license": "gpl-3.0", "language": "python", "size": 10514}
{"docstring": "\"\"\"\nEntropy of a label sequence\n\"\"\"\n", "func_signal": "def entropy(y):\n", "code": "hist = np.bincount(y)\nps = hist / np.sum(hist)\nreturn -np.sum([p * np.log2(p) for p in ps if p > 0])", "path": "numpy-ml/numpy_ml/trees/dt.py", "commit_date": "2020-01-01 00:00:00", "repo_name": "ddbourgin/numpy-ml", "stars": 14486, "license": "gpl-3.0", "language": "python", "size": 10514}
{"docstring": "\"\"\"\nPredict the target value for each entry in `X`.\n\nParameters\n----------\nX : :py:class:`ndarray <numpy.ndarray>` of shape `(N, M)`\n    The training data of `N` examples, each with `M` features.\n\nReturns\n-------\ny_pred : :py:class:`ndarray <numpy.ndarray>` of shape `(N,)`\n    Model predictions for each entry in `X`.\n\"\"\"\n", "func_signal": "def predict(self, X):\n", "code": "tree_preds = np.array([[t._traverse(x, t.root) for x in X] for t in self.trees])\nreturn self._vote(tree_preds)", "path": "numpy-ml/numpy_ml/trees/rf.py", "commit_date": "2019-08-13 00:00:00", "repo_name": "ddbourgin/numpy-ml", "stars": 14486, "license": "gpl-3.0", "language": "python", "size": 10514}
{"docstring": "# (batch, input_size, seq_len) -> (seq_len, batch, input_size)\n", "func_signal": "def forward(self, X):\n", "code": "self.X = np.moveaxis(X, [0, 1, 2], [-2, -1, -3])\n\nif not isinstance(self.X, torch.Tensor):\n    self.X = torchify(self.X)\n\nself.X.retain_grad()\n\n# initial hidden state is 0\nn_ex, n_in, n_timesteps = self.X.shape\nn_out, n_out = self.layer1.weight_hh_l0.shape\n\n# forward pass\nself.A, (At, Ct) = self.layer1(self.X)\nself.A.retain_grad()\nreturn self.A", "path": "numpy-ml/numpy_ml/tests/nn_torch_models.py", "commit_date": "2020-06-20 00:00:00", "repo_name": "ddbourgin/numpy-ml", "stars": 14486, "license": "gpl-3.0", "language": "python", "size": 10514}
{"docstring": "\"\"\"\nUse the trained decision tree to return the class probabilities for the\nexamples in `X`.\n\nParameters\n----------\nX : :py:class:`ndarray <numpy.ndarray>` of shape `(N, M)`\n    The training data of `N` examples, each with `M` features\n\nReturns\n-------\npreds : :py:class:`ndarray <numpy.ndarray>` of shape `(N, n_classes)`\n    The class probabilities predicted for each example in `X`.\n\"\"\"\n", "func_signal": "def predict_class_probs(self, X):\n", "code": "assert self.classifier, \"`predict_class_probs` undefined for classifier = False\"\nreturn np.array([self._traverse(x, self.root, prob=True) for x in X])", "path": "numpy-ml/numpy_ml/trees/dt.py", "commit_date": "2020-01-01 00:00:00", "repo_name": "ddbourgin/numpy-ml", "stars": 14486, "license": "gpl-3.0", "language": "python", "size": 10514}
{"docstring": "\"\"\"\nGini impurity (local entropy) of a label sequence\n\"\"\"\n", "func_signal": "def gini(y):\n", "code": "hist = np.bincount(y)\nN = np.sum(hist)\nreturn 1 - sum([(i / N) ** 2 for i in hist])", "path": "numpy-ml/numpy_ml/trees/dt.py", "commit_date": "2020-01-01 00:00:00", "repo_name": "ddbourgin/numpy-ml", "stars": 14486, "license": "gpl-3.0", "language": "python", "size": 10514}
{"docstring": "# (N, W, C) -> (N, C, W)\n", "func_signal": "def forward(self, X):\n", "code": "self.X = np.moveaxis(X, [0, 1, 2], [0, -1, -2])\nif not isinstance(self.X, torch.Tensor):\n    self.X = torchify(self.X)\n\nself.X.retain_grad()\n\nself.Z = self.layer1(self.X)\nself.Z.retain_grad()\n\nself.Y = self.act_fn(self.Z)\nself.Y.retain_grad()\nreturn self.Y", "path": "numpy-ml/numpy_ml/tests/nn_torch_models.py", "commit_date": "2020-06-20 00:00:00", "repo_name": "ddbourgin/numpy-ml", "stars": 14486, "license": "gpl-3.0", "language": "python", "size": 10514}
{"docstring": "# (N, W, C) -> (N, C, W)\n", "func_signal": "def forward(self, X_main, X_skip):\n", "code": "self.X_main = np.moveaxis(X_main, [0, 1, 2], [0, -1, -2])\nself.X_main = torchify(self.X_main)\nself.X_main.retain_grad()\n\nself.conv_dilation_out = self.conv_dilation(self.X_main)\nself.conv_dilation_out.retain_grad()\n\nself.tanh_out = torch.tanh(self.conv_dilation_out)\nself.sigm_out = torch.sigmoid(self.conv_dilation_out)\n\nself.tanh_out.retain_grad()\nself.sigm_out.retain_grad()\n\nself.multiply_gate_out = self.tanh_out * self.sigm_out\nself.multiply_gate_out.retain_grad()\n\nself.conv_1x1_out = self.conv_1x1(self.multiply_gate_out)\nself.conv_1x1_out.retain_grad()\n\nself.X_skip = torch.zeros_like(self.conv_1x1_out)\nif X_skip is not None:\n    self.X_skip = torchify(np.moveaxis(X_skip, [0, 1, 2], [0, -1, -2]))\nself.X_skip.retain_grad()\n\nself.Y_skip = self.X_skip + self.conv_1x1_out\nself.Y_main = self.X_main + self.conv_1x1_out\n\nself.Y_skip.retain_grad()\nself.Y_main.retain_grad()", "path": "numpy-ml/numpy_ml/tests/nn_torch_models.py", "commit_date": "2020-06-20 00:00:00", "repo_name": "ddbourgin/numpy-ml", "stars": 14486, "license": "gpl-3.0", "language": "python", "size": 10514}
{"docstring": "# (N, H, W, C) -> (N, C, H, W)\n", "func_signal": "def forward(self, X):\n", "code": "if X.ndim == 4:\n    X = np.moveaxis(X, [0, 1, 2, 3], [0, -2, -1, -3])\n\nif not isinstance(X, torch.Tensor):\n    X = torchify(X)\n\nself.X = X\nself.Y = self.layer1(self.X)\nself.Y.retain_grad()", "path": "numpy-ml/numpy_ml/tests/nn_torch_models.py", "commit_date": "2020-06-20 00:00:00", "repo_name": "ddbourgin/numpy-ml", "stars": 14486, "license": "gpl-3.0", "language": "python", "size": 10514}
{"docstring": "# (N, H, W, C) -> (N, C, H, W)\n", "func_signal": "def forward(self, X):\n", "code": "if X.ndim == 4:\n    X = np.moveaxis(X, [0, 1, 2, 3], [0, -2, -1, -3])\n\nif not isinstance(X, torch.Tensor):\n    X = torchify(X)\n\nself.X = X\nself.Y = self.layer1(self.X)\nself.Y.retain_grad()", "path": "numpy-ml/numpy_ml/tests/nn_torch_models.py", "commit_date": "2020-06-20 00:00:00", "repo_name": "ddbourgin/numpy-ml", "stars": 14486, "license": "gpl-3.0", "language": "python", "size": 10514}
{"docstring": "# Generate some fake data\n", "func_signal": "def generate_corpus():\n", "code": "D = 300\nT = 10\nV = 30\nN = np.random.randint(150, 200, size=D)\n\n# Create a document-topic distribution for 3 different types of documents\nalpha1 = np.array((20, 15, 10, 1, 1, 1, 1, 1, 1, 1))\nalpha2 = np.array((1, 1, 1, 10, 15, 20, 1, 1, 1, 1))\nalpha3 = np.array((1, 1, 1, 1, 1, 1, 10, 12, 15, 18))\n\n# Arbitrarily choose each topic to have 3 very common, diagnostic words\n# These words are barely shared with any other topic\nbeta_probs = (\n    np.ones((V, T)) + np.array([np.arange(V) % T == t for t in range(T)]).T * 19\n)\nbeta_gen = np.array(list(map(lambda x: np.random.dirichlet(x), beta_probs.T))).T\n\ncorpus = []\ntheta = np.empty((D, T))\n\n# Generate each document from the LDA model\nfor d in range(D):\n\n    # Draw topic distribution for the document\n    if d < (D / 3):\n        theta[d, :] = np.random.dirichlet(alpha1, 1)[0]\n    elif d < 2 * (D / 3):\n        theta[d, :] = np.random.dirichlet(alpha2, 1)[0]\n    else:\n        theta[d, :] = np.random.dirichlet(alpha3, 1)[0]\n\n    doc = np.array([])\n    for n in range(N[d]):\n        # Draw a topic according to the document's topic distribution\n        z_n = np.random.choice(np.arange(T), p=theta[d, :])\n\n        # Draw a word according to the topic-word distribution\n        w_n = np.random.choice(np.arange(V), p=beta_gen[:, z_n])\n        doc = np.append(doc, w_n)\n\n    corpus.append(doc)\nreturn corpus, T", "path": "numpy-ml/numpy_ml/plots/lda_plots.py", "commit_date": "2020-06-20 00:00:00", "repo_name": "ddbourgin/numpy-ml", "stars": 14486, "license": "gpl-3.0", "language": "python", "size": 10514}
{"docstring": "# (N, H, W, C) -> (N, C, H, W)\n", "func_signal": "def forward(self, X):\n", "code": "self.X = np.moveaxis(X, [0, 1, 2, 3], [0, -2, -1, -3])\nif not isinstance(self.X, torch.Tensor):\n    self.X = torchify(self.X)\n\nself.X.retain_grad()\nself.Y = self.layer1(self.X)\nself.Y.retain_grad()\nreturn self.Y", "path": "numpy-ml/numpy_ml/tests/nn_torch_models.py", "commit_date": "2020-06-20 00:00:00", "repo_name": "ddbourgin/numpy-ml", "stars": 14486, "license": "gpl-3.0", "language": "python", "size": 10514}
{"docstring": "\"\"\"\nFind the optimal split rule (feature index and split threshold) for the\ndata according to `self.criterion`.\n\"\"\"\n", "func_signal": "def _segment(self, X, Y, feat_idxs):\n", "code": "best_gain = -np.inf\nsplit_idx, split_thresh = None, None\nfor i in feat_idxs:\n    vals = X[:, i]\n    levels = np.unique(vals)\n    thresholds = (levels[:-1] + levels[1:]) / 2 if len(levels) > 1 else levels\n    gains = np.array([self._impurity_gain(Y, t, vals) for t in thresholds])\n\n    if gains.max() > best_gain:\n        split_idx = i\n        best_gain = gains.max()\n        split_thresh = thresholds[gains.argmax()]\n\nreturn split_idx, split_thresh", "path": "numpy-ml/numpy_ml/trees/dt.py", "commit_date": "2020-01-01 00:00:00", "repo_name": "ddbourgin/numpy-ml", "stars": 14486, "license": "gpl-3.0", "language": "python", "size": 10514}
{"docstring": "\"\"\"\nFit a binary decision tree to a dataset.\n\nParameters\n----------\nX : :py:class:`ndarray <numpy.ndarray>` of shape `(N, M)`\n    The training data of `N` examples, each with `M` features\nY : :py:class:`ndarray <numpy.ndarray>` of shape `(N,)`\n    An array of integer class labels for each example in `X` if\n    self.classifier = True, otherwise the set of target values for\n    each example in `X`.\n\"\"\"\n", "func_signal": "def fit(self, X, Y):\n", "code": "self.n_classes = max(Y) + 1 if self.classifier else None\nself.n_feats = X.shape[1] if not self.n_feats else min(self.n_feats, X.shape[1])\nself.root = self._grow(X, Y)", "path": "numpy-ml/numpy_ml/trees/dt.py", "commit_date": "2020-01-01 00:00:00", "repo_name": "ddbourgin/numpy-ml", "stars": 14486, "license": "gpl-3.0", "language": "python", "size": 10514}
{"docstring": "\"\"\"\nCompute the impurity gain associated with a given split.\n\nIG(split) = loss(parent) - weighted_avg[loss(left_child), loss(right_child)]\n\"\"\"\n", "func_signal": "def _impurity_gain(self, Y, split_thresh, feat_values):\n", "code": "if self.criterion == \"entropy\":\n    loss = entropy\nelif self.criterion == \"gini\":\n    loss = gini\nelif self.criterion == \"mse\":\n    loss = mse\n\nparent_loss = loss(Y)\n\n# generate split\nleft = np.argwhere(feat_values <= split_thresh).flatten()\nright = np.argwhere(feat_values > split_thresh).flatten()\n\nif len(left) == 0 or len(right) == 0:\n    return 0\n\n# compute the weighted avg. of the loss for the children\nn = len(Y)\nn_l, n_r = len(left), len(right)\ne_l, e_r = loss(Y[left]), loss(Y[right])\nchild_loss = (n_l / n) * e_l + (n_r / n) * e_r\n\n# impurity gain is difference in loss before vs. after split\nig = parent_loss - child_loss\nreturn ig", "path": "numpy-ml/numpy_ml/trees/dt.py", "commit_date": "2020-01-01 00:00:00", "repo_name": "ddbourgin/numpy-ml", "stars": 14486, "license": "gpl-3.0", "language": "python", "size": 10514}
{"docstring": "# if all labels are the same, return a leaf\n", "func_signal": "def _grow(self, X, Y, cur_depth=0):\n", "code": "if len(set(Y)) == 1:\n    if self.classifier:\n        prob = np.zeros(self.n_classes)\n        prob[Y[0]] = 1.0\n    return Leaf(prob) if self.classifier else Leaf(Y[0])\n\n# if we have reached max_depth, return a leaf\nif cur_depth >= self.max_depth:\n    v = np.mean(Y, axis=0)\n    if self.classifier:\n        v = np.bincount(Y, minlength=self.n_classes) / len(Y)\n    return Leaf(v)\n\ncur_depth += 1\nself.depth = max(self.depth, cur_depth)\n\nN, M = X.shape\nfeat_idxs = np.random.choice(M, self.n_feats, replace=False)\n\n# greedily select the best split according to `criterion`\nfeat, thresh = self._segment(X, Y, feat_idxs)\nl = np.argwhere(X[:, feat] <= thresh).flatten()\nr = np.argwhere(X[:, feat] > thresh).flatten()\n\n# grow the children that result from the split\nleft = self._grow(X[l, :], Y[l], cur_depth)\nright = self._grow(X[r, :], Y[r], cur_depth)\nreturn Node(left, right, (feat, thresh))", "path": "numpy-ml/numpy_ml/trees/dt.py", "commit_date": "2020-01-01 00:00:00", "repo_name": "ddbourgin/numpy-ml", "stars": 14486, "license": "gpl-3.0", "language": "python", "size": 10514}
{"docstring": "\"\"\"\nPlot the cumulative reward per episode as a function of episode number.\n\nNotes\n-----\nSaves plot to the file ``./img/<agent>-<env>.png``\n\nParameters\n----------\nrwd_greedy : float\n    The cumulative reward earned with a final execution of a greedy\n    target policy.\n\"\"\"\n", "func_signal": "def plot_rewards(self, rwd_greedy):\n", "code": "try:\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n\n    # https://seaborn.pydata.org/generated/seaborn.set_context.html\n    # https://seaborn.pydata.org/generated/seaborn.set_style.html\n    sns.set_style(\"white\")\n    sns.set_context(\"notebook\", font_scale=1)\nexcept:\n    fstr = \"Error importing `matplotlib` and `seaborn` -- plotting functionality is disabled\"\n    raise ImportError(fstr)\n\nR = self.rewards\nfig, ax = plt.subplots()\nx = np.arange(len(R[\"total\"]))\ny = R[\"smooth_total\"]\ny_raw = R[\"total\"]\n\nax.plot(x, y, label=\"smoothed\")\nax.plot(x, y_raw, alpha=0.5, label=\"raw\")\nax.axhline(y=rwd_greedy, xmin=min(x), xmax=max(x), ls=\":\", label=\"final greedy\")\nax.legend()\nsns.despine()\n\nenv = self.agent.env_info[\"id\"]\nagent = self.agent.hyperparameters[\"agent\"]\n\nax.set_xlabel(\"Episode\")\nax.set_ylabel(\"Cumulative reward\")\nax.set_title(\"{} on '{}'\".format(agent, env))\nplt.savefig(\"img/{}-{}.png\".format(agent, env))\nplt.close(\"all\")", "path": "numpy-ml/numpy_ml/rl_models/trainer.py", "commit_date": "2019-08-12 00:00:00", "repo_name": "ddbourgin/numpy-ml", "stars": 14486, "license": "gpl-3.0", "language": "python", "size": 10514}
{"docstring": "# (N, H, W, C) -> (N, C, H, W)\n", "func_signal": "def forward(self, X):\n", "code": "self.X = np.moveaxis(X, [0, 1, 2, 3], [0, -2, -1, -3])\nif not isinstance(self.X, torch.Tensor):\n    self.X = torchify(self.X)\n\nself.X.retain_grad()\n\nself.Z = self.layer1(self.X)\nself.Z.retain_grad()\n\nself.Y = self.act_fn(self.Z)\nself.Y.retain_grad()\nreturn self.Y", "path": "numpy-ml/numpy_ml/tests/nn_torch_models.py", "commit_date": "2020-06-20 00:00:00", "repo_name": "ddbourgin/numpy-ml", "stars": 14486, "license": "gpl-3.0", "language": "python", "size": 10514}
{"docstring": "# (N, H, W, C) -> (N, C, H, W)\n", "func_signal": "def forward(self, X):\n", "code": "self.X = np.moveaxis(X, [0, 1, 2, 3], [0, -2, -1, -3])\nif not isinstance(self.X, torch.Tensor):\n    self.X = torchify(self.X)\n\nself.X.retain_grad()\n\nself.Z = self.layer1(self.X)\nself.Z.retain_grad()\n\nself.Y = self.act_fn(self.Z)\nself.Y.retain_grad()\nreturn self.Y", "path": "numpy-ml/numpy_ml/tests/nn_torch_models.py", "commit_date": "2020-06-20 00:00:00", "repo_name": "ddbourgin/numpy-ml", "stars": 14486, "license": "gpl-3.0", "language": "python", "size": 10514}
{"docstring": "# First run one gradient descent step for Q.\n", "func_signal": "def update(data):\n", "code": "q_optimizer.zero_grad()\nloss_q, loss_info = compute_loss_q(data)\nloss_q.backward()\nq_optimizer.step()\n\n# Freeze Q-network so you don't waste computational effort \n# computing gradients for it during the policy learning step.\nfor p in ac.q.parameters():\n    p.requires_grad = False\n\n# Next run one gradient descent step for pi.\npi_optimizer.zero_grad()\nloss_pi = compute_loss_pi(data)\nloss_pi.backward()\npi_optimizer.step()\n\n# Unfreeze Q-network so you can optimize it at next DDPG step.\nfor p in ac.q.parameters():\n    p.requires_grad = True\n\n# Record things\nlogger.store(LossQ=loss_q.item(), LossPi=loss_pi.item(), **loss_info)\n\n# Finally, update target networks by polyak averaging.\nwith torch.no_grad():\n    for p, p_targ in zip(ac.parameters(), ac_targ.parameters()):\n        # NB: We use an in-place operations \"mul_\", \"add_\" to update target\n        # params, as opposed to \"mul\" and \"add\", which would make new tensors.\n        p_targ.data.mul_(polyak)\n        p_targ.data.add_((1 - polyak) * p.data)", "path": "spinningup/spinup/algos/pytorch/ddpg/ddpg.py", "commit_date": "2020-01-30 00:00:00", "repo_name": "openai/spinningup", "stars": 9460, "license": "mit", "language": "python", "size": 31762}
{"docstring": "# Create a default shorthand for the key, built from the first \n# three letters of each colon-separated part.\n# But if the first three letters contains something which isn't\n# alphanumeric, shear that off.\n", "func_signal": "def _default_shorthand(self, key):\n", "code": "valid_chars = \"%s%s\" % (string.ascii_letters, string.digits)\ndef shear(x):\n    return ''.join(z for z in x[:3] if z in valid_chars)\nsh = '-'.join([shear(x) for x in key.split(':')])\nreturn sh", "path": "spinningup/spinup/utils/run_utils.py", "commit_date": "2019-03-01 00:00:00", "repo_name": "openai/spinningup", "stars": 9460, "license": "mit", "language": "python", "size": 31762}
{"docstring": "# Produce action distributions for given observations, and \n# optionally compute the log likelihood of given actions under\n# those distributions.\n", "func_signal": "def forward(self, obs, act=None):\n", "code": "pi = self._distribution(obs)\nlogp_a = None\nif act is not None:\n    logp_a = self._log_prob_from_distribution(pi, act)\nreturn pi, logp_a", "path": "spinningup/spinup/algos/pytorch/vpg/core.py", "commit_date": "2020-01-30 00:00:00", "repo_name": "openai/spinningup", "stars": 9460, "license": "mit", "language": "python", "size": 31762}
{"docstring": "\"\"\" \nConvert a value or values to a string which could go in a filepath.\n\nPartly based on `this gist`_.\n\n.. _`this gist`: https://gist.github.com/seanh/93666\n\n\"\"\"\n", "func_signal": "def valid_str(v):\n", "code": "if hasattr(v, '__name__'):\n    return valid_str(v.__name__)\n\nif isinstance(v, tuple) or isinstance(v, list):\n    return '-'.join([valid_str(x) for x in v])\n\n# Valid characters are '-', '_', and alphanumeric. Replace invalid chars\n# with '-'. \nstr_v = str(v).lower()\nvalid_chars = \"-_%s%s\" % (string.ascii_letters, string.digits)\nstr_v = ''.join(c if c in valid_chars else '-' for c in str_v)\nreturn str_v", "path": "spinningup/spinup/utils/run_utils.py", "commit_date": "2019-03-01 00:00:00", "repo_name": "openai/spinningup", "stars": 9460, "license": "mit", "language": "python", "size": 31762}
{"docstring": "# Produce action distributions for given observations, and \n# optionally compute the log likelihood of given actions under\n# those distributions.\n", "func_signal": "def forward(self, obs, act=None):\n", "code": "pi = self._distribution(obs)\nlogp_a = None\nif act is not None:\n    logp_a = self._log_prob_from_distribution(pi, act)\nreturn pi, logp_a", "path": "spinningup/spinup/algos/pytorch/ppo/core.py", "commit_date": "2020-01-30 00:00:00", "repo_name": "openai/spinningup", "stars": 9460, "license": "mit", "language": "python", "size": 31762}
{"docstring": "\"\"\"\nBuilds symbols to sample actions and compute log-probs of actions.\n\nSpecial instructions: Make log_std a tf variable with the same shape as\nthe action vector, independent of x, initialized to [-0.5, -0.5, ..., -0.5].\n\nArgs:\n    x: Input tensor of states. Shape [batch, obs_dim].\n\n    a: Input tensor of actions. Shape [batch, act_dim].\n\n    hidden_sizes: Sizes of hidden layers for action network MLP.\n\n    activation: Activation function for all layers except last.\n\n    output_activation: Activation function for last layer (action layer).\n\n    action_space: A gym.spaces object describing the action space of the\n        environment this agent will interact with.\n\nReturns:\n    pi: A symbol for sampling stochastic actions from a Gaussian \n        distribution.\n\n    logp: A symbol for computing log-likelihoods of actions from a Gaussian \n        distribution.\n\n    logp_pi: A symbol for computing log-likelihoods of actions in pi from a \n        Gaussian distribution.\n\n\"\"\"\n#######################\n#                     #\n#   YOUR CODE HERE    #\n#                     #\n#######################\n# mu = \n# log_std = \n# pi = \n\n", "func_signal": "def mlp_gaussian_policy(x, a, hidden_sizes, activation, output_activation, action_space):\n", "code": "logp = exercise1_1.gaussian_likelihood(a, mu, log_std)\nlogp_pi = exercise1_1.gaussian_likelihood(pi, mu, log_std)\nreturn pi, logp, logp_pi", "path": "spinningup/spinup/exercises/tf1/problem_set_1/exercise1_2.py", "commit_date": "2020-01-30 00:00:00", "repo_name": "openai/spinningup", "stars": 9460, "license": "mit", "language": "python", "size": 31762}
{"docstring": "\"\"\"\nRecursively builds list of valid variants.\n\"\"\"\n", "func_signal": "def _variants(self, keys, vals):\n", "code": "if len(keys)==1:\n    pre_variants = [dict()]\nelse:\n    pre_variants = self._variants(keys[1:], vals[1:])\n\nvariants = []\nfor val in vals[0]:\n    for pre_v in pre_variants:\n        v = {}\n        v[keys[0]] = val\n        v.update(pre_v)\n        variants.append(v)\nreturn variants", "path": "spinningup/spinup/utils/run_utils.py", "commit_date": "2019-03-01 00:00:00", "repo_name": "openai/spinningup", "stars": 9460, "license": "mit", "language": "python", "size": 31762}
{"docstring": "\"\"\"\nAdd a parameter (key) to the grid config, with potential values (vals).\n\nBy default, if a shorthand isn't given, one is automatically generated\nfrom the key using the first three letters of each colon-separated\nterm. To disable this behavior, change ``DEFAULT_SHORTHAND`` in the\n``spinup/user_config.py`` file to ``False``. \n\nArgs:\n    key (string): Name of parameter.\n\n    vals (value or list of values): Allowed values of parameter.\n\n    shorthand (string): Optional, shortened name of parameter. For \n        example, maybe the parameter ``steps_per_epoch`` is shortened\n        to ``steps``. \n\n    in_name (bool): When constructing variant names, force the\n        inclusion of this parameter into the name.\n\"\"\"\n", "func_signal": "def add(self, key, vals, shorthand=None, in_name=False):\n", "code": "assert isinstance(key, str), \"Key must be a string.\"\nassert shorthand is None or isinstance(shorthand, str), \\\n    \"Shorthand must be a string.\"\nif not isinstance(vals, list):\n    vals = [vals]\nif DEFAULT_SHORTHAND and shorthand is None:\n    shorthand = self._default_shorthand(key)\nself.keys.append(key)\nself.vals.append(vals)\nself.shs.append(shorthand)\nself.in_names.append(in_name)", "path": "spinningup/spinup/utils/run_utils.py", "commit_date": "2019-03-01 00:00:00", "repo_name": "openai/spinningup", "stars": 9460, "license": "mit", "language": "python", "size": 31762}
{"docstring": "\"\"\"\nAppend one timestep of agent-environment interaction to the buffer.\n\"\"\"\n", "func_signal": "def store(self, obs, act, rew, val, logp):\n", "code": "assert self.ptr < self.max_size     # buffer has to have room so you can store\nself.obs_buf[self.ptr] = obs\nself.act_buf[self.ptr] = act\nself.rew_buf[self.ptr] = rew\nself.val_buf[self.ptr] = val\nself.logp_buf[self.ptr] = logp\nself.ptr += 1", "path": "spinningup/spinup/algos/tf1/vpg/vpg.py", "commit_date": "2020-01-30 00:00:00", "repo_name": "openai/spinningup", "stars": 9460, "license": "mit", "language": "python", "size": 31762}
{"docstring": "# Build a feedforward neural network.\n", "func_signal": "def mlp(sizes, activation=nn.Tanh, output_activation=nn.Identity):\n", "code": "layers = []\nfor j in range(len(sizes)-1):\n    act = activation if j < len(sizes)-2 else output_activation\n    layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\nreturn nn.Sequential(*layers)", "path": "spinningup/spinup/examples/pytorch/pg_math/1_simple_pg.py", "commit_date": "2020-01-30 00:00:00", "repo_name": "openai/spinningup", "stars": 9460, "license": "mit", "language": "python", "size": 31762}
{"docstring": "\"\"\" Convert obj to a version which can be serialized with JSON. \"\"\"\n", "func_signal": "def convert_json(obj):\n", "code": "if is_json_serializable(obj):\n    return obj\nelse:\n    if isinstance(obj, dict):\n        return {convert_json(k): convert_json(v) \n                for k,v in obj.items()}\n\n    elif isinstance(obj, tuple):\n        return (convert_json(x) for x in obj)\n\n    elif isinstance(obj, list):\n        return [convert_json(x) for x in obj]\n\n    elif hasattr(obj,'__name__') and not('lambda' in obj.__name__):\n        return convert_json(obj.__name__)\n\n    elif hasattr(obj,'__dict__') and obj.__dict__:\n        obj_dict = {convert_json(k): convert_json(v) \n                    for k,v in obj.__dict__.items()}\n        return {str(obj): obj_dict}\n\n    return str(obj)", "path": "spinningup/spinup/utils/serialization_utils.py", "commit_date": "2018-11-08 00:00:00", "repo_name": "openai/spinningup", "stars": 9460, "license": "mit", "language": "python", "size": 31762}
{"docstring": "\"\"\"\nRun each variant in the grid with function 'thunk'.\n\nNote: 'thunk' must be either a callable function, or a string. If it is\na string, it must be the name of a parameter whose values are all \ncallable functions.\n\nUses ``call_experiment`` to actually launch each experiment, and gives\neach variant a name using ``self.variant_name()``. \n\nMaintenance note: the args for ExperimentGrid.run should track closely\nto the args for call_experiment. However, ``seed`` is omitted because\nwe presume the user may add it as a parameter in the grid.\n\"\"\"\n\n# Print info about self.\n", "func_signal": "def run(self, thunk, num_cpu=1, data_dir=None, datestamp=False):\n", "code": "self.print()\n\n# Make the list of all variants.\nvariants = self.variants()\n\n# Print variant names for the user.\nvar_names = set([self.variant_name(var) for var in variants])\nvar_names = sorted(list(var_names))\nline = '='*DIV_LINE_WIDTH\npreparing = colorize('Preparing to run the following experiments...', \n                     color='green', bold=True)\njoined_var_names = '\\n'.join(var_names)\nannouncement = f\"\\n{preparing}\\n\\n{joined_var_names}\\n\\n{line}\"\nprint(announcement)\n\n\nif WAIT_BEFORE_LAUNCH > 0:\n    delay_msg = colorize(dedent(\"\"\"\n    Launch delayed to give you a few seconds to review your experiments.\n\n    To customize or disable this behavior, change WAIT_BEFORE_LAUNCH in\n    spinup/user_config.py.\n\n    \"\"\"), color='cyan', bold=True)+line\n    print(delay_msg)\n    wait, steps = WAIT_BEFORE_LAUNCH, 100\n    prog_bar = trange(steps, desc='Launching in...', \n                      leave=False, ncols=DIV_LINE_WIDTH, \n                      mininterval=0.25,\n                      bar_format='{desc}: {bar}| {remaining} {elapsed}')\n    for _ in prog_bar:\n        time.sleep(wait/steps)\n\n# Run the variants.\nfor var in variants:\n    exp_name = self.variant_name(var)\n\n    # Figure out what the thunk is.\n    if isinstance(thunk, str):\n        # Assume one of the variant parameters has the same\n        # name as the string you passed for thunk, and that \n        # variant[thunk] is a valid callable function.\n        thunk_ = var[thunk]\n        del var[thunk]\n    else:\n        # Assume thunk is given as a function.\n        thunk_ = thunk\n\n    call_experiment(exp_name, thunk_, num_cpu=num_cpu, \n                    data_dir=data_dir, datestamp=datestamp, **var)", "path": "spinningup/spinup/utils/run_utils.py", "commit_date": "2019-03-01 00:00:00", "repo_name": "openai/spinningup", "stars": 9460, "license": "mit", "language": "python", "size": 31762}
{"docstring": "\"\"\"\nCall this at the end of a trajectory, or when one gets cut off\nby an epoch ending. This looks back in the buffer to where the\ntrajectory started, and uses rewards and value estimates from\nthe whole trajectory to compute advantage estimates with GAE-Lambda,\nas well as compute the rewards-to-go for each state, to use as\nthe targets for the value function.\n\nThe \"last_val\" argument should be 0 if the trajectory ended\nbecause the agent reached a terminal state (died), and otherwise\nshould be V(s_T), the value function estimated for the last state.\nThis allows us to bootstrap the reward-to-go calculation to account\nfor timesteps beyond the arbitrary episode horizon (or epoch cutoff).\n\"\"\"\n\n", "func_signal": "def finish_path(self, last_val=0):\n", "code": "path_slice = slice(self.path_start_idx, self.ptr)\nrews = np.append(self.rew_buf[path_slice], last_val)\nvals = np.append(self.val_buf[path_slice], last_val)\n\n# the next two lines implement GAE-Lambda advantage calculation\ndeltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\nself.adv_buf[path_slice] = core.discount_cumsum(deltas, self.gamma * self.lam)\n\n# the next line computes rewards-to-go, to be targets for the value function\nself.ret_buf[path_slice] = core.discount_cumsum(rews, self.gamma)[:-1]\n\nself.path_start_idx = self.ptr", "path": "spinningup/spinup/algos/tf1/vpg/vpg.py", "commit_date": "2020-01-30 00:00:00", "repo_name": "openai/spinningup", "stars": 9460, "license": "mit", "language": "python", "size": 31762}
{"docstring": "\"\"\"\nCall this at the end of an epoch to get all of the data from\nthe buffer, with advantages appropriately normalized (shifted to have\nmean zero and std one). Also, resets some pointers in the buffer.\n\"\"\"\n", "func_signal": "def get(self):\n", "code": "assert self.ptr == self.max_size    # buffer has to be full before you can get\nself.ptr, self.path_start_idx = 0, 0\n# the next two lines implement the advantage normalization trick\nadv_mean, adv_std = mpi_statistics_scalar(self.adv_buf)\nself.adv_buf = (self.adv_buf - adv_mean) / adv_std\nreturn [self.obs_buf, self.act_buf, self.adv_buf, \n        self.ret_buf, self.logp_buf]", "path": "spinningup/spinup/algos/tf1/vpg/vpg.py", "commit_date": "2020-01-30 00:00:00", "repo_name": "openai/spinningup", "stars": 9460, "license": "mit", "language": "python", "size": 31762}
{"docstring": "# Make 'env_fn' from 'env_name'\n", "func_signal": "def thunk_plus():\n", "code": "if 'env_name' in kwargs:\n    import gym\n    env_name = kwargs['env_name']\n    kwargs['env_fn'] = lambda : gym.make(env_name)\n    del kwargs['env_name']\n\n# Fork into multiple processes\nmpi_fork(num_cpu)\n\n# Run thunk\nthunk(**kwargs)", "path": "spinningup/spinup/utils/run_utils.py", "commit_date": "2019-03-01 00:00:00", "repo_name": "openai/spinningup", "stars": 9460, "license": "mit", "language": "python", "size": 31762}
{"docstring": "\"\"\"\nMakes a list of dicts, where each dict is a valid config in the grid.\n\nThere is special handling for variant parameters whose names take\nthe form\n\n    ``'full:param:name'``.\n\nThe colons are taken to indicate that these parameters should\nhave a nested dict structure. eg, if there are two params,\n\n    ====================  ===\n    Key                   Val\n    ====================  ===\n    ``'base:param:a'``    1\n    ``'base:param:b'``    2\n    ====================  ===\n\nthe variant dict will have the structure\n\n.. parsed-literal::\n\n    variant = {\n        base: {\n            param : {\n                a : 1,\n                b : 2\n                }\n            }    \n        }\n\"\"\"\n", "func_signal": "def variants(self):\n", "code": "flat_variants = self._variants(self.keys, self.vals)\n\ndef unflatten_var(var):\n    \"\"\" \n    Build the full nested dict version of var, based on key names.\n    \"\"\"\n    new_var = dict()\n    unflatten_set = set()\n\n    for k,v in var.items():\n        if ':' in k:\n            splits = k.split(':')\n            k0 = splits[0]\n            assert k0 not in new_var or isinstance(new_var[k0], dict), \\\n                \"You can't assign multiple values to the same key.\"\n\n            if not(k0 in new_var):\n                new_var[k0] = dict()\n\n            sub_k = ':'.join(splits[1:])\n            new_var[k0][sub_k] = v\n            unflatten_set.add(k0)\n        else:\n            assert not(k in new_var), \\\n                \"You can't assign multiple values to the same key.\"\n            new_var[k] = v\n\n    # Make sure to fill out the nested dicts.\n    for k in unflatten_set:\n        new_var[k] = unflatten_var(new_var[k])\n\n    return new_var\n\nnew_variants = [unflatten_var(var) for var in flat_variants]\nreturn new_variants", "path": "spinningup/spinup/utils/run_utils.py", "commit_date": "2019-03-01 00:00:00", "repo_name": "openai/spinningup", "stars": 9460, "license": "mit", "language": "python", "size": 31762}
{"docstring": "\"\"\"\nSets up the output_dir for a logger and returns a dict for logger kwargs.\n\nIf no seed is given and datestamp is false, \n\n::\n\n    output_dir = data_dir/exp_name\n\nIf a seed is given and datestamp is false,\n\n::\n\n    output_dir = data_dir/exp_name/exp_name_s[seed]\n\nIf datestamp is true, amend to\n\n::\n\n    output_dir = data_dir/YY-MM-DD_exp_name/YY-MM-DD_HH-MM-SS_exp_name_s[seed]\n\nYou can force datestamp=True by setting ``FORCE_DATESTAMP=True`` in \n``spinup/user_config.py``. \n\nArgs:\n\n    exp_name (string): Name for experiment.\n\n    seed (int): Seed for random number generators used by experiment.\n\n    data_dir (string): Path to folder where results should be saved.\n        Default is the ``DEFAULT_DATA_DIR`` in ``spinup/user_config.py``.\n\n    datestamp (bool): Whether to include a date and timestamp in the\n        name of the save directory.\n\nReturns:\n\n    logger_kwargs, a dict containing output_dir and exp_name.\n\"\"\"\n\n# Datestamp forcing\n", "func_signal": "def setup_logger_kwargs(exp_name, seed=None, data_dir=None, datestamp=False):\n", "code": "datestamp = datestamp or FORCE_DATESTAMP\n\n# Make base path\nymd_time = time.strftime(\"%Y-%m-%d_\") if datestamp else ''\nrelpath = ''.join([ymd_time, exp_name])\n\nif seed is not None:\n    # Make a seed-specific subfolder in the experiment directory.\n    if datestamp:\n        hms_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n        subfolder = ''.join([hms_time, '-', exp_name, '_s', str(seed)])\n    else:\n        subfolder = ''.join([exp_name, '_s', str(seed)])\n    relpath = osp.join(relpath, subfolder)\n\ndata_dir = data_dir or DEFAULT_DATA_DIR\nlogger_kwargs = dict(output_dir=osp.join(data_dir, relpath), \n                     exp_name=exp_name)\nreturn logger_kwargs", "path": "spinningup/spinup/utils/run_utils.py", "commit_date": "2019-03-01 00:00:00", "repo_name": "openai/spinningup", "stars": 9460, "license": "mit", "language": "python", "size": 31762}
{"docstring": "# make some empty lists for logging.\n", "func_signal": "def train_one_epoch():\n", "code": "batch_obs = []          # for observations\nbatch_acts = []         # for actions\nbatch_weights = []      # for R(tau) weighting in policy gradient\nbatch_rets = []         # for measuring episode returns\nbatch_lens = []         # for measuring episode lengths\n\n# reset episode-specific variables\nobs = env.reset()       # first obs comes from starting distribution\ndone = False            # signal from environment that episode is over\nep_rews = []            # list for rewards accrued throughout ep\n\n# render first episode of each epoch\nfinished_rendering_this_epoch = False\n\n# collect experience by acting in the environment with current policy\nwhile True:\n\n    # rendering\n    if (not finished_rendering_this_epoch) and render:\n        env.render()\n\n    # save obs\n    batch_obs.append(obs.copy())\n\n    # act in the environment\n    act = get_action(torch.as_tensor(obs, dtype=torch.float32))\n    obs, rew, done, _ = env.step(act)\n\n    # save action, reward\n    batch_acts.append(act)\n    ep_rews.append(rew)\n\n    if done:\n        # if episode is over, record info about episode\n        ep_ret, ep_len = sum(ep_rews), len(ep_rews)\n        batch_rets.append(ep_ret)\n        batch_lens.append(ep_len)\n\n        # the weight for each logprob(a|s) is R(tau)\n        batch_weights += [ep_ret] * ep_len\n\n        # reset episode-specific variables\n        obs, done, ep_rews = env.reset(), False, []\n\n        # won't render again this epoch\n        finished_rendering_this_epoch = True\n\n        # end experience loop if we have enough of it\n        if len(batch_obs) > batch_size:\n            break\n\n# take a single policy gradient update step\noptimizer.zero_grad()\nbatch_loss = compute_loss(obs=torch.as_tensor(batch_obs, dtype=torch.float32),\n                          act=torch.as_tensor(batch_acts, dtype=torch.int32),\n                          weights=torch.as_tensor(batch_weights, dtype=torch.float32)\n                          )\nbatch_loss.backward()\noptimizer.step()\nreturn batch_loss, batch_rets, batch_lens", "path": "spinningup/spinup/examples/pytorch/pg_math/1_simple_pg.py", "commit_date": "2020-01-30 00:00:00", "repo_name": "openai/spinningup", "stars": 9460, "license": "mit", "language": "python", "size": 31762}
{"docstring": "\"\"\"Print a helpful report about the experiment grid.\"\"\"\n", "func_signal": "def print(self):\n", "code": "print('='*DIV_LINE_WIDTH)\n\n# Prepare announcement at top of printing. If the ExperimentGrid has a\n# short name, write this as one line. If the name is long, break the\n# announcement over two lines.\nbase_msg = 'ExperimentGrid %s runs over parameters:\\n'\nname_insert = '['+self._name+']'\nif len(base_msg%name_insert) <= 80:\n    msg = base_msg%name_insert\nelse:\n    msg = base_msg%(name_insert+'\\n')\nprint(colorize(msg, color='green', bold=True))\n\n# List off parameters, shorthands, and possible values.\nfor k, v, sh in zip(self.keys, self.vals, self.shs):\n    color_k = colorize(k.ljust(40), color='cyan', bold=True)\n    print('', color_k, '['+sh+']' if sh is not None else '', '\\n')\n    for i, val in enumerate(v):\n        print('\\t' + str(convert_json(val)))\n    print()\n\n# Count up the number of variants. The number counting seeds\n# is the total number of experiments that will run; the number not\n# counting seeds is the total number of otherwise-unique configs\n# being investigated.\nnvars_total = int(np.prod([len(v) for v in self.vals]))\nif 'seed' in self.keys:\n    num_seeds = len(self.vals[self.keys.index('seed')])\n    nvars_seedless = int(nvars_total / num_seeds)\nelse:\n    nvars_seedless = nvars_total\nprint(' Variants, counting seeds: '.ljust(40), nvars_total)\nprint(' Variants, not counting seeds: '.ljust(40), nvars_seedless)\nprint()\nprint('='*DIV_LINE_WIDTH)", "path": "spinningup/spinup/utils/run_utils.py", "commit_date": "2019-03-01 00:00:00", "repo_name": "openai/spinningup", "stars": 9460, "license": "mit", "language": "python", "size": 31762}
{"docstring": "\"\"\"\nGiven a variant (dict of valid param/value pairs), make an exp_name.\n\nA variant's name is constructed as the grid name (if you've given it \none), plus param names (or shorthands if available) and values \nseparated by underscores.\n\nNote: if ``seed`` is a parameter, it is not included in the name.\n\"\"\"\n\n", "func_signal": "def variant_name(self, variant):\n", "code": "def get_val(v, k):\n    # Utility method for getting the correct value out of a variant\n    # given as a nested dict. Assumes that a parameter name, k, \n    # describes a path into the nested dict, such that k='a:b:c'\n    # corresponds to value=variant['a']['b']['c']. Uses recursion\n    # to get this.\n    if k in v:\n        return v[k]\n    else:\n        splits = k.split(':')\n        k0, k1 = splits[0], ':'.join(splits[1:])\n        return get_val(v[k0], k1)\n\n# Start the name off with the name of the variant generator.\nvar_name = self._name\n\n# Build the rest of the name by looping through all parameters,\n# and deciding which ones need to go in there.\nfor k, v, sh, inn in zip(self.keys, self.vals, self.shs, self.in_names):\n\n    # Include a parameter in a name if either 1) it can take multiple\n    # values, or 2) the user specified that it must appear in the name.\n    # Except, however, when the parameter is 'seed'. Seed is handled\n    # differently so that runs of the same experiment, with different \n    # seeds, will be grouped by experiment name.\n    if (len(v)>1 or inn) and not(k=='seed'):\n\n        # Use the shorthand if available, otherwise the full name.\n        param_name = sh if sh is not None else k\n        param_name = valid_str(param_name)\n\n        # Get variant value for parameter k\n        variant_val = get_val(variant, k)\n\n        # Append to name\n        if all_bools(v): \n            # If this is a param which only takes boolean values,\n            # only include in the name if it's True for this variant.\n            var_name += ('_' + param_name) if variant_val else ''\n        else:\n            var_name += '_' + param_name + valid_str(variant_val)\n\nreturn var_name.lstrip('_')", "path": "spinningup/spinup/utils/run_utils.py", "commit_date": "2019-03-01 00:00:00", "repo_name": "openai/spinningup", "stars": 9460, "license": "mit", "language": "python", "size": 31762}
{"docstring": "\"\"\"Helper to recursively yield the matches.\"\"\"\n", "func_signal": "def _recursive_matches(self, nodes, count) -> Iterator[Tuple[int, _Results]]:\n", "code": "assert self.content is not None\nif count >= self.min:\n    yield 0, {}\nif count < self.max:\n    for alt in self.content:\n        for c0, r0 in generate_matches(alt, nodes):\n            for c1, r1 in self._recursive_matches(nodes[c0:], count + 1):\n                r = {}\n                r.update(r0)\n                r.update(r1)\n                yield c0 + c1, r", "path": "black/src/blib2to3/pytree.py", "commit_date": "2020-11-13 00:00:00", "repo_name": "psf/black", "stars": 37011, "license": "mit", "language": "python", "size": 6734}
{"docstring": "\"\"\"Return the line number which generated the invocant node.\"\"\"\n", "func_signal": "def get_lineno(self) -> Optional[int]:\n", "code": "node = self\nwhile not isinstance(node, Leaf):\n    if not node.children:\n        return None\n    node = node.children[0]\nreturn node.lineno", "path": "black/src/blib2to3/pytree.py", "commit_date": "2020-11-13 00:00:00", "repo_name": "psf/black", "stars": 37011, "license": "mit", "language": "python", "size": 6734}
{"docstring": "\"\"\"Dump the grammar tables to standard output, for debugging.\"\"\"\n", "func_signal": "def report(self) -> None:\n", "code": "from pprint import pprint\n\nprint(\"s2n\")\npprint(self.symbol2number)\nprint(\"n2s\")\npprint(self.number2symbol)\nprint(\"states\")\npprint(self.states)\nprint(\"dfas\")\npprint(self.dfas)\nprint(\"labels\")\npprint(self.labels)\nprint(\"start\", self.start)", "path": "black/src/blib2to3/pgen2/grammar.py", "commit_date": "2020-05-08 00:00:00", "repo_name": "psf/black", "stars": 37011, "license": "mit", "language": "python", "size": 6734}
{"docstring": "\"\"\"\nThe node immediately following the invocant in their parent's children\nlist. If the invocant does not have a next sibling, it is None\n\"\"\"\n", "func_signal": "def next_sibling(self) -> Optional[NL]:\n", "code": "if self.parent is None:\n    return None\n\nif self.parent.next_sibling_map is None:\n    self.parent.update_sibling_maps()\nassert self.parent.next_sibling_map is not None\nreturn self.parent.next_sibling_map[id(self)]", "path": "black/src/blib2to3/pytree.py", "commit_date": "2020-11-13 00:00:00", "repo_name": "psf/black", "stars": 37011, "license": "mit", "language": "python", "size": 6734}
{"docstring": "\"\"\"Return a post-order iterator for the tree.\"\"\"\n", "func_signal": "def post_order(self) -> Iterator[NL]:\n", "code": "for child in self.children:\n    yield from child.post_order()\nyield self", "path": "black/src/blib2to3/pytree.py", "commit_date": "2020-11-13 00:00:00", "repo_name": "psf/black", "stars": 37011, "license": "mit", "language": "python", "size": 6734}
{"docstring": "# ALT: ITEM+\n", "func_signal": "def parse_alt(self) -> Tuple[\"NFAState\", \"NFAState\"]:\n", "code": "a, b = self.parse_item()\nwhile self.value in (\"(\", \"[\") or self.type in (token.NAME, token.STRING):\n    c, d = self.parse_item()\n    b.addarc(c)\n    b = d\nreturn a, b", "path": "black/src/blib2to3/pgen2/pgen.py", "commit_date": "2020-09-06 00:00:00", "repo_name": "psf/black", "stars": 37011, "license": "mit", "language": "python", "size": 6734}
{"docstring": "\"\"\"Load the grammar tables from the text files written by pgen.\"\"\"\n", "func_signal": "def run(self, graminit_h, graminit_c):\n", "code": "self.parse_graminit_h(graminit_h)\nself.parse_graminit_c(graminit_c)\nself.finish_off()", "path": "black/src/blib2to3/pgen2/conv.py", "commit_date": "2020-05-08 00:00:00", "repo_name": "psf/black", "stars": 37011, "license": "mit", "language": "python", "size": 6734}
{"docstring": "# ITEM: '[' RHS ']' | ATOM ['+' | '*']\n", "func_signal": "def parse_item(self) -> Tuple[\"NFAState\", \"NFAState\"]:\n", "code": "if self.value == \"[\":\n    self.gettoken()\n    a, z = self.parse_rhs()\n    self.expect(token.OP, \"]\")\n    a.addarc(z)\n    return a, z\nelse:\n    a, z = self.parse_atom()\n    value = self.value\n    if value not in (\"+\", \"*\"):\n        return a, z\n    self.gettoken()\n    z.addarc(a)\n    if value == \"+\":\n        return a, z\n    else:\n        return a, a", "path": "black/src/blib2to3/pgen2/pgen.py", "commit_date": "2020-09-06 00:00:00", "repo_name": "psf/black", "stars": 37011, "license": "mit", "language": "python", "size": 6734}
{"docstring": "\"\"\"Return a canonical string representation.\"\"\"\n", "func_signal": "def __repr__(self) -> str:\n", "code": "from .pgen2.token import tok_name\n\nassert self.type is not None\nreturn \"%s(%s, %r)\" % (\n    self.__class__.__name__,\n    tok_name.get(self.type, self.type),\n    self.value,\n)", "path": "black/src/blib2to3/pytree.py", "commit_date": "2020-11-13 00:00:00", "repo_name": "psf/black", "stars": 37011, "license": "mit", "language": "python", "size": 6734}
{"docstring": "\"\"\"Return a pre-order iterator for the tree.\"\"\"\n", "func_signal": "def pre_order(self) -> Iterator[NL]:\n", "code": "yield self\nfor child in self.children:\n    yield from child.pre_order()", "path": "black/src/blib2to3/pytree.py", "commit_date": "2020-11-13 00:00:00", "repo_name": "psf/black", "stars": 37011, "license": "mit", "language": "python", "size": 6734}
{"docstring": "\"\"\"Return a canonical string representation.\"\"\"\n", "func_signal": "def __repr__(self) -> Text:\n", "code": "assert self.type is not None\nreturn \"%s(%s, %r)\" % (\n    self.__class__.__name__,\n    type_repr(self.type),\n    self.children,\n)", "path": "black/src/blib2to3/pytree.py", "commit_date": "2020-11-13 00:00:00", "repo_name": "psf/black", "stars": 37011, "license": "mit", "language": "python", "size": 6734}
{"docstring": "\"\"\"Constructor that prevents Base from being instantiated.\"\"\"\n", "func_signal": "def __new__(cls, *args, **kwds):\n", "code": "assert cls is not Base, \"Cannot instantiate Base\"\nreturn object.__new__(cls)", "path": "black/src/blib2to3/pytree.py", "commit_date": "2020-11-13 00:00:00", "repo_name": "psf/black", "stars": 37011, "license": "mit", "language": "python", "size": 6734}
{"docstring": "\"\"\"Helper to iteratively yield the matches.\"\"\"\n", "func_signal": "def _iterative_matches(self, nodes) -> Iterator[Tuple[int, _Results]]:\n", "code": "nodelen = len(nodes)\nif 0 >= self.min:\n    yield 0, {}\n\nresults = []\n# generate matches that use just one alt from self.content\nfor alt in self.content:\n    for c, r in generate_matches(alt, nodes):\n        yield c, r\n        results.append((c, r))\n\n# for each match, iterate down the nodes\nwhile results:\n    new_results = []\n    for c0, r0 in results:\n        # stop if the entire set of nodes has been matched\n        if c0 < nodelen and c0 <= self.max:\n            for alt in self.content:\n                for c1, r1 in generate_matches(alt, nodes[c0:]):\n                    if c1 > 0:\n                        r = {}\n                        r.update(r0)\n                        r.update(r1)\n                        yield c0 + c1, r\n                        new_results.append((c0 + c1, r))\n    results = new_results", "path": "black/src/blib2to3/pytree.py", "commit_date": "2020-11-13 00:00:00", "repo_name": "psf/black", "stars": 37011, "license": "mit", "language": "python", "size": 6734}
{"docstring": "\"\"\"\nThe whitespace and comments preceding this node in the input.\n\"\"\"\n", "func_signal": "def prefix(self) -> Text:\n", "code": "if not self.children:\n    return \"\"\nreturn self.children[0].prefix", "path": "black/src/blib2to3/pytree.py", "commit_date": "2020-11-13 00:00:00", "repo_name": "psf/black", "stars": 37011, "license": "mit", "language": "python", "size": 6734}
{"docstring": "\"\"\"\nReturn the string immediately following the invocant node. This is\neffectively equivalent to node.next_sibling.prefix\n\"\"\"\n", "func_signal": "def get_suffix(self) -> Text:\n", "code": "next_sib = self.next_sibling\nif next_sib is None:\n    return \"\"\nprefix = next_sib.prefix\nreturn prefix", "path": "black/src/blib2to3/pytree.py", "commit_date": "2020-11-13 00:00:00", "repo_name": "psf/black", "stars": 37011, "license": "mit", "language": "python", "size": 6734}
{"docstring": "\"\"\"\nThe node immediately preceding the invocant in their parent's children\nlist. If the invocant does not have a previous sibling, it is None.\n\"\"\"\n", "func_signal": "def prev_sibling(self) -> Optional[NL]:\n", "code": "if self.parent is None:\n    return None\n\nif self.parent.prev_sibling_map is None:\n    self.parent.update_sibling_maps()\nassert self.parent.prev_sibling_map is not None\nreturn self.parent.prev_sibling_map[id(self)]", "path": "black/src/blib2to3/pytree.py", "commit_date": "2020-11-13 00:00:00", "repo_name": "psf/black", "stars": 37011, "license": "mit", "language": "python", "size": 6734}
{"docstring": "\"\"\"Parse the .c file written by pgen.  (Internal)\n\nThe file looks as follows.  The first two lines are always this:\n\n#include \"pgenheaders.h\"\n#include \"grammar.h\"\n\nAfter that come four blocks:\n\n1) one or more state definitions\n2) a table defining dfas\n3) a table defining labels\n4) a struct defining the grammar\n\nA state definition has the following form:\n- one or more arc arrays, each of the form:\n  static arc arcs_<n>_<m>[<k>] = {\n          {<i>, <j>},\n          ...\n  };\n- followed by a state array, of the form:\n  static state states_<s>[<t>] = {\n          {<k>, arcs_<n>_<m>},\n          ...\n  };\n\n\"\"\"\n", "func_signal": "def parse_graminit_c(self, filename):\n", "code": "try:\n    f = open(filename)\nexcept OSError as err:\n    print(\"Can't open %s: %s\" % (filename, err))\n    return False\n# The code below essentially uses f's iterator-ness!\nlineno = 0\n\n# Expect the two #include lines\nlineno, line = lineno + 1, next(f)\nassert line == '#include \"pgenheaders.h\"\\n', (lineno, line)\nlineno, line = lineno + 1, next(f)\nassert line == '#include \"grammar.h\"\\n', (lineno, line)\n\n# Parse the state definitions\nlineno, line = lineno + 1, next(f)\nallarcs = {}\nstates = []\nwhile line.startswith(\"static arc \"):\n    while line.startswith(\"static arc \"):\n        mo = re.match(r\"static arc arcs_(\\d+)_(\\d+)\\[(\\d+)\\] = {$\", line)\n        assert mo, (lineno, line)\n        n, m, k = list(map(int, mo.groups()))\n        arcs = []\n        for _ in range(k):\n            lineno, line = lineno + 1, next(f)\n            mo = re.match(r\"\\s+{(\\d+), (\\d+)},$\", line)\n            assert mo, (lineno, line)\n            i, j = list(map(int, mo.groups()))\n            arcs.append((i, j))\n        lineno, line = lineno + 1, next(f)\n        assert line == \"};\\n\", (lineno, line)\n        allarcs[(n, m)] = arcs\n        lineno, line = lineno + 1, next(f)\n    mo = re.match(r\"static state states_(\\d+)\\[(\\d+)\\] = {$\", line)\n    assert mo, (lineno, line)\n    s, t = list(map(int, mo.groups()))\n    assert s == len(states), (lineno, line)\n    state = []\n    for _ in range(t):\n        lineno, line = lineno + 1, next(f)\n        mo = re.match(r\"\\s+{(\\d+), arcs_(\\d+)_(\\d+)},$\", line)\n        assert mo, (lineno, line)\n        k, n, m = list(map(int, mo.groups()))\n        arcs = allarcs[n, m]\n        assert k == len(arcs), (lineno, line)\n        state.append(arcs)\n    states.append(state)\n    lineno, line = lineno + 1, next(f)\n    assert line == \"};\\n\", (lineno, line)\n    lineno, line = lineno + 1, next(f)\nself.states = states\n\n# Parse the dfas\ndfas = {}\nmo = re.match(r\"static dfa dfas\\[(\\d+)\\] = {$\", line)\nassert mo, (lineno, line)\nndfas = int(mo.group(1))\nfor i in range(ndfas):\n    lineno, line = lineno + 1, next(f)\n    mo = re.match(r'\\s+{(\\d+), \"(\\w+)\", (\\d+), (\\d+), states_(\\d+),$', line)\n    assert mo, (lineno, line)\n    symbol = mo.group(2)\n    number, x, y, z = list(map(int, mo.group(1, 3, 4, 5)))\n    assert self.symbol2number[symbol] == number, (lineno, line)\n    assert self.number2symbol[number] == symbol, (lineno, line)\n    assert x == 0, (lineno, line)\n    state = states[z]\n    assert y == len(state), (lineno, line)\n    lineno, line = lineno + 1, next(f)\n    mo = re.match(r'\\s+(\"(?:\\\\\\d\\d\\d)*\")},$', line)\n    assert mo, (lineno, line)\n    first = {}\n    rawbitset = eval(mo.group(1))\n    for i, c in enumerate(rawbitset):\n        byte = ord(c)\n        for j in range(8):\n            if byte & (1 << j):\n                first[i * 8 + j] = 1\n    dfas[number] = (state, first)\nlineno, line = lineno + 1, next(f)\nassert line == \"};\\n\", (lineno, line)\nself.dfas = dfas\n\n# Parse the labels\nlabels = []\nlineno, line = lineno + 1, next(f)\nmo = re.match(r\"static label labels\\[(\\d+)\\] = {$\", line)\nassert mo, (lineno, line)\nnlabels = int(mo.group(1))\nfor i in range(nlabels):\n    lineno, line = lineno + 1, next(f)\n    mo = re.match(r'\\s+{(\\d+), (0|\"\\w+\")},$', line)\n    assert mo, (lineno, line)\n    x, y = mo.groups()\n    x = int(x)\n    if y == \"0\":\n        y = None\n    else:\n        y = eval(y)\n    labels.append((x, y))\nlineno, line = lineno + 1, next(f)\nassert line == \"};\\n\", (lineno, line)\nself.labels = labels\n\n# Parse the grammar struct\nlineno, line = lineno + 1, next(f)\nassert line == \"grammar _PyParser_Grammar = {\\n\", (lineno, line)\nlineno, line = lineno + 1, next(f)\nmo = re.match(r\"\\s+(\\d+),$\", line)\nassert mo, (lineno, line)\nndfas = int(mo.group(1))\nassert ndfas == len(self.dfas)\nlineno, line = lineno + 1, next(f)\nassert line == \"\\tdfas,\\n\", (lineno, line)\nlineno, line = lineno + 1, next(f)\nmo = re.match(r\"\\s+{(\\d+), labels},$\", line)\nassert mo, (lineno, line)\nnlabels = int(mo.group(1))\nassert nlabels == len(self.labels), (lineno, line)\nlineno, line = lineno + 1, next(f)\nmo = re.match(r\"\\s+(\\d+)$\", line)\nassert mo, (lineno, line)\nstart = int(mo.group(1))\nassert start in self.number2symbol, (lineno, line)\nself.start = start\nlineno, line = lineno + 1, next(f)\nassert line == \"};\\n\", (lineno, line)\ntry:\n    lineno, line = lineno + 1, next(f)\nexcept StopIteration:\n    pass\nelse:\n    assert 0, (lineno, line)", "path": "black/src/blib2to3/pgen2/conv.py", "commit_date": "2020-05-08 00:00:00", "repo_name": "psf/black", "stars": 37011, "license": "mit", "language": "python", "size": 6734}
{"docstring": "\"\"\"Constructor that prevents BasePattern from being instantiated.\"\"\"\n", "func_signal": "def __new__(cls, *args, **kwds):\n", "code": "assert cls is not BasePattern, \"Cannot instantiate BasePattern\"\nreturn object.__new__(cls)", "path": "black/src/blib2to3/pytree.py", "commit_date": "2020-11-13 00:00:00", "repo_name": "psf/black", "stars": 37011, "license": "mit", "language": "python", "size": 6734}
{"docstring": "\"\"\"Parse the .h file written by pgen.  (Internal)\n\nThis file is a sequence of #define statements defining the\nnonterminals of the grammar as numbers.  We build two tables\nmapping the numbers to names and back.\n\n\"\"\"\n", "func_signal": "def parse_graminit_h(self, filename):\n", "code": "try:\n    f = open(filename)\nexcept OSError as err:\n    print(\"Can't open %s: %s\" % (filename, err))\n    return False\nself.symbol2number = {}\nself.number2symbol = {}\nlineno = 0\nfor line in f:\n    lineno += 1\n    mo = re.match(r\"^#define\\s+(\\w+)\\s+(\\d+)$\", line)\n    if not mo and line.strip():\n        print(\"%s(%s): can't parse %s\" % (filename, lineno, line.strip()))\n    else:\n        symbol, number = mo.groups()\n        number = int(number)\n        assert symbol not in self.symbol2number\n        assert number not in self.number2symbol\n        self.symbol2number[symbol] = number\n        self.number2symbol[number] = symbol\nreturn True", "path": "black/src/blib2to3/pgen2/conv.py", "commit_date": "2020-05-08 00:00:00", "repo_name": "psf/black", "stars": 37011, "license": "mit", "language": "python", "size": 6734}
{"docstring": "# RHS: ALT ('|' ALT)*\n", "func_signal": "def parse_rhs(self) -> Tuple[\"NFAState\", \"NFAState\"]:\n", "code": "a, z = self.parse_alt()\nif self.value != \"|\":\n    return a, z\nelse:\n    aa = NFAState()\n    zz = NFAState()\n    aa.addarc(a)\n    z.addarc(zz)\n    while self.value == \"|\":\n        self.gettoken()\n        a, z = self.parse_alt()\n        aa.addarc(a)\n        z.addarc(zz)\n    return aa, zz", "path": "black/src/blib2to3/pgen2/pgen.py", "commit_date": "2020-09-06 00:00:00", "repo_name": "psf/black", "stars": 37011, "license": "mit", "language": "python", "size": 6734}
{"docstring": "\"\"\"\nIsolate a glossary present inside a word.\n\nReturns a list of subwords. In which all 'glossary' glossaries are isolated \n\nFor example, if 'USA' is the glossary and '1934USABUSA' the word, the return value is:\n    ['1934', 'USA', 'B', 'USA']\n\"\"\"\n# regex equivalent of (if word == glossary or glossary not in word)\n", "func_signal": "def isolate_glossary(word, glossary):\n", "code": "if re.match('^'+glossary+'$', word) or not re.search(glossary, word):\n    return [word]\nelse:\n    segments = re.split(r'({})'.format(glossary), word)\n    segments, ending = segments[:-1], segments[-1]\n    segments = list(filter(None, segments)) # Remove empty strings in regex group.\n    return segments + [ending.strip('\\r\\n ')] if ending != '' else segments", "path": "attention-is-all-you-need-pytorch/apply_bpe.py", "commit_date": "2019-12-05 00:00:00", "repo_name": "jadore801120/attention-is-all-you-need-pytorch", "stars": 8304, "license": "mit", "language": "python", "size": 166}
{"docstring": "\"\"\"segment single sentence (whitespace-tokenized string) with BPE encoding\"\"\"\n", "func_signal": "def segment(self, sentence, dropout=0):\n", "code": "segments = self.segment_tokens(sentence.strip('\\r\\n ').split(' '), dropout)\nreturn ' '.join(segments)", "path": "attention-is-all-you-need-pytorch/apply_bpe.py", "commit_date": "2019-12-05 00:00:00", "repo_name": "jadore801120/attention-is-all-you-need-pytorch", "stars": 8304, "license": "mit", "language": "python", "size": 166}
{"docstring": "\"\"\"segment a sequence of tokens with BPE encoding\"\"\"\n", "func_signal": "def segment_tokens(self, tokens, dropout=0):\n", "code": "output = []\nfor word in tokens:\n    # eliminate double spaces\n    if not word:\n        continue\n    new_word = [out for segment in self._isolate_glossaries(word)\n                for out in encode(segment,\n                                  self.bpe_codes,\n                                  self.bpe_codes_reverse,\n                                  self.vocab,\n                                  self.separator,\n                                  self.version,\n                                  self.cache,\n                                  self.glossaries_regex,\n                                  dropout)]\n\n    for item in new_word[:-1]:\n        output.append(item + self.separator)\n    output.append(new_word[-1])\n\nreturn output", "path": "attention-is-all-you-need-pytorch/apply_bpe.py", "commit_date": "2019-12-05 00:00:00", "repo_name": "jadore801120/attention-is-all-you-need-pytorch", "stars": 8304, "license": "mit", "language": "python", "size": 166}
{"docstring": "''' Epoch operation in evaluation phase '''\n\n", "func_signal": "def eval_epoch(model, validation_data, device, opt):\n", "code": "model.eval()\ntotal_loss, n_word_total, n_word_correct = 0, 0, 0\n\ndesc = '  - (Validation) '\nwith torch.no_grad():\n    for batch in tqdm(validation_data, mininterval=2, desc=desc, leave=False):\n\n        # prepare data\n        src_seq = patch_src(batch.src, opt.src_pad_idx).to(device)\n        trg_seq, gold = map(lambda x: x.to(device), patch_trg(batch.trg, opt.trg_pad_idx))\n\n        # forward\n        pred = model(src_seq, trg_seq)\n        loss, n_correct, n_word = cal_performance(\n            pred, gold, opt.trg_pad_idx, smoothing=False)\n\n        # note keeping\n        n_word_total += n_word\n        n_word_correct += n_correct\n        total_loss += loss.item()\n\nloss_per_word = total_loss/n_word_total\naccuracy = n_word_correct/n_word_total\nreturn loss_per_word, accuracy", "path": "attention-is-all-you-need-pytorch/train.py", "commit_date": "2019-12-08 00:00:00", "repo_name": "jadore801120/attention-is-all-you-need-pytorch", "stars": 8304, "license": "mit", "language": "python", "size": 166}
{"docstring": "\"\"\"Check for each segment in word if it is in-vocabulary,\nand segment OOV segments into smaller units by reversing the BPE merge operations\"\"\"\n\n", "func_signal": "def check_vocab_and_split(orig, bpe_codes, vocab, separator):\n", "code": "out = []\n\nfor segment in orig[:-1]:\n    if segment + separator in vocab:\n        out.append(segment)\n    else:\n        #sys.stderr.write('OOV: {0}\\n'.format(segment))\n        for item in recursive_split(segment, bpe_codes, vocab, separator, False):\n            out.append(item)\n\nsegment = orig[-1]\nif segment in vocab:\n    out.append(segment)\nelse:\n    #sys.stderr.write('OOV: {0}\\n'.format(segment))\n    for item in recursive_split(segment, bpe_codes, vocab, separator, True):\n        out.append(item)\n\nreturn out", "path": "attention-is-all-you-need-pytorch/apply_bpe.py", "commit_date": "2019-12-05 00:00:00", "repo_name": "jadore801120/attention-is-all-you-need-pytorch", "stars": 8304, "license": "mit", "language": "python", "size": 166}
{"docstring": "\"\"\"Recursively split segment into smaller units (by reversing BPE merges)\nuntil all units are either in-vocabulary, or cannot be split futher.\"\"\"\n\n", "func_signal": "def recursive_split(segment, bpe_codes, vocab, separator, final=False):\n", "code": "try:\n    if final:\n        left, right = bpe_codes[segment + '</w>']\n        right = right[:-4]\n    else:\n        left, right = bpe_codes[segment]\nexcept:\n    #sys.stderr.write('cannot split {0} further.\\n'.format(segment))\n    yield segment\n    return\n\nif left + separator in vocab:\n    yield left\nelse:\n    for item in recursive_split(left, bpe_codes, vocab, separator, False):\n        yield item\n\nif (final and right in vocab) or (not final and right + separator in vocab):\n    yield right\nelse:\n    for item in recursive_split(right, bpe_codes, vocab, separator, final):\n        yield item", "path": "attention-is-all-you-need-pytorch/apply_bpe.py", "commit_date": "2019-12-05 00:00:00", "repo_name": "jadore801120/attention-is-all-you-need-pytorch", "stars": 8304, "license": "mit", "language": "python", "size": 166}
{"docstring": "'''\nUsage: python preprocess.py -lang_src de -lang_trg en -save_data multi30k_de_en.pkl -share_vocab\n'''\n\n", "func_signal": "def main_wo_bpe():\n", "code": "spacy_support_langs = ['de', 'el', 'en', 'es', 'fr', 'it', 'lt', 'nb', 'nl', 'pt']\n\nparser = argparse.ArgumentParser()\nparser.add_argument('-lang_src', required=True, choices=spacy_support_langs)\nparser.add_argument('-lang_trg', required=True, choices=spacy_support_langs)\nparser.add_argument('-save_data', required=True)\nparser.add_argument('-data_src', type=str, default=None)\nparser.add_argument('-data_trg', type=str, default=None)\n\nparser.add_argument('-max_len', type=int, default=100)\nparser.add_argument('-min_word_count', type=int, default=3)\nparser.add_argument('-keep_case', action='store_true')\nparser.add_argument('-share_vocab', action='store_true')\n#parser.add_argument('-ratio', '--train_valid_test_ratio', type=int, nargs=3, metavar=(8,1,1))\n#parser.add_argument('-vocab', default=None)\n\nopt = parser.parse_args()\nassert not any([opt.data_src, opt.data_trg]), 'Custom data input is not support now.'\nassert not any([opt.data_src, opt.data_trg]) or all([opt.data_src, opt.data_trg])\nprint(opt)\n\nsrc_lang_model = spacy.load(opt.lang_src)\ntrg_lang_model = spacy.load(opt.lang_trg)\n\ndef tokenize_src(text):\n    return [tok.text for tok in src_lang_model.tokenizer(text)]\n\ndef tokenize_trg(text):\n    return [tok.text for tok in trg_lang_model.tokenizer(text)]\n\nSRC = torchtext.data.Field(\n    tokenize=tokenize_src, lower=not opt.keep_case,\n    pad_token=Constants.PAD_WORD, init_token=Constants.BOS_WORD, eos_token=Constants.EOS_WORD)\n\nTRG = torchtext.data.Field(\n    tokenize=tokenize_trg, lower=not opt.keep_case,\n    pad_token=Constants.PAD_WORD, init_token=Constants.BOS_WORD, eos_token=Constants.EOS_WORD)\n\nMAX_LEN = opt.max_len\nMIN_FREQ = opt.min_word_count\n\nif not all([opt.data_src, opt.data_trg]):\n    assert {opt.lang_src, opt.lang_trg} == {'de', 'en'}\nelse:\n    # Pack custom txt file into example datasets\n    raise NotImplementedError\n\ndef filter_examples_with_length(x):\n    return len(vars(x)['src']) <= MAX_LEN and len(vars(x)['trg']) <= MAX_LEN\n\ntrain, val, test = torchtext.datasets.Multi30k.splits(\n        exts = ('.' + opt.lang_src, '.' + opt.lang_trg),\n        fields = (SRC, TRG),\n        filter_pred=filter_examples_with_length)\n\nSRC.build_vocab(train.src, min_freq=MIN_FREQ)\nprint('[Info] Get source language vocabulary size:', len(SRC.vocab))\nTRG.build_vocab(train.trg, min_freq=MIN_FREQ)\nprint('[Info] Get target language vocabulary size:', len(TRG.vocab))\n\nif opt.share_vocab:\n    print('[Info] Merging two vocabulary ...')\n    for w, _ in SRC.vocab.stoi.items():\n        # TODO: Also update the `freq`, although it is not likely to be used.\n        if w not in TRG.vocab.stoi:\n            TRG.vocab.stoi[w] = len(TRG.vocab.stoi)\n    TRG.vocab.itos = [None] * len(TRG.vocab.stoi)\n    for w, i in TRG.vocab.stoi.items():\n        TRG.vocab.itos[i] = w\n    SRC.vocab.stoi = TRG.vocab.stoi\n    SRC.vocab.itos = TRG.vocab.itos\n    print('[Info] Get merged vocabulary size:', len(TRG.vocab))\n\n\ndata = {\n    'settings': opt,\n    'vocab': {'src': SRC, 'trg': TRG},\n    'train': train.examples,\n    'valid': val.examples,\n    'test': test.examples}\n\nprint('[Info] Dumping the processed data to pickle file', opt.save_data)\npickle.dump(data, open(opt.save_data, 'wb'))", "path": "attention-is-all-you-need-pytorch/preprocess.py", "commit_date": "2019-12-08 00:00:00", "repo_name": "jadore801120/attention-is-all-you-need-pytorch", "stars": 8304, "license": "mit", "language": "python", "size": 166}
{"docstring": "# Only accept batch size equals to 1 in this function.\n# TODO: expand to batch operation.\n", "func_signal": "def translate_sentence(self, src_seq):\n", "code": "assert src_seq.size(0) == 1\n\nsrc_pad_idx, trg_eos_idx = self.src_pad_idx, self.trg_eos_idx \nmax_seq_len, beam_size, alpha = self.max_seq_len, self.beam_size, self.alpha \n\nwith torch.no_grad():\n    src_mask = get_pad_mask(src_seq, src_pad_idx)\n    enc_output, gen_seq, scores = self._get_init_state(src_seq, src_mask)\n\n    ans_idx = 0   # default\n    for step in range(2, max_seq_len):    # decode up to max length\n        dec_output = self._model_decode(gen_seq[:, :step], enc_output, src_mask)\n        gen_seq, scores = self._get_the_best_score_and_idx(gen_seq, dec_output, scores, step)\n\n        # Check if all path finished\n        # -- locate the eos in the generated sequences\n        eos_locs = gen_seq == trg_eos_idx   \n        # -- replace the eos with its position for the length penalty use\n        seq_lens, _ = self.len_map.masked_fill(~eos_locs, max_seq_len).min(1)\n        # -- check if all beams contain eos\n        if (eos_locs.sum(1) > 0).sum(0).item() == beam_size:\n            # TODO: Try different terminate conditions.\n            _, ans_idx = scores.div(seq_lens.float() ** alpha).max(0)\n            ans_idx = ans_idx.item()\n            break\nreturn gen_seq[ans_idx][:seq_lens[ans_idx]].tolist()", "path": "attention-is-all-you-need-pytorch/transformer/Translator.py", "commit_date": "2019-12-22 00:00:00", "repo_name": "jadore801120/attention-is-all-you-need-pytorch", "stars": 8304, "license": "mit", "language": "python", "size": 166}
{"docstring": "\"\"\"segment line, dealing with leading and trailing whitespace\"\"\"\n\n", "func_signal": "def process_line(self, line, dropout=0):\n", "code": "out = \"\"\n\nleading_whitespace = len(line)-len(line.lstrip('\\r\\n '))\nif leading_whitespace:\n    out += line[:leading_whitespace]\n\nout += self.segment(line, dropout)\n\ntrailing_whitespace = len(line)-len(line.rstrip('\\r\\n '))\nif trailing_whitespace and trailing_whitespace != len(line):\n    out += line[-trailing_whitespace:]\n\nreturn out", "path": "attention-is-all-you-need-pytorch/apply_bpe.py", "commit_date": "2019-12-05 00:00:00", "repo_name": "jadore801120/attention-is-all-you-need-pytorch", "stars": 8304, "license": "mit", "language": "python", "size": 166}
{"docstring": "\"\"\"Encode word based on list of BPE merge operations, which are applied consecutively\n\"\"\"\n\n", "func_signal": "def encode(orig, bpe_codes, bpe_codes_reverse, vocab, separator, version, cache, glossaries_regex=None, dropout=0):\n", "code": "if not dropout and orig in cache:\n    return cache[orig]\n\nif glossaries_regex and glossaries_regex.match(orig):\n    cache[orig] = (orig,)\n    return (orig,)\n\nif len(orig) == 1:\n    return orig\n\nif version == (0, 1):\n    word = list(orig) + ['</w>']\nelif version == (0, 2): # more consistent handling of word-final segments\n    word = list(orig[:-1]) + [orig[-1] + '</w>']\nelse:\n    raise NotImplementedError\n\nwhile len(word) > 1:\n\n    # get list of symbol pairs; optionally apply dropout\n    pairs = [(bpe_codes[pair],i,pair) for (i,pair) in enumerate(zip(word, word[1:])) if (not dropout or random.random() > dropout) and pair in bpe_codes]\n\n    if not pairs:\n        break\n\n    #get first merge operation in list of BPE codes\n    bigram = min(pairs)[2]\n\n    # find start position of all pairs that we want to merge\n    positions = [i for (rank,i,pair) in pairs if pair == bigram]\n\n    i = 0\n    new_word = []\n    bigram = ''.join(bigram)\n    for j in positions:\n        # merges are invalid if they start before current position. This can happen if there are overlapping pairs: (x x x -> xx x)\n        if j < i:\n            continue\n        new_word.extend(word[i:j]) # all symbols before merged pair\n        new_word.append(bigram) # merged pair\n        i = j+2 # continue after merged pair\n    new_word.extend(word[i:]) # add all symbols until end of word\n    word = new_word\n\n# don't print end-of-word symbols\nif word[-1] == '</w>':\n    word = word[:-1]\nelif word[-1].endswith('</w>'):\n    word[-1] = word[-1][:-4]\n\nword = tuple(word)\nif vocab:\n    word = check_vocab_and_split(word, bpe_codes_reverse, vocab, separator)\n\ncache[orig] = word\nreturn word", "path": "attention-is-all-you-need-pytorch/apply_bpe.py", "commit_date": "2019-12-05 00:00:00", "repo_name": "jadore801120/attention-is-all-you-need-pytorch", "stars": 8304, "license": "mit", "language": "python", "size": 166}
{"docstring": "\"\"\"read vocabulary file produced by get_vocab.py, and filter according to frequency threshold.\n\"\"\"\n\n", "func_signal": "def read_vocabulary(vocab_file, threshold):\n", "code": "vocabulary = set()\n\nfor line in vocab_file:\n    word, freq = line.strip('\\r\\n ').split(' ')\n    freq = int(freq)\n    if threshold == None or freq >= threshold:\n        vocabulary.add(word)\n\nreturn vocabulary", "path": "attention-is-all-you-need-pytorch/apply_bpe.py", "commit_date": "2019-12-05 00:00:00", "repo_name": "jadore801120/attention-is-all-you-need-pytorch", "stars": 8304, "license": "mit", "language": "python", "size": 166}
{"docstring": "''' For masking out the subsequent info. '''\n", "func_signal": "def get_subsequent_mask(seq):\n", "code": "sz_b, len_s = seq.size()\nsubsequent_mask = (1 - torch.triu(\n    torch.ones((1, len_s, len_s), device=seq.device), diagonal=1)).bool()\nreturn subsequent_mask", "path": "attention-is-all-you-need-pytorch/transformer/Models.py", "commit_date": "2020-06-07 00:00:00", "repo_name": "jadore801120/attention-is-all-you-need-pytorch", "stars": 8304, "license": "mit", "language": "python", "size": 166}
{"docstring": "''' Apply label smoothing if needed '''\n\n", "func_signal": "def cal_performance(pred, gold, trg_pad_idx, smoothing=False):\n", "code": "loss = cal_loss(pred, gold, trg_pad_idx, smoothing=smoothing)\n\npred = pred.max(1)[1]\ngold = gold.contiguous().view(-1)\nnon_pad_mask = gold.ne(trg_pad_idx)\nn_correct = pred.eq(gold).masked_select(non_pad_mask).sum().item()\nn_word = non_pad_mask.sum().item()\n\nreturn loss, n_correct, n_word", "path": "attention-is-all-you-need-pytorch/train.py", "commit_date": "2019-12-08 00:00:00", "repo_name": "jadore801120/attention-is-all-you-need-pytorch", "stars": 8304, "license": "mit", "language": "python", "size": 166}
{"docstring": "''' Epoch operation in training phase'''\n\n", "func_signal": "def train_epoch(model, training_data, optimizer, opt, device, smoothing):\n", "code": "model.train()\ntotal_loss, n_word_total, n_word_correct = 0, 0, 0 \n\ndesc = '  - (Training)   '\nfor batch in tqdm(training_data, mininterval=2, desc=desc, leave=False):\n\n    # prepare data\n    src_seq = patch_src(batch.src, opt.src_pad_idx).to(device)\n    trg_seq, gold = map(lambda x: x.to(device), patch_trg(batch.trg, opt.trg_pad_idx))\n\n    # forward\n    optimizer.zero_grad()\n    pred = model(src_seq, trg_seq)\n\n    # backward and update parameters\n    loss, n_correct, n_word = cal_performance(\n        pred, gold, opt.trg_pad_idx, smoothing=smoothing) \n    loss.backward()\n    optimizer.step_and_update_lr()\n\n    # note keeping\n    n_word_total += n_word\n    n_word_correct += n_correct\n    total_loss += loss.item()\n\nloss_per_word = total_loss/n_word_total\naccuracy = n_word_correct/n_word_total\nreturn loss_per_word, accuracy", "path": "attention-is-all-you-need-pytorch/train.py", "commit_date": "2019-12-08 00:00:00", "repo_name": "jadore801120/attention-is-all-you-need-pytorch", "stars": 8304, "license": "mit", "language": "python", "size": 166}
{"docstring": "''' Learning rate scheduling per step '''\n\n", "func_signal": "def _update_learning_rate(self):\n", "code": "self.n_steps += 1\nlr = self.init_lr * self._get_lr_scale()\n\nfor param_group in self._optimizer.param_groups:\n    param_group['lr'] = lr", "path": "attention-is-all-you-need-pytorch/transformer/Optim.py", "commit_date": "2019-11-30 00:00:00", "repo_name": "jadore801120/attention-is-all-you-need-pytorch", "stars": 8304, "license": "mit", "language": "python", "size": 166}
{"docstring": "''' Start training '''\n\n", "func_signal": "def train(model, training_data, validation_data, optimizer, device, opt):\n", "code": "log_train_file, log_valid_file = None, None\n\nif opt.log:\n    log_train_file = opt.log + '.train.log'\n    log_valid_file = opt.log + '.valid.log'\n\n    print('[Info] Training performance will be written to file: {} and {}'.format(\n        log_train_file, log_valid_file))\n\n    with open(log_train_file, 'w') as log_tf, open(log_valid_file, 'w') as log_vf:\n        log_tf.write('epoch,loss,ppl,accuracy\\n')\n        log_vf.write('epoch,loss,ppl,accuracy\\n')\n\ndef print_performances(header, loss, accu, start_time):\n    print('  - {header:12} ppl: {ppl: 8.5f}, accuracy: {accu:3.3f} %, '\\\n          'elapse: {elapse:3.3f} min'.format(\n              header=f\"({header})\", ppl=math.exp(min(loss, 100)),\n              accu=100*accu, elapse=(time.time()-start_time)/60))\n\n#valid_accus = []\nvalid_losses = []\nfor epoch_i in range(opt.epoch):\n    print('[ Epoch', epoch_i, ']')\n\n    start = time.time()\n    train_loss, train_accu = train_epoch(\n        model, training_data, optimizer, opt, device, smoothing=opt.label_smoothing)\n    print_performances('Training', train_loss, train_accu, start)\n\n    start = time.time()\n    valid_loss, valid_accu = eval_epoch(model, validation_data, device, opt)\n    print_performances('Validation', valid_loss, valid_accu, start)\n\n    valid_losses += [valid_loss]\n\n    checkpoint = {'epoch': epoch_i, 'settings': opt, 'model': model.state_dict()}\n\n    if opt.save_model:\n        if opt.save_mode == 'all':\n            model_name = opt.save_model + '_accu_{accu:3.3f}.chkpt'.format(accu=100*valid_accu)\n            torch.save(checkpoint, model_name)\n        elif opt.save_mode == 'best':\n            model_name = opt.save_model + '.chkpt'\n            if valid_loss <= min(valid_losses):\n                torch.save(checkpoint, model_name)\n                print('    - [Info] The checkpoint file has been updated.')\n\n    if log_train_file and log_valid_file:\n        with open(log_train_file, 'a') as log_tf, open(log_valid_file, 'a') as log_vf:\n            log_tf.write('{epoch},{loss: 8.5f},{ppl: 8.5f},{accu:3.3f}\\n'.format(\n                epoch=epoch_i, loss=train_loss,\n                ppl=math.exp(min(train_loss, 100)), accu=100*train_accu))\n            log_vf.write('{epoch},{loss: 8.5f},{ppl: 8.5f},{accu:3.3f}\\n'.format(\n                epoch=epoch_i, loss=valid_loss,\n                ppl=math.exp(min(valid_loss, 100)), accu=100*valid_accu))", "path": "attention-is-all-you-need-pytorch/train.py", "commit_date": "2019-12-08 00:00:00", "repo_name": "jadore801120/attention-is-all-you-need-pytorch", "stars": 8304, "license": "mit", "language": "python", "size": 166}
{"docstring": "'''Main Function'''\n\n", "func_signal": "def main():\n", "code": "parser = argparse.ArgumentParser(description='translate.py')\n\nparser.add_argument('-model', required=True,\n                    help='Path to model weight file')\nparser.add_argument('-data_pkl', required=True,\n                    help='Pickle file with both instances and vocabulary.')\nparser.add_argument('-output', default='pred.txt',\n                    help=\"\"\"Path to output the predictions (each line will\n                    be the decoded sequence\"\"\")\nparser.add_argument('-beam_size', type=int, default=5)\nparser.add_argument('-max_seq_len', type=int, default=100)\nparser.add_argument('-no_cuda', action='store_true')\n\n# TODO: Translate bpe encoded files \n#parser.add_argument('-src', required=True,\n#                    help='Source sequence to decode (one line per sequence)')\n#parser.add_argument('-vocab', required=True,\n#                    help='Source sequence to decode (one line per sequence)')\n# TODO: Batch translation\n#parser.add_argument('-batch_size', type=int, default=30,\n#                    help='Batch size')\n#parser.add_argument('-n_best', type=int, default=1,\n#                    help=\"\"\"If verbose is set, will output the n_best\n#                    decoded sentences\"\"\")\n\nopt = parser.parse_args()\nopt.cuda = not opt.no_cuda\n\ndata = pickle.load(open(opt.data_pkl, 'rb'))\nSRC, TRG = data['vocab']['src'], data['vocab']['trg']\nopt.src_pad_idx = SRC.vocab.stoi[Constants.PAD_WORD]\nopt.trg_pad_idx = TRG.vocab.stoi[Constants.PAD_WORD]\nopt.trg_bos_idx = TRG.vocab.stoi[Constants.BOS_WORD]\nopt.trg_eos_idx = TRG.vocab.stoi[Constants.EOS_WORD]\n\ntest_loader = Dataset(examples=data['test'], fields={'src': SRC, 'trg': TRG})\n\ndevice = torch.device('cuda' if opt.cuda else 'cpu')\ntranslator = Translator(\n    model=load_model(opt, device),\n    beam_size=opt.beam_size,\n    max_seq_len=opt.max_seq_len,\n    src_pad_idx=opt.src_pad_idx,\n    trg_pad_idx=opt.trg_pad_idx,\n    trg_bos_idx=opt.trg_bos_idx,\n    trg_eos_idx=opt.trg_eos_idx).to(device)\n\nunk_idx = SRC.vocab.stoi[SRC.unk_token]\nwith open(opt.output, 'w') as f:\n    for example in tqdm(test_loader, mininterval=2, desc='  - (Test)', leave=False):\n        #print(' '.join(example.src))\n        src_seq = [SRC.vocab.stoi.get(word, unk_idx) for word in example.src]\n        pred_seq = translator.translate_sentence(torch.LongTensor([src_seq]).to(device))\n        pred_line = ' '.join(TRG.vocab.itos[idx] for idx in pred_seq)\n        pred_line = pred_line.replace(Constants.BOS_WORD, '').replace(Constants.EOS_WORD, '')\n        #print(pred_line)\n        f.write(pred_line.strip() + '\\n')\n\nprint('[Info] Finished.')", "path": "attention-is-all-you-need-pytorch/translate.py", "commit_date": "2019-12-17 00:00:00", "repo_name": "jadore801120/attention-is-all-you-need-pytorch", "stars": 8304, "license": "mit", "language": "python", "size": 166}
{"docstring": "''' Calculate cross entropy loss, apply label smoothing if needed. '''\n\n", "func_signal": "def cal_loss(pred, gold, trg_pad_idx, smoothing=False):\n", "code": "gold = gold.contiguous().view(-1)\n\nif smoothing:\n    eps = 0.1\n    n_class = pred.size(1)\n\n    one_hot = torch.zeros_like(pred).scatter(1, gold.view(-1, 1), 1)\n    one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n    log_prb = F.log_softmax(pred, dim=1)\n\n    non_pad_mask = gold.ne(trg_pad_idx)\n    loss = -(one_hot * log_prb).sum(dim=1)\n    loss = loss.masked_select(non_pad_mask).sum()  # average later\nelse:\n    loss = F.cross_entropy(pred, gold, ignore_index=trg_pad_idx, reduction='sum')\nreturn loss", "path": "attention-is-all-you-need-pytorch/train.py", "commit_date": "2019-12-08 00:00:00", "repo_name": "jadore801120/attention-is-all-you-need-pytorch", "stars": 8304, "license": "mit", "language": "python", "size": 166}
{"docstring": "''' Sinusoid position encoding table '''\n# TODO: make it with torch instead of numpy\n\n", "func_signal": "def _get_sinusoid_encoding_table(self, n_position, d_hid):\n", "code": "def get_position_angle_vec(position):\n    return [position / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)]\n\nsinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)])\nsinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\nsinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n\nreturn torch.FloatTensor(sinusoid_table).unsqueeze(0)", "path": "attention-is-all-you-need-pytorch/transformer/Models.py", "commit_date": "2020-06-07 00:00:00", "repo_name": "jadore801120/attention-is-all-you-need-pytorch", "stars": 8304, "license": "mit", "language": "python", "size": 166}
{"docstring": "''' \nUsage:\npython train.py -data_pkl m30k_deen_shr.pkl -log m30k_deen_shr -embs_share_weight -proj_share_weight -label_smoothing -save_model trained -b 256 -warmup 128000\n'''\n\n", "func_signal": "def main():\n", "code": "parser = argparse.ArgumentParser()\n\nparser.add_argument('-data_pkl', default=None)     # all-in-1 data pickle or bpe field\n\nparser.add_argument('-train_path', default=None)   # bpe encoded data\nparser.add_argument('-val_path', default=None)     # bpe encoded data\n\nparser.add_argument('-epoch', type=int, default=10)\nparser.add_argument('-b', '--batch_size', type=int, default=2048)\n\nparser.add_argument('-d_model', type=int, default=512)\nparser.add_argument('-d_inner_hid', type=int, default=2048)\nparser.add_argument('-d_k', type=int, default=64)\nparser.add_argument('-d_v', type=int, default=64)\n\nparser.add_argument('-n_head', type=int, default=8)\nparser.add_argument('-n_layers', type=int, default=6)\nparser.add_argument('-warmup','--n_warmup_steps', type=int, default=4000)\n\nparser.add_argument('-dropout', type=float, default=0.1)\nparser.add_argument('-embs_share_weight', action='store_true')\nparser.add_argument('-proj_share_weight', action='store_true')\n\nparser.add_argument('-log', default=None)\nparser.add_argument('-save_model', default=None)\nparser.add_argument('-save_mode', type=str, choices=['all', 'best'], default='best')\n\nparser.add_argument('-no_cuda', action='store_true')\nparser.add_argument('-label_smoothing', action='store_true')\n\nopt = parser.parse_args()\nopt.cuda = not opt.no_cuda\nopt.d_word_vec = opt.d_model\n\nif not opt.log and not opt.save_model:\n    print('No experiment result will be saved.')\n    raise\n\nif opt.batch_size < 2048 and opt.n_warmup_steps <= 4000:\n    print('[Warning] The warmup steps may be not enough.\\n'\\\n          '(sz_b, warmup) = (2048, 4000) is the official setting.\\n'\\\n          'Using smaller batch w/o longer warmup may cause '\\\n          'the warmup stage ends with only little data trained.')\n\ndevice = torch.device('cuda' if opt.cuda else 'cpu')\n\n#========= Loading Dataset =========#\n\nif all((opt.train_path, opt.val_path)):\n    training_data, validation_data = prepare_dataloaders_from_bpe_files(opt, device)\nelif opt.data_pkl:\n    training_data, validation_data = prepare_dataloaders(opt, device)\nelse:\n    raise\n\nprint(opt)\n\ntransformer = Transformer(\n    opt.src_vocab_size,\n    opt.trg_vocab_size,\n    src_pad_idx=opt.src_pad_idx,\n    trg_pad_idx=opt.trg_pad_idx,\n    trg_emb_prj_weight_sharing=opt.proj_share_weight,\n    emb_src_trg_weight_sharing=opt.embs_share_weight,\n    d_k=opt.d_k,\n    d_v=opt.d_v,\n    d_model=opt.d_model,\n    d_word_vec=opt.d_word_vec,\n    d_inner=opt.d_inner_hid,\n    n_layers=opt.n_layers,\n    n_head=opt.n_head,\n    dropout=opt.dropout).to(device)\n\noptimizer = ScheduledOptim(\n    optim.Adam(transformer.parameters(), betas=(0.9, 0.98), eps=1e-09),\n    2.0, opt.d_model, opt.n_warmup_steps)\n\ntrain(transformer, training_data, validation_data, optimizer, device, opt)", "path": "attention-is-all-you-need-pytorch/train.py", "commit_date": "2019-12-08 00:00:00", "repo_name": "jadore801120/attention-is-all-you-need-pytorch", "stars": 8304, "license": "mit", "language": "python", "size": 166}
{"docstring": "\"\"\"\nInitiate a list of destination groups which should be printed out.\n\"\"\"\n", "func_signal": "def __initiate_allow(self):\n", "code": "self.__allowable = ('cw<ss<char-style',\n                    'cw<it<listtable_',\n                    'cw<it<revi-table',\n                    'cw<ls<list-lev-d',\n                    # Field allowed\n                    'cw<fd<field-inst',\n                    'cw<an<book-mk-st',\n                    'cw<an<book-mk-en',\n                    'cw<an<annotation',\n                    'cw<cm<comment___',\n                    'cw<it<lovr-table',\n                    # info table\n                    'cw<di<company___',\n                    # 'cw<ls<list______',\n                )\nself.__not_allowable = (\n        'cw<un<unknown___',\n        'cw<un<company___',\n        'cw<ls<list-level',\n        'cw<fd<datafield_',\n        )\nself.__state = 'default'\nself.__state_dict = {\n    'default'           : self.__default_func,\n    'after_asterisk'    : self.__asterisk_func,\n    'delete'            : self.__delete_func,\n    'list'              : self.__list_func,\n}", "path": "calibre/src/calibre/ebooks/rtf2xml/delete_info.py", "commit_date": "2020-08-22 00:00:00", "repo_name": "kovidgoyal/calibre", "stars": 17955, "license": "gpl-3.0", "language": "python", "size": 301569}
{"docstring": "\"\"\"\nFlip the writability of a file and return the old mode. Returns None\nif the file is already writable.\n\"\"\"\n", "func_signal": "def flipwritable(fn, mode=None):\n", "code": "if os.access(fn, os.W_OK):\n    return None\nold_mode = os.stat(fn).st_mode\nos.chmod(fn, stat.S_IWRITE | old_mode)\nreturn old_mode", "path": "calibre/bypy/macos/__main__.py", "commit_date": "2020-11-25 00:00:00", "repo_name": "kovidgoyal/calibre", "stars": 17955, "license": "gpl-3.0", "language": "python", "size": 301569}
{"docstring": "# The Kindle apparently has incorrect behavior for duplicate anchors, see\n# https://bugs.launchpad.net/calibre/+bug/1454199\n", "func_signal": "def remove_duplicate_anchors(oeb):\n", "code": "for item in oeb.spine:\n    if not hasattr(item.data, 'xpath'):\n        continue\n    seen = set()\n    for tag in item.data.xpath('//*[@id or @name]'):\n        for attr in ('id', 'name'):\n            anchor = tag.get(attr)\n            if anchor is not None:\n                if anchor in seen:\n                    oeb.log.debug('Removing duplicate anchor:', anchor)\n                    tag.attrib.pop(attr)\n                else:\n                    seen.add(anchor)", "path": "calibre/src/calibre/ebooks/mobi/writer8/cleanup.py", "commit_date": "2020-08-22 00:00:00", "repo_name": "kovidgoyal/calibre", "stars": 17955, "license": "gpl-3.0", "language": "python", "size": 301569}
{"docstring": "\"\"\"Handle lines when in no special state. Look for an asterisk to\nbegin a special state. Otherwise, print out line.\"\"\"\n# cw<ml<asterisk__<nu<true\n", "func_signal": "def __default_func(self,line):\n", "code": "if self.__token_info == 'cw<ml<asterisk__':\n    self.__state = 'after_asterisk'\n    self.__delete_count = self.__ob_count\nelif self.__token_info == 'ob<nu<open-brack':\n    # write previous bracket, if exists\n    if self.__ob:\n        self.__write_obj.write(self.__ob)\n    self.__ob = line\n    return False\nelse:\n    # write previous bracket, since didn't find asterisk\n    if self.__ob:\n        self.__write_obj.write(self.__ob)\n        self.__ob = 0\n    return True", "path": "calibre/src/calibre/ebooks/rtf2xml/delete_info.py", "commit_date": "2020-08-22 00:00:00", "repo_name": "kovidgoyal/calibre", "stars": 17955, "license": "gpl-3.0", "language": "python", "size": 301569}
{"docstring": "''' Copy a directory d into a dmg named volname '''\n", "func_signal": "def makedmg(self, d, volname, internet_enable=True):\n", "code": "print('\\nSigning...')\nsys.stdout.flush()\ndestdir = OUTPUT_DIR\ntry:\n    shutil.rmtree(destdir)\nexcept EnvironmentError as err:\n    if err.errno != errno.ENOENT:\n        raise\nos.mkdir(destdir)\ndmg = join(destdir, volname + '.dmg')\nif os.path.exists(dmg):\n    os.unlink(dmg)\ntdir = tempfile.mkdtemp()\nappdir = join(tdir, os.path.basename(d))\nshutil.copytree(d, appdir, symlinks=True)\nif self.sign_installers or self.notarize:\n    with timeit() as times:\n        sign_app(appdir, self.notarize)\n    print('Signing completed in %d minutes %d seconds' % tuple(times))\nos.symlink('/Applications', join(tdir, 'Applications'))\nsize_in_mb = int(subprocess.check_output(['du', '-s', '-k', tdir]).decode('utf-8').split()[0]) / 1024.\n# UDBZ gives the best compression, better than ULFO\ncmd = ['/usr/bin/hdiutil', 'create', '-srcfolder', tdir, '-volname', volname, '-format', 'UDBZ']\nif 190 < size_in_mb < 250:\n    # We need -size 255m because of a bug in hdiutil. When the size of\n    # srcfolder is close to 200MB hdiutil fails with\n    # diskimages-helper: resize request is above maximum size allowed.\n    cmd += ['-size', '255m']\nprint('\\nCreating dmg...')\nwith timeit() as times:\n    subprocess.check_call(cmd + [dmg])\n    if internet_enable:\n        subprocess.check_call(['/usr/bin/hdiutil', 'internet-enable', '-yes', dmg])\nprint('dmg created in %d minutes and %d seconds' % tuple(times))\nshutil.rmtree(tdir)\nsize = os.stat(dmg).st_size / (1024 * 1024.)\nprint('\\nInstaller size: %.2fMB\\n' % size)\nreturn dmg", "path": "calibre/bypy/macos/__main__.py", "commit_date": "2020-11-25 00:00:00", "repo_name": "kovidgoyal/calibre", "stars": 17955, "license": "gpl-3.0", "language": "python", "size": 301569}
{"docstring": "# Remove blank space form beginning of paragraph.\n", "func_signal": "def tidy_up(self, text):\n", "code": "text = re.sub('(?msu)^[ ]{1,3}', '', text)\n# pre has 4 spaces. We trimmed 3 so anything with a space left is a pre.\ntext = re.sub('(?msu)^[ ]', '    ', text)\n\n# Remove tabs that aren't at the beinning of a line\nnew_text = []\nfor l in text.splitlines():\n    start = re.match('\\t+', l)\n    if start:\n        start = start.group()\n    else:\n        start = ''\n    l = re.sub('\\t', '', l)\n    new_text.append(start + l)\ntext = '\\n'.join(new_text)\n\n# Remove spaces from blank lines.\ntext = re.sub('(?msu)^[ ]+$', '', text)\n\n# Reduce blank lines\ntext = re.sub('(?msu)\\n{7,}', '\\n' * 6, text)\n\n# Remove blank lines at beginning and end of document.\ntext = re.sub(r'^\\s*', '', text)\ntext = re.sub(r'\\s*$', '\\n\\n', text)\n\nreturn text", "path": "calibre/src/calibre/ebooks/txt/markdownml.py", "commit_date": "2020-08-22 00:00:00", "repo_name": "kovidgoyal/calibre", "stars": 17955, "license": "gpl-3.0", "language": "python", "size": 301569}
{"docstring": "# Flat the path\n", "func_signal": "def ProcessFileName(fileName):\n", "code": "fileName = fileName.replace(\"/\", \"_\").replace(os.sep, \"_\")\n# Handle bookmark for HTML file\nfileName = fileName.replace(\"#\", \"_\")\n# Make it lower case\nfileName = fileName.lower()\n# Change all images to jpg\nroot, ext = os.path.splitext(fileName)\nif ext in ['.jpeg', '.jpg', '.gif', '.svg', '.png']:\n    fileName = root + '.jpg'\nreturn fileName", "path": "calibre/src/calibre/ebooks/snb/snbml.py", "commit_date": "2020-08-22 00:00:00", "repo_name": "kovidgoyal/calibre", "stars": 17955, "license": "gpl-3.0", "language": "python", "size": 301569}
{"docstring": "\"\"\"Main method for handling other methods. Read one line at\na time, and determine whether to print the line based on the state.\"\"\"\n", "func_signal": "def delete_info(self):\n", "code": "with open_for_read(self.__file) as read_obj:\n    with open_for_write(self.__write_to) as self.__write_obj:\n        for line in read_obj:\n            # ob<nu<open-brack<0001\n            self.__token_info = line[:16]\n            if self.__token_info == 'ob<nu<open-brack':\n                self.__ob_count = line[-5:-1]\n            if self.__token_info == 'cb<nu<clos-brack':\n                self.__cb_count = line[-5:-1]\n            # Get action to perform\n            action = self.__state_dict.get(self.__state)\n            if not action:\n                sys.stderr.write('No action in dictionary state is \"%s\" \\n'\n                        % self.__state)\n            # Print if allowed by action\n            if action(line):\n                self.__write_obj.write(line)\ncopy_obj = copy.Copy(bug_handler=self.__bug_handler)\nif self.__copy:\n    copy_obj.copy_file(self.__write_to, \"delete_info.data\")\ncopy_obj.rename(self.__write_to, self.__file)\nos.remove(self.__write_to)\nreturn self.__found_delete", "path": "calibre/src/calibre/ebooks/rtf2xml/delete_info.py", "commit_date": "2020-08-22 00:00:00", "repo_name": "kovidgoyal/calibre", "stars": 17955, "license": "gpl-3.0", "language": "python", "size": 301569}
{"docstring": "\"\"\"Parse a CSS *group of selectors*.\n\n:param css:\n    A *group of selectors* as an Unicode string.\n:raises:\n    :class:`SelectorSyntaxError` on invalid selectors.\n:returns:\n    A list of parsed :class:`Selector` objects, one for each\n    selector in the comma-separated group.\n\n\"\"\"\n# Fast path for simple cases\n", "func_signal": "def parse(css):\n", "code": "match = _el_re.match(css)\nif match:\n    return [Selector(Element(element=match.group(1)))]\nmatch = _id_re.match(css)\nif match is not None:\n    return [Selector(Hash(Element(element=match.group(1) or None),\n                          match.group(2)))]\nmatch = _class_re.match(css)\nif match is not None:\n    return [Selector(Class(Element(element=match.group(1) or None),\n                           match.group(2)))]\n\nstream = TokenStream(tokenize(css))\nstream.source = css\nreturn list(parse_selector_group(stream))", "path": "calibre/src/css_selectors/parser.py", "commit_date": "2020-08-22 00:00:00", "repo_name": "kovidgoyal/calibre", "stars": 17955, "license": "gpl-3.0", "language": "python", "size": 301569}
{"docstring": "\"\"\"\nDetermine whether to delete info in group\nNote on self.__cb flag.\nIf you find that you are in a delete group, and the previous\ntoken in not an open bracket (self.__ob = 0), that means\nthat the delete group is nested inside another acceptable\ndetination group. In this case, you have already written\nthe open bracket, so you will need to write the closed one\nas well.\n\"\"\"\n# Test for {\\*}, in which case don't enter\n# delete state\n", "func_signal": "def __asterisk_func(self,line):\n", "code": "self.__found_delete = True\nif self.__token_info == 'cb<nu<clos-brack':\n    if self.__delete_count == self.__cb_count:\n        self.__state = 'default'\n        self.__ob = 0\n        # changed this because haven't printed out start\n        return False\n    else:\n        # not sure what happens here!\n        # believe I have a '{\\*}\n        if self.__run_level > 3:\n            msg = 'Flag problem\\n'\n            raise self.__bug_handler(msg)\n        return True\nelif self.__token_info in self.__allowable :\n    if self.__ob:\n        self.__write_obj.write(self.__ob)\n        self.__ob = 0\n        self.__state = 'default'\n    else:\n        pass\n    return True\nelif self.__token_info == 'cw<ls<list______':\n    self.__ob = 0\n    self.__found_list_func(line)\nelif self.__token_info in self.__not_allowable:\n    if not self.__ob:\n        self.__write_cb = True\n    self.__ob = 0\n    self.__state = 'delete'\n    self.__cb_count = 0\n    return False\nelse:\n    if self.__run_level > 5:\n        msg = ('After an asterisk, and found neither an allowable or non-allowable token\\n\\\n                    token is \"%s\"\\n') % self.__token_info\n        raise self.__bug_handler(msg)\n    if not self.__ob:\n        self.__write_cb = True\n    self.__ob = 0\n    self.__state = 'delete'\n    self.__cb_count = 0\n    return False", "path": "calibre/src/calibre/ebooks/rtf2xml/delete_info.py", "commit_date": "2020-08-22 00:00:00", "repo_name": "kovidgoyal/calibre", "stars": 17955, "license": "gpl-3.0", "language": "python", "size": 301569}
{"docstring": "# Fetch the extension-specific CLI options from the plugin\n# library.catalogs.<format>.py\n", "func_signal": "def add_plugin_parser_options(fmt, parser):\n", "code": "plugin = plugin_for_catalog_format(fmt)\np = parser.add_option_group(_('{} OPTIONS').format(fmt.upper()))\nfor option in plugin.cli_options:\n    if option.action:\n        p.add_option(\n            option.option,\n            default=option.default,\n            dest=option.dest,\n            action=option.action,\n            help=option.help\n        )\n    else:\n        p.add_option(\n            option.option,\n            default=option.default,\n            dest=option.dest,\n            help=option.help\n        )", "path": "calibre/src/calibre/db/cli/cmd_catalog.py", "commit_date": "2020-10-03 00:00:00", "repo_name": "kovidgoyal/calibre", "stars": 17955, "license": "gpl-3.0", "language": "python", "size": 301569}
{"docstring": "'''\nReturns True if all the characters in text have glyphs in this font.\n'''\n", "func_signal": "def supports_text(self, text, has_non_printable_chars=True):\n", "code": "if not isinstance(text, unicode_type):\n    raise TypeError('%r is not a unicode object'%text)\nif has_non_printable_chars:\n    from calibre.utils.fonts.utils import get_printable_characters\n    text = get_printable_characters(text)\nchars = tuple(frozenset(map(ord, text)))\nreturn self.face.supports_text(chars)", "path": "calibre/src/calibre/utils/fonts/free_type.py", "commit_date": "2020-10-18 00:00:00", "repo_name": "kovidgoyal/calibre", "stars": 17955, "license": "gpl-3.0", "language": "python", "size": 301569}
{"docstring": "\"\"\"Handle lines when in delete state. Don't print out lines\nunless the state has ended.\"\"\"\n", "func_signal": "def __delete_func(self,line):\n", "code": "if self.__delete_count == self.__cb_count:\n    self.__state = 'default'\n    if self.__write_cb:\n        self.__write_cb = True\n        return True\n    return False", "path": "calibre/src/calibre/ebooks/rtf2xml/delete_info.py", "commit_date": "2020-08-22 00:00:00", "repo_name": "kovidgoyal/calibre", "stars": 17955, "license": "gpl-3.0", "language": "python", "size": 301569}
{"docstring": "\"\"\"Return the specificity_ of this selector as a tuple of 3 integers.\n\n.. _specificity: http://www.w3.org/TR/selectors/#specificity\n\n\"\"\"\n", "func_signal": "def specificity(self):\n", "code": "a, b, c = self.parsed_tree.specificity()\nif self.pseudo_element:\n    c += 1\nreturn a, b, c", "path": "calibre/src/css_selectors/parser.py", "commit_date": "2020-08-22 00:00:00", "repo_name": "kovidgoyal/calibre", "stars": 17955, "license": "gpl-3.0", "language": "python", "size": 301569}
{"docstring": "'''\n@elem: The element in the etree that we are working on.\n@stylizer: The style information attached to the element.\n'''\n\n# We can only processes tags. If there isn't a tag return any text.\n", "func_signal": "def dump_text(self, elem, stylizer):\n", "code": "if not isinstance(elem.tag, string_or_bytes) \\\n   or namespace(elem.tag) != XHTML_NS:\n    p = elem.getparent()\n    if p is not None and isinstance(p.tag, string_or_bytes) and namespace(p.tag) == XHTML_NS \\\n            and elem.tail:\n        return [elem.tail]\n    return ['']\n\n# Setup our variables.\ntext = []\nstyle = stylizer.style(elem)\ntags = []\ntag = barename(elem.tag)\nattribs = elem.attrib\n\n# Ignore anything that is set to not be displayed.\nif style['display'] in ('none', 'oeb-page-head', 'oeb-page-foot') \\\n   or style['visibility'] == 'hidden':\n    if hasattr(elem, 'tail') and elem.tail:\n        return [elem.tail]\n    return ['']\n\n# Soft scene breaks.\nif 'margin-top' in style.cssdict() and style['margin-top'] != 'auto':\n    ems = int(round(float(style.marginTop) / style.fontSize) - 1)\n    if ems >= 1:\n        text.append(u'\\n\\n' * ems)\n\nbq = '> ' * self.blockquotes\n# Block level elements\nif tag in ('h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'div'):\n    h_tag = ''\n    if tag in ('h1', 'h2', 'h3', 'h4', 'h5', 'h6'):\n        h_tag = '#' * int(tag[1]) + ' '\n    text.append('\\n' + bq + h_tag)\n    tags.append('\\n')\n    self.remove_space_after_newline = True\n\nif style['font-style'] == 'italic' or tag in ('i', 'em'):\n    if tag not in ('h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'cite'):\n        if self.style_italic == False:  # noqa\n            text.append('*')\n            tags.append('*')\n            self.style_italic = True\nif style['font-weight'] in ('bold', 'bolder') or tag in ('b', 'strong'):\n    if tag not in ('h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'th'):\n        if self.style_bold == False:  # noqa\n            text.append('**')\n            tags.append('**')\n            self.style_bold = True\nif tag == 'br':\n    text.append('  \\n')\n    self.remove_space_after_newline = True\nif tag == 'blockquote':\n    self.blockquotes += 1\n    tags.append('>')\n    text.append('> ' * self.blockquotes)\nelif tag == 'code':\n    if not self.in_pre and not self.in_code:\n        text.append('`')\n        tags.append('`')\n        self.in_code = True\nelif tag == 'pre':\n    if not self.in_pre:\n        text.append('\\n')\n        tags.append('pre')\n        self.in_pre = True\nelif tag == 'hr':\n    text.append('\\n* * *')\n    tags.append('\\n')\nelif tag == 'a':\n    # Only write links with absolute (external) urls.\n    if self.opts.keep_links and 'href' in attribs and '://' in attribs['href']:\n        title = ''\n        if 'title' in attribs:\n            title = ' \"' + attribs['title'] + '\"'\n            remove_space = self.remove_space_after_newline\n            title = self.remove_newlines(title)\n            self.remove_space_after_newline = remove_space\n        text.append('[')\n        tags.append('](' + attribs['href'] + title + ')')\nelif tag == 'img':\n    if self.opts.keep_image_references:\n        txt = '!'\n        if 'alt' in attribs:\n            remove_space = self.remove_space_after_newline\n            txt += '[' + self.remove_newlines(attribs['alt']) + ']'\n            self.remove_space_after_newline = remove_space\n        txt += '(' + attribs['src'] + ')'\n        text.append(txt)\nelif tag in ('ol', 'ul'):\n    tags.append(tag)\n    # Add the list to our lists of lists so we can track\n    # nested lists.\n    self.list.append({'name': tag, 'num': 0})\nelif tag == 'li':\n    # Get the last list from our list of lists\n    if self.list:\n        li = self.list[-1]\n    else:\n        li = {'name': 'ul', 'num': 0}\n    # Add a new line to start the item\n    text.append('\\n')\n    # Add indent if we have nested lists.\n    list_count = len(self.list)\n    # We only care about indenting nested lists.\n    if (list_count - 1) > 0:\n        text.append('\\t' * (list_count - 1))\n    # Add blockquote if we have a blockquote in a list item.\n    text.append(bq)\n    # Write the proper sign for ordered and unorded lists.\n    if li['name'] == 'ul':\n        text.append('+ ')\n    elif li['name'] == 'ol':\n        li['num'] += 1\n        text.append(unicode_type(li['num']) + '. ')\n\n# Process tags that contain text.\nif hasattr(elem, 'text') and elem.text:\n    txt = elem.text\n    if self.in_pre:\n        txt = self.prepare_string_for_pre(txt)\n    elif self.in_code:\n        txt = self.remove_newlines(txt)\n    else:\n        txt = self.prepare_string_for_markdown(self.remove_newlines(txt))\n    text.append(txt)\n\n# Recurse down into tags within the tag we are in.\nfor item in elem:\n    text += self.dump_text(item, stylizer)\n\n# Close all open tags.\ntags.reverse()\nfor t in tags:\n    if t in ('pre', 'ul', 'ol', '>'):\n        if t == 'pre':\n            self.in_pre = False\n            text.append('\\n')\n        elif t == '>':\n            self.blockquotes -= 1\n        elif t in ('ul', 'ol'):\n            if self.list:\n                self.list.pop()\n            text.append('\\n')\n    else:\n        if t == '**':\n            self.style_bold = False\n        elif t == '*':\n            self.style_italic = False\n        elif t == '`':\n            self.in_code = False\n        text.append('%s' % t)\n\n# Soft scene breaks.\nif 'margin-bottom' in style.cssdict() and style['margin-bottom'] != 'auto':\n    ems = int(round((float(style.marginBottom) / style.fontSize) - 1))\n    if ems >= 1:\n        text.append(u'\\n\\n' * ems)\n\n# Add the text that is outside of the tag.\nif hasattr(elem, 'tail') and elem.tail:\n    tail = elem.tail\n    if self.in_pre:\n        tail = self.prepare_string_for_pre(tail)\n    elif self.in_code:\n        tail = self.remove_newlines(tail)\n    else:\n        tail = self.prepare_string_for_markdown(self.remove_newlines(tail))\n    text.append(tail)\n\nreturn text", "path": "calibre/src/calibre/ebooks/txt/markdownml.py", "commit_date": "2020-08-22 00:00:00", "repo_name": "kovidgoyal/calibre", "stars": 17955, "license": "gpl-3.0", "language": "python", "size": 301569}
{"docstring": "\"\"\"\nStrip a list of files\n\"\"\"\n", "func_signal": "def strip_files(files, argv_max=(256 * 1024)):\n", "code": "tostrip = [(fn, flipwritable(fn)) for fn in files if os.path.exists(fn)]\nwhile tostrip:\n    cmd = list(STRIPCMD)\n    flips = []\n    pathlen = reduce(operator.add, [len(s) + 1 for s in cmd])\n    while pathlen < argv_max:\n        if not tostrip:\n            break\n        added, flip = tostrip.pop()\n        pathlen += len(added) + 1\n        cmd.append(added)\n        flips.append((added, flip))\n    else:\n        cmd.pop()\n        tostrip.append(flips.pop())\n    os.spawnv(os.P_WAIT, cmd[0], cmd)\n    for args in flips:\n        flipwritable(*args)", "path": "calibre/bypy/macos/__main__.py", "commit_date": "2020-11-25 00:00:00", "repo_name": "kovidgoyal/calibre", "stars": 17955, "license": "gpl-3.0", "language": "python", "size": 301569}
{"docstring": "# We should also be checking for replay attacks by using nonce_count,\n# however, various HTTP clients, most prominently Firefox dont\n# implement nonce-counts correctly, so we cannot do the check.\n# https://bugzil.la/114451\n", "func_signal": "def validate_request(self, pw, data, log=None):\n", "code": "path = parse_uri(self.uri.encode('utf-8'))[1]\nif path != data.path:\n    if log is not None:\n        log.warn('Authorization URI mismatch: %s != %s from client: %s' % (\n            data.path, path, data.remote_addr))\n    raise HTTPSimpleResponse(http_client.BAD_REQUEST, 'The uri in the Request Line and the Authorization header do not match')\nreturn self.response is not None and data.path == path and self.request_digest(pw, data) == self.response", "path": "calibre/src/calibre/srv/auth.py", "commit_date": "2020-08-22 00:00:00", "repo_name": "kovidgoyal/calibre", "stars": 17955, "license": "gpl-3.0", "language": "python", "size": 301569}
{"docstring": "\"\"\"Returns the H(A2) string. See :rfc:`2617` section 3.2.2.3.\"\"\"\n# RFC 2617 3.2.2.3\n# If the \"qop\" directive's value is \"auth\" or is unspecified,\n# then A2 is:\n#    A2 = method \":\" digest-uri-value\n#\n# If the \"qop\" value is \"auth-int\", then A2 is:\n#    A2 = method \":\" digest-uri-value \":\" H(entity-body)\n", "func_signal": "def H_A2(self, data):\n", "code": "if self.qop == \"auth-int\":\n    a2 = \"%s:%s:%s\" % (data.method, self.uri, self.H(data.peek()))\nelse:\n    a2 = '%s:%s' % (data.method, self.uri)\nreturn self.H(a2)", "path": "calibre/src/calibre/srv/auth.py", "commit_date": "2020-08-22 00:00:00", "repo_name": "kovidgoyal/calibre", "stars": 17955, "license": "gpl-3.0", "language": "python", "size": 301569}
{"docstring": "\"\"\"\nParses the arguments for :nth-child() and friends.\n\n:raises: A list of tokens\n:returns: :``(a, b)``\n\n\"\"\"\n", "func_signal": "def parse_series(tokens):\n", "code": "for token in tokens:\n    if token.type == 'STRING':\n        raise ValueError('String tokens not allowed in series.')\ns = ''.join(token.value for token in tokens).strip()\nif s == 'odd':\n    return (2, 1)\nelif s == 'even':\n    return (2, 0)\nelif s == 'n':\n    return (1, 0)\nif 'n' not in s:\n    # Just b\n    return (0, int(s))\na, b = s.split('n', 1)\nif not a:\n    a = 1\nelif a == '-' or a == '+':\n    a = int(a+'1')\nelse:\n    a = int(a)\nif not b:\n    b = 0\nelse:\n    b = int(b)\nreturn (a, b)", "path": "calibre/src/css_selectors/parser.py", "commit_date": "2020-08-22 00:00:00", "repo_name": "kovidgoyal/calibre", "stars": 17955, "license": "gpl-3.0", "language": "python", "size": 301569}
{"docstring": "\"\"\"\nCheck to see if the group has ended.\nReturn True for all control words.\nReturn False otherwise.\n\"\"\"\n", "func_signal": "def __list_func(self, line):\n", "code": "if self.__delete_count == self.__cb_count and \\\n        self.__token_info == 'cb<nu<clos-brack':\n    self.__state = 'default'\n    if self.__write_cb:\n        self.__write_cb = False\n        return True\n    return False\nelif line[0:2] == 'cw':\n    return True\nelse:\n    return False", "path": "calibre/src/calibre/ebooks/rtf2xml/delete_info.py", "commit_date": "2020-08-22 00:00:00", "repo_name": "kovidgoyal/calibre", "stars": 17955, "license": "gpl-3.0", "language": "python", "size": 301569}
{"docstring": "\"\"\"Return the list of ranks whose coordinates match the provided criteria.\n\nExample:\n    >>> X = ProcessTopology(axes=['pipe', 'data', 'model'], dims=[2, 2, 2])\n    >>> X.filter_match(pipe=0, data=1)\n    [2, 3]\n    >>> [X.get_coord(rank) for rank in X.filter_match(pipe=0, data=1)]\n    [ProcessCoord(pipe=0, data=1, model=0), ProcessCoord(pipe=0, data=1, model=1)]\n\nArguments:\n    **filter_kwargs (dict): criteria used to select coordinates.\n\nReturns:\n    The list of ranks whose coordinates match filter_kwargs.\n\"\"\"\n", "func_signal": "def filter_match(self, **filter_kwargs):\n", "code": "def _filter_helper(x):\n    for key, val in filter_kwargs.items():\n        if getattr(x, key) != val:\n            return False\n    return True\n\ncoords = filter(_filter_helper, self.mapping.keys())\nreturn [self.mapping[coo] for coo in coords]", "path": "DeepSpeed/deepspeed/runtime/pipe/topology.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "microsoft/DeepSpeed", "stars": 31725, "license": "apache-2.0", "language": "python", "size": 211815}
{"docstring": "\"\"\"\nNot supporting closure.\n\"\"\"\n", "func_signal": "def step(self, closure=None):\n", "code": "if self.fused_lamb_legacy:\n    return self.step_fused_lamb()\n\nself.overflow = self.overflow_checker.check()\nprev_scale = self.cur_scale\n\nself._update_scale(self.overflow)\nif self.overflow:\n    if self.verbose:\n        logger.info(\"[deepspeed] OVERFLOW! Skipping step. Attempted loss \"\n                    \"scale: {}, reducing to {}\".format(\n                        prev_scale,\n                        self.cur_scale))\n    return self.overflow\n\nnorm_groups = []\nfor i, group in enumerate(self.fp16_groups):\n    norm_groups.append(get_grad_norm(group, mpu=self.mpu))\n\n    # copying gradients to fp32 to work with fp32 parameters\n    for fp32_param, fp16_param in zip(self.fp32_groups[i], self.fp16_groups[i]):\n        if fp16_param.grad is None:\n            fp32_param.grad = torch.zeros(fp16_param.size(),\n                                          dtype=fp32_param.dtype,\n                                          device=fp32_param.device)\n        else:\n            fp32_param.grad = fp16_param.grad.to(fp32_param.dtype)\n\nself.unscale_and_clip_grads(norm_groups)\n\nself.optimizer.step()\n\nfor fp32_group, fp16_group in zip(self.fp32_groups, self.fp16_groups):\n    for fp32_param, fp16_param in zip(fp32_group, fp16_group):\n\n        #remove the fp32 grad\n        fp32_param.grad = None\n\n        #copy data from fp32 to fp16\n        fp16_param.data.copy_(fp32_param.data)\n\nreturn self.overflow", "path": "DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "microsoft/DeepSpeed", "stars": 31725, "license": "apache-2.0", "language": "python", "size": 211815}
{"docstring": "\"\"\"Return the coordinate owned by a process rank.\n\nThe axes of the returned namedtuple can be directly accessed as members. For\nexample:\n    >>> X = ProcessTopology(axes=['x', 'y'], dims=[2,3])\n    >>> coord = X.get_coord(rank=1)\n    >>> coord.x\n    0\n    >>> coord.y\n    1\n\"\"\"\n", "func_signal": "def get_coord(self, rank):\n", "code": "for coord, idx in self.mapping.items():\n    if idx == rank:\n        return coord\nraise ValueError(f'rank {rank} not found in topology.')", "path": "DeepSpeed/deepspeed/runtime/pipe/topology.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "microsoft/DeepSpeed", "stars": 31725, "license": "apache-2.0", "language": "python", "size": 211815}
{"docstring": "\"\"\"This function updates the position embedding length of a tokenizer to a new max position.\n\nArguments:\n    tokenizer: required: a transformer tokenizer\n    max_position: required: an integer determining new position embedding size\nReturn:\n    tokenizer: updated tokenizer; in which model maximum length has been extended based on new size\n\"\"\"\n\n", "func_signal": "def update_tokenizer_model_max_length(tokenizer, max_position):\n", "code": "tokenizer.model_max_length = max_position\ntokenizer.init_kwargs['model_max_length'] = max_position\nprint(f'updated tokenizer model max imum length to {max_position}')\n\nreturn tokenizer", "path": "DeepSpeed/deepspeed/ops/sparse_attention/sparse_attention_utils.py", "commit_date": "2020-09-02 00:00:00", "repo_name": "microsoft/DeepSpeed", "stars": 31725, "license": "apache-2.0", "language": "python", "size": 211815}
{"docstring": "# TODO use process_group if provided\n", "func_signal": "def __init__(self, topology=None, process_group=None):\n", "code": "self.global_rank = dist.get_rank()\nself.world_size = dist.get_world_size()\nif topology is not None:\n    if self.global_rank == 0:\n        print('Using topology:', topology)\n    self._topo = topology\nelse:\n    num_pp = 1\n    num_dp = 1\n    for idx, prime in enumerate(_prime_factors(self.world_size)):\n        if idx % 2 == 0:\n            num_pp *= prime\n        else:\n            num_dp *= prime\n    self._topo = PipeDataParallelTopology(num_dp=num_dp, num_pp=num_pp)\nself.data_parallel_size = max(self._topo.get_dim('data'), 1)\nself.pipe_parallel_size = max(self._topo.get_dim('pipe'), 1)\nself.model_parallel_size = max(self._topo.get_dim('model'), 1)\nassert self._is_grid_valid(), \"Invalid Grid\"\n\nself.stage_id = self.get_stage_id()\nself.data_parallel_id = self.get_data_parallel_id()\n\n# Create new ProcessGroups for all model parallelism. DeepSpeedLight uses these\n# to detect overflow, etc.\nself.ds_model_proc_group = None\nself.ds_model_rank = -1\nfor dp in range(self.data_parallel_size):\n    ranks = sorted(self._topo.get_axis_list(axis='data', idx=dp))\n    if self.global_rank == 0:\n        #print(f'RANK={self.global_rank} building DeepSpeed model group: {ranks}')\n        pass\n    proc_group = dist.new_group(ranks=ranks)\n    if self.global_rank in ranks:\n        self.ds_model_proc_group = proc_group\n        self.ds_model_world_size = len(ranks)\n        self.ds_model_rank = ranks.index(self.global_rank)\nassert self.ds_model_rank > -1\nassert self.ds_model_proc_group is not None\n\n# Create new ProcessGroup for gradient all-reduces - these are the data parallel groups\nself.dp_group = []\nself.dp_groups = self._topo.get_axis_comm_lists('data')\nfor g in self.dp_groups:\n    proc_group = dist.new_group(ranks=g)\n    if self.global_rank in g:\n        self.dp_group = g\n        self.dp_proc_group = proc_group\n\nself.is_first_stage = (self.stage_id == 0)\nself.is_last_stage = (self.stage_id == (self.pipe_parallel_size - 1))\n\nself.p2p_groups = self._build_p2p_groups()\n\n# Create new ProcessGroup for pipeline collectives - these are pipe parallel groups\nself.pp_group = []\nself.pp_proc_group = None\nself.pipe_groups = self._topo.get_axis_comm_lists('pipe')\nfor ranks in self.pipe_groups:\n    if self.global_rank == 0:\n        #print(f'RANK={self.global_rank} building pipeline group: {ranks}')\n        pass\n    proc_group = dist.new_group(ranks=ranks)\n    if self.global_rank in ranks:\n        self.pp_group = ranks\n        self.pp_proc_group = proc_group\nassert self.pp_proc_group is not None\n\n# Create new ProcessGroup for model (tensor-slicing) collectives\n\n# Short circuit case without model parallelism.\n# TODO: it would be nice if topology had bcast semantics to avoid this branching\n# case?\nif self.model_parallel_size == 1:\n    for group_rank in range(self.world_size):\n        group_rank = [group_rank]\n        group = dist.new_group(ranks=group_rank)\n        if group_rank[0] == self.global_rank:\n            self.slice_group = group_rank\n            self.slice_proc_group = group\n    return\nelse:\n    self.mp_group = []\n    self.model_groups = self._topo.get_axis_comm_lists('model')\n    for g in self.model_groups:\n        proc_group = dist.new_group(ranks=g)\n        if self.global_rank in g:\n            self.slice_group = g\n            self.slice_proc_group = proc_group", "path": "DeepSpeed/deepspeed/runtime/pipe/topology.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "microsoft/DeepSpeed", "stars": 31725, "license": "apache-2.0", "language": "python", "size": 211815}
{"docstring": "\"\"\"Returns the list of global ranks whose coordinate in an axis is idx.\n\nFor example:\n    >>> X = ProcessTopology(axes=['x', 'y'], dims=[2,3])\n    >>> X.get_axis_list(axis='x', idx=0)\n    [0, 1, 2]\n    >>> X.get_axis_list(axis='y', idx=0)\n    [0, 3]\n\"\"\"\n\n# This could be faster by generating the desired keys directly instead of\n# filtering.\n", "func_signal": "def get_axis_list(self, axis, idx):\n", "code": "axis_num = self.axes.index(axis)\nranks = [self.mapping[k] for k in self.mapping.keys() if k[axis_num] == idx]\nreturn ranks", "path": "DeepSpeed/deepspeed/runtime/pipe/topology.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "microsoft/DeepSpeed", "stars": 31725, "license": "apache-2.0", "language": "python", "size": 211815}
{"docstring": "\"\"\"\nZero FP16 parameter grads.\n\"\"\"\n# FP32 grad should never exist outside of the step function\n# For speed, set model fp16 grad to None by default\n", "func_signal": "def zero_grad(self, set_grads_to_None=True):\n", "code": "for group in self.fp16_groups:\n    for p in group:\n        if set_grads_to_None:\n            p.grad = None\n        else:\n            if p.grad is not None:\n                p.grad.detach_()\n                p.grad.zero_()", "path": "DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "microsoft/DeepSpeed", "stars": 31725, "license": "apache-2.0", "language": "python", "size": 211815}
{"docstring": "\"\"\"\nReturns a dict containing the current state of this :class:`FP16_Optimizer` instance.\nThis dict contains attributes of :class:`FP16_Optimizer`, as well as the state_dict\nof the contained Pytorch optimizer.\nExample::\n    checkpoint = {}\n    checkpoint['model'] = model.state_dict()\n    checkpoint['optimizer'] = optimizer.state_dict()\n    torch.save(checkpoint, \"saved.pth\")\n\"\"\"\n", "func_signal": "def state_dict(self):\n", "code": "state_dict = {}\nstate_dict['dynamic_loss_scale'] = self.dynamic_loss_scale\nstate_dict['cur_scale'] = self.cur_scale\nstate_dict['cur_iter'] = self.cur_iter\nif state_dict['dynamic_loss_scale']:\n    state_dict['last_overflow_iter'] = self.last_overflow_iter\n    state_dict['scale_factor'] = self.scale_factor\n    state_dict['scale_window'] = self.scale_window\nstate_dict['optimizer_state_dict'] = self.optimizer.state_dict()\nstate_dict['fp32_groups'] = self.fp32_groups\nreturn state_dict", "path": "DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "microsoft/DeepSpeed", "stars": 31725, "license": "apache-2.0", "language": "python", "size": 211815}
{"docstring": "\"\"\"This function extends the position embedding weights of a model loaded from a checkpoint.\nIt assumes the new max position is bigger than the original max length.\n\nArguments:\n    model: required: a transformer model\n    max_position: required: an integer determining new position embedding size\nReturn:\n    model: updated model; in which position embedding weights have been extended based on new size\n\"\"\"\n\n", "func_signal": "def extend_position_embedding(model, max_position):\n", "code": "if hasattr(model, 'bert'):\n    original_max_position = model.bert.embeddings.position_embeddings.weight.size(\n        0)\n    assert max_position > original_max_position\n    extend_multiples = max(1, max_position // original_max_position)\n    model.bert.embeddings.position_embeddings.weight.data = model.bert.embeddings.position_embeddings.weight.repeat(\n        extend_multiples,\n        1)\nelif hasattr(model, 'roberta'):\n    # RoBERTa has positions 0 & 1 reserved, so embedding size is max position + 2\n    original_max_position, embed_size = model.roberta.embeddings.position_embeddings.weight.shape\n    original_max_position -= 2\n    extend_multiples = max(1, max_position // original_max_position)\n    assert max_position > original_max_position\n    max_position += 2\n    extended_position_embedding = model.roberta.embeddings.position_embeddings.weight.new_empty(\n        max_position,\n        embed_size)\n    k = 2\n    for i in range(extend_multiples):\n        extended_position_embedding[k:(\n            k + original_max_position\n        )] = model.roberta.embeddings.position_embeddings.weight[2:]\n        k += original_max_position\n    model.roberta.embeddings.position_embeddings.weight.data = extended_position_embedding\nelse:\n    raise ValueError(\n        'Please extend \\\"extend_position_embedding\\\" function to support your model type. It currently only supports \\\"bert\\\" & \\\"roberta\\\"!'\n    )\n\nmodel.config.max_position_embeddings = max_position\nprint(\n    f'Extended position embeddings to {original_max_position * extend_multiples}'\n)\n\nreturn model", "path": "DeepSpeed/deepspeed/ops/sparse_attention/sparse_attention_utils.py", "commit_date": "2020-09-02 00:00:00", "repo_name": "microsoft/DeepSpeed", "stars": 31725, "license": "apache-2.0", "language": "python", "size": 211815}
{"docstring": "\"\"\"This function unpads sequence output if inputs of the model were padded.\n   This is a requirement for Sparse Transformer in which the self attention layer works on sequences of length multiple of block size.\n   It needs to be called in your model, such as BertModel, right before you return the model outputs.\n\n   Arguments:\n       pad_len: required: an integer determining how much model inputs have been padded to transfer sequence length dimension to multiple of block size.\n       sequence_output: required: sequence output of the encoder layer.\n\n   Return:\n       sequence_output: unpaded sequence output of the encoder layer.\n\"\"\"\n\n", "func_signal": "def unpad_sequence_output(pad_len, sequence_output):\n", "code": "if (pad_len > 0):\n    sequence_output = sequence_output[:, :-pad_len]\nreturn sequence_output", "path": "DeepSpeed/deepspeed/ops/sparse_attention/sparse_attention_utils.py", "commit_date": "2020-09-02 00:00:00", "repo_name": "microsoft/DeepSpeed", "stars": 31725, "license": "apache-2.0", "language": "python", "size": 211815}
{"docstring": "\"\"\"Return the number of processes along the given axis.\n\nFor example:\n    >>> X = ProcessTopology(axes=['x', 'y'], dims=[2,3])\n    >>> X.get_dim('y')\n    3\n\"\"\"\n", "func_signal": "def get_dim(self, axis):\n", "code": "if axis not in self.axes:\n    return 0\nreturn self.dims[self.axes.index(axis)]", "path": "DeepSpeed/deepspeed/runtime/pipe/topology.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "microsoft/DeepSpeed", "stars": 31725, "license": "apache-2.0", "language": "python", "size": 211815}
{"docstring": "\"\"\"Applies forward phase of bert sparse self attention\n\nArguments:\n    hidden_states: required: hidde_states tensor of the bert model\n    attn_mask: required: a mask tensor of size (SequenceLength X SequenceLength); currently only 2D is supported\n\nReturn:\n     context_layer: a dense tensor containing attnetion context\n\"\"\"\n", "func_signal": "def forward(self, hidden_states, attention_mask):\n", "code": "mixed_query_layer = self.query(hidden_states)\nmixed_key_layer = self.key(hidden_states)\nmixed_value_layer = self.value(hidden_states)\n\nquery_layer = self.transpose_for_scores(mixed_query_layer)\nkey_layer = self.transpose_for_scores(mixed_key_layer)\nvalue_layer = self.transpose_for_scores(mixed_value_layer)\n\ncontext_layer = self.sparse_self_attention(query_layer,\n                                           key_layer,\n                                           value_layer,\n                                           key_padding_mask=attention_mask)\n\ncontext_layer = context_layer.permute(0, 2, 1, 3).contiguous()\nnew_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size, )\ncontext_layer = context_layer.view(*new_context_layer_shape)\nreturn context_layer", "path": "DeepSpeed/deepspeed/ops/sparse_attention/bert_sparse_self_attention.py", "commit_date": "2020-09-02 00:00:00", "repo_name": "microsoft/DeepSpeed", "stars": 31725, "license": "apache-2.0", "language": "python", "size": 211815}
{"docstring": "\"\"\" Construct lists suitable for a communicator group along axis ``axis``.\n\nExample:\n    >>> topo = Topo(axes=['pipe', 'data', 'model'], dims=[2, 2, 2])\n    >>> topo.get_axis_comm_lists('pipe')\n    [\n        [0, 4], # data=0, model=0\n        [1, 5], # data=0, model=1\n        [2, 6], # data=1, model=0\n        [3, 7], # data=1, model=1\n    ]\n\nReturns:\n    A list of lists whose coordinates match in all axes *except* ``axis``.\n\"\"\"\n\n# We don't want to RuntimeError because it allows us to write more generalized\n# code for hybrid parallelisms.\n", "func_signal": "def get_axis_comm_lists(self, axis):\n", "code": "if axis not in self.axes:\n    return []\n\n# Grab all axes but `axis`\nother_axes = [a for a in self.axes if a != axis]\n\nlists = []\n\n# Construct all combinations of coords with other_axes\nranges = [range(self.get_dim(a)) for a in other_axes]\nfor coord in cartesian_product(*ranges):\n    other_keys = {a: coord[other_axes.index(a)] for a in other_axes}\n    # now go over all ranks in `axis`.\n    sub_list = []\n    for axis_key in range(self.get_dim(axis)):\n        key = self.ProcessCoord(**other_keys, **{axis: axis_key})\n        sub_list.append(self.mapping[key])\n    lists.append(sub_list)\n\nreturn lists", "path": "DeepSpeed/deepspeed/runtime/pipe/topology.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "microsoft/DeepSpeed", "stars": 31725, "license": "apache-2.0", "language": "python", "size": 211815}
{"docstring": "\"\"\"Groups for sending and receiving activations and gradients across model\nparallel stages.\n\"\"\"\n", "func_signal": "def _build_p2p_groups(self):\n", "code": "comm_lists = self._topo.get_axis_comm_lists('pipe')\np2p_lists = []\nfor rank in range(self.world_size):\n    for l in comm_lists:\n        assert len(l) == self.pipe_parallel_size\n        if rank in l:\n            idx = l.index(rank)\n            buddy_rank = l[(idx + 1) % self.pipe_parallel_size]\n            p2p_lists.append([rank, buddy_rank])\n            break  # next global rank\nassert len(p2p_lists) == self.world_size\nreturn p2p_lists", "path": "DeepSpeed/deepspeed/runtime/pipe/topology.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "microsoft/DeepSpeed", "stars": 31725, "license": "apache-2.0", "language": "python", "size": 211815}
{"docstring": "\"\"\"\n:attr:`backward` performs the following steps:\n\n1. fp32_loss = loss.float()\n2. scaled_loss = fp32_loss*loss_scale\n3. scaled_loss.backward(), which accumulates scaled gradients into the ``.grad`` attributes of the model's fp16 leaves\n\"\"\"\n", "func_signal": "def backward(self, loss):\n", "code": "scaled_loss = (loss.float()) * self.cur_scale\nscaled_loss.backward()", "path": "DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "microsoft/DeepSpeed", "stars": 31725, "license": "apache-2.0", "language": "python", "size": 211815}
{"docstring": "\"\"\"\nNot supporting closure.\n\"\"\"\n# First compute norm for all group so we know if there is overflow\n", "func_signal": "def step_fused_lamb(self, closure=None):\n", "code": "grads_groups_flat = []\ngrads_groups = []\nnorm_groups = []\nfor i, group in enumerate(self.fp16_groups):\n    grads = [\n        torch.zeros(p.size(),\n                    dtype=p.dtype,\n                    device=p.device) if p.grad is None else p.grad for p in group\n    ]\n    grads_groups.append(grads)\n    grads_groups_flat.append(_flatten_dense_tensors(grads))\n    norm_groups.append(get_weight_norm(grads_groups_flat[i], mpu=self.mpu))\n\nself.overflow = self.overflow_checker.check_using_norm(norm_groups)\nprev_scale = self.cur_scale\n\nself._update_scale(self.overflow)\nif self.overflow:\n    if self.verbose:\n        logger.info(\"[deepspeed] OVERFLOW! Skipping step. Attempted loss \"\n                    \"scale: {}, reducing to {}\".format(\n                        prev_scale,\n                        self.cur_scale))\n    return self.overflow\n\ncombined_scale = self.unscale_and_clip_grads(norm_groups, apply_scale=False)\nself.optimizer.step(grads=grads_groups,\n                    output_params=self.fp16_groups,\n                    scale=combined_scale)\n\nreturn self.overflow", "path": "DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "microsoft/DeepSpeed", "stars": 31725, "license": "apache-2.0", "language": "python", "size": 211815}
{"docstring": "\"\"\"\nLoads a state_dict created by an earlier call to state_dict().\nIf ``fp16_optimizer_instance`` was constructed from some ``init_optimizer``,\nwhose parameters in turn came from ``model``, it is expected that the user\nwill call ``model.load_state_dict()`` before\n``fp16_optimizer_instance.load_state_dict()`` is called.\nExample::\n    model = torch.nn.Linear(D_in, D_out).cuda().half()\n    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n    optimizer = FP16_Optimizer(optimizer, static_loss_scale = 128.0)\n    ...\n    checkpoint = torch.load(\"saved.pth\")\n    model.load_state_dict(checkpoint['model'])\n    optimizer.load_state_dict(checkpoint['optimizer'])\n\"\"\"\n# I think it should actually be ok to reload the optimizer before the model.\n", "func_signal": "def load_state_dict(self, state_dict, load_optimizer_states=True):\n", "code": "self.dynamic_loss_scale = state_dict['dynamic_loss_scale']\nself.cur_scale = state_dict['cur_scale']\nself.cur_iter = state_dict['cur_iter']\nif state_dict['dynamic_loss_scale']:\n    self.last_overflow_iter = state_dict['last_overflow_iter']\n    self.scale_factor = state_dict['scale_factor']\n    self.scale_window = state_dict['scale_window']\n\nif load_optimizer_states:\n    self.optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n# At this point, the optimizer's references to the model's fp32 parameters are up to date.\n# The optimizer's hyperparameters and internal buffers are also up to date.\n# However, the fp32 master copies of the model's fp16 params stored by the optimizer are still\n# out of date.  There are two options.\n# 1:  Refresh the master params from the model's fp16 params.\n# This requires less storage but incurs precision loss.\n# 2:  Save and restore the fp32 master copies separately.\n# We choose option 2.\n#\n# Pytorch Optimizer.load_state_dict casts saved buffers (e.g. momentum) to the type and device\n# of their associated parameters, because it's possible those buffers might not exist yet in\n# the current optimizer instance.  In our case, as long as the current FP16_Optimizer has been\n# constructed in the same way as the one whose state_dict we are loading, the same master params\n# are guaranteed to exist, so we can just copy_() from the saved master params.\nfor current_group, saved_group in zip(self.fp32_groups, state_dict['fp32_groups']):\n    for current, saved in zip(current_group, saved_group):\n        current.data.copy_(saved.data)", "path": "DeepSpeed/deepspeed/runtime/fp16/unfused_optimizer.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "microsoft/DeepSpeed", "stars": 31725, "license": "apache-2.0", "language": "python", "size": 211815}
{"docstring": "\"\"\"Build an index of source files and their basenames in a given directory.\n\nArgs:\n    directory (string): the directory to index\n    suffix (string): index files with this suffix\n\nReturns:\n    list: A list of tuples of the form [(basename, absolute path), ...]\n\"\"\"\n\n", "func_signal": "def _build_file_index(directory, suffix='.tr'):\n", "code": "index = []\n\nfor fname in os.listdir(directory):\n    if fname.endswith(suffix):\n        basename = fname[:fname.rfind(suffix)]  # strip the suffix\n        path = os.path.join(directory, fname)\n        index.append((basename, path))\n\nreturn index", "path": "DeepSpeed/deepspeed/ops/sparse_attention/trsrc/__init__.py", "commit_date": "2020-09-02 00:00:00", "repo_name": "microsoft/DeepSpeed", "stars": 31725, "license": "apache-2.0", "language": "python", "size": 211815}
{"docstring": "\"\"\"Return the global rank of a process via its coordinates.\n\nCoordinates are specified as kwargs. For example:\n\n    >>> X = ProcessTopology(axes=['x', 'y'], dims=[2,3])\n    >>> X.get_rank(x=0, y=1)\n    1\n\"\"\"\n", "func_signal": "def get_rank(self, **coord_kwargs):\n", "code": "if len(coord_kwargs) != len(self.axes):\n    raise ValueError('get_rank() does not support slices. Use filter_match())')\n\nkey = self.ProcessCoord(**coord_kwargs)\nassert key in self.mapping, f'key {kwargs} invalid'\nreturn self.mapping[key]", "path": "DeepSpeed/deepspeed/runtime/pipe/topology.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "microsoft/DeepSpeed", "stars": 31725, "license": "apache-2.0", "language": "python", "size": 211815}
{"docstring": "\"\"\"Create a mapping of n-dimensional tensor coordinates to linear indices.\n\nArguments:\n    axes (list): the names of the tensor axes\n    dims (list): the dimension (length) of each axis of the topology tensor\n\"\"\"\n\n", "func_signal": "def __init__(self, axes, dims):\n", "code": "self.axes = axes  # names of each topology axis\nself.dims = dims  # length of each topology axis\n\n# This is actually a class that lets us hash {'row':3, 'col':2} mappings\nself.ProcessCoord = namedtuple('ProcessCoord', axes)\n\nself.mapping = {}\nranges = [range(d) for d in dims]\n# example: 1, (0,0,1)\nfor global_rank, coord in enumerate(cartesian_product(*ranges)):\n    key = {axis: coord[self.axes.index(axis)] for axis in self.axes}\n    key = self.ProcessCoord(**key)\n    # for example, {ProcessCoord(row=0, col=1) : 1}\n    self.mapping[key] = global_rank", "path": "DeepSpeed/deepspeed/runtime/pipe/topology.py", "commit_date": "2020-09-10 00:00:00", "repo_name": "microsoft/DeepSpeed", "stars": 31725, "license": "apache-2.0", "language": "python", "size": 211815}
{"docstring": "\"\"\"Copy files from one location to another.\n\nThe source and destination must both be s3 paths or both be local paths.\n\n:type source: str\n:param source: A local path or s3 path to backup.\n\n:type destination: str\n:param destination: A local path or s3 path to backup the source to.\n\n:type recursive: bool\n:param recursive: if True, the source will be treated as a directory.\n\"\"\"\n", "func_signal": "def copy(source, destination, recursive):\n", "code": "if 's3://' in [source[:5], destination[:5]]:\n    cp_args = ['aws', 's3', 'cp', source, destination, '--quiet']\n    if recursive:\n        cp_args.append('--recursive')\n    subprocess.check_call(cp_args)\n    return\n\nif recursive:\n    shutil.copytree(source, destination)\nelse:\n    shutil.copy(source, destination)", "path": "aws-cli/scripts/performance/benchmark_utils.py", "commit_date": "2016-09-21 00:00:00", "repo_name": "aws/aws-cli", "stars": 14741, "license": "other", "language": "python", "size": 206027}
{"docstring": "\"\"\"Backup a given source to a temporary location.\n\n:type source: str\n:param source: A local path or s3 path to backup.\n\n:type recursive: bool\n:param recursive: if True, the source will be treated as a directory.\n\"\"\"\n", "func_signal": "def backup(source, recursive):\n", "code": "if source[:5] == 's3://':\n    parts = source.split('/')\n    parts.insert(3, str(uuid.uuid4()))\n    backup_path = '/'.join(parts)\nelse:\n    name = os.path.split(source)[-1]\n    temp_dir = tempfile.mkdtemp()\n    backup_path = os.path.join(temp_dir, name)\n\ncopy(source, backup_path, recursive)\nreturn backup_path", "path": "aws-cli/scripts/performance/benchmark_utils.py", "commit_date": "2016-09-21 00:00:00", "repo_name": "aws/aws-cli", "stars": 14741, "license": "other", "language": "python", "size": 206027}
{"docstring": "\"\"\"Get an ArgumentParser with all the base benchmark arguments added in.\"\"\"\n", "func_signal": "def get_default_argparser():\n", "code": "parser = argparse.ArgumentParser()\nparser.add_argument(\n    '--no-cleanup', action='store_true', default=False,\n    help='Do not remove the destination after the tests complete.'\n)\nparser.add_argument(\n    '--recursive', action='store_true', default=False,\n    help='Indicates that this is a recursive transfer.'\n)\nbenchmark_script = get_benchmark_script()\nparser.add_argument(\n    '--benchmark-script', default=benchmark_script,\n    required=benchmark_script is None,\n    help=('The benchmark script to run the commands with. This should be '\n          'from s3transfer.')\n)\nsummarize_script = get_summarize_script()\nparser.add_argument(\n    '--summarize-script', default=summarize_script,\n    required=summarize_script is None,\n    help=('The summarize script to run the commands with. This should be '\n          'from s3transfer.')\n)\nparser.add_argument(\n    '-o', '--result-dir', default='results',\n    help='The directory to output performance results to. Existing '\n         'results will be deleted.'\n)\nparser.add_argument(\n    '--dry-run', default=False, action='store_true',\n    help='If set, commands will only be printed out, not executed.'\n)\nparser.add_argument(\n    '--quiet', default=False, action='store_true',\n    help='If set, output is suppressed.'\n)\nparser.add_argument(\n    '-n', '--num-iterations', default=1, type=int,\n    help='The number of times to run the test.'\n)\nreturn parser", "path": "aws-cli/scripts/performance/benchmark_utils.py", "commit_date": "2016-09-21 00:00:00", "repo_name": "aws/aws-cli", "stars": 14741, "license": "other", "language": "python", "size": 206027}
{"docstring": "\"\"\"Create a random subdirectory in a given directory.\"\"\"\n", "func_signal": "def create_random_subfolder(destination):\n", "code": "folder_name = str(uuid.uuid4())\nif destination.startswith('s3://'):\n    parts = destination.split('/')\n    parts.append(folder_name)\n    return '/'.join(parts)\nelse:\n    parts = list(os.path.split(destination))\n    parts.append(folder_name)\n    path = os.path.join(*parts)\n    os.makedirs(path)\n    return path", "path": "aws-cli/scripts/performance/benchmark_utils.py", "commit_date": "2016-09-21 00:00:00", "repo_name": "aws/aws-cli", "stars": 14741, "license": "other", "language": "python", "size": 206027}
{"docstring": "\"\"\"Retrieves an s3transfer performance script if available.\"\"\"\n", "func_signal": "def _get_s3transfer_performance_script(script_name):\n", "code": "s3transfer_directory = os.path.dirname(s3transfer.__file__)\ns3transfer_directory = os.path.dirname(s3transfer_directory)\nscripts_directory = 'scripts/performance'\nscripts_directory = os.path.join(s3transfer_directory, scripts_directory)\nscript = os.path.join(scripts_directory, script_name)\n\nif os.path.isfile(script):\n    return script\nelse:\n    return None", "path": "aws-cli/scripts/performance/benchmark_utils.py", "commit_date": "2016-09-21 00:00:00", "repo_name": "aws/aws-cli", "stars": 14741, "license": "other", "language": "python", "size": 206027}
{"docstring": "\"\"\"Method to verify if a particular role exists\"\"\"\n", "func_signal": "def check_if_role_exists(self, role_name):\n", "code": "try:\n    self.iam_client.get_role(RoleName=role_name)\nexcept self.iam_client.exceptions.NoSuchEntityException:\n    return False\nreturn True", "path": "aws-cli/awscli/customizations/dlm/iam.py", "commit_date": "2018-07-10 00:00:00", "repo_name": "aws/aws-cli", "stars": 14741, "license": "other", "language": "python", "size": 206027}
{"docstring": "# Because we disable parsing blobs into a binary type and leave them as\n# a base64 string if a binary field is present in the continuation token\n# as is the case with dynamodb the binary will be double encoded. This\n# ensures that the continuation token is properly converted to binary to\n# avoid double encoding the contination token.\n", "func_signal": "def parse_last_evaluated_key_binary(parsed, **kwargs):\n", "code": "last_evaluated_key = parsed.get('LastEvaluatedKey', None)\nif last_evaluated_key is None:\n    return\nfor key, val in last_evaluated_key.items():\n    if 'B' in val:\n        val['B'] = base64.b64decode(val['B'])", "path": "aws-cli/awscli/customizations/dynamodb.py", "commit_date": "2020-01-08 00:00:00", "repo_name": "aws/aws-cli", "stars": 14741, "license": "other", "language": "python", "size": 206027}
{"docstring": "\"\"\"\nConverts arguments that are passed as list of \"Key=Value\" strings\ninto a real dictionary.\n\n:param arg_value list: Array of strings, where each string is of\n    form Key=Value\n:param argname string: Name of the argument that contains the value\n:return dict: Dictionary representing the key/value pairs\n\"\"\"\n", "func_signal": "def parse_key_value_arg(self, arg_value, argname):\n", "code": "result = {}\nfor data in arg_value:\n\n    # Split at first '=' from left\n    key_value_pair = data.split(\"=\", 1)\n\n    if len(key_value_pair) != 2:\n        raise exceptions.InvalidKeyValuePairArgumentError(\n                argname=argname,\n                value=key_value_pair)\n\n    result[key_value_pair[0]] = key_value_pair[1]\n\nreturn result", "path": "aws-cli/awscli/customizations/cloudformation/deploy.py", "commit_date": "2019-04-16 00:00:00", "repo_name": "aws/aws-cli", "stars": 14741, "license": "other", "language": "python", "size": 206027}
{"docstring": "\"\"\"Run the given summary script on every file in the given directory.\n\n:param script: A summarization script that takes a list of csv files.\n:param result_dir: A directory containing csv performance result files.\n:param summary_dir: The directory to put the summary file in.\n\"\"\"\n", "func_signal": "def summarize(script, result_dir, summary_dir):\n", "code": "summarize_args = [script]\nfor f in os.listdir(result_dir):\n    path = os.path.join(result_dir, f)\n    if os.path.isfile(path):\n        summarize_args.append(path)\n\nwith open(os.path.join(summary_dir, 'summary.txt'), 'wb') as f:\n    subprocess.check_call(summarize_args, stdout=f)\nwith open(os.path.join(summary_dir, 'summary.json'), 'wb') as f:\n    summarize_args.extend(['--output-format', 'json'])\n    subprocess.check_call(summarize_args, stdout=f)", "path": "aws-cli/scripts/performance/benchmark_utils.py", "commit_date": "2016-09-21 00:00:00", "repo_name": "aws/aws-cli", "stars": 14741, "license": "other", "language": "python", "size": 206027}
{"docstring": "\"\"\"\nCloudFormation CreateChangeset requires a value for every parameter\nfrom the template, either specifying a new value or use previous value.\nFor convenience, this method will accept new parameter values and\ngenerates a dict of all parameters in a format that ChangeSet API\nwill accept\n\n:param parameter_overrides:\n:return:\n\"\"\"\n", "func_signal": "def merge_parameters(self, template_dict, parameter_overrides):\n", "code": "parameter_values = []\n\nif not isinstance(template_dict.get(\"Parameters\", None), dict):\n    return parameter_values\n\nfor key, value in template_dict[\"Parameters\"].items():\n\n    obj = {\n        \"ParameterKey\": key\n    }\n\n    if key in parameter_overrides:\n        obj[\"ParameterValue\"] = parameter_overrides[key]\n    else:\n        obj[\"UsePreviousValue\"] = True\n\n    parameter_values.append(obj)\n\nreturn parameter_values", "path": "aws-cli/awscli/customizations/cloudformation/deploy.py", "commit_date": "2019-04-16 00:00:00", "repo_name": "aws/aws-cli", "stars": 14741, "license": "other", "language": "python", "size": 206027}
{"docstring": "# Strip off the 'aws ' part and break it out into a list.\n", "func_signal": "def _parse_next_command(self, filename, original_cmd, args_list, parser):\n", "code": "errors = []\nparser._print_message = lambda message, file: errors.append(\n    message)\ntry:\n    parsed_args, remaining = parser.parse_known_args(args_list)\n    return parsed_args, remaining\nexcept SystemExit:\n    # Yes...we have to catch SystemExit. argparse raises this\n    # when you have an invalid command.\n    error_msg = [\n        'Invalid CLI command: %s\\n\\n' % original_cmd,\n    ]\n    if errors:\n        error_msg.extend(errors)\n    raise AssertionError(''.join(error_msg))", "path": "aws-cli/tests/functional/docs/test_examples.py", "commit_date": "2019-09-26 00:00:00", "repo_name": "aws/aws-cli", "stars": 14741, "license": "other", "language": "python", "size": 206027}
{"docstring": "\"\"\"Method to attach LifecyclePolicy to role specified by role_name\"\"\"\n", "func_signal": "def attach_policy_to_role(self, policy_arn, role_name):\n", "code": "return self.iam_client.attach_role_policy(\n    PolicyArn=policy_arn,\n    RoleName=role_name\n)", "path": "aws-cli/awscli/customizations/dlm/iam.py", "commit_date": "2018-07-10 00:00:00", "repo_name": "aws/aws-cli", "stars": 14741, "license": "other", "language": "python", "size": 206027}
{"docstring": "\"\"\"Asserts the scanned tags by checking the saved JSON index\"\"\"\n", "func_signal": "def assert_json_index(self, file_paths, reference_tag_dict):\n", "code": "json_index = self.file_creator.create_file('index.json', '')\nself.topic_tag_db = TopicTagDB(index_file=json_index)\nself.topic_tag_db.scan(file_paths)\nself.topic_tag_db.save_to_json_index()\nwith open(json_index, 'r') as f:\n    saved_index = json.loads(f.read())\n    self.assertEqual(saved_index, reference_tag_dict)", "path": "aws-cli/tests/unit/test_topictags.py", "commit_date": "2015-04-16 00:00:00", "repo_name": "aws/aws-cli", "stars": 14741, "license": "other", "language": "python", "size": 206027}
{"docstring": "# Get each line and filter out empty lines\n", "func_signal": "def assert_contains_exact_lines_in_order(self, actual, expected):\n", "code": "contents = actual.split(b'\\n')\ncontents = [line for line in contents if line and not line.isspace()]\n\nfor line in expected:\n    self.assertIn(line, contents)\n    beginning = contents.index(line)\n    contents = contents[beginning:]", "path": "aws-cli/tests/unit/bcdoc/test_docstringparser.py", "commit_date": "2020-09-15 00:00:00", "repo_name": "aws/aws-cli", "stars": 14741, "license": "other", "language": "python", "size": 206027}
{"docstring": "# Iterate over all rst doc examples0\n", "func_signal": "def iter_all_doc_examples():\n", "code": "_dname = os.path.dirname\nfor rootdir, _, filenames in os.walk(EXAMPLES_DIR):\n    for filename in filenames:\n        if not filename.endswith('.rst'):\n            continue\n        full_path = os.path.join(rootdir, filename)\n        yield full_path", "path": "aws-cli/tests/functional/docs/test_examples.py", "commit_date": "2019-09-26 00:00:00", "repo_name": "aws/aws-cli", "stars": 14741, "license": "other", "language": "python", "size": 206027}
{"docstring": "# Hooked up to building-command-table.rds\n# We don't need the modify-option-group operation.\n", "func_signal": "def _building_command_table(command_table, session, **kwargs):\n", "code": "del command_table['modify-option-group']\n# We're going to replace modify-option-group with two commands:\n# add-option-group and remove-option-group\nrds_model = session.get_service_model('rds')\nmodify_operation_model = rds_model.operation_model('ModifyOptionGroup')\ncommand_table['add-option-to-option-group'] = ServiceOperation(\n    parent_name='rds', name='add-option-to-option-group',\n    operation_caller=CLIOperationCaller(session),\n    session=session,\n    operation_model=modify_operation_model)\ncommand_table['remove-option-from-option-group'] = ServiceOperation(\n    parent_name='rds', name='remove-option-from-option-group',\n    session=session,\n    operation_model=modify_operation_model,\n    operation_caller=CLIOperationCaller(session))", "path": "aws-cli/awscli/customizations/rds.py", "commit_date": "2017-04-26 00:00:00", "repo_name": "aws/aws-cli", "stars": 14741, "license": "other", "language": "python", "size": 206027}
{"docstring": "\"\"\"Get a full cli transfer command.\n\nPerforms common transformations, e.g. adding --quiet\n\"\"\"\n", "func_signal": "def get_transfer_command(command, recursive, quiet):\n", "code": "cli_command = 'aws s3 ' + command\n\nif recursive:\n    cli_command += ' --recursive'\n\nif quiet:\n    cli_command += ' --quiet'\nelse:\n    print(cli_command)\n\nreturn cli_command", "path": "aws-cli/scripts/performance/benchmark_utils.py", "commit_date": "2016-09-21 00:00:00", "repo_name": "aws/aws-cli", "stars": 14741, "license": "other", "language": "python", "size": 206027}
{"docstring": "# CLIDriver can take up a lot of resources so we'll just create one\n# instance and use it for all the validation tests.\n", "func_signal": "def test_all_doc_examples():\n", "code": "driver = create_clidriver()\ncommand_validator = CommandValidator(driver)\n\nfor example_file in iter_all_doc_examples():\n    yield verify_has_only_ascii_chars, example_file\n    yield verify_is_valid_rst, example_file\n    yield verify_cli_commands_valid, example_file, command_validator", "path": "aws-cli/tests/functional/docs/test_examples.py", "commit_date": "2019-09-26 00:00:00", "repo_name": "aws/aws-cli", "stars": 14741, "license": "other", "language": "python", "size": 206027}
{"docstring": "\"\"\"Method to create role with a given role name\n    and assume_role_policy\n\"\"\"\n", "func_signal": "def create_role_with_trust_policy(self, role_name, assume_role_policy):\n", "code": "return self.iam_client.create_role(\n    RoleName=role_name,\n    AssumeRolePolicyDocument=json.dumps(assume_role_policy))", "path": "aws-cli/awscli/customizations/dlm/iam.py", "commit_date": "2018-07-10 00:00:00", "repo_name": "aws/aws-cli", "stars": 14741, "license": "other", "language": "python", "size": 206027}
{"docstring": "\"\"\"Method to verify if a particular policy exists\"\"\"\n", "func_signal": "def check_if_policy_exists(self, policy_arn):\n", "code": "try:\n    self.iam_client.get_policy(PolicyArn=policy_arn)\nexcept self.iam_client.exceptions.NoSuchEntityException:\n    return False\nreturn True", "path": "aws-cli/awscli/customizations/dlm/iam.py", "commit_date": "2018-07-10 00:00:00", "repo_name": "aws/aws-cli", "stars": 14741, "license": "other", "language": "python", "size": 206027}
{"docstring": "\"\"\" Perform ratio mask separation\n\n:param output_dict: dictionary of estimated spectrogram (key: instrument\n    name, value: estimated spectrogram of the instrument)\n:returns: dictionary of separated waveforms (key: instrument name,\n    value: estimated waveform of the instrument)\n\"\"\"\n\n", "func_signal": "def _build_manual_output_waveform(self, masked_stft):\n", "code": "output_waveform = {}\nfor instrument, stft_data in masked_stft.items():\n    output_waveform[instrument] = self._inverse_stft(stft_data)\nreturn output_waveform", "path": "spleeter/spleeter/model/__init__.py", "commit_date": "2020-12-08 00:00:00", "repo_name": "deezer/spleeter", "stars": 24616, "license": "mit", "language": "python", "size": 9615}
{"docstring": "\"\"\"\n    Lazy loading access method for internal prediction generator\n    returned by the predict method of a tensorflow estimator.\n\n    Returns:\n        Generator:\n            Generator of prediction.\n\"\"\"\n", "func_signal": "def _get_prediction_generator(self) -> Generator:\n", "code": "if self._prediction_generator is None:\n    estimator = create_estimator(self._params, self._MWF)\n\n    def get_dataset():\n        return tf.data.Dataset.from_generator(\n            self._data_generator,\n            output_types={\n                'waveform': tf.float32,\n                'audio_id': tf.string},\n            output_shapes={\n                'waveform': (None, 2),\n                'audio_id': ()})\n\n    self._prediction_generator = estimator.predict(\n        get_dataset,\n        yield_single_examples=False)\nreturn self._prediction_generator", "path": "spleeter/spleeter/separator.py", "commit_date": "2020-12-08 00:00:00", "repo_name": "deezer/spleeter", "stars": 24616, "license": "mit", "language": "python", "size": 9615}
{"docstring": "\"\"\" Builder interface for creating model instance that aims to perform\nmodel training. The output of such estimator will be a dictionary\nwith a key \"<instrument>_spectrogram\" per separated instrument,\nassociated to the estimated separated instrument magnitude spectrogram.\n\n:param labels: Model labels.\n:returns: An estimator for performing model training.\n\"\"\"\n", "func_signal": "def build_train_model(self, labels):\n", "code": "loss, metrics = self._build_loss(labels)\noptimizer = self._build_optimizer()\ntrain_operation = optimizer.minimize(\n        loss=loss,\n        global_step=tf.compat.v1.train.get_global_step())\nreturn tf.estimator.EstimatorSpec(\n    mode=tf.estimator.ModeKeys.TRAIN,\n    loss=loss,\n    train_op=train_operation,\n    eval_metric_ops=metrics,\n)", "path": "spleeter/spleeter/model/__init__.py", "commit_date": "2020-12-08 00:00:00", "repo_name": "deezer/spleeter", "stars": 24616, "license": "mit", "language": "python", "size": 9615}
{"docstring": "\"\"\" Extend mask, from reduced number of frequency bin to the number of\nfrequency bin in the STFT.\n\n:param mask: restricted mask\n:returns: extended mask\n:raise ValueError: If invalid mask_extension parameter is set.\n\"\"\"\n", "func_signal": "def _extend_mask(self, mask):\n", "code": "extension = self._params['mask_extension']\n# Extend with average\n# (dispatch according to energy in the processed band)\nif extension == \"average\":\n    extension_row = tf.reduce_mean(mask, axis=2, keepdims=True)\n# Extend with 0\n# (avoid extension artifacts but not conservative separation)\nelif extension == \"zeros\":\n    mask_shape = tf.shape(mask)\n    extension_row = tf.zeros((\n        mask_shape[0],\n        mask_shape[1],\n        1,\n        mask_shape[-1]))\nelse:\n    raise ValueError(f'Invalid mask_extension parameter {extension}')\nn_extra_row = self._frame_length // 2 + 1 - self._F\nextension = tf.tile(extension_row, [1, 1, n_extra_row, 1])\nreturn tf.concat([mask, extension], axis=2)", "path": "spleeter/spleeter/model/__init__.py", "commit_date": "2020-12-08 00:00:00", "repo_name": "deezer/spleeter", "stars": 24616, "license": "mit", "language": "python", "size": 9615}
{"docstring": "\"\"\"\nCompute masks from the output spectrograms of the model.\n:return:\n\"\"\"\n", "func_signal": "def _build_masks(self):\n", "code": "output_dict = self.model_outputs\nstft_feature = self.stft_feature\nseparation_exponent = self._params['separation_exponent']\noutput_sum = tf.reduce_sum(\n    [e ** separation_exponent for e in output_dict.values()],\n    axis=0\n) + self.EPSILON\nout = {}\nfor instrument in self._instruments:\n    output = output_dict[f'{instrument}_spectrogram']\n    # Compute mask with the model.\n    instrument_mask = (output ** separation_exponent\n                       + (self.EPSILON / len(output_dict))) / output_sum\n    # Extend mask;\n    instrument_mask = self._extend_mask(instrument_mask)\n    # Stack back mask.\n    old_shape = tf.shape(instrument_mask)\n    new_shape = tf.concat(\n        [[old_shape[0] * old_shape[1]], old_shape[2:]],\n        axis=0)\n    instrument_mask = tf.reshape(instrument_mask, new_shape)\n    # Remove padded part (for mask having the same size as STFT);\n\n    instrument_mask = instrument_mask[\n                      :tf.shape(stft_feature)[0], ...]\n    out[instrument] = instrument_mask\nself._masks = out", "path": "spleeter/spleeter/model/__init__.py", "commit_date": "2020-12-08 00:00:00", "repo_name": "deezer/spleeter", "stars": 24616, "license": "mit", "language": "python", "size": 9615}
{"docstring": "\"\"\" Builds an optimizer instance from internal parameter values.\n\nDefault to AdamOptimizer if not specified.\n\n:returns: Optimizer instance from internal configuration.\n\"\"\"\n", "func_signal": "def _build_optimizer(self):\n", "code": "name = self._params.get('optimizer')\nif name == self.ADADELTA:\n    return tf.compat.v1.train.AdadeltaOptimizer()\nrate = self._params['learning_rate']\nif name == self.SGD:\n    return tf.compat.v1.train.GradientDescentOptimizer(rate)\nreturn tf.compat.v1.train.AdamOptimizer(rate)", "path": "spleeter/spleeter/model/__init__.py", "commit_date": "2020-12-08 00:00:00", "repo_name": "deezer/spleeter", "stars": 24616, "license": "mit", "language": "python", "size": 9615}
{"docstring": "\"\"\" Inverse and reshape the given STFT\n\n:param stft_t: input STFT\n:returns: inverse STFT (waveform)\n\"\"\"\n", "func_signal": "def _inverse_stft(self, stft_t, time_crop=None):\n", "code": "inversed = inverse_stft(\n    tf.transpose(stft_t, perm=[2, 0, 1]),\n    self._frame_length,\n    self._frame_step,\n    window_fn=lambda frame_length, dtype: (\n        hann_window(frame_length, periodic=True, dtype=dtype))\n) * self.WINDOW_COMPENSATION_FACTOR\nreshaped = tf.transpose(inversed)\nif time_crop is None:\n    time_crop = tf.shape(self._features['waveform'])[0]\nreturn reshaped[self._frame_length:self._frame_length+time_crop, :]", "path": "spleeter/spleeter/model/__init__.py", "commit_date": "2020-12-08 00:00:00", "repo_name": "deezer/spleeter", "stars": 24616, "license": "mit", "language": "python", "size": 9615}
{"docstring": "\"\"\" Created a batch_sizexTxFxn_channels input tensor containing\nmix magnitude spectrogram, then an output dict from it according\nto the selected model in internal parameters.\n\n:returns: Build output dict.\n:raise ValueError: If required model_type is not supported.\n\"\"\"\n\n", "func_signal": "def _build_model_outputs(self):\n", "code": "input_tensor = self.spectrogram_feature\nmodel = self._params.get('model', None)\nif model is not None:\n    model_type = model.get('type', self.DEFAULT_MODEL)\nelse:\n    model_type = self.DEFAULT_MODEL\ntry:\n    apply_model = get_model_function(model_type)\nexcept ModuleNotFoundError:\n    raise ValueError(f'No model function {model_type} found')\nself._model_outputs = apply_model(\n    input_tensor,\n    self._instruments,\n    self._params['model']['params'])", "path": "spleeter/spleeter/model/__init__.py", "commit_date": "2020-12-08 00:00:00", "repo_name": "deezer/spleeter", "stars": 24616, "license": "mit", "language": "python", "size": 9615}
{"docstring": "\"\"\" Builder interface for creating model instance that aims to perform\nmodel evaluation. The output of such estimator will be a dictionary\nwith a key \"<instrument>_spectrogram\" per separated instrument,\nassociated to the estimated separated instrument magnitude spectrogram.\n\n:param labels: Model labels.\n:returns: An estimator for performing model evaluation.\n\"\"\"\n", "func_signal": "def build_evaluation_model(self, labels):\n", "code": "loss, metrics = self._build_loss(labels)\nreturn tf.estimator.EstimatorSpec(\n    tf.estimator.ModeKeys.EVAL,\n    loss=loss,\n    eval_metric_ops=metrics)", "path": "spleeter/spleeter/model/__init__.py", "commit_date": "2020-12-08 00:00:00", "repo_name": "deezer/spleeter", "stars": 24616, "license": "mit", "language": "python", "size": 9615}
{"docstring": "\"\"\"\n    Get tensorflow function of the model to be applied to the input tensor.\n    For instance \"unet.softmax_unet\" will return the softmax_unet function\n    in the \"unet.py\" submodule of the current module (spleeter.model).\n\n    Params:\n    - model_type: str\n    the relative module path to the model function.\n\n    Returns:\n    A tensorflow function to be applied to the input tensor to get the\n    multitrack output.\n\"\"\"\n", "func_signal": "def get_model_function(model_type):\n", "code": "relative_path_to_module = '.'.join(model_type.split('.')[:-1])\nmodel_name = model_type.split('.')[-1]\nmain_module = '.'.join((__name__, 'functions'))\npath_to_module = f'{main_module}.{relative_path_to_module}'\nmodule = importlib.import_module(path_to_module)\nmodel_function = getattr(module, model_name)\nreturn model_function", "path": "spleeter/spleeter/model/__init__.py", "commit_date": "2020-12-08 00:00:00", "repo_name": "deezer/spleeter", "stars": 24616, "license": "mit", "language": "python", "size": 9615}
{"docstring": "\"\"\" Compute STFT of waveform and slice the STFT in segment\n with the right length to feed the network.\n\"\"\"\n\n", "func_signal": "def _build_stft_feature(self):\n", "code": "stft_name = self.stft_name\nspec_name = self.spectrogram_name\n\nif stft_name not in self._features:\n    # pad input with a frame of zeros\n    waveform =  tf.concat([\n                    tf.zeros((self._frame_length, self._n_channels)),\n                    self._features['waveform']\n                    ],\n                    0\n                )\n    stft_feature = tf.transpose(\n        stft(\n            tf.transpose(waveform),\n            self._frame_length,\n            self._frame_step,\n            window_fn=lambda frame_length, dtype: (\n                hann_window(frame_length, periodic=True, dtype=dtype)),\n            pad_end=True),\n        perm=[1, 2, 0])\n    self._features[f'{self._mix_name}_stft'] = stft_feature\nif spec_name not in self._features:\n    self._features[spec_name] = tf.abs(\n        pad_and_partition(self._features[stft_name], self._T))[:, :, :self._F, :]", "path": "spleeter/spleeter/model/__init__.py", "commit_date": "2020-12-08 00:00:00", "repo_name": "deezer/spleeter", "stars": 24616, "license": "mit", "language": "python", "size": 9615}
{"docstring": "\"\"\" Default constructor. Depending on built model\nusage, the provided features should be different:\n\n* In train/eval mode:   features is a dictionary with a\n                        \"mix_spectrogram\" key, associated to the\n                        mix magnitude spectrogram.\n* In predict mode:      features is a dictionary with a \"waveform\"\n                        key, associated to the waveform of the sound\n                        to be separated.\n\n:param features: The input features for the estimator.\n:param params: Some hyperparameters as a dictionary.\n\"\"\"\n\n", "func_signal": "def __init__(self, features, params):\n", "code": "self._features = features\nself._params = params\n# Get instrument name.\nself._mix_name = params['mix_name']\nself._instruments = params['instrument_list']\n# Get STFT/signals parameters\nself._n_channels = params['n_channels']\nself._T = params['T']\nself._F = params['F']\nself._frame_length = params['frame_length']\nself._frame_step = params['frame_step']", "path": "spleeter/spleeter/model/__init__.py", "commit_date": "2020-12-08 00:00:00", "repo_name": "deezer/spleeter", "stars": 24616, "license": "mit", "language": "python", "size": 9615}
{"docstring": "\"\"\" Build output waveform from given output dict in order to be used in\nprediction context. Regarding of the configuration building method will\nbe using MWF.\n\n:returns: Built output waveform.\n\"\"\"\n\n", "func_signal": "def _build_output_waveform(self, masked_stft):\n", "code": "if self._params.get('MWF', False):\n    output_waveform = self._build_mwf_output_waveform()\nelse:\n    output_waveform = self._build_manual_output_waveform(masked_stft)\nreturn output_waveform", "path": "spleeter/spleeter/model/__init__.py", "commit_date": "2020-12-08 00:00:00", "repo_name": "deezer/spleeter", "stars": 24616, "license": "mit", "language": "python", "size": 9615}
{"docstring": "\"\"\" Perform separation with multichannel Wiener Filtering using Norbert.\nNote: multichannel Wiener Filtering is not coded in Tensorflow and thus\nmay be quite slow.\n\n:returns: dictionary of separated waveforms (key: instrument name,\n    value: estimated waveform of the instrument)\n\"\"\"\n", "func_signal": "def _build_mwf_output_waveform(self):\n", "code": "import norbert  # pylint: disable=import-error\noutput_dict = self.model_outputs\nx = self.stft_feature\nv = tf.stack(\n    [\n        pad_and_reshape(\n            output_dict[f'{instrument}_spectrogram'],\n            self._frame_length,\n            self._F)[:tf.shape(x)[0], ...]\n        for instrument in self._instruments\n    ],\n    axis=3)\ninput_args = [v, x]\nstft_function = tf.py_function(\n    lambda v, x: norbert.wiener(v.numpy(), x.numpy()),\n    input_args,\n    tf.complex64),\nreturn {\n    instrument: self._inverse_stft(stft_function[0][:, :, :, k])\n    for k, instrument in enumerate(self._instruments)\n}", "path": "spleeter/spleeter/model/__init__.py", "commit_date": "2020-12-08 00:00:00", "repo_name": "deezer/spleeter", "stars": 24616, "license": "mit", "language": "python", "size": 9615}
{"docstring": "\"\"\" Construct tensorflow loss and metrics\n\n:param output_dict: dictionary of network outputs (key: instrument\n    name, value: estimated spectrogram of the instrument)\n:param labels: dictionary of target outputs (key: instrument\n    name, value: ground truth spectrogram of the instrument)\n:returns: tensorflow (loss, metrics) tuple.\n\"\"\"\n", "func_signal": "def _build_loss(self, labels):\n", "code": "output_dict = self.model_outputs\nloss_type = self._params.get('loss_type', self.L1_MASK)\nif loss_type == self.L1_MASK:\n    losses = {\n        name: tf.reduce_mean(tf.abs(output - labels[name]))\n        for name, output in output_dict.items()\n    }\nelif loss_type == self.WEIGHTED_L1_MASK:\n    losses = {\n        name: tf.reduce_mean(\n            tf.reduce_mean(\n                labels[name],\n                axis=[1, 2, 3],\n                keep_dims=True) *\n            tf.abs(output - labels[name]))\n        for name, output in output_dict.items()\n    }\nelse:\n    raise ValueError(f\"Unkwnown loss type: {loss_type}\")\nloss = tf.reduce_sum(list(losses.values()))\n# Add metrics for monitoring each instrument.\nmetrics = {k: tf.compat.v1.metrics.mean(v) for k, v in losses.items()}\nmetrics['absolute_difference'] = tf.compat.v1.metrics.mean(loss)\nreturn loss, metrics", "path": "spleeter/spleeter/model/__init__.py", "commit_date": "2020-12-08 00:00:00", "repo_name": "deezer/spleeter", "stars": 24616, "license": "mit", "language": "python", "size": 9615}
{"docstring": "\"\"\"\n    generate fake evaluation dataset\n\"\"\"\n", "func_signal": "def generate_fake_eval_dataset(path):\n", "code": "aa = AudioAdapter.default()\nn_songs = 2\nfs = 44100\nduration = 3\nn_channels = 2\nrng = np.random.RandomState(seed=0)\nfor song in range(n_songs):\n    song_path = join(path, 'test', f'song{song}')\n    makedirs(song_path, exist_ok=True)\n    for instr in ['mixture', 'vocals', 'bass', 'drums', 'other']:\n        filename = join(song_path, f'{instr}.wav')\n        data = rng.rand(duration*fs, n_channels)-0.5\n        aa.save(filename, data, fs)", "path": "spleeter/tests/test_eval.py", "commit_date": "2020-12-11 00:00:00", "repo_name": "deezer/spleeter", "stars": 24616, "license": "mit", "language": "python", "size": 9615}
{"docstring": "\"\"\"\n\n:param features:\n:param labels: \n:param mode: Estimator mode.\n:param params: \n:param config: TF configuration (not used).\n:returns: Built EstimatorSpec.\n:raise ValueError: If estimator mode is not supported.\n\"\"\"\n", "func_signal": "def model_fn(features, labels, mode, params, config):\n", "code": "builder = EstimatorSpecBuilder(features, params)\nif mode == tf.estimator.ModeKeys.PREDICT:\n    return builder.build_predict_model()\nelif mode == tf.estimator.ModeKeys.EVAL:\n    return builder.build_evaluation_model(labels)\nelif mode == tf.estimator.ModeKeys.TRAIN:\n    return builder.build_train_model(labels)\nraise ValueError(f'Unknown mode {mode}')", "path": "spleeter/spleeter/model/__init__.py", "commit_date": "2020-12-08 00:00:00", "repo_name": "deezer/spleeter", "stars": 24616, "license": "mit", "language": "python", "size": 9615}
{"docstring": "\"\"\" Generation process. \"\"\"\n", "func_signal": "def __call__(self) -> Generator:\n", "code": "buffer = self._current_data\nwhile buffer:\n    yield buffer\n    buffer = self._current_data", "path": "spleeter/spleeter/separator.py", "commit_date": "2020-12-08 00:00:00", "repo_name": "deezer/spleeter", "stars": 24616, "license": "mit", "language": "python", "size": 9615}
{"docstring": "\"\"\"\n    Initialize tensorflow estimator that will perform separation\n\n    Params:\n    - params: a dictionary of parameters for building the model\n\n    Returns:\n        a tensorflow estimator\n\"\"\"\n# Load model.\n", "func_signal": "def create_estimator(params, MWF):\n", "code": "provider: ModelProvider = ModelProvider.default()\nparams['model_dir'] = provider.get(params['model_dir'])\nparams['MWF'] = MWF\n# Setup config\nsession_config = tf.compat.v1.ConfigProto()\nsession_config.gpu_options.per_process_gpu_memory_fraction = 0.7\nconfig = tf.estimator.RunConfig(session_config=session_config)\n# Setup estimator\nestimator = tf.estimator.Estimator(\n    model_fn=model_fn,\n    model_dir=params['model_dir'],\n    params=params,\n    config=config)\nreturn estimator", "path": "spleeter/spleeter/separator.py", "commit_date": "2020-12-08 00:00:00", "repo_name": "deezer/spleeter", "stars": 24616, "license": "mit", "language": "python", "size": 9615}
{"docstring": "\"\"\"\n    Default constructor, ensure FFMPEG binaries are available.\n\n    Raises:\n        SpleeterError:\n            If ffmpeg or ffprobe is not found.\n\"\"\"\n", "func_signal": "def __init__(_) -> None:\n", "code": "for binary in ('ffmpeg', 'ffprobe'):\n    if shutil.which(binary) is None:\n        raise SpleeterError('{} binary not found'.format(binary))", "path": "spleeter/spleeter/audio/ffmpeg.py", "commit_date": "2020-12-08 00:00:00", "repo_name": "deezer/spleeter", "stars": 24616, "license": "mit", "language": "python", "size": 9615}
{"docstring": "\"\"\"\nComplex Hootopy a function Cx[0, 1] to C\n\"\"\"\n", "func_signal": "def __init__(self, complex_homotopy, mobject, **kwargs):\n", "code": "def homotopy(x, y, z, t):\n    c = complex_homotopy(complex(x, y), t)\n    return (c.real, c.imag, z)\nHomotopy.__init__(self, homotopy, mobject, **kwargs)", "path": "manim/manimlib/animation/movement.py", "commit_date": "2019-03-21 00:00:00", "repo_name": "3b1b/manim", "stars": 56681, "license": "mit", "language": "python", "size": 75921}
{"docstring": "\"\"\"\nUpdates things like starting_mobject, and (for\nTransforms) target_mobject.  Note, since typically\n(always?) self.mobject will have its updating\nsuspended during the animation, this will do\nnothing to self.mobject.\n\"\"\"\n", "func_signal": "def update_mobjects(self, dt):\n", "code": "for mob in self.get_all_mobjects_to_update():\n    mob.update(dt)", "path": "manim/manimlib/animation/animation.py", "commit_date": "2020-08-30 00:00:00", "repo_name": "3b1b/manim", "stars": 56681, "license": "mit", "language": "python", "size": 75921}
{"docstring": "# This could conceivably be smarter about handling existing differing extensions\n", "func_signal": "def add_extension_if_not_present(file_name, extension):\n", "code": "if(file_name[-len(extension):] != extension):\n    return file_name + extension\nelse:\n    return file_name", "path": "manim/manimlib/utils/file_ops.py", "commit_date": "2019-06-04 00:00:00", "repo_name": "3b1b/manim", "stars": 56681, "license": "mit", "language": "python", "size": 75921}
{"docstring": "\"\"\"\nMore functional version of always, where instead\nof taking in args, it takes in functions which ouput\nthe relevant arguments.\n\"\"\"\n", "func_signal": "def f_always(method, *arg_generators, **kwargs):\n", "code": "assert_is_mobject_method(method)\nmobject = method.__self__\nfunc = method.__func__\n\ndef updater(mob):\n    args = [\n        arg_generator()\n        for arg_generator in arg_generators\n    ]\n    func(mob, *args, **kwargs)\n\nmobject.add_updater(updater)\nreturn mobject", "path": "manim/manimlib/mobject/mobject_update_utils.py", "commit_date": "2020-02-19 00:00:00", "repo_name": "3b1b/manim", "stars": 56681, "license": "mit", "language": "python", "size": 75921}
{"docstring": "# Zero first and second derivatives at t=0 and t=1.\n# Equivalent to bezier([0, 0, 0, 1, 1, 1])\n", "func_signal": "def smooth(t):\n", "code": "s = 1 - t\nreturn (t**3) * (10 * s * s + 5 * s * t + t * t)", "path": "manim/manimlib/utils/rate_functions.py", "commit_date": "2020-07-23 00:00:00", "repo_name": "3b1b/manim", "stars": 56681, "license": "mit", "language": "python", "size": 75921}
{"docstring": "\"\"\"\nHomotopy is a function from\n(x, y, z, t) to (x', y', z')\n\"\"\"\n", "func_signal": "def __init__(self, homotopy, mobject, **kwargs):\n", "code": "self.homotopy = homotopy\nsuper().__init__(mobject, **kwargs)", "path": "manim/manimlib/animation/movement.py", "commit_date": "2019-03-21 00:00:00", "repo_name": "3b1b/manim", "stars": 56681, "license": "mit", "language": "python", "size": 75921}
{"docstring": "\"\"\"\nCreates a list of triplets of the form\n(anim, start_time, end_time)\n\"\"\"\n", "func_signal": "def build_animations_with_timings(self):\n", "code": "self.anims_with_timings = []\ncurr_time = 0\nfor anim in self.animations:\n    start_time = curr_time\n    end_time = start_time + anim.get_run_time()\n    self.anims_with_timings.append(\n        (anim, start_time, end_time)\n    )\n    # Start time of next animation is based on\n    # the lag_ratio\n    curr_time = interpolate(\n        start_time, end_time, self.lag_ratio\n    )", "path": "manim/manimlib/animation/composition.py", "commit_date": "2020-02-19 00:00:00", "repo_name": "3b1b/manim", "stars": 56681, "license": "mit", "language": "python", "size": 75921}
{"docstring": "# TODO, make this more understanable, and/or combine\n# its functionality with AnimationGroup's method\n# build_animations_with_timings\n", "func_signal": "def get_sub_alpha(self, alpha, index, num_submobjects):\n", "code": "lag_ratio = self.lag_ratio\nfull_length = (num_submobjects - 1) * lag_ratio + 1\nvalue = alpha * full_length\nlower = index * lag_ratio\nreturn clip((value - lower), 0, 1)", "path": "manim/manimlib/animation/animation.py", "commit_date": "2020-08-30 00:00:00", "repo_name": "3b1b/manim", "stars": 56681, "license": "mit", "language": "python", "size": 75921}
{"docstring": "\"\"\"\nTakes in a list, and returns a list of tuples, (batch, prop)\nsuch that all items in a batch have the same output when\nput into property_func, and such that chaining all these\nbatches together would give the original list (i.e. order is\npreserved)\n\"\"\"\n", "func_signal": "def batch_by_property(items, property_func):\n", "code": "batch_prop_pairs = []\ncurr_batch = []\ncurr_prop = None\nfor item in items:\n    prop = property_func(item)\n    if prop != curr_prop:\n        # Add current batch\n        if len(curr_batch) > 0:\n            batch_prop_pairs.append((curr_batch, curr_prop))\n        # Redefine curr\n        curr_prop = prop\n        curr_batch = [item]\n    else:\n        curr_batch.append(item)\nif len(curr_batch) > 0:\n    batch_prop_pairs.append((curr_batch, curr_prop))\nreturn batch_prop_pairs", "path": "manim/manimlib/utils/iterables.py", "commit_date": "2020-06-15 00:00:00", "repo_name": "3b1b/manim", "stars": 56681, "license": "mit", "language": "python", "size": 75921}
{"docstring": "# The surrounding scene typically handles\n# updating of self.mobject.  Besides, in\n# most cases its updating is suspended anyway\n", "func_signal": "def get_all_mobjects_to_update(self):\n", "code": "return list(filter(\n    lambda m: m is not self.mobject,\n    self.get_all_mobjects()\n))", "path": "manim/manimlib/animation/animation.py", "commit_date": "2020-08-30 00:00:00", "repo_name": "3b1b/manim", "stars": 56681, "license": "mit", "language": "python", "size": 75921}
{"docstring": "# TODO, should this really exist in SampleSpaceScene\n", "func_signal": "def add_title(self, title=\"Sample space\", buff=MED_SMALL_BUFF):\n", "code": "title_mob = TextMobject(title)\nif title_mob.get_width() > self.get_width():\n    title_mob.set_width(self.get_width())\ntitle_mob.next_to(self, UP, buff=buff)\nself.title = title_mob\nself.add(title_mob)", "path": "manim/manimlib/mobject/probability.py", "commit_date": "2020-02-05 00:00:00", "repo_name": "3b1b/manim", "stars": 56681, "license": "mit", "language": "python", "size": 75921}
{"docstring": "\"\"\"\nUsed instead of list(set(l)) to maintain order\nKeeps the last occurance of each element\n\"\"\"\n", "func_signal": "def remove_list_redundancies(l):\n", "code": "reversed_result = []\nused = set()\nfor x in reversed(l):\n    if x not in used:\n        reversed_result.append(x)\n        used.add(x)\nreversed_result.reverse()\nreturn reversed_result", "path": "manim/manimlib/utils/iterables.py", "commit_date": "2020-06-15 00:00:00", "repo_name": "3b1b/manim", "stars": 56681, "license": "mit", "language": "python", "size": 75921}
{"docstring": "# This is called right as an animation is being\n# played.  As much initialization as possible,\n# especially any mobject copying, should live in\n# this method\n", "func_signal": "def begin(self):\n", "code": "self.mobject.prepare_for_animation()\nself.starting_mobject = self.create_starting_mobject()\nif self.suspend_mobject_updating:\n    # All calls to self.mobject's internal updaters\n    # during the animation, either from this Animation\n    # or from the surrounding scene, should do nothing.\n    # It is, however, okay and desirable to call\n    # the internal updaters of self.starting_mobject,\n    # or any others among self.get_all_mobjects()\n    self.mobject.suspend_updating()\nself.families = list(self.get_all_families_zipped())\nself.interpolate(0)", "path": "manim/manimlib/animation/animation.py", "commit_date": "2020-08-30 00:00:00", "repo_name": "3b1b/manim", "stars": 56681, "license": "mit", "language": "python", "size": 75921}
{"docstring": "\"\"\"For debugging purposes\"\"\"\n", "func_signal": "def print_family(mobject, n_tabs=0):\n", "code": "print(\"\\t\" * n_tabs, mobject, id(mobject))\nfor submob in mobject.submobjects:\n    print_family(submob, n_tabs + 1)", "path": "manim/manimlib/utils/debug.py", "commit_date": "2020-02-27 00:00:00", "repo_name": "3b1b/manim", "stars": 56681, "license": "mit", "language": "python", "size": 75921}
{"docstring": "# Not actual time, but something which passes at\n# an altered rate to make the implementation below\n# cleaner\n", "func_signal": "def update_boundary_copies(self, dt):\n", "code": "time = self.total_time * self.cycle_rate\ngrowing, fading = self.boundary_copies\ncolors = self.colors\nmsw = self.max_stroke_width\nvmobject = self.vmobject\n\nindex = int(time % len(colors))\nalpha = time % 1\ndraw_alpha = self.draw_rate_func(alpha)\nfade_alpha = self.fade_rate_func(alpha)\n\nif self.back_and_forth and int(time) % 2 == 1:\n    bounds = (1 - draw_alpha, 1)\nelse:\n    bounds = (0, draw_alpha)\nself.full_family_become_partial(growing, vmobject, *bounds)\ngrowing.set_stroke(colors[index], width=msw)\n\nif time >= 1:\n    self.full_family_become_partial(fading, vmobject, 0, 1)\n    fading.set_stroke(\n        color=colors[index - 1],\n        width=(1 - fade_alpha) * msw\n    )\n\nself.total_time += dt", "path": "manim/manimlib/mobject/changing.py", "commit_date": "2019-07-03 00:00:00", "repo_name": "3b1b/manim", "stars": 56681, "license": "mit", "language": "python", "size": 75921}
{"docstring": "\"\"\"\nAdd an updater to the animation's mobject which applies\nthe interpolation and update functions of the animation\n\nIf cycle is True, this repeats over and over.  Otherwise,\nthe updater will be popped uplon completion\n\"\"\"\n", "func_signal": "def turn_animation_into_updater(animation, cycle=False, **kwargs):\n", "code": "mobject = animation.mobject\nanimation.update_config(**kwargs)\nanimation.suspend_mobject_updating = False\nanimation.begin()\nanimation.total_time = 0\n\ndef update(m, dt):\n    run_time = animation.get_run_time()\n    time_ratio = animation.total_time / run_time\n    if cycle:\n        alpha = time_ratio % 1\n    else:\n        alpha = clip(time_ratio, 0, 1)\n        if alpha >= 1:\n            animation.finish()\n            m.remove_updater(update)\n            return\n    animation.interpolate(alpha)\n    animation.update_mobjects(dt)\n    animation.total_time += dt\n\nmobject.add_updater(update)\nreturn mobject", "path": "manim/manimlib/mobject/mobject_update_utils.py", "commit_date": "2020-02-19 00:00:00", "repo_name": "3b1b/manim", "stars": 56681, "license": "mit", "language": "python", "size": 75921}
{"docstring": "# Obviously this would optimally be removed at\n# some point.\n", "func_signal": "def yell_about_depricated_configuration(self, **kwargs):\n", "code": "for attr in [\"tracked_mobject\", \"position_update_func\"]:\n    if attr in kwargs:\n        warnings.warn(\"\"\"\n            Don't use {} for ChangingDecimal,\n            that functionality has been depricated\n            and you should use a mobject updater\n            instead\n        \"\"\".format(attr)\n    )", "path": "manim/manimlib/animation/numbers.py", "commit_date": "2020-03-13 00:00:00", "repo_name": "3b1b/manim", "stars": 56681, "license": "mit", "language": "python", "size": 75921}
{"docstring": "\"\"\"\nMatrix can either either include numbres, tex_strings,\nor mobjects\n\"\"\"\n", "func_signal": "def __init__(self, matrix, **kwargs):\n", "code": "VMobject.__init__(self, **kwargs)\nmatrix = self.matrix = np.array(matrix, ndmin=2)\nmob_matrix = self.matrix_to_mob_matrix(matrix)\nself.organize_mob_matrix(mob_matrix)\nself.elements = VGroup(*mob_matrix.flatten())\nself.add(self.elements)\nself.add_brackets()\nself.center()\nself.mob_matrix = mob_matrix\nif self.add_background_rectangles_to_entries:\n    for mob in self.elements:\n        mob.add_background_rectangle()\nif self.include_background_rectangle:\n    self.add_background_rectangle()", "path": "manim/manimlib/mobject/matrix.py", "commit_date": "2020-08-30 00:00:00", "repo_name": "3b1b/manim", "stars": 56681, "license": "mit", "language": "python", "size": 75921}
{"docstring": "# Note, if the run_time of AnimationGroup has been\n# set to something other than its default, these\n# times might not correspond to actual times,\n# e.g. of the surrounding scene.  Instead they'd\n# be a rescaled version.  But that's okay!\n", "func_signal": "def interpolate(self, alpha):\n", "code": "time = alpha * self.max_end_time\nfor anim, start_time, end_time in self.anims_with_timings:\n    anim_time = end_time - start_time\n    if anim_time == 0:\n        sub_alpha = 0\n    else:\n        sub_alpha = clip(\n            (time - start_time) / anim_time,\n            0, 1\n        )\n    anim.interpolate(sub_alpha)", "path": "manim/manimlib/animation/composition.py", "commit_date": "2020-02-19 00:00:00", "repo_name": "3b1b/manim", "stars": 56681, "license": "mit", "language": "python", "size": 75921}
{"docstring": "\"\"\"\nConverts a dvi, which potentially has multiple slides, into a\ndirectory full of enumerated pngs corresponding with these slides.\nReturns a list of PIL Image objects for these images sorted as they\nwhere in the dvi\n\"\"\"\n", "func_signal": "def dvi_to_svg(dvi_file, regen_if_exists=False):\n", "code": "result = dvi_file.replace(\".dvi\" if not TEX_USE_CTEX else \".xdv\", \".svg\")\nif not os.path.exists(result):\n    commands = [\n        \"dvisvgm\",\n        \"\\\"{}\\\"\".format(dvi_file),\n        \"-n\",\n        \"-v\",\n        \"0\",\n        \"-o\",\n        \"\\\"{}\\\"\".format(result),\n        \">\",\n        os.devnull\n    ]\n    os.system(\" \".join(commands))\nreturn result", "path": "manim/manimlib/utils/tex_file_writing.py", "commit_date": "2019-06-17 00:00:00", "repo_name": "3b1b/manim", "stars": 56681, "license": "mit", "language": "python", "size": 75921}
{"docstring": "\"\"\"Displays a VMobject in the cairo context\n\nParameters\n----------\nvmobject : VMobject\n    The Vectorized Mobject to display\nctx : cairo.Context\n    The cairo context to use.\n\nReturns\n-------\nCamera\n    The camera object\n\"\"\"\n", "func_signal": "def display_vectorized(self, vmobject, ctx):\n", "code": "self.set_cairo_context_path(ctx, vmobject)\nself.apply_stroke(ctx, vmobject, background=True)\nself.apply_fill(ctx, vmobject)\nself.apply_stroke(ctx, vmobject)\nreturn self", "path": "manim/manim/camera/camera.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "ManimCommunity/manim", "stars": 17709, "license": "mit", "language": "python", "size": 40568}
{"docstring": "# Create a new template\n", "func_signal": "def construct(self):\n", "code": "myTemplate = TexTemplate()\n\n# Add packages to the template\nmyTemplate.add_to_preamble(r\"\\usepackage{esvect}\")\n\n# Set the compiler and output format (default: latex and .dvi)\n# possible tex compilers: \"latex\", \"pdflatex\", \"xelatex\", \"lualatex\", \"luatex\"\n# possible output formats: \".dvi\",  \".pdf\", and \".xdv\"\nmyTemplate.tex_compiler = \"pdflatex\"\nmyTemplate.output_format = \".pdf\"\n\n# To use this template in a Tex() or MathTex() object\n# use the keyword argument tex_template\ntext = MathTex(r\"\\vv{vb}\", tex_template=myTemplate)\nself.play(Write(text))\nself.wait(1)", "path": "manim/example_scenes/customtex.py", "commit_date": "2020-10-12 00:00:00", "repo_name": "ManimCommunity/manim", "stars": 17709, "license": "mit", "language": "python", "size": 40568}
{"docstring": "# This is to address a strange bug where deepcopying\n# will result in a segfault, which is somehow related\n# to the aggdraw library\n", "func_signal": "def __deepcopy__(self, memo):\n", "code": "self.canvas = None\nreturn copy.copy(self)", "path": "manim/manim/camera/camera.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "ManimCommunity/manim", "stars": 17709, "license": "mit", "language": "python", "size": 40568}
{"docstring": "\"\"\"Returns thickened coordinates for a passed array of pixel coords and\na thickness to thicken by.\n\nParameters\n----------\npixel_coords : np.array\n    Pixel coordinates\nthickness : int, float\n    Thickness\n\nReturns\n-------\nnp.array\n    Array of thickened pixel coords.\n\"\"\"\n", "func_signal": "def thickened_coordinates(self, pixel_coords, thickness):\n", "code": "nudges = self.get_thickening_nudges(thickness)\npixel_coords = np.array([pixel_coords + nudge for nudge in nudges])\nsize = pixel_coords.size\nreturn pixel_coords.reshape((size // 2, 2))", "path": "manim/manim/camera/camera.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "ManimCommunity/manim", "stars": 17709, "license": "mit", "language": "python", "size": 40568}
{"docstring": "\"\"\"Displays a PMobject by modifying the Pixel array suitably..\nTODO: Write a description for the rgbas argument.\nParameters\n----------\npmobject : PMobject\n    Point Cloud Mobject\npoints : list\n    The points to display in the point cloud mobject\nrgbas : np.array\n\nthickness : int, float\n    The thickness of each point of the PMobject\npixel_array : np.array\n    The pixel array to modify.\n\"\"\"\n", "func_signal": "def display_point_cloud(self, pmobject, points, rgbas, thickness, pixel_array):\n", "code": "if len(points) == 0:\n    return\npixel_coords = self.points_to_pixel_coords(pmobject, points)\npixel_coords = self.thickened_coordinates(pixel_coords, thickness)\nrgba_len = pixel_array.shape[2]\n\nrgbas = (self.rgb_max_val * rgbas).astype(self.pixel_array_dtype)\ntarget_len = len(pixel_coords)\nfactor = target_len // len(rgbas)\nrgbas = np.array([rgbas] * factor).reshape((target_len, rgba_len))\n\non_screen_indices = self.on_screen_pixels(pixel_coords)\npixel_coords = pixel_coords[on_screen_indices]\nrgbas = rgbas[on_screen_indices]\n\nph = self.pixel_height\npw = self.pixel_width\n\nflattener = np.array([1, pw], dtype=\"int\")\nflattener = flattener.reshape((2, 1))\nindices = np.dot(pixel_coords, flattener)[:, 0]\nindices = indices.astype(\"int\")\n\nnew_pa = pixel_array.reshape((ph * pw, rgba_len))\nnew_pa[indices] = rgbas\npixel_array[:, :] = new_pa.reshape((ph, pw, rgba_len))", "path": "manim/manim/camera/camera.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "ManimCommunity/manim", "stars": 17709, "license": "mit", "language": "python", "size": 40568}
{"docstring": "\"\"\"Checks whether the passed mobject is in\nframe or not.\n\nParameters\n----------\nmobject : Mobject\n    The mobject for which the checking needs to be done.\n\nReturns\n-------\nbool\n    True if in frame, False otherwise.\n\"\"\"\n", "func_signal": "def is_in_frame(self, mobject):\n", "code": "fc = self.frame_center\nfh = self.frame_height\nfw = self.frame_width\nreturn not reduce(\n    op.or_,\n    [\n        mobject.get_right()[0] < fc[0] - fw,\n        mobject.get_bottom()[1] > fc[1] + fh,\n        mobject.get_left()[0] > fc[0] + fw,\n        mobject.get_top()[1] < fc[1] - fh,\n    ],\n)", "path": "manim/manim/camera/camera.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "ManimCommunity/manim", "stars": 17709, "license": "mit", "language": "python", "size": 40568}
{"docstring": "\"\"\"Returns the cairo context for a pixel array after\ncaching it to self.pixel_array_to_cairo_context\nIf that array has already been cached, it returns the\ncached version instead.\n\nParameters\n----------\npixel_array : np.array\n    The Pixel array to get the cairo context of.\n\nReturns\n-------\ncairo.Context\n    The cairo context of the pixel array.\n\"\"\"\n", "func_signal": "def get_cairo_context(self, pixel_array):\n", "code": "cached_ctx = self.get_cached_cairo_context(pixel_array)\nif cached_ctx:\n    return cached_ctx\npw = self.pixel_width\nph = self.pixel_height\nfw = self.frame_width\nfh = self.frame_height\nfc = self.frame_center\nsurface = cairo.ImageSurface.create_for_data(\n    pixel_array, cairo.FORMAT_ARGB32, pw, ph\n)\nctx = cairo.Context(surface)\nctx.scale(pw, ph)\nctx.set_matrix(\n    cairo.Matrix(\n        fdiv(pw, fw),\n        0,\n        0,\n        -fdiv(ph, fh),\n        (pw / 2) - fc[0] * fdiv(pw, fw),\n        (ph / 2) + fc[1] * fdiv(ph, fh),\n    )\n)\nself.cache_cairo_context(pixel_array, ctx)\nreturn ctx", "path": "manim/manim/camera/camera.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "ManimCommunity/manim", "stars": 17709, "license": "mit", "language": "python", "size": 40568}
{"docstring": "\"\"\"Return the type of mobject, if it is a type that can be rendered.\n\nIf `mobject` is an instance of a class that inherits from a class that\ncan be rendered, return the super class.  For example, an instance of a\nSquare is also an instance of VMobject, and these can be rendered.\nTherefore, `type_or_raise(Square())` returns True.\n\nParameters\n----------\nmobject : :class:`~.Mobject`\n    The object to take the type of.\n\nNotes\n-----\nFor a list of classes that can currently be rendered, see :meth:`display_funcs`.\n\nReturns\n-------\nType[:class:`~.Mobject`]\n    The type of mobjects, if it can be rendered.\n\nRaises\n------\n:exc:`TypeError`\n    When mobject is not an instance of a class that can be rendered.\n\"\"\"\n", "func_signal": "def type_or_raise(self, mobject):\n", "code": "self.display_funcs = {\n    VMobject: self.display_multiple_vectorized_mobjects,\n    PMobject: self.display_multiple_point_cloud_mobjects,\n    AbstractImageMobject: self.display_multiple_image_mobjects,\n    Mobject: lambda batch, pa: batch,  # Do nothing\n}\n# We have to check each type in turn because we are dealing with\n# super classes.  For example, if square = Square(), then\n# type(square) != VMobject, but isinstance(square, VMobject) == True.\nfor _type in self.display_funcs:\n    if isinstance(mobject, _type):\n        return _type\nelse:\n    raise TypeError(f\"Displaying an object of class {_type} is not supported\")", "path": "manim/manim/camera/camera.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "ManimCommunity/manim", "stars": 17709, "license": "mit", "language": "python", "size": 40568}
{"docstring": "\"\"\"Gets the background array that has the passed file_name.\n\nParameters\n----------\nfile_name : str\n    The file_name of the background image.\n\nReturns\n-------\nnp.ndarray\n    The pixel array of the file whose file name is `file_name`\n\"\"\"\n", "func_signal": "def get_background_array(self, file_name):\n", "code": "if file_name in self.file_name_to_pixel_array_map:\n    return self.file_name_to_pixel_array_map[file_name]\nfull_path = get_full_raster_image_path(file_name)\nimage = Image.open(full_path)\nback_array = np.array(image)\n\npixel_array = self.pixel_array\nif not np.all(pixel_array.shape == back_array.shape):\n    back_array = self.resize_background_array_to_match(back_array, pixel_array)\n\nself.file_name_to_pixel_array_map[file_name] = back_array\nreturn back_array", "path": "manim/manim/camera/camera.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "ManimCommunity/manim", "stars": 17709, "license": "mit", "language": "python", "size": 40568}
{"docstring": "\"\"\"Initialize the background.\nIf self.background_image is the path of an image\nthe image is set as background; else, the default\nbackground color fills the background.\n\"\"\"\n", "func_signal": "def init_background(self):\n", "code": "height = self.pixel_height\nwidth = self.pixel_width\nif self.background_image is not None:\n    path = get_full_raster_image_path(self.background_image)\n    image = Image.open(path).convert(self.image_mode)\n    # TODO, how to gracefully handle backgrounds\n    # with different sizes?\n    self.background = np.array(image)[:height, :width]\n    self.background = self.background.astype(self.pixel_array_dtype)\nelse:\n    background_rgba = color_to_int_rgba(\n        self.background_color, self.background_opacity\n    )\n    self.background = np.zeros(\n        (height, width, self.n_channels), dtype=self.pixel_array_dtype\n    )\n    self.background[:, :] = background_rgba", "path": "manim/manim/camera/camera.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "ManimCommunity/manim", "stars": 17709, "license": "mit", "language": "python", "size": 40568}
{"docstring": "\"\"\"Displays multiple PMobjects by modifying the passed pixel array.\n\nParameters\n----------\npmobjects : list\n    List of PMobjects\npixel_array : np.array\n    The pixel array to modify.\n\"\"\"\n", "func_signal": "def display_multiple_point_cloud_mobjects(self, pmobjects, pixel_array):\n", "code": "for pmobject in pmobjects:\n    self.display_point_cloud(\n        pmobject,\n        pmobject.points,\n        pmobject.rgbas,\n        self.adjusted_thickness(pmobject.stroke_width),\n        pixel_array,\n    )", "path": "manim/manim/camera/camera.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "ManimCommunity/manim", "stars": 17709, "license": "mit", "language": "python", "size": 40568}
{"docstring": "\"\"\"Displays multiple vmobjects that have the same color as the background.\n\nParameters\n----------\ncvmobjects : list\n    List of Colored VMobjects\npixel_array : np.array\n    The pixel array.\n\nReturns\n-------\nCamera\n    The camera object.\n\"\"\"\n", "func_signal": "def display_multiple_background_colored_vmobjects(self, cvmobjects, pixel_array):\n", "code": "displayer = self.get_background_colored_vmobject_displayer()\ncvmobject_pixel_array = displayer.display(*cvmobjects)\nself.overlay_rgba_array(pixel_array, cvmobject_pixel_array)\nreturn self", "path": "manim/manim/camera/camera.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "ManimCommunity/manim", "stars": 17709, "license": "mit", "language": "python", "size": 40568}
{"docstring": "\"\"\"Displays an ImageMobject by changing the pixel_array suitably.\n\nParameters\n----------\nimage_mobject : ImageMobject\n    The imageMobject to display\npixel_array : np.ndarray\n    The Pixel array to put the imagemobject in.\n\"\"\"\n", "func_signal": "def display_image_mobject(self, image_mobject, pixel_array):\n", "code": "corner_coords = self.points_to_pixel_coords(image_mobject, image_mobject.points)\nul_coords, ur_coords, dl_coords = corner_coords\nright_vect = ur_coords - ul_coords\ndown_vect = dl_coords - ul_coords\ncenter_coords = ul_coords + (right_vect + down_vect) / 2\n\nsub_image = Image.fromarray(image_mobject.get_pixel_array(), mode=\"RGBA\")\n\n# Reshape\npixel_width = max(int(pdist([ul_coords, ur_coords])), 1)\npixel_height = max(int(pdist([ul_coords, dl_coords])), 1)\nsub_image = sub_image.resize(\n    (pixel_width, pixel_height), resample=Image.BICUBIC\n)\n\n# Rotate\nangle = angle_of_vector(right_vect)\nadjusted_angle = -int(360 * angle / TAU)\nif adjusted_angle != 0:\n    sub_image = sub_image.rotate(\n        adjusted_angle, resample=Image.BICUBIC, expand=1\n    )\n\n# TODO, there is no accounting for a shear...\n\n# Paste into an image as large as the camear's pixel array\nfull_image = Image.fromarray(\n    np.zeros((self.pixel_height, self.pixel_width)), mode=\"RGBA\"\n)\nnew_ul_coords = center_coords - np.array(sub_image.size) / 2\nnew_ul_coords = new_ul_coords.astype(int)\nfull_image.paste(\n    sub_image,\n    box=(\n        new_ul_coords[0],\n        new_ul_coords[1],\n        new_ul_coords[0] + sub_image.size[0],\n        new_ul_coords[1] + sub_image.size[1],\n    ),\n)\n# Paint on top of existing pixel array\nself.overlay_PIL_image(pixel_array, full_image)", "path": "manim/manim/camera/camera.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "ManimCommunity/manim", "stars": 17709, "license": "mit", "language": "python", "size": 40568}
{"docstring": "\"\"\"This method resets the height and width\nof a single pixel to the passed new_heigh and new_width.\n\nParameters\n----------\nnew_height : int, float\n    The new height of the entire scene in pixels\nnew_width : int, float\n    The new width of the entire scene in pixels\n\"\"\"\n", "func_signal": "def reset_pixel_shape(self, new_height, new_width):\n", "code": "self.pixel_width = new_width\nself.pixel_height = new_height\nself.init_background()\nself.resize_frame_shape()\nself.reset()", "path": "manim/manim/camera/camera.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "ManimCommunity/manim", "stars": 17709, "license": "mit", "language": "python", "size": 40568}
{"docstring": "\"\"\"Converts a pixel array from values that have floats in then\nto proper RGB values.\n\nParameters\n----------\npixel_array : np.array, list, tuple\n    Pixel array to convert.\nconvert_from_floats : bool, optional\n    Whether or not to convert float values to ints, by default False\n\nReturns\n-------\nnp.array\n    The new, converted pixel array.\n\"\"\"\n", "func_signal": "def convert_pixel_array(self, pixel_array, convert_from_floats=False):\n", "code": "retval = np.array(pixel_array)\nif convert_from_floats:\n    retval = np.apply_along_axis(\n        lambda f: (f * self.rgb_max_val).astype(self.pixel_array_dtype),\n        2,\n        retval,\n    )\nreturn retval", "path": "manim/manim/camera/camera.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "ManimCommunity/manim", "stars": 17709, "license": "mit", "language": "python", "size": 40568}
{"docstring": "\"\"\"Displays multiple VMobjects in the cairo context, as long as they don't have\nbackground colors.\n\nParameters\n----------\nvmobjects : list\n    list of the VMobjects\npixel_array : np.ndarray\n    The Pixel array to add the VMobjects to.\n\"\"\"\n", "func_signal": "def display_multiple_non_background_colored_vmobjects(self, vmobjects, pixel_array):\n", "code": "ctx = self.get_cairo_context(pixel_array)\nfor vmobject in vmobjects:\n    self.display_vectorized(vmobject, ctx)", "path": "manim/manim/camera/camera.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "ManimCommunity/manim", "stars": 17709, "license": "mit", "language": "python", "size": 40568}
{"docstring": "\"\"\"Resizes the background array to match the passed pixel array.\n\nParameters\n----------\nbackground_array : np.array\n    The prospective pixel array.\npixel_array : np.array\n    The pixel array whose width and height should be matched.\n\nReturns\n-------\nnp.array\n    The resized background array.\n\"\"\"\n", "func_signal": "def resize_background_array_to_match(self, background_array, pixel_array):\n", "code": "height, width = pixel_array.shape[:2]\nmode = \"RGBA\" if pixel_array.shape[2] == 4 else \"RGB\"\nreturn self.resize_background_array(background_array, width, height, mode)", "path": "manim/manim/camera/camera.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "ManimCommunity/manim", "stars": 17709, "license": "mit", "language": "python", "size": 40568}
{"docstring": "\"\"\"Fills the cairo context\n\nParameters\n----------\nctx : cairo.Context\n    The cairo context\nvmobject : VMobject\n    The VMobject\n\nReturns\n-------\nCamera\n    The camera object.\n\"\"\"\n", "func_signal": "def apply_fill(self, ctx, vmobject):\n", "code": "self.set_cairo_context_color(ctx, self.get_fill_rgbas(vmobject), vmobject)\nctx.fill_preserve()\nreturn self", "path": "manim/manim/camera/camera.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "ManimCommunity/manim", "stars": 17709, "license": "mit", "language": "python", "size": 40568}
{"docstring": "\"\"\"Displays multiple VMobjects in the pixel_array\n\nParameters\n----------\nvmobjects : list\n    list of VMobjects to display\npixel_array : np.array\n    The pixel array\n\"\"\"\n", "func_signal": "def display_multiple_vectorized_mobjects(self, vmobjects, pixel_array):\n", "code": "if len(vmobjects) == 0:\n    return\nbatch_file_pairs = it.groupby(\n    vmobjects, lambda vm: vm.get_background_image_file()\n)\nfor file_name, batch in batch_file_pairs:\n    if file_name:\n        self.display_multiple_background_colored_vmobjects(batch, pixel_array)\n    else:\n        self.display_multiple_non_background_colored_vmobjects(\n            batch, pixel_array\n        )", "path": "manim/manim/camera/camera.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "ManimCommunity/manim", "stars": 17709, "license": "mit", "language": "python", "size": 40568}
{"docstring": "\"\"\"\nChanges frame_shape to match the aspect ratio\nof the pixels, where fixed_dimension determines\nwhether frame_height or frame_width\nremains fixed while the other changes accordingly.\n\nParameters\n----------\nfixed_dimension : int\n    If 0, height is scaled with respect to width\n    else, width is scaled with respect to height.\n\"\"\"\n", "func_signal": "def resize_frame_shape(self, fixed_dimension=0):\n", "code": "pixel_height = self.pixel_height\npixel_width = self.pixel_width\nframe_height = self.frame_height\nframe_width = self.frame_width\naspect_ratio = fdiv(pixel_width, pixel_height)\nif fixed_dimension == 0:\n    frame_height = frame_width / aspect_ratio\nelse:\n    frame_width = aspect_ratio * frame_height\nself.frame_height = frame_height\nself.frame_width = frame_width", "path": "manim/manim/camera/camera.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "ManimCommunity/manim", "stars": 17709, "license": "mit", "language": "python", "size": 40568}
{"docstring": "\"\"\"\nTest the add user to group function\n\"\"\"\n", "func_signal": "def test_adduser(self):\n", "code": "self.run_function(\"group.add\", [self._group], gid=self._gid)\nself.run_function(\"user.add\", [self._user])\nself.assertTrue(self.run_function(\"group.adduser\", [self._group, self._user]))\ngroup_info = self.run_function(\"group.info\", [self._group])\nself.assertIn(self._user, str(group_info[\"members\"]))\n# try add a non existing user\nself.assertFalse(\n    self.run_function(\"group.adduser\", [self._group, self._no_user])\n)\n# try add a user to non existing group\nself.assertFalse(\n    self.run_function(\"group.adduser\", [self._no_group, self._user])\n)\n# try add a non existing user to a non existing group\nself.assertFalse(\n    self.run_function(\"group.adduser\", [self._no_group, self._no_user])\n)", "path": "salt/tests/integration/modules/test_groupadd.py", "commit_date": "2020-05-07 00:00:00", "repo_name": "saltstack/salt", "stars": 13768, "license": "apache-2.0", "language": "python", "size": 539318}
{"docstring": "\"\"\"\nTest the add group function with system=True\n\"\"\"\n\n", "func_signal": "def test_add_system_group(self):\n", "code": "gid_min, gid_max = self.__get_system_group_gid_range()\n\n# add a new system group\nself.assertTrue(self.run_function(\"group.add\", [self._group, None, True]))\ngroup_info = self.run_function(\"group.info\", [self._group])\nself.assertEqual(group_info[\"name\"], self._group)\nself.assertTrue(gid_min <= group_info[\"gid\"] <= gid_max)\n# try adding the group again\nself.assertFalse(self.run_function(\"group.add\", [self._group]))", "path": "salt/tests/integration/modules/test_groupadd.py", "commit_date": "2020-05-07 00:00:00", "repo_name": "saltstack/salt", "stars": 13768, "license": "apache-2.0", "language": "python", "size": 539318}
{"docstring": "\"\"\"\nTest the members function\n\"\"\"\n", "func_signal": "def test_members(self):\n", "code": "self.run_function(\"group.add\", [self._group], gid=self._gid)\nself.run_function(\"user.add\", [self._user])\nself.run_function(\"user.add\", [self._user1])\nm = \"{0},{1}\".format(self._user, self._user1)\nret = self.run_function(\"group.members\", [self._group, m])\nself.assertTrue(ret)\ngroup_info = self.run_function(\"group.info\", [self._group])\nself.assertIn(self._user, str(group_info[\"members\"]))\nself.assertIn(self._user1, str(group_info[\"members\"]))", "path": "salt/tests/integration/modules/test_groupadd.py", "commit_date": "2020-05-07 00:00:00", "repo_name": "saltstack/salt", "stars": 13768, "license": "apache-2.0", "language": "python", "size": 539318}
{"docstring": "\"\"\"\nTest the add group function with system=True and a specific gid\n\"\"\"\n\n", "func_signal": "def test_add_system_group_gid(self):\n", "code": "gid = self.__get_free_system_gid()\n\n# add a new system group\nself.assertTrue(self.run_function(\"group.add\", [self._group, gid, True]))\ngroup_info = self.run_function(\"group.info\", [self._group])\nself.assertEqual(group_info[\"name\"], self._group)\nself.assertEqual(group_info[\"gid\"], gid)\n# try adding the group again\nself.assertFalse(self.run_function(\"group.add\", [self._group, gid]))", "path": "salt/tests/integration/modules/test_groupadd.py", "commit_date": "2020-05-07 00:00:00", "repo_name": "saltstack/salt", "stars": 13768, "license": "apache-2.0", "language": "python", "size": 539318}
{"docstring": "\"\"\"\nGet current settings\n\"\"\"\n", "func_signal": "def setUp(self):\n", "code": "super(GroupModuleTest, self).setUp()\nself._user = random_string(\"tg-\", uppercase=False)\nself._user1 = random_string(\"tg-\", uppercase=False)\nself._no_user = random_string(\"tg-\", uppercase=False)\nself._group = random_string(\"tg-\", uppercase=False)\nself._no_group = random_string(\"tg-\", uppercase=False)\n_gid = _new_gid = None\nif not salt.utils.platform.is_windows():\n    _gid = 64989\n    _new_gid = 64998\nself._gid = _gid\nself._new_gid = _new_gid", "path": "salt/tests/integration/modules/test_groupadd.py", "commit_date": "2020-05-07 00:00:00", "repo_name": "saltstack/salt", "stars": 13768, "license": "apache-2.0", "language": "python", "size": 539318}
{"docstring": "\"\"\"\nTest the change gid function\n\"\"\"\n", "func_signal": "def test_chgid(self):\n", "code": "self.run_function(\"group.add\", [self._group], gid=self._gid)\nself.assertTrue(self.run_function(\"group.chgid\", [self._group, self._new_gid]))\ngroup_info = self.run_function(\"group.info\", [self._group])\nself.assertEqual(group_info[\"gid\"], self._new_gid)", "path": "salt/tests/integration/modules/test_groupadd.py", "commit_date": "2020-05-07 00:00:00", "repo_name": "saltstack/salt", "stars": 13768, "license": "apache-2.0", "language": "python", "size": 539318}
{"docstring": "\"\"\"\nTest the delete group function\n\"\"\"\n", "func_signal": "def test_delete(self):\n", "code": "self.assertTrue(self.run_function(\"group.add\", [self._group]))\n\n# correct functionality\nself.assertTrue(self.run_function(\"group.delete\", [self._group]))\n\n# group does not exist\nself.assertFalse(self.run_function(\"group.delete\", [self._no_group]))", "path": "salt/tests/integration/modules/test_groupadd.py", "commit_date": "2020-05-07 00:00:00", "repo_name": "saltstack/salt", "stars": 13768, "license": "apache-2.0", "language": "python", "size": 539318}
{"docstring": "\"\"\"\nReset to original settings\n\"\"\"\n", "func_signal": "def tearDown(self):\n", "code": "self.run_function(\"user.delete\", [self._user])\nself.run_function(\"user.delete\", [self._user1])\nself.run_function(\"group.delete\", [self._group])", "path": "salt/tests/integration/modules/test_groupadd.py", "commit_date": "2020-05-07 00:00:00", "repo_name": "saltstack/salt", "stars": 13768, "license": "apache-2.0", "language": "python", "size": 539318}
{"docstring": "\"\"\"\nTurn the layout data into usable vdevs spedcification\n\nWe need to support 2 ways of passing the layout:\n\n.. code::\n    layout_new:\n      - mirror:\n        - disk0\n        - disk1\n      - mirror:\n        - disk2\n        - disk3\n\n.. code:\n    layout_legacy:\n      mirror-0:\n        disk0\n        disk1\n      mirror-1:\n        disk2\n        disk3\n\n\"\"\"\n", "func_signal": "def _layout_to_vdev(layout, device_dir=None):\n", "code": "vdevs = []\n\n# NOTE: check device_dir exists\nif device_dir and not os.path.exists(device_dir):\n    device_dir = None\n\n# NOTE: handle list of OrderedDicts (new layout)\nif isinstance(layout, list):\n    # NOTE: parse each vdev as a tiny layout and just append\n    for vdev in layout:\n        if isinstance(vdev, OrderedDict):\n            vdevs.extend(_layout_to_vdev(vdev, device_dir))\n        else:\n            if device_dir and vdev[0] != \"/\":\n                vdev = os.path.join(device_dir, vdev)\n            vdevs.append(vdev)\n\n# NOTE: handle nested OrderedDict (legacy layout)\n#       this is also used to parse the nested OrderedDicts\n#       from the new layout\nelif isinstance(layout, OrderedDict):\n    for vdev in layout:\n        # NOTE: extract the vdev type and disks in the vdev\n        vdev_type = vdev.split(\"-\")[0]\n        vdev_disk = layout[vdev]\n\n        # NOTE: skip appending the dummy type 'disk'\n        if vdev_type != \"disk\":\n            vdevs.append(vdev_type)\n\n        # NOTE: ensure the disks are a list (legacy layout are not)\n        if not isinstance(vdev_disk, list):\n            vdev_disk = vdev_disk.split(\" \")\n\n        # NOTE: also append the actualy disks behind the type\n        #       also prepend device_dir to disks if required\n        for disk in vdev_disk:\n            if device_dir and disk[0] != \"/\":\n                disk = os.path.join(device_dir, disk)\n            vdevs.append(disk)\n\n# NOTE: we got invalid data for layout\nelse:\n    vdevs = None\n\nreturn vdevs", "path": "salt/salt/states/zpool.py", "commit_date": "2020-07-13 00:00:00", "repo_name": "saltstack/salt", "stars": 13768, "license": "apache-2.0", "language": "python", "size": 539318}
{"docstring": "\"\"\"\nProvides zpool state\n\"\"\"\n", "func_signal": "def __virtual__():\n", "code": "if not __grains__.get(\"zfs_support\"):\n    return False, \"The zpool state cannot be loaded: zfs not supported\"\nreturn __virtualname__", "path": "salt/salt/states/zpool.py", "commit_date": "2020-07-13 00:00:00", "repo_name": "saltstack/salt", "stars": 13768, "license": "apache-2.0", "language": "python", "size": 539318}
{"docstring": "\"\"\"\nTest the add group function\n\"\"\"\n# add a new group\n", "func_signal": "def test_add(self):\n", "code": "self.assertTrue(self.run_function(\"group.add\", [self._group], gid=self._gid))\ngroup_info = self.run_function(\"group.info\", [self._group])\nself.assertEqual(group_info[\"gid\"], self._gid)\nself.assertEqual(group_info[\"name\"], self._group)\n# try adding the group again\nself.assertFalse(self.run_function(\"group.add\", [self._group], gid=self._gid))", "path": "salt/tests/integration/modules/test_groupadd.py", "commit_date": "2020-05-07 00:00:00", "repo_name": "saltstack/salt", "stars": 13768, "license": "apache-2.0", "language": "python", "size": 539318}
{"docstring": "\"\"\"\nRemove the named group\n\nCLI Example:\n\n.. code-block:: bash\n\n    salt '*' group.delete foo\n\"\"\"\n", "func_signal": "def delete(name):\n", "code": "ret = __salt__[\"cmd.run_all\"](\"groupdel {0}\".format(name), python_shell=False)\n\nreturn not ret[\"retcode\"]", "path": "salt/salt/modules/solaris_group.py", "commit_date": "2020-04-03 00:00:00", "repo_name": "saltstack/salt", "stars": 13768, "license": "apache-2.0", "language": "python", "size": 539318}
{"docstring": "\"\"\"\nChange the gid for a named group\n\nCLI Example:\n\n.. code-block:: bash\n\n    salt '*' group.chgid foo 4376\n\"\"\"\n", "func_signal": "def chgid(name, gid):\n", "code": "pre_gid = __salt__[\"file.group_to_gid\"](name)\nif gid == pre_gid:\n    return True\ncmd = \"groupmod -g {0} {1}\".format(gid, name)\n__salt__[\"cmd.run\"](cmd, python_shell=False)\npost_gid = __salt__[\"file.group_to_gid\"](name)\nif post_gid != pre_gid:\n    return post_gid == gid\nreturn False", "path": "salt/salt/modules/solaris_group.py", "commit_date": "2020-04-03 00:00:00", "repo_name": "saltstack/salt", "stars": 13768, "license": "apache-2.0", "language": "python", "size": 539318}
{"docstring": "\"\"\"\nFind a free system gid\n\"\"\"\n\n", "func_signal": "def __get_free_system_gid(self):\n", "code": "gid_min, gid_max = self.__get_system_group_gid_range()\n\nbusy_gids = [x.gr_gid for x in grp.getgrall() if gid_min <= x.gr_gid <= gid_max]\n\n# find free system gid\nfor gid in range(gid_min, gid_max + 1):\n    if gid not in busy_gids:\n        return gid", "path": "salt/tests/integration/modules/test_groupadd.py", "commit_date": "2020-05-07 00:00:00", "repo_name": "saltstack/salt", "stars": 13768, "license": "apache-2.0", "language": "python", "size": 539318}
{"docstring": "\"\"\"\nAdd the specified group\n\nCLI Example:\n\n.. code-block:: bash\n\n    salt '*' group.add foo 3456\n\"\"\"\n", "func_signal": "def add(name, gid=None, **kwargs):\n", "code": "if salt.utils.data.is_true(kwargs.pop(\"system\", False)):\n    log.warning(\"solaris_group module does not support the 'system' \" \"argument\")\nif kwargs:\n    log.warning(\"Invalid kwargs passed to group.add\")\n\ncmd = \"groupadd \"\nif gid:\n    cmd += \"-g {0} \".format(gid)\ncmd += name\n\nret = __salt__[\"cmd.run_all\"](cmd, python_shell=False)\n\nreturn not ret[\"retcode\"]", "path": "salt/salt/modules/solaris_group.py", "commit_date": "2020-04-03 00:00:00", "repo_name": "saltstack/salt", "stars": 13768, "license": "apache-2.0", "language": "python", "size": 539318}
{"docstring": "\"\"\"\nReturns (SYS_GID_MIN, SYS_GID_MAX)\n\"\"\"\n", "func_signal": "def __get_system_group_gid_range(self):\n", "code": "try:\n    login_defs = {}\n    with salt.utils.files.fopen(\"/etc/login.defs\") as defs_fd:\n        for line in defs_fd:\n            line = salt.utils.stringutils.to_unicode(line).strip()\n            if line.startswith(\"#\"):\n                continue\n            try:\n                key, val = line.split()\n            except ValueError:\n                pass\n            else:\n                login_defs[key] = val\nexcept OSError:\n    login_defs = {\"SYS_GID_MIN\": 101, \"SYS_GID_MAX\": 999}\n\ngid_min = login_defs.get(\"SYS_GID_MIN\", 101)\ngid_max = login_defs.get(\n    \"SYS_GID_MAX\", int(login_defs.get(\"GID_MIN\", 1000)) - 1\n)\n\nreturn int(gid_min), int(gid_max)", "path": "salt/tests/integration/modules/test_groupadd.py", "commit_date": "2020-05-07 00:00:00", "repo_name": "saltstack/salt", "stars": 13768, "license": "apache-2.0", "language": "python", "size": 539318}
{"docstring": "\"\"\"\nReturn information about a group\n\nCLI Example:\n\n.. code-block:: bash\n\n    salt '*' group.info foo\n\"\"\"\n", "func_signal": "def info(name):\n", "code": "try:\n    grinfo = grp.getgrnam(name)\nexcept KeyError:\n    return {}\nelse:\n    return {\n        \"name\": grinfo.gr_name,\n        \"passwd\": grinfo.gr_passwd,\n        \"gid\": grinfo.gr_gid,\n        \"members\": grinfo.gr_mem,\n    }", "path": "salt/salt/modules/solaris_group.py", "commit_date": "2020-04-03 00:00:00", "repo_name": "saltstack/salt", "stars": 13768, "license": "apache-2.0", "language": "python", "size": 539318}
{"docstring": "\"\"\"\nensure storage pool is absent on the system\n\nname : string\n    name of storage pool\nexport : boolean\n    export instead of destroy the zpool if present\nforce : boolean\n    force destroy or export\n\n\"\"\"\n", "func_signal": "def absent(name, export=False, force=False):\n", "code": "ret = {\"name\": name, \"changes\": {}, \"result\": None, \"comment\": \"\"}\n\n# log configuration\nlog.debug(\"zpool.absent::%s::config::force = %s\", name, force)\nlog.debug(\"zpool.absent::%s::config::export = %s\", name, export)\n\n# ensure the pool is absent\nif __salt__[\"zpool.exists\"](name):  # looks like we need to do some work\n    mod_res = {}\n    ret[\"result\"] = False\n\n    # NOTE: handle test\n    if __opts__[\"test\"]:\n        ret[\"result\"] = True\n\n    # NOTE: try to export the pool\n    elif export:\n        mod_res = __salt__[\"zpool.export\"](name, force=force)\n        ret[\"result\"] = mod_res[\"exported\"]\n\n    # NOTE: try to destroy the pool\n    else:\n        mod_res = __salt__[\"zpool.destroy\"](name, force=force)\n        ret[\"result\"] = mod_res[\"destroyed\"]\n\n    if ret[\"result\"]:  # update the changes and comment\n        ret[\"changes\"][name] = \"exported\" if export else \"destroyed\"\n        ret[\"comment\"] = \"storage pool {0} was {1}\".format(\n            name, ret[\"changes\"][name]\n        )\n    elif \"error\" in mod_res:\n        ret[\"comment\"] = mod_res[\"error\"]\n\nelse:  # we are looking good\n    ret[\"result\"] = True\n    ret[\"comment\"] = \"storage pool {0} is absent\".format(name)\n\nreturn ret", "path": "salt/salt/states/zpool.py", "commit_date": "2020-07-13 00:00:00", "repo_name": "saltstack/salt", "stars": 13768, "license": "apache-2.0", "language": "python", "size": 539318}
{"docstring": "\"\"\"\nTest the getent function\n\"\"\"\n", "func_signal": "def test_getent(self):\n", "code": "self.run_function(\"group.add\", [self._group], gid=self._gid)\nself.run_function(\"user.add\", [self._user])\nself.run_function(\"group.adduser\", [self._group, self._user])\nginfo = self.run_function(\"user.getent\")\nself.assertIn(self._group, six.text_type(ginfo))\nself.assertIn(self._user, six.text_type(ginfo))\nself.assertNotIn(self._no_group, six.text_type(ginfo))\nself.assertNotIn(self._no_user, six.text_type(ginfo))", "path": "salt/tests/integration/modules/test_groupadd.py", "commit_date": "2020-05-07 00:00:00", "repo_name": "saltstack/salt", "stars": 13768, "license": "apache-2.0", "language": "python", "size": 539318}
{"docstring": "\"\"\"\nTest the info group function\n\"\"\"\n", "func_signal": "def test_info(self):\n", "code": "self.run_function(\"group.add\", [self._group], gid=self._gid)\nself.run_function(\"user.add\", [self._user])\nself.run_function(\"group.adduser\", [self._group, self._user])\ngroup_info = self.run_function(\"group.info\", [self._group])\n\nself.assertEqual(group_info[\"name\"], self._group)\nself.assertEqual(group_info[\"gid\"], self._gid)\nself.assertIn(self._user, str(group_info[\"members\"]))", "path": "salt/tests/integration/modules/test_groupadd.py", "commit_date": "2020-05-07 00:00:00", "repo_name": "saltstack/salt", "stars": 13768, "license": "apache-2.0", "language": "python", "size": 539318}
{"docstring": "\"\"\"\n    With this test, a case which has a task (TestWrapperTask), requires two another tasks (TestErrorTask1,TestErrorTask1) which both is failed, is\n    tested.\n\n    Task TestErrorTask1 has default retry_count which is 1, but Task TestErrorTask2 has retry_count at task level as 2.\n\n    This test is running on single worker\n\"\"\"\n\n", "func_signal": "def test_with_all_disabled_with_single_worker(self):\n", "code": "class TestErrorTask1(DummyErrorTask):\n    pass\n\ne1 = TestErrorTask1()\n\nclass TestErrorTask2(DummyErrorTask):\n    retry_count = self.per_task_retry_count\n\ne2 = TestErrorTask2()\n\nclass TestWrapperTask(luigi.WrapperTask):\n    def requires(self):\n        return [e2, e1]\n\nwt = TestWrapperTask()\n\nwith Worker(scheduler=self.sch, worker_id='X', keep_alive=True, wait_interval=0.1, wait_jitter=0.05) as w1:\n    self.assertTrue(w1.add(wt))\n\n    self.assertFalse(w1.run())\n\n    self.assertEqual([wt.task_id], list(self.sch.task_list('PENDING', 'UPSTREAM_DISABLED').keys()))\n\n    self.assertEqual(sorted([e1.task_id, e2.task_id]), sorted(self.sch.task_list('DISABLED', '').keys()))\n\n    self.assertEqual(0, self.sch._state.get_task(wt.task_id).num_failures())\n    self.assertEqual(self.per_task_retry_count, self.sch._state.get_task(e2.task_id).num_failures())\n    self.assertEqual(self.default_retry_count, self.sch._state.get_task(e1.task_id).num_failures())", "path": "luigi/test/worker_test.py", "commit_date": "2020-06-09 00:00:00", "repo_name": "spotify/luigi", "stars": 17178, "license": "apache-2.0", "language": "python", "size": 10798}
{"docstring": "\"\"\"\nFill test database with fake data\n\"\"\"\n", "func_signal": "def setUp(self):\n", "code": "self.mongo_client = pymongo.MongoClient(HOST, PORT)\nself.collection = self.mongo_client[INDEX][COLLECTION]\n\nself.collection.delete_many({})\n\ntest_docs = [\n    {'_id': 'person_1', 'age': 11, 'experience': 10, 'content': \"Lorem ipsum, dolor sit amet. Consectetur adipiscing elit.\"},\n    {'_id': 'person_2', 'age': 12, 'experience': 22, 'content': \"Sed purus nisl. Faucibus in, erat eu. Rhoncus mattis velit.\"},\n    {'_id': 'person_3', 'age': 13, 'content': \"Nulla malesuada, fringilla lorem at pellentesque.\"},\n    {'_id': 'person_4', 'age': 14, 'content': \"Curabitur condimentum. Venenatis fringilla.\"}\n]\n\nself.collection.insert_many(test_docs)", "path": "luigi/test/contrib/mongo_test.py", "commit_date": "2018-11-21 00:00:00", "repo_name": "spotify/luigi", "stars": 17178, "license": "apache-2.0", "language": "python", "size": 10798}
{"docstring": "\"\"\" verify configured jitter amount \"\"\"\n", "func_signal": "def test_wait_jitter(self, mock_sleep, mock_random):\n", "code": "mock_random.return_value = 1.0\n\nw = Worker()\nx = w._sleeper()\nnext(x)\nmock_random.assert_called_with(0, 10.0)\nmock_sleep.assert_called_with(2.0)\n\nmock_random.return_value = 2.0\nnext(x)\nmock_random.assert_called_with(0, 10.0)\nmock_sleep.assert_called_with(3.0)", "path": "luigi/test/worker_test.py", "commit_date": "2020-06-09 00:00:00", "repo_name": "spotify/luigi", "stars": 17178, "license": "apache-2.0", "language": "python", "size": 10798}
{"docstring": "\"\"\"\nCloning can pull non-source-parameters from source to target parameter.\n\"\"\"\n\n", "func_signal": "def test_inheritance_from_non_parameter(self):\n", "code": "class SubTask(luigi.Task):\n    lo = 1\n\n    @property\n    def hi(self):\n        return 2\n\nt1 = SubTask()\nt2 = t1.clone(cls=LinearSum)\nself.assertEqual(t2.lo, 1)\nself.assertEqual(t2.hi, 2)", "path": "luigi/test/clone_test.py", "commit_date": "2016-11-01 00:00:00", "repo_name": "spotify/luigi", "stars": 17178, "license": "apache-2.0", "language": "python", "size": 10798}
{"docstring": "\"\"\"\nReturns a temporary file path based on a MD5 hash generated with the task's name and its arguments\n\"\"\"\n", "func_signal": "def get_path(self):\n", "code": "md5_hash = hashlib.md5(self.task_id.encode()).hexdigest()\nlogger.debug('Hash %s corresponds to task %s', md5_hash, self.task_id)\n\nreturn os.path.join(self.temp_dir, str(self.unique.value), md5_hash)", "path": "luigi/luigi/contrib/simulate.py", "commit_date": "2019-12-02 00:00:00", "repo_name": "spotify/luigi", "stars": 17178, "license": "apache-2.0", "language": "python", "size": 10798}
{"docstring": "\"\"\"\nRun either the mapper, combiner, or reducer from the class instance in the file \"job-instance.pickle\".\n\nArguments:\n\nkind -- is either map, combiner, or reduce\n\"\"\"\n", "func_signal": "def main(args=None, stdin=sys.stdin, stdout=sys.stdout, print_exception=print_exception):\n", "code": "try:\n    # Set up logging.\n    logging.basicConfig(level=logging.WARN)\n\n    kind = args is not None and args[1] or sys.argv[1]\n    Runner().run(kind, stdin=stdin, stdout=stdout)\nexcept Exception as exc:\n    # Dump encoded data that we will try to fetch using mechanize\n    print_exception(exc)\n    raise", "path": "luigi/luigi/contrib/mrrunner.py", "commit_date": "2020-06-01 00:00:00", "repo_name": "spotify/luigi", "stars": 17178, "license": "apache-2.0", "language": "python", "size": 10798}
{"docstring": "\"\"\"\nTest that Worker() asserts that it's sanely configured\n\"\"\"\n", "func_signal": "def test_asserts_for_worker(self):\n", "code": "Worker(wait_interval=1)  # This shouldn't raise\nself.assertRaises(AssertionError, Worker, wait_interval=0)", "path": "luigi/test/worker_test.py", "commit_date": "2020-06-09 00:00:00", "repo_name": "spotify/luigi", "stars": 17178, "license": "apache-2.0", "language": "python", "size": 10798}
{"docstring": "\"\"\"\nMake sure the test database is in clean state\n\"\"\"\n", "func_signal": "def tearDown(self):\n", "code": "self.collection.drop()\nself.mongo_client.drop_database(INDEX)", "path": "luigi/test/contrib/mongo_test.py", "commit_date": "2018-11-21 00:00:00", "repo_name": "spotify/luigi", "stars": 17178, "license": "apache-2.0", "language": "python", "size": 10798}
{"docstring": "\"\"\"\nreplace to\n```\n(_self, *args), *_ = inspect.getfullargspec(Cursor.__init__)\n```\nafter py2-deprecation\n\"\"\"\n", "func_signal": "def _kwargs():\n", "code": "args = inspect.getargspec(Cursor.__init__)[0][1:]\nfor parameter in args:\n    val = getattr(self, parameter)\n    if val:\n        yield parameter, val", "path": "luigi/luigi/contrib/presto.py", "commit_date": "2020-06-01 00:00:00", "repo_name": "spotify/luigi", "stars": 17178, "license": "apache-2.0", "language": "python", "size": 10798}
{"docstring": "\"\"\"\nEnsure that keyboard interrupts causes luigi to quit when you are\nexecuting tasks.\n\nTODO: Add a test that tests the multiprocessing (--worker >1) case\n\"\"\"\n", "func_signal": "def test_propagation_when_executing(self):\n", "code": "class KeyboardInterruptTask(luigi.Task):\n    def run(self):\n        raise KeyboardInterrupt()\n\ncmd = 'KeyboardInterruptTask --local-scheduler --no-lock'.split(' ')\nself.assertRaises(KeyboardInterrupt, luigi_run, cmd)", "path": "luigi/test/worker_test.py", "commit_date": "2020-06-09 00:00:00", "repo_name": "spotify/luigi", "stars": 17178, "license": "apache-2.0", "language": "python", "size": 10798}
{"docstring": "\"\"\"\n    With this test, a case includes dependency tasks(TestErrorTask1,TestErrorTask2) which both are failed.\n\n    Task TestErrorTask1 has default retry_count which is 1, but Task TestErrorTask2 has retry_count at task level as 2.\n\n    This test is running on single worker\n\"\"\"\n\n", "func_signal": "def test_with_dynamic_dependencies_with_single_worker(self):\n", "code": "class TestErrorTask1(DummyErrorTask):\n    pass\n\ne1 = TestErrorTask1()\n\nclass TestErrorTask2(DummyErrorTask):\n    retry_count = self.per_task_retry_count\n\ne2 = TestErrorTask2()\n\nclass TestSuccessTask1(DummyTask):\n    pass\n\ns1 = TestSuccessTask1()\n\nclass TestWrapperTask(DummyTask):\n    def requires(self):\n        return [s1]\n\n    def run(self):\n        super(TestWrapperTask, self).run()\n        yield e2, e1\n\nwt = TestWrapperTask()\n\nwith Worker(scheduler=self.sch, worker_id='X', keep_alive=True, wait_interval=0.1, wait_jitter=0.05) as w1:\n    self.assertTrue(w1.add(wt))\n\n    self.assertFalse(w1.run())\n\n    self.assertEqual([wt.task_id], list(self.sch.task_list('PENDING', 'UPSTREAM_DISABLED').keys()))\n\n    self.assertEqual(sorted([e1.task_id, e2.task_id]), sorted(self.sch.task_list('DISABLED', '').keys()))\n\n    self.assertEqual(0, self.sch._state.get_task(wt.task_id).num_failures())\n    self.assertEqual(0, self.sch._state.get_task(s1.task_id).num_failures())\n    self.assertEqual(self.per_task_retry_count, self.sch._state.get_task(e2.task_id).num_failures())\n    self.assertEqual(self.default_retry_count, self.sch._state.get_task(e1.task_id).num_failures())", "path": "luigi/test/worker_test.py", "commit_date": "2020-06-09 00:00:00", "repo_name": "spotify/luigi", "stars": 17178, "license": "apache-2.0", "language": "python", "size": 10798}
{"docstring": "\"\"\"\n    With this test, a case which has a task (TestWrapperTask), requires one (TestErrorTask1) FAILED and one (TestSuccessTask1) SUCCESS, is tested.\n\n    Task TestSuccessTask1 will be DONE successfully, but Task TestErrorTask1 will be failed and it has retry_count at task level as 2.\n\n    This test is running on multiple worker\n\"\"\"\n\n", "func_signal": "def test_with_includes_success_with_multiple_worker(self):\n", "code": "class TestSuccessTask1(DummyTask):\n    pass\n\ns1 = TestSuccessTask1()\n\nclass TestErrorTask1(DummyErrorTask):\n    retry_count = self.per_task_retry_count\n\ne1 = TestErrorTask1()\n\nclass TestWrapperTask(luigi.WrapperTask):\n    def requires(self):\n        return [e1, s1]\n\nwt = TestWrapperTask()\n\nwith Worker(scheduler=self.sch, worker_id='X', keep_alive=True, wait_interval=0.1, wait_jitter=0.05) as w1:\n    with Worker(scheduler=self.sch, worker_id='Y', keep_alive=True, wait_interval=0.1, wait_jitter=0.05) as w2:\n        with Worker(scheduler=self.sch, worker_id='Z', keep_alive=True, wait_interval=0.1, wait_jitter=0.05) as w3:\n            self.assertTrue(w1.add(wt))\n            self.assertTrue(w2.add(e1))\n            self.assertTrue(w3.add(s1))\n\n            self.assertTrue(w3.run())\n            self.assertFalse(w2.run())\n            self.assertTrue(w1.run())\n\n            self.assertEqual([wt.task_id], list(self.sch.task_list('PENDING', 'UPSTREAM_DISABLED').keys()))\n            self.assertEqual([e1.task_id], list(self.sch.task_list('DISABLED', '').keys()))\n            self.assertEqual([s1.task_id], list(self.sch.task_list('DONE', '').keys()))\n\n            self.assertEqual(0, self.sch._state.get_task(wt.task_id).num_failures())\n            self.assertEqual(self.per_task_retry_count, self.sch._state.get_task(e1.task_id).num_failures())\n            self.assertEqual(0, self.sch._state.get_task(s1.task_id).num_failures())", "path": "luigi/test/worker_test.py", "commit_date": "2020-06-09 00:00:00", "repo_name": "spotify/luigi", "stars": 17178, "license": "apache-2.0", "language": "python", "size": 10798}
{"docstring": "# Test using multiple workers\n# Also test generating classes dynamically since this may reflect issues with\n# various platform and how multiprocessing is implemented. If it's using os.fork\n# under the hood it should be fine, but dynamic classses can't be pickled, so\n# other implementations of multiprocessing (using spawn etc) may fail\n", "func_signal": "def test_multiple_workers(self):\n", "code": "class MyDynamicTask(luigi.Task):\n    x = luigi.Parameter()\n\n    def run(self):\n        time.sleep(0.1)\n\nt0 = time.time()\nluigi.build([MyDynamicTask(i) for i in range(100)], workers=100, local_scheduler=True)\nself.assertTrue(time.time() < t0 + 5.0)  # should ideally take exactly 0.1s, but definitely less than 10.0", "path": "luigi/test/worker_test.py", "commit_date": "2020-06-09 00:00:00", "repo_name": "spotify/luigi", "stars": 17178, "license": "apache-2.0", "language": "python", "size": 10798}
{"docstring": "# Running the test in another process because the PID is used to determine if the target exists\n", "func_signal": "def test_output_again(self):\n", "code": "p = Process(target=self.test_output)\np.start()\np.join()", "path": "luigi/test/simulate_test.py", "commit_date": "2017-11-04 00:00:00", "repo_name": "spotify/luigi", "stars": 17178, "license": "apache-2.0", "language": "python", "size": 10798}
{"docstring": "\"\"\"\nCreates the client as specified in the `luigi.cfg` configuration.\n\"\"\"\n", "func_signal": "def get_autoconfig_client(client_cache=_AUTOCONFIG_CLIENT):\n", "code": "try:\n    return client_cache.client\nexcept AttributeError:\n    configured_client = hdfs_config.get_configured_hdfs_client()\n    if configured_client == \"webhdfs\":\n        client_cache.client = hdfs_webhdfs_client.WebHdfsClient()\n    elif configured_client == \"hadoopcli\":\n        client_cache.client = hdfs_hadoopcli_clients.create_hadoopcli_client()\n    else:\n        raise Exception(\"Unknown hdfs client \" + configured_client)\n    return client_cache.client", "path": "luigi/luigi/contrib/hdfs/clients.py", "commit_date": "2020-06-01 00:00:00", "repo_name": "spotify/luigi", "stars": 17178, "license": "apache-2.0", "language": "python", "size": 10798}
{"docstring": "\"\"\"\n\n:param query: query to run\n:param parameters: parameters should be injected in the query\n:param mode: \"fetch\" - yields rows, \"watch\" - yields log entries\n:return:\n\"\"\"\n", "func_signal": "def execute(self, query, parameters=None, mode=None):\n", "code": "class Mode(Enum):\n    watch = 'watch'\n    fetch = 'fetch'\n\n_mode = Mode(mode) if mode else Mode.watch\n\nwith closing(self._connection.cursor()) as cursor:\n    cursor.execute(query, parameters)\n    status = self._status\n    while status:\n        sleep(self.sleep_time)\n        status = cursor.poll()\n        if status:\n            if _mode == Mode.watch:\n                yield status\n            self._status = status\n\n    if _mode == Mode.fetch:\n        for row in cursor.fetchall():\n            yield row", "path": "luigi/luigi/contrib/presto.py", "commit_date": "2020-06-01 00:00:00", "repo_name": "spotify/luigi", "stars": 17178, "license": "apache-2.0", "language": "python", "size": 10798}
{"docstring": "\"\"\"Test work with pathlib.Path\"\"\"\n", "func_signal": "def test_pathlib(self):\n", "code": "import pathlib\npath = pathlib.Path(self.path)\nself.assertFalse(path.exists())\ntarget = LocalTarget(path)\nself.assertFalse(target.exists())\nwith path.open('w') as stream:\n    stream.write('test me')\nself.assertTrue(target.exists())", "path": "luigi/test/local_target_test.py", "commit_date": "2020-06-01 00:00:00", "repo_name": "spotify/luigi", "stars": 17178, "license": "apache-2.0", "language": "python", "size": 10798}
{"docstring": "\"\"\"\nEnsure that `Task.disable_window` impacts the task retrying policy:\n- with the scheduler retry policy (disable_window=3), task fails twice and gets disabled\n- with the task retry policy (disable_window=0.5) task never gets into the DISABLED state\n\"\"\"\n", "func_signal": "def test_per_task_disable_persist_with_single_worker(self):\n", "code": "class TwoErrorsThenSuccessTask(Task):\n    \"\"\"\n    The task is failing two times and then succeeds, waiting 1s before each try\n    \"\"\"\n    retry_index = 0\n    disable_window = None\n\n    def run(self):\n        time.sleep(1)\n        self.retry_index += 1\n        if self.retry_index < 3:\n            raise Exception(\"Retry index is %s for %s\" % (self.retry_index, self.task_family))\n\nt = TwoErrorsThenSuccessTask()\n\nsch = Scheduler(retry_delay=0.1, retry_count=2, prune_on_get_work=True, disable_window=2)\nwith Worker(scheduler=sch, worker_id='X', keep_alive=True, wait_interval=0.1, wait_jitter=0.05) as w:\n    self.assertTrue(w.add(t))\n    self.assertFalse(w.run())\n\n    self.assertEqual(2, t.retry_index)\n    self.assertEqual([t.task_id], list(sch.task_list('DISABLED').keys()))\n    self.assertEqual(2, sch._state.get_task(t.task_id).num_failures())\n\nt = TwoErrorsThenSuccessTask()\nt.retry_index = 0\nt.disable_window = 0.5\n\nsch = Scheduler(retry_delay=0.1, retry_count=2, prune_on_get_work=True, disable_window=2)\nwith Worker(scheduler=sch, worker_id='X', keep_alive=True, wait_interval=0.1, wait_jitter=0.05) as w:\n    self.assertTrue(w.add(t))\n    # Worker.run return False even if a task failed first but eventually succeeded.\n    self.assertFalse(w.run())\n\n    self.assertEqual(3, t.retry_index)\n    self.assertEqual([t.task_id], list(sch.task_list('DONE').keys()))\n    self.assertEqual(1, len(sch._state.get_task(t.task_id).failures))", "path": "luigi/test/worker_test.py", "commit_date": "2020-06-09 00:00:00", "repo_name": "spotify/luigi", "stars": 17178, "license": "apache-2.0", "language": "python", "size": 10798}
{"docstring": "\"\"\"\nTest that we do not send error emails on the failures of external tasks\n\"\"\"\n", "func_signal": "def test_external_task_retries(self, emails):\n", "code": "class A(luigi.ExternalTask):\n    pass\n\na = A()\nluigi.build([a], workers=2, local_scheduler=True)\nself.assertEqual(emails, [])", "path": "luigi/test/worker_test.py", "commit_date": "2020-06-09 00:00:00", "repo_name": "spotify/luigi", "stars": 17178, "license": "apache-2.0", "language": "python", "size": 10798}
{"docstring": "\"\"\"\nFill test database with fake data\n\"\"\"\n", "func_signal": "def setUp(self):\n", "code": "self.mongo_client = pymongo.MongoClient(HOST, PORT)\nself.collection = self.mongo_client[INDEX][COLLECTION]\n\nself.collection.delete_many({})\n\ntest_docs = [\n    {'_id': 'person_1', 'name': 'Mike', 'infos': {'family': 'single'}},\n    {'_id': 'person_2', 'name': 'Laura', 'surname': 'Gilmore'},\n    {'_id': 'person_3', 'surname': 'Specter'},\n    {'_id': 'person_4', 'surname': '', 'infos': {'family': {'children': ['jack', 'rose']}}}\n]\n\nself.collection.insert_many(test_docs)", "path": "luigi/test/contrib/mongo_test.py", "commit_date": "2018-11-21 00:00:00", "repo_name": "spotify/luigi", "stars": 17178, "license": "apache-2.0", "language": "python", "size": 10798}
{"docstring": "# Create a star graph in which half the nodes are directed in\n# and half are directed out.\n", "func_signal": "def test_unweighted_directed(self):\n", "code": "G = nx.DiGraph()\nG.add_edges_from((0, v) for v in range(1, 26))\nG.add_edges_from((v, 0) for v in range(26, 51))\ncover = min_weighted_vertex_cover(G)\nassert 2 == len(cover)\nassert is_cover(G, cover)", "path": "networkx/networkx/algorithms/approximation/tests/test_vertex_cover.py", "commit_date": "2020-01-01 00:00:00", "repo_name": "networkx/networkx", "stars": 13985, "license": "other", "language": "python", "size": 92784}
{"docstring": "\"\"\"Returns True if graph G is bipartite, False if not.\n\nParameters\n----------\nG : NetworkX graph\n\nExamples\n--------\n>>> from networkx.algorithms import bipartite\n>>> G = nx.path_graph(4)\n>>> print(bipartite.is_bipartite(G))\nTrue\n\nSee Also\n--------\ncolor, is_bipartite_node_set\n\"\"\"\n", "func_signal": "def is_bipartite(G):\n", "code": "try:\n    color(G)\n    return True\nexcept nx.NetworkXError:\n    return False", "path": "networkx/networkx/algorithms/bipartite/basic.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "networkx/networkx", "stars": 13985, "license": "other", "language": "python", "size": 92784}
{"docstring": "\"\"\"Returns Davis Southern women social network.\n\nThis is a bipartite graph.\n\nReferences\n----------\n.. [1] A. Davis, Gardner, B. B., Gardner, M. R., 1941. Deep South.\n    University of Chicago Press, Chicago, IL.\n\"\"\"\n", "func_signal": "def davis_southern_women_graph():\n", "code": "G = nx.Graph()\n# Top nodes\nwomen = [\n    \"Evelyn Jefferson\",\n    \"Laura Mandeville\",\n    \"Theresa Anderson\",\n    \"Brenda Rogers\",\n    \"Charlotte McDowd\",\n    \"Frances Anderson\",\n    \"Eleanor Nye\",\n    \"Pearl Oglethorpe\",\n    \"Ruth DeSand\",\n    \"Verne Sanderson\",\n    \"Myra Liddel\",\n    \"Katherina Rogers\",\n    \"Sylvia Avondale\",\n    \"Nora Fayette\",\n    \"Helen Lloyd\",\n    \"Dorothy Murchison\",\n    \"Olivia Carleton\",\n    \"Flora Price\",\n]\nG.add_nodes_from(women, bipartite=0)\n# Bottom nodes\nevents = [\n    \"E1\",\n    \"E2\",\n    \"E3\",\n    \"E4\",\n    \"E5\",\n    \"E6\",\n    \"E7\",\n    \"E8\",\n    \"E9\",\n    \"E10\",\n    \"E11\",\n    \"E12\",\n    \"E13\",\n    \"E14\",\n]\nG.add_nodes_from(events, bipartite=1)\n\nG.add_edges_from(\n    [\n        (\"Evelyn Jefferson\", \"E1\"),\n        (\"Evelyn Jefferson\", \"E2\"),\n        (\"Evelyn Jefferson\", \"E3\"),\n        (\"Evelyn Jefferson\", \"E4\"),\n        (\"Evelyn Jefferson\", \"E5\"),\n        (\"Evelyn Jefferson\", \"E6\"),\n        (\"Evelyn Jefferson\", \"E8\"),\n        (\"Evelyn Jefferson\", \"E9\"),\n        (\"Laura Mandeville\", \"E1\"),\n        (\"Laura Mandeville\", \"E2\"),\n        (\"Laura Mandeville\", \"E3\"),\n        (\"Laura Mandeville\", \"E5\"),\n        (\"Laura Mandeville\", \"E6\"),\n        (\"Laura Mandeville\", \"E7\"),\n        (\"Laura Mandeville\", \"E8\"),\n        (\"Theresa Anderson\", \"E2\"),\n        (\"Theresa Anderson\", \"E3\"),\n        (\"Theresa Anderson\", \"E4\"),\n        (\"Theresa Anderson\", \"E5\"),\n        (\"Theresa Anderson\", \"E6\"),\n        (\"Theresa Anderson\", \"E7\"),\n        (\"Theresa Anderson\", \"E8\"),\n        (\"Theresa Anderson\", \"E9\"),\n        (\"Brenda Rogers\", \"E1\"),\n        (\"Brenda Rogers\", \"E3\"),\n        (\"Brenda Rogers\", \"E4\"),\n        (\"Brenda Rogers\", \"E5\"),\n        (\"Brenda Rogers\", \"E6\"),\n        (\"Brenda Rogers\", \"E7\"),\n        (\"Brenda Rogers\", \"E8\"),\n        (\"Charlotte McDowd\", \"E3\"),\n        (\"Charlotte McDowd\", \"E4\"),\n        (\"Charlotte McDowd\", \"E5\"),\n        (\"Charlotte McDowd\", \"E7\"),\n        (\"Frances Anderson\", \"E3\"),\n        (\"Frances Anderson\", \"E5\"),\n        (\"Frances Anderson\", \"E6\"),\n        (\"Frances Anderson\", \"E8\"),\n        (\"Eleanor Nye\", \"E5\"),\n        (\"Eleanor Nye\", \"E6\"),\n        (\"Eleanor Nye\", \"E7\"),\n        (\"Eleanor Nye\", \"E8\"),\n        (\"Pearl Oglethorpe\", \"E6\"),\n        (\"Pearl Oglethorpe\", \"E8\"),\n        (\"Pearl Oglethorpe\", \"E9\"),\n        (\"Ruth DeSand\", \"E5\"),\n        (\"Ruth DeSand\", \"E7\"),\n        (\"Ruth DeSand\", \"E8\"),\n        (\"Ruth DeSand\", \"E9\"),\n        (\"Verne Sanderson\", \"E7\"),\n        (\"Verne Sanderson\", \"E8\"),\n        (\"Verne Sanderson\", \"E9\"),\n        (\"Verne Sanderson\", \"E12\"),\n        (\"Myra Liddel\", \"E8\"),\n        (\"Myra Liddel\", \"E9\"),\n        (\"Myra Liddel\", \"E10\"),\n        (\"Myra Liddel\", \"E12\"),\n        (\"Katherina Rogers\", \"E8\"),\n        (\"Katherina Rogers\", \"E9\"),\n        (\"Katherina Rogers\", \"E10\"),\n        (\"Katherina Rogers\", \"E12\"),\n        (\"Katherina Rogers\", \"E13\"),\n        (\"Katherina Rogers\", \"E14\"),\n        (\"Sylvia Avondale\", \"E7\"),\n        (\"Sylvia Avondale\", \"E8\"),\n        (\"Sylvia Avondale\", \"E9\"),\n        (\"Sylvia Avondale\", \"E10\"),\n        (\"Sylvia Avondale\", \"E12\"),\n        (\"Sylvia Avondale\", \"E13\"),\n        (\"Sylvia Avondale\", \"E14\"),\n        (\"Nora Fayette\", \"E6\"),\n        (\"Nora Fayette\", \"E7\"),\n        (\"Nora Fayette\", \"E9\"),\n        (\"Nora Fayette\", \"E10\"),\n        (\"Nora Fayette\", \"E11\"),\n        (\"Nora Fayette\", \"E12\"),\n        (\"Nora Fayette\", \"E13\"),\n        (\"Nora Fayette\", \"E14\"),\n        (\"Helen Lloyd\", \"E7\"),\n        (\"Helen Lloyd\", \"E8\"),\n        (\"Helen Lloyd\", \"E10\"),\n        (\"Helen Lloyd\", \"E11\"),\n        (\"Helen Lloyd\", \"E12\"),\n        (\"Dorothy Murchison\", \"E8\"),\n        (\"Dorothy Murchison\", \"E9\"),\n        (\"Olivia Carleton\", \"E9\"),\n        (\"Olivia Carleton\", \"E11\"),\n        (\"Flora Price\", \"E9\"),\n        (\"Flora Price\", \"E11\"),\n    ]\n)\nG.graph[\"top\"] = women\nG.graph[\"bottom\"] = events\nreturn G", "path": "networkx/networkx/generators/social.py", "commit_date": "2020-08-19 00:00:00", "repo_name": "networkx/networkx", "stars": 13985, "license": "other", "language": "python", "size": 92784}
{"docstring": "# test if cut can be locally improved\n", "func_signal": "def _cut_is_locally_optimal(G, cut_size, set1):\n", "code": "for i, node in enumerate(set1):\n    cut_size_without_node = nx.algorithms.cut_size(\n        G, set1 - {node}, weight=\"weight\"\n    )\n    assert cut_size_without_node <= cut_size", "path": "networkx/networkx/algorithms/approximation/tests/test_maxcut.py", "commit_date": "2020-12-15 00:00:00", "repo_name": "networkx/networkx", "stars": 13985, "license": "other", "language": "python", "size": 92784}
{"docstring": "\"\"\"Returns bipartite node sets of graph G.\n\nRaises an exception if the graph is not bipartite or if the input\ngraph is disconnected and thus more than one valid solution exists.\nSee :mod:`bipartite documentation <networkx.algorithms.bipartite>`\nfor further details on how bipartite graphs are handled in NetworkX.\n\nParameters\n----------\nG : NetworkX graph\n\ntop_nodes : container, optional\n  Container with all nodes in one bipartite node set. If not supplied\n  it will be computed. But if more than one solution exists an exception\n  will be raised.\n\nReturns\n-------\nX : set\n  Nodes from one side of the bipartite graph.\nY : set\n  Nodes from the other side.\n\nRaises\n------\nAmbiguousSolution\n  Raised if the input bipartite graph is disconnected and no container\n  with all nodes in one bipartite set is provided. When determining\n  the nodes in each bipartite set more than one valid solution is\n  possible if the input graph is disconnected.\nNetworkXError\n  Raised if the input graph is not bipartite.\n\nExamples\n--------\n>>> from networkx.algorithms import bipartite\n>>> G = nx.path_graph(4)\n>>> X, Y = bipartite.sets(G)\n>>> list(X)\n[0, 2]\n>>> list(Y)\n[1, 3]\n\nSee Also\n--------\ncolor\n\n\"\"\"\n", "func_signal": "def sets(G, top_nodes=None):\n", "code": "if G.is_directed():\n    is_connected = nx.is_weakly_connected\nelse:\n    is_connected = nx.is_connected\nif top_nodes is not None:\n    X = set(top_nodes)\n    Y = set(G) - X\nelse:\n    if not is_connected(G):\n        msg = \"Disconnected graph: Ambiguous solution for bipartite sets.\"\n        raise nx.AmbiguousSolution(msg)\n    c = color(G)\n    X = {n for n, is_top in c.items() if is_top}\n    Y = {n for n, is_top in c.items() if not is_top}\nreturn (X, Y)", "path": "networkx/networkx/algorithms/bipartite/basic.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "networkx/networkx", "stars": 13985, "license": "other", "language": "python", "size": 92784}
{"docstring": "# FC article claims 2d grid graph of size n is (3,3)-connected\n# and (5,9)-connected, but I don't think it is (5,9)-connected\n", "func_signal": "def test_2d_grid_graph():\n", "code": "G = nx.grid_2d_graph(8, 8, periodic=True)\nassert nx.is_kl_connected(G, 3, 3)\nassert not nx.is_kl_connected(G, 5, 9)\n(H, graphOK) = nx.kl_connected_subgraph(G, 5, 9, same_as_graph=True)\nassert not graphOK", "path": "networkx/networkx/algorithms/tests/test_hybrid.py", "commit_date": "2020-07-10 00:00:00", "repo_name": "networkx/networkx", "stars": 13985, "license": "other", "language": "python", "size": 92784}
{"docstring": "\"\"\"Returns coappearance network of characters in the novel Les Miserables.\n\nReferences\n----------\n.. [1] D. E. Knuth, 1993.\n   The Stanford GraphBase: a platform for combinatorial computing,\n   pp. 74-87. New York: AcM Press.\n\"\"\"\n", "func_signal": "def les_miserables_graph():\n", "code": "G = nx.Graph()\nG.add_edge(\"Napoleon\", \"Myriel\", weight=1)\nG.add_edge(\"MlleBaptistine\", \"Myriel\", weight=8)\nG.add_edge(\"MmeMagloire\", \"Myriel\", weight=10)\nG.add_edge(\"MmeMagloire\", \"MlleBaptistine\", weight=6)\nG.add_edge(\"CountessDeLo\", \"Myriel\", weight=1)\nG.add_edge(\"Geborand\", \"Myriel\", weight=1)\nG.add_edge(\"Champtercier\", \"Myriel\", weight=1)\nG.add_edge(\"Cravatte\", \"Myriel\", weight=1)\nG.add_edge(\"Count\", \"Myriel\", weight=2)\nG.add_edge(\"OldMan\", \"Myriel\", weight=1)\nG.add_edge(\"Valjean\", \"Labarre\", weight=1)\nG.add_edge(\"Valjean\", \"MmeMagloire\", weight=3)\nG.add_edge(\"Valjean\", \"MlleBaptistine\", weight=3)\nG.add_edge(\"Valjean\", \"Myriel\", weight=5)\nG.add_edge(\"Marguerite\", \"Valjean\", weight=1)\nG.add_edge(\"MmeDeR\", \"Valjean\", weight=1)\nG.add_edge(\"Isabeau\", \"Valjean\", weight=1)\nG.add_edge(\"Gervais\", \"Valjean\", weight=1)\nG.add_edge(\"Listolier\", \"Tholomyes\", weight=4)\nG.add_edge(\"Fameuil\", \"Tholomyes\", weight=4)\nG.add_edge(\"Fameuil\", \"Listolier\", weight=4)\nG.add_edge(\"Blacheville\", \"Tholomyes\", weight=4)\nG.add_edge(\"Blacheville\", \"Listolier\", weight=4)\nG.add_edge(\"Blacheville\", \"Fameuil\", weight=4)\nG.add_edge(\"Favourite\", \"Tholomyes\", weight=3)\nG.add_edge(\"Favourite\", \"Listolier\", weight=3)\nG.add_edge(\"Favourite\", \"Fameuil\", weight=3)\nG.add_edge(\"Favourite\", \"Blacheville\", weight=4)\nG.add_edge(\"Dahlia\", \"Tholomyes\", weight=3)\nG.add_edge(\"Dahlia\", \"Listolier\", weight=3)\nG.add_edge(\"Dahlia\", \"Fameuil\", weight=3)\nG.add_edge(\"Dahlia\", \"Blacheville\", weight=3)\nG.add_edge(\"Dahlia\", \"Favourite\", weight=5)\nG.add_edge(\"Zephine\", \"Tholomyes\", weight=3)\nG.add_edge(\"Zephine\", \"Listolier\", weight=3)\nG.add_edge(\"Zephine\", \"Fameuil\", weight=3)\nG.add_edge(\"Zephine\", \"Blacheville\", weight=3)\nG.add_edge(\"Zephine\", \"Favourite\", weight=4)\nG.add_edge(\"Zephine\", \"Dahlia\", weight=4)\nG.add_edge(\"Fantine\", \"Tholomyes\", weight=3)\nG.add_edge(\"Fantine\", \"Listolier\", weight=3)\nG.add_edge(\"Fantine\", \"Fameuil\", weight=3)\nG.add_edge(\"Fantine\", \"Blacheville\", weight=3)\nG.add_edge(\"Fantine\", \"Favourite\", weight=4)\nG.add_edge(\"Fantine\", \"Dahlia\", weight=4)\nG.add_edge(\"Fantine\", \"Zephine\", weight=4)\nG.add_edge(\"Fantine\", \"Marguerite\", weight=2)\nG.add_edge(\"Fantine\", \"Valjean\", weight=9)\nG.add_edge(\"MmeThenardier\", \"Fantine\", weight=2)\nG.add_edge(\"MmeThenardier\", \"Valjean\", weight=7)\nG.add_edge(\"Thenardier\", \"MmeThenardier\", weight=13)\nG.add_edge(\"Thenardier\", \"Fantine\", weight=1)\nG.add_edge(\"Thenardier\", \"Valjean\", weight=12)\nG.add_edge(\"Cosette\", \"MmeThenardier\", weight=4)\nG.add_edge(\"Cosette\", \"Valjean\", weight=31)\nG.add_edge(\"Cosette\", \"Tholomyes\", weight=1)\nG.add_edge(\"Cosette\", \"Thenardier\", weight=1)\nG.add_edge(\"Javert\", \"Valjean\", weight=17)\nG.add_edge(\"Javert\", \"Fantine\", weight=5)\nG.add_edge(\"Javert\", \"Thenardier\", weight=5)\nG.add_edge(\"Javert\", \"MmeThenardier\", weight=1)\nG.add_edge(\"Javert\", \"Cosette\", weight=1)\nG.add_edge(\"Fauchelevent\", \"Valjean\", weight=8)\nG.add_edge(\"Fauchelevent\", \"Javert\", weight=1)\nG.add_edge(\"Bamatabois\", \"Fantine\", weight=1)\nG.add_edge(\"Bamatabois\", \"Javert\", weight=1)\nG.add_edge(\"Bamatabois\", \"Valjean\", weight=2)\nG.add_edge(\"Perpetue\", \"Fantine\", weight=1)\nG.add_edge(\"Simplice\", \"Perpetue\", weight=2)\nG.add_edge(\"Simplice\", \"Valjean\", weight=3)\nG.add_edge(\"Simplice\", \"Fantine\", weight=2)\nG.add_edge(\"Simplice\", \"Javert\", weight=1)\nG.add_edge(\"Scaufflaire\", \"Valjean\", weight=1)\nG.add_edge(\"Woman1\", \"Valjean\", weight=2)\nG.add_edge(\"Woman1\", \"Javert\", weight=1)\nG.add_edge(\"Judge\", \"Valjean\", weight=3)\nG.add_edge(\"Judge\", \"Bamatabois\", weight=2)\nG.add_edge(\"Champmathieu\", \"Valjean\", weight=3)\nG.add_edge(\"Champmathieu\", \"Judge\", weight=3)\nG.add_edge(\"Champmathieu\", \"Bamatabois\", weight=2)\nG.add_edge(\"Brevet\", \"Judge\", weight=2)\nG.add_edge(\"Brevet\", \"Champmathieu\", weight=2)\nG.add_edge(\"Brevet\", \"Valjean\", weight=2)\nG.add_edge(\"Brevet\", \"Bamatabois\", weight=1)\nG.add_edge(\"Chenildieu\", \"Judge\", weight=2)\nG.add_edge(\"Chenildieu\", \"Champmathieu\", weight=2)\nG.add_edge(\"Chenildieu\", \"Brevet\", weight=2)\nG.add_edge(\"Chenildieu\", \"Valjean\", weight=2)\nG.add_edge(\"Chenildieu\", \"Bamatabois\", weight=1)\nG.add_edge(\"Cochepaille\", \"Judge\", weight=2)\nG.add_edge(\"Cochepaille\", \"Champmathieu\", weight=2)\nG.add_edge(\"Cochepaille\", \"Brevet\", weight=2)\nG.add_edge(\"Cochepaille\", \"Chenildieu\", weight=2)\nG.add_edge(\"Cochepaille\", \"Valjean\", weight=2)\nG.add_edge(\"Cochepaille\", \"Bamatabois\", weight=1)\nG.add_edge(\"Pontmercy\", \"Thenardier\", weight=1)\nG.add_edge(\"Boulatruelle\", \"Thenardier\", weight=1)\nG.add_edge(\"Eponine\", \"MmeThenardier\", weight=2)\nG.add_edge(\"Eponine\", \"Thenardier\", weight=3)\nG.add_edge(\"Anzelma\", \"Eponine\", weight=2)\nG.add_edge(\"Anzelma\", \"Thenardier\", weight=2)\nG.add_edge(\"Anzelma\", \"MmeThenardier\", weight=1)\nG.add_edge(\"Woman2\", \"Valjean\", weight=3)\nG.add_edge(\"Woman2\", \"Cosette\", weight=1)\nG.add_edge(\"Woman2\", \"Javert\", weight=1)\nG.add_edge(\"MotherInnocent\", \"Fauchelevent\", weight=3)\nG.add_edge(\"MotherInnocent\", \"Valjean\", weight=1)\nG.add_edge(\"Gribier\", \"Fauchelevent\", weight=2)\nG.add_edge(\"MmeBurgon\", \"Jondrette\", weight=1)\nG.add_edge(\"Gavroche\", \"MmeBurgon\", weight=2)\nG.add_edge(\"Gavroche\", \"Thenardier\", weight=1)\nG.add_edge(\"Gavroche\", \"Javert\", weight=1)\nG.add_edge(\"Gavroche\", \"Valjean\", weight=1)\nG.add_edge(\"Gillenormand\", \"Cosette\", weight=3)\nG.add_edge(\"Gillenormand\", \"Valjean\", weight=2)\nG.add_edge(\"Magnon\", \"Gillenormand\", weight=1)\nG.add_edge(\"Magnon\", \"MmeThenardier\", weight=1)\nG.add_edge(\"MlleGillenormand\", \"Gillenormand\", weight=9)\nG.add_edge(\"MlleGillenormand\", \"Cosette\", weight=2)\nG.add_edge(\"MlleGillenormand\", \"Valjean\", weight=2)\nG.add_edge(\"MmePontmercy\", \"MlleGillenormand\", weight=1)\nG.add_edge(\"MmePontmercy\", \"Pontmercy\", weight=1)\nG.add_edge(\"MlleVaubois\", \"MlleGillenormand\", weight=1)\nG.add_edge(\"LtGillenormand\", \"MlleGillenormand\", weight=2)\nG.add_edge(\"LtGillenormand\", \"Gillenormand\", weight=1)\nG.add_edge(\"LtGillenormand\", \"Cosette\", weight=1)\nG.add_edge(\"Marius\", \"MlleGillenormand\", weight=6)\nG.add_edge(\"Marius\", \"Gillenormand\", weight=12)\nG.add_edge(\"Marius\", \"Pontmercy\", weight=1)\nG.add_edge(\"Marius\", \"LtGillenormand\", weight=1)\nG.add_edge(\"Marius\", \"Cosette\", weight=21)\nG.add_edge(\"Marius\", \"Valjean\", weight=19)\nG.add_edge(\"Marius\", \"Tholomyes\", weight=1)\nG.add_edge(\"Marius\", \"Thenardier\", weight=2)\nG.add_edge(\"Marius\", \"Eponine\", weight=5)\nG.add_edge(\"Marius\", \"Gavroche\", weight=4)\nG.add_edge(\"BaronessT\", \"Gillenormand\", weight=1)\nG.add_edge(\"BaronessT\", \"Marius\", weight=1)\nG.add_edge(\"Mabeuf\", \"Marius\", weight=1)\nG.add_edge(\"Mabeuf\", \"Eponine\", weight=1)\nG.add_edge(\"Mabeuf\", \"Gavroche\", weight=1)\nG.add_edge(\"Enjolras\", \"Marius\", weight=7)\nG.add_edge(\"Enjolras\", \"Gavroche\", weight=7)\nG.add_edge(\"Enjolras\", \"Javert\", weight=6)\nG.add_edge(\"Enjolras\", \"Mabeuf\", weight=1)\nG.add_edge(\"Enjolras\", \"Valjean\", weight=4)\nG.add_edge(\"Combeferre\", \"Enjolras\", weight=15)\nG.add_edge(\"Combeferre\", \"Marius\", weight=5)\nG.add_edge(\"Combeferre\", \"Gavroche\", weight=6)\nG.add_edge(\"Combeferre\", \"Mabeuf\", weight=2)\nG.add_edge(\"Prouvaire\", \"Gavroche\", weight=1)\nG.add_edge(\"Prouvaire\", \"Enjolras\", weight=4)\nG.add_edge(\"Prouvaire\", \"Combeferre\", weight=2)\nG.add_edge(\"Feuilly\", \"Gavroche\", weight=2)\nG.add_edge(\"Feuilly\", \"Enjolras\", weight=6)\nG.add_edge(\"Feuilly\", \"Prouvaire\", weight=2)\nG.add_edge(\"Feuilly\", \"Combeferre\", weight=5)\nG.add_edge(\"Feuilly\", \"Mabeuf\", weight=1)\nG.add_edge(\"Feuilly\", \"Marius\", weight=1)\nG.add_edge(\"Courfeyrac\", \"Marius\", weight=9)\nG.add_edge(\"Courfeyrac\", \"Enjolras\", weight=17)\nG.add_edge(\"Courfeyrac\", \"Combeferre\", weight=13)\nG.add_edge(\"Courfeyrac\", \"Gavroche\", weight=7)\nG.add_edge(\"Courfeyrac\", \"Mabeuf\", weight=2)\nG.add_edge(\"Courfeyrac\", \"Eponine\", weight=1)\nG.add_edge(\"Courfeyrac\", \"Feuilly\", weight=6)\nG.add_edge(\"Courfeyrac\", \"Prouvaire\", weight=3)\nG.add_edge(\"Bahorel\", \"Combeferre\", weight=5)\nG.add_edge(\"Bahorel\", \"Gavroche\", weight=5)\nG.add_edge(\"Bahorel\", \"Courfeyrac\", weight=6)\nG.add_edge(\"Bahorel\", \"Mabeuf\", weight=2)\nG.add_edge(\"Bahorel\", \"Enjolras\", weight=4)\nG.add_edge(\"Bahorel\", \"Feuilly\", weight=3)\nG.add_edge(\"Bahorel\", \"Prouvaire\", weight=2)\nG.add_edge(\"Bahorel\", \"Marius\", weight=1)\nG.add_edge(\"Bossuet\", \"Marius\", weight=5)\nG.add_edge(\"Bossuet\", \"Courfeyrac\", weight=12)\nG.add_edge(\"Bossuet\", \"Gavroche\", weight=5)\nG.add_edge(\"Bossuet\", \"Bahorel\", weight=4)\nG.add_edge(\"Bossuet\", \"Enjolras\", weight=10)\nG.add_edge(\"Bossuet\", \"Feuilly\", weight=6)\nG.add_edge(\"Bossuet\", \"Prouvaire\", weight=2)\nG.add_edge(\"Bossuet\", \"Combeferre\", weight=9)\nG.add_edge(\"Bossuet\", \"Mabeuf\", weight=1)\nG.add_edge(\"Bossuet\", \"Valjean\", weight=1)\nG.add_edge(\"Joly\", \"Bahorel\", weight=5)\nG.add_edge(\"Joly\", \"Bossuet\", weight=7)\nG.add_edge(\"Joly\", \"Gavroche\", weight=3)\nG.add_edge(\"Joly\", \"Courfeyrac\", weight=5)\nG.add_edge(\"Joly\", \"Enjolras\", weight=5)\nG.add_edge(\"Joly\", \"Feuilly\", weight=5)\nG.add_edge(\"Joly\", \"Prouvaire\", weight=2)\nG.add_edge(\"Joly\", \"Combeferre\", weight=5)\nG.add_edge(\"Joly\", \"Mabeuf\", weight=1)\nG.add_edge(\"Joly\", \"Marius\", weight=2)\nG.add_edge(\"Grantaire\", \"Bossuet\", weight=3)\nG.add_edge(\"Grantaire\", \"Enjolras\", weight=3)\nG.add_edge(\"Grantaire\", \"Combeferre\", weight=1)\nG.add_edge(\"Grantaire\", \"Courfeyrac\", weight=2)\nG.add_edge(\"Grantaire\", \"Joly\", weight=2)\nG.add_edge(\"Grantaire\", \"Gavroche\", weight=1)\nG.add_edge(\"Grantaire\", \"Bahorel\", weight=1)\nG.add_edge(\"Grantaire\", \"Feuilly\", weight=1)\nG.add_edge(\"Grantaire\", \"Prouvaire\", weight=1)\nG.add_edge(\"MotherPlutarch\", \"Mabeuf\", weight=3)\nG.add_edge(\"Gueulemer\", \"Thenardier\", weight=5)\nG.add_edge(\"Gueulemer\", \"Valjean\", weight=1)\nG.add_edge(\"Gueulemer\", \"MmeThenardier\", weight=1)\nG.add_edge(\"Gueulemer\", \"Javert\", weight=1)\nG.add_edge(\"Gueulemer\", \"Gavroche\", weight=1)\nG.add_edge(\"Gueulemer\", \"Eponine\", weight=1)\nG.add_edge(\"Babet\", \"Thenardier\", weight=6)\nG.add_edge(\"Babet\", \"Gueulemer\", weight=6)\nG.add_edge(\"Babet\", \"Valjean\", weight=1)\nG.add_edge(\"Babet\", \"MmeThenardier\", weight=1)\nG.add_edge(\"Babet\", \"Javert\", weight=2)\nG.add_edge(\"Babet\", \"Gavroche\", weight=1)\nG.add_edge(\"Babet\", \"Eponine\", weight=1)\nG.add_edge(\"Claquesous\", \"Thenardier\", weight=4)\nG.add_edge(\"Claquesous\", \"Babet\", weight=4)\nG.add_edge(\"Claquesous\", \"Gueulemer\", weight=4)\nG.add_edge(\"Claquesous\", \"Valjean\", weight=1)\nG.add_edge(\"Claquesous\", \"MmeThenardier\", weight=1)\nG.add_edge(\"Claquesous\", \"Javert\", weight=1)\nG.add_edge(\"Claquesous\", \"Eponine\", weight=1)\nG.add_edge(\"Claquesous\", \"Enjolras\", weight=1)\nG.add_edge(\"Montparnasse\", \"Javert\", weight=1)\nG.add_edge(\"Montparnasse\", \"Babet\", weight=2)\nG.add_edge(\"Montparnasse\", \"Gueulemer\", weight=2)\nG.add_edge(\"Montparnasse\", \"Claquesous\", weight=2)\nG.add_edge(\"Montparnasse\", \"Valjean\", weight=1)\nG.add_edge(\"Montparnasse\", \"Gavroche\", weight=1)\nG.add_edge(\"Montparnasse\", \"Eponine\", weight=1)\nG.add_edge(\"Montparnasse\", \"Thenardier\", weight=1)\nG.add_edge(\"Toussaint\", \"Cosette\", weight=2)\nG.add_edge(\"Toussaint\", \"Javert\", weight=1)\nG.add_edge(\"Toussaint\", \"Valjean\", weight=1)\nG.add_edge(\"Child1\", \"Gavroche\", weight=2)\nG.add_edge(\"Child2\", \"Gavroche\", weight=2)\nG.add_edge(\"Child2\", \"Child1\", weight=3)\nG.add_edge(\"Brujon\", \"Babet\", weight=3)\nG.add_edge(\"Brujon\", \"Gueulemer\", weight=3)\nG.add_edge(\"Brujon\", \"Thenardier\", weight=3)\nG.add_edge(\"Brujon\", \"Gavroche\", weight=1)\nG.add_edge(\"Brujon\", \"Eponine\", weight=1)\nG.add_edge(\"Brujon\", \"Claquesous\", weight=1)\nG.add_edge(\"Brujon\", \"Montparnasse\", weight=1)\nG.add_edge(\"MmeHucheloup\", \"Bossuet\", weight=1)\nG.add_edge(\"MmeHucheloup\", \"Joly\", weight=1)\nG.add_edge(\"MmeHucheloup\", \"Grantaire\", weight=1)\nG.add_edge(\"MmeHucheloup\", \"Bahorel\", weight=1)\nG.add_edge(\"MmeHucheloup\", \"Courfeyrac\", weight=1)\nG.add_edge(\"MmeHucheloup\", \"Gavroche\", weight=1)\nG.add_edge(\"MmeHucheloup\", \"Enjolras\", weight=1)\nreturn G", "path": "networkx/networkx/generators/social.py", "commit_date": "2020-08-19 00:00:00", "repo_name": "networkx/networkx", "stars": 13985, "license": "other", "language": "python", "size": 92784}
{"docstring": "\"\"\"Returns Florentine families graph.\n\nReferences\n----------\n.. [1] Ronald L. Breiger and Philippa E. Pattison\n   Cumulated social roles: The duality of persons and their algebras,1\n   Social Networks, Volume 8, Issue 3, September 1986, Pages 215-256\n\"\"\"\n", "func_signal": "def florentine_families_graph():\n", "code": "G = nx.Graph()\nG.add_edge(\"Acciaiuoli\", \"Medici\")\nG.add_edge(\"Castellani\", \"Peruzzi\")\nG.add_edge(\"Castellani\", \"Strozzi\")\nG.add_edge(\"Castellani\", \"Barbadori\")\nG.add_edge(\"Medici\", \"Barbadori\")\nG.add_edge(\"Medici\", \"Ridolfi\")\nG.add_edge(\"Medici\", \"Tornabuoni\")\nG.add_edge(\"Medici\", \"Albizzi\")\nG.add_edge(\"Medici\", \"Salviati\")\nG.add_edge(\"Salviati\", \"Pazzi\")\nG.add_edge(\"Peruzzi\", \"Strozzi\")\nG.add_edge(\"Peruzzi\", \"Bischeri\")\nG.add_edge(\"Strozzi\", \"Ridolfi\")\nG.add_edge(\"Strozzi\", \"Bischeri\")\nG.add_edge(\"Ridolfi\", \"Tornabuoni\")\nG.add_edge(\"Tornabuoni\", \"Guadagni\")\nG.add_edge(\"Albizzi\", \"Ginori\")\nG.add_edge(\"Albizzi\", \"Guadagni\")\nG.add_edge(\"Bischeri\", \"Guadagni\")\nG.add_edge(\"Guadagni\", \"Lamberteschi\")\nreturn G", "path": "networkx/networkx/generators/social.py", "commit_date": "2020-08-19 00:00:00", "repo_name": "networkx/networkx", "stars": 13985, "license": "other", "language": "python", "size": 92784}
{"docstring": "# Greedy one exchange should find the optimal solution for this graph (14)\n", "func_signal": "def test_one_exchange_optimal():\n", "code": "G = nx.Graph()\nG.add_edge(1, 2, weight=3)\nG.add_edge(1, 3, weight=3)\nG.add_edge(1, 4, weight=3)\nG.add_edge(1, 5, weight=3)\nG.add_edge(2, 3, weight=5)\n\ncut_size, (set1, set2) = maxcut.one_exchange(G, weight=\"weight\", seed=5)\n\n_is_valid_cut(G, set1, set2)\n_cut_is_locally_optimal(G, cut_size, set1)\n# check global optimality\nassert cut_size == 14", "path": "networkx/networkx/algorithms/approximation/tests/test_maxcut.py", "commit_date": "2020-12-15 00:00:00", "repo_name": "networkx/networkx", "stars": 13985, "license": "other", "language": "python", "size": 92784}
{"docstring": "\"\"\"Compute a partitioning of the graphs nodes and the corresponding cut value.\n\nUse a greedy one exchange strategy to find a locally maximal cut\nand its value, it works by finding the best node (one that gives\nthe highest gain to the cut value) to add to the current cut\nand repeats this process until no improvement can be made.\n\nParameters\n----------\nG : networkx Graph\n    Graph to find a maximum cut for.\n\ninitial_cut : set\n    Cut to use as a starting point. If not supplied the algorithm\n    starts with an empty cut.\n\nseed : integer, random_state, or None (default)\n    Indicator of random number generation state.\n    See :ref:`Randomness<randomness>`.\n\nweight : object\n    Edge attribute key to use as weight. If not specified, edges\n    have weight one.\n\nReturns\n-------\ncut_value : scalar\n    Value of the maximum cut.\n\npartition : pair of node sets\n    A partitioning of the nodes that defines a maximum cut.\n\"\"\"\n", "func_signal": "def one_exchange(G, initial_cut=None, seed=None, weight=None):\n", "code": "if initial_cut is None:\n    initial_cut = set()\ncut = set(initial_cut)\ncurrent_cut_size = nx.algorithms.cut_size(G, cut, weight=weight)\nwhile True:\n    nodes = list(G.nodes())\n    # Shuffling the nodes ensures random tie-breaks in the following call to max\n    seed.shuffle(nodes)\n    best_node_to_swap = max(\n        nodes,\n        key=lambda v: nx.algorithms.cut_size(\n            G, _swap_node_partition(cut, v), weight=weight\n        ),\n        default=None,\n    )\n    potential_cut = _swap_node_partition(cut, best_node_to_swap)\n    potential_cut_size = nx.algorithms.cut_size(G, potential_cut, weight=weight)\n\n    if potential_cut_size > current_cut_size:\n        cut = potential_cut\n        current_cut_size = potential_cut_size\n    else:\n        break\n\npartition = (cut, G.nodes - cut)\nreturn current_cut_size, partition", "path": "networkx/networkx/algorithms/approximation/maxcut.py", "commit_date": "2020-12-15 00:00:00", "repo_name": "networkx/networkx", "stars": 13985, "license": "other", "language": "python", "size": 92784}
{"docstring": "\"\"\"Returns the degrees of the two node sets in the bipartite graph B.\n\nParameters\n----------\nG : NetworkX graph\n\nnodes: list or container\n  Nodes in one node set of the bipartite graph.\n\nweight : string or None, optional (default=None)\n   The edge attribute that holds the numerical value used as a weight.\n   If None, then each edge has weight 1.\n   The degree is the sum of the edge weights adjacent to the node.\n\nReturns\n-------\n(degX,degY) : tuple of dictionaries\n   The degrees of the two bipartite sets as dictionaries keyed by node.\n\nExamples\n--------\n>>> from networkx.algorithms import bipartite\n>>> G = nx.complete_bipartite_graph(3, 2)\n>>> Y = set([3, 4])\n>>> degX, degY = bipartite.degrees(G, Y)\n>>> dict(degX)\n{0: 2, 1: 2, 2: 2}\n\nNotes\n-----\nThe container of nodes passed as argument must contain all nodes\nin one of the two bipartite node sets to avoid ambiguity in the\ncase of disconnected graphs.\nSee :mod:`bipartite documentation <networkx.algorithms.bipartite>`\nfor further details on how bipartite graphs are handled in NetworkX.\n\nSee Also\n--------\ncolor, density\n\"\"\"\n", "func_signal": "def degrees(B, nodes, weight=None):\n", "code": "bottom = set(nodes)\ntop = set(B) - bottom\nreturn (B.degree(top, weight), B.degree(bottom, weight))", "path": "networkx/networkx/algorithms/bipartite/basic.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "networkx/networkx", "stars": 13985, "license": "other", "language": "python", "size": 92784}
{"docstring": "\"\"\"Progressive widening beam search to find a node.\n\nThe progressive widening beam search involves a repeated beam\nsearch, starting with a small beam width then extending to\nprogressively larger beam widths if the target node is not\nfound. This implementation simply returns the first node found that\nmatches the termination condition.\n\n`G` is a NetworkX graph.\n\n`source` is a node in the graph. The search for the node of interest\nbegins here and extends only to those nodes in the (weakly)\nconnected component of this node.\n\n`value` is a function that returns a real number indicating how good\na potential neighbor node is when deciding which neighbor nodes to\nenqueue in the breadth-first search. Only the best nodes within the\ncurrent beam width will be enqueued at each step.\n\n`condition` is the termination condition for the search. This is a\nfunction that takes a node as input and return a Boolean indicating\nwhether the node is the target. If no node matches the termination\ncondition, this function raises :exc:`NodeNotFound`.\n\n`initial_width` is the starting beam width for the beam search (the\ndefault is one). If no node matching the `condition` is found with\nthis beam width, the beam search is restarted from the `source` node\nwith a beam width that is twice as large (so the beam width\nincreases exponentially). The search terminates after the beam width\nexceeds the number of nodes in the graph.\n\n\"\"\"\n# Check for the special case in which the source node satisfies the\n# termination condition.\n", "func_signal": "def progressive_widening_search(G, source, value, condition, initial_width=1):\n", "code": "if condition(source):\n    return source\n# The largest possible value of `i` in this range yields a width at\n# least the number of nodes in the graph, so the final invocation of\n# `bfs_beam_edges` is equivalent to a plain old breadth-first\n# search. Therefore, all nodes will eventually be visited.\nlog_m = math.ceil(math.log2(len(G)))\nfor i in range(log_m):\n    width = initial_width * pow(2, i)\n    # Since we are always starting from the same source node, this\n    # search may visit the same nodes many times (depending on the\n    # implementation of the `value` function).\n    for u, v in nx.bfs_beam_edges(G, source, value, width):\n        if condition(v):\n            return v\n# At this point, since all nodes have been visited, we know that\n# none of the nodes satisfied the termination condition.\nraise nx.NodeNotFound(\"no node satisfied the termination condition\")", "path": "networkx/examples/algorithms/plot_beam_search.py", "commit_date": "2020-10-24 00:00:00", "repo_name": "networkx/networkx", "stars": 13985, "license": "other", "language": "python", "size": 92784}
{"docstring": "\"\"\"Implementation of the Edmonds-Karp algorithm.\"\"\"\n", "func_signal": "def edmonds_karp_impl(G, s, t, capacity, residual, cutoff):\n", "code": "if s not in G:\n    raise nx.NetworkXError(f\"node {str(s)} not in graph\")\nif t not in G:\n    raise nx.NetworkXError(f\"node {str(t)} not in graph\")\nif s == t:\n    raise nx.NetworkXError(\"source and sink are the same node\")\n\nif residual is None:\n    R = build_residual_network(G, capacity)\nelse:\n    R = residual\n\n# Initialize/reset the residual network.\nfor u in R:\n    for e in R[u].values():\n        e[\"flow\"] = 0\n\nif cutoff is None:\n    cutoff = float(\"inf\")\nR.graph[\"flow_value\"] = edmonds_karp_core(R, s, t, cutoff)\n\nreturn R", "path": "networkx/networkx/algorithms/flow/edmondskarp.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "networkx/networkx", "stars": 13985, "license": "other", "language": "python", "size": 92784}
{"docstring": "\"\"\"Returns True if nodes and G/nodes are a bipartition of G.\n\nParameters\n----------\nG : NetworkX graph\n\nnodes: list or container\n  Check if nodes are a one of a bipartite set.\n\nExamples\n--------\n>>> from networkx.algorithms import bipartite\n>>> G = nx.path_graph(4)\n>>> X = set([1, 3])\n>>> bipartite.is_bipartite_node_set(G, X)\nTrue\n\nNotes\n-----\nFor connected graphs the bipartite sets are unique.  This function handles\ndisconnected graphs.\n\"\"\"\n", "func_signal": "def is_bipartite_node_set(G, nodes):\n", "code": "S = set(nodes)\nfor CC in (G.subgraph(c).copy() for c in connected_components(G)):\n    X, Y = sets(CC)\n    if not (\n        (X.issubset(S) and Y.isdisjoint(S)) or (Y.issubset(S) and X.isdisjoint(S))\n    ):\n        return False\nreturn True", "path": "networkx/networkx/algorithms/bipartite/basic.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "networkx/networkx", "stars": 13985, "license": "other", "language": "python", "size": 92784}
{"docstring": "\"\"\"Returns density of bipartite graph B.\n\nParameters\n----------\nG : NetworkX graph\n\nnodes: list or container\n  Nodes in one node set of the bipartite graph.\n\nReturns\n-------\nd : float\n   The bipartite density\n\nExamples\n--------\n>>> from networkx.algorithms import bipartite\n>>> G = nx.complete_bipartite_graph(3, 2)\n>>> X = set([0, 1, 2])\n>>> bipartite.density(G, X)\n1.0\n>>> Y = set([3, 4])\n>>> bipartite.density(G, Y)\n1.0\n\nNotes\n-----\nThe container of nodes passed as argument must contain all nodes\nin one of the two bipartite node sets to avoid ambiguity in the\ncase of disconnected graphs.\nSee :mod:`bipartite documentation <networkx.algorithms.bipartite>`\nfor further details on how bipartite graphs are handled in NetworkX.\n\nSee Also\n--------\ncolor\n\"\"\"\n", "func_signal": "def density(B, nodes):\n", "code": "n = len(B)\nm = nx.number_of_edges(B)\nnb = len(nodes)\nnt = n - nb\nif m == 0:  # includes cases n==0 and n==1\n    d = 0.0\nelse:\n    if B.is_directed():\n        d = m / (2.0 * float(nb * nt))\n    else:\n        d = m / float(nb * nt)\nreturn d", "path": "networkx/networkx/algorithms/bipartite/basic.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "networkx/networkx", "stars": 13985, "license": "other", "language": "python", "size": 92784}
{"docstring": "\"\"\"Returns a two-coloring of the graph.\n\nRaises an exception if the graph is not bipartite.\n\nParameters\n----------\nG : NetworkX graph\n\nReturns\n-------\ncolor : dictionary\n    A dictionary keyed by node with a 1 or 0 as data for each node color.\n\nRaises\n------\nNetworkXError\n    If the graph is not two-colorable.\n\nExamples\n--------\n>>> from networkx.algorithms import bipartite\n>>> G = nx.path_graph(4)\n>>> c = bipartite.color(G)\n>>> print(c)\n{0: 1, 1: 0, 2: 1, 3: 0}\n\nYou can use this to set a node attribute indicating the biparite set:\n\n>>> nx.set_node_attributes(G, c, \"bipartite\")\n>>> print(G.nodes[0][\"bipartite\"])\n1\n>>> print(G.nodes[1][\"bipartite\"])\n0\n\"\"\"\n", "func_signal": "def color(G):\n", "code": "if G.is_directed():\n    import itertools\n\n    def neighbors(v):\n        return itertools.chain.from_iterable([G.predecessors(v), G.successors(v)])\n\nelse:\n    neighbors = G.neighbors\n\ncolor = {}\nfor n in G:  # handle disconnected graphs\n    if n in color or len(G[n]) == 0:  # skip isolates\n        continue\n    queue = [n]\n    color[n] = 1  # nodes seen with color (1 or 0)\n    while queue:\n        v = queue.pop()\n        c = 1 - color[v]  # opposite color of node v\n        for w in neighbors(v):\n            if w in color:\n                if color[w] == color[v]:\n                    raise nx.NetworkXError(\"Graph is not bipartite.\")\n            else:\n                color[w] = c\n                queue.append(w)\n# color isolates with 0\ncolor.update(dict.fromkeys(nx.isolates(G), 0))\nreturn color", "path": "networkx/networkx/algorithms/bipartite/basic.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "networkx/networkx", "stars": 13985, "license": "other", "language": "python", "size": 92784}
{"docstring": "\"\"\"Compute the non-randomness of graph G.\n\nThe first returned value nr is the sum of non-randomness values of all\nedges within the graph (where the non-randomness of an edge tends to be\nsmall when the two nodes linked by that edge are from two different\ncommunities).\n\nThe second computed value nr_rd is a relative measure that indicates\nto what extent graph G is different from random graphs in terms\nof probability. When it is close to 0, the graph tends to be more\nlikely generated by an Erdos Renyi model.\n\nParameters\n----------\nG : NetworkX graph\n    Graph must be binary, symmetric, connected, and without self-loops.\n\nk : int\n    The number of communities in G.\n    If k is not set, the function will use a default community\n    detection algorithm to set it.\n\nReturns\n-------\nnon-randomness : (float, float) tuple\n    Non-randomness, Relative non-randomness w.r.t.\n    Erdos Renyi random graphs.\n\nExamples\n--------\n>>> G = nx.karate_club_graph()\n>>> nr, nr_rd = nx.non_randomness(G, 2)\n\nNotes\n-----\nThis computes Eq. (4.4) and (4.5) in Ref. [1]_.\n\nReferences\n----------\n.. [1] Xiaowei Ying and Xintao Wu,\n       On Randomness Measures for Social Networks,\n       SIAM International Conference on Data Mining. 2009\n\"\"\"\n", "func_signal": "def non_randomness(G, k=None):\n", "code": "import numpy as np\n\nif not nx.is_connected(G):\n    raise nx.NetworkXException(\"Non connected graph.\")\nif len(list(nx.selfloop_edges(G))) > 0:\n    raise nx.NetworkXError(\"Graph must not contain self-loops\")\n\nif k is None:\n    k = len(tuple(nx.community.label_propagation_communities(G)))\n\n# eq. 4.4\nnr = np.real(np.sum(np.linalg.eigvals(nx.to_numpy_array(G))[:k]))\n\nn = G.number_of_nodes()\nm = G.number_of_edges()\np = (2 * k * m) / (n * (n - k))\n\n# eq. 4.5\nnr_rd = (nr - ((n - 2 * k) * p + k)) / math.sqrt(2 * k * p * (1 - p))\n\nreturn nr, nr_rd", "path": "networkx/networkx/algorithms/non_randomness.py", "commit_date": "2020-12-12 00:00:00", "repo_name": "networkx/networkx", "stars": 13985, "license": "other", "language": "python", "size": 92784}
{"docstring": "\"\"\"Compute a random partitioning of the graph nodes and its cut value.\n\nA partitioning is calculated by observing each node\nand deciding to add it to the partition with probability `p`,\nreturning a random cut and its corresponding value (the\nsum of weights of edges connecting different partitions).\n\nParameters\n----------\nG : NetworkX graph\n\nseed : integer, random_state, or None (default)\n    Indicator of random number generation state.\n    See :ref:`Randomness<randomness>`.\n\np : scalar\n    Probability for each node to be part of the first partition.\n    Should be in [0,1]\n\nweight : object\n    Edge attribute key to use as weight. If not specified, edges\n    have weight one.\n\nReturns\n-------\ncut_size : scalar\n    Value of the minimum cut.\n\npartition : pair of node sets\n    A partitioning of the nodes that defines a minimum cut.\n\"\"\"\n", "func_signal": "def randomized_partitioning(G, seed=None, p=0.5, weight=None):\n", "code": "cut = {node for node in G.nodes() if seed.random() < p}\ncut_size = nx.algorithms.cut_size(G, cut, weight=weight)\npartition = (cut, G.nodes - cut)\nreturn cut_size, partition", "path": "networkx/networkx/algorithms/approximation/maxcut.py", "commit_date": "2020-12-15 00:00:00", "repo_name": "networkx/networkx", "stars": 13985, "license": "other", "language": "python", "size": 92784}
{"docstring": "# create a simple star graph\n", "func_signal": "def test_unweighted_undirected(self):\n", "code": "size = 50\nsg = nx.star_graph(size)\ncover = min_weighted_vertex_cover(sg)\nassert 2 == len(cover)\nassert is_cover(sg, cover)", "path": "networkx/networkx/algorithms/approximation/tests/test_vertex_cover.py", "commit_date": "2020-01-01 00:00:00", "repo_name": "networkx/networkx", "stars": 13985, "license": "other", "language": "python", "size": 92784}
{"docstring": "\"\"\"Implementation of the Edmonds-Karp algorithm.\"\"\"\n", "func_signal": "def edmonds_karp_core(R, s, t, cutoff):\n", "code": "R_nodes = R.nodes\nR_pred = R.pred\nR_succ = R.succ\n\ninf = R.graph[\"inf\"]\n\ndef augment(path):\n    \"\"\"Augment flow along a path from s to t.\"\"\"\n    # Determine the path residual capacity.\n    flow = inf\n    it = iter(path)\n    u = next(it)\n    for v in it:\n        attr = R_succ[u][v]\n        flow = min(flow, attr[\"capacity\"] - attr[\"flow\"])\n        u = v\n    if flow * 2 > inf:\n        raise nx.NetworkXUnbounded(\"Infinite capacity path, flow unbounded above.\")\n    # Augment flow along the path.\n    it = iter(path)\n    u = next(it)\n    for v in it:\n        R_succ[u][v][\"flow\"] += flow\n        R_succ[v][u][\"flow\"] -= flow\n        u = v\n    return flow\n\ndef bidirectional_bfs():\n    \"\"\"Bidirectional breadth-first search for an augmenting path.\"\"\"\n    pred = {s: None}\n    q_s = [s]\n    succ = {t: None}\n    q_t = [t]\n    while True:\n        q = []\n        if len(q_s) <= len(q_t):\n            for u in q_s:\n                for v, attr in R_succ[u].items():\n                    if v not in pred and attr[\"flow\"] < attr[\"capacity\"]:\n                        pred[v] = u\n                        if v in succ:\n                            return v, pred, succ\n                        q.append(v)\n            if not q:\n                return None, None, None\n            q_s = q\n        else:\n            for u in q_t:\n                for v, attr in R_pred[u].items():\n                    if v not in succ and attr[\"flow\"] < attr[\"capacity\"]:\n                        succ[v] = u\n                        if v in pred:\n                            return v, pred, succ\n                        q.append(v)\n            if not q:\n                return None, None, None\n            q_t = q\n\n# Look for shortest augmenting paths using breadth-first search.\nflow_value = 0\nwhile flow_value < cutoff:\n    v, pred, succ = bidirectional_bfs()\n    if pred is None:\n        break\n    path = [v]\n    # Trace a path from s to v.\n    u = v\n    while u != s:\n        u = pred[u]\n        path.append(u)\n    path.reverse()\n    # Trace a path from v to t.\n    u = v\n    while u != t:\n        u = succ[u]\n        path.append(u)\n    flow_value += augment(path)\n\nreturn flow_value", "path": "networkx/networkx/algorithms/flow/edmondskarp.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "networkx/networkx", "stars": 13985, "license": "other", "language": "python", "size": 92784}
{"docstring": "\"\"\"\n@example 'Akemi Sato'\n\"\"\"\n", "func_signal": "def romanized_name_female(self):\n", "code": "pattern = self.random_element(self.romanized_formats_female)\nreturn self.generator.parse(pattern)", "path": "faker/faker/providers/person/ja_JP/__init__.py", "commit_date": "2020-11-19 00:00:00", "repo_name": "joke2k/faker", "stars": 16923, "license": "mit", "language": "python", "size": 9517}
{"docstring": "\"\"\"\n@example 'Akira Sato'\n\"\"\"\n", "func_signal": "def romanized_name_male(self):\n", "code": "pattern = self.random_element(self.romanized_formats_male)\nreturn self.generator.parse(pattern)", "path": "faker/faker/providers/person/ja_JP/__init__.py", "commit_date": "2020-11-19 00:00:00", "repo_name": "joke2k/faker", "stars": 16923, "license": "mit", "language": "python", "size": 9517}
{"docstring": "\"\"\"\nReturns a 13 digits Swiss SSN named AHV (German) or\n                                    AVS (French and Italian)\nSee: http://www.bsv.admin.ch/themen/ahv/00011/02185/\n\"\"\"\n", "func_signal": "def ssn(self):\n", "code": "def _checksum(digits):\n    evensum = sum(digits[:-1:2])\n    oddsum = sum(digits[1::2])\n    return (10 - ((evensum + oddsum * 3) % 10)) % 10\n\ndigits = [7, 5, 6]\n# create an array of first 9 elements initialized randomly\ndigits += self.generator.random.sample(range(10), 9)\n# determine the last digit to make it qualify the test\ndigits.append(_checksum(digits))\n# repeat steps until it does qualify the test\n\ndigits = ''.join([str(d) for d in digits])\nssn = digits[:3] + '.' \\\n                 + digits[3:7] + '.' \\\n                 + digits[7:11] + '.' \\\n                 + digits[11:]\nreturn ssn", "path": "faker/faker/providers/ssn/fr_CH/__init__.py", "commit_date": "2020-08-25 00:00:00", "repo_name": "joke2k/faker", "stars": 16923, "license": "mit", "language": "python", "size": 9517}
{"docstring": "\"\"\"Generate ``None``, ``True``, or ``False``, each with equal probability.\n\n:sample size=15:\n\"\"\"\n", "func_signal": "def null_boolean(self):\n", "code": "return {\n    0: None,\n    1: True,\n    -1: False,\n}[self.generator.random.randint(-1, 1)]", "path": "faker/faker/providers/misc/__init__.py", "commit_date": "2020-12-30 00:00:00", "repo_name": "joke2k/faker", "stars": 16923, "license": "mit", "language": "python", "size": 9517}
{"docstring": "\"\"\"\n:example '791 Crist Parks, Sashabury, IL 86039-9874'\n\"\"\"\n", "func_signal": "def address(self):\n", "code": "pattern = self.random_element(self.address_formats)\nreturn self.generator.parse(pattern)", "path": "faker/faker/providers/address/__init__.py", "commit_date": "2020-01-14 00:00:00", "repo_name": "joke2k/faker", "stars": 16923, "license": "mit", "language": "python", "size": 9517}
{"docstring": "\"\"\"\n:example 'Sashabury'\n\"\"\"\n", "func_signal": "def city(self):\n", "code": "pattern = self.random_element(self.city_formats)\nreturn self.generator.parse(pattern)", "path": "faker/faker/providers/address/__init__.py", "commit_date": "2020-01-14 00:00:00", "repo_name": "joke2k/faker", "stars": 16923, "license": "mit", "language": "python", "size": 9517}
{"docstring": "\"\"\"Generate a license plate.\"\"\"\n", "func_signal": "def license_plate(self):\n", "code": "temp = re.sub(r'\\?',\n              lambda x: self.random_element(ascii_uppercase),\n              self.random_element(self.license_formats))\nreturn self.numerify(temp)", "path": "faker/faker/providers/automotive/__init__.py", "commit_date": "2020-09-22 00:00:00", "repo_name": "joke2k/faker", "stars": 16923, "license": "mit", "language": "python", "size": 9517}
{"docstring": "\"\"\"\n:example '791 Crist Parks'\n\"\"\"\n", "func_signal": "def street_address(self):\n", "code": "pattern = self.random_element(self.street_address_formats)\nreturn self.generator.parse(pattern)", "path": "faker/faker/providers/address/__init__.py", "commit_date": "2020-01-14 00:00:00", "repo_name": "joke2k/faker", "stars": 16923, "license": "mit", "language": "python", "size": 9517}
{"docstring": "\"\"\"\n@example '\u30b5\u30c8\u30a6 \u30a2\u30b1\u30df'\n\"\"\"\n", "func_signal": "def kana_name_female(self):\n", "code": "pattern = self.random_element(self.kana_formats_female)\nreturn self.generator.parse(pattern)", "path": "faker/faker/providers/person/ja_JP/__init__.py", "commit_date": "2020-11-19 00:00:00", "repo_name": "joke2k/faker", "stars": 16923, "license": "mit", "language": "python", "size": 9517}
{"docstring": "'''\n@example 'Chao Bai'\n'''\n", "func_signal": "def romanized_name(self):\n", "code": "pattern = self.random_element(self.romanized_formats)\nreturn self.generator.parse(pattern)", "path": "faker/faker/providers/person/zh_CN/__init__.py", "commit_date": "2020-01-14 00:00:00", "repo_name": "joke2k/faker", "stars": 16923, "license": "mit", "language": "python", "size": 9517}
{"docstring": "\"\"\"\n@example 'Akemi Sato'\n\"\"\"\n", "func_signal": "def romanized_name(self):\n", "code": "pattern = self.random_element(self.romanized_formats)\nreturn self.generator.parse(pattern)", "path": "faker/faker/providers/person/ja_JP/__init__.py", "commit_date": "2020-11-19 00:00:00", "repo_name": "joke2k/faker", "stars": 16923, "license": "mit", "language": "python", "size": 9517}
{"docstring": "\"\"\"Generate a bytes object containing a random valid tar file.\n\nThe number and sizes of files contained inside the resulting archive can be controlled\nusing the following arguments:\n\n- ``uncompressed_size`` - the total size of files before compression, 16 KiB by default\n- ``num_files`` - the number of files archived in resulting zip file, 1 by default\n- ``min_file_size`` - the minimum size of each file before compression, 4 KiB by default\n\nNo compression is used by default, but setting ``compression`` to one of the values listed\nbelow will use the corresponding compression type.\n\n- ``'bzip2'`` or ``'bz2'`` for BZIP2\n- ``'lzma'`` or ``'xz'`` for LZMA\n- ``'gzip'`` or ``'gz'`` for GZIP\n\n:sample: uncompressed_size=256, num_files=4, min_file_size=32\n:sample: uncompressed_size=256, num_files=32, min_file_size=4, compression='bz2'\n\"\"\"\n", "func_signal": "def tar(self, uncompressed_size=65536, num_files=1, min_file_size=4096, compression=None):\n", "code": "if any([\n    not isinstance(num_files, int) or num_files <= 0,\n    not isinstance(min_file_size, int) or min_file_size <= 0,\n    not isinstance(uncompressed_size, int) or uncompressed_size <= 0,\n]):\n    raise ValueError(\n        '`num_files`, `min_file_size`, and `uncompressed_size` must be positive integers',\n    )\nif min_file_size * num_files > uncompressed_size:\n    raise AssertionError(\n        '`uncompressed_size` is smaller than the calculated minimum required size',\n    )\nif compression in ['gzip', 'gz']:\n    mode = 'w:gz'\nelif compression in ['bzip2', 'bz2']:\n    mode = 'w:bz2'\nelif compression in ['lzma', 'xz']:\n    mode = 'w:xz'\nelse:\n    mode = 'w'\n\ntar_buffer = io.BytesIO()\nremaining_size = uncompressed_size\nwith tarfile.open(mode=mode, fileobj=tar_buffer) as tar_handle:\n    for file_number in range(1, num_files + 1):\n        file_buffer = io.BytesIO()\n        filename = self.generator.pystr() + str(file_number)\n\n        max_allowed_size = remaining_size - (num_files - file_number) * min_file_size\n        if file_number < num_files:\n            file_size = self.generator.random.randint(min_file_size, max_allowed_size)\n            remaining_size = remaining_size - file_size\n        else:\n            file_size = remaining_size\n\n        tarinfo = tarfile.TarInfo(name=filename)\n        data = self.generator.binary(file_size)\n        file_buffer.write(data)\n        tarinfo.size = len(file_buffer.getvalue())\n        file_buffer.seek(0)\n        tar_handle.addfile(tarinfo, file_buffer)\n        file_buffer.close()\nreturn tar_buffer.getvalue()", "path": "faker/faker/providers/misc/__init__.py", "commit_date": "2020-12-30 00:00:00", "repo_name": "joke2k/faker", "stars": 16923, "license": "mit", "language": "python", "size": 9517}
{"docstring": "\"\"\"Generate random tab-separated values.\n\nFor more information on the different arguments of this method, please refer to\n:meth:`dsv() <faker.providers.misc.Provider.dsv>` which is used under the hood.\n\n:sample: data_columns=('{{name}}', '{{address}}'), num_rows=10, include_row_ids=False\n:sample: header=('Name', 'Address', 'Favorite Color'),\n        data_columns=('{{name}}', '{{address}}', '{{safe_color_name}}'),\n        num_rows=10, include_row_ids=True\n\"\"\"\n", "func_signal": "def tsv(self, header=None, data_columns=('{{name}}', '{{address}}'), num_rows=10, include_row_ids=False):\n", "code": "return self.dsv(\n    header=header, data_columns=data_columns, num_rows=num_rows,\n    include_row_ids=include_row_ids, delimiter='\\t',\n)", "path": "faker/faker/providers/misc/__init__.py", "commit_date": "2020-12-30 00:00:00", "repo_name": "joke2k/faker", "stars": 16923, "license": "mit", "language": "python", "size": 9517}
{"docstring": "\"\"\"Generate a random MD5 hash.\n\nIf ``raw_output`` is ``False`` (default), a hexadecimal string representation of the MD5 hash\nwill be returned. If ``True``, a ``bytes`` object representation will be returned instead.\n\n:sample: raw_output=False\n:sample: raw_output=True\n\"\"\"\n", "func_signal": "def md5(self, raw_output=False):\n", "code": "res = hashlib.md5(str(self.generator.random.random()).encode())\nif raw_output:\n    return res.digest()\nreturn res.hexdigest()", "path": "faker/faker/providers/misc/__init__.py", "commit_date": "2020-12-30 00:00:00", "repo_name": "joke2k/faker", "stars": 16923, "license": "mit", "language": "python", "size": 9517}
{"docstring": "\"\"\"Generate a random binary blob of ``length`` bytes.\n\n:sample: length=64\n\"\"\"\n", "func_signal": "def binary(self, length=(1 * 1024 * 1024)):\n", "code": "blob = [self.generator.random.randrange(256) for _ in range(length)]\nreturn bytes(blob)", "path": "faker/faker/providers/misc/__init__.py", "commit_date": "2020-12-30 00:00:00", "repo_name": "joke2k/faker", "stars": 16923, "license": "mit", "language": "python", "size": 9517}
{"docstring": "\"\"\"Generate a random SHA256 hash.\n\nIf ``raw_output`` is ``False`` (default), a hexadecimal string representation of the SHA56 hash\nwill be returned. If ``True``, a ``bytes`` object representation will be returned instead.\n\n:sample: raw_output=False\n:sample: raw_output=True\n\"\"\"\n", "func_signal": "def sha256(self, raw_output=False):\n", "code": "res = hashlib.sha256(\n    str(self.generator.random.random()).encode())\nif raw_output:\n    return res.digest()\nreturn res.hexdigest()", "path": "faker/faker/providers/misc/__init__.py", "commit_date": "2020-12-30 00:00:00", "repo_name": "joke2k/faker", "stars": 16923, "license": "mit", "language": "python", "size": 9517}
{"docstring": "'''\n@example 'Chao Bai'\n'''\n", "func_signal": "def romanized_name(self):\n", "code": "pattern = self.random_element(self.romanized_formats)\nreturn self.generator.parse(pattern)", "path": "faker/faker/providers/person/zh_TW/__init__.py", "commit_date": "2020-01-14 00:00:00", "repo_name": "joke2k/faker", "stars": 16923, "license": "mit", "language": "python", "size": 9517}
{"docstring": "\"\"\"\n:example 'John Doe'\n\"\"\"\n", "func_signal": "def name(self):\n", "code": "pattern = self.random_element(self.formats)\nreturn self.generator.parse(pattern)", "path": "faker/faker/providers/person/__init__.py", "commit_date": "2020-08-04 00:00:00", "repo_name": "joke2k/faker", "stars": 16923, "license": "mit", "language": "python", "size": 9517}
{"docstring": "\"\"\"Generate a random UUID4 object and cast it to another type if specified using a callable ``cast_to``.\n\nBy default, ``cast_to`` is set to ``str``.\n\nMay be called with ``cast_to=None`` to return a full-fledged ``UUID``.\n\n:sample:\n:sample: cast_to=None\n\"\"\"\n# Based on http://stackoverflow.com/q/41186818\n", "func_signal": "def uuid4(self, cast_to=str):\n", "code": "generated_uuid = uuid.UUID(int=self.generator.random.getrandbits(128), version=4)\nif cast_to is not None:\n    generated_uuid = cast_to(generated_uuid)\nreturn generated_uuid", "path": "faker/faker/providers/misc/__init__.py", "commit_date": "2020-12-30 00:00:00", "repo_name": "joke2k/faker", "stars": 16923, "license": "mit", "language": "python", "size": 9517}
{"docstring": "\"\"\"Generate a random SHA1 hash.\n\nIf ``raw_output`` is ``False`` (default), a hexadecimal string representation of the SHA1 hash\nwill be returned. If ``True``, a ``bytes`` object representation will be returned instead.\n\n:sample: raw_output=False\n:sample: raw_output=True\n\"\"\"\n", "func_signal": "def sha1(self, raw_output=False):\n", "code": "res = hashlib.sha1(str(self.generator.random.random()).encode())\nif raw_output:\n    return res.digest()\nreturn res.hexdigest()", "path": "faker/faker/providers/misc/__init__.py", "commit_date": "2020-12-30 00:00:00", "repo_name": "joke2k/faker", "stars": 16923, "license": "mit", "language": "python", "size": 9517}
{"docstring": "\"\"\" Return the most recent value for a key.\n\n    :param default: The default value to be returned if the key is not\n           present or the type conversion fails.\n    :param index: An index for the list of available values.\n    :param type: If defined, this callable is used to cast the value\n            into a specific type. Exception are suppressed and result in\n            the default value to be returned.\n\"\"\"\n", "func_signal": "def get(self, key, default=None, index=-1, type=None):\n", "code": "try:\n    val = self.dict[key][index]\n    return type(val) if type else val\nexcept Exception:\n    pass\nreturn default", "path": "sqlmap/thirdparty/bottle/bottle.py", "commit_date": "2020-07-16 00:00:00", "repo_name": "sqlmapproject/sqlmap", "stars": 30110, "license": "other", "language": "python", "size": 84330}
{"docstring": "\"\"\" Translate header field name to CGI/WSGI environ key. \"\"\"\n", "func_signal": "def _ekey(self, key):\n", "code": "key = key.replace('-', '_').upper()\nif key in self.cgikeys:\n    return key\nreturn 'HTTP_' + key", "path": "sqlmap/thirdparty/bottle/bottle.py", "commit_date": "2020-07-16 00:00:00", "repo_name": "sqlmapproject/sqlmap", "stars": 30110, "license": "other", "language": "python", "size": 84330}
{"docstring": "\"\"\" Yield all Plugins affecting this route. \"\"\"\n", "func_signal": "def all_plugins(self):\n", "code": "unique = set()\nfor p in reversed(self.app.plugins + self.plugins):\n    if True in self.skiplist: break\n    name = getattr(p, 'name', False)\n    if name and (name in self.skiplist or name in unique): continue\n    if p in self.skiplist or type(p) in self.skiplist: continue\n    if name: unique.add(name)\n    yield p", "path": "sqlmap/thirdparty/bottle/bottle.py", "commit_date": "2020-07-16 00:00:00", "repo_name": "sqlmapproject/sqlmap", "stars": 30110, "license": "other", "language": "python", "size": 84330}
{"docstring": "\"\"\" An instance of :class:`HeaderDict`, a case-insensitive dict-like\n    view on the response headers. \"\"\"\n", "func_signal": "def headers(self):\n", "code": "hdict = HeaderDict()\nhdict.dict = self._headers\nreturn hdict", "path": "sqlmap/thirdparty/bottle/bottle.py", "commit_date": "2020-07-16 00:00:00", "repo_name": "sqlmapproject/sqlmap", "stars": 30110, "license": "other", "language": "python", "size": 84330}
{"docstring": "\"\"\" Attach a callback to a hook. Three hooks are currently implemented:\n\n    before_request\n        Executed once before each request. The request context is\n        available, but no routing has happened yet.\n    after_request\n        Executed once after each request regardless of its outcome.\n    app_reset\n        Called whenever :meth:`Bottle.reset` is called.\n\"\"\"\n", "func_signal": "def add_hook(self, name, func):\n", "code": "if name in self.__hook_reversed:\n    self._hooks[name].insert(0, func)\nelse:\n    self._hooks[name].append(func)", "path": "sqlmap/thirdparty/bottle/bottle.py", "commit_date": "2020-07-16 00:00:00", "repo_name": "sqlmapproject/sqlmap", "stars": 30110, "license": "other", "language": "python", "size": 84330}
{"docstring": "\"\"\" Render the template using keyword arguments as local variables. \"\"\"\n", "func_signal": "def render(self, *args, **kwargs):\n", "code": "env = {}\nstdout = []\nfor dictarg in args:\n    env.update(dictarg)\nenv.update(kwargs)\nself.execute(stdout, env)\nreturn ''.join(stdout)", "path": "sqlmap/thirdparty/bottle/bottle.py", "commit_date": "2020-07-16 00:00:00", "repo_name": "sqlmapproject/sqlmap", "stars": 30110, "license": "other", "language": "python", "size": 84330}
{"docstring": "\"\"\" Import a module or fetch an object from a module.\n\n    * ``package.module`` returns `module` as a module object.\n    * ``pack.mod:name`` returns the module variable `name` from `pack.mod`.\n    * ``pack.mod:func()`` calls `pack.mod.func()` and returns the result.\n\n    The last form accepts not only function calls, but any type of\n    expression. Keyword arguments passed to this function are available as\n    local variables. Example: ``import_string('re:compile(x)', x='[a-z]')``\n\"\"\"\n", "func_signal": "def load(target, **namespace):\n", "code": "module, target = target.split(\":\", 1) if ':' in target else (target, None)\nif module not in sys.modules: __import__(module)\nif not target: return sys.modules[module]\nif target.isalnum(): return getattr(sys.modules[module], target)\npackage_name = module.split('.')[0]\nnamespace[package_name] = sys.modules[package_name]\nreturn eval('%s.%s' % (module, target), namespace)", "path": "sqlmap/thirdparty/bottle/bottle.py", "commit_date": "2020-07-16 00:00:00", "repo_name": "sqlmapproject/sqlmap", "stars": 30110, "license": "other", "language": "python", "size": 84330}
{"docstring": "\"\"\" Uninstall plugins. Pass an instance to remove a specific plugin, a type\n    object to remove all plugins that match that type, a string to remove\n    all plugins with a matching ``name`` attribute or ``True`` to remove all\n    plugins. Return the list of removed plugins. \"\"\"\n", "func_signal": "def uninstall(self, plugin):\n", "code": "removed, remove = [], plugin\nfor i, plugin in list(enumerate(self.plugins))[::-1]:\n    if remove is True or remove is plugin or remove is type(plugin) \\\n    or getattr(plugin, 'name', True) == remove:\n        removed.append(plugin)\n        del self.plugins[i]\n        if hasattr(plugin, 'close'): plugin.close()\nif removed: self.reset()\nreturn removed", "path": "sqlmap/thirdparty/bottle/bottle.py", "commit_date": "2020-07-16 00:00:00", "repo_name": "sqlmapproject/sqlmap", "stars": 30110, "license": "other", "language": "python", "size": 84330}
{"docstring": "\"\"\" Load values from an ``*.ini`` style config file.\n\n    If the config file contains sections, their names are used as\n    namespaces for the values within. The two special sections\n    ``DEFAULT`` and ``bottle`` refer to the root namespace (no prefix).\n\"\"\"\n", "func_signal": "def load_config(self, filename):\n", "code": "conf = ConfigParser()\nconf.read(filename)\nfor section in conf.sections():\n    for key, value in conf.items(section):\n        if section not in ('DEFAULT', 'bottle'):\n            key = section + '.' + key\n        self[key] = value\nreturn self", "path": "sqlmap/thirdparty/bottle/bottle.py", "commit_date": "2020-07-16 00:00:00", "repo_name": "sqlmapproject/sqlmap", "stars": 30110, "license": "other", "language": "python", "size": 84330}
{"docstring": "\"\"\" Change an environ value and clear all caches that depend on it. \"\"\"\n\n", "func_signal": "def __setitem__(self, key, value):\n", "code": "if self.environ.get('bottle.request.readonly'):\n    raise KeyError('The environ dictionary is read-only.')\n\nself.environ[key] = value\ntodelete = ()\n\nif key == 'wsgi.input':\n    todelete = ('body', 'forms', 'files', 'params', 'post', 'json')\nelif key == 'QUERY_STRING':\n    todelete = ('query', 'params')\nelif key.startswith('HTTP_'):\n    todelete = ('headers', 'cookies')\n\nfor key in todelete:\n    self.environ.pop('bottle.request.' + key, None)", "path": "sqlmap/thirdparty/bottle/bottle.py", "commit_date": "2020-07-16 00:00:00", "repo_name": "sqlmapproject/sqlmap", "stars": 30110, "license": "other", "language": "python", "size": 84330}
{"docstring": "\"\"\" Lookup a config field and return its value, first checking the\n    route.config, then route.app.config.\"\"\"\n", "func_signal": "def get_config(self, key, default=None):\n", "code": "for conf in (self.config, self.app.config):\n    if key in conf: return conf[key]\nreturn default", "path": "sqlmap/thirdparty/bottle/bottle.py", "commit_date": "2020-07-16 00:00:00", "repo_name": "sqlmapproject/sqlmap", "stars": 30110, "license": "other", "language": "python", "size": 84330}
{"docstring": "\"\"\" Load values from a dictionary structure. Nesting can be used to\n    represent namespaces.\n\n    >>> c = ConfigDict()\n    >>> c.load_dict({'some': {'namespace': {'key': 'value'} } })\n    {'some.namespace.key': 'value'}\n\"\"\"\n", "func_signal": "def load_dict(self, source, namespace=''):\n", "code": "for key, value in source.items():\n    if isinstance(key, basestring):\n        nskey = (namespace + '.' + key).strip('.')\n        if isinstance(value, dict):\n            self.load_dict(value, namespace=nskey)\n        else:\n            self[nskey] = value\n    else:\n        raise TypeError('Key has type %r (not a string)' % type(key))\nreturn self", "path": "sqlmap/thirdparty/bottle/bottle.py", "commit_date": "2020-07-16 00:00:00", "repo_name": "sqlmapproject/sqlmap", "stars": 30110, "license": "other", "language": "python", "size": 84330}
{"docstring": "\"\"\" Decorator: Register an output handler for a HTTP error code\"\"\"\n\n", "func_signal": "def error(self, code=500):\n", "code": "def wrapper(handler):\n    self.error_handler[int(code)] = handler\n    return handler\n\nreturn wrapper", "path": "sqlmap/thirdparty/bottle/bottle.py", "commit_date": "2020-07-16 00:00:00", "repo_name": "sqlmapproject/sqlmap", "stars": 30110, "license": "other", "language": "python", "size": 84330}
{"docstring": "\"\"\" Return a callable that relays calls to the current default app. \"\"\"\n\n", "func_signal": "def make_default_app_wrapper(name):\n", "code": "@functools.wraps(getattr(Bottle, name))\ndef wrapper(*a, **ka):\n    return getattr(app(), name)(*a, **ka)\n\nreturn wrapper", "path": "sqlmap/thirdparty/bottle/bottle.py", "commit_date": "2020-07-16 00:00:00", "repo_name": "sqlmapproject/sqlmap", "stars": 30110, "license": "other", "language": "python", "size": 84330}
{"docstring": "\"\"\" The values of :attr:`forms` and :attr:`files` combined into a single\n    :class:`FormsDict`. Values are either strings (form values) or\n    instances of :class:`cgi.FieldStorage` (file uploads).\n\"\"\"\n", "func_signal": "def POST(self):\n", "code": "post = FormsDict()\n# We default to application/x-www-form-urlencoded for everything that\n# is not multipart and take the fast path (also: 3.1 workaround)\nif not self.content_type.startswith('multipart/'):\n    pairs = _parse_qsl(tonat(self._get_body_string(), 'latin1'))\n    for key, value in pairs:\n        post[key] = value\n    return post\n\nsafe_env = {'QUERY_STRING': ''}  # Build a safe environment for cgi\nfor key in ('REQUEST_METHOD', 'CONTENT_TYPE', 'CONTENT_LENGTH'):\n    if key in self.environ: safe_env[key] = self.environ[key]\nargs = dict(fp=self.body, environ=safe_env, keep_blank_values=True)\nif py31:\n    args['fp'] = NCTextIOWrapper(args['fp'],\n                                 encoding='utf8',\n                                 newline='\\n')\nelif py3k:\n    args['encoding'] = 'utf8'\ndata = cgi.FieldStorage(**args)\nself['_cgi.FieldStorage'] = data  #http://bugs.python.org/issue18394\ndata = data.list or []\nfor item in data:\n    if item.filename:\n        post[item.name] = FileUpload(item.file, item.name,\n                                     item.filename, item.headers)\n    else:\n        post[item.name] = item.value\nreturn post", "path": "sqlmap/thirdparty/bottle/bottle.py", "commit_date": "2020-07-16 00:00:00", "repo_name": "sqlmapproject/sqlmap", "stars": 30110, "license": "other", "language": "python", "size": 84330}
{"docstring": "\"\"\" Add a new :class:`Bottle` instance to the stack \"\"\"\n", "func_signal": "def push(self, value=None):\n", "code": "if not isinstance(value, Bottle):\n    value = Bottle()\nself.append(value)\nreturn value", "path": "sqlmap/thirdparty/bottle/bottle.py", "commit_date": "2020-07-16 00:00:00", "repo_name": "sqlmapproject/sqlmap", "stars": 30110, "license": "other", "language": "python", "size": 84330}
{"docstring": "\"\"\" Change the debug level.\nThere is only one debug level supported at the moment.\"\"\"\n", "func_signal": "def debug(mode=True):\n", "code": "global DEBUG\n#if mode: warnings.simplefilter('default')  # neutralizing already set warning filters (e.g. DeprecationWarning inside sqlmapapi.py)\nDEBUG = bool(mode)", "path": "sqlmap/thirdparty/bottle/bottle.py", "commit_date": "2020-07-16 00:00:00", "repo_name": "sqlmapproject/sqlmap", "stars": 30110, "license": "other", "language": "python", "size": 84330}
{"docstring": "\"\"\" Returns a copy with all keys and values de- or recoded to match\n    :attr:`input_encoding`. Some libraries (e.g. WTForms) want a\n    unicode dictionary. \"\"\"\n", "func_signal": "def decode(self, encoding=None):\n", "code": "copy = FormsDict()\nenc = copy.input_encoding = encoding or self.input_encoding\ncopy.recode_unicode = False\nfor key, value in self.allitems():\n    copy.append(self._fix(key, enc), self._fix(value, enc))\nreturn copy", "path": "sqlmap/thirdparty/bottle/bottle.py", "commit_date": "2020-07-16 00:00:00", "repo_name": "sqlmapproject/sqlmap", "stars": 30110, "license": "other", "language": "python", "size": 84330}
{"docstring": "\"\"\" The HTTP request body as a seek-able file-like object. Depending on\n    :attr:`MEMFILE_MAX`, this is either a temporary file or a\n    :class:`io.BytesIO` instance. Accessing this property for the first\n    time reads and replaces the ``wsgi.input`` environ variable.\n    Subsequent accesses just do a `seek(0)` on the file object. \"\"\"\n", "func_signal": "def body(self):\n", "code": "self._body.seek(0)\nreturn self._body", "path": "sqlmap/thirdparty/bottle/bottle.py", "commit_date": "2020-07-16 00:00:00", "repo_name": "sqlmapproject/sqlmap", "stars": 30110, "license": "other", "language": "python", "size": 84330}
{"docstring": "\"\"\" The client IP as a string. Note that this information can be forged\n    by malicious clients. \"\"\"\n", "func_signal": "def remote_addr(self):\n", "code": "route = self.remote_route\nreturn route[0] if route else None", "path": "sqlmap/thirdparty/bottle/bottle.py", "commit_date": "2020-07-16 00:00:00", "repo_name": "sqlmapproject/sqlmap", "stars": 30110, "license": "other", "language": "python", "size": 84330}
{"docstring": "\"\"\"\nhttps://github.com/httpie/httpie/issues/242\n\n\"\"\"\n", "func_signal": "def test_only_username_in_url(url):\n", "code": "args = httpie.cli.definition.parser.parse_args(args=[url], env=MockEnvironment())\nassert args.auth\nassert args.auth.username == 'username'\nassert args.auth.password == ''", "path": "cli/tests/test_auth.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "httpie/cli", "stars": 31331, "license": "bsd-3-clause", "language": "python", "size": 6885}
{"docstring": "\"\"\"\nUse keyword arguments to overwrite\nany of the class attributes for this instance.\n\n\"\"\"\n", "func_signal": "def __init__(self, devnull=None, **kwargs):\n", "code": "assert all(hasattr(type(self), attr) for attr in kwargs.keys())\nself.__dict__.update(**kwargs)\n\n# The original STDERR unaffected by --quiet\u2019ing.\nself._orig_stderr = self.stderr\nself._devnull = devnull\n\n# Keyword arguments > stream.encoding > default utf8\nif self.stdin and self.stdin_encoding is None:\n    self.stdin_encoding = getattr(\n        self.stdin, 'encoding', None) or 'utf8'\nif self.stdout_encoding is None:\n    actual_stdout = self.stdout\n    if is_windows:\n        # noinspection PyUnresolvedReferences\n        from colorama import AnsiToWin32\n        if isinstance(self.stdout, AnsiToWin32):\n            # noinspection PyUnresolvedReferences\n            actual_stdout = self.stdout.wrapped\n    self.stdout_encoding = getattr(\n        actual_stdout, 'encoding', None) or 'utf8'", "path": "cli/httpie/context.py", "commit_date": "2020-08-15 00:00:00", "repo_name": "httpie/cli", "stars": 31331, "license": "bsd-3-clause", "language": "python", "size": 6885}
{"docstring": "\"\"\"\n:param env: an class:`Environment` instance\n:param kwargs: additional keyword argument that some\n               formatters might require.\n\n\"\"\"\n", "func_signal": "def __init__(self, **kwargs):\n", "code": "self.enabled = True\nself.kwargs = kwargs\nself.format_options = kwargs['format_options']", "path": "cli/httpie/plugins/base.py", "commit_date": "2020-06-16 00:00:00", "repo_name": "httpie/cli", "stars": 31331, "license": "bsd-3-clause", "language": "python", "size": 6885}
{"docstring": "\"\"\"For every `--no-OPTION` in `no_options`, set `args.OPTION` to\nits default value. This allows for un-setting of options, e.g.,\nspecified in config.\n\n\"\"\"\n", "func_signal": "def _apply_no_options(self, no_options):\n", "code": "invalid = []\n\nfor option in no_options:\n    if not option.startswith('--no-'):\n        invalid.append(option)\n        continue\n\n    # --no-option => --option\n    inverted = '--' + option[5:]\n    for action in self._actions:\n        if inverted in action.option_strings:\n            setattr(self.args, action.dest, action.default)\n            break\n    else:\n        invalid.append(option)\n\nif invalid:\n    msg = 'unrecognized arguments: %s'\n    self.error(msg % ' '.join(invalid))", "path": "cli/httpie/cli/argparser.py", "commit_date": "2020-09-28 00:00:00", "repo_name": "httpie/cli", "stars": 31331, "license": "bsd-3-clause", "language": "python", "size": 6885}
{"docstring": "# This one gets handled by requests (no --auth, --auth-type present),\n# that\u2019s why we patch inside `requests.sessions`.\n", "func_signal": "def test_netrc(httpbin_both):\n", "code": "with mock.patch('requests.sessions.get_netrc_auth') as get_netrc_auth:\n    get_netrc_auth.return_value = ('httpie', 'password')\n    r = http(httpbin_both + '/basic-auth/httpie/password')\n    assert get_netrc_auth.call_count == 1\n    assert HTTP_OK in r", "path": "cli/tests/test_auth.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "httpie/cli", "stars": 31331, "license": "bsd-3-clause", "language": "python", "size": 6885}
{"docstring": "# Sneak in our stderr/stdout.\n", "func_signal": "def _print_message(self, message, file=None):\n", "code": "file = {\n    sys.stdout: self.env.stdout,\n    sys.stderr: self.env.stderr,\n    None: self.env.stderr\n}.get(file, file)\nif not hasattr(file, 'buffer') and isinstance(message, str):\n    message = message.encode(self.env.stdout_encoding)\nsuper()._print_message(message, file)", "path": "cli/httpie/cli/argparser.py", "commit_date": "2020-09-28 00:00:00", "repo_name": "httpie/cli", "stars": 31331, "license": "bsd-3-clause", "language": "python", "size": 6885}
{"docstring": "\"\"\"Test that --stream works with prettified redirected output.\"\"\"\n", "func_signal": "def test_pretty_redirected_stream(httpbin):\n", "code": "env = MockEnvironment(\n    colors=256,\n    stdin=StdinBytesIO(BIN_FILE_PATH.read_bytes()),\n    stdin_isatty=False,\n    stdout_isatty=False,\n)\nr = http('--verbose', '--pretty=all', '--stream', 'GET',\n         httpbin.url + '/get', env=env)\nassert BINARY_SUPPRESSED_NOTICE.decode() in r", "path": "cli/tests/test_stream.py", "commit_date": "2020-09-28 00:00:00", "repo_name": "httpie/cli", "stars": 31331, "license": "bsd-3-clause", "language": "python", "size": 6885}
{"docstring": "\"\"\"\nParse `args.request_items` into `args.headers`, `args.data`,\n`args.params`, and `args.files`.\n\n\"\"\"\n", "func_signal": "def _parse_items(self):\n", "code": "try:\n    request_items = RequestItems.from_args(\n        request_item_args=self.args.request_items,\n        as_form=self.args.form,\n    )\nexcept ParseError as e:\n    if self.args.traceback:\n        raise\n    self.error(e.args[0])\nelse:\n    self.args.headers = request_items.headers\n    self.args.data = request_items.data\n    self.args.files = request_items.files\n    self.args.params = request_items.params\n    self.args.multipart_data = request_items.multipart_data\n\nif self.args.files and not self.args.form:\n    # `http url @/path/to/file`\n    file_fields = list(self.args.files.keys())\n    if file_fields != ['']:\n        self.error(\n            'Invalid file fields (perhaps you meant --form?): %s'\n            % ','.join(file_fields))\n\n    fn, fd, ct = self.args.files['']\n    self.args.files = {}\n\n    self._body_from_file(fd)\n\n    if 'Content-Type' not in self.args.headers:\n        content_type = get_content_type(fn)\n        if content_type:\n            self.args.headers['Content-Type'] = content_type", "path": "cli/httpie/cli/argparser.py", "commit_date": "2020-09-28 00:00:00", "repo_name": "httpie/cli", "stars": 31331, "license": "bsd-3-clause", "language": "python", "size": 6885}
{"docstring": "\"\"\"\nModify `env.stdout` and `env.stdout_isatty` based on args, if needed.\n\n\"\"\"\n\n", "func_signal": "def _setup_standard_streams(self):\n", "code": "self.args.output_file_specified = bool(self.args.output_file)\nif self.args.download:\n    # FIXME: Come up with a cleaner solution.\n    if not self.args.output_file and not self.env.stdout_isatty:\n        # Use stdout as the download output file.\n        self.args.output_file = self.env.stdout\n    # With `--download`, we write everything that would normally go to\n    # `stdout` to `stderr` instead. Let's replace the stream so that\n    # we don't have to use many `if`s throughout the codebase.\n    # The response body will be treated separately.\n    self.env.stdout = self.env.stderr\n    self.env.stdout_isatty = self.env.stderr_isatty\n\nelif self.args.output_file:\n    # When not `--download`ing, then `--output` simply replaces\n    # `stdout`. The file is opened for appending, which isn't what\n    # we want in this case.\n    self.args.output_file.seek(0)\n    try:\n        self.args.output_file.truncate()\n    except IOError as e:\n        if e.errno == errno.EINVAL:\n            # E.g. /dev/null on Linux.\n            pass\n        else:\n            raise\n    self.env.stdout = self.args.output_file\n    self.env.stdout_isatty = False\n\nif self.args.quiet:\n    self.env.stderr = self.env.devnull\n    if not (self.args.output_file_specified and not self.args.download):\n        self.env.stdout = self.env.devnull", "path": "cli/httpie/cli/argparser.py", "commit_date": "2020-09-28 00:00:00", "repo_name": "httpie/cli", "stars": 31331, "license": "bsd-3-clause", "language": "python", "size": 6885}
{"docstring": "# TODO: refactor & simplify this method.\n", "func_signal": "def _process_auth(self):\n", "code": "self.args.auth_plugin = None\ndefault_auth_plugin = plugin_manager.get_auth_plugins()[0]\nauth_type_set = self.args.auth_type is not None\nurl = urlsplit(self.args.url)\n\nif self.args.auth is None and not auth_type_set:\n    if url.username is not None:\n        # Handle http://username:password@hostname/\n        username = url.username\n        password = url.password or ''\n        self.args.auth = AuthCredentials(\n            key=username,\n            value=password,\n            sep=SEPARATOR_CREDENTIALS,\n            orig=SEPARATOR_CREDENTIALS.join([username, password])\n        )\n\nif self.args.auth is not None or auth_type_set:\n    if not self.args.auth_type:\n        self.args.auth_type = default_auth_plugin.auth_type\n    plugin = plugin_manager.get_auth_plugin(self.args.auth_type)()\n\n    if (not self.args.ignore_netrc\n            and self.args.auth is None\n            and plugin.netrc_parse):\n        # Only host needed, so it\u2019s OK URL not finalized.\n        netrc_credentials = get_netrc_auth(self.args.url)\n        if netrc_credentials:\n            self.args.auth = AuthCredentials(\n                key=netrc_credentials[0],\n                value=netrc_credentials[1],\n                sep=SEPARATOR_CREDENTIALS,\n                orig=SEPARATOR_CREDENTIALS.join(netrc_credentials)\n            )\n\n    if plugin.auth_require and self.args.auth is None:\n        self.error('--auth required')\n\n    plugin.raw_auth = self.args.auth\n    self.args.auth_plugin = plugin\n    already_parsed = isinstance(self.args.auth, AuthCredentials)\n\n    if self.args.auth is None or not plugin.auth_parse:\n        self.args.auth = plugin.get_auth()\n    else:\n        if already_parsed:\n            # from the URL\n            credentials = self.args.auth\n        else:\n            credentials = parse_auth(self.args.auth)\n\n        if (not credentials.has_password()\n                and plugin.prompt_password):\n            if self.args.ignore_stdin:\n                # Non-tty stdin read by now\n                self.error(\n                    'Unable to prompt for passwords because'\n                    ' --ignore-stdin is set.'\n                )\n            credentials.prompt_password(url.netloc)\n        self.args.auth = plugin.get_auth(\n            username=credentials.key,\n            password=credentials.value,\n        )\nif not self.args.auth and self.args.ignore_netrc:\n    # Set a no-op auth to force requests to ignore .netrc\n    # <https://github.com/psf/requests/issues/2773#issuecomment-174312831>\n    self.args.auth = ExplicitNullAuth()", "path": "cli/httpie/cli/argparser.py", "commit_date": "2020-09-28 00:00:00", "repo_name": "httpie/cli", "stars": 31331, "license": "bsd-3-clause", "language": "python", "size": 6885}
{"docstring": "\"\"\"Parse credentials from `s`.\n\n(\"username\" or \"username:password\").\n\n\"\"\"\n", "func_signal": "def __call__(self, s):\n", "code": "try:\n    return super().__call__(s)\nexcept argparse.ArgumentTypeError:\n    # No password provided, will prompt for it later.\n    return self.key_value_class(\n        key=s,\n        value=None,\n        sep=SEPARATOR_CREDENTIALS,\n        orig=s\n    )", "path": "cli/httpie/cli/argtypes.py", "commit_date": "2020-09-28 00:00:00", "repo_name": "httpie/cli", "stars": 31331, "license": "bsd-3-clause", "language": "python", "size": 6885}
{"docstring": "\"\"\"Test that --stream works with non-prettified\nredirected terminal output.\"\"\"\n", "func_signal": "def test_encoded_stream(httpbin):\n", "code": "env = MockEnvironment(\n    stdin=StdinBytesIO(BIN_FILE_PATH.read_bytes()),\n    stdin_isatty=False,\n)\nr = http('--pretty=none', '--stream', '--verbose', 'GET',\n         httpbin.url + '/get', env=env)\nassert BINARY_SUPPRESSED_NOTICE.decode() in r", "path": "cli/tests/test_stream.py", "commit_date": "2020-09-28 00:00:00", "repo_name": "httpie/cli", "stars": 31331, "license": "bsd-3-clause", "language": "python", "size": 6885}
{"docstring": "\"\"\"Set `args.method` if not specified to either POST or GET\nbased on whether the request has data or not.\n\n\"\"\"\n", "func_signal": "def _guess_method(self):\n", "code": "if self.args.method is None:\n    # Invoked as `http URL'.\n    assert not self.args.request_items\n    if self.has_stdin_data:\n        self.args.method = HTTP_POST\n    else:\n        self.args.method = HTTP_GET\n\n# FIXME: False positive, e.g., \"localhost\" matches but is a valid URL.\nelif not re.match('^[a-zA-Z]+$', self.args.method):\n    # Invoked as `http URL item+'. The URL is now in `args.method`\n    # and the first ITEM is now incorrectly in `args.url`.\n    try:\n        # Parse the URL as an ITEM and store it as the first ITEM arg.\n        self.args.request_items.insert(0, KeyValueArgType(\n            *SEPARATOR_GROUP_ALL_ITEMS).__call__(self.args.url))\n\n    except argparse.ArgumentTypeError as e:\n        if self.args.traceback:\n            raise\n        self.error(e.args[0])\n\n    else:\n        # Set the URL correctly\n        self.args.url = self.args.method\n        # Infer the method\n        has_data = (\n            self.has_stdin_data\n            or any(\n                item.sep in SEPARATOR_GROUP_DATA_ITEMS\n                for item in self.args.request_items)\n        )\n        self.args.method = HTTP_POST if has_data else HTTP_GET", "path": "cli/httpie/cli/argparser.py", "commit_date": "2020-09-28 00:00:00", "repo_name": "httpie/cli", "stars": 31331, "license": "bsd-3-clause", "language": "python", "size": 6885}
{"docstring": "\"\"\"Test that --stream works with non-prettified\nredirected terminal output.\"\"\"\n", "func_signal": "def test_redirected_stream(httpbin):\n", "code": "env = MockEnvironment(\n    stdout_isatty=False,\n    stdin_isatty=False,\n    stdin=StdinBytesIO(BIN_FILE_PATH.read_bytes()),\n)\nr = http('--pretty=none', '--stream', '--verbose', 'GET',\n         httpbin.url + '/get', env=env)\nassert BIN_FILE_CONTENT in r", "path": "cli/tests/test_stream.py", "commit_date": "2020-09-28 00:00:00", "repo_name": "httpie/cli", "stars": 31331, "license": "bsd-3-clause", "language": "python", "size": 6885}
{"docstring": "\"\"\"When credentials are passed in URL and via -a at the same time,\n then the ones from -a are used.\"\"\"\n", "func_signal": "def test_credentials_in_url_auth_flag_has_priority(httpbin_both):\n", "code": "url = add_auth(httpbin_both.url + '/basic-auth/user/password',\n               auth='user:wrong')\nr = http('--auth=user:password', 'GET', url)\nassert HTTP_OK in r\nassert r.json == {'authenticated': True, 'user': 'user'}", "path": "cli/tests/test_auth.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "httpie/cli", "stars": 31331, "license": "bsd-3-clause", "language": "python", "size": 6885}
{"docstring": "\"\"\"Return an iterator over `self.msg`.\"\"\"\n", "func_signal": "def __iter__(self) -> Iterable[bytes]:\n", "code": "if self.with_headers:\n    yield self.get_headers()\n    yield b'\\r\\n\\r\\n'\n\nif self.with_body:\n    try:\n        for chunk in self.iter_body():\n            yield chunk\n            if self.on_body_chunk_downloaded:\n                self.on_body_chunk_downloaded(chunk)\n    except DataSuppressedError as e:\n        if self.with_headers:\n            yield b'\\n'\n        yield e.message", "path": "cli/httpie/output/streams.py", "commit_date": "2020-09-28 00:00:00", "repo_name": "httpie/cli", "stars": 31331, "license": "bsd-3-clause", "language": "python", "size": 6885}
{"docstring": "# A smaller indent for args help.\n", "func_signal": "def __init__(self, max_help_position=6, *args, **kwargs):\n", "code": "kwargs['max_help_position'] = max_help_position\nsuper().__init__(*args, **kwargs)", "path": "cli/httpie/cli/argparser.py", "commit_date": "2020-09-28 00:00:00", "repo_name": "httpie/cli", "stars": 31331, "license": "bsd-3-clause", "language": "python", "size": 6885}
{"docstring": "\"\"\"There can only be one source of request data.\n\nBytes are always read.\n\n\"\"\"\n", "func_signal": "def _body_from_file(self, fd):\n", "code": "if self.args.data or self.args.files:\n    self.error('Request body (from stdin or a file) and request '\n               'data (key=value) cannot be mixed. Pass '\n               '--ignore-stdin to let key/value take priority. '\n               'See https://httpie.org/doc#scripting for details.')\nself.args.data = getattr(fd, 'buffer', fd)", "path": "cli/httpie/cli/argparser.py", "commit_date": "2020-09-28 00:00:00", "repo_name": "httpie/cli", "stars": 31331, "license": "bsd-3-clause", "language": "python", "size": 6885}
{"docstring": "# Read the whole body before prettifying it,\n# but bail out immediately if the body is binary.\n", "func_signal": "def iter_body(self) -> Iterable[bytes]:\n", "code": "converter = None\nbody = bytearray()\n\nfor chunk in self.msg.iter_body(self.CHUNK_SIZE):\n    if not converter and b'\\0' in chunk:\n        converter = self.conversion.get_converter(self.mime)\n        if not converter:\n            raise BinarySuppressedError()\n    body.extend(chunk)\n\nif converter:\n    self.mime, body = converter.convert(body)\n\nyield self.process_body(body)", "path": "cli/httpie/output/streams.py", "commit_date": "2020-09-28 00:00:00", "repo_name": "httpie/cli", "stars": 31331, "license": "bsd-3-clause", "language": "python", "size": 6885}
{"docstring": "\"\"\"Apply defaults to output options, or validate the provided ones.\n\nThe default output options are stdout-type-sensitive.\n\n\"\"\"\n\n", "func_signal": "def _process_output_options(self):\n", "code": "def check_options(value, option):\n    unknown = set(value) - OUTPUT_OPTIONS\n    if unknown:\n        self.error('Unknown output options: {0}={1}'.format(\n            option,\n            ','.join(unknown)\n        ))\n\nif self.args.verbose:\n    self.args.all = True\n\nif self.args.output_options is None:\n    if self.args.verbose:\n        self.args.output_options = ''.join(OUTPUT_OPTIONS)\n    elif self.args.offline:\n        self.args.output_options = OUTPUT_OPTIONS_DEFAULT_OFFLINE\n    elif not self.env.stdout_isatty:\n        self.args.output_options = OUTPUT_OPTIONS_DEFAULT_STDOUT_REDIRECTED\n    else:\n        self.args.output_options = OUTPUT_OPTIONS_DEFAULT\n\nif self.args.output_options_history is None:\n    self.args.output_options_history = self.args.output_options\n\ncheck_options(self.args.output_options, '--print')\ncheck_options(self.args.output_options_history, '--history-print')\n\nif self.args.download and OUT_RESP_BODY in self.args.output_options:\n    # Response body is always downloaded with --download and it goes\n    # through a different routine, so we remove it.\n    self.args.output_options = str(\n        set(self.args.output_options) - set(OUT_RESP_BODY))", "path": "cli/httpie/cli/argparser.py", "commit_date": "2020-09-28 00:00:00", "repo_name": "httpie/cli", "stars": 31331, "license": "bsd-3-clause", "language": "python", "size": 6885}
{"docstring": "'''Deep copy the spec\n\n:param dict copied:\n\tInternal dictionary used for storing already copied values. This \n\tparameter should not be used.\n\n:return: New :py:class:`Spec` object that is a deep copy of ``self``.\n'''\n", "func_signal": "def copy(self, copied=None):\n", "code": "copied = copied or {}\ntry:\n\treturn copied[id(self)]\nexcept KeyError:\n\tinstance = self.__class__()\n\tcopied[id(self)] = instance\n\treturn self.__class__()._update(self.__dict__, copied)", "path": "powerline/powerline/lint/spec.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "powerline/powerline", "stars": 14118, "license": "other", "language": "python", "size": 4261}
{"docstring": "'''Describe value that is an identifier like ``foo:bar`` or ``foo``\n\n:param function msg_func:\n\tFunction that should accept checked value and return message that \n\tdescribes the problem with this value. Default value will emit \n\tsomething like \u201cString \"xyz\" is not an \u2026 identifier\u201d.\n\n:return: self.\n'''\n", "func_signal": "def ident(self, msg_func=None):\n", "code": "msg_func = (\n\tmsg_func\n\tor (lambda value: 'String \"{0}\" is not an alphanumeric/underscore colon-separated identifier'.format(value))\n)\nreturn self.re('^\\w+(?::\\w+)?$', msg_func)", "path": "powerline/powerline/lint/spec.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "powerline/powerline", "stars": 14118, "license": "other", "language": "python", "size": 4261}
{"docstring": "'''Check that each value in the list matches given specification\n\n:param function item_func:\n\tCallable like ``func`` from :py:meth:`Spec.check_func`. Unlike \n\t``func`` this callable is called for each value in the list and may \n\tbe a :py:class:`Spec` object index.\n:param func msg_func:\n\tCallable like ``msg_func`` from :py:meth:`Spec.check_func`. Should \n\taccept one problematic item and is not used for :py:class:`Spec` \n\tobject indices in ``item_func`` method.\n\n:return: proceed, hadproblem.\n'''\n", "func_signal": "def check_list(self, value, context_mark, data, context, echoerr, item_func, msg_func):\n", "code": "havemarks(value)\ni = 0\nhadproblem = False\nfor item in value:\n\thavemarks(item)\n\tif isinstance(item_func, int):\n\t\tspec = self.specs[item_func]\n\t\tproceed, fhadproblem = spec.match(\n\t\t\titem,\n\t\t\tvalue.mark,\n\t\t\tdata,\n\t\t\tcontext.enter_item('list item ' + unicode(i), item),\n\t\t\techoerr\n\t\t)\n\telse:\n\t\tproceed, echo, fhadproblem = item_func(item, data, context, echoerr)\n\t\tif echo and fhadproblem:\n\t\t\techoerr(context=self.cmsg.format(key=context.key + '/list item ' + unicode(i)),\n\t\t\t        context_mark=value.mark,\n\t\t\t        problem=msg_func(item),\n\t\t\t        problem_mark=item.mark)\n\tif fhadproblem:\n\t\thadproblem = True\n\tif not proceed:\n\t\treturn proceed, hadproblem\n\ti += 1\nreturn True, hadproblem", "path": "powerline/powerline/lint/spec.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "powerline/powerline", "stars": 14118, "license": "other", "language": "python", "size": 4261}
{"docstring": "'''Mark value as optional\n\nOnly useful for key specs in :py:meth:`Spec.__init__` and \n:py:meth:`Spec.update` and some last supplied to :py:meth:`Spec.tuple`.\n\n:return: self.\n'''\n", "func_signal": "def optional(self):\n", "code": "self.isoptional = True\nreturn self", "path": "powerline/powerline/lint/spec.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "powerline/powerline", "stars": 14118, "license": "other", "language": "python", "size": 4261}
{"docstring": "'''Describe list with any number of elements, each matching given spec\n\n:param item_func:\n\t:py:class:`Spec` instance or a callable. Check out \n\t:py:meth:`Spec.check_list` documentation for more details. Note that \n\tin :py:meth:`Spec.check_list` description :py:class:`Spec` instance \n\tis replaced with its index in ``self.specs``.\n:param function msg_func:\n\tFunction that should accept checked value and return message that \n\tdescribes the problem with this value. Default value will emit just \n\t\u201cfailed check\u201d, which is rather indescriptive.\n\n:return: self.\n'''\n", "func_signal": "def list(self, item_func, msg_func=None):\n", "code": "self.type(list)\nif isinstance(item_func, Spec):\n\tself.specs.append(item_func)\n\titem_func = len(self.specs) - 1\nself.checks.append(('check_list', item_func, msg_func or (lambda item: 'failed check')))\nreturn self", "path": "powerline/powerline/lint/spec.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "powerline/powerline", "stars": 14118, "license": "other", "language": "python", "size": 4261}
{"docstring": "'''Describe additional keys that may be present in given JSON value\n\nIf called with some keyword arguments implies that described value is \na dictionary. If called without keyword parameters it is no-op.\n\n:return: self.\n'''\n", "func_signal": "def update(self, **keys):\n", "code": "for k, v in keys.items():\n\tself.keys[k] = len(self.specs)\n\tself.specs.append(v)\nif self.keys and not self.did_type:\n\tself.type(dict)\n\tself.did_type = True\nreturn self", "path": "powerline/powerline/lint/spec.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "powerline/powerline", "stars": 14118, "license": "other", "language": "python", "size": 4261}
{"docstring": "'''Describe value that is checked by the given function\n\nCheck out :py:meth:`Spec.check_func` documentation for more details.\n'''\n", "func_signal": "def func(self, func, msg_func=None):\n", "code": "self.checks.append(('check_func', func, msg_func or (lambda value: 'failed check')))\nreturn self", "path": "powerline/powerline/lint/spec.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "powerline/powerline", "stars": 14118, "license": "other", "language": "python", "size": 4261}
{"docstring": "'''Describe value that has one of the types given in arguments\n\n:param args:\n\tList of accepted types. Since :py:class:`Spec` is supposed to \n\tdescribe JSON values only ``dict``, ``list``, ``unicode``, ``bool``, \n\t``float`` and ``NoneType`` types make any sense.\n\n:return: self.\n'''\n", "func_signal": "def type(self, *args):\n", "code": "self.checks.append(('check_type', args))\nreturn self", "path": "powerline/powerline/lint/spec.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "powerline/powerline", "stars": 14118, "license": "other", "language": "python", "size": 4261}
{"docstring": "'''Describe value that is equal to one of the value in the collection\n\n:param set collection:\n\tA collection of possible values.\n:param function msg_func:\n\tFunction that should accept checked value and return message that \n\tdescribes the problem with this value. Default value will emit \n\tsomething like \u201c\"xyz\" must be one of {'abc', 'def', 'ghi'}\u201d.\n\n:return: self.\n'''\n", "func_signal": "def oneof(self, collection, msg_func=None):\n", "code": "msg_func = msg_func or (lambda value: '\"{0}\" must be one of {1!r}'.format(value, list(collection)))\nself.checks.append((\n\t'check_func',\n\t(lambda value, *args: (True, True, value not in collection)),\n\tmsg_func\n))\nreturn self", "path": "powerline/powerline/lint/spec.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "powerline/powerline", "stars": 14118, "license": "other", "language": "python", "size": 4261}
{"docstring": "'''Check that given value matches given type(s)\n\n:param tuple types:\n\tList of accepted types. Since :py:class:`Spec` is supposed to \n\tdescribe JSON values only ``dict``, ``list``, ``unicode``, ``bool``, \n\t``float`` and ``NoneType`` types make any sense.\n\n:return: proceed, hadproblem.\n'''\n", "func_signal": "def check_type(self, value, context_mark, data, context, echoerr, types):\n", "code": "havemarks(value)\nif type(value.value) not in types:\n\techoerr(\n\t\tcontext=self.cmsg.format(key=context.key),\n\t\tcontext_mark=context_mark,\n\t\tproblem='{0!r} must be a {1} instance, not {2}'.format(\n\t\t\tvalue,\n\t\t\t', '.join((t.__name__ for t in types)),\n\t\t\ttype(value.value).__name__\n\t\t),\n\t\tproblem_mark=value.mark\n\t)\n\treturn False, True\nreturn True, False", "path": "powerline/powerline/lint/spec.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "powerline/powerline", "stars": 14118, "license": "other", "language": "python", "size": 4261}
{"docstring": "'''Define message which will be used when unknown key was found\n\n\u201cUnknown\u201d is a key that was not provided at the initialization and via \n:py:meth:`Spec.update` and did not match any ``keyfunc`` provided via \n:py:meth:`Spec.unknown_spec`.\n\n:param msgfunc:\n\tFunction that takes that unknown key as an argument and returns the \n\tmessage text. Text will appear at the top (start of the sentence).\n\n:return: self.\n'''\n", "func_signal": "def unknown_msg(self, msgfunc):\n", "code": "self.ufailmsg = msgfunc\nreturn self", "path": "powerline/powerline/lint/spec.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "powerline/powerline", "stars": 14118, "license": "other", "language": "python", "size": 4261}
{"docstring": "'''Define message that describes context\n\n:param str msg:\n\tMessage that describes context. Is written using the \n\t:py:meth:`str.format` syntax and is expected to display keyword \n\tparameter ``key``.\n\n:return: self.\n'''\n", "func_signal": "def context_message(self, msg):\n", "code": "self.cmsg = msg\nfor spec in self.specs:\n\tif not spec.cmsg:\n\t\tspec.context_message(msg)\nreturn self", "path": "powerline/powerline/lint/spec.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "powerline/powerline", "stars": 14118, "license": "other", "language": "python", "size": 4261}
{"docstring": "'''Describe value that must not be there\n\nUseful for giving more descriptive errors for some specific keys then \njust \u201cfound unknown key: shutdown_event\u201d or for forbidding certain \nvalues when :py:meth:`Spec.unknown_spec` was used.\n\n:param str msg:\n\tMessage given for the offending value. It is formatted using \n\t:py:meth:`str.format` with the only positional parameter which is \n\tthe value itself.\n\n:return: self.\n'''\n", "func_signal": "def error(self, msg):\n", "code": "self.checks.append((\n\t'check_func',\n\t(lambda *args: (True, True, True)),\n\t(lambda value: msg.format(value))\n))\nreturn self", "path": "powerline/powerline/lint/spec.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "powerline/powerline", "stars": 14118, "license": "other", "language": "python", "size": 4261}
{"docstring": "'''Helper for the :py:meth:`Spec.copy` function\n\nPopulates new instance with values taken from the old one.\n\n:param dict d:\n\t``__dict__`` of the old instance.\n:param dict copied:\n\tStorage for already copied values.\n'''\n", "func_signal": "def _update(self, d, copied):\n", "code": "self.__dict__.update(d)\nself.keys = copy(self.keys)\nself.checks = copy(self.checks)\nself.uspecs = copy(self.uspecs)\nself.specs = [spec.copy(copied) for spec in self.specs]\nreturn self", "path": "powerline/powerline/lint/spec.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "powerline/powerline", "stars": 14118, "license": "other", "language": "python", "size": 4261}
{"docstring": "'''Displays currently executed context name\n\nThis is similar to :py:func:`current_code_name`, but gives more details.\n\nCurrently it only gives module file name if code_name happens to be \n``<module>``.\n'''\n", "func_signal": "def current_context(pl, segment_info):\n", "code": "name = segment_info['curframe'].f_code.co_name\nif name == '<module>':\n\tname = os.path.basename(segment_info['curframe'].f_code.co_filename)\nreturn name", "path": "powerline/powerline/segments/pdb.py", "commit_date": "2015-01-31 00:00:00", "repo_name": "powerline/powerline", "stars": 14118, "license": "other", "language": "python", "size": 4261}
{"docstring": "'''Process checks registered for the given value\n\nProcesses only \u201ctop-level\u201d checks: key specifications given using at the \ninitialization or via :py:meth:`Spec.unknown_spec` are processed by \n:py:meth:`Spec.match`.\n\n:return: proceed, hadproblem.\n'''\n", "func_signal": "def match_checks(self, *args):\n", "code": "hadproblem = False\nfor check in self.checks:\n\tproceed, chadproblem = getattr(self, check[0])(*(args + check[1:]))\n\tif chadproblem:\n\t\thadproblem = True\n\tif not proceed:\n\t\treturn False, hadproblem\nreturn True, hadproblem", "path": "powerline/powerline/lint/spec.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "powerline/powerline", "stars": 14118, "license": "other", "language": "python", "size": 4261}
{"docstring": "'''Check value using given function\n\n:param function func:\n\tCallable that should accept four positional parameters:\n\n\t#. checked value,\n\t#. ``data`` parameter with arbitrary data (supplied by top-level \n\t   caller),\n\t#. current context and\n\t#. function used for echoing errors.\n\n\tThis callable should return three values:\n\n\t#. determines whether ``check_func`` caller should proceed \n\t   calling other checks,\n\t#. determines whether ``check_func`` should echo error on its own \n\t   (it should be set to False if ``func`` echoes error itself) and\n\t#. determines whether function has found some errors in the checked \n\t   value.\n\n:param function msg_func:\n\tCallable that takes checked value as the only positional parameter \n\tand returns a string that describes the problem. Only useful for \n\tsmall checker functions since it is ignored when second returned \n\tvalue is false.\n\n:return: proceed, hadproblem.\n'''\n", "func_signal": "def check_func(self, value, context_mark, data, context, echoerr, func, msg_func):\n", "code": "havemarks(value)\nproceed, echo, hadproblem = func(value, data, context, echoerr)\nif echo and hadproblem:\n\techoerr(context=self.cmsg.format(key=context.key),\n\t        context_mark=context_mark,\n\t        problem=msg_func(value),\n\t        problem_mark=value.mark)\nreturn proceed, hadproblem", "path": "powerline/powerline/lint/spec.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "powerline/powerline", "stars": 14118, "license": "other", "language": "python", "size": 4261}
{"docstring": "'''Describes value that matches one of the given specs\n\nCheck out :py:meth:`Spec.check_either` method documentation for more \ndetails, but note that there a list of specs was replaced by start and \nend indices in ``self.specs``.\n\n:return: self.\n'''\n", "func_signal": "def either(self, *specs):\n", "code": "start = len(self.specs)\nself.specs.extend(specs)\nself.checks.append(('check_either', start, len(self.specs)))\nreturn self", "path": "powerline/powerline/lint/spec.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "powerline/powerline", "stars": 14118, "license": "other", "language": "python", "size": 4261}
{"docstring": "'''Describe value that is a number or string that has given property\n\n:param str comparison:\n\tType of the comparison. Valid values: ``le``, ``lt``, ``ge``, \n\t``gt``, ``eq``. This argument will restrict the number or string to \n\temit True on the given comparison.\n:param cint:\n\tNumber or string with which value is compared. Type of this \n\tparameter affects required type of the checked value: ``str`` and \n\t``unicode`` types imply ``unicode`` values, ``float`` type implies \n\tthat value can be either ``int`` or ``float``, ``int`` type implies \n\t``int`` value and for any other type the behavior is undefined.\n:param function msg_func:\n\tFunction that should accept checked value and return message that \n\tdescribes the problem with this value. Default value will emit \n\tsomething like \u201c10 is not greater then 10\u201d.\n\n:return: self.\n'''\n", "func_signal": "def cmp(self, comparison, cint, msg_func=None):\n", "code": "if type(cint) is str:\n\tself.type(unicode)\nelif type(cint) is float:\n\tself.type(int, float)\nelse:\n\tself.type(type(cint))\ncmp_func = self.cmp_funcs[comparison]\nmsg_func = msg_func or (lambda value: '{0} is not {1} {2}'.format(value, self.cmp_msgs[comparison], cint))\nself.checks.append((\n\t'check_func',\n\t(lambda value, *args: (True, True, not cmp_func(value.value, cint))),\n\tmsg_func\n))\nreturn self", "path": "powerline/powerline/lint/spec.py", "commit_date": "2020-12-22 00:00:00", "repo_name": "powerline/powerline", "stars": 14118, "license": "other", "language": "python", "size": 4261}
{"docstring": "'''Displays current stack depth\n\nResult is relative to the stack depth at the time prompt was first run.\n\n:param bool full_stack:\n\tIf true then absolute depth is used.\n'''\n", "func_signal": "def stack_depth(pl, segment_info, full_stack=False):\n", "code": "return str(len(segment_info['pdb'].stack) - (\n\t0 if full_stack else segment_info['initial_stack_length']))", "path": "powerline/powerline/segments/pdb.py", "commit_date": "2015-01-31 00:00:00", "repo_name": "powerline/powerline", "stars": 14118, "license": "other", "language": "python", "size": 4261}
{"docstring": "# Simple completion menu.\n", "func_signal": "def main():\n", "code": "print(\"(The completion menu displays colors.)\")\nprompt(\"Type a color: \", completer=FuzzyCompleter(ColorCompleter()))\n\n# Multi-column menu.\nprompt(\n    \"Type a color: \",\n    completer=FuzzyCompleter(ColorCompleter()),\n    complete_style=CompleteStyle.MULTI_COLUMN,\n)\n\n# Readline-like\nprompt(\n    \"Type a color: \",\n    completer=FuzzyCompleter(ColorCompleter()),\n    complete_style=CompleteStyle.READLINE_LIKE,\n)", "path": "python-prompt-toolkit/examples/prompts/auto-completion/fuzzy-custom-completer.py", "commit_date": "2019-11-28 00:00:00", "repo_name": "prompt-toolkit/python-prompt-toolkit", "stars": 8874, "license": "bsd-3-clause", "language": "python", "size": 9498}
{"docstring": "# Create user interface.\n", "func_signal": "def main():\n", "code": "hello_world_window()\n\n# Enable threading in GTK. (Otherwise, GTK will keep the GIL.)\ngtk.gdk.threads_init()\n\n# Read input from the command line, using an event loop with this hook.\n# We use `patch_stdout`, because clicking the button will print something;\n# and that should print nicely 'above' the input line.\nwith patch_stdout():\n    session = PromptSession(\n        \"Python >>> \", inputhook=inputhook, lexer=PygmentsLexer(PythonLexer)\n    )\n    result = session.prompt()\nprint(\"You said: %s\" % result)", "path": "python-prompt-toolkit/examples/prompts/inputhook.py", "commit_date": "2020-07-30 00:00:00", "repo_name": "prompt-toolkit/python-prompt-toolkit", "stars": 8874, "license": "bsd-3-clause", "language": "python", "size": 9498}
{"docstring": "# Create some history first. (Easy for testing.)\n", "func_signal": "def main():\n", "code": "history = InMemoryHistory()\nhistory.append_string(\"import os\")\nhistory.append_string('print(\"hello\")')\nhistory.append_string('print(\"world\")')\nhistory.append_string(\"import path\")\n\n# Print help.\nprint(\"This CLI has fish-style auto-suggestion enable.\")\nprint('Type for instance \"pri\", then you\\'ll see a suggestion.')\nprint(\"Press the right arrow to insert the suggestion.\")\nprint(\"Press Control-C to retry. Control-D to exit.\")\nprint()\n\nsession = PromptSession(\n    history=history,\n    auto_suggest=AutoSuggestFromHistory(),\n    enable_history_search=True,\n)\n\nwhile True:\n    try:\n        text = session.prompt(\"Say something: \")\n    except KeyboardInterrupt:\n        pass  # Ctrl-C pressed. Try again.\n    else:\n        break\n\nprint(\"You said: %s\" % text)", "path": "python-prompt-toolkit/examples/prompts/auto-suggestion.py", "commit_date": "2019-11-28 00:00:00", "repo_name": "prompt-toolkit/python-prompt-toolkit", "stars": 8874, "license": "bsd-3-clause", "language": "python", "size": 9498}
{"docstring": "\"\"\"\nUsing ANSI for the formatting.\n\"\"\"\n", "func_signal": "def example_3():\n", "code": "answer = prompt(\n    ANSI(\n        \"\\x1b[31mjohn\\x1b[0m@\"\n        \"\\x1b[44mlocalhost\\x1b[0m:\"\n        \"\\x1b[4m/user/john\\x1b[0m\"\n        \"# \"\n    )\n)\nprint(\"You said: %s\" % answer)", "path": "python-prompt-toolkit/examples/prompts/colored-prompt.py", "commit_date": "2019-11-28 00:00:00", "repo_name": "prompt-toolkit/python-prompt-toolkit", "stars": 8874, "license": "bsd-3-clause", "language": "python", "size": 9498}
{"docstring": "\"\"\"\ntest whether we receive the correct previous_key_sequence.\n\"\"\"\n", "func_signal": "def test_previous_key_sequence(processor):\n", "code": "with set_dummy_app():\n    events = []\n\n    def handler(event):\n        events.append(event)\n\n    # Build registry.\n    registry = KeyBindings()\n    registry.add(\"a\", \"a\")(handler)\n    registry.add(\"b\", \"b\")(handler)\n    processor = KeyProcessor(registry)\n\n    # Create processor and feed keys.\n    processor.feed(KeyPress(\"a\", \"a\"))\n    processor.feed(KeyPress(\"a\", \"a\"))\n    processor.feed(KeyPress(\"b\", \"b\"))\n    processor.feed(KeyPress(\"b\", \"b\"))\n    processor.process_keys()\n\n    # Test.\n    assert len(events) == 2\n    assert len(events[0].key_sequence) == 2\n    assert events[0].key_sequence[0].key == \"a\"\n    assert events[0].key_sequence[0].data == \"a\"\n    assert events[0].key_sequence[1].key == \"a\"\n    assert events[0].key_sequence[1].data == \"a\"\n    assert events[0].previous_key_sequence == []\n\n    assert len(events[1].key_sequence) == 2\n    assert events[1].key_sequence[0].key == \"b\"\n    assert events[1].key_sequence[0].data == \"b\"\n    assert events[1].key_sequence[1].key == \"b\"\n    assert events[1].key_sequence[1].data == \"b\"\n    assert len(events[1].previous_key_sequence) == 2\n    assert events[1].previous_key_sequence[0].key == \"a\"\n    assert events[1].previous_key_sequence[0].data == \"a\"\n    assert events[1].previous_key_sequence[1].key == \"a\"\n    assert events[1].previous_key_sequence[1].data == \"a\"", "path": "python-prompt-toolkit/tests/test_key_binding.py", "commit_date": "2020-01-21 00:00:00", "repo_name": "prompt-toolkit/python-prompt-toolkit", "stars": 8874, "license": "bsd-3-clause", "language": "python", "size": 9498}
{"docstring": "\"\"\"\nWhen the eventloop of prompt-toolkit is idle, call this inputhook.\n\nThis will run the GTK main loop until the file descriptor\n`context.fileno()` becomes ready.\n\n:param context: An `InputHookContext` instance.\n\"\"\"\n\n", "func_signal": "def inputhook(context):\n", "code": "def _main_quit(*a, **kw):\n    gtk.main_quit()\n    return False\n\ngobject.io_add_watch(context.fileno(), gobject.IO_IN, _main_quit)\ngtk.main()", "path": "python-prompt-toolkit/examples/prompts/inputhook.py", "commit_date": "2020-07-30 00:00:00", "repo_name": "prompt-toolkit/python-prompt-toolkit", "stars": 8874, "license": "bsd-3-clause", "language": "python", "size": 9498}
{"docstring": "\"\"\"\nTest `Variable` with varname.\n\"\"\"\n", "func_signal": "def test_variable_varname():\n", "code": "g = compile(\"((?P<varname>hello|world)|test)\")\n\nm = g.match(\"hello\")\nvariables = m.variables()\nassert isinstance(variables, Variables)\nassert variables.get(\"varname\") == \"hello\"\nassert variables[\"varname\"] == \"hello\"\n\nm = g.match(\"world\")\nvariables = m.variables()\nassert isinstance(variables, Variables)\nassert variables.get(\"varname\") == \"world\"\nassert variables[\"varname\"] == \"world\"\n\nm = g.match(\"test\")\nvariables = m.variables()\nassert isinstance(variables, Variables)\nassert variables.get(\"varname\") is None\nassert variables[\"varname\"] is None", "path": "python-prompt-toolkit/tests/test_regular_languages.py", "commit_date": "2020-01-21 00:00:00", "repo_name": "prompt-toolkit/python-prompt-toolkit", "stars": 8874, "license": "bsd-3-clause", "language": "python", "size": 9498}
{"docstring": "# Example 1: fixed text.\n", "func_signal": "def main():\n", "code": "text = prompt(\"Say something: \", bottom_toolbar=\"This is a toolbar\")\nprint(\"You said: %s\" % text)\n\n# Example 2: fixed text from a callable:\ndef get_toolbar():\n    return \"Bottom toolbar: time=%r\" % time.time()\n\ntext = prompt(\"Say something: \", bottom_toolbar=get_toolbar, refresh_interval=0.5)\nprint(\"You said: %s\" % text)\n\n# Example 3: Using HTML:\ntext = prompt(\n    \"Say something: \",\n    bottom_toolbar=HTML(\n        '(html) <b>This</b> <u>is</u> a <style bg=\"ansired\">toolbar</style>'\n    ),\n)\nprint(\"You said: %s\" % text)\n\n# Example 4: Using ANSI:\ntext = prompt(\n    \"Say something: \",\n    bottom_toolbar=ANSI(\n        \"(ansi): \\x1b[1mThis\\x1b[0m \\x1b[4mis\\x1b[0m a \\x1b[91mtoolbar\"\n    ),\n)\nprint(\"You said: %s\" % text)\n\n# Example 5: styling differently.\nstyle = Style.from_dict(\n    {\n        \"bottom-toolbar\": \"#aaaa00 bg:#ff0000\",\n        \"bottom-toolbar.text\": \"#aaaa44 bg:#aa4444\",\n    }\n)\n\ntext = prompt(\"Say something: \", bottom_toolbar=\"This is a toolbar\", style=style)\nprint(\"You said: %s\" % text)\n\n# Example 6: Using a list of tokens.\ndef get_bottom_toolbar():\n    return [\n        (\"\", \" \"),\n        (\"bg:#ff0000 fg:#000000\", \"This\"),\n        (\"\", \" is a \"),\n        (\"bg:#ff0000 fg:#000000\", \"toolbar\"),\n        (\"\", \". \"),\n    ]\n\ntext = prompt(\"Say something: \", bottom_toolbar=get_bottom_toolbar)\nprint(\"You said: %s\" % text)\n\n# Example 7: multiline fixed text.\ntext = prompt(\"Say something: \", bottom_toolbar=\"This is\\na multiline toolbar\")\nprint(\"You said: %s\" % text)", "path": "python-prompt-toolkit/examples/prompts/bottom-toolbar.py", "commit_date": "2019-11-28 00:00:00", "repo_name": "prompt-toolkit/python-prompt-toolkit", "stars": 8874, "license": "bsd-3-clause", "language": "python", "size": 9498}
{"docstring": "# Simple completion menu.\n", "func_signal": "def main():\n", "code": "print(\"(The completion menu displays colors.)\")\nprompt(\"Type an animal: \", completer=AnimalCompleter())\n\n# Multi-column menu.\nprompt(\n    \"Type an animal: \",\n    completer=AnimalCompleter(),\n    complete_style=CompleteStyle.MULTI_COLUMN,\n)\n\n# Readline-like\nprompt(\n    \"Type an animal: \",\n    completer=AnimalCompleter(),\n    complete_style=CompleteStyle.READLINE_LIKE,\n)", "path": "python-prompt-toolkit/examples/prompts/auto-completion/colored-completions-with-formatted-text.py", "commit_date": "2019-11-28 00:00:00", "repo_name": "prompt-toolkit/python-prompt-toolkit", "stars": 8874, "license": "bsd-3-clause", "language": "python", "size": 9498}
{"docstring": "# In this case, our style has both class 'a' and 'b'.\n# The style that is defined the latest get priority.\n", "func_signal": "def test_class_combinations_2():\n", "code": "style = Style(\n    [\n        (\"a b\", \"#ff0000\"),\n        (\"b\", \"#00ff00\"),\n        (\"a\", \"#0000ff\"),\n    ]\n)\nexpected = Attrs(\n    color=\"00ff00\",\n    bgcolor=\"\",\n    bold=False,\n    underline=False,\n    italic=False,\n    blink=False,\n    reverse=False,\n    hidden=False,\n)\nassert style.get_attrs_for_style_str(\"class:a class:b\") == expected\nassert style.get_attrs_for_style_str(\"class:a,b\") == expected\nassert style.get_attrs_for_style_str(\"class:a,b,c\") == expected\n\n# Defining 'a' latest should give priority to 'a'.\nexpected = Attrs(\n    color=\"0000ff\",\n    bgcolor=\"\",\n    bold=False,\n    underline=False,\n    italic=False,\n    blink=False,\n    reverse=False,\n    hidden=False,\n)\nassert style.get_attrs_for_style_str(\"class:b class:a\") == expected\nassert style.get_attrs_for_style_str(\"class:b,a\") == expected", "path": "python-prompt-toolkit/tests/test_style.py", "commit_date": "2020-08-29 00:00:00", "repo_name": "prompt-toolkit/python-prompt-toolkit", "stars": 8874, "license": "bsd-3-clause", "language": "python", "size": 9498}
{"docstring": "\"\"\"\nCreate a GTK window with one 'Hello world' button.\n\"\"\"\n# Create a new window.\n", "func_signal": "def hello_world_window():\n", "code": "window = gtk.Window(gtk.WINDOW_TOPLEVEL)\nwindow.set_border_width(50)\n\n# Create a new button with the label \"Hello World\".\nbutton = gtk.Button(\"Hello World\")\nwindow.add(button)\n\n# Clicking the button prints some text.\ndef clicked(data):\n    print(\"Button clicked!\")\n\nbutton.connect(\"clicked\", clicked)\n\n# Display the window.\nbutton.show()\nwindow.show()", "path": "python-prompt-toolkit/examples/prompts/inputhook.py", "commit_date": "2020-07-30 00:00:00", "repo_name": "prompt-toolkit/python-prompt-toolkit", "stars": 8874, "license": "bsd-3-clause", "language": "python", "size": 9498}
{"docstring": "# Invalid sequence that has at two characters in common with other\n# sequences.\n", "func_signal": "def test_invalid(processor, stream):\n", "code": "stream.feed(\"\\x1b[*\")\n\nassert len(processor.keys) == 3\nassert processor.keys[0].key == Keys.Escape\nassert processor.keys[1].key == \"[\"\nassert processor.keys[2].key == \"*\"", "path": "python-prompt-toolkit/tests/test_inputstream.py", "commit_date": "2020-01-24 00:00:00", "repo_name": "prompt-toolkit/python-prompt-toolkit", "stars": 8874, "license": "bsd-3-clause", "language": "python", "size": 9498}
{"docstring": "\"\"\"\nUsing HTML for the formatting.\n\"\"\"\n", "func_signal": "def example_2():\n", "code": "answer = prompt(\n    HTML(\n        \"<username>john</username><at>@</at>\"\n        \"<host>localhost</host>\"\n        \"<colon>:</colon>\"\n        \"<path>/user/john</path>\"\n        '<style bg=\"#00aa00\" fg=\"#ffffff\">#</style> '\n    ),\n    style=style,\n)\nprint(\"You said: %s\" % answer)", "path": "python-prompt-toolkit/examples/prompts/colored-prompt.py", "commit_date": "2019-11-28 00:00:00", "repo_name": "prompt-toolkit/python-prompt-toolkit", "stars": 8874, "license": "bsd-3-clause", "language": "python", "size": 9498}
{"docstring": "# Create some history first. (Easy for testing.)\n", "func_signal": "def main():\n", "code": "history = InMemoryHistory()\nhistory.append_string(\"import os\")\nhistory.append_string('print(\"hello\")')\nhistory.append_string('print(\"world\")')\nhistory.append_string(\"import path\")\n\n# Print help.\nprint(\"This CLI has up-arrow partial string matching enabled.\")\nprint('Type for instance \"pri\" followed by up-arrow and you')\nprint('get the last items starting with \"pri\".')\nprint(\"Press Control-C to retry. Control-D to exit.\")\nprint()\n\nsession = PromptSession(history=history, enable_history_search=True)\n\nwhile True:\n    try:\n        text = session.prompt(\"Say something: \")\n    except KeyboardInterrupt:\n        pass  # Ctrl-C pressed. Try again.\n    else:\n        break\n\nprint(\"You said: %s\" % text)", "path": "python-prompt-toolkit/examples/prompts/up-arrow-partial-string-matching.py", "commit_date": "2019-11-28 00:00:00", "repo_name": "prompt-toolkit/python-prompt-toolkit", "stars": 8874, "license": "bsd-3-clause", "language": "python", "size": 9498}
{"docstring": "# Make sure that the newline is not included in the CPR response.\n", "func_signal": "def test_cpr_response_2(processor, stream):\n", "code": "stream.feed(\"\\x1b[40;1R\\n\")\nassert len(processor.keys) == 2\nassert processor.keys[0].key == Keys.CPRResponse\nassert processor.keys[1].key == Keys.ControlJ", "path": "python-prompt-toolkit/tests/test_inputstream.py", "commit_date": "2020-01-24 00:00:00", "repo_name": "prompt-toolkit/python-prompt-toolkit", "stars": 8874, "license": "bsd-3-clause", "language": "python", "size": 9498}
{"docstring": "# Send left key in two parts without flush.\n", "func_signal": "def test_flush_1(processor, stream):\n", "code": "stream.feed(\"\\x1b\")\nstream.feed(\"[D\")\n\nassert len(processor.keys) == 1\nassert processor.keys[0].key == Keys.Left\nassert processor.keys[0].data == \"\\x1b[D\"", "path": "python-prompt-toolkit/tests/test_inputstream.py", "commit_date": "2020-01-24 00:00:00", "repo_name": "prompt-toolkit/python-prompt-toolkit", "stars": 8874, "license": "bsd-3-clause", "language": "python", "size": 9498}
{"docstring": "# We start with a `Registry` of default key bindings.\n", "func_signal": "def main():\n", "code": "bindings = KeyBindings()\n\n# Create the decorators to be used for registering text objects and\n# operators in this registry.\noperator = create_operator_decorator(bindings)\ntext_object = create_text_object_decorator(bindings)\n\n# Create a custom operator.\n\n@operator(\"R\")\ndef _(event, text_object):\n    \" Custom operator that reverses text. \"\n    buff = event.current_buffer\n\n    # Get relative start/end coordinates.\n    start, end = text_object.operator_range(buff.document)\n    start += buff.cursor_position\n    end += buff.cursor_position\n\n    text = buff.text[start:end]\n    text = \"\".join(reversed(text))\n\n    event.app.current_buffer.text = buff.text[:start] + text + buff.text[end:]\n\n# Create a text object.\n\n@text_object(\"A\")\ndef _(event):\n    \" A custom text object that involves everything. \"\n    # Note that a `TextObject` has coordinates, relative to the cursor position.\n    buff = event.current_buffer\n    return TextObject(\n        -buff.document.cursor_position,  # The start.\n        len(buff.text) - buff.document.cursor_position,\n    )  # The end.\n\n# Read input.\nprint('There is a custom text object \"A\" that applies to everything')\nprint('and a custom operator \"r\" that reverses the text object.\\n')\n\nprint(\"Things that are possible:\")\nprint(\"-  Riw    - reverse inner word.\")\nprint(\"-  yA     - yank everything.\")\nprint(\"-  RA     - reverse everything.\")\n\ntext = prompt(\n    \"> \", default=\"hello world\", key_bindings=bindings, editing_mode=EditingMode.VI\n)\nprint(\"You said: %s\" % text)", "path": "python-prompt-toolkit/examples/prompts/custom-vi-operator-and-text-object.py", "commit_date": "2019-11-28 00:00:00", "repo_name": "prompt-toolkit/python-prompt-toolkit", "stars": 8874, "license": "bsd-3-clause", "language": "python", "size": 9498}
{"docstring": "\"\"\"\nReturn a context manager that makes sure that this dummy application is\nactive. This is important, because we need an `Application` with\n`is_done=False` flag, otherwise no keys will be processed.\n\"\"\"\n", "func_signal": "def set_dummy_app():\n", "code": "app = Application(\n    layout=Layout(Window()), output=DummyOutput(), input=create_pipe_input()\n)\nreturn set_app(app)", "path": "python-prompt-toolkit/tests/test_key_binding.py", "commit_date": "2020-01-21 00:00:00", "repo_name": "prompt-toolkit/python-prompt-toolkit", "stars": 8874, "license": "bsd-3-clause", "language": "python", "size": 9498}
{"docstring": "# The layout.\n", "func_signal": "def main():\n", "code": "search_field = SearchToolbar()  # For reverse search.\n\noutput_field = TextArea(style=\"class:output-field\", text=help_text)\ninput_field = TextArea(\n    height=1,\n    prompt=\">>> \",\n    style=\"class:input-field\",\n    multiline=False,\n    wrap_lines=False,\n    search_field=search_field,\n)\n\ncontainer = HSplit(\n    [\n        output_field,\n        Window(height=1, char=\"-\", style=\"class:line\"),\n        input_field,\n        search_field,\n    ]\n)\n\n# Attach accept handler to the input field. We do this by assigning the\n# handler to the `TextArea` that we created earlier. it is also possible to\n# pass it to the constructor of `TextArea`.\n# NOTE: It's better to assign an `accept_handler`, rather then adding a\n#       custom ENTER key binding. This will automatically reset the input\n#       field and add the strings to the history.\ndef accept(buff):\n    # Evaluate \"calculator\" expression.\n    try:\n        output = \"\\n\\nIn:  {}\\nOut: {}\".format(\n            input_field.text, eval(input_field.text)\n        )  # Don't do 'eval' in real code!\n    except BaseException as e:\n        output = \"\\n\\n{}\".format(e)\n    new_text = output_field.text + output\n\n    # Add text to output buffer.\n    output_field.buffer.document = Document(\n        text=new_text, cursor_position=len(new_text)\n    )\n\ninput_field.accept_handler = accept\n\n# The key bindings.\nkb = KeyBindings()\n\n@kb.add(\"c-c\")\n@kb.add(\"c-q\")\ndef _(event):\n    \" Pressing Ctrl-Q or Ctrl-C will exit the user interface. \"\n    event.app.exit()\n\n# Style.\nstyle = Style(\n    [\n        (\"output-field\", \"bg:#000044 #ffffff\"),\n        (\"input-field\", \"bg:#000000 #ffffff\"),\n        (\"line\", \"#004400\"),\n    ]\n)\n\n# Run application.\napplication = Application(\n    layout=Layout(container, focused_element=input_field),\n    key_bindings=kb,\n    style=style,\n    mouse_support=True,\n    full_screen=True,\n)\n\napplication.run()", "path": "python-prompt-toolkit/examples/full-screen/calculator.py", "commit_date": "2019-11-28 00:00:00", "repo_name": "prompt-toolkit/python-prompt-toolkit", "stars": 8874, "license": "bsd-3-clause", "language": "python", "size": 9498}
{"docstring": "\"\"\"\nStyle and list of (style, text) tuples.\n\"\"\"\n# Not that we can combine class names and inline styles.\n", "func_signal": "def example_1():\n", "code": "prompt_fragments = [\n    (\"class:username\", \"john\"),\n    (\"class:at\", \"@\"),\n    (\"class:host\", \"localhost\"),\n    (\"class:colon\", \":\"),\n    (\"class:path\", \"/user/john\"),\n    (\"bg:#00aa00 #ffffff\", \"#\"),\n    (\"\", \" \"),\n]\n\nanswer = prompt(prompt_fragments, style=style)\nprint(\"You said: %s\" % answer)", "path": "python-prompt-toolkit/examples/prompts/colored-prompt.py", "commit_date": "2019-11-28 00:00:00", "repo_name": "prompt-toolkit/python-prompt-toolkit", "stars": 8874, "license": "bsd-3-clause", "language": "python", "size": 9498}
{"docstring": "\"\"\"\nThis function raises ``PyAutoGUIException``. It's used for the PyTweening function names if the PyTweening\nmodule failed to be imported.\n\"\"\"\n", "func_signal": "def _couldNotImportPyTweening():\n", "code": "raise PyAutoGUIException(\n    \"PyAutoGUI was unable to import pytweening. Please install this module to enable the function you tried to call.\"\n)", "path": "pyautogui/pyautogui/__init__.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "asweigart/pyautogui", "stars": 9327, "license": "bsd-3-clause", "language": "python", "size": 2346}
{"docstring": "# Check that all the functions are defined.\n\n# mouse-related API\n", "func_signal": "def test_accessibleNames(self):\n", "code": "pyautogui.moveTo\npyautogui.moveRel\npyautogui.dragTo\npyautogui.dragRel\npyautogui.mouseDown\npyautogui.mouseUp\npyautogui.click\npyautogui.rightClick\npyautogui.doubleClick\npyautogui.tripleClick\n\n# keyboard-related API\npyautogui.typewrite\npyautogui.hotkey\npyautogui.keyDown\npyautogui.keyUp\npyautogui.press\npyautogui.hold\n\n# The functions implemented in the platform-specific modules should also show up in the pyautogui namespace:\npyautogui.position\npyautogui.size\npyautogui.scroll\npyautogui.hscroll\npyautogui.vscroll\n\n# util API\npyautogui.KEYBOARD_KEYS\npyautogui.isShiftCharacter\n\n# Screenshot-related API\npyautogui.locateAll\npyautogui.locate\npyautogui.locateOnScreen\npyautogui.locateAllOnScreen\npyautogui.locateCenterOnScreen\npyautogui.center\npyautogui.pixelMatchesColor\npyautogui.pixel\npyautogui.screenshot\npyautogui.grab\n\n# TODO(denilsonsa): I believe we should get rid of these symbols. If someone wants tweening, import pytweening module instead!\n# Tweening-related API\npyautogui.getPointOnLine\npyautogui.linear\npyautogui.easeInQuad\npyautogui.easeOutQuad\npyautogui.easeInOutQuad\npyautogui.easeInCubic\npyautogui.easeOutCubic\npyautogui.easeInOutCubic\npyautogui.easeInQuart\npyautogui.easeOutQuart\npyautogui.easeInOutQuart\npyautogui.easeInQuint\npyautogui.easeOutQuint\npyautogui.easeInOutQuint\npyautogui.easeInSine\npyautogui.easeOutSine\npyautogui.easeInOutSine\npyautogui.easeInExpo\npyautogui.easeOutExpo\npyautogui.easeInOutExpo\npyautogui.easeInCirc\npyautogui.easeOutCirc\npyautogui.easeInOutCirc\npyautogui.easeInElastic\npyautogui.easeOutElastic\npyautogui.easeInOutElastic\npyautogui.easeInBack\npyautogui.easeOutBack\npyautogui.easeInOutBack\npyautogui.easeInBounce\npyautogui.easeOutBounce\npyautogui.easeInOutBounce", "path": "pyautogui/tests/test_pyautogui.py", "commit_date": "2020-02-21 00:00:00", "repo_name": "asweigart/pyautogui", "stars": 9327, "license": "bsd-3-clause", "language": "python", "size": 2346}
{"docstring": "\"\"\"\nThis function raises PyAutoGUIException. It's used for the MouseInfo function names if the MouseInfo module\nfailed to be imported.\n\"\"\"\n", "func_signal": "def mouseInfo():\n", "code": "raise PyAutoGUIException(\n    \"PyAutoGUI was unable to import mouseinfo. Please install this module to enable the function you tried to call.\"\n)", "path": "pyautogui/pyautogui/__init__.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "asweigart/pyautogui", "stars": 9327, "license": "bsd-3-clause", "language": "python", "size": 2346}
{"docstring": "# start at the center\n", "func_signal": "def test_moveRel(self):\n", "code": "desired = self.center\npyautogui.moveTo(*desired)\nmousepos = P(*pyautogui.position())\nself.assertEqual(mousepos, desired)\n\n# move down and right\ndesired += P(42, 42)\npyautogui.moveRel(42, 42)\nmousepos = P(*pyautogui.position())\nself.assertEqual(mousepos, desired)\n\n# move up and left\ndesired -= P(42, 42)\npyautogui.moveRel(-42, -42)\nmousepos = P(*pyautogui.position())\nself.assertEqual(mousepos, desired)\n\n# move right\ndesired += P(42, 0)\npyautogui.moveRel(42, 0)\nmousepos = P(*pyautogui.position())\nself.assertEqual(mousepos, desired)\n\n# move down\ndesired += P(0, 42)\npyautogui.moveRel(0, 42)\nmousepos = P(*pyautogui.position())\nself.assertEqual(mousepos, desired)\n\n# move left\ndesired += P(-42, 0)\npyautogui.moveRel(-42, 0)\nmousepos = P(*pyautogui.position())\nself.assertEqual(mousepos, desired)\n\n# move up\ndesired += P(0, -42)\npyautogui.moveRel(0, -42)\nmousepos = P(*pyautogui.position())\nself.assertEqual(mousepos, desired)\n\n# Passing a list instead of separate x and y.\ndesired += P(42, 42)\npyautogui.moveRel([42, 42])\nmousepos = P(*pyautogui.position())\nself.assertEqual(mousepos, desired)\n\n# Passing a tuple instead of separate x and y.\ndesired -= P(42, 42)\npyautogui.moveRel((-42, -42))\nmousepos = P(*pyautogui.position())\nself.assertEqual(mousepos, desired)\n\n# Passing a sequence-like object instead of separate x and y.\ndesired += P(42, 42)\npyautogui.moveRel(P(42, 42))\nmousepos = P(*pyautogui.position())\nself.assertEqual(mousepos, desired)", "path": "pyautogui/tests/test_pyautogui.py", "commit_date": "2020-02-21 00:00:00", "repo_name": "asweigart/pyautogui", "stars": 9327, "license": "bsd-3-clause", "language": "python", "size": 2346}
{"docstring": "# TODO - for now, we only test that the \"return None\" and \"raise pyautogui.ImageNotFoundException\" is raised.\n\n", "func_signal": "def test_locateFunctions(self):\n", "code": "pyautogui.useImageNotFoundException()\nwith self.assertRaises(pyautogui.ImageNotFoundException):\n    pyautogui.locate(\"100x100blueimage.png\", \"100x100redimage.png\")\n# Commenting out the locateAll*() functions because they return generators, even if the image can't be found. Should they instead raise an exception? This is a question for pyscreeze's design.\n# with self.assertRaises(pyautogui.ImageNotFoundException):\n#    pyautogui.locateAll('100x100blueimage.png', '100x100redimage.png')\n\n# with self.assertRaises(pyautogui.ImageNotFoundException):\n#    pyautogui.locateAllOnScreen('100x100blueimage.png') # NOTE: This test fails if there is a blue square visible on the screen.\nwith self.assertRaises(pyautogui.ImageNotFoundException):\n    pyautogui.locateOnScreen(\n        \"100x100blueimage.png\"\n    )  # NOTE: This test fails if there is a blue square visible on the screen.\nwith self.assertRaises(pyautogui.ImageNotFoundException):\n    pyautogui.locateCenterOnScreen(\n        \"100x100blueimage.png\"\n    )  # NOTE: This test fails if there is a blue square visible on the screen.\n\npyautogui.useImageNotFoundException(False)\nself.assertEqual(pyautogui.locate(\"100x100blueimage.png\", \"100x100redimage.png\"), None)\n# self.assertEqual(pyautogui.locateAll('100x100blueimage.png', '100x100redimage.png'), None)\n# self.assertEqual(pyautogui.locateAllOnScreen('100x100blueimage.png'), None) # NOTE: This test fails if there is a blue square visible on the screen.\nself.assertEqual(\n    pyautogui.locateOnScreen(\"100x100blueimage.png\"), None\n)  # NOTE: This test fails if there is a blue square visible on the screen.\nself.assertEqual(\n    pyautogui.locateCenterOnScreen(\"100x100blueimage.png\"), None\n)  # NOTE: This test fails if there is a blue square visible on the screen.", "path": "pyautogui/tests/test_pyautogui.py", "commit_date": "2020-02-21 00:00:00", "repo_name": "asweigart/pyautogui", "stars": 9327, "license": "bsd-3-clause", "language": "python", "size": 2346}
{"docstring": "\"\"\"Moves the mouse cursor to a point on the screen, relative to its current\nposition.\n\nThe x and y parameters detail where the mouse event happens. If None, the\ncurrent mouse position is used. If a float value, it is rounded down. If\noutside the boundaries of the screen, the event happens at edge of the\nscreen.\n\nArgs:\n  x (int, float, None, tuple, optional): How far left (for negative values) or\n    right (for positive values) to move the cursor. 0 by default. If tuple, this is used for x and y.\n  y (int, float, None, optional): How far up (for negative values) or\n    down (for positive values) to move the cursor. 0 by default.\n  duration (float, optional): The amount of time it takes to move the mouse\n    cursor to the new xy coordinates. If 0, then the mouse cursor is moved\n    instantaneously. 0.0 by default.\n  tween (func, optional): The tweening function used if the duration is not\n    0. A linear tween is used by default.\n\nReturns:\n  None\n\"\"\"\n", "func_signal": "def moveRel(xOffset=None, yOffset=None, duration=0.0, tween=linear, logScreenshot=False, _pause=True):\n", "code": "xOffset, yOffset = _normalizeXYArgs(xOffset, yOffset)\n\n_logScreenshot(logScreenshot, \"moveRel\", \"%s,%s\" % (xOffset, yOffset), folder=\".\")\n_mouseMoveDrag(\"move\", None, None, xOffset, yOffset, duration, tween)", "path": "pyautogui/pyautogui/__init__.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "asweigart/pyautogui", "stars": 9327, "license": "bsd-3-clause", "language": "python", "size": 2346}
{"docstring": "\"\"\"Performs releasing a mouse button up (but not down beforehand).\n\nThe x and y parameters detail where the mouse event happens. If None, the\ncurrent mouse position is used. If a float value, it is rounded down. If\noutside the boundaries of the screen, the event happens at edge of the\nscreen.\n\nArgs:\n  x (int, float, None, tuple, optional): The x position on the screen where the\n    mouse up happens. None by default. If tuple, this is used for x and y.\n    If x is a str, it's considered a filename of an image to find on\n    the screen with locateOnScreen() and click the center of.\n  y (int, float, None, optional): The y position on the screen where the\n    mouse up happens. None by default.\n  button (str, int, optional): The mouse button released. TODO\n\nReturns:\n  None\n\nRaises:\n  PyAutoGUIException: If button is not one of 'left', 'middle', 'right', 1, 2, or 3\n\"\"\"\n", "func_signal": "def mouseUp(x=None, y=None, button=PRIMARY, duration=0.0, tween=linear, logScreenshot=None, _pause=True):\n", "code": "button = _normalizeButton(button)\nx, y = _normalizeXYArgs(x, y)\n\n_mouseMoveDrag(\"move\", x, y, 0, 0, duration=0, tween=None)\n\n_logScreenshot(logScreenshot, \"mouseUp\", \"%s,%s\" % (x, y), folder=\".\")\nplatformModule._mouseUp(x, y, button)", "path": "pyautogui/pyautogui/__init__.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "asweigart/pyautogui", "stars": 9327, "license": "bsd-3-clause", "language": "python", "size": 2346}
{"docstring": "\"\"\"\nReturns ``n``, where ``n`` is the float argument between ``0.0`` and ``1.0``. This function is for the default\nlinear tween for mouse moving functions.\n\nThis function was copied from PyTweening module, so that it can be called even if PyTweening is not installed.\n\"\"\"\n\n# We use this function instead of pytweening.linear for the default tween function just in case pytweening couldn't be imported.\n", "func_signal": "def linear(n):\n", "code": "if not 0.0 <= n <= 1.0:\n    raise PyAutoGUIException(\"Argument must be between 0.0 and 1.0.\")\nreturn n", "path": "pyautogui/pyautogui/__init__.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "asweigart/pyautogui", "stars": 9327, "license": "bsd-3-clause", "language": "python", "size": 2346}
{"docstring": "\"\"\"Returns whether the given xy coordinates are on the primary screen or not.\n\nNote that this function doesn't work for secondary screens.\n\nArgs:\n  Either the arguments are two separate values, first arg for x and second\n    for y, or there is a single argument of a sequence with two values, the\n    first x and the second y.\n    Example: onScreen(x, y) or onScreen([x, y])\n\nReturns:\n  bool: True if the xy coordinates are on the screen at its current\n    resolution, otherwise False.\n\"\"\"\n", "func_signal": "def onScreen(x, y=None):\n", "code": "x, y = _normalizeXYArgs(x, y)\nx = int(x)\ny = int(y)\n\nwidth, height = platformModule._size()\nreturn 0 <= x < width and 0 <= y < height", "path": "pyautogui/pyautogui/__init__.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "asweigart/pyautogui", "stars": 9327, "license": "bsd-3-clause", "language": "python", "size": 2346}
{"docstring": "\"\"\"Performs an explicitly vertical scroll of the mouse scroll wheel,\nif this is supported by the operating system. (Currently just Linux.)\n\nThe x and y parameters detail where the mouse event happens. If None, the\ncurrent mouse position is used. If a float value, it is rounded down. If\noutside the boundaries of the screen, the event happens at edge of the\nscreen.\n\nArgs:\n  clicks (int, float): The amount of scrolling to perform.\n  x (int, float, None, tuple, optional): The x position on the screen where the\n    click happens. None by default. If tuple, this is used for x and y.\n  y (int, float, None, optional): The y position on the screen where the\n    click happens. None by default.\n\nReturns:\n  None\n\"\"\"\n", "func_signal": "def vscroll(clicks, x=None, y=None, logScreenshot=None, _pause=True):\n", "code": "if type(x) in (tuple, list):\n    x, y = x[0], x[1]\nx, y = position(x, y)\n\n_logScreenshot(logScreenshot, \"vscroll\", \"%s,%s,%s\" % (clicks, x, y), folder=\".\")\nplatformModule._vscroll(clicks, x, y)", "path": "pyautogui/pyautogui/__init__.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "asweigart/pyautogui", "stars": 9327, "license": "bsd-3-clause", "language": "python", "size": 2346}
{"docstring": "\"\"\"\nReturns an (x, y) tuple of the point that has progressed a proportion ``n`` along the line defined by the two\n``x1``, ``y1`` and ``x2``, ``y2`` coordinates.\n\nThis function was copied from pytweening module, so that it can be called even if PyTweening is not installed.\n\"\"\"\n", "func_signal": "def getPointOnLine(x1, y1, x2, y2, n):\n", "code": "x = ((x2 - x1) * n) + x1\ny = ((y2 - y1) * n) + y1\nreturn (x, y)", "path": "pyautogui/pyautogui/__init__.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "asweigart/pyautogui", "stars": 9327, "license": "bsd-3-clause", "language": "python", "size": 2346}
{"docstring": "# moving the mouse\n", "func_signal": "def test_moveTo(self):\n", "code": "desired = self.center\npyautogui.moveTo(*desired)\nmousepos = P(*pyautogui.position())\nself.assertEqual(mousepos, desired)\n\n# no coordinate specified (should be a NO-OP)\npyautogui.moveTo(None, None)\nmousepos = P(*pyautogui.position())\nself.assertEqual(mousepos, desired)\n\n# moving the mouse to a new location\ndesired += P(42, 42)\npyautogui.moveTo(*desired)\nmousepos = P(*pyautogui.position())\nself.assertEqual(mousepos, desired)\n\n# moving the mouse over time (1/5 second)\ndesired -= P(42, 42)\npyautogui.moveTo(desired.x, desired.y, duration=0.2)\nmousepos = P(*pyautogui.position())\nself.assertEqual(mousepos, desired)\n\n# Passing a list instead of separate x and y.\ndesired += P(42, 42)\npyautogui.moveTo(list(desired))\nmousepos = P(*pyautogui.position())\nself.assertEqual(mousepos, desired)\n\n# Passing a tuple instead of separate x and y.\ndesired += P(42, 42)\npyautogui.moveTo(tuple(desired))\nmousepos = P(*pyautogui.position())\nself.assertEqual(mousepos, desired)\n\n# Passing a sequence-like object instead of separate x and y.\ndesired -= P(42, 42)\npyautogui.moveTo(desired)\nmousepos = P(*pyautogui.position())\nself.assertEqual(mousepos, desired)", "path": "pyautogui/tests/test_pyautogui.py", "commit_date": "2020-02-21 00:00:00", "repo_name": "asweigart/pyautogui", "stars": 9327, "license": "bsd-3-clause", "language": "python", "size": 2346}
{"docstring": "\"\"\"Gets the comma token at the start of commandStr.\n\nGiven ',' returns ','\nGiven '  ,', returns '  ,'\n\nRaises an exception if a comma isn't found.\n\"\"\"\n", "func_signal": "def _getCommaToken(commandStr):\n", "code": "pattern = re.compile(r\"^((\\s*),)\")\nmo = pattern.search(commandStr)\nif mo is None:\n    raise PyAutoGUIException(\"Invalid command at index 0: a comma was expected\")\n\nreturn mo.group(1)", "path": "pyautogui/pyautogui/__init__.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "asweigart/pyautogui", "stars": 9327, "license": "bsd-3-clause", "language": "python", "size": 2346}
{"docstring": "\"\"\"Performs a keyboard key press without the release. This will put that\nkey in a held down state.\n\nNOTE: For some reason, this does not seem to cause key repeats like would\nhappen if a keyboard key was held down on a text field.\n\nArgs:\n  key (str): The key to be pressed down. The valid names are listed in\n  KEYBOARD_KEYS.\n\nReturns:\n  None\n\"\"\"\n", "func_signal": "def keyDown(key, logScreenshot=None, _pause=True):\n", "code": "if len(key) > 1:\n    key = key.lower()\n\n_logScreenshot(logScreenshot, \"keyDown\", key, folder=\".\")\nplatformModule._keyDown(key)", "path": "pyautogui/pyautogui/__init__.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "asweigart/pyautogui", "stars": 9327, "license": "bsd-3-clause", "language": "python", "size": 2346}
{"docstring": "\"\"\"This function is meant to be run from the command line. It will\nautomatically display the location and RGB of the mouse cursor.\"\"\"\n", "func_signal": "def displayMousePosition(xOffset=0, yOffset=0):\n", "code": "try:\n    runningIDLE = sys.stdin.__module__.startswith(\"idlelib\")\nexcept:\n    runningIDLE = False\n\nprint(\"Press Ctrl-C to quit.\")\nif xOffset != 0 or yOffset != 0:\n    print(\"xOffset: %s yOffset: %s\" % (xOffset, yOffset))\ntry:\n    while True:\n        # Get and print the mouse coordinates.\n        x, y = position()\n        positionStr = \"X: \" + str(x - xOffset).rjust(4) + \" Y: \" + str(y - yOffset).rjust(4)\n        if not onScreen(x - xOffset, y - yOffset) or sys.platform == \"darwin\":\n            # Pixel color can only be found for the primary monitor, and also not on mac due to the screenshot having the mouse cursor in the way.\n            pixelColor = (\"NaN\", \"NaN\", \"NaN\")\n        else:\n            pixelColor = pyscreeze.screenshot().getpixel(\n                (x, y)\n            )  # NOTE: On Windows & Linux, getpixel() returns a 3-integer tuple, but on macOS it returns a 4-integer tuple.\n        positionStr += \" RGB: (\" + str(pixelColor[0]).rjust(3)\n        positionStr += \", \" + str(pixelColor[1]).rjust(3)\n        positionStr += \", \" + str(pixelColor[2]).rjust(3) + \")\"\n        sys.stdout.write(positionStr)\n        if not runningIDLE:\n            # If this is a terminal, than we can erase the text by printing \\b backspaces.\n            sys.stdout.write(\"\\b\" * len(positionStr))\n        else:\n            # If this isn't a terminal (i.e. IDLE) then we can only append more text. Print a newline instead and pause a second (so we don't send too much output).\n            sys.stdout.write(\"\\n\")\n            time.sleep(1)\n        sys.stdout.flush()\nexcept KeyboardInterrupt:\n    sys.stdout.write(\"\\n\")\n    sys.stdout.flush()", "path": "pyautogui/pyautogui/__init__.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "asweigart/pyautogui", "stars": 9327, "license": "bsd-3-clause", "language": "python", "size": 2346}
{"docstring": "# TODO - currently this just checks that scrolling doesn't result in an error.\n", "func_signal": "def test_scroll(self):\n", "code": "pyautogui.scroll(1)\npyautogui.scroll(-1)\npyautogui.hscroll(1)\npyautogui.hscroll(-1)\npyautogui.vscroll(1)\npyautogui.vscroll(-1)", "path": "pyautogui/tests/test_pyautogui.py", "commit_date": "2020-02-21 00:00:00", "repo_name": "asweigart/pyautogui", "stars": 9327, "license": "bsd-3-clause", "language": "python", "size": 2346}
{"docstring": "\"\"\"\nA helper function that creates a screenshot to act as a logging mechanism. When a PyAutoGUI function is called,\nthis function is also called to capture the state of the screen when that function was called.\n\nIf ``logScreenshot`` is ``False`` (or None and the ``LOG_SCREENSHOTS`` constant is ``False``), no screenshot is taken.\n\nThe ``funcName`` argument is a string of the calling function's name. It's used in the screenshot's filename.\n\nThe ``funcArgs`` argument is a string describing the arguments passed to the calling function. It's limited to\ntweleve characters to keep it short.\n\nThe ``folder`` argument is the folder to place the screenshot file in, and defaults to the current working directory.\n\"\"\"\n", "func_signal": "def _logScreenshot(logScreenshot, funcName, funcArgs, folder=\".\"):\n", "code": "if logScreenshot == False:\n    return  # Don't take a screenshot.\nif logScreenshot is None and LOG_SCREENSHOTS == False:\n    return  # Don't take a screenshot.\n\n# Ensure that the \"specifics\" string isn't too long for the filename:\nif len(funcArgs) > 12:\n    funcArgs = funcArgs[:12] + \"...\"\n\nnow = datetime.datetime.now()\nfilename = \"%s-%s-%s_%s-%s-%s-%s_%s_%s.png\" % (\n    now.year,\n    str(now.month).rjust(2, \"0\"),\n    str(now.day).rjust(2, \"0\"),\n    now.hour,\n    now.minute,\n    now.second,\n    str(now.microsecond)[:3],\n    funcName,\n    funcArgs,\n)\nfilepath = os.path.join(folder, filename)\n\n# Delete the oldest screenshot if we've reached the maximum:\nif (LOG_SCREENSHOTS_LIMIT is not None) and (len(G_LOG_SCREENSHOTS_FILENAMES) >= LOG_SCREENSHOTS_LIMIT):\n    os.unlink(os.path.join(folder, G_LOG_SCREENSHOTS_FILENAMES[0]))\n    del G_LOG_SCREENSHOTS_FILENAMES[0]\n\nscreenshot(filepath)\nG_LOG_SCREENSHOTS_FILENAMES.append(filename)", "path": "pyautogui/pyautogui/__init__.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "asweigart/pyautogui", "stars": 9327, "license": "bsd-3-clause", "language": "python", "size": 2346}
{"docstring": "\"\"\"\nReturns a ``Point`` object based on ``firstArg`` and ``secondArg``, which are the first two arguments passed to\nseveral PyAutoGUI functions. If ``firstArg`` and ``secondArg`` are both ``None``, returns the current mouse cursor\nposition.\n\n``firstArg`` and ``secondArg`` can be integers, a sequence of integers, or a string representing an image filename\nto find on the screen (and return the center coordinates of).\n\"\"\"\n", "func_signal": "def _normalizeXYArgs(firstArg, secondArg):\n", "code": "if firstArg is None and secondArg is None:\n    return position()\n\nelif isinstance(firstArg, str):\n    # If x is a string, we assume it's an image filename to locate on the screen:\n    try:\n        location = locateOnScreen(firstArg)\n        # The following code only runs if pyscreeze.USE_IMAGE_NOT_FOUND_EXCEPTION is not set to True, meaning that\n        # locateOnScreen() returns None if the image can't be found.\n        if location is not None:\n            return center(location)\n        else:\n            return None\n    except pyscreeze.ImageNotFoundException:\n        raise ImageNotFoundException\n\n    return center(locateOnScreen(firstArg))\n\nelif isinstance(firstArg, collectionsSequence):\n    if len(firstArg) == 2:\n        # firstArg is a two-integer tuple: (x, y)\n        if secondArg is None:\n            return Point(int(firstArg[0]), int(firstArg[1]))\n        else:\n            raise PyAutoGUIException(\n                \"When passing a sequence for firstArg, secondArg must not be passed (received {0}).\".format(\n                    repr(secondArg)\n                )\n            )\n    elif len(firstArg) == 4:\n        # firstArg is a four-integer tuple, (left, top, width, height), we should return the center point\n        if secondArg is None:\n            return center(firstArg)\n        else:\n            raise PyAutoGUIException(\n                \"When passing a sequence for firstArg, secondArg must not be passed and default to None (received {0}).\".format(\n                    repr(secondArg)\n                )\n            )\n    else:\n        raise PyAutoGUIException(\n            \"The supplied sequence must have exactly 2 or exactly 4 elements ({0} were received).\".format(\n                len(firstArg)\n            )\n        )\nelse:\n    return Point(int(firstArg), int(secondArg))  # firstArg and secondArg are just x and y number values", "path": "pyautogui/pyautogui/__init__.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "asweigart/pyautogui", "stars": 9327, "license": "bsd-3-clause", "language": "python", "size": 2346}
{"docstring": "# TODO feature not finished\n", "func_signal": "def _snapshot(tag, folder=None, region=None, radius=None):\n", "code": "if region is not None and radius is not None:\n    raise Exception(\"Either region or radius arguments (or neither) can be passed to snapshot, but not both\")\n\nif radius is not None:\n    x, y = platformModule._position()\n\nif folder is None:\n    folder = os.getcwd()\n\nnow = datetime.datetime.now()\nfilename = \"%s-%s-%s_%s-%s-%s-%s_%s.png\" % (\n    now.year,\n    str(now.month).rjust(2, \"0\"),\n    str(now.day).rjust(2, \"0\"),\n    now.hour,\n    now.minute,\n    now.second,\n    str(now.microsecond)[:3],\n    tag,\n)\nfilepath = os.path.join(folder, filename)\nscreenshot(filepath)", "path": "pyautogui/pyautogui/__init__.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "asweigart/pyautogui", "stars": 9327, "license": "bsd-3-clause", "language": "python", "size": 2346}
{"docstring": "\"\"\"Performs a triple click.\n\nThis is a wrapper function for click('left', x, y, 3, interval).\n\nThe x and y parameters detail where the mouse event happens. If None, the\ncurrent mouse position is used. If a float value, it is rounded down. If\noutside the boundaries of the screen, the event happens at edge of the\nscreen.\n\nArgs:\n  x (int, float, None, tuple, optional): The x position on the screen where the\n    click happens. None by default. If tuple, this is used for x and y.\n    If x is a str, it's considered a filename of an image to find on\n    the screen with locateOnScreen() and click the center of.\n  y (int, float, None, optional): The y position on the screen where the\n    click happens. None by default.\n  interval (float, optional): The number of seconds in between each click,\n    if the number of clicks is greater than 1. 0.0 by default, for no\n    pause in between clicks.\n  button (str, int, optional): The mouse button released. TODO\n\nReturns:\n  None\n\nRaises:\n  PyAutoGUIException: If button is not one of 'left', 'middle', 'right', 1, 2, 3, 4,\n    5, 6, or 7\n\"\"\"\n# Multiple clicks work different in OSX\n", "func_signal": "def tripleClick(x=None, y=None, interval=0.0, button=LEFT, duration=0.0, tween=linear, logScreenshot=None, _pause=True):\n", "code": "if sys.platform == \"darwin\":\n    x, y = _normalizeXYArgs(x, y)\n    _mouseMoveDrag(\"move\", x, y, 0, 0, duration=0, tween=None)\n    x, y = platformModule._position()\n    _logScreenshot(logScreenshot, \"click\", \"%s,3,%s,%s\" % (x, y), folder=\".\")\n    platformModule._multiClick(x, y, button, 3)\nelse:\n    # Click for Windows or Linux:\n    click(x, y, 3, interval, button, duration, tween, logScreenshot, _pause=False)", "path": "pyautogui/pyautogui/__init__.py", "commit_date": "2020-10-06 00:00:00", "repo_name": "asweigart/pyautogui", "stars": 9327, "license": "bsd-3-clause", "language": "python", "size": 2346}
{"docstring": "\"\"\"\nDetermine a default set of validators for any unique_together constraints.\n\"\"\"\n", "func_signal": "def get_unique_together_validators(self):\n", "code": "model_class_inheritance_tree = (\n    [self.Meta.model] +\n    list(self.Meta.model._meta.parents)\n)\n\n# The field names we're passing though here only include fields\n# which may map onto a model field. Any dotted field name lookups\n# cannot map to a field, and must be a traversal, so we're not\n# including those.\nfield_sources = OrderedDict(\n    (field.field_name, field.source) for field in self._writable_fields\n    if (field.source != '*') and ('.' not in field.source)\n)\n\n# Special Case: Add read_only fields with defaults.\nfield_sources.update(OrderedDict(\n    (field.field_name, field.source) for field in self.fields.values()\n    if (field.read_only) and (field.default != empty) and (field.source != '*') and ('.' not in field.source)\n))\n\n# Invert so we can find the serializer field names that correspond to\n# the model field names in the unique_together sets. This also allows\n# us to check that multiple fields don't map to the same source.\nsource_map = defaultdict(list)\nfor name, source in field_sources.items():\n    source_map[source].append(name)\n\n# Note that we make sure to check `unique_together` both on the\n# base model class, but also on any parent classes.\nvalidators = []\nfor parent_class in model_class_inheritance_tree:\n    for unique_together in parent_class._meta.unique_together:\n        # Skip if serializer does not map to all unique together sources\n        if not set(source_map).issuperset(unique_together):\n            continue\n\n        for source in unique_together:\n            assert len(source_map[source]) == 1, (\n                \"Unable to create `UniqueTogetherValidator` for \"\n                \"`{model}.{field}` as `{serializer}` has multiple \"\n                \"fields ({fields}) that map to this model field. \"\n                \"Either remove the extra fields, or override \"\n                \"`Meta.validators` with a `UniqueTogetherValidator` \"\n                \"using the desired field names.\"\n                .format(\n                    model=self.Meta.model.__name__,\n                    serializer=self.__class__.__name__,\n                    field=source,\n                    fields=', '.join(source_map[source]),\n                )\n            )\n\n        field_names = tuple(source_map[f][0] for f in unique_together)\n        validator = UniqueTogetherValidator(\n            queryset=parent_class._default_manager,\n            fields=field_names\n        )\n        validators.append(validator)\nreturn validators", "path": "django-rest-framework/rest_framework/serializers.py", "commit_date": "2020-09-28 00:00:00", "repo_name": "encode/django-rest-framework", "stars": 27235, "license": "other", "language": "python", "size": 52361}
{"docstring": "\"\"\"\nReturn the dict of field names -> field instances that should be\nused for `self.fields` when instantiating the serializer.\n\"\"\"\n", "func_signal": "def get_fields(self):\n", "code": "if self.url_field_name is None:\n    self.url_field_name = api_settings.URL_FIELD_NAME\n\nassert hasattr(self, 'Meta'), (\n    'Class {serializer_class} missing \"Meta\" attribute'.format(\n        serializer_class=self.__class__.__name__\n    )\n)\nassert hasattr(self.Meta, 'model'), (\n    'Class {serializer_class} missing \"Meta.model\" attribute'.format(\n        serializer_class=self.__class__.__name__\n    )\n)\nif model_meta.is_abstract_model(self.Meta.model):\n    raise ValueError(\n        'Cannot use ModelSerializer with Abstract Models.'\n    )\n\ndeclared_fields = copy.deepcopy(self._declared_fields)\nmodel = getattr(self.Meta, 'model')\ndepth = getattr(self.Meta, 'depth', 0)\n\nif depth is not None:\n    assert depth >= 0, \"'depth' may not be negative.\"\n    assert depth <= 10, \"'depth' may not be greater than 10.\"\n\n# Retrieve metadata about fields & relationships on the model class.\ninfo = model_meta.get_field_info(model)\nfield_names = self.get_field_names(declared_fields, info)\n\n# Determine any extra field arguments and hidden fields that\n# should be included\nextra_kwargs = self.get_extra_kwargs()\nextra_kwargs, hidden_fields = self.get_uniqueness_extra_kwargs(\n    field_names, declared_fields, extra_kwargs\n)\n\n# Determine the fields that should be included on the serializer.\nfields = OrderedDict()\n\nfor field_name in field_names:\n    # If the field is explicitly declared on the class then use that.\n    if field_name in declared_fields:\n        fields[field_name] = declared_fields[field_name]\n        continue\n\n    extra_field_kwargs = extra_kwargs.get(field_name, {})\n    source = extra_field_kwargs.get('source', '*')\n    if source == '*':\n        source = field_name\n\n    # Determine the serializer field class and keyword arguments.\n    field_class, field_kwargs = self.build_field(\n        source, info, model, depth\n    )\n\n    # Include any kwargs defined in `Meta.extra_kwargs`\n    field_kwargs = self.include_extra_kwargs(\n        field_kwargs, extra_field_kwargs\n    )\n\n    # Create the serializer field.\n    fields[field_name] = field_class(**field_kwargs)\n\n# Add in any hidden fields.\nfields.update(hidden_fields)\n\nreturn fields", "path": "django-rest-framework/rest_framework/serializers.py", "commit_date": "2020-09-28 00:00:00", "repo_name": "encode/django-rest-framework", "stars": 27235, "license": "other", "language": "python", "size": 52361}
{"docstring": "\"\"\"\nThis proves that when an AttributeError is raised inside of the request.user\nproperty, that we can handle this and report the true, underlying error.\n\"\"\"\n", "func_signal": "def test_calling_user_fails_when_attribute_error_is_raised(self):\n", "code": "class AuthRaisesAttributeError:\n    def authenticate(self, request):\n        self.MISSPELLED_NAME_THAT_DOESNT_EXIST\n\nrequest = Request(self.wrapped_request, authenticators=(AuthRaisesAttributeError(),))\n\n# The middleware processes the underlying Django request, sets anonymous user\nassert self.wrapped_request.user.is_anonymous\n\n# The DRF request object does not have a user and should run authenticators\nexpected = r\"no attribute 'MISSPELLED_NAME_THAT_DOESNT_EXIST'\"\nwith pytest.raises(WrappedAttributeError, match=expected):\n    request.user\n\nwith pytest.raises(WrappedAttributeError, match=expected):\n    hasattr(request, 'user')\n\nwith pytest.raises(WrappedAttributeError, match=expected):\n    login(request, self.user)", "path": "django-rest-framework/tests/test_request.py", "commit_date": "2020-10-09 00:00:00", "repo_name": "encode/django-rest-framework", "stars": 27235, "license": "other", "language": "python", "size": 52361}
{"docstring": "\"\"\"\nEnsure request.POST returns no content for POST request with file content.\n\"\"\"\n", "func_signal": "def test_request_POST_with_files(self):\n", "code": "upload = SimpleUploadedFile(\"file.txt\", b\"file_content\")\nrequest = Request(factory.post('/', {'upload': upload}))\nrequest.parsers = (FormParser(), MultiPartParser())\nassert list(request.POST) == []\nassert list(request.FILES) == ['upload']", "path": "django-rest-framework/tests/test_request.py", "commit_date": "2020-10-09 00:00:00", "repo_name": "encode/django-rest-framework", "stars": 27235, "license": "other", "language": "python", "size": 52361}
{"docstring": "\"\"\"\nInclude any 'extra_kwargs' that have been included for this field,\npossibly removing any incompatible existing keyword arguments.\n\"\"\"\n", "func_signal": "def include_extra_kwargs(self, kwargs, extra_kwargs):\n", "code": "if extra_kwargs.get('read_only', False):\n    for attr in [\n        'required', 'default', 'allow_blank', 'allow_null',\n        'min_length', 'max_length', 'min_value', 'max_value',\n        'validators', 'queryset'\n    ]:\n        kwargs.pop(attr, None)\n\nif extra_kwargs.get('default') and kwargs.get('required') is False:\n    kwargs.pop('required')\n\nif extra_kwargs.get('read_only', kwargs.get('read_only', False)):\n    extra_kwargs.pop('required', None)  # Read only fields should always omit the 'required' argument.\n\nkwargs.update(extra_kwargs)\n\nreturn kwargs", "path": "django-rest-framework/rest_framework/serializers.py", "commit_date": "2020-09-28 00:00:00", "repo_name": "encode/django-rest-framework", "stars": 27235, "license": "other", "language": "python", "size": 52361}
{"docstring": "\"\"\"\nTest a relationship that spans a GenericForeignKey field.\nIE. A forward generic relationship.\n\"\"\"\n\n", "func_signal": "def test_generic_fk(self):\n", "code": "class TagSerializer(serializers.ModelSerializer):\n    tagged_item = serializers.StringRelatedField()\n\n    class Meta:\n        model = Tag\n        fields = ('tag', 'tagged_item')\n\nserializer = TagSerializer(Tag.objects.all(), many=True)\nexpected = [\n    {\n        'tag': 'django',\n        'tagged_item': 'Bookmark: https://www.djangoproject.com/'\n    },\n    {\n        'tag': 'python',\n        'tagged_item': 'Bookmark: https://www.djangoproject.com/'\n    },\n    {\n        'tag': 'reminder',\n        'tagged_item': 'Note: Remember the milk'\n    }\n]\nassert serializer.data == expected", "path": "django-rest-framework/tests/generic_relations/test_generic_relations.py", "commit_date": "2019-04-30 00:00:00", "repo_name": "encode/django-rest-framework", "stars": 27235, "license": "other", "language": "python", "size": 52361}
{"docstring": "\"\"\"\nWe have a bit of extra checking around this in order to provide\ndescriptive messages when something goes wrong, but this method is\nessentially just:\n\n    return ExampleModel.objects.create(**validated_data)\n\nIf there are many to many fields present on the instance then they\ncannot be set until the model is instantiated, in which case the\nimplementation is like so:\n\n    example_relationship = validated_data.pop('example_relationship')\n    instance = ExampleModel.objects.create(**validated_data)\n    instance.example_relationship = example_relationship\n    return instance\n\nThe default implementation also does not handle nested relationships.\nIf you want to support writable nested relationships you'll need\nto write an explicit `.create()` method.\n\"\"\"\n", "func_signal": "def create(self, validated_data):\n", "code": "raise_errors_on_nested_writes('create', self, validated_data)\n\nModelClass = self.Meta.model\n\n# Remove many-to-many relationships from validated_data.\n# They are not valid arguments to the default `.create()` method,\n# as they require that the instance has already been saved.\ninfo = model_meta.get_field_info(ModelClass)\nmany_to_many = {}\nfor field_name, relation_info in info.relations.items():\n    if relation_info.to_many and (field_name in validated_data):\n        many_to_many[field_name] = validated_data.pop(field_name)\n\ntry:\n    instance = ModelClass._default_manager.create(**validated_data)\nexcept TypeError:\n    tb = traceback.format_exc()\n    msg = (\n        'Got a `TypeError` when calling `%s.%s.create()`. '\n        'This may be because you have a writable field on the '\n        'serializer class that is not a valid argument to '\n        '`%s.%s.create()`. You may need to make the field '\n        'read-only, or override the %s.create() method to handle '\n        'this correctly.\\nOriginal exception was:\\n %s' %\n        (\n            ModelClass.__name__,\n            ModelClass._default_manager.name,\n            ModelClass.__name__,\n            ModelClass._default_manager.name,\n            self.__class__.__name__,\n            tb\n        )\n    )\n    raise TypeError(msg)\n\n# Save many-to-many relationships after the instance is created.\nif many_to_many:\n    for field_name, value in many_to_many.items():\n        field = getattr(instance, field_name)\n        field.set(value)\n\nreturn instance", "path": "django-rest-framework/rest_framework/serializers.py", "commit_date": "2020-09-28 00:00:00", "repo_name": "encode/django-rest-framework", "stars": 27235, "license": "other", "language": "python", "size": 52361}
{"docstring": "# We override the default field access in order to support\n# nested HTML forms.\n", "func_signal": "def get_value(self, dictionary):\n", "code": "if html.is_html_input(dictionary):\n    return html.parse_html_dict(dictionary, prefix=self.field_name) or empty\nreturn dictionary.get(self.field_name, empty)", "path": "django-rest-framework/rest_framework/serializers.py", "commit_date": "2020-09-28 00:00:00", "repo_name": "encode/django-rest-framework", "stars": 27235, "license": "other", "language": "python", "size": 52361}
{"docstring": "# Pass request object through session middleware so session is\n# available to login and logout functions\n", "func_signal": "def setUp(self):\n", "code": "self.wrapped_request = factory.get('/')\nself.request = Request(self.wrapped_request)\n\ndef dummy_get_response(request):  # pragma: no cover\n    return None\n\nSessionMiddleware(dummy_get_response).process_request(self.wrapped_request)\nAuthenticationMiddleware(dummy_get_response).process_request(self.wrapped_request)\n\nUser.objects.create_user('ringo', 'starr@thebeatles.com', 'yellow')\nself.user = authenticate(username='ringo', password='yellow')", "path": "django-rest-framework/tests/test_request.py", "commit_date": "2020-10-09 00:00:00", "repo_name": "encode/django-rest-framework", "stars": 27235, "license": "other", "language": "python", "size": 52361}
{"docstring": "# We override this method in order to automatically create\n# `ListSerializer` classes instead when `many=True` is set.\n", "func_signal": "def __new__(cls, *args, **kwargs):\n", "code": "if kwargs.pop('many', False):\n    return cls.many_init(*args, **kwargs)\nreturn super().__new__(cls, *args, **kwargs)", "path": "django-rest-framework/rest_framework/serializers.py", "commit_date": "2020-09-28 00:00:00", "repo_name": "encode/django-rest-framework", "stars": 27235, "license": "other", "language": "python", "size": 52361}
{"docstring": "\"\"\"\nEnsure request.data returns content for PUT request with form content.\n\"\"\"\n", "func_signal": "def test_standard_behaviour_determines_form_content_PUT(self):\n", "code": "data = {'qwerty': 'uiop'}\nrequest = Request(factory.put('/', data))\nrequest.parsers = (FormParser(), MultiPartParser())\nassert list(request.data.items()) == list(data.items())", "path": "django-rest-framework/tests/test_request.py", "commit_date": "2020-10-09 00:00:00", "repo_name": "encode/django-rest-framework", "stars": 27235, "license": "other", "language": "python", "size": 52361}
{"docstring": "\"\"\"\nEnsure request.data returns empty QueryDict for HEAD request.\n\"\"\"\n", "func_signal": "def test_standard_behaviour_determines_no_content_HEAD(self):\n", "code": "request = Request(factory.head('/'))\nassert request.data == {}", "path": "django-rest-framework/tests/test_request.py", "commit_date": "2020-10-09 00:00:00", "repo_name": "encode/django-rest-framework", "stars": 27235, "license": "other", "language": "python", "size": 52361}
{"docstring": "\"\"\"\nEnsure request.data returns empty QueryDict for GET request.\n\"\"\"\n", "func_signal": "def test_standard_behaviour_determines_no_content_GET(self):\n", "code": "request = Request(factory.get('/'))\nassert request.data == {}", "path": "django-rest-framework/tests/test_request.py", "commit_date": "2020-10-09 00:00:00", "repo_name": "encode/django-rest-framework", "stars": 27235, "license": "other", "language": "python", "size": 52361}
{"docstring": "\"\"\"\nWe override the default `run_validation`, because the validation\nperformed by validators and the `.validate()` method should\nbe coerced into an error dictionary with a 'non_fields_error' key.\n\"\"\"\n", "func_signal": "def run_validation(self, data=empty):\n", "code": "(is_empty_value, data) = self.validate_empty_values(data)\nif is_empty_value:\n    return data\n\nvalue = self.to_internal_value(data)\ntry:\n    self.run_validators(value)\n    value = self.validate(value)\n    assert value is not None, '.validate() should return the validated data'\nexcept (ValidationError, DjangoValidationError) as exc:\n    raise ValidationError(detail=as_serializer_error(exc))\n\nreturn value", "path": "django-rest-framework/rest_framework/serializers.py", "commit_date": "2020-09-28 00:00:00", "repo_name": "encode/django-rest-framework", "stars": 27235, "license": "other", "language": "python", "size": 52361}
{"docstring": "\"\"\"\nCreate regular model fields.\n\"\"\"\n", "func_signal": "def build_standard_field(self, field_name, model_field):\n", "code": "field_mapping = ClassLookupDict(self.serializer_field_mapping)\n\nfield_class = field_mapping[model_field]\nfield_kwargs = get_field_kwargs(field_name, model_field)\n\n# Special case to handle when a OneToOneField is also the primary key\nif model_field.one_to_one and model_field.primary_key:\n    field_class = self.serializer_related_field\n    field_kwargs['queryset'] = model_field.related_model.objects\n\nif 'choices' in field_kwargs:\n    # Fields with choices get coerced into `ChoiceField`\n    # instead of using their regular typed field.\n    field_class = self.serializer_choice_field\n    # Some model fields may introduce kwargs that would not be valid\n    # for the choice field. We need to strip these out.\n    # Eg. models.DecimalField(max_digits=3, decimal_places=1, choices=DECIMAL_CHOICES)\n    valid_kwargs = {\n        'read_only', 'write_only',\n        'required', 'default', 'initial', 'source',\n        'label', 'help_text', 'style',\n        'error_messages', 'validators', 'allow_null', 'allow_blank',\n        'choices'\n    }\n    for key in list(field_kwargs):\n        if key not in valid_kwargs:\n            field_kwargs.pop(key)\n\nif not issubclass(field_class, ModelField):\n    # `model_field` is only valid for the fallback case of\n    # `ModelField`, which is used when no other typed field\n    # matched to the model field.\n    field_kwargs.pop('model_field', None)\n\nif not issubclass(field_class, CharField) and not issubclass(field_class, ChoiceField):\n    # `allow_blank` is only valid for textual fields.\n    field_kwargs.pop('allow_blank', None)\n\nis_django_jsonfield = hasattr(models, 'JSONField') and isinstance(model_field, models.JSONField)\nif (postgres_fields and isinstance(model_field, postgres_fields.JSONField)) or is_django_jsonfield:\n    # Populate the `encoder` argument of `JSONField` instances generated\n    # for the model `JSONField`.\n    field_kwargs['encoder'] = getattr(model_field, 'encoder', None)\n    if is_django_jsonfield:\n        field_kwargs['decoder'] = getattr(model_field, 'decoder', None)\n\nif postgres_fields and isinstance(model_field, postgres_fields.ArrayField):\n    # Populate the `child` argument on `ListField` instances generated\n    # for the PostgreSQL specific `ArrayField`.\n    child_model_field = model_field.base_field\n    child_field_class, child_field_kwargs = self.build_standard_field(\n        'child', child_model_field\n    )\n    field_kwargs['child'] = child_field_class(**child_field_kwargs)\n\nreturn field_class, field_kwargs", "path": "django-rest-framework/rest_framework/serializers.py", "commit_date": "2020-09-28 00:00:00", "repo_name": "encode/django-rest-framework", "stars": 27235, "license": "other", "language": "python", "size": 52361}
{"docstring": "\"\"\"\nList of dicts of native values <- List of dicts of primitive datatypes.\n\"\"\"\n", "func_signal": "def to_internal_value(self, data):\n", "code": "if html.is_html_input(data):\n    data = html.parse_html_list(data, default=[])\n\nif not isinstance(data, list):\n    message = self.error_messages['not_a_list'].format(\n        input_type=type(data).__name__\n    )\n    raise ValidationError({\n        api_settings.NON_FIELD_ERRORS_KEY: [message]\n    }, code='not_a_list')\n\nif not self.allow_empty and len(data) == 0:\n    message = self.error_messages['empty']\n    raise ValidationError({\n        api_settings.NON_FIELD_ERRORS_KEY: [message]\n    }, code='empty')\n\nret = []\nerrors = []\n\nfor item in data:\n    try:\n        validated = self.child.run_validation(item)\n    except ValidationError as exc:\n        errors.append(exc.detail)\n    else:\n        ret.append(validated)\n        errors.append({})\n\nif any(errors):\n    raise ValidationError(errors)\n\nreturn ret", "path": "django-rest-framework/rest_framework/serializers.py", "commit_date": "2020-09-28 00:00:00", "repo_name": "encode/django-rest-framework", "stars": 27235, "license": "other", "language": "python", "size": 52361}
{"docstring": "\"\"\"\nDict of native values <- Dict of primitive datatypes.\n\"\"\"\n", "func_signal": "def to_internal_value(self, data):\n", "code": "if not isinstance(data, Mapping):\n    message = self.error_messages['invalid'].format(\n        datatype=type(data).__name__\n    )\n    raise ValidationError({\n        api_settings.NON_FIELD_ERRORS_KEY: [message]\n    }, code='invalid')\n\nret = OrderedDict()\nerrors = OrderedDict()\nfields = self._writable_fields\n\nfor field in fields:\n    validate_method = getattr(self, 'validate_' + field.field_name, None)\n    primitive_value = field.get_value(data)\n    try:\n        validated_value = field.run_validation(primitive_value)\n        if validate_method is not None:\n            validated_value = validate_method(validated_value)\n    except ValidationError as exc:\n        errors[field.field_name] = exc.detail\n    except DjangoValidationError as exc:\n        errors[field.field_name] = get_error_detail(exc)\n    except SkipField:\n        pass\n    else:\n        set_value(ret, field.source_attrs, validated_value)\n\nif errors:\n    raise ValidationError(errors)\n\nreturn ret", "path": "django-rest-framework/rest_framework/serializers.py", "commit_date": "2020-09-28 00:00:00", "repo_name": "encode/django-rest-framework", "stars": 27235, "license": "other", "language": "python", "size": 52361}
{"docstring": "\"\"\"\nThis method implements the creation of a `ListSerializer` parent\nclass when `many=True` is used. You can customize it if you need to\ncontrol which keyword arguments are passed to the parent, and\nwhich are passed to the child.\n\nNote that we're over-cautious in passing most arguments to both parent\nand child classes in order to try to cover the general case. If you're\noverriding this method you'll probably want something much simpler, eg:\n\n@classmethod\ndef many_init(cls, *args, **kwargs):\n    kwargs['child'] = cls()\n    return CustomListSerializer(*args, **kwargs)\n\"\"\"\n", "func_signal": "def many_init(cls, *args, **kwargs):\n", "code": "allow_empty = kwargs.pop('allow_empty', None)\nchild_serializer = cls(*args, **kwargs)\nlist_kwargs = {\n    'child': child_serializer,\n}\nif allow_empty is not None:\n    list_kwargs['allow_empty'] = allow_empty\nlist_kwargs.update({\n    key: value for key, value in kwargs.items()\n    if key in LIST_SERIALIZER_KWARGS\n})\nmeta = getattr(cls, 'Meta', None)\nlist_serializer_class = getattr(meta, 'list_serializer_class', ListSerializer)\nreturn list_serializer_class(*args, **list_kwargs)", "path": "django-rest-framework/rest_framework/serializers.py", "commit_date": "2020-09-28 00:00:00", "repo_name": "encode/django-rest-framework", "stars": 27235, "license": "other", "language": "python", "size": 52361}
{"docstring": "# This implementation is the same as the default,\n# except that we use lists, rather than dicts, as the empty case.\n", "func_signal": "def is_valid(self, raise_exception=False):\n", "code": "assert hasattr(self, 'initial_data'), (\n    'Cannot call `.is_valid()` as no `data=` keyword argument was '\n    'passed when instantiating the serializer instance.'\n)\n\nif not hasattr(self, '_validated_data'):\n    try:\n        self._validated_data = self.run_validation(self.initial_data)\n    except ValidationError as exc:\n        self._validated_data = []\n        self._errors = exc.detail\n    else:\n        self._errors = []\n\nif self._errors and raise_exception:\n    raise ValidationError(self.errors)\n\nreturn not bool(self._errors)", "path": "django-rest-framework/rest_framework/serializers.py", "commit_date": "2020-09-28 00:00:00", "repo_name": "encode/django-rest-framework", "stars": 27235, "license": "other", "language": "python", "size": 52361}
{"docstring": "# Coerce HTTP header value to unicode.\n", "func_signal": "def unicode_http_header(value):\n", "code": "if isinstance(value, bytes):\n    return value.decode('iso-8859-1')\nreturn value", "path": "django-rest-framework/rest_framework/compat.py", "commit_date": "2020-08-17 00:00:00", "repo_name": "encode/django-rest-framework", "stars": 27235, "license": "other", "language": "python", "size": 52361}
{"docstring": "# TODO merge with setup_socket?\n# Setup socket to server\n", "func_signal": "def setup_polling(self):\n", "code": "def on_message(message):\n    message = json.loads(message)\n    if 'command' in message:\n        # Handle server commands\n        if message['command'] == 'alive':\n            if 'data' in message and message['data'] == 'vis_alive':\n                logger.info('Visdom successfully connected to server')\n                self.socket_alive = True\n                self.socket_connection_achieved = True\n            else:\n                logger.warn('Visdom server failed handshake, may not '\n                            'be properly connected')\n    if 'target' in message:\n        for handler in list(\n                self.event_handlers.get(message['target'], [])):\n            handler(message)\n\ndef on_close(ws):\n    self.socket_alive = False\n\ndef run_socket(*args):\n    # open a socket\n    resp_json = self._handle_post(\n        \"{0}:{1}{2}/vis_socket_wrap\".format(self.server, self.port,\n                                            self.base_url),\n        data=json.dumps({'message_type': 'init'}),\n    )\n    resp = json.loads(resp_json)\n    self.vis_sid = resp['sid']\n    while self.use_socket:\n        resp_json = self._handle_post(\n            \"{0}:{1}{2}/vis_socket_wrap\".format(self.server, self.port,\n                                                self.base_url),\n            data=json.dumps(\n                {'message_type': 'query', 'sid': self.vis_sid}\n            ),\n        )\n        resp = json.loads(resp_json)\n        for msg in resp['messages']:\n            on_message(msg)\n        time.sleep(0.1)\n\n# Start listening thread\nself.socket_thread = threading.Thread(\n    target=run_socket,\n    name='Visdom-Socket-Thread'\n)\nself.socket_thread.start()", "path": "visdom/py/visdom/__init__.py", "commit_date": "2020-11-30 00:00:00", "repo_name": "fossasia/visdom", "stars": 9861, "license": "apache-2.0", "language": "python", "size": 36543}
{"docstring": "\"\"\"\nThis function prints text in a box. It takes as input an `text` string.\nNo specific `opts` are currently supported.\n\"\"\"\n", "func_signal": "def text(self, text, win=None, env=None, opts=None, append=False):\n", "code": "opts = {} if opts is None else opts\n_title2str(opts)\n_assert_opts(opts)\ndata = [{'content': text, 'type': 'text'}]\n\nif append:\n    endpoint = 'update'\nelse:\n    endpoint = 'events'\n\nreturn self._send({\n    'data': data,\n    'win': win,\n    'eid': env,\n    'opts': opts,\n}, endpoint=endpoint)", "path": "visdom/py/visdom/__init__.py", "commit_date": "2020-11-30 00:00:00", "repo_name": "fossasia/visdom", "stars": 9861, "license": "apache-2.0", "language": "python", "size": 36543}
{"docstring": "\"\"\"\nThis function returns md5 hash of the contents\nof a window if it exists on the server.\nReturns None, otherwise\n\"\"\"\n", "func_signal": "def win_hash(self, win, env=None):\n", "code": "try:\n    e = self._win_hash_wrap(win, env)\nexcept ConnectionError:\n    print(\"Error connecting to Visdom server!\")\n    return None\n\nif re.match(r\"([a-fA-F\\d]{32})\", e):\n    return e\n\nreturn None", "path": "visdom/py/visdom/__init__.py", "commit_date": "2020-11-30 00:00:00", "repo_name": "fossasia/visdom", "stars": 9861, "license": "apache-2.0", "language": "python", "size": 36543}
{"docstring": "\"\"\"\nThis function draws an img. It takes as input an `CxHxW` or `HxW` tensor\n`img` that contains the image. The array values can be float in [0,1] or\nuint8 in [0, 255].\n\"\"\"\n", "func_signal": "def image(self, img, win=None, env=None, opts=None):\n", "code": "opts = {} if opts is None else opts\n_title2str(opts)\n_assert_opts(opts)\nopts['width'] = opts.get('width', img.shape[img.ndim - 1])\nopts['height'] = opts.get('height', img.shape[img.ndim - 2])\n\nnchannels = img.shape[0] if img.ndim == 3 else 1\nif nchannels == 1:\n    img = np.squeeze(img)\n    img = img[np.newaxis, :, :].repeat(3, axis=0)\n\nif 'float' in str(img.dtype):\n    if img.max() <= 1:\n        img = img * 255.\n    img = np.uint8(img)\n\nimg = np.transpose(img, (1, 2, 0))\nim = Image.fromarray(img)\nbuf = BytesIO()\nimage_type = 'png'\nimsave_args = {}\nif 'jpgquality' in opts:\n    image_type = 'jpeg'\n    imsave_args['quality'] = opts['jpgquality']\n\nim.save(buf, format=image_type.upper(), **imsave_args)\n\nb64encoded = b64.b64encode(buf.getvalue()).decode('utf-8')\n\ndata = [{\n    'content': {\n        'src': 'data:image/' + image_type + ';base64,' + b64encoded,\n        'caption': opts.get('caption'),\n    },\n    'type': 'image_history' if opts.get('store_history') else 'image',\n}]\n\nendpoint = 'events'\nif opts.get('store_history'):\n    if win is not None and self.win_exists(win, env):\n        endpoint = 'update'\n\nreturn self._send({\n    'data': data,\n    'win': win,\n    'eid': env,\n    'opts': opts,\n}, endpoint=endpoint)", "path": "visdom/py/visdom/__init__.py", "commit_date": "2020-11-30 00:00:00", "repo_name": "fossasia/visdom", "stars": 9861, "license": "apache-2.0", "language": "python", "size": 36543}
{"docstring": "\"\"\"\nThis function takes the contents of a visdom log and replays them to\nthe current server to restore the state or handle any missing entries.\n\"\"\"\n", "func_signal": "def replay_log(self, log_filename):\n", "code": "with open(log_filename) as f:\n    log_entries = f.readlines()\nfor entry in log_entries:\n    endpoint, msg = json.loads(entry)\n    self._send(msg, endpoint, from_log=True)", "path": "visdom/py/visdom/__init__.py", "commit_date": "2020-11-30 00:00:00", "repo_name": "fossasia/visdom", "stars": 9861, "license": "apache-2.0", "language": "python", "size": 36543}
{"docstring": "\"\"\"\nThis function draws a regular, stacked, or grouped bar plot. It takes as\ninput an `N` or `NxM` tensor `X` that specifies the height of each\nbar. If `X` contains `M` columns, the values corresponding to each row\nare either stacked or grouped (dependending on how `opts.stacked` is\nset). In addition to `X`, an (optional) `N` tensor `Y` can be specified\nthat contains the corresponding x-axis values.\n\nThe following plot-specific `opts` are currently supported:\n\n    - `opts.rownames`: `list` containing x-axis labels\n- `opts.stacked` : stack multiple columns in `X`\n    - `opts.legend`  : `list` containing legend labels\n\"\"\"\n", "func_signal": "def bar(self, X, Y=None, win=None, env=None, opts=None):\n", "code": "X = np.squeeze(X)\nassert X.ndim == 1 or X.ndim == 2, 'X should be one or two-dimensional'\nif X.ndim == 1:\n    if opts is not None and opts.get('legend') is not None:\n        X = X[None, :]\n        assert opts.get('rownames') is None, \\\n            'both rownames and legend cannot be specified \\\n            for one-dimensional X values'\n    else:\n        X = X[:, None]\nif Y is not None:\n    Y = np.squeeze(Y)\n    assert Y.ndim == 1, 'Y should be one-dimensional'\n    assert len(X) == len(Y), 'sizes of X and Y should match'\nelse:\n    Y = np.arange(1, len(X) + 1)\n\nopts = {} if opts is None else opts\nopts['stacked'] = opts.get('stacked', False)\n\n_title2str(opts)\n_assert_opts(opts)\n\nif opts.get('rownames') is not None:\n    assert len(opts['rownames']) == X.shape[0], \\\n        'number of row names should match number of rows in X'\n\nif opts.get('legend') is not None:\n    assert len(opts['legend']) == X.shape[1], \\\n        'number of legend labels must match number of columns in X'\n\ndata = []\nfor k in range(X.shape[1]):\n    _data = {\n        'y': X.take(k, 1).tolist(),\n        'x': opts.get('rownames', Y.tolist()),\n        'type': 'bar',\n    }\n    if opts.get('legend'):\n        _data['name'] = opts['legend'][k]\n    data.append(_data)\n\nreturn self._send({\n    'data': data,\n    'win': win,\n    'eid': env,\n    'layout': _opts2layout(opts),\n    'opts': opts,\n})", "path": "visdom/py/visdom/__init__.py", "commit_date": "2020-11-30 00:00:00", "repo_name": "fossasia/visdom", "stars": 9861, "license": "apache-2.0", "language": "python", "size": 36543}
{"docstring": "\"\"\"\nThis function returns all the window data for a specified window in\nan environment. Use `win=None` to get all the windows in the given\nenvironment. Env defaults to main\n\"\"\"\n\n", "func_signal": "def get_window_data(self, win=None, env=None):\n", "code": "return self._send(\n    msg={'win': win, 'eid': env},\n    endpoint='win_data',\n    create=False,\n)", "path": "visdom/py/visdom/__init__.py", "commit_date": "2020-11-30 00:00:00", "repo_name": "fossasia/visdom", "stars": 9861, "license": "apache-2.0", "language": "python", "size": 36543}
{"docstring": "\"\"\"\nThis function draws a Matplotlib `plot`. The function supports\none plot-specific option: `resizable`. When set to `True` the plot\nis resized with the pane. You need `beautifulsoup4` and `lxml`\npackages installed to use this option.\n\"\"\"\n", "func_signal": "def matplot(self, plot, opts=None, env=None, win=None):\n", "code": "opts = {} if opts is None else opts\n_title2str(opts)\n_assert_opts(opts)\n\n# write plot to SVG buffer:\nbuffer = StringIO()\nplot.savefig(buffer, format='svg')\nbuffer.seek(0)\nsvg = buffer.read()\nbuffer.close()\n\nif opts.get('resizable', False):\n    if not BS4_AVAILABLE:\n        raise ImportError(\"No module named 'bs4'\")\n    else:\n        try:\n            soup = bs4.BeautifulSoup(svg, 'xml')\n        except bs4.FeatureNotFound as e:\n            import six\n            six.raise_from(ImportError(\"No module named 'lxml'\"), e)\n        height = soup.svg.attrs.pop('height', None)\n        width = soup.svg.attrs.pop('width', None)\n        svg = str(soup)\nelse:\n    height = None\n    width = None\n\n# show SVG:\nif 'height' not in opts:\n    height = height or re.search(r'height\\=\"([0-9\\.]*)pt\"', svg)\n    if height is not None:\n        if not isstr(height):\n            height = height.group(1)\n        height = height.replace(\"pt\",\"00\")\n        opts['height'] = 1.4 * int(math.ceil(float(height)))\nif 'width' not in opts:\n    width = width or re.search(r'width\\=\"([0-9\\.]*)pt\"', svg)\n    if width is not None:\n        if not isstr(width):\n            width = width.group(1)\n        width = width.replace(\"pt\",\"00\")\n        opts['width'] = 1.35 * int(math.ceil(float(width)))\nreturn self.svg(svgstr=svg, opts=opts, env=env, win=win)", "path": "visdom/py/visdom/__init__.py", "commit_date": "2020-11-30 00:00:00", "repo_name": "fossasia/visdom", "stars": 9861, "license": "apache-2.0", "language": "python", "size": 36543}
{"docstring": "\"\"\"\nThis function draws a histogram of the specified data. It takes as input\nan `N` tensor `X` that specifies the data of which to construct the\nhistogram.\n\nThe following plot-specific `opts` are currently supported:\n\n- `opts.numbins`: number of bins (`number`; default = 30)\n\"\"\"\n\n", "func_signal": "def histogram(self, X, win=None, env=None, opts=None):\n", "code": "X = np.squeeze(X)\nassert X.ndim == 1, 'X should be one-dimensional'\n\nopts = {} if opts is None else opts\nopts['numbins'] = opts.get('numbins', min(30, len(X)))\n_title2str(opts)\n_assert_opts(opts)\n\nminx, maxx = X.min(), X.max()\nbins = np.histogram(X, bins=opts['numbins'], range=(minx, maxx))[0]\nlinrange = np.linspace(minx, maxx, opts['numbins'])\n\nreturn self.bar(\n    X=bins,\n    Y=linrange,\n    opts=opts,\n    win=win,\n    env=env\n)", "path": "visdom/py/visdom/__init__.py", "commit_date": "2020-11-30 00:00:00", "repo_name": "fossasia/visdom", "stars": 9861, "license": "apache-2.0", "language": "python", "size": 36543}
{"docstring": "\"\"\"\nThis function plays audio. It takes as input the filename of the audio\nfile or an `N` tensor containing the waveform (use an `Nx2` matrix for\nstereo audio). The function does not support any plot-specific `opts`.\n\nThe following `opts` are supported:\n\n- `opts.sample_frequency`: sample frequency (`integer` > 0; default = 44100)\n\"\"\"\n", "func_signal": "def audio(self, tensor=None, audiofile=None, win=None, env=None, opts=None):\n", "code": "opts = {} if opts is None else opts\nopts['sample_frequency'] = opts.get('sample_frequency', 44100)\n_title2str(opts)\n_assert_opts(opts)\nassert tensor is not None or audiofile is not None, \\\n    'should specify audio tensor or file'\nif tensor is not None:\n    assert tensor.ndim == 1 or (tensor.ndim == 2 and tensor.shape[1] == 2), \\\n        'tensor should be 1D vector or 2D matrix with 2 columns'\n\nif tensor is not None:\n    import scipy.io.wavfile # type: ignore\n    import tempfile\n    audiofile = os.path.join(\n        tempfile.gettempdir(),\n        '%s.wav' % next(tempfile._get_candidate_names()))\n    tensor = np.int16(tensor / np.max(np.abs(tensor)) * 32767)\n    scipy.io.wavfile.write(audiofile, opts.get('sample_frequency'), tensor)\n\nextension = audiofile.split('.')[-1].lower()\nmimetypes = {'wav': 'wav', 'mp3': 'mp3', 'ogg': 'ogg', 'flac': 'flac'}\nmimetype = mimetypes.get(extension)\nassert mimetype is not None, 'unknown audio type: %s' % extension\n\nbytestr = loadfile(audiofile)\naudiodata = \"\"\"\n    <audio controls>\n        <source type=\"audio/%s\" src=\"data:audio/%s;base64,%s\">\n        Your browser does not support the audio tag.\n    </audio>\n\"\"\" % (mimetype, mimetype, base64.b64encode(bytestr).decode('utf-8'))\nopts['height'] = 80\nopts['width'] = 330\nreturn self.text(text=audiodata, win=win, env=env, opts=opts)", "path": "visdom/py/visdom/__init__.py", "commit_date": "2020-11-30 00:00:00", "repo_name": "fossasia/visdom", "stars": 9861, "license": "apache-2.0", "language": "python", "size": 36543}
{"docstring": "\"\"\"This function deletes a specific environment.\"\"\"\n", "func_signal": "def delete_env(self, env):\n", "code": "return self._send(\n    msg={'eid': env},\n    endpoint='delete_env'\n)", "path": "visdom/py/visdom/__init__.py", "commit_date": "2020-11-30 00:00:00", "repo_name": "fossasia/visdom", "stars": 9861, "license": "apache-2.0", "language": "python", "size": 36543}
{"docstring": "\"\"\"\nThis function draws a mesh plot from a set of vertices defined in an\n`Nx2` or `Nx3` matrix `X`, and polygons defined in an optional `Mx2` or\n`Mx3` matrix `Y`.\n\nThe following `opts` are supported:\n\n- `opts.color`: color (`string`)\n- `opts.opacity`: opacity of polygons (`number` between 0 and 1)\n\"\"\"\n", "func_signal": "def mesh(self, X, Y=None, win=None, env=None, opts=None):\n", "code": "opts = {} if opts is None else opts\n_title2str(opts)\n_assert_opts(opts)\n\nX = np.asarray(X)\nassert X.ndim == 2, 'X must have 2 dimensions'\nassert X.shape[1] == 2 or X.shape[1] == 3, 'X must have 2 or 3 columns'\nis3d = X.shape[1] == 3\n\nispoly = Y is not None\nif ispoly:\n    Y = np.asarray(Y)\n    assert Y.ndim == 2, 'Y must have 2 dimensions'\n    assert Y.shape[1] == X.shape[1], \\\n        'X and Y must have same number of columns'\n\ndata = [{\n    'x': X[:, 0].tolist(),\n    'y': X[:, 1].tolist(),\n    'z': X[:, 2].tolist() if is3d else None,\n    'i': Y[:, 0].tolist() if ispoly else None,\n    'j': Y[:, 1].tolist() if ispoly else None,\n    'k': Y[:, 2].tolist() if is3d and ispoly else None,\n    'color': opts.get('color'),\n    'opacity': opts.get('opacity'),\n    'type': 'mesh3d' if is3d else 'mesh',\n}]\nreturn self._send({\n    'data': data,\n    'win': win,\n    'eid': env,\n    'layout': _opts2layout(opts),\n    'opts': opts,\n})", "path": "visdom/py/visdom/__init__.py", "commit_date": "2020-11-30 00:00:00", "repo_name": "fossasia/visdom", "stars": 9861, "license": "apache-2.0", "language": "python", "size": 36543}
{"docstring": "\"\"\"\nThis function allows the user to save envs that are alive on the\nTornado server. The envs can be specified as a list of env ids.\n\"\"\"\n", "func_signal": "def save(self, envs):\n", "code": "assert isinstance(envs, list), 'envs should be a list'\nif len(envs) > 0:\n    for env in envs:\n        assert isstr(env), 'env should be a string'\n\nreturn self._send({\n    'data': envs,\n}, 'save')", "path": "visdom/py/visdom/__init__.py", "commit_date": "2020-11-30 00:00:00", "repo_name": "fossasia/visdom", "stars": 9861, "license": "apache-2.0", "language": "python", "size": 36543}
{"docstring": "\"\"\"This function allows the user to fork environments.\"\"\"\n", "func_signal": "def fork_env(self, prev_eid, eid):\n", "code": "assert isstr(prev_eid), 'prev_eid should be a string'\nassert isstr(eid), 'eid should be a string'\n\nreturn self._send(\n    msg={'prev_eid': prev_eid, 'eid': eid},\n    endpoint='fork_env'\n)", "path": "visdom/py/visdom/__init__.py", "commit_date": "2020-11-30 00:00:00", "repo_name": "fossasia/visdom", "stars": 9861, "license": "apache-2.0", "language": "python", "size": 36543}
{"docstring": "\"\"\"\nThis follows the [PyAV cookbook]\n(http://docs.mikeboers.com/pyav/develop/cookbook/numpy.html#generating-video)\n\"\"\"\n", "func_signal": "def _encode(self, tensor, fps):\n", "code": "import av  # type: ignore\n\n# Float tensors are assumed to have a domain of [0, 1], for\n# backward-compatibility with OpenCV.\nif np.issubdtype(tensor.dtype, np.floating):\n    tensor = (255 * tensor)\ntensor = tensor.astype(np.uint8).clip(0, 255)\n\n# Use BGR for backward-compatibility with OpenCV\npixelformats = {1: 'gray', 3: 'bgr24'}\npixelformat = pixelformats[tensor.shape[3]]\n\ncontent = BytesIO()\ncontainer = av.open(content, 'w', 'mp4')\n\nstream = container.add_stream('h264', rate=fps)\nstream.height = tensor.shape[1]\nstream.width = tensor.shape[2]\nstream.pix_fmt = 'yuv420p'\n\nfor arr in tensor:\n    frame = av.VideoFrame.from_ndarray(arr, format=pixelformat)\n    container.mux(stream.encode(frame))\n# Flushing the stream here causes a deprecation warning in ffmpeg\n# https://ffmpeg.zeranoe.com/forum/viewtopic.php?t=3678\n# It's old and benign and possibly only apparent in homebrew-installed ffmpeg?\ncontainer.mux(stream.encode())\n\ncontainer.close()\ncontent = content.getvalue()\n\nreturn content, 'mp4'", "path": "visdom/py/visdom/__init__.py", "commit_date": "2020-11-30 00:00:00", "repo_name": "fossasia/visdom", "stars": 9861, "license": "apache-2.0", "language": "python", "size": 36543}
{"docstring": "\"\"\"\nThis function draws a stem plot. It takes as input an `N` or `NxM`tensor\n`X` that specifies the values of the `N` points in the `M` time series.\nAn optional `N` or `NxM` tensor `Y` containing timestamps can be given\nas well; if `Y` is an `N` tensor then all `M` time series are assumed to\nhave the same timestamps.\n\nThe following `opts` are supported:\n\n- `opts.colormap`: colormap (`string`; default = `'Viridis'`)\n- `opts.legend`  : `list` containing legend names\n\"\"\"\n\n", "func_signal": "def stem(self, X, Y=None, win=None, env=None, opts=None):\n", "code": "X = np.squeeze(X)\nassert X.ndim == 1 or X.ndim == 2, 'X should be one or two-dimensional'\nif X.ndim == 1:\n    X = X[:, None]\n\nif Y is None:\n    Y = np.arange(1, X.shape[0] + 1)\nif Y.ndim == 1:\n    Y = Y[:, None]\nassert Y.shape[0] == X.shape[0], 'number of rows in X and Y must match'\nassert Y.shape[1] == 1 or Y.shape[1] == X.shape[1], \\\n    'Y should be a single column or the same number of columns as X'\n\nif Y.shape[1] < X.shape[1]:\n    Y = np.tile(Y, (1, X.shape[1]))\n\nZ = np.zeros((Y.shape))  # Zeros\nwith np.errstate(divide='ignore', invalid='ignore'):\n    N = Z / Z                # NaNs\nX = np.column_stack((Z, X, N)).reshape((X.shape[0] * 3, X.shape[1]))\nY = np.column_stack((Y, Y, N)).reshape((Y.shape[0] * 3, Y.shape[1]))\n\ndata = np.column_stack((Y.flatten(), X.flatten()))\nlabels = np.arange(1, X.shape[1] + 1)[None, :]\nlabels = np.tile(labels, (X.shape[0], 1)).flatten()\n\nopts = {} if opts is None else opts\nopts['mode'] = 'lines'\n_title2str(opts)\n_assert_opts(opts)\n\nreturn self.scatter(X=data, Y=labels, opts=opts, win=win, env=env)", "path": "visdom/py/visdom/__init__.py", "commit_date": "2020-11-30 00:00:00", "repo_name": "fossasia/visdom", "stars": 9861, "license": "apache-2.0", "language": "python", "size": 36543}
{"docstring": "\"\"\"\nThis function sets all the window data for a specified window in\nan environment. Use `win=None` to set the data for all the windows in\nthe given environment. Env defaults to main. `data` should be as returned\nfrom `get_window_data`.\n\"\"\"\n", "func_signal": "def set_window_data(self, data, win=None, env=None):\n", "code": "return self._send(\n    msg={'win': win, 'eid': env, 'data': data},\n    endpoint='win_data',\n    create=False,\n)", "path": "visdom/py/visdom/__init__.py", "commit_date": "2020-11-30 00:00:00", "repo_name": "fossasia/visdom", "stars": 9861, "license": "apache-2.0", "language": "python", "size": 36543}
{"docstring": "\"\"\"\nThis function allows pushing new options to an existing plot window\nwithout updating the content\n\"\"\"\n", "func_signal": "def update_window_opts(self, win, opts, env=None):\n", "code": "data_to_send = {\n    'win': win,\n    'eid': env,\n    'layout': _opts2layout(opts),\n    'opts': opts,\n}\nreturn self._send(data_to_send, endpoint='update')", "path": "visdom/py/visdom/__init__.py", "commit_date": "2020-11-30 00:00:00", "repo_name": "fossasia/visdom", "stars": 9861, "license": "apache-2.0", "language": "python", "size": 36543}
{"docstring": "\"\"\"\nThis function returns a string indicating whether\nor not a window exists on the server already. ['true' or 'false']\nReturns False if something went wrong\n\"\"\"\n", "func_signal": "def _win_exists_wrap(self, win, env=None):\n", "code": "assert win is not None\n\nreturn self._send({\n    'win': win,\n    'eid': env,\n}, endpoint='win_exists', quiet=True)", "path": "visdom/py/visdom/__init__.py", "commit_date": "2020-11-30 00:00:00", "repo_name": "fossasia/visdom", "stars": 9861, "license": "apache-2.0", "language": "python", "size": 36543}
{"docstring": "\"\"\"\nThis function returns a bool indicating whether or\nnot the server is connected.\n\"\"\"\n", "func_signal": "def _has_connection(self):\n", "code": "return (self.win_exists('') is not None) and \\\n    (self.socket_alive or not self.use_socket)", "path": "visdom/py/visdom/__init__.py", "commit_date": "2020-11-30 00:00:00", "repo_name": "fossasia/visdom", "stars": 9861, "license": "apache-2.0", "language": "python", "size": 36543}
{"docstring": "# check font size fallback works on small canvas\n", "func_signal": "def test_small_canvas():\n", "code": "wc = WordCloud(max_words=50, width=21, height=21)\nwc.generate(SMALL_CANVAS)\nassert len(wc.layout_) > 0", "path": "word_cloud/test/test_wordcloud.py", "commit_date": "2020-09-25 00:00:00", "repo_name": "amueller/word_cloud", "stars": 9883, "license": "mit", "language": "python", "size": 119777}
{"docstring": "\"\"\"Draw mask contour on a pillow image.\"\"\"\n", "func_signal": "def _draw_contour(self, img):\n", "code": "if self.mask is None or self.contour_width == 0:\n    return img\n\nmask = self._get_bolean_mask(self.mask) * 255\ncontour = Image.fromarray(mask.astype(np.uint8))\ncontour = contour.resize(img.size)\ncontour = contour.filter(ImageFilter.FIND_EDGES)\ncontour = np.array(contour)\n\n# make sure borders are not drawn before changing width\ncontour[[0, -1], :] = 0\ncontour[:, [0, -1]] = 0\n\n# use gaussian to change width, divide by 10 to give more resolution\nradius = self.contour_width / 10\ncontour = Image.fromarray(contour)\ncontour = contour.filter(ImageFilter.GaussianBlur(radius=radius))\ncontour = np.array(contour) > 0\ncontour = np.dstack((contour, contour, contour))\n\n# color the contour\nret = np.array(img) * np.invert(contour)\nif self.contour_color != 'black':\n    color = Image.new(img.mode, img.size, self.contour_color)\n    ret += np.array(color) * contour\n\nreturn Image.fromarray(ret)", "path": "word_cloud/wordcloud/wordcloud.py", "commit_date": "2020-09-25 00:00:00", "repo_name": "amueller/word_cloud", "stars": 9883, "license": "mit", "language": "python", "size": 119777}
{"docstring": "# grey is special as it's a corner case\n", "func_signal": "def test_single_color_func_grey():\n", "code": "random = Random(42)\n\nred_function = get_single_color_func('darkgrey')\nassert red_function(random_state=random) == 'rgb(181, 181, 181)'\nassert red_function(random_state=random) == 'rgb(56, 56, 56)'", "path": "word_cloud/test/test_wordcloud.py", "commit_date": "2020-09-25 00:00:00", "repo_name": "amueller/word_cloud", "stars": 9883, "license": "mit", "language": "python", "size": 119777}
{"docstring": "\"\"\"Create a color function which returns a single hue and saturation with.\ndifferent values (HSV). Accepted values are color strings as usable by\nPIL/Pillow.\n\n>>> color_func1 = get_single_color_func('deepskyblue')\n>>> color_func2 = get_single_color_func('#00b4d2')\n\"\"\"\n", "func_signal": "def get_single_color_func(color):\n", "code": "old_r, old_g, old_b = ImageColor.getrgb(color)\nrgb_max = 255.\nh, s, v = colorsys.rgb_to_hsv(old_r / rgb_max, old_g / rgb_max,\n                              old_b / rgb_max)\n\ndef single_color_func(word=None, font_size=None, position=None,\n                      orientation=None, font_path=None, random_state=None):\n    \"\"\"Random color generation.\n\n    Additional coloring method. It picks a random value with hue and\n    saturation based on the color given to the generating function.\n\n    Parameters\n    ----------\n    word, font_size, position, orientation  : ignored.\n\n    random_state : random.Random object or None, (default=None)\n      If a random object is given, this is used for generating random\n      numbers.\n\n    \"\"\"\n    if random_state is None:\n        random_state = Random()\n    r, g, b = colorsys.hsv_to_rgb(h, s, random_state.uniform(0.2, 1))\n    return 'rgb({:.0f}, {:.0f}, {:.0f})'.format(r * rgb_max, g * rgb_max,\n                                                b * rgb_max)\nreturn single_color_func", "path": "word_cloud/wordcloud/wordcloud.py", "commit_date": "2020-09-25 00:00:00", "repo_name": "amueller/word_cloud", "stars": 9883, "license": "mit", "language": "python", "size": 119777}
{"docstring": "# test that default word cloud creation and conversions work\n", "func_signal": "def test_default():\n", "code": "wc = WordCloud(max_words=50)\nwc.generate(THIS)\n\n# check for proper word extraction\nassert len(wc.words_) == wc.max_words\n\n# check that we got enough words\nassert len(wc.layout_) == wc.max_words\n\n# check image export\nwc_image = wc.to_image()\nassert wc_image.size == (wc.width, wc.height)\n\n# check that numpy conversion works\nwc_array = np.array(wc)\nassert_array_equal(wc_array, wc.to_array())\n\n# check size\nassert wc_array.shape == (wc.height, wc.width, 3)", "path": "word_cloud/test/test_wordcloud.py", "commit_date": "2020-09-25 00:00:00", "repo_name": "amueller/word_cloud", "stars": 9883, "license": "mit", "language": "python", "size": 119777}
{"docstring": "# test mask contour is created, learn more at:\n# https://github.com/amueller/word_cloud/pull/348#issuecomment-370883873\n", "func_signal": "def test_mask_contour():\n", "code": "mask = np.zeros((234, 456), dtype=np.int)\nmask[100:150, 300:400] = 255\n\nsm = WordCloud(mask=mask, contour_width=1, contour_color='blue')\nsm.generate(THIS)\nsm_array = np.array(sm)\nsm_total = sm_array[100:150, 300:400].sum()\n\nlg = WordCloud(mask=mask, contour_width=20, contour_color='blue')\nlg.generate(THIS)\nlg_array = np.array(lg)\nlg_total = lg_array[100:150, 300:400].sum()\n\nsc = WordCloud(mask=mask, contour_width=1, scale=2, contour_color='blue')\nsc.generate(THIS)\nsc_array = np.array(sc)\nsc_total = sc_array[100:150, 300:400].sum()\n\n# test `contour_width`\nassert lg_total > sm_total\n\n# test contour varies with `scale`\nassert sc_total > sm_total\n\n# test `contour_color`\nassert all(sm_array[100, 300] == [0, 0, 255])", "path": "word_cloud/test/test_wordcloud.py", "commit_date": "2020-09-25 00:00:00", "repo_name": "amueller/word_cloud", "stars": 9883, "license": "mit", "language": "python", "size": 119777}
{"docstring": "# test that word processing is influenced by `regexp`\n", "func_signal": "def test_process_text_regexp_parameter():\n", "code": "wc = WordCloud(max_words=50, regexp=r'\\w{5}')\nwords = wc.process_text(THIS)\n\nassert 'than' not in words", "path": "word_cloud/test/test_wordcloud.py", "commit_date": "2020-09-25 00:00:00", "repo_name": "amueller/word_cloud", "stars": 9883, "license": "mit", "language": "python", "size": 119777}
{"docstring": "# check no exception is raised when default colour is used\n", "func_signal": "def test_recolor_too_small_set_default():\n", "code": "colouring = np.array(Image.new('RGB', size=(20, 20)))\nwc = WordCloud(max_words=50, width=30, height=30, min_font_size=1).generate(THIS)\nimage_colors = ImageColorGenerator(colouring, default_color=(0, 0, 0))\nwc.recolor(color_func=image_colors)", "path": "word_cloud/test/test_wordcloud.py", "commit_date": "2020-09-25 00:00:00", "repo_name": "amueller/word_cloud", "stars": 9883, "license": "mit", "language": "python", "size": 119777}
{"docstring": "\"\"\"Export to SVG.\n\nFont is assumed to be available to the SVG reader. Otherwise, text\ncoordinates may produce artifacts when rendered with replacement font.\nIt is also possible to include a subset of the original font in WOFF\nformat using ``embed_font`` (requires `fontTools`).\n\nNote that some renderers do not handle glyphs the same way, and may\ndiffer from ``to_image`` result. In particular, Complex Text Layout may\nnot be supported. In this typesetting, the shape or positioning of a\ngrapheme depends on its relation to other graphemes.\n\nPillow, since version 4.2.0, supports CTL using ``libraqm``. However,\ndue to dependencies, this feature is not always enabled. Hence, the\nsame rendering differences may appear in ``to_image``. As this\nrasterized output is used to compute the layout, this also affects the\nlayout generation. Use ``PIL.features.check`` to test availability of\n``raqm``.\n\nConsistant rendering is therefore expected if both Pillow and the SVG\nrenderer have the same support of CTL.\n\nContour drawing is not supported.\n\nParameters\n----------\nembed_font : bool, default=False\n    Whether to include font inside resulting SVG file.\n\noptimize_embedded_font : bool, default=True\n    Whether to be aggressive when embedding a font, to reduce size. In\n    particular, hinting tables are dropped, which may introduce slight\n    changes to character shapes (w.r.t. `to_image` baseline).\n\nembed_image : bool, default=False\n    Whether to include rasterized image inside resulting SVG file.\n    Useful for debugging.\n\nReturns\n-------\ncontent : string\n    Word cloud image as SVG string\n\"\"\"\n\n# TODO should add option to specify URL for font (i.e. WOFF file)\n\n# Make sure layout is generated\n", "func_signal": "def to_svg(self, embed_font=False, optimize_embedded_font=True, embed_image=False):\n", "code": "self._check_generated()\n\n# Get output size, in pixels\nif self.mask is not None:\n    width = self.mask.shape[1]\n    height = self.mask.shape[0]\nelse:\n    height, width = self.height, self.width\n\n# Get max font size\nif self.max_font_size is None:\n    max_font_size = max(w[1] for w in self.layout_)\nelse:\n    max_font_size = self.max_font_size\n\n# Text buffer\nresult = []\n\n# Get font information\nfont = ImageFont.truetype(self.font_path, int(max_font_size * self.scale))\nraw_font_family, raw_font_style = font.getname()\n# TODO properly escape/quote this name?\nfont_family = repr(raw_font_family)\n# TODO better support for uncommon font styles/weights?\nraw_font_style = raw_font_style.lower()\nif 'bold' in raw_font_style:\n    font_weight = 'bold'\nelse:\n    font_weight = 'normal'\nif 'italic' in raw_font_style:\n    font_style = 'italic'\nelif 'oblique' in raw_font_style:\n    font_style = 'oblique'\nelse:\n    font_style = 'normal'\n\n# Add header\nresult.append(\n    '<svg'\n    ' xmlns=\"http://www.w3.org/2000/svg\"'\n    ' width=\"{}\"'\n    ' height=\"{}\"'\n    '>'\n    .format(\n        width * self.scale,\n        height * self.scale\n    )\n)\n\n# Embed font, if requested\nif embed_font:\n\n    # Import here, to avoid hard dependency on fonttools\n    import fontTools\n    import fontTools.subset\n\n    # Subset options\n    options = fontTools.subset.Options(\n\n        # Small impact on character shapes, but reduce size a lot\n        hinting=not optimize_embedded_font,\n\n        # On small subsets, can improve size\n        desubroutinize=optimize_embedded_font,\n\n        # Try to be lenient\n        ignore_missing_glyphs=True,\n    )\n\n    # Load and subset font\n    ttf = fontTools.subset.load_font(self.font_path, options)\n    subsetter = fontTools.subset.Subsetter(options)\n    characters = {c for item in self.layout_ for c in item[0][0]}\n    text = ''.join(characters)\n    subsetter.populate(text=text)\n    subsetter.subset(ttf)\n\n    # Export as WOFF\n    # TODO is there a better method, i.e. directly export to WOFF?\n    buffer = io.BytesIO()\n    ttf.saveXML(buffer)\n    buffer.seek(0)\n    woff = fontTools.ttLib.TTFont(flavor='woff')\n    woff.importXML(buffer)\n\n    # Create stylesheet with embedded font face\n    buffer = io.BytesIO()\n    woff.save(buffer)\n    data = base64.b64encode(buffer.getbuffer()).decode('ascii')\n    url = 'data:application/font-woff;charset=utf-8;base64,' + data\n    result.append(\n        '<style>'\n        '@font-face{{'\n        'font-family:{};'\n        'font-weight:{};'\n        'font-style:{};'\n        'src:url(\"{}\")format(\"woff\");'\n        '}}'\n        '</style>'\n        .format(\n            font_family,\n            font_weight,\n            font_style,\n            url\n        )\n    )\n\n# Select global style\nresult.append(\n    '<style>'\n    'text{{'\n    'font-family:{};'\n    'font-weight:{};'\n    'font-style:{};'\n    '}}'\n    '</style>'\n    .format(\n        font_family,\n        font_weight,\n        font_style\n    )\n)\n\n# Add background\nif self.background_color is not None:\n    result.append(\n        '<rect'\n        ' width=\"100%\"'\n        ' height=\"100%\"'\n        ' style=\"fill:{}\"'\n        '>'\n        '</rect>'\n        .format(self.background_color)\n    )\n\n# Embed image, useful for debug purpose\nif embed_image:\n    image = self.to_image()\n    data = io.BytesIO()\n    image.save(data, format='JPEG')\n    data = base64.b64encode(data.getbuffer()).decode('ascii')\n    result.append(\n        '<image'\n        ' width=\"100%\"'\n        ' height=\"100%\"'\n        ' href=\"data:image/jpg;base64,{}\"'\n        '/>'\n        .format(data)\n    )\n\n# For each word in layout\nfor (word, count), font_size, (y, x), orientation, color in self.layout_:\n    x *= self.scale\n    y *= self.scale\n\n    # Get text metrics\n    font = ImageFont.truetype(self.font_path, int(font_size * self.scale))\n    (size_x, size_y), (offset_x, offset_y) = font.font.getsize(word)\n    ascent, descent = font.getmetrics()\n\n    # Compute text bounding box\n    min_x = -offset_x\n    max_x = size_x - offset_x\n    max_y = ascent - offset_y\n\n    # Compute text attributes\n    attributes = {}\n    if orientation == Image.ROTATE_90:\n        x += max_y\n        y += max_x - min_x\n        transform = 'translate({},{}) rotate(-90)'.format(x, y)\n    else:\n        x += min_x\n        y += max_y\n        transform = 'translate({},{})'.format(x, y)\n\n    # Create node\n    attributes = ' '.join('{}=\"{}\"'.format(k, v) for k, v in attributes.items())\n    result.append(\n        '<text'\n        ' transform=\"{}\"'\n        ' font-size=\"{}\"'\n        ' style=\"fill:{}\"'\n        '>'\n        '{}'\n        '</text>'\n        .format(\n            transform,\n            font_size * self.scale,\n            color,\n            saxutils.escape(word)\n        )\n    )\n\n# TODO draw contour\n\n# Complete SVG file\nresult.append('</svg>')\nreturn '\\n'.join(result)", "path": "word_cloud/wordcloud/wordcloud.py", "commit_date": "2020-09-25 00:00:00", "repo_name": "amueller/word_cloud", "stars": 9883, "license": "mit", "language": "python", "size": 119777}
{"docstring": "\"\"\"Export to image file.\n\nParameters\n----------\nfilename : string\n    Location to write to.\n\nReturns\n-------\nself\n\"\"\"\n\n", "func_signal": "def to_file(self, filename):\n", "code": "img = self.to_image()\nimg.save(filename, optimize=True)\nreturn self", "path": "word_cloud/wordcloud/wordcloud.py", "commit_date": "2020-09-25 00:00:00", "repo_name": "amueller/word_cloud", "stars": 9883, "license": "mit", "language": "python", "size": 119777}
{"docstring": "\"\"\"Generate a color for a given word using a fixed image.\"\"\"\n# get the font to get the box size\n", "func_signal": "def __call__(self, word, font_size, font_path, position, orientation, **kwargs):\n", "code": "font = ImageFont.truetype(font_path, font_size)\ntransposed_font = ImageFont.TransposedFont(font,\n                                           orientation=orientation)\n# get size of resulting text\nbox_size = transposed_font.getsize(word)\nx = position[0]\ny = position[1]\n# cut out patch under word box\npatch = self.image[x:x + box_size[0], y:y + box_size[1]]\nif patch.ndim == 3:\n    # drop alpha channel if any\n    patch = patch[:, :, :3]\nif patch.ndim == 2:\n    raise NotImplementedError(\"Gray-scale images TODO\")\n# check if the text is within the bounds of the image\nreshape = patch.reshape(-1, 3)\nif not np.all(reshape.shape):\n    if self.default_color is None:\n        raise ValueError('ImageColorGenerator is smaller than the canvas')\n    return \"rgb(%d, %d, %d)\" % tuple(self.default_color)\ncolor = np.mean(reshape, axis=0)\nreturn \"rgb(%d, %d, %d)\" % tuple(color)", "path": "word_cloud/wordcloud/color_from_image.py", "commit_date": "2018-06-06 00:00:00", "repo_name": "amueller/word_cloud", "stars": 9883, "license": "mit", "language": "python", "size": 119777}
{"docstring": "# test masks\n\n# check that using an empty mask is equivalent to not using a mask\n", "func_signal": "def test_mask():\n", "code": "wc = WordCloud(random_state=42)\nwc.generate(THIS)\nmask = np.zeros(np.array(wc).shape[:2], dtype=np.int)\nwc_mask = WordCloud(mask=mask, random_state=42)\nwc_mask.generate(THIS)\nassert_array_equal(wc, wc_mask)\n\n# use actual nonzero mask\nmask = np.zeros((234, 456), dtype=np.int)\nmask[100:150, 300:400] = 255\n\nwc = WordCloud(mask=mask)\nwc.generate(THIS)\nwc_array = np.array(wc)\nassert mask.shape == wc_array.shape[:2]\nassert_array_equal(wc_array[mask != 0], 0)\nassert wc_array[mask == 0].sum() > 10000", "path": "word_cloud/test/test_wordcloud.py", "commit_date": "2020-09-25 00:00:00", "repo_name": "amueller/word_cloud", "stars": 9883, "license": "mit", "language": "python", "size": 119777}
{"docstring": "\"\"\"Generate wordcloud from text.\n\nThe input \"text\" is expected to be a natural text. If you pass a sorted\nlist of words, words will appear in your output twice. To remove this\nduplication, set ``collocations=False``.\n\nCalls process_text and generate_from_frequencies.\n\n..versionchanged:: 1.2.2\n    Argument of generate_from_frequencies() is not return of\n    process_text() any more.\n\nReturns\n-------\nself\n\"\"\"\n", "func_signal": "def generate_from_text(self, text):\n", "code": "words = self.process_text(text)\nself.generate_from_frequencies(words)\nreturn self", "path": "word_cloud/wordcloud/wordcloud.py", "commit_date": "2020-09-25 00:00:00", "repo_name": "amueller/word_cloud", "stars": 9883, "license": "mit", "language": "python", "size": 119777}
{"docstring": "# test that generate_from_frequencies() takes input argument dicts\n", "func_signal": "def test_generate_from_frequencies():\n", "code": "wc = WordCloud(max_words=50)\nwords = wc.process_text(THIS)\nresult = wc.generate_from_frequencies(words)\n\nassert isinstance(result, WordCloud)", "path": "word_cloud/test/test_wordcloud.py", "commit_date": "2020-09-25 00:00:00", "repo_name": "amueller/word_cloud", "stars": 9883, "license": "mit", "language": "python", "size": 119777}
{"docstring": "\"\"\"Check if ``layout_`` was computed, otherwise raise error.\"\"\"\n", "func_signal": "def _check_generated(self):\n", "code": "if not hasattr(self, \"layout_\"):\n    raise ValueError(\"WordCloud has not been calculated, call generate\"\n                     \" first.\")", "path": "word_cloud/wordcloud/wordcloud.py", "commit_date": "2020-09-25 00:00:00", "repo_name": "amueller/word_cloud", "stars": 9883, "license": "mit", "language": "python", "size": 119777}
{"docstring": "\"\"\"Returns a single_color_func associated with the word\"\"\"\n", "func_signal": "def get_color_func(self, word):\n", "code": "try:\n    color_func = next(\n        color_func for (color_func, words) in self.color_func_to_words\n        if word in words)\nexcept StopIteration:\n    color_func = self.default_color_func\n\nreturn color_func", "path": "word_cloud/examples/colored_by_group.py", "commit_date": "2017-03-21 00:00:00", "repo_name": "amueller/word_cloud", "stars": 9883, "license": "mit", "language": "python", "size": 119777}
{"docstring": "\"\"\"Recolor existing layout.\n\nApplying a new coloring is much faster than generating the whole\nwordcloud.\n\nParameters\n----------\nrandom_state : RandomState, int, or None, default=None\n    If not None, a fixed random state is used. If an int is given, this\n    is used as seed for a random.Random state.\n\ncolor_func : function or None, default=None\n    Function to generate new color from word count, font size, position\n    and orientation.  If None, self.color_func is used.\n\ncolormap : string or matplotlib colormap, default=None\n    Use this colormap to generate new colors. Ignored if color_func\n    is specified. If None, self.color_func (or self.color_map) is used.\n\nReturns\n-------\nself\n\"\"\"\n", "func_signal": "def recolor(self, random_state=None, color_func=None, colormap=None):\n", "code": "if isinstance(random_state, int):\n    random_state = Random(random_state)\nself._check_generated()\n\nif color_func is None:\n    if colormap is None:\n        color_func = self.color_func\n    else:\n        color_func = colormap_color_func(colormap)\nself.layout_ = [(word_freq, font_size, position, orientation,\n                 color_func(word=word_freq[0], font_size=font_size,\n                            position=position, orientation=orientation,\n                            random_state=random_state,\n                            font_path=self.font_path))\n                for word_freq, font_size, position, orientation, _\n                in self.layout_]\nreturn self", "path": "word_cloud/wordcloud/wordcloud.py", "commit_date": "2020-09-25 00:00:00", "repo_name": "amueller/word_cloud", "stars": 9883, "license": "mit", "language": "python", "size": 119777}
{"docstring": "# test single color function for different color formats\n", "func_signal": "def test_single_color_func():\n", "code": "random = Random(42)\n\nred_function = get_single_color_func('red')\nassert red_function(random_state=random) == 'rgb(181, 0, 0)'\n\nhex_function = get_single_color_func('#00b4d2')\nassert hex_function(random_state=random) == 'rgb(0, 48, 56)'\n\nrgb_function = get_single_color_func('rgb(0,255,0)')\nassert rgb_function(random_state=random) == 'rgb(0, 107, 0)'\n\nrgb_perc_fun = get_single_color_func('rgb(80%,60%,40%)')\nassert rgb_perc_fun(random_state=random) == 'rgb(97, 72, 48)'\n\nhsl_function = get_single_color_func('hsl(0,100%,50%)')\nassert hsl_function(random_state=random) == 'rgb(201, 0, 0)'", "path": "word_cloud/test/test_wordcloud.py", "commit_date": "2020-09-25 00:00:00", "repo_name": "amueller/word_cloud", "stars": 9883, "license": "mit", "language": "python", "size": 119777}
{"docstring": "# test that process function returns a dict\n", "func_signal": "def test_process_text():\n", "code": "wc = WordCloud(max_words=50)\nresult = wc.process_text(THIS)\n\n# check for proper return type\nassert isinstance(result, dict)", "path": "word_cloud/test/test_wordcloud.py", "commit_date": "2020-09-25 00:00:00", "repo_name": "amueller/word_cloud", "stars": 9883, "license": "mit", "language": "python", "size": 119777}
{"docstring": "# test originally empty text raises an exception\n", "func_signal": "def test_empty_text():\n", "code": "wc = WordCloud(stopwords=[])\nwith pytest.raises(ValueError):\n    wc.generate('')\n\n# test empty-after-filtering text raises an exception\nwc = WordCloud(stopwords=['a', 'b'])\nwith pytest.raises(ValueError):\n    wc.generate('a b a')", "path": "word_cloud/test/test_wordcloud.py", "commit_date": "2020-09-25 00:00:00", "repo_name": "amueller/word_cloud", "stars": 9883, "license": "mit", "language": "python", "size": 119777}
{"docstring": "\"\"\"Return the name of the type for this PathId.\"\"\"\n", "func_signal": "def target_name_hint(self) -> s_name.QualName:\n", "code": "if self.target.material_type is not None:\n    material_type = self.target.material_type\nelse:\n    material_type = self.target\nreturn material_type.name_hint", "path": "edgedb/edb/ir/pathid.py", "commit_date": "2020-11-12 00:00:00", "repo_name": "edgedb/edgedb", "stars": 12109, "license": "apache-2.0", "language": "python", "size": 57917}
{"docstring": "\"\"\"Prevent this process from generating a core dump.\"\"\"\n", "func_signal": "def prevent_core_dump():\n", "code": "core_resource = resource.RLIMIT_CORE\n\ntry:\n    resource.getrlimit(core_resource)\nexcept ValueError as ex:\n    raise DaemonError(\n        'Unable to limit core dump size: '\n        'system does not support RLIMIT_CORE resource limit') from ex\n\n# Set hard & soft limits to 0, i.e. no core dump at all\nresource.setrlimit(core_resource, (0, 0))", "path": "edgedb/edb/server/daemon/lib.py", "commit_date": "2019-08-18 00:00:00", "repo_name": "edgedb/edgedb", "stars": 12109, "license": "apache-2.0", "language": "python", "size": 57917}
{"docstring": "\"\"\"Return a copy of this ``PathId`` with weak namespace portion\n   removed.\"\"\"\n", "func_signal": "def strip_weak_namespaces(self) -> PathId:\n", "code": "if self._namespace is not None:\n    stripped_ns = {bit for bit in self._namespace\n                   if not isinstance(bit, WeakNamespace)}\n    result = self.replace_namespace(stripped_ns)\n\n    if result._prefix is not None:\n        result._prefix = result._get_minimal_prefix(\n            result._prefix.strip_weak_namespaces())\n\nelse:\n    result = self\n\nreturn result", "path": "edgedb/edb/ir/pathid.py", "commit_date": "2020-11-12 00:00:00", "repo_name": "edgedb/edgedb", "stars": 12109, "license": "apache-2.0", "language": "python", "size": 57917}
{"docstring": "# We cannot simply use expanduser() as that returns the user's\n# home directory, whereas Postgres stores its config in\n# %AppData% on Windows.\n", "func_signal": "def get_pg_home_directory() -> Optional[pathlib.Path]:\n", "code": "buf = ctypes.create_unicode_buffer(ctypes.wintypes.MAX_PATH)\nr = ctypes.windll.shell32.SHGetFolderPathW(  # type: ignore\n    0, CSIDL_APPDATA, 0, 0, buf)\nif r:\n    return None\nelse:\n    return pathlib.Path(buf.value) / 'postgresql'", "path": "edgedb/edb/server/pgconnparams.py", "commit_date": "2020-02-20 00:00:00", "repo_name": "edgedb/edgedb", "stars": 12109, "license": "apache-2.0", "language": "python", "size": 57917}
{"docstring": "\"\"\"Datach process context.\n\nDoes it in three steps:\n\n1. Forks and exists parent process.\nThis detaches us from shell, and since the child will have a new\nPID but will inherit the Group PID from parent, the new process\nwill not be a group leader.\n\n2. Call 'setsid' to create a new session.\nThis makes the process a session leader of a new session, process\nbecomes the process group leader of a new process group and it\ndoesn't have a controlling terminal.\n\n3. Form and exit parent again.\nThis guarantees that the daemon is not a session leader, which\nprevents it from acquiring a controlling terminal.\n\nReference: \u201cAdvanced Programming in the Unix Environment\u201d,\nsection 13.3, by W. Richard Stevens.\n\"\"\"\n", "func_signal": "def detach_process_context():\n", "code": "def fork_and_exit_parent(error_message):\n    try:\n        if os.fork() > 0:\n            # Don't need to call 'sys.exit', as we don't want to\n            # run any python interpreter clean-up handlers\n            os._exit(0)\n    except OSError as ex:\n        raise DaemonError(\n            '{}: [{}] {}'.format(error_message, ex.errno,\n                                 ex.strerror)) from ex\n\nfork_and_exit_parent(error_message='Failed the first fork')\nos.setsid()\nfork_and_exit_parent(error_message='Failed the second fork')", "path": "edgedb/edb/server/daemon/lib.py", "commit_date": "2019-08-18 00:00:00", "repo_name": "edgedb/edgedb", "stars": 12109, "license": "apache-2.0", "language": "python", "size": 57917}
{"docstring": "\"\"\"Return a new ``PathId`` instance that is a \"pointer prefix\" of this\n   ``PathId``.\n\n   A pointer prefix is the common path prefix shared by paths to\n   link properties of the same link, i.e\n\n       common_path_id(Foo.bar@prop1, Foo.bar@prop2)\n           == PathId(Foo.bar).ptr_path()\n\"\"\"\n", "func_signal": "def ptr_path(self) -> PathId:\n", "code": "if self._is_ptr:\n    return self\nelse:\n    result = self.__class__(self)\n    result._is_ptr = True\n    return result", "path": "edgedb/edb/ir/pathid.py", "commit_date": "2020-11-12 00:00:00", "repo_name": "edgedb/edgedb", "stars": 12109, "license": "apache-2.0", "language": "python", "size": 57917}
{"docstring": "\"\"\"Pretty PathId format for user-visible messages.\"\"\"\n", "func_signal": "def pformat(self) -> str:\n", "code": "result = ''\n\nif not self._path:\n    return ''\n\npath = self._path\n\nstart_name = s_name.shortname_from_fullname(\n    path[0].name_hint)  # type: ignore\nresult += f'{start_name.name}'\n\nfor i in range(1, len(path) - 1, 2):\n    ptrspec = cast(\n        Tuple[irast.BasePointerRef, s_pointers.PointerDirection],\n        path[i],\n    )\n\n    ptr_name = ptrspec[0].shortname\n    ptrdir = ptrspec[1]\n    is_lprop = ptrspec[0].source_ptr is not None\n\n    if is_lprop:\n        step = '@'\n    else:\n        step = '.'\n        if ptrdir == s_pointers.PointerDirection.Inbound:\n            step += ptrdir\n\n    result += f'{step}{ptr_name.name}'\n\nif self._is_ptr:\n    result += '@'\n\nreturn result", "path": "edgedb/edb/ir/pathid.py", "commit_date": "2020-11-12 00:00:00", "repo_name": "edgedb/edgedb", "stars": 12109, "license": "apache-2.0", "language": "python", "size": 57917}
{"docstring": "\"\"\"Return a ``PathId`` instance representing an immediate path prefix\n   of this ``PathId``, i.e\n   ``PathId('Foo.bar.baz').src_path() == PathId('Foo.bar')``.\n\n   If this PathId represents a non-path expression, ``src_path()``\n   will return ``None``.\n\"\"\"\n", "func_signal": "def src_path(self) -> Optional[PathId]:\n", "code": "if len(self._path) > 1:\n    return self._get_prefix(-2)\nelse:\n    return None", "path": "edgedb/edb/ir/pathid.py", "commit_date": "2020-11-12 00:00:00", "repo_name": "edgedb/edgedb", "stars": 12109, "license": "apache-2.0", "language": "python", "size": 57917}
{"docstring": "# Test cases where the prefix is in a different namespace\n\n", "func_signal": "def test_edgeql_ir_pathid_namespace_02(self):\n", "code": "Card = self.schema.get('test::Card')\nUser = self.schema.get('test::User')\nowners_ptr = Card.getptr(self.schema, 'owners')\nowners_ptr_ref = irtyputils.ptrref_from_ptrcls(\n    schema=self.schema,\n    ptrcls=owners_ptr,\n)\ndeck_ptr = User.getptr(self.schema, 'deck')\ndeck_ptr_ref = irtyputils.ptrref_from_ptrcls(\n    schema=self.schema,\n    ptrcls=deck_ptr,\n)\ncount_prop = deck_ptr.getptr(self.schema, 'count')\ncount_prop_ref = irtyputils.ptrref_from_ptrcls(\n    schema=self.schema,\n    ptrcls=count_prop,\n)\n\nns_1 = frozenset(('foo',))\nns_2 = frozenset(('bar',))\n\npid_1 = pathid.PathId.from_type(self.schema, Card)\npid_2 = pid_1.extend(ptrref=owners_ptr_ref, ns=ns_1,\n                     schema=self.schema)\npid_2_no_ns = pid_1.extend(ptrref=owners_ptr_ref, schema=self.schema)\n\nself.assertNotEqual(pid_2, pid_2_no_ns)\nself.assertEqual(pid_2.src_path(), pid_1)\n\npid_3 = pid_2.extend(ptrref=deck_ptr_ref, ns=ns_2, schema=self.schema)\nptr_pid = pid_3.ptr_path()\nprop_pid = ptr_pid.extend(ptrref=count_prop_ref, schema=self.schema)\n\nself.assertEqual(prop_pid.src_path().namespace, ns_1 | ns_2)\nself.assertEqual(prop_pid.src_path().src_path().namespace, ns_1)\nself.assertFalse(prop_pid.src_path().src_path().src_path().namespace)\n\nprefixes = [str(p) for p in pid_3.iter_prefixes()]\n\nself.assertEqual(\n    prefixes,\n    [\n        '(test::Card)',\n        'foo@@(test::Card).>owners[IS test::User]',\n        'bar@foo@@(test::Card).>owners[IS test::User]'\n        '.>deck[IS test::Card]',\n    ]\n)", "path": "edgedb/tests/test_edgeql_ir_pathid.py", "commit_date": "2019-12-15 00:00:00", "repo_name": "edgedb/edgedb", "stars": 12109, "license": "apache-2.0", "language": "python", "size": 57917}
{"docstring": "\"\"\"Determine whether detaching process context is required.\n\nReturns ``True`` if:\n    - Process was started by `init`; or\n    - Process was started by `inetd`.\n\"\"\"\n", "func_signal": "def is_detach_process_context_required():\n", "code": "return not is_process_started_by_init(\n) and not is_process_started_by_superserver()", "path": "edgedb/edb/server/daemon/lib.py", "commit_date": "2019-08-18 00:00:00", "repo_name": "edgedb/edgedb", "stars": 12109, "license": "apache-2.0", "language": "python", "size": 57917}
{"docstring": "\"\"\"Return the descriptor of a pointer for the last path step, if any.\n\n   If this PathId represents a non-path expression, ``rptr()``\n   will return ``None``.\n\"\"\"\n", "func_signal": "def rptr(self) -> Optional[irast.BasePointerRef]:\n", "code": "if len(self._path) > 1:\n    return self._path[-2][0]  # type: ignore\nelse:\n    return None", "path": "edgedb/edb/ir/pathid.py", "commit_date": "2020-11-12 00:00:00", "repo_name": "edgedb/edgedb", "stars": 12109, "license": "apache-2.0", "language": "python", "size": 57917}
{"docstring": "\"\"\"Load the compiler modules.  This is done once per process.\"\"\"\n\n", "func_signal": "def _load() -> None:\n", "code": "global _LOADED\nglobal dispatch_mod, inference_mod, irast, ireval, norm_mod, stmtctx_mod\n\nfrom edb.ir import ast as _irast\nfrom edb.ir import staeval as _ireval\n\nfrom . import expr as _expr_compiler  # NOQA\nfrom . import config as _config_compiler  # NOQA\nfrom . import stmt as _stmt_compiler  # NOQA\n\nfrom . import dispatch\nfrom . import inference\nfrom . import normalization\nfrom . import stmtctx\n\ndispatch_mod = dispatch\ninference_mod = inference\nirast = _irast\nireval = _ireval\nnorm_mod = normalization\nstmtctx_mod = stmtctx\n_LOADED = True", "path": "edgedb/edb/edgeql/compiler/__init__.py", "commit_date": "2020-12-26 00:00:00", "repo_name": "edgedb/edgedb", "stars": 12109, "license": "apache-2.0", "language": "python", "size": 57917}
{"docstring": "# allow coercing ints to floats\n", "func_signal": "def is_type_sub_type_of(schema, maybe_subtype, super_type):\n", "code": "if super_type is graphql.GraphQLFloat:\n    if maybe_subtype is graphql.GraphQLInt:\n        return True\nreturn old_is_type_sub_type_of(schema, maybe_subtype, super_type)", "path": "edgedb/edb/graphql/_patch_core.py", "commit_date": "2020-03-25 00:00:00", "repo_name": "edgedb/edgedb", "stars": 12109, "license": "apache-2.0", "language": "python", "size": 57917}
{"docstring": "\"\"\"Generate keywords, builtins, operators, etc. which can be used\nfor EdgeQL and SDL grammars.\n\nNAME - at the moment there's only one option 'edgeql'\n\"\"\"\n\n", "func_signal": "def gen_meta_grammars(names):\n", "code": "if names:\n    for name in names:\n        if name not in NAMES:\n            die(f'{name} is not a valid NAME')\n\n    if len(names) > 2:\n        die(f'too many NAMES')\n\ncon = None\ntry:\n    con = edgedb.connect(user=edgedb_defines.EDGEDB_SUPERUSER,\n                         database=edgedb_defines.EDGEDB_SUPERUSER_DB)\n    main(names, con)\nexcept Exception as ex:\n    die(str(ex))\nfinally:\n    if con is not None:\n        con.close()", "path": "edgedb/edb/tools/gen_meta_grammars.py", "commit_date": "2020-07-13 00:00:00", "repo_name": "edgedb/edgedb", "stars": 12109, "license": "apache-2.0", "language": "python", "size": 57917}
{"docstring": "\"\"\"Return True if this PathId represents a type intersection\n   expression, i.e ``Foo[IS Bar]``.\"\"\"\n", "func_signal": "def is_type_intersection_path(self) -> bool:\n", "code": "rptr_name = self.rptr_name()\nif rptr_name is None:\n    return False\nelse:\n    return str(rptr_name) in (\n        '__type__::indirection',\n        '__type__::optindirection',\n    )", "path": "edgedb/edb/ir/pathid.py", "commit_date": "2020-11-12 00:00:00", "repo_name": "edgedb/edgedb", "stars": 12109, "license": "apache-2.0", "language": "python", "size": 57917}
{"docstring": "\"\"\"Return the name of a pointer for the last path step, if any.\n\n   If this PathId represents a non-path expression, ``rptr_name()``\n   will return ``None``.\n\"\"\"\n", "func_signal": "def rptr_name(self) -> Optional[s_name.QualName]:\n", "code": "rptr = self.rptr()\nif rptr is not None:\n    return rptr.shortname\nelse:\n    return None", "path": "edgedb/edb/ir/pathid.py", "commit_date": "2020-11-12 00:00:00", "repo_name": "edgedb/edgedb", "stars": 12109, "license": "apache-2.0", "language": "python", "size": 57917}
{"docstring": "\"\"\"Check if `stream` is an open io.IOBase instance.\"\"\"\n", "func_signal": "def validate_stream(stream, *, stream_name):\n", "code": "if not isinstance(stream, io.IOBase):\n    raise DaemonError(\n        'Invalid {} stream object, an instance of io.IOBase is expected'.\n        format(stream_name))\n\nif stream.closed:\n    raise DaemonError('Stream {} is already closed'.format(stream_name))", "path": "edgedb/edb/server/daemon/lib.py", "commit_date": "2019-08-18 00:00:00", "repo_name": "edgedb/edgedb", "stars": 12109, "license": "apache-2.0", "language": "python", "size": 57917}
{"docstring": "\"\"\"Determine if the current process is started by the superserver.\"\"\"\n# The internet superserver creates a network socket, and\n# attaches it to the standard streams of the child process.\n\n", "func_signal": "def is_process_started_by_superserver():\n", "code": "try:\n    fileno = sys.__stdin__.fileno()\nexcept Exception:\n    return False\nelse:\n    return is_socket(fileno)", "path": "edgedb/edb/server/daemon/lib.py", "commit_date": "2019-08-18 00:00:00", "repo_name": "edgedb/edgedb", "stars": 12109, "license": "apache-2.0", "language": "python", "size": 57917}
{"docstring": "\"\"\"If this is a pointer prefix, return the ``PathId`` representing\n   the path to the target of the pointer.\n\n   This is the inverse of :meth:`~PathId.ptr_path`.\n\"\"\"\n", "func_signal": "def tgt_path(self) -> PathId:\n", "code": "if not self._is_ptr:\n    return self\nelse:\n    result = self.__class__(self)\n    result._is_ptr = False\n    return result", "path": "edgedb/edb/ir/pathid.py", "commit_date": "2020-11-12 00:00:00", "repo_name": "edgedb/edgedb", "stars": 12109, "license": "apache-2.0", "language": "python", "size": 57917}
{"docstring": "# Define keyword tokens\n\n", "func_signal": "def _gen_keyword_tokens():\n", "code": "mod = sys.modules[__name__]\n\ndef clsexec(ns):\n    ns['__module__'] = __name__\n    return ns\n\nfor token, _ in keywords.edgeql_keywords.values():\n    clsname = 'T_{}'.format(token)\n    clskwds = dict(metaclass=parsing.TokenMeta, token=token)\n    cls = types.new_class(clsname, (Token,), clskwds, clsexec)\n    setattr(mod, clsname, cls)", "path": "edgedb/edb/edgeql/parser/grammar/tokens.py", "commit_date": "2019-12-17 00:00:00", "repo_name": "edgedb/edgedb", "stars": 12109, "license": "apache-2.0", "language": "python", "size": 57917}
{"docstring": "# Restore envvar values\n", "func_signal": "def tearDown(self):\n", "code": "for name in self.homes.values():\n    if name in self.orig_envvars:\n        os.environ[name] = self.orig_envvars[name]\n    else:\n        del os.environ[name]\n\nshutil.rmtree(self.temp_dir)", "path": "http-prompt/tests/base.py", "commit_date": "2017-08-18 00:00:00", "repo_name": "httpie/http-prompt", "stars": 8854, "license": "mit", "language": "python", "size": 736}
{"docstring": "# Level 1 (root)\n", "func_signal": "def test_add_path_and_find_child(self):\n", "code": "self.assertEqual(set(c.name for c in self.root.children), set('ah'))\n\n# Level 2\nnode_a = self.root.find_child('a')\nnode_h = self.root.find_child('h')\nself.assertEqual(set(c.name for c in node_a.children), set('bd'))\nself.assertEqual(set(c.name for c in node_h.children), set('in'))\n\n# Level 3\nnode_b = node_a.find_child('b')\nnode_i = node_h.find_child('i')\nself.assertEqual(set(c.name for c in node_b.children), set('cf'))\nself.assertEqual(set(c.name for c in node_i.children), set('k'))\n\n# Level 4\nnode_c = node_b.find_child('c')\nnode_k = node_i.find_child('k')\nself.assertEqual(set(c.name for c in node_c.children), set())\nself.assertEqual(set(c.name for c in node_k.children), set('lmp'))\n\n# Return None if child can't be found\nself.assertFalse(node_c.find_child('x'))", "path": "http-prompt/tests/test_tree.py", "commit_date": "2017-03-13 00:00:00", "repo_name": "httpie/http-prompt", "stars": 8854, "license": "mit", "language": "python", "size": 736}
{"docstring": "# Append '\\n' to simulate behavior of click.echo_via_pager(),\n# which we use whenever we want to output anything to stdout\n", "func_signal": "def assert_stdout(self, expected_msg):\n", "code": "printed_msg = self.echo_via_pager.call_args[0][0] + '\\n'\nself.assertEqual(printed_msg, expected_msg)", "path": "http-prompt/tests/test_execution.py", "commit_date": "2020-08-17 00:00:00", "repo_name": "httpie/http-prompt", "stars": 8854, "license": "mit", "language": "python", "size": 736}
{"docstring": "\"\"\"Read user config file and return it as a dict.\"\"\"\n", "func_signal": "def load_user():\n", "code": "config_path = get_user_config_path()\nconfig = {}\n\n# TODO: This may be overkill and too slow just for reading a config file\nwith open(config_path) as f:\n    code = compile(f.read(), config_path, 'exec')\nexec(code, config)\n\nkeys = list(config.keys())\nfor k in keys:\n    if k.startswith('_'):\n        del config[k]\n\nreturn config", "path": "http-prompt/http_prompt/config.py", "commit_date": "2020-08-17 00:00:00", "repo_name": "httpie/http-prompt", "stars": 8854, "license": "mit", "language": "python", "size": 736}
{"docstring": "\"\"\"Format a list of strings like ls does multi-column output.\"\"\"\n", "func_signal": "def colformat(strings, num_sep_spaces=1, terminal_width=None):\n", "code": "if terminal_width is None:\n    terminal_width = get_terminal_size().columns\n\nif not strings:\n    return\n\nnum_items = len(strings)\nmax_len = max([len(strip_ansi_escapes(s)) for s in strings])\n\nnum_columns = min(\n    int((terminal_width + num_sep_spaces) / (max_len + num_sep_spaces)),\n    num_items)\nnum_columns = max(1, num_columns)\n\nnum_lines = int(math.ceil(float(num_items) / num_columns))\nnum_columns = int(math.ceil(float(num_items) / num_lines))\n\nnum_elements_last_column = num_items % num_lines\nif num_elements_last_column == 0:\n    num_elements_last_column = num_lines\n\nlines = []\nfor i in range(num_lines):\n    line_size = num_columns\n    if i >= num_elements_last_column:\n        line_size -= 1\n    lines.append([None] * line_size)\n\nfor i, line in enumerate(lines):\n    line_size = len(line)\n    for j in range(line_size):\n        k = i + num_lines * j\n        item = strings[k]\n        if j % line_size != line_size - 1:\n            item_len = len(strip_ansi_escapes(item))\n            item = item + ' ' * (max_len - item_len)\n        line[j] = item\n\nsep = ' ' * num_sep_spaces\nfor line in lines:\n    yield sep.join(line)", "path": "http-prompt/http_prompt/utils.py", "commit_date": "2020-08-17 00:00:00", "repo_name": "httpie/http-prompt", "stars": 8854, "license": "mit", "language": "python", "size": 736}
{"docstring": "\"\"\"Transform a Context object to a list of arguments that can be passed to\nHTTPie main function.\n\"\"\"\n", "func_signal": "def extract_args_for_httpie_main(context, method=None):\n", "code": "args = _extract_httpie_options(context)\n\nif method:\n    args.append(method.upper())\n\nargs.append(context.url)\nargs += _extract_httpie_request_items(context)\nreturn args", "path": "http-prompt/http_prompt/context/transform.py", "commit_date": "2020-08-17 00:00:00", "repo_name": "httpie/http-prompt", "stars": 8854, "license": "mit", "language": "python", "size": 736}
{"docstring": "\"\"\"Create a file under self.temp_dir and return the path.\"\"\"\n", "func_signal": "def make_tempfile(self, data='', subdir_name=''):\n", "code": "full_tempdir = os.path.join(self.temp_dir, subdir_name)\nif not os.path.exists(full_tempdir):\n    os.makedirs(full_tempdir)\n\nif isinstance(data, six.text_type):\n    data = data.encode('utf-8')\n\nwith tempfile.NamedTemporaryFile(dir=full_tempdir, delete=False) as f:\n    f.write(data)\n    return f.name", "path": "http-prompt/tests/base.py", "commit_date": "2017-08-18 00:00:00", "repo_name": "httpie/http-prompt", "stars": 8854, "license": "mit", "language": "python", "size": 736}
{"docstring": "\"\"\"Initialize a default config file if it doesn't exist yet.\n\nReturns:\n    tuple: A tuple of (copied, dst_path). `copied` is a bool indicating if\n        this function created the default config file. `dst_path` is the\n        path of the user config file.\n\"\"\"\n", "func_signal": "def initialize():\n", "code": "dst_path = get_user_config_path()\ncopied = False\nif not os.path.exists(dst_path):\n    src_path = os.path.join(os.path.dirname(__file__), 'defaultconfig.py')\n    shutil.copyfile(src_path, dst_path)\n    copied = True\nreturn copied, dst_path", "path": "http-prompt/http_prompt/config.py", "commit_date": "2020-08-17 00:00:00", "repo_name": "httpie/http-prompt", "stars": 8854, "license": "mit", "language": "python", "size": 736}
{"docstring": "# TODO: Escape\n", "func_signal": "def smart_quote(s):\n", "code": "if ' ' in s or r'\\:' in s:\n    s = \"'\" + s + \"'\"\nreturn s", "path": "http-prompt/http_prompt/utils.py", "commit_date": "2020-08-17 00:00:00", "repo_name": "httpie/http-prompt", "stars": 8854, "license": "mit", "language": "python", "size": 736}
{"docstring": "\"\"\"Read default and user config files and return them as a dict.\"\"\"\n", "func_signal": "def load():\n", "code": "config = load_default()\nconfig.update(load_user())\nreturn config", "path": "http-prompt/http_prompt/config.py", "commit_date": "2020-08-17 00:00:00", "repo_name": "httpie/http-prompt", "stars": 8854, "license": "mit", "language": "python", "size": 736}
{"docstring": "\"\"\"https://github.com/amjith/fuzzyfinder\"\"\"\n", "func_signal": "def fuzzyfinder(text, collection):\n", "code": "suggestions = []\nif not isinstance(text, str):\n    text = str(text)\npat = '.*?'.join(map(re.escape, text))\nregex = re.compile(pat, flags=re.IGNORECASE)\nfor item in collection:\n    r = regex.search(item)\n    if r:\n        suggestions.append((len(r.group()), r.start(), item))\n\nreturn (z for _, _, z in sorted(suggestions))", "path": "http-prompt/http_prompt/completer.py", "commit_date": "2020-08-17 00:00:00", "repo_name": "httpie/http-prompt", "stars": 8854, "license": "mit", "language": "python", "size": 736}
{"docstring": "# Config file is not there at the beginning\n", "func_signal": "def test_config_file(self):\n", "code": "config_path = os.path.join(xdg.get_config_dir(), 'config.py')\nself.assertFalse(os.path.exists(config_path))\n\n# After user runs it for the first time, a default config file should\n# be created\nresult, context = run_and_exit(['//example.com'])\nself.assertEqual(result.exit_code, 0)\nself.assertTrue(os.path.exists(config_path))", "path": "http-prompt/tests/test_cli.py", "commit_date": "2020-08-17 00:00:00", "repo_name": "httpie/http-prompt", "stars": 8854, "license": "mit", "language": "python", "size": 736}
{"docstring": "\"\"\"Format a Context object to an HTTPie command.\"\"\"\n", "func_signal": "def format_to_httpie(context, method=None):\n", "code": "cmd = ['http'] + _extract_httpie_options(context, quote=True,\n                                         join_key_value=True)\nif method:\n    cmd.append(method.upper())\ncmd.append(context.url)\ncmd += _extract_httpie_request_items(context, quote=True)\nreturn ' '.join(cmd) + '\\n'", "path": "http-prompt/http_prompt/context/transform.py", "commit_date": "2020-08-17 00:00:00", "repo_name": "httpie/http-prompt", "stars": 8854, "license": "mit", "language": "python", "size": 736}
{"docstring": "\"\"\"Run http-prompt from terminal.\"\"\"\n", "func_signal": "def run_http_prompt(args):\n", "code": "bin_path = get_http_prompt_path()\np = subprocess.Popen([bin_path] + args, stdin=PIPE, stdout=PIPE)\nreturn p.communicate()", "path": "http-prompt/tests/test_installation.py", "commit_date": "2016-05-30 00:00:00", "repo_name": "httpie/http-prompt", "stars": 8854, "license": "mit", "language": "python", "size": 736}
{"docstring": "\"\"\"Get the path to http-prompt executable.\"\"\"\n", "func_signal": "def get_http_prompt_path():\n", "code": "python_dir = os.path.dirname(sys.executable)\nbin_name = 'http-prompt'\nif sys.platform == 'win32':\n    bin_name += '.exe'\n\npaths = [\n    os.path.join(python_dir, bin_name),\n    os.path.join(python_dir, 'Scripts', bin_name),  # Windows\n    '/usr/bin/http-prompt'  # Homebrew installation\n]\nfor path in paths:\n    if os.path.exists(path):\n        return path\n\nraise OSError(\"could not locate http-prompt executable, \"\n              \"Python directory: %s\" % python_dir)", "path": "http-prompt/tests/utils.py", "commit_date": "2016-05-30 00:00:00", "repo_name": "httpie/http-prompt", "stars": 8854, "license": "mit", "language": "python", "size": 736}
{"docstring": "# Create a temp dir that will contain data and config directories\n", "func_signal": "def setUp(self):\n", "code": "self.temp_dir = tempfile.mkdtemp()\n\nif sys.platform == 'win32':\n    self.homes = {\n        # subdir_name: envvar_name\n        'data': 'LOCALAPPDATA',\n        'config': 'LOCALAPPDATA'\n    }\nelse:\n    self.homes = {\n        # subdir_name: envvar_name\n        'data': 'XDG_DATA_HOME',\n        'config': 'XDG_CONFIG_HOME'\n    }\n\n# Used to restore\nself.orig_envvars = {}\n\nfor subdir_name, envvar_name in self.homes.items():\n    if envvar_name in os.environ:\n        self.orig_envvars[envvar_name] = os.environ[envvar_name]\n    os.environ[envvar_name] = os.path.join(self.temp_dir, subdir_name)", "path": "http-prompt/tests/base.py", "commit_date": "2017-08-18 00:00:00", "repo_name": "httpie/http-prompt", "stars": 8854, "license": "mit", "language": "python", "size": 736}
{"docstring": "\"\"\"Run http-prompt executable, execute some prompt commands, and exit.\"\"\"\n", "func_signal": "def run_and_exit(cli_args=None, prompt_commands=None):\n", "code": "if cli_args is None:\n    cli_args = []\n\n    # Make sure last command is 'exit'\nif prompt_commands is None:\n    prompt_commands = ['exit']\nelse:\n    prompt_commands += ['exit']\n\n# Fool cli() so that it believes we're running from CLI instead of pytest.\n# We will restore it at the end of the function.\norig_argv = sys.argv\nsys.argv = ['http-prompt'] + cli_args\n\ntry:\n    with patch.multiple('http_prompt.cli',\n                        prompt=DEFAULT, execute=DEFAULT) as mocks:\n        mocks['execute'].side_effect = execute\n\n        # prompt() is mocked to return the command in 'prompt_commands' in\n        # sequence, i.e., prompt() returns prompt_commands[i-1] when it is\n        # called for the ith time\n        mocks['prompt'].side_effect = prompt_commands\n\n        result = CliRunner().invoke(cli, cli_args)\n        context = mocks['execute'].call_args[0][1]\n\n    return result, context\nfinally:\n    sys.argv = orig_argv", "path": "http-prompt/tests/test_cli.py", "commit_date": "2020-08-17 00:00:00", "repo_name": "httpie/http-prompt", "stars": 8854, "license": "mit", "language": "python", "size": 736}
{"docstring": "# Make a tree like this:\n#          root\n#     a             h\n#  b     d        i   n\n# c f   e g     k     o\n#             l m p\n", "func_signal": "def setUp(self):\n", "code": "self.root = Node('root')\nself.root.add_path('a', 'b', 'c')\nself.root.add_path('a', 'b', 'f')\nself.root.add_path('a', 'd', 'e')\nself.root.add_path('a', 'd', 'g')\nself.root.add_path('h', 'i', 'k', 'l')\nself.root.add_path('h', 'i', 'k', 'm')\nself.root.add_path('h', 'i', 'k', 'p')\nself.root.add_path('h', 'n', 'o')", "path": "http-prompt/tests/test_tree.py", "commit_date": "2017-03-13 00:00:00", "repo_name": "httpie/http-prompt", "stars": 8854, "license": "mit", "language": "python", "size": 736}
{"docstring": "# Config file doesn't exist at first\n", "func_signal": "def test_initialize(self):\n", "code": "expected_path = config.get_user_config_path()\nself.assertFalse(os.path.exists(expected_path))\n\n# Config file should exist after initialization\ncopied, actual_path = config.initialize()\nself.assertTrue(copied)\nself.assertEqual(actual_path, expected_path)\nself.assertTrue(os.path.exists(expected_path))\n\n# Change config file and hash the content to see if it's changed\nwith open(expected_path, 'a') as f:\n    f.write('dont_care\\n')\norig_hash = _hash_file(expected_path)\n\n# Make sure it's fine to call config.initialize() twice\ncopied, actual_path = config.initialize()\nself.assertFalse(copied)\nself.assertEqual(actual_path, expected_path)\nself.assertTrue(os.path.exists(expected_path))\n\n# Make sure config file is unchanged\nnew_hash = _hash_file(expected_path)\nself.assertEqual(new_hash, orig_hash)", "path": "http-prompt/tests/test_config.py", "commit_date": "2016-06-12 00:00:00", "repo_name": "httpie/http-prompt", "stars": 8854, "license": "mit", "language": "python", "size": 736}
{"docstring": "\"\"\"Format a Context object to HTTP Prompt commands.\"\"\"\n", "func_signal": "def format_to_http_prompt(context, excluded_options=None):\n", "code": "cmds = _extract_httpie_options(context, quote=True, join_key_value=True,\n                               excluded_keys=excluded_options)\ncmds.append('cd ' + smart_quote(context.url))\ncmds += _extract_httpie_request_items(context, quote=True)\nreturn '\\n'.join(cmds) + '\\n'", "path": "http-prompt/http_prompt/context/transform.py", "commit_date": "2020-08-17 00:00:00", "repo_name": "httpie/http-prompt", "stars": 8854, "license": "mit", "language": "python", "size": 736}
{"docstring": "# Given a loadable resource instance that contains a reference\n# to another resource which has a resource data path, the\n# referenced resource should be loaded with all of the data\n# contained at that path. This allows loading references\n# which would otherwise not be loadable (missing load method)\n# and prevents extra load calls for others when we already\n# have the data available.\n", "func_signal": "def test_dangling_resource_loads_data(self):\n", "code": "self.defs = {\n    'Instance': {\n        'identifiers': [{'name': 'Id'}],\n        'has': {\n            'NetworkInterface': {\n                'resource': {\n                    'type': 'NetworkInterface',\n                    'identifiers': [\n                        {'target': 'Id', 'source': 'data',\n                         'path': 'NetworkInterface.Id'}\n                    ],\n                    'path': 'NetworkInterface'\n                }\n            }\n        }\n    },\n    'NetworkInterface': {\n        'identifiers': [{'name': 'Id'}],\n        'shape': 'NetworkInterfaceShape'\n    }\n}\nself.model = self.defs['Instance']\nshape = DenormalizedStructureBuilder().with_members({\n    'Id': {\n        'type': 'string',\n    },\n    'PublicIp': {\n        'type': 'string'\n    }\n}).build_model()\nservice_model = mock.Mock()\nservice_model.shape_for.return_value = shape\n\ncls = self.load('Instance', self.model, self.defs, service_model)\ninstance = cls('instance-id')\n\n# Set some data as if we had completed a load action.\ndef set_meta_data():\n    instance.meta.data = {\n        'NetworkInterface': {\n            'Id': 'network-interface-id',\n            'PublicIp': '127.0.0.1'\n        }\n    }\ninstance.load = mock.Mock(side_effect=set_meta_data)\n\n# Now, get the reference and make sure it has its data\n# set as expected.\ninterface = instance.network_interface\nself.assertIsNotNone(interface.meta.data)\nself.assertEqual(interface.public_ip, '127.0.0.1')", "path": "boto3/tests/unit/resources/test_factory.py", "commit_date": "2016-11-20 00:00:00", "repo_name": "boto/boto3", "stars": 8612, "license": "apache-2.0", "language": "python", "size": 7954}
{"docstring": "# This is just a smoke test to make sure that\n# setting use_threads to False has no issues transferring files as\n# the non-threaded implementation is ran under the same integration\n# and functional tests in s3transfer as the normal threaded\n# implementation\n#\n# The methods used are arbitrary other than one of the methods\n# use ``boto3.s3.transfer.S3Transfer`` and the other should be\n# using ``s3transfer.manager.TransferManager`` directly\n", "func_signal": "def test_transfer_methods_do_not_use_threads(self):\n", "code": "content = b'my content'\nfilename = self.files.create_file('myfile', content.decode('utf-8'))\nkey = 'foo'\nconfig = boto3.s3.transfer.TransferConfig(use_threads=False)\n\nself.client.upload_file(\n    Bucket=self.bucket_name, Key=key, Filename=filename,\n    Config=config)\nself.addCleanup(self.delete_object, key)\nself.assertTrue(self.object_exists(key))\n\nfileobj = six.BytesIO()\nself.client.download_fileobj(\n    Bucket=self.bucket_name, Key='foo', Fileobj=fileobj, Config=config)\nself.assertEqual(fileobj.getvalue(), content)", "path": "boto3/tests/integration/test_s3.py", "commit_date": "2019-07-03 00:00:00", "repo_name": "boto/boto3", "stars": 8612, "license": "apache-2.0", "language": "python", "size": 7954}
{"docstring": "\"\"\"\nCreate a copy of this metadata object.\n\"\"\"\n", "func_signal": "def copy(self):\n", "code": "params = self.__dict__.copy()\nservice_name = params.pop('service_name')\nreturn ResourceMeta(service_name, **params)", "path": "boto3/boto3/resources/base.py", "commit_date": "2016-11-20 00:00:00", "repo_name": "boto/boto3", "stars": 8612, "license": "apache-2.0", "language": "python", "size": 7954}
{"docstring": "# Verify #98, where the callback was being invoked\n# twice when using signature version 4.\n", "func_signal": "def test_callback_called_once_with_sigv4(self):\n", "code": "self.amount_seen = 0\nlock = threading.Lock()\ndef progress_callback(amount):\n    with lock:\n        self.amount_seen += amount\n\nclient = self.session.client(\n    's3', self.region,\n    config=Config(signature_version='s3v4'))\ntransfer = boto3.s3.transfer.S3Transfer(client)\nfilename = self.files.create_file_with_size(\n    '10mb.txt', filesize=10 * 1024 * 1024)\ntransfer.upload_file(filename, self.bucket_name,\n                     '10mb.txt', callback=progress_callback)\nself.addCleanup(self.delete_object, '10mb.txt')\n\nself.assertEqual(self.amount_seen, 10 * 1024 * 1024)", "path": "boto3/tests/integration/test_s3.py", "commit_date": "2019-07-03 00:00:00", "repo_name": "boto/boto3", "stars": 8612, "license": "apache-2.0", "language": "python", "size": 7954}
{"docstring": "# Always work on a copy of meta, otherwise we would affect other\n# instances of the same subclass.\n", "func_signal": "def __init__(self, *args, **kwargs):\n", "code": "self.meta = self.meta.copy()\n\n# Create a default client if none was passed\nif kwargs.get('client') is not None:\n    self.meta.client = kwargs.get('client')\nelse:\n    self.meta.client = boto3.client(self.meta.service_name)\n\n# Allow setting identifiers as positional arguments in the order\n# in which they were defined in the ResourceJSON.\nfor i, value in enumerate(args):\n    setattr(self, '_' + self.meta.identifiers[i], value)\n\n# Allow setting identifiers via keyword arguments. Here we need\n# extra logic to ignore other keyword arguments like ``client``.\nfor name, value in kwargs.items():\n    if name == 'client':\n        continue\n\n    if name not in self.meta.identifiers:\n        raise ValueError('Unknown keyword argument: {0}'.format(name))\n\n    setattr(self, '_' + name, value)\n\n# Validate that all identifiers have been set.\nfor identifier in self.meta.identifiers:\n    if getattr(self, identifier) is None:\n        raise ValueError(\n            'Required parameter {0} not set'.format(identifier))", "path": "boto3/boto3/resources/base.py", "commit_date": "2016-11-20 00:00:00", "repo_name": "boto/boto3", "stars": 8612, "license": "apache-2.0", "language": "python", "size": 7954}
{"docstring": "# This is just a sanity check to ensure that the bucket interface work.\n", "func_signal": "def test_transfer_methods_through_bucket(self):\n", "code": "key = 'bucket.txt'\nbucket = self.session.resource('s3').Bucket(self.bucket_name)\nfilename = self.files.create_file_with_size(key, 1024*1024)\nbucket.upload_file(Filename=filename, Key=key)\nself.addCleanup(self.delete_object, key)\ndownload_path = os.path.join(self.files.rootdir, unique_id('foo'))\nbucket.download_file(Key=key, Filename=download_path)\nassert_files_equal(filename, download_path)", "path": "boto3/tests/integration/test_s3.py", "commit_date": "2019-07-03 00:00:00", "repo_name": "boto/boto3", "stars": 8612, "license": "apache-2.0", "language": "python", "size": 7954}
{"docstring": "# Two metas are equal if their components are all equal\n", "func_signal": "def __eq__(self, other):\n", "code": "if other.__class__.__name__ != self.__class__.__name__:\n    return False\n\nreturn self.__dict__ == other.__dict__", "path": "boto3/boto3/resources/base.py", "commit_date": "2016-11-20 00:00:00", "repo_name": "boto/boto3", "stars": 8612, "license": "apache-2.0", "language": "python", "size": 7954}
{"docstring": "# Create some function to register.\n", "func_signal": "def test_events_attribute(self):\n", "code": "def my_handler(my_list, **kwargs):\n    return my_list.append('my_handler called')\n\n# Register the handler to the event.\nself.session.events.register('myevent', my_handler)\n\ninitial_list = []\n# Emit the event.\nself.session.events.emit('myevent', my_list=initial_list)\n# Ensure that the registered handler was called.\nself.assertEqual(initial_list, ['my_handler called'])", "path": "boto3/tests/functional/test_session.py", "commit_date": "2016-03-03 00:00:00", "repo_name": "boto/boto3", "stars": 8612, "license": "apache-2.0", "language": "python", "size": 7954}
{"docstring": "# We're picking the customer provided sse feature\n# of S3 to test the extra_args functionality of\n# S3.\n", "func_signal": "def test_can_send_extra_params_on_download(self):\n", "code": "key_bytes = os.urandom(32)\nextra_args = {\n    'SSECustomerKey': key_bytes,\n    'SSECustomerAlgorithm': 'AES256',\n}\nself.client.put_object(Bucket=self.bucket_name,\n                       Key='foo.txt',\n                       Body=b'hello world',\n                       **extra_args)\nself.addCleanup(self.delete_object, 'foo.txt')\ntransfer = self.create_s3_transfer()\n\ndownload_path = os.path.join(self.files.rootdir, 'downloaded.txt')\nself.wait_until_object_exists('foo.txt', extra_params=extra_args)\ntransfer.download_file(self.bucket_name, 'foo.txt',\n                       download_path, extra_args=extra_args)\nwith open(download_path, 'rb') as f:\n    self.assertEqual(f.read(), b'hello world')", "path": "boto3/tests/integration/test_s3.py", "commit_date": "2019-07-03 00:00:00", "repo_name": "boto/boto3", "stars": 8612, "license": "apache-2.0", "language": "python", "size": 7954}
{"docstring": "# This model has NO load method. Cached data should\n# never be cleared since it cannot be reloaded!\n", "func_signal": "def test_resource_action_leaves_data(self, action_cls):\n", "code": "model = {\n    'actions': {\n        'GetMessageStatus': {\n            'request': {\n                'operation': 'DescribeMessageStatus'\n            }\n        }\n    }\n}\n\nqueue = self.load('Queue', model)()\n\n# Simulate loaded data\nqueue.meta.data = {'some': 'data'}\n\n# Perform a call\nqueue.get_message_status()\n\n# Cached data should not be cleared\nself.assertEqual(queue.meta.data, {'some': 'data'})", "path": "boto3/tests/unit/resources/test_factory.py", "commit_date": "2016-11-20 00:00:00", "repo_name": "boto/boto3", "stars": 8612, "license": "apache-2.0", "language": "python", "size": 7954}
{"docstring": "# If the alias name is used, make sure we set the name that it points\n# to as that is what actually is used in governing the TransferManager.\n", "func_signal": "def __setattr__(self, name, value):\n", "code": "if name in self.ALIAS:\n    super(TransferConfig, self).__setattr__(self.ALIAS[name], value)\n# Always set the value of the actual name provided.\nsuper(TransferConfig, self).__setattr__(name, value)", "path": "boto3/boto3/s3/transfer.py", "commit_date": "2019-12-16 00:00:00", "repo_name": "boto/boto3", "stars": 8612, "license": "apache-2.0", "language": "python", "size": 7954}
{"docstring": "# Create a bucket\n", "func_signal": "def test_s3_resource_waiter(self):\n", "code": "bucket_name = random_bucket_name()\nbucket = self.create_bucket_resource(bucket_name)\n# Wait till the bucket exists\nbucket.wait_until_exists()\n# Confirm the bucket exists by finding it in a list of all of our\n# buckets\nself.assertIn(bucket_name,\n              [b.name for b in self.s3.buckets.all()])\n\n# Create an object\nobj = bucket.Object('test.txt')\nobj.put(\n    Body='hello, world')\nself.addCleanup(obj.delete)\n\n# Wait till the bucket exists\nobj.wait_until_exists()\n\n# List objects and make sure ours is present\nself.assertIn('test.txt', [o.key for o in bucket.objects.all()])", "path": "boto3/tests/integration/test_s3.py", "commit_date": "2019-07-03 00:00:00", "repo_name": "boto/boto3", "stars": 8612, "license": "apache-2.0", "language": "python", "size": 7954}
{"docstring": "# Only services should get dangling defs\n", "func_signal": "def test_non_service_resource_missing_defs(self):\n", "code": "defs = {\n    'Queue': {\n        'identifiers': [\n            {'name': 'Url'}\n        ]\n    },\n    'Message': {\n        'identifiers': [\n            {'name': 'QueueUrl'},\n            {'name': 'ReceiptHandle'}\n        ]\n    }\n}\n\nmodel = defs['Queue']\n\nqueue = self.load('Queue', model, defs)('url')\n\nself.assertTrue(not hasattr(queue, 'Queue'))\nself.assertTrue(not hasattr(queue, 'Message'))", "path": "boto3/tests/unit/resources/test_factory.py", "commit_date": "2016-11-20 00:00:00", "repo_name": "boto/boto3", "stars": 8612, "license": "apache-2.0", "language": "python", "size": 7954}
{"docstring": "# To simplify we'll assume this is hooked up\n# to a single filename.\n", "func_signal": "def __call__(self, bytes_amount):\n", "code": "with self._lock:\n    self._seen_so_far += bytes_amount\n    percentage = (self._seen_so_far / self._size) * 100\n    sys.stdout.write(\n        \"\\r%s  %s / %s  (%.2f%%)\" % (\n            self._filename, self._seen_so_far, self._size,\n            percentage))\n    sys.stdout.flush()", "path": "boto3/boto3/s3/transfer.py", "commit_date": "2019-12-16 00:00:00", "repo_name": "boto/boto3", "stars": 8612, "license": "apache-2.0", "language": "python", "size": 7954}
{"docstring": "# This has to be an integration test because the fileobj will never\n# actually be read from when using the stubber and therefore the\n# progress callbacks will not be invoked.\n", "func_signal": "def test_upload_fileobj_progress(self):\n", "code": "chunksize = 5 * (1024 ** 2)\nconfig = boto3.s3.transfer.TransferConfig(\n    multipart_chunksize=chunksize,\n    multipart_threshold=chunksize,\n    max_concurrency=1\n)\nfileobj = six.BytesIO(b'0' * (chunksize * 3))\n\ndef progress_callback(amount):\n    self.progress += amount\n\nself.client.upload_fileobj(\n    Fileobj=fileobj, Bucket=self.bucket_name, Key='foo',\n    Config=config, Callback=progress_callback)\nself.addCleanup(self.delete_object, 'foo')\n\nself.object_exists('foo')\nself.assertEqual(self.progress, chunksize * 3)", "path": "boto3/tests/integration/test_s3.py", "commit_date": "2019-07-03 00:00:00", "repo_name": "boto/boto3", "stars": 8612, "license": "apache-2.0", "language": "python", "size": 7954}
{"docstring": "# Create the multipart upload\n", "func_signal": "def test_s3_multipart(self):\n", "code": "mpu = self.bucket.Object('mp-test.txt').initiate_multipart_upload()\nself.addCleanup(mpu.abort)\n\n# Create and upload a part\npart = mpu.Part(1)\nresponse = part.upload(Body='hello, world!')\n\n# Complete the upload, which requires info on all of the parts\npart_info = {\n    'Parts': [\n        {\n            'PartNumber': 1,\n            'ETag': response['ETag']\n        }\n    ]\n}\n\nmpu.complete(MultipartUpload=part_info)\nself.addCleanup(self.bucket.Object('mp-test.txt').delete)\n\ncontents = self.bucket.Object('mp-test.txt').get()['Body'].read()\nself.assertEqual(contents, b'hello, world!')", "path": "boto3/tests/integration/test_s3.py", "commit_date": "2019-07-03 00:00:00", "repo_name": "boto/boto3", "stars": 8612, "license": "apache-2.0", "language": "python", "size": 7954}
{"docstring": "\"\"\"Creates a transfer manager based on configuration\n\n:type client: boto3.client\n:param client: The S3 client to use\n\n:type config: boto3.s3.transfer.TransferConfig\n:param config: The transfer config to use\n\n:type osutil: s3transfer.utils.OSUtils\n:param osutil: The os utility to use\n\n:rtype: s3transfer.manager.TransferManager\n:returns: A transfer manager based on parameters provided\n\"\"\"\n", "func_signal": "def create_transfer_manager(client, config, osutil=None):\n", "code": "executor_cls = None\nif not config.use_threads:\n    executor_cls = NonThreadedExecutor\nreturn TransferManager(client, config, osutil, executor_cls)", "path": "boto3/boto3/s3/transfer.py", "commit_date": "2019-12-16 00:00:00", "repo_name": "boto/boto3", "stars": 8612, "license": "apache-2.0", "language": "python", "size": 7954}
{"docstring": "\"\"\"Append contents to a file\n\n``filename`` should be a relative path, e.g. \"foo/bar/baz.txt\"\nIt will be translated into a full path in a tmp dir.\n\nReturns the full path to the file.\n\"\"\"\n", "func_signal": "def append_file(self, filename, contents):\n", "code": "full_path = os.path.join(self.rootdir, filename)\nif not os.path.isdir(os.path.dirname(full_path)):\n    os.makedirs(os.path.dirname(full_path))\nwith open(full_path, 'a') as f:\n    f.write(contents)\nreturn full_path", "path": "boto3/tests/integration/test_s3.py", "commit_date": "2019-07-03 00:00:00", "repo_name": "boto/boto3", "stars": 8612, "license": "apache-2.0", "language": "python", "size": 7954}
{"docstring": "\"\"\"Creates a file in a tmpdir\n\n``filename`` should be a relative path, e.g. \"foo/bar/baz.txt\"\nIt will be translated into a full path in a tmp dir.\n\n``mode`` is the mode the file should be opened either as ``w`` or\n`wb``.\n\nReturns the full path to the file.\n\n\"\"\"\n", "func_signal": "def create_file(self, filename, contents, mode='w'):\n", "code": "full_path = os.path.join(self.rootdir, filename)\nif not os.path.isdir(os.path.dirname(full_path)):\n    os.makedirs(os.path.dirname(full_path))\nwith open(full_path, mode) as f:\n    f.write(contents)\nreturn full_path", "path": "boto3/tests/integration/test_s3.py", "commit_date": "2019-07-03 00:00:00", "repo_name": "boto/boto3", "stars": 8612, "license": "apache-2.0", "language": "python", "size": 7954}
{"docstring": "# Should be instances of the same resource class\n", "func_signal": "def __eq__(self, other):\n", "code": "if other.__class__.__name__ != self.__class__.__name__:\n    return False\n\n# Each of the identifiers should have the same value in both\n# instances, e.g. two buckets need the same name to be equal.\nfor identifier in self.meta.identifiers:\n    if getattr(self, identifier) != getattr(other, identifier):\n        return False\n\nreturn True", "path": "boto3/boto3/resources/base.py", "commit_date": "2016-11-20 00:00:00", "repo_name": "boto/boto3", "stars": 8612, "license": "apache-2.0", "language": "python", "size": 7954}
{"docstring": "\"\"\"This is not really a test but a reminder that if you change the\nautoload_nb_js.js template then you should make sure that insertion of\nplots into notebooks is working as expected. In particular, this test was\ncreated as part of https://github.com/bokeh/bokeh/issues/7125.\n\"\"\"\n", "func_signal": "def test_autoload_template_has_changed() -> None:\n", "code": "with open(join(TOP_PATH, \"_templates/autoload_nb_js.js\"), mode=\"rb\") as f:\n    current_template_sha256 = compute_sha256(_crlf_cr_2_lf_bin(f.read()))\n    assert pinned_template_sha256 == current_template_sha256, \"\"\"\\\n        It seems that the template autoload_nb_js.js has changed.\n        If this is voluntary and that proper testing of plots insertion\n        in notebooks has been completed successfully, update this test\n        with the new file SHA256 signature.\"\"\"", "path": "bokeh/tests/unit/bokeh/core/test_templates.py", "commit_date": "2020-11-08 00:00:00", "repo_name": "bokeh/bokeh", "stars": 18648, "license": "bsd-3-clause", "language": "python", "size": 337335}
{"docstring": "# label is a field\n", "func_signal": "def test_compound_legend_behavior_initiated_if_labels_are_same_on_multiple_renderers_and_are_field(self, p, source) -> None:\n", "code": "square = p.square(x='x', y='y', legend_field='label', source=source)\ncircle = p.circle(x='x', y='y', legend_field='label', source=source)\nlegends = p.select(Legend)\nassert len(legends) == 1\nassert legends[0].items[0].renderers == [square, circle]\nassert legends[0].items[0].label == {'field': 'label'}", "path": "bokeh/tests/unit/bokeh/plotting/test_figure.py", "commit_date": "2020-07-27 00:00:00", "repo_name": "bokeh/bokeh", "stars": 18648, "license": "bsd-3-clause", "language": "python", "size": 337335}
{"docstring": "''' Basic usage of Bokeh should not result in any client/server code being\nimported. This test ensures that importing basic modules does not bring in\nbokeh.client or bokeh.server.\n\n'''\n", "func_signal": "def test_no_client_server_common() -> None:\n", "code": "code = \"import sys; %s; sys.exit(1 if any(('bokeh.client' in x or 'bokeh.server' in x) for x in sys.modules.keys()) else 0)\"\nproc = Popen([executable, \"-c\", code % \";\".join(BASIC_IMPORTS)],stdout=PIPE)\nproc.wait()\nif proc.returncode != 0:\n    assert False", "path": "bokeh/tests/codebase/test_no_client_server_common.py", "commit_date": "2020-02-07 00:00:00", "repo_name": "bokeh/bokeh", "stars": 18648, "license": "bsd-3-clause", "language": "python", "size": 337335}
{"docstring": "# The test has probabilistic nature - there's a slight change it'll give a false negative\n", "func_signal": "def test_adding_next_tick_from_another_thread(self) -> None:\n", "code": "with LoopAndGroup(quit_after=15) as ctx:\n    n = 1000\n    func = _make_invocation_counter(ctx.io_loop, stop_after=n)\n    tpe = ThreadPoolExecutor(n)\n    list(tpe.map(ctx.group.add_next_tick_callback, repeat(func, n)))\nassert n == func.count()", "path": "bokeh/tests/unit/bokeh/server/test_callbacks__server.py", "commit_date": "2020-07-27 00:00:00", "repo_name": "bokeh/bokeh", "stars": 18648, "license": "bsd-3-clause", "language": "python", "size": 337335}
{"docstring": "\"\"\"Test effect of 'restrict=False' with explicit JS callback\"\"\"\n", "func_signal": "def test_no_restriction(self, bokeh_model_page) -> None:\n", "code": "text_input = AutocompleteInput(css_classes=[\"foo\"], completions = [\"aAaBbb\", \"aAaBbB\"], restrict=False)\ntext_input.js_on_change('value', CustomJS(code=RECORD(\"value\", \"cb_obj.value\")))\n\npage = bokeh_model_page(text_input)\n\nel = page.driver.find_element_by_css_selector('.foo input')\ntext = \"not in completions\"\nenter_text_in_element(page.driver, el, text, click=1, enter=True)\n\nresults = page.results\nassert results['value'] == text\nassert page.has_no_console_errors()", "path": "bokeh/tests/integration/widgets/test_autocomplete_input.py", "commit_date": "2020-09-17 00:00:00", "repo_name": "bokeh/bokeh", "stars": 18648, "license": "bsd-3-clause", "language": "python", "size": 337335}
{"docstring": "''' Applies a collection of general codebase style and quality rules to\nevery file inm the repository. Unless specifically excepted:\n\n* Files should not contain tabs\n* Files should not start with newlines\n* Files should end with one empty line\n* Lines should not contain trailing whitespace\n* Lines should not exceed 160 characters\n\n'''\n", "func_signal": "def test_code_quality() -> None:\n", "code": "errors = collect_errors()\nassert len(errors) == 0, \"Code quality issues:\\n%s\" % \"\\n\".join(errors)", "path": "bokeh/tests/codebase/test_code_quality.py", "commit_date": "2020-07-27 00:00:00", "repo_name": "bokeh/bokeh", "stars": 18648, "license": "bsd-3-clause", "language": "python", "size": 337335}
{"docstring": "\"\"\"Test that input entered manually doesn't end up in the value.\"\"\"\n", "func_signal": "def test_server_restriction_to_list(self, bokeh_server_page) -> None:\n", "code": "text_input = AutocompleteInput(css_classes=[\"foo\"], completions = [\"aAaBbb\"], restrict=True)\n\ndef add_autocomplete(doc):\n    # note: for some reason, bokeh_server_page requires a 'canvas' in the document\n    plot = Plot()\n    doc.add_root(column(text_input,plot))\n\npage = bokeh_server_page(add_autocomplete)\n\nel = page.driver.find_element_by_css_selector('.foo input')\ntext = \"not in completions\"\nenter_text_in_element(page.driver, el, text, click=1, enter=True)\n\nassert text_input.value == ''\nassert page.has_no_console_errors()", "path": "bokeh/tests/integration/widgets/test_autocomplete_input.py", "commit_date": "2020-09-17 00:00:00", "repo_name": "bokeh/bokeh", "stars": 18648, "license": "bsd-3-clause", "language": "python", "size": 337335}
{"docstring": "# label is a field\n", "func_signal": "def test_compound_legend_behavior_initiated_if_labels_are_same_on_multiple_renderers_and_are_field(self, p, source) -> None:\n", "code": "square = p.square(x='x', y='y', legend='label', source=source)\ncircle = p.circle(x='x', y='y', legend='label', source=source)\nlegends = p.select(Legend)\nassert len(legends) == 1\nassert legends[0].items[0].renderers == [square, circle]\nassert legends[0].items[0].label == {'field': 'label'}", "path": "bokeh/tests/unit/bokeh/plotting/test_figure.py", "commit_date": "2020-07-27 00:00:00", "repo_name": "bokeh/bokeh", "stars": 18648, "license": "bsd-3-clause", "language": "python", "size": 337335}
{"docstring": "# case_sensitive=True by default\n", "func_signal": "def test_case_sensitivity(self, bokeh_model_page) -> None:\n", "code": "text_input = AutocompleteInput(title=\"title\", css_classes=[\"foo\"], completions = [\"100001\", \"aAaaaa\", \"aAaBbb\", \"AAAaAA\", \"aAaBbB\"])\n\npage = bokeh_model_page(text_input)\n\nel = page.driver.find_element_by_css_selector('.foo .bk-menu')\nassert 'display: none;' in el.get_attribute('style')\n\n# double click to highlight and overwrite old text\nel = page.driver.find_element_by_css_selector('.foo input')\nenter_text_in_element(page.driver, el, \"aAa\", click=2, enter=False)\n\nel = page.driver.find_element_by_css_selector('.foo .bk-menu')\nassert 'display: none;' not in el.get_attribute('style')\n\nitems = el.find_elements_by_tag_name(\"div\")\nassert len(items) == 3\nassert items[0].text == \"aAaaaa\"\nassert items[1].text == \"aAaBbb\"\nassert items[2].text == \"aAaBbB\"\nassert \"bk-active\" in items[0].get_attribute('class')\n\nel = page.driver.find_element_by_css_selector('.foo input')\nenter_text_in_element(page.driver, el, \"aAaB\", click=2, enter=False)\n\nel = page.driver.find_element_by_css_selector('.foo .bk-menu')\nassert 'display: none;' not in el.get_attribute('style')\n\nitems = el.find_elements_by_tag_name(\"div\")\nassert len(items) == 2\nassert items[0].text == \"aAaBbb\"\nassert items[1].text == \"aAaBbB\"\nassert \"bk-active\" in items[0].get_attribute('class')\n\nenter_text_in_element(page.driver, el, Keys.DOWN, click=0, enter=False)\nitems = el.find_elements_by_tag_name(\"div\")\nassert len(items) == 2\nassert items[0].text == \"aAaBbb\"\nassert items[1].text == \"aAaBbB\"\nassert \"bk-active\" not in items[0].get_attribute('class')\nassert \"bk-active\" in items[1].get_attribute('class')\n\nassert page.has_no_console_errors()", "path": "bokeh/tests/integration/widgets/test_autocomplete_input.py", "commit_date": "2020-09-17 00:00:00", "repo_name": "bokeh/bokeh", "stars": 18648, "license": "bsd-3-clause", "language": "python", "size": 337335}
{"docstring": "# test whether adding a figure (*and* it's extra ranges)\n# to another's references doesn't create a false positive\n", "func_signal": "def test_bad_extra_range_only_immediate_refs(self) -> None:\n", "code": "p, dep = figure(), figure()\ndep.extra_x_ranges['foo'] = Range1d()\ndep.grid.x_range_name=\"foo\"\np.grid[0].js_on_change(\"dimension\", CustomJS(code = \"\", args = {\"toto\": dep.grid[0]}))\nwith mock.patch('bokeh.core.validation.check.log') as mock_logger:\n    check_integrity([p])\nassert mock_logger.error.call_count == 0", "path": "bokeh/tests/unit/bokeh/models/test_plots.py", "commit_date": "2020-08-31 00:00:00", "repo_name": "bokeh/bokeh", "stars": 18648, "license": "bsd-3-clause", "language": "python", "size": 337335}
{"docstring": "# TODO: use this when soft=False\n#\n#with pytest.raises(ValueError):\n#    bms.ColumnDataSource(data=dict(a=[10, 11], b=[20, 21, 22]))\n#\n#ds = bms.ColumnDataSource()\n#with pytest.raises(ValueError):\n#    ds.data = dict(a=[10, 11], b=[20, 21, 22])\n#\n#ds = bms.ColumnDataSource(data=dict(a=[10, 11]))\n#with pytest.raises(ValueError):\n#    ds.data[\"b\"] = [20, 21, 22]\n#\n#ds = bms.ColumnDataSource(data=dict(a=[10, 11], b=[20, 21]))\n#with pytest.raises(ValueError):\n#    ds.data.update(dict(a=[10, 11, 12]))\n\n", "func_signal": "def test_data_column_lengths(self) -> None:\n", "code": "with warnings.catch_warnings(record=True) as warns:\n    bms.ColumnDataSource(data=dict(a=[10, 11], b=[20, 21, 22]))\nassert len(warns) == 1\nassert str(warns[0].message) == \"ColumnDataSource's columns must be of the same length. Current lengths: ('a', 2), ('b', 3)\"\n\nds = bms.ColumnDataSource()\nwith warnings.catch_warnings(record=True) as warns:\n    ds.data = dict(a=[10, 11], b=[20, 21, 22])\nassert len(warns) == 1\nassert str(warns[0].message) == \"ColumnDataSource's columns must be of the same length. Current lengths: ('a', 2), ('b', 3)\"\n\nds = bms.ColumnDataSource(data=dict(a=[10, 11]))\nwith warnings.catch_warnings(record=True) as warns:\n    ds.data[\"b\"] = [20, 21, 22]\nassert len(warns) == 1\nassert str(warns[0].message) == \"ColumnDataSource's columns must be of the same length. Current lengths: ('a', 2), ('b', 3)\"\n\nds = bms.ColumnDataSource(data=dict(a=[10, 11], b=[20, 21]))\nwith warnings.catch_warnings(record=True) as warns:\n    ds.data.update(dict(a=[10, 11, 12]))\nassert len(warns) == 1\nassert str(warns[0].message) == \"ColumnDataSource's columns must be of the same length. Current lengths: ('a', 3), ('b', 2)\"", "path": "bokeh/tests/unit/bokeh/models/test_sources.py", "commit_date": "2020-07-27 00:00:00", "repo_name": "bokeh/bokeh", "stars": 18648, "license": "bsd-3-clause", "language": "python", "size": 337335}
{"docstring": "# other tests may have interacted with the global biw.webdriver_control,\n# so create a new instance only to check default values\n", "func_signal": "def test_default(self) -> None:\n", "code": "wc = biw._WebdriverState()\nassert wc.reuse == True\nassert wc.kind == None\nassert wc.current is None", "path": "bokeh/tests/unit/bokeh/io/test_webdriver.py", "commit_date": "2020-08-12 00:00:00", "repo_name": "bokeh/bokeh", "stars": 18648, "license": "bsd-3-clause", "language": "python", "size": 337335}
{"docstring": "''' Basic usage of Bokeh should not result in any Tornado code being\nimported. This test ensures that importing basic modules does not bring in\nTornado.\n\n'''\n", "func_signal": "def test_no_tornado_common() -> None:\n", "code": "proc = Popen([\n    executable, \"-c\", \"import sys; %s; sys.exit(1 if any('tornado' in x for x in sys.modules.keys()) else 0)\" % \";\".join(BASIC_IMPORTS)\n],stdout=PIPE)\nproc.communicate()\nproc.wait()\nif proc.returncode != 0:\n    assert False", "path": "bokeh/tests/codebase/test_no_tornado_common.py", "commit_date": "2020-02-07 00:00:00", "repo_name": "bokeh/bokeh", "stars": 18648, "license": "bsd-3-clause", "language": "python", "size": 337335}
{"docstring": "# This test is pretty clunky by assures that callbacks triggered by\n# events use the correct value of curdoc()\n", "func_signal": "def test__trigger_event_wraps_curdoc(self) -> None:\n", "code": "from bokeh.io.doc import set_curdoc\nfrom bokeh.io import curdoc\noldcd = curdoc()\nd1 = Document()\nd2 = Document()\nset_curdoc(d1)\nout = {}\ndef cb():\n    out['curdoc'] = curdoc()\nm = cbm.EventCallbackManager()\nm.subscribed_events = []\nm.on_event('foo', cb)\nm.id = 10\nm._document = d2\n\nassert len(m._event_callbacks) == 1\nassert m._event_callbacks['foo'] == [cb]\n\nclass ev:\n    _model_id = 10\n    event_name = \"foo\"\n\nm._trigger_event(ev())\nassert out['curdoc'] is d2\n\nset_curdoc(oldcd)", "path": "bokeh/tests/unit/bokeh/util/test_callback_manager.py", "commit_date": "2020-07-27 00:00:00", "repo_name": "bokeh/bokeh", "stars": 18648, "license": "bsd-3-clause", "language": "python", "size": 337335}
{"docstring": "\"\"\"Test effect of 'restrict=False' without explicit callback.\"\"\"\n", "func_signal": "def test_server_no_restriction(self, bokeh_server_page) -> None:\n", "code": "text_input = AutocompleteInput(css_classes=[\"foo\"], completions = [\"aAaBbb\", \"aAaBbB\"], restrict=False)\n\ndef add_autocomplete(doc):\n    # note: for some reason, bokeh_server_page requires a 'canvas' in the document\n    plot = Plot()\n    doc.add_root(column(text_input,plot))\n\npage = bokeh_server_page(add_autocomplete)\n\nel = page.driver.find_element_by_css_selector('.foo input')\ntext = \"not in completions\"\nenter_text_in_element(page.driver, el, text, click=1, enter=True)\n\nassert text_input.value == text\nassert page.has_no_console_errors()", "path": "bokeh/tests/integration/widgets/test_autocomplete_input.py", "commit_date": "2020-09-17 00:00:00", "repo_name": "bokeh/bokeh", "stars": 18648, "license": "bsd-3-clause", "language": "python", "size": 337335}
{"docstring": "\"\"\" Returns True if there are tabs in the leading whitespace of a line,\n    including the whitespace of docstring code samples.\n\"\"\"\n", "func_signal": "def tab_in_leading(s):\n", "code": "n = len(s) - len(s.lstrip())\nif not s[n:n + 3] in ['...', '>>>']:\n    check = s[:n]\nelse:\n    smore = s[n + 3:]\n    check = s[:n] + smore[:len(smore) - len(smore.lstrip())]\nreturn check.expandtabs() != check", "path": "bokeh/tests/codebase/test_code_quality.py", "commit_date": "2020-07-27 00:00:00", "repo_name": "bokeh/bokeh", "stars": 18648, "license": "bsd-3-clause", "language": "python", "size": 337335}
{"docstring": "# 'compound legend string' is just a value\n", "func_signal": "def test_compound_legend_behavior_initiated_if_labels_are_same_on_multiple_renderers(self, p, source) -> None:\n", "code": "square = p.square(x='x', y='y', legend='compound legend string')\ncircle = p.circle(x='x', y='y', legend='compound legend string')\nlegends = p.select(Legend)\nassert len(legends) == 1\nassert legends[0].items[0].renderers == [square, circle]\nassert legends[0].items[0].label == value('compound legend string')", "path": "bokeh/tests/unit/bokeh/plotting/test_figure.py", "commit_date": "2020-07-27 00:00:00", "repo_name": "bokeh/bokeh", "stars": 18648, "license": "bsd-3-clause", "language": "python", "size": 337335}
{"docstring": "''' Running python with -OO will discard docstrings (__doc__ is None)\nwhich can cause problems if docstrings are naively formatted.\n\nThis test ensures that the all modules are importable, even with -OO set.\n\nIf you encounter a new problem with docstrings being formatted, try\nusing format_docstring.\n'''\n", "func_signal": "def test_python_execution_with_OO() -> None:\n", "code": "os.chdir(TOP_PATH)\n\nimports = []\nfor path, _, files in os.walk(\"bokeh\"):\n    if \"tests\" in path:\n        continue\n\n    for file in files:\n        if not file.endswith(\".py\"):\n            continue\n        if file.endswith(\"__main__.py\"):\n            continue\n\n        if file.endswith(\"__init__.py\"):\n            mod = path.replace(os.sep, \".\")\n        else:\n            mod = path.replace(os.sep, \".\") + \".\" + file[:-3]\n\n        if mod in skiplist:\n            continue\n\n        imports.append(\"import \" + mod)\n\ntest_env = os.environ.copy()\ntest_env['BOKEH_DOCS_MISSING_API_KEY_OK'] = 'yes'\n\nproc = Popen([executable, \"-OO\", \"-\"], stdout=PIPE, stdin=PIPE, env=test_env)\nproc.communicate(\"\\n\".join(imports).encode(\"utf-8\"))\nproc.wait()\n\nif proc.returncode != 0:\n    assert False", "path": "bokeh/tests/codebase/test_python_execution_with_OO.py", "commit_date": "2020-06-12 00:00:00", "repo_name": "bokeh/bokeh", "stars": 18648, "license": "bsd-3-clause", "language": "python", "size": 337335}
{"docstring": "# 'compound legend string' is just a value\n", "func_signal": "def test_compound_legend_behavior_initiated_if_labels_are_same_on_multiple_renderers(self, p, source) -> None:\n", "code": "square = p.square(x='x', y='y', legend_label='compound legend string')\ncircle = p.circle(x='x', y='y', legend_label='compound legend string')\nlegends = p.select(Legend)\nassert len(legends) == 1\nassert legends[0].items[0].renderers == [square, circle]\nassert legends[0].items[0].label == value('compound legend string')", "path": "bokeh/tests/unit/bokeh/plotting/test_figure.py", "commit_date": "2020-07-27 00:00:00", "repo_name": "bokeh/bokeh", "stars": 18648, "license": "bsd-3-clause", "language": "python", "size": 337335}
{"docstring": "# we aren't trying to replace test_query here, only test\n# our wrappers around it, so no need to try every kind of\n# query\n", "func_signal": "def test_select(self) -> None:\n", "code": "d = document.Document()\nroot1 = SomeModelInTestDocument(foo=42, name='a')\nchild1 = SomeModelInTestDocument(foo=43, name='b')\nroot2 = SomeModelInTestDocument(foo=44, name='c')\nroot3 = SomeModelInTestDocument(foo=44, name='d')\nchild3 = SomeModelInTestDocument(foo=45, name='c')\nroot4 = AnotherModelInTestDocument(bar=20, name='A')\nroot1.child = child1\nroot3.child = child3\nd.add_root(root1)\nd.add_root(root2)\nd.add_root(root3)\nd.add_root(root4)\n\n# select()\nassert {root1} == set(d.select(dict(foo=42)))\nassert {root1} == set(d.select(dict(name=\"a\")))\nassert {root2, child3} == set(d.select(dict(name=\"c\")))\nassert set() == set(d.select(dict(name=\"nope\")))\n\n# select() on object\nassert set() == set(root3.select(dict(name=\"a\")))\nassert {child3} == set(root3.select(dict(name=\"c\")))\n\n# select_one()\nassert root3 == d.select_one(dict(name='d'))\nassert None == d.select_one(dict(name='nope'))\ngot_error = False\ntry:\n    d.select_one(dict(name='c'))\nexcept ValueError as e:\n    got_error = True\n    assert 'Found more than one' in repr(e)\nassert got_error\n\n# select_one() on object\nassert None == root3.select_one(dict(name='a'))\nassert child3 == root3.select_one(dict(name='c'))\n\n# set_select()\nd.set_select(dict(foo=44), dict(name=\"c\"))\nassert {root2, child3, root3} == set(d.select(dict(name=\"c\")))\n\n# set_select() on object\nroot3.set_select(dict(name='c'), dict(foo=57))\nassert {child3, root3} == set(d.select(dict(foo=57)))\nassert {child3, root3} == set(root3.select(dict(foo=57)))\n\n# set_select() on class\nd.set_select(SomeModelInTestDocument, dict(name='new_name'))\nassert len(d.select(dict(name='new_name'))) == 5\n\n# set_select() on different class\nassert len(d.select(dict(name=\"A\"))) == 1\nd.set_select(AnotherModelInTestDocument, dict(name=\"B\"))\nassert {root4} == set(d.select(dict(name=\"B\")))", "path": "bokeh/tests/unit/bokeh/document/test_document.py", "commit_date": "2020-10-14 00:00:00", "repo_name": "bokeh/bokeh", "stars": 18648, "license": "bsd-3-clause", "language": "python", "size": 337335}
{"docstring": "\"\"\" Mix the four bytes of every column in a linear way\n    This is the opposite operation of Mixcolumn \"\"\"\n", "func_signal": "def InvMixColumns(a):\n", "code": "Sprime = [0,0,0,0]\nfor j in range(a.Nb):    # for each column\n    Sprime[0] = mul(0x0E,a.state[j][0])^mul(0x0B,a.state[j][1])^mul(0x0D,a.state[j][2])^mul(0x09,a.state[j][3])\n    Sprime[1] = mul(0x09,a.state[j][0])^mul(0x0E,a.state[j][1])^mul(0x0B,a.state[j][2])^mul(0x0D,a.state[j][3])\n    Sprime[2] = mul(0x0D,a.state[j][0])^mul(0x09,a.state[j][1])^mul(0x0E,a.state[j][2])^mul(0x0B,a.state[j][3])\n    Sprime[3] = mul(0x0B,a.state[j][0])^mul(0x0D,a.state[j][1])^mul(0x09,a.state[j][2])^mul(0x0E,a.state[j][3])\n    for i in range(4):\n        a.state[j][i] = Sprime[i]", "path": "DeDRM_tools/DeDRM_plugin/kindlekey.py", "commit_date": "2020-11-28 00:00:00", "repo_name": "apprenticeharper/DeDRM_tools", "stars": 14119, "license": "None", "language": "python", "size": 24858}
{"docstring": "\"\"\" CBC algorithms are created by initializing with a BlockCipher instance \"\"\"\n", "func_signal": "def __init__(self, blockCipherInstance, padding = padWithPadLen()):\n", "code": "self.baseCipher = blockCipherInstance\nself.name       = self.baseCipher.name + '_CBC'\nself.blockSize  = self.baseCipher.blockSize\nself.keySize    = self.baseCipher.keySize\nself.padding    = padding\nself.baseCipher.padding = noPadding()   # baseCipher should NOT pad!!\nself.r          = Random()            # for IV generation, currently uses\n                                      # mediocre standard distro version     <----------------\nimport time\nnewSeed = time.ctime()+str(self.r)    # seed with instance location\nself.r.seed(newSeed)                  # to make unique\nself.reset()", "path": "DeDRM_tools/DeDRM_plugin/kindlekey.py", "commit_date": "2020-11-28 00:00:00", "repo_name": "apprenticeharper/DeDRM_tools", "stars": 14119, "license": "None", "language": "python", "size": 24858}
{"docstring": "\"\"\" Remove padding from a binary string \"\"\"\n", "func_signal": "def removePad(self, paddedBinaryString, blockSize):\n", "code": "if not(0<len(paddedBinaryString)):\n    raise DecryptNotBlockAlignedError('Expected More Data')\nreturn paddedBinaryString[:-ord(paddedBinaryString[-1])]", "path": "DeDRM_tools/DeDRM_plugin/kindlekey.py", "commit_date": "2020-11-28 00:00:00", "repo_name": "apprenticeharper/DeDRM_tools", "stars": 14119, "license": "None", "language": "python", "size": 24858}
{"docstring": "\"\"\" CBC block encryption, IV is set with 'encrypt' \"\"\"\n", "func_signal": "def encryptBlock(self, plainTextBlock):\n", "code": "auto_IV = ''\nif self.encryptBlockCount == 0:\n    if self.iv == None:\n        # generate IV and use\n        self.iv = ''.join([chr(self.r.randrange(256)) for i in range(self.blockSize)])\n        self.prior_encr_CT_block = self.iv\n        auto_IV = self.prior_encr_CT_block    # prepend IV if it's automatic\n    else:                       # application provided IV\n        assert(len(self.iv) == self.blockSize ),'IV must be same length as block'\n        self.prior_encr_CT_block = self.iv\n\"\"\" encrypt the prior CT XORed with the PT \"\"\"\nct = self.baseCipher.encryptBlock( xor(self.prior_encr_CT_block, plainTextBlock) )\nself.prior_encr_CT_block = ct\nreturn auto_IV+ct", "path": "DeDRM_tools/DeDRM_plugin/kindlekey.py", "commit_date": "2020-11-28 00:00:00", "repo_name": "apprenticeharper/DeDRM_tools", "stars": 14119, "license": "None", "language": "python", "size": 24858}
{"docstring": "\"\"\" XOR the algorithm state with a block of key material \"\"\"\n", "func_signal": "def AddRoundKey(algInstance, keyBlock):\n", "code": "for column in range(algInstance.Nb):\n    for row in range(4):\n        algInstance.state[column][row] ^= keyBlock[column][row]", "path": "DeDRM_tools/DeDRM_plugin/kindlekey.py", "commit_date": "2020-11-28 00:00:00", "repo_name": "apprenticeharper/DeDRM_tools", "stars": 14119, "license": "None", "language": "python", "size": 24858}
{"docstring": "'''\nRecursively decipher X.\n'''\n", "func_signal": "def decipher_all(decipher, objid, genno, x):\n", "code": "if isinstance(x, str):\n    return decipher(objid, genno, x)\ndecf = lambda v: decipher_all(decipher, objid, genno, v)\nif isinstance(x, list):\n    x = [decf(v) for v in x]\nelif isinstance(x, dict):\n    x = dict((k, decf(v)) for (k, v) in x.iteritems())\nreturn x", "path": "DeDRM_tools/DeDRM_plugin/ignoblepdf.py", "commit_date": "2020-10-14 00:00:00", "repo_name": "apprenticeharper/DeDRM_tools", "stars": 14119, "license": "None", "language": "python", "size": 24858}
{"docstring": "'''\nFetches a next line backword. This is used to locate\nthe trailers at the end of a file.\n'''\n", "func_signal": "def revreadlines(self):\n", "code": "self.fp.seek(0, 2)\npos = self.fp.tell()\nbuf = ''\nwhile 0 < pos:\n    prevpos = pos\n    pos = max(0, pos-self.BUFSIZ)\n    self.fp.seek(pos)\n    s = self.fp.read(prevpos-pos)\n    if not s: break\n    while 1:\n        n = max(s.rfind('\\r'), s.rfind('\\n'))\n        if n == -1:\n            buf = s + buf\n            break\n        yield s[n:]+buf\n        s = s[:n]\n        buf = ''\nreturn", "path": "DeDRM_tools/DeDRM_plugin/ignoblepdf.py", "commit_date": "2020-10-14 00:00:00", "repo_name": "apprenticeharper/DeDRM_tools", "stars": 14119, "license": "None", "language": "python", "size": 24858}
{"docstring": "# change email and password to utf-8 if unicode\n", "func_signal": "def fetch_key(email, password):\n", "code": "if type(email)==str:\n    email = email.encode('utf-8')\nif type(password)==str:\n    password = password.encode('utf-8')\n\nimport random\nrandom = \"%030x\" % random.randrange(16**30)\n\nimport urllib.parse, urllib.request, re\n\n# try the URL from nook for PC\nfetch_url = \"https://cart4.barnesandnoble.com/services/service.aspx?Version=2&acctPassword=\"\nfetch_url += urllib.parse.quote(password,'')+\"&devID=PC_BN_2.5.6.9575_\"+random+\"&emailAddress=\"\nfetch_url += urllib.parse.quote(email,\"\")+\"&outFormat=5&schema=1&service=1&stage=deviceHashB\"\n#print fetch_url\n\nfound = ''\ntry:\n    response = urllib.request.urlopen(fetch_url)\n    the_page = response.read()\n    #print the_page\n    found = re.search('ccHash>(.+?)</ccHash', the_page).group(1)\nexcept:\n    found = ''\nif len(found)!=28:\n    # try the URL from android devices\n    fetch_url = \"https://cart4.barnesandnoble.com/services/service.aspx?Version=2&acctPassword=\"\n    fetch_url += urllib.parse.quote(password,'')+\"&devID=hobbes_9.3.50818_\"+random+\"&emailAddress=\"\n    fetch_url += urllib.parse.quote(email,\"\")+\"&outFormat=5&schema=1&service=1&stage=deviceHashB\"\n    #print fetch_url\n\n    found = ''\n    try:\n        response = urllib.request.urlopen(fetch_url)\n        the_page = response.read()\n        #print the_page\n        found = re.search('ccHash>(.+?)</ccHash', the_page).group(1)\n    except:\n        found = ''\n\nreturn found", "path": "DeDRM_tools/DeDRM_plugin/ignoblekeyfetch.py", "commit_date": "2020-10-14 00:00:00", "repo_name": "apprenticeharper/DeDRM_tools", "stars": 14119, "license": "None", "language": "python", "size": 24858}
{"docstring": "# remove spaces and case from name and CC numbers.\n", "func_signal": "def generate_key(name, ccn):\n", "code": "name = normalize_name(name)\nccn = normalize_name(ccn)\n\nif type(name)==str:\n    name = name.encode('utf-8')\nif type(ccn)==str:\n    ccn = ccn.encode('utf-8')\n\nname = name + b'\\x00'\nccn = ccn + b'\\x00'\n\nname_sha = hashlib.sha1(name).digest()[:16]\nccn_sha = hashlib.sha1(ccn).digest()[:16]\nboth_sha = hashlib.sha1(name + ccn).digest()\naes = AES(ccn_sha, name_sha)\ncrypt = aes.encrypt(both_sha + (b'\\x0c' * 0x0c))\nuserkey = hashlib.sha1(crypt).digest()\nreturn base64.b64encode(userkey)", "path": "DeDRM_tools/DeDRM_plugin/ignoblekeygen.py", "commit_date": "2020-10-14 00:00:00", "repo_name": "apprenticeharper/DeDRM_tools", "stars": 14119, "license": "None", "language": "python", "size": 24858}
{"docstring": "# substitute filename unfriendly characters\n", "func_signal": "def cleanup_name(name):\n", "code": "name = name.replace(\"<\",\"[\").replace(\">\",\"]\").replace(\" : \",\" \u2013 \").replace(\": \",\" \u2013 \").replace(\":\",\"\u2014\").replace(\"/\",\"_\").replace(\"\\\\\",\"_\").replace(\"|\",\"_\").replace(\"\\\"\",\"\\'\").replace(\"*\",\"_\").replace(\"?\",\"\")\n# white space to single space, delete leading and trailing while space\nname = re.sub(r\"\\s\", \" \", name).strip()\n# delete control characters\nname = \"\".join(char for char in name if ord(char)>=32)\n# delete non-ascii characters\nname = \"\".join(char for char in name if ord(char)<=126)\n# remove leading dots\nwhile len(name)>0 and name[0] == \".\":\n    name = name[1:]\n# remove trailing dots (Windows doesn't like them)\nwhile name.endswith(\".\"):\n    name = name[:-1]\nif len(name)==0:\n    name=\"DecryptedBook\"\nreturn name", "path": "DeDRM_tools/DeDRM_plugin/k4mobidedrm.py", "commit_date": "2020-10-16 00:00:00", "repo_name": "apprenticeharper/DeDRM_tools", "stars": 14119, "license": "None", "language": "python", "size": 24858}
{"docstring": "# file searches can take a long time on some systems, so just look in known specific places.\n", "func_signal": "def getKindleInfoFiles():\n", "code": "kInfoFiles=[]\nfound = False\nhome = os.getenv('HOME')\n# check for  .kinf2018 file in new location (App Store Kindle for Mac)\ntestpath = home + '/Library/Containers/com.amazon.Kindle/Data/Library/Application Support/Kindle/storage/.kinf2018'\nif os.path.isfile(testpath):\n    kInfoFiles.append(testpath)\n    print('Found k4Mac kinf2018 file: ' + testpath)\n    found = True\n# check for  .kinf2018 files\ntestpath = home + '/Library/Application Support/Kindle/storage/.kinf2018'\nif os.path.isfile(testpath):\n    kInfoFiles.append(testpath)\n    print('Found k4Mac kinf2018 file: ' + testpath)\n    found = True\n# check for  .kinf2011 file in new location (App Store Kindle for Mac)\ntestpath = home + '/Library/Containers/com.amazon.Kindle/Data/Library/Application Support/Kindle/storage/.kinf2011'\nif os.path.isfile(testpath):\n    kInfoFiles.append(testpath)\n    print('Found k4Mac kinf2011 file: ' + testpath)\n    found = True\n# check for  .kinf2011 files from 1.10\ntestpath = home + '/Library/Application Support/Kindle/storage/.kinf2011'\nif os.path.isfile(testpath):\n    kInfoFiles.append(testpath)\n    print('Found k4Mac kinf2011 file: ' + testpath)\n    found = True\n# check for  .rainier-2.1.1-kinf files from 1.6\ntestpath = home + '/Library/Application Support/Kindle/storage/.rainier-2.1.1-kinf'\nif os.path.isfile(testpath):\n    kInfoFiles.append(testpath)\n    print('Found k4Mac rainier file: ' + testpath)\n    found = True\n# check for  .kindle-info files from 1.4\ntestpath = home + '/Library/Application Support/Kindle/storage/.kindle-info'\nif os.path.isfile(testpath):\n    kInfoFiles.append(testpath)\n    print('Found k4Mac kindle-info file: ' + testpath)\n    found = True\n# check for  .kindle-info file from 1.2.2\ntestpath = home + '/Library/Application Support/Amazon/Kindle/storage/.kindle-info'\nif os.path.isfile(testpath):\n    kInfoFiles.append(testpath)\n    print('Found k4Mac kindle-info file: ' + testpath)\n    found = True\n# check for  .kindle-info file from 1.0 beta 1 (27214)\ntestpath = home + '/Library/Application Support/Amazon/Kindle for Mac/storage/.kindle-info'\nif os.path.isfile(testpath):\n    kInfoFiles.append(testpath)\n    print('Found k4Mac kindle-info file: ' + testpath)\n    found = True\nif not found:\n    print('No k4Mac kindle-info/rainier/kinf2011 files have been found.')\nreturn kInfoFiles", "path": "DeDRM_tools/DeDRM_plugin/kindlekey.py", "commit_date": "2020-11-28 00:00:00", "repo_name": "apprenticeharper/DeDRM_tools", "stars": 14119, "license": "None", "language": "python", "size": 24858}
{"docstring": "\"\"\" Encrypt a string and return a binary string \"\"\"\n", "func_signal": "def encrypt(self, plainText, more = None):\n", "code": "self.bytesToEncrypt += plainText  # append plainText to any bytes from prior encrypt\nnumBlocks, numExtraBytes = divmod(len(self.bytesToEncrypt), self.blockSize)\ncipherText = ''\nfor i in range(numBlocks):\n    bStart = i*self.blockSize\n    ctBlock = self.encryptBlock(self.bytesToEncrypt[bStart:bStart+self.blockSize])\n    self.encryptBlockCount += 1\n    cipherText += ctBlock\nif numExtraBytes > 0:        # save any bytes that are not block aligned\n    self.bytesToEncrypt = self.bytesToEncrypt[-numExtraBytes:]\nelse:\n    self.bytesToEncrypt = ''\n\nif more == None:   # no more data expected from caller\n    finalBytes = self.padding.addPad(self.bytesToEncrypt,self.blockSize)\n    if len(finalBytes) > 0:\n        ctBlock = self.encryptBlock(finalBytes)\n        self.encryptBlockCount += 1\n        cipherText += ctBlock\n    self.resetEncrypt()\nreturn cipherText", "path": "DeDRM_tools/DeDRM_plugin/kindlekey.py", "commit_date": "2020-11-28 00:00:00", "repo_name": "apprenticeharper/DeDRM_tools", "stars": 14119, "license": "None", "language": "python", "size": 24858}
{"docstring": "\"\"\" decrypt a block (array of bytes) \"\"\"\n", "func_signal": "def decryptBlock(self, encryptedBlock):\n", "code": "self.state = self._toBlock(encryptedBlock)\nAddRoundKey(self, self.__expandedKey[self.Nr*self.Nb:(self.Nr+1)*self.Nb])\nfor round in range(self.Nr-1,0,-1):\n    InvShiftRows(self)\n    InvSubBytes(self)\n    AddRoundKey(self, self.__expandedKey[round*self.Nb:(round+1)*self.Nb])\n    InvMixColumns(self)\nInvShiftRows(self)\nInvSubBytes(self)\nAddRoundKey(self, self.__expandedKey[0:self.Nb])\nreturn self._toBString(self.state)", "path": "DeDRM_tools/DeDRM_plugin/kindlekey.py", "commit_date": "2020-11-28 00:00:00", "repo_name": "apprenticeharper/DeDRM_tools", "stars": 14119, "license": "None", "language": "python", "size": 24858}
{"docstring": "# handle the obvious cases at the beginning\n", "func_signal": "def GetDecryptedBook(infile, kDatabases, androidFiles, serials, pids, starttime = time.time()):\n", "code": "if not os.path.isfile(infile):\n    raise DrmException(\"Input file does not exist.\")\n\nmobi = True\nmagic8 = open(infile,'rb').read(8)\nif magic8 == b'\\xeaDRMION\\xee':\n    raise DrmException(\"The .kfx DRMION file cannot be decrypted by itself. A .kfx-zip archive containing a DRM voucher is required.\")\n\nmagic3 = magic8[:3]\nif magic3 == b'TPZ':\n    mobi = False\n\nif magic8[:4] == b'PK\\x03\\x04':\n    mb = kfxdedrm.KFXZipBook(infile)\nelif mobi:\n    mb = mobidedrm.MobiBook(infile)\nelse:\n    mb = topazextract.TopazBook(infile)\n\nbookname = unescape(mb.getBookTitle())\nprint(\"Decrypting {1} ebook: {0}\".format(bookname, mb.getBookType()))\n\n# copy list of pids\ntotalpids = list(pids)\n# extend list of serials with serials from android databases\nfor aFile in androidFiles:\n    serials.extend(androidkindlekey.get_serials(aFile))\n# extend PID list with book-specific PIDs from seriala and kDatabases\nmd1, md2 = mb.getPIDMetaInfo()\ntotalpids.extend(kgenpids.getPidList(md1, md2, serials, kDatabases))\n# remove any duplicates\ntotalpids = list(set(totalpids))\nprint(\"Found {1:d} keys to try after {0:.1f} seconds\".format(time.time()-starttime, len(totalpids)))\n#print totalpids\n\ntry:\n    mb.processBook(totalpids)\nexcept:\n    mb.cleanup\n    raise\n\nprint(\"Decryption succeeded after {0:.1f} seconds\".format(time.time()-starttime))\nreturn mb", "path": "DeDRM_tools/DeDRM_plugin/k4mobidedrm.py", "commit_date": "2020-10-16 00:00:00", "repo_name": "apprenticeharper/DeDRM_tools", "stars": 14119, "license": "None", "language": "python", "size": 24858}
{"docstring": "'''\nResolve an object. If this is an array or dictionary,\nit may still contains some indirect objects inside.\n'''\n", "func_signal": "def resolve1(x):\n", "code": "while isinstance(x, PDFObjRef):\n    x = x.resolve()\nreturn x", "path": "DeDRM_tools/DeDRM_plugin/ignoblepdf.py", "commit_date": "2020-10-14 00:00:00", "repo_name": "apprenticeharper/DeDRM_tools", "stars": 14119, "license": "None", "language": "python", "size": 24858}
{"docstring": "'''\nFetches a next line that ends either with \\\\r or \\\\n.\n'''\n", "func_signal": "def nextline(self):\n", "code": "linebuf = ''\nlinepos = self.bufpos + self.charpos\neol = False\nwhile 1:\n    self.fillbuf()\n    if eol:\n        c = self.buf[self.charpos]\n        # handle '\\r\\n'\n        if c == '\\n':\n            linebuf += c\n            self.charpos += 1\n        break\n    m = EOL.search(self.buf, self.charpos)\n    if m:\n        linebuf += self.buf[self.charpos:m.end(0)]\n        self.charpos = m.end(0)\n        if linebuf[-1] == '\\r':\n            eol = True\n        else:\n            break\n    else:\n        linebuf += self.buf[self.charpos:]\n        self.charpos = len(self.buf)\nreturn (linepos, linebuf)", "path": "DeDRM_tools/DeDRM_plugin/ignoblepdf.py", "commit_date": "2020-10-14 00:00:00", "repo_name": "apprenticeharper/DeDRM_tools", "stars": 14119, "license": "None", "language": "python", "size": 24858}
{"docstring": "\"\"\" Expand a byte array of size keySize into a larger array \"\"\"\n", "func_signal": "def keyExpansion(algInstance, keyArray):\n", "code": "Nk, Nb, Nr = algInstance.Nk, algInstance.Nb, algInstance.Nr # for readability\nw = [[keyArray[4*i],keyArray[4*i+1],keyArray[4*i+2],keyArray[4*i+3]] for i in range(Nk)]\nfor i in range(Nk,Nb*(Nr+1)):\n    temp = w[i-1]        # a four byte column\n    if (i%Nk) == 0 :\n        temp     = temp[1:]+[temp[0]]  # RotWord(temp)\n        temp     = [ Sbox[byte] for byte in temp ]\n        temp[0] ^= Rcon[i//Nk]\n    elif Nk > 6 and  i%Nk == 4 :\n        temp     = [ Sbox[byte] for byte in temp ]  # SubWord(temp)\n    w.append( [ w[i-Nk][byte]^temp[byte] for byte in range(4) ] )\nreturn w", "path": "DeDRM_tools/DeDRM_plugin/kindlekey.py", "commit_date": "2020-11-28 00:00:00", "repo_name": "apprenticeharper/DeDRM_tools", "stars": 14119, "license": "None", "language": "python", "size": 24858}
{"docstring": "# Return all possible ID Strings\n", "func_signal": "def GetIDStrings():\n", "code": "strings = []\nstrings.extend(GetMACAddressesMunged())\nstrings.extend(GetVolumesSerialNumbers())\nstrings.extend(GetDiskPartitionNames())\nstrings.extend(GetDiskPartitionUUIDs())\nstrings.append(b'9999999999')\n#print \"ID Strings:\\n\",strings\nreturn strings", "path": "DeDRM_tools/DeDRM_plugin/kindlekey.py", "commit_date": "2020-11-28 00:00:00", "repo_name": "apprenticeharper/DeDRM_tools", "stars": 14119, "license": "None", "language": "python", "size": 24858}
{"docstring": "\"\"\" XOR two byte arrays, to lesser length \"\"\"\n", "func_signal": "def xor(a,b):\n", "code": "x = []\nfor i in range(min(len(a),len(b))):\n    x.append( a[i] ^ b[i])\nreturn bytes(x)", "path": "DeDRM_tools/DeDRM_plugin/kindlekey.py", "commit_date": "2020-11-28 00:00:00", "repo_name": "apprenticeharper/DeDRM_tools", "stars": 14119, "license": "None", "language": "python", "size": 24858}
{"docstring": "\"\"\" Convert binary string to array of bytes, state[col][row]\"\"\"\n", "func_signal": "def _toBlock(self, bs):\n", "code": "assert ( len(bs) == 4*self.Nb ), 'Rijndarl blocks must be of size blockSize'\nreturn [[bs[4*i],bs[4*i+1],bs[4*i+2],bs[4*i+3]] for i in range(self.Nb)]", "path": "DeDRM_tools/DeDRM_plugin/kindlekey.py", "commit_date": "2020-11-28 00:00:00", "repo_name": "apprenticeharper/DeDRM_tools", "stars": 14119, "license": "None", "language": "python", "size": 24858}
{"docstring": "# Compute expected shape from num_classes.\n", "func_signal": "def get_expected_shape(self):\n", "code": "if self.num_classes == 2 and not self.multi_label:\n    expected = [1]\nelse:\n    expected = [self.num_classes]\nreturn expected", "path": "autokeras/autokeras/analysers/output_analysers.py", "commit_date": "2020-10-10 00:00:00", "repo_name": "keras-team/autokeras", "stars": 9048, "license": "apache-2.0", "language": "python", "size": 45472}
{"docstring": "\"\"\"Update the statistics with a batch of data.\n\n# Arguments\n    data: tf.Tensor. One batch of data from tf.data.Dataset.\n\"\"\"\n", "func_signal": "def update(self, data):\n", "code": "if self.dtype is None:\n    self.dtype = data.dtype\nif self.shape is None:\n    self.shape = data.shape.as_list()\nif self.batch_size is None:\n    self.batch_size = data.shape.as_list()[0]\nself.num_samples += data.shape.as_list()[0]", "path": "autokeras/autokeras/engine/analyser.py", "commit_date": "2020-09-24 00:00:00", "repo_name": "keras-team/autokeras", "stars": 9048, "license": "apache-2.0", "language": "python", "size": 45472}
{"docstring": "\"\"\"Share the information between blocks.\"\"\"\n", "func_signal": "def compile(self):\n", "code": "for block in self.blocks:\n    for func in COMPILE_FUNCTIONS.get(block.__class__, []):\n        func(block)", "path": "autokeras/autokeras/graph.py", "commit_date": "2020-11-11 00:00:00", "repo_name": "keras-team/autokeras", "stars": 9048, "license": "apache-2.0", "language": "python", "size": 45472}
{"docstring": "\"\"\"Record any information needed by transform.\"\"\"\n", "func_signal": "def check(self, x):\n", "code": "if not isinstance(x, (pd.DataFrame, np.ndarray, tf.data.Dataset)):\n    raise TypeError(\n        \"Expect the data in TimeseriesInput to be numpy.ndarray\"\n        \" or tf.data.Dataset or pd.DataFrame, but got {type}.\".format(\n            type=type(x)\n        )\n    )", "path": "autokeras/autokeras/adapters/input_adapters.py", "commit_date": "2020-09-14 00:00:00", "repo_name": "keras-team/autokeras", "stars": 9048, "license": "apache-2.0", "language": "python", "size": 45472}
{"docstring": "\"\"\"Add the HyperParameter (self) to the HyperParameters.\n\n# Arguments\n    hp: kerastuner.HyperParameters.\n    name: String. If left unspecified, the hp name is used.\n\"\"\"\n", "func_signal": "def add_to_hp(hp, hps, name=None):\n", "code": "kwargs = hp.get_config()\nif name is None:\n    name = hp.name\nkwargs.pop(\"conditions\")\nkwargs.pop(\"name\")\nclass_name = hp.__class__.__name__\nfunc = getattr(hps, class_name)\nreturn func(name=name, **kwargs)", "path": "autokeras/autokeras/utils/utils.py", "commit_date": "2020-11-21 00:00:00", "repo_name": "keras-team/autokeras", "stars": 9048, "license": "apache-2.0", "language": "python", "size": 45472}
{"docstring": "\"\"\"Potentially restict samples & labels to a training or validation split.\n\n# Arguments\n    samples: List of elements.\n    labels: List of corresponding labels.\n    validation_split: Float, fraction of data to reserve for validation.\n    subset: Subset of the data to return.\n        Either \"training\", \"validation\", or None.\n        If None, we return all of the data.\n\n# Returns\n    tuple (samples, labels), potentially restricted to the specified subset.\n\"\"\"\n", "func_signal": "def get_training_or_validation_split(samples, labels, validation_split, subset):\n", "code": "if not validation_split:\n    return samples, labels\n\nnum_val_samples = int(validation_split * len(samples))\nif subset == \"training\":\n    print(\"Using %d files for training.\" % (len(samples) - num_val_samples,))\n    samples = samples[:-num_val_samples]\n    labels = labels[:-num_val_samples]\nelif subset == \"validation\":\n    print(\"Using %d files for validation.\" % (num_val_samples,))\n    samples = samples[-num_val_samples:]\n    labels = labels[-num_val_samples:]\nelse:\n    raise ValueError(\n        '`subset` must be either \"training\" '\n        'or \"validation\", received: %s' % (subset,)\n    )\nreturn samples, labels", "path": "autokeras/autokeras/utils/io_utils.py", "commit_date": "2020-11-01 00:00:00", "repo_name": "keras-team/autokeras", "stars": 9048, "license": "apache-2.0", "language": "python", "size": 45472}
{"docstring": "\"\"\"Record any information needed by transform.\"\"\"\n", "func_signal": "def check(self, x):\n", "code": "if not isinstance(x, (np.ndarray, tf.data.Dataset)):\n    raise TypeError(\n        \"Expect the data to ImageInput to be numpy.ndarray or \"\n        \"tf.data.Dataset, but got {type}.\".format(type=type(x))\n    )\nif isinstance(x, np.ndarray) and not np.issubdtype(x.dtype, np.number):\n    raise TypeError(\n        \"Expect the data to ImageInput to be numerical, but got \"\n        \"{type}.\".format(type=x.dtype)\n    )", "path": "autokeras/autokeras/adapters/input_adapters.py", "commit_date": "2020-09-14 00:00:00", "repo_name": "keras-team/autokeras", "stars": 9048, "license": "apache-2.0", "language": "python", "size": 45472}
{"docstring": "\"\"\"Record any information needed by transform.\"\"\"\n", "func_signal": "def check(self, x):\n", "code": "if not isinstance(x, (np.ndarray, tf.data.Dataset)):\n    raise TypeError(\n        \"Expect the data to TextInput to be numpy.ndarray or \"\n        \"tf.data.Dataset, but got {type}.\".format(type=type(x))\n    )", "path": "autokeras/autokeras/adapters/input_adapters.py", "commit_date": "2020-09-14 00:00:00", "repo_name": "keras-team/autokeras", "stars": 9048, "license": "apache-2.0", "language": "python", "size": 45472}
{"docstring": "# Initialize the classifier.\n", "func_signal": "def main():\n", "code": "train_file_path = tf.keras.utils.get_file(\"train.csv\", TRAIN_DATA_URL)\ntest_file_path = tf.keras.utils.get_file(\"eval.csv\", TEST_DATA_URL)\nclf = ak.StructuredDataClassifier(max_trials=10, directory='tmp_dir', overwrite=True)\n\nstart_time = timeit.default_timer()\n# x is the path to the csv file. y is the column name of the column to predict.\nclf.fit(train_file_path, 'survived')\nstop_time = timeit.default_timer()\n\n# Evaluate the accuracy of the found model.\naccuracy = clf.evaluate(test_file_path, 'survived')[1]\nprint('Accuracy: {accuracy}%'.format(accuracy=round(accuracy * 100, 2)))\nprint('Total time: {time} seconds.'.format(time=round(stop_time - start_time, 2)))", "path": "autokeras/examples/titanic.py", "commit_date": "2020-11-08 00:00:00", "repo_name": "keras-team/autokeras", "stars": 9048, "license": "apache-2.0", "language": "python", "size": 45472}
{"docstring": "\"\"\"\n# Arguments\n     hp: HyperParameters. The hyperparameters for building the model.\n     inputs: Tensor of Shape [batch_size, seq_len, embedding_dim]\n\n# Returns\n    Self-Attention outputs of shape `[batch_size, seq_len, embedding_dim]`.\n\"\"\"\n", "func_signal": "def build(self, hp, inputs=None):\n", "code": "inputs = nest.flatten(inputs)\nutils.validate_num_inputs(inputs, 1)\ninput_node = inputs[0]\nnum_heads = self.num_heads\nhead_size = (\n    self.head_size\n    or hp.Choice(\"head_size_factor\", [4, 8, 16, 32, 64], default=16)\n    * num_heads\n)\n\nprojection_dim = head_size // num_heads\nquery_dense = layers.Dense(head_size)\nkey_dense = layers.Dense(head_size)\nvalue_dense = layers.Dense(head_size)\ncombine_heads = layers.Dense(head_size)\nbatch_size = tf.shape(input_node)[0]\nquery = query_dense(input_node)  # (batch_size, seq_len, head_size)\nkey = key_dense(input_node)  # (batch_size, seq_len, head_size)\nvalue = value_dense(input_node)  # (batch_size, seq_len, head_size)\nquery, key, value = [\n    self.separate_heads(var, batch_size, num_heads, projection_dim)\n    for var in [query, key, value]\n]\nattention, weights = self.attention(query, key, value)\nattention = tf.transpose(\n    attention, perm=[0, 2, 1, 3]\n)  # (batch_size, seq_len, num_heads, projection_dim)\nconcat_attention = tf.reshape(\n    attention, (batch_size, tf.shape(attention)[1], self.head_size)\n)  # (batch_size, seq_len, head_size)\noutput = combine_heads(concat_attention)  # (batch_size, seq_len, head_size)\nreturn output", "path": "autokeras/autokeras/blocks/basic.py", "commit_date": "2020-12-11 00:00:00", "repo_name": "keras-team/autokeras", "stars": 9048, "license": "apache-2.0", "language": "python", "size": 45472}
{"docstring": "\"\"\"Build the HyperModel into a Keras Model.\"\"\"\n", "func_signal": "def build(self, hp):\n", "code": "self.compile()\nkeras_nodes = {}\nkeras_input_nodes = []\nfor node in self.inputs:\n    node_id = self._node_to_id[node]\n    input_node = node.build_node(hp)\n    output_node = node.build(hp, input_node)\n    keras_input_nodes.append(input_node)\n    keras_nodes[node_id] = output_node\nfor block in self.blocks:\n    temp_inputs = [\n        keras_nodes[self._node_to_id[input_node]]\n        for input_node in block.inputs\n    ]\n    outputs = block.build(hp, inputs=temp_inputs)\n    outputs = nest.flatten(outputs)\n    for output_node, real_output_node in zip(block.outputs, outputs):\n        keras_nodes[self._node_to_id[output_node]] = real_output_node\nmodel = tf.keras.Model(\n    keras_input_nodes,\n    [\n        keras_nodes[self._node_to_id[output_node]]\n        for output_node in self.outputs\n    ],\n)\n\nreturn self._compile_keras_model(hp, model)", "path": "autokeras/autokeras/graph.py", "commit_date": "2020-11-11 00:00:00", "repo_name": "keras-team/autokeras", "stars": 9048, "license": "apache-2.0", "language": "python", "size": 45472}
{"docstring": "\"\"\"Fetch the column_types and column_names.\n\nThe values are fetched for FeatureEncoding from StructuredDataInput.\n\"\"\"\n", "func_signal": "def feature_encoding_input(block):\n", "code": "if not isinstance(block.inputs[0], nodes_module.StructuredDataInput):\n    raise TypeError(\n        \"CategoricalToNumerical can only be used with StructuredDataInput.\"\n    )\nblock.column_types = block.inputs[0].column_types\nblock.column_names = block.inputs[0].column_names", "path": "autokeras/autokeras/graph.py", "commit_date": "2020-11-11 00:00:00", "repo_name": "keras-team/autokeras", "stars": 9048, "license": "apache-2.0", "language": "python", "size": 45472}
{"docstring": "# Prepare the dataset.\n", "func_signal": "def load_data():\n", "code": "TRAIN_DATA_URL = (\n    \"https://storage.googleapis.com/\"\n    \"download.tensorflow.org/data/iris_training.csv\"\n)\nx_train = tf.keras.utils.get_file(\"iris_train.csv\", TRAIN_DATA_URL)\n\nTEST_DATA_URL = (\n    \"https://storage.googleapis.com/\"\n    \"download.tensorflow.org/data/iris_test.csv\"\n)\nx_test = tf.keras.utils.get_file(\"iris_test.csv\", TEST_DATA_URL)\n\nreturn (x_train, \"virginica\"), (x_test, \"virginica\")", "path": "autokeras/benchmark/experiments/structured_data.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "keras-team/autokeras", "stars": 9048, "license": "apache-2.0", "language": "python", "size": 45472}
{"docstring": "\"\"\"\nConvert Matlab datenum into Python datetime.\n\"\"\"\n", "func_signal": "def datenum_to_datetime(datenum):\n", "code": "days = datenum % 1\nhours = days % 1 * 24\nminutes = hours % 1 * 60\nseconds = minutes % 1 * 60\ntry:\n    return (\n        datetime.fromordinal(int(datenum))\n        + timedelta(days=int(days))\n        + timedelta(hours=int(hours))\n        + timedelta(minutes=int(minutes))\n        + timedelta(seconds=round(seconds))\n        - timedelta(days=366)\n    )\nexcept:\n    return datenum_to_datetime(700000)", "path": "autokeras/examples/celeb_age.py", "commit_date": "2020-06-15 00:00:00", "repo_name": "keras-team/autokeras", "stars": 9048, "license": "apache-2.0", "language": "python", "size": 45472}
{"docstring": "\"\"\"Evaluate the best model for the given data.\n\n# Arguments\n    x: String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset.\n        Testing data x. If the data is from a csv file, it should be a\n        string specifying the path of the csv file of the testing data.\n    y: String, numpy.ndarray, or tensorflow.Dataset. Testing data y.\n        If the data is from a csv file, it should be a string corresponding\n        to the label column.\n    **kwargs: Any arguments supported by keras.Model.evaluate.\n\n# Returns\n    Scalar test loss (if the model has a single output and no metrics) or\n    list of scalars (if the model has multiple outputs and/or metrics).\n    The attribute model.metrics_names will give you the display labels for\n    the scalar outputs.\n\"\"\"\n", "func_signal": "def evaluate(self, x, y=None, **kwargs):\n", "code": "if isinstance(x, str):\n    x, y = self._read_from_csv(x, y)\nreturn super().evaluate(x=x[: len(y)], y=y[self.lookback - 1 :], **kwargs)", "path": "autokeras/autokeras/tasks/time_series_forecaster.py", "commit_date": "2020-12-11 00:00:00", "repo_name": "keras-team/autokeras", "stars": 9048, "license": "apache-2.0", "language": "python", "size": 45472}
{"docstring": "\"\"\"\n# Arguments\n     hp: HyperParameters. The hyperparameters for building the model.\n     inputs: Tensor of Shape [batch_size, seq_len]\n\n# Returns\n    Output Tensor of shape `[batch_size, seq_len, embedding_dim]`.\n\"\"\"\n", "func_signal": "def build(self, hp, inputs=None):\n", "code": "inputs = nest.flatten(inputs)\nutils.validate_num_inputs(inputs, 1)\npretraining = self.pretraining or hp.Choice(\n    \"pretraining\",\n    [\"random\", \"glove\", \"fasttext\", \"word2vec\", \"none\"],\n    default=\"none\",\n)\nembedding_dim = self.embedding_dim or hp.Choice(\n    \"embedding_dim\", [32, 64, 128, 256, 512], default=128\n)\nnum_heads = self.num_heads or hp.Choice(\"num_heads\", [8, 16, 32], default=8)\n\ndense_dim = self.dense_dim or hp.Choice(\n    \"dense_dim\", [128, 256, 512, 1024, 2048], default=2048\n)\ndropout = self.dropout or hp.Choice(\"dropout\", [0.0, 0.25, 0.5], default=0)\n\nffn = tf.keras.Sequential(\n    [\n        layers.Dense(dense_dim, activation=\"relu\"),\n        layers.Dense(embedding_dim),\n    ]\n)\n\nlayernorm1 = layers.LayerNormalization(epsilon=1e-6)\nlayernorm2 = layers.LayerNormalization(epsilon=1e-6)\ndropout1 = layers.Dropout(dropout)\ndropout2 = layers.Dropout(dropout)\n# Token and Position Embeddings\ninput_node = nest.flatten(inputs)[0]\ntoken_embedding = Embedding(\n    max_features=self.max_features,\n    pretraining=pretraining,\n    embedding_dim=embedding_dim,\n    dropout=dropout,\n).build(hp, input_node)\nmaxlen = input_node.shape[-1]\nbatch_size = tf.shape(input_node)[0]\npositions = self.pos_array_funct(maxlen, batch_size)\nposition_embedding = Embedding(\n    max_features=maxlen,\n    pretraining=pretraining,\n    embedding_dim=embedding_dim,\n    dropout=dropout,\n).build(hp, positions)\noutput_node = tf.keras.layers.Add()([token_embedding, position_embedding])\nattn_output = MultiHeadSelfAttention(embedding_dim, num_heads).build(\n    hp, output_node\n)\nattn_output = dropout1(attn_output)\nadd_inputs_1 = tf.keras.layers.Add()([output_node, attn_output])\nout1 = layernorm1(add_inputs_1)\nffn_output = ffn(out1)\nffn_output = dropout2(ffn_output)\nadd_inputs_2 = tf.keras.layers.Add()([out1, ffn_output])\noutput = layernorm2(add_inputs_2)\nreturn output", "path": "autokeras/autokeras/blocks/basic.py", "commit_date": "2020-12-11 00:00:00", "repo_name": "keras-team/autokeras", "stars": 9048, "license": "apache-2.0", "language": "python", "size": 45472}
{"docstring": "\"\"\"Record any information needed by transform.\"\"\"\n", "func_signal": "def check(self, x):\n", "code": "if not isinstance(x, (np.ndarray, tf.data.Dataset)):\n    raise TypeError(\n        \"Expect the data to Input to be numpy.ndarray or \"\n        \"tf.data.Dataset, but got {type}.\".format(type=type(x))\n    )\nif isinstance(x, np.ndarray) and not np.issubdtype(x.dtype, np.number):\n    raise TypeError(\n        \"Expect the data to Input to be numerical, but got \"\n        \"{type}.\".format(type=x.dtype)\n    )", "path": "autokeras/autokeras/adapters/input_adapters.py", "commit_date": "2020-09-14 00:00:00", "repo_name": "keras-team/autokeras", "stars": 9048, "license": "apache-2.0", "language": "python", "size": 45472}
{"docstring": "\"\"\"Get a generator returning n_batches random data of batch_size with n_features.\"\"\"\n\n", "func_signal": "def get_data_generator(n_batches, batch_size, n_features):\n", "code": "def data_generator():\n    for _ in range(n_batches * batch_size):\n        x = np.random.randn(n_features)\n        y = x.sum(axis=0) / n_features > 0.5\n        yield x, y\n\nreturn data_generator", "path": "autokeras/docs/py/load.py", "commit_date": "2020-11-30 00:00:00", "repo_name": "keras-team/autokeras", "stars": 9048, "license": "apache-2.0", "language": "python", "size": 45472}
{"docstring": "# TODO: support raw string labels for multi-label.\n", "func_signal": "def finalize(self):\n", "code": "self.labels = sorted(list(self.labels))\n\n# Infer the num_classes if not specified.\nif not self.num_classes:\n    if self.encoded:\n        # Single column with 0s and 1s.\n        if len(self.shape) == 1 or self.shape[1:] == [1]:\n            self.num_classes = 2\n        else:\n            self.num_classes = self.shape[1]\n    else:\n        self.num_classes = len(self.labels)\n\nif self.num_classes < 2:\n    raise ValueError(\n        \"Expect the target data for {name} to have \"\n        \"at least 2 classes, but got {num_classes}.\".format(\n            name=self.name, num_classes=self.num_classes\n        )\n    )\n\n# Check shape equals expected shape.\nexpected = self.get_expected_shape()\nactual = self.shape[1:]\nif len(actual) == 0:\n    actual = [1]\nif self.encoded and actual != expected:\n    raise ValueError(\n        \"Expect the target data for {name} to have \"\n        \"shape {expected}, but got {actual}.\".format(\n            name=self.name, expected=expected, actual=self.shape[1:]\n        )\n    )", "path": "autokeras/autokeras/analysers/output_analysers.py", "commit_date": "2020-10-10 00:00:00", "repo_name": "keras-team/autokeras", "stars": 9048, "license": "apache-2.0", "language": "python", "size": 45472}
{"docstring": "# Specify hyperparameters from compile(...)\n", "func_signal": "def _compile_keras_model(self, hp, model):\n", "code": "optimizer_name = hp.Choice(\n    \"optimizer\",\n    [\"adam\", \"sgd\", \"adam_weight_decay\"],\n    default=\"adam\",\n)\n# TODO: add adadelta optimizer when it can optimize embedding layer on GPU.\nlearning_rate = hp.Choice(\n    \"learning_rate\", [1e-1, 1e-2, 1e-3, 1e-4, 2e-5, 1e-5], default=1e-3\n)\n\nif optimizer_name == \"adam\":\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\nelif optimizer_name == \"sgd\":\n    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\nelif optimizer_name == \"adam_weight_decay\":\n    steps_per_epoch = int(self.num_samples / self.batch_size)\n    num_train_steps = steps_per_epoch * self.epochs\n    warmup_steps = int(\n        self.epochs * self.num_samples * 0.1 / self.batch_size\n    )\n\n    lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n        initial_learning_rate=learning_rate,\n        decay_steps=num_train_steps,\n        end_learning_rate=0.0,\n    )\n    if warmup_steps:\n        lr_schedule = keras_layers.WarmUp(\n            initial_learning_rate=learning_rate,\n            decay_schedule_fn=lr_schedule,\n            warmup_steps=warmup_steps,\n        )\n\n    optimizer = keras_layers.AdamWeightDecay(\n        learning_rate=lr_schedule,\n        weight_decay_rate=0.01,\n        beta_1=0.9,\n        beta_2=0.999,\n        epsilon=1e-6,\n        exclude_from_weight_decay=[\"LayerNorm\", \"layer_norm\", \"bias\"],\n    )\n\nmodel.compile(\n    optimizer=optimizer, metrics=self._get_metrics(), loss=self._get_loss()\n)\n\nreturn model", "path": "autokeras/autokeras/graph.py", "commit_date": "2020-11-11 00:00:00", "repo_name": "keras-team/autokeras", "stars": 9048, "license": "apache-2.0", "language": "python", "size": 45472}
{"docstring": "\"\"\"Remove lowercase flags if the respective uppercase flag exists\n\n>>> squash_flags('abc')\n'abc'\n>>> squash_flags('abcC')\n'ab'\n>>> squash_flags('CabcAd')\n'bd'\n\"\"\"\n", "func_signal": "def squash_flags(flags):\n", "code": "exclude = ''.join(f.upper() + f.lower() for f in flags if f == f.upper())\nreturn ''.join(f for f in flags if f not in exclude)", "path": "ranger/ranger/ext/rifle.py", "commit_date": "2020-09-08 00:00:00", "repo_name": "ranger/ranger", "stars": 14682, "license": "gpl-3.0", "language": "python", "size": 10410}
{"docstring": "\"\"\"Remote url\"\"\"\n", "func_signal": "def _remote_url(self):\n", "code": "try:\n    return self._run(['showconfig', 'paths.default']) or None\nexcept VcsError:\n    return None", "path": "ranger/ranger/ext/vcs/hg.py", "commit_date": "2017-01-21 00:00:00", "repo_name": "ranger/ranger", "stars": 14682, "license": "gpl-3.0", "language": "python", "size": 10410}
{"docstring": "\"\"\"Return all executable files in $PATH + Cache them.\"\"\"\n", "func_signal": "def get_executables():\n", "code": "global _CACHED_EXECUTABLES  # pylint: disable=global-statement\nif _CACHED_EXECUTABLES is not None:\n    return _CACHED_EXECUTABLES\n\nif 'PATH' in os.environ:\n    paths = os.environ['PATH'].split(':')\nelse:\n    paths = ['/usr/bin', '/bin']\n\nfrom stat import S_IXOTH, S_IFREG\npaths_seen = set()\n_CACHED_EXECUTABLES = set()\nfor path in paths:\n    if path in paths_seen:\n        continue\n    paths_seen.add(path)\n    try:\n        content = os.listdir(path)\n    except OSError:\n        continue\n    for item in content:\n        abspath = path + '/' + item\n        try:\n            filestat = os.stat(abspath)\n        except OSError:\n            continue\n        if filestat.st_mode & (S_IXOTH | S_IFREG):\n            _CACHED_EXECUTABLES.add(item)\nreturn _CACHED_EXECUTABLES", "path": "ranger/ranger/ext/rifle.py", "commit_date": "2020-09-08 00:00:00", "repo_name": "ranger/ranger", "stars": 14682, "license": "gpl-3.0", "language": "python", "size": 10410}
{"docstring": "# Spawn \"file\" to determine the mime-type of the given file.\n", "func_signal": "def get_mimetype(self, fname):\n", "code": "if self._mimetype:\n    return self._mimetype\n\nimport mimetypes\nif not mimetypes.inited:\n    mimetypes.init(mimetypes.knownfiles + self._mimetype_known_files)\nself._mimetype, _ = mimetypes.guess_type(fname)\n\nif not self._mimetype:\n    process = Popen([\"file\", \"--mime-type\", \"-Lb\", fname], stdout=PIPE, stderr=PIPE)\n    mimetype, _ = process.communicate()\n    self._mimetype = mimetype.decode(ENCODING).strip()\n    if self._mimetype == 'application/octet-stream':\n        try:\n            process = Popen([\"mimetype\", \"--output-format\", \"%m\", fname],\n                            stdout=PIPE, stderr=PIPE)\n            mimetype, _ = process.communicate()\n            self._mimetype = mimetype.decode(ENCODING).strip()\n        except OSError:\n            pass\nreturn self._mimetype", "path": "ranger/ranger/ext/rifle.py", "commit_date": "2020-09-08 00:00:00", "repo_name": "ranger/ranger", "stars": 14682, "license": "gpl-3.0", "language": "python", "size": 10410}
{"docstring": "\"\"\"Copy data from src to dst\"\"\"\n", "func_signal": "def copyfile(src, dst):\n", "code": "if _samefile(src, dst):\n    raise Error(\"`%s` and `%s` are the same file\" % (src, dst))\n\nfor fn in [src, dst]:  # pylint: disable=invalid-name\n    try:\n        st = os.stat(fn)  # pylint: disable=invalid-name\n    except OSError:\n        # File most likely does not exist\n        pass\n    else:\n        # XXX What about other special files? (sockets, devices...)\n        if stat.S_ISFIFO(st.st_mode):\n            raise SpecialFileError(\"`%s` is a named pipe\" % fn)\n\nwith open(src, 'rb') as fsrc:\n    with open(dst, 'wb') as fdst:\n        for done in copyfileobj(fsrc, fdst):\n            yield done", "path": "ranger/ranger/ext/shutil_generatorized.py", "commit_date": "2020-07-05 00:00:00", "repo_name": "ranger/ranger", "stars": 14682, "license": "gpl-3.0", "language": "python", "size": 10410}
{"docstring": "\"\"\"List all commands that are applicable for the given files\n\nReturns one 4-tuple for all currently applicable commands\nThe 4-tuple contains (count, command, label, flags).\ncount is the index, counted from 0 upwards,\ncommand is the command that will be executed.\nlabel and flags are the label and flags specified in the rule.\n\"\"\"\n", "func_signal": "def list_commands(self, files, mimetype=None, skip_ask=False):\n", "code": "self._mimetype = mimetype\ncount = -1\nfor cmd, tests in self.rules:\n    self._skip = None\n    self._app_flags = ''\n    self._app_label = None\n    if skip_ask and cmd == ASK_COMMAND:\n        # TODO(vifon): Fix properly, see\n        # https://github.com/ranger/ranger/pull/1341#issuecomment-537264495\n        count += 1\n        continue\n    for test in tests:\n        if not self._eval_condition(test, files, None):\n            break\n    else:\n        if self._skip is None:\n            count += 1\n        else:\n            count = self._skip\n        yield (count, cmd, self._app_label, self._app_flags)", "path": "ranger/ranger/ext/rifle.py", "commit_date": "2020-09-08 00:00:00", "repo_name": "ranger/ranger", "stars": 14682, "license": "gpl-3.0", "language": "python", "size": 10410}
{"docstring": "# Handle the negation of conditions starting with an exclamation mark,\n# then pass on the arguments to _eval_condition2().\n\n", "func_signal": "def _eval_condition(self, condition, files, label):\n", "code": "if not condition:\n    return True\nif condition[0].startswith('!'):\n    new_condition = tuple([condition[0][1:]]) + tuple(condition[1:])\n    return not self._eval_condition2(new_condition, files, label)\nreturn self._eval_condition2(condition, files, label)", "path": "ranger/ranger/ext/rifle.py", "commit_date": "2020-09-08 00:00:00", "repo_name": "ranger/ranger", "stars": 14682, "license": "gpl-3.0", "language": "python", "size": 10410}
{"docstring": "\"\"\"Move relative in history\"\"\"\n", "func_signal": "def history_go(self, relative):\n", "code": "if self.history:\n    self.history.move(relative).go(history=False)", "path": "ranger/ranger/core/tab.py", "commit_date": "2020-12-05 00:00:00", "repo_name": "ranger/ranger", "stars": 14682, "license": "gpl-3.0", "language": "python", "size": 10410}
{"docstring": "\"\"\"assign attributes such as self.video according to the mimetype\"\"\"\n", "func_signal": "def set_mimetype(self):\n", "code": "bname = self.basename\nif self.extension == 'part':\n    bname = bname[0:-5]\n# pylint: disable=attribute-defined-outside-init\nself._mimetype = self.fm.mimetypes.guess_type(bname, False)[0]\nif self._mimetype is None:\n    self._mimetype = ''\n# pylint: enable=attribute-defined-outside-init\n\nself.video = self._mimetype.startswith('video')\nself.image = self._mimetype.startswith('image')\nself.audio = self._mimetype.startswith('audio')\nself.media = self.video or self.image or self.audio\nself.document = self._mimetype.startswith('text') \\\n    or self.extension in DOCUMENT_EXTENSIONS \\\n    or self.basename.lower() in DOCUMENT_BASENAMES\nself.container = self.extension in CONTAINER_EXTENSIONS\n\n# pylint: disable=attribute-defined-outside-init\nkeys = ('video', 'audio', 'image', 'media', 'document', 'container')\nself._mimetype_tuple = tuple(key for key in keys if getattr(self, key))\n\nif self._mimetype == '':\n    self._mimetype = None\n# pylint: enable=attribute-defined-outside-init", "path": "ranger/ranger/container/fsobject.py", "commit_date": "2020-07-05 00:00:00", "repo_name": "ranger/ranger", "stars": 14682, "license": "gpl-3.0", "language": "python", "size": 10410}
{"docstring": "\"\"\"Copy all stat info (mode bits, atime, mtime, flags) from src to dst\"\"\"\n", "func_signal": "def copystat(src, dst):\n", "code": "st = os.stat(src)  # pylint: disable=invalid-name\nmode = stat.S_IMODE(st.st_mode)\nif hasattr(os, 'utime'):\n    try:\n        os.utime(dst, (st.st_atime, st.st_mtime))\n    except OSError:\n        pass\nif hasattr(os, 'chmod'):\n    try:\n        os.chmod(dst, mode)\n    except OSError:\n        pass\nif hasattr(os, 'chflags') and hasattr(st, 'st_flags'):\n    try:\n        os.chflags(dst, st.st_flags)  # pylint: disable=no-member\n    except OSError:\n        pass", "path": "ranger/ranger/ext/shutil_generatorized.py", "commit_date": "2020-07-05 00:00:00", "repo_name": "ranger/ranger", "stars": 14682, "license": "gpl-3.0", "language": "python", "size": 10410}
{"docstring": "\"\"\"Copy all stat info (mode bits, atime, mtime, flags) from src to dst.\n\nIf the optional flag `follow_symlinks` is not set, symlinks aren't followed if and\nonly if both `src` and `dst` are symlinks.\n\n\"\"\"\n", "func_signal": "def copystat(src, dst, follow_symlinks=True):\n", "code": "def _nop(*args, **kwargs):  # pylint: disable=unused-argument\n    pass\n\n# follow symlinks (aka don't not follow symlinks)\nfollow = follow_symlinks or not (os.path.islink(src) and os.path.islink(dst))\nif follow:\n    # use the real function if it exists\n    def lookup(name):\n        return getattr(os, name, _nop)\nelse:\n    # use the real function only if it exists\n    # *and* it supports follow_symlinks\n    def lookup(name):\n        fn = getattr(os, name, _nop)  # pylint: disable=invalid-name\n        if fn in os.supports_follow_symlinks:  # pylint: disable=no-member\n            return fn\n        return _nop\n\nst = lookup(\"stat\")(src, follow_symlinks=follow)  # pylint: disable=invalid-name\nmode = stat.S_IMODE(st.st_mode)\ntry:\n    lookup(\"utime\")(dst, ns=(st.st_atime_ns, st.st_mtime_ns),\n                    follow_symlinks=follow)\nexcept OSError:\n    pass\ntry:\n    lookup(\"chmod\")(dst, mode, follow_symlinks=follow)\nexcept NotImplementedError:\n    # if we got a NotImplementedError, it's because\n    #   * follow_symlinks=False,\n    #   * lchown() is unavailable, and\n    #   * either\n    #       * fchownat() is unavailable or\n    #       * fchownat() doesn't implement AT_SYMLINK_NOFOLLOW.\n    #         (it returned ENOSUP.)\n    # therefore we're out of options--we simply cannot chown the\n    # symlink.  give up, suppress the error.\n    # (which is what shutil always did in this circumstance.)\n    pass\nexcept OSError:\n    pass\nif hasattr(st, 'st_flags'):\n    try:\n        lookup(\"chflags\")(dst, st.st_flags, follow_symlinks=follow)\n    except OSError:\n        pass\ntry:\n    _copyxattr(src, dst, follow_symlinks=follow)\nexcept OSError:\n    pass", "path": "ranger/ranger/ext/shutil_generatorized.py", "commit_date": "2020-07-05 00:00:00", "repo_name": "ranger/ranger", "stars": 14682, "license": "gpl-3.0", "language": "python", "size": 10410}
{"docstring": "# Get the flags\n", "func_signal": "def _build_command(self, files, action, flags):\n", "code": "if isinstance(flags, str):\n    self._app_flags += flags\nself._app_flags = squash_flags(self._app_flags)\nfilenames = \"' '\".join(f.replace(\"'\", \"'\\\\\\''\") for f in files if \"\\x00\" not in f)\nreturn \"set -- '%s'; %s\" % (filenames, action)", "path": "ranger/ranger/ext/rifle.py", "commit_date": "2020-09-08 00:00:00", "repo_name": "ranger/ranger", "stars": 14682, "license": "gpl-3.0", "language": "python", "size": 10410}
{"docstring": "\"\"\"Calls load() if the currently cached information is outdated\"\"\"\n", "func_signal": "def load_if_outdated(self):\n", "code": "if not self.loaded:\n    self.load()\n    return True\ntry:\n    real_ctime = stat(self.path).st_ctime\nexcept OSError:\n    real_ctime = None\nif not self.stat or self.stat.st_ctime != real_ctime:\n    self.load()\n    return True\nreturn False", "path": "ranger/ranger/container/fsobject.py", "commit_date": "2020-07-05 00:00:00", "repo_name": "ranger/ranger", "stars": 14682, "license": "gpl-3.0", "language": "python", "size": 10410}
{"docstring": "\"\"\"Translate status code\"\"\"\n", "func_signal": "def _status_translate(self, code):\n", "code": "for code_x, status in self._status_translations:\n    if code in code_x:\n        return status\nreturn 'unknown'", "path": "ranger/ranger/ext/vcs/hg.py", "commit_date": "2017-01-21 00:00:00", "repo_name": "ranger/ranger", "stars": 14682, "license": "gpl-3.0", "language": "python", "size": 10410}
{"docstring": "\"\"\"Copy data and all stat info (\"cp -p src dst\").\n\nThe destination may be a directory.\n\n\"\"\"\n", "func_signal": "def copy2(src, dst, overwrite=False, symlinks=False, make_safe_path=get_safe_path):\n", "code": "if os.path.isdir(dst):\n    dst = os.path.join(dst, os.path.basename(src))\nif not overwrite:\n    dst = make_safe_path(dst)\nif symlinks and os.path.islink(src):\n    linkto = os.readlink(src)\n    if overwrite and os.path.lexists(dst):\n        os.unlink(dst)\n    os.symlink(linkto, dst)\nelse:\n    for done in copyfile(src, dst):\n        yield done\n    copystat(src, dst)", "path": "ranger/ranger/ext/shutil_generatorized.py", "commit_date": "2020-07-05 00:00:00", "repo_name": "ranger/ranger", "stars": 14682, "license": "gpl-3.0", "language": "python", "size": 10410}
{"docstring": "\"\"\"Returns the FileSystemObject at the given level.\n\nlevel >0 => previews\nlevel 0 => current file/directory\nlevel <0 => parent directories\n\"\"\"\n", "func_signal": "def at_level(self, level):\n", "code": "if level <= 0:\n    try:\n        return self.pathway[level - 1]\n    except IndexError:\n        return None\nelse:\n    directory = self.thisdir\n    for _ in range(level):\n        if directory is None:\n            return None\n        if directory.is_directory:\n            directory = directory.pointed_obj\n        else:\n            return None\n    return directory", "path": "ranger/ranger/core/tab.py", "commit_date": "2020-12-05 00:00:00", "repo_name": "ranger/ranger", "stars": 14682, "license": "gpl-3.0", "language": "python", "size": 10410}
{"docstring": "# Find configuration file path\n", "func_signal": "def find_conf_path():\n", "code": "if 'XDG_CONFIG_HOME' in os.environ and os.environ['XDG_CONFIG_HOME']:\n    conf_path = os.environ['XDG_CONFIG_HOME'] + '/ranger/rifle.conf'\nelse:\n    conf_path = os.path.expanduser('~/.config/ranger/rifle.conf')\ndefault_conf_path = conf_path\nif not os.path.isfile(conf_path):\n    conf_path = os.path.normpath(os.path.join(os.path.dirname(__file__),\n                                              '../config/rifle.conf'))\nif not os.path.isfile(conf_path):\n    try:\n        # if ranger is installed, get the configuration from ranger\n        import ranger\n    except ImportError:\n        pass\n    else:\n        conf_path = os.path.join(ranger.__path__[0], \"config\", \"rifle.conf\")\n\nif not os.path.isfile(conf_path):\n    sys.stderr.write(\"Could not find a configuration file.\\n\"\n                     \"Please create one at %s.\\n\" % default_conf_path)\n    return None\n\nreturn conf_path", "path": "ranger/ranger/ext/rifle.py", "commit_date": "2020-09-08 00:00:00", "repo_name": "ranger/ranger", "stars": 14682, "license": "gpl-3.0", "language": "python", "size": 10410}
{"docstring": "# Check if stdin (file descriptor 0), stdout (fd 1) and\n# stderr (fd 2) are connected to a terminal\n", "func_signal": "def _is_terminal():\n", "code": "try:\n    os.ttyname(0)\n    os.ttyname(1)\n    os.ttyname(2)\nexcept OSError:\n    return False\nreturn True", "path": "ranger/ranger/ext/rifle.py", "commit_date": "2020-09-08 00:00:00", "repo_name": "ranger/ranger", "stars": 14682, "license": "gpl-3.0", "language": "python", "size": 10410}
{"docstring": "\"\"\"Recursively move a file or directory to another location. This is\nsimilar to the Unix \"mv\" command.\n\nIf the destination is a directory or a symlink to a directory, the source\nis moved inside the directory. The destination path must not already\nexist.\n\nIf the destination already exists but is not a directory, it may be\noverwritten depending on os.rename() semantics.\n\nIf the destination is on our current filesystem, then rename() is used.\nOtherwise, src is copied to the destination and then removed.\nA lot more could be done here...  A look at a mv.c shows a lot of\nthe issues this implementation glosses over.\n\n\"\"\"\n", "func_signal": "def move(src, dst, overwrite=False, make_safe_path=get_safe_path):\n", "code": "real_dst = dst\nif os.path.isdir(dst):\n    if _samefile(src, dst):\n        # We might be on a case insensitive filesystem,\n        # perform the rename anyway.\n        os.rename(src, dst)\n        return\n\n    real_dst = os.path.join(dst, _basename(src))\nif not overwrite:\n    real_dst = make_safe_path(real_dst)\ntry:\n    os.rename(src, real_dst)\nexcept OSError:\n    if os.path.isdir(src):\n        if _destinsrc(src, dst):\n            raise Error(\"Cannot move a directory '%s' into itself '%s'.\" % (src, dst))\n        for done in copytree(src, real_dst, symlinks=True, overwrite=overwrite,\n                             make_safe_path=make_safe_path):\n            yield done\n        rmtree(src)\n    else:\n        for done in copy2(src, real_dst, symlinks=True, overwrite=overwrite,\n                          make_safe_path=make_safe_path):\n            yield done\n        os.unlink(src)", "path": "ranger/ranger/ext/shutil_generatorized.py", "commit_date": "2020-07-05 00:00:00", "repo_name": "ranger/ranger", "stars": 14682, "license": "gpl-3.0", "language": "python", "size": 10410}
{"docstring": "\"\"\"Enter given path\"\"\"\n# TODO: Ensure that there is always a self.thisdir\n", "func_signal": "def enter_dir(self, path, history=True):\n", "code": "if path is None:\n    return None\npath = str(path)\n\n# clear filter in the folder we're leaving\nif self.fm.settings.clear_filters_on_dir_change and self.thisdir:\n    self.thisdir.filter = None\n    self.thisdir.refilter()\n\nprevious = self.thisdir\n\n# get the absolute path\npath = normpath(join(self.path, expanduser(path)))\nselectfile = None\n\nif not isdir(path):\n    selectfile = path\n    path = dirname(path)\nnew_thisdir = self.fm.get_directory(path)\n\ntry:\n    os.chdir(path)\nexcept OSError:\n    return True\nself.path = path\nself.thisdir = new_thisdir\n\nself.thisdir.load_content_if_outdated()\n\n# build the pathway, a tuple of directory objects which lie\n# on the path to the current directory.\nif path == '/':\n    self.pathway = (self.fm.get_directory('/'), )\nelse:\n    pathway = []\n    currentpath = '/'\n    for comp in path.split('/'):\n        currentpath = join(currentpath, comp)\n        pathway.append(self.fm.get_directory(currentpath))\n    self.pathway = tuple(pathway)\n\nself.assign_cursor_positions_for_subdirs()\n\n# set the current file.\nself.thisdir.sort_directories_first = self.fm.settings.sort_directories_first\nself.thisdir.sort_reverse = self.fm.settings.sort_reverse\nself.thisdir.sort_if_outdated()\nif selectfile:\n    self.thisdir.move_to_obj(selectfile)\nif previous and previous.path != path:\n    self.thisfile = self.thisdir.pointed_obj\nelse:\n    # This avoids setting self.pointer (through the 'move' signal) and\n    # is required so that you can use enter_dir when switching tabs\n    # without messing up the pointer.\n    self._thisfile = self.thisdir.pointed_obj\n\nif history:\n    self.history.add(new_thisdir)\n\nself.fm.signal_emit('cd', previous=previous, new=self.thisdir)\n\nreturn True", "path": "ranger/ranger/core/tab.py", "commit_date": "2020-12-05 00:00:00", "repo_name": "ranger/ranger", "stars": 14682, "license": "gpl-3.0", "language": "python", "size": 10410}
{"docstring": "\"\"\"Returns a delayed response (max of 10 seconds).\n---\ntags:\n  - Dynamic data\nparameters:\n  - in: path\n    name: delay\n    type: int\nproduces:\n  - application/json\nresponses:\n  200:\n    description: A delayed response.\n\"\"\"\n", "func_signal": "def delay_response(delay):\n", "code": "delay = min(float(delay), 10)\n\ntime.sleep(delay)\n\nreturn jsonify(\n    get_dict(\"url\", \"args\", \"form\", \"data\", \"origin\", \"headers\", \"files\")\n)", "path": "httpbin/httpbin/core.py", "commit_date": "2018-08-31 00:00:00", "repo_name": "postmanlabs/httpbin", "stars": 12298, "license": "isc", "language": "python", "size": 750}
{"docstring": "\"\"\"The request's DELETE parameters.\n---\ntags:\n  - HTTP Methods\nproduces:\n  - application/json\nresponses:\n  200:\n    description: The request's DELETE parameters.\n\"\"\"\n\n", "func_signal": "def view_delete():\n", "code": "return jsonify(\n    get_dict(\"url\", \"args\", \"form\", \"data\", \"origin\", \"headers\", \"files\", \"json\")\n)", "path": "httpbin/httpbin/core.py", "commit_date": "2018-08-31 00:00:00", "repo_name": "postmanlabs/httpbin", "stars": 12298, "license": "isc", "language": "python", "size": 750}
{"docstring": "\"\"\"Return the incoming requests's User-Agent header.\n---\ntags:\n  - Request inspection\nproduces:\n  - application/json\nresponses:\n  200:\n    description: The request's User-Agent header.\n\"\"\"\n\n", "func_signal": "def view_user_agent():\n", "code": "headers = get_headers()\n\nreturn jsonify({\"user-agent\": headers[\"user-agent\"]})", "path": "httpbin/httpbin/core.py", "commit_date": "2018-08-31 00:00:00", "repo_name": "postmanlabs/httpbin", "stars": 12298, "license": "isc", "language": "python", "size": 750}
{"docstring": "\"\"\"Absolutely 302 Redirects n times.\n---\ntags:\n  - Redirects\nparameters:\n  - in: path\n    name: n\n    type: int\nproduces:\n  - text/html\nresponses:\n  302:\n    description: A redirection.\n\"\"\"\n\n", "func_signal": "def absolute_redirect_n_times(n):\n", "code": "assert n > 0\n\nif n == 1:\n    return redirect(url_for(\"view_get\", _external=True))\n\nreturn _redirect(\"absolute\", n, True)", "path": "httpbin/httpbin/core.py", "commit_date": "2018-08-31 00:00:00", "repo_name": "postmanlabs/httpbin", "stars": 12298, "license": "isc", "language": "python", "size": 750}
{"docstring": "\"\"\"302/3XX Redirects to the given URL.\n---\ntags:\n  - Redirects\nproduces:\n  - text/html\nget:\n  parameters:\n    - in: query\n      name: url\n      type: string\n      required: true\n    - in: query\n      name: status_code\n      type: int\npost:\n  consumes:\n    - application/x-www-form-urlencoded\n  parameters:\n    - in: formData\n      name: url\n      type: string\n      required: true\n    - in: formData\n      name: status_code\n      type: int\n      required: false\npatch:\n  consumes:\n    - application/x-www-form-urlencoded\n  parameters:\n    - in: formData\n      name: url\n      type: string\n      required: true\n    - in: formData\n      name: status_code\n      type: int\n      required: false\nput:\n  consumes:\n    - application/x-www-form-urlencoded\n  parameters:\n    - in: formData\n      name: url\n      type: string\n      required: true\n    - in: formData\n      name: status_code\n      type: int\n      required: false\nresponses:\n  302:\n    description: A redirection.\n\"\"\"\n\n", "func_signal": "def redirect_to():\n", "code": "args_dict = request.args.items()\nargs = CaseInsensitiveDict(args_dict)\n\n# We need to build the response manually and convert to UTF-8 to prevent\n# werkzeug from \"fixing\" the URL. This endpoint should set the Location\n# header to the exact string supplied.\nresponse = app.make_response(\"\")\nresponse.status_code = 302\nif \"status_code\" in args:\n    status_code = int(args[\"status_code\"])\n    if status_code >= 300 and status_code < 400:\n        response.status_code = status_code\nresponse.headers[\"Location\"] = args[\"url\"].encode(\"utf-8\")\n\nreturn response", "path": "httpbin/httpbin/core.py", "commit_date": "2018-08-31 00:00:00", "repo_name": "postmanlabs/httpbin", "stars": 12298, "license": "isc", "language": "python", "size": 750}
{"docstring": "\"\"\"Sets a Cache-Control header for n seconds.\n---\ntags:\n  - Response inspection\nparameters:\n  - in: path\n    name: value\n    type: integer\nproduces:\n  - application/json\nresponses:\n  200:\n    description: Cache control set\n\"\"\"\n", "func_signal": "def cache_control(value):\n", "code": "response = view_get()\nresponse.headers[\"Cache-Control\"] = \"public, max-age={0}\".format(value)\nreturn response", "path": "httpbin/httpbin/core.py", "commit_date": "2018-08-31 00:00:00", "repo_name": "postmanlabs/httpbin", "stars": 12298, "license": "isc", "language": "python", "size": 750}
{"docstring": "\"\"\"Assumes the resource has the given etag and responds to If-None-Match and If-Match headers appropriately.\n---\ntags:\n  - Response inspection\nparameters:\n  - in: header\n    name: If-None-Match\n  - in: header\n    name: If-Match\nproduces:\n  - application/json\nresponses:\n  200:\n    description: Normal response\n  412:\n    description: match\n\n\"\"\"\n", "func_signal": "def etag(etag):\n", "code": "if_none_match = parse_multi_value_header(request.headers.get(\"If-None-Match\"))\nif_match = parse_multi_value_header(request.headers.get(\"If-Match\"))\n\nif if_none_match:\n    if etag in if_none_match or \"*\" in if_none_match:\n        response = status_code(304)\n        response.headers[\"ETag\"] = etag\n        return response\nelif if_match:\n    if etag not in if_match and \"*\" not in if_match:\n        return status_code(412)\n\n# Special cases don't apply, return normal response\nresponse = view_get()\nresponse.headers[\"ETag\"] = etag\nreturn response", "path": "httpbin/httpbin/core.py", "commit_date": "2018-08-31 00:00:00", "repo_name": "postmanlabs/httpbin", "stars": 12298, "license": "isc", "language": "python", "size": 750}
{"docstring": "\"\"\"Returns a simple image of the type suggest by the Accept header.\n---\ntags:\n  - Images\nproduces:\n  - image/webp\n  - image/svg+xml\n  - image/jpeg\n  - image/png\n  - image/*\nresponses:\n  200:\n    description: An image.\n\"\"\"\n\n", "func_signal": "def image():\n", "code": "headers = get_headers()\nif \"accept\" not in headers:\n    return image_png()  # Default media type to png\n\naccept = headers[\"accept\"].lower()\n\nif \"image/webp\" in accept:\n    return image_webp()\nelif \"image/svg+xml\" in accept:\n    return image_svg()\nelif \"image/jpeg\" in accept:\n    return image_jpeg()\nelif \"image/png\" in accept or \"image/*\" in accept:\n    return image_png()\nelse:\n    return status_code(406)  # Unsupported media type", "path": "httpbin/httpbin/core.py", "commit_date": "2018-08-31 00:00:00", "repo_name": "postmanlabs/httpbin", "stars": 12298, "license": "isc", "language": "python", "size": 750}
{"docstring": "\"\"\"302 Redirects n times.\n---\ntags:\n  - Redirects\nparameters:\n  - in: path\n    name: n\n    type: int\nproduces:\n  - text/html\nresponses:\n  302:\n    description: A redirection.\n\"\"\"\n", "func_signal": "def redirect_n_times(n):\n", "code": "assert n > 0\n\nabsolute = request.args.get(\"absolute\", \"false\").lower() == \"true\"\n\nif n == 1:\n    return redirect(url_for(\"view_get\", _external=absolute))\n\nif absolute:\n    return _redirect(\"absolute\", n, True)\nelse:\n    return _redirect(\"relative\", n, False)", "path": "httpbin/httpbin/core.py", "commit_date": "2018-08-31 00:00:00", "repo_name": "postmanlabs/httpbin", "stars": 12298, "license": "isc", "language": "python", "size": 750}
{"docstring": "\"\"\"Returns a set of response headers from the query string.\n---\ntags:\n  - Response inspection\nparameters:\n  - in: query\n    name: freeform\n    explode: true\n    allowEmptyValue: true\n    schema:\n      type: object\n      additionalProperties:\n        type: string\n    style: form\nproduces:\n  - application/json\nresponses:\n  200:\n    description: Response headers\n\"\"\"\n# Pending swaggerUI update\n# https://github.com/swagger-api/swagger-ui/issues/3850\n", "func_signal": "def response_headers():\n", "code": "headers = MultiDict(request.args.items(multi=True))\nresponse = jsonify(list(headers.lists()))\n\nwhile True:\n    original_data = response.data\n    d = {}\n    for key in response.headers.keys():\n        value = response.headers.get_all(key)\n        if len(value) == 1:\n            value = value[0]\n        d[key] = value\n    response = jsonify(d)\n    for key, value in headers.items(multi=True):\n        response.headers.add(key, value)\n    response_has_changed = response.data != original_data\n    if not response_has_changed:\n        break\nreturn response", "path": "httpbin/httpbin/core.py", "commit_date": "2018-08-31 00:00:00", "repo_name": "postmanlabs/httpbin", "stars": 12298, "license": "isc", "language": "python", "size": 750}
{"docstring": "\"\"\"Drips data over a duration after an optional initial delay.\n---\ntags:\n  - Dynamic data\nparameters:\n  - in: query\n    name: duration\n    type: number\n    description: The amount of time (in seconds) over which to drip each byte\n    default: 2\n    required: false\n  - in: query\n    name: numbytes\n    type: integer\n    description: The number of bytes to respond with\n    default: 10\n    required: false\n  - in: query\n    name: code\n    type: integer\n    description: The response code that will be returned\n    default: 200\n    required: false\n  - in: query\n    name: delay\n    type: number\n    description: The amount of time (in seconds) to delay before responding\n    default: 2\n    required: false\nproduces:\n  - application/octet-stream\nresponses:\n  200:\n    description: A dripped response.\n\"\"\"\n", "func_signal": "def drip():\n", "code": "args = CaseInsensitiveDict(request.args.items())\nduration = float(args.get(\"duration\", 2))\nnumbytes = min(int(args.get(\"numbytes\", 10)), (10 * 1024 * 1024))  # set 10MB limit\ncode = int(args.get(\"code\", 200))\n\nif numbytes <= 0:\n    response = Response(\"number of bytes must be positive\", status=400)\n    return response\n\ndelay = float(args.get(\"delay\", 0))\nif delay > 0:\n    time.sleep(delay)\n\npause = duration / numbytes\n\ndef generate_bytes():\n    for i in xrange(numbytes):\n        yield b\"*\"\n        time.sleep(pause)\n\nresponse = Response(\n    generate_bytes(),\n    headers={\n        \"Content-Type\": \"application/octet-stream\",\n        \"Content-Length\": str(numbytes),\n    },\n)\n\nresponse.status_code = code\n\nreturn response", "path": "httpbin/httpbin/core.py", "commit_date": "2018-08-31 00:00:00", "repo_name": "postmanlabs/httpbin", "stars": 12298, "license": "isc", "language": "python", "size": 750}
{"docstring": "\"\"\"Deletes cookie(s) as provided by the query string and redirects to cookie list.\n---\ntags:\n  - Cookies\nparameters:\n  - in: query\n    name: freeform\n    explode: true\n    allowEmptyValue: true\n    schema:\n      type: object\n      additionalProperties:\n        type: string\n    style: form\nproduces:\n  - text/plain\nresponses:\n  200:\n    description: Redirect to cookie list\n\"\"\"\n\n", "func_signal": "def delete_cookies():\n", "code": "cookies = dict(request.args.items())\nr = app.make_response(redirect(url_for(\"view_cookies\")))\nfor key, value in cookies.items():\n    r.delete_cookie(key=key)\n\nreturn r", "path": "httpbin/httpbin/core.py", "commit_date": "2018-08-31 00:00:00", "repo_name": "postmanlabs/httpbin", "stars": 12298, "license": "isc", "language": "python", "size": 750}
{"docstring": "\"\"\"The request's POST parameters.\n---\ntags:\n  - HTTP Methods\nproduces:\n  - application/json\nresponses:\n  200:\n    description: The request's POST parameters.\n\"\"\"\n\n", "func_signal": "def view_post():\n", "code": "return jsonify(\n    get_dict(\"url\", \"args\", \"form\", \"data\", \"origin\", \"headers\", \"files\", \"json\")\n)", "path": "httpbin/httpbin/core.py", "commit_date": "2018-08-31 00:00:00", "repo_name": "postmanlabs/httpbin", "stars": 12298, "license": "isc", "language": "python", "size": 750}
{"docstring": "\"\"\"Prompts the user for authorization using bearer authentication.\n---\ntags:\n  - Auth\nparameters:\n  - in: header\n    name: Authorization\n    schema:\n      type: string\nproduces:\n  - application/json\nresponses:\n  200:\n    description: Sucessful authentication.\n  401:\n    description: Unsuccessful authentication.\n\"\"\"\n", "func_signal": "def bearer_auth():\n", "code": "authorization = request.headers.get(\"Authorization\")\nif not (authorization and authorization.startswith(\"Bearer \")):\n    response = app.make_response(\"\")\n    response.headers[\"WWW-Authenticate\"] = \"Bearer\"\n    response.status_code = 401\n    return response\nslice_start = len(\"Bearer \")\ntoken = authorization[slice_start:]\n\nreturn jsonify(authenticated=True, token=token)", "path": "httpbin/httpbin/core.py", "commit_date": "2018-08-31 00:00:00", "repo_name": "postmanlabs/httpbin", "stars": 12298, "license": "isc", "language": "python", "size": 750}
{"docstring": "\"\"\"Returns a 304 if an If-Modified-Since header or If-None-Match is present. Returns the same as a GET otherwise.\n---\ntags:\n  - Response inspection\nparameters:\n  - in: header\n    name: If-Modified-Since\n  - in: header\n    name: If-None-Match\nproduces:\n  - application/json\nresponses:\n  200:\n    description: Cached response\n  304:\n    description: Modified\n\n\"\"\"\n", "func_signal": "def cache():\n", "code": "is_conditional = request.headers.get(\"If-Modified-Since\") or request.headers.get(\n    \"If-None-Match\"\n)\n\nif is_conditional is None:\n    response = view_get()\n    response.headers[\"Last-Modified\"] = http_date()\n    response.headers[\"ETag\"] = uuid.uuid4().hex\n    return response\nelse:\n    return status_code(304)", "path": "httpbin/httpbin/core.py", "commit_date": "2018-08-31 00:00:00", "repo_name": "postmanlabs/httpbin", "stars": 12298, "license": "isc", "language": "python", "size": 750}
{"docstring": "\"\"\"Returns a simple JSON document.\n---\ntags:\n  - Response formats\nproduces:\n  - application/json\nresponses:\n  200:\n    description: An JSON document.\n\"\"\"\n", "func_signal": "def a_json_endpoint():\n", "code": "return flask_jsonify(\n    slideshow={\n        \"title\": \"Sample Slide Show\",\n        \"date\": \"date of publication\",\n        \"author\": \"Yours Truly\",\n        \"slides\": [\n            {\"type\": \"all\", \"title\": \"Wake up to WonderWidgets!\"},\n            {\n                \"type\": \"all\",\n                \"title\": \"Overview\",\n                \"items\": [\n                    \"Why <em>WonderWidgets</em> are great\",\n                    \"Who <em>buys</em> WonderWidgets\",\n                ],\n            },\n        ],\n    }\n)", "path": "httpbin/httpbin/core.py", "commit_date": "2018-08-31 00:00:00", "repo_name": "postmanlabs/httpbin", "stars": 12298, "license": "isc", "language": "python", "size": 750}
{"docstring": "\"\"\"Stream n JSON responses\n---\ntags:\n  - Dynamic data\nparameters:\n  - in: path\n    name: n\n    type: int\nproduces:\n  - application/json\nresponses:\n  200:\n    description: Streamed JSON responses.\n\"\"\"\n", "func_signal": "def stream_n_messages(n):\n", "code": "response = get_dict(\"url\", \"args\", \"headers\", \"origin\")\nn = min(n, 100)\n\ndef generate_stream():\n    for i in range(n):\n        response[\"id\"] = i\n        yield json.dumps(response) + \"\\n\"\n\nreturn Response(generate_stream(), headers={\"Content-Type\": \"application/json\"})", "path": "httpbin/httpbin/core.py", "commit_date": "2018-08-31 00:00:00", "repo_name": "postmanlabs/httpbin", "stars": 12298, "license": "isc", "language": "python", "size": 750}
{"docstring": "\"\"\"Streams n random bytes generated with given seed, at given chunk size per packet.\n---\ntags:\n  - Dynamic data\nparameters:\n  - in: path\n    name: numbytes\n    type: int\nproduces:\n  - application/octet-stream\nresponses:\n  200:\n    description: Bytes.\n\"\"\"\n\n", "func_signal": "def range_request(numbytes):\n", "code": "if numbytes <= 0 or numbytes > (100 * 1024):\n    response = Response(\n        headers={\"ETag\": \"range%d\" % numbytes, \"Accept-Ranges\": \"bytes\"}\n    )\n    response.status_code = 404\n    response.data = \"number of bytes must be in the range (0, 102400]\"\n    return response\n\nparams = CaseInsensitiveDict(request.args.items())\nif \"chunk_size\" in params:\n    chunk_size = max(1, int(params[\"chunk_size\"]))\nelse:\n    chunk_size = 10 * 1024\n\nduration = float(params.get(\"duration\", 0))\npause_per_byte = duration / numbytes\n\nrequest_headers = get_headers()\nfirst_byte_pos, last_byte_pos = get_request_range(request_headers, numbytes)\nrange_length = (last_byte_pos + 1) - first_byte_pos\n\nif (\n    first_byte_pos > last_byte_pos\n    or first_byte_pos not in xrange(0, numbytes)\n    or last_byte_pos not in xrange(0, numbytes)\n):\n    response = Response(\n        headers={\n            \"ETag\": \"range%d\" % numbytes,\n            \"Accept-Ranges\": \"bytes\",\n            \"Content-Range\": \"bytes */%d\" % numbytes,\n            \"Content-Length\": \"0\",\n        }\n    )\n    response.status_code = 416\n    return response\n\ndef generate_bytes():\n    chunks = bytearray()\n\n    for i in xrange(first_byte_pos, last_byte_pos + 1):\n\n        # We don't want the resource to change across requests, so we need\n        # to use a predictable data generation function\n        chunks.append(ord(\"a\") + (i % 26))\n        if len(chunks) == chunk_size:\n            yield (bytes(chunks))\n            time.sleep(pause_per_byte * chunk_size)\n            chunks = bytearray()\n\n    if chunks:\n        time.sleep(pause_per_byte * len(chunks))\n        yield (bytes(chunks))\n\ncontent_range = \"bytes %d-%d/%d\" % (first_byte_pos, last_byte_pos, numbytes)\nresponse_headers = {\n    \"Content-Type\": \"application/octet-stream\",\n    \"ETag\": \"range%d\" % numbytes,\n    \"Accept-Ranges\": \"bytes\",\n    \"Content-Length\": str(range_length),\n    \"Content-Range\": content_range,\n}\n\nresponse = Response(generate_bytes(), headers=response_headers)\n\nif (first_byte_pos == 0) and (last_byte_pos == (numbytes - 1)):\n    response.status_code = 200\nelse:\n    response.status_code = 206\n\nreturn response", "path": "httpbin/httpbin/core.py", "commit_date": "2018-08-31 00:00:00", "repo_name": "postmanlabs/httpbin", "stars": 12298, "license": "isc", "language": "python", "size": 750}
{"docstring": "\"\"\"Returns a simple PNG image.\n---\ntags:\n  - Images\nproduces:\n  - image/png\nresponses:\n  200:\n    description: A PNG image.\n\"\"\"\n", "func_signal": "def image_png():\n", "code": "data = resource(\"images/pig_icon.png\")\nreturn Response(data, headers={\"Content-Type\": \"image/png\"})", "path": "httpbin/httpbin/core.py", "commit_date": "2018-08-31 00:00:00", "repo_name": "postmanlabs/httpbin", "stars": 12298, "license": "isc", "language": "python", "size": 750}
{"docstring": "\"\"\"Returns anything passed in request data.\n---\ntags:\n  - Anything\nproduces:\n  - application/json\nresponses:\n  200:\n    description: Anything passed in request\n\"\"\"\n\n", "func_signal": "def view_anything(anything=None):\n", "code": "return jsonify(\n    get_dict(\n        \"url\",\n        \"args\",\n        \"headers\",\n        \"origin\",\n        \"method\",\n        \"form\",\n        \"data\",\n        \"files\",\n        \"json\",\n    )\n)", "path": "httpbin/httpbin/core.py", "commit_date": "2018-08-31 00:00:00", "repo_name": "postmanlabs/httpbin", "stars": 12298, "license": "isc", "language": "python", "size": 750}
{"docstring": "\"\"\" Toggle optional annotations on or off after the user depresses an optional button.\n\nParameters\n----------\nannotation: [\"mesh\", \"mask\"]\n    The optional annotation to toggle on or off\n\"\"\"\n", "func_signal": "def _toggle_annotations(self, annotation):\n", "code": "state = \"normal\" if self.optional_annotations[annotation] else \"hidden\"\nlogger.debug(\"Toggle annotation: (annotation: %s, state: %s)\", annotation, state)\nif annotation == \"mesh\":\n    self._view.toggle_mesh(state)\nif annotation == \"mask\":\n    self._view.toggle_mask(state, self.selected_mask)", "path": "faceswap/tools/manual/faceviewer/frame.py", "commit_date": "2020-12-15 00:00:00", "repo_name": "deepfakes/faceswap", "stars": 48712, "license": "gpl-3.0", "language": "python", "size": 201971}
{"docstring": "\"\"\" Bind mouse wheel to scroll the :class:`FacesViewer` canvas. \"\"\"\n", "func_signal": "def _bind_mouse_wheel_scrolling(self):\n", "code": "if platform.system() == \"Linux\":\n    self.bind(\"<Button-4>\", self._scroll)\n    self.bind(\"<Button-5>\", self._scroll)\nelse:\n    self.bind(\"<MouseWheel>\", self._scroll)", "path": "faceswap/tools/manual/faceviewer/frame.py", "commit_date": "2020-12-15 00:00:00", "repo_name": "deepfakes/faceswap", "stars": 48712, "license": "gpl-3.0", "language": "python", "size": 201971}
{"docstring": "\"\"\" Pop up the context menu on a right click mouse event.\n\nParameters\n----------\nevent: :class:`tkinter.Event`\n    The mouse event that has triggered the pop up menu\n\"\"\"\n", "func_signal": "def _pop_menu(self, event):\n", "code": "frame_idx, face_idx = self._canvas.viewport.face_from_point(\n    self._canvas.canvasx(event.x), self._canvas.canvasy(event.y))[:2]\nif frame_idx == -1:\n    logger.trace(\"No valid item under mouse\")\n    self._frame_index = self._face_index = None\n    return\nself._frame_index = frame_idx\nself._face_index = face_idx\nlogger.trace(\"Popping right click menu\")\nself._menu.popup(event)", "path": "faceswap/tools/manual/faceviewer/frame.py", "commit_date": "2020-12-15 00:00:00", "repo_name": "deepfakes/faceswap", "stars": 48712, "license": "gpl-3.0", "language": "python", "size": 201971}
{"docstring": "\"\"\"\nParameters:\n-------\nimage: NumPy array\n    OpenCV image in L*a*b* color space\n\nReturns:\n-------\nTuple of mean and standard deviations for the L*, a*, and b*\nchannels, respectively\n\"\"\"\n# compute the mean and standard deviation of each channel\n", "func_signal": "def image_stats(image):\n", "code": "(light, col_a, col_b) = cv2.split(image)  # pylint: disable=no-member\n(l_mean, l_std) = (light.mean(), light.std())\n(a_mean, a_std) = (col_a.mean(), col_a.std())\n(b_mean, b_std) = (col_b.mean(), col_b.std())\n\n# return the color statistics\nreturn (l_mean, l_std, a_mean, a_std, b_mean, b_std)", "path": "faceswap/plugins/convert/color/color_transfer.py", "commit_date": "2019-04-21 00:00:00", "repo_name": "deepfakes/faceswap", "stars": 48712, "license": "gpl-3.0", "language": "python", "size": 201971}
{"docstring": "\"\"\":class:`numpy.ndarray`:  A numpy array of shape (`4`, `rows`, `columns`) corresponding\nto the viewable area of the display grid. 1st dimension contains frame indices, 2nd\ndimension face indices. The 3rd and 4th dimension contain the x and y position of the top\nleft corner of the face respectively.\n\nAny locations that are not populated by a face will have a frame and face index of -1\n\"\"\"\n", "func_signal": "def visible_area(self):\n", "code": "if not self._is_valid:\n    retval = None, None\nelse:\n    top, bottom = self._visible_row_indices\n    retval = self._grid[:, top:bottom, :], self._display_faces[top:bottom, :]\nlogger.trace([r if r is None else r.shape for r in retval])\nreturn retval", "path": "faceswap/tools/manual/faceviewer/frame.py", "commit_date": "2020-12-15 00:00:00", "repo_name": "deepfakes/faceswap", "stars": 48712, "license": "gpl-3.0", "language": "python", "size": 201971}
{"docstring": "\"\"\" Callback on scrollbar scroll. Updates the canvas location and displays/hides\nthumbnail images.\n\nParameters\n----------\nevent :class:`tkinter.Event`\n    The scrollbar callback event\n\"\"\"\n", "func_signal": "def _on_scroll(self, *event):\n", "code": "self._canvas.yview(*event)\nself._canvas.viewport.update()", "path": "faceswap/tools/manual/faceviewer/frame.py", "commit_date": "2020-12-15 00:00:00", "repo_name": "deepfakes/faceswap", "stars": 48712, "license": "gpl-3.0", "language": "python", "size": 201971}
{"docstring": "\"\"\"\nParameters:\n-------\nsource: NumPy array\n    OpenCV image in BGR color space (the source image)\ntarget: NumPy array\n    OpenCV image in BGR color space (the target image)\nclip: Should components of L*a*b* image be scaled by np.clip before\n    converting back to BGR color space?\n    If False then components will be min-max scaled appropriately.\n    Clipping will keep target image brightness truer to the input.\n    Scaling will adjust image brightness to avoid washed out portions\n    in the resulting color transfer that can be caused by clipping.\npreserve_paper: Should color transfer strictly follow methodology\n    layed out in original paper? The method does not always produce\n    aesthetically pleasing results.\n    If False then L*a*b* components will scaled using the reciprocal of\n    the scaling factor proposed in the paper.  This method seems to produce\n    more consistently aesthetically pleasing results\n\nReturns:\n-------\ntransfer: NumPy array\n    OpenCV image (w, h, 3) NumPy array (uint8)\n\"\"\"\n", "func_signal": "def process(self, old_face, new_face, raw_mask):\n", "code": "clip = self.config.get(\"clip\", True)\npreserve_paper = self.config.get(\"preserve_paper\", True)\n\n# convert the images from the RGB to L*ab* color space, being\n# sure to utilizing the floating point data type (note: OpenCV\n# expects floats to be 32-bit, so use that instead of 64-bit)\nsource = cv2.cvtColor(  # pylint: disable=no-member\n    np.rint(old_face * raw_mask * 255.0).astype(\"uint8\"),\n    cv2.COLOR_BGR2LAB).astype(\"float32\")  # pylint: disable=no-member\ntarget = cv2.cvtColor(  # pylint: disable=no-member\n    np.rint(new_face * raw_mask * 255.0).astype(\"uint8\"),\n    cv2.COLOR_BGR2LAB).astype(\"float32\")  # pylint: disable=no-member\n# compute color statistics for the source and target images\n(l_mean_src, l_std_src,\n a_mean_src, a_std_src,\n b_mean_src, b_std_src) = self.image_stats(source)\n(l_mean_tar, l_std_tar,\n a_mean_tar, a_std_tar,\n b_mean_tar, b_std_tar) = self.image_stats(target)\n\n# subtract the means from the target image\n(light, col_a, col_b) = cv2.split(target)  # pylint: disable=no-member\nlight -= l_mean_tar\ncol_a -= a_mean_tar\ncol_b -= b_mean_tar\n\nif preserve_paper:\n    # scale by the standard deviations using paper proposed factor\n    light = (l_std_tar / l_std_src) * light\n    col_a = (a_std_tar / a_std_src) * col_a\n    col_b = (b_std_tar / b_std_src) * col_b\nelse:\n    # scale by the standard deviations using reciprocal of paper proposed factor\n    light = (l_std_src / l_std_tar) * light\n    col_a = (a_std_src / a_std_tar) * col_a\n    col_b = (b_std_src / b_std_tar) * col_b\n\n# add in the source mean\nlight += l_mean_src\ncol_a += a_mean_src\ncol_b += b_mean_src\n\n# clip/scale the pixel intensities to [0, 255] if they fall\n# outside this range\nlight = self._scale_array(light, clip=clip)\ncol_a = self._scale_array(col_a, clip=clip)\ncol_b = self._scale_array(col_b, clip=clip)\n\n# merge the channels together and convert back to the RGB color\n# space, being sure to utilize the 8-bit unsigned integer data\n# type\ntransfer = cv2.merge([light, col_a, col_b])  # pylint: disable=no-member\ntransfer = cv2.cvtColor(  # pylint: disable=no-member\n    transfer.astype(\"uint8\"),\n    cv2.COLOR_LAB2BGR).astype(\"float32\") / 255.0  # pylint: disable=no-member\nbackground = new_face * (1 - raw_mask)\nmerged = transfer + background\n# return the color transferred image\nreturn merged", "path": "faceswap/plugins/convert/color/color_transfer.py", "commit_date": "2019-04-21 00:00:00", "repo_name": "deepfakes/faceswap", "stars": 48712, "license": "gpl-3.0", "language": "python", "size": 201971}
{"docstring": "\"\"\" Handle mouse wheel scrolling over the :class:`FacesViewer` canvas.\n\nUpdate is run in a thread to avoid repeated scroll actions stacking and locking up the GUI.\n\nParameters\n----------\nevent: :class:`tkinter.Event`\n    The event fired by the mouse scrolling\n\"\"\"\n", "func_signal": "def _scroll(self, event):\n", "code": "if self._event.is_set():\n    logger.trace(\"Update already running. Aborting repeated mousewheel\")\n    return\nif platform.system() == \"Darwin\":\n    adjust = event.delta\nelif platform.system() == \"Windows\":\n    adjust = event.delta / 120\nelif event.num == 5:\n    adjust = -1\nelse:\n    adjust = 1\nself._event.set()\nthread = Thread(target=self.canvas_scroll, args=(-1 * adjust, \"units\", self._event))\nthread.start()", "path": "faceswap/tools/manual/faceviewer/frame.py", "commit_date": "2020-12-15 00:00:00", "repo_name": "deepfakes/faceswap", "stars": 48712, "license": "gpl-3.0", "language": "python", "size": 201971}
{"docstring": "\"\"\" Update the mesh color when user updates the control panel. \"\"\"\n", "func_signal": "def _update_mesh_color(self):\n", "code": "color = self.get_muted_color(\"Mesh\")\nif self._annotation_colors[\"mesh\"] == color:\n    return\nhighlight_color = self.control_colors[\"Mesh\"]\n\nself.itemconfig(\"viewport_polygon\", outline=color)\nself.itemconfig(\"viewport_line\", fill=color)\nself.itemconfig(\"active_mesh_polygon\", outline=highlight_color)\nself.itemconfig(\"active_mesh_line\", fill=highlight_color)\nself._annotation_colors[\"mesh\"] = color", "path": "faceswap/tools/manual/faceviewer/frame.py", "commit_date": "2020-12-15 00:00:00", "repo_name": "deepfakes/faceswap", "stars": 48712, "license": "gpl-3.0", "language": "python", "size": 201971}
{"docstring": "\"\"\" Add the display buttons to the Faces window.\n\nReturns\n-------\ndict\n    The display name and its associated button.\n\"\"\"\n", "func_signal": "def _add_buttons(self):\n", "code": "frame = ttk.Frame(self)\nframe.pack(side=tk.TOP, fill=tk.Y)\nbuttons = dict()\nfor display in self.key_bindings.values():\n    var = tk.BooleanVar()\n    var.set(False)\n    self._tk_vars[display] = var\n\n    lookup = \"landmarks\" if display == \"mesh\" else display\n    button = ttk.Button(frame,\n                        image=get_images().icons[lookup],\n                        command=lambda t=display: self.on_click(t),\n                        style=\"display_deselected.TButton\")\n    button.state([\"!pressed\", \"!focus\"])\n    button.pack()\n    Tooltip(button, text=self._helptext[display])\n    buttons[display] = button\nreturn buttons", "path": "faceswap/tools/manual/faceviewer/frame.py", "commit_date": "2020-12-15 00:00:00", "repo_name": "deepfakes/faceswap", "stars": 48712, "license": "gpl-3.0", "language": "python", "size": 201971}
{"docstring": "\"\"\" Set the tkinter variable call backs.\n\nRedraw the grid on a face size change, a filter change or on add/remove faces.\nUpdates the annotation colors when user amends a color drop down.\nUpdates the mask type when the user changes the selected mask types\nToggles the face viewer annotations on an optional annotation button press.\n\"\"\"\n", "func_signal": "def _set_tk_callbacks(self, detected_faces):\n", "code": "for var in (self._globals.tk_faces_size, self._globals.tk_filter_mode):\n    var.trace(\"w\", lambda *e, v=var: self.refresh_grid(v))\nvar = detected_faces.tk_face_count_changed\nvar.trace(\"w\", lambda *e, v=var: self.refresh_grid(v, retain_position=True))\n\nself._display_frame.tk_control_colors[\"Mesh\"].trace(\n    \"w\", lambda *e: self._update_mesh_color())\nself._display_frame.tk_control_colors[\"ExtractBox\"].trace(\n    \"w\", lambda *e: self._update_box_color())\nself._display_frame.tk_selected_mask.trace(\"w\", lambda *e: self._update_mask_type())\n\nfor opt, var in self._tk_optional_annotations.items():\n    var.trace(\"w\", lambda *e, o=opt: self._toggle_annotations(o))\n\nself.bind(\"<Configure>\", lambda *e: self._view.update())", "path": "faceswap/tools/manual/faceviewer/frame.py", "commit_date": "2020-12-15 00:00:00", "repo_name": "deepfakes/faceswap", "stars": 48712, "license": "gpl-3.0", "language": "python", "size": 201971}
{"docstring": "\"\"\" tuple: the (`columns`, `rows`) required to hold all display images. \"\"\"\n", "func_signal": "def columns_rows(self):\n", "code": "retval = tuple(reversed(self._grid.shape[1:])) if self._is_valid else (0, 0)\nreturn retval", "path": "faceswap/tools/manual/faceviewer/frame.py", "commit_date": "2020-12-15 00:00:00", "repo_name": "deepfakes/faceswap", "stars": 48712, "license": "gpl-3.0", "language": "python", "size": 201971}
{"docstring": "\"\"\" Add a scrollbar to the faces frame \"\"\"\n", "func_signal": "def _add_scrollbar(self):\n", "code": "logger.debug(\"Add Faces Viewer Scrollbar\")\nscrollbar = ttk.Scrollbar(self._faces_frame, command=self._on_scroll)\nscrollbar.pack(side=tk.RIGHT, fill=tk.Y)\nself._canvas.config(yscrollcommand=scrollbar.set)\nself.bind(\"<Configure>\", self._update_viewport)\nlogger.debug(\"Added Faces Viewer Scrollbar\")\nself.update_idletasks()  # Update so scrollbar width is correct\nreturn scrollbar.winfo_width()", "path": "faceswap/tools/manual/faceviewer/frame.py", "commit_date": "2020-12-15 00:00:00", "repo_name": "deepfakes/faceswap", "stars": 48712, "license": "gpl-3.0", "language": "python", "size": 201971}
{"docstring": "\"\"\" Get the frame and face index for each grid position for the current filter.\n\nReturns\n-------\n:class:`numpy.ndarray`\n    Array of dimensions (2, rows, columns) corresponding to the display grid, with frame\n    index as the first dimension and face index within the frame as the 2nd dimension.\n\n    Any remaining placeholders at the end of the grid which are not populated with a face\n    are given the index -1\n\"\"\"\n", "func_signal": "def _get_labels(self):\n", "code": "face_count = len(self._raw_indices[\"frame\"])\nself._is_valid = face_count != 0\nif not self._is_valid:\n    return None\ncolumns = self._canvas.winfo_width() // self._face_size\nrows = ceil(face_count / columns)\nremainder = face_count % columns\npadding = [] if remainder == 0 else [-1 for _ in range(columns - remainder)]\nlabels = np.array((self._raw_indices[\"frame\"] + padding,\n                   self._raw_indices[\"face\"] + padding),\n                  dtype=\"int\").reshape((2, rows, columns))\nlogger.debug(labels.shape)\nreturn labels", "path": "faceswap/tools/manual/faceviewer/frame.py", "commit_date": "2020-12-15 00:00:00", "repo_name": "deepfakes/faceswap", "stars": 48712, "license": "gpl-3.0", "language": "python", "size": 201971}
{"docstring": "\"\"\" Get the grid information for faces currently displayed in the :class:`FacesViewer`.\n\nReturns\n:class:`numpy.ndarray`\n    A numpy array of shape (`4`, `rows`, `columns`) corresponding to the display grid.\n    1st dimension contains frame indices, 2nd dimension face indices. The 3rd and 4th\n    dimension contain the x and y position of the top left corner of the face respectively.\n\n    Any locations that are not populated by a face will have a frame and face index of -1\n\"\"\"\n", "func_signal": "def _get_grid(self):\n", "code": "labels = self._get_labels()\nif not self._is_valid:\n    logger.debug(\"Setting grid to None for no faces.\")\n    self._grid = None\n    return\nx_coords = np.linspace(0,\n                       labels.shape[2] * self._face_size,\n                       num=labels.shape[2],\n                       endpoint=False,\n                       dtype=\"int\")\ny_coords = np.linspace(0,\n                       labels.shape[1] * self._face_size,\n                       num=labels.shape[1],\n                       endpoint=False,\n                       dtype=\"int\")\nself._grid = np.array((*labels, *np.meshgrid(x_coords, y_coords)), dtype=\"int\")\nlogger.debug(self._grid.shape)", "path": "faceswap/tools/manual/faceviewer/frame.py", "commit_date": "2020-12-15 00:00:00", "repo_name": "deepfakes/faceswap", "stars": 48712, "license": "gpl-3.0", "language": "python", "size": 201971}
{"docstring": "\"\"\" int: The currently selected thumbnail size in pixels \"\"\"\n", "func_signal": "def face_size(self):\n", "code": "scaling = get_config().scaling_factor\nsize = self._sizes[self._globals.tk_faces_size.get().lower().replace(\" \", \"\")]\nreturn int(round(size * scaling))", "path": "faceswap/tools/manual/faceviewer/frame.py", "commit_date": "2020-12-15 00:00:00", "repo_name": "deepfakes/faceswap", "stars": 48712, "license": "gpl-3.0", "language": "python", "size": 201971}
{"docstring": "\"\"\" Update the underlying grid.\n\nCalled on initialization, on a filter change or on add/remove faces. Recalculates the\nunderlying grid for the current filter view and updates the attributes :attr:`_grid`,\n:attr:`_display_faces`, :attr:`_raw_indices`, :attr:`_frames_list` and :attr:`is_valid`\n\"\"\"\n", "func_signal": "def update(self):\n", "code": "self._face_size = self._canvas.face_size\nself._raw_indices = self._detected_faces.filter.raw_indices\nself._frames_list = self._detected_faces.filter.frames_list\nself._get_grid()\nself._get_display_faces()\nself._canvas.coords(\"backdrop\", 0, 0, *self.dimensions)\nself._canvas.configure(scrollregion=(self._canvas.bbox(\"backdrop\")))\nself._canvas.yview_moveto(0.0)", "path": "faceswap/tools/manual/faceviewer/frame.py", "commit_date": "2020-12-15 00:00:00", "repo_name": "deepfakes/faceswap", "stars": 48712, "license": "gpl-3.0", "language": "python", "size": 201971}
{"docstring": "\"\"\" Delete the selected face on a right click mouse delete action. \"\"\"\n", "func_signal": "def _delete_face(self):\n", "code": "logger.trace(\"Right click delete received. frame_id: %s, face_id: %s\",\n             self._frame_index, self._face_index)\nself._detected_faces.update.delete(self._frame_index, self._face_index)\nself._frame_index = self._face_index = None", "path": "faceswap/tools/manual/faceviewer/frame.py", "commit_date": "2020-12-15 00:00:00", "repo_name": "deepfakes/faceswap", "stars": 48712, "license": "gpl-3.0", "language": "python", "size": 201971}
{"docstring": "\"\"\" Update the active box color when user updates the control panel. \"\"\"\n", "func_signal": "def _update_box_color(self):\n", "code": "color = self.control_colors[\"ExtractBox\"]\n\nif self._annotation_colors[\"box\"] == color:\n    return\nself.itemconfig(\"active_highlighter\", outline=color)\nself._annotation_colors[\"box\"] = color", "path": "faceswap/tools/manual/faceviewer/frame.py", "commit_date": "2020-12-15 00:00:00", "repo_name": "deepfakes/faceswap", "stars": 48712, "license": "gpl-3.0", "language": "python", "size": 201971}
{"docstring": "\"\"\" Recalculate the full grid and redraw. Used when the active filter pull down is used, a\nface has been added or removed, or the face thumbnail size has changed.\n\nParameters\n----------\ntrigger_var: :class:`tkinter.BooleanVar`\n    The tkinter variable that has triggered the grid update. Will either be the variable\n    indicating that the face size have been changed, or the variable indicating that the\n    selected filter mode has been changed.\nretain_position: bool, optional\n    ``True`` if the grid should be set back to the position it was at after the update has\n    been processed, otherwise ``False``. Default: ``False``.\n\"\"\"\n", "func_signal": "def refresh_grid(self, trigger_var, retain_position=False):\n", "code": "if not trigger_var.get():\n    return\nsize_change = isinstance(trigger_var, tk.StringVar)\nmove_to = self.yview()[0] if retain_position else 0.0\nself._grid.update()\nif move_to != 0.0:\n    self.yview_moveto(move_to)\nif size_change:\n    self._view.reset()\nself._view.update(refresh_annotations=retain_position)\nif not size_change:\n    trigger_var.set(False)", "path": "faceswap/tools/manual/faceviewer/frame.py", "commit_date": "2020-12-15 00:00:00", "repo_name": "deepfakes/faceswap", "stars": 48712, "license": "gpl-3.0", "language": "python", "size": 201971}
{"docstring": "# def a function eho updates name in database\n", "func_signal": "def update_age():\n", "code": "def update_age_in_database():\n    new_age = entry_name.get()\n    r = check_string_in_account_no(new_age)\n    if len(new_age) != 0 and r:\n        # function in backend that updates name in table\n        backend.update_age_in_bank_table(new_age, acc_no)\n        entry_name.destroy()\n        submit_button.destroy()\n        age_label.destroy()\n    else:\n        tkinter.messagebox.showinfo('Error', 'Please enter age')\n        entry_name.destroy()\n        submit_button.destroy()\n        age_label.destroy()\n\nglobal age_label\nage_label = Label(update_customer_frame, text='Enter new Age:')\nage_label.grid(row=2, column=1)\nglobal entry_name\nentry_name = Entry(update_customer_frame)\nentry_name.grid(row=2, column=2, padx=2)\nglobal submit_button\nsubmit_button = Button(update_customer_frame, text='Update', command=update_age_in_database)\nsubmit_button.grid(row=2, column=3)", "path": "Python/bank_managment_system/frontend.py", "commit_date": "2019-10-10 00:00:00", "repo_name": "geekcomputers/Python", "stars": 29280, "license": "mit", "language": "python", "size": 36614}
{"docstring": "# for manipulating the data base\n", "func_signal": "def add_button(p1):\n", "code": "global cursor\nglobal connection\nif (len(w.inputTitle.get()) > 0 and len(w.inputNotice.get(1.0, END)) > 0):\n    w.errorOutput.configure(text=\"\")\n    title = w.inputTitle.get()\n    note = w.inputNotice.get(1.0, END)\n    sql_command = \"\"\"INSERT INTO notes (title,note) VALUES (\"{0}\",\"{1}\"); \"\"\"\n    sql_command = sql_command.format(title, note)\n    cursor.execute(sql_command)\n    connection.commit()\nelse:\n    w.errorOutput.configure(text=\"Please fill the fields. \")", "path": "Python/notepad/notepad_support.py", "commit_date": "2020-04-24 00:00:00", "repo_name": "geekcomputers/Python", "stars": 29280, "license": "mit", "language": "python", "size": 36614}
{"docstring": "'''\nReturns prime factors of n as a list.\n'''\n", "func_signal": "def prime_factors(n):\n", "code": "i = 2\nfactors = []\nwhile i * i <= n:\n    if n % i:\n        i += 1\n    else:\n        n //= i\n        factors.append(i)\nif n > 1:\n    factors.append(n)\nreturn factors", "path": "Python/MobiusFunction.py", "commit_date": "2019-10-10 00:00:00", "repo_name": "geekcomputers/Python", "stars": 29280, "license": "mit", "language": "python", "size": 36614}
{"docstring": "'''\nThis functions takes a list of prime factors as input.\nreturns True if the factors are square free.\n'''\n", "func_signal": "def is_square_free(factors):\n", "code": "for i in factors:\n    if factors.count(i) > 1:\n        return False\nreturn True", "path": "Python/MobiusFunction.py", "commit_date": "2019-10-10 00:00:00", "repo_name": "geekcomputers/Python", "stars": 29280, "license": "mit", "language": "python", "size": 36614}
{"docstring": "\"\"\"\n    for creating a new database\n\"\"\"\n", "func_signal": "def create_button(p1):\n", "code": "global cursor\n\nsql_command = \"\"\"\nCREATE TABLE notes ( \nid INTEGER PRIMARY KEY, \ntitle TEXT, \nnote TEXT);\"\"\"\n\ntry:\n    cursor.execute(sql_command)\n    w.errorOutput.configure(text=\"\")\nexcept:\n    w.errorOutput.configure(text=\"The database already exists\")", "path": "Python/notepad/notepad_support.py", "commit_date": "2020-04-24 00:00:00", "repo_name": "geekcomputers/Python", "stars": 29280, "license": "mit", "language": "python", "size": 36614}
{"docstring": "'''\nDefines Mobius function\n'''\n", "func_signal": "def mobius_function(n):\n", "code": "factors = prime_factors(n)\nif is_square_free(factors):\n    if len(factors) % 2 == 0:\n        return 1\n    elif len(factors) % 2 != 0:\n        return -1\nelse:\n    return 0", "path": "Python/MobiusFunction.py", "commit_date": "2019-10-10 00:00:00", "repo_name": "geekcomputers/Python", "stars": 29280, "license": "mit", "language": "python", "size": 36614}
{"docstring": "\"\"\"\nCalls all the other methods to process the input. Pads the data, then splits into\nblocks and then does a series of operations for each block (including expansion).\nFor each block, the variable h that was initialized is copied to a,b,c,d,e\nand these 5 variables a,b,c,d,e undergo several changes. After all the blocks are\nprocessed, these 5 variables are pairwise added to h ie a to h[0], b to h[1] and so on.\nThis h becomes our final hash which is returned.\n\"\"\"\n", "func_signal": "def final_hash(self):\n", "code": "self.padded_data = self.padding()\nself.blocks = self.split_blocks()\nfor block in self.blocks:\n    expanded_block = self.expand_block(block)\n    a, b, c, d, e = self.h\n    for i in range(0, 80):\n        if 0 <= i < 20:\n            f = (b & c) | ((~b) & d)\n            k = 0x5A827999\n        elif 20 <= i < 40:\n            f = b ^ c ^ d\n            k = 0x6ED9EBA1\n        elif 40 <= i < 60:\n            f = (b & c) | (b & d) | (c & d)\n            k = 0x8F1BBCDC\n        elif 60 <= i < 80:\n            f = b ^ c ^ d\n            k = 0xCA62C1D6\n        a, b, c, d, e = self.rotate(a, 5) + f + e + k + expanded_block[i] & 0xffffffff, \\\n                        a, self.rotate(b, 30), c, d\nself.h = self.h[0] + a & 0xffffffff, \\\n         self.h[1] + b & 0xffffffff, \\\n         self.h[2] + c & 0xffffffff, \\\n         self.h[3] + d & 0xffffffff, \\\n         self.h[4] + e & 0xffffffff\nreturn '%08x%08x%08x%08x%08x' % tuple(self.h)", "path": "Python/sha1.py", "commit_date": "2019-10-10 00:00:00", "repo_name": "geekcomputers/Python", "stars": 29280, "license": "mit", "language": "python", "size": 36614}
{"docstring": "\"\"\"\n    main-program\n    purpose: handles user input and prints \n             information to the console.\n\"\"\"\n\n", "func_signal": "def main():\n", "code": "print(\"\\nScientific Calculator\\n\\nFor Example: sin(rad(90)) + 50% * (sqrt(16)) + round(1.42^2)\" +\n      \"- 12mod3\\n\\nEnter quit to exit\")\n\nif sys.version_info.major >= 3:\n    while True:\n        k = input(\"\\nWhat is \")\n        if k == 'quit':\n            break\n        result(k)\n\nelse:\n    while True:\n        k = raw_input(\"\\nWhat is \")\n        if k == 'quit':\n            break\n        result(k)", "path": "Python/calculator.py", "commit_date": "2019-10-19 00:00:00", "repo_name": "geekcomputers/Python", "stars": 29280, "license": "mit", "language": "python", "size": 36614}
{"docstring": "# def a function eho updates name in database\n", "func_signal": "def update_name():\n", "code": "def update_name_in_database():\n    new_name = entry_name.get()\n    r = check_string_in_account_no(new_name)\n    if len(new_name) != 0:\n        # function in backend that updates name in table\n        backend.update_name_in_bank_table(new_name, acc_no)\n        entry_name.destroy()\n        submit_button.destroy()\n        name_label.destroy()\n    else:\n        tkinter.messagebox.showinfo('Error', 'Please fill blanks')\n        entry_name.destroy()\n        submit_button.destroy()\n        name_label.destroy()\n\nglobal entry_name\nglobal name_label\nname_label = Label(update_customer_frame, text='Enter new name')\nname_label.grid(row=1, column=1)\nentry_name = Entry(update_customer_frame)\nentry_name.grid(row=1, column=2, padx=2)\nglobal submit_button\nsubmit_button = Button(update_customer_frame, text='Update', command=update_name_in_database)\nsubmit_button.grid(row=1, column=3)", "path": "Python/bank_managment_system/frontend.py", "commit_date": "2019-10-10 00:00:00", "repo_name": "geekcomputers/Python", "stars": 29280, "license": "mit", "language": "python", "size": 36614}
{"docstring": "\"\"\"\nInititates the variables data and h. h is a list of 5 8-digit Hexadecimal\nnumbers corresponding to (1732584193, 4023233417, 2562383102, 271733878, 3285377520)\nrespectively. We will start with this as a message digest. 0x is how you write\nHexadecimal numbers in Python\n\"\"\"\n", "func_signal": "def __init__(self, data):\n", "code": "self.data = data\nself.h = [0x67452301, 0xEFCDAB89, 0x98BADCFE, 0x10325476, 0xC3D2E1F0]", "path": "Python/sha1.py", "commit_date": "2019-10-10 00:00:00", "repo_name": "geekcomputers/Python", "stars": 29280, "license": "mit", "language": "python", "size": 36614}
{"docstring": "# initialize the dimensions of the image to be resized and\n# grab the image size\n", "func_signal": "def image_resize(image, width=None, height=None, inter=cv2.INTER_AREA):\n", "code": "dim = None\n(h, w) = image.shape[:2]\n# if both the width and height are None, then return the\n# original image\nif width is None and height is None:\n    return image\n# check to see if the width is None\nif width is None:\n    # calculate the ratio of the height and construct the\n    # dimensions\n    r = height / float(h)\n    dim = (int(w * r), height)\n# otherwise, the height is None\nelse:\n    # calculate the ratio of the width and construct the\n    # dimensions\n    r = width / float(w)\n    dim = (width, int(h * r))\n\n# resize the image\nresized = cv2.resize(image, dim, interpolation=inter)\n# return the resized image\nreturn resized", "path": "Python/thired-party-haarcascade-mustache-on-face/utils.py", "commit_date": "2019-10-10 00:00:00", "repo_name": "geekcomputers/Python", "stars": 29280, "license": "mit", "language": "python", "size": 36614}
{"docstring": "# to animate bird\n", "func_signal": "def image(self):\n", "code": "if pygame.time.get_ticks() % 500 >= 250:\n    return self._img_wingup\nelse:\n    return self._img_wingdown", "path": "Python/flappyBird_pygame/flappy_bird.py", "commit_date": "2019-10-14 00:00:00", "repo_name": "geekcomputers/Python", "stars": 29280, "license": "mit", "language": "python", "size": 36614}
{"docstring": "\"\"\"\nM\u00e9todo para obter uma lista contendo todas as\nletras do alfabeto e n\u00fameros.\n\"\"\"\n", "func_signal": "def getChars():\n", "code": "chars = []\n\n# Acrescenta \u00e0 lista todas as letras mai\u00fasculas\nfor id_ in range(ord(\"A\"), ord(\"Z\") + 1):\n    chars.append(chr(id_))\n\n# Acrescenta \u00e0 lista todas as letras min\u00fasculas\nfor id_ in range(ord(\"a\"), ord(\"z\") + 1):\n    chars.append(chr(id_))\n\n# Acrescenta \u00e0 lista todos os n\u00fameros\nfor number in range(10):\n    chars.append(str(number))\n\nreturn chars", "path": "Python/BruteForce.py", "commit_date": "2020-10-01 00:00:00", "repo_name": "geekcomputers/Python", "stars": 29280, "license": "mit", "language": "python", "size": 36614}
{"docstring": "# def a function eho updates name in database\n", "func_signal": "def update_address():\n", "code": "def update_address_in_database():\n    new_address = entry_name.get()\n    if len(new_address) != 0:\n        # function in backend that updates name in table\n        backend.update_address_in_bank_table(new_address, acc_no)\n        entry_name.destroy()\n        submit_button.destroy()\n        address_label.destroy()\n    else:\n        tkinter.messagebox.showinfo('Error', 'Please fill address')\n        entry_name.destroy()\n        submit_button.destroy()\n        address_label.destroy()\n\nglobal address_label\n\naddress_label = Label(update_customer_frame, text='Enter new Address:')\naddress_label.grid(row=3, column=1)\nglobal entry_name\nentry_name = Entry(update_customer_frame)\nentry_name.grid(row=3, column=2, padx=2)\nglobal submit_button\nsubmit_button = Button(update_customer_frame, text='Update', command=update_address_in_database)\nsubmit_button.grid(row=3, column=3)", "path": "Python/bank_managment_system/frontend.py", "commit_date": "2019-10-10 00:00:00", "repo_name": "geekcomputers/Python", "stars": 29280, "license": "mit", "language": "python", "size": 36614}
{"docstring": "# Function which closes the window.\n", "func_signal": "def destroy_window():\n", "code": "global top_level\ntop_level.destroy()\ntop_level = None", "path": "Python/notepad/notepad_support.py", "commit_date": "2020-04-24 00:00:00", "repo_name": "geekcomputers/Python", "stars": 29280, "license": "mit", "language": "python", "size": 36614}
{"docstring": "# collision detection\n", "func_signal": "def mask(self):\n", "code": "if pygame.time.get_ticks() % 500 >= 250:\n    return self._mask_wingup\nelse:\n    return self._mask_wingdown", "path": "Python/flappyBird_pygame/flappy_bird.py", "commit_date": "2019-10-14 00:00:00", "repo_name": "geekcomputers/Python", "stars": 29280, "license": "mit", "language": "python", "size": 36614}
{"docstring": "'''Function to create a TF-IDF list of dictionaries for a corpus of docs.\nIf you opt for dumping the data, you can provide a file_path with .tfidfpkl extension(standard made for better understanding)\nand also re-generate a new tfidf list which overrides over an old one by mentioning its path.\n\n@Args:\n--\nfile_names : paths of files to be processed on, you can give many small sized file, rather than one large file.\nprev_file_path : path of old .tfidfpkl file, if available. (default=None)\ndump_path : directory-path where to dump generated lists.(default=None)\n\n@returns:\n--\nidf : a dict of unique words in corpus,with their document frequency as values.\ntf_idf : the generated tf-idf list of dictionaries for mentioned docs.\n'''\n", "func_signal": "def find_tf_idf(file_names=['./../test/testdata'], prev_file_path=None, dump_path=None):\n", "code": "tf_idf = []  # will hold a dict of word_count for every doc(line in a doc in this case)\nidf = {}\n\n# this statement is useful for altering existant tf-idf file and adding new docs in itself.(## memory is now the biggest issue)\nif prev_file_path:\n    print(TAG, 'modifying over exising file.. @', prev_file_path)\n    idf, tf_idf = pickle.load(open(prev_file_path, 'rb'))\n    prev_doc_count = len(idf)\n    prev_corpus_length = len(tf_idf)\n\nfor f in file_names:\n\n    file1 = open(f, 'r')  # never use 'rb' for textual data, it creates something like,  {b'line-inside-the-doc'}\n\n    # create word_count dict for all docs\n    for line in file1:\n        dict = {}\n        # find the amount of doc a word is in\n        for i in set(line.split()):\n            if i in idf:\n                idf[i] += 1\n            else:\n                idf[i] = 1\n        for word in line.split():\n            # find the count of all words in every doc\n            if word not in dict:\n                dict[word] = 1\n            else:\n                dict[word] += 1\n        tf_idf.append(dict)\n    file1.close()\n\n# calculating final TF-IDF values  for all words in all docs(line in a doc in this case)\nfor doc in tf_idf:\n    for key in doc:\n        true_idf = math.log(len(tf_idf) / idf[key])\n        true_tf = doc[key] / len(doc)\n        doc[key] = true_tf * true_idf\n\n# do not get overwhelmed, just for logging the quantity of words that have been processed.\nprint(TAG, 'Total number of unique words in corpus', len(idf),\n      '( ' + paint('++' + str(len(idf) - prev_doc_count), 'g') + ' )' if prev_file_path else '')\nprint(TAG, 'Total number of docs in corpus:', len(tf_idf),\n      '( ' + paint('++' + str(len(tf_idf) - prev_corpus_length), 'g') + ' )' if prev_file_path else '')\n\n# dump if a dir-path is given\nif dump_path:\n    if dump_path[-8:] != 'tfidfpkl': raise Exception(\n        TAG + \"Please provide a .tfidfpkl file_path, it is the standard format of this module.\")\n    pickle.dump((idf, tf_idf), open(dump_path, 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\n    print(TAG, 'Dumping TF-IDF vars @', dump_path)\nreturn idf, tf_idf", "path": "Python/tf_idf_generator.py", "commit_date": "2019-10-10 00:00:00", "repo_name": "geekcomputers/Python", "stars": 29280, "license": "mit", "language": "python", "size": 36614}
{"docstring": "\"\"\"\nPads the input message with zeros so that padded_data has 64 bytes or 512 bits\n\"\"\"\n", "func_signal": "def padding(self):\n", "code": "padding = b'\\x80' + b'\\x00' * (63 - (len(self.data) + 8) % 64)\npadded_data = self.data + padding + struct.pack('>Q', 8 * len(self.data))\nreturn padded_data", "path": "Python/sha1.py", "commit_date": "2019-10-10 00:00:00", "repo_name": "geekcomputers/Python", "stars": 29280, "license": "mit", "language": "python", "size": 36614}
{"docstring": "\"\"\"\nTakes a bytestring-block of length 64, unpacks it to a list of integers and returns a\nlist of 80 integers pafter some bit operations\n\"\"\"\n", "func_signal": "def expand_block(self, block):\n", "code": "w = list(struct.unpack('>16L', block)) + [0] * 64\nfor i in range(16, 80):\n    w[i] = self.rotate((w[i - 3] ^ w[i - 8] ^ w[i - 14] ^ w[i - 16]), 1)\nreturn w", "path": "Python/sha1.py", "commit_date": "2019-10-10 00:00:00", "repo_name": "geekcomputers/Python", "stars": 29280, "license": "mit", "language": "python", "size": 36614}
{"docstring": "'''Utility func, for printing colorful logs in console...\n\n@args:\n--\nstr : String to be modified.\ncolor : color code to which the string will be formed. default is 'r'=RED\n\n@returns:\n--\nstr : final modified string with foreground color as per parameters.\n\n'''\n", "func_signal": "def paint(str, color='r'):\n", "code": "if color in switcher:\n    str = switcher[color] + str + Style.RESET_ALL\nreturn str", "path": "Python/tf_idf_generator.py", "commit_date": "2019-10-10 00:00:00", "repo_name": "geekcomputers/Python", "stars": 29280, "license": "mit", "language": "python", "size": 36614}
{"docstring": "'''Cancels a Queue for data delivery'''\n# pop ts (tickers) and with the result qs (queues)\n", "func_signal": "def cancelQueue(self, q, sendnone=False):\n", "code": "tickerId = self.ts.pop(q, None)\nself.qs.pop(tickerId, None)\n\nself.iscash.pop(tickerId, None)\n\nif sendnone:\n    q.put(None)", "path": "backtrader/backtrader/stores/ibstore.py", "commit_date": "2020-05-28 00:00:00", "repo_name": "mementum/backtrader", "stars": 12685, "license": "gpl-3.0", "language": "python", "size": 22460}
{"docstring": "# Given two dates calculates the smallest possible duration according\n# to the table from the Historical Data API limitations provided by IB\n#\n# Seconds: 'x S' (x: [60, 120, 180, 300, 600, 900, 1200, 1800, 3600,\n#                     7200, 10800, 14400, 28800])\n# Days: 'x D' (x: [1, 2]\n# Weeks: 'x W' (x: [1, 2])\n# Months: 'x M' (x: [1, 11])\n# Years: 'x Y' (x: [1])\n\n", "func_signal": "def histduration(self, dt1, dt2):\n", "code": "td = dt2 - dt1  # get a timedelta for calculations\n\n# First: array of secs\ntsecs = td.total_seconds()\nsecs = [60, 120, 180, 300, 600, 900, 1200, 1800, 3600, 7200, 10800,\n        14400, 28800]\n\nidxsec = bisect.bisect_left(secs, tsecs)\nif idxsec < len(secs):\n    return '{} S'.format(secs[idxsec])\n\ntdextra = bool(td.seconds or td.microseconds)  # over days/weeks\n\n# Next: 1 or 2 days\ndays = td.days + tdextra\nif td.days <= 2:\n    return '{} D'.format(days)\n\n# Next: 1 or 2 weeks\nweeks, d = divmod(td.days, 7)\nweeks += bool(d or tdextra)\nif weeks <= 2:\n    return '{} W'.format(weeks)\n\n# Get references to dt components\ny2, m2, d2 = dt2.year, dt2.month, dt2.day\ny1, m1, d1 = dt1.year, dt1.month, dt2.day\n\nH2, M2, S2, US2 = dt2.hour, dt2.minute, dt2.second, dt2.microsecond\nH1, M1, S1, US1 = dt1.hour, dt1.minute, dt1.second, dt1.microsecond\n\n# Next: 1 -> 11 months (11 incl)\nmonths = (y2 * 12 + m2) - (y1 * 12 + m1) + (\n    (d2, H2, M2, S2, US2) > (d1, H1, M1, S1, US1))\nif months <= 1:  # months <= 11\n    return '1 M'  # return '{} M'.format(months)\nelif months <= 11:\n    return '2 M'  # cap at 2 months to keep the table clean\n\n# Next: years\n# y = y2 - y1 + (m2, d2, H2, M2, S2, US2) > (m1, d1, H1, M1, S1, US1)\n# return '{} Y'.format(y)\n\nreturn '1 Y'  # to keep the table clean", "path": "backtrader/backtrader/stores/ibstore.py", "commit_date": "2020-05-28 00:00:00", "repo_name": "mementum/backtrader", "stars": 12685, "license": "gpl-3.0", "language": "python", "size": 22460}
{"docstring": "'''Cancels an existing MarketData subscription\n\nParams:\n  - q: the Queue returned by reqMktData\n'''\n", "func_signal": "def cancelMktData(self, q):\n", "code": "with self._lock_q:\n    tickerId = self.ts.get(q, None)\n    if tickerId is not None:\n        self.conn.cancelMktData(tickerId)\n\n    self.cancelQueue(q, True)", "path": "backtrader/backtrader/stores/ibstore.py", "commit_date": "2020-05-28 00:00:00", "repo_name": "mementum/backtrader", "stars": 12685, "license": "gpl-3.0", "language": "python", "size": 22460}
{"docstring": "# The isConnected method is available through __getattr__ indirections\n# and may not be present, which indicates that no connection has been\n# made because the subattribute sender has not yet been created, hence\n# the check for the AttributeError exception\n", "func_signal": "def connected(self):\n", "code": "try:\n    return self.conn.isConnected()\nexcept AttributeError:\n    pass\n\nreturn False  # non-connected (including non-initialized)", "path": "backtrader/backtrader/stores/ibstore.py", "commit_date": "2020-05-28 00:00:00", "repo_name": "mementum/backtrader", "stars": 12685, "license": "gpl-3.0", "language": "python", "size": 22460}
{"docstring": "'''Creates ticker/Queue for data delivery to a data feed'''\n", "func_signal": "def getTickerQueue(self, start=False):\n", "code": "q = queue.Queue()\nif start:\n    q.put(None)\n    return q\n\nwith self._lock_q:\n    tickerId = self.nextTickerId()\n    self.qs[tickerId] = q  # can be managed from other thread\n    self.ts[q] = tickerId\n    self.iscash[tickerId] = False\n\nreturn tickerId, q", "path": "backtrader/backtrader/stores/ibstore.py", "commit_date": "2020-05-28 00:00:00", "repo_name": "mementum/backtrader", "stars": 12685, "license": "gpl-3.0", "language": "python", "size": 22460}
{"docstring": "'''Reuses queue for tickerId, returning the new tickerId and q'''\n", "func_signal": "def reuseQueue(self, tickerId):\n", "code": "with self._lock_q:\n    # Invalidate tickerId in qs (where it is a key)\n    q = self.qs.pop(tickerId, None)  # invalidate old\n    iscash = self.iscash.pop(tickerId, None)\n\n    # Update ts: q -> ticker\n    tickerId = self.nextTickerId()  # get new tickerId\n    self.ts[q] = tickerId  # Update ts: q -> tickerId\n    self.qs[tickerId] = q  # Update qs: tickerId -> q\n    self.iscash[tickerId] = iscash\n\nreturn tickerId, q", "path": "backtrader/backtrader/stores/ibstore.py", "commit_date": "2020-05-28 00:00:00", "repo_name": "mementum/backtrader", "stars": 12685, "license": "gpl-3.0", "language": "python", "size": 22460}
{"docstring": "'''Receives the events of a historical data request'''\n# For multi-tiered downloads we'd need to rebind the queue to a new\n# tickerId (in case tickerIds are not reusable) and instead of putting\n# None, issue a new reqHistData with the new data and move formward\n", "func_signal": "def historicalData(self, msg):\n", "code": "tickerId = msg.reqId\nq = self.qs[tickerId]\nif msg.date.startswith('finished-'):\n    self.histfmt.pop(tickerId, None)\n    self.histsend.pop(tickerId, None)\n    self.histtz.pop(tickerId, None)\n    kargs = self.histexreq.pop(tickerId, None)\n    if kargs is not None:\n        self.reqHistoricalDataEx(tickerId=tickerId, **kargs)\n        return\n\n    msg.date = None\n    self.cancelQueue(q)\nelse:\n    dtstr = msg.date  # Format when string req: YYYYMMDD[  HH:MM:SS]\n    if self.histfmt[tickerId]:\n        sessionend = self.histsend[tickerId]\n        dt = datetime.strptime(dtstr, '%Y%m%d')\n        dteos = datetime.combine(dt, sessionend)\n        tz = self.histtz[tickerId]\n        if tz:\n            dteostz = tz.localize(dteos)\n            dteosutc = dteostz.astimezone(UTC).replace(tzinfo=None)\n            # When requesting for example daily bars, the current day\n            # will be returned with the already happened data. If the\n            # session end were added, the new ticks wouldn't make it\n            # through, because they happen before the end of time\n        else:\n            dteosutc = dteos\n\n        if dteosutc <= datetime.utcnow():\n            dt = dteosutc\n\n        msg.date = dt\n    else:\n        msg.date = datetime.utcfromtimestamp(long(dtstr))\n\nq.put(msg)", "path": "backtrader/backtrader/stores/ibstore.py", "commit_date": "2020-05-28 00:00:00", "repo_name": "mementum/backtrader", "stars": 12685, "license": "gpl-3.0", "language": "python", "size": 22460}
{"docstring": "'''Cancels an existing MarketData subscription\n\nParams:\n  - q: the Queue returned by reqMktData\n'''\n", "func_signal": "def cancelRealTimeBars(self, q):\n", "code": "with self._lock_q:\n    tickerId = self.ts.get(q, None)\n    if tickerId is not None:\n        self.conn.cancelRealTimeBars(tickerId)\n\n    self.cancelQueue(q, True)", "path": "backtrader/backtrader/stores/ibstore.py", "commit_date": "2020-05-28 00:00:00", "repo_name": "mementum/backtrader", "stars": 12685, "license": "gpl-3.0", "language": "python", "size": 22460}
{"docstring": "# This method must be an invariant in that it can be called several\n# times from the same source and must be consistent. An exampler would\n# be 5 datas which are being received simultaneously and all request a\n# reconnect\n\n# Policy:\n#  - if dontreconnect has been set, no option to connect is possible\n#  - check connection and use the absence of isConnected as signal of\n#    first ever connection (add 1 to retries too)\n#  - Calculate the retries (forever or not)\n#  - Try to connct\n#  - If achieved and fromstart is false, the datas will be\n#    re-kickstarted to recreate the subscription\n", "func_signal": "def reconnect(self, fromstart=False, resub=False):\n", "code": "firstconnect = False\ntry:\n    if self.conn.isConnected():\n        if resub:\n            self.startdatas()\n        return True  # nothing to do\nexcept AttributeError:\n    # Not connected, several __getattr__ indirections to\n    # self.conn.sender.client.isConnected\n    firstconnect = True\n\nif self.dontreconnect:\n    return False\n\n# This is only invoked from the main thread by datas and therefore no\n# lock is needed to control synchronicity to it\nretries = self.p.reconnect\nif retries >= 0:\n    retries += firstconnect\n\nwhile retries < 0 or retries:\n    if not firstconnect:\n        time.sleep(self.p.timeout)\n\n    firstconnect = False\n\n    if self.conn.connect():\n        if not fromstart or resub:\n            self.startdatas()\n        return True  # connection successful\n\n    if retries > 0:\n        retries -= 1\n\nself.dontreconnect = True\nreturn False  # connection/reconnection failed", "path": "backtrader/backtrader/stores/ibstore.py", "commit_date": "2020-05-28 00:00:00", "repo_name": "mementum/backtrader", "stars": 12685, "license": "gpl-3.0", "language": "python", "size": 22460}
{"docstring": "# get a ticker/queue for identification/data delivery\n", "func_signal": "def reqContractDetails(self, contract):\n", "code": "tickerId, q = self.getTickerQueue()\nself.conn.reqContractDetails(tickerId, contract)\nreturn q", "path": "backtrader/backtrader/stores/ibstore.py", "commit_date": "2020-05-28 00:00:00", "repo_name": "mementum/backtrader", "stars": 12685, "license": "gpl-3.0", "language": "python", "size": 22460}
{"docstring": "# 100-199 Order/Data/Historical related\n# 200-203 tickerId and Order Related\n# 300-399 A mix of things: orders, connectivity, tickers, misc errors\n# 400-449 Seem order related again\n# 500-531 Connectivity/Communication Errors\n# 10000-100027 Mix of special orders/routing\n# 1100-1102 TWS connectivy to the outside\n# 1300- Socket dropped in client-TWS communication\n# 2100-2110 Informative about Data Farm status (id=-1)\n\n# All errors are logged to the environment (cerebro), because many\n# errors in Interactive Brokers are actually informational and many may\n# actually be of interest to the user\n", "func_signal": "def error(self, msg):\n", "code": "if not self.p.notifyall:\n    self.notifs.put((msg, tuple(msg.values()), dict(msg.items())))\n\n# Manage those events which have to do with connection\nif msg.errorCode is None:\n    # Usually received as an error in connection of just before disconn\n    pass\nelif msg.errorCode in [200, 203, 162, 320, 321, 322]:\n    # cdetails 200 security not found, notify over right queue\n    # cdetails 203 security not allowed for acct\n    try:\n        q = self.qs[msg.id]\n    except KeyError:\n        pass  # should not happend but it can\n    else:\n        self.cancelQueue(q, True)\n\nelif msg.errorCode in [354, 420]:\n    # 354 no subscription, 420 no real-time bar for contract\n    # the calling data to let the data know ... it cannot resub\n    try:\n        q = self.qs[msg.id]\n    except KeyError:\n        pass  # should not happend but it can\n    else:\n        q.put(-msg.errorCode)\n        self.cancelQueue(q)\n\nelif msg.errorCode == 10225:\n    # 10225-Bust event occurred, current subscription is deactivated.\n    # Please resubscribe real-time bars immediately.\n    try:\n        q = self.qs[msg.id]\n    except KeyError:\n        pass  # should not happend but it can\n    else:\n        q.put(-msg.errorCode)\n\nelif msg.errorCode == 326:  # not recoverable, clientId in use\n    self.dontreconnect = True\n    self.conn.disconnect()\n    self.stopdatas()\n\nelif msg.errorCode == 502:\n    # Cannot connect to TWS: port, config not open, tws off (504 then)\n    self.conn.disconnect()\n    self.stopdatas()\n\nelif msg.errorCode == 504:  # Not Connected for data op\n    # Once for each data\n    pass  # don't need to manage it\n\nelif msg.errorCode == 1300:\n    # TWS has been closed. The port for a new connection is there\n    # newport = int(msg.errorMsg.split('-')[-1])  # bla bla bla -7496\n    self.conn.disconnect()\n    self.stopdatas()\n\nelif msg.errorCode == 1100:\n    # Connection lost - Notify ... datas will wait on the queue\n    # with no messages arriving\n    for q in self.ts:  # key: queue -> ticker\n        q.put(-msg.errorCode)\n\nelif msg.errorCode == 1101:\n    # Connection restored and tickerIds are gone\n    for q in self.ts:  # key: queue -> ticker\n        q.put(-msg.errorCode)\n\nelif msg.errorCode == 1102:\n    # Connection restored and tickerIds maintained\n    for q in self.ts:  # key: queue -> ticker\n        q.put(-msg.errorCode)\n\nelif msg.errorCode < 500:\n    # Given the myriad of errorCodes, start by assuming is an order\n    # error and if not, the checks there will let it go\n    if msg.id < self.REQIDBASE:\n        if self.broker is not None:\n            self.broker.push_ordererror(msg)\n    else:\n        # Cancel the queue if a \"data\" reqId error is given: sanity\n        q = self.qs[msg.id]\n        self.cancelQueue(q, True)", "path": "backtrader/backtrader/stores/ibstore.py", "commit_date": "2020-05-28 00:00:00", "repo_name": "mementum/backtrader", "stars": 12685, "license": "gpl-3.0", "language": "python", "size": 22460}
{"docstring": "'''Calculate a duration in between 2 datetimes. Returns single size'''\n", "func_signal": "def calcduration(self, dtbegin, dtend):\n", "code": "duration, sizes = self._calcdurations(dtbegin, dtend)\nreturn duration, sizes[0]", "path": "backtrader/backtrader/stores/ibstore.py", "commit_date": "2020-05-28 00:00:00", "repo_name": "mementum/backtrader", "stars": 12685, "license": "gpl-3.0", "language": "python", "size": 22460}
{"docstring": "# Transforms a RTVolume timestamp to a datetime object\n", "func_signal": "def _ts2dt(tstamp=None):\n", "code": "if not tstamp:\n    return datetime.utcnow()\n\nsec, msec = divmod(long(tstamp), 1000)\nusec = msec * 1000\nreturn datetime.utcfromtimestamp(sec).replace(microsecond=usec)", "path": "backtrader/backtrader/stores/ibstore.py", "commit_date": "2020-05-28 00:00:00", "repo_name": "mementum/backtrader", "stars": 12685, "license": "gpl-3.0", "language": "python", "size": 22460}
{"docstring": "'''Creates a MarketData subscription\n\nParams:\n  - contract: a ib.ext.Contract.Contract intance\n\nReturns:\n  - a Queue the client can wait on to receive a RTVolume instance\n'''\n# get a ticker/queue for identification/data delivery\n", "func_signal": "def reqMktData(self, contract, what=None):\n", "code": "tickerId, q = self.getTickerQueue()\nticks = '233'  # request RTVOLUME tick delivered over tickString\n\nif contract.m_secType in ['CASH', 'CFD']:\n    self.iscash[tickerId] = True\n    ticks = ''  # cash markets do not get RTVOLUME\n    if what == 'ASK':\n        self.iscash[tickerId] = 2\n\n# q.put(None)  # to kickstart backfilling\n# Can request 233 also for cash ... nothing will arrive\nself.conn.reqMktData(tickerId, contract, bytes(ticks), False)\nreturn q", "path": "backtrader/backtrader/stores/ibstore.py", "commit_date": "2020-05-28 00:00:00", "repo_name": "mementum/backtrader", "stars": 12685, "license": "gpl-3.0", "language": "python", "size": 22460}
{"docstring": "# Lock access to the position dicts. This is called from main thread\n# and updates could be happening in the background\n", "func_signal": "def getposition(self, contract, clone=False):\n", "code": "with self._lock_pos:\n    position = self.positions[contract.m_conId]\n    if clone:\n        return copy(position)\n\n    return position", "path": "backtrader/backtrader/stores/ibstore.py", "commit_date": "2020-05-28 00:00:00", "repo_name": "mementum/backtrader", "stars": 12685, "license": "gpl-3.0", "language": "python", "size": 22460}
{"docstring": "'''Returns the net liquidation value sent by TWS during regular updates\nWaits for at least 1 successful download\n\nIf ``account`` is ``None`` then a dictionary with accounts as keys will\nbe returned containing all accounts\n\nIf account is specified or the system has only 1 account the dictionary\ncorresponding to that account is returned\n'''\n# Wait for at least 1 account update download to have been finished\n# before the value can be returned to the calling client\n", "func_signal": "def get_acc_value(self, account=None):\n", "code": "if self.connected():\n    self._event_accdownload.wait()\n# Lock access to acc_cash to avoid an event intefering\nwith self._lock_accupd:\n    if account is None:\n        # wait for the managedAccount Messages\n        if self.connected():\n            self._event_managed_accounts.wait()\n\n        if not self.managed_accounts:\n            return float()\n\n        elif len(self.managed_accounts) > 1:\n            return sum(self.acc_value.values())\n\n        # Only 1 account, fall through to return only 1\n        account = self.managed_accounts[0]\n\n    try:\n        return self.acc_value[account]\n    except KeyError:\n        pass\n\n    return float()", "path": "backtrader/backtrader/stores/ibstore.py", "commit_date": "2020-05-28 00:00:00", "repo_name": "mementum/backtrader", "stars": 12685, "license": "gpl-3.0", "language": "python", "size": 22460}
{"docstring": "# Signals the end of an account update\n# the event indicates it's over. It's only false once, and can be used\n# to find out if it has at least been downloaded once\n", "func_signal": "def accountDownloadEnd(self, msg):\n", "code": "self._event_accdownload.set()\nif False:\n    if self.port_update:\n        self.broker.push_portupdate()\n\n        self.port_update = False", "path": "backtrader/backtrader/stores/ibstore.py", "commit_date": "2020-05-28 00:00:00", "repo_name": "mementum/backtrader", "stars": 12685, "license": "gpl-3.0", "language": "python", "size": 22460}
{"docstring": "# kickstrat datas, not returning until all of them have been done\n", "func_signal": "def startdatas(self):\n", "code": "ts = list()\nfor data in self.datas:\n    t = threading.Thread(target=data.reqdata)\n    t.start()\n    ts.append(t)\n\nfor t in ts:\n    t.join()", "path": "backtrader/backtrader/stores/ibstore.py", "commit_date": "2020-05-28 00:00:00", "repo_name": "mementum/backtrader", "stars": 12685, "license": "gpl-3.0", "language": "python", "size": 22460}
{"docstring": "'''Receives x seconds Real Time Bars (at the time of writing only 5\nseconds are supported)\n\nNot valid for cash markets\n'''\n# Get a naive localtime object\n", "func_signal": "def realtimeBar(self, msg):\n", "code": "msg.time = datetime.utcfromtimestamp(float(msg.time))\nself.qs[msg.reqId].put(msg)", "path": "backtrader/backtrader/stores/ibstore.py", "commit_date": "2020-05-28 00:00:00", "repo_name": "mementum/backtrader", "stars": 12685, "license": "gpl-3.0", "language": "python", "size": 22460}
{"docstring": "# Lock access to the position dicts. This is called in sub-thread and\n# can kick in at any time\n", "func_signal": "def updatePortfolio(self, msg):\n", "code": "with self._lock_pos:\n    if not self._event_accdownload.is_set():  # 1st event seen\n        position = Position(msg.position, msg.averageCost)\n        self.positions[msg.contract.m_conId] = position\n    else:\n        position = self.positions[msg.contract.m_conId]\n        if not position.fix(msg.position, msg.averageCost):\n            err = ('The current calculated position and '\n                   'the position reported by the broker do not match. '\n                   'Operation can continue, but the trades '\n                   'calculated in the strategy may be wrong')\n\n            self.notifs.put((err, (), {}))\n\n        # Flag signal to broker at the end of account download\n        # self.port_update = True\n        self.broker.push_portupdate()", "path": "backtrader/backtrader/stores/ibstore.py", "commit_date": "2020-05-28 00:00:00", "repo_name": "mementum/backtrader", "stars": 12685, "license": "gpl-3.0", "language": "python", "size": 22460}
{"docstring": "'''\nPerform CORAL, then predict using 1NN classifier\n:param Xs: ns * n_feature, source feature\n:param Ys: ns * 1, source label\n:param Xt: nt * n_feature, target feature\n:param Yt: nt * 1, target label\n:return: Accuracy and predicted labels of target domain\n'''\n", "func_signal": "def fit_predict(self, Xs, Ys, Xt, Yt):\n", "code": "Xs_new = self.fit(Xs, Xt)\nclf = sklearn.neighbors.KNeighborsClassifier(n_neighbors=1)\nclf.fit(Xs_new, Ys.ravel())\ny_pred = clf.predict(Xt)\nacc = sklearn.metrics.accuracy_score(Yt, y_pred)\nreturn acc, y_pred", "path": "transferlearning/code/traditional/CORAL/CORAL.py", "commit_date": "2020-09-30 00:00:00", "repo_name": "jindongwang/transferlearning", "stars": 12665, "license": "mit", "language": "python", "size": 36893}
{"docstring": "#total_progress_bar = tqdm.tqdm(desc='Train iter', total=args.epochs)\n", "func_signal": "def train(epoch, model, source_loader, target_loader):\n", "code": "LEARNING_RATE = args.lr / math.pow((1 + 10 * (epoch - 1) / args.epochs), 0.75)\nif args.diff_lr:\n    optimizer = torch.optim.SGD([\n        {'params': model.sharedNet.parameters()},\n        {'params': model.bottleneck.parameters()},\n        {'params': model.domain_classifier.parameters()},\n        {'params': model.dcis.parameters()},\n        {'params': model.source_fc.parameters(), 'lr': LEARNING_RATE},\n    ], lr=LEARNING_RATE / 10, momentum=args.momentum, weight_decay=args.l2_decay)\nelse:\n    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=args.momentum,weight_decay = args.l2_decay)\n\nprint_learning_rate(optimizer)\n\nglobal D_M, D_C, MU\nmodel.train()\nlen_dataloader = len(source_loader)\nDEV = DEVICE\n\nd_m = 0\nd_c = 0\n''' update mu per epoch '''\nif D_M==0 and D_C==0 and MU==0:\n    MU = 0.5\nelse:\n    D_M = D_M/len_dataloader\n    D_C = D_C/len_dataloader\n    MU = 1 - D_M/(D_M + D_C)\n\nfor batch_idx, (source_data, source_label) in tqdm.tqdm(enumerate(source_loader),\n                                total=len_dataloader,\n                                desc='Train epoch = {}'.format(epoch), ncols=80, leave=False):\n    p = float(batch_idx+1 + epoch * len_dataloader) / args.epochs / len_dataloader\n    alpha = 2. / (1. + np.exp(-10 * p)) - 1\n    optimizer.zero_grad()\n    source_data, source_label = source_data.to(DEVICE), source_label.to(DEVICE)\n    for target_data, target_label in target_loader:\n        target_data, target_label = target_data.to(DEVICE), target_label.to(DEVICE)\n        break\n    out = model(source_data, target_data, source_label, DEV, alpha)\n    s_output, s_domain_output, t_domain_output = out[0],out[1],out[2]\n    s_out = out[3]\n    t_out = out[4]\n\n    #global loss\n    sdomain_label = torch.zeros(args.batch_size).long().to(DEV)\n    err_s_domain = F.nll_loss(F.log_softmax(s_domain_output, dim=1), sdomain_label)\n    tdomain_label = torch.ones(args.batch_size).long().to(DEV)\n    err_t_domain = F.nll_loss(F.log_softmax(t_domain_output, dim=1), tdomain_label)\n\n    #local loss\n    loss_s = 0.0\n    loss_t = 0.0\n    tmpd_c = 0\n    for i in range(args.num_class):\n        loss_si = F.nll_loss(F.log_softmax(s_out[i], dim=1), sdomain_label)\n        loss_ti = F.nll_loss(F.log_softmax(t_out[i], dim=1), tdomain_label)\n        loss_s += loss_si\n        loss_t += loss_ti\n        tmpd_c += 2 * (1 - 2 * (loss_si + loss_ti))\n    tmpd_c /= args.num_class\n\n    d_c = d_c + tmpd_c.cpu().item()\n\n    global_loss = 0.05*(err_s_domain + err_t_domain)\n    local_loss = 0.01*(loss_s + loss_t)\n\n    d_m = d_m + 2 * (1 - 2 * global_loss.cpu().item())\n\n    join_loss = (1 - MU) * global_loss + MU * local_loss\n    soft_loss = F.nll_loss(F.log_softmax(s_output, dim=1), source_label)\n    if args.gamma == 1:\n        gamma = 2 / (1 + math.exp(-10 * (epoch) / args.epochs)) - 1\n    if args.gamma == 2:\n        gamma = epoch /args.epochs\n    loss = soft_loss + join_loss\n    loss.backward()\n    optimizer.step()\n\n    if batch_idx % args.log_interval == 0:\n        print('\\nLoss: {:.6f},  label_Loss: {:.6f},  join_Loss: {:.6f}, global_Loss:{:.4f}, local_Loss:{:.4f}'.format(\n            loss.item(), soft_loss.item(), join_loss.item(), global_loss.item(), local_loss.item()))\n    #total_progress_bar.update(1)\nD_M = np.copy(d_m).item()\nD_C = np.copy(d_c).item()", "path": "transferlearning/code/deep/DAAN/train.py", "commit_date": "2020-01-31 00:00:00", "repo_name": "jindongwang/transferlearning", "stars": 12665, "license": "mit", "language": "python", "size": 36893}
{"docstring": "\"\"\"\nZ-Normaliza\n\"\"\"\n", "func_signal": "def znorm(self, data):\n", "code": "mu = np.average(data, axis=0)\nstd = np.std(data, axis=0)\ndata = (data - mu) / std\nreturn data, mu, std", "path": "transferlearning/code/traditional/GFK/GFK.py", "commit_date": "2018-11-15 00:00:00", "repo_name": "jindongwang/transferlearning", "stars": 12665, "license": "mit", "language": "python", "size": 36893}
{"docstring": "'''\nPerform CORAL on the source domain features\n:param Xs: ns * n_feature, source feature\n:param Xt: nt * n_feature, target feature\n:return: New source domain features\n'''\n", "func_signal": "def fit(self, Xs, Xt):\n", "code": "cov_src = np.cov(Xs.T) + np.eye(Xs.shape[1])\ncov_tar = np.cov(Xt.T) + np.eye(Xt.shape[1])\nA_coral = np.dot(scipy.linalg.fractional_matrix_power(cov_src, -0.5),\n                 scipy.linalg.fractional_matrix_power(cov_tar, 0.5))\nXs_new = np.real(np.dot(Xs, A_coral))\nreturn Xs_new", "path": "transferlearning/code/traditional/CORAL/CORAL.py", "commit_date": "2020-09-30 00:00:00", "repo_name": "jindongwang/transferlearning", "stars": 12665, "license": "mit", "language": "python", "size": 36893}
{"docstring": "'''\nFit and use 1NN to classify\n:param Xs: ns * n_feature, source feature\n:param Ys: ns * 1, source label\n:param Xt: nt * n_feature, target feature\n:param Yt: nt * 1, target label\n:return: Accuracy, predicted labels of target domain, and G\n'''\n", "func_signal": "def fit_predict(self, Xs, Ys, Xt, Yt):\n", "code": "G, Xs_new, Xt_new = self.fit(Xs, Xt)\nclf = KNeighborsClassifier(n_neighbors=1)\nclf.fit(Xs_new, Ys.ravel())\ny_pred = clf.predict(Xt_new)\nacc = np.mean(y_pred == Yt.ravel())\nreturn acc, y_pred, G", "path": "transferlearning/code/traditional/GFK/GFK.py", "commit_date": "2018-11-15 00:00:00", "repo_name": "jindongwang/transferlearning", "stars": 12665, "license": "mit", "language": "python", "size": 36893}
{"docstring": "'''\nTransform and Predict\n:param Xs: ns * n_feature, source feature\n:param Ys: ns * 1, source label\n:param Xt: nt * n_feature, target feature\n:param Yt: nt * 1, target label\n:return: acc, y_pred, list_acc\n'''\n", "func_signal": "def fit_predict(self, Xs, Ys, Xt, Yt):\n", "code": "gfk = GFK.GFK(dim=self.dim)\n_, Xs_new, Xt_new = gfk.fit(Xs, Xt)\nXs_new, Xt_new = Xs_new.T, Xt_new.T\nX = np.hstack((Xs_new, Xt_new))\nn, m = Xs_new.shape[1], Xt_new.shape[1]\nC = len(np.unique(Ys))\nlist_acc = []\nYY = np.zeros((n, C))\nfor c in range(1, C + 1):\n    ind = np.where(Ys == c)\n    YY[ind, c - 1] = 1\nYY = np.vstack((YY, np.zeros((m, C))))\nYY[0, 1:] = 0\n\nX /= np.linalg.norm(X, axis=0)\nL = 0  # Graph Laplacian is on the way...\nknn_clf = KNeighborsClassifier(n_neighbors=1)\nknn_clf.fit(X[:, :n].T, Ys.ravel())\nCls = knn_clf.predict(X[:, n:].T)\nK = kernel(self.kernel_type, X, X2=None, gamma=self.gamma)\nE = np.diagflat(np.vstack((np.ones((n, 1)), np.zeros((m, 1)))))\nfor t in range(1, self.T + 1):\n    mu = self.estimate_mu(Xs_new.T, Ys, Xt_new.T, Cls)\n    e = np.vstack((1 / n * np.ones((n, 1)), -1 / m * np.ones((m, 1))))\n    M = e * e.T * C\n    N = 0\n    for c in range(1, C + 1):\n        e = np.zeros((n + m, 1))\n        tt = Ys == c\n        e[np.where(tt == True)] = 1 / len(Ys[np.where(Ys == c)])\n        yy = Cls == c\n        ind = np.where(yy == True)\n        inds = [item + n for item in ind]\n        e[tuple(inds)] = -1 / len(Cls[np.where(Cls == c)])\n        e[np.isinf(e)] = 0\n        N += np.dot(e, e.T)\n    M = (1 - mu) * M + mu * N\n    M /= np.linalg.norm(M, 'fro')\n    left = np.dot(E + self.lamb * M + self.rho * L, K) + self.eta * np.eye(n + m, n + m)\n    Beta = np.dot(np.linalg.inv(left), np.dot(E, YY))\n    F = np.dot(K, Beta)\n    Cls = np.argmax(F, axis=1) + 1\n    Cls = Cls[n:]\n    acc = np.mean(Cls == Yt.ravel())\n    list_acc.append(acc)\n    print('MEDA iteration [{}/{}]: mu={:.2f}, Acc={:.4f}'.format(t, self.T, mu, acc))\nreturn acc, Cls, list_acc", "path": "transferlearning/code/traditional/MEDA/MEDA.py", "commit_date": "2019-04-21 00:00:00", "repo_name": "jindongwang/transferlearning", "stars": 12665, "license": "mit", "language": "python", "size": 36893}
{"docstring": "'''\nObtain the kernel G\n:param Xs: ns * n_feature, source feature\n:param Xt: nt * n_feature, target feature\n:param norm_inputs: normalize the inputs or not\n:return: GFK kernel G\n'''\n", "func_signal": "def fit(self, Xs, Xt, norm_inputs=None):\n", "code": "if norm_inputs:\n    source, mu_source, std_source = self.znorm(Xs)\n    target, mu_target, std_target = self.znorm(Xt)\nelse:\n    mu_source = np.zeros(shape=(Xs.shape[1]))\n    std_source = np.ones(shape=(Xs.shape[1]))\n    mu_target = np.zeros(shape=(Xt.shape[1]))\n    std_target = np.ones(shape=(Xt.shape[1]))\n    source = Xs\n    target = Xt\n\nPs = self.train_pca(source, mu_source, std_source, 0.99)\nPt = self.train_pca(target, mu_target, std_target, 0.99)\nPs = np.hstack((Ps.weights, scipy.linalg.null_space(Ps.weights.T)))\nPt = Pt.weights[:, :self.dim]\nN = Ps.shape[1]\ndim = Pt.shape[1]\n\n# Principal angles between subspaces\nQPt = np.dot(Ps.T, Pt)\n\n# [V1,V2,V,Gam,Sig] = gsvd(QPt(1:dim,:), QPt(dim+1:end,:));\nA = QPt[0:dim, :].copy()\nB = QPt[dim:, :].copy()\n\n# Equation (2)\n[V1, V2, V, Gam, Sig] = bob.math.gsvd(A, B)\nV2 = -V2\n\n# Some sanity checks with the GSVD\nI = np.eye(V1.shape[1])\nI_check = np.dot(Gam.T, Gam) + np.dot(Sig.T, Sig)\nassert np.sum(abs(I - I_check)) < 1e-10\n\ntheta = np.arccos(np.diagonal(Gam))\n\n# Equation (6)\nB1 = np.diag(0.5 * (1 + (np.sin(2 * theta) / (2. * np.maximum\n(theta, 1e-20)))))\nB2 = np.diag(0.5 * ((np.cos(2 * theta) - 1) / (2 * np.maximum(\n    theta, self.eps))))\nB3 = B2\nB4 = np.diag(0.5 * (1 - (np.sin(2 * theta) / (2. * np.maximum\n(theta, self.eps)))))\n\n# Equation (9) of the suplementary matetial\ndelta1_1 = np.hstack((V1, np.zeros(shape=(dim, N - dim))))\ndelta1_2 = np.hstack((np.zeros(shape=(N - dim, dim)), V2))\ndelta1 = np.vstack((delta1_1, delta1_2))\n\ndelta2_1 = np.hstack((B1, B2, np.zeros(shape=(dim, N - 2 * dim))))\ndelta2_2 = np.hstack((B3, B4, np.zeros(shape=(dim, N - 2 * dim))))\ndelta2_3 = np.zeros(shape=(N - 2 * dim, N))\ndelta2 = np.vstack((delta2_1, delta2_2, delta2_3))\n\ndelta3_1 = np.hstack((V1, np.zeros(shape=(dim, N - dim))))\ndelta3_2 = np.hstack((np.zeros(shape=(N - dim, dim)), V2))\ndelta3 = np.vstack((delta3_1, delta3_2)).T\n\ndelta = np.dot(np.dot(delta1, delta2), delta3)\nG = np.dot(np.dot(Ps, delta), Ps.T)\nsqG = scipy.real(scipy.linalg.fractional_matrix_power(G, 0.5))\nXs_new, Xt_new = np.dot(sqG, Xs.T).T, np.dot(sqG, Xt.T).T\nreturn G, Xs_new, Xt_new", "path": "transferlearning/code/traditional/GFK/GFK.py", "commit_date": "2018-11-15 00:00:00", "repo_name": "jindongwang/transferlearning", "stars": 12665, "license": "mit", "language": "python", "size": 36893}
{"docstring": "'''\nInit func\n:param kernel_type: kernel, values: 'primal' | 'linear' | 'rbf'\n:param dim: dimension after transfer\n:param lamb: lambda value in equation\n:param rho: rho in equation\n:param eta: eta in equation\n:param p: number of neighbors\n:param gamma: kernel bandwidth for rbf kernel\n:param T: iteration number\n'''\n", "func_signal": "def __init__(self, kernel_type='primal', dim=30, lamb=1, rho=1.0, eta=0.1, p=10, gamma=1, T=10):\n", "code": "self.kernel_type = kernel_type\nself.dim = dim\nself.lamb = lamb\nself.rho = rho\nself.eta = eta\nself.gamma = gamma\nself.p = p\nself.T = T", "path": "transferlearning/code/traditional/MEDA/MEDA.py", "commit_date": "2019-04-21 00:00:00", "repo_name": "jindongwang/transferlearning", "stars": 12665, "license": "mit", "language": "python", "size": 36893}
{"docstring": "#model = ResNet.DANNet(num_classes=31)\n#model = torch.load('./models/alex/model_20.pkl')\n", "func_signal": "def print_model_parm_nums(model):\n", "code": "total = sum([param.nelement() for param in model.parameters()])\nprint('  + Number of params: %.2fM' % (total / 1e6))", "path": "transferlearning/code/deep/TCP/tools.py", "commit_date": "2019-04-06 00:00:00", "repo_name": "jindongwang/transferlearning", "stars": 12665, "license": "mit", "language": "python", "size": 36893}
{"docstring": "'''\nInit func\n:param dim: dimension after GFK\n'''\n", "func_signal": "def __init__(self, dim=20):\n", "code": "self.dim = dim\nself.eps = 1e-20", "path": "transferlearning/code/traditional/GFK/GFK.py", "commit_date": "2018-11-15 00:00:00", "repo_name": "jindongwang/transferlearning", "stars": 12665, "license": "mit", "language": "python", "size": 36893}
{"docstring": "\"\"\"\nGet the best value for the number of subspaces\nFor more details, read section 3.4 of the paper.\n**Parameters**\n  Ps: Source subspace\n  Pt: Target subspace\n  Pst: Source + Target subspace\n\"\"\"\n\n", "func_signal": "def subspace_disagreement_measure(self, Ps, Pt, Pst):\n", "code": "def compute_angles(A, B):\n    _, S, _ = np.linalg.svd(np.dot(A.T, B))\n    S[np.where(np.isclose(S, 1, atol=self.eps) == True)[0]] = 1\n    return np.arccos(S)\n\nmax_d = min(Ps.shape[1], Pt.shape[1], Pst.shape[1])\nalpha_d = compute_angles(Ps, Pst)\nbeta_d = compute_angles(Pt, Pst)\nd = 0.5 * (np.sin(alpha_d) + np.sin(beta_d))\nreturn np.argmax(d)", "path": "transferlearning/code/traditional/GFK/GFK.py", "commit_date": "2018-11-15 00:00:00", "repo_name": "jindongwang/transferlearning", "stars": 12665, "license": "mit", "language": "python", "size": 36893}
{"docstring": "'''\ntransform the origianl data by add new features\nParam X: original data\noutput x_new: X with new features\n'''\n", "func_signal": "def transform(self, X):\n", "code": "X_new = np.concatenate((np.dot(X, self.W),X), axis=1)\nreturn X_new", "path": "transferlearning/code/traditional/SCL.py", "commit_date": "2019-12-14 00:00:00", "repo_name": "jindongwang/transferlearning", "stars": 12665, "license": "mit", "language": "python", "size": 36893}
{"docstring": "'''\nTransform Xs and Xt\n:param Xs: ns * n_feature, source feature\n:param Xt: nt * n_feature, target feature\n:return: Xs_new and Xt_new after TCA\n'''\n", "func_signal": "def fit(self, Xs, Xt):\n", "code": "X = np.hstack((Xs.T, Xt.T))\nX /= np.linalg.norm(X, axis=0)\nm, n = X.shape\nns, nt = len(Xs), len(Xt)\ne = np.vstack((1 / ns * np.ones((ns, 1)), -1 / nt * np.ones((nt, 1))))\nM = e * e.T\nM = M / np.linalg.norm(M, 'fro')\nH = np.eye(n) - 1 / n * np.ones((n, n))\nK = kernel(self.kernel_type, X, None, gamma=self.gamma)\nn_eye = m if self.kernel_type == 'primal' else n\na, b = np.linalg.multi_dot([K, M, K.T]) + self.lamb * np.eye(n_eye), np.linalg.multi_dot([K, H, K.T])\nw, V = scipy.linalg.eig(a, b)\nind = np.argsort(w)\nA = V[:, ind[:self.dim]]\nZ = np.dot(A.T, K)\nZ /= np.linalg.norm(Z, axis=0)\nXs_new, Xt_new = Z[:, :ns].T, Z[:, ns:].T\nreturn Xs_new, Xt_new", "path": "transferlearning/code/traditional/TCA/TCA.py", "commit_date": "2019-04-21 00:00:00", "repo_name": "jindongwang/transferlearning", "stars": 12665, "license": "mit", "language": "python", "size": 36893}
{"docstring": "\"\"\"\nCompute the Proxy-A-Distance of a source/target representation\n\"\"\"\n", "func_signal": "def proxy_a_distance(source_X, target_X):\n", "code": "nb_source = np.shape(source_X)[0]\nnb_target = np.shape(target_X)[0]\ntrain_X = np.vstack((source_X, target_X))\ntrain_Y = np.hstack((np.zeros(nb_source, dtype=int), np.ones(nb_target, dtype=int)))\nclf = svm.LinearSVC(random_state=0)\nclf.fit(train_X, train_Y)\ny_pred = clf.predict(train_X)\nerror = metrics.mean_absolute_error(train_Y, y_pred)\ndist = 2 * (1 - 2 * error)\nreturn dist", "path": "transferlearning/code/traditional/MEDA/MEDA.py", "commit_date": "2019-04-21 00:00:00", "repo_name": "jindongwang/transferlearning", "stars": 12665, "license": "mit", "language": "python", "size": 36893}
{"docstring": "''' Plot TSNE figure. Set save_eps=True if you want to save a .eps file.\n'''\n", "func_signal": "def plot_tsne(self, save_eps=False):\n", "code": "tsne = TSNE(n_components=2, init='pca', random_state=0)\nfeatures = tsne.fit_transform(self.features)\nx_min, x_max = np.min(features, 0), np.max(features, 0)\ndata = (features - x_min) / (x_max - x_min)\ndel features\nfor i in range(data.shape[0]):\n    plt.text(data[i, 0], data[i, 1], str(self.labels[i]),\n             color=plt.cm.Set1(self.labels[i] / 10.),\n             fontdict={'weight': 'bold', 'size': 9})\nplt.xticks([])\nplt.yticks([])\nplt.title('T-SNE')\nif save_eps:\n    plt.savefig('tsne.eps', dpi=600, format='eps')\nplt.show()", "path": "transferlearning/code/utils/feature_vis.py", "commit_date": "2020-04-22 00:00:00", "repo_name": "jindongwang/transferlearning", "stars": 12665, "license": "mit", "language": "python", "size": 36893}
{"docstring": "\"\"\"\nCompute the principal angles between source (:math:`P_s`) and target (:math:`P_t`) subspaces in a Grassman which is defined as the following:\n\n:math:`d^{2}(P_s, P_t) = \\sum_{i}( \\theta_i^{2} )`,\n\n\"\"\"\n# S = cos(theta_1, theta_2, ..., theta_n)\n", "func_signal": "def principal_angles(self, Ps, Pt):\n", "code": "_, S, _ = np.linalg.svd(np.dot(Ps.T, Pt))\nthetas_squared = np.arccos(S) ** 2\n\nreturn np.sum(thetas_squared)", "path": "transferlearning/code/traditional/GFK/GFK.py", "commit_date": "2018-11-15 00:00:00", "repo_name": "jindongwang/transferlearning", "stars": 12665, "license": "mit", "language": "python", "size": 36893}
{"docstring": "'''\nInitialization function\n:param kernel_type: 'linear' | 'rbf'\n:param gamma: kernel bandwidth for rbf kernel\n:param B: bound for beta\n:param eps: bound for sigma_beta\n'''\n", "func_signal": "def __init__(self, kernel_type='linear', gamma=1.0, B=1.0, eps=None):\n", "code": "self.kernel_type = kernel_type\nself.gamma = gamma\nself.B = B\nself.eps = eps", "path": "transferlearning/code/traditional/KMM.py", "commit_date": "2020-09-30 00:00:00", "repo_name": "jindongwang/transferlearning", "stars": 12665, "license": "mit", "language": "python", "size": 36893}
{"docstring": "'''\nModified PCA function, different from the one in sklearn\n:param data: data matrix\n:param mu_data: mu\n:param std_data: std\n:param subspace_dim: dim\n:return: a wrapped machine object\n'''\n", "func_signal": "def train_pca(self, data, mu_data, std_data, subspace_dim):\n", "code": "t = bob.learn.linear.PCATrainer()\nmachine, variances = t.train(data)\n\n# For re-shaping, we need to copy...\nvariances = variances.copy()\n\n# compute variance percentage, if desired\nif isinstance(subspace_dim, float):\n    cummulated = np.cumsum(variances) / np.sum(variances)\n    for index in range(len(cummulated)):\n        if cummulated[index] > subspace_dim:\n            subspace_dim = index\n            break\n    subspace_dim = index\nmachine.resize(machine.shape[0], subspace_dim)\nmachine.input_subtract = mu_data\nmachine.input_divide = std_data\n\nreturn machine", "path": "transferlearning/code/traditional/GFK/GFK.py", "commit_date": "2018-11-15 00:00:00", "repo_name": "jindongwang/transferlearning", "stars": 12665, "license": "mit", "language": "python", "size": 36893}
{"docstring": "\"\"\"Constructs a ResNet-50 model.\n\nArgs:\n    pretrained (bool): If True, returns a model pre-trained on ImageNet\n\"\"\"\n", "func_signal": "def resnet50(pretrained=False, **kwargs):\n", "code": "model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\nif pretrained:\n    model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\nreturn model", "path": "transferlearning/code/deep/MRAN/ResNet.py", "commit_date": "2019-10-22 00:00:00", "repo_name": "jindongwang/transferlearning", "stars": 12665, "license": "mit", "language": "python", "size": 36893}
{"docstring": "'''\nFit source and target using KMM (compute the coefficients)\n:param Xs: ns * dim\n:param Xt: nt * dim\n:return: Coefficients (Pt / Ps) value vector (Beta in the paper)\n'''\n", "func_signal": "def fit(self, Xs, Xt):\n", "code": "ns = Xs.shape[0]\nnt = Xt.shape[0]\nif self.eps == None:\n    self.eps = self.B / np.sqrt(ns)\nK = kernel(self.kernel_type, Xs, None, self.gamma)\nkappa = np.sum(kernel(self.kernel_type, Xs, Xt, self.gamma) * float(ns) / float(nt), axis=1)\n\nK = matrix(K.astype(np.double))\nkappa = matrix(kappa.astype(np.double))\nG = matrix(np.r_[np.ones((1, ns)), -np.ones((1, ns)), np.eye(ns), -np.eye(ns)])\nh = matrix(np.r_[ns * (1 + self.eps), ns * (self.eps - 1), self.B * np.ones((ns,)), np.zeros((ns,))])\n\nsol = solvers.qp(K, -kappa, G, h)\nbeta = np.array(sol['x'])\nreturn beta", "path": "transferlearning/code/traditional/KMM.py", "commit_date": "2020-09-30 00:00:00", "repo_name": "jindongwang/transferlearning", "stars": 12665, "license": "mit", "language": "python", "size": 36893}
{"docstring": "# Ensure on SQLite < 3.24 we cannot update or preserve values.\n", "func_signal": "def test_no_preserve_update_where(self):\n", "code": "base = Emp.insert(first='foo', last='bar', empno='125')\n\npreserve = base.on_conflict(preserve=[Emp.last])\nself.assertRaises(ValueError, preserve.execute)\n\nupdate = base.on_conflict(update={Emp.empno: 'xxx'})\nself.assertRaises(ValueError, update.execute)\n\nwhere = base.on_conflict(where=(Emp.id > 10))\nself.assertRaises(ValueError, where.execute)", "path": "peewee/tests/models.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "coleifer/peewee", "stars": 10683, "license": "mit", "language": "python", "size": 14756}
{"docstring": "# Test that a model-alias can be both the source and the dest by\n# joining from User -> Tweet -> User (as \"foo\").\n", "func_signal": "def test_join_alias_twice(self):\n", "code": "TA = Tweet.alias('ta')\nUA = User.alias('ua')\nwith self.assertQueryCount(1):\n    query = (User\n             .select(User, TA, UA)\n             .join(TA)\n             .join(UA, on=(TA.user_id == UA.id).alias('foo'))\n             .order_by(User.username, TA.content))\n\n    data = [(row.username, row.tweet.content, row.tweet.foo.username)\n            for row in query]\n\nself.assertEqual(data, [\n    ('huey', 'meow', 'huey'),\n    ('huey', 'purr', 'huey'),\n    ('mickey', 'woof', 'mickey'),\n    ('zaizee', 'hiss', 'zaizee')])", "path": "peewee/tests/models.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "coleifer/peewee", "stars": 10683, "license": "mit", "language": "python", "size": 14756}
{"docstring": "# We have duplicated users. Select a maximum of 2 instances of the\n# username.\n", "func_signal": "def test_subquery_emulate_window(self):\n", "code": "name2count = {\n    'beanie': 6,\n    'huey': 5,\n    'mickey': 3,\n    'pipey': 1,\n    'zaizee': 4}\nnames = []\nfor name, count in sorted(name2count.items()):\n    names += [name] * count\nUser.insert_many([(i, n) for i, n in enumerate(names, 1)],\n                 [User.id, User.username]).execute()\n\n# The results we are trying to obtain.\nexpected = [\n    ('beanie', 1), ('beanie', 2),\n    ('huey', 7), ('huey', 8),\n    ('mickey', 12), ('mickey', 13),\n    ('pipey', 15),\n    ('zaizee', 16), ('zaizee', 17)]\n\nwith self.assertQueryCount(1):\n    # Using a self-join.\n    UA = User.alias()\n    query = (User\n             .select(User.username, UA.id)\n             .join(UA, on=((UA.username == User.username) &\n                           (UA.id >= User.id)))\n             .group_by(User.username, UA.id)\n             .having(fn.COUNT(UA.id) < 3)\n             .order_by(User.username, UA.id))\n    self.assertEqual(query.tuples()[:], expected)\n\nwith self.assertQueryCount(1):\n    # Using a correlated subquery.\n    subq = (UA\n            .select(UA.id)\n            .where(User.username == UA.username)\n            .order_by(UA.id)\n            .limit(2))\n    query = (User\n             .select(User.username, User.id)\n             .where(User.id.in_(subq.alias('subq')))\n             .order_by(User.username, User.id))\n    self.assertEqual(query.tuples()[:], expected)", "path": "peewee/tests/models.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "coleifer/peewee", "stars": 10683, "license": "mit", "language": "python", "size": 14756}
{"docstring": "# Unique constraint on first/last would fail - replace.\n", "func_signal": "def test_replace(self):\n", "code": "query = (Emp\n         .insert(first='mickey', last='dog', empno='1337')\n         .on_conflict('replace')\n         .execute())\nself.assertData([\n    ('huey', 'cat', '123'),\n    ('zaizee', 'cat', '124'),\n    ('mickey', 'dog', '1337')])\n\n# Unique constraint on empno would fail - replace.\nquery = (Emp\n         .insert(first='nuggie', last='dog', empno='123')\n         .on_conflict('replace')\n         .execute())\nself.assertData([\n    ('zaizee', 'cat', '124'),\n    ('mickey', 'dog', '1337'),\n    ('nuggie', 'dog', '123')])\n\n# No problems, data added.\nquery = (Emp\n         .insert(first='beanie', last='cat', empno='126')\n         .on_conflict('replace')\n         .execute())\nself.assertData([\n    ('zaizee', 'cat', '124'),\n    ('mickey', 'dog', '1337'),\n    ('nuggie', 'dog', '123'),\n    ('beanie', 'cat', '126')])", "path": "peewee/tests/models.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "coleifer/peewee", "stars": 10683, "license": "mit", "language": "python", "size": 14756}
{"docstring": "# Unique constraint on first/last would fail - replace.\n", "func_signal": "def test_replace(self):\n", "code": "query = (Emp\n         .insert(first='mickey', last='dog', empno='1337')\n         .on_conflict('replace')\n         .execute())\nself.assertData([\n    ('huey', 'cat', '123'),\n    ('zaizee', 'cat', '124'),\n    ('mickey', 'dog', '1337')])\n\n# Unique constraint on empno would fail - replace.\nquery = (Emp\n         .insert(first='nuggie', last='dog', empno='123')\n         .on_conflict('replace')\n         .execute())\nself.assertData([\n    ('zaizee', 'cat', '124'),\n    ('mickey', 'dog', '1337'),\n    ('nuggie', 'dog', '123')])\n\n# No problems, data added.\nquery = (Emp\n         .insert(first='beanie', last='cat', empno='126')\n         .on_conflict('replace')\n         .execute())\nself.assertData([\n    ('zaizee', 'cat', '124'),\n    ('mickey', 'dog', '1337'),\n    ('nuggie', 'dog', '123'),\n    ('beanie', 'cat', '126')])", "path": "peewee/tests/models.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "coleifer/peewee", "stars": 10683, "license": "mit", "language": "python", "size": 14756}
{"docstring": "# Attach cache database so we can reference \"cache.\" as the schema.\n", "func_signal": "def test_model_indexes_with_schema(self):\n", "code": "self.database.execute_sql(\"attach database ':memory:' as cache;\")\nself.assertCreateTable(CacheData, [\n    ('CREATE TABLE \"cache\".\"cache_data\" ('\n     '\"id\" INTEGER NOT NULL PRIMARY KEY, \"key\" TEXT NOT NULL, '\n     '\"value\" TEXT NOT NULL)'),\n    ('CREATE UNIQUE INDEX \"cache\".\"cache_data_key\" ON \"cache_data\" '\n     '(\"key\")')])\n\n# Actually create the table to verify it works correctly.\nCacheData.create_table()\n\n# Introspect the database and get indexes for the \"cache\" schema.\nindexes = self.database.get_indexes('cache_data', 'cache')\nself.assertEqual(len(indexes), 1)\nindex_metadata = indexes[0]\nself.assertEqual(index_metadata.name, 'cache_data_key')\n\n# Verify the index does not exist in the main schema.\nself.assertEqual(len(self.database.get_indexes('cache_data')), 0)\n\nclass TestDatabase(Database):\n    index_schema_prefix = False\n\n# When \"index_schema_prefix == False\", the index name is not prefixed\n# with the schema, and the schema is referenced via the table name.\nwith CacheData.bind_ctx(TestDatabase(None)):\n    self.assertCreateTable(CacheData, [\n        ('CREATE TABLE \"cache\".\"cache_data\" ('\n         '\"id\" INTEGER NOT NULL PRIMARY KEY, \"key\" TEXT NOT NULL, '\n         '\"value\" TEXT NOT NULL)'),\n        ('CREATE UNIQUE INDEX \"cache_data_key\" ON \"cache\".\"cache_data\"'\n         ' (\"key\")')])", "path": "peewee/tests/schema.py", "commit_date": "2020-08-11 00:00:00", "repo_name": "coleifer/peewee", "stars": 10683, "license": "mit", "language": "python", "size": 14756}
{"docstring": "# Conflict on empno - we'll preserve name and update the ID. This will\n# overwrite the previous row and set a new ID.\n", "func_signal": "def test_update(self):\n", "code": "res = (Emp\n       .insert(first='foo', last='bar', empno='125')\n       .on_conflict(\n           conflict_target=(Emp.empno,),\n           preserve=(Emp.first, Emp.last),\n           update={Emp.empno: '125.1'})\n       .execute())\nself.assertData([\n    ('huey', 'cat', '123'),\n    ('zaizee', 'cat', '124'),\n    ('foo', 'bar', '125.1')])\n\n# Conflicts on first/last name. The first name is preserved while the\n# last-name is updated. The new empno is thrown out.\nres = (Emp\n       .insert(first='foo', last='bar', empno='126')\n       .on_conflict(\n           conflict_target=(Emp.first, Emp.last),\n           preserve=(Emp.first,),\n           update={Emp.last: 'baze'})\n       .execute())\nself.assertData([\n    ('huey', 'cat', '123'),\n    ('zaizee', 'cat', '124'),\n    ('foo', 'baze', '125.1')])", "path": "peewee/tests/models.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "coleifer/peewee", "stars": 10683, "license": "mit", "language": "python", "size": 14756}
{"docstring": "# self.models is a singleton, essentially, shared among all\n# classes that use this metadata implementation.\n", "func_signal": "def schema(self, value):\n", "code": "for model in self.models:\n    model._meta._schema = value", "path": "peewee/tests/models.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "coleifer/peewee", "stars": 10683, "license": "mit", "language": "python", "size": 14756}
{"docstring": "# Set up some relationships such that there exists a relationship from\n# the left-hand to the right-hand name.\n", "func_signal": "def test_join_subquery(self):\n", "code": "data = (\n    ('charlie', None),\n    ('huey', 'charlie'),\n    ('mickey', 'charlie'),\n    ('zaizee', 'charlie'),\n    ('zaizee', 'huey'))\npeople = {}\ndef get_person(name):\n    if name not in people:\n        people[name] = Person.create(first=name, last=name,\n                                     dob=datetime.date(2017, 1, 1))\n    return people[name]\n\nfor person, related_to in data:\n    p1 = get_person(person)\n    if related_to is not None:\n        p2 = get_person(related_to)\n        Relationship.create(from_person=p1, to_person=p2)\n\n# Create the subquery.\nFriend = Person.alias('friend')\nsubq = (Relationship\n        .select(Friend.first.alias('friend_name'),\n                Relationship.from_person)\n        .join(Friend, on=(Relationship.to_person == Friend.id))\n        .alias('subq'))\n\n# Outer query does a LEFT OUTER JOIN. We join on the subquery because\n# it uses an INNER JOIN, saving us doing two LEFT OUTER joins in the\n# single query.\nquery = (Person\n         .select(Person.first, subq.c.friend_name)\n         .join(subq, JOIN.LEFT_OUTER,\n               on=(Person.id == subq.c.from_person_id))\n         .order_by(Person.first, subq.c.friend_name))\nself.assertSQL(query, (\n    'SELECT \"t1\".\"first\", \"subq\".\"friend_name\" '\n    'FROM \"person\" AS \"t1\" '\n    'LEFT OUTER JOIN ('\n    'SELECT \"friend\".\"first\" AS \"friend_name\", \"t2\".\"from_person_id\" '\n    'FROM \"relationship\" AS \"t2\" '\n    'INNER JOIN \"person\" AS \"friend\" '\n    'ON (\"t2\".\"to_person_id\" = \"friend\".\"id\")) AS \"subq\" '\n    'ON (\"t1\".\"id\" = \"subq\".\"from_person_id\") '\n    'ORDER BY \"t1\".\"first\", \"subq\".\"friend_name\"'), [])\n\ndb_data = [row for row in query.tuples()]\nself.assertEqual(db_data, list(data))", "path": "peewee/tests/models.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "coleifer/peewee", "stars": 10683, "license": "mit", "language": "python", "size": 14756}
{"docstring": "# Add a new row with the given \"a\" value. If a conflict occurs,\n# re-insert with b=b+2 so long as the original b < 3.\n", "func_signal": "def test_update_where_clause(self):\n", "code": "query = OCTest.insert(a='foo', b=1).on_conflict(\n    conflict_target=(OCTest.a,),\n    update={OCTest.b: OCTest.b + 2},\n    where=(OCTest.b < 3))\n\n# First execution returns rowid=1. Second execution hits the conflict-\n# resolution, and will update the value in \"b\" from 1 -> 3.\nrowid1 = query.execute()\nrowid2 = query.clone().execute()\nself.assertEqual(rowid1, rowid2)\n\nobj = OCTest.get()\nself.assertEqual(obj.a, 'foo')\nself.assertEqual(obj.b, 3)\n\n# Third execution also returns rowid=1. The WHERE clause prevents us\n# from updating \"b\" again. If this is SQLite, we get the rowid back, if\n# this is Postgresql we get None (since nothing happened).\nrowid3 = query.clone().execute()\nif IS_SQLITE:\n    self.assertEqual(rowid1, rowid3)\nelse:\n    self.assertTrue(rowid3 is None)\n\n# Because we didn't satisfy the WHERE clause, the value in \"b\" is\n# not incremented again.\nobj = OCTest.get()\nself.assertEqual(obj.a, 'foo')\nself.assertEqual(obj.b, 3)", "path": "peewee/tests/models.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "coleifer/peewee", "stars": 10683, "license": "mit", "language": "python", "size": 14756}
{"docstring": "# Verify fields are inferred and values are read correctly, when\n# partial data is given and a field has default values.\n", "func_signal": "def test_insert_many_defaults(self):\n", "code": "s2 = {'counter': 2, 'value': 2.}\ns3 = {'counter': 3}\nself.assertSQL(Sample.insert_many([s2, s3]), (\n    'INSERT INTO \"sample\" (\"counter\", \"value\") VALUES (?, ?), (?, ?)'),\n    [2, 2., 3, 1.])\n\nself.assertSQL(Sample.insert_many([s3, s2]), (\n    'INSERT INTO \"sample\" (\"counter\", \"value\") VALUES (?, ?), (?, ?)'),\n    [3, 1., 2, 2.])", "path": "peewee/tests/model_sql.py", "commit_date": "2020-11-07 00:00:00", "repo_name": "coleifer/peewee", "stars": 10683, "license": "mit", "language": "python", "size": 14756}
{"docstring": "# Add a new row with the given \"a\" value. If a conflict occurs,\n# re-insert with b=b+2.\n", "func_signal": "def test_update_atomic(self):\n", "code": "query = OCTest.insert(a='foo', b=1).on_conflict(\n    conflict_target=(OCTest.a,),\n    update={OCTest.b: OCTest.b + 2})\n\n# First execution returns rowid=1. Second execution hits the conflict-\n# resolution, and will update the value in \"b\" from 1 -> 3.\nrowid1 = query.execute()\nrowid2 = query.clone().execute()\nself.assertEqual(rowid1, rowid2)\n\nobj = OCTest.get()\nself.assertEqual(obj.a, 'foo')\nself.assertEqual(obj.b, 3)\n\nquery = OCTest.insert(a='foo', b=4, c=5).on_conflict(\n    conflict_target=[OCTest.a],\n    preserve=[OCTest.c],\n    update={OCTest.b: OCTest.b + 100})\nself.assertEqual(query.execute(), rowid2)\n\nobj = OCTest.get()\nself.assertEqual(obj.a, 'foo')\nself.assertEqual(obj.b, 103)\nself.assertEqual(obj.c, 5)", "path": "peewee/tests/models.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "coleifer/peewee", "stars": 10683, "license": "mit", "language": "python", "size": 14756}
{"docstring": "# Ensure that on >= 3.24 any updates meet the minimum criteria.\n", "func_signal": "def test_update_meets_requirements(self):\n", "code": "base = Emp.insert(first='foo', last='bar', empno='125')\n\n# Must specify update or preserve.\nno_update_preserve = base.on_conflict(conflict_target=(Emp.empno,))\nself.assertRaises(ValueError, no_update_preserve.execute)\n\n# Must specify a conflict target.\nno_conflict_target = base.on_conflict(update={Emp.empno: '125.1'})\nself.assertRaises(ValueError, no_conflict_target.execute)", "path": "peewee/tests/models.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "coleifer/peewee", "stars": 10683, "license": "mit", "language": "python", "size": 14756}
{"docstring": "# Define the base case of our recursive CTE. This will be categories that\n# have a null parent foreign-key.\n", "func_signal": "def test_recursive_cte_docs_example(self):\n", "code": "Base = Category.alias()\nlevel = Value(1).cast('integer').alias('level')\npath = Base.name.cast('text').alias('path')\nbase_case = (Base\n             .select(Base.name, Base.parent, level, path)\n             .where(Base.parent.is_null())\n             .cte('base', recursive=True))\n\n# Define the recursive terms.\nRTerm = Category.alias()\nrlevel = (base_case.c.level + 1).alias('level')\nrpath = base_case.c.path.concat('->').concat(RTerm.name).alias('path')\nrecursive = (RTerm\n             .select(RTerm.name, RTerm.parent, rlevel, rpath)\n             .join(base_case, on=(RTerm.parent == base_case.c.name)))\n\n# The recursive CTE is created by taking the base case and UNION ALL with\n# the recursive term.\ncte = base_case.union_all(recursive)\n\n# We will now query from the CTE to get the categories, their levels,  and\n# their paths.\nquery = (cte\n         .select_from(cte.c.name, cte.c.level, cte.c.path)\n         .order_by(cte.c.path))\ndata = [(obj.name, obj.level, obj.path) for obj in query]\nself.assertEqual(data, [\n    ('root', 1, 'root'),\n    ('p1', 2, 'root->p1'),\n    ('c11', 3, 'root->p1->c11'),\n    ('c12', 3, 'root->p1->c12'),\n    ('p2', 2, 'root->p2'),\n    ('p3', 2, 'root->p3'),\n    ('c31', 3, 'root->p3->c31')])", "path": "peewee/tests/models.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "coleifer/peewee", "stars": 10683, "license": "mit", "language": "python", "size": 14756}
{"docstring": "# Make sure an account id and url were specified.\n", "func_signal": "def analyze():\n", "code": "if not request.args.get('id') or not request.args.get('url'):\n    abort(404)\n\n# Ensure the account id is valid.\ntry:\n    account = Account.get(Account.id == request.args['id'])\nexcept Account.DoesNotExist:\n    abort(404)\n\n# Ensure the account id matches the domain of the URL we wish to record.\nif not account.verify_url(request.args['url']):\n    abort(403)\n\n# Store the page-view data in the database.\nPageView.create_from_request(account, request)\n\n# Return a 1px gif.\nresponse = Response(app.config['BEACON'], mimetype='image/gif')\nresponse.headers['Cache-Control'] = 'private, no-cache'\nreturn response", "path": "peewee/examples/analytics/app.py", "commit_date": "2020-05-27 00:00:00", "repo_name": "coleifer/peewee", "stars": 10683, "license": "mit", "language": "python", "size": 14756}
{"docstring": "# Although value is not specified, it has a default, which is included\n# in the INSERT.\n", "func_signal": "def test_default_present_on_insert(self):\n", "code": "query = Sample.insert(counter=0)\nself.assertSQL(query, (\n    'INSERT INTO \"sample\" (\"counter\", \"value\") '\n    'VALUES (?, ?)'), [0, 1.0])\n\n# Default values are also included when doing bulk inserts.\nquery = Sample.insert_many([\n    {'counter': '0'},\n    {'counter': 1, 'value': 2},\n    {'counter': '2'}])\nself.assertSQL(query, (\n    'INSERT INTO \"sample\" (\"counter\", \"value\") '\n    'VALUES (?, ?), (?, ?), (?, ?)'), [0, 1.0, 1, 2.0, 2, 1.0])\n\nquery = Sample.insert_many([(0,), (1, 2.)],\n                           fields=[Sample.counter])\nself.assertSQL(query, (\n    'INSERT INTO \"sample\" (\"counter\", \"value\") '\n    'VALUES (?, ?), (?, ?)'), [0, 1.0, 1, 2.0])", "path": "peewee/tests/models.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "coleifer/peewee", "stars": 10683, "license": "mit", "language": "python", "size": 14756}
{"docstring": "# Ensure value is converted on INSERT.\n", "func_signal": "def test_value_conversion(self):\n", "code": "insert = UpperModel.insert({UpperModel.name: 'huey'})\nself.assertSQL(insert, (\n    'INSERT INTO \"upper_model\" (\"name\") VALUES (UPPER(?))'), ['huey'])\nuid = insert.execute()\n\nobj = UpperModel.get(UpperModel.id == uid)\nself.assertEqual(obj.name, 'HUEY')\n\n# Ensure value is converted on UPDATE.\nupdate = (UpperModel\n          .update({UpperModel.name: 'zaizee'})\n          .where(UpperModel.id == uid))\nself.assertSQL(update, (\n    'UPDATE \"upper_model\" SET \"name\" = UPPER(?) '\n    'WHERE (\"upper_model\".\"id\" = ?)'),\n    ['zaizee', uid])\nupdate.execute()\n\n# Ensure it works with SELECT (or more generally, WHERE expressions).\nselect = UpperModel.select().where(UpperModel.name == 'zaizee')\nself.assertSQL(select, (\n    'SELECT \"t1\".\"id\", \"t1\".\"name\" FROM \"upper_model\" AS \"t1\" '\n    'WHERE (\"t1\".\"name\" = UPPER(?))'), ['zaizee'])\nobj = select.get()\nself.assertEqual(obj.name, 'ZAIZEE')\n\n# Ensure it works with DELETE.\ndelete = UpperModel.delete().where(UpperModel.name == 'zaizee')\nself.assertSQL(delete, (\n    'DELETE FROM \"upper_model\" '\n    'WHERE (\"upper_model\".\"name\" = UPPER(?))'), ['zaizee'])\nself.assertEqual(delete.execute(), 1)", "path": "peewee/tests/expressions.py", "commit_date": "2018-11-21 00:00:00", "repo_name": "coleifer/peewee", "stars": 10683, "license": "mit", "language": "python", "size": 14756}
{"docstring": "# k1/v1/e1, k2/v2/e0, k3/v3/e1\n", "func_signal": "def test_conflict_ambiguous_column(self):\n", "code": "for i in [1, 2, 3]:\n    UKV.create(key='k%s' % i, value='v%s' % i, extra='e%s' % (i % 2))\n\nUKVRel.create(key='k1', value='v1', extra='x1')\nUKVRel.create(key='k2', value='v2', extra='x2')\n\nsubq = UKV.select(UKV.key, UKV.value, UKV.extra)\nquery = (UKVRel\n         .insert_from(subq, [UKVRel.key, UKVRel.value, UKVRel.extra])\n         .on_conflict(conflict_target=[UKVRel.key, UKVRel.value],\n                      preserve=[UKVRel.extra],\n                      where=(UKVRel.key != 'k2')))\nself.assertSQL(query, (\n    'INSERT INTO \"ukv_rel\" (\"key\", \"value\", \"extra\") '\n    'SELECT \"t1\".\"key\", \"t1\".\"value\", \"t1\".\"extra\" FROM \"ukv\" AS \"t1\" '\n    'ON CONFLICT (\"key\", \"value\") DO UPDATE '\n    'SET \"extra\" = EXCLUDED.\"extra\" '\n    'WHERE (\"ukv_rel\".\"key\" != ?) RETURNING \"ukv_rel\".\"id\"'), ['k2'])\n\nquery.execute()\nquery = (UKVRel\n         .select(UKVRel.key, UKVRel.value, UKVRel.extra)\n         .order_by(UKVRel.key))\nself.assertEqual(list(query.tuples()), [\n    ('k1', 'v1', 'e1'),\n    ('k2', 'v2', 'x2'),\n    ('k3', 'v3', 'e1')])", "path": "peewee/tests/models.py", "commit_date": "2020-11-26 00:00:00", "repo_name": "coleifer/peewee", "stars": 10683, "license": "mit", "language": "python", "size": 14756}
{"docstring": "# Add a new row with the given \"a\" value. If a conflict occurs,\n# re-insert with b=b+2 so long as the original b < 3.\n", "func_signal": "def test_update_where_clause(self):\n", "code": "query = OCTest.insert(a='foo', b=1).on_conflict(\n    conflict_target=(OCTest.a,),\n    update={OCTest.b: OCTest.b + 2},\n    where=(OCTest.b < 3))\nself.assertSQL(query, (\n    'INSERT INTO \"oc_test\" (\"a\", \"b\", \"c\") VALUES (?, ?, ?) '\n    'ON CONFLICT (\"a\") DO UPDATE SET \"b\" = (\"oc_test\".\"b\" + ?) '\n    'WHERE (\"oc_test\".\"b\" < ?) '\n    'RETURNING \"oc_test\".\"id\"'), ['foo', 1, 0, 2, 3])", "path": "peewee/tests/model_sql.py", "commit_date": "2020-11-07 00:00:00", "repo_name": "coleifer/peewee", "stars": 10683, "license": "mit", "language": "python", "size": 14756}
{"docstring": "# Get the first page, default is limit of 20.\n", "func_signal": "def test_paginate(self):\n", "code": "query = User.select().paginate(1)\nself.assertSQL(query, (\n    'SELECT \"t1\".\"id\", \"t1\".\"username\" FROM \"users\" AS \"t1\" '\n    'LIMIT ? OFFSET ?'), [20, 0])\n\n# Page 3 contains rows 31-45.\nquery = User.select().paginate(3, 15)\nself.assertSQL(query, (\n    'SELECT \"t1\".\"id\", \"t1\".\"username\" FROM \"users\" AS \"t1\" '\n    'LIMIT ? OFFSET ?'), [15, 30])", "path": "peewee/tests/model_sql.py", "commit_date": "2020-11-07 00:00:00", "repo_name": "coleifer/peewee", "stars": 10683, "license": "mit", "language": "python", "size": 14756}
{"docstring": "\"\"\"Checks that no two forms refer to the same collection object\"\"\"\n", "func_signal": "def clean(self):\n", "code": "if any(self.errors):\n    # Don't bother validating the formset unless each form is valid on its own\n    return\n\ncollections = [\n    form.cleaned_data['collection']\n    for form in self.forms\n    # need to check for presence of 'collection' in cleaned_data,\n    # because a completely blank form passes validation\n    if form not in self.deleted_forms and 'collection' in form.cleaned_data\n]\nif len(set(collections)) != len(collections):\n    # collections list contains duplicates\n    raise forms.ValidationError(\n        _(\"You cannot have multiple permission records for the same collection.\")\n    )", "path": "wagtail/wagtail/admin/forms/collections.py", "commit_date": "2020-09-03 00:00:00", "repo_name": "wagtail/wagtail", "stars": 16885, "license": "bsd-3-clause", "language": "python", "size": 206512}
{"docstring": "\"\"\"Reverse the above additions of permissions.\"\"\"\n", "func_signal": "def remove_admin_access_permissions(apps, schema_editor):\n", "code": "ContentType = apps.get_model('contenttypes.ContentType')\nPermission = apps.get_model('auth.Permission')\nwagtailadmin_content_type = ContentType.objects.get(\n    app_label='wagtailadmin',\n    model='admin',\n)\n# This cascades to Group\nPermission.objects.filter(\n    content_type=wagtailadmin_content_type,\n    codename='access_admin',\n).delete()", "path": "wagtail/wagtail/admin/migrations/0001_create_admin_access_permissions.py", "commit_date": "2020-12-23 00:00:00", "repo_name": "wagtail/wagtail", "stars": 16885, "license": "bsd-3-clause", "language": "python", "size": 206512}
{"docstring": "\"\"\"\nThis bit of code attempts to match the objects in the A revision with\ntheir counterpart in the B revision.\n\nA match is firstly attempted by PK (where a matching ID indicates they're the same).\nWe compare remaining the objects by their field data; the objects with the fewest\nfields changed are matched until there are no more possible matches left.\n\nThis returns 4 values:\n - map_forwards => a mapping of object indexes from the B version to the A version\n - map_backwards => a mapping of object indexes from the A version to the B version\n - added => a list of indices for objects that didn't exist in the B version\n - deleted => a list of indices for objects that didn't exist in the A version\n\nNote the indices are 0-based array indices indicating the location of the object in either\nthe objs_a or objs_b arrays.\n\nFor example:\n\nobjs_a => A, B, C, D\nobjs_b => B, C, D, E\n\nWill return the following:\n\nmap_forwards = {\n    1: 0,  # B (objs_a: objs_b)\n    2: 1,  # C (objs_a: objs_b)\n    3: 2,  # D (objs_a: objs_b)\n}\nmap_backwards = {\n    0: 1,  # B (objs_b: objs_a)\n    1: 2,  # C (objs_b: objs_a)\n    2: 3,  # D (objs_b: objs_a)\n}\nadded = [4]  # D in objs_b\ndeleted = [0]  # A in objs_a\n\"\"\"\n", "func_signal": "def get_mapping(self, objs_a, objs_b):\n", "code": "map_forwards = {}\nmap_backwards = {}\nadded = []\ndeleted = []\n\n# Match child objects on PK (ID)\nfor a_idx, a_child in enumerate(objs_a):\n    for b_idx, b_child in enumerate(objs_b):\n        if b_idx in map_backwards:\n            continue\n\n        if a_child.pk is not None and b_child.pk is not None and a_child.pk == b_child.pk:\n            map_forwards[a_idx] = b_idx\n            map_backwards[b_idx] = a_idx\n\n# Now try to match them by data\nmatches = []\nfor a_idx, a_child in enumerate(objs_a):\n    if a_idx not in map_forwards:\n        for b_idx, b_child in enumerate(objs_b):\n            if b_idx not in map_backwards:\n                # If they both have a PK (ID) that is different, they can't be the same child object\n                if a_child.pk and b_child.pk and a_child.pk != b_child.pk:\n                    continue\n\n                comparison = self.get_child_comparison(objs_a[a_idx], objs_b[b_idx])\n                num_differences = comparison.get_num_differences()\n\n                matches.append((a_idx, b_idx, num_differences))\n\n# Objects with the least differences will be matched first. So only the best possible matches are made\nmatches.sort(key=lambda match: match[2])\nfor a_idx, b_idx, num_differences in matches:\n    # Make sure both objects were not matched previously\n    if a_idx in map_forwards or b_idx in map_backwards:\n        continue\n\n    # Match!\n    map_forwards[a_idx] = b_idx\n    map_backwards[b_idx] = a_idx\n\n# Mark unmapped objects as added/deleted\nfor a_idx, a_child in enumerate(objs_a):\n    if a_idx not in map_forwards:\n        deleted.append(a_idx)\n\nfor b_idx, b_child in enumerate(objs_b):\n    if b_idx not in map_backwards:\n        added.append(b_idx)\n\nreturn map_forwards, map_backwards, added, deleted", "path": "wagtail/wagtail/admin/compare.py", "commit_date": "2020-09-23 00:00:00", "repo_name": "wagtail/wagtail", "stars": 16885, "license": "bsd-3-clause", "language": "python", "size": 206512}
{"docstring": "\"\"\"\nReturns true if any changes were made to any of the child objects. This includes\nadding, deleting and reordering.\n\"\"\"\n", "func_signal": "def has_changed(self):\n", "code": "objs_a = list(self.val_a.all())\nobjs_b = list(self.val_b.all())\n\nmap_forwards, map_backwards, added, deleted = self.get_mapping(objs_a, objs_b)\n\nif added or deleted:\n    return True\n\nfor a_idx, b_idx in map_forwards.items():\n    comparison = self.get_child_comparison(objs_a[a_idx], objs_b[b_idx])\n\n    if comparison.has_changed():\n        return True\n\nreturn False", "path": "wagtail/wagtail/admin/compare.py", "commit_date": "2020-09-23 00:00:00", "repo_name": "wagtail/wagtail", "stars": 16885, "license": "bsd-3-clause", "language": "python", "size": 206512}
{"docstring": "\"\"\"\nPerforms a diffing algorithm on two pieces of text. Returns\na string of HTML containing the content of both texts with\n<span> tags inserted indicating where the differences are.\n\"\"\"\n", "func_signal": "def diff_text(a, b):\n", "code": "def tokenise(text):\n    \"\"\"\n    Tokenises a string by spliting it into individual characters\n    and grouping the alphanumeric ones together.\n\n    This means that punctuation, whitespace, CJK characters, etc\n    become separate tokens and words/numbers are merged together\n    to form bigger tokens.\n\n    This makes the output of the diff easier to read as words are\n    not broken up.\n    \"\"\"\n    tokens = []\n    current_token = \"\"\n\n    for c in text or \"\":\n        if c.isalnum():\n            current_token += c\n        else:\n            if current_token:\n                tokens.append(current_token)\n                current_token = \"\"\n\n            tokens.append(c)\n\n    if current_token:\n        tokens.append(current_token)\n\n    return tokens\n\na_tok = tokenise(a)\nb_tok = tokenise(b)\nsm = difflib.SequenceMatcher(lambda t: len(t) <= 4, a_tok, b_tok)\n\nchanges = []\n\nfor op, i1, i2, j1, j2 in sm.get_opcodes():\n    if op == 'replace':\n        for token in a_tok[i1:i2]:\n            changes.append(('deletion', token))\n        for token in b_tok[j1:j2]:\n            changes.append(('addition', token))\n    elif op == 'delete':\n        for token in a_tok[i1:i2]:\n            changes.append(('deletion', token))\n    elif op == 'insert':\n        for token in b_tok[j1:j2]:\n            changes.append(('addition', token))\n    elif op == 'equal':\n        for token in a_tok[i1:i2]:\n            changes.append(('equal', token))\n\n# Merge ajacent changes which have the same type. This just cleans up the HTML a bit\nmerged_changes = []\ncurrent_value = []\ncurrent_change_type = None\nfor change_type, value in changes:\n    if change_type != current_change_type:\n        if current_change_type is not None:\n            merged_changes.append((current_change_type, ''.join(current_value)))\n            current_value = []\n\n        current_change_type = change_type\n\n    current_value.append(value)\n\nif current_value:\n    merged_changes.append((current_change_type, ''.join(current_value)))\n\nreturn TextDiff(merged_changes)", "path": "wagtail/wagtail/admin/compare.py", "commit_date": "2020-09-23 00:00:00", "repo_name": "wagtail/wagtail", "stars": 16885, "license": "bsd-3-clause", "language": "python", "size": 206512}
{"docstring": "# Get tags\n", "func_signal": "def htmldiff(self):\n", "code": "items_a, items_b = self.get_items()\n\n# Calculate changes\nsm = difflib.SequenceMatcher(0, items_a, items_b)\nchanges = []\nfor op, i1, i2, j1, j2 in sm.get_opcodes():\n    if op == 'replace':\n        for item in items_a[i1:i2]:\n            changes.append(('deletion', self.get_item_display(item)))\n        for item in items_b[j1:j2]:\n            changes.append(('addition', self.get_item_display(item)))\n    elif op == 'delete':\n        for item in items_a[i1:i2]:\n            changes.append(('deletion', self.get_item_display(item)))\n    elif op == 'insert':\n        for item in items_b[j1:j2]:\n            changes.append(('addition', self.get_item_display(item)))\n    elif op == 'equal':\n        for item in items_a[i1:i2]:\n            changes.append(('equal', self.get_item_display(item)))\n\n# Convert changelist to HTML\nreturn TextDiff(changes, separator=\", \").to_html()", "path": "wagtail/wagtail/admin/compare.py", "commit_date": "2020-09-23 00:00:00", "repo_name": "wagtail/wagtail", "stars": 16885, "license": "bsd-3-clause", "language": "python", "size": 206512}
{"docstring": "# Open file if it is closed\n", "func_signal": "def open_file(self):\n", "code": "close_file = False\nf = self.file\n\nif f.closed:\n    # Reopen the file\n    if self.is_stored_locally():\n        f.open('rb')\n    else:\n        # Some external storage backends don't allow reopening\n        # the file. Get a fresh file instance. #1397\n        storage = self._meta.get_field('file').storage\n        f = storage.open(f.name, 'rb')\n\n    close_file = True\n\n# Seek to beginning\nf.seek(0)\n\ntry:\n    yield f\nfinally:\n    if close_file:\n        f.close()", "path": "wagtail/wagtail/documents/models.py", "commit_date": "2020-10-16 00:00:00", "repo_name": "wagtail/wagtail", "stars": 16885, "license": "bsd-3-clause", "language": "python", "size": 206512}
{"docstring": "# Check if this is the file field\n", "func_signal": "def formfield_for_dbfield(db_field, **kwargs):\n", "code": "if db_field.name == 'file':\n    return WagtailImageField(label=capfirst(db_field.verbose_name), **kwargs)\nelif db_field.name == 'collection':\n    return CollectionChoiceField(queryset=Collection.objects.all(), empty_label=None, **kwargs)\n\n# For all other fields, just call its formfield() method.\nreturn db_field.formfield(**kwargs)", "path": "wagtail/wagtail/images/forms.py", "commit_date": "2020-12-09 00:00:00", "repo_name": "wagtail/wagtail", "stars": 16885, "license": "bsd-3-clause", "language": "python", "size": 206512}
{"docstring": "\"\"\"\nParses strings into booleans using the following mapping (case-sensitive):\n\n'true'   => True\n'false'  => False\n'1'      => True\n'0'      => False\n\"\"\"\n", "func_signal": "def parse_boolean(value):\n", "code": "if value in ['true', '1']:\n    return True\nelif value in ['false', '0']:\n    return False\nelse:\n    raise ValueError(\"expected 'true' or 'false', got '%s'\" % value)", "path": "wagtail/wagtail/api/v2/utils.py", "commit_date": "2020-09-30 00:00:00", "repo_name": "wagtail/wagtail", "stars": 16885, "license": "bsd-3-clause", "language": "python", "size": 206512}
{"docstring": "\"\"\"\nParses the ?fields= GET parameter. As this parameter is supposed to be used\nby developers, the syntax is quite tight (eg, not allowing any whitespace).\nHaving a strict syntax allows us to extend the it at a later date with less\nchance of breaking anyone's code.\n\nThis function takes a string and returns a list of tuples representing each\ntop-level field. Each tuple contains three items:\n - The name of the field (string)\n - Whether the field has been negated (boolean)\n - A list of nested fields if there are any, None otherwise\n\nSome examples of how this function works:\n\n>>> parse_fields_parameter(\"foo\")\n[\n    ('foo', False, None),\n]\n\n>>> parse_fields_parameter(\"foo,bar\")\n[\n    ('foo', False, None),\n    ('bar', False, None),\n]\n\n>>> parse_fields_parameter(\"-foo\")\n[\n    ('foo', True, None),\n]\n\n>>> parse_fields_parameter(\"foo(bar,baz)\")\n[\n    ('foo', False, [\n        ('bar', False, None),\n        ('baz', False, None),\n    ]),\n]\n\nIt raises a FieldsParameterParseError (subclass of ValueError) if it\nencounters a syntax error\n\"\"\"\n\n", "func_signal": "def parse_fields_parameter(fields_str):\n", "code": "def get_position(current_str):\n    return len(fields_str) - len(current_str)\n\ndef parse_field_identifier(fields_str):\n    first_char = True\n    negated = False\n    ident = \"\"\n\n    while fields_str:\n        char = fields_str[0]\n\n        if char in ['(', ')', ',']:\n            if not ident:\n                raise FieldsParameterParseError(\"unexpected char '%s' at position %d\" % (char, get_position(fields_str)))\n\n            if ident in ['*', '_'] and char == '(':\n                # * and _ cannot have nested fields\n                raise FieldsParameterParseError(\"unexpected char '%s' at position %d\" % (char, get_position(fields_str)))\n\n            return ident, negated, fields_str\n\n        elif char == '-':\n            if not first_char:\n                raise FieldsParameterParseError(\"unexpected char '%s' at position %d\" % (char, get_position(fields_str)))\n\n            negated = True\n\n        elif char in ['*', '_']:\n            if ident and char == '*':\n                raise FieldsParameterParseError(\"unexpected char '%s' at position %d\" % (char, get_position(fields_str)))\n\n            ident += char\n\n        elif char.isalnum() or char == '_':\n            if ident == '*':\n                # * can only be on its own\n                raise FieldsParameterParseError(\"unexpected char '%s' at position %d\" % (char, get_position(fields_str)))\n\n            ident += char\n\n        elif char.isspace():\n            raise FieldsParameterParseError(\"unexpected whitespace at position %d\" % get_position(fields_str))\n        else:\n            raise FieldsParameterParseError(\"unexpected char '%s' at position %d\" % (char, get_position(fields_str)))\n\n        first_char = False\n        fields_str = fields_str[1:]\n\n    return ident, negated, fields_str\n\ndef parse_fields(fields_str, expect_close_bracket=False):\n    first_ident = None\n    is_first = True\n    fields = []\n\n    while fields_str:\n        sub_fields = None\n        ident, negated, fields_str = parse_field_identifier(fields_str)\n\n        # Some checks specific to '*' and '_'\n        if ident in ['*', '_']:\n            if not is_first:\n                raise FieldsParameterParseError(\"'%s' must be in the first position\" % ident)\n\n            if negated:\n                raise FieldsParameterParseError(\"'%s' cannot be negated\" % ident)\n\n        if fields_str and fields_str[0] == '(':\n            if negated:\n                # Negated fields cannot contain subfields\n                raise FieldsParameterParseError(\"unexpected char '(' at position %d\" % get_position(fields_str))\n\n            sub_fields, fields_str = parse_fields(fields_str[1:], expect_close_bracket=True)\n\n        if is_first:\n            first_ident = ident\n        else:\n            # Negated fields can't be used with '_'\n            if first_ident == '_' and negated:\n                # _,foo is allowed but _,-foo is not\n                raise FieldsParameterParseError(\"negated fields with '_' doesn't make sense\")\n\n            # Additional fields without sub fields can't be used with '*'\n            if first_ident == '*' and not negated and not sub_fields:\n                # *,foo(bar) and *,-foo are allowed but *,foo is not\n                raise FieldsParameterParseError(\"additional fields with '*' doesn't make sense\")\n\n        fields.append((ident, negated, sub_fields))\n\n        if fields_str and fields_str[0] == ')':\n            if not expect_close_bracket:\n                raise FieldsParameterParseError(\"unexpected char ')' at position %d\" % get_position(fields_str))\n\n            return fields, fields_str[1:]\n\n        if fields_str and fields_str[0] == ',':\n            fields_str = fields_str[1:]\n\n            # A comma can not exist immediately before another comma or the end of the string\n            if not fields_str or fields_str[0] == ',':\n                raise FieldsParameterParseError(\"unexpected char ',' at position %d\" % get_position(fields_str))\n\n        is_first = False\n\n    if expect_close_bracket:\n        # This parser should've exited with a close bracket but instead we\n        # hit the end of the input. Raise an error\n        raise FieldsParameterParseError(\"unexpected end of input (did you miss out a close bracket?)\")\n\n    return fields, fields_str\n\nfields, _ = parse_fields(fields_str)\n\nreturn fields", "path": "wagtail/wagtail/api/v2/utils.py", "commit_date": "2020-09-30 00:00:00", "repo_name": "wagtail/wagtail", "stars": 16885, "license": "bsd-3-clause", "language": "python", "size": 206512}
{"docstring": "\"\"\"\nReturns the number of fields that differ between the two\nobjects.\n\"\"\"\n", "func_signal": "def get_num_differences(self):\n", "code": "num_differences = 0\n\nfor comparison in self.get_field_comparisons():\n    if comparison.has_changed():\n        num_differences += 1\n\nreturn num_differences", "path": "wagtail/wagtail/admin/compare.py", "commit_date": "2020-09-23 00:00:00", "repo_name": "wagtail/wagtail", "stars": 16885, "license": "bsd-3-clause", "language": "python", "size": 206512}
{"docstring": "\"\"\"\nReturns the change in position as an integer. Positive if the object\nwas moved down, negative if it moved up.\n\nFor example: '3' indicates the object moved down three spaces. '-1'\nindicates the object moved up one space.\n\"\"\"\n", "func_signal": "def get_position_change(self):\n", "code": "if not self.is_addition() and not self.is_deletion():\n    sort_a = getattr(self.obj_a, 'sort_order', 0) or 0\n    sort_b = getattr(self.obj_b, 'sort_order', 0) or 0\n    return sort_b - sort_a", "path": "wagtail/wagtail/admin/compare.py", "commit_date": "2020-09-23 00:00:00", "repo_name": "wagtail/wagtail", "stars": 16885, "license": "bsd-3-clause", "language": "python", "size": 206512}
{"docstring": "\"\"\"\nReturns a label for this field to be displayed to the user\n\"\"\"\n", "func_signal": "def field_label(self):\n", "code": "verbose_name = getattr(self.field, 'verbose_name', None)\n\nif verbose_name is None:\n    # Relations don't have a verbose_name\n    verbose_name = self.field.name.replace('_', ' ')\n\nreturn capfirst(verbose_name)", "path": "wagtail/wagtail/admin/compare.py", "commit_date": "2020-09-23 00:00:00", "repo_name": "wagtail/wagtail", "stars": 16885, "license": "bsd-3-clause", "language": "python", "size": 206512}
{"docstring": "# Our method for diffing streamfields relies on the blocks in both revisions having UUIDs.\n# But as UUIDs were added in Wagtail 1.11 we can't compare revisions that were created before\n# that Wagtail version.\n", "func_signal": "def htmldiff(self):\n", "code": "if self.has_block_ids(self.val_a) and self.has_block_ids(self.val_b):\n    return StreamBlockComparison(self.field.stream_block, True, True, self.val_a, self.val_b).htmldiff()\nelse:\n    # Fall back to diffing the HTML representation\n    return diff_text(\n        text_from_html(self.val_a),\n        text_from_html(self.val_b)\n    ).to_html()", "path": "wagtail/wagtail/admin/compare.py", "commit_date": "2020-09-23 00:00:00", "repo_name": "wagtail/wagtail", "stars": 16885, "license": "bsd-3-clause", "language": "python", "size": 206512}
{"docstring": "# Find provider\n", "func_signal": "def find_embed(self, url, max_width=None):\n", "code": "endpoint = self._get_endpoint(url)\nif endpoint is None:\n    raise EmbedNotFoundException\n\n# Work out params\nparams = self.options.copy()\nparams['url'] = url\nparams['format'] = 'json'\nif max_width:\n    params['maxwidth'] = max_width\n\n# Perform request\nrequest = Request(endpoint + '?' + urlencode(params))\nrequest.add_header('User-agent', 'Mozilla/5.0')\ntry:\n    r = urllib_request.urlopen(request)\n    oembed = json.loads(r.read().decode('utf-8'))\nexcept (URLError, json.decoder.JSONDecodeError):\n    raise EmbedNotFoundException\n\n# Convert photos into HTML\nif oembed['type'] == 'photo':\n    html = '<img src=\"%s\" alt=\"\">' % (oembed['url'], )\nelse:\n    html = oembed.get('html')\n\n# Return embed as a dict\nreturn {\n    'title': oembed['title'] if 'title' in oembed else '',\n    'author_name': oembed['author_name'] if 'author_name' in oembed else '',\n    'provider_name': oembed['provider_name'] if 'provider_name' in oembed else '',\n    'type': oembed['type'],\n    'thumbnail_url': oembed.get('thumbnail_url'),\n    'width': oembed.get('width'),\n    'height': oembed.get('height'),\n    'html': html,\n}", "path": "wagtail/wagtail/embeds/finders/oembed.py", "commit_date": "2020-12-18 00:00:00", "repo_name": "wagtail/wagtail", "stars": 16885, "license": "bsd-3-clause", "language": "python", "size": 206512}
{"docstring": "\"\"\"\nChecks for WAGTAILDOCS_EXTENSIONS and validates the uploaded file\nbased on allowed extensions that were specified.\nWarning : This doesn't always ensure that the uploaded file is valid\nas files can be renamed to have an extension no matter what\ndata they contain.\n\nMore info : https://docs.djangoproject.com/en/3.1/ref/validators/#fileextensionvalidator\n\"\"\"\n", "func_signal": "def clean(self):\n", "code": "allowed_extensions = getattr(settings, \"WAGTAILDOCS_EXTENSIONS\", None)\nif allowed_extensions:\n    validate = FileExtensionValidator(allowed_extensions)\n    validate(self.file)", "path": "wagtail/wagtail/documents/models.py", "commit_date": "2020-10-16 00:00:00", "repo_name": "wagtail/wagtail", "stars": 16885, "license": "bsd-3-clause", "language": "python", "size": 206512}
{"docstring": "\"\"\"\nReturns True if the image is hosted on the local filesystem\n\"\"\"\n", "func_signal": "def is_stored_locally(self):\n", "code": "try:\n    self.file.path\n\n    return True\nexcept NotImplementedError:\n    return False", "path": "wagtail/wagtail/documents/models.py", "commit_date": "2020-10-16 00:00:00", "repo_name": "wagtail/wagtail", "stars": 16885, "license": "bsd-3-clause", "language": "python", "size": 206512}
{"docstring": "\"\"\"\nReturns a list of ChildObjectComparison objects. Representing all child\nobjects that existed in either version.\n\nThey are returned in the order they appear in the B version with deletions\nappended at the end.\n\nAll child objects are returned, regardless of whether they were actually changed.\n\"\"\"\n", "func_signal": "def get_child_comparisons(self):\n", "code": "objs_a = list(self.val_a.all())\nobjs_b = list(self.val_b.all())\n\nmap_forwards, map_backwards, added, deleted = self.get_mapping(objs_a, objs_b)\nobjs_a = dict(enumerate(objs_a))\nobjs_b = dict(enumerate(objs_b))\n\ncomparisons = []\n\nfor b_idx, b_child in objs_b.items():\n    if b_idx in added:\n        comparisons.append(self.get_child_comparison(None, b_child))\n    else:\n        comparisons.append(self.get_child_comparison(objs_a[map_backwards[b_idx]], b_child))\n\nfor a_idx, a_child in objs_a.items():\n    if a_idx in deleted:\n        comparisons.append(self.get_child_comparison(a_child, None))\n\nreturn comparisons", "path": "wagtail/wagtail/admin/compare.py", "commit_date": "2020-09-23 00:00:00", "repo_name": "wagtail/wagtail", "stars": 16885, "license": "bsd-3-clause", "language": "python", "size": 206512}
{"docstring": "\"\"\"\nReturns a list of comparisons for all the fields in this object.\nFields that haven't changed are included as well.\n\"\"\"\n", "func_signal": "def get_field_comparisons(self):\n", "code": "comparisons = []\n\nif self.is_addition() or self.is_deletion():\n    # Display the fields without diff as one of the versions are missing\n    obj = self.obj_a or self.obj_b\n\n    for field_comparison in self.field_comparisons:\n        comparisons.append(field_comparison(obj, obj))\nelse:\n    for field_comparison in self.field_comparisons:\n        comparisons.append(field_comparison(self.obj_a, self.obj_b))\n\nreturn comparisons", "path": "wagtail/wagtail/admin/compare.py", "commit_date": "2020-09-23 00:00:00", "repo_name": "wagtail/wagtail", "stars": 16885, "license": "bsd-3-clause", "language": "python", "size": 206512}
{"docstring": "\"\"\"\nReturns a label for this field to be displayed to the user\n\"\"\"\n", "func_signal": "def field_label(self):\n", "code": "verbose_name = getattr(self.field, 'verbose_name', None)\n\nif verbose_name is None:\n    # Relations don't have a verbose_name\n    verbose_name = self.field.name.replace('_', ' ')\n\nreturn capfirst(verbose_name)", "path": "wagtail/wagtail/admin/compare.py", "commit_date": "2020-09-23 00:00:00", "repo_name": "wagtail/wagtail", "stars": 16885, "license": "bsd-3-clause", "language": "python", "size": 206512}
{"docstring": "'''Manage the `OutputCapturer`'s context.'''\n", "func_signal": "def __enter__(self):\n", "code": "self._stdout_temp_setter.__enter__()\nself._stderr_temp_setter.__enter__()\nreturn self", "path": "PySnooper/tests/mini_toolbox/__init__.py", "commit_date": "2020-05-05 00:00:00", "repo_name": "cool-RR/PySnooper", "stars": 16209, "license": "mit", "language": "python", "size": 495}
{"docstring": "\"\"\"\nRename this path to the given path.\n\"\"\"\n", "func_signal": "def rename(self, target):\n", "code": "if self._closed:\n    self._raise_closed()\nself._accessor.rename(self, target)", "path": "PySnooper/tests/mini_toolbox/pathlib.py", "commit_date": "2019-06-15 00:00:00", "repo_name": "cool-RR/PySnooper", "stars": 16209, "license": "mit", "language": "python", "size": 495}
{"docstring": "# We don't store the instance to avoid reference cycles\n", "func_signal": "def __init__(self, path):\n", "code": "self._pathcls = type(path)\nself._drv = path._drv\nself._root = path._root\nself._parts = path._parts", "path": "PySnooper/tests/mini_toolbox/pathlib.py", "commit_date": "2019-06-15 00:00:00", "repo_name": "cool-RR/PySnooper", "stars": 16209, "license": "mit", "language": "python", "size": 495}
{"docstring": "# py2 => minimal unicode support\n", "func_signal": "def _py2_fsencode(parts):\n", "code": "assert six.PY2\nreturn [part.encode('ascii') if isinstance(part, six.text_type)\n        else part for part in parts]", "path": "PySnooper/tests/mini_toolbox/pathlib.py", "commit_date": "2019-06-15 00:00:00", "repo_name": "cool-RR/PySnooper", "stars": 16209, "license": "mit", "language": "python", "size": 495}
{"docstring": "\"\"\"\nLike stat(), except if the path points to a symlink, the symlink's\nstatus information is returned, rather than its target's.\n\"\"\"\n", "func_signal": "def lstat(self):\n", "code": "if self._closed:\n    self._raise_closed()\nreturn self._accessor.lstat(self)", "path": "PySnooper/tests/mini_toolbox/pathlib.py", "commit_date": "2019-06-15 00:00:00", "repo_name": "cool-RR/PySnooper", "stars": 16209, "license": "mit", "language": "python", "size": 495}
{"docstring": "\"\"\"Iterate over all child paths of `parent_path` matched by this\nselector.  This can contain parent_path itself.\"\"\"\n", "func_signal": "def select_from(self, parent_path):\n", "code": "path_cls = type(parent_path)\nis_dir = path_cls.is_dir\nexists = path_cls.exists\nscandir = parent_path._accessor.scandir\nif not is_dir(parent_path):\n    return iter([])\nreturn self._select_from(parent_path, is_dir, exists, scandir)", "path": "PySnooper/tests/mini_toolbox/pathlib.py", "commit_date": "2019-06-15 00:00:00", "repo_name": "cool-RR/PySnooper", "stars": 16209, "license": "mit", "language": "python", "size": 495}
{"docstring": "\"\"\"Return a new path with the file name changed.\"\"\"\n", "func_signal": "def with_name(self, name):\n", "code": "if not self.name:\n    raise ValueError(\"%r has an empty name\" % (self,))\ndrv, root, parts = self._flavour.parse_parts((name,))\nif (not name or name[-1] in [self._flavour.sep, self._flavour.altsep]\n        or drv or root or len(parts) != 1):\n    raise ValueError(\"Invalid name %r\" % (name))\nreturn self._from_parsed_parts(self._drv, self._root,\n                               self._parts[:-1] + [name])", "path": "PySnooper/tests/mini_toolbox/pathlib.py", "commit_date": "2019-06-15 00:00:00", "repo_name": "cool-RR/PySnooper", "stars": 16209, "license": "mit", "language": "python", "size": 495}
{"docstring": "\"\"\"Return the string representation of the path, suitable for\npassing to system calls.\"\"\"\n", "func_signal": "def __str__(self):\n", "code": "try:\n    return self._str\nexcept AttributeError:\n    self._str = self._format_parsed_parts(self._drv, self._root,\n                                          self._parts) or '.'\n    return self._str", "path": "PySnooper/tests/mini_toolbox/pathlib.py", "commit_date": "2019-06-15 00:00:00", "repo_name": "cool-RR/PySnooper", "stars": 16209, "license": "mit", "language": "python", "size": 495}
{"docstring": "# This is an optimization used for dir walking.  `part` must be\n# a single part relative to this path.\n", "func_signal": "def _make_child_relpath(self, part):\n", "code": "parts = self._parts + [part]\nreturn self._from_parsed_parts(self._drv, self._root, parts)", "path": "PySnooper/tests/mini_toolbox/pathlib.py", "commit_date": "2019-06-15 00:00:00", "repo_name": "cool-RR/PySnooper", "stars": 16209, "license": "mit", "language": "python", "size": 495}
{"docstring": "\"\"\"\nRename this path to the given path, clobbering the existing\ndestination if it exists.\n\"\"\"\n", "func_signal": "def replace(self, target):\n", "code": "if sys.version_info < (3, 3):\n    raise NotImplementedError(\"replace() is only available \"\n                              \"with Python 3.3 and later\")\nif self._closed:\n    self._raise_closed()\nself._accessor.replace(self, target)", "path": "PySnooper/tests/mini_toolbox/pathlib.py", "commit_date": "2019-06-15 00:00:00", "repo_name": "cool-RR/PySnooper", "stars": 16209, "license": "mit", "language": "python", "size": 495}
{"docstring": "\"\"\"Helper to correctly register callbacks to __exit__ methods\"\"\"\n", "func_signal": "def _push_cm_exit(self, cm, cm_exit):\n", "code": "def _exit_wrapper(*exc_details):\n    return cm_exit(cm, *exc_details)\n_exit_wrapper.__self__ = cm\nself.push(_exit_wrapper)", "path": "PySnooper/tests/mini_toolbox/contextlib.py", "commit_date": "2019-06-15 00:00:00", "repo_name": "cool-RR/PySnooper", "stars": 16209, "license": "mit", "language": "python", "size": 495}
{"docstring": "# NOTE: the rules for reserved names seem somewhat complicated\n# (e.g. r\"..\\NUL\" is reserved but not r\"foo\\NUL\").\n# We err on the side of caution and return True for paths which are\n# not considered reserved by Windows.\n", "func_signal": "def is_reserved(self, parts):\n", "code": "if not parts:\n    return False\nif parts[0].startswith('\\\\\\\\'):\n    # UNC paths are never reserved\n    return False\nreturn parts[-1].partition('.')[0].upper() in self.reserved_names", "path": "PySnooper/tests/mini_toolbox/pathlib.py", "commit_date": "2019-06-15 00:00:00", "repo_name": "cool-RR/PySnooper", "stars": 16209, "license": "mit", "language": "python", "size": 495}
{"docstring": "\"\"\"The logical parent of the path.\"\"\"\n", "func_signal": "def parent(self):\n", "code": "drv = self._drv\nroot = self._root\nparts = self._parts\nif len(parts) == 1 and (drv or root):\n    return self\nreturn self._from_parsed_parts(drv, root, parts[:-1])", "path": "PySnooper/tests/mini_toolbox/pathlib.py", "commit_date": "2019-06-15 00:00:00", "repo_name": "cool-RR/PySnooper", "stars": 16209, "license": "mit", "language": "python", "size": 495}
{"docstring": "\"\"\"Return the path as a 'file' URI.\"\"\"\n", "func_signal": "def as_uri(self):\n", "code": "if not self.is_absolute():\n    raise ValueError(\"relative path can't be expressed as a file URI\")\nreturn self._flavour.make_uri(self)", "path": "PySnooper/tests/mini_toolbox/pathlib.py", "commit_date": "2019-06-15 00:00:00", "repo_name": "cool-RR/PySnooper", "stars": 16209, "license": "mit", "language": "python", "size": 495}
{"docstring": "\"\"\"\nMake the path absolute, resolving all symlinks on the way and also\nnormalizing it (for example turning slashes into backslashes under\nWindows).\n\"\"\"\n", "func_signal": "def resolve(self, strict=False):\n", "code": "if self._closed:\n    self._raise_closed()\ns = self._flavour.resolve(self, strict=strict)\nif s is None:\n    # No symlink resolution => for consistency, raise an error if\n    # the path doesn't exist or is forbidden\n    self.stat()\n    s = str(self.absolute())\n# Now we have no symlinks in the path, it's safe to normalize it.\nnormed = self._flavour.pathmod.normpath(s)\nobj = self._from_parts((normed,), init=False)\nobj._init(template=self)\nreturn obj", "path": "PySnooper/tests/mini_toolbox/pathlib.py", "commit_date": "2019-06-15 00:00:00", "repo_name": "cool-RR/PySnooper", "stars": 16209, "license": "mit", "language": "python", "size": 495}
{"docstring": "\"\"\"Registers an arbitrary callback and arguments.\n\nCannot suppress exceptions.\n\"\"\"\n", "func_signal": "def callback(self, callback, *args, **kwds):\n", "code": "def _exit_wrapper(exc_type, exc, tb):\n    callback(*args, **kwds)\n# We changed the signature, so using @wraps is not appropriate, but\n# setting __wrapped__ may still help with introspection\n_exit_wrapper.__wrapped__ = callback\nself.push(_exit_wrapper)\nreturn callback # Allow use as a decorator", "path": "PySnooper/tests/mini_toolbox/contextlib.py", "commit_date": "2019-06-15 00:00:00", "repo_name": "cool-RR/PySnooper", "stars": 16209, "license": "mit", "language": "python", "size": 495}
{"docstring": "\"\"\"\nWhether this path is a symbolic link.\n\"\"\"\n", "func_signal": "def is_symlink(self):\n", "code": "try:\n    return S_ISLNK(self.lstat().st_mode)\nexcept OSError as e:\n    if e.errno not in (ENOENT, ENOTDIR):\n        raise\n    # Path doesn't exist\n    return False", "path": "PySnooper/tests/mini_toolbox/pathlib.py", "commit_date": "2019-06-15 00:00:00", "repo_name": "cool-RR/PySnooper", "stars": 16209, "license": "mit", "language": "python", "size": 495}
{"docstring": "\"\"\"Return whether other_path is the same or not as this file\n(as returned by os.path.samefile()).\n\"\"\"\n", "func_signal": "def samefile(self, other_path):\n", "code": "if hasattr(os.path, \"samestat\"):\n    st = self.stat()\n    try:\n        other_st = other_path.stat()\n    except AttributeError:\n        other_st = os.stat(other_path)\n    return os.path.samestat(st, other_st)\nelse:\n    filename1 = six.text_type(self)\n    filename2 = six.text_type(other_path)\n    st1 = _win32_get_unique_path_id(filename1)\n    st2 = _win32_get_unique_path_id(filename2)\n    return st1 == st2", "path": "PySnooper/tests/mini_toolbox/pathlib.py", "commit_date": "2019-06-15 00:00:00", "repo_name": "cool-RR/PySnooper", "stars": 16209, "license": "mit", "language": "python", "size": 495}
{"docstring": "\"\"\"The final component's last suffix, if any.\"\"\"\n", "func_signal": "def suffix(self):\n", "code": "name = self.name\ni = name.rfind('.')\nif 0 < i < len(name) - 1:\n    return name[i:]\nelse:\n    return ''", "path": "PySnooper/tests/mini_toolbox/pathlib.py", "commit_date": "2019-06-15 00:00:00", "repo_name": "cool-RR/PySnooper", "stars": 16209, "license": "mit", "language": "python", "size": 495}
{"docstring": "# We need to call _parse_args on the instance, so as to get the\n# right flavour.\n", "func_signal": "def _from_parts(cls, args, init=True):\n", "code": "self = object.__new__(cls)\ndrv, root, parts = self._parse_args(args)\nself._drv = drv\nself._root = root\nself._parts = parts\nif init:\n    self._init()\nreturn self", "path": "PySnooper/tests/mini_toolbox/pathlib.py", "commit_date": "2019-06-15 00:00:00", "repo_name": "cool-RR/PySnooper", "stars": 16209, "license": "mit", "language": "python", "size": 495}
{"docstring": "# Generate moving averages of all losses and associated summaries.\n", "func_signal": "def train(total_loss, global_step, optimizer, learning_rate, moving_average_decay, update_gradient_vars, log_histograms=True):\n", "code": "loss_averages_op = _add_loss_summaries(total_loss)\n\n# Compute gradients.\nwith tf.control_dependencies([loss_averages_op]):\n    if optimizer=='ADAGRAD':\n        opt = tf.train.AdagradOptimizer(learning_rate)\n    elif optimizer=='ADADELTA':\n        opt = tf.train.AdadeltaOptimizer(learning_rate, rho=0.9, epsilon=1e-6)\n    elif optimizer=='ADAM':\n        opt = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999, epsilon=0.1)\n    elif optimizer=='RMSPROP':\n        opt = tf.train.RMSPropOptimizer(learning_rate, decay=0.9, momentum=0.9, epsilon=1.0)\n    elif optimizer=='MOM':\n        opt = tf.train.MomentumOptimizer(learning_rate, 0.9, use_nesterov=True)\n    else:\n        raise ValueError('Invalid optimization algorithm')\n\n    grads = opt.compute_gradients(total_loss, update_gradient_vars)\n    \n# Apply gradients.\napply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n  \n# Add histograms for trainable variables.\nif log_histograms:\n    for var in tf.trainable_variables():\n        tf.summary.histogram(var.op.name, var)\n   \n# Add histograms for gradients.\nif log_histograms:\n    for grad, var in grads:\n        if grad is not None:\n            tf.summary.histogram(var.op.name + '/gradients', grad)\n  \n# Track the moving averages of all trainable variables.\nvariable_averages = tf.train.ExponentialMovingAverage(\n    moving_average_decay, global_step)\nvariables_averages_op = variable_averages.apply(tf.trainable_variables())\n  \nwith tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n    train_op = tf.no_op(name='train')\n  \nreturn train_op", "path": "facenet/src/facenet.py", "commit_date": "2018-04-07 00:00:00", "repo_name": "davidsandberg/facenet", "stars": 13421, "license": "mit", "language": "python", "size": 2991}
{"docstring": "\"\"\"Calculate the triplet loss according to the FaceNet paper\n\nArgs:\n  anchor: the embeddings for the anchor images.\n  positive: the embeddings for the positive images.\n  negative: the embeddings for the negative images.\n  \nReturns:\n  the triplet loss according to the FaceNet paper as a float tensor.\n\"\"\"\n", "func_signal": "def triplet_loss(anchor, positive, negative, alpha):\n", "code": "with tf.variable_scope('triplet_loss'):\n    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), 1)\n    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), 1)\n    \n    basic_loss = tf.add(tf.subtract(pos_dist,neg_dist), alpha)\n    loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0), 0)\n  \nreturn loss", "path": "facenet/src/facenet.py", "commit_date": "2018-04-07 00:00:00", "repo_name": "davidsandberg/facenet", "stars": 13421, "license": "mit", "language": "python", "size": 2991}
{"docstring": "\"\"\"Builds the 35x35 resnet block.\"\"\"\n", "func_signal": "def block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n", "code": "with tf.variable_scope(scope, 'Block35', [net], reuse=reuse):\n    with tf.variable_scope('Branch_0'):\n        tower_conv = slim.conv2d(net, 32, 1, scope='Conv2d_1x1')\n    with tf.variable_scope('Branch_1'):\n        tower_conv1_0 = slim.conv2d(net, 32, 1, scope='Conv2d_0a_1x1')\n        tower_conv1_1 = slim.conv2d(tower_conv1_0, 32, 3, scope='Conv2d_0b_3x3')\n    with tf.variable_scope('Branch_2'):\n        tower_conv2_0 = slim.conv2d(net, 32, 1, scope='Conv2d_0a_1x1')\n        tower_conv2_1 = slim.conv2d(tower_conv2_0, 32, 3, scope='Conv2d_0b_3x3')\n        tower_conv2_2 = slim.conv2d(tower_conv2_1, 32, 3, scope='Conv2d_0c_3x3')\n    mixed = tf.concat([tower_conv, tower_conv1_1, tower_conv2_2], 3)\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope='Conv2d_1x1')\n    net += scale * up\n    if activation_fn:\n        net = activation_fn(net)\nreturn net", "path": "facenet/src/models/inception_resnet_v1.py", "commit_date": "2018-03-31 00:00:00", "repo_name": "davidsandberg/facenet", "stars": 13421, "license": "mit", "language": "python", "size": 2991}
{"docstring": "# Save the model checkpoint\n", "func_signal": "def save_variables_and_metagraph(sess, saver, summary_writer, model_dir, model_name, step):\n", "code": "print('Saving variables')\nstart_time = time.time()\ncheckpoint_path = os.path.join(model_dir, 'model-%s.ckpt' % model_name)\nsaver.save(sess, checkpoint_path, global_step=step, write_meta_graph=False)\nsave_time_variables = time.time() - start_time\nprint('Variables saved in %.2f seconds' % save_time_variables)\nmetagraph_filename = os.path.join(model_dir, 'model-%s.meta' % model_name)\nsave_time_metagraph = 0  \nif not os.path.exists(metagraph_filename):\n    print('Saving metagraph')\n    start_time = time.time()\n    saver.export_meta_graph(metagraph_filename)\n    save_time_metagraph = time.time() - start_time\n    print('Metagraph saved in %.2f seconds' % save_time_metagraph)\nsummary = tf.Summary()\n#pylint: disable=maybe-no-member\nsummary.value.add(tag='time/save_variables', simple_value=save_time_variables)\nsummary.value.add(tag='time/save_metagraph', simple_value=save_time_metagraph)\nsummary_writer.add_summary(summary, step)", "path": "facenet/src/train_softmax.py", "commit_date": "2018-04-07 00:00:00", "repo_name": "davidsandberg/facenet", "stars": 13421, "license": "mit", "language": "python", "size": 2991}
{"docstring": "\"\"\"Center loss based on the paper \"A Discriminative Feature Learning Approach for Deep Face Recognition\"\n   (http://ydwen.github.io/papers/WenECCV16.pdf)\n\"\"\"\n", "func_signal": "def center_loss(features, label, alfa, nrof_classes):\n", "code": "nrof_features = features.get_shape()[1]\ncenters = tf.get_variable('centers', [nrof_classes, nrof_features], dtype=tf.float32,\n    initializer=tf.constant_initializer(0), trainable=False)\nlabel = tf.reshape(label, [-1])\ncenters_batch = tf.gather(centers, label)\ndiff = (1 - alfa) * (centers_batch - features)\ncenters = tf.scatter_sub(centers, label, diff)\nwith tf.control_dependencies([centers]):\n    loss = tf.reduce_mean(tf.square(features - centers_batch))\nreturn loss, centers", "path": "facenet/src/facenet.py", "commit_date": "2018-04-07 00:00:00", "repo_name": "davidsandberg/facenet", "stars": 13421, "license": "mit", "language": "python", "size": 2991}
{"docstring": "\"\"\"Return the type of the activations, weights, and placeholder variables.\"\"\"\n", "func_signal": "def data_type():\n", "code": "if FLAGS.use_fp16:\n    return tf.float16\nelse:\n    return tf.float32", "path": "facenet/tmp/mnist_center_loss.py", "commit_date": "2017-08-02 00:00:00", "repo_name": "davidsandberg/facenet", "stars": 13421, "license": "mit", "language": "python", "size": 2991}
{"docstring": "\"\"\"Get all predictions for a dataset by running it in small batches.\"\"\"\n", "func_signal": "def calculate_embeddings(data, sess):\n", "code": "size = data.shape[0]\nif size < EVAL_BATCH_SIZE:\n    raise ValueError(\"batch size for evals larger than dataset: %d\" % size)\npredictions = np.ndarray(shape=(size, 2), dtype=np.float32)\nfor begin in xrange(0, size, EVAL_BATCH_SIZE):\n    end = begin + EVAL_BATCH_SIZE\n    if end <= size:\n        predictions[begin:end, :] = sess.run(\n            eval_embeddings,\n            feed_dict={eval_data: data[begin:end, ...]})\n    else:\n        batch_predictions = sess.run(\n            eval_embeddings,\n            feed_dict={eval_data: data[-EVAL_BATCH_SIZE:, ...]})\n        predictions[begin:, :] = batch_predictions[begin - size:, :]\nreturn predictions", "path": "facenet/tmp/mnist_center_loss.py", "commit_date": "2017-08-02 00:00:00", "repo_name": "davidsandberg/facenet", "stars": 13421, "license": "mit", "language": "python", "size": 2991}
{"docstring": "\"\"\"Return the error rate based on dense predictions and sparse labels.\"\"\"\n", "func_signal": "def error_rate(predictions, labels):\n", "code": "return 100.0 - (\n    100.0 *\n    np.sum(np.argmax(predictions, 1) == labels) /\n    predictions.shape[0])", "path": "facenet/tmp/mnist_center_loss.py", "commit_date": "2017-08-02 00:00:00", "repo_name": "davidsandberg/facenet", "stars": 13421, "license": "mit", "language": "python", "size": 2991}
{"docstring": "\"\"\"Add summaries for losses.\n  \nGenerates moving average for all losses and associated summaries for\nvisualizing the performance of the network.\n  \nArgs:\n  total_loss: Total loss from loss().\nReturns:\n  loss_averages_op: op for generating moving averages of losses.\n\"\"\"\n# Compute the moving average of all individual losses and the total loss.\n", "func_signal": "def _add_loss_summaries(total_loss):\n", "code": "loss_averages = tf.train.ExponentialMovingAverage(0.9, name='avg')\nlosses = tf.get_collection('losses')\nloss_averages_op = loss_averages.apply(losses + [total_loss])\n  \n# Attach a scalar summmary to all individual losses and the total loss; do the\n# same for the averaged version of the losses.\nfor l in losses + [total_loss]:\n    # Name each loss as '(raw)' and name the moving average version of the loss\n    # as the original loss name.\n    tf.summary.scalar(l.op.name +' (raw)', l)\n    tf.summary.scalar(l.op.name, loss_averages.average(l))\n  \nreturn loss_averages_op", "path": "facenet/src/facenet.py", "commit_date": "2018-04-07 00:00:00", "repo_name": "davidsandberg/facenet", "stars": 13421, "license": "mit", "language": "python", "size": 2991}
{"docstring": "\"\"\"Builds the 8x8 resnet block.\"\"\"\n", "func_signal": "def block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n", "code": "with tf.variable_scope(scope, 'Block8', [net], reuse=reuse):\n    with tf.variable_scope('Branch_0'):\n        tower_conv = slim.conv2d(net, 192, 1, scope='Conv2d_1x1')\n    with tf.variable_scope('Branch_1'):\n        tower_conv1_0 = slim.conv2d(net, 192, 1, scope='Conv2d_0a_1x1')\n        tower_conv1_1 = slim.conv2d(tower_conv1_0, 224, [1, 3],\n                                    scope='Conv2d_0b_1x3')\n        tower_conv1_2 = slim.conv2d(tower_conv1_1, 256, [3, 1],\n                                    scope='Conv2d_0c_3x1')\n    mixed = tf.concat([tower_conv, tower_conv1_2], 3)\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope='Conv2d_1x1')\n    net += scale * up\n    if activation_fn:\n        net = activation_fn(net)\nreturn net", "path": "facenet/src/models/inception_resnet_v2.py", "commit_date": "2018-03-31 00:00:00", "repo_name": "davidsandberg/facenet", "stars": 13421, "license": "mit", "language": "python", "size": 2991}
{"docstring": "\"\"\"\nBatch normalization on convolutional maps.\nArgs:\n    x:           Tensor, 4D BHWD input maps\n    n_out:       integer, depth of input maps\n    phase_train: boolean tf.Variable, true indicates training phase\n    scope:       string, variable scope\n    affn:      whether to affn-transform outputs\nReturn:\n    normed:      batch-normalized maps\nRef: http://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/33950177\n\"\"\"\n", "func_signal": "def batch_norm(x, phase_train):\n", "code": "name = 'batch_norm'\nwith tf.variable_scope(name):\n    phase_train = tf.convert_to_tensor(phase_train, dtype=tf.bool)\n    n_out = int(x.get_shape()[3])\n    beta = tf.Variable(tf.constant(0.0, shape=[n_out], dtype=x.dtype),\n                       name=name+'/beta', trainable=True, dtype=x.dtype)\n    gamma = tf.Variable(tf.constant(1.0, shape=[n_out], dtype=x.dtype),\n                        name=name+'/gamma', trainable=True, dtype=x.dtype)\n  \n    batch_mean, batch_var = tf.nn.moments(x, [0,1,2], name='moments')\n    ema = tf.train.ExponentialMovingAverage(decay=0.9)\n    def mean_var_with_update():\n        ema_apply_op = ema.apply([batch_mean, batch_var])\n        with tf.control_dependencies([ema_apply_op]):\n            return tf.identity(batch_mean), tf.identity(batch_var)\n    mean, var = control_flow_ops.cond(phase_train,\n                                      mean_var_with_update,\n                                      lambda: (ema.average(batch_mean), ema.average(batch_var)))\n    normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-3)\nreturn normed", "path": "facenet/tmp/network.py", "commit_date": "2017-05-09 00:00:00", "repo_name": "davidsandberg/facenet", "stars": 13421, "license": "mit", "language": "python", "size": 2991}
{"docstring": "\"\"\"Extract the labels into a vector of int64 label IDs.\"\"\"\n", "func_signal": "def extract_labels(filename, num_images):\n", "code": "print('Extracting', filename)\nwith gzip.open(filename) as bytestream:\n    bytestream.read(8)\n    buf = bytestream.read(1 * num_images)\n    labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\nreturn labels", "path": "facenet/tmp/mnist_center_loss.py", "commit_date": "2017-08-02 00:00:00", "repo_name": "davidsandberg/facenet", "stars": 13421, "license": "mit", "language": "python", "size": 2991}
{"docstring": "\"\"\"Extract the images into a 4D tensor [image index, y, x, channels].\nValues are rescaled from [0, 255] down to [-0.5, 0.5].\n\"\"\"\n", "func_signal": "def extract_data(filename, num_images):\n", "code": "print('Extracting', filename)\nwith gzip.open(filename) as bytestream:\n    bytestream.read(16)\n    buf = bytestream.read(IMAGE_SIZE * IMAGE_SIZE * num_images * NUM_CHANNELS)\n    data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n    data = (data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH\n    data = data.reshape(num_images, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)\n    return data", "path": "facenet/tmp/mnist_center_loss.py", "commit_date": "2017-08-02 00:00:00", "repo_name": "davidsandberg/facenet", "stars": 13421, "license": "mit", "language": "python", "size": 2991}
{"docstring": "\"\"\"Download the data from Yann's website, unless it's already here.\"\"\"\n", "func_signal": "def maybe_download(filename):\n", "code": "if not tf.gfile.Exists(WORK_DIRECTORY):\n    tf.gfile.MakeDirs(WORK_DIRECTORY)\nfilepath = os.path.join(WORK_DIRECTORY, filename)\nif not tf.gfile.Exists(filepath):\n    filepath, _ = urllib.request.urlretrieve(SOURCE_URL + filename, filepath)\n    with tf.gfile.GFile(filepath) as f:\n        size = f.size()\n    print('Successfully downloaded', filename, size, 'bytes.')\nreturn filepath", "path": "facenet/tmp/mnist_center_loss.py", "commit_date": "2017-08-02 00:00:00", "repo_name": "davidsandberg/facenet", "stars": 13421, "license": "mit", "language": "python", "size": 2991}
{"docstring": "\"\"\"Builds the 17x17 resnet block.\"\"\"\n", "func_signal": "def block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n", "code": "with tf.variable_scope(scope, 'Block17', [net], reuse=reuse):\n    with tf.variable_scope('Branch_0'):\n        tower_conv = slim.conv2d(net, 128, 1, scope='Conv2d_1x1')\n    with tf.variable_scope('Branch_1'):\n        tower_conv1_0 = slim.conv2d(net, 128, 1, scope='Conv2d_0a_1x1')\n        tower_conv1_1 = slim.conv2d(tower_conv1_0, 128, [1, 7],\n                                    scope='Conv2d_0b_1x7')\n        tower_conv1_2 = slim.conv2d(tower_conv1_1, 128, [7, 1],\n                                    scope='Conv2d_0c_7x1')\n    mixed = tf.concat([tower_conv, tower_conv1_2], 3)\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope='Conv2d_1x1')\n    net += scale * up\n    if activation_fn:\n        net = activation_fn(net)\nreturn net", "path": "facenet/src/models/inception_resnet_v1.py", "commit_date": "2018-03-31 00:00:00", "repo_name": "davidsandberg/facenet", "stars": 13421, "license": "mit", "language": "python", "size": 2991}
{"docstring": "\"\"\"Builds the 35x35 resnet block.\"\"\"\n", "func_signal": "def block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n", "code": "with tf.variable_scope(scope, 'Block35', [net], reuse=reuse):\n    with tf.variable_scope('Branch_0'):\n        tower_conv = slim.conv2d(net, 32, 1, scope='Conv2d_1x1')\n    with tf.variable_scope('Branch_1'):\n        tower_conv1_0 = slim.conv2d(net, 32, 1, scope='Conv2d_0a_1x1')\n        tower_conv1_1 = slim.conv2d(tower_conv1_0, 32, 3, scope='Conv2d_0b_3x3')\n    with tf.variable_scope('Branch_2'):\n        tower_conv2_0 = slim.conv2d(net, 32, 1, scope='Conv2d_0a_1x1')\n        tower_conv2_1 = slim.conv2d(tower_conv2_0, 48, 3, scope='Conv2d_0b_3x3')\n        tower_conv2_2 = slim.conv2d(tower_conv2_1, 64, 3, scope='Conv2d_0c_3x3')\n    mixed = tf.concat([tower_conv, tower_conv1_1, tower_conv2_2], 3)\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope='Conv2d_1x1')\n    net += scale * up\n    if activation_fn:\n        net = activation_fn(net)\nreturn net", "path": "facenet/src/models/inception_resnet_v2.py", "commit_date": "2018-03-31 00:00:00", "repo_name": "davidsandberg/facenet", "stars": 13421, "license": "mit", "language": "python", "size": 2991}
{"docstring": "\"\"\"Define a L2Loss, useful for regularize, i.e. weight decay.\nArgs:\n  tensor: tensor to regularize.\n  weight: an optional weight to modulate the loss.\n  scope: Optional scope for op_scope.\nReturns:\n  the L2 loss op.\n\"\"\"\n", "func_signal": "def l2_loss(tensor, weight=1.0, scope=None):\n", "code": "with tf.name_scope(scope):\n    weight = tf.convert_to_tensor(weight,\n                                  dtype=tensor.dtype.base_dtype,\n                                  name='loss_weight')\n    loss = tf.multiply(weight, tf.nn.l2_loss(tensor), name='value')\nreturn loss", "path": "facenet/tmp/network.py", "commit_date": "2017-05-09 00:00:00", "repo_name": "davidsandberg/facenet", "stars": 13421, "license": "mit", "language": "python", "size": 2991}
{"docstring": "\"\"\" Define an inference network for face recognition based \n       on inception modules using batch normalization\n\nArgs:\n  images: The images to run inference on, dimensions batch_size x height x width x channels\n  phase_train: True if batch normalization should operate in training mode\n\"\"\"\n", "func_signal": "def inference(images, keep_probability, phase_train=True, weight_decay=0.0):\n", "code": "endpoints = {}\nnet = network.conv(images, 3, 64, 7, 7, 2, 2, 'SAME', 'conv1_7x7', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\nendpoints['conv1'] = net\nnet = network.mpool(net,  3, 3, 2, 2, 'SAME', 'pool1')\nendpoints['pool1'] = net\nnet = network.conv(net,  64, 64, 1, 1, 1, 1, 'SAME', 'conv2_1x1', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\nendpoints['conv2_1x1'] = net\nnet = network.conv(net,  64, 192, 3, 3, 1, 1, 'SAME', 'conv3_3x3', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\nendpoints['conv3_3x3'] = net\nnet = network.mpool(net,  3, 3, 2, 2, 'SAME', 'pool3')\nendpoints['pool3'] = net\n  \nnet = network.inception(net, 192, 1, 64, 96, 128, 16, 32, 3, 32, 1, 'MAX', 'incept3a', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\nendpoints['incept3a'] = net\nnet = network.inception(net, 256, 1, 64, 96, 128, 32, 64, 3, 64, 1, 'MAX', 'incept3b', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\nendpoints['incept3b'] = net\nnet = network.inception(net, 320, 2, 0, 128, 256, 32, 64, 3, 0, 2, 'MAX', 'incept3c', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\nendpoints['incept3c'] = net\n\nnet = network.inception(net, 640, 1, 256, 96, 192, 32, 64, 3, 128, 1, 'MAX', 'incept4a', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\nendpoints['incept4a'] = net\nnet = network.inception(net, 640, 1, 224, 112, 224, 32, 64, 3, 128, 1, 'MAX', 'incept4b', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\nendpoints['incept4b'] = net\nnet = network.inception(net, 640, 1, 192, 128, 256, 32, 64, 3, 128, 1, 'MAX', 'incept4c', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\nendpoints['incept4c'] = net\nnet = network.inception(net, 640, 1, 160, 144, 288, 32, 64, 3, 128, 1, 'MAX', 'incept4d', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\nendpoints['incept4d'] = net\nnet = network.inception(net, 640, 2, 0, 160, 256, 64, 128, 3, 0, 2, 'MAX', 'incept4e', phase_train=phase_train, use_batch_norm=True)\nendpoints['incept4e'] = net\n\nnet = network.inception(net, 1024, 1, 384, 192, 384, 0, 0, 3, 128, 1, 'MAX', 'incept5a', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\nendpoints['incept5a'] = net\nnet = network.inception(net, 896, 1, 384, 192, 384, 0, 0, 3, 128, 1, 'MAX', 'incept5b', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\nendpoints['incept5b'] = net\nnet = network.apool(net,  3, 3, 1, 1, 'VALID', 'pool6')\nendpoints['pool6'] = net\nnet = tf.reshape(net, [-1, 896])\nendpoints['prelogits'] = net\nnet = tf.nn.dropout(net, keep_probability)\nendpoints['dropout'] = net\n\nreturn net, endpoints", "path": "facenet/tmp/nn4.py", "commit_date": "2017-05-09 00:00:00", "repo_name": "davidsandberg/facenet", "stars": 13421, "license": "mit", "language": "python", "size": 2991}
{"docstring": "# Map label to center\n#     label_to_center_dict = { \n#          0:(-3,-3),  1:(-3,-1),  2:(-3,1),  3:(-3,3),\n#          4:(-1,-3),  5:(-1,-1),  6:(-1,1),  7:(-1,3),\n#          8:( 1,-3),  9:( 1,-1), 10:( 1,1), 11:( 1,3),\n#         12:( 3,-3), 13:( 3,-1), 14:( 3,1), 15:( 3,3),\n#         }\n# Create array of features corresponding to the labels\n", "func_signal": "def create_features(label_to_center, batch_size, nrof_features, labels):\n", "code": "feats = np.zeros((batch_size, nrof_features))\nfor i in range(batch_size):\n    cntr =  label_to_center[labels[i]]\n    for j in range(nrof_features):\n        feats[i,j] = cntr[j]\nreturn feats", "path": "facenet/test/center_loss_test.py", "commit_date": "2017-01-19 00:00:00", "repo_name": "davidsandberg/facenet", "stars": 13421, "license": "mit", "language": "python", "size": 2991}
{"docstring": "\"\"\" Define an inference network for face recognition based \n       on inception modules using batch normalization\n\nArgs:\n  images: The images to run inference on, dimensions batch_size x height x width x channels\n  phase_train: True if batch normalization should operate in training mode\n\"\"\"\n", "func_signal": "def inference(images, keep_probability, phase_train=True, weight_decay=0.0):\n", "code": "endpoints = {}\nnet = network.conv(images, 3, 64, 7, 7, 2, 2, 'SAME', 'conv1_7x7', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\nendpoints['conv1'] = net\nnet = network.mpool(net,  3, 3, 2, 2, 'SAME', 'pool1')\nendpoints['pool1'] = net\nnet = network.conv(net,  64, 64, 1, 1, 1, 1, 'SAME', 'conv2_1x1', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\nendpoints['conv2_1x1'] = net\nnet = network.conv(net,  64, 192, 3, 3, 1, 1, 'SAME', 'conv3_3x3', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\nendpoints['conv3_3x3'] = net\nnet = network.mpool(net,  3, 3, 2, 2, 'SAME', 'pool3')\nendpoints['pool3'] = net\n  \nnet = network.inception(net, 192, 1, 64, 96, 128, 16, 32, 3, 32, 1, 'MAX', 'incept3a', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\nendpoints['incept3a'] = net\nnet = network.inception(net, 256, 1, 64, 96, 128, 32, 64, 3, 64, 1, 'MAX', 'incept3b', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\nendpoints['incept3b'] = net\nnet = network.inception(net, 320, 2, 0, 128, 256, 32, 64, 3, 0, 2, 'MAX', 'incept3c', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\nendpoints['incept3c'] = net\n\nnet = network.inception(net, 640, 1, 256, 96, 192, 32, 64, 3, 128, 1, 'MAX', 'incept4a', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\nendpoints['incept4a'] = net\nnet = network.inception(net, 640, 1, 224, 112, 224, 32, 64, 3, 128, 1, 'MAX', 'incept4b', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\nendpoints['incept4b'] = net\nnet = network.inception(net, 640, 1, 192, 128, 256, 32, 64, 3, 128, 1, 'MAX', 'incept4c', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\nendpoints['incept4c'] = net\nnet = network.inception(net, 640, 1, 160, 144, 288, 32, 64, 3, 128, 1, 'MAX', 'incept4d', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\nendpoints['incept4d'] = net\nnet = network.inception(net, 640, 2, 0, 160, 256, 64, 128, 3, 0, 2, 'MAX', 'incept4e', phase_train=phase_train, use_batch_norm=True)\nendpoints['incept4e'] = net\n\nnet = network.inception(net, 1024, 1, 384, 192, 384, 48, 128, 3, 128, 1, 'MAX', 'incept5a', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\nendpoints['incept5a'] = net\nnet = network.inception(net, 1024, 1, 384, 192, 384, 48, 128, 3, 128, 1, 'MAX', 'incept5b', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\nendpoints['incept5b'] = net\nnet = network.apool(net,  7, 7, 1, 1, 'VALID', 'pool6')\nendpoints['pool6'] = net\nnet = tf.reshape(net, [-1, 1024])\nendpoints['prelogits'] = net\nnet = tf.nn.dropout(net, keep_probability)\nendpoints['dropout'] = net\n\nreturn net, endpoints", "path": "facenet/tmp/nn2.py", "commit_date": "2017-05-09 00:00:00", "repo_name": "davidsandberg/facenet", "stars": 13421, "license": "mit", "language": "python", "size": 2991}
{"docstring": "\"\"\"\n\u56de\u590d\u6d88\u606f\uff0c\u5e76\u8fd4\u56de\u7b54\u590d\u6587\u672c\n\n:param msg: Message \u5bf9\u8c61\n:return: \u7b54\u590d\u6587\u672c\n\"\"\"\n\n", "func_signal": "def do_reply(self, msg):\n", "code": "ret = self.reply_text(msg)\nmsg.reply(ret)\nreturn ret", "path": "wxpy/wxpy/ext/xiaoi.py", "commit_date": "2017-05-21 00:00:00", "repo_name": "youfou/wxpy", "stars": 13724, "license": "mit", "language": "python", "size": 600}
{"docstring": "# \u63a5\u53d7\u597d\u53cb\u8bf7\u6c42\n", "func_signal": "def auto_accept_friends(msg):\n", "code": "new_friend = msg.card.accept()\n# \u5411\u65b0\u7684\u597d\u53cb\u53d1\u9001\u6d88\u606f\nnew_friend.send('\u54c8\u54c8\uff0c\u6211\u81ea\u52a8\u63a5\u53d7\u4e86\u4f60\u7684\u597d\u53cb\u8bf7\u6c42')", "path": "wxpy/wxpy/__init__.py", "commit_date": "2017-06-27 00:00:00", "repo_name": "youfou/wxpy", "stars": 13724, "license": "mit", "language": "python", "size": 600}
{"docstring": "\"\"\"\n\u8bbe\u7f6e\u6216\u4fee\u6539\u597d\u53cb\u7684\u5907\u6ce8\u540d\u79f0\n\n:param remark_name: \u65b0\u7684\u5907\u6ce8\u540d\u79f0\n\"\"\"\n\n", "func_signal": "def set_remark_name(self, remark_name):\n", "code": "logger.info('setting remark name for {}: {}'.format(self, remark_name))\n\nreturn self.bot.core.set_alias(userName=self.user_name, alias=remark_name)", "path": "wxpy/wxpy/api/chats/user.py", "commit_date": "2017-06-05 00:00:00", "repo_name": "youfou/wxpy", "stars": 13724, "license": "mit", "language": "python", "size": 600}
{"docstring": "\"\"\"\n* \u82e5\u6d88\u606f\u6765\u81ea\u7fa4\u804a\uff0c\u5219\u6b64\u5c5e\u6027\u4e3a\u6d88\u606f\u7684\u5b9e\u9645\u53d1\u9001\u4eba(\u5177\u4f53\u7684\u7fa4\u6210\u5458)\n* \u82e5\u6d88\u606f\u6765\u81ea\u5176\u4ed6\u804a\u5929\u5bf9\u8c61(\u975e\u7fa4\u804a)\uff0c\u5219\u6b64\u5c5e\u6027\u4e3a None\n\n:rtype: NoneType, :class:`wxpy.Member`\n\"\"\"\n\n", "func_signal": "def member(self):\n", "code": "if isinstance(self.chat, Group):\n    if self.sender == self.bot.self:\n        return self.chat.self\n    else:\n        actual_user_name = self.raw.get('ActualUserName')\n        for _member in self.chat.members:\n            if _member.user_name == actual_user_name:\n                return _member\n        return Member(dict(\n            UserName=actual_user_name,\n            NickName=self.raw.get('ActualNickName')\n        ), self.chat)", "path": "wxpy/wxpy/api/messages/message.py", "commit_date": "2017-06-16 00:00:00", "repo_name": "youfou/wxpy", "stars": 13724, "license": "mit", "language": "python", "size": 600}
{"docstring": "\"\"\"\n\u6d88\u606f\u6240\u5728\u7684\u804a\u5929\u4f1a\u8bdd\uff0c\u5373:\n\n* \u5bf9\u4e8e\u81ea\u5df1\u53d1\u9001\u7684\u6d88\u606f\uff0c\u4e3a\u6d88\u606f\u7684\u63a5\u6536\u8005\n* \u5bf9\u4e8e\u522b\u4eba\u53d1\u9001\u7684\u6d88\u606f\uff0c\u4e3a\u6d88\u606f\u7684\u53d1\u9001\u8005\n\n:rtype: :class:`wxpy.User`, :class:`wxpy.Group`\n\"\"\"\n\n", "func_signal": "def chat(self):\n", "code": "if self.raw.get('FromUserName') == self.bot.self.user_name:\n    return self.receiver\nelse:\n    return self.sender", "path": "wxpy/wxpy/api/messages/message.py", "commit_date": "2017-06-16 00:00:00", "repo_name": "youfou/wxpy", "stars": 13724, "license": "mit", "language": "python", "size": 600}
{"docstring": "\"\"\"\n\u670d\u52a1\u7aef\u53d1\u9001\u65f6\u95f4\n\"\"\"\n# noinspection PyBroadException\n", "func_signal": "def create_time(self):\n", "code": "try:\n    return datetime.fromtimestamp(self.raw.get('CreateTime'))\nexcept:\n    pass", "path": "wxpy/wxpy/api/messages/message.py", "commit_date": "2017-06-16 00:00:00", "repo_name": "youfou/wxpy", "stars": 13724, "license": "mit", "language": "python", "size": 600}
{"docstring": "\"\"\"\n\u6d88\u606f\u7684\u6587\u672c\u5185\u5bb9\n\"\"\"\n", "func_signal": "def text(self):\n", "code": "_type = self.type\n_card = self.card\n\nif _type == MAP:\n    location = self.location\n    if location:\n        return location.get('label')\nelif _card:\n    if _type == CARD:\n        return _card.name\n    elif _type == FRIENDS:\n        return _card.raw.get('Content')\n\nret = self.raw.get('Text')\nif isinstance(ret, str):\n    return ret", "path": "wxpy/wxpy/api/messages/message.py", "commit_date": "2017-06-16 00:00:00", "repo_name": "youfou/wxpy", "stars": 13724, "license": "mit", "language": "python", "size": 600}
{"docstring": "\"\"\"\n\u751f\u6210\u8bf7\u6c42\u8ba4\u8bc1\n\"\"\"\n\n", "func_signal": "def _make_http_header_xauth(self):\n", "code": "sign = self._make_signature()\n\nret = {\n    \"X-Auth\": \"app_key=\\\"{0}\\\",nonce=\\\"{1}\\\",signature=\\\"{2}\\\"\".format(\n        self.key, sign.nonce, sign.signature)\n}\n\nreturn ret", "path": "wxpy/wxpy/ext/xiaoi.py", "commit_date": "2017-05-21 00:00:00", "repo_name": "youfou/wxpy", "stars": 13724, "license": "mit", "language": "python", "size": 600}
{"docstring": "\"\"\"\n| \u9700\u8981\u901a\u8fc7\u6ce8\u518c\u83b7\u5f97 key \u548c secret\n| \u514d\u8d39\u7533\u8bf7: http://cloud.xiaoi.com/\n\n:param key: \u4f60\u7533\u8bf7\u7684 key\n:param secret: \u4f60\u7533\u8bf7\u7684 secret\n\"\"\"\n\n", "func_signal": "def __init__(self, key, secret):\n", "code": "self.key = key\nself.secret = secret\n\nself.realm = \"xiaoi.com\"\nself.http_method = \"POST\"\nself.uri = \"/ask.do\"\nself.url = \"http://nlp.xiaoi.com/ask.do?platform=custom\"\n\nxauth = self._make_http_header_xauth()\n\nheaders = {\n    \"Content-type\": \"application/x-www-form-urlencoded\",\n    \"Accept\": \"text/plain\",\n}\n\nheaders.update(xauth)\n\nself.session = requests.Session()\nself.session.headers.update(headers)\nenhance_connection(self.session)", "path": "wxpy/wxpy/ext/xiaoi.py", "commit_date": "2017-05-21 00:00:00", "repo_name": "youfou/wxpy", "stars": 13724, "license": "mit", "language": "python", "size": 600}
{"docstring": "\"\"\"\n\u5c06\u672c\u6d88\u606f\u8f6c\u53d1\u7ed9\u5176\u4ed6\u804a\u5929\u5bf9\u8c61\n\n\u652f\u6301\u4ee5\u4e0b\u6d88\u606f\u7c7b\u578b\n    * \u6587\u672c (`TEXT`)\n    * \u89c6\u9891\uff08`VIDEO`)\n    * \u6587\u4ef6 (`ATTACHMENT`)\n    * \u56fe\u7247/\u81ea\u5b9a\u4e49\u8868\u60c5 (`PICTURE`)\n\n        * \u4f46\u4e0d\u652f\u6301\u8868\u60c5\u5546\u5e97\u4e2d\u7684\u8868\u60c5\n\n    * \u540d\u7247 (`CARD`)\n\n        * \u4ec5\u652f\u6301\u516c\u4f17\u53f7\u540d\u7247\uff0c\u4ee5\u53ca\u81ea\u5df1\u53d1\u51fa\u7684\u4e2a\u4eba\u53f7\u540d\u7247\n\n    * \u5206\u4eab (`SHARING`)\n\n        * \u4f1a\u8f6c\u5316\u4e3a `\u6807\u9898 + \u94fe\u63a5` \u5f62\u5f0f\u7684\u6587\u672c\u6d88\u606f\n\n    * \u8bed\u97f3 (`RECORDING`)\n\n        * \u4f1a\u4ee5\u6587\u4ef6\u65b9\u5f0f\u53d1\u9001\n    \n    * \u5730\u56fe (`MAP`)\n        \n        * \u4f1a\u8f6c\u5316\u4e3a `\u4f4d\u7f6e\u540d\u79f0 + \u5730\u56fe\u94fe\u63a5` \u5f62\u5f0f\u7684\u6587\u672c\u6d88\u606f\n\n:param Chat chat: \u63a5\u6536\u8f6c\u53d1\u6d88\u606f\u7684\u804a\u5929\u5bf9\u8c61\n:param str prefix: \u8f6c\u53d1\u65f6\u589e\u52a0\u7684 **\u524d\u7f00** \u6587\u672c\uff0c\u539f\u6d88\u606f\u4e3a\u6587\u672c\u65f6\u4f1a\u81ea\u52a8\u6362\u884c\n:param str suffix: \u8f6c\u53d1\u65f6\u589e\u52a0\u7684 **\u540e\u7f00** \u6587\u672c\uff0c\u539f\u6d88\u606f\u4e3a\u6587\u672c\u65f6\u4f1a\u81ea\u52a8\u6362\u884c\n:param bool raise_for_unsupported:\n    | \u4e3a True \u65f6\uff0c\u5c06\u4e3a\u4e0d\u652f\u6301\u7684\u6d88\u606f\u7c7b\u578b\u629b\u51fa `NotImplementedError` \u5f02\u5e38\n\n\u4f8b\u5982\uff0c\u5c06\u516c\u53f8\u7fa4\u4e2d\u7684\u8001\u677f\u6d88\u606f\u8f6c\u53d1\u51fa\u6765::\n\n    from wxpy import *\n\n    bot = Bot()\n\n    # \u5b9a\u4f4d\u516c\u53f8\u7fa4\n    company_group = ensure_one(bot.groups().search('\u516c\u53f8\u5fae\u4fe1\u7fa4'))\n\n    # \u5b9a\u4f4d\u8001\u677f\n    boss = ensure_one(company_group.search('\u8001\u677f\u5927\u540d'))\n\n    # \u5c06\u8001\u677f\u7684\u6d88\u606f\u8f6c\u53d1\u5230\u6587\u4ef6\u4f20\u8f93\u52a9\u624b\n    @bot.register(company_group)\n    def forward_boss_message(msg):\n        if msg.member == boss:\n            msg.forward(bot.file_helper, prefix='\u8001\u677f\u53d1\u8a00')\n\n    # \u5835\u585e\u7ebf\u7a0b\n    embed()\n\n\"\"\"\n\n", "func_signal": "def forward(self, chat, prefix=None, suffix=None, raise_for_unsupported=False):\n", "code": "logger.info('{}: forwarding to {}: {}'.format(self.bot, chat, self))\n\ndef wrapped_send(send_type, *args, **kwargs):\n    if send_type == 'msg':\n        if args:\n            text = args[0]\n        elif kwargs:\n            text = kwargs['msg']\n        else:\n            text = self.text\n        ret = chat.send_msg('{}{}{}'.format(\n            str(prefix) + '\\n' if prefix else '',\n            text,\n            '\\n' + str(suffix) if suffix else '',\n        ))\n    else:\n        if prefix:\n            chat.send_msg(prefix)\n        ret = getattr(chat, 'send_{}'.format(send_type))(*args, **kwargs)\n        if suffix:\n            chat.send_msg(suffix)\n\n    return ret\n\ndef download_and_send():\n    fd, path = tempfile.mkstemp(\n        suffix='_{}'.format(self.file_name),\n        dir=self.bot.temp_dir.name\n    )\n\n    try:\n        self.get_file(path)\n        if self.type == PICTURE:\n            return wrapped_send('image', path)\n        elif self.type == VIDEO:\n            return wrapped_send('video', path)\n        else:\n            return wrapped_send('file', path)\n    finally:\n        os.close(fd)\n\ndef raise_properly(text):\n    logger.warning(text)\n    if raise_for_unsupported:\n        raise NotImplementedError(text)\n\nif self.type == TEXT:\n    return wrapped_send('msg')\n\nelif self.type == SHARING:\n    return wrapped_send('msg', '{}\\n{}'.format(self.text, self.url))\n\nelif self.type == MAP:\n    return wrapped_send('msg', '{}: {}\\n{}'.format(\n        self.location['poiname'], self.location['label'], self.url\n    ))\n\nelif self.type == ATTACHMENT:\n\n    # noinspection SpellCheckingInspection\n    content = \\\n        \"<appmsg appid='wxeb7ec651dd0aefa9' sdkver=''>\" \\\n        \"<title>{file_name}</title><des></des><action></action>\" \\\n        \"<type>6</type><content></content><url></url><lowurl></lowurl>\" \\\n        \"<appattach><totallen>{file_size}</totallen><attachid>{media_id}</attachid>\" \\\n        \"<fileext>{file_ext}</fileext></appattach><extinfo></extinfo></appmsg>\"\n\n    content = content.format(\n        file_name=self.file_name,\n        file_size=self.file_size,\n        media_id=self.media_id,\n        file_ext=os.path.splitext(self.file_name)[1].replace('.', '')\n    )\n\n    return wrapped_send(\n        send_type='raw_msg',\n        raw_type=self.raw['MsgType'],\n        raw_content=content,\n        uri='/webwxsendappmsg?fun=async&f=json'\n    )\n\nelif self.type == CARD:\n    if self.card.raw.get('AttrStatus') and self.sender != self.bot.self:\n        # \u4e3a\u4e2a\u4eba\u540d\u7247\uff0c\u4e14\u4e0d\u4e3a\u81ea\u5df1\u6240\u53d1\u51fa\n        raise_properly('Personal cards sent from others are unsupported:\\n{}'.format(self))\n    else:\n        return wrapped_send(\n            send_type='raw_msg',\n            raw_type=self.raw['MsgType'],\n            raw_content=self.raw['Content'],\n            uri='/webwxsendmsg'\n        )\n\nelif self.type == PICTURE:\n    if self.raw.get('HasProductId'):\n        # \u6765\u81ea\u8868\u60c5\u5546\u5e97\u7684\u8868\u60c5\n        raise_properly('Stickers from store are unsupported:\\n{}'.format(self))\n    else:\n        return download_and_send()\n\nelif self.type == VIDEO:\n    return download_and_send()\n\nelif self.type == RECORDING:\n    return download_and_send()\n\nelse:\n    raise_properly('Unsupported message type:\\n{}'.format(self))", "path": "wxpy/wxpy/api/messages/message.py", "commit_date": "2017-06-16 00:00:00", "repo_name": "youfou/wxpy", "stars": 13724, "license": "mit", "language": "python", "size": 600}
{"docstring": "\"\"\"\n\u4ec5\u8fd4\u56de\u6d88\u606f\u7684\u7b54\u590d\u6587\u672c\n\n:param msg: Message \u5bf9\u8c61\n:param at_member: \u82e5\u6d88\u606f\u6765\u81ea\u7fa4\u804a\uff0c\u56de\u590d\u65f6 @\u53d1\u6d88\u606f\u7684\u7fa4\u6210\u5458\n:return: \u7b54\u590d\u6587\u672c\n:rtype: str\n\"\"\"\n\n", "func_signal": "def reply_text(self, msg, at_member=True):\n", "code": "def process_answer():\n\n    logger.debug('Tuling answer:\\n' + pprint.pformat(answer))\n\n    ret = str()\n    if at_member:\n        if len(msg.chat) > 2 and msg.member.name and not self.is_last_member(msg):\n            ret += '@{} '.format(msg.member.name)\n\n    code = -1\n    if answer:\n        code = answer.get('code', -1)\n\n    if code >= 100000:\n        text = answer.get('text')\n        if not text or (text == msg.text and len(text) > 3):\n            text = next_topic()\n        url = answer.get('url')\n        items = answer.get('list', list())\n\n        ret += str(text)\n        if url:\n            ret += '\\n{}'.format(url)\n        for item in items:\n            ret += '\\n\\n{}\\n{}'.format(\n                item.get('article') or item.get('name'),\n                item.get('detailurl')\n            )\n\n    else:\n        ret += next_topic()\n\n    return ret\n\ndef get_location(_chat):\n\n    province = getattr(_chat, 'province', None) or ''\n    city = getattr(_chat, 'city', None) or ''\n\n    if province in ('\u5317\u4eac', '\u4e0a\u6d77', '\u5929\u6d25', '\u91cd\u5e86'):\n        return '{}\u5e02{}\u533a'.format(province, city)\n    elif province and city:\n        return '{}\u7701{}\u5e02'.format(province, city)\n\nif not msg.bot:\n    raise ValueError('bot not found: {}'.format(msg))\n\nif not msg.text:\n    return\n\nfrom wxpy.api.chats import Group\nif at_member and isinstance(msg.chat, Group) and msg.member:\n    location = get_location(msg.member)\nelse:\n    # \u4f7f\u8be5\u9009\u9879\u5931\u6548\uff0c\u9632\u6b62\u9519\u8bef @ \u4eba\n    at_member = False\n    location = get_location(msg.chat)\n\nuser_id = get_context_user_id(msg)\n\nif location:\n    location = location[:30]\n\ninfo = str(get_text_without_at_bot(msg))[-30:]\n\npayload = dict(\n    key=self.api_key,\n    info=info,\n    userid=user_id,\n    loc=location\n)\n\nlogger.debug('Tuling payload:\\n' + pprint.pformat(payload))\n\n# noinspection PyBroadException\ntry:\n    r = self.session.post(self.url, json=payload)\n    answer = r.json()\nexcept:\n    answer = None\nfinally:\n    return process_answer()", "path": "wxpy/wxpy/ext/tuling.py", "commit_date": "2017-05-21 00:00:00", "repo_name": "youfou/wxpy", "stars": 13724, "license": "mit", "language": "python", "size": 600}
{"docstring": "\"\"\"\n\u6d88\u606f\u7684\u5ef6\u8fdf\u79d2\u6570 (\u53d1\u9001\u65f6\u95f4\u548c\u63a5\u6536\u65f6\u95f4\u7684\u5dee\u503c)\n\"\"\"\n", "func_signal": "def latency(self):\n", "code": "create_time = self.create_time\nif create_time:\n    return (self.receive_time - create_time).total_seconds()", "path": "wxpy/wxpy/api/messages/message.py", "commit_date": "2017-06-16 00:00:00", "repo_name": "youfou/wxpy", "stars": 13724, "license": "mit", "language": "python", "size": 600}
{"docstring": "\"\"\"\n\u4fdd\u5b58\u5f53\u524d\u673a\u5668\u4eba\u6240\u6709\u5df2\u6ce8\u518c\u7684\u6d88\u606f\u914d\u7f6e\n\n:param bot: \u6240\u5c5e\u7684\u673a\u5668\u4eba\n\"\"\"\n", "func_signal": "def __init__(self, bot):\n", "code": "super(Registered, self).__init__()\nself.bot = weakref.proxy(bot)", "path": "wxpy/wxpy/api/messages/registered.py", "commit_date": "2017-06-16 00:00:00", "repo_name": "youfou/wxpy", "stars": 13724, "license": "mit", "language": "python", "size": 600}
{"docstring": "\"\"\"\n\u751f\u6210\u8bf7\u6c42\u7b7e\u540d\n\"\"\"\n\n# 40\u4f4d\u968f\u673a\u5b57\u7b26\n# nonce = \"\".join([str(randint(0, 9)) for _ in range(40)])\n", "func_signal": "def _make_signature(self):\n", "code": "nonce = \"4103657107305326101203516108016101205331\"\n\nsha1 = \"{0}:{1}:{2}\".format(self.key, self.realm, self.secret).encode(\"utf-8\")\nsha1 = hashlib.sha1(sha1).hexdigest()\nsha2 = \"{0}:{1}\".format(self.http_method, self.uri).encode(\"utf-8\")\nsha2 = hashlib.sha1(sha2).hexdigest()\n\nsignature = \"{0}:{1}:{2}\".format(sha1, nonce, sha2).encode(\"utf-8\")\nsignature = hashlib.sha1(signature).hexdigest()\n\nret = collections.namedtuple(\"signature_return\", \"signature nonce\")\nret.signature = signature\nret.nonce = nonce\n\nreturn ret", "path": "wxpy/wxpy/ext/xiaoi.py", "commit_date": "2017-05-21 00:00:00", "repo_name": "youfou/wxpy", "stars": 13724, "license": "mit", "language": "python", "size": 600}
{"docstring": "\"\"\"\n\u5206\u4eab\u7c7b\u6d88\u606f\u4e2d\u7684\u7f51\u9875 URL\n\"\"\"\n", "func_signal": "def url(self):\n", "code": "_url = self.raw.get('Url')\nif isinstance(_url, str):\n    _url = html.unescape(_url)\n\nreturn _url", "path": "wxpy/wxpy/api/messages/message.py", "commit_date": "2017-06-16 00:00:00", "repo_name": "youfou/wxpy", "stars": 13724, "license": "mit", "language": "python", "size": 600}
{"docstring": "\"\"\"\n\u901a\u8fc7\u7ed9\u5b9a\u7684\u51fd\u6570\u627e\u5230\u5bf9\u5e94\u7684\u6ce8\u518c\u914d\u7f6e\n\n:param func: \u7ed9\u5b9a\u7684\u51fd\u6570\n:return: \u5bf9\u5e94\u7684\u6ce8\u518c\u914d\u7f6e\n\"\"\"\n\n", "func_signal": "def get_config_by_func(self, func):\n", "code": "for conf in self:\n    if conf.func == func:\n        return conf", "path": "wxpy/wxpy/api/messages/registered.py", "commit_date": "2017-06-16 00:00:00", "repo_name": "youfou/wxpy", "stars": 13724, "license": "mit", "language": "python", "size": 600}
{"docstring": "\"\"\"\n\u516c\u4f17\u53f7\u63a8\u9001\u4e2d\u7684\u6587\u7ae0\u5217\u8868 (\u9996\u7bc7\u7684 \u6807\u9898/\u5730\u5740 \u4e0e\u6d88\u606f\u4e2d\u7684 text/url \u76f8\u540c)\n\n\u5176\u4e2d\uff0c\u6bcf\u7bc7\u6587\u7ae0\u5747\u6709\u4ee5\u4e0b\u5c5e\u6027:\n\n* `title`: \u6807\u9898\n* `summary`: \u6458\u8981\n* `url`: \u6587\u7ae0 URL\n* `cover`: \u5c01\u9762\u6216\u7f29\u7565\u56fe URL\n\"\"\"\n\n", "func_signal": "def articles(self):\n", "code": "from wxpy import MP\nif self.type == SHARING and isinstance(self.sender, MP):\n    tree = ETree.fromstring(self.raw['Content'])\n    # noinspection SpellCheckingInspection\n    items = tree.findall('.//mmreader/category/item')\n\n    article_list = list()\n\n    for item in items:\n        def find_text(tag):\n            found = item.find(tag)\n            if found is not None:\n                return found.text\n\n        article = Article()\n        article.title = find_text('title')\n        article.summary = find_text('digest')\n        article.url = find_text('url')\n        article.cover = find_text('cover')\n        article_list.append(article)\n\n    return article_list", "path": "wxpy/wxpy/api/messages/message.py", "commit_date": "2017-06-16 00:00:00", "repo_name": "youfou/wxpy", "stars": 13724, "license": "mit", "language": "python", "size": 600}
{"docstring": "\"\"\"\n* \u597d\u53cb\u8bf7\u6c42\u4e2d\u7684\u8bf7\u6c42\u7528\u6237\n* \u540d\u7247\u6d88\u606f\u4e2d\u7684\u63a8\u8350\u7528\u6237\n\"\"\"\n", "func_signal": "def card(self):\n", "code": "if self.type in (CARD, FRIENDS):\n    return User(self.raw.get('RecommendInfo'), self.bot)", "path": "wxpy/wxpy/api/messages/message.py", "commit_date": "2017-06-16 00:00:00", "repo_name": "youfou/wxpy", "stars": 13724, "license": "mit", "language": "python", "size": 600}
{"docstring": "\"\"\"\n\u4f4d\u7f6e\u6d88\u606f\u4e2d\u7684\u5730\u7406\u4f4d\u7f6e\u4fe1\u606f\n\"\"\"\n", "func_signal": "def location(self):\n", "code": "try:\n    ret = ETree.fromstring(self.raw['OriContent']).find('location').attrib\n    try:\n        ret['x'] = float(ret['x'])\n        ret['y'] = float(ret['y'])\n        ret['scale'] = int(ret['scale'])\n        ret['maptype'] = int(ret['maptype'])\n    except (KeyError, ValueError):\n        pass\n    return ret\nexcept (TypeError, KeyError, ValueError, ETree.ParseError):\n    pass", "path": "wxpy/wxpy/api/messages/message.py", "commit_date": "2017-06-16 00:00:00", "repo_name": "youfou/wxpy", "stars": 13724, "license": "mit", "language": "python", "size": 600}
{"docstring": "\"\"\"\n\u4e0b\u8f7d\u56fe\u7247\u3001\u89c6\u9891\u3001\u8bed\u97f3\u3001\u9644\u4ef6\u6d88\u606f\u4e2d\u7684\u6587\u4ef6\u5185\u5bb9\u3002\n\n\u53ef\u4e0e :any:`Message.file_name` \u914d\u5408\u4f7f\u7528\u3002\n\n:param save_path: \u6587\u4ef6\u7684\u4fdd\u5b58\u8def\u5f84\u3002\u82e5\u4e3a None\uff0c\u5c06\u76f4\u63a5\u8fd4\u56de\u5b57\u8282\u6570\u636e\n\"\"\"\n\n", "func_signal": "def get_file(self, save_path=None):\n", "code": "_text = self.raw.get('Text')\nif callable(_text) and self.type in (PICTURE, RECORDING, ATTACHMENT, VIDEO):\n    return _text(save_path)\nelse:\n    raise ValueError('download method not found, or invalid message type')", "path": "wxpy/wxpy/api/messages/message.py", "commit_date": "2017-06-16 00:00:00", "repo_name": "youfou/wxpy", "stars": 13724, "license": "mit", "language": "python", "size": 600}
{"docstring": "\"\"\"Return system CPU times as a namedtuple\"\"\"\n", "func_signal": "def per_cpu_times():\n", "code": "if cpu_count_logical() == 1:\n    return [cpu_times()]\nif per_cpu_times.__called__:\n    raise NotImplementedError(\"supported only starting from FreeBSD 8\")\nper_cpu_times.__called__ = True\nreturn [cpu_times()]", "path": "psutil/psutil/_psbsd.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "giampaolo/psutil", "stars": 9837, "license": "bsd-3-clause", "language": "python", "size": 36792}
{"docstring": "\"\"\"Decorator which translates bare OSError exceptions into\nNoSuchProcess and AccessDenied.\n\"\"\"\n", "func_signal": "def wrap_exceptions(fun):\n", "code": "@functools.wraps(fun)\ndef wrapper(self, *args, **kwargs):\n    try:\n        return fun(self, *args, **kwargs)\n    except ProcessLookupError:\n        if is_zombie(self.pid):\n            raise ZombieProcess(self.pid, self._name, self._ppid)\n        else:\n            raise NoSuchProcess(self.pid, self._name)\n    except PermissionError:\n        raise AccessDenied(self.pid, self._name)\n    except OSError:\n        if self.pid == 0:\n            if 0 in pids():\n                raise AccessDenied(self.pid, self._name)\n            else:\n                raise\n        raise\nreturn wrapper", "path": "psutil/psutil/_psbsd.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "giampaolo/psutil", "stars": 9837, "license": "bsd-3-clause", "language": "python", "size": 36792}
{"docstring": "# sleep some time\n", "func_signal": "def poll(interval):\n", "code": "time.sleep(interval)\nprocs = []\nprocs_status = {}\nfor p in psutil.process_iter():\n    try:\n        p.dict = p.as_dict(['username', 'nice', 'memory_info',\n                            'memory_percent', 'cpu_percent',\n                            'cpu_times', 'name', 'status'])\n        try:\n            procs_status[p.dict['status']] += 1\n        except KeyError:\n            procs_status[p.dict['status']] = 1\n    except psutil.NoSuchProcess:\n        pass\n    else:\n        procs.append(p)\n\n# return processes sorted by CPU percent usage\nprocesses = sorted(procs, key=lambda p: p.dict['cpu_percent'],\n                   reverse=True)\nreturn (processes, procs_status)", "path": "psutil/scripts/top.py", "commit_date": "2020-10-08 00:00:00", "repo_name": "giampaolo/psutil", "stars": 9837, "license": "bsd-3-clause", "language": "python", "size": 36792}
{"docstring": "\"\"\"Retrieves multiple process info in one shot as a raw tuple.\"\"\"\n", "func_signal": "def oneshot(self):\n", "code": "ret = cext.proc_oneshot_info(self.pid)\nassert len(ret) == len(kinfo_proc_map)\nreturn ret", "path": "psutil/psutil/_psbsd.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "giampaolo/psutil", "stars": 9837, "license": "bsd-3-clause", "language": "python", "size": 36792}
{"docstring": "\"\"\"Returns a list of PIDs currently running on the system.\"\"\"\n", "func_signal": "def pids():\n", "code": "ret = cext.pids()\nif OPENBSD and (0 not in ret) and _pid_0_exists():\n    # On OpenBSD the kernel does not return PID 0 (neither does\n    # ps) but it's actually querable (Process(0) will succeed).\n    ret.insert(0, 0)\nreturn ret", "path": "psutil/psutil/_psbsd.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "giampaolo/psutil", "stars": 9837, "license": "bsd-3-clause", "language": "python", "size": 36792}
{"docstring": "\"\"\"A thin wrapper around curses's addstr().\"\"\"\n", "func_signal": "def printl(line, color=None, bold=False, highlight=False):\n", "code": "global lineno\ntry:\n    flags = 0\n    if color:\n        flags |= curses.color_pair(colors_map[color])\n    if bold:\n        flags |= curses.A_BOLD\n    if highlight:\n        line += \" \" * (win.getmaxyx()[1] - len(line))\n        flags |= curses.A_STANDOUT\n    win.addstr(lineno, 0, line, flags)\nexcept curses.error:\n    lineno = 0\n    win.refresh()\n    raise\nelse:\n    lineno += 1", "path": "psutil/scripts/top.py", "commit_date": "2020-10-08 00:00:00", "repo_name": "giampaolo/psutil", "stars": 9837, "license": "bsd-3-clause", "language": "python", "size": 36792}
{"docstring": "\"\"\"Same as above, for routines relying on reading /proc fs.\"\"\"\n", "func_signal": "def wrap_exceptions_procfs(inst):\n", "code": "try:\n    yield\nexcept (ProcessLookupError, FileNotFoundError):\n    # ENOENT (no such file or directory) gets raised on open().\n    # ESRCH (no such process) can get raised on read() if\n    # process is gone in meantime.\n    if is_zombie(inst.pid):\n        raise ZombieProcess(inst.pid, inst._name, inst._ppid)\n    else:\n        raise NoSuchProcess(inst.pid, inst._name)\nexcept PermissionError:\n    raise AccessDenied(inst.pid, inst._name)", "path": "psutil/psutil/_psbsd.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "giampaolo/psutil", "stars": 9837, "license": "bsd-3-clause", "language": "python", "size": 36792}
{"docstring": "\"\"\"Return system per-CPU times as a namedtuple\"\"\"\n", "func_signal": "def cpu_times():\n", "code": "user, nice, system, idle, irq = cext.cpu_times()\nreturn scputimes(user, nice, system, idle, irq)", "path": "psutil/psutil/_psbsd.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "giampaolo/psutil", "stars": 9837, "license": "bsd-3-clause", "language": "python", "size": 36792}
{"docstring": "\"\"\"System swap memory as (total, used, free, sin, sout) namedtuple.\"\"\"\n", "func_signal": "def swap_memory():\n", "code": "total, used, free, sin, sout = cext.swap_mem()\npercent = usage_percent(used, total, round_=1)\nreturn _common.sswap(total, used, free, percent, sin, sout)", "path": "psutil/psutil/_psbsd.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "giampaolo/psutil", "stars": 9837, "license": "bsd-3-clause", "language": "python", "size": 36792}
{"docstring": "\"\"\"Return frequency metrics for CPUs. As of Dec 2018 only\nCPU 0 appears to be supported by FreeBSD and all other cores\nmatch the frequency of CPU 0.\n\"\"\"\n", "func_signal": "def cpu_freq():\n", "code": "ret = []\nnum_cpus = cpu_count_logical()\nfor cpu in range(num_cpus):\n    try:\n        current, available_freq = cext.cpu_frequency(cpu)\n    except NotImplementedError:\n        continue\n    if available_freq:\n        try:\n            min_freq = int(available_freq.split(\" \")[-1].split(\"/\")[0])\n        except(IndexError, ValueError):\n            min_freq = None\n        try:\n            max_freq = int(available_freq.split(\" \")[0].split(\"/\")[0])\n        except(IndexError, ValueError):\n            max_freq = None\n    ret.append(_common.scpufreq(current, min_freq, max_freq))\nreturn ret", "path": "psutil/psutil/_psbsd.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "giampaolo/psutil", "stars": 9837, "license": "bsd-3-clause", "language": "python", "size": 36792}
{"docstring": "\"\"\"Return mounted disk partitions as a list of namedtuples.\n'all' argument is ignored, see:\nhttps://github.com/giampaolo/psutil/issues/906\n\"\"\"\n", "func_signal": "def disk_partitions(all=False):\n", "code": "retlist = []\npartitions = cext.disk_partitions()\nfor partition in partitions:\n    device, mountpoint, fstype, opts = partition\n    maxfile = maxpath = None  # set later\n    ntuple = _common.sdiskpart(device, mountpoint, fstype, opts,\n                               maxfile, maxpath)\n    retlist.append(ntuple)\nreturn retlist", "path": "psutil/psutil/_psbsd.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "giampaolo/psutil", "stars": 9837, "license": "bsd-3-clause", "language": "python", "size": 36792}
{"docstring": "# --- system\n\n", "func_signal": "def main():\n", "code": "public_apis = []\nignore = ['wait_procs', 'process_iter', 'win_service_get',\n          'win_service_iter']\nif psutil.MACOS:\n    ignore.append('net_connections')  # raises AD\nfor name in psutil.__all__:\n    obj = getattr(psutil, name, None)\n    if inspect.isfunction(obj):\n        if name not in ignore:\n            public_apis.append(name)\n\nprint_color(templ % (\"SYSTEM APIS\", \"SECONDS\"), color=None, bold=True)\nprint(\"-\" * 34)\nfor name in public_apis:\n    fun = getattr(psutil, name)\n    args = ()\n    if name == 'pid_exists':\n        args = (os.getpid(), )\n    elif name == 'disk_usage':\n        args = (os.getcwd(), )\n    timecall(name, fun, *args)\ntimecall('cpu_count (cores)', psutil.cpu_count, logical=False)\ntimecall('process_iter (all)', lambda: list(psutil.process_iter()))\nprint_timings()\n\n# --- process\nprint(\"\")\nprint_color(templ % (\"PROCESS APIS\", \"SECONDS\"), color=None, bold=True)\nprint(\"-\" * 34)\nignore = ['send_signal', 'suspend', 'resume', 'terminate', 'kill', 'wait',\n          'as_dict', 'parent', 'parents', 'memory_info_ex', 'oneshot',\n          'pid', 'rlimit']\nif psutil.MACOS:\n    ignore.append('memory_maps')  # XXX\np = psutil.Process()\nfor name in sorted(dir(p)):\n    if not name.startswith('_') and name not in ignore:\n        fun = getattr(p, name)\n        timecall(name, fun)\nprint_timings()", "path": "psutil/scripts/internal/print_api_speed.py", "commit_date": "2020-02-15 00:00:00", "repo_name": "giampaolo/psutil", "stars": 9837, "license": "bsd-3-clause", "language": "python", "size": 36792}
{"docstring": "\"\"\"Return process current working directory.\"\"\"\n# sometimes we get an empty string, in which case we turn\n# it into None\n", "func_signal": "def cwd(self):\n", "code": "if OPENBSD and self.pid == 0:\n    return None  # ...else it would raise EINVAL\nelif NETBSD or HAS_PROC_OPEN_FILES:\n    # FreeBSD < 8 does not support functions based on\n    # kinfo_getfile() and kinfo_getvmmap()\n    return cext.proc_cwd(self.pid) or None\nelse:\n    raise NotImplementedError(\n        \"supported only starting from FreeBSD 8\" if\n        FREEBSD else \"\")", "path": "psutil/psutil/_psbsd.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "giampaolo/psutil", "stars": 9837, "license": "bsd-3-clause", "language": "python", "size": 36792}
{"docstring": "\"\"\"Return the number of file descriptors opened by this process.\"\"\"\n", "func_signal": "def num_fds(self):\n", "code": "ret = cext.proc_num_fds(self.pid)\nif NETBSD:\n    self._assert_alive()\nreturn ret", "path": "psutil/psutil/_psbsd.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "giampaolo/psutil", "stars": 9837, "license": "bsd-3-clause", "language": "python", "size": 36792}
{"docstring": "\"\"\"Return the number of CPU cores in the system.\"\"\"\n# From the C module we'll get an XML string similar to this:\n# http://manpages.ubuntu.com/manpages/precise/man4/smp.4freebsd.html\n# We may get None in case \"sysctl kern.sched.topology_spec\"\n# is not supported on this BSD version, in which case we'll mimic\n# os.cpu_count() and return None.\n", "func_signal": "def cpu_count_cores():\n", "code": "ret = None\ns = cext.cpu_topology()\nif s is not None:\n    # get rid of padding chars appended at the end of the string\n    index = s.rfind(\"</groups>\")\n    if index != -1:\n        s = s[:index + 9]\n        root = ET.fromstring(s)\n        try:\n            ret = len(root.findall('group/children/group/cpu')) or None\n        finally:\n            # needed otherwise it will memleak\n            root.clear()\nif not ret:\n    # If logical CPUs == 1 it's obvious we' have only 1 core.\n    if cpu_count_logical() == 1:\n        return 1\nreturn ret", "path": "psutil/psutil/_psbsd.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "giampaolo/psutil", "stars": 9837, "license": "bsd-3-clause", "language": "python", "size": 36792}
{"docstring": "\"\"\"Return system CPU times as a namedtuple\"\"\"\n", "func_signal": "def per_cpu_times():\n", "code": "ret = []\nfor cpu_t in cext.per_cpu_times():\n    user, nice, system, idle, irq = cpu_t\n    item = scputimes(user, nice, system, idle, irq)\n    ret.append(item)\nreturn ret", "path": "psutil/psutil/_psbsd.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "giampaolo/psutil", "stars": 9837, "license": "bsd-3-clause", "language": "python", "size": 36792}
{"docstring": "# Pre-emptively check if CPUs are valid because the C\n# function has a weird behavior in case of invalid CPUs,\n# see: https://github.com/giampaolo/psutil/issues/586\n", "func_signal": "def cpu_affinity_set(self, cpus):\n", "code": "allcpus = tuple(range(len(per_cpu_times())))\nfor cpu in cpus:\n    if cpu not in allcpus:\n        raise ValueError(\"invalid CPU #%i (choose between %s)\"\n                         % (cpu, allcpus))\ntry:\n    cext.proc_cpu_affinity_set(self.pid, cpus)\nexcept OSError as err:\n    # 'man cpuset_setaffinity' about EDEADLK:\n    # <<the call would leave a thread without a valid CPU to run\n    # on because the set does not overlap with the thread's\n    # anonymous mask>>\n    if err.errno in (errno.EINVAL, errno.EDEADLK):\n        for cpu in cpus:\n            if cpu not in allcpus:\n                raise ValueError(\n                    \"invalid CPU #%i (choose between %s)\" % (\n                        cpu, allcpus))\n    raise", "path": "psutil/psutil/_psbsd.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "giampaolo/psutil", "stars": 9837, "license": "bsd-3-clause", "language": "python", "size": 36792}
{"docstring": "\"\"\"Return True if pid exists.\"\"\"\n", "func_signal": "def pid_exists(pid):\n", "code": "exists = _psposix.pid_exists(pid)\nif not exists:\n    # We do this because _psposix.pid_exists() lies in case of\n    # zombie processes.\n    return pid in pids()\nelse:\n    return True", "path": "psutil/psutil/_psbsd.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "giampaolo/psutil", "stars": 9837, "license": "bsd-3-clause", "language": "python", "size": 36792}
{"docstring": "\"\"\"Return various CPU stats as a named tuple.\"\"\"\n", "func_signal": "def cpu_stats():\n", "code": "if FREEBSD:\n    # Note: the C ext is returning some metrics we are not exposing:\n    # traps.\n    ctxsw, intrs, soft_intrs, syscalls, traps = cext.cpu_stats()\nelif NETBSD:\n    # XXX\n    # Note about intrs: the C extension returns 0. intrs\n    # can be determined via /proc/stat; it has the same value as\n    # soft_intrs thought so the kernel is faking it (?).\n    #\n    # Note about syscalls: the C extension always sets it to 0 (?).\n    #\n    # Note: the C ext is returning some metrics we are not exposing:\n    # traps, faults and forks.\n    ctxsw, intrs, soft_intrs, syscalls, traps, faults, forks = \\\n        cext.cpu_stats()\n    with open('/proc/stat', 'rb') as f:\n        for line in f:\n            if line.startswith(b'intr'):\n                intrs = int(line.split()[1])\nelif OPENBSD:\n    # Note: the C ext is returning some metrics we are not exposing:\n    # traps, faults and forks.\n    ctxsw, intrs, soft_intrs, syscalls, traps, faults, forks = \\\n        cext.cpu_stats()\nreturn _common.scpustats(ctxsw, intrs, soft_intrs, syscalls)", "path": "psutil/psutil/_psbsd.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "giampaolo/psutil", "stars": 9837, "license": "bsd-3-clause", "language": "python", "size": 36792}
{"docstring": "\"\"\"Return battery info.\"\"\"\n", "func_signal": "def sensors_battery():\n", "code": "try:\n    percent, minsleft, power_plugged = cext.sensors_battery()\nexcept NotImplementedError:\n    # See: https://github.com/giampaolo/psutil/issues/1074\n    return None\npower_plugged = power_plugged == 1\nif power_plugged:\n    secsleft = _common.POWER_TIME_UNLIMITED\nelif minsleft == -1:\n    secsleft = _common.POWER_TIME_UNKNOWN\nelse:\n    secsleft = minsleft * 60\nreturn _common.sbattery(percent, secsleft, power_plugged)", "path": "psutil/psutil/_psbsd.py", "commit_date": "2020-12-24 00:00:00", "repo_name": "giampaolo/psutil", "stars": 9837, "license": "bsd-3-clause", "language": "python", "size": 36792}
{"docstring": "\"\"\"\nSubstitute domains in source_tree with files and substitutions,\n    and save the pre-domain substitution archive to presubdom_archive.\n\nregex_path is a pathlib.Path to domain_regex.list\nfiles_path is a pathlib.Path to domain_substitution.list\nsource_tree is a pathlib.Path to the source tree.\ndomainsub_cache is a pathlib.Path to the domain substitution cache.\n\nRaises NotADirectoryError if the patches directory is not a directory or does not exist\nRaises FileNotFoundError if the source tree or required directory does not exist.\nRaises FileExistsError if the domain substitution cache already exists.\nRaises ValueError if an entry in the domain substitution list contains the file index\n    hash delimiter.\n\"\"\"\n", "func_signal": "def apply_substitution(regex_path, files_path, source_tree, domainsub_cache):\n", "code": "if not source_tree.exists():\n    raise FileNotFoundError(source_tree)\nif not regex_path.exists():\n    raise FileNotFoundError(regex_path)\nif not files_path.exists():\n    raise FileNotFoundError(files_path)\nif domainsub_cache.exists():\n    raise FileExistsError(domainsub_cache)\nresolved_tree = source_tree.resolve()\nregex_pairs = DomainRegexList(regex_path).regex_pairs\nfileindex_content = io.BytesIO()\nwith tarfile.open(\n        str(domainsub_cache), 'w:%s' % domainsub_cache.suffix[1:],\n        compresslevel=1) as cache_tar:\n    for relative_path in filter(len, files_path.read_text().splitlines()):\n        if _INDEX_HASH_DELIMITER in relative_path:\n            # Cache tar will be incomplete; remove it for convenience\n            cache_tar.close()\n            domainsub_cache.unlink()\n            raise ValueError(\n                'Path \"%s\" contains the file index hash delimiter \"%s\"' % relative_path,\n                _INDEX_HASH_DELIMITER)\n        path = resolved_tree / relative_path\n        if not path.exists():\n            get_logger().warning('Skipping non-existant path: %s', path)\n            continue\n        if path.is_symlink():\n            get_logger().warning('Skipping path that has become a symlink: %s', path)\n            continue\n        with _update_timestamp(path, set_new=True):\n            crc32_hash, orig_content = _substitute_path(path, regex_pairs)\n        if crc32_hash is None:\n            get_logger().info('Path has no substitutions: %s', relative_path)\n            continue\n        fileindex_content.write('{}{}{:08x}\\n'.format(relative_path, _INDEX_HASH_DELIMITER,\n                                                      crc32_hash).encode(ENCODING))\n        orig_tarinfo = tarfile.TarInfo(str(Path(_ORIG_DIR) / relative_path))\n        orig_tarinfo.size = len(orig_content)\n        with io.BytesIO(orig_content) as orig_file:\n            cache_tar.addfile(orig_tarinfo, orig_file)\n    fileindex_tarinfo = tarfile.TarInfo(_INDEX_LIST)\n    fileindex_tarinfo.size = fileindex_content.tell()\n    fileindex_content.seek(0)\n    cache_tar.addfile(fileindex_tarinfo, fileindex_content)", "path": "ungoogled-chromium/utils/domain_substitution.py", "commit_date": "2020-11-21 00:00:00", "repo_name": "ungoogled-software/ungoogled-chromium", "stars": 18486, "license": "bsd-3-clause", "language": "python", "size": 14775}
{"docstring": "\"\"\"Reads an iterable of pathlib.Path to download.ini files\"\"\"\n", "func_signal": "def __init__(self, ini_paths):\n", "code": "self._data = configparser.ConfigParser()\nfor path in ini_paths:\n    self._data.read_dict(self._parse_data(path))", "path": "ungoogled-chromium/utils/downloads.py", "commit_date": "2020-08-25 00:00:00", "repo_name": "ungoogled-software/ungoogled-chromium", "stars": 18486, "license": "bsd-3-clause", "language": "python", "size": 14775}
{"docstring": "\"\"\"CLI Entrypoint\"\"\"\n", "func_signal": "def main():\n", "code": "parser = argparse.ArgumentParser()\nadd_common_params(parser)\nparser.set_defaults(callback=_callback)\nsubparsers = parser.add_subparsers(title='', dest='packaging')\n\n# apply\napply_parser = subparsers.add_parser(\n    'apply',\n    help='Apply domain substitution',\n    description='Applies domain substitution and creates the domain substitution cache.')\napply_parser.add_argument(\n    '-r', '--regex', type=Path, required=True, help='Path to domain_regex.list')\napply_parser.add_argument(\n    '-f', '--files', type=Path, required=True, help='Path to domain_substitution.list')\napply_parser.add_argument(\n    '-c',\n    '--cache',\n    type=Path,\n    required=True,\n    help='The path to the domain substitution cache. The path must not already exist.')\napply_parser.add_argument(\n    'directory', type=Path, help='The directory to apply domain substitution')\napply_parser.set_defaults(reverting=False)\n\n# revert\nrevert_parser = subparsers.add_parser(\n    'revert',\n    help='Revert domain substitution',\n    description='Reverts domain substitution based only on the domain substitution cache.')\nrevert_parser.add_argument(\n    'directory', type=Path, help='The directory to reverse domain substitution')\nrevert_parser.add_argument(\n    '-c',\n    '--cache',\n    type=Path,\n    required=True,\n    help=('The path to the domain substitution cache. '\n          'The path must exist and will be removed if successful.'))\nrevert_parser.set_defaults(reverting=True)\n\nargs = parser.parse_args()\nargs.callback(args)", "path": "ungoogled-chromium/utils/domain_substitution.py", "commit_date": "2020-11-21 00:00:00", "repo_name": "ungoogled-software/ungoogled-chromium", "stars": 18486, "license": "bsd-3-clause", "language": "python", "size": 14775}
{"docstring": "\"\"\"Generates a regex pair tuple for the given line\"\"\"\n", "func_signal": "def _compile_regex(self, line):\n", "code": "pattern, replacement = line.split(self._PATTERN_REPLACE_DELIM)\nreturn self._regex_pair_tuple(re.compile(pattern), replacement)", "path": "ungoogled-chromium/utils/domain_substitution.py", "commit_date": "2020-11-21 00:00:00", "repo_name": "ungoogled-software/ungoogled-chromium", "stars": 18486, "license": "bsd-3-clause", "language": "python", "size": 14775}
{"docstring": "\"\"\"\nDownloads a file from url to the specified path file_path if necessary.\n\nIf show_progress is True, download progress is printed to the console.\n\"\"\"\n", "func_signal": "def _download_if_needed(file_path, url, show_progress, disable_ssl_verification):\n", "code": "if file_path.exists():\n    get_logger().info('%s already exists. Skipping download.', file_path)\n    return\n\n# File name for partially download file\ntmp_file_path = file_path.with_name(file_path.name + '.partial')\n\nif tmp_file_path.exists():\n    get_logger().debug('Resuming downloading URL %s ...', url)\nelse:\n    get_logger().debug('Downloading URL %s ...', url)\n\n# Perform download\nif shutil.which('curl'):\n    get_logger().debug('Using curl')\n    try:\n        subprocess.run(['curl', '-L', '-o', str(tmp_file_path), '-C', '-', url], check=True)\n    except subprocess.CalledProcessError as exc:\n        get_logger().error('curl failed. Re-run the download command to resume downloading.')\n        raise exc\nelse:\n    get_logger().debug('Using urllib')\n    _download_via_urllib(url, tmp_file_path, show_progress, disable_ssl_verification)\n\n# Download complete; rename file\ntmp_file_path.rename(file_path)", "path": "ungoogled-chromium/utils/downloads.py", "commit_date": "2020-08-25 00:00:00", "repo_name": "ungoogled-software/ungoogled-chromium", "stars": 18486, "license": "bsd-3-clause", "language": "python", "size": 14775}
{"docstring": "\"\"\"Iterator for the download properties sorted by output path\"\"\"\n", "func_signal": "def properties_iter(self):\n", "code": "return sorted(\n    map(lambda x: (x, self[x]), self), key=(lambda x: str(Path(x[1].output_path))))", "path": "ungoogled-chromium/utils/downloads.py", "commit_date": "2020-08-25 00:00:00", "repo_name": "ungoogled-software/ungoogled-chromium", "stars": 18486, "license": "bsd-3-clause", "language": "python", "size": 14775}
{"docstring": "\"\"\"\nCheck integrity of the downloads cache.\n\ndownload_info is the DownloadInfo of downloads to unpack.\ncache_dir is the pathlib.Path to the downloads cache.\n\nRaises source_retrieval.HashMismatchError when the computed and expected hashes do not match.\n\"\"\"\n", "func_signal": "def check_downloads(download_info, cache_dir):\n", "code": "for download_name, download_properties in download_info.properties_iter():\n    get_logger().info('Verifying hashes for \"%s\" ...', download_name)\n    download_path = cache_dir / download_properties.download_filename\n    with download_path.open('rb') as file_obj:\n        archive_data = file_obj.read()\n    for hash_name, hash_hex in _get_hash_pairs(download_properties, cache_dir):\n        get_logger().debug('Verifying %s hash...', hash_name)\n        hasher = hashlib.new(hash_name, data=archive_data)\n        if not hasher.hexdigest().lower() == hash_hex.lower():\n            raise HashMismatchError(download_path)", "path": "ungoogled-chromium/utils/downloads.py", "commit_date": "2020-08-25 00:00:00", "repo_name": "ungoogled-software/ungoogled-chromium", "stars": 18486, "license": "bsd-3-clause", "language": "python", "size": 14775}
{"docstring": "\"\"\"\nReturns an object with keys as attributes and\nvalues already pre-processed strings\n\"\"\"\n", "func_signal": "def __getitem__(self, section):\n", "code": "return self._DownloadsProperties(self._data[section], self._passthrough_properties,\n                                 self._hashes)", "path": "ungoogled-chromium/utils/downloads.py", "commit_date": "2020-08-25 00:00:00", "repo_name": "ungoogled-software/ungoogled-chromium", "stars": 18486, "license": "bsd-3-clause", "language": "python", "size": 14775}
{"docstring": "\"\"\"\nValidation of file index and hashes against the source tree.\n    Updates cache_index_files\n\nReturns True if the file index is valid; False otherwise\n\"\"\"\n", "func_signal": "def _validate_file_index(index_file, resolved_tree, cache_index_files):\n", "code": "all_hashes_valid = True\ncrc32_regex = re.compile(r'^[a-zA-Z0-9]{8}$')\nfor entry in index_file.read().decode(ENCODING).splitlines():\n    try:\n        relative_path, file_hash = entry.split(_INDEX_HASH_DELIMITER)\n    except ValueError as exc:\n        get_logger().error('Could not split entry \"%s\": %s', entry, exc)\n        continue\n    if not relative_path or not file_hash:\n        get_logger().error('Entry %s of domain substitution cache file index is not valid',\n                           _INDEX_HASH_DELIMITER.join((relative_path, file_hash)))\n        all_hashes_valid = False\n        continue\n    if not crc32_regex.match(file_hash):\n        get_logger().error('File index hash for %s does not appear to be a CRC32 hash',\n                           relative_path)\n        all_hashes_valid = False\n        continue\n    if zlib.crc32((resolved_tree / relative_path).read_bytes()) != int(file_hash, 16):\n        get_logger().error('Hashes do not match for: %s', relative_path)\n        all_hashes_valid = False\n        continue\n    if relative_path in cache_index_files:\n        get_logger().error('File %s shows up at least twice in the file index', relative_path)\n        all_hashes_valid = False\n        continue\n    cache_index_files.add(relative_path)\nreturn all_hashes_valid", "path": "ungoogled-chromium/utils/domain_substitution.py", "commit_date": "2020-11-21 00:00:00", "repo_name": "ungoogled-software/ungoogled-chromium", "stars": 18486, "license": "bsd-3-clause", "language": "python", "size": 14775}
{"docstring": "\"\"\"\nRevert domain substitution on source_tree using the pre-domain\n    substitution archive presubdom_archive.\nIt first checks if the hashes of the substituted files match the hashes\n    computed during the creation of the domain substitution cache, raising\n    KeyError if there are any mismatches. Then, it proceeds to\n    reverting files in the source_tree.\ndomainsub_cache is removed only if all the files from the domain substitution cache\n    were relocated to the source tree.\n\ndomainsub_cache is a pathlib.Path to the domain substitution cache.\nsource_tree is a pathlib.Path to the source tree.\n\nRaises KeyError if:\n    * There is a hash mismatch while validating the cache\n    * The cache's file index is corrupt or missing\n    * The cache is corrupt or is not consistent with the file index\nRaises FileNotFoundError if the source tree or domain substitution cache do not exist.\n\"\"\"\n# This implementation trades disk space/wear for performance (unless a ramdisk is used\n#   for the source tree)\n# Assumptions made for this process:\n# * The correct tar file was provided (so no huge amount of space is wasted)\n# * The tar file is well-behaved (e.g. no files extracted outside of destination path)\n# * Cache file index and cache contents are already consistent (i.e. no files exclusive to\n#   one or the other)\n", "func_signal": "def revert_substitution(domainsub_cache, source_tree):\n", "code": "if not domainsub_cache.exists():\n    raise FileNotFoundError(domainsub_cache)\nif not source_tree.exists():\n    raise FileNotFoundError(source_tree)\nresolved_tree = source_tree.resolve()\n\ncache_index_files = set() # All files in the file index\n\nwith tempfile.TemporaryDirectory(\n        prefix='domsubcache_files', dir=str(resolved_tree)) as tmp_extract_name:\n    extract_path = Path(tmp_extract_name)\n    get_logger().debug('Extracting domain substitution cache...')\n    extract_tar_file(domainsub_cache, extract_path, None)\n\n    # Validate source tree file hashes match\n    get_logger().debug('Validating substituted files in source tree...')\n    with (extract_path / _INDEX_LIST).open('rb') as index_file: #pylint: disable=no-member\n        if not _validate_file_index(index_file, resolved_tree, cache_index_files):\n            raise KeyError('Domain substitution cache file index is corrupt or hashes mismatch '\n                           'the source tree.')\n\n    # Move original files over substituted ones\n    get_logger().debug('Moving original files over substituted ones...')\n    for relative_path in cache_index_files:\n        with _update_timestamp(resolved_tree / relative_path, set_new=False):\n            (extract_path / _ORIG_DIR / relative_path).replace(resolved_tree / relative_path)\n\n    # Quick check for unused files in cache\n    orig_has_unused = False\n    for orig_path in (extract_path / _ORIG_DIR).rglob('*'): #pylint: disable=no-member\n        if orig_path.is_file():\n            get_logger().warning('Unused file from cache: %s', orig_path)\n            orig_has_unused = True\n\nif orig_has_unused:\n    get_logger().warning('Cache contains unused files. Not removing.')\nelse:\n    domainsub_cache.unlink()", "path": "ungoogled-chromium/utils/domain_substitution.py", "commit_date": "2020-11-21 00:00:00", "repo_name": "ungoogled-software/ungoogled-chromium", "stars": 18486, "license": "bsd-3-clause", "language": "python", "size": 14775}
{"docstring": "# Use total_blocks to handle case total_size < block_size\n# total_blocks is ceiling of total_size / block_size\n# Ceiling division from: https://stackoverflow.com/a/17511341\n", "func_signal": "def __call__(self, block_count, block_size, total_size):\n", "code": "total_blocks = -(-total_size // block_size)\nif total_blocks > 0:\n    # Do not needlessly update the console. Since the console is\n    # updated synchronously, we don't want updating the console to\n    # bottleneck downloading. Thus, only refresh the output when the\n    # displayed value should change.\n    percentage = round(block_count / total_blocks, ndigits=3)\n    if percentage == self._last_percentage:\n        return\n    self._last_percentage = percentage\n    print('\\r' + ' ' * self._max_len_printed, end='')\n    status_line = 'Progress: {:.1%} of {:,d} B'.format(percentage, total_size)\nelse:\n    downloaded_estimate = block_count * block_size\n    status_line = 'Progress: {:,d} B of unknown size'.format(downloaded_estimate)\nself._max_len_printed = len(status_line)\nprint('\\r' + status_line, end='')", "path": "ungoogled-chromium/utils/downloads.py", "commit_date": "2020-08-25 00:00:00", "repo_name": "ungoogled-software/ungoogled-chromium", "stars": 18486, "license": "bsd-3-clause", "language": "python", "size": 14775}
{"docstring": "\"\"\"\nReturns a single expression to search for domains\n\"\"\"\n", "func_signal": "def search_regex(self):\n", "code": "return re.compile('|'.join(\n    map(lambda x: x.split(self._PATTERN_REPLACE_DELIM, 1)[0], self._data)))", "path": "ungoogled-chromium/utils/domain_substitution.py", "commit_date": "2020-11-21 00:00:00", "repo_name": "ungoogled-software/ungoogled-chromium", "stars": 18486, "license": "bsd-3-clause", "language": "python", "size": 14775}
{"docstring": "\"\"\"\nUnpack downloads in the downloads cache to output_dir. Assumes all downloads are retrieved.\n\ndownload_info is the DownloadInfo of downloads to unpack.\ncache_dir is the pathlib.Path directory containing the download cache\noutput_dir is the pathlib.Path directory to unpack the downloads to.\nextractors is a dictionary of PlatformEnum to a command or path to the\n    extractor binary. Defaults to 'tar' for tar, and '_use_registry' for 7-Zip and WinRAR.\n\nMay raise undetermined exceptions during archive unpacking.\n\"\"\"\n", "func_signal": "def unpack_downloads(download_info, cache_dir, output_dir, extractors=None):\n", "code": "for download_name, download_properties in download_info.properties_iter():\n    download_path = cache_dir / download_properties.download_filename\n    get_logger().info('Unpacking \"%s\" to %s ...', download_name,\n                      download_properties.output_path)\n    extractor_name = download_properties.extractor or ExtractorEnum.TAR\n    if extractor_name == ExtractorEnum.SEVENZIP:\n        extractor_func = extract_with_7z\n    elif extractor_name == ExtractorEnum.WINRAR:\n        extractor_func = extract_with_winrar\n    elif extractor_name == ExtractorEnum.TAR:\n        extractor_func = extract_tar_file\n    else:\n        raise NotImplementedError(extractor_name)\n\n    if download_properties.strip_leading_dirs is None:\n        strip_leading_dirs_path = None\n    else:\n        strip_leading_dirs_path = Path(download_properties.strip_leading_dirs)\n\n    extractor_func(\n        archive_path=download_path,\n        output_dir=output_dir / Path(download_properties.output_path),\n        relative_to=strip_leading_dirs_path,\n        extractors=extractors)", "path": "ungoogled-chromium/utils/downloads.py", "commit_date": "2020-08-25 00:00:00", "repo_name": "ungoogled-software/ungoogled-chromium", "stars": 18486, "license": "bsd-3-clause", "language": "python", "size": 14775}
{"docstring": "\"\"\"\nReturns a tuple of compiled regex pairs\n\"\"\"\n", "func_signal": "def regex_pairs(self):\n", "code": "if not self._compiled_regex:\n    self._compiled_regex = tuple(map(self._compile_regex, self._data))\nreturn self._compiled_regex", "path": "ungoogled-chromium/utils/domain_substitution.py", "commit_date": "2020-11-21 00:00:00", "repo_name": "ungoogled-software/ungoogled-chromium", "stars": 18486, "license": "bsd-3-clause", "language": "python", "size": 14775}
{"docstring": "\"\"\"CLI Callback\"\"\"\n", "func_signal": "def _callback(args):\n", "code": "if args.reverting:\n    revert_substitution(args.cache, args.directory)\nelse:\n    apply_substitution(args.regex, args.files, args.directory, args.cache)", "path": "ungoogled-chromium/utils/domain_substitution.py", "commit_date": "2020-11-21 00:00:00", "repo_name": "ungoogled-software/ungoogled-chromium", "stars": 18486, "license": "bsd-3-clause", "language": "python", "size": 14775}
{"docstring": "\"\"\"\nRetrieve downloads into the downloads cache.\n\ndownload_info is the DowloadInfo of downloads to retrieve.\ncache_dir is the pathlib.Path to the downloads cache.\nshow_progress is a boolean indicating if download progress is printed to the console.\ndisable_ssl_verification is a boolean indicating if certificate verification\n    should be disabled for downloads using HTTPS.\n\nRaises FileNotFoundError if the downloads path does not exist.\nRaises NotADirectoryError if the downloads path is not a directory.\n\"\"\"\n", "func_signal": "def retrieve_downloads(download_info, cache_dir, show_progress, disable_ssl_verification=False):\n", "code": "if not cache_dir.exists():\n    raise FileNotFoundError(cache_dir)\nif not cache_dir.is_dir():\n    raise NotADirectoryError(cache_dir)\nfor download_name, download_properties in download_info.properties_iter():\n    get_logger().info('Downloading \"%s\" to \"%s\" ...', download_name,\n                      download_properties.download_filename)\n    download_path = cache_dir / download_properties.download_filename\n    _download_if_needed(download_path, download_properties.url, show_progress,\n                        disable_ssl_verification)\n    if download_properties.has_hash_url():\n        get_logger().info('Downloading hashes for \"%s\"', download_name)\n        _, hash_filename, hash_url = download_properties.hashes['hash_url']\n        _download_if_needed(cache_dir / hash_filename, hash_url, show_progress,\n                            disable_ssl_verification)", "path": "ungoogled-chromium/utils/downloads.py", "commit_date": "2020-08-25 00:00:00", "repo_name": "ungoogled-software/ungoogled-chromium", "stars": 18486, "license": "bsd-3-clause", "language": "python", "size": 14775}
{"docstring": "\"\"\"\nPerform domain substitution on path and add it to the domain substitution cache.\n\npath is a pathlib.Path to the file to be domain substituted.\nregex_iter is an iterable of regular expression namedtuple like from\n    config.DomainRegexList.regex_pairs()\n\nReturns a tuple of the CRC32 hash of the substituted raw content and the\n    original raw content; None for both entries if no substitutions were made.\n\nRaises FileNotFoundError if path does not exist.\nRaises UnicodeDecodeError if path's contents cannot be decoded.\n\"\"\"\n", "func_signal": "def _substitute_path(path, regex_iter):\n", "code": "if not os.access(path, os.W_OK):\n    # If the patch cannot be written to, it cannot be opened for updating\n    print(str(path) + \" cannot be opened for writing! Adding write permission...\")\n    path.chmod(path.stat().st_mode | stat.S_IWUSR)\nwith path.open('r+b') as input_file:\n    original_content = input_file.read()\n    if not original_content:\n        return (None, None)\n    content = None\n    encoding = None\n    for encoding in TREE_ENCODINGS:\n        try:\n            content = original_content.decode(encoding)\n            break\n        except UnicodeDecodeError:\n            continue\n    if not content:\n        raise UnicodeDecodeError('Unable to decode with any encoding: %s' % path)\n    file_subs = 0\n    for regex_pair in regex_iter:\n        content, sub_count = regex_pair.pattern.subn(regex_pair.replacement, content)\n        file_subs += sub_count\n    if file_subs > 0:\n        substituted_content = content.encode(encoding)\n        input_file.seek(0)\n        input_file.write(content.encode(encoding))\n        input_file.truncate()\n        return (zlib.crc32(substituted_content), original_content)\n    return (None, None)", "path": "ungoogled-chromium/utils/domain_substitution.py", "commit_date": "2020-11-21 00:00:00", "repo_name": "ungoogled-software/ungoogled-chromium", "stars": 18486, "license": "bsd-3-clause", "language": "python", "size": 14775}
{"docstring": "\"\"\"\nParses an INI file located at path\n\nRaises schema.SchemaError if validation fails\n\"\"\"\n\n", "func_signal": "def _parse_data(self, path):\n", "code": "def _section_generator(data):\n    for section in data:\n        if section == configparser.DEFAULTSECT:\n            continue\n        yield section, dict(\n            filter(lambda x: x[0] not in self._ini_vars, data.items(section)))\n\nnew_data = configparser.ConfigParser(defaults=self._ini_vars)\nwith path.open(encoding=ENCODING) as ini_file:\n    new_data.read_file(ini_file, source=str(path))\ntry:\n    self._schema.validate(dict(_section_generator(new_data)))\nexcept schema.SchemaError as exc:\n    get_logger().error('downloads.ini failed schema validation (located in %s)', path)\n    raise exc\nreturn new_data", "path": "ungoogled-chromium/utils/downloads.py", "commit_date": "2020-08-25 00:00:00", "repo_name": "ungoogled-software/ungoogled-chromium", "stars": 18486, "license": "bsd-3-clause", "language": "python", "size": 14775}
{"docstring": "\"\"\"CLI Entrypoint\"\"\"\n", "func_signal": "def main():\n", "code": "parser = argparse.ArgumentParser(description=__doc__)\nadd_common_params(parser)\nsubparsers = parser.add_subparsers(title='Download actions', dest='action')\n\n# retrieve\nretrieve_parser = subparsers.add_parser(\n    'retrieve',\n    help='Retrieve and check download files',\n    description=('Retrieves and checks downloads without unpacking. '\n                 'The downloader will attempt to use CLI command \"curl\". '\n                 'If it is not present, Python\\'s urllib will be used. However, only '\n                 'the CLI-based downloaders can be resumed if the download is aborted.'))\n_add_common_args(retrieve_parser)\nretrieve_parser.add_argument(\n    '--hide-progress-bar',\n    action='store_false',\n    dest='show_progress',\n    help='Hide the download progress.')\nretrieve_parser.add_argument(\n    '--disable-ssl-verification',\n    action='store_true',\n    help='Disables certification verification for downloads using HTTPS.')\nretrieve_parser.set_defaults(callback=_retrieve_callback)\n\n# unpack\nunpack_parser = subparsers.add_parser(\n    'unpack',\n    help='Unpack download files',\n    description='Verifies hashes of and unpacks download files into the specified directory.')\n_add_common_args(unpack_parser)\nunpack_parser.add_argument(\n    '--tar-path',\n    default='tar',\n    help=('(Linux and macOS only) Command or path to the BSD or GNU tar '\n          'binary for extraction. Default: %(default)s'))\nunpack_parser.add_argument(\n    '--7z-path',\n    dest='sevenz_path',\n    default=USE_REGISTRY,\n    help=('Command or path to 7-Zip\\'s \"7z\" binary. If \"_use_registry\" is '\n          'specified, determine the path from the registry. Default: %(default)s'))\nunpack_parser.add_argument(\n    '--winrar-path',\n    dest='winrar_path',\n    default=USE_REGISTRY,\n    help=('Command or path to WinRAR\\'s \"winrar\" binary. If \"_use_registry\" is '\n          'specified, determine the path from the registry. Default: %(default)s'))\nunpack_parser.add_argument('output', type=Path, help='The directory to unpack to.')\nunpack_parser.set_defaults(callback=_unpack_callback)\n\nargs = parser.parse_args()\nargs.callback(args)", "path": "ungoogled-chromium/utils/downloads.py", "commit_date": "2020-08-25 00:00:00", "repo_name": "ungoogled-software/ungoogled-chromium", "stars": 18486, "license": "bsd-3-clause", "language": "python", "size": 14775}
{"docstring": "\"\"\"Generator of (hash_name, hash_hex) for the given download\"\"\"\n", "func_signal": "def _get_hash_pairs(download_properties, cache_dir):\n", "code": "for entry_type, entry_value in download_properties.hashes.items():\n    if entry_type == 'hash_url':\n        hash_processor, hash_filename, _ = entry_value\n        if hash_processor == 'chromium':\n            yield from _chromium_hashes_generator(cache_dir / hash_filename)\n        else:\n            raise ValueError('Unknown hash_url processor: %s' % hash_processor)\n    else:\n        yield entry_type, entry_value", "path": "ungoogled-chromium/utils/downloads.py", "commit_date": "2020-08-25 00:00:00", "repo_name": "ungoogled-software/ungoogled-chromium", "stars": 18486, "license": "bsd-3-clause", "language": "python", "size": 14775}
{"docstring": "# Initialize new population\n", "func_signal": "def run(self, iterations):\n", "code": "self._initialize()\n\nfor epoch in range(iterations):\n    population_fitness = self._calculate_fitness()\n\n    fittest_individual = self.population[np.argmax(population_fitness)]\n    highest_fitness = max(population_fitness)\n\n    # If we have found individual which matches the target => Done\n    if fittest_individual == self.target:\n        break\n\n    # Set the probability that the individual should be selected as a parent\n    # proportionate to the individual's fitness.\n    parent_probabilities = [fitness / sum(population_fitness) for fitness in population_fitness]\n\n    # Determine the next generation\n    new_population = []\n    for i in np.arange(0, self.population_size, 2):\n        # Select two parents randomly according to probabilities\n        parent1, parent2 = np.random.choice(self.population, size=2, p=parent_probabilities, replace=False)\n        # Perform crossover to produce offspring\n        child1, child2 = self._crossover(parent1, parent2)\n        # Save mutated offspring for next generation\n        new_population += [self._mutate(child1), self._mutate(child2)]\n\n    print (\"[%d Closest Candidate: '%s', Fitness: %.2f]\" % (epoch, fittest_individual, highest_fitness))\n    self.population = new_population\n\nprint (\"[%d Answer: '%s']\" % (epoch, fittest_individual))", "path": "ML-From-Scratch/mlfromscratch/unsupervised_learning/genetic_algorithm.py", "commit_date": "2018-08-30 00:00:00", "repo_name": "eriklindernoren/ML-From-Scratch", "stars": 22984, "license": "mit", "language": "python", "size": 553}
{"docstring": "# If not gradient descent => Least squares approximation of w\n", "func_signal": "def fit(self, X, y):\n", "code": "if not self.gradient_descent:\n    # Insert constant ones for bias weights\n    X = np.insert(X, 0, 1, axis=1)\n    # Calculate weights by least squares (using Moore-Penrose pseudoinverse)\n    U, S, V = np.linalg.svd(X.T.dot(X))\n    S = np.diag(S)\n    X_sq_reg_inv = V.dot(np.linalg.pinv(S)).dot(U.T)\n    self.w = X_sq_reg_inv.dot(X.T).dot(y)\nelse:\n    super(LinearRegression, self).fit(X, y)", "path": "ML-From-Scratch/mlfromscratch/supervised_learning/regression.py", "commit_date": "2018-01-12 00:00:00", "repo_name": "eriklindernoren/ML-From-Scratch", "stars": 22984, "license": "mit", "language": "python", "size": 553}
{"docstring": "# Avoid division by zero\n", "func_signal": "def gradient(self, y, p):\n", "code": "p = np.clip(p, 1e-15, 1 - 1e-15)\nreturn - (y / p) + (1 - y) / (1 - p)", "path": "ML-From-Scratch/mlfromscratch/deep_learning/loss_functions.py", "commit_date": "2017-09-20 00:00:00", "repo_name": "eriklindernoren/ML-From-Scratch", "stars": 22984, "license": "mit", "language": "python", "size": 553}
{"docstring": "# Insert constant ones for bias weights\n", "func_signal": "def predict(self, X):\n", "code": "X = np.insert(X, 0, 1, axis=1)\ny_pred = X.dot(self.w)\nreturn y_pred", "path": "ML-From-Scratch/mlfromscratch/supervised_learning/regression.py", "commit_date": "2018-01-12 00:00:00", "repo_name": "eriklindernoren/ML-From-Scratch", "stars": 22984, "license": "mit", "language": "python", "size": 553}
{"docstring": "\"\"\" Return the index of the closest medoid to the sample \"\"\"\n", "func_signal": "def _closest_medoid(self, sample, medoids):\n", "code": "closest_i = None\nclosest_distance = float(\"inf\")\nfor i, medoid in enumerate(medoids):\n    distance = euclidean_distance(sample, medoid)\n    if distance < closest_distance:\n        closest_i = i\n        closest_distance = distance\nreturn closest_i", "path": "ML-From-Scratch/mlfromscratch/unsupervised_learning/partitioning_around_medoids.py", "commit_date": "2017-09-25 00:00:00", "repo_name": "eriklindernoren/ML-From-Scratch", "stars": 22984, "license": "mit", "language": "python", "size": 553}
{"docstring": "\"\"\" Fit the dataset to the number of principal components specified in the\nconstructor and return the transformed dataset \"\"\"\n", "func_signal": "def transform(self, X, n_components):\n", "code": "covariance_matrix = calculate_covariance_matrix(X)\n\n# Where (eigenvector[:,0] corresponds to eigenvalue[0])\neigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n\n# Sort the eigenvalues and corresponding eigenvectors from largest\n# to smallest eigenvalue and select the first n_components\nidx = eigenvalues.argsort()[::-1]\neigenvalues = eigenvalues[idx][:n_components]\neigenvectors = np.atleast_1d(eigenvectors[:, idx])[:, :n_components]\n\n# Project the data onto principal components\nX_transformed = X.dot(eigenvectors)\n\nreturn X_transformed", "path": "ML-From-Scratch/mlfromscratch/unsupervised_learning/principal_component_analysis.py", "commit_date": "2018-11-02 00:00:00", "repo_name": "eriklindernoren/ML-From-Scratch", "stars": 22984, "license": "mit", "language": "python", "size": 553}
{"docstring": "\"\"\" Calculate the cost (total distance between samples and their medoids) \"\"\"\n", "func_signal": "def _calculate_cost(self, X, clusters, medoids):\n", "code": "cost = 0\n# For each cluster\nfor i, cluster in enumerate(clusters):\n    medoid = medoids[i]\n    for sample_i in cluster:\n        # Add distance between sample and medoid as cost\n        cost += euclidean_distance(X[sample_i], medoid)\nreturn cost", "path": "ML-From-Scratch/mlfromscratch/unsupervised_learning/partitioning_around_medoids.py", "commit_date": "2017-09-25 00:00:00", "repo_name": "eriklindernoren/ML-From-Scratch", "stars": 22984, "license": "mit", "language": "python", "size": 553}
{"docstring": "# Load the dataset\n", "func_signal": "def main():\n", "code": "X, y = datasets.make_blobs()\n\n# Cluster the data using K-Means\nclf = KMeans(k=3)\ny_pred = clf.predict(X)\n\n# Project the data onto the 2 primary principal components\np = Plot()\np.plot_in_2d(X, y_pred, title=\"K-Means Clustering\")\np.plot_in_2d(X, y, title=\"Actual Clustering\")", "path": "ML-From-Scratch/mlfromscratch/examples/k_means.py", "commit_date": "2017-09-20 00:00:00", "repo_name": "eriklindernoren/ML-From-Scratch", "stars": 22984, "license": "mit", "language": "python", "size": 553}
{"docstring": "# Load the dataset\n", "func_signal": "def main():\n", "code": "X, y = datasets.make_blobs()\n\n# Cluster the data\nclf = GaussianMixtureModel(k=3)\ny_pred = clf.predict(X)\n\np = Plot()\np.plot_in_2d(X, y_pred, title=\"GMM Clustering\")\np.plot_in_2d(X, y, title=\"Actual Clustering\")", "path": "ML-From-Scratch/mlfromscratch/examples/gaussian_mixture_model.py", "commit_date": "2017-09-18 00:00:00", "repo_name": "eriklindernoren/ML-From-Scratch", "stars": 22984, "license": "mit", "language": "python", "size": 553}
{"docstring": "\"\"\" Assign the samples to the closest medoids to create clusters \"\"\"\n", "func_signal": "def _create_clusters(self, X, medoids):\n", "code": "clusters = [[] for _ in range(self.k)]\nfor sample_i, sample in enumerate(X):\n    medoid_i = self._closest_medoid(sample, medoids)\n    clusters[medoid_i].append(sample_i)\nreturn clusters", "path": "ML-From-Scratch/mlfromscratch/unsupervised_learning/partitioning_around_medoids.py", "commit_date": "2017-09-25 00:00:00", "repo_name": "eriklindernoren/ML-From-Scratch", "stars": 22984, "license": "mit", "language": "python", "size": 553}
{"docstring": "\"\"\" Initialize population with random strings \"\"\"\n", "func_signal": "def _initialize(self):\n", "code": "self.population = []\nfor _ in range(self.population_size):\n    # Select random letters as new individual\n    individual = \"\".join(np.random.choice(self.letters, size=len(self.target)))\n    self.population.append(individual)", "path": "ML-From-Scratch/mlfromscratch/unsupervised_learning/genetic_algorithm.py", "commit_date": "2018-08-30 00:00:00", "repo_name": "eriklindernoren/ML-From-Scratch", "stars": 22984, "license": "mit", "language": "python", "size": 553}
{"docstring": "# Avoid division by zero\n", "func_signal": "def loss(self, y, p):\n", "code": "p = np.clip(p, 1e-15, 1 - 1e-15)\nreturn - y * np.log(p) - (1 - y) * np.log(1 - p)", "path": "ML-From-Scratch/mlfromscratch/deep_learning/loss_functions.py", "commit_date": "2017-09-20 00:00:00", "repo_name": "eriklindernoren/ML-From-Scratch", "stars": 22984, "license": "mit", "language": "python", "size": 553}
{"docstring": "\"\"\" Initialize the medoids as random samples \"\"\"\n", "func_signal": "def _init_random_medoids(self, X):\n", "code": "n_samples, n_features = np.shape(X)\nmedoids = np.zeros((self.k, n_features))\nfor i in range(self.k):\n    medoid = X[np.random.choice(range(n_samples))]\n    medoids[i] = medoid\nreturn medoids", "path": "ML-From-Scratch/mlfromscratch/unsupervised_learning/partitioning_around_medoids.py", "commit_date": "2017-09-25 00:00:00", "repo_name": "eriklindernoren/ML-From-Scratch", "stars": 22984, "license": "mit", "language": "python", "size": 553}
{"docstring": "\"\"\" Randomly change the individual's characters with probability\nself.mutation_rate \"\"\"\n", "func_signal": "def _mutate(self, individual):\n", "code": "individual = list(individual)\nfor j in range(len(individual)):\n    # Make change with probability mutation_rate\n    if np.random.random() < self.mutation_rate:\n        individual[j] = np.random.choice(self.letters)\n# Return mutated individual as string\nreturn \"\".join(individual)", "path": "ML-From-Scratch/mlfromscratch/unsupervised_learning/genetic_algorithm.py", "commit_date": "2018-08-30 00:00:00", "repo_name": "eriklindernoren/ML-From-Scratch", "stars": 22984, "license": "mit", "language": "python", "size": 553}
{"docstring": "# Separate data by class\n", "func_signal": "def fit(self, X, y):\n", "code": "X1 = X[y == 0]\nX2 = X[y == 1]\n\n# Calculate the covariance matrices of the two datasets\ncov1 = calculate_covariance_matrix(X1)\ncov2 = calculate_covariance_matrix(X2)\ncov_tot = cov1 + cov2\n\n# Calculate the mean of the two datasets\nmean1 = X1.mean(0)\nmean2 = X2.mean(0)\nmean_diff = np.atleast_1d(mean1 - mean2)\n\n# Determine the vector which when X is projected onto it best separates the\n# data by class. w = (mean1 - mean2) / (cov1 + cov2)\nself.w = np.linalg.pinv(cov_tot).dot(mean_diff)", "path": "ML-From-Scratch/mlfromscratch/supervised_learning/linear_discriminant_analysis.py", "commit_date": "2018-05-03 00:00:00", "repo_name": "eriklindernoren/ML-From-Scratch", "stars": 22984, "license": "mit", "language": "python", "size": 553}
{"docstring": "\"\"\" Classify samples as the index of their clusters \"\"\"\n# One prediction for each sample\n", "func_signal": "def _get_cluster_labels(self, clusters, X):\n", "code": "y_pred = np.zeros(np.shape(X)[0])\nfor cluster_i in range(len(clusters)):\n    cluster = clusters[cluster_i]\n    for sample_i in cluster:\n        y_pred[sample_i] = cluster_i\nreturn y_pred", "path": "ML-From-Scratch/mlfromscratch/unsupervised_learning/partitioning_around_medoids.py", "commit_date": "2017-09-25 00:00:00", "repo_name": "eriklindernoren/ML-From-Scratch", "stars": 22984, "license": "mit", "language": "python", "size": 553}
{"docstring": "\"\"\" Returns a list of all samples that are not currently medoids \"\"\"\n", "func_signal": "def _get_non_medoids(self, X, medoids):\n", "code": "non_medoids = []\nfor sample in X:\n    if not sample in medoids:\n        non_medoids.append(sample)\nreturn non_medoids", "path": "ML-From-Scratch/mlfromscratch/unsupervised_learning/partitioning_around_medoids.py", "commit_date": "2017-09-25 00:00:00", "repo_name": "eriklindernoren/ML-From-Scratch", "stars": 22984, "license": "mit", "language": "python", "size": 553}
{"docstring": "\"\"\" Calculates the fitness of each individual in the population \"\"\"\n", "func_signal": "def _calculate_fitness(self):\n", "code": "population_fitness = []\nfor individual in self.population:\n    # Calculate loss as the alphabetical distance between\n    # the characters in the individual and the target string\n    loss = 0\n    for i in range(len(individual)):\n        letter_i1 = self.letters.index(individual[i])\n        letter_i2 = self.letters.index(self.target[i])\n        loss += abs(letter_i1 - letter_i2)\n    fitness = 1 / (loss + 1e-6)\n    population_fitness.append(fitness)\nreturn population_fitness", "path": "ML-From-Scratch/mlfromscratch/unsupervised_learning/genetic_algorithm.py", "commit_date": "2018-08-30 00:00:00", "repo_name": "eriklindernoren/ML-From-Scratch", "stars": 22984, "license": "mit", "language": "python", "size": 553}
{"docstring": "\"\"\" Create children from parents by crossover \"\"\"\n# Select random crossover point\n", "func_signal": "def _crossover(self, parent1, parent2):\n", "code": "cross_i = np.random.randint(0, len(parent1))\nchild1 = parent1[:cross_i] + parent2[cross_i:]\nchild2 = parent2[:cross_i] + parent1[cross_i:]\nreturn child1, child2", "path": "ML-From-Scratch/mlfromscratch/unsupervised_learning/genetic_algorithm.py", "commit_date": "2018-08-30 00:00:00", "repo_name": "eriklindernoren/ML-From-Scratch", "stars": 22984, "license": "mit", "language": "python", "size": 553}
{"docstring": "\"\"\" Do Partitioning Around Medoids and return the cluster labels \"\"\"\n# Initialize medoids randomly\n", "func_signal": "def predict(self, X):\n", "code": "medoids = self._init_random_medoids(X)\n# Assign samples to closest medoids\nclusters = self._create_clusters(X, medoids)\n\n# Calculate the initial cost (total distance between samples and\n# corresponding medoids)\ncost = self._calculate_cost(X, clusters, medoids)\n\n# Iterate until we no longer have a cheaper cost\nwhile True:\n    best_medoids = medoids\n    lowest_cost = cost\n    for medoid in medoids:\n        # Get all non-medoid samples\n        non_medoids = self._get_non_medoids(X, medoids)\n        # Calculate the cost when swapping medoid and samples\n        for sample in non_medoids:\n            # Swap sample with the medoid\n            new_medoids = medoids.copy()\n            new_medoids[medoids == medoid] = sample\n            # Assign samples to new medoids\n            new_clusters = self._create_clusters(X, new_medoids)\n            # Calculate the cost with the new set of medoids\n            new_cost = self._calculate_cost(\n                X, new_clusters, new_medoids)\n            # If the swap gives us a lower cost we save the medoids and cost\n            if new_cost < lowest_cost:\n                lowest_cost = new_cost\n                best_medoids = new_medoids\n    # If there was a swap that resultet in a lower cost we save the\n    # resulting medoids from the best swap and the new cost \n    if lowest_cost < cost:\n        cost = lowest_cost\n        medoids = best_medoids \n    # Else finished\n    else:\n        break\n\nfinal_clusters = self._create_clusters(X, medoids)\n# Return the samples cluster indices as labels\nreturn self._get_cluster_labels(final_clusters, X)", "path": "ML-From-Scratch/mlfromscratch/unsupervised_learning/partitioning_around_medoids.py", "commit_date": "2017-09-25 00:00:00", "repo_name": "eriklindernoren/ML-From-Scratch", "stars": 22984, "license": "mit", "language": "python", "size": 553}
{"docstring": "\"\"\"Make sure the CLI object's name is the app's name and not the app itself\"\"\"\n", "func_signal": "def test_cli_name(test_apps):\n", "code": "from cliapp.app import testapp\n\nassert testapp.cli.name == testapp.name", "path": "flask/tests/test_cli.py", "commit_date": "2020-07-31 00:00:00", "repo_name": "pallets/flask", "stars": 65917, "license": "bsd-3-clause", "language": "python", "size": 10437}
{"docstring": "# change the post author to another user\n", "func_signal": "def test_author_required(app, client, auth):\n", "code": "with app.app_context():\n    db = get_db()\n    db.execute(\"UPDATE post SET author_id = 2 WHERE id = 1\")\n    db.commit()\n\nauth.login()\n# current user can't modify other user's post\nassert client.post(\"/1/update\").status_code == 403\nassert client.post(\"/1/delete\").status_code == 403\n# current user doesn't see edit link\nassert b'href=\"/1/update\"' not in client.get(\"/\").data", "path": "flask/examples/tutorial/tests/test_blog.py", "commit_date": "2019-06-01 00:00:00", "repo_name": "pallets/flask", "stars": 65917, "license": "bsd-3-clause", "language": "python", "size": 10437}
{"docstring": "\"\"\"Test blueprint commands register correctly to the application\"\"\"\n", "func_signal": "def test_cli_blueprints(app):\n", "code": "custom = Blueprint(\"custom\", __name__, cli_group=\"customized\")\nnested = Blueprint(\"nested\", __name__)\nmerged = Blueprint(\"merged\", __name__, cli_group=None)\nlate = Blueprint(\"late\", __name__)\n\n@custom.cli.command(\"custom\")\ndef custom_command():\n    click.echo(\"custom_result\")\n\n@nested.cli.command(\"nested\")\ndef nested_command():\n    click.echo(\"nested_result\")\n\n@merged.cli.command(\"merged\")\ndef merged_command():\n    click.echo(\"merged_result\")\n\n@late.cli.command(\"late\")\ndef late_command():\n    click.echo(\"late_result\")\n\napp.register_blueprint(custom)\napp.register_blueprint(nested)\napp.register_blueprint(merged)\napp.register_blueprint(late, cli_group=\"late_registration\")\n\napp_runner = app.test_cli_runner()\n\nresult = app_runner.invoke(args=[\"customized\", \"custom\"])\nassert \"custom_result\" in result.output\n\nresult = app_runner.invoke(args=[\"nested\", \"nested\"])\nassert \"nested_result\" in result.output\n\nresult = app_runner.invoke(args=[\"merged\"])\nassert \"merged_result\" in result.output\n\nresult = app_runner.invoke(args=[\"late_registration\", \"late\"])\nassert \"late_result\" in result.output", "path": "flask/tests/test_cli.py", "commit_date": "2020-07-31 00:00:00", "repo_name": "pallets/flask", "stars": 65917, "license": "bsd-3-clause", "language": "python", "size": 10437}
{"docstring": "\"\"\"Get the Flask app's logger and configure it if needed.\n\nThe logger name will be the same as\n:attr:`app.import_name <flask.Flask.name>`.\n\nWhen :attr:`~flask.Flask.debug` is enabled, set the logger level to\n:data:`logging.DEBUG` if it is not set.\n\nIf there is no handler for the logger's effective level, add a\n:class:`~logging.StreamHandler` for\n:func:`~flask.logging.wsgi_errors_stream` with a basic format.\n\"\"\"\n", "func_signal": "def create_logger(app):\n", "code": "logger = logging.getLogger(app.name)\n\nif app.debug and not logger.level:\n    logger.setLevel(logging.DEBUG)\n\nif not has_level_handler(logger):\n    logger.addHandler(default_handler)\n\nreturn logger", "path": "flask/src/flask/logging.py", "commit_date": "2020-04-04 00:00:00", "repo_name": "pallets/flask", "stars": 65917, "license": "bsd-3-clause", "language": "python", "size": 10437}
{"docstring": "# can't use monkeypatch.delitem since the keys don't exist yet\n", "func_signal": "def test_load_dotenv(monkeypatch):\n", "code": "for item in (\"FOO\", \"BAR\", \"SPAM\"):\n    monkeypatch._setitem.append((os.environ, item, notset))\n\nmonkeypatch.setenv(\"EGGS\", \"3\")\nmonkeypatch.chdir(test_path)\nassert load_dotenv()\nassert os.getcwd() == test_path\n# .flaskenv doesn't overwrite .env\nassert os.environ[\"FOO\"] == \"env\"\n# set only in .flaskenv\nassert os.environ[\"BAR\"] == \"bar\"\n# set only in .env\nassert os.environ[\"SPAM\"] == \"1\"\n# set manually, files don't overwrite\nassert os.environ[\"EGGS\"] == \"3\"\n\n# Non existent file should not load\nassert not load_dotenv(\"non-existent-file\")", "path": "flask/tests/test_cli.py", "commit_date": "2020-07-31 00:00:00", "repo_name": "pallets/flask", "stars": 65917, "license": "bsd-3-clause", "language": "python", "size": 10437}
{"docstring": "\"\"\"Check if there is a handler in the logging chain that will handle the\ngiven logger's :meth:`effective level <~logging.Logger.getEffectiveLevel>`.\n\"\"\"\n", "func_signal": "def has_level_handler(logger):\n", "code": "level = logger.getEffectiveLevel()\ncurrent = logger\n\nwhile current:\n    if any(handler.level <= level for handler in current.handlers):\n        return True\n\n    if not current.propagate:\n        break\n\n    current = current.parent\n\nreturn False", "path": "flask/src/flask/logging.py", "commit_date": "2020-04-04 00:00:00", "repo_name": "pallets/flask", "stars": 65917, "license": "bsd-3-clause", "language": "python", "size": 10437}
{"docstring": "\"\"\"Binds the app context to the current context.\"\"\"\n", "func_signal": "def push(self):\n", "code": "self._refcnt += 1\n_app_ctx_stack.push(self)\nappcontext_pushed.send(self.app)", "path": "flask/src/flask/ctx.py", "commit_date": "2020-04-04 00:00:00", "repo_name": "pallets/flask", "stars": 65917, "license": "bsd-3-clause", "language": "python", "size": 10437}
{"docstring": "\"\"\"Generic ``Exception`` will handle all exceptions directly,\nincluding ``HTTPExceptions``.\n\"\"\"\n\n", "func_signal": "def test_handle_generic(self, app, client):\n", "code": "@app.errorhandler(Exception)\ndef handle_exception(e):\n    return self.report_error(e)\n\nassert client.get(\"/custom\").data == b\"direct Custom\"\nassert client.get(\"/error\").data == b\"direct KeyError\"\nassert client.get(\"/abort\").data == b\"direct InternalServerError\"\nassert client.get(\"/not-found\").data == b\"direct NotFound\"", "path": "flask/tests/test_user_error_handler.py", "commit_date": "2020-04-16 00:00:00", "repo_name": "pallets/flask", "stars": 65917, "license": "bsd-3-clause", "language": "python", "size": 10437}
{"docstring": "\"\"\"Create and configure an instance of the Flask application.\"\"\"\n", "func_signal": "def create_app(test_config=None):\n", "code": "app = Flask(__name__, instance_relative_config=True)\napp.config.from_mapping(\n    # a default secret that should be overridden by instance config\n    SECRET_KEY=\"dev\",\n    # store the database in the instance folder\n    DATABASE=os.path.join(app.instance_path, \"flaskr.sqlite\"),\n)\n\nif test_config is None:\n    # load the instance config, if it exists, when not testing\n    app.config.from_pyfile(\"config.py\", silent=True)\nelse:\n    # load the test config if passed in\n    app.config.update(test_config)\n\n# ensure the instance folder exists\ntry:\n    os.makedirs(app.instance_path)\nexcept OSError:\n    pass\n\n@app.route(\"/hello\")\ndef hello():\n    return \"Hello, World!\"\n\n# register the database commands\nfrom flaskr import db\n\ndb.init_app(app)\n\n# apply the blueprints to the app\nfrom flaskr import auth, blog\n\napp.register_blueprint(auth.bp)\napp.register_blueprint(blog.bp)\n\n# make url_for('index') == url_for('blog.index')\n# in another app, you might define a separate main index here with\n# app.route, while giving the blog blueprint a url_prefix, but for\n# the tutorial the blog will be the main index\napp.add_url_rule(\"/\", endpoint=\"index\")\n\nreturn app", "path": "flask/examples/tutorial/flaskr/__init__.py", "commit_date": "2019-05-06 00:00:00", "repo_name": "pallets/flask", "stars": 65917, "license": "bsd-3-clause", "language": "python", "size": 10437}
{"docstring": "\"\"\"Executes a function after this request.  This is useful to modify\nresponse objects.  The function is passed the response object and has\nto return the same or a new one.\n\nExample::\n\n    @app.route('/')\n    def index():\n        @after_this_request\n        def add_header(response):\n            response.headers['X-Foo'] = 'Parachute'\n            return response\n        return 'Hello World!'\n\nThis is more useful if a function other than the view function wants to\nmodify a response.  For instance think of a decorator that wants to add\nsome headers without converting the return value into a response object.\n\n.. versionadded:: 0.9\n\"\"\"\n", "func_signal": "def after_this_request(f):\n", "code": "_request_ctx_stack.top._after_request_functions.append(f)\nreturn f", "path": "flask/src/flask/ctx.py", "commit_date": "2020-04-04 00:00:00", "repo_name": "pallets/flask", "stars": 65917, "license": "bsd-3-clause", "language": "python", "size": 10437}
{"docstring": "\"\"\"Pops the request context and unbinds it by doing that.  This will\nalso trigger the execution of functions registered by the\n:meth:`~flask.Flask.teardown_request` decorator.\n\n.. versionchanged:: 0.9\n   Added the `exc` argument.\n\"\"\"\n", "func_signal": "def pop(self, exc=_sentinel):\n", "code": "app_ctx = self._implicit_app_ctx_stack.pop()\nclear_request = False\n\ntry:\n    if not self._implicit_app_ctx_stack:\n        self.preserved = False\n        self._preserved_exc = None\n        if exc is _sentinel:\n            exc = sys.exc_info()[1]\n        self.app.do_teardown_request(exc)\n\n        request_close = getattr(self.request, \"close\", None)\n        if request_close is not None:\n            request_close()\n        clear_request = True\nfinally:\n    rv = _request_ctx_stack.pop()\n\n    # get rid of circular dependencies at the end of the request\n    # so that we don't require the GC to be active.\n    if clear_request:\n        rv.request.environ[\"werkzeug.request\"] = None\n\n    # Get rid of the app as well if necessary.\n    if app_ctx is not None:\n        app_ctx.pop(exc)\n\n    assert (\n        rv is self\n    ), f\"Popped wrong request context. ({rv!r} instead of {self!r})\"", "path": "flask/src/flask/ctx.py", "commit_date": "2020-04-04 00:00:00", "repo_name": "pallets/flask", "stars": 65917, "license": "bsd-3-clause", "language": "python", "size": 10437}
{"docstring": "\"\"\"Pops the app context.\"\"\"\n", "func_signal": "def pop(self, exc=_sentinel):\n", "code": "try:\n    self._refcnt -= 1\n    if self._refcnt <= 0:\n        if exc is _sentinel:\n            exc = sys.exc_info()[1]\n        self.app.do_teardown_appcontext(exc)\nfinally:\n    rv = _app_ctx_stack.pop()\nassert rv is self, f\"Popped wrong app context.  ({rv!r} instead of {self!r})\"\nappcontext_popped.send(self.app)", "path": "flask/src/flask/ctx.py", "commit_date": "2020-04-04 00:00:00", "repo_name": "pallets/flask", "stars": 65917, "license": "bsd-3-clause", "language": "python", "size": 10437}
{"docstring": "\"\"\"Renders a template from the given template source string\nwith the given context. Template variables will be autoescaped.\n\n:param source: the source code of the template to be\n               rendered\n:param context: the variables that should be available in the\n                context of the template.\n\"\"\"\n", "func_signal": "def render_template_string(source, **context):\n", "code": "ctx = _app_ctx_stack.top\nctx.app.update_template_context(context)\nreturn _render(ctx.app.jinja_env.from_string(source), context, ctx.app)", "path": "flask/src/flask/templating.py", "commit_date": "2020-04-04 00:00:00", "repo_name": "pallets/flask", "stars": 65917, "license": "bsd-3-clause", "language": "python", "size": 10437}
{"docstring": "\"\"\"If a Blueprint's CLI group is empty, do not register it.\"\"\"\n", "func_signal": "def test_cli_empty(app):\n", "code": "bp = Blueprint(\"blue\", __name__, cli_group=\"blue\")\napp.register_blueprint(bp)\n\nresult = app.test_cli_runner().invoke(args=[\"blue\", \"--help\"])\nassert result.exit_code == 2, f\"Unexpected success:\\n\\n{result.output}\"", "path": "flask/tests/test_cli.py", "commit_date": "2020-07-31 00:00:00", "repo_name": "pallets/flask", "stars": 65917, "license": "bsd-3-clause", "language": "python", "size": 10437}
{"docstring": "\"\"\"Renders the template and fires the signal\"\"\"\n\n", "func_signal": "def _render(template, context, app):\n", "code": "before_render_template.send(app, template=template, context=context)\nrv = template.render(context)\ntemplate_rendered.send(app, template=template, context=context)\nreturn rv", "path": "flask/src/flask/templating.py", "commit_date": "2020-04-04 00:00:00", "repo_name": "pallets/flask", "stars": 65917, "license": "bsd-3-clause", "language": "python", "size": 10437}
{"docstring": "\"\"\"``HTTPException`` should only receive ``HTTPException``\nsubclasses. It will receive ``404`` routing exceptions.\n\"\"\"\n\n", "func_signal": "def test_handle_generic_http(self, app, client):\n", "code": "@app.errorhandler(HTTPException)\ndef handle_http(e):\n    assert isinstance(e, HTTPException)\n    return str(e.code)\n\nassert client.get(\"/error\").data == b\"500\"\nassert client.get(\"/abort\").data == b\"500\"\nassert client.get(\"/not-found\").data == b\"404\"", "path": "flask/tests/test_user_error_handler.py", "commit_date": "2020-04-16 00:00:00", "repo_name": "pallets/flask", "stars": 65917, "license": "bsd-3-clause", "language": "python", "size": 10437}
{"docstring": "\"\"\"``InternalServerError`` and ``500`` are aliases, they should\nhave the same behavior. Both should only receive\n``InternalServerError``, which might wrap another error.\n\"\"\"\n\n", "func_signal": "def test_handle_class_or_code(self, app, client, to_handle):\n", "code": "@app.errorhandler(to_handle)\ndef handle_500(e):\n    assert isinstance(e, InternalServerError)\n    return self.report_error(e)\n\nassert client.get(\"/custom\").data == b\"wrapped Custom\"\nassert client.get(\"/error\").data == b\"wrapped KeyError\"\nassert client.get(\"/abort\").data == b\"direct InternalServerError\"\nassert client.get(\"/raise\").data == b\"direct InternalServerError\"", "path": "flask/tests/test_user_error_handler.py", "commit_date": "2020-04-16 00:00:00", "repo_name": "pallets/flask", "stars": 65917, "license": "bsd-3-clause", "language": "python", "size": 10437}
{"docstring": "# skip the header and match the start of each row\n", "func_signal": "def expect_order(self, order, output):\n", "code": "for expect, line in zip(order, output.splitlines()[2:]):\n    # do this instead of startswith for nicer pytest output\n    assert line[: len(expect)] == expect", "path": "flask/tests/test_cli.py", "commit_date": "2020-07-31 00:00:00", "repo_name": "pallets/flask", "stars": 65917, "license": "bsd-3-clause", "language": "python", "size": 10437}
{"docstring": "\"\"\"Expect the correct path to be set and the correct import and app names\nto be returned.\n\n:func:`prepare_exec_for_file` has a side effect where the parent directory\nof the given import is added to :data:`sys.path`. This is reset after the\ntest runs.\n\"\"\"\n", "func_signal": "def test_prepare_import(request, value, path, result):\n", "code": "original_path = sys.path[:]\n\ndef reset_path():\n    sys.path[:] = original_path\n\nrequest.addfinalizer(reset_path)\n\nassert prepare_import(value) == result\nassert sys.path[0] == path", "path": "flask/tests/test_cli.py", "commit_date": "2020-07-31 00:00:00", "repo_name": "pallets/flask", "stars": 65917, "license": "bsd-3-clause", "language": "python", "size": 10437}
{"docstring": "\"\"\"Default template context processor.  Injects `request`,\n`session` and `g`.\n\"\"\"\n", "func_signal": "def _default_template_ctx_processor():\n", "code": "reqctx = _request_ctx_stack.top\nappctx = _app_ctx_stack.top\nrv = {}\nif appctx is not None:\n    rv[\"g\"] = appctx.g\nif reqctx is not None:\n    rv[\"request\"] = reqctx.request\n    rv[\"session\"] = reqctx.session\nreturn rv", "path": "flask/src/flask/templating.py", "commit_date": "2020-04-04 00:00:00", "repo_name": "pallets/flask", "stars": 65917, "license": "bsd-3-clause", "language": "python", "size": 10437}
{"docstring": "\"\"\"Read a SQuAD json file into a list of SquadExample.\"\"\"\n", "func_signal": "def read_squad_examples(input_file, is_training):\n", "code": "with tf.gfile.Open(input_file, \"r\") as reader:\n  input_data = json.load(reader)[\"data\"]\n\ndef is_whitespace(c):\n  if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n    return True\n  return False\n\nexamples = []\nfor entry in input_data:\n  for paragraph in entry[\"paragraphs\"]:\n    paragraph_text = paragraph[\"context\"]\n    doc_tokens = []\n    char_to_word_offset = []\n    prev_is_whitespace = True\n    for c in paragraph_text:\n      if is_whitespace(c):\n        prev_is_whitespace = True\n      else:\n        if prev_is_whitespace:\n          doc_tokens.append(c)\n        else:\n          doc_tokens[-1] += c\n        prev_is_whitespace = False\n      char_to_word_offset.append(len(doc_tokens) - 1)\n\n    for qa in paragraph[\"qas\"]:\n      qas_id = qa[\"id\"]\n      question_text = qa[\"question\"]\n      start_position = None\n      end_position = None\n      orig_answer_text = None\n      is_impossible = False\n      if is_training:\n\n        if FLAGS.version_2_with_negative:\n          is_impossible = qa[\"is_impossible\"]\n        if (len(qa[\"answers\"]) != 1) and (not is_impossible):\n          raise ValueError(\n              \"For training, each question should have exactly 1 answer.\")\n        if not is_impossible:\n          answer = qa[\"answers\"][0]\n          orig_answer_text = answer[\"text\"]\n          answer_offset = answer[\"answer_start\"]\n          answer_length = len(orig_answer_text)\n          start_position = char_to_word_offset[answer_offset]\n          end_position = char_to_word_offset[answer_offset + answer_length -\n                                             1]\n          # Only add answers where the text can be exactly recovered from the\n          # document. If this CAN'T happen it's likely due to weird Unicode\n          # stuff so we will just skip the example.\n          #\n          # Note that this means for training mode, every example is NOT\n          # guaranteed to be preserved.\n          actual_text = \" \".join(\n              doc_tokens[start_position:(end_position + 1)])\n          cleaned_answer_text = \" \".join(\n              tokenization.whitespace_tokenize(orig_answer_text))\n          if actual_text.find(cleaned_answer_text) == -1:\n            tf.logging.warning(\"Could not find answer: '%s' vs. '%s'\",\n                               actual_text, cleaned_answer_text)\n            continue\n        else:\n          start_position = -1\n          end_position = -1\n          orig_answer_text = \"\"\n\n      example = SquadExample(\n          qas_id=qas_id,\n          question_text=question_text,\n          doc_tokens=doc_tokens,\n          orig_answer_text=orig_answer_text,\n          start_position=start_position,\n          end_position=end_position,\n          is_impossible=is_impossible)\n      examples.append(example)\n\nreturn examples", "path": "bert/run_squad.py", "commit_date": "2018-12-18 00:00:00", "repo_name": "google-research/bert", "stars": 36595, "license": "apache-2.0", "language": "python", "size": 317}
{"docstring": "\"\"\"Compute the union of the current variables and checkpoint variables.\"\"\"\n", "func_signal": "def get_assignment_map_from_checkpoint(tvars, init_checkpoint):\n", "code": "assignment_map = {}\ninitialized_variable_names = {}\n\nname_to_variable = collections.OrderedDict()\nfor var in tvars:\n  name = var.name\n  m = re.match(\"^(.*):\\\\d+$\", name)\n  if m is not None:\n    name = m.group(1)\n  name_to_variable[name] = var\n\ninit_vars = tf.train.list_variables(init_checkpoint)\n\nassignment_map = collections.OrderedDict()\nfor x in init_vars:\n  (name, var) = (x[0], x[1])\n  if name not in name_to_variable:\n    continue\n  assignment_map[name] = name\n  initialized_variable_names[name] = 1\n  initialized_variable_names[name + \":0\"] = 1\n\nreturn (assignment_map, initialized_variable_names)", "path": "bert/modeling.py", "commit_date": "2019-02-07 00:00:00", "repo_name": "google-research/bert", "stars": 36595, "license": "apache-2.0", "language": "python", "size": 317}
{"docstring": "\"\"\"Reshapes a rank 2 tensor back to its original rank >= 2 tensor.\"\"\"\n", "func_signal": "def reshape_from_matrix(output_tensor, orig_shape_list):\n", "code": "if len(orig_shape_list) == 2:\n  return output_tensor\n\noutput_shape = get_shape_list(output_tensor)\n\norig_dims = orig_shape_list[0:-1]\nwidth = output_shape[-1]\n\nreturn tf.reshape(output_tensor, orig_dims + [width])", "path": "bert/modeling.py", "commit_date": "2019-02-07 00:00:00", "repo_name": "google-research/bert", "stars": 36595, "license": "apache-2.0", "language": "python", "size": 317}
{"docstring": "\"\"\"Project the tokenized prediction back to the original text.\"\"\"\n\n# When we created the data, we kept track of the alignment between original\n# (whitespace tokenized) tokens and our WordPiece tokenized tokens. So\n# now `orig_text` contains the span of our original text corresponding to the\n# span that we predicted.\n#\n# However, `orig_text` may contain extra characters that we don't want in\n# our prediction.\n#\n# For example, let's say:\n#   pred_text = steve smith\n#   orig_text = Steve Smith's\n#\n# We don't want to return `orig_text` because it contains the extra \"'s\".\n#\n# We don't want to return `pred_text` because it's already been normalized\n# (the SQuAD eval script also does punctuation stripping/lower casing but\n# our tokenizer does additional normalization like stripping accent\n# characters).\n#\n# What we really want to return is \"Steve Smith\".\n#\n# Therefore, we have to apply a semi-complicated alignment heruistic between\n# `pred_text` and `orig_text` to get a character-to-charcter alignment. This\n# can fail in certain cases in which case we just return `orig_text`.\n\n", "func_signal": "def get_final_text(pred_text, orig_text, do_lower_case):\n", "code": "def _strip_spaces(text):\n  ns_chars = []\n  ns_to_s_map = collections.OrderedDict()\n  for (i, c) in enumerate(text):\n    if c == \" \":\n      continue\n    ns_to_s_map[len(ns_chars)] = i\n    ns_chars.append(c)\n  ns_text = \"\".join(ns_chars)\n  return (ns_text, ns_to_s_map)\n\n# We first tokenize `orig_text`, strip whitespace from the result\n# and `pred_text`, and check if they are the same length. If they are\n# NOT the same length, the heuristic has failed. If they are the same\n# length, we assume the characters are one-to-one aligned.\ntokenizer = tokenization.BasicTokenizer(do_lower_case=do_lower_case)\n\ntok_text = \" \".join(tokenizer.tokenize(orig_text))\n\nstart_position = tok_text.find(pred_text)\nif start_position == -1:\n  if FLAGS.verbose_logging:\n    tf.logging.info(\n        \"Unable to find text: '%s' in '%s'\" % (pred_text, orig_text))\n  return orig_text\nend_position = start_position + len(pred_text) - 1\n\n(orig_ns_text, orig_ns_to_s_map) = _strip_spaces(orig_text)\n(tok_ns_text, tok_ns_to_s_map) = _strip_spaces(tok_text)\n\nif len(orig_ns_text) != len(tok_ns_text):\n  if FLAGS.verbose_logging:\n    tf.logging.info(\"Length not equal after stripping spaces: '%s' vs '%s'\",\n                    orig_ns_text, tok_ns_text)\n  return orig_text\n\n# We then project the characters in `pred_text` back to `orig_text` using\n# the character-to-character alignment.\ntok_s_to_ns_map = {}\nfor (i, tok_index) in six.iteritems(tok_ns_to_s_map):\n  tok_s_to_ns_map[tok_index] = i\n\norig_start_position = None\nif start_position in tok_s_to_ns_map:\n  ns_start_position = tok_s_to_ns_map[start_position]\n  if ns_start_position in orig_ns_to_s_map:\n    orig_start_position = orig_ns_to_s_map[ns_start_position]\n\nif orig_start_position is None:\n  if FLAGS.verbose_logging:\n    tf.logging.info(\"Couldn't map start position\")\n  return orig_text\n\norig_end_position = None\nif end_position in tok_s_to_ns_map:\n  ns_end_position = tok_s_to_ns_map[end_position]\n  if ns_end_position in orig_ns_to_s_map:\n    orig_end_position = orig_ns_to_s_map[ns_end_position]\n\nif orig_end_position is None:\n  if FLAGS.verbose_logging:\n    tf.logging.info(\"Couldn't map end position\")\n  return orig_text\n\noutput_text = orig_text[orig_start_position:(orig_end_position + 1)]\nreturn output_text", "path": "bert/run_squad.py", "commit_date": "2018-12-18 00:00:00", "repo_name": "google-research/bert", "stars": 36595, "license": "apache-2.0", "language": "python", "size": 317}
{"docstring": "\"\"\"Validate the input FLAGS or throw an exception.\"\"\"\n", "func_signal": "def validate_flags_or_throw(bert_config):\n", "code": "tokenization.validate_case_matches_checkpoint(FLAGS.do_lower_case,\n                                              FLAGS.init_checkpoint)\n\nif not FLAGS.do_train and not FLAGS.do_predict:\n  raise ValueError(\"At least one of `do_train` or `do_predict` must be True.\")\n\nif FLAGS.do_train:\n  if not FLAGS.train_file:\n    raise ValueError(\n        \"If `do_train` is True, then `train_file` must be specified.\")\nif FLAGS.do_predict:\n  if not FLAGS.predict_file:\n    raise ValueError(\n        \"If `do_predict` is True, then `predict_file` must be specified.\")\n\nif FLAGS.max_seq_length > bert_config.max_position_embeddings:\n  raise ValueError(\n      \"Cannot use sequence length %d because the BERT model \"\n      \"was only trained up to sequence length %d\" %\n      (FLAGS.max_seq_length, bert_config.max_position_embeddings))\n\nif FLAGS.max_seq_length <= FLAGS.max_query_length + 3:\n  raise ValueError(\n      \"The max_seq_length (%d) must be greater than max_query_length \"\n      \"(%d) + 3\" % (FLAGS.max_seq_length, FLAGS.max_query_length))", "path": "bert/run_squad.py", "commit_date": "2018-12-18 00:00:00", "repo_name": "google-research/bert", "stars": 36595, "license": "apache-2.0", "language": "python", "size": 317}
{"docstring": "\"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n\n", "func_signal": "def input_fn_builder(input_file, seq_length, is_training, drop_remainder):\n", "code": "name_to_features = {\n    \"unique_ids\": tf.FixedLenFeature([], tf.int64),\n    \"input_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n    \"input_mask\": tf.FixedLenFeature([seq_length], tf.int64),\n    \"segment_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n}\n\nif is_training:\n  name_to_features[\"start_positions\"] = tf.FixedLenFeature([], tf.int64)\n  name_to_features[\"end_positions\"] = tf.FixedLenFeature([], tf.int64)\n\ndef _decode_record(record, name_to_features):\n  \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n  example = tf.parse_single_example(record, name_to_features)\n\n  # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n  # So cast all int64 to int32.\n  for name in list(example.keys()):\n    t = example[name]\n    if t.dtype == tf.int64:\n      t = tf.to_int32(t)\n    example[name] = t\n\n  return example\n\ndef input_fn(params):\n  \"\"\"The actual input function.\"\"\"\n  batch_size = params[\"batch_size\"]\n\n  # For training, we want a lot of parallel reading and shuffling.\n  # For eval, we want no shuffling and parallel reading doesn't matter.\n  d = tf.data.TFRecordDataset(input_file)\n  if is_training:\n    d = d.repeat()\n    d = d.shuffle(buffer_size=100)\n\n  d = d.apply(\n      tf.contrib.data.map_and_batch(\n          lambda record: _decode_record(record, name_to_features),\n          batch_size=batch_size,\n          drop_remainder=drop_remainder))\n\n  return d\n\nreturn input_fn", "path": "bert/run_squad.py", "commit_date": "2018-12-18 00:00:00", "repo_name": "google-research/bert", "stars": 36595, "license": "apache-2.0", "language": "python", "size": 317}
{"docstring": "\"\"\"Serializes this instance to a Python dictionary.\"\"\"\n", "func_signal": "def to_dict(self):\n", "code": "output = copy.deepcopy(self.__dict__)\nreturn output", "path": "bert/modeling.py", "commit_date": "2019-02-07 00:00:00", "repo_name": "google-research/bert", "stars": 36595, "license": "apache-2.0", "language": "python", "size": 317}
{"docstring": "\"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\n", "func_signal": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng):\n", "code": "while True:\n  total_length = len(tokens_a) + len(tokens_b)\n  if total_length <= max_num_tokens:\n    break\n\n  trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n  assert len(trunc_tokens) >= 1\n\n  # We want to sometimes truncate from the front and sometimes from the\n  # back to add more randomness and avoid biases.\n  if rng.random() < 0.5:\n    del trunc_tokens[0]\n  else:\n    trunc_tokens.pop()", "path": "bert/create_pretraining_data.py", "commit_date": "2019-05-31 00:00:00", "repo_name": "google-research/bert", "stars": 36595, "license": "apache-2.0", "language": "python", "size": 317}
{"docstring": "\"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"\n", "func_signal": "def from_dict(cls, json_object):\n", "code": "config = BertConfig(vocab_size=None)\nfor (key, value) in six.iteritems(json_object):\n  config.__dict__[key] = value\nreturn config", "path": "bert/modeling.py", "commit_date": "2019-02-07 00:00:00", "repo_name": "google-research/bert", "stars": 36595, "license": "apache-2.0", "language": "python", "size": 317}
{"docstring": "\"\"\"Write a InputFeature to the TFRecordWriter as a tf.train.Example.\"\"\"\n", "func_signal": "def process_feature(self, feature):\n", "code": "self.num_features += 1\n\ndef create_int_feature(values):\n  feature = tf.train.Feature(\n      int64_list=tf.train.Int64List(value=list(values)))\n  return feature\n\nfeatures = collections.OrderedDict()\nfeatures[\"unique_ids\"] = create_int_feature([feature.unique_id])\nfeatures[\"input_ids\"] = create_int_feature(feature.input_ids)\nfeatures[\"input_mask\"] = create_int_feature(feature.input_mask)\nfeatures[\"segment_ids\"] = create_int_feature(feature.segment_ids)\n\nif self.is_training:\n  features[\"start_positions\"] = create_int_feature([feature.start_position])\n  features[\"end_positions\"] = create_int_feature([feature.end_position])\n  impossible = 0\n  if feature.is_impossible:\n    impossible = 1\n  features[\"is_impossible\"] = create_int_feature([impossible])\n\ntf_example = tf.train.Example(features=tf.train.Features(feature=features))\nself._writer.write(tf_example.SerializeToString())", "path": "bert/run_squad.py", "commit_date": "2018-12-18 00:00:00", "repo_name": "google-research/bert", "stars": 36595, "license": "apache-2.0", "language": "python", "size": 317}
{"docstring": "\"\"\"Returns a list of the shape of tensor, preferring static dimensions.\n\nArgs:\n  tensor: A tf.Tensor object to find the shape of.\n  expected_rank: (optional) int. The expected rank of `tensor`. If this is\n    specified and the `tensor` has a different rank, and exception will be\n    thrown.\n  name: Optional name of the tensor for the error message.\n\nReturns:\n  A list of dimensions of the shape of tensor. All static dimensions will\n  be returned as python integers, and dynamic dimensions will be returned\n  as tf.Tensor scalars.\n\"\"\"\n", "func_signal": "def get_shape_list(tensor, expected_rank=None, name=None):\n", "code": "if name is None:\n  name = tensor.name\n\nif expected_rank is not None:\n  assert_rank(tensor, expected_rank, name)\n\nshape = tensor.shape.as_list()\n\nnon_static_indexes = []\nfor (index, dim) in enumerate(shape):\n  if dim is None:\n    non_static_indexes.append(index)\n\nif not non_static_indexes:\n  return shape\n\ndyn_shape = tf.shape(tensor)\nfor index in non_static_indexes:\n  shape[index] = dyn_shape[index]\nreturn shape", "path": "bert/modeling.py", "commit_date": "2019-02-07 00:00:00", "repo_name": "google-research/bert", "stars": 36595, "license": "apache-2.0", "language": "python", "size": 317}
{"docstring": "\"\"\"Get the n-best logits from a list.\"\"\"\n", "func_signal": "def _get_best_indexes(logits, n_best_size):\n", "code": "index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n\nbest_indexes = []\nfor i in range(len(index_and_score)):\n  if i >= n_best_size:\n    break\n  best_indexes.append(index_and_score[i][0])\nreturn best_indexes", "path": "bert/run_squad.py", "commit_date": "2018-12-18 00:00:00", "repo_name": "google-research/bert", "stars": 36595, "license": "apache-2.0", "language": "python", "size": 317}
{"docstring": "\"\"\"Run layer normalization on the last dimension of the tensor.\"\"\"\n", "func_signal": "def layer_norm(input_tensor, name=None):\n", "code": "return tf.contrib.layers.layer_norm(\n    inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=name)", "path": "bert/modeling.py", "commit_date": "2019-02-07 00:00:00", "repo_name": "google-research/bert", "stars": 36595, "license": "apache-2.0", "language": "python", "size": 317}
{"docstring": "\"\"\"Maps a string to a Python function, e.g., \"relu\" => `tf.nn.relu`.\n\nArgs:\n  activation_string: String name of the activation function.\n\nReturns:\n  A Python function corresponding to the activation function. If\n  `activation_string` is None, empty, or \"linear\", this will return None.\n  If `activation_string` is not a string, it will return `activation_string`.\n\nRaises:\n  ValueError: The `activation_string` does not correspond to a known\n    activation.\n\"\"\"\n\n# We assume that anything that\"s not a string is already an activation\n# function, so we just return it.\n", "func_signal": "def get_activation(activation_string):\n", "code": "if not isinstance(activation_string, six.string_types):\n  return activation_string\n\nif not activation_string:\n  return None\n\nact = activation_string.lower()\nif act == \"linear\":\n  return None\nelif act == \"relu\":\n  return tf.nn.relu\nelif act == \"gelu\":\n  return gelu\nelif act == \"tanh\":\n  return tf.tanh\nelse:\n  raise ValueError(\"Unsupported activation: %s\" % act)", "path": "bert/modeling.py", "commit_date": "2019-02-07 00:00:00", "repo_name": "google-research/bert", "stars": 36595, "license": "apache-2.0", "language": "python", "size": 317}
{"docstring": "\"\"\"Perform dropout.\n\nArgs:\n  input_tensor: float Tensor.\n  dropout_prob: Python float. The probability of dropping out a value (NOT of\n    *keeping* a dimension as in `tf.nn.dropout`).\n\nReturns:\n  A version of `input_tensor` with dropout applied.\n\"\"\"\n", "func_signal": "def dropout(input_tensor, dropout_prob):\n", "code": "if dropout_prob is None or dropout_prob == 0.0:\n  return input_tensor\n\noutput = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\nreturn output", "path": "bert/modeling.py", "commit_date": "2019-02-07 00:00:00", "repo_name": "google-research/bert", "stars": 36595, "license": "apache-2.0", "language": "python", "size": 317}
{"docstring": "\"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"\n", "func_signal": "def from_json_file(cls, json_file):\n", "code": "with tf.gfile.GFile(json_file, \"r\") as reader:\n  text = reader.read()\nreturn cls.from_dict(json.loads(text))", "path": "bert/modeling.py", "commit_date": "2019-02-07 00:00:00", "repo_name": "google-research/bert", "stars": 36595, "license": "apache-2.0", "language": "python", "size": 317}
{"docstring": "\"\"\"Check if this is the 'max context' doc span for the token.\"\"\"\n\n# Because of the sliding window approach taken to scoring documents, a single\n# token can appear in multiple documents. E.g.\n#  Doc: the man went to the store and bought a gallon of milk\n#  Span A: the man went to the\n#  Span B: to the store and bought\n#  Span C: and bought a gallon of\n#  ...\n#\n# Now the word 'bought' will have two scores from spans B and C. We only\n# want to consider the score with \"maximum context\", which we define as\n# the *minimum* of its left and right context (the *sum* of left and\n# right context will always be the same, of course).\n#\n# In the example the maximum context for 'bought' would be span C since\n# it has 1 left context and 3 right context, while span B has 4 left context\n# and 0 right context.\n", "func_signal": "def _check_is_max_context(doc_spans, cur_span_index, position):\n", "code": "best_score = None\nbest_span_index = None\nfor (span_index, doc_span) in enumerate(doc_spans):\n  end = doc_span.start + doc_span.length - 1\n  if position < doc_span.start:\n    continue\n  if position > end:\n    continue\n  num_left_context = position - doc_span.start\n  num_right_context = end - position\n  score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n  if best_score is None or score > best_score:\n    best_score = score\n    best_span_index = span_index\n\nreturn cur_span_index == best_span_index", "path": "bert/run_squad.py", "commit_date": "2018-12-18 00:00:00", "repo_name": "google-research/bert", "stars": 36595, "license": "apache-2.0", "language": "python", "size": 317}
{"docstring": "\"\"\"Raises an exception if the tensor rank is not of the expected rank.\n\nArgs:\n  tensor: A tf.Tensor to check the rank of.\n  expected_rank: Python integer or list of integers, expected rank.\n  name: Optional name of the tensor for the error message.\n\nRaises:\n  ValueError: If the expected shape doesn't match the actual shape.\n\"\"\"\n", "func_signal": "def assert_rank(tensor, expected_rank, name=None):\n", "code": "if name is None:\n  name = tensor.name\n\nexpected_rank_dict = {}\nif isinstance(expected_rank, six.integer_types):\n  expected_rank_dict[expected_rank] = True\nelse:\n  for x in expected_rank:\n    expected_rank_dict[x] = True\n\nactual_rank = tensor.shape.ndims\nif actual_rank not in expected_rank_dict:\n  scope_name = tf.get_variable_scope().name\n  raise ValueError(\n      \"For the tensor `%s` in scope `%s`, the actual rank \"\n      \"`%d` (shape = %s) is not equal to the expected rank `%s`\" %\n      (name, scope_name, actual_rank, str(tensor.shape), str(expected_rank)))", "path": "bert/modeling.py", "commit_date": "2019-02-07 00:00:00", "repo_name": "google-research/bert", "stars": 36595, "license": "apache-2.0", "language": "python", "size": 317}
{"docstring": "\"\"\"Compute softmax probability over raw logits.\"\"\"\n", "func_signal": "def _compute_softmax(scores):\n", "code": "if not scores:\n  return []\n\nmax_score = None\nfor score in scores:\n  if max_score is None or score > max_score:\n    max_score = score\n\nexp_scores = []\ntotal_sum = 0.0\nfor score in scores:\n  x = math.exp(score - max_score)\n  exp_scores.append(x)\n  total_sum += x\n\nprobs = []\nfor score in exp_scores:\n  probs.append(score / total_sum)\nreturn probs", "path": "bert/run_squad.py", "commit_date": "2018-12-18 00:00:00", "repo_name": "google-research/bert", "stars": 36595, "license": "apache-2.0", "language": "python", "size": 317}
{"docstring": "\"\"\"Gaussian Error Linear Unit.\n\nThis is a smoother version of the RELU.\nOriginal paper: https://arxiv.org/abs/1606.08415\nArgs:\n  x: float Tensor to perform activation.\n\nReturns:\n  `x` with the GELU activation applied.\n\"\"\"\n", "func_signal": "def gelu(x):\n", "code": "cdf = 0.5 * (1.0 + tf.tanh(\n    (np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\nreturn x * cdf", "path": "bert/modeling.py", "commit_date": "2019-02-07 00:00:00", "repo_name": "google-research/bert", "stars": 36595, "license": "apache-2.0", "language": "python", "size": 317}
{"docstring": "\"\"\"Optional[:class:`datetime.datetime`]: When the user started doing this activity in UTC, if applicable.\"\"\"\n", "func_signal": "def start(self):\n", "code": "try:\n    return datetime.datetime.utcfromtimestamp(self.timestamps['start'] / 1000)\nexcept KeyError:\n    return None", "path": "discord.py/discord/activity.py", "commit_date": "2020-11-28 00:00:00", "repo_name": "Rapptz/discord.py", "stars": 14096, "license": "mit", "language": "python", "size": 19811}
{"docstring": "\"\"\"Sets the decoder gain in dB, from -128 to 128.\"\"\"\n\n", "func_signal": "def set_gain(self, dB):\n", "code": "dB_Q8 = max(-32768, min(32767, round(dB * 256))) # dB * 2^n where n is 8 (Q8)\nreturn self._set_gain(dB_Q8)", "path": "discord.py/discord/opus.py", "commit_date": "2020-12-13 00:00:00", "repo_name": "Rapptz/discord.py", "stars": 14096, "license": "mit", "language": "python", "size": 19811}
{"docstring": "\"\"\"Optional[:class:`str`]: If provided, the twitch name of the user streaming.\n\nThis corresponds to the ``large_image`` key of the :attr:`Streaming.assets`\ndictionary if it starts with ``twitch:``. Typically set by the Discord client.\n\"\"\"\n\n", "func_signal": "def twitch_name(self):\n", "code": "try:\n    name = self.assets['large_image']\nexcept KeyError:\n    return None\nelse:\n    return name[7:] if name[:7] == 'twitch:' else None", "path": "discord.py/discord/activity.py", "commit_date": "2020-11-28 00:00:00", "repo_name": "Rapptz/discord.py", "stars": 14096, "license": "mit", "language": "python", "size": 19811}
{"docstring": "\"\"\"Compute the next delay\n\nReturns the next delay to wait according to the exponential\nbackoff algorithm.  This is a value between 0 and base * 2^exp\nwhere exponent starts off at 1 and is incremented at every\ninvocation of this method up to a maximum of 10.\n\nIf a period of more than base * 2^11 has passed since the last\nretry, the exponent is reset to 1.\n\"\"\"\n", "func_signal": "def delay(self):\n", "code": "invocation = time.monotonic()\ninterval = invocation - self._last_invocation\nself._last_invocation = invocation\n\nif interval > self._reset_time:\n    self._exp = 0\n\nself._exp = min(self._exp + 1, self._max)\nreturn self._randfunc(0, self._base * 2 ** self._exp)", "path": "discord.py/discord/backoff.py", "commit_date": "2020-01-20 00:00:00", "repo_name": "Rapptz/discord.py", "stars": 14096, "license": "mit", "language": "python", "size": 19811}
{"docstring": "\"\"\"Loads the libopus shared library for use with voice.\n\nIf this function is not called then the library uses the function\n:func:`ctypes.util.find_library` and then loads that one if available.\n\nNot loading a library and attempting to use PCM based AudioSources will\nlead to voice not working.\n\nThis function propagates the exceptions thrown.\n\n.. warning::\n\n    The bitness of the library must match the bitness of your python\n    interpreter. If the library is 64-bit then your python interpreter\n    must be 64-bit as well. Usually if there's a mismatch in bitness then\n    the load will throw an exception.\n\n.. note::\n\n    On Windows, this function should not need to be called as the binaries\n    are automatically loaded.\n\n.. note::\n\n    On Windows, the .dll extension is not necessary. However, on Linux\n    the full extension is required to load the library, e.g. ``libopus.so.1``.\n    On Linux however, :func:`ctypes.util.find_library` will usually find the library automatically\n    without you having to call this.\n\nParameters\n----------\nname: :class:`str`\n    The filename of the shared library.\n\"\"\"\n", "func_signal": "def load_opus(name):\n", "code": "global _lib\n_lib = libopus_loader(name)", "path": "discord.py/discord/opus.py", "commit_date": "2020-12-13 00:00:00", "repo_name": "Rapptz/discord.py", "stars": 14096, "license": "mit", "language": "python", "size": 19811}
{"docstring": "\"\"\"Optional[:class:`datetime.datetime`]: When the user started playing this game in UTC, if applicable.\"\"\"\n", "func_signal": "def start(self):\n", "code": "if self._start:\n    return datetime.datetime.utcfromtimestamp(self._start / 1000)\nreturn None", "path": "discord.py/discord/activity.py", "commit_date": "2020-11-28 00:00:00", "repo_name": "Rapptz/discord.py", "stars": 14096, "license": "mit", "language": "python", "size": 19811}
{"docstring": "\"\"\"Optional[:class:`datetime.datetime`]: When the user will stop playing this game in UTC, if applicable.\"\"\"\n", "func_signal": "def end(self):\n", "code": "if self._end:\n    return datetime.datetime.utcfromtimestamp(self._end / 1000)\nreturn None", "path": "discord.py/discord/activity.py", "commit_date": "2020-11-28 00:00:00", "repo_name": "Rapptz/discord.py", "stars": 14096, "license": "mit", "language": "python", "size": 19811}
{"docstring": "# the total count of lines for each index letter, used to distribute\n# the entries into two columns\n", "func_signal": "def write_genindex(self) -> None:\n", "code": "genindex = IndexEntries(self.env).create_index(self, group_entries=False)\nindexcounts = []\nfor _k, entries in genindex:\n    indexcounts.append(sum(1 + len(subitems)\n                           for _, (_, subitems, _) in entries))\n\ngenindexcontext = {\n    'genindexentries': genindex,\n    'genindexcounts': indexcounts,\n    'split_index': self.config.html_split_index,\n}\n\nif self.config.html_split_index:\n    self.handle_page('genindex', genindexcontext,\n                     'genindex-split.html')\n    self.handle_page('genindex-all', genindexcontext,\n                     'genindex.html')\n    for (key, entries), count in zip(genindex, indexcounts):\n        ctx = {'key': key, 'entries': entries, 'count': count,\n               'genindexentries': genindex}\n        self.handle_page('genindex-' + key, ctx,\n                         'genindex-single.html')\nelse:\n    self.handle_page('genindex', genindexcontext, 'genindex.html')", "path": "discord.py/docs/extensions/builder.py", "commit_date": "2020-12-19 00:00:00", "repo_name": "Rapptz/discord.py", "stars": 14096, "license": "mit", "language": "python", "size": 19811}
{"docstring": "\"\"\"Optional[:class:`str`]: Returns a URL pointing to the small image asset of this activity if applicable.\"\"\"\n", "func_signal": "def small_image_url(self):\n", "code": "if self.application_id is None:\n    return None\n\ntry:\n    small_image = self.assets['small_image']\nexcept KeyError:\n    return None\nelse:\n    return Asset.BASE + '/app-assets/{0}/{1}.png'.format(self.application_id, small_image)", "path": "discord.py/discord/activity.py", "commit_date": "2020-11-28 00:00:00", "repo_name": "Rapptz/discord.py", "stars": 14096, "license": "mit", "language": "python", "size": 19811}
{"docstring": "\"\"\"This is necessary because RTD injects their own for some reason.\"\"\"\n", "func_signal": "def add_builders(app):\n", "code": "try:\n    original = app.registry.builders['readthedocs']\nexcept KeyError:\n    app.set_translator('html', DPYHTML5Translator, override=True)\n    app.add_builder(DPYStandaloneHTMLBuilder, override=True)\nelse:\n    injected_mro = tuple(base if base is not StandaloneHTMLBuilder else DPYStandaloneHTMLBuilder\n                         for base in original.mro()[1:])\n    new_builder = type(original.__name__, injected_mro, {'name': 'readthedocs'})\n    app.set_translator('readthedocs', DPYHTML5Translator, override=True)\n    app.add_builder(new_builder, override=True)", "path": "discord.py/docs/extensions/builder.py", "commit_date": "2020-12-19 00:00:00", "repo_name": "Rapptz/discord.py", "stars": 14096, "license": "mit", "language": "python", "size": 19811}
{"docstring": "\"\"\"Optional[:class:`datetime.datetime`]: When the user started doing this activity in UTC.\n\n.. versionadded:: 1.3\n\"\"\"\n", "func_signal": "def created_at(self):\n", "code": "if self._created_at is not None:\n    return datetime.datetime.utcfromtimestamp(self._created_at / 1000)", "path": "discord.py/discord/activity.py", "commit_date": "2020-11-28 00:00:00", "repo_name": "Rapptz/discord.py", "stars": 14096, "license": "mit", "language": "python", "size": 19811}
{"docstring": "\"\"\"If you're curious on the HTML this is meant to generate:\n\n<div class=\"py-attribute-table\">\n    <div class=\"py-attribute-table-column\">\n        <span>_('Attributes')</span>\n        <ul>\n            <li>\n                <a href=\"...\">\n            </li>\n        </ul>\n    </div>\n    <div class=\"py-attribute-table-column\">\n        <span>_('Methods')</span>\n        <ul>\n            <li>\n                <a href=\"...\"></a>\n                <span class=\"py-attribute-badge\" title=\"decorator\">D</span>\n            </li>\n        </ul>\n    </div>\n</div>\n\nHowever, since this requires the tree to be complete\nand parsed, it'll need to be done at a different stage and then\nreplaced.\n\"\"\"\n", "func_signal": "def run(self):\n", "code": "content = self.arguments[0].strip()\nnode = attributetableplaceholder('')\nmodulename, name = self.parse_name(content)\nnode['python-module'] = modulename\nnode['python-class'] = name\nnode['python-full-name'] = '%s.%s' % (modulename, name)\nreturn [node]", "path": "discord.py/docs/extensions/attributetable.py", "commit_date": "2020-12-19 00:00:00", "repo_name": "Rapptz/discord.py", "stars": 14096, "license": "mit", "language": "python", "size": 19811}
{"docstring": "\"\"\"Optional[:class:`datetime.datetime`]: When the user will stop doing this activity in UTC, if applicable.\"\"\"\n", "func_signal": "def end(self):\n", "code": "try:\n    return datetime.datetime.utcfromtimestamp(self.timestamps['end'] / 1000)\nexcept KeyError:\n    return None", "path": "discord.py/discord/activity.py", "commit_date": "2020-11-28 00:00:00", "repo_name": "Rapptz/discord.py", "stars": 14096, "license": "mit", "language": "python", "size": 19811}
{"docstring": "# create the library...\n", "func_signal": "def libopus_loader(name):\n", "code": "lib = ctypes.cdll.LoadLibrary(name)\n\n# register the functions...\nfor item in exported_functions:\n    func = getattr(lib, item[0])\n\n    try:\n        if item[1]:\n            func.argtypes = item[1]\n\n        func.restype = item[2]\n    except KeyError:\n        pass\n\n    try:\n        if item[3]:\n            func.errcheck = item[3]\n    except KeyError:\n        log.exception(\"Error assigning check function to %s\", func)\n\nreturn lib", "path": "discord.py/discord/opus.py", "commit_date": "2020-12-13 00:00:00", "repo_name": "Rapptz/discord.py", "stars": 14096, "license": "mit", "language": "python", "size": 19811}
{"docstring": "\"\"\"Gets the duration (in samples) of the last packet successfully decoded or concealed.\"\"\"\n\n", "func_signal": "def _get_last_packet_duration(self):\n", "code": "ret = ctypes.c_int32()\n_lib.opus_decoder_ctl(self._state, CTL_LAST_PACKET_DURATION, ctypes.byref(ret))\nreturn ret.value", "path": "discord.py/discord/opus.py", "commit_date": "2020-12-13 00:00:00", "repo_name": "Rapptz/discord.py", "stars": 14096, "license": "mit", "language": "python", "size": 19811}
{"docstring": "# Given an environment, load up a lookup table of\n# full-class-name: objects\n", "func_signal": "def build_lookup_table(env):\n", "code": "result = {}\ndomain = env.domains['py']\n\nignored = {\n    'data', 'exception', 'module', 'class',\n}\n\nfor (fullname, _, objtype, docname, _, _) in domain.get_objects():\n    if objtype in ignored:\n        continue\n\n    classname, _, child = fullname.rpartition('.')\n    try:\n        result[classname].append(child)\n    except KeyError:\n        result[classname] = [child]\n\nreturn result", "path": "discord.py/docs/extensions/attributetable.py", "commit_date": "2020-12-19 00:00:00", "repo_name": "Rapptz/discord.py", "stars": 14096, "license": "mit", "language": "python", "size": 19811}
{"docstring": "\"\"\"Optional[:class:`datetime.datetime`]: When the user started listening in UTC.\n\n.. versionadded:: 1.3\n\"\"\"\n", "func_signal": "def created_at(self):\n", "code": "if self._created_at is not None:\n    return datetime.datetime.utcfromtimestamp(self._created_at / 1000)", "path": "discord.py/discord/activity.py", "commit_date": "2020-11-28 00:00:00", "repo_name": "Rapptz/discord.py", "stars": 14096, "license": "mit", "language": "python", "size": 19811}
{"docstring": "\"\"\":class:`str`: The album cover image URL from Spotify's CDN.\"\"\"\n", "func_signal": "def album_cover_url(self):\n", "code": "large_image = self._assets.get('large_image', '')\nif large_image[:8] != 'spotify:':\n    return ''\nalbum_image_id = large_image[8:]\nreturn 'https://i.scdn.co/image/' + album_image_id", "path": "discord.py/discord/activity.py", "commit_date": "2020-11-28 00:00:00", "repo_name": "Rapptz/discord.py", "stars": 14096, "license": "mit", "language": "python", "size": 19811}
{"docstring": "\"\"\"Function to check if opus lib is successfully loaded either\nvia the :func:`ctypes.util.find_library` call of :func:`load_opus`.\n\nThis must return ``True`` for voice to work.\n\nReturns\n-------\n:class:`bool`\n    Indicates if the opus library has been loaded.\n\"\"\"\n", "func_signal": "def is_loaded():\n", "code": "global _lib\nreturn _lib is not None", "path": "discord.py/discord/opus.py", "commit_date": "2020-12-13 00:00:00", "repo_name": "Rapptz/discord.py", "stars": 14096, "license": "mit", "language": "python", "size": 19811}
{"docstring": "\"\"\"Optional[:class:`str`]: Returns a URL pointing to the large image asset of this activity if applicable.\"\"\"\n", "func_signal": "def large_image_url(self):\n", "code": "if self.application_id is None:\n    return None\n\ntry:\n    large_image = self.assets['large_image']\nexcept KeyError:\n    return None\nelse:\n    return Asset.BASE + '/app-assets/{0}/{1}.png'.format(self.application_id, large_image)", "path": "discord.py/discord/activity.py", "commit_date": "2020-11-28 00:00:00", "repo_name": "Rapptz/discord.py", "stars": 14096, "license": "mit", "language": "python", "size": 19811}
{"docstring": "\"\"\"Build the youtube-dl command line string.\"\"\"\n\n", "func_signal": "def build_command(options_list, url):\n", "code": "def escape(option):\n    \"\"\"Wrap option with double quotes if it contains special symbols.\"\"\"\n    special_symbols = [\" \", \"(\", \")\"]\n\n    for symbol in special_symbols:\n        if symbol in option:\n            return \"\\\"{}\\\"\".format(option)\n\n    return option\n\n# If option has special symbols wrap it with double quotes\n# Probably not the best solution since if the option already contains\n# double quotes it will be a mess, see issue #173\noptions = [escape(option) for option in options_list]\n\n# Always wrap the url with double quotes\nurl = \"\\\"{}\\\"\".format(url)\n\nreturn \" \".join([YOUTUBEDL_BIN] + options + [url])", "path": "youtube-dl-gui/youtube_dl_gui/utils.py", "commit_date": "2017-12-09 00:00:00", "repo_name": "MrS0m30n3/youtube-dl-gui", "stars": 9121, "license": "unlicense", "language": "python", "size": 2329}
{"docstring": "\"\"\"Update youtube-dl binary to the latest version. \"\"\"\n", "func_signal": "def _update_youtubedl(self):\n", "code": "if self.download_manager is not None and self.download_manager.is_alive():\n    self._create_popup(self.DOWNLOAD_ACTIVE,\n                       self.WARNING_LABEL,\n                       wx.OK | wx.ICON_EXCLAMATION)\nelif self.update_thread is not None and self.update_thread.is_alive():\n    self._create_popup(self.UPDATE_ACTIVE,\n                       self.INFO_LABEL,\n                       wx.OK | wx.ICON_INFORMATION)\nelse:\n    self.update_thread = UpdateThread(self.opt_manager.options['youtubedl_path'])", "path": "youtube-dl-gui/youtube_dl_gui/mainframe.py", "commit_date": "2018-01-14 00:00:00", "repo_name": "MrS0m30n3/youtube-dl-gui", "stars": 9121, "license": "unlicense", "language": "python", "size": 2329}
{"docstring": "\"\"\"Resets GUI widgets after update or download process. \"\"\"\n", "func_signal": "def _reset_widgets(self):\n", "code": "self._buttons[\"start\"].SetLabel(_(\"Start\"))\nself._buttons[\"start\"].SetToolTip(wx.ToolTip(_(\"Start\")))\nself._buttons[\"start\"].SetBitmap(self._bitmaps[\"start\"], wx.TOP)", "path": "youtube-dl-gui/youtube_dl_gui/mainframe.py", "commit_date": "2018-01-14 00:00:00", "repo_name": "MrS0m30n3/youtube-dl-gui", "stars": 9121, "license": "unlicense", "language": "python", "size": 2329}
{"docstring": "\"\"\"Display download stats in the status bar. \"\"\"\n", "func_signal": "def _print_stats(self):\n", "code": "suc_downloads = self.download_manager.successful\ndtime = get_time(self.download_manager.time_it_took)\n\nmsg = self.SUCC_REPORT_MSG.format(suc_downloads,\n                                  dtime['days'],\n                                  dtime['hours'],\n                                  dtime['minutes'],\n                                  dtime['seconds'])\n\nself._status_bar_write(msg)", "path": "youtube-dl-gui/youtube_dl_gui/mainframe.py", "commit_date": "2018-01-14 00:00:00", "repo_name": "MrS0m30n3/youtube-dl-gui", "stars": 9121, "license": "unlicense", "language": "python", "size": 2329}
{"docstring": "\"\"\"Event handler of the self._update_btn widget.\n\nThis method is used when the update button is pressed to start\nthe update process.\n\nNote:\n    Currently there is not way to stop the update process.\n\n\"\"\"\n", "func_signal": "def _on_update(self, event):\n", "code": "if self.opt_manager.options[\"disable_update\"]:\n    self._create_popup(_(\"Updates are disabled for your system. Please use the system's package manager to update youtube-dl.\"),\n                       self.INFO_LABEL,\n                       wx.OK | wx.ICON_INFORMATION)\nelse:\n    self._update_youtubedl()", "path": "youtube-dl-gui/youtube_dl_gui/mainframe.py", "commit_date": "2018-01-14 00:00:00", "repo_name": "MrS0m30n3/youtube-dl-gui", "stars": 9121, "license": "unlicense", "language": "python", "size": 2329}
{"docstring": "\"\"\"Allow script calls from the 'devscripts' dir and the package dir.\"\"\"\n", "func_signal": "def manage_directory():\n", "code": "if os.path.basename(os.getcwd()) == \"devscripts\":\n    os.chdir(\"..\")", "path": "youtube-dl-gui/devscripts/new-locale.py", "commit_date": "2017-04-17 00:00:00", "repo_name": "MrS0m30n3/youtube-dl-gui", "stars": 9121, "license": "unlicense", "language": "python", "size": 2329}
{"docstring": "\"\"\"Sets the layout of the main window. \"\"\"\n", "func_signal": "def _set_layout(self):\n", "code": "main_sizer = wx.BoxSizer()\npanel_sizer = wx.BoxSizer(wx.VERTICAL)\n\ntop_sizer = wx.BoxSizer(wx.HORIZONTAL)\ntop_sizer.Add(self._url_text, 0, wx.ALIGN_BOTTOM | wx.BOTTOM, 5)\ntop_sizer.AddSpacer((-1, -1), 1)\ntop_sizer.Add(self._settings_button)\npanel_sizer.Add(top_sizer, 0, wx.EXPAND)\n\npanel_sizer.Add(self._url_list, 1, wx.EXPAND)\n\nmid_sizer = wx.BoxSizer(wx.HORIZONTAL)\nmid_sizer.Add(self._folder_icon)\nmid_sizer.AddSpacer((3, -1))\nmid_sizer.Add(self._path_combobox, 2, wx.ALIGN_CENTER_VERTICAL)\nmid_sizer.AddSpacer((5, -1))\nmid_sizer.Add(self._buttons[\"savepath\"], flag=wx.ALIGN_CENTER_VERTICAL)\nmid_sizer.AddSpacer((10, -1), 1)\nmid_sizer.Add(self._videoformat_combobox, 1, wx.ALIGN_CENTER_VERTICAL)\nmid_sizer.AddSpacer((5, -1))\nmid_sizer.Add(self._buttons[\"add\"], flag=wx.ALIGN_CENTER_VERTICAL)\npanel_sizer.Add(mid_sizer, 0, wx.EXPAND | wx.ALL, 10)\n\npanel_sizer.Add(self._download_text, 0, wx.BOTTOM, 5)\npanel_sizer.Add(self._status_list, 2, wx.EXPAND)\n\nbottom_sizer = wx.BoxSizer(wx.HORIZONTAL)\nbottom_sizer.Add(self._buttons[\"delete\"])\nbottom_sizer.AddSpacer((5, -1))\nbottom_sizer.Add(self._buttons[\"play\"])\nbottom_sizer.AddSpacer((5, -1))\nbottom_sizer.Add(self._buttons[\"up\"])\nbottom_sizer.AddSpacer((5, -1))\nbottom_sizer.Add(self._buttons[\"down\"])\nbottom_sizer.AddSpacer((5, -1))\nbottom_sizer.Add(self._buttons[\"reload\"])\nbottom_sizer.AddSpacer((5, -1))\nbottom_sizer.Add(self._buttons[\"pause\"])\nbottom_sizer.AddSpacer((10, -1), 1)\nbottom_sizer.Add(self._buttons[\"start\"])\npanel_sizer.Add(bottom_sizer, 0, wx.EXPAND | wx.TOP, 5)\n\nmain_sizer.Add(panel_sizer, 1, wx.ALL | wx.EXPAND, 10)\nself._panel.SetSizer(main_sizer)\n\nself._panel.Layout()", "path": "youtube-dl-gui/youtube_dl_gui/mainframe.py", "commit_date": "2018-01-14 00:00:00", "repo_name": "MrS0m30n3/youtube-dl-gui", "stars": 9121, "license": "unlicense", "language": "python", "size": 2329}
{"docstring": "\"\"\"Decorator to convert string inputs & outputs.\n\nCovert string inputs & outputs between 'str' and 'unicode' at the\napplication bounds using the preferred system encoding. It will convert\nall the string params (args, kwargs) to 'str' type and all the\nreturned strings values back to 'unicode'.\n\n\"\"\"\n", "func_signal": "def convert_on_bounds(func):\n", "code": "def wrapper(*args, **kwargs):\n    returned_value = func(*convert_item(args), **convert_item(kwargs))\n\n    return convert_item(returned_value, True)\n\nreturn wrapper", "path": "youtube-dl-gui/youtube_dl_gui/utils.py", "commit_date": "2017-12-09 00:00:00", "repo_name": "MrS0m30n3/youtube-dl-gui", "stars": 9121, "license": "unlicense", "language": "python", "size": 2329}
{"docstring": "\"\"\"Run tasks after download process has been completed.\n\nNote:\n    Here you can add any tasks you want to run after the\n    download process has been completed.\n\n\"\"\"\n", "func_signal": "def _after_download(self):\n", "code": "if self.opt_manager.options['shutdown']:\n    dlg = ShutdownDialog(self, 60, _(\"Shutting down in {0} second(s)\"), _(\"Shutdown\"))\n    result = dlg.ShowModal() == wx.ID_OK\n    dlg.Destroy()\n\n    if result:\n        self.opt_manager.save_to_file()\n        success = shutdown_sys(self.opt_manager.options['sudo_password'])\n\n        if success:\n            self._status_bar_write(self.SHUTDOWN_MSG)\n        else:\n            self._status_bar_write(self.SHUTDOWN_ERR)\nelse:\n    if self.opt_manager.options[\"show_completion_popup\"]:\n        self._create_popup(self.DL_COMPLETED_MSG, self.INFO_LABEL, wx.OK | wx.ICON_INFORMATION)", "path": "youtube-dl-gui/youtube_dl_gui/mainframe.py", "commit_date": "2018-01-14 00:00:00", "repo_name": "MrS0m30n3/youtube-dl-gui", "stars": 9121, "license": "unlicense", "language": "python", "size": 2329}
{"docstring": "\"\"\"Format bytes to youtube-dl size output strings.\"\"\"\n", "func_signal": "def format_bytes(bytes):\n", "code": "if bytes == 0.0:\n    exponent = 0\nelse:\n    exponent = int(math.log(bytes, KILO_SIZE))\n\nsuffix = FILESIZE_METRICS[exponent]\noutput_value = bytes / (KILO_SIZE ** exponent)\n\nreturn \"%.2f%s\" % (output_value, suffix)", "path": "youtube-dl-gui/youtube_dl_gui/utils.py", "commit_date": "2017-12-09 00:00:00", "repo_name": "MrS0m30n3/youtube-dl-gui", "stars": 9121, "license": "unlicense", "language": "python", "size": 2329}
{"docstring": "\"\"\"Event handler of the self._url_list widget.\n\nThis method is triggered when the users pastes text into\nthe URLs list either by using CTRL+V or by using the middle\nclick of the mouse.\n\n\"\"\"\n", "func_signal": "def _on_urllist_edit(self, event):\n", "code": "if event.GetEventType() == wx.EVT_TEXT_PASTE.typeId:\n    self._paste_from_clipboard()\nelse:\n    wx.TheClipboard.UsePrimarySelection(True)\n    self._paste_from_clipboard()\n    wx.TheClipboard.UsePrimarySelection(False)", "path": "youtube-dl-gui/youtube_dl_gui/mainframe.py", "commit_date": "2018-01-14 00:00:00", "repo_name": "MrS0m30n3/youtube-dl-gui", "stars": 9121, "license": "unlicense", "language": "python", "size": 2329}
{"docstring": "\"\"\"Open file in file_path using the default OS application.\n\nReturns:\n    True on success else False.\n\n\"\"\"\n", "func_signal": "def open_file(file_path):\n", "code": "file_path = remove_shortcuts(file_path)\n\nif not os_path_exists(file_path):\n    return False\n\nif os.name == \"nt\":\n    os_startfile(file_path)\nelse:\n    subprocess.call((\"xdg-open\", file_path))\n\nreturn True", "path": "youtube-dl-gui/youtube_dl_gui/utils.py", "commit_date": "2017-12-09 00:00:00", "repo_name": "MrS0m30n3/youtube-dl-gui", "stars": 9121, "license": "unlicense", "language": "python", "size": 2329}
{"docstring": "\"\"\"Event handler of the self._options_btn widget.\n\nThis method is used when the options button is pressed to show\nthe options window.\n\n\"\"\"\n", "func_signal": "def _on_options(self, event):\n", "code": "self._options_frame.load_all_options()\nself._options_frame.Show()", "path": "youtube-dl-gui/youtube_dl_gui/mainframe.py", "commit_date": "2018-01-14 00:00:00", "repo_name": "MrS0m30n3/youtube-dl-gui", "stars": 9121, "license": "unlicense", "language": "python", "size": 2329}
{"docstring": "\"\"\"Search for youtube-dlg locale file.\n\nReturns:\n    The path to youtube-dlg locale file if exists else None.\n\nNote:\n    Paths that get_locale_file() func searches.\n\n    __main__ dir, library dir\n\n\"\"\"\n", "func_signal": "def get_locale_file():\n", "code": "DIR_NAME = \"locale\"\n\nSEARCH_DIRS = [\n    os.path.join(absolute_path(sys.argv[0]), DIR_NAME),\n    os.path.join(os_path_dirname(__file__), DIR_NAME),\n]\n\nfor directory in SEARCH_DIRS:\n    if os_path_isdir(directory):\n        return directory\n\nreturn None", "path": "youtube-dl-gui/youtube_dl_gui/utils.py", "commit_date": "2017-12-09 00:00:00", "repo_name": "MrS0m30n3/youtube-dl-gui", "stars": 9121, "license": "unlicense", "language": "python", "size": 2329}
{"docstring": "\"\"\"Return absolute path to the pixmaps icons folder.\n\nNote:\n    Paths we search: __main__ dir, library dir\n\n\"\"\"\n", "func_signal": "def get_pixmaps_dir():\n", "code": "search_dirs = [\n    os.path.join(absolute_path(sys.argv[0]), \"data\"),\n    os.path.join(os_path_dirname(__file__), \"data\")\n]\n\nfor directory in search_dirs:\n    pixmaps_dir = os.path.join(directory, \"pixmaps\")\n\n    if os_path_exists(pixmaps_dir):\n        return pixmaps_dir\n\nreturn None", "path": "youtube-dl-gui/youtube_dl_gui/utils.py", "commit_date": "2017-12-09 00:00:00", "repo_name": "MrS0m30n3/youtube-dl-gui", "stars": 9121, "license": "unlicense", "language": "python", "size": 2329}
{"docstring": "\"\"\"downloadmanager.Worker thread handler.\n\nHandles messages from the Worker thread.\n\nArgs:\n    See downloadmanager.Worker _talk_to_gui() method.\n\n\"\"\"\n", "func_signal": "def _download_worker_handler(self, msg):\n", "code": "signal, data = msg.data\n\ndownload_item = self._download_list.get_item(data[\"index\"])\ndownload_item.update_stats(data)\nrow = self._download_list.index(data[\"index\"])\n\nself._status_list._update_from_item(row, download_item)", "path": "youtube-dl-gui/youtube_dl_gui/mainframe.py", "commit_date": "2018-01-14 00:00:00", "repo_name": "MrS0m30n3/youtube-dl-gui", "stars": 9121, "license": "unlicense", "language": "python", "size": 2329}
{"docstring": "\"\"\"Shuts down the system.\nReturns True if no errors occur else False.\n\nArgs:\n    password (string): SUDO password for linux.\n\nNote:\n    On Linux you need to provide sudo password if you don't\n    have elevated privileges.\n\n\"\"\"\n", "func_signal": "def shutdown_sys(password=None):\n", "code": "_stderr = subprocess.PIPE\n_stdin = None\ninfo = None\nencoding = get_encoding()\n\nif os.name == 'nt':\n    cmd = ['shutdown', '/s', '/t', '1']\n\n    # Hide subprocess window\n    info = subprocess.STARTUPINFO()\n    info.dwFlags |= subprocess.STARTF_USESHOWWINDOW\nelse:\n    if password:\n        _stdin = subprocess.PIPE\n        password = ('%s\\n' % password).encode(encoding)\n        cmd = ['sudo', '-S', '/sbin/shutdown', '-h', 'now']\n    else:\n        cmd = ['/sbin/shutdown', '-h', 'now']\n\ncmd = [item.encode(encoding, 'ignore') for item in cmd]\n\nshutdown_proc = subprocess.Popen(cmd,\n                                 stderr=_stderr,\n                                 stdin=_stdin,\n                                 startupinfo=info)\n\noutput = shutdown_proc.communicate(password)[1]\n\nreturn not output or output == \"Password:\"", "path": "youtube-dl-gui/youtube_dl_gui/utils.py", "commit_date": "2017-12-09 00:00:00", "repo_name": "MrS0m30n3/youtube-dl-gui", "stars": 9121, "license": "unlicense", "language": "python", "size": 2329}
{"docstring": "\"\"\"Create path if not exist. \"\"\"\n", "func_signal": "def check_path(path):\n", "code": "if not os_path_exists(path):\n    os_makedirs(path)", "path": "youtube-dl-gui/youtube_dl_gui/utils.py", "commit_date": "2017-12-09 00:00:00", "repo_name": "MrS0m30n3/youtube-dl-gui", "stars": 9121, "license": "unlicense", "language": "python", "size": 2329}
{"docstring": "\"\"\"Convert given seconds to days, hours, minutes and seconds.\n\nArgs:\n    seconds (float): Time in seconds.\n\nReturns:\n    Dictionary that contains the corresponding days, hours, minutes\n    and seconds of the given seconds.\n\n\"\"\"\n", "func_signal": "def get_time(seconds):\n", "code": "dtime = dict(seconds=0, minutes=0, hours=0, days=0)\n\ndtime['days'] = int(seconds / 86400)\ndtime['hours'] = int(seconds % 86400 / 3600)\ndtime['minutes'] = int(seconds % 86400 % 3600 / 60)\ndtime['seconds'] = int(seconds % 86400 % 3600 % 60)\n\nreturn dtime", "path": "youtube-dl-gui/youtube_dl_gui/utils.py", "commit_date": "2017-12-09 00:00:00", "repo_name": "MrS0m30n3/youtube-dl-gui", "stars": 9121, "license": "unlicense", "language": "python", "size": 2329}
{"docstring": "\"\"\"Event handler for the wx.EVT_CLOSE event.\n\nThis method is used when the user tries to close the program\nto save the options and make sure that the download & update\nprocesses are not running.\n\n\"\"\"\n", "func_signal": "def _on_close(self, event):\n", "code": "if self.opt_manager.options[\"confirm_exit\"]:\n    dlg = wx.MessageDialog(self, _(\"Are you sure you want to exit?\"), _(\"Exit\"), wx.YES_NO | wx.ICON_QUESTION)\n\n    result = dlg.ShowModal() == wx.ID_YES\n    dlg.Destroy()\nelse:\n    result = True\n\nif result:\n    self.close()", "path": "youtube-dl-gui/youtube_dl_gui/mainframe.py", "commit_date": "2018-01-14 00:00:00", "repo_name": "MrS0m30n3/youtube-dl-gui", "stars": 9121, "license": "unlicense", "language": "python", "size": 2329}
{"docstring": "\"\"\"Initialize weights of the head.\"\"\"\n", "func_signal": "def init_weights(self):\n", "code": "for m in self.cls_convs[0]:\n    normal_init(m.conv, std=0.01)\nfor m in self.reg_convs[0]:\n    normal_init(m.conv, std=0.01)\nbias_cls = bias_init_with_prob(0.01)\nnormal_init(self.retina_cls, std=0.01, bias=bias_cls)\nnormal_init(self.retina_reg, std=0.01)", "path": "mmdetection/mmdet/models/dense_heads/retina_sepbn_head.py", "commit_date": "2020-06-26 00:00:00", "repo_name": "open-mmlab/mmdetection", "stars": 27227, "license": "apache-2.0", "language": "python", "size": 64742}
{"docstring": "\"\"\"Initialize the weights of module.\"\"\"\n", "func_signal": "def init_weights(self):\n", "code": "for m in self.modules():\n    if isinstance(m, nn.Conv2d):\n        caffe2_xavier_init(m)", "path": "mmdetection/mmdet/models/necks/hrfpn.py", "commit_date": "2020-07-20 00:00:00", "repo_name": "open-mmlab/mmdetection", "stars": 27227, "license": "apache-2.0", "language": "python", "size": 64742}
{"docstring": "\"\"\"Get top k positions from heatmap.\n\nArgs:\n    scores (Tensor): Target heatmap with shape\n        [batch, num_classes, height, width].\n    k (int): Target number. Default: 20.\n\nReturns:\n    tuple[torch.Tensor]: Scores, indexes, categories and coords of\n        topk keypoint. Containing following Tensors:\n\n    - topk_scores (Tensor): Max scores of each topk keypoint.\n    - topk_inds (Tensor): Indexes of each topk keypoint.\n    - topk_clses (Tensor): Categories of each topk keypoint.\n    - topk_ys (Tensor): Y-coord of each topk keypoint.\n    - topk_xs (Tensor): X-coord of each topk keypoint.\n\"\"\"\n", "func_signal": "def _topk(self, scores, k=20):\n", "code": "batch, _, height, width = scores.size()\ntopk_scores, topk_inds = torch.topk(scores.view(batch, -1), k)\ntopk_clses = topk_inds // (height * width)\ntopk_inds = topk_inds % (height * width)\ntopk_ys = topk_inds // width\ntopk_xs = (topk_inds % width).int().float()\nreturn topk_scores, topk_inds, topk_clses, topk_ys, topk_xs", "path": "mmdetection/mmdet/models/dense_heads/corner_head.py", "commit_date": "2020-09-25 00:00:00", "repo_name": "open-mmlab/mmdetection", "stars": 27227, "license": "apache-2.0", "language": "python", "size": 64742}
{"docstring": "\"\"\"Load annotation from WIDERFace XML style annotation file.\n\nArgs:\n    ann_file (str): Path of XML file.\n\nReturns:\n    list[dict]: Annotation info from XML file.\n\"\"\"\n\n", "func_signal": "def load_annotations(self, ann_file):\n", "code": "data_infos = []\nimg_ids = mmcv.list_from_file(ann_file)\nfor img_id in img_ids:\n    filename = f'{img_id}.jpg'\n    xml_path = osp.join(self.img_prefix, 'Annotations',\n                        f'{img_id}.xml')\n    tree = ET.parse(xml_path)\n    root = tree.getroot()\n    size = root.find('size')\n    width = int(size.find('width').text)\n    height = int(size.find('height').text)\n    folder = root.find('folder').text\n    data_infos.append(\n        dict(\n            id=img_id,\n            filename=osp.join(folder, filename),\n            width=width,\n            height=height))\n\nreturn data_infos", "path": "mmdetection/mmdet/datasets/wider_face.py", "commit_date": "2020-07-04 00:00:00", "repo_name": "open-mmlab/mmdetection", "stars": 27227, "license": "apache-2.0", "language": "python", "size": 64742}
{"docstring": "\"\"\"Compute location targets for guided anchoring.\n\nEach feature map is divided into positive, negative and ignore regions.\n- positive regions: target 1, weight 1\n- ignore regions: target 0, weight 0\n- negative regions: target 0, weight 0.1\n\nArgs:\n    gt_bboxes_list (list[Tensor]): Gt bboxes of each image.\n    featmap_sizes (list[tuple]): Multi level sizes of each feature\n        maps.\n\nReturns:\n    tuple\n\"\"\"\n", "func_signal": "def ga_loc_targets(self, gt_bboxes_list, featmap_sizes):\n", "code": "anchor_scale = self.approx_anchor_generator.octave_base_scale\nanchor_strides = self.approx_anchor_generator.strides\n# Currently only supports same stride in x and y direction.\nfor stride in anchor_strides:\n    assert (stride[0] == stride[1])\nanchor_strides = [stride[0] for stride in anchor_strides]\n\ncenter_ratio = self.train_cfg.center_ratio\nignore_ratio = self.train_cfg.ignore_ratio\nimg_per_gpu = len(gt_bboxes_list)\nnum_lvls = len(featmap_sizes)\nr1 = (1 - center_ratio) / 2\nr2 = (1 - ignore_ratio) / 2\nall_loc_targets = []\nall_loc_weights = []\nall_ignore_map = []\nfor lvl_id in range(num_lvls):\n    h, w = featmap_sizes[lvl_id]\n    loc_targets = torch.zeros(\n        img_per_gpu,\n        1,\n        h,\n        w,\n        device=gt_bboxes_list[0].device,\n        dtype=torch.float32)\n    loc_weights = torch.full_like(loc_targets, -1)\n    ignore_map = torch.zeros_like(loc_targets)\n    all_loc_targets.append(loc_targets)\n    all_loc_weights.append(loc_weights)\n    all_ignore_map.append(ignore_map)\nfor img_id in range(img_per_gpu):\n    gt_bboxes = gt_bboxes_list[img_id]\n    scale = torch.sqrt((gt_bboxes[:, 2] - gt_bboxes[:, 0]) *\n                       (gt_bboxes[:, 3] - gt_bboxes[:, 1]))\n    min_anchor_size = scale.new_full(\n        (1, ), float(anchor_scale * anchor_strides[0]))\n    # assign gt bboxes to different feature levels w.r.t. their scales\n    target_lvls = torch.floor(\n        torch.log2(scale) - torch.log2(min_anchor_size) + 0.5)\n    target_lvls = target_lvls.clamp(min=0, max=num_lvls - 1).long()\n    for gt_id in range(gt_bboxes.size(0)):\n        lvl = target_lvls[gt_id].item()\n        # rescaled to corresponding feature map\n        gt_ = gt_bboxes[gt_id, :4] / anchor_strides[lvl]\n        # calculate ignore regions\n        ignore_x1, ignore_y1, ignore_x2, ignore_y2 = calc_region(\n            gt_, r2, featmap_sizes[lvl])\n        # calculate positive (center) regions\n        ctr_x1, ctr_y1, ctr_x2, ctr_y2 = calc_region(\n            gt_, r1, featmap_sizes[lvl])\n        all_loc_targets[lvl][img_id, 0, ctr_y1:ctr_y2 + 1,\n                             ctr_x1:ctr_x2 + 1] = 1\n        all_loc_weights[lvl][img_id, 0, ignore_y1:ignore_y2 + 1,\n                             ignore_x1:ignore_x2 + 1] = 0\n        all_loc_weights[lvl][img_id, 0, ctr_y1:ctr_y2 + 1,\n                             ctr_x1:ctr_x2 + 1] = 1\n        # calculate ignore map on nearby low level feature\n        if lvl > 0:\n            d_lvl = lvl - 1\n            # rescaled to corresponding feature map\n            gt_ = gt_bboxes[gt_id, :4] / anchor_strides[d_lvl]\n            ignore_x1, ignore_y1, ignore_x2, ignore_y2 = calc_region(\n                gt_, r2, featmap_sizes[d_lvl])\n            all_ignore_map[d_lvl][img_id, 0, ignore_y1:ignore_y2 + 1,\n                                  ignore_x1:ignore_x2 + 1] = 1\n        # calculate ignore map on nearby high level feature\n        if lvl < num_lvls - 1:\n            u_lvl = lvl + 1\n            # rescaled to corresponding feature map\n            gt_ = gt_bboxes[gt_id, :4] / anchor_strides[u_lvl]\n            ignore_x1, ignore_y1, ignore_x2, ignore_y2 = calc_region(\n                gt_, r2, featmap_sizes[u_lvl])\n            all_ignore_map[u_lvl][img_id, 0, ignore_y1:ignore_y2 + 1,\n                                  ignore_x1:ignore_x2 + 1] = 1\nfor lvl_id in range(num_lvls):\n    # ignore negative regions w.r.t. ignore map\n    all_loc_weights[lvl_id][(all_loc_weights[lvl_id] < 0)\n                            & (all_ignore_map[lvl_id] > 0)] = 0\n    # set negative regions with weight 0.1\n    all_loc_weights[lvl_id][all_loc_weights[lvl_id] < 0] = 0.1\n# loc average factor to balance loss\nloc_avg_factor = sum(\n    [t.size(0) * t.size(-1) * t.size(-2)\n     for t in all_loc_targets]) / 200\nreturn all_loc_targets, all_loc_weights, loc_avg_factor", "path": "mmdetection/mmdet/models/dense_heads/guided_anchor_head.py", "commit_date": "2020-09-30 00:00:00", "repo_name": "open-mmlab/mmdetection", "stars": 27227, "license": "apache-2.0", "language": "python", "size": 64742}
{"docstring": "\"\"\"Test det bboxes with test time augmentation.\n\nArgs:\n    feats (list[Tensor]): the outer list indicates test-time\n        augmentations and inner Tensor should have a shape NxCxHxW,\n        which contains features for all images in the batch.\n    img_metas (list[list[dict]]): the outer list indicates test-time\n        augs (multiscale, flip, etc.) and the inner list indicates\n        images in a batch. each dict has image information.\n    rescale (bool, optional): Whether to rescale the results.\n        Defaults to False.\n\nReturns:\n    list[ndarray]: bbox results of each class\n\"\"\"\n# check with_nms argument\n", "func_signal": "def aug_test_bboxes(self, feats, img_metas, rescale=False):\n", "code": "gb_sig = signature(self.get_bboxes)\ngb_args = [p.name for p in gb_sig.parameters.values()]\ngbs_sig = signature(self._get_bboxes_single)\ngbs_args = [p.name for p in gbs_sig.parameters.values()]\nassert ('with_nms' in gb_args) and ('with_nms' in gbs_args), \\\n    f'{self.__class__.__name__}' \\\n    ' does not support test-time augmentation'\n\naug_bboxes = []\naug_scores = []\naug_factors = []  # score_factors for NMS\nfor x, img_meta in zip(feats, img_metas):\n    # only one image in the batch\n    outs = self.forward(x)\n    bbox_inputs = outs + (img_meta, self.test_cfg, False, False)\n    bbox_outputs = self.get_bboxes(*bbox_inputs)[0]\n    aug_bboxes.append(bbox_outputs[0])\n    aug_scores.append(bbox_outputs[1])\n    # bbox_outputs of some detectors (e.g., ATSS, FCOS, YOLOv3)\n    # contains additional element to adjust scores before NMS\n    if len(bbox_outputs) >= 3:\n        aug_factors.append(bbox_outputs[2])\n\n# after merging, bboxes will be rescaled to the original image size\nmerged_bboxes, merged_scores = self.merge_aug_bboxes(\n    aug_bboxes, aug_scores, img_metas)\nmerged_factors = torch.cat(aug_factors, dim=0) if aug_factors else None\ndet_bboxes, det_labels = multiclass_nms(\n    merged_bboxes,\n    merged_scores,\n    self.test_cfg.score_thr,\n    self.test_cfg.nms,\n    self.test_cfg.max_per_img,\n    score_factors=merged_factors)\n\nif rescale:\n    _det_bboxes = det_bboxes\nelse:\n    _det_bboxes = det_bboxes.clone()\n    _det_bboxes[:, :4] *= det_bboxes.new_tensor(\n        img_metas[0][0]['scale_factor'])\nbbox_results = bbox2result(_det_bboxes, det_labels, self.num_classes)\nreturn bbox_results", "path": "mmdetection/mmdet/models/dense_heads/dense_test_mixins.py", "commit_date": "2020-10-01 00:00:00", "repo_name": "open-mmlab/mmdetection", "stars": 27227, "license": "apache-2.0", "language": "python", "size": 64742}
{"docstring": "\"\"\"Forward features from the upstream network.\n\nArgs:\n    feats (tuple[Tensor]): Features from the upstream network, each is\n        a 4D-tensor.\n\nReturns:\n    tuple: Usually a tuple of classification scores and bbox prediction\n        cls_scores (list[Tensor]): Classification scores for all scale\n            levels, each is a 4D-tensor, the channels number is\n            num_anchors * num_classes.\n        bbox_preds (list[Tensor]): Box energies / deltas for all scale\n            levels, each is a 4D-tensor, the channels number is\n            num_anchors * 4.\n\"\"\"\n", "func_signal": "def forward(self, feats):\n", "code": "cls_scores = []\nbbox_preds = []\nfor i, x in enumerate(feats):\n    cls_feat = feats[i]\n    reg_feat = feats[i]\n    for cls_conv in self.cls_convs[i]:\n        cls_feat = cls_conv(cls_feat)\n    for reg_conv in self.reg_convs[i]:\n        reg_feat = reg_conv(reg_feat)\n    cls_score = self.retina_cls(cls_feat)\n    bbox_pred = self.retina_reg(reg_feat)\n    cls_scores.append(cls_score)\n    bbox_preds.append(bbox_pred)\nreturn cls_scores, bbox_preds", "path": "mmdetection/mmdet/models/dense_heads/retina_sepbn_head.py", "commit_date": "2020-06-26 00:00:00", "repo_name": "open-mmlab/mmdetection", "stars": 27227, "license": "apache-2.0", "language": "python", "size": 64742}
{"docstring": "\"\"\"Initialize corner embedding layers.\n\nOnly include corner embedding branch with two parts: prefix `tl_` for\ntop-left and `br_` for bottom-right.\n\"\"\"\n", "func_signal": "def _init_corner_emb_layers(self):\n", "code": "self.tl_emb, self.br_emb = nn.ModuleList(), nn.ModuleList()\n\nfor _ in range(self.num_feat_levels):\n    self.tl_emb.append(\n        self._make_layers(\n            out_channels=self.corner_emb_channels,\n            in_channels=self.in_channels))\n    self.br_emb.append(\n        self._make_layers(\n            out_channels=self.corner_emb_channels,\n            in_channels=self.in_channels))", "path": "mmdetection/mmdet/models/dense_heads/corner_head.py", "commit_date": "2020-09-25 00:00:00", "repo_name": "open-mmlab/mmdetection", "stars": 27227, "license": "apache-2.0", "language": "python", "size": 64742}
{"docstring": "\"\"\"Gather feature according to index.\n\nArgs:\n    feat (Tensor): Target feature map.\n    ind (Tensor): Target coord index.\n    mask (Tensor | None): Mask of featuremap. Default: None.\n\nReturns:\n    feat (Tensor): Gathered feature.\n\"\"\"\n", "func_signal": "def _gather_feat(self, feat, ind, mask=None):\n", "code": "dim = feat.size(2)\nind = ind.unsqueeze(2).repeat(1, 1, dim)\nfeat = feat.gather(1, ind)\nif mask is not None:\n    mask = mask.unsqueeze(2).expand_as(feat)\n    feat = feat[mask]\n    feat = feat.view(-1, dim)\nreturn feat", "path": "mmdetection/mmdet/models/dense_heads/corner_head.py", "commit_date": "2020-09-25 00:00:00", "repo_name": "open-mmlab/mmdetection", "stars": 27227, "license": "apache-2.0", "language": "python", "size": 64742}
{"docstring": "\"\"\"Initialize layers of the head.\"\"\"\n", "func_signal": "def _init_layers(self):\n", "code": "self.relu = nn.ReLU(inplace=True)\nself.cls_convs = nn.ModuleList()\nself.reg_convs = nn.ModuleList()\nfor i in range(self.num_ins):\n    cls_convs = nn.ModuleList()\n    reg_convs = nn.ModuleList()\n    for i in range(self.stacked_convs):\n        chn = self.in_channels if i == 0 else self.feat_channels\n        cls_convs.append(\n            ConvModule(\n                chn,\n                self.feat_channels,\n                3,\n                stride=1,\n                padding=1,\n                conv_cfg=self.conv_cfg,\n                norm_cfg=self.norm_cfg))\n        reg_convs.append(\n            ConvModule(\n                chn,\n                self.feat_channels,\n                3,\n                stride=1,\n                padding=1,\n                conv_cfg=self.conv_cfg,\n                norm_cfg=self.norm_cfg))\n    self.cls_convs.append(cls_convs)\n    self.reg_convs.append(reg_convs)\nfor i in range(self.stacked_convs):\n    for j in range(1, self.num_ins):\n        self.cls_convs[j][i].conv = self.cls_convs[0][i].conv\n        self.reg_convs[j][i].conv = self.reg_convs[0][i].conv\nself.retina_cls = nn.Conv2d(\n    self.feat_channels,\n    self.num_anchors * self.cls_out_channels,\n    3,\n    padding=1)\nself.retina_reg = nn.Conv2d(\n    self.feat_channels, self.num_anchors * 4, 3, padding=1)", "path": "mmdetection/mmdet/models/dense_heads/retina_sepbn_head.py", "commit_date": "2020-06-26 00:00:00", "repo_name": "open-mmlab/mmdetection", "stars": 27227, "license": "apache-2.0", "language": "python", "size": 64742}
{"docstring": "\"\"\"Initialize conv sequential for CornerHead.\"\"\"\n", "func_signal": "def _make_layers(self, out_channels, in_channels=256, feat_channels=256):\n", "code": "return nn.Sequential(\n    ConvModule(in_channels, feat_channels, 3, padding=1),\n    ConvModule(\n        feat_channels, out_channels, 1, norm_cfg=None, act_cfg=None))", "path": "mmdetection/mmdet/models/dense_heads/corner_head.py", "commit_date": "2020-09-25 00:00:00", "repo_name": "open-mmlab/mmdetection", "stars": 27227, "license": "apache-2.0", "language": "python", "size": 64742}
{"docstring": "\"\"\"Forward feature of a single scale level.\n\nArgs:\n    x (Tensor): Features of a single scale level.\n\nReturns:\n    tuple:\n        cls_score (Tensor): Cls scores for a single scale level\n            the channels number is num_anchors * num_classes.\n        bbox_pred (Tensor): Box energies / deltas for a single scale\n            level, the channels number is num_anchors * 4.\n\"\"\"\n", "func_signal": "def forward_single(self, x):\n", "code": "cls_feat = x\nreg_feat = x\nfor cls_conv in self.cls_convs:\n    cls_feat = cls_conv(cls_feat)\nfor reg_conv in self.reg_convs:\n    reg_feat = reg_conv(reg_feat)\ncls_score = self.retina_cls(cls_feat)\nbbox_pred = self.retina_reg(reg_feat)\nreturn cls_score, bbox_pred", "path": "mmdetection/mmdet/models/dense_heads/retina_head.py", "commit_date": "2020-07-04 00:00:00", "repo_name": "open-mmlab/mmdetection", "stars": 27227, "license": "apache-2.0", "language": "python", "size": 64742}
{"docstring": "\"\"\"Extract local maximum pixel with given kernal.\n\nArgs:\n    heat (Tensor): Target heatmap.\n    kernel (int): Kernel size of max pooling. Default: 3.\n\nReturns:\n    heat (Tensor): A heatmap where local maximum pixels maintain its\n        own value and other positions are 0.\n\"\"\"\n", "func_signal": "def _local_maximum(self, heat, kernel=3):\n", "code": "pad = (kernel - 1) // 2\nhmax = F.max_pool2d(heat, kernel, stride=1, padding=pad)\nkeep = (hmax == heat).float()\nreturn heat * keep", "path": "mmdetection/mmdet/models/dense_heads/corner_head.py", "commit_date": "2020-09-25 00:00:00", "repo_name": "open-mmlab/mmdetection", "stars": 27227, "license": "apache-2.0", "language": "python", "size": 64742}
{"docstring": "\"\"\"Transpose and gather feature according to index.\n\nArgs:\n    feat (Tensor): Target feature map.\n    ind (Tensor): Target coord index.\n\nReturns:\n    feat (Tensor): Transposed and gathered feature.\n\"\"\"\n", "func_signal": "def _transpose_and_gather_feat(self, feat, ind):\n", "code": "feat = feat.permute(0, 2, 3, 1).contiguous()\nfeat = feat.view(feat.size(0), -1, feat.size(3))\nfeat = self._gather_feat(feat, ind)\nreturn feat", "path": "mmdetection/mmdet/models/dense_heads/corner_head.py", "commit_date": "2020-09-25 00:00:00", "repo_name": "open-mmlab/mmdetection", "stars": 27227, "license": "apache-2.0", "language": "python", "size": 64742}
{"docstring": "\"\"\"Forward features from the upstream network.\n\nArgs:\n    x (tensor): Input feature of BiCornerPool.\n\nReturns:\n    conv2 (tensor): Output feature of BiCornerPool.\n\"\"\"\n", "func_signal": "def forward(self, x):\n", "code": "direction1_conv = self.direction1_conv(x)\ndirection2_conv = self.direction2_conv(x)\ndirection1_feat = self.direction1_pool(direction1_conv)\ndirection2_feat = self.direction2_pool(direction2_conv)\naftpool_conv = self.aftpool_conv(direction1_feat + direction2_feat)\nconv1 = self.conv1(x)\nrelu = self.relu(aftpool_conv + conv1)\nconv2 = self.conv2(relu)\nreturn conv2", "path": "mmdetection/mmdet/models/dense_heads/corner_head.py", "commit_date": "2020-09-25 00:00:00", "repo_name": "open-mmlab/mmdetection", "stars": 27227, "license": "apache-2.0", "language": "python", "size": 64742}
{"docstring": "\"\"\"Forward function.\"\"\"\n", "func_signal": "def forward(self, inputs):\n", "code": "assert len(inputs) == self.num_ins\nouts = [inputs[0]]\nfor i in range(1, self.num_ins):\n    outs.append(\n        F.interpolate(inputs[i], scale_factor=2**i, mode='bilinear'))\nout = torch.cat(outs, dim=1)\nif out.requires_grad and self.with_cp:\n    out = checkpoint(self.reduction_conv, out)\nelse:\n    out = self.reduction_conv(out)\nouts = [out]\nfor i in range(1, self.num_outs):\n    outs.append(self.pooling(out, kernel_size=2**i, stride=2**i))\noutputs = []\n\nfor i in range(self.num_outs):\n    if outs[i].requires_grad and self.with_cp:\n        tmp_out = checkpoint(self.fpn_convs[i], outs[i])\n    else:\n        tmp_out = self.fpn_convs[i](outs[i])\n    outputs.append(tmp_out)\nreturn tuple(outputs)", "path": "mmdetection/mmdet/models/necks/hrfpn.py", "commit_date": "2020-07-20 00:00:00", "repo_name": "open-mmlab/mmdetection", "stars": 27227, "license": "apache-2.0", "language": "python", "size": 64742}
{"docstring": "\"\"\"Merge augmented detection bboxes and scores.\n\nArgs:\n    aug_bboxes (list[Tensor]): shape (n, 4*#class)\n    aug_scores (list[Tensor] or None): shape (n, #class)\n    img_shapes (list[Tensor]): shape (3, ).\n\nReturns:\n    tuple: (bboxes, scores)\n\"\"\"\n", "func_signal": "def merge_aug_bboxes(self, aug_bboxes, aug_scores, img_metas):\n", "code": "recovered_bboxes = []\nfor bboxes, img_info in zip(aug_bboxes, img_metas):\n    img_shape = img_info[0]['img_shape']\n    scale_factor = img_info[0]['scale_factor']\n    flip = img_info[0]['flip']\n    flip_direction = img_info[0]['flip_direction']\n    bboxes = bbox_mapping_back(bboxes, img_shape, scale_factor, flip,\n                               flip_direction)\n    recovered_bboxes.append(bboxes)\nbboxes = torch.cat(recovered_bboxes, dim=0)\nif aug_scores is None:\n    return bboxes\nelse:\n    scores = torch.cat(aug_scores, dim=0)\n    return bboxes, scores", "path": "mmdetection/mmdet/models/dense_heads/dense_test_mixins.py", "commit_date": "2020-10-01 00:00:00", "repo_name": "open-mmlab/mmdetection", "stars": 27227, "license": "apache-2.0", "language": "python", "size": 64742}
{"docstring": "\"\"\"Initialize weights of the head.\"\"\"\n", "func_signal": "def init_weights(self):\n", "code": "bias_init = bias_init_with_prob(0.1)\nfor i in range(self.num_feat_levels):\n    # The initialization of parameters are different between nn.Conv2d\n    # and ConvModule. Our experiments show that using the original\n    # initialization of nn.Conv2d increases the final mAP by about 0.2%\n    self.tl_heat[i][-1].conv.reset_parameters()\n    self.tl_heat[i][-1].conv.bias.data.fill_(bias_init)\n    self.br_heat[i][-1].conv.reset_parameters()\n    self.br_heat[i][-1].conv.bias.data.fill_(bias_init)\n    self.tl_off[i][-1].conv.reset_parameters()\n    self.br_off[i][-1].conv.reset_parameters()\n    if self.with_corner_emb:\n        self.tl_emb[i][-1].conv.reset_parameters()\n        self.br_emb[i][-1].conv.reset_parameters()", "path": "mmdetection/mmdet/models/dense_heads/corner_head.py", "commit_date": "2020-09-25 00:00:00", "repo_name": "open-mmlab/mmdetection", "stars": 27227, "license": "apache-2.0", "language": "python", "size": 64742}
{"docstring": "\"\"\"Initialize layers of the head.\"\"\"\n", "func_signal": "def _init_layers(self):\n", "code": "self.relu = nn.ReLU(inplace=True)\nself.cls_convs = nn.ModuleList()\nself.reg_convs = nn.ModuleList()\nfor i in range(self.stacked_convs):\n    chn = self.in_channels if i == 0 else self.feat_channels\n    self.cls_convs.append(\n        ConvModule(\n            chn,\n            self.feat_channels,\n            3,\n            stride=1,\n            padding=1,\n            conv_cfg=self.conv_cfg,\n            norm_cfg=self.norm_cfg))\n    self.reg_convs.append(\n        ConvModule(\n            chn,\n            self.feat_channels,\n            3,\n            stride=1,\n            padding=1,\n            conv_cfg=self.conv_cfg,\n            norm_cfg=self.norm_cfg))\nself.retina_cls = nn.Conv2d(\n    self.feat_channels,\n    self.num_anchors * self.cls_out_channels,\n    3,\n    padding=1)\nself.retina_reg = nn.Conv2d(\n    self.feat_channels, self.num_anchors * 4, 3, padding=1)", "path": "mmdetection/mmdet/models/dense_heads/retina_head.py", "commit_date": "2020-07-04 00:00:00", "repo_name": "open-mmlab/mmdetection", "stars": 27227, "license": "apache-2.0", "language": "python", "size": 64742}
{"docstring": "\"\"\"Initialize weights of the head.\"\"\"\n", "func_signal": "def init_weights(self):\n", "code": "for m in self.cls_convs:\n    normal_init(m.conv, std=0.01)\nfor m in self.reg_convs:\n    normal_init(m.conv, std=0.01)\nbias_cls = bias_init_with_prob(0.01)\nnormal_init(self.retina_cls, std=0.01, bias=bias_cls)\nnormal_init(self.retina_reg, std=0.01)", "path": "mmdetection/mmdet/models/dense_heads/retina_head.py", "commit_date": "2020-07-04 00:00:00", "repo_name": "open-mmlab/mmdetection", "stars": 27227, "license": "apache-2.0", "language": "python", "size": 64742}
