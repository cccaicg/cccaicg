{"docstring": "\"\"\"Convert values with si multipliers to numbers\"\"\"\n", "func_signal": "def multipliers(x):\n", "code": "try:\n    return float(x)\nexcept:\n    pass\ntry:\n    a = x[-1]\n    y = float(x[:-1])\n    endings = {'G':9,'Meg':6,'k':3,'m':-3,'u':-6,'n':-9,'p':-12,'s':0}\n    return y*(10**endings[a])\nexcept:\n    raise ValueError(\"I don't know what {} means\".format(x))", "path": "evolutionary\\cgp.py", "repo_name": "Ttl/evolutionary-circuits", "stars": 243, "license": "mit", "language": "python", "size": 281}
{"docstring": "\"\"\"Generates a random device from parts list.\n\"sigma\" is the gaussian distribution standard deviation,\nwhich is used for generating connecting nodes.\"\"\"\n", "func_signal": "def random_device(parts):\n", "code": "name = random.choice(parts.keys())\nr = parts[name]\nkw,value,model = [None]*3\ncost = 0\nif 'kwvalues' in r.keys():\n    kw = {i:value_dist(r['kwvalues'][i]) for i in r['kwvalues'].keys()}\nif 'value' in r.keys():\n    value = value_dist(r['value'])\nif 'model' in r.keys():\n    model = r['model'] if type(r['model'])==str else random.choice(r['model'])\nif 'cost' in r.keys():\n    cost = r['cost']\nreturn Device(name, value, kw, model, cost)", "path": "evolutionary\\chromosomes\\chain.py", "repo_name": "Ttl/evolutionary-circuits", "stars": 243, "license": "mit", "language": "python", "size": 281}
{"docstring": "\"\"\"Generates a random circuit.\nparts - dictionary of available devices.\ninst_limit - maximum number of instructions.\nsigma - standard deviation of nodes.\ninputs - input nodes.\noutputs - output nodes.\nspecial_nodes - power supplies and other useful but not necessary nodes.\nspecial_node_prob - Probability of having a special node in single instruction,\n    can be a list or single number.\n\"\"\"\n#Normalize probabilities and check that probabilities are valid\n", "func_signal": "def random_circuit(parts, inst_limit, sigma, inputs, outputs, special_nodes, special_node_prob, extra_value=None):\n", "code": "special = special_nodes[:]\nif type(special_node_prob)==list:\n    if not all(0<i<1 for i in special_node_prob):\n        raise ValueError(\"Invalid probability in special node probabilities list. All probabilities need to be in the open interval (0,1).\")\n    if len(special_node_prob)!=len(special):\n        raise ValueError(\"Special node lists are of different length.\")\nelse:\n    if not 0<special_node_prob<1:\n        raise ValueError(\"Invalid special node probability. Probability needs to be in the open interval (0,1).\")\n#Add the ground\nif '0' not in special:\n    special.append('0')\n    if type(special_node_prob)==list:\n        special_node_prob.append(0.1)\nif max(len(inputs),len(outputs)) > inst_limit:\n    raise ValueError(\"Instruction limit is too small.\")\nspecial = special + inputs + outputs\nif type(special_node_prob)==list:\n    special_node_prob = special_node_prob + [0.1]*(len(inputs)+len(outputs))\ninst = [random_instruction(parts, sigma, special, special_node_prob) for i in xrange(random.randint(max(len(inputs),len(outputs)),inst_limit))]\nfor e,i in enumerate(inputs):\n    nodes = inst[e].args\n    nodes[argmin(nodes)] = i\nfor e,i in enumerate(outputs,1):\n    nodes = inst[-e].args\n    nodes[argmax(nodes)] = i\nreturn Circuit(inst, parts, inst_limit, (parts,sigma,special, special_node_prob), extra_value)", "path": "evolutionary\\chromosomes\\chain.py", "repo_name": "Ttl/evolutionary-circuits", "stars": 243, "license": "mit", "language": "python", "size": 281}
{"docstring": "#Constraint for static current consumption\n", "func_signal": "def _constraint0(f,x,k,**kwargs):\n", "code": "if k[0]=='v':\n    if f<=kwargs['extra'][0]-0.2:\n        return x>kwargs['extra'][0]+0.2\n    elif f>=kwargs['extra'][0]+0.2:\n        return x<kwargs['extra'][0]-0.2\nif k[0]=='i':\n    #Limits current taken and going into the logic input to 2mA\n    #Currents taken from the supply are negative and currents going into to\n    #the supply are positive\n    return abs(x)<5e-3+0.1/kwargs['generation']**0.5\nreturn True", "path": "examples\\inverter.py", "repo_name": "Ttl/evolutionary-circuits", "stars": 243, "license": "mit", "language": "python", "size": 281}
{"docstring": "\"\"\"Mutates keyword values. Same logic as in value mutations.\"\"\"\n", "func_signal": "def mutatekwvalue(self, r):\n", "code": "kw = random.choice(self.kwvalues.keys())\nr = r[kw]\nif len(r)==3:\n    self.kwvalues[kw] = r[3](*r[:2])\nelse:\n    if r[0]>0:\n        self.kwvalues[kw] = log_dist(*r)\n    else:\n        self.kwvalues[kw] = random.uniform(*r)", "path": "evolutionary\\chromosomes\\chain.py", "repo_name": "Ttl/evolutionary-circuits", "stars": 243, "license": "mit", "language": "python", "size": 281}
{"docstring": "#Second transient simulation\n", "func_signal": "def _transient_goal_inv2(f,k,**kwargs):\n", "code": "n = 10**(-9)\nif k[0]=='v':\n    if f<=10*n:\n        return 0\n    elif 20*n<f:\n        return 5\nreturn 0", "path": "examples\\inverter.py", "repo_name": "Ttl/evolutionary-circuits", "stars": 243, "license": "mit", "language": "python", "size": 281}
{"docstring": "#Transform relative nodes to absolute by adding current node\n", "func_signal": "def __call__(self, current_node, device_number):\n", "code": "nodes = [current_node + i if type(i)==int else i for i in self.args]\n#nodes = [i if i>=0 else 0 for i in nodes]\nif self.command == 0:\n    #Connect\n    return (self.device.spice(nodes,device_number),current_node)\nif self.command == 1:\n    #Connect and move\n    return (self.device.spice(nodes,device_number),current_node+1)\n#if self.command == 2:\n#    #Move to current_node + self.args\n#    return ('',current_node + self.args)\nraise Exception(\"Invalid instruction: {}\".format(self.command))", "path": "evolutionary\\chromosomes\\chain.py", "repo_name": "Ttl/evolutionary-circuits", "stars": 243, "license": "mit", "language": "python", "size": 281}
{"docstring": "#if len(self.instructions)<len(other.instructions):\n#    return other.crossover(self)\n", "func_signal": "def crossover(self, other):\n", "code": "r = random.randint(0,1)\nl = max(len(self.instructions),len(other.instructions))\nr1 = random.randint(0,l)\nr2 = random.randint(0,l)\nif r1>r2:\n    r1,r2=r2,r1\nif r==0:\n    #Two point crossover\n    self.instructions = self.instructions[:r1]+other.instructions[r1:r2]+self.instructions[r2:]\n    self.instructions = self.instructions[:self.inst_limit]\nelse:\n    #Single point crossover\n    self.instructions = self.instructions[:r1]+other.instructions[r1:]\n    self.instructions = self.instructions[:self.inst_limit]", "path": "evolutionary\\chromosomes\\chain.py", "repo_name": "Ttl/evolutionary-circuits", "stars": 243, "license": "mit", "language": "python", "size": 281}
{"docstring": "#Goal for first transient simulation\n", "func_signal": "def _transient_goal_inv(f,k,**kwargs):\n", "code": "n = 10**(-9)\nif k[0]=='v':\n    if f<=10*n:\n        return 5\n    elif 20*n<f:\n        return 0\nreturn 0#For current", "path": "examples\\inverter.py", "repo_name": "Ttl/evolutionary-circuits", "stars": 243, "license": "mit", "language": "python", "size": 281}
{"docstring": "\"\"\"Run for newgens more generations.\n\nReturn best parameter vector from the entire run.\n\"\"\"\n", "func_signal": "def solve(self, newgens=100):\n", "code": "for gen in xrange(self.generations+1, self.generations+newgens+1):\n    for candidate in range(self.npop):\n        trial = self.get_trial(candidate)\n        trial_value = self.func(trial, *self.args)\n        if trial_value < self.pop_values[candidate]:\n            self.population[candidate] = trial\n            self.pop_values[candidate] = trial_value\n            if trial_value < self.best_value:\n                self.best_vector = trial\n                self.best_value = trial_value\n    self.best_val_history.append(self.best_value)\n    self.best_vec_history.append(self.best_vector)\n    if self.converged():\n        break\nself.generations = gen\nreturn self.best_vector", "path": "evolutionary\\optimization\\diff_evolve.py", "repo_name": "Ttl/evolutionary-circuits", "stars": 243, "license": "mit", "language": "python", "size": 281}
{"docstring": "#For every measurement in results\n", "func_signal": "def save_plot(v,k,goal_val,sim_type,generation,score,output_path,plot_title=None,yrange=None,log_plot=False,constraints=None,name=''):\n", "code": "plt.figure()\nfreq = v[0]\ngain = v[1]\nif log_plot:#Logarithmic plot\n    plt.semilogx(freq,gain,'g',basex=10)\n    plt.semilogx(freq,goal_val,'b',basex=10)\n    #if self.plot_weight:\n    #    plt.semilogx(freq,weight_val,'r--',basex=10)\n    if constraints!=None:\n        plt.plot(*zip(*constraints), marker='.', color='r', ls='')\nelse:\n    plt.plot(freq,gain,'g')\n    plt.plot(freq,goal_val,'b')\n    #if self.plot_weight:\n    #    plt.plot(freq,weight_val,'r--')\n    if constraints!=None:\n        plt.plot(*zip(*constraints), marker='.', color='r', ls='')\n\n# update axis ranges\nax = []\nax[0:4] = plt.axis()\n# check if we were given a frequency range for the plot\nif yrange!=None:\n    plt.axis([min(freq),max(freq),yrange[0],yrange[1]])\nelse:\n    plt.axis([min(freq),max(freq),min(-0.5,-0.5+min(goal_val)),max(1.5,0.5+max(goal_val))])\n\nif sim_type=='dc':\n    plt.xlabel(\"Input (V)\")\nif sim_type=='ac':\n    plt.xlabel(\"Input (Hz)\")\nif sim_type=='tran':\n    plt.xlabel(\"Time (s)\")\n\nif plot_title!=None:\n    plt.title(plot_title)\nelse:\n    plt.title(k)\n\nplt.annotate('Generation '+str(generation),xy=(0.05,0.95),xycoords='figure fraction')\nif score!=None:\n    plt.annotate('Score '+'{0:.2f}'.format(score),xy=(0.75,0.95),xycoords='figure fraction')\nplt.grid(True)\n# turn on the minor gridlines to give that awesome log-scaled look\nplt.grid(True,which='minor')\nif len(k)>=3 and k[1:3] == 'db':\n    plt.ylabel(\"Output (dB)\")\nelif k[0]=='v':\n    plt.ylabel(\"Output (V)\")\nelif k[0]=='i':\n    plt.ylabel(\"Output (A)\")\n\nplt.savefig(path_join(output_path,strftime(\"%Y-%m-%d %H:%M:%S\")+'-'+k+'-'+str(name)+'.png'))", "path": "evolutionary\\plotting.py", "repo_name": "Ttl/evolutionary-circuits", "stars": 243, "license": "mit", "language": "python", "size": 281}
{"docstring": "\"\"\"k is the name of measurement. eq. v(out)\"\"\"\n", "func_signal": "def _fitness_function1(f,k,**kwargs):\n", "code": "if k[0]=='v':\n    #-100dB/decade\n    return -43.43*log(f)+300 if f>=1000 else 0\nelif k[0]=='i':\n    #Goal for current use is 0\n    return 0", "path": "examples\\lowpass.py", "repo_name": "Ttl/evolutionary-circuits", "stars": 243, "license": "mit", "language": "python", "size": 281}
{"docstring": "#Available mutations\n", "func_signal": "def mutate(self):\n", "code": "m = [\n        len(self.instructions)>0,\n        len(self.instructions)>1,\n        len(self.instructions)>1,\n        len(self.instructions)<self.inst_limit,\n        self.extra_value != None\n    ]\nm = [i for i,c in enumerate(m) if c==True]\nr = random.choice(m)\nif r==0:\n    #Single instruction mutation\n    i = random.choice(self.instructions)\n    i.mutate(self.parts)\nelif r==1:\n    #Exchange two instructions\n    i = random.randint(0,len(self.instructions)-1)\n    c = random.randint(0,len(self.instructions)-1)\n    self.instructions[i],self.instructions[c] = self.instructions[c],self.instructions[i]\nelif r==2:\n    #Delete instruction\n    i = random.randint(0,len(self.instructions)-1)\n    del self.instructions[i]\nelif r==3:\n    #Add instructions\n    i = random_instruction(*self.inst_args)\n    self.instructions.insert(random.randint(0,len(self.instructions)),i)\nelif r==4:\n    #Change extra value\n    self.extra_value = [random.uniform(*i) for i in self.extra_range]", "path": "evolutionary\\chromosomes\\chain.py", "repo_name": "Ttl/evolutionary-circuits", "stars": 243, "license": "mit", "language": "python", "size": 281}
{"docstring": "\"\"\"Generate random instruction with random device.\n\"sigma\" is the standard deviation of nodes.\"\"\"\n", "func_signal": "def random_instruction(parts, sigma, special_nodes, special_node_prob, mu=0.5):\n", "code": "d = random_device(parts)\nnodes = [int(round(random.gauss(mu,sigma))) for i in xrange(parts[d.name]['nodes'])]\nwhile same(nodes):\n    nodes = [int(round(random.gauss(mu,sigma))) for i in xrange(parts[d.name]['nodes'])]\n    #Sprinkle some special nodes\n    if type(special_node_prob)==list:\n        for i in xrange(len(nodes)):\n            if random.random() < special_node_prob[i]:\n                nodes[i] = lst_random(special_nodes,special_node_prob)\n    else:\n        for i in xrange(len(nodes)):\n            if random.random() < special_node_prob:\n                nodes[i] = random.choice(special_nodes)\n\ncommand = random.randint(0,1)\nreturn Instruction(command, d, sigma, nodes, special_nodes, special_node_prob)", "path": "evolutionary\\chromosomes\\chain.py", "repo_name": "Ttl/evolutionary-circuits", "stars": 243, "license": "mit", "language": "python", "size": 281}
{"docstring": "\"\"\"Mirrors values over bounds back to bounded area,\nor randomly generates a new coordinate if mirroring failed.\"\"\"\n", "func_signal": "def mirror_bounds(self,trial):\n", "code": "for i in xrange(self.ndim):\n    if trial[i]<self.lbound[i]:\n        trial[i] = 2*self.lbound[i]-trial[i]\n        if trial[i]<self.lbound[i]:\n            trial[i] = random.random()*(self.ubound[i]-self.lbound[i]) + self.lbound[i]\n    elif trial[i]>self.ubound[i]:\n        trial[i] = 2*self.ubound[i]-trial[i]\n        if trial[i]>self.ubound[i]:\n            trial[i] = random.random()*(self.ubound[i]-self.lbound[i]) + self.lbound[i]\nreturn trial", "path": "evolutionary\\optimization\\diff_evolve.py", "repo_name": "Ttl/evolutionary-circuits", "stars": 243, "license": "mit", "language": "python", "size": 281}
{"docstring": "\"\"\"Mutates device value. If r is 3-tuple third value is a random\nnumber distribution. Two first are lower and upper limits.\"\"\"\n", "func_signal": "def mutatevalue(self, r):\n", "code": "if len(r)==3:\n    self.value = r[3](*r[:2])\nelse:\n    if r[0]>0:\n        self.value = log_dist(*r)\n    else:\n        self.value = random.uniform(*r)", "path": "evolutionary\\chromosomes\\chain.py", "repo_name": "Ttl/evolutionary-circuits", "stars": 243, "license": "mit", "language": "python", "size": 281}
{"docstring": "\"\"\"k is the name of measurement. eq. v(out)\"\"\"\n#This is the DC transfer curve goal function\n", "func_signal": "def _goalinv(f,k,**kwargs):\n", "code": "if k=='v(out)':\n    #kwargs['extra'][0] is the transition voltage\n    return 5 if f<=kwargs['extra'][0] else 0\nelif k[0]=='i':\n    #Goal for current use is 0\n    return 0", "path": "examples\\inverter.py", "repo_name": "Ttl/evolutionary-circuits", "stars": 243, "license": "mit", "language": "python", "size": 281}
{"docstring": "\"\"\"Used in plotting, when only 1 circuits needs to be simulated\"\"\"\n", "func_signal": "def eval_single(self, ckt, options):\n", "code": "program = ckt.spice(options)\nthread = circuits.spice_thread(program)\nthread.start()\nthread.join(2*self.timeout)\nreturn thread.result", "path": "evolutionary\\cgp.py", "repo_name": "Ttl/evolutionary-circuits", "stars": 243, "license": "mit", "language": "python", "size": 281}
{"docstring": "\"\"\"f is input, x is output, k is measurement, extra is extra value of chromosome\"\"\"\n#Same as last one, but with other way around\n", "func_signal": "def _constraint2(f,x,k,**kwargs):\n", "code": "if k[0]=='v' and f<9e-9:\n    return x<kwargs['extra'][0]-0.2\nif k[0]=='v' and f>350e-9:\n    return x>kwargs['extra'][0]+0.2\nif k[0]=='i':\n    return abs(x)<10e-3+0.1/kwargs['generation']**0.5\nreturn True", "path": "examples\\inverter.py", "repo_name": "Ttl/evolutionary-circuits", "stars": 243, "license": "mit", "language": "python", "size": 281}
{"docstring": "#Select chromosomes from pool using weighted probablities\n", "func_signal": "def sf(pool,weight=1):\n", "code": "r=random.random()**weight\nreturn random.choice(pool[:1+int(len(pool)*r)])[1]", "path": "evolutionary\\cgp.py", "repo_name": "Ttl/evolutionary-circuits", "stars": 243, "license": "mit", "language": "python", "size": 281}
{"docstring": "\"\"\"\nStore join times for current nicknames when we first join.\n\"\"\"\n", "func_signal": "def handle_joined(self, connection, event):\n", "code": "nicknames = [s.lstrip(\"@+\") for s in event.arguments()[-1].split()]\nfor nickname in nicknames:\n    self.joined[nickname] = datetime.now()", "path": "gnotty\\bots\\commands.py", "repo_name": "stephenmcd/gnotty", "stars": 159, "license": "bsd-2-clause", "language": "python", "size": 814}
{"docstring": "\"\"\"\nSets up logging and interval events.\n\"\"\"\n", "func_signal": "def __init__(self, *args, **kwargs):\n", "code": "super(BaseBot, self).__init__(*args, **kwargs)\nfmt = Formatter(\"[%(server)s%(channel)s] %(nickname)s: %(message)s\")\nhandler = StreamHandler()\nhandler.setFormatter(fmt)\nlogger = getLogger(\"irc.message\")\nlogger.setLevel(settings.LOG_LEVEL)\nlogger.addHandler(handler)\n# Spawn a thread (greenlet) for each timer event handler.\nfor handler in self.events.get(\"timer\", []):\n    spawn(self.handle_timer_event, handler)", "path": "gnotty\\bots\\base.py", "repo_name": "stephenmcd/gnotty", "stars": 159, "license": "bsd-2-clause", "language": "python", "size": 814}
{"docstring": "\"\"\"\nWebhook handler - each handler for the webhook event\ntakes an initial pattern argument for matching the URL\nrequested. Here we match the URL to the pattern for each\nwebhook handler, and bail out if it returns a response.\n\"\"\"\n", "func_signal": "def handle_webhook_event(self, environ, url, params):\n", "code": "for handler in self.events[\"webhook\"]:\n    urlpattern = handler.event.args[\"urlpattern\"]\n    if not urlpattern or match(urlpattern, url):\n        response = handler(self, environ, url, params)\n        if response:\n            return response", "path": "gnotty\\bots\\base.py", "repo_name": "stephenmcd/gnotty", "stars": 159, "license": "bsd-2-clause", "language": "python", "size": 814}
{"docstring": "\"\"\"\nCommand handler - treats each word in the message\nthat triggered the command as an argument to the command,\nand does some validation to ensure that the number of\narguments match.\n\"\"\"\n", "func_signal": "def handle_command_event(self, event, command, args):\n", "code": "argspec = getargspec(command)\nnum_all_args = len(argspec.args) - 2  # Ignore self/event args\nnum_pos_args = num_all_args - len(argspec.defaults or [])\nif num_pos_args <= len(args) <= num_all_args:\n    response = command(self, event, *args)\nelif num_all_args == num_pos_args:\n    s = \"s are\" if num_all_args != 1 else \" is\"\n    response = \"%s arg%s required\" % (num_all_args, s)\nelse:\n    bits = (num_pos_args, num_all_args)\n    response = \"between %s and %s args are required\" % bits\nresponse = \"%s: %s\" % (self.get_nickname(event), response)\nself.message_channel(response)", "path": "gnotty\\bots\\base.py", "repo_name": "stephenmcd/gnotty", "stars": 159, "license": "bsd-2-clause", "language": "python", "size": 814}
{"docstring": "\"\"\"\nShows the amount of time since the given nickname was last\nseen in the channel.\n\"\"\"\n", "func_signal": "def seen(self, event, nickname):\n", "code": "try:\n    self.joined[nickname]\nexcept KeyError:\n    pass\nelse:\n    if nickname == self.get_nickname(event):\n        prefix = \"you are\"\n    else:\n        prefix = \"%s is\" % nickname\n    return \"%s here right now\" % prefix\ntry:\n    seen = self.timesince(self.quit[nickname])\nexcept KeyError:\n    return \"%s has never been seen\" % nickname\nelse:\n    return \"%s was last seen %s ago\" % (nickname,  seen)", "path": "gnotty\\bots\\commands.py", "repo_name": "stephenmcd/gnotty", "stars": 159, "license": "bsd-2-clause", "language": "python", "size": 814}
{"docstring": "\"\"\"\nShows the amount of time since the given nickname has been\nin the channel. If no nickname is given, I'll use my own.\n\"\"\"\n", "func_signal": "def uptime(self, event, nickname=None):\n", "code": "if nickname and nickname != self.nickname:\n    try:\n        uptime = self.timesince(self.joined[nickname])\n    except KeyError:\n        return \"%s is not in the channel\" % nickname\n    else:\n        if nickname == self.get_nickname(event):\n            prefix = \"you have\"\n        else:\n            prefix = \"%s has\" % nickname\n        return \"%s been here for %s\" % (prefix, uptime)\nuptime = self.timesince(self.joined[self.nickname])\nreturn \"I've been here for %s\" % uptime", "path": "gnotty\\bots\\commands.py", "repo_name": "stephenmcd/gnotty", "stars": 159, "license": "bsd-2-clause", "language": "python", "size": 814}
{"docstring": "\"\"\"\nShows the help message for the bot. Takes an optional command name\nwhich when given, will show help for that command.\n\"\"\"\n", "func_signal": "def help(self, event, command_name=None):\n", "code": "if command_name is None:\n    return (\"Type !commands for a list of all commands. Type \"\n            \"!help [command] to see help for a specific command.\")\ntry:\n    command = self.commands_dict()[command_name]\nexcept KeyError:\n    return \"%s is not a command\" % command_name\n\nargspec = getargspec(command)\nargs = argspec.args[2:]\ndefaults = argspec.defaults or []\nfor i in range(-1, -len(defaults) - 1, -1):\n    args[i] = \"%s [default: %s]\" % (args[i], defaults[i])\nargs = \", \".join(args)\nhelp = getdoc(command).replace(\"\\n\", \" \")\nreturn \"help for %s: (args: %s) %s\" % (command_name, args, help)", "path": "gnotty\\bots\\commands.py", "repo_name": "stephenmcd/gnotty", "stars": 159, "license": "bsd-2-clause", "language": "python", "size": 814}
{"docstring": "\"\"\"\nLists all available commands.\n\"\"\"\n", "func_signal": "def commands(self, event):\n", "code": "commands = sorted(self.commands_dict().keys())\nreturn \"Available commands: %s\" % \" \".join(commands)", "path": "gnotty\\bots\\commands.py", "repo_name": "stephenmcd/gnotty", "stars": 159, "license": "bsd-2-clause", "language": "python", "size": 814}
{"docstring": "\"\"\"\nStore quit time for a nickname when it quits.\n\"\"\"\n", "func_signal": "def handle_quit(self, connection, event):\n", "code": "nickname = self.get_nickname(event)\nself.quit[nickname] = datetime.now()\ndel self.joined[nickname]", "path": "gnotty\\bots\\commands.py", "repo_name": "stephenmcd/gnotty", "stars": 159, "license": "bsd-2-clause", "language": "python", "size": 814}
{"docstring": "\"\"\"\nTry and initialize with Django settings.\n\"\"\"\n", "func_signal": "def __init__(self):\n", "code": "self.option_list = parser.option_groups[0].option_list\n# Some constants about the software.\nself[\"GNOTTY_VERSION\"] = __version__\nself[\"GNOTTY_VERSION_STRING\"] = __version_string__\nself[\"GNOTTY_PROJECT_URL\"] = __url__\ntry:\n    from django.conf import settings\n    for k, v in parser.defaults.items():\n        self[k] = getattr(settings, \"GNOTTY_%s\" % k, v)\n    self.set_max_message_length()\nexcept ImportError:\n    pass", "path": "gnotty\\conf.py", "repo_name": "stephenmcd/gnotty", "stars": 159, "license": "bsd-2-clause", "language": "python", "size": 814}
{"docstring": "\"\"\"\nWe won't receive our own messages, so log them manually.\n\"\"\"\n", "func_signal": "def message_channel(self, message):\n", "code": "self.log(None, message)\nsuper(BaseBot, self).message_channel(message)", "path": "gnotty\\bots\\base.py", "repo_name": "stephenmcd/gnotty", "stars": 159, "license": "bsd-2-clause", "language": "python", "size": 814}
{"docstring": "\"\"\"\nIterates through each of the feed URLs, parses their items, and\nsends any items to the channel that have not been previously\nbeen parsed.\n\"\"\"\n", "func_signal": "def parse_feeds(self, message_channel=True):\n", "code": "if parse:\n    for feed_url in self.feeds:\n        feed = parse(feed_url)\n        for item in feed.entries:\n            if item[\"id\"] not in self.feed_items:\n                self.feed_items.add(item[\"id\"])\n                if message_channel:\n                    message = self.format_item_message(feed, item)\n                    self.message_channel(message)\n                    return", "path": "gnotty\\bots\\rss.py", "repo_name": "stephenmcd/gnotty", "stars": 159, "license": "bsd-2-clause", "language": "python", "size": 814}
{"docstring": "\"\"\"\nLog any public messages, and also handle the command event.\n\"\"\"\n", "func_signal": "def on_pubmsg(self, connection, event):\n", "code": "for message in event.arguments():\n    self.log(event, message)\n    command_args = filter(None, message.split())\n    command_name = command_args.pop(0)\n    for handler in self.events[\"command\"]:\n        if handler.event.args[\"command\"] == command_name:\n            self.handle_command_event(event, handler, command_args)", "path": "gnotty\\bots\\base.py", "repo_name": "stephenmcd/gnotty", "stars": 159, "license": "bsd-2-clause", "language": "python", "size": 814}
{"docstring": "\"\"\"\nShows version information.\n\"\"\"\n", "func_signal": "def version(self, event):\n", "code": "name = \"%s.%s\" % (self.__class__.__module__, self.__class__.__name__)\nreturn \"%s [%s]\" % (settings.GNOTTY_VERSION_STRING, name)", "path": "gnotty\\bots\\commands.py", "repo_name": "stephenmcd/gnotty", "stars": 159, "license": "bsd-2-clause", "language": "python", "size": 814}
{"docstring": "\"\"\"\nStore join time for a nickname when it joins.\n\"\"\"\n", "func_signal": "def handle_join(self, connection, event):\n", "code": "nickname = self.get_nickname(event)\nself.joined[nickname] = datetime.now()", "path": "gnotty\\bots\\commands.py", "repo_name": "stephenmcd/gnotty", "stars": 159, "license": "bsd-2-clause", "language": "python", "size": 814}
{"docstring": "\"\"\"\nRuns each timer handler in a separate greenlet thread.\n\"\"\"\n", "func_signal": "def handle_timer_event(self, handler):\n", "code": "while True:\n    handler(self)\n    sleep(handler.event.args[\"seconds\"])", "path": "gnotty\\bots\\base.py", "repo_name": "stephenmcd/gnotty", "stars": 159, "license": "bsd-2-clause", "language": "python", "size": 814}
{"docstring": "\"\"\"\nReturns human friendly version of the timespan between now\nand the given datetime.\n\"\"\"\n", "func_signal": "def timesince(self, when):\n", "code": "units = (\n    (\"year\",   60 * 60 * 24 * 365),\n    (\"week\",   60 * 60 * 24 * 7),\n    (\"day\",    60 * 60 * 24),\n    (\"hour\",   60 * 60),\n    (\"minute\", 60),\n    (\"second\", 1),\n)\ndelta = datetime.now() - when\ntotal_seconds = delta.days * 60 * 60 * 24 + delta.seconds\nparts = []\nfor name, seconds in units:\n    value = total_seconds / seconds\n    if value > 0:\n        total_seconds %= seconds\n        s = \"s\" if value != 1 else \"\"\n        parts.append(\"%s %s%s\" % (value, name, s))\nreturn \" and \".join(\", \".join(parts).rsplit(\", \", 1))", "path": "gnotty\\bots\\commands.py", "repo_name": "stephenmcd/gnotty", "stars": 159, "license": "bsd-2-clause", "language": "python", "size": 814}
{"docstring": "\"\"\"\nThis is the method in ``SimpleIRCClient`` that all IRC events\nget passed through. Here we map events to our own custom\nevent handlers, and call them.\n\"\"\"\n", "func_signal": "def _dispatcher(self, connection, event):\n", "code": "super(BaseBot, self)._dispatcher(connection, event)\nfor handler in self.events[event.eventtype()]:\n    handler(self, connection, event)", "path": "gnotty\\bots\\base.py", "repo_name": "stephenmcd/gnotty", "stars": 159, "license": "bsd-2-clause", "language": "python", "size": 814}
{"docstring": "\"\"\"\nBuild a dict mapping event names to their handler methods,\nwhich are any methods defined on any classes in the\ninheritance heirarchy, that have been marked with an\n\"event\" attribute, which gets assgined by the\n``gnotty.bots.events.on`` decorator.\n\"\"\"\n", "func_signal": "def __new__(cls, name, bases, attrs):\n", "code": "def all_bases(bases):\n    for base in bases:\n        yield base\n        for base in all_bases(base.__bases__):\n            yield base\nbase_values = [b.__dict__.values() for b in set(all_bases(bases))]\nattrs[\"events\"] = defaultdict(list)\nfor member in sum(base_values, attrs.values()):\n    if hasattr(member, \"event\"):\n        attrs[\"events\"][member.event.name].append(member)\nreturn type.__new__(cls, name, bases, attrs)", "path": "gnotty\\bots\\base.py", "repo_name": "stephenmcd/gnotty", "stars": 159, "license": "bsd-2-clause", "language": "python", "size": 814}
{"docstring": "\"\"\"\nPause for a random few seconds before messaging, to seem less\nbot like.\n\"\"\"\n", "func_signal": "def message_channel_delayed(self, message):\n", "code": "sleep(randint(2, 5))\nself.message_channel(message)", "path": "gnotty\\bots\\chat.py", "repo_name": "stephenmcd/gnotty", "stars": 159, "license": "bsd-2-clause", "language": "python", "size": 814}
{"docstring": "\"\"\"\nReturns a two column :class:`pandas.DataFrame` with columns \n``['outperformance', 'num_upticks']`` that shows the cumulative \noutperformance (in log returns) and the ``num_upticks`` number of \ndays the uptick lasted\n\n:ARGS:\n\n    series: :class:`pandas.Series` of asset prices\n\n    benchmark: :class:`pandas.Series` of prices to compare \n    ``series`` against\n\n:RETURNS:\n\n    :class:`pandas.DataFrame` of ``['outperformance',\n    'num_upticks']``. Outperformance is in log returns and \n    num_upticks the number of consecutive upticks for which the \n    outperformance was generated\n\"\"\"\n", "func_signal": "def consecutive_uptick_relative_performance(series, benchmark, n_ticks = 3):\n", "code": "def _consecutive_uptick_relative_performance(series, benchmark, n_ticks):\n    upticks = consecutive_upticks(benchmark, n_ticks = n_ticks)\n    series_up  = series[upticks.index]\n    bench_up = benchmark[upticks.index]\n    st, fin = upticks == 0, (upticks == 0).shift(-1).fillna(True)\n    n_per = upticks[fin]\n    series_rets = numpy.log(numpy.divide(series_up[fin], \n                                         series_up[st]))\n    bench_rets = numpy.log(numpy.divide(bench_up[fin], bench_up[st]))\n    return pandas.DataFrame({'outperformance':series_rets.subtract(\n        bench_rets), 'num_upticks':n_per, series.name: series_rets,\n        benchmark.name: bench_rets}, columns = [benchmark.name,\n        series.name, 'outperformance', 'num_upticks'] )\n\nif isinstance(benchmark, pandas.DataFrame):\n    return map(lambda x: _consecutive_uptick_relative_performance(\n           series = series, benchmark = benchmark[x], n_ticks = n_ticks),\n           benchmark.columns)\nelse:\n    return _consecutive_uptick_relative_performance(\n           series = series, benchmark = benchmark, n_ticks = n_ticks)", "path": "visualize_wealth\\analyze.py", "repo_name": "benjaminmgross/visualize-wealth", "stars": 130, "license": "None", "language": "python", "size": 90096}
{"docstring": "\"\"\"\nReturns the proportion of ``series``'s cumulative positive returns \nto ``benchmark``'s cumulative  returns, given benchmark's returns \nwere positive in that period\n\n:ARGS:\n\n    series: :class:`pandas.Series` of prices\n\n    benchmark: :class:`pandas.Series` of prices to compare ``series`` \n    against\n\n:RETURNS:\n\n    float: of the upcapture of cumulative positive returns\n\n.. seealso:: :py:data:`median_upcature(series, benchmark)`\n\n\"\"\"\n", "func_signal": "def upcapture(series, benchmark):\n", "code": "def _upcapture(series, benchmark):\n    series_rets = log_returns(series)\n    bench_rets = log_returns(benchmark)\n    index = bench_rets > 0.\n    return series_rets[index].mean() / bench_rets[index].mean()\n\nif isinstance(benchmark, pandas.DataFrame):\n    return benchmark.apply(lambda x: _upcapture(series, x))\nelse:\n    return _upcapture(series, benchmark)", "path": "visualize_wealth\\analyze.py", "repo_name": "benjaminmgross/visualize-wealth", "stars": 130, "license": "None", "language": "python", "size": 90096}
{"docstring": "\"\"\"\nValue at Risk (\"VaR\") of the :math:`p = \\\\alpha` quantile, defines \nthe loss, such that there is an :math:`\\\\alpha` percent chance of \na loss, greater than or equal to :math:`\\\\textrm{VaR}_\\\\alpha`. \n:meth:`var_norm` fits a normal distribution to the log returns of \nthe series, and then estimates the :math:`\\\\textrm{VaR}_\\\\alpha`\n\n:ARGS:\n\n    series: :class:`pandas.Series` or :class:`pandas.DataFrame` \n    of prices\n\n    p: :class:`float` of the :math:`\\\\alpha` quantile for which to \n    estimate VaR\n\n:RETURNS:\n\n    :class:`float` or :class:`pandas.Series` of VaR\n\n.. note:: Derivation of Value at Risk\n\n    Let :math:`Y \\\\sim N(\\\\mu, \\\\sigma^2)`, we choose \n    :math:`y_\\\\alpha` such that \n    :math:`\\\\mathbb{P}(Y < y_\\\\alpha) = \\\\alpha`. \n\n    Then,\n\n    .. math::\n\n        \\\\mathbb{P}(Y < y_\\\\alpha) &= \\\\alpha \\\\\\\\\n        \\\\Rightarrow \\\\mathbb{P}(\\\\frac{Y - \\\\mu}{\\\\sigma} < \n        \\\\frac{y_\\\\alpha - \\\\mu}{\\\\sigma}) &= \\\\alpha \n        \\\\\\\\\n        \\\\Rightarrow \\\\mathbb{P}(Z < \\\\frac{y_\\\\alpha - \n        \\\\mu}{\\sigma} &= \\\\alpha\n        \\\\\\\\\n        \\\\Rightarrow \\\\Phi(\\\\frac{y_\\\\alpha - \\\\mu}{\\\\sigma} ) \n        &= \\\\alpha, \n\n    where :math:`\\\\Phi(.)` is the standard normal cdf operator.\n    Then using the inverse of the function :math:`\\\\Phi`, \n    we have:\n\n    .. math::\n\n        \\\\Phi^{-1}( \\\\Phi(\\\\frac{y_\\\\alpha - \\\\mu}{\\\\sigma} ) ) \n        &= \\\\Phi^{-1}(\\\\alpha) \n        \\\\\\\\\n        \\\\Rightarrow \\\\Phi^{-1}(\\\\alpha)\\\\cdot\\\\sigma + \\\\mu \n        = y_\\\\alpha \n\n    But :math:`y_\\\\alpha` is negative and VaR is always \n    positive, so,\n\n    .. math:: \n\n        VaR_\\\\alpha = -y_\\\\alpha &= -\\\\Phi^{-1}\n        (\\\\alpha)\\\\cdot\\\\sigma - \\\\mu\n        \\\\\\\\\n        &= \\\\Phi^{-1}(1 - \\\\alpha) - \\\\mu \\\\\\\\\n\n.. seealso:: :meth:var_cf :meth:var_np\n         \n\"\"\"\n", "func_signal": "def var_norm(series, p = .01):\n", "code": "def _var_norm(series, p):\n    series_rets = log_returns(series)\n    mu, sigma = series_rets.mean(), series_rets.std()\n    v = lambda alpha: scipy.stats.distributions.norm.ppf(1 - alpha)\n    return numpy.exp(sigma * v(p) - mu) - 1\n\nif isinstance(series, pandas.DataFrame):\n    return series.apply(lambda x: _var_norm(x, p = p))\nelse:\n    return _var_norm(series, p = p)", "path": "visualize_wealth\\analyze.py", "repo_name": "benjaminmgross/visualize-wealth", "stars": 130, "license": "None", "language": "python", "size": 90096}
{"docstring": "\"\"\"\nA measure of the risk-adjusted return of a financial security or portfolio\nthat is equal to the active return divided by the tracking error between the \nportfolio and the benchmark (MATE is used here, see the benefits of \n    MATE over TE)\n\nseries: :class:`pandas.Series` or `pandas.DataFrame` of asset prices\n\nbenchamrk: :class:`pandas.Series` of prices\n\nfreq: :class:`string` either ['daily' , 'monthly', 'quarterly', or yearly']\nindicating the frequency of the data. Default, 'daily'\n\nrfr: :class:`float` of the risk free rate\n\n.. note:: Calculating Information Ratio\n\n    .. math:: \n\n        \\\\textrm{IR} \\\\triangleq \\\\frac{\\\\alpha}{\\\\omega} \\\\\\\\\n    \n    where,\n\n    .. math::\n\n        \\\\alpha &= \\\\textrm{active return} \\\\\\\\\n        \\\\omega &= \\\\textrm{tracking error} \\\\\\\\\n\n\"\"\"\n", "func_signal": "def information_ratio(series, benchmark, freq = 'daily'):\n", "code": "def _information_ratio(series, benchmark, freq = freq):\n    ar = active_return(series, benchmark, freq = freq)\n    mate = mean_absolute_tracking_error(series, benchmark, freq = freq)\n    return ar / mate\n\nif isinstance(benchmark, pandas.DataFrame):\n    return benchmark.apply(lambda x: _information_ratio(series, x, freq = freq))\nelse:\n    return _information_ratio(series, benchmark, freq = freq)", "path": "visualize_wealth\\analyze.py", "repo_name": "benjaminmgross/visualize-wealth", "stars": 130, "license": "None", "language": "python", "size": 90096}
{"docstring": "\"\"\"\nThe Adjusted R-Squared that incorporates the number of \nindependent variates using the `Formula Found of Wikipedia\n<http://en.wikipedia.org/wiki/Coefficient_of_determination#Adjusted_R2>_`\n\n:ARGS:\n\n    series: :class:`pandas.Series` of asset returns\n\n    benchmark: :class:`pandas.DataFrame` of benchmark returns to \n    explain the returns of the ``series``\n\n    weights: :class:`pandas.Series` of weights to weight each column \n    of the benchmark\n\n:RETURNS:\n\n    :class:float of the adjusted r-squared`\n\"\"\"\n", "func_signal": "def r2_adj(series, benchmark):\n", "code": "n = len(series)\np = 1\nreturn 1 - (1 - r2(series, benchmark))*(n - 1)/(n - p - 1)", "path": "visualize_wealth\\analyze.py", "repo_name": "benjaminmgross/visualize-wealth", "stars": 130, "license": "None", "language": "python", "size": 90096}
{"docstring": "\"\"\"\nReturns the geometric difference of returns where\n\n:ARGS:\n\n    a: :class:`pandas.Series` or :class:`float`\n\n    b: :class:`pandas.Series` or :class:`float`\n\n:RETURNS:\n\n    same class as inputs\n\n.. math::\n\n    \\\\textrm{GD} = \\\\frac{(1 + a )}{(1 + b)}  - 1 \\\\\\\\\n\"\"\"\n", "func_signal": "def geometric_difference(a, b):\n", "code": "if isinstance(a, pandas.Series):\n    msg = \"index must be equal for pandas.Series\"\n    assert a.index.equals(b.index), msg\n    return (1. + a).divide(1. + b) - 1.\nelse:\n    return (1. + a) / (1. + b) - 1.", "path": "visualize_wealth\\analyze.py", "repo_name": "benjaminmgross/visualize-wealth", "stars": 130, "license": "None", "language": "python", "size": 90096}
{"docstring": "\"\"\"\nAlpha is defined as excess return, over and above its \nexpected return, derived from an asset's sensitivity to an given benchmark, \nand the return of that benchmrk.\n\nseries: :class:`pandas.Series` or `pandas.DataFrame` of asset prices\n\nbenchamrk: :class:`pandas.Series` of prices\n\nfreq: :class:`string` either ['daily' , 'monthly', 'quarterly', or yearly']\nindicating the frequency of the data. Default, 'daily'\n\nrfr: :class:`float` of the risk free rate\n\n.. math::\n\n    \\\\alpha \\\\triangleq (R_p - r_f) - \\\\beta_i \\\\cdot ( R_b - rf ) \n    \n    \\\\textrm{where},\n\n        R_p &= \\\\textrm{Portfolio Annualized Return} \\\\\\\\\n        R_b &= \\\\textrm{Benchmark Annualized Return} \\\\\\\\\n        r_f &= \\\\textrm{Risk Free Rate} \\\\\\\\\n        \\\\beta &= \\\\textrm{Portfolio Sensitivity to the Benchmark}\n\"\"\"\n", "func_signal": "def alpha(series, benchmark, freq = 'daily', rfr = 0.0):\n", "code": "def _alpha(series, benchmark, freq = 'daily', rfr = rfr):\n    R_p = annualized_return(series, freq = freq)\n    R_b = annualized_return(benchmark, freq = freq)\n    b = beta(series, benchmark)\n    return  R_p - rfr - b * (R_b - rfr)\n\nif isinstance(benchmark, pandas.DataFrame):\n    return benchmark.apply(lambda x: _alpha(\n        series, x, freq = freq, rfr = rfr))\nelse:\n    return _alpha(series, benchmark, freq = freq, rfr = rfr)", "path": "visualize_wealth\\analyze.py", "repo_name": "benjaminmgross/visualize-wealth", "stars": 130, "license": "None", "language": "python", "size": 90096}
{"docstring": "\"\"\"\nUsing the :func:`num_consecutive`, returns a :class:`pandas.Series` \nof the consecutive upticks in the series with greater than 3 \nconsecutive upticks\n\n:ARGS:\n\n    series: :class:`pandas.Series` of the asset prices\n\n:RETURNS:\n\n    :class:`pandas.Series` of the consecutive downticks of the series\n\"\"\"\n", "func_signal": "def consecutive_upticks(series, n_ticks = 3):\n", "code": "w = consecutive( (series > series.shift(1)).astype(int) )\nagg_ind = w[w > n_ticks - 1].index.union_many(\n          map(lambda x: w[w.shift(-x) == n_ticks].index,\n          numpy.arange(n_ticks + 1) ))\n\nreturn w[agg_ind]", "path": "visualize_wealth\\analyze.py", "repo_name": "benjaminmgross/visualize-wealth", "stars": 130, "license": "None", "language": "python", "size": 90096}
{"docstring": "\"\"\"\nReturns the exponentially weighted, annualized standard deviation\n\n:ARGS:\n\n    series: :class:`Series` or :class:`DataFrame` of prices\n\n    theta: coefficient of decay, default BARRA's value of .94 \n    which roughly equates to a span of 33 days\n\n    freq: :class:`string` of either ['daily', 'monthly', 'quarterly', \n    'yearly']\n\"\"\"\n", "func_signal": "def ew_vol(series, theta = 0.94, freq = 'daily'):\n", "code": "span = (1. + theta)/(1 - theta)\n\nlog_rets = log_returns(series)\n\nfac = _interval_to_factor(freq)\n\new_vol = pandas.ewmstd(log_rets,\n                       span = span,\n                       min_periods = span\n)\n\nreturn ew_vol*numpy.sqrt(fac)", "path": "visualize_wealth\\analyze.py", "repo_name": "benjaminmgross/visualize-wealth", "stars": 130, "license": "None", "language": "python", "size": 90096}
{"docstring": "\"\"\"\nReturns the volatility of the returns that are less than zero\n\n:ARGS:\n\n    series:``pandas.Series`` of prices\n\n    freq: ``str`` of frequency, either ``daily, monthly, quarterly, \n    or yearly``\n\n:RETURNS:\n\n    float: of the downside standard deviation\n\n\"\"\"\n", "func_signal": "def downside_deviation(series, freq = 'daily'):\n", "code": "def _downside_deviation(series, freq = 'daily'):\n    fac = _interval_to_factor(freq)\n    series_rets = log_returns(series)\n    index = series_rets < 0.    \n    return series_rets[index].std()*numpy.sqrt(fac)\n\nif isinstance(series, pandas.DataFrame):\n    return series.apply(lambda x: _downside_deviation(x, freq = freq))\nelse:\n    return _downside_deviation(series, freq = freq)", "path": "visualize_wealth\\analyze.py", "repo_name": "benjaminmgross/visualize-wealth", "stars": 130, "license": "None", "language": "python", "size": 90096}
{"docstring": "\"\"\"\nArray logic (no for loops) and fast method to determine the number of\nconsecutive ones given a `pandas.Series` of integers Derived from \n`Stack Overflow\n<http://stackoverflow.com/questions/18196811/cumsum-reset-at-nan>`_\n\n:ARGS:\n\n    int_series: :class:`pandas.Series` of integers as 0s or 1s\n\n:RETURNS:\n\n    :class:`pandas.Series` of the consecutive ones\n\"\"\"\n", "func_signal": "def consecutive(int_series):\n", "code": "n = int_series == 0\na = ~n\nc = a.cumsum()\nindex = c[n].index\nd = pandas.Series(numpy.diff(numpy.hstack(( [0.], c[n] ))) , \n                  index =index)\nint_series[n] = -d\nreturn int_series.cumsum()", "path": "visualize_wealth\\analyze.py", "repo_name": "benjaminmgross/visualize-wealth", "stars": 130, "license": "None", "language": "python", "size": 90096}
{"docstring": "\"\"\"\nReturns the maximum drawdown, or the maximum peak to trough linear \ndistance, as a positive drawdown value\n\n:ARGS:\n\n    series: ``pandas.Series`` of prices\n\n:RETURNS:\n\n    float: the maximum drawdown of the period, expressed as a \n    positive number\n\n.. code::\n\n    import visualize_wealth.performance as vwp\n\n    max_dd = vwp.max_drawdown(price_series)\n    \"\"\"\n", "func_signal": "def max_drawdown(series):\n", "code": "def _max_drawdown(series):\n    return numpy.max(1 - series/series.cummax())\nif isinstance(series, pandas.DataFrame):\n    return series.apply(_max_drawdown)\nelse:\n    return _max_drawdown(series)", "path": "visualize_wealth\\analyze.py", "repo_name": "benjaminmgross/visualize-wealth", "stars": 130, "license": "None", "language": "python", "size": 90096}
{"docstring": "\"\"\"\nReturns the exponentially weighted sensitivity of one return \nseries to a chosen benchmark\n\n:ARGS:\n\n    series: :class:`Series` of prices\n\n    benchmark: :class:`Series` of a benchmark to calculate the \n    sensitivity\n\n    theta: :class:`float` of the exponential smoothing constant\n    default to 0.94 MSCI Barra's ew constant\n\n:RETURNS:\n\n    float: the sensitivity of the series to the benchmark\n\n\"\"\"\n", "func_signal": "def beta_ew(series, benchmark, theta = 0.94):\n", "code": "span = (1. + theta) / (1. - theta)\nseries_rets = log_returns(series)\nbench_rets = log_returns(benchmark)\n\ncov = pandas.ewmcov(series_rets, \n                    bench_rets,\n                    span = span,\n                    min_periods = span\n)\n\nvar = pandas.ewmvar(bench_rets, \n                    span = span, \n                    min_periods = span\n)\n\nreturn cov.div(var)", "path": "visualize_wealth\\analyze.py", "repo_name": "benjaminmgross/visualize-wealth", "stars": 130, "license": "None", "language": "python", "size": 90096}
{"docstring": "\"\"\"\nReturns the volatility of the returns that are greater than zero\n\n:ARGS:\n\n    series: :class:`pandas.Series` of prices\n\n    freq: ``str`` of frequency, either ``daily, monthly, quarterly, or \n    yearly``\n\n:RETURNS:\n\n    ``float`` of the upside standard deviation\n\"\"\"\n", "func_signal": "def upside_deviation(series, freq = 'daily'):\n", "code": "def _upside_deviation(series, freq = 'daily'):\n    fac = _interval_to_factor(freq)\n    series_rets = log_returns(series)\n    index = series_rets > 0.\n    return series_rets[index].std()*numpy.sqrt(fac)\n\nif isinstance(series, pandas.DataFrame):\n    return series.apply(lambda x: _upside_deviation(x, freq = freq))\nelse:\n    return _upside_deviation(series, freq)", "path": "visualize_wealth\\analyze.py", "repo_name": "benjaminmgross/visualize-wealth", "stars": 130, "license": "None", "language": "python", "size": 90096}
{"docstring": "\"\"\"\nReturns Carol Alexander's calculation for Mean Absolute Tracking \nError (\"MATE\").\n\n\n:ARGS:\n\n    series: ``pandas.Series`` of prices\n\n    benchmark: ``pandas.Series`` to compare ``series`` against\n\n    freq: ``str`` of frequency, either ``daily, monthly, quarterly, \n    or yearly`` \n\n\n:RETURNS:\n\n    ``float`` of the mean absolute tracking error\n    \n.. note:: Why Mean Absolute Tracking Error\n\n    One of the downfalls of \n    `Tracking Error <http://en.wikipedia.org/wiki/Tracking_error>`_ \n    (\"TE\") is that diverging price series that diverge at a constant \n    rate **may** have low TE.  MATE addresses this issue.\n    \n    .. math::\n\n       \\\\sqrt{\\\\frac{(T-1)}{T}\\\\cdot \\\\tau^2 + \\\\bar{R}} \\\\: \n       \\\\textrm{where}\n\n       \\\\tau &= \\\\textrm{Tracking Error} \\\\\\\\\n       \\\\bar{R} &= \\\\textrm{mean of the active returns}\n\n\"\"\"\n", "func_signal": "def mean_absolute_tracking_error(series, benchmark, freq = 'daily'):\n", "code": "def _mean_absolute_tracking_error(series, benchmark, freq = 'daily'):\n    active_rets = active_returns(series = series, \n                                 benchmark = benchmark)\n    N = active_rets.shape[0]\n    return numpy.sqrt((N - 1)/float(N) * tracking_error(\n        series, benchmark, freq)**2 + active_rets.mean()**2)\n\nif isinstance(benchmark, pandas.DataFrame):\n    return benchmark.apply(lambda x: _mean_absolute_tracking_error(\n        series, x, freq = freq))\nelse:\n    return _mean_absolute_tracking_error(series, benchmark, \n                                         freq = freq)", "path": "visualize_wealth\\analyze.py", "repo_name": "benjaminmgross/visualize-wealth", "stars": 130, "license": "None", "language": "python", "size": 90096}
{"docstring": "\"\"\"\nCalculate each asset's contribution to CVaR based on it's \new volatility and correlation to other assets, and it's \ncurrent portfolio weight\n\n:ARGS:\n\n    wts: :class:`Series` of current weights\n\n    prices: :class:`DataFrame` of prices\n\n    alpha: :class:`float` of the cvar parameter\n\n    n_sims: :class:`int` the number of simulations\n\n:RETURNS:\n\n    :class:`pandas.Series` of proportional contribution\n\n.. note:: alternative parameters\n\n    currently the span (for exponentially weighted stats)\n    and phi (for the degrees of freedom of the t-distribution)\n    are not changeable for the function\n\"\"\"\n\n", "func_signal": "def cvar_contrib(wts, prices, alpha = .10, n_sims = 100000):\n", "code": "def _spectral_fun(alpha, n_sims):\n    \n    th = numpy.ceil(alpha*n_sims) # threshold\n    th = int(th) \n    spc = pandas.Series(\n              numpy.zeros([n_sims,])\n    )\n    \n    spc[:th] = 1\n    return spc/spc.sum()\n\nm, n = prices.shape\n\nrets = analyze.log_returns(prices)\ncov = pandas.ewmcov(rets, span = 21., min_periods = 21)\nzs = pandas.Series(numpy.zeros(n,), index = prices.columns)\n\nsims = mvt_rnd(mu = zs, \n               covm = cov.iloc[-1, :, :],\n               phi = 3,\n               n_sim = n_sims\n)\n\npsi = sims.dot(wts)\nspec = _spectral_fun(alpha = alpha, \n                     n_sims = n_sims\n)\n\nsrtd = psi.copy()\nind = psi.argsort()\nsrtd.sort()\n\n# pandas multiplies using indexes, so remove index \n#cvar = srtd[ind].dot(spec.values)\n\nd = {}\n\nfor asset in wts.index:\n    d[asset] = sims.loc[ind, asset].dot(spec.values)\n\nacvar = pandas.Series(d)\ntmp = acvar.mul(wts)\nreturn tmp/tmp.sum()", "path": "visualize_wealth\\analyze.py", "repo_name": "benjaminmgross/visualize-wealth", "stars": 130, "license": "None", "language": "python", "size": 90096}
{"docstring": "\"\"\"\nReturns the `Sortino Ratio \n<http://en.wikipedia.org/wiki/Sortino_ratio>`_, or excess returns \nper unit downside volatility\n\n:ARGS:\n\n    series: ``pandas.Series`` of prices\n\n    freq: ``str`` of either ``daily, monthly, quarterly, or yearly``    \n    indicating the frequency of the data ``default=`` daily\n\n:RETURNS:\n\n    float of the Sortino Ratio\n\n.. note:: Calculating the Sortino Ratio\n\n    There are several calculation methodologies for the Sortino \n    Ratio, this method using downside volatility, where\n    \n    .. math::\n\n        \\\\textrm{Sortino Ratio} = \\\\frac{(R-r_f)}\n        {\\\\sigma_\\\\textrm{downside}}\n\n.. code:: \n\n    import visualize_wealth.performance as vwp\n\n    sortino_ratio = vwp.sortino_ratio(price_series, \n        frequency = 'monthly')\n    \n\"\"\"\n", "func_signal": "def sortino_ratio(series, freq = 'daily', rfr = 0.0):\n", "code": "def _sortino_ratio(series, freq = 'daily'):\n    return annualized_return(series, freq = freq)/downside_deviation(\n        series, freq = freq)\n\nif isinstance(series, pandas.DataFrame):\n    return series.apply(lambda x: _sortino_ratio(x, freq = freq))\nelse:\n    return _sortino_ratio(series, freq = freq)", "path": "visualize_wealth\\analyze.py", "repo_name": "benjaminmgross/visualize-wealth", "stars": 130, "license": "None", "language": "python", "size": 90096}
{"docstring": "\"\"\"\nReturns the `Sharpe Ratio <http://en.wikipedia.org/wiki/Sharpe_ratio>`_ \nof an asset, given a price series, risk free rate of ``rfr``, and \n``frequency`` of the \ntime series\n\n:ARGS:\n\n    series: ``pandas.Series`` of prices\n\n    rfr: ``float`` of the risk free rate\n\n    freq: ``str`` of frequency, either ``daily, monthly, quarterly, or \n    yearly``\n\n:RETURN:\n\n    ``float`` of the Sharpe Ratio\n\n.. note:: Calculating Sharpe \n\n    .. math::\n\n        \\\\textrm{SR} = \\\\frac{(R_p - r_f)}{\\\\sigma} \\\\: \n        \\\\textrm{where},\n\n        R_p &= \\\\textrm{series annualized return} \\\\\\\\\n        r_f &= \\\\textrm{Risk free rate} \\\\\\\\\n        \\\\sigma &= \\\\textrm{Portfolio annualized volatility}\n\n\"\"\"\n", "func_signal": "def sharpe_ratio(series, rfr = 0., freq = 'daily'):\n", "code": "def _sharpe_ratio(series, rfr = 0., freq = 'daily'):\n    return (annualized_return(series, freq) - rfr)/annualized_vol(\n        series, freq)\n\nif isinstance(series, pandas.DataFrame):\n    return series.apply(lambda x: _sharpe_ratio(x, rfr = rfr, freq = freq))\nelse:\n    return _sharpe_ratio(series, rfr = rfr, freq = freq)", "path": "visualize_wealth\\analyze.py", "repo_name": "benjaminmgross/visualize-wealth", "stars": 130, "license": "None", "language": "python", "size": 90096}
{"docstring": "\"\"\"\nGiven a price series and explanatory factors factor_df, determine\nthe weights of attribution to each factor or asset\n\n:ARGS:\n\n    series: :class:`pandas.Series` of asset prices to explain given\n    the factors or sub_classes in factor_df\n\n    factor_df: :class:`pandas.DataFrame` of the prices of the\n    factors or sub_classes to to which the asset prices can be\n    attributed\n\n:RETURNS:\n\n    given an optimal solution, a :class:`pandas.Series` of asset\n    factor weights (summing to one) which best explain the\n    series.  If an optimal solution is not found, None type is\n    returned (with accompanying message)\n\"\"\"\n", "func_signal": "def attribution_weights(series, factor_df):\n", "code": "def obj_fun(weights):\n    tol = 1.e-5\n    est = factor_df.apply(lambda x: numpy.multiply(weights, x),\n                          axis = 1).sum(axis = 1)\n    n = len(series)\n    \n    #when a variable is \"excluded\" reduce p for higher adj-r2\n    p = len(weights[weights > tol])\n    rsq = r2(series = series, benchmark = est)\n    adj_rsq = 1 - (1 - rsq)*(n - 1)/(n - p - 1)\n    return -1.*adj_rsq\n\n#linear returns\nseries = linear_returns(series).dropna()\n\n#if isinstance(series, pandas.DataFrame) & len(series.columns == 1):\n    #it's an n x 1 dataframe with a valid result\n    #series = series[series.columns[0]]\n\nfactor_df = linear_returns(factor_df).dropna()\nguess = numpy.random.rand(factor_df.shape[1])\nguess = pandas.Series(guess/guess.sum(), index = factor_df.columns)\nbounds = [(0., 1.) for i in numpy.arange(len(guess))]\n\nopt_fun = scipy.optimize.minimize(fun = obj_fun, \n                                  x0 = guess,\n                                  bounds = bounds\n)\nopt_wts = pandas.Series(opt_fun.x, index = guess.index)\nopt_wts = opt_wts.div(opt_wts.sum())\nreturn opt_wts", "path": "visualize_wealth\\analyze.py", "repo_name": "benjaminmgross/visualize-wealth", "stars": 130, "license": "None", "language": "python", "size": 90096}
{"docstring": "\"\"\"\nReturns the systematic risk, or the volatility that is directly \nattributable to the benchmark\n\n:ARGS:\n\n    series: ``pandas.Series`` of prices\n\n    benchmark: ``pandas.Series`` to compare ``series`` against\n\n    freq: ``str`` of frequency, either ``daily, monthly, quarterly, or \n    yearly``\n\n:RETURNS:\n\n    ``float`` of the systematic volatility (not variance)\n\n.. note::  Calculating Systematic Risk\n\n    .. math::\n        \\\\sigma_b &= \\\\textrm{Volatility of the Benchmark} \\\\\\\\\n        \\\\sigma^2_{\\\\beta} &= \\\\textrm{Systematic Risk} \\\\\\\\\n        \\\\beta &= \\\\frac{\\\\sigma^2_{s, b}}{\\\\sigma^2_{b}} \\\\: \n        \\\\textrm{then,}\n\n        \\\\sigma^2_{\\\\beta} &= \\\\beta^2 \\\\cdot \\\\sigma^2_{b}\n        \\\\Rightarrow \\\\sigma_{\\\\beta} &= \\\\beta \\\\cdot \\\\sigma_{b}\n\"\"\"\n", "func_signal": "def systematic_risk(series, benchmark, freq = 'daily'):\n", "code": "def _systematic_risk(series, benchmark, freq = 'daily'):\n    bench_rets = log_returns(benchmark)\n    benchmark_vol = annualized_vol(benchmark)\n    return benchmark_vol * beta(series, benchmark)\n\nif isinstance(benchmark, pandas.DataFrame):\n    return benchmark.apply(lambda x: _systematic_risk(series, x, freq))\nelse:\n    return _systematic_risk(series, benchmark, freq)", "path": "visualize_wealth\\analyze.py", "repo_name": "benjaminmgross/visualize-wealth", "stars": 130, "license": "None", "language": "python", "size": 90096}
{"docstring": "'''\nSet the current directory for logging and output.\n'''\n", "func_signal": "def set_output_dir(dirname):\n", "code": "assert os.path.exists(dirname)\n_config.output_dir = dirname\n_setup_logs()", "path": "common\\logutil.py", "repo_name": "hjimce/Depth-Map-Prediction", "stars": 177, "license": "gpl-3.0", "language": "python", "size": 4730}
{"docstring": "'''\nPrints a one-line histogram with a percent in each bin.\nMin and max values are displayed on the ends.\n'''\n", "func_signal": "def hist_bins(x, m=None, M=None, width=50, sep=''):\n", "code": "w = 7\nbins = width / w\n(h, hbins, m, M) = _gethist(x, bins, m, M)\nhstr = sep.join([str(int(np.round(x*100))).center(w-2) for x in h])\nreturn '% .2f ||%s|| %.2f' % (m, hstr, M)", "path": "common\\strhist.py", "repo_name": "hjimce/Depth-Map-Prediction", "stars": 177, "license": "gpl-3.0", "language": "python", "size": 4730}
{"docstring": "'''\nprecomputed means and stdev\n'''\n# just hardcoding for this release, was in meta.mat file\n", "func_signal": "def define_meta(self):\n", "code": "images_mean = 109.31410628\nimages_std = 76.18328376\nimages_istd = 1.0 / images_std\ndepths_mean = 2.53434899\ndepths_std = 1.22576694\ndepths_istd = 1.0 / depths_std\nlogdepths_mean = 0.82473954\nlogdepths_std = 0.45723134\nlogdepths_istd = 1.0 / logdepths_std\nself.meta = MachinePart(locals())", "path": "models\\depth.py", "repo_name": "hjimce/Depth-Map-Prediction", "stars": 177, "license": "gpl-3.0", "language": "python", "size": 4730}
{"docstring": "'''\nSave a matplotlib figure to fn in the current output dir.\nargs same as for pyplot.savefig().\n'''\n", "func_signal": "def save_fig(fn, *args, **kwargs):\n", "code": "with open(fn, 'w') as f:\n    pyplot.savefig(f, *args, **kwargs)", "path": "common\\logutil.py", "repo_name": "hjimce/Depth-Map-Prediction", "stars": 177, "license": "gpl-3.0", "language": "python", "size": 4730}
{"docstring": "'''theano function decorator'''\n", "func_signal": "def theano_function(*vars_by_pos, **kwargs):\n", "code": "mode = kwargs.pop('mode', 'FAST_RUN')\ncheck_valid = kwargs.pop('check_valid', False)\nchecks = kwargs.pop('checks', ())\nvars_by_name = kwargs\ndef compile_func(f):\n    argnames = f.func_code.co_varnames[:f.func_code.co_argcount]\n    if any([a in vars_by_name for a in argnames[:len(vars_by_pos)]]):\n        raise ValueError('Argument supplied twice to %s' % f.func_name)\n    varspec = dict(vars_by_name)\n    varspec.update(zip(argnames[:len(vars_by_pos)], vars_by_pos))\n    argvars = []\n    for name in argnames:\n        spec = varspec[name]\n        if isinstance(spec, (tuple, list)):\n            (var, test_val) = spec\n        else:\n            var = spec\n            test_val = None\n        assert isinstance(var, T.Variable)\n        var.name = name\n        if test_val is not None:\n            var.tag.test_value = test_val\n        argvars.append(var)\n    return function(argvars, f(*argvars),\n                    check_valid=check_valid,\n                    checks=checks,\n                    mode=mode)\nreturn compile_func", "path": "thutil.py", "repo_name": "hjimce/Depth-Map-Prediction", "stars": 177, "license": "gpl-3.0", "language": "python", "size": 4730}
{"docstring": "'''\nInitializes parameters, either from a file or from initialization code\nfor the unit.  This looks for parameters to use in the following\norder (highest precedence first):\n\n* load overrides for debug and interactive sessions\n    1. params_file in unit config\n    2. load_key in [load] config section\n    3. default load dir (\"all\" in [load] config section)\n\n* params saved during training, loaded when resuming a run\n    4. current training state in output\n    5. current output directory\n\n* initializations, loaded once nothing was found for resuming\n    6. load_key in [init] config section\n    7. default init dir (\"all\" in [init] config section)\n\n* initialize by calling unit init code (since no was file specified)\n    8. call unit _init_params()\n'''\n", "func_signal": "def init_params(self, *args, **kwargs):\n", "code": "params_dir = None\nparams_file = None\nfn = self._params_filename()\n\n# first check if a file is explicitly specified in unit config\n# if so, use it (even if it doesn't exist -- that case should error)\ncase = 'in_config'\nparams_file = self.conf.get('params_file', None)\n\n# if not, look in the dir for the load key specified for this unit\nif self.conf.parent.has_section('load'):\n    if params_file is None and self.load_key is not None:\n        case = 'load_key'\n        params_dir = self.conf.parent.get('load', self.load_key, None)\n        params_file = self._check_file(params_dir, fn)\n\n    # then check in the default load dir\n    if params_file is None:\n        case = 'load_default'\n        params_dir = self.conf.parent.get('load', 'all', None)\n        params_file = self._check_file(params_dir, fn)\n\n# check current training state and output dir if the run is resumptive\nif self.conf.parent.getboolean('train', 'resumptive', True):\n    if params_file is None:\n        case = 'resume_current'\n        params_dir = logutil.filename(self.machine.state_dir.current)\n        params_file = self._check_file(params_dir, fn,\n                                       check_state_dir=0)\n\n    if params_file is None:\n        case = 'resume_current'\n        params_dir = logutil.filename(logutil.output_dir())\n        params_file = self._check_file(params_dir, fn,\n                                       check_state_dir=0)\n\n# next, look for initializations by key, then default init\nif self.conf.parent.has_section('init'):\n    if params_file is None and self.load_key is not None:\n        case = 'init_key'\n        params_dir = self.conf.parent.get('init', self.load_key, None)\n        params_file = self._check_file(params_dir, fn)\n\n    if params_file is None:\n        case = 'init_default'\n        params_dir = self.conf.parent.get('init', 'all', None)\n        params_file = self._check_file(params_dir, fn)\n\n# if we did not find a params file, init with _init_params()\nif params_file is None:\n    case = 'none'\n\nkwargs['tie_params'] = self.tie_params\nfor (k, x) in self.tie_params.iteritems():\n    setattr(self, k, x)\n\n# \u53c2\u6570\u52a0\u8f7d \u5982\u679cparams_file\u4e0d\u4e3a None ,\u90a3\u4e48\u6211\u4eec\u5c31\u52a0\u8f7d\u53c2\u6570\u6587\u4ef6\nif params_file is not None:\n    assert case != 'none'\n    self.load_params(params_file)\n    self.loaded = case in ('in_config', 'load_key', 'load_default')\n    self.resumed = case in ('resume_current',)\n    self.init_from_load = case in ('init_key', 'init_default')\n#\u5982\u679c\u4e3aNone\uff0c\u90a3\u4e48\u6211\u4eec\u5c31\u91c7\u7528\u521d\u59cb\u5316\u7684\u65b9\u6cd5\nelse:\n    self.params = []\n    self._init_params(*args, **kwargs)\n    self.loaded = False\n    self.resumed = False\n    self.init_from_load = False", "path": "net.py", "repo_name": "hjimce/Depth-Map-Prediction", "stars": 177, "license": "gpl-3.0", "language": "python", "size": 4730}
{"docstring": "'''\nApplies logsoftmax to x over the given axis (i.e. exp/sum(exp)).\n'''\n", "func_signal": "def logsoftmax(x, axis=None):\n", "code": "if isinstance(axis, int):\n    m = T.max(x, axis=axis, keepdims=True)\nelse:\n    m = T.max(x)\nexp_x = T.exp(x - m)\nZ = T.sum(exp_x, axis=axis, keepdims=True)\nreturn x - m - T.log(Z)", "path": "net.py", "repo_name": "hjimce/Depth-Map-Prediction", "stars": 177, "license": "gpl-3.0", "language": "python", "size": 4730}
{"docstring": "'''\nKeeps 1-p entries of x and zeros out a random subset with prob p\n'''\n", "func_signal": "def random_zero(x, p):\n", "code": "return x * theano_rng.binomial(size=x.shape,\n                               n=1,\n                               p=1-p,\n                               dtype=x.dtype)", "path": "net.py", "repo_name": "hjimce/Depth-Map-Prediction", "stars": 177, "license": "gpl-3.0", "language": "python", "size": 4730}
{"docstring": "'''\nTranspose/Reshape feature maps into (bsize*ni*nj, #feature maps)\n'''\n", "func_signal": "def feature_map_vectors(x):\n", "code": "(bsize, nc, ni, nj) = x.shape\nreturn x.transpose((0,2,3,1)).reshape((bsize*ni*nj, nc))", "path": "net.py", "repo_name": "hjimce/Depth-Map-Prediction", "stars": 177, "license": "gpl-3.0", "language": "python", "size": 4730}
{"docstring": "'''\nPrints a histogram with one bin per line.\n'''\n", "func_signal": "def hist_bars(x, m=None, M=None, bins=10, width=50):\n", "code": "(h, hbins, m, M) = _gethist(x, bins, m, M)\nbarlengths = np.round(width * h / np.maximum(1e-8, np.max(h)))\ns = ['% .3f ~ % .3f | %s' % (hbins[i], hbins[i+1], '*' * barlengths[i])\n     for i in xrange(len(h))]\nreturn '\\n'.join(s)", "path": "common\\strhist.py", "repo_name": "hjimce/Depth-Map-Prediction", "stars": 177, "license": "gpl-3.0", "language": "python", "size": 4730}
{"docstring": "'''color a grayscale array (currently red/blue by sign)'''\n", "func_signal": "def colormap(x, m=None, M=None, center=0, colors=None):\n", "code": "if center is None:\n    center = 0\nif colors is None:\n    colors = np.array(((0, 0.7, 1),\n                       (0,   0, 0),\n                       (1,   0, 0)),\n                      dtype=float)\nif x.shape[-1] == 1:\n    x = x[..., 0]\nx = scale_values(x, min=m, max=M, center=center)\ny = np.empty(x.shape + (3,))\nfor c in xrange(3):\n    y[..., c] = np.interp(x, (0, 0.5, 1), colors[:, c])\nreturn y", "path": "common\\imgutil.py", "repo_name": "hjimce/Depth-Map-Prediction", "stars": 177, "license": "gpl-3.0", "language": "python", "size": 4730}
{"docstring": "'''\nApplies softmax to x over the given axis (i.e. exp/sum(exp)).\n'''\n", "func_signal": "def softmax(x, axis=None):\n", "code": "if isinstance(axis, int):\n    m = T.max(x, axis=axis, keepdims=True)\nelse:\n    m = T.max(x)\nexp_x = T.exp(x - m)\nZ = T.sum(exp_x, axis=axis, keepdims=True)\nreturn exp_x / Z", "path": "net.py", "repo_name": "hjimce/Depth-Map-Prediction", "stars": 177, "license": "gpl-3.0", "language": "python", "size": 4730}
{"docstring": "'''Tiles given images together in a single montage image.\n   imgs is an iterable of (h, w) or (h, w, c) arrays.\n'''\n", "func_signal": "def montage(imgs, layout=None, fill=0, border=0):\n", "code": "sz = imgs[0].shape\nassert all([sz == x.shape for x in imgs])\nif len(sz) == 3:\n    (h, w, c) = sz\nelif len(sz) == 2:\n    (h, w) = sz\n    c = 1\nelse:\n    raise ValueError('images must be 2 or 3 dimensional')\n\nbw = bh = 0\nif border:\n    try:\n        (bh, bw) = border\n    except TypeError:\n        bh = bw = int(border)\nnimgs = len(imgs)\n\nif layout is None:\n    (ncols, nrows) = (None, None)\nelse:\n    (nrows, ncols) = layout\n\nif not (nrows and nrows > 0) and not (ncols and ncols > 0):\n    if w >= h:\n        ncols = np.ceil(np.sqrt(nimgs * h / float(w)))\n        nrows = np.ceil(nimgs / float(ncols))\n    else:\n        nrows = np.ceil(np.sqrt(nimgs * w / float(h)))\n        ncols = np.ceil(nimgs / float(nrows))\nelif not (nrows and nrows > 0):\n    nrows = np.ceil(nimgs / float(ncols))\nelif not (ncols and ncols > 0):\n    ncols = np.ceil(nimgs / float(nrows))\n\nmw = w * ncols + bw * (ncols-1)\nmh = h * nrows + bh * (nrows-1)\nassert mh * mw >= w*h*nimgs, 'layout not big enough to for images'\nM = np.zeros((mh, mw, c))\nM += fill\ni = 0\nj = 0\nfor img in imgs:\n    M[i:i+h, j:j+w, :] = img.reshape((h, w, c))\n    j += w + bw\n    if j >= mw:\n        i += h + bh\n        j = 0\nif len(sz) == 1:\n    M = M.reshape((mh, mw))\nreturn M", "path": "common\\imgutil.py", "repo_name": "hjimce/Depth-Map-Prediction", "stars": 177, "license": "gpl-3.0", "language": "python", "size": 4730}
{"docstring": "# pproc slightly different from imagenet because no cmrnorm\n", "func_signal": "def define_fine_stack(self, x0):\n", "code": "x0_pproc = (x0 - self.meta.images_mean) \\\n           * self.meta.images_istd\n\nconv_s2_1 = self.create_unit('conv_s2_1')\nz_s2_1    = relu(conv_s2_1.infer(x0_pproc))\n\npool_s2_1 = self.create_unit('pool_s2_1')\n(p_s2_1, s_s2_1) = pool_s2_1.infer(z_s2_1)\n\n# concat input features with coarse prediction\n(h, w) = self.output_size\ncoarse_drop = self.coarse.pred_drop.reshape((self.bsize, 1, h, w))\ncoarse_mean = self.coarse.pred_mean.reshape((self.bsize, 1, h, w))\np_1_concat_drop = T.concatenate(\n                      (coarse_drop,\n                       p_s2_1[:, 1:, :, :]),\n                      axis=1)\np_1_concat_mean = T.concatenate(\n                      (coarse_mean,\n                       p_s2_1[:, 1:, :, :]),\n                      axis=1)\n\nconv_s2_2 = self.create_unit('conv_s2_2')\nz_s2_2_drop = relu(conv_s2_2.infer(p_1_concat_drop))\nz_s2_2_mean = relu(conv_s2_2.infer(p_1_concat_mean))\n\nconv_s2_3 = self.create_unit('conv_s2_3')\nz_s2_3_drop = conv_s2_3.infer(z_s2_2_drop)\nz_s2_3_mean = conv_s2_3.infer(z_s2_2_mean)\n\n# prediction\npred_drop = z_s2_3_drop[:,0,:,:]\npred_mean = z_s2_3_mean[:,0,:,:]\n\nself.fine = MachinePart(locals())", "path": "models\\depth.py", "repo_name": "hjimce/Depth-Map-Prediction", "stars": 177, "license": "gpl-3.0", "language": "python", "size": 4730}
{"docstring": "'''\nReturns the region of a 320x240 image covered by the predicted\ndepth map (y0 y1 x0 x1) where y runs the 240-dim and x runs the 320-dim.\n'''\n", "func_signal": "def get_predicted_depth_region(self):\n", "code": "(orig_h, orig_w) = self.orig_input_size # input before transforms\n(input_h, input_w) = self.input_size # input after transforms\ndt = self.target_crop # net output size difference from valid convs\noff_h = (orig_h - input_h + dt) / 2\noff_w = (orig_w - input_w + dt) / 2\nreturn (off_h, off_h + input_h,\n        off_w, off_w + input_w)", "path": "models\\depth.py", "repo_name": "hjimce/Depth-Map-Prediction", "stars": 177, "license": "gpl-3.0", "language": "python", "size": 4730}
{"docstring": "'''Given (i,j) center of filter y placed in x, and shapes (ilen, jlen) of \n   image x and filter y, returns slices for x and y s.t. y gets truncated\n   at x's boundary.  Example:\n   (xbox, ybox) = filter_truncate(i, j, recons.shape, filter.shape)\n   recons[xbox] += k * filter[ybox]\n'''\n", "func_signal": "def filter_truncate(i, j, xshape, yshape):\n", "code": "(xi, xj) = xshape\n(yi, yj) = yshape\n\nxi0 = i - yi//2\nxi1 = i + yi//2 + (int(yi) % 2)\nxj0 = j - yj//2\nxj1 = j + yj//2 + (int(yi) % 2)\nyi0 = 0\nyi1 = yi\nyj0 = 0\nyj1 = yj\n\nif xi0 < 0:\n    yi0 -= xi0\n    xi0 = 0\nif xi1 > xi:\n    yi1 -= (xi1 - xi)\n    xi1 = xi\nif xj0 < 0:\n    yj0 -= xj0\n    xj0 = 0\nif xj1 > xj:\n    yj1 -= (xj1 - xj)\n    xj1 = xj\n\nreturn (boxslice((xi0, xj0), (xi1, xj1)),\n        boxslice((yi0, yj0), (yi1, yj1)))", "path": "common\\imgutil.py", "repo_name": "hjimce/Depth-Map-Prediction", "stars": 177, "license": "gpl-3.0", "language": "python", "size": 4730}
{"docstring": "'''\nCopy a file to the output directory.\n\nIf dst is None, uses basename(src).  Otherwise, dst is the name of the\nfile within the current output directory.\n'''\n", "func_signal": "def copy(src, dst=None):\n", "code": "if dst is None:\n    dst = os.path.basename(src)\ndst = filename(dst)\nif os.path.realpath(src) != os.path.realpath(dst):\n    shutil.copy(src, dst)", "path": "common\\logutil.py", "repo_name": "hjimce/Depth-Map-Prediction", "stars": 177, "license": "gpl-3.0", "language": "python", "size": 4730}
{"docstring": "'''\nInfers depth maps for a list of 320x240 images.\nimages is a nimgs x 240 x 320 x 3 numpy uint8 array.\nreturns depths (nimgs x 55 x 74) corresponding to the center box\nin the original rgb image.\n'''\n", "func_signal": "def infer_depth(self, images):\n", "code": "images = images.transpose((0,3,1,2))\n(nimgs, nc, nh, nw) = images.shape\nassert (nc, nh, nw) == (3, 240, 320)#\u7f51\u7edc\u7684\u8f93\u51fa\u56fe\u7247\u6570\u636e\u4e3a(1,3, 240, 320)\n\n(input_h, input_w) = self.input_size#\u7f51\u7edc\u8f93\u5165feature map \u56fe\u7247\u7684\u5927\u5c0f\n(output_h, output_w) = self.output_size#\u7f51\u7edc\u8f93\u51fafeature map\u5927\u5c0f\n\nbsize = self.bsize\nb = 0\n\n# pred_depth\u4e3a\u8f93\u51fa\uff0cTensor \u7c7b\u578b\u53d8\u91cf\uff0c\nv = self.vars\npred_depth = self.inverse_depth_transform(self.fine.pred_mean)\ninfer_f = theano.function([v.images], pred_depth)\n\ndepths = np.zeros((nimgs, output_h, output_w), dtype=np.float32)\n\n# \u4e00\u5f20\u56fe\u7247\u7684\u4e2d\u5fc3 bbox \uff0c(i0, i1)\u4e3a\u77e9\u5f62\u7684\u5de6\u4e0a\u89d2\u3001(j0, j1)\u4e3a\u77e9\u5f62\u7684\u53f3\u4e0b\u89d2\ndh = nh - input_h\ndw = nw - input_w\n(i0, i1) = (dh/2, nh - dh/2)\n(j0, j1) = (dw/2, nw - dw/2)\n\n# infer depth for images in batches\nb = 0\nwhile b < nimgs:\n    batch = images[b:b+bsize]\n    n = len(batch)\n    if n < bsize:\n        batch = _zero_pad_batch(batch, bsize)\n\n    # crop to network input size\n    batch = batch[:, :, i0:i1, j0:j1]\n\n    # infer depth with nnet\n    depths[b:b+n] = infer_f(batch)[:n]\n    \n    b += n\n\nreturn depths", "path": "models\\depth.py", "repo_name": "hjimce/Depth-Map-Prediction", "stars": 177, "license": "gpl-3.0", "language": "python", "size": 4730}
{"docstring": "'''\nTranspose/Reshape feature map vectors back to xshape == (bsize, nc, ni, nj)\n'''\n", "func_signal": "def feature_map_maps(x, xshape):\n", "code": "(bsize, nc, ni, nj) = xshape\nreturn x.reshape((bsize, ni, nj, nc)).transpose((0,3,1,2))", "path": "net.py", "repo_name": "hjimce/Depth-Map-Prediction", "stars": 177, "license": "gpl-3.0", "language": "python", "size": 4730}
{"docstring": "'''\nBilinearly upsamples x:\n(nimgs, nfeat, h, w) -> (nimgs, nfeat, h*scale, w*scale)\n'''\n", "func_signal": "def upsample_bilinear(x, scale):\n", "code": "kx = np.linspace(0, 1, scale + 1)[1:-1]\nkx = np.concatenate((kx, [1], kx[::-1]))\nker = kx[xx,:] * kx[:, xx]\nker = T.constant(ker[xx,xx,:,:].astype(np.float32))\nxbatch = x.reshape((x.shape[0] * x.shape[1], 1, x.shape[2], x.shape[3]))\nxup = conv(xbatch, ker, 'valid', transpose=True, stride=scale)\nreturn xup.reshape((x.shape[0], x.shape[1], xup.shape[2], xup.shape[3]))", "path": "net.py", "repo_name": "hjimce/Depth-Map-Prediction", "stars": 177, "license": "gpl-3.0", "language": "python", "size": 4730}
{"docstring": "\"\"\"\nEnsures we have a connection to the email server. Returns whether or\nnot a new connection was required (True or False).\n\"\"\"\n", "func_signal": "def open(self):\n", "code": "if self.connection:\n    # Nothing to do if the connection is already open.\n    return False\ntry:\n    # If local_hostname is not specified, socket.getfqdn() gets used.\n    # For performance, we use the cached FQDN for local_hostname.\n    self.connection = smtplib.SMTP(self.host, self.port,\n                                   local_hostname=DNS_NAME.get_fqdn())\n    if self.use_tls:\n        self.connection.ehlo()\n        self.connection.starttls()\n        self.connection.ehlo()\n    if self.username and self.password:\n        self.connection.login(self.username, self.password)\n    return True\nexcept:\n    if not self.fail_silently:\n        raise", "path": "tornado_utils\\send_mail\\backends\\smtp.py", "repo_name": "peterbe/tornado-utils", "stars": 134, "license": "None", "language": "python", "size": 245}
{"docstring": "\"\"\" returns the index of certain set of attributes (of a link) in the\n    self.a list\n \n    If the set of attributes is not found, returns None\n\"\"\"\n", "func_signal": "def previousIndex(self, attrs):\n", "code": "if not has_key(attrs, 'href'): return None\n\ni = -1\nfor a in self.a:\n    i += 1\n    match = 0\n    \n    if has_key(a, 'href') and a['href'] == attrs['href']:\n        if has_key(a, 'title') or has_key(attrs, 'title'):\n                if (has_key(a, 'title') and has_key(attrs, 'title') and\n                    a['title'] == attrs['title']):\n                    match = True\n        else:\n            match = True\n\n    if match: return i", "path": "tornado_utils\\html2text.py", "repo_name": "peterbe/tornado-utils", "stars": 134, "license": "None", "language": "python", "size": 245}
{"docstring": "\"\"\"A helper method that does the actual sending.\"\"\"\n", "func_signal": "def _send(self, email_message):\n", "code": "if not email_message.recipients():\n    return False\ntry:\n    self.connection.sendmail(email_message.from_email,\n            email_message.recipients(),\n            email_message.message().as_string())\nexcept:\n    if not self.fail_silently:\n        raise\n    return False\nreturn True", "path": "tornado_utils\\send_mail\\backends\\smtp.py", "repo_name": "peterbe/tornado-utils", "stars": 134, "license": "None", "language": "python", "size": 245}
{"docstring": "\"\"\"Return the entire formatted message as a string.\nOptional `unixfrom' when True, means include the Unix From_ envelope\nheader.\n\nThis overrides the default as_string() implementation to not mangle\nlines that begin with 'From '. See bug #13433 for details.\n\"\"\"\n", "func_signal": "def as_string(self, unixfrom=False):\n", "code": "fp = StringIO()\ng = Generator(fp, mangle_from_ = False)\ng.flatten(self, unixfrom=unixfrom)\nreturn fp.getvalue()", "path": "tornado_utils\\send_mail\\send_email.py", "repo_name": "peterbe/tornado-utils", "stars": 134, "license": "None", "language": "python", "size": 245}
{"docstring": "\"\"\"Attaches a file from the filesystem.\"\"\"\n", "func_signal": "def attach_file(self, path, mimetype=None):\n", "code": "filename = os.path.basename(path)\ncontent = open(path, 'rb').read()\nself.attach(filename, content, mimetype)", "path": "tornado_utils\\send_mail\\send_email.py", "repo_name": "peterbe/tornado-utils", "stars": 134, "license": "None", "language": "python", "size": 245}
{"docstring": "\"\"\"\nReturns a bytestring version of 's', encoded as specified in 'encoding'.\n\nIf strings_only is True, don't convert (some) non-string-like objects.\n\"\"\"\n", "func_signal": "def smart_str(s, encoding='utf-8', strings_only=False, errors='strict'):\n", "code": "if strings_only and isinstance(s, (types.NoneType, int)):\n    return s\nif isinstance(s, Promise):\n    return unicode(s).encode(encoding, errors)\nelif not isinstance(s, basestring):\n    try:\n        return str(s)\n    except UnicodeEncodeError:\n        if isinstance(s, Exception):\n            # An Exception subclass containing non-ASCII data that doesn't\n            # know how to print itself properly. We shouldn't raise a\n            # further exception.\n            return ' '.join([smart_str(arg, encoding, strings_only,\n                    errors) for arg in s])\n        return unicode(s).encode(encoding, errors)\nelif isinstance(s, unicode):\n    return s.encode(encoding, errors)\nelif s and encoding != 'utf-8':\n    return s.decode('utf-8', errors).encode(encoding, errors)\nelse:\n    return s", "path": "tornado_utils\\send_mail\\send_email.py", "repo_name": "peterbe/tornado-utils", "stars": 134, "license": "None", "language": "python", "size": 245}
{"docstring": "\"\"\"Returns a string suitable for RFC 2822 compliant Message-ID, e.g:\n\n<20020201195627.33539.96671@nightshade.la.mastaler.com>\n\nOptional idstring if given is a string used to strengthen the\nuniqueness of the message id.\n\"\"\"\n", "func_signal": "def make_msgid(idstring=None):\n", "code": "timeval = time.time()\nutcdate = time.strftime('%Y%m%d%H%M%S', time.gmtime(timeval))\ntry:\n    pid = os.getpid()\nexcept AttributeError:\n    # No getpid() in Jython, for example.\n    pid = 1\nrandint = random.randrange(100000)\nif idstring is None:\n    idstring = ''\nelse:\n    idstring = '.' + idstring\nidhost = DNS_NAME\nmsgid = '<%s.%s.%s%s@%s>' % (utcdate, pid, randint, idstring, idhost)\nreturn msgid", "path": "tornado_utils\\send_mail\\send_email.py", "repo_name": "peterbe/tornado-utils", "stars": 134, "license": "None", "language": "python", "size": 245}
{"docstring": "\"\"\"Attach an alternative content representation.\"\"\"\n", "func_signal": "def attach_alternative(self, content, mimetype):\n", "code": "assert content is not None\nassert mimetype is not None\nself.alternatives.append((content, mimetype))", "path": "tornado_utils\\send_mail\\send_email.py", "repo_name": "peterbe/tornado-utils", "stars": 134, "license": "None", "language": "python", "size": 245}
{"docstring": "\"\"\"Return true if the line does only consist of whitespace characters.\"\"\"\n", "func_signal": "def onlywhite(line):\n", "code": "for c in line:\n    if c is not ' ' and c is not '  ':\n        return c is ' '\nreturn line", "path": "tornado_utils\\html2text.py", "repo_name": "peterbe/tornado-utils", "stars": 134, "license": "None", "language": "python", "size": 245}
{"docstring": "\"\"\"\nConverts the content, mimetype pair into a MIME attachment object.\n\"\"\"\n", "func_signal": "def _create_mime_attachment(self, content, mimetype):\n", "code": "basetype, subtype = mimetype.split('/', 1)\nif basetype == 'text':\n    encoding = self.encoding or 'utf-8'\n    attachment = SafeMIMEText(smart_str(content, encoding), subtype, encoding)\nelse:\n    # Encode non-text attachments with base64.\n    attachment = MIMEBase(basetype, subtype)\n    attachment.set_payload(content)\n    Encoders.encode_base64(attachment)\nreturn attachment", "path": "tornado_utils\\send_mail\\send_email.py", "repo_name": "peterbe/tornado-utils", "stars": 134, "license": "None", "language": "python", "size": 245}
{"docstring": "#attrs = fixattrs(attrs)\n    \n", "func_signal": "def handle_tag(self, tag, attrs, start):\n", "code": "if hn(tag):\n    self.p()\n    if start: self.o(hn(tag)*\"#\" + ' ')\n\nif tag in ['p', 'div']: self.p()\n\nif tag == \"br\" and start: self.o(\"  \\n\")\n\nif tag == \"hr\" and start:\n    self.p()\n    self.o(\"* * *\")\n    self.p()\n\nif tag in [\"head\", \"style\", 'script']: \n    if start: self.quiet += 1\n    else: self.quiet -= 1\n\nif tag in [\"body\"]:\n    self.quiet = 0 # sites like 9rules.com never close <head>\n\nif tag == \"blockquote\":\n    if start: \n        self.p(); self.o('> ', 0, 1); self.start = 1\n        self.blockquote += 1\n    else:\n        self.blockquote -= 1\n        self.p()\n\nif tag in ['em', 'i', 'u']: self.o(\"_\")\nif tag in ['strong', 'b']: self.o(\"**\")\nif tag == \"code\" and not self.pre: self.o('`') #TODO: `` `this` ``\nif tag == \"abbr\":\n    if start:\n        attrsD = {}\n        for (x, y) in attrs: attrsD[x] = y\n        attrs = attrsD\n        \n        self.abbr_title = None\n        self.abbr_data = ''\n        if has_key(attrs, 'title'):\n            self.abbr_title = attrs['title']\n    else:\n        if self.abbr_title != None:\n            self.abbr_list[self.abbr_data] = self.abbr_title\n            self.abbr_title = None\n        self.abbr_data = ''\n\nif tag == \"a\":\n    if start:\n        attrsD = {}\n        for (x, y) in attrs: attrsD[x] = y\n        attrs = attrsD\n        if has_key(attrs, 'href') and not (SKIP_INTERNAL_LINKS and attrs['href'].startswith('#')): \n            self.astack.append(attrs)\n            self.o(\"[\")\n        else:\n            self.astack.append(None)\n    else:\n        if self.astack:\n            a = self.astack.pop()\n            if a:\n                i = self.previousIndex(a)\n                if i is not None:\n                    a = self.a[i]\n                else:\n                    self.acount += 1\n                    a['count'] = self.acount\n                    a['outcount'] = self.outcount\n                    self.a.append(a)\n                self.o(\"][\" + str(a['count']) + \"]\")\n\nif tag == \"img\" and start:\n    attrsD = {}\n    for (x, y) in attrs: attrsD[x] = y\n    attrs = attrsD\n    if has_key(attrs, 'src'):\n        attrs['href'] = attrs['src']\n        alt = attrs.get('alt', '')\n        i = self.previousIndex(attrs)\n        if i is not None:\n            attrs = self.a[i]\n        else:\n            self.acount += 1\n            attrs['count'] = self.acount\n            attrs['outcount'] = self.outcount\n            self.a.append(attrs)\n        self.o(\"![\")\n        self.o(alt)\n        self.o(\"][\"+ str(attrs['count']) +\"]\")\n\nif tag == 'dl' and start: self.p()\nif tag == 'dt' and not start: self.pbr()\nif tag == 'dd' and start: self.o('    ')\nif tag == 'dd' and not start: self.pbr()\n\nif tag in [\"ol\", \"ul\"]:\n    if start:\n        self.list.append({'name':tag, 'num':0})\n    else:\n        if self.list: self.list.pop()\n    \n    self.p()\n\nif tag == 'li':\n    if start:\n        self.pbr()\n        if self.list: li = self.list[-1]\n        else: li = {'name':'ul', 'num':0}\n        self.o(\"  \"*len(self.list)) #TODO: line up <ol><li>s > 9 correctly.\n        if li['name'] == \"ul\": self.o(\"* \")\n        elif li['name'] == \"ol\":\n            li['num'] += 1\n            self.o(str(li['num'])+\". \")\n        self.start = 1\n    else:\n        self.pbr()\n\nif tag in [\"table\", \"tr\"] and start: self.p()\nif tag == 'td': self.pbr()\n\nif tag == \"pre\":\n    if start:\n        self.startpre = 1\n        self.pre = 1\n    else:\n        self.pre = 0\n    self.p()", "path": "tornado_utils\\html2text.py", "repo_name": "peterbe/tornado-utils", "stars": 134, "license": "None", "language": "python", "size": 245}
{"docstring": "\"\"\"Write all messages to the stream in a thread-safe way.\"\"\"\n", "func_signal": "def send_messages(self, email_messages):\n", "code": "if not email_messages:\n    return\nself._lock.acquire()\ntry:\n    # The try-except is nested to allow for\n    # Python 2.4 support (Refs #12147)\n    try:\n        stream_created = self.open()\n        for message in email_messages:\n            self.stream.write('%s\\n' % message.message().as_string())\n            self.stream.write('-'*79)\n            self.stream.write('\\n')\n            self.stream.flush()  # flush after each message\n        if stream_created:\n            self.close()\n    except:\n        if not self.fail_silently:\n            raise\nfinally:\n    self._lock.release()\nreturn len(email_messages)", "path": "tornado_utils\\send_mail\\backends\\console.py", "repo_name": "peterbe/tornado-utils", "stars": 134, "license": "None", "language": "python", "size": 245}
{"docstring": "\"\"\"\nConverts the filename, content, mimetype triple into a MIME attachment\nobject.\n\"\"\"\n", "func_signal": "def _create_attachment(self, filename, content, mimetype=None):\n", "code": "if mimetype is None:\n    mimetype, _ = mimetypes.guess_type(filename)\n    if mimetype is None:\n        mimetype = DEFAULT_ATTACHMENT_MIME_TYPE\nattachment = self._create_mime_attachment(content, mimetype)\nif filename:\n    attachment.add_header('Content-Disposition', 'attachment',\n                          filename=filename)\nreturn attachment", "path": "tornado_utils\\send_mail\\send_email.py", "repo_name": "peterbe/tornado-utils", "stars": 134, "license": "None", "language": "python", "size": 245}
{"docstring": "\"\"\"Sends the email message.\"\"\"\n", "func_signal": "def send(self, fail_silently=False):\n", "code": "if not self.recipients():\n    # Don't bother creating the network connection if there's nobody to\n    # send to.\n    return 0\nreturn self.get_connection(fail_silently).send_messages([self])", "path": "tornado_utils\\send_mail\\send_email.py", "repo_name": "peterbe/tornado-utils", "stars": 134, "license": "None", "language": "python", "size": 245}
{"docstring": "\"\"\"Wrap all paragraphs in the provided text.\"\"\"\n", "func_signal": "def optwrap(text):\n", "code": "if not BODY_WIDTH:\n    return text\n\nassert wrap, \"Requires Python 2.3.\"\nresult = ''\nnewlines = 0\nfor para in text.split(\"\\n\"):\n    if len(para) > 0:\n        if para[0] != ' ' and para[0] != '-' and para[0] != '*':\n            for line in wrap(para, BODY_WIDTH):\n                result += line + \"\\n\"\n            result += \"\\n\"\n            newlines = 2\n        else:\n            if not onlywhite(para):\n                result += para + \"\\n\"\n                newlines = 1\n    else:\n        if newlines < 2:\n            result += \"\\n\"\n            newlines += 1\nreturn result", "path": "tornado_utils\\html2text.py", "repo_name": "peterbe/tornado-utils", "stars": 134, "license": "None", "language": "python", "size": 245}
{"docstring": "#from django.core.mail import get_connection\n", "func_signal": "def get_connection(self, fail_silently=False):\n", "code": "if not self.connection:\n    raise NotImplementedError\n    #self.connection = get_connection(fail_silently=fail_silently)\nreturn self.connection", "path": "tornado_utils\\send_mail\\send_email.py", "repo_name": "peterbe/tornado-utils", "stars": 134, "license": "None", "language": "python", "size": 245}
{"docstring": "\"\"\"\nSends one or more EmailMessage objects and returns the number of email\nmessages sent.\n\"\"\"\n", "func_signal": "def send_messages(self, email_messages):\n", "code": "if not email_messages:\n    return\nself._lock.acquire()\ntry:\n    new_conn_created = self.open()\n    if not self.connection:\n        # We failed silently on open().\n        # Trying to send would be pointless.\n        return\n    num_sent = 0\n    for message in email_messages:\n        sent = self._send(message)\n        if sent:\n            num_sent += 1\n    if new_conn_created:\n        self.close()\nfinally:\n    self._lock.release()\nreturn num_sent", "path": "tornado_utils\\send_mail\\backends\\smtp.py", "repo_name": "peterbe/tornado-utils", "stars": 134, "license": "None", "language": "python", "size": 245}
{"docstring": "\"\"\"\nAttaches a file with the given filename and content. The filename can\nbe omitted and the mimetype is guessed, if not provided.\n\nIf the first parameter is a MIMEBase subclass it is inserted directly\ninto the resulting message attachments.\n\"\"\"\n", "func_signal": "def attach(self, filename=None, content=None, mimetype=None):\n", "code": "if isinstance(filename, MIMEBase):\n    assert content == mimetype == None\n    self.attachments.append(filename)\nelse:\n    assert content is not None\n    self.attachments.append((filename, content, mimetype))", "path": "tornado_utils\\send_mail\\send_email.py", "repo_name": "peterbe/tornado-utils", "stars": 134, "license": "None", "language": "python", "size": 245}
{"docstring": "\"\"\"gets called when we class decorate\"\"\"\n", "func_signal": "def __call__(self, _handler):\n", "code": "name = self.name and self.name or _handler.__name__\nself._routes.append(tornado.web.url(self._uri, _handler, name=name))\nreturn _handler", "path": "tornado_utils\\routes.py", "repo_name": "peterbe/tornado-utils", "stars": 134, "license": "None", "language": "python", "size": 245}
{"docstring": "\"\"\"Forbids multi-line headers, to prevent header injection.\"\"\"\n", "func_signal": "def forbid_multi_line_headers(name, val, encoding):\n", "code": "encoding = encoding or 'utf-8'\nval = force_unicode(val, encoding)\nif '\\n' in val or '\\r' in val:\n    raise BadHeaderError(\"Header values can't contain newlines (got %r for header %r)\" % (val, name))\ntry:\n    val = val.encode('ascii')\nexcept UnicodeEncodeError:\n    if name.lower() in ('to', 'from', 'cc'):\n        result = []\n        for nm, addr in getaddresses((val,)):\n            nm = str(Header(nm.encode(encoding), encoding))\n            result.append(formataddr((nm, str(addr))))\n        val = ', '.join(result)\n    else:\n        val = Header(val.encode(encoding), encoding)\nelse:\n    if name.lower() == 'subject':\n        val = Header(val)\nreturn name, val", "path": "tornado_utils\\send_mail\\send_email.py", "repo_name": "peterbe/tornado-utils", "stars": 134, "license": "None", "language": "python", "size": 245}
{"docstring": "\"\"\"\nKNN binary Classification\n\"\"\"\n", "func_signal": "def performKNNClass(X_train, y_train, X_test, y_test):\n", "code": "clf = neighbors.KNeighborsClassifier()\nclf.fit(X_train, y_train)\naccuracy = clf.score(X_test, y_test)\n#auc = roc_auc_score(y_test, clf.predict(X_test))\nreturn accuracy", "path": "functions.py", "repo_name": "FraPochetti/StocksProject", "stars": 239, "license": "None", "language": "python", "size": 1456}
{"docstring": "\"\"\"\nremoves punctuation, stopwords and returns lowercase text in a list of single words\n\"\"\"\n", "func_signal": "def cleanText(text):\n", "code": "text = text.lower()    \n\nfrom bs4 import BeautifulSoup\ntext = BeautifulSoup(text).get_text()\n\nfrom nltk.tokenize import RegexpTokenizer\ntokenizer = RegexpTokenizer(r'\\w+')\ntext = tokenizer.tokenize(text)\n\nfrom nltk.corpus import stopwords\nclean = [word for word in text if word not in stopwords.words('english')]\n\nreturn clean", "path": "SentimentAnalysis\\Sentiment.py", "repo_name": "FraPochetti/StocksProject", "stars": 239, "license": "None", "language": "python", "size": 1456}
{"docstring": "\"\"\"\ndownloads stock which is gonna be the output of prediciton\n\"\"\"\n", "func_signal": "def getStock(symbol, start, end):\n", "code": "out =  pd.io.data.get_data_yahoo(symbol, start, end)\n\nout.columns.values[-1] = 'AdjClose'\nout.columns = out.columns + '_Out'\nout['Return_Out'] = out['AdjClose_Out'].pct_change()\nreturn out", "path": "functions.py", "repo_name": "FraPochetti/StocksProject", "stars": 239, "license": "None", "language": "python", "size": 1456}
{"docstring": "\"\"\"\nBagging Regression\n\"\"\"\n  \n", "func_signal": "def performBaggingReg(train, test, features, output):\n", "code": "clf = BaggingRegressor()\nclf.fit(train[features], train[output])\nPredicted = clf.predict(test[features])\n\nplt.plot(test[output])\nplt.plot(Predicted, color='red')\nplt.show()        \n\nreturn mean_squared_error(test[output],Predicted), r2_score(test[output], Predicted)", "path": "functions.py", "repo_name": "FraPochetti/StocksProject", "stars": 239, "license": "None", "language": "python", "size": 1456}
{"docstring": "\"\"\"\ncounts negative and positive words in cleantext and returns a score accordingly\n\"\"\"\n", "func_signal": "def getSentiment(cleantext, negative, positive):\n", "code": "positive = loadPositive()\nnegative = loadNegative()\nreturn (countPos(cleantext, positive) - countNeg(cleantext, negative))", "path": "SentimentAnalysis\\Sentiment.py", "repo_name": "FraPochetti/StocksProject", "stars": 239, "license": "None", "language": "python", "size": 1456}
{"docstring": "\"\"\"\nGradient Boosting Regression\n\"\"\"\n\n", "func_signal": "def performGradBoostReg(train, test, features, output):\n", "code": "clf = GradientBoostingRegressor()\nclf.fit(test[features], train[output])\nPredicted = clf.predict(test[features])\n\nplt.plot(test[output])\nplt.plot(Predicted, color='red')\nplt.show()    \n\nreturn mean_squared_error(test[output],Predicted), r2_score(test[output], Predicted)", "path": "functions.py", "repo_name": "FraPochetti/StocksProject", "stars": 239, "license": "None", "language": "python", "size": 1456}
{"docstring": "\"\"\"\nGradient Tree Boosting binary Classification\n\"\"\"\n", "func_signal": "def performQDAClass(X_train, y_train, X_test, y_test):\n", "code": "clf = QDA()\nclf.fit(X_train, y_train)\naccuracy = clf.score(X_test, y_test)\n#auc = roc_auc_score(y_test, clf.predict(X_test))\nreturn accuracy", "path": "functions.py", "repo_name": "FraPochetti/StocksProject", "stars": 239, "license": "None", "language": "python", "size": 1456}
{"docstring": "\"\"\"\nloading positive dictionary\n\"\"\"\n#C:\\Users\\Fra\\Dropbox\\DSR\\StocksProject\\LoughranMcDonald_Positive.csv\n#/home/francesco/Dropbox/DSR/StocksProject/LoughranMcDonald_Positive.csv\n", "func_signal": "def loadPositive():\n", "code": "myfile = open('/home/francesco/Dropbox/DSR/StocksProject/LoughranMcDonald_Positive.csv', \"r\")\npositives = myfile.readlines()\npositive = [pos.strip().lower() for pos in positives]\nreturn positive", "path": "SentimentAnalysis\\Sentiment.py", "repo_name": "FraPochetti/StocksProject", "stars": 239, "license": "None", "language": "python", "size": 1456}
{"docstring": "\"\"\"\ntakes as argument a list of datasets (cleaned using the function prepareToConcat\nto be merged row wise in order to recreate one single data frame for a financial blog\nin case more csv files were produced for single blog\n\"\"\"\n", "func_signal": "def recreateUniqueBlogDataset(listOfFiles):\n", "code": "df = pd.read_csv(listOfFiles[0], parse_dates=['date'])    \nfor data in listOfFiles[1:]:\n    second = pd.read_csv(data, parse_dates=['date'])        \n    df = df.append(second)\ndf = df.drop_duplicates(subset = ['text'])\ndf = df.drop('text', 1)\ndf = df.dropna()\ndf = df.groupby(['date']).mean()\nname = re.search( r'/(\\w+).csv', listOfFiles[0])\ndf.columns.values[0] = name.group(1)\nreturn df", "path": "SentimentAnalysis\\Sentiment.py", "repo_name": "FraPochetti/StocksProject", "stars": 239, "license": "None", "language": "python", "size": 1456}
{"docstring": "\"\"\"\noperates on two columns of dataframe:\n- n >= 2\n- given Return_* computes the return of day i respect to day i-n. \n- given AdjClose_* computes its moving average on n days\n\n\"\"\"\n\n", "func_signal": "def addFeatures(dataframe, adjclose, returns, n):\n", "code": "return_n = adjclose[9:] + \"Time\" + str(n)\ndataframe[return_n] = dataframe[adjclose].pct_change(n)\n\nroll_n = returns[7:] + \"RolMean\" + str(n)\ndataframe[roll_n] = pd.rolling_mean(dataframe[returns], n)", "path": "functions.py", "repo_name": "FraPochetti/StocksProject", "stars": 239, "license": "None", "language": "python", "size": 1456}
{"docstring": "\"\"\"\ncounts negative words in cleantext\n\"\"\"\n", "func_signal": "def countNeg(cleantext, negative):\n", "code": "negs = [word for word in cleantext if word in negative]\nreturn len(negs)", "path": "SentimentAnalysis\\Sentiment.py", "repo_name": "FraPochetti/StocksProject", "stars": 239, "license": "None", "language": "python", "size": 1456}
{"docstring": "\"\"\"\ncounts negative words in cleantext\n\"\"\"\n", "func_signal": "def countPos(cleantext, positive):\n", "code": "pos = [word for word in cleantext if word in positive]\nreturn len(pos)", "path": "SentimentAnalysis\\Sentiment.py", "repo_name": "FraPochetti/StocksProject", "stars": 239, "license": "None", "language": "python", "size": 1456}
{"docstring": "\"\"\"\nRandom Forest Regression\n\"\"\"\n\n", "func_signal": "def performRFReg(train, test, features, output):\n", "code": "forest = RandomForestRegressor(n_estimators=100, n_jobs=-1)\nforest = forest.fit(train[features], train[output])\nPredicted = forest.predict(test[features])\n\n\nplt.plot(test[output])\nplt.plot(Predicted, color='red')\nplt.show()        \n\nreturn mean_squared_error(test[output], Predicted), r2_score(test[output], Predicted)", "path": "functions.py", "repo_name": "FraPochetti/StocksProject", "stars": 239, "license": "None", "language": "python", "size": 1456}
{"docstring": "\"\"\"\nload a csv file and gets a score for the day\n\"\"\"\n", "func_signal": "def prepareToConcat(filename):\n", "code": "df = pd.read_csv(filename, parse_dates=['date'])\ndf = df.drop('text', 1)\ndf = df.dropna()\ndf = df.groupby(['date']).mean()\nname = re.search( r'/(\\w+).csv', filename)\ndf.columns.values[0] = name.group(1)\nreturn df", "path": "SentimentAnalysis\\Sentiment.py", "repo_name": "FraPochetti/StocksProject", "stars": 239, "license": "None", "language": "python", "size": 1456}
{"docstring": "\"\"\"\ngenerates categorical to be predicted column, attach to dataframe \nand label the categories\n\"\"\"\n", "func_signal": "def prepareDataForClassification(dataset, start_test):\n", "code": "le = preprocessing.LabelEncoder()\n\ndataset['UpDown'] = dataset['Return_Out']\ndataset.UpDown[dataset.UpDown >= 0] = 'Up'\ndataset.UpDown[dataset.UpDown < 0] = 'Down'\ndataset.UpDown = le.fit(dataset.UpDown).transform(dataset.UpDown)\n\nfeatures = dataset.columns[1:-1]\nX = dataset[features]    \ny = dataset.UpDown    \n\nX_train = X[X.index < start_test]\ny_train = y[y.index < start_test]    \n\nX_test = X[X.index >= start_test]    \ny_test = y[y.index >= start_test]\n\nreturn X_train, y_train, X_test, y_test", "path": "functions.py", "repo_name": "FraPochetti/StocksProject", "stars": 239, "license": "None", "language": "python", "size": 1456}
{"docstring": "\"\"\"\nSVM binary Classification\n\"\"\"\n", "func_signal": "def performSVMClass(X_train, y_train, X_test, y_test):\n", "code": "clf = SVC()\nclf.fit(X_train, y_train)\naccuracy = clf.score(X_test, y_test)\n#auc = roc_auc_score(y_test, clf.predict(X_test))\nreturn accuracy", "path": "functions.py", "repo_name": "FraPochetti/StocksProject", "stars": 239, "license": "None", "language": "python", "size": 1456}
{"docstring": "\"\"\"\nSVM Regression\n\"\"\"\n\n", "func_signal": "def performSVMReg(train, test, features, output):\n", "code": "clf = SVR()\nclf.fit(train[features], train[output])\nPredicted = clf.predict(test[features])\n\nplt.plot(test[output])\nplt.plot(Predicted, color='red')\nplt.show()        \n\nreturn mean_squared_error(test[output],Predicted), r2_score(test[output], Predicted)", "path": "functions.py", "repo_name": "FraPochetti/StocksProject", "stars": 239, "license": "None", "language": "python", "size": 1456}
{"docstring": "\"\"\"\napply time lag to return columns selected according  to delta.\nDays to lag are contained in the lads list passed as argument.\nReturns a NaN free dataset obtained cutting the lagged dataset\nat head and tail\n\"\"\"\n\n", "func_signal": "def applyTimeLag(dataset, lags, delta, back, target):\n", "code": "if target == 'CLASSIFICATION':\n    maxLag = max(lags)\n\n    columns = dataset.columns[::(2*max(delta)-1)]\n    for column in columns:\n        for lag in lags:\n            newcolumn = column + str(lag)\n            dataset[newcolumn] = dataset[column].shift(lag)\n\n    return dataset.iloc[maxLag:-1,:]", "path": "functions.py", "repo_name": "FraPochetti/StocksProject", "stars": 239, "license": "None", "language": "python", "size": 1456}
{"docstring": "\"\"\"\nreads a json file and returns a clean pandas data frame\n\"\"\"\n", "func_signal": "def readJson(filename):\n", "code": "import pandas as pd\ndf = pd.read_json(filename)\n\ndef unlist(element):\n    return ''.join(element)\n\nfor column in df.columns:\n    df[column] = df[column].apply(unlist)\n\nif filename == '/home/francesco/BigData/Project/ritho.json':\n    def getCorrectDate(wrongdate):\n        mon_day_year = re.search( r'(\\w+) (\\d+)\\w+, (\\d+)', wrongdate)\n        month, day, year = mon_day_year.group(1), mon_day_year.group(2), mon_day_year.group(3)\n        return month + ' ' + day + ' ' + year\n        \n    df['date'] = df['date'].apply(getCorrectDate)\n    df['date'] = pd.to_datetime(df['date'])\nelse:\n    df['date'] = df['date'].apply(lambda x: x[:10])\n    df['date'] = pd.to_datetime(df['date'])\n\ndf = df.drop_duplicates(subset = ['keywords'])\ndf = df.sort(columns='date')\n#df = df.set_index('date')\ndf['text'] = df['keywords'] + df['body'] \n\ndf = df.drop('body', 1)\ndf = df.drop('keywords', 1)\n\nreturn df", "path": "SentimentAnalysis\\Sentiment.py", "repo_name": "FraPochetti/StocksProject", "stars": 239, "license": "None", "language": "python", "size": 1456}
{"docstring": "\"\"\"\nmerges datasets in the list \n\"\"\"\n", "func_signal": "def mergeDataframes(datasets, index, target):\n", "code": "subset = []\nsubset = [dataset.iloc[:, index:] for dataset in datasets[1:]]\n\nif target == 'CLASSIFICATION':    \n    return datasets[0].iloc[:, index:].join(subset, how = 'outer')\n#elif target == 'REGRESSION':\n#    return datasets[0].iloc[:, index:].join(subset, how = 'outer')", "path": "functions.py", "repo_name": "FraPochetti/StocksProject", "stars": 239, "license": "None", "language": "python", "size": 1456}
{"docstring": "''' returns a result set of github audit results '''\n", "func_signal": "def get_gh_audit_result(oid):\n", "code": "client = connect_mongo()\ncol = client['github_audit']['results']\ntry:\n    c = col.find({'_id': bson.ObjectId(oid)})\n    return c[0]\nexcept:\n    return {'string': 'Not Found'}", "path": "viewer_process\\ghcc\\libs\\utils.py", "repo_name": "jfalken/github_commit_crawler", "stars": 164, "license": "None", "language": "python", "size": 1753}
{"docstring": "''' return count of all results '''\n", "func_signal": "def get_gh_count():\n", "code": "client = connect_mongo()\ncol = client['github_audit']['results']\nreturn col.count()", "path": "viewer_process\\ghcc\\libs\\utils.py", "repo_name": "jfalken/github_commit_crawler", "stars": 164, "license": "None", "language": "python", "size": 1753}
{"docstring": "''' disables service lock; call before process restarts '''\n", "func_signal": "def disable_lock():\n", "code": "client = MongoClient()\ndb = client['github_audit']\ncol = db['lock']\ncol.update({'type': 'lock'},\n           {'type': 'lock',\n            'status': 'not_locked'},\n           upsert=True)\nreturn", "path": "viewer_process\\ghcc\\libs\\utils.py", "repo_name": "jfalken/github_commit_crawler", "stars": 164, "license": "None", "language": "python", "size": 1753}
{"docstring": "# set the config upon first load\n", "func_signal": "def index():\n", "code": "c = ConfigChanger('/ghcc_process/config/config.yaml')\nif not c.config_file_ok():\n    return redirect('/config')\n\nreturn redirect('/ghaudit?s=0&l=100')", "path": "viewer_process\\ghcc\\views.py", "repo_name": "jfalken/github_commit_crawler", "stars": 164, "license": "None", "language": "python", "size": 1753}
{"docstring": "''' return a list of github user objects in org_name '''\n", "func_signal": "def get_github_org_users(ghapi, org_name):\n", "code": "try:\n    org = ghapi.get_organization(org_name)\n    return [i for i in org.get_members()]\nexcept:\n    raise Exception('Error getting Github users: %s'\n                    % str(sys.exc_info()[1]))\npass", "path": "ghcc_process\\libs\\github_audit_libs.py", "repo_name": "jfalken/github_commit_crawler", "stars": 164, "license": "None", "language": "python", "size": 1753}
{"docstring": "''' wrapper for a generic authenticated GET request using github api '''\n", "func_signal": "def _ghapi_get(ghapi, url, method='GET'):\n", "code": "try:\n    resp = ghapi._Github__requester.requestJson('GET', url)\n    return {'resp_code': resp[0],\n            'headers': resp[1],\n            'content': json.loads(resp[2])}\nexcept:\n    raise Exception('Cannot get url \"%s\": %s'\n                    % (url, str(sys.exc_info()[1])))", "path": "ghcc_process\\libs\\github_audit_libs.py", "repo_name": "jfalken/github_commit_crawler", "stars": 164, "license": "None", "language": "python", "size": 1753}
{"docstring": "''' disables service lock '''\n", "func_signal": "def disable_lock():\n", "code": "db = _connect_mongo()\ncol = db['lock']\ncol.update({'type': 'lock'},\n           {'type': 'lock',\n            'status': 'not_locked'},\n           upsert=True)\nreturn", "path": "ghcc_process\\libs\\mongo_utils.py", "repo_name": "jfalken/github_commit_crawler", "stars": 164, "license": "None", "language": "python", "size": 1753}
{"docstring": "''' updates the audit event tracking collection\n    :param audit_event: an audit event object\n'''\n", "func_signal": "def audit_event_update(audit_event):\n", "code": "event_unique_id = '%s-%s' % (audit_event.actor.login, str(audit_event.id))\ndb = _connect_mongo()\ncol = db['history']\ncol.ensure_index('uid')\noid = col.insert({'uid': event_unique_id})\nreturn oid", "path": "ghcc_process\\libs\\mongo_utils.py", "repo_name": "jfalken/github_commit_crawler", "stars": 164, "license": "None", "language": "python", "size": 1753}
{"docstring": "''' inserts result records into mongodb\n    :param results: a list of result dicts, from `do_audit_events`\n\n    returns object id of the insert\n'''\n", "func_signal": "def mdb_insert_results(results):\n", "code": "db = _connect_mongo()\ncol = db['results']\ncol.ensure_index('uid')\ncol.ensure_index('matched')\noid = col.insert(results)\nreturn oid", "path": "ghcc_process\\libs\\mongo_utils.py", "repo_name": "jfalken/github_commit_crawler", "stars": 164, "license": "None", "language": "python", "size": 1753}
{"docstring": "''' returns a list of lines from the log file, chronologically sorted\n    this was added to easily read logs when in docker; this may not work\n    in other implementations depending on where your log file is\n'''\n", "func_signal": "def get_log(logfile_path='ghcc.log'):\n", "code": "try:\n    logf = open(logfile_path, 'r').readlines()\n    logf = logf[::-1]  # reverse for newest on top\n    logf = logf[:1000]  # limit the output\n    return [i.strip() for i in logf]\nexcept:\n    return []", "path": "viewer_process\\ghcc\\libs\\utils.py", "repo_name": "jfalken/github_commit_crawler", "stars": 164, "license": "None", "language": "python", "size": 1753}
{"docstring": "''' parses input_string to see if it contains AWS credentials\n    returns boolean if set of matching regexes are found\n'''\n# regexes from http://blogs.aws.amazon.com/security/blog/tag/key+rotation\n", "func_signal": "def _parse_for_aws_creds(input_string):\n", "code": "ak_pat = '(?<![A-Z0-9])[A-Z0-9]{20}(?![A-Z0-9])'\nsk_pat = '(?<![A-Za-z0-9/+=])[A-Za-z0-9/+=]{40}(?![A-Za-z0-9/+=])'\nif re.search(ak_pat, input_string):\n    if re.search(sk_pat, input_string):\n        return True\nreturn False", "path": "ghcc_process\\libs\\github_audit_libs.py", "repo_name": "jfalken/github_commit_crawler", "stars": 164, "license": "None", "language": "python", "size": 1753}
{"docstring": "''' returns a result set of github audit results '''\n", "func_signal": "def get_gh_audit_results(limit, skip=0):\n", "code": "client = connect_mongo()\ncol = client['github_audit']['results']\nc = col.find().sort('audit_date', -1).skip(skip).limit(limit)\nreturn [i for i in c]", "path": "viewer_process\\ghcc\\libs\\utils.py", "repo_name": "jfalken/github_commit_crawler", "stars": 164, "license": "None", "language": "python", "size": 1753}
{"docstring": "''' connects to mongodb and returns client object '''\n", "func_signal": "def connect_mongo():\n", "code": "client = MongoClient()\nreturn client", "path": "viewer_process\\ghcc\\libs\\utils.py", "repo_name": "jfalken/github_commit_crawler", "stars": 164, "license": "None", "language": "python", "size": 1753}
{"docstring": "''' audits event(s). returns results object '''\n", "func_signal": "def do_audit_event(audit_event, ghapi):\n", "code": "results = []\nif audit_event.type == 'PushEvent':\n    results += _parse_push_event(audit_event, ghapi)\nif len(results) == 0:\n    return None\nelse:\n    return results", "path": "ghcc_process\\libs\\github_audit_libs.py", "repo_name": "jfalken/github_commit_crawler", "stars": 164, "license": "None", "language": "python", "size": 1753}
{"docstring": "''' returns a mongodb collection to github database '''\n", "func_signal": "def _connect_mongo():\n", "code": "client = MongoClient()\nreturn client['github_audit']", "path": "ghcc_process\\libs\\mongo_utils.py", "repo_name": "jfalken/github_commit_crawler", "stars": 164, "license": "None", "language": "python", "size": 1753}
{"docstring": "''' parses a push event payload and returns a dict of (source, txt)\n    of the files modified in that push event '''\n", "func_signal": "def _parse_push_event(event, ghapi):\n", "code": "results = []\naudit_dt = datetime.datetime.utcnow()\ntry:\n    # get all the commit(s)\n    event_unique_id = '%s-%s' % (event.actor.login, str(event.id))\n    author = event.actor.login\n    for commit in event.payload['commits']:\n        # unique id for tracking\n        # get the details\n        resp = _ghapi_get(ghapi, commit['url'])\n        content = resp['content']\n        # review the patches\n        for f in content['files']:\n            if 'patch' in f:\n                patch = f['patch']\n                # look for keyword matches\n                matches = _parse_keywords(patch)\n                if len(matches) > 0:\n                    for m in matches:\n                        results.append({\n                                       'audit_date': audit_dt,\n                                       'blob'      : f['blob_url'],\n                                       'string'    : m['string'],\n                                       'matched'   : m['matched_word'],\n                                       'commit_url': commit['url'],\n                                       'author'    : author,\n                                       'uid'       : event_unique_id,\n                                       'html_url'  : content['html_url']})\nexcept:\n    pass\n    # print str(sys.exc_info())\nreturn results", "path": "ghcc_process\\libs\\github_audit_libs.py", "repo_name": "jfalken/github_commit_crawler", "stars": 164, "license": "None", "language": "python", "size": 1753}
{"docstring": "''' checks to see if an audit event has already been processed\n    :param audit_event: an audit_event object\n\n    returns bool\n'''\n", "func_signal": "def audit_event_already_done(audit_event):\n", "code": "event_unique_id = '%s-%s' % (audit_event.actor.login, str(audit_event.id))\n\ndb = _connect_mongo()\ncol = db['history']\nc = col.find({'uid': event_unique_id})\nif c.count() > 0:\n    return True\nelse:\n    return False", "path": "ghcc_process\\libs\\mongo_utils.py", "repo_name": "jfalken/github_commit_crawler", "stars": 164, "license": "None", "language": "python", "size": 1753}
{"docstring": "''' parses input_string for keywords and aws credentials '''\n", "func_signal": "def _parse_keywords(input_string):\n", "code": "keywords = KEYWORDS\nmatches = []\nfor kw in keywords:\n    if kw.lower() in input_string.lower():\n        match = {'string'      : input_string,\n                 'matched_word': kw}\n        matches.append(match)\nif _parse_for_aws_creds(input_string):\n    matches.append({'string'      : input_string,\n                    'matched_word': 'AWS Credential'})\nreturn matches", "path": "ghcc_process\\libs\\github_audit_libs.py", "repo_name": "jfalken/github_commit_crawler", "stars": 164, "license": "None", "language": "python", "size": 1753}
{"docstring": "''' enabled service lock '''\n", "func_signal": "def enable_lock():\n", "code": "db = _connect_mongo()\ncol = db['lock']\ncol.update({'type': 'lock'},\n           {'type': 'lock',\n            'status': 'locked'},\n           upsert = True)\nreturn", "path": "ghcc_process\\libs\\mongo_utils.py", "repo_name": "jfalken/github_commit_crawler", "stars": 164, "license": "None", "language": "python", "size": 1753}
{"docstring": "''' returns latest 1000 lines of log file, chronologically '''\n", "func_signal": "def get_proc_log(self, name):\n", "code": "info = self.get_procs_info()\ntry:\n    assert name in [i['name'] for i in info]\nexcept:\n    return\nlog = self.p.supervisor.readProcessStdoutLog(name, 0, 0)\nreturn log.split('\\n')[::-1][:1000]", "path": "viewer_process\\ghcc\\libs\\superxmlrpc.py", "repo_name": "jfalken/github_commit_crawler", "stars": 164, "license": "None", "language": "python", "size": 1753}
{"docstring": "\"\"\"Computes the geometric average over the first dimension of a matrix.\n\"\"\"\n# Convert to log domain\n", "func_signal": "def norm_geometric_average(x, weights=None, eps=1e-7):\n", "code": "x_log = np.log(x + eps)\n# Compute the mean\ngeom_av_log = np.average(x_log, weights=weights, axis=0)\n# Go back to normal domain and renormalise\ngeom_av_log = geom_av_log - np.max(geom_av_log)\ngeom_av = np.exp(geom_av_log)\nreturn geom_av / geom_av.sum()", "path": "utils.py", "repo_name": "317070/kaggle-heart", "stars": 157, "license": "mit", "language": "python", "size": 1683}
{"docstring": "\"\"\"\nThis wrapper function is faster than skimage.transform.warp\n\"\"\"\n", "func_signal": "def fast_warp(img, tf, output_shape=(50, 50), mode='constant', order=1):\n", "code": "m = tf.params # tf._matrix is\nreturn skimage.transform._warps_cy._warp_fast(img, m, output_shape=output_shape, mode=mode, order=order)", "path": "image_transform.py", "repo_name": "317070/kaggle-heart", "stars": 157, "license": "mit", "language": "python", "size": 1683}
{"docstring": "\"\"\"\nestimating the correct rescaling transform is slow, so just use the\ndownscale_factor to define a transform directly. This probably isn't\n100% correct, but it shouldn't matter much in practice.\n\"\"\"\n", "func_signal": "def build_rescale_transform(downscale_factor, image_shape, target_shape):\n", "code": "rows, cols = image_shape\ntrows, tcols = target_shape\ntform_ds = skimage.transform.AffineTransform(scale=(downscale_factor, downscale_factor))\n\n# centering\nshift_x = cols / (2.0 * downscale_factor) - tcols / 2.0\nshift_y = rows / (2.0 * downscale_factor) - trows / 2.0\ntform_shift_ds = skimage.transform.SimilarityTransform(translation=(shift_x, shift_y))\nreturn tform_shift_ds + tform_ds", "path": "image_transform.py", "repo_name": "317070/kaggle-heart", "stars": 157, "license": "mit", "language": "python", "size": 1683}
{"docstring": "\"\"\"Computes the geometric average over the first dimension of a matrix.\n\"\"\"\n# Convert to log domain\n", "func_signal": "def geometric_average(x, eps=1e-7):\n", "code": "x_log = np.log(x+eps)\n# Compute the mean\ngeom_av_log = np.mean(x_log, axis=0)\n# Go back to normal domain and renormalise\ngeom_av_log = geom_av_log\ngeom_av = np.exp(geom_av_log)\nreturn geom_av", "path": "utils.py", "repo_name": "317070/kaggle-heart", "stars": 157, "license": "mit", "language": "python", "size": 1683}
{"docstring": "# don't use patients who don't have mre than 6 slices\n", "func_signal": "def filter_samples(folders):\n", "code": "return [\n    folder for folder in folders\n    if data_loader.compute_nr_slices(folder) > 6]", "path": "configurations\\je_ospre_joniscale64small_360.py", "repo_name": "317070/kaggle-heart", "stars": 157, "license": "mit", "language": "python", "size": 1683}
{"docstring": "# l2 regu on certain layers\n", "func_signal": "def build_objective(interface_layers):\n", "code": "l2_penalty = nn.regularization.regularize_layer_params_weighted(\n    interface_layers[\"regularizable\"], nn.regularization.l2)\n# build objective\nreturn objectives.RMSEObjective(interface_layers[\"outputs\"], penalty=l2_penalty)", "path": "configurations\\j4_iranet2.py", "repo_name": "317070/kaggle-heart", "stars": 157, "license": "mit", "language": "python", "size": 1683}
{"docstring": "\"\"\"\ncreate a (600, ) array which is linear weighted around the desired value\n:param value:\n:return:\n\"\"\"\n", "func_signal": "def linear_weighted(value):\n", "code": "n = np.arange(600, dtype='float32')\ndist = np.abs(n-value)\nnormed = dist / np.mean(dist)\nreturn normed", "path": "utils.py", "repo_name": "317070/kaggle-heart", "stars": 157, "license": "mit", "language": "python", "size": 1683}
{"docstring": "\"\"\"\nGroups slices into stacks with the same image orientation\n:param slice_stack:\n:return: list of slice stacks\n\"\"\"\n", "func_signal": "def group_slices(slice_stack):\n", "code": "img_orientations = []\nfor s in slice_stack:\n    img_orientations.append(tuple(s['metadata']['ImageOrientationPatient']))\nimg_orientations = list(set(img_orientations))\nif len(img_orientations) == 1:\n    return [slice_stack]\nelse:\n    slice_groups = [[] for _ in xrange(len(img_orientations))]\n    for s in slice_stack:\n        group = img_orientations.index(tuple(s['metadata']['ImageOrientationPatient']))\n        slice_groups[group].append(s)\n    return slice_groups", "path": "generate_roi_pkl.py", "repo_name": "317070/kaggle-heart", "stars": 157, "license": "mit", "language": "python", "size": 1683}
{"docstring": "# l2 regu on certain layers\n", "func_signal": "def build_objective(interface_layers):\n", "code": "l2_penalty = nn.regularization.regularize_layer_params_weighted(\n    interface_layers[\"regularizable\"], nn.regularization.l2)\n# build objective\nreturn objectives.KaggleObjective(interface_layers[\"outputs\"], penalty=l2_penalty)", "path": "configurations\\j6_2ch_128mm.py", "repo_name": "317070/kaggle-heart", "stars": 157, "license": "mit", "language": "python", "size": 1683}
{"docstring": "\"\"\"Randomly selects elements.\n\nIf there are not enough elements, repetition is allowed.\n\"\"\"\n# Expand untill there is enough repetition\n", "func_signal": "def pick_random(arr, no_picks):\n", "code": "arr_to_pick_from = arr\nwhile len(arr_to_pick_from) < no_picks:\n    arr_to_pick_from += arr\n# Pick\nrandom.shuffle(arr_to_pick_from)\nreturn arr_to_pick_from[:no_picks]", "path": "utils.py", "repo_name": "317070/kaggle-heart", "stars": 157, "license": "mit", "language": "python", "size": 1683}
{"docstring": "\"\"\"\nscale is a DOWNSCALING factor.\n\"\"\"\n", "func_signal": "def perturb_rescaled_fixed(img, scale, tform_augment, target_shape=(50, 50)):\n", "code": "tform_rescale = build_rescale_transform(scale, img.shape, target_shape) # also does centering\n\ntform_center, tform_uncenter = build_center_uncenter_transforms(img.shape)\ntform_augment = tform_uncenter + tform_augment + tform_center # shift to center, augment, shift back (for the rotation/shearing)\nreturn fast_warp(img, tform_rescale + tform_augment, output_shape=target_shape, mode='constant').astype('float32')", "path": "image_transform.py", "repo_name": "317070/kaggle-heart", "stars": 157, "license": "mit", "language": "python", "size": 1683}
{"docstring": "# l2 regu on certain layers\n", "func_signal": "def build_objective(interface_layers):\n", "code": "l2_penalty = nn.regularization.regularize_layer_params_weighted(\n    interface_layers[\"regularizable\"], nn.regularization.l2)\n# build objective\nreturn objectives.KaggleObjective(interface_layers[\"outputs\"], penalty=l2_penalty)", "path": "configurations\\j6_sax_96.py", "repo_name": "317070/kaggle-heart", "stars": 157, "license": "mit", "language": "python", "size": 1683}
{"docstring": "\"\"\"Shifts the center of the image to a given location.\n\nThis function tries to include as much as possible of the image in the patch\ncentered around the new center. If the patch arount the ideal center\nlocation doesn't fit within the image, we shift the center to the right so\nthat it does.\n\"\"\"\n", "func_signal": "def build_shift_center_transform(image_shape, center_location, patch_size):\n", "code": "center_absolute_location = [\n    center_location[0]*image_shape[1], center_location[1]*image_shape[0]]\n\n# Check for overlap at the edges\ncenter_absolute_location[0] = max(\n    center_absolute_location[0], patch_size[1]/2.0)\ncenter_absolute_location[1] = max(\n    center_absolute_location[1], patch_size[0]/2.0)\ncenter_absolute_location[0] = min(\n    center_absolute_location[0], image_shape[1] - patch_size[1]/2.0)\ncenter_absolute_location[1] = min(\n    center_absolute_location[1], image_shape[0] - patch_size[0]/2.0)\n\n# Check for overlap at both edges\nif patch_size[0] > image_shape[0]:\n    center_absolute_location[1] = image_shape[0] / 2.0\nif patch_size[1] > image_shape[1]:\n    center_absolute_location[0] = image_shape[1] / 2.0\n\n# Build transform\nnew_center = np.array(center_absolute_location)\ntranslation_center = new_center - 0.5\ntranslation_uncenter = -np.array((patch_size[1]/2.0, patch_size[0]/2.0)) - 0.5\nreturn (\n    skimage.transform.SimilarityTransform(translation=translation_center),\n    skimage.transform.SimilarityTransform(translation=translation_uncenter))", "path": "image_transform.py", "repo_name": "317070/kaggle-heart", "stars": 157, "license": "mit", "language": "python", "size": 1683}
{"docstring": "\"\"\"\nThese are used to ensure that zooming and rotation happens around the center of the image.\nUse these transforms to center and uncenter the image around such a transform.\n\"\"\"\n", "func_signal": "def build_center_uncenter_transforms(image_shape):\n", "code": "center_shift = np.array([image_shape[1], image_shape[0]]) / 2.0 - 0.5 # need to swap rows and cols here apparently! confusing!\ntform_uncenter = skimage.transform.SimilarityTransform(translation=-center_shift)\ntform_center = skimage.transform.SimilarityTransform(translation=center_shift)\nreturn tform_center, tform_uncenter", "path": "image_transform.py", "repo_name": "317070/kaggle-heart", "stars": 157, "license": "mit", "language": "python", "size": 1683}
{"docstring": "# l2 regu on certain layers\n", "func_signal": "def build_objective(interface_layers):\n", "code": "l2_penalty = nn.regularization.regularize_layer_params_weighted(\n    interface_layers[\"regularizable\"], nn.regularization.l2)\n# build objective\nreturn objectives.KaggleObjective(interface_layers[\"outputs\"], penalty=l2_penalty)", "path": "configurations\\je_ss_smcrps_nrmsc_500_dropoutput_test.py", "repo_name": "317070/kaggle-heart", "stars": 157, "license": "mit", "language": "python", "size": 1683}
{"docstring": "# l2 regu on certain layers\n", "func_signal": "def build_objective(interface_layers):\n", "code": "l2_penalty = nn.regularization.regularize_layer_params_weighted(\n    interface_layers[\"regularizable\"], nn.regularization.l2)\n# build objective\nreturn objectives.KaggleObjective(interface_layers[\"outputs\"], penalty=l2_penalty)", "path": "configurations\\je_ss_jonisc80_leaky_gauss.py", "repo_name": "317070/kaggle-heart", "stars": 157, "license": "mit", "language": "python", "size": 1683}
{"docstring": "# Check if already cleaned\n", "func_signal": "def clean_metadata(metadatadict):\n", "code": "if _is_clean(metadatadict):\n    return metadatadict\n# Do cleaning\nkeys = sorted(list(metadatadict.keys()))\nfor key in keys:\n    value = metadatadict[key]\n    if key == 'PatientAge':\n        metadatadict[key] = int(value[:-1])\n    if key == 'PatientSex':\n        metadatadict[key] = 1 if value == 'F' else -1\n    else:\n        if isinstance(value, Sequence):\n            #convert to list\n            value = [i for i in value]\n        if isinstance(value, (list,)):\n            metadatadict[key] = [convert_to_number(i) for i in value]\n        else:\n            metadatadict[key] = convert_to_number(value)\n_tag_clean(metadatadict)\nreturn metadatadict", "path": "utils.py", "repo_name": "317070/kaggle-heart", "stars": 157, "license": "mit", "language": "python", "size": 1683}
{"docstring": "# # DEBUG: draw a border to see where the image ends up\n# img[0, :] = 0.5\n# img[-1, :] = 0.5\n# img[:, 0] = 0.5\n# img[:, -1] = 0.5\n", "func_signal": "def perturb(img, augmentation_params, target_shape=(50, 50), rng=np.random):\n", "code": "tform_centering = build_centering_transform(img.shape, target_shape)\ntform_center, tform_uncenter = build_center_uncenter_transforms(img.shape)\ntform_augment = random_perturbation_transform(rng=rng, **augmentation_params)\ntform_augment = tform_uncenter + tform_augment + tform_center # shift to center, augment, shift back (for the rotation/shearing)\nreturn fast_warp(img, tform_centering + tform_augment, output_shape=target_shape, mode='constant').astype('float32')", "path": "image_transform.py", "repo_name": "317070/kaggle-heart", "stars": 157, "license": "mit", "language": "python", "size": 1683}
{"docstring": "# l2 regu on certain layers\n", "func_signal": "def build_objective(interface_layers):\n", "code": "l2_penalty = nn.regularization.regularize_layer_params_weighted(\n    interface_layers[\"regularizable\"], nn.regularization.l2)\n# build objective\nreturn objectives.KaggleObjective(interface_layers[\"outputs\"], penalty=l2_penalty)", "path": "configurations\\je_ospre_joniscale64small_360.py", "repo_name": "317070/kaggle-heart", "stars": 157, "license": "mit", "language": "python", "size": 1683}
{"docstring": "\"\"\"\nscale is a DOWNSCALING factor.\n\"\"\"\n", "func_signal": "def perturb_rescaled(img, scale, augmentation_params, target_shape=(50, 50), rng=np.random):\n", "code": "tform_rescale = build_rescale_transform(scale, img.shape, target_shape) # also does centering\ntform_center, tform_uncenter = build_center_uncenter_transforms(img.shape)\ntform_augment = random_perturbation_transform(rng=rng, **augmentation_params)\ntform_augment = tform_uncenter + tform_augment + tform_center # shift to center, augment, shift back (for the rotation/shearing)\nreturn fast_warp(img, tform_rescale + tform_augment, output_shape=target_shape, mode='constant').astype('float32')", "path": "image_transform.py", "repo_name": "317070/kaggle-heart", "stars": 157, "license": "mit", "language": "python", "size": 1683}
{"docstring": "\"\"\"\nUpdate a given field in the database.\n\"\"\"\n", "func_signal": "def update_database(self, field, value):\n", "code": "ret = True\nif self.database:\n    try:\n        cur = self.database.cursor()\n        cur.execute(\"UPDATE image SET \" + field + \"='\" + value +\n                    \"' WHERE id=%s\", (self.tag, ))\n        self.database.commit()\n    except BaseException:\n        ret = False\n        traceback.print_exc()\n        self.database.rollback()\n    finally:\n        if cur:\n            cur.close()\nreturn ret", "path": "lib\\extractor.py", "repo_name": "misterch0c/firminator_backend", "stars": 196, "license": "None", "language": "python", "size": 34300}
{"docstring": "#there should be only one ip\n", "func_signal": "def buildConfig(brif, iface, vlans, macs):\n", "code": "ip = brif[1]\nbr = brif[0]\n\n#strip vlanid from interface name (e.g., eth2.2 -> eth2)\ndev = iface.split(\".\")[0]\n\n#check whether there is a different mac set\nmac = None\nd = dict(macs)\nif br in d:\n    mac = d[br]\nelif dev in d:\n    mac = d[dev]\n\nvlan_id = None\nif len(vlans):\n    vlan_id = vlans[0]\n\nreturn (ip, dev, vlan_id, mac)", "path": "scripts\\makeNetwork.py", "repo_name": "misterch0c/firminator_backend", "stars": 196, "license": "None", "language": "python", "size": 34300}
{"docstring": "# hashes ... all the hashes in the tar file\n", "func_signal": "def getOids(objs, cur):\n", "code": "hashes = [x[1] for x in objs]\nhashes_str = \",\".join([\"\"\"'%s'\"\"\" % x for x in hashes])\nquery = \"\"\"SELECT id,hash FROM object WHERE hash IN (%s)\"\"\"\ncur.execute(query % hashes_str)\nres = [(int(x), y) for (x, y) in cur.fetchall()]\n\nexistingHashes = [x[1] for x in res]\n\nmissingHashes = set(hashes).difference(set(existingHashes))\n\nnewObjs = createObjects(missingHashes, cur)\n\nres += newObjs\n\nresult = dict([(y, x) for (x, y) in res])\nreturn result", "path": "scripts\\tar2db.py", "repo_name": "misterch0c/firminator_backend", "stars": 196, "license": "None", "language": "python", "size": 34300}
{"docstring": "\"\"\"\nPerform extraction of firmware updates from input to tarballs in output\ndirectory using a thread pool.\n\"\"\"\n", "func_signal": "def extract(self):\n", "code": "if os.path.isdir(self._input):\n    for path, _, files in os.walk(self._input):\n        for item in files:\n            self._list.append(os.path.join(path, item))\nelif os.path.isfile(self._input):\n    self._list.append(self._input)\n\nif self.output_dir and not os.path.isdir(self.output_dir):\n    os.makedirs(self.output_dir)\n\nif self._pool:\n    self._pool.map(self._extract_item, self._list)\nelse:\n    for item in self._list:\n        self._extract_item(item)", "path": "lib\\extractor.py", "repo_name": "misterch0c/firminator_backend", "stars": 196, "license": "None", "language": "python", "size": 34300}
{"docstring": "\"\"\"\nAttempts to find a Linux root directory.\n\"\"\"\n\n# Recurse into single directory chains, e.g. jffs2-root/fs_1/.../\n", "func_signal": "def io_find_rootfs(start, recurse=True):\n", "code": "path = start\nwhile (len(os.listdir(path)) == 1 and\n       os.path.isdir(os.path.join(path, os.listdir(path)[0]))):\n    path = os.path.join(path, os.listdir(path)[0])\n\n# count number of unix-like directories\ncount = 0\nfor subdir in os.listdir(path):\n    if subdir in Extractor.UNIX_DIRS and \\\n        os.path.isdir(os.path.join(path, subdir)):\n        count += 1\n\n# check for extracted filesystem, otherwise update queue\nif count >= Extractor.UNIX_THRESHOLD:\n    return (True, path)\n\n# in some cases, multiple filesystems may be extracted, so recurse to\n# find best one\nif recurse:\n    for subdir in os.listdir(path):\n        if os.path.isdir(os.path.join(path, subdir)):\n            res = Extractor.io_find_rootfs(os.path.join(path, subdir),\n                                           False)\n            if res[0]:\n                return res\n\nreturn (False, start)", "path": "lib\\extractor.py", "repo_name": "misterch0c/firminator_backend", "stars": 196, "license": "None", "language": "python", "size": 34300}
{"docstring": "\"\"\"\nUnified implementation for checking both \"archive\" and \"compressed\"\nitems.\n\"\"\"\n", "func_signal": "def _check_recursive(self, fmt):\n", "code": "desc = None\n# perform extraction\nfor module in binwalk.scan(self.item, \"-e\", \"-r\", \"-y\", fmt,\n                           signature=True, quiet=True):\n    for entry in module.results:\n        # skip cpio/initrd files since they should be included with\n        # kernel\n        # if \"cpio archive\" in entry.description:\n        #     self.printf(\">> Skipping: cpio: %s\" % entry.description)\n        #     self.terminate = True\n        #     return True\n        desc = entry.description\n        self.printf(\">>>> %s\" % entry.description)\n        break\n\n    if module.extractor.directory:\n        unix = Extractor.io_find_rootfs(module.extractor.directory)\n\n        # check for extracted filesystem, otherwise update queue\n        if unix[0]:\n            self.printf(\">>>> Found Linux filesystem in %s!\" % unix[1])\n            if self.output:\n                shutil.make_archive(self.output, \"gztar\",\n                                    root_dir=unix[1])\n            else:\n                self.extractor.do_rootfs = False\n            return True\n        else:\n            count = 0\n            self.printf(\">> Recursing into %s ...\" % fmt)\n            for root, _, files in os.walk(module.extractor.directory):\n                # sort both descending alphabetical and increasing\n                # length\n                files.sort()\n                files.sort(key=len)\n\n                # handle case where original file name is restored; put\n                # it to front of queue\n                if desc and \"original file name:\" in desc:\n                    orig = None\n                    for stmt in desc.split(\",\"):\n                        if \"original file name:\" in stmt:\n                            orig = stmt.split(\"\\\"\")[1]\n                    if orig and orig in files:\n                        files.remove(orig)\n                        files.insert(0, orig)\n\n                for filename in files:\n                    if count > ExtractionItem.RECURSION_BREADTH:\n                        self.printf(\">> Skipping: recursion breadth %d\"\\\n                            % ExtractionItem.RECURSION_BREADTH)\n                        self.terminate = True\n                        return True\n                    else:\n                        new_item = ExtractionItem(self.extractor,\n                                                  os.path.join(root,\n                                                               filename),\n                                                  self.depth + 1,\n                                                  self.tag)\n                        if new_item.extract():\n                            # check that we are actually done before\n                            # performing early termination. for example,\n                            # we might decide to skip on one subitem,\n                            # but we still haven't finished\n                            if self.update_status():\n                                return True\n                    count += 1\nreturn False", "path": "lib\\extractor.py", "repo_name": "misterch0c/firminator_backend", "stars": 196, "license": "None", "language": "python", "size": 34300}
{"docstring": "#lines = data.split(\"\\r\\n\")\n", "func_signal": "def findNonLoInterfaces(data, endianness):\n", "code": "lines = stripTimestamps(data)\ncandidates = filter(lambda l: l.startswith(\"__inet_insert_ifa\"), lines) # logs for the inconfig process\nif debug:\n    print(\"Candidate ifaces: %r\" % candidates)\nresult = []\nif endianness == \"eb\":\n    fmt = \">I\"\nelif endianness == \"el\":\n    fmt = \"<I\"\nfor c in candidates:\n    g = re.match(r\"^__inet_insert_ifa\\[[^\\]]+\\]: device:([^ ]+) ifa:0x([0-9a-f]+)\", c)\n    if g:\n        (iface, addr) = g.groups()\n        addr = socket.inet_ntoa(struct.pack(fmt, int(addr, 16)))\n        if addr != \"127.0.0.1\" and addr != \"0.0.0.0\":\n            result.append((iface, addr))\nreturn result", "path": "scripts\\makeNetwork.py", "repo_name": "misterch0c/firminator_backend", "stars": 196, "license": "None", "language": "python", "size": 34300}
{"docstring": "\"\"\"\nIf this file contains a kernel version string, assume it is a kernel.\nOnly Linux kernels are currently extracted.\n\"\"\"\n", "func_signal": "def _check_kernel(self):\n", "code": "if not self.get_kernel_status():\n    for module in binwalk.scan(self.item, \"-y\", \"kernel\",\n                               signature=True, quiet=True):\n        for entry in module.results:\n            if \"kernel version\" in entry.description:\n                self.update_database(\"kernel_version\",\n                                     entry.description)\n                if \"Linux\" in entry.description:\n                    if self.get_kernel_path():\n                        shutil.copy(self.item, self.get_kernel_path())\n                    else:\n                        self.extractor.do_kernel = False\n                    self.printf(\">>>> %s\" % entry.description)\n                    return True\n                # VxWorks, etc\n                else:\n                    self.printf(\">>>> Ignoring: %s\" % entry.description)\n                    return False\n        return False\nreturn False", "path": "lib\\extractor.py", "repo_name": "misterch0c/firminator_backend", "stars": 196, "license": "None", "language": "python", "size": 34300}
{"docstring": "\"\"\"\nEliminate attributes that should not be pickled.\n\"\"\"\n", "func_signal": "def __getstate__(self):\n", "code": "self_dict = self.__dict__.copy()\ndel self_dict[\"_pool\"]\ndel self_dict[\"_list\"]\nreturn self_dict", "path": "lib\\extractor.py", "repo_name": "misterch0c/firminator_backend", "stars": 196, "license": "None", "language": "python", "size": 34300}
{"docstring": "\"\"\"\nGiven a path to a target file, extract size bytes from specified offset\nto given output file.\n\"\"\"\n", "func_signal": "def io_dd(indir, offset, size, outdir):\n", "code": "if not size:\n    return\n\nwith open(indir, \"rb\") as ifp:\n    with open(outdir, \"wb\") as ofp:\n        ifp.seek(offset, 0)\n        ofp.write(ifp.read(size))", "path": "lib\\extractor.py", "repo_name": "misterch0c/firminator_backend", "stars": 196, "license": "None", "language": "python", "size": 34300}
{"docstring": "\"\"\"\nIf this file contains a known filesystem type, extract it.\n\"\"\"\n\n", "func_signal": "def _check_rootfs(self):\n", "code": "if not self.get_rootfs_status():\n    for module in binwalk.scan(self.item, \"-e\", \"-r\", \"-y\",\n                               \"filesystem\", signature=True,\n                               quiet=True):\n        for entry in module.results:\n            self.printf(\">>>> %s\" % entry.description)\n            break\n\n        if module.extractor.directory:\n            unix = Extractor.io_find_rootfs(module.extractor.directory)\n\n            if not unix[0]:\n                self.printf(\">>>> Extraction failed!\")\n                return False\n\n            self.printf(\">>>> Found Linux filesystem in %s!\" % unix[1])\n            if self.output:\n                shutil.make_archive(self.output, \"gztar\",root_dir=unix[1])\n            else:\n                self.extractor.do_rootfs = False\n            return True\nreturn False", "path": "lib\\extractor.py", "repo_name": "misterch0c/firminator_backend", "stars": 196, "license": "None", "language": "python", "size": 34300}
{"docstring": "\"\"\"\nPerform the actual extraction of firmware updates, recursively. Returns\nTrue if extraction complete, otherwise False.\n\"\"\"\n", "func_signal": "def extract(self):\n", "code": "self.printf(\"\\n\" + self.item.encode(\"utf-8\", \"replace\").decode(\"utf-8\"))\n\n# check if item is complete\nif self.get_status():\n    self.printf(\">> Skipping: completed!\")\n    return True\n\n# check if exceeding recursion depth\nif self.depth > ExtractionItem.RECURSION_DEPTH:\n    self.printf(\">> Skipping: recursion depth %d\" % self.depth)\n    return self.get_status()\n\n# check if checksum is in visited set\nself.printf(\">> MD5: %s\" % self.checksum)\nwith Extractor.visited_lock:\n    if self.checksum in self.extractor.visited:\n        self.printf(\">> Skipping: %s...\" % self.checksum)\n        return self.get_status()\n    else:\n        self.extractor.visited.add(self.checksum)\n\n# check if filetype is blacklisted\nif self._check_blacklist():\n    return self.get_status()\n\n# create working directory\nself.temp = tempfile.mkdtemp()\n\ntry:\n    self.printf(\">> Tag: %s\" % self.tag)\n    self.printf(\">> Temp: %s\" % self.temp)\n    self.printf(\">> Status: Kernel: %s, Rootfs: %s, Do_Kernel: %s, \\\n        Do_Rootfs: %s\" % (self.get_kernel_status(),\n                          self.get_rootfs_status(),\n                          self.extractor.do_kernel,\n                          self.extractor.do_rootfs))\n\n    for analysis in [self._check_archive, self._check_firmware,\n                     self._check_kernel, self._check_rootfs,\n                     self._check_compressed]:\n        # Move to temporary directory so binwalk does not write to input\n        os.chdir(self.temp)\n\n        # Update status only if analysis changed state\n        if analysis():\n            if self.update_status():\n                self.printf(\">> Skipping: completed!\")\n                return True\n\nexcept Exception:\n    traceback.print_exc()\n\nreturn False", "path": "lib\\extractor.py", "repo_name": "misterch0c/firminator_backend", "stars": 196, "license": "None", "language": "python", "size": 34300}
{"docstring": "\"\"\"\nPerforms file magic while maintaining compatibility with different\nlibraries.\n\"\"\"\n\n", "func_signal": "def magic(indata, mime=False):\n", "code": "try:\n    if mime:\n        mymagic = magic.open(magic.MAGIC_MIME_TYPE)\n    else:\n        mymagic = magic.open(magic.MAGIC_NONE)\n    mymagic.load()\nexcept AttributeError:\n    mymagic = magic.Magic(mime)\n    mymagic.file = mymagic.from_file\nreturn mymagic.file(indata)", "path": "lib\\extractor.py", "repo_name": "misterch0c/firminator_backend", "stars": 196, "license": "None", "language": "python", "size": 34300}
{"docstring": "\"\"\"\nCheck if this file is blacklisted for analysis based on file type.\n\"\"\"\n# First, use MIME-type to exclude large categories of files\n", "func_signal": "def _check_blacklist(self):\n", "code": "filetype = Extractor.magic(self.item.encode(\"utf-8\", \"surrogateescape\"),\n                           mime=True)\nif any(s in filetype for s in [b\"application/x-executable\",\n                               b\"application/x-dosexec\",\n                               b\"application/x-object\",\n                               b\"application/pdf\",\n                               b\"application/msword\",\n                               b\"image/\", b\"text/\", b\"video/\"]):\n    self.printf(\">> Skipping: %s...\" % filetype)\n    return True\n\n# Next, check for specific file types that have MIME-type\n# 'application/octet-stream'\nfiletype = Extractor.magic(self.item.encode(\"utf-8\", \"surrogateescape\"))\nif any(s in filetype for s in [b\"executable\", b\"universal binary\",\n                               b\"relocatable\", b\"bytecode\", b\"applet\"]):\n    self.printf(\">> Skipping: %s...\" % filetype)\n    return True\n\n# Finally, check for specific file extensions that would be incorrectly\n# identified\nif self.item.endswith(\".dmg\"):\n    self.printf(\">> Skipping: %s...\" % (self.item))\n    return True\n\nreturn False", "path": "lib\\extractor.py", "repo_name": "misterch0c/firminator_backend", "stars": 196, "license": "None", "language": "python", "size": 34300}
{"docstring": "#lines = data.split(\"\\r\\n\")\n", "func_signal": "def findVlanInfoForDev(data, dev):\n", "code": "lines = stripTimestamps(data)\nresults = []\ncandidates = filter(lambda l: l.startswith(\"register_vlan_dev\"), lines)\nfor c in candidates:\n    g = re.match(r\"register_vlan_dev\\[[^\\]]+\\]: dev:%s vlan_id:([0-9]+)\" % dev, c)\n    if g:\n        results.append(int(g.group(1)))\nreturn results", "path": "scripts\\makeNetwork.py", "repo_name": "misterch0c/firminator_backend", "stars": 196, "license": "None", "language": "python", "size": 34300}
{"docstring": "\"\"\"\nIf this file is of a known firmware type, directly attempt to extract\nthe kernel and root filesystem.\n\"\"\"\n", "func_signal": "def _check_firmware(self):\n", "code": "for module in binwalk.scan(self.item, \"-y\", \"header\", signature=True,\n                           quiet=True):\n    for entry in module.results:\n        # uImage\n        if \"uImage header\" in entry.description:\n            if not self.get_kernel_status() and \\\n                \"OS Kernel Image\" in entry.description:\n                kernel_offset = entry.offset + 64\n                kernel_size = 0\n\n                for stmt in entry.description.split(','):\n                    if \"image size:\" in stmt:\n                        kernel_size = int(''.join(\n                            i for i in stmt if i.isdigit()), 10)\n\n                if kernel_size != 0 and kernel_offset + kernel_size \\\n                    <= os.path.getsize(self.item):\n                    self.printf(\">>>> %s\" % entry.description)\n\n                    tmp_fd, tmp_path = tempfile.mkstemp(dir=self.temp)\n                    os.close(tmp_fd)\n                    Extractor.io_dd(self.item, kernel_offset,\n                                    kernel_size, tmp_path)\n                    kernel = ExtractionItem(self.extractor, tmp_path,\n                                            self.depth, self.tag)\n\n                    return kernel.extract()\n            # elif \"RAMDisk Image\" in entry.description:\n            #     self.printf(\">>>> %s\" % entry.description)\n            #     self.printf(\">>>> Skipping: RAMDisk / initrd\")\n            #     self.terminate = True\n            #     return True\n\n        # TP-Link or TRX\n        elif not self.get_kernel_status() and \\\n            not self.get_rootfs_status() and \\\n            \"rootfs offset: \" in entry.description and \\\n            \"kernel offset: \" in entry.description:\n            kernel_offset = 0\n            kernel_size = 0\n            rootfs_offset = 0\n            rootfs_size = 0\n\n            for stmt in entry.description.split(','):\n                if \"kernel offset:\" in stmt:\n                    kernel_offset = int(stmt.split(':')[1], 16)\n                elif \"kernel length:\" in stmt:\n                    kernel_size = int(stmt.split(':')[1], 16)\n                elif \"rootfs offset:\" in stmt:\n                    rootfs_offset = int(stmt.split(':')[1], 16)\n                elif \"rootfs length:\" in stmt:\n                    rootfs_size = int(stmt.split(':')[1], 16)\n\n            # compute sizes if only offsets provided\n            if kernel_offset != rootfs_size and kernel_size == 0 and \\\n                rootfs_size == 0:\n                kernel_size = rootfs_offset - kernel_offset\n                rootfs_size = os.path.getsize(self.item) - rootfs_offset\n\n            # ensure that computed values are sensible\n            if (kernel_size > 0 and kernel_offset + kernel_size \\\n                <= os.path.getsize(self.item)) and \\\n                (rootfs_size != 0 and rootfs_offset + rootfs_size \\\n                    <= os.path.getsize(self.item)):\n                self.printf(\">>>> %s\" % entry.description)\n\n                tmp_fd, tmp_path = tempfile.mkstemp(dir=self.temp)\n                os.close(tmp_fd)\n                Extractor.io_dd(self.item, kernel_offset, kernel_size,\n                                tmp_path)\n                kernel = ExtractionItem(self.extractor, tmp_path,\n                                        self.depth, self.tag)\n                kernel.extract()\n\n                tmp_fd, tmp_path = tempfile.mkstemp(dir=self.temp)\n                os.close(tmp_fd)\n                Extractor.io_dd(self.item, rootfs_offset, rootfs_size,\n                                tmp_path)\n                rootfs = ExtractionItem(self.extractor, tmp_path,\n                                        self.depth, self.tag)\n                rootfs.extract()\n\n                return self.update_status()\nreturn False", "path": "lib\\extractor.py", "repo_name": "misterch0c/firminator_backend", "stars": 196, "license": "None", "language": "python", "size": 34300}
{"docstring": "\"\"\"\nPerforms MD5 with a block size of 64kb.\n\"\"\"\n", "func_signal": "def io_md5(target):\n", "code": "blocksize = 65536\nhasher = hashlib.md5()\n\nwith open(target, 'rb') as ifp:\n    buf = ifp.read(blocksize)\n    while buf:\n        hasher.update(buf)\n        buf = ifp.read(blocksize)\n    return hasher.hexdigest()", "path": "lib\\extractor.py", "repo_name": "misterch0c/firminator_backend", "stars": 196, "license": "None", "language": "python", "size": 34300}
{"docstring": "\"\"\"\nUpdates the status flags using the tag to determine completion status.\n\"\"\"\n", "func_signal": "def update_status(self):\n", "code": "kernel_done = os.path.isfile(self.get_kernel_path()) if \\\n    self.extractor.do_kernel and self.output else \\\n    not self.extractor.do_kernel\nrootfs_done = os.path.isfile(self.get_rootfs_path()) if \\\n    self.extractor.do_rootfs and self.output else \\\n    not self.extractor.do_rootfs\nself.status = (kernel_done, rootfs_done)\n\n\nif self.database and kernel_done and self.extractor.do_kernel:\n    self.update_database(\"kernel_extracted\", \"True\")\n\nif self.database and rootfs_done and self.extractor.do_rootfs:\n    self.update_database(\"rootfs_extracted\", \"True\")\n\nreturn self.get_status()", "path": "lib\\extractor.py", "repo_name": "misterch0c/firminator_backend", "stars": 196, "license": "None", "language": "python", "size": 34300}
{"docstring": "#lines = data.split(\"\\r\\n\")\n", "func_signal": "def findIfacesForBridge(data, brif):\n", "code": "lines = stripTimestamps(data)\nresult = []\ncandidates = filter(lambda l: l.startswith(\"br_dev_ioctl\") or l.startswith(\"br_add_if\"), lines)\nfor c in candidates:\n    pat = r\"^br_dev_ioctl\\[[^\\]]+\\]: br:%s dev:(.*)\" % brif\n    g = re.match(pat, c)\n    if g:\n        iface = g.group(1)\n        #we only add it if the interface is not the bridge itself\n        #there are images that call brctl addif br0 br0 (e.g., 5152)\n        if iface != brif:\n            result.append(g.group(1))\n    pat = r\"^br_add_if\\[[^\\]]+\\]: br:%s dev:(.*)\" % brif\n    g = re.match(pat, c)\n    if g:\n        iface = g.group(1)\n        if iface != brif:\n            result.append(g.group(1))\nreturn result", "path": "scripts\\makeNetwork.py", "repo_name": "misterch0c/firminator_backend", "stars": 196, "license": "None", "language": "python", "size": 34300}
{"docstring": "# Temporary directory\n", "func_signal": "def __init__(self, extractor, path, depth, tag=None):\n", "code": "self.temp = None\n\n# Recursion depth counter\nself.depth = depth\n\n# Reference to parent extractor object\nself.extractor = extractor\n\n# File path\nself.item = path\n\n# Database connection\nif self.extractor.database:\n    import psycopg2\n    self.database = psycopg2.connect(database=settings.DATABASES[\"default\"][\"NAME\"],\n                                     user=settings.DATABASES[\"default\"][\"USER\"],\n                                     password=settings.DATABASES[\"default\"][\"PASSWORD\"],\n                                     host=settings.DATABASES[\"default\"][\"HOST\"])\nelse:\n    self.database = None\n\n# Checksum\nself.checksum = Extractor.io_md5(path)\n\n# Tag\nself.tag = tag if tag else self.generate_tag()\n\n# Output file path and filename prefix\nself.output = os.path.join(self.extractor.output_dir, self.tag) if \\\n                           self.extractor.output_dir else None\n\n# Status, with terminate indicating early termination for this item\nself.terminate = False\nself.status = None\nself.update_status()", "path": "lib\\extractor.py", "repo_name": "misterch0c/firminator_backend", "stars": 196, "license": "None", "language": "python", "size": 34300}
{"docstring": "\"\"\"\nReturn a safely quoted version of a value.\n\nRaise a ConfigObjError if the value cannot be safely quoted.\nIf multiline is ``True`` (default) then use triple quotes\nif necessary.\n\nDon't quote values that don't need it.\nRecursively quote members of a list and return a comma joined list.\nMultiline is ``False`` for lists.\nObey list syntax for empty and single member lists.\n\nIf ``list_values=False`` then the value is only quoted if it contains\na ``\\n`` (is multiline) or '#'.\n\nIf ``write_empty_values`` is set, and the value is an empty string, it\nwon't be quoted.\n\"\"\"\n", "func_signal": "def _quote(self, value, multiline=True):\n", "code": "if multiline and self.write_empty_values and value == '':\n    # Only if multiline is set, so that it is used for values not\n    # keys, and not values that are part of a list\n    return ''\n\nif multiline and isinstance(value, (list, tuple)):\n    if not value:\n        return ','\n    elif len(value) == 1:\n        return self._quote(value[0], multiline=False) + ','\n    return ', '.join([self._quote(val, multiline=False)\n        for val in value])\nif not isinstance(value, StringTypes):\n    if self.stringify:\n        value = str(value)\n    else:\n        raise TypeError('Value \"%s\" is not a string.' % value)\n\nif not value:\n    return '\"\"'\n\nno_lists_no_quotes = not self.list_values and '\\n' not in value and '#' not in value\nneed_triple = multiline and (((\"'\" in value) and ('\"' in value)) or ('\\n' in value ))\nhash_triple_quote = multiline and not need_triple and (\"'\" in value) and ('\"' in value) and ('#' in value)\ncheck_for_single = (no_lists_no_quotes or not need_triple) and not hash_triple_quote\n\nif check_for_single:\n    if not self.list_values:\n        # we don't quote if ``list_values=False``\n        quot = noquot\n    # for normal values either single or double quotes will do\n    elif '\\n' in value:\n        # will only happen if multiline is off - e.g. '\\n' in key\n        raise ConfigObjError('Value \"%s\" cannot be safely quoted.' % value)\n    elif ((value[0] not in wspace_plus) and\n            (value[-1] not in wspace_plus) and\n            (',' not in value)):\n        quot = noquot\n    else:\n        quot = self._get_single_quote(value)\nelse:\n    # if value has '\\n' or \"'\" *and* '\"', it will need triple quotes\n    quot = self._get_triple_quote(value)\n\nif quot == noquot and '#' in value and self.list_values:\n    quot = self._get_single_quote(value)\n        \nreturn quot % value", "path": "Release 2.0\\configobj.py", "repo_name": "tachang/EyeFiServer", "stars": 187, "license": "None", "language": "python", "size": 3478}
{"docstring": "\"\"\"The function that does the actual work.\n\n``value``: the string we're trying to interpolate.\n``section``: the section in which that string was found\n``backtrail``: a dict to keep track of where we've been,\nto detect and prevent infinite recursion loops\n\nThis is similar to a depth-first-search algorithm.\n\"\"\"\n# Have we been here already?\n", "func_signal": "def recursive_interpolate(key, value, section, backtrail):\n", "code": "if backtrail.has_key((key, section.name)):\n    # Yes - infinite loop detected\n    raise InterpolationLoopError(key)\n# Place a marker on our backtrail so we won't come back here again\nbacktrail[(key, section.name)] = 1\n\n# Now start the actual work\nmatch = self._KEYCRE.search(value)\nwhile match:\n    # The actual parsing of the match is implementation-dependent,\n    # so delegate to our helper function\n    k, v, s = self._parse_match(match)\n    if k is None:\n        # That's the signal that no further interpolation is needed\n        replacement = v\n    else:\n        # Further interpolation may be needed to obtain final value\n        replacement = recursive_interpolate(k, v, s, backtrail)\n    # Replace the matched string with its final value\n    start, end = match.span()\n    value = ''.join((value[:start], replacement, value[end:]))\n    new_search_start = start + len(replacement)\n    # Pick up the next interpolation key, if any, for next time\n    # through the while loop\n    match = self._KEYCRE.search(value, new_search_start)\n\n# Now safe to come back here again; remove marker from backtrail\ndel backtrail[(key, section.name)]\n\nreturn value", "path": "Release 2.0\\configobj.py", "repo_name": "tachang/EyeFiServer", "stars": 187, "license": "None", "language": "python", "size": 3478}
{"docstring": "# If the name of the element is a key in the dictionary elementsToExtract\n    # set the value to False\n", "func_signal": "def endElement(self, name):\n", "code": "    if name in self.elementsToExtract:\n      self.elementsToExtract[name] = False", "path": "Old\\EyeFiServerv2.py", "repo_name": "tachang/EyeFiServer", "stars": 187, "license": "None", "language": "python", "size": 3478}
{"docstring": "\"\"\"Remove items from the sequence when deleting.\"\"\"\n", "func_signal": "def __delitem__(self, key):\n", "code": "dict. __delitem__(self, key)\nif key in self.scalars:\n    self.scalars.remove(key)\nelse:\n    self.sections.remove(key)\ndel self.comments[key]\ndel self.inline_comments[key]", "path": "Release 2.0\\configobj.py", "repo_name": "tachang/EyeFiServer", "stars": 187, "license": "None", "language": "python", "size": 3478}
{"docstring": "\"\"\"\nDecode infile to unicode. Using the specified encoding.\n\nif is a string, it also needs converting to a list.\n\"\"\"\n", "func_signal": "def _decode(self, infile, encoding):\n", "code": "if isinstance(infile, StringTypes):\n    # can't be unicode\n    # NOTE: Could raise a ``UnicodeDecodeError``\n    return infile.decode(encoding).splitlines(True)\nfor i, line in enumerate(infile):\n    if not isinstance(line, unicode):\n        # NOTE: The isinstance test here handles mixed lists of unicode/string\n        # NOTE: But the decode will break on any non-string values\n        # NOTE: Or could raise a ``UnicodeDecodeError``\n        infile[i] = line.decode(encoding)\nreturn infile", "path": "Release 2.0\\configobj.py", "repo_name": "tachang/EyeFiServer", "stars": 187, "license": "None", "language": "python", "size": 3478}
{"docstring": "# Valid name (in or out of braces): fetch value from section\n", "func_signal": "def _parse_match(self, match):\n", "code": "key = match.group('named') or match.group('braced')\nif key is not None:\n    value, section = self._fetch(key)\n    return key, value, section\n# Escaped delimiter (e.g., $$): return single delimiter\nif match.group('escaped') is not None:\n    # Return None for key and section to indicate it's time to stop\n    return None, self._delimiter, None\n# Anything else: ignore completely, just return it unchanged\nreturn None, match.group(), None", "path": "Release 2.0\\configobj.py", "repo_name": "tachang/EyeFiServer", "stars": 187, "license": "None", "language": "python", "size": 3478}
{"docstring": "\"\"\"\nCorrectly set a value.\n\nMaking dictionary values Section instances.\n(We have to special case 'Section' instances - which are also dicts)\n\nKeys must be strings.\nValues need only be strings (or lists of strings) if\n``main.stringify`` is set.\n\n`unrepr`` must be set when setting a value to a dictionary, without\ncreating a new sub-section.\n\"\"\"\n", "func_signal": "def __setitem__(self, key, value, unrepr=False):\n", "code": "if not isinstance(key, StringTypes):\n    raise ValueError('The key \"%s\" is not a string.' % key)\n\n# add the comment\nif not self.comments.has_key(key):\n    self.comments[key] = []\n    self.inline_comments[key] = ''\n# remove the entry from defaults\nif key in self.defaults:\n    self.defaults.remove(key)\n#\nif isinstance(value, Section):\n    if not self.has_key(key):\n        self.sections.append(key)\n    dict.__setitem__(self, key, value)\nelif isinstance(value, dict) and not unrepr:\n    # First create the new depth level,\n    # then create the section\n    if not self.has_key(key):\n        self.sections.append(key)\n    new_depth = self.depth + 1\n    dict.__setitem__(\n        self,\n        key,\n        Section(\n            self,\n            new_depth,\n            self.main,\n            indict=value,\n            name=key))\nelse:\n    if not self.has_key(key):\n        self.scalars.append(key)\n    if not self.main.stringify:\n        if isinstance(value, StringTypes):\n            pass\n        elif isinstance(value, (list, tuple)):\n            for entry in value:\n                if not isinstance(entry, StringTypes):\n                    raise TypeError('Value is not a string \"%s\".' % entry)\n        else:\n            raise TypeError('Value is not a string \"%s\".' % value)\n    dict.__setitem__(self, key, value)", "path": "Release 2.0\\configobj.py", "repo_name": "tachang/EyeFiServer", "stars": 187, "license": "None", "language": "python", "size": 3478}
{"docstring": "\"\"\"A deprecated version of ``as_bool``.\"\"\"\n", "func_signal": "def istrue(self, key):\n", "code": "warn('use of ``istrue`` is deprecated. Use ``as_bool`` method '\n        'instead.', DeprecationWarning)\nreturn self.as_bool(key)", "path": "Release 2.0\\configobj.py", "repo_name": "tachang/EyeFiServer", "stars": 187, "license": "None", "language": "python", "size": 3478}
{"docstring": "\"\"\"\nHandle an error according to the error settings.\n\nEither raise the error or store it.\nThe error will have occured at ``cur_index``\n\"\"\"\n", "func_signal": "def _handle_error(self, text, ErrorClass, infile, cur_index):\n", "code": "line = infile[cur_index]\ncur_index += 1\nmessage = text % cur_index\nerror = ErrorClass(message, cur_index, line)\nif self.raise_errors:\n    # raise the error - parsing stops here\n    raise error\n# store the error\n# reraise when parsing has finished\nself._errors.append(error)", "path": "Release 2.0\\configobj.py", "repo_name": "tachang/EyeFiServer", "stars": 187, "license": "None", "language": "python", "size": 3478}
{"docstring": "\"\"\"A dummy check method, always returns the value unchanged.\"\"\"\n", "func_signal": "def check(self, check, member, missing=False):\n", "code": "if missing:\n    raise self.baseErrorClass()\nreturn member", "path": "Release 2.0\\configobj.py", "repo_name": "tachang/EyeFiServer", "stars": 187, "license": "None", "language": "python", "size": 3478}
{"docstring": "\"\"\"\n'D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised'\n\"\"\"\n", "func_signal": "def pop(self, key, *args):\n", "code": "val = dict.pop(self, key, *args)\nif key in self.scalars:\n    del self.comments[key]\n    del self.inline_comments[key]\n    self.scalars.remove(key)\nelif key in self.sections:\n    del self.comments[key]\n    del self.inline_comments[key]\n    self.sections.remove(key)\nif self.main.interpolation and isinstance(val, StringTypes):\n    return self._interpolate(key, val)\nreturn val", "path": "Release 2.0\\configobj.py", "repo_name": "tachang/EyeFiServer", "stars": 187, "license": "None", "language": "python", "size": 3478}
{"docstring": "\"\"\"\nA recursive update - useful for merging config files.\n\n>>> a = '''[section1]\n...     option1 = True\n...     [[subsection]]\n...     more_options = False\n...     # end of file'''.splitlines()\n>>> b = '''# File is user.ini\n...     [section1]\n...     option1 = False\n...     # end of file'''.splitlines()\n>>> c1 = ConfigObj(b)\n>>> c2 = ConfigObj(a)\n>>> c2.merge(c1)\n>>> c2\n{'section1': {'option1': 'False', 'subsection': {'more_options': 'False'}}}\n\"\"\"\n", "func_signal": "def merge(self, indict):\n", "code": "for key, val in indict.items():\n    if (key in self and isinstance(self[key], dict) and\n                        isinstance(val, dict)):\n        self[key].merge(val)\n    else:   \n        self[key] = val", "path": "Release 2.0\\configobj.py", "repo_name": "tachang/EyeFiServer", "stars": 187, "license": "None", "language": "python", "size": 3478}
{"docstring": "\"\"\"Extract the value, where we are in a multiline situation.\"\"\"\n", "func_signal": "def _multiline(self, value, infile, cur_index, maxline):\n", "code": "quot = value[:3]\nnewvalue = value[3:]\nsingle_line = self._triple_quote[quot][0]\nmulti_line = self._triple_quote[quot][1]\nmat = single_line.match(value)\nif mat is not None:\n    retval = list(mat.groups())\n    retval.append(cur_index)\n    return retval\nelif newvalue.find(quot) != -1:\n    # somehow the triple quote is missing\n    raise SyntaxError()\n#\nwhile cur_index < maxline:\n    cur_index += 1\n    newvalue += '\\n'\n    line = infile[cur_index]\n    if line.find(quot) == -1:\n        newvalue += line\n    else:\n        # end of multiline, process it\n        break\nelse:\n    # we've got to the end of the config, oops...\n    raise SyntaxError()\nmat = multi_line.match(line)\nif mat is None:\n    # a badly formed line\n    raise SyntaxError()\n(value, comment) = mat.groups()\nreturn (newvalue + value, comment, cur_index)", "path": "Release 2.0\\configobj.py", "repo_name": "tachang/EyeFiServer", "stars": 187, "license": "None", "language": "python", "size": 3478}
{"docstring": "\"\"\"\nA version of update that uses our ``__setitem__``.\n\"\"\"\n", "func_signal": "def update(self, indict):\n", "code": "for entry in indict:\n    self[entry] = indict[entry]", "path": "Release 2.0\\configobj.py", "repo_name": "tachang/EyeFiServer", "stars": 187, "license": "None", "language": "python", "size": 3478}
{"docstring": "# If the name of the element is a key in the dictionary elementsToExtract\n    # set the value to True\n", "func_signal": "def startElement(self, name, attributes):\n", "code": "    if name in self.elementsToExtract:\n      self.elementsToExtract[name] = True", "path": "Old\\EyeFiServer.py", "repo_name": "tachang/EyeFiServer", "stars": 187, "license": "None", "language": "python", "size": 3478}
{"docstring": "\"\"\"\nAn example function that will turn a nested dictionary of results\n(as returned by ``ConfigObj.validate``) into a flat list.\n\n``cfg`` is the ConfigObj instance being checked, ``res`` is the results\ndictionary returned by ``validate``.\n\n(This is a recursive function, so you shouldn't use the ``levels`` or\n``results`` arguments - they are used by the function.\n\nReturns a list of keys that failed. Each member of the list is a tuple :\n::\n\n    ([list of sections...], key, result)\n\nIf ``validate`` was called with ``preserve_errors=False`` (the default)\nthen ``result`` will always be ``False``.\n\n*list of sections* is a flattened list of sections that the key was found\nin.\n\nIf the section was missing then key will be ``None``.\n\nIf the value (or section) was missing then ``result`` will be ``False``.\n\nIf ``validate`` was called with ``preserve_errors=True`` and a value\nwas present, but failed the check, then ``result`` will be the exception\nobject returned. You can use this as a string that describes the failure.\n\nFor example *The value \"3\" is of the wrong type*.\n\n>>> import validate\n>>> vtor = validate.Validator()\n>>> my_ini = '''\n...     option1 = True\n...     [section1]\n...     option1 = True\n...     [section2]\n...     another_option = Probably\n...     [section3]\n...     another_option = True\n...     [[section3b]]\n...     value = 3\n...     value2 = a\n...     value3 = 11\n...     '''\n>>> my_cfg = '''\n...     option1 = boolean()\n...     option2 = boolean()\n...     option3 = boolean(default=Bad_value)\n...     [section1]\n...     option1 = boolean()\n...     option2 = boolean()\n...     option3 = boolean(default=Bad_value)\n...     [section2]\n...     another_option = boolean()\n...     [section3]\n...     another_option = boolean()\n...     [[section3b]]\n...     value = integer\n...     value2 = integer\n...     value3 = integer(0, 10)\n...         [[[section3b-sub]]]\n...         value = string\n...     [section4]\n...     another_option = boolean()\n...     '''\n>>> cs = my_cfg.split('\\\\n')\n>>> ini = my_ini.split('\\\\n')\n>>> cfg = ConfigObj(ini, configspec=cs)\n>>> res = cfg.validate(vtor, preserve_errors=True)\n>>> errors = []\n>>> for entry in flatten_errors(cfg, res):\n...     section_list, key, error = entry\n...     section_list.insert(0, '[root]')\n...     if key is not None:\n...        section_list.append(key)\n...     else:\n...         section_list.append('[missing]')\n...     section_string = ', '.join(section_list)\n...     errors.append((section_string, ' = ', error))\n>>> errors.sort()\n>>> for entry in errors:\n...     print entry[0], entry[1], (entry[2] or 0)\n[root], option2  =  0\n[root], option3  =  the value \"Bad_value\" is of the wrong type.\n[root], section1, option2  =  0\n[root], section1, option3  =  the value \"Bad_value\" is of the wrong type.\n[root], section2, another_option  =  the value \"Probably\" is of the wrong type.\n[root], section3, section3b, section3b-sub, [missing]  =  0\n[root], section3, section3b, value2  =  the value \"a\" is of the wrong type.\n[root], section3, section3b, value3  =  the value \"11\" is too big.\n[root], section4, [missing]  =  0\n\"\"\"\n", "func_signal": "def flatten_errors(cfg, res, levels=None, results=None):\n", "code": "if levels is None:\n    # first time called\n    levels = []\n    results = []\nif res is True:\n    return results\nif res is False:\n    results.append((levels[:], None, False))\n    if levels:\n        levels.pop()\n    return results\nfor (key, val) in res.items():\n    if val == True:\n        continue\n    if isinstance(cfg.get(key), dict):\n        # Go down one level\n        levels.append(key)\n        flatten_errors(cfg[key], val, levels, results)\n        continue\n    results.append((levels[:], key, val))\n#\n# Go up one level\nif levels:\n    levels.pop()\n#\nreturn results", "path": "Release 2.0\\configobj.py", "repo_name": "tachang/EyeFiServer", "stars": 187, "license": "None", "language": "python", "size": 3478}
{"docstring": "\"\"\"\nRecursively restore default values to all members\nthat have them.\n\nThis method will only work for a ConfigObj that was created\nwith a configspec and has been validated.\n\nIt doesn't delete or modify entries without default values.\n\"\"\"\n", "func_signal": "def restore_defaults(self):\n", "code": "for key in self.default_values:\n    self.restore_default(key)\n    \nfor section in self.sections:\n    self[section].restore_defaults()", "path": "Release 2.0\\configobj.py", "repo_name": "tachang/EyeFiServer", "stars": 187, "license": "None", "language": "python", "size": 3478}
{"docstring": "\"\"\"A version of setdefault that sets sequence if appropriate.\"\"\"\n", "func_signal": "def setdefault(self, key, default=None):\n", "code": "try:\n    return self[key]\nexcept KeyError:\n    self[key] = default\n    return self[key]", "path": "Release 2.0\\configobj.py", "repo_name": "tachang/EyeFiServer", "stars": 187, "license": "None", "language": "python", "size": 3478}
{"docstring": "\"\"\"\nReturn a deepcopy of self as a dictionary.\n\nAll members that are ``Section`` instances are recursively turned to\nordinary dictionaries - by calling their ``dict`` method.\n\n>>> n = a.dict()\n>>> n == a\n1\n>>> n is a\n0\n\"\"\"\n", "func_signal": "def dict(self):\n", "code": "newdict = {}\nfor entry in self:\n    this_entry = self[entry]\n    if isinstance(this_entry, Section):\n        this_entry = this_entry.dict()\n    elif isinstance(this_entry, list):\n        # create a copy rather than a reference\n        this_entry = list(this_entry)\n    elif isinstance(this_entry, tuple):\n        # create a copy rather than a reference\n        this_entry = tuple(this_entry)\n    newdict[entry] = this_entry\nreturn newdict", "path": "Release 2.0\\configobj.py", "repo_name": "tachang/EyeFiServer", "stars": 187, "license": "None", "language": "python", "size": 3478}
{"docstring": "\"\"\"\nChange a keyname to another, without changing position in sequence.\n\nImplemented so that transformations can be made on keys,\nas well as on values. (used by encode and decode)\n\nAlso renames comments.\n\"\"\"\n", "func_signal": "def rename(self, oldkey, newkey):\n", "code": "if oldkey in self.scalars:\n    the_list = self.scalars\nelif oldkey in self.sections:\n    the_list = self.sections\nelse:\n    raise KeyError('Key \"%s\" not found.' % oldkey)\npos = the_list.index(oldkey)\n#\nval = self[oldkey]\ndict.__delitem__(self, oldkey)\ndict.__setitem__(self, newkey, val)\nthe_list.remove(oldkey)\nthe_list.insert(pos, newkey)\ncomm = self.comments[oldkey]\ninline_comment = self.inline_comments[oldkey]\ndel self.comments[oldkey]\ndel self.inline_comments[oldkey]\nself.comments[newkey] = comm\nself.inline_comments[newkey] = inline_comment", "path": "Release 2.0\\configobj.py", "repo_name": "tachang/EyeFiServer", "stars": 187, "license": "None", "language": "python", "size": 3478}
{"docstring": "\"\"\" Paint the items in the table.\n    \n    If the item referred to by <index> is a StarRating, we handle the\n    painting ourselves. For the other items, we let the base class\n    handle the painting as usual.\n\n    In a polished application, we'd use a better check than the \n    column number to find out if we needed to paint the stars, but\n    it works for the purposes of this example.\n\"\"\"\n", "func_signal": "def paint(self, painter, option, index):\n", "code": "if index.column() == 3:\n    starRating = StarRating(index.data())\n    \n    # If the row is currently selected, we need to make sure we\n    # paint the background accordingly.\n    if option.state & QStyle.State_Selected:\n        # The original C++ example used option.palette.foreground() to\n        # get the brush for painting, but there are a couple of \n        # problems with that:\n        #   - foreground() is obsolete now, use windowText() instead\n        #   - more importantly, windowText() just returns a brush\n        #     containing a flat color, where sometimes the style \n        #     would have a nice subtle gradient or something. \n        # Here we just use the brush of the painter object that's\n        # passed in to us, which keeps the row highlighting nice\n        # and consistent.\n        painter.fillRect(option.rect, painter.brush())\n    \n    # Now that we've painted the background, call starRating.paint()\n    # to paint the stars.\n    starRating.paint(painter, option.rect, option.palette)\nelse:\n    QStyledItemDelegate.paint(self, painter, option, index)", "path": "examples\\widgets\\itemviews\\stardelegate\\stardelegate.py", "repo_name": "pyside/pyside2-examples", "stars": 136, "license": "None", "language": "python", "size": 10571}
{"docstring": "# Some arguments should be processed before others.  Handle them now.\n", "func_signal": "def parseArgs(cls, argv):\n", "code": "if \"-verbose\" in argv:\n    cls.verbose = True\n\ncls.detectSystemResources()\n\n# Handle the rest of the arguments.  They may override attributes\n# already set.\nfor s in argv:\n    if s == \"-opengl\":\n        cls.openGlRendering = True\n    elif s == \"-direct3d\":\n        cls.direct3dRendering = True\n    elif s == \"-software\":\n        cls.softwareRendering = True\n    elif s == \"-no-opengl\":\n        cls.softwareRendering = True\n    elif s == \"-no-ticker\":\n        cls.noTicker = True\n    elif s.startswith(\"-ticker\"):\n        cls.noTicker =  not bool(parseFloat(s, \"-ticker\"))\n    elif s == \"-no-animations\":\n        cls.noAnimations = True\n    elif s.startswith(\"-animations\"):\n        cls.noAnimations = not bool(parseFloat(s, \"-animations\"))\n    elif s == \"-no-adapt\":\n        cls.noAdapt = True\n    elif s == \"-low\":\n        cls.setLowSettings()\n    elif s == \"-no-rescale\":\n        cls.noRescale = True\n    elif s == \"-use-pixmaps\":\n        cls.usePixmaps = True\n    elif s == \"-fullscreen\":\n        cls.fullscreen = True\n    elif s == \"-show-br\":\n        cls.showBoundingRect = True\n    elif s == \"-show-fps\":\n        cls.showFps = True\n    elif s == \"-no-blending\":\n        cls.noBlending = True\n    elif s == \"-no-sync\":\n        cls.noScreenSync = True\n    elif s.startswith(\"-menu\"):\n        cls.menuCount = int(parseFloat(s, \"-menu\"))\n    elif s.startswith(\"-use-timer-update\"):\n        cls.noTimerUpdate = not bool(parseFloat(s, \"-use-timer-update\"))\n    elif s.startswith(\"-pause\"):\n        cls.pause = bool(parseFloat(s, \"-pause\"))\n    elif s == \"-no-ticker-morph\":\n        cls.noTickerMorph = True\n    elif s == \"-use-window-mask\":\n        cls.noWindowMask = False\n    elif s == \"-use-loop\":\n        cls.useLoop = True\n    elif s == \"-use-8bit\":\n        cls.useEightBitPalette = True\n    elif s.startswith(\"-8bit\"):\n        cls.useEightBitPalette = bool(parseFloat(s, \"-8bit\"))\n    elif s == \"-use-balls\":\n        cls.useButtonBalls = True\n    elif s.startswith(\"-ticker-letters\"):\n        cls.tickerLetterCount = int(parseFloat(s, \"-ticker-letters\"))\n    elif s.startswith(\"-ticker-text\"):\n        cls.tickerText = parseText(s, \"-ticker-text\")\n    elif s.startswith(\"-ticker-speed\"):\n        cls.tickerMoveSpeed = parseFloat(s, \"-ticker-speed\")\n    elif s.startswith(\"-ticker-morph-speed\"):\n        cls.tickerMorphSpeed = parseFloat(s, \"-ticker-morph-speed\")\n    elif s.startswith(\"-animation-speed\"):\n        cls.animSpeed = parseFloat(s, \"-animation-speed\")\n    elif s.startswith(\"-fps\"):\n        cls.fps = int(parseFloat(s, \"-fps\"))\n    elif s.startswith(\"-h\") or s.startswith(\"-help\"):\n        QtGui.QMessageBox.warning(None, \"Arguments\",\n                \"Usage: qtdemo.py [-verbose] [-no-adapt] [-opengl] \"\n                \"[-direct3d] [-software] [-fullscreen] [-ticker[0|1]] \"\n                \"[-animations[0|1]] [-no-blending] [-no-sync] \"\n                \"[-use-timer-update[0|1]] [-pause[0|1]] \"\n                \"[-use-window-mask] [-no-rescale] [-use-pixmaps] \"\n                \"[-show-fps] [-show-br] [-8bit[0|1]] [-menu<int>] \"\n                \"[-use-loop] [-use-balls] [-animation-speed<float>] \"\n                \"[-fps<int>] [-low] [-ticker-letters<int>] \"\n                \"[-ticker-speed<float>] [-no-ticker-morph] \"\n                \"[-ticker-morph-speed<float>] [-ticker-text<string>]\")\n        sys.exit(0)\n\ncls.postConfigure()", "path": "examples\\demos\\qtdemo\\colors.py", "repo_name": "pyside/pyside2-examples", "stars": 136, "license": "None", "language": "python", "size": 10571}
{"docstring": "\"\"\" As the mouse moves inside the editor, track the position and \n    update the editor to display as many stars as necessary. \n\"\"\"\n", "func_signal": "def mouseMoveEvent(self, event):\n", "code": "star = self.starAtPosition(event.x())\n\nif (star != self.starRating.starCount) and (star != -1):\n    self.starRating.starCount = star\n    self.update()", "path": "examples\\widgets\\itemviews\\stardelegate\\stareditor.py", "repo_name": "pyside/pyside2-examples", "stars": 136, "license": "None", "language": "python", "size": 10571}
{"docstring": "\"\"\" Paint the editor, offloading the work to the StarRating class. \"\"\"\n", "func_signal": "def paintEvent(self, event):\n", "code": "painter = QPainter(self)\nself.starRating.paint(painter, self.rect(), self.palette(), isEditable=True)", "path": "examples\\widgets\\itemviews\\stardelegate\\stareditor.py", "repo_name": "pyside/pyside2-examples", "stars": 136, "license": "None", "language": "python", "size": 10571}
{"docstring": "# Create the main menuBar menu items\n", "func_signal": "def createMenus(self):\n", "code": "fileMenu = self.menuBar().addMenu(\"&File\")\ntoolMenu = self.menuBar().addMenu(\"&Tools\")\n\n# Populate the File menu\nopenAction = self.createAction(\"&Open...\", fileMenu, self.openFile)\nsaveAction = self.createAction(\"&Save As...\", fileMenu, self.saveFile)\nfileMenu.addSeparator()\nexitAction = self.createAction(\"E&xit\", fileMenu, self.close)\n\n# Populate the Tools menu\naddAction = self.createAction(\"&Add Entry...\", toolMenu, self.addressWidget.addEntry)\nself.editAction = self.createAction(\"&Edit Entry...\", toolMenu, self.addressWidget.editEntry)\ntoolMenu.addSeparator()\nself.removeAction = self.createAction(\"&Remove Entry\", toolMenu, self.addressWidget.removeEntry)\n\n# Disable the edit and remove menu items initally, as there are\n# no items yet.\nself.editAction.setEnabled(False)\nself.removeAction.setEnabled(False)\n\n# Wire up the updateActions slot\nself.addressWidget.selectionChanged.connect(self.updateActions)", "path": "examples\\widgets\\itemviews\\addressbook\\addressbook.py", "repo_name": "pyside/pyside2-examples", "stars": 136, "license": "None", "language": "python", "size": 10571}
{"docstring": "\"\"\" Sets the data to be displayed and edited by our custom editor. \"\"\"\n", "func_signal": "def setEditorData(self, editor, index):\n", "code": "if index.column() == 3:\n    editor.starRating = StarRating(index.data())\nelse:\n    QStyledItemDelegate.setEditorData(self, editor, index)", "path": "examples\\widgets\\itemviews\\stardelegate\\stardelegate.py", "repo_name": "pyside/pyside2-examples", "stars": 136, "license": "None", "language": "python", "size": 10571}
{"docstring": "\"\"\" Get the data from our custom editor and stuffs it into the model.\n\"\"\"\n", "func_signal": "def setModelData(self, editor, model, index):\n", "code": "if index.column() == 3:\n    model.setData(index, editor.starRating.starCount)\nelse:\n    QStyledItemDelegate.setModelData(self, editor, model, index)", "path": "examples\\widgets\\itemviews\\stardelegate\\stardelegate.py", "repo_name": "pyside/pyside2-examples", "stars": 136, "license": "None", "language": "python", "size": 10571}
{"docstring": "'''\ntest to ensure basic find files functionality is working. \n'''\n", "func_signal": "def test_basic_search(qtbot, tmpdir):\n", "code": "tmpdir.join('video1.avi').ensure()\ntmpdir.join('video1.srt').ensure()\n\ntmpdir.join('video2.avi').ensure()\ntmpdir.join('video2.srt').ensure()\n\nwindow = Window() \nwindow.show()\nqtbot.addWidget(window)\n\nwindow.fileComboBox.clear()\nqtbot.keyClicks(window.fileComboBox, '*.avi')\n\nwindow.directoryComboBox.clear()\nqtbot.keyClicks(window.directoryComboBox, str(tmpdir))\n\nqtbot.mouseClick(window.findButton, QtCore.Qt.LeftButton)\n    \nassert window.filesTable.rowCount() == 2\nassert window.filesTable.item(0, 0).text() == 'video1.avi'\nassert window.filesTable.item(1, 0).text() == 'video2.avi'", "path": "examples\\widgets\\dialogs\\findfiles_test.py", "repo_name": "pyside/pyside2-examples", "stars": 136, "license": "None", "language": "python", "size": 10571}
{"docstring": "\"\"\" Only allow the user to remove or edit an item if an item\n    is actually selected.\n\"\"\"\n", "func_signal": "def updateActions(self, selection):\n", "code": "indexes = selection.indexes()\n\nif len(indexes) > 0:\n    self.removeAction.setEnabled(True)\n    self.editAction.setEnabled(True)\nelse:\n    self.removeAction.setEnabled(False)\n    self.editAction.setEnabled(False)", "path": "examples\\widgets\\itemviews\\addressbook\\addressbook.py", "repo_name": "pyside/pyside2-examples", "stars": 136, "license": "None", "language": "python", "size": 10571}
{"docstring": "\"\"\" Erm... commits the data and closes the editor. :) \"\"\"\n", "func_signal": "def commitAndCloseEditor(self):\n", "code": "editor = self.sender()\n\n# The commitData signal must be emitted when we've finished editing\n# and need to write our changed back to the model.\nself.commitData.emit(editor)\nself.closeEditor.emit(editor, QStyledItemDelegate.NoHint)", "path": "examples\\widgets\\itemviews\\stardelegate\\stardelegate.py", "repo_name": "pyside/pyside2-examples", "stars": 136, "license": "None", "language": "python", "size": 10571}
{"docstring": "\"\"\" Creates and returns the custom StarEditor object we'll use to edit \n    the StarRating.\n\"\"\"\n", "func_signal": "def createEditor(self, parent, option, index):\n", "code": "if index.column() == 3:\n    editor = StarEditor(parent)\n    editor.editingFinished.connect(self.commitAndCloseEditor)\n    return editor\nelse:\n    return QStyledItemDelegate.createEditor(self, parent, option, index)", "path": "examples\\widgets\\itemviews\\stardelegate\\stardelegate.py", "repo_name": "pyside/pyside2-examples", "stars": 136, "license": "None", "language": "python", "size": 10571}
{"docstring": "# return the field which the name data should be saved in.\n", "func_signal": "def nameField(self):\n", "code": "if not self.manager:\n    return \"\"\n\n#TODO: remove ()\ndefs = self.manager.detailDefinitions(QContactType().TypeContact)\nnameDef = defs[QContactName().DefinitionName]\nif QContactName().FieldCustomLabel in nameDef.fields():\n    return QContactName().FieldCustomLabel\nelif QContactName().FieldFirstName in nameDef.fields():\n    return QContactName().FieldFirstName\nelse:\n    return \"\"", "path": "mobility\\samplephonebook\\samplephonebook.py", "repo_name": "pyside/pyside2-examples", "stars": 136, "license": "None", "language": "python", "size": 10571}
{"docstring": "\"\"\" Calculate which star the user's mouse cursor is currently \n    hovering over.\n\"\"\"\n", "func_signal": "def starAtPosition(self, x):\n", "code": "star = (x / (self.starRating.sizeHint().width() /\n             self.starRating.maxStarCount)) + 1\nif (star <= 0) or (star > self.starRating.maxStarCount):\n    return -1\n\nreturn star", "path": "examples\\widgets\\itemviews\\stardelegate\\stareditor.py", "repo_name": "pyside/pyside2-examples", "stars": 136, "license": "None", "language": "python", "size": 10571}
{"docstring": "# hide background\n", "func_signal": "def onMaximizeFinished(self):\n", "code": "self._background.hide()\n\n# move menu to front to not block events\nself._item.setFlag(QGraphicsItem.ItemStacksBehindParent, False)\n\nself.emit(SIGNAL(\"maximizeFinished()\"))\n\n# detach from parent to avoid inherit transformations\nself._item.setParentItem(None)\nself._item.setPos(0, 0)", "path": "examples\\hyperui\\hyperuilib\\draggablepreview.py", "repo_name": "pyside/pyside2-examples", "stars": 136, "license": "None", "language": "python", "size": 10571}
{"docstring": "\"\"\" Initialize the editor object, making sure we can watch mouse\n    events.\n\"\"\"\n", "func_signal": "def __init__(self, parent=None):\n", "code": "super(StarEditor, self).__init__(parent)\n\nself.setMouseTracking(True)\nself.setAutoFillBackground(True)", "path": "examples\\widgets\\itemviews\\stardelegate\\stareditor.py", "repo_name": "pyside/pyside2-examples", "stars": 136, "license": "None", "language": "python", "size": 10571}
{"docstring": "\"\"\" Returns the size needed to display the item in a QSize object. \"\"\"\n", "func_signal": "def sizeHint(self, option, index):\n", "code": "if index.column() == 3:\n    starRating = StarRating(index.data())\n    return starRating.sizeHint()\nelse:\n    return QStyledItemDelegate.sizeHint(self, option, index)", "path": "examples\\widgets\\itemviews\\stardelegate\\stardelegate.py", "repo_name": "pyside/pyside2-examples", "stars": 136, "license": "None", "language": "python", "size": 10571}
{"docstring": "# add background item\n", "func_signal": "def setupInterface(self):\n", "code": "self._backgroundPixmap = Resource.pixmap(\"screen_unlock.png\")\nself._background = QGraphicsPixmapItem(self._backgroundPixmap, self)\nself._background.setPos(0, 0)\nself._background.setFlag(QGraphicsItem.ItemStacksBehindParent)\nself._background.setShapeMode(QGraphicsPixmapItem.BoundingRectShape)\n\n# add embedded widget\nself._item.setParentItem(self)\nself._item.setFlag(QGraphicsItem.ItemStacksBehindParent)\nself._item.setPos(self._leftMargin + self._border, self._topMargin + self._border)\nself._item.resize(QSizeF(self._screenSize))\n\n# resize to the background size\nself.resize(QSizeF(self._backgroundPixmap.size()))\n\nsw = self._screenSize.width()\nsh = self._screenSize.height()\n\nminimumScale = Resource.doubleValue(\"draggable-preview/minimum-scale\")\ndraggableScale = Resource.doubleValue(\"draggable-preview/first-zoom-scale\")\n\nself._draggablePos = self.SCALED_POS(sw, sh, draggableScale)\nminimumPos = self.SCALED_POS(sw, sh, minimumScale)\nmaximumPos = QPointF(-self._leftMargin - self._border, -self._topMargin - self._border)\n\nself._maximumOffset = minimumPos.y()\n\nself._machine = QStateMachine(self)\n\nself._minimizedState = QState()\nself._minimizedState.assignProperty(self, \"pos\", minimumPos)\nself._minimizedState.assignProperty(self, \"scale\", minimumScale)\n\nself._draggableState = QState()\nself._draggableState.assignProperty(self, \"pos\", self._draggablePos)\nself._draggableState.assignProperty(self, \"scale\", draggableScale)\n\nself._maximizedState = QState()\nself._maximizedState.assignProperty(self, \"pos\", maximumPos)\nself._maximizedState.assignProperty(self, \"scale\", 1.0)\n\nrestoreTime = Resource.intValue(\"draggable-preview/restore-time\")\nmaximizeTime = Resource.intValue(\"draggable-preview/maximize-time\")\nfirstZoomTime = Resource.intValue(\"draggable-preview/first-zoom-time\")\n\n# create minimized > draggable state transition\ntransition = self._minimizedState.addTransition(self, SIGNAL(\"draggableStarted()\"),\n                                                      self._draggableState)\n\ntransition.addAnimation(self.createAnimation(firstZoomTime))\n\n# create draggable > minimized state transition\ntransition = self._draggableState.addTransition(self, SIGNAL(\"minimizeStarted()\"),\n                                                      self._minimizedState)\ntransition.addAnimation(self.createAnimation(restoreTime))\n\n# create draggable > maximized state transition\ntransition = self._draggableState.addTransition(self, SIGNAL(\"maximizeStarted()\"),\n                                                      self._maximizedState)\ntransition.addAnimation(self.createAnimation(maximizeTime, SLOT(\"onMaximizeFinished()\")))\n\n# this is used just to update the final value when still animating\ntransition = self._draggableState.addTransition(self, SIGNAL(\"draggableUpdate()\"),\n                                                      self._draggableState)\ntransition.addAnimation(self.createAnimation(0))\n\n# add states\nself._machine.addState(self._minimizedState)\nself._machine.addState(self._draggableState)\nself._machine.addState(self._maximizedState)\n\nself.setPos(minimumPos)\nself.setScale(minimumScale)\n\nself._machine.setInitialState(self._minimizedState)\nself._machine.start()", "path": "examples\\hyperui\\hyperuilib\\draggablepreview.py", "repo_name": "pyside/pyside2-examples", "stars": 136, "license": "None", "language": "python", "size": 10571}
{"docstring": "# return the field which the name data should be saved in.\n", "func_signal": "def nameField(self):\n", "code": "if not self.manager:\n    return \"\"\n\ndefs = self.manager.detailDefinitions(QContactType.TypeContact)\nnameDef = defs[QContactName.DefinitionName]\nif QContactName.FieldCustomLabel in nameDef.fields():\n    return QContactName.FieldCustomLabel\nelif QContactName.FieldFirstName in nameDef.fields():\n    return QContactName.FieldFirstName\nelse:\n    return \"\"", "path": "mobility\\samplephonebook\\qml\\qmlsamplephonebook.py", "repo_name": "pyside/pyside2-examples", "stars": 136, "license": "None", "language": "python", "size": 10571}
{"docstring": "\"\"\" Helper function to save typing when populating menus \n    with action. \n\"\"\"\n", "func_signal": "def createAction(self, text, menu, slot):\n", "code": "action = QAction(text, self)\nmenu.addAction(action)\naction.triggered.connect(slot)\nreturn action", "path": "examples\\widgets\\itemviews\\addressbook\\addressbook.py", "repo_name": "pyside/pyside2-examples", "stars": 136, "license": "None", "language": "python", "size": 10571}
{"docstring": "# tries to set all the settings picked.\n", "func_signal": "def test(self):\n", "code": "self.testResult.clear()\nif not self.deviceInfo.isNull():\n    if self.deviceInfo.isFormatSupported(self.settings):\n        self.testResult.setText(self.tr(\"Success\"))\n        self.nearestFreq.setText(\"\")\n        self.nearestChannel.setText(\"\")\n        self.nearestCodec.setText(\"\")\n        self.nearestSampleSize.setText(\"\")\n        self.nearestSampleType.setText(\"\")\n        self.nearestEndian.setText(\"\")\n    else:\n        nearest = self.deviceInfo.nearestFormat(self.settings)\n        self.testResult.setText(self.tr(\"Failed\"))\n        self.nearestFreq.setText(str(nearest.frequency()))\n        self.nearestChannel.setText(str(nearest.channels()))\n        self.nearestCodec.setText(nearest.codec())\n        self.nearestSampleSize.setText(str(nearest.sampleSize()))\n        self.nearestSampleType.setText(sampletoString(nearest.sampleType()))\n        self. nearestEndian.setText(byteOrdertoString(nearest.byteOrder()))\nelse:\n    self.testResult.setText(self.tr(\"No Device\"))", "path": "mobility\\audiodevices\\audiodevices.py", "repo_name": "pyside/pyside2-examples", "stars": 136, "license": "None", "language": "python", "size": 10571}
{"docstring": "\"\"\"\nuse a while-looop to click an element. For stubborn links\nand general unexpected browser errors.\n\"\"\"\n", "func_signal": "def robust_click(driver, delay, selector):\n", "code": "try:\n    driver.find_element_by_xpath(selector).click()\nexcept Exception as e:\n    print(\"  The job post link was likely hidden,\\n    An \" \\\n            \"error was encountered while attempting to click link\" \\\n            \"\\n    {}\".format(e))\n    attempts = 1\n    clicked = False\n    while not clicked:\n        try:\n            driver.find_element_by_xpath(selector).click()\n        except Exception as e:\n            pass\n        else:\n            clicked = True\n            print(\"  Successfully navigated to job post page \"\\\n                        \"after {} attempts\".format(attempts))\n        finally:\n            attempts += 1\n            if attempts % 100 == 0:\n                print(\"--------------  refreshing page\")\n                driver.refresh()\n                time.sleep(5)\n            if attempts > 10**3:\n                print(selector)\n                print(\"  robust_click method failed after too many attempts\")\n                break", "path": "client.py", "repo_name": "kirkhunter/linkedin-jobs-scraper", "stars": 170, "license": "mit", "language": "python", "size": 32}
{"docstring": "\"\"\"\nexecute the job search by entering job and location information.\nThe location is pre-filled with text, so we must clear it before\nentering our search.\n\"\"\"\n", "func_signal": "def enter_search_keys(self):\n", "code": "driver = self.driver\nWebDriverWait(driver, 120).until(\n    EC.presence_of_element_located(\n        (By.ID, \"keyword-search-box\")\n    )\n)\n# Enter search criteria\nelem = driver.find_element_by_id(\"keyword-search-box\")\nelem.send_keys(self.keyword)\n# clear the text in the location box then enter location\nelem = driver.find_element_by_id(\"location-search-box\").clear()\nelem = driver.find_element_by_id(\"location-search-box\")\nelem.send_keys(self.location)\nelem.send_keys(Keys.RETURN)\ntime.sleep(3)", "path": "client.py", "repo_name": "kirkhunter/linkedin-jobs-scraper", "stars": 170, "license": "mit", "language": "python", "size": 32}
{"docstring": "\"\"\"\nselect the appropriate user-defined search radius from the \ndropdown window\n\"\"\"\n", "func_signal": "def adjust_search_radius(driver, search_radius):\n", "code": "if search_radius == '50':\n    return\ndistance_selector = \"select#advs-distance > option[value='{}']\"\ndistance_selector = distance_selector.format(search_radius)\ntry:\n    driver.find_element_by_css_selector(distance_selector).click()\nexcept Exception as e:\n    print(e)\nelse:\n    time.sleep(3)\n    try:\n        driver.find_element_by_css_selector(\"input.submit-advs\").click()\n        time.sleep(3)\n    except Exception as e:\n        print(e)", "path": "client.py", "repo_name": "kirkhunter/linkedin-jobs-scraper", "stars": 170, "license": "mit", "language": "python", "size": 32}
{"docstring": "\"\"\"sort results by either relevance or date posted\"\"\"\n", "func_signal": "def sort_results_by(driver, sorting_criteria):\n", "code": "if sorting_criteria.lower() == 'relevance':\n    return\nbutton = '//select[@id=\"jserp-sort-select\"]'\noption_path = '//option[@value=\"DD\"]'\ntime.sleep(3)\ntry:\n    driver.find_element_by_xpath(button).click()\nexcept Exception as e:\n    print(e)\n    print(\"  Could not sort results by '{}'\".format(sorting_criteria))\nelse:\n    time.sleep(3)\n    try:\n        driver.find_element_by_xpath(option_path).click()\n    except Exception as e:\n        print(\"  Could not select 'sort by' option\")\n    else:\n        time.sleep(3)", "path": "client.py", "repo_name": "kirkhunter/linkedin-jobs-scraper", "stars": 170, "license": "mit", "language": "python", "size": 32}
{"docstring": "\"\"\"\nnavigate to the next page of search results. If an error is encountered\nthen the process ends or new search criteria are entered as the current \nsearch results may have been exhausted.\n\"\"\"\n", "func_signal": "def next_results_page(driver, delay):\n", "code": "try:\n    # wait for the next page button to load\n    print(\"  Moving to the next page of search results... \\n\" \\\n            \"  If search results are exhausted, will wait {} seconds \" \\\n            \"then either execute new search or quit\".format(delay))\n    wait_for_clickable_element_css(driver, delay, \"a.next-btn\")\n    # navigate to next page\n    driver.find_element_by_css_selector(\"a.next-btn\").click()\nexcept Exception as e:\n    print (\"\\nFailed to click next page link; Search results \" \\\n                            \"may have been exhausted\\n{}\".format(e))\n    raise ValueError(\"Next page link not detected; search results exhausted\")\nelse:\n    # wait until the first job post button has loaded\n    first_job_button = \"a.job-title-link\"\n    # wait for the first job post button to load\n    wait_for_clickable_element_css(driver, delay, first_job_button)", "path": "client.py", "repo_name": "kirkhunter/linkedin-jobs-scraper", "stars": 170, "license": "mit", "language": "python", "size": 32}
{"docstring": "\"\"\"\ngo to a specific results page in case of an error, can restart \nthe webdriver where the error occurred.\n\"\"\"\n", "func_signal": "def go_to_specific_results_page(driver, delay, results_page):\n", "code": "if results_page < 2:\n    return\ncurrent_page = 1\nfor i in range(results_page):\n    current_page += 1\n    time.sleep(5)\n    try:\n        next_results_page(driver, delay)\n        print(\"\\n**************************************************\")\n        print(\"\\n\\n\\nNavigating to results page {}\" \\\n              \"\\n\\n\\n\".format(current_page))\n    except ValueError:\n        print(\"**************************************************\")\n        print(\"\\n\\n\\n\\n\\nSearch results exhausted\\n\\n\\n\\n\\n\")", "path": "client.py", "repo_name": "kirkhunter/linkedin-jobs-scraper", "stars": 170, "license": "mit", "language": "python", "size": 32}
{"docstring": "\"\"\"\noutput the current job title, company, job id, then write\nthe scraped data to file\n\"\"\"\n", "func_signal": "def write_line_to_file(filename, data):\n", "code": "job_title = data[\"job_info\"][\"job_title\"]\ncompany   = data[\"job_info\"][\"company\"]\njob_id    = data[\"job_info\"][\"job_id\"]\nmessage = u\"Writing data to file for job listing:\"\nmessage += \"\\n  {}  {};   job id  {}\\n\"\ntry:\n    print(message.format(job_title, company, job_id))\nexcept Exception as e:\n    print(\"Encountered a unicode encode error while attempting to print \" \\\n                \"the job post information;  job id {}\".format(job_id))\nwith open(filename, \"a\") as f:\n    f.write(json.dumps(data) + '\\n')", "path": "client.py", "repo_name": "kirkhunter/linkedin-jobs-scraper", "stars": 170, "license": "mit", "language": "python", "size": 32}
{"docstring": "\"\"\"select a specific date range for job postings\"\"\"\n", "func_signal": "def adjust_date_range(driver, date_range):\n", "code": "if date_range == 'All':\n    return\nindex = ['', 'All', '1', '2-7', '8-14', '15-30'].index(date_range)\nbutton_path = \"html/body/div[3]/div/div[2]/div[1]/div[4]/form/div/ul/li\" \\\n              \"[3]/fieldset/button\"\ndate_path = \"html/body/div[3]/div/div[2]/div[1]/div[4]/form/div/ul/li\" \\\n            \"[3]/fieldset/div/ol/li[{}]/div/label\".format(index)\nattempts = 1\nwhile True:\n    try:\n        elem = driver.find_element_by_xpath(button_path)\n        time.sleep(3)\n    except Exception as e:\n        attempts += 1\n        if attempts > 25:\n            break\n    else:\n        elem.click()\n        time.sleep(3)\n        driver.find_element_by_xpath(date_path).click()\n        time.sleep(3)\n        break", "path": "client.py", "repo_name": "kirkhunter/linkedin-jobs-scraper", "stars": 170, "license": "mit", "language": "python", "size": 32}
{"docstring": "\"\"\"sort results by either relevance or date posted\"\"\"\n", "func_signal": "def customize_search_results(self):\n", "code": "adjust_date_range(self.driver, self.date_range)\nadjust_salary_range(self.driver, self.salary_range)\n# adjust_search_radius(self.driver, self.search_radius) # deprecated\n# scroll to top of page so the sorting menu is in view\nself.driver.execute_script(\"window.scrollTo(0, 0);\")\nsort_results_by(self.driver, self.sort_by)", "path": "client.py", "repo_name": "kirkhunter/linkedin-jobs-scraper", "stars": 170, "license": "mit", "language": "python", "size": 32}
{"docstring": "\"\"\"print the number of search results to console\"\"\"\n# scroll to top of page so first result is in view\n", "func_signal": "def print_num_search_results(driver, keyword, location):\n", "code": "driver.execute_script(\"window.scrollTo(0, 0);\")\nselector = \"div.results-context div strong\"\ntry:\n    num_results = driver.find_element_by_css_selector(selector).text\nexcept Exception as e:\n    num_results = ''\nprint(\"**************************************************\")\nprint(\"\\n\\n\\n\\n\\nSearching  {}  results for  '{}'  jobs in  '{}' \" \\\n        \"\\n\\n\\n\\n\\n\".format(num_results, keyword, location))", "path": "client.py", "repo_name": "kirkhunter/linkedin-jobs-scraper", "stars": 170, "license": "mit", "language": "python", "size": 32}
{"docstring": "\"\"\"use WebDriverWait to wait for an element to become clickable\"\"\"\n", "func_signal": "def wait_for_clickable_element(driver, delay, selector):\n", "code": "obj = WebDriverWait(driver, delay).until(\n        EC.element_to_be_clickable(\n            (By.XPATH, selector)\n        )\n    )\nreturn obj", "path": "client.py", "repo_name": "kirkhunter/linkedin-jobs-scraper", "stars": 170, "license": "mit", "language": "python", "size": 32}
{"docstring": "\"\"\"\nnavigate to the 'Jobs' page since it is a convenient page to \nenter a custom job search.\n\"\"\"\n# Click the Jobs search page\n", "func_signal": "def navigate_to_jobs_page(self):\n", "code": "jobs_link_clickable = False\nattempts = 1\nurl = \"https://www.linkedin.com/jobs/?trk=nav_responsive_sub_nav_jobs\"\nwhile not jobs_link_clickable:\n    try:\n        self.driver.get(url)\n    except Exception as e:\n        attempts += 1\n        if attempts > 10**3: \n            print(\"  jobs page not detected\")\n            break\n        pass\n    else:\n        print(\"**************************************************\")\n        print (\"\\n\\n\\nSuccessfully navigated to jobs search page\\n\\n\\n\")\n        jobs_link_clickable = True", "path": "client.py", "repo_name": "kirkhunter/linkedin-jobs-scraper", "stars": 170, "license": "mit", "language": "python", "size": 32}
{"docstring": "\"\"\" wait for css selector to load \"\"\"\n", "func_signal": "def robust_wait_for_clickable_element(driver, delay, selector):\n", "code": "clickable = False\nattempts = 1\ntry:\n    driver.find_element_by_xpath(selector)\nexcept Exception as e:\n    print(\"  Selector not found: {}\".format(selector))\nelse:\n    while not clickable:\n        try:\n            # wait for job post link to load\n            wait_for_clickable_element(driver, delay, selector)\n        except Exception as e:\n            print(\"  {}\".format(e))\n            attempts += 1\n            if attempts % 100 == 0:\n                driver.refresh()\n            if attempts > 10**3: \n                print(\"  \\nrobust_wait_for_clickable_element failed \" \\\n                                \"after too many attempts\\n\")\n                break\n            pass\n        else:\n            clickable = True", "path": "client.py", "repo_name": "kirkhunter/linkedin-jobs-scraper", "stars": 170, "license": "mit", "language": "python", "size": 32}
{"docstring": "\"\"\"\nverify that the link selector is present and print the search \ndetails to console. This method is particularly useful for catching\nthe last link on the last page of search results\n\"\"\"\n", "func_signal": "def link_is_present(driver, delay, selector, index, results_page):\n", "code": "try:\n    WebDriverWait(driver, delay).until(\n        EC.presence_of_element_located(\n            (By.XPATH, selector)\n        )\n    )\n    print(\"**************************************************\")\n    print(\"\\nScraping data for result  {}\" \\\n            \"  on results page  {} \\n\".format(index, results_page))\nexcept Exception as e:\n    print(e)\n    if index < 25:\n        print(\"\\nWas not able to wait for job_selector to load. Search \" \\\n                \"results may have been exhausted.\")\n        return True\n    else:\n        return False\nelse:\n    return True", "path": "client.py", "repo_name": "kirkhunter/linkedin-jobs-scraper", "stars": 170, "license": "mit", "language": "python", "size": 32}
{"docstring": "\"\"\"\nget the full date along with the hour of the search, just for \ncompleteness. Allows us to solve for of the original post \ndate.\n\"\"\"\n", "func_signal": "def get_date_time():\n", "code": "now   =  datetime.datetime.now()\nmonth =  str(now.month) if now.month > 9 else '0' + str(now.month)\nday   =  str(now.day) if now.day > 9 else '0' + str(now.day)\ndate  =  ''.join(str(t) for t in [now.year, month, day, now.time().hour])\nreturn date", "path": "client.py", "repo_name": "kirkhunter/linkedin-jobs-scraper", "stars": 170, "license": "mit", "language": "python", "size": 32}
{"docstring": "\"\"\"\ncheck results page for the search suggestion box,\nas this causes some errors in navigate search results.\n\"\"\"\n", "func_signal": "def search_suggestion_box_is_present(driver, selector, index, results_page):\n", "code": "if (index == 1) and (results_page == 1):\n    try:\n        # This try-except statement allows us to avoid the \n        # problems cause by the LinkedIn search suggestion box\n        driver.find_element_by_css_selector(\"div.suggested-search.bd\")\n    except Exception as e:\n        pass\n    else:\n        return True\nelse:\n    return False", "path": "client.py", "repo_name": "kirkhunter/linkedin-jobs-scraper", "stars": 170, "license": "mit", "language": "python", "size": 32}
{"docstring": "\"\"\"adjust the salary range, default is All salaries\"\"\"\n", "func_signal": "def adjust_salary_range(driver, salary):\n", "code": "if salary == 'All': \n    return\nindex = ['', 'All', '40+', '60+', '80+', '100+', \n                    '120+', '160+', '180+', '200+'].index(salary)\nsalary_button = \"html/body/div[3]/div/div[2]/div[1]/div[4]/form/div/ul/\" \\\n                                \"li[4]/fieldset/button\"\nsalary_path = \"html/body/div[3]/div/div[2]/div[1]/div[4]/\" \\\n              \"form/div/ul/li[4]/fieldset/div[1]/ol/li[{}\" \\\n              \"]/div/label\".format(index)\nattempts = 1\nwhile True:\n    try:\n        elem = driver.find_element_by_xpath(salary_button)\n        time.sleep(3)\n    except Exception as e:\n        attempts += 1\n        if attempts > 25: \n            break\n    else:\n        elem.click()\n        time.sleep(3)\n        driver.find_element_by_xpath(salary_path).click()\n        break", "path": "client.py", "repo_name": "kirkhunter/linkedin-jobs-scraper", "stars": 170, "license": "mit", "language": "python", "size": 32}
{"docstring": "\"\"\"login to linkedin then wait 3 seconds for page to load\"\"\"\n# Enter login credentials\n", "func_signal": "def login(self):\n", "code": "WebDriverWait(self.driver, 120).until(\n    EC.element_to_be_clickable(\n        (By.ID, \"session_key-login\")\n    )\n)\nelem = self.driver.find_element_by_id(\"session_key-login\")\nelem.send_keys(self.username)\nelem = self.driver.find_element_by_id(\"session_password-login\")\nelem.send_keys(self.password)\n# Enter credentials with Keys.RETURN\nelem.send_keys(Keys.RETURN)\n# Wait a few seconds for the page to load\ntime.sleep(3)", "path": "client.py", "repo_name": "kirkhunter/linkedin-jobs-scraper", "stars": 170, "license": "mit", "language": "python", "size": 32}
{"docstring": "\"\"\"use WebDriverWait to wait for an element to become clickable\"\"\"\n", "func_signal": "def wait_for_clickable_element_css(driver, delay, selector):\n", "code": "obj = WebDriverWait(driver, delay).until(\n        EC.element_to_be_clickable(\n            (By.CSS_SELECTOR, selector)\n        )\n    )\nreturn obj", "path": "client.py", "repo_name": "kirkhunter/linkedin-jobs-scraper", "stars": 170, "license": "mit", "language": "python", "size": 32}
{"docstring": "\"\"\"\nscrape postings for all pages in search results\n\"\"\"\n", "func_signal": "def navigate_search_results(self):\n", "code": "driver = self.driver\nsearch_results_exhausted = False\nresults_page = self.results_page\ndelay = 60\ndate = get_date_time()\n# css elements to view job pages\nlist_element_tag = '/descendant::a[@class=\"job-title-link\"]['\nprint_num_search_results(driver, self.keyword, self.location)\n# go to a specific results page number if one is specified\ngo_to_specific_results_page(driver, delay, results_page)\nresults_page = results_page if results_page > 1 else 1\n\nwhile not search_results_exhausted:\n    for i in range(1,26):  # 25 results per page\n        # define the css selector for the blue 'View' button for job i\n        job_selector = list_element_tag + str(i) + ']'\n        if search_suggestion_box_is_present(driver, \n                                    job_selector, i, results_page):\n            continue\n        # wait for the selector for the next job posting to load.\n        # if on last results page, then throw exception as job_selector \n        # will not be detected on the page\n        if not link_is_present(driver, delay, \n                                 job_selector, i, results_page):\n            continue\n        robust_wait_for_clickable_element(driver, delay, job_selector)\n        extract_transform_load(driver,\n                               delay,\n                               job_selector,\n                               date,\n                               self.keyword,\n                               self.location,\n                               self.filename)\n    # attempt to navigate to the next page of search results\n    # if the link is not present, then the search results have been \n    # exhausted\n    try:\n        next_results_page(driver, delay)\n        print(\"\\n**************************************************\")\n        print(\"\\n\\n\\nNavigating to results page  {}\" \\\n              \"\\n\\n\\n\".format(results_page + 1))\n    except ValueError:\n        search_results_exhausted = True\n        print(\"**************************************************\")\n        print(\"\\n\\n\\n\\n\\nSearch results exhausted\\n\\n\\n\\n\\n\")\n    else:\n        results_page += 1", "path": "client.py", "repo_name": "kirkhunter/linkedin-jobs-scraper", "stars": 170, "license": "mit", "language": "python", "size": 32}
{"docstring": "\"\"\"\nWrite a PNG chunk to the output file, including length and\nchecksum.\n\"\"\"\n\n# http://www.w3.org/TR/PNG/#5Chunk-layout\n", "func_signal": "def write_chunk(outfile, tag, data=''):\n", "code": "outfile.write(struct.pack(\"!I\", len(data)))\noutfile.write(tag)\noutfile.write(data)\nchecksum = zlib.crc32(tag)\nchecksum = zlib.crc32(data, checksum)\noutfile.write(struct.pack(\"!i\", checksum))", "path": "src\\auxil\\png.py", "repo_name": "mortcanty/CRCPython", "stars": 146, "license": "None", "language": "python", "size": 975}
{"docstring": "\"\"\"Test that the command line tool can read PGM files.\"\"\"\n", "func_signal": "def testPGMin(self):\n", "code": "def do():\n    return _main(['testPGMin'])\ns = StringIO()\ns.write('P5 2 2 3\\n')\ns.write('\\x00\\x01\\x02\\x03')\ns.flush()\ns.seek(0)\no = StringIO()\ntestWithIO(s, o, do)\nr = Reader(bytes=o.getvalue())\nx,y,pixels,meta = r.read()\nself.assert_(r.greyscale)\nself.assertEqual(r.bitdepth, 2)", "path": "src\\auxil\\png.py", "repo_name": "mortcanty/CRCPython", "stars": 146, "license": "None", "language": "python", "size": 975}
{"docstring": "\"\"\"numpy uint16.\"\"\"\n\n", "func_signal": "def testNumpyuint16(self):\n", "code": "try:\n    import numpy\nexcept ImportError:\n    print >>sys.stderr, \"skipping numpy test\"\n    return\n\nrows = [map(numpy.uint16, range(0,0x10000,0x5555))]\nb = topngbytes('numpyuint16.png', rows, 4, 1,\n    greyscale=True, alpha=False, bitdepth=16)", "path": "src\\auxil\\png.py", "repo_name": "mortcanty/CRCPython", "stars": 146, "license": "None", "language": "python", "size": 975}
{"docstring": "\"\"\"numpy bool.\"\"\"\n\n", "func_signal": "def testNumpybool(self):\n", "code": "try:\n    import numpy\nexcept ImportError:\n    print >>sys.stderr, \"skipping numpy test\"\n    return\n\nrows = [map(numpy.bool, [0,1])]\nb = topngbytes('numpybool.png', rows, 2, 1,\n    greyscale=True, alpha=False, bitdepth=1)", "path": "src\\auxil\\png.py", "repo_name": "mortcanty/CRCPython", "stars": 146, "license": "None", "language": "python", "size": 975}
{"docstring": "\"\"\"\nExtract the image metadata by reading the initial part of the PNG\nfile up to the start of the ``IDAT`` chunk.  All the chunks that\nprecede the ``IDAT`` chunk are read and either processed for\nmetadata or discarded.\n\"\"\"\n\n", "func_signal": "def preamble(self):\n", "code": "self.validate_signature()\n\nwhile True:\n    if not self.atchunk:\n        self.atchunk = self.chunklentype()\n        if self.atchunk is None:\n            raise FormatError(\n              'This PNG file has no IDAT chunks.')\n    if self.atchunk[1] == 'IDAT':\n        return\n    self.process_chunk()", "path": "src\\auxil\\png.py", "repo_name": "mortcanty/CRCPython", "stars": 146, "license": "None", "language": "python", "size": 975}
{"docstring": "\"\"\"\nWrite a PNG image to the output file.\n\n\tMost users are expected to find the :meth:`write` or\n\t:meth:`write_array` method more convenient.\n\n\tThe rows should be given to this method in the order that\n\tthey appear in the output file.  For straightlaced images,\n\tthis is the usual top to bottom ordering, but for interlaced\n\timages the rows should have already been interlaced before\n\tpassing them to this function.\n\n\t`rows` should be an iterable that yields each row.  When\n`packed` is ``False`` the rows should be in boxed row flat pixel\nformat; when `packed` is ``True`` each row should be a packed\nsequence of bytes.\n\n\"\"\"\n\n# http://www.w3.org/TR/PNG/#5PNG-file-signature\n", "func_signal": "def write_passes(self, outfile, rows, packed=False):\n", "code": "outfile.write(_signature)\n\n# http://www.w3.org/TR/PNG/#11IHDR\nwrite_chunk(outfile, 'IHDR',\n            struct.pack(\"!2I5B\", self.width, self.height,\n                        self.bitdepth, self.color_type,\n                        0, 0, self.interlace))\n\n# See :chunk:order\n# http://www.w3.org/TR/PNG/#11gAMA\nif self.gamma is not None:\n    write_chunk(outfile, 'gAMA',\n                struct.pack(\"!L\", int(round(self.gamma*1e5))))\n\n# See :chunk:order\n# http://www.w3.org/TR/PNG/#11sBIT\nif self.rescale:\n    write_chunk(outfile, 'sBIT',\n        struct.pack('%dB' % self.planes,\n                    *[self.rescale[0]]*self.planes))\n\n# :chunk:order: Without a palette (PLTE chunk), ordering is\n# relatively relaxed.  With one, gAMA chunk must precede PLTE\n# chunk which must precede tRNS and bKGD.\n# See http://www.w3.org/TR/PNG/#5ChunkOrdering\nif self.palette:\n    p,t = self.make_palette()\n    write_chunk(outfile, 'PLTE', p)\n    if t:\n        # tRNS chunk is optional.  Only needed if palette entries\n        # have alpha.\n        write_chunk(outfile, 'tRNS', t)\n\n# http://www.w3.org/TR/PNG/#11tRNS\nif self.transparent is not None:\n    if self.greyscale:\n        write_chunk(outfile, 'tRNS',\n                    struct.pack(\"!1H\", *self.transparent))\n    else:\n        write_chunk(outfile, 'tRNS',\n                    struct.pack(\"!3H\", *self.transparent))\n\n# http://www.w3.org/TR/PNG/#11bKGD\nif self.background is not None:\n    if self.greyscale:\n        write_chunk(outfile, 'bKGD',\n                    struct.pack(\"!1H\", *self.background))\n    else:\n        write_chunk(outfile, 'bKGD',\n                    struct.pack(\"!3H\", *self.background))\n\n# http://www.w3.org/TR/PNG/#11IDAT\nif self.compression is not None:\n    compressor = zlib.compressobj(self.compression)\nelse:\n    compressor = zlib.compressobj()\n\n# Choose an extend function based on the bitdepth.  The extend\n# function packs/decomposes the pixel values into bytes and\n# stuffs them onto the data array.\ndata = array('B')\nif self.bitdepth == 8 or packed:\n    extend = data.extend\nelif self.bitdepth == 16:\n    # Decompose into bytes\n    def extend(sl):\n        fmt = '!%dH' % len(sl)\n        data.extend(array('B', struct.pack(fmt, *sl)))\nelse:\n    # Pack into bytes\n    assert self.bitdepth < 8\n    # samples per byte\n    spb = int(8/self.bitdepth)\n    def extend(sl):\n        a = array('B', sl)\n        # Adding padding bytes so we can group into a whole\n        # number of spb-tuples.\n        l = float(len(a))\n        extra = math.ceil(l / float(spb))*spb - l\n        a.extend([0]*int(extra))\n        # Pack into bytes\n        l = group(a, spb)\n        l = map(lambda e: reduce(lambda x,y:\n                                   (x << self.bitdepth) + y, e), l)\n        data.extend(l)\nif self.rescale:\n    oldextend = extend\n    factor = \\\n      float(2**self.rescale[1]-1) / float(2**self.rescale[0]-1)\n    def extend(sl):\n        oldextend(map(lambda x: int(round(factor*x)), sl))\n\n# Build the first row, testing mostly to see if we need to\n# changed the extend function to cope with NumPy integer types\n# (they cause our ordinary definition of extend to fail, so we\n# wrap it).  See\n# http://code.google.com/p/pypng/issues/detail?id=44\nenumrows = enumerate(rows)\ndel rows\n\n# First row's filter type.\ndata.append(0)\n# :todo: Certain exceptions in the call to ``.next()`` or the\n# following try would indicate no row data supplied.\n# Should catch.\ni,row = enumrows.next()\ntry:\n    # If this fails...\n    extend(row)\nexcept:\n    # ... try a version that converts the values to int first.\n    # Not only does this work for the (slightly broken) NumPy\n    # types, there are probably lots of other, unknown, \"nearly\"\n    # int types it works for.\n    def wrapmapint(f):\n        return lambda sl: f(map(int, sl))\n    extend = wrapmapint(extend)\n    del wrapmapint\n    extend(row)\n\nfor i,row in enumrows:\n    # Add \"None\" filter type.  Currently, it's essential that\n    # this filter type be used for every scanline as we do not\n    # mark the first row of a reduced pass image; that means we\n    # could accidentally compute the wrong filtered scanline if\n    # we used \"up\", \"average\", or \"paeth\" on such a line.\n    data.append(0)\n    extend(row)\n    if len(data) > self.chunk_limit:\n        compressed = compressor.compress(tostring(data))\n        if len(compressed):\n            # print >> sys.stderr, len(data), len(compressed)\n            write_chunk(outfile, 'IDAT', compressed)\n        # Because of our very witty definition of ``extend``,\n        # above, we must re-use the same ``data`` object.  Hence\n        # we use ``del`` to empty this one, rather than create a\n        # fresh one (which would be my natural FP instinct).\n        del data[:]\nif len(data):\n    compressed = compressor.compress(tostring(data))\nelse:\n    compressed = ''\nflushed = compressor.flush()\nif len(compressed) or len(flushed):\n    # print >> sys.stderr, len(data), len(compressed), len(flushed)\n    write_chunk(outfile, 'IDAT', compressed + flushed)\n# http://www.w3.org/TR/PNG/#11IEND\nwrite_chunk(outfile, 'IEND')\nreturn i+1", "path": "src\\auxil\\png.py", "repo_name": "mortcanty/CRCPython", "stars": 146, "license": "None", "language": "python", "size": 975}
{"docstring": "\"\"\"\nRead the next PNG chunk from the input file; returns type (as a 4\ncharacter string) and data.  If the optional `seek` argument is\nspecified then it will keep reading chunks until it either runs\nout of file or finds the type specified by the argument.  Note\nthat in general the order of chunks in PNGs is unspecified, so\nusing `seek` can cause you to miss chunks.\n\"\"\"\n\n", "func_signal": "def chunk(self, seek=None):\n", "code": "self.validate_signature()\n\nwhile True:\n    # http://www.w3.org/TR/PNG/#5Chunk-layout\n    if not self.atchunk:\n        self.atchunk = self.chunklentype()\n    length,type = self.atchunk\n    self.atchunk = None\n    data = self.file.read(length)\n    if len(data) != length:\n        raise ChunkError('Chunk %s too short for required %i octets.'\n          % (type, length))\n    checksum = self.file.read(4)\n    if len(checksum) != 4:\n        raise ValueError('Chunk %s too short for checksum.', tag)\n    if seek and type != seek:\n        continue\n    verify = zlib.crc32(type)\n    verify = zlib.crc32(data, verify)\n    # Whether the output from zlib.crc32 is signed or not varies\n    # according to hideous implementation details, see\n    # http://bugs.python.org/issue1202 .\n    # We coerce it to be positive here (in a way which works on\n    # Python 2.3 and older).\n    verify &= 2**32 - 1\n    verify = struct.pack('!I', verify)\n    if checksum != verify:\n        # print repr(checksum)\n        (a, ) = struct.unpack('!I', checksum)\n        (b, ) = struct.unpack('!I', verify)\n        raise ChunkError(\n          \"Checksum error in %s chunk: 0x%08X != 0x%08X.\" %\n          (type, a, b))\n    return type, data", "path": "src\\auxil\\png.py", "repo_name": "mortcanty/CRCPython", "stars": 146, "license": "None", "language": "python", "size": 975}
{"docstring": "\"\"\"Liberally convert from hex string to binary string.\"\"\"\n", "func_signal": "def _dehex(s):\n", "code": "import re\n\n# Remove all non-hexadecimal digits\ns = re.sub(r'[^a-fA-F\\d]', '', s)\nreturn s.decode('hex')", "path": "src\\auxil\\png.py", "repo_name": "mortcanty/CRCPython", "stars": 146, "license": "None", "language": "python", "size": 975}
{"docstring": "\"\"\"If signature (header) has not been read then read and\nvalidate it; otherwise do nothing.\n\"\"\"\n\n", "func_signal": "def validate_signature(self):\n", "code": "if self.signature:\n    return\nself.signature = self.file.read(8)\nif self.signature != _signature:\n    raise FormatError(\"PNG file has invalid signature.\")", "path": "src\\auxil\\png.py", "repo_name": "mortcanty/CRCPython", "stars": 146, "license": "None", "language": "python", "size": 975}
{"docstring": "\"\"\"Test the dictionary returned by a `read` method can be used\nas args for :meth:`Writer`.\n\"\"\"\n", "func_signal": "def testWinfo(self):\n", "code": "r = Reader(bytes=_pngsuite['basn2c16'])\ninfo = r.read()[3]\nw = Writer(**info)", "path": "src\\auxil\\png.py", "repo_name": "mortcanty/CRCPython", "stars": 146, "license": "None", "language": "python", "size": 975}
{"docstring": "\"\"\"Test that reading an interlaced PNG yields each row as an\narray.\"\"\"\n", "func_signal": "def testInterlacedArray(self):\n", "code": "r = Reader(bytes=_pngsuite['basi0g08'])\nlist(r.read()[2])[0].tostring", "path": "src\\auxil\\png.py", "repo_name": "mortcanty/CRCPython", "stars": 146, "license": "None", "language": "python", "size": 975}
{"docstring": "\"\"\"Create the byte sequences for a ``PLTE`` and if necessary a\n``tRNS`` chunk.  Returned as a pair (*p*, *t*).  *t* will be\n``None`` if no ``tRNS`` chunk is necessary.\n\"\"\"\n\n", "func_signal": "def make_palette(self):\n", "code": "p = array('B')\nt = array('B')\n\nfor x in self.palette:\n    p.extend(x[0:3])\n    if len(x) > 3:\n        t.append(x[3])\np = tostring(p)\nt = tostring(t)\nif t:\n    return p,t\nreturn p,None", "path": "src\\auxil\\png.py", "repo_name": "mortcanty/CRCPython", "stars": 146, "license": "None", "language": "python", "size": 975}
{"docstring": "\"\"\"Check a palette argument (to the :class:`Writer` class) for validity.\nReturns the palette as a list if okay; raises an exception otherwise.\n\"\"\"\n\n# None is the default and is allowed.\n", "func_signal": "def check_palette(palette):\n", "code": "if palette is None:\n    return None\n\np = list(palette)\nif not (0 < len(p) <= 256):\n    raise ValueError(\"a palette must have between 1 and 256 entries\")\nseen_triple = False\nfor i,t in enumerate(p):\n    if len(t) not in (3,4):\n        raise ValueError(\n          \"palette entry %d: entries must be 3- or 4-tuples.\" % i)\n    if len(t) == 3:\n        seen_triple = True\n    if seen_triple and len(t) == 4:\n        raise ValueError(\n          \"palette entry %d: all 4-tuples must precede all 3-tuples\" % i)\n    for x in t:\n        if int(x) != x or not(0 <= x <= 255):\n            raise ValueError(\n              \"palette entry %d: values must be integer: 0 <= x <= 255\" % i)\nreturn p", "path": "src\\auxil\\png.py", "repo_name": "mortcanty/CRCPython", "stars": 146, "license": "None", "language": "python", "size": 975}
{"docstring": "\"\"\"Checks that a colour argument for transparent or\nbackground options is the right form.  Also \"corrects\" bare\nintegers to 1-tuples.\n\"\"\"\n\n", "func_signal": "def check_color(c, which):\n", "code": "if c is None:\n    return c\nif greyscale:\n    try:\n        l = len(c)\n    except TypeError:\n        c = (c,)\n    if len(c) != 1:\n        raise ValueError(\"%s for greyscale must be 1-tuple\" %\n            which)\n    if not isinteger(c[0]):\n        raise ValueError(\n            \"%s colour for greyscale must be integer\" %\n            which)\nelse:\n    if not (len(c) == 3 and\n            isinteger(c[0]) and\n            isinteger(c[1]) and\n            isinteger(c[2])):\n        raise ValueError(\n            \"%s colour must be a triple of integers\" %\n            which)\nreturn c", "path": "src\\auxil\\png.py", "repo_name": "mortcanty/CRCPython", "stars": 146, "license": "None", "language": "python", "size": 975}
{"docstring": "\"\"\"\nCreate a PNG decoder object.\n\nThe constructor expects exactly one keyword argument. If you\nsupply a positional argument instead, it will guess the input\ntype. You can choose among the following keyword arguments:\n\nfilename\n  Name of input file (a PNG file).\nfile\n  A file-like object (object with a read() method).\nbytes\n  ``array`` or ``string`` with PNG data.\n\n\"\"\"\n", "func_signal": "def __init__(self, _guess=None, **kw):\n", "code": "if ((_guess is not None and len(kw) != 0) or\n    (_guess is None and len(kw) != 1)):\n    raise TypeError(\"Reader() takes exactly 1 argument\")\n\n# Will be the first 8 bytes, later on.  See validate_signature.\nself.signature = None\nself.transparent = None\n# A pair of (len,type) if a chunk has been read but its data and\n# checksum have not (in other words the file position is just\n# past the 4 bytes that specify the chunk type).  See preamble\n# method for how this is used.\nself.atchunk = None\n\nif _guess is not None:\n    if isarray(_guess):\n        kw[\"bytes\"] = _guess\n    elif isinstance(_guess, str):\n        kw[\"filename\"] = _guess\n    elif isinstance(_guess, file):\n        kw[\"file\"] = _guess\n\nif \"filename\" in kw:\n    self.file = file(kw[\"filename\"], \"rb\")\nelif \"file\" in kw:\n    self.file = kw[\"file\"]\nelif \"bytes\" in kw:\n    self.file = _readable(kw[\"bytes\"])\nelse:\n    raise TypeError(\"expecting filename, file or bytes array\")", "path": "src\\auxil\\png.py", "repo_name": "mortcanty/CRCPython", "stars": 146, "license": "None", "language": "python", "size": 975}
{"docstring": "\"\"\"Return image pixels as per :meth:`asDirect` method, but scale\nall pixel values to be floating point values between 0.0 and\n*maxval*.\n\"\"\"\n\n", "func_signal": "def asFloat(self, maxval=1.0):\n", "code": "x,y,pixels,info = self.asDirect()\nsourcemaxval = 2**info['bitdepth']-1\ndel info['bitdepth']\ninfo['maxval'] = float(maxval)\nfactor = float(maxval)/float(sourcemaxval)\ndef iterfloat():\n    for row in pixels:\n        yield map(factor.__mul__, row)\nreturn x,y,iterfloat(),info", "path": "src\\auxil\\png.py", "repo_name": "mortcanty/CRCPython", "stars": 146, "license": "None", "language": "python", "size": 975}
{"docstring": "\"\"\"Return image as RGB pixels.  Greyscales are expanded into RGB\ntriplets.  An alpha channel in the source image will raise an\nexception.  The return values are as for the :meth:`read` method\nexcept that the *metadata* reflect the returned pixels, not the\nsource image.  In particular, for this method\n``metadata['greyscale']`` will be ``False``.\n\"\"\"\n\n", "func_signal": "def asRGB(self):\n", "code": "width,height,pixels,meta = self.asDirect()\nif meta['alpha']:\n    raise Error(\"will not convert image with alpha channel to RGB\")  # commented out because GAE SDK not same as GAE\nif not meta['greyscale']:\n    return width,height,pixels,meta\nmeta['greyscale'] = False\ntypecode = 'BH'[meta['bitdepth'] > 8]\ndef iterrgb():\n    for row in pixels:\n        a = array(typecode, [0]) * 3 * width\n        for i in range(3):\n            a[i::3] = row\n        yield a\nreturn width,height,iterrgb(),meta", "path": "src\\auxil\\png.py", "repo_name": "mortcanty/CRCPython", "stars": 146, "license": "None", "language": "python", "size": 975}
{"docstring": "\"\"\"Calls the function `f` with ``sys.stdin`` changed to `inp`\nand ``sys.stdout`` changed to `out`.  They are restored when `f`\nreturns.  This function returns whatever `f` returns.\n\"\"\"\n", "func_signal": "def testWithIO(inp, out, f):\n", "code": "try:\n    oldin,sys.stdin = sys.stdin,inp\n    oldout,sys.stdout = sys.stdout,out\n    x = f()\nfinally:\n    sys.stdin = oldin\n    sys.stdout = oldout\nreturn x", "path": "src\\auxil\\png.py", "repo_name": "mortcanty/CRCPython", "stars": 146, "license": "None", "language": "python", "size": 975}
{"docstring": "\"\"\"\nGenerates boxed rows in flat pixel format, from the input file\n`infile`.  It assumes that the input file is in a \"Netpbm-like\"\nbinary format, and is positioned at the beginning of the first\npixel.  The number of pixels to read is taken from the image\ndimensions (`width`, `height`, `planes`) and the number of bytes\nper value is implied by the image `bitdepth`.\n\"\"\"\n\n# Values per row\n", "func_signal": "def file_scanlines(self, infile):\n", "code": "vpr = self.width * self.planes\nrow_bytes = vpr\nif self.bitdepth > 8:\n    assert self.bitdepth == 16\n    row_bytes *= 2\n    fmt = '>%dH' % vpr\n    def line():\n        return array('H', struct.unpack(fmt, infile.read(row_bytes)))\nelse:\n    def line():\n        scanline = array('B', infile.read(row_bytes))\n        return scanline\nfor y in range(self.height):\n    yield line()", "path": "src\\auxil\\png.py", "repo_name": "mortcanty/CRCPython", "stars": 146, "license": "None", "language": "python", "size": 975}
{"docstring": "\"\"\"Return an iterator that will yield each chunk as a\n(*chunktype*, *content*) pair.\n\"\"\"\n\n", "func_signal": "def chunks(self):\n", "code": "while True:\n    t,v = self.chunk()\n    yield t,v\n    if t == 'IEND':\n        break", "path": "src\\auxil\\png.py", "repo_name": "mortcanty/CRCPython", "stars": 146, "license": "None", "language": "python", "size": 975}
{"docstring": "\"\"\"Given an API Secret key and data, create a BitMEX-compatible signature.\"\"\"\n", "func_signal": "def bitmex_signature(apiSecret, verb, url, nonce, postdict=None):\n", "code": "data = ''\nif postdict:\n    # separators remove spaces from json\n    # BitMEX expects signatures from JSON built without spaces\n    data = json.dumps(postdict, separators=(',', ':'))\nparsedURL = urlparse.urlparse(url)\npath = parsedURL.path\nif parsedURL.query:\n    path = path + '?' + parsedURL.query\n# print(\"Computing HMAC: %s\" % verb + path + str(nonce) + data)\nmessage = bytes(verb + path + str(nonce) + data).encode('utf-8')\n\nsignature = hmac.new(apiSecret, message, digestmod=hashlib.sha256).hexdigest()\nreturn signature", "path": "test\\websocket-apikey-auth-test.py", "repo_name": "Behappy123/market-maker", "stars": 221, "license": "None", "language": "python", "size": 182}
{"docstring": "\"\"\"Restart if any files we're watching have changed.\"\"\"\n", "func_signal": "def check_file_change(self):\n", "code": "for f, mtime in watched_files_mtimes:\n    if getmtime(f) > mtime:\n        self.restart()", "path": "market_maker\\market_maker.py", "repo_name": "Behappy123/market-maker", "stars": 221, "license": "None", "language": "python", "size": 182}
{"docstring": "\"\"\"Init with Key & Secret.\"\"\"\n", "func_signal": "def __init__(self, apiKey, apiSecret):\n", "code": "self.apiKey = apiKey\nself.apiSecret = apiSecret", "path": "market_maker\\auth\\APIKeyAuthWithExpires.py", "repo_name": "Behappy123/market-maker", "stars": 221, "license": "None", "language": "python", "size": 182}
{"docstring": "\"\"\"This function checks whether the order book is empty\"\"\"\n", "func_signal": "def check_if_orderbook_empty(self):\n", "code": "instrument = self.get_instrument()\nif instrument['midPrice'] is None:\n    raise errors.MarketEmptyError(\"Orderbook is empty, cannot quote\")\n    sys.exit()", "path": "market_maker\\market_maker.py", "repo_name": "Behappy123/market-maker", "stars": 221, "license": "None", "language": "python", "size": 182}
{"docstring": "\"\"\"Calculate currency delta for portfolio\"\"\"\n", "func_signal": "def calc_delta(self):\n", "code": "portfolio = self.get_portfolio()\nspot_delta = 0\nmark_delta = 0\nfor symbol in portfolio:\n    item = portfolio[symbol]\n    if item['futureType'] == \"Quanto\":\n        spot_delta += item['currentQty'] * item['multiplier'] * item['spot']\n        mark_delta += item['currentQty'] * item['multiplier'] * item['markPrice']\n    elif item['futureType'] == \"Inverse\":\n        spot_delta += (item['multiplier'] / item['spot']) * item['currentQty']\n        mark_delta += (item['multiplier'] / item['markPrice']) * item['currentQty']\nbasis_delta = mark_delta - spot_delta\ndelta = {\n    \"spot\": spot_delta,\n    \"mark_price\": mark_delta,\n    \"basis\": basis_delta\n}\nreturn delta", "path": "market_maker\\market_maker.py", "repo_name": "Behappy123/market-maker", "stars": 221, "license": "None", "language": "python", "size": 182}
{"docstring": "# This is up to you, most use microtime but you may have your own scheme so long as it's increasing\n# and doesn't repeat.\n", "func_signal": "def test_with_querystring():\n", "code": "nonce = int(round(time.time() * 1000))\n# See signature generation reference at https://www.bitmex.com/app/apiKeys\nsignature = bitmex_signature(API_SECRET, VERB, ENDPOINT, nonce)\n\n# Initial connection - BitMEX sends a welcome message.\nws = create_connection(BITMEX_URL + ENDPOINT +\n                       \"?api-nonce=%s&api-signature=%s&api-key=%s\" % (nonce, signature, API_KEY))\nprint(\"Receiving Welcome Message...\")\nresult = ws.recv()\nprint(\"Received '%s'\" % result)\n\n# Send a request that requires authorization.\nrequest = {\"op\": \"subscribe\", \"args\": \"position\"}\nws.send(json.dumps(request))\nprint(\"Sent subscribe\")\nresult = ws.recv()\nprint(\"Received '%s'\" % result)\nresult = ws.recv()\nprint(\"Received '%s'\" % result)\n\nws.close()", "path": "test\\websocket-apikey-auth-test.py", "repo_name": "Behappy123/market-maker", "stars": 221, "license": "None", "language": "python", "size": 182}
{"docstring": "\"\"\"Create order items for use in convergence.\"\"\"\n\n", "func_signal": "def place_orders(self):\n", "code": "buy_orders = []\nsell_orders = []\n# Create orders from the outside in. This is intentional - let's say the inner order gets taken;\n# then we match orders from the outside in, ensuring the fewest number of orders are amended and only\n# a new order is created in the inside. If we did it inside-out, all orders would be amended\n# down and a new order would be created at the outside.\nif self.enough_liquidity():\n    for i in reversed(range(1, settings.ORDER_PAIRS + 1)):\n        if not self.long_position_limit_exceeded():\n            buy_orders.append(self.prepare_order(-i))\n        if not self.short_position_limit_exceeded():\n            sell_orders.append(self.prepare_order(i))\n\nreturn self.converge_orders(buy_orders, sell_orders)", "path": "market_maker\\market_maker.py", "repo_name": "Behappy123/market-maker", "stars": 221, "license": "None", "language": "python", "size": 182}
{"docstring": "\"\"\"Print the current MM status.\"\"\"\n\n", "func_signal": "def print_status(self):\n", "code": "margin = self.exchange.get_margin()\nposition = self.exchange.get_position()\nself.running_qty = self.exchange.get_delta()\nself.start_XBt = margin[\"marginBalance\"]\n\nlogger.info(\"Current XBT Balance: %.6f\" % XBt_to_XBT(self.start_XBt))\nlogger.info(\"Current Contract Position: %d\" % self.running_qty)\nif settings.CHECK_POSITION_LIMITS:\n    logger.info(\"Position limits: %d/%d\" % (settings.MIN_POSITION, settings.MAX_POSITION))\nif position['currentQty'] != 0:\n    logger.info(\"Avg Cost Price: %.2f\" % float(position['avgCostPrice']))\n    logger.info(\"Avg Entry Price: %.2f\" % float(position['avgEntryPrice']))\nlogger.info(\"Contracts Traded This Run: %d\" % (self.running_qty - self.starting_qty))\nlogger.info(\"Total Contract Delta: %.4f XBT\" % self.exchange.calc_delta()['spot'])", "path": "market_maker\\market_maker.py", "repo_name": "Behappy123/market-maker", "stars": 221, "license": "None", "language": "python", "size": 182}
{"docstring": "\"\"\"Converge the orders we currently have in the book with what we want to be in the book.\n   This involves amending any open orders and creating new ones if any have filled completely.\n   We start from the closest orders outward.\"\"\"\n\n", "func_signal": "def converge_orders(self, buy_orders, sell_orders):\n", "code": "tickLog = self.exchange.get_instrument()['tickLog']\nto_amend = []\nto_create = []\nto_cancel = []\nbuys_matched = 0\nsells_matched = 0\nexisting_orders = self.exchange.get_orders()\n\n# Check all existing orders and match them up with what we want to place.\n# If there's an open one, we might be able to amend it to fit what we want.\nfor order in existing_orders:\n    try:\n        if order['side'] == 'Buy':\n            desired_order = buy_orders[buys_matched]\n            buys_matched += 1\n        else:\n            desired_order = sell_orders[sells_matched]\n            sells_matched += 1\n\n        # Found an existing order. Do we need to amend it?\n        if desired_order['orderQty'] != order['leavesQty'] or (\n                # If price has changed, and the change is more than our RELIST_INTERVAL, amend.\n                desired_order['price'] != order['price'] and\n                abs((desired_order['price'] / order['price']) - 1) > settings.RELIST_INTERVAL):\n            to_amend.append({'orderID': order['orderID'], 'leavesQty': desired_order['orderQty'],\n                             'price': desired_order['price'], 'side': order['side']})\n    except IndexError:\n        # Will throw if there isn't a desired order to match. In that case, cancel it.\n        to_cancel.append(order)\n\nwhile buys_matched < len(buy_orders):\n    to_create.append(buy_orders[buys_matched])\n    buys_matched += 1\n\nwhile sells_matched < len(sell_orders):\n    to_create.append(sell_orders[sells_matched])\n    sells_matched += 1\n\nif len(to_amend) > 0:\n    for amended_order in reversed(to_amend):\n        reference_order = [o for o in existing_orders if o['orderID'] == amended_order['orderID']][0]\n        logger.info(\"Amending %4s: %d @ %.*f to %d @ %.*f (%+.*f)\" % (\n            amended_order['side'],\n            reference_order['leavesQty'], tickLog, reference_order['price'],\n            amended_order['leavesQty'], tickLog, amended_order['price'],\n            tickLog, (amended_order['price'] - reference_order['price'])\n        ))\n    # This can fail if an order has closed in the time we were processing.\n    # The API will send us `invalid ordStatus`, which means that the order's status (Filled/Canceled)\n    # made it not amendable.\n    # If that happens, we need to catch it and re-tick.\n    try:\n        self.exchange.amend_bulk_orders(to_amend)\n    except requests.exceptions.HTTPError as e:\n        errorObj = e.response.json()\n        if errorObj['error']['message'] == 'Invalid ordStatus':\n            logger.warn(\"Amending failed. Waiting for order data to converge and retrying.\")\n            sleep(0.5)\n            return self.place_orders()\n        else:\n            logger.error(\"Unknown error on amend: %s. Exiting\" % errorObj)\n            sys.exit(1)\n\nif len(to_create) > 0:\n    logger.info(\"Creating %d orders:\" % (len(to_create)))\n    for order in reversed(to_create):\n        logger.info(\"%4s %d @ %.*f\" % (order['side'], order['orderQty'], tickLog, order['price']))\n    self.exchange.create_bulk_orders(to_create)\n\n# Could happen if we exceed a delta limit\nif len(to_cancel) > 0:\n    logger.info(\"Canceling %d orders:\" % (len(to_cancel)))\n    for order in reversed(to_cancel):\n        logger.info(\"%4s %d @ %.*f\" % (order['side'], order['leavesQty'], tickLog, order['price']))\n    self.exchange.cancel_bulk_orders(to_cancel)", "path": "market_maker\\market_maker.py", "repo_name": "Behappy123/market-maker", "stars": 221, "license": "None", "language": "python", "size": 182}
{"docstring": "\"\"\"Given an index (1, -1, 2, -2, etc.) return the price for that side of the book.\n   Negative is a buy, positive is a sell.\"\"\"\n# Maintain existing spreads for max profit\n", "func_signal": "def get_price_offset(self, index):\n", "code": "if settings.MAINTAIN_SPREADS:\n    start_position = self.start_position_buy if index < 0 else self.start_position_sell\n    # First positions (index 1, -1) should start right at start_position, others should branch from there\n    index = index + 1 if index < 0 else index - 1\nelse:\n    # Offset mode: ticker comes from a reference exchange and we define an offset.\n    start_position = self.start_position_buy if index < 0 else self.start_position_sell\n\n    # If we're attempting to sell, but our sell price is actually lower than the buy,\n    # move over to the sell side.\n    if index > 0 and start_position < self.start_position_buy:\n        start_position = self.start_position_sell\n    # Same for buys.\n    if index < 0 and start_position > self.start_position_sell:\n        start_position = self.start_position_buy\n\nreturn round(start_position * (1 + settings.INTERVAL) ** index, self.instrument['tickLog'])", "path": "market_maker\\market_maker.py", "repo_name": "Behappy123/market-maker", "stars": 221, "license": "None", "language": "python", "size": 182}
{"docstring": "\"\"\"\nCalled when forming a request - generates api key headers. This call uses `expires` instead of nonce.\n\nThis way it will not collide with other processes using the same API Key if requests arrive out of order.\nFor more details, see https://www.bitmex.com/app/apiKeys\n\"\"\"\n# modify and return the request\n", "func_signal": "def __call__(self, r):\n", "code": "expires = int(round(time.time()) + 5)  # 5s grace period in case of clock skew\nr.headers['api-expires'] = str(expires)\nr.headers['api-key'] = self.apiKey\nr.headers['api-signature'] = self.generate_signature(self.apiSecret, r.method, r.url, expires, r.body or '')\n\nreturn r", "path": "market_maker\\auth\\APIKeyAuthWithExpires.py", "repo_name": "Behappy123/market-maker", "stars": 221, "license": "None", "language": "python", "size": 182}
{"docstring": "\"\"\"Init with Key & Secret.\"\"\"\n", "func_signal": "def __init__(self, apiKey, apiSecret):\n", "code": "self.apiKey = apiKey\nself.apiSecret = apiSecret", "path": "market_maker\\auth\\APIKeyAuth.py", "repo_name": "Behappy123/market-maker", "stars": 221, "license": "None", "language": "python", "size": 182}
{"docstring": "\"\"\"Called when forming a request - generates access token header.\"\"\"\n", "func_signal": "def __call__(self, r):\n", "code": "if (self.token):\n    r.headers['access-token'] = self.token\nreturn r", "path": "market_maker\\auth\\AccessTokenAuth.py", "repo_name": "Behappy123/market-maker", "stars": 221, "license": "None", "language": "python", "size": 182}
{"docstring": "# This is up to you, most use microtime but you may have your own scheme so long as it's increasing\n# and doesn't repeat.\n", "func_signal": "def test_with_message():\n", "code": "nonce = int(round(time.time() * 1000))\n# See signature generation reference at https://www.bitmex.com/app/apiKeys\nsignature = bitmex_signature(API_SECRET, VERB, ENDPOINT, nonce)\n\n# Initial connection - BitMEX sends a welcome message.\nws = create_connection(BITMEX_URL + ENDPOINT)\nprint(\"Receiving Welcome Message...\")\nresult = ws.recv()\nprint(\"Received '%s'\" % result)\n\n# Send API Key with signed message.\nrequest = {\"op\": \"authKey\", \"args\": [API_KEY, nonce, signature]}\nws.send(json.dumps(request))\nprint(\"Sent Auth request\")\nresult = ws.recv()\nprint(\"Received '%s'\" % result)\n\n# Send a request that requires authorization.\nrequest = {\"op\": \"subscribe\", \"args\": \"position\"}\nws.send(json.dumps(request))\nprint(\"Sent subscribe\")\nresult = ws.recv()\nprint(\"Received '%s'\" % result)\nresult = ws.recv()\nprint(\"Received '%s'\" % result)\n\nws.close()", "path": "test\\websocket-apikey-auth-test.py", "repo_name": "Behappy123/market-maker", "stars": 221, "license": "None", "language": "python", "size": 182}
{"docstring": "\"\"\"Authenticate with the BitMEX API & request account information.\"\"\"\n", "func_signal": "def main():\n", "code": "test_with_message()\ntest_with_querystring()", "path": "test\\websocket-apikey-auth-test.py", "repo_name": "Behappy123/market-maker", "stars": 221, "license": "None", "language": "python", "size": 182}
{"docstring": "\"\"\"Generate a request signature compatible with BitMEX.\"\"\"\n# Parse the url so we can remove the base and extract just the path.\n", "func_signal": "def generate_signature(self, secret, verb, url, nonce, data):\n", "code": "parsedURL = urlparse(url)\npath = parsedURL.path\nif parsedURL.query:\n    path = path + '?' + parsedURL.query\n\n# print \"Computing HMAC: %s\" % verb + path + str(nonce) + data\nmessage = verb + path + str(nonce) + data\n\nsignature = hmac.new(bytes(secret, 'utf8'), bytes(message, 'utf8'), digestmod=hashlib.sha256).hexdigest()\nreturn signature", "path": "market_maker\\auth\\APIKeyAuthWithExpires.py", "repo_name": "Behappy123/market-maker", "stars": 221, "license": "None", "language": "python", "size": 182}
{"docstring": "\"\"\"Create an order object.\"\"\"\n\n", "func_signal": "def prepare_order(self, index):\n", "code": "if settings.RANDOM_ORDER_SIZE is True:\n    quantity = random.randint(settings.MIN_ORDER_SIZE, settings.MAX_ORDER_SIZE)\nelse:\n    quantity = settings.ORDER_START_SIZE + ((abs(index) - 1) * settings.ORDER_STEP_SIZE)\n\nprice = self.get_price_offset(index)\n\nreturn {'price': price, 'orderQty': quantity, 'side': \"Buy\" if index < 0 else \"Sell\"}", "path": "market_maker\\market_maker.py", "repo_name": "Behappy123/market-maker", "stars": 221, "license": "None", "language": "python", "size": 182}
{"docstring": "\"\"\"Perform checks before placing orders.\"\"\"\n\n# Check if OB is empty - if so, can't quote.\n", "func_signal": "def sanity_check(self):\n", "code": "self.exchange.check_if_orderbook_empty()\n\n# Ensure market is still open.\nself.exchange.check_market_open()\n\n# Get ticker, which sets price offsets and prints some debugging info.\nticker = self.get_ticker()\n\n# Sanity check:\nif self.get_price_offset(-1) >= ticker[\"sell\"] or self.get_price_offset(1) <= ticker[\"buy\"]:\n    logger.error(self.start_position_buy, self.start_position_sell)\n    logger.error(\"%s %s %s %s\" % (self.get_price_offset(-1), ticker[\"sell\"], self.get_price_offset(1), ticker[\"buy\"]))\n    logger.error(\"Sanity check failed, exchange data is inconsistent\")\n    sys.exit()\n\n# Messanging if the position limits are reached\nif self.long_position_limit_exceeded():\n    logger.info(\"Long delta limit exceeded\")\n    logger.info(\"Current Position: %.f, Maximum Position: %.f\" %\n                (self.exchange.get_delta(), settings.MAX_POSITION))\n\nif self.short_position_limit_exceeded():\n    logger.info(\"Short delta limit exceeded\")\n    logger.info(\"Current Position: %.f, Minimum Position: %.f\" %\n                (self.exchange.get_delta(), settings.MIN_POSITION))", "path": "market_maker\\market_maker.py", "repo_name": "Behappy123/market-maker", "stars": 221, "license": "None", "language": "python", "size": 182}
{"docstring": "\"\"\"Called when forming a request - generates api key headers.\"\"\"\n# modify and return the request\n", "func_signal": "def __call__(self, r):\n", "code": "nonce = generate_nonce()\nr.headers['api-nonce'] = str(nonce)\nr.headers['api-key'] = self.apiKey\nr.headers['api-signature'] = generate_signature(self.apiSecret, r.method, r.url, nonce, r.body or '')\n\nreturn r", "path": "market_maker\\auth\\APIKeyAuth.py", "repo_name": "Behappy123/market-maker", "stars": 221, "license": "None", "language": "python", "size": 182}
{"docstring": "\"\"\"Generate a request signature compatible with BitMEX.\"\"\"\n# Parse the url so we can remove the base and extract just the path.\n", "func_signal": "def generate_signature(secret, verb, url, nonce, data):\n", "code": "parsedURL = urlparse(url)\npath = parsedURL.path\nif parsedURL.query:\n    path = path + '?' + parsedURL.query\n\n# print \"Computing HMAC: %s\" % verb + path + str(nonce) + data\nmessage = verb + path + str(nonce) + data\n\nsignature = hmac.new(bytes(secret, 'utf8'), bytes(message, 'utf8'), digestmod=hashlib.sha256).hexdigest()\nreturn signature", "path": "market_maker\\auth\\APIKeyAuth.py", "repo_name": "Behappy123/market-maker", "stars": 221, "license": "None", "language": "python", "size": 182}
{"docstring": "\"\"\"\nReturns site-specific profile for this user. Raises\nSiteProfileNotAvailable if this site does not allow profiles.\n\"\"\"\n", "func_signal": "def get_profile(self):\n", "code": "if not hasattr(self, '_profile_cache'):\n    from django.conf import settings\n    if not getattr(settings, 'AUTH_PROFILE_MODULE', False):\n        raise SiteProfileNotAvailable\n    try:\n        app_label, model_name = settings.AUTH_PROFILE_MODULE.split('.')\n        model = models.get_model(app_label, model_name)\n        self._profile_cache = model._default_manager.get(user__id__exact=self.id)\n        self._profile_cache.user = self\n    except (ImportError, ImproperlyConfigured):\n        raise SiteProfileNotAvailable\nreturn self._profile_cache", "path": "stores\\apps\\auth\\models.py", "repo_name": "StephenPower/CollectorCity-Market-Place", "stars": 214, "license": "None", "language": "python", "size": 9237}
{"docstring": "\"\"\" Create Blog Page to shop \"\"\"\n", "func_signal": "def update(self):\n", "code": "from blog_pages.models import Home, About, Menu\nfrom themes.models import Theme\n        \ntry:\n    Home.objects.filter(shop=self).get()\nexcept Home.DoesNotExist:\n    Home(shop=self).save()\n    \ntry:\n    About.objects.filter(shop=self).get()\nexcept About.DoesNotExist:\n    About(shop=self).save()\nMenu.create_default(self)\n\n\"\"\" Create default pages whit default theme \"\"\"\n\n#Theme.create_default(self)", "path": "stores\\apps\\shops\\models.py", "repo_name": "StephenPower/CollectorCity-Market-Place", "stars": 214, "license": "None", "language": "python", "size": 9237}
{"docstring": "\"\"\"\nReturns a boolean of whether the raw_password was correct. Handles\nencryption formats behind the scenes.\n\"\"\"\n# Backwards-compatibility check. Older passwords won't include the\n# algorithm or salt.\n", "func_signal": "def check_password(self, raw_password):\n", "code": "if '$' not in self.password:\n    is_correct = (self.password == get_hexdigest('md5', '', raw_password))\n    if is_correct:\n        # Convert the password to the new, more secure format.\n        self.set_password(raw_password)\n        self.save()\n    return is_correct\nreturn check_password(raw_password, self.password)", "path": "stores\\apps\\auth\\models.py", "repo_name": "StephenPower/CollectorCity-Market-Place", "stars": 214, "license": "None", "language": "python", "size": 9237}
{"docstring": "# Adding model 'Lot'\n", "func_signal": "def forwards(self, orm):\n", "code": "        db.create_table('lots_lot', (\n            ('product_ptr', self.gf('django.db.models.fields.related.OneToOneField')(to=orm['inventory.Product'], unique=True, primary_key=True)),\n            ('starting_bid', self.gf('django.db.models.fields.DecimalField')(max_digits=11, decimal_places=2)),\n            ('reserve', self.gf('django.db.models.fields.DecimalField')(max_digits=11, decimal_places=2)),\n            ('session', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['auctions.AuctionSession'])),\n            ('state', self.gf('django.db.models.fields.CharField')(default='A', max_length=1)),\n            ('bid_actual', self.gf('django.db.models.fields.related.OneToOneField')(related_name='lot_history', unique=True, null=True, to=orm['lots.BidHistory'])),\n        ))\n        db.send_create_signal('lots', ['Lot'])\n\n        # Adding model 'ImageLot'\n        db.create_table('lots_imagelot', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('image', self.gf('core.thumbs.ImageWithThumbsField')(max_length=100)),\n            ('lot', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['lots.Lot'])),\n            ('primary_picture', self.gf('django.db.models.fields.BooleanField')(default=False, blank=True)),\n        ))\n        db.send_create_signal('lots', ['ImageLot'])\n\n        # Adding model 'BidHistory'\n        db.create_table('lots_bidhistory', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('lot', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['lots.Lot'])),\n            ('bidder', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['auth.User'])),\n            ('bid_amount', self.gf('django.db.models.fields.DecimalField')(max_digits=11, decimal_places=2)),\n            ('bid_time', self.gf('django.db.models.fields.DateTimeField')(auto_now_add=True, blank=True)),\n        ))\n        db.send_create_signal('lots', ['BidHistory'])", "path": "stores\\apps\\lots\\migrations\\0001_initial.py", "repo_name": "StephenPower/CollectorCity-Market-Place", "stars": 214, "license": "None", "language": "python", "size": 9237}
{"docstring": "# Deleting model 'Lot'\n", "func_signal": "def backwards(self, orm):\n", "code": "        db.delete_table('lots_lot')\n\n        # Deleting model 'ImageLot'\n        db.delete_table('lots_imagelot')\n\n        # Deleting model 'BidHistory'\n        db.delete_table('lots_bidhistory')", "path": "stores\\apps\\lots\\migrations\\0001_initial.py", "repo_name": "StephenPower/CollectorCity-Market-Place", "stars": 214, "license": "None", "language": "python", "size": 9237}
{"docstring": "# Adding model 'DynamicPageContent'\n", "func_signal": "def forwards(self, orm):\n", "code": "        db.create_table('blog_pages_dynamicpagecontent', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('shop', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['shops.Shop'])),\n            ('page', self.gf('django.db.models.fields.CharField')(max_length=100)),\n            ('meta_content', self.gf('django.db.models.fields.TextField')(null=True, blank=True)),\n        ))\n        db.send_create_signal('blog_pages', ['DynamicPageContent'])", "path": "stores\\apps\\blog_pages\\migrations\\0007_auto__add_dynamicpagecontent.py", "repo_name": "StephenPower/CollectorCity-Market-Place", "stars": 214, "license": "None", "language": "python", "size": 9237}
{"docstring": "# Changing field 'Profile.user'\n", "func_signal": "def forwards(self, orm):\n", "code": "        db.alter_column('users_profile', 'user_id', self.gf('django.db.models.fields.related.OneToOneField')(to=orm['auth.User'], unique=True))\n\n        # Changing field 'Profile.photo'\n        db.alter_column('users_profile', 'photo', self.gf('core.thumbs.ImageWithThumbsField')(max_length=100))\n\n        # Adding field 'EmailVerify.verified'\n        db.add_column('users_emailverify', 'verified', self.gf('django.db.models.fields.BooleanField')(default=False, blank=True), keep_default=False)", "path": "stores\\apps\\users\\migrations\\0003_auto__chg_field_profile_user__chg_field_profile_photo__add_field_email.py", "repo_name": "StephenPower/CollectorCity-Market-Place", "stars": 214, "license": "None", "language": "python", "size": 9237}
{"docstring": "\"\"\"\nReturns a boolean of whether the raw_password was correct. Handles\nencryption formats behind the scenes.\n\"\"\"\n", "func_signal": "def check_password(raw_password, enc_password):\n", "code": "algo, salt, hsh = enc_password.split('$')\nreturn hsh == get_hexdigest(algo, salt, raw_password)", "path": "stores\\apps\\auth\\models.py", "repo_name": "StephenPower/CollectorCity-Market-Place", "stars": 214, "license": "None", "language": "python", "size": 9237}
{"docstring": "# Adding model 'PostEditorPick'\n", "func_signal": "def forwards(self, orm):\n", "code": "        db.create_table('market_community_posteditorpick', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('marketplace', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['market.MarketPlace'])),\n            ('order', self.gf('django.db.models.fields.IntegerField')(default=5)),\n            ('post', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['blog_pages.Post'])),\n        ))\n        db.send_create_signal('market_community', ['PostEditorPick'])\n\n        # Adding field 'FAQCategory.order'\n        db.add_column('market_community_faqcategory', 'order', self.gf('django.db.models.fields.IntegerField')(default=5), keep_default=False)\n\n        # Adding field 'FAQEntry.order'\n        db.add_column('market_community_faqentry', 'order', self.gf('django.db.models.fields.IntegerField')(default=5), keep_default=False)", "path": "marketplaces\\apps\\market_community\\migrations\\0002_auto__add_posteditorpick__add_field_faqcategory_order__add_field_faqen.py", "repo_name": "StephenPower/CollectorCity-Market-Place", "stars": 214, "license": "None", "language": "python", "size": 9237}
{"docstring": "\"\"\"\nReturns a string of the hexdigest of the given plaintext password and salt\nusing the given algorithm ('md5', 'sha1' or 'crypt').\n\"\"\"\n", "func_signal": "def get_hexdigest(algorithm, salt, raw_password):\n", "code": "raw_password, salt = smart_str(raw_password), smart_str(salt)\nif algorithm == 'crypt':\n    try:\n        import crypt\n    except ImportError:\n        raise ValueError('\"crypt\" password algorithm not supported in this environment')\n    return crypt.crypt(raw_password, salt)\n\nif algorithm == 'md5':\n    return md5_constructor(salt + raw_password).hexdigest()\nelif algorithm == 'sha1':\n    return sha_constructor(salt + raw_password).hexdigest()\nraise ValueError(\"Got unknown password algorithm type in password.\")", "path": "stores\\apps\\auth\\models.py", "repo_name": "StephenPower/CollectorCity-Market-Place", "stars": 214, "license": "None", "language": "python", "size": 9237}
{"docstring": "\"\"\"\nReturns True if the user has the specified permission. This method\nqueries all available auth backends, but returns immediately if any\nbackend returns True. Thus, a user who has permission from a single\nauth backend is assumed to have permission in general.\n\"\"\"\n# Inactive users have no permissions.\n", "func_signal": "def has_perm(self, perm):\n", "code": "if not self.is_active:\n    return False\n\n# Superusers have all permissions.\nif self.is_superuser:\n    return True\n\n# Otherwise we need to check the backends.\nfor backend in auth.get_backends():\n    if hasattr(backend, \"has_perm\"):\n        if backend.has_perm(self, perm):\n            return True\nreturn False", "path": "stores\\apps\\auth\\models.py", "repo_name": "StephenPower/CollectorCity-Market-Place", "stars": 214, "license": "None", "language": "python", "size": 9237}
{"docstring": "# Deleting model 'Post'\n", "func_signal": "def backwards(self, orm):\n", "code": "        db.delete_table('blog_pages_post')\n\n        # Deleting model 'Home'\n        db.delete_table('blog_pages_home')\n\n        # Deleting model 'About'\n        db.delete_table('blog_pages_about')\n\n        # Deleting model 'Page'\n        db.delete_table('blog_pages_page')\n\n        # Deleting model 'Menu'\n        db.delete_table('blog_pages_menu')\n\n        # Deleting model 'Link'\n        db.delete_table('blog_pages_link')", "path": "stores\\apps\\blog_pages\\migrations\\0001_initial.py", "repo_name": "StephenPower/CollectorCity-Market-Place", "stars": 214, "license": "None", "language": "python", "size": 9237}
{"docstring": "# Deleting model 'PostEditorPick'\n", "func_signal": "def backwards(self, orm):\n", "code": "        db.delete_table('market_community_posteditorpick')\n\n        # Deleting field 'FAQCategory.order'\n        db.delete_column('market_community_faqcategory', 'order')\n\n        # Deleting field 'FAQEntry.order'\n        db.delete_column('market_community_faqentry', 'order')", "path": "marketplaces\\apps\\market_community\\migrations\\0002_auto__add_posteditorpick__add_field_faqcategory_order__add_field_faqen.py", "repo_name": "StephenPower/CollectorCity-Market-Place", "stars": 214, "license": "None", "language": "python", "size": 9237}
{"docstring": "\"\"\"\nReturns True if the user has any permissions in the given app\nlabel. Uses pretty much the same logic as has_perm, above.\n\"\"\"\n", "func_signal": "def has_module_perms(self, app_label):\n", "code": "if not self.is_active:\n    return False\n\nif self.is_superuser:\n    return True\n\nfor backend in auth.get_backends():\n    if hasattr(backend, \"has_module_perms\"):\n        if backend.has_module_perms(self, app_label):\n            return True\nreturn False", "path": "stores\\apps\\auth\\models.py", "repo_name": "StephenPower/CollectorCity-Market-Place", "stars": 214, "license": "None", "language": "python", "size": 9237}
{"docstring": "# Adding model 'Post'\n", "func_signal": "def forwards(self, orm):\n", "code": "        db.create_table('blog_pages_post', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('shop', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['shops.Shop'])),\n            ('title', self.gf('django.db.models.fields.CharField')(max_length=60)),\n            ('body', self.gf('django.db.models.fields.TextField')()),\n            ('date_time', self.gf('django.db.models.fields.DateTimeField')(auto_now_add=True, blank=True)),\n        ))\n        db.send_create_signal('blog_pages', ['Post'])\n\n        # Adding model 'Home'\n        db.create_table('blog_pages_home', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('shop', self.gf('django.db.models.fields.related.OneToOneField')(to=orm['shops.Shop'], unique=True)),\n            ('title', self.gf('django.db.models.fields.CharField')(default='Welcome to My Store', max_length=60)),\n            ('body', self.gf('django.db.models.fields.TextField')(default='Praesent a enim ac nunc egestas egestas. Integer auctor justo et lorem pulvinar eleifend. Curabitur accumsan massa lectus. Pellentesque ac ipsum sed odio mattis aliquam at egestas odio. Vestibulum gravida augue sapien, sit amet posuere quam. Duis dui mauris, pretium sed cursus quis, semper vitae metus. Sed et ante quam. Morbi nunc diam, tristique at vulputate a, ornare sed odio. Donec semper dolor nisl. Maecenas ac felis mauris, eget ornare metus. Pellentesque ac vehicula ligula. Nam semper nibh quis tortor eleifend et ultricies sapien tempus.')),\n            ('image', self.gf('django.db.models.fields.files.ImageField')(max_length=100)),\n            ('last_updated', self.gf('django.db.models.fields.DateTimeField')(auto_now=True, blank=True)),\n        ))\n        db.send_create_signal('blog_pages', ['Home'])\n\n        # Adding model 'About'\n        db.create_table('blog_pages_about', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('shop', self.gf('django.db.models.fields.related.OneToOneField')(to=orm['shops.Shop'], unique=True)),\n            ('title', self.gf('django.db.models.fields.CharField')(default='About Us', max_length=60)),\n            ('body', self.gf('django.db.models.fields.TextField')(default='Nam est mauris, pretium eu imperdiet ut, iaculis sit amet sapien. Ut aliquet laoreet odio, ut hendrerit lectus suscipit quis. Sed condimentum elementum sollicitudin. Praesent accumsan, nisi nec sagittis dignissim, ante massa lobortis diam, id tincidunt arcu ipsum non purus. Duis et leo non diam feugiat congue ut in nulla. Suspendisse et faucibus mi. Fusce imperdiet volutpat sollicitudin. Suspendisse potenti.')),\n            ('location', self.gf('django.db.models.fields.CharField')(default='39.29038,-76.61219', max_length=255)),\n            ('last_updated', self.gf('django.db.models.fields.DateTimeField')(auto_now=True, blank=True)),\n        ))\n        db.send_create_signal('blog_pages', ['About'])\n\n        # Adding model 'Page'\n        db.create_table('blog_pages_page', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('shop', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['shops.Shop'])),\n            ('name', self.gf('django.db.models.fields.CharField')(max_length=60)),\n            ('name_link', self.gf('django.db.models.fields.CharField')(max_length=60)),\n            ('title', self.gf('django.db.models.fields.CharField')(max_length=60)),\n            ('body', self.gf('django.db.models.fields.TextField')()),\n            ('last_updated', self.gf('django.db.models.fields.DateTimeField')(auto_now=True, blank=True)),\n            ('visible', self.gf('django.db.models.fields.BooleanField')(default=True, blank=True)),\n        ))\n        db.send_create_signal('blog_pages', ['Page'])\n\n        # Adding model 'Menu'\n        db.create_table('blog_pages_menu', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('shop', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['shops.Shop'])),\n            ('name', self.gf('django.db.models.fields.CharField')(max_length=60)),\n        ))\n        db.send_create_signal('blog_pages', ['Menu'])\n\n        # Adding model 'Link'\n        db.create_table('blog_pages_link', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('name', self.gf('django.db.models.fields.CharField')(max_length=60)),\n            ('to', self.gf('django.db.models.fields.CharField')(max_length=120)),\n            ('title', self.gf('django.db.models.fields.CharField')(max_length=60)),\n            ('menu', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['blog_pages.Menu'])),\n            ('order', self.gf('django.db.models.fields.PositiveIntegerField')()),\n        ))\n        db.send_create_signal('blog_pages', ['Link'])", "path": "stores\\apps\\blog_pages\\migrations\\0001_initial.py", "repo_name": "StephenPower/CollectorCity-Market-Place", "stars": 214, "license": "None", "language": "python", "size": 9237}
{"docstring": "# Adding model 'AssetRenderingSecure'\n", "func_signal": "def forwards(self, orm):\n", "code": "        db.create_table('themes_assetrenderingsecure', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('asset', self.gf('django.db.models.fields.related.OneToOneField')(to=orm['themes.Asset'], unique=True)),\n            ('file', self.gf('django.db.models.fields.files.FileField')(max_length=100)),\n        ))\n        db.send_create_signal('themes', ['AssetRenderingSecure'])", "path": "stores\\apps\\themes\\migrations\\0003_auto__add_assetrenderingsecure.py", "repo_name": "StephenPower/CollectorCity-Market-Place", "stars": 214, "license": "None", "language": "python", "size": 9237}
{"docstring": "\"\"\"\nReturns a list of permission strings that this user has through\nhis/her groups. This method queries all available auth backends.\n\"\"\"\n", "func_signal": "def get_group_permissions(self):\n", "code": "permissions = set()\nfor backend in auth.get_backends():\n    if hasattr(backend, \"get_group_permissions\"):\n        permissions.update(backend.get_group_permissions(self))\nreturn permissions", "path": "stores\\apps\\auth\\models.py", "repo_name": "StephenPower/CollectorCity-Market-Place", "stars": 214, "license": "None", "language": "python", "size": 9237}
{"docstring": "#from django.contrib.auth import login\n", "func_signal": "def test_email_backend(self):\n", "code": "request =  object()\nrequest.POST = {'username': 'core_tester',\n                'password': 'test'}\n\nfrom auth import login\nlogin(request, self.user)", "path": "stores\\apps\\core\\tests.py", "repo_name": "StephenPower/CollectorCity-Market-Place", "stars": 214, "license": "None", "language": "python", "size": 9237}
{"docstring": "\"\"\"Returns True if the user has each of the specified permissions.\"\"\"\n", "func_signal": "def has_perms(self, perm_list):\n", "code": "for perm in perm_list:\n    if not self.has_perm(perm):\n        return False\nreturn True", "path": "stores\\apps\\auth\\models.py", "repo_name": "StephenPower/CollectorCity-Market-Place", "stars": 214, "license": "None", "language": "python", "size": 9237}
{"docstring": "# Changing field 'Profile.user'\n", "func_signal": "def backwards(self, orm):\n", "code": "        db.alter_column('users_profile', 'user_id', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['auth.User'], unique=True))\n\n        # Changing field 'Profile.photo'\n        db.alter_column('users_profile', 'photo', self.gf('django.db.models.fields.files.ImageField')(max_length=100, null=True, blank=True))\n\n        # Deleting field 'EmailVerify.verified'\n        db.delete_column('users_emailverify', 'verified')", "path": "stores\\apps\\users\\migrations\\0003_auto__chg_field_profile_user__chg_field_profile_photo__add_field_email.py", "repo_name": "StephenPower/CollectorCity-Market-Place", "stars": 214, "license": "None", "language": "python", "size": 9237}
{"docstring": "'''Remove line from file by creating a temporary file containing all lines\nfrom original file except those matching the given line, then copying the\ntemporary file back into the original file, overwriting its contents.\n'''\n", "func_signal": "def remove_line(fname, line):\n", "code": "with lockfile.FileLock(fname):\n\ttmp = tempfile.TemporaryFile()\n\tfp = open(fname, 'rw+')\n\t# write all lines from orig file, except if matches given line\n\tfor l in fp:\n\t\tif l.strip() != line:\n\t\t\ttmp.write(l)\n\t\n\t# reset file pointers so entire files are copied\n\tfp.seek(0)\n\ttmp.seek(0)\n\t# copy tmp into fp, then truncate to remove trailing line(s)\n\tshutil.copyfileobj(tmp, fp)\n\tfp.truncate()\n\tfp.close()\n\ttmp.close()", "path": "corpus.py", "repo_name": "japerk/nltk3-cookbook", "stars": 142, "license": "None", "language": "python", "size": 200}
{"docstring": "'''Train Ngram taggers on chunked sentences'''\n", "func_signal": "def __init__(self, train_chunks, tagger_classes=[UnigramTagger, BigramTagger]):\n", "code": "train_sents = conll_tag_chunks(train_chunks)\nself.tagger = backoff_tagger(train_sents, tagger_classes)", "path": "chunkers.py", "repo_name": "japerk/nltk3-cookbook", "stars": 142, "license": "None", "language": "python", "size": 200}
{"docstring": "'''Move subject to before the noun preceding the infinitive.\n>>> swap_infinitive_phrase([('book', 'NN'), ('of', 'IN'), ('recipes', 'NNS')])\n[('recipes', 'NNS'), ('book', 'NN')]\n>>> swap_infinitive_phrase([('tastes', 'VBZ'), ('like', 'IN'), ('chicken', 'NN')])\n[('tastes', 'VBZ'), ('like', 'IN'), ('chicken', 'NN')]\n>>> swap_infinitive_phrase([('delicious', 'JJ'), ('book', 'NN'), ('of', 'IN'), ('recipes', 'NNS')])\n[('delicious', 'JJ'), ('recipes', 'NNS'), ('book', 'NN')]\n'''\n", "func_signal": "def swap_infinitive_phrase(chunk):\n", "code": "def inpred(wt):\n\tword, tag = wt\n\treturn tag == 'IN' and word != 'like'\n\ninidx = first_chunk_index(chunk, inpred)\n\nif inidx is None:\n\treturn chunk\n\nnnidx = first_chunk_index(chunk, tag_startswith('NN'), start=inidx, step=-1) or 0\nreturn chunk[:nnidx] + chunk[inidx+1:] + chunk[nnidx:inidx]", "path": "transforms.py", "repo_name": "japerk/nltk3-cookbook", "stars": 142, "license": "None", "language": "python", "size": 200}
{"docstring": "\"\"\" Try to replace negations with antonyms in the tokenized sentence.\n>>> replacer = AntonymReplacer()\n>>> replacer.replace_negations(['do', 'not', 'uglify', 'our', 'code'])\n['do', 'beautify', 'our', 'code']\n>>> replacer.replace_negations(['good', 'is', 'not', 'evil'])\n['good', 'is', 'not', 'evil']\n\"\"\"\n", "func_signal": "def replace_negations(self, sent):\n", "code": "i, l = 0, len(sent)\nwords = []\n\nwhile i < l:\n\tword = sent[i]\n\t\n\tif word == 'not' and i+1 < l:\n\t\tant = self.replace(sent[i+1])\n\t\t\n\t\tif ant:\n\t\t\twords.append(ant)\n\t\t\ti += 2\n\t\t\tcontinue\n\t\n\twords.append(word)\n\ti += 1\n\nreturn words", "path": "replacers.py", "repo_name": "japerk/nltk3-cookbook", "stars": 142, "license": "None", "language": "python", "size": 200}
{"docstring": "# gazetteers is a WordListCorpusReader of many different location words\n", "func_signal": "def __init__(self):\n", "code": "self.locations = set(gazetteers.words())\nself.lookahead = 0\n# need to know how many words to lookahead in the tagged sentence to find a location\nfor loc in self.locations:\n\tnwords = loc.count(' ')\n\t\n\tif nwords > self.lookahead:\n\t\tself.lookahead = nwords", "path": "chunkers.py", "repo_name": "japerk/nltk3-cookbook", "stars": 142, "license": "None", "language": "python", "size": 200}
{"docstring": "# NOTE: in addition to cat_pattern, ConllChunkCorpusReader also requires\n# chunk_types as third argument, which defaults to ('NP','VP','PP')\n", "func_signal": "def __init__(self, *args, **kwargs):\n", "code": "CategorizedCorpusReader.__init__(self, kwargs)\nConllChunkCorpusReader.__init__(self, *args, **kwargs)", "path": "catchunked.py", "repo_name": "japerk/nltk3-cookbook", "stars": 142, "license": "None", "language": "python", "size": 200}
{"docstring": "'''Move modifier phrase after verb to front of chunk and drop the verb.\n>>> swap_verb_phrase([('the', 'DT'), ('book', 'NN'), ('was', 'VBD'), ('great', 'JJ')])\n[('great', 'JJ'), ('the', 'DT'), ('book', 'NN')]\n>>> swap_verb_phrase([('this', 'DT'), ('gripping', 'VBG'), ('book', 'NN'), ('is', 'VBZ'), ('fantastic', 'JJ')])\n[('fantastic', 'JJ'), ('this', 'DT'), ('gripping', 'VBG'), ('book', 'NN')]\n'''\n# find location of verb\n", "func_signal": "def swap_verb_phrase(chunk):\n", "code": "def vbpred(wt):\n\tword, tag = wt\n\treturn tag != 'VBG' and tag.startswith('VB') and len(tag) > 2\n\nvbidx = first_chunk_index(chunk, vbpred)\n\nif vbidx is None:\n\treturn chunk\n\nreturn chunk[vbidx+1:] + chunk[:vbidx]", "path": "transforms.py", "repo_name": "japerk/nltk3-cookbook", "stars": 142, "license": "None", "language": "python", "size": 200}
{"docstring": "'''Remove insignificant words from the chunk.\n>>> filter_insignificant([('the', 'DT'), ('terrible', 'JJ'), ('movie', 'NN')])\n[('terrible', 'JJ'), ('movie', 'NN')]\n'''\n", "func_signal": "def filter_insignificant(chunk, tag_suffixes=['DT', 'CC']):\n", "code": "good = []\n\nfor word, tag in chunk:\n\tok = True\n\t\n\tfor suffix in tag_suffixes:\n\t\tif tag.endswith(suffix):\n\t\t\tok = False\n\t\t\tbreak\n\t\n\tif ok:\n\t\tgood.append((word, tag))\n\nreturn good", "path": "transforms.py", "repo_name": "japerk/nltk3-cookbook", "stars": 142, "license": "None", "language": "python", "size": 200}
{"docstring": "'''\n>>> shallow_tree(Tree('S', [Tree('NP-SBJ', [Tree('NP', [Tree('NNP', ['Pierre']), Tree('NNP', ['Vinken'])]), Tree(',', [',']), Tree('ADJP', [Tree('NP', [Tree('CD', ['61']), Tree('NNS', ['years'])]), Tree('JJ', ['old'])]), Tree(',', [','])]), Tree('VP', [Tree('MD', ['will']), Tree('VP', [Tree('VB', ['join']), Tree('NP', [Tree('DT', ['the']), Tree('NN', ['board'])]), Tree('PP-CLR', [Tree('IN', ['as']), Tree('NP', [Tree('DT', ['a']), Tree('JJ', ['nonexecutive']), Tree('NN', ['director'])])]), Tree('NP-TMP', [Tree('NNP', ['Nov.']), Tree('CD', ['29'])])])]), Tree('.', ['.'])]))\nTree('S', [Tree('NP-SBJ', [('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ',')]), Tree('VP', [('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD')]), ('.', '.')])\n'''\n", "func_signal": "def shallow_tree(tree):\n", "code": "children = []\n\nfor t in tree:\n\tif t.height() < 3:\n\t\tchildren.extend(t.pos())\n\telse:\n\t\tchildren.append(Tree(t.label(), t.pos()))\n\nreturn Tree(tree.label(), children)", "path": "transforms.py", "repo_name": "japerk/nltk3-cookbook", "stars": 142, "license": "None", "language": "python", "size": 200}
{"docstring": "'''Move a cardinal that occurs after a noun to immediately before the noun.\n>>> swap_noun_cardinal([('Dec.', 'NNP'), ('10', 'CD')])\n[('10', 'CD'), ('Dec.', 'NNP')]\n>>> swap_noun_cardinal([('the', 'DT'), ('top', 'NN'), ('10', 'CD')])\n[('the', 'DT'), ('10', 'CD'), ('top', 'NN')]\n'''\n", "func_signal": "def swap_noun_cardinal(chunk):\n", "code": "cdidx = first_chunk_index(chunk, tag_equals('CD'))\n# cdidx must be > 0 and there must be a noun immediately before it\nif not cdidx or not chunk[cdidx-1][1].startswith('NN'):\n\treturn chunk\n\nnoun, nntag = chunk[cdidx-1]\nchunk[cdidx-1] = chunk[cdidx]\nchunk[cdidx] = noun, nntag\nreturn chunk", "path": "transforms.py", "repo_name": "japerk/nltk3-cookbook", "stars": 142, "license": "None", "language": "python", "size": 200}
{"docstring": "'''Parsed tagged tokens into parse Tree of chunks'''\n", "func_signal": "def parse(self, tagged_sent):\n", "code": "if not tagged_sent: return None\n(words, tags) = zip(*tagged_sent)\nchunks = self.tagger.tag(tags)\n# create conll str for tree parsing\nwtc = zip(words, chunks)\nreturn conlltags2tree([(w,t,c) for (w,(t,c)) in wtc])", "path": "chunkers.py", "repo_name": "japerk/nltk3-cookbook", "stars": 142, "license": "None", "language": "python", "size": 200}
{"docstring": "\"\"\" Returns the antonym of a word, but only if there is no ambiguity.\n>>> replacer = AntonymReplacer()\n>>> replacer.replace('good')\n>>> replacer.replace('uglify')\n'beautify'\n>>> replacer.replace('beautify')\n'uglify'\n\"\"\"\n", "func_signal": "def replace(self, word, pos=None):\n", "code": "antonyms = set()\n\nfor syn in wordnet.synsets(word, pos=pos):\n\tfor lemma in syn.lemmas():\n\t\tfor antonym in lemma.antonyms():\n\t\t\tantonyms.add(antonym.name())\n\nif len(antonyms) == 1:\n\treturn antonyms.pop()\nelse:\n\treturn None", "path": "replacers.py", "repo_name": "japerk/nltk3-cookbook", "stars": 142, "license": "None", "language": "python", "size": 200}
{"docstring": "'''\n>>> bag_of_non_stopwords(['the', 'quick', 'brown', 'fox'])\n{'quick': True, 'brown': True, 'fox': True}\n'''\n", "func_signal": "def bag_of_non_stopwords(words, stopfile='english'):\n", "code": "badwords = stopwords.words(stopfile)\nreturn bag_of_words_not_in_set(words, badwords)", "path": "featx.py", "repo_name": "japerk/nltk3-cookbook", "stars": 142, "license": "None", "language": "python", "size": 200}
{"docstring": "'''Convert a parse tree to a sentence, with correct punctuation.\n>>> from nltk.tree import Tree\n>>> chunk_tree_to_sent(Tree('S', [Tree('NP', [('Pierre', 'NNP'), ('Vinken', 'NNP')]), (',', ','), Tree('NP', [('61', 'CD'), ('years', 'NNS')]), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), Tree('NP', [('the', 'DT'), ('board', 'NN')]), ('as', 'IN'), Tree('NP', [('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD')]), ('.', '.')]))\n'Pierre Vinken, 61 years old, will join the board as a nonexecutive director Nov. 29.'\n'''\n", "func_signal": "def chunk_tree_to_sent(tree, concat=' '):\n", "code": "s = concat.join(nltk.tag.untag(tree.leaves()))\nreturn re.sub(punct_re, r'\\g<1>', s)", "path": "transforms.py", "repo_name": "japerk/nltk3-cookbook", "stars": 142, "license": "None", "language": "python", "size": 200}
{"docstring": "'''Correct plural/singular verb mistakes.\n>>> correct_verbs([('is', 'VBZ'), ('our', 'PRP$'), ('children', 'NNS'), ('learning', 'VBG')])\n[('are', 'VBP'), ('our', 'PRP$'), ('children', 'NNS'), ('learning', 'VBG')]\n>>> correct_verbs([('our', 'PRP$'), ('children', 'NNS'), ('is', 'VBZ'), ('learning', 'VBG')])\n[('our', 'PRP$'), ('children', 'NNS'), ('are', 'VBP'), ('learning', 'VBG')]\n>>> correct_verbs([('our', 'PRP$'), ('child', 'NN'), ('were', 'VBD'), ('learning', 'VBG')])\n[('our', 'PRP$'), ('child', 'NN'), ('was', 'VBD'), ('learning', 'VBG')]\n>>> correct_verbs([('our', 'PRP$'), ('child', 'NN'), ('is', 'VBZ'), ('learning', 'VBG')])\n[('our', 'PRP$'), ('child', 'NN'), ('is', 'VBZ'), ('learning', 'VBG')]\n'''\n", "func_signal": "def correct_verbs(chunk):\n", "code": "vbidx = first_chunk_index(chunk, tag_startswith('VB'))\n# if no verb found, do nothing\nif vbidx is None:\n\treturn chunk\n\nverb, vbtag = chunk[vbidx]\nnnpred = tag_startswith('NN')\n# find nearest noun to the right of verb\nnnidx = first_chunk_index(chunk, nnpred, start=vbidx+1)\n# if no noun found to right, look to the left\nif nnidx is None:\n\tnnidx = first_chunk_index(chunk, nnpred, start=vbidx-1, step=-1)\n# if no noun found, do nothing\nif nnidx is None:\n\treturn chunk\n\nnoun, nntag = chunk[nnidx]\n# get correct verb form and insert into chunk\nif nntag.endswith('S'):\n\tchunk[vbidx] = plural_verb_forms.get((verb, vbtag), (verb, vbtag))\nelse:\n\tchunk[vbidx] = singular_verb_forms.get((verb, vbtag), (verb, vbtag))\n\nreturn chunk", "path": "transforms.py", "repo_name": "japerk/nltk3-cookbook", "stars": 142, "license": "None", "language": "python", "size": 200}
{"docstring": "'''Go through chunk and return the first index where pred(chunk[index])\nreturns True.\n>>> first_chunk_index([('61', 'CD'), ('years', 'NNS')], tag_equals('CD'))\n0\n>>> first_chunk_index([('61', 'CD'), ('years', 'NNS')], tag_equals('NNS'))\n1\n>>> first_chunk_index([('61', 'CD'), ('years', 'NNS')], tag_equals('CD'), start=1, step=-1)\n0\n>>> first_chunk_index([('61', 'CD'), ('years', 'NNS')], tag_equals('VB'))\n'''\n", "func_signal": "def first_chunk_index(chunk, pred, start=0, step=1):\n", "code": "l = len(chunk)\nend = l if step > 0 else -1\n\nfor i in range(start, end, step):\n\tif pred(chunk[i]):\n\t\treturn i\n\nreturn None", "path": "transforms.py", "repo_name": "japerk/nltk3-cookbook", "stars": 142, "license": "None", "language": "python", "size": 200}
{"docstring": "'''\n>>> convert_tree_labels(Tree('S', [Tree('NP-SBJ', [('foo', 'NN')])]), {'NP-SBJ': 'NP'})\nTree('S', [Tree('NP', [('foo', 'NN')])])\n'''\n", "func_signal": "def convert_tree_labels(tree, mapping):\n", "code": "children = []\n\nfor t in tree:\n\tif isinstance(t, Tree):\n\t\tchildren.append(convert_tree_labels(t, mapping))\n\telse:\n\t\tchildren.append(t)\n\nlabel = mapping.get(tree.label(), tree.label())\nreturn Tree(label, children)", "path": "transforms.py", "repo_name": "japerk/nltk3-cookbook", "stars": 142, "license": "None", "language": "python", "size": 200}
{"docstring": "'''If a plural noun is followed by another noun, singularize the plural noun.\n>>> singularize_plural_noun([('recipes', 'NNS'), ('book', 'NN')])\n[('recipe', 'NN'), ('book', 'NN')]\n'''\n", "func_signal": "def singularize_plural_noun(chunk):\n", "code": "nnsidx = first_chunk_index(chunk, tag_equals('NNS'))\n\nif nnsidx is not None and nnsidx+1 < len(chunk) and chunk[nnsidx+1][1][:2] == 'NN':\n\tnoun, nnstag = chunk[nnsidx]\n\tchunk[nnsidx] = (noun.rstrip('s'), nnstag.rstrip('S'))\n\nreturn chunk", "path": "transforms.py", "repo_name": "japerk/nltk3-cookbook", "stars": 142, "license": "None", "language": "python", "size": 200}
{"docstring": "'''\n>>> transform_chunk([('the', 'DT'), ('book', 'NN'), ('of', 'IN'), ('recipes', 'NNS'), ('is', 'VBZ'), ('delicious', 'JJ')])\n[('delicious', 'JJ'), ('recipe', 'NN'), ('book', 'NN')]\n'''\n", "func_signal": "def transform_chunk(chunk, chain=[filter_insignificant, swap_verb_phrase, swap_infinitive_phrase, singularize_plural_noun], trace=0):\n", "code": "for f in chain:\n\tchunk = f(chunk)\n\t\n\tif trace:\n\t\tprint('%s : %s' % (f.__name__, chunk))\n\nreturn chunk", "path": "transforms.py", "repo_name": "japerk/nltk3-cookbook", "stars": 142, "license": "None", "language": "python", "size": 200}
{"docstring": "# lock for writing, released when fp is closed\n", "func_signal": "def append_line(fname, line):\n", "code": "with lockfile.FileLock(fname):\n\tfp = open(fname, 'a+')\n\tfp.write(line)\n\tfp.write('\\n')\n\tfp.close()", "path": "corpus.py", "repo_name": "japerk/nltk3-cookbook", "stars": 142, "license": "None", "language": "python", "size": 200}
{"docstring": "\"\"\"Generate signatures for the entry point of a PE file.\n\nCreates a signature whose name will be the parameter 'name'\nand the section number and its name.\n\"\"\"\n\n", "func_signal": "def generate_ep_signature(self, pe, name, sig_length=512):\n", "code": "offset = pe.get_offset_from_rva(pe.OPTIONAL_HEADER.AddressOfEntryPoint)\n\nreturn self.__generate_signature(\n    pe, offset, name, ep_only=True, sig_length=sig_length)", "path": "peutils.py", "repo_name": "hiddenillusion/AnalyzePE", "stars": 197, "license": "None", "language": "python", "size": 164}
{"docstring": "\"\"\"Returns the next Range of the file that is to be hashed.\n\nFor all fingers, inspect their next expected range, and return the\nlowest uninterrupted range of interest. If the range is larger than\nBLOCK_SIZE, truncate it.\n\nReturns:\n  Next range of interest in a Range namedtuple.\n\"\"\"\n", "func_signal": "def _GetNextInterval(self):\n", "code": "starts = set([x.CurrentRange().start for x in self.fingers if x.ranges])\nends = set([x.CurrentRange().end for x in self.fingers if x.ranges])\nif not starts:\n  return None\nmin_start = min(starts)\nstarts.remove(min_start)\nends |= starts\nmin_end = min(ends)\nif min_end - min_start > self.BLOCK_SIZE:\n  min_end = min_start + self.BLOCK_SIZE\nreturn Range(min_start, min_end)", "path": "fingerprint.py", "repo_name": "hiddenillusion/AnalyzePE", "stars": 197, "license": "None", "language": "python", "size": 164}
{"docstring": "\"\"\" Determine if a PE's entry point is suspicious \"\"\"\n", "func_signal": "def check_ep_section(self, pe):\n", "code": "name = ''\nep = pe.OPTIONAL_HEADER.AddressOfEntryPoint\npos = 0\nfor sec in pe.sections:\n    if (ep >= sec.VirtualAddress) and \\\n       (ep < (sec.VirtualAddress + sec.Misc_VirtualSize)):\n        name = sec.Name.replace('\\x00', '')\n        break\n    else: \n        pos += 1\nreturn (ep, name, pos)", "path": "pescanner.py", "repo_name": "hiddenillusion/AnalyzePE", "stars": 197, "license": "None", "language": "python", "size": 164}
{"docstring": "\"\"\"Parses PECOFF headers.\n\nReads header magic and some data structures in a file to determine if\nit is a valid PECOFF header, and figure out the offsets at which\nrelevant data is stored.\nWhile this code contains multiple seeks and small reads, that is\ncompensated by the underlying libc buffering mechanism.\n\nReturns:\n  None if the parsed file is not PECOFF.\n  A dict with offsets and lengths for CheckSum, CertTable, and SignedData\n  fields in the PECOFF binary, for those that are present.\n\"\"\"\n", "func_signal": "def _PecoffHeaderParser(self):\n", "code": "extents = {}\nself.file.seek(0, os.SEEK_SET)\nbuf = self.file.read(2)\nif buf != 'MZ':\n  return None\nself.file.seek(0x3C, os.SEEK_SET)\nbuf = self.file.read(4)\npecoff_sig_offset = struct.unpack('<I', buf)[0]\nif pecoff_sig_offset >= self.filelength:\n  return None\nself.file.seek(pecoff_sig_offset, os.SEEK_SET)\nbuf = self.file.read(4)\nif buf != 'PE\\0\\0':\n  return None\nself.file.seek(pecoff_sig_offset + 20, os.SEEK_SET)\nbuf = self.file.read(2)\noptional_header_size = struct.unpack('<H', buf)[0]\noptional_header_offset = pecoff_sig_offset + 4 + 20\nif optional_header_size + optional_header_offset > self.filelength:\n  # This is not strictly a failure for windows, but such files better\n  # be treated as generic files. They can not be carrying SignedData.\n  return None\nif optional_header_size < 68:\n  # We can't do authenticode-style hashing. If this is a valid binary,\n  # which it can be, the header still does not even contain a checksum.\n  return None\nself.file.seek(optional_header_offset, os.SEEK_SET)\nbuf = self.file.read(2)\nimage_magic = struct.unpack('<H', buf)[0]\nif image_magic == 0x10b:\n  # 32 bit\n  rva_base = optional_header_offset + 92\n  cert_base = optional_header_offset + 128\nelif image_magic == 0x20b:\n  # 64 bit\n  rva_base = optional_header_offset + 108\n  cert_base = optional_header_offset + 144\nelse:\n  # A ROM image or such, not in the PE/COFF specs. Not sure what to do.\n  return None\nextents['CheckSum'] = RelRange(optional_header_offset + 64, 4)\nself.file.seek(rva_base, os.SEEK_SET)\nbuf = self.file.read(4)\nnumber_of_rva = struct.unpack('<I', buf)[0]\nif (number_of_rva < 5 or\n    optional_header_offset + optional_header_size < cert_base + 8):\n  return extents\nextents['CertTable'] = RelRange(cert_base, 8)\n\nself.file.seek(cert_base, os.SEEK_SET)\nbuf = self.file.read(8)\nstart, length = struct.unpack('<II', buf)\nif (length == 0 or\n    start < optional_header_offset + optional_header_size or\n    start + length > self.filelength):\n  # The location of the SignedData blob is just wrong (or there is none).\n  # Ignore it -- everything else we did still makes sense.\n  return extents\nextents['SignedData'] = RelRange(start, length)\nreturn extents", "path": "fingerprint.py", "repo_name": "hiddenillusion/AnalyzePE", "stars": 197, "license": "None", "language": "python", "size": 164}
{"docstring": "\"\"\" Determine the version info in a PE file \"\"\"\n", "func_signal": "def check_verinfo(self, pe):\n", "code": "ret = []\n\nif hasattr(pe, 'VS_VERSIONINFO'):\n    if hasattr(pe, 'FileInfo'):\n        for entry in pe.FileInfo:\n            if hasattr(entry, 'StringTable'):\n                for st_entry in entry.StringTable:\n                    for str_entry in st_entry.entries.items():\n                        # yes... it annoyed me that much .. ocd whatttt\n                        if 'OriginalFilename' in str_entry:\n                            ret.append(convert_to_printable(str_entry[0]) + ': ' + convert_to_printable(str_entry[1]) )\n                        else:\n                            ret.append(convert_to_printable(str_entry[0]) + '\\t: ' + convert_to_printable(str_entry[1]) )\n            elif hasattr(entry, 'Var'):\n                for var_entry in entry.Var:\n                    if hasattr(var_entry, 'entry'):\n                        ret.append(convert_to_printable(var_entry.entry.keys()[0]) + '\\t: ' + var_entry.entry.values()[0])\nreturn '\\n'.join(ret)", "path": "pescanner.py", "repo_name": "hiddenillusion/AnalyzePE", "stars": 197, "license": "None", "language": "python", "size": 164}
{"docstring": "\"\"\"\nunusual locations of import tables\nnon recognized section names\npresence of long ASCII strings\n\"\"\"\n\n", "func_signal": "def is_suspicious( pe ):\n", "code": "relocations_overlap_entry_point = False\nsequential_relocs = 0\n\n# If relocation data is found and the entries go over the entry point, and also are very\n# continuous or point outside section's boundaries => it might imply that an obfuscation\n# trick is being used or the relocations are corrupt (maybe intentionally)\n#\nif hasattr(pe, 'DIRECTORY_ENTRY_BASERELOC'):\n    for base_reloc in pe.DIRECTORY_ENTRY_BASERELOC:\n        last_reloc_rva = None\n        for reloc in base_reloc.entries:\n            if reloc.rva <= pe.OPTIONAL_HEADER.AddressOfEntryPoint <= reloc.rva + 4:\n                relocations_overlap_entry_point = True\n                \n            if last_reloc_rva is not None and last_reloc_rva <= reloc.rva <= last_reloc_rva + 4:\n                sequential_relocs += 1\n                \n            last_reloc_rva = reloc.rva\n\n\n\n# If import tables or strings exist (are pointed to) to within the header or in the area \n# between the PE header and the first section that's supicious\n#\n# IMPLEMENT\n\n\nwarnings_while_parsing = False\n# If we have warnings, that's suspicious, some of those will be because of out-of-ordinary\n# values are found in the PE header fields\n# Things that are reported in warnings: \n# (parsing problems, special section characteristics i.e. W & X, uncommon values of fields,\n# unusual entrypoint, suspicious imports)\n# \nwarnings = pe.get_warnings()\nif warnings:\n    warnings_while_parsing\n\n# If there are few or none (should come with a standard \"density\" of strings/kilobytes of data) longer (>8) \n# ascii sequences that might indicate packed data, (this is similar to the entropy test in some ways but\n# might help to discard cases of legitimate installer or compressed data)\n\n# If compressed data (high entropy) and is_driver => uuuuhhh, nasty\n\npass", "path": "peutils.py", "repo_name": "hiddenillusion/AnalyzePE", "stars": 197, "license": "None", "language": "python", "size": 164}
{"docstring": "\"\"\"Consumes an entire range, or part thereof.\n\nIf the finger has no ranges left, or the curent range start is higher\nthan the end of the consumed block, nothing happens. Otherwise,\nthe current range is adjusted for the consumed block, or removed,\nif the entire block is consumed. For things to work, the consumed\nrange and the current finger starts must be equal, and the length\nof the consumed range may not exceed the length of the current range.\n\nArgs:\n  start: Beginning of range to be consumed.\n  end: First offset after the consumed range (end + 1).\n\nRaises:\n  RuntimeError: if the start position of the consumed range is\n      higher than the start of the current range in the finger, or if\n      the consumed range cuts accross block boundaries.\n\"\"\"\n", "func_signal": "def ConsumeRange(self, start, end):\n", "code": "old = self.CurrentRange()\nif old is None:\n  return\nif old.start > start:\n  if old.start < end:\n    raise RuntimeError('Block end too high.')\n  return\nif old.start < start:\n  raise RuntimeError('Block start too high.')\nif old.end == end:\n  del(self.ranges[0])\nelif old.end > end:\n  self.ranges[0] = Range(end, old.end)\nelse:\n  raise RuntimeError('Block length exceeds range.')", "path": "fingerprint.py", "repo_name": "hiddenillusion/AnalyzePE", "stars": 197, "license": "None", "language": "python", "size": 164}
{"docstring": "\"\"\"\nsource  : http://sourceforge.net/projects/malclassifier.adobe/\nscoring : 0 = clean, 1 = dirty or unkown\n\"\"\"\n", "func_signal": "def adobe_classifer(file):\n", "code": "cmd = 'python AdobeMalwareClassifier.py' + ' ' + '-f' + ' ' + file\np = subprocess.Popen(cmd,stderr=subprocess.PIPE,stdout=subprocess.PIPE,shell=True)\n(stdout, stderr) = p.communicate()\nif stdout:\n    if \"0\" in stdout: return \"Clean\"\n    elif \"1\" in stdout: return \"Dirty\"\n    # ! repetitive print here but don't want to stop the analysis if something goes wrong\n    elif \"UNKNOWN\" in stdout: return \"Unknown\"", "path": "AnalyzePE.py", "repo_name": "hiddenillusion/AnalyzePE", "stars": 197, "license": "None", "language": "python", "size": 164}
{"docstring": "\"\"\"Utility function to print out the results string. Very basic.\n\nJust a quick hack so main can give results to look at.\n\nArgs:\n  file_obj: File object whose name will be printed out.\n  results: Array of dicts with all the results in it.\n\nReturns:\n  Printable string for the contents of the results array.\n\"\"\"\n", "func_signal": "def FormatResults(file_obj, results):\n", "code": "out = file_obj.name + ':\\n'\nfor result in results:\n  out += '  ' + result['name'] + ' fingerprint type\\n'\n  for key, value in sorted(result.items()):\n    if key == 'name':\n      continue\n    out += '    ' + key + ': '\n    # this is soooo unelegant. python can do better, I am sure.\n    # quick and dirty for now. functional style might win later.\n    if type(value) is list:\n      for v in value:\n        if type(v) is tuple:\n          out += ('(rev=%d, type=%d, cert len=%d bytes)' %\n                  (v[0], v[1], len(v[2])))\n        else:\n          out += v.encode('hex') + ','\n    else:\n      out += value.encode('hex')\n    out += '\\n'\nreturn out", "path": "fingerprint.py", "repo_name": "hiddenillusion/AnalyzePE", "stars": 197, "license": "None", "language": "python", "size": 164}
{"docstring": "\"\"\"Given a data block, feed it to all the registered hashers.\"\"\"\n", "func_signal": "def HashBlock(self, block):\n", "code": "for hasher in self.hashers:\n  hasher.update(block)", "path": "fingerprint.py", "repo_name": "hiddenillusion/AnalyzePE", "stars": 197, "license": "None", "language": "python", "size": 164}
{"docstring": "\"\"\"If the file is a PE/COFF file, computes authenticode hashes on it.\n\nThis checks if the input file is a valid PE/COFF image file (e.g. a\nWindows binary, driver, or DLL) and if yes, sets up a 'finger' for\nfingerprinting in Authenticode style.\nIf available, the 'SignedData' section of the image file is retrieved,\nand parsed into its constituent parts. An array of tuples of these\nparts is added to results by HashIt()\n\nArgs:\n  hashers: An iterable of hash classes (e.g. out of hashlib) which will\n           be instantiated for use. If 'None' is provided, a default set\n           of hashers is used. To select no hash function (e.g. to only\n           extract metadata), use an empty iterable.\n\nReturns:\n  True if the file is detected as a valid PE/COFF image file,\n  False otherwise.\n\"\"\"\n", "func_signal": "def EvalPecoff(self, hashers=None):\n", "code": "try:\n  extents = self._PecoffHeaderParser()\nexcept struct.error:\n  # Parsing the header failed. Just ignore this, and claim\n  # that the file is not a valid PE/COFF image file.\n  extents = None\nif extents is None:\n  return False\n\nsigned_data = None\nranges = []\nstart = 0\n# Ordering of these conditions matches expected order in file.\n# If a condition holds true, the matching range is skipped for hashing.\nif 'CheckSum' in extents:\n  ranges.append(Range(start, end=extents['CheckSum'].start))\n  start = sum(extents['CheckSum'])\n  # New start now points past CheckSum area.\nif 'CertTable' in extents:\n  ranges.append(Range(start, end=extents['CertTable'].start))\n  start = sum(extents['CertTable'])\n  # New start now points past CertTable area.\nif 'SignedData' in extents:\n  # Exclude the range even if the blob itself can't be parsed correctly.\n  ranges.append(Range(start, end=extents['SignedData'].start))\n  start = sum(extents['SignedData'])\n  # New start now points past SignedData area.\n  signed_data = self._CollectSignedData(extents['SignedData'])\nranges.append(Range(start, end=self.filelength))\n\nif hashers is None:\n  hashers = Fingerprinter.AUTHENTICODE_HASH_CLASSES\nhashfuncs = [x() for x in hashers]\nmetadata = {'name': 'pecoff'}\nif signed_data:\n  metadata['SignedData'] = signed_data\nfinger = Finger(hashfuncs, ranges, metadata)\nself.fingers.append(finger)\nreturn True", "path": "fingerprint.py", "repo_name": "hiddenillusion/AnalyzePE", "stars": 197, "license": "None", "language": "python", "size": 164}
{"docstring": "\"\"\"Matches and returns all the likely matches.\"\"\"\n\n", "func_signal": "def match_all(self, pe, ep_only=True, section_start_only=False):\n", "code": "matches = self.__match(pe, ep_only, section_start_only)\n\nif matches:\n    if ep_only == False:\n        # Get the most exact match for each list of matches\n        # at a given offset\n        #\n        return matches\n    \n    return matches[1]\n\nreturn None", "path": "peutils.py", "repo_name": "hiddenillusion/AnalyzePE", "stars": 197, "license": "None", "language": "python", "size": 164}
{"docstring": "\"\"\"\nsource: http://securityxploded.com/exe-scan.php\nnotes:  using the peutils version from : http://malware-analysis.googlecode.com/svn-history/r74/trunk/MalwareAnalysis/malware_analysis/pe_struct/peutils.py\n\"\"\"\n", "func_signal": "def anomalies(file):\n", "code": "ret = []\n\n# Entropy based check.. imported from peutils\npack = peutils.is_probably_packed(pe)\nif pack == 1:\n    ret.append(\"Based on the sections entropy check, the file is possibly packed\")\n\n# SizeOfRawData Check.. some times size of raw data value is used to crash some debugging tools.\nnsec = pe.FILE_HEADER.NumberOfSections\nfor i in range(0,nsec-1):\n    if i == nsec-1:\n        break", "path": "AnalyzePE.py", "repo_name": "hiddenillusion/AnalyzePE", "stars": 197, "license": "None", "language": "python", "size": 164}
{"docstring": "\"\"\"There are two versions of python-magic floating around, and annoyingly, the interface \nchanged between versions, so we try one method and if it fails, then we try the other.\nNOTE: you may need to alter the magic_file for your system to point to the magic file.\"\"\"\n", "func_signal": "def get_filetype(data):\n", "code": "if sys.modules.has_key('magic'):\n    try:\n        ms = magic.open(magic.MAGIC_NONE) \n        ms.load() \n        return ms.buffer(data)\n    except:\n        try:\n            return magic.from_buffer(data)\n        except magic.MagicException:\n            magic_custom = magic.Magic(magic_file='C:\\windows\\system32\\magic')\n            return magic_custom.from_buffer(data)\nreturn ''", "path": "pescanner.py", "repo_name": "hiddenillusion/AnalyzePE", "stars": 197, "license": "None", "language": "python", "size": 164}
{"docstring": "\"\"\"Finalizing function for the Fingerprint class.\n\nThis method applies all the different hash functions over the\npreviously specified different ranges of the input file, and\ncomputes the resulting hashes.\n\nAfter calling HashIt, the state of the object is reset to its\ninitial state, with no fingers defined.\n\nReturns:\n  An array of dicts, with each dict containing name of fingerprint\n  type, names of hashes and values, and additional, type-dependent\n  key / value pairs, such as an array of SignedData tuples for the\n  PE/COFF fingerprint type.\n\nRaises:\n   RuntimeError: when internal inconsistencies occur.\n\"\"\"\n", "func_signal": "def HashIt(self):\n", "code": "while True:\n  interval = self._GetNextInterval()\n  if interval is None:\n    break\n  self.file.seek(interval.start, os.SEEK_SET)\n  block = self.file.read(interval.end - interval.start)\n  if len(block) != interval.end - interval.start:\n    raise RuntimeError('Short read on file.')\n  self._HashBlock(block, interval.start, interval.end)\n  self._AdjustIntervals(interval.start, interval.end)\n\nresults = []\nfor finger in self.fingers:\n  res = {}\n  leftover = finger.CurrentRange()\n  if leftover:\n    if (len(finger.ranges) > 1 or\n        leftover.start != self.filelength or\n        leftover.end != self.filelength):\n      raise RuntimeError('Non-empty range remains.')\n  res.update(finger.metadata)\n  for hasher in finger.hashers:\n    res[hasher.name] = hasher.digest()\n  results.append(res)\n\n# Clean out things for a fresh start (on the same file object).\nself.fingers = []\n\n# Make sure the results come back in 'standard' order, regardless of the\n# order in which fingers were added. Helps with reproducing test results.\nreturn sorted(results, key=lambda r: r['name'])", "path": "fingerprint.py", "repo_name": "hiddenillusion/AnalyzePE", "stars": 197, "license": "None", "language": "python", "size": 164}
{"docstring": "\"\"\"\nsource: https://code.google.com/p/pyew\n\"\"\"\n", "func_signal": "def antivm(file):\n", "code": "tricks = {\n    \"Red Pill\":\"\\x0f\\x01\\x0d\\x00\\x00\\x00\\x00\\xc3\",\n    \"VirtualPc trick\":\"\\x0f\\x3f\\x07\\x0b\",\n    \"VMware trick\":\"VMXh\",\n    \"VMCheck.dll\":\"\\x45\\xC7\\x00\\x01\",\n    \"VMCheck.dll for VirtualPC\":\"\\x0f\\x3f\\x07\\x0b\\xc7\\x45\\xfc\\xff\\xff\\xff\\xff\",\n    \"Xen\":\"XenVMM\",\n    \"Bochs & QEmu CPUID Trick\":\"\\x44\\x4d\\x41\\x63\",\n    \"Torpig VMM Trick\": \"\\xE8\\xED\\xFF\\xFF\\xFF\\x25\\x00\\x00\\x00\\xFF\\x33\\xC9\\x3D\\x00\\x00\\x00\\x80\\x0F\\x95\\xC1\\x8B\\xC1\\xC3\",\n    \"Torpig (UPX) VMM Trick\": \"\\x51\\x51\\x0F\\x01\\x27\\x00\\xC1\\xFB\\xB5\\xD5\\x35\\x02\\xE2\\xC3\\xD1\\x66\\x25\\x32\\xBD\\x83\\x7F\\xB7\\x4E\\x3D\\x06\\x80\\x0F\\x95\\xC1\\x8B\\xC1\\xC3\"\n      }\nret = []\nwith open(file,\"rb\") as f:\n    buf = f.read()\n    for trick in tricks:\n        pos = buf.find(tricks[trick])\n        if pos > -1:\n            ret.append(\"\\t[+] 0x%x %s\" % (pos, trick))\n\nif len(ret):\n    resp = \"Yes\"\n    if verb == True:\n        antis = '\\n'.join(ret)\n        resp = resp + '\\n' + antis\n    return resp", "path": "AnalyzePE.py", "repo_name": "hiddenillusion/AnalyzePE", "stars": 197, "license": "None", "language": "python", "size": 164}
{"docstring": "\"\"\"There are two Python bindings for ssdeep, each with a different interface. So we try\nJose's pyssdeep first and if it fails, try the one from pypi. Just install one or the other:\nhttp://code.google.com/p/pyssdeep/\nhttp://pypi.python.org/packages/source/s/ssdeep/ssdeep-2.5.tar.gz#md5=fd9e5271c01ca389cc621ae306327ab6\n\"\"\"\n", "func_signal": "def get_ssdeep(filename):\n", "code": "try:\n    from ssdeep import ssdeep \n    s = ssdeep()\n    return s.hash_file(filename)\nexcept:\n    try:\n        import ssdeep\n        return ssdeep.hash_from_file(filename)\n    except:\n        pass\nreturn ''", "path": "pescanner.py", "repo_name": "hiddenillusion/AnalyzePE", "stars": 197, "license": "None", "language": "python", "size": 164}
{"docstring": "\"\"\"Causes the entire file to be hashed by the given hash functions.\n\nThis sets up a 'finger' for fingerprinting, where the entire file\nis passed through a pre-defined (or user defined) set of hash functions.\n\nArgs:\n  hashers: An iterable of hash classes (e.g. out of hashlib) which will\n           be instantiated for use. If hashers is not provided, or is\n           provided as 'None', the default hashers will get used. To\n           invoke this without hashers, provide an empty list.\n\nReturns:\n  Always True, as all files are 'generic' files.\n\"\"\"\n", "func_signal": "def EvalGeneric(self, hashers=None):\n", "code": "if hashers is None:\n  hashers = Fingerprinter.GENERIC_HASH_CLASSES\nhashfuncs = [x() for x in hashers]\nfinger = Finger(hashfuncs,\n                [Range(0, self.filelength)],\n                {'name': 'generic'})\nself.fingers.append(finger)\nreturn True", "path": "fingerprint.py", "repo_name": "hiddenillusion/AnalyzePE", "stars": 197, "license": "None", "language": "python", "size": 164}
{"docstring": "\"\"\"\nsource: https://code.google.com/p/pyew/\nnotes:  loading a file in pyew may take some time if it has to analyze all of the functions\n\"\"\"\n#pyew.codeanalysis = False # ... will shows initial hex dump\n#pyew.loadFile(file) # from pyew_core ... will load file & give basic overview\n#pyew.loadFile(file) # from pyew_core ... will load file & give basic overview\n#pyew.plugins[\"url\"](pyew)(file)\n#ret = []\n#check = pyew.plugins[\"url\"](pyew)\n#if len(check):\n#    resp = \"Yes\"\n#    if verb == True:\n#        for site in check: \n#            ret.append('\\t' + site)\n#        urls = '\\n'.join(ret)\n#        resp = resp + '\\n' + urls\n#    return resp\n\n", "func_signal": "def urlcheck(file):\n", "code": "def doFind(x, buf):\n    ret = []\n    for l in x.findall(buf, re.IGNORECASE | re.MULTILINE):\n        for url in l:\n            if len(url) > 8 and url not in ret:\n                ret.append(url)\n    return ret\n\nurl_regex = [\n    re.compile(\"((http|ftp|mailto|telnet|ssh)(s){0,1}\\:\\/\\/[\\w|\\/|\\.|\\#|\\?|\\&|\\=|\\-|\\%]+)+\", re.IGNORECASE | re.MULTILINE)\n]\n\nwith open(file, 'rb') as f:\n    f.seek(0)\n    buf = f.read()\n    ret = []\n    urls = []\n\n    # ASCII check\n    for x in url_regex:\n        ret += doFind(x, buf)\n\n    # UNICODE check\n    buf = buf.replace(\"\\x00\", \"\")\n    for x in url_regex:\n        ret += doFind(x, buf)\n\n# Uniquely print them so no duplicates from ASCII/UNICODE\nif len(ret):\n    resp = \"Yes\"\n    if verb == True:\n        all_urls = []\n        for site in list(set(ret)):\n                urls.append('\\t[+] ' + site)\n        all_urls = '\\n'.join(urls)\n        resp = resp + '\\n' + all_urls\n    return resp", "path": "AnalyzePE.py", "repo_name": "hiddenillusion/AnalyzePE", "stars": 197, "license": "None", "language": "python", "size": 164}
{"docstring": "\"\"\"_HashBlock feeds data blocks into the hashers of fingers.\n\nThis function must be called before adjusting fingers for next\ninterval, otherwise the lack of remaining ranges will cause the\nblock not to be hashed for a specific finger.\n\nStart and end are used to validate the expected ranges, to catch\nunexpected use of that logic.\n\nArgs:\n  block: The data block.\n  start: Beginning offset of this block.\n  end: Offset of the next byte after the block.\n\nRaises:\n  RuntimeError: If the provided and expected ranges don't match.\n\"\"\"\n", "func_signal": "def _HashBlock(self, block, start, end):\n", "code": "for finger in self.fingers:\n  expected_range = finger.CurrentRange()\n  if expected_range is None:\n    continue\n  if (start > expected_range.start or\n      (start == expected_range.start and end > expected_range.end) or\n      (start < expected_range.start and end > expected_range.start)):\n    raise RuntimeError('Cutting across fingers.')\n  if start == expected_range.start:\n    finger.HashBlock(block)", "path": "fingerprint.py", "repo_name": "hiddenillusion/AnalyzePE", "stars": 197, "license": "None", "language": "python", "size": 164}
{"docstring": "\"\"\"Compute and return the source text (all equalities and deletions).\n\nArgs:\n    diffs: Array of diff tuples.\n\nReturns:\n    Source text.\n\"\"\"\n", "func_signal": "def diff_text1(self, diffs):\n", "code": "text = []\nfor (op, data) in diffs:\n    if op != self.DIFF_INSERT:\n        text.append(data)\nreturn \"\".join(text)", "path": "plugin\\floo\\common\\lib\\diff_match_patch.py", "repo_name": "Floobits/floobits-vim", "stars": 247, "license": "apache-2.0", "language": "python", "size": 1990}
{"docstring": "\"\"\"Split two texts into an array of strings.  Reduce the texts to a string\nof hashes where each Unicode character represents one line.\n\nArgs:\n    text1: First string.\n    text2: Second string.\n\nReturns:\n    Three element tuple, containing the encoded text1, the encoded text2 and\n    the array of unique strings.  The zeroth element of the array of unique\n    strings is intentionally blank.\n\"\"\"\n", "func_signal": "def diff_linesToChars(self, text1, text2):\n", "code": "lineArray = []  # e.g. lineArray[4] == \"Hello\\n\"\nlineHash = {}   # e.g. lineHash[\"Hello\\n\"] == 4\n\n# \"\\x00\" is a valid character, but various debuggers don't like it.\n# So we'll insert a junk entry to avoid generating a null character.\nlineArray.append('')\n\ndef diff_linesToCharsMunge(text):\n    \"\"\"Split a text into an array of strings.  Reduce the texts to a string\n    of hashes where each Unicode character represents one line.\n    Modifies linearray and linehash through being a closure.\n\n    Args:\n        text: String to encode.\n\n    Returns:\n        Encoded string.\n    \"\"\"\n    chars = []\n    # Walk the text, pulling out a substring for each line.\n    # text.split('\\n') would would temporarily double our memory footprint.\n    # Modifying text would create many large strings to garbage collect.\n    lineStart = 0\n    lineEnd = -1\n    while lineEnd < len(text) - 1:\n        lineEnd = text.find('\\n', lineStart)\n        if lineEnd == -1:\n            lineEnd = len(text) - 1\n        line = text[lineStart:lineEnd + 1]\n        lineStart = lineEnd + 1\n\n        if line in lineHash:\n            chars.append(unichr(lineHash[line]))\n        else:\n            lineArray.append(line)\n            lineHash[line] = len(lineArray) - 1\n            chars.append(unichr(len(lineArray) - 1))\n    return \"\".join(chars)\n\nchars1 = diff_linesToCharsMunge(text1)\nchars2 = diff_linesToCharsMunge(text2)\nreturn (chars1, chars2, lineArray)", "path": "plugin\\floo\\common\\lib\\diff_match_patch.py", "repo_name": "Floobits/floobits-vim", "stars": 247, "license": "apache-2.0", "language": "python", "size": 1990}
{"docstring": "\"\"\"Compute and return the destination text (all equalities and insertions).\n\nArgs:\n    diffs: Array of diff tuples.\n\nReturns:\n    Destination text.\n\"\"\"\n", "func_signal": "def diff_text2(self, diffs):\n", "code": "text = []\nfor (op, data) in diffs:\n    if op != self.DIFF_DELETE:\n        text.append(data)\nreturn \"\".join(text)", "path": "plugin\\floo\\common\\lib\\diff_match_patch.py", "repo_name": "Floobits/floobits-vim", "stars": 247, "license": "apache-2.0", "language": "python", "size": 1990}
{"docstring": "\"\"\"Determine the common prefix of two strings.\n\nArgs:\n    text1: First string.\n    text2: Second string.\n\nReturns:\n    The number of characters common to the start of each string.\n\"\"\"\n# Quick check for common null cases.\n", "func_signal": "def diff_commonPrefix(self, text1, text2):\n", "code": "if not text1 or not text2 or text1[0] != text2[0]:\n    return 0\n# Binary search.\n# Performance analysis: http://neil.fraser.name/news/2007/10/09/\npointermin = 0\npointermax = min(len(text1), len(text2))\npointermid = pointermax\npointerstart = 0\nwhile pointermin < pointermid:\n    if text1[pointerstart:pointermid] == text2[pointerstart:pointermid]:\n        pointermin = pointermid\n        pointerstart = pointermin\n    else:\n        pointermax = pointermid\n    pointermid = (pointermax - pointermin) // 2 + pointermin\nreturn pointermid", "path": "plugin\\floo\\common\\lib\\diff_match_patch.py", "repo_name": "Floobits/floobits-vim", "stars": 247, "license": "apache-2.0", "language": "python", "size": 1990}
{"docstring": "\"\"\"Locate the best instance of 'pattern' in 'text' near 'loc'.\n\nArgs:\n    text: The text to search.\n    pattern: The pattern to search for.\n    loc: The location to search around.\n\nReturns:\n    Best match index or -1.\n\"\"\"\n# Check for null inputs.\n", "func_signal": "def match_main(self, text, pattern, loc):\n", "code": "if text is None or pattern is None:\n    raise ValueError(\"Null inputs. (match_main)\")\n\nloc = max(0, min(loc, len(text)))\nif text == pattern:\n    # Shortcut (potentially not guaranteed by the algorithm)\n    return 0\nelif not text:\n    # Nothing to match.\n    return -1\nelif text[loc:loc + len(pattern)] == pattern:\n    # Perfect match at the perfect spot!  (Includes case of null pattern)\n    return loc\nelse:\n    # Do a fuzzy compare.\n    match = self.match_bitap(text, pattern, loc)\n    return match", "path": "plugin\\floo\\common\\lib\\diff_match_patch.py", "repo_name": "Floobits/floobits-vim", "stars": 247, "license": "apache-2.0", "language": "python", "size": 1990}
{"docstring": "\"\"\"Reorder and merge like edit sections.  Merge equalities.\nAny edit section can move as long as it doesn't cross an equality.\n\nArgs:\n    diffs: Array of diff tuples.\n\"\"\"\n", "func_signal": "def diff_cleanupMerge(self, diffs):\n", "code": "diffs.append((self.DIFF_EQUAL, ''))  # Add a dummy entry at the end.\npointer = 0\ncount_delete = 0\ncount_insert = 0\ntext_delete = ''\ntext_insert = ''\nwhile pointer < len(diffs):\n    if diffs[pointer][0] == self.DIFF_INSERT:\n        count_insert += 1\n        text_insert += diffs[pointer][1]\n        pointer += 1\n    elif diffs[pointer][0] == self.DIFF_DELETE:\n        count_delete += 1\n        text_delete += diffs[pointer][1]\n        pointer += 1\n    elif diffs[pointer][0] == self.DIFF_EQUAL:\n        # Upon reaching an equality, check for prior redundancies.\n        if count_delete + count_insert > 1:\n            if count_delete != 0 and count_insert != 0:\n                # Factor out any common prefixies.\n                commonlength = self.diff_commonPrefix(text_insert, text_delete)\n                if commonlength != 0:\n                    x = pointer - count_delete - count_insert - 1\n                    if x >= 0 and diffs[x][0] == self.DIFF_EQUAL:\n                        diffs[x] = (diffs[x][0], diffs[x][1] +\n                                    text_insert[:commonlength])\n                    else:\n                        diffs.insert(0, (self.DIFF_EQUAL, text_insert[:commonlength]))\n                        pointer += 1\n                    text_insert = text_insert[commonlength:]\n                    text_delete = text_delete[commonlength:]\n                # Factor out any common suffixies.\n                commonlength = self.diff_commonSuffix(text_insert, text_delete)\n                if commonlength != 0:\n                    diffs[pointer] = (diffs[pointer][0], text_insert[-commonlength:] +\n                                      diffs[pointer][1])\n                    text_insert = text_insert[:-commonlength]\n                    text_delete = text_delete[:-commonlength]\n            # Delete the offending records and add the merged ones.\n            if count_delete == 0:\n                diffs[(pointer - count_insert):pointer] = [(self.DIFF_INSERT, text_insert)]\n            elif count_insert == 0:\n                diffs[(pointer - count_delete):pointer] = [(self.DIFF_DELETE, text_delete)]\n            else:\n                diffs[(pointer - count_delete - count_insert):pointer] = [\n                    (self.DIFF_DELETE, text_delete),\n                    (self.DIFF_INSERT, text_insert)]\n            pointer = pointer - count_delete - count_insert + 1\n            if count_delete != 0:\n                pointer += 1\n            if count_insert != 0:\n                pointer += 1\n        elif pointer != 0 and diffs[pointer - 1][0] == self.DIFF_EQUAL:\n            # Merge this equality with the previous one.\n            diffs[pointer - 1] = (diffs[pointer - 1][0], diffs[pointer - 1][1] + diffs[pointer][1])\n            del diffs[pointer]\n        else:\n            pointer += 1\n\n        count_insert = 0\n        count_delete = 0\n        text_delete = ''\n        text_insert = ''\n\nif diffs[-1][1] == '':\n    diffs.pop()  # Remove the dummy entry at the end.\n\n# Second pass: look for single edits surrounded on both sides by equalities\n# which can be shifted sideways to eliminate an equality.\n# e.g: A<ins>BA</ins>C -> <ins>AB</ins>AC\nchanges = False\npointer = 1\n# Intentionally ignore the first and last element (don't need checking).\nwhile pointer < len(diffs) - 1:\n    if (diffs[pointer - 1][0] == self.DIFF_EQUAL and\n            diffs[pointer + 1][0] == self.DIFF_EQUAL):\n        # This is a single edit surrounded by equalities.\n        if diffs[pointer][1].endswith(diffs[pointer - 1][1]):\n            # Shift the edit over the previous equality.\n            diffs[pointer] = (diffs[pointer][0],\n                              diffs[pointer - 1][1] +\n                              diffs[pointer][1][:-len(diffs[pointer - 1][1])])\n            diffs[pointer + 1] = (diffs[pointer + 1][0],\n                                  diffs[pointer - 1][1] + diffs[pointer + 1][1])\n            del diffs[pointer - 1]\n            changes = True\n        elif diffs[pointer][1].startswith(diffs[pointer + 1][1]):\n            # Shift the edit over the next equality.\n            diffs[pointer - 1] = (diffs[pointer - 1][0],\n                                  diffs[pointer - 1][1] + diffs[pointer + 1][1])\n            diffs[pointer] = (diffs[pointer][0],\n                              diffs[pointer][1][len(diffs[pointer + 1][1]):] +\n                              diffs[pointer + 1][1])\n            del diffs[pointer + 1]\n            changes = True\n    pointer += 1\n\n# If shifts were made, the diff needs reordering and another shift sweep.\nif changes:\n    self.diff_cleanupMerge(diffs)", "path": "plugin\\floo\\common\\lib\\diff_match_patch.py", "repo_name": "Floobits/floobits-vim", "stars": 247, "license": "apache-2.0", "language": "python", "size": 1990}
{"docstring": "\"\"\"Take a list of patches and return a textual representation.\n\nArgs:\n    patches: Array of Patch objects.\n\nReturns:\n    Text representation of patches.\n\"\"\"\n", "func_signal": "def patch_toText(self, patches):\n", "code": "text = []\nfor patch in patches:\n    text.append(str(patch))\nreturn \"\".join(text)", "path": "plugin\\floo\\common\\lib\\diff_match_patch.py", "repo_name": "Floobits/floobits-vim", "stars": 247, "license": "apache-2.0", "language": "python", "size": 1990}
{"docstring": "\"\"\"Find the differences between two texts.  Simplifies the problem by\n    stripping any common prefix or suffix off the texts before diffing.\n\nArgs:\n    text1: Old string to be diffed.\n    text2: New string to be diffed.\n    checklines: Optional speedup flag.  If present and false, then don't run\n        a line-level diff first to identify the changed areas.\n        Defaults to true, which does a faster, slightly less optimal diff.\n    deadline: Optional time when the diff should be complete by.  Used\n        internally for recursive calls.  Users should set DiffTimeout instead.\n\nReturns:\n    Array of changes.\n\"\"\"\n# Set a deadline by which time the diff must be complete.\n", "func_signal": "def diff_main(self, text1, text2, checklines=True, deadline=None):\n", "code": "if deadline is None:\n    # Unlike in most languages, Python counts time in seconds.\n    if self.Diff_Timeout <= 0:\n        deadline = sys.maxsize\n    else:\n        deadline = time.time() + self.Diff_Timeout\n\n# Check for null inputs.\nif text1 is None or text2 is None:\n    raise ValueError(\"Null inputs. (diff_main)\")\n\n# Check for equality (speedup).\nif text1 == text2:\n    if text1:\n        return [(self.DIFF_EQUAL, text1)]\n    return []\n\n# Trim off common prefix (speedup).\ncommonlength = self.diff_commonPrefix(text1, text2)\ncommonprefix = text1[:commonlength]\ntext1 = text1[commonlength:]\ntext2 = text2[commonlength:]\n\n# Trim off common suffix (speedup).\ncommonlength = self.diff_commonSuffix(text1, text2)\nif commonlength == 0:\n    commonsuffix = ''\nelse:\n    commonsuffix = text1[-commonlength:]\n    text1 = text1[:-commonlength]\n    text2 = text2[:-commonlength]\n\n# Compute the diff on the middle block.\ndiffs = self.diff_compute(text1, text2, checklines, deadline)\n\n# Restore the prefix and suffix.\nif commonprefix:\n    diffs[:0] = [(self.DIFF_EQUAL, commonprefix)]\nif commonsuffix:\n    diffs.append((self.DIFF_EQUAL, commonsuffix))\nself.diff_cleanupMerge(diffs)\nreturn diffs", "path": "plugin\\floo\\common\\lib\\diff_match_patch.py", "repo_name": "Floobits/floobits-vim", "stars": 247, "license": "apache-2.0", "language": "python", "size": 1990}
{"docstring": "\"\"\"Rehydrate the text in a diff from a string of line hashes to real lines\nof text.\n\nArgs:\n    diffs: Array of diff tuples.\n    lineArray: Array of unique strings.\n\"\"\"\n", "func_signal": "def diff_charsToLines(self, diffs, lineArray):\n", "code": "for x in range(len(diffs)):\n    text = []\n    for char in diffs[x][1]:\n        text.append(lineArray[ord(char)])\n    diffs[x] = (diffs[x][0], \"\".join(text))", "path": "plugin\\floo\\common\\lib\\diff_match_patch.py", "repo_name": "Floobits/floobits-vim", "stars": 247, "license": "apache-2.0", "language": "python", "size": 1990}
{"docstring": "\"\"\"Determine if the suffix of one string is the prefix of another.\n\nArgs:\n    text1 First string.\n    text2 Second string.\n\nReturns:\n    The number of characters common to the end of the first\n    string and the start of the second string.\n\"\"\"\n# Cache the text lengths to prevent multiple calls.\n", "func_signal": "def diff_commonOverlap(self, text1, text2):\n", "code": "text1_length = len(text1)\ntext2_length = len(text2)\n# Eliminate the null case.\nif text1_length == 0 or text2_length == 0:\n    return 0\n# Truncate the longer string.\nif text1_length > text2_length:\n    text1 = text1[-text2_length:]\nelif text1_length < text2_length:\n    text2 = text2[:text1_length]\ntext_length = min(text1_length, text2_length)\n# Quick check for the worst case.\nif text1 == text2:\n    return text_length\n\n# Start by looking for a single character match\n# and increase length until no match is found.\n# Performance analysis: http://neil.fraser.name/news/2010/11/04/\nbest = 0\nlength = 1\nwhile True:\n    pattern = text1[-length:]\n    found = text2.find(pattern)\n    if found == -1:\n        return best\n    length += found\n    if found == 0 or text1[-length:] == text2[:length]:\n        best = length\n        length += 1", "path": "plugin\\floo\\common\\lib\\diff_match_patch.py", "repo_name": "Floobits/floobits-vim", "stars": 247, "license": "apache-2.0", "language": "python", "size": 1990}
{"docstring": "\"\"\"Find the 'middle snake' of a diff, split the problem in two\n    and return the recursively constructed diff.\n    See Myers 1986 paper: An O(ND) Difference Algorithm and Its Variations.\n\nArgs:\n    text1: Old string to be diffed.\n    text2: New string to be diffed.\n    deadline: Time at which to bail if not yet complete.\n\nReturns:\n    Array of diff tuples.\n\"\"\"\n\n# Cache the text lengths to prevent multiple calls.\n", "func_signal": "def diff_bisect(self, text1, text2, deadline):\n", "code": "text1_length = len(text1)\ntext2_length = len(text2)\nmax_d = (text1_length + text2_length + 1) // 2\nv_offset = max_d\nv_length = 2 * max_d\nv1 = [-1] * v_length\nv1[v_offset + 1] = 0\nv2 = v1[:]\ndelta = text1_length - text2_length\n# If the total number of characters is odd, then the front path will\n# collide with the reverse path.\nfront = (delta % 2 != 0)\n# Offsets for start and end of k loop.\n# Prevents mapping of space beyond the grid.\nk1start = 0\nk1end = 0\nk2start = 0\nk2end = 0\nfor d in range(max_d):\n    # Bail out if deadline is reached.\n    if time.time() > deadline:\n        break\n\n    # Walk the front path one step.\n    for k1 in range(-d + k1start, d + 1 - k1end, 2):\n        k1_offset = v_offset + k1\n        if k1 == -d or (k1 != d and v1[k1_offset - 1] < v1[k1_offset + 1]):\n            x1 = v1[k1_offset + 1]\n        else:\n            x1 = v1[k1_offset - 1] + 1\n        y1 = x1 - k1\n        while (x1 < text1_length and y1 < text2_length and text1[x1] == text2[y1]):\n            x1 += 1\n            y1 += 1\n        v1[k1_offset] = x1\n        if x1 > text1_length:\n            # Ran off the right of the graph.\n            k1end += 2\n        elif y1 > text2_length:\n            # Ran off the bottom of the graph.\n            k1start += 2\n        elif front:\n            k2_offset = v_offset + delta - k1\n            if k2_offset >= 0 and k2_offset < v_length and v2[k2_offset] != -1:\n                # Mirror x2 onto top-left coordinate system.\n                x2 = text1_length - v2[k2_offset]\n                if x1 >= x2:\n                    # Overlap detected.\n                    return self.diff_bisectSplit(text1, text2, x1, y1, deadline)\n\n    # Walk the reverse path one step.\n    for k2 in range(-d + k2start, d + 1 - k2end, 2):\n        k2_offset = v_offset + k2\n        if k2 == -d or (k2 != d and v2[k2_offset - 1] < v2[k2_offset + 1]):\n            x2 = v2[k2_offset + 1]\n        else:\n            x2 = v2[k2_offset - 1] + 1\n        y2 = x2 - k2\n        while (x2 < text1_length and y2 < text2_length and text1[-x2 - 1] == text2[-y2 - 1]):\n            x2 += 1\n            y2 += 1\n        v2[k2_offset] = x2\n        if x2 > text1_length:\n            # Ran off the left of the graph.\n            k2end += 2\n        elif y2 > text2_length:\n            # Ran off the top of the graph.\n            k2start += 2\n        elif not front:\n            k1_offset = v_offset + delta - k2\n            if k1_offset >= 0 and k1_offset < v_length and v1[k1_offset] != -1:\n                x1 = v1[k1_offset]\n                y1 = v_offset + x1 - k1_offset\n                # Mirror x2 onto top-left coordinate system.\n                x2 = text1_length - x2\n                if x1 >= x2:\n                    # Overlap detected.\n                    return self.diff_bisectSplit(text1, text2, x1, y1, deadline)\n\n# Diff took too long and hit the deadline or\n# number of diffs equals number of characters, no commonality at all.\nreturn [(self.DIFF_DELETE, text1), (self.DIFF_INSERT, text2)]", "path": "plugin\\floo\\common\\lib\\diff_match_patch.py", "repo_name": "Floobits/floobits-vim", "stars": 247, "license": "apache-2.0", "language": "python", "size": 1990}
{"docstring": "\"\"\"Look for single edits surrounded on both sides by equalities\nwhich can be shifted sideways to align the edit to a word boundary.\ne.g: The c<ins>at c</ins>ame. -> The <ins>cat </ins>came.\n\nArgs:\n    diffs: Array of diff tuples.\n\"\"\"\n\n", "func_signal": "def diff_cleanupSemanticLossless(self, diffs):\n", "code": "def diff_cleanupSemanticScore(one, two):\n    \"\"\"Given two strings, compute a score representing whether the\n    internal boundary falls on logical boundaries.\n    Scores range from 6 (best) to 0 (worst).\n    Closure, but does not reference any external variables.\n\n    Args:\n        one: First string.\n        two: Second string.\n\n    Returns:\n        The score.\n    \"\"\"\n    if not one or not two:\n        # Edges are the best.\n        return 6\n\n    # Each port of this function behaves slightly differently due to\n    # subtle differences in each language's definition of things like\n    # 'whitespace'.  Since this function's purpose is largely cosmetic,\n    # the choice has been made to use each language's native features\n    # rather than force total conformity.\n    char1 = one[-1]\n    char2 = two[0]\n    nonAlphaNumeric1 = not char1.isalnum()\n    nonAlphaNumeric2 = not char2.isalnum()\n    whitespace1 = nonAlphaNumeric1 and char1.isspace()\n    whitespace2 = nonAlphaNumeric2 and char2.isspace()\n    lineBreak1 = whitespace1 and (char1 == \"\\r\" or char1 == \"\\n\")\n    lineBreak2 = whitespace2 and (char2 == \"\\r\" or char2 == \"\\n\")\n    blankLine1 = lineBreak1 and self.BLANKLINEEND.search(one)\n    blankLine2 = lineBreak2 and self.BLANKLINESTART.match(two)\n\n    if blankLine1 or blankLine2:\n        # Five points for blank lines.\n        return 5\n    elif lineBreak1 or lineBreak2:\n        # Four points for line breaks.\n        return 4\n    elif nonAlphaNumeric1 and not whitespace1 and whitespace2:\n        # Three points for end of sentences.\n        return 3\n    elif whitespace1 or whitespace2:\n        # Two points for whitespace.\n        return 2\n    elif nonAlphaNumeric1 or nonAlphaNumeric2:\n        # One point for non-alphanumeric.\n        return 1\n    return 0\n\npointer = 1\n# Intentionally ignore the first and last element (don't need checking).\nwhile pointer < len(diffs) - 1:\n    if (diffs[pointer - 1][0] == self.DIFF_EQUAL and\n            diffs[pointer + 1][0] == self.DIFF_EQUAL):\n        # This is a single edit surrounded by equalities.\n        equality1 = diffs[pointer - 1][1]\n        edit = diffs[pointer][1]\n        equality2 = diffs[pointer + 1][1]\n\n        # First, shift the edit as far left as possible.\n        commonOffset = self.diff_commonSuffix(equality1, edit)\n        if commonOffset:\n            commonString = edit[-commonOffset:]\n            equality1 = equality1[:-commonOffset]\n            edit = commonString + edit[:-commonOffset]\n            equality2 = commonString + equality2\n\n        # Second, step character by character right, looking for the best fit.\n        bestEquality1 = equality1\n        bestEdit = edit\n        bestEquality2 = equality2\n        bestScore = (diff_cleanupSemanticScore(equality1, edit) +\n                     diff_cleanupSemanticScore(edit, equality2))\n        while edit and equality2 and edit[0] == equality2[0]:\n            equality1 += edit[0]\n            edit = edit[1:] + equality2[0]\n            equality2 = equality2[1:]\n            score = (diff_cleanupSemanticScore(equality1, edit) +\n                     diff_cleanupSemanticScore(edit, equality2))\n            # The >= encourages trailing rather than leading whitespace on edits.\n            if score >= bestScore:\n                bestScore = score\n                bestEquality1 = equality1\n                bestEdit = edit\n                bestEquality2 = equality2\n\n        if diffs[pointer - 1][1] != bestEquality1:\n            # We have an improvement, save it back to the diff.\n            if bestEquality1:\n                diffs[pointer - 1] = (diffs[pointer - 1][0], bestEquality1)\n            else:\n                del diffs[pointer - 1]\n                pointer -= 1\n            diffs[pointer] = (diffs[pointer][0], bestEdit)\n            if bestEquality2:\n                diffs[pointer + 1] = (diffs[pointer + 1][0], bestEquality2)\n            else:\n                del diffs[pointer + 1]\n                pointer -= 1\n    pointer += 1", "path": "plugin\\floo\\common\\lib\\diff_match_patch.py", "repo_name": "Floobits/floobits-vim", "stars": 247, "license": "apache-2.0", "language": "python", "size": 1990}
{"docstring": "\"\"\"Emmulate GNU diff's format.\nHeader: @@ -382,8 +481,9 @@\nIndicies are printed as 1-based, not 0-based.\n\nReturns:\n    The GNU diff string.\n\"\"\"\n", "func_signal": "def __str__(self):\n", "code": "if self.length1 == 0:\n    coords1 = str(self.start1) + \",0\"\nelif self.length1 == 1:\n    coords1 = str(self.start1 + 1)\nelse:\n    coords1 = str(self.start1 + 1) + \",\" + str(self.length1)\nif self.length2 == 0:\n    coords2 = str(self.start2) + \",0\"\nelif self.length2 == 1:\n    coords2 = str(self.start2 + 1)\nelse:\n    coords2 = str(self.start2 + 1) + \",\" + str(self.length2)\ntext = [\"@@ -\", coords1, \" +\", coords2, \" @@\\n\"]\n# Escape the body of the patch with %xx notation.\nfor (op, data) in self.diffs:\n    if op == diff_match_patch.DIFF_INSERT:\n        text.append(\"+\")\n    elif op == diff_match_patch.DIFF_DELETE:\n        text.append(\"-\")\n    elif op == diff_match_patch.DIFF_EQUAL:\n        text.append(\" \")\n    # High ascii will raise UnicodeDecodeError.  Use Unicode instead.\n    data = data.encode(\"utf-8\")\n    text.append(parse.quote(data, \"!~*'();/?:@&=+$,# \") + \"\\n\")\nreturn \"\".join(text)", "path": "plugin\\floo\\common\\lib\\diff_match_patch.py", "repo_name": "Floobits/floobits-vim", "stars": 247, "license": "apache-2.0", "language": "python", "size": 1990}
{"docstring": "\"\"\"Compute the Levenshtein distance; the number of inserted, deleted or\nsubstituted characters.\n\nArgs:\n    diffs: Array of diff tuples.\n\nReturns:\n    Number of changes.\n\"\"\"\n", "func_signal": "def diff_levenshtein(self, diffs):\n", "code": "levenshtein = 0\ninsertions = 0\ndeletions = 0\nfor (op, data) in diffs:\n    if op == self.DIFF_INSERT:\n        insertions += len(data)\n    elif op == self.DIFF_DELETE:\n        deletions += len(data)\n    elif op == self.DIFF_EQUAL:\n        # A deletion and an insertion is one substitution.\n        levenshtein += max(insertions, deletions)\n        insertions = 0\n        deletions = 0\nlevenshtein += max(insertions, deletions)\nreturn levenshtein", "path": "plugin\\floo\\common\\lib\\diff_match_patch.py", "repo_name": "Floobits/floobits-vim", "stars": 247, "license": "apache-2.0", "language": "python", "size": 1990}
{"docstring": "\"\"\"Convert a diff array into a pretty HTML report.\n\nArgs:\n    diffs: Array of diff tuples.\n\nReturns:\n    HTML representation.\n\"\"\"\n", "func_signal": "def diff_prettyHtml(self, diffs):\n", "code": "html = []\nfor (op, data) in diffs:\n    text = (data.replace(\"&\", \"&amp;\").replace(\"<\", \"&lt;\")\n            .replace(\">\", \"&gt;\").replace(\"\\n\", \"&para;<br>\"))\n    if op == self.DIFF_INSERT:\n        html.append(\"<ins style=\\\"background:#e6ffe6;\\\">%s</ins>\" % text)\n    elif op == self.DIFF_DELETE:\n        html.append(\"<del style=\\\"background:#ffe6e6;\\\">%s</del>\" % text)\n    elif op == self.DIFF_EQUAL:\n        html.append(\"<span>%s</span>\" % text)\nreturn \"\".join(html)", "path": "plugin\\floo\\common\\lib\\diff_match_patch.py", "repo_name": "Floobits/floobits-vim", "stars": 247, "license": "apache-2.0", "language": "python", "size": 1990}
{"docstring": "\"\"\"Given the original text1, and an encoded string which describes the\noperations required to transform text1 into text2, compute the full diff.\n\nArgs:\n    text1: Source string for the diff.\n    delta: Delta text.\n\nReturns:\n    Array of diff tuples.\n\nRaises:\n    ValueError: If invalid input.\n\"\"\"\n", "func_signal": "def diff_fromDelta(self, text1, delta):\n", "code": "if type(delta) == str:\n    # Deltas should be composed of a subset of ascii chars, Unicode not\n    # required.  If this encode raises UnicodeEncodeError, delta is invalid.\n    delta.encode(\"ascii\")\ndiffs = []\npointer = 0  # Cursor in text1\ntokens = delta.split(\"\\t\")\nfor token in tokens:\n    if token == \"\":\n        # Blank tokens are ok (from a trailing \\t).\n        continue\n    # Each token begins with a one character parameter which specifies the\n    # operation of this token (delete, insert, equality).\n    param = token[1:]\n    if token[0] == \"+\":\n        param = unquote(param)\n        diffs.append((self.DIFF_INSERT, param))\n    elif token[0] == \"-\" or token[0] == \"=\":\n        try:\n            n = int(param)\n        except ValueError:\n            raise ValueError(\"Invalid number in diff_fromDelta: \" + param)\n        if n < 0:\n            raise ValueError(\"Negative number in diff_fromDelta: \" + param)\n        text = text1[pointer:(pointer + n)]\n        pointer += n\n        if token[0] == \"=\":\n            diffs.append((self.DIFF_EQUAL, text))\n        else:\n            diffs.append((self.DIFF_DELETE, text))\n    else:\n        # Anything else is an error.\n        raise ValueError(\"Invalid diff operation in diff_fromDelta: \" + token[0])\nif pointer != len(text1):\n    raise ValueError(\n        \"Delta length (%d) does not equal source text length (%d).\" %\n        (pointer, len(text1)))\nreturn diffs", "path": "plugin\\floo\\common\\lib\\diff_match_patch.py", "repo_name": "Floobits/floobits-vim", "stars": 247, "license": "apache-2.0", "language": "python", "size": 1990}
{"docstring": "\"\"\"Compute a list of patches to turn text1 into text2.\nUse diffs if provided, otherwise compute it ourselves.\nThere are four ways to call this function, depending on what data is\navailable to the caller:\nMethod 1:\na = text1, b = text2\nMethod 2:\na = diffs\nMethod 3 (optimal):\na = text1, b = diffs\nMethod 4 (deprecated, use method 3):\na = text1, b = text2, c = diffs\n\nArgs:\n    a: text1 (methods 1,3,4) or Array of diff tuples for text1 to\n            text2 (method 2).\n    b: text2 (methods 1,4) or Array of diff tuples for text1 to\n            text2 (method 3) or undefined (method 2).\n    c: Array of diff tuples for text1 to text2 (method 4) or\n            undefined (methods 1,2,3).\n\nReturns:\n    Array of Patch objects.\n\"\"\"\n", "func_signal": "def patch_make(self, a, b=None, c=None):\n", "code": "text1 = None\ndiffs = None\n# Note that texts may arrive as 'str' or 'unicode'.\nif isinstance(a, str_instances) and isinstance(b, str_instances) and c is None:\n    # Method 1: text1, text2\n    # Compute diffs from text1 and text2.\n    text1 = a\n    diffs = self.diff_main(text1, b, True)\n    if len(diffs) > 2:\n        self.diff_cleanupSemantic(diffs)\n        self.diff_cleanupEfficiency(diffs)\nelif isinstance(a, list) and b is None and c is None:\n    # Method 2: diffs\n    # Compute text1 from diffs.\n    diffs = a\n    text1 = self.diff_text1(diffs)\nelif isinstance(a, str_instances) and isinstance(b, list) and c is None:\n    # Method 3: text1, diffs\n    text1 = a\n    diffs = b\nelif (isinstance(a, str_instances) and isinstance(b, str_instances) and isinstance(c, list)):\n    # Method 4: text1, text2, diffs\n    # text2 is not used.\n    text1 = a\n    diffs = c\nelse:\n    raise ValueError(\"Unknown call format to patch_make.\")\n\nif not diffs:\n    return []  # Get rid of the None case.\npatches = []\npatch = patch_obj()\nchar_count1 = 0  # Number of characters into the text1 string.\nchar_count2 = 0  # Number of characters into the text2 string.\nprepatch_text = text1  # Recreate the patches to determine context info.\npostpatch_text = text1\nfor x in range(len(diffs)):\n    (diff_type, diff_text) = diffs[x]\n    if len(patch.diffs) == 0 and diff_type != self.DIFF_EQUAL:\n        # A new patch starts here.\n        patch.start1 = char_count1\n        patch.start2 = char_count2\n    if diff_type == self.DIFF_INSERT:\n        # Insertion\n        patch.diffs.append(diffs[x])\n        patch.length2 += len(diff_text)\n        postpatch_text = (postpatch_text[:char_count2] + diff_text +\n                          postpatch_text[char_count2:])\n    elif diff_type == self.DIFF_DELETE:\n        # Deletion.\n        patch.length1 += len(diff_text)\n        patch.diffs.append(diffs[x])\n        postpatch_text = (postpatch_text[:char_count2] +\n                          postpatch_text[char_count2 + len(diff_text):])\n    elif (diff_type == self.DIFF_EQUAL and\n          len(diff_text) <= 2 * self.Patch_Margin and\n          len(patch.diffs) != 0 and len(diffs) != x + 1):\n        # Small equality inside a patch.\n        patch.diffs.append(diffs[x])\n        patch.length1 += len(diff_text)\n        patch.length2 += len(diff_text)\n\n    if (diff_type == self.DIFF_EQUAL and\n            len(diff_text) >= 2 * self.Patch_Margin):\n        # Time for a new patch.\n        if len(patch.diffs) != 0:\n            self.patch_addContext(patch, prepatch_text)\n            patches.append(patch)\n            patch = patch_obj()\n            # Unlike Unidiff, our patch lists have a rolling context.\n            # http://code.google.com/p/google-diff-match-patch/wiki/Unidiff\n            # Update prepatch text & pos to reflect the application of the\n            # just completed patch.\n            prepatch_text = postpatch_text\n            char_count1 = char_count2\n\n    # Update the current character count.\n    if diff_type != self.DIFF_INSERT:\n        char_count1 += len(diff_text)\n    if diff_type != self.DIFF_DELETE:\n        char_count2 += len(diff_text)\n\n# Pick up the leftover patch if not empty.\nif len(patch.diffs) != 0:\n    self.patch_addContext(patch, prepatch_text)\n    patches.append(patch)\nreturn patches", "path": "plugin\\floo\\common\\lib\\diff_match_patch.py", "repo_name": "Floobits/floobits-vim", "stars": 247, "license": "apache-2.0", "language": "python", "size": 1990}
{"docstring": "\"\"\"Look through the patches and break up any which are longer than the\nmaximum limit of the match algorithm.\nIntended to be called only from within patch_apply.\n\nArgs:\n    patches: Array of Patch objects.\n\"\"\"\n", "func_signal": "def patch_splitMax(self, patches):\n", "code": "patch_size = self.Match_MaxBits\nif patch_size == 0:\n    # Python has the option of not splitting strings due to its ability\n    # to handle integers of arbitrary precision.\n    return\nfor x in range(len(patches)):\n    if patches[x].length1 <= patch_size:\n        continue\n    bigpatch = patches[x]\n    # Remove the big old patch.\n    del patches[x]\n    x -= 1\n    start1 = bigpatch.start1\n    start2 = bigpatch.start2\n    precontext = ''\n    while len(bigpatch.diffs) != 0:\n        # Create one of several smaller patches.\n        patch = patch_obj()\n        empty = True\n        patch.start1 = start1 - len(precontext)\n        patch.start2 = start2 - len(precontext)\n        if precontext:\n            patch.length1 = patch.length2 = len(precontext)\n            patch.diffs.append((self.DIFF_EQUAL, precontext))\n\n        while (len(bigpatch.diffs) != 0 and\n               patch.length1 < patch_size - self.Patch_Margin):\n            (diff_type, diff_text) = bigpatch.diffs[0]\n            if diff_type == self.DIFF_INSERT:\n                # Insertions are harmless.\n                patch.length2 += len(diff_text)\n                start2 += len(diff_text)\n                patch.diffs.append(bigpatch.diffs.pop(0))\n                empty = False\n            elif (diff_type == self.DIFF_DELETE and len(patch.diffs) == 1 and\n                    patch.diffs[0][0] == self.DIFF_EQUAL and\n                    len(diff_text) > 2 * patch_size):\n                # This is a large deletion.  Let it pass in one chunk.\n                patch.length1 += len(diff_text)\n                start1 += len(diff_text)\n                empty = False\n                patch.diffs.append((diff_type, diff_text))\n                del bigpatch.diffs[0]\n            else:\n                # Deletion or equality.  Only take as much as we can stomach.\n                diff_text = diff_text[:patch_size - patch.length1 - self.Patch_Margin]\n                patch.length1 += len(diff_text)\n                start1 += len(diff_text)\n                if diff_type == self.DIFF_EQUAL:\n                    patch.length2 += len(diff_text)\n                    start2 += len(diff_text)\n                else:\n                    empty = False\n\n                patch.diffs.append((diff_type, diff_text))\n                if diff_text == bigpatch.diffs[0][1]:\n                    del bigpatch.diffs[0]\n                else:\n                    bigpatch.diffs[0] = (bigpatch.diffs[0][0], bigpatch.diffs[0][1][len(diff_text):])\n\n        # Compute the head context for the next patch.\n        precontext = self.diff_text2(patch.diffs)\n        precontext = precontext[-self.Patch_Margin:]\n        # Append the end context for this patch.\n        postcontext = self.diff_text1(bigpatch.diffs)[:self.Patch_Margin]\n        if postcontext:\n            patch.length1 += len(postcontext)\n            patch.length2 += len(postcontext)\n            if len(patch.diffs) != 0 and patch.diffs[-1][0] == self.DIFF_EQUAL:\n                patch.diffs[-1] = (self.DIFF_EQUAL, patch.diffs[-1][1] + postcontext)\n            else:\n                patch.diffs.append((self.DIFF_EQUAL, postcontext))\n\n        if not empty:\n            x += 1\n            patches.insert(x, patch)", "path": "plugin\\floo\\common\\lib\\diff_match_patch.py", "repo_name": "Floobits/floobits-vim", "stars": 247, "license": "apache-2.0", "language": "python", "size": 1990}
{"docstring": "\"\"\"Add some padding on text start and end so that edges can match\nsomething.  Intended to be called only from within patch_apply.\n\nArgs:\n    patches: Array of Patch objects.\n\nReturns:\n    The padding string added to each side.\n\"\"\"\n", "func_signal": "def patch_addPadding(self, patches):\n", "code": "paddingLength = self.Patch_Margin\nnullPadding = \"\"\nfor x in range(1, paddingLength + 1):\n    nullPadding += unichr(x)\n\n# Bump all the patches forward.\nfor patch in patches:\n    patch.start1 += paddingLength\n    patch.start2 += paddingLength\n\n# Add some padding on start of first diff.\npatch = patches[0]\ndiffs = patch.diffs\nif not diffs or diffs[0][0] != self.DIFF_EQUAL:\n    # Add nullPadding equality.\n    diffs.insert(0, (self.DIFF_EQUAL, nullPadding))\n    patch.start1 -= paddingLength  # Should be 0.\n    patch.start2 -= paddingLength  # Should be 0.\n    patch.length1 += paddingLength\n    patch.length2 += paddingLength\nelif paddingLength > len(diffs[0][1]):\n    # Grow first equality.\n    extraLength = paddingLength - len(diffs[0][1])\n    newText = nullPadding[len(diffs[0][1]):] + diffs[0][1]\n    diffs[0] = (diffs[0][0], newText)\n    patch.start1 -= extraLength\n    patch.start2 -= extraLength\n    patch.length1 += extraLength\n    patch.length2 += extraLength\n\n# Add some padding on end of last diff.\npatch = patches[-1]\ndiffs = patch.diffs\nif not diffs or diffs[-1][0] != self.DIFF_EQUAL:\n    # Add nullPadding equality.\n    diffs.append((self.DIFF_EQUAL, nullPadding))\n    patch.length1 += paddingLength\n    patch.length2 += paddingLength\nelif paddingLength > len(diffs[-1][1]):\n    # Grow last equality.\n    extraLength = paddingLength - len(diffs[-1][1])\n    newText = diffs[-1][1] + nullPadding[:extraLength]\n    diffs[-1] = (diffs[-1][0], newText)\n    patch.length1 += extraLength\n    patch.length2 += extraLength\n\nreturn nullPadding", "path": "plugin\\floo\\common\\lib\\diff_match_patch.py", "repo_name": "Floobits/floobits-vim", "stars": 247, "license": "apache-2.0", "language": "python", "size": 1990}
{"docstring": "\"\"\"Inits a diff_match_patch object with default settings.\nRedefine these in your program to override the defaults.\n\"\"\"\n\n# Number of seconds to map a diff before giving up (0 for infinity).\n", "func_signal": "def __init__(self):\n", "code": "self.Diff_Timeout = 1.0\n# Cost of an empty edit operation in terms of edit characters.\nself.Diff_EditCost = 4\n# At what point is no match declared (0.0 = perfection, 1.0 = very loose).\n# self.Match_Threshold = 0.5\nself.Match_Threshold = 0.375\n# How far to search for a match (0 = exact location, 1000+ = broad match).\n# A match this many characters away from the expected location will add\n# 1.0 to the score (0.0 is a perfect match).\n# self.Match_Distance = 1000\nself.Match_Distance = 100\n# When deleting a large block of text (over ~64 characters), how close do\n# the contents have to be to match the expected contents. (0.0 = perfection,\n# 1.0 = very loose).  Note that Match_Threshold controls how closely the\n# end points of a delete need to match.\n# self.Patch_DeleteThreshold = 0.5\nself.Patch_DeleteThreshold = 0.375\n# Chunk size for context length.\nself.Patch_Margin = 4\n\n# The number of bits in an int.\n# Python has no maximum, thus to disable patch splitting set to 0.\n# However to avoid long patches in certain pathological cases, use 32.\n# Multiple short patches (using native ints) are much faster than long ones.\nself.Match_MaxBits = 32", "path": "plugin\\floo\\common\\lib\\diff_match_patch.py", "repo_name": "Floobits/floobits-vim", "stars": 247, "license": "apache-2.0", "language": "python", "size": 1990}
{"docstring": "\"\"\"Creates a test that runs a js file with the stdlib.\"\"\"\n", "func_signal": "def run_with_stdlib(file_path, file_name=None):\n", "code": "file_name = file_name if file_name else file_path\n\nclass TestStdLib(unittest.TestCase):\n    \"\"\"Tests js code with the stdlib\"\"\"\n    templ = {\n        \"js_path\": file_path,\n        \"js_unix_path\": get_posix_path(file_path),\n        \"js_out_path\": file_path + \".out\",\n        \"js_error\": file_path + \".err\",\n        \"name\": file_name,\n    }\n    def reportProgres(self):\n        \"\"\"Should be overloaded by the test result class.\"\"\"\n\n    def runTest(self):\n        \"\"\"The actual test goes here.\"\"\"\n        cmd = (\n              'js -f \"py-builtins.js\" '\n              '-f \"%(js_path)s\" > \"%(js_out_path)s\" 2> \"%(js_error)s\"'\n              )% self.templ\n        self.assertEqual(0, subprocess.call([cmd], shell = True))\n        self.reportProgres()\n    def __str__(self):\n        return \"%(js_unix_path)s [1]: \" % self.templ\n\nreturn TestStdLib", "path": "testtools\\util.py", "repo_name": "chrivers/pyjaco", "stars": 141, "license": "mit", "language": "python", "size": 1712}
{"docstring": "\"\"\"\nTake a boundary edge (a,b), and in the list of points\nfind a point 'c' that lies on the left of ab and maximizes\nthe angle acb\n\"\"\"\n", "func_signal": "def find_third_point(a, b, pts_list, edges):\n", "code": "found = 0\nminimum = 10**8   #this is dirty\nc_index = -1\npt_index = -1\nfor c_point in pts_list:\n    c_index += 1\n    if c_index != a and c_index != b and is_on_the_left(c_index, a, b, pts_list):\n        edge_intersects = \\\n                edge_intersects_edges((a, c_index), pts_list, edges) or \\\n                edge_intersects_edges((b, c_index), pts_list, edges)\n        if not edge_intersects:\n            crit = criterion(a, b, c_index, pts_list)\n            if crit < minimum:\n                minimum = crit\n                pt_index = c_index\n                found = 1\nif found == 0:\n    raise TriangulationError(\"ERROR: Optimal point not found in find_third_point().\")\nreturn pt_index", "path": "tests\\algorithms\\triangulation.py", "repo_name": "chrivers/pyjaco", "stars": 141, "license": "mit", "language": "python", "size": 1712}
{"docstring": "\"\"\"Creates a test that tests if a file can be compiled by python\"\"\"\n", "func_signal": "def compile_file_test(file_path, file_name=None):\n", "code": "file_name = file_name if file_name else file_path\n\nclass CompileFile(unittest.TestCase):\n    \"\"\"Test if a file can be compiled by python.\"\"\"\n\n    templ = {\n        \"py_executable\": sys.executable,\n        \"py_path\": file_path,\n        \"py_unix_path\": get_posix_path(file_path),\n        \"py_out_path\": file_path + \".out\",\n        \"py_error\": file_path + \".err\",\n        \"name\": file_name,\n    }\n    def reportProgres(self):\n        \"\"\"Should be overloaded by the test result class\"\"\"\n\n    def runTest(self):\n        \"\"\"The actual test goes here.\"\"\"\n        commands = (\n            (\n            '%(py_executable)s \"%(py_path)s\" > '\n            '\"%(py_out_path)s\" 2> \"%(py_error)s\"'\n            ) % self.templ,\n          )\n        for cmd in commands:\n            self.assertEqual(0, subprocess.call([cmd], shell = True))\n            self.reportProgres()\n    def __str__(self):\n        return \"%(py_unix_path)s [1]: \" % self.templ\nreturn CompileFile", "path": "testtools\\util.py", "repo_name": "chrivers/pyjaco", "stars": 141, "license": "mit", "language": "python", "size": 1712}
{"docstring": "\"\"\"\nCreate a triangulation using the advancing front method.\n\"\"\"\n# create empty list of elements\n", "func_signal": "def triangulate_af(pts_list, bdy_edges):\n", "code": "elems = []\nbdy_edges = bdy_edges[:]\n# main loop\nwhile bdy_edges != []:\n    # take the last item from the list of bdy edges (and remove it)\n    a,b = bdy_edges.pop()\n    c = find_third_point(a, b, pts_list, bdy_edges)\n    elems.append((a,b,c))\n    if is_boundary_edge(c, a, bdy_edges):\n        bdy_edges.remove((c,a))\n    else:\n        bdy_edges.append((a,c))\n    if is_boundary_edge(b, c, bdy_edges):\n        bdy_edges.remove((b,c))\n    else:\n        bdy_edges.append((c,b))\nreturn elems", "path": "tests\\algorithms\\triangulation.py", "repo_name": "chrivers/pyjaco", "stars": 141, "license": "mit", "language": "python", "size": 1712}
{"docstring": "\"\"\"\nChecks whether the two edges intersect.\n\nIt assumes that e1 and e2 are tuples of (a_id, b_id) of ids into the nodes.\n\"\"\"\n", "func_signal": "def two_edges_intersect(nodes, e1, e2):\n", "code": "A = nodes[e1[0]]\nB = nodes[e1[1]]\nC = nodes[e2[0]]\nD = nodes[e2[1]]\nreturn intersect(A, B, C, D)", "path": "examples\\triangulation.py", "repo_name": "chrivers/pyjaco", "stars": 141, "license": "mit", "language": "python", "size": 1712}
{"docstring": "\"\"\"\nChecks whether edge (a, b) is in the list of boundary edges\n\"\"\"\n", "func_signal": "def is_boundary_edge(a, b, bdy_edges):\n", "code": "for edge in bdy_edges:\n    a0, b0 = edge\n    if a == a0 and b == b0:\n        return True\nreturn False", "path": "tests\\test_compile_js.py", "repo_name": "chrivers/pyjaco", "stars": 141, "license": "mit", "language": "python", "size": 1712}
{"docstring": "\"\"\"\nCreate a triangulation using the advancing front method.\n\"\"\"\n# create empty list of elements\n", "func_signal": "def triangulate_af(pts_list, bdy_edges):\n", "code": "elems = []\nbdy_edges = bdy_edges[:]\n# main loop\nwhile bdy_edges != []:\n    # take the last item from the list of bdy edges (and remove it)\n    a,b = bdy_edges.pop()\n    c = find_third_point(a, b, pts_list, bdy_edges)\n    elems.append((a,b,c))\n    if is_boundary_edge(c, a, bdy_edges):\n        bdy_edges.remove((c,a))\n    else:\n        bdy_edges.append((a,c))\n    if is_boundary_edge(b, c, bdy_edges):\n        bdy_edges.remove((b,c))\n    else:\n        bdy_edges.append((c,b))\nreturn elems", "path": "tests\\test_compile_js.py", "repo_name": "chrivers/pyjaco", "stars": 141, "license": "mit", "language": "python", "size": 1712}
{"docstring": "\"\"\"\nTake a boundary edge (a,b), and in the list of points\nfind a point 'c' that lies on the left of ab and maximizes\nthe angle acb\n\"\"\"\n", "func_signal": "def find_third_point(a, b, pts_list, edges):\n", "code": "found = 0\nminimum = exp(100)   #this is dirty\nc_index = -1\npt_index = -1\nfor c_point in pts_list:\n    c_index += 1\n    if c_index != a and c_index != b and is_on_the_left(c_index, a, b, pts_list):\n        edge_intersects = \\\n                edge_intersects_edges((a, c_index), pts_list, edges) or \\\n                edge_intersects_edges((b, c_index), pts_list, edges)\n        if not edge_intersects:\n            crit = criterion(a, b, c_index, pts_list)\n            if crit < minimum:\n                minimum = crit\n                pt_index = c_index\n                found = 1\nif found == 0:\n    raise TriangulationError(\"ERROR: Optimal point not found in find_third_point().\")\nreturn pt_index", "path": "tests\\test_compile_js.py", "repo_name": "chrivers/pyjaco", "stars": 141, "license": "mit", "language": "python", "size": 1712}
{"docstring": "\"\"\"Should be overloaded by the test result class\"\"\"\n\n", "func_signal": "def stop(self):\n", "code": "\n\"\"\"The actual test goes here.\"\"\"\nif os.system(\n    \"echo | js > %s\" %\n    os.path.join(\n        tempfile.gettempdir(),\n        tempfile.gettempprefix()\n        )\n    ):\n    self.stop()\n    raise RuntimeError(\"\"\"Can't find the \"js\" command.\"\"\")\nself.reportProgres()\nif not os.path.exists(\"py-builtins.js\"):\n    self.stop()\n    raise RuntimeError(\"\"\"Can't find the \"py-builtins.js\" command.\"\"\")\nself.reportProgres()", "path": "testtools\\env_tests.py", "repo_name": "chrivers/pyjaco", "stars": 141, "license": "mit", "language": "python", "size": 1712}
{"docstring": "\"\"\"Turn a test to a failing test\"\"\"\n", "func_signal": "def compile_and_run_file_failing_test(*a, **k):\n", "code": "_class = compile_and_run_file_test(*a, **k)\n\nclass FailingTest(_class):\n    \"\"\"Failing test\"\"\"\n    @unittest.expectedFailure\n    def runTest(self):\n        return super(FailingTest, self).runTest()\n\nreturn FailingTest", "path": "testtools\\util.py", "repo_name": "chrivers/pyjaco", "stars": 141, "license": "mit", "language": "python", "size": 1712}
{"docstring": "\"\"\"translates path to a posix path\"\"\"\n", "func_signal": "def get_posix_path(path):\n", "code": "heads = []\ntail = path\nwhile tail != '':\n    tail, head = os.path.split(tail)\n    heads.append(head)\nreturn posixpath.join(*heads[::-1])", "path": "testtools\\util.py", "repo_name": "chrivers/pyjaco", "stars": 141, "license": "mit", "language": "python", "size": 1712}
{"docstring": "\"\"\"\nReturns True if 'e1' intersects any edge from 'edges'.\n\"\"\"\n", "func_signal": "def edge_intersects_edges(e1, nodes, edges):\n", "code": "for i in range(len(edges)):\n    e2 = edges[i]\n    if e1[1] == e2[0] or e1[0] == e2[1]:\n        continue\n    if two_edges_intersect(nodes, e1, e2):\n        return True\nreturn False", "path": "examples\\triangulation.py", "repo_name": "chrivers/pyjaco", "stars": 141, "license": "mit", "language": "python", "size": 1712}
{"docstring": "\"\"\"\nChecks whether the two edges intersect.\n\nIt assumes that e1 and e2 are tuples of (a_id, b_id) of ids into the nodes.\n\"\"\"\n", "func_signal": "def two_edges_intersect(nodes, e1, e2):\n", "code": "A = nodes[e1[0]]\nB = nodes[e1[1]]\nC = nodes[e2[0]]\nD = nodes[e2[1]]\nreturn intersect(A, B, C, D)", "path": "tests\\algorithms\\triangulation.py", "repo_name": "chrivers/pyjaco", "stars": 141, "license": "mit", "language": "python", "size": 1712}
{"docstring": "\"\"\"\nReturns True if any two edges intersect.\n\"\"\"\n", "func_signal": "def any_edges_intersect(nodes, edges):\n", "code": "for i in range(len(edges)):\n    for j in range(i+1, len(edges)):\n        e1 = edges[i]\n        e2 = edges[j]\n        if e1[1] == e2[0] or e1[0] == e2[1]:\n            continue\n        if two_edges_intersect(nodes, e1, e2):\n            return True\nreturn False", "path": "tests\\algorithms\\triangulation.py", "repo_name": "chrivers/pyjaco", "stars": 141, "license": "mit", "language": "python", "size": 1712}
{"docstring": "\"\"\"\nChecks whether edge (a, b) is in the list of boundary edges\n\"\"\"\n", "func_signal": "def is_boundary_edge(a, b, bdy_edges):\n", "code": "for edge in bdy_edges:\n    a0, b0 = edge\n    if a == a0 and b == b0:\n        return True\nreturn False", "path": "examples\\triangulation.py", "repo_name": "chrivers/pyjaco", "stars": 141, "license": "mit", "language": "python", "size": 1712}
{"docstring": "\"\"\"\nChecks whether edge (a, b) is in the list of boundary edges\n\"\"\"\n", "func_signal": "def is_boundary_edge(a, b, bdy_edges):\n", "code": "for edge in bdy_edges:\n    a0, b0 = edge\n    if a == a0 and b == b0:\n        return True\nreturn False", "path": "tests\\algorithms\\triangulation.py", "repo_name": "chrivers/pyjaco", "stars": 141, "license": "mit", "language": "python", "size": 1712}
{"docstring": "\"\"\"\nReturns True if \"e1\" intersects any edge from \"edges\".\n\"\"\"\n", "func_signal": "def edge_intersects_edges(e1, nodes, edges):\n", "code": "for i in range(len(edges)):\n    e2 = edges[i]\n    if e1[1] == e2[0] or e1[0] == e2[1]:\n        continue\n    if two_edges_intersect(nodes, e1, e2):\n        return True\nreturn False", "path": "tests\\algorithms\\triangulation.py", "repo_name": "chrivers/pyjaco", "stars": 141, "license": "mit", "language": "python", "size": 1712}
{"docstring": "\"\"\"Compile a python expression into javascript\"\"\"\n", "func_signal": "def compile_string(script, jsvars = None):\n", "code": "comp = Compiler(jsvars)\ncomp.append_string(script)\nreturn str(comp)", "path": "pyjaco\\__init__.py", "repo_name": "chrivers/pyjaco", "stars": 141, "license": "mit", "language": "python", "size": 1712}
{"docstring": "\"\"\"Creates a test that compiles and runs the python file as js\"\"\"\n", "func_signal": "def compile_and_run_file_test(file_path, file_name=None):\n", "code": "file_name = file_name if file_name else file_path\n\nclass CompileAndRunFile(unittest.TestCase):\n    \"\"\"Tests that a file can be compiled and run as js\"\"\"\n    templ = {\n    \"py_executable\": sys.executable,\n    \"py_path\": file_path,\n    \"py_unix_path\": get_posix_path(file_path),\n    \"py_out_path\": file_path + \".out\",\n    \"js_path\": file_path + \".js\",\n    \"js_out_path\": file_path + \".js.out\",\n    \"py_error\": file_path + \".err\",\n    \"js_error\": file_path + \".js.err\",\n    \"compiler_error\": file_path + \".comp.err\",\n    \"name\": file_name,\n    }\n    def reportProgres(self):\n        \"\"\"Should be overloaded by the test result class\"\"\"\n\n    def runTest(self):\n        \"\"\"The actual test goes here.\"\"\"\n        mtime_src = os.path.getmtime(self.templ['py_path'])\n        try:\n            mtime_py_res = os.path.getmtime(self.templ['py_out_path'])\n        except OSError:\n            mtime_py_res = 0\n        python_command = (\n            '%(py_executable)s \"%(py_path)s\" > \"%(py_out_path)s\" 2> '\n            '\"%(py_error)s\"'\n            ) % self.templ\n\n        try:\n            mtime_js_res = os.path.getmtime(self.templ['js_path'])\n        except OSError:\n            mtime_js_res = 0\n        compile_command = (\n            '%(py_executable)s pyjs.py -I -q '\n            '\"%(py_path)s\" > \"%(js_path)s\" 2> '\n            '\"%(compiler_error)s\"'\n            ) % self.templ\n\n        javascript_command = (\n            'js -f \"%(js_path)s\" > \"%(js_out_path)s\" 2> '\n            '\"%(js_error)s\"'\n            ) % self.templ\n\n        commands = []\n        if mtime_py_res < mtime_src:\n            commands.append(python_command)\n        if mtime_js_res < mtime_src:\n            commands.append(compile_command)\n        commands.append(javascript_command)\n\n        for cmd in commands:\n            self.assertEqual(0, subprocess.call([cmd], shell = True))\n            self.reportProgres()\n        self.assertEqual(\n            file(self.templ[\"py_out_path\"]).read(),\n            file(self.templ[\"js_out_path\"]).read()\n            )\n        self.reportProgres()\n\n    def __str__(self):\n        return \"%(py_unix_path)s: \" % self.templ\n\nreturn CompileAndRunFile", "path": "testtools\\util.py", "repo_name": "chrivers/pyjaco", "stars": 141, "license": "mit", "language": "python", "size": 1712}
{"docstring": "\"\"\"\nPrints a text on the screen.\n\nIt uses file.write(), so no readline library is necessary.\n\ncolor ... choose from the colors below, \"\" means default color\nalign ... left/right, left is a normal print, right is aligned on the\n          right hand side of the screen, filled with \" \" if necessary\nwidth ... the screen width\n\"\"\"\n\n", "func_signal": "def write(self, text, color=\"\", align=\"left\", width=80):\n", "code": "\n\"Ignores color but uses alignment and width\"\n_color = color\nif align == \"right\":\n    if self._write_pos + len(text) > width:\n        # we don't fit on the current line, create a new line\n        self.write(\"\\n\")\n    self.write(\" \" * (width - self._write_pos - len(text)))\n\nif self._line_wrap:\n    if text != \"\" and text[0] != \"\\n\":\n        self._file.write(\"\\n\")\n\nself._file.write(text)\nself._file.flush()\n\nnext_new_line = text.rfind(\"\\n\")\n\nif next_new_line == -1:\n    self._write_pos += len(text)\nelse:\n    self._write_pos = len(text) - next_new_line - 1\nself._line_wrap = self._write_pos >= width\nself._write_pos %= width", "path": "testtools\\writer.py", "repo_name": "chrivers/pyjaco", "stars": 141, "license": "mit", "language": "python", "size": 1712}
{"docstring": "\"\"\"Extract the cookies from the response into a CookieJar.\n\n:param jar: cookielib.CookieJar (not necessarily a RequestsCookieJar)\n:param request: our own requests.Request object\n:param response: urllib3.HTTPResponse object\n\"\"\"\n", "func_signal": "def extract_cookies_to_jar(jar, request, response):\n", "code": "if not (hasattr(response, '_original_response') and\n        response._original_response):\n    return\n# the _original_response field is the wrapped httplib.HTTPResponse object,\nreq = MockRequest(request)\n# pull out the HTTPMessage with the headers and put it in the mock:\nres = MockResponse(response._original_response.msg)\njar.extract_cookies(res, req)", "path": "libs\\requests\\cookies.py", "repo_name": "ring04h/dirfuzz", "stars": 144, "license": "None", "language": "python", "size": 17175}
{"docstring": "\"\"\"Dict-like iterkeys() that returns an iterator of names of cookies from the jar.\nSee itervalues() and iteritems().\"\"\"\n", "func_signal": "def iterkeys(self):\n", "code": "for cookie in iter(self):\n    yield cookie.name", "path": "libs\\requests\\cookies.py", "repo_name": "ring04h/dirfuzz", "stars": 144, "license": "None", "language": "python", "size": 17175}
{"docstring": "\"\"\"Sends a GET request. Returns :class:`Response` object.\n\n:param url: URL for the new :class:`Request` object.\n:param \\*\\*kwargs: Optional arguments that ``request`` takes.\n\"\"\"\n\n", "func_signal": "def get(url, **kwargs):\n", "code": "kwargs.setdefault('allow_redirects', True)\nreturn request('get', url, **kwargs)", "path": "libs\\requests\\api.py", "repo_name": "ring04h/dirfuzz", "stars": 144, "license": "None", "language": "python", "size": 17175}
{"docstring": "\"\"\"Takes as an argument an optional domain and path and returns a plain old\nPython dict of name-value pairs of cookies that meet the requirements.\"\"\"\n", "func_signal": "def get_dict(self, domain=None, path=None):\n", "code": "dictionary = {}\nfor cookie in iter(self):\n    if (domain is None or cookie.domain == domain) and (path is None\n                                        or cookie.path == path):\n        dictionary[cookie.name] = cookie.value\nreturn dictionary", "path": "libs\\requests\\cookies.py", "repo_name": "ring04h/dirfuzz", "stars": 144, "license": "None", "language": "python", "size": 17175}
{"docstring": "\"\"\"Utility method to list all the domains in the jar.\"\"\"\n", "func_signal": "def list_domains(self):\n", "code": "domains = []\nfor cookie in iter(self):\n    if cookie.domain not in domains:\n        domains.append(cookie.domain)\nreturn domains", "path": "libs\\requests\\cookies.py", "repo_name": "ring04h/dirfuzz", "stars": 144, "license": "None", "language": "python", "size": 17175}
{"docstring": "\"\"\"Sends a HEAD request. Returns :class:`Response` object.\n\n:param url: URL for the new :class:`Request` object.\n:param \\*\\*kwargs: Optional arguments that ``request`` takes.\n\"\"\"\n\n", "func_signal": "def head(url, **kwargs):\n", "code": "kwargs.setdefault('allow_redirects', False)\nreturn request('head', url, **kwargs)", "path": "libs\\requests\\api.py", "repo_name": "ring04h/dirfuzz", "stars": 144, "license": "None", "language": "python", "size": 17175}
{"docstring": "\"\"\"Unlike a normal CookieJar, this class is pickleable.\"\"\"\n", "func_signal": "def __getstate__(self):\n", "code": "state = self.__dict__.copy()\n# remove the unpickleable RLock object\nstate.pop('_cookies_lock')\nreturn state", "path": "libs\\requests\\cookies.py", "repo_name": "ring04h/dirfuzz", "stars": 144, "license": "None", "language": "python", "size": 17175}
{"docstring": "\"\"\"Requests uses this method internally to get cookie values. Takes as args name\nand optional domain and path. Returns a cookie.value. If there are conflicting cookies,\n_find arbitrarily chooses one. See _find_no_duplicates if you want an exception thrown\nif there are conflicting cookies.\"\"\"\n", "func_signal": "def _find(self, name, domain=None, path=None):\n", "code": "for cookie in iter(self):\n    if cookie.name == name:\n        if domain is None or cookie.domain == domain:\n            if path is None or cookie.path == path:\n                return cookie.value\n\nraise KeyError('name=%r, domain=%r, path=%r' % (name, domain, path))", "path": "libs\\requests\\cookies.py", "repo_name": "ring04h/dirfuzz", "stars": 144, "license": "None", "language": "python", "size": 17175}
{"docstring": "\"\"\"Return a copy of this RequestsCookieJar.\"\"\"\n", "func_signal": "def copy(self):\n", "code": "new_cj = RequestsCookieJar()\nnew_cj.update(self)\nreturn new_cj", "path": "libs\\requests\\cookies.py", "repo_name": "ring04h/dirfuzz", "stars": 144, "license": "None", "language": "python", "size": 17175}
{"docstring": "\"\"\"Dict-like itervalues() that returns an iterator of values of cookies from the jar.\nSee iterkeys() and iteritems().\"\"\"\n", "func_signal": "def itervalues(self):\n", "code": "for cookie in iter(self):\n    yield cookie.value", "path": "libs\\requests\\cookies.py", "repo_name": "ring04h/dirfuzz", "stars": 144, "license": "None", "language": "python", "size": 17175}
{"docstring": "\"\"\"Updates this jar with cookies from another CookieJar or dict-like\"\"\"\n", "func_signal": "def update(self, other):\n", "code": "if isinstance(other, cookielib.CookieJar):\n    for cookie in other:\n        self.set_cookie(cookie)\nelse:\n    super(RequestsCookieJar, self).update(other)", "path": "libs\\requests\\cookies.py", "repo_name": "ring04h/dirfuzz", "stars": 144, "license": "None", "language": "python", "size": 17175}
{"docstring": "\"\"\"Dict-like get() that also supports optional domain and path args in\norder to resolve naming collisions from using one cookie jar over\nmultiple domains. Caution: operation is O(n), not O(1).\"\"\"\n", "func_signal": "def get(self, name, default=None, domain=None, path=None):\n", "code": "try:\n    return self._find_no_duplicates(name, domain, path)\nexcept KeyError:\n    return default", "path": "libs\\requests\\cookies.py", "repo_name": "ring04h/dirfuzz", "stars": 144, "license": "None", "language": "python", "size": 17175}
{"docstring": "# Only return the response's URL if the user hadn't set the Host\n# header\n", "func_signal": "def get_full_url(self):\n", "code": "if not self._r.headers.get('Host'):\n    return self._r.url\n# If they did set it, retrieve it and reconstruct the expected domain\nhost = self._r.headers['Host']\nparsed = urlparse(self._r.url)\n# Reconstruct the URL as we expect it\nreturn urlunparse([\n    parsed.scheme, host, parsed.path, parsed.params, parsed.query,\n    parsed.fragment\n])", "path": "libs\\requests\\cookies.py", "repo_name": "ring04h/dirfuzz", "stars": 144, "license": "None", "language": "python", "size": 17175}
{"docstring": "\"\"\"Constructs and sends a :class:`Request <Request>`.\nReturns :class:`Response <Response>` object.\n\n:param method: method for the new :class:`Request` object.\n:param url: URL for the new :class:`Request` object.\n:param params: (optional) Dictionary or bytes to be sent in the query string for the :class:`Request`.\n:param data: (optional) Dictionary, bytes, or file-like object to send in the body of the :class:`Request`.\n:param json: (optional) json data to send in the body of the :class:`Request`.\n:param headers: (optional) Dictionary of HTTP Headers to send with the :class:`Request`.\n:param cookies: (optional) Dict or CookieJar object to send with the :class:`Request`.\n:param files: (optional) Dictionary of ``'name': file-like-objects`` (or ``{'name': ('filename', fileobj)}``) for multipart encoding upload.\n:param auth: (optional) Auth tuple to enable Basic/Digest/Custom HTTP Auth.\n:param timeout: (optional) How long to wait for the server to send data\n    before giving up, as a float, or a (`connect timeout, read timeout\n    <user/advanced.html#timeouts>`_) tuple.\n:type timeout: float or tuple\n:param allow_redirects: (optional) Boolean. Set to True if POST/PUT/DELETE redirect following is allowed.\n:type allow_redirects: bool\n:param proxies: (optional) Dictionary mapping protocol to the URL of the proxy.\n:param verify: (optional) if ``True``, the SSL cert will be verified. A CA_BUNDLE path can also be provided.\n:param stream: (optional) if ``False``, the response content will be immediately downloaded.\n:param cert: (optional) if String, path to ssl client cert file (.pem). If Tuple, ('cert', 'key') pair.\n\nUsage::\n\n  >>> import requests\n  >>> req = requests.request('GET', 'http://httpbin.org/get')\n  <Response [200]>\n\"\"\"\n\n", "func_signal": "def request(method, url, **kwargs):\n", "code": "session = sessions.Session()\nresponse = session.request(method=method, url=url, **kwargs)\n# By explicitly closing the session, we avoid leaving sockets open which\n# can trigger a ResourceWarning in some cases, and look like a memory leak\n# in others.\nsession.close()\nreturn response", "path": "libs\\requests\\api.py", "repo_name": "ring04h/dirfuzz", "stars": 144, "license": "None", "language": "python", "size": 17175}
{"docstring": "\"\"\"Add cookies to cookiejar and returns a merged CookieJar.\n\n:param cookiejar: CookieJar object to add the cookies to.\n:param cookies: Dictionary or CookieJar object to be added.\n\"\"\"\n", "func_signal": "def merge_cookies(cookiejar, cookies):\n", "code": "if not isinstance(cookiejar, cookielib.CookieJar):\n    raise ValueError('You can only merge into CookieJar')\n\nif isinstance(cookies, dict):\n    cookiejar = cookiejar_from_dict(\n        cookies, cookiejar=cookiejar, overwrite=False)\nelif isinstance(cookies, cookielib.CookieJar):\n    try:\n        cookiejar.update(cookies)\n    except AttributeError:\n        for cookie_in_jar in cookies:\n            cookiejar.set_cookie(cookie_in_jar)\n\nreturn cookiejar", "path": "libs\\requests\\cookies.py", "repo_name": "ring04h/dirfuzz", "stars": 144, "license": "None", "language": "python", "size": 17175}
{"docstring": "\"\"\"Sends a OPTIONS request. Returns :class:`Response` object.\n\n:param url: URL for the new :class:`Request` object.\n:param \\*\\*kwargs: Optional arguments that ``request`` takes.\n\"\"\"\n\n", "func_signal": "def options(url, **kwargs):\n", "code": "kwargs.setdefault('allow_redirects', True)\nreturn request('options', url, **kwargs)", "path": "libs\\requests\\api.py", "repo_name": "ring04h/dirfuzz", "stars": 144, "license": "None", "language": "python", "size": 17175}
{"docstring": "\"\"\"Make a cookie from underspecified parameters.\n\nBy default, the pair of `name` and `value` will be set for the domain ''\nand sent on every request (this is sometimes called a \"supercookie\").\n\"\"\"\n", "func_signal": "def create_cookie(name, value, **kwargs):\n", "code": "result = dict(\n    version=0,\n    name=name,\n    value=value,\n    port=None,\n    domain='',\n    path='/',\n    secure=False,\n    expires=None,\n    discard=True,\n    comment=None,\n    comment_url=None,\n    rest={'HttpOnly': None},\n    rfc2109=False,)\n\nbadargs = set(kwargs) - set(result)\nif badargs:\n    err = 'create_cookie() got unexpected keyword arguments: %s'\n    raise TypeError(err % list(badargs))\n\nresult.update(kwargs)\nresult['port_specified'] = bool(result['port'])\nresult['domain_specified'] = bool(result['domain'])\nresult['domain_initial_dot'] = result['domain'].startswith('.')\nresult['path_specified'] = bool(result['path'])\n\nreturn cookielib.Cookie(**result)", "path": "libs\\requests\\cookies.py", "repo_name": "ring04h/dirfuzz", "stars": 144, "license": "None", "language": "python", "size": 17175}
{"docstring": "\"\"\"Dict-like set() that also supports optional domain and path args in\norder to resolve naming collisions from using one cookie jar over\nmultiple domains.\"\"\"\n# support client code that unsets cookies by assignment of a None value:\n", "func_signal": "def set(self, name, value, **kwargs):\n", "code": "if value is None:\n    remove_cookie_by_name(self, name, domain=kwargs.get('domain'), path=kwargs.get('path'))\n    return\n\nif isinstance(value, Morsel):\n    c = morsel_to_cookie(value)\nelse:\n    c = create_cookie(name, value, **kwargs)\nself.set_cookie(c)\nreturn c", "path": "libs\\requests\\cookies.py", "repo_name": "ring04h/dirfuzz", "stars": 144, "license": "None", "language": "python", "size": 17175}
{"docstring": "\"\"\"Returns a CookieJar from a key/value dictionary.\n\n:param cookie_dict: Dict of key/values to insert into CookieJar.\n:param cookiejar: (optional) A cookiejar to add the cookies to.\n:param overwrite: (optional) If False, will not replace cookies\n    already in the jar with new ones.\n\"\"\"\n", "func_signal": "def cookiejar_from_dict(cookie_dict, cookiejar=None, overwrite=True):\n", "code": "if cookiejar is None:\n    cookiejar = RequestsCookieJar()\n\nif cookie_dict is not None:\n    names_from_jar = [cookie.name for cookie in cookiejar]\n    for name in cookie_dict:\n        if overwrite or (name not in names_from_jar):\n            cookiejar.set_cookie(create_cookie(name, cookie_dict[name]))\n\nreturn cookiejar", "path": "libs\\requests\\cookies.py", "repo_name": "ring04h/dirfuzz", "stars": 144, "license": "None", "language": "python", "size": 17175}
{"docstring": "\"\"\"Produce an appropriate Cookie header string to be sent with `request`, or None.\"\"\"\n", "func_signal": "def get_cookie_header(jar, request):\n", "code": "r = MockRequest(request)\njar.add_cookie_header(r)\nreturn r.get_new_headers().get('Cookie')", "path": "libs\\requests\\cookies.py", "repo_name": "ring04h/dirfuzz", "stars": 144, "license": "None", "language": "python", "size": 17175}
{"docstring": "\"\"\"\ninitialized an ant, to traverse the map\ninit_location -> marks where in the map that the ant starts\npossible_locations -> a list of possible nodes the ant can go to\n\twhen used internally, gives a list of possible locations the ant can traverse to _minus those nodes already visited_\npheromone_map -> map of pheromone values for each traversal between each node\ndistance_callback -> is a function to calculate the distance between two nodes\nalpha -> a parameter from the ACO algorithm to control the influence of the amount of pheromone when making a choice in _pick_path()\nbeta -> a parameters from ACO that controls the influence of the distance to the next node in _pick_path()\nfirst_pass -> if this is a first pass on a map, then do some steps differently, noted in methods below\n\nroute -> a list that is updated with the labels of the nodes that the ant has traversed\npheromone_trail -> a list of pheromone amounts deposited along the ants trail, maps to each traversal in route\ndistance_traveled -> total distance tranveled along the steps in route\nlocation -> marks where the ant currently is\ntour_complete -> flag to indicate the ant has completed its traversal\n\tused by get_route() and get_distance_traveled()\n\"\"\"\n", "func_signal": "def __init__(self, init_location, possible_locations, pheromone_map, distance_callback, alpha, beta, first_pass=False):\n", "code": "Thread.__init__(self)\n\nself.init_location = init_location\nself.possible_locations = possible_locations\t\t\t\nself.route = []\nself.distance_traveled = 0.0\nself.location = init_location\nself.pheromone_map = pheromone_map\nself.distance_callback = distance_callback\nself.alpha = alpha\nself.beta = beta\nself.first_pass = first_pass\n\n#append start location to route, before doing random walk\nself._update_route(init_location)\n\nself.tour_complete = False", "path": "ant_colony.py", "repo_name": "pjmattingly/ant-colony-optimization", "stars": 147, "license": "None", "language": "python", "size": 69}
{"docstring": "\"\"\"\nsetup a matrix NxN (where n = size)\nused in both self.distance_matrix and self.pheromone_map\nas they require identical matrixes besides which value to initialize to\n\"\"\"\n", "func_signal": "def _init_matrix(self, size, value=0.0):\n", "code": "ret = []\nfor row in range(size):\n\tret.append([float(value) for x in range(size)])\nreturn ret", "path": "ant_colony.py", "repo_name": "pjmattingly/ant-colony-optimization", "stars": 147, "license": "None", "language": "python", "size": 69}
{"docstring": "\"\"\"\ncreate a mapping of internal id numbers (0 .. n) to the keys in the nodes passed \ncreate a mapping of the id's to the values of nodes\nwe use id_to_key to return the route in the node names the caller expects in mainloop()\n\"\"\"\n", "func_signal": "def _init_nodes(self, nodes):\n", "code": "id_to_key = dict()\nid_to_values = dict()\n\nid = 0\nfor key in sorted(nodes.keys()):\n\tid_to_key[id] = key\n\tid_to_values[id] = nodes[key]\n\tid += 1\n\t\nreturn id_to_key, id_to_values", "path": "ant_colony.py", "repo_name": "pjmattingly/ant-colony-optimization", "stars": 147, "license": "None", "language": "python", "size": 69}
{"docstring": "\"\"\"\ncreate a mapping of internal id numbers (0 .. n) to the keys in the nodes passed \ncreate a mapping of the id's to the values of nodes\nwe use id_to_key to return the route in the node names the caller expects in mainloop()\n\"\"\"\n", "func_signal": "def test_init_nodes(self, nodes):\n", "code": "id_to_key = dict()\nid_to_values = dict()\n\nid = 0\nfor key in sorted(nodes.keys()):\n\tid_to_key[id] = key\n\tid_to_values[id] = nodes[key]\n\tid += 1\n\t\nreturn id_to_key, id_to_values", "path": "test\\test_ant_colony_init.py", "repo_name": "pjmattingly/ant-colony-optimization", "stars": 147, "license": "None", "language": "python", "size": 69}
{"docstring": "\"\"\"\non first pass:\n\tcreate a number of ant objects\non subsequent passes, just call __init__ on each to reset them\nby default, all ants start at the first node, 0\nas per problem description: https://www.codeeval.com/open_challenges/90/\n\"\"\"\n#allocate new ants on the first pass\n", "func_signal": "def _init_ants(self, start):\n", "code": "if self.first_pass:\n\treturn [self.ant(start, self.nodes.keys(), self.pheromone_map, self._get_distance,\n\t\tself.alpha, self.beta, first_pass=True) for x in range(self.ant_count)]\n#else, just reset them to use on another pass\nfor ant in self.ants:\n\tant.__init__(start, self.nodes.keys(), self.pheromone_map, self._get_distance, self.alpha, self.beta)", "path": "ant_colony.py", "repo_name": "pjmattingly/ant-colony-optimization", "stars": 147, "license": "None", "language": "python", "size": 69}
{"docstring": "\"\"\"\ncreate a mapping of internal id numbers (0 .. n) to the keys in the nodes passed \ncreate a mapping of the id's to the values of nodes\nwe use id_to_key to return the route in the node names the caller expects in mainloop()\n\"\"\"\n", "func_signal": "def test_init_nodes(self, nodes):\n", "code": "id_to_key = dict()\nid_to_values = dict()\n\nid = 0\nfor key in sorted(nodes.keys()):\n\tid_to_key[id] = key\n\tid_to_values[id] = nodes[key]\n\tid += 1\n\t\nreturn id_to_key, id_to_values", "path": "test\\test_ant_colony_init.py", "repo_name": "pjmattingly/ant-colony-optimization", "stars": 147, "license": "None", "language": "python", "size": 69}
{"docstring": "\"\"\"\ninitializes an ant colony (houses a number of worker ants that will traverse a map to find an optimal route as per ACO [Ant Colony Optimization])\nsource: https://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms\n\nnodes -> is assumed to be a dict() mapping node ids to values \n\tthat are understandable by distance_callback\n\t\ndistance_callback -> is assumed to take a pair of coordinates and return the distance between them\n\tpopulated into distance_matrix on each call to get_distance()\n\t\nstart -> if set, then is assumed to be the node where all ants start their traversal\n\tif unset, then assumed to be the first key of nodes when sorted()\n\ndistance_matrix -> holds values of distances calculated between nodes\n\tpopulated on demand by _get_distance()\n\npheromone_map -> holds final values of pheromones\n\tused by ants to determine traversals\n\tpheromone dissipation happens to these values first, before adding pheromone values from the ants during their traversal\n\t(in ant_updated_pheromone_map)\n\t\nant_updated_pheromone_map -> a matrix to hold the pheromone values that the ants lay down\n\tnot used to dissipate, values from here are added to pheromone_map after dissipation step\n\t(reset for each traversal)\n\t\nalpha -> a parameter from the ACO algorithm to control the influence of the amount of pheromone when an ant makes a choice\n\nbeta -> a parameters from ACO that controls the influence of the distance to the next node in ant choice making\n\npheromone_constant -> a parameter used in depositing pheromones on the map (Q in ACO algorithm)\n\tused by _update_pheromone_map()\n\t\npheromone_evaporation_coefficient -> a parameter used in removing pheromone values from the pheromone_map (rho in ACO algorithm)\n\tused by _update_pheromone_map()\n\nants -> holds worker ants\n\tthey traverse the map as per ACO\n\tnotable properties:\n\t\ttotal distance traveled\n\t\troute\n\t\nfirst_pass -> flags a first pass for the ants, which triggers unique behavior\n\niterations -> how many iterations to let the ants traverse the map\n\nshortest_distance -> the shortest distance seen from an ant traversal\n\nshortets_path_seen -> the shortest path seen from a traversal (shortest_distance is the distance along this path)\n\"\"\"\n#nodes\n", "func_signal": "def __init__(self, nodes, distance_callback, start=None, ant_count=50, alpha=.5, beta=1.2,  pheromone_evaporation_coefficient=.40, pheromone_constant=1000.0, iterations=80):\n", "code": "if type(nodes) is not dict:\n\traise TypeError(\"nodes must be dict\")\n\nif len(nodes) < 1:\n\traise ValueError(\"there must be at least one node in dict nodes\")\n\n#create internal mapping and mapping for return to caller\nself.id_to_key, self.nodes = self._init_nodes(nodes)\n#create matrix to hold distance calculations between nodes\nself.distance_matrix = self._init_matrix(len(nodes))\n#create matrix for master pheromone map, that records pheromone amounts along routes\nself.pheromone_map = self._init_matrix(len(nodes))\n#create a matrix for ants to add their pheromones to, before adding those to pheromone_map during the update_pheromone_map step\nself.ant_updated_pheromone_map = self._init_matrix(len(nodes))\n\n#distance_callback\nif not callable(distance_callback):\n\traise TypeError(\"distance_callback is not callable, should be method\")\n\t\nself.distance_callback = distance_callback\n\n#start\nif start is None:\n\tself.start = 0\nelse:\n\tself.start = None\n\t#init start to internal id of node id passed\n\tfor key, value in self.id_to_key.items():\n\t\tif value == start:\n\t\t\tself.start = key\n\t\n\t#if we didn't find a key in the nodes passed in, then raise\n\tif self.start is None:\n\t\traise KeyError(\"Key: \" + str(start) + \" not found in the nodes dict passed.\")\n\n#ant_count\nif type(ant_count) is not int:\n\traise TypeError(\"ant_count must be int\")\n\t\nif ant_count < 1:\n\traise ValueError(\"ant_count must be >= 1\")\n\nself.ant_count = ant_count\n\n#alpha\t\nif (type(alpha) is not int) and type(alpha) is not float:\n\traise TypeError(\"alpha must be int or float\")\n\nif alpha < 0:\n\traise ValueError(\"alpha must be >= 0\")\n\nself.alpha = float(alpha)\n\n#beta\nif (type(beta) is not int) and type(beta) is not float:\n\traise TypeError(\"beta must be int or float\")\n\t\nif beta < 1:\n\traise ValueError(\"beta must be >= 1\")\n\t\nself.beta = float(beta)\n\n#pheromone_evaporation_coefficient\nif (type(pheromone_evaporation_coefficient) is not int) and type(pheromone_evaporation_coefficient) is not float:\n\traise TypeError(\"pheromone_evaporation_coefficient must be int or float\")\n\nself.pheromone_evaporation_coefficient = float(pheromone_evaporation_coefficient)\n\n#pheromone_constant\nif (type(pheromone_constant) is not int) and type(pheromone_constant) is not float:\n\traise TypeError(\"pheromone_constant must be int or float\")\n\nself.pheromone_constant = float(pheromone_constant)\n\n#iterations\nif (type(iterations) is not int):\n\traise TypeError(\"iterations must be int\")\n\nif iterations < 0:\n\traise ValueError(\"iterations must be >= 0\")\n\t\nself.iterations = iterations\n\n#other internal variable init\nself.first_pass = True\nself.ants = self._init_ants(self.start)\nself.shortest_distance = None\nself.shortest_path_seen = None", "path": "ant_colony.py", "repo_name": "pjmattingly/ant-colony-optimization", "stars": 147, "license": "None", "language": "python", "size": 69}
{"docstring": "#_DEBUG(\"[testing_distance_callback()] START\")\n# _DEBUG(start)\n# _DEBUG(end)\n", "func_signal": "def testing_distance_callback(start, end):\n", "code": "if (start == (0, 0) and end == (1, 1)) or (start == (1, 1) and end == (0, 0)):\n\t# _DEBUG(\"[testing_distance_callback()] saw 0 -> 1\")\n\t# _DEBUG(\"[testing_distance_callback()] END\")\n\treturn 1.0\nif (start == (1, 1) and end == (2, 2))or (start == (2, 2) and end == (1, 1)):\n\t# _DEBUG(\"[testing_distance_callback()] saw 1 -> 2\")\n\t# _DEBUG(\"[testing_distance_callback()] END\")\n\treturn 1.0\nif (start == (2, 2) and end == (3, 3))or (start == (3, 3) and end == (2, 2)):\n\t# _DEBUG(\"[testing_distance_callback()] saw 2 -> 3\")\n\t# _DEBUG(\"[testing_distance_callback()] END\")\n\treturn 1.0\n#_DEBUG(\"[testing_distance_callback()] END\")\nreturn 3.0", "path": "test\\test_ant_colony_integration_testing.py", "repo_name": "pjmattingly/ant-colony-optimization", "stars": 147, "license": "None", "language": "python", "size": 69}
{"docstring": "\"\"\"\ncreate a mapping of internal id numbers (0 .. n) to the keys in the nodes passed \ncreate a mapping of the id's to the values of nodes\nwe use id_to_key to return the route in the node names the caller expects in mainloop()\n\"\"\"\n", "func_signal": "def test_init_nodes(self, nodes):\n", "code": "id_to_key = dict()\nid_to_values = dict()\n\nid = 0\nfor key in sorted(nodes.keys()):\n\tid_to_key[id] = key\n\tid_to_values[id] = nodes[key]\n\tid += 1\n\t\nreturn id_to_key, id_to_values", "path": "test\\test_ant_colony_init.py", "repo_name": "pjmattingly/ant-colony-optimization", "stars": 147, "license": "None", "language": "python", "size": 69}
{"docstring": "\"\"\"\ngiven an ant, populate ant_updated_pheromone_map with pheromone values according to ACO\nalong the ant's route\ncalled from:\n\tmainloop()\n\t( before _update_pheromone_map() )\n\"\"\"\n", "func_signal": "def _populate_ant_updated_pheromone_map(self, ant):\n", "code": "route = ant.get_route()\nfor i in range(len(route)-1):\n\t#find the pheromone over the route the ant traversed\n\tcurrent_pheromone_value = float(self.ant_updated_pheromone_map[route[i]][route[i+1]])\n\n\t#update the pheromone along that section of the route\n\t#(ACO)\n\t#\tdelta tau_xy_k = Q / L_k\n\tnew_pheromone_value = self.pheromone_constant/ant.get_distance_traveled()\n\t\n\tself.ant_updated_pheromone_map[route[i]][route[i+1]] = current_pheromone_value + new_pheromone_value\n\tself.ant_updated_pheromone_map[route[i+1]][route[i]] = current_pheromone_value + new_pheromone_value", "path": "ant_colony.py", "repo_name": "pjmattingly/ant-colony-optimization", "stars": 147, "license": "None", "language": "python", "size": 69}
{"docstring": "\"\"\"\nsource: https://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms#Edge_selection\nimplements the path selection algorithm of ACO\ncalculate the attractiveness of each possible transition from the current location\nthen randomly choose a next path, based on its attractiveness\n\"\"\"\n#on the first pass (no pheromones), then we can just choice() to find the next one\n", "func_signal": "def _pick_path(self):\n", "code": "if self.first_pass:\n\timport random\n\treturn random.choice(self.possible_locations)\n\nattractiveness = dict()\nsum_total = 0.0\n#for each possible location, find its attractiveness (it's (pheromone amount)*1/distance [tau*eta, from the algortihm])\n#sum all attrativeness amounts for calculating probability of each route in the next step\nfor possible_next_location in self.possible_locations:\n\t#NOTE: do all calculations as float, otherwise we get integer division at times for really hard to track down bugs\n\tpheromone_amount = float(self.pheromone_map[self.location][possible_next_location])\n\tdistance = float(self.distance_callback(self.location, possible_next_location))\n\t\n\t#tau^alpha * eta^beta\n\tattractiveness[possible_next_location] = pow(pheromone_amount, self.alpha)*pow(1/distance, self.beta)\n\tsum_total += attractiveness[possible_next_location]\n\n#it is possible to have small values for pheromone amount / distance, such that with rounding errors this is equal to zero\n#rare, but handle when it happens\nif sum_total == 0.0:\n\t#increment all zero's, such that they are the smallest non-zero values supported by the system\n\t#source: http://stackoverflow.com/a/10426033/5343977\n\tdef next_up(x):\n\t\timport math\n\t\timport struct\n\t\t# NaNs and positive infinity map to themselves.\n\t\tif math.isnan(x) or (math.isinf(x) and x > 0):\n\t\t\treturn x\n\n\t\t# 0.0 and -0.0 both map to the smallest +ve float.\n\t\tif x == 0.0:\n\t\t\tx = 0.0\n\n\t\tn = struct.unpack('<q', struct.pack('<d', x))[0]\n\t\t\n\t\tif n >= 0:\n\t\t\tn += 1\n\t\telse:\n\t\t\tn -= 1\n\t\treturn struct.unpack('<d', struct.pack('<q', n))[0]\n\t\t\n\tfor key in attractiveness:\n\t\tattractiveness[key] = next_up(attractiveness[key])\n\tsum_total = next_up(sum_total)\n\n#cumulative probability behavior, inspired by: http://stackoverflow.com/a/3679747/5343977\n#randomly choose the next path\nimport random\ntoss = random.random()\n\t\t\ncummulative = 0\nfor possible_next_location in attractiveness:\n\tweight = (attractiveness[possible_next_location] / sum_total)\n\tif toss <= weight + cummulative:\n\t\treturn possible_next_location\n\tcummulative += weight", "path": "ant_colony.py", "repo_name": "pjmattingly/ant-colony-optimization", "stars": 147, "license": "None", "language": "python", "size": 69}
{"docstring": "\"\"\"\n_update_route() to show new traversal\n_update_distance_traveled() to record new distance traveled\nself.location update to new location\ncalled from run()\n\"\"\"\n", "func_signal": "def _traverse(self, start, end):\n", "code": "self._update_route(end)\nself._update_distance_traveled(start, end)\nself.location = end", "path": "ant_colony.py", "repo_name": "pjmattingly/ant-colony-optimization", "stars": 147, "license": "None", "language": "python", "size": 69}
{"docstring": "\"\"\"\nGives distance between two points.\nstart and end are assumed to be in decimal representation of latitude and longitude\neach lat / long pair are assumed to be in standard lat / long order\nsources: http://www.movable-type.co.uk/scripts/latlong.html\n\thttp://www.codecodex.com/wiki/Calculate_Distance_Between_Two_Points_on_a_Globe#Python\n\t(modified the above code, from non-standard input ordering [long / lat])\nExample: distance((37.7689269, -122.4029053), (37.7768800, -122.3911496))\nReturn: 1.36002518696 km\n(example lat / long pairs are, respectively: airbnb and dropbox headquarters in SF CA)\n\"\"\"\n", "func_signal": "def distance_on_earth(start, end):\n", "code": "import math\n\ndef recalculate_coordinate(val,  _as=None):  \n\t\"\"\" \n\tAccepts a coordinate as a tuple (degree, minutes, seconds) \n\tYou can give only one of them (e.g. only minutes as a floating point number) and it will be duly \n\trecalculated into degrees, minutes and seconds. \n\tReturn value can be specified as 'deg', 'min' or 'sec'; default return value is a proper coordinate tuple. \n\t\"\"\"  \n\tdeg,  min,  sec = val  \n\t# pass outstanding values from right to left  \n\tmin = (min or 0) + int(sec) / 60  \n\tsec = sec % 60  \n\tdeg = (deg or 0) + int(min) / 60  \n\tmin = min % 60  \n\t# pass decimal part from left to right  \n\tdfrac,  dint = math.modf(deg)  \n\tmin = min + dfrac * 60  \n\tdeg = dint  \n\tmfrac,  mint = math.modf(min)  \n\tsec = sec + mfrac * 60  \n\tmin = mint  \n\tif _as:  \n\t\tsec = sec + min * 60 + deg * 3600\n\t\tif _as == 'sec': return sec\n\t\tif _as == 'min': return sec / 60\n\t\tif _as == 'deg': return sec / 3600\n\treturn deg,  min,  sec\n\ndef points2distance(start,  end):  \n\t\"\"\" \n\tCalculate distance (in kilometers) between two points given as (long, latt) pairs \n\tbased on Haversine formula (http://en.wikipedia.org/wiki/Haversine_formula). \n\tImplementation inspired by JavaScript implementation from http://www.movable-type.co.uk/scripts/latlong.html \n\tAccepts coordinates as tuples (deg, min, sec), but coordinates can be given in any form - e.g. \n\tcan specify only minutes: \n\t(0, 3133.9333, 0)  \n\tis interpreted as  \n\t(52.0, 13.0, 55.998000000008687) \n\twhich, not accidentally, is the lattitude of Warsaw, Poland. \n\t\"\"\"  \n\tstart_long = math.radians(recalculate_coordinate(start[1],  'deg'))  \n\tstart_latt = math.radians(recalculate_coordinate(start[0],  'deg'))  \n\tend_long = math.radians(recalculate_coordinate(end[1],  'deg'))  \n\tend_latt = math.radians(recalculate_coordinate(end[0],  'deg'))\n\td_latt = end_latt - start_latt  \n\td_long = end_long - start_long  \n\ta = math.sin(d_latt/2)**2 + math.cos(start_latt) * math.cos(end_latt) * math.sin(d_long/2)**2  \n\tc = 2 * math.asin(math.sqrt(a))\n\treturn 6371 * c\n\ndef decdeg2dms(dd):\n\t\"\"\"\n\tSource: http://stackoverflow.com/a/12737895/5343977\n\t\"\"\"\n\tnegative = dd < 0\n\tdd = abs(dd)\n\tminutes,seconds = divmod(dd*3600,60)\n\tdegrees,minutes = divmod(minutes,60)\n\tif negative:\n\t\tif degrees > 0:\n\t\t\tdegrees = -degrees\n\t\telif minutes > 0:\n\t\t\tminutes = -minutes\n\t\telse:\n\t\t\tseconds = -seconds\n\treturn (degrees,minutes,seconds)\n\n#converting to degrees / minutes / seconds representation, as points2distance() requires it\nstart_dms = (decdeg2dms(start[0]), decdeg2dms(start[1]))\nend_dms = (decdeg2dms(end[0]), decdeg2dms(end[1]))\nreturn float(points2distance(start_dms, end_dms))", "path": "test\\test_ant_colony_integration_testing.py", "repo_name": "pjmattingly/ant-colony-optimization", "stars": 147, "license": "None", "language": "python", "size": 69}
{"docstring": "\"\"\"\nsetup a matrix NxN (where n = size)\nused in both self.distance_matrix and self.pheromone_map\nas they require identical matrixes besides which value to initialize to\n\"\"\"\n", "func_signal": "def _init_matrix(size, value=None):\n", "code": "ret = []\nfor row in range(size):\n\tret.append([value for x in range(size)])\nreturn ret", "path": "test\\test_ant_colony_get_distance.py", "repo_name": "pjmattingly/ant-colony-optimization", "stars": 147, "license": "None", "language": "python", "size": 69}
{"docstring": "\"\"\"\nuntil self.possible_locations is empty (the ant has visited all nodes)\n\t_pick_path() to find a next node to traverse to\n\t_traverse() to:\n\t\t_update_route() (to show latest traversal)\n\t\t_update_distance_traveled() (after traversal)\nreturn the ants route and its distance, for use in ant_colony:\n\tdo pheromone updates\n\tcheck for new possible optimal solution with this ants latest tour\n\"\"\"\n", "func_signal": "def run(self):\n", "code": "while self.possible_locations:\n\tnext = self._pick_path()\n\tself._traverse(self.location, next)\n\t\nself.tour_complete = True", "path": "ant_colony.py", "repo_name": "pjmattingly/ant-colony-optimization", "stars": 147, "license": "None", "language": "python", "size": 69}
{"docstring": "\"\"\"\nRuns the worker ants, collects their returns and updates the pheromone map with pheromone values from workers\n\tcalls:\n\t_update_pheromones()\n\tant.run()\nruns the simulation self.iterations times\n\"\"\"\n\n", "func_signal": "def mainloop(self):\n", "code": "for _ in range(self.iterations):\n\t#start the multi-threaded ants, calls ant.run() in a new thread\n\tfor ant in self.ants:\n\t\tant.start()\n\t\n\t#source: http://stackoverflow.com/a/11968818/5343977\n\t#wait until the ants are finished, before moving on to modifying shared resources\n\tfor ant in self.ants:\n\t\tant.join()\n\t\n\tfor ant in self.ants:\t\n\t\t#update ant_updated_pheromone_map with this ant's constribution of pheromones along its route\n\t\tself._populate_ant_updated_pheromone_map(ant)\n\t\t\n\t\t#if we haven't seen any paths yet, then populate for comparisons later\n\t\tif not self.shortest_distance:\n\t\t\tself.shortest_distance = ant.get_distance_traveled()\n\t\t\n\t\tif not self.shortest_path_seen:\n\t\t\tself.shortest_path_seen = ant.get_route()\n\t\t\t\n\t\t#if we see a shorter path, then save for return\n\t\tif ant.get_distance_traveled() < self.shortest_distance:\n\t\t\tself.shortest_distance = ant.get_distance_traveled()\n\t\t\tself.shortest_path_seen = ant.get_route()\n\t\n\t#decay current pheromone values and add all pheromone values we saw during traversal (from ant_updated_pheromone_map)\n\tself._update_pheromone_map()\n\t\n\t#flag that we finished the first pass of the ants traversal\n\tif self.first_pass:\n\t\tself.first_pass = False\n\t\n\t#reset all ants to default for the next iteration\n\tself._init_ants(self.start)\n\t\n\t#reset ant_updated_pheromone_map to record pheromones for ants on next pass\n\tself.ant_updated_pheromone_map = self._init_matrix(len(self.nodes), value=0)\n\n#translate shortest path back into callers node id's\nret = []\nfor id in self.shortest_path_seen:\n\tret.append(self.id_to_key[id])\n\nreturn ret", "path": "ant_colony.py", "repo_name": "pjmattingly/ant-colony-optimization", "stars": 147, "license": "None", "language": "python", "size": 69}
{"docstring": "#Note: can't do this in setup because of python2's wonky OOP, doing it in the test instead\n\n#inherit from ant so we can call _update_distance_traveled correctly\n", "func_signal": "def test_correct(self):\n", "code": "class test_empty_object(module.ant_colony.ant):\n\t#override each method EXCEPT _update_distance_traveled, to get a clean testing environment\n\tdef __init__(self): pass\n\tdef run(self): pass\n\tdef _traverse(self): pass\n\tdef _update_route(self): pass\n\tdef _pick_path(self): pass\n\t#def _update_distance_traveled(self): pass\ntest_object = test_empty_object()\n\n#setting up object environment\ntest_object.distance_traveled = 0\n\t\t\n#setup the distance callback to get a value for the distance between nodes\ndef mock_distance_callback(start, end):\n\treturn 1\n\ntest_object.distance_callback = mock_distance_callback\n\ntest_object._update_distance_traveled(0, 1)\n\nself.assertEqual(test_object.distance_traveled, 1)", "path": "test\\test_ant_update_distance_traveled.py", "repo_name": "pjmattingly/ant-colony-optimization", "stars": 147, "license": "None", "language": "python", "size": 69}
{"docstring": "\"\"\"\nsetup a matrix NxN (where n = size)\nused in both self.distance_matrix and self.pheromone_map\nas they require identical matrixes besides which value to initialize to\n\"\"\"\n", "func_signal": "def _init_matrix(size, value=None):\n", "code": "ret = []\nfor row in range(size):\n\tret.append([value for x in range(size)])\nreturn ret", "path": "test\\test_ant_colony_main_loop.py", "repo_name": "pjmattingly/ant-colony-optimization", "stars": 147, "license": "None", "language": "python", "size": 69}
{"docstring": "\"\"\"\n1)\tUpdate self.pheromone_map by decaying values contained therein via the ACO algorithm\n2)\tAdd pheromone_values from all ants from ant_updated_pheromone_map\ncalled by:\n\tmainloop()\n\t(after all ants have traveresed)\n\"\"\"\n#always a square matrix\n", "func_signal": "def _update_pheromone_map(self):\n", "code": "for start in range(len(self.pheromone_map)):\n\tfor end in range(len(self.pheromone_map)):\n\t\t#decay the pheromone value at this location\n\t\t#tau_xy <- (1-rho)*tau_xy\t(ACO)\n\t\tself.pheromone_map[start][end] = (1-self.pheromone_evaporation_coefficient)*self.pheromone_map[start][end]\n\t\t\n\t\t#then add all contributions to this location for each ant that travered it\n\t\t#(ACO)\n\t\t#tau_xy <- tau_xy + delta tau_xy_k\n\t\t#\tdelta tau_xy_k = Q / L_k\n\t\tself.pheromone_map[start][end] += self.ant_updated_pheromone_map[start][end]", "path": "ant_colony.py", "repo_name": "pjmattingly/ant-colony-optimization", "stars": 147, "license": "None", "language": "python", "size": 69}
{"docstring": "\"\"\"\nadd new node to self.route\nremove new node form self.possible_location\ncalled from _traverse() & __init__()\n\"\"\"\n", "func_signal": "def _update_route(self, new):\n", "code": "self.route.append(new)\nself.possible_locations.remove(new)", "path": "ant_colony.py", "repo_name": "pjmattingly/ant-colony-optimization", "stars": 147, "license": "None", "language": "python", "size": 69}
{"docstring": "\"\"\"\nReturn a new S instance with filter args combined with\nexisting set with AND.\n\n:arg filters: this will be instances of F\n:arg kw: this will be in the form of ``field__action=value``\n\nExamples:\n\n>>> s = S().filter(foo='bar')\n>>> s = S().filter(F(foo='bar'))\n>>> s = S().filter(foo='bar', bat='baz')\n>>> s = S().filter(foo='bar').filter(bat='baz')\n\nBy default, everything is combined using AND. If you provide\nmultiple filters in a single filter call, those are ANDed\ntogether. If you provide multiple filters in multiple filter\ncalls, those are ANDed together.\n\nIf you want something different, use the F class which supports\n``&`` (and), ``|`` (or) and ``~`` (not) operators. Then call\nfilter once with the resulting F instance.\n\nSee the documentation on :py:class:`elasticutils.F` for more\ndetails on composing filters with F.\n\nSee the documentation on :py:class:`elasticutils.S` for more\ndetails on adding support for new filter types.\n\n\"\"\"\n", "func_signal": "def filter(self, *filters, **kw):\n", "code": "items = kw.items()\nif six.PY3:\n    items = list(items)\nreturn self._clone(\n    next_step=('filter', list(filters) + items))", "path": "elasticutils\\__init__.py", "repo_name": "mozilla/elasticutils", "stars": 246, "license": "bsd-3-clause", "language": "python", "size": 2550}
{"docstring": "\"\"\"Wrap a callable and return None if ES_DISABLED is False.\n\nThis also adds an additional `es` argument to the callable\ngiving you an ElasticSearch instance to use.\n\n\"\"\"\n", "func_signal": "def es_required(fun):\n", "code": "@wraps(fun)\ndef wrapper(*args, **kw):\n    if getattr(settings, 'ES_DISABLED', False):\n        log.debug('Search disabled for %s.' % fun)\n        return\n\n    return fun(*args, es=get_es(), **kw)\nreturn wrapper", "path": "elasticutils\\contrib\\django\\__init__.py", "repo_name": "mozilla/elasticutils", "stars": 246, "license": "bsd-3-clause", "language": "python", "size": 2550}
{"docstring": "\"\"\"\nReturn a new S instance with query args combined with existing\nset in a must boolean query.\n\n:arg queries: instances of Q\n:arg kw: queries in the form of ``field__action=value``\n\nThere are three special flags you can use:\n\n* ``must=True``: Specifies that the queries and kw queries\n  **must match** in order for a document to be in the result.\n\n  If you don't specify a special flag, this is the default.\n\n* ``should=True``: Specifies that the queries and kw queries\n  **should match** in order for a document to be in the result.\n\n* ``must_not=True``: Specifies the queries and kw queries\n  **must not match** in order for a document to be in the result.\n\nThese flags work by putting those queries in the appropriate\nclause of an Elasticsearch boolean query.\n\nExamples:\n\n>>> s = S().query(foo='bar')\n>>> s = S().query(Q(foo='bar'))\n>>> s = S().query(foo='bar', bat__match='baz')\n>>> s = S().query(foo='bar', should=True)\n>>> s = S().query(foo='bar', should=True).query(baz='bat', must=True)\n\nNotes:\n\n1. Don't specify multiple special flags, but if you did, `should`\n   takes precedence.\n2. If you don't specify any, it defaults to `must`.\n3. You can specify special flags in the\n   :py:class:`elasticutils.Q`, too. If you're building your\n   query incrementally, using :py:class:`elasticutils.Q` helps\n   a lot.\n\nSee the documentation on :py:class:`elasticutils.Q` for more\ndetails on composing queries with Q.\n\nSee the documentation on :py:class:`elasticutils.S` for more\ndetails on adding support for more query types.\n\n\"\"\"\n", "func_signal": "def query(self, *queries, **kw):\n", "code": "q = Q()\nfor query in queries:\n    q += query\n\nif 'or_' in kw:\n    # Backwards compatibile with pre-0.7 version.\n    or_query = kw.pop('or_')\n\n    # or_query here is a dict of key/val pairs. or_ indicates\n    # they're in a should clause, so we generate the\n    # equivalent Q and then add it in.\n    or_query['should'] = True\n    q += Q(**or_query)\n\nq += Q(**kw)\n\nreturn self._clone(next_step=('query', q))", "path": "elasticutils\\__init__.py", "repo_name": "mozilla/elasticutils", "stars": 246, "license": "bsd-3-clause", "language": "python", "size": 2550}
{"docstring": "\"\"\"Returns the list of doctypes to use.\"\"\"\n", "func_signal": "def get_doctypes(self, default_doctypes=DEFAULT_DOCTYPES):\n", "code": "for action, value in reversed(self.steps):\n    if action == 'doctypes':\n        return list(value)\n\nif self.type is not None:\n    return [self.type.get_mapping_type_name()]\n\nreturn default_doctypes", "path": "elasticutils\\__init__.py", "repo_name": "mozilla/elasticutils", "stars": 246, "license": "bsd-3-clause", "language": "python", "size": 2550}
{"docstring": "\"\"\"Tears down environment\n\n* unfixes settings\n* deletes the test index\n\n\"\"\"\n", "func_signal": "def tearDownClass(cls):\n", "code": "if not cls.skip_tests:\n    # If we didn't skip these tests, we need to do some\n    # cleanup.\n    for index in settings.ES_INDEXES.values():\n        cls.cleanup_index(index)\n\n    # Restore settings\n    settings.ES_DISABLED = cls._old_es_disabled\n    settings.ES_INDEXES = cls._old_es_indexes\n\nsuper(ESTestCase, cls).tearDownClass()", "path": "elasticutils\\contrib\\django\\estestcase.py", "repo_name": "mozilla/elasticutils", "stars": 246, "license": "bsd-3-clause", "language": "python", "size": 2550}
{"docstring": "\"\"\"Builds the Elasticsearch search body represented by this S.\n\nLoop over self.steps to build the search body that will be\nsent to Elasticsearch. This returns a Python dict.\n\nIf you want the JSON that actually gets sent, then pass the return\nvalue through :py:func:`elasticutils.utils.to_json`.\n\n:returns: a Python dict\n\n\"\"\"\n", "func_signal": "def build_search(self):\n", "code": "filters = []\nfilters_raw = None\nqueries = []\nquery_raw = None\nsort = []\ndict_fields = set()\nlist_fields = set()\nfacets = {}\nfacets_raw = {}\ndemote = None\nhighlight_fields = set()\nhighlight_options = {}\nsuggestions = {}\nexplain = False\nas_list = as_dict = False\nsearch_type = None\n\nfor action, value in self.steps:\n    if action == 'order_by':\n        sort = []\n        for key in value:\n            if isinstance(key, string_types) and key.startswith('-'):\n                sort.append({key[1:]: 'desc'})\n            else:\n                sort.append(key)\n    elif action == 'values_list':\n        if not value:\n            list_fields = set()\n        else:\n            list_fields |= set(value)\n        as_list, as_dict = True, False\n    elif action == 'values_dict':\n        if not value:\n            dict_fields = set()\n        else:\n            dict_fields |= set(value)\n        as_list, as_dict = False, True\n    elif action == 'explain':\n        explain = value\n    elif action == 'query':\n        queries.append(value)\n    elif action == 'query_raw':\n        query_raw = value\n    elif action == 'demote':\n        # value here is a tuple of (negative_boost, query)\n        demote = value\n    elif action == 'filter':\n        filters.extend(self._process_filters(value))\n    elif action == 'filter_raw':\n        filters_raw = value\n    elif action == 'facet':\n        # value here is a (args, kwargs) tuple\n        facets.update(_process_facets(*value))\n    elif action == 'facet_raw':\n        facets_raw.update(dict(value))\n    elif action == 'highlight':\n        if value[0] == (None,):\n            highlight_fields = set()\n        else:\n            highlight_fields |= set(value[0])\n        highlight_options.update(value[1])\n    elif action == 'search_type':\n        search_type = value\n    elif action == 'suggest':\n        suggestions[value[0]] = (value[1], value[2])\n    elif action in ('es', 'indexes', 'doctypes', 'boost'):\n        # Ignore these--we use these elsewhere, but want to\n        # make sure lack of handling it here doesn't throw an\n        # error.\n        pass\n    else:\n        raise NotImplementedError(action)\n\nqs = {}\n\n# If there's a filters_raw, we use that.\nif filters_raw:\n    qs['filter'] = filters_raw\nelse:\n    if len(filters) > 1:\n        qs['filter'] = {'and': filters}\n    elif filters:\n        qs['filter'] = filters[0]\n\n# If there's a query_raw, we use that. Otherwise we use\n# whatever we got from query and demote.\nif query_raw:\n    qs['query'] = query_raw\n\nelse:\n    pq = self._process_queries(queries)\n\n    if demote is not None:\n        qs['query'] = {\n            'boosting': {\n                'negative': self._process_queries([demote[1]]),\n                'negative_boost': demote[0]\n                }\n            }\n        if pq:\n            qs['query']['boosting']['positive'] = pq\n\n    elif pq:\n        qs['query'] = pq\n\nif as_list:\n    fields = qs['fields'] = list(list_fields) if list_fields else ['*']\nelif as_dict:\n    fields = qs['fields'] = list(dict_fields) if dict_fields else ['*']\nelse:\n    fields = set()\n\nif facets:\n    qs['facets'] = facets\n    # Hunt for `facet_filter` shells and update those. We use\n    # None as a shell, so if it's explicitly set to None, then\n    # we update it.\n    for facet in facets.values():\n        if facet.get('facet_filter', 1) is None and 'filter' in qs:\n            facet['facet_filter'] = qs['filter']\n\nif facets_raw:\n    qs.setdefault('facets', {}).update(facets_raw)\n\nif sort:\n    qs['sort'] = sort\nif self.start:\n    qs['from'] = self.start\nif self.stop is not None:\n    qs['size'] = self.stop - self.start\n\nif highlight_fields:\n    qs['highlight'] = self._build_highlight(\n        highlight_fields, highlight_options)\n\nif explain:\n    qs['explain'] = True\n\nfor suggestion, (term, kwargs) in six.iteritems(suggestions):\n    qs.setdefault('suggest', {})[suggestion] = {\n        'text': term,\n        'term': {\n            'field': kwargs.get('field', '_all'),\n        },\n    }\n\nself.fields, self.as_list, self.as_dict = fields, as_list, as_dict\nself.search_type = search_type\nreturn qs", "path": "elasticutils\\__init__.py", "repo_name": "mozilla/elasticutils", "stars": 246, "license": "bsd-3-clause", "language": "python", "size": 2550}
{"docstring": "\"\"\"Return obj decorated with es_meta object\"\"\"\n# Create es_meta object with Elasticsearch metadata about this\n# search result\n", "func_signal": "def decorate_with_metadata(obj, result):\n", "code": "obj.es_meta = Metadata(\n    # Elasticsearch id\n    id=result.get('_id', 0),\n    # Source data\n    source=result.get('_source', {}),\n    # The search result score\n    score=result.get('_score', None),\n    # The document type\n    type=result.get('_type', None),\n    # Explanation of score\n    explanation=result.get('_explanation', {}),\n    # Highlight bits\n    highlight=result.get('highlight', {})\n)\n# Put the id on the object for convenience\nobj._id = result.get('_id', 0)\nreturn obj", "path": "elasticutils\\__init__.py", "repo_name": "mozilla/elasticutils", "stars": 246, "license": "bsd-3-clause", "language": "python", "size": 2550}
{"docstring": "\"\"\"Returns the queryset of ids of all things to be indexed.\n\nDefaults to::\n\n    cls.get_model().objects.order_by('id').values_list(\n        'id', flat=True)\n\n:returns: iterable of ids of objects to be indexed\n\n\"\"\"\n", "func_signal": "def get_indexable(cls):\n", "code": "model = cls.get_model()\nreturn model.objects.order_by('id').values_list('id', flat=True)", "path": "elasticutils\\contrib\\django\\__init__.py", "repo_name": "mozilla/elasticutils", "stars": 246, "license": "bsd-3-clause", "language": "python", "size": 2550}
{"docstring": "\"\"\"\nReturns a new S instance with boosting query and demotion.\n\nYou can demote documents that match query criteria::\n\n    q = (S().query(title='trucks')\n            .demote(0.5, description__match='gross'))\n\n    q = (S().query(title='trucks')\n            .demote(0.5, Q(description__match='gross')))\n\nThis is implemented using the boosting query in\nElasticsearch. Anything you specify with ``.query()`` goes\ninto the positive section. The negative query and negative\nboost portions are specified as the first and second arguments\nto ``.demote()``.\n\n.. Note::\n\n   Calling this again will overwrite previous ``.demote()``\n   calls.\n\n\"\"\"\n", "func_signal": "def demote(self, amount_, *queries, **kw):\n", "code": "q = Q()\nfor query in queries:\n    q += query\nq += Q(**kw)\n\nreturn self._clone(next_step=('demote', (amount_, q)))", "path": "elasticutils\\__init__.py", "repo_name": "mozilla/elasticutils", "stars": 246, "license": "bsd-3-clause", "language": "python", "size": 2550}
{"docstring": "\"\"\"\nOR and AND will create a new F, with the filters from both F\nobjects combined with the connector `conn`.\n\"\"\"\n", "func_signal": "def _combine(self, other, conn='and'):\n", "code": "f = F()\n\nself_filters = copy.deepcopy(self.filters)\nother_filters = copy.deepcopy(other.filters)\n\nif not self.filters:\n    f.filters = other_filters\nelif not other.filters:\n    f.filters = self_filters\nelif conn in self.filters[0]:\n    f.filters = self_filters\n    f.filters[0][conn].extend(other_filters)\nelif conn in other.filters[0]:\n    f.filters = other_filters\n    f.filters[0][conn].extend(self_filters)\nelse:\n    f.filters = [{conn: self_filters + other_filters}]\n\nreturn f", "path": "elasticutils\\__init__.py", "repo_name": "mozilla/elasticutils", "stars": 246, "license": "bsd-3-clause", "language": "python", "size": 2550}
{"docstring": "\"\"\"Adds or updates a batch of documents.\n\n:arg documents: List of Python dicts representing individual\n    documents to be added to the index\n\n    .. Note::\n\n       This must be serializable into JSON.\n\n:arg id_field: The name of the field to use as the document\n    id. This defaults to 'id'.\n\n:arg es: The `Elasticsearch` to use. If you don't specify an\n    `Elasticsearch`, it'll use `cls.get_es()`.\n\n:arg index: The name of the index to use. If you don't specify one\n    it'll use `cls.get_index()`.\n\n.. Note::\n\n   If you need the documents available for searches\n   immediately, make sure to refresh the index by calling\n   ``refresh_index()``.\n\n\"\"\"\n", "func_signal": "def bulk_index(cls, documents, id_field='id', es=None, index=None):\n", "code": "if es is None:\n    es = cls.get_es()\n\nif index is None:\n    index = cls.get_index()\n\ndocuments = (dict(d, _id=d[id_field]) for d in documents)\n\nbulk_index(\n    es,\n    documents,\n    index=index,\n    doc_type=cls.get_mapping_type_name(),\n    raise_on_error=True\n)", "path": "elasticutils\\__init__.py", "repo_name": "mozilla/elasticutils", "stars": 246, "license": "bsd-3-clause", "language": "python", "size": 2550}
{"docstring": "\"\"\"Takes a list of queries and returns query clause value\n\n:arg queries: list of Q instances\n\n:returns: dict which is the query clause value\n\n\"\"\"\n# First, let's mush everything into a single Q. Then we can\n# parse that into bits.\n", "func_signal": "def _process_queries(self, queries):\n", "code": "new_q = Q()\n\nfor query in queries:\n    new_q += query\n\n# Now we have a single Q that needs to be processed.\nshould_q = [self._process_query(query) for query in new_q.should_q]\nmust_q = [self._process_query(query) for query in new_q.must_q]\nmust_not_q = [self._process_query(query) for query in new_q.must_not_q]\n\nif len(must_q) > 1 or (len(should_q) + len(must_not_q) > 0):\n    # If there's more than one must_q or there are must_not_q\n    # or should_q, then we need to wrap the whole thing in a\n    # boolean query.\n    bool_query = {}\n    if must_q:\n        bool_query['must'] = must_q\n    if should_q:\n        bool_query['should'] = should_q\n    if must_not_q:\n        bool_query['must_not'] = must_not_q\n    return {'bool': bool_query}\n\nif must_q:\n    # There's only one must_q query and that's it, so we hoist\n    # that.\n    return must_q[0]\n\nreturn {}", "path": "elasticutils\\__init__.py", "repo_name": "mozilla/elasticutils", "stars": 246, "license": "bsd-3-clause", "language": "python", "size": 2550}
{"docstring": "\"\"\"Returns the Elasticsearch object to use.\n\n:arg default_builder: The function that takes a bunch of\n    arguments and generates a elasticsearch Elasticsearch\n    object.\n\n.. Note::\n\n   If you desire special behavior regarding building the\n   Elasticsearch object for this S, subclass S and override\n   this method.\n\n\"\"\"\n# .es() calls are incremental, so we go through them all and\n# update bits that are specified.\n", "func_signal": "def get_es(self, default_builder=get_es):\n", "code": "args = {}\nfor action, value in self.steps:\n    if action == 'es':\n        args.update(**value)\n\n# TODO: store the Elasticsearch on the S if we've already\n# created one since we don't need to do it multiple times.\nreturn default_builder(**args)", "path": "elasticutils\\__init__.py", "repo_name": "mozilla/elasticutils", "stars": 246, "license": "bsd-3-clause", "language": "python", "size": 2550}
{"docstring": "\"\"\"Returns indexes with '_eutest' suffix.\n\n:arg indexes: dict of mapping type name -> index name(s)\n\n:returns: dict with ``_eutest`` appended to all index names\n\n\"\"\"\n", "func_signal": "def testify(indexes):\n", "code": "ret = {}\nfor k, v in indexes.items():\n    if isinstance(v, six.string_types):\n        ret[k] = v + '_eutest'\n    elif isinstance(v, (list, tuple)):\n        ret[k] = [v_item + '_eutest' for v_item in v]\nreturn ret", "path": "elasticutils\\contrib\\django\\estestcase.py", "repo_name": "mozilla/elasticutils", "stars": 246, "license": "bsd-3-clause", "language": "python", "size": 2550}
{"docstring": "# Order the settings by key and then turn it into a string with\n# repr. There are a lot of edge cases here, but the worst that\n# happens is that the key is different and so you get a new\n# Elasticsearch. We'll probably have to tweak this.\n", "func_signal": "def _build_key(urls, timeout, **settings):\n", "code": "settings = sorted(settings.items(), key=lambda item: item[0])\nsettings = repr([(k, v) for k, v in settings])\n\n# elasticsearch allows urls to be a string, so we make sure to\n# account for that when converting whatever it is into a tuple.\nif isinstance(urls, string_types):\n    urls = (urls,)\nelse:\n    urls = tuple(urls)\n\n# Generate a tuple of all the bits and return that as the key\n# because that's hashable.\nkey = (urls, timeout, settings)\nreturn key", "path": "elasticutils\\__init__.py", "repo_name": "mozilla/elasticutils", "stars": 246, "license": "bsd-3-clause", "language": "python", "size": 2550}
{"docstring": "\"\"\"Returns the results class to use\n\nThe results class should be a subclass of SearchResults.\n\n\"\"\"\n", "func_signal": "def get_results_class(self):\n", "code": "if self.as_list:\n    return ListSearchResults\nelif self.as_dict:\n    return DictSearchResults\nelse:\n    return ObjectSearchResults", "path": "elasticutils\\__init__.py", "repo_name": "mozilla/elasticutils", "stars": 246, "license": "bsd-3-clause", "language": "python", "size": 2550}
{"docstring": "\"\"\"Skips the test if this class is skipping tests.\"\"\"\n", "func_signal": "def setUp(self):\n", "code": "if self.skip_tests:\n    return skip_this_test()\nsuper(ESTestCase, self).setUp()", "path": "elasticutils\\contrib\\django\\estestcase.py", "repo_name": "mozilla/elasticutils", "stars": 246, "license": "bsd-3-clause", "language": "python", "size": 2550}
{"docstring": "\"\"\"Skip the test if the Elasticsearch version is less than specified.\n\n:arg minimum_version: string; the minimum Elasticsearch version required\n\n\"\"\"\n\n", "func_signal": "def require_version(minimum_version):\n", "code": "def decorated(test):\n    \"\"\"Decorator to only run the test if ES version is greater or\n    equal than specified.\n\n    \"\"\"\n\n    @wraps(test)\n    def test_with_version(self):\n        \"Only run the test if ES version is not less than specified.\"\n        actual_version = self.get_es().info()['version']['number']\n\n        if LooseVersion(actual_version) >= LooseVersion(minimum_version):\n            test(self)\n        else:\n            raise SkipTest\n\n    return test_with_version\n\nreturn decorated", "path": "elasticutils\\tests\\__init__.py", "repo_name": "mozilla/elasticutils", "stars": 246, "license": "bsd-3-clause", "language": "python", "size": 2550}
{"docstring": "\"\"\"Return explanation in an easier to read format\n\nEasier to read for me, at least.\n\n\"\"\"\n", "func_signal": "def format_explanation(explanation, indent='  ', indent_level=0):\n", "code": "if not explanation:\n    return ''\n\n# Note: This is probably a crap implementation, but it's an\n# interesting starting point for a better formatter.\nline = ('%s%s %2.4f' % ((indent * indent_level),\n                        explanation['description'],\n                        explanation['value']))\n\nif 'details' in explanation:\n    details = '\\n'.join(\n        [format_explanation(subtree, indent, indent_level + 1)\n         for subtree in explanation['details']])\n    return line + '\\n' + details\n\nreturn line", "path": "elasticutils\\utils.py", "repo_name": "mozilla/elasticutils", "stars": 246, "license": "bsd-3-clause", "language": "python", "size": 2550}
{"docstring": "\"\"\"Create an elasticsearch `Elasticsearch` object and return it.\n\nThis will aggressively re-use `Elasticsearch` objects with the\nfollowing rules:\n\n1. if you pass the same argument values to `get_es()`, then it\n   will return the same `Elasticsearch` object\n2. if you pass different argument values to `get_es()`, then it\n   will return different `Elasticsearch` object\n3. it caches each `Elasticsearch` object that gets created\n4. if you pass in `force_new=True`, then you are guaranteed to get\n   a fresh `Elasticsearch` object AND that object will not be\n   cached\n\n:arg urls: list of uris; Elasticsearch hosts to connect to,\n    defaults to ``['http://localhost:9200']``\n:arg timeout: int; the timeout in seconds, defaults to 5\n:arg force_new: Forces get_es() to generate a new Elasticsearch\n    object rather than pulling it from cache.\n:arg settings: other settings to pass into Elasticsearch\n    constructor; See\n    `<http://elasticsearch-py.readthedocs.org/>`_ for more details.\n\nExamples::\n\n    # Returns cached Elasticsearch object\n    es = get_es()\n\n    # Returns a new Elasticsearch object\n    es = get_es(force_new=True)\n\n    es = get_es(urls=['localhost'])\n\n    es = get_es(urls=['localhost:9200'], timeout=10,\n                max_retries=3)\n\n\"\"\"\n# Cheap way of de-None-ifying things\n", "func_signal": "def get_es(urls=None, timeout=DEFAULT_TIMEOUT, force_new=False, **settings):\n", "code": "urls = urls or DEFAULT_URLS\n\n# v0.7: Check for 'hosts' instead of 'urls'. Take this out in v1.0.\nif 'hosts' in settings:\n    raise DeprecationWarning('\"hosts\" is deprecated in favor of \"urls\".')\n\nif not force_new:\n    key = _build_key(urls, timeout, **settings)\n    if key in _cached_elasticsearch:\n        return _cached_elasticsearch[key]\n\nes = Elasticsearch(urls, timeout=timeout, **settings)\n\nif not force_new:\n    # We don't need to rebuild the key here since we built it in\n    # the previous if block, so it's in the namespace. Having said\n    # that, this is a little ew.\n    _cached_elasticsearch[key] = es\n\nreturn es", "path": "elasticutils\\__init__.py", "repo_name": "mozilla/elasticutils", "stars": 246, "license": "bsd-3-clause", "language": "python", "size": 2550}
{"docstring": "\"\"\"Concatenates the consumer key and secret.\"\"\"\n", "func_signal": "def build_signature_base_string(self, oauth_request, consumer, token):\n", "code": "sig = '%s&' % escape(consumer.secret)\nif token:\n    sig = sig + escape(token.secret)\nreturn sig, sig", "path": "contact_importer\\lib\\oauth1.py", "repo_name": "mengu/contact_importer", "stars": 172, "license": "None", "language": "python", "size": 172}
{"docstring": "\"\"\"Serialize as a header for an HTTPAuth request.\"\"\"\n", "func_signal": "def to_header(self, realm=''):\n", "code": "auth_header = 'OAuth realm=\"%s\"' % realm\n# Add the oauth parameters.\nif self.parameters:\n    for k, v in self.parameters.iteritems():\n        if k[:6] == 'oauth_':\n            auth_header += ', %s=\"%s\"' % (k, escape(str(v)))\nreturn {'Authorization': auth_header}", "path": "contact_importer\\lib\\oauth1.py", "repo_name": "mengu/contact_importer", "stars": 172, "license": "None", "language": "python", "size": 172}
{"docstring": "\"\"\"Processes an access_token request and returns the\naccess token on success.\n\"\"\"\n", "func_signal": "def fetch_access_token(self, oauth_request):\n", "code": "version = self._get_version(oauth_request)\nconsumer = self._get_consumer(oauth_request)\ntry:\n    verifier = self._get_verifier(oauth_request)\nexcept OAuthError:\n    verifier = None\n    # Get the request token.\ntoken = self._get_token(oauth_request, 'request')\nself._check_signature(oauth_request, consumer, token)\nnew_token = self.data_store.fetch_access_token(consumer, token, verifier)\nreturn new_token", "path": "contact_importer\\lib\\oauth1.py", "repo_name": "mengu/contact_importer", "stars": 172, "license": "None", "language": "python", "size": 172}
{"docstring": "\"\"\"Parses the URL and rebuilds it to be scheme://host/path.\"\"\"\n", "func_signal": "def get_normalized_http_url(self):\n", "code": "parts = urlparse.urlparse(self.http_url)\nscheme, netloc, path = parts[:3]\n# Exclude default port numbers.\nif scheme == 'http' and netloc[-3:] == ':80':\n    netloc = netloc[:-3]\nelif scheme == 'https' and netloc[-4:] == ':443':\n    netloc = netloc[:-4]\nreturn '%s://%s%s' % (scheme, netloc, path)", "path": "contact_importer\\lib\\oauth1.py", "repo_name": "mengu/contact_importer", "stars": 172, "license": "None", "language": "python", "size": 172}
{"docstring": "\"\"\"Processes a request_token request and returns the\nrequest token on success.\n\"\"\"\n", "func_signal": "def fetch_request_token(self, oauth_request):\n", "code": "try:\n    # Get the request token for authorization.\n    token = self._get_token(oauth_request, 'request')\nexcept OAuthError:\n    # No token required for the initial token request.\n    version = self._get_version(oauth_request)\n    consumer = self._get_consumer(oauth_request)\n    try:\n        callback = self.get_callback(oauth_request)\n    except OAuthError:\n        callback = None # 1.0, no callback specified.\n    self._check_signature(oauth_request, consumer, None)\n    # Fetch a new token.\n    token = self.data_store.fetch_request_token(consumer, callback)\nreturn token", "path": "contact_importer\\lib\\oauth1.py", "repo_name": "mengu/contact_importer", "stars": 172, "license": "None", "language": "python", "size": 172}
{"docstring": "\"\"\"Try to find the token for the provided request token key.\"\"\"\n", "func_signal": "def _get_token(self, oauth_request, token_type='access'):\n", "code": "token_field = oauth_request.get_parameter('oauth_token')\ntoken = self.data_store.lookup_token(token_type, token_field)\nif not token:\n    raise OAuthError('Invalid %s token: %s' % (token_type, token_field))\nreturn token", "path": "contact_importer\\lib\\oauth1.py", "repo_name": "mengu/contact_importer", "stars": 172, "license": "None", "language": "python", "size": 172}
{"docstring": "\"\"\" Returns a token from something like:\noauth_token_secret=xxx&oauth_token=xxx\n\"\"\"\n", "func_signal": "def from_string(s):\n", "code": "params = cgi.parse_qs(s, keep_blank_values=False)\nkey = params['oauth_token'][0]\nsecret = params['oauth_token_secret'][0]\ntoken = OAuthToken(key, secret)\ntry:\n    token.callback_confirmed = params['oauth_callback_confirmed'][0]\nexcept KeyError:\n    pass # 1.0, no callback confirmed.\nreturn token", "path": "contact_importer\\lib\\oauth1.py", "repo_name": "mengu/contact_importer", "stars": 172, "license": "None", "language": "python", "size": 172}
{"docstring": "\"\"\"Figure out the signature with some defaults.\"\"\"\n", "func_signal": "def _get_signature_method(self, oauth_request):\n", "code": "try:\n    signature_method = oauth_request.get_parameter(\n        'oauth_signature_method')\nexcept Exception:\n    signature_method = SIGNATURE_METHOD\ntry:\n    # Get the signature method object.\n    signature_method = self.signature_methods[signature_method]\nexcept KeyError:\n    signature_method_names = ', '.join(self.signature_methods.keys())\n    raise OAuthError('Signature method %s not supported try one of the '\n                     'following: %s' % (signature_method, signature_method_names))\n\nreturn signature_method", "path": "contact_importer\\lib\\oauth1.py", "repo_name": "mengu/contact_importer", "stars": 172, "license": "None", "language": "python", "size": 172}
{"docstring": "\"\"\"Builds the base signature string.\"\"\"\n", "func_signal": "def build_signature(self, oauth_request, consumer, token):\n", "code": "key, raw = self.build_signature_base_string(oauth_request, consumer,\n                                            token)\n\n# HMAC object.\ntry:\n    import hashlib # 2.5\n    hashed = hmac.new(key, raw, hashlib.sha1)\nexcept ImportError:\n    import sha # Deprecated\n    hashed = hmac.new(key, raw, sha)\n\n# Calculate the digest base 64.\nreturn binascii.b2a_base64(hashed.digest())[:-1]", "path": "contact_importer\\lib\\oauth1.py", "repo_name": "mengu/contact_importer", "stars": 172, "license": "None", "language": "python", "size": 172}
{"docstring": "\"\"\"Serialize as post data for a POST request.\"\"\"\n", "func_signal": "def to_postdata(self):\n", "code": "return '&'.join(['%s=%s' % (escape(str(k)), escape(str(v))) \\\n                 for k, v in self.parameters.iteritems()])", "path": "contact_importer\\lib\\oauth1.py", "repo_name": "mengu/contact_importer", "stars": 172, "license": "None", "language": "python", "size": 172}
{"docstring": "\"\"\"Verify that the nonce is uniqueish.\"\"\"\n", "func_signal": "def _check_nonce(self, consumer, token, nonce):\n", "code": "nonce = self.data_store.lookup_nonce(consumer, token, nonce)\nif nonce:\n    raise OAuthError('Nonce already used: %s' % str(nonce))", "path": "contact_importer\\lib\\oauth1.py", "repo_name": "mengu/contact_importer", "stars": 172, "license": "None", "language": "python", "size": 172}
{"docstring": "\"\"\"Get any non-OAuth parameters.\"\"\"\n", "func_signal": "def get_nonoauth_parameters(self):\n", "code": "parameters = {}\nfor k, v in self.parameters.iteritems():\n    # Ignore oauth parameters.\n    if k.find('oauth_') < 0:\n        parameters[k] = v\nreturn parameters", "path": "contact_importer\\lib\\oauth1.py", "repo_name": "mengu/contact_importer", "stars": 172, "license": "None", "language": "python", "size": 172}
{"docstring": "\"\"\"Verifies an api call and checks all the parameters.\"\"\"\n# -> consumer and token\n", "func_signal": "def verify_request(self, oauth_request):\n", "code": "version = self._get_version(oauth_request)\nconsumer = self._get_consumer(oauth_request)\n# Get the access token.\ntoken = self._get_token(oauth_request, 'access')\nself._check_signature(oauth_request, consumer, token)\nparameters = oauth_request.get_nonoauth_parameters()\nreturn consumer, token, parameters", "path": "contact_importer\\lib\\oauth1.py", "repo_name": "mengu/contact_importer", "stars": 172, "license": "None", "language": "python", "size": 172}
{"docstring": "\"\"\"Turn URL string into parameters.\"\"\"\n", "func_signal": "def _split_url_string(param_str):\n", "code": "parameters = cgi.parse_qs(param_str, keep_blank_values=False)\nfor k, v in parameters.iteritems():\n    parameters[k] = urllib.unquote(v[0])\nreturn parameters", "path": "contact_importer\\lib\\oauth1.py", "repo_name": "mengu/contact_importer", "stars": 172, "license": "None", "language": "python", "size": 172}
{"docstring": "\"\"\"Return a string that contains the parameters that must be signed.\"\"\"\n", "func_signal": "def get_normalized_parameters(self):\n", "code": "params = self.parameters\ntry:\n    # Exclude the signature if it exists.\n    del params['oauth_signature']\nexcept KeyError:\n    pass\n    # Escape key values before sorting.\nkey_values = [(escape(_utf8_str(k)), escape(_utf8_str(v))) \\\n              for k,v in params.items()]\n# Sort lexicographically, first after key, then after value.\nkey_values.sort()\n# Combine key value pairs into a string.\nreturn '&'.join(['%s=%s' % (k, v) for k, v in key_values])", "path": "contact_importer\\lib\\oauth1.py", "repo_name": "mengu/contact_importer", "stars": 172, "license": "None", "language": "python", "size": 172}
{"docstring": "\"\"\"Verify that timestamp is recentish.\"\"\"\n", "func_signal": "def _check_timestamp(self, timestamp):\n", "code": "timestamp = int(timestamp)\nnow = int(time.time())\nlapsed = abs(now - timestamp)\nif lapsed > self.timestamp_threshold:\n    raise OAuthError('Expired timestamp: given %d and now %s has a '\n                     'greater difference than threshold %d' %\n                     (timestamp, now, self.timestamp_threshold))", "path": "contact_importer\\lib\\oauth1.py", "repo_name": "mengu/contact_importer", "stars": 172, "license": "None", "language": "python", "size": 172}
{"docstring": "\"\"\"Turn Authorization: header into parameters.\"\"\"\n", "func_signal": "def _split_header(header):\n", "code": "params = {}\nparts = header.split(',')\nfor param in parts:\n    # Ignore realm parameter.\n    if param.find('realm') > -1:\n        continue\n        # Remove whitespace.\n    param = param.strip()\n    # Split key-value.\n    param_parts = param.split('=', 1)\n    # Remove quotes and unescape the value.\n    params[param_parts[0]] = urllib.unquote(param_parts[1].strip('\\\"'))\nreturn params", "path": "contact_importer\\lib\\oauth1.py", "repo_name": "mengu/contact_importer", "stars": 172, "license": "None", "language": "python", "size": 172}
{"docstring": "\"\"\"Convert unicode to utf-8.\"\"\"\n", "func_signal": "def _utf8_str(s):\n", "code": "if isinstance(s, unicode):\n    return s.encode(\"utf-8\")\nelse:\n    return str(s)", "path": "contact_importer\\lib\\oauth1.py", "repo_name": "mengu/contact_importer", "stars": 172, "license": "None", "language": "python", "size": 172}
{"docstring": "\"\"\"Set the signature parameter to the result of build_signature.\"\"\"\n# Set the signature method.\n", "func_signal": "def sign_request(self, signature_method, consumer, token):\n", "code": "self.set_parameter('oauth_signature_method',\n                   signature_method.get_name())\n# Set the signature.\nself.set_parameter('oauth_signature',\n                   self.build_signature(signature_method, consumer, token))", "path": "contact_importer\\lib\\oauth1.py", "repo_name": "mengu/contact_importer", "stars": 172, "license": "None", "language": "python", "size": 172}
{"docstring": "\"\"\"Verify the correct version request for this server.\"\"\"\n", "func_signal": "def _get_version(self, oauth_request):\n", "code": "try:\n    version = oauth_request.get_parameter('oauth_version')\nexcept Exception:\n    version = VERSION\nif version and version != self.version:\n    raise OAuthError('OAuth version %s not supported.' % str(version))\nreturn version", "path": "contact_importer\\lib\\oauth1.py", "repo_name": "mengu/contact_importer", "stars": 172, "license": "None", "language": "python", "size": 172}
{"docstring": "\"\"\"\n  \u5904\u7406update\u4e8b\u4ef6\uff0c\u53cd\u5411\u751f\u6210insert\u8bed\u53e5\uff0c\u4fdd\u7559\u73b0\u573a\u6570\u636e\u5230\u6587\u4ef6\n:param event: mysql\u7684\u4e8b\u4ef6\n:param file_dict: \u6587\u4ef6\u63cf\u8ff0\u7b26\uff0c\u5305\u62ec\uff1a\u53cd\u5411sql\u6587\u4ef6,\u4fdd\u7559\u73b0\u573a\u6570\u636e\u6587\u4ef6\n:param stat:mysqlbinlog\u5f62\u6210\u7684\u7edf\u8ba1\u7ed3\u679c\n:return:\n\"\"\"\n#logger.debug(\"===logfile {0}\".format(logfile))\n", "func_signal": "def deal_update_rows(event,file_dict,stat,keep_data=False,keep_current=False,logfile=None,add_schema_name=False):\n", "code": "deal_common_event(event,file_dict,logfile)\nfor row in event.rows:\n    sql=joint_update_sql(event.schema,event.table,event.primary_key,row,Constant.UPDATE_PK,add_schema_name)\n    sql_keep=joint_keep_data_sql(\"update\",event.timestamp,event.table,row)\n    write_file(file_dict[\"flashback\"],sql)\n    write_file(file_dict[\"data\"],sql_keep)\n    add_stat(stat,\"update\",event.schema,event.table)", "path": "flashback.py", "repo_name": "58daojia-dba/mysqlbinlog_flashback", "stars": 190, "license": "apache-2.0", "language": "python", "size": 299}
{"docstring": "\"\"\"\n  \u5904\u7406delete\u4e8b\u4ef6\uff0c\u53cd\u5411\u751f\u6210insert\u8bed\u53e5\uff0c\u4fdd\u7559\u73b0\u573a\u6570\u636e\u5230\u6587\u4ef6\n:param event: mysql\u7684\u4e8b\u4ef6\n:param file_dict: \u6587\u4ef6\u63cf\u8ff0\u7b26\uff0c\u5305\u62ec\uff1a\u53cd\u5411sql\u6587\u4ef6,\u4fdd\u7559\u73b0\u573a\u6570\u636e\u6587\u4ef6\n:param stat:mysqlbinlog\u5f62\u6210\u7684\u7edf\u8ba1\u7ed3\u679c\n:return:\n\"\"\"\n", "func_signal": "def deal_insert_rows(event,file_dict,stat,keep_data=False,keep_current=False,logfile=None,add_schema_name=False):\n", "code": "deal_common_event(event,file_dict,logfile)\nfor row in event.rows:\n    sql=joint_delete_sql(event.schema,event.table,event.primary_key,row,add_schema_name)\n    sql_keep=joint_keep_data_sql(\"insert\",event.timestamp,event.table,row)\n    write_file(file_dict[\"flashback\"],sql)\n    write_file(file_dict[\"data\"],sql_keep)\n    add_stat(stat,\"delete\",event.schema,event.table)", "path": "flashback.py", "repo_name": "58daojia-dba/mysqlbinlog_flashback", "stars": 190, "license": "apache-2.0", "language": "python", "size": 299}
{"docstring": "\"\"\"\n  \u7edf\u8ba1\u751f\u6210\u4e86\u591a\u5c11sql\n:param stat:\u683c\u5f0f\u4e3astat[\"flash_sql\"][\"schema\"][\"table\"][\"\u64cd\u4f5c\u7c7b\u578b\"] \uff1a\n:param op_type: update/delete/insert\n:param schema:\n:param table:\n:return:\n\"\"\"\n", "func_signal": "def add_stat(stat,op_type,schema,table):\n", "code": "if not stat[\"flash_sql\"].has_key(schema):\n   stat[\"flash_sql\"][schema]={}\nif not stat[\"flash_sql\"][schema].has_key(table):\n    stat[\"flash_sql\"][schema][table]={}\n    stat[\"flash_sql\"][schema][table][\"update\"]=0\n    stat[\"flash_sql\"][schema][table][\"insert\"]=0\n    stat[\"flash_sql\"][schema][table][\"delete\"]=0\nstat[\"flash_sql\"][schema][table][op_type]+=1", "path": "flashback.py", "repo_name": "58daojia-dba/mysqlbinlog_flashback", "stars": 190, "license": "apache-2.0", "language": "python", "size": 299}
{"docstring": "\"\"\"Read MySQL BIT type\"\"\"\n", "func_signal": "def __read_bit(self, column):\n", "code": "resp = \"\"\nfor byte in range(0, column.bytes):\n    current_byte = \"\"\n    data = self.packet.read_uint8()\n    if byte == 0:\n        if column.bytes == 1:\n            end = column.bits\n        else:\n            end = column.bits % 8\n            if end == 0:\n                end = 8\n    else:\n        end = 8\n    for bit in range(0, end):\n        if data & (1 << bit):\n            current_byte += \"1\"\n        else:\n            current_byte += \"0\"\n    resp += current_byte[::-1]\nreturn resp", "path": "pymysqlreplication\\row_event.py", "repo_name": "58daojia-dba/mysqlbinlog_flashback", "stars": 190, "license": "apache-2.0", "language": "python", "size": 299}
{"docstring": "\"\"\"\n  \u5904\u7406\u4e8b\u4ef6\u7684\u516c\u5171\u51fd\u6570\uff0c\u5199\u65f6\u95f4\u6233\uff0c\u4f4d\u7f6e\u7b49\u516c\u5171\u4fe1\u606f\n:param event: mysql\u7684\u4e8b\u4ef6\n:param file_dict: \u6587\u4ef6\u63cf\u8ff0\u7b26\uff0c\u5305\u62ec\uff1a\u53cd\u5411sql\u6587\u4ef6,\u4fdd\u7559\u73b0\u573a\u6570\u636e\u6587\u4ef6\n:return:\n\"\"\"\n\n", "func_signal": "def deal_common_event(event,file_dict,logfile=None):\n", "code": "content=u\"#end_log_pos {0}  {1} {2} {3}\".format(event.packet.log_pos,\n                                        datetime.fromtimestamp(event.timestamp).isoformat(),event.timestamp,logfile)\nwrite_file(file_dict[\"flashback\"],content)\nwrite_file(file_dict[\"data\"],content)", "path": "flashback.py", "repo_name": "58daojia-dba/mysqlbinlog_flashback", "stars": 190, "license": "apache-2.0", "language": "python", "size": 299}
{"docstring": "\"\"\"set sql_mode to test with same sql_mode (mysql 5.7 sql_mode default is changed)\"\"\"\n", "func_signal": "def set_sql_mode(self):\n", "code": "version = float(self.getMySQLVersion().rsplit('.', 1)[0])\nif version == 5.7:\n    self.execute(\"set @@sql_mode='NO_ENGINE_SUBSTITUTION'\")", "path": "pymysqlreplication\\tests\\base.py", "repo_name": "58daojia-dba/mysqlbinlog_flashback", "stars": 190, "license": "apache-2.0", "language": "python", "size": 299}
{"docstring": "\"\"\"DATETIME\n\n1 bit  sign           (1= non-negative, 0= negative)\n17 bits year*13+month  (year 0-9999, month 0-12)\n 5 bits day            (0-31)\n 5 bits hour           (0-23)\n 6 bits minute         (0-59)\n 6 bits second         (0-59)\n---------------------------\n40 bits = 5 bytes\n\"\"\"\n", "func_signal": "def __read_datetime2(self, column):\n", "code": "data = self.packet.read_int_be_by_size(5)\nyear_month = self.__read_binary_slice(data, 1, 17, 40)\ntry:\n    t = datetime.datetime(\n        year=int(year_month / 13),\n        month=year_month % 13,\n        day=self.__read_binary_slice(data, 18, 5, 40),\n        hour=self.__read_binary_slice(data, 23, 5, 40),\n        minute=self.__read_binary_slice(data, 28, 6, 40),\n        second=self.__read_binary_slice(data, 34, 6, 40))\nexcept ValueError:\n    return None\nreturn self.__add_fsp_to_time(t, column)", "path": "pymysqlreplication\\row_event.py", "repo_name": "58daojia-dba/mysqlbinlog_flashback", "stars": 190, "license": "apache-2.0", "language": "python", "size": 299}
{"docstring": "# log_pos (4) -- position in the binlog-file to start the stream with\n# flags (2) BINLOG_DUMP_NON_BLOCK (0 or 1)\n# server_id (4) -- server id of this slave\n# log_file (string.EOF) -- filename of the binlog on the master\n", "func_signal": "def __connect_to_stream(self):\n", "code": "self._stream_connection = self.pymysql_wrapper(**self.__connection_settings)\n\nself.__use_checksum = self.__checksum_enabled()\n\n# If checksum is enabled we need to inform the server about the that\n# we support it\nif self.__use_checksum:\n    cur = self._stream_connection.cursor()\n    cur.execute(\"set @master_binlog_checksum= @@global.binlog_checksum\")\n    cur.close()\n\nif self.slave_uuid:\n    cur = self._stream_connection.cursor()\n    cur.execute(\"set @slave_uuid= '%s'\" % self.slave_uuid)\n    cur.close()\n\nself._register_slave()\n\nif not self.auto_position:\n    # only when log_file and log_pos both provided, the position info is\n    # valid, if not, get the current position from master\n    if self.log_file is None or self.log_pos is None:\n        cur = self._stream_connection.cursor()\n        cur.execute(\"SHOW MASTER STATUS\")\n        self.log_file, self.log_pos = cur.fetchone()[:2]\n        cur.close()\n\n    prelude = struct.pack('<i', len(self.log_file) + 11) \\\n        + int2byte(COM_BINLOG_DUMP)\n\n    if self.__resume_stream:\n        prelude += struct.pack('<I', self.log_pos)\n    else:\n        prelude += struct.pack('<I', 4)\n\n    if self.__blocking:\n        prelude += struct.pack('<h', 0)\n    else:\n        prelude += struct.pack('<h', 1)\n\n    prelude += struct.pack('<I', self.__server_id)\n    prelude += self.log_file.encode()\nelse:\n    # Format for mysql packet master_auto_position\n    #\n    # All fields are little endian\n    # All fields are unsigned\n\n    # Packet length   uint   4bytes\n    # Packet type     byte   1byte   == 0x1e\n    # Binlog flags    ushort 2bytes  == 0 (for retrocompatibilty)\n    # Server id       uint   4bytes\n    # binlognamesize  uint   4bytes\n    # binlogname      str    Nbytes  N = binlognamesize\n    #                                Zeroified\n    # binlog position uint   4bytes  == 4\n    # payload_size    uint   4bytes\n\n    # What come next, is the payload, where the slave gtid_executed\n    # is sent to the master\n    # n_sid           ulong  8bytes  == which size is the gtid_set\n    # | sid           uuid   16bytes UUID as a binary\n    # | n_intervals   ulong  8bytes  == how many intervals are sent for this gtid\n    # | | start       ulong  8bytes  Start position of this interval\n    # | | stop        ulong  8bytes  Stop position of this interval\n\n    # A gtid set looks like:\n    #   19d69c1e-ae97-4b8c-a1ef-9e12ba966457:1-3:8-10,\n    #   1c2aad49-ae92-409a-b4df-d05a03e4702e:42-47:80-100:130-140\n    #\n    # In this particular gtid set, 19d69c1e-ae97-4b8c-a1ef-9e12ba966457:1-3:8-10\n    # is the first member of the set, it is called a gtid.\n    # In this gtid, 19d69c1e-ae97-4b8c-a1ef-9e12ba966457 is the sid\n    # and have two intervals, 1-3 and 8-10, 1 is the start position of the first interval\n    # 3 is the stop position of the first interval.\n\n    gtid_set = GtidSet(self.auto_position)\n    encoded_data_size = gtid_set.encoded_length\n\n    header_size = (2 +  # binlog_flags\n                   4 +  # server_id\n                   4 +  # binlog_name_info_size\n                   4 +  # empty binlog name\n                   8 +  # binlog_pos_info_size\n                   4)  # encoded_data_size\n\n    prelude = b'' + struct.pack('<i', header_size + encoded_data_size) \\\n        + int2byte(COM_BINLOG_DUMP_GTID)\n\n    # binlog_flags = 0 (2 bytes)\n    prelude += struct.pack('<H', 0)\n    # server_id (4 bytes)\n    prelude += struct.pack('<I', self.__server_id)\n    # binlog_name_info_size (4 bytes)\n    prelude += struct.pack('<I', 3)\n    # empty_binlog_name (4 bytes)\n    prelude += b'\\0\\0\\0'\n    # binlog_pos_info (8 bytes)\n    prelude += struct.pack('<Q', 4)\n\n    # encoded_data_size (4 bytes)\n    prelude += struct.pack('<I', gtid_set.encoded_length)\n    # encoded_data\n    prelude += gtid_set.encoded()\n#pdb.set_trace()\nif pymysql.__version__ < \"0.6\":\n    self._stream_connection.wfile.write(prelude)\n    self._stream_connection.wfile.flush()\nelse:\n    self._stream_connection._write_bytes(prelude)\n    self._stream_connection._next_seq_id = 1\nself.__connected_stream = True", "path": "pymysqlreplication\\binlogstream.py", "repo_name": "58daojia-dba/mysqlbinlog_flashback", "stars": 190, "license": "apache-2.0", "language": "python", "size": 299}
{"docstring": "\"\"\"\n  \u5904\u7406delete\u4e8b\u4ef6\uff0c\u53cd\u5411\u751f\u6210insert\u8bed\u53e5\uff0c\u4fdd\u7559\u73b0\u573a\u6570\u636e\u5230\u6587\u4ef6\n:param event: mysql\u7684\u4e8b\u4ef6\n:param file_dict: \u6587\u4ef6\u63cf\u8ff0\u7b26\uff0c\u5305\u62ec\uff1a\u53cd\u5411sql\u6587\u4ef6,\u4fdd\u7559\u73b0\u573a\u6570\u636e\u6587\u4ef6\n:param stat:mysqlbinlog\u5f62\u6210\u7684\u7edf\u8ba1\u7ed3\u679c\n:return:\n\"\"\"\n", "func_signal": "def deal_delete_rows(event,file_dict,stat,keep_data=False,keep_current=False,logfile=None,add_schema_name=False):\n", "code": "deal_common_event(event,file_dict,logfile)\nfor row in event.rows:\n    sql=joint_insert_sql(event.schema,event.table,event.primary_key,row,add_schema_name)\n    sql_keep=joint_keep_data_sql(\"delete\",event.timestamp,event.table,row)\n    write_file(file_dict[\"flashback\"],sql)\n    write_file(file_dict[\"data\"],sql_keep)\n    add_stat(stat,\"insert\",event.schema,event.table)", "path": "flashback.py", "repo_name": "58daojia-dba/mysqlbinlog_flashback", "stars": 190, "license": "apache-2.0", "language": "python", "size": 299}
{"docstring": "\"\"\"\n\u5199\u6587\u4ef6\n:param file:\n:param content:\n:return:\n\"\"\"\n", "func_signal": "def write_file(file,content):\n", "code": "file.write(content)\nfile.write(\";\\n\")", "path": "flashback.py", "repo_name": "58daojia-dba/mysqlbinlog_flashback", "stars": 190, "license": "apache-2.0", "language": "python", "size": 299}
{"docstring": "\"\"\"Read a 'Length Coded String' from the data buffer.\n\nA 'Length Coded String' consists first of a length coded\n(unsigned, positive) integer represented in 1-9 bytes followed by\nthat many bytes of binary data.  (For example \"cat\" would be \"3cat\".)\n\nFrom PyMYSQL source code\n\"\"\"\n", "func_signal": "def read_length_coded_string(self):\n", "code": "length = self.read_length_coded_binary()\nif length is None:\n    return None\nreturn self.read(length).decode()", "path": "pymysqlreplication\\packet.py", "repo_name": "58daojia-dba/mysqlbinlog_flashback", "stars": 190, "license": "apache-2.0", "language": "python", "size": 299}
{"docstring": "\"\"\"TIME encoding for nonfractional part:\n\n 1 bit sign    (1= non-negative, 0= negative)\n 1 bit unused  (reserved for future extensions)\n10 bits hour   (0-838)\n 6 bits minute (0-59)\n 6 bits second (0-59)\n---------------------\n24 bits = 3 bytes\n\"\"\"\n", "func_signal": "def __read_time2(self, column):\n", "code": "data = self.packet.read_int_be_by_size(3)\n\nsign = 1 if self.__read_binary_slice(data, 0, 1, 24) else -1\nif sign == -1:\n    # negative integers are stored as 2's compliment \n    # hence take 2's compliment again to get the right value.\n    data = ~data + 1\n\nt = datetime.timedelta(\n    hours=sign*self.__read_binary_slice(data, 2, 10, 24),\n    minutes=self.__read_binary_slice(data, 12, 6, 24),\n    seconds=self.__read_binary_slice(data, 18, 6, 24),\n    microseconds=self.__read_fsp(column)\n)\nreturn t", "path": "pymysqlreplication\\row_event.py", "repo_name": "58daojia-dba/mysqlbinlog_flashback", "stars": 190, "license": "apache-2.0", "language": "python", "size": 299}
{"docstring": "\"\"\"\n  \u5904\u7406xid\u4e8b\u4ef6\uff0c\u4ec5\u4ec5\u5199commit\u8bed\u53e5\u5230\u6587\u4ef6\u4e2d\n:param event: mysql\u7684\u4e8b\u4ef6\n:param file_dict: \u6587\u4ef6\u63cf\u8ff0\u7b26\uff0c\u5305\u62ec\uff1a\u53cd\u5411sql\u6587\u4ef6,\u4fdd\u7559\u73b0\u573a\u6570\u636e\u6587\u4ef6\n:param stat:mysqlbinlog\u5f62\u6210\u7684\u7edf\u8ba1\u7ed3\u679c\n:return:\n\"\"\"\n", "func_signal": "def deal_xid(event,file_dict,stat,keep_data=False,keep_current=False):\n", "code": "content=\"commit\"\ndeal_common_event(event,file_dict)\nlogger.debug(u\"xid event content\")\nstat[\"commit\"] += 1\nwrite_file(file_dict[\"flashback\"],content)\nwrite_file(file_dict[\"data\"],content)", "path": "flashback.py", "repo_name": "58daojia-dba/mysqlbinlog_flashback", "stars": 190, "license": "apache-2.0", "language": "python", "size": 299}
{"docstring": "\"\"\"\nAttributes:\n    value: string or tuple\n           if string, then it will be used hostname\n           if tuple it will be used as (hostname, user, password, port)\n\"\"\"\n\n", "func_signal": "def __init__(self, value):\n", "code": "if isinstance(value, (tuple, list)):\n    try:\n        self.hostname = value[0]\n        self.username = value[1]\n        self.password = value[2]\n        self.port = int(value[3])\n    except IndexError:\n        pass\nelif isinstance(value, dict):\n    for key in ['hostname', 'username', 'password', 'port']:\n        try:\n            setattr(self, key, value[key])\n        except KeyError:\n            pass\nelse:\n    self.hostname = value", "path": "pymysqlreplication\\binlogstream.py", "repo_name": "58daojia-dba/mysqlbinlog_flashback", "stars": 190, "license": "apache-2.0", "language": "python", "size": 299}
{"docstring": "'''Push again data in data buffer. It's use when you want\nto extract a bit from a value a let the rest of the code normally\nread the datas'''\n", "func_signal": "def unread(self, data):\n", "code": "self.read_bytes -= len(data)\nself.__data_buffer += data", "path": "pymysqlreplication\\packet.py", "repo_name": "58daojia-dba/mysqlbinlog_flashback", "stars": 190, "license": "apache-2.0", "language": "python", "size": 299}
{"docstring": "\"\"\"Read and add the fractional part of time\nFor more details about new date format:\nhttp://dev.mysql.com/doc/internals/en/date-and-time-data-type-representation.html\n\"\"\"\n", "func_signal": "def __add_fsp_to_time(self, time, column):\n", "code": "microsecond = self.__read_fsp(column)\nif microsecond > 0:\n    time = time.replace(microsecond=microsecond)\nreturn time", "path": "pymysqlreplication\\row_event.py", "repo_name": "58daojia-dba/mysqlbinlog_flashback", "stars": 190, "license": "apache-2.0", "language": "python", "size": 299}
{"docstring": "\"\"\"Read MySQL's new decimal format introduced in MySQL 5\"\"\"\n\n# This project was a great source of inspiration for\n# understanding this storage format.\n# https://github.com/jeremycole/mysql_binlog\n\n", "func_signal": "def __read_new_decimal(self, column):\n", "code": "digits_per_integer = 9\ncompressed_bytes = [0, 1, 1, 2, 2, 3, 3, 4, 4, 4]\nintegral = (column.precision - column.decimals)\nuncomp_integral = int(integral / digits_per_integer)\nuncomp_fractional = int(column.decimals / digits_per_integer)\ncomp_integral = integral - (uncomp_integral * digits_per_integer)\ncomp_fractional = column.decimals - (uncomp_fractional\n                                     * digits_per_integer)\n\n# Support negative\n# The sign is encoded in the high bit of the the byte\n# But this bit can also be used in the value\nvalue = self.packet.read_uint8()\nif value & 0x80 != 0:\n    res = \"\"\n    mask = 0\nelse:\n    mask = -1\n    res = \"-\"\nself.packet.unread(struct.pack('<B', value ^ 0x80))\n\nsize = compressed_bytes[comp_integral]\nif size > 0:\n    value = self.packet.read_int_be_by_size(size) ^ mask\n    res += str(value)\n\nfor i in range(0, uncomp_integral):\n    value = struct.unpack('>i', self.packet.read(4))[0] ^ mask\n    res += '%09d' % value\n\nres += \".\"\n\nfor i in range(0, uncomp_fractional):\n    value = struct.unpack('>i', self.packet.read(4))[0] ^ mask\n    res += '%09d' % value\n\nsize = compressed_bytes[comp_fractional]\nif size > 0:\n    value = self.packet.read_int_be_by_size(size) ^ mask\n    res += '%0*d' % (comp_fractional, value)\n\nreturn decimal.Decimal(res)", "path": "pymysqlreplication\\row_event.py", "repo_name": "58daojia-dba/mysqlbinlog_flashback", "stars": 190, "license": "apache-2.0", "language": "python", "size": 299}
{"docstring": "\"\"\"Return True if binlog-checksum = CRC32. Only for MySQL > 5.6\"\"\"\n", "func_signal": "def __checksum_enabled(self):\n", "code": "cur = self._stream_connection.cursor()\ncur.execute(\"SHOW GLOBAL VARIABLES LIKE 'BINLOG_CHECKSUM'\")\nresult = cur.fetchone()\ncur.close()\n\nif result is None:\n    return False\nvar, value = result[:2]\nif value == 'NONE':\n    return False\nreturn True", "path": "pymysqlreplication\\binlogstream.py", "repo_name": "58daojia-dba/mysqlbinlog_flashback", "stars": 190, "license": "apache-2.0", "language": "python", "size": 299}
{"docstring": "\"\"\"\n \u8bbf\u95ee\u6570\u636e\u5e93\u8bbe\u7f6e\u7f3a\u7701\u503cstart_binlog_file\uff0cend_to_timestamp\n\"\"\"\n", "func_signal": "def set_defaut(self):\n", "code": "if self.start_binlog_file is None:\n    mysql_table=MysqlTable()\n    mysql_table.connect(self.mysql_setting)\n    log_name=mysql_table.get_last_binary_log_name()\n    self.start_binlog_file=log_name\n\nif self.end_to_timestamp is None:\n    mysql_table=MysqlTable()\n    mysql_table.connect(self.mysql_setting)\n    current_dt=mysql_table.get_current_datetime()\n    current_dt=current_dt - timedelta(minutes = Constant.TIMEDELTA_CURRENT_TIME)\n    self.end_to_timestamp=convert_datetime_to_timestamp(current_dt)", "path": "flashback.py", "repo_name": "58daojia-dba/mysqlbinlog_flashback", "stars": 190, "license": "apache-2.0", "language": "python", "size": 299}
{"docstring": "\"\"\"\n \u5f97\u5230\u6587\u4ef6\u540d\n:param :\n:return:\n\"\"\"\n", "func_signal": "def get_file_name(self,filename_type,current_time=None):\n", "code": "if current_time is None:\n    current_time=datetime.strftime( datetime.now(), '%Y%m%d_%H%M%S')\nfilename=\"{0}/{1}_{2}_{3}.sql\".format(self.output_file_path,filename_type,self.schema,current_time)\nreturn filename", "path": "flashback.py", "repo_name": "58daojia-dba/mysqlbinlog_flashback", "stars": 190, "license": "apache-2.0", "language": "python", "size": 299}
{"docstring": "'''\nCreate a list of all dependency hierarchy nodes in order of generation.\n'''\n", "func_signal": "def generationList(self):\n", "code": "generationList = []\ngenerationDict = self.generationDict()\ngenerationDictKeys = generationDict.keys()\ngenerationDictKeys.sort()\nfor key in generationDictKeys: generationList.extend(generationDict[key])\nreturn generationList", "path": "tools\\dependencyHierarchyNode.py", "repo_name": "bungnoid/glTools", "stars": 195, "license": "mit", "language": "python", "size": 868}
{"docstring": "'''\nReturn the dependency depth (generation) for the current node\n'''\n", "func_signal": "def getGeneration(self):\n", "code": "generation = 0\nparent = self.parent\nwhile parent:\n\tgeneration += 1\n\tparent = parent.parent\nreturn generation", "path": "tools\\dependencyHierarchyNode.py", "repo_name": "bungnoid/glTools", "stars": 195, "license": "mit", "language": "python", "size": 868}
{"docstring": "'''\nReturn a dependency node name list from the current hierarchy\n@param longNames: Return a list of long object names\n@type longNames: bool\n'''\n", "func_signal": "def flatList(self,longNames=False):\n", "code": "flatList = []\nif longNames:\tflatList.append(self.fullName)\nelse:\t\tflatList.append(self.shortName)\nflatList.extend(self.listChildren(longNames=longNames,recursive=True))\nreturn flatList", "path": "tools\\dependencyHierarchyNode.py", "repo_name": "bungnoid/glTools", "stars": 195, "license": "mit", "language": "python", "size": 868}
{"docstring": "'''\nReturn a dependency node list from the current hierarchy\n'''\n", "func_signal": "def flatListNodes(self):\n", "code": "nodeList = []\nnodeList.append(self)\nnodeList.extend(self.listChildNodes(recursive=True))\nreturn nodeList", "path": "tools\\dependencyHierarchyNode.py", "repo_name": "bungnoid/glTools", "stars": 195, "license": "mit", "language": "python", "size": 868}
{"docstring": "'''\nExecute replaceGeometry from the UI\n'''\n# Get selection\n", "func_signal": "def replaceGeometryFromUI():\n", "code": "sel = mc.ls(sl=True)\nif len(sel) != 2:\n\tprint('Incorrect selection! First select the replacement geometry and then the geometry to be replaced!!')\n\treturn\n\n# Replace geometry\nglTools.utils.geometry.replace(sel[0],sel[1])", "path": "ui\\replaceGeometry.py", "repo_name": "bungnoid/glTools", "stars": 195, "license": "mit", "language": "python", "size": 868}
{"docstring": "'''\nReturn the portion of name minus the last element separated by the name delineator.\nUseful for determining a name prefix from an existing object name.\n@param name: String name to strip the suffix from\n@type name: str\n@param delineator: String delineator to split the string name with. If default will inherit the class delineator string.\n@type delineator: str\n'''\n# Determine string delineator\n", "func_signal": "def stripSuffix(self,name,delineator=''):\n", "code": "if not delineator: delineator = self.delineator\n# Check for delineator in name\nif not name.count(delineator): return name\n# Determine name suffix\nsuffix = name.split(self.delineator)[-1]\n# Remove suffix\nnewName = name.replace(self.delineator+suffix,'')\n# Return result\nreturn newName", "path": "tools\\namingConvention.py", "repo_name": "bungnoid/glTools", "stars": 195, "license": "mit", "language": "python", "size": 868}
{"docstring": "'''\nThis generates the menu for all modelTools.\n'''\n# This is a temporary hack to get maya to evaluate $gMainWindow\n", "func_signal": "def create():\n", "code": "gMainWindow = mm.eval('string $temp = $gMainWindow')\nif mc.menu('modelToolsMenu',q=True,ex=True): mc.deleteUI('modelToolsMenu')\n\nif gMainWindow:\n\t\n\tmc.setParent(gMainWindow)\n\tmc.menu('modelToolsMenu', label='Model Tools', tearOff=True, allowOptionBoxes=True)\n\t\n\t#----------------------------------------#\n\t\n\tmc.menuItem(label='Checks', subMenu=True, tearOff=True)\n\t\n\tmc.menuItem(label='Run Checks', command='import glTools.spotcheck.runChecks;reload(glTools.spotcheck.runChecks);glTools.spotcheck.runChecks.run(envKey=\"IKA_MODEL_SPOTCHECKS\",checkTitle=\"Rig Checks\",selectedNodes=False)')\n\tmc.menuItem(label='Run Checks On Selected', command='import glTools.spotcheck.runChecks;reload(glTools.spotcheck.runChecks);glTools.spotcheck.runChecks.run(envKey=\"IKA_MODEL_SPOTCHECKS\",checkTitle=\"Rig Checks\",selectedNodes=True)')\n\t\n\tmc.setParent('..', menu=True)\n\t\n\t#----------------------------------------#\n\t\n\tmc.menuItem(divider=True)\n\t\n\t# GENERAL\n\tmc.menuItem(allowOptionBoxes=True, label='General', subMenu=True, tearOff=True)\n\t\n\tmc.menuItem(label='Align to Ground Plane', command='import glTools.model.utils;reload(glTools.model.utils);[glTools.model.utils.setVerticalPlacement(i) for i in mc.ls(sl=True)]')\n\tmc.menuItem(label='Build Poly Edge Curves', command='import glTools.tools.polyEdgeCurve;reload(glTools.tools.polyEdgeCurve);glTools.tools.polyEdgeCurve.buildEdgeCurvesUI()')\n\t\n\tmc.setParent('..', menu=True)\n\t\n\tmc.menuItem(divider=True)\n\t\n\t# TOOLS\n\tmc.menuItem(allowOptionBoxes=True, label='Tools', subMenu=True, tearOff=True)\n\t\n\tmc.menuItem(label='Slide Deformer', command='import glTools.model.utils;reload(glTools.model.utils);[glTools.model.utils.slideDeformer(i) for i in mc.ls(sl=1)]')\n\tmc.menuItem(label='Strain Relaxer', command='import glTools.model.utils;reload(glTools.model.utils);[glTools.model.utils.strainRelaxer(i) for i in mc.ls(sl=1)]')\n\tmc.menuItem(label='Directional Smooth', command='import glTools.model.utils;reload(glTools.model.utils);[glTools.model.utils.directionalSmooth(i) for i in mc.ls(sl=1)]')\n\tmc.menuItem(label='Straighten Vertices...', command='import glTools.model.straightenVerts;reload(glTools.model.straightenVerts);glTools.model.straightenVerts.straightenVertsUI()')\n\tmc.menuItem(label='Even Edge Spacing...', command='import glTools.model.straightenVerts;reload(glTools.model.straightenVerts);glTools.model.straightenVerts.evenEdgeSpacingUI()')\n\tmc.menuItem(label='Smooth Edge Line...', command='import glTools.model.straightenVerts;reload(glTools.model.straightenVerts);glTools.model.straightenVerts.smoothEdgeLineUI()')\n\t\n\tmc.menuItem(allowOptionBoxes=True, label='Mirror Tools', subMenu=True, tearOff=True)\n\t\n\tmc.menuItem(label='Auto Mirror', command='import glTools.utils.edgeFlowMirror;reload(glTools.utils.edgeFlowMirror);glTools.utils.edgeFlowMirror.autoMirror()')\n\tmc.menuItem(label='Mirror X (+ -> -)', command='import glTools.utils.edgeFlowMirror;reload(glTools.utils.edgeFlowMirror);glTools.utils.edgeFlowMirror.mirrorGeo(mc.ls(sl=True,fl=True)[0])')\n\tmc.menuItem(label='Mirror X (- -> +)', command='import glTools.utils.edgeFlowMirror;reload(glTools.utils.edgeFlowMirror);glTools.utils.edgeFlowMirror.mirrorGeo(mc.ls(sl=True,fl=True)[0],posToNeg=False)')\n\tmc.menuItem(label='Mirror Y (+ -> -)', command='import glTools.utils.edgeFlowMirror;reload(glTools.utils.edgeFlowMirror);glTools.utils.edgeFlowMirror.mirrorGeo(mc.ls(sl=True,fl=True)[0]),axis=\"y\"')\n\tmc.menuItem(label='Mirror Y (- -> +)', command='import glTools.utils.edgeFlowMirror;reload(glTools.utils.edgeFlowMirror);glTools.utils.edgeFlowMirror.mirrorGeo(mc.ls(sl=True,fl=True)[0],axis=\"y\",posToNeg=False)')\n\tmc.menuItem(label='Mirror Z (+ -> -)', command='import glTools.utils.edgeFlowMirror;reload(glTools.utils.edgeFlowMirror);glTools.utils.edgeFlowMirror.mirrorGeo(mc.ls(sl=True,fl=True)[0]),axis=\"z\"')\n\tmc.menuItem(label='Mirror Z (- -> +)', command='import glTools.utils.edgeFlowMirror;reload(glTools.utils.edgeFlowMirror);glTools.utils.edgeFlowMirror.mirrorGeo(mc.ls(sl=True,fl=True)[0],axis=\"z\",posToNeg=False)')\n\t\n\tmc.setParent('..', menu=True)\n\t\n\tmc.menuItem(allowOptionBoxes=True, label='Align Tools', subMenu=True, tearOff=True)\n\t\n\tmc.menuItem(label='Align to Best Fit', command='import glTools.model.utils;reload(glTools.model.utils);glTools.model.utils.alignControlPoints(mc.ls(sl=True,fl=True),axis=\"bestFitPlane\",localAxis=False)')\n\t\n\tmc.menuItem(label='Align (+X) Local', command='import glTools.model.utils;reload(glTools.model.utils);glTools.model.utils.alignControlPoints(mc.ls(sl=True,fl=True),axis=\"+x\",localAxis=True)')\n\tmc.menuItem(label='Align (-X) Local', command='import glTools.model.utils;reload(glTools.model.utils);glTools.model.utils.alignControlPoints(mc.ls(sl=True,fl=True),axis=\"-x\",localAxis=True)')\n\tmc.menuItem(label='Align (+Y) Local', command='import glTools.model.utils;reload(glTools.model.utils);glTools.model.utils.alignControlPoints(mc.ls(sl=True,fl=True),axis=\"+y\",localAxis=True)')\n\tmc.menuItem(label='Align (-Y) Local', command='import glTools.model.utils;reload(glTools.model.utils);glTools.model.utils.alignControlPoints(mc.ls(sl=True,fl=True),axis=\"-y\",localAxis=True)')\n\tmc.menuItem(label='Align (+Z) Local', command='import glTools.model.utils;reload(glTools.model.utils);glTools.model.utils.alignControlPoints(mc.ls(sl=True,fl=True),axis=\"+z\",localAxis=True)')\n\tmc.menuItem(label='Align (-Z) Local', command='import glTools.model.utils;reload(glTools.model.utils);glTools.model.utils.alignControlPoints(mc.ls(sl=True,fl=True),axis=\"-z\",localAxis=True)')\n\t\n\tmc.menuItem(label='Align (+X) World', command='import glTools.model.utils;reload(glTools.model.utils);glTools.model.utils.alignControlPoints(mc.ls(sl=True,fl=True),axis=\"+x\",localAxis=False)')\n\tmc.menuItem(label='Align (-X) World', command='import glTools.model.utils;reload(glTools.model.utils);glTools.model.utils.alignControlPoints(mc.ls(sl=True,fl=True),axis=\"-x\",localAxis=False)')\n\tmc.menuItem(label='Align (+Y) World', command='import glTools.model.utils;reload(glTools.model.utils);glTools.model.utils.alignControlPoints(mc.ls(sl=True,fl=True),axis=\"+y\",localAxis=False)')\n\tmc.menuItem(label='Align (-Y) World', command='import glTools.model.utils;reload(glTools.model.utils);glTools.model.utils.alignControlPoints(mc.ls(sl=True,fl=True),axis=\"-y\",localAxis=False)')\n\tmc.menuItem(label='Align (+Z) World', command='import glTools.model.utils;reload(glTools.model.utils);glTools.model.utils.alignControlPoints(mc.ls(sl=True,fl=True),axis=\"+z\",localAxis=False)')\n\tmc.menuItem(label='Align (-Z) World', command='import glTools.model.utils;reload(glTools.model.utils);glTools.model.utils.alignControlPoints(mc.ls(sl=True,fl=True),axis=\"-z\",localAxis=False)')\n\t\n\tmc.setParent('..', menu=True)\n\t\n\tmc.setParent('..', menu=True)\n\t\n\t#----------------------------------------#\n\t\n\tmc.menuItem(divider =True)\n\tmc.menuItem(label='Refresh Menu', command='import glTools.model.menu;reload(glTools.model.menu);glTools.model.menu.create()')\n\tmc.setParent('..')", "path": "model\\menu.py", "repo_name": "bungnoid/glTools", "stars": 195, "license": "mit", "language": "python", "size": 868}
{"docstring": "'''\nThis will map an entire hierarchy from a given root node.\n@param root: The root transform from which the hierarchy will be mapped\n@type root: str\n'''\n# Check root exists\n", "func_signal": "def buildHierarchyFromNode(self,root):\n", "code": "if not mc.objExists(root): raise UserInputError('Root object '+root+' does not exists!')\n\n# Get root information\nself.fullName = mc.ls(root,l=True)[0]\nself.shortName = self.fullName.split('|')[1]\nself.parent = None\nself.childList = []\nself.childCache = {}\n\n# Traverse all decendant children\nself.mapDecendants(recursive=True)", "path": "tools\\dependencyHierarchyNode.py", "repo_name": "bungnoid/glTools", "stars": 195, "license": "mit", "language": "python", "size": 868}
{"docstring": "'''\nReturn the string equivalent for the specified iteger index.\n@param index: The index to get the string equivalent for\n@type index: int\n@param padding: The number of characters for the index string\n@type padding: int\n'''\n# Convert to string\n", "func_signal": "def stringIndex(self,index,padding=2):\n", "code": "strInd = str(index)\n# Prepend padding\nfor i in range(padding-len(strInd)): strInd = '0'+strInd\n# Return string result\nreturn strInd", "path": "tools\\namingConvention.py", "repo_name": "bungnoid/glTools", "stars": 195, "license": "mit", "language": "python", "size": 868}
{"docstring": "'''\nReturn a valid control name based on the provided input argument values\n@param side: Side component of the name\n@type side: str\n@param part: Part component of the name\n@type part: str\n@param optSide: Optional side component of the name\n@type optSide: str\n@param subPart: Optional sub-part component of the name\n@type subPart: str\n@param node: Node type component of the name\n@type node: str\n@param optSideIndex: Optional side index of the name\n@type optSideIndex: int\n@param partIndex: Part index of the name\n@type partIndex: int\n@param subPartIndex: Optional sub-part index of the name\n@type subPartIndex: int\n'''\n# Check input arguments\n", "func_signal": "def getName(self,side='',part='',optSide='',subPart='',node='',optSideIndex=1,partIndex=1,subPartIndex=1):\n", "code": "if not self.side.has_key(side): raise UserInputError('Invalid side (\"'+side+'\") provided!')\nif not self.part.has_key(part): raise UserInputError('Invalid part name (\"'+part+'\") provided!')\nif not self.node.has_key(node): raise UserInputError('Invalid node type (\"'+node+'\") provided!')\nif optSide:\n\tif not self.side.has_key(optSide): raise UserInputError('Invalid otional side (\"'+optSide+'\") provided!')\nif subPart:\n\tif not self.subPart.has_key(subPart): raise UserInputError('Invalid sub-part name (\"'+subPart+'\") provided!')\n\n#---------------------\n# Get index variables\noptSideInd = str(optSideIndex)\nif optSideIndex < 10: optSideInd = '0'+optSideInd\npartInd = str(partIndex)\nif partIndex < 10: partInd = '0'+partInd\nsubPartInd = str(subPartIndex)\nif subPartIndex < 10: subPartInd = '0'+subPartInd\n\n# Build name string1.00\n#-------------------\n# Side\nif side:\n\tnameStr = self.side[side]\n\tnameStr += self.delineator # -\n# Part name\nif part:\n\tnameStr += self.part[part]\n\tnameStr += partInd\n\tnameStr += self.delineator # -\n# Optional Side\nif optSide:\n\tnameStr += self.side[optSide]\n\tnameStr += optSideInd\n\tnameStr += self.delineator # -\n# Sub part name\nif subPart:\n\tnameStr += self.subPart[subPart]\n\tnameStr += subPartInd\n\tnameStr += self.delineator # -\n# Node type\nif node: nameStr += self.node[node]\n\nreturn nameStr", "path": "tools\\namingConvention.py", "repo_name": "bungnoid/glTools", "stars": 195, "license": "mit", "language": "python", "size": 868}
{"docstring": "'''\n@param surface: Nurbs surface to constrain to\n@type surface: str\n@param targetList: List of target transforms/positions/coordinates\n@type targetList: list\n@param surfacePointsNode: Name for a new or existing surfacePoints node\n@type surfacePointsNode: str\n@param alignTo: Surface direction to align to. This option is ignored if the specified surface points node already exists\n@type alignTo: str\n@param rotate: Calculate rotation\n@type rotate: bool\n@param tangentUAxis: Transform axis to align with the surface U tangent\n@type tangentUAxis: str\n@param tangentVAxis: Transform axis to align with the surface V tangent\n@type tangentVAxis: str\n@param prefix: Name prefix for newly created nodes\n@type prefix: str\n'''\n# Check surface\n", "func_signal": "def add(surface,targetList,surfacePointsNode='',alignTo='u',rotate=True,tangentUAxis='x',tangentVAxis='y',prefix=''):\n", "code": "if not glTools.utils.surface.isSurface(surface):\n\traise Exception('Object \"\" is not a valid nurbs surface!!')\n\n# Check prefix\nif not prefix: prefix = glTools.utils.stringUtils.stripSuffix(surface)\n\n# Check targetList\nif not targetList: raise Exception('Invalid target list!!')\n\n# Check surfacePoints node\nif not surfacePointsNode:\n\tsurfacePointsNode = prefix+'surfacePoints'\nif mc.objExists(surfacePointsNode):\n\tif not mc.objectType(surfacePointsNode) == 'surfacePoints':\n\t\traise Exception('Object \"'+surfacePointsNode+'\" is not a valid surfacePoints node!!')\nelse:\n\t# Create new surface points node\n\tsurfacePointsNode = mc.createNode('surfacePoints',n=surfacePointsNode)\n\tmc.connectAttr(surface+'.worldSpace[0]',surfacePointsNode+'.inputSurface')\n\t# Apply settings\n\tmc.setAttr(surfacePointsNode+'.calculateRotation',rotate)\n\tif alignTo=='u': mc.setAttr(surfacePointsNode+'.alignTo',0)\n\telse: mc.setAttr(surfacePointsNode+'.alignTo',1)\n\t# Tangent Axis\n\taxisDict = {'x':0,'y':1,'z':2,'-x':3,'-y':4,'-z':5}\n\tmc.setAttr(surfacePointsNode+'.tangentUAxis',axisDict[tangentUAxis])\n\tmc.setAttr(surfacePointsNode+'.tangentVAxis',axisDict[tangentVAxis])\n\n# Find next available input index\nnextIndex = getNextAvailableIndex(surfacePointsNode)\n\n# Create surface constraints\ntransformList = []\nfor i in range(len(targetList)):\n\t\n\t# Get current input index\n\tind = glTools.utils.stringUtils.stringIndex(i+1,2)\n\t\n\t# Initialize UV parameter variable\n\tuv = (0.0,0.0)\n\tpos = (0.0,0.0,0.0)\n\t# Get target surface point for current target\n\tif type(targetList[i])==str or type(targetList[i])==unicode:\n\t\tif not mc.objExists(targetList[i]): raise Exception('Target list object \"'+targetList[i]+'\" does not exist')\n\t\tpos = mc.pointPosition(targetList[i])\n\t\tuv = glTools.utils.surface.closestPoint(surface,pos)\n\telif type(targetList[i])==tuple or type(targetList[i])==list:\n\t\tif len(targetList[i]) == 3:\n\t\t\tpos = targetList[i]\n\t\t\tuv = glTools.utils.surface.closestPoint(surface,pos)\n\t\telif len(targetList[i]) == 2:\n\t\t\tuv = targetList[i]\n\t\t\tpos = mc.pointOnSurface(surface,u=uv[0],v=uv[1],p=True)\n\tparamU = uv[0]\n\tparamV = uv[1]\n\t\n\t# Get surface point information\n\tpnt = mc.pointOnSurface(surface,u=paramU,v=paramV,p=True)\n\tnormal = mc.pointOnSurface(surface,u=paramU,v=paramV,nn=True)\n\ttangentU = mc.pointOnSurface(surface,u=paramU,v=paramV,ntu=True)\n\ttangentV = mc.pointOnSurface(surface,u=paramU,v=paramV,ntv=True)\n\t\n\t# Clamp param to safe values\n\tminU = mc.getAttr(surface+'.minValueU')\n\tmaxU = mc.getAttr(surface+'.maxValueU')\n\tminV = mc.getAttr(surface+'.minValueV')\n\tmaxV = mc.getAttr(surface+'.maxValueV')\n\tif paramU < (minU+0.001): paramU = minU\n\telif paramU > (maxU-0.001): paramU = maxU\n\tif paramV < (minV+0.001): paramV = minV\n\telif paramV > (maxV-0.001): paramV = maxV\n\t\n\t# Create constraint transform\n\ttransform = prefix+'_surfacePoint'+ind+'_transform'\n\ttransform = mc.createNode('transform',n=transform)\n\ttransformList.append(transform)\n\t\n\t# Add param attributes\n\tmc.addAttr(transform,ln='param',at='compound',numberOfChildren=2)\n\tmc.addAttr(transform,ln='paramU',at='double',min=minU,max=maxU,dv=paramU,k=True,p='param')\n\tmc.addAttr(transform,ln='paramV',at='double',min=minV,max=maxV,dv=paramV,k=True,p='param')\n\t\n\t# Connect to surfacePoints node\n\t#mc.setAttr(surfacePointsNode+'.offset['+ind+']',offsetU,offsetV,offsetN)\n\tmc.connectAttr(transform+'.param',surfacePointsNode+'.param['+ind+']',f=True)\n\tmc.connectAttr(transform+'.parentMatrix',surfacePointsNode+'.targetMatrix['+ind+']',f=True)\n\t\n\t# Connect to transform\n\tmc.connectAttr(surfacePointsNode+'.outTranslate['+ind+']',transform+'.translate',f=True)\n\tif rotate: mc.connectAttr(surfacePointsNode+'.outRotate['+ind+']',transform+'.rotate',f=True)\n\n# Return result\nreturn (surfacePointsNode,transformList)", "path": "utils\\surfacePoints.py", "repo_name": "bungnoid/glTools", "stars": 195, "license": "mit", "language": "python", "size": 868}
{"docstring": "'''\nFinds maya DAG children for the current object and records hierarchy information to class member array types\n@param recursive: Traverse the entire downstream decendant hierarchy\n@type recursive: bool\n'''\n# Get dag children\n", "func_signal": "def mapDecendants(self,recursive=True):\n", "code": "children = mc.listRelatives(self.fullName,c=True,pa=True,type=['transform','joint','ikHandle'])\n\n# Escape if no children\nif not children: return\n\n# Build child array\nfor child in children:\n\tchildNode = DependencyHierarchyNode()\n\tchildNode.fullName = str(mc.ls(child,l=True)[0])\n\tchildNode.shortName = childNode.fullName.split('|')[-1]\n\tchildNode.parent = self\n\tself.childList.append(childNode)\n\tself.childCache[childNode.fullName] = childNode\n# Recurse through decendant hierarchy\nif recursive:\n\tfor childNode in self.childList:\n\t\tchildNode.mapDecendants(recursive=True)", "path": "tools\\dependencyHierarchyNode.py", "repo_name": "bungnoid/glTools", "stars": 195, "license": "mit", "language": "python", "size": 868}
{"docstring": "'''\nCreate a generation based dictionary of all nodes in the dependency hierarchy.\n'''\n", "func_signal": "def generationDict(self):\n", "code": "generationDict = {}\nfor node in self.flatListNodes():\n\tgen = node.getGeneration()\n\tif not generationDict.has_key(gen): generationDict[gen] = []\n\tgenerationDict[gen].append(node.shortName)\nreturn generationDict", "path": "tools\\dependencyHierarchyNode.py", "repo_name": "bungnoid/glTools", "stars": 195, "license": "mit", "language": "python", "size": 868}
{"docstring": "# Base Hierarchy\n", "func_signal": "def __init__(self):\n", "code": "\t\tself.base = {}\n\t\tself.base['all'] = 'all'\n\t\tself.base['constrain'] = 'constrain'\n\t\tself.base['gimbal'] = 'gimbal'\n\t\tself.base['misc'] = 'misc'\n\t\tself.base['model'] = 'model'\n\t\tself.base['orient'] = 'orient'\n\t\tself.base['orientOffset'] = 'orientOffset'\n\t\tself.base['supermover'] = 'supermover'\n\t\tself.base['xform'] = 'xform'\n# Valid name elements\n\t\tself.namePattern = ['[a-z]{2}','[a-z]{3}\\d{2}','[a-z]{2}\\d{2}','[a-z]{3}']\n\t\tself.elemCount = (3,5)\n# Delineator\n\t\tself.delineator = '_'\n# Side\n\t\tself.side = {}\n\t\tself.side['center'] = 'cn'\n\t\tself.side['left'] = 'lf'\n\t\tself.side['right'] = 'rt'\n\t\tself.side['front'] = 'ft'\n\t\tself.side['back'] = 'bk'\n\t\tself.side['top'] = 'tp'\n\t\tself.side['bottom'] = 'bt'\n\t\tself.side['upper'] = 'up'\n\t\tself.side['lower'] = 'lw'\n\t\tself.side['high'] = 'hi'\n\t\tself.side['low'] = 'lo'\n\t\tself.side['middle'] = 'md'\n# Part\n\t\tself.part = {}\n\t\tself.part['ankle'] = 'ank'\n\t\tself.part['antenna'] = 'ant'\n\t\tself.part['arm'] = 'arm'\n\t\tself.part['back'] = 'bck'\n\t\tself.part['ball'] = 'bal'\n\t\tself.part['beak'] = 'bek'\n\t\tself.part['body'] = 'bdy'\n\t\tself.part['brow'] = 'brw'\n\t\tself.part['cheek'] = 'chk'\n\t\tself.part['chest'] = 'cht'\n\t\tself.part['chin'] = 'chn'\n\t\tself.part['clavicle'] = 'clv'\n\t\tself.part['claw'] = 'clw'\n\t\tself.part['ear'] = 'ear'\n\t\tself.part['elbow'] = 'elb'\n\t\tself.part['eye'] = 'eye'\n\t\tself.part['eyelid'] = 'lid'\n\t\tself.part['face'] = 'fac'\n\t\tself.part['fang'] = 'fng'\n\t\tself.part['feather'] = 'fth'\n\t\tself.part['finger'] = 'fng'\n\t\tself.part['foot'] = 'fot'\n\t\tself.part['hair'] = 'har'\n\t\tself.part['hand'] = 'hnd'\n\t\tself.part['head'] = 'hed'\n\t\tself.part['hip'] = 'hip'\n\t\tself.part['hips'] = 'hip'\n\t\tself.part['knee'] = 'kne'\n\t\tself.part['leg'] = 'leg'\n\t\tself.part['lip'] = 'lip'\n\t\tself.part['mouth'] = 'mth'\n\t\tself.part['neck'] = 'nck'\n\t\tself.part['pelvis'] = 'plv'\n\t\tself.part['shin'] = 'shn'\n\t\tself.part['spine'] = 'spn'\n\t\tself.part['sternum'] = 'str'\n\t\tself.part['stomache'] = 'stm'\n\t\tself.part['tail'] = 'tal'\n\t\tself.part['tendon'] = 'tnd'\n\t\tself.part['tooth'] = 'tth'\n\t\tself.part['teeth'] = 'tth'\n\t\tself.part['thigh'] = 'thg'\n\t\tself.part['toe'] = 'toe'\n\t\tself.part['tounge'] = 'tng'\n\t\tself.part['whisker'] = 'wsk'\n\t\tself.part['wing'] = 'wng'\n\t\tself.part['wrist'] = 'wst'\n# Sub Part\n\t\tself.subPart = {}\n\t\tself.subPart['ankle'] = 'an'\n\t\tself.subPart['ball'] = 'bl'\n\t\tself.subPart['base'] = 'bs'\n\t\tself.subPart['bend'] = 'bd'\n\t\tself.subPart['bendPosition'] = 'bp'\n\t\tself.subPart['blend'] = 'bn'\n\t\tself.subPart['claw'] = 'cw'\t\t# \"cl\" reserved for \"cluster\" sub-part\n\t\tself.subPart['cluster'] = 'cl'\n\t\tself.subPart['combine'] = 'cm'\n\t\tself.subPart['controlPoint'] = 'cv'\n\t\tself.subPart['curve'] = 'cr'\n\t\tself.subPart['default'] = 'xx'\n\t\tself.subPart['distance'] = 'dt'\n\t\tself.subPart['edge'] = 'eg'\n\t\tself.subPart['editPoint'] = 'ep'\n\t\tself.subPart['elbow'] = 'el'\n\t\tself.subPart['finger'] = 'fn'\n\t\tself.subPart['fixed'] = 'fx'\n\t\tself.subPart['FK'] = 'fk'\n\t\tself.subPart['feather'] = 'fh'\n\t\tself.subPart['free'] = 'fr'\n\t\tself.subPart['gimbal'] = 'gb'\n\t\tself.subPart['IK'] = 'ik'\n\t\tself.subPart['joint'] = 'jt'\n\t\tself.subPart['knee'] = 'kn'\n\t\tself.subPart['lattice'] = 'lt'\n\t\tself.subPart['latticeBase'] = 'lb'\n\t\tself.subPart['length'] = 'ln'\n\t\tself.subPart['negate'] = 'ng'\n\t\tself.subPart['point'] = 'pt'\n\t\tself.subPart['poleVector'] = 'pv'\n\t\tself.subPart['rotate'] = 'rt'\n\t\tself.subPart['round'] = 'rd'\n\t\tself.subPart['rebuild'] = 'rb'\n\t\tself.subPart['scale'] = 'sc'\n\t\tself.subPart['shin'] = 'sh'\t\t# ----\n\t\tself.subPart['shoulder'] = 'sh'\t\t# Duplicate value !!\n\t\tself.subPart['stretch'] = 'st'\n\t\tself.subPart['surfacePoint'] = 'sp'\n\t\tself.subPart['target'] = 'tr'\n\t\tself.subPart['tendon'] = 'tn'\n\t\tself.subPart['thigh'] = 'th'\n\t\tself.subPart['toe'] = 'to'\n\t\tself.subPart['toggle'] = 'tg'\n\t\tself.subPart['translate'] = 'tr'\n\t\tself.subPart['twist'] = 'tw'\n\t\tself.subPart['vertex'] = 'vx'\n\t\tself.subPart['wire'] = 'wi'\n\t\tself.subPart['wireBase'] = 'wb'\n\t\tself.subPart['wrist'] = 'wr'\n# Node Type\n\t\tself.node = {}\n\t\tself.node['addDoubleLinear'] = 'adl'\n\t\tself.node['angleBetween'] = 'abn'\n\t\tself.node['attachCurve'] = 'acn'\n\t\tself.node['attachSurface'] = 'asn'\n\t\tself.node['aimConstraint'] = 'amc'\n\t\tself.node['avgCurves'] = 'avc'\n\t\tself.node['bindPreMatrix'] = 'bpm'\n\t\tself.node['blendColors'] = 'blc'\n\t\tself.node['blendShape'] = 'bld'\n\t\tself.node['blendTwoAttr'] = 'bta'\n\t\tself.node['blendWeighted'] = 'bwt'\n\t\tself.node['buffer'] = 'buf'\n\t\tself.node['bufferJoint'] = 'bfj'\n\t\tself.node['choice'] = 'chc'\n\t\tself.node['chooser'] = 'chs'\n\t\tself.node['clamp'] = 'cpl'\n\t\tself.node['closestPointOnMesh'] = 'cpm'\n\t\tself.node['closestPointOnSurface'] = 'cps'\n\t\tself.node['condition'] = 'cnd'\n\t\tself.node['contrast'] = 'cnt'\n\t\tself.node['control'] = 'ccc'\n\t\tself.node['curveInfo'] = 'cin'\n\t\tself.node['curve'] = 'crv'\n\t\tself.node['curveFromMeshEdge'] = 'cme'\n\t\tself.node['curveFromSurface'] = 'cfs'\n\t\tself.node['decomposeMatrix'] = 'dcm'\n\t\tself.node['deformerHandle'] = 'ddd'\n\t\tself.node['deformer'] = 'dfm'\n\t\tself.node['detachCurve'] = 'dcn'\n\t\tself.node['detachSurface'] = 'dsn'\n\t\tself.node['distanceBetween'] = 'dst'\n\t\tself.node['endEffector'] = 'fff'\n\t\tself.node['expression'] = 'exp'\n\t\tself.node['geometryConstraint'] = 'gmc'\n\t\tself.node['gimbal'] = 'gbl'\n\t\tself.node['gluster'] = 'gls'\n\t\tself.node['group'] = 'grp'\n\t\tself.node['ikHandle'] = 'hhh'\n\t\tself.node['ikEffector'] = 'fff'\n\t\tself.node['joint'] = 'jjj'\n\t\tself.node['lidSurface'] = 'lsn'\n\t\tself.node['locator'] = 'loc'\n\t\tself.node['loft'] = 'lft'\n\t\tself.node['mesh'] = 'msh'\n\t\tself.node['multDoubleLinear'] = 'mdl'\n\t\tself.node['multiplyDivide'] = 'mdn'\n\t\tself.node['multMatrix'] = 'mmx'\n\t\tself.node['normalConstraint'] = 'nrc'\n\t\tself.node['nurbsCurve'] = 'crv'\n\t\tself.node['nurbsSurface'] = 'srf'\n\t\tself.node['orient'] = 'ori'\n\t\tself.node['orientConstraint'] = 'orc'\n\t\tself.node['parentConstraint'] = 'prc'\n\t\tself.node['plusMinusAverage'] = 'pma'\n\t\tself.node['pointConstraint'] = 'ptc'\n\t\tself.node['pointMatrixMult'] = 'pmm'\n\t\tself.node['pointOnCurveInfo'] = 'poc'\n\t\tself.node['pointOnSurfaceInfo'] = 'pos'\n\t\tself.node['poleVectorConstraint'] = 'pvc'\n\t\tself.node['poseTarget'] = 'ptg'\n\t\tself.node['poseTransform'] = 'ptf'\n\t\tself.node['rebuildCurve'] = 'rbc'\n\t\tself.node['rebuildSurface'] = 'rbs'\n\t\tself.node['remapValue'] = 'rvn'\n\t\tself.node['reverse'] = 'rvs'\n\t\tself.node['revolve'] = 'rvl'\n\t\tself.node['sampler'] = 'spl'\n\t\tself.node['samplerInfo'] = 'smp'\n\t\tself.node['sculpt'] = 'scp'\n\t\tself.node['scaleConstraint'] = 'scc'\n\t\tself.node['setRange'] = 'srn'\n\t\tself.node['skinCluster'] = 'scl'\n\t\tself.node['smoothCurve'] = 'scv'\n\t\tself.node['smoothProxy'] = 'prx'\n\t\tself.node['softMod'] = 'smd'\n\t\tself.node['subCurve'] = 'sbc'\n\t\tself.node['subSurface'] = 'sbs'\n\t\tself.node['surface'] = 'srf'\n\t\tself.node['surfPts'] = 'srp'\n\t\tself.node['surfacePoints'] = 'srp'\n\t\tself.node['surfaceSkin'] = 'skn'\n\t\tself.node['tangentConstraint'] = 'tgc'\n\t\tself.node['transform'] = 'xfm'\n\t\tself.node['uniformRebuild'] = 'urb'\n\t\tself.node['unitConversion'] = 'ucv'\n\t\tself.node['vectorProduct'] = 'vpn'\n\t\tself.node['wire'] = 'wir'\n\t\tself.node['wrap'] = 'wrp'", "path": "tools\\namingConvention.py", "repo_name": "bungnoid/glTools", "stars": 195, "license": "mit", "language": "python", "size": 868}
{"docstring": "'''\nReturn a string name based on an original name and an append string. There are options to \nmaintain string element count to satisfy the naming convention rules.\n@param name: Original name string to append to\n@type name: str\n@param appendString: String to append to the original name string\n@type appendString: str\n@param stripNameSuffix: Automatically strip the suffix from the original name string\n@type stripNameSuffix: bool\n@param trimName: Maintain string element length to fit within naming convention bounds. This will remove string elements from the original name if necessary.\n@type trimName: bool\n'''\n# Check name string\n", "func_signal": "def appendName(self,name='',appendString='',stripNameSuffix=False,trimName=False):\n", "code": "nameElem = name.split(self.delineator)\nnameStrCount = len(nameElem)\nif stripNameSuffix and (nameStrCount > 1): nameStrCount -= 1\n# Check append string\nappendElem = appendString.split(self.delineator)\nappendStrCount = len(appendElem)\n\n# Determine string intersection point\nif trimName and (nameStrCount + appendStrCount) > self.elemCount[1]:\n\tmaxNameIndex = self.elemCount[1] - appendStrCount\nelse:\tmaxNameIndex = nameStrCount\n\n# Build new name string\nnewName = ''\nfor i in range(maxNameIndex):\n\tnewName += nameElem[i]+self.delineator\nfor i in range(appendStrCount-1):\n\tnewName += appendElem[i]+self.delineator\nnewName += appendElem[-1]\n\n# Return new name\nreturn newName", "path": "tools\\namingConvention.py", "repo_name": "bungnoid/glTools", "stars": 195, "license": "mit", "language": "python", "size": 868}
{"docstring": "'''\nReturn a flat list of child dependency nodes\n@param recursive: Traverse the entire downstream decendant hierarchy\n@type recursive: bool\n'''\n", "func_signal": "def listChildNodes(self,longNames=False,recursive=False):\n", "code": "childList = []\nfor child in self.childList: childList.append(child)\nfor child in self.childList:\n\tif recursive: childList.extend(child.listChildNodes(recursive=True))\nreturn childList", "path": "tools\\dependencyHierarchyNode.py", "repo_name": "bungnoid/glTools", "stars": 195, "license": "mit", "language": "python", "size": 868}
{"docstring": "'''\nReturn a flat list of child dependency nodes\n@param longNames: Return a list of long object names\n@type longNames: bool\n@param recursive: Traverse the entire downstream decendant hierarchy\n@type recursive: bool\n'''\n", "func_signal": "def listChildren(self,longNames=False,recursive=False):\n", "code": "childList = []\nfor child in self.childList:\n\tif longNames:\tchildList.append(child.fullName)\n\telse:\t\tchildList.append(child.shortName)\nfor child in self.childList:\n\tif recursive: childList.extend(child.listChildren(longNames=longNames,recursive=True))\nreturn childList", "path": "tools\\dependencyHierarchyNode.py", "repo_name": "bungnoid/glTools", "stars": 195, "license": "mit", "language": "python", "size": 868}
{"docstring": "'''\n@param surfacePointsNode: SurfacePoints node to query\n@type surfacePointsNode: str\n'''\n# Get MObject\n", "func_signal": "def getNextAvailableIndex(surfacePointsNode):\n", "code": "sel = OpenMaya.MSelectionList()\nOpenMaya.MGlobal.getSelectionListByName(surfacePointsNode,sel)\nobj = OpenMaya.MObject()\nsel.getDependNode(0,obj)\n\n# Get param plug\nparamPlug = OpenMaya.MFnDependencyNode(obj).findPlug('param')\n\n# Get valid index list\nindexList = OpenMaya.MIntArray()\nparamPlug.getExistingArrayAttributeIndices(indexList)\n\n# Determine next available index\nnextIndex = 0\nif indexList.length(): nextIndex = indexList[-1] + 1\n\n# Return next available index\nreturn nextIndex", "path": "utils\\surfacePoints.py", "repo_name": "bungnoid/glTools", "stars": 195, "license": "mit", "language": "python", "size": 868}
{"docstring": "'''\nReturn the dependency based path for the current node\n@param delineator: String that will be used to separate nodes in the return path string\n@type delineator: str\n'''\n", "func_signal": "def getDependPath(self,delineator='|'):\n", "code": "path = self.shortName\nparent = self.parent\nwhile parent:\n\tpath = parent.shortName+delineator+path\n\tparent = parent.parent\nreturn path", "path": "tools\\dependencyHierarchyNode.py", "repo_name": "bungnoid/glTools", "stars": 195, "license": "mit", "language": "python", "size": 868}
{"docstring": "'''\nFind and return the node representing the specified maya object.\n@param fullName: Find the node representing this object\n@type fullName: str\n'''\n# Check node exists\n", "func_signal": "def findDependNode(self,fullName):\n", "code": "if not mc.objExists(fullName): raise UserInputError('Object '+fullName+' does not exists!')\nfullName = mc.ls(fullName,l=True)[0]\n\n# Search for node in hierarchy\ndependNode = None\nif self.fullName == fullName: return self\nif self.childCache.has_key(fullName): return self.childCache[fullName]\nfor child in self.childList:\n\tif child.childCache.has_key(fullName): return child.childCache[fullName]\nfor child in self.childList:\n\tdependNode = child.findDependNode(fullName)\n\tif dependNode: break\nreturn dependNode", "path": "tools\\dependencyHierarchyNode.py", "repo_name": "bungnoid/glTools", "stars": 195, "license": "mit", "language": "python", "size": 868}
{"docstring": "\"\"\"Create and return an AIML parser object.\"\"\"\n", "func_signal": "def create_parser():\n", "code": "parser = xml.sax.make_parser()\nhandler = AimlHandler(\"UTF-8\")\nparser.setContentHandler(handler)\n#parser.setFeature(xml.sax.handler.feature_namespaces, True)\nreturn parser", "path": "aiml\\AimlParser.py", "repo_name": "messense/wechat-bot", "stars": 181, "license": "mit", "language": "python", "size": 440}
{"docstring": "\"\"\"Process a <template> AIML element.\n\n<template> elements recursively process their contents, and\nreturn the results.  <template> is the root node of any AIML\nresponse tree.\n\n\"\"\"\n", "func_signal": "def _processTemplate(self,elem, sessionID):\n", "code": "response = \"\"\nfor e in elem[2:]:\n    response += self._processElement(e, sessionID)\nreturn response", "path": "aiml\\Kernel.py", "repo_name": "messense/wechat-bot", "stars": 181, "license": "mit", "language": "python", "size": 440}
{"docstring": "\"\"\"Retrieve the value of the specified bot predicate.\n\nIf name is not a valid bot predicate, the empty string is returned.        \n\n\"\"\"\n", "func_signal": "def getBotPredicate(self, name):\n", "code": "try: return self._botPredicates[name]\nexcept KeyError: return \"\"", "path": "aiml\\Kernel.py", "repo_name": "messense/wechat-bot", "stars": 181, "license": "mit", "language": "python", "size": 440}
{"docstring": "\"\"\"Process a <person> AIML element.\n\n<person> elements process their contents recursively, and then\nconvert all pronouns in the results from 1st person to 2nd\nperson, and vice versa.  This subsitution is handled by the\naiml.WordSub module.\n\nIf the <person> tag is used atomically (e.g. <person/>), it is\na shortcut for <person><star/></person>.\n\n\"\"\"\n", "func_signal": "def _processPerson(self,elem, sessionID):\n", "code": "response = \"\"\nfor e in elem[2:]:\n    response += self._processElement(e, sessionID)\nif len(elem[2:]) == 0:  # atomic <person/> = <person><star/></person>\n    response = self._processElement(['star',{}], sessionID)    \nreturn self._subbers['person'].sub(response)", "path": "aiml\\Kernel.py", "repo_name": "messense/wechat-bot", "stars": 181, "license": "mit", "language": "python", "size": 440}
{"docstring": "\"\"\"Process a <person2> AIML element.\n\n<person2> elements process their contents recursively, and then\nconvert all pronouns in the results from 1st person to 3rd\nperson, and vice versa.  This subsitution is handled by the\naiml.WordSub module.\n\nIf the <person2> tag is used atomically (e.g. <person2/>), it is\na shortcut for <person2><star/></person2>.\n\n\"\"\"\n", "func_signal": "def _processPerson2(self,elem, sessionID):\n", "code": "response = \"\"\nfor e in elem[2:]:\n    response += self._processElement(e, sessionID)\nif len(elem[2:]) == 0:  # atomic <person2/> = <person2><star/></person2>\n    response = self._processElement(['star',{}], sessionID)\nreturn self._subbers['person2'].sub(response)", "path": "aiml\\Kernel.py", "repo_name": "messense/wechat-bot", "stars": 181, "license": "mit", "language": "python", "size": 440}
{"docstring": "\"\"\"Return the template which is the closest match to pattern. The\n'that' parameter contains the bot's previous response. The 'topic'\nparameter contains the current topic of conversation.\n\nReturns None if no template is found.\n\n\"\"\"\n", "func_signal": "def match(self, pattern, that, topic):\n", "code": "if len(pattern) == 0:\n    return None\npattern = self._lang_support(pattern)\n# Mutilate the input.  Remove all punctuation and convert the\n# text to all caps.\ninput = string.upper(pattern)\ninput = self._puncStripRE.sub(\"\", input)\ninput = self._upuncStripRE.sub(u\"\", input)\n#print input\nif that.strip() == u\"\": that = u\"ULTRABOGUSDUMMYTHAT\" # 'that' must never be empty\nthatInput = string.upper(that)\nthatInput = re.sub(self._whitespaceRE, \" \", thatInput)\nthatInput = re.sub(self._puncStripRE, \"\", thatInput)\nif topic.strip() == u\"\": topic = u\"ULTRABOGUSDUMMYTOPIC\" # 'topic' must never be empty\ntopicInput = string.upper(topic)\ntopicInput = re.sub(self._puncStripRE, \"\", topicInput)\n\n# Pass the input off to the recursive call\npatMatch, template = self._match(input.split(), thatInput.split(), topicInput.split(), self._root)\nreturn template", "path": "aiml\\PatternMgr.py", "repo_name": "messense/wechat-bot", "stars": 181, "license": "mit", "language": "python", "size": 440}
{"docstring": "\"\"\"Translate text, returns the modified text.\"\"\"\n", "func_signal": "def sub(self, text):\n", "code": "if self._regexIsDirty:\n    self._update_regex()\nreturn self._regex.sub(self, text)", "path": "aiml\\WordSub.py", "repo_name": "messense/wechat-bot", "stars": 181, "license": "mit", "language": "python", "size": 440}
{"docstring": "\"\"\"Process a <set> AIML element.\n\nRequired element attributes:\n    name: The name of the predicate to set.\n\n<set> elements process their contents recursively, and assign the results to a predicate\n(given by their 'name' attribute) in the current session.  The contents of the element\nare also returned.\n\n\"\"\"\n", "func_signal": "def _processSet(self, elem, sessionID):\n", "code": "value = \"\"\nfor e in elem[2:]:\n    value += self._processElement(e, sessionID)\nself.setPredicate(elem[1]['name'], value, sessionID)    \nreturn value", "path": "aiml\\Kernel.py", "repo_name": "messense/wechat-bot", "stars": 181, "license": "mit", "language": "python", "size": 440}
{"docstring": "\"\"\"Process a <bot> AIML element.\n\nRequired element attributes:\n    name: The name of the bot predicate to retrieve.\n\n<bot> elements are used to fetch the value of global,\nread-only \"bot predicates.\"  These predicates cannot be set\nfrom within AIML; you must use the setBotPredicate() function.\n\n\"\"\"\n", "func_signal": "def _processBot(self, elem, sessionID):\n", "code": "attrName = elem[1]['name']\nreturn self.getBotPredicate(attrName)", "path": "aiml\\Kernel.py", "repo_name": "messense/wechat-bot", "stars": 181, "license": "mit", "language": "python", "size": 440}
{"docstring": "\"\"\"Retrieve the current value of the predicate 'name' from the\nspecified session.\n\nIf name is not a valid predicate in the session, the empty\nstring is returned.\n\n\"\"\"\n", "func_signal": "def getPredicate(self, name, sessionID = _globalSessionID):\n", "code": "try: return self._sessions[sessionID][name]\nexcept KeyError: return \"\"", "path": "aiml\\Kernel.py", "repo_name": "messense/wechat-bot", "stars": 181, "license": "mit", "language": "python", "size": 440}
{"docstring": "\"\"\"Process a <that> AIML element.\n\nOptional element attributes:\n    index: Specifies which element from the output history to\n    return.  1 is the most recent response, 2 is the next most\n    recent, and so on.\n\n<that> elements (when they appear inside <template> elements)\nare the output equivilant of <input> elements; they return one\nof the Kernel's previous responses.\n\n\"\"\"\n", "func_signal": "def _processThat(self,elem, sessionID):\n", "code": "outputHistory = self.getPredicate(self._outputHistory, sessionID)\nindex = 1\ntry:\n    # According to the AIML spec, the optional index attribute\n    # can either have the form \"x\" or \"x,y\". x refers to how\n    # far back in the output history to go.  y refers to which\n    # sentence of the specified response to return.\n    index = int(elem[1]['index'].split(',')[0])\nexcept:\n    pass\ntry: return outputHistory[-index]\nexcept IndexError:\n    if self._verboseMode:\n        err = \"No such index %d while processing <that> element.\\n\" % index\n        sys.stderr.write(err)\n    return \"\"", "path": "aiml\\Kernel.py", "repo_name": "messense/wechat-bot", "stars": 181, "license": "mit", "language": "python", "size": 440}
{"docstring": "\"\"\"Process a <think> AIML element.\n\n<think> elements process their contents recursively, and then\ndiscard the results and return the empty string.  They're\nuseful for setting predicates and learning AIML files without\ngenerating any output.\n\n\"\"\"\n", "func_signal": "def _processThink(self,elem, sessionID):\n", "code": "for e in elem[2:]:\n    self._processElement(e, sessionID)\nreturn \"\"", "path": "aiml\\Kernel.py", "repo_name": "messense/wechat-bot", "stars": 181, "license": "mit", "language": "python", "size": 440}
{"docstring": "\"\"\"Process an <li> AIML element.\n\nOptional attribute elements:\n    name: the name of a predicate to query.\n    value: the value to check that predicate for.\n\n<li> elements process their contents recursively and return\nthe results. They can only appear inside <condition> and\n<random> elements.  See _processCondition() and\n_processRandom() for details of their usage.\n \n\"\"\"\n", "func_signal": "def _processLi(self,elem, sessionID):\n", "code": "response = \"\"\nfor e in elem[2:]:\n    response += self._processElement(e, sessionID)\nreturn response", "path": "aiml\\Kernel.py", "repo_name": "messense/wechat-bot", "stars": 181, "license": "mit", "language": "python", "size": 440}
{"docstring": "\"\"\"Load a substitutions file.\n\nThe file must be in the Windows-style INI format (see the\nstandard ConfigParser module docs for information on this\nformat).  Each section of the file is loaded into its own\nsubstituter.\n\n\"\"\"\n", "func_signal": "def loadSubs(self, filename):\n", "code": "inFile = file(filename)\nparser = ConfigParser()\nparser.readfp(inFile, filename)\ninFile.close()\nfor s in parser.sections():\n    # Add a new WordSub instance for this section.  If one already\n    # exists, delete it.\n    if self._subbers.has_key(s):\n        del(self._subbers[s])\n    self._subbers[s] = WordSub()\n    # iterate over the key,value pairs and add them to the subber\n    for k,v in parser.items(s):\n        self._subbers[s][k] = v", "path": "aiml\\Kernel.py", "repo_name": "messense/wechat-bot", "stars": 181, "license": "mit", "language": "python", "size": 440}
{"docstring": "\"\"\"Process an AIML element.\n\nThe first item of the elem list is the name of the element's\nXML tag.  The second item is a dictionary containing any\nattributes passed to that tag, and their values.  Any further\nitems in the list are the elements enclosed by the current\nelement's begin and end tags; they are handled by each\nelement's handler function.\n\n\"\"\"\n", "func_signal": "def _processElement(self,elem, sessionID):\n", "code": "try:\n    handlerFunc = self._elementProcessors[elem[0]]\nexcept:\n    # Oops -- there's no handler function for this element\n    # type!\n    if self._verboseMode:\n        err = \"WARNING: No handler found for <%s> element\\n\" % elem[0].encode(self._textEncoding, 'replace')\n        sys.stderr.write(err)\n    return \"\"\nreturn handlerFunc(elem, sessionID)", "path": "aiml\\Kernel.py", "repo_name": "messense/wechat-bot", "stars": 181, "license": "mit", "language": "python", "size": 440}
{"docstring": "\"\"\"Return the Kernel's response to the input string.\"\"\"\n", "func_signal": "def respond(self, input, sessionID = _globalSessionID):\n", "code": "if len(input) == 0:\n    return \"\"\n\n#ensure that input is a unicode string\ntry: input = input.decode(self._textEncoding, 'replace')\nexcept UnicodeError: pass\nexcept AttributeError: pass\n\n# prevent other threads from stomping all over us.\nself._respondLock.acquire()\n\n# Add the session, if it doesn't already exist\nself._addSession(sessionID)\n\n# split the input into discrete sentences\nsentences = Utils.sentences(input)\nfinalResponse = \"\"\nfor s in sentences:\n    # Add the input to the history list before fetching the\n    # response, so that <input/> tags work properly.\n    inputHistory = self.getPredicate(self._inputHistory, sessionID)\n    inputHistory.append(s)\n    while len(inputHistory) > self._maxHistorySize:\n        inputHistory.pop(0)\n    self.setPredicate(self._inputHistory, inputHistory, sessionID)\n    \n    # Fetch the response\n    response = self._respond(s, sessionID)\n\n    # add the data from this exchange to the history lists\n    outputHistory = self.getPredicate(self._outputHistory, sessionID)\n    outputHistory.append(response)\n    while len(outputHistory) > self._maxHistorySize:\n        outputHistory.pop(0)\n    self.setPredicate(self._outputHistory, outputHistory, sessionID)\n\n    # append this response to the final response.\n    finalResponse += (response + \"  \")\nfinalResponse = finalResponse.strip()\n\nassert(len(self.getPredicate(self._inputStack, sessionID)) == 0)\n\n# release the lock and return\nself._respondLock.release()\ntry: return finalResponse.encode(self._textEncoding)\nexcept UnicodeError: return finalResponse", "path": "aiml\\Kernel.py", "repo_name": "messense/wechat-bot", "stars": 181, "license": "mit", "language": "python", "size": 440}
{"docstring": "\"\"\"Build re object based on the keys of the current\ndictionary.\n\n\"\"\"\n", "func_signal": "def _update_regex(self):\n", "code": "self._regex = re.compile(\"|\".join(map(self._wordToRegex, self.keys())))\nself._regexIsDirty = False", "path": "aiml\\WordSub.py", "repo_name": "messense/wechat-bot", "stars": 181, "license": "mit", "language": "python", "size": 440}
{"docstring": "\"\"\"Reset the brain to its initial state.\n\nThis is essentially equivilant to:\n    del(kern)\n    kern = aiml.Kernel()\n\n\"\"\"\n", "func_signal": "def resetBrain(self):\n", "code": "del(self._brain)\nself.__init__()", "path": "aiml\\Kernel.py", "repo_name": "messense/wechat-bot", "stars": 181, "license": "mit", "language": "python", "size": 440}
{"docstring": "\"\"\"Process a <star> AIML element.\n\nOptional attribute elements:\n    index: Which \"*\" character in the current pattern should\n    be matched?\n\n<star> elements return the text fragment matched by the \"*\"\ncharacter in the current input pattern.  For example, if the\ninput \"Hello Tom Smith, how are you?\" matched the pattern\n\"HELLO * HOW ARE YOU\", then a <star> element in the template\nwould evaluate to \"Tom Smith\".\n\n\"\"\"\n", "func_signal": "def _processStar(self, elem, sessionID):\n", "code": "try: index = int(elem[1]['index'])\nexcept KeyError: index = 1\n# fetch the user's last input\ninputStack = self.getPredicate(self._inputStack, sessionID)\ninput = self._subbers['normal'].sub(inputStack[-1])\n# fetch the Kernel's last response (for 'that' context)\noutputHistory = self.getPredicate(self._outputHistory, sessionID)\ntry: that = self._subbers['normal'].sub(outputHistory[-1])\nexcept: that = \"\" # there might not be any output yet\ntopic = self.getPredicate(\"topic\", sessionID)\nresponse = self._brain.star(\"star\", input, that, topic, index)\nreturn response", "path": "aiml\\Kernel.py", "repo_name": "messense/wechat-bot", "stars": 181, "license": "mit", "language": "python", "size": 440}
{"docstring": "\"\"\"Process a <random> AIML element.\n\n<random> elements contain zero or more <li> elements.  If\nnone, the empty string is returned.  If one or more <li>\nelements are present, one of them is selected randomly to be\nprocessed recursively and have its results returned.  Only the\nchosen <li> element's contents are processed.  Any non-<li> contents are\nignored.\n\n\"\"\"\n", "func_signal": "def _processRandom(self, elem, sessionID):\n", "code": "listitems = []\nfor e in elem[2:]:\n    if e[0] == 'li':\n        listitems.append(e)\nif len(listitems) == 0:\n    return \"\"\n        \n# select and process a random listitem.\nrandom.shuffle(listitems)\nreturn self._processElement(listitems[0], sessionID)", "path": "aiml\\Kernel.py", "repo_name": "messense/wechat-bot", "stars": 181, "license": "mit", "language": "python", "size": 440}
{"docstring": "\"\"\"Modifies the segment lines in place, projecting them to 2D coordinates.\"\"\"\n", "func_signal": "def project(self, camera_x, curve, curve_delta, position, player_y):\n", "code": "self.__project_line(\"top\", camera_x - curve - curve_delta, position, player_y)\nself.__project_line(\"bottom\", camera_x - curve, position, player_y)", "path": "swervin_mervin\\segment.py", "repo_name": "buntine/SwervinMervin", "stars": 227, "license": "gpl-2.0", "language": "python", "size": 40774}
{"docstring": "\"\"\"Defines the special text to show and for how long we should show it.\"\"\"\n", "func_signal": "def __set_special_text(self, text, time):\n", "code": "st = self.special_text\n\nif not st or st[2] != text:\n    self.special_text = [datetime.datetime.now(), time, text]", "path": "swervin_mervin\\player.py", "repo_name": "buntine/SwervinMervin", "stars": 227, "license": "gpl-2.0", "language": "python", "size": 40774}
{"docstring": "\"\"\"Updates speed at appropriate acceleration level.\"\"\"\n", "func_signal": "def accelerate(self):\n", "code": "curr_speed = self.speed_boost * (self.speed + ((self.settings[\"top_speed\"] / self.settings[\"acceleration_factor\"]) * self.acceleration))\nself.speed = u.limit(curr_speed, 0, self.speed_boost * self.settings[\"top_speed\"])", "path": "swervin_mervin\\player.py", "repo_name": "buntine/SwervinMervin", "stars": 227, "license": "gpl-2.0", "language": "python", "size": 40774}
{"docstring": "\"\"\"Renders the player sprite to the given surface.\"\"\"\n", "func_signal": "def render(self, window, segment):\n", "code": "top    = segment.top\nbottom = segment.bottom\nwidth  = s.DIMENSIONS[0] / 2\nheight = s.DIMENSIONS[1] / 2\nscale  = s.CAMERA_DEPTH / (s.CAMERA_HEIGHT * s.CAMERA_DEPTH)\nsprite = \"straight\"\n\nif self.direction > 0:\n    sprite = \"right\"\nelif self.direction < 0:\n    sprite = \"left\"\n\nif top[\"world\"][\"y\"] > bottom[\"world\"][\"y\"]:\n    sprite = \"uphill_\" + sprite\nelif top[\"world\"][\"y\"] < (bottom[\"world\"][\"y\"] - 10):  # TODO: Fix this. Should not need -10 here.\n    sprite = \"downhill_\" + sprite\n\nif self.speed > 0:\n    self.animation_frame += 1\n\n    if self.animation_frame > (s.PLAYER_ANIM_HOLD * 2):\n        self.animation_frame = 1\n\n# Show smoke if player is fangin' it around a corner.\nif abs(segment.curve) > s.MINIMUM_CORNER_SMOKE and\\\n   self.direction != 0 and\\\n   self.speed > (self.settings[\"top_speed\"] / 1.2):\n    sprite += \"_smoke\"\n    self.__run_screech()\nelif self.screech_sfx:\n    self.__stop_screech()\n\nsprite += \"1\" if (self.animation_frame < s.PLAYER_ANIM_HOLD) else \"2\"\n\nsprite   = self.settings[\"sprites\"][sprite]\ns_width  = int(sprite[\"width\"] * scale * s.ROAD_WIDTH * 1.2)\ns_height = int(sprite[\"height\"] * scale * s.ROAD_WIDTH * 1.2)\n\np = pygame.image.load(os.path.join(\"lib\", sprite[\"path\"]))\np = pygame.transform.scale(p, (s_width, s_height))\n\nself.rendered_area = [width - (s_width / 2), width + (s_width / 2)]\n\nwindow.blit(p, (width - (s_width / 2), s.DIMENSIONS[1] - s_height - s.BOTTOM_OFFSET))\n\n# Finish up the round.\nif self.status != PlayerStatus.alive:\n    self.level_over_lag -= 1", "path": "swervin_mervin\\player.py", "repo_name": "buntine/SwervinMervin", "stars": 227, "license": "gpl-2.0", "language": "python", "size": 40774}
{"docstring": "\"\"\"Detects and handles player collisions with sprites.\"\"\"\n", "func_signal": "def detect_collisions(self, segment):\n", "code": "if not self.crashed:\n    for sp in segment.sprites:\n        if sp.sprite.has_key(\"collision\") and self.__collided_with_sprite(sp):\n            if sp.is_hooker():\n                if not sp.hit:\n                    sp.hit = True\n                    self.__hit_hooker()\n            elif sp.is_bonus():\n                segment.remove_sprite(sp)\n                self.__hit_bonus()\n            elif sp.is_speed_boost():\n                self.__hit_speed_boost()\n            else:\n                self.__hit_world_object()\n\n            break\n\n    for comp in segment.competitors:\n        if self.__collided_with_sprite(comp):\n            self.__hit_competitor()\n\n            break", "path": "swervin_mervin\\player.py", "repo_name": "buntine/SwervinMervin", "stars": 227, "license": "gpl-2.0", "language": "python", "size": 40774}
{"docstring": "\"\"\"Returns true if this segment will be projected behind a hill, or behind us, etc.\"\"\"\n", "func_signal": "def should_ignore(self, segment):\n", "code": "return self.top[\"camera\"][\"z\"] <= s.CAMERA_DEPTH or\\\n       self.top[\"screen\"][\"y\"] <= segment.top[\"screen\"][\"y\"] or\\\n       self.bottom[\"screen\"][\"y\"] >= self.top[\"screen\"][\"y\"]", "path": "swervin_mervin\\segment.py", "repo_name": "buntine/SwervinMervin", "stars": 227, "license": "gpl-2.0", "language": "python", "size": 40774}
{"docstring": "\"\"\"Renders a Head-Up display on the active window.\"\"\"\n", "func_signal": "def render_hud(self, window):\n", "code": "center      = (75, s.DIMENSIONS[1] - 80)\nspeedo_rect = (35, s.DIMENSIONS[1] - 120, 80, 80)\norbit_pos   = (self.speed / (self.settings[\"top_speed\"] / 4.7)) + 2.35\nstart       = self.__circular_orbit(center, -10, orbit_pos)\nfinish      = self.__circular_orbit(center, 36, orbit_pos)\nspeed       = round((self.speed / s.SEGMENT_HEIGHT) * 1.5, 1)\nfont        = pygame.font.Font(s.FONTS[\"retro_computer\"], 16)\nst          = self.special_text\ntime_colour = s.COLOURS[\"text\"] if self.time_left > 5 else s.COLOURS[\"red\"]\n\n# Speedometer.\npygame.draw.circle(window, s.COLOURS[\"black\"], center, 50, 2)\npygame.draw.circle(window, s.COLOURS[\"black\"], center, 4)\npygame.draw.line(window, s.COLOURS[\"black\"], start, finish, 3)\npygame.draw.arc(window, s.COLOURS[\"black\"], speedo_rect, 0.2, math.pi * 1.25, 5)\npygame.draw.arc(window, s.COLOURS[\"red\"], speedo_rect, -0.73, 0.2, 5)\n\nu.render_text(\"kmph\", window, font, s.COLOURS[\"text\"], (110, s.DIMENSIONS[1] - 24))\nu.render_text(str(speed), window, font, s.COLOURS[\"text\"], (10, s.DIMENSIONS[1] - 24))\nu.render_text(\"Lap\", window, font, s.COLOURS[\"text\"], (s.DIMENSIONS[0] - 130, 10))\nu.render_text(\"%s/%s\" % (self.lap, self.total_laps) , window, font, s.COLOURS[\"text\"], (s.DIMENSIONS[0] - 58, 10))\n\nu.render_text(\"Time\", window, font, time_colour, (10, 10))\nu.render_text(str(math.trunc(self.time_left)), window, font, time_colour, (90, 10))\n\n# Render special text.\nif st:\n    td = (datetime.datetime.now() - st[0])\n\n    if td.seconds > st[1]:\n        self.special_text = None\n    else:\n        bonus_colour = \"bonus_a\" if (td.microseconds / 25000.0) % 10 > 5 else \"bonus_b\"\n        u.render_text(st[2], window, font, s.COLOURS[bonus_colour], (10, 36))\n\n# Points rendering needs more care because it grows so fast.\np_val_text  = font.render(str(math.trunc(self.points)), 1, s.COLOURS[\"text\"])\np_name_text = font.render(\"Points\", 1, s.COLOURS[\"text\"])\np_val_x     = s.DIMENSIONS[0] - p_val_text.get_width() - 10\n\nwindow.blit(p_val_text, (p_val_x, s.DIMENSIONS[1] - 24))\nwindow.blit(p_name_text, (p_val_x - 112, s.DIMENSIONS[1] - 24))\n\n# Hit a point milestone.\nif self.points > self.next_milestone and self.status == PlayerStatus.alive:\n    milestone_sfx = pygame.mixer.Sound(os.path.join(\"lib\", \"excellent.ogg\"))\n    milestone_sfx.play()\n\n    self.next_milestone += s.POINT_MILESTONE\n\n    self.__set_special_text(\"Nice driving!\", 2)\n\n# On the leaderboard!\nif self.high_score > 0 and self.points > self.high_score:\n    high_score_sfx = pygame.mixer.Sound(os.path.join(\"lib\", \"excellent.ogg\"))\n    high_score_sfx.play()\n\n    self.high_score = 0\n\n    self.__set_special_text(\"New High Score!\", 2)\n\nif self.status == PlayerStatus.game_over:\n    self.__game_over_overlay(window)\nelif self.status == PlayerStatus.level_over:\n    self.__level_over_overlay(window)\n\n# Display lap difference (unless we've only done one lap).\nif self.lap_margin != 0 and self.lap > 2 and self.lap_percent < 20:\n    diff = self.lap_margin\n\n    if diff <= 0:\n        colour = \"red\"\n        sign   = \"+\"\n    else:\n        colour = \"green\"\n        sign   = \"-\"\n\n    u.render_text(sign + str(round(abs(diff), 1)), window, font, s.COLOURS[colour], (10, 40))", "path": "swervin_mervin\\player.py", "repo_name": "buntine/SwervinMervin", "stars": 227, "license": "gpl-2.0", "language": "python", "size": 40774}
{"docstring": "\"\"\"Renders a blood splatter if we've killed someone.\"\"\"\n", "func_signal": "def render_blood(self, window):\n", "code": "b = pygame.image.load(os.path.join(\"lib\", \"blood.png\"))\nb.set_alpha(self.blood_alpha)\n\nx = (s.DIMENSIONS[0] - b.get_size()[0]) / 2\ny = ((s.DIMENSIONS[1] - b.get_size()[1]) / 2) - 30\n \nwindow.blit(b, (x, y))\n\nself.blood_alpha -= 1", "path": "swervin_mervin\\player.py", "repo_name": "buntine/SwervinMervin", "stars": 227, "license": "gpl-2.0", "language": "python", "size": 40774}
{"docstring": "\"\"\"Renders the road for this segment to the given surface.\"\"\"\n", "func_signal": "def render_road(self, window):\n", "code": "top      = self.top[\"screen\"]\nbottom   = self.bottom[\"screen\"]\ny_top    = (s.DIMENSIONS[1] - top[\"y\"])\ny_bottom = (s.DIMENSIONS[1] - bottom[\"y\"])\ncol      = self.palette\n\n# Road.\npoints = [((bottom[\"x\"] - bottom[\"w\"]), y_bottom),\n          ((bottom[\"x\"] + bottom[\"w\"]), y_bottom),\n          ((top[\"x\"] + top[\"w\"]),       y_top),\n          ((top[\"x\"] - top[\"w\"]),       y_top)]\npygame.draw.polygon(window, col[\"road\"], points)\n\n# Speed boost.\nif self.speed_boost and self.index % 5 == 0:\n    points = [(bottom[\"x\"], y_bottom),\n              (bottom[\"x\"] + bottom[\"w\"], y_bottom),\n              (top[\"x\"] + (top[\"w\"] / 2), y_top),\n              (bottom[\"x\"], y_bottom)]\n    pygame.draw.polygon(window, s.COLOURS[\"green\"], points)\n\ntop_footpath_width    = top[\"w\"] / (s.LANES / 2.8)\nbottom_footpath_width = bottom[\"w\"] / (s.LANES / 2.8)\n\n# Left footpath strip.\nif not self.in_tunnel:\n    points = [((bottom[\"x\"] - bottom[\"w\"] - bottom_footpath_width), y_bottom),\n              ((bottom[\"x\"] - bottom[\"w\"]),                         y_bottom),\n              ((top[\"x\"] - top[\"w\"]),                               y_top),\n              ((top[\"x\"] - top[\"w\"] - top_footpath_width),          y_top)]\n    pygame.draw.polygon(window, col[\"footpath\"], points)\n\n    # Left gutter.\n    pygame.draw.line(window, s.COLOURS[\"gutter\"],\n      (bottom[\"x\"] - bottom[\"w\"], y_bottom), (top[\"x\"] - top[\"w\"], y_top))\n\n    # Right footpath strip.\n    points = [((bottom[\"x\"] + bottom[\"w\"] + bottom_footpath_width), y_bottom),\n              ((bottom[\"x\"] + bottom[\"w\"]),                         y_bottom),\n              ((top[\"x\"] + top[\"w\"]),                               y_top),\n              ((top[\"x\"] + top[\"w\"] + top_footpath_width),          y_top)]\n    pygame.draw.polygon(window, col[\"footpath\"], points)\n\n    # Right gutter.\n    pygame.draw.line(window, s.COLOURS[\"gutter\"],\n      (bottom[\"x\"] + bottom[\"w\"], y_bottom), (top[\"x\"] + top[\"w\"], y_top))\n\nif (self.index / s.RUMBLE_LENGTH) % 2 == 0:\n    # Road lanes.\n    top_line_width    = top[\"w\"] / (s.LANES * 8)\n    bottom_line_width = bottom[\"w\"] / (s.LANES * 8)\n    step              = 1 / float(s.LANES)\n\n    # Render each lane separator.\n    for lane in range(s.LANES - 1):\n        lane_percent  = step * (lane + 1)\n        lane_bottom_w = (bottom[\"w\"] * 2) * lane_percent\n        lane_top_w    = (top[\"w\"] * 2) * lane_percent\n        bottom_left   = bottom[\"x\"] - bottom[\"w\"] + lane_bottom_w\n        bottom_right  = bottom_left + bottom_line_width\n        top_left      = top[\"x\"] - top[\"w\"] + lane_top_w\n        top_right     = top_left + top_line_width\n\n        points = [(bottom_left,  y_bottom),\n                  (bottom_right, y_bottom),\n                  (top_right,    y_top),\n                  (top_left,     y_top)]\n        pygame.draw.polygon(window, col[\"line\"], points)", "path": "swervin_mervin\\segment.py", "repo_name": "buntine/SwervinMervin", "stars": 227, "license": "gpl-2.0", "language": "python", "size": 40774}
{"docstring": "\"\"\"Updates position, reflecting how far we've travelled since the last frame.\"\"\"\n", "func_signal": "def travel(self, track_length, window):\n", "code": "pos        = self.position + (s.FRAME_RATE * self.speed)\ntd         = (datetime.datetime.now() - self.last_checkpoint)\ntotal_secs = (td.microseconds + (td.seconds + td.days * 24 * 3600) * 10**6) / 10**6 # td.total_seconds() not implemented in Python 2.6\n\nself.new_lap = False\n\nif self.speed_boost > 1:\n    self.speed_boost -= s.SPEED_BOOST_DECREASE\n\nif self.status == PlayerStatus.alive:\n    self.points   += (self.speed / s.SEGMENT_HEIGHT) / s.POINTS\n    self.time_left = round(self.checkpoint - total_secs, 1) + self.lap_bonus\n\n    if self.time_left <= 0:\n        go_sfx = pygame.mixer.Sound(os.path.join(\"lib\", \"loser.ogg\"))\n        go_sfx.play()\n        self.status = PlayerStatus.game_over\n    elif self.time_left == 5:\n        self.__set_special_text(\"Hurry up!\", 2)\n\n# New lap.\nif pos >= track_length:\n    self.__set_checkpoint()\n\n    self.lap_bonus  = 0\n    self.new_lap    = True\n    self.lap_time   = total_secs\n    self.lap_margin = self.fastest_lap - self.lap_time\n\n    # Finished level.\n    if self.status == PlayerStatus.alive and self.lap == self.total_laps:\n        self.status = PlayerStatus.level_over\n    else:    \n        self.lap += 1\n\n        lap_sfx = pygame.mixer.Sound(os.path.join(\"lib\", \"570.wav\"))\n        lap_sfx.play()\n\n    if self.status != PlayerStatus.game_over:\n        # Reduce checkpoint time every lap to increase difficulty.\n        checkpoint_diff = (self.checkpoint - self.lap_time) / s.LAP_DIFFICULTY_FACTOR\n        bonus_points    = self.time_left * s.POINTS * self.lap\n\n        self.checkpoint -= max(checkpoint_diff, s.MINIMUM_DIFFICULTY)\n        self.time_bonus += bonus_points\n        self.points     += bonus_points\n\n        if self.__fastest_lap():\n            # Congratulate player if they've broken personal record.\n            if self.lap > 2:\n                self.points += self.lap_margin * s.POINTS * self.lap\n                fast_lap_sfx = pygame.mixer.Sound(os.path.join(\"lib\", \"jim.ogg\"))\n                fast_lap_sfx.play()\n\n            self.fastest_lap = self.lap_time\n\n    pos -= track_length\n\nif pos < 0:\n    pos += track_length\n\nself.position    = pos\nself.lap_percent = round((pos / track_length) * 100)", "path": "swervin_mervin\\player.py", "repo_name": "buntine/SwervinMervin", "stars": 227, "license": "gpl-2.0", "language": "python", "size": 40774}
{"docstring": "\"\"\"Renders the tunnel roof, accounting for the exit hole if it's visible.\"\"\"\n", "func_signal": "def render_tunnel_roof(self, window, highest_y):\n", "code": "if not self.tunnel_end:\n    pygame.draw.rect(window, s.COLOURS[\"tunnel\"],\n      (0, 0, s.DIMENSIONS[0], s.DIMENSIONS[1] - highest_y))\nelse:\n    # I am mirroring the roof and road segment heights here.\n    # Not sure if this will work in all circumstances (hills, etc).\n    pygame.draw.rect(window, s.COLOURS[\"tunnel\"],\n      (0, 0, s.DIMENSIONS[0], self.top[\"screen\"][\"y\"] - (s.TUNNEL_HEIGHT / 4)))", "path": "swervin_mervin\\segment.py", "repo_name": "buntine/SwervinMervin", "stars": 227, "license": "gpl-2.0", "language": "python", "size": 40774}
{"docstring": "\"\"\"Renders the polygons/shapes (if any) for this segment to the given surface.\n   These are rendered after the track, but before the sprites.\"\"\"\n", "func_signal": "def render_polygons(self, window, full_clip):\n", "code": "for obj in self.pre_polygons:\n    obj.render(window, self.bottom[\"screen\"], self.clip, full_clip)", "path": "swervin_mervin\\segment.py", "repo_name": "buntine/SwervinMervin", "stars": 227, "license": "gpl-2.0", "language": "python", "size": 40774}
{"docstring": "\"\"\"Proceeds player through crash state.\"\"\"\n", "func_signal": "def handle_crash(self):\n", "code": "if self.crashed:\n    step = -0.025 if self.x > 0 else 0.025\n\n    if round(self.x, 1) != 0:\n        self.x += step\n    else:\n        pygame.mixer.music.set_volume(s.MUSIC_VOLUME)\n        self.crashed = False", "path": "swervin_mervin\\player.py", "repo_name": "buntine/SwervinMervin", "stars": 227, "license": "gpl-2.0", "language": "python", "size": 40774}
{"docstring": "\"\"\"Renders the sprites/competitors (if any) for this segment to the given surface.\"\"\"\n", "func_signal": "def render_world_objects(self, window):\n", "code": "for obj in (self.sprites + self.competitors + self.post_polygons):\n    obj.render(window, self)", "path": "swervin_mervin\\segment.py", "repo_name": "buntine/SwervinMervin", "stars": 227, "license": "gpl-2.0", "language": "python", "size": 40774}
{"docstring": "\"\"\"Updates y to simulate hill and valley ascension.\"\"\"\n", "func_signal": "def climb(self, segment):\n", "code": "top_y    = segment.top[\"world\"][\"y\"]\nbottom_y = segment.bottom[\"world\"][\"y\"]\n\nself.y = top_y + (top_y - bottom_y) * self.speed_percent()", "path": "swervin_mervin\\player.py", "repo_name": "buntine/SwervinMervin", "stars": 227, "license": "gpl-2.0", "language": "python", "size": 40774}
{"docstring": "\"\"\"Updates the direction the player is going, accepts a key-map.\"\"\"\n", "func_signal": "def set_direction(self, keys):\n", "code": "d = 0\n\nif self.status == PlayerStatus.alive:\n    if keys[K_LEFT]:\n        d = -self.direction_speed()\n    elif keys[K_RIGHT]:\n        d = self.direction_speed()\n\nself.direction = d", "path": "swervin_mervin\\player.py", "repo_name": "buntine/SwervinMervin", "stars": 227, "license": "gpl-2.0", "language": "python", "size": 40774}
{"docstring": "\"\"\"Returns the X/Y coordinate for a given time (t) in a circular orbit.\"\"\"\n", "func_signal": "def __circular_orbit(self, center, radius, t):\n", "code": "theta = math.fmod(t, math.pi * 2)\nc     = math.cos(theta)\ns     = math.sin(theta)\n\nreturn center[0] + radius * c, center[1] + radius * s", "path": "swervin_mervin\\player.py", "repo_name": "buntine/SwervinMervin", "stars": 227, "license": "gpl-2.0", "language": "python", "size": 40774}
{"docstring": "\"\"\"Updates x to simulate steering.\"\"\"\n", "func_signal": "def steer(self, segment):\n", "code": "bounds = s.TUNNEL_BOUNDS if self.in_tunnel else s.BOUNDS\nself.x = u.limit(self.x + self.direction, -bounds, bounds)\n\n# Apply centrifugal force if we are going around a corner.\nif segment.curve != 0 and self.status == PlayerStatus.alive:\n    # Congratulate player if they've broken personal record.\n    self.x -= (self.direction_speed() * self.speed_percent() * segment.curve * self.settings[\"centrifugal_force\"])", "path": "swervin_mervin\\player.py", "repo_name": "buntine/SwervinMervin", "stars": 227, "license": "gpl-2.0", "language": "python", "size": 40774}
{"docstring": "\"\"\"Updates the acceleration factor depending on world conditions.\"\"\"\n", "func_signal": "def set_acceleration(self, keys):\n", "code": "a = -s.FRAME_RATE\n\n# Slow player down if they are on the grass or crashed.\nif self.crashed:\n    a = 0\nelse:\n    if (self.x > 1.0 or self.x < -1.0) and self.speed > (self.settings[\"top_speed\"] / self.settings[\"offroad_top_speed_factor\"]):\n        a = a * 3\n    else:\n        if keys[K_UP] or keys[K_x] or s.AUTO_DRIVE or self.status != PlayerStatus.alive:\n            a = s.FRAME_RATE\n        elif keys[K_DOWN]:\n            a = -(s.FRAME_RATE * self.settings[\"deceleration\"])\n\nself.acceleration = a", "path": "swervin_mervin\\player.py", "repo_name": "buntine/SwervinMervin", "stars": 227, "license": "gpl-2.0", "language": "python", "size": 40774}
{"docstring": "\"\"\"Permanently removes the given sprite from this segment.\"\"\"\n", "func_signal": "def remove_sprite(self, sprite):\n", "code": "try:\n    self.sprites.remove(sprite)\nexcept Exception:\n    pass", "path": "swervin_mervin\\segment.py", "repo_name": "buntine/SwervinMervin", "stars": 227, "license": "gpl-2.0", "language": "python", "size": 40774}
{"docstring": "# This is a little strange, since most HTML parsers don't allow\n# markup like this to come through. But in general, we don't\n# know what the parser would or wouldn't have allowed, so\n# I'm letting this succeed for now.\n", "func_signal": "def test_insert_works_on_empty_element_tag(self):\n", "code": "soup = self.soup(\"<br/>\")\nsoup.br.insert(1, \"Contents\")\nself.assertEqual(str(soup.br), \"<br>Contents</br>\")", "path": "bs4\\tests\\test_tree.py", "repo_name": "liuslevis/weiquncrawler", "stars": 140, "license": "None", "language": "python", "size": 280}
{"docstring": "\"\"\"You can search the tree for text nodes.\"\"\"\n", "func_signal": "def test_find_all_text_nodes(self):\n", "code": "soup = self.soup(\"<html>Foo<b>bar</b>\\xbb</html>\")\n# Exact match.\nself.assertEqual(soup.find_all(text=\"bar\"), [u\"bar\"])\n# Match any of a number of strings.\nself.assertEqual(\n    soup.find_all(text=[\"Foo\", \"bar\"]), [u\"Foo\", u\"bar\"])\n# Match a regular expression.\nself.assertEqual(soup.find_all(text=re.compile('.*')),\n                 [u\"Foo\", u\"bar\", u'\\xbb'])\n# Match anything.\nself.assertEqual(soup.find_all(text=True),\n                 [u\"Foo\", u\"bar\", u'\\xbb'])", "path": "bs4\\tests\\test_tree.py", "repo_name": "liuslevis/weiquncrawler", "stars": 140, "license": "None", "language": "python", "size": 280}
{"docstring": "\"\"\"Accessing a Python member .foo invokes find('foo')\"\"\"\n", "func_signal": "def test_member_access_invokes_find(self):\n", "code": "soup = self.soup('<b><i></i></b>')\nself.assertEqual(soup.b, soup.find('b'))\nself.assertEqual(soup.b.i, soup.find('b').find('i'))\nself.assertEqual(soup.a, None)", "path": "bs4\\tests\\test_tree.py", "repo_name": "liuslevis/weiquncrawler", "stars": 140, "license": "None", "language": "python", "size": 280}
{"docstring": "# Unlike other NavigableString subclasses, a DOCTYPE always ends\n# in a newline.\n", "func_signal": "def test_doctype_ends_in_newline(self):\n", "code": "doctype = Doctype(\"foo\")\nsoup = self.soup(\"\")\nsoup.insert(1, doctype)\nself.assertEqual(soup.encode(), b\"<!DOCTYPE foo>\\n\")", "path": "bs4\\tests\\test_tree.py", "repo_name": "liuslevis/weiquncrawler", "stars": 140, "license": "None", "language": "python", "size": 280}
{"docstring": "# The <b> tag containing the \"Three\" node is the predecessor\n# of the \"Three\" node itself, which is why \"Three\" shows up\n# here.\n", "func_signal": "def test_find_all_previous(self):\n", "code": "self.assertSelects(\n    self.end.find_all_previous('b'), [\"Three\", \"Two\", \"One\"])\nself.assertSelects(self.end.find_all_previous(id=1), [\"One\"])", "path": "bs4\\tests\\test_tree.py", "repo_name": "liuslevis/weiquncrawler", "stars": 140, "license": "None", "language": "python", "size": 280}
{"docstring": "\"\"\"Text inside a CData object is passed into the formatter.\n\nBut the return value is ignored.\n\"\"\"\n\n", "func_signal": "def test_cdata_is_never_formatted(self):\n", "code": "self.count = 0\ndef increment(*args):\n    self.count += 1\n    return \"BITTER FAILURE\"\n\nsoup = self.soup(\"\")\ncdata = CData(\"<><><>\")\nsoup.insert(1, cdata)\nself.assertEqual(\n    b\"<![CDATA[<><><>]]>\", soup.encode(formatter=increment))\nself.assertEqual(1, self.count)", "path": "bs4\\tests\\test_tree.py", "repo_name": "liuslevis/weiquncrawler", "stars": 140, "license": "None", "language": "python", "size": 280}
{"docstring": "# Pickling a tree, then unpickling it, yields a tree identical\n# to the original.\n", "func_signal": "def test_pickle_and_unpickle_identity(self):\n", "code": "dumped = pickle.dumps(self.tree, 2)\nloaded = pickle.loads(dumped)\nself.assertEqual(loaded.__class__, BeautifulSoup)\nself.assertEqual(loaded.decode(), self.tree.decode())", "path": "bs4\\tests\\test_tree.py", "repo_name": "liuslevis/weiquncrawler", "stars": 140, "license": "None", "language": "python", "size": 280}
{"docstring": "# written by Michael Norton (http://docondev.blogspot.com/)\n", "func_signal": "def convert_to_utf8_str(arg):\n", "code": "if isinstance(arg, unicode):\n    arg = arg.encode('utf-8')\nelif not isinstance(arg, str):\n    arg = str(arg)\nreturn arg", "path": "weibopy\\utils.py", "repo_name": "liuslevis/weiquncrawler", "stars": 140, "license": "None", "language": "python", "size": 280}
{"docstring": "# You can pass a list of attribute values instead of just one,\n# and you'll get tags that match any of the values.\n", "func_signal": "def test_find_all_with_list_attribute_values(self):\n", "code": "tree = self.soup(\"\"\"<a id=\"1\">1</a>\n                    <a id=\"2\">2</a>\n                    <a id=\"3\">3</a>\n                    <a>No ID.</a>\"\"\")\nself.assertSelects(tree.find_all(id=[\"1\", \"3\", \"4\"]),\n                   [\"1\", \"3\"])", "path": "bs4\\tests\\test_tree.py", "repo_name": "liuslevis/weiquncrawler", "stars": 140, "license": "None", "language": "python", "size": 280}
{"docstring": "\"\"\"Find the currently active prefix for the given namespace.\"\"\"\n", "func_signal": "def _prefix_for_namespace(self, namespace):\n", "code": "if namespace is None:\n    return None\nfor inverted_nsmap in reversed(self.nsmaps):\n    if inverted_nsmap is not None and namespace in inverted_nsmap:\n        return inverted_nsmap[namespace]", "path": "bs4\\builder\\_lxml.py", "repo_name": "liuslevis/weiquncrawler", "stars": 140, "license": "None", "language": "python", "size": 280}
{"docstring": "# If you search for a number, it's treated as a string.\n", "func_signal": "def test_find_all_with_numeric_attribute(self):\n", "code": "tree = self.soup(\"\"\"<a id=1>Unquoted attribute.</a>\n                    <a id=\"1\">Quoted attribute.</a>\"\"\")\n\nexpected = [\"Unquoted attribute.\", \"Quoted attribute.\"]\nself.assertSelects(tree.find_all(id=1), expected)\nself.assertSelects(tree.find_all(id=\"1\"), expected)", "path": "bs4\\tests\\test_tree.py", "repo_name": "liuslevis/weiquncrawler", "stars": 140, "license": "None", "language": "python", "size": 280}
{"docstring": "# A tree containing Unicode characters can be pickled.\n", "func_signal": "def test_unicode_pickle(self):\n", "code": "html = u\"<b>\\N{SNOWMAN}</b>\"\nsoup = self.soup(html)\ndumped = pickle.dumps(soup, pickle.HIGHEST_PROTOCOL)\nloaded = pickle.loads(dumped)\nself.assertEqual(loaded.decode(), soup.decode())", "path": "bs4\\tests\\test_tree.py", "repo_name": "liuslevis/weiquncrawler", "stars": 140, "license": "None", "language": "python", "size": 280}
{"docstring": "# A tag with no children has no .stirng.\n", "func_signal": "def test_empty_tag_has_no_string(self):\n", "code": "soup = self.soup(\"<b></b>\")\nself.assertEqual(soup.b.string, None)", "path": "bs4\\tests\\test_tree.py", "repo_name": "liuslevis/weiquncrawler", "stars": 140, "license": "None", "language": "python", "size": 280}
{"docstring": "# None of the current builders turn CDATA sections into CData\n# objects, but you can create them manually.\n", "func_signal": "def test_cdata(self):\n", "code": "soup = self.soup(\"\")\ncdata = CData(\"foo\")\nsoup.insert(1, cdata)\nself.assertEqual(str(soup), \"<![CDATA[foo]]>\")\nself.assertEqual(soup.find(text=\"foo\"), \"foo\")\nself.assertEqual(soup.contents[0], \"foo\")", "path": "bs4\\tests\\test_tree.py", "repo_name": "liuslevis/weiquncrawler", "stars": 140, "license": "None", "language": "python", "size": 280}
{"docstring": "# A tag with a single child which has a .string inherits that\n# .string.\n", "func_signal": "def test_tag_with_recursive_string_has_string(self):\n", "code": "soup = self.soup(\"<a><b>foo</b></a>\")\nself.assertEqual(soup.a.string, \"foo\")\nself.assertEqual(soup.string, \"foo\")", "path": "bs4\\tests\\test_tree.py", "repo_name": "liuslevis/weiquncrawler", "stars": 140, "license": "None", "language": "python", "size": 280}
{"docstring": "\"\"\"has_attr() checks for the presence of an attribute.\n\nPlease note note: has_attr() is different from\n__in__. has_attr() checks the tag's attributes and __in__\nchecks the tag's chidlren.\n\"\"\"\n", "func_signal": "def test_has_attr(self):\n", "code": "soup = self.soup(\"<foo attr='bar'>\")\nself.assertTrue(soup.foo.has_attr('attr'))\nself.assertFalse(soup.foo.has_attr('attr2'))", "path": "bs4\\tests\\test_tree.py", "repo_name": "liuslevis/weiquncrawler", "stars": 140, "license": "None", "language": "python", "size": 280}
{"docstring": "'''\n\u83b7\u5f97uid(string)\u5728\u8868list[strings]\u4e2d\u7684\u4f4d\u7f6e\n\u8fd4\u56de -1 \u5982\u679c\u4e0d\u518d\u8868\u5185\n'''\n", "func_signal": "def getuidindex(uid,uid_list):\n", "code": "i=-1\nif uid in uid_list:\n    i = uid_list.index(uid)\n    \nreturn i", "path": "convert_userid_result.py", "repo_name": "liuslevis/weiquncrawler", "stars": 140, "license": "None", "language": "python", "size": 280}
{"docstring": "# You can pass in None as the value of an attribute to find_all.\n# This will match tags that have that attribute set to any value.\n", "func_signal": "def test_find_all_with_defined_attribute(self):\n", "code": "tree = self.soup(\"\"\"<a id=\"1\">ID present.</a>\n                    <a>No ID present.</a>\n                    <a id=\"\">ID is empty.</a>\"\"\")\nself.assertSelects(\n    tree.find_all(id=True), [\"ID present.\", \"ID is empty.\"])", "path": "bs4\\tests\\test_tree.py", "repo_name": "liuslevis/weiquncrawler", "stars": 140, "license": "None", "language": "python", "size": 280}
{"docstring": "\"\"\"Tag.text and Tag.get_text(sep=u\"\") -> all child text, concatenated\"\"\"\n", "func_signal": "def test_all_text(self):\n", "code": "soup = self.soup(\"<a>a<b>r</b>   <r> t </r></a>\")\nself.assertEqual(soup.a.text, \"ar  t \")\nself.assertEqual(soup.a.get_text(strip=True), \"art\")\nself.assertEqual(soup.a.get_text(\",\"), \"a,r, , t \")\nself.assertEqual(soup.a.get_text(\",\", strip=True), \"a,r,t\")", "path": "bs4\\tests\\test_tree.py", "repo_name": "liuslevis/weiquncrawler", "stars": 140, "license": "None", "language": "python", "size": 280}
{"docstring": "# Split the namespace URL out of a fully-qualified lxml tag\n# name. Copied from lxml's src/lxml/sax.py.\n", "func_signal": "def _getNsTag(self, tag):\n", "code": "if tag[0] == '{':\n    return tuple(tag[1:].split('}', 1))\nelse:\n    return (None, tag)", "path": "bs4\\builder\\_lxml.py", "repo_name": "liuslevis/weiquncrawler", "stars": 140, "license": "None", "language": "python", "size": 280}
{"docstring": "\"\"\"Generate an entry\"\"\"\n", "func_signal": "def generate_doc(self, type, server, label, code, target, date):\n", "code": "doc = {}\ndoc[\"type\"] = type\ndoc[\"origin_server\"] = server\ndoc[\"info\"] = {}\ndoc[\"info\"][\"state\"] = label\ndoc[\"info\"][\"state_code\"] = code\ndoc[\"info\"][\"server\"] = target\ndoc[\"date\"] = date\nreturn doc", "path": "test\\test_replacing_clock_skew.py", "repo_name": "mongodb-labs/edda", "stars": 232, "license": "None", "language": "python", "size": 1451}
{"docstring": "\"\"\"Helper method to test documents generated by rs_status.process()\"\"\"\n", "func_signal": "def check_state(self, msg, state, code, server):\n", "code": "date = datetime.now()\ndoc = process(msg, date)\nassert doc\nassert doc[\"type\"] == \"status\"\nassert doc[\"info\"][\"state_code\"] == code\nassert doc[\"info\"][\"state\"] == state\nassert doc[\"info\"][\"server\"] == server", "path": "test\\test_rs_status.py", "repo_name": "mongodb-labs/edda", "stars": 232, "license": "None", "language": "python", "size": 1451}
{"docstring": "\"\"\"Does the given log line fit the criteria for this filter?\nIf yes, return an integer code.  If not, return -1.\n\"\"\"\n", "func_signal": "def criteria(msg):\n", "code": "if 'replSetReconfig' in msg:\n    return 1\nreturn 0", "path": "edda\\filters\\rs_reconfig.py", "repo_name": "mongodb-labs/edda", "stars": 232, "license": "None", "language": "python", "size": 1451}
{"docstring": "\"\"\"Organizes entries from .entries collection into lists\nsorted by date, one per origin server, as follows:\n{ \"server1\" : [doc1, doc2, doc3...]}\n{ \"server2\" : [doc1, doc2, doc3...]} and\nreturns these lists in one larger list, with the server-\nspecific lists indexed by server_num\"\"\"\n", "func_signal": "def organize_servers(db, collName):\n", "code": "servers_list = {}\n\nentries = db[collName + \".entries\"]\nservers = db[collName + \".servers\"]\n\nfor server in servers.find():\n    num = server[\"server_num\"]\n    servers_list[num] = sorted(list(entries.find({\"origin_server\": num})), key=itemgetter(\"date\"))\n\nreturn servers_list", "path": "edda\\post\\event_matchup.py", "repo_name": "mongodb-labs/edda", "stars": 232, "license": "None", "language": "python", "size": 1451}
{"docstring": "\"\"\"Given an event, generates and returns a one-line,\nmnemonic summary for that event\n\"\"\"\n# for reconfig messages\n", "func_signal": "def generate_summary(event, hostname):\n", "code": "if event[\"type\"] == \"reconfig\":\n    return \"All servers received a reconfig message\"\n\nsummary = hostname\n\n# for status messages\nif event[\"type\"] == \"status\":\n    summary += \" is now \" + event[\"state\"]\n    #if event[\"state\"] == \"ARBITER\":\n\n# for connection messages\nelif (event[\"type\"].find(\"conn\") >= 0):\n    if event[\"type\"] == \"new_conn\":\n        summary += \" opened connection #\"\n    elif event[\"type\"] == \"end_conn\":\n        summary += \" closed connection #\"\n    summary += event[\"conn_number\"] + \" to user \" + event[\"conn_addr\"]\n\n# for exit messages\nelif event[\"type\"] == \"exit\":\n    summary += \" is now exiting\"\n\n# for locking messages\nelif event[\"type\"] == \"UNLOCKED\":\n    summary += \" is unlocking itself\"\nelif event[\"type\"] == \"LOCKED\":\n    summary += \" is locking itself\"\nelif event[\"type\"] == \"FSYNC\":\n    summary += \" is in FSYNC\"\n\n# for stale messages\nelif event[\"type\"] == \"stale\":\n    summary += \" is going stale\"\n\n# for syncing messages\nelif event[\"type\"] == \"sync\":\n    summary += \" is syncing to \" + event[\"sync_to\"]\n\n# for any uncaught messages\nelse:\n    summary += \" is reporting status \" + event[\"type\"]\n\nreturn summary", "path": "edda\\post\\event_matchup.py", "repo_name": "mongodb-labs/edda", "stars": 232, "license": "None", "language": "python", "size": 1451}
{"docstring": "\"\"\"Set up a database for use by tests\"\"\"\n", "func_signal": "def db_setup(self):\n", "code": "c = MongoClient()\ndb = c[\"test\"]\nservers = db[\"fruit.servers\"]\nentries = db[\"fruit.entries\"]\nclock_skew = db[\"fruit.clock_skew\"]\ndb.drop_collection(servers)\ndb.drop_collection(entries)\ndb.drop_collection(clock_skew)\nreturn [servers, entries, clock_skew, db]", "path": "test\\test_replacing_clock_skew.py", "repo_name": "mongodb-labs/edda", "stars": 232, "license": "None", "language": "python", "size": 1451}
{"docstring": "\"\"\"Goes over the list of events and for each event where\nthe number of dissenters > the number of witnesses,\nattempts to match that event to another corresponding\nevent outside the margin of allowable network delay\"\"\"\n# useful for cases with undetected clock skew\n", "func_signal": "def resolve_dissenters(events):\n", "code": "LOGGER.info(\"--------------------------------\"\n            \"Attempting to resolve dissenters\"\n            \"--------------------------------\")\nfor a in events[:]:\n    if len(a[\"dissenters\"]) >= len(a[\"witnesses\"]):\n        events_b = events[:]\n        for b in events_b:\n            if a[\"summary\"] == b[\"summary\"]:\n                for wit_a in a[\"witnesses\"]:\n                    if wit_a in b[\"witnesses\"]:\n                        break\n                else:\n                    LOGGER.debug(\"Corresponding, \"\n                                 \"clock-skewed events found, merging events\")\n                    LOGGER.debug(\"skew is {0}\".format(a[\"date\"] - b[\"date\"]))\n                    events.remove(a)\n                    # resolve witnesses and dissenters lists\n                    for wit_a in a[\"witnesses\"]:\n                        b[\"witnesses\"].append(wit_a)\n                        if wit_a in b[\"dissenters\"]:\n                            b[\"dissenters\"].remove(wit_a)\n                    # we've already found a match, stop looking\n                    break\n                LOGGER.debug(\"Match not found for this event\")\n                continue\nreturn events", "path": "edda\\post\\event_matchup.py", "repo_name": "mongodb-labs/edda", "stars": 232, "license": "None", "language": "python", "size": 1451}
{"docstring": "\"\"\"Test that program captures hostnames from\nSTARTUP messages as well as IP addresses\"\"\"\n", "func_signal": "def test_startup_with_hostname(self):\n", "code": "self.check_state_with_addr(\"Mon Jun 11 15:56:16 [rsStart]\"\n    \" replSet I am sam@10gen.com:27018\", \"STARTUP1\", 0, \"sam@10gen.com:27018\")", "path": "test\\test_rs_status.py", "repo_name": "mongodb-labs/edda", "stars": 232, "license": "None", "language": "python", "size": 1451}
{"docstring": "\"\"\"test the process() method of this module\"\"\"\n", "func_signal": "def test_process(self):\n", "code": "date = datetime.now()\n# invalid lines\nassert process(\"Mon Jun 11 15:56:16 \"\n    \"[rsStart] replSet localhost:27018\", date) == None\nassert process(\"Mon Jun 11 15:56:18 \"\n    \"[rsHealthPoll] replSet member localhost:27019 is up\", date) == None\n# valid lines\nself.check_state(\"Mon Jun 11 15:56:16 \"\n    \"[rsStart] replSet I am\", \"STARTUP1\", 0, \"self\")\nself.check_state(\"[rsMgr] replSet PRIMARY\", \"PRIMARY\", 1, \"self\")\nself.check_state(\"[rsSync] replSet SECONDARY\", \"SECONDARY\", 2, \"self\")\nself.check_state(\"[rsSync] replSet is RECOVERING\", \"RECOVERING\", 3, \"self\")\nself.check_state(\"[rsSync] replSet member \"\n    \"encountered FATAL ERROR\", \"FATAL\", 4, \"self\")\nself.check_state(\"[rsStart] replSet STARTUP2\", \"STARTUP2\", 5, \"self\")\nself.check_state(\n    \"Mon Jul 11 11:56:32 [rsSync] replSet member\"\n    \" 10.4.3.56:45456 is now in state UNKNOWN\",\n    \"UNKNOWN\", 6, \"10.4.3.56:45456\")\nself.check_state(\"Mon Jul 11 11:56:32\"\n                 \" [rsHealthPoll] replSet member localhost:27019\"\n                 \" is now in state ARBITER\", \"ARBITER\", 7, \"localhost:27019\")\nself.check_state(\"Mon Jul 11 11:56:32\"\n                 \" [rsHealthPoll] replSet member \"\n                 \"localhost:27017 is now in state DOWN\", \"DOWN\", 8, \"localhost:27017\")\nself.check_state(\"Mon Jul 11 11:56:32\"\n                 \" [rsSync] replSet member example@domain.com:22234\"\n    \" is now in state ROLLBACK\", \"ROLLBACK\", 9, \"example@domain.com:22234\")\nself.check_state(\"Mon Jul 11 11:56:32\"\n                 \" [rsSync] replSet member my-MacBook-pro:43429 has been REMOVED\"\n    \"\", \"REMOVED\", 10, \"my-MacBook-pro:43429\")", "path": "test\\test_rs_status.py", "repo_name": "mongodb-labs/edda", "stars": 232, "license": "None", "language": "python", "size": 1451}
{"docstring": "\"\"\" Does the given log line fit the criteria for this filter?\n    If yes, return an integer code.  If not, return 0.\n\"\"\"\n", "func_signal": "def criteria(msg):\n", "code": "if ('[initandlisten] MongoDB starting' in msg or\n    '[mongosMain] MongoS' in msg or\n    '[mongosMain] mongos' in msg):\n    return 1\nif 'db version' in msg:\n    return 2\nif 'options:' in msg:\n    return 3\nif 'build info:' in msg:\n    return 4\nreturn 0", "path": "edda\\filters\\init_and_listen.py", "repo_name": "mongodb-labs/edda", "stars": 232, "license": "None", "language": "python", "size": 1451}
{"docstring": "\"\"\"Does the given log line fit the criteria for this filter?\nIf yes, return an integer code.  If not, return 0.\n\"\"\"\n", "func_signal": "def criteria(msg):\n", "code": "if 'dbexit: really exiting now' in msg:\n    return 1\nreturn 0", "path": "edda\\filters\\rs_exit.py", "repo_name": "mongodb-labs/edda", "stars": 232, "license": "None", "language": "python", "size": 1451}
{"docstring": "\"\"\"If the given log line fits the criteria for this filter,\nprocess the line and create a document of the following format:\ndocument = {\n   \"date\" : date,\n   \"type\" : \"start_migration\",\n   \"msg\" : msg,\n   \"info\" : {\n      \"server\" : \"self\",\n      }\n}\n\nFor actual data transfer information:\ndocument = {\n   \"date\" : date,\n   \"type\" : \"migration\" or \"commit_migration\" or \"abort_migration\",\n   \"msg\"  : msg,\n   \"info\" : {\n      \"server\"     : \"self\",\n      \"from_shard\" : \"string\",\n      \"to_shard\"   : \"string\"\n   }\n\"\"\"\n", "func_signal": "def process(msg, date):\n", "code": "messageType = criteria(msg)\nif not messageType:\n    return None\n\ndoc = {}\ndoc[\"date\"] = date\ndoc[\"info\"] = {}\ndoc[\"msg\"] = msg\n\n# populate info\ndoc[\"info\"][\"server\"] = \"self\"\n\n# starts\nif messageType == 1:\n    doc[\"type\"] = \"start_migration\"\n# successful migrations\nelif messageType == 2:\n    doc[\"type\"] = \"commit_migration\"\n# aborted migrations\nelif messageType == 3:\n    doc[\"type\"] = \"abort_migration\"\n# progress messages\nelif messageType == 4:\n    doc[\"type\"] = \"migration\"\n    label = \"sessionId: \\\"\"\n    shards = msg[msg.find(label) + len(label):].split('_')\n    doc[\"info\"][\"from_shard\"] = shards[0]\n    doc[\"info\"][\"to_shard\"] = shards[1]\n\n# clean this up\nif messageType == 2 or messageType == 3:\n    from_label = \"from: \\\"\"\n    doc[\"info\"][\"from_shard\"] = msg[msg.find(from_label) + len(from_label):].split(\"\\\"\")[0]\n    to_label = \"to: \\\"\"\n    doc[\"info\"][\"to_shard\"] = msg[msg.find(to_label) + len(to_label):].split(\"\\\"\")[0]\n\nlogger = logging.getLogger(__name__)\nlogger.debug(doc)\nreturn doc", "path": "edda\\filters\\chunk_migration.py", "repo_name": "mongodb-labs/edda", "stars": 232, "license": "None", "language": "python", "size": 1451}
{"docstring": "\"\"\"Given two .entries documents, perform checks specific to\ntheir type to see if they refer to corresponding events\n\"\"\"\n\n", "func_signal": "def type_check(entry_a, entry_b):\n", "code": "if entry_a[\"type\"] == entry_b[\"type\"]:\n    if entry_a[\"type\"] == \"status\":\n        if entry_a[\"info\"][\"state\"] != entry_b[\"info\"][\"state\"]:\n            return None\n    return entry_a[\"type\"]\n\n# handle exit messages carefully\n# if exit and down messages, save as \"exit\" type\nif entry_a[\"type\"] == \"exit\" and entry_b[\"type\"] == \"status\":\n    if entry_b[\"info\"][\"state\"] == \"DOWN\":\n        return \"exit\"\nelif entry_b[\"type\"] == \"exit\" and entry_a[\"type\"] == \"status\":\n    if entry_a[\"info\"][\"state\"] == \"DOWN\":\n        return \"exit\"\nreturn None", "path": "edda\\post\\event_matchup.py", "repo_name": "mongodb-labs/edda", "stars": 232, "license": "None", "language": "python", "size": 1451}
{"docstring": "\"\"\"Test programs's ability to capture IP address from\na STARTUP message\"\"\"\n", "func_signal": "def test_startup_with_network_name(self):\n", "code": "self.check_state_with_addr(\"Mon Jun 11 15:56:16 [rsStart]\"\n    \" replSet I am 10.4.65.7:27018\", \"STARTUP1\", 0, \"10.4.65.7:27018\")", "path": "test\\test_rs_status.py", "repo_name": "mongodb-labs/edda", "stars": 232, "license": "None", "language": "python", "size": 1451}
{"docstring": "\"\"\"If the given log line fits the criteria for\nthis filter, processes the line and creates\na document of the following format:\n\n\"init\" type documents:\ndoc = {\n   \"date\" : date,\n   \"type\" : \"init\",\n   \"msg\" : msg,\n   \"origin_server\" : name --> this field is added in the main file\n   \"info\" field structure varies with subtype:\n   (startup) \"info\" : {\n      \"subtype\" : \"startup\",\n      \"addr\" : \"hostaddr:port\",\n      \"type\" : mongos, mongod, config\n   }\n   (new_conn) \"info\" : {\n      \"subtype\" : \"new_conn\",\n      \"server\" : \"hostaddr:port\",\n      \"conn_number\" : int,\n   }\n}\n\n\"version\" type documents:\ndoc = {\n   \"date\" : date,\n   \"type\" : \"version\",\n   \"msg\" : msg,\n   \"version\" : version number,\n   \"info\" : {\n      \"server\" : \"self\"\n   }\n}\n\n\"startup_options\" documents:\ndoc = {\n   \"date\" : date,\n   \"type\" : \"startup_options\",\n   \"msg\" : msg,\n   \"info\" : {\n      \"replSet\" : replica set name (if there is one),\n      \"options\" : all options, as a string\n   }\n}\n\n\"build_info\" documents:\ndoc = {\n   \"date\" : date,\n   \"type\" : \"build_info\",\n   \"msg\" : msg,\n   \"info\" : {\n      \"build_info\" : string\n   }\n}\n\"\"\"\n\n", "func_signal": "def process(msg, date):\n", "code": "result = criteria(msg)\nif not result:\n    return None\ndoc = {}\ndoc[\"date\"] = date\ndoc[\"info\"] = {}\ndoc[\"info\"][\"server\"] = \"self\"\n\n# initial startup message\nif result == 1:\n    doc[\"type\"] = \"init\"\n    doc[\"msg\"] = msg\n    return starting_up(msg, doc)\n\n# db version\nif result == 2:\n    doc[\"type\"] = \"version\"\n    m = msg.find(\"db version v\")\n    # ick, but supports older-style log messages\n    doc[\"version\"] = msg[m + 12:].split()[0].split(',')[0]\n    return doc\n\n# startup options\nif result == 3:\n    doc[\"type\"] = \"startup_options\"\n    m = msg.find(\"replSet:\")\n    if m > -1:\n        doc[\"info\"][\"replSet\"] = msg[m:].split(\"\\\"\")[1]\n    doc[\"info\"][\"options\"] = msg[msg.find(\"options:\") + 9:]\n    return doc\n\n# build info\nif result == 4:\n    doc[\"type\"] = \"build_info\"\n    m = msg.find(\"build info:\")\n    doc[\"info\"][\"build_info\"] = msg[m + 12:]\n    return doc", "path": "edda\\filters\\init_and_listen.py", "repo_name": "mongodb-labs/edda", "stars": 232, "license": "None", "language": "python", "size": 1451}
{"docstring": "\"\"\"Given two .entries documents, are they talking about the\nsame sever?  (these should never be from the same\norigin_server) Return True or False.\n\nSide effect: may update servers entries in the db.\"\"\"\n\n", "func_signal": "def target_server_match(entry_a, entry_b, servers):\n", "code": "target_a = entry_a[\"info\"][\"server\"]\ntarget_b = entry_b[\"info\"][\"server\"]\n\nif target_a == \"self\" and target_b == \"self\":\n    # a and b are both talking about themselves\n    return False\n\na_doc = servers.find_one({\"server_num\": entry_a[\"origin_server\"]})\nb_doc = servers.find_one({\"server_num\": entry_b[\"origin_server\"]})\n\nif target_a == \"self\" and target_b == a_doc[\"network_name\"]:\n    # a is talking about itself, and b is talking about a\n    return True\n\nif target_b == \"self\" and target_a == b_doc[\"network_name\"]:\n    # b is talking about itself, and a is talking about b\n    return True\n\n# If one target is talking about itself and it doesn't have a network name,\n# assume that these entries match, and name the unnamed server\nif target_a == \"self\":\n    return check_and_assign(target_b, a_doc, servers)\n\nif target_b == \"self\":\n    return check_and_assign(target_b, b_doc, servers)\n\nreturn target_a == target_b", "path": "edda\\post\\event_matchup.py", "repo_name": "mongodb-labs/edda", "stars": 232, "license": "None", "language": "python", "size": 1451}
{"docstring": "\"\"\"This method sorts through the db's entries to\nfind discrete events that happen across servers.  It will\norganize these entries into a list of \"events\", which are\neach a dictionary built as follows:\nevent = {\n    \"type\"       = type of event, see list below\n    \"date\"       = datetime, as string\n    \"target\"     = affected server\n    \"witnesses\"  = servers who agree on this event\n    \"dissenters\" = servers who did not see this event\n                   (not for connection or sync messages)\n    \"log_line\"   = original log message (one from the witnesses)\n    \"summary\"    = a mnemonic summary of the event\n(event-specific additional fields:)\n    \"conn_addr\"    = for new_conn or end_conn messages\n    \"conn_num\"     = for new_conn or end_conn messages\n    \"from_shard\"   = for chunk migrations\n    \"to_shard\"     = for chunk migrations\n    \"state\"        = for status type messages (label, not code)\n    \"sync_to\"      = for sync type messages\n    }\n\npossible event types include:\n\n\"exit\"     : a replica set is exiting\n\"end_conn\" : end a user connection\n\"lock\"     : a server requests to lock itself from writes\n\"new_conn\" : new user connections\n\"reconfig\" : new config information was received\n\"restart\"  : a server was restarted\n\"stale\"    : a secondary is going stale\n\"status\"   : a status change for a server\n\"sync\"     : a new sync pattern for a server\n\"unlock\"   : a server requests to unlock itself from writes\n\nThis module assumes that normal network delay can account\nfor up to 2 second of lag between server logs.  Beyond this\nmargin, module assumes that servers are no longer in sync.\n\"\"\"\n# put events in ordered lists by date, one per origin_server\n# last communication with the db!\n", "func_signal": "def event_matchup(db, coll_name):\n", "code": "entries = organize_servers(db, coll_name)\nevents = []\n\nserver_coll = db[coll_name + \".servers\"]\nserver_nums = server_coll.distinct(\"server_num\")\n\n# make events\nwhile(True):\n    event = next_event(server_nums, entries, db, coll_name)\n    if not event:\n        break\n    events.append(event)\n\n# attempt to resolve any undetected skew in events\nevents = resolve_dissenters(events)\nreturn events", "path": "edda\\post\\event_matchup.py", "repo_name": "mongodb-labs/edda", "stars": 232, "license": "None", "language": "python", "size": 1451}
{"docstring": "\"\"\"if the given log line fits the criteria for this filter,\nprocesses the line and creates a document for it.\ndocument = {\n   \"date\" : date,\n   \"type\" : \"exit\",\n   \"info\" : {\n      \"server\": \"self\"\n   }\n   \"msg\" : msg\n}\n\"\"\"\n\n", "func_signal": "def process(msg, date):\n", "code": "messagetype = criteria(msg)\nif not messagetype:\n    return None\n\ndoc = {}\ndoc[\"date\"] = date\ndoc[\"type\"] = \"exit\"\ndoc[\"info\"] = {}\ndoc[\"msg\"] = msg\ndoc[\"info\"][\"server\"] = \"self\"\nreturn doc", "path": "edda\\filters\\rs_exit.py", "repo_name": "mongodb-labs/edda", "stars": 232, "license": "None", "language": "python", "size": 1451}
{"docstring": "\"\"\"Generate a document for a server startup event.\"\"\"\n", "func_signal": "def starting_up(msg, doc):\n", "code": "doc[\"info\"][\"subtype\"] = \"startup\"\n\n# what type of server is this?\nif ('MongoS' in msg) or ('mongos' in msg):\n    doc[\"info\"][\"type\"] = \"mongos\"\n    # mongos startup does not provide an address\n    doc[\"info\"][\"addr\"] = \"unknown\"\n    LOGGER.debug(\"Returning mongos startup doc\")\n    return doc\nelif msg.find(\"MongoDB\") > -1:\n    doc[\"info\"][\"type\"] = \"mongod\"\n\n# isolate port number\nm = PORT_NUMBER.search(msg)\nif m is None:\n    LOGGER.debug(\"malformed starting_up message: no port number found\")\n    return None\n\nport = m.group(0)[5:]\nhost = msg[msg.find(\"host=\") + 5:].split()[0]\n\naddr = host + \":\" + port\naddr = addr.replace('\\n', \"\")\naddr = addr.replace(\" \", \"\")\ndoc[\"info\"][\"addr\"] = addr\ndeb = \"Returning new doc for a message of type: initandlisten: starting_up\"\nLOGGER.debug(deb)\nreturn doc", "path": "edda\\filters\\init_and_listen.py", "repo_name": "mongodb-labs/edda", "stars": 232, "license": "None", "language": "python", "size": 1451}
{"docstring": "\"\"\"test the criteria() method of this module\"\"\"\n# invalid messages\n", "func_signal": "def test_criteria(self):\n", "code": "assert criteria(\"this is an invalid message\") < 0\nassert criteria(\"I am the primary\") < 0\nassert criteria(\"I am the secondary\") < 0\nassert criteria(\"the server is down\") < 0\nassert criteria(\"the server is back up\") < 0\n# check for proper return codes\nassert criteria(\n    \"Mon Jun 11 15:56:16 [rsStart] replSet I am localhost:27018\") == 0\nassert criteria(\"Mon Jun 11 15:57:04 [rsMgr] replSet PRIMARY\") == 1\nassert criteria(\"Mon Jun 11 15:56:16 [rsSync] replSet SECONDARY\") == 2\nassert criteria(\"replSet RECOVERING\") == 3\nassert criteria(\"replSet encountered a FATAL ERROR\") == 4\nassert criteria(\"Mon Jun 11 15:56:16 [rsStart] replSet STARTUP2\") == 5\nassert criteria(\"replSet member is now in state UNKNOWN\") == 6\nassert criteria(\"Mon Jun 11 15:56:18 [rsHealthPoll] \"\n    \"replSet member localhost:27019 is now in state ARBITER\") == 7\nassert criteria(\"Mon Jun 11 15:56:58 [rsHealthPoll] \"\n    \"replSet member localhost:27017 is now in state DOWN\") == 8\nassert criteria(\"replSet member is now in state ROLLBACK\") == 9\nassert criteria(\"replSet member is now in state REMOVED\") == 10\nreturn", "path": "test\\test_rs_status.py", "repo_name": "mongodb-labs/edda", "stars": 232, "license": "None", "language": "python", "size": 1451}
{"docstring": "\"\"\"\nGet a package name list from disk cache or PyPI\n\"\"\"\n#This is used by external programs that import `CheeseShop` and don't\n#want a cache file written to ~/.pypi and query PyPI every time.\n", "func_signal": "def get_cache(self):\n", "code": "if self.no_cache:\n    self.pkg_list = self.list_packages()\n    return\n\nif not os.path.exists(self.yolk_dir):\n    os.mkdir(self.yolk_dir)\nif os.path.exists(self.pkg_cache_file):\n    self.pkg_list = self.query_cached_package_list()\nelse:\n    self.logger.debug(\"DEBUG: Fetching package list cache from PyPi...\")\n    self.fetch_pkg_list()", "path": "yolk\\pypi.py", "repo_name": "cakebread/yolk", "stars": 165, "license": "bsd-3-clause", "language": "python", "size": 606}
{"docstring": "\"\"\"Configure the plugin and system, based on selected options.\n\nThe base plugin class sets the plugin to enabled if the enable option\nfor the plugin (self.enableOpt) is true.\n\"\"\"\n", "func_signal": "def configure(self, options, conf):\n", "code": "self.conf = conf\nif hasattr(options, self.enable_opt):\n    self.enabled = getattr(options, self.enable_opt)", "path": "examples\\plugins\\yolk_pkg_manager\\yolk_acme.py", "repo_name": "cakebread/yolk", "stars": 165, "license": "bsd-3-clause", "language": "python", "size": 606}
{"docstring": "\"\"\"Add plugin's options to yolk\"\"\"\n", "func_signal": "def add_options(self, parser):\n", "code": "parser.add_option('--%s' % self.name, action='store_true', \n        dest=self.enable_opt,\n        help=\"Show which packages are installed via the \" +\n             \" %s package manager. Use with -l\" % self.name)", "path": "examples\\plugins\\yolk_pkg_manager\\yolk_acme.py", "repo_name": "cakebread/yolk", "stars": 165, "license": "bsd-3-clause", "language": "python", "size": 606}
{"docstring": "\"\"\"Fetch list of available versions for a package from The CheeseShop\"\"\"\n", "func_signal": "def query_versions_pypi(self, package_name):\n", "code": "if not package_name in self.pkg_list:\n    self.logger.debug(\"Package %s not in cache, querying PyPI...\" \\\n            % package_name)\n    self.fetch_pkg_list()\n#I have to set version=[] for edge cases like \"Magic file extensions\"\n#but I'm not sure why this happens. It's included with Python or\n#because it has a space in it's name?\nversions = []\nfor pypi_pkg in self.pkg_list:\n    if pypi_pkg.lower() == package_name.lower():\n        if self.debug:\n            self.logger.debug(\"DEBUG: %s\" % package_name)\n        versions = self.package_releases(pypi_pkg)\n        package_name = pypi_pkg\n        break\nreturn (package_name, versions)", "path": "yolk\\pypi.py", "repo_name": "cakebread/yolk", "stars": 165, "license": "bsd-3-clause", "language": "python", "size": 606}
{"docstring": "\"\"\"Query PYPI via XMLRPC interface for a pkg's available versions\"\"\"\n", "func_signal": "def package_releases(self, package_name):\n", "code": "if self.debug:\n    self.logger.debug(\"DEBUG: querying PyPI for versions of \" \\\n            + package_name)\nreturn self.xmlrpc.package_releases(package_name)", "path": "yolk\\pypi.py", "repo_name": "cakebread/yolk", "stars": 165, "license": "bsd-3-clause", "language": "python", "size": 606}
{"docstring": "\"\"\"\nReturns URL of specified file type\n'source', 'egg', or 'all'\n\"\"\"\n", "func_signal": "def filter_url(pkg_type, url):\n", "code": "bad_stuff = [\"?modtime\", \"#md5=\"]\nfor junk in bad_stuff:\n    if junk in url:\n        url = url.split(junk)[0]\n        break\n\n#pkg_spec==dev (svn)\nif url.endswith(\"-dev\"):\n    url = url.split(\"#egg=\")[0]\n\nif pkg_type == \"all\":\n    return url\n\nelif pkg_type == \"source\":\n    valid_source_types = [\".tgz\", \".tar.gz\", \".zip\", \".tbz2\", \".tar.bz2\"]\n    for extension in valid_source_types:\n        if url.lower().endswith(extension):\n            return url\n\nelif pkg_type == \"egg\":\n    if url.lower().endswith(\".egg\"):\n        return url", "path": "yolk\\pypi.py", "repo_name": "cakebread/yolk", "stars": 165, "license": "bsd-3-clause", "language": "python", "size": 606}
{"docstring": "\"\"\"\nReturns PyPI's XML-RPC server instance\n\"\"\"\n", "func_signal": "def get_xmlrpc_server(self):\n", "code": "check_proxy_setting()\nif os.environ.has_key('XMLRPC_DEBUG'):\n    debug = 1\nelse:\n    debug = 0\ntry:\n    return xmlrpclib.Server(XML_RPC_SERVER, transport=ProxyTransport(), verbose=debug)\nexcept IOError:\n    self.logger(\"ERROR: Can't connect to XML-RPC server: %s\" \\\n            % XML_RPC_SERVER)", "path": "yolk\\pypi.py", "repo_name": "cakebread/yolk", "stars": 165, "license": "bsd-3-clause", "language": "python", "size": 606}
{"docstring": "\"\"\"\nReturns True if package manager 'owns' file\nReturns False if package manager does not 'own' file\n\nThere is currently no way to determine if distutils or\nsetuptools installed a package. A future feature of setuptools\nwill make a package manifest which can be checked.\n   \n'filename' must be the full path to file\n\n\"\"\"\n#Installed by distutils/setuptools or external package manager?\n#If location is in site-packages dir, check for .egg-info file\n", "func_signal": "def package_manager_owns(self, dist):\n", "code": "if dist.location.lower() == get_python_lib().lower():\n    filename = os.path.join(dist.location, dist.egg_name() + \".egg-info\")\nelse:\n    filename = dist.location\n\nstatus, output = getstatusoutput(\"/usr/bin/acmefile -q %s\" % filename)\n#status == 0 (file was installed by Acme)\n#status == 256 (file was not installed by Acme)\nif status == 0:\n    return self.name\nelse:\n    return \"\"", "path": "examples\\plugins\\yolk_pkg_manager\\yolk_acme.py", "repo_name": "cakebread/yolk", "stars": 165, "license": "bsd-3-clause", "language": "python", "size": 606}
{"docstring": "\"\"\"Return package name and version\"\"\"\n#XXX Fix package names with spaces bug.\n", "func_signal": "def get_pkg_ver(pv, add_quotes=True):\n", "code": "n = len(pv.split())\nif n == 2:\n    #Normal package_name 1.0\n    pkg_name, ver = pv.split()\nelse:\n    parts = pv.split()\n    ver = parts[-1:]\n    if add_quotes:\n        pkg_name = \"'%s'\" % \" \".join(parts[:-1])\n    else:\n        pkg_name = \"%s\" % \" \".join(parts[:-1])\nreturn pkg_name, ver", "path": "tests\\rss_feed.py", "repo_name": "cakebread/yolk", "stars": 165, "license": "bsd-3-clause", "language": "python", "size": 606}
{"docstring": "\"\"\"Configure the plugin and system, based on selected options.\n\nThe base plugin class sets the plugin to enabled if the enable option\nfor the plugin (self.enableOpt) is true.\n\"\"\"\n", "func_signal": "def configure(self, options, conf):\n", "code": "self.conf = conf\nif hasattr(options, self.enable_opt):\n    self.enabled = getattr(options, self.enable_opt)", "path": "examples\\plugins\\yolk_portage\\yolk_portage.py", "repo_name": "cakebread/yolk", "stars": 165, "license": "bsd-3-clause", "language": "python", "size": 606}
{"docstring": "\"\"\"\nReturns True if package manager 'owns' file\nReturns False if package manager does not 'own' file\n\nThere is currently no way to determine if distutils or\nsetuptools installed a package. A future feature of setuptools\nwill make a package manifest which can be checked.\n   \n'filename' must be the full path to file\n\n\"\"\"\n#Installed by distutils/setuptools or external package manager?\n#If location is in site-packages dir, check for .egg-info file\n", "func_signal": "def package_manager_owns(self, dist):\n", "code": "if dist.location.lower() == get_python_lib().lower():\n    filename = os.path.join(dist.location, dist.egg_name() + \".egg-info\")\nelse:\n    filename = dist.location\n\nstatus, output = getstatusoutput(\"/usr/bin/qfile -q %s\" % filename)\n#status == 0 (file was installed by Gentoo Portage)\n#status == 256 (file was not installed by Gentoo Portage)\nif status == 0:\n    return output.split()[0]\nelse:\n    return \"\"", "path": "examples\\plugins\\yolk_portage\\yolk_portage.py", "repo_name": "cakebread/yolk", "stars": 165, "license": "bsd-3-clause", "language": "python", "size": 606}
{"docstring": "\"\"\"Query PyPI for pkg download URI for a packge\"\"\"\n\n", "func_signal": "def get_download_urls(self, package_name, version=\"\", pkg_type=\"all\"):\n", "code": "if version:\n    versions = [version]\nelse:\n\n    #If they don't specify version, show em all.\n\n    (package_name, versions) = self.query_versions_pypi(package_name)\n\nall_urls = []\nfor ver in versions:\n    metadata = self.release_data(package_name, ver)\n    for urls in self.release_urls(package_name, ver):\n        if pkg_type == \"source\" and urls['packagetype'] == \"sdist\":\n            all_urls.append(urls['url'])\n        elif pkg_type == \"egg\" and \\\n                urls['packagetype'].startswith(\"bdist\"):\n            all_urls.append(urls['url'])\n        elif pkg_type == \"all\":\n            #All\n            all_urls.append(urls['url'])\n\n    #Try the package's metadata directly in case there's nothing\n    #returned by XML-RPC's release_urls()\n    if metadata and metadata.has_key('download_url') and \\\n                metadata['download_url'] != \"UNKNOWN\" and \\\n                metadata['download_url'] != None:\n        if metadata['download_url'] not in all_urls:\n            if pkg_type != \"all\":\n                url = filter_url(pkg_type, metadata['download_url'])\n                if url:\n                    all_urls.append(url)\nreturn all_urls", "path": "yolk\\pypi.py", "repo_name": "cakebread/yolk", "stars": 165, "license": "bsd-3-clause", "language": "python", "size": 606}
{"docstring": "\"\"\"\nReturn list of all installed packages\n\nNote: It returns one project name per pkg no matter how many versions\nof a particular package is installed\n\n@returns: list of project name strings for every installed pkg\n\n\"\"\"\n\n", "func_signal": "def get_pkglist():\n", "code": "dists = Distributions()\nprojects = []\nfor (dist, _active) in dists.get_distributions(\"all\"):\n    if dist.project_name not in projects:\n        projects.append(dist.project_name)\nreturn projects", "path": "yolk\\setuptools_support.py", "repo_name": "cakebread/yolk", "stars": 165, "license": "bsd-3-clause", "language": "python", "size": 606}
{"docstring": "\"\"\"\nIf the environmental variable 'HTTP_PROXY' is set, it will most likely be\nin one of these forms:\n\n      proxyhost:8080\n      http://proxyhost:8080\n\nurlllib2 requires the proxy URL to start with 'http://'\nThis routine does that, and returns the transport for xmlrpc.\n\"\"\"\n", "func_signal": "def check_proxy_setting():\n", "code": "try:\n    http_proxy = os.environ['HTTP_PROXY']\nexcept KeyError:\n    return\n\nif not http_proxy.startswith('http://'):\n    match = re.match('(http://)?([-_\\.A-Za-z]+):(\\d+)', http_proxy)\n    #if not match:\n    #    raise Exception('Proxy format not recognised: [%s]' % http_proxy)\n    os.environ['HTTP_PROXY'] = 'http://%s:%s' % (match.group(2),\n            match.group(3))\nreturn", "path": "yolk\\pypi.py", "repo_name": "cakebread/yolk", "stars": 165, "license": "bsd-3-clause", "language": "python", "size": 606}
{"docstring": "\"\"\"Return list of pickled package names from PYPI\"\"\"\n", "func_signal": "def query_cached_package_list(self):\n", "code": "if self.debug:\n    self.logger.debug(\"DEBUG: reading pickled cache file\")\nreturn cPickle.load(open(self.pkg_cache_file, \"r\"))", "path": "yolk\\pypi.py", "repo_name": "cakebread/yolk", "stars": 165, "license": "bsd-3-clause", "language": "python", "size": 606}
{"docstring": "\"\"\"Call all method on plugins in list, that define it, with provided\narguments. The first response that is not None is returned.\n\"\"\"\n", "func_signal": "def call_plugins(plugins, method, *arg, **kw):\n", "code": "for plug in plugins:\n    func = getattr(plug, method, None)\n    if func is None:\n        continue\n    #LOG.debug(\"call plugin %s: %s\", plug.name, method)\n    result = func(*arg, **kw)\n    if result is not None:\n        return result\nreturn None", "path": "yolk\\plugins\\__init__.py", "repo_name": "cakebread/yolk", "stars": 165, "license": "bsd-3-clause", "language": "python", "size": 606}
{"docstring": "'''Send xml-rpc request using proxy'''\n#We get a traceback if we don't have this attribute:\n", "func_signal": "def request(self, host, handler, request_body, verbose):\n", "code": "self.verbose = verbose\nurl = 'http://' + host + handler\nrequest = urllib2.Request(url)\nrequest.add_data(request_body)\n# Note: 'Host' and 'Content-Length' are added automatically\nrequest.add_header('User-Agent', self.user_agent)\nrequest.add_header('Content-Type', 'text/xml')\nproxy_handler = urllib2.ProxyHandler()\nopener = urllib2.build_opener(proxy_handler)\nfhandle = opener.open(request)\nreturn(self.parse_response(fhandle))", "path": "yolk\\pypi.py", "repo_name": "cakebread/yolk", "stars": 165, "license": "bsd-3-clause", "language": "python", "size": 606}
{"docstring": "\"\"\"Query PYPI via XMLRPC interface for a pkg's metadata\"\"\"\n", "func_signal": "def release_data(self, package_name, version):\n", "code": "try:\n    return self.xmlrpc.release_data(package_name, version)\nexcept xmlrpclib.Fault:\n    #XXX Raises xmlrpclib.Fault if you give non-existant version\n    #Could this be server bug?\n    return", "path": "yolk\\pypi.py", "repo_name": "cakebread/yolk", "stars": 165, "license": "bsd-3-clause", "language": "python", "size": 606}
{"docstring": "\"\"\"Load plugins, either builtin, others, or both.\n\"\"\"\n", "func_signal": "def load_plugins(builtin=True, others=True):\n", "code": "for entry_point in pkg_resources.iter_entry_points('yolk.plugins'):\n    #LOG.debug(\"load plugin %s\" % entry_point)\n    try:\n        plugin = entry_point.load()\n    except KeyboardInterrupt:\n        raise\n    except Exception as err_msg:\n        # never want a plugin load to exit yolk\n        # but we can't log here because the logger is not yet\n        # configured\n        warn(\"Unable to load plugin %s: %s\" % \\\n                (entry_point, err_msg), RuntimeWarning)\n        continue\n    if plugin.__module__.startswith('yolk.plugins'):\n        if builtin:\n            yield plugin\n    elif others:\n        yield plugin", "path": "yolk\\plugins\\__init__.py", "repo_name": "cakebread/yolk", "stars": 165, "license": "bsd-3-clause", "language": "python", "size": 606}
{"docstring": "\"\"\"Fetch and cache master list of package names from PYPI\"\"\"\n", "func_signal": "def fetch_pkg_list(self):\n", "code": "self.logger.debug(\"DEBUG: Fetching package name list from PyPI\")\npackage_list = self.list_packages()\ncPickle.dump(package_list, open(self.pkg_cache_file, \"w\"))\nself.pkg_list = package_list", "path": "yolk\\pypi.py", "repo_name": "cakebread/yolk", "stars": 165, "license": "bsd-3-clause", "language": "python", "size": 606}
{"docstring": "\"\"\"\nConverts a floating point value into a percentage value.\n\nNumber of decimal places set by the `decimal_places` kwarg. Default is one.\n\nBy default the number is multiplied by 100. You can prevent it from doing\nthat by setting the `multiply` keyword argument to False.\n\nIf the submitted value isn't a string, returns the `failure_string` keyword\nargument.\n\"\"\"\n", "func_signal": "def percentage(value, decimal_places=1, multiply=True, failure_string='N/A'):\n", "code": "try:\n    value = float(value)\nexcept ValueError:\n    return failure_string\nif multiply:\n    value = value * 100\nreturn _saferound(value, decimal_places) + '%'", "path": "table_fu\\formatting.py", "repo_name": "eyeseast/python-tablefu", "stars": 232, "license": "mit", "language": "python", "size": 516}
{"docstring": "\"\"\"\nBorrowed from django.contrib.humanize\n\nConverts an integer to a string containing commas every three digits.\nFor example, 3000 becomes '3,000' and 45000 becomes '45,000'.\n\"\"\"\n", "func_signal": "def intcomma(value):\n", "code": "orig = str(value)\nnew = re.sub(\"^(-?\\d+)(\\d{3})\", '\\g<1>,\\g<2>', orig)\nif orig == new:\n    return new\nelse:\n    return intcomma(new)", "path": "table_fu\\formatting.py", "repo_name": "eyeseast/python-tablefu", "stars": 232, "license": "mit", "language": "python", "size": 516}
{"docstring": "\"\"\"\nAccepts a URL and returns an HTML image tag ready to be displayed.\n\nOptionally, you can set the height and width with keyword arguments.\n\"\"\"\n", "func_signal": "def image(value, width='', height=''):\n", "code": "style = \"\"\nif width:\n    style += \"width:%s\" % width\nif height:\n    style += \"height:%s\" % height\ndata_dict = dict(src=value, style=style)\nreturn '<img src=\"%(src)s\" style=\"%(style)s\">' % data_dict", "path": "table_fu\\formatting.py", "repo_name": "eyeseast/python-tablefu", "stars": 232, "license": "mit", "language": "python", "size": 516}
{"docstring": "\"\"\"\nConverts a state's name, postal abbreviation or FIPS to ProPublica's stateface\nfont code.\n\nExample usage:\n\n    >> stateface(\"California\")\n    'E'\n\nDocumentation: http://propublica.github.com/stateface/\n\"\"\"\n", "func_signal": "def stateface(value):\n", "code": "try:\n    return statestyle.get(value).stateface\nexcept:\n    return value", "path": "table_fu\\formatting.py", "repo_name": "eyeseast/python-tablefu", "stars": 232, "license": "mit", "language": "python", "size": 516}
{"docstring": "\"\"\"\nCalling str(datum) should check first for a formatted\nversion of value, then fall back to the default value\nif there's no set formatting.\n\"\"\"\n", "func_signal": "def __str__(self):\n", "code": "if self.table.formatting.has_key(self.column_name):\n    func = self.table.formatting[self.column_name].get('filter', None)\n    args = self.table.formatting[self.column_name].get('args', [])\n    kwargs = self.table.formatting[self.column_name].get('options', {})\n    if func:\n        row = self.table[self.row_num]\n        args = [row[arg].value for arg in args]\n        return format(self.value, func, *args, **kwargs)\nreturn self.value", "path": "table_fu\\__init__.py", "repo_name": "eyeseast/python-tablefu", "stars": 232, "license": "mit", "language": "python", "size": 516}
{"docstring": "\"\"\"\nConverts a string into titlecase.\n\nLifted from Django.\n\"\"\"\n", "func_signal": "def title(value, failure_string='N/A'):\n", "code": "try:\n    value = value.lower()\n    t = re.sub(\"([a-z])'([A-Z])\", lambda m: m.group(0).lower(), value.title())\n    result = re.sub(\"\\d([A-Z])\", lambda m: m.group(0).lower(), t)\n    if not result:\n        return failure_string\n    return result\nexcept:\n    return failure_string", "path": "table_fu\\formatting.py", "repo_name": "eyeseast/python-tablefu", "stars": 232, "license": "mit", "language": "python", "size": 516}
{"docstring": "\"\"\"\nFaceting creates new TableFu instances with rows matching\neach possible value.\n\"\"\"\n", "func_signal": "def facet_by(self, column):\n", "code": "faceted_spreadsheets = {}\nfor row in self.rows:\n    if row[column]:\n        col = row[column].value\n        if faceted_spreadsheets.has_key(col):\n            faceted_spreadsheets[col].append(row.cells)\n        else:\n            faceted_spreadsheets[col] = []\n            faceted_spreadsheets[col].append(row.cells)\n\n# create a new TableFu instance for each facet\ntables = []\nfor k, v in faceted_spreadsheets.items():\n    v.insert(0, self.default_columns)\n    table = TableFu(v)\n    table.faceted_on = k\n    table.formatting = self.formatting\n    table.options = self.options\n    tables.append(table)\n\ntables.sort(key=lambda t: t.faceted_on)\nreturn tables", "path": "table_fu\\__init__.py", "repo_name": "eyeseast/python-tablefu", "stars": 232, "license": "mit", "language": "python", "size": 516}
{"docstring": "\"\"\"\nConverts an integer into the corresponding number of dollar sign symbols.\n\nIf the submitted value isn't a string, returns the `failure_string` keyword\nargument.\n\nMeant to emulate the illustration of price range on Yelp.\n\"\"\"\n", "func_signal": "def dollar_signs(value, failure_string='N/A'):\n", "code": "try:\n    count = int(value)\nexcept ValueError:\n    return failure_string\nstring = ''\nfor i in range(0, count):\n    string += '$'\nreturn string", "path": "table_fu\\formatting.py", "repo_name": "eyeseast/python-tablefu", "stars": 232, "license": "mit", "language": "python", "size": 516}
{"docstring": "\"\"\"\nSet the value for a given cell\n\"\"\"\n", "func_signal": "def __setitem__(self, column_name, value):\n", "code": "if not column_name in self.table.default_columns:\n    raise KeyError(\"%s isn't a column in this table\" % column_name)\nindex = self.table.default_columns.index(column_name)\nself.cells[index] = value", "path": "table_fu\\__init__.py", "repo_name": "eyeseast/python-tablefu", "stars": 232, "license": "mit", "language": "python", "size": 516}
{"docstring": "\"\"\"\nCreates a new TableFu instance from a file or path\n\"\"\"\n", "func_signal": "def from_file(fn, **options):\n", "code": "if hasattr(fn, 'read'):\n    return TableFu(fn, **options)\nwith open(fn) as f:\n    return TableFu(f, **options)", "path": "table_fu\\__init__.py", "repo_name": "eyeseast/python-tablefu", "stars": 232, "license": "mit", "language": "python", "size": 516}
{"docstring": "\"\"\"\nTables can be filtered in one of two ways:\n - Simple keyword arguments return rows where values match *exactly*\n - Pass in a function and return rows where that function evaluates to True\n\nIn either case, a new TableFu instance is returned\n\"\"\"\n", "func_signal": "def filter(self, func=None, **query):\n", "code": "if callable(func):\n    result = filter(func, self)\n    result.insert(0, self.default_columns)\n    return TableFu(result, **self.options)\nelse:\n    result = self\n    for column, value in query.items():\n        result = result.filter(lambda r: r[column] == value)\n    return result", "path": "table_fu\\__init__.py", "repo_name": "eyeseast/python-tablefu", "stars": 232, "license": "mit", "language": "python", "size": 516}
{"docstring": "\"\"\"\nConverts a floating point value a X:1 ratio.\n\nNumber of decimal places set by the `precision` kwarg. Default is one.\n\"\"\"\n", "func_signal": "def ratio(value, decimal_places=0, failure_string='N/A'):\n", "code": "try:\n    f = float(value)\nexcept ValueError:\n    return failure_string\nreturn _saferound(f, decimal_places) + ':1'", "path": "table_fu\\formatting.py", "repo_name": "eyeseast/python-tablefu", "stars": 232, "license": "mit", "language": "python", "size": 516}
{"docstring": "\"\"\"\nConverts a floating point value into a percentage change value.\n\nNumber of decimal places set by the `precision` kwarg. Default is one.\n\nNon-floats are assumed to be zero division errors and are presented as\n'N/A' in the output.\n\nBy default the number is multiplied by 100. You can prevent it from doing\nthat by setting the `multiply` keyword argument to False.\n\"\"\"\n", "func_signal": "def percent_change(value, decimal_places=1, multiply=True, failure_string='N/A'):\n", "code": "try:\n    f = float(value)\n    if multiply:\n        f = f * 100\nexcept ValueError:\n   return  failure_string\ns = _saferound(f, decimal_places)\nif f > 0:\n    return '+' + s + '%'\nelse:\n    return s + '%'", "path": "table_fu\\formatting.py", "repo_name": "eyeseast/python-tablefu", "stars": 232, "license": "mit", "language": "python", "size": 516}
{"docstring": "\"\"\"\nConverts a state's name, or FIPS to its postal abbreviation\n\nExample usage:\n\n    >> ap_state(\"California\")\n    'Calif.'\n\n\"\"\"\n", "func_signal": "def state_postal(value):\n", "code": "try:\n    return statestyle.get(value).postal\nexcept:\n    return value", "path": "table_fu\\formatting.py", "repo_name": "eyeseast/python-tablefu", "stars": 232, "license": "mit", "language": "python", "size": 516}
{"docstring": "\"\"\"\nConverts a state's name, postal abbreviation or FIPS to A.P. style.\n\nExample usage:\n\n    >> ap_state(\"California\")\n    'Calif.'\n\n\"\"\"\n", "func_signal": "def ap_state(value, failure_string=None):\n", "code": "try:\n    return statestyle.get(value).ap\nexcept:\n    if failure_string:\n        return failure_string\n    else:\n        return value", "path": "table_fu\\formatting.py", "repo_name": "eyeseast/python-tablefu", "stars": 232, "license": "mit", "language": "python", "size": 516}
{"docstring": "\"\"\"\nMap a function to rows, or to given columns\n\"\"\"\n", "func_signal": "def map(self, func, *columns):\n", "code": "if not columns:\n    return map(func, self.rows)\nelse:\n    values = (self.values(column) for column in columns)\n    result = [map(func, v) for v in values]\n    if len(columns) == 1:\n        return result[0]\n    else:\n        return result", "path": "table_fu\\__init__.py", "repo_name": "eyeseast/python-tablefu", "stars": 232, "license": "mit", "language": "python", "size": 516}
{"docstring": "\"\"\"\nExport this table as a CSV\n\"\"\"\n", "func_signal": "def csv(self, **kwargs):\n", "code": "out = StringIO()\nwriter = csv.DictWriter(out, self.columns, **kwargs)\nwriter.writerow(dict(zip(self.columns, self.columns)))\nwriter.writerows(dict(row.items()) for row in self.rows)\n\nreturn out", "path": "table_fu\\__init__.py", "repo_name": "eyeseast/python-tablefu", "stars": 232, "license": "mit", "language": "python", "size": 516}
{"docstring": "\"\"\"\nSort rows in this table, preserving a record of how that\nsorting is done in TableFu.options['sorted_by']\n\"\"\"\n", "func_signal": "def sort(self, column_name=None, reverse=False):\n", "code": "if not column_name and self.options.has_key('sorted_by'):\n    column_name = self.options['sorted_by'].keys()[0]\nif column_name not in self.default_columns:\n    raise ValueError(\"%s isn't a column in this table\" % column_name)\nindex = self.default_columns.index(column_name)\nself.table.sort(key = lambda row: row[index], reverse=reverse)\nself.options['sorted_by'] = {column_name: {'reverse': reverse}}", "path": "table_fu\\__init__.py", "repo_name": "eyeseast/python-tablefu", "stars": 232, "license": "mit", "language": "python", "size": 516}
{"docstring": "\"\"\"\nCapitalizes the first character of the value.\n\nIf the submitted value isn't a string, returns the `failure_string` keyword\nargument.\n\nCribbs from django's default filter set\n\"\"\"\n", "func_signal": "def capfirst(value, failure_string='N/A'):\n", "code": "try:\n    value = value.lower()\n    return value[0].upper() + value[1:]\nexcept:\n    return failure_string", "path": "table_fu\\formatting.py", "repo_name": "eyeseast/python-tablefu", "stars": 232, "license": "mit", "language": "python", "size": 516}
{"docstring": "\"\"\"\nReturn the Datum for column_name, or default.\n\"\"\"\n", "func_signal": "def get(self, column_name, default=None):\n", "code": "if column_name in self.table.default_columns:\n    index = self.table.default_columns.index(column_name)\n    return Datum(self.cells[index], self.row_num, column_name, self.table)\nreturn default", "path": "table_fu\\__init__.py", "repo_name": "eyeseast/python-tablefu", "stars": 232, "license": "mit", "language": "python", "size": 516}
{"docstring": "\"\"\"\nWith pysqlite 2.4.0 you needed to use a string or a APSW connection\nobject for opening database connections.\n\nFormerly, both bytestrings and unicode strings used to work.\n\nLet's make sure unicode strings work in the future.\n\"\"\"\n", "func_signal": "def CheckUnicodeConnect(self):\n", "code": "con = sqlite.connect(u\":memory:\")\ncon.close()", "path": "sqlite3\\test\\regression.py", "repo_name": "pyqteval/evlal_win", "stars": 190, "license": "None", "language": "python", "size": 36373}
{"docstring": "\"\"\"\nCreate a \"mix-in\" class with a python type and a\nPackable with the given struct format\n\"\"\"\n", "func_signal": "def pypackable(name, pytype, format):\n", "code": "size, items = _formatinfo(format)\nreturn type(Packable)(name, (pytype, Packable), {\n    '_format_': format,\n    '_size_': size,\n    '_items_': items,\n})", "path": "pyinstaller-2.0\\PyInstaller\\lib\\macholib\\ptypes.py", "repo_name": "pyqteval/evlal_win", "stars": 190, "license": "None", "language": "python", "size": 36373}
{"docstring": "# pysqlite 2.1.0 to 2.2.0 have the problem that not all statements are\n# reset before a rollback, but only those that are still in the\n# statement cache. The others are not accessible from the connection object.\n", "func_signal": "def CheckStatementReset(self):\n", "code": "con = sqlite.connect(\":memory:\", cached_statements=5)\ncursors = [con.cursor() for x in xrange(5)]\ncursors[0].execute(\"create table test(x)\")\nfor i in range(10):\n    cursors[0].executemany(\"insert into test(x) values (?)\", [(x,) for x in xrange(10)])\n\nfor i in range(5):\n    cursors[i].execute(\" \" * i + \"select x from test\")\n\ncon.rollback()", "path": "sqlite3\\test\\regression.py", "repo_name": "pyqteval/evlal_win", "stars": 190, "license": "None", "language": "python", "size": 36373}
{"docstring": "# isolation level is a string, not an integer\n", "func_signal": "def CheckInvalidIsolationLevelType(self):\n", "code": "self.assertRaises(TypeError,\n                  sqlite.connect, \":memory:\", isolation_level=123)", "path": "sqlite3\\test\\regression.py", "repo_name": "pyqteval/evlal_win", "stars": 190, "license": "None", "language": "python", "size": 36373}
{"docstring": "\"\"\"Like iteritems(), but with all lowercase keys.\"\"\"\n", "func_signal": "def lower_items(self):\n", "code": "return (\n    (lowerkey, keyval[1])\n    for (lowerkey, keyval)\n    in self._store.items()\n)", "path": "Clicked_Controller\\requests\\structures.py", "repo_name": "pyqteval/evlal_win", "stars": 190, "license": "None", "language": "python", "size": 36373}
{"docstring": "\"\"\"\nReturn the size of an object when packed\n\"\"\"\n", "func_signal": "def sizeof(s):\n", "code": "if hasattr(s, '_size_'):\n    return s._size_\n\nelif isinstance(s, bytes):\n    return len(s)\n\nraise ValueError(s)", "path": "pyinstaller-2.0\\PyInstaller\\lib\\macholib\\ptypes.py", "repo_name": "pyqteval/evlal_win", "stars": 190, "license": "None", "language": "python", "size": 36373}
{"docstring": "# This used to crash pysqlite because this pragma command returns NULL for the column name\n", "func_signal": "def CheckPragmaUserVersion(self):\n", "code": "cur = self.con.cursor()\ncur.execute(\"pragma user_version\")", "path": "sqlite3\\test\\regression.py", "repo_name": "pyqteval/evlal_win", "stars": 190, "license": "None", "language": "python", "size": 36373}
{"docstring": "\"\"\"\nhttp://bugs.python.org/issue10811\n\nRecursively using a cursor, such as when reusing it from a generator led to segfaults.\nNow we catch recursive cursor usage and raise a ProgrammingError.\n\"\"\"\n", "func_signal": "def CheckRecursiveCursorUse(self):\n", "code": "con = sqlite.connect(\":memory:\")\n\ncur = con.cursor()\ncur.execute(\"create table a (bar)\")\ncur.execute(\"create table b (baz)\")\n\ndef foo():\n    cur.execute(\"insert into a (bar) values (?)\", (1,))\n    yield 1\n\nwith self.assertRaises(sqlite.ProgrammingError):\n    cur.executemany(\"insert into b (baz) values (?)\",\n                    ((i,) for i in foo()))", "path": "sqlite3\\test\\regression.py", "repo_name": "pyqteval/evlal_win", "stars": 190, "license": "None", "language": "python", "size": 36373}
{"docstring": "\"\"\"\nCalculate the size and number of items in a struct format.\n\"\"\"\n", "func_signal": "def _formatinfo(format):\n", "code": "size = struct.calcsize(format)\nreturn size, len(struct.unpack(format, b'\\x00' * size))", "path": "pyinstaller-2.0\\PyInstaller\\lib\\macholib\\ptypes.py", "repo_name": "pyqteval/evlal_win", "stars": 190, "license": "None", "language": "python", "size": 36373}
{"docstring": "# This still crashed pysqlite <= 2.2.1\n", "func_signal": "def CheckPragmaSchemaVersion(self):\n", "code": "con = sqlite.connect(\":memory:\", detect_types=sqlite.PARSE_COLNAMES)\ntry:\n    cur = self.con.cursor()\n    cur.execute(\"pragma schema_version\")\nfinally:\n    cur.close()\n    con.close()", "path": "sqlite3\\test\\regression.py", "repo_name": "pyqteval/evlal_win", "stars": 190, "license": "None", "language": "python", "size": 36373}
{"docstring": "\"\"\"\nVerifies that cursor methods check whether base class __init__ was\ncalled.\n\"\"\"\n", "func_signal": "def CheckCursorConstructorCallCheck(self):\n", "code": "class Cursor(sqlite.Cursor):\n    def __init__(self, con):\n        pass\n\ncon = sqlite.connect(\":memory:\")\ncur = Cursor(con)\ntry:\n    cur.execute(\"select 4+5\").fetchall()\n    self.fail(\"should have raised ProgrammingError\")\nexcept sqlite.ProgrammingError:\n    pass\nexcept:\n    self.fail(\"should have raised ProgrammingError\")", "path": "sqlite3\\test\\regression.py", "repo_name": "pyqteval/evlal_win", "stars": 190, "license": "None", "language": "python", "size": 36373}
{"docstring": "\"\"\"\nVerifies that running a PRAGMA statement that does an autocommit does\nwork. This did not work in 2.5.3/2.5.4.\n\"\"\"\n", "func_signal": "def CheckPragmaAutocommit(self):\n", "code": "cur = self.con.cursor()\ncur.execute(\"create table foo(bar)\")\ncur.execute(\"insert into foo(bar) values (5)\")\n\ncur.execute(\"pragma page_size\")\nrow = cur.fetchone()", "path": "sqlite3\\test\\regression.py", "repo_name": "pyqteval/evlal_win", "stars": 190, "license": "None", "language": "python", "size": 36373}
{"docstring": "\"\"\"\nVerifies that subclassed cursor classes are correctly registered with\nthe connection object, too.  (fetch-across-rollback problem)\n\"\"\"\n", "func_signal": "def CheckCursorRegistration(self):\n", "code": "class Connection(sqlite.Connection):\n    def cursor(self):\n        return Cursor(self)\n\nclass Cursor(sqlite.Cursor):\n    def __init__(self, con):\n        sqlite.Cursor.__init__(self, con)\n\ncon = Connection(\":memory:\")\ncur = con.cursor()\ncur.execute(\"create table foo(x)\")\ncur.executemany(\"insert into foo(x) values (?)\", [(3,), (4,), (5,)])\ncur.execute(\"select x from foo\")\ncon.rollback()\ntry:\n    cur.fetchall()\n    self.fail(\"should have raised InterfaceError\")\nexcept sqlite.InterfaceError:\n    pass\nexcept:\n    self.fail(\"should have raised InterfaceError\")", "path": "sqlite3\\test\\regression.py", "repo_name": "pyqteval/evlal_win", "stars": 190, "license": "None", "language": "python", "size": 36373}
{"docstring": "\"\"\"\npysqlite would crash with older SQLite versions unless\na workaround is implemented.\n\"\"\"\n", "func_signal": "def CheckWorkaroundForBuggySqliteTransferBindings(self):\n", "code": "self.con.execute(\"create table foo(bar)\")\nself.con.execute(\"drop table foo\")\nself.con.execute(\"create table foo(bar)\")", "path": "sqlite3\\test\\regression.py", "repo_name": "pyqteval/evlal_win", "stars": 190, "license": "None", "language": "python", "size": 36373}
{"docstring": "\"\"\"\nSee http://bugs.python.org/issue7478\n\nIt was possible to successfully register callbacks that could not be\nhashed. Return codes of PyDict_SetItem were not checked properly.\n\"\"\"\n", "func_signal": "def CheckSetDict(self):\n", "code": "class NotHashable:\n    def __call__(self, *args, **kw):\n        pass\n    def __hash__(self):\n        raise TypeError()\nvar = NotHashable()\nself.assertRaises(TypeError, self.con.create_function, var)\nself.assertRaises(TypeError, self.con.create_aggregate, var)\nself.assertRaises(TypeError, self.con.set_authorizer, var)\nself.assertRaises(TypeError, self.con.set_progress_handler, var)", "path": "sqlite3\\test\\regression.py", "repo_name": "pyqteval/evlal_win", "stars": 190, "license": "None", "language": "python", "size": 36373}
{"docstring": "# Issue #21147\n", "func_signal": "def CheckNullCharacter(self):\n", "code": "con = sqlite.connect(\":memory:\")\nself.assertRaises(ValueError, con, \"\\0select 1\")\nself.assertRaises(ValueError, con, \"select 1\\0\")\ncur = con.cursor()\nself.assertRaises(ValueError, cur.execute, \" \\0select 2\")\nself.assertRaises(ValueError, cur.execute, \"select 2\\0\")", "path": "sqlite3\\test\\regression.py", "repo_name": "pyqteval/evlal_win", "stars": 190, "license": "None", "language": "python", "size": 36373}
{"docstring": "\"\"\"\nVerifies that connection methods check whether base class __init__ was\ncalled.\n\"\"\"\n", "func_signal": "def CheckConnectionConstructorCallCheck(self):\n", "code": "class Connection(sqlite.Connection):\n    def __init__(self, name):\n        pass\n\ncon = Connection(\":memory:\")\ntry:\n    cur = con.cursor()\n    self.fail(\"should have raised ProgrammingError\")\nexcept sqlite.ProgrammingError:\n    pass\nexcept:\n    self.fail(\"should have raised ProgrammingError\")", "path": "sqlite3\\test\\regression.py", "repo_name": "pyqteval/evlal_win", "stars": 190, "license": "None", "language": "python", "size": 36373}
{"docstring": "\"\"\"\nSee issue 3312.\n\"\"\"\n", "func_signal": "def CheckSetIsolationLevel(self):\n", "code": "con = sqlite.connect(\":memory:\")\nself.assertRaises(UnicodeEncodeError, setattr, con,\n                  \"isolation_level\", u\"\\xe9\")", "path": "sqlite3\\test\\regression.py", "repo_name": "pyqteval/evlal_win", "stars": 190, "license": "None", "language": "python", "size": 36373}
{"docstring": "\"\"\"\npysqlite until 2.4.1 did not rebuild the row_cast_map when recompiling\na statement. This test exhibits the problem.\n\"\"\"\n", "func_signal": "def CheckTypeMapUsage(self):\n", "code": "SELECT = \"select * from foo\"\ncon = sqlite.connect(\":memory:\",detect_types=sqlite.PARSE_DECLTYPES)\ncon.execute(\"create table foo(bar timestamp)\")\ncon.execute(\"insert into foo(bar) values (?)\", (datetime.datetime.now(),))\ncon.execute(SELECT)\ncon.execute(\"drop table foo\")\ncon.execute(\"create table foo(bar integer)\")\ncon.execute(\"insert into foo(bar) values (5)\")\ncon.execute(SELECT)", "path": "sqlite3\\test\\regression.py", "repo_name": "pyqteval/evlal_win", "stars": 190, "license": "None", "language": "python", "size": 36373}
{"docstring": "# pysqlite versions <= 2.3.3 only finalized statements in the statement\n# cache when closing the database. statements that were still\n# referenced in cursors weren't closed an could provoke \"\n# \"OperationalError: Unable to close due to unfinalised statements\".\n", "func_signal": "def CheckStatementFinalizationOnCloseDb(self):\n", "code": "con = sqlite.connect(\":memory:\")\ncursors = []\n# default statement cache size is 100\nfor i in range(105):\n    cur = con.cursor()\n    cursors.append(cur)\n    cur.execute(\"select 1 x union select \" + str(i))\ncon.close()", "path": "sqlite3\\test\\regression.py", "repo_name": "pyqteval/evlal_win", "stars": 190, "license": "None", "language": "python", "size": 36373}
{"docstring": "\"\"\"Create a refactoring object for inlining\n\nBased on `resource` and `offset` it returns an instance of\n`InlineMethod`, `InlineVariable` or `InlineParameter`.\n\n\"\"\"\n", "func_signal": "def create_inline(project, resource, offset):\n", "code": "pycore = project.pycore\npyname = _get_pyname(pycore, resource, offset)\nmessage = 'Inline refactoring should be performed on ' \\\n          'a method, local variable or parameter.'\nif pyname is None:\n    raise rope.base.exceptions.RefactoringError(message)\nif isinstance(pyname, pynames.ImportedName):\n    pyname = pyname._get_imported_pyname()\nif isinstance(pyname, pynames.AssignedName):\n    return InlineVariable(project, resource, offset)\nif isinstance(pyname, pynames.ParameterName):\n    return InlineParameter(project, resource, offset)\nif isinstance(pyname.get_object(), pyobjects.PyFunction):\n    return InlineMethod(project, resource, offset)\nelse:\n    raise rope.base.exceptions.RefactoringError(message)", "path": "rope\\refactor\\inline.py", "repo_name": "JulianEberius/SublimeRope", "stars": 251, "license": "gpl-2.0", "language": "python", "size": 526}
{"docstring": "# A header is created which initializes parameters\n# to the values passed to the function.\n", "func_signal": "def _calculate_header(self, primary, pyname, call):\n", "code": "call_info = rope.refactor.functionutils.CallInfo.read(\n    primary, pyname, self.definition_info, call)\nparamdict = self.definition_params\nmapping = rope.refactor.functionutils.ArgumentMapping(\n    self.definition_info, call_info)\nfor param_name, value in mapping.param_dict.items():\n    paramdict[param_name] = value\nheader = ''\nto_be_inlined = []\nmod = self.pycore.get_string_module(self.body)\nall_names = mod.get_scope().get_names()\nassigned_names = [name for name in all_names if\n    isinstance(all_names[name], rope.base.pynamesdef.AssignedName)]\nfor name, value in paramdict.items():\n    if name != value and value is not None:\n        header += name + ' = ' + value.replace('\\n', ' ') + '\\n'\n        to_be_inlined.append(name)\nreturn header, to_be_inlined", "path": "rope\\refactor\\inline.py", "repo_name": "JulianEberius/SublimeRope", "stars": 251, "license": "gpl-2.0", "language": "python", "size": 526}
{"docstring": "\"\"\"Transform a `PyObject` to textual form\"\"\"\n", "func_signal": "def transform(self, pyobject):\n", "code": "if pyobject is None:\n    return ('none',)\nobject_type = type(pyobject)\ntry:\n    method = getattr(self, object_type.__name__ + '_to_textual')\n    return method(pyobject)\nexcept AttributeError:\n    return ('unknown',)", "path": "rope\\base\\oi\\transform.py", "repo_name": "JulianEberius/SublimeRope", "stars": 251, "license": "gpl-2.0", "language": "python", "size": 526}
{"docstring": "\"\"\"Return the offset in which the completion should be inserted\n\nUsually code assist proposals should be inserted like::\n\n    completion = proposal.name\n    result = (source_code[:starting_offset] +\n              completion + source_code[offset:])\n\nWhere starting_offset is the offset returned by this function.\n\n\"\"\"\n", "func_signal": "def starting_offset(source_code, offset):\n", "code": "word_finder = worder.Worder(source_code, True)\nexpression, starting, starting_offset = \\\n    word_finder.get_splitted_primary_before(offset)\nreturn starting_offset", "path": "rope\\contrib\\codeassist.py", "repo_name": "JulianEberius/SublimeRope", "stars": 251, "license": "gpl-2.0", "language": "python", "size": 526}
{"docstring": "\"\"\"Get a string representation of a param's default value.\n\nReturns None if there is no default value for this param.\n\"\"\"\n", "func_signal": "def get_default(self):\n", "code": "definfo = functionutils.DefinitionInfo.read(self._function)\nfor arg, default in definfo.args_with_defaults:\n    if self.argname == arg:\n        return default\nreturn None", "path": "rope\\contrib\\codeassist.py", "repo_name": "JulianEberius/SublimeRope", "stars": 251, "license": "gpl-2.0", "language": "python", "size": 526}
{"docstring": "\"\"\"The sample code from :PEP:`257`\"\"\"\n", "func_signal": "def _trim_docstring(self, docstring, indents=0):\n", "code": "if not docstring:\n    return ''\n# Convert tabs to spaces (following normal Python rules)\n# and split into a list of lines:\nlines = docstring.expandtabs().splitlines()\n# Determine minimum indentation (first line doesn't count):\nindent = sys.maxint\nfor line in lines[1:]:\n    stripped = line.lstrip()\n    if stripped:\n        indent = min(indent, len(line) - len(stripped))\n# Remove indentation (first line is special):\ntrimmed = [lines[0].strip()]\nif indent < sys.maxint:\n    for line in lines[1:]:\n        trimmed.append(line[indent:].rstrip())\n# Strip off trailing and leading blank lines:\nwhile trimmed and not trimmed[-1]:\n    trimmed.pop()\nwhile trimmed and not trimmed[0]:\n    trimmed.pop(0)\n# Return a single string:\nreturn '\\n'.join((' ' * indents + line for line in trimmed))", "path": "rope\\contrib\\codeassist.py", "repo_name": "JulianEberius/SublimeRope", "stars": 251, "license": "gpl-2.0", "language": "python", "size": 526}
{"docstring": "\"\"\"The names of the parameters the function takes.\n\nReturns None if this completion is not a function.\n\"\"\"\n", "func_signal": "def parameters(self):\n", "code": "pyname = self.pyname\nif isinstance(pyname, pynames.ImportedName):\n    pyname = pyname._get_imported_pyname()\nif isinstance(pyname, pynames.DefinedName):\n    pyobject = pyname.get_object()\n    if isinstance(pyobject, pyobjects.AbstractFunction):\n        return pyobject.get_param_names()", "path": "rope\\contrib\\codeassist.py", "repo_name": "JulianEberius/SublimeRope", "stars": 251, "license": "gpl-2.0", "language": "python", "size": 526}
{"docstring": "# caching already calculated definitions\n", "func_signal": "def get_definition(self, primary, pyname, call, host_vars=[],returns=False):\n", "code": "return self._calculate_definition(primary, pyname, call,\n                   host_vars, returns)", "path": "rope\\refactor\\inline.py", "repo_name": "JulianEberius/SublimeRope", "stars": 251, "license": "gpl-2.0", "language": "python", "size": 526}
{"docstring": "\"\"\"Construct an `ArgumentReorderer`\n\nNote that the `new_order` is a list containing the new\nposition of parameters; not the position each parameter\nis going to be moved to. (changed in ``0.5m4``)\n\nFor example changing ``f(a, b, c)`` to ``f(c, a, b)``\nrequires passing ``[2, 0, 1]`` and *not* ``[1, 2, 0]``.\n\nThe `autodef` (automatic default) argument, forces rope to use\nit as a default if a default is needed after the change.  That\nhappens when an argument without default is moved after\nanother that has a default value.  Note that `autodef` should\nbe a string or `None`; the latter disables adding automatic\ndefault.\n\n\"\"\"\n", "func_signal": "def __init__(self, new_order, autodef=None):\n", "code": "self.new_order = new_order\nself.autodef = autodef", "path": "rope\\refactor\\change_signature.py", "repo_name": "JulianEberius/SublimeRope", "stars": 251, "license": "gpl-2.0", "language": "python", "size": 526}
{"docstring": "\"\"\"Transform an object from textual form to `PyObject`\"\"\"\n", "func_signal": "def transform(self, textual):\n", "code": "if textual is None:\n    return None\ntype = textual[0]\ntry:\n    method = getattr(self, type + '_to_pyobject')\n    return method(textual)\nexcept AttributeError:\n    return None", "path": "rope\\base\\oi\\transform.py", "repo_name": "JulianEberius/SublimeRope", "stars": 251, "license": "gpl-2.0", "language": "python", "size": 526}
{"docstring": "\"\"\"Return the expression to complete\"\"\"\n", "func_signal": "def starting_expression(source_code, offset):\n", "code": "word_finder = worder.Worder(source_code, True)\nexpression, starting, starting_offset = \\\n    word_finder.get_splitted_primary_before(offset)\nif expression:\n    return expression + '.' + starting\nreturn starting", "path": "rope\\contrib\\codeassist.py", "repo_name": "JulianEberius/SublimeRope", "stars": 251, "license": "gpl-2.0", "language": "python", "size": 526}
{"docstring": "\"\"\"Return not `ast.expr_context` children of `node`\"\"\"\n", "func_signal": "def _get_children(self, node):\n", "code": "children = ast.get_children(node)\nreturn [child for child in children\n        if not isinstance(child, ast.expr_context)]", "path": "rope\\refactor\\similarfinder.py", "repo_name": "JulianEberius/SublimeRope", "stars": 251, "license": "gpl-2.0", "language": "python", "size": 526}
{"docstring": "\"\"\"Search for `code` in source and return a list of `Match`\\es\n\n`code` can contain wildcards.  ``${name}`` matches normal\nnames and ``${?name} can match any expression.  You can use\n`Match.get_ast()` for getting the node that has matched a\ngiven pattern.\n\n\"\"\"\n", "func_signal": "def get_matches(self, code, start=0, end=None, skip=None):\n", "code": "if end is None:\n    end = len(self.source)\nfor match in self._get_matched_asts(code):\n    match_start, match_end = match.get_region()\n    if start <= match_start and match_end <= end:\n        if skip is not None and (skip[0] < match_end and\n                                 skip[1] > match_start):\n            continue                    \n        yield match", "path": "rope\\refactor\\similarfinder.py", "repo_name": "JulianEberius/SublimeRope", "stars": 251, "license": "gpl-2.0", "language": "python", "size": 526}
{"docstring": "\"\"\"Get the pydoc\"\"\"\n", "func_signal": "def get_doc(project, source_code, offset, resource=None, maxfixes=1):\n", "code": "fixer = fixsyntax.FixSyntax(project.pycore, source_code,\n                            resource, maxfixes)\npymodule = fixer.get_pymodule()\npyname = fixer.pyname_at(offset)\nif pyname is None:\n    return None\npyobject = pyname.get_object()\nreturn PyDocExtractor().get_doc(pyobject)", "path": "rope\\contrib\\codeassist.py", "repo_name": "JulianEberius/SublimeRope", "stars": 251, "license": "gpl-2.0", "language": "python", "size": 526}
{"docstring": "\"\"\"Return a list of `CodeAssistProposal`\"\"\"\n", "func_signal": "def get_sorted_proposal_list(self):\n", "code": "proposals = {}\nfor proposal in self.proposals:\n    proposals.setdefault(proposal.scope, []).append(proposal)\nresult = []\nfor scope in self.scopepref:\n    scope_proposals = proposals.get(scope, [])\n    scope_proposals = [proposal for proposal in scope_proposals\n                      if proposal.type in self.typerank]\n    scope_proposals.sort(self._proposal_cmp)\n    result.extend(scope_proposals)\nreturn result", "path": "rope\\contrib\\codeassist.py", "repo_name": "JulianEberius/SublimeRope", "stars": 251, "license": "gpl-2.0", "language": "python", "size": 526}
{"docstring": "\"\"\"Searches the given pattern in the body AST.\n\nbody is an AST node and pattern can be either an AST node or\na list of ASTs nodes\n\"\"\"\n", "func_signal": "def __init__(self, body, pattern, does_match):\n", "code": "self.body = body\nself.pattern = pattern\nself.matches = None\nself.ropevar = _RopeVariable()\nself.matches_callback = does_match", "path": "rope\\refactor\\similarfinder.py", "repo_name": "JulianEberius/SublimeRope", "stars": 251, "license": "gpl-2.0", "language": "python", "size": 526}
{"docstring": "\"\"\"Get the proposed object's docstring.\n\nReturns None if it can not be get.\n\"\"\"\n", "func_signal": "def get_doc(self):\n", "code": "if not self.pyname:\n    return None\npyobject = self.pyname.get_object()\nif not hasattr(pyobject, 'get_doc'):\n    return None\nreturn self.pyname.get_object().get_doc()", "path": "rope\\contrib\\codeassist.py", "repo_name": "JulianEberius/SublimeRope", "stars": 251, "license": "gpl-2.0", "language": "python", "size": 526}
{"docstring": "\"\"\"Find possible bad name and attribute accesses\n\nIt returns a list of `Error`\\s.\n\"\"\"\n", "func_signal": "def find_errors(project, resource):\n", "code": "pymodule = project.pycore.resource_to_pyobject(resource)\nfinder = _BadAccessFinder(pymodule)\nast.walk(pymodule.get_ast(), finder)\nreturn finder.errors", "path": "rope\\contrib\\finderrors.py", "repo_name": "JulianEberius/SublimeRope", "stars": 251, "license": "gpl-2.0", "language": "python", "size": 526}
{"docstring": "\"\"\"Sort a list of proposals\n\nReturn a sorted list of the given `CodeAssistProposal`\\s.\n\n`scopepref` can be a list of proposal scopes.  Defaults to\n``['parameter_keyword', 'local', 'global', 'imported',\n'attribute', 'builtin', 'keyword']``.\n\n`typepref` can be a list of proposal types.  Defaults to\n``['class', 'function', 'instance', 'module', None]``.\n(`None` stands for completions with no type like keywords.)\n\"\"\"\n", "func_signal": "def sorted_proposals(proposals, scopepref=None, typepref=None):\n", "code": "sorter = _ProposalSorter(proposals, scopepref, typepref)\nreturn sorter.get_sorted_proposal_list()", "path": "rope\\contrib\\codeassist.py", "repo_name": "JulianEberius/SublimeRope", "stars": 251, "license": "gpl-2.0", "language": "python", "size": 526}
{"docstring": "\"\"\"Construct a SimilarFinder\"\"\"\n", "func_signal": "def __init__(self, pymodule, wildcards=None):\n", "code": "self.source = pymodule.source_code\nself.raw_finder = RawSimilarFinder(\n    pymodule.source_code, pymodule.get_ast(), self._does_match)\nself.pymodule = pymodule\nif wildcards is None:\n    self.wildcards = {}\n    for wildcard in [rope.refactor.wildcards.\n                     DefaultWildcard(pymodule.pycore.project)]:\n        self.wildcards[wildcard.get_name()] = wildcard\nelse:\n    self.wildcards = wildcards", "path": "rope\\refactor\\similarfinder.py", "repo_name": "JulianEberius/SublimeRope", "stars": 251, "license": "gpl-2.0", "language": "python", "size": 526}
{"docstring": "\"\"\"Return the file cache path based on the URL.\n\nThis does not ensure the file exists!\n\"\"\"\n", "func_signal": "def url_to_file_path(url, filecache):\n", "code": "key = CacheController.cache_url(url)\nreturn filecache._fn(key)", "path": "lib\\python2.7\\site-packages\\pip\\_vendor\\cachecontrol\\caches\\file_cache.py", "repo_name": "llSourcell/AI_Artist", "stars": 237, "license": "None", "language": "python", "size": 67954}
{"docstring": "# Make a new comment token and give it as value all the characters\n# until the first > or EOF (charsUntil checks for EOF automatically)\n# and emit it.\n", "func_signal": "def bogusCommentState(self):\n", "code": "data = self.stream.charsUntil(\">\")\ndata = data.replace(\"\\u0000\", \"\\uFFFD\")\nself.tokenQueue.append(\n    {\"type\": tokenTypes[\"Comment\"], \"data\": data})\n\n# Eat the character directly after the bogus comment which is either a\n# \">\" or an EOF.\nself.stream.char()\nself.state = self.dataState\nreturn True", "path": "lib\\python2.7\\site-packages\\pip\\_vendor\\html5lib\\tokenizer.py", "repo_name": "llSourcell/AI_Artist", "stars": 237, "license": "None", "language": "python", "size": 67954}
{"docstring": "# Hopefully unnecessary for symlink.\n# try:\n#     open(self.unique_name, \"wb\").close()\n# except IOError:\n#     raise LockFailed(\"failed to create %s\" % self.unique_name)\n", "func_signal": "def acquire(self, timeout=None):\n", "code": "timeout = timeout if timeout is not None else self.timeout\nend_time = time.time()\nif timeout is not None and timeout > 0:\n    end_time += timeout\n\nwhile True:\n    # Try and create a symbolic link to it.\n    try:\n        os.symlink(self.unique_name, self.lock_file)\n    except OSError:\n        # Link creation failed.  Maybe we've double-locked?\n        if self.i_am_locking():\n            # Linked to out unique name. Proceed.\n            return\n        else:\n            # Otherwise the lock creation failed.\n            if timeout is not None and time.time() > end_time:\n                if timeout > 0:\n                    raise LockTimeout(\"Timeout waiting to acquire\"\n                                      \" lock for %s\" %\n                                      self.path)\n                else:\n                    raise AlreadyLocked(\"%s is already locked\" %\n                                        self.path)\n            time.sleep(timeout / 10 if timeout is not None else 0.1)\n    else:\n        # Link creation succeeded.  We're good to go.\n        return", "path": "lib\\python2.7\\site-packages\\pip\\_vendor\\lockfile\\symlinklockfile.py", "repo_name": "llSourcell/AI_Artist", "stars": 237, "license": "None", "language": "python", "size": 67954}
{"docstring": "# See of the distribution option has been set, otherwise check the\n# setuptools default.\n", "func_signal": "def run_2to3(self, files, doctests = False):\n", "code": "if self.distribution.use_2to3 is not True:\n    return\nif not files:\n    return\nlog.info(\"Fixing \"+\" \".join(files))\nself.__build_fixer_names()\nself.__exclude_fixers()\nif doctests:\n    if setuptools.run_2to3_on_doctests:\n        r = DistutilsRefactoringTool(self.fixer_names)\n        r.refactor(files, write=True, doctests_only=True)\nelse:\n    _Mixin2to3.run_2to3(self, files)", "path": "lib\\python2.7\\site-packages\\setuptools\\lib2to3_ex.py", "repo_name": "llSourcell/AI_Artist", "stars": 237, "license": "None", "language": "python", "size": 67954}
{"docstring": "\"\"\" This is where the magic happens.\n\nWe do our usually processing through the states and when we have a token\nto return we yield the token which pauses processing until the next token\nis requested.\n\"\"\"\n", "func_signal": "def __iter__(self):\n", "code": "self.tokenQueue = deque([])\n# Start processing. When EOF is reached self.state will return False\n# instead of True and the loop will terminate.\nwhile self.state():\n    while self.stream.errors:\n        yield {\"type\": tokenTypes[\"ParseError\"], \"data\": self.stream.errors.pop(0)}\n    while self.tokenQueue:\n        yield self.tokenQueue.popleft()", "path": "lib\\python2.7\\site-packages\\pip\\_vendor\\html5lib\\tokenizer.py", "repo_name": "llSourcell/AI_Artist", "stars": 237, "license": "None", "language": "python", "size": 67954}
{"docstring": "\"\"\"\nEnsure that the given path is decoded,\nNONE when no expected encoding works\n\"\"\"\n\n", "func_signal": "def filesys_decode(path):\n", "code": "if isinstance(path, six.text_type):\n    return path\n\nfs_enc = sys.getfilesystemencoding() or 'utf-8'\ncandidates = fs_enc, 'utf-8'\n\nfor enc in candidates:\n    try:\n        return path.decode(enc)\n    except UnicodeDecodeError:\n        continue", "path": "lib\\python2.7\\site-packages\\setuptools\\unicode_utils.py", "repo_name": "llSourcell/AI_Artist", "stars": 237, "license": "None", "language": "python", "size": 67954}
{"docstring": "\"\"\"\nStrips comments and filter empty lines.\n\"\"\"\n", "func_signal": "def ignore_comments(lines_enum):\n", "code": "for line_number, line in lines_enum:\n    line = COMMENT_RE.sub('', line)\n    line = line.strip()\n    if line:\n        yield line_number, line", "path": "lib\\python2.7\\site-packages\\pip\\req\\req_file.py", "repo_name": "llSourcell/AI_Artist", "stars": 237, "license": "None", "language": "python", "size": 67954}
{"docstring": "\"\"\"Wrapper around six.text_type to convert None to empty string\"\"\"\n", "func_signal": "def to_text(s, blank_if_none=True):\n", "code": "if s is None:\n    if blank_if_none:\n        return \"\"\n    else:\n        return None\nelif isinstance(s, text_type):\n    return s\nelse:\n    return text_type(s)", "path": "lib\\python2.7\\site-packages\\pip\\_vendor\\html5lib\\treewalkers\\_base.py", "repo_name": "llSourcell/AI_Artist", "stars": 237, "license": "None", "language": "python", "size": 67954}
{"docstring": "\"\"\"\nSkip lines that match '--skip-requirements-regex' pattern\n\nNote: the regex pattern is only built once\n\"\"\"\n", "func_signal": "def skip_regex(lines_enum, options):\n", "code": "skip_regex = options.skip_requirements_regex if options else None\nif skip_regex:\n    pattern = re.compile(skip_regex)\n    lines_enum = filterfalse(\n        lambda e: pattern.search(e[1]),\n        lines_enum)\nreturn lines_enum", "path": "lib\\python2.7\\site-packages\\pip\\req\\req_file.py", "repo_name": "llSourcell/AI_Artist", "stars": 237, "license": "None", "language": "python", "size": 67954}
{"docstring": "\"\"\"This function returns either U+FFFD or the character based on the\ndecimal or hexadecimal representation. It also discards \";\" if present.\nIf not present self.tokenQueue.append({\"type\": tokenTypes[\"ParseError\"]}) is invoked.\n\"\"\"\n\n", "func_signal": "def consumeNumberEntity(self, isHex):\n", "code": "allowed = digits\nradix = 10\nif isHex:\n    allowed = hexDigits\n    radix = 16\n\ncharStack = []\n\n# Consume all the characters that are in range while making sure we\n# don't hit an EOF.\nc = self.stream.char()\nwhile c in allowed and c is not EOF:\n    charStack.append(c)\n    c = self.stream.char()\n\n# Convert the set of characters consumed to an int.\ncharAsInt = int(\"\".join(charStack), radix)\n\n# Certain characters get replaced with others\nif charAsInt in replacementCharacters:\n    char = replacementCharacters[charAsInt]\n    self.tokenQueue.append({\"type\": tokenTypes[\"ParseError\"], \"data\":\n                            \"illegal-codepoint-for-numeric-entity\",\n                            \"datavars\": {\"charAsInt\": charAsInt}})\nelif ((0xD800 <= charAsInt <= 0xDFFF) or\n      (charAsInt > 0x10FFFF)):\n    char = \"\\uFFFD\"\n    self.tokenQueue.append({\"type\": tokenTypes[\"ParseError\"], \"data\":\n                            \"illegal-codepoint-for-numeric-entity\",\n                            \"datavars\": {\"charAsInt\": charAsInt}})\nelse:\n    # Should speed up this check somehow (e.g. move the set to a constant)\n    if ((0x0001 <= charAsInt <= 0x0008) or\n        (0x000E <= charAsInt <= 0x001F) or\n        (0x007F <= charAsInt <= 0x009F) or\n        (0xFDD0 <= charAsInt <= 0xFDEF) or\n        charAsInt in frozenset([0x000B, 0xFFFE, 0xFFFF, 0x1FFFE,\n                                0x1FFFF, 0x2FFFE, 0x2FFFF, 0x3FFFE,\n                                0x3FFFF, 0x4FFFE, 0x4FFFF, 0x5FFFE,\n                                0x5FFFF, 0x6FFFE, 0x6FFFF, 0x7FFFE,\n                                0x7FFFF, 0x8FFFE, 0x8FFFF, 0x9FFFE,\n                                0x9FFFF, 0xAFFFE, 0xAFFFF, 0xBFFFE,\n                                0xBFFFF, 0xCFFFE, 0xCFFFF, 0xDFFFE,\n                                0xDFFFF, 0xEFFFE, 0xEFFFF, 0xFFFFE,\n                                0xFFFFF, 0x10FFFE, 0x10FFFF])):\n        self.tokenQueue.append({\"type\": tokenTypes[\"ParseError\"],\n                                \"data\":\n                                \"illegal-codepoint-for-numeric-entity\",\n                                \"datavars\": {\"charAsInt\": charAsInt}})\n    try:\n        # Try/except needed as UCS-2 Python builds' unichar only works\n        # within the BMP.\n        char = chr(charAsInt)\n    except ValueError:\n        v = charAsInt - 0x10000\n        char = chr(0xD800 | (v >> 10)) + chr(0xDC00 | (v & 0x3FF))\n\n# Discard the ; if present. Otherwise, put it back on the queue and\n# invoke parseError on parser.\nif c != \";\":\n    self.tokenQueue.append({\"type\": tokenTypes[\"ParseError\"], \"data\":\n                            \"numeric-entity-without-semicolon\"})\n    self.stream.unget(c)\n\nreturn char", "path": "lib\\python2.7\\site-packages\\pip\\_vendor\\html5lib\\tokenizer.py", "repo_name": "llSourcell/AI_Artist", "stars": 237, "license": "None", "language": "python", "size": 67954}
{"docstring": "\"\"\"\nReturn a parser for parsing requirement lines\n\"\"\"\n", "func_signal": "def build_parser():\n", "code": "parser = optparse.OptionParser(add_help_option=False)\n\noption_factories = SUPPORTED_OPTIONS + SUPPORTED_OPTIONS_REQ\nfor option_factory in option_factories:\n    option = option_factory()\n    parser.add_option(option)\n\n# By default optparse sys.exits on parsing errors. We want to wrap\n# that in our own exception.\ndef parser_exit(self, msg):\n    raise RequirementsFileParseError(msg)\nparser.exit = parser_exit\n\nreturn parser", "path": "lib\\python2.7\\site-packages\\pip\\req\\req_file.py", "repo_name": "llSourcell/AI_Artist", "stars": 237, "license": "None", "language": "python", "size": 67954}
{"docstring": "\"\"\"This method is a generic handler for emitting the tags. It also sets\nthe state to \"data\" because that's what's needed after a token has been\nemitted.\n\"\"\"\n", "func_signal": "def emitCurrentToken(self):\n", "code": "token = self.currentToken\n# Add token to the queue to be yielded\nif (token[\"type\"] in tagTokenTypes):\n    if self.lowercaseElementName:\n        token[\"name\"] = token[\"name\"].translate(asciiUpper2Lower)\n    if token[\"type\"] == tokenTypes[\"EndTag\"]:\n        if token[\"data\"]:\n            self.tokenQueue.append({\"type\": tokenTypes[\"ParseError\"],\n                                    \"data\": \"attributes-in-end-tag\"})\n        if token[\"selfClosing\"]:\n            self.tokenQueue.append({\"type\": tokenTypes[\"ParseError\"],\n                                    \"data\": \"self-closing-flag-on-end-tag\"})\nself.tokenQueue.append(token)\nself.state = self.dataState", "path": "lib\\python2.7\\site-packages\\pip\\_vendor\\html5lib\\tokenizer.py", "repo_name": "llSourcell/AI_Artist", "stars": 237, "license": "None", "language": "python", "size": 67954}
{"docstring": "\"\"\"Joins a line ending in '\\' with the previous line (except when following\ncomments).  The joined line takes on the index of the first line.\n\"\"\"\n", "func_signal": "def join_lines(lines_enum):\n", "code": "primary_line_number = None\nnew_line = []\nfor line_number, line in lines_enum:\n    if not line.endswith('\\\\') or COMMENT_RE.match(line):\n        if COMMENT_RE.match(line):\n            # this ensures comments are always matched later\n            line = ' ' + line\n        if new_line:\n            new_line.append(line)\n            yield primary_line_number, ''.join(new_line)\n            new_line = []\n        else:\n            yield line_number, line\n    else:\n        if not new_line:\n            primary_line_number = line_number\n        new_line.append(line.strip('\\\\'))\n\n# last line contains \\\nif new_line:\n    yield primary_line_number, ''.join(new_line)\n\n# TODO: handle space after '\\'.", "path": "lib\\python2.7\\site-packages\\pip\\req\\req_file.py", "repo_name": "llSourcell/AI_Artist", "stars": 237, "license": "None", "language": "python", "size": 67954}
{"docstring": "# Initialise to the default output for when no entity is matched\n", "func_signal": "def consumeEntity(self, allowedChar=None, fromAttribute=False):\n", "code": "output = \"&\"\n\ncharStack = [self.stream.char()]\nif (charStack[0] in spaceCharacters or charStack[0] in (EOF, \"<\", \"&\")\n        or (allowedChar is not None and allowedChar == charStack[0])):\n    self.stream.unget(charStack[0])\n\nelif charStack[0] == \"#\":\n    # Read the next character to see if it's hex or decimal\n    hex = False\n    charStack.append(self.stream.char())\n    if charStack[-1] in (\"x\", \"X\"):\n        hex = True\n        charStack.append(self.stream.char())\n\n    # charStack[-1] should be the first digit\n    if (hex and charStack[-1] in hexDigits) \\\n            or (not hex and charStack[-1] in digits):\n        # At least one digit found, so consume the whole number\n        self.stream.unget(charStack[-1])\n        output = self.consumeNumberEntity(hex)\n    else:\n        # No digits found\n        self.tokenQueue.append({\"type\": tokenTypes[\"ParseError\"],\n                                \"data\": \"expected-numeric-entity\"})\n        self.stream.unget(charStack.pop())\n        output = \"&\" + \"\".join(charStack)\n\nelse:\n    # At this point in the process might have named entity. Entities\n    # are stored in the global variable \"entities\".\n    #\n    # Consume characters and compare to these to a substring of the\n    # entity names in the list until the substring no longer matches.\n    while (charStack[-1] is not EOF):\n        if not entitiesTrie.has_keys_with_prefix(\"\".join(charStack)):\n            break\n        charStack.append(self.stream.char())\n\n    # At this point we have a string that starts with some characters\n    # that may match an entity\n    # Try to find the longest entity the string will match to take care\n    # of &noti for instance.\n    try:\n        entityName = entitiesTrie.longest_prefix(\"\".join(charStack[:-1]))\n        entityLength = len(entityName)\n    except KeyError:\n        entityName = None\n\n    if entityName is not None:\n        if entityName[-1] != \";\":\n            self.tokenQueue.append({\"type\": tokenTypes[\"ParseError\"], \"data\":\n                                    \"named-entity-without-semicolon\"})\n        if (entityName[-1] != \";\" and fromAttribute and\n            (charStack[entityLength] in asciiLetters or\n             charStack[entityLength] in digits or\n             charStack[entityLength] == \"=\")):\n            self.stream.unget(charStack.pop())\n            output = \"&\" + \"\".join(charStack)\n        else:\n            output = entities[entityName]\n            self.stream.unget(charStack.pop())\n            output += \"\".join(charStack[entityLength:])\n    else:\n        self.tokenQueue.append({\"type\": tokenTypes[\"ParseError\"], \"data\":\n                                \"expected-named-entity\"})\n        self.stream.unget(charStack.pop())\n        output = \"&\" + \"\".join(charStack)\n\nif fromAttribute:\n    self.currentToken[\"data\"][-1][1] += output\nelse:\n    if output in spaceCharacters:\n        tokenType = \"SpaceCharacters\"\n    else:\n        tokenType = \"Characters\"\n    self.tokenQueue.append({\"type\": tokenTypes[tokenType], \"data\": output})", "path": "lib\\python2.7\\site-packages\\pip\\_vendor\\html5lib\\tokenizer.py", "repo_name": "llSourcell/AI_Artist", "stars": 237, "license": "None", "language": "python", "size": 67954}
{"docstring": "\"\"\"Break up the line into an args and options string.  We only want to shlex\n(and then optparse) the options, not the args.  args can contain markers\nwhich are corrupted by shlex.\n\"\"\"\n", "func_signal": "def break_args_options(line):\n", "code": "tokens = line.split(' ')\nargs = []\noptions = tokens[:]\nfor token in tokens:\n    if token.startswith('-') or token.startswith('--'):\n        break\n    else:\n        args.append(token)\n        options.pop(0)\nreturn ' '.join(args), ' '.join(options)", "path": "lib\\python2.7\\site-packages\\pip\\req\\req_file.py", "repo_name": "llSourcell/AI_Artist", "stars": 237, "license": "None", "language": "python", "size": 67954}
{"docstring": "# super(SymlinkLockFile).__init(...)\n", "func_signal": "def __init__(self, path, threaded=True, timeout=None):\n", "code": "LockBase.__init__(self, path, threaded, timeout)\n# split it back!\nself.unique_name = os.path.split(self.unique_name)[1]", "path": "lib\\python2.7\\site-packages\\pip\\_vendor\\lockfile\\symlinklockfile.py", "repo_name": "llSourcell/AI_Artist", "stars": 237, "license": "None", "language": "python", "size": 67954}
{"docstring": "\"\"\"Split, filter, and join lines, and return a line iterator\n\n:param content: the content of the requirements file\n:param options: cli options\n\"\"\"\n", "func_signal": "def preprocess(content, options):\n", "code": "lines_enum = enumerate(content.splitlines(), start=1)\nlines_enum = join_lines(lines_enum)\nlines_enum = ignore_comments(lines_enum)\nlines_enum = skip_regex(lines_enum, options)\nreturn lines_enum", "path": "lib\\python2.7\\site-packages\\pip\\req\\req_file.py", "repo_name": "llSourcell/AI_Artist", "stars": 237, "license": "None", "language": "python", "size": 67954}
{"docstring": "# NOTE: This method should not change as some may depend on it.\n#       See: https://github.com/ionrock/cachecontrol/issues/63\n", "func_signal": "def _fn(self, name):\n", "code": "hashed = self.encode(name)\nparts = list(hashed[:5]) + [hashed]\nreturn os.path.join(self.directory, *parts)", "path": "lib\\python2.7\\site-packages\\pip\\_vendor\\cachecontrol\\caches\\file_cache.py", "repo_name": "llSourcell/AI_Artist", "stars": 237, "license": "None", "language": "python", "size": 67954}
{"docstring": "\"\"\"Return message given signature+message and the verifying key.\"\"\"\n", "func_signal": "def crypto_sign_open(signed, vk):\n", "code": "if len(vk) != PUBLICKEYBYTES:\n    raise ValueError(\"Bad verifying key length %d\" % len(vk))\nrc = djbec.checkvalid(signed[:SIGNATUREBYTES], signed[SIGNATUREBYTES:], vk)\nif not rc:\n    raise ValueError(\"rc != True\", rc)    \nreturn signed[SIGNATUREBYTES:]", "path": "lib\\python2.7\\site-packages\\wheel\\signatures\\ed25519py.py", "repo_name": "llSourcell/AI_Artist", "stars": 237, "license": "None", "language": "python", "size": 67954}
{"docstring": "\"\"\"Return signature+message given message and secret key.\nThe signature is the first SIGNATUREBYTES bytes of the return value.\nA copy of msg is in the remainder.\"\"\"\n", "func_signal": "def crypto_sign(msg, sk):\n", "code": "if len(sk) != SECRETKEYBYTES:\n    raise ValueError(\"Bad signing key length %d\" % len(sk))\nvkbytes = sk[PUBLICKEYBYTES:]\nskbytes = sk[:PUBLICKEYBYTES]\nsig = djbec.signature(msg, skbytes, vkbytes)\nreturn sig + msg", "path": "lib\\python2.7\\site-packages\\wheel\\signatures\\ed25519py.py", "repo_name": "llSourcell/AI_Artist", "stars": 237, "license": "None", "language": "python", "size": 67954}
{"docstring": "\"\"\"\nStem the acceptance list given by the path. This should be done before data preparation for that specific list.\n\n@param path: The path to the acceptance list.\n\"\"\"\n", "func_signal": "def stem_acceptance_list(path):\n", "code": "global EnglishStemmer\nfrom nltk.stem.porter import PorterStemmer as ES\n\nEnglishStemmer = ES\n\nacceptance_lst = open(path).read().replace(\" \", \"\").split(\"\\n\")\nstemmer = EnglishStemmer()\nacceptance_lst_stemmed = []\nfor word in acceptance_lst:\n    acceptance_lst_stemmed.append(stemmer.stem(word.lower()))\n\nf = open(env_paths.get_acceptance_lst_path(), 'w')\nfor w in acceptance_lst_stemmed[:-1]:\n    f.write(w + \"\\n\")\nf.write(acceptance_lst_stemmed[-1])\nf.close()", "path": "DataPreparation\\data_processing.py", "repo_name": "larsmaaloee/deep-belief-nets-for-topic-modeling", "stars": 143, "license": "None", "language": "python", "size": 398242}
{"docstring": "\"\"\"\nSave batches for the document loading process in the initialization phase. This is done due to vast sizes\nof data - lack of memory.\n\n@param batch_number: Representing the number of documents in the batch.\n@param docs_list: List containing a string for each document in the batch.\n@param docs_names: List containing the names of each document in the same order as the docs_list.\n@param class_indices: List containing which class/folder each document belongs to.\n\"\"\"\n# Serialize all relevant variables\n", "func_signal": "def __save_batch_loading_docs(self, batch_number, docs_list, docs_names, class_indices):\n", "code": "s.dump(docs_list, open(env_paths.get_doc_list_path(self.training, batch_number), \"wb\"))\ns.dump(docs_names, open(env_paths.get_doc_names_path(self.training, batch_number), \"wb\"))\ns.dump(class_indices, open(env_paths.get_class_indices_path(self.training, batch_number), \"wb\"))", "path": "DataPreparation\\data_processing.py", "repo_name": "larsmaaloee/deep-belief-nets-for-topic-modeling", "stars": 143, "license": "None", "language": "python", "size": 398242}
{"docstring": "\"\"\"\nSerialize the output of the rbm.\n\n@param batch_index: Index of the batch.\n@param outputs: The output probabilitites of the rbm\n\"\"\"\n\n", "func_signal": "def __save_output__(self, batch_index, outputs):\n", "code": "s.dump(outputs.tolist(),\n       open(env_paths.get_rbm_output_path(str(self.num_hid), batch_index, self.layer_index), \"wb\"))", "path": "DBN\\pretraining.py", "repo_name": "larsmaaloee/deep-belief-nets-for-topic-modeling", "stars": 143, "license": "None", "language": "python", "size": 398242}
{"docstring": "\"\"\"\nVisualise the input data or the output data of the DBN on a 3D PCA plot for principal components 1 and 2.\n\nParameters\n----------\ninput_data: False if the output data of the DBN should be plottet. Otherwise True.\n\"\"\"\n", "func_signal": "def visualise_data_pca_3d(self, component1, component2, component3, input_data=False):\n", "code": "if input_data:\n    self.__generate_input_data()\n    pca_3d(array(self.input_data), component1, component2, component3, self.class_indices, self.path,\n           'high_dimension_data', self.legend)\nelse:\n    self.__generate_output_data()\n    pca_3d(array(self.output_data), component1, component2, component3, self.class_indices, self.path,\n           'low_dimension_data', self.legend)", "path": "Testing\\visualise.py", "repo_name": "larsmaaloee/deep-belief-nets-for-topic-modeling", "stars": 143, "license": "None", "language": "python", "size": 398242}
{"docstring": "\"\"\"\nVisualise the input data or the output data of the DBN on a 3D PCA plot movie for principal components 1 and 2.\n\nParameters\n----------\ninput_data: False if the output data of the DBN should be plottet. Otherwise True.\n\"\"\"\n", "func_signal": "def visualise_data_pca_3d_movie(self, component1, component2, component3, input_data=False):\n", "code": "if input_data:\n    self.__generate_input_data()\n    pca_3d_movie(array(self.input_data), component1, component2, component3, self.class_indices, self.path,\n                 'high_dimension_data', self.legend)\nelse:\n    self.__generate_output_data()\n    pca_3d_movie(array(self.output_data), component1, component2, component3, self.class_indices, self.path,\n                 'low_dimension_data', self.legend)", "path": "Testing\\visualise.py", "repo_name": "larsmaaloee/deep-belief-nets-for-topic-modeling", "stars": 143, "license": "None", "language": "python", "size": 398242}
{"docstring": "\"\"\"\nRetrieve the word-count matrix from HDD.\n\n@param batch_index: Index of the batch.\n\n@return: The word-count matrix corresponding to the batch_index.\n\"\"\"\n", "func_signal": "def __get_input_data__(self, batch_index, first_layer):\n", "code": "if first_layer:\n    return DataPreparation.data_processing.get_bag_of_words_matrix(self.batches[batch_index])\nreturn array(s.load(open(env_paths.get_rbm_output_path(self.num_vis, batch_index, self.layer_index - 1), \"rb\")))", "path": "DBN\\pretraining.py", "repo_name": "larsmaaloee/deep-belief-nets-for-topic-modeling", "stars": 143, "license": "None", "language": "python", "size": 398242}
{"docstring": "\"\"\"\nIn case the number of output units of the DBN is 2, this method will visualise the output of the DBN on a\n2D plot.\n\"\"\"\n", "func_signal": "def visualise_2d_data(self):\n", "code": "self.__generate_output_data()\nif len(self.output_data[0]) != 2:  # The output dimensions must be 2\n    return\nf = Plot.figure()\nf.hold()\nplt.title('2D data')\nfor c in sorted(set(self.class_indices)):\n    class_mask = mat(self.class_indices).T.A.ravel() == c\n    plt.plot(array(self.output_data)[class_mask, 0], array(self.output_data)[class_mask, 1], 'o')\nplt.legend(self.legend)\nplt.show()\nplt.savefig(self.path + '/2dplotlow.png', dpi=200)", "path": "Testing\\visualise.py", "repo_name": "larsmaaloee/deep-belief-nets-for-topic-modeling", "stars": 143, "license": "None", "language": "python", "size": 398242}
{"docstring": "\"\"\"\nGenerate the output data of the DBN so that it can be visualised.\n\"\"\"\n", "func_signal": "def __generate_output_data(self):\n", "code": "if not len(self.output_data) == 0:\n    return\ntry:\n    self.output_data = s.load(open('output/output_data.p', 'rb'))\n    self.class_indices = s.load(open('output/class_indices.p', 'rb'))\n    if not self.classes_to_visualise == None:\n        self.__filter_output_data(self.classes_to_visualise)\nexcept:\n    self.output_data = generate_output_for_test_data(image_data=self.image_data,\n                                                     binary_output=self.binary_output) if self.testing else generate_output_for_train_data(\n        image_data=self.image_data, binary_output=self.binary_output)\n    self.class_indices = get_all_class_indices(training=False) if self.testing else get_all_class_indices()\n    if not self.classes_to_visualise == None:\n        self.__filter_output_data(self.classes_to_visualise)\n    s.dump([out.tolist() for out in self.output_data], open('output/output_data.p', 'wb'))\n    s.dump(self.class_indices, open('output/class_indices.p', 'wb'))\n\nself.legend = get_class_names_for_class_indices(list(set(sorted(self.class_indices))))", "path": "Testing\\visualise.py", "repo_name": "larsmaaloee/deep-belief-nets-for-topic-modeling", "stars": 143, "license": "None", "language": "python", "size": 398242}
{"docstring": "# Extract class names to python list,\n# then encode with integers (dict)\n", "func_signal": "def pca(X, class_indices):\n", "code": "classLabels = class_indices\nclassNames = sorted(set(classLabels))\nclassDict = dict(zip(classNames, range(len(classNames))))\n# Extract vector y, convert to NumPy matrix and transpose\ny = np.mat([classDict[value] for value in classLabels]).T\n# Compute values of N, M and C.\nN, _ = X.shape\nC = len(classNames)\n\n\n# Subtract mean value from data\nY = X - np.ones((N, 1)) * X.mean(0)\n# PCA by computing SVD of Y\n_, _, V = linalg.svd(Y, full_matrices=False)\nV = mat(V).T\nZ = Y * V  # Project the centered data onto principal component space\n\nreturn C, y, Z", "path": "Testing\\pca.py", "repo_name": "larsmaaloee/deep-belief-nets-for-topic-modeling", "stars": 143, "license": "None", "language": "python", "size": 398242}
{"docstring": "\"\"\"\nGenerate the input data for the DBN so that it can be visualized.\n\"\"\"\n", "func_signal": "def __generate_input_data(self):\n", "code": "if not len(self.input_data) == 0:\n    return\n\ntry:\n    self.input_data = s.load(open('output/input_data.p', 'rb'))\n    self.class_indices = s.load(open('output/class_indices.p', 'rb'))\n    if not self.classes_to_visualise == None:\n        self.__filter_input_data(self.classes_to_visualise)\nexcept:\n    self.input_data = generate_input_data_list(training=False) if self.testing else generate_input_data_list()\n    self.class_indices = get_all_class_indices(training=False) if self.testing else get_all_class_indices()\n    if not self.classes_to_visualise == None:\n        self.__filter_input_data(self.classes_to_visualise)\n    s.dump([input.tolist() for input in self.input_data], open('output/input_data.p', 'wb'))\n    s.dump(self.class_indices, open('output/class_indices.p', 'wb'))\n\nself.legend = get_class_names_for_class_indices(list(set(sorted(self.class_indices))))", "path": "Testing\\visualise.py", "repo_name": "larsmaaloee/deep-belief-nets-for-topic-modeling", "stars": 143, "license": "None", "language": "python", "size": 398242}
{"docstring": "\"\"\"\nGet document names.\n\n@param batch: the number of the batch.\n@param training: is this the training set or the test set.\n\"\"\"\n", "func_signal": "def get_document_names(batch, training=True):\n", "code": "names = s.load(open(env_paths.get_doc_names_path(training, batch), \"rb\"))\nreturn names", "path": "DataPreparation\\data_processing.py", "repo_name": "larsmaaloee/deep-belief-nets-for-topic-modeling", "stars": 143, "license": "None", "language": "python", "size": 398242}
{"docstring": "\"\"\"\nVisualise the input data or the output data of the DBN on a 2D PCA plot. Depending on the number of components,\nthe plot will contain an X amount of subplots.\n\n@param input_data: False if the output data of the DBN should be plottet. Otherwise True.\n@param number_of_components: The number of principal components for the PCA plot.\n\"\"\"\n\n", "func_signal": "def visualise_data_pca_2d(self, input_data=False, number_of_components=9):\n", "code": "if input_data:\n    self.__generate_input_data()\n    pca_2d(array(self.input_data), self.class_indices, self.path, 'high_dimension_data', number_of_components,\n           self.legend)\nelse:\n    self.__generate_output_data()\n    pca_2d(array(self.output_data), self.class_indices, self.path, 'low_dimension_data', number_of_components,\n           self.legend)", "path": "Testing\\visualise.py", "repo_name": "larsmaaloee/deep-belief-nets-for-topic-modeling", "stars": 143, "license": "None", "language": "python", "size": 398242}
{"docstring": "\"\"\"\nThe learning of the first layer RBM (Replicated Softmax Model). The higher value of epochs will result in\nmore training.\n\n@param epochs: The number of epochs.\n\"\"\"\n", "func_signal": "def rsm_learn(self, epochs):\n", "code": "for epoch in range(epochs):\n    perplexity = 0\n    batch_index = 0\n\n    for _ in self.batches:\n\n        # Positive phase - generate data from visible to hidden units.\n        pos_vis = self.__get_input_data__(batch_index, first_layer=True)\n        batch_size = len(pos_vis)\n        D = sum(pos_vis, axis=1)\n        if epoch == 0:\n            self.words += sum(pos_vis)  # Calculate the number of words in order to calculate the perplexity.\n\n        pos_hid_prob = dbn.sigmoid(dot(pos_vis, self.weights) + outer(D, self.hidden_biases))\n        self.__save_output__(batch_index, pos_hid_prob)  # Serialize the output of the RBM\n\n        # If probabilities are higher than randomly generated, the states are 1\n        randoms = rand.rand(batch_size, self.num_hid)\n        pos_hid = array(randoms < pos_hid_prob, dtype=int)\n\n        # Negative phase - generate data from hidden to visible units and then again to hidden units.\n        neg_vis = pos_vis\n        neg_hid_prob = pos_hid\n        for i in range(self.gibbs_steps):  # There is only 1 step of contrastive divergence\n            neg_vis, neg_hid_prob, D, p = self.__contrastive_divergence_rsm__(neg_vis, pos_hid_prob, D)\n            if i == 0:\n                perplexity += p\n\n        pos_products = dot(pos_vis.T, pos_hid_prob)\n        pos_visible_bias_activation = sum(pos_vis, axis=0)\n        pos_hidden_bias_activation = sum(pos_hid_prob, axis=0)\n        neg_products = dot(neg_vis.T, neg_hid_prob)\n        neg_visibe_bias_activation = sum(neg_vis, axis=0)\n        neg_hidden_bias_activation = sum(neg_hid_prob, axis=0)\n\n        # Update the weights and biases\n        self.delta_weights = self.momentum * self.delta_weights + self.learning_rate * (\n            (pos_products - neg_products) / batch_size - self.weight_cost * self.weights)\n        self.delta_visible_biases = (self.momentum * self.delta_visible_biases + (\n            pos_visible_bias_activation - neg_visibe_bias_activation)) * (self.learning_rate / batch_size)\n        self.delta_hidden_biases = (self.momentum * self.delta_hidden_biases + (\n            pos_hidden_bias_activation - neg_hidden_bias_activation)) * (self.learning_rate / batch_size)\n        self.weights += self.delta_weights\n        self.visible_biases += self.delta_visible_biases\n        self.hidden_biases += self.delta_hidden_biases\n        batch_index += 1\n\n    if not epoch == 0:  # Output error score.\n        perplexity = exp(-perplexity / self.words)\n        err_str = \"Epoch[%2d]: Perplexity = %.02f\" % (epoch, perplexity)\n        self.fout(err_str)\n        self.error += [perplexity]", "path": "DBN\\pretraining.py", "repo_name": "larsmaaloee/deep-belief-nets-for-topic-modeling", "stars": 143, "license": "None", "language": "python", "size": 398242}
{"docstring": "\"\"\"\nCheck for DBN network data.\n\"\"\"\n", "func_signal": "def check_for_data():\n", "code": "if not (os.path.exists(ep.get_test_data_path()) or os.path.exists(ep.get_dbn_weight_path())):\n    return False\nreturn True", "path": "Testing\\dbn_testing.py", "repo_name": "larsmaaloee/deep-belief-nets-for-topic-modeling", "stars": 143, "license": "None", "language": "python", "size": 398242}
{"docstring": "\"\"\"\nGet all class indices for all batches in one list.\n\n@param training: is this the training set or the test set.\n\"\"\"\n", "func_signal": "def get_all_class_indices(training=True):\n", "code": "batches = get_batch_list(training)\nindices_collected = []\n\nfor batch in batches:\n    indices_collected += list(s.load(open(env_paths.get_class_indices_path(training, int(batch)), \"rb\")))\n\nreturn indices_collected", "path": "DataPreparation\\data_processing.py", "repo_name": "larsmaaloee/deep-belief-nets-for-topic-modeling", "stars": 143, "license": "None", "language": "python", "size": 398242}
{"docstring": "\"\"\"\nCheck for DBN network data.\n\"\"\"\n", "func_signal": "def check_for_data():\n", "code": "if not (os.path.exists(ep.get_test_data_path()) or os.path.exists(ep.get_dbn_weight_path())):\n    return False\nreturn True", "path": "Testing\\visualise.py", "repo_name": "larsmaaloee/deep-belief-nets-for-topic-modeling", "stars": 143, "license": "None", "language": "python", "size": 398242}
{"docstring": "# Compute distances between o1 and remaining outputs\n", "func_signal": "def generate_proximity_indices(o1, output_data, idx, e, binary_output):\n", "code": "distances = distance(o1, output_data)\ndistances[idx] = np.inf\n\n# Retrieve the indices of the n smallest values\nminimum_values = nsmallest(e, distances)\n\nindices = []\nfor m in minimum_values:\n    i = list(np.where(np.array(distances) == m)[0])\n    indices += i\n\nreturn indices", "path": "Testing\\dbn_testing.py", "repo_name": "larsmaaloee/deep-belief-nets-for-topic-modeling", "stars": 143, "license": "None", "language": "python", "size": 398242}
{"docstring": "\"\"\"\nGet all class names for the class indices.\n@param class_indices: The class indices for whom class names must be retrieved.\n\"\"\"\n", "func_signal": "def get_class_names_for_class_indices(class_indices):\n", "code": "class_names = get_all_class_names()\n\nclass_names_filtered = []\nfor idx in class_indices:\n    class_names_filtered.append(class_names[idx])\nreturn class_names_filtered", "path": "DataPreparation\\data_processing.py", "repo_name": "larsmaaloee/deep-belief-nets-for-topic-modeling", "stars": 143, "license": "None", "language": "python", "size": 398242}
{"docstring": "\"\"\"\nGet all class indices of the documents in a batch.\n\n@param batch: the number of the batch.\n@param training: is this the training set or the test set.\n\"\"\"\n\n", "func_signal": "def get_class_indices(batch, training=True):\n", "code": "indices = s.load(env_paths.get_class_indices_path(training, batch), \"rb\")\nreturn indices", "path": "DataPreparation\\data_processing.py", "repo_name": "larsmaaloee/deep-belief-nets-for-topic-modeling", "stars": 143, "license": "None", "language": "python", "size": 398242}
{"docstring": "\"\"\"\nThe class of a document corresponding to a row\nin a batch.\n\n@param row: row in the bag of words matrix in batch.\n@param batch: the number of the batch.\n@param training: is this the training set or the test set.\n\"\"\"\n", "func_signal": "def get_document_class(row, batch, training=True):\n", "code": "class_indices_for_batch = s.load(open(env_paths.get_class_indices_path(training, batch), \"rb\"))\nclass_names_for_batch = s.load(open(env_paths.get_class_names_path(training), \"rb\"))\nreturn class_names_for_batch[class_indices_for_batch[row]]", "path": "DataPreparation\\data_processing.py", "repo_name": "larsmaaloee/deep-belief-nets-for-topic-modeling", "stars": 143, "license": "None", "language": "python", "size": 398242}
{"docstring": "\"\"\" Calculate the score weights based on the size of the functions\n\nLarger functions are given a heavier weight because its harder to\nmatch two large functions than it is to match two small functions\nsize_list - the sizes of every function in the given binary\ndebug - print additional information on binaries\"\"\"\n\n", "func_signal": "def calculate_weights(size_list, debug):\n", "code": "weights = []\ntotal = sum(size_list)\n\nfor s in size_list:\n    weights.append(float(s) / float(total))\n\nif(debug):\n    print(\"Function weights: \", weights)\n\nreturn(weights)", "path": "malfunction\\malfunction.py", "repo_name": "Dynetics/Malfunction", "stars": 191, "license": "lgpl-2.1", "language": "python", "size": 1003}
{"docstring": "\"\"\" Set up argparse arguments \n-o output       -- output file for hashes\n-u unpack       -- automatic unpacking\"\"\"\n\n", "func_signal": "def argparse_setup():\n", "code": "parser = argparse.ArgumentParser(prog=\"python3 malget.py\")\nparser.add_argument(\"PATH\", help=\"Path to the binary or binaries\")\nparser.add_argument(\"-o\", \"--output\", type=str, help=\"output file for \"\n                    \"signatures\")\nparser.add_argument(\"-u\", \"--unpack\", action=\"store_true\",\n                    help=\"Unpacks packed executables before disassembly. \"\n                    \"Currently not implemented.\")\nreturn parser.parse_args()", "path": "malfunction\\malget.py", "repo_name": "Dynetics/Malfunction", "stars": 191, "license": "lgpl-2.1", "language": "python", "size": 1003}
{"docstring": "\"\"\" Set up argparse arguments\n\n -a author           -- Suspected author of binary\n -f filenames        -- Potential filenames that the binary came from\n -c comments         -- Comments on the binaryy\n -o overwrite        -- Overwrite existing instances in the database\n -u unpack           -- Automatic unpacking of executables \n -s sigsOnly         -- Skip disassembly and add sig file to database\n -D database         -- Change the path of the database\"\"\"\n\n", "func_signal": "def argparse_setup():\n", "code": "parser = argparse.ArgumentParser(prog=\"python3 mal-learn\")\nparser.add_argument(\"-a\", \"--author\", type=str, default=\"unknown\",\n                    help=\"The Author of the given binary\")\nparser.add_argument(\"PATH\", help=\"Path to the target binary or binaries\")\nparser.add_argument(\"-f\", \"--filenames\", type=str, default=\"unknown\",\n                    help=\"The name of the file that the Binary came from\")\nparser.add_argument(\"-u\", \"--unpack\", action=\"store_true\",\n                    help=\"Unpacks packed executables before disassembly. \"\n                    \"Currently not implemented.\")\nparser.add_argument(\"-c\", \"--comment\", type=str, default=\"\",\n                    help=\"A comment to add to the database\")\nparser.add_argument(\"trustlevel\", type=str,\n                    choices=[\"blacklist\", \"whitelist\"],\n                    help=\"The classification of the binary, Ex. WhiteList,\"\n                         \" BlackList.\")\nparser.add_argument(\"-o\", \"--overwrite\", action=\"store_true\",\n                    help=\"Overwrites an older instance of a binary \"\n                         \"in the database with the given one\")\nparser.add_argument(\"-s\", \"--sigsOnly\", action=\"store_true\",\n                    help=\"Skips the disassembly and just learns the \"\n                    \"signatures\")\nparser.add_argument(\"-D\", \"--database\", type=str, default='malfunction.db',\n                    help=\"Path to malfunction database. \"\n                    \"Default=malfunction.db in current directory\")\nargs = parser.parse_args()\nreturn args", "path": "malfunction\\mallearn.py", "repo_name": "Dynetics/Malfunction", "stars": 191, "license": "lgpl-2.1", "language": "python", "size": 1003}
{"docstring": "\"\"\" Set up argparse arguments\n\n-d debug            -- Details about internal workings\n-u unpack           -- Automatic unpacking of packed binaries\n--leave-db-on-disk  -- Don't load the binary into memory\n                    (slower but less costly)\n--add-strong-matches-- Adds the strong matches to the database\n                    automatically (use with caution)\n-D database         -- The path to the database the user wants\n                    to use, Default='malfunction.db' \n-a all              -- Compare to all files, not just same type\"\"\"\n\n", "func_signal": "def argparse_setup():\n", "code": "parser = argparse.ArgumentParser(prog=\"python3 malfunction.py\")\nparser.add_argument(\"PATH\", help=\"Binary or directory of binaries to run \"\n                                 \"comparisions against\")\nparser.add_argument(\"-d\", \"--debug\", action=\"store_true\",\n                    help=\"Prints details about internal objects to the \"\n                         \"terminal\")\nparser.add_argument(\"-u\", \"--unpack\", action=\"store_true\",\n                    help=\"Unpacks packed executables before disassembly. \"\n                    \"Current not implemented.\")\nparser.add_argument(\"--leave-db-on-disk\", action=\"store_true\",\n                    help=\"Prevents malfunction from loading the database \"\n                         \"into memory.\")\nparser.add_argument(\"--add-strong-matches\", action=\"store_true\",\n                    help=\"This flag adds all the very strongly matched \"\n                         \" binaries into the database.\")\nparser.add_argument(\"-D\", \"--database\", type=str, default='malfunction.db',\n                    help=\"Path to malfunction database. \"\n                    \"Default=malfunction.db in current directory\")\nparser.add_argument(\"-a\", \"--all\", action=\"store_true\",\n                    help=\"Compare to every file in the database instead of \"\n                         \"just files of the same filetype\")\nreturn parser.parse_args()", "path": "malfunction\\malfunction.py", "repo_name": "Dynetics/Malfunction", "stars": 191, "license": "lgpl-2.1", "language": "python", "size": 1003}
{"docstring": "\"\"\" Creates the binary tuple for use in Malfunction and Mallearn\n\nResults in the form: (Binary Hash, [**ssdeep hashes])\"\"\"\n\n", "func_signal": "def get_hash_tuple(functions, filename):\n", "code": "function_hashes = []\nbinary_hash = get_binary_hash(filename)\nfor function in functions:\n    function_hashes.append(ssdeep.hash(function))\nreturn (binary_hash, function_hashes)", "path": "malfunction\\malget.py", "repo_name": "Dynetics/Malfunction", "stars": 191, "license": "lgpl-2.1", "language": "python", "size": 1003}
{"docstring": "\"\"\" Create the needed tables if they don't exist and\nreturn the cursor for the database\n\nleave_db - boolean to prevent loading database to memory\ndatabase - path to the malfunction database file\"\"\"\n\n", "func_signal": "def prepare_database(leave_db, database=None):\n", "code": "conn = apsw.Connection(database)\ncursor = conn.cursor()\ncursor.execute(\"CREATE TABLE IF NOT EXISTS functions(hash TEXT,binaryID\"\n               \" TEXT,FOREIGN KEY(binaryID) REFERENCES \"\n               \"binaries(binaryID))\")\ncursor.execute(\"CREATE TABLE IF NOT EXISTS binaries(binaryID TEXT, \"\n               \"author TEXT, filenames TEXT, comment TEXT, \"\n               \"trustlevel TEXT, filetype TEXT);\")\nmemcon = apsw.Connection(\":memory:\")\n\nif (not leave_db):\n    with memcon.backup(\"main\", conn, \"main\") as backup:\n        mem = psutil.virtual_memory()\n        backup.step(1)\n        remaining_pages = backup.remaining\n\n        if mem.free >= remaining_pages * 1024:\n            backup.step()\n        else:\n            print(\"Not enough free memory to load the database to memory, \"\n                  \"switching to reading from disk...\")\n\n    cursor = memcon.cursor()\n\nreturn cursor", "path": "malfunction\\malfunction.py", "repo_name": "Dynetics/Malfunction", "stars": 191, "license": "lgpl-2.1", "language": "python", "size": 1003}
{"docstring": "\"\"\" Get the md5 hash of the file to put at the top of the document \"\"\"\n\n", "func_signal": "def get_binary_hash(filename):\n", "code": "blocksize = 65536\nhasher = hashlib.md5()\nwith open(filename, \"rb\") as afile:\n    buf = afile.read(blocksize)\n    while len(buf) > 0:\n        hasher.update(buf)\n        buf = afile.read(blocksize)\nreturn hasher.hexdigest()", "path": "malfunction\\malget.py", "repo_name": "Dynetics/Malfunction", "stars": 191, "license": "lgpl-2.1", "language": "python", "size": 1003}
{"docstring": "\"\"\" Run malfunction, a tool for software analysis\n\nUsage: python3 malfunction.py <file> \"\"\"\n\n", "func_signal": "def main():\n", "code": "args = argparse_setup()\n\n# TODO: Make files with spaces compatible\nif \"\\ \" in args.PATH or \" \" in args.PATH:\n    print(\"The radare2 commands we are using for disassembly do not \"\n          \"play nice with spaces in the filename. Rename the file\")\n    return False\n\ncursor = prepare_database(args.leave_db_on_disk, args.database)\n\npath = args.PATH\ndir_check = os.path.isdir(path)\n\nif dir_check:\n    directory_malfunction(args, cursor)\nelse:\n    try:\n        hash_tuple, size_list = malget.malget(path, args.unpack)\n        filetype = get_filetype(path)\n        compute_score(cursor, hash_tuple, size_list, filetype,\n                      args.add_strong_matches, args.all, args.debug)\n    except ValueError:\n        print(\"That file cannot be disassembled\")\n        return False\n    except Exception as err:\n        print(\"There was an error reading that file, are you sure it \"\n              \"was an executable?\\nError Information:\")\n        print('\\t', type(err))\n        print('\\t', err)\n        return False\n    except Warning:\n        print(\"That file is already in the database.\")\n        return False\nreturn True", "path": "malfunction\\malfunction.py", "repo_name": "Dynetics/Malfunction", "stars": 191, "license": "lgpl-2.1", "language": "python", "size": 1003}
{"docstring": "\"\"\" Learns an entire directory and its inner directories to db\n\nRuns through a folder and extends all of its folders, and\nadds them all to Malfunction database \"\"\"\n\n", "func_signal": "def directory_learn(args):\n", "code": "path = args.PATH\nif not path.endswith(\"/\"):\n    path += \"/\"\ndirectory_path = path\ndirectory = os.listdir(path)\nfor file in directory:\n    try:\n        path = directory_path + file\n        # Check for links so we don't accidently get an infinite loop.\n        if os.path.islink(path):\n            continue\n        # Extend the list of files by one layer.\n        if os.path.isdir(path):\n            inner_directory = os.listdir(path)\n            inner_directory = [file+\"/\"+x for x in inner_directory]\n            directory.extend(inner_directory)\n            continue\n        # Learn the sigs if path isn't to a folder or link.\n        if args.sigsOnly is True:\n            f = open(path, \"r\")\n            binary_hash = f.readline().strip()\n            hash_list = [x.strip() for x in f]\n            f.close()\n            check_format(binary_hash, hash_list)\n        else:\n            hash_tuple, sizes = malget.malget(path, args.unpack)\n            binary_hash = hash_tuple[0]\n            hash_list = [x.strip() for x in hash_tuple[1]]\n        print(\"Adding {0} to database...\".format(path))\n        args.filenames = directory_path+file\n        filetype = get_filetype(path)\n        mallearn(args, binary_hash, hash_list, filetype)\n        print(\"-\"*30)\n    except ValueError:\n        print(\"That file cannot be disassembled\")\n        print(\"-\"*30)\n        continue\n    except Warning:\n        print(\"That file is already in the database, use the -o flag to \"\n              \"overwrite\")\n        print(\"-\"*30)\n        continue\n    except Exception as err:\n        print(\"There was an error reading that file, are you sure it \"\n              \"was an executable?\\nError Information:\")\n        print('\\t', type(err))\n        print('\\t', err)\n        print(\"-\"*30)\n        continue", "path": "malfunction\\mallearn.py", "repo_name": "Dynetics/Malfunction", "stars": 191, "license": "lgpl-2.1", "language": "python", "size": 1003}
{"docstring": "\"\"\" Add signatures to database from signature file rather than binary \"\"\"\n\n", "func_signal": "def add_sigs(binary_hash, hash_list, trustlevel, filetype):\n", "code": "hash_list = [x.strip() for x in hash_list]\nprint(\"=>\\tAdding to database...\")\nmallearn(None, binary_hash, hash_list, filetype, trustlevel=trustlevel)", "path": "malfunction\\mallearn.py", "repo_name": "Dynetics/Malfunction", "stars": 191, "license": "lgpl-2.1", "language": "python", "size": 1003}
{"docstring": "\"\"\" Determine if the given binary is already in the database \"\"\"\n\n", "func_signal": "def check_overlap(cursor, binary):\n", "code": "cursor.execute(\"SELECT binaryID FROM binaries\")\nrows = cursor.fetchall()\nfor row in rows:\n    if (row[0] == binary):\n        return True\nreturn False", "path": "malfunction\\mallearn.py", "repo_name": "Dynetics/Malfunction", "stars": 191, "license": "lgpl-2.1", "language": "python", "size": 1003}
{"docstring": "\"\"\" Learns the given binary to the database. Used if you know if a binary is\ngood or bad (Notepad vs. a known malware)\n\nUsage:\n   python3 mallearn malware.exe blacklist -a 'Bad Guy' -c 'Some malware'\n   python3 mallearn notepad.exe whitelist -a 'Microsoft' -D 'test.db'\"\"\"\n\n", "func_signal": "def main():\n", "code": "args = argparse_setup()\n\n# Check if path is a directory, if so, run mal-get and mal-learn\n# for the entire directory\nis_directory = os.path.isdir(args.PATH)\nif is_directory:\n    directory_learn(args)\n    return True\nelse:\n    try:\n        path = args.PATH\n        # This makes sure if sigsOnly is used that it follows\n        # the binary_hash, hash_list format\n        if args.sigsOnly is True:\n            f = open(path, \"r\")\n            binary_hash = f.readline().strip()\n            hash_list = [x.strip() for x in f]\n            f.close()\n            check_format(binary_hash, hash_list)\n        else:\n                hash_tuple, sizes = malget.malget(path, args.unpack)\n                binary_hash = hash_tuple[0]\n                hash_list = [x.strip() for x in hash_tuple[1]]\n        if args.filenames == 'unknown':\n            args.filenames = path\n        filetype = get_filetype(path)\n        mallearn(args, binary_hash, hash_list, filetype)\n        return True\n    except ValueError:\n        print(\"That file cannot be disassembled\")\n        return False\n    except Warning:\n        print(\"That file is already in the database, use the -o flag to \"\n              \"overwrite\")\n        return False\n    except Exception as err:\n        print(\"There was an error reading that file, are you sure it \"\n              \"was an executable?\\nError Information:\")\n        print('\\t', type(err))\n        print('\\t', err)\n        return False\n\nreturn True", "path": "malfunction\\mallearn.py", "repo_name": "Dynetics/Malfunction", "stars": 191, "license": "lgpl-2.1", "language": "python", "size": 1003}
{"docstring": "\"\"\" Verify all the function hashes are ssdeep hashes \"\"\"\n\n", "func_signal": "def check_ssdeep(function_hash):\n", "code": "var = re.findall(r\"([0-9]+:[a-zA-Z\\d\\/+]+:[a-zA-z\\d\\/+]+)\", function_hash)\nif str(var) != (\"['\"+function_hash+\"']\"):\n    return False\nreturn True", "path": "malfunction\\mallearn.py", "repo_name": "Dynetics/Malfunction", "stars": 191, "license": "lgpl-2.1", "language": "python", "size": 1003}
{"docstring": "\"\"\" Make sure the functions and binary signatures are\nin the correct format \"\"\"\n\n", "func_signal": "def check_format(binary_hash, hash_list):\n", "code": "if not (check_binary_hash(binary_hash)):\n    print(\"The md5 hash at the top of the signatures provided was not \"\n          \"in the proper format. Use malget to ensure it is properly \"\n          \"formatted.\")\n    sys.exit()\nfor function_hash in hash_list:\n    if not (check_ssdeep(function_hash)):\n        print(\"The ssdeep hash {0} was not in the proper format. Use \"\n              \"malget to ensure the file is in the proper \"\n              \"format\".format(function_hash))\n        sys.exit()\nprint(\"All hashes match the proper format\")\nreturn True", "path": "malfunction\\mallearn.py", "repo_name": "Dynetics/Malfunction", "stars": 191, "license": "lgpl-2.1", "language": "python", "size": 1003}
{"docstring": "\"\"\" Determines the file type then outputs the binary md5 hash\n and the function fuzzy hashes\n\n Usage:\n    python malget.py [FILE] \"\"\"\n\n", "func_signal": "def main():\n", "code": "args = argparse_setup()\n\noutput_file = \"malgetOutput.txt\"\nif args.output:\n    output_file = args.output\nbinary_tuple, sizes = malget(args.PATH, args.unpack)\nwith open(output_file, \"w\") as f:\n    f.write(binary_tuple[0]+\"\\n\")\n    for item in binary_tuple[1]:\n        f.write(item + \"\\n\")\nprint(\"Output to file {0}\".format(output_file))", "path": "malfunction\\malget.py", "repo_name": "Dynetics/Malfunction", "stars": 191, "license": "lgpl-2.1", "language": "python", "size": 1003}
{"docstring": "\"\"\" Prints the underscores for the unfilled side of the gradient \"\"\"\n\n", "func_signal": "def print_underscores(colors, color_amount, padding=0):\n", "code": "for color in colors:\n    if padding:\n        print(((\"\\033[38;5;\"+str(color)+\"m\"\n                +\"_\\033[0m\")*(color_amount-1)), end=\"\")\n        padding -= 1\n    else:\n        print(((\"\\033[38;5;\"+str(color)+\"m\"\n                +\"_\\033[0m\")*color_amount), end=\"\")", "path": "malfunction\\gradient.py", "repo_name": "Dynetics/Malfunction", "stars": 191, "license": "lgpl-2.1", "language": "python", "size": 1003}
{"docstring": "\"\"\" Get the file type from the given binary\n\nGotten by the getting the first 20 character of the 'file' command\"\"\"\n\n", "func_signal": "def get_filetype(path):\n", "code": "output = str(subprocess.check_output([\"file\", path]))\nfiletype = output.split(':')[1].strip()[0:19].strip()\nreturn filetype", "path": "malfunction\\mallearn.py", "repo_name": "Dynetics/Malfunction", "stars": 191, "license": "lgpl-2.1", "language": "python", "size": 1003}
{"docstring": "\"\"\" Linux only solution for checking if a file is unpacked\"\"\"\n\n", "func_signal": "def check_packed(filename, unpack):\n", "code": "if not shutil.which(\"grep\"):\n    print(\"Cannot check if binary is packed\")\n    return False\n\npackage_breadcrumbs = [\"UPX\", \"aspack\", \"NSP\", \"NTKrnl\",\n                       \"PEC2\", \"PECompact2\", \"Thermida\", \"aPa2Wa\"]\nprint(\"Determining if {0} is packed\".format(filename), end=\"...\")\nfor packer in package_breadcrumbs:\n    returncode = subprocess.call([\"grep\", packer, filename])\n    if returncode == 0:\n        print(\"That file is most likely packed by {0}\".format(packer))\n        return True\nprint(\"That file is likely not packed by common packers\")\nreturn False", "path": "malfunction\\malget.py", "repo_name": "Dynetics/Malfunction", "stars": 191, "license": "lgpl-2.1", "language": "python", "size": 1003}
{"docstring": "\"\"\" Verify that the binary hash is an md5 hash \"\"\"\n\n", "func_signal": "def check_binary_hash(binary_hash):\n", "code": "var = re.findall(r\"([a-fA-F\\d]{32})\", binary_hash)\nif str(var) != (\"['\"+binary_hash+\"']\"):\n    return False\nreturn True", "path": "malfunction\\mallearn.py", "repo_name": "Dynetics/Malfunction", "stars": 191, "license": "lgpl-2.1", "language": "python", "size": 1003}
{"docstring": "\"\"\" Runs malfunction on a folder and all files within \"\"\"\n\n", "func_signal": "def directory_malfunction(args, cursor):\n", "code": "path = args.PATH\nif not path.endswith(\"/\"):\n    path += \"/\"\ndirectory = os.listdir(path)\nfor file in directory:\n    try:\n        # Don't follow links so we don't end up in loops.\n        if os.path.islink(path + file):\n            continue\n        # Extend the list if the path points to a directory\n        if os.path.isdir(path + file):\n            inner_directory = os.listdir(path + file)\n            inner_directory = [file+\"/\"+x for x in inner_directory]\n            directory.extend(inner_directory)\n            continue\n        hash_tuple, size_list = malget.malget(path + file,\n                                              args.unpack)\n        filetype = get_filetype(path+file)\n        compute_score(cursor, hash_tuple, size_list, filetype,\n                      args.add_strong_matches, args.all, args.debug)\n        print(\"-\"*30)\n    except ValueError:\n        print(\"That file cannot be disassembled\")\n        print(\"-\"*30)\n        continue\n    except Exception as err:\n        print(\"There was an error reading that file, are you sure it \"\n              \"was an executable?\\nError Information:\")\n        print('\\t', type(err))\n        print('\\t', err)\n        print(\"-\"*30)\n        continue\n    except Warning:\n        print(\"That file is already in the database.\")\n        print(\"-\"*30)\n        continue", "path": "malfunction\\malfunction.py", "repo_name": "Dynetics/Malfunction", "stars": 191, "license": "lgpl-2.1", "language": "python", "size": 1003}
{"docstring": "# TODO: not working on localhost\n", "func_signal": "def read_file_or_url(self, fname):\n", "code": "if isinstance(fname, file):\n    result = open(fname, 'r')\nelse:\n    match = self.urlre.match(fname)\n    if match:\n        result = urllib.urlopen(match.group(1))\n    else:\n        fname = os.path.expanduser(fname)\n        try:\n            result = open(os.path.expanduser(fname), 'r')\n        except IOError:\n            result = open('%s.%s' % (os.path.expanduser(fname),\n                                     self.defaultExtension), 'r')\nreturn result", "path": "lib\\cmd2.py", "repo_name": "n0tr00t/Beehive", "stars": 155, "license": "gpl-3.0", "language": "python", "size": 268}
{"docstring": "'''\nPreserves the spacing originally in the argument after\nthe removal of options.\n\n>>> remaining_args('-f bar   bar   cow', ['bar', 'cow'])\n'bar   cow'\n'''\n", "func_signal": "def remaining_args(oldArgs, newArgList):\n", "code": "pattern = '\\s+'.join(re.escape(a) for a in newArgList) + '\\s*$'\nmatchObj = re.search(pattern, oldArgs)\nreturn oldArgs[matchObj.start():]", "path": "lib\\cmd2.py", "repo_name": "n0tr00t/Beehive", "stars": 155, "license": "gpl-3.0", "language": "python", "size": 268}
{"docstring": "'''Used as a decorator and passed a list of optparse-style options,\n   alters a cmd2 method to populate its ``opts`` argument from its\n   raw text argument.\n\n   Example: transform\n   def do_something(self, arg):\n\n   into\n   @options([make_option('-q', '--quick', action=\"store_true\",\n             help=\"Makes things fast\")],\n             \"source dest\")\n   def do_something(self, arg, opts):\n       if opts.quick:\n           self.fast_button = True\n   '''\n", "func_signal": "def options(option_list, arg_desc=\"arg\"):\n", "code": "if not isinstance(option_list, list):\n    option_list = [option_list]\nfor opt in option_list:\n    options_defined.append(pyparsing.Literal(opt.get_opt_string()))\ndef option_setup(func):\n    optionParser = OptionParser()\n    for opt in option_list:\n        optionParser.add_option(opt)\n    optionParser.set_usage(\"%s [options] %s\" % (func.__name__[3:], arg_desc))\n    optionParser._func = func\n    def new_func(instance, arg):\n        try:\n            opts, newArgList = optionParser.parse_args(arg.split())\n            # Must find the remaining args in the original argument list, but\n            # mustn't include the command itself\n            #if hasattr(arg, 'parsed') and newArgList[0] == arg.parsed.command:\n            #    newArgList = newArgList[1:]\n            newArgs = remaining_args(arg, newArgList)\n            if isinstance(arg, ParsedString):\n                arg = arg.with_args_replaced(newArgs)\n            else:\n                arg = newArgs\n        except optparse.OptParseError as e:\n            print (e)\n            optionParser.print_help()\n            return\n        if hasattr(opts, '_exit'):\n            return None\n        result = func(instance, arg, opts)\n        return result\n    new_func.__doc__ = '%s\\n%s' % (func.__doc__, optionParser.format_help())\n    return new_func\nreturn option_setup", "path": "lib\\cmd2.py", "repo_name": "n0tr00t/Beehive", "stars": 155, "license": "gpl-3.0", "language": "python", "size": 268}
{"docstring": "'''Convenient shortcut for self.stdout.write(); adds newline if necessary.'''\n", "func_signal": "def poutput(self, msg):\n", "code": "if msg:\n    self.stdout.write(msg)\n    if msg[-1] != '\\n':\n        self.stdout.write('\\n')", "path": "lib\\cmd2.py", "repo_name": "n0tr00t/Beehive", "stars": 155, "license": "gpl-3.0", "language": "python", "size": 268}
{"docstring": "'''Given a string (``val``), returns that string wrapped in UNIX-style\n   special characters that turn on (and then off) text color and style.\n   If the ``colors`` environment paramter is ``False``, or the application\n   is running on Windows, will return ``val`` unchanged.\n   ``color`` should be one of the supported strings (or styles):\n   red/blue/green/cyan/magenta, bold, underline'''\n", "func_signal": "def colorize(self, val, color):\n", "code": "if self.colors and (self.stdout == self.initial_stdout):\n    return self.colorcodes[color][True] + val + self.colorcodes[color][False]\nreturn val", "path": "lib\\cmd2.py", "repo_name": "n0tr00t/Beehive", "stars": 155, "license": "gpl-3.0", "language": "python", "size": 268}
{"docstring": "\"\"\"For printing nonessential feedback.  Can be silenced with `quiet`.\n   Inclusion in redirected output is controlled by `feedback_to_output`.\"\"\"\n", "func_signal": "def pfeedback(self, msg):\n", "code": "if not self.quiet:\n    if self.feedback_to_output:\n        self.poutput(msg)\n    else:\n        print (msg)", "path": "lib\\cmd2.py", "repo_name": "n0tr00t/Beehive", "stars": 155, "license": "gpl-3.0", "language": "python", "size": 268}
{"docstring": "\"\"\"Repeatedly issue a prompt, accept input, parse an initial prefix\noff the received input, and dispatch to action methods, passing them\nthe remainder of the line as argument.\n\"\"\"\n\n# An almost perfect copy from Cmd; however, the pseudo_raw_input portion\n# has been split out so that it can be called separately\n\n", "func_signal": "def _cmdloop(self, intro=None):\n", "code": "self.preloop()\nif self.use_rawinput and self.completekey:\n    try:\n        import readline\n        self.old_completer = readline.get_completer()\n        readline.set_completer(self.complete)\n        readline.parse_and_bind(self.completekey+\": complete\")\n    except ImportError:\n        pass\ntry:\n    if intro is not None:\n        self.intro = intro\n    if self.intro:\n        self.stdout.write(str(self.intro)+\"\\n\")\n    stop = None\n    while not stop:\n        if self.cmdqueue:\n            line = self.cmdqueue.pop(0)\n        else:\n            line = self.pseudo_raw_input(self.prompt)\n        if (self.echo) and (isinstance(self.stdin, file)):\n            self.stdout.write(line + '\\n')\n        stop = self.onecmd_plus_hooks(line)\n    self.postloop()\nfinally:\n    if self.use_rawinput and self.completekey:\n        try:\n            import readline\n            readline.set_completer(self.old_completer)\n        except ImportError:\n            pass\n    return stop", "path": "lib\\cmd2.py", "repo_name": "n0tr00t/Beehive", "stars": 155, "license": "gpl-3.0", "language": "python", "size": 268}
{"docstring": "\"\"\"`save [N] [filename.ext]`\n\nSaves command from history to file.\n\n| N => Number of command (from history), or `*`;\n|      most recent command if omitted\"\"\"\n\n", "func_signal": "def do_save(self, arg):\n", "code": "try:\n    args = self.saveparser.parseString(arg)\nexcept pyparsing.ParseException:\n    self.perror('Could not understand save target %s' % arg)\n    raise SyntaxError(self.do_save.__doc__)\nfname = args.fname or self.default_file_name\nif args.idx == '*':\n    saveme = '\\n\\n'.join(self.history[:])\nelif args.idx:\n    saveme = self.history[int(args.idx)-1]\nelse:\n    saveme = self.history[-1]\ntry:\n    f = open(os.path.expanduser(fname), 'w')\n    f.write(saveme)\n    f.close()\n    self.pfeedback('Saved to %s' % (fname))\nexcept Exception as e:\n    self.perror('Error saving %s' % (fname))\n    raise", "path": "lib\\cmd2.py", "repo_name": "n0tr00t/Beehive", "stars": 155, "license": "gpl-3.0", "language": "python", "size": 268}
{"docstring": "'''Summary report of interactive parameters.'''\n", "func_signal": "def do_cmdenvironment(self, args):\n", "code": "self.stdout.write(\"\"\"\nCommands are %(casesensitive)scase-sensitive.\nCommands may be terminated with: %(terminators)s\nSettable parameters: %(settable)s\\n\"\"\" % \\\n                                       { 'casesensitive': (self.case_insensitive and 'not ') or '',\n                                         'terminators': str(self.terminators),\n                                         'settable': ' '.join(self.settable)\n                                         })", "path": "lib\\cmd2.py", "repo_name": "n0tr00t/Beehive", "stars": 155, "license": "gpl-3.0", "language": "python", "size": 268}
{"docstring": "'''Shows value of a parameter.'''\n", "func_signal": "def do_show(self, arg, opts):\n", "code": "param = arg.strip().lower()\nresult = {}\nmaxlen = 0\nfor p in self.settable:\n    if (not param) or p.startswith(param):\n        result[p] = '%s: %s' % (p, str(getattr(self, p)))\n        maxlen = max(maxlen, len(result[p]))\nif result:\n    for p in sorted(result):\n        if opts.long:\n            self.poutput('%s # %s' % (result[p].ljust(maxlen), self.settable[p]))\n        else:\n            self.poutput(result[p])\nelse:\n    raise NotImplementedError(\"Parameter '%s' not supported (type 'show' for list of parameters).\" % param)", "path": "lib\\cmd2.py", "repo_name": "n0tr00t/Beehive", "stars": 155, "license": "gpl-3.0", "language": "python", "size": 268}
{"docstring": "# TODO: need to normalize target in Poc\n", "func_signal": "def _runPoc(self, target):\n", "code": "target = target.strip()\nreturn self._pocObj.run(target=target, verify=self.verify)", "path": "lib\\scanner.py", "repo_name": "n0tr00t/Beehive", "stars": 155, "license": "gpl-3.0", "language": "python", "size": 268}
{"docstring": "'''print poc_info'''\n", "func_signal": "def showInfo(self):\n", "code": "from pprint import pprint\npprint(self.module.MyPoc.poc_info)", "path": "lib\\poc.py", "repo_name": "n0tr00t/Beehive", "stars": 155, "license": "gpl-3.0", "language": "python", "size": 268}
{"docstring": "\"\"\"Interpret the argument as though it had been typed in response\nto the prompt.\n\nThis may be overridden, but should not normally need to be;\nsee the precmd() and postcmd() methods for useful execution hooks.\nThe return value is a flag indicating whether interpretation of\ncommands by the interpreter should stop.\n\nThis (`cmd2`) version of `onecmd` already override's `cmd`'s `onecmd`.\n\n\"\"\"\n", "func_signal": "def onecmd(self, line):\n", "code": "statement = self.parsed(line)\nself.lastcmd = statement.parsed.raw\nfuncname = self.func_named(statement.parsed.command)\nif not funcname:\n    return self._default(statement)\ntry:\n    func = getattr(self, funcname)\nexcept AttributeError:\n    return self._default(statement)\nstop = func(statement)\nreturn stop", "path": "lib\\cmd2.py", "repo_name": "n0tr00t/Beehive", "stars": 155, "license": "gpl-3.0", "language": "python", "size": 268}
{"docstring": "\"\"\"Tries to force a new value into the same type as the current.\"\"\"\n", "func_signal": "def cast(current, new):\n", "code": "typ = type(current)\nif typ == bool:\n    try:\n        return bool(int(new))\n    except (ValueError, TypeError):\n        pass\n    try:\n        new = new.lower()\n    except:\n        pass\n    if (new=='on') or (new[0] in ('y','t')):\n        return True\n    if (new=='off') or (new[0] in ('n','f')):\n        return False\nelse:\n    try:\n        return typ(new)\n    except:\n        pass\nprint (\"Problem setting parameter (now %s) to %s; incorrect type?\" % (current, new))\nreturn current", "path": "lib\\cmd2.py", "repo_name": "n0tr00t/Beehive", "stars": 155, "license": "gpl-3.0", "language": "python", "size": 268}
{"docstring": "'''Returns an attribute's value, or None (no error) if undefined.\n   Analagous to .get() for dictionaries.  Useful when checking for\n   value of options that may not have been defined on a given\n   method.'''\n", "func_signal": "def _attr_get_(obj, attr):\n", "code": "try:\n    return getattr(obj, attr)\nexcept AttributeError:\n    return None", "path": "lib\\cmd2.py", "repo_name": "n0tr00t/Beehive", "stars": 155, "license": "gpl-3.0", "language": "python", "size": 268}
{"docstring": "\"\"\"Handle pretty printing operations onto a stream using a set of\nconfigured parameters.\n\nindent\n    Number of spaces to indent for each level of nesting.\n\nwidth\n    Attempted maximum number of columns in the output.\n\ndepth\n    The maximum depth to print out nested structures.\n\nstream\n    The desired output stream.  If omitted (or false), the standard\n    output stream available at construction will be used.\n\n\"\"\"\n", "func_signal": "def __init__(self, indent=1, width=80, depth=None, stream=None):\n", "code": "indent = int(indent)\nwidth = int(width)\nassert indent >= 0, \"indent must be >= 0\"\nassert depth is None or depth > 0, \"depth must be > 0\"\nassert width, \"width must be != 0\"\nself._depth = depth\nself._indent_per_level = indent\nself._width = width\nif stream is not None:\n    self._stream = stream\nelse:\n    self._stream = _sys.stdout", "path": "lib\\pprint2.py", "repo_name": "n0tr00t/Beehive", "stars": 155, "license": "gpl-3.0", "language": "python", "size": 268}
{"docstring": "\"\"\"list [arg]: lists last command issued\n\nno arg -> list most recent command\narg is integer -> list one history item, by index\na..b, a:b, a:, ..b -> list spans from a (or start) to b (or end)\narg is string -> list all commands matching string search\narg is /enclosed in forward-slashes/ -> regular expression search\n\"\"\"\n", "func_signal": "def do_list(self, arg):\n", "code": "try:\n    history = self.history.span(arg or '-1')\nexcept IndexError:\n    history = self.history.search(arg)\nfor hi in history:\n    self.poutput(hi.pr())", "path": "lib\\cmd2.py", "repo_name": "n0tr00t/Beehive", "stars": 155, "license": "gpl-3.0", "language": "python", "size": 268}
{"docstring": "# The outermost level of try/finally nesting can be condensed once\n# Python 2.4 support can be dropped.\n", "func_signal": "def onecmd_plus_hooks(self, line):\n", "code": "stop = 0\ntry:\n    try:\n        statement = self.complete_statement(line)\n        (stop, statement) = self.postparsing_precmd(statement)\n        if stop:\n            return self.postparsing_postcmd(stop)\n        if statement.parsed.command not in self.excludeFromHistory:\n            self.history.append(statement.parsed.raw)\n        try:\n            self.redirect_output(statement)\n            timestart = datetime.datetime.now()\n            statement = self.precmd(statement)\n            stop = self.onecmd(statement)\n            stop = self.postcmd(stop, statement)\n            if self.timing:\n                self.pfeedback('\\nElapsed: %s\\n' % str(datetime.datetime.now() - timestart))\n        finally:\n            self.restore_output(statement)\n    except EmptyStatement:\n        return 0\n    except Exception as e:\n        self.perror(str(e), statement)\nfinally:\n    return self.postparsing_postcmd(stop)", "path": "lib\\cmd2.py", "repo_name": "n0tr00t/Beehive", "stars": 155, "license": "gpl-3.0", "language": "python", "size": 268}
{"docstring": "\"\"\"history [arg]: lists past commands issued\n\n| no arg:         list all\n| arg is integer: list one history item, by index\n| arg is string:  string search\n| arg is /enclosed in forward-slashes/: regular expression search\n\"\"\"\n", "func_signal": "def do_history(self, arg, opts):\n", "code": "if arg:\n    history = self.history.get(arg)\nelse:\n    history = self.history\nfor hi in history:\n    if opts.script:\n        self.poutput(hi)\n    else:\n        self.stdout.write(hi.pr())", "path": "lib\\cmd2.py", "repo_name": "n0tr00t/Beehive", "stars": 155, "license": "gpl-3.0", "language": "python", "size": 268}
{"docstring": "'''\npy <command>: Executes a Python command.\npy: Enters interactive Python mode.\nEnd with ``Ctrl-D`` (Unix) / ``Ctrl-Z`` (Windows), ``quit()``, '`exit()``.\nNon-python commands can be issued with ``cmd(\"your command\")``.\nRun python code from external files with ``run(\"filename.py\")``\n'''\n", "func_signal": "def do_py(self, arg):\n", "code": "self.pystate['self'] = self\narg = arg.parsed.raw[2:].strip()\nlocalvars = (self.locals_in_py and self.pystate) or {}\ninterp = InteractiveConsole(locals=localvars)\ninterp.runcode('import sys, os;sys.path.insert(0, os.getcwd())')\nif arg.strip():\n    interp.runcode(arg)\nelse:\n    def quit():\n        raise EmbeddedConsoleExit\n    def onecmd_plus_hooks(arg):\n        return self.onecmd_plus_hooks(arg + '\\n')\n    def run(arg):\n        try:\n            file = open(arg)\n            interp.runcode(file.read())\n            file.close()\n        except IOError as e:\n            self.perror(e)\n    self.pystate['quit'] = quit\n    self.pystate['exit'] = quit\n    self.pystate['cmd'] = onecmd_plus_hooks\n    self.pystate['run'] = run\n    try:\n        cprt = 'Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.'\n        keepstate = Statekeeper(sys, ('stdin','stdout'))\n        sys.stdout = self.stdout\n        sys.stdin = self.stdin\n        interp.interact(banner= \"Python %s on %s\\n%s\\n(%s)\\n%s\" %\n                        (sys.version, sys.platform, cprt, self.__class__.__name__, self.do_py.__doc__))\n    except EmbeddedConsoleExit:\n        pass\n    keepstate.restore()", "path": "lib\\cmd2.py", "repo_name": "n0tr00t/Beehive", "stars": 155, "license": "gpl-3.0", "language": "python", "size": 268}
{"docstring": "\"\"\"Return a string used to separated rows or separate header from\nrows\n\"\"\"\n", "func_signal": "def _build_hline(self, is_header=False):\n", "code": "horiz = self._char_horiz\nif (is_header):\n    horiz = self._char_header\n# compute cell separator\ns = \"%s%s%s\" % (horiz, [horiz, self._char_corner][self._has_vlines()],\n    horiz)\n# build the line\nl = s.join([horiz * n for n in self._width])\n# add border if needed\nif self._has_border():\n    l = \"%s%s%s%s%s\\n\" % (self._char_corner, horiz, l, horiz,\n        self._char_corner)\nelse:\n    l += \"\\n\"\nreturn l", "path": "dvol_python\\texttable.py", "repo_name": "ClusterHQ/dvol", "stars": 252, "license": "apache-2.0", "language": "python", "size": 6688}
{"docstring": "\"\"\"Split each element of line to fit the column width\n\nEach element is turned into a list, result of the wrapping of the\nstring to the desired width\n\"\"\"\n\n", "func_signal": "def _splitit(self, line, isheader):\n", "code": "line_wrapped = []\nfor cell, width in zip(line, self._width):\n    array = []\n    original_cell = cell\n    lost_color = bcolors.WHITE\n    for attr in bcolors_public_props():\n        cell = cell.replace(\n            getattr(bcolors, attr), '').replace(bcolors.ENDC,'')\n        if cell.replace(bcolors.ENDC,'') != original_cell.replace(\n                bcolors.ENDC,'') and attr != 'ENDC':\n            if not lost_color:\n                lost_color = attr\n    for c in cell.split('\\n'):\n        if type(c) is not str:\n            try:\n                c = str(c, 'utf')\n            except UnicodeDecodeError as strerror:\n                sys.stderr.write(\"UnicodeDecodeError exception for string '%s': %s\\n\" % (c, strerror))\n                c = str(c, 'utf', 'replace')\n        try:\n            array.extend(\n                [get_color_string(\n                    getattr(bcolors, lost_color),x\n                    ) for x in  textwrap.wrap(c, width)\n                ]\n            )\n        except AttributeError:\n            array.extend(textwrap.wrap(c, width))\n    line_wrapped.append(array)\nmax_cell_lines = reduce(max, list(map(len, line_wrapped)))\nfor cell, valign in zip(line_wrapped, self._valign):\n    if isheader:\n        valign = \"t\"\n    if valign == \"m\":\n        missing = max_cell_lines - len(cell)\n        cell[:0] = [\"\"] * (missing // 2)\n        cell.extend([\"\"] * (missing // 2 + missing % 2))\n    elif valign == \"b\":\n        cell[:0] = [\"\"] * (max_cell_lines - len(cell))\n    else:\n        cell.extend([\"\"] * (max_cell_lines - len(cell)))\nreturn line_wrapped", "path": "dvol_python\\texttable.py", "repo_name": "ClusterHQ/dvol", "stars": 252, "license": "apache-2.0", "language": "python", "size": 6688}
{"docstring": "\"\"\"Handles string formatting of cell data\n\n    i - index of the cell datatype in self._dtype\n    x - cell data to format\n\"\"\"\n", "func_signal": "def _str(self, i, x):\n", "code": "if type(x) is str:\n    return x\nelse:\n    if x is None:\n        return str(x)\n    else:\n        return str(x.encode('utf-8'))", "path": "dvol_python\\texttable.py", "repo_name": "ClusterHQ/dvol", "stars": 252, "license": "apache-2.0", "language": "python", "size": 6688}
{"docstring": "\"\"\"Check if alignment has been specified, set default one if not\n\"\"\"\n\n", "func_signal": "def _check_align(self):\n", "code": "if not hasattr(self, \"_align\"):\n    self._align = [\"l\"] * self._row_size\nif not hasattr(self, \"_valign\"):\n    self._valign = [\"t\"] * self._row_size", "path": "dvol_python\\texttable.py", "repo_name": "ClusterHQ/dvol", "stars": 252, "license": "apache-2.0", "language": "python", "size": 6688}
{"docstring": "\"\"\"Specify the header of the table\n\"\"\"\n\n", "func_signal": "def header(self, array):\n", "code": "self._check_row_size(array)\nself._header = list(map(str, array))", "path": "dvol_python\\texttable.py", "repo_name": "ClusterHQ/dvol", "stars": 252, "license": "apache-2.0", "language": "python", "size": 6688}
{"docstring": "\"\"\"Redefining len here so it will be able to work with non-ASCII characters\n\"\"\"\n", "func_signal": "def len(iterable):\n", "code": "if not isinstance(iterable, str):\n    return iterable.__len__()\n\ntry:\n    return len(str(iterable, 'utf'))\nexcept:\n    return iterable.__len__()", "path": "dvol_python\\texttable.py", "repo_name": "ClusterHQ/dvol", "stars": 252, "license": "apache-2.0", "language": "python", "size": 6688}
{"docstring": "\"\"\"Check that the specified array fits the previous rows size\n\"\"\"\n\n", "func_signal": "def _check_row_size(self, array):\n", "code": "if not self._row_size:\n    self._row_size = len(array)\nelif self._row_size != len(array):\n    raise ArraySizeError(\"array should contain %d elements\" \\\n        % self._row_size)", "path": "dvol_python\\texttable.py", "repo_name": "ClusterHQ/dvol", "stars": 252, "license": "apache-2.0", "language": "python", "size": 6688}
{"docstring": "\"\"\"Print an horizontal line\n\"\"\"\n\n", "func_signal": "def _hline(self):\n", "code": "if not self._hline_string:\n    self._hline_string = self._build_hline()\nreturn self._hline_string", "path": "dvol_python\\texttable.py", "repo_name": "ClusterHQ/dvol", "stars": 252, "license": "apache-2.0", "language": "python", "size": 6688}
{"docstring": "\"\"\"Return the width of the cell\n\nSpecial characters are taken into account to return the width of the\ncell, such like newlines and tabs\n\"\"\"\n\n", "func_signal": "def _len_cell(self, cell):\n", "code": "for attr in bcolors_public_props():\n    cell = cell.replace(getattr(bcolors, attr), '').replace(bcolors.ENDC,'')\n\ncell_lines = cell.split('\\n')\nmaxi = 0\nfor line in cell_lines:\n    length = 0\n    parts = line.split('\\t')\n    for part, i in zip(parts, list(range(1, len(parts) + 1))):\n        for attr in bcolors_public_props():\n            part = part.replace(getattr(bcolors, attr), '')\n        length = length + len(part)\n        if i < len(parts):\n            length = (length//8 + 1) * 8\n    maxi = max(maxi, length)\nreturn maxi", "path": "dvol_python\\texttable.py", "repo_name": "ClusterHQ/dvol", "stars": 252, "license": "apache-2.0", "language": "python", "size": 6688}
{"docstring": "\"\"\"Set the desired columns alignment\n\n- the elements of the array should be either \"l\", \"c\" or \"r\":\n\n    * \"l\": column flushed left\n    * \"c\": column centered\n    * \"r\": column flushed right\n\"\"\"\n\n", "func_signal": "def set_cols_align(self, array):\n", "code": "self._check_row_size(array)\nself._align = array", "path": "dvol_python\\texttable.py", "repo_name": "ClusterHQ/dvol", "stars": 252, "license": "apache-2.0", "language": "python", "size": 6688}
{"docstring": "\"\"\"Draw the table\n\n- the table is returned as a whole string\n\"\"\"\n\n", "func_signal": "def draw(self):\n", "code": "if not self._header and not self._rows:\n    return\nself._compute_cols_width()\nself._check_align()\nout = \"\"\nif self._has_border():\n    out += self._hline()\nif self._header:\n    out += self._draw_line(self._header, isheader=True)\n    if self._has_header():\n        out += self._hline_header()\nlength = 0\nfor row in self._rows:\n    length += 1\n    out += self._draw_line(row)\n    if self._has_hlines() and length < len(self._rows):\n        out += self._hline()\nif self._has_border():\n    out += self._hline()\nreturn out[:-1]", "path": "dvol_python\\texttable.py", "repo_name": "ClusterHQ/dvol", "stars": 252, "license": "apache-2.0", "language": "python", "size": 6688}
{"docstring": "\"\"\"Return an array with the width of each column\n\nIf a specific width has been specified, exit. If the total of the\ncolumns width exceed the table desired width, another width will be\ncomputed to fit, and cells will be wrapped.\n\"\"\"\n\n", "func_signal": "def _compute_cols_width(self):\n", "code": "if hasattr(self, \"_width\"):\n    return\nmaxi = []\nif self._header:\n    maxi = [ self._len_cell(x) for x in self._header ]\nfor row in self._rows:\n    for cell,i in zip(row, list(range(len(row)))):\n        try:\n            maxi[i] = max(maxi[i], self._len_cell(cell))\n        except (TypeError, IndexError):\n            maxi.append(self._len_cell(cell))\nitems = len(maxi)\nlength = reduce(lambda x,y: x+y, maxi)\nif self._max_width and length + items * 3 + 1 > self._max_width:\n    max_lengths = maxi\n    maxi = [(self._max_width - items * 3 -1) // items \\\n        for n in range(items)]\n\n    # free space to distribute\n    free = 0\n\n    # how many columns are oversized\n    oversized = 0\n\n    # reduce size of columns that need less space and calculate how\n    # much space is freed\n    for col, max_len in enumerate(max_lengths):\n        current_length = maxi[col]\n\n        # column needs less space, adjust and\n        # update free space\n        if current_length > max_len:\n            free += current_length - max_len\n            maxi[col] = max_len\n\n        # column needs more space, count it\n        elif max_len > current_length:\n            oversized += 1\n\n    # as long as free space is available, distribute it\n    while free > 0:\n        # available free space for each oversized column\n        free_part = int(math.ceil(float(free) / float(oversized)))\n\n        for col, max_len in enumerate(max_lengths):\n            current_length = maxi[col]\n\n            # column needs more space\n            if current_length < max_len:\n\n                # how much space is needed\n                needed = max_len - current_length\n\n                # enough free space for column\n                if needed <= free_part:\n                    maxi[col] = max_len\n                    free -= needed\n                    oversized -= 1\n\n                # still oversized after re-sizing\n                else:\n                    maxi[col] = maxi[col] + free_part\n                    free -= free_part\nself._width = maxi", "path": "dvol_python\\texttable.py", "repo_name": "ClusterHQ/dvol", "stars": 252, "license": "apache-2.0", "language": "python", "size": 6688}
{"docstring": "\"\"\"Reset the instance\n\n- reset rows and header\n\"\"\"\n\n", "func_signal": "def reset(self):\n", "code": "self._hline_string = None\nself._row_size = None\nself._header = []\nself._rows = []", "path": "dvol_python\\texttable.py", "repo_name": "ClusterHQ/dvol", "stars": 252, "license": "apache-2.0", "language": "python", "size": 6688}
{"docstring": "\"\"\"Set the characters used to draw lines between rows and columns\n\n- the array should contain 4 fields:\n\n    [horizontal, vertical, corner, header]\n\n- default is set to:\n\n    ['-', '|', '+', '=']\n\"\"\"\n\n", "func_signal": "def set_chars(self, array):\n", "code": "if len(array) != 4:\n    raise ArraySizeError(\"array should contain 4 characters\")\narray = [ x[:1] for x in [ str(s) for s in array ] ]\n(self._char_horiz, self._char_vert,\n    self._char_corner, self._char_header) = array", "path": "dvol_python\\texttable.py", "repo_name": "ClusterHQ/dvol", "stars": 252, "license": "apache-2.0", "language": "python", "size": 6688}
{"docstring": "\"\"\"Set the desired columns vertical alignment\n\n- the elements of the array should be either \"t\", \"m\" or \"b\":\n\n    * \"t\": column aligned on the top of the cell\n    * \"m\": column aligned on the middle of the cell\n    * \"b\": column aligned on the bottom of the cell\n\"\"\"\n\n", "func_signal": "def set_cols_valign(self, array):\n", "code": "self._check_row_size(array)\nself._valign = array", "path": "dvol_python\\texttable.py", "repo_name": "ClusterHQ/dvol", "stars": 252, "license": "apache-2.0", "language": "python", "size": 6688}
{"docstring": "\"\"\"Draw a line\n\nLoop over a single cell length, over all the cells\n\"\"\"\n\n", "func_signal": "def _draw_line(self, line, isheader=False):\n", "code": "line = self._splitit(line, isheader)\nspace = \" \"\nout  = \"\"\nfor i in range(len(line[0])):\n    if self._has_border():\n        out += \"%s \" % self._char_vert\n    length = 0\n    for cell, width, align in zip(line, self._width, self._align):\n        length += 1\n        cell_line = cell[i]\n        lost_color = bcolors.WHITE\n        original_cell = cell_line\n        for attr in bcolors_public_props():\n            cell_line = cell_line.replace(\n                getattr(bcolors, attr), '').replace(bcolors.ENDC,''\n            )\n            if cell_line.replace(bcolors.ENDC,'') != original_cell.replace(\n                    bcolors.ENDC,'') and attr != 'ENDC':\n                if not lost_color:\n                    lost_color = attr\n        fill = width - len(cell_line)\n        try:\n            cell_line = get_color_string(\n                getattr(bcolors, lost_color),cell_line\n            )\n        except AttributeError:\n            pass\n        if isheader:\n            align = \"c\"\n        if align == \"r\":\n            out += \"%s \" % (fill * space + cell_line)\n        elif align == \"c\":\n            out += \"%s \" % (fill//2 * space + cell_line \\\n                    + (fill//2 + fill%2) * space)\n        else:\n            out += \"%s \" % (cell_line + fill * space)\n        if length < len(line):\n            out += \"%s \" % [space, self._char_vert][self._has_vlines()]\n    out += \"%s\\n\" % ['', self._char_vert][self._has_border()]\nreturn out", "path": "dvol_python\\texttable.py", "repo_name": "ClusterHQ/dvol", "stars": 252, "license": "apache-2.0", "language": "python", "size": 6688}
{"docstring": "\"\"\"Set the desired columns width\n\n- the elements of the array should be integers, specifying the\n  width of each column. For example:\n\n        [10, 20, 5]\n\"\"\"\n\n", "func_signal": "def set_cols_width(self, array):\n", "code": "self._check_row_size(array)\ntry:\n    array = list(map(int, array))\n    if reduce(min, array) <= 0:\n        raise ValueError\nexcept ValueError:\n    sys.stderr.write(\"Wrong argument in column width specification\\n\")\n    raise\nself._width = array", "path": "dvol_python\\texttable.py", "repo_name": "ClusterHQ/dvol", "stars": 252, "license": "apache-2.0", "language": "python", "size": 6688}
{"docstring": "\"\"\"Set the desired precision for float/exponential formats\n\n- width must be an integer >= 0\n\n- default value is set to 3\n\"\"\"\n\n", "func_signal": "def set_precision(self, width):\n", "code": "if not type(width) is int or width < 0:\n    raise ValueError('width must be an integer greater then 0')\nself._precision = width", "path": "dvol_python\\texttable.py", "repo_name": "ClusterHQ/dvol", "stars": 252, "license": "apache-2.0", "language": "python", "size": 6688}
{"docstring": "\"\"\"Add a row in the rows stack\n\n- cells can contain newlines and tabs\n\"\"\"\n\n", "func_signal": "def add_row(self, array):\n", "code": "self._check_row_size(array)\n\nif not hasattr(self, \"_dtype\"):\n    self._dtype = [\"a\"] * self._row_size\n\ncells = []\nfor i,x in enumerate(array):\n    cells.append(self._str(i,x))\nself._rows.append(cells)", "path": "dvol_python\\texttable.py", "repo_name": "ClusterHQ/dvol", "stars": 252, "license": "apache-2.0", "language": "python", "size": 6688}
{"docstring": "\"\"\"Constructor\n\n- max_width is an integer, specifying the maximum width of the table\n- if set to 0, size is unlimited, therefore cells won't be wrapped\n\"\"\"\n\n", "func_signal": "def __init__(self, max_width=80):\n", "code": "if max_width <= 0:\n    max_width = False\nself._max_width = max_width\nself._precision = 3\n\nself._deco = Texttable.VLINES | Texttable.HLINES | Texttable.BORDER | \\\n    Texttable.HEADER\nself.set_chars(['-', '|', '+', '='])\nself.reset()", "path": "dvol_python\\texttable.py", "repo_name": "ClusterHQ/dvol", "stars": 252, "license": "apache-2.0", "language": "python", "size": 6688}
{"docstring": "# Tunnel not found properties to the underlying array.\n", "func_signal": "def __getattr__(self, name):\n", "code": "flat = super().__getattribute__('flat')\nreturn getattr(flat, name)", "path": "layered\\network.py", "repo_name": "danijar/layered", "stars": 237, "license": "mit", "language": "python", "size": 647}
{"docstring": "# pylint: disable=redefined-variable-type\n", "func_signal": "def test_result(self):\n", "code": "iterable = range(14)\nbatches = batched(iterable, 3)\nbatches = list(batches)\nassert len(batches) == 5\nassert len(batches[0]) == 3\nassert len(batches[-1]) == 2", "path": "test\\test_utility.py", "repo_name": "danijar/layered", "stars": 237, "license": "mit", "language": "python", "size": 647}
{"docstring": "\"\"\"\nClose the window and stops the worker thread. The main thread will\nresume with the next command after the `start()` call.\n\"\"\"\n", "func_signal": "def stop(self):\n", "code": "assert threading.current_thread() == self.thread\nassert self.state.running\nself.state.running = False", "path": "layered\\plot.py", "repo_name": "danijar/layered", "stars": 237, "license": "mit", "language": "python", "size": 647}
{"docstring": "\"\"\"\nFind the symbol of the specified name inside the module or raise an\nexception.\n\"\"\"\n", "func_signal": "def _find_symbol(self, module, name, fallback=None):\n", "code": "if not hasattr(module, name) and fallback:\n    return self._find_symbol(module, fallback, None)\nreturn getattr(module, name)", "path": "layered\\problem.py", "repo_name": "danijar/layered", "stars": 237, "license": "mit", "language": "python", "size": 647}
{"docstring": "# pylint: disable=attribute-defined-outside-init\n", "func_signal": "def _load_symbols(self):\n", "code": "self.cost = self._find_symbol(layered.cost, self.cost)()\nself.dataset = self._find_symbol(layered.dataset, self.dataset)()", "path": "layered\\problem.py", "repo_name": "danijar/layered", "stars": 237, "license": "mit", "language": "python", "size": 647}
{"docstring": "\"\"\"\nStore the incoming activation, apply the activation function and store\nthe result as outgoing activation.\n\"\"\"\n", "func_signal": "def apply(self, incoming):\n", "code": "assert len(incoming) == self.size\nself.incoming = incoming\noutgoing = self.activation(self.incoming)\nassert len(outgoing) == self.size\nself.outgoing = outgoing", "path": "layered\\network.py", "repo_name": "danijar/layered", "stars": 237, "license": "mit", "language": "python", "size": 647}
{"docstring": "# Propagate backwards through the hidden layers but not the input\n# layer. The current weight matrix is the one to the right of the\n# current layer.\n", "func_signal": "def _delta_layers(self, weights, delta_output):\n", "code": "gradient = [delta_output]\nhidden = list(zip(weights[1:], self.network.layers[1:-1]))\nassert all(x.shape[0] - 1 == len(y) for x, y in hidden)\nfor weight, layer in reversed(hidden):\n    delta = self._delta_layer(layer, weight, gradient[-1])\n    gradient.append(delta)\nreturn reversed(gradient)", "path": "layered\\gradient.py", "repo_name": "danijar/layered", "stars": 237, "license": "mit", "language": "python", "size": 647}
{"docstring": "\"\"\"\nFrom http://stackoverflow.com/a/12377059/1079110\n\"\"\"\n", "func_signal": "def listify(fn=None, wrapper=list):\n", "code": "def listify_return(fn):\n    @functools.wraps(fn)\n    def listify_helper(*args, **kw):\n        return wrapper(fn(*args, **kw))\n    return listify_helper\n\nif fn is None:\n    return listify_return\nreturn listify_return(fn)", "path": "layered\\utility.py", "repo_name": "danijar/layered", "stars": 237, "license": "mit", "language": "python", "size": 647}
{"docstring": "# pylint: disable=too-many-arguments, redefined-variable-type\n", "func_signal": "def __init__(self, title, xlabel, ylabel, style=None, fixed=None):\n", "code": "super().__init__(title, xlabel, ylabel, style or {})\nself.max_ = 0\nif not fixed:\n    self.xdata = []\n    self.ydata = []\nelse:\n    self.xdata = list(range(fixed))\n    self.ydata = collections.deque([None] * fixed, maxlen=fixed)\n    self.width = fixed", "path": "layered\\plot.py", "repo_name": "danijar/layered", "stars": 237, "license": "mit", "language": "python", "size": 647}
{"docstring": "# The empty dictionary causes defaults to be loaded even if the\n# definition is None.\n", "func_signal": "def _load_definition(self, definition):\n", "code": "if not definition:\n    definition = {}\nfor name, default in self._defaults().items():\n    type_ = type(default)\n    self.__dict__[name] = type_(definition.pop(name, default))", "path": "layered\\problem.py", "repo_name": "danijar/layered", "stars": 237, "license": "mit", "language": "python", "size": 647}
{"docstring": "# pylint: disable=arguments-differ\n", "func_signal": "def parse(self, train_x, train_y, test_x, test_y):\n", "code": "training = list(self.read(train_x, train_y))\ntesting = list(self.read(test_x, test_y))\nreturn training, testing", "path": "layered\\dataset.py", "repo_name": "danijar/layered", "stars": 237, "license": "mit", "language": "python", "size": 647}
{"docstring": "# pylint: disable=attribute-defined-outside-init\n", "func_signal": "def _load_weight_tying(self):\n", "code": "self.weight_tying = [[y.split(',') for y in x]\n                     for x in self.weight_tying]\nfor i, group in enumerate(self.weight_tying):\n    for j, slices in enumerate(group):\n        for k, slice_ in enumerate(slices):\n            slice_ = [int(s) if s else None for s in slice_.split(':')]\n            self.weight_tying[i][j][k] = slice(*slice_)\nfor i, group in enumerate(self.weight_tying):\n    for j, slices in enumerate(group):\n        assert not slices[0].start and not slices[0].step, (\n            'Ranges are not allowed in the first dimension.')\n        self.weight_tying[i][j][0] = slices[0].stop", "path": "layered\\problem.py", "repo_name": "danijar/layered", "stars": 237, "license": "mit", "language": "python", "size": 647}
{"docstring": "\"\"\"\nUtility function that can be used within the parse() implementation of\nsub classes to split a list of example into two lists for training and\ntesting.\n\"\"\"\n", "func_signal": "def split(examples, ratio=0.8):\n", "code": "split = int(ratio * len(examples))\nreturn examples[:split], examples[split:]", "path": "layered\\dataset.py", "repo_name": "danijar/layered", "stars": 237, "license": "mit", "language": "python", "size": 647}
{"docstring": "\"\"\"\nHand the main thread to the window and continue work in the provided\nfunction. A state is passed as the first argument that contains a\n`running` flag. The function is expected to exit if the flag becomes\nfalse. The flag can also be set to false to stop the window event loop\nand continue in the main thread after the `start()` call.\n\"\"\"\n", "func_signal": "def start(self, work):\n", "code": "assert threading.current_thread() == threading.main_thread()\nassert not self.state.running\nself.state.running = True\nself.thread = threading.Thread(target=work, args=(self.state,))\nself.thread.start()\nwhile self.state.running:\n    try:\n        before = time.time()\n        self.update()\n        duration = time.time() - before\n        plt.pause(max(0.001, self.refresh - duration))\n    except KeyboardInterrupt:\n        self.state.running = False\n        self.thread.join()\n        return", "path": "layered\\plot.py", "repo_name": "danijar/layered", "stars": 237, "license": "mit", "language": "python", "size": 647}
{"docstring": "# The gradient at a layer is computed as the derivative of both the\n# local activation and the weighted sum of the derivatives in the\n# deeper layer.\n", "func_signal": "def _delta_layer(self, layer, weight, above):\n", "code": "backward = self.network.backward(weight, above)\ndelta = layer.delta(backward)\nassert len(layer) == len(backward) == len(delta)\nreturn delta", "path": "layered\\gradient.py", "repo_name": "danijar/layered", "stars": 237, "license": "mit", "language": "python", "size": 647}
{"docstring": "# Add bias input of one.\n", "func_signal": "def forward(weight, activations):\n", "code": "activations = np.insert(activations, 0, 1)\nassert activations[0] == 1\nright = activations.dot(weight)\nreturn right", "path": "layered\\network.py", "repo_name": "danijar/layered", "stars": 237, "license": "mit", "language": "python", "size": 647}
{"docstring": "# The gradient with respect to the weights is computed as the gradient\n# at the target neuron multiplied by the activation of the source\n# neuron.\n", "func_signal": "def _delta_weights(self, delta_layers):\n", "code": "gradient = Matrices(self.network.shapes)\nprev_and_delta = zip(self.network.layers[:-1], delta_layers)\nfor index, (previous, delta) in enumerate(prev_and_delta):\n    # We want to tweak the bias weights so we need them in the\n    # gradient.\n    activations = np.insert(previous.outgoing, 0, 1)\n    assert activations[0] == 1\n    gradient[index] = np.outer(activations, delta)\nreturn gradient", "path": "layered\\gradient.py", "repo_name": "danijar/layered", "stars": 237, "license": "mit", "language": "python", "size": 647}
{"docstring": "\"\"\"\nRedraw the figure to show changed data. This is automatically called\nafter `start()` was run.\n\"\"\"\n", "func_signal": "def update(self):\n", "code": "assert threading.current_thread() == threading.main_thread()\nfor axis, line, interface in self.interfaces:\n    line.set_xdata(interface.xdata)\n    line.set_ydata(interface.ydata)\n    axis.set_xlim(0, interface.width or 1, emit=False)\n    axis.set_ylim(0, interface.height or 1, emit=False)\nself.figure.canvas.draw()", "path": "layered\\plot.py", "repo_name": "danijar/layered", "stars": 237, "license": "mit", "language": "python", "size": 647}
{"docstring": "\"\"\"\nEvaluate the network with alternative weights on the input data and\nreturn the output activation.\n\"\"\"\n", "func_signal": "def feed(self, weights, data):\n", "code": "assert len(data) == self.layers[0].size\nself.layers[0].apply(data)\n# Propagate trough the remaining layers.\nconnections = zip(self.layers[:-1], weights, self.layers[1:])\nfor previous, weight, current in connections:\n    incoming = self.forward(weight, previous.outgoing)\n    current.apply(incoming)\n# Return the activations of the output layer.\nreturn self.layers[-1].outgoing", "path": "layered\\network.py", "repo_name": "danijar/layered", "stars": 237, "license": "mit", "language": "python", "size": 647}
{"docstring": "\"\"\"\nConstruct a problem. If content is specified, try to load it as a YAML\npath and otherwise treat it as an inline YAML string.\n\"\"\"\n", "func_signal": "def __init__(self, content=None):\n", "code": "if content and os.path.isfile(content):\n    with open(content) as file_:\n        self.parse(file_)\nelif content:\n    self.parse(content)\nself._validate()", "path": "layered\\problem.py", "repo_name": "danijar/layered", "stars": 237, "license": "mit", "language": "python", "size": 647}
{"docstring": "\"\"\"\nEnsures a ValueError exception is raised if one attempts to get a\ncontact from the k-bucket with an id that doesn't exist in the\nk-bucket.\n\"\"\"\n", "func_signal": "def test_get_contact_with_bad_id(self):\n", "code": "range_min = 12345\nrange_max = 98765\nbucket = Bucket(range_min, range_max)\ncontact = PeerNode(\"12345\", \"192.168.0.2\", 8888, 123)\nbucket.add_contact(contact)\nwith self.assertRaises(ValueError):\n    bucket.get_contact(\"54321\")", "path": "tests\\dht\\test_bucket.py", "repo_name": "ntoll/drogulus", "stars": 201, "license": "other", "language": "python", "size": 14225}
{"docstring": "\"\"\"\nOverride the string representation of the object to be something\nuseful.\n\"\"\"\n", "func_signal": "def __str__(self):\n", "code": "return str({\n    'network_id': self.network_id,\n    'public_key': self.public_key,\n    'version': self.version,\n    'uri': self.uri,\n    'last_seen': self.last_seen,\n    'failed_rpc': self.failed_RPCs\n})", "path": "drogulus\\dht\\contact.py", "repo_name": "ntoll/drogulus", "stars": 201, "license": "other", "language": "python", "size": 14225}
{"docstring": "\"\"\"\nIf the k-bucket is empty, the result of getContacts is an empty list.\n\"\"\"\n", "func_signal": "def test_get_contacts_empty(self):\n", "code": "range_min = 12345\nrange_max = 98765\nbucket = Bucket(range_min, range_max)\nresult = bucket.get_contacts()\nself.assertEqual(0, len(result))", "path": "tests\\dht\\test_bucket.py", "repo_name": "ntoll/drogulus", "stars": 201, "license": "other", "language": "python", "size": 14225}
{"docstring": "\"\"\"\nIf a contact is passed as the excludeContact argument then it won't be\nin the result list.\n\"\"\"\n", "func_signal": "def test_get_contacts_with_exclusion(self):\n", "code": "range_min = 12345\nrange_max = 98765\nbucket = Bucket(range_min, range_max)\nfor i in range(K):\n    contact = PeerNode(\"%d\" % i, \"192.168.0.%d\" % i, 9999, 123)\n    bucket.add_contact(contact)\nresult = bucket.get_contacts(count=20, exclude_contact=contact)\nself.assertEqual(19, len(result))\nself.assertFalse(contact in result)", "path": "tests\\dht\\test_bucket.py", "repo_name": "ntoll/drogulus", "stars": 201, "license": "other", "language": "python", "size": 14225}
{"docstring": "\"\"\"\nReturns the path to the OS appropriate user log directory for the\ndrogulus application.\n\nIf this directory does not exist it will be created.\n\"\"\"\n", "func_signal": "def log_dir():\n", "code": "uld = user_log_dir(APPNAME, APPAUTHOR, get_version())\nif not os.path.exists(uld):\n    os.makedirs(uld)\nreturn uld", "path": "drogulus\\commands\\utils.py", "repo_name": "ntoll/drogulus", "stars": 201, "license": "other", "language": "python", "size": 14225}
{"docstring": "\"\"\"\nWill return a string representation of both the private and public\nRSA keys found in the locations specified by priv_file and pub_file args.\nSince the private key is password protected the passphrase argument is\nused to decrypt it. If no file paths are given then sane default\nlocation and names are used.\n\"\"\"\n", "func_signal": "def get_keys(passphrase, priv_file=None, pub_file=None):\n", "code": "if not pub_file:\n    pub_file = os.path.join(data_dir(), '{}.pub'.format(APPNAME))\nif not priv_file:\n    priv_file = os.path.join(data_dir(), '{}.scrypt'.format(APPNAME))\npub = open(pub_file, 'rb').read()\ntry:\n    with pyscrypt.ScryptFile(priv_file, passphrase.encode('utf-8')) as f:\n        priv = f.read()\nexcept InvalidScryptFileFormat:\n    # Make the exception a bit more human.\n    msg = 'Unable to read private key file. Check your passphrase!'\n    raise ValueError(msg)\nreturn (priv, pub)", "path": "drogulus\\commands\\utils.py", "repo_name": "ntoll/drogulus", "stars": 201, "license": "other", "language": "python", "size": 14225}
{"docstring": "\"\"\"\nEnsures a key just below the k-bucket's range is identified as out of\nrange.\n\"\"\"\n", "func_signal": "def test_key_in_range_no_too_low(self):\n", "code": "bucket = Bucket(5, 9)\nself.assertFalse(bucket.key_in_range('2'))", "path": "tests\\dht\\test_bucket.py", "repo_name": "ntoll/drogulus", "stars": 201, "license": "other", "language": "python", "size": 14225}
{"docstring": "\"\"\"\nGiven private and public keys as bytes, a passphrase and paths to private\nand public output files will save the keys in the appropriate file path\nlocation. In the case of the private key, will use the scrypt module (see:\nhttps://en.wikipedia.org/wiki/Scrypt) and the passphrase to encrypt it.\n\"\"\"\n", "func_signal": "def save_keys(private_key, public_key, passphrase, priv_file, pub_file):\n", "code": "with open(pub_file, 'wb') as fpub:\n    fpub.write(public_key)\n# PyScrypt has problems using the 'with' keyword and saving content.\nfp = open(priv_file, 'wb')\ntry:\n    fpriv = pyscrypt.ScryptFile(fp, passphrase.encode('utf-8'), N=1024,\n                                r=1, p=1)\n    fpriv.write(private_key)\nfinally:\n    fpriv.close()", "path": "drogulus\\commands\\utils.py", "repo_name": "ntoll/drogulus", "stars": 201, "license": "other", "language": "python", "size": 14225}
{"docstring": "\"\"\"\nOverride equals to work with a string representation of the contact's\nid.\n\"\"\"\n", "func_signal": "def __eq__(self, other):\n", "code": "if isinstance(other, PeerNode):\n    return self.network_id == other.network_id\nelif isinstance(other, str):\n    return self.network_id == other\nelse:\n    return False", "path": "drogulus\\dht\\contact.py", "repo_name": "ntoll/drogulus", "stars": 201, "license": "other", "language": "python", "size": 14225}
{"docstring": "\"\"\"\nIf the \"count\" argument is bigger than the number of contacts in the\nbucket then all the contacts are returned.\n\"\"\"\n", "func_signal": "def test_get_contacts_count_too_big(self):\n", "code": "range_min = 12345\nrange_max = 98765\nbucket = Bucket(range_min, range_max)\nfor i in range(10):\n    contact = PeerNode(\"%d\" % i, \"192.168.0.%d\" % i, 9999, 123)\n    bucket.add_contact(contact)\nresult = bucket.get_contacts(count=20)\nself.assertEqual(10, len(result))", "path": "tests\\dht\\test_bucket.py", "repo_name": "ntoll/drogulus", "stars": 201, "license": "other", "language": "python", "size": 14225}
{"docstring": "\"\"\"\nEnsures that a key within the appropriate range is identified as such.\n\"\"\"\n", "func_signal": "def test_key_in_range_yes(self):\n", "code": "bucket = Bucket(1, 9)\nself.assertTrue(bucket.key_in_range('2'))", "path": "tests\\dht\\test_bucket.py", "repo_name": "ntoll/drogulus", "stars": 201, "license": "other", "language": "python", "size": 14225}
{"docstring": "\"\"\"\nEnsures it is possible to get a contact from the k-bucket with a valid\nid.\n\"\"\"\n", "func_signal": "def test_get_contact(self):\n", "code": "range_min = 12345\nrange_max = 98765\nbucket = Bucket(range_min, range_max)\nfor i in range(K):\n    contact = PeerNode(PUBLIC_KEY, \"192.168.0.%d\" % i, 9999, 123)\n    contact.network_id = hex(i)\n    bucket.add_contact(contact)\nfor i in range(K):\n    self.assertTrue(bucket.get_contact(hex(i)),\n                    \"Could not get contact with id %d\" % i)", "path": "tests\\dht\\test_bucket.py", "repo_name": "ntoll/drogulus", "stars": 201, "license": "other", "language": "python", "size": 14225}
{"docstring": "\"\"\"\nReturns a tuple containing information about this contact.\n\"\"\"\n", "func_signal": "def __repr__(self):\n", "code": "return str((self.network_id, self.public_key, self.version, self.uri,\n            self.last_seen, self.failed_RPCs))", "path": "drogulus\\dht\\contact.py", "repo_name": "ntoll/drogulus", "stars": 201, "license": "other", "language": "python", "size": 14225}
{"docstring": "\"\"\"\nEnsures that if one attempts to add a contact to a bucket whose size is\ngreater than the constant K, then the BucketFull exception is raised.\n\"\"\"\n", "func_signal": "def test_add_contact_to_full_bucket(self):\n", "code": "range_min = 12345\nrange_max = 98765\nbucket = Bucket(range_min, range_max)\nfor i in range(K):\n    contact = PeerNode(\"%d\" % i, \"192.168.0.%d\" % i, 9999, 123)\n    bucket.add_contact(contact)\nwith self.assertRaises(BucketFull):\n    contact_too_many = PeerNode(\"12345\", \"192.168.0.2\", 8888, 123)\n    bucket.add_contact(contact_too_many)", "path": "tests\\dht\\test_bucket.py", "repo_name": "ntoll/drogulus", "stars": 201, "license": "other", "language": "python", "size": 14225}
{"docstring": "\"\"\"\nAttempts to get the user's whoami information.\n\"\"\"\n", "func_signal": "def get_whoami(input_file=None):\n", "code": "if not input_file:\n    input_file = os.path.join(data_dir(), 'whoami.json')\nreturn json.load(open(input_file, 'r'))", "path": "drogulus\\commands\\utils.py", "repo_name": "ntoll/drogulus", "stars": 201, "license": "other", "language": "python", "size": 14225}
{"docstring": "\"\"\"\nEnsures a key just above the k-bucket's range is identified as out of\nrange.\n\"\"\"\n", "func_signal": "def test_key_in_range_no_too_high(self):\n", "code": "bucket = Bucket(1, 5)\nself.assertFalse(bucket.key_in_range('7'))", "path": "tests\\dht\\test_bucket.py", "repo_name": "ntoll/drogulus", "stars": 201, "license": "other", "language": "python", "size": 14225}
{"docstring": "\"\"\"\nEnsures get_contacts works as expected.\n\"\"\"\n", "func_signal": "def test_get_contacts_all(self):\n", "code": "range_min = 12345\nrange_max = 98765\nbucket = Bucket(range_min, range_max)\nfor i in range(K):\n    contact = PeerNode(\"%d\" % i, \"192.168.0.%d\" % i, 9999, 123)\n    bucket.add_contact(contact)\nresult = bucket.get_contacts()\nself.assertEqual(20, len(result))", "path": "tests\\dht\\test_bucket.py", "repo_name": "ntoll/drogulus", "stars": 201, "license": "other", "language": "python", "size": 14225}
{"docstring": "\"\"\"\nStarts a local instance of the drogulus given the parsed arguments to\nuse. The command falls back to sane defaults if none are given.\n\"\"\"\n", "func_signal": "def take_action(self, parsed_args):\n", "code": "passphrase = parsed_args.passphrase\nif not passphrase:\n    print('You must supply a passphrase.')\n    passphrase = getpass().strip()\n    if not passphrase:\n        raise ValueError('You must supply a passphrase.')\nport = parsed_args.port\nwhoami = parsed_args.whoami\nkey_dir = parsed_args.keys\npeer_file = parsed_args.peers\n\n# Setup logging\nlogfile = os.path.join(log_dir(), 'drogulus.log')\nhandler = logging.handlers.TimedRotatingFileHandler(logfile,\n                                                    when='midnight',\n                                                    interval=1)\nf = ' '.join(['%(asctime)s', '%(processName)-10s', '%(name)s',\n             '%(levelname)-8s', '%(message)s'])\nformatter = logging.Formatter(f)\nhandler.setFormatter(formatter)\nroot = logging.getLogger()\nroot.addHandler(handler)\nlog = logging.getLogger(__name__)\nprint('Logging to {}'.format(logfile))\n\n# RSA key config.\npriv_path = None\npub_path = None\nif key_dir:\n    priv_path = os.path.join(key_dir, '{}.scrypt'.format(APPNAME))\n    pub_path = os.path.join(key_dir, '{}.pub'.format(APPNAME))\ntry:\n    private_key, public_key = get_keys(passphrase, priv_path,\n                                       pub_path)\nexcept Exception as ex:\n    log.error('Unable to get keys from {}'.format(key_dir))\n    log.error(ex)\n    raise ex\n\n# Whoami\ntry:\n    whoami = get_whoami(whoami)\nexcept:\n    log.error('Unable to get whoami file.')\n    whoami = None\n\n# Asyncio boilerplate.\nevent_loop = asyncio.get_event_loop()\nconnector = HttpConnector(event_loop)  # NetstringConnector(event_loop)\ninstance = Drogulus(private_key, public_key, event_loop, connector,\n                    port, whoami)\napp = make_http_handler(event_loop, connector, instance._node)\napp_task = event_loop.create_server(app, '0.0.0.0', port)\nserver = event_loop.run_until_complete(app_task)\n\n# Join the network\nif peer_file:\n    peer_details = json.load(open(peer_file))\n    instance.join(peer_details)\n\n# Run the server\ntry:\n    event_loop.run_forever()\nexcept KeyboardInterrupt:\n    log.info('Manual exit')\nfinally:\n    # dump peers\n    if not peer_file:\n        peer_file = os.path.join(data_dir(), 'peers.json')\n    with open(peer_file, 'w') as output:\n        json.dump(instance._node.routing_table.dump(), output,\n                  indent=2)\n        log.info('Dumped peers')\n    log.info('STOPPED')\n    server.close()\n    event_loop.close()", "path": "drogulus\\commands\\start.py", "repo_name": "ntoll/drogulus", "stars": 201, "license": "other", "language": "python", "size": 14225}
{"docstring": "\"\"\"\nGiven a public_key as a string will return a canonical network_id.\n\nThe network id is created as the hexdigest of the SHA512 of the public\nkey.\n\nWill raise a ValueError if the incoming public_key is empty.\n\"\"\"\n", "func_signal": "def make_network_id(public_key):\n", "code": "if public_key:\n    return sha512(public_key.encode('ascii')).hexdigest()\nelse:\n    raise ValueError('Cannot create network_id from empty public key.')", "path": "drogulus\\dht\\contact.py", "repo_name": "ntoll/drogulus", "stars": 201, "license": "other", "language": "python", "size": 14225}
{"docstring": "\"\"\"\nInitialise the peer node with a unique id within the network (derived\nfrom its public key), the drogulus version the contact is running, a\nURI that identifies where to contact the peer node and a timestamp\nindicating when the last connection was made with the contact\n(defaults to 0).\n\"\"\"\n", "func_signal": "def __init__(self, public_key, version, uri, last_seen=0.0):\n", "code": "self.network_id = make_network_id(public_key)\nself.public_key = public_key\nself.version = version\nself.uri = uri\nself.last_seen = last_seen\n# failed_RPCs keeps track of the number of failed RPCs to this peer.\n# If this number reaches a threshold then it is evicted from a\n# bucket and replaced with another node that is more reliable.\nself.failed_RPCs = 0", "path": "drogulus\\dht\\contact.py", "repo_name": "ntoll/drogulus", "stars": 201, "license": "other", "language": "python", "size": 14225}
{"docstring": "\"\"\"\nReturns the processed view's group and index\n\n@param key: a key to get value\n\"\"\"\n", "func_signal": "def get_view_index(self, key):\n", "code": "group_settings = self.get(key)\nview_group = -1\nview_index = -1\nwindow = sublime.active_window()\nview = window.active_view()\ncurrent_group, current_index = window.get_view_index(view)\nfor group_setting in group_settings:\n    if isinstance(group_setting, int):\n        view_group = group_setting\n        view_index = len(window.views_in_group(view_group))\n    elif (isinstance(group_setting, list) or\n            isinstance(group_setting, tuple)) and group_setting:\n        view_group = group_setting[0]\n        view_index = len(window.views_in_group(view_group))\n        if len(group_setting) > 1:\n            view_index = group_setting[1]\n    if view_group >= window.num_groups() or view_group < 0:\n        view_index = -1\n        continue\n    elif (view_index > len(window.views_in_group(view_group)) or\n            view_index < 0):\n        continue\n    break\n\nif view_group >= window.num_groups() or view_group < 0:\n    view_group = current_group\n    view_index = len(window.views_in_group(view_group))\nelif (view_index >= len(window.views_in_group(view_group)) or\n        view_index < 0):\n    view_index = len(window.views_in_group(view_group))\nreturn (view_group, view_index)", "path": "core\\settings.py", "repo_name": "spywhere/Javatar", "stars": 171, "license": "other", "language": "python", "size": 908}
{"docstring": "\"\"\"\nReset all changes (used on restart)\n\"\"\"\n", "func_signal": "def reset(self):\n", "code": "self.settings = None\nself.sublime_settings = None\nself.settings_base = \"Javatar.sublime-settings\"\nself.sublime_base = \"Preferences.sublime-settings\"", "path": "core\\settings.py", "repo_name": "spywhere/Javatar", "stars": 171, "license": "other", "language": "python", "size": 908}
{"docstring": "\"\"\"\nConverts a number into a readable file size\n\n@param filesize: a value to convert, if provided as a number\n    if provided as a string, will thread as a file and\n    will gather a file size for calculation\n@param base: a base value for calculation\n    Decimal use 1000\n    Binary use 1024\n\"\"\"\n", "func_signal": "def to_readable_size(filesize, base=1000):\n", "code": "if isinstance(filesize, str):\n    if filesize[:9] == \"Packages/\":\n        filesize = os.path.join(sublime.packages_path(), filesize[9:])\n    return Utils.to_readable_size(os.path.getsize(filesize), base)\nscales = [\n    [base ** 5, \"PB\"],\n    [base ** 4, \"TB\"],\n    [base ** 3, \"GB\"],\n    [base ** 2, \"MB\"],\n    [base ** 1, \"kB\"],\n    [base ** 0, \"B\"]\n]\nfor scale in scales:\n    if filesize >= scale[0]:\n        break\nreturn \"%.2f%s\" % (filesize / scale[0], scale[1])", "path": "utils\\utils.py", "repo_name": "spywhere/Javatar", "stars": 171, "license": "other", "language": "python", "size": 908}
{"docstring": "\"\"\"\nReturns whether specified path is a source folder or not\n\n@param path: a directory path\n@param can_empty: a boolean specified whether the empty folder will\n    consider as a source folder\n\"\"\"\n", "func_signal": "def is_source_folder(self, path, can_empty=True):\n", "code": "empty = True\nfor name in os.listdir(path):\n    empty = False\n    path_name = os.path.join(path, name)\n    if can_empty:\n        if os.path.isdir(path_name):\n            if self.is_source_folder(path_name, can_empty):\n                return True\n    if os.path.isfile(path_name) and self.is_java(path_name):\n        return True\nreturn can_empty and empty", "path": "core\\state_property.py", "repo_name": "spywhere/Javatar", "stars": 171, "license": "other", "language": "python", "size": 908}
{"docstring": "\"\"\"\nReturns a source folder contains a specified file\n\nSource folder will be used to create a new Java file and another tasks\n    this should be adapt with current state of the project\n\n@param file_path: a file path\n\"\"\"\n", "func_signal": "def get_source_folder(self, file_path=None):\n", "code": "file_path = file_path or self.get_file()\nfrom ..utils import Utils\nif self.is_project():\n    source_folders = self.get_source_folders(file_path=file_path)\n    if source_folders:\n        if not file_path:\n            return source_folders[0]\n        for source_folder in source_folders:\n            if Utils.contains_file(source_folder, file_path):\n                return source_folder\nif self.get_dir(file_path=file_path):\n    return self.get_dir(file_path=file_path)\nreturn None", "path": "core\\state_property.py", "repo_name": "spywhere/Javatar", "stars": 171, "license": "other", "language": "python", "size": 908}
{"docstring": "\"\"\"\nReturns a value in project settings file\n\n@param key: a key to get value\n@param default: a return value if specified key is not exists\n@param as_tuple: a boolean specified whether returns as a tuple contains\n    value and a boolean specified if value gather from project\n    settings or not\n\"\"\"\n", "func_signal": "def get_local(self, key, default=None, as_tuple=False):\n", "code": "if as_tuple:\n    return (self.get_local(key, default, as_tuple=False), False)\nelse:\n    project_data = sublime.active_window().project_data()\n    if (project_data and \"javatar\" in project_data and\n            key in project_data[\"javatar\"]):\n        return project_data[\"javatar\"][key]\nreturn default", "path": "core\\settings.py", "repo_name": "spywhere/Javatar", "stars": 171, "license": "other", "language": "python", "size": 908}
{"docstring": "\"\"\"\nReturns a file within specified view\n\n@param view: a target view used as a target view,\n    otherwise, active view will be used\n\"\"\"\n", "func_signal": "def get_file(self, view=None):\n", "code": "view = view or sublime.active_window().active_view()\nif view:\n    return view.file_name()\nreturn None", "path": "core\\state_property.py", "repo_name": "spywhere/Javatar", "stars": 171, "license": "other", "language": "python", "size": 908}
{"docstring": "\"\"\"\nSet a value to specified key in settings\n\n@param key: a key to set value\n@param val: a value to be set, if provided as not None\n    otherwise key will be deleted instead\n@param to_global: a boolean specified whether set the value to\n    default settings or not\n@param fallback: a boolean specified whether set the value to\n    default settings on failed to save to local or not\n\"\"\"\n", "func_signal": "def set(self, key, val, to_global=False, fallback=True):\n", "code": "if to_global:\n    if val is None:\n        if self.settings.has(key):\n            self.settings.erase(key)\n        else:\n            # Return here so it won't wasting time saving old settings\n            return\n    else:\n        self.settings.set(key, val)\n    sublime.save_settings(self.settings_base)\nelse:\n    window = sublime.active_window()\n    project_data = window.project_data()\n    if not project_data:\n        if fallback:\n            self.set(key, val, to_global=True)\n        return\n    if val is None:\n        if (project_data and \"javatar\" in project_data and\n                key in project_data[\"javatar\"]):\n            del project_data[\"javatar\"][key]\n        else:\n            # Return here so it won't wasting time saving old settings\n            return\n    else:\n        if project_data and\"javatar\" in project_data:\n            data = project_data[\"javatar\"]\n        else:\n            data = {}\n        data[key] = val\n        project_data[\"javatar\"] = data\n    window.set_project_data(project_data)", "path": "core\\settings.py", "repo_name": "spywhere/Javatar", "stars": 171, "license": "other", "language": "python", "size": 908}
{"docstring": "\"\"\"\nChecks if a directory contains specified file\n\n@param directory: a directory path to be checked\n@param file_path: a file path\n\"\"\"\n", "func_signal": "def contains_file(directory, file_path):\n", "code": "return os.path.normcase(os.path.normpath(file_path)).startswith(\n    os.path.normcase(os.path.normpath(directory))\n)", "path": "utils\\utils.py", "repo_name": "spywhere/Javatar", "stars": 171, "license": "other", "language": "python", "size": 908}
{"docstring": "\"\"\"\nReturns whether specified window is a project or not\n\n@param window: a target window\n    if provided, will use as a target window\n\n    otherwise, active window will be used\n\"\"\"\n", "func_signal": "def is_project(self, window=None):\n", "code": "window = window or sublime.active_window()\nif window:\n    return len(window.folders()) > 0\nreturn False", "path": "core\\state_property.py", "repo_name": "spywhere/Javatar", "stars": 171, "license": "other", "language": "python", "size": 908}
{"docstring": "\"\"\"\nRead all settings from files\n\"\"\"\n", "func_signal": "def startup(self):\n", "code": "self.settings = sublime.load_settings(self.settings_base)\nself.sublime_settings = sublime.load_settings(self.sublime_base)", "path": "core\\settings.py", "repo_name": "spywhere/Javatar", "stars": 171, "license": "other", "language": "python", "size": 908}
{"docstring": "\"\"\"\nSplits a path into path components\n\n@param path: path to split\n\"\"\"\n", "func_signal": "def split_path(path):\n", "code": "rest, tail = os.path.split(path)\nif len(rest) == 0:\n    return (tail,)\nreturn Utils.split_path(rest) + (tail,)", "path": "utils\\utils.py", "repo_name": "spywhere/Javatar", "stars": 171, "license": "other", "language": "python", "size": 908}
{"docstring": "\"\"\"\nReturns a value in settings\n\n@param key: a key to get value\n@param default: a return value if specified key is not exists\n@param from_global: a boolean specified whether the settings is read\n    from default settings or not\n@param as_tuple: a boolean specified whether returns as a tuple contains\n    value and a boolean specified if value gather from project\n    settings or not\n\nThis method must return in local-default prioritize order\n\"\"\"\n", "func_signal": "def get(self, key, default=None, from_global=None, as_tuple=False):\n", "code": "if from_global is None:\n    value = self.get(\n        key,\n        default=None,\n        from_global=False,\n        as_tuple=as_tuple\n    )\n    if (isinstance(value, tuple) and value[0] is None) or value is None:\n        value = self.get(\n            key,\n            default=default,\n            from_global=True,\n            as_tuple=as_tuple\n        )\n    return value\nelif from_global:\n    return self.get_global(key, default, as_tuple)\nelse:\n    return self.get_local(key, default, as_tuple)", "path": "core\\settings.py", "repo_name": "spywhere/Javatar", "stars": 171, "license": "other", "language": "python", "size": 908}
{"docstring": "\"\"\"\nRefresh the source folders menu\n\"\"\"\n", "func_signal": "def refresh_source_folders(self):\n", "code": "source_folder_menu = {\n    \"selected_index\": 2,\n    \"items\": [\n        [\n            \"Back\",\n            \"Back to previous menu\"\n        ], [\n            \"Add Source Folder\",\n            \"Add a source folder to specified as default package\"\n        ]\n    ],\n    \"actions\": [\n        {\n            \"name\": \"project_settings\"\n        }, {\n            \"command\": \"javatar_project_settings\",\n            \"args\": {\n                \"action_type\": \"add_source_folder\"\n            }\n        }\n    ]\n}\n\nsource_folders, from_settings = self.get_source_folders(\n    as_tuple=True, include_missing=True\n)\nfor source_folder in source_folders:\n    name = os.path.basename(source_folder)\n    source_folder_menu[\"actions\"].append({\n        \"command\": \"javatar_project_settings\",\n        \"args\": {\n            \"action_type\": \"remove_source_folder\",\n            \"source_folder\": source_folder\n        }\n    })\n    source_folder_menu[\"items\"].append([\n        (\n            \"[Missing] \" if not os.path.exists(source_folder) else \"\"\n        ) + name,\n        (\n            \"Select to remove from the list\"\n            if from_settings\n            else \"Default source folder.\" +\n            \" Add a new one to override this folder\"\n        )\n    ])\nsublime.active_window().run_command(\"javatar\", {\"replaceMenu\": {\n    \"name\": \"local_source_folders\",\n    \"menu\": source_folder_menu\n}})", "path": "core\\state_property.py", "repo_name": "spywhere/Javatar", "stars": 171, "license": "other", "language": "python", "size": 908}
{"docstring": "\"\"\"\nReturns whether specified file or view is a Java file or not\n\n@param file_path: a target file path\n    if provided, will use as a target file path\n\n    otherwise, a file from the active view will be used\n@param view: a target view\n    if provided, will use as a target view\n\n    otherwise, active view will be used\n\"\"\"\n", "func_signal": "def is_java(self, file_path=None, view=None):\n", "code": "if not file_path and not view:\n    view = sublime.active_window().active_view()\n    if view.file_name():\n        return (\n            self.is_file(view) and\n            self.is_java(file_path=view.file_name())\n        )\nelif file_path:\n    _, ext = os.path.splitext(os.path.basename(file_path))\n    return ext in Settings().get(\"java_extensions\")\nreturn (\n    view and\n    view.find_by_selector(Settings().get(\"java_source_selector\"))\n)", "path": "core\\state_property.py", "repo_name": "spywhere/Javatar", "stars": 171, "license": "other", "language": "python", "size": 908}
{"docstring": "\"\"\"\nReturns a list of folders opened in the project,\n    otherwise, return a list of a current directory\n\n@param window: a target window used as a target window,\n    otherwise, active window will be used\n@param file_path: a file path\n\"\"\"\n", "func_signal": "def get_project_dirs(self, window=None, file_path=None):\n", "code": "window = window or sublime.active_window()\nif window:\n    return window.folders()\nreturn [self.get_dir(file_path=file_path)]", "path": "core\\state_property.py", "repo_name": "spywhere/Javatar", "stars": 171, "license": "other", "language": "python", "size": 908}
{"docstring": "\"\"\"\nReturns library path list\n\n@param from_global: a boolean specified whether returns a library path\n    list from global settings or local settings\n@param include_missing: a boolean specified whether returns a list\n    with missing library paths or not\n\"\"\"\n", "func_signal": "def get_library_paths(self, from_global=False, include_missing=False):\n", "code": "out_library_paths = []\nlibrary_paths = Settings().get(\n    \"library_paths\", from_global=from_global\n)\n\nif library_paths is not None:\n    out_library_paths.extend(\n        [library_path, from_global]\n        for library_path in library_paths\n        if os.path.exists(library_path) or include_missing\n    )\n\nif not from_global:\n    out_library_paths.extend(\n        [library_path, True]\n        for library_path in Settings().get(\n            \"library_paths\", default=[], from_global=True\n        )\n        if os.path.exists(library_path) or include_missing\n    )\n\nreturn out_library_paths", "path": "core\\state_property.py", "repo_name": "spywhere/Javatar", "stars": 171, "license": "other", "language": "python", "size": 908}
{"docstring": "\"\"\"\nUpgrade the outdated settings\n\n@param keys: a list of key to upgrade\n\"\"\"\n", "func_signal": "def upgrade_settings(self, keys):\n", "code": "upgradable_keys = {\n    \"project_dir\": \"%root_dir%\",\n    \"source_folder\": \"%source_folder%\",\n    \"packages_path\": \"%packages_path%\",\n    \"sep\": \"%sep%\",\n    \"$\": \"$\"\n}\nfor key in keys:\n    value, from_global = self.get(key, as_tuple=True)\n    value = value.replace(\"%\", \"%%%\")\n    for k in upgradable_keys:\n        value = value.replace(\"$\" + k, upgradable_keys[k])\n    self.set(key, value, to_global=from_global)", "path": "core\\settings.py", "repo_name": "spywhere/Javatar", "stars": 171, "license": "other", "language": "python", "size": 908}
{"docstring": "\"\"\"\nReturns a proper root folder in the project\n\nRoot folder will be used to run a project and use for another tasks\n    this should be adapt with current state of the project\n\n@param file_path: a file path\n@param view: a target view used as a target view,\n    otherwise, active view will be used\n\"\"\"\n", "func_signal": "def get_root_dir(self, file_path=None, view=None):\n", "code": "if self.is_project():\n    project_folders = self.get_project_dirs(file_path=file_path)\n    if project_folders:\n        return project_folders[0]\nif self.get_dir():\n    return self.get_dir(file_path=file_path, view=view)\nreturn None", "path": "core\\state_property.py", "repo_name": "spywhere/Javatar", "stars": 171, "license": "other", "language": "python", "size": 908}
{"docstring": "\"\"\"\nReturns a directory contains a file in a specified view if not a\n    specified file path\n\n@param file_path: a file path\n@param view: a target view used as a target view,\n    otherwise, active view will be used\n\"\"\"\n", "func_signal": "def get_dir(self, file_path=None, view=None):\n", "code": "file_path = file_path or self.get_file(view)\nif file_path:\n    return os.path.dirname(file_path)\nreturn None", "path": "core\\state_property.py", "repo_name": "spywhere/Javatar", "stars": 171, "license": "other", "language": "python", "size": 908}
{"docstring": "\"\"\"Call before every test case.\"\"\"\n", "func_signal": "def setUp(self):\n", "code": "self.l2btools = Learn2BanTools()\nself.l2btools.load_train2ban_config()\n\nself.l2btools.retrieve_experiments()\nself.log_files = [[self.TEST_LOG_ID, self.TEST_LOG_FILENAME]]\n\n#we are testing trainin\nself.test_trainer = Train2Ban(self.l2btools.construct_svm_classifier())\nself.test_trainer._training_set = TrainingSet() #clean the training set\nself.test_trainer.add_to_sample(self.l2btools.gather_all_features([self.TEST_LOG_FILENAME]))", "path": "src\\test\\test_trainer.py", "repo_name": "equalitie/learn2ban", "stars": 134, "license": "None", "language": "python", "size": 95}
{"docstring": "'''\n   Save the data to a CSV file in the format required by mRMR.\n   - first row is the name of the features.\n   - first col is the class names.\n   - data is organized, with a sample per row.\n   Returns the filename (a temporary file with the .csv extension).\n   '''\n\n", "func_signal": "def _savemrmrdatafile(data, featNames, classNames):\n", "code": "f = tempfile.NamedTemporaryFile(suffix='.csv', prefix='tmp_mrmr_', delete=False, mode='w')\nf.write(','.join(['class']+featNames)+os.linesep)\ndata = np.asarray(data)\nfor i in range(data.shape[0]):\n    f.write(','.join([str(classNames[i])]+[str(d) for d in data[i, :]])+os.linesep)\nf.close()\nreturn f.name", "path": "src\\analysis\\mRMR.py", "repo_name": "equalitie/learn2ban", "stars": 134, "license": "None", "language": "python", "size": 95}
{"docstring": "\"\"\"\nconvert the time value to total no of seconds passed\nsince ???? to facilitate computation.\n\"\"\"\n#find to ignore time-zone\n", "func_signal": "def time_to_second(self):\n", "code": "digested_time = strptime(self.time[:self.time.find(' ')], self.ATS_TIME_FORMAT)\nreturn mktime(digested_time)", "path": "src\\ip_sieve_shlex.py", "repo_name": "equalitie/learn2ban", "stars": 134, "license": "None", "language": "python", "size": 95}
{"docstring": "\"\"\"\nIt takes an array of log lines\n\"\"\"\n", "func_signal": "def set_log_lines(self, log_lines):\n", "code": "self.dict_invalid = True\nself._log_lines = log_lines", "path": "src\\ip_sieve_shlex.py", "repo_name": "equalitie/learn2ban", "stars": 134, "license": "None", "language": "python", "size": 95}
{"docstring": "\"\"\"\nSimply calls the parent constructor\n\"\"\"\n", "func_signal": "def __init__(self, ip_sieve, ip_feature_db):\n", "code": "Learn2BanFeature.__init__(self, ip_sieve, ip_feature_db)\n\n#Each feature need to have unique index as the field number\n#in ip_feature_db\nself._FEATURE_INDEX = 5", "path": "src\\features\\src\\feature_payload_size_average.py", "repo_name": "equalitie/learn2ban", "stars": 134, "license": "None", "language": "python", "size": 95}
{"docstring": "'''\n   A wrapper around the mrmr executable.\n\n   Arguments:\n   data: a 2D array (size NxF)\n   featNames: list of feature names (F elements)\n   classNames: list of class names (N elements)\n\n   Optional Arguments:\n   threshold: data must be discrete or discretized. The default value (None)\n           assumes that the data has already been discretized. Otherwise\n           it has to be discretized as below u-t*s, above u+t*s or between:\n           -1, +1 or 0, where u is the mean, s the standard deviation and\n           t the threshold. This is done feature by feature.\n   nFeats: the number of feature to select. If not given it defaults to all\n           features. This will only sort the features.\n   selectionMethod: either 'MID' or 'MIQ'. Default is 'MID'\n   mrmrexe: the path to the mrmr executable. Defaults to './mrmr'\n\n   Returns:\n   A dictionnary with 2 elements: MaxRel and mRMR, which are the 2 results\n   returned by mrmr (the 2 different feature selection criterions).\n   Each is a dictionnary, with fields Fea and Score, holding the feature\n   numbers and the scores respectively.\n\n   Example:\n   Generate some data: 200 samples, 2 classes, 7 features, the 2 first\n   features are correlated with the class label, the 5 others are\n   irrelevant. Feature names (fn) with a capital F are the relevant\n   features.\n   >>> N = 100\n   >>> data = np.r_[ np.random.randn(N,2)+2, np.random.randn(N,2)-2 ]\n   >>> data = np.c_[ data, np.random.randn(N*2,5) ]\n   >>> c = [1]*N+[-1]*N\n   >>> fn = ['F%d' % n for n in range(2)] + ['f%d' % n for n in range(5)]\n\n   Pass to the mRMR program\n   >>> mrmrout = mrmr(data, fn, c, threshold=0.5)\n\n   Get the result:\n   >>> R = mrmrout['mRMR']\n   >>> print 'Order \\t Fea \\t Name \\t Score'\n   >>> for i in range(len(R['Fea'])):\n   ...     print '%d \\t %d \\t %s \\t %f\\n' % \\\n   ...           (i, R['Fea'][i], fn[R['Fea'][i]], R['Score'][i])\n   ...\n\n   Order    Fea     Name    Score\n   0        1       F1      0.131000\n   1        0       F0      0.128000\n   2        4       f4      -0.008000\n   3        0       f0      -0.009000\n   4        3       f3      -0.010000\n   5        1       f1      -0.013000\n   6        2       f2      -0.015000\n   '''\n\n", "func_signal": "def mrmr(data, featNames, classNames, threshold=None, nFeats=None, selectionMethod='MID', mrmrexe='./mrmr'):\n", "code": "data = np.asarray(data)\nN, M = data.shape\n\nif nFeats is None:\n    nFeats = M\nelse:\n    assert nFeats <= M\n\nmrmrexe = os.path.abspath(mrmrexe)\n\nassert os.path.exists(mrmrexe) and os.access(mrmrexe, os.X_OK)\n\n# Save data to a temporary file that can be understood by the mrmr binary\nfn = _savemrmrdatafile(data, featNames, classNames)\n\n# Generate the command line. See the help of mrmr for info on options\ncmdstr = mrmrexe\ncmdstr += ' -i %s -n %d -s %d -v %d -m %s' % (fn, nFeats, N, M, selectionMethod)\nif threshold is not None:\n    assert threshold > 0\n    cmdstr += ' -t ' + str(threshold)\n\n# Call mrmr. The result is printed to stdout.\nmrmrout = Popen(cmdstr, stdout=PIPE, shell=True).stdout.read().split('\\n')\n\n# delete the temporary file\nos.remove(fn)\n\n# A function to parse the result\ndef extractRes(key):\n    Fea = []\n    Score = []\n    state = 0\n    for l in mrmrout:\n        if state == 0:\n            if l.find(key) != -1:\n                state = 1\n        elif state == 1:\n            state = 2\n        elif state == 2:\n            if l == '':\n                break\n            else:\n                n, f, fn, s = l.split(' \\t ')\n                Fea.append(int(f)-1)\n                Score.append(float(s))\n    return {'Fea': np.asarray(Fea), 'Score': np.asarray(Score)}\n\n# Return a dictionnary holding the features and their score for both the\n# MaxRel and mRMR criterions\nreturn {'MaxRel': extractRes('MaxRel features'),\n        'mRMR': extractRes('mRMR features')}", "path": "src\\analysis\\mRMR.py", "repo_name": "equalitie/learn2ban", "stars": 134, "license": "None", "language": "python", "size": 95}
{"docstring": "\"\"\"\nSimply calls the parent constructor\n\"\"\"\n", "func_signal": "def __init__(self, ip_sieve, ip_feature_db):\n", "code": "Learn2BanFeature.__init__(self, ip_sieve, ip_feature_db)\n\n#Each feature need to have unique index as the field number\n#in ip_feature_db\nself._FEATURE_INDEX = 2", "path": "src\\features\\src\\feature_cycling_user_agent.py", "repo_name": "equalitie/learn2ban", "stars": 134, "license": "None", "language": "python", "size": 95}
{"docstring": "\"\"\"\nIt takes the name of the log file and store it in a list\n\"\"\"\n", "func_signal": "def add_log_file(self, log_filename):\n", "code": "self._log_file_list.append(log_filename)\nself.dict_invalid = True", "path": "src\\ip_sieve.py", "repo_name": "equalitie/learn2ban", "stars": 134, "license": "None", "language": "python", "size": 95}
{"docstring": "\"\"\"\nretrieve the ip dictionary and compute the average for each \nip to determine the change rate of UA per IP.\n\"\"\"\n", "func_signal": "def compute(self):\n", "code": "ip_recs = self._ip_sieve.ordered_records()\n\nfor cur_ip_rec in ip_recs:\n    ua_request_map = {}\n    total_requests = 0\n    highest_percentage_UA = 0;\n    for payload in ip_recs[cur_ip_rec]:\n        cur_UA = payload.get_UA()\n        if cur_UA not in ua_request_map:                    \n            ua_request_map[cur_UA] = 1\n        else:\n            ua_request_map[cur_UA] += 1\n    \n    #Sort UAs by number of requests\n    sorted_ua_request_map = sorted(ua_request_map.iteritems(), key=operator.itemgetter(1), reverse=True)\n    #Percentage of times UA has changed over the course of the requests\n    feature_value = float(sorted_ua_request_map[0][1])/float(len(ip_recs[cur_ip_rec]))\n\n    self.append_feature(cur_ip_rec, feature_value)", "path": "src\\features\\src\\feature_cycling_user_agent.py", "repo_name": "equalitie/learn2ban", "stars": 134, "license": "None", "language": "python", "size": 95}
{"docstring": "\"\"\"\nWrapper for the record dictionary\n\"\"\"\n", "func_signal": "def ordered_records(self):\n", "code": "if (self.dict_invalid):\n    self.parse_log()\n\nreturn self._ordered_records", "path": "src\\ip_sieve.py", "repo_name": "equalitie/learn2ban", "stars": 134, "license": "None", "language": "python", "size": 95}
{"docstring": "\"\"\"\nIt takes a list of name of the log files and extend the filelist to it\n\"\"\"\n", "func_signal": "def add_log_files(self, log_filename_list):\n", "code": "self._log_file_list.extend(log_filename_list)\nself.dict_invalid = True", "path": "src\\ip_sieve.py", "repo_name": "equalitie/learn2ban", "stars": 134, "license": "None", "language": "python", "size": 95}
{"docstring": "\"\"\"\nSimply calls the parent constructor\n\"\"\"\n", "func_signal": "def __init__(self, ip_sieve, ip_feature_db):\n", "code": "Learn2BanFeature.__init__(self, ip_sieve, ip_feature_db)\n\n#Each feature need to have unique index as the field number\n#in ip_feature_db\nself._FEATURE_INDEX = 3", "path": "src\\features\\src\\feature_html_to_image_ratio.py", "repo_name": "equalitie/learn2ban", "stars": 134, "license": "None", "language": "python", "size": 95}
{"docstring": "\"\"\"\nIt sets the order records directy to the dictionary\nsupplied by the user\n\"\"\"\n", "func_signal": "def set_pre_seived_order_records(self, pre_seived_records):\n", "code": "self.dict_invalid = False\nself._ordered_records = pre_seived_records", "path": "src\\ip_sieve.py", "repo_name": "equalitie/learn2ban", "stars": 134, "license": "None", "language": "python", "size": 95}
{"docstring": "\"\"\"\nretrieve the ip dictionary and compute the average for each\nip to determine the change rate of UA per IP.\n\"\"\"\n", "func_signal": "def compute(self):\n", "code": "ip_recs = self._ip_sieve.ordered_records()\n\n#Vmon: obviously the total size comparative to all other sizes is important\n#so we are better off not to divide because the normalizer will\n#compute that value and apply it during prediction\n\n#Vmon: totally bullshit. Normalization divides by all request of all \n#requesters while here we are only average for this specific ip\nfor cur_ip_rec in ip_recs:\n    total_size = 0\n    for payload in ip_recs[cur_ip_rec]:\n        total_size += int(payload.get_payload_size())\n\n    #Calculate average pyalod size for given IP\n    self.append_feature(cur_ip_rec, (total_size > 0) and total_size / len(ip_recs[cur_ip_rec]) or 0)", "path": "src\\features\\src\\feature_payload_size_average.py", "repo_name": "equalitie/learn2ban", "stars": 134, "license": "None", "language": "python", "size": 95}
{"docstring": "\"\"\"\nIt takes the name of the log file and open the file\nthrow and exception if not successful.\n\"\"\"\n", "func_signal": "def set_log_file(self, log_filename):\n", "code": "self.dict_invalid = True\nself._log_filename = log_filename\nself._log_lines = open(self._log_filename)", "path": "src\\ip_sieve_shlex.py", "repo_name": "equalitie/learn2ban", "stars": 134, "license": "None", "language": "python", "size": 95}
{"docstring": "\"\"\"\nretrieve the ip dictionary and compute the average for each\nip. This is basically the time of the last request  - first / no of requests.\n\"\"\"\n", "func_signal": "def compute(self):\n", "code": "ip_recs = self._ip_sieve.ordered_records()\nfor cur_ip_rec in ip_recs:\n    # print len(ip_recs[cur_ip_rec])\n    # print ip_recs[cur_ip_rec][-1].time_to_second(), ip_recs[cur_ip_rec][0].time_to_second()\n    # print (ip_recs[cur_ip_rec][-1].time_to_second() - ip_recs[cur_ip_rec][0].time_to_second())/(len(ip_recs[cur_ip_rec])-1.0)\n    feature_value = (len(ip_recs[cur_ip_rec]) > 1) and (ip_recs[cur_ip_rec][-1].time_to_second() - ip_recs[cur_ip_rec][0].time_to_second())/(len(ip_recs[cur_ip_rec])-1.0) or self.MAX_IDEAL_SESSION_LENGTH #If there's only one request then what average time mean? It should be infinity instead fo zero\n    # print feature_value\n    self.append_feature(cur_ip_rec, feature_value)", "path": "src\\features\\src\\feature_average_request_interval.py", "repo_name": "equalitie/learn2ban", "stars": 134, "license": "None", "language": "python", "size": 95}
{"docstring": "\"\"\"\nRead each line of the log file and batch the\nrecords corresponding to each (client (ip), session)\nmake a dictionary of lists each consisting of all records of that session\n\"\"\"\n", "func_signal": "def parse_log_old(self):\n", "code": "for cur_rec in self._log_lines:\n    #Here (at least for now) we only care about the ip and the time record.\n    time_pos = cur_rec.find('-')\n    if time_pos == -1: #Not a valid record\n        continue\n\n    http_req_pos = cur_rec.find('\"')\n    cur_ip = cur_rec[:time_pos-1]\n    rec_time = cur_rec[time_pos + 3:http_req_pos - 2]\n    rec_payload = cur_rec[http_req_pos:]\n    #check if we have already encountered this ip\n\n\n    cur_ats_rec = ATSRecord(cur_ip, rec_time, rec_payload)\n    if not cur_ip in self._ordered_records:\n        self._ordered_records[cur_ip] = [cur_ats_rec]\n    else:\n        self._ordered_records[cur_ip].append(cur_ats_rec)\n\nself.dict_invalid = False", "path": "src\\ip_sieve.py", "repo_name": "equalitie/learn2ban", "stars": 134, "license": "None", "language": "python", "size": 95}
{"docstring": "\"\"\"\nIt takes an array of log lines\n\"\"\"\n", "func_signal": "def set_log_lines(self, log_lines):\n", "code": "self.dict_invalid = True\nself._log_lines = log_lines", "path": "src\\ip_sieve.py", "repo_name": "equalitie/learn2ban", "stars": 134, "license": "None", "language": "python", "size": 95}
{"docstring": "\"\"\"\nSimply calls the parent constructor\n\"\"\"\n", "func_signal": "def __init__(self, ip_sieve, ip_feature_db):\n", "code": "Learn2BanFeature.__init__(self, ip_sieve, ip_feature_db)\n\n#Each feature need to have unique index as the field number\n#in ip_feature_db\nself._FEATURE_INDEX = 1", "path": "src\\features\\src\\feature_average_request_interval.py", "repo_name": "equalitie/learn2ban", "stars": 134, "license": "None", "language": "python", "size": 95}
{"docstring": "\"\"\"\nRead each line of the log file and batch the\nrecords corresponding to each client (ip) \nmake a dictionary of lists each consisting of all records\n\"\"\"\n", "func_signal": "def parse_log(self):\n", "code": "for cur_rec in self._log_lines:\n    #Here (at least for now) we only care about the ip and the time record.\n    time_pos = cur_rec.find('-')\n    if time_pos == -1: #Not a valid record\n        continue\n\n    http_req_pos = cur_rec.find('\"')\n    cur_ip = cur_rec[:time_pos-1]\n    rec_time = cur_rec[time_pos + 3:http_req_pos - 2]\n    rec_payload = cur_rec[http_req_pos:]\n    #check if we have already encountered this ip\n    cur_ats_rec = ATSRecord(cur_ip, rec_time, rec_payload)\n    if not cur_ip in self._ordered_records:\n        self._ordered_records[cur_ip] = [cur_ats_rec]\n    else:\n        self._ordered_records[cur_ip].append(cur_ats_rec)\n\nself.dict_invalid = False", "path": "src\\ip_sieve_shlex.py", "repo_name": "equalitie/learn2ban", "stars": 134, "license": "None", "language": "python", "size": 95}
{"docstring": "#if len(bitcoinpayments)!=len(addresses_shares):\n#    raise Exception(\"\")\n", "func_signal": "def withdraw_all(cls, bitcoinpayments, addresses_shares):\n", "code": "amounts_all=Payment.calculate_amounts(bitcoinpayments, addresses_shares)\nfor bp in bitcoinpayments:\n    am=bp.withdraw_amounts(addresses_shares)\n    bp.withdraw_addresses=\",\".join(addresses_shares.keys())\n    bp.withdraw_proportions=\",\".join(\n        [str(x) for x in addresses_shares.values()])\n    bp.withdraw_amounts=\",\".join(\n        [str(x) for x in am])\n    bp.withdrawn_at=datetime.datetime.now()\n    bp.withdrawn_total=sum(am)\n    bp.save()\nfor i, share in enumerate(addresses_shares.keys()):\n    bitcoind.send(share, amounts_all[i])\nreturn True", "path": "django_bitcoin\\models.py", "repo_name": "kangasbros/django-bitcoin", "stars": 177, "license": "mit", "language": "python", "size": 1145}
{"docstring": "'''\nDisplay a bitcoin address with plus the link to its blockexplorer page.\n'''\n# note: i disapprove including somewhat unnecessary depencies such as this, especially since blockexplorer is  unreliable service\n", "func_signal": "def show_addr(address, arg):\n", "code": "link =\"<a href='http://blockexplorer.com/%s/'>%s</a>\"\nif arg == 'long':\n    return link % (address, address)\nelse:\n    return link % (address, address[:8])", "path": "django_bitcoin\\templatetags\\currency_conversions.py", "repo_name": "kangasbros/django-bitcoin", "stars": 177, "license": "mit", "language": "python", "size": 1145}
{"docstring": "# Deleting field 'DepositTransaction.confirmations'\n", "func_signal": "def backwards(self, orm):\n", "code": "db.delete_column('django_bitcoin_deposittransaction', 'confirmations')\n\n# Deleting field 'DepositTransaction.txid'\ndb.delete_column('django_bitcoin_deposittransaction', 'txid')\n\n# Deleting field 'WalletTransaction.txid'\ndb.delete_column('django_bitcoin_wallettransaction', 'txid')", "path": "django_bitcoin\\migrations\\0010_auto__add_field_deposittransaction_confirmations__add_field_deposittra.py", "repo_name": "kangasbros/django-bitcoin", "stars": 177, "license": "mit", "language": "python", "size": 1145}
{"docstring": "\"\"\"hash address -> percentage (string -> Decimal)\"\"\"\n", "func_signal": "def withdraw_amounts(self, addresses_shares):\n", "code": "if self.amount_paid<self.amount:\n    raise Exception(\"Not paid.\")\nif self.withdrawn_at:\n    raise Exception(\"Trying to withdraw again.\")\nif sum(addresses_shares.values())>100:\n    raise Exception(\"Sum of proportions must be <=100.\")\n#self.withdraw_addresses=\",\".join(addresses)\n#self.withdraw_proportions=\",\".join([str(x) for x in proportions])\namounts=[]\nfor p in addresses_shares.values():\n    if p<=0:\n        raise Exception()\n    am=quantitize_bitcoin(Decimal((p/Decimal(\"100.0\"))*self.amount))\n    amounts.append(am)\n#self.withdraw_proportions=\",\".join([str(x) for x in ])\nif sum(amounts)>self.amount:\n    raise Exception(\"Sum of calculated amounts exceeds funds.\")\nreturn amounts", "path": "django_bitcoin\\models.py", "repo_name": "kangasbros/django-bitcoin", "stars": 177, "license": "mit", "language": "python", "size": 1145}
{"docstring": "\"\"\"Gets the current equivalent amount of the given Money in\nthe target currency\n\"\"\"\n", "func_signal": "def __call__(self, money, target=\"BTC\"):\n", "code": "if not hasattr(money, \"identifier\"):\n    raise ConversionError(\n        \"Use annotated currency (e.g. Money) as \"\n        \"the unit argument\")\n\nif money.identifier not in self.currencies:\n    raise ConversionError(\n        \"Unknown source currency %(identifier)s. \"\n        \"Available currencies: %(currency_list)s\" % {\n            \"identifier\": money.identifier,\n            \"currency_list\": u\", \".join(self.currencies.keys())})\n\nif target not in self.currencies:\n    raise ConversionError(\n        \"Unknown target currency %(identifier)s. \"\n        \"Available currencies: %(currency_list)s\" % {\n            \"identifier\": target,\n            \"currency_list\": u\", \".join(self.currencies.keys())})\n\nbtc = self.currencies[money.identifier].to_btc(money.amount)\nreturn Money(target, self.currencies[target].from_btc(btc))", "path": "django_bitcoin\\currency.py", "repo_name": "kangasbros/django-bitcoin", "stars": 177, "license": "mit", "language": "python", "size": 1145}
{"docstring": "# Adding field 'DepositTransaction.confirmations'\n", "func_signal": "def forwards(self, orm):\n", "code": "db.add_column('django_bitcoin_deposittransaction', 'confirmations',\n              self.gf('django.db.models.fields.IntegerField')(default=0),\n              keep_default=False)\n\n# Adding field 'DepositTransaction.txid'\ndb.add_column('django_bitcoin_deposittransaction', 'txid',\n              self.gf('django.db.models.fields.CharField')(max_length=100, null=True, blank=True),\n              keep_default=False)\n\n# Adding field 'WalletTransaction.txid'\ndb.add_column('django_bitcoin_wallettransaction', 'txid',\n              self.gf('django.db.models.fields.CharField')(max_length=100, null=True, blank=True),\n              keep_default=False)", "path": "django_bitcoin\\migrations\\0010_auto__add_field_deposittransaction_confirmations__add_field_deposittra.py", "repo_name": "kangasbros/django-bitcoin", "stars": 177, "license": "mit", "language": "python", "size": 1145}
{"docstring": "# Adding field 'WalletTransaction.deposit_transaction'\n", "func_signal": "def forwards(self, orm):\n", "code": "db.add_column('django_bitcoin_wallettransaction', 'deposit_transaction',\n              self.gf('django.db.models.fields.related.OneToOneField')(to=orm['django_bitcoin.DepositTransaction'], unique=True, null=True),\n              keep_default=False)", "path": "django_bitcoin\\migrations\\0015_auto__add_field_wallettransaction_deposit_transaction.py", "repo_name": "kangasbros/django-bitcoin", "stars": 177, "license": "mit", "language": "python", "size": 1145}
{"docstring": "'''No need for labels.'''\n", "func_signal": "def save(self, *args, **kwargs):\n", "code": "self.updated_at = datetime.datetime.now()\nsuper(Wallet, self).save(*args, **kwargs)\n#super(Wallet, self).save(*args, **kwargs)", "path": "django_bitcoin\\models.py", "repo_name": "kangasbros/django-bitcoin", "stars": 177, "license": "mit", "language": "python", "size": 1145}
{"docstring": "# Adding field 'WalletTransaction.deposit_address'\n", "func_signal": "def forwards(self, orm):\n", "code": "db.add_column('django_bitcoin_wallettransaction', 'deposit_address',\n              self.gf('django.db.models.fields.related.ForeignKey')(to=orm['django_bitcoin.BitcoinAddress'], null=True),\n              keep_default=False)", "path": "django_bitcoin\\migrations\\0013_auto__add_field_wallettransaction_deposit_address.py", "repo_name": "kangasbros/django-bitcoin", "stars": 177, "license": "mit", "language": "python", "size": 1145}
{"docstring": "# Deleting model 'HistoricalPrice'\n", "func_signal": "def backwards(self, orm):\n", "code": "db.delete_table('django_bitcoin_historicalprice')\n\n\n# Changing field 'Wallet.last_balance'\ndb.alter_column('django_bitcoin_wallet', 'last_balance', self.gf('django.db.models.fields.DecimalField')(max_digits=5, decimal_places=2))", "path": "django_bitcoin\\migrations\\0008_auto__add_historicalprice__chg_field_wallet_last_balance.py", "repo_name": "kangasbros/django-bitcoin", "stars": 177, "license": "mit", "language": "python", "size": 1145}
{"docstring": "# Deleting model 'Transaction'\n", "func_signal": "def backwards(self, orm):\n", "code": "        db.delete_table('django_bitcoin_transaction')\n\n        # Deleting model 'BitcoinAddress'\n        db.delete_table('django_bitcoin_bitcoinaddress')\n\n        # Deleting model 'Payment'\n        db.delete_table('django_bitcoin_payment')\n\n        # Removing M2M table for field transactions on 'Payment'\n        db.delete_table('django_bitcoin_payment_transactions')\n\n        # Deleting model 'WalletTransaction'\n        db.delete_table('django_bitcoin_wallettransaction')\n\n        # Deleting model 'Wallet'\n        db.delete_table('django_bitcoin_wallet')\n\n        # Removing M2M table for field addresses on 'Wallet'\n        db.delete_table('django_bitcoin_wallet_addresses')", "path": "django_bitcoin\\migrations\\0001_initial.py", "repo_name": "kangasbros/django-bitcoin", "stars": 177, "license": "mit", "language": "python", "size": 1145}
{"docstring": "\"\"\"return bitcoin price from any service we can get it\"\"\"\n", "func_signal": "def bitcoinprice_usd():\n", "code": "warnings.warn(\"Use django_bitcoin.currency.exchange.get_rate('USD')\",\n              DeprecationWarning)\nreturn {\"24h\": currency.exchange.get_rate(\"USD\")}", "path": "django_bitcoin\\utils.py", "repo_name": "kangasbros/django-bitcoin", "stars": 177, "license": "mit", "language": "python", "size": 1145}
{"docstring": "# Adding model 'HistoricalPrice'\n", "func_signal": "def forwards(self, orm):\n", "code": "db.create_table('django_bitcoin_historicalprice', (\n    ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n    ('created_at', self.gf('django.db.models.fields.DateTimeField')(auto_now_add=True, blank=True)),\n    ('price', self.gf('django.db.models.fields.DecimalField')(max_digits=16, decimal_places=2)),\n    ('params', self.gf('django.db.models.fields.CharField')(max_length=50)),\n    ('currency', self.gf('django.db.models.fields.CharField')(max_length=10)),\n))\ndb.send_create_signal('django_bitcoin', ['HistoricalPrice'])\n\n\n# Changing field 'Wallet.last_balance'\ndb.alter_column('django_bitcoin_wallet', 'last_balance', self.gf('django.db.models.fields.DecimalField')(max_digits=16, decimal_places=8))", "path": "django_bitcoin\\migrations\\0008_auto__add_historicalprice__chg_field_wallet_last_balance.py", "repo_name": "kangasbros/django-bitcoin", "stars": 177, "license": "mit", "language": "python", "size": 1145}
{"docstring": "\"\"\"Returns True if this wallet was any transacion history.\"\"\"\n", "func_signal": "def has_history(self):\n", "code": "if self.received_transactions.all().count():\n    return True\nif self.sent_transactions.all().count():\n    return True\nif filter(lambda x: x.received(), self.addresses.all()):\n    return True\nreturn False", "path": "django_bitcoin\\models.py", "repo_name": "kangasbros/django-bitcoin", "stars": 177, "license": "mit", "language": "python", "size": 1145}
{"docstring": "# cryptographically safe random\n", "func_signal": "def generateuniquehash(length=43, extradata=''):\n", "code": "r=str(os.urandom(64))\nm = hashlib.sha256()\nm.update(r+str(extradata))\nkey=m.digest()\nkey=base64.urlsafe_b64encode(key)\nreturn key[:min(length, 43)]", "path": "django_bitcoin\\utils.py", "repo_name": "kangasbros/django-bitcoin", "stars": 177, "license": "mit", "language": "python", "size": 1145}
{"docstring": "\"\"\"Rate is inferred from a dummy exchange\"\"\"\n", "func_signal": "def get_rate(self, currency, target=\"BTC\"):\n", "code": "start = Money(currency, \"1.0\")\nend = self(start, target)\nreturn end.amount", "path": "django_bitcoin\\currency.py", "repo_name": "kangasbros/django-bitcoin", "stars": 177, "license": "mit", "language": "python", "size": 1145}
{"docstring": "# print \"bitcoinformat\", value\n", "func_signal": "def bitcoinformat(value):\n", "code": "if value == None:\n    return None\nif not (isinstance(value, float) or isinstance(value, Decimal)):\n    return str(value).rstrip('0').rstrip('.')\nreturn (\"%.8f\" % value).rstrip('0').rstrip('.')", "path": "django_bitcoin\\templatetags\\currency_conversions.py", "repo_name": "kangasbros/django-bitcoin", "stars": 177, "license": "mit", "language": "python", "size": 1145}
{"docstring": "# Adding model 'Transaction'\n", "func_signal": "def forwards(self, orm):\n", "code": "        db.create_table('django_bitcoin_transaction', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('created_at', self.gf('django.db.models.fields.DateTimeField')(default=datetime.datetime.now)),\n            ('amount', self.gf('django.db.models.fields.DecimalField')(default='0.0', max_digits=16, decimal_places=8)),\n            ('address', self.gf('django.db.models.fields.CharField')(max_length=50)),\n        ))\n        db.send_create_signal('django_bitcoin', ['Transaction'])\n\n        # Adding model 'BitcoinAddress'\n        db.create_table('django_bitcoin_bitcoinaddress', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('address', self.gf('django.db.models.fields.CharField')(max_length=50, blank=True)),\n            ('created_at', self.gf('django.db.models.fields.DateTimeField')(default=datetime.datetime.now)),\n            ('active', self.gf('django.db.models.fields.BooleanField')(default=False)),\n            ('least_received', self.gf('django.db.models.fields.DecimalField')(default='0', max_digits=16, decimal_places=8)),\n        ))\n        db.send_create_signal('django_bitcoin', ['BitcoinAddress'])\n\n        # Adding model 'Payment'\n        db.create_table('django_bitcoin_payment', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('description', self.gf('django.db.models.fields.CharField')(max_length=255, blank=True)),\n            ('address', self.gf('django.db.models.fields.CharField')(max_length=50)),\n            ('amount', self.gf('django.db.models.fields.DecimalField')(default='0.0', max_digits=16, decimal_places=8)),\n            ('amount_paid', self.gf('django.db.models.fields.DecimalField')(default='0.0', max_digits=16, decimal_places=8)),\n            ('active', self.gf('django.db.models.fields.BooleanField')(default=False)),\n            ('created_at', self.gf('django.db.models.fields.DateTimeField')(default=datetime.datetime.now)),\n            ('updated_at', self.gf('django.db.models.fields.DateTimeField')()),\n            ('paid_at', self.gf('django.db.models.fields.DateTimeField')(default=None, null=True)),\n            ('withdrawn_total', self.gf('django.db.models.fields.DecimalField')(default='0.0', max_digits=16, decimal_places=8)),\n        ))\n        db.send_create_signal('django_bitcoin', ['Payment'])\n\n        # Adding M2M table for field transactions on 'Payment'\n        db.create_table('django_bitcoin_payment_transactions', (\n            ('id', models.AutoField(verbose_name='ID', primary_key=True, auto_created=True)),\n            ('payment', models.ForeignKey(orm['django_bitcoin.payment'], null=False)),\n            ('transaction', models.ForeignKey(orm['django_bitcoin.transaction'], null=False))\n        ))\n        db.create_unique('django_bitcoin_payment_transactions', ['payment_id', 'transaction_id'])\n\n        # Adding model 'WalletTransaction'\n        db.create_table('django_bitcoin_wallettransaction', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('created_at', self.gf('django.db.models.fields.DateTimeField')(default=datetime.datetime.now)),\n            ('from_wallet', self.gf('django.db.models.fields.related.ForeignKey')(related_name='sent_transactions', to=orm['django_bitcoin.Wallet'])),\n            ('to_wallet', self.gf('django.db.models.fields.related.ForeignKey')(related_name='received_transactions', null=True, to=orm['django_bitcoin.Wallet'])),\n            ('to_bitcoinaddress', self.gf('django.db.models.fields.CharField')(max_length=50, blank=True)),\n            ('amount', self.gf('django.db.models.fields.DecimalField')(default='0.0', max_digits=16, decimal_places=8)),\n            ('description', self.gf('django.db.models.fields.CharField')(max_length=100, blank=True)),\n        ))\n        db.send_create_signal('django_bitcoin', ['WalletTransaction'])\n\n        # Adding model 'Wallet'\n        db.create_table('django_bitcoin_wallet', (\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('created_at', self.gf('django.db.models.fields.DateTimeField')(default=datetime.datetime.now)),\n            ('updated_at', self.gf('django.db.models.fields.DateTimeField')()),\n            ('label', self.gf('django.db.models.fields.CharField')(max_length=50, blank=True)),\n        ))\n        db.send_create_signal('django_bitcoin', ['Wallet'])\n\n        # Adding M2M table for field addresses on 'Wallet'\n        db.create_table('django_bitcoin_wallet_addresses', (\n            ('id', models.AutoField(verbose_name='ID', primary_key=True, auto_created=True)),\n            ('wallet', models.ForeignKey(orm['django_bitcoin.wallet'], null=False)),\n            ('bitcoinaddress', models.ForeignKey(orm['django_bitcoin.bitcoinaddress'], null=False))\n        ))\n        db.create_unique('django_bitcoin_wallet_addresses', ['wallet_id', 'bitcoinaddress_id'])", "path": "django_bitcoin\\migrations\\0001_initial.py", "repo_name": "kangasbros/django-bitcoin", "stars": 177, "license": "mit", "language": "python", "size": 1145}
{"docstring": "\"\"\"\nReturns the total confirmed balance from the Wallet.\n\"\"\"\n", "func_signal": "def total_balance(self, minconf=settings.BITCOIN_MINIMUM_CONFIRMATIONS):\n", "code": "if not settings.BITCOIN_UNCONFIRMED_TRANSFERS:\n    # if settings.BITCOIN_TRANSACTION_SIGNALING:\n    #     if minconf == settings.BITCOIN_MINIMUM_CONFIRMATIONS:\n    #         return self.total_balance_sql()\n    #     elif mincof == 0:\n    #         self.total_balance_sql(False)\n    if minconf >= settings.BITCOIN_MINIMUM_CONFIRMATIONS:\n        self.last_balance = self.total_received(minconf) - self.total_sent()\n        return self.last_balance\n    else:\n        return self.total_received(minconf) - self.total_sent()\nelse:\n    return self.balance(minconf)[1]", "path": "django_bitcoin\\models.py", "repo_name": "kangasbros/django-bitcoin", "stars": 177, "license": "mit", "language": "python", "size": 1145}
{"docstring": "\"\"\"Returns the raw ammount ever received by this wallet.\"\"\"\n", "func_signal": "def total_received(self, minconf=settings.BITCOIN_MINIMUM_CONFIRMATIONS):\n", "code": "if settings.BITCOIN_TRANSACTION_SIGNALING:\n    if minconf == settings.BITCOIN_MINIMUM_CONFIRMATIONS:\n        s = self.addresses.filter(migrated_to_transactions=False).aggregate(models.Sum(\"least_received_confirmed\"))['least_received_confirmed__sum'] or Decimal(0)\n    elif minconf == 0:\n        s = self.addresses.all().aggregate(models.Sum(\"least_received\"))['least_received__sum'] or Decimal(0)\n    else:\n        s = sum([a.received(minconf=minconf) for a in self.addresses.filter(migrated_to_transactions=False)])\nelse:\n    s = sum([a.received(minconf=minconf) for a in self.addresses.filter(migrated_to_transactions=False)])\nif minconf == 0:\n    rt = self.received_transactions.filter(from_wallet__gte=1).aggregate(models.Sum(\"amount\"))['amount__sum'] or Decimal(0)\nelse:\n    rt = self.received_transactions.aggregate(models.Sum(\"amount\"))['amount__sum'] or Decimal(0)\nreturn (s + rt)", "path": "django_bitcoin\\models.py", "repo_name": "kangasbros/django-bitcoin", "stars": 177, "license": "mit", "language": "python", "size": 1145}
{"docstring": "''' Encode and sign a pickle-able object. Return a (byte) string '''\n", "func_signal": "def cookie_encode(data, key):\n", "code": "msg = base64.b64encode(pickle.dumps(data, -1))\nsig = base64.b64encode(hmac.new(tob(key), msg).digest())\nreturn tob('!') + sig + tob('?') + msg", "path": "bottle.py", "repo_name": "douban/graph-index", "stars": 129, "license": "mit", "language": "python", "size": 871}
{"docstring": "\"\"\" Return a decorator that attaches a callback to a hook. Three hooks\n    are currently implemented:\n\n    - before_request: Executed once before each request\n    - after_request: Executed once after each request\n    - app_reset: Called whenever :meth:`reset` is called.\n\"\"\"\n", "func_signal": "def hook(self, name):\n", "code": "def wrapper(func):\n    self.hooks.add(name, func)\n    return func\nreturn wrapper", "path": "bottle.py", "repo_name": "douban/graph-index", "stars": 129, "license": "mit", "language": "python", "size": 871}
{"docstring": "\"\"\" Return a string that matches a named route \"\"\"\n", "func_signal": "def get_url(self, routename, **kargs):\n", "code": "scriptname = request.environ.get('SCRIPT_NAME', '').strip('/') + '/'\nlocation = self.router.build(routename, **kargs).lstrip('/')\nreturn urljoin(urljoin('/', scriptname), location)", "path": "bottle.py", "repo_name": "douban/graph-index", "stars": 129, "license": "mit", "language": "python", "size": 871}
{"docstring": "\"\"\" Wrap a WSGI environ dictionary. \"\"\"\n#: The wrapped WSGI environ dictionary. This is the only real attribute.\n#: All other attributes actually are read-only properties.\n", "func_signal": "def __init__(self, environ=None):\n", "code": "self.environ = {} if environ is None else environ\nself.environ['bottle.request'] = self", "path": "bottle.py", "repo_name": "douban/graph-index", "stars": 129, "license": "mit", "language": "python", "size": 871}
{"docstring": "''' WSGI conform list of (header, value) tuples. '''\n", "func_signal": "def headerlist(self):\n", "code": "out = []\nheaders = list(self._headers.items())\nif 'Content-Type' not in self._headers:\n    headers.append(('Content-Type', [self.default_content_type]))\nif self._status_code in self.bad_headers:\n    bad_headers = self.bad_headers[self._status_code]\n    headers = [h for h in headers if h[0] not in bad_headers]\nout += [(name, val) for name, vals in headers for val in vals]\nif self._cookies:\n    for c in self._cookies.values():\n        out.append(('Set-Cookie', c.OutputString()))\nreturn out", "path": "bottle.py", "repo_name": "douban/graph-index", "stars": 129, "license": "mit", "language": "python", "size": 871}
{"docstring": "\"\"\" Load a bottle application from a module and make sure that the import\n    does not affect the current default application, but returns a separate\n    application object. See :func:`load` for the target parameter. \"\"\"\n", "func_signal": "def load_app(target):\n", "code": "global NORUN; NORUN, nr_old = True, NORUN\ntry:\n    tmp = default_app.push() # Create a new \"default application\"\n    rv = load(target) # Import the target module\n    return rv if callable(rv) else tmp\nfinally:\n    default_app.remove(tmp) # Remove the temporary added default application\n    NORUN = nr_old", "path": "bottle.py", "repo_name": "douban/graph-index", "stars": 129, "license": "mit", "language": "python", "size": 871}
{"docstring": "''' Return a (target, url_agrs) tuple or raise HTTPError(400/404/405). '''\n", "func_signal": "def match(self, environ):\n", "code": "path, targets, urlargs = environ['PATH_INFO'] or '/', None, {}\nif path in self.static:\n    targets = self.static[path]\nelse:\n    for combined, rules in self.dynamic:\n        match = combined.match(path)\n        if not match: continue\n        getargs, targets = rules[match.lastindex - 1]\n        urlargs = getargs(path) if getargs else {}\n        break\n\nif not targets:\n    raise HTTPError(404, \"Not found: \" + repr(environ['PATH_INFO']))\nmethod = environ['REQUEST_METHOD'].upper()\nif method in targets:\n    return targets[method], urlargs\nif method == 'HEAD' and 'GET' in targets:\n    return targets['GET'], urlargs\nif 'ANY' in targets:\n    return targets['ANY'], urlargs\nallowed = [verb for verb in targets if verb != 'ANY']\nif 'GET' in allowed and 'HEAD' not in allowed:\n    allowed.append('HEAD')\nraise HTTPError(405, \"Method not allowed.\", Allow=\",\".join(allowed))", "path": "bottle.py", "repo_name": "douban/graph-index", "stars": 129, "license": "mit", "language": "python", "size": 871}
{"docstring": "#: If true, most exceptions are caught and returned as :exc:`HTTPError`\n", "func_signal": "def __init__(self, catchall=True, autojson=True):\n", "code": "self.catchall = catchall\n\n#: A :class:`ResourceManager` for application files\nself.resources = ResourceManager()\n\n#: A :class:`ConfigDict` for app specific configuration.\nself.config = ConfigDict()\nself.config.autojson = autojson\n\nself.routes = [] # List of installed :class:`Route` instances.\nself.router = Router() # Maps requests to :class:`Route` instances.\nself.error_handler = {}\n\n# Core plugins\nself.plugins = [] # List of installed plugins.\nself.hooks = HooksPlugin()\nself.install(self.hooks)\nif self.config.autojson:\n    self.install(JSONPlugin())\nself.install(TemplatePlugin())", "path": "bottle.py", "repo_name": "douban/graph-index", "stars": 129, "license": "mit", "language": "python", "size": 871}
{"docstring": "''' Find a resource and return a file object, or raise IOError. '''\n", "func_signal": "def open(self, name, mode='r', *args, **kwargs):\n", "code": "fname = self.lookup(name)\nif not fname: raise IOError(\"Resource %r not found.\" % name)\nreturn self.opener(name, mode=mode, *args, **kwargs)", "path": "bottle.py", "repo_name": "douban/graph-index", "stars": 129, "license": "mit", "language": "python", "size": 871}
{"docstring": "''' The :attr:`url` string as an :class:`urlparse.SplitResult` tuple.\n    The tuple contains (scheme, host, path, query_string and fragment),\n    but the fragment is always empty because it is not visible to the\n    server. '''\n", "func_signal": "def urlparts(self):\n", "code": "env = self.environ\nhttp = env.get('HTTP_X_FORWARDED_PROTO') or env.get('wsgi.url_scheme', 'http')\nhost = env.get('HTTP_X_FORWARDED_HOST') or env.get('HTTP_HOST')\nif not host:\n    # HTTP 1.1 requires a Host-header. This is for HTTP/1.0 clients.\n    host = env.get('SERVER_NAME', '127.0.0.1')\n    port = env.get('SERVER_PORT')\n    if port and port != ('80' if http == 'http' else '443'):\n        host += ':' + port\npath = urlquote(self.fullpath)\nreturn UrlSplitResult(http, host, path, env.get('QUERY_STRING'), '')", "path": "bottle.py", "repo_name": "douban/graph-index", "stars": 129, "license": "mit", "language": "python", "size": 871}
{"docstring": "''' Shift path fragments from PATH_INFO to SCRIPT_NAME and vice versa.\n\n    :return: The modified paths.\n    :param script_name: The SCRIPT_NAME path.\n    :param script_name: The PATH_INFO path.\n    :param shift: The number of path fragments to shift. May be negative to\n      change the shift direction. (default: 1)\n'''\n", "func_signal": "def path_shift(script_name, path_info, shift=1):\n", "code": "if shift == 0: return script_name, path_info\npathlist = path_info.strip('/').split('/')\nscriptlist = script_name.strip('/').split('/')\nif pathlist and pathlist[0] == '': pathlist = []\nif scriptlist and scriptlist[0] == '': scriptlist = []\nif shift > 0 and shift <= len(pathlist):\n    moved = pathlist[:shift]\n    scriptlist = scriptlist + moved\n    pathlist = pathlist[shift:]\nelif shift < 0 and shift >= -len(scriptlist):\n    moved = scriptlist[shift:]\n    pathlist = moved + pathlist\n    scriptlist = scriptlist[:shift]\nelse:\n    empty = 'SCRIPT_NAME' if shift < 0 else 'PATH_INFO'\n    raise AssertionError(\"Cannot shift. Nothing left from %s\" % empty)\nnew_script_name = '/' + '/'.join(scriptlist)\nnew_path_info = '/' + '/'.join(pathlist)\nif path_info.endswith('/') and pathlist: new_path_info += '/'\nreturn new_script_name, new_path_info", "path": "bottle.py", "repo_name": "douban/graph-index", "stars": 129, "license": "mit", "language": "python", "size": 871}
{"docstring": "''' Attach a callback to a hook. '''\n", "func_signal": "def add(self, name, func):\n", "code": "was_empty = self._empty()\nself.hooks.setdefault(name, []).append(func)\nif self.app and was_empty and not self._empty(): self.app.reset()", "path": "bottle.py", "repo_name": "douban/graph-index", "stars": 129, "license": "mit", "language": "python", "size": 871}
{"docstring": "# Without this guard, pickle generates a cryptic TypeError:\n", "func_signal": "def __getattr__(self, name, default=unicode()):\n", "code": "if name.startswith('__') and name.endswith('__'):\n    return super(FormsDict, self).__getattr__(name)\nreturn self.getunicode(name, default=default)", "path": "bottle.py", "repo_name": "douban/graph-index", "stars": 129, "license": "mit", "language": "python", "size": 871}
{"docstring": "''' Escape and quote a string to be used as an HTTP attribute.'''\n", "func_signal": "def html_quote(string):\n", "code": "return '\"%s\"' % html_escape(string).replace('\\n','%#10;')\\\n                .replace('\\r','&#13;').replace('\\t','&#9;')", "path": "bottle.py", "repo_name": "douban/graph-index", "stars": 129, "license": "mit", "language": "python", "size": 871}
{"docstring": "\"\"\" Parse rfc2617 HTTP authentication header string (basic) and return (user,pass) tuple or None\"\"\"\n", "func_signal": "def parse_auth(header):\n", "code": "try:\n    method, data = header.split(None, 1)\n    if method.lower() == 'basic':\n        user, pwd = touni(base64.b64decode(tob(data))).split(':',1)\n        return user, pwd\nexcept (KeyError, ValueError):\n    return None", "path": "bottle.py", "repo_name": "douban/graph-index", "stars": 129, "license": "mit", "language": "python", "size": 871}
{"docstring": "''' Add a route object, but do not change the :data:`Route.app`\n    attribute.'''\n", "func_signal": "def add_route(self, route):\n", "code": "self.routes.append(route)\nself.router.add(route.rule, route.method, route, name=route.name)\nif DEBUG: route.prepare()", "path": "bottle.py", "repo_name": "douban/graph-index", "stars": 129, "license": "mit", "language": "python", "size": 871}
{"docstring": "\"\"\" Open a file in a safe way and return :exc:`HTTPResponse` with status\n    code 200, 305, 401 or 404. Set Content-Type, Content-Encoding,\n    Content-Length and Last-Modified header. Obey If-Modified-Since header\n    and HEAD requests.\n\"\"\"\n", "func_signal": "def static_file(filename, root, mimetype='auto', download=False):\n", "code": "root = os.path.abspath(root) + os.sep\nfilename = os.path.abspath(os.path.join(root, filename.strip('/\\\\')))\nheaders = dict()\n\nif not filename.startswith(root):\n    return HTTPError(403, \"Access denied.\")\nif not os.path.exists(filename) or not os.path.isfile(filename):\n    return HTTPError(404, \"File does not exist.\")\nif not os.access(filename, os.R_OK):\n    return HTTPError(403, \"You do not have permission to access this file.\")\n\nif mimetype == 'auto':\n    mimetype, encoding = mimetypes.guess_type(filename)\n    if mimetype: headers['Content-Type'] = mimetype\n    if encoding: headers['Content-Encoding'] = encoding\nelif mimetype:\n    headers['Content-Type'] = mimetype\n\nif download:\n    download = os.path.basename(filename if download == True else download)\n    headers['Content-Disposition'] = 'attachment; filename=\"%s\"' % download\n\nstats = os.stat(filename)\nheaders['Content-Length'] = clen = stats.st_size\nlm = time.strftime(\"%a, %d %b %Y %H:%M:%S GMT\", time.gmtime(stats.st_mtime))\nheaders['Last-Modified'] = lm\n\nims = request.environ.get('HTTP_IF_MODIFIED_SINCE')\nif ims:\n    ims = parse_date(ims.split(\";\")[0].strip())\nif ims is not None and ims >= int(stats.st_mtime):\n    headers['Date'] = time.strftime(\"%a, %d %b %Y %H:%M:%S GMT\", time.gmtime())\n    return HTTPResponse(status=304, **headers)\n\nbody = '' if request.method == 'HEAD' else open(filename, 'rb')\n\nheaders[\"Accept-Ranges\"] = \"bytes\"\nranges = request.environ.get('HTTP_RANGE')\nif 'HTTP_RANGE' in request.environ:\n    ranges = list(parse_range_header(request.environ['HTTP_RANGE'], clen))\n    if not ranges:\n        return HTTPError(416, \"Requested Range Not Satisfiable\")\n    offset, end = ranges[0]\n    headers[\"Content-Range\"] = \"bytes %d-%d/%d\" % (offset, end-1, clen)\n    headers[\"Content-Length\"] = str(end-offset)\n    if body: body = _file_iter_range(body, offset, end-offset)\n    return HTTPResponse(body, status=206, **headers)\nreturn HTTPResponse(body, **headers)", "path": "bottle.py", "repo_name": "douban/graph-index", "stars": 129, "license": "mit", "language": "python", "size": 871}
{"docstring": "''' Verify and decode an encoded string. Return an object or None.'''\n", "func_signal": "def cookie_decode(data, key):\n", "code": "data = tob(data)\nif cookie_is_encoded(data):\n    sig, msg = data.split(tob('?'), 1)\n    if _lscmp(sig[1:], base64.b64encode(hmac.new(tob(key), msg).digest())):\n        return pickle.loads(base64.b64decode(msg))\nreturn None", "path": "bottle.py", "repo_name": "douban/graph-index", "stars": 129, "license": "mit", "language": "python", "size": 871}
{"docstring": "\"\"\" Change an environ value and clear all caches that depend on it. \"\"\"\n\n", "func_signal": "def __setitem__(self, key, value):\n", "code": "if self.environ.get('bottle.request.readonly'):\n    raise KeyError('The environ dictionary is read-only.')\n\nself.environ[key] = value\ntodelete = ()\n\nif key == 'wsgi.input':\n    todelete = ('body', 'forms', 'files', 'params', 'post', 'json')\nelif key == 'QUERY_STRING':\n    todelete = ('query', 'params')\nelif key.startswith('HTTP_'):\n    todelete = ('headers', 'cookies')\n\nfor key in todelete:\n    self.environ.pop('bottle.request.'+key, None)", "path": "bottle.py", "repo_name": "douban/graph-index", "stars": 129, "license": "mit", "language": "python", "size": 871}
{"docstring": "''' True if the request was triggered by a XMLHttpRequest. This only\n    works with JavaScript libraries that support the `X-Requested-With`\n    header (most of the popular libraries do). '''\n", "func_signal": "def is_xhr(self):\n", "code": "requested_with = self.environ.get('HTTP_X_REQUESTED_WITH','')\nreturn requested_with.lower() == 'xmlhttprequest'", "path": "bottle.py", "repo_name": "douban/graph-index", "stars": 129, "license": "mit", "language": "python", "size": 871}
{"docstring": "# get outputs\n", "func_signal": "def interpret_test_results(self, retcode, data):\n", "code": "if re.search(r\"Test passed.\", data):\n\t# single test passed\n\tself.log(\"\\n=> TEST PASSED.\\n\")\n\nelif re.search(r\"All \\d+ tests passed.\", data):\n\t# multiple tests passed\n\tpassed_count = re.search(r\"All (\\d+) tests passed.\", data).group(1)\n\tself.log(\"\\n=> %s TESTS PASSED.\\n\" % passed_count)\n\nelif re.search(r\"Failed: \\d+.\", data):\n\t# some tests failed\n\tfailed_count = re.search(r\"Failed: (\\d+).\", data).group(1)\n\tself.log(\"\\n=> %s TEST(S) FAILED.\\n\" % failed_count)\n\nelif re.search(r\"There were no tests to run.\", data):\n\tself.log(\"\\n=> NO TESTS TO RUN.\\n\")\n\nelse:\n\tself.log(data)\n\tself.log(\"\\n=> TEST(S) FAILED.\\n\")\n\n# free test\nself.on_test_ended()", "path": "sublimerl_tests_integration.py", "repo_name": "ostinelli/SublimErl", "stars": 190, "license": "None", "language": "python", "size": 10031}
{"docstring": "# get current line position\n", "func_signal": "def get_test_function_name(self):\n", "code": "cursor_position = self.view.sel()[0].a\n# get module content\nregion_full = sublime.Region(0, self.view.size())\nmodule = SUBLIMERL.strip_code_for_parsing(self.view.substr(region_full))\n# parse regions\nregex = re.compile(r\"([a-z0-9][a-zA-Z0-9_]*_test(_)?\\s*\\(\\s*\\)\\s*->[^.]*\\.)\", re.MULTILINE)\nfor m in regex.finditer(module):\n\tif m.start() <= cursor_position and cursor_position <= m.end():\n\t\tfunction_content = m.groups()[0]\n\t\treturn function_content[:function_content.index('(')]", "path": "sublimerl_tests_integration.py", "repo_name": "ostinelli/SublimErl", "stars": 190, "license": "None", "language": "python", "size": 10031}
{"docstring": "# save current caret position\n", "func_signal": "def format(self):\n", "code": "current_region = self.view.sel()[0]\n# save current file contents to temp file\nregion_full = sublime.Region(0, self.view.size())\ncontent = self.view.substr(region_full).encode('utf-8')\ntemp = tempfile.NamedTemporaryFile(delete=False)\ntemp.write(content)\ntemp.close()\n# call erlang formatter\nos.chdir(SUBLIMERL.support_path)\nescript_command = \"sublimerl_formatter.erl %s\" % SUBLIMERL.shellquote(temp.name)\nretcode, data = SUBLIMERL.execute_os_command('%s %s' % (SUBLIMERL.escript_path, escript_command))\n# delete temp file\nos.remove(temp.name)\nif retcode == 0:\n\t# substitute text\n\tself.view.replace(self.edit, region_full, data.decode('utf-8'))\n\t# reset caret to original position\n\tself.view.sel().clear()\n\tself.view.sel().add(current_region)\n\tself.view.show(current_region)", "path": "sublimerl_formatter.py", "repo_name": "ostinelli/SublimErl", "stars": 190, "license": "None", "language": "python", "size": 10031}
{"docstring": "# init super\n", "func_signal": "def __init__(self, view):\n", "code": "SublimErlProjectLoader.__init__(self, view)\n\n# init\nself.initialized = False\nself.panel_name = 'sublimerl_tests'\nself.panel_buffer = ''\n\n# don't proceed if a test is already running\nglobal SUBLIMERL\nif SUBLIMERL.test_in_progress == True: return\nSUBLIMERL.test_in_progress = True\n\n# setup panel\nself.setup_panel()\n# run\nif self.init_tests() == True:\n\tself.initialized = True\nelse:\n\tSUBLIMERL.test_in_progress = False", "path": "sublimerl_tests_integration.py", "repo_name": "ostinelli/SublimErl", "stars": 190, "license": "None", "language": "python", "size": 10031}
{"docstring": "# get export portion in code module\n\n", "func_signal": "def get_completions(self, module):\n", "code": "all_completions = []\nall_line_numbers = []\nfor m in self.regex['export_section'].finditer(module):\n\texport_section = m.groups()[0]\n\tif export_section:\n\t\t# get list of exports\n\t\texports = self.get_code_list(export_section)\n\t\tif len(exports) > 0:\n\t\t\t# add to existing completions\n\t\t\tcompletions, line_numbers = self.generate_module_completions(module, exports)\n\t\t\tall_completions.extend(completions)\n\t\t\tall_line_numbers.extend(line_numbers)\n# return all_completions\nreturn (all_completions, all_line_numbers)", "path": "support\\sublimerl_libparser.py", "repo_name": "ostinelli/SublimErl", "stars": 190, "license": "None", "language": "python", "size": 10031}
{"docstring": "# get outputs\n", "func_signal": "def interpret_test_results(self, retcode, data):\n", "code": "if re.search(r\"passed successfully\", data):\n\tself.log(\"\\n=> TEST(S) PASSED.\\n\")\nelse:\n\tself.log(\"\\n=> TEST(S) FAILED.\\n\")\n\n# free test\nself.on_test_ended()", "path": "sublimerl_tests_integration.py", "repo_name": "ostinelli/SublimErl", "stars": 190, "license": "None", "language": "python", "size": 10031}
{"docstring": "# default BIFs not available in modules\n", "func_signal": "def bif_completions(self):\n", "code": "return {\n\t'erlang': [\n\t\t('abs/1', 'abs(${1:Number}) $2'),\n\t\t('atom_to_binary/2', 'atom_to_binary(${1:Atom}, ${2:Encoding}) $3'),\n\t\t('atom_to_list/1', 'atom_to_list(${1:Atom}) $2'),\n\t\t('binary_part/2', 'binary_part(${1:Subject}, ${2:PosLen}) $3'),\n\t\t('binary_part/3', 'binary_part(${1:Subject}, ${2:Start}, ${3:Length}) $4'),\n\t\t('binary_to_atom/2', 'binary_to_atom(${1:Binary}, ${2:Encoding}) $3'),\n\t\t('binary_to_existing_atom/2', 'binary_to_existing_atom(${1:Binary}, ${2:Encoding}) $3'),\n\t\t('binary_to_list/1', 'binary_to_list(${1:Binary}) $2'),\n\t\t('binary_to_list/3', 'binary_to_list(${1:Binary}, ${2:Start}, ${3:Stop}) $4'),\n\t\t('bitstring_to_list/1', 'bitstring_to_list(${1:Bitstring}) $2'),\n\t\t('binary_to_term/2', 'binary_to_term(${1:Binary}, ${2:Opts}) $3'),\n\t\t('bit_size/1', 'bit_size(${1:Bitstring}) $2'),\n\t\t('byte_size/1', 'byte_size(${1:Bitstring}) $2'),\n\t\t('check_old_code/1', 'check_old_code(${1:Module}) $2'),\n\t\t('check_process_code/2', 'check_process_code(${1:Pid}, ${2:Module}) $3'),\n\t\t('date/0', 'date() $1'),\n\t\t('delete_module/1', 'delete_module(${1:Module}) $2'),\n\t\t('demonitor/1', 'demonitor(${1:MonitorRef}) $2'),\n\t\t('demonitor/2', 'demonitor(${1:MonitorRef}, ${2:OptionList}) $3'),\n\t\t('element/2', 'element(${1:N}, ${2:Tuple}) $3'),\n\t\t('erase/0', 'erase() $1'),\n\t\t('erase/1', 'erase(${1:Key}) $2'),\n\t\t('error/1', 'error(${1:Reason}) $2'),\n\t\t('error/2', 'error(${1:Reason}, ${2:Args}) $3'),\n\t\t('exit/1', 'exit(${1:Reason}) $2'),\n\t\t('exit/2', 'exit(${1:Reason}, ${2:Args}) $3'),\n\t\t('float/1', 'float(${1:Number}) $2'),\n\t\t('float_to_list/1', 'float_to_list(${1:Float}) $2'),\n\t\t('garbage_collect/0', 'garbage_collect() $1'),\n\t\t('garbage_collect/1', 'garbage_collect(${1:Pid}) $2'),\n\t\t('get/0', 'get() $1'),\n\t\t('get/1', 'get(${1:Key}) $2'),\n\t\t('get_keys/1', 'get_keys(${1:Val}) $2'),\n\t\t('group_leader/0', 'group_leader() $1'),\n\t\t('group_leader/2', 'group_leader(${1:GroupLeader}, ${2:Pid}) $3'),\n\t\t('halt/0', 'halt() $1'),\n\t\t('halt/1', 'halt(${1:Status}) $2'),\n\t\t('halt/2', 'halt(${1:Status}, ${2:Options}) $3'),\n\t\t('hd/1', 'hd(${1:List}) $2'),\n\t\t('integer_to_list/1', 'integer_to_list(${1:Integer}) $2'),\n\t\t('iolist_to_binary/1', 'iolist_to_binary(${1:IoListOrBinary}) $2'),\n\t\t('iolist_size/1', 'iolist_size(${1:Item}) $2'),\n\t\t('is_alive/0', 'is_alive() $1'),\n\t\t('is_atom/1', 'is_atom(${1:Term}) $2'),\n\t\t('is_binary/1', 'is_binary(${1:Term}) $2'),\n\t\t('is_bitstring/1', 'is_bitstring(${1:Term}) $2'),\n\t\t('is_boolean/1', 'is_boolean(${1:Term}) $2'),\n\t\t('is_float/1', 'is_float(${1:Term}) $2'),\n\t\t('is_function/1', 'is_function(${1:Term}) $2'),\n\t\t('is_function/2', 'is_function(${1:Term}, ${2:Arity}) $3'),\n\t\t('is_integer/1', 'is_integer(${1:Term}) $2'),\n\t\t('is_list/1', 'is_list(${1:Term}) $2'),\n\t\t('is_number/1', 'is_number(${1:Term}) $2'),\n\t\t('is_pid/1', 'is_pid(${1:Term}) $2'),\n\t\t('is_port/1', 'is_port(${1:Term}) $2'),\n\t\t('is_process_alive/1', 'is_process_alive(${1:Pid}) $2'),\n\t\t('is_record/2', 'is_record(${1:Term}, ${2:RecordTag}) $3'),\n\t\t('is_record/3', 'is_record(${1:Term}, ${2:RecordTag}, ${3:Size}) $4'),\n\t\t('is_reference/1', 'is_reference(${1:Term}) $2'),\n\t\t('is_tuple/1', 'is_tuple(${1:Term}) $2'),\n\t\t('length/1', 'length(${1:List}) $2'),\n\t\t('link/1', 'link(${1:Pid}) $2'),\n\t\t('list_to_atom/1', 'list_to_atom(${1:String}) $2'),\n\t\t('list_to_binary/1', 'list_to_binary(${1:IoList}) $2'),\n\t\t('list_to_bitstring/1', 'list_to_bitstring(${1:BitstringList}) $2'),\n\t\t('list_to_existing_atom/1', 'list_to_existing_atom(${1:String}) $2'),\n\t\t('list_to_float/1', 'list_to_float(${1:String}) $2'),\n\t\t('list_to_integer/1', 'list_to_integer(${1:String}) $2'),\n\t\t('list_to_pid/1', 'list_to_pid(${1:String}) $2'),\n\t\t('list_to_tuple/1', 'list_to_tuple(${1:List}) $2'),\n\t\t('load_module/2', 'load_module(${1:Module}, ${2:Binary}) $3'),\n\t\t('make_ref/0', 'make_ref() $1'),\n\t\t('module_loaded/1', 'module_loaded(${1:Module}) $2'),\n\t\t('monitor/2', 'monitor(${1:Type}, ${2:Item}) $3'),\n\t\t('monitor_node/2', 'monitor_node(${1:Node}, ${2:Flag}) $3'),\n\t\t('node/0', 'node() $1'),\n\t\t('node/1', 'node(${1:Arg}) $2'),\n\t\t('nodes/1', 'nodes(${1:Arg}) $2'),\n\t\t('now/0', 'now() $1'),\n\t\t('open_port/2', 'open_port(${1:PortName}, ${2:PortSettings}) $3'),\n\t\t('pid_to_list/1', 'pid_to_list(${1:Pid}) $2'),\n\t\t('port_close/1', 'port_close(${1:Port}) $2'),\n\t\t('port_command/2', 'port_command(${1:Port}, ${2:Data}) $3'),\n\t\t('port_command/3', 'port_command(${1:Port}, ${2:Data}, ${3:OptionList}) $4'),\n\t\t('port_connect/2', 'port_connect(${1:Port}, ${2:Pid}) $3'),\n\t\t('port_control/3', 'port_control(${1:Port}, ${2:Operation}, ${3:Data}) $4'),\n\t\t('pre_loaded/0', 'pre_loaded() $1'),\n\t\t('process_flag/2', 'process_flag(${1:Flag}, ${2:Value}) $3'),\n\t\t('process_flag/3', 'process_flag(${1:Pid}, ${2:Flag}, ${3:Value}) $4'),\n\t\t('process_info/1', 'process_info(${1:Pid}) $2'),\n\t\t('process_info/2', 'process_info(${1:Pid}, ${2:ItemSpec}) $3'),\n\t\t('processes/0', 'processes() $1'),\n\t\t('purge_module/1', 'purge_module(${1:Module}) $2'),\n\t\t('put/2', 'put(${1:Key}, ${2:Val}) $3'),\n\t\t('register/2', 'put(${1:RegName}, ${2:PidOrPort}) $3'),\n\t\t('registered/0', 'registered() $1'),\n\t\t('round/1', 'round(${1:Number}) $2'),\n\t\t('self/0', 'self() $1'),\n\t\t('setelement/3', 'setelement(${1:Index}, ${2:Tuple1}, ${3:Value}) $4'),\n\t\t('size/1', 'size(${1:Item}) $2'),\n\t\t('spawn/3', 'spawn(${1:Module}, ${2:Function}) $3, ${3:Args}) $4'),\n\t\t('spawn_link/3', 'spawn_link(${1:Module}, ${2:Function}, ${3:Args}) $4'),\n\t\t('split_binary/2', 'split_binary(${1:Bin}, ${2:Pos}) $3'),\n\t\t('statistics/1', 'statistics(${1:Type}) $2'),\n\t\t('term_to_binary/1', 'term_to_binary(${1:Term}) $2'),\n\t\t('term_to_binary/2', 'term_to_binary(${1:Term}, ${2:Options}) $3'),\n\t\t('throw/1', 'throw(${1:Any}) $2'),\n\t\t('time/0', 'time() $1'),\n\t\t('tl/1', 'tl(${1:List1}) $2'),\n\t\t('trunc/1', 'trunc(${1:Number}) $2'),\n\t\t('tuple_size/1', 'tuple_size(${1:Tuple}) $2'),\n\t\t('tuple_to_list/1', 'tuple_to_list(${1:Tuple}) $2'),\n\t\t('unlink/1', 'unlink(${1:Id}) $2'),\n\t\t('unregister/1', 'unregister(${1:RegName}) $2'),\n\t\t('whereis/1', 'whereis(${1:RegName}) $2')\n\t],\n\t'lists': [\n\t\t('member/2', 'member(${1:Elem}, ${2:List}) $3'),\n\t\t('reverse/2', 'reverse(${1:List1}, ${2:Tail}) $3'),\n\t\t('keymember/3', 'keymember(${1:Key}, ${2:N}, ${3:TupleList}) $4'),\n\t\t('keysearch/3', 'keysearch(${1:Key}, ${2:N}, ${3:TupleList}) $4'),\n\t\t('keyfind/3', 'keyfind(${1:Key}, ${2:N}, ${3:TupleList}) $4')\n\t]\n}", "path": "support\\sublimerl_libparser.py", "repo_name": "ostinelli/SublimErl", "stars": 190, "license": "None", "language": "python", "size": 10031}
{"docstring": "# generate params for a specific function name\n\n# get params count\n", "func_signal": "def generate_params(self, fun, module):\n", "code": "arity = int(fun[1])\n# init\ncurrent_params = []\nlineno = 0\n# get params\nregex = re.compile(r\"%s\\((.*)\\)\\s*->\" % re.escape(fun[0]), re.MULTILINE)\nfor m in regex.finditer(module):\n\tparams = m.groups()[0]\n\t# strip out the eventual condition part ('when')\n\tparams = params.split('when')[0].strip()\n\tif params[-1:] == ')': params = params[:-1]\n\t# split\n\tparams = self.split_params(params)\n\tif len(params) == arity:\n\t\t# function definition has the correct arity\n\t\t# get match line number if this is not a -spec line\n\t\tspec_def_pos = module.rfind('-spec', 0, m.start())\n\t\tnot_a_spec_definition = spec_def_pos == -1 or len(module[spec_def_pos + 5:m.start()].strip()) > 0\n\t\tif not_a_spec_definition and lineno == 0: lineno = module.count('\\n', 0, m.start()) + 1\n\t\t# add to params\n\t\tif current_params != []:\n\t\t\tfor i in range(0, len(params)):\n\t\t\t\tif current_params[i] == '*' and self.regex['varname'].search(params[i]):\n\t\t\t\t\t# found a valid variable name\n\t\t\t\t\tcurrent_params[i] = params[i]\n\t\telse:\n\t\t\t# init params\n\t\t\tcurrent_params = params\n# ensure current params have variable names\nfor i in range(0, len(current_params)):\n\tif current_params[i] == '*':\n\t\tcurrent_params[i] = '${%d:Param%d}' % (i + 1, i + 1)\n\telse:\n\t\tcurrent_params[i] = '${%d:%s}' % (i + 1, current_params[i])\n# return\nreturn ('(' + ', '.join(current_params) + ') $%d' % (len(current_params) + 1), lineno)", "path": "support\\sublimerl_libparser.py", "repo_name": "ostinelli/SublimErl", "stars": 190, "license": "None", "language": "python", "size": 10031}
{"docstring": "# set modules\n", "func_signal": "def show(self):\n", "code": "self.set_module_names()\n# open quick panel\nsublime.active_window().show_quick_panel(self.module_names, self.on_select)", "path": "sublimerl_man.py", "repo_name": "ostinelli/SublimErl", "stars": 190, "license": "None", "language": "python", "size": 10031}
{"docstring": "# init\n", "func_signal": "def run_command(self, edit):\n", "code": "if SUBLIMERL.last_test_type == 'dialyzer': SublimErlTestRunners().dialyzer_test(self.view, new=False)\nelif SUBLIMERL.last_test_type == 'eunit' or SUBLIMERL.last_test_type == 'ct': SublimErlTestRunners().ct_or_eunit_test(self.view, new=False)", "path": "sublimerl_tests_integration.py", "repo_name": "ostinelli/SublimErl", "stars": 190, "license": "None", "language": "python", "size": 10031}
{"docstring": "# open CT results\n", "func_signal": "def run_command(self, edit):\n", "code": "loader = SublimErlProjectLoader(self.view)\nindex_path = os.path.abspath(os.path.join(loader.project_root, 'logs', 'index.html'))\nif os.path.exists(index_path): webbrowser.open(index_path)", "path": "sublimerl_tests_integration.py", "repo_name": "ostinelli/SublimErl", "stars": 190, "license": "None", "language": "python", "size": 10031}
{"docstring": "# init\n", "func_signal": "def generate_completions(self, starting_dir, dest_file_base):\n", "code": "disasms = {}\ncompletions = []\nsearches = []\n# loop directory\nrel_dirs = []\nfor root, dirnames, filenames in os.walk(starting_dir):\n\tif 'reltool.config' in filenames:\n\t\t# found a release directory, we will ignore autocompletion for these files\n\t\trel_dirs.append(root)\n\t# loop filenames ending in .erl\n\tfor filename in fnmatch.filter(filenames, r\"*.erl\"):\n\t\tif '.eunit' not in root.split('/'):\n\t\t\t# exclude eunit files\n\t\t\tfilepath = os.path.join(root, filename)\n\t\t\t# check if in release directory\n\t\t\tif not (True in [filepath.find(rel_dir) != -1 for rel_dir in rel_dirs]):\n\t\t\t\t# not in a release directory, get module name\n\t\t\t\tmodule_name, module_ext = os.path.splitext(filename)\n\t\t\t\t# get module content\n\t\t\t\tf = open(filepath, 'r')\n\t\t\t\tmodule = self.strip_comments(f.read())\n\t\t\t\tf.close()\n\t\t\t\t# get completions\n\t\t\t\tmodule_completions, line_numbers = self.get_completions(module)\n\t\t\t\tif len(module_completions) > 0:\n\t\t\t\t\t# set disasm\n\t\t\t\t\tdisasms[module_name] = sorted(module_completions, key=lambda k: k[0])\n\t\t\t\t\t# set searches\n\t\t\t\t\tfor i in range(0, len(module_completions)):\n\t\t\t\t\t\tfunction, completion = module_completions[i]\n\t\t\t\t\t\tsearches.append((\"%s:%s\" % (module_name, function), filepath, line_numbers[i]))\n\t\t\t\t\t# set module completions\n\t\t\t\t\tcompletions.append(\"{ \\\"trigger\\\": \\\"%s\\\", \\\"contents\\\": \\\"%s\\\" }\" % (module_name, module_name))\n\n# add BIF completions?\nif disasms.has_key('erlang'):\n\t# we are generating erlang disasm\n\tbif_completions = self.bif_completions()\n\tfor k in bif_completions.keys():\n\t\tdisasms[k].extend(bif_completions[k])\n\t\t# sort\n\t\tdisasms[k] = sorted(disasms[k], key=lambda k: k[0])\n\t# erlang completions\n\tfor c in bif_completions['erlang']:\n\t\tcompletions.append(\"{ \\\"trigger\\\": \\\"%s\\\", \\\"contents\\\": \\\"%s\\\" }\" % (c[0], c[1]))\nelse:\n\t# we are generating project disasm -> write to files: searches\n\tf_searches = open(\"%s.searches\" % dest_file_base, 'wb')\n\tpickle.dump(sorted(searches, key=lambda k: k[0]), f_searches)\n\tf_searches.close()\n\n# write to files: disasms\nf_disasms = open(\"%s.disasm\" % dest_file_base, 'wb')\npickle.dump(disasms, f_disasms)\nf_disasms.close()\n# write to files: completions\nf_completions = open(\"%s.sublime-completions\" % dest_file_base, 'wb')\nif len(completions) > 0:\n\tf_completions.write(\"{ \\\"scope\\\": \\\"source.erlang\\\", \\\"completions\\\": [\\n\" + ',\\n'.join(completions) + \"\\n]}\")\nelse:\n\tf_completions.write(\"{}\")\nf_completions.close()", "path": "support\\sublimerl_libparser.py", "repo_name": "ostinelli/SublimErl", "stars": 190, "license": "None", "language": "python", "size": 10031}
{"docstring": "# do not continue if no previous test exists and a redo was asked\n", "func_signal": "def start_test(self, new=True):\n", "code": "if SUBLIMERL.last_test == None and new == False: return\n# set test\nif new == True: self.reset_last_test()\n# test callback\nself.log(\"Starting tests (SublimErl v%s).\\n\" % SUBLIMERL_VERSION)\nself.start_test_cmd(new)", "path": "sublimerl_tests_integration.py", "repo_name": "ostinelli/SublimErl", "stars": 190, "license": "None", "language": "python", "size": 10031}
{"docstring": "# loop every line and add code lines\n", "func_signal": "def get_code_list(self, code):\n", "code": "cleaned_code_list = []\nfor m in self.regex['all'].finditer(code):\n\tgroups = m.groups()\n\tfor i in range(0, len(groups)):\n\t\tcode_line = groups[i].strip()\n\t\tif len(code_line) > 0:\n\t\t\tcode_lines = code_line.split(',')\n\t\t\tfor code_line in code_lines:\n\t\t\t\tcode_line = code_line.strip()\n\t\t\t\tif len(code_line) > 0:\n\t\t\t\t\tcleaned_code_list.append(code_line)\nreturn cleaned_code_list", "path": "support\\sublimerl_libparser.py", "repo_name": "ostinelli/SublimErl", "stars": 190, "license": "None", "language": "python", "size": 10031}
{"docstring": "# compile default regexes\n", "func_signal": "def __init__(self):\n", "code": "self.regex = {\n\t'all': re.compile(r\"(.*)\", re.MULTILINE),\n\t'export_section': re.compile(r\"^\\s*-\\s*export\\s*\\(\\s*\\[\\s*([^\\]]*)\\s*\\]\\s*\\)\\s*\\.\", re.DOTALL + re.MULTILINE),\n\t'varname': re.compile(r\"^[A-Z][a-zA-Z0-9_]*$\"),\n\t'{': re.compile(r\"\\{.*\\}\"),\n\t'<<': re.compile(r\"<<.*>>\"),\n\t'[': re.compile(r\"\\[.*\\]\")\n}", "path": "support\\sublimerl_libparser.py", "repo_name": "ostinelli/SublimErl", "stars": 190, "license": "None", "language": "python", "size": 10031}
{"docstring": "# run CT for suite\n", "func_signal": "def ct_test(self, module_tests_name):\n", "code": "self.log(\"Running tests of Common Tests SUITE \\\"%s_SUITE.erl\\\".\\n\\n\" % module_tests_name)\nos_cmd = '%s ct suites=%s skip_deps=true' % (SUBLIMERL.rebar_path, module_tests_name)\n# compile all source code\nself.compile_source()\n# run suite\nretcode, data = self.execute_os_command(os_cmd, dir_type='test', block=False)\n# interpret\nself.interpret_test_results(retcode, data)", "path": "sublimerl_tests_integration.py", "repo_name": "ostinelli/SublimErl", "stars": 190, "license": "None", "language": "python", "size": 10031}
{"docstring": "# load file\n", "func_signal": "def set_module_names(self):\n", "code": "modules_filepath = os.path.join(SUBLIMERL.plugin_path, \"completion\", \"Erlang-libs.sublime-completions\")\nf = open(modules_filepath, 'r')\ncontents = eval(f.read())\nf.close()\n# strip out just the module names to be displayed\nmodule_names = []\nfor t in contents['completions']:\n\tmodule_names.append(t['trigger'])\nself.module_names = module_names", "path": "sublimerl_man.py", "repo_name": "ostinelli/SublimErl", "stars": 190, "license": "None", "language": "python", "size": 10031}
{"docstring": "# return list of params, with proper variable name or wildcard if invalid\n\n# replace content of graffles with *\n", "func_signal": "def split_params(self, params):\n", "code": "params = self.regex['{'].sub(\"*\", params)\n# replace content of <<>> with *\nparams = self.regex['<<'].sub(\"*\", params)\n# replace content of [] with *\nparams = self.regex['['].sub(\"*\", params)\n# take away comments and split per line\nparams = self.get_code_list(params)\nfor p in range(0, len(params)):\n\t# split on =\n\tsplitted_param = params[p].split('=')\n\tif len(splitted_param) > 1:\n\t\tparams[p] = splitted_param[1].strip()\n\t# spit on :: for spec declarations\n\tparams[p] = params[p].split('::')[0]\n\t# convert to * where necessary\n\tif not self.regex['varname'].search(params[p]):\n\t\tparams[p] = '*'\n# return\nreturn params", "path": "support\\sublimerl_libparser.py", "repo_name": "ostinelli/SublimErl", "stars": 190, "license": "None", "language": "python", "size": 10031}
{"docstring": "# get outputs\n", "func_signal": "def interpret_test_results(self, retcode, data):\n", "code": "if re.search(r\"DONE.\", data):\n\t# test passed\n\tpassed_count = re.search(r\"(\\d+) ok, 0 failed(?:, 1 skipped)? of \\d+ test cases\", data).group(1)\n\tif int(passed_count) > 0:\n\t\tself.log(\"=> %s TEST(S) PASSED.\\n\" % passed_count)\n\telse:\n\t\tself.log(\"=> NO TESTS TO RUN.\\n\")\n\nelif re.search(r\"ERROR: One or more tests failed\", data):\n\tfailed_count = re.search(r\"\\d+ ok, (\\d+) failed(?:, 1 skipped)? of \\d+ test cases\", data).group(1)\n\tself.log(\"\\n=> %s TEST(S) FAILED.\\n\" % failed_count)\n\nelse:\n\tself.log(\"\\n=> TEST(S) FAILED.\\n\")\n\n# free test\nself.on_test_ended()", "path": "sublimerl_tests_integration.py", "repo_name": "ostinelli/SublimErl", "stars": 190, "license": "None", "language": "python", "size": 10031}
{"docstring": "# init\n", "func_signal": "def __init__(self, view):\n", "code": "self.view = view\nself.window = view.window()\nself.module_names = []\n\nself.panel_name = 'sublimerl_man'\nself.panel_buffer = ''\n# setup panel\nself.setup_panel()", "path": "sublimerl_man.py", "repo_name": "ostinelli/SublimErl", "stars": 190, "license": "None", "language": "python", "size": 10031}
{"docstring": "\"\"\"\nSave to filename a version of the image with all colors quantized\nto the nearest color in the given palette.\n\nParameters\n----------\npalette : rayleigh.Palette\n    Containing K colors.\nfilename : string\n    Where image will be written.\n\"\"\"\n", "func_signal": "def output_quantized_to_palette(self, palette, filename):\n", "code": "dist = euclidean_distances(\n    palette.lab_array, self.lab_array, squared=True).T\nmin_ind = np.argmin(dist, axis=1)\nquantized_lab_array = palette.lab_array[min_ind, :]\nimg = lab2rgb(quantized_lab_array.reshape((self.h, self.w, self.d)))\nimsave(filename, img)", "path": "rayleigh\\image.py", "repo_name": "sergeyk/rayleigh", "stars": 220, "license": "mit", "language": "python", "size": 88559}
{"docstring": "# Create a palette and save it to file\n", "func_signal": "def test_histogram(self):\n", "code": "num_hues = 7\nsat_range = light_range = 2\npalette = rayleigh.Palette(num_hues, sat_range, light_range)\ndirname = rayleigh.util.makedirs(os.path.join(temp_dirname, 'image'))\npalette.output(dirname)\npalette_filename = os.path.join(dirname, 'palette.png')\nassert(os.path.exists(palette_filename))\n\nimg = rayleigh.Image(palette_filename)\n\n# Output unsmoothed histogram\nimg.histogram_colors(palette, palette_filename + '_hist.png')\n\n# Test smoothed histogram\nsigma = 20\nfname = palette_filename + '_hist_direct_sigma_{}.png'.format(sigma)\nimg.histogram_colors_smoothed(palette, sigma, fname, direct=True)\nfname = palette_filename + '_hist_sigma_{}.png'.format(sigma)\nimg.histogram_colors_smoothed(palette, sigma, fname, direct=False)\n\nfname = palette_filename + '_quant.png'\nimg.output_quantized_to_palette(palette, fname)\nassert(os.path.exists(fname))", "path": "test\\image.py", "repo_name": "sergeyk/rayleigh", "stars": 220, "license": "mit", "language": "python", "size": 88559}
{"docstring": "\"\"\"\nLoad the data in given filename and return the first num_images urls\n(or all of them if num_images exceeds the total number).\n\nParameters\n----------\ndata_filename : string\n    JSON file that will contain the dataset.\nnum_images : int\n\nReturns\n-------\nids : list\n    the Flickr IDs of the photos\nurls : list\n    URLs of the photos\n\"\"\"\n", "func_signal": "def ids_and_urls_from_dataset(data_filename, num_images):\n", "code": "with gzip.open(data_filename) as f:\n    urls_by_date = json.load(f)\nurls = reduce(operator.add, [urls for urls in urls_by_date.values()])\n\ndef get_id(url):\n    _id = re.search('flickr.com/\\d+/(.+)_.+_.+.jpg', url).groups()[0]\n    return 'flickr_{}'.format(_id)\nids = [get_id(url) for url in urls]\nassert(len(set(ids)) == len(ids))\nreturn ids[:num_images], urls[:num_images]", "path": "rayleigh\\assemble_flickr_dataset.py", "repo_name": "sergeyk/rayleigh", "stars": 220, "license": "mit", "language": "python", "size": 88559}
{"docstring": "# Saving the flann object results in memory errors, so we use its own\n# method to save its index in a separate file.\n", "func_signal": "def load(filename):\n", "code": "sic = cPickle.load(open(filename))\nreturn sic.build_index(filename + '_flann_index')", "path": "rayleigh\\searchable_collection.py", "repo_name": "sergeyk/rayleigh", "stars": 220, "license": "mit", "language": "python", "size": 88559}
{"docstring": "# see http://www.flickr.com/services/api/misc.urls.html for size options\n", "func_signal": "def get_url(photo):\n", "code": "size = 'm'  # 240px on the longest side\nurl = \"http://farm{farm}.staticflickr.com/{server}/{id}_{secret}_{size}.jpg\"\nreturn url.format(size=size, **photo)", "path": "rayleigh\\assemble_flickr_dataset.py", "repo_name": "sergeyk/rayleigh", "stars": 220, "license": "mit", "language": "python", "size": 88559}
{"docstring": "\"\"\"\nCompute and store PCA dimensionality-reduced histograms.\n\"\"\"\n", "func_signal": "def reduce_dimensionality(self):\n", "code": "tt.tic('reduce_dimensionality')\nself.pca = PCA(n_components=self.num_dimensions, whiten=True)\nself.pca.fit(self.hists_reduced)\nself.hists_reduced = self.pca.transform(self.hists_reduced)\ntt.toc('reduce_dimensionality')", "path": "rayleigh\\searchable_collection.py", "repo_name": "sergeyk/rayleigh", "stars": 220, "license": "mit", "language": "python", "size": 88559}
{"docstring": "\"\"\"\nSearch images in database by color similarity to the given histogram.\n\nParameters\n----------\ncolor_hist : (K,) ndarray\n    histogram over the color palette\nnum : int, optional\n    number of nearest neighbors to ret\nreduced : boolean, optional\n    is the given color_hist already reduced in dimensionality?\n\nReturns\n-------\nquery_img : dict\n    info about the query image\nresults : list\n    list of dicts of nearest neighbors to query\n\"\"\"\n", "func_signal": "def search_by_color_hist(self, color_hist, num=20, reduced=False):\n", "code": "if self.num_dimensions > 0 and not reduced:\n    color_hist = self.pca.transform(color_hist)\ntt.tic('nn_ind')\nnn_ind, nn_dists = self.nn_ind(color_hist, num)\ntime_elapsed = tt.qtoc('nn_ind')\nresults = []\n# TODO: tone up the amount of data returned: don't need resized size,\n# _id, maybe something else?\nfor ind, dist in zip(nn_ind, nn_dists):\n    img_id = self.id_ind_map[ind]\n    img = self.ic.get_image(img_id, no_hist=True)\n    img['url'] = cgi.escape(img['url'])\n    img['distance'] = dist\n    results.append(img)\nreturn results, time_elapsed", "path": "rayleigh\\searchable_collection.py", "repo_name": "sergeyk/rayleigh", "stars": 220, "license": "mit", "language": "python", "size": 88559}
{"docstring": "\"\"\"\nReturn the smoothed image histogram of the image with the given id.\n\nParameters\n----------\nimg_id : string\n\nReturns\n-------\ncolor_hist : ndarray\n\"\"\"\n", "func_signal": "def get_image_hist(self, img_id):\n", "code": "img_ind = self.id_ind_map[img_id]\ncolor_hist = self.hists_reduced[img_ind, :]\nreturn color_hist", "path": "rayleigh\\searchable_collection.py", "repo_name": "sergeyk/rayleigh", "stars": 220, "license": "mit", "language": "python", "size": 88559}
{"docstring": "\"\"\"\nSmooth histograms with a Gaussian.\n\"\"\"\n", "func_signal": "def smooth_histograms(self):\n", "code": "for i in range(self.hists_reduced.shape[0]):\n    color_hist = self.hists_reduced[i, :]\n    self.hists_reduced[i, :] = util.smooth_histogram(\n        color_hist, self.ic.palette, self.sigma)", "path": "rayleigh\\searchable_collection.py", "repo_name": "sergeyk/rayleigh", "stars": 220, "license": "mit", "language": "python", "size": 88559}
{"docstring": "\"\"\"\nSave a solid color image of the given hex color to the given directory.\n\"\"\"\n# omit the # sign in the filename\n", "func_signal": "def save_synthetic_image(color, dirname, size=100):\n", "code": "filename = os.path.join(dirname, color[1:] + '.png')\ncmd = \"convert -size {size}x{size} 'xc:{color}' '{filename}'\"\nos.system(cmd.format(**locals()))\nreturn filename", "path": "test\\context.py", "repo_name": "sergeyk/rayleigh", "stars": 220, "license": "mit", "language": "python", "size": 88559}
{"docstring": "\"\"\"\nSearch images in database for similarity to the image with img_id in\nthe database.\n\nSee search_by_color_hist() for implementation.\n\nParameters\n----------\nimg_id : string\nnum : int, optional\n\nReturns\n-------\nquery_img_data : dict\nresults : list\n    list of dicts of nearest neighbors to query\n\"\"\"\n", "func_signal": "def search_by_image_in_dataset(self, img_id, num=20):\n", "code": "query_img_data = self.ic.get_image(img_id, no_hist=True)\ncolor_hist = self.get_image_hist(img_id)\nresults, time_elapsed = self.search_by_color_hist(color_hist, num, reduced=True)\nreturn query_img_data, results, time_elapsed", "path": "rayleigh\\searchable_collection.py", "repo_name": "sergeyk/rayleigh", "stars": 220, "license": "mit", "language": "python", "size": 88559}
{"docstring": "# set up jinja template\n", "func_signal": "def test_synthetic_search(self):\n", "code": "from jinja2 import Environment, FileSystemLoader\nenv = Environment(loader=FileSystemLoader(support_dirname))\ntemplate = env.get_template('matches.html')\n\n# create a collection and output nearest matches to every color\nic = rayleigh.ImageCollection(self.palette)\nic.add_images(self.filenames)\nsic = rayleigh.SearchableImageCollectionExact(ic, 'euclidean', 0)\n\n# search several query images and output to html summary\nmatches_filename = os.path.join(self.dirname, 'matches.html')\ndata = (sic.search_by_image(fname) for fname in self.filenames)\n# data is a list of (query_img, results) tuples\nwith open(matches_filename, 'w') as f:\n    f.write(template.render(data=data))", "path": "test\\collection.py", "repo_name": "sergeyk/rayleigh", "stars": 220, "license": "mit", "language": "python", "size": 88559}
{"docstring": "# save palette histograms and quantized versions\n", "func_signal": "def test_synthetic_creation(self):\n", "code": "sigma = 10\nn_samples = len(self.filenames) / 3\ns_filenames = shuffle(self.filenames, random_state=0, n_samples=n_samples)\nfor filename in s_filenames:\n    img = rayleigh.Image(filename)\n\n    fname = filename + '_hist_sigma_{}.png'.format(sigma)\n    img.histogram_colors_smoothed(self.palette, sigma, fname, direct=False)\n\n    q_filename = filename + '_quant.png'\n    img.output_quantized_to_palette(self.palette, q_filename)", "path": "test\\collection.py", "repo_name": "sergeyk/rayleigh", "stars": 220, "license": "mit", "language": "python", "size": 88559}
{"docstring": "\"\"\"\nLoad subset of the Flickr interestingness dataset that can be assembled\nwith the rayleigh.assemble_flickr_dataset module.\n\"\"\"\n# Parametrization of our test.\n", "func_signal": "def test_flickr(self):\n", "code": "image_list_name = 'flickr_100K'\nnum_images = int(1e5)\n\ndirname = rayleigh.util.makedirs(os.path.join(temp_dirname, image_list_name))\nnum_queries = 50\npalette = rayleigh.Palette(num_hues=10, light_range=3, sat_range=2)\npalette.output(dirname=dirname)\n\n# Set up jinja template.\nfrom jinja2 import Environment, FileSystemLoader\nenv = Environment(loader=FileSystemLoader(support_dirname))\ntemplate = env.get_template('matches.html')\n\n# Load the image collection.\ndata_filename = os.path.join(repo_dirname, 'data', 'flickr_1M.json.gz')\nids, urls = rayleigh.ids_and_urls_from_dataset(data_filename, num_images)\n\nic_filename = os.path.join(\n    temp_dirname, '{}.pickle'.format(image_list_name))\n\nif os.path.exists(ic_filename):\n    print(\"Loading ImageCollection from cache.\")\n    ic = rayleigh.ImageCollection.load(ic_filename)\nelse:\n    ic = rayleigh.ImageCollection(palette)\n    ic.add_images(urls, ids)\n    ic.save(ic_filename)\n\n# Make several searchable collections.\ndef create_or_load_sic(algorithm, distance_metric, sigma, num_dimensions):\n    if algorithm == 'exact':\n        sic_class = SearchableImageCollectionExact\n    elif algorithm == 'flann':\n        sic_class = SearchableImageCollectionFLANN\n    elif algorithm == 'ckdtree':\n        sic_class = SearchableImageCollectionCKDTree\n    else:\n        raise Exception(\"Unknown algorithm.\")\n\n    filename = os.path.join(dirname, '{}_{}_{}_{}_{}.pickle'.format(\n        image_list_name, algorithm, distance_metric, sigma, num_dimensions))\n\n    if os.path.exists(filename):\n        sic = sic_class.load(filename)\n    else:\n        sic = sic_class(ic, distance_metric, sigma, num_dimensions)\n        sic.save(filename)\n\n    return sic\n\n# search several query images and output to html summary\nnp.random.seed(0)\nimage_inds = np.random.permutation(range(len(urls)))\nimage_inds = image_inds[:num_queries]\n\n# there are 88 dimensions in our palette.\nmodes = [\n    # ('exact', 'euclidean', 8, 22),\n    # ('exact', 'euclidean', 8, 0),\n    # ('exact', 'euclidean', 16, 0),\n\n    # ('exact', 'manhattan', 8, 22),\n    # ('exact', 'manhattan', 8, 0),\n    # ('exact', 'manhattan', 16, 0),\n\n    # ('exact', 'chi_square', 8, 0),\n    # ('exact', 'chi_square', 16, 0),\n\n    #('flann', 'euclidean', 16, 22),\n    ('flann', 'euclidean', 16, 0),\n\n    #('flann', 'manhattan', 16, 22),\n    ('flann', 'manhattan', 16, 0),\n\n    ('flann', 'chi_square', 16, 0),\n    \n    #('ckdtree', 'euclidean', 8, 22),\n    #('ckdtree', 'manhattan', 8, 22)\n]\n\ntime_elapsed = {}\nfor mode in modes:\n    sic = create_or_load_sic(*mode)\n    tt.tic(mode)\n    data = [sic.search_by_image_in_dataset(ids[ind]) for ind in image_inds]\n    time_elapsed[mode] = tt.qtoc(mode)\n    print(\"Time elapsed for %s: %.3f s\" % (mode, time_elapsed[mode]))\n\n    filename = os.path.join(\n        dirname, 'matches_{}_{}_{}_{}.html'.format(*mode))\n    with open(filename, 'w') as f:\n        f.write(template.render(\n            num_queries=num_queries, time_elapsed=time_elapsed[mode],\n            data=data))", "path": "test\\collection.py", "repo_name": "sergeyk/rayleigh", "stars": 220, "license": "mit", "language": "python", "size": 88559}
{"docstring": "\"\"\"\nOutput an image of the palette, josn list of the hex\ncolors, and an HTML color picker for it.\n\nParameters\n----------\ndirname : string\n    directory for the files to be output\n\"\"\"\n", "func_signal": "def output(self, dirname):\n", "code": "def get_palette_html():\n    \"\"\"\n    Return HTML for a color picker using the given palette.\n    \"\"\"\n    html = \"\"\"\n    <style>\n        span {\n            width: 20px;\n            height: 20px;\n            margin: 2px;\n            padding: 0px;\n            display: inline-block;\n        }\n    </style>\n    \"\"\"\n    for row in self.rgb_image:\n        for rgb_color in row:\n            s = '<a id=\"{0}\"><span style=\"background-color: {0}\" /></a>\\n'\n            html += s.format(rgb2hex(rgb_color))\n        html += \"<br />\\n\"\n    return html\n\nimsave(os.path.join(dirname, 'palette.png'), self.rgb_image)\nwith open(os.path.join(dirname, 'palette.html'), 'w') as f:\n    f.write(get_palette_html())", "path": "rayleigh\\palette.py", "repo_name": "sergeyk/rayleigh", "stars": 220, "license": "mit", "language": "python", "size": 88559}
{"docstring": "# See comment in load().\n", "func_signal": "def save(self, filename):\n", "code": "flann = self.flann\nself.flann = None\ncPickle.dump(self, open(filename, 'w'), 2)\nflann.save_index(filename + '_flann_index')\nself.flann = flann", "path": "rayleigh\\searchable_collection.py", "repo_name": "sergeyk/rayleigh", "stars": 220, "license": "mit", "language": "python", "size": 88559}
{"docstring": "\"\"\"\nExact nearest neighbor seach through exhaustive comparison.\n\"\"\"\n", "func_signal": "def nn_ind(self, color_hist, num):\n", "code": "if self.distance_metric == 'manhattan':\n    dists = manhattan_distances(color_hist, self.hists_reduced)\nelif self.distance_metric == 'euclidean':\n    dists = euclidean_distances(color_hist, self.hists_reduced, squared=True)\nelif self.distance_metric == 'chi_square':\n    dists = -additive_chi2_kernel(color_hist, self.hists_reduced)\n\ndists = dists.flatten()\nnn_ind = np.argsort(dists).flatten()[:num]\nnn_dists = dists[nn_ind]\n\nreturn nn_ind, nn_dists", "path": "rayleigh\\searchable_collection.py", "repo_name": "sergeyk/rayleigh", "stars": 220, "license": "mit", "language": "python", "size": 88559}
{"docstring": "\"\"\"\nAssemble dataset containing the specified number of images using Flickr\n'interestingness' API calls.\nReturns nothing; writes data to file.\n\nParameters\n----------\napi_filename : string\n    File should contain only one line, with your Flickr API key.\ndata_filename : string\n    Gzipped JSON file that will contain the dataset.\n    If it already exists, we will load the data in it and not repeat\n    the work done.\nnum_images_to_load : int\n\"\"\"\n", "func_signal": "def assemble_flickr_dataset(api_filename, data_filename, num_images_to_load):\n", "code": "with open(api_filename) as f:\n    api_key = f.readline().strip()\n\ntry:\n    with gzip.open(data_filename) as f:\n        urls_by_date = json.load(f)\nexcept:\n    urls_by_date = {}\n\n# Begin from yesterday and go backwards\none_day = datetime.timedelta(days=1)\ndate = datetime.date.today() - one_day\n\nprint(\"Loading {} images\".format(num_images_to_load))\nnum_loaded = sum((len(urls) for date, urls in urls_by_date.iteritems()))\nwhile num_loaded < num_images_to_load:\n    if str(date) in urls_by_date:\n        date -= one_day\n        continue\n    \n    data = get_photos_list(api_key, date)\n    urls = [get_url(photo) for photo in data['photos']['photo']]\n    urls_by_date[str(date)] = urls\n    \n    num_loaded = sum((len(urls) for date, urls in urls_by_date.iteritems()))\n    print(\"Finished {0} with {1} images loaded\".format(date, num_loaded))\n    date -= one_day\nwith gzip.open(data_filename, 'wb') as f:\n    json.dump(urls_by_date, f)", "path": "rayleigh\\assemble_flickr_dataset.py", "repo_name": "sergeyk/rayleigh", "stars": 220, "license": "mit", "language": "python", "size": 88559}
{"docstring": "\"\"\"\nSearch images in database by color similarity to image.\n\nSee search_by_color_hist().\n\"\"\"\n", "func_signal": "def search_by_image(self, image_filename, num=20):\n", "code": "query_img = Image(image_filename)\ncolor_hist = util.histogram_colors_smoothed(\n    query_img.lab_array, self.ic.palette,\n    sigma=self.sigma, direct=False)\nresults, time_elapsed = self.search_by_color_hist(color_hist)\nreturn query_img.as_dict(), results, time_elapsed", "path": "rayleigh\\searchable_collection.py", "repo_name": "sergeyk/rayleigh", "stars": 220, "license": "mit", "language": "python", "size": 88559}
{"docstring": "# TODO: output a single HTML file instead of using the file browser to\n# look at all these files\n", "func_signal": "def test_flickr(self):\n", "code": "dirname = rayleigh.util.makedirs(os.path.join(temp_dirname, 'image_flickr'))\npalette = rayleigh.Palette(num_hues=10, light_range=3, sat_range=2)\npalette.output(dirname)\n\nimage_filenames = glob(os.path.join(support_dirname, 'images', '*'))\nsigmas = [10]\nfor ind, img_fname in enumerate(image_filenames):\n    for sigma in sigmas:\n        output_fname = os.path.join(dirname, str(ind))\n        shutil.copy(img_fname, output_fname + '.jpg')\n        img = rayleigh.Image(img_fname)\n\n        img.histogram_colors(palette, output_fname + '_hist.png')\n        \n        fname = output_fname + '_hist_direct_sigma_{}.png'.format(sigma)\n        img.histogram_colors_smoothed(palette, sigma, fname, direct=True)\n\n        fname = output_fname + '_hist_sigma_{}.png'.format(sigma)\n        img.histogram_colors_smoothed(palette, sigma, fname, direct=False)\n        \n        img.output_quantized_to_palette(\n            palette, output_fname + '_quantized.png')\n\n        img.output_color_palette_image(\n            palette, output_fname + '_my_palette.png')", "path": "test\\image.py", "repo_name": "sergeyk/rayleigh", "stars": 220, "license": "mit", "language": "python", "size": 88559}
{"docstring": "'''Get all edge names in the graph, include edges in all subgraph.'''\n", "func_signal": "def EG_get_all_edge_names(self, root_graph=None):\n", "code": "if root_graph is None:\n    root_graph = self\n\nedges = root_graph.get_edges()\nresult = [ ( remove_double_quote( e.get_source() ), \\\n             remove_double_quote( e.get_destination() ) ) \\\n          for e in edges ]\n\nsgs = root_graph.get_subgraphs()\nfor sg in sgs:\n    result += self.EG_get_all_edge_names(sg)\n\nreturn list(set(result))", "path": "ExtGraph.py", "repo_name": "vincenthEE/DotEditor", "stars": 221, "license": "None", "language": "python", "size": 1279}
{"docstring": "'''Change the zoom of preview panel.'''\n\n", "func_signal": "def changeZoom(self, zoom_ratio):\n", "code": "if zoom_ratio == self.bitmap_zoom_ratio:\n    return True\n\nmin_zoom = self.__calc_best_zoom_ratio()\n\nif zoom_ratio > self.MAX_ZOOM : zoom_ratio = self.MAX_ZOOM\nif zoom_ratio < min_zoom: zoom_ratio = min_zoom\n\nself.bitmap_zoom_ratio = zoom_ratio\nself.m_staticText_zoom.SetLabel('Zoom:%3d%%'%(zoom_ratio*100.0))\n\nimg = self.data_graph.get_bitmap()\nself.m_panel_paint.SetVirtualSize((img.GetWidth()*self.bitmap_zoom_ratio, \\\n                                   img.GetHeight()*self.bitmap_zoom_ratio))\n\nself.m_panel_paint.SetScrollRate(20,20)\nself.m_panel_paint.Refresh()\n\nreturn", "path": "DotEditor.py", "repo_name": "vincenthEE/DotEditor", "stars": 221, "license": "None", "language": "python", "size": 1279}
{"docstring": "'''End the drag motion.'''\n\n", "func_signal": "def onLeftButtonUp(self, event):\n", "code": "self.is_dragging = False\n\ncursor = wx.StockCursor(wx.CURSOR_DEFAULT)\nself.SetCursor(cursor)", "path": "DotEditor.py", "repo_name": "vincenthEE/DotEditor", "stars": 221, "license": "None", "language": "python", "size": 1279}
{"docstring": "'''Refresh comboxB, filled it with suggested node name, based on the value of comboxA.'''\n", "func_signal": "def __refresh_comboBoxB_suggestion(self):\n", "code": "if self.m_radioBox_type.GetSelection() != 1:\n    self.m_comboBox_nodeB.Clear()\n    return\n    \nnodes = self.data_graph.EG_get_all_node_names()\nedges = self.data_graph.EG_get_all_edge_names()\nendpoints = set()\nfor s, d in edges:\n    endpoints.add(s); endpoints.add(d)\n\nnA = self.m_comboBox_nodeA.GetValue().strip().encode('utf8')\n\n# Get all node's name.\ns_n = set(nodes).union(endpoints)  \ninvalid_points = set()\nfor s,d in edges:\n    if s == nA:\n        invalid_points.add(d)\n             \ns_n = s_n - invalid_points \ns_n = list(s_n); s_n.sort()\n\n# Save the value in old_v.\nold_v = self.m_comboBox_nodeB.GetValue()\nself.m_comboBox_nodeB.Clear()\nfor n in s_n:\n    self.m_comboBox_nodeB.Append(n.decode('utf8'))\n\n# Restore the nodeA value.\nself.m_comboBox_nodeB.SetValue(old_v)\n\nreturn", "path": "DotEditor.py", "repo_name": "vincenthEE/DotEditor", "stars": 221, "license": "None", "language": "python", "size": 1279}
{"docstring": "'''Check if wildcard nodes existed in root_graph and all subgraph.'''\n", "func_signal": "def __check_wildcard_existed(self, root_graph=None):\n", "code": "if root_graph is None:\n    root_graph = self\n\nfor n_name in ['node', 'edge']:\n    node = self.EG_get_node_by_name(n_name, root_graph)\n    if node is None:\n        n = pydot.Node(n_name)\n        n.set_comment('Wildcard node added automatic in EG.')\n        root_graph.add_node(n)\n        \n    ### Anyway, push the wildcard node to the front of all other nodes.\n    if n_name == 'node':\n        root_graph.obj_dict['nodes']['node'][0]['sequence'] = 0.5\n    else:\n        root_graph.obj_dict['nodes']['edge'][0]['sequence'] = 0\n    ### -------------------------------------------------------------\n\nsgs  = root_graph.get_subgraphs()\nfor sg in sgs:\n    self.__check_wildcard_existed(sg)\n\nreturn", "path": "ExtGraph.py", "repo_name": "vincenthEE/DotEditor", "stars": 221, "license": "None", "language": "python", "size": 1279}
{"docstring": "'''Refill comboxB when value of combox changed.'''\n", "func_signal": "def onNodeAChanged(self, event):\n", "code": "checked = self.m_radioBox_type.GetSelection() \nif checked == 1:\n    self.__refresh_comboBoxB_suggestion()", "path": "DotEditor.py", "repo_name": "vincenthEE/DotEditor", "stars": 221, "license": "None", "language": "python", "size": 1279}
{"docstring": "'''Start the drag motion.'''\n", "func_signal": "def onLeftButtonDown(self, event):\n", "code": "cursor = wx.StockCursor(wx.CURSOR_SIZING)\nself.SetCursor(cursor)\n\nself.CurrentCursorPos = event.GetPosition()\n\nself.is_dragging = True\n\nself.LastClickPos = self.CurrentCursorPos\n \nevent.Skip()", "path": "DotEditor.py", "repo_name": "vincenthEE/DotEditor", "stars": 221, "license": "None", "language": "python", "size": 1279}
{"docstring": "'''Get node by name. Return None if not found.'''\n\n", "func_signal": "def EG_get_node_by_name(self, name, root_graph=None):\n", "code": "if root_graph is None:\n    root_graph = self\n\nn_name = remove_double_quote(name)\n\nnodes = root_graph.get_nodes()\nr = None\nfor n in nodes:\n    _name = remove_double_quote( n.get_name() )\n    if _name == n_name:\n        r = n\n        break\n\nreturn r", "path": "ExtGraph.py", "repo_name": "vincenthEE/DotEditor", "stars": 221, "license": "None", "language": "python", "size": 1279}
{"docstring": "'''Get all node names in the graph, include nodes in all subgraph.'''\n", "func_signal": "def EG_get_all_node_names(self, root_graph=None):\n", "code": "if root_graph is None:\n    root_graph = self\n\nnodes = root_graph.get_nodes()\nresult = [ remove_double_quote(n.get_name()) for n in nodes ]\n\nsgs = root_graph.get_subgraphs()\nfor sg in sgs:\n    result += self.EG_get_all_node_names(sg)\n\n### Remove wildcard node.\ntry:\n    result.remove('node'); result.remove('edge')\nexcept:\n    pass\n\nreturn list(set(result))", "path": "ExtGraph.py", "repo_name": "vincenthEE/DotEditor", "stars": 221, "license": "None", "language": "python", "size": 1279}
{"docstring": "'''Update data graph and then refresh the whole UI. IF graph == None, just refresh the preview.'''\n\n", "func_signal": "def update_graph(self, graph):\n", "code": "try:\n    del self.data_graph\nexcept:\n    pass\n\nself.data_graph = ExtGraph.ExtGraph(obj_dict=graph.obj_dict)\n\n### Refresh image panel.\nself.data_graph.refresh_bitmap()\nimg = self.data_graph.get_bitmap()\n\nself.m_panel_paint.SetVirtualSize((img.GetWidth(), \\\n                                   img.GetHeight()))\nself.m_panel_paint.SetScrollRate(20,20)\nself.m_panel_paint.Refresh()\n\nreturn", "path": "DotEditor.py", "repo_name": "vincenthEE/DotEditor", "stars": 221, "license": "None", "language": "python", "size": 1279}
{"docstring": "'''Capture the motion of \"Ctrl+wheel\" to zoom.'''\n\n", "func_signal": "def onMouseZoom(self, event):\n", "code": "if event.ControlDown():\n    \n    wd = event.GetWheelRotation()\n    zoom_ratio = self.bitmap_zoom_ratio\n    if wd > 0:\n        cursor = wx.StockCursor(wx.CURSOR_MAGNIFIER)\n        self.m_panel_paint.SetCursor(cursor)\n        zoom_ratio = self.bitmap_zoom_ratio + 0.05\n    elif wd < 0:\n        cursor = wx.StockCursor(wx.CURSOR_MAGNIFIER)\n        self.m_panel_paint.SetCursor(cursor)\n        zoom_ratio = self.bitmap_zoom_ratio - 0.05\n        \n    self.changeZoom(zoom_ratio)\n    cursor = wx.StockCursor(wx.CURSOR_DEFAULT)\n    self.m_panel_paint.SetCursor(cursor)\n\nreturn", "path": "DotEditor.py", "repo_name": "vincenthEE/DotEditor", "stars": 221, "license": "None", "language": "python", "size": 1279}
{"docstring": "\"\"\"Returns a string representation of the graph in dot language.\nThis version try to make string looking better than to_string().\n\"\"\"\n", "func_signal": "def EG_to_string(self, indent=0, root_graph=None):\n", "code": "idt =  ' '*(4 + indent)\ngraph = list()\n\nif root_graph is None:\n    root_graph = self\n\nif root_graph != root_graph.get_parent_graph():\n    graph.append(' '*indent)\n\nif root_graph.obj_dict.get('strict', None) is not None:\n    if root_graph==root_graph.get_parent_graph() and root_graph.obj_dict['strict']:\n        graph.append('strict ')\n\nif root_graph.obj_dict['name'] == '':\n    graph.append( '{\\n' )\nelse:\n    graph.append( '%s %s {\\n' % (root_graph.obj_dict['type'], root_graph.obj_dict['name']) )\n\nfor attr in root_graph.obj_dict['attributes'].keys():\n    if root_graph.obj_dict['attributes'].get(attr, None) is not None:\n        graph.append( idt+'%s=' % attr )\n        val = root_graph.obj_dict['attributes'].get(attr)\n        graph.append( pydot.quote_if_necessary(val) )\n        graph.append( ';\\n' )\n\n\nedges_done = set()\n\nedge_obj_dicts = list()\nfor e in root_graph.obj_dict['edges'].values():\n    edge_obj_dicts.extend(e)\n    \nif edge_obj_dicts:\n    edge_src_set, edge_dst_set = zip( *[obj['points'] for obj in edge_obj_dicts] )\n    edge_src_set, edge_dst_set = set(edge_src_set), set(edge_dst_set)\nelse:\n    edge_src_set, edge_dst_set = set(), set()\n    \nnode_obj_dicts = list()\nfor e in root_graph.obj_dict['nodes'].values():\n    node_obj_dicts.extend(e)\n\nsgraph_obj_dicts = list()\nfor sg in root_graph.obj_dict['subgraphs'].values():\n    sgraph_obj_dicts.extend(sg)\n\nobj_list = [ (obj['sequence'], obj) for obj in (edge_obj_dicts + node_obj_dicts + sgraph_obj_dicts) ]\nobj_list.sort()\n\nfor _, obj in obj_list:\n    if obj['type'] == 'node':\n        node = pydot.Node(obj_dict=obj)\n        if root_graph.obj_dict.get('suppress_disconnected', False):\n            if (node.get_name() not in edge_src_set and\n                node.get_name() not in edge_dst_set):\n                continue\n            \n        graph.append( DEUtils.smart_indent(node.to_string(), idt) + '\\n' )\n\n    elif obj['type'] == 'edge':\n        edge = pydot.Edge(obj_dict=obj)\n        if root_graph.obj_dict.get('simplify', False) and edge in edges_done:\n            continue\n        \n        graph.append( DEUtils.smart_indent(edge.to_string(), idt) + '\\n' )\n        \n        edges_done.add(edge)\n        \n    else:\n        sgraph = pydot.Subgraph(obj_dict=obj)\n        sg_str = self.EG_to_string(indent+4, sgraph)\n        graph.append( sg_str+'\\n' )\n\nif root_graph != root_graph.get_parent_graph():\n    graph.append(' '*indent)\n    \ngraph.append( '}\\n' )\n\nreturn ''.join(graph)", "path": "ExtGraph.py", "repo_name": "vincenthEE/DotEditor", "stars": 221, "license": "None", "language": "python", "size": 1279}
{"docstring": "'''Capture the \"drag\" motion in preview window.'''\n", "func_signal": "def onMouseMove(self, event):\n", "code": "self.CurrentCursorPos = event.GetPosition()\n\nif self.is_dragging:\n    \n    p = self.m_panel_paint.GetScrollPixelsPerUnit()\n\n    diff = [0.0, 0.0]\n    delta_x = self.CurrentCursorPos[0] - self.LastClickPos[0]\n    delta_y = self.CurrentCursorPos[1] - self.LastClickPos[1]\n    \n    # Why sign*abs/p? Cause the '/' operator is different to positive and nagative num.\n    sign = lambda x:math.copysign(1, x)\n    \n    diff[0] = sign(delta_x)*(abs(delta_x) / p[0])\n    diff[1] = sign(delta_y)*(abs(delta_y) / p[1])\n\n    start_pos = self.m_panel_paint.GetViewStart()\n    \n    x = start_pos[0] - diff[0]\n    y = start_pos[1] - diff[1]\n\n    self.m_panel_paint.Scroll(x, y)\n\n    ### Record the old position. If some direction no enough step to \n    ### scroll, leave it alone.\n    if diff[0] != 0:\n        if diff[1] != 0:\n            self.LastClickPos = self.CurrentCursorPos\n        else:\n            self.LastClickPos[0] = self.CurrentCursorPos[0]\n    else:\n        if diff[1] != 0:\n            self.LastClickPos[1] = self.CurrentCursorPos[1]\n        else:\n            pass\n    \n    self.m_panel_paint.Refresh()", "path": "DotEditor.py", "repo_name": "vincenthEE/DotEditor", "stars": 221, "license": "None", "language": "python", "size": 1279}
{"docstring": "'''Updata data graph and then refresh the whole UI. IF graph == None, just refresh the preview.'''\n\n", "func_signal": "def update_graph(self, graph=None):\n", "code": "if not (graph is None):\n    \n    self.data_graph = ExtGraph.ExtGraph(obj_dict=graph.obj_dict)\n    \n    self.m_tree.DeleteAllItems()\n    self.m_pgManager1.Clear()\n            \n    ### Insert wildcard item at 1st.\n    g_name = remove_double_quote(self.data_graph.get_name().decode('utf8'))\n    root = self.m_tree.AddRoot(g_name, 1)\n    self.m_tree.SetItemPyData(root, ('graph', self.data_graph))\n    self.m_tree.SetItemImage(root, self.img_dict[('graph', 'color')])\n    \n    self.__spread_tree(root)\n    \n    self.m_tree.ExpandAll()\n    self.m_tree.SelectItem(root)\n    self.onItemSelected(None)\n\n    # Refresh image panel.\n    self.data_graph.refresh_bitmap()\n\n    # Set zoom ratio.\n    self.bitmap_zoom_ratio = 1.0\n    self.m_staticText_zoom.SetLabel('Zoom:%3d%%'%100)\n\nelse:\n    \n    # Just refresh image panel.\n    self.data_graph.refresh_bitmap()\n\n# Set scroll.\nimg = self.data_graph.get_bitmap()\n\nvw = img.GetWidth()*self.bitmap_zoom_ratio\nvh = img.GetHeight()*self.bitmap_zoom_ratio\n\nself.m_panel_paint.SetVirtualSize((vw, vh))\nself.m_panel_paint.SetScrollRate(20,20)\n#self.m_panel_paint.SetScrollbars(1,1,vw, vh)\nself.m_panel_paint.Refresh()\n\n### Set window title.\ntitle = 'Dot Editor'\nif self.file_path is None:\n    if self.is_data_changed: title += ' - *'\nelse:\n    if self.is_data_changed:\n        title = 'Dot Editor - *'+self.file_path\n    else:\n        title = 'Dot Editor - '+self.file_path\n        \nself.SetTitle(title)\n\nreturn", "path": "DotEditor.py", "repo_name": "vincenthEE/DotEditor", "stars": 221, "license": "None", "language": "python", "size": 1279}
{"docstring": "'''Get the attribute infomation by name and g_type. g_type should in ['node', 'group', 'graph']. Return a dict include all info.'''\n\n", "func_signal": "def get_dot_attr(attr_name, g_type):\n", "code": "if g_type not in ['node', 'edge', 'graph']: raise Exception('g_type \"%s\" not valid'%g_type)\nAD = eval(g_type.upper()+'_ATTRS_DEFINE')\n\nresult = {}\nfor line in AD:\n    a_name = line[0]\n    if a_name == attr_name:\n          \n        result['name']              = line[0]\n        result['default_value']     = line[1]\n        result['category']          = line[2]\n        result['group']             = line[3]\n        result['type']              = line[4]\n        result['param']             = line[5]\n        result['description']       = line[6]\n        \nif len(result) == 0:\n    raise KeyError(\"Can't find the attr_name '%s' in '%s' attributes define.\"%(attr_name, g_type))\n\nreturn result", "path": "AttrsDef.py", "repo_name": "vincenthEE/DotEditor", "stars": 221, "license": "None", "language": "python", "size": 1279}
{"docstring": "'''\nPut attributs structure in a dict like:{'cate1':{None:[attr_name1, attr_name2, ...], \n                                                 'group1':[attr_name3, attr_name4], ...},\n                                        'cate2':{...},\n                                        ...}\nPay attention the 'None' group inclue all attr names not in any group.\nreturn value is tuple(category_list, group_list, attrs_structure_dict).\n'''\n\n", "func_signal": "def get_dot_attr_structure(g_type):\n", "code": "if g_type not in ['node', 'edge', 'graph']: raise Exception('g_type \"%s\" not valid'%g_type)\nAD = eval(g_type.upper()+'_ATTRS_DEFINE')\n\n### List all category and groups, and attrs in each group.\ncates = []; groups = []; cate_dict={}; group_dict = {}\nfor line in AD:\n    ### Build cates list.\n    c = line[2]\n    if c not in cates: cates.append(c)\n    \n    ### Init cate_dict key place.\n    if not cate_dict.has_key(c): cate_dict[c] = []\n    \n    ### Build groups list.\n    g = line[3]\n    if (not g is None) and (g not in groups): groups.append(g)\n    \n    ### Put g(if not None) into the FIRST category has the group name.\n    if not g is None:\n        is_g_in_any_category = False\n        for _cate in cate_dict:\n            if g in cate_dict[_cate]:\n                is_g_in_any_category = True\n                break\n        if not is_g_in_any_category:\n            cate_dict[c].append(g)\n\n    ### Build group_dict.\n    a_name = line[0]\n    # Create a group \"cate_name_None\" to store that attrs without group.\n    if g is None: g = c+\"_None\" \n    if not group_dict.has_key(g):\n        group_dict[g] = []\n    group_dict[g].append(a_name)\n\n### Build tree structure as result.\nresult = {}\nfor c in cates:\n    result[c] = {}\n    for g in cate_dict[c]:\n        result[c][g] = group_dict[g]\n    ### Don't forget the 'cate_name_None' group.\n    result[c][None] = group_dict[c+\"_None\"]\n    \nreturn cates, groups, result", "path": "AttrsDef.py", "repo_name": "vincenthEE/DotEditor", "stars": 221, "license": "None", "language": "python", "size": 1279}
{"docstring": "'''A  recursive function to add all data from graph into m_tree_ctrl.'''\n\n", "func_signal": "def __spread_tree(self, rootid):\n", "code": "_, graph = self.m_tree.GetItemPyData(rootid)\n\n### Add wildcard nodes.\nfor n in ['node', 'edge']:\n    n_id = self.m_tree.AppendItem(rootid, n)\n    n_data = self.data_graph.EG_get_node_by_name(n, root_graph=graph)\n    self.m_tree.SetItemPyData(n_id, ('node',  n_data))\n    self.m_tree.SetItemImage(n_id, self.img_dict[(n, 'gray')])\n\n### Load nodes.\nfor n in graph.get_nodes():\n    n_name = remove_double_quote( n.get_name() )\n    if n_name.lower() in ['node', 'edge']: ### Skip the wildcard node.\n        continue\n    \n    n_id = self.m_tree.AppendItem(rootid, n_name.decode('utf8'))\n    self.m_tree.SetItemPyData(n_id, ('node', n))\n    self.m_tree.SetItemImage(n_id, self.img_dict[('node', 'color')])\n    \n\n### Load edges.\nfor e in graph.get_edges():\n    n1 = remove_double_quote( e.get_source().decode('utf8') )\n    n2 = remove_double_quote( e.get_destination().decode('utf8') )\n    n_id = self.m_tree.AppendItem(rootid, \"%s -> %s\"%(n1,n2))\n    self.m_tree.SetItemPyData(n_id, ('edge', e))\n    self.m_tree.SetItemImage(n_id, self.img_dict[('edge', 'color')])\n\n### Load subgraphs.\nfor sg in graph.get_subgraphs():\n    sg_name = remove_double_quote( sg.get_name().decode('utf8') )\n    n_id = self.m_tree.AppendItem(rootid, sg_name)\n    self.m_tree.SetItemPyData(n_id, ('graph', sg))\n    self.m_tree.SetItemImage(n_id, self.img_dict[('graph', 'color')])\n    self.__spread_tree(n_id)\n\nreturn", "path": "DotEditor.py", "repo_name": "vincenthEE/DotEditor", "stars": 221, "license": "None", "language": "python", "size": 1279}
{"docstring": "'''Get edge by names of source and destination. Return None if not found.'''\n", "func_signal": "def EG_get_edge_by_names(self, name_pair, root_graph=None):\n", "code": "if root_graph is None:\n    root_graph = self\n\nnameA = remove_double_quote(name_pair[0])\nnameB = remove_double_quote(name_pair[1])\n\nedges = root_graph.get_edges()\nr = None\nfor e in edges:\n    n = remove_double_quote( e.get_source() )\n    n1 = remove_double_quote( e.get_destination() )\n    if ((n == nameA )  \n        and (n1 == nameB)):  \n        r = e\n        break\n    \nreturn r", "path": "ExtGraph.py", "repo_name": "vincenthEE/DotEditor", "stars": 221, "license": "None", "language": "python", "size": 1279}
{"docstring": "'''If \"append node\" checked, return node in string; else return nodes in tuple.'''\n", "func_signal": "def getAppendValue(self):\n", "code": "vCheck = self.m_radioBox_type.GetSelection()\nvA = remove_double_quote( self.m_comboBox_nodeA.GetValue().encode('utf8') )\nvB = remove_double_quote( self.m_comboBox_nodeB.GetValue().encode('utf8') )\n\nif vCheck == 0:\n    return 'node', escape_dot_string(vA)\nelif vCheck == 1:\n    return 'edge', (escape_dot_string(vA), escape_dot_string(vB))\nelse:\n    return 'subgraph', escape_dot_string(vA)", "path": "DotEditor.py", "repo_name": "vincenthEE/DotEditor", "stars": 221, "license": "None", "language": "python", "size": 1279}
{"docstring": "'''Refresh both combox when the value of append_type_checkbox changed.'''\n", "func_signal": "def onTypeChange(self, event):\n", "code": "checked = self.m_radioBox_type.GetSelection() \nif checked == 0: # Case Node.\n    self.m_comboBox_nodeB.Clear()\n    self.m_comboBox_nodeB.Disable()\n    self.__refresh_comboBoxA_suggestion()\n    self.__refresh_comboBoxB_suggestion()\nelif checked == 1: # Case Edge.\n    self.m_comboBox_nodeB.Enable()\n    self.__refresh_comboBoxA_suggestion()\n    self.__refresh_comboBoxB_suggestion()\nelse: # Case Subgraph\n    self.m_comboBox_nodeB.Disable()\n    \n    self.m_comboBox_nodeA.Clear()\n    self.m_comboBox_nodeA.SetValue('clusterN')\n    self.m_comboBox_nodeA.SetFocus()\n    self.m_comboBox_nodeA.SetMark(7, -1)", "path": "DotEditor.py", "repo_name": "vincenthEE/DotEditor", "stars": 221, "license": "None", "language": "python", "size": 1279}
{"docstring": "''' Should create a procedure called deleteme\n    that returns two result sets, first the \n\t    number of rows in booze then \"name from booze\"\n'''\n", "func_signal": "def help_nextset_setUp(self,cur):\n", "code": "sql=\"\"\"\n   create procedure deleteme()\n   begin\n       select count(*) from %(tp)sbooze;\n       select name from %(tp)sbooze;\n   end\n\"\"\" % dict(tp=self.table_prefix)\ncur.execute(sql)", "path": "tests\\test_MySQLdb_dbapi20.py", "repo_name": "farcepest/moist", "stars": 195, "license": "None", "language": "python", "size": 750}
{"docstring": "\"\"\"Fetches a single row from the cursor. None indicates that\nno more rows are available.\"\"\"\n", "func_signal": "def fetchone(self):\n", "code": "self._check_executed()\nif not self._result:\n    return None\nreturn self._result.fetchone()", "path": "MySQLdb\\cursors.py", "repo_name": "farcepest/moist", "stars": 195, "license": "None", "language": "python", "size": 750}
{"docstring": "\"\"\"Fetch up to size rows from the cursor. Result set may be smaller\nthan size. If size is not defined, cursor.arraysize is used.\"\"\"\n", "func_signal": "def fetchmany(self, size=None):\n", "code": "self._check_executed()\nif not self._result:\n    return []\nif size is None:\n    size = self.arraysize\nreturn self._result.fetchmany(size)", "path": "MySQLdb\\cursors.py", "repo_name": "farcepest/moist", "stars": 195, "license": "None", "language": "python", "size": 750}
{"docstring": "\"\"\"Advance to the next result set.\n\nReturns False if there are no more result sets.\n\"\"\"\n", "func_signal": "def nextset(self):\n", "code": "db = self._get_db()\nself._result.clear()\nself._result = None\nif self._pending_results:\n    self._result = self._pending_results[0]\n    del self._pending_results[0]\n    return True\nif db.next_result():\n    self._result = Result(self)\n    return True\nreturn False", "path": "MySQLdb\\cursors.py", "repo_name": "farcepest/moist", "stars": 195, "license": "None", "language": "python", "size": 750}
{"docstring": "\"\"\"Fetches all available rows from the cursor.\"\"\"\n", "func_signal": "def fetchall(self):\n", "code": "self._check_executed()\nif not self._result:\n    return []\nreturn self._result.fetchall()", "path": "MySQLdb\\cursors.py", "repo_name": "farcepest/moist", "stars": 195, "license": "None", "language": "python", "size": 750}
{"docstring": "# Character data\n", "func_signal": "def test_small_CHAR(self):\n", "code": "def generator(row,col):\n    i = (row*col+62)%256\n    if i == 62: return ''\n    if i == 63: return None\n    return chr(i)\nself.check_data_integrity(\n    ('col1 char(1)','col2 char(1)'),\n    generator)", "path": "tests\\test_MySQLdb_capabilities.py", "repo_name": "farcepest/moist", "stars": 195, "license": "None", "language": "python", "size": 750}
{"docstring": "\"\"\"Execute stored procedure procname with args\n\nprocname\n    string, name of procedure to execute on server\n\nargs\n    Sequence of parameters to use with procedure\n\nReturns the original args.\n\nCompatibility warning: PEP-249 specifies that any modified\nparameters must be returned. This is currently impossible\nas they are only available by storing them in a server\nvariable and then retrieved by a query. Since stored\nprocedures return zero or more result sets, there is no\nreliable way to get at OUT or INOUT parameters via callproc.\nThe server variables are named @_procname_n, where procname\nis the parameter above and n is the position of the parameter\n(from zero). Once all result sets generated by the procedure\nhave been fetched, you can issue a SELECT @_procname_0, ...\nquery using .execute() to get any OUT or INOUT values.\n\nCompatibility warning: The act of calling a stored procedure\nitself creates an empty result set. This appears after any\nresult sets generated by the procedure. This is non-standard\nbehavior with respect to the DB-API. Be sure to use nextset()\nto advance through all result sets; otherwise you may get\ndisconnected.\n\"\"\"\n\n", "func_signal": "def callproc(self, procname, args=()):\n", "code": "db = self._get_db()\ncharset = self.connection.character_set_name()\nfor index, arg in enumerate(args):\n    query = \"SET @_%s_%d=%s\" % (procname, index,\n                                self.connection.literal(arg))\n    if isinstance(query, unicode):\n        query = query.encode(charset)\n    self._query(query)\n    self.nextset()\n\nquery = \"CALL %s(%s)\" % (procname,\n                         ','.join(['@_%s_%d' % (procname, i)\n                                   for i in range(len(args))]))\nif isinstance(query, unicode):\n    query = query.encode(charset)\nself._query(query)\nif not self._defer_warnings:\n    self._warning_check()\nreturn args", "path": "MySQLdb\\cursors.py", "repo_name": "farcepest/moist", "stars": 195, "license": "None", "language": "python", "size": 750}
{"docstring": "\"\"\"Does nothing, required by DB API.\"\"\"\n\n", "func_signal": "def setoutputsizes(self, *args):\n", "code": "\n\"\"\"Get the database connection.\n\nRaises ProgrammingError if the connection has been closed.\"\"\"\nif not self.connection:\n    self.errorhandler(self, self.ProgrammingError, \"cursor closed\")\nreturn self.connection._db", "path": "MySQLdb\\cursors.py", "repo_name": "farcepest/moist", "stars": 195, "license": "None", "language": "python", "size": 750}
{"docstring": "\"\"\"_flush() reads to the end of the current result set, buffering what\nit can, and then releases the result set.\"\"\"\n", "func_signal": "def _flush(self):\n", "code": "if self._result:\n    self._result.flush()\n    self._result = None\ndb = self._get_db()\nwhile db.next_result():\n    result = Result(self)\n    result.flush()\n    self._pending_results.append(result)", "path": "MySQLdb\\cursors.py", "repo_name": "farcepest/moist", "stars": 195, "license": "None", "language": "python", "size": 750}
{"docstring": "# Number data\n", "func_signal": "def test_INT(self):\n", "code": "def generator(row,col):\n    return row*row\nself.check_data_integrity(\n    ('col1 INT',),\n    generator)", "path": "tests\\capabilities.py", "repo_name": "farcepest/moist", "stars": 195, "license": "None", "language": "python", "size": 750}
{"docstring": "\"\"\"Check for warnings, and report via the warnings module.\"\"\"\n", "func_signal": "def _warning_check(self):\n", "code": "from warnings import warn\nif self._warnings:\n    warnings = self._get_db()._show_warnings()\n    if warnings:\n        # This is done in two loops in case\n        # Warnings are set to raise exceptions.\n        for warning in warnings:\n            self.messages.append((self.Warning, warning))\n        for warning in warnings:\n            warn(warning[-1], self.Warning, 3)\n    elif self._info:\n        self.messages.append((self.Warning, self._info))\n        warn(self._info, self.Warning, 3)", "path": "MySQLdb\\cursors.py", "repo_name": "farcepest/moist", "stars": 195, "license": "None", "language": "python", "size": 750}
{"docstring": "\"\"\"Close the cursor. No further queries will be possible.\"\"\"\n", "func_signal": "def close(self):\n", "code": "if not self.connection:\n    return\n\nself._flush()\ntry:\n    while self.nextset():\n        pass\nexcept:\n    pass\nself.connection = None", "path": "MySQLdb\\cursors.py", "repo_name": "farcepest/moist", "stars": 195, "license": "None", "language": "python", "size": 750}
{"docstring": "# Character data\n", "func_signal": "def test_CHAR(self):\n", "code": "def generator(row,col):\n    return ('%i' % ((row+col) % 10)) * 255\nself.check_data_integrity(\n    ('col1 char(255)','col2 char(255)'),\n    generator)", "path": "tests\\capabilities.py", "repo_name": "farcepest/moist", "stars": 195, "license": "None", "language": "python", "size": 750}
{"docstring": "\"\"\"Version information sanity.\"\"\"\n", "func_signal": "def test_version(self):\n", "code": "self.assertTrue(isinstance(_mysql.__version__, str))\n\nself.assertTrue(isinstance(_mysql.version_info, tuple))\nself.assertEqual(len(_mysql.version_info), 5)", "path": "tests\\test_MySQLdb_nonstandard.py", "repo_name": "farcepest/moist", "stars": 195, "license": "None", "language": "python", "size": 750}
{"docstring": "\"\"\"Ensure that .execute() has been called.\"\"\"\n", "func_signal": "def _check_executed(self):\n", "code": "if not self._executed:\n    self.errorhandler(self, self.ProgrammingError, \"execute() first\")", "path": "MySQLdb\\cursors.py", "repo_name": "farcepest/moist", "stars": 195, "license": "None", "language": "python", "size": 750}
{"docstring": "\"\"\"Check for warnings, and report via the warnings module.\"\"\"\n", "func_signal": "def warning_check(self):\n", "code": "if self.warning_count:\n    cursor = self.cursor\n    warnings = cursor._get_db()._show_warnings()\n    if warnings:\n        # This is done in two loops in case\n        # Warnings are set to raise exceptions.\n        for warning in warnings:\n            cursor.warnings.append((self.Warning, warning))\n        for warning in warnings:\n            warn(warning[-1], self.Warning, 3)\n    elif self._info:\n        cursor.messages.append((self.Warning, self._info))\n        warn(self._info, self.Warning, 3)", "path": "MySQLdb\\cursors.py", "repo_name": "farcepest/moist", "stars": 195, "license": "None", "language": "python", "size": 750}
{"docstring": "# DECIMAL\n", "func_signal": "def test_DECIMAL(self):\n", "code": "def generator(row,col):\n    from decimal import Decimal\n    return Decimal(\"%d.%02d\" % (row, col))\nself.check_data_integrity(\n    ('col1 DECIMAL(5,2)',),\n    generator)", "path": "tests\\capabilities.py", "repo_name": "farcepest/moist", "stars": 195, "license": "None", "language": "python", "size": 750}
{"docstring": "# Number data\n", "func_signal": "def test_TINYINT(self):\n", "code": "def generator(row,col):\n    v = (row*row) % 256\n    if v > 127:\n        v = v-256\n    return v\nself.check_data_integrity(\n    ('col1 TINYINT',),\n    generator)", "path": "tests\\test_MySQLdb_capabilities.py", "repo_name": "farcepest/moist", "stars": 195, "license": "None", "language": "python", "size": 750}
{"docstring": "\"\"\"Low-level; executes query, gets result, sets up decoders.\"\"\"\n", "func_signal": "def _query(self, query):\n", "code": "connection = self._get_db()\nself._flush()\nself._executed = query\nconnection.query(query)\nself._result = Result(self)", "path": "MySQLdb\\cursors.py", "repo_name": "farcepest/moist", "stars": 195, "license": "None", "language": "python", "size": 750}
{"docstring": "\"\"\"Fetch up to size rows from the cursor. Result set may be smaller\nthan size. If size is not defined, cursor.arraysize is used.\"\"\"\n", "func_signal": "def fetchmany(self, size):\n", "code": "row_end = self.row_index + size\nif self.result:\n    while self.row_index >= len(self.rows):\n        row = self.result.fetch_row()\n        if row is None:\n            break\n        self.rows.append(self.row_formatter(self.row_decoders, row))\nif self.row_index >= len(self.rows):\n    return []\nif row_end >= len(self.rows):\n    row_end = len(self.rows)\nrows = self.rows[self.row_index:row_end]\nself.row_index = row_end\nreturn rows", "path": "MySQLdb\\cursors.py", "repo_name": "farcepest/moist", "stars": 195, "license": "None", "language": "python", "size": 750}
{"docstring": "\"\"\"\n\"\"\"\n", "func_signal": "def load_places(input_files, geometry, name_field, placement_field):\n", "code": "rows = load_inputs(input_files, geometry, name_field, placement_field)\n\nfor (name, fontfile, fontsize, location, point, radius, properties, row, preferred) in rows:\n    #radius = int(row.get('point size', radius))\n\n    kwargs = dict(preferred=preferred)\n    \n    yield places.Point(name, fontfile, fontsize, location, point, radius, properties, **kwargs)", "path": "Dymo\\__init__.py", "repo_name": "migurski/Dymo", "stars": 144, "license": "None", "language": "python", "size": 56651}
{"docstring": "\"\"\"\n\"\"\"\n", "func_signal": "def load_inputs(input_files, geometry, name_field, placement_field):\n", "code": "for input_file in input_files:\n    name, ext = splitext(input_file)\n\n    if ext == '.gz':\n        input = GzipFile(input_file, 'r')\n        input_file = name\n    else:\n        input = open(input_file, 'r')\n\n    name, ext = splitext(input_file)\n    \n    if ext == '.csv':\n        dialect = 'excel'\n    elif ext in ('.tsv', '.txt'):\n        dialect = 'excel-tab'\n    \n    rows = list(DictReader(input, dialect=dialect))\n    types = dict()\n    \n    for row in rows:\n        for (key, value) in row.items():\n            if int_pat.match(value):\n                if key not in types:\n                    types[key] = int\n            elif float_pat.match(value):\n                if key not in types or types[key] is int:\n                    types[key] = float\n            else:\n                # it's not really a type, but it's like unicode()\n                types[key] = lambda s: s.decode('utf-8')\n    \n    for row in rows:\n        name = row[ name_field ].decode('utf-8')\n        radius = int(row.get('point size', 8))\n        \n        fontsize = int(row.get('font size', 12))\n        fontfile = row.get('font file', 'fonts/DejaVuSans.ttf')\n        \n        location, point = geometry.location_point(*row_location(row))\n        \n        properties = dict([(key_pat.sub(r'_', key), types[key](value))\n                           for (key, value) in row.items()\n                           if key not in ('latitude', 'longitude')])\n        \n        preferred = (placement_field in row and row[placement_field]) or None\n        \n        yield name, fontfile, fontsize, location, point, radius, properties, row, preferred", "path": "Dymo\\__init__.py", "repo_name": "migurski/Dymo", "stars": 144, "license": "None", "language": "python", "size": 56651}
{"docstring": "\"\"\" Return a registration point and text justification.\n\"\"\"\n", "func_signal": "def registration(self):\n", "code": "xmin, ymin, xmax, ymax = self._label_shape.bounds\ny = ymin + self._baseline\n\nif self.placement in (Blob.ENE, Blob.EE, Blob.ESE, Blob.NE, Blob.E, Blob.SE):\n    x, justification = xmin, 'left'\n\nelif self.placement in (Blob.S, Blob.C, Blob.N):\n    x, justification = xmin/2 + xmax/2, 'center'\n\nelif self.placement in (Blob.WNW, Blob.WW, Blob.WSW, Blob.NW, Blob.W, Blob.SW):\n    x, justification = xmax, 'right'\n\nreturn geometry.Point(x, y), justification", "path": "Dymo\\places.py", "repo_name": "migurski/Dymo", "stars": 144, "license": "None", "language": "python", "size": 56651}
{"docstring": "\"\"\" Return a location and point object for the lat, lon pair.\n\"\"\"\n", "func_signal": "def location_point(self, lat, lon):\n", "code": "location = Location(float(lat), float(lon))\nx, y = self.proj(location.lon, location.lat)\npoint = Point(x/self.scale, y/self.scale)\n\nreturn location, point", "path": "Dymo\\__init__.py", "repo_name": "migurski/Dymo", "stars": 144, "license": "None", "language": "python", "size": 56651}
{"docstring": "\"\"\"\n\"\"\"\n", "func_signal": "def __init__(self, projection, scale):\n", "code": "if 'pyproj' not in globals():\n    raise ImportError('No module named pyproj')\n\nself.proj = pyproj.Proj(projection)\nself.scale = float(scale)", "path": "Dymo\\__init__.py", "repo_name": "migurski/Dymo", "stars": 144, "license": "None", "language": "python", "size": 56651}
{"docstring": "\"\"\" Rectangular area occupied by a label placed by a point with radius.\n\"\"\"\n", "func_signal": "def label_bounds(x, y, width, height, radius, placement):\n", "code": "if placement in (Point.NE, Point.ENE, Point.ESE, Point.SE):\n    # to the right\n    x += radius + width/2\n\nif placement in (Point.NW, Point.WNW, Point.WSW, Point.SW):\n    # to the left\n    x -= radius + width/2\n    \nif placement in (Point.NW, Point.NE):\n    # way up high\n    y += height/2\n    \nif placement in (Point.SW, Point.SE):\n    # way down low\n    y -= height/2\n    \nif placement in (Point.ENE, Point.WNW):\n    # just a little above\n    y += height/6\n    \nif placement in (Point.ESE, Point.WSW):\n    # just a little below\n    y -= height/6\n\nif placement in (Point.NNE, Point.SSE, Point.SSW, Point.NNW):\n    _x = radius * cos(pi/4) + width/2\n    _y = radius * sin(pi/4) + height/2\n    \n    if placement in (Point.NNE, Point.SSE):\n        x += _x\n    else:\n        x -= _x\n    \n    if placement in (Point.SSE, Point.SSW):\n        y -= _y\n    else:\n        y += _y\n\nif placement == Point.N:\n    # right on top\n    y += radius + height / 2\n\nif placement == Point.S:\n    # right on the bottom\n    y -= radius + height / 2\n\nx1, y1 = x - width/2, y + height/2\nx2, y2 = x + width/2, y - height/2\n\nreturn geometry.Polygon(((x1, y1), (x1, y2), (x2, y2), (x2, y1), (x1, y1)))", "path": "Dymo\\places.py", "repo_name": "migurski/Dymo", "stars": 144, "license": "None", "language": "python", "size": 56651}
{"docstring": "\"\"\"\n\"\"\"\n", "func_signal": "def __init__(self, zoom):\n", "code": "self.zoom = zoom\nself.radius = 2 ** (zoom + 7)", "path": "Dymo\\__init__.py", "repo_name": "migurski/Dymo", "stars": 144, "license": "None", "language": "python", "size": 56651}
{"docstring": "\"\"\" Return a location and point object for the lat, lon pair.\n\"\"\"\n", "func_signal": "def location_point(self, lat, lon):\n", "code": "try:\n    location = Location(float(lat), float(lon))\n    coord = _osm.locationCoordinate(location).zoomTo(self.zoom + 8)\n    point = Point(coord.column - self.radius, self.radius - coord.row)\n    \n    return location, point\n    \nexcept ValueError:\n    raise Exception((lat, lon, zoom))", "path": "Dymo\\__init__.py", "repo_name": "migurski/Dymo", "stars": 144, "license": "None", "language": "python", "size": 56651}
{"docstring": "\"\"\" Set values for self._label_shapes, _footprint_shape, and others.\n\"\"\"\n", "func_signal": "def _populate_shapes(self):\n", "code": "point = geometry.Point(self.position.x, self.position.y)\npoint_buffered = point.buffer(self.radius + self.buffer, 3)\nself._point_shape = point.buffer(self.radius, 3)\n\nscale = 10.0\nfont = truetype(self.fontfile, int(self.fontsize * scale), encoding='unic')\n\nx, y = self.position.x, self.position.y\nw, h = font.getsize(self.name)\nw, h = w/scale, h/scale\n\nfor placement in Point.placements:\n    label_shape = Point.label_bounds(x, y, w, h, self.radius, placement)\n    mask_shape = label_shape.buffer(self.buffer, 2).union(point_buffered)\n    \n    self._label_shapes[placement] = label_shape\n    self._mask_shapes[placement] = mask_shape\n    \nunionize = lambda a, b: a.union(b)\nself._label_footprint = reduce(unionize, self._label_shapes.values())\nself._mask_footprint = reduce(unionize, self._mask_shapes.values())\n\n# number of pixels from the top of the label based on the bottom of a \".\"\nself._baseline = font.getmask('.').getbbox()[3] / scale", "path": "Dymo\\places.py", "repo_name": "migurski/Dymo", "stars": 144, "license": "None", "language": "python", "size": 56651}
{"docstring": "\"\"\" Set values for self._placements.\n\"\"\"\n# local copy of placement energies\n", "func_signal": "def _populate_placements(self, preferred):\n", "code": "self._placements = deepcopy(Point.placements)\n\n# top right is the Imhof-approved default\nif preferred == 'top right' or not preferred:\n    return\n\n# bump up the cost of every placement artificially to leave room for new preferences\nself._placements = dict([ (key, .4 + v*.6) for (key, v) in self._placements.items() ])\n\nif preferred == 'top':\n    self.placement = Point.N\n    self._placements.update({ Point.N: .0, Point.NNW: .3, Point.NNE: .3 })\n\nelif preferred == 'top left':\n    self.placement = Point.NW\n    self._placements.update({ Point.NW: .0, Point.WNW: .1, Point.NNW: .1 })\n\nelif preferred == 'bottom':\n    self.placement = Point.S\n    self._placements.update({ Point.S: .0, Point.SSW: .3, Point.SSE: .3 })\n\nelif preferred == 'bottom right':\n    self.placement = Point.SE\n    self._placements.update({ Point.SE: .0, Point.ESE: .1, Point.SSE: .1 })\n\nelif preferred == 'bottom left':\n    self.placement = Point.SW\n    self._placements.update({ Point.SW: .0, Point.WSW: .1, Point.SSW: .1 })\n\nelse:\n    raise Exception('Unknown preferred placement \"%s\"' % preferred)", "path": "Dymo\\places.py", "repo_name": "migurski/Dymo", "stars": 144, "license": "None", "language": "python", "size": 56651}
{"docstring": "\"\"\"\n\"\"\"\n", "func_signal": "def __deepcopy__(self, memo_dict):\n", "code": "extras = dict(energy = self.energy,\n              previous = (self.keep_chain and self or None),\n              _places = deepcopy(self._places, memo_dict),\n              _neighbors = deepcopy(self._neighbors, memo_dict),\n              _moveable = deepcopy(self._moveable, memo_dict))\n\nreturn Places(self.keep_chain, **extras)", "path": "Dymo\\places.py", "repo_name": "migurski/Dymo", "stars": 144, "license": "None", "language": "python", "size": 56651}
{"docstring": "\"\"\"\n\"\"\"\n", "func_signal": "def prepare_file(name, mode):\n", "code": "base, ext = splitext(name)\n\nif ext == '.gz':\n    file = GzipFile(name, mode)\n    name = base\nelif ext in ('.csv', '.txt', '.tsv'):\n    file = open(name, mode)\nelse:\n    raise Exception('Bad extension \"%(ext)s\" in \"%(name)s\"' % locals())\n\nbase, ext = splitext(name)\n\nif ext == '.csv':\n    dialect = 'excel'\nelif ext in ('.txt', '.tsv'):\n    dialect = 'excel-tab'\nelse:\n    raise Exception('Bad extension \"%(ext)s\" in \"%(name)s\"' % locals())\n\nif mode == 'r':\n    return DictReader(file, dialect=dialect)\nelif mode == 'w':\n    return writer(file, dialect=dialect)", "path": "dymo-prepare-places.py", "repo_name": "migurski/Dymo", "stars": 144, "license": "None", "language": "python", "size": 56651}
{"docstring": "\"\"\" Return a registration point and text justification.\n\"\"\"\n", "func_signal": "def registration(self):\n", "code": "xmin, ymin, xmax, ymax = self._label_shape.bounds\ny = ymin + self._baseline\n\nif self.placement in (Point.NNE, Point.NE, Point.ENE, Point.ESE, Point.SE, Point.SSE):\n    x, justification = xmin, 'left'\n\nelif self.placement in (Point.S, Point.N):\n    x, justification = xmin/2 + xmax/2, 'center'\n\nelif self.placement in (Point.SSW, Point.SW, Point.WSW, Point.WNW, Point.NW, Point.NNW):\n    x, justification = xmax, 'right'\n\nreturn geometry.Point(x, y), justification", "path": "Dymo\\places.py", "repo_name": "migurski/Dymo", "stars": 144, "license": "None", "language": "python", "size": 56651}
{"docstring": "\"\"\" Override deep copy to spend less time copying.\n\n    Profiling showed that a significant percentage of time was spent\n    deep-copying annealer state from step to step, and testing with\n    z5 U.S. data shows a 4000% speed increase, so yay.\n\"\"\"\n", "func_signal": "def __deepcopy__(self, memo_dict):\n", "code": "extras = dict(placement = self.placement,\n              _label_shapes = self._label_shapes,\n              _mask_shapes = self._mask_shapes,\n              _label_footprint = self._label_footprint,\n              _mask_footprint = self._mask_footprint,\n              _placements = self._placements,\n              _baseline = self._baseline)\n\nreturn Blob(self.name, self.fontfile, self.fontsize, self.location,\n            self.position, self.properties, self.rank, **extras)", "path": "Dymo\\places.py", "repo_name": "migurski/Dymo", "stars": 144, "license": "None", "language": "python", "size": 56651}
{"docstring": "\"\"\" Partition places into mutually-overlapping collections.\n\n    Return value is a list of tuples, each with a Places instance,\n    a list of indexes back to the parent Places instance, a weight\n    for that instance based on connectivity density, and a total\n    weight for all pieces together.\n\"\"\"\n", "func_signal": "def in_pieces(self):\n", "code": "partition, places = [], self._places[:]\n\nwhile places:\n    group, neighbors = [], [places.pop(0)]\n    \n    while neighbors:\n        place = neighbors.pop(0)\n        group.append(place)\n        \n        if place in places:\n            # can't be in any other group\n            places.remove(place)\n        \n        for neighbor in self._neighbors[place]:\n            if neighbor not in group and neighbor not in neighbors:\n                neighbors.append(neighbor)\n    \n    partition.append(group)\n\n#\n# partition is now a list of lists of places.\n#\n\npieces = []\n\nfor place_list in partition:\n    places = Places(self.keep_chain)\n    indexes = []\n    weight = 0\n    \n    for place in place_list:\n        places.add(place)\n        weight += len(self._neighbors[place])\n        indexes.append(self._places.index(place))\n    \n    pieces.append((places, indexes, weight))\n\ntotal_weight = sum([weight for (p, i, weight) in pieces])\npieces = [(p, i, w/2, total_weight/2) for (p, i, w) in pieces]\npieces.sort(key=lambda piece: piece[2], reverse=True)\n\n#\n# pieces is now a list of tuples, each with an instance of Places,\n# a list of indexes back to self._places (the original collections),\n# and the numerator and denominator of a fractional weight based on\n# connectivity and expected processing time.\n#\n\nreturn pieces", "path": "Dymo\\places.py", "repo_name": "migurski/Dymo", "stars": 144, "license": "None", "language": "python", "size": 56651}
{"docstring": "\"\"\" Energy of an overlap between two places, if it exists.\n\"\"\"\n", "func_signal": "def _overlap_energy(self, this, that):\n", "code": "if not this.overlaps(that):\n    return 0.0\n\nreturn min(10.0 / this.rank, 10.0 / that.rank)", "path": "Dymo\\places.py", "repo_name": "migurski/Dymo", "stars": 144, "license": "None", "language": "python", "size": 56651}
{"docstring": "\"\"\"\n\"\"\"\n", "func_signal": "def load_blobs(input_files, geometry, name_field, placement_field):\n", "code": "rows = load_inputs(input_files, geometry, name_field, placement_field)\n\nfor (name, fontfile, fontsize, location, point, radius, properties, row, preferred) in rows:\n    yield places.Blob(name, fontfile, fontsize, location, point, properties)", "path": "Dymo\\__init__.py", "repo_name": "migurski/Dymo", "stars": 144, "license": "None", "language": "python", "size": 56651}
{"docstring": "\"\"\" Return a longitude, latitude tuple from pixels.\n\"\"\"\n", "func_signal": "def point_lonlat(self, x, y):\n", "code": "try:\n    coord = Coordinate(self.radius - y, x + self.radius, self.zoom + 8)\n    location = _osm.coordinateLocation(coord)\n    \n    return location.lon, location.lat\n    \nexcept ValueError:\n    raise Exception((x, y, zoom))", "path": "Dymo\\__init__.py", "repo_name": "migurski/Dymo", "stars": 144, "license": "None", "language": "python", "size": 56651}
{"docstring": "\"\"\" Set values for self._label_shapes, _footprint_shape, and others.\n\"\"\"\n", "func_signal": "def _populate_shapes(self):\n", "code": "scale = 10.0\nfont = truetype(self.fontfile, int(self.fontsize * scale), encoding='unic')\n\nx, y = self.position.x, self.position.y\nw, h = font.getsize(self.name)\nw, h = w/scale, h/scale\n\nfor placement in Blob.placements:\n    label_shape = Blob.label_bounds(x, y, w, h, placement)\n    mask_shape = label_shape.buffer(self.buffer, 2)\n\n    self._label_shapes[placement] = label_shape\n    self._mask_shapes[placement] = mask_shape\n    \nunionize = lambda a, b: a.union(b)\nself._label_footprint = reduce(unionize, self._label_shapes.values())\nself._mask_footprint = reduce(unionize, self._mask_shapes.values())\n\n# number of pixels from the top of the label based on the bottom of a \".\"\nself._baseline = font.getmask('.').getbbox()[3] / scale", "path": "Dymo\\places.py", "repo_name": "migurski/Dymo", "stars": 144, "license": "None", "language": "python", "size": 56651}
{"docstring": "\"\"\" Return a floating point latitude, longitude pair from a row.\n\"\"\"\n", "func_signal": "def row_location(row):\n", "code": "if 'latitude' in row:\n    lat = row['latitude']\nelif 'LATITUDE' in row:\n    lat = row['LATITUDE']\nelif 'lat' in row:\n    lat = row['lat']\nelif 'LAT' in row:\n    lat = row['LAT']\nelse:\n    raise Exception('Missing \"latitude\" or \"lat\" field in row')\n\nif 'longitude' in row:\n    lon = row['longitude']\nelif 'LONGITUDE' in row:\n    lon = row['LONGITUDE']\nelif 'long' in row:\n    lon = row['long']\nelif 'LONG' in row:\n    lon = row['LONG']\nelif 'lon' in row:\n    lon = row['lon']\nelif 'LON' in row:\n    lon = row['LON']\nelse:\n    raise Exception('Missing \"longitude\", \"long\", or \"lon\" field in row')\n\nreturn float(lat), float(lon)", "path": "Dymo\\__init__.py", "repo_name": "migurski/Dymo", "stars": 144, "license": "None", "language": "python", "size": 56651}
{"docstring": "# Test unauthenticated\n", "func_signal": "def test_index(self):\n", "code": "response = self.cli.get('/api/annotations')\nresults = json.loads(response.data)\nassert results == [], \"unauthenticated user should get an empty list\"\n\n# Test as bob (authorized to read)\nresponse = self.cli.get('/api/annotations',\n                   headers=self.bob_headers)\nresults = json.loads(response.data)\nassert results and results[0]['id'] == self.anno_id, \"bob should see his own annotation\"\n\n# Test as charlie (unauthorized)\nresponse = self.cli.get('/api/annotations',\n                   headers=self.charlie_headers)\nresults = json.loads(response.data)\nassert results == []", "path": "tests\\test_store.py", "repo_name": "openannotation/annotator-store", "stars": 174, "license": "mit", "language": "python", "size": 723}
{"docstring": "# Test search retrieve\n", "func_signal": "def test_search(self):\n", "code": "d = Document({\n    \"id\": \"1\",\n    \"title\": \"document\",\n    \"link\": [peerj[\"html\"], peerj[\"pdf\"]]\n})\nd.save()\nres = Document.search(query={'title': 'document'})\nassert_equal(len(res), 1)", "path": "tests\\test_document.py", "repo_name": "openannotation/annotator-store", "stars": 174, "license": "mit", "language": "python", "size": 723}
{"docstring": "# Create a match query for each keyword\n", "func_signal": "def _build_query(query, offset, limit, sort, order):\n", "code": "match_clauses = [{'match': {k: v}} for k, v in iteritems(query)]\n\nif len(match_clauses) == 0:\n    # Elasticsearch considers an empty conjunction to be false..\n    match_clauses.append({'match_all': {}})\n\nreturn {\n    'sort': [{sort: {\n        # Sort most recent first\n        'order': order,\n        # While we do always provide a mapping for the field, elasticsearch\n        # will bomb if there are no documents in the index. Although this\n        # is an edge case, we don't want the API to return a 500 with an\n        # empty index, so ignore this sort instruction if the field appears\n        # unmapped due to an empty index.\n        'ignore_unmapped': True,\n    }}],\n    'from': max(0, offset),\n    'size': min(RESULTS_MAX_SIZE, max(0, limit)),\n    'query': {'bool': {'must': match_clauses}}\n}", "path": "annotator\\elasticsearch.py", "repo_name": "openannotation/annotator-store", "stars": 174, "license": "mit", "language": "python", "size": 723}
{"docstring": "# Configure index mappings\n", "func_signal": "def get_index_config(self):\n", "code": "index_config = {'mappings': {}}\nfor model in self.es_models:\n    index_config['mappings'].update(model.get_mapping())\nreturn index_config", "path": "annotator\\reindexer.py", "repo_name": "openannotation/annotator-store", "stars": 174, "license": "mit", "language": "python", "size": 723}
{"docstring": "\"\"\"Like search, but only count the number of matches.\"\"\"\n", "func_signal": "def count(cls, **kwargs):\n", "code": "kwargs.setdefault('params', {})\nkwargs['params'].update({'search_type': 'count'})\nres = cls.search(raw_result=True, **kwargs)\nreturn res['hits']['total']", "path": "annotator\\elasticsearch.py", "repo_name": "openannotation/annotator-store", "stars": 174, "license": "mit", "language": "python", "size": 723}
{"docstring": "\"\"\"Test if operation type is 'index' when an id field is present\"\"\"\n", "func_signal": "def test_op_type_index(self, es_mock):\n", "code": "m = self.Model(bla='blub', id=123)\nm.save()\n\nconn = es_mock.return_value\ncall_kwargs = conn.index.call_args_list[0][1]\nassert call_kwargs['op_type'] == 'index', \"Operation should be: index\"", "path": "tests\\test_elasticsearch.py", "repo_name": "openannotation/annotator-store", "stars": 174, "license": "mit", "language": "python", "size": 723}
{"docstring": "\"\"\"Build a file path from *paths* and return the contents.\"\"\"\n", "func_signal": "def read(*paths):\n", "code": "with open(os.path.join(*paths), 'r') as f:\n    return f.read()", "path": "setup.py", "repo_name": "openannotation/annotator-store", "stars": 174, "license": "mit", "language": "python", "size": 723}
{"docstring": "# Logged in as Bob: 1 result\n", "func_signal": "def test_search_raw_authorized(self):\n", "code": "results = self._get_search_raw_results(headers=self.bob_headers)\nassert results['hits']['total'] == 1\nassert results['hits']['hits'][0]['_id'] == self.anno_id\n\n# Logged in as Charlie: 0 results\nresults = self._get_search_raw_results(headers=self.charlie_headers)\nassert results['hits']['total'] == 0\nassert results['hits']['hits'] == []", "path": "tests\\test_store.py", "repo_name": "openannotation/annotator-store", "stars": 174, "license": "mit", "language": "python", "size": 723}
{"docstring": "# Not logged in: no results\n", "func_signal": "def test_search_raw_public(self):\n", "code": "results = self._get_search_raw_results()\nassert results['hits']['total'] == 0\nassert results['hits']['hits'] == []", "path": "tests\\test_store.py", "repo_name": "openannotation/annotator-store", "stars": 174, "license": "mit", "language": "python", "size": 723}
{"docstring": "# Test that bad links are not saved\n", "func_signal": "def test_deficient_links(self):\n", "code": "d = Document({\n    \"id\": \"1\",\n    \"title\": \"Chaos monkey: The messed up links\",\n    \"link\": [{\n        \"href\": \"http://cuckoo.baboon/\"\n    }, {\n        # I'm an empty link entry\n    }, {\n        \"type\": \"text/html\"\n    }, {\n        \"href\": \"http://cuckoo.baboon/\",\n        \"type\": \"text/html\"\n    }]\n})\nd.save()\nd = Document.fetch(\"1\")\nassert_equal(len(d['link']), 2)\nassert_equal(d['link'][0]['href'], \"http://cuckoo.baboon/\")\nassert_equal(d['link'][1]['href'], \"http://cuckoo.baboon/\")\nassert_equal(d['link'][1]['type'], \"text/html\")", "path": "tests\\test_document.py", "repo_name": "openannotation/annotator-store", "stars": 174, "license": "mit", "language": "python", "size": 723}
{"docstring": "# Make sure that only the document with the given uri is retrieved\n\n", "func_signal": "def test_get_by_uri(self):\n", "code": "d = Document({\n    \"id\": \"1\",\n    \"title\": \"document1\",\n    \"link\": [peerj[\"html\"], peerj[\"pdf\"]]\n})\nd.save()\n\nd = Document({\n    \"id\": \"2\",\n    \"title\": \"document2\",\n    \"link\": [\n        {\n            \"href\": \"http://nature.com/123/\",\n            \"type\": \"text/html\"\n        }\n    ],\n})\nd.save()\n\nd = Document({\n    \"id\": \"3\",\n    \"title\": \"document3\",\n    \"link\": [peerj[\"doc\"]]\n})\nd.save()\n\ndoc = Document.get_by_uri(\"https://peerj.com/articles/53/\")\nassert doc\nassert_equal(doc['title'], \"document1\")", "path": "tests\\test_document.py", "repo_name": "openannotation/annotator-store", "stars": 174, "license": "mit", "language": "python", "size": 723}
{"docstring": "\"\"\"Reindex documents using the current mappings.\"\"\"\n", "func_signal": "def reindex(self, old_index, new_index):\n", "code": "conn = self.conn\n\nif not conn.indices.exists(old_index):\n    raise ValueError(\"Index {0} does not exist!\".format(old_index))\n\nif conn.indices.exists(new_index):\n    self._print(\"Index {0} already exists. \"\n                \"The mapping will not be changed.\".format(new_index))\nelse:\n    # Create the new index with (presumably) new mapping config\n    conn.indices.create(new_index, body=self.get_index_config())\n\n# Do the actual reindexing.\nself._print(\"Reindexing {0} to {1}...\".format(old_index, new_index))\nhelpers.reindex(conn, old_index, new_index)\nself._print(\"Reindexing done.\")", "path": "annotator\\reindexer.py", "repo_name": "openannotation/annotator-store", "stars": 174, "license": "mit", "language": "python", "size": 723}
{"docstring": "\"\"\"Indexing and search should not apply lowercase to strings\n   (this requirement might be changed sometime)\n\"\"\"\n# https://github.com/openannotation/annotator-store/issues/73\n", "func_signal": "def test_case_sensitivity(self):\n", "code": "anno = Annotation(uri='http://example.com/1234',\n                  text='Foobar',\n                  user='alice',\n                  consumer='testconsumer',\n                  custom_field='CaseSensitive')\nanno.save()\n\nuser = h.MockUser('alice', 'testconsumer')\nres = Annotation.search(user=user,\n                        query={'custom_field':'CaseSensitive'})\nassert_equal(len(res), 1)\n\nres = Annotation.search(user=user,\n                        query={'custom_field':'casesensitive'})\nassert_equal(len(res), 0)", "path": "tests\\test_annotation.py", "repo_name": "openannotation/annotator-store", "stars": 174, "license": "mit", "language": "python", "size": 723}
{"docstring": "\"\"\"\nRetrieve any request token from the passed request, verify its\nauthenticity and validity, and return the parsed contents of the token\nif and only if all such checks pass.\n\nArguments:\nrequest -- a Flask Request object\n\"\"\"\n\n", "func_signal": "def _decode_request_token(self, request):\n", "code": "token = request.headers.get('x-annotator-auth-token')\nif token is None:\n    return False\n\ntry:\n    unsafe_token = decode_token(token, verify=False)\nexcept TokenInvalid:  # catch junk tokens\n    return False\n\nkey = unsafe_token.get('consumerKey')\nif not key:\n    return False\n\nconsumer = self.consumer_fetcher(key)\nif not consumer:\n    return False\n\ntry:\n    return decode_token(token,\n                        secret=consumer.secret,\n                        ttl=consumer.ttl)\nexcept TokenInvalid:  # catch inauthentic or expired tokens\n    return False", "path": "annotator\\auth.py", "repo_name": "openannotation/annotator-store", "stars": 174, "license": "mit", "language": "python", "size": 723}
{"docstring": "\"\"\"Perform a raw Elasticsearch query\n\nAny ElasticsearchExceptions are to be caught by the caller.\n\nKeyword arguments:\nquery -- Query to send to Elasticsearch\nparams -- Extra keyword arguments to pass to Elasticsearch.search\nraw_result -- Return Elasticsearch's response as is\n\"\"\"\n", "func_signal": "def search_raw(cls, query=None, params=None, raw_result=False):\n", "code": "if query is None:\n    query = {}\nif params is None:\n    params = {}\nres = cls.es.conn.search(index=cls.es.index,\n                         doc_type=cls.__type__,\n                         body=query,\n                         **params)\nif not raw_result:\n    docs = res['hits']['hits']\n    res = [cls(d['_source'], id=d['_id']) for d in docs]\nreturn res", "path": "annotator\\elasticsearch.py", "repo_name": "openannotation/annotator-store", "stars": 174, "license": "mit", "language": "python", "size": 723}
{"docstring": "# Creating a single document and verifies the saved attributes\n", "func_signal": "def test_basics(self):\n", "code": "d = Document({\n    \"id\": \"1\",\n    \"title\": \"Annotations: The Missing Manual\",\n    \"link\": [peerj[\"html\"], peerj[\"pdf\"]]\n})\nd.save()\nd = Document.fetch(\"1\")\nassert_equal(d[\"title\"], \"Annotations: The Missing Manual\")\nassert_equal(len(d['link']), 2)\nassert_equal(d['link'][0]['href'], \"https://peerj.com/articles/53/\")\nassert_equal(d['link'][0]['type'], \"text/html\")\nassert_equal(d['link'][1]['href'], \"https://peerj.com/articles/53.pdf\")\nassert_equal(d['link'][1]['type'], \"application/pdf\")\nassert d['created']\nassert d['updated']", "path": "tests\\test_document.py", "repo_name": "openannotation/annotator-store", "stars": 174, "license": "mit", "language": "python", "size": 723}
{"docstring": "# Not logged in: no results\n", "func_signal": "def test_search_public(self):\n", "code": "results = self._get_search_results()\nassert results['total'] == 0\nassert results['rows'] == []", "path": "tests\\test_store.py", "repo_name": "openannotation/annotator-store", "stars": 174, "license": "mit", "language": "python", "size": 723}
{"docstring": "# Test deleting a document\n", "func_signal": "def test_delete(self):\n", "code": "ann = Document(id=1)\nann.save()\n\nnewdoc = Document.fetch(1)\nnewdoc.delete()\n\nnodoc = Document.fetch(1)\nassert nodoc is None", "path": "tests\\test_document.py", "repo_name": "openannotation/annotator-store", "stars": 174, "license": "mit", "language": "python", "size": 723}
{"docstring": "\"\"\"Filter an ElasticSearch query by the permissions of the current user\"\"\"\n\n# Scenario 1\n", "func_signal": "def permissions_filter(user=None):\n", "code": "perm_f = {'term': {'permissions.read': GROUP_WORLD}}\n\nif user is not None:\n    # Fail fast if this looks dodgy\n    if user.id.startswith('group:'):\n        return False\n\n    perm_f = {'or': [perm_f]}\n\n    # Scenario 2\n    perm_f['or'].append(\n        {'and': [{'term': {'consumer': user.consumer.key}},\n                 {'or': [{'term': {'user': user.id}},\n                         {'term': {'user.id': user.id}}]}]})\n\n    # Scenario 3\n    perm_f['or'].append(\n        {'term': {'permissions.read': GROUP_AUTHENTICATED}})\n\n    # Scenario 4\n    perm_f['or'].append(\n        {'and': [{'term': {'consumer': user.consumer.key}},\n                 {'term': {'permissions.read': GROUP_CONSUMER}}]})\n\n    # Scenario 5\n    perm_f['or'].append(\n        {'and': [{'term': {'consumer': user.consumer.key}},\n                 {'term': {'permissions.read': user.id}}]})\n\n    # Scenario 6\n    if user.is_admin:\n        perm_f['or'].append({'term': {'consumer': user.consumer.key}})\n\nreturn perm_f", "path": "annotator\\authz.py", "repo_name": "openannotation/annotator-store", "stars": 174, "license": "mit", "language": "python", "size": 723}
{"docstring": "# Logged in as Bob: 1 result\n", "func_signal": "def test_search_authenticated(self):\n", "code": "results = self._get_search_results(headers=self.bob_headers)\nassert results['total'] == 1\nassert results['rows'][0]['id'] == self.anno_id\n\n# Logged in as Charlie: 0 results\nresults = self._get_search_results(headers=self.charlie_headers)\nassert results['total'] == 0\nassert results['rows'] == []", "path": "tests\\test_store.py", "repo_name": "openannotation/annotator-store", "stars": 174, "license": "mit", "language": "python", "size": 723}
{"docstring": "\"\"\" Create a new li and parse the block with it as the parent. \"\"\"\n", "func_signal": "def create_item(self, parent, block):\n", "code": "li = util.etree.SubElement(parent, 'li')\nself.parser.parseBlocks(li, [block])", "path": "markdown\\blockprocessors.py", "repo_name": "iSECPartners/LibTech-Auditing-Cheatsheet", "stars": 198, "license": "None", "language": "python", "size": 81}
{"docstring": "\"\"\" Build the default postprocessors for Markdown. \"\"\"\n", "func_signal": "def build_postprocessors(md_instance, **kwargs):\n", "code": "postprocessors = odict.OrderedDict()\npostprocessors[\"raw_html\"] = RawHtmlPostprocessor(md_instance)\npostprocessors[\"amp_substitute\"] = AndSubstitutePostprocessor()\npostprocessors[\"unescape\"] = UnescapePostprocessor()\nreturn postprocessors", "path": "markdown\\postprocessors.py", "repo_name": "iSECPartners/LibTech-Auditing-Cheatsheet", "stars": 198, "license": "None", "language": "python", "size": 81}
{"docstring": "# Check fr multiple items in one block.\n", "func_signal": "def run(self, parent, blocks):\n", "code": "items = self.get_items(blocks.pop(0))\nsibling = self.lastChild(parent)\n\nif sibling and sibling.tag in ['ol', 'ul']:\n    # Previous block was a list item, so set that as parent\n    lst = sibling\n    # make sure previous item is in a p- if the item has text, then it\n    # it isn't in a p\n    if lst[-1].text: \n        # since it's possible there are other children for this sibling,\n        # we can't just SubElement the p, we need to insert it as the \n        # first item\n        p = util.etree.Element('p')\n        p.text = lst[-1].text\n        lst[-1].text = ''\n        lst[-1].insert(0, p)\n    # if the last item has a tail, then the tail needs to be put in a p\n    # likely only when a header is not followed by a blank line\n    lch = self.lastChild(lst[-1])\n    if lch is not None and lch.tail:\n        p = util.etree.SubElement(lst[-1], 'p')\n        p.text = lch.tail.lstrip()\n        lch.tail = ''\n\n    # parse first block differently as it gets wrapped in a p.\n    li = util.etree.SubElement(lst, 'li')\n    self.parser.state.set('looselist')\n    firstitem = items.pop(0)\n    self.parser.parseBlocks(li, [firstitem])\n    self.parser.state.reset()\nelif parent.tag in ['ol', 'ul']:\n    # this catches the edge case of a multi-item indented list whose \n    # first item is in a blank parent-list item:\n    # * * subitem1\n    #     * subitem2\n    # see also ListIndentProcessor\n    lst = parent\nelse:\n    # This is a new list so create parent with appropriate tag.\n    lst = util.etree.SubElement(parent, self.TAG)\n    # Check if a custom start integer is set\n    if not self.parser.markdown.lazy_ol and self.STARTSWITH !='1':\n        lst.attrib['start'] = self.STARTSWITH\n\nself.parser.state.set('list')\n# Loop through items in block, recursively parsing each with the\n# appropriate parent.\nfor item in items:\n    if item.startswith(' '*self.tab_length):\n        # Item is indented. Parse with last item as parent\n        self.parser.parseBlocks(lst[-1], [item])\n    else:\n        # New item. Create li and parse with it as parent\n        li = util.etree.SubElement(lst, 'li')\n        self.parser.parseBlocks(li, [item])\nself.parser.state.reset()", "path": "markdown\\blockprocessors.py", "repo_name": "iSECPartners/LibTech-Auditing-Cheatsheet", "stars": 198, "license": "None", "language": "python", "size": 81}
{"docstring": "\"\"\" Basic html escaping \"\"\"\n", "func_signal": "def escape(self, html):\n", "code": "html = html.replace('&', '&amp;')\nhtml = html.replace('<', '&lt;')\nhtml = html.replace('>', '&gt;')\nreturn html.replace('\"', '&quot;')", "path": "markdown\\postprocessors.py", "repo_name": "iSECPartners/LibTech-Auditing-Cheatsheet", "stars": 198, "license": "None", "language": "python", "size": 81}
{"docstring": "\"\"\"Return entity definition by code, or the code if not defined.\"\"\"\n", "func_signal": "def codepoint2name(code):\n", "code": "entity = htmlentitydefs.codepoint2name.get(code)\nif entity:\n    return \"%s%s;\" % (util.AMP_SUBSTITUTE, entity)\nelse:\n    return \"%s#%d;\" % (util.AMP_SUBSTITUTE, code)", "path": "markdown\\inlinepatterns.py", "repo_name": "iSECPartners/LibTech-Auditing-Cheatsheet", "stars": 198, "license": "None", "language": "python", "size": 81}
{"docstring": "\"\"\" Break a block into list items. \"\"\"\n", "func_signal": "def get_items(self, block):\n", "code": "items = []\nfor line in block.split('\\n'):\n    m = self.CHILD_RE.match(line)\n    if m:\n        # This is a new list item\n        # Check first item for the start index\n        if not items and self.TAG=='ol':\n            # Detect the integer value of first list item\n            INTEGER_RE = re.compile('(\\d+)')\n            self.STARTSWITH = INTEGER_RE.match(m.group(1)).group()\n        # Append to the list\n        items.append(m.group(3))\n    elif self.INDENT_RE.match(line):\n        # This is an indented (possibly nested) item.\n        if items[-1].startswith(' '*self.tab_length):\n            # Previous item was indented. Append to that item.\n            items[-1] = '%s\\n%s' % (items[-1], line)\n        else:\n            items.append(line)\n    else:\n        # This is another line of previous item. Append to that item.\n        items[-1] = '%s\\n%s' % (items[-1], line)\nreturn items", "path": "markdown\\blockprocessors.py", "repo_name": "iSECPartners/LibTech-Auditing-Cheatsheet", "stars": 198, "license": "None", "language": "python", "size": 81}
{"docstring": "\"\"\" Build the default set of inline patterns for Markdown. \"\"\"\n", "func_signal": "def build_inlinepatterns(md_instance, **kwargs):\n", "code": "inlinePatterns = odict.OrderedDict()\ninlinePatterns[\"backtick\"] = BacktickPattern(BACKTICK_RE)\ninlinePatterns[\"escape\"] = EscapePattern(ESCAPE_RE, md_instance)\ninlinePatterns[\"reference\"] = ReferencePattern(REFERENCE_RE, md_instance)\ninlinePatterns[\"link\"] = LinkPattern(LINK_RE, md_instance)\ninlinePatterns[\"image_link\"] = ImagePattern(IMAGE_LINK_RE, md_instance)\ninlinePatterns[\"image_reference\"] = \\\n        ImageReferencePattern(IMAGE_REFERENCE_RE, md_instance)\ninlinePatterns[\"short_reference\"] = \\\n        ReferencePattern(SHORT_REF_RE, md_instance)\ninlinePatterns[\"autolink\"] = AutolinkPattern(AUTOLINK_RE, md_instance)\ninlinePatterns[\"automail\"] = AutomailPattern(AUTOMAIL_RE, md_instance)\ninlinePatterns[\"linebreak2\"] = SubstituteTagPattern(LINE_BREAK_2_RE, 'br')\ninlinePatterns[\"linebreak\"] = SubstituteTagPattern(LINE_BREAK_RE, 'br')\nif md_instance.safeMode != 'escape':\n    inlinePatterns[\"html\"] = HtmlPattern(HTML_RE, md_instance)\ninlinePatterns[\"entity\"] = HtmlPattern(ENTITY_RE, md_instance)\ninlinePatterns[\"not_strong\"] = SimpleTextPattern(NOT_STRONG_RE)\ninlinePatterns[\"strong_em\"] = DoubleTagPattern(STRONG_EM_RE, 'strong,em')\ninlinePatterns[\"strong\"] = SimpleTagPattern(STRONG_RE, 'strong')\ninlinePatterns[\"emphasis\"] = SimpleTagPattern(EMPHASIS_RE, 'em')\nif md_instance.smart_emphasis:\n    inlinePatterns[\"emphasis2\"] = SimpleTagPattern(SMART_EMPHASIS_RE, 'em')\nelse:\n    inlinePatterns[\"emphasis2\"] = SimpleTagPattern(EMPHASIS_2_RE, 'em')\nreturn inlinePatterns", "path": "markdown\\inlinepatterns.py", "repo_name": "iSECPartners/LibTech-Auditing-Cheatsheet", "stars": 198, "license": "None", "language": "python", "size": 81}
{"docstring": "\"\"\" Iterate over html stash and restore \"safe\" html. \"\"\"\n", "func_signal": "def run(self, text):\n", "code": "for i in range(self.markdown.htmlStash.html_counter):\n    html, safe  = self.markdown.htmlStash.rawHtmlBlocks[i]\n    if self.markdown.safeMode and not safe:\n        if str(self.markdown.safeMode).lower() == 'escape':\n            html = self.escape(html)\n        elif str(self.markdown.safeMode).lower() == 'remove':\n            html = ''\n        else:\n            html = self.markdown.html_replacement_text\n    if self.isblocklevel(html) and (safe or not self.markdown.safeMode):\n        text = text.replace(\"<p>%s</p>\" % \n                    (self.markdown.htmlStash.get_placeholder(i)),\n                    html + \"\\n\")\n    text =  text.replace(self.markdown.htmlStash.get_placeholder(i), \n                         html)\nreturn text", "path": "markdown\\postprocessors.py", "repo_name": "iSECPartners/LibTech-Auditing-Cheatsheet", "stars": 198, "license": "None", "language": "python", "size": 81}
{"docstring": "\"\"\" Create a HtmlStash. \"\"\"\n", "func_signal": "def __init__ (self):\n", "code": "self.html_counter = 0 # for counting inline html segments\nself.rawHtmlBlocks=[]", "path": "markdown\\util.py", "repo_name": "iSECPartners/LibTech-Auditing-Cheatsheet", "stars": 198, "license": "None", "language": "python", "size": 81}
{"docstring": "\"\"\" Remove ``>`` from beginning of a line. \"\"\"\n", "func_signal": "def clean(self, line):\n", "code": "m = self.RE.match(line)\nif line.strip() == \">\":\n    return \"\"\nelif m:\n    return m.group(2)\nelse:\n    return line", "path": "markdown\\blockprocessors.py", "repo_name": "iSECPartners/LibTech-Auditing-Cheatsheet", "stars": 198, "license": "None", "language": "python", "size": 81}
{"docstring": "\"\"\" Build the default set of preprocessors used by Markdown. \"\"\"\n", "func_signal": "def build_preprocessors(md_instance, **kwargs):\n", "code": "preprocessors = odict.OrderedDict()\nif md_instance.safeMode != 'escape':\n    preprocessors[\"html_block\"] = HtmlBlockPreprocessor(md_instance)\npreprocessors[\"reference\"] = ReferencePreprocessor(md_instance)\nreturn preprocessors", "path": "markdown\\preprocessors.py", "repo_name": "iSECPartners/LibTech-Auditing-Cheatsheet", "stars": 198, "license": "None", "language": "python", "size": 81}
{"docstring": "\"\"\" Build the default block parser used by Markdown. \"\"\"\n", "func_signal": "def build_block_parser(md_instance, **kwargs):\n", "code": "parser = BlockParser(md_instance)\nparser.blockprocessors['empty'] = EmptyBlockProcessor(parser)\nparser.blockprocessors['indent'] = ListIndentProcessor(parser)\nparser.blockprocessors['code'] = CodeBlockProcessor(parser)\nparser.blockprocessors['hashheader'] = HashHeaderProcessor(parser)\nparser.blockprocessors['setextheader'] = SetextHeaderProcessor(parser)\nparser.blockprocessors['hr'] = HRProcessor(parser)\nparser.blockprocessors['olist'] = OListProcessor(parser)\nparser.blockprocessors['ulist'] = UListProcessor(parser)\nparser.blockprocessors['quote'] = BlockQuoteProcessor(parser)\nparser.blockprocessors['paragraph'] = ParagraphProcessor(parser)\nreturn parser", "path": "markdown\\blockprocessors.py", "repo_name": "iSECPartners/LibTech-Auditing-Cheatsheet", "stars": 198, "license": "None", "language": "python", "size": 81}
{"docstring": "\"\"\" Process blocks of markdown text and attach to given etree node. \n\nGiven a list of ``blocks``, each blockprocessor is stepped through\nuntil there are no blocks left. While an extension could potentially\ncall this method directly, it's generally expected to be used internally.\n\nThis is a public method as an extension may need to add/alter additional\nBlockProcessors which call this method to recursively parse a nested\nblock.\n\n\"\"\"\n", "func_signal": "def parseBlocks(self, parent, blocks):\n", "code": "while blocks:\n   for processor in self.blockprocessors.values():\n       if processor.test(parent, blocks[0]):\n           processor.run(parent, blocks)\n           break", "path": "markdown\\blockparser.py", "repo_name": "iSECPartners/LibTech-Auditing-Cheatsheet", "stars": 198, "license": "None", "language": "python", "size": 81}
{"docstring": "\"\"\" Return a setting for the given key or an empty string. \"\"\"\n", "func_signal": "def getConfig(self, key, default=''):\n", "code": "if key in self.config:\n    return self.config[key][0]\nelse:\n    return default", "path": "markdown\\extensions\\__init__.py", "repo_name": "iSECPartners/LibTech-Auditing-Cheatsheet", "stars": 198, "license": "None", "language": "python", "size": 81}
{"docstring": "\"\"\" Return unescaped text given text with an inline placeholder. \"\"\"\n", "func_signal": "def unescape(self, text):\n", "code": "try:\n    stash = self.markdown.treeprocessors['inline'].stashed_nodes\nexcept KeyError:\n    return text\ndef get_stash(m):\n    id = m.group(1)\n    if id in stash:\n        return stash.get(id)\nreturn util.INLINE_PLACEHOLDER_RE.sub(get_stash, text)", "path": "markdown\\inlinepatterns.py", "repo_name": "iSECPartners/LibTech-Auditing-Cheatsheet", "stars": 198, "license": "None", "language": "python", "size": 81}
{"docstring": "\"\"\" Return the last child of an etree element. \"\"\"\n", "func_signal": "def lastChild(self, parent):\n", "code": "if len(parent):\n    return parent[-1]\nelse:\n    return None", "path": "markdown\\blockprocessors.py", "repo_name": "iSECPartners/LibTech-Auditing-Cheatsheet", "stars": 198, "license": "None", "language": "python", "size": 81}
{"docstring": "\"\"\" Return unescaped text given text with an inline placeholder. \"\"\"\n", "func_signal": "def unescape(self, text):\n", "code": "try:\n    stash = self.markdown.treeprocessors['inline'].stashed_nodes\nexcept KeyError:\n    return text\ndef get_stash(m):\n    id = m.group(1)\n    value = stash.get(id)\n    if value is not None:\n        try:\n            return self.markdown.serializer(value)\n        except:\n            return '\\%s' % value\n    \nreturn util.INLINE_PLACEHOLDER_RE.sub(get_stash, text)", "path": "markdown\\inlinepatterns.py", "repo_name": "iSECPartners/LibTech-Auditing-Cheatsheet", "stars": 198, "license": "None", "language": "python", "size": 81}
{"docstring": "\"\"\" Get level of indent based on list level. \"\"\"\n# Get indent level\n", "func_signal": "def get_level(self, parent, block):\n", "code": "m = self.INDENT_RE.match(block)\nif m:\n    indent_level = len(m.group(1))/self.tab_length\nelse:\n    indent_level = 0\nif self.parser.state.isstate('list'):\n    # We're in a tightlist - so we already are at correct parent.\n    level = 1\nelse:\n    # We're in a looselist - so we need to find parent.\n    level = 0\n# Step through children of tree to find matching indent level.\nwhile indent_level > level:\n    child = self.lastChild(parent)\n    if child and (child.tag in self.LIST_TYPES or child.tag in self.ITEM_TYPES):\n        if child.tag in self.LIST_TYPES:\n            level += 1\n        parent = child\n    else:\n        # No more child levels. If we're short of indent_level,\n        # we have a code block. So we stop here.\n        break\nreturn level, parent", "path": "markdown\\blockprocessors.py", "repo_name": "iSECPartners/LibTech-Auditing-Cheatsheet", "stars": 198, "license": "None", "language": "python", "size": 81}
{"docstring": "\"\"\" Remove a tab from front of lines but allowing dedented lines. \"\"\"\n", "func_signal": "def looseDetab(self, text, level=1):\n", "code": "lines = text.split('\\n')\nfor i in range(len(lines)):\n    if lines[i].startswith(' '*self.tab_length*level):\n        lines[i] = lines[i][self.tab_length*level:]\nreturn '\\n'.join(lines)", "path": "markdown\\blockprocessors.py", "repo_name": "iSECPartners/LibTech-Auditing-Cheatsheet", "stars": 198, "license": "None", "language": "python", "size": 81}
{"docstring": "\"\"\"Set values of an element based on attribute definitions ({@id=123}).\"\"\"\n", "func_signal": "def handleAttributes(text, parent):\n", "code": "def attributeCallback(match):\n    parent.set(match.group(1), match.group(2).replace('\\n', ' '))\nreturn ATTR_RE.sub(attributeCallback, text)", "path": "markdown\\inlinepatterns.py", "repo_name": "iSECPartners/LibTech-Auditing-Cheatsheet", "stars": 198, "license": "None", "language": "python", "size": 81}
{"docstring": "#print \";----\" + sys._getframe().f_code.co_name + \"----\"\n\n", "func_signal": "def visitAdd(self, node):\n", "code": "lTy = typer.inferType(node.left)\nrTy = typer.inferType(node.right)\nif rTy != lTy:\n    raise PyllvmError(\"CodeGen:  TypeMismatch: lTy = %s, rTy = %s for %s, line %d\" % (lTy, rTy, node, node.lineno))\n\nlLLInst = self.visit(node.left)\nrLLInst = self.visit(node.right)\n\ntmpSym = symbolTable.genUniqueSymbol(lTy)\n\nif( lTy == vec ):\n    return self.emitVAdd(lLLInst, rLLInst)\nelif( lTy == float ):\n    return self.builder.fadd(lLLInst, rLLInst, tmpSym.name)\nelif( lTy == int ):\n    return self.builder.add(lLLInst, rLLInst, tmpSym.name)\nelse:\n    raise PyllvmError(\"CodeGen: arithmatic not supporte for type\", lTy)", "path": "CodeGenLLVM.py", "repo_name": "aherlihy/PythonLLVM", "stars": 130, "license": "None", "language": "python", "size": 2445}
{"docstring": "# get length and type of list\n", "func_signal": "def emitStringInst(self, node):\n", "code": "lenList = len(node.value)\n# get int value for each char and put in list\narrTy = llvm.core.Type.array(llIntType, lenList)\nl_ptr = self.builder.alloca_array(arrTy, llvm.core.Constant.int(llIntType, lenList))\n# populate str\nzero = llvm.core.Constant.int(llIntType, 0)\nfor i in range(lenList):\n    index = llvm.core.Constant.int(llIntType, i)\n    v = llvm.core.Constant.int(llIntType, ord(node.value[i]))\n    l = self.builder.gep(l_ptr, [zero, index])\n    self.builder.store(v, l)\n\nreturn l_ptr", "path": "CodeGenLLVM.py", "repo_name": "aherlihy/PythonLLVM", "stars": 130, "license": "None", "language": "python", "size": 2445}
{"docstring": "# create vector to return\n", "func_signal": "def emitVSub(self, lLLInst, rLLInst):\n", "code": "ret_v = llvm.core.Constant.vector([llvm.core.Constant.real(llFloatType, \"0.0\")] * 4)\nfor i in range(4):\n    i0 = llvm.core.Constant.int(llIntType, i);\n    tmp0  = symbolTable.genUniqueSymbol(float)\n    tmp1  = symbolTable.genUniqueSymbol(float)\n    a1   = self.builder.extract_element(lLLInst, i0, tmp0.name)\n    a2   = self.builder.extract_element(rLLInst, i0, tmp1.name)\n    a = self.builder.fsub(a1, a2)\n    ret_v = self.builder.insert_element(ret_v, a, i0)\nreturn ret_v", "path": "CodeGenLLVM.py", "repo_name": "aherlihy/PythonLLVM", "stars": 130, "license": "None", "language": "python", "size": 2445}
{"docstring": "#print \";----\" + sys._getframe().f_code.co_name + \"----\"\n\n", "func_signal": "def getExternalSymbolInstruction(self, name):\n", "code": "if self.externals.has_key(name):\n    return self.externals[name]\nelse:\n    raise PyllvmError(\"CodeGen:  Unknown external symbol:\", name, self.externals)", "path": "CodeGenLLVM.py", "repo_name": "aherlihy/PythonLLVM", "stars": 130, "license": "None", "language": "python", "size": 2445}
{"docstring": "#print \";----\" + sys._getframe().f_code.co_name + \" = \" + str(value) + \"----\"\n\n", "func_signal": "def mkLLConstInst(self, ty, value):\n", "code": "llTy   = toLLVMTy(ty)\nbufSym = symbolTable.genUniqueSymbol(ty)\ntmpSym = symbolTable.genUniqueSymbol(ty)\n\n# %tmp  = alloca ty\n# store ty val, %tmp\n# %inst = load ty, %tmp\n\nallocInst = self.builder.alloca(llTy, bufSym.name)\n\nllConst   = None\nif llTy   == llIntType:\n    llConst = llvm.core.Constant.int(llIntType, value)\n\nelif llTy == llFloatType:\n    llConst = llvm.core.Constant.real(llFloatType, value)\n\nelif llTy == llFVec4Type:\n    raise PyllvmError(\"CodeGen:  muda\")\n\nstoreInst = self.builder.store(llConst, allocInst)\nloadInst  = self.builder.load(allocInst, tmpSym.name)\n\n\nreturn loadInst", "path": "CodeGenLLVM.py", "repo_name": "aherlihy/PythonLLVM", "stars": 130, "license": "None", "language": "python", "size": 2445}
{"docstring": "#print \";----\" + sys._getframe().f_code.co_name + \"----\"\n\n", "func_signal": "def visitMul(self, node):\n", "code": "lTy = typer.inferType(node.left)\nrTy = typer.inferType(node.right)\n\nif rTy != lTy:\n    raise PyllvmError(\"CodeGen:  TypeMismatch: lTy = %s, rTy = %s for %s, line %d\" % (lTy, rTy, node, node.lineno))\n\nlLLInst = self.visit(node.left)\nrLLInst = self.visit(node.right)\n\ntmpSym = symbolTable.genUniqueSymbol(lTy)\n\nif( lTy == vec ):\n    return self.emitVMul(lLLInst, rLLInst)\nif( lTy == float ):\n    return self.builder.fmul(lLLInst, rLLInst, tmpSym.name)\nelif( lTy == int ):\n    return self.builder.mul(lLLInst, rLLInst, tmpSym.name)\nelse:\n    raise PyllvmError(\"CodeGen: arithmatic not supporte for type\", lTy)", "path": "CodeGenLLVM.py", "repo_name": "aherlihy/PythonLLVM", "stars": 130, "license": "None", "language": "python", "size": 2445}
{"docstring": "\"\"\"\nFind a symbol with name.\nIf a symbol was not found, return None.\n\"\"\"\n\n", "func_signal": "def find(self, name):\n", "code": "for i in range(len(self.symbols)):\n\n    d = self.symbols[i][1]\n    if d.has_key(name):\n        return d[name]\n\nreturn None", "path": "SymbolTable.py", "repo_name": "aherlihy/PythonLLVM", "stars": 130, "license": "None", "language": "python", "size": 2445}
{"docstring": "# NOTE: could use built-in llvm abs function also.\n        # NOTE: These math functions don't throw errors if you pass in bad numbers (i.e. sqrt of negative)\n        # integer abs\n", "func_signal": "def emitMmath(self):\n", "code": "        abs_funcType = llvm.core.Type.function(llIntType, [llIntType])\n        mabs = self.module.add_function(abs_funcType, 'iabs')\n        self._mabs = mabs\n        bb = mabs.append_basic_block('absb')\n        b = llvm.core.Builder.new(bb)\n        \n        result = b.icmp(llvm.core.ICMP_SLE, mabs.args[0], llvm.core.Constant.int(llIntType, 0), 'cmptmp')\n        function = b.basic_block.function\n        then_block = function.append_basic_block('abs_then')\n        else_block = function.append_basic_block('abs_else')\n        b.cbranch(result, then_block, else_block) \n        b.position_at_end(then_block)\n        pos = b.sub(llvm.core.Constant.int(llIntType, 0), mabs.args[0])\n        b.ret(pos)\n        b.position_at_end(else_block)\n        b.ret(mabs.args[0])\n        #fabs\n        fabs_funcType = llvm.core.Type.function(llFloatType, [llFloatType])\n        fmabs = self.module.add_function(fabs_funcType, 'fabs')\n        self._fmabs = fmabs\n        bf = fmabs.append_basic_block('fabsb')\n        b.position_at_end(bf)\n        \n        fresult = b.fcmp(llvm.core.FCMP_OLE, fmabs.args[0], llvm.core.Constant.real(llFloatType, 0.0), 'cmp2tmp')\n        ffunction = b.basic_block.function\n        fthen_block = ffunction.append_basic_block('fabs_then')\n        felse_block = ffunction.append_basic_block('fabs_else')\n        b.cbranch(fresult, fthen_block, felse_block) \n        b.position_at_end(fthen_block)\n        posf = b.fsub(llvm.core.Constant.real(llFloatType, 0.0), fmabs.args[0])\n        b.ret(posf)\n        b.position_at_end(felse_block)\n        b.ret(fmabs.args[0])\n# sqrt \n        funcType = llvm.core.Type.function(llvm.core.Type.double(), [llvm.core.Type.double()], False)\n        self.sqrt = self.module.add_function(funcType, 'sqrt')\n        \n        fsqrt_funcType = llvm.core.Type.function(llFloatType, [llFloatType])\n        fsqrt = self.module.add_function(fsqrt_funcType, '_fsqrt')\n        self._fsqrt = fsqrt\n        \n        bfs = fsqrt.append_basic_block('fsqrt')\n        b.position_at_end(bfs)\n        \n        d2f = b.fpext(fsqrt.args[0], llvm.core.Type.double(), 'f2d')\n        r = b.call(self.sqrt, [d2f], 'root')\n        retd = b.fptrunc(r, llFloatType, 'd2f')\n        b.ret(retd)\n# pow\n        funcType = llvm.core.Type.function(llvm.core.Type.double(), [llvm.core.Type.double(), llvm.core.Type.double()], False)\n        self.pow = self.module.add_function(funcType, 'pow')\n        \n        fpow_funcType = llvm.core.Type.function(llFloatType, [llFloatType, llFloatType])\n        fpow = self.module.add_function(fpow_funcType, '_fpow')\n        self._fpow = fpow\n        \n        bfp = fpow.append_basic_block('bfpow')\n        b.position_at_end(bfp)\n        \n        f2d_base = b.fpext(fpow.args[0], llvm.core.Type.double(), 'f2d_base')\n        f2d_exp = b.fpext(fpow.args[1], llvm.core.Type.double(), 'f2d_exp')\n        r = b.call(self.pow, [f2d_base, f2d_exp], 'pow_res')\n        retd = b.fptrunc(r, llFloatType, 'd2f_pow')\n        b.ret(retd)\n# log\n        funcType = llvm.core.Type.function(llvm.core.Type.double(), [llvm.core.Type.double()], False)\n        self.log = self.module.add_function(funcType, 'log')\n        \n        flog_funcType = llvm.core.Type.function(llFloatType, [llFloatType])\n        flog = self.module.add_function(flog_funcType, '_flog')\n        self._flog = flog\n        \n        bfp = flog.append_basic_block('bflog')\n        b.position_at_end(bfp)\n        \n        f2d_log = b.fpext(flog.args[0], llvm.core.Type.double(), 'f2d_log')\n        r = b.call(self.log, [f2d_log], 'log_res')\n        retd = b.fptrunc(r, llFloatType, 'd2f_log')\n        b.ret(retd)\n# exp\n        funcType = llvm.core.Type.function(llvm.core.Type.double(), [llvm.core.Type.double()], False)\n        self.exp = self.module.add_function(funcType, 'exp')\n        \n        exp_funcType = llvm.core.Type.function(llFloatType, [llFloatType])\n        exp = self.module.add_function(exp_funcType, '_exp')\n        self._exp = exp\n        \n        bfp = exp.append_basic_block('bexp')\n        b.position_at_end(bfp)\n        \n        f2d_exp = b.fpext(exp.args[0], llvm.core.Type.double(), 'f2d_exp')\n        r = b.call(self.exp, [f2d_exp], 'exp_res')\n        retd = b.fptrunc(r, llFloatType, 'd2f_exp')\n        b.ret(retd)", "path": "CodeGenLLVM.py", "repo_name": "aherlihy/PythonLLVM", "stars": 130, "license": "None", "language": "python", "size": 2445}
{"docstring": "#print \";----\" + sys._getframe().f_code.co_name + \"----\"\n\n", "func_signal": "def visitDiv(self, node):\n", "code": "lTy = typer.inferType(node.left)\nrTy = typer.inferType(node.right)\n\nif rTy != lTy:\n    raise PyllvmError(\"CodeGen:  TypeMismatch: lTy = %s, rTy = %s for %s, line %d\" % (lTy, rTy, node, node.lineno))\n\nlLLInst = self.visit(node.left)\nrLLInst = self.visit(node.right)\n\ntmpSym = symbolTable.genUniqueSymbol(lTy)\n\nif( lTy == vec ):\n    return self.emitVDiv(lLLInst, rLLInst)\nif typer.isFloatType(lTy):\n    divInst = self.builder.fdiv(lLLInst, rLLInst, tmpSym.name)\n#if lTy == int:\n#    divInst = self.builder.sdiv(lLLInst, rLLInst, tmpSym.name)\nelse:\n    divInst = self.builder.udiv(lLLInst, rLLInst, tmpSym.name)\n\nreturn divInst", "path": "CodeGenLLVM.py", "repo_name": "aherlihy/PythonLLVM", "stars": 130, "license": "None", "language": "python", "size": 2445}
{"docstring": "#print \";----\" + sys._getframe().f_code.co_name + \"----\"\n", "func_signal": "def visitAssign(self, node):\n", "code": "if len(node.nodes) != 1:\n    raise PyllvmError(\"CodeGen: assignment to multiple nodes not supported\", node)\n\nrTy     = typer.inferType(node.expr)\n# if this is a list, will be a pointer. Otherwise a value\nrLLInst = self.visit(node.expr)\nlhsNode = node.nodes[0]\nlTy = None\nif isinstance(lhsNode, compiler.ast.AssName):\n    sym = symbolTable.find(lhsNode.name)\n    if sym is None:\n        # The variable appears here firstly.\n      \n        # alloc storage\n        # if array, already alloca'd in visit() so will set value to pointer\n        if(rTy==list):\n            listType = None\n            listLen = 0\n            listType, listLen, isStr = self.getListDim(node.expr)\n            llTy = llvm.core.Type.pointer(llvm.core.Type.array(toLLVMTy(listType), listLen))\n            # create space for LHS node of type llTy, addr in llStorage\n            llStorage = self.builder.alloca(llTy, lhsNode.name)\n            sym = Symbol(lhsNode.name, rTy, \"variable\", llstorage = llStorage, dim=(listType, listLen, isStr))\n            symbolTable.append(sym)\n        else:\n            # get type of new value\n            llTy = toLLVMTy(rTy)\n            # create space for LHS node, addr in llStorage\n            llStorage = self.builder.alloca(llTy, lhsNode.name)\n            # create symbol for LHS, with name lhsNode.name and address of newly made space\n            sym = Symbol(lhsNode.name, rTy, \"variable\", llstorage = llStorage)\n            # add to symbol table\n            symbolTable.append(sym)\n\n        lTy = rTy\n\n    else:\n        # symbol is already defined.\n        lTy = sym.type\nelif isinstance(lhsNode, compiler.ast.Subscript):\n    return self.emitListAssign(lhsNode, rLLInst)\n\nelse:\n    raise PyllvmError(\"CodeGen:  assigning to non-mutable type:\", lhsNode)\n\nif rTy != lTy:\n    raise PyllvmError(\"CodeGen:  TypeMismatch: lTy = %s, rTy = %s. Cannot dynamically reassign vars to different types\" % (lTy, rTy))\n\n# get space for LHS node\nlSym = symbolTable.find(lhsNode.name)\n# store value of RHS into address of LHS\n# if this is list, storing the value (i.e. address) of rLLInst into L\nstoreInst = self.builder.store(rLLInst, lSym.llstorage)\n# No return", "path": "CodeGenLLVM.py", "repo_name": "aherlihy/PythonLLVM", "stars": 130, "license": "None", "language": "python", "size": 2445}
{"docstring": "# create vector to return\n", "func_signal": "def emitVDiv(self, lLLInst, rLLInst):\n", "code": "ret_v = llvm.core.Constant.vector([llvm.core.Constant.real(llFloatType, \"0.0\")] * 4)\nfor i in range(4):\n    i0 = llvm.core.Constant.int(llIntType, i);\n    tmp0  = symbolTable.genUniqueSymbol(float)\n    tmp1  = symbolTable.genUniqueSymbol(float)\n    a1   = self.builder.extract_element(lLLInst, i0, tmp0.name)\n    a2   = self.builder.extract_element(rLLInst, i0, tmp1.name)\n    a = self.builder.fdiv(a1, a2)\n    ret_v = self.builder.insert_element(ret_v, a, i0)\nreturn ret_v", "path": "CodeGenLLVM.py", "repo_name": "aherlihy/PythonLLVM", "stars": 130, "license": "None", "language": "python", "size": 2445}
{"docstring": "#print \";----\" + sys._getframe().f_code.co_name + \"----\"\n\n", "func_signal": "def visitAnd(self, node):\n", "code": "if typer.inferType(node.nodes[0]) == vec:\n    if len(node.nodes) != 2:\n        raise PyllvmError(\"CodeGen:  and only supported for 2 vectors\")\n    l = self.visit(node.nodes[0])\n    r = self.visit(node.nodes[1])\n    return self.emitVAnd('==', l,r)\n\na = self.visit(node.nodes[0])\na_sym = symbolTable.genUniqueSymbol(llTruthType)\na_int = self.builder.fptoui(a, llTruthType, a_sym.name)\n\nfor i in range(1, len(node.nodes)):\n    b = self.visit(node.nodes[i])\n    b_sym = symbolTable.genUniqueSymbol(llTruthType)\n    b_int = self.builder.fptoui(b, llTruthType, b_sym.name)\n    \n    c_sym = symbolTable.genUniqueSymbol(llTruthType)\n    c_int = self.builder.and_(a_int, b_int, c_sym.name)\n    a_int = c_int\n\nreturn self.builder.uitofp(a_int, llFloatType, 'andtmp')", "path": "CodeGenLLVM.py", "repo_name": "aherlihy/PythonLLVM", "stars": 130, "license": "None", "language": "python", "size": 2445}
{"docstring": "#print \";----\" + sys._getframe().f_code.co_name + \"----\"\n\n", "func_signal": "def visitGetattr(self, node):\n", "code": "d = { 'x' : llvm.core.Constant.int(llIntType, 0)\n    , 'y' : llvm.core.Constant.int(llIntType, 1)\n    , 'z' : llvm.core.Constant.int(llIntType, 2)\n    , 'w' : llvm.core.Constant.int(llIntType, 3)\n    }\n\n\nty = typer.inferType(node)\n\nrLLInst  = self.visit(node.expr)\ntmpSym   = symbolTable.genUniqueSymbol(ty)\n\nif len(node.attrname) == 1:\n    # emit extract element\n    s = node.attrname[0]\n\n    inst = self.builder.extract_element(rLLInst, d[s], tmpSym.name)\n\nreturn inst", "path": "CodeGenLLVM.py", "repo_name": "aherlihy/PythonLLVM", "stars": 130, "license": "None", "language": "python", "size": 2445}
{"docstring": "#print \";----\" + sys._getframe().f_code.co_name + \"----\"\n\n", "func_signal": "def visitOr(self, node):\n", "code": "a = self.visit(node.nodes[0])\na_sym = symbolTable.genUniqueSymbol(llTruthType)\na_int = self.builder.fptoui(a, llTruthType, a_sym.name)\n\nfor i in range(1, len(node.nodes)):\n    b = self.visit(node.nodes[i])\n    b_sym = symbolTable.genUniqueSymbol(llTruthType)\n    b_int = self.builder.fptoui(b, llTruthType, b_sym.name)\n    \n    c_sym = symbolTable.genUniqueSymbol(llTruthType)\n    c_int = self.builder.or_(a_int, b_int, c_sym.name)\n    a_int = c_int\n\nreturn self.builder.uitofp(a_int, llFloatType, 'ortmp')", "path": "CodeGenLLVM.py", "repo_name": "aherlihy/PythonLLVM", "stars": 130, "license": "None", "language": "python", "size": 2445}
{"docstring": "#print \";----\" + sys._getframe().f_code.co_name + \"----\"\n# get type of return node\n", "func_signal": "def visitReturn(self, node):\n", "code": "ty   = typer.inferType(node.value)\nif(ty==list):\n    if self.currFuncRetType is None:\n        self.currFuncRetType = ty\n        self.prevFuncRetNode = node\n    return self.builder.ret(self.emitMakeArray(node.value))\n\n\n# Return(Const(None))\nif isinstance(node.value, compiler.ast.Const):\n    if node.value.value == None:\n        self.currFuncRetType = void\n        self.currFuncRetList = (None, 0, False)\n        self.prevFuncRetNode = node\n        return self.builder.ret_void()\n\nexpr = self.visit(node.value)\n\nif self.currFuncRetType is None:\n    self.currFuncRetType = ty\n    self.prevFuncRetNode = node\n\nelif self.currFuncRetType != ty:\n    raise PyllvmError(\"CodeGen:  Different type for return expression: expected %s(lineno=%d, %s) but got %s(lineno=%d, %s)\" % (self.currFuncRetType, self.prevFuncRetNode.lineno, self.prevFuncRetNode, ty, node.lineno, node))\n\nreturn self.builder.ret(expr)", "path": "CodeGenLLVM.py", "repo_name": "aherlihy/PythonLLVM", "stars": 130, "license": "None", "language": "python", "size": 2445}
{"docstring": "#print \";----\" + sys._getframe().f_code.co_name + \"----\"\n", "func_signal": "def visitIf(self, node):\n", "code": "is_else = (node.else_ is not None)\ncond = self.getTruthy(node.tests[0][0])\nthen_ret, then_type = self.testRet(node.tests[0][1])\nif(is_else):\n    else_ret, else_type = self.testRet(node.else_)\ncondition_bool = self.builder.fcmp(llvm.core.FCMP_ONE, cond, llvm.core.Constant.real(llFloatType, 0), 'ifcond')\n# get function\nfunction = self.builder.basic_block.function\n\n# create blocks\nthen_block = function.append_basic_block('if_then')\nif(is_else):\n    else_block = function.append_basic_block('if_else')\nmerge_block = function.append_basic_block('if_merge')\nif(is_else):\n    self.builder.cbranch(condition_bool, then_block, else_block) \nelse:\n    self.builder.cbranch(condition_bool, then_block, merge_block) \n    \n# emit then\nself.builder.position_at_end(then_block)\nsymbolTable.pushScope(\"if_\")\nthen_val = self.visit(node.tests[0][1])\nsymbolTable.popScope() \nself.builder.branch(merge_block)\n# update then for phi \nthen_block = self.builder.basic_block\nif(is_else):\n    # emit else\n    self.builder.position_at_end(else_block)\n    symbolTable.pushScope(\"else_\")\n    else_val = self.visit(node.else_)\n    symbolTable.popScope()\n    self.builder.branch(merge_block)\n    else_block = self.builder.basic_block\n\n# emit merge\nself.builder.position_at_end(merge_block)\nif(is_else):\n    if else_ret and then_ret:\n        if(else_type != then_type):\n            raise PyllvmError(\"CodeGen: unable to have if statement blocks that have different return types\")\n        # get null value and return\n        return self.builder.unreachable()#self.builder.ret(llvm.core.Constant.null(else_type))\n    elif not (not else_ret and not then_ret):\n        raise PyllvmError(\"CodeGen: unable to have if statement blocks that return if the else doesn't also return\")", "path": "CodeGenLLVM.py", "repo_name": "aherlihy/PythonLLVM", "stars": 130, "license": "None", "language": "python", "size": 2445}
{"docstring": "#print \";----\" + sys._getframe().f_code.co_name + \"----\"\n\n", "func_signal": "def __init__(self):\n", "code": "self.body             = \"\"\nself.globalscope      = \"\"\n\nself.module           = llvm.core.Module.new(\"module\")\nself.funcs            = []\nself.func             = None # Current function\nself.builder          = None\n\nself.currFuncRetType  = None\nself.currListFuncRet  = (None, 0)\nself.prevFuncRetNode  = None    # for reporiting err\n\nself.externals        = {}\nself.newline          = None\nself.printf           = None\nself.vec              = None\nself.mmath                 = mMathFuncs(self)\nself.typer            = typer", "path": "CodeGenLLVM.py", "repo_name": "aherlihy/PythonLLVM", "stars": 130, "license": "None", "language": "python", "size": 2445}
{"docstring": "\"\"\"\nFind a symbol with name.\nIf a symbol was not found, raise a exeption.\n\"\"\"\n\n", "func_signal": "def lookup(self, name):\n", "code": "for i in range(len(self.symbols)):\n\n    d = self.symbols[i][1]\n    if d.has_key(name):\n        return d[name]\n\nraise PyllvmError(\"Symbol Table: Undefine symbol: \", name)", "path": "SymbolTable.py", "repo_name": "aherlihy/PythonLLVM", "stars": 130, "license": "None", "language": "python", "size": 2445}
{"docstring": "\"\"\"\nGenerate unique symbol.\n\"\"\"\n", "func_signal": "def genUniqueSymbol(self, type):\n", "code": "nMax = 1000\n\nbaseName = \"tmp\" \n\ndone = False\ni = 0\nwhile 1:\n\n    name = baseName + str(self.genNum)\n\n    if self.find(name) == None:\n\n        newSym = Symbol(name, type, \"variable\")\n        self.append(newSym)\n\n        return newSym\n\n    self.genNum += 1\n    i           += 1\n    \n    if i > nMax:\n        raise PyllvmError(\"Symbol Table: Can't define unique symbol.\")", "path": "SymbolTable.py", "repo_name": "aherlihy/PythonLLVM", "stars": 130, "license": "None", "language": "python", "size": 2445}
{"docstring": "#print \";----\" + sys._getframe().f_code.co_name + \"----\"\n", "func_signal": "def visitNot(self, node):\n", "code": "e = self.visit(node.expr) \ne_sym = symbolTable.genUniqueSymbol(llTruthType)\ne_int = self.builder.fptoui(e, llTruthType, e_sym.name)\n\nnot_sym = symbolTable.genUniqueSymbol(llTruthType)\ne_not = self.builder.not_(e_int, not_sym.name)\n\nret_sym = symbolTable.genUniqueSymbol(llTruthType)\nreturn self.builder.uitofp(e_not, llFloatType, ret_sym.name)", "path": "CodeGenLLVM.py", "repo_name": "aherlihy/PythonLLVM", "stars": 130, "license": "None", "language": "python", "size": 2445}
{"docstring": "# this code is writte by jan Schluter\n# copied from https://github.com/benanne/Lasagne/issues/12\n", "func_signal": "def threaded_generator(generator, num_cached=50):\n", "code": "import Queue\nqueue = Queue.Queue(maxsize=num_cached)\nsentinel = object()  # guaranteed unique reference\n\n# define producer (putting items into queue)\ndef producer():\n    for item in generator:\n        queue.put(item)\n    queue.put(sentinel)\n\n# start producer (in a background thread)\nimport threading\nthread = threading.Thread(target=producer)\nthread.daemon = True\nthread.start()\n\n# run as consumer (read items from queue, in current thread)\nitem = queue.get()\nwhile item is not sentinel:\n    yield item\n    queue.task_done()\n    item = queue.get()", "path": "deepmodels\\batchiterator.py", "repo_name": "skaae/lasagne-draw", "stars": 200, "license": "None", "language": "python", "size": 14672}
{"docstring": "'''\n\nGenerate digits see http://arxiv.org/abs/1502.04623v1 section 2.3\n\n'''\n\n", "func_signal": "def generate(self, n_digits, y=None, *args, **kwargs):\n", "code": "if y is None and self.use_y is True:\n        raise ValueError('y must be given when use_y is true')\ndef step(z,\n         cell_previous_dec, hid_previous_dec,\n         canvas_previous, l_write_previous,\n         y,\n         W_z_to_gates_dec, b_gates_dec,\n         W_hid_to_gates_dec,\n         W_celldec_to_dec_gates,\n         W_dec_to_canvas_patch, W_write, b_write\n         ):\n    N_write =self.N_filters_write\n    img_shp = self.imgshp\n\n\n    # DECODER\n    if self.use_y:\n        print('STEP: using Y')\n        in_gates_dec = T.concatenate([y, z], axis=1)\n    else:\n        print('STEP: Not using Y')\n        in_gates_dec = z\n\n\n    gates_dec = T.dot(in_gates_dec, W_z_to_gates_dec) + b_gates_dec\n    gates_dec += T.dot(hid_previous_dec, W_hid_to_gates_dec)\n    # equation (7)\n    cell_dec, hid_dec = self._lstm(gates_dec,\n                             cell_previous_dec,\n                             W_celldec_to_dec_gates,\n                             self.nonlinearity_out_decoder)\n\n    # WRITE\n    l_write = T.dot(hid_dec, W_write) + b_write\n    w = T.dot(hid_dec, W_dec_to_canvas_patch)\n    att_write = nn2att(l_write, N_write,  img_shp)\n    canvas_upd = write(w, att_write, N_write, img_shp)\n    canvas_upd = 1.0/(att_write['gamma']+1e-4) * canvas_upd\n    canvas = canvas_previous + canvas_upd\n\n    return [cell_dec, hid_dec, canvas, l_write]\n\n\nones = T.ones((n_digits,1))\nif theano.config.compute_test_value is 'off':\n    z_samples = _srng.normal((self.glimpses,n_digits,self.dimz))\nelse:\n    print(\"draw.py: is not using random generator\"+\"!#>\"*30)\n    z_samples = T.ones(\n        (self.glimpses,n_digits,self.dimz), theano.config.floatX) * 0.3\n\nif y is None:\n    y = T.zeros((1))\natt_vals_write_init = T.zeros((n_digits, 5))\nseqs = [z_samples]\ninit = [T.dot(ones, self.cell_init_dec), T.dot(ones, self.hid_init_dec),\n        T.dot(ones, self.canvas_init),att_vals_write_init]\nnon_seqs = [y, self.W_z_to_gates_dec, self.b_gates_dec,\n         self.W_hid_to_gates_dec,\n         self.W_celldec_to_dec_gates,\n         self.W_dec_to_canvas_patch,\n         self.W_write, self.b_write]\n\noutput_scan = theano.scan(step, sequences=seqs,\n                     outputs_info=init,\n                     non_sequences=non_seqs,\n                     go_backwards=False)[0]\n\n\ncanvas = output_scan[2]\nl_write = output_scan[3]\nreturn T.nnet.sigmoid(canvas.dimshuffle(1,0,2)), l_write.dimshuffle(1,0,2)", "path": "deepmodels\\layers\\draw.py", "repo_name": "skaae/lasagne-draw", "stars": 200, "license": "None", "language": "python", "size": 14672}
{"docstring": "\"\"\"\nCalculates global accuracy\n:return: accuracy\n:example: >>> conf = ConfusionMatrix(3)\n          >>> conf.batchAdd([0,0,1],[0,0,2])\n          >>> print conf.accuracy()\n\"\"\"\n", "func_signal": "def accuracy(self):\n", "code": "tp, _, _, _ = self.getErrors()\nn_samples = np.sum(self.mat)\nreturn np.sum(tp) / n_samples", "path": "deepmodels\\confusionmatrix.py", "repo_name": "skaae/lasagne-draw", "stars": 200, "license": "None", "language": "python", "size": 14672}
{"docstring": "\"\"\"Transform a value into a shared variable of type floatX.\nParameters\n----------\nvalue : :class:`~numpy.ndarray`\nThe value to associate with the Theano shared.\nname : :obj:`str`, optional\nThe name for the shared variable. Defaults to `None`.\nborrow : :obj:`bool`, optional\nIf set to True, the given `value` will not be copied if possible.\nThis can save memory and speed. Defaults to False.\ndtype : :obj:`str`, optional\nThe `dtype` of the shared variable. Default value is\n:attr:`config.floatX`.\nReturns\n-------\n:class:`tensor.TensorSharedVariable`\nA Theano shared variable with the requested value and `dtype`.\n\"\"\"\n", "func_signal": "def shared_floatx(value, name=None, borrow=False, dtype=None):\n", "code": "if dtype is None:\n    dtype = theano.config.floatX\nreturn theano.shared(theano._asarray(value, dtype=dtype),\n        name=name,\n        borrow=borrow)", "path": "examples\\helper_functions.py", "repo_name": "skaae/lasagne-draw", "stars": 200, "license": "None", "language": "python", "size": 14672}
{"docstring": "\"\"\"\nThis is copied from jbornschein code:\nhttps://github.com/jbornschein/draw/blob/master/draw/attention.py\n\nBatched version of dot-product.\nFor A[dim_1, dim_2, dim_3] and B[dim_1, dim_3, dim_4] this\nis \\approx equal to:\nfor i in range(dim_1):\nC[i] = tensor.dot(A, B)\nReturns\n-------\nC : shape (dim_1 \\times dim_2 \\times dim_4)\n\"\"\"\n", "func_signal": "def my_batched_dot(A, B):\n", "code": "C = A.dimshuffle([0,1,2,'x']) * B.dimshuffle([0,'x',1,2])\nreturn C.sum(axis=-2)", "path": "deepmodels\\layers\\draw_helpers.py", "repo_name": "skaae/lasagne-draw", "stars": 200, "license": "None", "language": "python", "size": 14672}
{"docstring": "\"\"\" Plot 100 MNIST images in a 10 by 10 table. Note that we crop\nthe images so that they appear reasonably close together. The\nimage is post-processed to give the appearance of being continued.\"\"\"\n#image = np.concatenate(images, axis=1)\n", "func_signal": "def plot_n_by_n_images(images,epoch=None,folder=None, n = 10, shp=[28,28]):\n", "code": "i = 0\na,b = shp\nimg_out = np.zeros((a*n, b*n))\nfor x in range(n):\n    for y in range(n):\n        xa,xb = x*a, (x+1)*b\n        ya,yb = y*a, (y+1)*b\n        im = np.reshape(images[i], (a,b))\n        img_out[xa:xb, ya:yb] = im\n        i+=1\n#matshow(img_out*100.0, cmap = matplotlib.cm.binary)\nimg_out = (255*img_out).astype(np.uint8)\nimg_out = Image.fromarray(img_out)\nif folder is not None and epoch is not None:\n    img_out.save(os.path.join(folder,epoch + \".png\"))\nreturn img_out", "path": "examples\\helper_functions.py", "repo_name": "skaae/lasagne-draw", "stars": 200, "license": "None", "language": "python", "size": 14672}
{"docstring": "\"\"\"\nCalculate differetn error types\n:return: vetors of true postives (tp) false negatives (fn), false positives (fp) and true negatives (tn)\n         pos 0 is first class, pos 1 is second class etc.\n\"\"\"\n", "func_signal": "def getErrors(self):\n", "code": "tp = np.asarray(np.diag(self.mat).flatten(),dtype='float')\nfn = np.asarray(np.sum(self.mat, axis=1).flatten(),dtype='float') - tp\nfp = np.asarray(np.sum(self.mat, axis=0).flatten(),dtype='float') - tp\ntn = np.asarray(np.sum(self.mat)*np.ones(self.n_classes).flatten(),dtype='float') - tp - fn - fp\nreturn tp,fn,fp,tn", "path": "deepmodels\\confusionmatrix.py", "repo_name": "skaae/lasagne-draw", "stars": 200, "license": "None", "language": "python", "size": 14672}
{"docstring": "\"\"\"Computes the total L2 norm of a set of tensors.\n\nConverts all operands to :class:`~tensor.TensorVariable`\n(see :func:`~tensor.as_tensor_variable`).\n\nParameters\n----------\ntensors : iterable of :class:`~tensor.TensorVariable` (or compatible)\n    The tensors.\n\n\"\"\"\n", "func_signal": "def l2_norm(tensors):\n", "code": "flattened = [T.as_tensor_variable(t).flatten() for t in tensors]\nflattened = [(t if t.ndim > 0 else t.dimshuffle('x'))\n             for t in flattened]\njoined = T.join(0, *flattened)\nreturn T.sqrt(T.sqr(joined).sum())", "path": "examples\\helper_functions.py", "repo_name": "skaae/lasagne-draw", "stars": 200, "license": "None", "language": "python", "size": 14672}
{"docstring": "\"\"\"Rescales an entire step if its L2 norm exceeds a threshold.\n\nWhen the previous steps are the gradients, this step rule performs\ngradient clipping.\n\nParameters\n----------\nthreshold : float, optional\n    The maximum permitted L2 norm for the step. The step\n    will be rescaled to be not higher than this quanity.\n    If ``None``, no rescaling will be applied.\n\nAttributes\n----------\nthreshold : :class:`.tensor.TensorSharedVariable`\n    The shared variable storing the clipping threshold used.\n\n\"\"\"\n# calculate the step size as the previous values minus the\n", "func_signal": "def step_clipping(steps, threshold, to_zero=False):\n", "code": "threshold = shared_floatx(threshold)\n\nnorm = l2_norm(steps)   # return total norm\nif to_zero:\n    print(\"clipping to zero\")\n    scale = 1e-8  # smallstep\nelse:\n    scale = threshold / norm\nmultiplier = T.switch(norm < threshold,\n                            1.0, scale)\n\nreturn [step*multiplier for step in steps], norm, multiplier", "path": "examples\\helper_functions.py", "repo_name": "skaae/lasagne-draw", "stars": 200, "license": "None", "language": "python", "size": 14672}
{"docstring": "# calucate row and column sums\n", "func_signal": "def __str__(self):\n", "code": "col_sum = np.sum(self.mat, axis=1)\nrow_sum = np.sum(self.mat, axis=0)\n\ns = []\n\nmat_str = self.mat.__str__()\nmat_str = mat_str.replace('[','').replace(']','').split('\\n')\n\nfor idx,row in enumerate(mat_str):\n    if idx == 0:\n        pad = \" \"\n    else:\n        pad = \"\"\n    class_name = self.class_names[idx]\n    class_name = \" \" + class_name + \" |\"\n    row_str =  class_name + pad +  row\n    row_str += \" |\" + str(col_sum[idx])\n    s.append(row_str)\n\n\nrow_sum =  [(self.max_len+4)*\" \"+\" \".join(map(str, row_sum))]\nhline = [(1+self.max_len)*\" \"+\"-\"*len(row_sum[0])]\n\ns = hline + s + hline + row_sum\n\n# add linebreaks\ns_out = [line+'\\n' for line in s]\n\n\nreturn \"\".join(s_out)", "path": "deepmodels\\confusionmatrix.py", "repo_name": "skaae/lasagne-draw", "stars": 200, "license": "None", "language": "python", "size": 14672}
{"docstring": "\"\"\"\nParameters\n----------\nimages : T.matrix    (shape: batch_size x img_size)\n    Batch of images. Internally it will be reshaped to be a\n    (batch_size, img_height, img_width)-shaped stack of images.\ncenter_y : T.vector (shape: batch_size)\ncenter_x : T.vector (shape: batch_size)\ndelta : T.vector    (shape: batch_size)\nsigma : T.vector    (shape: batch_size)\n\nReturns\n-------\nwindow : T.matrix   (shape: batch_size x N**2)\n\"\"\"\n", "func_signal": "def read(images,images_err, att_read, N, imgshp):\n", "code": "center_y = att_read['center_y']\ncenter_x = att_read['center_x']\ndelta = att_read['delta']\nsigma = att_read['sigma']\nA, B = imgshp\nbatch_size = images.shape[0]\n\n# Reshape input into proper 2d images\nI = images.reshape( (batch_size, A, B) )\nI_err = images_err.reshape( (batch_size, A, B) )\n\n# Get separable filterbank\nFY, FX = filterbank_matrices(center_y, center_x, delta, sigma, N, imgshp)\n\n# apply to the batch of images\n\nW = my_batched_dot(my_batched_dot(FY, I), FX.transpose([0,2,1]))\nW_err = my_batched_dot(my_batched_dot(FY, I_err), FX.transpose([0,2,1]))\n\nreturn W.reshape((batch_size, N*N)), W_err.reshape((batch_size, N*N))", "path": "deepmodels\\layers\\draw_helpers.py", "repo_name": "skaae/lasagne-draw", "stars": 200, "license": "None", "language": "python", "size": 14672}
{"docstring": "'''\nGet all initital parameters of this layer.\n:returns:\n    - init_params : list of theano.shared\n        List of all initial parameters\n'''\n", "func_signal": "def get_init_params(self):\n", "code": "if self.learn_hid_init:\n    params =  [self.hid_init_enc, self.cell_init_enc,\n        self.hid_init_dec, self.cell_init_dec]\nelse:\n    params = []\nreturn params", "path": "deepmodels\\layers\\draw.py", "repo_name": "skaae/lasagne-draw", "stars": 200, "license": "None", "language": "python", "size": 14672}
{"docstring": "\"\"\"\nParameters\n----------\nimages : T.matrix    (shape: batch_size x img_size)\n    Batch of images. Internally it will be reshaped to be a\n    (batch_size, img_height, img_width)-shaped stack of images.\ncenter_y : T.vector (shape: batch_size)\ncenter_x : T.vector (shape: batch_size)\ndelta : T.vector    (shape: batch_size)\nsigma : T.vector    (shape: batch_size)\n\nReturns\n-------\nwindow : T.matrix   (shape: batch_size x N**2)\n\"\"\"\n", "func_signal": "def read_single(images, att_read, N, imgshp):\n", "code": "center_y = att_read['center_y']\ncenter_x = att_read['center_x']\ndelta = att_read['delta']\nsigma = att_read['sigma']\nA, B = imgshp\nbatch_size = images.shape[0]\n\n# Reshape input into proper 2d images\nI = images.reshape( (batch_size, A, B) )\n\n# Get separable filterbank\nFY, FX = filterbank_matrices(center_y, center_x, delta, sigma, N, imgshp)\n\n# apply to the batch of images\n\nW = my_batched_dot(my_batched_dot(FY, I), FX.transpose([0,2,1]))\n\nreturn W.reshape((batch_size, N*N))", "path": "deepmodels\\layers\\draw_helpers.py", "repo_name": "skaae/lasagne-draw", "stars": 200, "license": "None", "language": "python", "size": 14672}
{"docstring": "# LSTM step\n# Gate names are taken from http://arxiv.org/abs/1409.2329 figure 1\n", "func_signal": "def _lstm(self, gates, cell_previous,W_cell_to_gates, nonlinearity_out):\n", "code": "def slice_w(x, n):\n    start = n*self.num_units_encoder_and_decoder\n    stop = (n+1)*self.num_units_encoder_and_decoder\n    return x[:, start:stop]\n\ndef slice_c(x, n):\n    start = n*self.num_units_encoder_and_decoder\n    stop = (n+1)*self.num_units_encoder_and_decoder\n    return x[start:stop]\n\ndef clip(x):\n    return theano.gradient.grad_clip(\n        x, self.grad_clip_vals_in[0], self.grad_clip_vals_in[1])\n\ningate = slice_w(gates, 0)\nforgetgate = slice_w(gates, 1)\nmodulationgate = slice_w(gates, 2)\noutgate = slice_w(gates, 3)\n\nif self.peepholes:\n    ingate += cell_previous*slice_c(W_cell_to_gates, 0)\n    forgetgate += cell_previous*slice_c(W_cell_to_gates, 1)\n\nif self.grad_clip_vals_in is not None:\n    print('STEP: CLipping gradients IN', self.grad_clip_vals_in)\n    ingate = clip(ingate)\n    forgetgate = clip(forgetgate)\n    modulationgate = clip(modulationgate)\ningate = self.nonlinearity_ingate(ingate)\nforgetgate = self.nonlinearity_forgetgate(forgetgate)\nmodulationgate = self.nonlinearity_modulationgate(modulationgate)\nif self.grad_clip_vals_in is not None:\n    ingate = clip(ingate)\n    forgetgate = clip(forgetgate)\n    modulationgate = clip(modulationgate)\n\ncell = forgetgate*cell_previous + ingate*modulationgate\nif self.peepholes:\n    outgate += cell*slice_c(W_cell_to_gates, 2)\n\nif self.grad_clip_vals_in is not None:\n    outgate = clip(outgate)\n\noutgate = self.nonlinearity_outgate(outgate)\nif self.grad_clip_vals_in is not None:\n    outgate = clip(outgate)\n\nhid = outgate*nonlinearity_out(cell)\nreturn [cell, hid]", "path": "deepmodels\\layers\\draw.py", "repo_name": "skaae/lasagne-draw", "stars": 200, "license": "None", "language": "python", "size": 14672}
{"docstring": "'''\nGet all weights of this layer\n:returns:\n    - weight_params : list of theano.shared\n        List of all weight parameters\n'''\n", "func_signal": "def get_weight_params(self):\n", "code": "return [self.W_enc_gates,\n        self.W_hid_to_gates_enc,\n        self.W_z_to_gates_dec,\n        self.W_hid_to_gates_dec,\n        self.W_dec_to_canvas_patch,\n        self.W_enc_to_z_mu,\n        self.W_enc_to_z_sigma,\n        self.W_read,\n        self.W_write\n]", "path": "deepmodels\\layers\\draw.py", "repo_name": "skaae/lasagne-draw", "stars": 200, "license": "None", "language": "python", "size": 14672}
{"docstring": "# return a list of permuted batch indeces\n", "func_signal": "def _get_permuted_batches(self,n_batches):\n", "code": "batches = []\nfor i in range(n_batches):\n\n    # extend random permuation if shorter than batchsize\n    if len(self.perm) <= self.batchsize:\n        new_perm = self.createindices()\n        self.perm = np.hstack([self.perm, new_perm])\n\n    batches.append(self.perm[:self.batchsize])\n    self.perm = self.perm[self.batchsize:]\nreturn batches", "path": "deepmodels\\batchiterator.py", "repo_name": "skaae/lasagne-draw", "stars": 200, "license": "None", "language": "python", "size": 14672}
{"docstring": "\"\"\"Create a Fy and a Fx\n\nParameters\n----------\ncenter_y : T.vector (shape: batch_size)\ncenter_x : T.vector (shape: batch_size)\n    Y and X center coordinates for the attention window\ndelta : T.vector (shape: batch_size)\nsigma : T.vector (shape: batch_size)\n\nReturns\n-------\n    FY, FX\n\"\"\"\n", "func_signal": "def filterbank_matrices(center_y, center_x, delta, sigma, N, imgshp):\n", "code": "tol = 1e-4\nimg_height, img_width = imgshp\nmuX = center_x.dimshuffle([0, 'x']) + delta.dimshuffle([0, 'x'])*(T.arange(N)-N/2-0.5)\nmuY = center_y.dimshuffle([0, 'x']) + delta.dimshuffle([0, 'x'])*(T.arange(N)-N/2-0.5)\n\na = T.arange(img_width)\nb = T.arange(img_height)\n\nFX = T.exp( -(a-muX.dimshuffle([0,1,'x']))**2 / 2. / sigma.dimshuffle([0,'x','x'])**2 )\nFY = T.exp( -(b-muY.dimshuffle([0,1,'x']))**2 / 2. / sigma.dimshuffle([0,'x','x'])**2 )\nFX = FX / (FX.sum(axis=-1).dimshuffle(0, 1, 'x') + tol)\nFY = FY / (FY.sum(axis=-1).dimshuffle(0, 1, 'x') + tol)\n\nreturn FY, FX", "path": "deepmodels\\layers\\draw_helpers.py", "repo_name": "skaae/lasagne-draw", "stars": 200, "license": "None", "language": "python", "size": 14672}
{"docstring": "'''\nGet all parameters of this layer.\n\n:returns:\n    - params : list of theano.shared\n        List of all parameters\n'''\n", "func_signal": "def get_params(self):\n", "code": "params = self.get_weight_params() + self.get_bias_params()\nif self.peepholes:\n    params.extend(self.get_peephole_params())\n\nif self.learn_hid_init:\n    params.extend(self.get_init_params())\n\nif self.learn_canvas_init:\n    params += [self.canvas_init]\n\nreturn params", "path": "deepmodels\\layers\\draw.py", "repo_name": "skaae/lasagne-draw", "stars": 200, "license": "None", "language": "python", "size": 14672}
{"docstring": "\"\"\"Convert neural-net outputs to attention parameters\n\nParameters\n----------\nl : tensor (batch_size x 5)\n\nReturns\n-------\ncenter_y : vector (batch_size)\ncenter_x : vector (batch_size)\ndelta : vector (batch_size)\nsigma : vector (batch_size)\ngamma : vector (batch_size)\n\"\"\"\n", "func_signal": "def nn2att(l, N,  imgshp):\n", "code": "A,B = imgshp\ncenter_y  = l[:, 0]\ncenter_x  = l[:, 1]   # sigmoid all thee\nlog_delta = l[:, 2]   # sigmoid all these\nlog_sigma2 = l[:, 3]\nlog_gamma = l[:, 4]\n\ndelta = T.exp(log_delta)\nsigma = T.exp(log_sigma2/2.)\ngamma = T.exp(log_gamma).dimshuffle(0, 'x')\n\n# normalize coordinates\ndelta = (max(A, B)-1) / (N-1) * delta\ncenter_x = (center_x+1.)* (B+1)*0.5\ncenter_y = (center_y+1.)* (A+1)*0.5\n\n\n\n\nreturn {'center_y':center_y, 'center_x':center_x, 'delta':delta,\n        'sigma':sigma, 'gamma':gamma}", "path": "deepmodels\\layers\\draw_helpers.py", "repo_name": "skaae/lasagne-draw", "stars": 200, "license": "None", "language": "python", "size": 14672}
{"docstring": "\"\"\"\nCompute layer cost.\n\n:parameters:\n    - input : theano.TensorType\n        Symbolic input variable\n\n:returns:\n    - layer_output : theano.TensorType\n        Symbolic output variable\n\"\"\"\n\n\n\n", "func_signal": "def get_cost(self, x, y=None, *args, **kwargs):\n", "code": "if y is None and self.use_y is True:\n        raise ValueError('y must be given when use_y is true')\n\ndef step(eps_n,\n         ######### REUCCRENT\n         cell_previous_enc, hid_previous_enc,\n         cell_previous_dec, hid_previous_dec,\n         canvas_previous, mu_z_previous, log_sigma_z_previous,\n         z_previous, l_read_previous, l_write_previous,\n         #kl_previous,\n         ######### x and WEIGHTS\n         x, y,\n         W_enc_gates,\n         W_hid_to_gates_enc,\n         b_gates_enc,\n         W_cellenc_to_enc_gates,\n         W_read, b_read,\n         W_z_to_gates_dec, b_gates_dec,\n         W_hid_to_gates_dec,\n\n         W_celldec_to_dec_gates,\n         W_enc_to_z_mu, b_enc_to_z_mu,\n         W_enc_to_z_sigma, b_enc_to_z_sigma,\n         W_dec_to_canvas_patch, W_write, b_write,\n         ):\n    # calculate gates pre-activations and slice\n    N_read = self.N_filters_read\n    N_write = self.N_filters_write\n    img_shp = self.imgshp\n\n    x_err = x - T.nnet.sigmoid(canvas_previous)\n    att_read = nn2att(l_read_previous, N_read,  img_shp)\n    x_org_in, x_err_in = read(x,x_err, att_read, N_read, img_shp)\n\n    x_org_in = att_read['gamma']*x_org_in\n    x_err_in = att_read['gamma']*x_err_in\n\n    if self.use_y:\n        in_gates_enc = T.concatenate([y, x_org_in, x_err_in,hid_previous_dec], axis=1)\n    else:\n        in_gates_enc = T.concatenate([x_org_in, x_err_in,hid_previous_dec], axis=1)\n\n    # equation (5)~ish\n    #slice_gates_idx = 4*self.num_units_encoder_and_decoder\n    # ENCODER\n    gates_enc = T.dot(in_gates_enc, W_enc_gates) + b_gates_enc\n    gates_enc += T.dot(hid_previous_enc, W_hid_to_gates_enc)\n    #gates_enc +=T.dot(hid_previous_enc, W_hidenc_to_enc_gates)\n    cell_enc, hid_enc = self._lstm(gates_enc,\n                             cell_previous_enc,\n                             W_cellenc_to_enc_gates,\n                             self.nonlinearity_out_encoder)\n\n    # VARIATIONAL\n    # eq 6\n    mu_z = T.dot(hid_enc, W_enc_to_z_mu) + b_enc_to_z_mu\n    log_sigma_z = 0.5*(T.dot(hid_enc, W_enc_to_z_sigma) + b_enc_to_z_sigma)\n    z = mu_z + T.exp(log_sigma_z)*eps_n\n\n    if self.use_y:\n        print('STEP: using Y')\n        in_gates_dec = T.concatenate([y, z], axis=1)\n    else:\n        print('STEP: Not using Y')\n        in_gates_dec = z\n\n\n    # DECODER\n    gates_dec = T.dot(in_gates_dec, W_z_to_gates_dec) + b_gates_dec  # i_dec\n    gates_dec += T.dot(hid_previous_dec, W_hid_to_gates_dec)\n    # equation (7)\n    cell_dec, hid_dec = self._lstm(gates_dec,\n                             cell_previous_dec,\n                             W_celldec_to_dec_gates,\n                             self.nonlinearity_out_decoder)\n\n    # WRITE\n    l_write = T.dot(hid_dec, W_write) + b_write\n    w = T.dot(hid_dec, W_dec_to_canvas_patch)\n    att_write = nn2att(l_write, N_write,  img_shp)\n    canvas_upd = write(w, att_write, N_write, img_shp)\n    canvas_upd = 1.0/(att_write['gamma']+1e-4) * canvas_upd\n    canvas = canvas_previous + canvas_upd\n\n    l_read = T.dot(hid_dec, W_read) + b_read\n\n    # Todo: some of the (all?) gradient clips are redundant\n    # + I'm unsure if I use grad_clip correct and in correct places...\n    # The description of gradient clipping is in\n    # Generating sequences with recurrent neural networks\n    # section: 2.1 Long Short-Term Memory\n    #\n    #if self.grad_clip_vals_out is not None:\n    #    print('STEP: CLipping gradients Out', self.grad_clip_vals_out)\n    #    cell_enc = theano.gradient.grad_clip(cell_enc, self.grad_clip_vals_out[0], self.grad_clip_vals_out[1])\n    #    hid_enc = theano.gradient.grad_clip(hid_enc, self.grad_clip_vals_out[0], self.grad_clip_vals_out[1])\n    #    cell_dec = theano.gradient.grad_clip(cell_dec, self.grad_clip_vals_out[0], self.grad_clip_vals_out[1])\n    #    hid_dec = theano.gradient.grad_clip(hid_dec, self.grad_clip_vals_out[0], self.grad_clip_vals_out[1])\n\n    return [cell_enc, hid_enc, cell_dec, hid_dec, canvas,\n            mu_z, log_sigma_z, z, l_read, l_write]\n\n\nones = T.ones((self.num_batch,1))\nmu_z_init = T.zeros((self.num_batch, self.dimz))\nlog_sigma_z_init = T.zeros((self.num_batch, self.dimz))\nz_init = T.zeros((self.num_batch, self.dimz))\natt_vals_write_init = T.zeros((self.num_batch, 5))\n\nif theano.config.compute_test_value is 'off':\n    eps = _srng.normal((self.glimpses,self.num_batch))\nelse:\n    # for testing\n    print(\"draw.py: is not using random generator\"+\"!#>\"*30)\n    eps = T.ones((self.glimpses,self.num_batch), theano.config.floatX) * 0.3\n\nif y is None:\n    y = T.zeros((1))\n\n\n# Todo: cleanup this somehow\n# Todo: Will it slow down theano optimization if I dont pass in\n# non seqs as arguments, but just call them with self.XXXX?\nseqs = [eps]\ninit = [T.dot(ones, self.cell_init_enc), T.dot(ones, self.hid_init_enc),\n        T.dot(ones, self.cell_init_dec), T.dot(ones, self.hid_init_dec),\n        T.dot(ones, self.canvas_init), mu_z_init, log_sigma_z_init,\n        z_init, T.dot(ones, self.read_init), att_vals_write_init]\nnonseqs_input = [x, y]\nnonseqs_enc = [self.W_enc_gates,\n               self.W_hid_to_gates_enc,\n               self.b_gates_enc,\n               self.W_cellenc_to_enc_gates,\n               self.W_read, self.b_read]\nnonseqs_dec = [self.W_z_to_gates_dec, self.b_gates_dec,\n               self.W_hid_to_gates_dec,\n               self.W_celldec_to_dec_gates]\nnonseqs_variational = [self.W_enc_to_z_mu, self.b_enc_to_z_mu,\n                       self.W_enc_to_z_sigma, self.b_enc_to_z_sigma]\nnonseqs_other = [self.W_dec_to_canvas_patch, self.W_write, self.b_write]\nnon_seqs = nonseqs_input +  nonseqs_enc + nonseqs_dec + nonseqs_variational \\\n           + nonseqs_other\n\noutput_scan = theano.scan(step, sequences=seqs,\n                     outputs_info=init,\n                     non_sequences=non_seqs,\n                     go_backwards=False)[0]\n\n\ncell_enc, hid_enc, cell_dec, hid_dec, canvas, mu_z, log_sigma_z, \\\nz, l_read, l_write = output_scan\n\n# because we model the output as bernoulli we take sigmoid to ensure\n# range (0,1)\nlast_reconstruction = T.nnet.sigmoid(canvas[-1, :, :])\n# select distribution of p(x|z)\n\n# LOSS\n# The loss is the negative loglikelihood of the data plus the\n# KL divergence between the the variational approximation to z and\n# the prior on z:\n# Loss = -logD(x) + D_kl(Q(z|h)||p(z))\n# If we assume that x is bernoulli then\n# -logD(x) = -(t*log(o) +(1-t)*log(1-o)) = cross_ent(t,o)\n# D_kl(Q(z|h)||p(z)) can in some cases be solved analytically as\n# D_kl(Q(z|h)||p(z)) = 0.5(sum_T(mu^2 + sigma^2 - 1 -log(sigma^2)))\n# We add these terms and return minus the cost, i.e return the\n# lowerbound\n\nL_x = T.nnet.binary_crossentropy(last_reconstruction, x).sum()\n#L_x = cross_ent(last_reconstruction, x).sum()\nL_z =  T.sum(0.5*( mu_z**2 + T.exp(log_sigma_z*2) - 1 - log_sigma_z*2))\nself.L_x = L_x\nself.L_z = L_z\nL =  L_x + L_z\n\nself.canvas = canvas\nself.att_vals_read = l_read\nself.att_vals_write = l_write\n\nreturn L / self.num_batch", "path": "deepmodels\\layers\\draw.py", "repo_name": "skaae/lasagne-draw", "stars": 200, "license": "None", "language": "python", "size": 14672}
{"docstring": "\"\"\"GAEProxyHandler setup, init domain/iplist map\"\"\"\n", "func_signal": "def first_run(self):\n", "code": "if not common.PROXY_ENABLE:\n    logging.info('resolve common.IPLIST_ALIAS names=%s to iplist', list(common.IPLIST_ALIAS))\n    common.resolve_iplist()\nrandom.shuffle(common.GAE_APPIDS)\nself.__class__.handler_plugins['gae'] = GAEFetchPlugin(common.GAE_APPIDS, common.GAE_PASSWORD, common.GAE_PATH, common.GAE_MODE, common.GAE_CACHESOCK, common.GAE_KEEPALIVE, common.GAE_OBFUSCATE, common.GAE_PAGESPEED, common.GAE_VALIDATE, common.GAE_OPTIONS)\nif not common.PROXY_ENABLE:\n    net2 = AdvancedNet2(window=common.GAE_WINDOW, ssl_version=common.GAE_SSLVERSION, dns_servers=common.DNS_SERVERS, dns_blacklist=common.DNS_BLACKLIST)\n    for name, iplist in common.IPLIST_ALIAS.items():\n        net2.add_iplist_alias(name, iplist)\n        if name == 'google_hk':\n            for delay in (30, 60, 150, 240, 300, 450, 600, 900):\n                spawn_later(delay, self.extend_iplist, name)\n    net2.add_fixed_iplist(common.IPLIST_PREDEFINED)\n    for pattern, hosts in common.RULE_MAP.items():\n        net2.add_rule(pattern, hosts)\n    if common.GAE_CACHESOCK:\n        net2.enable_connection_cache()\n    if common.GAE_KEEPALIVE:\n        net2.enable_connection_keepalive()\n    net2.enable_openssl_session_cache()\n    self.__class__.net2 = net2", "path": "local\\proxy.py", "repo_name": "DIYgod/EasyGoAgent", "stars": 174, "license": "None", "language": "python", "size": 22605}
{"docstring": "\"\"\"Send a MIME header.\"\"\"\n", "func_signal": "def send_header(self, keyword, value):\n", "code": "base_send_header = BaseHTTPServer.BaseHTTPRequestHandler.send_header\nkeyword = keyword.title()\nif keyword == 'Set-Cookie':\n    for cookie in re.split(r', (?=[^ =]+(?:=|$))', value):\n        base_send_header(self, keyword, cookie)\nelif keyword == 'Content-Disposition' and '\"' not in value:\n    value = re.sub(r'filename=([^\"\\']+)', 'filename=\"\\\\1\"', value)\n    base_send_header(self, keyword, value)\nelse:\n    base_send_header(self, keyword, value)", "path": "local\\proxylib.py", "repo_name": "DIYgod/EasyGoAgent", "stars": 174, "license": "None", "language": "python", "size": 22605}
{"docstring": "\"\"\"https://developers.google.com/appengine/docs/python/urlfetch/\"\"\"\n", "func_signal": "def filter(self, handler):\n", "code": "if handler.command == 'CONNECT':\n    do_ssl_handshake = 440 <= handler.port <= 450 or 1024 <= handler.port <= 65535\n    alias = handler.net2.getaliasbyname(handler.path)\n    if alias:\n        return 'direct', {'cache_key': '%s:%d' % (alias, handler.port), 'headfirst': '.google' in handler.host}\n    else:\n        return 'strip', {'do_ssl_handshake': do_ssl_handshake}\nelif handler.command in ('GET', 'POST', 'HEAD', 'PUT', 'DELETE', 'PATCH'):\n    alias = handler.net2.getaliasbyname(handler.path)\n    if alias:\n        return 'direct', {'cache_key': '%s:%d' % (alias, handler.port), 'headfirst': '.google' in handler.host}\n    else:\n        return 'gae', {}\nelse:\n    if 'php' in handler.handler_plugins:\n        return 'php', {}\n    else:\n        logging.warning('\"%s %s\" not supported by GAE, please enable PHP mode!', handler.command, handler.path)\n        return 'direct', {}", "path": "local\\proxy.py", "repo_name": "DIYgod/EasyGoAgent", "stars": 174, "license": "None", "language": "python", "size": 22605}
{"docstring": "\"\"\"PHPProxyHandler setup, init domain/iplist map\"\"\"\n", "func_signal": "def first_run(self):\n", "code": "if not common.PROXY_ENABLE:\n    hostname = urlparse.urlsplit(common.PHP_FETCHSERVER).hostname\n    net2 = AdvancedNet2(window=4, ssl_version='TLSv1', dns_servers=common.DNS_SERVERS, dns_blacklist=common.DNS_BLACKLIST)\n    if not common.PHP_HOSTS:\n        common.PHP_HOSTS = net2.gethostsbyname(hostname)\n    net2.add_iplist_alias('php_fetchserver', common.PHP_HOSTS)\n    net2.add_fixed_iplist(common.PHP_HOSTS)\n    net2.add_rule(hostname, 'php_fetchserver')\n    net2.enable_connection_cache()\n    if common.PHP_KEEPALIVE:\n        net2.enable_connection_keepalive()\n    net2.enable_openssl_session_cache()\n    self.__class__.net2 = net2", "path": "local\\proxy.py", "repo_name": "DIYgod/EasyGoAgent", "stars": 174, "license": "None", "language": "python", "size": 22605}
{"docstring": "\"\"\"forward socket\"\"\"\n", "func_signal": "def forward_socket(self, local, remote, timeout, bufsize):\n", "code": "tick = 1\ncount = timeout\nwhile 1:\n    count -= tick\n    if count <= 0:\n        break\n    ins, _, errors = select.select([local, remote], [], [local, remote], tick)\n    if remote in errors:\n        local.close()\n        remote.close()\n        return\n    if local in errors:\n        local.close()\n        remote.close()\n        return\n    if remote in ins:\n        data = remote.recv(bufsize)\n        if not data:\n            remote.close()\n            local.close()\n            return\n        local.sendall(data)\n    if local in ins:\n        data = local.recv(bufsize)\n        if not data:\n            remote.close()\n            local.close()\n            return\n        remote.sendall(data)\n    if ins:\n        count = timeout", "path": "local\\proxy.py", "repo_name": "DIYgod/EasyGoAgent", "stars": 174, "license": "None", "language": "python", "size": 22605}
{"docstring": "\"\"\"http://dev.maxmind.com/geoip/legacy/codes/iso3166/\"\"\"\n", "func_signal": "def get_country_code(self, hostname, dnsservers):\n", "code": "try:\n    return self.region_cache[hostname]\nexcept KeyError:\n    pass\ntry:\n    if re.match(r'^\\d+\\.\\d+\\.\\d+\\.\\d+$', hostname) or ':' in hostname:\n        iplist = [hostname]\n    elif dnsservers:\n        iplist = dnslib_record2iplist(dnslib_resolve_over_udp(hostname, dnsservers, timeout=2))\n    else:\n        iplist = socket.gethostbyname_ex(hostname)[-1]\n    if iplist[0].startswith(('127.', '192.168.', '10.')):\n        country_code = 'LOCAL'\n    else:\n        country_code = self.geoip.country_code_by_addr(iplist[0])\nexcept StandardError as e:\n    logging.warning('DirectRegionFilter cannot determine region for hostname=%r %r', hostname, e)\n    country_code = ''\nself.region_cache[hostname] = country_code\nreturn country_code", "path": "local\\proxylib.py", "repo_name": "DIYgod/EasyGoAgent", "stars": 174, "license": "None", "language": "python", "size": 22605}
{"docstring": "\"\"\"\nhttp://gfwrev.blogspot.com/2009/11/gfwdns.html\nhttp://zh.wikipedia.org/wiki/%E5%9F%9F%E5%90%8D%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%BC%93%E5%AD%98%E6%B1%A1%E6%9F%93\nhttp://support.microsoft.com/kb/241352\nhttps://gist.github.com/klzgrad/f124065c0616022b65e5\n\"\"\"\n", "func_signal": "def dnslib_resolve_over_udp(query, dnsservers, timeout, **kwargs):\n", "code": "if not isinstance(query, (basestring, dnslib.DNSRecord)):\n    raise TypeError('query argument requires string/DNSRecord')\nblacklist = kwargs.get('blacklist', ())\nturstservers = kwargs.get('turstservers', ())\ndns_v4_servers = [x for x in dnsservers if ':' not in x]\ndns_v6_servers = [x for x in dnsservers if ':' in x]\nsock_v4 = sock_v6 = None\nsocks = []\nif dns_v4_servers:\n    sock_v4 = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n    socks.append(sock_v4)\nif dns_v6_servers:\n    sock_v6 = socket.socket(socket.AF_INET6, socket.SOCK_DGRAM)\n    socks.append(sock_v6)\ntimeout_at = time.time() + timeout\ntry:\n    for _ in xrange(4):\n        try:\n            for dnsserver in dns_v4_servers:\n                if isinstance(query, basestring):\n                    if dnsserver in ('8.8.8.8', '8.8.4.4'):\n                        query = '.'.join(x[:-1] + x[-1].upper() for x in query.split('.')).title()\n                    query = dnslib.DNSRecord(q=dnslib.DNSQuestion(query))\n                query_data = query.pack()\n                if query.q.qtype == 1 and dnsserver in ('8.8.8.8', '8.8.4.4'):\n                    query_data = query_data[:-5] + '\\xc0\\x04' + query_data[-4:]\n                sock_v4.sendto(query_data, parse_hostport(dnsserver, 53))\n            for dnsserver in dns_v6_servers:\n                if isinstance(query, basestring):\n                    query = dnslib.DNSRecord(q=dnslib.DNSQuestion(query, qtype=dnslib.QTYPE.AAAA))\n                query_data = query.pack()\n                sock_v6.sendto(query_data, parse_hostport(dnsserver, 53))\n            while time.time() < timeout_at:\n                ins, _, _ = select.select(socks, [], [], 0.1)\n                for sock in ins:\n                    reply_data, reply_address = sock.recvfrom(512)\n                    reply_server = reply_address[0]\n                    record = dnslib.DNSRecord.parse(reply_data)\n                    iplist = [str(x.rdata) for x in record.rr if x.rtype in (1, 28, 255)]\n                    if any(x in blacklist for x in iplist):\n                        logging.warning('query=%r dnsservers=%r record bad iplist=%r', query, dnsservers, iplist)\n                    elif record.header.rcode and not iplist and reply_server in turstservers:\n                        logging.info('query=%r trust reply_server=%r record rcode=%s', query, reply_server, record.header.rcode)\n                        return record\n                    elif iplist:\n                        logging.debug('query=%r reply_server=%r record iplist=%s', query, reply_server, iplist)\n                        return record\n                    else:\n                        logging.debug('query=%r reply_server=%r record null iplist=%s', query, reply_server, iplist)\n                        continue\n        except socket.error as e:\n            logging.warning('handle dns query=%s socket: %r', query, e)\n    raise socket.gaierror(11004, 'getaddrinfo %r from %r failed' % (query, dnsservers))\nfinally:\n    for sock in socks:\n        sock.close()", "path": "local\\dnsproxy.py", "repo_name": "DIYgod/EasyGoAgent", "stars": 174, "license": "None", "language": "python", "size": 22605}
{"docstring": "\"\"\"mock response\"\"\"\n", "func_signal": "def handle(self, handler, status=400, headers={}, body=''):\n", "code": "logging.info('%s \"MOCK %s %s %s\" %d %d', handler.address_string(), handler.command, handler.path, handler.protocol_version, status, len(body))\nheaders = dict((k.title(), v) for k, v in headers.items())\nif 'Transfer-Encoding' in headers:\n    del headers['Transfer-Encoding']\nif 'Content-Length' not in headers:\n    headers['Content-Length'] = len(body)\nif 'Connection' not in headers:\n    headers['Connection'] = 'close'\nhandler.send_response(status)\nfor key, value in headers.items():\n    handler.send_header(key, value)\nhandler.end_headers()\nhandler.wfile.write(body)", "path": "local\\proxylib.py", "repo_name": "DIYgod/EasyGoAgent", "stars": 174, "license": "None", "language": "python", "size": 22605}
{"docstring": "\"\"\"strip connect\"\"\"\n", "func_signal": "def handle(self, handler, do_ssl_handshake=True):\n", "code": "logging.info('%s \"STRIP %s %s:%d %s\" - -', handler.address_string(), handler.command, handler.host, handler.port, handler.protocol_version)\nhandler.send_response(200)\nhandler.end_headers()\nif do_ssl_handshake:\n    try:\n        self.do_ssl_handshake(handler)\n    except (socket.error, ssl.SSLError, OpenSSL.SSL.Error) as e:\n        if e.args[0] not in (errno.ECONNABORTED, errno.ECONNRESET) or (len(e.args) > 1 and e.args[1] == 'Unexpected EOF'):\n            logging.exception('ssl.wrap_socket(connection=%r) failed: %s', handler.connection, e)\n        return\ntry:\n    handler.raw_requestline = handler.rfile.readline(65537)\n    if len(handler.raw_requestline) > 65536:\n        handler.requestline = ''\n        handler.request_version = ''\n        handler.command = ''\n        handler.send_error(414)\n        handler.wfile.close()\n        return\n    if not handler.raw_requestline:\n        handler.close_connection = 1\n        return\n    if not handler.parse_request():\n        handler.send_error(400)\n        handler.wfile.close()\n        return\nexcept (socket.error, ssl.SSLError, OpenSSL.SSL.Error) as e:\n    if e.args[0] in (errno.ECONNABORTED, errno.ECONNRESET, errno.EPIPE):\n        handler.close_connection = 1\n        return\n    else:\n        raise\ntry:\n    handler.do_METHOD()\nexcept (socket.error, ssl.SSLError, OpenSSL.SSL.Error) as e:\n    if e.args[0] not in (errno.ECONNABORTED, errno.ETIMEDOUT, errno.EPIPE):\n        raise", "path": "local\\proxylib.py", "repo_name": "DIYgod/EasyGoAgent", "stars": 174, "license": "None", "language": "python", "size": 22605}
{"docstring": "\"\"\"Constructor.  May be extended, do not override.\"\"\"\n", "func_signal": "def __init__(self, listener, RequestHandlerClass, bind_and_activate=True):\n", "code": "if hasattr(listener, 'getsockname'):\n    SocketServer.BaseServer.__init__(self, listener.getsockname(), RequestHandlerClass)\n    self.socket = listener\nelse:\n    SocketServer.ThreadingTCPServer.__init__(self, listener, RequestHandlerClass, bind_and_activate)", "path": "local\\proxylib.py", "repo_name": "DIYgod/EasyGoAgent", "stars": 174, "license": "None", "language": "python", "size": 22605}
{"docstring": "\"\"\"make ThreadingTCPServer happy\"\"\"\n", "func_signal": "def handle_error(self, *args):\n", "code": "exc_info = sys.exc_info()\nerror = exc_info and len(exc_info) and exc_info[1]\nif isinstance(error, (socket.error, ssl.SSLError, OpenSSL.SSL.Error)) and len(error.args) > 1 and 'bad write retry' in error.args[1]:\n    exc_info = error = None\nelse:\n    del exc_info, error\n    SocketServer.ThreadingTCPServer.handle_error(self, *args)", "path": "local\\proxylib.py", "repo_name": "DIYgod/EasyGoAgent", "stars": 174, "license": "None", "language": "python", "size": 22605}
{"docstring": "\"\"\"dns query over tcp\"\"\"\n", "func_signal": "def dnslib_resolve_over_tcp(query, dnsservers, timeout, **kwargs):\n", "code": "if not isinstance(query, (basestring, dnslib.DNSRecord)):\n    raise TypeError('query argument requires string/DNSRecord')\nblacklist = kwargs.get('blacklist', ())\ndef do_resolve(query, dnsserver, timeout, queobj):\n    if isinstance(query, basestring):\n        qtype = dnslib.QTYPE.AAAA if ':' in dnsserver else dnslib.QTYPE.A\n        query = dnslib.DNSRecord(q=dnslib.DNSQuestion(query, qtype=qtype))\n    query_data = query.pack()\n    sock_family = socket.AF_INET6 if ':' in dnsserver else socket.AF_INET\n    sock = socket.socket(sock_family)\n    rfile = None\n    try:\n        sock.settimeout(timeout or None)\n        sock.connect(parse_hostport(dnsserver, 53))\n        sock.send(struct.pack('>h', len(query_data)) + query_data)\n        rfile = sock.makefile('r', 1024)\n        reply_data_length = rfile.read(2)\n        if len(reply_data_length) < 2:\n            raise socket.gaierror(11004, 'getaddrinfo %r from %r failed' % (query, dnsserver))\n        reply_data = rfile.read(struct.unpack('>h', reply_data_length)[0])\n        record = dnslib.DNSRecord.parse(reply_data)\n        iplist = [str(x.rdata) for x in record.rr if x.rtype in (1, 28, 255)]\n        if any(x in blacklist for x in iplist):\n            logging.debug('query=%r dnsserver=%r record bad iplist=%r', query, dnsserver, iplist)\n            raise socket.gaierror(11004, 'getaddrinfo %r from %r failed' % (query, dnsserver))\n        else:\n            logging.debug('query=%r dnsserver=%r record iplist=%s', query, dnsserver, iplist)\n            queobj.put(record)\n    except socket.error as e:\n        logging.debug('query=%r dnsserver=%r failed %r', query, dnsserver, e)\n        queobj.put(e)\n    finally:\n        if rfile:\n            rfile.close()\n        sock.close()\nqueobj = Queue.Queue()\nfor dnsserver in dnsservers:\n    thread.start_new_thread(do_resolve, (query, dnsserver, timeout, queobj))\nfor i in range(len(dnsservers)):\n    try:\n        result = queobj.get(timeout)\n    except Queue.Empty:\n        raise socket.gaierror(11004, 'getaddrinfo %r from %r failed' % (query, dnsservers))\n    if result and not isinstance(result, Exception):\n        return result\n    elif i == len(dnsservers) - 1:\n        logging.warning('dnslib_resolve_over_tcp %r with %s return %r', query, dnsservers, result)\nraise socket.gaierror(11004, 'getaddrinfo %r from %r failed' % (query, dnsservers))", "path": "local\\dnsproxy.py", "repo_name": "DIYgod/EasyGoAgent", "stars": 174, "license": "None", "language": "python", "size": 22605}
{"docstring": "\"\"\"forward socket\"\"\"\n", "func_signal": "def handle_connect(self, handler, kwargs):\n", "code": "host = handler.host\nport = handler.port\nlocal = handler.connection\nremote = None\nhandler.connection.send('HTTP/1.1 200 OK\\r\\n\\r\\n')\nhandler.close_connection = 1\nfor i in xrange(self.max_retry):\n    try:\n        remote = handler.net2.create_tcp_connection(host, port, handler.net2.connect_timeout, **kwargs)\n    except StandardError as e:\n        logging.exception('%s \"FORWARD %s %s:%d %s\" %r', handler.address_string(), handler.command, host, port, handler.protocol_version, e)\n        if hasattr(remote, 'close'):\n            remote.close()\n        if i == self.max_retry - 1:\n            raise\nlogging.info('%s \"FORWARD %s %s:%d %s\" - -', handler.address_string(), handler.command, host, port, handler.protocol_version)\nif hasattr(remote, 'fileno'):\n    # reset timeout default to avoid long http upload failure, but it will delay timeout retry :(\n    remote.settimeout(None)\nforward_socket(local, remote, 60, bufsize=256*1024)", "path": "local\\proxylib.py", "repo_name": "DIYgod/EasyGoAgent", "stars": 174, "license": "None", "language": "python", "size": 22605}
{"docstring": "#Check CA exists\n", "func_signal": "def check_ca(self):\n", "code": "capath = os.path.join(os.path.dirname(os.path.abspath(__file__)), self.ca_keyfile)\ncertdir = os.path.join(os.path.dirname(os.path.abspath(__file__)), self.ca_certdir)\nif not os.path.exists(capath):\n    if os.path.exists(certdir):\n        any(os.remove(x) for x in glob.glob(certdir+'/*.crt')+glob.glob(certdir+'/.*.crt'))\n    if os.name == 'nt':\n        try:\n            self.remove_ca(self.ca_vendor)\n        except Exception as e:\n            logging.warning('self.remove_ca failed: %r', e)\n    self.dump_ca()\nwith open(capath, 'rb') as fp:\n    self.ca_thumbprint = OpenSSL.crypto.load_certificate(OpenSSL.crypto.FILETYPE_PEM, fp.read()).digest('sha1')\n#Check Certs\ncertfiles = glob.glob(certdir+'/*.crt')+glob.glob(certdir+'/.*.crt')\nif certfiles:\n    filename = random.choice(certfiles)\n    commonname = os.path.splitext(os.path.basename(filename))[0]\n    with open(filename, 'rb') as fp:\n        serial_number = OpenSSL.crypto.load_certificate(OpenSSL.crypto.FILETYPE_PEM, fp.read()).get_serial_number()\n    if serial_number != self.get_cert_serial_number(commonname):\n        any(os.remove(x) for x in certfiles)\n#Check CA imported\nif self.import_ca(capath) != 0:\n    logging.warning('install root certificate failed, Please run as administrator/root/sudo')\n#Check Certs Dir\nif not os.path.exists(certdir):\n    os.makedirs(certdir)", "path": "local\\proxylib.py", "repo_name": "DIYgod/EasyGoAgent", "stars": 174, "license": "None", "language": "python", "size": 22605}
{"docstring": "# https://support.google.com/websearch/answer/186669?hl=zh-Hans\n", "func_signal": "def resolve_iplist(self):\n", "code": "def do_local_resolve(host, queue):\n    assert isinstance(host, basestring)\n    for _ in xrange(3):\n        try:\n            family = socket.AF_INET6 if self.GAE_IPV6 else socket.AF_INET\n            iplist = [x[-1][0] for x in socket.getaddrinfo(host, 80, family)]\n            queue.put((host, iplist))\n        except (socket.error, OSError) as e:\n            logging.warning('socket.getaddrinfo host=%r failed: %s', host, e)\n            time.sleep(0.1)\ngoogle_blacklist = ['216.239.32.20'] + list(self.DNS_BLACKLIST)\ngoogle_blacklist_prefix = tuple(x for x in self.DNS_BLACKLIST if x.endswith('.'))\nfor name, need_resolve_hosts in list(self.IPLIST_ALIAS.items()):\n    if all(re.match(r'\\d+\\.\\d+\\.\\d+\\.\\d+', x) or ':' in x for x in need_resolve_hosts):\n        continue\n    need_resolve_remote = [x for x in need_resolve_hosts if ':' not in x and not re.match(r'\\d+\\.\\d+\\.\\d+\\.\\d+', x)]\n    resolved_iplist = [x for x in need_resolve_hosts if x not in need_resolve_remote]\n    result_queue = Queue.Queue()\n    for host in need_resolve_remote:\n        logging.debug('local resolve host=%r', host)\n        thread.start_new_thread(do_local_resolve, (host, result_queue))\n    for _ in xrange(len(need_resolve_remote)):\n        try:\n            host, iplist = result_queue.get(timeout=8)\n            resolved_iplist += iplist\n        except Queue.Empty:\n            break\n    if name.startswith('google_') and name not in ('google_cn', 'google_hk') and resolved_iplist:\n        iplist_prefix = re.split(r'[\\.:]', resolved_iplist[0])[0]\n        resolved_iplist = list(set(x for x in resolved_iplist if x.startswith(iplist_prefix)))\n    else:\n        resolved_iplist = list(set(resolved_iplist))\n    if name.startswith('google_'):\n        resolved_iplist = list(set(resolved_iplist) - set(google_blacklist))\n        resolved_iplist = [x for x in resolved_iplist if not x.startswith(google_blacklist_prefix)]\n    if len(resolved_iplist) == 0 and name in ('google_hk', 'google_cn') and not self.GAE_IPV6:\n        logging.error('resolve %s host return empty! please retry!', name)\n        sys.exit(-1)\n    logging.info('resolve name=%s host to iplist=%r', name, resolved_iplist)\n    common.IPLIST_ALIAS[name] = resolved_iplist\nif self.IPLIST_ALIAS.get('google_cn', []):\n    try:\n        for _ in xrange(4):\n            socket.create_connection((random.choice(self.IPLIST_ALIAS['google_cn']), 80), timeout=2).close()\n    except socket.error:\n        self.IPLIST_ALIAS['google_cn'] = []\nif len(self.IPLIST_ALIAS.get('google_cn', [])) < 4 and self.IPLIST_ALIAS.get('google_hk', []):\n    logging.warning('google_cn resolved too short iplist=%s, switch to google_hk', self.IPLIST_ALIAS.get('google_cn', []))\n    self.IPLIST_ALIAS['google_cn'] = self.IPLIST_ALIAS['google_hk']", "path": "local\\proxy.py", "repo_name": "DIYgod/EasyGoAgent", "stars": 174, "license": "None", "language": "python", "size": 22605}
{"docstring": "\"\"\"convert dnslib.DNSRecord to iplist\"\"\"\n", "func_signal": "def dnslib_record2iplist(record):\n", "code": "assert isinstance(record, dnslib.DNSRecord)\niplist = [x for x in (str(r.rdata) for r in record.rr) if re.match(r'^\\d+\\.\\d+\\.\\d+\\.\\d+$', x) or ':' in x]\nreturn iplist", "path": "local\\proxylib.py", "repo_name": "DIYgod/EasyGoAgent", "stars": 174, "license": "None", "language": "python", "size": 22605}
{"docstring": "\"\"\"\nhttp://gfwrev.blogspot.com/2009/11/gfwdns.html\nhttp://zh.wikipedia.org/wiki/%E5%9F%9F%E5%90%8D%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%BC%93%E5%AD%98%E6%B1%A1%E6%9F%93\nhttp://support.microsoft.com/kb/241352\nhttps://gist.github.com/klzgrad/f124065c0616022b65e5\n\"\"\"\n", "func_signal": "def dnslib_resolve_over_udp(query, dnsservers, timeout, **kwargs):\n", "code": "if not isinstance(query, (basestring, dnslib.DNSRecord)):\n    raise TypeError('query argument requires string/DNSRecord')\nblacklist = kwargs.get('blacklist', ())\nblacklist_prefix = tuple(x for x in blacklist if x.endswith('.'))\nturstservers = kwargs.get('turstservers', ())\ndns_v4_servers = [x for x in dnsservers if ':' not in x]\ndns_v6_servers = [x for x in dnsservers if ':' in x]\nsock_v4 = sock_v6 = None\nsocks = []\nif dns_v4_servers:\n    sock_v4 = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n    socks.append(sock_v4)\nif dns_v6_servers:\n    sock_v6 = socket.socket(socket.AF_INET6, socket.SOCK_DGRAM)\n    socks.append(sock_v6)\ntimeout_at = time.time() + timeout\ntry:\n    for _ in xrange(4):\n        try:\n            for dnsserver in dns_v4_servers:\n                if isinstance(query, basestring):\n                    if dnsserver in ('8.8.8.8', '8.8.4.4'):\n                        query = '.'.join(x[:-1] + x[-1].upper() for x in query.split('.')).title()\n                    query = dnslib.DNSRecord(q=dnslib.DNSQuestion(query))\n                query_data = query.pack()\n                if query.q.qtype == 1 and dnsserver in ('8.8.8.8', '8.8.4.4'):\n                    query_data = query_data[:-5] + '\\xc0\\x04' + query_data[-4:]\n                sock_v4.sendto(query_data, parse_hostport(dnsserver, 53))\n            for dnsserver in dns_v6_servers:\n                if isinstance(query, basestring):\n                    query = dnslib.DNSRecord(q=dnslib.DNSQuestion(query, qtype=dnslib.QTYPE.AAAA))\n                query_data = query.pack()\n                sock_v6.sendto(query_data, parse_hostport(dnsserver, 53))\n            while time.time() < timeout_at:\n                ins, _, _ = select.select(socks, [], [], 0.1)\n                for sock in ins:\n                    reply_data, reply_address = sock.recvfrom(512)\n                    reply_server = reply_address[0]\n                    record = dnslib.DNSRecord.parse(reply_data)\n                    iplist = [str(x.rdata) for x in record.rr if x.rtype in (1, 28, 255)]\n                    if any(x in blacklist or x.startswith(blacklist_prefix) for x in iplist):\n                        logging.warning('qname=%r dnsservers=%r record bad iplist=%r', query.q.qname, dnsservers, iplist)\n                    elif record.header.rcode and not iplist and reply_server in turstservers:\n                        logging.info('qname=%r trust reply_server=%r record rcode=%s', query.q.qname, reply_server, record.header.rcode)\n                        return record\n                    elif iplist:\n                        logging.debug('qname=%r reply_server=%r record iplist=%s', query.q.qname, reply_server, iplist)\n                        return record\n                    else:\n                        logging.debug('qname=%r reply_server=%r record null iplist=%s', query.q.qname, reply_server, iplist)\n                        continue\n        except socket.error as e:\n            logging.warning('handle dns query=%s socket: %r', query, e)\n    raise socket.gaierror(11004, 'getaddrinfo %r from %r failed' % (query, dnsservers))\nfinally:\n    for sock in socks:\n        sock.close()", "path": "local\\proxylib.py", "repo_name": "DIYgod/EasyGoAgent", "stars": 174, "license": "None", "language": "python", "size": 22605}
{"docstring": "\"\"\"make python2 BaseHTTPRequestHandler happy\"\"\"\n", "func_signal": "def finish(self):\n", "code": "try:\n    BaseHTTPServer.BaseHTTPRequestHandler.finish(self)\nexcept (socket.error, ssl.SSLError, OpenSSL.SSL.Error) as e:\n    if e.args[0] not in (errno.ECONNABORTED, errno.ECONNRESET, errno.EPIPE):\n        raise", "path": "local\\proxylib.py", "repo_name": "DIYgod/EasyGoAgent", "stars": 174, "license": "None", "language": "python", "size": 22605}
{"docstring": "\"\"\"forward socket\"\"\"\n", "func_signal": "def forward_socket(local, remote, timeout, bufsize):\n", "code": "try:\n    tick = 1\n    timecount = timeout\n    while 1:\n        timecount -= tick\n        if timecount <= 0:\n            break\n        (ins, _, errors) = select.select([local, remote], [], [local, remote], tick)\n        if errors:\n            break\n        for sock in ins:\n            data = sock.recv(bufsize)\n            if not data:\n                break\n            if sock is remote:\n                local.sendall(data)\n                timecount = timeout\n            else:\n                remote.sendall(data)\n                timecount = timeout\nexcept socket.timeout:\n    pass\nexcept (socket.error, ssl.SSLError, OpenSSL.SSL.Error) as e:\n    if e.args[0] not in (errno.ECONNABORTED, errno.ECONNRESET, errno.ENOTCONN, errno.EPIPE):\n        raise\n    if e.args[0] in (errno.EBADF,):\n        return\nfinally:\n    for sock in (remote, local):\n        try:\n            sock.close()\n        except StandardError:\n            pass", "path": "local\\proxylib.py", "repo_name": "DIYgod/EasyGoAgent", "stars": 174, "license": "None", "language": "python", "size": 22605}
{"docstring": "\"\"\"load config from proxy.ini\"\"\"\n", "func_signal": "def __init__(self):\n", "code": "ConfigParser.RawConfigParser.OPTCRE = re.compile(r'(?P<option>\\S+)\\s+(?P<vi>[=])\\s+(?P<value>.*)$')\nself.CONFIG = ConfigParser.ConfigParser()\nself.CONFIG_FILENAME = os.path.splitext(os.path.abspath(__file__))[0]+'.ini'\nself.CONFIG_USER_FILENAME = re.sub(r'\\.ini$', '.user.ini', self.CONFIG_FILENAME)\nself.CONFIG.read([self.CONFIG_FILENAME, self.CONFIG_USER_FILENAME])\n\nfor key, value in os.environ.items():\n    m = re.match(r'^%s([A-Z]+)_([A-Z\\_\\-]+)$' % self.ENV_CONFIG_PREFIX, key)\n    if m:\n        self.CONFIG.set(m.group(1).lower(), m.group(2).lower(), value)\n\nself.LISTEN_IP = self.CONFIG.get('listen', 'ip')\nself.LISTEN_PORT = self.CONFIG.getint('listen', 'port')\nself.LISTEN_USERNAME = self.CONFIG.get('listen', 'username') if self.CONFIG.has_option('listen', 'username') else ''\nself.LISTEN_PASSWORD = self.CONFIG.get('listen', 'password') if self.CONFIG.has_option('listen', 'password') else ''\nself.LISTEN_VISIBLE = self.CONFIG.getint('listen', 'visible')\nself.LISTEN_DEBUGINFO = self.CONFIG.getint('listen', 'debuginfo')\n\nself.GAE_ENABLE = self.CONFIG.getint('gae', 'enable')\nself.GAE_APPIDS = re.findall(r'[\\w\\-\\.]+', self.CONFIG.get('gae', 'appid').replace('.appspot.com', ''))\nself.GAE_PASSWORD = self.CONFIG.get('gae', 'password').strip()\nself.GAE_PATH = self.CONFIG.get('gae', 'path')\nself.GAE_MODE = self.CONFIG.get('gae', 'mode')\nself.GAE_IPV6 = self.CONFIG.getint('gae', 'ipv6')\nself.GAE_WINDOW = self.CONFIG.getint('gae', 'window')\nself.GAE_KEEPALIVE = self.CONFIG.getint('gae', 'keepalive')\nself.GAE_CACHESOCK = self.CONFIG.getint('gae', 'cachesock')\nself.GAE_HEADFIRST = self.CONFIG.getint('gae', 'headfirst')\nself.GAE_OBFUSCATE = self.CONFIG.getint('gae', 'obfuscate')\nself.GAE_VALIDATE = self.CONFIG.getint('gae', 'validate')\nself.GAE_TRANSPORT = self.CONFIG.getint('gae', 'transport') if self.CONFIG.has_option('gae', 'transport') else 0\nself.GAE_OPTIONS = self.CONFIG.get('gae', 'options')\nself.GAE_REGIONS = set(x.upper() for x in self.CONFIG.get('gae', 'regions').split('|') if x.strip())\nself.GAE_SSLVERSION = self.CONFIG.get('gae', 'sslversion')\nself.GAE_PAGESPEED = self.CONFIG.getint('gae', 'pagespeed') if self.CONFIG.has_option('gae', 'pagespeed') else 0\n\nif self.GAE_IPV6:\n    sock = None\n    try:\n        sock = socket.socket(socket.AF_INET6, socket.SOCK_DGRAM)\n        sock.connect(('2001:4860:4860::8888', 53))\n        logging.info('use ipv6 interface %s for gae', sock.getsockname()[0])\n    except Exception as e:\n        logging.info('Fail try use ipv6 %r, fallback ipv4', e)\n        self.GAE_IPV6 = 0\n    finally:\n        if sock:\n            sock.close()\n\nif 'USERDNSDOMAIN' in os.environ and re.match(r'^\\w+\\.\\w+$', os.environ['USERDNSDOMAIN']):\n    self.CONFIG.set('profile', '.' + os.environ['USERDNSDOMAIN'], 'direct')\n\nwithgae_sites = []\nwithphp_sites = []\ncrlf_sites = []\nnocrlf_sites = []\nforcehttps_sites = []\nnoforcehttps_sites = []\nfakehttps_sites = []\nnofakehttps_sites = []\ndns_servers = []\nurlrewrite_map = collections.OrderedDict()\nrule_map = collections.OrderedDict()\n\nfor pattern, rule in self.CONFIG.items('profile'):\n    rules = [x.strip() for x in re.split(r'[,\\|]', rule) if x.strip()]\n    if rule.startswith(('file://', 'http://', 'https://')) or '$1' in rule:\n        urlrewrite_map[pattern] = rule\n        continue\n    for rule, sites in [('withgae', withgae_sites),\n                        ('withphp', withphp_sites),\n                        ('crlf', crlf_sites),\n                        ('nocrlf', nocrlf_sites),\n                        ('forcehttps', forcehttps_sites),\n                        ('noforcehttps', noforcehttps_sites),\n                        ('fakehttps', fakehttps_sites),\n                        ('nofakehttps', nofakehttps_sites)]:\n        if rule in rules:\n            sites.append(pattern)\n            rules.remove(rule)\n    if rules:\n        rule_map[pattern] = rules[0]\n\nself.HTTP_DNS = dns_servers\nself.WITHGAE_SITES = tuple(withgae_sites)\nself.WITHPHP_SITES = tuple(withphp_sites)\nself.CRLF_SITES = tuple(crlf_sites)\nself.NOCRLF_SITES = set(nocrlf_sites)\nself.FORCEHTTPS_SITES = tuple(forcehttps_sites)\nself.NOFORCEHTTPS_SITES = set(noforcehttps_sites)\nself.FAKEHTTPS_SITES = tuple(fakehttps_sites)\nself.NOFAKEHTTPS_SITES = set(nofakehttps_sites)\nself.URLREWRITE_MAP = urlrewrite_map\nself.RULE_MAP = rule_map\n\nself.IPLIST_ALIAS = collections.OrderedDict((k, v.split('|') if v else []) for k, v in self.CONFIG.items('iplist'))\nself.IPLIST_PREDEFINED = [x for x in sum(self.IPLIST_ALIAS.values(), []) if re.match(r'^\\d+\\.\\d+\\.\\d+\\.\\d+$', x) or ':' in x]\n\nif self.GAE_IPV6 and 'google_ipv6' in self.IPLIST_ALIAS:\n    for name in self.IPLIST_ALIAS.keys():\n        if name.startswith('google') and name not in ('google_ipv6', 'google_talk'):\n            self.IPLIST_ALIAS[name] = self.IPLIST_ALIAS['google_ipv6']\n\nself.PAC_ENABLE = self.CONFIG.getint('pac', 'enable')\nself.PAC_IP = self.CONFIG.get('pac', 'ip')\nself.PAC_PORT = self.CONFIG.getint('pac', 'port')\nself.PAC_FILE = self.CONFIG.get('pac', 'file').lstrip('/')\nself.PAC_GFWLIST = self.CONFIG.get('pac', 'gfwlist')\nself.PAC_ADBLOCK = self.CONFIG.get('pac', 'adblock')\nself.PAC_ADMODE = self.CONFIG.getint('pac', 'admode')\nself.PAC_EXPIRED = self.CONFIG.getint('pac', 'expired')\n\nself.PHP_ENABLE = self.CONFIG.getint('php', 'enable')\nself.PHP_LISTEN = self.CONFIG.get('php', 'listen')\nself.PHP_PASSWORD = self.CONFIG.get('php', 'password') if self.CONFIG.has_option('php', 'password') else ''\nself.PHP_CRLF = self.CONFIG.getint('php', 'crlf') if self.CONFIG.has_option('php', 'crlf') else 1\nself.PHP_VALIDATE = self.CONFIG.getint('php', 'validate') if self.CONFIG.has_option('php', 'validate') else 0\nself.PHP_KEEPALIVE = self.CONFIG.getint('php', 'keepalive')\nself.PHP_FETCHSERVER = self.CONFIG.get('php', 'fetchserver')\nself.PHP_HOSTS = self.CONFIG.get('php', 'hosts').split('|') if self.CONFIG.get('php', 'hosts') else []\n\nself.VPS_ENABLE = self.CONFIG.getint('vps', 'enable')\nself.VPS_LISTEN = self.CONFIG.get('vps', 'listen')\nself.VPS_FETCHSERVER = self.CONFIG.get('vps', 'fetchserver')\n\nself.PROXY_ENABLE = self.CONFIG.getint('proxy', 'enable')\nself.PROXY_AUTODETECT = self.CONFIG.getint('proxy', 'autodetect') if self.CONFIG.has_option('proxy', 'autodetect') else 0\nself.PROXY_HOST = self.CONFIG.get('proxy', 'host')\nself.PROXY_PORT = self.CONFIG.getint('proxy', 'port')\nself.PROXY_USERNAME = self.CONFIG.get('proxy', 'username')\nself.PROXY_PASSWROD = self.CONFIG.get('proxy', 'password')\n\nif not self.PROXY_ENABLE and self.PROXY_AUTODETECT:\n    system_proxy = ProxyUtil.get_system_proxy()\n    if system_proxy and self.LISTEN_IP not in system_proxy:\n        _, username, password, address = ProxyUtil.parse_proxy(system_proxy)\n        proxyhost, _, proxyport = address.rpartition(':')\n        self.PROXY_ENABLE = 1\n        self.PROXY_USERNAME = username\n        self.PROXY_PASSWROD = password\n        self.PROXY_HOST = proxyhost\n        self.PROXY_PORT = int(proxyport)\nif self.PROXY_ENABLE:\n    self.GAE_MODE = 'https'\n\nself.AUTORANGE_HOSTS = self.CONFIG.get('autorange', 'hosts').split('|')\nself.AUTORANGE_ENDSWITH = tuple(self.CONFIG.get('autorange', 'endswith').split('|'))\nself.AUTORANGE_NOENDSWITH = tuple(self.CONFIG.get('autorange', 'noendswith').split('|'))\nself.AUTORANGE_MAXSIZE = self.CONFIG.getint('autorange', 'maxsize')\nself.AUTORANGE_WAITSIZE = self.CONFIG.getint('autorange', 'waitsize')\nself.AUTORANGE_BUFSIZE = self.CONFIG.getint('autorange', 'bufsize')\nself.AUTORANGE_THREADS = self.CONFIG.getint('autorange', 'threads')\n\nself.FETCHMAX_LOCAL = self.CONFIG.getint('fetchmax', 'local') if self.CONFIG.get('fetchmax', 'local') else 3\nself.FETCHMAX_SERVER = self.CONFIG.get('fetchmax', 'server')\n\nself.DNS_ENABLE = self.CONFIG.getint('dns', 'enable')\nself.DNS_LISTEN = self.CONFIG.get('dns', 'listen')\nself.DNS_SERVERS = self.HTTP_DNS or self.CONFIG.get('dns', 'servers').split('|')\nself.DNS_BLACKLIST = set(self.CONFIG.get('dns', 'blacklist').split('|'))\nself.DNS_TCPOVER = tuple(self.CONFIG.get('dns', 'tcpover').split('|')) if self.CONFIG.get('dns', 'tcpover').strip() else tuple()\nif self.GAE_IPV6:\n    self.DNS_SERVERS = [x for x in self.DNS_SERVERS if ':' in x]\nelse:\n    self.DNS_SERVERS = [x for x in self.DNS_SERVERS if ':' not in x]\n\nself.USERAGENT_ENABLE = self.CONFIG.getint('useragent', 'enable')\nself.USERAGENT_STRING = self.CONFIG.get('useragent', 'string')\n\nself.LOVE_ENABLE = self.CONFIG.getint('love', 'enable')\nself.LOVE_TIP = self.CONFIG.get('love', 'tip').encode('utf8').decode('unicode-escape').split('|')", "path": "local\\proxy.py", "repo_name": "DIYgod/EasyGoAgent", "stars": 174, "license": "None", "language": "python", "size": 22605}
{"docstring": "\"\"\"Returns the snippets for all 'filetypes' (in order) and their\nparents matching the text 'before'. If 'possible' is true, a partial\nmatch is enough. Base classes can override this method to provide means\nof creating snippets on the fly.\n\nReturns a list of SnippetDefinition s.\n\n\"\"\"\n", "func_signal": "def get_snippets(self, filetypes, before, possible, autotrigger_only):\n", "code": "result = []\nfor ft in self._get_existing_deep_extends(filetypes):\n    snips = self._snippets[ft]\n    result.extend(snips.get_matching_snippets(before, possible,\n                                              autotrigger_only))\nreturn result", "path": "pythonx\\UltiSnips\\snippet\\source\\_base.py", "repo_name": "vim-scripts/UltiSnips", "stars": 130, "license": "None", "language": "python", "size": 1508}
{"docstring": "\"\"\"Unshift the indentation level. Note that this uses the shiftwidth\nbecause thats what code formatters use.\n\n:amount: the amount by which to unshift.\n\n\"\"\"\n", "func_signal": "def unshift(self, amount=1):\n", "code": "by = -self._ind.shiftwidth * amount\ntry:\n    self.indent = self.indent[:by]\nexcept IndexError:\n    self.indent = ''", "path": "pythonx\\UltiSnips\\text_objects\\_python_code.py", "repo_name": "vim-scripts/UltiSnips", "stars": 130, "license": "None", "language": "python", "size": 1508}
{"docstring": "\"\"\"Appends the given line to rv using mkline.\"\"\"\n", "func_signal": "def __add__(self, value):\n", "code": "self.rv += '\\n'  # pylint:disable=invalid-name\nself.rv += self.mkline(value)\nreturn self", "path": "pythonx\\UltiSnips\\text_objects\\_python_code.py", "repo_name": "vim-scripts/UltiSnips", "stars": 130, "license": "None", "language": "python", "size": 1508}
{"docstring": "\"\"\"\nJust passing call to the vim.current.window.buffer.__getitem__.\n\"\"\"\n", "func_signal": "def __getitem__(self, key):\n", "code": "if isinstance(key, slice):\n    return [as_unicode(l) for l in self._buffer[key.start:key.stop]]\nelse:\n    return as_unicode(self._buffer[key])", "path": "pythonx\\UltiSnips\\buffer_proxy.py", "repo_name": "vim-scripts/UltiSnips", "stars": 130, "license": "None", "language": "python", "size": 1508}
{"docstring": "\"\"\"\nVery fast diffing algorithm when changes are across many lines.\n\"\"\"\n", "func_signal": "def _get_diff(self, start, end, new_value):\n", "code": "for line_number in range(start, end):\n    if line_number < 0:\n        line_number = len(self._buffer) + line_number\n    yield ('D', line_number, 0, self._buffer[line_number])\n\nif start < 0:\n    start = len(self._buffer) + start\nfor line_number in range(0, len(new_value)):\n    yield ('I', start+line_number, 0, new_value[line_number])", "path": "pythonx\\UltiSnips\\buffer_proxy.py", "repo_name": "vim-scripts/UltiSnips", "stars": 130, "license": "None", "language": "python", "size": 1508}
{"docstring": "\"\"\"\nApply changeset to current snippets stack, correctly moving around\nsnippet itself or its child.\n\"\"\"\n", "func_signal": "def _apply_change(self, change):\n", "code": "if not self._snippets_stack:\n    return\n\nline_number = change[1]\ncolumn_number = change[2]\nline_before = line_number <= self._snippets_stack[0]._start.line\ncolumn_before = column_number <= self._snippets_stack[0]._start.col\nif line_before and column_before:\n    direction = 1\n    if change[0] == 'D':\n        direction = -1\n\n    self._snippets_stack[0]._move(\n        Position(line_number, 0),\n        Position(direction, 0)\n    )\nelse:\n    if line_number > self._snippets_stack[0]._end.line:\n        return\n    if column_number >= self._snippets_stack[0]._end.col:\n        return\n    self._snippets_stack[0]._do_edit(change)", "path": "pythonx\\UltiSnips\\buffer_proxy.py", "repo_name": "vim-scripts/UltiSnips", "stars": 130, "license": "None", "language": "python", "size": 1508}
{"docstring": "\"\"\"Write the code to a temporary file.\"\"\"\n", "func_signal": "def _run_shell_command(cmd, tmpdir):\n", "code": "cmdsuf = ''\nif platform.system() == 'Windows':\n    # suffix required to run command on windows\n    cmdsuf = '.bat'\n    # turn echo off\n    cmd = '@echo off\\r\\n' + cmd\nhandle, path = tempfile.mkstemp(text=True, dir=tmpdir, suffix=cmdsuf)\nos.write(handle, cmd.encode('utf-8'))\nos.close(handle)\nos.chmod(path, stat.S_IRWXU)\n\n# Execute the file and read stdout\nproc = Popen(path, shell=True, stdout=PIPE, stderr=PIPE)\nproc.wait()\nstdout, _ = proc.communicate()\nos.unlink(path)\nreturn _chomp(as_unicode(stdout))", "path": "pythonx\\UltiSnips\\text_objects\\_shell_code.py", "repo_name": "vim-scripts/UltiSnips", "stars": 130, "license": "None", "language": "python", "size": 1508}
{"docstring": "\"\"\"\nSame as vim.current.window.buffer.append(), but with tracking changes.\n\"\"\"\n", "func_signal": "def append(self, line, line_number=-1):\n", "code": "if line_number < 0:\n    line_number = len(self)\nif not isinstance(line, list):\n    line = [line]\nself[line_number:line_number] = [as_vimencoding(l) for l in line]", "path": "pythonx\\UltiSnips\\buffer_proxy.py", "repo_name": "vim-scripts/UltiSnips", "stars": 130, "license": "None", "language": "python", "size": 1508}
{"docstring": "\"\"\"Creates a properly set up line.\n\n:line: the text to add\n:indent: the indentation to have at the beginning\n         if None, it uses the default amount\n\n\"\"\"\n", "func_signal": "def mkline(self, line='', indent=None):\n", "code": "if indent is None:\n    indent = self.indent\n    # this deals with the fact that the first line is\n    # already properly indented\n    if '\\n' not in self._rv:\n        try:\n            indent = indent[len(self._initial_indent):]\n        except IndexError:\n            indent = ''\n    indent = self._ind.spaces_to_indent(indent)\n\nreturn indent + line", "path": "pythonx\\UltiSnips\\text_objects\\_python_code.py", "repo_name": "vim-scripts/UltiSnips", "stars": 130, "license": "None", "language": "python", "size": 1508}
{"docstring": "\"\"\"Get a set of cleared snippets marked by clearsnippets with arguments\nfor specified filetypes.\"\"\"\n", "func_signal": "def get_cleared(self, filetypes):\n", "code": "cleared = {}\nfor ft in self._get_existing_deep_extends(filetypes):\n    snippets = self._snippets[ft]\n    for key, value in snippets._cleared.items():\n        if key not in cleared or value > cleared[key]:\n            cleared[key] = value\nreturn cleared", "path": "pythonx\\UltiSnips\\snippet\\source\\_base.py", "repo_name": "vim-scripts/UltiSnips", "stars": 130, "license": "None", "language": "python", "size": 1508}
{"docstring": "\"\"\"\nForward all changes made in the buffer to the current snippet stack while\nfunction call.\n\"\"\"\n", "func_signal": "def use_proxy_buffer(snippets_stack, vstate):\n", "code": "buffer_proxy = VimBufferProxy(snippets_stack, vstate)\nold_buffer = _vim.buf\ntry:\n    _vim.buf = buffer_proxy\n    yield\nfinally:\n    _vim.buf = old_buffer\nbuffer_proxy.validate_buffer()", "path": "pythonx\\UltiSnips\\buffer_proxy.py", "repo_name": "vim-scripts/UltiSnips", "stars": 130, "license": "None", "language": "python", "size": 1508}
{"docstring": "\"\"\"Gets the snippet ready for another update.\n\n:cur: the new value for c.\n\n\"\"\"\n", "func_signal": "def _reset(self, cur):\n", "code": "self._ind.reset()\nself._cur = cur\nself._rv = ''\nself._changed = False\nself.reset_indent()", "path": "pythonx\\UltiSnips\\text_objects\\_python_code.py", "repo_name": "vim-scripts/UltiSnips", "stars": 130, "license": "None", "language": "python", "size": 1508}
{"docstring": "\"\"\"Find an executable tmp directory.\"\"\"\n", "func_signal": "def _get_tmp():\n", "code": "userdir = os.path.expanduser('~')\nfor testdir in [tempfile.gettempdir(), os.path.join(userdir, '.cache'),\n                os.path.join(userdir, '.tmp'), userdir]:\n    if (not os.path.exists(testdir) or\n            not _run_shell_command('echo success', testdir) == 'success'):\n        continue\n    return testdir\nreturn ''", "path": "pythonx\\UltiSnips\\text_objects\\_shell_code.py", "repo_name": "vim-scripts/UltiSnips", "stars": 130, "license": "None", "language": "python", "size": 1508}
{"docstring": "\"\"\"Rather than rstrip(), remove only the last newline and preserve\npurposeful whitespace.\"\"\"\n", "func_signal": "def _chomp(string):\n", "code": "if len(string) and string[-1] == '\\n':\n    string = string[:-1]\nif len(string) and string[-1] == '\\r':\n    string = string[:-1]\nreturn string", "path": "pythonx\\UltiSnips\\text_objects\\_shell_code.py", "repo_name": "vim-scripts/UltiSnips", "stars": 130, "license": "None", "language": "python", "size": 1508}
{"docstring": "\"\"\"\nPrevents changes being applied to the snippet stack while function call.\n\"\"\"\n", "func_signal": "def suspend_proxy_edits():\n", "code": "if not isinstance(_vim.buf, VimBufferProxy):\n    yield\nelse:\n    try:\n        _vim.buf._disable_edits()\n        yield\n    finally:\n        _vim.buf._enable_edits()", "path": "pythonx\\UltiSnips\\buffer_proxy.py", "repo_name": "vim-scripts/UltiSnips", "stars": 130, "license": "None", "language": "python", "size": 1508}
{"docstring": "\"\"\"Get a list of filetypes that is either directed or indirected\nextended by given base filetypes.\n\nNote that the returned list include the root filetype itself.\n\n\"\"\"\n", "func_signal": "def get_deep_extends(self, base_filetypes):\n", "code": "seen = set(base_filetypes)\ntodo_fts = list(set(base_filetypes))\nwhile todo_fts:\n    todo_ft = todo_fts.pop()\n    unseen_extends = set(\n        ft for ft in self._extends[todo_ft] if ft not in seen)\n    seen.update(unseen_extends)\n    todo_fts.extend(unseen_extends)\nreturn seen", "path": "pythonx\\UltiSnips\\snippet\\source\\_base.py", "repo_name": "vim-scripts/UltiSnips", "stars": 130, "license": "None", "language": "python", "size": 1508}
{"docstring": "\"\"\"\nInstantiate new object.\n\nsnippets_stack is a slice of currently active snippets.\n\"\"\"\n", "func_signal": "def __init__(self, snippets_stack, vstate):\n", "code": "self._snippets_stack = snippets_stack\nself._buffer = vim.current.buffer\nself._change_tick = int(vim.eval(\"b:changedtick\"))\nself._forward_edits = True\nself._vstate = vstate", "path": "pythonx\\UltiSnips\\buffer_proxy.py", "repo_name": "vim-scripts/UltiSnips", "stars": 130, "license": "None", "language": "python", "size": 1508}
{"docstring": "\"\"\"\nRaises exception if buffer is changes beyound proxy object.\n\"\"\"\n", "func_signal": "def validate_buffer(self):\n", "code": "if self.is_buffer_changed_outside():\n    raise RuntimeError('buffer was modified using vim.command or ' +\n    'vim.current.buffer; that changes are untrackable and leads to ' +\n    'errors in snippet expansion; use special variable `snip.buffer` '\n    'for buffer modifications.\\n\\n' +\n    'See :help UltiSnips-buffer-proxy for more info.')", "path": "pythonx\\UltiSnips\\buffer_proxy.py", "repo_name": "vim-scripts/UltiSnips", "stars": 130, "license": "None", "language": "python", "size": 1508}
{"docstring": "\"\"\"\nUse precise diffing for tracking changes in single line.\n\"\"\"\n", "func_signal": "def _get_line_diff(self, line_number, before, after):\n", "code": "if before == '':\n    for change in self._get_diff(line_number, line_number+1, [after]):\n        yield change\nelse:\n    for change in diff(before, after):\n        yield (change[0], line_number, change[2], change[3])", "path": "pythonx\\UltiSnips\\buffer_proxy.py", "repo_name": "vim-scripts/UltiSnips", "stars": 130, "license": "None", "language": "python", "size": 1508}
{"docstring": "\"\"\"Helper for get all existing filetypes extended by base filetypes.\"\"\"\n", "func_signal": "def _get_existing_deep_extends(self, base_filetypes):\n", "code": "deep_extends = self.get_deep_extends(base_filetypes)\nreturn [ft for ft in deep_extends if ft in self._snippets]", "path": "pythonx\\UltiSnips\\snippet\\source\\_base.py", "repo_name": "vim-scripts/UltiSnips", "stars": 130, "license": "None", "language": "python", "size": 1508}
{"docstring": "'''\n@summary: Changes which dissector will be called after the current.\n@param name: A protocol's name\n@param length: The number of bytes to be dissected.\n'''\n", "func_signal": "def set_next_dissector(self, name, length = REMAINING_LENGTH):\n", "code": "if not hasattr(self, \"_next_dissector\"):\n    self._next_dissector = DissectorItem(name, length)\nelse:\n    self._next_dissector.set(name, length)", "path": "python\\cal\\cal_types.py", "repo_name": "ashdnazg/pyreshark", "stars": 159, "license": "None", "language": "python", "size": 661}
{"docstring": "'''\n@summary: See ItemBase.\n'''\n", "func_signal": "def generate_filter_name(self, prefix):\n", "code": "for subitem in getattr(self, \"_subitems\", []):\n    subitem.generate_filter_name(prefix)", "path": "python\\cal\\cal_types.py", "repo_name": "ashdnazg/pyreshark", "stars": 159, "license": "None", "language": "python", "size": 661}
{"docstring": "'''\n@summary: Generates the item's filter name according to the item's name and the given prefix\n'''\n", "func_signal": "def generate_filter_name(self, prefix):\n", "code": "if hasattr(self, \"_name\"):\n    if prefix != \"\":\n        self._filter_name = \".\".join((prefix, self._name))\n    else:\n        self._filter_name = self._name", "path": "python\\cal\\cal_types.py", "repo_name": "ashdnazg/pyreshark", "stars": 159, "license": "None", "language": "python", "size": 661}
{"docstring": "'''\n@summary: Reads an item from the items dictionary passed to PyFunctionItem, adds it to the tree and advances the offset.\n@param item_key: The key of the item in the items_dict\n'''\n", "func_signal": "def read_item(self, item_key):\n", "code": "node_list = self._items_dict[item_key].get_node_list()\ntemp_offset = pointer(c_int(self.offset))\ntvb_and_tree = pointer(PStvbuff_and_tree(self.p_new_tvb, self.p_new_tree))\nfor func, params in node_list:\n    if params is None:\n        p_params = None\n    else:\n        p_params = addressof(params)\n    func.argtypes = PS_DISSECT_FUNC_ARGS\n    func(tvb_and_tree, self._p_pinfo, temp_offset, p_params)\n\nself.offset = temp_offset.contents.value", "path": "python\\cal\\cal_types.py", "repo_name": "ashdnazg/pyreshark", "stars": 159, "license": "None", "language": "python", "size": 661}
{"docstring": "'''\n@summary: Initializes the CAL, instantiates the python plugins and registers them.\n'''\n", "func_signal": "def __init__(self):\n", "code": "self._protocols = []\nself._cal = cal.CAL()\nprotocol_files = glob(os.path.join(\"%s\" % (PROTOCOLS_DIR,), \"*.py\"))\n\nfor p_file in protocol_files:\n    try:\n        proto_module = __import__(p_file.replace(\"%s%s\" % (PROTOCOLS_DIR, os.path.sep), \"\").replace(\".py\", \"\"))\n        self._protocols.append(proto_module.Protocol())\n    except:\n        exc = traceback.format_exc().splitlines()\n        message = \"Pyreshark error:\\n\" + \"\\n\".join(exc[-3:])\n        self._cal.error_message(message)\n        traceback.print_exc(file=sys.stderr)\n\nself._cal.register_protocols(self._protocols)", "path": "python\\pyreshark.py", "repo_name": "ashdnazg/pyreshark", "stars": 159, "license": "None", "language": "python", "size": 661}
{"docstring": "'''\n@summary: A constructor.\n@param p_hf_index: A pointer to the index of a registered field.\n@param name: The name of this item.\n@param length: Length of the field in bytes. Note that the offset is not advanced. (default: 0)\n@param encoding: Encoding for reading the field. (default: ENC_BIG_ENDIAN=ENC_NA=0)\n'''\n", "func_signal": "def __init__(self, p_hf_index, name, length=0, encoding = ENC_NA):\n", "code": "self._params = PSadd_tree_item_params(p_hf_index, length, encoding, None)\nself.pointer = self._params.out_item\nself._name = name", "path": "python\\cal\\cal_types.py", "repo_name": "ashdnazg/pyreshark", "stars": 159, "license": "None", "language": "python", "size": 661}
{"docstring": "'''\n@summary: Registers the preference with Wireshark\n@param cal: A CAL object.\n@param pref_index: The order to display the user preference\n@param name: The name used for the module in the preferences file\n'''\n", "func_signal": "def register(self, cal, pref_index, name):\n", "code": "self._cal = cal\nself._pref_index = pref_index\nself.name = c_char_p(name)\nself._cal.wslib.prefs_register_enum_preference(self._pref_index,\n                                               self.name,\n                                               self.title,\n                                               self.description,\n                                               byref(self.enum_pref),\n                                               byref(self.enums),\n                                               self.radio_buttons)", "path": "python\\cal\\cal_types.py", "repo_name": "ashdnazg/pyreshark", "stars": 159, "license": "None", "language": "python", "size": 661}
{"docstring": "'''\n@summary: See ItemBase.\n'''\n", "func_signal": "def generate_filter_name(self, prefix):\n", "code": "self.get_parent_item().generate_filter_name(prefix)\nnew_prefix = self.get_parent_item()._filter_name\nfor item in self.get_child_items():\n    item.generate_filter_name(new_prefix)", "path": "python\\cal\\cal_types.py", "repo_name": "ashdnazg/pyreshark", "stars": 159, "license": "None", "language": "python", "size": 661}
{"docstring": "'''\n@summary: See ItemBase.\n'''\n", "func_signal": "def generate_filter_name(self, prefix):\n", "code": "super(FieldItem, self).generate_filter_name(prefix)\nself._field.abbrev = self._filter_name", "path": "python\\cal\\cal_types.py", "repo_name": "ashdnazg/pyreshark", "stars": 159, "license": "None", "language": "python", "size": 661}
{"docstring": "'''\n@summary: Add a FT_BOOLEAN to a proto_tree.\n@param item_key: The key of the item in the items_dict\n@param value: data to display\n@param length: The number of bytes that'll be marked when selecting the item. The packet's offset is not advanced. (default: 0)\n@param offset: The beginning offset for the marked bytes. If set to None, offset=self.offset. (default: None)\n@param tree: the tree to append this item to (default: parent)\n@return: the newly created item\n'''\n", "func_signal": "def add_boolean(self, item_key, value, length=0, offset=None, tree=None):\n", "code": "tree = tree or self.p_new_tree\nif offset is None:\n    offset = self.offset\nreturn self._cal.wslib.proto_tree_add_boolean(tree, self._items_dict[item_key]._index, self.p_new_tvb, offset, length, value)", "path": "python\\cal\\cal_types.py", "repo_name": "ashdnazg/pyreshark", "stars": 159, "license": "None", "language": "python", "size": 661}
{"docstring": "'''\n@summary: A constructor.\n@param parent_item: The subtree's parent item.\n@param item_list: The subtree's children - A list of items.\n@param tree_name: Used by Wireshark for remembering which trees are expanded. Put AUTO_TREE for the name of parent_item. (default: AUTO_TREE)\n'''\n", "func_signal": "def __init__(self, parent_item, item_list, tree_name = AUTO_TREE):\n", "code": "self.start_offset = c_int(0)\nself.old_tree = c_void_p(0)\nself._params = PSpush_tree_params(pointer(parent_item.pointer), None, pointer(self.start_offset), pointer(self.old_tree))\nself._pop_params = PSpop_tree_params(pointer(self.start_offset), pointer(self.old_tree))\nself._subitems = [parent_item] + item_list\nself._tree_name = tree_name", "path": "python\\cal\\cal_types.py", "repo_name": "ashdnazg/pyreshark", "stars": 159, "license": "None", "language": "python", "size": 661}
{"docstring": "'''\n@summary: A constructor.\n@param name: A protocol's name\n@param length: The number of bytes to be dissected.\n'''\n", "func_signal": "def __init__(self, name, length = REMAINING_LENGTH):\n", "code": "self._dissector_name = c_char_p(name)\nself._length = c_int(length)\nself._params = PScall_next_dissector_params(pointer(self._dissector_name), pointer(self._length), name, length)", "path": "python\\cal\\cal_types.py", "repo_name": "ashdnazg/pyreshark", "stars": 159, "license": "None", "language": "python", "size": 661}
{"docstring": "'''\n@summary: Unpacks values from the packet's buffer, using struct.unpack\n@param format: A format string. (see Python's documentation for the module struct)\n@param offset: The offset from which the values will be read. None will set it to the current offset. (default: None)\n'''\n", "func_signal": "def unpack(self, format, offset = None):\n", "code": "if offset is None:\n    _offset = self.offset\nelse:\n    _offset = offset\n    \nreturn unpack(format, self.buffer[_offset:_offset+calcsize(format)])", "path": "python\\cal\\cal_types.py", "repo_name": "ashdnazg/pyreshark", "stars": 159, "license": "None", "language": "python", "size": 661}
{"docstring": "'''\n@summary: Add one of FT_UINT8, FT_UINT16, FT_UINT24 or FT_UINT32 to a proto_tree.\n@param item_key: The key of the item in the items_dict\n@param value: data to display\n@param length: The number of bytes that'll be marked when selecting the item. The packet's offset is not advanced. (default: 0)\n@param offset: The beginning offset for the marked bytes. If set to None, offset=self.offset. (default: None)\n@param tree: the tree to append this item to (default: parent)\n@return: the newly created item\n'''\n", "func_signal": "def add_uint(self, item_key, value, length=0, offset=None, tree=None):\n", "code": "tree = tree or self.p_new_tree\nif offset is None:\n    offset = self.offset\nreturn self._cal.wslib.proto_tree_add_uint(tree, self._items_dict[item_key]._index, self.p_new_tvb, offset, length, value)", "path": "python\\cal\\cal_types.py", "repo_name": "ashdnazg/pyreshark", "stars": 159, "license": "None", "language": "python", "size": 661}
{"docstring": "'''\n@summary: A constructor.\n@param name: The name of the field. Used for generating the filter name.\n@param text: The text that will be added to the tree.\n@param length: Length of the field in bytes. Note that the offset is not advanced. (default: 0)\n'''\n", "func_signal": "def __init__(self, name, text, length = 0):\n", "code": "super(TextItem, self).__init__(name, FT_NONE, length=length)\nself._params = PSadd_text_item_params(pointer(self._index), length, text, None)\nself.pointer = self._params.out_item", "path": "python\\cal\\cal_types.py", "repo_name": "ashdnazg/pyreshark", "stars": 159, "license": "None", "language": "python", "size": 661}
{"docstring": "'''\n@summary: Calls the handoff function in each one of the protocols.\n'''\n", "func_signal": "def handoff(self):\n", "code": "for proto in self._protocols:\n    proto.handoff()", "path": "python\\pyreshark.py", "repo_name": "ashdnazg/pyreshark", "stars": 159, "license": "None", "language": "python", "size": 661}
{"docstring": "'''\n@summary: A constructor.\n@param create_data_func: The python function that returns the new source's bytes as a string. It'll be called with a single parameter: a Packet instance.\n@param items_list: A list of the items that will be read from the new source.\n'''\n", "func_signal": "def __init__(self, source_name, create_data_func, items_list):\n", "code": "self._create_data_func = create_data_func\nself._c_callback = PSdissect_func(self._callback)\nself._subitems = items_list\n\nself.old_offset = c_int(0)\nself.old_tvb = c_void_p(0)\nself._push_params = PSpush_tvb_params(source_name, None, 0, pointer(self.old_offset), pointer(self.old_tvb))\nself._pop_params = PSpop_tvb_params(pointer(self.old_offset), pointer(self.old_tvb))", "path": "python\\cal\\cal_types.py", "repo_name": "ashdnazg/pyreshark", "stars": 159, "license": "None", "language": "python", "size": 661}
{"docstring": "'''\n@summary: See ItemBase.\n'''\n", "func_signal": "def get_node_list(self):\n", "code": "node_list = self.get_parent_item().get_node_list() + [(self._cal.pslib.push_tree, self._params)]\nfor item in self.get_child_items():\n    node_list.extend(item.get_node_list())\nnode_list.extend([(self._cal.pslib.pop_tree, self._pop_params)])\nreturn node_list", "path": "python\\cal\\cal_types.py", "repo_name": "ashdnazg/pyreshark", "stars": 159, "license": "None", "language": "python", "size": 661}
{"docstring": "'''\n@summary: Adds a new text item to the tree.\n@param text: The text to be added.\n@param length: The number of bytes that'll be marked when selecting the item. The packet's offset is not advanced. (default: 0)\n@param offset: The beginning offset for the marked bytes. If set to None, offset=self.offset. (default: None)\n@return: the newly created item\n'''\n", "func_signal": "def add_text(self, text, length = 0, offset = None):\n", "code": "if offset is None:\n    _offset = self.offset\nelse:\n    _offset = offset\nreturn self._cal.wslib.proto_tree_add_text(self.p_new_tree, self.p_new_tvb, _offset, length, text)", "path": "python\\cal\\cal_types.py", "repo_name": "ashdnazg/pyreshark", "stars": 159, "license": "None", "language": "python", "size": 661}
{"docstring": "'''\n@summary: Generates the strings structure in the header_field_info.\n@strings_dict: The dictionary for translating the field's value into text.\n@return: A tuple of the form: (The new structure's address or 'None', the appropriate display flag). \n'''\n", "func_signal": "def _generate_strings_struct(self, strings_dict):\n", "code": "keys_type = None\nstr_display = 0\nfor key in strings_dict.iterkeys():\n    if keys_type is None:\n        keys_type = type(key)\n    elif keys_type != type(key):\n        raise Exception(\"String key %s is not of type %s\" % (key, keys_type))\nif keys_type == int:\n    vals_array_type = WSvalue_string * (len(strings_dict) + 1)\n    self._strings = vals_array_type(*([WSvalue_string(value, s) for value, s in strings_dict.iteritems()] + [WSvalue_string(0,None)]))\nelif keys_type == bool:\n    self._strings = WStrue_false_string(strings_dict[True], strings_dict[False])\nelif keys_type == tuple:\n    rvals_array_type = WSrange_string * (len(strings_dict) + 1)\n    self._strings = rvals_array_type(*([WSrange_string(min, max, s) for (min, max), s in strings_dict.iteritems()] + [WSrange_string(0,0, None)]))\n    str_display = BASE_RANGE_STRING\n\nreturn (addressof(self._strings), str_display)", "path": "python\\cal\\cal_types.py", "repo_name": "ashdnazg/pyreshark", "stars": 159, "license": "None", "language": "python", "size": 661}
{"docstring": "\"\"\"Set the base class for bucket objects created in the connection to\nthe MimicDB bucket class.\n\"\"\"\n", "func_signal": "def __init__(self, *args, **kwargs):\n", "code": "kwargs['bucket_class'] = Bucket\nsuper(S3Connection, self).__init__(*args, **kwargs)", "path": "mimicdb\\s3\\connection.py", "repo_name": "nathancahill/mimicdb", "stars": 234, "license": "bsd-2-clause", "language": "python", "size": 347}
{"docstring": "\"\"\"Called internally for any type of upload. After upload finishes,\nmake sure the key is in the bucket set and save the metadata.\n\"\"\"\n", "func_signal": "def _send_file_internal(self, *args, **kwargs):\n", "code": "super(Key, self)._send_file_internal(*args, **kwargs)\n\nmimicdb.backend.sadd(tpl.bucket % self.bucket.name, self.name)\nmimicdb.backend.hmset(tpl.key % (self.bucket.name, self.name),\n                    dict(size=self.size, md5=self.md5))", "path": "mimicdb\\s3\\key.py", "repo_name": "nathancahill/mimicdb", "stars": 234, "license": "bsd-2-clause", "language": "python", "size": 347}
{"docstring": "\"\"\"If 'force' is in the headers, retrieve the list of keys from S3.\nOtherwise, use the list() function to retrieve the keys from MimicDB.\n\"\"\"\n", "func_signal": "def _get_all(self, *args, **kwargs):\n", "code": "headers = kwargs.get('headers', args[2] if len(args) > 2 else None) or dict()\n\nif 'force' in headers:\n    keys = super(Bucket, self)._get_all(*args, **kwargs)\n\n    for key in keys:\n        mimicdb.backend.sadd(tpl.bucket % self.name, key.name)\n        mimicdb.backend.hmset(tpl.key % (self.name, key.name), dict(size=key.size, md5=key.etag.strip('\"')))\n\n        key.name = key.name\n\n    return keys\n\nprefix = kwargs.get('prefix', '')\n\nreturn list(self.list(prefix=prefix))", "path": "mimicdb\\s3\\bucket.py", "repo_name": "nathancahill/mimicdb", "stars": 234, "license": "bsd-2-clause", "language": "python", "size": 347}
{"docstring": "\"\"\"Add the key to the bucket set if the key name is set and metadata is\navailable for it, otherwise wait until uploaded or downloaded.\n\"\"\"\n", "func_signal": "def __init__(self, *args, **kwargs):\n", "code": "bucket = kwargs.get('bucket', args[0] if args else None)\nname = kwargs.get('name', args[1] if len(args) > 1 else None)\n\nself._name = name\n\nif name and bucket:\n    meta = mimicdb.backend.hgetall(tpl.key % (bucket.name, name))\n\n    if meta:\n        mimicdb.backend.sadd(tpl.bucket % bucket.name, name)\n        self._load_meta(meta['size'], meta['md5'])\n\nsuper(Key, self).__init__(*args, **kwargs)", "path": "mimicdb\\s3\\key.py", "repo_name": "nathancahill/mimicdb", "stars": 234, "license": "bsd-2-clause", "language": "python", "size": 347}
{"docstring": "\"\"\"Return the key from MimicDB.\n\n:param boolean force: If true, API call is forced to S3\n\"\"\"\n", "func_signal": "def get_key(self, *args, **kwargs):\n", "code": "if kwargs.pop('force', None):\n    headers = kwargs.get('headers', {})\n    headers['force'] = True\n    kwargs['headers'] = headers\n\nreturn super(Bucket, self).get_key(*args, **kwargs)", "path": "mimicdb\\s3\\bucket.py", "repo_name": "nathancahill/mimicdb", "stars": 234, "license": "bsd-2-clause", "language": "python", "size": 347}
{"docstring": "\"\"\"Set key attributes to retrived metadata. Might be extended in the\nfuture to support more attributes.\n\"\"\"\n", "func_signal": "def _load_meta(self, size, md5):\n", "code": "if not hasattr(self, 'local_hashes'):\n    self.local_hashes = {}\n\nself.size = int(size)\n\nif (re.match('^[a-fA-F0-9]{32}$', md5)):\n    self.md5 = md5", "path": "mimicdb\\s3\\key.py", "repo_name": "nathancahill/mimicdb", "stars": 234, "license": "bsd-2-clause", "language": "python", "size": 347}
{"docstring": "\"\"\"Return a list of keys from MimicDB.\n\n:param boolean force: If true, API call is forced to S3\n\"\"\"\n", "func_signal": "def get_all_keys(self, *args, **kwargs):\n", "code": "if kwargs.pop('force', None):\n    headers = kwargs.get('headers', args[0] if len(args) else None) or dict()\n    headers['force'] = True\n    kwargs['headers'] = headers\n\nreturn super(Bucket, self).get_all_keys(*args, **kwargs)", "path": "mimicdb\\s3\\bucket.py", "repo_name": "nathancahill/mimicdb", "stars": 234, "license": "bsd-2-clause", "language": "python", "size": 347}
{"docstring": "\"\"\"Remove each key or key name in an iterable from the bucket set.\n\"\"\"\n", "func_signal": "def delete_keys(self, *args, **kwargs):\n", "code": "ikeys = iter(kwargs.get('keys', args[0] if args else []))\n\nwhile True:\n    try:\n        key = ikeys.next()\n    except StopIteration:\n        break\n\n    if isinstance(key, basestring):\n        mimicdb.backend.srem(tpl.bucket % self.name, key)\n        mimicdb.backend.delete(tpl.key % (self.name, key))\n    elif isinstance(key, BotoKey) or isinstance(key, Key):\n        mimicdb.backend.srem(tpl.bucket % self.name, key.name)\n        mimicdb.backend.delete(tpl.key % (self.name, key.name))\n\nreturn super(Bucket, self).delete_keys(*args, **kwargs)", "path": "mimicdb\\s3\\bucket.py", "repo_name": "nathancahill/mimicdb", "stars": 234, "license": "bsd-2-clause", "language": "python", "size": 347}
{"docstring": "\"\"\"Return a bucket from MimicDB if it exists. Return a\nS3ResponseError if the bucket does not exist and validate is passed.\n\n:param boolean force: If true, API call is forced to S3\n\"\"\"\n", "func_signal": "def get_bucket(self, bucket_name, validate=True, headers=None, force=None):\n", "code": "if force:\n    bucket = super(S3Connection, self).get_bucket(bucket_name, validate, headers)\n    mimicdb.backend.sadd(tpl.connection, bucket.name)\n    return bucket\n\nif mimicdb.backend.sismember(tpl.connection, bucket_name):\n    return Bucket(self, bucket_name)\nelse:\n    if validate:\n        raise S3ResponseError(404, 'NoSuchBucket')\n    else:\n        return Bucket(self, bucket_name)", "path": "mimicdb\\s3\\connection.py", "repo_name": "nathancahill/mimicdb", "stars": 234, "license": "bsd-2-clause", "language": "python", "size": 347}
{"docstring": "\"\"\"Sync either a list of buckets or the entire connection.\n\nForce all API calls to S3 and populate the database with the current\nstate of S3.\n\n:param \\*string \\*buckets: Buckets to sync\n\"\"\"\n", "func_signal": "def sync(self, *buckets):\n", "code": "if buckets:\n    for _bucket in buckets:\n        for key in mimicdb.backend.smembers(tpl.bucket % _bucket):\n            mimicdb.backend.delete(tpl.key % (_bucket, key))\n\n        mimicdb.backend.delete(tpl.bucket % _bucket)\n\n        bucket = self.get_bucket(_bucket, force=True)\n\n        for key in bucket.list(force=True):\n            mimicdb.backend.sadd(tpl.bucket % bucket.name, key.name)\n            mimicdb.backend.hmset(tpl.key % (bucket.name, key.name), dict(size=key.size, md5=key.etag.strip('\"')))\nelse:\n    for bucket in mimicdb.backend.smembers(tpl.connection):\n        for key in mimicdb.backend.smembers(tpl.bucket % bucket):\n            mimicdb.backend.delete(tpl.key % (bucket, key))\n\n        mimicdb.backend.delete(tpl.bucket % bucket)\n\n    for bucket in self.get_all_buckets(force=True):\n        for key in bucket.list(force=True):\n            mimicdb.backend.sadd(tpl.bucket % bucket.name, key.name)\n            mimicdb.backend.hmset(tpl.key % (bucket.name, key.name), dict(size=key.size, md5=key.etag.strip('\"')))", "path": "mimicdb\\s3\\connection.py", "repo_name": "nathancahill/mimicdb", "stars": 234, "license": "bsd-2-clause", "language": "python", "size": 347}
{"docstring": "\"\"\"Set the class for key objects created in the bucket to the MimicDB\nkey class.\n\"\"\"\n", "func_signal": "def __init__(self, *args, **kwargs):\n", "code": "kwargs['key_class'] = Key\nsuper(Bucket, self).__init__(*args, **kwargs)", "path": "mimicdb\\s3\\bucket.py", "repo_name": "nathancahill/mimicdb", "stars": 234, "license": "bsd-2-clause", "language": "python", "size": 347}
{"docstring": "\"\"\"Called internally for any type of download. After download finishes,\nmake sure the key is in the bucket set and save the metadata.\n\"\"\"\n", "func_signal": "def _get_file_internal(self, *args, **kwargs):\n", "code": "super(Key, self)._get_file_internal(*args, **kwargs)\n\nmimicdb.backend.sadd(tpl.bucket % self.bucket.name, self.name)\nmimicdb.backend.hmset(tpl.key % (self.bucket.name, self.name),\n                    dict(size=self.size, md5=self.md5))", "path": "mimicdb\\s3\\key.py", "repo_name": "nathancahill/mimicdb", "stars": 234, "license": "bsd-2-clause", "language": "python", "size": 347}
{"docstring": "\"\"\"Return a list of buckets in MimicDB.\n\n:param boolean force: If true, API call is forced to S3\n\"\"\"\n", "func_signal": "def get_all_buckets(self, *args, **kwargs):\n", "code": "if kwargs.pop('force', None):\n    buckets = super(S3Connection, self).get_all_buckets(*args, **kwargs)\n\n    for bucket in buckets:\n        mimicdb.backend.sadd(tpl.connection, bucket.name)\n\n    return buckets\n\nreturn [Bucket(self, bucket) for bucket in mimicdb.backend.smembers(tpl.connection)]", "path": "mimicdb\\s3\\connection.py", "repo_name": "nathancahill/mimicdb", "stars": 234, "license": "bsd-2-clause", "language": "python", "size": 347}
{"docstring": "\"\"\"Sync a bucket.\n\nForce all API calls to S3 and populate the database with the current state of S3.\n\"\"\"\n", "func_signal": "def sync(self):\n", "code": "for key in mimicdb.backend.smembers(tpl.bucket % self.name):\n    mimicdb.backend.delete(tpl.key % (self.name, key))\n\nmimicdb.backend.delete(tpl.bucket % self.name)\nmimicdb.backend.sadd(tpl.connection, self.name)\n\nfor key in self.list(force=True):\n    mimicdb.backend.sadd(tpl.bucket % self.name, key.name)\n    mimicdb.backend.hmset(tpl.key % (self.name, key.name), dict(size=key.size, md5=key.etag.strip('\"')))", "path": "mimicdb\\s3\\bucket.py", "repo_name": "nathancahill/mimicdb", "stars": 234, "license": "bsd-2-clause", "language": "python", "size": 347}
{"docstring": "\"\"\"Initialze the MimicDB backend with an optional namespace.\n\"\"\"\n", "func_signal": "def __init__(self, backend=None, namespace=None):\n", "code": "if not backend:\n    from .backends.default import Redis\n    backend = Redis()\n\nglobals()['backend'] = backend\n\nif namespace:\n    from .backends import tpl\n    tpl.set_namespace(namespace)", "path": "mimicdb\\__init__.py", "repo_name": "nathancahill/mimicdb", "stars": 234, "license": "bsd-2-clause", "language": "python", "size": 347}
{"docstring": "\"\"\"Remove key name from bucket set.\n\"\"\"\n", "func_signal": "def _delete_key_internal(self, *args, **kwargs):\n", "code": "mimicdb.backend.srem(tpl.bucket % self.name, args[0])\nmimicdb.backend.delete(tpl.key % (self.name, args[0]))\n\nreturn super(Bucket, self)._delete_key_internal(*args, **kwargs)", "path": "mimicdb\\s3\\bucket.py", "repo_name": "nathancahill/mimicdb", "stars": 234, "license": "bsd-2-clause", "language": "python", "size": 347}
{"docstring": "\"\"\"Delete the bucket on S3 before removing it from MimicDB.\nIf the delete fails (usually because the bucket is not empty), do\nnot remove the bucket from the set.\n\"\"\"\n", "func_signal": "def delete_bucket(self, *args, **kwargs):\n", "code": "super(S3Connection, self).delete_bucket(*args, **kwargs)\n\nbucket = kwargs.get('bucket_name', args[0] if args else None)\n\nif bucket:\n    mimicdb.backend.srem(tpl.connection, bucket)", "path": "mimicdb\\s3\\connection.py", "repo_name": "nathancahill/mimicdb", "stars": 234, "license": "bsd-2-clause", "language": "python", "size": 347}
{"docstring": "\"\"\"Return None if key is not in the bucket set.\n\nPass 'force' in the headers to check S3 for the key, and after fetching\nthe key from S3, save the metadata and key to the bucket set.\n\"\"\"\n", "func_signal": "def _get_key_internal(self, *args, **kwargs):\n", "code": "if args[1] is not None and 'force' in args[1]:\n    key, res = super(Bucket, self)._get_key_internal(*args, **kwargs)\n\n    if key:\n        mimicdb.backend.sadd(tpl.bucket % self.name, key.name)\n        mimicdb.backend.hmset(tpl.key % (self.name, key.name),\n                            dict(size=key.size,\n                                 md5=key.etag.strip('\"')))\n    return key, res\n\nkey = None\n\nif mimicdb.backend.sismember(tpl.bucket % self.name, args[0]):\n    key = Key(self)\n    key.name = args[0]\n\nreturn key, None", "path": "mimicdb\\s3\\bucket.py", "repo_name": "nathancahill/mimicdb", "stars": 234, "license": "bsd-2-clause", "language": "python", "size": 347}
{"docstring": "\"\"\"Add the bucket to MimicDB after successful creation.\n\"\"\"\n", "func_signal": "def create_bucket(self, *args, **kwargs):\n", "code": "bucket = super(S3Connection, self).create_bucket(*args, **kwargs)\n\nif bucket:\n    mimicdb.backend.sadd(tpl.connection, bucket.name)\n\nreturn bucket", "path": "mimicdb\\s3\\connection.py", "repo_name": "nathancahill/mimicdb", "stars": 234, "license": "bsd-2-clause", "language": "python", "size": 347}
{"docstring": "\"\"\"Key name can be set by Key.key or Key.name. Key.key sets Key.name\ninternally, so just handle this property. When changing the key\nname, try to load it's metadata from MimicDB. If it's not available,\nthe key hasn't been uploaded, downloaded or synced so don't add it to\nthe bucket set (it also might have just been deleted,\nsee boto.s3.bucket.py#785)\n\"\"\"\n", "func_signal": "def name(self, value):\n", "code": "self._name = value\n\nif value:\n    meta = mimicdb.backend.hgetall(tpl.key % (self.bucket.name, value))\n\n    if meta:\n        mimicdb.backend.sadd(tpl.bucket % self.bucket.name, value)\n        self._load_meta(meta['size'], meta['md5'])", "path": "mimicdb\\s3\\key.py", "repo_name": "nathancahill/mimicdb", "stars": 234, "license": "bsd-2-clause", "language": "python", "size": 347}
{"docstring": "# do not export @-css-compiler to online \n", "func_signal": "def compress(self, browser = ALL):\n", "code": "if self.isOpmOperator():\n    return ''\n\nif not self.browser & browser:\n    return ''\nmsg = Cleaner.clean(self.statement)\nif not msg.endswith('}') and not msg.endswith(';'):\n    msg = msg + ';'\nreturn msg", "path": "ckstyle\\entity\\ExtraStatement.py", "repo_name": "wangjeaf/CSSCheckStyle", "stars": 142, "license": "bsd-3-clause", "language": "python", "size": 1532}
{"docstring": "'''\u8bb0\u5f55StyleSheet\u7684\u95ee\u9898'''\n", "func_signal": "def logStyleSheetMessage(self, checker, styleSheet, errors = None):\n", "code": "errorLevel = checker.getLevel()\nif errors is None:\n    errors = [checker.getMsg()]\nfor errorMsg in errors:\n    obj = {}\n    if errorMsg is None or errorMsg == '':\n        console.error('[TOOL] no errorMsg in your plugin, please check it')\n\n    #if errorMsg.find('${file}') == -1:\n    #    errorMsg = errorMsg + ' (from \"' + styleSheet.getFile() + '\")'\n    #else:\n    #    errorMsg = errorMsg.replace('${file}', styleSheet.getFile())\n\n    obj[\"errorMsg\"] = errorMsg\n    obj[\"file\"] = styleSheet.getFile()\n    obj[\"level\"] = 'stylesheet'\n    self.remember(errorLevel, obj);", "path": "ckstyle\\CssCheckerWrapper.py", "repo_name": "wangjeaf/CSSCheckStyle", "stars": 142, "license": "bsd-3-clause", "language": "python", "size": 1532}
{"docstring": "# make sure we use the last statement, so reverse and filter and reverse again\n# [a1, a2, b, c] ==> [c, b, a2, a1] ==> [c, b, a2] ==> [a2, b, c]\n", "func_signal": "def fix(self, ruleSet, config):\n", "code": "rules = ruleSet.getRules()\nrules.reverse()\nnewRules = []\ncollector = []\nfor rule in rules:\n    info = self.ruleInfo(rule)\n    if not info in collector:\n        collector.append(info)\n        newRules.append(rule)\nnewRules.reverse()\nruleSet.setRules(newRules)", "path": "ckstyle\\plugins\\FEDRemoveDuplicatedAttr.py", "repo_name": "wangjeaf/CSSCheckStyle", "stars": 142, "license": "bsd-3-clause", "language": "python", "size": 1532}
{"docstring": "'''\u8bb0\u5f55\u4e00\u4e2a\"\u89c4\u5219\u96c6\"\u4e2d\u7684\u95ee\u9898'''\n", "func_signal": "def logRuleSetMessage(self, checker, ruleSet, errors = None):\n", "code": "errorLevel = checker.getLevel()\nif errors is None:\n    errors = [checker.getMsg()]\nfor errorMsg in errors:\n    obj = {}\n    #if errorMsg.find('${selector}') == -1:\n    #    errorMsg = errorMsg + ' (from \"' + ruleSet.selector + '\")'\n    #else:\n    #    errorMsg = errorMsg.replace('${selector}', ruleSet.selector)\n    obj[\"errorMsg\"] = errorMsg\n    obj[\"selector\"] = ruleSet.selector\n    obj[\"level\"] = 'ruleset'\n    self.remember(errorLevel, obj);", "path": "ckstyle\\CssCheckerWrapper.py", "repo_name": "wangjeaf/CSSCheckStyle", "stars": 142, "license": "bsd-3-clause", "language": "python", "size": 1532}
{"docstring": "'''\u901a\u8fc7\u8def\u5f84\u68c0\u67e5css\u6587\u4ef6'''\n", "func_signal": "def checkFile(filePath, config = defaultConfig):\n", "code": "fileContent = open(filePath).read()\nconsole.log('[ckstyle] checking %s' % filePath)\nchecker = doCheck(fileContent, filePath, config)\npath = os.path.realpath(filePath + config.extension)\nif checker.hasError():\n    reporter = ReporterUtil.getReporter('json' if config.exportJson else 'text', checker)\n    reporter.doReport()\n    if config.printFlag:\n        if os.path.exists(path):\n            os.remove(path)\n        console.show(reporter.export() + '\\n')\n    else:\n        open(path, 'w').write(reporter.export())\n        console.show('[ckstyle] @see %s\\n' % path)\n    return False\nelse:\n    if config.exportJson:\n        console.show('{\"status\":\"ok\",\"result\":\"%s is ok\"}' % filePath)\n    else:\n        console.show('[ckstyle] %s is ok\\n' % filePath)\n    if os.path.exists(path):\n        os.remove(path)\n    return True", "path": "ckstyle\\doCssCheck.py", "repo_name": "wangjeaf/CSSCheckStyle", "stars": 142, "license": "bsd-3-clause", "language": "python", "size": 1532}
{"docstring": "'''\u6839\u636e\u68c0\u67e5\u5668\u7c7b\u578b\u7684\u4e0d\u540c\uff0c\u5206\u522b\u6ce8\u518c\u5230\u4e0d\u540c\u7684\u68c0\u67e5\u5668\u5217\u8868\u4e2d'''\n", "func_signal": "def registerChecker(self, checker):\n", "code": "if isinstance(checker, RuleChecker):\n    self.registerRuleChecker(checker)\nelif isinstance(checker, RuleSetChecker):\n    self.registerRuleSetChecker(checker)\nelif isinstance(checker, StyleSheetChecker):\n    self.registerStyleSheetChecker(checker)\nelse:\n    self.registerExtraChecker(checker)", "path": "ckstyle\\CssCheckerWrapper.py", "repo_name": "wangjeaf/CSSCheckStyle", "stars": 142, "license": "bsd-3-clause", "language": "python", "size": 1532}
{"docstring": "'''\u5c01\u88c5\u4e00\u4e0b'''\n", "func_signal": "def doFix(fileContent, fileName = '', config = defaultConfig):\n", "code": "config.operation = 'fixstyle'\n\nparser = CssParser(fileContent, fileName)\nparser.doParse(config)\n\nchecker = CssChecker(parser, config)\n\nchecker.loadPlugins(os.path.realpath(os.path.join(__file__, '../plugins')))\nfixed = checker.doFix()\n\nreturn checker, fixed", "path": "ckstyle\\doCssFix.py", "repo_name": "wangjeaf/CSSCheckStyle", "stars": 142, "license": "bsd-3-clause", "language": "python", "size": 1532}
{"docstring": "'''\u5141\u8bb8\u7528\u6237\u901a\u8fc7ckstyle install\u6dfb\u52a0plugin\uff0c\u6267\u884c\u7684\u65f6\u5019\u8f7d\u5165\u4e4b'''\n", "func_signal": "def loadFromUserPlugins(self):\n", "code": "include = self.config.include\nexclude = self.config.exclude\nsafeMode = self.config.safeMode\nsafeModeExcludes = 'combine-same-rulesets'\n\ndef import_module(name):\n    mod = __import__(name)\n    components = name.split('.')\n    for comp in components[1:]:\n        mod = getattr(mod, comp)\n    return mod\n    \nroot = os.path.realpath(os.path.join(__file__, '../userplugins/plugins'))\nmodulePath = self.getModulePath(root)\npluginClassName = 'PluginClass'\nfor filename in os.listdir(root):\n    if filename.startswith('.') or filename.startswith('_'):\n        continue\n    if not os.path.isdir(os.path.realpath(os.path.join(root, filename))):\n        continue\n    plugin = None\n    try:\n        plugin = import_module(modulePath + filename + '.index')\n    except Exception as e:\n        console.showError('Orz... can not load %s' % modulePath + filename + '.index')\n        continue\n    pluginClass = None\n    if hasattr(plugin, pluginClassName):\n        pluginClass = getattr(plugin, pluginClassName)\n    else:\n        console.showError('class %s should exist in %s.py' % (pluginClassName, filename + '/index'))\n        continue\n    self.registerPluginClass(pluginClass)", "path": "ckstyle\\CssCheckerWrapper.py", "repo_name": "wangjeaf/CSSCheckStyle", "stars": 142, "license": "bsd-3-clause", "language": "python", "size": 1532}
{"docstring": "'''\u8bb0\u5f55\u4ee3\u7801\u4e2d\u7684\u95ee\u9898'''\n", "func_signal": "def remember(self, errorLevel, errorMsg):\n", "code": "if errorLevel == ERROR_LEVEL.LOG:\n    if self.config.errorLevel > 1:\n        self.logMsgs.append(errorMsg)\nelif errorLevel == ERROR_LEVEL.WARNING:\n    if self.config.errorLevel > 0:\n        self.warningMsgs.append(errorMsg)\nelif errorLevel == ERROR_LEVEL.ERROR:\n    self.errorMsgs.append(errorMsg)\nelse:\n    console.error('[TOOL] wrong ErrorLevel for ' + errorMsg)", "path": "ckstyle\\CssCheckerWrapper.py", "repo_name": "wangjeaf/CSSCheckStyle", "stars": 142, "license": "bsd-3-clause", "language": "python", "size": 1532}
{"docstring": "'''\u8bb0\u5f55\u4e00\u6761key/value\u7684\u95ee\u9898'''\n", "func_signal": "def logRuleMessage(self, checker, rule, errors = None):\n", "code": "errorLevel = checker.getLevel()\nif errors is None:\n    errors = [checker.getMsg()]\nfor errorMsg in errors:\n    obj = {}\n    if errorMsg is None or errorMsg == '':\n        console.error('[TOOL] no errorMsg in your plugin, please check it')\n    #if errorMsg.find('${selector}') == -1:\n    #    errorMsg = errorMsg + ' (from \"' + rule.selector + '\")'\n    #else:\n    #    errorMsg = errorMsg.replace('${selector}', rule.selector)\n    #errorMsg = errorMsg.replace('${name}', rule.roughName.strip())\n    #errorMsg = errorMsg.replace('${value}', rule.value.strip())\n    obj[\"errorMsg\"] = errorMsg\n    obj[\"selector\"] = rule.selector\n    obj[\"name\"] = rule.roughName.strip()\n    obj[\"value\"] = rule.value.strip()\n    obj[\"level\"] = 'rule'\n    self.remember(errorLevel, obj);", "path": "ckstyle\\CssCheckerWrapper.py", "repo_name": "wangjeaf/CSSCheckStyle", "stars": 142, "license": "bsd-3-clause", "language": "python", "size": 1532}
{"docstring": "'''\u5c01\u88c5\u4e00\u4e0b'''\n\n", "func_signal": "def doCheck(fileContent, fileName = '', config = defaultConfig):\n", "code": "config.operation = 'ckstyle'\nparser = CssParser(fileContent, fileName)\nparser.doParse(config)\n\nchecker = CssChecker(parser, config)\n\nchecker.loadPlugins(os.path.realpath(os.path.join(__file__, '../plugins')))\nchecker.doCheck()\n\nreturn checker", "path": "ckstyle\\doCssCheck.py", "repo_name": "wangjeaf/CSSCheckStyle", "stars": 142, "license": "bsd-3-clause", "language": "python", "size": 1532}
{"docstring": "# make sure we use the last statement, so reverse and filter and reverse again\n# [a1, a2, b, c] ==> [c, b, a2, a1] ==> [c, b, a2] ==> [a2, b, c]\n", "func_signal": "def fix(self, ruleSet, config):\n", "code": "rules = ruleSet.getRules()\nrules.reverse()\nnewRules = []\ncollector = []\nfor rule in rules:\n    info = self.ruleInfo(rule)\n    if not info in collector:\n        collector.append(info)\n        newRules.append(rule)\nnewRules.reverse()\nruleSet.setRules(newRules)", "path": "utils\\AllRules.py", "repo_name": "wangjeaf/CSSCheckStyle", "stars": 142, "license": "bsd-3-clause", "language": "python", "size": 1532}
{"docstring": "#[\"help\", \"browsers=\", \"compressExtension=\", \"combineFile=\"]\n\n", "func_signal": "def parseCompressCmdArgs(defaultConfigFile, opts, args, debug = False):\n", "code": "browsers = None\nextension = None\ncombineFile = None\nsafeMode = None\nnoBak = None\nfor op, value in opts:\n    if op == \"--help\" or op == '-h':\n        usage_compress()\n        sys.exit()\n    elif op == '--compressExtension':\n        extension = getExtension(value)\n    elif op == '--browsers':\n        browsers = analyse(getValue(value).replace('\"', ''))\n    elif op == '--combineFile':\n        combineFile = getValue(value).lower() == 'true'\n    elif op == '--safeMode':\n        safeMode = True\n    elif op == '--noBak':\n        noBak = True\nconfig = parseCkStyleCmdArgs(defaultConfigFile, opts, args, debug, True)\nargs = config.compressConfig\nif safeMode is not None: config.safeMode = safeMode\nif browsers is not None: args.browsers = browsers\nif extension is not None: args.extension = extension\nif combineFile is not None: args.combineFile = combineFile\nif noBak is not None: args.noBak = noBak\n\nreturn config", "path": "ckstyle\\command\\ConsoleCommandParser.py", "repo_name": "wangjeaf/CSSCheckStyle", "stars": 142, "license": "bsd-3-clause", "language": "python", "size": 1532}
{"docstring": "# \u5ffd\u7565\u7684\u89c4\u5219\u96c6\uff08\u76ee\u524d\u53ea\u5ffd\u7565\u5355\u5143\u6d4b\u8bd5\u7684selector\uff09\n", "func_signal": "def doCheck(self):\n", "code": "ignoreRuleSets = self.config.ignoreRuleSets\n\ndef findInArray(array, value):\n    return value in array or value.strip() in array\n\ndef isBoolean(value):\n    return type(value) == type(True)\n\ndef isList(value):\n    return isinstance(value, list)\n\n# \u68c0\u67e5\u89c4\u5219\u96c6\ndef checkRuleSet(ruleSet):\n    for checker in self.ruleSetCheckers:\n        if not hasattr(checker, 'check'):\n            continue\n        result = checker.check(ruleSet, self.config)\n        if isBoolean(result):\n            if not result:\n                self.logRuleSetMessage(checker, ruleSet)\n        elif isList(result) and len(result) != 0:\n            self.logRuleSetMessage(checker, ruleSet, result)\n        else:\n            console.error('check should be boolean/list, %s is not.' % checker.id)\n\n# \u68c0\u67e5\u89c4\u5219\ndef checkRule(ruleSet):\n    for checker in self.ruleCheckers:\n        for rule in ruleSet.getRules():\n            if not hasattr(checker, 'check'):\n                continue\n            result = checker.check(rule, self.config)\n            if isBoolean(result):\n                if not result:\n                    self.logRuleMessage(checker, rule)\n            elif isList(result) and len(result) != 0:\n                self.logRuleMessage(checker, rule, result)\n            else:\n                console.error('check should be boolean/list, %s is not.' % checker.id)\n\n# \u68c0\u67e5\u89c4\u5219\ndef checkExtraRule(ruleSet):\n    for checker in self.extraCheckers:\n        if not hasattr(checker, 'check'):\n            continue\n        result = checker.check(ruleSet, self.config)\n        if isBoolean(result):\n            if not result:\n                self.logRuleSetMessage(checker, ruleSet)\n        elif isList(result) and len(result) != 0:\n            self.logRuleSetMessage(checker, ruleSet, result)\n        else:\n            console.error('check should be boolean/list, %s is not.' % checker.id)\n\n# \u68c0\u67e5\u6837\u5f0f\u8868\nstyleSheet = self.parser.styleSheet\nfor checker in self.styleSheetCheckers:\n    if not hasattr(checker, 'check'):\n        continue\n    result = checker.check(styleSheet, self.config)\n    if isBoolean(result):\n        if not result:\n            self.logStyleSheetMessage(checker, styleSheet)\n    elif isList(result) and len(result) != 0:\n        self.logStyleSheetMessage(checker, styleSheet, result)\n    else:\n        console.error('check should be boolean/list, %s is not.' % checker.id)\n\nfor ruleSet in styleSheet.getRuleSets():\n    if ruleSet.extra:\n        checkExtraRule(ruleSet)\n        continue\n    # \u5224\u65ad\u6b64\u89c4\u5219\u662f\u5426\u5ffd\u7565\n    if findInArray(ignoreRuleSets, ruleSet.selector):\n        continue\n    checkRuleSet(ruleSet)\n    checkRule(ruleSet)", "path": "ckstyle\\CssCheckerWrapper.py", "repo_name": "wangjeaf/CSSCheckStyle", "stars": 142, "license": "bsd-3-clause", "language": "python", "size": 1532}
{"docstring": "# if fix upper to lower, will cause error in HTML, do not do evil\n", "func_signal": "def _upper():\n", "code": "return\nfixer, msg = doFix('.TEST {width:100px;}', '')\nstyleSheet = fixer.getStyleSheet()\nruleSet = styleSheet.getRuleSets()[0]\nequal(ruleSet.fixedSelector, '.test', 'selector all upper is ok')\n\nfixer, msg = doFix('.Test {width:100px;}', '')\nstyleSheet = fixer.getStyleSheet()\nruleSet = styleSheet.getRuleSets()[0]\nequal(ruleSet.fixedSelector, '.test', 'selector one upper is ok')\n\nfixer, msg = doFix('.Test-WRAPPER {width:100px;}', '')\nstyleSheet = fixer.getStyleSheet()\nruleSet = styleSheet.getRuleSets()[0]\nequal(ruleSet.fixedSelector, '.test-wrapper', 'selector upper with - is ok')", "path": "tests\\unit\\fix\\FixFEDUseLowerCaseSelector.py", "repo_name": "wangjeaf/CSSCheckStyle", "stars": 142, "license": "bsd-3-clause", "language": "python", "size": 1532}
{"docstring": "'''\u5c01\u88c5\u4e00\u4e0b'''\n", "func_signal": "def doCompress(fileContent, fileName = '', config = defaultConfig):\n", "code": "checker = prepare(fileContent, fileName, config)\nmessage = checker.doCompress()\nreturn checker, message", "path": "ckstyle\\doCssCompress.py", "repo_name": "wangjeaf/CSSCheckStyle", "stars": 142, "license": "bsd-3-clause", "language": "python", "size": 1532}
{"docstring": "#self.resetStyleSheet()\n# \u5ffd\u7565\u7684\u89c4\u5219\u96c6\uff08\u76ee\u524d\u53ea\u5ffd\u7565\u5355\u5143\u6d4b\u8bd5\u7684selector\uff09\n", "func_signal": "def doFix(self, browser = ALL):\n", "code": "ignoreRuleSets = self.config.ignoreRuleSets\n\ndef findInArray(array, value):\n    return value in array or value.strip() in array\n\n# fix\u89c4\u5219\u96c6\ndef fixRuleSet(ruleSet):\n    for checker in self.ruleSetCheckers:\n        if not hasattr(checker, 'fix'):\n            continue\n        if ruleSet.fixedSelector == '':\n            ruleSet.fixedSelector = ruleSet.selector\n            ruleSet.fixedComment = ruleSet.comment\n        checker.fix(ruleSet, self.config)\n\n# fix\u89c4\u5219\ndef fixRules(ruleSet):\n    for checker in self.ruleCheckers:\n        for rule in ruleSet.getRules():\n            if not hasattr(checker, 'fix'):\n                continue\n\n            # \u786e\u4fddfixedName/fixedValue\u4e00\u5b9a\u6709\u503c\n            # fix\u4e2d\u4e00\u5b9a\u8981\u9488\u5bf9fixedName/fixedValue\u6765\u5224\u65ad\uff0c\u786e\u4fdd\u5176\u4ed6plugin\u7684fix\u4e0d\u4f1a\u88ab\u8986\u76d6\n            if rule.fixedValue == '':\n                rule.fixedValue = rule.value\n                rule.fixedName = rule.strippedName\n\n            #print checker.id, checker, rule.fixedValue\n            checker.fix(rule, self.config)\n\ndef fixExtraRules(ruleSet):\n    for checker in self.extraCheckers:\n        if not hasattr(checker, 'fix'):\n            continue\n        if ruleSet.fixedSelector == '':\n            ruleSet.fixedSelector = ruleSet.selector\n            ruleSet.fixedStatement = ruleSet.statement\n\n        checker.fix(ruleSet, self.config)\n\nstyleSheet = self.parser.styleSheet\n\nfor ruleSet in styleSheet.getRuleSets():\n    if ruleSet.extra:\n        fixExtraRules(ruleSet)\n        continue\n    # \u5224\u65ad\u6b64\u89c4\u5219\u662f\u5426\u5ffd\u7565\n    if findInArray(ignoreRuleSets, ruleSet.selector):\n        continue\n    # \u5148fix rule\n    fixRules(ruleSet)\n    # \u518dfix ruleSet\n    fixRuleSet(ruleSet)\n\n# \u6700\u540efix styleSheet\nfor checker in self.styleSheetCheckers:\n    if hasattr(checker, 'fix'):\n        checker.fix(styleSheet, self.config)\nreturn self.getStyleSheet().fixed(self.config)", "path": "ckstyle\\CssCheckerWrapper.py", "repo_name": "wangjeaf/CSSCheckStyle", "stars": 142, "license": "bsd-3-clause", "language": "python", "size": 1532}
{"docstring": "'''\u5982\u679c\u662fdebug\u6a21\u5f0f\uff0c\u5219\u4ece\u7ec6\u5c0f\u7684plugin\u6587\u4ef6\u4e2d\u8f7d\u5165\u63d2\u4ef6\uff0c\u5426\u5219\uff0c\u4ece\u5927\u6587\u4ef6\u4e2d\u8f7d\u5165\u63d2\u4ef6'''\n", "func_signal": "def loadPlugins(self, pluginDir, debug = True):\n", "code": "if debug:\n    self.loadFromSubFiles(pluginDir)\n    self.loadFromUserPlugins()\nelse:\n    self.loadFromBigFile(pluginDir)", "path": "ckstyle\\CssCheckerWrapper.py", "repo_name": "wangjeaf/CSSCheckStyle", "stars": 142, "license": "bsd-3-clause", "language": "python", "size": 1532}
{"docstring": "'''\u4eceplugins\u76ee\u5f55\u52a8\u6001\u8f7d\u5165\u68c0\u67e5\u7c7b'''\n", "func_signal": "def loadFromSubFiles(self, pluginDir):\n", "code": "modulePath = self.getModulePath(pluginDir)\nfor filename in os.listdir(pluginDir):\n    if not filename.endswith('.py') or filename.startswith('_'):\n        continue\n    if filename == 'Base.py' or filename == 'helper.py':\n        continue\n    pluginName = os.path.splitext(filename)[0]\n\n    # \u83b7\u53d6plugins\u7684\u5f15\u7528\n    plugin = __import__(modulePath + pluginName, fromlist = [pluginName])\n    pluginClass = None\n    if hasattr(plugin, pluginName):\n        pluginClass = getattr(plugin, pluginName)\n    else:\n        console.error('[TOOL] class %s should exist in %s.py' % (pluginName, pluginName))\n        continue\n    self.registerPluginClass(pluginClass)", "path": "ckstyle\\CssCheckerWrapper.py", "repo_name": "wangjeaf/CSSCheckStyle", "stars": 142, "license": "bsd-3-clause", "language": "python", "size": 1532}
{"docstring": "'''\u4eceAllRules.py\u52a8\u6001\u8f7d\u5165\u68c0\u67e5\u7c7b'''\n", "func_signal": "def loadFromBigFile(self, pluginDir):\n", "code": "plugin = __import__(\"ckstyle.plugins.AllRules\", fromlist = ['AllRules'])\nprops = dir(plugin)\nfor prop in props:\n    if not prop.startswith('FED'):\n        continue\n    pluginClass = getattr(plugin, prop)\n    self.registerPluginClass(pluginClass)", "path": "ckstyle\\CssCheckerWrapper.py", "repo_name": "wangjeaf/CSSCheckStyle", "stars": 142, "license": "bsd-3-clause", "language": "python", "size": 1532}
{"docstring": "\"\"\"\npred class os instance X\n\"\"\"\n", "func_signal": "def single_pred(self, record):\n", "code": "if self.value:\n    return self.value\nelse:\n    # goes to sub tree\n    feature_idx, theta = self.branch_params\n    branch = record[feature_idx] - theta\n    if branch < 0:\n        return self.left.single_pred(record)\n    else:\n        return self.right.single_pred(record)", "path": "decision-tree\\DecisionTree\\tree\\dtree.py", "repo_name": "leezqcst/machine-learning-algorithm", "stars": 133, "license": "None", "language": "python", "size": 44438}
{"docstring": "\"\"\"Gaussian distribution in log\"\"\"\n", "func_signal": "def prob(self, x, mean, std):\n", "code": "exponent = math.exp(- ((x - mean)**2 / (2 * std**2)))\nreturn math.log(exponent / (math.sqrt(2 * math.pi) * std))", "path": "Naive-bayes\\naive_bayes.py", "repo_name": "leezqcst/machine-learning-algorithm", "stars": 133, "license": "None", "language": "python", "size": 44438}
{"docstring": "\"\"\"\n\u7528\u8bad\u7ec3\u597d\u7684\u51b3\u7b56\u6811\u5bf9\u8f93\u5165\u6570\u636e\u5206\u7c7b\n\u51b3\u7b56\u6811\u7684\u6784\u5efa\u662f\u4e00\u4e2a\u9012\u5f52\u7684\u8fc7\u7a0b\uff0c\u7528\u51b3\u7b56\u6811\u5206\u7c7b\u4e5f\u662f\u4e00\u4e2a\u9012\u5f52\u7684\u8fc7\u7a0b\n_classify()\u4e00\u6b21\u53ea\u80fd\u5bf9\u4e00\u4e2a\u6837\u672c\uff08sample\uff09\u5206\u7c7b\nTo Do: \u591a\u4e2asample\u7684\u9884\u6d4b\u600e\u6837\u5e76\u884c\u5316\uff1f\n\"\"\"\n", "func_signal": "def _classify(tree,sample):\n", "code": "featIndex = list(tree.keys())[0]\nsecondDict = tree[featIndex]\nkey = sample[int(featIndex[1:])]\nvalueOfkey = secondDict[key]\nif isinstance(valueOfkey, dict):\n    label = _classify(valueOfkey,sample)\nelse: label = valueOfkey\nreturn label", "path": "decision-tree\\id3_c45.py", "repo_name": "leezqcst/machine-learning-algorithm", "stars": 133, "license": "None", "language": "python", "size": 44438}
{"docstring": "\"\"\"\nreturn tuple (feature_idx, sign, theta)\n\"\"\"\n", "func_signal": "def _best_split(cls, X, y):\n", "code": "n = X.shape[0]\nnum_feature = X.shape[1]\ny_types = np.unique(y)\n\n# initialize\nmin_score = float(n)\nfeature_idx = None\nbest_theta = None\nbest_idx = None\n\nfor feature_idx in xrange(num_feature):\n    # counter for y\n    cumulate_y = Counter()\n    rest_y = Counter()\n    for y_type in y_types:\n        cnt = np.where(y == y_type)[0].shape[0]\n        rest_y[y_type] = cnt\n\n    # sorted data\n    sorted_idx = np.argsort(X[:, feature_idx])\n    sorted_X = np.copy(X)\n    sorted_y = np.copy(y)\n    sorted_X = sorted_X[sorted_idx]\n    sorted_y = sorted_y[sorted_idx]\n    #print \"_best_split:\", sorted_X.shape, sorted_y.shape\n\n    for idx in xrange(n-1):\n        theta = (sorted_X[idx, feature_idx] + sorted_X[idx + 1, feature_idx]) / 2\n        y_label = sorted_y[idx]\n        cumulate_y[y_label] += 1\n        rest_y[y_label] -= 1\n        left_cnt = sum(cumulate_y.values())\n        right_cnt = sum(rest_y.values())\n        w_1 = left_cnt * cls._gini_index(cumulate_y.values())\n        w_2 = right_cnt * cls._gini_index(rest_y.values())\n        score = w_1 + w_2\n        if score < min_score:\n            min_score = score\n            best_theta = theta\n            best_idx = feature_idx\n            #print('new min score: %.3f' % score)\n            #print('feature: %d, theta: %.3f' % (best_idx, best_theta))\n            #print('left: %d, right: %d' % (left_cnt, right_cnt))\nprint('feature: %d, theta: %.3f' % (best_idx, best_theta))\nreturn (best_idx, best_theta)", "path": "decision-tree\\DecisionTree\\tree\\dtree.py", "repo_name": "leezqcst/machine-learning-algorithm", "stars": 133, "license": "None", "language": "python", "size": 44438}
{"docstring": "\"\"\"\nMake linear prediction based on cost and gradient descent\n:param X: new data to make predictions on\n:return: return prediction\n\"\"\"\n", "func_signal": "def predict(self, X):\n", "code": "if self.intercept:\n    intercept = np.ones((np.shape(X)[0],1))\n    X = np.concatenate((intercept, X), 1)\n\nnum_examples, num_features = np.shape(X)\nprediction = []\nfor sample in range(num_examples):\n    yhat = 0\n    for value in range(num_features):\n        yhat += X[sample, value] * self.theta[value]\n    prediction.append(yhat)\n        \nreturn prediction", "path": "linear-regression\\LinearRegression.py", "repo_name": "leezqcst/machine-learning-algorithm", "stars": 133, "license": "None", "language": "python", "size": 44438}
{"docstring": "\"\"\"\nreturn tuple (feature_idx, sign, theta)\n\"\"\"\n", "func_signal": "def _best_split(cls, X, y):\n", "code": "n = X.shape[0]\nnum_feature = X.shape[1]\ny_types = np.unique(y)\n\n# initialize\nmin_score = float(n)\nfeature_idx = None\nbest_theta = None\nbest_idx = None\n\nfor feature_idx in xrange(num_feature):\n    # counter for y\n    cumulate_y = Counter()\n    rest_y = Counter()\n    for y_type in y_types:\n        cnt = np.where(y == y_type)[0].shape[0]\n        rest_y[y_type] = cnt\n\n    # sorted data\n    sorted_idx = np.argsort(X[:, feature_idx])\n    sorted_X = np.copy(X)\n    sorted_y = np.copy(y)\n    sorted_X = sorted_X[sorted_idx]\n    sorted_y = sorted_y[sorted_idx]\n    #print \"_best_split:\", sorted_X.shape, sorted_y.shape\n\n    for idx in xrange(n-1):\n        theta = (sorted_X[idx, feature_idx] + sorted_X[idx + 1, feature_idx]) / 2\n        y_label = sorted_y[idx]\n        cumulate_y[y_label] += 1\n        rest_y[y_label] -= 1\n        left_cnt = sum(cumulate_y.values())\n        right_cnt = sum(rest_y.values())\n        w_1 = left_cnt * cls._gini_index(cumulate_y.values())\n        w_2 = right_cnt * cls._gini_index(rest_y.values())\n        score = w_1 + w_2\n        if score < min_score:\n            min_score = score\n            best_theta = theta\n            best_idx = feature_idx\n            #print('new min score: %.3f' % score)\n            #print('feature: %d, theta: %.3f' % (best_idx, best_theta))\n            #print('left: %d, right: %d' % (left_cnt, right_cnt))\nprint('feature: %d, theta: %.3f' % (best_idx, best_theta))\nreturn (best_idx, best_theta)", "path": "decision-tree\\DecisionTree\\tree\\dtree.py", "repo_name": "leezqcst/machine-learning-algorithm", "stars": 133, "license": "None", "language": "python", "size": 44438}
{"docstring": "\"\"\"\nactivataion function\n\nx: N x m array\nw: m x n array\n\"\"\"\n", "func_signal": "def predict(self, x):\n", "code": "y = np.dot(x, self.w)\nreturn np.where(y > 0, 1, 0)", "path": "perceptron\\perceptron_.py", "repo_name": "leezqcst/machine-learning-algorithm", "stars": 133, "license": "None", "language": "python", "size": 44438}
{"docstring": "\"\"\"\n\u51fd\u6570\u529f\u80fd\uff1a\u8ba1\u7b97\u71b5\n\u53c2\u6570y\uff1a\u6570\u636e\u96c6\u7684\u6807\u7b7e\n\"\"\"\n", "func_signal": "def _calcEntropy(self,y):\n", "code": "num = y.shape[0]\n#\u7edf\u8ba1y\u4e2d\u4e0d\u540clabel\u503c\u7684\u4e2a\u6570\uff0c\u5e76\u7528\u5b57\u5178labelCounts\u5b58\u50a8\nlabelCounts = {}\nfor label in y:\n    if label not in labelCounts.keys(): labelCounts[label] = 0\n    labelCounts[label] += 1\n#\u8ba1\u7b97\u71b5\nentropy = 0.0\nfor key in labelCounts:\n    prob = float(labelCounts[key])/num\n    entropy -= prob * np.log2(prob)\nreturn entropy", "path": "decision-tree\\id3_c45.py", "repo_name": "leezqcst/machine-learning-algorithm", "stars": 133, "license": "None", "language": "python", "size": 44438}
{"docstring": "\"\"\"\u5efa\u7acb\u51b3\u7b56\u6811\nfeatureIndex\uff0c\u7c7b\u578b\u662f\u5143\u7ec4\uff0c\u5b83\u8bb0\u5f55\u4e86X\u4e2d\u7684\u7279\u5f81\u5728\u539f\u59cb\u6570\u636e\u4e2d\u5bf9\u5e94\u7684\u4e0b\u6807\u3002\n\"\"\"\n", "func_signal": "def _createTree(self,X,y,featureIndex):\n", "code": "labelList = list(y)\n#\u6240\u6709label\u90fd\u76f8\u540c\u7684\u8bdd\uff0c\u5219\u505c\u6b62\u5206\u5272\uff0c\u8fd4\u56de\u8be5label\nif labelList.count(labelList[0]) == len(labelList):\n    return labelList[0]\n#\u6ca1\u6709\u7279\u5f81\u53ef\u5206\u5272\u65f6\uff0c\u505c\u6b62\u5206\u5272\uff0c\u8fd4\u56de\u51fa\u73b0\u6b21\u6570\u6700\u591a\u7684label\nif len(featureIndex) == 0:\n    return self._majorityCnt(labelList)\n\n#\u53ef\u4ee5\u7ee7\u7eed\u5206\u5272\u7684\u8bdd\uff0c\u786e\u5b9a\u6700\u4f73\u5206\u5272\u7279\u5f81\nif self._mode == 'C4.5':\n    bestFeatIndex = self._chooseBestFeatureToSplit_C45(X,y)\nelif self._mode == 'ID3':\n    bestFeatIndex = self._chooseBestFeatureToSplit_ID3(X,y)\n\nbestFeatStr = featureIndex[bestFeatIndex]\nfeatureIndex = list(featureIndex)\nfeatureIndex.remove(bestFeatStr)\nfeatureIndex = tuple(featureIndex)\n#\u7528\u5b57\u5178\u5b58\u50a8\u51b3\u7b56\u6811\u3002\u6700\u4f73\u5206\u5272\u7279\u5f81\u4f5c\u4e3akey\uff0c\u800c\u5bf9\u5e94\u7684\u952e\u503c\u4ecd\u7136\u662f\u4e00\u68f5\u6811\uff08\u4ecd\u7136\u7528\u5b57\u5178\u5b58\u50a8\uff09\nmyTree = {bestFeatStr:{}}\nfeatValues = X[:,bestFeatIndex]\nuniqueVals = set(featValues)\nfor value in uniqueVals:\n    #\u5bf9\u6bcf\u4e2avalue\u9012\u5f52\u5730\u521b\u5efa\u6811\n    sub_X,sub_y = self._splitDataSet(X,y, bestFeatIndex, value)\n    myTree[bestFeatStr][value] = self._createTree(sub_X,sub_y,featureIndex)\nreturn myTree", "path": "decision-tree\\id3_c45.py", "repo_name": "leezqcst/machine-learning-algorithm", "stars": 133, "license": "None", "language": "python", "size": 44438}
{"docstring": "\"\"\"\n\u51fd\u6570\u529f\u80fd\uff1a\u8fd4\u56delabelList\u4e2d\u51fa\u73b0\u6b21\u6570\u6700\u591a\u7684label\n\"\"\"\n", "func_signal": "def _majorityCnt(self,labelList):\n", "code": "labelCount={}\nfor vote in labelList:\n    if vote not in labelCount.keys(): labelCount[vote] = 0\n    labelCount[vote] += 1\nsortedClassCount = sorted(labelCount.iteritems(),key=lambda x:x[1], reverse=True)\nreturn sortedClassCount[0][0]", "path": "decision-tree\\id3_c45.py", "repo_name": "leezqcst/machine-learning-algorithm", "stars": 133, "license": "None", "language": "python", "size": 44438}
{"docstring": "\"\"\"\nMake linear prediction based on cost and gradient descent\n\n:param X: new data to make predictions on\n:param labels: boolean\n:return: return prediction\n\"\"\"\n", "func_signal": "def predict(self, X, labels):\n", "code": "if self.intercept:\n    intercept = np.ones((np.shape(X)[0],1))\n    X = np.concatenate((intercept, X), 1)\n    \nnum_examples, num_features = np.shape(X)\nprediction = []\nfor sample in range(num_examples):\n    yhat = 0\n    for value in range(num_features):\n        yhat += X[sample, value] * self.theta[value]\n    \n    pred = self.sigmoid(yhat)\n    \n    if labels:\n        if pred > 0.5:\n            prediction.append(int(1))\n        else:\n            prediction.append(int(0))\n    else:\n        prediction.append(yhat)   \n        \nreturn prediction", "path": "logistic-regression\\LogisticClassifier.py", "repo_name": "leezqcst/machine-learning-algorithm", "stars": 133, "license": "None", "language": "python", "size": 44438}
{"docstring": "\"\"\"\npred class os instance X\n\"\"\"\n# goes to sub tree\n", "func_signal": "def single_pred(self, record):\n", "code": "sign, feature_idx, theta = self.branch_params\nbranch = sign * (record[feature_idx] - theta)\nreturn -1. if branch < 0 else 1.", "path": "decision-tree\\DecisionTree\\tree\\dtree.py", "repo_name": "leezqcst/machine-learning-algorithm", "stars": 133, "license": "None", "language": "python", "size": 44438}
{"docstring": "\"\"\"\nAllocate a LeNetConvPoolLayer with shared variable internal parameters.\n\n:type rng: numpy.random.RandomState\n:param rng: a random number generator used to initialize weights\n\n:type input: theano.tensor.dtensor4\n:param input: symbolic image tensor, of shape image_shape\n\n:type filter_shape: tuple or list of length 4\n:param filter_shape: (number of filters, num input feature maps,\n                      filter height, filter width)\n\n:type image_shape: tuple or list of length 4\n:param image_shape: (batch size, num input feature maps,\n                     image height, image width)\n\n:type poolsize: tuple or list of length 2\n:param poolsize: the downsampling (pooling) factor (#rows, #cols)\n\"\"\"\n\n", "func_signal": "def __init__(self, rng, input, filter_shape, image_shape, poolsize=(2, 2)):\n", "code": "assert image_shape[1] == filter_shape[1]\nself.input = input\n\n# there are \"num input feature maps * filter height * filter width\"\n# inputs to each hidden unit\nfan_in = numpy.prod(filter_shape[1:])\n# each unit in the lower layer receives a gradient from:\n# \"num output feature maps * filter height * filter width\" /\n#   pooling size\nfan_out = (filter_shape[0] * numpy.prod(filter_shape[2:]) /\n           numpy.prod(poolsize))\n# initialize weights with random weights\nW_bound = numpy.sqrt(6. / (fan_in + fan_out))\nself.W = theano.shared(\n    numpy.asarray(\n        rng.uniform(low=-W_bound, high=W_bound, size=filter_shape),\n        dtype=theano.config.floatX\n    ),\n    borrow=True\n)\n\n# the bias is a 1D tensor -- one bias per output feature map\nb_values = numpy.zeros((filter_shape[0],), dtype=theano.config.floatX)\nself.b = theano.shared(value=b_values, borrow=True)\n\n# convolve input feature maps with filters\nconv_out = conv.conv2d(\n    input=input,\n    filters=self.W,\n    filter_shape=filter_shape,\n    image_shape=image_shape\n)\n\n# downsample each feature map individually, using maxpooling\npooled_out = downsample.max_pool_2d(\n    input=conv_out,\n    ds=poolsize,\n    ignore_border=True\n)\n\n# add the bias term. Since the bias is a vector (1D array), we first\n# reshape it to a tensor of shape (1, n_filters, 1, 1). Each bias will\n# thus be broadcasted across mini-batches and feature map\n# width & height\nself.output = T.tanh(pooled_out + self.b.dimshuffle('x', 0, 'x', 'x'))\n\n# store parameters of this layer\nself.params = [self.W, self.b]", "path": "Deep-Learning\\DeepLearning Tutorials\\cnn_LeNet\\convolutional_mlp.py", "repo_name": "leezqcst/machine-learning-algorithm", "stars": 133, "license": "None", "language": "python", "size": 44438}
{"docstring": "\"\"\"\n\u51fd\u6570\u529f\u80fd\uff1a\u8fd4\u56de\u6570\u636e\u96c6\u4e2d\u7279\u5f81\u4e0b\u6807\u4e3aindex\uff0c\u7279\u5f81\u503c\u7b49\u4e8evalue\u7684\u5b50\u6570\u636e\u96c6\n\"\"\"\n", "func_signal": "def _splitDataSet(self,X,y,index,value):\n", "code": "ret = []\nfeatVec = X[:,index]\nX = X[:,[i for i in range(X.shape[1]) if i!=index]]\nfor i in range(len(featVec)):\n    if featVec[i]==value:\n        ret.append(i)\nreturn X[ret,:],y[ret]", "path": "decision-tree\\id3_c45.py", "repo_name": "leezqcst/machine-learning-algorithm", "stars": 133, "license": "None", "language": "python", "size": 44438}
{"docstring": "\"\"\"Gaussian distribution\"\"\"\n", "func_signal": "def calculateProbability(x, mean, stdev):\n", "code": "exponent = math.exp(- ((x - mean)**2 / (2 * stdev**2)))\nreturn exponent / (math.sqrt(2 * math.pi) * stdev)", "path": "Naive-bayes\\old_naive_bayes.py", "repo_name": "leezqcst/machine-learning-algorithm", "stars": 133, "license": "None", "language": "python", "size": 44438}
{"docstring": "# setup sample data\n", "func_signal": "def main():\n", "code": "lnum = 2\ncolors = ['r', 'b']\ndtrain = np.random.randint(0, 100, (50, 2)).astype(np.float32)\nltrain = np.random.randint(0, lnum, (50, 1)).astype(np.float32)\ndtest = np.random.randint(0, 100, (5, 2)).astype(np.float32)\n\n# plot train data\nred = dtrain[ltrain.ravel() == 0]\nblue = dtrain[ltrain.ravel() == 1]\nplt.scatter(red[:, 0], red[:, 1], c='r')\nplt.scatter(blue[:, 0], blue[:, 1], c='b', marker='x')\n\n# predcit test data into class\nltest = knn_predict(dtrain, dtest, ltrain, 3)\nfor i in range(lnum):\n    plt.scatter(dtest[ltest == i, 0],\n                dtest[ltest == i, 1],\n                color=colors[i], marker='D')\n\n# show results\nplt.show()", "path": "KNN\\knn.py", "repo_name": "leezqcst/machine-learning-algorithm", "stars": 133, "license": "None", "language": "python", "size": 44438}
{"docstring": "\"\"\"C4.5\n    ID3\u7b97\u6cd5\u8ba1\u7b97\u7684\u662f\u4fe1\u606f\u589e\u76ca\uff0cC4.5\u7b97\u6cd5\u8ba1\u7b97\u7684\u662f\u4fe1\u606f\u589e\u76ca\u6bd4\uff0c\u5bf9\u4e0a\u9762ID3\u7248\u672c\u7684\u51fd\u6570\u7a0d\u4f5c\u4fee\u6539\u5373\u53ef\n\"\"\"\n", "func_signal": "def _chooseBestFeatureToSplit_C45(self,X,y):\n", "code": "numFeatures = X.shape[1]\noldEntropy = self._calcEntropy(y)\nbestGainRatio = 0.0\nbestFeatureIndex = -1\n#\u5bf9\u6bcf\u4e2a\u7279\u5f81\u90fd\u8ba1\u7b97\u4e00\u4e0bgainRatio=infoGain/splitInformation\nfor i in range(numFeatures):\n    featList = X[:,i]\n    uniqueVals = set(featList)\n    newEntropy = 0.0\n    splitInformation = 0.0\n    #\u5bf9\u7b2ci\u4e2a\u7279\u5f81\u7684\u5404\u4e2avalue\uff0c\u5f97\u5230\u5404\u4e2a\u5b50\u6570\u636e\u96c6\uff0c\u8ba1\u7b97\u5404\u4e2a\u5b50\u6570\u636e\u96c6\u7684\u71b5\uff0c\n    #\u8fdb\u4e00\u6b65\u5730\u53ef\u4ee5\u8ba1\u7b97\u5f97\u5230\u6839\u636e\u7b2ci\u4e2a\u7279\u5f81\u5206\u5272\u539f\u59cb\u6570\u636e\u96c6\u540e\u7684\u71b5newEntropy\n    for value in uniqueVals:\n        sub_X,sub_y = self._splitDataSet(X,y,i,value)\n        prob = len(sub_y)/float(len(y))\n        newEntropy += prob * self._calcEntropy(sub_y)\n        splitInformation -= prob * np.log2(prob)\n    #\u8ba1\u7b97\u4fe1\u606f\u589e\u76ca\u6bd4\uff0c\u6839\u636e\u4fe1\u606f\u589e\u76ca\u6bd4\u9009\u62e9\u6700\u4f73\u5206\u5272\u7279\u5f81\n    #splitInformation\u82e5\u4e3a0\uff0c\u8bf4\u660e\u8be5\u7279\u5f81\u7684\u6240\u6709\u503c\u90fd\u662f\u76f8\u540c\u7684\uff0c\u663e\u7136\u4e0d\u80fd\u4f5c\u4e3a\u5206\u5272\u7279\u5f81\n    if splitInformation==0.0:\n        pass\n    else:\n        infoGain = oldEntropy - newEntropy\n        gainRatio = infoGain/splitInformation\n        if(gainRatio > bestGainRatio):\n            bestGainRatio = gainRatio\n            bestFeatureIndex = i\nreturn bestFeatureIndex", "path": "decision-tree\\id3_c45.py", "repo_name": "leezqcst/machine-learning-algorithm", "stars": 133, "license": "None", "language": "python", "size": 44438}
{"docstring": "'''\n\u8ba1\u7b97\u71b5\n'''\n", "func_signal": "def calc_entropy(dataset):\n", "code": "num = dataset.shape[0]\n# \u7edf\u8ba1y\u4e2d\u4e0d\u540clabel\u503c\u7684\u4e2a\u6570\nlabel_count = {}\nfor label in dataset:\n    label = dataset\n    if label not in label_count:\n        label_count[label] = 0\n    label_count[label] += 1\nentroy = 0.0\nfor key in label_count:\n    prob = float(label_count[key])/num\n    entroy -= prob * np.log2(prob)\nreturn entroy", "path": "decision-tree\\id3.py", "repo_name": "leezqcst/machine-learning-algorithm", "stars": 133, "license": "None", "language": "python", "size": 44438}
{"docstring": "# initialize linear regression parameters\n", "func_signal": "def demo():\n", "code": "iterations = 2000\nlearning_rate = 0.1\nl2 = 0.0001\n\nlinearReg = LinReg(learning_rate = learning_rate, iterations = iterations, verbose = 1, l2 = l2)\n\ndata = np.genfromtxt('Data/blood_pressure.csv', delimiter = ',', skip_header = 1)\nX = data[:, 1:]\ny = data[:, 0]\n\n# scale data\nmax = np.amax(X)\nX /= max\nprint(X)\nprint(y)\n\n# fit the linear reg\nlinearReg.fit(X = X, y = y)\n\n# load testing dataset\ntest = np.genfromtxt('Data/blood_pressure.csv', delimiter = ',', skip_header = 1)\nX_test = test[:, 1:]\ny_test = test[:, 0]\n\nmax = np.amax(X_test)\nX_test /= max\nprint(X_test)\n\npredictions = np.array(linearReg.predict(X_test))\n\nprint('correct: ', y_test)\nprint('prediction: ', predictions)", "path": "linear-regression\\LinearRegression.py", "repo_name": "leezqcst/machine-learning-algorithm", "stars": 133, "license": "None", "language": "python", "size": 44438}
{"docstring": "\"\"\"X : array-like, shape (n_samples, n_features)\n\ty : array-like, shape (n_samples,)\"\"\"\n# scheme of separated\n# {0: [[2, 21]], 1: [[1, 20], [3, 22]]}\n", "func_signal": "def fit(self, X, y):\n", "code": "separated = {c: [x for x, t in zip(X, y) if t==c] for c in np.unique(y)}\nself.model = {c: [(np.mean(attr), np.std(attr)) for attr in zip(*instances)]\n\t\t\t\tfor c, instances in separated.items()}\nreturn self", "path": "Naive-bayes\\naive_bayes.py", "repo_name": "leezqcst/machine-learning-algorithm", "stars": 133, "license": "None", "language": "python", "size": 44438}
{"docstring": "\"\"\"training DQN, which has two actions: 0-exit, 1-stay\n\nArgs:\n    data (DataFrame): stock price for self.n_feature companies\n\"\"\"\n", "func_signal": "def train(self, input_data, noise_scale=0.1):\n", "code": "stock_data = input_data.values\ndate = input_data.index\nT = len(stock_data)\nself.noise_scale = noise_scale\n\n# frequency for output\nprint_freq = int(T / 100)\nif print_freq == 0:\n    print_freq = 1\nprint (\"training....\")\nst = time.time()\n#  udpate rate for prioritizing parameter\ndb = (1 - self.beta) / 1000\n\n# result for return value\nvalues = [[] for _ in range(self.n_stock)]\ndate_label = [[] for _ in range(self.n_stock)]\ndate_use = []\nstock_use = []\n# will not train until getting enough data\nt0 = self.n_history + self.n_batch\nself.initialize_memory(stock_data[:t0], scale=noise_scale)\nsave_data_freq = 10\nsave_weight_freq = 10\ncount = 0\ninput_data.to_csv(\"stock_price.csv\")\nfor t in range(t0, T):\n    stock_use.append(stock_data[t])\n    date_use.append(date[t])\n    action = self.predict_action(stock_data[t])\n    for i in range(self.n_stock):\n        if action[i] == 0:\n            date_label[i].append(date[t])\n            values[i].append(stock_data[t][i])\n    self.update_memory(stock_data[t])\n    count += 1\n    for epoch in range(self.n_epochs):    \n        # select transition from pool\n        self.update_weight()\n        # update prioritizing paramter untill it goes over 1\n    self.beta  += db\n    if self.beta >= 1.0:\n        self.beta = 1.0\n    idx = np.random.randint(0, self.n_memory)\n    \n    experiences, weights = self.memory[idx].sample(self.n_batch, self.n_history, self.alpha, self.beta)\n    max_idx = self.get_max_idx(experiences.state1)\n    target_value = self.sess.run(self.target_value,\n                             feed_dict={self.state_target: experiences.state1,\n                         self.reward: experiences.reward,\n                                       self.max_idx_target: max_idx})\n    \n    if t % print_freq == 0:\n        print (\"time:\",  date[t])\n        error = self.sess.run(self.error,\n                      feed_dict={self.state: experiences.state0,\n                                 self.target: target_value,\n                                 self.reward: experiences.reward,\n                                 K.learning_phase(): 0})\n        print(\"error:\", np.mean(error))\n        action = self.predict_action(stock_data[t])\n        print(\"portfolio:\", action)\n        print (\"elapsed time\", time.time() - st)\n        print(\"********************************************************************\")\n        \n    if count % save_data_freq == 0:\n        for i in range(self.n_stock):\n            result = pd.DataFrame(values[i], index=pd.DatetimeIndex(date_label[i]))\n            result.to_csv(\"exit_result_{}.csv\".format(i))\n        data_use = pd.DataFrame(stock_use, index=pd.DatetimeIndex(date_use))\n        data_use.to_csv(\"stock_price.csv\")\n        \n    if count % save_weight_freq == 0:\n        save_path = self.saver.save(self.sess, self.save_path)\n        print(\"Model saved in file: %s\" % self.save_path)\n\nsave_path = self.saver.save(self.sess, self.save_path)\nprint(\"Model saved in file: %s\" % self.save_path)\nprint (\"finished training\")\n\nreturn [pd.DataFrame(values[i], index=pd.DatetimeIndex(date_label[i])) for i in range(self.n_stock)]", "path": "model\\dqn.py", "repo_name": "jjakimoto/DQN", "stars": 163, "license": "None", "language": "python", "size": 14460}
{"docstring": "# pararel memory update\n", "func_signal": "def update_weight(self):\n", "code": "idx = np.random.randint(0, self.n_memory)\nexperiences, weights = self.memory[idx].sample(self.n_batch, self.n_history, self.alpha, self.beta)\nself.sess.run(self.critic_optim, \n              feed_dict={self.state: experiences.state0,\n                         self.state_target: experiences.state1,\n                         self.reward: experiences.reward,\n                         self.action: experiences.action,\n                         self.weights: weights,\n                         self.learning_rate: self.lr,\n                         K.learning_phase(): 1})  \nself.sess.run(self.actor_optim,\n              feed_dict={self.state: experiences.state0,\n                         self.learning_rate: self.lr,\n                         K.learning_phase(): 1})  \n        \nerror = self.sess.run(self.error,\n                      feed_dict={self.state: experiences.state0,\n                                 self.state_target: experiences.state1,\n                                 self.reward: experiences.reward,\n                                 self.action: experiences.action,\n                                 K.learning_phase(): 0})\nself.memory[idx].update_priority(error)\n            \n# softupdate for critic network\nold_weights = self.critic_target.get_weights()\nnew_weights = self.critic.get_weights()\nweights = [self.update_rate * new_w + (1 - self.update_rate) * old_w\n           for new_w, old_w in zip(new_weights, old_weights)]\nself.critic_target.set_weights(weights)", "path": "model\\ddpg.py", "repo_name": "jjakimoto/DQN", "stars": 163, "license": "None", "language": "python", "size": 14460}
{"docstring": "\"\"\"Preduct Optimal strategy\n\nArgs:\n    state(float): stock data with size: [self.n_stock, ]\nRetrun:\n    integer: 0-exit, 1-stay\n\"\"\"\n", "func_signal": "def predict_action(self, state):\n", "code": "pred_state = self.memory[0].sample_state_uniform(self.n_batch, self.n_history)\nnew_state = pred_state[-1]\nnew_state = np.concatenate((new_state[1:], [state]), axis=0)\npred_state = np.concatenate((pred_state[:-1], [new_state]), axis=0)\naction = self.max_action.eval(\n    session=self.sess,\n    feed_dict={self.state: pred_state, K.learning_phase(): 0})[-1]\nreturn action", "path": "model\\dqn.py", "repo_name": "jjakimoto/DQN", "stars": 163, "license": "None", "language": "python", "size": 14460}
{"docstring": "\"\"\"Get ticker symbols constituting S&P\n\nArgs:\n    name(str): should be 'sap500' or 'sap100'\n\"\"\"\n", "func_signal": "def get_sap_symbols(name='sap500'):\n", "code": "if name == 'sap500':\n    site = 'http://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\nelif name == 'sap100':\n    site = 'https://en.wikipedia.org/wiki/S%26P_100'\nelse:\n    raise NameError('invalid input: name should be \"sap500\" or \"sap100\"')\n# fetch data from yahoo finance\npage = urlopen(site)\nsoup = BeautifulSoup(page, 'html.parser')\ntable = soup.find('table', {'class': 'wikitable sortable'})\nsymbols = []\nfor row in table.findAll('tr'):\n    col = row.findAll('td')\n    if len(col) > 0:\n        symbol = col[0].string.replace('.', '-')\n        symbols.append(str(symbol))\nreturn symbols", "path": "utils.py", "repo_name": "jjakimoto/DQN", "stars": 163, "license": "None", "language": "python", "size": 14460}
{"docstring": "\"\"\"Preduct Optimal Portfolio\n\nArgs:\n    state(float): stock data with size: [self.n_stock, ]\nRetrun:\n    np.array with size: [self.n_stock, ]\n\"\"\"\n", "func_signal": "def predict_action(self, state):\n", "code": "pred_state = self.memory[0].sample_state_uniform(self.n_batch, self.n_history)\nnew_state = pred_state[-1]\nnew_state = np.concatenate((new_state[1:], [state]), axis=0)\npred_state = np.concatenate((pred_state[:-1], [new_state]), axis=0)\naction = self.actor_output.eval(\n    session=self.sess,\n    feed_dict={self.state: pred_state, K.learning_phase(): 0})[-1]\n# action = self.norm_action(action)\nreturn action", "path": "model\\ddpg.py", "repo_name": "jjakimoto/DQN", "stars": 163, "license": "None", "language": "python", "size": 14460}
{"docstring": "# update memory without updating weight\n", "func_signal": "def update_memory(self, state, state_forward):\n", "code": "for i in range(self.n_memory):\n    self.memory[i].observations.append(state)\n    self.memory[i].priority.append(1.0)\n# to stabilize batch normalization, use other samples for prediction\npred_state = self.memory[0].sample_state_uniform(self.n_batch, self.n_history)\n# off policy action and update portfolio\nactor_action = self.actor_output.eval(session=self.sess,\n                              feed_dict={self.state: pred_state,\n                                                  K.learning_phase(): 0})[-1]\naction_scale = np.mean(np.abs(actor_action))\n# action_off = np.round(actor_value_off + np.random.normal(0, noise_scale, self.n_stock))\nfor i in range(self.n_memory):\n    action_off = actor_action + np.random.normal(0, action_scale * self.noise_scale, self.n_stock)\n    action_off = self.norm_action(action_off)\n    # action_off = actor_value_off\n    reward_off = reward = np.sum((state_forward - state) * action_off)\n    self.memory[i].rewards.append(reward_off)\n    self.memory[i].actions.append(action_off)", "path": "model\\ddpg.py", "repo_name": "jjakimoto/DQN", "stars": 163, "license": "None", "language": "python", "size": 14460}
{"docstring": "\"\"\"Build network\"\"\"\n", "func_signal": "def build_network(self, conf, model=None, input_shape=None, is_conv=True):\n", "code": "_model = model\nmodel = Sequential()\nif _model is None:\n    model.add(Lambda(lambda x: x,  input_shape=input_shape))\nelse:\n    model.add(_model)\n    \nfor x in conf:\n    if x['is_drop']:\n        model.add(Dropout(x['drop_rate']))\n    if x['type'] is 'full':\n        if is_conv:\n            model.add(Flatten())\n            is_conv = False\n        model.add(Dense(x['n_feature']))\n    elif x['type'] is 'conv':\n        model.add(Convolution2D(nb_filter=x['n_feature'], \n                                nb_row=x['kw'], \n                                nb_col=1, \n                                border_mode='same'))  \n        is_conv=True\n    if x['is_batch']:\n        if x['type'] is 'full':\n            model.add(BatchNormalization(mode=1, axis=-1))\n        if x['type'] is 'conv':\n            model.add(BatchNormalization(mode=2, axis=-1))\n    model.add(x['activation'])\nreturn model", "path": "model\\ddpg.py", "repo_name": "jjakimoto/DQN", "stars": 163, "license": "None", "language": "python", "size": 14460}
{"docstring": "\"\"\"Transform data into the Multi Scaled one\n\nArgs:\n    input: tensor with shape: [None, self.n_history, self.n_stock]\nReturn:\n    list of the same shape tensors, [None, self.length_history, self.n_stock]\n\"\"\"\n# the last data is the newest information\n", "func_signal": "def transform_input(self, input):\n", "code": "raw = input[:, self.n_history - self.history_length:, :, :]\n# smooth data\nsmoothed = []\nfor n_sm in range(2, self.n_smooth + 2):\n    smoothed.append(\n        tf.reduce_mean(tf.pack([input[:, self.n_history - st - self.history_length:self.n_history - st, :, :]\n                                for st in range(n_sm)]),0))\n# downsample data\ndown = []\nfor n_dw in range(2, self.n_down + 2):\n    sampled_ = tf.pack([input[:, idx, :, :] \n                        for idx in range(self.n_history-n_dw*self.history_length, self.n_history, n_dw)])\n    down.append(tf.transpose(sampled_, [1, 0, 2, 3]))\nreturn raw, smoothed, down", "path": "model\\ddpg.py", "repo_name": "jjakimoto/DQN", "stars": 163, "license": "None", "language": "python", "size": 14460}
{"docstring": "\"\"\"Build all of the network and optimizations\n\njust for conveninece of trainig, seprate placehoder for train and target network\ncritic network input: [raw_data, smoothed, downsampled, action]\nactor network input: [raw_data, smoothed, downsampled]\n\"\"\"\n", "func_signal": "def build_model(self):\n", "code": "self.critic = self.build_critic()\nself.critic_target = self.build_critic()\n# actor network input should be [raw_data, smoothed, downsampled]\nself.actor = self.build_actor()\n# transform input into the several scales and smoothing\nself.state =  tf.placeholder(tf.float32, [None, self.n_history, self.n_stock], name='state')\nself.state_target = tf.placeholder(tf.float32, [None, self.n_history, self.n_stock], name='state_target')\n# reshape to convolutional input\nstate_ = tf.reshape(self.state, [-1, self.n_history, self.n_stock, 1])\nstate_target_ = tf.reshape(self.state_target, [-1, self.n_history, self.n_stock, 1])\nraw, smoothed, down = self.transform_input(state_)\nraw_target, smoothed_target, down_target = self.transform_input(state_target_)\n\n# build graph for citic training\nself.action = tf.placeholder(tf.float32, [None, self.n_stock])\ninput_q = [raw,] +  smoothed + down + [self.action,]\nself.Q = tf.squeeze(self.critic(input_q))\n# target network\n# for double q-learning we use actor network not for target network\nself.actor_target_output = self.actor([raw_target,] +  smoothed_target + down_target)\ninput_q_target = [raw_target,] +  smoothed_target + down_target + [self.actor_target_output,]\nQ_target = tf.squeeze(self.critic_target(input_q_target))\nself.reward = tf.placeholder(tf.float32, [None], name='reward')\ntarget = self.reward  + self.gamma * Q_target\nself.target_value = self.reward  + self.gamma * Q_target\n# optimization\nself.learning_rate = tf.placeholder(tf.float32, shape=[], name=\"learning_rate\")\n# get rid of bias of prioritized\nself.weights = tf.placeholder(tf.float32, shape=[None], name=\"weights\")\nself.loss = tf.reduce_mean(self.weights * tf.square(target - self.Q), name='loss')\n# TD-error for priority\nself.error = tf.abs(target - self.Q)\nself.critic_optim = tf.train.AdamOptimizer(self.learning_rate) \\\n    .minimize(self.loss, var_list=self.critic.trainable_weights)\n\n# build graph for actor training\nself.actor_output = self.actor([raw,] +  smoothed + down)\ninput_q_actor = [raw,] +  smoothed + down + [self.actor_output,]\nself.Q_actor = tf.squeeze(self.critic(input_q_actor))\n# optimization\nself.actor_optim = tf.train.AdamOptimizer(self.learning_rate) \\\n    .minimize(-self.Q_actor, var_list=self.actor.trainable_weights)\n\nself.saver = tf.train.Saver()\nis_initialize = True\nif self.is_load:\n    if self.load(self.save_path):\n        print('succeded to load')\n        is_initialize = False\n    else:\n        print('failed to load')\n\n# initialize network\nif is_initialize:\n    tf.global_variables_initializer().run(session=self.sess)\n    weights = self.critic.get_weights()\n    self.critic_target.set_weights(weights)", "path": "model\\ddpg.py", "repo_name": "jjakimoto/DQN", "stars": 163, "license": "None", "language": "python", "size": 14460}
{"docstring": "\"\"\"Build critic network\n\nrecieve transformed tensor: raw_data, smooted_data, and downsampled_data\n\"\"\"\n", "func_signal": "def build_critic(self):\n", "code": "nf = self.n_feature\n# layer1\n# smoothed input\nsm_model = [Sequential() for _ in range(self.n_smooth)]\nfor m in sm_model:\n    m.add(Lambda(lambda x: x,  input_shape=(self.history_length, self.n_stock, 1)))\n    m.add(Convolution2D(nb_filter=nf, nb_row=self.k_w, nb_col=1, border_mode='same'))\n    m.add(BatchNormalization(mode=2, axis=-1))\n    m.add(PReLU())\n# down sampled input\ndw_model = [Sequential() for _ in range(self.n_down)]\nfor m in dw_model:\n    m.add(Lambda(lambda x: x,  input_shape=(self.history_length, self.n_stock, 1)))\n    m.add(Convolution2D(nb_filter=nf, nb_row=self.k_w, nb_col=1, border_mode='same'))\n    m.add(BatchNormalization(mode=2, axis=-1))\n    m.add(PReLU())\n# raw input\nstate = Sequential()\nnf = self.n_feature\nstate.add(Lambda(lambda x: x,  input_shape=(self.history_length, self.n_stock, 1)))\nstate.add(Convolution2D(nb_filter=nf, nb_row=self.k_w, nb_col=1, border_mode='same'))\nstate.add(BatchNormalization(mode=2, axis=-1))\nstate.add(PReLU())\nmerged = Merge([state,] + sm_model + dw_model, mode='concat', concat_axis=-1)\n# layer2\nnf = nf * 2\nmodel = Sequential()\nmodel.add(merged)\nmodel.add(Convolution2D(nb_filter=nf, nb_row=self.k_w, nb_col=1, border_mode='same'))\nmodel.add(BatchNormalization(mode=2, axis=-1))\nmodel.add(PReLU())\nmodel.add(Flatten())\n# layer3\nmodel.add(Dense(self.n_hidden))\nmodel.add(BatchNormalization(mode=1, axis=-1))\nmodel.add(PReLU())\n# layer4\nmodel.add(Dense(int(np.sqrt(self.n_hidden))))\nmodel.add(PReLU())\n# output\nmodel.add(Dense(2 * self.n_stock))\nmodel.add(Reshape((self.n_stock, 2)))\nreturn model", "path": "model\\dqn.py", "repo_name": "jjakimoto/DQN", "stars": 163, "license": "None", "language": "python", "size": 14460}
{"docstring": "\"\"\"initialized approximate value function\n\nconfig should have the following attributes\n\nArgs:\n    device: the device to use computation, e.g. '/gpu:0'\n    gamma(float): the decay rate for value at RL\n    history_length(int): input_length for each scale at CNN\n    n_feature(int): the number of type of input \n        (e.g. the number of company to use at stock trading)\n    trade_stock_idx(int): trading stock index\n    gam (float): discount factor\n    n_history(int): the nubmer of history that will be used as input\n    n_smooth, n_down(int): the number of smoothed and down sampling input at CNN\n    k_w(int): the size of filter at CNN\n    n_hidden(int): the size of fully connected layer\n    n_batch(int): the size of mini batch\n    n_epochs(int): the training epoch for each time\n    update_rate (0, 1): parameter for soft update\n    learning_rate(float): learning rate for SGD\n    memory_length(int): the length of Replay Memory\n    n_memory(int): the number of different Replay Memories\n    alpha, beta: [0, 1] parameters for Prioritized Replay Memories\n    action_scale(float): the scale of initialized ation\n\"\"\"\n", "func_signal": "def __init__(self, config):\n", "code": "self.device = config.device\nself.save_path = config.save_path\nself.is_load = config.is_load\nself.gamma = config.gamma\nself.history_length = config.history_length\nself.n_stock = config.n_stock\nself.n_smooth = config.n_smooth\nself.n_down = config.n_down\nself.n_batch = config.n_batch\nself.n_epoch = config.n_epoch\nself.update_rate = config.update_rate\nself.alpha = config.alpha\nself.beta = config.beta\nself.lr = config.learning_rate\nself.memory_length = config.memory_length\nself.n_memory = config.n_memory\nself.noise_scale = config.noise_scale\nself.model_config = config.model_config\n# the length of the data as input\nself.n_history = max(self.n_smooth + self.history_length, (self.n_down + 1) * self.history_length)\nprint (\"building model....\")\n# have compatibility with new tensorflow\ntf.python.control_flow_ops = tf\n# avoid creating _LEARNING_PHASE outside the network\nK.clear_session()\nself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False))\nK.set_session(self.sess)\nwith self.sess.as_default():\n    with tf.device(self.device):\n        self.build_model()\nprint('finished building model!')", "path": "model\\ddpg.py", "repo_name": "jjakimoto/DQN", "stars": 163, "license": "None", "language": "python", "size": 14460}
{"docstring": "\"\"\"Get historical data of key attribute\nfrom yahoo-finance as pd.DataFrame\n\nArgs:\n    symbols: list of ticker symbols, e.g. ['AFL', 'AAPL', ....]\n    st, end: start and end date for data, e.g. '2015-04-06'\n    key: attribute of data, e.g. Open, Close, Volume, Adj Close...\nReturn:\n    DataFrame\n\"\"\"\n", "func_signal": "def get_data_list_key(symbols, st, end, key='Open'):\n", "code": "values= []\nsucess_symbols = []\nfail_symbols = []\n# length of data \nmax_len = 0\nfor s in symbols:\n    try:\n        x = get_data(s, st, end)\n        n_data = len(x.index)\n        if n_data >= max_len:\n            if n_data != max_len:\n                max_len = n_data\n                date = x.index\n                fail_symbols += sucess_symbols\n                values= []\n                sucess_symbols = []\n            values.append(x[key])\n            sucess_symbols.append(s)\n        else:\n            fail_symbols.append(s)            \n    except:\n        fail_symbols.append(s)\n        pass\nif len(fail_symbols) > 0:\n    print('we cound not fetch data from the following companies')\n    print(fail_symbols)\nreturn pd.DataFrame(np.array(values).T, index = date, columns=sucess_symbols)", "path": "utils.py", "repo_name": "jjakimoto/DQN", "stars": 163, "license": "None", "language": "python", "size": 14460}
{"docstring": "# draw random indexes such that is bigger than window_length to enough length data\n", "func_signal": "def sample_state_uniform(self, batch_size, window_length):\n", "code": "batch_idx = np.random.random_integers(window_length, self.nb_entries - 1, size=batch_size - 1)\n# take the newest data\nbatch_idx = np.concatenate((batch_idx, [self.nb_entries]))\nassert len(batch_idx) == batch_size\n\n# create experiences\nstate = np.array([[self.observations[i] for i in range(idx - window_length, idx)] for idx in batch_idx])\nreturn state", "path": "memory.py", "repo_name": "jjakimoto/DQN", "stars": 163, "license": "None", "language": "python", "size": 14460}
{"docstring": "\"\"\"initialized approximate value function\n\nconfig should have the following attributes\n\nArgs:\n    device: the device to use computation, e.g. '/gpu:0'\n    gamma(float): the decay rate for value at RL\n    history_length(int): input_length for each scale at CNN\n    n_feature(int): the number of type of input \n        (e.g. the number of company to use at stock trading)\n    n_history(int): the nubmer of history that will be used as input\n    n_smooth, n_down(int): the number of smoothed and down sampling input at CNN\n    k_w(int): the size of filter at CNN\n    n_hidden(int): the size of fully connected layer\n    n_batch(int): the size of mini batch\n    n_epochs(int): the training epoch for each time\n    update_rate (0, 1): parameter for soft update\n    learning_rate(float): learning rate for SGD\n    memory_length(int): the length of Replay Memory\n    n_memory(int): the number of different Replay Memories\n    alpha, beta: [0, 1] parameters for Prioritized Replay Memories\n\"\"\"\n", "func_signal": "def __init__(self, config):\n", "code": "self.device = config.device\nself.save_path = config.save_path\nself.is_load = config.is_load\nself.gamma = config.gamma\nself.history_length = config.history_length\nself.n_stock = config.n_stock\nself.n_feature = config.n_feature\nself.n_smooth = config.n_smooth\nself.n_down = config.n_down\nself.k_w = config.k_w\nself.n_hidden = config.n_hidden\nself.n_batch = config.n_batch\nself.n_epochs = config.n_epochs\nself.update_rate = config.update_rate\nself.alpha = config.alpha\nself.beta = config.beta\nself.lr = config.learning_rate\nself.memory_length = config.memory_length\nself.n_memory = config.n_memory\n# the length of the data as input\nself.n_history = max(self.n_smooth + self.history_length, (self.n_down + 1) * self.history_length)\nprint (\"building model....\")\n# have compatibility with new tensorflow\ntf.python.control_flow_ops = tf\n# avoid creating _LEARNING_PHASE outside the network\nK.clear_session()\nself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False))\nK.set_session(self.sess)\nwith self.sess.as_default():\n    with tf.device(self.device):\n        self.build_model()\nprint('finished building model!')", "path": "model\\dqn.py", "repo_name": "jjakimoto/DQN", "stars": 163, "license": "None", "language": "python", "size": 14460}
{"docstring": "\"\"\"split time data into the list of ingegers\nArgs:\n    date: 'y-m-d', e.g.'2015-04-06'\nReturn:\n    List(int)\n\"\"\"\n", "func_signal": "def date_parse(date):\n", "code": "parsed = date.split('-')\nconverted = [int(t) for t in parsed]\nreturn converted", "path": "utils.py", "repo_name": "jjakimoto/DQN", "stars": 163, "license": "None", "language": "python", "size": 14460}
{"docstring": "\"\"\"Build actor network\n\nrecieve convereted tensor: raw_data, smooted_data, and downsampled_data\n\"\"\"\n# lower layer\n", "func_signal": "def build_actor(self):\n", "code": "lower_model = [self.build_network(self.model_config['actor_lower'], input_shape=(self.history_length, self.n_stock, 1)) \n               for _ in range(1  + self.n_smooth + self.n_down)]\nmerged = Merge(lower_model, mode='concat')\n# upper layer\nmodel = self.build_network(self.model_config['actor_upper'],  model=merged)\nreturn model", "path": "model\\ddpg.py", "repo_name": "jjakimoto/DQN", "stars": 163, "license": "None", "language": "python", "size": 14460}
{"docstring": "\"\"\"Build critic network\n\nrecieve convereted tensor: raw_data, smooted_data, and downsampled_data\n\"\"\"\n# lower layer\n", "func_signal": "def build_critic(self):\n", "code": "lower_model = [self.build_network(self.model_config['critic_lower'], input_shape=(self.history_length, self.n_stock, 1)) \n               for _ in range(1  + self.n_smooth + self.n_down)]\nmerged = Merge(lower_model, mode='concat')\n# upper layer\nupper_model = self.build_network(self.model_config['critic_upper'],  model=merged)\n# action layer\naction = self.build_network(self.model_config['critic_action'], input_shape=(self.n_stock,), is_conv=False)\n# output layer\nmerged = Merge([upper_model, action], mode='mul')\nmodel = Sequential()\nmodel.add(merged)\nmodel.add(Dense(1))\nreturn model", "path": "model\\ddpg.py", "repo_name": "jjakimoto/DQN", "stars": 163, "license": "None", "language": "python", "size": 14460}
{"docstring": "# to stabilize batch normalization, use other samples for prediction\n", "func_signal": "def take_action(self, state, state_forward):\n", "code": "pred_state = self.memory[0].sample_state_uniform(self.n_batch, self.n_history)\n# off policy action and update portfolio\naction = self.actor_output.eval(session=self.sess,\n                              feed_dict={self.state: pred_state,\n                                                  K.learning_phase(): 0})[-1]\nreward = np.sum((state_forward - state) * action)\nreturn reward", "path": "model\\ddpg.py", "repo_name": "jjakimoto/DQN", "stars": 163, "license": "None", "language": "python", "size": 14460}
{"docstring": "# udpate priority when sampling\n", "func_signal": "def sample(self, batch_size, window_length, alpha=1.0, beta=1.0, epsilon=0.05):\n", "code": "if len(self.priority) > self.limit:\n    self.priority = self.priority[-self.limit:]\n# draw random indexes such that is bigger than window_length to enough length data\nindex_space = np.arange(window_length, self.nb_entries)\n# prioritized sample\np = np.array(self.priority)[window_length:]\np_tilde = p + np.ones(self.nb_entries - window_length) * np.mean(p) * epsilon\np_tilde[-1] = np.mean(p)\np_tilde = p_tilde ** alpha\np_tilde = p_tilde / np.sum(p_tilde)\nbatch_idx = choice(index_space, p=p_tilde, size=batch_size - 1)\n# take the newest data\nbatch_idx = np.concatenate((batch_idx, [self.nb_entries - 1]))\nassert len(batch_idx) == batch_size\n# keep batch_idx to update pritority\nself.batch_idx = batch_idx\n\n# weights to modify biased update\nweights = 1. / (p_tilde**beta)\nweights = weights / np.max(weights)\nret_w = weights[batch_idx - window_length]\n\n# create experiences\nstate0 = np.array([[self.observations[i] for i in range(idx - window_length,idx)] for idx in batch_idx])\naction = np.array([self.actions[idx - 1] for idx in batch_idx])\nreward = np.array([self.rewards[idx - 1] for idx in batch_idx])\nstate1 = np.array([[self.observations[i] for i in range(idx - window_length + 1,idx + 1)] for idx in batch_idx])\nreturn Experiecne(state0, action, reward, state1), ret_w", "path": "memory.py", "repo_name": "jjakimoto/DQN", "stars": 163, "license": "None", "language": "python", "size": 14460}
{"docstring": "\"\"\"Update memory without updating weight\"\"\"\n", "func_signal": "def update_memory(self, state):\n", "code": "for i in range(self.n_memory):\n    self.memory[i].observations.append(state)\n    self.memory[i].priority.append(1.0)\n# to stabilize batch normalization, use other samples for prediction\npred_state = self.memory[0].sample_state_uniform(self.n_batch, self.n_history)\nfor i in range(self.n_memory):\n    action_off = None\n    reward_off = np.concatenate((np.reshape(state, (self.n_stock, 1)), np.zeros((self.n_stock, 1))), axis=-1)\n    self.memory[i].rewards.append(reward_off)\n    self.memory[i].actions.append(action_off)", "path": "model\\dqn.py", "repo_name": "jjakimoto/DQN", "stars": 163, "license": "None", "language": "python", "size": 14460}
{"docstring": "# \"Test BZ2File.readline()\"\n", "func_signal": "def testReadLine(self):\n", "code": "self.createTempFile()\nbz2f = BZ2File(self.filename)\nself.assertRaises(TypeError, bz2f.readline, None)\nsio = StringIO(self.TEXT)\nfor line in sio.readlines():\n    self.assertEqual(bz2f.readline(), line)\nbz2f.close()", "path": "Contents\\lib\\python\\test\\test_bz2.py", "repo_name": "OpenEndedGroup/Field", "stars": 201, "license": "None", "language": "python", "size": 116805}
{"docstring": "# \"Test BZ2Compressor.compress()/flush() with chunks of 10 bytes\"\n", "func_signal": "def testCompressChunks10(self):\n", "code": "bz2c = BZ2Compressor()\nn = 0\ndata = ''\nwhile 1:\n    str = self.TEXT[n*10:(n+1)*10]\n    if not str:\n        break\n    data += bz2c.compress(str)\n    n += 1\ndata += bz2c.flush()\nself.assertEqual(self.decompress(data), self.TEXT)", "path": "Contents\\lib\\python\\test\\test_bz2.py", "repo_name": "OpenEndedGroup/Field", "stars": 201, "license": "None", "language": "python", "size": 116805}
{"docstring": "# \"Test BZ2File.writelines()\"\n", "func_signal": "def testWriteLines(self):\n", "code": "bz2f = BZ2File(self.filename, \"w\")\nself.assertRaises(TypeError, bz2f.writelines)\nsio = StringIO(self.TEXT)\nbz2f.writelines(sio.readlines())\nbz2f.close()\n# patch #1535500\nself.assertRaises(ValueError, bz2f.writelines, [\"a\"])\nf = open(self.filename, 'rb')\nself.assertEqual(self.decompress(f.read()), self.TEXT)\nf.close()", "path": "Contents\\lib\\python\\test\\test_bz2.py", "repo_name": "OpenEndedGroup/Field", "stars": 201, "license": "None", "language": "python", "size": 116805}
{"docstring": "# \"Test BZ2File.seek(-150, 0)\"\n", "func_signal": "def testSeekPreStart(self):\n", "code": "self.createTempFile()\nbz2f = BZ2File(self.filename)\nbz2f.seek(-150)\nself.assertEqual(bz2f.tell(), 0)\nself.assertEqual(bz2f.read(), self.TEXT)\nbz2f.close()", "path": "Contents\\lib\\python\\test\\test_bz2.py", "repo_name": "OpenEndedGroup/Field", "stars": 201, "license": "None", "language": "python", "size": 116805}
{"docstring": "# \"Test BZ2Decompressor.decompress()\"\n", "func_signal": "def testDecompress(self):\n", "code": "bz2d = BZ2Decompressor()\nself.assertRaises(TypeError, bz2d.decompress)\ntext = bz2d.decompress(self.DATA)\nself.assertEqual(text, self.TEXT)", "path": "Contents\\lib\\python\\test\\test_bz2.py", "repo_name": "OpenEndedGroup/Field", "stars": 201, "license": "None", "language": "python", "size": 116805}
{"docstring": "# \"Test BZ2File.read()\"\n", "func_signal": "def testRead(self):\n", "code": "self.createTempFile()\nbz2f = BZ2File(self.filename)\nself.assertRaises(TypeError, bz2f.read, None)\nself.assertEqual(bz2f.read(), self.TEXT)\nbz2f.close()", "path": "Contents\\lib\\python\\test\\test_bz2.py", "repo_name": "OpenEndedGroup/Field", "stars": 201, "license": "None", "language": "python", "size": 116805}
{"docstring": "# \"Test BZ2File.seek(150, 0)\"\n", "func_signal": "def testSeekForward(self):\n", "code": "self.createTempFile()\nbz2f = BZ2File(self.filename)\nself.assertRaises(TypeError, bz2f.seek)\nbz2f.seek(150)\nself.assertEqual(bz2f.read(), self.TEXT[150:])\nbz2f.close()", "path": "Contents\\lib\\python\\test\\test_bz2.py", "repo_name": "OpenEndedGroup/Field", "stars": 201, "license": "None", "language": "python", "size": 116805}
{"docstring": "# \"Test BZ2File.read() with universal newlines (\\\\r\\\\n)\"\n", "func_signal": "def testUniversalNewlinesCRLF(self):\n", "code": "self.createTempFile(crlf=1)\nbz2f = BZ2File(self.filename, \"rU\")\nself.assertEqual(bz2f.read(), self.TEXT)\nself.assertEqual(bz2f.newlines, \"\\r\\n\")\nbz2f.close()", "path": "Contents\\lib\\python\\test\\test_bz2.py", "repo_name": "OpenEndedGroup/Field", "stars": 201, "license": "None", "language": "python", "size": 116805}
{"docstring": "# \"Test BZ2File.write() with chunks of 10 bytes\"\n", "func_signal": "def testWriteChunks10(self):\n", "code": "bz2f = BZ2File(self.filename, \"w\")\nn = 0\nwhile 1:\n    str = self.TEXT[n*10:(n+1)*10]\n    if not str:\n        break\n    bz2f.write(str)\n    n += 1\nbz2f.close()\nf = open(self.filename, 'rb')\nself.assertEqual(self.decompress(f.read()), self.TEXT)\nf.close()", "path": "Contents\\lib\\python\\test\\test_bz2.py", "repo_name": "OpenEndedGroup/Field", "stars": 201, "license": "None", "language": "python", "size": 116805}
{"docstring": "# \"Test BZ2Compressor.compress()/flush()\"\n", "func_signal": "def testCompress(self):\n", "code": "bz2c = BZ2Compressor()\nself.assertRaises(TypeError, bz2c.compress)\ndata = bz2c.compress(self.TEXT)\ndata += bz2c.flush()\nself.assertEqual(self.decompress(data), self.TEXT)", "path": "Contents\\lib\\python\\test\\test_bz2.py", "repo_name": "OpenEndedGroup/Field", "stars": 201, "license": "None", "language": "python", "size": 116805}
{"docstring": "\"\"\"Encode the message's payload in Base64.\n\nAlso, add an appropriate Content-Transfer-Encoding header.\n\"\"\"\n", "func_signal": "def encode_base64(msg):\n", "code": "orig = msg.get_payload()\nencdata = _bencode(orig)\nmsg.set_payload(encdata)\nmsg['Content-Transfer-Encoding'] = 'base64'", "path": "Contents\\lib\\python\\email\\encoders.py", "repo_name": "OpenEndedGroup/Field", "stars": 201, "license": "None", "language": "python", "size": 116805}
{"docstring": "# Make sure that using all zeros uses the proper default values.\n# No test for daylight savings since strftime() does not change output\n# based on its value.\n", "func_signal": "def test_default_values_for_zero(self):\n", "code": "if not test_support.is_jython:\n    expected = \"2000 01 01 00 00 00 1 001\"\nelse:\n    # XXX: Jython doesn't support the \"two digits years\" hack (turned\n    #      on/off by time.accept2dyears), so year 0 means exactly that\n    #      and it is not converted to 2000.\n    expected = \"0000 01 01 00 00 00 1 001\"\nresult = time.strftime(\"%Y %m %d %H %M %S %w %j\", (0,)*9)\nself.assertEquals(expected, result)", "path": "Contents\\lib\\python\\test\\test_time.py", "repo_name": "OpenEndedGroup/Field", "stars": 201, "license": "None", "language": "python", "size": 116805}
{"docstring": "# Not sure how to check the values, since the clock could tick\n# at any time.  Make sure these are at least accepted and\n# don't raise errors.\n", "func_signal": "def test_ctime_without_arg(self):\n", "code": "time.ctime()\ntime.ctime(None)", "path": "Contents\\lib\\python\\test\\test_time.py", "repo_name": "OpenEndedGroup/Field", "stars": 201, "license": "None", "language": "python", "size": 116805}
{"docstring": "# \"Test BZ2File.seek(-150, 1)\"\n", "func_signal": "def testSeekBackwards(self):\n", "code": "self.createTempFile()\nbz2f = BZ2File(self.filename)\nbz2f.read(500)\nbz2f.seek(-150, 1)\nself.assertEqual(bz2f.read(), self.TEXT[500-150:])\nbz2f.close()", "path": "Contents\\lib\\python\\test\\test_bz2.py", "repo_name": "OpenEndedGroup/Field", "stars": 201, "license": "None", "language": "python", "size": 116805}
{"docstring": "# We can't quite use base64.encodestring() since it tacks on a \"courtesy\n# newline\".  Blech!\n", "func_signal": "def _bencode(s):\n", "code": "if not s:\n    return s\nhasnewline = (s[-1] == '\\n')\nvalue = base64.encodestring(s)\nif not hasnewline and value[-1] == '\\n':\n    return value[:-1]\nreturn value", "path": "Contents\\lib\\python\\email\\encoders.py", "repo_name": "OpenEndedGroup/Field", "stars": 201, "license": "None", "language": "python", "size": 116805}
{"docstring": "# readlines() for files containing no newline\n", "func_signal": "def testBug1191043(self):\n", "code": "data = 'BZh91AY&SY\\xd9b\\x89]\\x00\\x00\\x00\\x03\\x80\\x04\\x00\\x02\\x00\\x0c\\x00 \\x00!\\x9ah3M\\x13<]\\xc9\\x14\\xe1BCe\\x8a%t'\nf = open(self.filename, \"wb\")\nf.write(data)\nf.close()\nbz2f = BZ2File(self.filename)\nlines = bz2f.readlines()\nbz2f.close()\nself.assertEqual(lines, ['Test'])\nbz2f = BZ2File(self.filename)\nxlines = list(bz2f.xreadlines())\nbz2f.close()\nself.assertEqual(xlines, ['Test'])", "path": "Contents\\lib\\python\\test\\test_bz2.py", "repo_name": "OpenEndedGroup/Field", "stars": 201, "license": "None", "language": "python", "size": 116805}
{"docstring": "# Make sure that strftime() checks the bounds of the various parts\n#of the time tuple (0 is valid for *all* values).\n\n# XXX: Jython supports more dates than CPython\n", "func_signal": "def test_strftime_bounds_checking(self):\n", "code": "if not test_support.is_jython:\n    # Check year [1900, max(int)]\n    self.assertRaises(ValueError, time.strftime, '',\n                      (1899, 1, 1, 0, 0, 0, 0, 1, -1))\nif time.accept2dyear:\n    self.assertRaises(ValueError, time.strftime, '',\n                        (-1, 1, 1, 0, 0, 0, 0, 1, -1))\n    self.assertRaises(ValueError, time.strftime, '',\n                        (100, 1, 1, 0, 0, 0, 0, 1, -1))\n# Check month [1, 12] + zero support\nself.assertRaises(ValueError, time.strftime, '',\n                    (1900, -1, 1, 0, 0, 0, 0, 1, -1))\nself.assertRaises(ValueError, time.strftime, '',\n                    (1900, 13, 1, 0, 0, 0, 0, 1, -1))\n# Check day of month [1, 31] + zero support\nself.assertRaises(ValueError, time.strftime, '',\n                    (1900, 1, -1, 0, 0, 0, 0, 1, -1))\nself.assertRaises(ValueError, time.strftime, '',\n                    (1900, 1, 32, 0, 0, 0, 0, 1, -1))\n# Check hour [0, 23]\nself.assertRaises(ValueError, time.strftime, '',\n                    (1900, 1, 1, -1, 0, 0, 0, 1, -1))\nself.assertRaises(ValueError, time.strftime, '',\n                    (1900, 1, 1, 24, 0, 0, 0, 1, -1))\n# Check minute [0, 59]\nself.assertRaises(ValueError, time.strftime, '',\n                    (1900, 1, 1, 0, -1, 0, 0, 1, -1))\nself.assertRaises(ValueError, time.strftime, '',\n                    (1900, 1, 1, 0, 60, 0, 0, 1, -1))\n# Check second [0, 61]\nself.assertRaises(ValueError, time.strftime, '',\n                    (1900, 1, 1, 0, 0, -1, 0, 1, -1))\n# C99 only requires allowing for one leap second, but Python's docs say\n# allow two leap seconds (0..61)\nself.assertRaises(ValueError, time.strftime, '',\n                    (1900, 1, 1, 0, 0, 62, 0, 1, -1))\n# No check for upper-bound day of week;\n#  value forced into range by a ``% 7`` calculation.\n# Start check at -2 since gettmarg() increments value before taking\n#  modulo.\nself.assertRaises(ValueError, time.strftime, '',\n                    (1900, 1, 1, 0, 0, 0, -2, 1, -1))\n# Check day of the year [1, 366] + zero support\nself.assertRaises(ValueError, time.strftime, '',\n                    (1900, 1, 1, 0, 0, 0, 0, -1, -1))\nself.assertRaises(ValueError, time.strftime, '',\n                    (1900, 1, 1, 0, 0, 0, 0, 367, -1))\n# Check daylight savings flag [-1, 1]\nself.assertRaises(ValueError, time.strftime, '',\n                    (1900, 1, 1, 0, 0, 0, 0, 1, -2))\nself.assertRaises(ValueError, time.strftime, '',\n                    (1900, 1, 1, 0, 0, 0, 0, 1, 2))", "path": "Contents\\lib\\python\\test\\test_time.py", "repo_name": "OpenEndedGroup/Field", "stars": 201, "license": "None", "language": "python", "size": 116805}
{"docstring": "# \"Test BZ2File.xreadlines()\"\n", "func_signal": "def testXReadLines(self):\n", "code": "self.createTempFile()\nbz2f = BZ2File(self.filename)\nsio = StringIO(self.TEXT)\nself.assertEqual(list(bz2f.xreadlines()), sio.readlines())\nbz2f.close()", "path": "Contents\\lib\\python\\test\\test_bz2.py", "repo_name": "OpenEndedGroup/Field", "stars": 201, "license": "None", "language": "python", "size": 116805}
{"docstring": "# \"Test compress() function\"\n", "func_signal": "def testCompress(self):\n", "code": "data = bz2.compress(self.TEXT)\nself.assertEqual(self.decompress(data), self.TEXT)", "path": "Contents\\lib\\python\\test\\test_bz2.py", "repo_name": "OpenEndedGroup/Field", "stars": 201, "license": "None", "language": "python", "size": 116805}
{"docstring": "# \"Test opening and deleting a file many times\"\n", "func_signal": "def testOpenDel(self):\n", "code": "self.createTempFile()\nfor i in xrange(10000):\n    o = BZ2File(self.filename)\n    del o", "path": "Contents\\lib\\python\\test\\test_bz2.py", "repo_name": "OpenEndedGroup/Field", "stars": 201, "license": "None", "language": "python", "size": 116805}
{"docstring": "# \u9996\u5148\u628a\u4e24\u4e2a\u8282\u70b9\u67e5\u5165\u5230\u56fe\u4e2d\n", "func_signal": "def add_edge(self, from_key, to_key):\n", "code": "self.add_vertex(from_key)\nself.add_vertex(to_key)\nself._vertexes[from_key].add_adjust(self._vertexes[to_key])\nself._vertexes[to_key].in_degree += 1\nself._vertexes[to_key].backup_in_degree += 1", "path": "src\\ssj\\graph\\digraph.py", "repo_name": "ssjssh/algorithm", "stars": 252, "license": "gpl-2.0", "language": "python", "size": 276}
{"docstring": "\"\"\"\n\u53d6\u51fa\u6811\u4e2d\u6700\u5c0f\u503c\n:return:\n\"\"\"\n", "func_signal": "def min(self):\n", "code": "cur_node = self.__root\nwhile not cur_node.is_leaf:\n    cur_node = cur_node.childs[0]\nreturn cur_node.keys[0]", "path": "src\\ssj\\tree\\BTree.py", "repo_name": "ssjssh/algorithm", "stars": 252, "license": "gpl-2.0", "language": "python", "size": 276}
{"docstring": "\"\"\"\n\u5220\u9664\u4e00\u4e2a\u5173\u952e\u5b57\n:param key:\n:return:\n\"\"\"\n", "func_signal": "def delete(self, key):\n", "code": "node, index = self.search(key)\nif node is None:\n    return False\n\nreturn self.__delete(node, index)", "path": "src\\ssj\\tree\\BTree.py", "repo_name": "ssjssh/algorithm", "stars": 252, "license": "gpl-2.0", "language": "python", "size": 276}
{"docstring": "\"\"\"\n\u63d2\u5165\u4e00\u4e2a\u8282\u70b9\n:param key:\n:return:\n\"\"\"\n", "func_signal": "def insert(self, key):\n", "code": "if self.__root is None:\n    self.__root = Node(True, [key])\n    return\n\ncur_node = self.__root\nwhile not cur_node.is_leaf:\n    self.__split(cur_node)\n    # \u8fd9\u4e2a\u5730\u65b9cur_node\u5e76\u6ca1\u6709\u88ab\u6539\u53d8\uff0c\u6240\u4ee5\u662f\u53ef\u4ee5\u7ee7\u7eed\u641c\u7d22\u7684\u3002\n    cur_node = cur_node.search_child(key)\n\nleft_node, right_node = self.__split(cur_node)\nif left_node is None or right_node is None:\n    # \u8fd4\u56deNone\u8868\u793a\u53f6\u8282\u70b9\u6ca1\u6709\u6ee1\n    cur_node.append(key)\nelse:\n    # \u627e\u5230\u5de6\u53f3\u8282\u70b9\u7684\u5171\u540c\u5173\u952e\u5b57\n    parent_key = None\n    parent = left_node.parent\n    for i in xrange(len(parent)):\n        if parent.childs[i] is left_node:\n            parent_key = parent.keys[i]\n\n    if parent_key <= key:\n        # \u8bf4\u660eleft_node\u4e2d\u7684\u6240\u6709\u8282\u70b9\u90fd\u6bd4key\u5c0f\uff0c\u6240\u4ee5\u628a\u65b0\u8282\u70b9\u63d2\u5165\u5230\u53f3\u8fb9\n        right_node.append(key)\n    else:\n        left_node.append(key)\n\nself.__size += 1", "path": "src\\ssj\\tree\\BTree.py", "repo_name": "ssjssh/algorithm", "stars": 252, "license": "gpl-2.0", "language": "python", "size": 276}
{"docstring": "\"\"\"\n\u5728\u8282\u70b9\u6ee1\u7684\u65f6\u5019\u5206\u88c2\u8282\u70b9\u3002\u8981\u6ce8\u610f\u4e24\u4e2a\u95ee\u9898\uff1a\n1\uff0c\u6839\u8282\u70b9\u5206\u88c2\u7684\u65f6\u5019\u9700\u8981\u91cd\u65b0\u8bbe\u7f6e\u6839\u8282\u70b9\n2\uff0c\u53f6\u8282\u70b9\u662f\u6ca1\u6709\u5b50\u8282\u70b9\u7684\uff0c\u4e00\u6b21\u8981\u65f6\u523b\u5224\u65ad\n:param node:\n:return:\n\"\"\"\n", "func_signal": "def __split(self, node):\n", "code": "if self.full(node):\n    parent_node = node.parent\n    middle_key = node.keys[self.__load_factor - 1]\n    if parent_node is None:\n        # \u5904\u7406\u6839\u8282\u70b9\n        parent_node = self.__root = Node(False, [])\n\n    parent_middle_index = parent_node.append(middle_key)\n    left_node = Node(node.is_leaf, node.keys[:self.__load_factor - 1],\n                     None if node.is_leaf else node.childs[:self.__load_factor],\n                     parent_node)\n\n    right_node = Node(node.is_leaf, node.keys[self.__load_factor:],\n                      None if node.is_leaf else node.childs[self.__load_factor:],\n                      parent_node)\n\n    # \u6ce8\u610f\u8bbe\u5b9a\u5206\u88c2\u8282\u70b9\u7684\u5b50\u8282\u70b9\u7684\u7236\u6307\u9488\uff0c\u56e0\u4e3a\u5982\u679cnode\u662f\u53f6\u8282\u70b9\uff0c\u90a3\u4e48\u4e24\u4e2a\u5b50\u8282\u70b9\u80af\u5b9a\u90fd\u662f\u53f6\u8282\u70b9\uff0c\u53cd\u4e4b\u540c\u7406\n    if not node.is_leaf:\n        for child in left_node.childs:\n            if child is not None:\n                child.parent = left_node\n        for child in right_node.childs:\n            if child is not None:\n                child.parent = right_node\n\n    parent_node.childs[parent_middle_index] = left_node\n    parent_node.childs[parent_middle_index + 1] = right_node\n    self.__root.is_leaf = False\n    return left_node, right_node\nreturn None, None", "path": "src\\ssj\\tree\\BTree.py", "repo_name": "ssjssh/algorithm", "stars": 252, "license": "gpl-2.0", "language": "python", "size": 276}
{"docstring": "\"\"\"\n\u67e5\u627e\u5173\u952e\u5b57\u7684\u540e\u7ee7\u8282\u70b9,\u4f4d\u7f6e\u4f7f\u7528\u4e00\u4e2a\u5143\u7956\u8868\u793a\uff0c\u524d\u9762\u7684\u4e00\u4e2a\u5143\u7d20\u662fnode,\u540e\u9762\u7684\u662findex\n\u5982\u679c\u8fd4\u56deNone\u8868\u793a\u6ca1\u6709\u540e\u7ee7\uff0c\u4e5f\u5c31\u662f\u8bf4\u662f\u6700\u5927\u5143\u7d20\n:param key:\n:return:(Node,index)|None\n\"\"\"\n", "func_signal": "def successor(self, key):\n", "code": "node, index = self.search(key)\nif node is None:\n    return None, None\nreturn self.__successor(node, index)", "path": "src\\ssj\\tree\\BTree.py", "repo_name": "ssjssh/algorithm", "stars": 252, "license": "gpl-2.0", "language": "python", "size": 276}
{"docstring": "\"\"\"\n\u53d6\u51fa\u6811\u4e2d\u6700\u5927\u503c\n:return:\n\"\"\"\n", "func_signal": "def max(self):\n", "code": "cur_node = self.__root\nwhile not cur_node.is_leaf:\n    cur_node = cur_node.childs[-1]\nreturn cur_node.keys[-1]", "path": "src\\ssj\\tree\\BTree.py", "repo_name": "ssjssh/algorithm", "stars": 252, "license": "gpl-2.0", "language": "python", "size": 276}
{"docstring": "\"\"\"\nB\u6811\u4e2d\u5e8f\u904d\u5386\n:param f:\n:return:\n\"\"\"\n", "func_signal": "def mid_order(self, f):\n", "code": "result = []\nstack = Stack()\ncur_node = self.__root\nif cur_node.is_leaf:\n    return map(f, cur_node.keys)\n\nwhile True:\n    if cur_node.is_leaf:\n        # \u5230\u53f6\u8282\u70b9\u4e86\uff0c\u5f00\u59cb\u628a\u53f6\u8282\u70b9\u7684\u6240\u6709\u5173\u952e\u5b57\u90fd\u904d\u5386\u6389\n        result.extend(map(f, cur_node.keys))\n        # \u5f00\u59cb\u4ece\u6808\u4e2d\u53d6\u5143\u7d20\uff0c\u904d\u5386\u4e0b\u4e00\u4e2a\u8282\u70b9\u53f6\u8282\u70b9\n        if stack.empty():\n            return result\n        cur_node, i = stack.pop()\n        result.append(f(cur_node.keys[i]))\n        if i < len(cur_node) - 1:\n            stack.push((cur_node, i + 1))\n        cur_node = cur_node.childs[i + 1]\n    else:\n        stack.push((cur_node, 0))\n        cur_node = cur_node.childs[0]\nreturn result", "path": "src\\ssj\\tree\\BTree.py", "repo_name": "ssjssh/algorithm", "stars": 252, "license": "gpl-2.0", "language": "python", "size": 276}
{"docstring": "\"\"\"\n:type s: str\n:rtype: int\n\"\"\"\n", "func_signal": "def lengthOfLongestSubstring(self, s):\n", "code": "if not s:\n    return 0\ntmp_dict = {}\n# \u5b57\u7b26\u4e32\u4e0d\u4e3a\u7a7a,\u56e0\u6b64\u6700\u5c0f\u503c\u662f1\nmax_length = 1\nstart_index = 0\nfor index, ch in enumerate(s):\n    if ch in tmp_dict:\n        if max_length < index - start_index:\n            max_length = index - start_index\n        dup_index = tmp_dict[ch]\n        for i in range(start_index, dup_index + 1):\n            del tmp_dict[s[i]]\n        start_index = dup_index + 1\n    tmp_dict[ch] = index\n# \u6ce8\u610f\u7ed3\u675f\u7684\u65f6\u5019\u5982\u679c\u6ca1\u6709\u91cd\u590d,\u90a3\u4e48\u4f1a\u5b89\u9759\u9000\u51fa\u5faa\u73af\u800c\u4e0d\u53d1\u751f\u4efb\u4f55\u6bd4\u8f83,\u6240\u4ee5\u5728\u9000\u51fa\u5faa\u73af\u7684\u65f6\u5019\u8981\u6bd4\u8f83\nreturn len(tmp_dict) if len(tmp_dict) > max_length else max_length", "path": "src\\ssj\\leecode\\longest_substring.py", "repo_name": "ssjssh/algorithm", "stars": 252, "license": "gpl-2.0", "language": "python", "size": 276}
{"docstring": "\"\"\"\n\u67e5\u627e\u5c0f\u4e8einstance\u7684\u5b50\u6811\n\"\"\"\n", "func_signal": "def search_child(self, instance):\n", "code": "for x in xrange(0, self.__size):\n    if self.keys[x] > instance:\n        return self.childs[x]\nreturn self.childs[self.__size]", "path": "src\\ssj\\tree\\BTree.py", "repo_name": "ssjssh/algorithm", "stars": 252, "license": "gpl-2.0", "language": "python", "size": 276}
{"docstring": "\"\"\"\nadjust_list \u8868\u793a\u90bb\u63a5\u8282\u70b9\n\"\"\"\n", "func_signal": "def __init__(self, key, weight=None):\n", "code": "self.key = key\nself.weight_list = []\nself.adjust_list = []\nself.in_degree = 0\nself.backup_in_degree = self.in_degree\nself.out_degree = 0\nself.backup_out_degree = self.out_degree\nself.dist = 0", "path": "src\\ssj\\graph\\digraph.py", "repo_name": "ssjssh/algorithm", "stars": 252, "license": "gpl-2.0", "language": "python", "size": 276}
{"docstring": "\"\"\"\nArgs:\n:param key:\u9876\u70b9\u5173\u952e\u5b57\nReturns:\n:rtype: bool\n\"\"\"\n", "func_signal": "def add_vertex(self, key):\n", "code": "if key in self._vertexes:\n    return False\nelse:\n    self._vertexes[key] = Vertex(key)\n    return True", "path": "src\\ssj\\graph\\digraph.py", "repo_name": "ssjssh/algorithm", "stars": 252, "license": "gpl-2.0", "language": "python", "size": 276}
{"docstring": "\"\"\"\n\u67e5\u627e\u5728node\u7684i\u5904\u7684\u8282\u70b9\u7684\u5173\u952e\u5b57\u7684\u540e\u7ee7\u8282\u70b9,\u4f4d\u7f6e\u4f7f\u7528\u4e00\u4e2a\u5143\u7956\u8868\u793a\uff0c\u524d\u9762\u7684\u4e00\u4e2a\u5143\u7d20\u662fnode,\u540e\u9762\u7684\u662findex\n\u5982\u679c\u8fd4\u56deNone\u8868\u793a\u6ca1\u6709\u540e\u7ee7\uff0c\u4e5f\u5c31\u662f\u8bf4\u662f\u6700\u5927\u5143\u7d20\n:param node:\n:param i:\n:return:(Node,index)|None\n\"\"\"\n", "func_signal": "def __successor(self, node, i):\n", "code": "if not node.is_leaf:\n    child_node = node.childs[i + 1]\n    while not child_node.is_leaf:\n        child_node = child_node.childs[0]\n    return self.__successor(child_node, -1)\nelse:\n    if i < len(node) - 1:\n        return node, i + 1\n    else:\n        return None, None", "path": "src\\ssj\\tree\\BTree.py", "repo_name": "ssjssh/algorithm", "stars": 252, "license": "gpl-2.0", "language": "python", "size": 276}
{"docstring": "# \u4e00\u5b9a\u4f1a\u627e\u5230\u4e00\u4e2a\u5143\u7d20\uff0c\u56e0\u6b64\u8fd9\u513f\u76f4\u63a5\u8fd4\u56de\u662f\u5bf9\u7684\u3002\n", "func_signal": "def selection(li, start, end, kth):\n", "code": "if start == end:\n    return start\nmid_index = partition(li, start, end)\nmidth = mid_index - start\nif midth == kth:\n    return mid_index\nelif midth > kth:\n    return selection(li, start, mid_index - 1, kth)\nelif midth < kth:\n    \"\"\"\n    \u8fd9\u513f\u540e\u9762\u5fc5\u987b\u51cf\u53bb1\uff0c\u8fd9\u662f\u56e0\u4e3amth\u548ckth\u90fd\u662f\u4ece0\u5f00\u59cb\u8ba1\u7b97\u7684\n    \"\"\"\n    return selection(li, mid_index + 1, end, kth - midth - 1)", "path": "src\\ssj\\clrs\\9\\9-2(c).py", "repo_name": "ssjssh/algorithm", "stars": 252, "license": "gpl-2.0", "language": "python", "size": 276}
{"docstring": "\"\"\"\n\u641c\u7d22\u6811\u4e2d\u8282\u70b9\n:param root:\n:param instance:\n:return:\n\"\"\"\n", "func_signal": "def __search(cls, root, instance):\n", "code": "cur_node = root\nwhile True:\n    cur_len = len(cur_node)\n    x = 0\n    while x < cur_len and cur_node.keys[x] < instance:\n        x += 1\n    if x < cur_len and cur_node.keys[x] == instance:\n        return cur_node, x\n    elif cur_node.is_leaf:\n        return None, None\n    else:\n        cur_node = cur_node.childs[x]", "path": "src\\ssj\\tree\\BTree.py", "repo_name": "ssjssh/algorithm", "stars": 252, "license": "gpl-2.0", "language": "python", "size": 276}
{"docstring": "\"\"\"\n\u5728\u771f\u5b9e\u7684\u5b9e\u73b0\u4e2d\u5e94\u8be5\u5728\u52a0\u4e0a\u4e00\u4e2a\u6570\u7ec4\u8868\u793a\u6307\u5411\u771f\u5b9e\u6570\u636e\u7684\u6307\u9488\nkeys\u548cchilds\u90fd\u662f\u4e00\u4e2a\u6570\u7ec4\uff0c\u5206\u522b\u8868\u793a\u5173\u952e\u5b57\u548c\u5b69\u5b50\n\"\"\"\n", "func_signal": "def __init__(self, is_leaf, keys, childs=None, parent=None):\n", "code": "self.keys = list(sorted(keys))\nself.is_leaf = is_leaf\nself.__size = len(self.keys)\nif not self.is_leaf and childs is None:\n    self.childs = [None for x in xrange(0, self.__size + 1)]\nelse:\n    self.childs = childs\nself.parent = parent", "path": "src\\ssj\\tree\\BTree.py", "repo_name": "ssjssh/algorithm", "stars": 252, "license": "gpl-2.0", "language": "python", "size": 276}
{"docstring": "\"\"\"\n\u5411B\u6811\u7684\u8282\u70b9\u4e2d\u63d2\u5165\u4e00\u4e2a\u5173\u952e\u5b57\uff0c\u8fd4\u56de\u8fd9\u4e2a\u5173\u952e\u5b57\u7684\u4e0b\u6807\n\"\"\"\n", "func_signal": "def append(self, key):\n", "code": "result = self.__size\nself.__size += 1\n\nfor x in xrange(0, result):\n    if self.keys[x] > key:\n        self.keys.insert(x, key)\n        if not self.is_leaf:\n            self.childs.insert(x, None)\n        return x\nself.keys.append(key)\nif not self.is_leaf:\n    self.childs.append(None)\nreturn result", "path": "src\\ssj\\tree\\BTree.py", "repo_name": "ssjssh/algorithm", "stars": 252, "license": "gpl-2.0", "language": "python", "size": 276}
{"docstring": "\"\"\"Constructor for BTree\"\"\"\n", "func_signal": "def __init__(self, load_factor=4, *vargs):\n", "code": "self.__root = None\nself.__load_factor = load_factor\nself.__size = 0\nmap(self.insert, vargs)", "path": "src\\ssj\\tree\\BTree.py", "repo_name": "ssjssh/algorithm", "stars": 252, "license": "gpl-2.0", "language": "python", "size": 276}
{"docstring": "\"\"\"\n\u68c0\u67e5\u5de6\u53f3\u7684\u5144\u5f1f\u8282\u70b9\u662f\u5426\u80fd\u591f\u501f\u51fa\u8282\u70b9\nint:\u7236\u8282\u70b9\u4e2d\u4ea4\u6362\u5143\u7d20\u7684\u4f4d\u7f6e\nNode:\u8981\u501f\u51fa\u5143\u7d20\u7684\u8282\u70b9\nint\uff1a\u501f\u51fa\u5143\u7d20\u7684\u4f4d\u7f6e\n\n\u5982\u679c\u4e0d\u80fd\u501f\u51fa\uff0c\u90a3\u4e48\u8fd4\u56de(None,None,None)\n:param node:\n:return:(int,Node,int)|(None,None,None)\n\"\"\"\n", "func_signal": "def __check_brother_borrow(self, node):\n", "code": "parent = node.parent\nnode_index = -1\n# \u5bfb\u627enode\u5728parent\u7684\u4f4d\u7f6e\nfor i in xrange(len(parent.childs)):\n    if parent.childs[i] is node:\n        node_index = i\n\n# \u5148\u4ece\u5de6\u8fb9\u7684\u5144\u5f1f\u8282\u70b9\u501f\nif node_index >= 1 and not self.will_starve(parent.childs[node_index - 1]):\n    return node_index - 1, parent.childs[node_index - 1], -1\n\nif node_index >= 0 and not self.will_starve(parent.childs[node_index + 1]):\n    return node_index, parent.childs[node_index + 1], 0\n\nreturn None, None, None", "path": "src\\ssj\\tree\\BTree.py", "repo_name": "ssjssh/algorithm", "stars": 252, "license": "gpl-2.0", "language": "python", "size": 276}
{"docstring": "\"\"\"\n\u8fd9\u513f\u7684\u5b9a\u4e49\u7b80\u76f4\u662f\u6700\u6838\u5fc3\u7684\u90e8\u5206\n\"\"\"\n", "func_signal": "def __cmp__(self, other):\n", "code": "if self.__end < other.__start:\n    return -1\nelif self.__start > other.__end:\n    return 1\nelse:\n    return 0", "path": "src\\ssj\\sort\\improve_fuzzy_space_quick_sort.py", "repo_name": "ssjssh/algorithm", "stars": 252, "license": "gpl-2.0", "language": "python", "size": 276}
{"docstring": "\"\"\"Return a dictionary of emacs-style local variables.\n\nParsing is done loosely according to this spec (and according to\nsome in-practice deviations from this):\nhttp://www.gnu.org/software/emacs/manual/html_node/emacs/Specifying-File-Variables.html#Specifying-File-Variables\n\"\"\"\n", "func_signal": "def _get_emacs_vars(self, text):\n", "code": "emacs_vars = {}\nSIZE = pow(2, 13) # 8kB\n\n# Search near the start for a '-*-'-style one-liner of variables.\nhead = text[:SIZE]\nif \"-*-\" in head:\n    match = self._emacs_oneliner_vars_pat.search(head)\n    if match:\n        emacs_vars_str = match.group(1)\n        assert '\\n' not in emacs_vars_str\n        emacs_var_strs = [s.strip() for s in emacs_vars_str.split(';')\n                          if s.strip()]\n        if len(emacs_var_strs) == 1 and ':' not in emacs_var_strs[0]:\n            # While not in the spec, this form is allowed by emacs:\n            #   -*- Tcl -*-\n            # where the implied \"variable\" is \"mode\". This form\n            # is only allowed if there are no other variables.\n            emacs_vars[\"mode\"] = emacs_var_strs[0].strip()\n        else:\n            for emacs_var_str in emacs_var_strs:\n                try:\n                    variable, value = emacs_var_str.strip().split(':', 1)\n                except ValueError:\n                    log.debug(\"emacs variables error: malformed -*- \"\n                              \"line: %r\", emacs_var_str)\n                    continue\n                # Lowercase the variable name because Emacs allows \"Mode\"\n                # or \"mode\" or \"MoDe\", etc.\n                emacs_vars[variable.lower()] = value.strip()\n\ntail = text[-SIZE:]\nif \"Local Variables\" in tail:\n    match = self._emacs_local_vars_pat.search(tail)\n    if match:\n        prefix = match.group(\"prefix\")\n        suffix = match.group(\"suffix\")\n        lines = match.group(\"content\").splitlines(0)\n        #print \"prefix=%r, suffix=%r, content=%r, lines: %s\"\\\n        #      % (prefix, suffix, match.group(\"content\"), lines)\n\n        # Validate the Local Variables block: proper prefix and suffix\n        # usage.\n        for i, line in enumerate(lines):\n            if not line.startswith(prefix):\n                log.debug(\"emacs variables error: line '%s' \"\n                          \"does not use proper prefix '%s'\"\n                          % (line, prefix))\n                return {}\n            # Don't validate suffix on last line. Emacs doesn't care,\n            # neither should we.\n            if i != len(lines)-1 and not line.endswith(suffix):\n                log.debug(\"emacs variables error: line '%s' \"\n                          \"does not use proper suffix '%s'\"\n                          % (line, suffix))\n                return {}\n\n        # Parse out one emacs var per line.\n        continued_for = None\n        for line in lines[:-1]: # no var on the last line (\"PREFIX End:\")\n            if prefix: line = line[len(prefix):] # strip prefix\n            if suffix: line = line[:-len(suffix)] # strip suffix\n            line = line.strip()\n            if continued_for:\n                variable = continued_for\n                if line.endswith('\\\\'):\n                    line = line[:-1].rstrip()\n                else:\n                    continued_for = None\n                emacs_vars[variable] += ' ' + line\n            else:\n                try:\n                    variable, value = line.split(':', 1)\n                except ValueError:\n                    log.debug(\"local variables error: missing colon \"\n                              \"in local variables entry: '%s'\" % line)\n                    continue\n                # Do NOT lowercase the variable name, because Emacs only\n                # allows \"mode\" (and not \"Mode\", \"MoDe\", etc.) in this block.\n                value = value.strip()\n                if value.endswith('\\\\'):\n                    value = value[:-1].rstrip()\n                    continued_for = variable\n                else:\n                    continued_for = None\n                emacs_vars[variable] = value\n\n# Unquote values.\nfor var, val in emacs_vars.items():\n    if len(val) > 1 and (val.startswith('\"') and val.endswith('\"')\n       or val.startswith('\"') and val.endswith('\"')):\n        emacs_vars[var] = val[1:-1]\n\nreturn emacs_vars", "path": "markdown.py", "repo_name": "finiteloop/blog", "stars": 149, "license": "None", "language": "python", "size": 177}
{"docstring": "\"\"\"_dedent(text, tabsize=8, skip_first_line=False) -> dedented text\n\n    \"text\" is the text to dedent.\n    \"tabsize\" is the tab width to use for indent width calculations.\n    \"skip_first_line\" is a boolean indicating if the first line should\n        be skipped for calculating the indent width and for dedenting.\n        This is sometimes useful for docstrings and similar.\n\ntextwrap.dedent(s), but don't expand tabs to spaces\n\"\"\"\n", "func_signal": "def _dedent(text, tabsize=8, skip_first_line=False):\n", "code": "lines = text.splitlines(1)\n_dedentlines(lines, tabsize=tabsize, skip_first_line=skip_first_line)\nreturn ''.join(lines)", "path": "markdown.py", "repo_name": "finiteloop/blog", "stars": 149, "license": "None", "language": "python", "size": 177}
{"docstring": "# <strong> must go first:\n", "func_signal": "def _do_italics_and_bold(self, text):\n", "code": "if \"code-friendly\" in self.extras:\n    text = self._code_friendly_strong_re.sub(r\"<strong>\\1</strong>\", text)\n    text = self._code_friendly_em_re.sub(r\"<em>\\1</em>\", text)\nelse:\n    text = self._strong_re.sub(r\"<strong>\\2</strong>\", text)\n    text = self._em_re.sub(r\"<em>\\2</em>\", text)\nreturn text", "path": "markdown.py", "repo_name": "finiteloop/blog", "stars": 149, "license": "None", "language": "python", "size": 177}
{"docstring": "# Python markdown note: the HTML tokenization here differs from\n# that in Markdown.pl, hence the behaviour for subtle cases can\n# differ (I believe the tokenizer here does a better job because\n# it isn't susceptible to unmatched '<' and '>' in HTML tags).\n# Note, however, that '>' is not allowed in an auto-link URL\n# here.\n", "func_signal": "def _escape_special_chars(self, text):\n", "code": "escaped = []\nis_html_markup = False\nfor token in self._sorta_html_tokenize_re.split(text):\n    if is_html_markup:\n        # Within tags/HTML-comments/auto-links, encode * and _\n        # so they don't conflict with their use in Markdown for\n        # italics and strong.  We're replacing each such\n        # character with its corresponding MD5 checksum value;\n        # this is likely overkill, but it should prevent us from\n        # colliding with the escape values by accident.\n        escaped.append(token.replace('*', g_escape_table['*'])\n                            .replace('_', g_escape_table['_']))\n    else:\n        escaped.append(self._encode_backslash_escapes(token))\n    is_html_markup = not is_html_markup\nreturn ''.join(escaped)", "path": "markdown.py", "repo_name": "finiteloop/blog", "stars": 149, "license": "None", "language": "python", "size": 177}
{"docstring": "# These are all the transformations that form block-level\n# tags like paragraphs, headers, and list items.\n\n", "func_signal": "def _run_block_gamut(self, text):\n", "code": "text = self._do_headers(text)\n\n# Do Horizontal Rules:\nhr = \"\\n<hr\"+self.empty_element_suffix+\"\\n\"\nfor hr_re in self._hr_res:\n    text = hr_re.sub(hr, text)\n\ntext = self._do_lists(text)\n\nif \"pyshell\" in self.extras:\n    text = self._prepare_pyshell_blocks(text)\n\ntext = self._do_code_blocks(text)\n\ntext = self._do_block_quotes(text)\n\n# We already ran _HashHTMLBlocks() before, in Markdown(), but that\n# was to escape raw HTML in the original Markdown source. This time,\n# we're escaping the markup we've just created, so that we don't wrap\n# <p> tags around block-level tags.\ntext = self._hash_html_blocks(text)\n\ntext = self._form_paragraphs(text)\n\nreturn text", "path": "markdown.py", "repo_name": "finiteloop/blog", "stars": 149, "license": "None", "language": "python", "size": 177}
{"docstring": "# Form HTML ordered (numbered) and unordered (bulleted) lists.\n\n", "func_signal": "def _do_lists(self, text):\n", "code": "for marker_pat in (self._marker_ul, self._marker_ol):\n    # Re-usable pattern to match any entire ul or ol list:\n    less_than_tab = self.tab_width - 1\n    whole_list = r'''\n        (                   # \\1 = whole list\n          (                 # \\2\n            [ ]{0,%d}\n            (%s)            # \\3 = first list item marker\n            [ \\t]+\n          )\n          (?:.+?)\n          (                 # \\4\n              \\Z\n            |\n              \\n{2,}\n              (?=\\S)\n              (?!           # Negative lookahead for another list item marker\n                [ \\t]*\n                %s[ \\t]+\n              )\n          )\n        )\n    ''' % (less_than_tab, marker_pat, marker_pat)\n\n    # We use a different prefix before nested lists than top-level lists.\n    # See extended comment in _process_list_items().\n    #\n    # Note: There's a bit of duplication here. My original implementation\n    # created a scalar regex pattern as the conditional result of the test on\n    # $g_list_level, and then only ran the $text =~ s{...}{...}egmx\n    # substitution once, using the scalar as the pattern. This worked,\n    # everywhere except when running under MT on my hosting account at Pair\n    # Networks. There, this caused all rebuilds to be killed by the reaper (or\n    # perhaps they crashed, but that seems incredibly unlikely given that the\n    # same script on the same server ran fine *except* under MT. I've spent\n    # more time trying to figure out why this is happening than I'd like to\n    # admit. My only guess, backed up by the fact that this workaround works,\n    # is that Perl optimizes the substition when it can figure out that the\n    # pattern will never change, and when this optimization isn't on, we run\n    # afoul of the reaper. Thus, the slightly redundant code to that uses two\n    # static s/// patterns rather than one conditional pattern.\n\n    if self.list_level:\n        sub_list_re = re.compile(\"^\"+whole_list, re.X | re.M | re.S)\n        text = sub_list_re.sub(self._list_sub, text)\n    else:\n        list_re = re.compile(r\"(?:(?<=\\n\\n)|\\A\\n?)\"+whole_list,\n                             re.X | re.M | re.S)\n        text = list_re.sub(self._list_sub, text)\n\nreturn text", "path": "markdown.py", "repo_name": "finiteloop/blog", "stars": 149, "license": "None", "language": "python", "size": 177}
{"docstring": "\"\"\"Decorate with this method to restrict to site admins.\"\"\"\n", "func_signal": "def administrator(method):\n", "code": "@functools.wraps(method)\ndef wrapper(self, *args, **kwargs):\n    if not self.current_user:\n        if self.request.method == \"GET\":\n            self.redirect(self.get_login_url())\n            return\n        raise web.HTTPError(403)\n    elif not self.current_user.administrator:\n        if self.request.method == \"GET\":\n            self.redirect(\"/\")\n            return\n        raise web.HTTPError(403)\n    else:\n        return method(self, *args, **kwargs)\nreturn wrapper", "path": "blog.py", "repo_name": "finiteloop/blog", "stars": 149, "license": "None", "language": "python", "size": 177}
{"docstring": "# These are all the transformations that occur *within* block-level\n# tags like paragraphs, headers, and list items.\n    \n", "func_signal": "def _run_span_gamut(self, text):\n", "code": "text = self._do_code_spans(text)\n    \ntext = self._escape_special_chars(text)\n    \n# Process anchor and image tags.\ntext = self._do_links(text)\n    \n# Make links out of things like `<http://example.com/>`\n# Must come after _do_links(), because you can use < and >\n# delimiters in inline links like [this](<url>).\ntext = self._do_auto_links(text)\n\nif \"link-patterns\" in self.extras:\n    text = self._do_link_patterns(text)\n    \ntext = self._encode_amps_and_angles(text)\n    \ntext = self._do_italics_and_bold(text)\n    \n# Do hard breaks:\ntext = re.sub(r\" {2,}\\n\", \" <br%s\\n\" % self.empty_element_suffix, text)\n    \nreturn text", "path": "markdown.py", "repo_name": "finiteloop/blog", "stars": 149, "license": "None", "language": "python", "size": 177}
{"docstring": "# Used for safe_mode.\n\n", "func_signal": "def _hash_html_spans(self, text):\n", "code": "def _is_auto_link(s):\n    if ':' in s and self._auto_link_re.match(s):\n        return True\n    elif '@' in s and self._auto_email_link_re.match(s):\n        return True\n    return False\n\ntokens = []\nis_html_markup = False\nfor token in self._sorta_html_tokenize_re.split(text):\n    if is_html_markup and not _is_auto_link(token):\n        sanitized = self._sanitize_html(token)\n        key = _hash_text(sanitized)\n        self.html_spans[key] = sanitized\n        tokens.append(key)\n    else:\n        tokens.append(token)\n    is_html_markup = not is_html_markup\nreturn ''.join(tokens)", "path": "markdown.py", "repo_name": "finiteloop/blog", "stars": 149, "license": "None", "language": "python", "size": 177}
{"docstring": "# Swap back in all the special characters we've hidden.\n", "func_signal": "def _unescape_special_chars(self, text):\n", "code": "for ch, hash in g_escape_table.items():\n    text = text.replace(hash, ch)\nreturn text", "path": "markdown.py", "repo_name": "finiteloop/blog", "stars": 149, "license": "None", "language": "python", "size": 177}
{"docstring": "\"\"\"Ensure that Python interactive shell sessions are put in\ncode blocks -- even if not properly indented.\n\"\"\"\n", "func_signal": "def _prepare_pyshell_blocks(self, text):\n", "code": "if \">>>\" not in text:\n    return text\n\nless_than_tab = self.tab_width - 1\n_pyshell_block_re = re.compile(r\"\"\"\n    ^([ ]{0,%d})>>>[ ].*\\n   # first line\n    ^(\\1.*\\S+.*\\n)*         # any number of subsequent lines\n    ^\\n                     # ends with a blank line\n    \"\"\" % less_than_tab, re.M | re.X)\n\nreturn _pyshell_block_re.sub(self._pyshell_block_sub, text)", "path": "markdown.py", "repo_name": "finiteloop/blog", "stars": 149, "license": "None", "language": "python", "size": 177}
{"docstring": "\"\"\"Convert the given text.\"\"\"\n# Main function. The order in which other subs are called here is\n# essential. Link and image substitutions need to happen before\n# _EscapeSpecialChars(), so that any *'s or _'s in the <a>\n# and <img> tags get encoded.\n\n# Clear the global hashes. If we don't clear these, you get conflicts\n# from other articles when generating a page which contains more than\n# one article (e.g. an index page that shows the N most recent\n# articles):\n", "func_signal": "def convert(self, text):\n", "code": "self.reset()\n\nif not isinstance(text, unicode):\n    #TODO: perhaps shouldn't presume UTF-8 for string input?\n    text = unicode(text, 'utf-8')\n\nif self.use_file_vars:\n    # Look for emacs-style file variable hints.\n    emacs_vars = self._get_emacs_vars(text)\n    if \"markdown-extras\" in emacs_vars:\n        splitter = re.compile(\"[ ,]+\")\n        for e in splitter.split(emacs_vars[\"markdown-extras\"]):\n            if '=' in e:\n                ename, earg = e.split('=', 1)\n                try:\n                    earg = int(earg)\n                except ValueError:\n                    pass\n            else:\n                ename, earg = e, None\n            self.extras[ename] = earg\n\n# Standardize line endings:\ntext = re.sub(\"\\r\\n|\\r\", \"\\n\", text)\n\n# Make sure $text ends with a couple of newlines:\ntext += \"\\n\\n\"\n\n# Convert all tabs to spaces.\ntext = self._detab(text)\n\n# Strip any lines consisting only of spaces and tabs.\n# This makes subsequent regexen easier to write, because we can\n# match consecutive blank lines with /\\n+/ instead of something\n# contorted like /[ \\t]*\\n+/ .\ntext = self._ws_only_line_re.sub(\"\", text)\n\nif self.safe_mode:\n    text = self._hash_html_spans(text)\n\n# Turn block-level HTML blocks into hash entries\ntext = self._hash_html_blocks(text, raw=True)\n\n# Strip link definitions, store in hashes.\nif \"footnotes\" in self.extras:\n    # Must do footnotes first because an unlucky footnote defn\n    # looks like a link defn:\n    #   [^4]: this \"looks like a link defn\"\n    text = self._strip_footnote_definitions(text)\ntext = self._strip_link_definitions(text)\n\ntext = self._run_block_gamut(text)\n\nif \"footnotes\" in self.extras:\n    text = self._add_footnotes(text)\n\ntext = self._unescape_special_chars(text)\n\nif self.safe_mode:\n    text = self._unhash_html_spans(text)\n\ntext += \"\\n\"\nreturn text", "path": "markdown.py", "repo_name": "finiteloop/blog", "stars": 149, "license": "None", "language": "python", "size": 177}
{"docstring": "# Setext-style headers:\n#     Header 1\n#     ========\n#  \n#     Header 2\n#     --------\n", "func_signal": "def _do_headers(self, text):\n", "code": "text = self._setext_h_re.sub(self._setext_h_sub, text)\n\n# atx-style headers:\n#   # Header 1\n#   ## Header 2\n#   ## Header 2 with closing hashes ##\n#   ...\n#   ###### Header 6\ntext = self._atx_h_re.sub(self._atx_h_sub, text)\n\nreturn text", "path": "markdown.py", "repo_name": "finiteloop/blog", "stars": 149, "license": "None", "language": "python", "size": 177}
{"docstring": "# Strips link definitions from text, stores the URLs and titles in\n# hash references.\n", "func_signal": "def _strip_link_definitions(self, text):\n", "code": "less_than_tab = self.tab_width - 1\n    \n# Link defs are in the form:\n#   [id]: url \"optional title\"\n_link_def_re = re.compile(r\"\"\"\n    ^[ ]{0,%d}\\[(.+)\\]: # id = \\1\n      [ \\t]*\n      \\n?               # maybe *one* newline\n      [ \\t]*\n    <?(.+?)>?           # url = \\2\n      [ \\t]*\n    (?:\n        \\n?             # maybe one newline\n        [ \\t]*\n        (?<=\\s)         # lookbehind for whitespace\n        ['\"(]\n        ([^\\n]*)        # title = \\3\n        ['\")]\n        [ \\t]*\n    )?  # title is optional\n    (?:\\n+|\\Z)\n    \"\"\" % less_than_tab, re.X | re.M | re.U)\nreturn _link_def_re.sub(self._extract_link_def_sub, text)", "path": "markdown.py", "repo_name": "finiteloop/blog", "stars": 149, "license": "None", "language": "python", "size": 177}
{"docstring": "# Smart processing for ampersands and angle brackets that need\n# to be encoded.\n", "func_signal": "def _encode_amps_and_angles(self, text):\n", "code": "text = self._ampersand_re.sub('&amp;', text)\n    \n# Encode naked <'s\ntext = self._naked_lt_re.sub('&lt;', text)\n\n# Encode naked >'s\n# Note: Other markdown implementations (e.g. Markdown.pl, PHP\n# Markdown) don't do this.\ntext = self._naked_gt_re.sub('&gt;', text)\nreturn text", "path": "markdown.py", "repo_name": "finiteloop/blog", "stars": 149, "license": "None", "language": "python", "size": 177}
{"docstring": "\"\"\"Turn Markdown link shortcuts into XHTML <a> and <img> tags.\n\nThis is a combination of Markdown.pl's _DoAnchors() and\n_DoImages(). They are done together because that simplified the\napproach. It was necessary to use a different approach than\nMarkdown.pl because of the lack of atomic matching support in\nPython's regex engine used in $g_nested_brackets.\n\"\"\"\n", "func_signal": "def _do_links(self, text):\n", "code": "MAX_LINK_TEXT_SENTINEL = 3000  # markdown2 issue 24\n\n# `anchor_allowed_pos` is used to support img links inside\n# anchors, but not anchors inside anchors. An anchor's start\n# pos must be `>= anchor_allowed_pos`.\nanchor_allowed_pos = 0\n\ncurr_pos = 0\nwhile True: # Handle the next link.\n    # The next '[' is the start of:\n    # - an inline anchor:   [text](url \"title\")\n    # - a reference anchor: [text][id]\n    # - an inline img:      ![text](url \"title\")\n    # - a reference img:    ![text][id]\n    # - a footnote ref:     [^id]\n    #   (Only if 'footnotes' extra enabled)\n    # - a footnote defn:    [^id]: ...\n    #   (Only if 'footnotes' extra enabled) These have already\n    #   been stripped in _strip_footnote_definitions() so no\n    #   need to watch for them.\n    # - a link definition:  [id]: url \"title\"\n    #   These have already been stripped in\n    #   _strip_link_definitions() so no need to watch for them.\n    # - not markup:         [...anything else...\n    try:\n        start_idx = text.index('[', curr_pos)\n    except ValueError:\n        break\n    text_length = len(text)\n\n    # Find the matching closing ']'.\n    # Markdown.pl allows *matching* brackets in link text so we\n    # will here too. Markdown.pl *doesn't* currently allow\n    # matching brackets in img alt text -- we'll differ in that\n    # regard.\n    bracket_depth = 0\n    for p in range(start_idx+1, min(start_idx+MAX_LINK_TEXT_SENTINEL, \n                                    text_length)):\n        ch = text[p]\n        if ch == ']':\n            bracket_depth -= 1\n            if bracket_depth < 0:\n                break\n        elif ch == '[':\n            bracket_depth += 1\n    else:\n        # Closing bracket not found within sentinel length.\n        # This isn't markup.\n        curr_pos = start_idx + 1\n        continue\n    link_text = text[start_idx+1:p]\n\n    # Possibly a footnote ref?\n    if \"footnotes\" in self.extras and link_text.startswith(\"^\"):\n        normed_id = re.sub(r'\\W', '-', link_text[1:])\n        if normed_id in self.footnotes:\n            self.footnote_ids.append(normed_id)\n            result = '<sup class=\"footnote-ref\" id=\"fnref-%s\">' \\\n                     '<a href=\"#fn-%s\">%s</a></sup>' \\\n                     % (normed_id, normed_id, len(self.footnote_ids))\n            text = text[:start_idx] + result + text[p+1:]\n        else:\n            # This id isn't defined, leave the markup alone.\n            curr_pos = p+1\n        continue\n\n    # Now determine what this is by the remainder.\n    p += 1\n    if p == text_length:\n        return text\n\n    # Inline anchor or img?\n    if text[p] == '(': # attempt at perf improvement\n        match = self._tail_of_inline_link_re.match(text, p)\n        if match:\n            # Handle an inline anchor or img.\n            is_img = start_idx > 0 and text[start_idx-1] == \"!\"\n            if is_img:\n                start_idx -= 1\n\n            url, title = match.group(\"url\"), match.group(\"title\")\n            if url and url[0] == '<':\n                url = url[1:-1]  # '<url>' -> 'url'\n            # We've got to encode these to avoid conflicting\n            # with italics/bold.\n            url = url.replace('*', g_escape_table['*']) \\\n                     .replace('_', g_escape_table['_'])\n            if title:\n                title_str = ' title=\"%s\"' \\\n                    % title.replace('*', g_escape_table['*']) \\\n                           .replace('_', g_escape_table['_']) \\\n                           .replace('\"', '&quot;')\n            else:\n                title_str = ''\n            if is_img:\n                result = '<img src=\"%s\" alt=\"%s\"%s%s' \\\n                    % (url, link_text.replace('\"', '&quot;'),\n                       title_str, self.empty_element_suffix)\n                curr_pos = start_idx + len(result)\n                text = text[:start_idx] + result + text[match.end():]\n            elif start_idx >= anchor_allowed_pos:\n                result_head = '<a href=\"%s\"%s>' % (url, title_str)\n                result = '%s%s</a>' % (result_head, link_text)\n                # <img> allowed from curr_pos on, <a> from\n                # anchor_allowed_pos on.\n                curr_pos = start_idx + len(result_head)\n                anchor_allowed_pos = start_idx + len(result)\n                text = text[:start_idx] + result + text[match.end():]\n            else:\n                # Anchor not allowed here.\n                curr_pos = start_idx + 1\n            continue\n\n    # Reference anchor or img?\n    else:\n        match = self._tail_of_reference_link_re.match(text, p)\n        if match:\n            # Handle a reference-style anchor or img.\n            is_img = start_idx > 0 and text[start_idx-1] == \"!\"\n            if is_img:\n                start_idx -= 1\n            link_id = match.group(\"id\").lower()\n            if not link_id:\n                link_id = link_text.lower()  # for links like [this][]\n            if link_id in self.urls:\n                url = self.urls[link_id]\n                # We've got to encode these to avoid conflicting\n                # with italics/bold.\n                url = url.replace('*', g_escape_table['*']) \\\n                         .replace('_', g_escape_table['_'])\n                title = self.titles.get(link_id)\n                if title:\n                    title = title.replace('*', g_escape_table['*']) \\\n                                 .replace('_', g_escape_table['_'])\n                    title_str = ' title=\"%s\"' % title\n                else:\n                    title_str = ''\n                if is_img:\n                    result = '<img src=\"%s\" alt=\"%s\"%s%s' \\\n                        % (url, link_text.replace('\"', '&quot;'),\n                           title_str, self.empty_element_suffix)\n                    curr_pos = start_idx + len(result)\n                    text = text[:start_idx] + result + text[match.end():]\n                elif start_idx >= anchor_allowed_pos:\n                    result = '<a href=\"%s\"%s>%s</a>' \\\n                        % (url, title_str, link_text)\n                    result_head = '<a href=\"%s\"%s>' % (url, title_str)\n                    result = '%s%s</a>' % (result_head, link_text)\n                    # <img> allowed from curr_pos on, <a> from\n                    # anchor_allowed_pos on.\n                    curr_pos = start_idx + len(result_head)\n                    anchor_allowed_pos = start_idx + len(result)\n                    text = text[:start_idx] + result + text[match.end():]\n                else:\n                    # Anchor not allowed here.\n                    curr_pos = start_idx + 1\n            else:\n                # This id isn't defined, leave the markup alone.\n                curr_pos = match.end()\n            continue\n\n    # Otherwise, it isn't markup.\n    curr_pos = start_idx + 1\n\nreturn text", "path": "markdown.py", "repo_name": "finiteloop/blog", "stars": 149, "license": "None", "language": "python", "size": 177}
{"docstring": "\"\"\"Encode/escape certain characters inside Markdown code runs.\nThe point is that in code, these characters are literals,\nand lose their special Markdown meanings.\n\"\"\"\n", "func_signal": "def _encode_code(self, text):\n", "code": "replacements = [\n    # Encode all ampersands; HTML entities are not\n    # entities within a Markdown code span.\n    ('&', '&amp;'),\n    # Do the angle bracket song and dance:\n    ('<', '&lt;'),\n    ('>', '&gt;'),\n    # Now, escape characters that are magic in Markdown:\n    ('*', g_escape_table['*']),\n    ('_', g_escape_table['_']),\n    ('{', g_escape_table['{']),\n    ('}', g_escape_table['}']),\n    ('[', g_escape_table['[']),\n    (']', g_escape_table[']']),\n    ('\\\\', g_escape_table['\\\\']),\n]\nfor before, after in replacements:\n    text = text.replace(before, after)\nreturn text", "path": "markdown.py", "repo_name": "finiteloop/blog", "stars": 149, "license": "None", "language": "python", "size": 177}
{"docstring": "# Let the templates access the users module to generate login URLs\n", "func_signal": "def render_string(self, template_name, **kwargs):\n", "code": "return tornado.web.RequestHandler.render_string(\n    self, template_name, users=users, **kwargs)", "path": "blog.py", "repo_name": "finiteloop/blog", "stars": 149, "license": "None", "language": "python", "size": 177}
{"docstring": "\"\"\"A function for use in a Pygments Formatter which\nwraps in <code> tags.\n\"\"\"\n", "func_signal": "def _wrap_code(self, inner):\n", "code": "yield 0, \"<code>\"\nfor tup in inner:\n    yield tup \nyield 0, \"</code>\"", "path": "markdown.py", "repo_name": "finiteloop/blog", "stars": 149, "license": "None", "language": "python", "size": 177}
{"docstring": "\"\"\"Standalone XML processing instruction regex.\"\"\"\n", "func_signal": "def _xml_oneliner_re_from_tab_width(tab_width):\n", "code": "return re.compile(r\"\"\"\n    (?:\n        (?<=\\n\\n)       # Starting after a blank line\n        |               # or\n        \\A\\n?           # the beginning of the doc\n    )\n    (                           # save in $1\n        [ ]{0,%d}\n        (?:\n            <\\?\\w+\\b\\s+.*?\\?>   # XML processing instruction\n            |\n            <\\w+:\\w+\\b\\s+.*?/>  # namespaced single tag\n        )\n        [ \\t]*\n        (?=\\n{2,}|\\Z)       # followed by a blank line or end of document\n    )\n    \"\"\" % (tab_width - 1), re.X)", "path": "markdown.py", "repo_name": "finiteloop/blog", "stars": 149, "license": "None", "language": "python", "size": 177}
{"docstring": "'''\nTotal detect time\n'''\n", "func_signal": "def total_detect_time():\n", "code": "secs = 0\nfor detects in DebugStats.total_rule_detects:\n    secs += detects[1]\nreturn secs", "path": "debug.py", "repo_name": "urule99/jsunpack-n", "stars": 157, "license": "gpl-2.0", "language": "python", "size": 13128}
{"docstring": "'''\nJs time\n'''\n", "func_signal": "def js_time(self):\n", "code": "secs = 0\nfor js_lauches in self.js_launches:\n    secs += js_lauches[1]\nreturn secs", "path": "debug.py", "repo_name": "urule99/jsunpack-n", "stars": 157, "license": "gpl-2.0", "language": "python", "size": 13128}
{"docstring": "''' Build a full URL from possible components/pieces\nurlin is the URL from the HTML\nif the path startswith http, return\nif the path starts with /, return server + path\nif the path is relative, return serverpath+path\n'''\n", "func_signal": "def build_url_from_path(self, path):\n", "code": "if path.find('\\\\/') > -1:\n    #fix escaped slashes\n    path = re.sub('\\\\\\/', '/', path)\n\nif path.startswith('http') or path.startswith('//'):\n    return re.sub('^[https]*:?//', '', path)\nif path.startswith('hcp:'):\n    return path\nif path.startswith('/'):\n    if self.url.startswith('/'):\n        server = '127.0.0.1'\n    else:\n        server = re.sub('([^/])/.*$', '\\\\1', self.url)\n    return server + path\n\n#relative, preserve directory (unless its a file)\nserverpath = re.sub('/[^\\/]*$', '/', self.url)\nif self.url.startswith('/'): #its a file\n    serverpath = '127.0.0.1/'\n\nresult = serverpath + path\nspaces = result.find(' ')\nif spaces > -1:\n    lessthan = result.find('<')\n    if -1 < lessthan < spaces:\n        result = result[:lessthan]\n    else:\n        result = result[:spaces]\nreturn result", "path": "jsunpackn.py", "repo_name": "urule99/jsunpack-n", "stars": 157, "license": "gpl-2.0", "language": "python", "size": 13128}
{"docstring": "'''returns [hostname,port]'''\n", "func_signal": "def hostname_from_url(self, url):\n", "code": "hostname = '0.0.0.0'\ndstport = 80\nslashIndex = url.find('/')\nif slashIndex > -1:\n    hostname = url[:slashIndex] #everything before the first /\nelse:\n    hostname = url #everything\n\nif hostname:\n    colonIndex = hostname.find(':')\n    if colonIndex > -1:\n        try:\n            dstport = int(hostname[colonIndex + 1:])\n        except:\n            pass #ignore errors in port\n        hostname = hostname[:colonIndex]\n\nreturn hostname, dstport", "path": "jsunpackn.py", "repo_name": "urule99/jsunpack-n", "stars": 157, "license": "gpl-2.0", "language": "python", "size": 13128}
{"docstring": "'''lookup is a sha1hash'''\n", "func_signal": "def file_exists(self, lookup):\n", "code": "for type, hash, data in self.files:\n    if lookup == hash:\n        return True\nreturn False", "path": "urlattr.py", "repo_name": "urule99/jsunpack-n", "stars": 157, "license": "gpl-2.0", "language": "python", "size": 13128}
{"docstring": "'''\nStart main\n'''\n", "func_signal": "def start_main(self):\n", "code": "self.before_decode = self.during_decode = time()\nself.responsibility = { 'init': 0, 'decoding':0, 'shellcode':0 }", "path": "debug.py", "repo_name": "urule99/jsunpack-n", "stars": 157, "license": "gpl-2.0", "language": "python", "size": 13128}
{"docstring": "'''returns JavaScript (if it exists)'''\n", "func_signal": "def find_urls(self, data, tcpaddr=[]):\n", "code": "jsdata = ''\n\nif data.find('http:') > -1:\n    varurl = re.findall('var[^=]*=[\\\\\\'\" ]+(http:[^\\'\"\\n]+)[\\\\\\'\"]', data, re.IGNORECASE)\n    for i in varurl:\n        if i.find('\\\\/') != -1:\n            i = re.sub('\\\\\\/', '/', i)\n\n        i = re.sub('^[https]+://', '', i)\n        self.rooturl[self.url].setChild(i, 'jsvar')\n        self.rooturl[self.url].log(self.OPTIONS.verbose, 0, '[javascript variable] URL=%s' % i)\n\nmetarefresh = re.findall('content\\s*=\\s*[\\\\\\'\"]?\\d+\\s*;\\s*url\\s*=\\s*([^ \\r\\\\\\'\"]+)', data, re.IGNORECASE)\nfor i in metarefresh:\n    i = self.build_url_from_path(i)\n    self.rooturl[self.url].setChild(i, 'metarefresh')\n    self.rooturl[self.url].log(self.OPTIONS.verbose, 0, '[meta refresh] URL=%s' % i)\n\nif data.find('//jsunpack.url') > -1:\n    fetch = re.findall('//jsunpack.url (.*?) = (.*?)\\n', data)\n    #//jsunpack.url setAttribute src = URL\n    for desc, i in fetch:\n        i = self.build_url_from_path(i)\n        self.rooturl[self.url].setChild(i, desc)\n        self.rooturl[self.url].log(self.OPTIONS.verbose, 0, '[%s] URL=%s' % (desc, i))\n\nif data.find(' src=') > -1:\n    iframe = re.findall('<(i?frame|embed|script|img|input)[^>]*?[ ]+src=\\\\\\\\?[\\\\\\'\"]?(.*?)\\\\\\\\?[\\\\\\'\"> ]', data, re.IGNORECASE)\n    for type, i in iframe:\n        type = type.lower()\n\n        i = self.build_url_from_path(i)\n        if i.startswith('hcp:'):\n            lt = i.find('%3c')\n            LT = i.find('%3C')\n            if lt == -1 and LT > -1:\n                lt = LT\n            if lt > -1:\n                jsdata += re.sub('%([a-fA-F0-9]{2})', lambda mo: convert(mo.group(1)), i[lt:])\n        self.rooturl[self.url].setChild(i, type)\n        self.rooturl[self.url].log(self.OPTIONS.verbose, 0, '[%s] %s' % (type, i))\nif data.find('<link ') > -1:\n    links = re.findall('<(link)[^>]*?[ ]+href=\\\\\\\\?[\\\\\\'\"]?(.*?)\\\\\\\\?[\\\\\\'\"> ]', data, re.IGNORECASE)\n    for type, i in links:\n        type = type.lower()\n        i = self.build_url_from_path(i)\n\n        self.rooturl[self.url].setChild(i, type)\nif data.find(' archive=') > -1:\n    #Ex. <applet mayscript='true' code='bpac.a.class' archive='bnktjvdpxuko4.jar\n    jars = re.findall('<(applet|object)([^>]*)[ ]+archive=\\\\\\\\?[\\\\\\'\"]?(.*?)\\\\\\\\?[\\\\\\'\"> ]', data, re.IGNORECASE)\n    for type, other_text, i in jars:\n        i = self.build_url_from_path(i)\n        self.rooturl[self.url].setChild(i, type)\n\nreturn jsdata", "path": "jsunpackn.py", "repo_name": "urule99/jsunpack-n", "stars": 157, "license": "gpl-2.0", "language": "python", "size": 13128}
{"docstring": "#recursive! append to returls parameter\n", "func_signal": "def getChildUrls(self, start, returls=[]):\n", "code": "returls.append(start)\n\nif start in self.rooturl:\n    for t, u in self.rooturl[start].children:\n        if not u in returls:\n            returls = self.getChildUrls(u, returls)\nreturn returls", "path": "urlattr.py", "repo_name": "urule99/jsunpack-n", "stars": 157, "license": "gpl-2.0", "language": "python", "size": 13128}
{"docstring": "'''\nInput: can be html code or raw JavaScript code\nOutput: an array of [headers, raw JavaScript]\n'''\n", "func_signal": "def htmlparse(self, data):\n", "code": "outheader, out = '', ''\ndata = re.sub('\\x00', '', data)\n\ntry:\n    soup = BeautifulSoup(data)\nexcept:\n    print('Fatal error during HTML parsing')\n    return '', '' \n\nfor tag, attrib, invals, hformat, outvals in self.html_parse_rules:\n    for htm in soup.findAll(tag, attrib):\n        now = {}\n        ignore = False #if a negated match occurs\n        for val in invals:\n            if val.startswith('!'):\n                #negated match\n                val = val[1:]\n                try:\n                    now[val] = str(htm[val])\n                    ignore = True\n                except:\n                    pass #expected behavior\n\n        if not ignore:\n            for val in outvals:\n                if val == '*':\n                    now['*'] = ''\n                elif val == 'contents':\n                    try: \n                        now['contents'] = ' '.join(map(str, \n                                                       htm.contents))\n                    except KeyError: \n                        now['contents'] = ''\n                    except UnicodeEncodeError: \n                        now['contents'] = ' '.join(map(str, \n                                                       str(htm.contents)\n                                                       ))\n                elif val == 'name':\n                    try: \n                        now['name'] = htm.name\n                    except KeyError: \n                        now['name'] = ''\n                else:\n                    try: \n                        now[val] = str(htm[val])\n                    except KeyError: \n                        now[val] = ''\n\n            #normalize when assigning to variables\n            for k in now: \n                # if this fails, it means that we are trying to get the\n                # result in python\n                if hformat in self.html_definitions:\n                    if not hformat.startswith('raw'):\n                        now[k] = re.sub('([^a-zA-Z0-9])', \n                                        lambda m: ('\\\\x%02x' \n                                                   % ord(m.group(1))), \n                                        now[k])\n                        now[k] = \"'%s'\" % now[k]\n\n            # if this fails, it means that we are trying to get the \n            # result in python\n            if hformat in self.html_definitions: \n                myfmt = re.sub('^\\s+', '', \n                               self.html_definitions[hformat]\n                               ).split('%s')\n                if len(myfmt) - 1 == len(outvals):\n                    lineout = ''\n                    for i in range(0, len(outvals)):\n                        lineout += myfmt[i]\n                        lineout += now[outvals[i]]\n                    lineout += myfmt[-1] + '\\n'\n\n                    if htm.name in self.html_filters:\n                        lineout = re.sub(self.html_filters[htm.name], \n                                         '', lineout)\n                    if '*' in self.html_filters:\n                        lineout = re.sub(self.html_filters['*'], '', \n                                         lineout, re.I)\n                    if hformat.startswith('header'):\n                        outheader += lineout\n                    else:\n                        out += lineout\n                else:\n                    print ('fatal: invalid htmlparse.config hformat, '\n                           'parameter count or definition problem')\n            else:\n                for i in range(0, len(outvals)):\n                    self.storage.append([hformat, now[outvals[i]]])\nreturn str(outheader), str(out)", "path": "html.py", "repo_name": "urule99/jsunpack-n", "stars": 157, "license": "gpl-2.0", "language": "python", "size": 13128}
{"docstring": "'''\nRecord main\n'''\n", "func_signal": "def record_main(self, record_type):\n", "code": "right_now = time()\nself.responsibility[record_type] = right_now - self.during_decode\nself.during_decode = right_now", "path": "debug.py", "repo_name": "urule99/jsunpack-n", "stars": 157, "license": "gpl-2.0", "language": "python", "size": 13128}
{"docstring": "'''\n    outdir is the directory prefix\n'''\n", "func_signal": "def create_sha1file(self, outdir, data, type='sha1'):\n", "code": "if len(data) <= 0:\n    return ''\n\nshash = sha1(data).hexdigest()\nsha1file = 'undefined'\nsha1file = '%s/%s_%s' % (outdir, type, shash)\n\nif outdir: #no output directory means don't output anything\n    if not os.path.isdir(outdir):\n        os.mkdir(outdir)\n    if os.path.isdir(outdir):\n        ffile = open(sha1file, 'wb')\n        ffile.write(data)\n        ffile.close()\n\n#self.files.append([type,shash,data])\nif not self.file_exists(shash):\n    self.files.append([type, shash, data])\n\nreturn sha1file", "path": "urlattr.py", "repo_name": "urule99/jsunpack-n", "stars": 157, "license": "gpl-2.0", "language": "python", "size": 13128}
{"docstring": "'''\n    add childurl as a child of self.url\n    if childurl already has a type (default, shellcode, jsvar, redirect)\n    and it already exists as a child, we'll keep the value previously set\n'''\n", "func_signal": "def setChild(self, childurl, type):\n", "code": "if len(childurl) <= 4:\n    #make sure length is > 4 (ie, a valid URL)\n    return\n\nchildurl = canonicalize(childurl)\nif self.url == childurl:\n    return # linking to itself is stupid\n\nif not childurl in self.rooturl:\n    self.rooturl[childurl] = urlattr(self.rooturl, childurl)\n    self.rooturl[childurl].type = type\n\n    #preserve method,hasParent,type\n    #child_urlattr.mergeEntries()\n\nif not type == 'default':\n    if self.rooturl[childurl].type == 'refer': #prefer other types over refer\n        self.rooturl[childurl].type = type \n    elif type == 'refer' and self.rooturl[childurl].type != 'default': #prefer other types over refer\n        type = self.rooturl[childurl].type\n        \n    #this logic determines whether childurl can safely be removed from the root\n    #setting the hasParent flag to True will disconnect it\n\n    if len(self.rooturl[childurl].children) <= 0:\n        #require that the node has no existing children \n        #to prevent it from being disconnected from the tree\n        self.rooturl[childurl].hasParent = True\n        self.rooturl[childurl].type = type\n\n    elif not self.url in self.getChildUrls(childurl):\n        #looks through self.rooturl[childurl].children, if you find self.url don't destroy the childurl type\n        #doing so is bad because it would disconnect the tree\n        self.rooturl[childurl].hasParent = True\n        self.rooturl[childurl].type = type\n    #else:\n    #   print 'setChild: ignored %s (whose parent should be %s) because it would disconnect the tree' % (childurl,self.url)\n\n\nif not self.child_exists(childurl):\n    self.rooturl[self.url].children.append([type, childurl])", "path": "urlattr.py", "repo_name": "urule99/jsunpack-n", "stars": 157, "license": "gpl-2.0", "language": "python", "size": 13128}
{"docstring": "'''\nDetect times\n'''\n", "func_signal": "def detect_time(self):\n", "code": "secs = 0\nfor rule_detects in self.rule_detects:\n    secs += rule_detects[1]\nreturn secs", "path": "debug.py", "repo_name": "urule99/jsunpack-n", "stars": 157, "license": "gpl-2.0", "language": "python", "size": 13128}
{"docstring": "#previously this was non-greedy, but js with '>>' does mess things up in that case\n#to solve the problem, do both\n\n#if pdf.DEBUG:\n#    print '\\tstarting object len %d' % len(self.indata)\n", "func_signal": "def parseObject(self):\n", "code": "tags = re.findall('<<(.*)>>[\\s\\r\\n%]*(?:stream[\\r\\n]*(.*?)[\\r\\n]*endstream)?', self.indata, re.MULTILINE | re.DOTALL | re.IGNORECASE)\nif tags:\n    for tag, stream in tags:\n        gttag = tag.find('>>')\n        streamtag = tag.find('stream')\n        endstreamTagEnd = self.indata.rfind('endstream')\n        endstreamTagBegin = self.indata.find('endstream')\n        #\n        # This means that there was an improper parsing because the tag shouldn't contain a stream object\n        if endstreamTagEnd != -1 and 0 < gttag < streamtag:\n            # do this in case the word stream is in the tag data somewhere...\n            streamLocation = re.search('>>[\\s\\r\\n%]*stream?', self.indata, re.MULTILINE | re.DOTALL | re.IGNORECASE)\n\n            streamStart = self.indata.find('stream', streamLocation.start())\n            streamMatch = re.search('stream[\\s\\r\\n]*(.*?)[\\r\\n]*endstream', self.indata, re.MULTILINE | re.DOTALL | re.IGNORECASE)\n            streamData = ''\n            # Only search to start of stream, a compressed stream can have >> in it, and that will through off the regex\n            tagMatch = re.search('<<(.*)>>', self.indata[0:streamStart], re.MULTILINE | re.DOTALL | re.IGNORECASE)\n            if tagMatch and streamMatch:\n                streamData = streamMatch.group(1)\n                tag = tagMatch.group(1)\n                tags = [(tag, streamData)]\n        #\n        # This checks if the word endstream happens inside the stream\n        if endstreamTagBegin != -1 and endstreamTagBegin != endstreamTagEnd:\n            streamLocation = re.search('>>[\\s\\r\\n%]*stream?', self.indata, re.MULTILINE | re.DOTALL | re.IGNORECASE)\n            streamStart = self.indata.find('stream', streamLocation.start())\n            streamMatch = re.search('stream[\\s\\r\\n]*(.*?)[\\r\\n]*endstream$', self.indata, re.MULTILINE | re.DOTALL | re.IGNORECASE)\n            tagMatch = re.search('<<(.*)>>', self.indata[0:streamStart], re.MULTILINE | re.DOTALL | re.IGNORECASE)\n            streamData = ''\n            if streamMatch and tagMatch:\n                streamData = streamMatch.group(1)\n                tag = tagMatch.group(1)\n                tags = [(tag, streamData)]\n\nif not tags: #Error parsing object!\n    return\n\nfor tag, stream in tags:\n    self.parseTag(tag, stream)\n    self.parseChildren()", "path": "pdf.py", "repo_name": "urule99/jsunpack-n", "stars": 157, "license": "gpl-2.0", "language": "python", "size": 13128}
{"docstring": "'''\nReset total stats\n'''\n", "func_signal": "def reset_total_stats():\n", "code": "DebugStats.total_js_launches = []\nDebugStats.total_rule_detects = []", "path": "debug.py", "repo_name": "urule99/jsunpack-n", "stars": 157, "license": "gpl-2.0", "language": "python", "size": 13128}
{"docstring": "#returns '#3a' substituted with ':', etc\n#strips newlines, '[', and ']' characters\n#this allows indexing in arrays\n\n", "func_signal": "def fixPound(i):\n", "code": "i = re.sub('[\\[\\]\\n]', '', i)\ni = re.sub('<<$', '', i)\nreturn re.sub('#([a-fA-F0-9]{2})', lambda mo: chr(int('0x' + mo.group(1), 0)), i)", "path": "pdf.py", "repo_name": "urule99/jsunpack-n", "stars": 157, "license": "gpl-2.0", "language": "python", "size": 13128}
{"docstring": "'''\nFinalize main\n'''\n", "func_signal": "def finalize_main(self):\n", "code": "if time() - self.before_decode > 3:\n    print ('main_decoder %s took %.02f seconds (ignored %d urls since '\n           'last print)' % (self.name[0:20],\n                            time() - self.before_decode,\n                            DebugStats.ignored_main))\n    if (self.responsibility['init'] > 0.5 or \n        self.responsibility['shellcode'] > 1):\n        print ('\\t(%.02f init, %.02f decoding, %.02f sc)' \n               % (self.responsibility['init'],\n                  self.responsibility['decoding'],\n                  self.responsibility['shellcode']))\n    DebugStats.ignored_main = 0\nelif DebugStats.ignored_main >= 10:\n    print ('main_decoder ignored %d urls since they are below threshold'\n           % (DebugStats.ignored_main))\n    DebugStats.ignored_main = 0\nelse:\n    DebugStats.ignored_main += 1", "path": "debug.py", "repo_name": "urule99/jsunpack-n", "stars": 157, "license": "gpl-2.0", "language": "python", "size": 13128}
{"docstring": "'''\n    Input: data is the data to decrypt, key is the obj information of the form '5 0'\n    Assumptions: self.encryptKey is set\n    Output: returns string of decrypted data\n'''\n", "func_signal": "def decryptRC4(self, data, key):\n", "code": "try:\n    obj, rev = key.split(' ')\n\n    keyLength = self.encryptObject['KeyLength'] + 5\n    if keyLength > 16:\n        keyLength = 16\n\n    decrypt_key = md5(self.encryptKey + struct.pack('L', int(obj))[0:3] + struct.pack('L', int(rev))[0:2]).digest()[0:keyLength]\n    cipher = ARC4.new(decrypt_key)\n    return cipher.decrypt(data)\nexcept:\n    return ''", "path": "pdf.py", "repo_name": "urule99/jsunpack-n", "stars": 157, "license": "gpl-2.0", "language": "python", "size": 13128}
{"docstring": "# Negative numbers result in reading all the lines\n", "func_signal": "def readlines(self, sizehint=0):\n", "code": "if sizehint <= 0:\n    sizehint = sys.maxint\nL = []\nwhile sizehint > 0:\n    line = self.readline()\n    if line == \"\":\n        break\n    L.append(line)\n    sizehint = sizehint - len(line)\n\nreturn L", "path": "gzip.py", "repo_name": "urule99/jsunpack-n", "stars": 157, "license": "gpl-2.0", "language": "python", "size": 13128}
{"docstring": "'''\n    Input: self.tags (must be populated)\n    Output: self.children\n'''\n", "func_signal": "def parseChildren(self):\n", "code": "for state, k, kval in self.tags:\n    hasRef = re.search('^(\\d+)\\s+(\\d+)\\s+R', kval)\n    if hasRef:\n        objkey = hasRef.group(1) + ' ' + hasRef.group(2)\n        self.children.append([k, objkey])\n    if k == 'XFA':\n        kids = re.findall('(\\d+\\s+\\d+)\\s+R', kval)\n        for kid in kids:\n            self.xfaChildren.append([k, kid])", "path": "pdf.py", "repo_name": "urule99/jsunpack-n", "stars": 157, "license": "gpl-2.0", "language": "python", "size": 13128}
{"docstring": "\"\"\" Check that a timeout attribute is valid\n\n:param value: The timeout value to validate\n:param name: The name of the timeout attribute to validate. This is used\n    for clear error messages\n:return: the value\n:raises ValueError: if the type is not an integer or a float, or if it\n    is a numeric value less than zero\n\"\"\"\n", "func_signal": "def _validate_timeout(cls, value, name):\n", "code": "if value is _Default:\n    return cls.DEFAULT_TIMEOUT\n\nif value is None or value is cls.DEFAULT_TIMEOUT:\n    return value\n\ntry:\n    float(value)\nexcept (TypeError, ValueError):\n    raise ValueError(\"Timeout value %s was %s, but it must be an \"\n                     \"int or float.\" % (name, value))\n\ntry:\n    if value < 0:\n        raise ValueError(\"Attempted to set %s timeout to %s, but the \"\n                         \"timeout cannot be set to a value less \"\n                         \"than 0.\" % (name, value))\nexcept TypeError: # Python 3\n    raise ValueError(\"Timeout value %s was %s, but it must be an \"\n                     \"int or float.\" % (name, value))\n\nreturn value", "path": "plotly_yun\\Linino\\requests\\packages\\urllib3\\util.py", "repo_name": "plotly/arduino-api", "stars": 208, "license": "None", "language": "python", "size": 6118}
{"docstring": "\"\"\"\nMake a request using :meth:`urlopen` with the ``fields`` encoded in\nthe url. This is useful for request methods like GET, HEAD, DELETE, etc.\n\"\"\"\n", "func_signal": "def request_encode_url(self, method, url, fields=None, **urlopen_kw):\n", "code": "if fields:\n    url += '?' + urlencode(fields)\nreturn self.urlopen(method, url, **urlopen_kw)", "path": "plotly_yun\\Linino\\requests\\packages\\urllib3\\request.py", "repo_name": "plotly/arduino-api", "stars": 208, "license": "None", "language": "python", "size": 6118}
{"docstring": "\"\"\"\nGiven a string and an iterable of delimiters, split on the first found\ndelimiter. Return two split parts and the matched delimiter.\n\nIf not found, then the first part is the full input string.\n\nExample: ::\n\n    >>> split_first('foo/bar?baz', '?/=')\n    ('foo', 'bar?baz', '/')\n    >>> split_first('foo/bar?baz', '123')\n    ('foo/bar?baz', '', None)\n\nScales linearly with number of delims. Not ideal for large number of delims.\n\"\"\"\n", "func_signal": "def split_first(s, delims):\n", "code": "min_idx = None\nmin_delim = None\nfor d in delims:\n    idx = s.find(d)\n    if idx < 0:\n        continue\n\n    if min_idx is None or idx < min_idx:\n        min_idx = idx\n        min_delim = d\n\nif min_idx is None or min_idx < 0:\n    return s, '', None\n\nreturn s[:min_idx], s[min_idx+1:], min_delim", "path": "plotly_yun\\Linino\\requests\\packages\\urllib3\\util.py", "repo_name": "plotly/arduino-api", "stars": 208, "license": "None", "language": "python", "size": 6118}
{"docstring": "\"\"\"Like iteritems(), but with all lowercase keys.\"\"\"\n", "func_signal": "def lower_items(self):\n", "code": "return (\n    (lowerkey, keyval[1])\n    for (lowerkey, keyval)\n    in self._store.items()\n)", "path": "plotly_yun\\Linino\\requests\\structures.py", "repo_name": "plotly/arduino-api", "stars": 208, "license": "None", "language": "python", "size": 6118}
{"docstring": "\"\"\"\nMake a request using :meth:`urlopen` with the appropriate encoding of\n``fields`` based on the ``method`` used.\n\nThis is a convenience method that requires the least amount of manual\neffort. It can be used in most situations, while still having the option\nto drop down to more specific methods when necessary, such as\n:meth:`request_encode_url`, :meth:`request_encode_body`,\nor even the lowest level :meth:`urlopen`.\n\"\"\"\n", "func_signal": "def request(self, method, url, fields=None, headers=None, **urlopen_kw):\n", "code": "method = method.upper()\n\nif method in self._encode_url_methods:\n    return self.request_encode_url(method, url, fields=fields,\n                                    headers=headers,\n                                    **urlopen_kw)\nelse:\n    return self.request_encode_body(method, url, fields=fields,\n                                     headers=headers,\n                                     **urlopen_kw)", "path": "plotly_yun\\Linino\\requests\\packages\\urllib3\\request.py", "repo_name": "plotly/arduino-api", "stars": 208, "license": "None", "language": "python", "size": 6118}
{"docstring": "\"\"\"Obtain the url to use when making the final request.\n\nIf the message is being sent through a HTTP proxy, the full URL has to\nbe used. Otherwise, we should only use the path portion of the URL.\n\nThis should not be called from user code, and is only exposed for use\nwhen subclassing the\n:class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.\n\n:param request: The :class:`PreparedRequest <PreparedRequest>` being sent.\n:param proxies: A dictionary of schemes to proxy URLs.\n\"\"\"\n", "func_signal": "def request_url(self, request, proxies):\n", "code": "proxies = proxies or {}\nscheme = urlparse(request.url).scheme\nproxy = proxies.get(scheme)\n\nif proxy and scheme != 'https':\n    url, _ = urldefrag(request.url)\nelse:\n    url = request.path_url\n\nreturn url", "path": "plotly_yun\\Linino\\requests\\adapters.py", "repo_name": "plotly/arduino-api", "stars": 208, "license": "None", "language": "python", "size": 6118}
{"docstring": "\"\"\"Builds a :class:`Response <requests.Response>` object from a urllib3\nresponse. This should not be called from user code, and is only exposed\nfor use when subclassing the\n:class:`HTTPAdapter <requests.adapters.HTTPAdapter>`\n\n:param req: The :class:`PreparedRequest <PreparedRequest>` used to generate the response.\n:param resp: The urllib3 response object.\n\"\"\"\n", "func_signal": "def build_response(self, req, resp):\n", "code": "response = Response()\n\n# Fallback to None if there's no status_code, for whatever reason.\nresponse.status_code = getattr(resp, 'status', None)\n\n# Make headers case-insensitive.\nresponse.headers = CaseInsensitiveDict(getattr(resp, 'headers', {}))\n\n# Set encoding.\nresponse.encoding = get_encoding_from_headers(response.headers)\nresponse.raw = resp\nresponse.reason = response.raw.reason\n\nif isinstance(req.url, bytes):\n    response.url = req.url.decode('utf-8')\nelse:\n    response.url = req.url\n\n# Add new cookies from the server.\nextract_cookies_to_jar(response.cookies, req, resp)\n\n# Give the Response some context.\nresponse.request = req\nresponse.connection = self\n\nreturn response", "path": "plotly_yun\\Linino\\requests\\adapters.py", "repo_name": "plotly/arduino-api", "stars": 208, "license": "None", "language": "python", "size": 6118}
{"docstring": "\"\"\"Absolute path including the query string.\"\"\"\n", "func_signal": "def request_uri(self):\n", "code": "uri = self.path or '/'\n\nif self.query is not None:\n    uri += '?' + self.query\n\nreturn uri", "path": "plotly_yun\\Linino\\requests\\packages\\urllib3\\util.py", "repo_name": "plotly/arduino-api", "stars": 208, "license": "None", "language": "python", "size": 6118}
{"docstring": "# Can't handle by adding 'proxy_manager' to self.__attrs__ because\n# because self.poolmanager uses a lambda function, which isn't pickleable.\n", "func_signal": "def __setstate__(self, state):\n", "code": "self.proxy_manager = {}\nself.config = {}\n\nfor attr, value in state.items():\n    setattr(self, attr, value)\n\nself.init_poolmanager(self._pool_connections, self._pool_maxsize,\n                      block=self._pool_block)", "path": "plotly_yun\\Linino\\requests\\adapters.py", "repo_name": "plotly/arduino-api", "stars": 208, "license": "None", "language": "python", "size": 6118}
{"docstring": "\"\"\" Start the timeout clock, used during a connect() attempt\n\n:raises urllib3.exceptions.TimeoutStateError: if you attempt\n    to start a timer that has been started already.\n\"\"\"\n", "func_signal": "def start_connect(self):\n", "code": "if self._start_connect is not None:\n    raise TimeoutStateError(\"Timeout timer has already been started.\")\nself._start_connect = current_time()\nreturn self._start_connect", "path": "plotly_yun\\Linino\\requests\\packages\\urllib3\\util.py", "repo_name": "plotly/arduino-api", "stars": 208, "license": "None", "language": "python", "size": 6118}
{"docstring": "\"\"\"\nlike resolve_cert_reqs\n\"\"\"\n", "func_signal": "def resolve_ssl_version(candidate):\n", "code": "if candidate is None:\n    return PROTOCOL_SSLv23\n\nif isinstance(candidate, str):\n    res = getattr(ssl, candidate, None)\n    if res is None:\n        res = getattr(ssl, 'PROTOCOL_' + candidate)\n    return res\n\nreturn candidate", "path": "plotly_yun\\Linino\\requests\\packages\\urllib3\\util.py", "repo_name": "plotly/arduino-api", "stars": 208, "license": "None", "language": "python", "size": 6118}
{"docstring": "\"\"\" Get the value to use when setting a connection timeout.\n\nThis will be a positive float or integer, the value None\n(never timeout), or the default system timeout.\n\n:return: the connect timeout\n:rtype: int, float, :attr:`Timeout.DEFAULT_TIMEOUT` or None\n\"\"\"\n", "func_signal": "def connect_timeout(self):\n", "code": "if self.total is None:\n    return self._connect\n\nif self._connect is None or self._connect is self.DEFAULT_TIMEOUT:\n    return self.total\n\nreturn min(self._connect, self.total)", "path": "plotly_yun\\Linino\\requests\\packages\\urllib3\\util.py", "repo_name": "plotly/arduino-api", "stars": 208, "license": "None", "language": "python", "size": 6118}
{"docstring": "\"\"\"Returns a urllib3 connection for the given URL. This should not be\ncalled from user code, and is only exposed for use when subclassing the\n:class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.\n\n:param url: The URL to connect to.\n:param proxies: (optional) A Requests-style dictionary of proxies used on this request.\n\"\"\"\n", "func_signal": "def get_connection(self, url, proxies=None):\n", "code": "proxies = proxies or {}\nproxy = proxies.get(urlparse(url.lower()).scheme)\n\nif proxy:\n    except_on_missing_scheme(proxy)\n    proxy_headers = self.proxy_headers(proxy)\n\n    if not proxy in self.proxy_manager:\n        self.proxy_manager[proxy] = proxy_from_url(\n                                        proxy,\n                                        proxy_headers=proxy_headers,\n                                        num_pools=self._pool_connections,\n                                        maxsize=self._pool_maxsize,\n                                        block=self._pool_block)\n\n    conn = self.proxy_manager[proxy].connection_from_url(url)\nelse:\n    # Only scheme should be lower case\n    parsed = urlparse(url)\n    url = parsed.geturl()\n    conn = self.poolmanager.connection_from_url(url)\n\nreturn conn", "path": "plotly_yun\\Linino\\requests\\adapters.py", "repo_name": "plotly/arduino-api", "stars": 208, "license": "None", "language": "python", "size": 6118}
{"docstring": "\"\"\" Initializes HTTPError with optional `response` object. \"\"\"\n", "func_signal": "def __init__(self, *args, **kwargs):\n", "code": "self.response = kwargs.pop('response', None)\nsuper(HTTPError, self).__init__(*args, **kwargs)", "path": "plotly_yun\\Linino\\requests\\exceptions.py", "repo_name": "plotly/arduino-api", "stars": 208, "license": "None", "language": "python", "size": 6118}
{"docstring": "\"\"\"Sends PreparedRequest object. Returns Response object.\n\n:param request: The :class:`PreparedRequest <PreparedRequest>` being sent.\n:param stream: (optional) Whether to stream the request content.\n:param timeout: (optional) The timeout on the request.\n:param verify: (optional) Whether to verify SSL certificates.\n:param cert: (optional) Any user-provided SSL certificate to be trusted.\n:param proxies: (optional) The proxies dictionary to apply to the request.\n\"\"\"\n\n", "func_signal": "def send(self, request, stream=False, timeout=None, verify=True, cert=None, proxies=None):\n", "code": "conn = self.get_connection(request.url, proxies)\n\nself.cert_verify(conn, request.url, verify, cert)\nurl = self.request_url(request, proxies)\nself.add_headers(request)\n\nchunked = not (request.body is None or 'Content-Length' in request.headers)\n\nif stream:\n    timeout = TimeoutSauce(connect=timeout)\nelse:\n    timeout = TimeoutSauce(connect=timeout, read=timeout)\n\ntry:\n    if not chunked:\n        resp = conn.urlopen(\n            method=request.method,\n            url=url,\n            body=request.body,\n            headers=request.headers,\n            redirect=False,\n            assert_same_host=False,\n            preload_content=False,\n            decode_content=False,\n            retries=self.max_retries,\n            timeout=timeout\n        )\n\n    # Send the request.\n    else:\n        if hasattr(conn, 'proxy_pool'):\n            conn = conn.proxy_pool\n\n        low_conn = conn._get_conn(timeout=timeout)\n\n        try:\n            low_conn.putrequest(request.method,\n                                url,\n                                skip_accept_encoding=True)\n\n            for header, value in request.headers.items():\n                low_conn.putheader(header, value)\n\n            low_conn.endheaders()\n\n            for i in request.body:\n                low_conn.send(hex(len(i))[2:].encode('utf-8'))\n                low_conn.send(b'\\r\\n')\n                low_conn.send(i)\n                low_conn.send(b'\\r\\n')\n            low_conn.send(b'0\\r\\n\\r\\n')\n\n            r = low_conn.getresponse()\n            resp = HTTPResponse.from_httplib(\n                r,\n                pool=conn,\n                connection=low_conn,\n                preload_content=False,\n                decode_content=False\n            )\n        except:\n            # If we hit any problems here, clean up the connection.\n            # Then, reraise so that we can handle the actual exception.\n            low_conn.close()\n            raise\n        else:\n            # All is well, return the connection to the pool.\n            conn._put_conn(low_conn)\n\nexcept socket.error as sockerr:\n    raise ConnectionError(sockerr)\n\nexcept MaxRetryError as e:\n    raise ConnectionError(e)\n\nexcept _ProxyError as e:\n    raise ProxyError(e)\n\nexcept (_SSLError, _HTTPError) as e:\n    if isinstance(e, _SSLError):\n        raise SSLError(e)\n    elif isinstance(e, TimeoutError):\n        raise Timeout(e)\n    else:\n        raise\n\nr = self.build_response(request, resp)\n\nif not stream:\n    r.content\n\nreturn r", "path": "plotly_yun\\Linino\\requests\\adapters.py", "repo_name": "plotly/arduino-api", "stars": 208, "license": "None", "language": "python", "size": 6118}
{"docstring": "\"\"\"\nChecks whether a given file-like object is closed.\n\n:param obj:\n    The file-like object to check.\n\"\"\"\n", "func_signal": "def is_fp_closed(obj):\n", "code": "if hasattr(obj, 'fp'):\n    # Object is a container for another file-like object that gets released\n    # on exhaustion (e.g. HTTPResponse)\n    return obj.fp is None\n\nreturn obj.closed", "path": "plotly_yun\\Linino\\requests\\packages\\urllib3\\util.py", "repo_name": "plotly/arduino-api", "stars": 208, "license": "None", "language": "python", "size": 6118}
{"docstring": "\"\"\"\nDeprecated. Use :func:`.parse_url` instead.\n\"\"\"\n", "func_signal": "def get_host(url):\n", "code": "p = parse_url(url)\nreturn p.scheme or 'http', p.hostname, p.port", "path": "plotly_yun\\Linino\\requests\\packages\\urllib3\\util.py", "repo_name": "plotly/arduino-api", "stars": 208, "license": "None", "language": "python", "size": 6118}
{"docstring": "\"\"\" Get the value for the read timeout.\n\nThis assumes some time has elapsed in the connection timeout and\ncomputes the read timeout appropriately.\n\nIf self.total is set, the read timeout is dependent on the amount of\ntime taken by the connect timeout. If the connection time has not been\nestablished, a :exc:`~urllib3.exceptions.TimeoutStateError` will be\nraised.\n\n:return: the value to use for the read timeout\n:rtype: int, float, :attr:`Timeout.DEFAULT_TIMEOUT` or None\n:raises urllib3.exceptions.TimeoutStateError: If :meth:`start_connect`\n    has not yet been called on this object.\n\"\"\"\n", "func_signal": "def read_timeout(self):\n", "code": "if (self.total is not None and\n    self.total is not self.DEFAULT_TIMEOUT and\n    self._read is not None and\n    self._read is not self.DEFAULT_TIMEOUT):\n    # in case the connect timeout has not yet been established.\n    if self._start_connect is None:\n        return self._read\n    return max(0, min(self.total - self.get_connect_duration(),\n                      self._read))\nelif self.total is not None and self.total is not self.DEFAULT_TIMEOUT:\n    return max(0, self.total - self.get_connect_duration())\nelse:\n    return self._read", "path": "plotly_yun\\Linino\\requests\\packages\\urllib3\\util.py", "repo_name": "plotly/arduino-api", "stars": 208, "license": "None", "language": "python", "size": 6118}
{"docstring": "\"\"\"\nChecks if given fingerprint matches the supplied certificate.\n\n:param cert:\n    Certificate as bytes object.\n:param fingerprint:\n    Fingerprint as string of hexdigits, can be interspersed by colons.\n\"\"\"\n\n# Maps the length of a digest to a possible hash function producing\n# this digest.\n", "func_signal": "def assert_fingerprint(cert, fingerprint):\n", "code": "hashfunc_map = {\n    16: md5,\n    20: sha1\n}\n\nfingerprint = fingerprint.replace(':', '').lower()\n\ndigest_length, rest = divmod(len(fingerprint), 2)\n\nif rest or digest_length not in hashfunc_map:\n    raise SSLError('Fingerprint is of invalid length.')\n\n# We need encode() here for py32; works on py2 and p33.\nfingerprint_bytes = unhexlify(fingerprint.encode())\n\nhashfunc = hashfunc_map[digest_length]\n\ncert_digest = hashfunc(cert).digest()\n\nif not cert_digest == fingerprint_bytes:\n    raise SSLError('Fingerprints did not match. Expected \"{0}\", got \"{1}\".'\n                   .format(hexlify(fingerprint_bytes),\n                           hexlify(cert_digest)))", "path": "plotly_yun\\Linino\\requests\\packages\\urllib3\\util.py", "repo_name": "plotly/arduino-api", "stars": 208, "license": "None", "language": "python", "size": 6118}
{"docstring": "\"\"\"\nResolves the argument to a numeric constant, which can be passed to\nthe wrap_socket function/method from the ssl module.\nDefaults to :data:`ssl.CERT_NONE`.\nIf given a string it is assumed to be the name of the constant in the\n:mod:`ssl` module or its abbrevation.\n(So you can specify `REQUIRED` instead of `CERT_REQUIRED`.\nIf it's neither `None` nor a string we assume it is already the numeric\nconstant which can directly be passed to wrap_socket.\n\"\"\"\n", "func_signal": "def resolve_cert_reqs(candidate):\n", "code": "if candidate is None:\n    return CERT_NONE\n\nif isinstance(candidate, str):\n    res = getattr(ssl, candidate, None)\n    if res is None:\n        res = getattr(ssl, 'CERT_' + candidate)\n    return res\n\nreturn candidate", "path": "plotly_yun\\Linino\\requests\\packages\\urllib3\\util.py", "repo_name": "plotly/arduino-api", "stars": 208, "license": "None", "language": "python", "size": 6118}
{"docstring": "'''Accept an input, and write a MIDI-compatible variable length stream\n\nThe MIDI format is a little strange, and makes use of so-called variable\nlength quantities. These quantities are a stream of bytes. If the most\nsignificant bit is 1, then more bytes follow. If it is zero, then the\nbyte in question is the last in the stream\n'''\n", "func_signal": "def writeVarLength(i):\n", "code": "input = int(i+0.5)\noutput = [0,0,0,0]\nreversed = [0,0,0,0]\ncount = 0\nresult = input & 0x7F\noutput[count] = result\ncount = count + 1\ninput = input >> 7\nwhile input > 0:\n    result = input & 0x7F \n    result = result | 0x80\n    output[count] = result\n    count = count + 1\n    input = input >> 7  \n\nreversed[0] = output[3]\nreversed[1] = output[2]\nreversed[2] = output[1]\nreversed[3] = output[0]\nreturn reversed[4-count:4]", "path": "midiutil\\MidiFile3.py", "repo_name": "cirlabs/miditime", "stars": 150, "license": "None", "language": "python", "size": 82}
{"docstring": "'''\nRemove duplicates from the eventList.\n\nThis function will remove duplicates from the eventList. This is necessary\nbecause we the MIDI event stream can become confused otherwise.\n'''\n\n# For this algorithm to work, the events in the eventList must be hashable \n# (that is, they must have a __hash__() and __eq__() function defined).\n\n", "func_signal": "def removeDuplicates(self):\n", "code": "tempDict = {}\nfor item in self.eventList:\n    tempDict[item] = 1\n    \nself.eventList = list(tempDict.keys())\n\n# Sort on type, them on time. Necessary because keys() has no requirement to return\n# things in any order.\n\nself.eventList.sort(key=lambda x: (x.type))\nself.eventList.sort(key=lambda x: (x.time)) #A bit of a hack.", "path": "midiutil\\MidiFile3.py", "repo_name": "cirlabs/miditime", "stars": 150, "license": "None", "language": "python", "size": 82}
{"docstring": "'''\nWrite the MIDI File.\n\nUse:\n    MyMIDI.writeFile(filehandle)\n\nArguments:\n    filehandle: a file handle that has been opened for binary writing.\n'''\n\n", "func_signal": "def writeFile(self,fileHandle):\n", "code": "self.header.writeFile(fileHandle)\n\n#Close the tracks and have them create the MIDI event data structures.\nself.close()\n\n#Write the MIDI Events to file.\nfor i in range(0,self.numTracks):\n    self.tracks[i].writeTrack(fileHandle)", "path": "midiutil\\MidiFile3.py", "repo_name": "cirlabs/miditime", "stars": 150, "license": "None", "language": "python", "size": 82}
{"docstring": "'''\nEquality operator for Generic Events and derived classes.\n\nIn the processing of the event list, we have need to remove duplicates. To do this\nwe rely on the fact that the classes are hashable, and must therefore have an \nequality operator (__hash__() and __eq__() must both be defined).\n\nThis is the most embarrassing portion of the code, and anyone who knows about OO\nprogramming would find this almost unbelievable. Here we have a base class that\nknows specifics about derived classes, thus breaking the very spirit of \nOO programming.\n\nI suppose I should go back and restructure the code, perhaps removing the derived\nclasses altogether. At some point perhaps I will.\n'''\n", "func_signal": "def __eq__(self, other):\n", "code": "if self.time != other.time or self.type != other.type:\n    return False\n    \n# What follows is code that encodes the concept of equality for each derived \n# class. Believe it f you dare.\n\nif self.type == 'note':\n    if self.pitch != other.pitch or self.channel != other.channel:\n        return False\nif self.type == 'tempo':\n    if self.tempo != other.tempo:\n        return False\nif self.type == 'programChange':\n    if self.programNumber != other.programNumber or self.channel != other.channel:\n        return False\nif self.type == 'trackName':\n    if self.trackName != other.trackName:\n        return False\nif self.type == 'controllerEvent':\n    if self.parameter1 != other.parameter1 or \\\n        self.channel != other.channel or \\\n        self.eventType != other.eventType:\n        return False\n        \nif self.type == 'SysEx':\n    if self.manID != other.manID:\n        return False\n        \nif self.type == 'UniversalSysEx':\n    if self.code != other.code or\\\n        self.subcode != other.subcode or \\\n        self.sysExChannel != other.sysExChannel:\n        return False\n        \nreturn True", "path": "midiutil\\MidiFile3.py", "repo_name": "cirlabs/miditime", "stars": 150, "license": "None", "language": "python", "size": 82}
{"docstring": "''' Initialize the data structures\n'''\n", "func_signal": "def __init__(self,numTracks):\n", "code": "self.headerString = struct.pack('cccc',b'M',b'T',b'h',b'd')\nself.headerSize = struct.pack('>L',6)\n# Format 1 = multi-track file\nself.format = struct.pack('>H',1)\nself.numTracks = struct.pack('>H',numTracks)\nself.ticksPerBeat = struct.pack('>H',TICKSPERBEAT)", "path": "midiutil\\MidiFile3.py", "repo_name": "cirlabs/miditime", "stars": 150, "license": "None", "language": "python", "size": 82}
{"docstring": "'''The reverse of frequencyTransform. Given a byte stream, return a frequency.\n'''\n", "func_signal": "def returnFrequency(freqBytes):\n", "code": "resolution = 16384.0\nbaseFrequency = 440 * pow(2.0, (float(freqBytes[0]-69.0)/12.0))\nfrac = (float((int(freqBytes[1]) << 7) + int(freqBytes[2])) * 100.0) / resolution\nfrequency = baseFrequency * pow(2.0, frac/1200.0)\nreturn frequency", "path": "midiutil\\MidiFile3.py", "repo_name": "cirlabs/miditime", "stars": 150, "license": "None", "language": "python", "size": 82}
{"docstring": "'''\nInitialize the class\n'''\n", "func_signal": "def __init__(self, numTracks, removeDuplicates=True,  deinterleave=True):\n", "code": "self.header = MIDIHeader(numTracks)\n\nself.tracks = list()\nself.numTracks = numTracks\nself.closed = False\n\nfor i in range(0,numTracks):\n    self.tracks.append(MIDITrack(removeDuplicates,  deinterleave))", "path": "midiutil\\MidiFile3.py", "repo_name": "cirlabs/miditime", "stars": 150, "license": "None", "language": "python", "size": 82}
{"docstring": "'''Called to close a track before writing\n\nThis function should be called to \"close a track,\" that is to\nprepare the actual data stream for writing. Duplicate events are\nremoved from the eventList, and the MIDIEventList is created.\n\nCalled by the parent MIDIFile object.\n'''\n\n", "func_signal": "def closeTrack(self):\n", "code": "if self.closed == True:\n    return\nself.closed = True\n\nif self.remdep:\n    self.removeDuplicates()\n    \n\nself.processEventList()", "path": "midiutil\\MidiFile3.py", "repo_name": "cirlabs/miditime", "stars": 150, "license": "None", "language": "python", "size": 82}
{"docstring": "'''\nWrite the events in MIDIEvents to the MIDI stream.\n'''\n", "func_signal": "def writeEventsToStream(self):\n", "code": "preciseTime = 0.0                   # Actual time of event, ignoring round-off\nactualTime = 0.0                    # Time as written to midi stream, include round-off\nfor event in self.MIDIEventList:\n\n    preciseTime = preciseTime + event.time\n\n    # Convert the time to variable length and back, to see how much\n    # error is introduced\n\n    testBuffer = bytes()\n    varTime = writeVarLength(event.time)\n    for timeByte in varTime:\n        testBuffer = testBuffer + struct.pack('>B',timeByte)\n    (roundedVal,discard) = readVarLength(0,testBuffer)\n    roundedTime = actualTime + roundedVal\n    # print \"Rounded, Precise: %15.10f %15.10f\" % (roundedTime, preciseTime)\n\n    # Calculate the delta between the two and apply it to the event time.\n\n    delta = preciseTime - roundedTime\n    event.time = event.time + delta\n\n    # Now update the actualTime value, using the updated event time.\n\n    testBuffer = bytes()\n    varTime = writeVarLength(event.time)\n    for timeByte in varTime:\n        testBuffer = testBuffer + struct.pack('>B',timeByte)\n    (roundedVal,discard) = readVarLength(0,testBuffer)\n    actualTime = actualTime + roundedVal\n\nfor event in self.MIDIEventList:\n    if event.type == \"NoteOn\":\n        code = 0x9 << 4 | event.channel\n        varTime = writeVarLength(event.time)\n        for timeByte in varTime:\n            self.MIDIdata = self.MIDIdata + struct.pack('>B',timeByte)\n        self.MIDIdata = self.MIDIdata + struct.pack('>B',code)\n        self.MIDIdata = self.MIDIdata + struct.pack('>B',event.pitch)\n        self.MIDIdata = self.MIDIdata + struct.pack('>B',event.volume)\n    elif event.type == \"NoteOff\":\n        code = 0x8 << 4 | event.channel\n        varTime = writeVarLength(event.time)\n        for timeByte in varTime:\n            self.MIDIdata = self.MIDIdata + struct.pack('>B',timeByte)\n        self.MIDIdata = self.MIDIdata + struct.pack('>B',code)\n        self.MIDIdata = self.MIDIdata + struct.pack('>B',event.pitch)\n        self.MIDIdata = self.MIDIdata + struct.pack('>B',event.volume)\n    elif event.type == \"Tempo\":\n        code = 0xFF\n        subcode = 0x51\n        fourbite = struct.pack('>L', event.tempo)\n        threebite = fourbite[1:4]       # Just discard the MSB\n        varTime = writeVarLength(event.time)\n        for timeByte in varTime:\n            self.MIDIdata = self.MIDIdata + struct.pack('>B',timeByte)\n        self.MIDIdata = self.MIDIdata + struct.pack('>B',code)\n        self.MIDIdata = self.MIDIdata + struct.pack('>B',subcode)\n        self.MIDIdata = self.MIDIdata + struct.pack('>B', 0x03) # Data length: 3\n        self.MIDIdata = self.MIDIdata + threebite\n    elif event.type == 'ProgramChange':\n        code = 0xC << 4 | event.channel\n        varTime = writeVarLength(event.time)\n        for timeByte in varTime:\n            self.MIDIdata = self.MIDIdata + struct.pack('>B',timeByte)\n        self.MIDIdata = self.MIDIdata + struct.pack('>B',code)\n        self.MIDIdata = self.MIDIdata + struct.pack('>B',event.programNumber)\n    elif event.type == 'TrackName':\n        varTime = writeVarLength(event.time)\n        for timeByte in varTime:\n            self.MIDIdata = self.MIDIdata + struct.pack('>B',timeByte)\n        self.MIDIdata = self.MIDIdata + struct.pack('B',0xFF) # Meta-event\n        self.MIDIdata = self.MIDIdata + struct.pack('B',0X03) # Event Type\n        dataLength = len(event.trackName)\n        dataLenghtVar = writeVarLength(dataLength)\n        for i in range(0,len(dataLenghtVar)):\n            self.MIDIdata = self.MIDIdata + struct.pack(\"b\",dataLenghtVar[i])\n        self.MIDIdata = self.MIDIdata + event.trackName.encode()\n    elif event.type == \"ControllerEvent\":\n        code = 0xB << 4 | event.channel\n        varTime = writeVarLength(event.time)\n        for timeByte in varTime:\n            self.MIDIdata = self.MIDIdata + struct.pack('>B',timeByte)\n        self.MIDIdata = self.MIDIdata + struct.pack('>B',code)\n        self.MIDIdata = self.MIDIdata + struct.pack('>B',event.eventType)\n        self.MIDIdata = self.MIDIdata + struct.pack('>B',event.paramerter1)\n    elif event.type == \"SysEx\":\n        code = 0xF0\n        varTime = writeVarLength(event.time)\n        for timeByte in varTime:\n            self.MIDIdata = self.MIDIdata + struct.pack('>B',timeByte)\n        self.MIDIdata = self.MIDIdata + struct.pack('>B', code)\n        \n        payloadLength = writeVarLength(len(event.payload)+2)\n        for lenByte in payloadLength:\n            self.MIDIdata = self.MIDIdata + struct.pack('>B',lenByte)\n            \n        self.MIDIdata = self.MIDIdata + struct.pack('>B', event.manID)\n        self.MIDIdata = self.MIDIdata + event.payload\n        self.MIDIdata = self.MIDIdata + struct.pack('>B',0xF7)\n    elif event.type == \"UniversalSysEx\":\n        code = 0xF0\n        varTime = writeVarLength(event.time)\n        for timeByte in varTime:\n            self.MIDIdata = self.MIDIdata + struct.pack('>B',timeByte)\n        self.MIDIdata = self.MIDIdata + struct.pack('>B', code)\n        \n        # Do we need to add a length?\n        payloadLength = writeVarLength(len(event.payload)+5)\n        for lenByte in payloadLength:\n            self.MIDIdata = self.MIDIdata + struct.pack('>B',lenByte)\n        \n        if event.realTime :\n            self.MIDIdata = self.MIDIdata + struct.pack('>B', 0x7F)\n        else:\n            self.MIDIdata = self.MIDIdata + struct.pack('>B', 0x7E)\n            \n        self.MIDIdata = self.MIDIdata + struct.pack('>B', event.sysExChannel)\n        self.MIDIdata = self.MIDIdata + struct.pack('>B', event.code)\n        self.MIDIdata = self.MIDIdata + struct.pack('>B', event.subcode)\n        self.MIDIdata = self.MIDIdata + event.payload\n        self.MIDIdata = self.MIDIdata + struct.pack('>B',0xF7)", "path": "midiutil\\MidiFile3.py", "repo_name": "cirlabs/miditime", "stars": 150, "license": "None", "language": "python", "size": 82}
{"docstring": "''' Sorting function for events.'''\n", "func_signal": "def __cmp__(self, other):\n", "code": "if self.time < other.time:\n    return -1\nelif self.time > other.time:\n    return 1\nelse:\n    if self.ord < other.ord:\n        return -1\n    elif self.ord > other.ord:\n        return 1\n    else:\n        return 0", "path": "midiutil\\MidiFile3.py", "repo_name": "cirlabs/miditime", "stars": 150, "license": "None", "language": "python", "size": 82}
{"docstring": "'''A function to read a MIDI variable length variable.\n\nIt returns a tuple of the value read and the number of bytes processed. The\ninput is an offset into the buffer, and the buffer itself.\n'''\n", "func_signal": "def readVarLength(offset, buffer):\n", "code": "toffset = offset\noutput = 0\nbytesRead = 0\nwhile True:\n    output = output << 7\n    byte = struct.unpack_from('>B',buffer,toffset)[0]\n    toffset = toffset + 1\n    bytesRead = bytesRead + 1\n    output = output + (byte & 127)\n    if (byte & 128) == 0:\n        break\nreturn (output, bytesRead)", "path": "midiutil\\MidiFile3.py", "repo_name": "cirlabs/miditime", "stars": 150, "license": "None", "language": "python", "size": 82}
{"docstring": "'''Correct Interleaved notes.\n\nBecause we are writing multiple notes in no particular order, we\ncan have notes which are interleaved with respect to their start\nand stop times. This method will correct that. It expects that the\nMIDIEventList has been time-ordered.\n'''\n\n", "func_signal": "def deInterleaveNotes(self):\n", "code": "tempEventList = []\nstack = {}\n\nfor event in self.MIDIEventList:\n    \n    if event.type == 'NoteOn':\n        if str(event.pitch)+str(event.channel) in stack:\n            stack[str(event.pitch)+str(event.channel)].append(event.time)\n        else:\n            stack[str(event.pitch)+str(event.channel)] = [event.time]\n        tempEventList.append(event)\n    elif event.type == 'NoteOff':\n        if len(stack[str(event.pitch)+str(event.channel)]) > 1:\n            event.time = stack[str(event.pitch)+str(event.channel)].pop()\n            tempEventList.append(event)\n        else:\n            stack[str(event.pitch)+str(event.channel)].pop()\n            tempEventList.append(event)\n    else:\n        tempEventList.append(event)\n            \nself.MIDIEventList = tempEventList\n\n# A little trickery here. We want to make sure that NoteOff events appear \n# before NoteOn events, so we'll do two sorts -- on on type, one on time. \n# This may have to be revisited, as it makes assumptions about how \n# the internal sort works, and is in essence creating a sort on a primary \n# and secondary key.\n\nself.MIDIEventList.sort(key=lambda x: (x.type))\nself.MIDIEventList.sort(key=lambda x: (x.time))", "path": "midiutil\\MidiFile3.py", "repo_name": "cirlabs/miditime", "stars": 150, "license": "None", "language": "python", "size": 82}
{"docstring": "'''\nAdjust Times to be relative, and zero-origined\n'''\n\n", "func_signal": "def adjustTime(self,origin):\n", "code": "if len(self.MIDIEventList) == 0:\n    return\ntempEventList = []\n    \nrunningTime = 0 \n\nfor event in self.MIDIEventList:\n    adjustedTime = event.time - origin\n    event.time = adjustedTime - runningTime\n    runningTime = adjustedTime\n    tempEventList.append(event)\n    \nself.MIDIEventList = tempEventList", "path": "midiutil\\MidiFile3.py", "repo_name": "cirlabs/miditime", "stars": 150, "license": "None", "language": "python", "size": 82}
{"docstring": "'''\nAdd a controller event.\n'''\n\n", "func_signal": "def addControllerEvent(self,channel,time,eventType, paramerter1):\n", "code": "self.eventList.append(MIDITrack.ControllerEvent(channel,time,eventType, \\\n                                     paramerter1))", "path": "midiutil\\MidiFile3.py", "repo_name": "cirlabs/miditime", "stars": 150, "license": "None", "language": "python", "size": 82}
{"docstring": "'''\nReturn a hash code for the object.\n\nThis is needed for the removal of duplicate objects from the event list. The only\nreal requirement for the algorithm is that the hash of equal objects must be equal.\nThere is probably great opportunity for improvements in the hashing function.\n'''\n# Robert Jenkin's 32 bit hash.\n", "func_signal": "def __hash__(self):\n", "code": "a = int(self.time)\na = (a+0x7ed55d16) + (a<<12)\na = (a^0xc761c23c) ^ (a>>19)\na = (a+0x165667b1) + (a<<5)\na = (a+0xd3a2646c) ^ (a<<9)\na = (a+0xfd7046c5) + (a<<3)\na = (a^0xb55a4f09) ^ (a>>16)\nreturn a", "path": "midiutil\\MidiFile3.py", "repo_name": "cirlabs/miditime", "stars": 150, "license": "None", "language": "python", "size": 82}
{"docstring": "'''Compare two notes for equality.\n'''\n", "func_signal": "def compare(self, other):\n", "code": "if self.pitch == other.pitch and \\\n    self.time == other.time and \\\n    self.duration == other.duration and \\\n    self.volume == other.volume and \\\n    self.type == other.type and \\\n    self.channel == other.channel:\n        return True\nelse:\n        return False", "path": "midiutil\\MidiFile3.py", "repo_name": "cirlabs/miditime", "stars": 150, "license": "None", "language": "python", "size": 82}
{"docstring": "'''Close the MIDIFile for further writing.\n\nTo close the File for events, we must close the tracks, adjust the time to be\nzero-origined, and have the tracks write to their MIDI Stream data structure.\n'''\n\n", "func_signal": "def close(self):\n", "code": "if self.closed == True:\n    return\n        \nfor i in range(0,self.numTracks):\n    self.tracks[i].closeTrack()\n    # We want things like program changes to come before notes when they are at the\n    # same time, so we sort the MIDI events by their ordinality\n    self.tracks[i].MIDIEventList.sort()\n    \norigin = self.findOrigin()\n\nfor i in range(0,self.numTracks):\n    self.tracks[i].adjustTime(origin)\n    self.tracks[i].writeMIDIStream()\n    \nself.closed = True", "path": "midiutil\\MidiFile3.py", "repo_name": "cirlabs/miditime", "stars": 150, "license": "None", "language": "python", "size": 82}
{"docstring": "\"\"\"Shift tracks to be zero-origined, or origined at offset.\n\nNote that the shifting of the time in the tracks uses the MIDIEventList -- in other\nwords it is assumed to be called in the stage where the MIDIEventList has been\ncreated. This function, however, it meant to operate on the eventList itself.\n\"\"\"\n", "func_signal": "def shiftTracks(self,  offset=0):\n", "code": "origin = 1000000 # A little silly, but we'll assume big enough\n\nfor track in self.tracks:\n        if len(track.eventList) > 0:\n            for event in track.eventList:\n                if event.time < origin:\n                    origin = event.time\n\nfor track in self.tracks:\n    tempEventList = []\n    #runningTime = 0 \n\n    for event in track.eventList:\n        adjustedTime = event.time - origin\n        #event.time = adjustedTime - runningTime + offset\n        event.time = adjustedTime + offset\n        #runningTime = adjustedTime\n        tempEventList.append(event)\n    \n    track.eventList = tempEventList", "path": "midiutil\\MidiFile3.py", "repo_name": "cirlabs/miditime", "stars": 150, "license": "None", "language": "python", "size": 82}
{"docstring": "'''\nProcess the event list, creating a MIDIEventList\n\nFor each item in the event list, one or more events in the MIDIEvent\nlist are created.\n'''\n\n# Loop over all items in the eventList\n\n", "func_signal": "def processEventList(self):\n", "code": "for thing in self.eventList:\n    if thing.type == 'note':\n        event = MIDIEvent()\n        event.type = \"NoteOn\"\n        event.time = thing.time * TICKSPERBEAT\n        event.pitch = thing.pitch\n        event.volume = thing.volume\n        event.channel = thing.channel\n        event.ord = 3\n        self.MIDIEventList.append(event)\n\n        event = MIDIEvent()\n        event.type = \"NoteOff\"\n        event.time = (thing.time + thing.duration) * TICKSPERBEAT\n        event.pitch = thing.pitch\n        event.volume = thing.volume\n        event.channel = thing.channel\n        event.ord = 2\n        self.MIDIEventList.append(event)\n\n    elif thing.type == 'tempo':\n        event = MIDIEvent()\n        event.type = \"Tempo\"\n        event.time = thing.time * TICKSPERBEAT\n        event.tempo = thing.tempo\n        event.ord = 3\n        self.MIDIEventList.append(event)\n\n    elif thing.type == 'programChange':\n        event = MIDIEvent()\n        event.type = \"ProgramChange\"\n        event.time = thing.time * TICKSPERBEAT\n        event.programNumber = thing.programNumber\n        event.channel = thing.channel\n        event.ord = 1\n        self.MIDIEventList.append(event)\n\n    elif thing.type == 'trackName':\n        event = MIDIEvent()\n        event.type = \"TrackName\"\n        event.time = thing.time * TICKSPERBEAT\n        event.trackName = thing.trackName\n        event.ord = 0\n        self.MIDIEventList.append(event)\n\n    elif thing.type == 'controllerEvent':\n        event = MIDIEvent()\n        event.type = \"ControllerEvent\"\n        event.time = thing.time * TICKSPERBEAT\n        event.eventType = thing.eventType\n        event.channel = thing.channel\n        event.paramerter1 = thing.parameter1\n        event.ord = 1\n        self.MIDIEventList.append(event)\n\n    elif thing.type == 'SysEx':\n        event = MIDIEvent()\n        event.type = \"SysEx\"\n        event.time = thing.time * TICKSPERBEAT\n        event.manID = thing.manID\n        event.payload = thing.payload\n        event.ord = 1\n        self.MIDIEventList.append(event)\n\n    elif thing.type == 'UniversalSysEx':\n        event = MIDIEvent()\n        event.type = \"UniversalSysEx\"\n        event.realTime = thing.realTime\n        event.sysExChannel = thing.sysExChannel\n        event.time = thing.time * TICKSPERBEAT\n        event.code = thing.code\n        event.subcode = thing.subcode\n        event.payload = thing.payload\n        event.ord = 1\n        self.MIDIEventList.append(event)\n\n    else:\n        print (\"Error in MIDITrack: Unknown event type\")\n        sys.exit(2)\n    \n# Assumptions in the code expect the list to be time-sorted.\n# self.MIDIEventList.sort(lambda x, y: x.time - y.time)\n\nself.MIDIEventList.sort(key=lambda x: (x.time))\n\nif self.deinterleave:    \n    self.deInterleaveNotes()", "path": "midiutil\\MidiFile3.py", "repo_name": "cirlabs/miditime", "stars": 150, "license": "None", "language": "python", "size": 82}
{"docstring": "''' Sorting function for events.'''\n", "func_signal": "def __lt__(self, other):\n", "code": "if self.time < other.time:\n    return True\nelif self.time > other.time:\n    return False\nelse:\n    if self.ord < other.ord:\n        return True\n    elif self.ord > other.ord:\n        return False\n    else:\n        return False", "path": "midiutil\\MidiFile3.py", "repo_name": "cirlabs/miditime", "stars": 150, "license": "None", "language": "python", "size": 82}
{"docstring": "\"\"\"Create our LSTM\"\"\"\n", "func_signal": "def get_network(frames, input_size, num_classes):\n", "code": "net = tflearn.input_data(shape=[None, frames, input_size])\nnet = tflearn.lstm(net, 128, dropout=0.8, return_seq=True)\nnet = tflearn.lstm(net, 128)\nnet = tflearn.fully_connected(net, num_classes, activation='softmax')\nnet = tflearn.regression(net, optimizer='adam',\n                         loss='categorical_crossentropy', name=\"output1\")\nreturn net", "path": "rnn_utils.py", "repo_name": "harvitronix/continuous-online-video-classification-blog", "stars": 202, "license": "mit", "language": "python", "size": 14081}
{"docstring": "\"\"\"Given a list of frames, predict all their classes.\"\"\"\n# Unpersists graph from file\n", "func_signal": "def predict_on_frames(frames, batch, labels):\n", "code": "with tf.gfile.FastGFile(\"./inception/retrained_graph.pb\", 'rb') as fin:\n    graph_def = tf.GraphDef()\n    graph_def.ParseFromString(fin.read())\n    _ = tf.import_graph_def(graph_def, name='')\n\nwith tf.Session() as sess:\n    softmax_tensor = sess.graph.get_tensor_by_name('final_result:0')\n\n    frame_predictions = []\n    image_path = 'images/' + batch + '/'\n    pbar = tqdm(total=len(frames))\n    for i, frame in enumerate(frames):\n        filename = frame[0]\n        label = frame[1]\n\n        # Get the image path.\n        image = image_path + filename + '.jpg'\n\n        # Read in the image_data\n        image_data = tf.gfile.FastGFile(image, 'rb').read()\n\n        try:\n            predictions = sess.run(\n                softmax_tensor,\n                {'DecodeJpeg/contents:0': image_data}\n            )\n            prediction = predictions[0]\n        except KeyboardInterrupt:\n            print(\"You quit with ctrl+c\")\n            sys.exit()\n        except:\n            print(\"Error making prediction, continuing.\")\n            continue\n\n        this_prediction = prediction.tolist()\n\n        max_value = max(this_prediction)\n        max_index = this_prediction.index(max_value)\n        predicted_label = labels[max_index]\n\n        if predicted_label != label:\n            print(image_path + filename + '.jpg')\n            print(predicted_label, label)\n            print(prediction)\n            print('------')\n\n        if i > 0 and i % 10 == 0:\n            pbar.update(10)\n\n    pbar.close()\n\n    return frame_predictions", "path": "find_bad_predictions.py", "repo_name": "harvitronix/continuous-online-video-classification-blog", "stars": 202, "license": "mit", "language": "python", "size": 14081}
{"docstring": "\"\"\"Stream images off the camera and save them.\"\"\"\n", "func_signal": "def capture_images(save_folder):\n", "code": "camera = PiCamera()\ncamera.resolution = (320, 240)\ncamera.framerate = 5\n\n# Warmup...\ntime.sleep(2)\n\n# And capture continuously forever.\nfor _ in camera.capture_continuous(\n        save_folder + '{timestamp}.jpg',\n        'jpeg', use_video_port=True\n):\n    pass", "path": "stream_images.py", "repo_name": "harvitronix/continuous-online-video-classification-blog", "stars": 202, "license": "mit", "language": "python", "size": 14081}
{"docstring": "\"\"\"Stream images off the camera and process them.\"\"\"\n\n", "func_signal": "def run_classification(labels):\n", "code": "camera = PiCamera()\ncamera.resolution = (320, 240)\ncamera.framerate = 2\nraw_capture = PiRGBArray(camera, size=(320, 240))\n\n# Warmup...\ntime.sleep(2)\n\n# Unpersists graph from file\nwith tf.gfile.FastGFile(\"../inception/retrained_graph.pb\", 'rb') as fin:\n    graph_def = tf.GraphDef()\n    graph_def.ParseFromString(fin.read())\n    _ = tf.import_graph_def(graph_def, name='')\n\nwith tf.Session() as sess:\n    # And capture continuously forever.\n    softmax_tensor = sess.graph.get_tensor_by_name('final_result:0')\n    for _, image in enumerate(\n            camera.capture_continuous(\n                raw_capture, format='bgr', use_video_port=True\n            )\n        ):\n        # Get the numpy version of the image.\n        decoded_image = image.array\n\n        # Make the prediction. Big thanks to this SO answer:\n        # http://stackoverflow.com/questions/34484148/feeding-image-data-in-tensorflow-for-transfer-learning\n        predictions = sess.run(softmax_tensor, {'DecodeJpeg:0': decoded_image})\n        prediction = predictions[0]\n\n        # Get the highest confidence category.\n        prediction = prediction.tolist()\n        max_value = max(prediction)\n        max_index = prediction.index(max_value)\n        predicted_label = labels[max_index]\n\n        print(\"%s (%.2f%%)\" % (predicted_label, max_value * 100))\n\n        # Reset the buffer so we're ready for the next one.\n        raw_capture.truncate(0)", "path": "online.py", "repo_name": "harvitronix/continuous-online-video-classification-blog", "stars": 202, "license": "mit", "language": "python", "size": 14081}
{"docstring": "\"\"\"Create a wider LSTM\"\"\"\n", "func_signal": "def get_network_wide(frames, input_size, num_classes):\n", "code": "net = tflearn.input_data(shape=[None, frames, input_size])\nnet = tflearn.lstm(net, 256, dropout=0.2)\nnet = tflearn.fully_connected(net, num_classes, activation='softmax')\nnet = tflearn.regression(net, optimizer='adam',\n                         loss='categorical_crossentropy', name='output1')\nreturn net", "path": "rnn_utils.py", "repo_name": "harvitronix/continuous-online-video-classification-blog", "stars": 202, "license": "mit", "language": "python", "size": 14081}
{"docstring": "\"\"\"Create a wider LSTM\"\"\"\n", "func_signal": "def get_network_wider(frames, input_size, num_classes):\n", "code": "net = tflearn.input_data(shape=[None, frames, input_size])\nnet = tflearn.lstm(net, 512, dropout=0.2)\nnet = tflearn.fully_connected(net, num_classes, activation='softmax')\nnet = tflearn.regression(net, optimizer='adam',\n                         loss='categorical_crossentropy', name='output1')\nreturn net", "path": "rnn_utils.py", "repo_name": "harvitronix/continuous-online-video-classification-blog", "stars": 202, "license": "mit", "language": "python", "size": 14081}
{"docstring": "\"\"\"Given an image, predict its class.\"\"\"\n", "func_signal": "def predict_on_image(image):\n", "code": "labels = get_labels()\n\n# Unpersists graph from file\nwith tf.gfile.FastGFile(\"./inception/retrained_graph.pb\", 'rb') as fin:\n    graph_def = tf.GraphDef()\n    graph_def.ParseFromString(fin.read())\n    _ = tf.import_graph_def(graph_def, name='')\n\nwith tf.Session() as sess:\n    softmax_tensor = sess.graph.get_tensor_by_name('final_result:0')\n\n    # Read in the image_data\n    image_data = tf.gfile.FastGFile(image, 'rb').read()\n\n    try:\n        predictions = sess.run(softmax_tensor, \\\n             {'DecodeJpeg/contents:0': image_data})\n        prediction = predictions[0]\n    except:\n        print(\"Error making prediction.\")\n        sys.exit()\n\n    # List of predictions. See retrained_labels.txt for labels.\n    print(prediction)\n    prediction = prediction.tolist()\n    max_value = max(prediction)\n    max_index = prediction.index(max_value)\n    predicted_label = labels[max_index]\n    print(predicted_label)", "path": "classify_image.py", "repo_name": "harvitronix/continuous-online-video-classification-blog", "stars": 202, "license": "mit", "language": "python", "size": 14081}
{"docstring": "\"\"\"Get a list of labels so we can see if it's an ad or not.\"\"\"\n", "func_signal": "def get_labels():\n", "code": "with open('./inception/retrained_labels.txt', 'r') as fin:\n    labels = [line.rstrip('\\n') for line in fin]\n    return labels", "path": "classify_image.py", "repo_name": "harvitronix/continuous-online-video-classification-blog", "stars": 202, "license": "mit", "language": "python", "size": 14081}
{"docstring": "\"\"\"After predicting on each batch, check that batch's\naccuracy to make sure things are good to go. This is\na simple accuracy metric, and so doesn't take confidence\ninto account, which would be a better metric to use to\ncompare changes in the model.\"\"\"\n", "func_signal": "def get_accuracy(predictions, labels):\n", "code": "correct = 0\nfor frame in predictions:\n    # Get the highest confidence class.\n    this_prediction = frame[0].tolist()\n    this_label = frame[1]\n\n    max_value = max(this_prediction)\n    max_index = this_prediction.index(max_value)\n    predicted_label = labels[max_index]\n\n    # Now see if it matches.\n    if predicted_label == this_label:\n        correct += 1\n\naccuracy = correct / len(predictions)\nreturn accuracy", "path": "make_predictions.py", "repo_name": "harvitronix/continuous-online-video-classification-blog", "stars": 202, "license": "mit", "language": "python", "size": 14081}
{"docstring": "\"\"\"Return a list of our trained labels so we can\ntest our training accuracy. The file is in the\nformat of one label per line, in the same order\nas the predictions are made. The order can change\nbetween training runs.\"\"\"\n", "func_signal": "def get_labels():\n", "code": "with open(\"./inception/retrained_labels.txt\", 'r') as fin:\n    labels = [line.rstrip('\\n') for line in fin]\nreturn labels", "path": "find_bad_predictions.py", "repo_name": "harvitronix/continuous-online-video-classification-blog", "stars": 202, "license": "mit", "language": "python", "size": 14081}
{"docstring": "\"\"\"Get the data from our saved predictions or pooled features.\"\"\"\n\n# Local vars.\n", "func_signal": "def get_data(filename, num_frames, num_classes, input_length):\n", "code": "X = []\ny = []\ntemp_list = deque()\n\n# Open and get the features.\nwith open(filename, 'rb') as fin:\n    frames = pickle.load(fin)\n\n    for i, frame in enumerate(frames):\n        features = frame[0]\n        actual = frame[1]\n\n        # Convert our labels into binary.\n        if actual == 'ad':\n            actual = 1\n        else:\n            actual = 0\n\n        # Add to the queue.\n        if len(temp_list) == num_frames - 1:\n            temp_list.append(features)\n            flat = list(temp_list)\n            X.append(np.array(flat))\n            y.append(actual)\n            temp_list.popleft()\n        else:\n            temp_list.append(features)\n            continue\n\nprint(\"Total dataset size: %d\" % len(X))\n\n# Numpy.\nX = np.array(X)\ny = np.array(y)\n\n# Reshape.\nX = X.reshape(-1, num_frames, input_length)\n\n# One-hot encoded categoricals.\ny = to_categorical(y, num_classes)\n\n# Split into train and test.\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.1, random_state=42)\n\nreturn X_train, X_test, y_train, y_test", "path": "rnn_utils.py", "repo_name": "harvitronix/continuous-online-video-classification-blog", "stars": 202, "license": "mit", "language": "python", "size": 14081}
{"docstring": "# Unpersists graph from file\n", "func_signal": "def predict_on_frames(frames, batch):\n", "code": "with tf.gfile.FastGFile(\"./inception/retrained_graph.pb\", 'rb') as f:\n    graph_def = tf.GraphDef()\n    graph_def.ParseFromString(f.read())\n    _ = tf.import_graph_def(graph_def, name='')\n\nwith tf.Session() as sess:\n    pool_tensor = sess.graph.get_tensor_by_name('pool_3:0')\n\n    # Moving this into the session to make it faster.\n    cnn_features = []\n    image_path = 'images/' + batch + '/'\n    pbar = tqdm(total=len(frames))\n    for i, frame in enumerate(frames):\n        filename = frame[0]\n        label = frame[1]\n\n        # For some reason, we have a tar reference in our pickled file.\n        if 'tar' in filename:\n            continue\n\n        # Get the image path.\n        image = image_path + filename + '.jpg'\n\n        # Read in the image_data\n        image_data = tf.gfile.FastGFile(image, 'rb').read()\n\n        try:\n            cnn_representation = sess.run(pool_tensor,\n                 {'DecodeJpeg/contents:0': image_data})\n        except KeyboardInterrupt:\n            print(\"You quit with ctrl+c\")\n            sys.exit()\n        except:\n            print(\"Error making prediction, continuing.\")\n            continue\n\n        # Save it out.\n        frame_row = [cnn_representation, label]\n        cnn_features.append(frame_row)\n\n        if i > 0 and i % 10 == 0:\n            pbar.update(10)\n\n    pbar.close()\n\n    return cnn_features", "path": "make_predictions_pool.py", "repo_name": "harvitronix/continuous-online-video-classification-blog", "stars": 202, "license": "mit", "language": "python", "size": 14081}
{"docstring": "\"\"\"From the blog post linked above.\"\"\"\n# Get our data.\n", "func_signal": "def main(filename, frames, batch_size, num_classes, input_length):\n", "code": "X_train, X_test, y_train, y_test = get_data(filename, frames, num_classes, input_length)\n\n# Get sizes.\nnum_classes = len(y_train[0])\n\n# Get our network.\nnet = get_network_wide(frames, input_length, num_classes)\n\n# Train the model.\nmodel = tflearn.DNN(net, tensorboard_verbose=0)\nmodel.fit(X_train, y_train, validation_set=(X_test, y_test),\n          show_metric=True, batch_size=batch_size, snapshot_step=100,\n          n_epoch=4)\n\n# Save it.\nmodel.save('checkpoints/rnn.tflearn')", "path": "rnn_train.py", "repo_name": "harvitronix/continuous-online-video-classification-blog", "stars": 202, "license": "mit", "language": "python", "size": 14081}
{"docstring": "\"\"\"Return a list of our trained labels so we can\ntest our training accuracy. The file is in the\nformat of one label per line, in the same order\nas the predictions are made. The order can change\nbetween training runs.\"\"\"\n", "func_signal": "def get_labels():\n", "code": "with open(\"./inception/retrained_labels.txt\", 'r') as fin:\n    labels = [line.rstrip('\\n') for line in fin]\nreturn labels", "path": "make_predictions.py", "repo_name": "harvitronix/continuous-online-video-classification-blog", "stars": 202, "license": "mit", "language": "python", "size": 14081}
{"docstring": "\"\"\"Given a timestamp and the commercials list, return if\nthis frame is a commercial or not. If not, return label.\"\"\"\n", "func_signal": "def get_label(timestamp, commercials):\n", "code": "for com in commercials['commercials']:\n    if com['start'] <= timestamp <= com['end']:\n        return 'ad'\nreturn commercials['class']", "path": "build_labels.py", "repo_name": "harvitronix/continuous-online-video-classification-blog", "stars": 202, "license": "mit", "language": "python", "size": 14081}
{"docstring": "\"\"\"From the blog post linked above.\"\"\"\n# Get our data.\n", "func_signal": "def main(filename, frames, batch_size, num_classes, input_length):\n", "code": "X_train, _, y_train, _ = get_data(filename, frames, num_classes, input_length)\n\n# Get sizes.\nnum_classes = len(y_train[0])\n\n# Get our network.\nnet = get_network_wide(frames, input_length, num_classes)\n\n# Get our model.\nmodel = tflearn.DNN(net, tensorboard_verbose=0)\nmodel.load('checkpoints/rnn.tflearn')\n\n# Evaluate.\nprint(model.evaluate(X_train, y_train))", "path": "rnn_eval.py", "repo_name": "harvitronix/continuous-online-video-classification-blog", "stars": 202, "license": "mit", "language": "python", "size": 14081}
{"docstring": "\"\"\"Label our frames.\"\"\"\n# Get all our images.\n", "func_signal": "def label_frames(batch, commercials, copyimage=True):\n", "code": "images = sorted(glob.glob('./images/' + str(batch) + '/*'))\nnum_images = len(images)\n\nprint(\"Labelling %d frames.\" % num_images)\n\n# Loop through our images and set our labels.\nlabeled_images = []\nnum_commercials = 0\nfor image in images:\n    # Get the timestamp.\n    timestamp = image.replace('.jpg', '').split('/')[-1]\n\n    # What is it?\n    label = get_label(timestamp, commercials)\n\n    # Save it.\n    labeled_images.append([timestamp, label])\n\n    # Copy it.\n    if copyimage:\n        copyfile(image, './images/classifications/' + label + '/' + timestamp + '.jpg')\n\n    # Info.\n    if label == 'ad':\n        num_commercials += 1\n\nprint(\"Done labelling, with %d commercial frames and %d not.\" %\n      (num_commercials, num_images - num_commercials))\n\nwith open('data/labeled-frames-' + str(batch) + '.pkl', 'wb') as fout:\n    pickle.dump(labeled_images, fout)\n\nreturn labeled_images", "path": "build_labels.py", "repo_name": "harvitronix/continuous-online-video-classification-blog", "stars": 202, "license": "mit", "language": "python", "size": 14081}
{"docstring": "\"\"\"Get a list of labels so we can see if it's an ad or not.\"\"\"\n", "func_signal": "def get_labels():\n", "code": "with open('../inception/retrained_labels.txt', 'r') as fin:\n    labels = [line.rstrip('\\n') for line in fin]\nreturn labels", "path": "online.py", "repo_name": "harvitronix/continuous-online-video-classification-blog", "stars": 202, "license": "mit", "language": "python", "size": 14081}
{"docstring": "\"\"\"Given a list of frames, predict all their classes.\"\"\"\n# Unpersists graph from file\n", "func_signal": "def predict_on_frames(frames, batch):\n", "code": "with tf.gfile.FastGFile(\"./inception/retrained_graph.pb\", 'rb') as fin:\n    graph_def = tf.GraphDef()\n    graph_def.ParseFromString(fin.read())\n    _ = tf.import_graph_def(graph_def, name='')\n\nwith tf.Session() as sess:\n    softmax_tensor = sess.graph.get_tensor_by_name('final_result:0')\n\n    frame_predictions = []\n    image_path = 'images/' + batch + '/'\n    pbar = tqdm(total=len(frames))\n    for i, frame in enumerate(frames):\n        filename = frame[0]\n        label = frame[1]\n\n        # Get the image path.\n        image = image_path + filename + '.jpg'\n\n        # Read in the image_data\n        image_data = tf.gfile.FastGFile(image, 'rb').read()\n\n        try:\n            predictions = sess.run(\n                softmax_tensor,\n                {'DecodeJpeg/contents:0': image_data}\n            )\n            prediction = predictions[0]\n        except KeyboardInterrupt:\n            print(\"You quit with ctrl+c\")\n            sys.exit()\n        except:\n            print(\"Error making prediction, continuing.\")\n            continue\n\n        # Save the probability that it's each of our classes.\n        frame_predictions.append([prediction, label])\n\n        if i > 0 and i % 10 == 0:\n            pbar.update(10)\n\n    pbar.close()\n\n    return frame_predictions", "path": "make_predictions.py", "repo_name": "harvitronix/continuous-online-video-classification-blog", "stars": 202, "license": "mit", "language": "python", "size": 14081}
{"docstring": "\"\"\"Create a deeper LSTM\"\"\"\n", "func_signal": "def get_network_deep(frames, input_size, num_classes):\n", "code": "net = tflearn.input_data(shape=[None, frames, input_size])\nnet = tflearn.lstm(net, 64, dropout=0.2, return_seq=True)\nnet = tflearn.lstm(net, 64, dropout=0.2, return_seq=True)\nnet = tflearn.lstm(net, 64, dropout=0.2)\nnet = tflearn.fully_connected(net, num_classes, activation='softmax')\nnet = tflearn.regression(net, optimizer='adam',\n                         loss='categorical_crossentropy', name=\"output1\")\nreturn net", "path": "rnn_utils.py", "repo_name": "harvitronix/continuous-online-video-classification-blog", "stars": 202, "license": "mit", "language": "python", "size": 14081}
{"docstring": "\"\"\"slice_obj.indices() except this one supports longs.\"\"\"\n# start stop step\n", "func_signal": "def slice_indices(slice_obj, size):\n", "code": "start = slice_obj.start\nstop = slice_obj.stop\nstep = slice_obj.step\n\n# We don't always update a value for negative indices (if we wrote it here\n# due to None).\nif step is None:\n    step = 1\nif start is None:\n    if step > 0:\n        start = 0\n    else:\n        start = size - 1\nelse:\n    start = _adjust_index(start, size)\n\nif stop is None:\n    if step > 0:\n        stop = size\n    else:\n        stop = -1\nelse:\n    stop = _adjust_index(stop, size)\n\nreturn (start, stop, step)", "path": "pyhashcat\\sre_yield\\__init__.py", "repo_name": "girishramnani/hacking-tools", "stars": 168, "license": "mit", "language": "python", "size": 231}
{"docstring": "\"\"\"This module can be executed on the command line for testing.\"\"\"\n", "func_signal": "def main(argv=None):\n", "code": "if argv is None:\n    argv = sys.argv\nfor arg in argv[1:]:\n    for i in AllStrings(arg):\n        print(i)", "path": "pyhashcat\\sre_yield\\__init__.py", "repo_name": "girishramnani/hacking-tools", "stars": 168, "license": "mit", "language": "python", "size": 231}
{"docstring": "# TODO keep group(0) only, and spans for the rest.\n", "func_signal": "def __init__(self, string, groups, named_groups):\n", "code": "self._string = string\nself._groups = groups\nself._named_groups = named_groups\nself.lastindex = len(groups) + 1", "path": "pyhashcat\\sre_yield\\__init__.py", "repo_name": "girishramnani/hacking-tools", "stars": 168, "license": "mit", "language": "python", "size": 231}
{"docstring": "# Special case which distinguishes branch from charset operator\n", "func_signal": "def in_values(self, items):\n", "code": "if items and items[0][0] == sre_constants.NEGATE:\n    items = self.branch_values(None, items[1:])\n    return [item for item in self.charset if item not in items]\nreturn self.branch_values(None, items)", "path": "pyhashcat\\sre_yield\\__init__.py", "repo_name": "girishramnani/hacking-tools", "stars": 168, "license": "mit", "language": "python", "size": 231}
{"docstring": "# xrange(start, stop) raises OverflowError in py2.7\n", "func_signal": "def test_bignum():\n", "code": "start = sys.maxsize\nstop = sys.maxsize + 5\n\nl = list(sre_yield._bigrange(start, stop))\nassert len(l) == 5", "path": "pyhashcat\\sre_yield\\tests\\test_bigrange.py", "repo_name": "girishramnani/hacking-tools", "stars": 168, "license": "mit", "language": "python", "size": 231}
{"docstring": "\"\"\"Sequential expansion of the count to be combinatorics.\"\"\"\n", "func_signal": "def max_repeat_values(self, min_count, max_count, items):\n", "code": "max_count = min(max_count, self.max_count)\nreturn RepetitiveSequence(\n    self.sub_values(items), min_count, max_count)", "path": "pyhashcat\\sre_yield\\__init__.py", "repo_name": "girishramnani/hacking-tools", "stars": 168, "license": "mit", "language": "python", "size": 231}
{"docstring": "\"\"\"Because xrange doesn't support longs :(\"\"\"\n# prefer real xrange if it works\n", "func_signal": "def _xrange(*args):\n", "code": "try:\n    return range(*args)\nexcept OverflowError:\n    return _bigrange(*args)", "path": "pyhashcat\\sre_yield\\__init__.py", "repo_name": "girishramnani/hacking-tools", "stars": 168, "license": "mit", "language": "python", "size": 231}
{"docstring": "# Derived classes will likely override this constructor\n", "func_signal": "def __init__(self, raw, slicer=None):\n", "code": "self.raw = raw\nif slicer is None:\n    self.start, self.stop, self.step = 0, raw.__len__(), 1\nelse:\n    self.start, self.stop, self.step = slice_indices(slicer, raw.__len__())\n\n# Integer round up, depending on step direction\nself.length = ((self.stop - self.start + self.step - _sign(self.step)) /\n               self.step)", "path": "pyhashcat\\sre_yield\\__init__.py", "repo_name": "girishramnani/hacking-tools", "stars": 168, "license": "mit", "language": "python", "size": 231}
{"docstring": "\"\"\"Typically only pass i.  d is an internal detail, for consistency with other classes.\n\nIf you care about the capture groups, you should use\nRegexMembershipSequenceMatches instead, which returns a Match object\ninstead of a string.\"\"\"\n", "func_signal": "def get_item(self, i, d=None):\n", "code": "if self.has_groupref or d is not None:\n    if d is None:\n        d = {}\n    return super(RegexMembershipSequence, self).get_item(i, d)\nelse:\n    return super(RegexMembershipSequence, self).get_item(i)", "path": "pyhashcat\\sre_yield\\__init__.py", "repo_name": "girishramnani/hacking-tools", "stars": 168, "license": "mit", "language": "python", "size": 231}
{"docstring": "# If the user wanted a slice, we provide a wrapper\n", "func_signal": "def __getitem__(self, i):\n", "code": "if isinstance(i, slice):\n    result = SlicedSequence(self, slicer=i)\n    if result.__len__() < 16:\n        # Short lists are unpacked\n        result = [item for item in result]\n    return result\ni = _adjust_index(i, self.length)\n# Usually we just call the user-provided function\nreturn self.get_item(i)", "path": "pyhashcat\\sre_yield\\__init__.py", "repo_name": "girishramnani/hacking-tools", "stars": 168, "license": "mit", "language": "python", "size": 231}
{"docstring": "\"\"\"\nlength: Length of this sequence.\nfunc: function(index)\ninc_func: function(index, value_of_previous)\n\"\"\"\n\n", "func_signal": "def __init__(self, func, length, inc_func=None):\n", "code": "self.func = func\nself.inc_func = inc_func\nself.length = length\nself._cache = {}", "path": "pyhashcat\\sre_yield\\cachingseq.py", "repo_name": "girishramnani/hacking-tools", "stars": 168, "license": "mit", "language": "python", "size": 231}
{"docstring": "\"\"\"Converts SRE parser data into literals and merges those lists.\"\"\"\n", "func_signal": "def branch_values(self, _, items):\n", "code": "return ConcatenatedSequence(\n    *[self.sub_values(parsed) for parsed in items])", "path": "pyhashcat\\sre_yield\\__init__.py", "repo_name": "girishramnani/hacking-tools", "stars": 168, "license": "mit", "language": "python", "size": 231}
{"docstring": "# http://mikestoolbox.com/powersum.html\n", "func_signal": "def powersum(x, low, high):\n", "code": "xm1 = x - 1\nif xm1 == 0:\n    return high - low + 1\na = x ** (high + 1)\nb = x ** low\nreturn (a - b) / xm1", "path": "pyhashcat\\sre_yield\\fastdivmod.py", "repo_name": "girishramnani/hacking-tools", "stars": 168, "license": "mit", "language": "python", "size": 231}
{"docstring": "\"\"\"Generate successive (x % by); x /= by, the obvious way.\n\nChunk is ignored.\n\"\"\"\n", "func_signal": "def divmod_iter_basic(x, by, chunk=None):\n", "code": "while x:\n    x, m = divmod(x, by)\n    yield m", "path": "pyhashcat\\sre_yield\\fastdivmod.py", "repo_name": "girishramnani/hacking-tools", "stars": 168, "license": "mit", "language": "python", "size": 231}
{"docstring": "\"\"\"\n\n:param filename:\n:param hashtype:\n\"\"\"\n", "func_signal": "def __init__(self,filename,hashtype):\n", "code": "try:\n\n    self.csvfile = open(filename,'w')\n    self.writer = csv.writer(self.csvfile,delimiter=\",\",quoting=csv.QUOTE_ALL)\n    self.writer.writerow(('File','Path','Size','Modified Time','Access Time','Created Time','hashType','Owner','Group','Mode'))\nexcept :\n    log.error(\"CSV store failed for \"+filename)", "path": "file_hasher\\_pfish_tools.py", "repo_name": "girishramnani/hacking-tools", "stars": 168, "license": "mit", "language": "python", "size": 231}
{"docstring": "# If the RE module cannot compile it, we give up quickly\n", "func_signal": "def __init__(self, pattern, flags=0, charset=CHARSET, max_count=None):\n", "code": "self.matcher = re.compile(r'(?:%s)\\Z' % pattern, flags)\nif not flags & re.DOTALL:\n    charset = ''.join(c for c in charset if c != '\\n')\nself.charset = charset\n\nself.named_group_lookup = self.matcher.groupindex\n\nif flags & re.IGNORECASE:\n    raise ParseError('Flag \"i\" not supported. https://github.com/google/sre_yield/issues/4')\nelif flags & re.UNICODE:\n    raise ParseError('Flag \"u\" not supported. https://github.com/google/sre_yield/issues/3')\nelif flags & re.LOCALE:\n    raise ParseError('Flag \"l\" not supported. https://github.com/google/sre_yield/issues/5')\n\nif max_count is None:\n    self.max_count = MAX_REPEAT_COUNT\nelse:\n    self.max_count = max_count\n\nself.has_groupref = False\n\n# Configure the parser backends\nself.backends = {\n    sre_constants.LITERAL: lambda y: [chr(y)],\n    sre_constants.RANGE: lambda l, h: [chr(c) for c in range(l, h+1)],\n    sre_constants.SUBPATTERN: self.maybe_save,\n    sre_constants.BRANCH: self.branch_values,\n    sre_constants.MIN_REPEAT: self.max_repeat_values,\n    sre_constants.MAX_REPEAT: self.max_repeat_values,\n    sre_constants.AT: self.empty_list,\n    sre_constants.ASSERT: self.empty_list,\n    sre_constants.ASSERT_NOT: self.empty_list,\n    sre_constants.ANY:\n        lambda _: self.in_values(((sre_constants.NEGATE,),)),\n    sre_constants.IN: self.in_values,\n    sre_constants.NOT_LITERAL: self.not_literal,\n    sre_constants.CATEGORY: self.category,\n    sre_constants.GROUPREF: self.groupref,\n}\n# Now build a generator that knows all possible patterns\nself.raw = self.sub_values(sre_parse.parse(pattern, flags))\n# Configure this class instance to know about that result\nself.length = self.raw.__len__()", "path": "pyhashcat\\sre_yield\\__init__.py", "repo_name": "girishramnani/hacking-tools", "stars": 168, "license": "mit", "language": "python", "size": 231}
{"docstring": "#so that the message has been published\n", "func_signal": "def show_messages(self):\n", "code": "time.sleep(LOG_INTERVAL//2)\nself.redis = redis.Redis()\nwhile True:\n    message =[]\n    for keys in self.childs:\n\n        out =self.redis.get(keys+\"-\"+REQ_KEY)\n        if out:\n            message.append(\" \".join([out.decode(),\"req/s\"]))\n    mess = \" | \".join(message)\n    print(mess+\"\\r\",end=\"\")\n    time.sleep(LOG_INTERVAL)", "path": "ddos\\controller.py", "repo_name": "girishramnani/hacking-tools", "stars": 168, "license": "mit", "language": "python", "size": 231}
{"docstring": "\"\"\"Generate successive (x % by); x /= by, but faster.\n\nIf provided, |chunk| must be a power of |by| (otherwise it is determined\nautomatically for 1024 per inner loop, based on analysis of bench_genmod.py)\n\"\"\"\n\n", "func_signal": "def divmod_iter_chunking(x, by, chunk=None):\n", "code": "if by == 1:\n    assert x == 0, x\n    yield 0\n    return\n\nif chunk is None:\n    digits_per_chunk = 1024\n    chunk = by ** digits_per_chunk\nelse:\n    digits_per_chunk = int(round(log(chunk) / log(by)))\n    if (by ** digits_per_chunk) != chunk:\n        raise ValueError(\"Chunk=%d must be a power of by=%d\" % (chunk, by))\n\nassert digits_per_chunk > 0\n\nwhile x:\n    x, this_chunk = divmod(x, chunk)\n    #this_chunk = int(this_chunk)\n    for _ in range(digits_per_chunk):\n        this_chunk, m = divmod(this_chunk, by)\n        yield m\n\n        if this_chunk == 0 and x == 0:\n            break", "path": "pyhashcat\\sre_yield\\fastdivmod.py", "repo_name": "girishramnani/hacking-tools", "stars": 168, "license": "mit", "language": "python", "size": 231}
{"docstring": "# Derived classes will likely override this constructor\n", "func_signal": "def __init__(self, raw):\n", "code": "self.raw = raw\n# Note that we can't use the function len() because it insists on trying\n# to convert the returned number from a long-int to an ordinary int.\nself.length = raw.__len__()", "path": "pyhashcat\\sre_yield\\__init__.py", "repo_name": "girishramnani/hacking-tools", "stars": 168, "license": "mit", "language": "python", "size": 231}
{"docstring": "\"\"\"Finds out how many repeats this index implies, then picks strings.\"\"\"\n", "func_signal": "def get_item(self, i, d=None):\n", "code": "if i < self.offset_break:\n    by_bisect = bisect.bisect_left(self.offsets, (i, -1), hi=self.index_of_offset)\nelse:\n    by_bisect = bisect.bisect_left(self.offsets, (i, -1), lo=self.index_of_offset)\n\nif by_bisect == len(self.offsets) or self.offsets[by_bisect][0] > i:\n    by_bisect -= 1\n\nnum = i - self.offsets[by_bisect][0]\ncount = self.offsets[by_bisect][1]\n\nif count > 100 and self.content_length < 1000:\n    content = list(self.content)\nelse:\n    content = self.content\n\nresult = []\n\nif count == 0:\n    return ''\n\nfor modulus in fastdivmod.divmod_iter(num, self.content_length):\n    result.append(content[modulus])\n\nleftover = count - len(result)\nif leftover:\n    assert leftover > 0\n    result.extend([content[0]] * leftover)\n\n# smallest place value ends up on the right\nreturn ''.join(result[::-1])", "path": "pyhashcat\\sre_yield\\__init__.py", "repo_name": "girishramnani/hacking-tools", "stars": 168, "license": "mit", "language": "python", "size": 231}
{"docstring": "\"\"\"\nCheck that we have all the required command line arguments,\nand that the input/output format values are supported.\n\"\"\"\n", "func_signal": "def check_arguments(args):\n", "code": "for required in REQUIRED_PARAMETERS:\n    if required not in args:\n        print_error(\"Argument '%s' is required\" % required)\n        sys.exit(2)\nif args.input_format not in INPUT_FORMATS:\n    print_error(\"Format '%s' is not a valid input format\" % args.input_format)\n    print_error(\"Valid input formats: %s\" % INPUT_FORMATS)\n    sys.exit(4)\nif args.output_format not in OUTPUT_FORMATS:\n    print_error(\"Format '%s' is not a valid output format\" % args.output_format)\n    print_error(\"Valid output formats: %s\" % OUTPUT_FORMATS)\n    sys.exit(4)", "path": "penelope\\command_line.py", "repo_name": "pettarin/penelope", "stars": 181, "license": "mit", "language": "python", "size": 12478}
{"docstring": "# result to be returned\n", "func_signal": "def write(dictionary, args, output_file_path):\n", "code": "result = None\n\n# get absolute path\noutput_file_path_absolute = os.path.abspath(output_file_path)\n\n# sort by headword, optionally ignoring case\ndictionary.sort(by_headword=True, ignore_case=args.sort_ignore_case)\n\n# create groups\nspecial_group, group_keys, group_dict = dictionary.group(\n    prefix_function_path=args.group_by_prefix_function,\n    prefix_length=int(args.group_by_prefix_length),\n    merge_min_size=int(args.group_by_prefix_merge_min_size),\n    merge_across_first=args.group_by_prefix_merge_across_first\n)\nall_group_keys = group_keys\nif special_group is not None:\n    all_group_keys += [u\"SPECIAL\"]\n\n# create mobi object\nmobi = DictionaryEbook(ebook_format=DictionaryEbook.MOBI, args=args)\n\n# add groups\nfor key in all_group_keys:\n    if key == u\"SPECIAL\":\n        group_entries = special_group\n    else:\n        group_entries = group_dict[key]\n    mobi.add_group(key, group_entries)\n\n# create output file\nprint_debug(\"Writing to file '%s'...\" % (output_file_path_absolute), args.debug)\nmobi.write(output_file_path_absolute, compress=False)\nresult = [output_file_path]\nprint_debug(\"Writing to file '%s'... done\" % (output_file_path_absolute), args.debug)\n\n# run kindlegen\ntmp_path = mobi.get_tmp_path()\nif args.mobi_no_kindlegen:\n    print_info(\"Not running kindlegen, the raw files are located in '%s'\" % tmp_path)\n    result = [tmp_path]\nelse:\n    try:\n        print_debug(\"Creating .mobi file with kindlegen...\", args.debug)\n        kindlegen_path = KINDLEGEN\n        opf_file_path_absolute = os.path.join(tmp_path, \"OEBPS\", \"content.opf\")\n        mobi_file_path_relative = u\"content.mobi\"\n        mobi_file_path_absolute = os.path.join(tmp_path, \"OEBPS\", mobi_file_path_relative)\n        if args.kindlegen_path is None:\n            print_info(\"  Running '%s' from $PATH\" % KINDLEGEN)\n        else:\n            kindlegen_path = args.kindlegen_path\n            print_info(\"  Running '%s' from '%s'\" % (KINDLEGEN, kindlegen_path))\n        proc = subprocess.Popen(\n            [kindlegen_path, opf_file_path_absolute, \"-o\", mobi_file_path_relative],\n            stdout=subprocess.PIPE,\n            stdin=subprocess.PIPE,\n            stderr=subprocess.PIPE\n        )\n        output = proc.communicate()\n        if args.debug:\n            output_unicode = (output[0]).decode(\"utf-8\")\n            print_debug(output_unicode, args.debug)\n        copy_file(mobi_file_path_absolute, output_file_path_absolute)\n        result = [output_file_path]\n        print_debug(\"Creating .mobi file with kindlegen... done\", args.debug)\n    except OSError as exc:\n        print_error(\"  Unable to run '%s' as '%s'\" % (KINDLEGEN, kindlegen_path))\n        print_error(\"  Please make sure '%s':\" % KINDLEGEN)\n        print_error(\"    1. is available on your $PATH or\")\n        print_error(\"    2. specify its path with --kindlegen-path\")\n\n# delete tmp directory\ntmp_path = mobi.get_tmp_path()\nif args.keep:\n    print_info(\"Not deleting temp dir '%s'\" % (tmp_path))\nelse:\n    mobi.delete()\n    print_debug(\"Deleted temp dir '%s'\" % (tmp_path), args.debug)\n\nreturn result", "path": "penelope\\format_mobi.py", "repo_name": "pettarin/penelope", "stars": 181, "license": "mit", "language": "python", "size": 12478}
{"docstring": "# result flag\n", "func_signal": "def read_single_file(dictionary, args, input_file_path):\n", "code": "result = False\n\n# create a tmp directory\ntmp_path = create_temp_directory()\nprint_debug(\"Working in temp dir '%s'\" % (tmp_path), args.debug)\n\n# find .ifo, .idx, .dict[.dz] and .syn files inside the zip\n# and extract them to tmp_path\ninput_file_obj = zipfile.ZipFile(input_file_path)\nfound_files = find_files(input_file_obj.namelist())\nextracted_files = {}\nif len(found_files) > 0:\n    for key in found_files:\n        entry = found_files[key]\n        ext_file_path = os.path.join(tmp_path, key)\n        ext_file_obj = io.open(ext_file_path, \"wb\")\n        zip_entry = input_file_obj.open(entry)\n        ext_file_obj.write(zip_entry.read())\n        zip_entry.close()\n        ext_file_obj.close()\n        print_debug(\"Extracted %s\" % (ext_file_path), args.debug)\n        extracted_files[key] = ext_file_path\n        # extract from compressed file, but only if \".idx\" is not present as well\n        if (key == \"d.idx.gz\") and (\"d.idx\" not in found_files):\n            extracted_files[\"d.idx\"] = uncompress_file(ext_file_path, tmp_path, \"d.idx\")\n        # extract from compressed file, but only if \".dict\" is not present as well\n        if ((key == \"d.dict.dz\") or (key == \"d.dz\")) and (\"d.dict\" not in found_files):\n            extracted_files[\"d.dict\"] = uncompress_file(ext_file_path, tmp_path, \"d.dict\")\ninput_file_obj.close()\n\n# here we have d.ifo, d.idx and d.dict (all uncompressed) and possibly d.syn\n\nhas_syn = \"d.syn\" in extracted_files\nif (has_syn) and (args.ignore_synonyms):\n    has_syn = False\n    print_debug(\"Dictionary has synonyms, but ignoring them (--ignore-synonym)\", args.debug)\nifo_dict = read_ifo(extracted_files[\"d.ifo\"], has_syn, args)\nprint_debug(\"Read .ifo file with values:\\n%s\" % (str(ifo_dict)), args.debug)\n\n# read dict file\ndict_file_obj = io.open(extracted_files[\"d.dict\"], \"rb\")\ndict_file_bytes = dict_file_obj.read()\ndict_file_obj.close()\n\n# read idx file\nidx_file_obj = io.open(extracted_files[\"d.idx\"], \"rb\")\nbyte_read = idx_file_obj.read(1)\nheadword = b\"\"\nwhile byte_read:\n    if byte_read == b\"\\0\":\n        # end of current word: read offset and size\n        offset_bytes = idx_file_obj.read(4)\n        offset_int = int((struct.unpack('>i', offset_bytes))[0])\n        size_bytes = idx_file_obj.read(4)\n        size_int = int((struct.unpack('>i', size_bytes))[0])\n        definition = dict_file_bytes[offset_int:(offset_int + size_int)].decode(args.input_file_encoding)\n        headword = headword.decode(\"utf-8\")\n        if args.ignore_case:\n            headword = headword.lower()\n        dictionary.add_entry(headword=headword, definition=definition)\n        headword = b\"\"\n    else:\n        # read next byte\n        headword += byte_read\n    byte_read = idx_file_obj.read(1)\nidx_file_obj.close()\nresult = True\n\n# read syn file, if present\nif has_syn:\n    print_debug(\"The input StarDict file contains a .syn file, parsing it...\", args.debug)\n    result = False\n    syn_file_obj = io.open(extracted_files[\"d.syn\"], \"rb\")\n    byte_read = syn_file_obj.read(1)\n    synonym = b\"\"\n    while byte_read:\n        if byte_read == b\"\\0\":\n            # end of current synonym: read index of original word\n            index_bytes = syn_file_obj.read(4)\n            index_int = int((struct.unpack('>i', index_bytes))[0])\n            synonym = synonym.decode(\"utf-8\")\n            if index_int < len(dictionary):\n                dictionary.add_synonym(synonym=synonym, headword_index=index_int)\n            else:\n                # emit a warning?\n                print_debug(\"Synonym '%s' points to index %d >= len(dictionary), skipping it\" % (index_int, synonym), args.debug)\n            synonym = b\"\"\n        else:\n            # read next byte\n            synonym += byte_read\n        byte_read = syn_file_obj.read(1)\n    syn_file_obj.close()\n    result = True\n    print_debug(\"The input StarDict file contains a .syn file, parsing it... done\", args.debug)\nelse:\n    print_debug(\"The input StarDict file does not contain a .syn file\", args.debug)\n\n# delete tmp directory\nif args.keep:\n    print_info(\"Not deleting temp dir '%s'\" % (tmp_path))\nelse:\n    delete_directory(tmp_path)\n    print_debug(\"Deleted temp dir '%s'\" % (tmp_path), args.debug)\n\nreturn result", "path": "penelope\\format_stardict.py", "repo_name": "pettarin/penelope", "stars": 181, "license": "mit", "language": "python", "size": 12478}
{"docstring": "# result to be returned\n", "func_signal": "def write(dictionary, args, output_file_path):\n", "code": "result = None\n\n# get absolute path\noutput_file_path_absolute = os.path.abspath(output_file_path)\n\n# create tmp directory\ncwd = os.getcwd()\ntmp_path = create_temp_directory()\nprint_debug(\"Working in temp dir '%s'\" % (tmp_path), args.debug)\nos.chdir(tmp_path)\n\n# sort by headword\ndictionary.sort(by_headword=True)\n\n# group by prefix\nfiles_to_compress = []\nprefix_length = int(args.group_by_prefix_length)\nspecial_group, group_keys, group_dict = dictionary.group(\n    prefix_function=get_prefix_kobo,\n    prefix_length=prefix_length,\n    merge_min_size=int(args.group_by_prefix_merge_min_size),\n    merge_across_first=args.group_by_prefix_merge_across_first\n)\nif special_group is not None:\n    special_group_key = u\"1\" * prefix_length\n    group_dict[special_group_key] = special_group\n    group_keys = [special_group_key] + group_keys\n\n# write files\nfor key in group_keys:\n    # write html file\n    file_html_path = key + u\".html\"\n    file_html_obj = io.open(file_html_path, \"wb\")\n    file_html_obj.write(u\"<?xml version=\\\"1.0\\\" encoding=\\\"utf-8\\\"?><html>\".encode(\"utf-8\"))\n    for entry in group_dict[key]:\n        headword = entry.headword\n        definition = entry.definition\n        file_html_obj.write((u\"<w><a name=\\\"%s\\\"/><div><b>%s</b><br/>%s</div></w>\" % (headword, headword, definition)).encode(\"utf-8\"))\n    file_html_obj.write((u\"</html>\").encode(\"utf-8\"))\n    file_html_obj.close()\n\n    # compress in gz format\n    file_html_obj = io.open(file_html_path, \"rb\")\n    file_gz_path = file_html_path + u\".gz\"\n    file_gz_obj = gzip.open(file_gz_path, \"wb\")\n    file_gz_obj.writelines(file_html_obj)\n    file_gz_obj.close()\n    file_html_obj.close()\n\n    # delete .html file\n    delete_file(None, file_html_path)\n    # rename .html.gz file into .html\n    rename_file(file_gz_path, file_html_path)\n    files_to_compress.append(file_html_path)\n\n# write words\nfile_words_path = WORDS_FILE_NAME\nkeys = sorted(dictionary.entries_index.keys())\ntry:\n    import marisa_trie\n    trie = marisa_trie.Trie(keys)\n    trie.save(file_words_path)\n    result = [file_words_path]\nexcept ImportError as exc:\n    # call MARISA with subprocess\n    print_info(\"  MARISA cannot be imported as Python module. You might want to install it with:\")\n    print_info(\"  $ [sudo] pip install marisa_trie\")\n    marisa_build_path = MARISA_BUILD\n    if args.marisa_bin_path is None:\n        print_info(\"  Running '%s' from $PATH\" % MARISA_BUILD)\n    else:\n        marisa_build_path = os.path.join(args.marisa_bin_path, MARISA_BUILD)\n        print_info(\"  Running '%s' from '%s'\" % (MARISA_BUILD, args.marisa_bin_path))\n    # TODO this is ugly, but it works\n    query = (u\"\\n\".join([x for x in keys]) + u\"\\n\").encode(\"utf-8\")\n\n    try:\n        proc = subprocess.Popen(\n            [marisa_build_path, \"-l\", \"-o\", file_words_path],\n            stdout=subprocess.PIPE,\n            stdin=subprocess.PIPE,\n            stderr=subprocess.PIPE\n        )\n        proc.communicate(input=query)[0].decode(\"utf-8\")\n        result = [file_words_path]\n    except OSError as exc:\n        print_error(\"  Unable to run '%s' as '%s'\" % (MARISA_BUILD, marisa_build_path))\n        print_error(\"  Please make sure '%s':\" % MARISA_BUILD)\n        print_error(\"    1. is available on your $PATH or\")\n        print_error(\"    2. specify its path with --marisa-bin-path or\")\n        print_error(\"    3. install the marisa_trie Python module\")\n        result = None\n\nif result is not None:\n    # add file_words_path to files to compress\n    files_to_compress.append(file_words_path)\n    # create output zip file\n    try:\n        print_debug(\"Writing to file '%s'...\" % (output_file_path_absolute), args.debug)\n        file_zip_obj = zipfile.ZipFile(output_file_path_absolute, \"w\", zipfile.ZIP_DEFLATED)\n        for file_to_compress in files_to_compress:\n            file_to_compress = os.path.basename(file_to_compress)\n            file_zip_obj.write(file_to_compress)\n        file_zip_obj.close()\n        result = [output_file_path]\n        print_debug(\"Writing to file '%s'... success\" % (output_file_path_absolute), args.debug)\n    except:\n        print_error(\"Writing to file '%s'... failure\" % (output_file_path_absolute))\n\n# delete tmp directory\nos.chdir(cwd)\nif args.keep:\n    print_info(\"Not deleting temp dir '%s'\" % (tmp_path))\nelse:\n    delete_directory(tmp_path)\n    print_debug(\"Deleted temp dir '%s'\" % (tmp_path), args.debug)\n\nreturn result", "path": "penelope\\format_kobo.py", "repo_name": "pettarin/penelope", "stars": 181, "license": "mit", "language": "python", "size": 12478}
{"docstring": "\"\"\"\nWrite the dictionary to file.\n\nReturn a list of paths, or None if failed.\n\"\"\"\n", "func_signal": "def write_dictionary(dictionary, args):\n", "code": "output_format = args.output_format\nif output_format == \"bookeen\":\n    # NOTE\n    # for bookeen format we cannot add the file extension\n    # as for the other formats, since we might have to generate\n    # a compressed .install file or an uncompressed pair (.dict.idx and .dict)\n    # this is dealt with directly in format_bookeen.py\n    import penelope.format_bookeen\n    return penelope.format_bookeen.write(dictionary, args, args.output_file)\nelif output_format == \"csv\":\n    output_file_path = add_extension(args.output_file, \".csv\")\n    import penelope.format_csv\n    return penelope.format_csv.write(dictionary, args, output_file_path)\nelif output_format == \"epub\":\n    output_file_path = add_extension(args.output_file, \".epub\")\n    import penelope.format_epub\n    return penelope.format_epub.write(dictionary, args, output_file_path)\nelif output_format == \"kobo\":\n    output_file_path = get_kobo_file_path(args)\n    import penelope.format_kobo\n    return penelope.format_kobo.write(dictionary, args, output_file_path)\nelif output_format == \"mobi\":\n    output_file_path = add_extension(args.output_file, \".mobi\")\n    import penelope.format_mobi\n    return penelope.format_mobi.write(dictionary, args, output_file_path)\nelif output_format == \"stardict\":\n    output_file_path = add_extension(args.output_file, \".zip\")\n    import penelope.format_stardict\n    return penelope.format_stardict.write(dictionary, args, output_file_path)\nelif output_format == \"xml\":\n    output_file_path = add_extension(args.output_file, \".xml\")\n    import penelope.format_xml\n    return penelope.format_xml.write(dictionary, args, output_file_path)\nreturn False", "path": "penelope\\dictionary.py", "repo_name": "pettarin/penelope", "stars": 181, "license": "mit", "language": "python", "size": 12478}
{"docstring": "\"\"\"\nImplement IcuNoCase collation for German.\n(I do not remember where the procedure comes from.)\n\n:param string1: first string\n:type  string1: unicode\n:param string2: second string\n:type  string2: unicode\n:rtype: int\n\"\"\"\n", "func_signal": "def collate_function(string1, string2):\n", "code": "b1 = string1.lower()\nb2 = string2.lower()\nfor (r, s) in REPLACEMENTS:\n    b1 = b1.replace(r, s)\n    b2 = b2.replace(r, s)\nb1 = utf_lower(b1, encoding=\"utf-8\", lower=True)\nb2 = utf_lower(b2, encoding=\"utf-8\", lower=True)\nc1 = utf_lower(string1, encoding=\"utf-8\", lower=True)\nc2 = utf_lower(string2, encoding=\"utf-8\", lower=True)\nd1 = utf_lower(b1, encoding=\"utf-16\", lower=False)\nd2 = utf_lower(b2, encoding=\"utf-16\", lower=False)\nif d1 == d2:\n    e1 = utf_lower(c1, encoding=\"utf-16\", lower=False)\n    e2 = utf_lower(c2, encoding=\"utf-16\", lower=False)\n    if e1 == e2:\n        return 0\n    if e1 < e2:\n        return -1\nif d1 < d2:\n    return -1\nreturn 1", "path": "penelope\\collation_german.py", "repo_name": "pettarin/penelope", "stars": 181, "license": "mit", "language": "python", "size": 12478}
{"docstring": "\"\"\"\nAdd a new entry for each synonym,\nusing the definition of the original headword.\nAt the end, reset the current sort order.\n(You will need to sort it later, if interested in having\nthe dictionary sorted by headword and/or definition.)\n\"\"\"\n", "func_signal": "def flatten_synonyms(self):\n", "code": "if not self.has_synonyms:\n    # nothing to do\n    return\n\nfor entry in self.entries:\n    for synonym in entry.get_synonyms():\n        self.add_entry(headword=synonym, definition=entry.definition)\n\nself.sort(False, False, False, False)", "path": "penelope\\dictionary.py", "repo_name": "pettarin/penelope", "stars": 181, "license": "mit", "language": "python", "size": 12478}
{"docstring": "# result to be returned\n", "func_signal": "def write(dictionary, args, output_file_path):\n", "code": "result = None\n\n# get absolute path\noutput_file_path_absolute = os.path.abspath(output_file_path)\n\n# create tmp directory\ncwd = os.getcwd()\ntmp_path = create_temp_directory()\nprint_debug(\"Working in temp dir '%s'\" % (tmp_path), args.debug)\nos.chdir(tmp_path)\n\n# get the basename and compute output file paths\nbase = os.path.basename(output_file_path)\nif base.endswith(\".zip\"):\n    base = base[:-4]\nifo_file_path = base + \".ifo\"\nidx_file_path = base + \".idx\"\ndict_file_path = base + \".dict\"\ndict_dz_file_path = base + \".dict.dz\"\nsyn_file_path = base + \".syn\"\n\n# TODO by spec, the index should be sorted\n# TODO using the comparator stardict_strcmp() defined in the spec\n# TODO (it calls g_ascii_strcasecmp() and/or strcmp() ),\n# TODO or with a user-defined collation function\n#\n# From https://developer.gnome.org/glib/2.28/glib-String-Utility-Functions.html#g-ascii-strcasecmp\n# gint g_ascii_strcasecmp (const gchar *s1, const gchar *s2);\n# Compare two strings, ignoring the case of ASCII characters.\n# Unlike the BSD strcasecmp() function, this only recognizes standard ASCII letters and ignores the locale, treating all non-ASCII bytes as if they are not letters.\n# This function should be used only on strings that are known to be in encodings where the bytes corresponding to ASCII letters always represent themselves. This includes UTF-8 and the ISO-8859-* charsets, but not for instance double-byte encodings like the Windows Codepage 932, where the trailing bytes of double-byte characters include all ASCII letters. If you compare two CP932 strings using this function, you will get false matches.\n#\n# using Python's builtin lower() and sort() by headword\n# should be equivalent for UTF-8 encoded dictionaries (and it is fast)\n#\ndictionary.sort(by_headword=True, ignore_case=True)\n\n# write .idx and .dict files\nprint_debug(\"Writing .idx and .dict files...\", args.debug)\nidx_file_obj = io.open(idx_file_path, \"wb\")\ndict_file_obj = io.open(dict_file_path, \"wb\")\ncurrent_offset = 0\ncurrent_idx_size = 0\nfor entry_index in dictionary.entries_index_sorted:\n    entry = dictionary.entries[entry_index]\n    headword_bytes = entry.headword.encode(\"utf-8\")\n    definition_bytes = entry.definition.encode(\"utf-8\")\n    definition_size = len(definition_bytes)\n    # write .idx\n    idx_file_obj.write(headword_bytes)\n    idx_file_obj.write(b\"\\0\")\n    idx_file_obj.write(struct.pack('>i', current_offset))\n    idx_file_obj.write(struct.pack('>i', definition_size))\n    current_idx_size += (len(headword_bytes) + 1 + 4 + 4)\n    # write .dict\n    dict_file_obj.write(definition_bytes)\n    current_offset += definition_size\nidx_file_obj.close()\ndict_file_obj.close()\nprint_debug(\"Writing .idx and .dict files... done\", args.debug)\n\n# list files to compress\nfiles_to_compress = []\nfiles_to_compress.append(ifo_file_path)\nfiles_to_compress.append(idx_file_path)\n\n# write .syn file\ndict_syns_len = 0\nif dictionary.has_synonyms:\n    if args.ignore_synonyms:\n        print_debug(\"Dictionary has synonyms, but ignoring them\", args.debug)\n    else:\n        print_debug(\"Dictionary has synonyms, writing .syn file...\", args.debug)\n        syn_file_obj = io.open(syn_file_path, \"wb\")\n        dict_syns = dictionary.get_synonyms()\n        dict_syns_len = len(dict_syns)\n        for pair in dict_syns:\n            synonym_bytes = pair[0].encode(\"utf-8\")\n            index = pair[1]\n            syn_file_obj.write(synonym_bytes)\n            syn_file_obj.write(b\"\\0\")\n            syn_file_obj.write(struct.pack('>i', index))\n        syn_file_obj.close()\n        files_to_compress.append(syn_file_path)\n        print_debug(\"Dictionary has synonyms, writing .syn file... done\", args.debug)\n\n# compress .dict file\nif args.sd_no_dictzip:\n    print_debug(\"Not compressing .dict file with dictzip\", args.debug)\n    files_to_compress.append(dict_file_path)\n    result = [dict_file_path]\nelse:\n    try:\n        print_debug(\"Compressing .dict file with dictzip...\", args.debug)\n        dictzip_path = DICTZIP\n        if args.dictzip_path is None:\n            print_info(\"  Running '%s' from $PATH\" % DICTZIP)\n        else:\n            dictzip_path = args.dictzip_path\n            print_info(\"  Running '%s' from '%s'\" % (DICTZIP, dictzip_path))\n        proc = subprocess.Popen(\n            [dictzip_path, \"-k\", dict_file_path],\n            stdout=subprocess.PIPE,\n            stdin=subprocess.PIPE,\n            stderr=subprocess.PIPE\n        )\n        proc.communicate()\n        result = [dict_dz_file_path]\n        files_to_compress.append(dict_dz_file_path)\n        print_debug(\"Compressing .dict file with dictzip... done\", args.debug)\n    except OSError as exc:\n        print_error(\"  Unable to run '%s' as '%s'\" % (DICTZIP, dictzip_path))\n        print_error(\"  Please make sure '%s':\" % DICTZIP)\n        print_error(\"    1. is available on your $PATH or\")\n        print_error(\"    2. specify its path with --dictzip-path or\")\n        print_error(\"    3. specify --no-dictzip to avoid compressing the .dict file\")\n        result = None\n\nif result is not None:\n    # create ifo file\n    ifo_file_obj = io.open(ifo_file_path, \"wb\")\n    ifo_file_obj.write((u\"StarDict's dict ifo file\\n\").encode(\"utf-8\"))\n    ifo_file_obj.write((u\"version=2.4.2\\n\").encode(\"utf-8\"))\n    ifo_file_obj.write((u\"wordcount=%d\\n\" % (len(dictionary))).encode(\"utf-8\"))\n    ifo_file_obj.write((u\"idxfilesize=%d\\n\" % (current_idx_size)).encode(\"utf-8\"))\n    ifo_file_obj.write((u\"bookname=%s\\n\" % (args.title)).encode(\"utf-8\"))\n    ifo_file_obj.write((u\"date=%s\\n\" % (args.year)).encode(\"utf-8\"))\n    ifo_file_obj.write((u\"sametypesequence=m\\n\").encode(\"utf-8\"))\n    ifo_file_obj.write((u\"description=%s\\n\" % (args.description)).encode(\"utf-8\"))\n    ifo_file_obj.write((u\"author=%s\\n\" % (args.author)).encode(\"utf-8\"))\n    ifo_file_obj.write((u\"email=%s\\n\" % (args.email)).encode(\"utf-8\"))\n    ifo_file_obj.write((u\"website=%s\\n\" % (args.website)).encode(\"utf-8\"))\n    if dict_syns_len > 0:\n        ifo_file_obj.write((u\"synwordcount=%d\\n\" % (dict_syns_len)).encode(\"utf-8\"))\n    ifo_file_obj.close()\n\n    # create output zip file\n    try:\n        print_debug(\"Writing to file '%s'...\" % (output_file_path_absolute), args.debug)\n        file_zip_obj = zipfile.ZipFile(output_file_path_absolute, \"w\", zipfile.ZIP_DEFLATED)\n        for file_to_compress in files_to_compress:\n            file_to_compress = os.path.basename(file_to_compress)\n            file_zip_obj.write(file_to_compress)\n            print_debug(\"Written %s\" % (file_to_compress), args.debug)\n        file_zip_obj.close()\n        result = [output_file_path]\n        print_debug(\"Writing to file '%s'... success\" % (output_file_path_absolute), args.debug)\n    except:\n        print_error(\"Writing to file '%s'... failure\" % (output_file_path_absolute))\n\n# delete tmp directory\nos.chdir(cwd)\nif args.keep:\n    print_info(\"Not deleting temp dir '%s'\" % (tmp_path))\nelse:\n    delete_directory(tmp_path)\n    print_debug(\"Deleted temp dir '%s'\" % (tmp_path), args.debug)\n\nreturn result", "path": "penelope\\format_stardict.py", "repo_name": "pettarin/penelope", "stars": 181, "license": "mit", "language": "python", "size": 12478}
{"docstring": "\"\"\"\nMerge definitions of entries with the same headword,\nusing merge_function or merge_separator to create the merged definition.\n\nNote that this function destroys the current dictionary,\nand re-populate its entries (with the new \"merged\" entries).\n\nAs a consequence, it resets the current sort order.\n(You will need to sort it later, if interested in having\nthe dictionary sorted by headword and/or definition.)\n\"\"\"\n\n", "func_signal": "def merge_definitions(self, merge_function=None, merge_separator=None):\n", "code": "def default_merge_function(headword, definitions):\n    \"\"\"\n    Merge definitions for the same headword in a custom way:\n    1 def   => definition\n    2+ defs => definition<SEP>definition<SEP>...\n    \"\"\"\n    return merge_separator.join(definitions)\n\nif (self.has_unique_headwords_only) or ((merge_function is None) and (merge_separator is None)):\n    # nothing to do\n    return\n\nif merge_function is None:\n    # use the default merge function, joining using the merge_separator string\n    merge_function = default_merge_function\n\n# copy previous data\noriginal_entries = self.entries\noriginal_entries_index = self.entries_index\n# delete all\nself.clear()\n# for all (unique) headwords\nfor headword in original_entries_index:\n    original_entries_index_headword = original_entries_index[headword]\n    definitions_to_be_merged = [original_entries[i].definition for i in original_entries_index_headword]\n    merged_definition = merge_function(headword, definitions_to_be_merged)\n    self.add_entry(headword=headword, definition=merged_definition)\n    new_headword_index = len(self) - 1\n    # add synonyms to this new entry, by adding all synonyms of the original entries\n    # merged into this new entry (with index new_headword_index in self.entries)\n    for i in original_entries_index_headword:\n        original_entry_synonyms = original_entries[i].get_synonyms()\n        for synonym in original_entry_synonyms:\n            self.add_synonym(synonym=synonym, headword_index=new_headword_index)\n\n# not needed, since we called self.clear()\n# self.sort(False, False, False, False)", "path": "penelope\\dictionary.py", "repo_name": "pettarin/penelope", "stars": 181, "license": "mit", "language": "python", "size": 12478}
{"docstring": "\"\"\"\nReturn a (list_special, list, dict),\nwhere the list contains the sorted keys of dict,\nand list_special contains the list of SPECIAL entries.\n\"\"\"\n", "func_signal": "def return_triple(groups):\n", "code": "spec = None\nif u\"SPECIAL\" in groups:\n    spec = groups[u\"SPECIAL\"]\n    del groups[u\"SPECIAL\"]\nkeys = sorted(groups.keys())\nreturn (spec, keys, groups)", "path": "penelope\\dictionary.py", "repo_name": "pettarin/penelope", "stars": 181, "license": "mit", "language": "python", "size": 12478}
{"docstring": "# result to be returned\n", "func_signal": "def write(dictionary, args, output_file_path):\n", "code": "result = None\n\n# get absolute path\noutput_file_path_absolute = os.path.abspath(output_file_path)\n\n# sort by headword, optionally ignoring case\ndictionary.sort(by_headword=True, ignore_case=args.sort_ignore_case)\n\n# create groups\nspecial_group, group_keys, group_dict = dictionary.group(\n    prefix_function_path=args.group_by_prefix_function,\n    prefix_length=int(args.group_by_prefix_length),\n    merge_min_size=int(args.group_by_prefix_merge_min_size),\n    merge_across_first=args.group_by_prefix_merge_across_first\n)\nall_group_keys = group_keys\nif special_group is not None:\n    all_group_keys += [u\"SPECIAL\"]\n\n# create epub object\nepub = DictionaryEbook(ebook_format=DictionaryEbook.EPUB2, args=args)\n\n# add groups\nfor key in all_group_keys:\n    if key == u\"SPECIAL\":\n        group_entries = special_group\n    else:\n        group_entries = group_dict[key]\n    epub.add_group(key, group_entries)\n\n# create output file\nif args.epub_no_compress:\n    print_debug(\"Not compressing the EPUB container\")\n    epub.write(output_file_path_absolute, compress=False)\nelse:\n    print_debug(\"Writing to file '%s'...\" % (output_file_path_absolute), args.debug)\n    epub.write(output_file_path_absolute, compress=True)\n    result = [output_file_path]\n    print_debug(\"Writing to file '%s'... done\" % (output_file_path_absolute), args.debug)\n\n# delete tmp directory\ntmp_path = epub.get_tmp_path()\nif args.epub_no_compress:\n    print_info(\"The uncompressed EPUB is inside dir '%s'\" % (tmp_path))\n    result = [tmp_path]\nelif args.keep:\n    print_info(\"Not deleting temp dir '%s'\" % (tmp_path))\n    if result is None:\n        result = [tmp_path]\nelse:\n    epub.delete()\n    print_debug(\"Deleted temp dir '%s'\" % (tmp_path), args.debug)\n\nreturn result", "path": "penelope\\format_epub.py", "repo_name": "pettarin/penelope", "stars": 181, "license": "mit", "language": "python", "size": 12478}
{"docstring": "# result flag\n", "func_signal": "def read_single_file(dictionary, args, input_file_path):\n", "code": "result = False\n\n# create a tmp file\ntmp_handler, tmp_path = create_temp_file()\n\n# copy the index file from the zip to the tmp file\ninput_file_obj = zipfile.ZipFile(input_file_path)\ntmp_file_obj = io.open(tmp_path, \"wb\")\ntmp_file_obj.write(input_file_obj.read(WORDS_FILE_NAME))\ntmp_file_obj.close()\ninput_file_obj.close()\n\n# read index with MARISA\ntry:\n    # call MARISA with marisa_trie module\n    import marisa_trie\n    trie = marisa_trie.Trie()\n    trie.load(tmp_path)\n    for pair in trie.items():\n        dictionary.add_entry(headword=pair[0], definition=u\"\")\n    result = True\nexcept ImportError as exc:\n    # call MARISA with subprocess\n    print_info(\"  MARISA cannot be imported as Python module. You might want to install it with:\")\n    print_info(\"  $ [sudo] pip install marisa_trie\")\n    marisa_reverse_lookup_path = MARISA_REVERSE_LOOKUP\n    if args.marisa_bin_path is None:\n        print_info(\"  Running '%s' from $PATH\" % MARISA_REVERSE_LOOKUP)\n    else:\n        marisa_reverse_lookup_path = os.path.join(args.marisa_bin_path, MARISA_REVERSE_LOOKUP)\n        print_info(\"  Running '%s' from '%s'\" % (MARISA_REVERSE_LOOKUP, args.marisa_bin_path))\n    # TODO this is ugly, but it works\n    query = (u\"\\n\".join([str(x) for x in range(int(args.marisa_index_size))]) + u\"\\n\").encode(\"utf-8\")\n\n    try:\n        proc = subprocess.Popen(\n            [marisa_reverse_lookup_path, tmp_path],\n            stdout=subprocess.PIPE,\n            stdin=subprocess.PIPE,\n            stderr=subprocess.PIPE\n        )\n        stdout = proc.communicate(input=query)[0].decode(\"utf-8\")\n        for line in stdout.splitlines():\n            array = line.split(\"\\t\")\n            if len(array) >= 2:\n                key = array[1]\n                if args.ignore_case:\n                    key = key.lower()\n                dictionary.add_entry(headword=key, definition=u\"\")\n        result = True\n    except OSError as exc:\n        print_error(\"  Unable to run '%s' as '%s'\" % (MARISA_REVERSE_LOOKUP, marisa_reverse_lookup_path))\n        print_error(\"  Please make sure '%s':\" % MARISA_REVERSE_LOOKUP)\n        print_error(\"    1. is available on your $PATH or\")\n        print_error(\"    2. specify its path with --marisa-bin-path or\")\n        print_error(\"    3. install the marisa_trie Python module\")\nexcept:\n    print_debug(\"Reading from file '%s'... failed\" % (input_file_path))\n\n# delete the tmp file\ndelete_file(tmp_handler, tmp_path)\nreturn result", "path": "penelope\\format_kobo.py", "repo_name": "pettarin/penelope", "stars": 181, "license": "mit", "language": "python", "size": 12478}
{"docstring": "\"\"\"\nGiven the input dictionary and arguments,\nreturn a (possibly, modified) dictionary.\n\nThe returned dictionary might be the same (input) instance,\nor a new one.\n\n:param dictionary: the (raw) input dictionary\n:type  dictionary: Dictionary\n:param arguments: the command line arguments\n:type  arguments: Namespace from argparse\n:rtype: Dictionary\n\"\"\"\n\n", "func_signal": "def parse(dictionary, arguments):\n", "code": "def clean_definition(entry):\n    \"\"\"\n    Remove any prefix of type \"<k>...</k>\"\n    from the entry.definition string.\n    \"\"\"\n    pos = entry.definition.find(\"</k>\")\n    if pos > -1:\n        entry.definition = entry.definition[(pos + len(\"</k>\")):].strip()\n\ndef custom_merge_function(headword, definitions):\n    \"\"\"\n    Merge definitions for the same headword in a custom way:\n    1 def   => <b>word<b>\n                <f>&nbsp;&ns;&nbsp;</f>definition\n    2+ defs => <b>word<b>\n                <f>&nbsp;&ns;&nbsp;</f><b>(1)&nbsp;definition\n                <f>&nbsp;&ns;&nbsp;</f><b>(2)&nbsp;definition...\n    \"\"\"\n    if len(definitions) > 1:\n        clean = u\"<b>%s<b>\" % (headword)\n        for i in range(len(definitions)):\n            clean += \"<f>&nbsp;&ns;&nbsp;</f><b>(%d)</b>&nbsp;%s\" % ((i + 1), definitions[i])\n        return clean\n    if len(definitions) == 1:\n        return u\"<b>%s<b><f>&nbsp;&ns;&nbsp;</f>%s\" % (headword, definitions[0])\n    return u\"\"\n\nif dictionary is None:\n    return None\nfor entry in dictionary.entries:\n    clean_definition(entry)\ndictionary.merge_definitions(merge_function=custom_merge_function)\nreturn dictionary", "path": "penelope\\input_parser_webster.py", "repo_name": "pettarin/penelope", "stars": 181, "license": "mit", "language": "python", "size": 12478}
{"docstring": "# create tmp directory\n", "func_signal": "def read_single_dict(dictionary, args, single_dict):\n", "code": "tmp_path = create_temp_directory()\nprint_debug(\"Working in temp dir '%s'\" % (tmp_path), args.debug)\n\nif len(single_dict) == 1:\n    print_debug(\"Unzipping .install file...\", args.debug)\n    zip_file_path = single_dict[0]\n    idx_file_path = os.path.join(tmp_path, \"d.dict.idx\")\n    dict_file_path = os.path.join(tmp_path, \"d.dict\")\n    zip_file_obj = zipfile.ZipFile(zip_file_path, \"r\")\n    for entry in zip_file_obj.namelist():\n        if entry.endswith(\".dict.idx\"):\n            zip_entry = zip_file_obj.open(entry)\n            idx_file_obj = io.open(idx_file_path, \"wb\")\n            idx_file_obj.write(zip_entry.read())\n            idx_file_obj.close()\n            zip_entry.close()\n        elif entry.endswith(\".dict\"):\n            zip_entry = zip_file_obj.open(entry)\n            dict_file_obj = io.open(dict_file_path, \"wb\")\n            dict_file_obj.write(zip_entry.read())\n            dict_file_obj.close()\n            zip_entry.close()\n    zip_file_obj.close()\n    print_debug(\"Unzipping .install file... done\", args.debug)\nelse:\n    print_debug(\"Files .dict.idx and .dict already uncompressed...\", args.debug)\n    idx_file_path = single_dict[0]\n    dict_file_path = single_dict[1]\n    for file_path in [idx_file_path, dict_file_path]:\n        if not os.path.exists(file_path):\n            print_error(\"File '%s' does not exist\" % file_path)\n            return False\n    print_debug(\"Files .dict.idx and .dict already uncompressed... done\", args.debug)\n\n# unzip .dict file into tmp_path\nprint_debug(\"Unzipping .dict file...\", args.debug)\nzip_file_obj = zipfile.ZipFile(dict_file_path, \"r\")\nfor entry in zip_file_obj.namelist():\n    if not entry.endswith(\"/\"):\n        zip_entry = zip_file_obj.open(entry)\n        entry_file_path = os.path.join(tmp_path, os.path.basename(entry))\n        entry_file_obj = io.open(entry_file_path, \"wb\")\n        entry_file_obj.write(zip_entry.read())\n        entry_file_obj.close()\n        zip_entry.close()\nzip_file_obj.close()\nprint_debug(\"Unzipping .dict file... done\", args.debug)\n\n# read .dict.idx\nprint_debug(\"Reading .dict.idx file...\", args.debug)\nsql_connection = sqlite3.connect(idx_file_path)\nsql_cursor = sql_connection.cursor()\nsql_cursor.execute(\"select * from T_DictIndex\")\nindex_data = sql_cursor.fetchall()\nchunk_index_to_entries = {}\nmax_chunk_index = 1\nfor index_entry in index_data:\n    headword = index_entry[1]\n    if args.ignore_case:\n        headword = headword.lower()\n    offset = index_entry[2]\n    size = index_entry[3]\n    chunk_index = index_entry[4]\n    if chunk_index not in chunk_index_to_entries:\n        chunk_index_to_entries[chunk_index] = []\n    if chunk_index > max_chunk_index:\n        max_chunk_index = chunk_index\n    chunk_index_to_entries[chunk_index].append([headword, offset, size])\nsql_cursor.close()\nsql_connection.close()\nprint_debug(\"Reading .dict.idx file... done\", args.debug)\n\n# read c_* files\nprint_debug(\"Reading c_* files...\", args.debug)\nfor chunk_index in range(1, max_chunk_index + 1):\n    print_debug(\"  Reading c_%d file...\" % (chunk_index), args.debug)\n    chunk_file_path = os.path.join(tmp_path, \"%s%d\" % (CHUNK_FILE_PREFIX, chunk_index))\n    chunk_file_obj = io.open(chunk_file_path, \"rb\")\n    for entry in chunk_index_to_entries[chunk_index]:\n        headword = entry[0]\n        offset = entry[1]\n        size = entry[2]\n        chunk_file_obj.seek(offset)\n        definition_bytes = chunk_file_obj.read(size)\n        definition_unicode = definition_bytes.decode(args.input_file_encoding)\n        dictionary.add_entry(headword=headword, definition=definition_unicode)\n    chunk_file_obj.close()\n    print_debug(\"  Reading c_%d file... done\" % (chunk_index), args.debug)\nprint_debug(\"Reading c_* files... done\", args.debug)\n\n# delete tmp directory\nif args.keep:\n    print_info(\"Not deleting temp dir '%s'\" % (tmp_path))\nelse:\n    delete_directory(tmp_path)\n    print_debug(\"Deleted temp dir '%s'\" % (tmp_path), args.debug)\nreturn True", "path": "penelope\\format_bookeen.py", "repo_name": "pettarin/penelope", "stars": 181, "license": "mit", "language": "python", "size": 12478}
{"docstring": "# result to be returned\n", "func_signal": "def write(dictionary, args, output_file_path):\n", "code": "result = None\n\n# get absolute path\noutput_file_path_absolute = os.path.abspath(output_file_path)\n\n# get absolute path for collation function file\nbookeen_collation_function_path = None\nif args.bookeen_collation_function is not None:\n    bookeen_collation_function_path = os.path.abspath(args.bookeen_collation_function)\n\n# create tmp directory\ncwd = os.getcwd()\ntmp_path = create_temp_directory()\nprint_debug(\"Working in temp dir '%s'\" % (tmp_path), args.debug)\nos.chdir(tmp_path)\n\n# get the basename\nbase = os.path.basename(output_file_path)\nif base.endswith(\".zip\"):\n    base = base[:-4]\n\n# copy empty.idx into tmp_path\nidx_file_path = base + u\".dict.idx\"\ndict_file_path = base + u\".dict\"\ncopy_file(EMPTY_FILE_PATH, idx_file_path)\n\n# open index\nsql_connection = sqlite3.connect(idx_file_path)\n\n# install collation in the index\ncollation_function = collate_function_default\nif bookeen_collation_function_path is not None:\n    try:\n        collation_function = imp.load_source(\"\", bookeen_collation_function_path).collate_function\n        print_debug(\"Using collation function from '%s'\" % (bookeen_collation_function_path), args.debug)\n    except:\n        print_error(\"Unable to load collation function from '%s'. Using the default collation function instead.\" % (bookeen_collation_function_path))\nsql_connection.create_collation(\"IcuNoCase\", collation_function)\nsql_connection.text_factory = str\n\n# get a cursor and delete any data from the index file\nsql_cursor = sql_connection.cursor()\nsql_cursor.execute(\"delete from T_DictIndex\")\n\n# write c_* files\n# each c_* file has MAX_CHUNK_SIZE < size <= (MAX_CHUNK_SIZE * 2) bytes (tentatively)\nprint_debug(\"Writing c_* files...\", args.debug)\nfiles_to_compress = []\ncurrent_offset = 0\nchunk_index = 1\nchunk_file_path = \"%s%d\" % (CHUNK_FILE_PREFIX, chunk_index)\nfiles_to_compress.append(chunk_file_path)\nchunk_file_obj = io.open(chunk_file_path, \"wb\")\nfor entry_index in dictionary.entries_index_sorted:\n    entry = dictionary.entries[entry_index]\n    definition_bytes = entry.definition.encode(\"utf-8\")\n    definition_size = len(definition_bytes)\n    chunk_file_obj.write(definition_bytes)\n    # insert headword into index file\n    sql_tuple = (0, entry.headword, current_offset, definition_size, chunk_index)\n    sql_cursor.execute(\"insert into T_DictIndex values (?,?,?,?,?)\", sql_tuple)\n    # insert synonyms into index file\n    if not args.ignore_synonyms:\n        for synonym in entry.get_synonyms():\n            sql_tuple = (0, synonym[0], current_offset, definition_size, chunk_index)\n            sql_cursor.execute(\"insert into T_DictIndex values (?,?,?,?,?)\", sql_tuple)\n    # update offset\n    current_offset += definition_size\n    # if we reached CHUNK_SIZE, open the next c_* file\n    if current_offset > CHUNK_SIZE:\n        chunk_file_obj.close()\n        chunk_index += 1\n        chunk_file_path = \"%s%d\" % (CHUNK_FILE_PREFIX, chunk_index)\n        files_to_compress.append(chunk_file_path)\n        chunk_file_obj = io.open(chunk_file_path, \"wb\")\n        current_offset = 0\nchunk_file_obj.close()\nprint_debug(\"Writing c_* files... done\", args.debug)\n\n# compress\nprint_debug(\"Compressing c_* files...\", args.debug)\nfile_zip_obj = zipfile.ZipFile(dict_file_path, \"w\", zipfile.ZIP_DEFLATED)\nfor file_to_compress in files_to_compress:\n    file_to_compress = os.path.basename(file_to_compress)\n    file_zip_obj.write(file_to_compress)\nfile_zip_obj.close()\nprint_debug(\"Compressing c_* files... done\", args.debug)\n\n# update index metadata\nprint_debug(\"Updating index metadata...\", args.debug)\nheader = HEADER % (args.language_from)\nsql_cursor.execute(\"update T_DictInfo set F_xhtmlHeader=?\", (header,))\nsql_cursor.execute(\"update T_DictInfo set F_LangFrom=?\", (args.language_from,))\nsql_cursor.execute(\"update T_DictInfo set F_LangTo=?\", (args.language_to,))\nsql_cursor.execute(\"update T_DictInfo set F_Licence=?\", (args.license,))\nsql_cursor.execute(\"update T_DictInfo set F_Copyright=?\", (args.copyright,))\nsql_cursor.execute(\"update T_DictInfo set F_Title=?\", (args.title,))\nsql_cursor.execute(\"update T_DictInfo set F_Description=?\", (args.description,))\nsql_cursor.execute(\"update T_DictInfo set F_Year=?\", (args.year,))\n# the meaning of the following is unknown\nsql_cursor.execute(\"update T_DictInfo set F_Alphabet=?\", (\"Z\",))\nsql_cursor.execute(\"update T_DictInfo set F_CollationLevel=?\", (\"1\",))\nsql_cursor.execute(\"update T_DictVersion set F_DictType=?\", (\"stardict\",))\nsql_cursor.execute(\"update T_DictVersion set F_Version=?\", (\"11\",))\nprint_debug(\"Updating index metadata... done\", args.debug)\n\n# compact and close\nsql_cursor.execute(\"vacuum\")\nsql_cursor.close()\nsql_connection.close()\n\n# create .install file or copy .dict.idx and .dict into requested output directory\nparent_output_directory = os.path.split(output_file_path_absolute)[0]\nif args.bookeen_install_file:\n    print_debug(\"Creating .install file...\", args.debug)\n    file_zip_path = os.path.join(parent_output_directory, base + u\".install\")\n    file_zip_obj = zipfile.ZipFile(file_zip_path, \"w\", zipfile.ZIP_DEFLATED)\n    for file_to_compress in [dict_file_path, idx_file_path]:\n        file_to_compress = os.path.basename(file_to_compress)\n        file_zip_obj.write(file_to_compress)\n    file_zip_obj.close()\n    result = [file_zip_path]\n    print_debug(\"Creating .install file... done\", args.debug)\nelse:\n    print_debug(\"Copying .dict.idx and .dict files...\", args.debug)\n    dict_file_path_final = os.path.join(parent_output_directory, os.path.basename(dict_file_path))\n    idx_file_path_final = os.path.join(parent_output_directory, os.path.basename(idx_file_path))\n    copy_file(dict_file_path, dict_file_path_final)\n    copy_file(idx_file_path, idx_file_path_final)\n    result = [idx_file_path_final, dict_file_path_final]\n    print_debug(\"Copying .dict.idx and .dict files... done\", args.debug)\n\n# delete tmp directory\nos.chdir(cwd)\nif args.keep:\n    print_info(\"Not deleting temp dir '%s'\" % (tmp_path))\nelse:\n    delete_directory(tmp_path)\n    print_debug(\"Deleted temp dir '%s'\" % (tmp_path), args.debug)\n\nreturn result", "path": "penelope\\format_bookeen.py", "repo_name": "pettarin/penelope", "stars": 181, "license": "mit", "language": "python", "size": 12478}
{"docstring": "\"\"\"\nRead the input dictionary from the input files specified\nin the given arguments.\n\nReturn a Dictionary, or None if failed.\n\"\"\"\n", "func_signal": "def read_dictionary(args):\n", "code": "metadata = DictionaryMetadata(\n    identifier_string=args.identifier,\n    author_string=args.author,\n    email_string=args.email,\n    website_string=args.website,\n    title_string=args.title,\n    copyright_string=args.copyright,\n    license_string=args.license,\n    year_string=args.year,\n    language_from=args.language_from,\n    language_to=args.language_to,\n    description_string=args.description\n)\ndictionary = Dictionary(metadata=metadata)\ninput_format = args.input_format\nif input_format == \"bookeen\":\n    # NOTE\n    # for bookeen format we cannot prepare the file paths\n    # as for the other formats, since we might have a compressed .install file\n    # or an uncompressed pair (.dict.idx and .dict)\n    # this is dealt with directly in format_bookeen.py\n    import penelope.format_bookeen\n    return penelope.format_bookeen.read(dictionary, args, args.input_file)\nelif input_format == \"csv\":\n    input_file_paths = prepare_file_paths(args.input_file, \".csv\")\n    if input_file_paths is None:\n        return None\n    import penelope.format_csv\n    return penelope.format_csv.read(dictionary, args, input_file_paths)\nelif input_format == \"kobo\":\n    input_file_paths = prepare_file_paths(args.input_file, \".zip\")\n    if input_file_paths is None:\n        return None\n    import penelope.format_kobo\n    return penelope.format_kobo.read(dictionary, args, input_file_paths)\nelif input_format == \"stardict\":\n    input_file_paths = prepare_file_paths(args.input_file, \".zip\")\n    if input_file_paths is None:\n        return None\n    import penelope.format_stardict\n    return penelope.format_stardict.read(dictionary, args, input_file_paths)\nelif input_format == \"xml\":\n    input_file_paths = prepare_file_paths(args.input_file, \".xml\")\n    if input_file_paths is None:\n        return None\n    import penelope.format_xml\n    return penelope.format_xml.read(dictionary, args, input_file_paths)\nreturn dictionary", "path": "penelope\\dictionary.py", "repo_name": "pettarin/penelope", "stars": 181, "license": "mit", "language": "python", "size": 12478}
{"docstring": "\"\"\"\nImplement default IcuNoCase collation,\nby simply lowercasing the UTF-8 encoded versions\nof the two strings.\n\n:param string1: first string\n:type  string1: unicode\n:param string2: second string\n:type  string2: unicode\n:rtype: int\n\"\"\"\n", "func_signal": "def collate_function(string1, string2):\n", "code": "b1 = utf_lower(string1, encoding=\"utf-8\", lower=True)\nb2 = utf_lower(string2, encoding=\"utf-8\", lower=True)\nif b1 == b2:\n    return 0\nelif b1 < b2:\n    return -1\nreturn 1", "path": "penelope\\collation_default.py", "repo_name": "pettarin/penelope", "stars": 181, "license": "mit", "language": "python", "size": 12478}
{"docstring": "\"\"\"\nReturn the prefix for the given headword,\nof length length.\n\nNote that the procedure implemented here is the result\nof reverse engineering, since no official specification\nhas been published by Kobo so far. YMMV.\n\n:param headword: the headword string\n:type  headword: unicode\n:param length: prefix length\n:type  length: int\n:rtype: unicode\n\"\"\"\n", "func_signal": "def get_prefix(headword, length):\n", "code": "def is_allowed(character):\n    # all non-ascii (x > 127) are ok\n    # all ASCII lowercase letters (97 <= x <= 122) are ok\n    # everything else is not ok\n    try:\n        code = ord(character)\n        return (code > 127) or ((code >= 97) and (code <= 122))\n    except:\n        pass\n    return True\n\n# defaults to u\"SPECIAL\", it will be mapped to u\"11...1\" later\nprefix = u\"SPECIAL\"\nheadword = headword.lower()\nif len(headword) > 0:\n    while len(headword) < length:\n        # for headwords shorter than length, append an 'a' at the end\n        # e.g. length=3, \"xy\" => \"xya\"\n        headword += u\"a\"\n    # TODO maybe the check should be done only for the first character\n    is_ok = True\n    for character in headword:\n        if not is_allowed(character):\n            is_ok = False\n            break\n    if is_ok:\n        prefix = headword[0:length]\nreturn prefix", "path": "penelope\\prefix_kobo.py", "repo_name": "pettarin/penelope", "stars": 181, "license": "mit", "language": "python", "size": 12478}
{"docstring": "\"\"\"\nReturn the prefix for the given headword,\nof length length.\n\n:param headword: the headword string\n:type  headword: unicode\n:param length: prefix length\n:type  length: int\n:rtype: unicode\n\"\"\"\n", "func_signal": "def get_prefix(headword, length):\n", "code": "if headword is None:\n    return None\nlowercased = headword.lower()\nif ord(lowercased[0]) < 97:\n    return u\"SPECIAL\"\nif len(lowercased) < length:\n    return lowercased\nreturn lowercased[0:length]", "path": "penelope\\prefix_default.py", "repo_name": "pettarin/penelope", "stars": 181, "license": "mit", "language": "python", "size": 12478}
{"docstring": "# get cover path\n", "func_signal": "def write(self, file_path_absolute, compress=True):\n", "code": "cover_path_absolute = self.args.cover_path\nif cover_path_absolute is not None:\n    cover_path_absolute = os.path.abspath(cover_path_absolute)\n\n# get custom css path\ncustom_css_path_absolute = self.args.apply_css\nif custom_css_path_absolute is not None:\n    custom_css_path_absolute = os.path.abspath(custom_css_path_absolute)\n\n# create new tmp directory and cd there\nself.root_directory_path = create_temp_directory()\ncwd = os.getcwd()\nos.chdir(self.root_directory_path)\nos.makedirs(u\"META-INF\")\nos.makedirs(u\"OEBPS\")\n\n# add mimetype and container.xml\nif self.ebook_format in [self.EPUB2]:   # add EPUB3 here\n    self.add_file(u\"mimetype\", self.MIMETYPE_CONTENTS, mode=zipfile.ZIP_STORED)\n    self.add_file(u\"META-INF/container.xml\", self.CONTAINER_XML_CONTENTS)\n\n# add cover\nself.write_cover(cover_path_absolute)\n\n# write CSS\nself.write_css(custom_css_path_absolute)\n\n# write index\nif self.args.include_index_page:\n    self.write_index()\n\n# write groups\nself.write_groups()\n\n# write ncx\nif self.ebook_format in [self.EPUB2]:   # add EPUB3 here\n    self.write_ncx()\n\n# write opf\nself.write_opf()\n\n# compress\nif compress:\n    output_file_obj = zipfile.ZipFile(file_path_absolute, \"w\", compression=zipfile.ZIP_DEFLATED)\n    for file_to_compress in self.files:\n        output_file_obj.write(file_to_compress[\"path\"], compress_type=file_to_compress[\"mode\"])\n    output_file_obj.close()\n\n# return to previous cwd\nos.chdir(cwd)", "path": "penelope\\dictionary_ebook.py", "repo_name": "pettarin/penelope", "stars": 181, "license": "mit", "language": "python", "size": 12478}
{"docstring": "\"\"\"\nverify that a password auth attempt will fallback to \"interactive\"\nif password auth isn't supported but interactive is.\n\"\"\"\n", "func_signal": "def test_5_interactive_auth_fallback(self):\n", "code": "self.start_server()\nself.tc.connect(hostkey=self.public_host_key)\nremain = self.tc.auth_password('commie', 'cat')\nself.assertEquals([], remain)\nself.verify_finished()", "path": "tests\\test_auth.py", "repo_name": "bitprophet/ssh", "stars": 143, "license": "lgpl-2.1", "language": "python", "size": 1797}
{"docstring": "\"\"\"\nverify keyboard-interactive auth works.\n\"\"\"\n", "func_signal": "def test_4_interactive_auth(self):\n", "code": "self.start_server()\nself.tc.connect(hostkey=self.public_host_key)\n\ndef handler(title, instructions, prompts):\n    self.got_title = title\n    self.got_instructions = instructions\n    self.got_prompts = prompts\n    return ['cat']\nremain = self.tc.auth_interactive('commie', handler)\nself.assertEquals(self.got_title, 'password')\nself.assertEquals(self.got_prompts, [('Password', False)])\nself.assertEquals([], remain)\nself.verify_finished()", "path": "tests\\test_auth.py", "repo_name": "bitprophet/ssh", "stars": 143, "license": "lgpl-2.1", "language": "python", "size": 1797}
{"docstring": "\"\"\"\nCheck to see if there is a \"Pageant\" agent we can talk to.\n\nThis checks both if we have the required libraries (win32all or ctypes)\nand if there is a Pageant currently running.\n\"\"\"\n", "func_signal": "def can_talk_to_agent():\n", "code": "if (_has_win32all or _has_ctypes) and _get_pageant_window_object():\n    return True\nreturn False", "path": "ssh\\win_pageant.py", "repo_name": "bitprophet/ssh", "stars": 143, "license": "lgpl-2.1", "language": "python", "size": 1797}
{"docstring": "\"\"\"\nLoad host keys from a system (read-only) file.  Host keys read with\nthis method will not be saved back by L{save_host_keys}.\n\nThis method can be called multiple times.  Each new set of host keys\nwill be merged with the existing set (new replacing old if there are\nconflicts).\n\nIf C{filename} is left as C{None}, an attempt will be made to read\nkeys from the user's local \"known hosts\" file, as used by OpenSSH,\nand no exception will be raised if the file can't be read.  This is\nprobably only useful on posix.\n\n@param filename: the filename to read, or C{None}\n@type filename: str\n\n@raise IOError: if a filename was provided and the file could not be\n    read\n\"\"\"\n", "func_signal": "def load_system_host_keys(self, filename=None):\n", "code": "if filename is None:\n    # try the user's .ssh key file, and mask exceptions\n    filename = os.path.expanduser('~/.ssh/known_hosts')\n    try:\n        self._system_host_keys.load(filename)\n    except IOError:\n        pass\n    return\nself._system_host_keys.load(filename)", "path": "ssh\\client.py", "repo_name": "bitprophet/ssh", "stars": 143, "license": "lgpl-2.1", "language": "python", "size": 1797}
{"docstring": "\"\"\"\nverify that we catch a server disconnecting during auth, and report\nit as an auth failure.\n\"\"\"\n", "func_signal": "def test_8_auth_gets_disconnected(self):\n", "code": "self.start_server()\nself.tc.connect(hostkey=self.public_host_key)\ntry:\n    remain = self.tc.auth_password('bad-server', 'hello')\nexcept:\n    etype, evalue, etb = sys.exc_info()\n    self.assert_(issubclass(etype, AuthenticationException))", "path": "tests\\test_auth.py", "repo_name": "bitprophet/ssh", "stars": 143, "license": "lgpl-2.1", "language": "python", "size": 1797}
{"docstring": "\"\"\"\nStart an interactive shell session on the SSH server.  A new L{Channel}\nis opened and connected to a pseudo-terminal using the requested\nterminal type and size.\n\n@param term: the terminal type to emulate (for example, C{\"vt100\"})\n@type term: str\n@param width: the width (in characters) of the terminal window\n@type width: int\n@param height: the height (in characters) of the terminal window\n@type height: int\n@return: a new channel connected to the remote shell\n@rtype: L{Channel}\n\n@raise SSHException: if the server fails to invoke a shell\n\"\"\"\n", "func_signal": "def invoke_shell(self, term='vt100', width=80, height=24):\n", "code": "chan = self._transport.open_session()\nchan.get_pty(term, width, height)\nchan.invoke_shell()\nreturn chan", "path": "ssh\\client.py", "repo_name": "bitprophet/ssh", "stars": 143, "license": "lgpl-2.1", "language": "python", "size": 1797}
{"docstring": "\"\"\"\nverify that read(-1) returns everything left in the file.\n\"\"\"\n", "func_signal": "def test_7_read_all(self):\n", "code": "f = LoopbackFile('r+', 16)\nf.write('The first thing you need to do is open your eyes. ')\nf.write('Then, you need to close them again.\\n')\ns = f.read(-1)\nself.assertEqual(s, 'The first thing you need to do is open your eyes. Then, you ' +\n                 'need to close them again.\\n')\nf.close()", "path": "tests\\test_file.py", "repo_name": "bitprophet/ssh", "stars": 143, "license": "lgpl-2.1", "language": "python", "size": 1797}
{"docstring": "\"\"\"\nverify that flushing happens automatically on buffer crossing.\n\"\"\"\n", "func_signal": "def test_6_buffering(self):\n", "code": "f = LoopbackFile('r+', 16)\nf.write('Too small.')\nself.assertEqual(f.read(4), '')\nf.write('  ')\nself.assertEqual(f.read(4), '')\nf.write('Enough.')\nself.assertEqual(f.read(20), 'Too small.  Enough.')\nf.close()", "path": "tests\\test_file.py", "repo_name": "bitprophet/ssh", "stars": 143, "license": "lgpl-2.1", "language": "python", "size": 1797}
{"docstring": "# generate an \"x\" (1 < x < (p-1)/2).\n", "func_signal": "def _generate_x(self):\n", "code": "q = (self.p - 1) // 2\nqnorm = util.deflate_long(q, 0)\nqhbyte = ord(qnorm[0])\nbytes = len(qnorm)\nqmask = 0xff\nwhile not (qhbyte & 0x80):\n    qhbyte <<= 1\n    qmask >>= 1\nwhile True:\n    x_bytes = self.transport.rng.read(bytes)\n    x_bytes = chr(ord(x_bytes[0]) & qmask) + x_bytes[1:]\n    x = util.inflate_long(x_bytes, 1)\n    if (x > 1) and (x < q):\n        break\nself.x = x", "path": "ssh\\kex_gex.py", "repo_name": "bitprophet/ssh", "stars": 143, "license": "lgpl-2.1", "language": "python", "size": 1797}
{"docstring": "# same as above, but without min_bits or max_bits (used by older clients like putty)\n", "func_signal": "def _parse_kexdh_gex_request_old(self, m):\n", "code": "self.preferred_bits = m.get_int()\n# smoosh the user's preferred size into our own limits\nif self.preferred_bits > self.max_bits:\n    self.preferred_bits = self.max_bits\nif self.preferred_bits < self.min_bits:\n    self.preferred_bits = self.min_bits\n# generate prime\npack = self.transport._get_modulus_pack()\nif pack is None:\n    raise SSHException('Can\\'t do server-side gex with no modulus pack')\nself.transport._log(DEBUG, 'Picking p (~ %d bits)' % (self.preferred_bits,))\nself.g, self.p = pack.get_modulus(self.min_bits, self.preferred_bits, self.max_bits)\nm = Message()\nm.add_byte(chr(_MSG_KEXDH_GEX_GROUP))\nm.add_mpint(self.p)\nm.add_mpint(self.g)\nself.transport._send_message(m)\nself.transport._expect_packet(_MSG_KEXDH_GEX_INIT)\nself.old_style = True", "path": "ssh\\kex_gex.py", "repo_name": "bitprophet/ssh", "stars": 143, "license": "lgpl-2.1", "language": "python", "size": 1797}
{"docstring": "\"\"\"\ntry to trick the linefeed detector.\n\"\"\"\n", "func_signal": "def test_3_lf(self):\n", "code": "f = LoopbackFile('r+U')\nf.write('First line.\\r')\nself.assertEqual(f.readline(), 'First line.\\n')\nf.write('\\nSecond.\\r\\n')\nself.assertEqual(f.readline(), 'Second.\\n')\nf.close()\nself.assertEqual(f.newlines, '\\r\\n')", "path": "tests\\test_file.py", "repo_name": "bitprophet/ssh", "stars": 143, "license": "lgpl-2.1", "language": "python", "size": 1797}
{"docstring": "\"\"\"\nverify that write buffering is on.\n\"\"\"\n", "func_signal": "def test_4_write(self):\n", "code": "f = LoopbackFile('r+', 1)\nf.write('Complete line.\\nIncomplete line.')\nself.assertEqual(f.readline(), 'Complete line.\\n')\nself.assertEqual(f.readline(), '')\nf.write('..\\n')\nself.assertEqual(f.readline(), 'Incomplete line...\\n')\nf.close()", "path": "tests\\test_file.py", "repo_name": "bitprophet/ssh", "stars": 143, "license": "lgpl-2.1", "language": "python", "size": 1797}
{"docstring": "\"\"\"\nReturn the canonical form of a path on the server.  For example,\nif the server's home folder is C{/home/foo}, the path\nC{\"../betty\"} would be canonicalized to C{\"/home/betty\"}.  Note\nthe obvious security issues: if you're serving files only from a\nspecific folder, you probably don't want this method to reveal path\nnames outside that folder.\n\nYou may find the python methods in C{os.path} useful, especially\nC{os.path.normpath} and C{os.path.realpath}.\n\nThe default implementation returns C{os.path.normpath('/' + path)}.\n\"\"\"\n", "func_signal": "def canonicalize(self, path):\n", "code": "if os.path.isabs(path):\n    out = os.path.normpath(path)\nelse:\n    out = os.path.normpath('/' + path)\nif sys.platform == 'win32':\n    # on windows, normalize backslashes to sftp/posix format\n    out = out.replace('\\\\', '/')\nreturn out", "path": "ssh\\sftp_si.py", "repo_name": "bitprophet/ssh", "stars": 143, "license": "lgpl-2.1", "language": "python", "size": 1797}
{"docstring": "\"\"\"\nSave the host keys back to a file.  Only the host keys loaded with\nL{load_host_keys} (plus any added directly) will be saved -- not any\nhost keys loaded with L{load_system_host_keys}.\n\n@param filename: the filename to save to\n@type filename: str\n\n@raise IOError: if the file could not be written\n\"\"\"\n", "func_signal": "def save_host_keys(self, filename):\n", "code": "f = open(filename, 'w')\nf.write('# SSH host keys collected by ssh\\n')\nfor hostname, keys in self._host_keys.iteritems():\n    for keytype, key in keys.iteritems():\n        f.write('%s %s %s\\n' % (hostname, keytype, key.get_base64()))\nf.close()", "path": "ssh\\client.py", "repo_name": "bitprophet/ssh", "stars": 143, "license": "lgpl-2.1", "language": "python", "size": 1797}
{"docstring": "\"\"\"\nverify that non-utf-8 encoded passwords can be used for broken\nservers.\n\"\"\"\n", "func_signal": "def test_7_auth_non_utf8(self):\n", "code": "self.start_server()\nself.tc.connect(hostkey=self.public_host_key)\nremain = self.tc.auth_password('non-utf8', '\\xff')\nself.assertEquals([], remain)\nself.verify_finished()", "path": "tests\\test_auth.py", "repo_name": "bitprophet/ssh", "stars": 143, "license": "lgpl-2.1", "language": "python", "size": 1797}
{"docstring": "\"\"\"\nLoad host keys from a local host-key file.  Host keys read with this\nmethod will be checked I{after} keys loaded via L{load_system_host_keys},\nbut will be saved back by L{save_host_keys} (so they can be modified).\nThe missing host key policy L{AutoAddPolicy} adds keys to this set and\nsaves them, when connecting to a previously-unknown server.\n\nThis method can be called multiple times.  Each new set of host keys\nwill be merged with the existing set (new replacing old if there are\nconflicts).  When automatically saving, the last hostname is used.\n\n@param filename: the filename to read\n@type filename: str\n\n@raise IOError: if the filename could not be read\n\"\"\"\n", "func_signal": "def load_host_keys(self, filename):\n", "code": "self._host_keys_filename = filename\nself._host_keys.load(filename)", "path": "ssh\\client.py", "repo_name": "bitprophet/ssh", "stars": 143, "license": "lgpl-2.1", "language": "python", "size": 1797}
{"docstring": "\"\"\"\nverify that a bad password gets the right exception, and that a retry\nwith the right password works.\n\"\"\"\n", "func_signal": "def test_2_bad_password(self):\n", "code": "self.start_server()\nself.tc.connect(hostkey=self.public_host_key)\ntry:\n    self.tc.auth_password(username='slowdive', password='error')\n    self.assert_(False)\nexcept:\n    etype, evalue, etb = sys.exc_info()\n    self.assert_(issubclass(etype, AuthenticationException))\nself.tc.auth_password(username='slowdive', password='pygmalion')\nself.verify_finished()", "path": "tests\\test_auth.py", "repo_name": "bitprophet/ssh", "stars": 143, "license": "lgpl-2.1", "language": "python", "size": 1797}
{"docstring": "\"\"\"\nCreate a new SSHClient.\n\"\"\"\n", "func_signal": "def __init__(self):\n", "code": "self._system_host_keys = HostKeys()\nself._host_keys = HostKeys()\nself._host_keys_filename = None\nself._log_channel = None\nself._policy = RejectPolicy()\nself._transport = None\nself._agent = None", "path": "ssh\\client.py", "repo_name": "bitprophet/ssh", "stars": 143, "license": "lgpl-2.1", "language": "python", "size": 1797}
{"docstring": "\"\"\"\nverify that utf-8 encoding happens in authentication.\n\"\"\"\n", "func_signal": "def test_6_auth_utf8(self):\n", "code": "self.start_server()\nself.tc.connect(hostkey=self.public_host_key)\nremain = self.tc.auth_password('utf8', u'\\u2022')\nself.assertEquals([], remain)\nself.verify_finished()", "path": "tests\\test_auth.py", "repo_name": "bitprophet/ssh", "stars": 143, "license": "lgpl-2.1", "language": "python", "size": 1797}
{"docstring": "\"\"\"\nClose this SSHClient and its underlying L{Transport}.\n\"\"\"\n", "func_signal": "def close(self):\n", "code": "if self._transport is None:\n    return\nself._transport.close()\nself._transport = None\n\nif self._agent != None:\n    self._agent.close()\n    self._agent = None", "path": "ssh\\client.py", "repo_name": "bitprophet/ssh", "stars": 143, "license": "lgpl-2.1", "language": "python", "size": 1797}
{"docstring": "# simulation dictionary has to have:\n# N - SharedArray of one entity enumerating the step of the simulation\n# stages - integer number of processing stages\n# num_proc - integer number of processors that will be utilized for workers\n# stage0 - a list of element parameters (dictionaries) for each execution element\n# stage0_size - number of execution elements in the stage\n# other parameters are model specific.\n", "func_signal": "def generate_dict():\n", "code": "simulation_dict = PVM_Create.create_blank_dictionary()\nsimulation_dict['stages'] = 1\nsimulation_dict['num_proc'] = mp.cpu_count()/2\nsimulation_dict['stage0'] = []\nsimulation_dict['execution_unit_module'] = 'PVM_models.demo00_unit'\nunit = importlib.import_module(simulation_dict['execution_unit_module'])\nsimulation_dict['stage0_size'] = 100\narena = SharedArray.SharedNumpyArray((500, 500), np.uint8)  # the arena will be 500x500 in this case\nsimulation_dict['arena'] = arena\nfor i in range(simulation_dict['stage0_size']):\n    unit_parameters = {}\n    unit_parameters['arena'] = arena\n    # block will define a sub-block of the arena that each unit will be processing\n    # it will contain (x0, y0, width, height)\n    unit_parameters['block'] = SharedArray.SharedNumpyArray((4,), np.int)\n    # each unit gets a 100x100 sub-block of the arena\n    unit_parameters['block'][0] = (i / 10)*50\n    unit_parameters['block'][1] = (i % 10)*50\n    unit_parameters['block'][2] = 50\n    unit_parameters['block'][3] = 50\n    simulation_dict['stage0'].append(unit_parameters)\nfor i in range(simulation_dict['stage0_size']):\n    unit.ExecutionUnit.generate_missing_parameters(simulation_dict['stage0'][i])\nreturn simulation_dict", "path": "PVM_models\\demo00_run.py", "repo_name": "braincorp/PVM", "stars": 149, "license": "other", "language": "python", "size": 195}
{"docstring": "\"\"\"\nMake predictions based on available information and publish them.\n\n\"\"\"\n# Prepare the input vector\n", "func_signal": "def execute0(self):\n", "code": "self.layers_internal_prediction[0]['activation'][:-1] = self.contexts_t_1\n# Make internal prediction\nself.MLP_internal_prediction.mlp_forward()\n# Make residual prediction\nself.layers_residual_prediction[0]['activation'][:-1] = np.concatenate((self.inputs_t, self.layers_internal_prediction[1][\"activation\"][:-1]))\nself.MLP_residual_prediction.mlp_forward()\n# Forward internal representations to the readout predictor\nself.layers_readout[0]['activation'][:-1] = self.layers_residual_prediction[self.output_layer]['activation'][:-1]\n# Make readout prediction\nself.MLP_readout.mlp_forward()\n# Update the output\nself.output_block[:] = self.min_max_normalize((self.layers_residual_prediction[self.output_layer]['activation'][:self.output_length]).reshape(self.output_block.shape), self.output_min, self.output_max)\n# Publish predictions (mainly for debugging and inspection)\ni = 0\ns = len(self.signal_blocks)\nfor (idx, (block, delta, pred_block1, pred_block2)) in enumerate(self.signal_blocks):\n    l = np.prod(pred_block1.shape)\n    pred_block1[:] = np.clip((self.layers_internal_prediction[-1]['activation'][i:i+l] +\n                             2 * (self.layers_residual_prediction[-1]['activation'][i:i+l]-0.5)).reshape(pred_block1.shape), 0, 1)\n    pred_block_local = self.internal_buffers[idx][0]\n    pred_block_local[:] = np.clip((self.layers_internal_prediction[-1]['activation'][i:i+l] +\n                                   2*(self.layers_residual_prediction[-1]['activation'][i:i+l]-0.5)).reshape(pred_block_local.shape), 0, 1)\n    pred_block2[:] = np.clip((self.layers_internal_prediction[-1]['activation'][s*l+i:s*l+i+l] +\n                             2 * (self.layers_residual_prediction[-1]['activation'][s*l+i:s*l+i+l]-0.5)).reshape(pred_block1.shape), 0, 1)\n    i += l\ni = 0\nfor (block, delta, readout_block) in self.readout_blocks:\n    l = np.prod(readout_block.shape)\n    readout_block[:] = (self.layers_readout[-1]['activation'][i:i+l]).reshape(readout_block.shape)\n    i += l\n# Since predicting two steps into the future, learning can only be performed on previous\n# activations, hence current activations are pushed into the FIFO\nif self.primary_learning_rate[0] != 0:\n    self.push_activation()", "path": "PVM_models\\PVM_unit_2step_residual_v1.py", "repo_name": "braincorp/PVM", "stars": 149, "license": "other", "language": "python", "size": 195}
{"docstring": "''' Create playback progress bar (allow user to move along the movie) '''\n", "func_signal": "def create_windows(self):\n", "code": "cv2.namedWindow(self.win_name)\ncv2.moveWindow(self.win_name, 150, 150)\ncv2.namedWindow(\"completeness\")\ncv2.moveWindow(\"completeness\", 150, 50)\ncv2.setMouseCallback(self.win_name, self.trackbar_window_onMouse)\ncv2.createTrackbar(\"Frame\", self.win_name, 0, len(self.movie) - 1, self.jump_to_frame_callback)\ncv2.createTrackbar(\"Trim start\", self.win_name, 0, len(self.movie) - 1, self.set_trim_start)\ncv2.createTrackbar(\"Trim end\", self.win_name, len(self.movie) - 1, len(self.movie) - 1, self.set_trim_end)\nim_height = 30  # 30 pixels high completeness bar\nim_width = len(self.movie)\nself.completeness_image = np.zeros((im_height, im_width, 3))\ncurrent_frame_index = int(im_width * float(self.current_frame_index) / len(self.movie))\nself.completeness_image[:, current_frame_index, :] = 1.\nself.completeness = np.zeros(im_width, dtype=np.uint8)\nfor i in xrange(len(self.movie)):\n    bounds = self.movie.Frame(i).get_label(channel=self.channel, target=self.target)\n    if bounds is None or bounds.empty:\n        if bounds is None:\n            self.movie.Frame(i).set_label(channel=self.channel, target=self.target, label=BoundingRegion())\n        self.completeness[i] = 2\n    else:\n        if bounds.is_keyframe():\n            self.completeness[i] = 1\n        else:\n            self.completeness[i] = 0", "path": "tracker_tools\\label_PVM_pickle.py", "repo_name": "braincorp/PVM", "stars": 149, "license": "other", "language": "python", "size": 195}
{"docstring": "'''\n:param image: input image\n:type image: numpy.ndarray\n:param bb: bounding box\n:type bb: numpy.ndarray\nIf bb (=(x, y, w, h)) is not None, the histogram is taken from the bounded\npart of the image. If use_background is True, bb cannot be None\n\n.. note::\n    The bounding box here is given in pixel coordinates!\n'''\n", "func_signal": "def calculateHistogram(self, image, bb=None):\n", "code": "assert not (self.use_background and bb is None), 'If using background, bb must be provided'\n\nif bb is not None:\n    x, y, w, h = bb\n    target = image[y:y + h, x:x + w]\nelse:\n    target = image\nhist_target = cv2.calcHist([target], channels=self._channels, mask=None, histSize=self.n_bins, ranges=self._ranges)\nif self.raw_hist_target is None:\n    self.raw_hist_target = hist_target\nelse:\n    self.raw_hist_target += hist_target\n\nif self.use_background:\n    hist_image = cv2.calcHist([image], channels=self._channels, mask=None, histSize=self.n_bins, ranges=self._ranges)\n    if self.raw_hist_image is None:\n        self.raw_hist_image = hist_image\n    else:\n        self.raw_hist_image += hist_image\nself._update_hist()", "path": "other_trackers\\backprojection.py", "repo_name": "braincorp/PVM", "stars": 149, "license": "other", "language": "python", "size": 195}
{"docstring": "\"\"\"\nThis will be called right before the simulation starts\n\"\"\"\n", "func_signal": "def start(self):\n", "code": "self.t_start = time.time()\nself._running = True\nself.steps = 0", "path": "PVM_models\\demo00_run.py", "repo_name": "braincorp/PVM", "stars": 149, "license": "other", "language": "python", "size": 195}
{"docstring": "# Mapping SharedArray items through .view(np.ndarray) improved the access time\n", "func_signal": "def __init__(self, parameters):\n", "code": "self.signal_blocks = self.open_views_into_shmem(parameters, \"signal_blocks\", len(ExecutionUnit.SIGNAL_BLOCK_CONTENTS))\nself.readout_blocks = self.open_views_into_shmem(parameters, \"readout_blocks\", len(ExecutionUnit.SUPERVISED_TASK_OUTPUTS))\nself.context_blocks = self.open_views_into_shmem(parameters, \"context_blocks\", len(ExecutionUnit.UNSUPERVISED_CONTEXT_INPUTS))\nself.internal_buffers = self.open_views_into_shmem(parameters, \"internal_buffers\", 3)\nself.output_block = parameters['output_block'].view(np.ndarray)  # Will appear in other places as either input or context\nself.output_min = parameters['output_min']\nself.output_max = parameters['output_max']\n\n# The perceptron\nself.MLP_internal_prediction = MLP.MLP(parameters[\"Primary_Predictor_params\"])\nself.MLP_residual_prediction = MLP.MLP(parameters[\"Residual_Predictor_params\"])\nself.MLP_readout = MLP.MLP(parameters[\"Readout_Predictor_params\"])  # This is a \"task\" supervised MLP (e.g., tracker)\nself.primary_learning_rate = parameters['primary_learning_rate']\nself.readout_learning_rate = parameters['readout_learning_rate']\nself.layers_internal_prediction = parameters[\"Primary_Predictor_params\"]['layers']\nself.layers_residual_prediction = parameters[\"Residual_Predictor_params\"]['layers']\nself.layers_readout = parameters[\"Readout_Predictor_params\"]['layers']  # These are the \"task\" supervised MLP layers\nself.tau = parameters['tau']  # Tau is the integration constant for the signal integral\n\n# Input buffers\nself.ninputs = 0\nself.npredictions = 0\nself.npinputs = 0\nself.ncontexts = 0\nfor (block, delta, pred_block1, pred_block2) in self.signal_blocks:\n    self.ninputs += len(ExecutionUnit.UNSUPERVISED_SIGNAL_INPUTS) * np.prod(block.shape)\n    self.npredictions += np.prod(block.shape)\nfor (teaching_block, delta, readout_block) in self.readout_blocks:\n    self.npinputs += np.prod(teaching_block.shape)\nself.inputs_t = np.zeros((self.ninputs,))\nself.actual_signal_t = np.zeros((self.npredictions,))\nself.readout_training_signal = np.zeros((self.npinputs,))\n\nself.inputs_t_1 = np.zeros((self.ninputs,))\nself.actual_signal_t_1 = np.zeros((self.npredictions,))\nself.pinputs_t_1 = np.zeros((self.npinputs,))\n\n# Context buffers\nfor (block, delta, factor) in self.context_blocks:\n    self.ncontexts += np.prod(block.shape)\nself.contexts_t_1 = np.zeros((self.ncontexts,))\nself.output_layer = len(self.layers_residual_prediction)-2  # Becuse the \"output\" of this Unit comes from its hidden layer\nself.output_length = np.prod(self.output_block.shape)\n# Buffer for storing activations\nself.activations_buffer = []\n# additional flags\nif 'complex' in parameters.keys() and parameters['complex']:\n    self.complex = True\nelse:\n    self.complex = False\nself.push_activation()", "path": "PVM_models\\PVM_unit_2step_residual_v1.py", "repo_name": "braincorp/PVM", "stars": 149, "license": "other", "language": "python", "size": 195}
{"docstring": "\"\"\"\nThis method does the actual execution.\n\"\"\"\n", "func_signal": "def execute0(self):\n", "code": "input = self.array[self.divider:, self.block[0]:self.block[0]+self.block[2], self.block[1]:self.block[1]+self.block[3]].flatten()\nfor block in self.context_blocks:\n    input = np.append(input, block.flatten())\noutput = self.array[0:self.divider, self.block[0]:self.block[0]+self.block[2], self.block[1]:self.block[1]+self.block[3]].flatten()\npredicted = self.MLP.train(input.astype(np.float)/255, output.astype(np.float)/255)\npredicted = 255*predicted.reshape((self.divider, self.block[2], self.block[3], 3))\nself.predicted_array[:, self.block[0]:self.block[0]+self.block[2], self.block[1]:self.block[1]+self.block[3]] = predicted.astype(np.uint8)", "path": "PVM_models\\demo03_unit.py", "repo_name": "braincorp/PVM", "stars": 149, "license": "other", "language": "python", "size": 195}
{"docstring": "\"\"\"\nSave the current network activations to a FIFO queue\n:return:\n\"\"\"\n", "func_signal": "def push_activation(self):\n", "code": "layers_copy = copy.deepcopy(self.layers_internal_prediction)\nlayers_res_copy = copy.deepcopy(self.layers_residual_prediction)\nself.activations_buffer.append((layers_copy, layers_res_copy))", "path": "PVM_models\\PVM_unit_2step_residual_v1.py", "repo_name": "braincorp/PVM", "stars": 149, "license": "other", "language": "python", "size": 195}
{"docstring": "\"\"\"\nSubtractive modification of the current histogram\n:param hist: a numpy array in the same shape as the current histogram\n:type hist: numpy.ndarray\n\"\"\"\n", "func_signal": "def subtract_color_histogram(self, hist):\n", "code": "if self.raw_hist_target is not None:\n    self.raw_hist_target -= hist\n    self.raw_hist_target[self.raw_hist_target < 0] = 0\nself._update_hist()", "path": "other_trackers\\backprojection.py", "repo_name": "braincorp/PVM", "stars": 149, "license": "other", "language": "python", "size": 195}
{"docstring": "\"\"\"\nGather the context signal in preparation for the next step.\nSome context blocks get their information from units lateral to this one,\nothers get their information from units above this one.\nThe \"factor\" is either the context_factor_lateral or context_factor_feedback\n\"\"\"\n# Prepare information for the next step\n# collect the new context\n", "func_signal": "def execute2(self):\n", "code": "i = 0\nfor (block, delta, factor) in self.context_blocks:\n    l = np.prod(block.shape)\n    self.contexts_t_1[i:i+l] = factor[0]*block.flatten()\n    i += l", "path": "PVM_models\\PVM_unit_2step_residual_v1.py", "repo_name": "braincorp/PVM", "stars": 149, "license": "other", "language": "python", "size": 195}
{"docstring": "\"\"\"\nIn this very simple demo a set of workers operate on a 500x500 image domain by randomly flipping\nselected bits. To make things faster the bit/byte flipping function is written in cython.\n:return:\n\"\"\"\n", "func_signal": "def run_demo():\n", "code": "filename = \"demo00_state.p.gz\"\nif os.path.isfile(filename):\n    state_dict = CoreUtils.load_model(filename)\nelse:\n    state_dict = generate_dict()\nmanager = Manager(state_dict, 1000)\nexecutor = CoreUtils.ModelExecution(prop_dict=state_dict, manager=manager)\nexecutor.start(blocking=True)\nCoreUtils.save_model(state_dict, filename)\nprint(\"Saving and running again in non blocking mode\")\nexecutor.start(blocking=False)\nwhile manager.running():\n    executor.step()\nexecutor.finish()\nCoreUtils.save_model(state_dict, filename)", "path": "PVM_models\\demo00_run.py", "repo_name": "braincorp/PVM", "stars": 149, "license": "other", "language": "python", "size": 195}
{"docstring": "\"\"\"\nAdditive modification of the current histogram\n:param hist: a numpy array in the same shape as the current histogram\n:type hist: numpy.ndarray\n\"\"\"\n", "func_signal": "def add_color_histogram(self, hist):\n", "code": "if self.raw_hist_target is None:\n    self.raw_hist_target = hist.copy()\nelse:\n    self.raw_hist_target += hist\nself._update_hist()", "path": "other_trackers\\backprojection.py", "repo_name": "braincorp/PVM", "stars": 149, "license": "other", "language": "python", "size": 195}
{"docstring": "\"\"\"\nPop the first available activations from the FIFO\n:return:\n\"\"\"\n", "func_signal": "def pop_activation(self):\n", "code": "(layers, layers_res) = self.activations_buffer.pop(0)\nfor layer in range(len(layers)):\n    self.layers_internal_prediction[layer]['activation'][:] = layers[layer]['activation']\n    self.layers_internal_prediction[layer]['error'][:] = layers[layer]['error']\n    self.layers_internal_prediction[layer]['delta'][:] = layers[layer]['delta']\nfor layer in range(len(layers_res)):\n    self.layers_residual_prediction[layer]['activation'][:] = layers_res[layer]['activation']\n    self.layers_residual_prediction[layer]['error'][:] = layers_res[layer]['error']\n    self.layers_residual_prediction[layer]['delta'][:] = layers_res[layer]['delta']", "path": "PVM_models\\PVM_unit_2step_residual_v1.py", "repo_name": "braincorp/PVM", "stars": 149, "license": "other", "language": "python", "size": 195}
{"docstring": "\"\"\"\nThis method can be called to generate all the missing dictionary parameters when all\nthe other relevant variables are known. Leave empty if there is nothing more to generate\n\"\"\"\n", "func_signal": "def generate_missing_parameters(parameters):\n", "code": "ninputs = ((2*parameters['array'].shape[0])/3)*parameters['block'][2]*parameters['block'][3]*3\nfor block in parameters[\"context_blocks\"]:\n    ninputs += np.prod(block.shape)\nnhidden = parameters['internal_rep_size']\nnoutputs = (parameters['array'].shape[0]-(2*parameters['array'].shape[0])/3)*parameters['block'][2]*parameters['block'][3]*3\nparameters[\"MLP_parameters\"] = {}\nparameters[\"MLP_parameters\"]['layers'] = [\n    {'activation': SharedArray.SharedNumpyArray((ninputs+1), np.float),\n     'error': SharedArray.SharedNumpyArray((ninputs+1), np.float),\n     'delta': SharedArray.SharedNumpyArray((ninputs+1), np.float)\n     },\n    {'activation': SharedArray.SharedNumpyArray((nhidden+1), np.float),\n     'error': SharedArray.SharedNumpyArray((nhidden+1), np.float),\n     'delta': SharedArray.SharedNumpyArray((nhidden+1), np.float)\n     },\n    {'activation': SharedArray.SharedNumpyArray((noutputs+1), np.float),\n     'error': SharedArray.SharedNumpyArray((noutputs+1), np.float),\n     'delta': SharedArray.SharedNumpyArray((noutputs+1), np.float)\n     },\n]\nparameters[\"MLP_parameters\"]['beta'] = SharedArray.SharedNumpyArray((1,), np.float)\nparameters[\"MLP_parameters\"]['beta'][0] = 1.0\nparameters[\"MLP_parameters\"]['learning_rate'] = parameters['learning_rate']\nparameters[\"MLP_parameters\"]['momentum'] = parameters['momentum']\nparameters[\"MLP_parameters\"]['mse'] = SharedArray.SharedNumpyArray((1,), np.float)\nparameters[\"MLP_parameters\"]['weights'] = [\n    MLP.initialize_weights(SharedArray.SharedNumpyArray((ninputs+1, nhidden), np.float)),\n    MLP.initialize_weights(SharedArray.SharedNumpyArray((nhidden+1, noutputs), np.float))\n]", "path": "PVM_models\\demo03_unit.py", "repo_name": "braincorp/PVM", "stars": 149, "license": "other", "language": "python", "size": 195}
{"docstring": "\"\"\"\nSimple min-max normalization\n:param a: array to be normalized\n:param min_a: recorded min values\n:param max_a: recorded max values\n:return:\n\"\"\"\n", "func_signal": "def min_max_normalize(self, a, min_a, max_a):\n", "code": "if self.primary_learning_rate[0] != 0:\n    min_a[:] = np.minimum(a, min_a) + 0.000001\n    max_a[:] = np.maximum(a, max_a) - 0.000001\nreturn np.divide(a-min_a, max_a-min_a)", "path": "PVM_models\\PVM_unit_2step_residual_v1.py", "repo_name": "braincorp/PVM", "stars": 149, "license": "other", "language": "python", "size": 195}
{"docstring": "# 0.2 secs => 5 Hz\n", "func_signal": "def schedule_callback(self):\n", "code": "self.timer = threading.Timer(0.1, self.timer_callback)\nself.timer.start()", "path": "tracker_tools\\label_PVM_pickle.py", "repo_name": "braincorp/PVM", "stars": 149, "license": "other", "language": "python", "size": 195}
{"docstring": "\"\"\"\nCollect new signals, perform association, prepare for additional learning\n\"\"\"\n# Collect the current signal\n", "func_signal": "def execute1(self):\n", "code": "i = 0\nj = 0\nfor (idx, (block, delta, pred_block1, pred_block2)) in enumerate(self.signal_blocks):\n    l = np.prod(block.shape)\n    # Calculate signal features\n    past_block = self.internal_buffers[idx][1]\n    self.inputs_t[i:i + l] = block.flatten()\n    self.inputs_t[i + l:i + 2 * l] = past_block.flatten()\n    past_block[:] = block\n    i += 2*l\n    # Collect the current signal as supervising signal\n    self.actual_signal_t[j:j+l] = block.flatten()\n    j += l\n# Predictive associative training\nif self.primary_learning_rate[0] != 0:\n    # pop the previous activaitons from the FIFO\n    self.pop_activation()\n    # Create the expected output block\n    training_signal = np.concatenate((self.actual_signal_t_1, self.actual_signal_t))\n    # Calculate the prediction error\n    error_primary = training_signal - self.layers_internal_prediction[-1]['activation'][:np.prod(training_signal.shape)]\n    error_residual = (error_primary + 1) * 0.5 - self.layers_residual_prediction[-1]['activation'][:np.prod(training_signal.shape)]\n    self.actual_signal_t_1[:] = self.actual_signal_t\n    # Make the association\n    self.MLP_internal_prediction.train2(error=error_primary)\n    self.MLP_residual_prediction.train2(error=error_residual)\n# Readout training\nif self.readout_learning_rate[0] != 0:\n    # Collect the curent readout training signal\n    i = 0\n    for (block, delta, pblock) in self.readout_blocks:\n        l = np.prod(block.shape)\n        self.readout_training_signal[i:i + l] = block.flatten()\n        i += l\n    error_readout = self.readout_training_signal - self.layers_readout[-1]['activation'][:-1]\n    self.MLP_readout.train2(error=error_readout)", "path": "PVM_models\\PVM_unit_2step_residual_v1.py", "repo_name": "braincorp/PVM", "stars": 149, "license": "other", "language": "python", "size": 195}
{"docstring": "\"\"\"\nCalculate the histogram packprojection on the given image using\nthe currently stored histogram\n:param image: numpy array with appropriate number of channels\n:type image: numpy.ndarray\n:return: a numpy array of type float32 containing the histogram backprojection\n:rtype: numpy.ndarray of type float32\n\"\"\"\n", "func_signal": "def calculate(self, image):\n", "code": "if self.hist is None:\n    return np.zeros(image.shape[:-1], dtype=np.float32)\n\nout = cv2.calcBackProject([image.astype('float32')], channels=self._channels, hist=self.hist, ranges=self._ranges, scale=1)\nself.out = out\nreturn self.out", "path": "other_trackers\\backprojection.py", "repo_name": "braincorp/PVM", "stars": 149, "license": "other", "language": "python", "size": 195}
{"docstring": "\"\"\"\nThis method can be called to generate all the missing dictionary parameters when all\nthe other relevant variables are known. Leave empty if there is nothing more to generate.\nWhen complex_unit is False, a standard 3-layer MLP is used.\nWhen complex_unit is True, an MLP with additional hidden layers is used.\n\nThere needs to be no return value, the method leaves a side effect by modifying the perameters dict.\n\n:param parameters: parameter dictionary\n:type parameters: dict\n\"\"\"\n", "func_signal": "def generate_missing_parameters(parameters, options):\n", "code": "complex_unit = options['unit_type'] == \"complex\"\npolynomial = options['polynomial'] == '1'\nautoencoder = options['autoencoder'] == '1'\n\nnhidden = np.prod(parameters['output_block'].shape)\nparameters['output_min'] = SharedArray.SharedNumpyArray_like(parameters['output_block'])\nparameters['output_max'] = SharedArray.SharedNumpyArray_like(parameters['output_block'])\nninputs = 0\nnoutputs = 0\nncontext = 0\n# Any additional memory buffers needed in the operation of the unit\nparameters['internal_buffers'] = []\nfor (block, delta, pred_block, pred_block2) in parameters['signal_blocks']:\n    block01 = SharedArray.SharedNumpyArray_like(block)\n    block02 = SharedArray.SharedNumpyArray_like(block)\n    block03 = SharedArray.SharedNumpyArray_like(block)\n    parameters['internal_buffers'].append((block01, block02, block03))\n\nfor (block, delta, pred_block, pred_block2) in parameters['signal_blocks']:\n    ninputs += np.prod(block.shape) * len(ExecutionUnit.UNSUPERVISED_SIGNAL_INPUTS)\nfor (block, delta, factor) in parameters['context_blocks']:\n    ncontext += np.prod(block.shape)\nfor (block, delta, pred_block, pred_block2) in parameters['signal_blocks']:\n    noutputs += 2 * np.prod(block.shape)  # to predict two steps into the future\nnadditional = 0\nfor (block, delta, pblock) in parameters['readout_blocks']:\n    nadditional += np.prod(block.shape)\nparameters[\"Primary_Predictor_params\"] = {}\nparameters[\"Residual_Predictor_params\"] = {}\nparameters[\"Readout_Predictor_params\"] = {}\nif complex_unit:  # 4 layer perceptron\n    parameters[\"Primary_Predictor_params\"]['layers'] = MLP.get_layers([ncontext+1, 3*nhidden+1, 2*nhidden+1, noutputs+1])\n    parameters[\"Residual_Predictor_params\"]['layers'] = MLP.get_layers([ninputs+1, 2*nhidden+1, nhidden+1, noutputs+1])\n    parameters[\"complex\"] = True\nelse:  # 3 layer perceptron Simple MLP Unit (not complex unit)\n    parameters[\"Primary_Predictor_params\"]['layers'] = MLP.get_layers([ncontext+1, 2*nhidden+1, noutputs+1])\n    parameters[\"Residual_Predictor_params\"]['layers'] = MLP.get_layers([ninputs+2*nhidden+1, nhidden+1, noutputs+1])\n\nparameters[\"Primary_Predictor_params\"]['beta'] = SharedArray.SharedNumpyArray((1,), np.float)\nparameters[\"Primary_Predictor_params\"]['beta'][0] = 1.0\nparameters[\"Primary_Predictor_params\"]['learning_rate'] = parameters['primary_learning_rate']\nparameters[\"Primary_Predictor_params\"]['momentum'] = parameters['momentum']\nparameters[\"Primary_Predictor_params\"]['mse'] = SharedArray.SharedNumpyArray((1,), np.float)\nparameters[\"Primary_Predictor_params\"]['weights'] = MLP.get_weights(parameters[\"Primary_Predictor_params\"]['layers'])\n\nparameters[\"Residual_Predictor_params\"]['beta'] = SharedArray.SharedNumpyArray((1,), np.float)\nparameters[\"Residual_Predictor_params\"]['beta'][0] = 1.0\nparameters[\"Residual_Predictor_params\"]['learning_rate'] = parameters['primary_learning_rate']\nparameters[\"Residual_Predictor_params\"]['momentum'] = parameters['momentum']\nparameters[\"Residual_Predictor_params\"]['mse'] = SharedArray.SharedNumpyArray((1,), np.float)\nparameters[\"Residual_Predictor_params\"]['weights'] = MLP.get_weights(parameters[\"Residual_Predictor_params\"]['layers'])\n\nparameters[\"Readout_Predictor_params\"]['layers'] = MLP.get_layers([nhidden+1, 2*nhidden+1, nadditional+1])\nparameters[\"Readout_Predictor_params\"]['beta'] = SharedArray.SharedNumpyArray((1,), np.float)\nparameters[\"Readout_Predictor_params\"]['beta'][0] = 1.0\nparameters[\"Readout_Predictor_params\"]['learning_rate'] = parameters['readout_learning_rate']\nparameters[\"Readout_Predictor_params\"]['momentum'] = parameters['momentum']\nparameters[\"Readout_Predictor_params\"]['mse'] = SharedArray.SharedNumpyArray((1,), np.float)\nparameters[\"Readout_Predictor_params\"]['weights'] = MLP.get_weights(parameters[\"Readout_Predictor_params\"]['layers'])\nparameters[\"Primary_Predictor_params\"]['polynomial'] = polynomial\nparameters[\"Residual_Predictor_params\"]['polynomial'] = polynomial\nparameters[\"Readout_Predictor_params\"]['polynomial'] = polynomial\nparameters['autoencoder'] = autoencoder", "path": "PVM_models\\PVM_unit_2step_residual_v1.py", "repo_name": "braincorp/PVM", "stars": 149, "license": "other", "language": "python", "size": 195}
{"docstring": "\"\"\"\nReset the object\n\"\"\"\n", "func_signal": "def reset(self):\n", "code": "self.raw_hist_target = None\nself.raw_hist_image = None\nself.hist = None", "path": "other_trackers\\backprojection.py", "repo_name": "braincorp/PVM", "stars": 149, "license": "other", "language": "python", "size": 195}
{"docstring": "\"\"\"returns a hash of the 'final' style attributes of the element\"\"\"\n", "func_signal": "def element_style(attrs, style_def, parent_style):\n", "code": "style = parent_style.copy()\nif 'class' in attrs:\n    for css_class in attrs['class'].split():\n        css_style = style_def['.' + css_class]\n        style.update(css_style)\nif 'style' in attrs:\n    immediate_style = dumb_property_dict(attrs['style'])\n    style.update(immediate_style)\nreturn style", "path": "wp2md\\html2text.py", "repo_name": "dreikanter/wp2md", "stars": 216, "license": "gpl-3.0", "language": "python", "size": 66}
{"docstring": "\"\"\"Stores base URL in configuration if it's not defined explicitly.\"\"\"\n", "func_signal": "def store_base_url(channel):\n", "code": "if conf['fix_urls'] and not conf['base_url']:\n    conf['base_url'] = channel.get('base_site_url', '')", "path": "wp2md\\wp2md.py", "repo_name": "dreikanter/wp2md", "stars": 216, "license": "gpl-3.0", "language": "python", "size": 66}
{"docstring": "\"\"\"Returns preconfigured export path format for specified\nRSS item type and metadata.\"\"\"\n\n", "func_signal": "def get_path_fmt(item_type, data):\n", "code": "if data.get('status', None).lower() == 'draft':\n    return conf['draft_path']\nis_post = item_type == 'post'\nreturn conf['post_path'] if is_post else conf['page_path']", "path": "wp2md\\wp2md.py", "repo_name": "dreikanter/wp2md", "stars": 216, "license": "gpl-3.0", "language": "python", "size": 66}
{"docstring": "#attrs = fixattrs(attrs)\n", "func_signal": "def handle_tag(self, tag, attrs, start):\n", "code": "if attrs is None:\n    attrs = {}\nelse:\n    attrs = dict(attrs)\n\nif self.google_doc:\n    # the attrs parameter is empty for a closing tag. in addition, we\n    # need the attributes of the parent nodes in order to get a\n    # complete style description for the current element. we assume\n    # that google docs export well formed html.\n    parent_style = {}\n    if start:\n        if self.tag_stack:\n          parent_style = self.tag_stack[-1][2]\n        tag_style = element_style(attrs, self.style_def, parent_style)\n        self.tag_stack.append((tag, attrs, tag_style))\n    else:\n        dummy, attrs, tag_style = self.tag_stack.pop()\n        if self.tag_stack:\n            parent_style = self.tag_stack[-1][2]\n\nif hn(tag):\n    self.p()\n    if start:\n        self.inheader = True\n        self.o(hn(tag)*\"#\" + ' ')\n    else:\n        self.inheader = False\n        return # prevent redundant emphasis marks on headers\n\nif tag in ['p', 'div']:\n    if self.google_doc:\n        if start and google_has_height(tag_style):\n            self.p()\n        else:\n            self.soft_br()\n    else:\n        self.p()\n\nif tag == \"br\" and start: self.o(\"  \\n\")\n\nif tag == \"hr\" and start:\n    self.p()\n    self.o(\"* * *\")\n    self.p()\n\nif tag in [\"head\", \"style\", 'script']:\n    if start: self.quiet += 1\n    else: self.quiet -= 1\n\nif tag == \"style\":\n    if start: self.style += 1\n    else: self.style -= 1\n\nif tag in [\"body\"]:\n    self.quiet = 0 # sites like 9rules.com never close <head>\n\nif tag == \"blockquote\":\n    if start:\n        self.p(); self.o('> ', 0, 1); self.start = 1\n        self.blockquote += 1\n    else:\n        self.blockquote -= 1\n        self.p()\n\nif tag in ['em', 'i', 'u'] and not self.ignore_emphasis: self.o(self.emphasis_mark)\nif tag in ['strong', 'b'] and not self.ignore_emphasis: self.o(self.strong_mark)\nif tag in ['del', 'strike', 's']:\n    if start:\n        self.o(\"<\"+tag+\">\")\n    else:\n        self.o(\"</\"+tag+\">\")\n\nif self.google_doc:\n    if not self.inheader:\n        # handle some font attributes, but leave headers clean\n        self.handle_emphasis(start, tag_style, parent_style)\n\nif tag in [\"code\", \"tt\"] and not self.pre: self.o('`') #TODO: `` `this` ``\nif tag == \"abbr\":\n    if start:\n        self.abbr_title = None\n        self.abbr_data = ''\n        if has_key(attrs, 'title'):\n            self.abbr_title = attrs['title']\n    else:\n        if self.abbr_title != None:\n            self.abbr_list[self.abbr_data] = self.abbr_title\n            self.abbr_title = None\n        self.abbr_data = ''\n\nif tag == \"a\" and not self.ignore_links:\n    if start:\n        if has_key(attrs, 'href') and not (self.skip_internal_links and attrs['href'].startswith('#')):\n            self.astack.append(attrs)\n            self.maybe_automatic_link = attrs['href']\n        else:\n            self.astack.append(None)\n    else:\n        if self.astack:\n            a = self.astack.pop()\n            if self.maybe_automatic_link:\n                self.maybe_automatic_link = None\n            elif a:\n                if self.inline_links:\n                    self.o(\"](\" + escape_md(a['href']) + \")\")\n                else:\n                    i = self.previousIndex(a)\n                    if i is not None:\n                        a = self.a[i]\n                    else:\n                        self.acount += 1\n                        a['count'] = self.acount\n                        a['outcount'] = self.outcount\n                        self.a.append(a)\n                    self.o(\"][\" + str(a['count']) + \"]\")\n\nif tag == \"img\" and start and not self.ignore_images:\n    if has_key(attrs, 'src'):\n        attrs['href'] = attrs['src']\n        alt = attrs.get('alt', '')\n        self.o(\"![\" + escape_md(alt) + \"]\")\n\n        if self.inline_links:\n            self.o(\"(\" + escape_md(attrs['href']) + \")\")\n        else:\n            i = self.previousIndex(attrs)\n            if i is not None:\n                attrs = self.a[i]\n            else:\n                self.acount += 1\n                attrs['count'] = self.acount\n                attrs['outcount'] = self.outcount\n                self.a.append(attrs)\n            self.o(\"[\" + str(attrs['count']) + \"]\")\n\nif tag == 'dl' and start: self.p()\nif tag == 'dt' and not start: self.pbr()\nif tag == 'dd' and start: self.o('    ')\nif tag == 'dd' and not start: self.pbr()\n\nif tag in [\"ol\", \"ul\"]:\n    # Google Docs create sub lists as top level lists\n    if (not self.list) and (not self.lastWasList):\n        self.p()\n    if start:\n        if self.google_doc:\n            list_style = google_list_style(tag_style)\n        else:\n            list_style = tag\n        numbering_start = list_numbering_start(attrs)\n        self.list.append({'name':list_style, 'num':numbering_start})\n    else:\n        if self.list: self.list.pop()\n    self.lastWasList = True\nelse:\n    self.lastWasList = False\n\nif tag == 'li':\n    self.pbr()\n    if start:\n        if self.list: li = self.list[-1]\n        else: li = {'name':'ul', 'num':0}\n        if self.google_doc:\n            nest_count = self.google_nest_count(tag_style)\n        else:\n            nest_count = len(self.list)\n        self.o(\"  \" * nest_count) #TODO: line up <ol><li>s > 9 correctly.\n        if li['name'] == \"ul\": self.o(self.ul_item_mark + \" \")\n        elif li['name'] == \"ol\":\n            li['num'] += 1\n            self.o(str(li['num'])+\". \")\n        self.start = 1\n\nif tag in [\"table\", \"tr\"] and start: self.p()\nif tag == 'td': self.pbr()\n\nif tag == \"pre\":\n    if start:\n        self.startpre = 1\n        self.pre = 1\n    else:\n        self.pre = 0\n    self.p()", "path": "wp2md\\html2text.py", "repo_name": "dreikanter/wp2md", "stars": 216, "license": "gpl-3.0", "language": "python", "size": 66}
{"docstring": "# If the text begins with four spaces or one tab, it's a code block; don't wrap\n", "func_signal": "def skipwrap(para):\n", "code": "if para[0:4] == '    ' or para[0] == '\\t':\n    return True\n# If the text begins with only two \"--\", possibly preceded by whitespace, that's\n# an emdash; so wrap.\nstripped = para.lstrip()\nif stripped[0:2] == \"--\" and len(stripped) > 2 and stripped[2] != \"-\":\n    return False\n# I'm not sure what this is for; I thought it was to detect lists, but there's\n# a <br>-inside-<span> case in one of the tests that also depends upon it.\nif stripped[0:1] == '-' or stripped[0:1] == '*':\n    return True\n# If the text begins with a single -, *, or +, followed by a space, or an integer,\n# followed by a ., followed by a space (in either case optionally preceeded by\n# whitespace), it's a list; don't wrap.\nif ordered_list_matcher.match(stripped) or unordered_list_matcher.match(stripped):\n    return True\nreturn False", "path": "wp2md\\html2text.py", "repo_name": "dreikanter/wp2md", "stars": 216, "license": "gpl-3.0", "language": "python", "size": 66}
{"docstring": "\"\"\"Inserts suffix to the end of file name (before extension).\nIf suffix is zero (or False in boolean representation), nothing\nwill be inserted.\n\nUsage:\n    >>> insert_suffix('c:/temp/hello.txt', 2)\n    c:/temp/hello-2.txt\n    >>> insert_suffix('readme.txt', 0)\n    readme.txt\n\nIntended to be used for numeric suffixes for file\nname uniquification (what a word!).\"\"\"\n\n", "func_signal": "def insert_suffix(file_name, suffix):\n", "code": "if not suffix:\n    return file_name\nbase, ext = os.path.splitext(file_name)\nreturn \"%s-%s%s\" % (base, suffix, ext)", "path": "wp2md\\wp2md.py", "repo_name": "dreikanter/wp2md", "stars": 216, "license": "gpl-3.0", "language": "python", "size": 66}
{"docstring": "\"\"\"finds out whether this is an ordered or unordered list\"\"\"\n", "func_signal": "def google_list_style(style):\n", "code": "if 'list-style-type' in style:\n    list_style = style['list-style-type']\n    if list_style in ['disc', 'circle', 'square', 'none']:\n        return 'ul'\nreturn 'ol'", "path": "wp2md\\html2text.py", "repo_name": "dreikanter/wp2md", "stars": 216, "license": "gpl-3.0", "language": "python", "size": 66}
{"docstring": "\"\"\"Dumps RSS channel item.\"\"\"\n", "func_signal": "def dump_item(data):\n", "code": "if not 'post_type' in data:\n    log.error('Malformed RSS item: item type is not specified.')\n    return\n\nitem_type = data['post_type']\nif item_type not in ['post', 'page', 'draft']:\n    return\n\nfields = WHAT2SAVE['item']\npdata = {}\nfor field in fields:\n    pdata[FIELD_MAP.get(field, field)] = data.get(field, '')\n\n# Post date\nformat = conf['date_fmt']\nfield = FIELD_MAP.get('post_date', 'post_date')\nvalue = pdata.get(field, None)\npdata[field] = value and parse_date(value, format, None)\n\n# Post date GMT\nfield = FIELD_MAP.get('post_date_gmt', 'post_date_gmt')\nvalue = pdata.get(field, None)\npdata[field] = value and parse_date(value, format, None)\n\ndump_path = get_path(item_type, data=pdata)\nlog.info(\"Dumping %s to '%s'\" % (item_type, dump_path))\n\nfields = [FIELD_MAP.get(field, field) for field in fields]\ndump(dump_path, pdata, fields)\n\nstatplusplus(item_type)\nif 'comments' in data:\n    statplusplus('comment', len(data['comments']))", "path": "wp2md\\wp2md.py", "repo_name": "dreikanter/wp2md", "stars": 216, "license": "gpl-3.0", "language": "python", "size": 66}
{"docstring": "\"\"\"Return true if the line does only consist of whitespace characters.\"\"\"\n", "func_signal": "def onlywhite(line):\n", "code": "for c in line:\n    if c is not ' ' and c is not '  ':\n        return c is ' '\nreturn line", "path": "wp2md\\html2text.py", "repo_name": "dreikanter/wp2md", "stars": 216, "license": "gpl-3.0", "language": "python", "size": 66}
{"docstring": "\"\"\" returns the index of certain set of attributes (of a link) in the\n    self.a list\n\n    If the set of attributes is not found, returns None\n\"\"\"\n", "func_signal": "def previousIndex(self, attrs):\n", "code": "if not has_key(attrs, 'href'): return None\n\ni = -1\nfor a in self.a:\n    i += 1\n    match = 0\n\n    if has_key(a, 'href') and a['href'] == attrs['href']:\n        if has_key(a, 'title') or has_key(attrs, 'title'):\n                if (has_key(a, 'title') and has_key(attrs, 'title') and\n                    a['title'] == attrs['title']):\n                    match = True\n        else:\n            match = True\n\n    if match: return i", "path": "wp2md\\html2text.py", "repo_name": "dreikanter/wp2md", "stars": 216, "license": "gpl-3.0", "language": "python", "size": 66}
{"docstring": "\"\"\"Removes expanded namespace from tag name.\"\"\"\n", "func_signal": "def tag_name(name):\n", "code": "result = name[name.find('}') + 1:]\nif result == 'encoded':\n    if name.find('/content/') > -1:\n        result = 'content'\n    elif name.find('/excerpt/') > -1:\n        result = 'excerpt'\nreturn result", "path": "wp2md\\wp2md.py", "repo_name": "dreikanter/wp2md", "stars": 216, "license": "gpl-3.0", "language": "python", "size": 66}
{"docstring": "\"\"\"Dumps a dictionary to YAML-like text file.\"\"\"\n", "func_signal": "def dump(file_name, data, order):\n", "code": "try:\n    dir_path = os.path.dirname(os.path.abspath(file_name))\n    if dir_path and not os.path.exists(dir_path):\n        os.makedirs(dir_path)\n\n    with codecs.open(file_name, 'w', 'utf-8') as f:\n        extras = {}\n        for field in filter(lambda x: x in data, [item for item in order]):\n            if field in ['content', 'comments', 'excerpt']:\n                # Fields for non-standard processing\n                extras[field] = data[field]\n            else:\n                if type(data[field]) == time.struct_time:\n                    value = time.strftime(conf['page_date_fmt'], data[field])\n                else:\n                    value = data[field] or ''\n                f.write(str_t(\"%s: %s\\n\") % (str_t(field), str_t(value)))\n\n        if extras:\n            excerpt = extras.get('excerpt', '')\n            excerpt = excerpt and '<!--%s-->' % excerpt\n\n            content = extras.get('content', '')\n            if conf['md_input']:\n                # Using new MD instance works 3x faster than\n                # reusing existing one for some reason\n                md = markdown.Markdown(extensions=[])\n                content = md.convert(content)\n\n            if conf['fix_urls']:\n                content = fix_urls(html2md(content))\n\n            if 'title' in data:\n                content = str_t(\"# %s\\n\\n%s\") % (data['title'], content)\n\n            comments = generate_comments(extras.get('comments', []))\n            extras = filter(None, [excerpt, content, comments])\n            f.write('\\n' + '\\n\\n'.join(extras))\n\nexcept Exception as e:\n    log.error(\"Error saving data to '%s'\" % (file_name))\n    log.debug(e)", "path": "wp2md\\wp2md.py", "repo_name": "dreikanter/wp2md", "stars": 216, "license": "gpl-3.0", "language": "python", "size": 66}
{"docstring": "\"\"\"check if the style of the element has the 'height' attribute explicitly defined\"\"\"\n", "func_signal": "def google_has_height(style):\n", "code": "if 'height' in style:\n    return True\nreturn False", "path": "wp2md\\html2text.py", "repo_name": "dreikanter/wp2md", "stars": 216, "license": "gpl-3.0", "language": "python", "size": 66}
{"docstring": "\"\"\"Generates full path for the generated file using configuration\nand explicitly specified name or RSS item data. At least one argument\nshould be specified. @file_name has higher priority during output\npath generation.\n\nArguments:\n    item_type -- 'post' or 'page'\n    file_name -- explicitly defined correct file name.\n    data -- preprocessed RSS item data dictionary.\"\"\"\n\n", "func_signal": "def get_path(item_type, file_name=None, data=None):\n", "code": "if not file_name and type(data) is not dict:\n    raise Exception('File name or RSS item data dict should be defined')\n\nroot = conf['dump_path']\nroot = root.format(date=time.strftime(conf['file_date_fmt']),\n                   year=time.strftime(\"%Y\"),\n                   month=time.strftime(\"%m\"),\n                   day=time.strftime(\"%d\"),\n                   source=os.path.basename(conf['source_file']))\n\nif file_name:\n    relpath = file_name\nelse:\n    name = data.get('post_name', '').strip()\n    name = name or data.get('post_id', UNTITLED)\n    relpath = get_path_fmt(item_type, data)\n    field = FIELD_MAP.get('post_date', 'post_date')\n    post_date = data[field]\n    relpath = relpath.format(year=time.strftime(\"%Y\", post_date),\n                             month=time.strftime(\"%m\", post_date),\n                             day=time.strftime(\"%d\", post_date),\n                             name=name,\n                             title=name)\n\nreturn uniquify(os.path.join(os.path.abspath(root), relpath))", "path": "wp2md\\wp2md.py", "repo_name": "dreikanter/wp2md", "stars": 216, "license": "gpl-3.0", "language": "python", "size": 66}
{"docstring": "\"\"\"Generates MD-formatted comments list from parsed data.\"\"\"\n\n", "func_signal": "def generate_comments(comments):\n", "code": "result = str_t('')\nfor comment in comments:\n    try:\n        approved = comment['comment_approved'] == '1'\n        pingback = comment.get('comment_type', '').lower() == 'pingback'\n        if approved and not pingback:\n            cmfmt = str_t(\"**[{author}](#{id} \\\"{timestamp}\\\"):** {content}\\n\\n\")\n            content = html2md(comment['comment_content'])\n            result += cmfmt.format(id=comment['comment_id'],\n                                   timestamp=comment['comment_date'],\n                                   author=comment['comment_author'],\n                                   content=content)\n    except:\n        # Ignore malformed data\n        pass\n\nreturn result and str_t(\"## Comments\\n\\n\" + result)", "path": "wp2md\\wp2md.py", "repo_name": "dreikanter/wp2md", "stars": 216, "license": "gpl-3.0", "language": "python", "size": 66}
{"docstring": "\"\"\"returns a hash of css selectors, each of which contains a hash of css attributes\"\"\"\n# remove @import sentences\n", "func_signal": "def dumb_css_parser(data):\n", "code": "data += ';'\nimportIndex = data.find('@import')\nwhile importIndex != -1:\n    data = data[0:importIndex] + data[data.find(';', importIndex) + 1:]\n    importIndex = data.find('@import')\n\n# parse the css. reverted from dictionary compehension in order to support older pythons\nelements =  [x.split('{') for x in data.split('}') if '{' in x.strip()]\ntry:\n    elements = dict([(a.strip(), dumb_property_dict(b)) for a, b in elements])\nexcept ValueError:\n    elements = {} # not that important\n\nreturn elements", "path": "wp2md\\html2text.py", "repo_name": "dreikanter/wp2md", "stars": 216, "license": "gpl-3.0", "language": "python", "size": 66}
{"docstring": "\"\"\"Removes base_url prefix from MD links and image sources.\"\"\"\n", "func_signal": "def fix_urls(text):\n", "code": "global MD_URL_RE\nif MD_URL_RE is None:\n    base_url = re.escape(conf['base_url'])\n    MD_URL_RE = re.compile(r'\\]\\(%s(.*)\\)' % base_url)\nreturn MD_URL_RE.sub(r'](\\1)', text)", "path": "wp2md\\wp2md.py", "repo_name": "dreikanter/wp2md", "stars": 216, "license": "gpl-3.0", "language": "python", "size": 66}
{"docstring": "\"\"\"Returns string representation for elapsed time since last\nstopwatch_set() call.\"\"\"\n", "func_signal": "def stopwatch_get():\n", "code": "delta = datetime.datetime.now() - globals().get('_stopwatch_start_time', 0)\ndelta = str(delta).strip('0:')\nreturn ('0' + delta) if delta[0] == '.' else delta", "path": "wp2md\\wp2md.py", "repo_name": "dreikanter/wp2md", "stars": 216, "license": "gpl-3.0", "language": "python", "size": 66}
{"docstring": "\"\"\"handles various text emphases\"\"\"\n", "func_signal": "def handle_emphasis(self, start, tag_style, parent_style):\n", "code": "tag_emphasis = google_text_emphasis(tag_style)\nparent_emphasis = google_text_emphasis(parent_style)\n\n# handle Google's text emphasis\nstrikethrough =  'line-through' in tag_emphasis and self.hide_strikethrough\nbold = 'bold' in tag_emphasis and not 'bold' in parent_emphasis\nitalic = 'italic' in tag_emphasis and not 'italic' in parent_emphasis\nfixed = google_fixed_width_font(tag_style) and not \\\n        google_fixed_width_font(parent_style) and not self.pre\n\nif start:\n    # crossed-out text must be handled before other attributes\n    # in order not to output qualifiers unnecessarily\n    if bold or italic or fixed:\n        self.emphasis += 1\n    if strikethrough:\n        self.quiet += 1\n    if italic:\n        self.o(self.emphasis_mark)\n        self.drop_white_space += 1\n    if bold:\n        self.o(self.strong_mark)\n        self.drop_white_space += 1\n    if fixed:\n        self.o('`')\n        self.drop_white_space += 1\n        self.code = True\nelse:\n    if bold or italic or fixed:\n        # there must not be whitespace before closing emphasis mark\n        self.emphasis -= 1\n        self.space = 0\n        self.outtext = self.outtext.rstrip()\n    if fixed:\n        if self.drop_white_space:\n            # empty emphasis, drop it\n            self.drop_last(1)\n            self.drop_white_space -= 1\n        else:\n            self.o('`')\n        self.code = False\n    if bold:\n        if self.drop_white_space:\n            # empty emphasis, drop it\n            self.drop_last(2)\n            self.drop_white_space -= 1\n        else:\n            self.o(self.strong_mark)\n    if italic:\n        if self.drop_white_space:\n            # empty emphasis, drop it\n            self.drop_last(1)\n            self.drop_white_space -= 1\n        else:\n            self.o(self.emphasis_mark)\n    # space is only allowed after *all* emphasis marks\n    if (bold or italic) and not self.emphasis:\n            self.o(\" \")\n    if strikethrough:\n        self.quiet -= 1", "path": "wp2md\\html2text.py", "repo_name": "dreikanter/wp2md", "stars": 216, "license": "gpl-3.0", "language": "python", "size": 66}
{"docstring": "\"\"\"Generates MD-formatted index page.\"\"\"\n", "func_signal": "def generate_toc(meta, items):\n", "code": "if not meta.get('description', ''):\n    content = '\\n\\n'\nelse:\n    content = meta.get('description', '') + '\\n\\n'\nfor item in items:\n    content += str_t(\"* {post_date}: [{title}]({link})\\n\").format(**item)\nreturn content", "path": "wp2md\\wp2md.py", "repo_name": "dreikanter/wp2md", "stars": 216, "license": "gpl-3.0", "language": "python", "size": 66}
{"docstring": "# A function may not exist, if these bindings are used with an older or\n# incompatible version of libclang.so.\n", "func_signal": "def register_function(lib, item, ignore_errors):\n", "code": "try:\n    func = getattr(lib, item[0])\nexcept AttributeError as e:\n    msg = str(e) + \". Please ensure that your python bindings are \"\\\n                   \"compatible with your libclang.so version.\"\n    if ignore_errors:\n        return\n    raise LibclangError(msg)\n\nif len(item) >= 2:\n    func.argtypes = item[1]\n\nif len(item) >= 3:\n    func.restype = item[2]\n\nif len(item) == 4:\n    func.errcheck = item[3]", "path": "misc\\clang\\cindex.py", "repo_name": "bbchung/clighter", "stars": 129, "license": "gpl-3.0", "language": "python", "size": 495}
{"docstring": "\"\"\"Obtain a SourceRange from this translation unit.\n\nThe bounds of the SourceRange must ultimately be defined by a start and\nend SourceLocation. For the locations argument, you can pass:\n\n  - 2 SourceLocation instances in a 2-tuple or list.\n  - 2 int file offsets via a 2-tuple or list.\n  - 2 2-tuple or lists of (line, column) pairs in a 2-tuple or list.\n\ne.g.\n\nget_extent('foo.c', (5, 10))\nget_extent('foo.c', ((1, 1), (1, 15)))\n\"\"\"\n", "func_signal": "def get_extent(self, filename, locations):\n", "code": "f = self.get_file(filename)\n\nif len(locations) < 2:\n    raise Exception('Must pass object with at least 2 elements')\n\nstart_location, end_location = locations\n\nif hasattr(start_location, '__len__'):\n    start_location = SourceLocation.from_position(self, f,\n        start_location[0], start_location[1])\nelif isinstance(start_location, int):\n    start_location = SourceLocation.from_offset(self, f,\n        start_location)\n\nif hasattr(end_location, '__len__'):\n    end_location = SourceLocation.from_position(self, f,\n        end_location[0], end_location[1])\nelif isinstance(end_location, int):\n    end_location = SourceLocation.from_offset(self, f, end_location)\n\nassert isinstance(start_location, SourceLocation)\nassert isinstance(end_location, SourceLocation)\n\nreturn SourceRange.from_locations(start_location, end_location)", "path": "misc\\clang\\cindex.py", "repo_name": "bbchung/clighter", "stars": 129, "license": "gpl-3.0", "language": "python", "size": 495}
{"docstring": "\"\"\"\nRetrieve the Type (if any) of the entity pointed at by the cursor.\n\"\"\"\n", "func_signal": "def type(self):\n", "code": "if not hasattr(self, '_type'):\n    self._type = conf.lib.clang_getCursorType(self)\n\nreturn self._type", "path": "misc\\clang\\cindex.py", "repo_name": "bbchung/clighter", "stars": 129, "license": "gpl-3.0", "language": "python", "size": 495}
{"docstring": "\"\"\"Builds a CompilationDatabase from the database found in buildDir\"\"\"\n", "func_signal": "def fromDirectory(buildDir):\n", "code": "errorCode = c_uint()\ntry:\n    cdb = conf.lib.clang_CompilationDatabase_fromDirectory(buildDir,\n        byref(errorCode))\nexcept CompilationDatabaseError as e:\n    raise CompilationDatabaseError(int(errorCode.value),\n                                   \"CompilationDatabase loading failed\")\nreturn cdb", "path": "misc\\clang\\cindex.py", "repo_name": "bbchung/clighter", "stars": 129, "license": "gpl-3.0", "language": "python", "size": 495}
{"docstring": "\"\"\"Retrieve a container for the non-variadic arguments for this type.\n\nThe returned object is iterable and indexable. Each item in the\ncontainer is a Type instance.\n\"\"\"\n", "func_signal": "def argument_types(self):\n", "code": "class ArgumentsIterator(collections.Sequence):\n    def __init__(self, parent):\n        self.parent = parent\n        self.length = None\n\n    def __len__(self):\n        if self.length is None:\n            self.length = conf.lib.clang_getNumArgTypes(self.parent)\n\n        return self.length\n\n    def __getitem__(self, key):\n        # FIXME Support slice objects.\n        if not isinstance(key, int):\n            raise TypeError(\"Must supply a non-negative int.\")\n\n        if key < 0:\n            raise IndexError(\"Only non-negative indexes are accepted.\")\n\n        if key >= len(self):\n            raise IndexError(\"Index greater than container length: \"\n                             \"%d > %d\" % ( key, len(self) ))\n\n        result = conf.lib.clang_getArgType(self.parent, key)\n        if result.kind == TypeKind.INVALID:\n            raise IndexError(\"Argument could not be retrieved.\")\n\n        return result\n\nassert self.kind == TypeKind.FUNCTIONPROTO\nreturn ArgumentsIterator(self)", "path": "misc\\clang\\cindex.py", "repo_name": "bbchung/clighter", "stars": 129, "license": "gpl-3.0", "language": "python", "size": 495}
{"docstring": "\"\"\"Retrieve the number of elements in this type.\n\nReturns an int.\n\nIf the Type is not an array or vector, this raises.\n\"\"\"\n", "func_signal": "def element_count(self):\n", "code": "result = conf.lib.clang_getNumElements(self)\nif result < 0:\n    raise Exception('Type does not have elements.')\n\nreturn result", "path": "misc\\clang\\cindex.py", "repo_name": "bbchung/clighter", "stars": 129, "license": "gpl-3.0", "language": "python", "size": 495}
{"docstring": "\"\"\"\nGet an iterable object providing each argument in the\ncommand line for the compiler invocation as a _CXString.\n\nInvariant : the first argument is the compiler executable\n\"\"\"\n", "func_signal": "def arguments(self):\n", "code": "length = conf.lib.clang_CompileCommand_getNumArgs(self.cmd)\nfor i in xrange(length):\n    yield conf.lib.clang_CompileCommand_getArg(self.cmd, i)", "path": "misc\\clang\\cindex.py", "repo_name": "bbchung/clighter", "stars": 129, "license": "gpl-3.0", "language": "python", "size": 495}
{"docstring": "\"\"\"Return the mangled name for the entity referenced by this cursor.\"\"\"\n", "func_signal": "def mangled_name(self):\n", "code": "if not hasattr(self, '_mangled_name'):\n    self._mangled_name = conf.lib.clang_Cursor_getMangling(self)\n\nreturn self._mangled_name", "path": "misc\\clang\\cindex.py", "repo_name": "bbchung/clighter", "stars": 129, "license": "gpl-3.0", "language": "python", "size": 495}
{"docstring": "\"\"\"The command-line option that disables this diagnostic.\"\"\"\n", "func_signal": "def disable_option(self):\n", "code": "disable = _CXString()\nconf.lib.clang_getDiagnosticOption(self, byref(disable))\n\nreturn conf.lib.clang_getCString(disable)", "path": "misc\\clang\\cindex.py", "repo_name": "bbchung/clighter", "stars": 129, "license": "gpl-3.0", "language": "python", "size": 495}
{"docstring": "\"\"\"Return an iterator for accessing the fields of this type.\"\"\"\n\n", "func_signal": "def get_fields(self):\n", "code": "def visitor(field, children):\n    assert field != conf.lib.clang_getNullCursor()\n\n    # Create reference to TU so it isn't GC'd before Cursor.\n    field._tu = self._tu\n    fields.append(field)\n    return 1 # continue\nfields = []\nconf.lib.clang_Type_visitFields(self,\n                    callbacks['fields_visit'](visitor), fields)\nreturn iter(fields)", "path": "misc\\clang\\cindex.py", "repo_name": "bbchung/clighter", "stars": 129, "license": "gpl-3.0", "language": "python", "size": 495}
{"docstring": "\"\"\"Depth-first preorder walk over the cursor and its descendants.\n\nYields cursors.\n\"\"\"\n", "func_signal": "def walk_preorder(self):\n", "code": "yield self\nfor child in self.get_children():\n    for descendant in child.walk_preorder():\n        yield descendant", "path": "misc\\clang\\cindex.py", "repo_name": "bbchung/clighter", "stars": 129, "license": "gpl-3.0", "language": "python", "size": 495}
{"docstring": "\"\"\"Set the path in which to search for libclang\"\"\"\n", "func_signal": "def set_library_path(path):\n", "code": "if Config.loaded:\n    raise Exception(\"library path must be set before before using \" \\\n                    \"any other functionalities in libclang.\")\n\nConfig.library_path = path", "path": "misc\\clang\\cindex.py", "repo_name": "bbchung/clighter", "stars": 129, "license": "gpl-3.0", "language": "python", "size": 495}
{"docstring": "\"\"\"Register function prototypes with a libclang library instance.\n\nThis must be called as part of library instantiation so Python knows how\nto call out to the shared library.\n\"\"\"\n\n", "func_signal": "def register_functions(lib, ignore_errors):\n", "code": "def register(item):\n    return register_function(lib, item, ignore_errors)\n\nmap(register, functionList)", "path": "misc\\clang\\cindex.py", "repo_name": "bbchung/clighter", "stars": 129, "license": "gpl-3.0", "language": "python", "size": 495}
{"docstring": "\"\"\"\nRetrieve the ref-qualifier of the type.\n\"\"\"\n", "func_signal": "def get_ref_qualifier(self):\n", "code": "return RefQualifierKind.from_id(\n        conf.lib.clang_Type_getCXXRefQualifier(self))", "path": "misc\\clang\\cindex.py", "repo_name": "bbchung/clighter", "stars": 129, "license": "gpl-3.0", "language": "python", "size": 495}
{"docstring": "\"\"\"Return the semantic parent for this cursor.\"\"\"\n", "func_signal": "def semantic_parent(self):\n", "code": "if not hasattr(self, '_semantic_parent'):\n    self._semantic_parent = conf.lib.clang_getCursorSemanticParent(self)\n\nreturn self._semantic_parent", "path": "misc\\clang\\cindex.py", "repo_name": "bbchung/clighter", "stars": 129, "license": "gpl-3.0", "language": "python", "size": 495}
{"docstring": "\"\"\"Load the translation unit from the given source code file by running\nclang and generating the AST before loading. Additional command line\nparameters can be passed to clang via the args parameter.\n\nIn-memory contents for files can be provided by passing a list of pairs\nto as unsaved_files, the first item should be the filenames to be mapped\nand the second should be the contents to be substituted for the\nfile. The contents may be passed as strings or file objects.\n\nIf an error was encountered during parsing, a TranslationUnitLoadError\nwill be raised.\n\"\"\"\n", "func_signal": "def parse(self, path, args=None, unsaved_files=None, options = 0):\n", "code": "return TranslationUnit.from_source(path, args, unsaved_files, options,\n                                   self)", "path": "misc\\clang\\cindex.py", "repo_name": "bbchung/clighter", "stars": 129, "license": "gpl-3.0", "language": "python", "size": 495}
{"docstring": "\"\"\"\nCheck if the record is anonymous.\n\"\"\"\n", "func_signal": "def is_anonymous(self):\n", "code": "if self.kind == CursorKind.FIELD_DECL:\n    return self.type.get_declaration().is_anonymous()\nreturn conf.lib.clang_Cursor_isAnonymous(self)", "path": "misc\\clang\\cindex.py", "repo_name": "bbchung/clighter", "stars": 129, "license": "gpl-3.0", "language": "python", "size": 495}
{"docstring": "\"\"\"Get the enumeration name of this storage class.\"\"\"\n", "func_signal": "def name(self):\n", "code": "if self._name_map is None:\n    self._name_map = {}\n    for key,value in StorageClass.__dict__.items():\n        if isinstance(value,StorageClass):\n            self._name_map[value] = key\nreturn self._name_map[self]", "path": "misc\\clang\\cindex.py", "repo_name": "bbchung/clighter", "stars": 129, "license": "gpl-3.0", "language": "python", "size": 495}
{"docstring": "\"\"\"\nReturn the source location (the starting character) of the entity\npointed at by the cursor.\n\"\"\"\n", "func_signal": "def location(self):\n", "code": "if not hasattr(self, '_loc'):\n    self._loc = conf.lib.clang_getCursorLocation(self)\n\nreturn self._loc", "path": "misc\\clang\\cindex.py", "repo_name": "bbchung/clighter", "stars": 129, "license": "gpl-3.0", "language": "python", "size": 495}
{"docstring": "\"\"\"\nFor a cursor that is a reference, returns a cursor\nrepresenting the entity that it references.\n\"\"\"\n", "func_signal": "def referenced(self):\n", "code": "if not hasattr(self, '_referenced'):\n    self._referenced = conf.lib.clang_getCursorReferenced(self)\n\nreturn self._referenced", "path": "misc\\clang\\cindex.py", "repo_name": "bbchung/clighter", "stars": 129, "license": "gpl-3.0", "language": "python", "size": 495}
{"docstring": "\"\"\"Get a **delete** message.\n\"\"\"\n", "func_signal": "def delete(collection_name, spec, safe):\n", "code": "data = __ZERO\ndata += bson._make_c_string(collection_name)\ndata += __ZERO\ndata += bson.BSON.from_dict(spec)\nif safe:\n    (_, remove_message) = __pack_message(2006, data)\n    (request_id, error_message) = __last_error()\n    return (request_id, remove_message + error_message)\nelse:\n    return __pack_message(2006, data)", "path": "Resources\\pymongo\\message.py", "repo_name": "bububa/MongoHub", "stars": 134, "license": "other", "language": "python", "size": 1194}
{"docstring": "\"\"\"Initialize a new ObjectId_.\n\nIf `oid` is ``None``, create a new (unique)\nObjectId_. If `oid` is an instance of (``string``,\n:class:`ObjectId`) validate it and use that.  Otherwise, a\n:class:`TypeError` is raised. If `oid` is invalid,\n:class:`~pymongo.errors.InvalidId` is raised.\n\n:Parameters:\n  - `oid` (optional): a valid ObjectId_ (12 byte binary or 24 character\n    hex string)\n\n.. _ObjectId: http://www.mongodb.org/display/DOCS/Object+IDs\n\"\"\"\n", "func_signal": "def __init__(self, oid=None):\n", "code": "if oid is None:\n    self.__generate()\nelse:\n    self.__validate(oid)", "path": "Resources\\pymongo\\objectid.py", "repo_name": "bububa/MongoHub", "stars": 134, "license": "other", "language": "python", "size": 1194}
{"docstring": "\"\"\"Get a **getMore** message.\n\"\"\"\n", "func_signal": "def get_more(collection_name, num_to_return, cursor_id):\n", "code": "data = __ZERO\ndata += bson._make_c_string(collection_name)\ndata += struct.pack(\"<i\", num_to_return)\ndata += struct.pack(\"<q\", cursor_id)\nreturn __pack_message(2005, data)", "path": "Resources\\pymongo\\message.py", "repo_name": "bububa/MongoHub", "stars": 134, "license": "other", "language": "python", "size": 1194}
{"docstring": "\"\"\"Manipulate an outgoing SON object.\n\n:Parameters:\n  - `son`: the SON object being retrieved from the database\n  - `collection`: the collection this object was stored in\n\"\"\"\n", "func_signal": "def transform_outgoing(self, son, collection):\n", "code": "if self.will_copy():\n    return SON(son)\nreturn son", "path": "Resources\\pymongo\\son_manipulator.py", "repo_name": "bububa/MongoHub", "stars": 134, "license": "other", "language": "python", "size": 1194}
{"docstring": "\"\"\"Add the _ns field to the incoming object\n\"\"\"\n", "func_signal": "def transform_incoming(self, son, collection):\n", "code": "son[\"_ns\"] = collection.name()\nreturn son", "path": "Resources\\pymongo\\son_manipulator.py", "repo_name": "bububa/MongoHub", "stars": 134, "license": "other", "language": "python", "size": 1194}
{"docstring": "\"\"\"Replace embedded documents with DBRefs.\n\"\"\"\n\n", "func_signal": "def transform_incoming(self, son, collection):\n", "code": "def transform_value(value):\n    if isinstance(value, types.DictType):\n        if \"_id\" in value and \"_ns\" in value:\n            return DBRef(value[\"_ns\"], transform_value(value[\"_id\"]))\n        else:\n            return transform_dict(SON(value))\n    elif isinstance(value, types.ListType):\n        return [transform_value(v) for v in value]\n    return value\n\ndef transform_dict(object):\n    for (key, value) in object.items():\n        object[key] = transform_value(value)\n    return object\n\nreturn transform_dict(SON(son))", "path": "Resources\\pymongo\\son_manipulator.py", "repo_name": "bububa/MongoHub", "stars": 134, "license": "other", "language": "python", "size": 1194}
{"docstring": "\"\"\"Validate and use the given id for this ObjectId.\n\nRaises TypeError if id is not an instance of (str, ObjectId) and\nInvalidId if it is not a valid ObjectId.\n\n:Parameters:\n  - `oid`: a valid ObjectId\n\"\"\"\n", "func_signal": "def __validate(self, oid):\n", "code": "if isinstance(oid, ObjectId):\n    self.__id = oid.__id\nelif isinstance(oid, types.StringType):\n    if len(oid) == 12:\n        self.__id = oid\n    elif len(oid) == 24:\n        self.__id = oid.decode(\"hex\")\n    else:\n        raise InvalidId(\"%s is not a valid ObjectId\" % oid)\nelse:\n    raise TypeError(\"id must be an instance of (str, ObjectId), \"\n                    \"not %s\" % type(oid))", "path": "Resources\\pymongo\\objectid.py", "repo_name": "bububa/MongoHub", "stars": 134, "license": "other", "language": "python", "size": 1194}
{"docstring": "\"\"\"Convert a SON document to a normal Python dictionary instance.\n\nThis is trickier than just *dict(...)* because it needs to be\nrecursive.\n\"\"\"\n\n", "func_signal": "def to_dict(self):\n", "code": "def transform_value(value):\n    if isinstance(value, types.ListType):\n        return [transform_value(v) for v in value]\n    if isinstance(value, SON):\n        value = dict(value)\n    if isinstance(value, types.DictType):\n        for k, v in value.iteritems():\n            value[k] = transform_value(v)\n    return value\n\nreturn transform_value(dict(self))", "path": "Resources\\pymongo\\son.py", "repo_name": "bububa/MongoHub", "stars": 134, "license": "other", "language": "python", "size": 1194}
{"docstring": "\"\"\"Takes message data and adds a message header based on the operation.\n\nReturns the resultant message string.\n\"\"\"\n", "func_signal": "def __pack_message(operation, data):\n", "code": "request_id = random.randint(-2**31 - 1, 2**31)\nmessage = struct.pack(\"<i\", 16 + len(data))\nmessage += struct.pack(\"<i\", request_id)\nmessage += __ZERO # responseTo\nmessage += struct.pack(\"<i\", operation)\nreturn (request_id, message + data)", "path": "Resources\\pymongo\\message.py", "repo_name": "bububa/MongoHub", "stars": 134, "license": "other", "language": "python", "size": 1194}
{"docstring": "\"\"\"Add an _id field if it is missing.\n\"\"\"\n", "func_signal": "def transform_incoming(self, son, collection):\n", "code": "if not \"_id\" in son:\n    son[\"_id\"] = ObjectId()\nreturn son", "path": "Resources\\pymongo\\son_manipulator.py", "repo_name": "bububa/MongoHub", "stars": 134, "license": "other", "language": "python", "size": 1194}
{"docstring": "\"\"\"Get a **killCursors** message.\n\"\"\"\n", "func_signal": "def kill_cursors(cursor_ids):\n", "code": "data = __ZERO\ndata += struct.pack(\"<i\", len(cursor_ids))\nfor cursor_id in cursor_ids:\n    data += struct.pack(\"<q\", cursor_id)\nreturn __pack_message(2007, data)", "path": "Resources\\pymongo\\message.py", "repo_name": "bububa/MongoHub", "stars": 134, "license": "other", "language": "python", "size": 1194}
{"docstring": "\"\"\"Get an **insert** message.\n\"\"\"\n", "func_signal": "def insert(collection_name, docs, check_keys, safe):\n", "code": "data = __ZERO\ndata += bson._make_c_string(collection_name)\ndata += \"\".join([bson.BSON.from_dict(doc, check_keys) for doc in docs])\nif safe:\n    (_, insert_message) = __pack_message(2002, data)\n    (request_id, error_message) = __last_error()\n    return (request_id, insert_message + error_message)\nelse:\n    return __pack_message(2002, data)", "path": "Resources\\pymongo\\message.py", "repo_name": "bububa/MongoHub", "stars": 134, "license": "other", "language": "python", "size": 1194}
{"docstring": "\"\"\"Move _id to the front if it's there.\n\"\"\"\n", "func_signal": "def transform_incoming(self, son, collection):\n", "code": "if not \"_id\" in son:\n    return son\ntransformed = SON({\"_id\": son[\"_id\"]})\ntransformed.update(son)\nreturn transformed", "path": "Resources\\pymongo\\son_manipulator.py", "repo_name": "bububa/MongoHub", "stars": 134, "license": "other", "language": "python", "size": 1194}
{"docstring": "\"\"\"Manipulate an incoming SON object.\n\n:Parameters:\n  - `son`: the SON object to be inserted into the database\n  - `collection`: the collection the object is being inserted into\n\"\"\"\n", "func_signal": "def transform_incoming(self, son, collection):\n", "code": "if self.will_copy():\n    return SON(son)\nreturn son", "path": "Resources\\pymongo\\son_manipulator.py", "repo_name": "bububa/MongoHub", "stars": 134, "license": "other", "language": "python", "size": 1194}
{"docstring": "\"\"\"Get the machine portion of an ObjectId.\n\"\"\"\n", "func_signal": "def _machine_bytes():\n", "code": "machine_hash = _md5func()\nmachine_hash.update(socket.gethostname())\nreturn machine_hash.digest()[0:3]", "path": "Resources\\pymongo\\objectid.py", "repo_name": "bububa/MongoHub", "stars": 134, "license": "other", "language": "python", "size": 1194}
{"docstring": "\"\"\"Get an **update** message.\n\"\"\"\n", "func_signal": "def update(collection_name, upsert, multi, spec, doc, safe):\n", "code": "options = 0\nif upsert:\n    options += 1\nif multi:\n    options += 2\n\ndata = __ZERO\ndata += bson._make_c_string(collection_name)\ndata += struct.pack(\"<i\", options)\ndata += bson.BSON.from_dict(spec)\ndata += bson.BSON.from_dict(doc)\nif safe:\n    (_, update_message) = __pack_message(2001, data)\n    (request_id, error_message) = __last_error()\n    return (request_id, update_message + error_message)\nelse:\n    return __pack_message(2001, data)", "path": "Resources\\pymongo\\message.py", "repo_name": "bububa/MongoHub", "stars": 134, "license": "other", "language": "python", "size": 1194}
{"docstring": "\"\"\"Replace DBRefs with embedded documents.\n\"\"\"\n\n", "func_signal": "def transform_outgoing(self, son, collection):\n", "code": "def transform_value(value):\n    if isinstance(value, DBRef):\n        return self.__database.dereference(value)\n    elif isinstance(value, types.ListType):\n        return [transform_value(v) for v in value]\n    elif isinstance(value, types.DictType):\n        return transform_dict(SON(value))\n    return value\n\ndef transform_dict(object):\n    for (key, value) in object.items():\n        object[key] = transform_value(value)\n    return object\n\nreturn transform_dict(SON(son))", "path": "Resources\\pymongo\\son_manipulator.py", "repo_name": "bububa/MongoHub", "stars": 134, "license": "other", "language": "python", "size": 1194}
{"docstring": "\"\"\"Create an instance of SON from an xml document.\n\nThis is really only used for testing, and is probably unnecessary.\n\"\"\"\n", "func_signal": "def from_xml(cls, xml):\n", "code": "try:\n    import xml.etree.ElementTree as ET\nexcept ImportError:\n    import elementtree.ElementTree as ET\n\nfrom code import Code\nfrom binary import Binary\nfrom objectid import ObjectId\nfrom dbref import DBRef\nfrom errors import UnsupportedTag\n\ndef pad(list, index):\n    while index >= len(list):\n        list.append(None)\n\ndef make_array(array):\n    doc = make_doc(array)\n    array = []\n    for (key, value) in doc.items():\n        index = int(key)\n        pad(array, index)\n        array[index] = value\n    return array\n\ndef make_string(string):\n    return string.text is not None and unicode(string.text) or u\"\"\n\ndef make_code(code):\n    return code.text is not None and Code(code.text) or Code(\"\")\n\ndef make_binary(binary):\n    if binary.text is not None:\n        return Binary(base64.decodestring(binary.text))\n    return Binary(\"\")\n\ndef make_boolean(bool):\n    return bool.text == \"true\"\n\ndef make_date(date):\n    return datetime.datetime.utcfromtimestamp(float(date.text) /\n                                              1000.0)\n\ndef make_ref(dbref):\n    return DBRef(make_elem(dbref[0]), make_elem(dbref[1]))\n\ndef make_oid(oid):\n    return ObjectId(binascii.unhexlify(oid.text))\n\ndef make_int(data):\n    return int(data.text)\n\ndef make_null(null):\n    return None\n\ndef make_number(number):\n    return float(number.text)\n\ndef make_regex(regex):\n    return re.compile(make_elem(regex[0]), make_elem(regex[1]))\n\ndef make_options(data):\n    options = 0\n    if not data.text:\n        return options\n    if \"i\" in data.text:\n        options |= re.IGNORECASE\n    if \"l\" in data.text:\n        options |= re.LOCALE\n    if \"m\" in data.text:\n        options |= re.MULTILINE\n    if \"s\" in data.text:\n        options |= re.DOTALL\n    if \"u\" in data.text:\n        options |= re.UNICODE\n    if \"x\" in data.text:\n        options |= re.VERBOSE\n    return options\n\ndef make_elem(elem):\n    try:\n        return {\"array\": make_array,\n                \"doc\": make_doc,\n                \"string\": make_string,\n                \"binary\": make_binary,\n                \"boolean\": make_boolean,\n                \"code\": make_code,\n                \"date\": make_date,\n                \"ref\": make_ref,\n                \"ns\": make_string,\n                \"oid\": make_oid,\n                \"int\": make_int,\n                \"null\": make_null,\n                \"number\": make_number,\n                \"pattern\": make_string,\n                \"options\": make_options,\n                }[elem.tag](elem)\n    except KeyError:\n        raise UnsupportedTag(\"cannot parse tag: %s\" % elem.tag)\n\ndef make_doc(doc):\n    son = SON()\n    for elem in doc:\n        son[elem.attrib[\"name\"]] = make_elem(elem)\n    return son\n\ntree = ET.XML(xml)\ndoc = tree[1]\n\nreturn make_doc(doc)", "path": "Resources\\pymongo\\son.py", "repo_name": "bububa/MongoHub", "stars": 134, "license": "other", "language": "python", "size": 1194}
{"docstring": "\"\"\"Generate a new value for this ObjectId.\n\"\"\"\n", "func_signal": "def __generate(self):\n", "code": "oid = \"\"\n\n# 4 bytes current time\noid += struct.pack(\">i\", int(time.time()))\n\n# 3 bytes machine\noid += ObjectId._machine_bytes\n\n# 2 bytes pid\noid += struct.pack(\">H\", os.getpid() % 0xFFFF)\n\n# 3 bytes inc\nObjectId._inc_lock.acquire()\noid += struct.pack(\">i\", ObjectId._inc)[1:4]\nObjectId._inc = (ObjectId._inc + 1) % 0xFFFFFF\nObjectId._inc_lock.release()\n\nself.__id = oid", "path": "Resources\\pymongo\\objectid.py", "repo_name": "bububa/MongoHub", "stars": 134, "license": "other", "language": "python", "size": 1194}
{"docstring": "# Make progressively weaker assumptions about \"other\"\n", "func_signal": "def update(self, other=None, **kwargs):\n", "code": "if other is None:\n    pass\nelif hasattr(other, 'iteritems'):  # iteritems saves memory and lookups\n    for k, v in other.iteritems():\n        self[k] = v\nelif hasattr(other, 'keys'):\n    for k in other.keys():\n        self[k] = other[k]\nelse:\n    for k, v in other:\n        self[k] = v\nif kwargs:\n    self.update(kwargs)", "path": "Resources\\pymongo\\son.py", "repo_name": "bububa/MongoHub", "stars": 134, "license": "other", "language": "python", "size": 1194}
{"docstring": "\"\"\"Special corner case 1\n    bottom left corner: |1|2|3|\n\n    top right corner:   |4|5|\n                          |6|\n                          |7|\n                          |8| \"\"\"\n\n", "func_signal": "def place_special_1(self, codeword):\n", "code": "self.place_bit((self.rows - 1, 0), (codeword & (0x01 << 7)) >> 7)\nself.place_bit((self.rows - 1, 1), (codeword & (0x01 << 6)) >> 6)\nself.place_bit((self.rows - 1, 2), (codeword & (0x01 << 5)) >> 5)\nself.place_bit((0, self.cols - 2), (codeword & (0x01 << 4)) >> 4)\nself.place_bit((0, self.cols - 1), (codeword & (0x01 << 3)) >> 3)\nself.place_bit((1, self.cols - 1), (codeword & (0x01 << 2)) >> 2)\nself.place_bit((2, self.cols - 1), (codeword & (0x01 << 1)) >> 1)\nself.place_bit((3, self.cols - 1), codeword & 0x01)", "path": "hubarcode\\datamatrix\\placement.py", "repo_name": "hudora/huBarcode", "stars": 146, "license": "None", "language": "python", "size": 4268}
{"docstring": "\"\"\"Create QR Code matrix\"\"\"\n\n", "func_signal": "def create_matrix(self):\n", "code": "matrix_content = self.minfo.create_matrix(self.version, self.codewords)\nself.mtx_size = len(matrix_content)\n\nLOG.debug(\"Matrix size is %d\", self.mtx_size)\n\nmask_number = self.minfo.calc_mask_number(matrix_content)\nmask_content = 1 << mask_number\n\nformat_info_value = ((self.ecl << 3) | mask_number)\nself.minfo.put_format_info(matrix_content, format_info_value)\nself.matrix = self.minfo.finalize(matrix_content, mask_content)", "path": "hubarcode\\qrcode\\textencoder.py", "repo_name": "hudora/huBarcode", "stars": 146, "license": "None", "language": "python", "size": 4268}
{"docstring": "\"\"\"Pad out the encoded text to the correct word length\"\"\"\n\n", "func_signal": "def pad(self):\n", "code": "pads = [236, 17]\npad_idx = 0\nfor _ in range(len(self.codewords), self.max_data_codewords):\n    self.codewords.append(pads[pad_idx])\n    pad_idx = 1 - pad_idx\n# end for", "path": "hubarcode\\qrcode\\textencoder.py", "repo_name": "hudora/huBarcode", "stars": 146, "license": "None", "language": "python", "size": 4268}
{"docstring": "\"\"\"Set up the encoder with the input text.\nThis will encode the text,\nand create a matrix with the resulting codewords\"\"\"\n\n", "func_signal": "def __init__(self, text, ecl=None):\n", "code": "enc = TextEncoder()\nself.matrix = enc.encode(text, ecl)\nself.height = 0\nself.width = 0", "path": "hubarcode\\qrcode\\__init__.py", "repo_name": "hudora/huBarcode", "stars": 146, "license": "None", "language": "python", "size": 4268}
{"docstring": "\"\"\"Return an ascii representation of the matrix\"\"\"\n", "func_signal": "def get_ascii(self):\n", "code": "qrc = QRCodeRenderer(self.matrix)\nreturn qrc.get_ascii()", "path": "hubarcode\\qrcode\\__init__.py", "repo_name": "hudora/huBarcode", "stars": 146, "license": "None", "language": "python", "size": 4268}
{"docstring": "\"\"\"Prepend 'bitsnum' bits to the begining of bit stream\"\"\"\n\n", "func_signal": "def prepend(self, value, bitsnum):\n", "code": "if (bitsnum < 1):\n    raise ValueError(\"Wrong value for number of bits (%d)\" % bitsnum)\n# end if\nfor i in range(0, bitsnum, 1):\n    self.data.insert(0, (value >> i) & 0x01)\n# end for", "path": "hubarcode\\qrcode\\textencoder.py", "repo_name": "hudora/huBarcode", "stars": 146, "license": "None", "language": "python", "size": 4268}
{"docstring": "\"\"\"Place all the given codewords into the given matrix\nMatrix should be correctly pre-sized\"\"\"\n\n", "func_signal": "def place(self, codewords, matrix):\n", "code": "self.matrix = matrix\nself.rows = len(matrix)\nself.cols = len(matrix[0])\n\nrow, col = 4, 0\n\ncw_list = [ord(codeword) for codeword in codewords]\n\nwhile True:\n\n    # Special corner cases\n    if row == self.rows and col == 0:\n        self.place_special_1(cw_list.pop(0))\n\n    elif row == self.rows - 2 and col == 0 and self.cols % 4:\n        self.place_special_2(cw_list.pop(0))\n\n    elif row == self.rows - 2 and col == 0 and (self.cols % 8 == 4):\n        self.place_special_3(cw_list.pop(0))\n\n    elif row == self.rows + 4 and col == 2 and (self.cols % 8 == 0):\n        self.place_special_4(cw_list.pop(0))\n\n    # Sweep upwards diagonally\n    while True:\n        if row < self.rows and col >= 0 and self.matrix[row][col] is None:\n            self.place_standard_shape((row, col), cw_list.pop(0))\n\n        row -= 2\n        col += 2\n\n        if row < 0 or col >= self.cols:\n            break\n\n    row += 1\n    col += 3\n\n    # Sweep downwards diagonally\n    while True:\n        if row >= 0 and col < self.cols and self.matrix[row][col] is None:\n            self.place_standard_shape((row, col), cw_list.pop(0))\n\n        row += 2\n        col -= 2\n\n        if row >= self.rows or col < 0:\n            break\n\n    row += 3\n    col += 1\n\n    if row >= self.rows and col >= self.cols:\n        break\n\n# Fill in any remaining Nones\nfor row in self.matrix:\n    for i in range(len(row)):\n        if row[i] is None:\n            row[i] = 0", "path": "hubarcode\\datamatrix\\placement.py", "repo_name": "hudora/huBarcode", "stars": 146, "license": "None", "language": "python", "size": 4268}
{"docstring": "\"\"\"Encode the given text into bitstream\"\"\"\n\n", "func_signal": "def encode_text(self, text):\n", "code": "char_count_num = 8\nresult_len = 4 + 8 * len(text)\nterminator_len = 4\n# Calculate smallest symbol version\nfor self.version in range(1, 42):\n    if self.version == 10:\n        char_count_num = 16\n        result_len += 8\n    elif self.version == 41:\n        raise ValueError(\"QRCode cannot store %d bits\" % result_len)\n    # end if\n\n    max_bits = isodata.MAX_DATA_BITS[self.version - 1 + 40 * self.ecl]\n    if max_bits >= result_len:\n        if max_bits - result_len < 4:\n            terminator_len = max_bits - result_len\n        # end if\n        self.max_data_codewords = max_bits >> 3\n        break\n    # end if\n# end for\n\nbitstream = BitStream()\nfor char in text:\n    bitstream.append(ord(char), 8)\n# end for\n\nbitstream.prepend(len(text), char_count_num)\n# write 'byte' mode\nbitstream.prepend(4, 4)\n# add terminator\nbitstream.append(0, terminator_len)\n# convert bitstream into codewords\nbyte = 0\nbit_num = 7\nfor bit in bitstream.data:\n    byte |= bit << bit_num\n    bit_num -= 1\n    if bit_num == -1:\n        self.codewords.append(byte)\n        bit_num = 7\n        byte = 0\n    # end if\n# end for", "path": "hubarcode\\qrcode\\textencoder.py", "repo_name": "hudora/huBarcode", "stars": 146, "license": "None", "language": "python", "size": 4268}
{"docstring": "\"\"\"Write the matrix out to an image file\"\"\"\n\n", "func_signal": "def save(self, filename, cellsize=5):\n", "code": "qrc = QRCodeRenderer(self.matrix)\nqrc.write_file(cellsize, filename)", "path": "hubarcode\\qrcode\\__init__.py", "repo_name": "hudora/huBarcode", "stars": 146, "license": "None", "language": "python", "size": 4268}
{"docstring": "\"\"\"Encode the given text and add padding and error codes\nalso set up the correct matrix size for the resulting codewords\"\"\"\n\n", "func_signal": "def encode(self, text, ecl=None):\n", "code": "self.__init__()\nif ecl is None:\n    ecl = 'M'\n# end if\nstr2ecl = {\"L\": 1, \"l\": 1, \"M\": 0, \"m\": 0, \"Q\": 3, \"q\": 3, \"H\": 2, \"h\": 2}\nself.ecl = str2ecl[ecl]\n\nself.encode_text(text)\n\nself.pad()\n\nself.minfo = isodata.MatrixInfo(self.version, self.ecl)\n\nself.append_error_codes()\n\nLOG.debug(\n    \"Codewords: \" + ' '.join([str(codeword) for codeword in self.codewords]))\n\nself.create_matrix()\n\nreturn self.matrix", "path": "hubarcode\\qrcode\\textencoder.py", "repo_name": "hudora/huBarcode", "stars": 146, "license": "None", "language": "python", "size": 4268}
{"docstring": "\"\"\"Modulo-10 calculation of the barcode check digit\nFirst, we take the rightmost digit of the value and consider it to be\nan \"odd\" character. We then move right-to-left, alternating between\nodd and even. We then sum the numeric value of all the even positions,\nand sum the numeric value multiplied by three of all the\nodd positions.\"\"\"\n\n", "func_signal": "def calculate_check_digit(self):\n", "code": "def sum_str(total, digit):\n    \"\"\"add a stringified digit to the total sum\"\"\"\n    return total + int(digit)\n\n# sum the \"odd\" digits (1,3,5,7,9,11) and multiply by 3\noddsum = reduce(sum_str, self.code[1::2], 0)\n\n# sum the \"even\" digits (0,2,4,6,8,10)\nevensum = reduce(sum_str, self.code[:12:2], 0)\n\n# add them up\ntotal = oddsum * 3 + evensum\n\n# check digit is the number that can be added to the total\n# to get to a multiple of 10\nreturn (10 - (total % 10)) % 10", "path": "hubarcode\\ean13\\__init__.py", "repo_name": "hudora/huBarcode", "stars": 146, "license": "None", "language": "python", "size": 4268}
{"docstring": "\"\"\"Write the barcode out to a PNG bytestream\"\"\"\n", "func_signal": "def get_imagedata(self, bar_width=3):\n", "code": "barcode = EAN13Renderer(\n    self.full_code, self.left_bars, self.right_bars, GUARDS)\nimagedata = barcode.get_imagedata(bar_width)\nself.height = barcode.height\nself.width = barcode.width\nreturn imagedata", "path": "hubarcode\\ean13\\__init__.py", "repo_name": "hudora/huBarcode", "stars": 146, "license": "None", "language": "python", "size": 4268}
{"docstring": "\"\"\"Encode the barcode number and return the left and right\ndata strings\"\"\"\n\n", "func_signal": "def encode(self):\n", "code": "parity_values = self.get_parity()\n\nself.left_bars = \"\"\nself.right_bars = \"\"\n\n# Exclude the first number system digit, this was\n# for determining the left parity\nfor parity, digit in zip(parity_values, self.full_code[1:7]):\n    self.left_bars += encoding.get_left_encoded(int(digit), parity)\nfor digit in self.full_code[7:]:\n    self.right_bars += encoding.get_right_encoded(int(digit))\n\nreturn self.left_bars, self.right_bars", "path": "hubarcode\\ean13\\__init__.py", "repo_name": "hudora/huBarcode", "stars": 146, "license": "None", "language": "python", "size": 4268}
{"docstring": "\"\"\"Initialize with an empty matrix\"\"\"\n", "func_signal": "def __init__(self):\n", "code": "self.matrix = None\nself.rows = 0\nself.cols = 0", "path": "hubarcode\\datamatrix\\placement.py", "repo_name": "hudora/huBarcode", "stars": 146, "license": "None", "language": "python", "size": 4268}
{"docstring": "\"\"\"Special corner case 3\n    bottom left corner: |1|\n                        |2|\n                        |3|\n\n    top right corner:   |4|5|\n                          |6|\n                          |7|\n                          |8| \"\"\"\n\n", "func_signal": "def place_special_3(self, codeword):\n", "code": "self.place_bit((self.rows - 3, 0), (codeword & (0x01 << 7)) >> 7)\nself.place_bit((self.rows - 2, 0), (codeword & (0x01 << 6)) >> 6)\nself.place_bit((self.rows - 1, 0), (codeword & (0x01 << 5)) >> 5)\nself.place_bit((0, self.cols - 2), (codeword & (0x01 << 4)) >> 4)\nself.place_bit((0, self.cols - 1), (codeword & (0x01 << 3)) >> 3)\nself.place_bit((1, self.cols - 1), (codeword & (0x01 << 2)) >> 2)\nself.place_bit((2, self.cols - 1), (codeword & (0x01 << 1)) >> 1)\nself.place_bit((3, self.cols - 1), codeword & 0x01)", "path": "hubarcode\\datamatrix\\placement.py", "repo_name": "hudora/huBarcode", "stars": 146, "license": "None", "language": "python", "size": 4268}
{"docstring": "\"\"\"Special corner case 4\n    bottom left corner: |1|\n\n    bottom right corner: |2|\n\n    top right corner: |3|4|5|\n                      |6|7|8| \"\"\"\n\n", "func_signal": "def place_special_4(self, codeword):\n", "code": "self.place_bit((self.rows - 1, 0), (codeword & (0x01 << 7)) >> 7)\nself.place_bit((self.rows - 1, self.cols - 1),\n               (codeword & (0x01 << 6)) >> 6)\nself.place_bit((0, self.cols - 3), (codeword & (0x01 << 5)) >> 5)\nself.place_bit((0, self.cols - 2), (codeword & (0x01 << 4)) >> 4)\nself.place_bit((0, self.cols - 1), (codeword & (0x01 << 3)) >> 3)\nself.place_bit((1, self.cols - 3), (codeword & (0x01 << 2)) >> 2)\nself.place_bit((1, self.cols - 2), (codeword & (0x01 << 1)) >> 1)\nself.place_bit((1, self.cols - 1), codeword & 0x01)", "path": "hubarcode\\datamatrix\\placement.py", "repo_name": "hudora/huBarcode", "stars": 146, "license": "None", "language": "python", "size": 4268}
{"docstring": "\"\"\"Append 'bitsnum' bits to the end of bit stream\"\"\"\n\n", "func_signal": "def append(self, value, bitsnum):\n", "code": "if (bitsnum < 1):\n    raise ValueError(\"Wrong value for number of bits (%d)\" % bitsnum)\n# end if\nfor i in range(bitsnum - 1, -1, -1):\n    self.data.append((value >> i) & 0x01)\n# end for", "path": "hubarcode\\qrcode\\textencoder.py", "repo_name": "hudora/huBarcode", "stars": 146, "license": "None", "language": "python", "size": 4268}
{"docstring": "# By the time finalize_options is called, install.install_lib is set to the\n# fixed directory, so we set the installdir to install_lib. The\n# install_data class uses ('install_data', 'install_dir') instead.\n", "func_signal": "def finalize_options(self):\n", "code": "self.set_undefined_options('install', ('install_lib', 'install_dir'))\ninstall_data.finalize_options(self)", "path": "setup.py", "repo_name": "hudora/huBarcode", "stars": 146, "license": "None", "language": "python", "size": 4268}
{"docstring": "\"\"\"Write the barcode out to an image file\"\"\"\n", "func_signal": "def save(self, filename, bar_width=3):\n", "code": "EAN13Renderer(self.full_code,\n              self.left_bars,\n              self.right_bars,\n              GUARDS).write_file(filename, bar_width)", "path": "hubarcode\\ean13\\__init__.py", "repo_name": "hudora/huBarcode", "stars": 146, "license": "None", "language": "python", "size": 4268}
{"docstring": "\"\"\"Set up the encoder with the concatenated input values.\ncode must be 12 digits long in the following format\nnnmmmmmppppp\nwhere n is the number system\nm is the manufacturing code\np is the product code\"\"\"\n\n# Make sure it's 12 digits long\n", "func_signal": "def __init__(self, code):\n", "code": "if len(code) == 13:\n    # cut of check digit\n    code = code[:-1]\nif code.isdigit() and len(code) == 12:\n    self.code = code\n    self.check_digit = self.calculate_check_digit()\n    self.full_code = self.code + str(self.check_digit)\n    self.left_bars = \"\"\n    self.right_bars = \"\"\n    self.height = 0\n    self.width = 0\n    self.encode()\nelse:\n    raise Exception(\"code must be 12 digits long\")", "path": "hubarcode\\ean13\\__init__.py", "repo_name": "hudora/huBarcode", "stars": 146, "license": "None", "language": "python", "size": 4268}
{"docstring": "\"\"\"create the member variables, but change hyphens to\nunderscores\n\"\"\"\n\n", "func_signal": "def initialize_options(self):\n", "code": "self.option_to_cmds = {}\nfor opt in self.__parser.option_list:\n    cmd_name = opt._long_opts[0][2:]\n    option_name = cmd_name.replace('-', '_')\n    self.option_to_cmds[option_name] = cmd_name\n    setattr(self, option_name, None)\nself.attr  = None", "path": "Contents\\Libraries\\Shared\\nose\\commands.py", "repo_name": "mikew/ss-plex.bundle", "stars": 137, "license": "None", "language": "python", "size": 4922}
{"docstring": "\"\"\"Convenient factory for objects implementing response interface.\n\ndata: string containing response body data\nheaders: sequence of (name, value) pairs\nurl: URL of response\ncode: integer response code (e.g. 200)\nmsg: string response code message (e.g. \"OK\")\n\n\"\"\"\n", "func_signal": "def make_response(data, headers, url, code, msg):\n", "code": "mime_headers = make_headers(headers)\nr = closeable_response(StringIO(data), mime_headers, url, code, msg)\nreturn response_seek_wrapper(r)", "path": "Contents\\Libraries\\Shared\\ss\\mechanize\\_response.py", "repo_name": "mikew/ss-plex.bundle", "stars": 137, "license": "None", "language": "python", "size": 4922}
{"docstring": "# Special case for test generators.\n", "func_signal": "def noseFunctionDescription(test):\n", "code": "if test.descriptor is not None:\n    if hasattr(test.test, 'description'):\n        return test.test.description\n    return \"holds for %s\" % ', '.join(map(str, test.arg))\nreturn test.test.func_doc or underscored2spec(test.test.func_name)", "path": "Contents\\Libraries\\Shared\\spec.py", "repo_name": "mikew/ss-plex.bundle", "stars": 137, "license": "None", "language": "python", "size": 4922}
{"docstring": "\"\"\"Return ``ContextSuite`` for tests. ``tests`` may either\nbe a callable (in which case the resulting ContextSuite will\nhave no parent context and be evaluated lazily) or an\niterable. In that case the tests will wrapped in\nnose.case.Test, be examined and the context of each found and a\nsuite of suites returned, organized into a stack with the\noutermost suites belonging to the outermost contexts.\n\"\"\"\n", "func_signal": "def __call__(self, tests, **kw):\n", "code": "log.debug(\"Create suite for %s\", tests)\ncontext = kw.pop('context', getattr(tests, 'context', None))\nlog.debug(\"tests %s context %s\", tests, context)\nif context is None:\n    tests = self.wrapTests(tests)\n    try:\n        context = self.findContext(tests)\n    except MixedContextError:\n        return self.makeSuite(self.mixedSuites(tests), None, **kw)\nreturn self.makeSuite(tests, context, **kw)", "path": "Contents\\Libraries\\Shared\\nose\\suite.py", "repo_name": "mikew/ss-plex.bundle", "stars": 137, "license": "None", "language": "python", "size": 4922}
{"docstring": "# in order to wrap response objects that are also exceptions, we must\n# dynamically subclass the exception :-(((\n", "func_signal": "def get_seek_wrapper_class(response):\n", "code": "if (isinstance(response, urllib2.HTTPError) and\n    not hasattr(response, \"seek\")):\n    if response.__class__.__module__ == \"__builtin__\":\n        exc_class_name = response.__class__.__name__\n    else:\n        exc_class_name = \"%s.%s\" % (\n            response.__class__.__module__, response.__class__.__name__)\n\n    class httperror_seek_wrapper(response_seek_wrapper, response.__class__):\n        # this only derives from HTTPError in order to be a subclass --\n        # the HTTPError behaviour comes from delegation\n\n        _exc_class_name = exc_class_name\n\n        def __init__(self, wrapped):\n            response_seek_wrapper.__init__(self, wrapped)\n            # be compatible with undocumented HTTPError attributes :-(\n            self.hdrs = wrapped.info()\n            self.filename = wrapped.geturl()\n\n        def __repr__(self):\n            return (\n                \"<%s (%s instance) at %s \"\n                \"whose wrapped object = %r>\" % (\n                self.__class__.__name__, self._exc_class_name,\n                hex(abs(id(self))), self.wrapped)\n                )\n    wrapper_class = httperror_seek_wrapper\nelse:\n    wrapper_class = response_seek_wrapper\nreturn wrapper_class", "path": "Contents\\Libraries\\Shared\\ss\\mechanize\\_response.py", "repo_name": "mikew/ss-plex.bundle", "stars": 137, "license": "None", "language": "python", "size": 4922}
{"docstring": "\"\"\"Run tests in suite inside of suite fixtures.\n\"\"\"\n# proxy the result for myself\n", "func_signal": "def run(self, result):\n", "code": "log.debug(\"suite %s (%s) run called, tests: %s\", id(self), self, self._tests)\n#import pdb\n#pdb.set_trace()\nif self.resultProxy:\n    result, orig = self.resultProxy(result, self), result\nelse:\n    result, orig = result, result\ntry:\n    self.setUp()\nexcept KeyboardInterrupt:\n    raise\nexcept:\n    self.error_context = 'setup'\n    result.addError(self, self._exc_info())\n    return\ntry:\n    for test in self._tests:\n        if result.shouldStop:\n            log.debug(\"stopping\")\n            break\n        # each nose.case.Test will create its own result proxy\n        # so the cases need the original result, to avoid proxy\n        # chains\n        test(orig)\nfinally:\n    self.has_run = True\n    try:\n        self.tearDown()\n    except KeyboardInterrupt:\n        raise\n    except:\n        self.error_context = 'teardown'\n        result.addError(self, self._exc_info())", "path": "Contents\\Libraries\\Shared\\nose\\suite.py", "repo_name": "mikew/ss-plex.bundle", "stars": 137, "license": "None", "language": "python", "size": 4922}
{"docstring": "\"\"\"Return a copy of response that supports seekable response interface.\n\nAccepts responses from both mechanize and urllib2 handlers.\n\nCopes with both ordinary response instances and HTTPError instances (which\ncan't be simply wrapped due to the requirement of preserving the exception\nbase class).\n\"\"\"\n", "func_signal": "def seek_wrapped_response(response):\n", "code": "if not hasattr(response, \"seek\"):\n    wrapper_class = get_seek_wrapper_class(response)\n    response = wrapper_class(response)\nassert hasattr(response, \"get_data\")\nreturn response", "path": "Contents\\Libraries\\Shared\\ss\\mechanize\\_response.py", "repo_name": "mikew/ss-plex.bundle", "stars": 137, "license": "None", "language": "python", "size": 4922}
{"docstring": "# Test generators set their own contexts.\n", "func_signal": "def testContext(test):\n", "code": "if isinstance(test.test, nose.case.FunctionTestCase) \\\n       and test.test.descriptor is not None:\n    return test.test.descriptor\n# So do doctests.\nelif isinstance(test.test, doctest.DocTestCase):\n    return test.test\nelse:\n    return test.context", "path": "Contents\\Libraries\\Shared\\spec.py", "repo_name": "mikew/ss-plex.bundle", "stars": 137, "license": "None", "language": "python", "size": 4922}
{"docstring": "\"\"\"Return the ancestry of the context (that is, all of the\npackages and modules containing the context), in order of\ndescent with the outermost ancestor last.\nThis method is a generator.\n\"\"\"\n", "func_signal": "def ancestry(self, context):\n", "code": "log.debug(\"get ancestry %s\", context)\nif context is None:\n    return\n# Methods include reference to module they are defined in, we\n# don't want that, instead want the module the class is in now\n# (classes are re-ancestored elsewhere).\nif hasattr(context, 'im_class'):\n    context = context.im_class\nelif hasattr(context, '__self__'):\n    context = context.__self__.__class__\nif hasattr(context, '__module__'):\n    ancestors = context.__module__.split('.')\nelif hasattr(context, '__name__'):\n    ancestors = context.__name__.split('.')[:-1]\nelse:\n    raise TypeError(\"%s has no ancestors?\" % context)\nwhile ancestors:\n    log.debug(\" %s ancestors %s\", context, ancestors)\n    yield resolve_name('.'.join(ancestors))\n    ancestors.pop()", "path": "Contents\\Libraries\\Shared\\nose\\suite.py", "repo_name": "mikew/ss-plex.bundle", "stars": 137, "license": "None", "language": "python", "size": 4922}
{"docstring": "\"\"\"Bottleneck to fix up IronPython string exceptions\n\"\"\"\n", "func_signal": "def _exc_info(self):\n", "code": "e = self.exc_info()\nif sys.platform == 'cli':\n    if isinstance(e[0], StringException):\n        # IronPython throws these StringExceptions, but\n        # traceback checks type(etype) == str. Make a real\n        # string here.\n        e = (str(e[0]), e[1], e[2])\n\nreturn e", "path": "Contents\\Libraries\\Shared\\nose\\suite.py", "repo_name": "mikew/ss-plex.bundle", "stars": 137, "license": "None", "language": "python", "size": 4922}
{"docstring": "\"\"\"Remove trailing needle string (if exists).\n\n>>> remove_trailing('Test', 'ThisAndThatTest')\n'ThisAndThat'\n>>> remove_trailing('Test', 'ArbitraryName')\n'ArbitraryName'\n\"\"\"\n", "func_signal": "def remove_trailing(needle, haystack):\n", "code": "if haystack[-len(needle):] == needle:\n    return haystack[:-len(needle)]\nreturn haystack", "path": "Contents\\Libraries\\Shared\\spec.py", "repo_name": "mikew/ss-plex.bundle", "stars": 137, "license": "None", "language": "python", "size": 4922}
{"docstring": "\"\"\"ensure tests are capable of being run, then\nrun nose.main with a reconstructed argument list\"\"\"\n", "func_signal": "def run(self):\n", "code": "if getattr(self.distribution, 'use_2to3', False):\n    # If we run 2to3 we can not do this inplace:\n\n    # Ensure metadata is up-to-date\n    self.reinitialize_command('build_py', inplace=0)\n    self.run_command('build_py')\n    bpy_cmd = self.get_finalized_command(\"build_py\")\n    build_path = bpy_cmd.build_lib\n\n    # Build extensions\n    self.reinitialize_command('egg_info', egg_base=build_path)\n    self.run_command('egg_info')\n\n    self.reinitialize_command('build_ext', inplace=0)\n    self.run_command('build_ext')\nelse:\n    self.run_command('egg_info')\n\n    # Build extensions in-place\n    self.reinitialize_command('build_ext', inplace=1)\n    self.run_command('build_ext')\n\nif self.distribution.install_requires:\n    self.distribution.fetch_build_eggs(\n        self.distribution.install_requires)\nif self.distribution.tests_require:\n    self.distribution.fetch_build_eggs(\n        self.distribution.tests_require)\n\nei_cmd = self.get_finalized_command(\"egg_info\")\nargv = ['nosetests', ei_cmd.egg_base] \nfor (option_name, cmd_name) in self.option_to_cmds.items():\n    if option_name in option_blacklist:\n        continue\n    value = getattr(self, option_name)\n    if value is not None:\n        argv.extend(\n            self.cfgToArg(option_name.replace('_', '-'), value))\nTestProgram(argv=argv, config=self.__config)", "path": "Contents\\Libraries\\Shared\\nose\\commands.py", "repo_name": "mikew/ss-plex.bundle", "stars": 137, "license": "None", "language": "python", "size": 4922}
{"docstring": "\"\"\"convert a optparse option list into a distutils option tuple list\"\"\"\n", "func_signal": "def get_user_options(parser):\n", "code": "opt_list = []\nfor opt in parser.option_list:\n    if opt._long_opts[0][2:] in option_blacklist: \n        continue\n    long_name = opt._long_opts[0][2:]\n    if opt.action not in ('store_true', 'store_false'):\n        long_name = long_name + \"=\"\n    short_name = None\n    if opt._short_opts:\n        short_name =  opt._short_opts[0][1:]\n    opt_list.append((long_name, short_name, opt.help or \"\"))\nreturn opt_list", "path": "Contents\\Libraries\\Shared\\nose\\commands.py", "repo_name": "mikew/ss-plex.bundle", "stars": 137, "license": "None", "language": "python", "size": 4922}
{"docstring": "\"\"\"Covert name from CamelCase to \"Normal case\".\n\n>>> camel2word('CamelCase')\n'Camel case'\n>>> camel2word('CaseWithSpec')\n'Case with spec'\n\"\"\"\n", "func_signal": "def camel2word(string):\n", "code": "def wordize(match):\n    return ' ' + match.group(1).lower()\n\nreturn string[0] + re.sub(r'([A-Z])', wordize, string[1:])", "path": "Contents\\Libraries\\Shared\\spec.py", "repo_name": "mikew/ss-plex.bundle", "stars": 137, "license": "None", "language": "python", "size": 4922}
{"docstring": "\"\"\"\n>>> complete_english('dont do this')\n\"don't do this\"\n>>> complete_english('doesnt is matched as well')\n\"doesn't is matched as well\"\n\"\"\"\n", "func_signal": "def complete_english(string):\n", "code": "for x,y in [(\"dont\"   , \"don't\"),\n            (\"doesnt\" , \"doesn't\"),\n            (\"wont\"   , \"won't\"),\n            (\"wasnt\"  , \"wasn't\")]:\n    string = string.replace(x, y)\nreturn string", "path": "Contents\\Libraries\\Shared\\spec.py", "repo_name": "mikew/ss-plex.bundle", "stars": 137, "license": "None", "language": "python", "size": 4922}
{"docstring": "\"\"\"The complex case where there are tests that don't all share\nthe same context. Groups tests into suites with common ancestors,\naccording to the following (essentially tail-recursive) procedure:\n\nStarting with the context of the first test, if it is not\nNone, look for tests in the remaining tests that share that\nancestor. If any are found, group into a suite with that\nancestor as the context, and replace the current suite with\nthat suite. Continue this process for each ancestor of the\nfirst test, until all ancestors have been processed. At this\npoint if any tests remain, recurse with those tests as the\ninput, returning a list of the common suite (which may be the\nsuite or test we started with, if no common tests were found)\nplus the results of recursion.\n\"\"\"\n", "func_signal": "def mixedSuites(self, tests):\n", "code": "if not tests:\n    return []\nhead = tests.pop(0)\nif not tests:\n    return [head] # short circuit when none are left to combine\nsuite = head # the common ancestry suite, so far\ntail = tests[:]\ncontext = getattr(head, 'context', None)\nif context is not None:\n    ancestors = [context] + [a for a in self.ancestry(context)]\n    for ancestor in ancestors:\n        common = [suite] # tests with ancestor in common, so far\n        remain = [] # tests that remain to be processed\n        for test in tail:\n            found_common = False\n            test_ctx = getattr(test, 'context', None)\n            if test_ctx is None:\n                remain.append(test)\n                continue\n            if test_ctx is ancestor:\n                common.append(test)\n                continue\n            for test_ancestor in self.ancestry(test_ctx):\n                if test_ancestor is ancestor:\n                    common.append(test)\n                    found_common = True\n                    break\n            if not found_common:\n                remain.append(test)\n        if common:\n            suite = self.makeSuite(common, ancestor)\n        tail = self.mixedSuites(remain)\nreturn [suite] + tail", "path": "Contents\\Libraries\\Shared\\nose\\suite.py", "repo_name": "mikew/ss-plex.bundle", "stars": 137, "license": "None", "language": "python", "size": 4922}
{"docstring": "# There are three obvious options here:\n# 1. truncate\n# 2. read to end\n# 3. close socket, pickle state including read position, then open\n#    again on unpickle and use Range header\n# XXXX um, 4. refuse to pickle unless .close()d.  This is better,\n#  actually (\"errors should never pass silently\").  Pickling doesn't\n#  work anyway ATM, because of http://python.org/sf/1144636 so fix\n#  this later\n\n# 2 breaks pickle protocol, because one expects the original object\n# to be left unscathed by pickling.  3 is too complicated and\n# surprising (and too much work ;-) to happen in a sane __getstate__.\n# So we do 1.\n\n", "func_signal": "def __getstate__(self):\n", "code": "state = self.__dict__.copy()\nnew_wrapped = eofresponse(\n    self._url, self._headers, self.code, self.msg)\nstate[\"wrapped\"] = new_wrapped\nreturn state", "path": "Contents\\Libraries\\Shared\\ss\\mechanize\\_response.py", "repo_name": "mikew/ss-plex.bundle", "stars": 137, "license": "None", "language": "python", "size": 4922}
{"docstring": "\"\"\"Return a copy of response that supports Browser response interface.\n\nBrowser response interface is that of \"seekable responses\"\n(response_seek_wrapper), plus the requirement that responses must be\nuseable after .close() (closeable_response).\n\nAccepts responses from both mechanize and urllib2 handlers.\n\nCopes with both ordinary response instances and HTTPError instances (which\ncan't be simply wrapped due to the requirement of preserving the exception\nbase class).\n\"\"\"\n", "func_signal": "def upgrade_response(response):\n", "code": "wrapper_class = get_seek_wrapper_class(response)\nif hasattr(response, \"closeable_response\"):\n    if not hasattr(response, \"seek\"):\n        response = wrapper_class(response)\n    assert hasattr(response, \"get_data\")\n    return copy.copy(response)\n\n# a urllib2 handler constructed the response, i.e. the response is an\n# urllib.addinfourl or a urllib2.HTTPError, instead of a\n# _Util.closeable_response as returned by e.g. mechanize.HTTPHandler\ntry:\n    code = response.code\nexcept AttributeError:\n    code = None\ntry:\n    msg = response.msg\nexcept AttributeError:\n    msg = None\n\n# may have already-.read() data from .seek() cache\ndata = None\nget_data = getattr(response, \"get_data\", None)\nif get_data:\n    data = get_data()\n\nresponse = closeable_response(\n    response.fp, response.info(), response.geturl(), code, msg)\nresponse = wrapper_class(response)\nif data:\n    response.set_data(data)\nreturn response", "path": "Contents\\Libraries\\Shared\\ss\\mechanize\\_response.py", "repo_name": "mikew/ss-plex.bundle", "stars": 137, "license": "None", "language": "python", "size": 4922}
{"docstring": "\"\"\"Remove leading needle string (if exists).\n\n>>> remove_leading('Test', 'TestThisAndThat')\n'ThisAndThat'\n>>> remove_leading('Test', 'ArbitraryName')\n'ArbitraryName'\n\"\"\"\n", "func_signal": "def remove_leading(needle, haystack):\n", "code": "if haystack[:len(needle)] == needle:\n    return haystack[len(needle):]\nreturn haystack", "path": "Contents\\Libraries\\Shared\\spec.py", "repo_name": "mikew/ss-plex.bundle", "stars": 137, "license": "None", "language": "python", "size": 4922}
{"docstring": "\"\"\"\nheaders: sequence of (name, value) pairs\n\"\"\"\n", "func_signal": "def make_headers(headers):\n", "code": "hdr_text = []\nfor name_value in headers:\n    hdr_text.append(\"%s: %s\" % name_value)\nreturn mimetools.Message(StringIO(\"\\n\".join(hdr_text)))", "path": "Contents\\Libraries\\Shared\\ss\\mechanize\\_response.py", "repo_name": "mikew/ss-plex.bundle", "stars": 137, "license": "None", "language": "python", "size": 4922}
{"docstring": "\"\"\"\nRemoves `amount` columns from the pixel array starting at column `offset`.\n\"\"\"\n", "func_signal": "def remove_columns(self, offset, amount):\n", "code": "if not amount:\n    return\nstart = offset * self.pixelsize\nend = start + (amount * self.pixelsize)\n# reversed is used because otherwise line_start would be all messed up.\nfor i in reversed(range(self.height)):\n    line_start = i * self.line_length\n    del self.data[line_start+start:line_start+end]\nself.width -= amount\nself._precalculate()", "path": "pymaging\\pixelarray.py", "repo_name": "ojii/pymaging", "stars": 246, "license": "other", "language": "python", "size": 124}
{"docstring": "\"\"\"\nSets the pixel (a tuple of length `self.pixelsize`) to the pixel array at x/y\n\"\"\"\n", "func_signal": "def set(self, x, y, pixel):\n", "code": "start = self._translate(x, y)\nself.data[start] = pixel[0]\nself.data[start + 1] = pixel[1]\nself.data[start + 2] = pixel[2]\nself.data[start + 3] = pixel[3]", "path": "pymaging\\pixelarray.py", "repo_name": "ojii/pymaging", "stars": 246, "license": "other", "language": "python", "size": 124}
{"docstring": "\"\"\"\nFlip the lines from top to bottom into a new copy of this pixel array\n\"\"\"\n", "func_signal": "def copy_flipped_top_bottom(self):\n", "code": "newarr = array.array('B', [0]) *  self.size\nfor i in range(self.height):\n    dst_start = i * self.line_length\n    dst_end = dst_start + self.line_length\n    src_start = ((self.height - i) * self.line_length) - self.line_length\n    src_end = src_start + self.line_length\n    newarr[dst_start:dst_end] = self.data[src_start:src_end]\nreturn get_pixel_array(newarr, self.width, self.height, self.pixelsize)", "path": "pymaging\\pixelarray.py", "repo_name": "ojii/pymaging", "stars": 246, "license": "other", "language": "python", "size": 124}
{"docstring": "\"\"\"\nReturns the pixel (a tuple of length `self.pixelsize`) from the pixel array at x/y\n\"\"\"\n", "func_signal": "def get(self, x, y):\n", "code": "start = self._translate(x, y)\nreturn [self.data[start], self.data[start + 1], self.data[start + 2]]", "path": "pymaging\\pixelarray.py", "repo_name": "ojii/pymaging", "stars": 246, "license": "other", "language": "python", "size": 124}
{"docstring": "\"\"\"\nSets the pixel (a tuple of length `self.pixelsize`) to the pixel array at x/y\n\"\"\"\n", "func_signal": "def set(self, x, y, pixel):\n", "code": "start = self._translate(x, y)\nself.data[start] = pixel[0]\nself.data[start + 1] = pixel[1]\nself.data[start + 2] = pixel[2]", "path": "pymaging\\pixelarray.py", "repo_name": "ojii/pymaging", "stars": 246, "license": "other", "language": "python", "size": 124}
{"docstring": "\"\"\"\nReturns the pixel (a tuple of length `self.pixelsize`) from the pixel array at x/y\n\"\"\"\n", "func_signal": "def get(self, x, y):\n", "code": "start = self._translate(x, y)\nreturn [self.data[start], self.data[start + 1], self.data[start + 2], self.data[start + 3]]", "path": "pymaging\\pixelarray.py", "repo_name": "ojii/pymaging", "stars": 246, "license": "other", "language": "python", "size": 124}
{"docstring": "\"\"\"\nFlip the lines from left to right into a new copy of this pixel array\n\"\"\"\n", "func_signal": "def copy_flipped_left_right(self):\n", "code": "new_pixel_array = get_pixel_array(array.array('B', [0]) *  self.size, \n                                  self.width, self.height, self.pixelsize)\nfor y in range(self.height):\n    for dst_x in range(self.width):\n        src_x = self.width - dst_x - 1\n        new_pixel_array.set(dst_x, y, self.get(src_x, y))\nreturn new_pixel_array", "path": "pymaging\\pixelarray.py", "repo_name": "ojii/pymaging", "stars": 246, "license": "other", "language": "python", "size": 124}
{"docstring": "\"\"\"\nSets the pixel (a tuple of length `self.pixelsize`) to the pixel array at x/y\n\"\"\"\n", "func_signal": "def set(self, x, y, pixel):\n", "code": "start = self._translate(x, y)\nself.data[start] = pixel[0]", "path": "pymaging\\pixelarray.py", "repo_name": "ojii/pymaging", "stars": 246, "license": "other", "language": "python", "size": 124}
{"docstring": "\"\"\"\nAdds `amount` columns to the pixel array starting at `offset` and fills them with `fill`.\n\"\"\"\n", "func_signal": "def add_columns(self, offset, amount, fill=0):\n", "code": "if not amount:\n    return\nstart = offset * self.pixelsize\nfor i in reversed(range(self.height)):\n    line_start = (i * self.line_length) + start\n    self.data[line_start:line_start] = array.array(self.data.typecode, [fill] * self.pixelsize * amount)\nself.width += amount\nself._precalculate()", "path": "pymaging\\pixelarray.py", "repo_name": "ojii/pymaging", "stars": 246, "license": "other", "language": "python", "size": 124}
{"docstring": "\"\"\"\nSets the pixel (a tuple of length `self.pixelsize`) to the pixel array at x/y\n\"\"\"\n", "func_signal": "def set(self, x, y, pixel):\n", "code": "start = self._translate(x, y)\nfor i in range(self.pixelsize):\n    self.data[start + i] = pixel[i]", "path": "pymaging\\pixelarray.py", "repo_name": "ojii/pymaging", "stars": 246, "license": "other", "language": "python", "size": 124}
{"docstring": "\"\"\"\nReturns the pixel (a tuple of length `self.pixelsize`) from the pixel array at x/y\n\"\"\"\n", "func_signal": "def get(self, x, y):\n", "code": "start = self._translate(x, y)\nreturn [self.data[start]]", "path": "pymaging\\pixelarray.py", "repo_name": "ojii/pymaging", "stars": 246, "license": "other", "language": "python", "size": 124}
{"docstring": "# identity inverse\n", "func_signal": "def test_inverse(self):\n", "code": "self.assertEqual(AffineTransform().inverse(), AffineTransform())\n\nself.assertEqual(\n    AffineTransform((2, 0, 0, 0, 2, 0, 0, 0, 2)).inverse(),\n    AffineTransform((0.5, 0, 0, 0, 0.5, 0, 0, 0, 0.5))\n)\n\n# inverted A*0 = A/0, which is an error\nself.assertRaises(ValueError, (AffineTransform() * 0).inverse)", "path": "pymaging\\tests\\test_affine.py", "repo_name": "ojii/pymaging", "stars": 246, "license": "other", "language": "python", "size": 124}
{"docstring": "\"\"\"\nPrecalculate some values, this must be called whenever self.width, self.height or self.pixelsize is changed.\n\"\"\"\n", "func_signal": "def _precalculate(self):\n", "code": "self.line_length = self.width * self.pixelsize\nself.size = self.line_length * self.height", "path": "pymaging\\pixelarray.py", "repo_name": "ojii/pymaging", "stars": 246, "license": "other", "language": "python", "size": 124}
{"docstring": "\"\"\"\nUse Xiaolin Wu's line algorithm: http://en.wikipedia.org/wiki/Xiaolin_Wu%27s_line_algorithm\n\nfunction plot(x, y, c) is\n    plot the pixel at (x, y) with brightness c (where 0 \u2264 c \u2264 1)\n\nfunction ipart(x) is\n    return integer part of x\n\nfunction round(x) is\n    return ipart(x + 0.5)\n\nfunction fpart(x) is\n    return fractional part of x\n\nfunction rfpart(x) is\n    return 1 - fpart(x)\n\nfunction drawLine(x1,y1,x2,y2) is\n    dx = x2 - x1\n    dy = y2 - y1\n    if abs(dx) < abs(dy) then                 \n      swap x1, y1\n      swap x2, y2\n      swap dx, dy\n    end if\n    if x2 < x1\n      swap x1, x2\n      swap y1, y2\n    end if\n    gradient = dy / dx\n    \n    // handle first endpoint\n    xend = round(x1)\n    yend = y1 + gradient * (xend - x1)\n    xgap = rfpart(x1 + 0.5)\n    xpxl1 = xend  // this will be used in the main loop\n    ypxl1 = ipart(yend)\n    plot(xpxl1, ypxl1, rfpart(yend) * xgap)\n    plot(xpxl1, ypxl1 + 1, fpart(yend) * xgap)\n    intery = yend + gradient // first y-intersection for the main loop\n    \n    // handle second endpoint\n    xend = round (x2)\n    yend = y2 + gradient * (xend - x2)\n    xgap = fpart(x2 + 0.5)\n    xpxl2 = xend  // this will be used in the main loop\n    ypxl2 = ipart (yend)\n    plot (xpxl2, ypxl2, rfpart (yend) * xgap)\n    plot (xpxl2, ypxl2 + 1, fpart (yend) * xgap)\n    \n    // main loop\n    for x from xpxl1 + 1 to xpxl2 - 1 do\n        plot (x, ipart (intery), rfpart (intery))\n        plot (x, ipart (intery) + 1, fpart (intery))\n        intery = intery + gradient\nend function\n\"\"\"\n", "func_signal": "def iter_pixels(self, color):\n", "code": "def _plot(x, y, c):\n    \"\"\"\n    plot the pixel at (x, y) with brightness c (where 0 \u2264 c \u2264 1)\n    \"\"\"\n    return int(x), int(y), color.get_for_brightness(c)\ndx = self.end_x - self.start_x\ndy = self.end_y - self.start_y\nx1, x2, y1, y2 = self.start_x, self.end_x, self.start_y, self.end_y\nif abs(dx) > abs(dy):\n    x1, y1 = y1, x1\n    x2, y2 = y2, x2\n    dx, dy = dy, dx\nif x2 < x1:\n    x1, x2 = x2, x1\n    y1, y2 = y2, y1\n\ngradient = fdiv(dy, dx)\n\nxend = round(x1)\nyend = y1 + gradient * (xend - x1)\nxgap = _rfpart(x1 + 0.5)\nxpxl1 = xend \nypxl1 = _ipart(yend)\nyield _plot(xpxl1, ypxl1, _rfpart(yend) * xgap)\nyield _plot(xpxl1, ypxl1 + 1, _fpart(yend) * xgap)\n\nintery = yend + gradient\n\nxend = _round(x2)\nyend = y2 + gradient *  (xend - x2)\nxgap = _fpart(x2 + 0.5)\nxpxl2 = xend\nypxl2 = _ipart(yend)\nyield _plot(xpxl2, ypxl2, _rfpart(yend) * xgap)\nyield _plot(xpxl2, ypxl2 + 1, _fpart(yend) * xgap)\n\nfor x in range (xpxl1 + 1, xpxl2 - 1):\n    yield _plot(x, _ipart(intery), _rfpart(intery))\n    yield _plot(x, _ipart(intery) + 1, _fpart(intery))\n    intery += gradient", "path": "pymaging\\shapes.py", "repo_name": "ojii/pymaging", "stars": 246, "license": "other", "language": "python", "size": 124}
{"docstring": "\"\"\"\nAdds `amount` lines to the pixel array starting at `offset` and fills them with `fill`.\n\"\"\"\n", "func_signal": "def add_lines(self, offset, amount, fill=0):\n", "code": "if not amount:\n    return\n    # special case for adding to the end of the array:\nif offset == self.height:\n    self.data.extend(array.array(self.data.typecode, [fill] *  self.line_length * amount))\nelse:\n    start = offset * self.line_length\n    self.data[start:start] = array.array(self.data.typecode, [fill] *  self.line_length * amount)\nself.height += amount\nself._precalculate()", "path": "pymaging\\pixelarray.py", "repo_name": "ojii/pymaging", "stars": 246, "license": "other", "language": "python", "size": 124}
{"docstring": "\"\"\"\nReturns the pixel (a tuple of length `self.pixelsize`) from the pixel array at x/y\n\"\"\"\n", "func_signal": "def get(self, x, y):\n", "code": "start = self._translate(x, y)\nreturn [self.data[start], self.data[start + 1]]", "path": "pymaging\\pixelarray.py", "repo_name": "ojii/pymaging", "stars": 246, "license": "other", "language": "python", "size": 124}
{"docstring": "\"\"\"\nSets the pixel (a tuple of length `self.pixelsize`) to the pixel array at x/y\n\"\"\"\n", "func_signal": "def set(self, x, y, pixel):\n", "code": "start = self._translate(x, y)\nself.data[start] = pixel[0]\nself.data[start + 1] = pixel[1]", "path": "pymaging\\pixelarray.py", "repo_name": "ojii/pymaging", "stars": 246, "license": "other", "language": "python", "size": 124}
{"docstring": "\"\"\"\nTakes an affine transform and a four-tuple of (x0, y0, x1, y1)\ncoordinates.\nTransforms each corner of the given box, and returns the\n(width, height) of the transformed box.\n\"\"\"\n", "func_signal": "def get_transformed_dimensions(transform, box):\n", "code": "(x0, y0, x1, y1) = box\nxs = []\nys = []\nfor corner in (\n    (x0, y0),\n    (x1, y0),\n    (x0, y1),\n    (x1, y1),\n):\n    x, y = transform * corner\n    xs.append(x)\n    ys.append(y)\n\nreturn int(ceil(max(xs))) - int(min(xs)), int(ceil(max(ys))) - int(min(ys))", "path": "pymaging\\helpers.py", "repo_name": "ojii/pymaging", "stars": 246, "license": "other", "language": "python", "size": 124}
{"docstring": "\"\"\"\nReturns the pixel (a tuple of length `self.pixelsize`) from the pixel array at x/y\n\"\"\"\n", "func_signal": "def get(self, x, y):\n", "code": "start = self._translate(x, y)\nreturn [self.data[start+i] for i in range(self.pixelsize)]", "path": "pymaging\\pixelarray.py", "repo_name": "ojii/pymaging", "stars": 246, "license": "other", "language": "python", "size": 124}
{"docstring": "\"\"\"\nRemoves `amount` lines from the pixel array starting at line `offset`.\n\"\"\"\n", "func_signal": "def remove_lines(self, offset, amount):\n", "code": "if not amount:\n    return\nstart = self.line_length * offset\nend = start + (amount * self.line_length)\nself.height -= amount\ndel self.data[start:end]\nself._precalculate()", "path": "pymaging\\pixelarray.py", "repo_name": "ojii/pymaging", "stars": 246, "license": "other", "language": "python", "size": 124}
{"docstring": "\"\"\"Juno can use status() to set status code\"\"\"\n", "func_signal": "def test500StatusCode(self):\n", "code": "status, _, _ = client.request('/3/')\nself.assertEqual(status, '500 Internal Server Error')", "path": "tests\\test.py", "repo_name": "breily/juno", "stars": 250, "license": "mit", "language": "python", "size": 362}
{"docstring": "# DB library imports\n", "func_signal": "def setup_database(self):\n", "code": "from sqlalchemy import (create_engine, Table, MetaData, Column, Integer, \n                        String, Unicode, Text, UnicodeText, Date, Numeric, \n                        Time, Float, DateTime, Interval, Binary, Boolean, \n                        PickleType)\nfrom sqlalchemy.orm import sessionmaker, mapper\n# Create global name mappings for model()\nglobal column_mapping\ncolumn_mapping = {'string': String,       'str': String,\n                 'integer': Integer,      'int': Integer, \n                 'unicode': Unicode,     'text': Text,\n             'unicodetext': UnicodeText, 'date': Date,\n                 'numeric': Numeric,     'time': Time,\n                   'float': Float,   'datetime': DateTime,\n                'interval': Interval,  'binary': Binary,\n                 'boolean': Boolean,     'bool': Boolean,\n              'pickletype': PickleType,\n}\n# Add a few SQLAlchemy types to globals() so we can use them in models\nglobals().update({'Column': Column, 'Table': Table, 'Integer': Integer,\n                  'MetaData': MetaData, 'mapper': mapper})\n# Ensures correct slash number for sqlite\nif self.config['db_type'] == 'sqlite':\n    self.config['db_location'] = '/' + self.config['db_location']\neng_name = self.config['db_type'] + '://' + self.config['db_location']\nself.config['db_engine'] = create_engine(eng_name)\nself.config['db_session'] = sessionmaker(bind=self.config['db_engine'])()", "path": "juno.py", "repo_name": "breily/juno", "stars": 250, "license": "mit", "language": "python", "size": 362}
{"docstring": "\"\"\"Append the content of a file to the response. Guesses file type if not\nincluded.  Returns 1 if requested file can't be accessed (often means doesn't \nexist).  Returns 2 if requested file is a directory.  Returns 7 on success. \"\"\"\n", "func_signal": "def yield_file(filename, type=None):\n", "code": "if not os.access(filename, os.F_OK): return 1\nif os.path.isdir(filename): return 2\nif type is None:\n    guess = mimetypes.guess_type(filename)[0]\n    if guess is None: content_type('text/plain')\n    else: content_type(guess)\nelse: content_type(type)\nappend(open(filename, 'r').read())\nreturn 7", "path": "juno.py", "repo_name": "breily/juno", "stars": 250, "license": "mit", "language": "python", "size": 362}
{"docstring": "\"\"\"Renders template object with an optional dictionary of values.\nDefined for Jinja2 and Mako - override it if you use another\nlibrary.  Takes a template object as the first parameter, with an\noptional **kwargs parameter.  Needs to return a string.\"\"\"\n", "func_signal": "def _render_template_handler(template_obj, **kwargs):\n", "code": "if config('template_lib') == 'mako': return template_obj.render(**kwargs)\nif config('template_lib') == 'jinja2':\n    # Jinja needs its output encoded here\n    return template_obj.render(**kwargs).encode(config('charset'))", "path": "juno.py", "repo_name": "breily/juno", "stars": 250, "license": "mit", "language": "python", "size": 362}
{"docstring": "\"\"\"Sets the response to a 500, sets the body to 500_template.\"\"\"\n", "func_signal": "def servererror(error='Unspecified error', file=None):\n", "code": "if config('log'): print >>sys.stderr, 'Error: (%s, %s, %s)' % sys.exc_info()\nstatus(500)\nif file is None: file = config('500_template')\n# Resets the response, in case the error occurred as we added data to it\n_response.config['body'] = ''\nreturn template(file, error=error)", "path": "juno.py", "repo_name": "breily/juno", "stars": 250, "license": "mit", "language": "python", "size": 362}
{"docstring": "\"\"\"Returns a 3-tuple (status_string, headers, body).\"\"\"\n", "func_signal": "def render(self):\n", "code": "status_string = '%s %s' %(self.config['status'],\n                          self.status_codes[self.config['status']])\nheaders = [(k, str(v)) for k, v in self.config['headers'].items()]\nbody = '%s' %self.config['body']\nreturn (status_string, headers, body)", "path": "juno.py", "repo_name": "breily/juno", "stars": 250, "license": "mit", "language": "python", "size": 362}
{"docstring": "\"\"\"Runs the Juno hub, in the set mode (default now is dev). \"\"\"\n# If no mode is specified, use the one from the config\n", "func_signal": "def run(self, mode=None):\n", "code": "if mode is None: mode = config('mode')\n# Otherwise store the specified mode\nelse: config('mode', mode)\n\nif   mode == 'dev':  run_dev('',  config('dev_port'),  self.request)\nelif mode == 'scgi': run_scgi('', config('scgi_port'), self.request)\nelif mode == 'fcgi': run_fcgi('', config('fcgi_port'), self.request)\nelif mode == 'wsgi': return run_wsgi(self.request)\nelif mode == 'appengine': run_appengine(self.request)\nelse:\n    print >>sys.stderr, 'Error: unrecognized mode'\n    print >>sys.stderr, '       exiting juno...'", "path": "juno.py", "repo_name": "breily/juno", "stars": 250, "license": "mit", "language": "python", "size": 362}
{"docstring": "# This may be temporary - I was getting weird errors where\n# the `environ` was None.  Seems to have stopped, but I don't\n# know why.  This message was to clarify what happened.\n", "func_signal": "def application(environ, start_response):\n", "code": "if environ is None:\n    print >>sys.stderr, 'Error: environ is None for some reason.'\n    print >>sys.stderr, 'Error: environ=%s' %environ\n    sys.exit()\n# Ensure some variable exist (WSGI doesn't guarantee them)\nif 'PATH_INFO' not in environ.keys() or not environ['PATH_INFO']: \n    environ['PATH_INFO'] = '/'\nif 'QUERY_STRING' not in environ.keys():\n    environ['QUERY_STRING'] = ''\nif 'CONTENT_LENGTH' not in environ.keys() or not environ['CONTENT_LENGTH']:\n    environ['CONTENT_LENGTH'] = '0'\n# Standardize some header names\nenviron['DOCUMENT_URI'] = environ['PATH_INFO']\nif environ['QUERY_STRING']:\n    environ['REQUEST_URI'] = environ['PATH_INFO']+'?'+environ['QUERY_STRING']\nelse:\n    environ['REQUEST_URI'] = environ['DOCUMENT_URI']\n# Parse query string arguments\nenviron['QUERY_DICT'] = cgi.parse_qs(environ['QUERY_STRING'],\n                                     keep_blank_values=1)\nif environ['REQUEST_METHOD'] in ('POST', 'PUT'):\n    fs = cgi.FieldStorage(fp=environ['wsgi.input'],\n                          environ=environ,\n                          keep_blank_values=True)\n    \n    post_dict = {}\n    if fs.list:\n        for field in fs.list:\n            if field.filename:\n                value = field\n            else:\n                value = field.value\n            \n            # Each element of post_dict will be a list, even if it contains only\n            # one item. This is in line with QUERY_DICT which also works like this.\n            if not field.name in post_dict:\n                post_dict[field.name] = [value]\n            else:\n                post_dict[field.name].append(value)\n    \n    environ['POST_DICT'] = post_dict\nelse: environ['POST_DICT'] = {}\n# Done parsing inputs, now ready to send to Juno\nstatus_str, headers, body = process_func(environ['PATH_INFO'],\n                                         environ['REQUEST_METHOD'],\n                                         **environ)\nstart_response(status_str, headers)\nreturn [body]", "path": "juno.py", "repo_name": "breily/juno", "stars": 250, "license": "mit", "language": "python", "size": 362}
{"docstring": "\"\"\"Set a response header. \"\"\"\n", "func_signal": "def header(key, value):\n", "code": "global _response\nreturn _response.header(key, value)", "path": "juno.py", "repo_name": "breily/juno", "stars": 250, "license": "mit", "language": "python", "size": 362}
{"docstring": "# No args: return the whole dictionary\n", "func_signal": "def input(self, arg=None):\n", "code": "if arg is None: return self.raw['input']\n# Otherwise try to return the value for that key\nif self.raw['input'].has_key(arg): \n    return self.raw['input'][arg]\nreturn None", "path": "juno.py", "repo_name": "breily/juno", "stars": 250, "license": "mit", "language": "python", "size": 362}
{"docstring": "\"\"\"Juno can use status() to set status code\"\"\"\n", "func_signal": "def test404StatusCode(self):\n", "code": "status, _, _ = client.request('/2/')\nself.assertEqual(status, '404 Not Found')", "path": "tests\\test.py", "repo_name": "breily/juno", "stars": 250, "license": "mit", "language": "python", "size": 362}
{"docstring": "\"\"\"Set up Juno with an optional configuration.\"\"\"\n# Only set it up if we haven't already. (Avoids multiple Juno objects)\n", "func_signal": "def init(configuration=None):\n", "code": "global _hub # Use global keyword here to avoid SyntaxWarning\nif _hub is None:\n    _hub = Juno(configuration)\nreturn _hub", "path": "juno.py", "repo_name": "breily/juno", "stars": 250, "license": "mit", "language": "python", "size": 362}
{"docstring": "\"\"\"Takes an optional configuration dictionary. \"\"\"\n", "func_signal": "def __init__(self, configuration=None):\n", "code": "global _hub\nif _hub is not None:\n    print >>sys.stderr, 'Warning: there is already a Juno object created;'\n    print >>sys.stderr, '         you might get some weird behavior.'\nelse: _hub = self\nself.routes = []\n# Find the directory of the user's app, so we can setup static/template_roots\nself.find_user_path()\n# Set options and merge in user-set options\nself.config = {\n        # General settings / meta information\n        'log':    True,\n        'routes': self.routes,\n        'self':   self,\n        # Types and encodings\n        'charset':      'utf-8',\n        'content_type': 'text/html',\n        # Server options\n        'mode':     'dev',\n        'scgi_port': 8000,\n        'fcgi_port': 8000,\n        'dev_port':  8000,\n        # Static file handling\n        'use_static':     True,\n        'static_url':     '/static/*:file/',\n        'static_root':    os.path.join(self.app_path, 'static/'),\n        'static_handler': static_serve,\n        # Template options\n        'use_templates':           True,\n        'template_lib':            'jinja2',\n        'get_template_handler':    _get_template_handler,\n        'render_template_handler': _render_template_handler,\n        'auto_reload_templates':   True,\n        'translations':            [], \n        'template_kwargs':         {},\n        'template_root':           os.path.join(self.app_path, 'templates/'),\n        '404_template':            '404.html',\n        '500_template':            '500.html',\n        # Database options\n        'use_db':      True,\n        'db_type':     'sqlite',\n        'db_location': ':memory:',\n        'db_models':   {},\n        # Session options\n        'use_sessions': False,\n        'session_lib':  'beaker',\n        # Debugger\n        'use_debugger': False,\n        'raise_view_exceptions': False,\n        # Custom middleware\n        'middleware': []\n}\nif configuration is not None: self.config.update(configuration)\n# Set up the static file handler\nif self.config['use_static']: \n    self.setup_static()\n# Set up templating\nif self.config['use_templates']: \n    self.setup_templates()\n# Set up the database \nif self.config['use_db']:\n    self.setup_database()", "path": "juno.py", "repo_name": "breily/juno", "stars": 250, "license": "mit", "language": "python", "size": 362}
{"docstring": "\"\"\"Automatically renders a template for a given path.  Currently can't\nuse any arguments in the url.\"\"\"\n", "func_signal": "def autotemplate(urls, template_path):\n", "code": "if type(urls) not in (list, tuple): urls = urls[urls]\nfor url in urls:\n    @route(url)\n    def temp(web): template(template_path)", "path": "juno.py", "repo_name": "breily/juno", "stars": 250, "license": "mit", "language": "python", "size": 362}
{"docstring": "\"\"\"Sets the response to a 404, sets the body to 404_template.\"\"\"\n", "func_signal": "def notfound(error='Unspecified error', file=None):\n", "code": "if config('log'): print >>sys.stderr, 'Not Found: %s' % error\nstatus(404)\nif file is None: file = config('404_template')\nreturn template(file, error=error)", "path": "juno.py", "repo_name": "breily/juno", "stars": 250, "license": "mit", "language": "python", "size": 362}
{"docstring": "# Try returning values from self.raw\n", "func_signal": "def __getattr__(self, attr):\n", "code": "if attr in self.keys(): return self.raw[attr]\nif attr == 'session' and config('log'):\n    print >>sys.stderr, \"Error: To use sessions, enable 'use_sessions'\"\n    print >>sys.stderr, \"       when calling juno.init()\"\n    print >>sys.stderr, \"\"\nreturn None", "path": "juno.py", "repo_name": "breily/juno", "stars": 250, "license": "mit", "language": "python", "size": 362}
{"docstring": "\"\"\"The default static file serve function. Maps arguments to dir structure.\"\"\"\n", "func_signal": "def static_serve(web, file):\n", "code": "file = os.path.join(config('static_root'), file)\nrealfile = os.path.realpath(file)\nif not realfile.startswith(os.path.realpath(config('static_root'))):\n    notfound(\"that file could not be found/served\")\nelif yield_file(file) != 7:\n    notfound(\"that file could not be found/served\")", "path": "juno.py", "repo_name": "breily/juno", "stars": 250, "license": "mit", "language": "python", "size": 362}
{"docstring": "\"\"\"Add text to response body. \"\"\"\n", "func_signal": "def append(body):\n", "code": "global _response\nreturn _response.append(body)", "path": "juno.py", "repo_name": "breily/juno", "stars": 250, "license": "mit", "language": "python", "size": 362}
{"docstring": "\"\"\"Append a rendered template to response.  If template_dict is provided,\nit is passed to the render function.  If not, kwargs is.\"\"\"\n# Retreive a template object.\n", "func_signal": "def template(template_path, template_dict=None, **kwargs):\n", "code": "t = get_template(template_path)\n# Render it without arguments.\nif not kwargs and not template_dict: \n    return append(render_template(t))\n# Render the template with a provided template dictionary\nif template_dict: \n    return append(render_template(t, **template_dict))\n# Render the template with **kwargs\nreturn append(render_template(t, **kwargs))", "path": "juno.py", "repo_name": "breily/juno", "stars": 250, "license": "mit", "language": "python", "size": 362}
{"docstring": "\"\"\"Matches a request uri to this url object. \"\"\"\n", "func_signal": "def match(self, request, method):\n", "code": "match_obj = self.url.match(request)\nif match_obj is None: return False\n# Make sure the request method matches\nif self.method != '*' and self.method != method: return False\n# Store the parts that matched\nself.params.update(match_obj.groupdict())\nreturn True", "path": "juno.py", "repo_name": "breily/juno", "stars": 250, "license": "mit", "language": "python", "size": 362}
{"docstring": "\"\"\"Gets a new access token using a refresh token.\n\nArgs:\n  refresh_token: The refresh token to use.\n\nReturns:\n  A valid access token or None if query was unsuccessful.\n\"\"\"\n", "func_signal": "def FetchAccessToken(refresh_token):\n", "code": "auth_params = {\n    'refresh_token': refresh_token,\n    'client_id': OAUTH_CLIENT_ID,\n    'client_secret': OAUTH_CLIENT_SECRET,\n    'grant_type': 'refresh_token'\n}\n\nreturn FetchCredentials(auth_params)", "path": "src\\controllers\\util\\analytics_auth_helper.py", "repo_name": "googleanalytics/google-analytics-super-proxy", "stars": 215, "license": "other", "language": "python", "size": 272}
{"docstring": "\"\"\"Retrieves credentials from OAuth 2.0 service.\n\nArgs:\n  code: The authorization code from the auth server\n\nReturns:\n  A dict indicating whether auth flow was a success and the auth\n  server response.\n\"\"\"\n", "func_signal": "def GetOAuthCredentials(code):\n", "code": "auth_params = {\n    'code': code,\n    'client_id': OAUTH_CLIENT_ID,\n    'client_secret': OAUTH_CLIENT_SECRET,\n    'redirect_uri': OAUTH_REDIRECT_URI,\n    'grant_type': 'authorization_code'\n}\n\nreturn FetchCredentials(auth_params)", "path": "src\\controllers\\util\\analytics_auth_helper.py", "repo_name": "googleanalytics/google-analytics-super-proxy", "stars": 215, "license": "other", "language": "python", "size": 272}
{"docstring": "\"\"\"Requires that the user owns the entity being accessed or is an admin.\n\n  If the request isn't made by the owner of the API Query or an admin then\n  they will be redirected to the owner index page.\n\nArgs:\n  original_request: The restricted request being made.\n\nReturns:\n  The wrapped request.\n\"\"\"\n", "func_signal": "def OwnerRestricted(original_request):\n", "code": "def Wrapper(self, *args, **kwargs):\n  query_id = self.request.get('query_id')\n  owner_has_access = UserOwnsApiQuery(query_id)\n  if owner_has_access or users.is_current_user_admin():\n    return original_request(self, *args, **kwargs)\n  else:\n    self.redirect(co.LINKS['owner_index'])\n    return\n\nreturn Wrapper", "path": "src\\controllers\\util\\access_control.py", "repo_name": "googleanalytics/google-analytics-super-proxy", "stars": 215, "license": "other", "language": "python", "size": 272}
{"docstring": "\"\"\"Returns how long since the API Query was updated.\n\nArgs:\n  api_query: The API Query from which to retrieve the modified timedelta.\n  from_time: A DateTime object representing the start time to calculate the\n             timedelta from.\n\nReturns:\n  A string that describes how long since the API Query has been updated in\n  the form of \"HH hours, MM minutes, ss seconds ago\" or None if the API Query\n  has never been updated.\n\"\"\"\n", "func_signal": "def GetModifiedTimedelta(api_query, from_time=None):\n", "code": "if not from_time:\n  from_time = datetime.utcnow()\n\napi_query_response = api_query.api_query_responses.get()\nif api_query_response:\n  time_delta = from_time - api_query_response.modified\n  return FormatTimedelta(time_delta)\nreturn None", "path": "src\\controllers\\util\\models_helper.py", "repo_name": "googleanalytics/google-analytics-super-proxy", "stars": 215, "license": "other", "language": "python", "size": 272}
{"docstring": "\"\"\"Revokes an OAuth token.\n\nArgs:\n  token: A refresh or access token\n\nReturns:\n  True if token successfully revoked, False otherwise\n\"\"\"\n", "func_signal": "def RevokeOAuthCredentials(token):\n", "code": "if token:\n  revoke_url = OAUTH_REVOKE_ENDPOINT + token\n\n  try:\n    response = urlfetch.fetch(url=revoke_url)\n\n    if response.status_code == 200:\n      return True\n  except urlfetch.Error:\n    return False\nreturn False", "path": "src\\controllers\\util\\analytics_auth_helper.py", "repo_name": "googleanalytics/google-analytics-super-proxy", "stars": 215, "license": "other", "language": "python", "size": 272}
{"docstring": "\"\"\"Returns how long since the API Query response was last requested.\n\nArgs:\n  api_query: The API Query from which to retrieve the last request timedelta.\n  from_time: A DateTime object representing the start time to calculate the\n             timedelta from.\n\nReturns:\n  A string that describes how long since the API Query response was last\n  requested in the form of \"HH hours, MM minutes, ss seconds ago\" or None\n  if the API Query response has never been requested.\n\"\"\"\n", "func_signal": "def GetLastRequestTimedelta(api_query, from_time=None):\n", "code": "if not from_time:\n  from_time = datetime.utcnow()\n\nif api_query.last_request:\n  time_delta = from_time - api_query.last_request\n  return FormatTimedelta(time_delta)\nreturn None", "path": "src\\controllers\\util\\models_helper.py", "repo_name": "googleanalytics/google-analytics-super-proxy", "stars": 215, "license": "other", "language": "python", "size": 272}
{"docstring": "\"\"\"Decorator to retrieve an access token and append it to an API Query URL.\n\nArgs:\n  fn: The original function being wrapped.\n\nReturns:\n  An API Query entity with an access token appended to request URL.\n\"\"\"\n", "func_signal": "def AuthorizeApiQuery(fn):\n", "code": "def Wrapper(api_query):\n  \"\"\"Returns the original function with an authorized API Query.\"\"\"\n  access_token = GetAccessTokenForApiQuery(api_query)\n  query = api_query.request\n  if access_token:\n    query = ('%s&access_token=%s&gasp=1' % (\n        urllib.unquote(api_query.request), access_token))\n\n  # Leave original API Query untouched by returning a copy with\n  # a valid access_token appended to the API request URL.\n  new_api_query = copy.copy(api_query)\n  new_api_query.request = query\n\n  return fn(new_api_query)\nreturn Wrapper", "path": "src\\controllers\\util\\analytics_auth_helper.py", "repo_name": "googleanalytics/google-analytics-super-proxy", "stars": 215, "license": "other", "language": "python", "size": 272}
{"docstring": "\"\"\"Returns the request count for an API Query.\n\nArgs:\n  query_id: The ID of the Query from which to retrieve the request count.\n\nReturns:\n  An integer representing the number of times the API Query has been\n  requested using the external public endpoint.\n\"\"\"\n", "func_signal": "def GetApiQueryRequestCount(query_id):\n", "code": "request_counter_key = co.REQUEST_COUNTER_KEY_TEMPLATE.format(query_id)\nrequest_count = memcache.get(request_counter_key)\nif not request_count:\n  request_count = request_counter_shard.GetCount(request_counter_key)\nreturn request_count", "path": "src\\controllers\\util\\models_helper.py", "repo_name": "googleanalytics/google-analytics-super-proxy", "stars": 215, "license": "other", "language": "python", "size": 272}
{"docstring": "\"\"\"Generate a signed token unique to this user.\n\nReturns:\n  An XSRF token unique to the user.\n\"\"\"\n", "func_signal": "def GetXsrfToken():\n", "code": "token = None\nuser = users.get_current_user()\nif user:\n  mac = hmac.new(config.XSRF_KEY, user.user_id(), hashlib.sha256)\n  token = mac.hexdigest()\nreturn token", "path": "src\\controllers\\util\\access_control.py", "repo_name": "googleanalytics/google-analytics-super-proxy", "stars": 215, "license": "other", "language": "python", "size": 272}
{"docstring": "\"\"\"Returns a boolean to indicate if the API Query reached the error limit.\"\"\"\n", "func_signal": "def IsErrorLimitReached(api_query):\n", "code": "return (api_query.api_query_errors.count(\n    limit=co.QUERY_ERROR_LIMIT) == co.QUERY_ERROR_LIMIT)", "path": "src\\controllers\\util\\models_helper.py", "repo_name": "googleanalytics/google-analytics-super-proxy", "stars": 215, "license": "other", "language": "python", "size": 272}
{"docstring": "\"\"\"Handles OAuth Responses from Google Accounts.\n\nThe function can handle code, revoke, and error requests.\n\nArgs:\n  request: The request object for the incoming request from Google Accounts.\n\nReturns:\n  A dict containing messages that can be used to display to a user to indicate\n  the outcome of the auth task.\n\"\"\"\n\n# Request to exchange auth code for refresh/access token\n", "func_signal": "def OAuthHandler(request):\n", "code": "if request.get('code'):\n  code_response = SaveAuthTokensForUser(request.get('code'))\n  if code_response.get('success'):\n    auth_values = {\n        'status': 'success',\n        'message': AUTH_MESSAGES['codeSuccess'],\n    }\n  else:\n    auth_values = {\n        'status': 'error',\n        'message': AUTH_MESSAGES['codeError'],\n        'message_detail': code_response.get('message')\n    }\n\n# Request to revoke an issued refresh/access token\nelif request.get('revoke'):\n  revoked = RevokeAuthTokensForUser()\n  if revoked:\n    auth_values = {\n        'status': 'success',\n        'message': AUTH_MESSAGES['revokeSuccess']\n    }\n  else:\n    auth_values = {\n        'status': 'error',\n        'message': AUTH_MESSAGES['revokeError']\n    }\n\n# Error returned from OAuth service\nelif request.get('error'):\n  auth_values = {\n      'status': 'error',\n      'message': AUTH_MESSAGES['badRequest'],\n      'message_detail': request.get('error')\n  }\nelse:\n  auth_values = {\n      'status': 'error',\n      'message': 'There was an error connecting to Google Analytics.',\n      'message_detail': AUTH_MESSAGES['badRequest']\n  }\n\nreturn auth_values", "path": "src\\controllers\\util\\analytics_auth_helper.py", "repo_name": "googleanalytics/google-analytics-super-proxy", "stars": 215, "license": "other", "language": "python", "size": 272}
{"docstring": "\"\"\"Revokes a user's auth tokens and removes them from the datastore.\n\nReturns:\n  A boolean indicating whether the revoke was successfully.\n\"\"\"\n", "func_signal": "def RevokeAuthTokensForUser():\n", "code": "user = users_helper.GetGaSuperProxyUser(users.get_current_user().user_id())\n\nif user and user.ga_refresh_token:\n  RevokeOAuthCredentials(user.ga_refresh_token)\n  users_helper.SetUserCredentials(users.get_current_user().user_id())\n  return True\nreturn False", "path": "src\\controllers\\util\\analytics_auth_helper.py", "repo_name": "googleanalytics/google-analytics-super-proxy", "stars": 215, "license": "other", "language": "python", "size": 272}
{"docstring": "\"\"\"Exchanges an auth code for tokens and saves it to the datastore for a user.\n\nArgs:\n  code: The auth code from Google Accounts to exchange for tokens.\n\nReturns:\n  A dict indicating whether the user's auth settings were successfully\n  saved to the datastore and any messages returned from the service.\n\"\"\"\n", "func_signal": "def SaveAuthTokensForUser(code):\n", "code": "response = {\n    'success': False\n}\nauth_response = GetOAuthCredentials(code)\nresponse_content = auth_response.get('content')\n\nif (auth_response.get('status_code') == 200\n    and response_content\n    and response_content.get('refresh_token')):\n\n  refresh_token = response_content.get('refresh_token')\n  access_token = response_content.get('access_token')\n\n  users_helper.SetUserCredentials(\n      users.get_current_user().user_id(),\n      refresh_token, access_token)\n  response['success'] = True\nelse:\n  response['message'] = response_content\n\nreturn response", "path": "src\\controllers\\util\\analytics_auth_helper.py", "repo_name": "googleanalytics/google-analytics-super-proxy", "stars": 215, "license": "other", "language": "python", "size": 272}
{"docstring": "\"\"\"Returns the timestamp of the last request.\n\nArgs:\n  query_id: The ID of the Query for which to retrieve the last request time.\n\nReturns:\n  A DateTime object specifying the time when the API Query was last\n  requested using the external public endpoint.\n\"\"\"\n", "func_signal": "def GetApiQueryLastRequest(query_id):\n", "code": "if query_id:\n  request_timestamp_key = co.REQUEST_TIMESTAMP_KEY_TEMPLATE.format(query_id)\n  request_timestamp = memcache.get(request_timestamp_key)\n  if not request_timestamp:\n    request_timestamp = request_timestamp_shard.GetTimestamp(\n        request_timestamp_key)\n  return request_timestamp\nreturn None", "path": "src\\controllers\\util\\models_helper.py", "repo_name": "googleanalytics/google-analytics-super-proxy", "stars": 215, "license": "other", "language": "python", "size": 272}
{"docstring": "\"\"\"Determines whether the API Query is considered abandoned.\n\nWhen an API Query response has not been requested for a period\nof time (configurable) then it is considered abandoned. Abandoned\nqueries will not be scheduled for a refresh. This saves quota and resources.\n\nIf any of the following 3 cases are true, then a query is considered to be\nabandoned:\n1) The timestamp of the last public request is greater than some multiple\n   of the query's refresh interval. The multiple is a configurable value,\n   defined as the constant ABANDONED_INTERVAL_MULTIPLE. For example, if the\n   refresh interval of a query is 30 seconds, and the\n   ABANDONED_INTERVAL_MULTIPLE is 2, and the last public request for the query\n   is greater than 60 seconds ago, then the query is considered abandoned.\n\nIf the query has never been publicly requested and there is no timestamp then\nthe modified date of the query is used. The query is considered abandoned\nwhen:\n2) The timestamp of the last modified date of the query is greater than some\n   multiple of the query's refresh interval. The multiple is a configurable\n   value, defined as the constant ABANDONED_INTERVAL_MULTIPLE.\n\nIf the query has never been publicly requested and there is no modified\ntimestamp then the query is considered abandoned when:\n3) A stored API Query Response exists for the query.\n\nArgs:\n  api_query: THe API Query to check if abandonded.\n\nReturns:\n  A boolean indicating if the query is considered abandoned.\n\"\"\"\n# Case 1: Use the last requested timestamp.\n", "func_signal": "def IsApiQueryAbandoned(api_query):\n", "code": "if api_query.last_request:\n  last_request_age = int(\n      (datetime.utcnow() - api_query.last_request).total_seconds())\n  max_timedelta = co.ABANDONED_INTERVAL_MULTIPLE * api_query.refresh_interval\n  return last_request_age > max_timedelta\n\n# Case 2: Use the last modified timestamp.\nelif api_query.modified:\n  last_modified_age = int(\n      (datetime.utcnow() - api_query.modified).total_seconds())\n  max_timedelta = co.ABANDONED_INTERVAL_MULTIPLE * api_query.refresh_interval\n  return last_modified_age > max_timedelta\n\n# Case 3: Check if there is a saved API Query Response.\nelse:\n  api_query_response = api_query.api_query_responses.get()\n  if api_query_response:\n    return True\n\nreturn False", "path": "src\\controllers\\util\\models_helper.py", "repo_name": "googleanalytics/google-analytics-super-proxy", "stars": 215, "license": "other", "language": "python", "size": 272}
{"docstring": "\"\"\"Requires that this is a valid user of the app.\n\n  If the request isn't made by an active Google Analytics superProxy user then\n  they will be redirected to the public index page.\n\nArgs:\n  original_request: The restricted request being made.\n\nReturns:\n  The wrapped request.\n\"\"\"\n", "func_signal": "def ActiveGaSuperProxyUser(original_request):\n", "code": "def Wrapper(self, *args, **kwargs):\n  user = users_helper.GetGaSuperProxyUser(users.get_current_user().user_id())\n  if user or users.is_current_user_admin():\n    return original_request(self, *args, **kwargs)\n  else:\n    self.redirect(co.LINKS['public_index'])\n    return\n\nreturn Wrapper", "path": "src\\controllers\\util\\access_control.py", "repo_name": "googleanalytics/google-analytics-super-proxy", "stars": 215, "license": "other", "language": "python", "size": 272}
{"docstring": "\"\"\"Attempts to retrieve a valid access token for an API Query.\n\nFirst retrieves the stored access token for the owner of the API Query, if\navailable. Checks if token has expired and refreshes token if required (and\nsaves it) before returning the token.\n\nArgs:\n  api_query: The API Query for which to retrieve an access token.\n\nReturns:\n  A valid access token if available or None.\n\"\"\"\n", "func_signal": "def GetAccessTokenForApiQuery(api_query):\n", "code": "user_settings = users_helper.GetGaSuperProxyUser(api_query.user.key().name())\nif user_settings.ga_refresh_token and user_settings.ga_access_token:\n\n  access_token = user_settings.ga_access_token\n\n  # Check for expired access_token\n  if datetime.utcnow() > user_settings.ga_token_expiry:\n    response = FetchAccessToken(user_settings.ga_refresh_token)\n    if (response.get('status_code') == 200 and response.get('content')\n        and response.get('content').get('access_token')):\n      access_token = response.get('content').get('access_token')\n      expires_in = int(response.get('content').get('expires_in', 0))\n\n      users_helper.SetUserCredentials(api_query.user.key().name(),\n                                      user_settings.ga_refresh_token,\n                                      access_token,\n                                      expires_in)\n  return access_token\nreturn None", "path": "src\\controllers\\util\\analytics_auth_helper.py", "repo_name": "googleanalytics/google-analytics-super-proxy", "stars": 215, "license": "other", "language": "python", "size": 272}
{"docstring": "\"\"\"Require a valid XSRF token in the environment, or error.\n\n  If the request doesn't include a valid XSRF token then they will be\n  redirected to the public index page.\n\nArgs:\n  original_handler: The handler that requires XSRF validation.\n\nReturns:\n  The wrapped handler.\n\"\"\"\n", "func_signal": "def ValidXsrfTokenRequired(original_handler):\n", "code": "def Handler(self, *args, **kwargs):\n  if self.request.get('xsrf_token') == GetXsrfToken():\n    return original_handler(self, *args, **kwargs)\n  else:\n    self.redirect(co.LINKS['public_index'])\n    return\n\nHandler.__name__ = original_handler.__name__\nreturn Handler", "path": "src\\controllers\\util\\access_control.py", "repo_name": "googleanalytics/google-analytics-super-proxy", "stars": 215, "license": "other", "language": "python", "size": 272}
{"docstring": "\"\"\"Formats a time delta into a sentence.\n\nArgs:\n  time_delta: A Timedelta object to format.\n\nReturns:\n  A string containing a nicely formatted time delta in the form of\n  \"HH hours, MM minutes, ss seconds ago\".\n\"\"\"\n", "func_signal": "def FormatTimedelta(time_delta):\n", "code": "seconds = int(time_delta.total_seconds())\ndays, time_left = divmod(seconds, 86400)  # 86400: seconds in a day = 24*60*60\nhours, time_left = divmod(time_left, 3600)  # 3600: seconds in an hour = 60*60\nminutes, seconds = divmod(time_left, 60)  # 60: seconds in a minute\n\npretty_label = '%ss ago' % seconds\nif days > 0:\n  pretty_label = '%sd, %sh, %sm ago' % (days, hours, minutes)\nelif hours > 0:\n  pretty_label = '%sh, %sm ago' % (hours, minutes)\nelif minutes > 0:\n  pretty_label = '%sm, %ss ago' % (minutes, seconds)\nreturn pretty_label", "path": "src\\controllers\\util\\models_helper.py", "repo_name": "googleanalytics/google-analytics-super-proxy", "stars": 215, "license": "other", "language": "python", "size": 272}
{"docstring": "\"\"\"Check if the currently logged in user owns the API Query.\n\nArgs:\n  query_id: The id of the API query.\n\nReturns:\n  A boolean to indicate whether the logged in user owns the API Query.\n\"\"\"\n", "func_signal": "def UserOwnsApiQuery(query_id):\n", "code": "user = users.get_current_user()\napi_query = query_helper.GetApiQuery(query_id)\n\nif user and user.user_id() and api_query:\n  return user.user_id() == api_query.user.key().name()\nreturn False", "path": "src\\controllers\\util\\access_control.py", "repo_name": "googleanalytics/google-analytics-super-proxy", "stars": 215, "license": "other", "language": "python", "size": 272}
{"docstring": "# TODO: pagination\n", "func_signal": "def get(self):\n", "code": "db = self.settings['db']\ndraft_docs = yield db.posts.find(\n    {'status': 'draft'},\n    {'original': False, 'body': False},\n).sort([('_id', -1)]).to_list(100)\n\ndrafts = [Post(**draft_doc) for draft_doc in draft_docs]\nself.render('admin-templates/drafts.html', drafts=drafts)", "path": "motor_blog\\web\\admin.py", "repo_name": "ajdavis/motor-blog", "stars": 150, "license": "None", "language": "python", "size": 3302}
{"docstring": "\"\"\"Render widgets. Returns (HTML, last_modified).\n\nTake a RequestHandler, a MotorDatabase, and HTML text, return HTML with\nwidgets rendered, and the maximum modified date.\n\"\"\"\n", "func_signal": "def process_widgets(handler, db, html):\n", "code": "rv = cStringIO.StringIO()\n\npos = 0\nmodified = None\nfor match in widget_pat.finditer(html):\n    parts = [p.strip() for p in match.group(1).split() if p.strip()]\n    if parts:\n        widget_name, options = parts[0], parts[1:]\n        if widget_name in all_widgets:\n            f = all_widgets[widget_name]\n            widget_html, m = yield f(handler, db, *options)\n\n            # Track latest last-modified value from all widgets.\n            if not modified or m > modified:\n                modified = m\n\n            # Text before the match.\n            rv.write(html[pos:match.start()])\n\n            # Replace widget with rendered version.\n            rv.write(widget_html)\n            pos = match.end()\n\nrv.write(html[pos:])\nraise gen.Return((rv.getvalue(), modified))", "path": "motor_blog\\web\\widgets.py", "repo_name": "ajdavis/motor-blog", "stars": 150, "license": "None", "language": "python", "size": 3302}
{"docstring": "# Create the static home page.\n", "func_signal": "def test_static_home_page(self):\n", "code": "self.new_page(title='test-home', body='test home body')\n\n# Some posts.\nself.new_post(title='foo')\nself.new_post(title='bar')\n\n# Fetch the home page.\npost_page = self.fetch('/test-blog/')\nself.assertEqual(200, post_page.code)\nsoup = BeautifulSoup(post_page.body)\nposts = soup.find_all('div', attrs={'class', 'post-content'})\nself.assertEqual(1, len(posts))\ntext = posts[0].find('p').text\nself.assertEqual('test home body', text)", "path": "test\\test_homepage.py", "repo_name": "ajdavis/motor-blog", "stars": 150, "license": "None", "language": "python", "size": 3302}
{"docstring": "# If-Modified-Since is rounded down to the second.\n", "func_signal": "def test_if_modified_since_microseconds(self):\n", "code": "post_id = self.new_post(title='title')\ndoc = self.sync_db.posts.find_one({'_id': ObjectId(post_id)})\ndt = doc['mod']\nslug = slugify.slugify('title')\nurl = self.reverse_url('post', slug)\nresponse = self.fetch(url, if_modified_since=dt.replace(microsecond=0))\nself.assertEqual(304, response.code)", "path": "test\\test_posts.py", "repo_name": "ajdavis/motor-blog", "stars": 150, "license": "None", "language": "python", "size": 3302}
{"docstring": "\"\"\"Call a method in our XML-RPC API, return the response as a dict.\n\nThe Motor-Blog server emulates WordPress's XML-RPC API, which is the\nMetaWeblog API plus WordPress extensions. Motor-Blog emulates only\nenough of the API to support MarsEdit.\n\"\"\"\n", "func_signal": "def fetch_rpc(self, method_name, args):\n", "code": "body = xmlrpclib.dumps(args, method_name, allow_none=1)\napi_url = self.reverse_url('api')\nresponse = self.fetch(api_url, method='POST', body=body)\nself.assertEqual(200, response.code)\n(data,), _ = xmlrpclib.loads(response.body)\nreturn data", "path": "test\\__init__.py", "repo_name": "ajdavis/motor-blog", "stars": 150, "license": "None", "language": "python", "size": 3302}
{"docstring": "\"\"\"Show summaries of N most recent posts.\"\"\"\n", "func_signal": "def recent_posts(handler, db, n, tag=None):\n", "code": "limit = int(n)\nquery = {'status': 'publish', 'type': 'post'}\nif tag:\n    query['tags'] = tag\n\ncursor = db.posts.find(query, {'original': False})\ndocs = yield cursor.sort([('pub_date', -1)]).limit(limit).to_list(limit)\nposts = [Post(**doc) for doc in docs]\nmodified = max(p.last_modified for p in posts) if posts else None\n\nrv = cStringIO.StringIO()\nrv.write('<ul class=\"post-list\">')\nfor post in posts:\n    rv.write(handler.render_string('post-summary.jade', post=post))\n\nrv.write('</ul>')\nraise gen.Return((rv.getvalue(), modified))", "path": "motor_blog\\web\\widgets.py", "repo_name": "ajdavis/motor-blog", "stars": 150, "license": "None", "language": "python", "size": 3302}
{"docstring": "# Could cache this as we do on the web side, but not worth the risk\n", "func_signal": "def wp_getCategories(self, blogid, user, password):\n", "code": "db = self.settings['db']\ncategories = yield db.categories.find().sort(\n    [('name', 1)]).to_list(100)\n\nself.result([\n    Category(**c).to_wordpress(self.application) for c in categories])", "path": "motor_blog\\api\\categories.py", "repo_name": "ajdavis/motor-blog", "stars": 150, "license": "None", "language": "python", "size": 3302}
{"docstring": "\"\"\"Include base_url in pattern\"\"\"\n", "func_signal": "def __init__(self, pattern, *args, **kwargs):\n", "code": "super(U, self).__init__(\n    '/' + base_url.strip('/') + '/' + pattern.lstrip('/'),\n    *args, **kwargs\n)", "path": "motor_blog\\web\\__init__.py", "repo_name": "ajdavis/motor-blog", "stars": 150, "license": "None", "language": "python", "size": 3302}
{"docstring": "\"\"\"Verify an XML-RPC method is authorized.\n\nCheck the 'user' and 'password' arguments against those in conf file\nor command line options. If unmatched, return an XML-RPC Fault, else\ncall wrapped method.\n\"\"\"\n", "func_signal": "def auth(fn):\n", "code": "argspec = inspect.getargspec(fn)\nassert 'user' in argspec.args\nassert 'password' in argspec.args\n\n@superwraps(fn)\n@gen.coroutine\ndef _auth(*args, **kwargs):\n    self = args[0]\n    user = args[argspec.args.index('user')]\n    password = args[argspec.args.index('password')]\n    if user != opts.user or password != opts.password:\n        self.result(xmlrpclib.Fault(403, 'Bad login/pass combination.'))\n    else:\n        raise gen.Return((yield fn(*args, **kwargs)))\n\nreturn _auth", "path": "motor_blog\\api\\__init__.py", "repo_name": "ajdavis/motor-blog", "stars": 150, "license": "None", "language": "python", "size": 3302}
{"docstring": "# Mocked to always return the user as admin\n", "func_signal": "def blogger_getUsersBlogs(self, blog_id, user, password):\n", "code": "doc = {\n    'blogid': \"1\",\n    'blogName': opts.blog_name,\n    'url': \"\",\n    'xmlrpc': \"\",\n    'isAdmin': True\n}\n\nself.result([doc])", "path": "motor_blog\\api\\users.py", "repo_name": "ajdavis/motor-blog", "stars": 150, "license": "None", "language": "python", "size": 3302}
{"docstring": "\"\"\"Make a dictionary by parsing something like:\n       ::: lang=\"python\" highlight=\"8,12,13,20\"\n\"\"\"\n", "func_signal": "def parse_code_header(header):\n", "code": "match = re.match(r':::\\s+(.+)$', header)\nif not match:\n    return {}\n\n# Like lang=\"Python Traceback\" highlight=\"8,12,13,20\"\noptions_list = match.group(1)\nmatches = re.findall(r'\\w+=\"[^\"]+\"', options_list)\nif not match:\n    raise Exception(\"Can't parse options: %s\" % options_list)\n\n# Python 2.7 dict literal.\nreturn {\n    key.strip(): value.strip('\"\\' ')\n    for key, value in [part.split('=') for part in matches]}", "path": "motor_blog\\tools\\migrate_cMarkdown_to_markdown.py", "repo_name": "ajdavis/motor-blog", "stars": 150, "license": "None", "language": "python", "size": 3302}
{"docstring": "# img_text will be like '<img width=\"600\" height=\"600\" src=\"...\">'.\n", "func_signal": "def sub(img_pat_match):\n", "code": "img_text = img_pat_match.group(0)\n\n# Remove width and height.\nimg_text = width_pat.sub(lambda match: '', img_text)\nimg_text = height_pat.sub(lambda match: '', img_text)\nreturn img_text", "path": "motor_blog\\tools\\remove_image_sizes.py", "repo_name": "ajdavis/motor-blog", "stars": 150, "license": "None", "language": "python", "size": 3302}
{"docstring": "# At the point Tornado\n", "func_signal": "def get_app(self):\n", "code": "patcher = mock.patch.object(\n    tornado_options.mockable(), 'port', self.get_http_port())\n\npatcher.start()\nself.patchers.append(patcher)\ndb = self.get_db()\n\nreturn application.get_application('..', db, tornado.options.options)", "path": "test\\__init__.py", "repo_name": "ajdavis/motor-blog", "stars": 150, "license": "None", "language": "python", "size": 3302}
{"docstring": "# Configure the \"home_page\" option.\n", "func_signal": "def get_app(self):\n", "code": "self.set_option('home_page', 'test-home')\nreturn super(StaticHomePageTest, self).get_app()", "path": "test\\test_homepage.py", "repo_name": "ajdavis/motor-blog", "stars": 150, "license": "None", "language": "python", "size": 3302}
{"docstring": "\"\"\"Replaces wrapper's arg spec with wrapped function's.\n\nFor when functools.wraps() just isn't enough.\n\"\"\"\n", "func_signal": "def superwraps(wrapped):\n", "code": "def wrap(wrapper):\n    argspec = inspect.getargspec(wrapped)\n    formatted_args = inspect.formatargspec(*argspec)\n    fndef = 'lambda %s: wrapper%s' % (\n        formatted_args.lstrip('(').rstrip(')'), formatted_args)\n\n    fake_fn = eval(fndef, {'wrapper': wrapper})\n    return functools.wraps(wrapped)(fake_fn)\n\nreturn wrap", "path": "motor_blog\\api\\__init__.py", "repo_name": "ajdavis/motor-blog", "stars": 150, "license": "None", "language": "python", "size": 3302}
{"docstring": "\"\"\"Get rid of final '?' -- Tornado's reverse_url() works poorly\n   with tornado.web.addslash\n\"\"\"\n", "func_signal": "def _find_groups(self):\n", "code": "path, group_count = super(U, self)._find_groups()\nif path.endswith('?'):\n    path = path[:-1]\nreturn path, group_count", "path": "motor_blog\\web\\__init__.py", "repo_name": "ajdavis/motor-blog", "stars": 150, "license": "None", "language": "python", "size": 3302}
{"docstring": "# Sometimes we receive only id from categories, e.g. from Windows\n# Live Writer, so we must query for the names.\n", "func_signal": "def mt_setPostCategories(self, postid, user, password, categories):\n", "code": "if categories and 'categoryName' not in categories[0]:\n    categories_ids = [\n        ObjectId(category.get('categoryId'))\n        for category in categories]\n\n    cursor = self.settings['db'].categories.find(\n        {\"_id\": {'$in': categories_ids}}, fields=[\"name\"])\n\n    categories = []\n    while (yield cursor.fetch_next):\n        category = cursor.next_object()\n        categories.append(dict(\n            categoryId=category['_id'],\n            categoryName=category['name']))\n\nembedded_cats = [\n    EmbeddedCategory.from_metaweblog(cat).to_python()\n    for cat in categories]\n\nresult = yield self.settings['db'].posts.update(\n    {'_id': ObjectId(postid)},\n    {'$set': {\n        'categories': embedded_cats,\n        'mod': datetime.datetime.utcnow(),\n    }})\n\nif result['n'] != 1:\n    self.result(xmlrpclib.Fault(404, 'Not found'))\nelse:\n    self.result('')\n    cache.event('post_changed')", "path": "motor_blog\\api\\categories.py", "repo_name": "ajdavis/motor-blog", "stars": 150, "license": "None", "language": "python", "size": 3302}
{"docstring": "\"\"\"Ensure a coroutine method returns None, not a Future.\n\ntornadorpc logs a warning if an async method returns non-None.\n\"\"\"\n", "func_signal": "def return_none(fn):\n", "code": "@superwraps(fn)\ndef _return_none(*args, **kwargs):\n    fn(*args, **kwargs)\n\nreturn _return_none", "path": "motor_blog\\api\\__init__.py", "repo_name": "ajdavis/motor-blog", "stars": 150, "license": "None", "language": "python", "size": 3302}
{"docstring": "\"\"\"Receive messages from browsers viewing draft pages.\n\nIn draft.jade we include the draft's timestamp at the moment it was\nrendered. When rendering completes, Javascript sends the timestamp\nback to see if the draft was modified while it was rendering; if so\nit reloads again.\n\nThis is surprisingly common. If a draft's categories change, MarsEdit\nmakes two API calls: first to edit the draft's body, then to set its\ncategories. The first call causes the draft to reload, and while it\nreloads the second API call completes. To ensure the browser has loaded\nthe latest version we must check at the end of each page-load whether\nwe're still up to date.\n\n# TODO: might reduce flickering by waiting a second to see if we'll\n    change again, before sending post_changed.\n\"\"\"\n", "func_signal": "def on_message(self, json_message):\n", "code": "try:\n    # The client tells us this page's last-modified date.\n    message = json.loads(json_message)\n    post_id = ObjectId(message['post_id'])\n    mod = message['mod']\n\n    db = self.session.handler.settings['db']\n    post_doc = yield db.posts.find_one({\n        'status': 'draft',\n        '_id': post_id})\n\n    post = Post(**post_doc)\n    if str(post.last_modified) != mod:\n        # Post changed since we served the draft page; make it reload.\n        self.send('post_changed')\n\nexcept Exception:\n    logging.exception('Processing message: %r' % json_message)", "path": "motor_blog\\web\\admin.py", "repo_name": "ajdavis/motor-blog", "stars": 150, "license": "None", "language": "python", "size": 3302}
{"docstring": "\"\"\"Generates an ASCII-only slug.\"\"\"\n", "func_signal": "def slugify(text, delim=u'-'):\n", "code": "result = []\nfor word in _punct_re.split(text.lower()):\n    result.extend(unidecode(word.decode(\"utf-8\")).split())\nreturn unicode(delim.join(result))", "path": "motor_blog\\text\\slugify.py", "repo_name": "ajdavis/motor-blog", "stars": 150, "license": "None", "language": "python", "size": 3302}
{"docstring": "\"\"\"\nReturns the URL that should be used to talk to the NoteStore for the\naccount represented by the provided authenticationToken.\nThis method isn't needed by most clients, who can retrieve the correct\nNoteStore URL from the AuthenticationResult returned from the authenticate\nor refreshAuthentication calls. This method is typically only needed\nto look up the correct URL for a long-lived session token (e.g. for an\nOAuth web service).\n\nParameters:\n - authenticationToken\n\"\"\"\n", "func_signal": "def getNoteStoreUrl(self, authenticationToken):\n", "code": "self.send_getNoteStoreUrl(authenticationToken)\nreturn self.recv_getNoteStoreUrl()", "path": "plugin\\py\\lib\\evernote\\edam\\userstore\\UserStore.py", "repo_name": "kakkyz81/evervim", "stars": 234, "license": "None", "language": "python", "size": 745}
{"docstring": "\"\"\"\nAsks the UserStore about the publicly available location information for\na particular username.\n\n@throws EDAMUserException <ul>\n  <li> DATA_REQUIRED \"username\" - username is empty\n</ul>\n\nParameters:\n - username\n\"\"\"\n", "func_signal": "def getPublicUserInfo(self, username):\n", "code": "self.send_getPublicUserInfo(username)\nreturn self.recv_getPublicUserInfo()", "path": "plugin\\py\\lib\\evernote\\edam\\userstore\\UserStore.py", "repo_name": "kakkyz81/evervim", "stars": 234, "license": "None", "language": "python", "size": 745}
{"docstring": "\"\"\"\nThis is used to take an existing authentication token (returned from\n'authenticate') and exchange it for a newer token which will not expire\nas soon.  This must be invoked before the previous token expires.\n\nThis function is only availabe to Evernote's internal applications.\n\n@param authenticationToken\n  The previous authentication token from the authenticate() result.\n\n@return\n  The result of the authentication, with the new token in\n  the result's \"authentication\" field.  The User field will\n  not be set in the reply.\n\nParameters:\n - authenticationToken\n\"\"\"\n", "func_signal": "def refreshAuthentication(self, authenticationToken):\n", "code": "self.send_refreshAuthentication(authenticationToken)\nreturn self.recv_refreshAuthentication()", "path": "plugin\\py\\lib\\evernote\\edam\\userstore\\UserStore.py", "repo_name": "kakkyz81/evervim", "stars": 234, "license": "None", "language": "python", "size": 745}
{"docstring": "\"\"\"\nThis provides bootstrap information to the client. Various bootstrap\nprofiles and settings may be used by the client to configure itself.\n\n@param locale\n  The client's current locale, expressed in language[_country]\n  format. E.g., \"en_US\". See ISO-639 and ISO-3166 for valid\n  language and country codes.\n\n@return\n  The bootstrap information suitable for this client.\n\nParameters:\n - locale\n\"\"\"\n", "func_signal": "def getBootstrapInfo(self, locale):\n", "code": "self.send_getBootstrapInfo(locale)\nreturn self.recv_getBootstrapInfo()", "path": "plugin\\py\\lib\\evernote\\edam\\userstore\\UserStore.py", "repo_name": "kakkyz81/evervim", "stars": 234, "license": "None", "language": "python", "size": 745}
{"docstring": "# self.__rbuf will already be empty here because fastbinary doesn't\n# ask for a refill until the previous buffer is empty.  Therefore,\n# we can start reading new frames immediately.\n", "func_signal": "def cstringio_refill(self, prefix, reqlen):\n", "code": "while len(prefix) < reqlen:\n  self.readFrame()\n  prefix += self.__rbuf.getvalue()\nself.__rbuf = StringIO(prefix)\nreturn self.__rbuf", "path": "plugin\\py\\lib\\thrift\\transport\\TTransport.py", "repo_name": "kakkyz81/evervim", "stars": 234, "license": "None", "language": "python", "size": 745}
{"docstring": "\"\"\"Does select on open connections.\"\"\"\n", "func_signal": "def _select(self):\n", "code": "readable = [self.socket.handle.fileno(), self._read.fileno()]\nwritable = []\nfor i, connection in self.clients.items():\n    if connection.is_readable():\n        readable.append(connection.fileno())\n    if connection.is_writeable():\n        writable.append(connection.fileno())\n    if connection.is_closed():\n        del self.clients[i]\nreturn select.select(readable, writable, readable)", "path": "plugin\\py\\lib\\thrift\\server\\TNonblockingServer.py", "repo_name": "kakkyz81/evervim", "stars": 234, "license": "None", "language": "python", "size": 745}
{"docstring": "\"\"\"\nThis is used to check a username and password in order to create an\nauthentication session that could be used for further actions.\n\nThis function is only availabe to Evernote's internal applications.\nThird party applications must authenticate using OAuth as\ndescribed at\n<a href=\"http://dev.evernote.com/documentation/cloud/\">dev.evernote.com</a>.\n\n@param username\n  The username (not numeric user ID) for the account to\n  authenticate against.  This function will also accept the user's\n  registered email address in this parameter.\n\n@param password\n  The plaintext password to check against the account.  Since\n  this is not protected by the EDAM protocol, this information must be\n  provided over a protected transport (e.g. SSL).\n\n@param consumerKey\n  A unique identifier for this client application, provided by Evernote\n  to developers who request an API key.  This must be provided to identify\n  the client.\n\n@param consumerSecret\n  If the client was given a \"consumer secret\" when the API key was issued,\n  it must be provided here to authenticate the application itself.\n\n@return\n  The result of the authentication.  If the authentication was successful,\n  the AuthenticationResult.user field will be set with the full information\n  about the User.\n\n@throws EDAMUserException <ul>\n  <li> DATA_REQUIRED \"username\" - username is empty\n  <li> DATA_REQUIRED \"password\" - password is empty\n  <li> DATA_REQUIRED \"consumerKey\" - consumerKey is empty\n  <li> INVALID_AUTH \"username\" - username not found\n  <li> INVALID_AUTH \"password\" - password did not match\n  <li> INVALID_AUTH \"consumerKey\" - consumerKey is not authorized\n  <li> INVALID_AUTH \"consumerSecret\" - consumerSecret is incorrect\n  <li> PERMISSION_DENIED \"User.active\" - user account is closed\n  <li> PERMISSION_DENIED \"User.tooManyFailuresTryAgainLater\" - user has\n    failed authentication too often\n</ul>\n\nParameters:\n - username\n - password\n - consumerKey\n - consumerSecret\n\"\"\"\n", "func_signal": "def authenticate(self, username, password, consumerKey, consumerSecret):\n", "code": "self.send_authenticate(username, password, consumerKey, consumerSecret)\nreturn self.recv_authenticate()", "path": "plugin\\py\\lib\\evernote\\edam\\userstore\\UserStore.py", "repo_name": "kakkyz81/evervim", "stars": 234, "license": "None", "language": "python", "size": 745}
{"docstring": "\"\"\"Serve forever.\"\"\"\n", "func_signal": "def serve(self):\n", "code": "self.prepare()\nwhile True:\n    self.handle()", "path": "plugin\\py\\lib\\thrift\\server\\TNonblockingServer.py", "repo_name": "kakkyz81/evervim", "stars": 234, "license": "None", "language": "python", "size": 745}
{"docstring": "\"\"\"\nReturns information regarding a user's Premium account corresponding to the\nprovided authentication token, or throws an exception if this token is not\nvalid.\n\nParameters:\n - authenticationToken\n\"\"\"\n", "func_signal": "def getPremiumInfo(self, authenticationToken):\n", "code": "self.send_getPremiumInfo(authenticationToken)\nreturn self.recv_getPremiumInfo()", "path": "plugin\\py\\lib\\evernote\\edam\\userstore\\UserStore.py", "repo_name": "kakkyz81/evervim", "stars": 234, "license": "None", "language": "python", "size": 745}
{"docstring": "\"\"\"Reads data from stream and switch state.\"\"\"\n", "func_signal": "def read(self):\n", "code": "assert self.status in (WAIT_LEN, WAIT_MESSAGE)\nif self.status == WAIT_LEN:\n    self._read_len()\n    # go back to the main loop here for simplicity instead of\n    # falling through, even though there is a good chance that\n    # the message is already available\nelif self.status == WAIT_MESSAGE:\n    read = self.socket.recv(self.len - len(self.message))\n    if len(read) == 0:\n        logging.error(\"can't read frame from socket (get %d of %d bytes)\" %\n            (len(self.message), self.len))\n        self.close()\n        return\n    self.message += read\n    if len(self.message) == self.len:\n        self.status = WAIT_PROCESS", "path": "plugin\\py\\lib\\thrift\\server\\TNonblockingServer.py", "repo_name": "kakkyz81/evervim", "stars": 234, "license": "None", "language": "python", "size": 745}
{"docstring": "\"\"\"Reads length of request.\n\nIt's really paranoic routine and it may be replaced by \nself.socket.recv(4).\"\"\"\n", "func_signal": "def _read_len(self):\n", "code": "read = self.socket.recv(4 - len(self.message))\nif len(read) == 0:\n    # if we read 0 bytes and self.message is empty, it means client close \n    # connection\n    if len(self.message) != 0:\n        logging.error(\"can't read frame size from socket\")\n    self.close()\n    return\nself.message += read\nif len(self.message) == 4:\n    self.len, = struct.unpack('!i', self.message)\n    if self.len < 0:\n        logging.error(\"negative frame size, it seems client\"\\\n            \" doesn't use FramedTransport\")\n        self.close()\n    elif self.len == 0:\n        logging.error(\"empty frame, it's really strange\")\n        self.close()\n    else:\n        self.message = ''\n        self.status = WAIT_MESSAGE", "path": "plugin\\py\\lib\\thrift\\server\\TNonblockingServer.py", "repo_name": "kakkyz81/evervim", "stars": 234, "license": "None", "language": "python", "size": 745}
{"docstring": "\"\"\" return note that set title, tags, content from buftext \"\"\"\n", "func_signal": "def buffer2note(self, note, buflines):\n", "code": "pref = EvervimPref.getInstance()\nif pref.usemarkdown == '0':\n    note.title = buflines[0]\n    note = self.api.editTag(note, buflines[1].replace('Tags:', ''))\n    note.content  = EvernoteAPI.NOTECONTENT_HEADER + \"\\n\".join(buflines[2:]) + EvernoteAPI.NOTECONTENT_FOOTER\nelse:\n    note.title = re.sub(r'^#', '',buflines[0]).strip()\n    note = self.api.editTag(note, buflines[1].replace('Tags:', ''))\n    parsedContent = markdownAndENML.parseMarkdown(\"\\n\".join(buflines[2:]))\n    note.content  = EvernoteAPI.NOTECONTENT_HEADER + parsedContent.encode('utf-8') + EvernoteAPI.NOTECONTENT_FOOTER\n\nreturn note", "path": "plugin\\py\\evervim_editor.py", "repo_name": "kakkyz81/evervim", "stars": 234, "license": "None", "language": "python", "size": 745}
{"docstring": "\"\"\"\nReturns the User corresponding to the provided authentication token,\nor throws an exception if this token is not valid.\nThe level of detail provided in the returned User structure depends on\nthe access level granted by the token, so a web service client may receive\nfewer fields than an integrated desktop client.\n\nParameters:\n - authenticationToken\n\"\"\"\n", "func_signal": "def getUser(self, authenticationToken):\n", "code": "self.send_getUser(authenticationToken)\nreturn self.recv_getUser()", "path": "plugin\\py\\lib\\evernote\\edam\\userstore\\UserStore.py", "repo_name": "kakkyz81/evervim", "stars": 234, "license": "None", "language": "python", "size": 745}
{"docstring": "\"\"\"Closes the server.\"\"\"\n", "func_signal": "def close(self):\n", "code": "for _ in xrange(self.threads):\n    self.tasks.put([None, None, None, None, None])\nself.socket.close()\nself.prepared = False", "path": "plugin\\py\\lib\\thrift\\server\\TNonblockingServer.py", "repo_name": "kakkyz81/evervim", "stars": 234, "license": "None", "language": "python", "size": 745}
{"docstring": "\"\"\"Writes data from socket and switch state.\"\"\"\n", "func_signal": "def write(self):\n", "code": "assert self.status == SEND_ANSWER\nsent = self.socket.send(self.message)\nif sent == len(self.message):\n    self.status = WAIT_LEN\n    self.message = ''\n    self.len = 0\nelse:\n    self.message = self.message[sent:]", "path": "plugin\\py\\lib\\thrift\\server\\TNonblockingServer.py", "repo_name": "kakkyz81/evervim", "stars": 234, "license": "None", "language": "python", "size": 745}
{"docstring": "\"\"\"Process queries from task queue, stop if processor is None.\"\"\"\n", "func_signal": "def run(self):\n", "code": "while True:\n    try:\n        processor, iprot, oprot, otrans, callback = self.queue.get()\n        if processor is None:\n            break\n        processor.process(iprot, oprot)\n        callback(True, otrans.getvalue())\n    except Exception:\n        logging.exception(\"Exception while processing request\")\n        callback(False, '')", "path": "plugin\\py\\lib\\thrift\\server\\TNonblockingServer.py", "repo_name": "kakkyz81/evervim", "stars": 234, "license": "None", "language": "python", "size": 745}
{"docstring": "\"\"\"Prepares server for serve requests.\"\"\"\n", "func_signal": "def prepare(self):\n", "code": "self.socket.listen()\nfor _ in xrange(self.threads):\n    thread = Worker(self.tasks)\n    thread.setDaemon(True)\n    thread.start()\nself.prepared = True", "path": "plugin\\py\\lib\\thrift\\server\\TNonblockingServer.py", "repo_name": "kakkyz81/evervim", "stars": 234, "license": "None", "language": "python", "size": 745}
{"docstring": "\"\"\"\nThis should be the first call made by a client to the EDAM service.  It\ntells the service what protocol version is used by the client.  The\nservice will then return true if the client is capable of talking to\nthe service, and false if the client's protocol version is incompatible\nwith the service, so the client must upgrade.  If a client receives a\nfalse value, it should report the incompatibility to the user and not\ncontinue with any more EDAM requests (UserStore or NoteStore).\n\n@param clientName\n  This string provides some information about the client for\n  tracking/logging on the service.  It should provide information about\n  the client's software and platform.  The structure should be:\n  application/version; platform/version; [ device/version ]\n  E.g.   \"Evernote Windows/3.0.1; Windows/XP SP3\" or\n  \"Evernote Clipper/1.0.1; JME/2.0; Motorola RAZR/2.0;\n\n@param edamVersionMajor\n  This should be the major protocol version that was compiled by the\n  client.  This should be the current value of the EDAM_VERSION_MAJOR\n  constant for the client.\n\n@param edamVersionMinor\n  This should be the major protocol version that was compiled by the\n  client.  This should be the current value of the EDAM_VERSION_MINOR\n  constant for the client.\n\nParameters:\n - clientName\n - edamVersionMajor\n - edamVersionMinor\n\"\"\"\n", "func_signal": "def checkVersion(self, clientName, edamVersionMajor, edamVersionMinor):\n", "code": "self.send_checkVersion(clientName, edamVersionMajor, edamVersionMinor)\nreturn self.recv_checkVersion()", "path": "plugin\\py\\lib\\evernote\\edam\\userstore\\UserStore.py", "repo_name": "kakkyz81/evervim", "stars": 234, "license": "None", "language": "python", "size": 745}
{"docstring": "\"\"\"Callback function for switching state and waking up main thread.\n\nThis function is the only function witch can be called asynchronous.\n\nThe ready can switch Connection to three states:\n    WAIT_LEN if request was oneway.\n    SEND_ANSWER if request was processed in normal way.\n    CLOSED if request throws unexpected exception.\n\nThe one wakes up main thread.\n\"\"\"\n", "func_signal": "def ready(self, all_ok, message):\n", "code": "assert self.status == WAIT_PROCESS\nif not all_ok:\n    self.close()\n    self.wake_up()\n    return\nself.len = ''\nif len(message) == 0:\n    # it was a oneway request, do not write answer\n    self.message = ''\n    self.status = WAIT_LEN\nelse:\n    self.message = struct.pack('!i', len(message)) + message\n    self.status = SEND_ANSWER\nself.wake_up()", "path": "plugin\\py\\lib\\thrift\\server\\TNonblockingServer.py", "repo_name": "kakkyz81/evervim", "stars": 234, "license": "None", "language": "python", "size": 745}
{"docstring": "\"\"\"Handle requests.\n       \nWARNING! You must call prepare BEFORE calling handle.\n\"\"\"\n", "func_signal": "def handle(self):\n", "code": "assert self.prepared, \"You have to call prepare before handle\"\nrset, wset, xset = self._select()\nfor readable in rset:\n    if readable == self._read.fileno():\n        # don't care i just need to clean readable flag\n        self._read.recv(1024) \n    elif readable == self.socket.handle.fileno():\n        client = self.socket.accept().handle\n        self.clients[client.fileno()] = Connection(client, self.wake_up)\n    else:\n        connection = self.clients[readable]\n        connection.read()\n        if connection.status == WAIT_PROCESS:\n            itransport = TTransport.TMemoryBuffer(connection.message)\n            otransport = TTransport.TMemoryBuffer()\n            iprot = self.in_protocol.getProtocol(itransport)\n            oprot = self.out_protocol.getProtocol(otransport)\n            self.tasks.put([self.processor, iprot, oprot, \n                            otransport, connection.ready])\nfor writeable in wset:\n    self.clients[writeable].write()\nfor oob in xset:\n    self.clients[oob].close()\n    del self.clients[oob]", "path": "plugin\\py\\lib\\thrift\\server\\TNonblockingServer.py", "repo_name": "kakkyz81/evervim", "stars": 234, "license": "None", "language": "python", "size": 745}
{"docstring": "\"\"\"Create a Flow from a clientsecrets file.\n\nWill create the right kind of Flow based on the contents of the clientsecrets\nfile or will raise InvalidClientSecretsError for unknown types of Flows.\n\nArgs:\n  filename: string, File name of client secrets.\n  scope: string or list of strings, scope(s) to request.\n  message: string, A friendly string to display to the user if the\n    clientsecrets file is missing or invalid. If message is provided then\n    sys.exit will be called in the case of an error. If message in not\n    provided then clientsecrets.InvalidClientSecretsError will be raised.\n\nReturns:\n  A Flow object.\n\nRaises:\n  UnknownClientSecretsFlowError if the file describes an unknown kind of Flow.\n  clientsecrets.InvalidClientSecretsError if the clientsecrets file is\n    invalid.\n\"\"\"\n", "func_signal": "def flow_from_clientsecrets(filename, scope, message=None):\n", "code": "try:\n  client_type, client_info = clientsecrets.loadfile(filename)\n  if client_type in [clientsecrets.TYPE_WEB, clientsecrets.TYPE_INSTALLED]:\n      return OAuth2WebServerFlow(\n          client_info['client_id'],\n          client_info['client_secret'],\n          scope,\n          None, # user_agent\n          client_info['auth_uri'],\n          client_info['token_uri'])\nexcept clientsecrets.InvalidClientSecretsError:\n  if message:\n    sys.exit(message)\n  else:\n    raise\nelse:\n  raise UnknownClientSecretsFlowError(\n      'This OAuth 2.0 flow is unsupported: \"%s\"' * client_type)", "path": "lib\\oauth2client\\client.py", "repo_name": "scotch/engineauth", "stars": 155, "license": "other", "language": "python", "size": 601}
{"docstring": "# Cookie session_id and user_id\n", "func_signal": "def test_laod_session_session_id_and_user_id(self):\n", "code": "s = models.Session.create()\ns_count = models.Session.query().count()\nself.assertTrue(s_count == 1)\nreq = EngineAuthRequest.blank('/auth/google')\nreq.cookies['_eauth'] = s.serialize()\nreq._load_session()\nself.assertTrue(req.session.session_id == s.session_id)\n# Assert No new session was created\ns_count2 = models.Session.query().count()\nself.assertTrue(s_count2 == 1)", "path": "tests\\test_middleware.py", "repo_name": "scotch/engineauth", "stars": 155, "license": "other", "language": "python", "size": 601}
{"docstring": "\"\"\"Instantiate a Credentials object from a JSON description of it. The JSON\nshould have been produced by calling .to_json() on the object.\n\nArgs:\n  data: dict, A deserialized JSON object.\n\nReturns:\n  An instance of a Credentials subclass.\n\"\"\"\n", "func_signal": "def from_json(cls, s):\n", "code": "data = simplejson.loads(s)\nif 'token_expiry' in data and not isinstance(data['token_expiry'],\n    datetime.datetime):\n  try:\n    data['token_expiry'] = datetime.datetime.strptime(\n        data['token_expiry'], EXPIRY_FORMAT)\n  except:\n    data['token_expiry'] = None\nretval = OAuth2Credentials(\n    data['access_token'],\n    data['client_id'],\n    data['client_secret'],\n    data['refresh_token'],\n    data['token_expiry'],\n    data['token_uri'],\n    data['user_agent'])\nretval.invalid = data['invalid']\nreturn retval", "path": "lib\\oauth2client\\client.py", "repo_name": "scotch/engineauth", "stars": 155, "license": "other", "language": "python", "size": 601}
{"docstring": "\"\"\"Authorize an httplib2.Http instance with these credentials.\n\nArgs:\n   http: An instance of httplib2.Http\n       or something that acts like it.\n\nReturns:\n   A modified instance of http that was passed in.\n\nExample:\n\n  h = httplib2.Http()\n  h = credentials.authorize(h)\n\nYou can't create a new OAuth subclass of httplib2.Authenication\nbecause it never gets passed the absolute URI, which is needed for\nsigning. So instead we have to overload 'request' with a closure\nthat adds in the Authorization header and then calls the original\nversion of 'request()'.\n\"\"\"\n", "func_signal": "def authorize(self, http):\n", "code": "request_orig = http.request\n\n# The closure that will replace 'httplib2.Http.request'.\ndef new_request(uri, method='GET', body=None, headers=None,\n                redirections=httplib2.DEFAULT_MAX_REDIRECTS,\n                connection_type=None):\n  if not self.access_token:\n    logger.info('Attempting refresh to obtain initial access_token')\n    self._refresh(request_orig)\n\n  # Modify the request headers to add the appropriate\n  # Authorization header.\n  if headers is None:\n    headers = {}\n  headers['authorization'] = 'OAuth ' + self.access_token\n\n  if self.user_agent is not None:\n    if 'user-agent' in headers:\n      headers['user-agent'] = self.user_agent + ' ' + headers['user-agent']\n    else:\n      headers['user-agent'] = self.user_agent\n\n  resp, content = request_orig(uri, method, body, headers,\n                               redirections, connection_type)\n\n  if resp.status == 401:\n    logger.info('Refreshing due to a 401')\n    self._refresh(request_orig)\n    headers['authorization'] = 'OAuth ' + self.access_token\n    return request_orig(uri, method, body, headers,\n                        redirections, connection_type)\n  else:\n    return (resp, content)\n\nhttp.request = new_request\nreturn http", "path": "lib\\oauth2client\\client.py", "repo_name": "scotch/engineauth", "stars": 155, "license": "other", "language": "python", "size": 601}
{"docstring": "\"\"\"True if the credential is expired or invalid.\n\nIf the token_expiry isn't set, we assume the token doesn't expire.\n\"\"\"\n", "func_signal": "def access_token_expired(self):\n", "code": "if self.invalid:\n  return True\n\nif not self.token_expiry:\n  return False\n\nnow = datetime.datetime.utcnow()\nif now >= self.token_expiry:\n  logger.info('access_token is expired. Now: %s, token_expiry: %s',\n              now, self.token_expiry)\n  return True\nreturn False", "path": "lib\\oauth2client\\client.py", "repo_name": "scotch/engineauth", "stars": 155, "license": "other", "language": "python", "size": 601}
{"docstring": "\"\"\"Create an instance of OAuth2Credentials\n\nThis is one of the few types if Credentials that you should contrust,\nCredentials objects are usually instantiated by a Flow.\n\nArgs:\n  access_token: string, access token.\n  user_agent: string, The HTTP User-Agent to provide for this application.\n\nNotes:\n  store: callable, a callable that when passed a Credential\n    will store the credential back to where it came from.\n\"\"\"\n", "func_signal": "def __init__(self, access_token, user_agent):\n", "code": "super(AccessTokenCredentials, self).__init__(\n    access_token,\n    None,\n    None,\n    None,\n    None,\n    None,\n    user_agent)", "path": "lib\\oauth2client\\client.py", "repo_name": "scotch/engineauth", "stars": 155, "license": "other", "language": "python", "size": 601}
{"docstring": "\"\"\"Generate the headers that will be used in the refresh request.\"\"\"\n", "func_signal": "def _generate_refresh_request_headers(self):\n", "code": "headers = {\n    'content-type': 'application/x-www-form-urlencoded',\n}\n\nif self.user_agent is not None:\n  headers['user-agent'] = self.user_agent\n\nreturn headers", "path": "lib\\oauth2client\\client.py", "repo_name": "scotch/engineauth", "stars": 155, "license": "other", "language": "python", "size": 601}
{"docstring": "# Cookie and not session\n", "func_signal": "def test_laod_session_cookie_and_no_session(self):\n", "code": "s = models.Session.create()\nold_sid = s.session_id\ns_serialized = s.serialize()\ns.key.delete()\ns_count = models.Session.query().count()\nself.assertTrue(s_count == 0)\nreq = EngineAuthRequest.blank('/auth/google')\nreq.cookies['_eauth'] = s_serialized\nreq._load_session()\n# Assert that a new session was created\nself.assertTrue(req.session.session_id != old_sid)\n# Assert No new session was created\ns_count2 = models.Session.query().count()\nself.assertTrue(s_count2 == 1)", "path": "tests\\test_middleware.py", "repo_name": "scotch/engineauth", "stars": 155, "license": "other", "language": "python", "size": 601}
{"docstring": "# Cookie session_id but no user_id\n", "func_signal": "def test_save_session(self):\n", "code": "s = models.Session.create()\ns_count = models.Session.query().count()\nself.assertTrue(s_count == 1)\n\nreq = EngineAuthRequest.blank('/auth/google')\nreq.cookies['_eauth'] = s.serialize()\nresp = req.get_response(app)\nresp.request = req\nresp._save_session()\n\nself.assertTrue(resp.request.session.session_id == s.session_id)\n# Assert No new session was created\ns_count2 = models.Session.query().count()\nself.assertTrue(s_count2 == 1)\n\n# Add a user_id to session\nresp.request.session.user_id = '1'\nresp._save_session()\n# a new session should be created with the user_id as it's id", "path": "tests\\test_middleware.py", "repo_name": "scotch/engineauth", "stars": 155, "license": "other", "language": "python", "size": 601}
{"docstring": "\"\"\"Trim the state down to something that can be pickled.\"\"\"\n", "func_signal": "def __getstate__(self):\n", "code": "d = copy.copy(self.__dict__)\ndel d['store']\nreturn d", "path": "lib\\oauth2client\\client.py", "repo_name": "scotch/engineauth", "stars": 155, "license": "other", "language": "python", "size": 601}
{"docstring": "\"\"\"Returns a URI to redirect to the provider.\n\nArgs:\n  redirect_uri: string, Either the string 'oob' for a non-web-based\n                application, or a URI that handles the callback from\n                the authorization server.\n\nIf redirect_uri is 'oob' then pass in the\ngenerated verification code to step2_exchange,\notherwise pass in the query parameters received\nat the callback uri to step2_exchange.\n\"\"\"\n\n", "func_signal": "def step1_get_authorize_url(self, redirect_uri='oob'):\n", "code": "self.redirect_uri = redirect_uri\nquery = {\n    'response_type': 'code',\n    'client_id': self.client_id,\n    'redirect_uri': redirect_uri,\n    'scope': self.scope,\n    }\nquery.update(self.params)\nparts = list(urlparse.urlparse(self.auth_uri))\nquery.update(dict(parse_qsl(parts[4]))) # 4 is the index of the query part\nparts[4] = urllib.urlencode(query)\nreturn urlparse.urlunparse(parts)", "path": "lib\\oauth2client\\client.py", "repo_name": "scotch/engineauth", "stars": 155, "license": "other", "language": "python", "size": 601}
{"docstring": "\"\"\"Reconstitute the state of the object from being pickled.\"\"\"\n", "func_signal": "def __setstate__(self, state):\n", "code": "self.__dict__.update(state)\nself.store = None", "path": "lib\\oauth2client\\client.py", "repo_name": "scotch/engineauth", "stars": 155, "license": "other", "language": "python", "size": 601}
{"docstring": "\"\"\"Utility class method to instantiate a Credentials subclass from a JSON\nrepresentation produced by to_json().\n\nArgs:\n  s: string, JSON from to_json().\n\nReturns:\n  An instance of the subclass of Credentials that was serialized with\n  to_json().\n\"\"\"\n", "func_signal": "def new_from_json(cls, s):\n", "code": "data = simplejson.loads(s)\n# Find and call the right classmethod from_json() to restore the object.\nmodule = data['_module']\nm = __import__(module, fromlist=module.split('.')[:-1])\nkls = getattr(m, data['_class'])\nfrom_json = getattr(kls, 'from_json')\nreturn from_json(s)", "path": "lib\\oauth2client\\client.py", "repo_name": "scotch/engineauth", "stars": 155, "license": "other", "language": "python", "size": 601}
{"docstring": "\"\"\"Write a credential.\n\nThe Storage lock must be held when this is called.\n\nArgs:\n  credentials: Credentials, the credentials to store.\n\"\"\"\n", "func_signal": "def put(self, credentials):\n", "code": "self.acquire_lock()\ntry:\n  self.locked_put(credentials)\nfinally:\n  self.release_lock()", "path": "lib\\oauth2client\\client.py", "repo_name": "scotch/engineauth", "stars": 155, "license": "other", "language": "python", "size": 601}
{"docstring": "# No existing User no logged in User\n", "func_signal": "def test__load_user_by_profile(self):\n", "code": "auth_id = 'test:12345'\nuser_info = {\n    'auth_id': auth_id,\n    'info': {},\n}\n# create profile\np = models.UserProfile.get_or_create(auth_id, user_info)\nreq = EngineAuthRequest.blank('/auth/google')\nreq._load_session()\nreq._load_user()\n\n# User Count before\nuser_count = models.User.query().count()\nself.assertEqual(user_count, 0)\n\nreq.load_user_by_profile(p)\n\n# User Count after\nuser_count = models.User.query().count()\nself.assertEqual(user_count, 1)\n\nuser = models.User.query().get()\nself.assertTrue(p.key.id() in user.auth_ids)\n\n# Yes existing User no logged in User\nreq = EngineAuthRequest.blank('/auth/google')\nreq._load_session()\nreq._load_user()\n\nreq.load_user_by_profile(p)\n\n# Test to no new User was created\nuser_count = models.User.query().count()\nself.assertEqual(user_count, 1)\n\n# Yes existing User yes logged in User new Profile\nauth_id = 'test:abc'\nuser_info = {\n    'auth_id': auth_id,\n    'info': {},\n    }\n# create profile\np1 = models.UserProfile.get_or_create(auth_id, user_info)\nreq.load_user_by_profile(p1)\n\n# Test to no new User was created\nuser_count = models.User.query().count()\nself.assertEqual(user_count, 1)", "path": "tests\\test_middleware.py", "repo_name": "scotch/engineauth", "stars": 155, "license": "other", "language": "python", "size": 601}
{"docstring": "\"\"\"Refreshes the access_token.\n\nThis method first checks by reading the Storage object if available.\nIf a refresh is still needed, it holds the Storage lock until the\nrefresh is completed.\n\"\"\"\n", "func_signal": "def _refresh(self, http_request):\n", "code": "if not self.store:\n  self._do_refresh_request(http_request)\nelse:\n  self.store.acquire_lock()\n  try:\n    new_cred = self.store.locked_get()\n    if (new_cred and not new_cred.invalid and\n        new_cred.access_token != self.access_token):\n      logger.info('Updated access_token read from Storage')\n      self._updateFromCredential(new_cred)\n    else:\n      self._do_refresh_request(http_request)\n  finally:\n    self.store.release_lock()", "path": "lib\\oauth2client\\client.py", "repo_name": "scotch/engineauth", "stars": 155, "license": "other", "language": "python", "size": 601}
{"docstring": "\"\"\"Refresh the access_token using the refresh_token.\n\nArgs:\n   http: An instance of httplib2.Http.request\n       or something that acts like it.\n\nRaises:\n  AccessTokenRefreshError: When the refresh fails.\n\"\"\"\n", "func_signal": "def _do_refresh_request(self, http_request):\n", "code": "body = self._generate_refresh_request_body()\nheaders = self._generate_refresh_request_headers()\n\nlogger.info('Refresing access_token')\nresp, content = http_request(\n    self.token_uri, method='POST', body=body, headers=headers)\nif resp.status == 200:\n  # TODO(jcgregorio) Raise an error if loads fails?\n  d = simplejson.loads(content)\n  self.access_token = d['access_token']\n  self.refresh_token = d.get('refresh_token', self.refresh_token)\n  if 'expires_in' in d:\n    self.token_expiry = datetime.timedelta(\n        seconds=int(d['expires_in'])) + datetime.datetime.utcnow()\n  else:\n    self.token_expiry = None\n  if self.store:\n    self.store.locked_put(self)\nelse:\n  # An {'error':...} response body means the token is expired or revoked,\n  # so we flag the credentials as such.\n  logger.error('Failed to retrieve access token: %s' % content)\n  error_msg = 'Invalid response %s.' % resp['status']\n  try:\n    d = simplejson.loads(content)\n    if 'error' in d:\n      error_msg = d['error']\n      self.invalid = True\n      if self.store:\n        self.store.locked_put(self)\n  except:\n    pass\n  raise AccessTokenRefreshError(error_msg)", "path": "lib\\oauth2client\\client.py", "repo_name": "scotch/engineauth", "stars": 155, "license": "other", "language": "python", "size": 601}
{"docstring": "\"\"\"Exhanges a code for OAuth2Credentials.\n\nArgs:\n  code: string or dict, either the code as a string, or a dictionary\n    of the query parameters to the redirect_uri, which contains\n    the code.\n  http: httplib2.Http, optional http instance to use to do the fetch\n\"\"\"\n\n", "func_signal": "def step2_exchange(self, code, http=None):\n", "code": "if not (isinstance(code, str) or isinstance(code, unicode)):\n  code = code['code']\n\nbody = urllib.urlencode({\n    'grant_type': 'authorization_code',\n    'client_id': self.client_id,\n    'client_secret': self.client_secret,\n    'code': code,\n    'redirect_uri': self.redirect_uri,\n    'scope': self.scope,\n    })\nheaders = {\n    'content-type': 'application/x-www-form-urlencoded',\n}\n\nif self.user_agent is not None:\n  headers['user-agent'] = self.user_agent\n\nif http is None:\n  http = httplib2.Http()\nresp, content = http.request(self.token_uri, method='POST', body=body,\n                             headers=headers)\nif resp.status == 200:\n  # TODO(jcgregorio) Raise an error if simplejson.loads fails?\n  try:\n    d = simplejson.loads(content)\n  except:\n    d = response_decoder(content)\n\n  access_token = d['access_token']\n  refresh_token = d.get('refresh_token', None)\n  token_expiry = None\n  if 'expires_in' in d:\n    token_expiry = datetime.datetime.utcnow() + datetime.timedelta(\n        seconds=int(d['expires_in']))\n\n  logger.info('Successfully retrieved access token: %s' % content)\n  return OAuth2Credentials(access_token, self.client_id,\n                           self.client_secret, refresh_token, token_expiry,\n                           self.token_uri, self.user_agent)\nelse:\n  logger.error('Failed to retrieve access token: %s' % content)\n  error_msg = 'Invalid response %s.' % resp['status']\n  try:\n    d = simplejson.loads(content)\n    if 'error' in d:\n      error_msg = d['error']\n  except:\n    pass\n\n  raise FlowExchangeError(error_msg)", "path": "lib\\oauth2client\\client.py", "repo_name": "scotch/engineauth", "stars": 155, "license": "other", "language": "python", "size": 601}
{"docstring": "\"\"\"Utility function for creating a JSON representation of an instance of Credentials.\n\nArgs:\n  strip: array, An array of names of members to not include in the JSON.\n\nReturns:\n   string, a JSON representation of this instance, suitable to pass to\n   from_json().\n\"\"\"\n", "func_signal": "def _to_json(self, strip):\n", "code": "t = type(self)\nd = copy.copy(self.__dict__)\nfor member in strip:\n  del d[member]\nif 'token_expiry' in d and isinstance(d['token_expiry'], datetime.datetime):\n  d['token_expiry'] = d['token_expiry'].strftime(EXPIRY_FORMAT)\n# Add in information we will need later to reconsistitue this instance.\nd['_class'] = t.__name__\nd['_module'] = t.__module__\nreturn simplejson.dumps(d)", "path": "lib\\oauth2client\\client.py", "repo_name": "scotch/engineauth", "stars": 155, "license": "other", "language": "python", "size": 601}
{"docstring": "# Cookie session_id but no user_id\n", "func_signal": "def test_laod_session_session_id_no_user_id(self):\n", "code": "s = models.Session.create()\ns_count = models.Session.query().count()\nself.assertTrue(s_count == 1)\nreq = EngineAuthRequest.blank('/auth/google')\nreq.cookies['_eauth'] = s.serialize()\nreq._load_session()\nself.assertTrue(req.session.session_id == s.session_id)\n# Assert No new session was created\ns_count2 = models.Session.query().count()\nself.assertTrue(s_count2 == 1)", "path": "tests\\test_middleware.py", "repo_name": "scotch/engineauth", "stars": 155, "license": "other", "language": "python", "size": 601}
{"docstring": "\"\"\"Parse a I[.F] seconds value into (seconds, microseconds).\"\"\"\n", "func_signal": "def _parsems(value):\n", "code": "if \".\" not in value:\n    return int(value), 0\nelse:\n    i, f = value.split(\".\")\n    return int(i), int(f.ljust(6, \"0\")[:6])", "path": "dateutil\\parser.py", "repo_name": "ynhacler/RedKindle", "stars": 180, "license": "None", "language": "python", "size": 32988}
{"docstring": "# Assuming all templates are html\n", "func_signal": "def __getattr__(self, name):\n", "code": "path = name + \".html\"\nt = self._lookup.get_template(path)\nreturn t.render", "path": "web\\contrib\\template.py", "repo_name": "ynhacler/RedKindle", "stars": 180, "license": "None", "language": "python", "size": 32988}
{"docstring": "\"\"\"Returns DOMname for given CSSname e.g. for CSSname 'font-style' returns\n'fontStyle'.\n\"\"\"\n", "func_signal": "def _toDOMname(CSSname):\n", "code": "def _doCSStoDOMname2(m): return m.group(0)[1].capitalize()\nreturn _reCSStoDOMname.sub(_doCSStoDOMname2, CSSname)", "path": "cssutils\\css\\cssproperties.py", "repo_name": "ynhacler/RedKindle", "stars": 180, "license": "None", "language": "python", "size": 32988}
{"docstring": "\"\"\"Return CSSname for given DOMname e.g. for DOMname 'fontStyle' returns\n'font-style'.\n\"\"\"\n", "func_signal": "def _toCSSname(DOMname):\n", "code": "def _doDOMtoCSSname2(m): return '-' + m.group(0).lower()\nreturn _reDOMtoCSSname.sub(_doDOMtoCSSname2, DOMname)", "path": "cssutils\\css\\cssproperties.py", "repo_name": "ynhacler/RedKindle", "stars": 180, "license": "None", "language": "python", "size": 32988}
{"docstring": "# give error if Chetah is not installed\n", "func_signal": "def __init__(self, path):\n", "code": "from Cheetah.Template import Template\nself.path = path", "path": "web\\contrib\\template.py", "repo_name": "ynhacler/RedKindle", "stars": 180, "license": "None", "language": "python", "size": 32988}
{"docstring": "\"\"\"\nPrints a prettyprinted version of `args` to stderr.\n\"\"\"\n", "func_signal": "def debug(*args):\n", "code": "try: \n    out = ctx.environ['wsgi.errors']\nexcept: \n    out = sys.stderr\nfor arg in args:\n    print >> out, pprint.pformat(arg)\nreturn ''", "path": "web\\webapi.py", "repo_name": "ynhacler/RedKindle", "stars": 180, "license": "None", "language": "python", "size": 32988}
{"docstring": "# Remain active as long as any of the model probers are active.\n", "func_signal": "def get_state(self):\n", "code": "if (self._mLogicalProber.get_state() == constants.eNotMe) and \\\n   (self._mVisualProber.get_state() == constants.eNotMe):\n    return constants.eNotMe\nreturn constants.eDetecting", "path": "chardet\\hebrewprober.py", "repo_name": "ynhacler/RedKindle", "stars": 180, "license": "None", "language": "python", "size": 32988}
{"docstring": "# Assuming all templates are html\n", "func_signal": "def __getattr__(self, name):\n", "code": "path = name + \".html\"\n\nif self._type == \"text\":\n    from genshi.template import TextTemplate\n    cls = TextTemplate\n    type = \"text\"\nelse:\n    cls = None\n    type = None\n\nt = self._loader.load(path, cls=cls)\ndef template(**kw):\n    stream = t.generate(**kw)\n    if type:\n        return stream.render(type)\n    else:\n        return stream.render()\nreturn template", "path": "web\\contrib\\template.py", "repo_name": "ynhacler/RedKindle", "stars": 180, "license": "None", "language": "python", "size": 32988}
{"docstring": "\"\"\"\nReturns a `storage` object with the GET and POST arguments. \nSee `storify` for how `requireds` and `defaults` work.\n\"\"\"\n", "func_signal": "def input(*requireds, **defaults):\n", "code": "_method = defaults.pop('_method', 'both')\nout = rawinput(_method)\ntry:\n    defaults.setdefault('_unicode', True) # force unicode conversion by default.\n    return storify(out, *requireds, **defaults)\nexcept KeyError:\n    raise badrequest()", "path": "web\\webapi.py", "repo_name": "ynhacler/RedKindle", "stars": 180, "license": "None", "language": "python", "size": 32988}
{"docstring": "\"\"\"Returns HTTPError with '404 Not Found' error from the active application.\n\"\"\"\n", "func_signal": "def NotFound(message=None):\n", "code": "if message:\n    return _NotFound(message)\nelif ctx.get('app_stack'):\n    return ctx.app_stack[-1].notfound()\nelse:\n    return _NotFound()", "path": "web\\webapi.py", "repo_name": "ynhacler/RedKindle", "stars": 180, "license": "None", "language": "python", "size": 32988}
{"docstring": "# move to info\n", "func_signal": "def validate(self, res):\n", "code": "if res.year is not None:\n    res.year = self.convertyear(res.year)\nif res.tzoffset == 0 and not res.tzname or res.tzname == 'Z':\n    res.tzname = \"UTC\"\n    res.tzoffset = 0\nelif res.tzoffset != 0 and res.tzname and self.utczone(res.tzname):\n    res.tzoffset = 0\nreturn True", "path": "dateutil\\parser.py", "repo_name": "ynhacler/RedKindle", "stars": 180, "license": "None", "language": "python", "size": 32988}
{"docstring": "# Final letter analysis for logical-visual decision.\n# Look for evidence that the received buffer is either logical Hebrew or \n# visual Hebrew.\n# The following cases are checked:\n# 1) A word longer than 1 letter, ending with a final letter. This is an \n#    indication that the text is laid out \"naturally\" since the final letter \n#    really appears at the end. +1 for logical score.\n# 2) A word longer than 1 letter, ending with a Non-Final letter. In normal\n#    Hebrew, words ending with Kaf, Mem, Nun, Pe or Tsadi, should not end with\n#    the Non-Final form of that letter. Exceptions to this rule are mentioned\n#    above in isNonFinal(). This is an indication that the text is laid out\n#    backwards. +1 for visual score\n# 3) A word longer than 1 letter, starting with a final letter. Final letters \n#    should not appear at the beginning of a word. This is an indication that \n#    the text is laid out backwards. +1 for visual score.\n# \n# The visual score and logical score are accumulated throughout the text and \n# are finally checked against each other in GetCharSetName().\n# No checking for final letters in the middle of words is done since that case\n# is not an indication for either Logical or Visual text.\n# \n# We automatically filter out all 7-bit characters (replace them with spaces)\n# so the word boundary detection works properly. [MAP]\n\n", "func_signal": "def feed(self, aBuf):\n", "code": "if self.get_state() == constants.eNotMe:\n    # Both model probers say it's not them. No reason to continue.\n    return constants.eNotMe\n\naBuf = self.filter_high_bit_only(aBuf)\n\nfor cur in aBuf:\n    if cur == ' ':\n        # We stand on a space - a word just ended\n        if self._mBeforePrev != ' ':\n            # next-to-last char was not a space so self._mPrev is not a 1 letter word\n            if self.is_final(self._mPrev):\n                # case (1) [-2:not space][-1:final letter][cur:space]\n                self._mFinalCharLogicalScore += 1\n            elif self.is_non_final(self._mPrev):\n                # case (2) [-2:not space][-1:Non-Final letter][cur:space]\n                self._mFinalCharVisualScore += 1\n    else:\n        # Not standing on a space\n        if (self._mBeforePrev == ' ') and (self.is_final(self._mPrev)) and (cur != ' '):\n            # case (3) [-2:space][-1:final letter][cur:not space]\n            self._mFinalCharVisualScore += 1\n    self._mBeforePrev = self._mPrev\n    self._mPrev = cur\n\n# Forever detecting, till the end or until both model probers return eNotMe (handled above)\nreturn constants.eDetecting", "path": "chardet\\hebrewprober.py", "repo_name": "ynhacler/RedKindle", "stars": 180, "license": "None", "language": "python", "size": 32988}
{"docstring": "\"\"\"Returns storage object with GET or POST arguments.\n\"\"\"\n", "func_signal": "def rawinput(method=None):\n", "code": "method = method or \"both\"\nfrom cStringIO import StringIO\n\ndef dictify(fs): \n    # hack to make web.input work with enctype='text/plain.\n    if fs.list is None:\n        fs.list = [] \n\n    return dict([(k, fs[k]) for k in fs.keys()])\n\ne = ctx.env.copy()\na = b = {}\n\nif method.lower() in ['both', 'post', 'put']:\n    if e['REQUEST_METHOD'] in ['POST', 'PUT']:\n        if e.get('CONTENT_TYPE', '').lower().startswith('multipart/'):\n            # since wsgi.input is directly passed to cgi.FieldStorage, \n            # it can not be called multiple times. Saving the FieldStorage\n            # object in ctx to allow calling web.input multiple times.\n            a = ctx.get('_fieldstorage')\n            if not a:\n                fp = e['wsgi.input']\n                a = cgi.FieldStorage(fp=fp, environ=e, keep_blank_values=1)\n                ctx._fieldstorage = a\n        else:\n            fp = StringIO(data())\n            a = cgi.FieldStorage(fp=fp, environ=e, keep_blank_values=1)\n        a = dictify(a)\n\nif method.lower() in ['both', 'get']:\n    e['REQUEST_METHOD'] = 'GET'\n    b = dictify(cgi.FieldStorage(environ=e, keep_blank_values=1))\n\ndef process_fieldstorage(fs):\n    if isinstance(fs, list):\n        return [process_fieldstorage(x) for x in fs]\n    elif fs.filename is None:\n        return fs.value\n    else:\n        return fs\n\nreturn storage([(k, process_fieldstorage(v)) for k, v in dictadd(b, a).items()])", "path": "web\\webapi.py", "repo_name": "ynhacler/RedKindle", "stars": 180, "license": "None", "language": "python", "size": 32988}
{"docstring": "'''\nConvert image setting all transparent pixels to white and changing format\nto JPEG. Ensure the resultant image has a byte size less than maxsizeb.\n\nIf dimen is not None, generate a thumbnail of\nwidth=dimen, height=dimen or width, height = dimen (depending on the type of dimen)\n\nReturns the image as a bytestring.\n'''\n", "func_signal": "def rescale_image(data, maxsizeb=4000000, dimen=None, png2jpg=False, graying=True, reduceto=(600,800)):\n", "code": "if not isinstance(data,StringIO):\n\tdata = StringIO(data)\nimg = Image.open(data)\nwidth, height = img.size\nfmt = img.format\nif graying and img.mode != \"L\":\n\timg = img.convert(\"L\")\n\nreducewidth, reduceheight = reduceto\n\nif dimen is not None:\n\tif hasattr(dimen, '__len__'):\n\t\twidth, height = dimen\n\telse:\n\t\twidth = height = dimen\n\n\timg.thumbnail((width, height))\n\tif png2jpg and fmt == 'PNG':\n\t\tfmt = 'JPEG'\n\tdata = StringIO()\n\timg.save(data, fmt)\nelif width > reducewidth or height > reduceheight:\n\tratio = min(float(reducewidth)/float(width), float(reduceheight)/float(height))\n\timg = img.resize((int(width*ratio), int(height*ratio)))\n\tif png2jpg and fmt == 'PNG':\n\t\tfmt = 'JPEG'\n\tdata = StringIO()\n\timg.save(data, fmt)\nelif png2jpg and fmt == 'PNG':\n\tdata = StringIO()\n\timg.save(data, 'JPEG')\nelse:\n\tdata = StringIO()\n\timg.save(data,fmt)\n\nreturn data.getvalue()", "path": "lib\\img.py", "repo_name": "ynhacler/RedKindle", "stars": 180, "license": "None", "language": "python", "size": 32988}
{"docstring": "\"\"\"Returns HTTPError with '500 internal error' error from the active application.\n\"\"\"\n", "func_signal": "def InternalError(message=None):\n", "code": "if message:\n    return _InternalError(message)\nelif ctx.get('app_stack'):\n    return ctx.app_stack[-1].internalerror()\nelse:\n    return _InternalError()", "path": "web\\webapi.py", "repo_name": "ynhacler/RedKindle", "stars": 180, "license": "None", "language": "python", "size": 32988}
{"docstring": "\"\"\"\nClosure to keep name known in each properties accessor function\nDOMname is converted to CSSname here, so actual calls use CSSname.\n\"\"\"\n", "func_signal": "def __named_property_def(DOMname):\n", "code": "CSSname = _toCSSname(DOMname)\ndef _get(self): return self._getP(CSSname)\ndef _set(self, value): self._setP(CSSname, value)\ndef _del(self): self._delP(CSSname)\nreturn _get, _set, _del", "path": "cssutils\\css\\cssproperties.py", "repo_name": "ynhacler/RedKindle", "stars": 180, "license": "None", "language": "python", "size": 32988}
{"docstring": "# Make the decision: is it Logical or Visual?\n# If the final letter score distance is dominant enough, rely on it.\n", "func_signal": "def get_charset_name(self):\n", "code": "finalsub = self._mFinalCharLogicalScore - self._mFinalCharVisualScore\nif finalsub >= MIN_FINAL_CHAR_DISTANCE:\n    return LOGICAL_HEBREW_NAME\nif finalsub <= -MIN_FINAL_CHAR_DISTANCE:\n    return VISUAL_HEBREW_NAME\n\n# It's not dominant enough, try to rely on the model scores instead.\nmodelsub = self._mLogicalProber.get_confidence() - self._mVisualProber.get_confidence()\nif modelsub > MIN_MODEL_DISTANCE:\n    return LOGICAL_HEBREW_NAME\nif modelsub < -MIN_MODEL_DISTANCE:\n    return VISUAL_HEBREW_NAME\n\n# Still no good, back to final letter distance, maybe it'll save the day.\nif finalsub < 0.0:\n    return VISUAL_HEBREW_NAME\n\n# (finalsub > 0 - Logical) or (don't know what to do) default to Logical.\nreturn LOGICAL_HEBREW_NAME", "path": "chardet\\hebrewprober.py", "repo_name": "ynhacler/RedKindle", "stars": 180, "license": "None", "language": "python", "size": 32988}
{"docstring": "\"\"\"Returns the data sent with the request.\"\"\"\n", "func_signal": "def data():\n", "code": "if 'data' not in ctx:\n    cl = intget(ctx.env.get('CONTENT_LENGTH'), 0)\n    ctx.data = ctx.env['wsgi.input'].read(cl)\nreturn ctx.data", "path": "web\\webapi.py", "repo_name": "ynhacler/RedKindle", "stars": 180, "license": "None", "language": "python", "size": 32988}
{"docstring": "\"\"\"Generate the document\n\n:param input: string of the html content.\n\nkwargs:\n    - attributes:\n    - debug: output debug messages\n    - min_text_length:\n    - retry_length:\n    - url: will allow adjusting links to be absolute\n\n\"\"\"\n", "func_signal": "def __init__(self, input, **options):\n", "code": "self.input = input\nself.options = options\nself.html = None", "path": "lib\\readability_old\\readability.py", "repo_name": "ynhacler/RedKindle", "stars": 180, "license": "None", "language": "python", "size": 32988}
{"docstring": "\"\"\"\nReturns a `status` redirect to the new URL. \n`url` is joined with the base URL so that things like \n`redirect(\"about\") will work properly.\n\"\"\"\n", "func_signal": "def __init__(self, url, status='301 Moved Permanently', absolute=False):\n", "code": "newloc = urlparse.urljoin(ctx.path, url)\n\nif newloc.startswith('/'):\n    if absolute:\n        home = ctx.realhome\n    else:\n        home = ctx.home\n    newloc = home + newloc\n\nheaders = {\n    'Content-Type': 'text/html',\n    'Location': newloc\n}\nHTTPError.__init__(self, status, headers, \"\")", "path": "web\\webapi.py", "repo_name": "ynhacler/RedKindle", "stars": 180, "license": "None", "language": "python", "size": 32988}
{"docstring": "\"\"\"\u5c06xml\u8f6c\u4e3aarray\"\"\"\n", "func_signal": "def xmlToArray(self, xml):\n", "code": "array_data = {}\nroot = ET.fromstring(xml)\nfor child in root:\n    value = child.text\n    array_data[child.tag] = value\nreturn array_data", "path": "wzhifuSDK.py", "repo_name": "Skycrab/wzhifuSDK", "stars": 196, "license": "None", "language": "python", "size": 49}
{"docstring": "\"\"\"\u4ea7\u751f\u968f\u673a\u5b57\u7b26\u4e32\uff0c\u4e0d\u957f\u4e8e32\u4f4d\"\"\"\n", "func_signal": "def createNoncestr(self, length = 32):\n", "code": "chars = \"abcdefghijklmnopqrstuvwxyz0123456789\"\nstrs = []\nfor x in range(length):\n    strs.append(chars[random.randrange(0, len(chars))])\nreturn \"\".join(strs)", "path": "wzhifuSDK.py", "repo_name": "Skycrab/wzhifuSDK", "stars": 196, "license": "None", "language": "python", "size": 49}
{"docstring": "\"\"\"\u83b7\u53d6\u7ed3\u679c\uff0c\u9ed8\u8ba4\u4e0d\u4f7f\u7528\u8bc1\u4e66\"\"\"\n", "func_signal": "def getResult(self):\n", "code": "self.postXml()\nself.result = self.xmlToArray(self.response)\nreturn self.result", "path": "wzhifuSDK.py", "repo_name": "Skycrab/wzhifuSDK", "stars": 196, "license": "None", "language": "python", "size": 49}
{"docstring": "\"\"\"\u751f\u6210\u63a5\u53e3\u53c2\u6570xml\"\"\"\n", "func_signal": "def createXml(self):\n", "code": "if any(self.parameters[key] is None for key in (\"out_trade_no\", \"out_refund_no\", \"total_fee\", \"refund_fee\", \"op_user_id\")):\n    raise ValueError(\"missing parameter\")\n\nself.parameters[\"appid\"] = WxPayConf_pub.APPID  #\u516c\u4f17\u8d26\u53f7ID\nself.parameters[\"mch_id\"] = WxPayConf_pub.MCHID  #\u5546\u6237\u53f7\nself.parameters[\"nonce_str\"] = self.createNoncestr()  #\u968f\u673a\u5b57\u7b26\u4e32\nself.parameters[\"sign\"] = self.getSign(self.parameters)  #\u7b7e\u540d\nreturn  self.arrayToXml(self.parameters)", "path": "wzhifuSDK.py", "repo_name": "Skycrab/wzhifuSDK", "stars": 196, "license": "None", "language": "python", "size": 49}
{"docstring": "\"\"\"\u751f\u6210\u63a5\u53e3\u53c2\u6570xml\"\"\"\n", "func_signal": "def createXml(self):\n", "code": "if any(self.parameters[key] is None for key in (\"long_url\", )):\n    raise ValueError(\"missing parameter\")\n\nself.parameters[\"appid\"] = WxPayConf_pub.APPID  #\u516c\u4f17\u8d26\u53f7ID\nself.parameters[\"mch_id\"] = WxPayConf_pub.MCHID  #\u5546\u6237\u53f7\nself.parameters[\"nonce_str\"] = self.createNoncestr()  #\u968f\u673a\u5b57\u7b26\u4e32\nself.parameters[\"sign\"] = self.getSign(self.parameters)  #\u7b7e\u540d\nreturn  self.arrayToXml(self.parameters)", "path": "wzhifuSDK.py", "repo_name": "Skycrab/wzhifuSDK", "stars": 196, "license": "None", "language": "python", "size": 49}
{"docstring": "#\u8bbe\u7f6e\u63a5\u53e3\u94fe\u63a5\n", "func_signal": "def __init__(self, timeout=WxPayConf_pub.CURL_TIMEOUT):\n", "code": "self.url = \"https://api.mch.weixin.qq.com/pay/downloadbill\"\n#\u8bbe\u7f6ecurl\u8d85\u65f6\u65f6\u95f4\nself.curl_timeout = timeout\nsuper(DownloadBill_pub, self).__init__()", "path": "wzhifuSDK.py", "repo_name": "Skycrab/wzhifuSDK", "stars": 196, "license": "None", "language": "python", "size": 49}
{"docstring": "\"\"\"\u751f\u6210\u63a5\u53e3\u53c2\u6570xml\"\"\"\n", "func_signal": "def createXml(self):\n", "code": "if self.returnParameters[\"return_code\"] == self.SUCCESS:\n    self.returnParameters[\"appid\"] = WxPayConf_pub.APPID #\u516c\u4f17\u8d26\u53f7ID\n    self.returnParameters[\"mch_id\"] = WxPayConf_pub.MCHID #\u5546\u6237\u53f7\n    self.returnParameters[\"nonce_str\"] = self.createNoncestr() #\u968f\u673a\u5b57\u7b26\u4e32\n    self.returnParameters[\"sign\"] = self.getSign(self.returnParameters) #\u7b7e\u540d\n\nreturn self.arrayToXml(self.returnParameters)", "path": "wzhifuSDK.py", "repo_name": "Skycrab/wzhifuSDK", "stars": 196, "license": "None", "language": "python", "size": 49}
{"docstring": "\"\"\"\u5c06xml\u6570\u636e\u8fd4\u56de\u5fae\u4fe1\"\"\"\n", "func_signal": "def returnXml(self):\n", "code": "returnXml = self.createXml()\nreturn returnXml", "path": "wzhifuSDK.py", "repo_name": "Skycrab/wzhifuSDK", "stars": 196, "license": "None", "language": "python", "size": 49}
{"docstring": "\"\"\" \u83b7\u53d6\u7ed3\u679c\uff0c\u4f7f\u7528\u8bc1\u4e66\u901a\u4fe1(\u9700\u8981\u53cc\u5411\u8bc1\u4e66)\"\"\"\n", "func_signal": "def getResult(self):\n", "code": "self.postXmlSSL()\nself.result = self.xmlToArray(self.response)\nreturn self.result", "path": "wzhifuSDK.py", "repo_name": "Skycrab/wzhifuSDK", "stars": 196, "license": "None", "language": "python", "size": 49}
{"docstring": "\"\"\"\u6821\u9a8c\u7b7e\u540d\"\"\"\n", "func_signal": "def checkSign(self):\n", "code": "tmpData = dict(self.data) #make a copy to save sign\ndel tmpData['sign']\nsign = self.getSign(tmpData) #\u672c\u5730\u7b7e\u540d\nif self.data['sign'] == sign:\n    return True\nreturn False", "path": "wzhifuSDK.py", "repo_name": "Skycrab/wzhifuSDK", "stars": 196, "license": "None", "language": "python", "size": 49}
{"docstring": "\"\"\"\u751f\u6210\u7b7e\u540d\"\"\"\n#\u7b7e\u540d\u6b65\u9aa4\u4e00\uff1a\u6309\u5b57\u5178\u5e8f\u6392\u5e8f\u53c2\u6570,formatBizQueryParaMap\u5df2\u505a\n", "func_signal": "def getSign(self, obj):\n", "code": "String = self.formatBizQueryParaMap(obj, False)\n#\u7b7e\u540d\u6b65\u9aa4\u4e8c\uff1a\u5728string\u540e\u52a0\u5165KEY\nString = \"{0}&key={1}\".format(String,WxPayConf_pub.KEY)\n#\u7b7e\u540d\u6b65\u9aa4\u4e09\uff1aMD5\u52a0\u5bc6\nString = hashlib.md5(String).hexdigest()\n#\u7b7e\u540d\u6b65\u9aa4\u56db\uff1a\u6240\u6709\u5b57\u7b26\u8f6c\u4e3a\u5927\u5199\nresult_ = String.upper()\nreturn result_", "path": "wzhifuSDK.py", "repo_name": "Skycrab/wzhifuSDK", "stars": 196, "license": "None", "language": "python", "size": 49}
{"docstring": "\"\"\"\u683c\u5f0f\u5316\u53c2\u6570\uff0c\u7b7e\u540d\u8fc7\u7a0b\u9700\u8981\u4f7f\u7528\"\"\"\n", "func_signal": "def formatBizQueryParaMap(self, paraMap, urlencode):\n", "code": "slist = sorted(paraMap)\nbuff = []\nfor k in slist:\n    v = quote(paraMap[k]) if urlencode else paraMap[k]\n    buff.append(\"{0}={1}\".format(k, v))\n\nreturn \"&\".join(buff)", "path": "wzhifuSDK.py", "repo_name": "Skycrab/wzhifuSDK", "stars": 196, "license": "None", "language": "python", "size": 49}
{"docstring": "\"\"\"\u8bbe\u7f6ejsapi\u7684\u53c2\u6570\"\"\"\n", "func_signal": "def  getParameters(self):\n", "code": "jsApiObj = {}\njsApiObj[\"appId\"] = WxPayConf_pub.APPID\ntimeStamp = int(time.time())\njsApiObj[\"timeStamp\"] = \"{0}\".format(timeStamp)\njsApiObj[\"nonceStr\"] = self.createNoncestr()\njsApiObj[\"package\"] = \"prepay_id={0}\".format(self.prepay_id)\njsApiObj[\"signType\"] = \"MD5\"\njsApiObj[\"paySign\"] = self.getSign(jsApiObj)\nself.parameters = json.dumps(jsApiObj)\n\nreturn self.parameters", "path": "wzhifuSDK.py", "repo_name": "Skycrab/wzhifuSDK", "stars": 196, "license": "None", "language": "python", "size": 49}
{"docstring": "\"\"\"\u4f7f\u7528\u8bc1\u4e66\"\"\"\n", "func_signal": "def postXmlSSL(self, xml, url, second=30, cert=True, post=True):\n", "code": "self.curl.setopt(pycurl.URL, url)\nself.curl.setopt(pycurl.TIMEOUT, second)\n#\u8bbe\u7f6e\u8bc1\u4e66\n#\u4f7f\u7528\u8bc1\u4e66\uff1acert \u4e0e key \u5206\u522b\u5c5e\u4e8e\u4e24\u4e2a.pem\u6587\u4ef6\n#\u9ed8\u8ba4\u683c\u5f0f\u4e3aPEM\uff0c\u53ef\u4ee5\u6ce8\u91ca\nif cert:\n    self.curl.setopt(pycurl.SSLKEYTYPE, \"PEM\")\n    self.curl.setopt(pycurl.SSLKEY, WxPayConf_pub.SSLKEY_PATH)\n    self.curl.setopt(pycurl.SSLCERTTYPE, \"PEM\")\n    self.curl.setopt(pycurl.SSLCERT, WxPayConf_pub.SSLCERT_PATH)\n#post\u63d0\u4ea4\u65b9\u5f0f\nif post:\n    self.curl.setopt(pycurl.POST, True)\n    self.curl.setopt(pycurl.POSTFIELDS, xml)\nbuff = StringIO()\nself.curl.setopt(pycurl.WRITEFUNCTION, buff.write)\n\nself.curl.perform()\nreturn buff.getvalue()", "path": "wzhifuSDK.py", "repo_name": "Skycrab/wzhifuSDK", "stars": 196, "license": "None", "language": "python", "size": 49}
{"docstring": "\"\"\"\u901a\u8fc7curl\u5411\u5fae\u4fe1\u63d0\u4ea4code\uff0c\u4ee5\u83b7\u53d6openid\"\"\"\n", "func_signal": "def getOpenid(self):\n", "code": "url = self.createOauthUrlForOpenid()\ndata = HttpClient().get(url)\nself.openid = json.loads(data)[\"openid\"]\nreturn self.openid", "path": "wzhifuSDK.py", "repo_name": "Skycrab/wzhifuSDK", "stars": 196, "license": "None", "language": "python", "size": 49}
{"docstring": "#\u8bbe\u7f6e\u63a5\u53e3\u94fe\u63a5\n", "func_signal": "def __init__(self, timeout=WxPayConf_pub.CURL_TIMEOUT):\n", "code": "self.url = \"https://api.mch.weixin.qq.com/secapi/pay/refund\"\n#\u8bbe\u7f6ecurl\u8d85\u65f6\u65f6\u95f4\nself.curl_timeout = timeout\nsuper(Refund_pub, self).__init__()", "path": "wzhifuSDK.py", "repo_name": "Skycrab/wzhifuSDK", "stars": 196, "license": "None", "language": "python", "size": 49}
{"docstring": "#\u8bbe\u7f6e\u63a5\u53e3\u94fe\u63a5\n", "func_signal": "def __init__(self, timeout=WxPayConf_pub.CURL_TIMEOUT):\n", "code": "self.url = \"https://api.mch.weixin.qq.com/pay/unifiedorder\"\n#\u8bbe\u7f6ecurl\u8d85\u65f6\u65f6\u95f4\nself.curl_timeout = timeout\nsuper(UnifiedOrder_pub, self).__init__()", "path": "wzhifuSDK.py", "repo_name": "Skycrab/wzhifuSDK", "stars": 196, "license": "None", "language": "python", "size": 49}
{"docstring": "\"\"\"\u751f\u6210\u53ef\u4ee5\u83b7\u5f97openid\u7684url\"\"\"\n", "func_signal": "def createOauthUrlForOpenid(self):\n", "code": "urlObj = {}\nurlObj[\"appid\"] = WxPayConf_pub.APPID\nurlObj[\"secret\"] = WxPayConf_pub.APPSECRET\nurlObj[\"code\"] = self.code\nurlObj[\"grant_type\"] = \"authorization_code\"\nbizString = self.formatBizQueryParaMap(urlObj, False)\nreturn \"https://api.weixin.qq.com/sns/oauth2/access_token?\"+bizString", "path": "wzhifuSDK.py", "repo_name": "Skycrab/wzhifuSDK", "stars": 196, "license": "None", "language": "python", "size": 49}
{"docstring": "\"\"\"\u83b7\u53d6prepay_id\"\"\"\n", "func_signal": "def getShortUrl(self):\n", "code": "self.postXml()\nprepay_id = self.result[\"short_url\"]\nreturn prepay_id", "path": "wzhifuSDK.py", "repo_name": "Skycrab/wzhifuSDK", "stars": 196, "license": "None", "language": "python", "size": 49}
{"docstring": "#\u8bbe\u7f6e\u63a5\u53e3\u94fe\u63a5\n", "func_signal": "def __init__(self, timeout=WxPayConf_pub.CURL_TIMEOUT):\n", "code": "self.url = \"https://api.mch.weixin.qq.com/pay/refundquery\"\n#\u8bbe\u7f6ecurl\u8d85\u65f6\u65f6\u95f4\nself.curl_timeout = timeout\nsuper(RefundQuery_pub, self).__init__()", "path": "wzhifuSDK.py", "repo_name": "Skycrab/wzhifuSDK", "stars": 196, "license": "None", "language": "python", "size": 49}
{"docstring": "# Smoke test of interrelated functionality, using an\n# easy-to-understand document.\n\n# Here it is in Unicode. Note that it claims to be in ISO-Latin-1.\n", "func_signal": "def test_real_iso_latin_document(self):\n", "code": "unicode_html = u'<html><head><meta content=\"text/html; charset=ISO-Latin-1\" http-equiv=\"Content-type\"/></head><body><p>Sacr\\N{LATIN SMALL LETTER E WITH ACUTE} bleu!</p></body></html>'\n\n# That's because we're going to encode it into ISO-Latin-1, and use\n# that to test.\niso_latin_html = unicode_html.encode(\"iso-8859-1\")\n\n# Parse the ISO-Latin-1 HTML.\nsoup = self.soup(iso_latin_html)\n# Encode it to UTF-8.\nresult = soup.encode(\"utf-8\")\n\n# What do we expect the result to look like? Well, it would\n# look like unicode_html, except that the META tag would say\n# UTF-8 instead of ISO-Latin-1.\nexpected = unicode_html.replace(\"ISO-Latin-1\", \"utf-8\")\n\n# And, of course, it would be in UTF-8, not Unicode.\nexpected = expected.encode(\"utf-8\")\n\n# Ta-da!\nself.assertEqual(result, expected)", "path": "beautifulsoup4-4.3.2\\bs4\\testing.py", "repo_name": "SilenceDut/nbaplus-server", "stars": 133, "license": "apache-2.0", "language": "python", "size": 159}
{"docstring": "# html5lib can set the attributes of the same tag many times\n# as it rearranges the tree. This has caused problems with\n# multivalued attributes.\n", "func_signal": "def test_deeply_nested_multivalued_attribute(self):\n", "code": "markup = '<table><div><div class=\"css\"></div></div></table>'\nsoup = self.soup(markup)\nself.assertEqual([\"css\"], soup.div.div['class'])", "path": "beautifulsoup4-4.3.2\\bs4\\testing.py", "repo_name": "SilenceDut/nbaplus-server", "stars": 133, "license": "apache-2.0", "language": "python", "size": 159}
{"docstring": "#print \"Push\", tag.name\n", "func_signal": "def pushTag(self, tag):\n", "code": "if self.currentTag:\n    self.currentTag.contents.append(tag)\nself.tagStack.append(tag)\nself.currentTag = self.tagStack[-1]\nif tag.name in self.builder.preserve_whitespace_tags:\n    self.preserve_whitespace_tag_stack.append(tag)", "path": "beautifulsoup4-4.3.2\\bs4\\__init__.py", "repo_name": "SilenceDut/nbaplus-server", "stars": 133, "license": "apache-2.0", "language": "python", "size": 159}
{"docstring": "\"\"\"A <br> tag is designated as an empty-element tag.\n\nSome parsers treat <br></br> as one <br/> tag, some parsers as\ntwo tags, but it should always be an empty-element tag.\n\"\"\"\n", "func_signal": "def test_br_is_always_empty_element_tag(self):\n", "code": "soup = self.soup(\"<br></br>\")\nself.assertTrue(soup.br.is_empty_element)\nself.assertEqual(str(soup.br), \"<br/>\")", "path": "beautifulsoup4-4.3.2\\bs4\\testing.py", "repo_name": "SilenceDut/nbaplus-server", "stars": 133, "license": "apache-2.0", "language": "python", "size": 159}
{"docstring": "# Both XML and HTML entities are converted to Unicode characters\n# during parsing.\n", "func_signal": "def test_entities_in_strings_converted_during_parsing(self):\n", "code": "text = \"<p>&lt;&lt;sacr&eacute;&#32;bleu!&gt;&gt;</p>\"\nexpected = u\"<p>&lt;&lt;sacr\\N{LATIN SMALL LETTER E WITH ACUTE} bleu!&gt;&gt;</p>\"\nself.assertSoupEquals(text, expected)", "path": "beautifulsoup4-4.3.2\\bs4\\testing.py", "repo_name": "SilenceDut/nbaplus-server", "stars": 133, "license": "apache-2.0", "language": "python", "size": 159}
{"docstring": "\"\"\"Block elements can be nested.\"\"\"\n", "func_signal": "def test_nested_block_level_elements(self):\n", "code": "soup = self.soup('<blockquote><p><b>Foo</b></p></blockquote>')\nblockquote = soup.blockquote\nself.assertEqual(blockquote.p.b.string, 'Foo')\nself.assertEqual(blockquote.b.string, 'Foo')", "path": "beautifulsoup4-4.3.2\\bs4\\testing.py", "repo_name": "SilenceDut/nbaplus-server", "stars": 133, "license": "apache-2.0", "language": "python", "size": 159}
{"docstring": "# Comments are represented as Comment objects.\n", "func_signal": "def test_comment(self):\n", "code": "markup = \"<p>foo<!--foobar-->baz</p>\"\nself.assertSoupEquals(markup)\n\nsoup = self.soup(markup)\ncomment = soup.find(text=\"foobar\")\nself.assertEqual(comment.__class__, Comment)\n\n# The comment is properly integrated into the tree.\nfoo = soup.find(text=\"foo\")\nself.assertEqual(comment, foo.next_element)\nbaz = soup.find(text=\"baz\")\nself.assertEqual(comment, baz.previous_element)", "path": "beautifulsoup4-4.3.2\\bs4\\testing.py", "repo_name": "SilenceDut/nbaplus-server", "stars": 133, "license": "apache-2.0", "language": "python", "size": 159}
{"docstring": "\"\"\"Add an object to the parse tree.\"\"\"\n", "func_signal": "def object_was_parsed(self, o, parent=None, most_recent_element=None):\n", "code": "parent = parent or self.currentTag\nmost_recent_element = most_recent_element or self._most_recent_element\no.setup(parent, most_recent_element)\n\nif most_recent_element is not None:\n    most_recent_element.next_element = o\nself._most_recent_element = o\nparent.contents.append(o)", "path": "beautifulsoup4-4.3.2\\bs4\\__init__.py", "repo_name": "SilenceDut/nbaplus-server", "stars": 133, "license": "apache-2.0", "language": "python", "size": 159}
{"docstring": "\"\"\"Whitespace must be preserved in <pre> and <textarea> tags.\"\"\"\n", "func_signal": "def test_preserved_whitespace_in_pre_and_textarea(self):\n", "code": "self.assertSoupEquals(\"<pre>   </pre>\")\nself.assertSoupEquals(\"<textarea> woo  </textarea>\")", "path": "beautifulsoup4-4.3.2\\bs4\\testing.py", "repo_name": "SilenceDut/nbaplus-server", "stars": 133, "license": "apache-2.0", "language": "python", "size": 159}
{"docstring": "\"\"\"A tag that's not closed by the end of the document should be closed.\n\nThis applies to all tags except empty-element tags.\n\"\"\"\n", "func_signal": "def test_unclosed_tags_get_closed(self):\n", "code": "self.assertSoupEquals(\"<p>\", \"<p></p>\")\nself.assertSoupEquals(\"<b>\", \"<b></b>\")\n\nself.assertSoupEquals(\"<br>\", \"<br/>\")", "path": "beautifulsoup4-4.3.2\\bs4\\testing.py", "repo_name": "SilenceDut/nbaplus-server", "stars": 133, "license": "apache-2.0", "language": "python", "size": 159}
{"docstring": "\"\"\"Generate and parse a document with the given doctype.\"\"\"\n", "func_signal": "def _document_with_doctype(self, doctype_fragment):\n", "code": "doctype = '<!DOCTYPE %s>' % doctype_fragment\nmarkup = doctype + '\\n<p>foo</p>'\nsoup = self.soup(markup)\nreturn doctype, soup", "path": "beautifulsoup4-4.3.2\\bs4\\testing.py", "repo_name": "SilenceDut/nbaplus-server", "stars": 133, "license": "apache-2.0", "language": "python", "size": 159}
{"docstring": "\"\"\"Print out the HTMLParser events that occur during parsing.\n\nThis lets you see how HTMLParser parses a document when no\nBeautiful Soup code is running.\n\"\"\"\n", "func_signal": "def htmlparser_trace(data):\n", "code": "parser = AnnouncingParser()\nparser.feed(data)", "path": "beautifulsoup4-4.3.2\\bs4\\diagnose.py", "repo_name": "SilenceDut/nbaplus-server", "stars": 133, "license": "apache-2.0", "language": "python", "size": 159}
{"docstring": "\"\"\"Parsers don't need to *understand* namespaces, but at the\nvery least they should not choke on namespaces or lose\ndata.\"\"\"\n\n", "func_signal": "def test_basic_namespaces(self):\n", "code": "markup = b'<html xmlns=\"http://www.w3.org/1999/xhtml\" xmlns:mathml=\"http://www.w3.org/1998/Math/MathML\" xmlns:svg=\"http://www.w3.org/2000/svg\"><head></head><body><mathml:msqrt>4</mathml:msqrt><b svg:fill=\"red\"></b></body></html>'\nsoup = self.soup(markup)\nself.assertEqual(markup, soup.encode())\nhtml = soup.html\nself.assertEqual('http://www.w3.org/1999/xhtml', soup.html['xmlns'])\nself.assertEqual(\n    'http://www.w3.org/1998/Math/MathML', soup.html['xmlns:mathml'])\nself.assertEqual(\n    'http://www.w3.org/2000/svg', soup.html['xmlns:svg'])", "path": "beautifulsoup4-4.3.2\\bs4\\testing.py", "repo_name": "SilenceDut/nbaplus-server", "stars": 133, "license": "apache-2.0", "language": "python", "size": 159}
{"docstring": "\"\"\"Print out the lxml events that occur during parsing.\n\nThis lets you see how lxml parses a document when no Beautiful\nSoup code is running.\n\"\"\"\n", "func_signal": "def lxml_trace(data, html=True, **kwargs):\n", "code": "from lxml import etree\nfor event, element in etree.iterparse(StringIO(data), html=html, **kwargs):\n    print(\"%s, %4s, %s\" % (event, element.tag, element.text))", "path": "beautifulsoup4-4.3.2\\bs4\\diagnose.py", "repo_name": "SilenceDut/nbaplus-server", "stars": 133, "license": "apache-2.0", "language": "python", "size": 159}
{"docstring": "# A real-world test to make sure we can convert ISO-8859-9 (a\n# Hebrew encoding) to UTF-8.\n", "func_signal": "def test_real_hebrew_document(self):\n", "code": "hebrew_document = b'<html><head><title>Hebrew (ISO 8859-8) in Visual Directionality</title></head><body><h1>Hebrew (ISO 8859-8) in Visual Directionality</h1>\\xed\\xe5\\xec\\xf9</body></html>'\nsoup = self.soup(\n    hebrew_document, from_encoding=\"iso8859-8\")\nself.assertEqual(soup.original_encoding, 'iso8859-8')\nself.assertEqual(\n    soup.encode('utf-8'),\n    hebrew_document.decode(\"iso8859-8\").encode(\"utf-8\"))", "path": "beautifulsoup4-4.3.2\\bs4\\testing.py", "repo_name": "SilenceDut/nbaplus-server", "stars": 133, "license": "apache-2.0", "language": "python", "size": 159}
{"docstring": "\"\"\"Assert that a given doctype string is handled correctly.\"\"\"\n", "func_signal": "def assertDoctypeHandled(self, doctype_fragment):\n", "code": "doctype_str, soup = self._document_with_doctype(doctype_fragment)\n\n# Make sure a Doctype object was created.\ndoctype = soup.contents[0]\nself.assertEqual(doctype.__class__, Doctype)\nself.assertEqual(doctype, doctype_fragment)\nself.assertEqual(str(soup)[:len(doctype_str)], doctype_str)\n\n# Make sure that the doctype was correctly associated with the\n# parse tree and that the rest of the document parsed.\nself.assertEqual(soup.p.contents[0], 'foo')", "path": "beautifulsoup4-4.3.2\\bs4\\testing.py", "repo_name": "SilenceDut/nbaplus-server", "stars": 133, "license": "apache-2.0", "language": "python", "size": 159}
{"docstring": "# Here's the <meta> tag saying that a document is\n# encoded in Shift-JIS.\n", "func_signal": "def test_html5_style_meta_tag_reflects_current_encoding(self):\n", "code": "meta_tag = ('<meta id=\"encoding\" charset=\"x-sjis\" />')\n\n# Here's a document incorporating that meta tag.\nshift_jis_html = (\n    '<html><head>\\n%s\\n'\n    '<meta http-equiv=\"Content-language\" content=\"ja\"/>'\n    '</head><body>Shift-JIS markup goes here.') % meta_tag\nsoup = self.soup(shift_jis_html)\n\n# Parse the document, and the charset is seemingly unaffected.\nparsed_meta = soup.find('meta', id=\"encoding\")\ncharset = parsed_meta['charset']\nself.assertEqual('x-sjis', charset)\n\n# But that value is actually a CharsetMetaAttributeValue object.\nself.assertTrue(isinstance(charset, CharsetMetaAttributeValue))\n\n# And it will take on a value that reflects its current\n# encoding.\nself.assertEqual('utf8', charset.encode(\"utf8\"))", "path": "beautifulsoup4-4.3.2\\bs4\\testing.py", "repo_name": "SilenceDut/nbaplus-server", "stars": 133, "license": "apache-2.0", "language": "python", "size": 159}
{"docstring": "# Here's the <meta> tag saying that a document is\n# encoded in Shift-JIS.\n", "func_signal": "def test_meta_tag_reflects_current_encoding(self):\n", "code": "meta_tag = ('<meta content=\"text/html; charset=x-sjis\" '\n            'http-equiv=\"Content-type\"/>')\n\n# Here's a document incorporating that meta tag.\nshift_jis_html = (\n    '<html><head>\\n%s\\n'\n    '<meta http-equiv=\"Content-language\" content=\"ja\"/>'\n    '</head><body>Shift-JIS markup goes here.') % meta_tag\nsoup = self.soup(shift_jis_html)\n\n# Parse the document, and the charset is seemingly unaffected.\nparsed_meta = soup.find('meta', {'http-equiv': 'Content-type'})\ncontent = parsed_meta['content']\nself.assertEqual('text/html; charset=x-sjis', content)\n\n# But that value is actually a ContentMetaAttributeValue object.\nself.assertTrue(isinstance(content, ContentMetaAttributeValue))\n\n# And it will take on a value that reflects its current\n# encoding.\nself.assertEqual('text/html; charset=utf8', content.encode(\"utf8\"))\n\n# For the rest of the story, see TestSubstitutions in\n# test_tree.py.", "path": "beautifulsoup4-4.3.2\\bs4\\testing.py", "repo_name": "SilenceDut/nbaplus-server", "stars": 133, "license": "apache-2.0", "language": "python", "size": 159}
{"docstring": "\"\"\"Create a new NavigableString associated with this soup.\"\"\"\n", "func_signal": "def new_string(self, s, subclass=NavigableString):\n", "code": "navigable = subclass(s)\nnavigable.setup()\nreturn navigable", "path": "beautifulsoup4-4.3.2\\bs4\\__init__.py", "repo_name": "SilenceDut/nbaplus-server", "stars": 133, "license": "apache-2.0", "language": "python", "size": 159}
{"docstring": "\"\"\"A <p> tag is never designated as an empty-element tag.\n\nEven if the markup shows it as an empty-element tag, it\nshouldn't be presented that way.\n\"\"\"\n", "func_signal": "def test_p_tag_is_never_empty_element(self):\n", "code": "soup = self.soup(\"<p/>\")\nself.assertFalse(soup.p.is_empty_element)\nself.assertEqual(str(soup.p), \"<p></p>\")", "path": "beautifulsoup4-4.3.2\\bs4\\testing.py", "repo_name": "SilenceDut/nbaplus-server", "stars": 133, "license": "apache-2.0", "language": "python", "size": 159}
{"docstring": "\"\"\"Save a keras model and its weights at the given path.\"\"\"\n\n", "func_signal": "def save_model(model, path):\n", "code": "print(\"Save trained model to {}.\".format(path))\nmodel_path = os.path.join(path, \"model.json\")\nmodel_json = model.to_json()\nwith open(model_path, \"w\") as json_file:\n    json_file.write(model_json)\n\nweights_path = os.path.join(path, \"weights.hdf5\")\nmodel.save_weights(weights_path)", "path": "waterNet\\io_util.py", "repo_name": "treigerm/WaterNet", "stars": 236, "license": "mit", "language": "python", "size": 19867}
{"docstring": "\"\"\"Save a bitmap given as a 2D matrix as a GeoTIFF.\"\"\"\n\n", "func_signal": "def save_bitmap(file_path, image, source):\n", "code": "print(\"Save result at {}.\".format(file_path))\nwith rasterio.open(\n        file_path,\n        'w',\n        driver='GTiff',\n        dtype=rasterio.uint8,\n        count=1,\n        width=source.width,\n        height=source.height,\n        transform=source.transform) as dst:\n    dst.write(image, indexes=1)", "path": "waterNet\\io_util.py", "repo_name": "treigerm/WaterNet", "stars": 236, "license": "mit", "language": "python", "size": 19867}
{"docstring": "\"\"\"Create a PNG with the precision-recall curve for our predictions.\"\"\"\n\n", "func_signal": "def precision_recall_curve(y_true, y_predicted, out_path):\n", "code": "print(\"Calculate precision recall curve.\")\nprecision, recall, thresholds = metrics.precision_recall_curve(y_true,\n                                                               y_predicted)\n\n# Save the raw precision and recall results to a pickle since we might want\n# to analyse them later.\nout_file = os.path.join(out_path, \"precision_recall.pickle\")\nwith open(out_file, \"wb\") as out:\n    pickle.dump({\n        \"precision\": precision,\n        \"recall\": recall,\n        \"thresholds\": thresholds\n    }, out)\n\n# Create the precision-recall curve.\nout_file = os.path.join(out_path, \"precision_recall.png\")\nplt.clf()\nplt.plot(recall, precision, label=\"Precision-Recall curve\")\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.savefig(out_file)", "path": "waterNet\\evaluation.py", "repo_name": "treigerm/WaterNet", "stars": 236, "license": "mit", "language": "python", "size": 19867}
{"docstring": "\"\"\"Create all the directories in the /data directories which are used for preprocessing/training/evaluating.\"\"\"\n\n", "func_signal": "def create_directories():\n", "code": "directories = [TILES_DIR, WATER_BITMAPS_DIR, WGS84_DIR, LABELS_DIR, MODELS_DIR, OUTPUT_DIR, TENSORBOARD_DIR]\nfor directory in directories:\n    save_makedirs(directory)", "path": "waterNet\\io_util.py", "repo_name": "treigerm/WaterNet", "stars": 236, "license": "mit", "language": "python", "size": 19867}
{"docstring": "\"\"\"Get false positives for the given predictions and labels.\"\"\"\n\n", "func_signal": "def get_false_positives(predictions, labels):\n", "code": "FP = np.logical_and(predictions == 1, labels == 0)\nfalse_positives = np.copy(predictions)\nfalse_positives[FP] = 1\nfalse_positives[np.logical_not(FP)] = 0\nreturn false_positives", "path": "waterNet\\evaluation.py", "repo_name": "treigerm/WaterNet", "stars": 236, "license": "mit", "language": "python", "size": 19867}
{"docstring": "\"\"\"Create the bitmap for a given satellite image.\"\"\"\n\n", "func_signal": "def create_bitmap(raster_dataset, shapefile_paths, satellite_path):\n", "code": "satellite_img_name = get_file_name(satellite_path)\ncache_file_name = \"{}_water.tif\".format(satellite_img_name)\ncache_path = os.path.join(WATER_BITMAPS_DIR, cache_file_name)\ntry:\n    # Try loading the water bitmap from cache.\n    print(\"Load water bitmap from {}\".format(cache_path))\n    bitmap = load_bitmap(cache_path)\n    bitmap[bitmap == 255] = 1\n    return bitmap\nexcept IOError as e:\n    print(\"No cache file found.\")\n\nwater_features = np.empty((0, ))\n\nprint(\"Create bitmap for water features.\")\nfor shapefile_path in shapefile_paths:\n    try:\n        print(\"Load shapefile {}.\".format(shapefile_path))\n        with fiona.open(shapefile_path) as shapefile:\n            # Each feature in the shapefile also contains meta information such as\n            # wether the features is a lake or a river. We only care about the geometry\n            # of the feature i.e. where it is located and what shape it has.\n            geometries = [feature['geometry'] for feature in shapefile]\n\n            water_features = np.concatenate(\n                (water_features, geometries), axis=0)\n    except IOError as e:\n        print(\"No shapefile found.\")\n        sys.exit(1)\n\n# Now that we have the vector data of all water features in our satellite image\n# we \"burn it\" into a new raster so that we get a B/W image with water features\n# in white and the rest in black. We choose the value 255 so that there is a stark\n# contrast between water and non-water pixels. This is only for visualisation\n# purposes. For the classifier we use 0s and 1s.\nbitmap_image = rasterio.features.rasterize(\n    ((g, 255) for g in water_features),\n    out_shape=raster_dataset.shape,\n    transform=raster_dataset.transform)\n\nsave_bitmap(cache_path, bitmap_image, raster_dataset)\n\nbitmap_image[bitmap_image == 255] = 1\nreturn bitmap_image", "path": "waterNet\\preprocessing.py", "repo_name": "treigerm/WaterNet", "stars": 236, "license": "mit", "language": "python", "size": 19867}
{"docstring": "\"\"\"Create a new GeoTIFF image which overlays the predictions of the model.\"\"\"\n\n", "func_signal": "def visualise_predictions(predictions, labels, false_positives, tile_size, out_path, out_format=\"GeoTIFF\"):\n", "code": "print(\"Create {} result files.\".format(out_format))\npredictions = np.reshape(predictions,\n                         (len(labels), tile_size, tile_size, 1))\nfalse_positives = np.reshape(false_positives,\n                         (len(labels), tile_size, tile_size, 1))\n\nresults = []\n# We want to overlay the predictions and false positives on a GeoTIFF but we don't\n# have any information about the source image and the position in the source for each\n# tile in the predictions and false postives. We get this information from the labels.\nfor i, (_, position, path_to_geotiff) in enumerate(labels):\n    prediction_tile = predictions[i, :, :, :]\n    false_positivle_tile = false_positives[i, :, :, :]\n    label_tile = labels[i][0]\n    results.append(\n        ((prediction_tile, label_tile, false_positivle_tile), position, path_to_geotiff))\n\nvisualise_results(results, tile_size, out_path, out_format=out_format)", "path": "waterNet\\evaluation.py", "repo_name": "treigerm/WaterNet", "stars": 236, "license": "mit", "language": "python", "size": 19867}
{"docstring": "\"\"\"Load a GeoTIFF which is a bitmap of our water and non-water features.\"\"\"\n", "func_signal": "def load_bitmap(file_path):\n", "code": "with rasterio.open(file_path) as src:\n    bitmap = src.read(1)\n\nreturn bitmap", "path": "waterNet\\io_util.py", "repo_name": "treigerm/WaterNet", "stars": 236, "license": "mit", "language": "python", "size": 19867}
{"docstring": "\"\"\"Transform a list of triples of features and labels. To a matrix which contains\nonly the tiles used for training the model.\"\"\"\n", "func_signal": "def get_matrix_form(features, labels, tile_size):\n", "code": "features = [tile for tile, position, path in features]\nlabels = [tile for tile, position, path in labels]\n\n# The model will have one output corresponding to each pixel in the feature tile.\n# So we need to transform the labels which are given as a 2D bitmap into a vector.\nlabels = np.reshape(labels, (len(labels), tile_size * tile_size))\nreturn np.array(features), np.array(labels)", "path": "waterNet\\model.py", "repo_name": "treigerm/WaterNet", "stars": 236, "license": "mit", "language": "python", "size": 19867}
{"docstring": "\"\"\"Create features and labels for a given dataset. The features are tiles which contain\nthe three RGB bands of the satellite image, so they have the form (tile_size, tile_size, 3).\nLabels are bitmaps with 1 indicating that the corresponding pixel in the satellite image\nrepresents water.\"\"\"\n\n", "func_signal": "def preprocess_data(tile_size, dataset, only_cache=False):\n", "code": "print('_' * 100)\nprint(\"Start preprocessing data.\")\n\nfeatures_train, labels_train = extract_features_and_labels(\n    dataset[\"train\"], tile_size, only_cache)\nfeatures_test, labels_test = extract_features_and_labels(\n    dataset[\"test\"], tile_size, only_cache)\n\nreturn features_train, features_test, labels_train, labels_test", "path": "waterNet\\preprocessing.py", "repo_name": "treigerm/WaterNet", "stars": 236, "license": "mit", "language": "python", "size": 19867}
{"docstring": "\"\"\"Normalise the features such that all values are in the range [0,1].\"\"\"\n", "func_signal": "def normalise_input(features):\n", "code": "features = features.astype(np.float32)\n\nreturn np.multiply(features, 1.0 / 255.0)", "path": "waterNet\\model.py", "repo_name": "treigerm/WaterNet", "stars": 236, "license": "mit", "language": "python", "size": 19867}
{"docstring": "\"\"\"Remove tiles which are on the edge of the satellite image and which contain blacked out\ncontent.\"\"\"\n\n", "func_signal": "def remove_edge_tiles(tiled_bands, tiled_bitmap, tile_size, source_shape):\n", "code": "EDGE_BUFFER = 350\n\nrows, cols = source_shape[0], source_shape[1]\n\nbands = []\nbitmap = []\nfor i, (tile, (row, col), _) in enumerate(tiled_bands):\n    is_in_center = EDGE_BUFFER <= row and row <= (\n        rows - EDGE_BUFFER) and EDGE_BUFFER <= col and col <= (\n            cols - EDGE_BUFFER)\n    # Checks wether our tile contains a pixel which is only black.\n    # This might also delete tiles which contain a natural feature which is\n    # totally black but these are only a small number of tiles and we don't\n    # care about deleting them as well.\n    contains_black_pixel = [0, 0, 0] in tile\n    is_edge_tile = contains_black_pixel and not is_in_center\n    if not is_edge_tile:\n        bands.append(tiled_bands[i])\n        bitmap.append(tiled_bitmap[i])\n\nreturn bands, bitmap", "path": "waterNet\\preprocessing.py", "repo_name": "treigerm/WaterNet", "stars": 236, "license": "mit", "language": "python", "size": 19867}
{"docstring": "\"\"\"Calculate several metrics for the model and create a visualisation of the test dataset.\"\"\"\n\n", "func_signal": "def evaluate_model(model, features, labels, tile_size, out_path, out_format=\"GeoTIFF\"):\n", "code": "print('_' * 100)\nprint(\"Start evaluating model.\")\n\nX, y_true = get_matrix_form(features, labels, tile_size)\n\ny_predicted = model.predict(X)\npredicted_bitmap = np.array(y_predicted)\n\n# Since the model only outputs probabilites for each pixel we have\n# to transform them into 0s and 1s. For the sake of simplicity we\n# simply use a cut off value of 0.5.\npredicted_bitmap[0.5 <= predicted_bitmap] = 1\npredicted_bitmap[predicted_bitmap < 0.5] = 0\n\nfalse_positives = get_false_positives(predicted_bitmap, y_true)\nvisualise_predictions(predicted_bitmap, labels, false_positives, tile_size, out_path, out_format=out_format)\n\n# We have to flatten our predictions and labels since by default the metrics are calculated by\n# comparing the elements in the list of labels and predictions elemtwise. So if we would not flatten\n# our results we would only get a true positive if we would predict every pixel in an entire tile right.\n# But we obviously only care about each pixel individually.\ny_true = y_true.flatten()\ny_predicted = y_predicted.flatten()\npredicted_bitmap = predicted_bitmap.flatten()\n\nprint(\"Accuracy on test set: {}\".format(metrics.accuracy_score(y_true, predicted_bitmap)))\nprint(\"Precision on test set: {}\".format(metrics.precision_score(y_true, predicted_bitmap)))\nprint(\"Recall on test set: {}\".format(metrics.recall_score(y_true, predicted_bitmap)))\nprecision_recall_curve(y_true, y_predicted, out_path)", "path": "waterNet\\evaluation.py", "repo_name": "treigerm/WaterNet", "stars": 236, "license": "mit", "language": "python", "size": 19867}
{"docstring": "\"\"\"Extract the file name without the file extension from a file path.\"\"\"\n\n", "func_signal": "def get_file_name(file_path):\n", "code": "basename = os.path.basename(file_path)\n# Make sure we don't include the file extension.\nreturn os.path.splitext(basename)[0]", "path": "waterNet\\io_util.py", "repo_name": "treigerm/WaterNet", "stars": 236, "license": "mit", "language": "python", "size": 19867}
{"docstring": "\"\"\"Load a keras model and its weights with the given ID.\"\"\"\n\n", "func_signal": "def load_model(model_id):\n", "code": "model_dir = os.path.join(MODELS_DIR, model_id)\n\nprint(\"Load model in {}.\".format(model_dir))\nmodel_file = os.path.join(model_dir, \"model.json\")\nwith open(model_file, \"r\") as f:\n    json_file = f.read()\n    model = keras.models.model_from_json(json_file)\n\nweights_file = os.path.join(model_dir, \"weights.hdf5\")\nmodel.load_weights(weights_file)\n\nreturn model", "path": "waterNet\\io_util.py", "repo_name": "treigerm/WaterNet", "stars": 236, "license": "mit", "language": "python", "size": 19867}
{"docstring": "\"\"\"Save the tile data for a satellite image as a pickle.\"\"\"\n\n", "func_signal": "def save_tiles(file_path, tiled_features, tiled_labels):\n", "code": "print(\"Store tile data at {}.\".format(file_path))\nwith open(file_path, \"wb\") as out:\n    pickle.dump({\"features\": tiled_features, \"labels\": tiled_labels}, out)", "path": "waterNet\\io_util.py", "repo_name": "treigerm/WaterNet", "stars": 236, "license": "mit", "language": "python", "size": 19867}
{"docstring": "\"\"\"Compile the keras model with the given hyperparameters.\"\"\"\n\n", "func_signal": "def compile_model(model, learning_rate, momentum, decay):\n", "code": "optimizer = SGD(lr=learning_rate, momentum=momentum, decay=decay)\n\nmodel.compile(\n    loss='categorical_crossentropy',\n    optimizer=optimizer,\n    metrics=['accuracy'])\n\nreturn model", "path": "waterNet\\model.py", "repo_name": "treigerm/WaterNet", "stars": 236, "license": "mit", "language": "python", "size": 19867}
{"docstring": "\"\"\"Create a directory and don't throw an exception if the\ndirectory doesn't exist yet.\nSee http://stackoverflow.com/questions/273192/how-to-check-if-a-directory-exists-and-create-it-if-necessary\"\"\"\n\n", "func_signal": "def save_makedirs(path):\n", "code": "try:\n    os.makedirs(path)\nexcept OSError as exception:\n    if exception.errno != errno.EEXIST:\n        raise", "path": "waterNet\\io_util.py", "repo_name": "treigerm/WaterNet", "stars": 236, "license": "mit", "language": "python", "size": 19867}
{"docstring": "\"\"\"Save the hyperparameters of a model and the model summary generated by keras to a .txt file.\"\"\"\n\n", "func_signal": "def save_model_summary(hyperparameters, model, path):\n", "code": "with open(os.path.join(path, \"hyperparameters.txt\"), \"wb\") as out:\n    for parameter, value in hyperparameters:\n        out.write(\"{}: {}\\n\".format(parameter, value))\n\n    # model.summary() prints to stdout. Because we want to write the\n    # summary to a file we have to set the stdout to the file.\n    stdout = sys.stdout\n    sys.stdout = out\n    model.summary()\n    sys.stdout = stdout", "path": "waterNet\\io_util.py", "repo_name": "treigerm/WaterNet", "stars": 236, "license": "mit", "language": "python", "size": 19867}
{"docstring": "\"\"\"For each satellite image and its corresponding shapefiles in the dataset create\ntiled features and labels.\"\"\"\n", "func_signal": "def extract_features_and_labels(dataset, tile_size, only_cache=False):\n", "code": "features = []\nlabels = []\n\nfor geotiff_path, shapefile_paths in dataset:\n    tiled_features, tiled_labels = create_tiled_features_and_labels(\n        geotiff_path, shapefile_paths, tile_size, only_cache)\n\n    features += tiled_features\n    labels += tiled_labels\n\nreturn features, labels", "path": "waterNet\\preprocessing.py", "repo_name": "treigerm/WaterNet", "stars": 236, "license": "mit", "language": "python", "size": 19867}
{"docstring": "# TODO Figure out what \"new_layers\" means in wsj model\n", "func_signal": "def get_brnn_model_file():\n", "code": "model_file = pjoin(MODEL_DIR, '%s_%d_%d_bitemporal_%d_step_1e-5_mom_.95_anneal_%.1f.bin' % (DATASET, NUM_LAYERS, LAYER_SIZE, TEMPORAL_LAYER, ANNEAL))\nassert os.path.exists(model_file)\nreturn model_file", "path": "ctc_fast\\decoder\\decoder_config.py", "repo_name": "amaas/stanford-ctc", "stars": 245, "license": "apache-2.0", "language": "python", "size": 269}
{"docstring": "\"\"\"\nRuns stochastic gradient descent with model as objective.\nUses single utterances instead of minibatches\n\"\"\"\n\n# momentum setup\n", "func_signal": "def run_seq(self,data_dict,alis,keys,sizes):\n", "code": "momIncrease = 100\nmom = 0.5", "path": "ctc\\sgd.py", "repo_name": "amaas/stanford-ctc", "stars": 245, "license": "apache-2.0", "language": "python", "size": 269}
{"docstring": "'''\nRead in data\n'''\n\n# NOTE Make sure synced with order dumped in runDecode.py\n", "func_signal": "def main(args):\n", "code": "fid = open(args.pk_file, 'rb')\nhyps = np.array(pickle.load(fid))\nrefs = np.array(pickle.load(fid))\nhypscores = np.array(pickle.load(fid))\nrefscores = np.array(pickle.load(fid))\nnumphones = np.array(pickle.load(fid))\nsubsets = pickle.load(fid)\nfid.close()\n\ncompute_and_display_stats(hyps, refs, hypscores, refscores, numphones, subsets, subset=None, display=args.display)", "path": "ctc_fast\\swbd-utils\\errorAnalysis.py", "repo_name": "amaas/stanford-ctc", "stars": 245, "license": "apache-2.0", "language": "python", "size": 269}
{"docstring": "\"\"\"\nSaves only the network parameters to the given fd.\n\"\"\"\n", "func_signal": "def toFile(self,fid):\n", "code": "import cPickle as pickle\npickle.dump(self.stack,fid)", "path": "ctc\\nnet.py", "repo_name": "amaas/stanford-ctc", "stars": 245, "license": "apache-2.0", "language": "python", "size": 269}
{"docstring": "# Working in log space\n", "func_signal": "def compute_pp(text, lm, order):\n", "code": "pp = 0.0\n\nn = 0\nfor utt in text:\n    # Skip p(<s> | []) = -inf\n    for k in range(1, len(utt)):\n        c = utt[k]\n        seq = utt[max(0, k-order+1):k]\n        #print c, seq\n        # Work w/ reversed context due to srilm\n        seq = seq[::-1]\n        pp = pp + -1 * lm.logprob_strings(c, seq)\n        n += 1\n\npp = pp / n\n\n# NOTE Assumes probs given back in log10\npp = 10 ** pp\n\nreturn pp", "path": "ctc_fast\\clm\\clm_pp.py", "repo_name": "amaas/stanford-ctc", "stars": 245, "license": "apache-2.0", "language": "python", "size": 269}
{"docstring": "# Initialize cublas\n", "func_signal": "def __init__(self,inputDim,outputDim,layerSize,numLayers,maxBatch,train=True):\n", "code": "cm.cublas_init()\n\nself.outputDim = outputDim\nself.inputDim = inputDim\nself.layerSizes = [layerSize]*numLayers\nself.maxBatch = maxBatch\nself.train = train", "path": "ctc_fast\\nnets\\nnet.py", "repo_name": "amaas/stanford-ctc", "stars": 245, "license": "apache-2.0", "language": "python", "size": 269}
{"docstring": "\"\"\"\nLoops through node list, returns on first encounter with free gpu.\nReturns node name and device id if free,\n-1,-1 otherwise.\n\"\"\"\n", "func_signal": "def get_next_free():\n", "code": "for n in nodes:\n    free_gpus = get_free_gpus(gorgon+str(n))\n    if free_gpus:\n        return n,free_gpus[0]\nreturn -1,-1", "path": "ctc_fast\\runAll.py", "repo_name": "amaas/stanford-ctc", "stars": 245, "license": "apache-2.0", "language": "python", "size": 269}
{"docstring": "\"\"\"\nThis is to do parameter updates in place. Performs the same whether RNN or not\n\"\"\"\n", "func_signal": "def updateParams(self,scale, update):\n", "code": "self.stack = [[ws[0]+scale*wsDelta[0],ws[1]+scale*wsDelta[1]] \n                for ws,wsDelta in zip(self.stack,update)]", "path": "ctc\\rnnet.py", "repo_name": "amaas/stanford-ctc", "stars": 245, "license": "apache-2.0", "language": "python", "size": 269}
{"docstring": "\"\"\"\nRuns stochastic gradient descent with model as objective.  Expects\ndata in n x m matrix where n is feature dimension and m is number of\ntraining examples\n\"\"\"\n", "func_signal": "def run(self,data,labels=None):\n", "code": "m = data.shape[1]\n\n# momentum setup\nmomIncrease = 10\nmom = 0.5\n\n# randomly select minibatch\nperm = np.random.permutation(range(m))\n\nfor i in xrange(0,m-self.minibatch+1,self.minibatch):\n    self.it += 1\n\n    mb_data = data[:,perm[i:i+self.minibatch]]\n    mb_data = gp.garray(mb_data)", "path": "py-simple-hybrid\\sgd.py", "repo_name": "amaas/stanford-ctc", "stars": 245, "license": "apache-2.0", "language": "python", "size": 269}
{"docstring": "\"\"\"\nForward prop entire utterance\nCall CTC cost function\nCompute gradient\n\ndata is a 2-D matrix where each column is a single time frame\nNumber of input frames changes across iterations\n\nlabels is a vector of symbol ids, length unknown and does not\ndepend on the number of time frames\n\"\"\"\n\n## forward prop\n", "func_signal": "def costAndGrad(self,data,labels,key=None):\n", "code": "T = data.shape[1]\nsizes = [self.inputDim]+self.layerSizes+[self.outputDim]\nstackMax = len(self.stack)-1\nif self.temporalLayer > 0:\n    stackMax -= 1\n\nself.hActs = [gp.empty((s,T)) for s in sizes]\nself.hActs[0] = data\n#for t in range(T):\ni = 1\nfor l in range(stackMax+1):\n    w,b = self.stack[l]\n\n    self.hActs[i] = w.dot(self.hActs[i-1]) + b\n    # loop over time for recurrent layer\n    if (self.temporalLayer-1) == l:\n        for t in range(T):\n            if t > 0:\n                self.hActs[i][:,t] += self.stack[-1][0].dot(self.hActs[i][:,t-1])\n            # nonlinearity \n            if i <= stackMax:\n                self.hActs[i][:,t] = self.activation(self.hActs[i][:,t])\n    # hidden layer activation function for batch forward prop\n    elif i <= stackMax:\n        self.hActs[i] = self.activation(self.hActs[i])\n\n    #    w_t,b_t = self.stack[-1][0]\n    #    self.hActs[i][:,t] += self.stack[-1][0].dot(self.hActs[i][:,t-1])\n    i += 1\n\n# convert final layer to probs after all time iteration complete\nprobs = self.hActs[-1]-gp.max(self.hActs[-1],axis=0)", "path": "ctc\\rnnet.py", "repo_name": "amaas/stanford-ctc", "stars": 245, "license": "apache-2.0", "language": "python", "size": 269}
{"docstring": "\"\"\"\nSets view of gpu memory to be correct size for utterance.\n\"\"\"\n", "func_signal": "def setViews(self,batchSize):\n", "code": "assert batchSize <= self.maxBatch, \"Batch size exceeds max batch\"\nself.hActs = [H.get_col_slice(0,batchSize) for H in self.hActs_M]\nself.hActsFor = self.hActsFor_M.get_col_slice(0,batchSize)\nself.hActsBack = self.hActsBack_M.get_col_slice(0,batchSize)\nself.probs = self.probs_M.get_col_slice(0,batchSize)\nself.rowVec = self.rowVec_M.get_col_slice(0,batchSize)\nif self.train:\n    self.deltasC = self.deltasC_M.get_col_slice(0,batchSize)\n    self.deltasOut = self.deltasOut_M.get_col_slice(0,batchSize)\n    self.deltasIn = self.deltasIn_M.get_col_slice(0,batchSize)\n    self.tmpGrad = self.tmpGrad_M.get_col_slice(0,batchSize)\n    if self.temporalLayer > 0:\n        self.tmpGradFor = self.tmpGrad # reuse tmpGrad mem\n        self.tmpGradBack = self.tmpGradBack_M.get_col_slice(0,batchSize)\n        self.deltasFor = self.deltasFor_M.get_col_slice(0,batchSize)\n        self.deltasBack = self.deltasBack_M.get_col_slice(0,batchSize)", "path": "ctc_fast\\nnets\\brnnet.py", "repo_name": "amaas/stanford-ctc", "stars": 245, "license": "apache-2.0", "language": "python", "size": 269}
{"docstring": "\"\"\"\nSaves only the network parameters to the given fd.\n\"\"\"\n", "func_signal": "def toFile(self,fid):\n", "code": "import cPickle as pickle\npickle.dump(self.stack,fid)", "path": "ctc\\rnnet.py", "repo_name": "amaas/stanford-ctc", "stars": 245, "license": "apache-2.0", "language": "python", "size": 269}
{"docstring": "\"\"\"\nSets view of gpu memory to be correct size for utterance.\n\"\"\"\n", "func_signal": "def setViews(self,batchSize):\n", "code": "assert batchSize <= self.maxBatch, \"Batch size exceeds max batch\"\nself.hActs = [H.get_col_slice(0,batchSize) for H in self.hActs_M]\nself.deltasC = self.deltasC_M.get_col_slice(0,batchSize)\nself.deltasOut = self.deltasOut_M.get_col_slice(0,batchSize)\nself.deltasIn = self.deltasIn_M.get_col_slice(0,batchSize)\nself.probs = self.probs_M.get_col_slice(0,batchSize)\nself.rowVec = self.rowVec_M.get_col_slice(0,batchSize)\nself.tmpGrad = self.tmpGrad_M.get_col_slice(0,batchSize)", "path": "ctc_fast\\nnets\\nnet.py", "repo_name": "amaas/stanford-ctc", "stars": 245, "license": "apache-2.0", "language": "python", "size": 269}
{"docstring": "\"\"\"\nVectorized version of CTC cost\ndata is a single utterance. Each column is a time index [0...T]\n\"\"\"\n", "func_signal": "def costAndGradVec(self,params,data,labels):\n", "code": "self.vecToStack(params)\ncost,grad = self.costAndGrad(data,labels)\nif (grad != None):\n    vecgrad = self.vectorize(grad)\nreturn cost,vecgrad", "path": "ctc\\rnnet.py", "repo_name": "amaas/stanford-ctc", "stars": 245, "license": "apache-2.0", "language": "python", "size": 269}
{"docstring": "\"\"\"\nConverts a stack object into a single parameter vector\nx is a stack\nreturns a single numpy array\nXXX or does this return a list of lists?\n\"\"\"\n", "func_signal": "def vectorize(self, x):\n", "code": "return [v for layer in x for wb in layer for w_or_b in wb for v in w_or_b]\n\n# r = []\n# for l in x:\n#     for wb in l:\n#         for w_or_b in wb:\n#             r.extend(w_or_b.reshape(-1).tolist())\n\n\n#r = [(v for v in w_or_b) if isinstance(w_or_b, np.ndarray) else w_or_b for layer in x for wb in layer for w_or_b in wb]\nreturn r\n#return [v for layer in x for wb in layer for w_or_b in wb for v in w_or_b]", "path": "ctc\\rnnet.py", "repo_name": "amaas/stanford-ctc", "stars": 245, "license": "apache-2.0", "language": "python", "size": 269}
{"docstring": "\"\"\"\nForward prop entire utterance\nCall CTC cost function\nCompute gradient\n\ndata is a 2-D matrix where each column is a single time frame\nNumber of input frames changes across iterations\n\nlabels is a vector of symbol ids, length unknown and does not\ndepend on the number of time frames\n\"\"\"\n\n## forward prop\n# this is the same as minibatch forward prop \n# since we pre-compute context window features for each time\n", "func_signal": "def costAndGrad(self,data,labels=None,key=None):\n", "code": "self.hActs[0] = data\ni = 1\nfor w,b in self.stack:\n    self.hActs[i] = w.dot(self.hActs[i-1])+b\n    if i <= len(self.layerSizes):\n        self.hActs[i] = self.activation(self.hActs[i])\n    i += 1\n\nprobs = self.hActs[-1]-gp.max(self.hActs[-1],axis=0)", "path": "ctc\\nnet.py", "repo_name": "amaas/stanford-ctc", "stars": 245, "license": "apache-2.0", "language": "python", "size": 269}
{"docstring": "\"\"\"\nWrites header for each utterance in Kaldi Style, assumes \ndata written in float32 and C order. \n\"\"\"\n", "func_signal": "def writeUttHeader(fid,key,uttSize,numClasses):\n", "code": "fid.write(key+' ') # write key\n# kaldi specific dat\ndat = struct.pack('b',0)\nfid.write(dat)\nfid.write('BFM ')\nfloatsize = struct.pack('b',4)\nfid.write(floatsize)\n# num rows (utterance size)\ndat = struct.pack('i',uttSize)\nfid.write(dat)\nfid.write(floatsize)\n# num cols (num classes)\ndat = struct.pack('i',numClasses)\nfid.write(dat)", "path": "ctc_fast\\analysis-utils\\writeLikelihoods.py", "repo_name": "amaas/stanford-ctc", "stars": 245, "license": "apache-2.0", "language": "python", "size": 269}
{"docstring": "\"\"\"\nVectorized version of CTC cost \n\"\"\"\n", "func_signal": "def costAndGradVec(self,params,data,labels):\n", "code": "self.vecToStack(params)\ncost,grad = self.costAndGrad(data,labels)\nif (grad != None):\n    vecgrad = self.vectorize(grad)\nreturn cost,vecgrad", "path": "ctc\\nnet.py", "repo_name": "amaas/stanford-ctc", "stars": 245, "license": "apache-2.0", "language": "python", "size": 269}
{"docstring": "\"\"\"\nLoads a data file but stores input frames in a dictionary keyed by utterance\nEach input dictionary entry is a 2-D matrix of length equal to that utterance\nOther variables returned are the same as the original loader\n\"\"\"\n", "func_signal": "def loadDataFileDict(self,filenum):\n", "code": "data_mat, alis, keys, sizes = self.loadDataFile(filenum)\ndata_dict = {}\nstartInd = 0\nfor k,s in izip(keys,sizes):\n    endInd = startInd + s\n    data_dict[k] = np.copy(data_mat[:,startInd:endInd])\n    startInd = endInd\n\n# startInd = all frames means we loaded all data\nassert startInd, data_mat.shape[1]\n\nreturn data_dict, alis, keys, sizes", "path": "ctc\\dataLoader.py", "repo_name": "amaas/stanford-ctc", "stars": 245, "license": "apache-2.0", "language": "python", "size": 269}
{"docstring": "# setup decoder\n", "func_signal": "def decode_utterance_clm(k, probs, labels, charmap_file, lm_file):\n", "code": "dec_lm = decoder.BeamLMDecoder()\ndec_lm.load_chars(charmap_file)\ndec_lm.load_lm(lm_file)\n\nhyp, hypscore = dec_lm.decode(probs.astype(np.double))\n\n# return (hyp, ref, hypscore, truescore)\nreturn hyp, None, hypscore, None", "path": "ctc_fast\\runDecode_new.py", "repo_name": "amaas/stanford-ctc", "stars": 245, "license": "apache-2.0", "language": "python", "size": 269}
{"docstring": "\"\"\"Decorator for memoizing a function.\"\"\"\n", "func_signal": "def memoize(func):\n", "code": "cache = {}\n@wraps(func)\ndef wrap(*args):\n    if args not in cache:\n        cache[args] = func(*args)\n    return cache[args]\nreturn wrap", "path": "galry\\tools.py", "repo_name": "rossant/galry", "stars": 189, "license": "other", "language": "python", "size": 9023}
{"docstring": "# record all visual variables in the shader creator\n", "func_signal": "def set_variables(self, **kwargs):\n", "code": "for key, value in kwargs.iteritems():\n    setattr(self, key, value)", "path": "galry\\visuals\\visual.py", "repo_name": "rossant/galry", "stars": 189, "license": "other", "language": "python", "size": 9023}
{"docstring": "\"\"\"Return the GLSL uniform declaration.\"\"\"\n", "func_signal": "def get_uniform_declaration(uniform):\n", "code": "tab = \"\"\nsize = uniform.get(\"size\", None)\nif size is not None:\n    tab = \"[%d]\" % max(1, size)  # ensure that the size is always >= 1\n# add uniform declaration\ndeclaration = \"uniform %s %s%s;\\n\" % \\\n    (_get_shader_type(uniform),\n     uniform[\"name\"],\n     tab)\nreturn declaration", "path": "galry\\visuals\\visual.py", "repo_name": "rossant/galry", "stars": 189, "license": "other", "language": "python", "size": 9023}
{"docstring": "\"\"\"Return the GLSL texture declaration.\"\"\"\n", "func_signal": "def get_texture_declaration(texture):\n", "code": "declaration = \"uniform sampler%dD %s;\\n\" % (texture[\"ndim\"], texture[\"name\"])\nreturn declaration", "path": "galry\\visuals\\visual.py", "repo_name": "rossant/galry", "stars": 189, "license": "other", "language": "python", "size": 9023}
{"docstring": "\"\"\"\nconvert hsv values in a numpy array to rgb values\nboth input and output arrays have shape (M,N,3)\n\"\"\"\n", "func_signal": "def hsv_to_rgb(hsv):\n", "code": "h = hsv[:, :, 0]\ns = hsv[:, :, 1]\nv = hsv[:, :, 2]\n\nr = np.empty_like(h)\ng = np.empty_like(h)\nb = np.empty_like(h)\n\ni = (h * 6.0).astype(np.int)\nf = (h * 6.0) - i\np = v * (1.0 - s)\nq = v * (1.0 - s * f)\nt = v * (1.0 - s * (1.0 - f))\n\nidx = i % 6 == 0\nr[idx] = v[idx]\ng[idx] = t[idx]\nb[idx] = p[idx]\n\nidx = i == 1\nr[idx] = q[idx]\ng[idx] = v[idx]\nb[idx] = p[idx]\n\nidx = i == 2\nr[idx] = p[idx]\ng[idx] = v[idx]\nb[idx] = t[idx]\n\nidx = i == 3\nr[idx] = p[idx]\ng[idx] = q[idx]\nb[idx] = v[idx]\n\nidx = i == 4\nr[idx] = t[idx]\ng[idx] = p[idx]\nb[idx] = v[idx]\n\nidx = i == 5\nr[idx] = v[idx]\ng[idx] = p[idx]\nb[idx] = q[idx]\n\nidx = s == 0\nr[idx] = v[idx]\ng[idx] = v[idx]\nb[idx] = v[idx]\n\nrgb = np.empty_like(hsv)\nrgb[:, :, 0] = r\nrgb[:, :, 1] = g\nrgb[:, :, 2] = b\nreturn rgb", "path": "galry\\tools.py", "repo_name": "rossant/galry", "stars": 189, "license": "other", "language": "python", "size": 9023}
{"docstring": "\"\"\"Return the name of the GL function used to update the uniform data.\n\nArguments:\n  * varinfo: the information dictionary about the variable.\n\nReturns:\n  * funname: the name of the OpenGL function.\n  * args: the tuple of arguments to this function. The data must be \n    appended to this tuple.\n\n\"\"\"\n# NOTE: varinfo == dict(vartype=vartype, ndim=ndim, size=size)\n", "func_signal": "def _get_uniform_function_name(varinfo):\n", "code": "float_suffix = {True: 'f', False: 'i'}\narray_suffix = {True: 'v', False: ''}\n    \nvartype = varinfo[\"vartype\"]\nndim = varinfo[\"ndim\"]\nsize = varinfo.get(\"size\", None)\nargs = ()\n\n# scalar or vector uniform\nif type(ndim) == int or type(ndim) == long:\n    # find function name\n    funname = \"glUniform%d%s%s\" % (ndim, \\\n                                   float_suffix[vartype == \"float\"], \\\n                                   array_suffix[size is not None])\n\n    # find function arguments\n    if size is not None:\n        args += (size,)\n        \n# matrix uniform\nelif type(ndim) == tuple:\n    # find function name\n    funname = \"glUniformMatrix%dfv\" % (ndim[0])\n    args += (1, False,)\n    \nreturn funname, args", "path": "galry\\visuals\\visual.py", "repo_name": "rossant/galry", "stars": 189, "license": "other", "language": "python", "size": 9023}
{"docstring": "\"\"\"Update incomplete varinfo dict from data.\n\nArguments:\n  * varinfo: a potentially incomplete variable information dictionary.\n  * data: the associated data, used to complete the information.\n  \nReturns:\n  * varinfo: the completed information dictionary.\n  \n\"\"\"\n", "func_signal": "def _update_varinfo(varinfo, data):\n", "code": "varinfo_data = _get_varinfo(data)\nif \"vartype\" not in varinfo:\n    varinfo.update(vartype=varinfo_data['vartype'])\nif \"ndim\" not in varinfo:\n    varinfo.update(ndim=varinfo_data['ndim'])\nif \"size\" not in varinfo:\n    varinfo.update(size=varinfo_data['size'])\nreturn varinfo", "path": "galry\\visuals\\visual.py", "repo_name": "rossant/galry", "stars": 189, "license": "other", "language": "python", "size": 9023}
{"docstring": "# super(CompoundVisual, self).__init__(scene, *args, **kwargs)\n", "func_signal": "def __init__(self, scene, *args, **kwargs):\n", "code": "self.visuals = []\nself.name = kwargs.pop('name')\nself.initialize(*args, **kwargs)", "path": "galry\\visuals\\visual.py", "repo_name": "rossant/galry", "stars": 189, "license": "other", "language": "python", "size": 9023}
{"docstring": "\"\"\"Handle window resize in shaders.\"\"\"\n", "func_signal": "def initialize_viewport(self):\n", "code": "self.add_uniform('viewport', vartype=\"float\", ndim=2, data=(1., 1.))\nself.add_uniform('window_size', vartype=\"float\", ndim=2)#, data=(600., 600.))\nif self.constrain_ratio:\n    self.add_vertex_main(\"gl_Position.xy = gl_Position.xy / viewport;\",\n        position='last', name='viewport')", "path": "galry\\visuals\\visual.py", "repo_name": "rossant/galry", "stars": 189, "license": "other", "language": "python", "size": 9023}
{"docstring": "\"\"\"Return the dict representation of the visual.\"\"\"\n", "func_signal": "def get_dic(self):\n", "code": "dic = {\n    'size': self.size,\n    'bounds': self.bounds,\n    'visible': self.visible,\n    'is_static': self.is_static,\n    'options': self.options,\n    'primitive_type': self.primitive_type,\n    'constrain_ratio': self.constrain_ratio,\n    'constrain_navigation': self.constrain_navigation,\n    'framebuffer': self.framebuffer,\n    # 'beforeclear': self.beforeclear,\n    'variables': self.get_variables_list(),\n    'vertex_shader': self.vertex_shader,\n    'fragment_shader': self.fragment_shader,\n}\nreturn dic", "path": "galry\\visuals\\visual.py", "repo_name": "rossant/galry", "stars": 189, "license": "other", "language": "python", "size": 9023}
{"docstring": "\"\"\"The visual should be defined here.\"\"\"\n    \n", "func_signal": "def initialize(self, *args, **kwargs):\n", "code": "\n\"\"\"Finalize the template to make sure that shaders are compilable.\n\nThis is the place to implement any post-processing algorithm on the\nshader sources, like custom template replacements at runtime.\n\n\"\"\"\n\n# self.size is a mandatory variable\nassert self.size is not None\n\n# default rendering options\nif self.bounds is None:\n    self.bounds = [0, self.size]\n# ensure the type of bounds\nself.bounds = np.array(self.bounds, dtype=np.int32)\n    \nif self.fragdata is not None:\n    self.set_fragdata(self.fragdata)\n    \nif self.primitive_type is None:\n    self.primitive_type = 'LINE_STRIP'", "path": "galry\\visuals\\visual.py", "repo_name": "rossant/galry", "stars": 189, "license": "other", "language": "python", "size": 9023}
{"docstring": "# for reinitialization, just record the data\n", "func_signal": "def add_foo(self, shader_type, name, **kwargs):\n", "code": "if self.reinitialization:\n    if 'data' in kwargs:\n        self.data_updating[name] = kwargs['data']\n    return\n# otherwise, add the variable normally\nkwargs['shader_type'] = shader_type\nkwargs['name'] = name\n# default parameters\nkwargs['vartype'] = kwargs.get('vartype', 'float')\nkwargs['size'] = kwargs.get('size', None)\nkwargs['ndim'] = kwargs.get('ndim', 1)\nself.variables[name] = kwargs", "path": "galry\\visuals\\visual.py", "repo_name": "rossant/galry", "stars": 189, "license": "other", "language": "python", "size": 9023}
{"docstring": "\"\"\"Return the GLSL variable declaration statement from a variable \ninformation.\n\nArguments:\n  * varinfo: a dictionary with the information about the variable,\n    in particular the type (int/float) and the number of dimensions\n    (scalar, vector or matrix).\n\nReturns:\n  * declaration: the string containing the variable declaration.\n    \n\"\"\"\n", "func_signal": "def _get_shader_type(varinfo):\n", "code": "if type(varinfo[\"ndim\"]) == int or type(varinfo[\"ndim\"]) == long:\n    if varinfo[\"ndim\"] == 1:\n        shader_type = varinfo[\"vartype\"]\n    elif varinfo[\"ndim\"] >= 2:\n        shader_type = \"vec%d\" % varinfo[\"ndim\"]\n        if varinfo[\"vartype\"] != \"float\":\n            shader_type = \"i\" + shader_type\n# matrix: (2,2) or (3,3) or (4,4)\nelif type(varinfo[\"ndim\"]) == tuple:\n    shader_type = \"mat%d\" % varinfo[\"ndim\"][0]\nreturn shader_type", "path": "galry\\visuals\\visual.py", "repo_name": "rossant/galry", "stars": 189, "license": "other", "language": "python", "size": 9023}
{"docstring": "# add the compound as a variable\n", "func_signal": "def add_compound(self, name, **kwargs):\n", "code": "self.add_foo('compound', name, **kwargs)\n# process the compound: add the associated data in the corresponding\n# variables\nfun = kwargs['fun']\ndata = kwargs['data']\nkwargs = fun(data)\nfor name, value in kwargs.iteritems():\n    self.variables[name]['data'] = value", "path": "galry\\visuals\\visual.py", "repo_name": "rossant/galry", "stars": 189, "license": "other", "language": "python", "size": 9023}
{"docstring": "\"\"\"Add code in the main function of the fragment shader.\nAt the end of the code, the vec4 variable `out_color` must have been\ndefined. It contains the color of the current pixel.\n\nArguments:\n  * code: the GLSL code as a string.\n  * index=None: the index of that code snippet in the final main\n    function. By default (index=None), the code is appended at the end\n    of the main function. With index=0, it is at the beginning of the\n    main function. Other integer values may be used when using several\n    calls to `add_fragment_main`.\n\n\"\"\"\n", "func_signal": "def add_fragment_main(self, *args, **kwargs):\n", "code": "kwargs['shader'] = 'fragment'\nself.add_main(*args, **kwargs)", "path": "galry\\visuals\\visual.py", "repo_name": "rossant/galry", "stars": 189, "license": "other", "language": "python", "size": 9023}
{"docstring": "\"\"\"Load the extension in IPython.\"\"\"\n", "func_signal": "def load_ipython_extension(ip):\n", "code": "global _loaded\nif not _loaded:\n    # Get the formatter.\n    mime = 'application/json'\n    formatter = ip.display_formatter.formatters[mime]\n\n    # Register handlers.\n    # The first argument is the full module name where the class is defined.\n    # The second argument is the class name.\n    # The third argument is the Python handler that takes an instance of\n    # this class, and returns a JSON string.\n    formatter.for_type_by_name('galry.galryplot', 'GalryPlot', get_json)\n\n    _loaded = True", "path": "experimental\\ipynb\\extensions\\ipynbgalry\\load.py", "repo_name": "rossant/galry", "stars": 189, "license": "other", "language": "python", "size": 9023}
{"docstring": "\"\"\"Add code in the main function of the vertex shader.\nAt the end of the code, the vec2 variable `position` must have been\ndefined, either as an attribute or uniform, or declared in the main\ncode. It contains the position of the current vertex.\nOther output variables include `gl_PointSize` for the size of vertices\nin the case when the primitive type is `Points`.\n\nArguments:\n  * code: the GLSL code as a string.\n  * index=None: the index of that code snippet in the final main\n    function. By default (index=None), the code is appended at the end\n    of the main function. With index=0, it is at the beginning of the\n    main function. Other integer values may be used when using several\n    calls to `add_vertex_main`. Or it can be 'end'.\n\n\"\"\"\n", "func_signal": "def add_vertex_main(self, *args, **kwargs):\n", "code": "kwargs['shader'] = 'vertex'\nself.add_main(*args, **kwargs)", "path": "galry\\visuals\\visual.py", "repo_name": "rossant/galry", "stars": 189, "license": "other", "language": "python", "size": 9023}
{"docstring": "\"\"\"Return a variable by its name, and for any given visual which \nis specified by its name.\"\"\"\n# get the variables list\n", "func_signal": "def get_variable(self, name, visual=None):\n", "code": "if visual is None:\n    variables = self.variables.values()\nelse:\n    variables = self.get_visual(visual)['variables']\nvariables = [v for v in variables if v.get('name', '') == name]\nif not variables:\n    return None\nreturn variables[0]", "path": "galry\\visuals\\visual.py", "repo_name": "rossant/galry", "stars": 189, "license": "other", "language": "python", "size": 9023}
{"docstring": "\"\"\"Load a PNG texture.\"\"\"\n", "func_signal": "def load_png(filename):\n", "code": "import matplotlib.image as mpimg\nreturn mpimg.imread(filename)", "path": "galry\\visuals\\fontmaps\\tools.py", "repo_name": "rossant/galry", "stars": 189, "license": "other", "language": "python", "size": 9023}
{"docstring": "\"\"\"Build the vertex and fragment final codes, using the declarations\nof all template variables.\"\"\"\n", "func_signal": "def get_shader_codes(self):\n", "code": "vs = VS_TEMPLATE\nfs = FS_TEMPLATE\n\n# Shader headers\nvs_header = self.get_header('vertex')\nfs_header = self.get_header('fragment')\n\n# Varyings\nfor varying in self.varyings:\n    s1, s2 = get_varying_declarations(varying)\n    vs_header += s1\n    fs_header += s2\n\n# vs_header += \"\".join(self.vs_headers)\n# fs_header += \"\".join(self.fs_headers)\n\n# Integrate shader headers\nvs = vs.replace(\"%VERTEX_HEADER%\", vs_header)\nfs = fs.replace(\"%FRAGMENT_HEADER%\", fs_header)\n\n# Vertex and fragment main code\nvs_main = self.get_main('vertex')\nfs_main = self.get_main('fragment')\n\n# Integrate shader headers\nvs = vs.replace(\"%VERTEX_MAIN%\", vs_main)\nfs = fs.replace(\"%FRAGMENT_MAIN%\", fs_main)\n\n# frag color or frag data\nif self.fragdata is None:\n    fs = fs.replace('%FRAG%', \"\"\"gl_FragColor = out_color;\"\"\")\nelse:\n    fs = fs.replace('%FRAG%', \"\"\"gl_FragData[%d] = out_color;\"\"\" % self.fragdata)\n\n# Make sure there are no Windows carriage returns\nvs = vs.replace(b\"\\r\\n\", b\"\\n\")\nfs = fs.replace(b\"\\r\\n\", b\"\\n\")\n\n# OLDGLSL does not know the texture function\nif not OLDGLSL:\n    fs = fs.replace(\"texture1D(\", \"texture(\" % 2)\n    fs = fs.replace(\"texture2D(\", \"texture(\" % 2)\n\n# set default color\nfs = fs.replace('%DEFAULT_COLOR%', str(self.default_color))\n\n# replace GLSL version header\nvs = vs.replace('%GLSL_VERSION_HEADER%', self.version_header)\nfs = fs.replace('%GLSL_VERSION_HEADER%', self.version_header)\n\n# replace GLSL precision header\nvs = vs.replace('%GLSL_PRECISION_HEADER%', self.precision_header)\nfs = fs.replace('%GLSL_PRECISION_HEADER%', self.precision_header)\n    \nreturn vs, fs", "path": "galry\\visuals\\visual.py", "repo_name": "rossant/galry", "stars": 189, "license": "other", "language": "python", "size": 9023}
{"docstring": "\"\"\"\n\tOpens the serial port and prepares for writing.\n\tport MUST be set, and values are operating system dependant.\n\"\"\"\n", "func_signal": "def __init__(self, port, baud, verbose=False):\n", "code": "self._verbose = verbose\n\nif self._verbose:\n\tprint >> sys.stdout, \"Opening serial port: \" + port\n\n#Timeout value 10\" max travel, 1RPM, 20 threads/in = 200 seconds\nself.ser = serial.Serial(port, baud, timeout=200)\n\nif self._verbose:\n\tprint >> sys.stdout, \"Serial Open?: \" + str(self.ser.isOpen())\n\tprint >> sys.stdout, \"Baud Rate: \" + str(self.ser.baudrate)", "path": "fabmetheus_utilities\\miscellaneous\\fabricate\\RepRapArduinoSerialSender.py", "repo_name": "ahmetcemturan/SFACT", "stars": 151, "license": "agpl-3.0", "language": "python", "size": 11819}
{"docstring": "\"\"\"Get a complex by removing one axis of this one.\n\nKeyword arguments:\nwhich -- the axis to drop (0=X, 1=Y, 2=Z)\"\"\"\n", "func_signal": "def dropAxis( self, which ):\n", "code": "if which == 0:\n\treturn complex( self.y, self.z )\nif which == 1:\n\treturn complex( self.x, self.z )\nif which == 2:\n\treturn complex( self.x, self.y )", "path": "fabmetheus_utilities\\miscellaneous\\nophead\\vector3.py", "repo_name": "ahmetcemturan/SFACT", "stars": 151, "license": "agpl-3.0", "language": "python", "size": 11819}
{"docstring": "\"\"\"Parse a gcode file and send the commands to the extruder.  If no filename is specified, parse all the gcode files which are not log files in this folder.\nThis function requires write access to the serial device, running as root is one way to get that access.\"\"\"\n", "func_signal": "def extrude( filename = ''):\n", "code": "if filename == '':\n\textrudeFiles( getGCodeFilesWhichAreNotLogFiles() )\n\treturn\nextrudeFile( filename )", "path": "fabmetheus_utilities\\miscellaneous\\fabricate\\extrude.py", "repo_name": "ahmetcemturan/SFACT", "stars": 151, "license": "agpl-3.0", "language": "python", "size": 11819}
{"docstring": "# here we draw interface controls\n# profiles menu\n", "func_signal": "def makeInterface (self):\n", "code": "  self.frame0=tk.Frame (self)\n  self.frame0.pack ()\n  self.frame0.place (x=c.frameLeftMargin, y=c.frameTopMargin, height=c.frameHeight, width=c.frameWidth)\n  self.label=tk.Label (self.frame0, text=c.profilesListLabel, fg=c.labelColor)\n  self.label.pack ()\n  self.label.place (x=c.labelMarginLeft)\n  self.profile=tk.StringVar (self)\n  self.profiles=tk.OptionMenu (self.frame0, self.profile, *self.profilesList, command=self.changingProfile)\n  self.profiles.config (width=c.superLargeMenuWidth)\n  self.profiles.pack ()\n  self.profiles.place (y=c.menuMarginTop)\n\n# layer thickness menu\n  self.frame1=tk.Frame (self)\n  self.frame1.pack ()\n  self.frame1.place (x=c.frameLeftMargin + c.frameRightOffset * 2, y=c.frameTopMargin, height=c.frameHeight, width=c.frameWidth) \n  self.label=tk.Label (self.frame1, text=c.thicknessListLabel, fg=c.labelColor)\n  self.label.pack ()\n  self.label.place (x=c.labelMarginLeft)  \n  self.layerThickness=tk.StringVar (self)\n  self.layerThicknesses=tk.OptionMenu (self.frame1, self.layerThickness, *c.layerThicknessList, command=self.refreshOptionMenus)\n  self.layerThicknesses.config (width=c.largeMenuWidth)\n  self.layerThicknesses.pack ()\n  self.layerThicknesses.place (y=c.menuMarginTop)  \n\n# feed rate menu\n  self.frame2=tk.Frame (self)\n  self.frame2.pack ()\n  self.frame2.place (x=c.frameLeftMargin, y=c.frameTopMargin + c.frameTopOffset, height=c.frameHeight, width=c.frameWidth)\n  self.label=tk.Label (self.frame2, text=c.feedRateListLabel, fg=c.labelColor)\n  self.label.pack ()\n  self.label.place (x=c.labelMarginLeft)\n  # validatecommand ==> error!\u2026\n  # self.feedRate=tk.Spinbox (self.frame2, from_=6, to=100, command=self.refreshSpinBoxes, validate=tk.ALL, validatecommand=self.refreshSpinBoxes) ==> error!\u2026\n  # ==> no modificationButtonState into spinboxes when content is modified via keyboard\n  self.feedRate=tk.Spinbox (self.frame2, from_=c.feedRateMinimumValue, to=c.feedRateMaximumValue, command=self.refreshFeedAndFlowRates, width=c.menuWidth)\n  self.feedRate.pack ()\n  self.feedRate.place (x=c.labelMarginLeft, y=c.menuMarginTop)  \n\n# flow rate menu\n  self.frame3=tk.Frame (self)\n  self.frame3.pack ()\n  self.frame3.place (x=c.frameLeftMargin + c.frameRightOffset, y=c.frameTopMargin + c.frameTopOffset, height=c.frameHeight, width=c.frameWidth)\n  self.label=tk.Label (self.frame3, text=c.flowRateListLabel, fg=c.labelColor)\n  self.label.pack ()\n  self.label.place (x=c.labelMarginLeft)\n  self.flowRate=tk.Spinbox (self.frame3, from_=c.flowRateMinimumValue, to=c.flowRateMaximumValue, command=self.refreshSpinBoxes, width=c.menuWidth)\n  self.flowRate.pack ()\n  self.flowRate.place (x=c.labelMarginLeft, y=c.menuMarginTop)\n\n# first layer menu\n  self.frame31=tk.Frame (self)\n  self.frame31.pack ()\n  self.frame31.place (x=c.frameLeftMargin + c.frameRightOffset * 2, y=c.frameTopMargin + c.frameTopOffset, height=c.frameHeight, width=c.frameWidth)  \n  self.label=tk.Label (self.frame31, text=c.firstLayerLabel, fg=c.labelColor)\n  self.label.pack ()\n  self.label.place (x=c.labelMarginLeft)  \n  self.firstLayerSpeed=tk.StringVar (self)\n  self.firstLayer=tk.OptionMenu (self.frame31, self.firstLayerSpeed, *c.firstLayerSpeedList, command=self.refreshOptionMenus)\n  self.firstLayer.config (width=c.largeMenuWidth)\n  self.firstLayer.pack ()\n  self.firstLayer.place (y=c.menuMarginTop)  \n\n# infill solidity menu\n  self.frame4=tk.Frame (self)\n  self.frame4.pack ()\n  self.frame4.place (x=c.frameLeftMargin, y=c.frameTopMargin + c.frameTopOffset * 2, height=c.frameHeight, width=c.frameWidth) \n  self.label=tk.Label (self.frame4, text=c.InfillSolidityListLabel, fg=c.labelColor)\n  self.label.pack ()\n  self.label.place (x=c.labelMarginLeft)\n  self.infillSolidity=tk.Spinbox (self.frame4, from_=4, to=100, increment=1, command=self.refreshSpinBoxes, width=c.menuWidth)\n  self.infillSolidity.pack ()\n  self.infillSolidity.place (x=c.labelMarginLeft, y=c.menuMarginTop)\n\n# perimeter menu\n  self.frame40=tk.Frame (self)\n  self.frame40.pack ()\n  self.frame40.place (x=c.frameLeftMargin + c.frameRightOffset, y=c.frameTopMargin + c.frameTopOffset * 2, height=c.frameHeight, width=c.frameWidth)  \n  self.label=tk.Label (self.frame40, text=c.perimeterLabel, fg=c.labelColor)\n  self.label.pack ()\n  self.label.place (x=c.labelMarginLeft)  \n  self.perimeterSpeed=tk.StringVar (self)\n  self.perimeterMenu=tk.OptionMenu (self.frame40, self.perimeterSpeed, *c.perimeterSpeedList, command=self.refreshOptionMenus)\n  self.perimeterMenu.config (width=c.largeMenuWidth)\n  self.perimeterMenu.pack ()\n  self.perimeterMenu.place (y=c.menuMarginTop)\n\n# multiply on/off radiobuttons\n  self.frame41=tk.Frame (self)\n  self.frame41.pack ()\n  self.frame41.place (x=c.frameLeftMargin, y=c.frameTopMargin + c.frameTopOffset * 3, height=c.frameHeight, width=c.frameWidth)  \n  self.label=tk.Label (self.frame41, text=c.multiplyActivityLabel, fg=c.labelColor)\n  self.label.pack ()\n  self.label.place (x=c.labelMarginLeft)  \n  self.multiplyActivity = tk.BooleanVar (self)\n  self.radio1 = tk.Radiobutton (self.frame41, text=\"On\", variable=self.multiplyActivity, value=True, command=self.refreshRadioButtons)\n  self.radio1.pack ()\n  self.radio1.place (y=c.menuMarginTop)\n  self.radio2 = tk.Radiobutton (self.frame41, text=\"Off\", variable=self.multiplyActivity, value=False, command=self.refreshRadioButtons)\n  self.radio2.pack ()\n  self.radio2.place (y=c.menuMarginTop, x=50)  \n  \n# multiply rows menu\n  self.frame5=tk.Frame (self)\n  self.frame5.pack ()\n  self.frame5.place (x=c.frameLeftMargin + c.frameRightOffset, y=c.frameTopMargin + c.frameTopOffset * 3, height=c.frameHeight, width=c.frameWidth)\n  self.label=tk.Label (self.frame5, text=c.multiplyRowListLabel, fg=c.labelColor)\n  self.label.pack ()\n  self.label.place (x=c.labelMarginLeft)\n  self.multiplyRow=tk.StringVar (self)\n  self.menu=tk.OptionMenu (self.frame5, self.multiplyRow, *c.multiplyRowList, command=self.refreshOptionMenus)\n  self.menu.config (width=c.menuWidth)\n  self.menu.pack ()\n  self.menu.place (y=c.menuMarginTop)", "path": "skFrontend.py", "repo_name": "ahmetcemturan/SFACT", "stars": 151, "license": "agpl-3.0", "language": "python", "size": 11819}
{"docstring": "# assume normal is normalized\n", "func_signal": "def reflect(self, normal):\n", "code": "assert isinstance(normal, Vector3)\nd = 2 * (self.x * normal.x + self.y * normal.y + self.z * normal.z)\nreturn Vector3(self.x - d * normal.x,\n\t\t\t   self.y - d * normal.y,\n\t\t\t   self.z - d * normal.z)", "path": "fabmetheus_utilities\\miscellaneous\\nophead\\vector3.py", "repo_name": "ahmetcemturan/SFACT", "stars": 151, "license": "agpl-3.0", "language": "python", "size": 11819}
{"docstring": "\"\"\"Parse a gcode line.\"\"\"\n", "func_signal": "def parseLine(self, line):\n", "code": "splitLine = getSplitLineBeforeBracketSemicolon(line)\nif len(splitLine) < 1:\n\treturn\nfirstWord = splitLine[0]\nif len(firstWord) < 1:\n\treturn\nif firstWord[0] == '(':\n\treturn\nif firstWord != 'G1':\n\tself.output.write(line + '\\n')\n\treturn\neString = getStringFromCharacterSplitLine('E', splitLine )\nxString = getStringFromCharacterSplitLine('X', splitLine )\nyString = getStringFromCharacterSplitLine('Y', splitLine )\nzString = getStringFromCharacterSplitLine('Z', splitLine )\nfeedRateString = getStringFromCharacterSplitLine('F', splitLine )\n\nif zString is not None and zString != self.lastZString:\n\tself.output.write('G1')\n\tself.output.write(' F50')\n\tself.output.write(' Z' + zString )\n\tself.output.write(' F50')\n\tself.output.write('\\n')\n\tself.output.write('G1')\nif feedRateString is not None :\n\tself.output.write(' F' + feedRateString )\n\tself.output.write('\\n')                  \n\nself.output.write('G1')\nif xString is not None:\n\tself.output.write(' X' + xString )\nif yString is not None:\n\tself.output.write(' Y' + yString )\n\"\"\"if zString is not None and zString != self.lastZString:\n\tself.output.write(' Z' + zString )\"\"\"\nif feedRateString is not None and feedRateString != self.lastFeedRateString:\n\tself.output.write(' F' + feedRateString )\nif eString is not None:\n\tself.output.write(' E' + eString )\nself.lastFeedRateString = feedRateString\nself.lastZString = zString\nself.output.write('\\n')", "path": "skeinforge_application\\skeinforge_plugins\\craft_plugins\\export_plugins\\static_plugins\\gcode_gen3.py", "repo_name": "ahmetcemturan/SFACT", "stars": 151, "license": "agpl-3.0", "language": "python", "size": 11819}
{"docstring": "\"\"\"Get the split line before a bracket or semicolon.\"\"\"\n", "func_signal": "def getSplitLineBeforeBracketSemicolon(line):\n", "code": "bracketSemicolonIndex = min( line.find(';'), line.find('(') )\nif bracketSemicolonIndex < 0:\n\treturn line.split()\nreturn line[ : bracketSemicolonIndex ].split()", "path": "skeinforge_application\\skeinforge_plugins\\craft_plugins\\export_plugins\\static_plugins\\gcode_gen3.py", "repo_name": "ahmetcemturan/SFACT", "stars": 151, "license": "agpl-3.0", "language": "python", "size": 11819}
{"docstring": "\"\"\"Parse gcode files and send the commands to the extruder.\nThis function requires write access to the serial device, running as root is one way to get that access.\"\"\"\n", "func_signal": "def extrudeFiles( filenames ):\n", "code": "for filename in filenames:\n\textrudeFile( filename )", "path": "fabmetheus_utilities\\miscellaneous\\fabricate\\extrude.py", "repo_name": "ahmetcemturan/SFACT", "stars": 151, "license": "agpl-3.0", "language": "python", "size": 11819}
{"docstring": "\"\"\"Parse gcode text and store the gcode.\"\"\"\n", "func_signal": "def getCraftedGcode( self, gcodeText ):\n", "code": "lines = getTextLines(gcodeText)\nfor line in lines:\n\tself.parseLine(line)\nreturn self.output.getvalue()", "path": "skeinforge_application\\skeinforge_plugins\\craft_plugins\\export_plugins\\static_plugins\\gcode_gen3.py", "repo_name": "ahmetcemturan/SFACT", "stars": 151, "license": "agpl-3.0", "language": "python", "size": 11819}
{"docstring": "\"\"\"\n\tCloses the serial port, terminating communications with the arduino.\n\"\"\"\n", "func_signal": "def close():\n", "code": "if self._verbose:\n\tprint >> sys.stdout, \"Closing serial port.\"\nself.ser.close()\n\nif self._verbose:\n\tprint >> sys.stdout, \"Serial Open?: \" + str(self.ser.isOpen())", "path": "fabmetheus_utilities\\miscellaneous\\fabricate\\RepRapArduinoSerialSender.py", "repo_name": "ahmetcemturan/SFACT", "stars": 151, "license": "agpl-3.0", "language": "python", "size": 11819}
{"docstring": "# FIXME: Somehow this fails if this is launched using the Preferences,\n# but works from the command-line.\n", "func_signal": "def update(self):\n", "code": "self.image = ImageTk.PhotoImage(self.images[self.index])\nself.canvas.create_image(0,0, anchor= Tkinter.NW, image = self.image)\nif self.index < len(self.images) - 1:\n    self.up_button.config(state = Tkinter.NORMAL)\nelse:\n    self.up_button.config(state = Tkinter.DISABLED)\nif self.index > 0:\n    self.down_button.config(state = Tkinter.NORMAL)\nelse:\n    self.down_button.config(state = Tkinter.DISABLED)", "path": "fabmetheus_utilities\\miscellaneous\\nophead\\preview.py", "repo_name": "ahmetcemturan/SFACT", "stars": 151, "license": "agpl-3.0", "language": "python", "size": 11819}
{"docstring": "\"\"\"Get the fileName basename if the file is in the current working directory, otherwise return the original full name.\"\"\"\n", "func_signal": "def getSummarizedFileName(fileName):\n", "code": "if os.getcwd() == os.path.dirname(fileName):\n\treturn os.path.basename(fileName)\nreturn fileName", "path": "skeinforge_application\\skeinforge_plugins\\craft_plugins\\export_plugins\\static_plugins\\gcode_gen3.py", "repo_name": "ahmetcemturan/SFACT", "stars": 151, "license": "agpl-3.0", "language": "python", "size": 11819}
{"docstring": "\"\"\"Get the string after the first occurence of the character in the split line.\"\"\"\n", "func_signal": "def getStringFromCharacterSplitLine(character, splitLine):\n", "code": "indexOfCharacter = getIndexOfStartingWithSecond(character, splitLine)\nif indexOfCharacter < 0:\n\treturn None\nreturn splitLine[indexOfCharacter][1 :]", "path": "skeinforge_application\\skeinforge_plugins\\craft_plugins\\export_plugins\\static_plugins\\gcode_gen3.py", "repo_name": "ahmetcemturan/SFACT", "stars": 151, "license": "agpl-3.0", "language": "python", "size": 11819}
{"docstring": "\"\"\"Parse a gcode file and send the commands to the extruder.\nThis function requires write access to the serial device, running as root is one way to get that access.\"\"\"\n", "func_signal": "def extrudeFile( filename ):\n", "code": "print('File ' + filename + ' is being extruded.')\nfileText = archive.getFileText( filename )\ngcodec.writeFileMessageSuffix( filename, extrudeText(fileText), 'The gcode log file is saved as ', '_log')", "path": "fabmetheus_utilities\\miscellaneous\\fabricate\\extrude.py", "repo_name": "ahmetcemturan/SFACT", "stars": 151, "license": "agpl-3.0", "language": "python", "size": 11819}
{"docstring": "\"\"\"Get index of the first occurence of the given letter in the split line, starting with the second word.  Return - 1 if letter is not found\"\"\"\n", "func_signal": "def getIndexOfStartingWithSecond(letter, splitLine):\n", "code": "for wordIndex in xrange( 1, len(splitLine) ):\n\tword = splitLine[ wordIndex ]\n\tfirstLetter = word[0]\n\tif firstLetter == letter:\n\t\treturn wordIndex\nreturn - 1", "path": "skeinforge_application\\skeinforge_plugins\\craft_plugins\\export_plugins\\static_plugins\\gcode_gen3.py", "repo_name": "ahmetcemturan/SFACT", "stars": 151, "license": "agpl-3.0", "language": "python", "size": 11819}
{"docstring": "\"\"\"Parse a gcode text and send the commands to the extruder.\nThis function requires write access to the serial device, running as root is one way to get that access.\"\"\"\n", "func_signal": "def extrudeText(gcodeText):\n", "code": "skein = extrudeSkein()\nskein.parseText(gcodeText)\nreturn skein.output", "path": "fabmetheus_utilities\\miscellaneous\\fabricate\\extrude.py", "repo_name": "ahmetcemturan/SFACT", "stars": 151, "license": "agpl-3.0", "language": "python", "size": 11819}
{"docstring": "#global serialPort\n", "func_signal": "def setNotify(self):\n", "code": "if self.active:\n\tp = snap.SNAPPacket( serialPort, self.address, snap.localAddress, 0, 1, [CMD_NOTIFY, snap.localAddress] ) \t# set notifications to be sent to host\n\tif p.send():\n\t\treturn True\nreturn False", "path": "fabmetheus_utilities\\miscellaneous\\fabricate\\reprap.py", "repo_name": "ahmetcemturan/SFACT", "stars": 151, "license": "agpl-3.0", "language": "python", "size": 11819}
{"docstring": "\"\"\"Add an extruder command to the output and evaluate the extruder command.\nDisplay the entire command, but only evaluate the command after the first equal sign.\"\"\"\n", "func_signal": "def evaluateCommand( self, command ):\n", "code": "self.addToOutput( command )\nfirstEqualIndex = command.find('=')\nexec( command )", "path": "fabmetheus_utilities\\miscellaneous\\fabricate\\extrude.py", "repo_name": "ahmetcemturan/SFACT", "stars": 151, "license": "agpl-3.0", "language": "python", "size": 11819}
{"docstring": "\"\"\"Convert list of image frames to a GIF animation file\"\"\"\n", "func_signal": "def makedelta(fp, sequence):\n", "code": "previous = None\nfor im in sequence:\n    if not previous:\n        # global header\n        for s in getheader(im) + getdata(im):\n            fp.write(s)\n    else:\n        # delta frame\n        delta = ImageChops.subtract_modulo(im, previous)\n        bbox = delta.getbbox()\n        if not bbox:\n            bbox = (0,0, 1,1)\n        # compress difference\n        for s in getdata(im.crop(bbox), offset = bbox[:2]):\n            fp.write(s)\n    previous = im.copy()\nfp.write(\";\")", "path": "fabmetheus_utilities\\miscellaneous\\nophead\\enrique.py", "repo_name": "ahmetcemturan/SFACT", "stars": 151, "license": "agpl-3.0", "language": "python", "size": 11819}
{"docstring": "# initiate axies with addresses\n", "func_signal": "def __init__(self):\n", "code": "self.x = axisClass(2)\nself.y = axisClass(3)\nself.z = axisClass(4)", "path": "fabmetheus_utilities\\miscellaneous\\fabricate\\reprap.py", "repo_name": "ahmetcemturan/SFACT", "stars": 151, "license": "agpl-3.0", "language": "python", "size": 11819}
{"docstring": "\"\"\"Return True if the runinfo passes the checks specified in the QuestionSet\n\nChecks is an optional dictionary with the keys being questionset.pk and the\nvalues being the checks of the contained questions.\n\nThis, in conjunction with fetch_checks allows for fewer\ndb roundtrips and greater performance.\n\nSadly, checks cannot be hashed and therefore the request cache is useless\nhere. Thankfully the benefits outweigh the costs in my tests.\n\"\"\"\n\n", "func_signal": "def questionset_satisfies_checks(questionset, runinfo, checks=None):\n", "code": "passes = check_parser(runinfo)\n\nif not passes(questionset.checks):\n    return False\n\nif not checks:\n    checks = dict()\n    checks[questionset.id] = []\n\n    for q in questionset.questions():\n        checks[questionset.id].append((q.checks, q.number))\n\n# questionsets that pass the checks but have no questions are shown\n# (comments, last page, etc.)\nif not checks[questionset.id]:\n    return True\n\n# if there are questions at least one needs to be visible\nfor check, number in checks[questionset.id]:\n    if number in skipped_questions(runinfo):\n        continue\n\n    if passes(check):\n        return True\n\nreturn False", "path": "questionnaire\\views.py", "repo_name": "seantis/seantis-questionnaire", "stars": 134, "license": "bsd-3-clause", "language": "python", "size": 479}
{"docstring": "\"\"\"\nA view that can generate a RunID instance anonymously,\nand then redirect to the questionnaire itself.\n\nIt uses a Subject with the givenname of 'Anonymous' and the\nsurname of 'User'.  If this Subject does not exist, it will\nbe created.\n\nThis can be used with a URL pattern like:\n(r'^take/(?P<questionnaire_id>[0-9]+)/$',\n 'questionnaire.views.generate_run'),\n\"\"\"\n\n", "func_signal": "def generate_run(request, questionnaire_id, subject_id=None):\n", "code": "qu = get_object_or_404(Questionnaire, id=questionnaire_id)\nqs = qu.questionsets()[0]\n\nif subject_id is not None:\n    su = get_object_or_404(Subject, pk=subject_id)\nelse:\n    su = Subject.objects.filter(givenname='Anonymous', surname='User')[0:1]\n    if su:\n        su = su[0]\n    else:\n        su = Subject(givenname='Anonymous', surname='User')\n        su.save()\n\nstr_to_hash = \"\".join(\n    map(lambda i: chr(random.randint(0, 255)), range(16))\n)\nstr_to_hash += settings.SECRET_KEY\nkey = md5(str_to_hash).hexdigest()\n\nrun = RunInfo(subject=su, random=key, runid=key, questionset=qs)\nrun.save()\nif not use_session:\n    kwargs = {'runcode': key}\nelse:\n    kwargs = {}\n    request.session['runcode'] = key\n\nquestionnaire_start.send(sender=None, runinfo=run, questionnaire=qu)\nreturn HttpResponseRedirect(reverse('questionnaire', kwargs=kwargs))", "path": "questionnaire\\views.py", "repo_name": "seantis/seantis-questionnaire", "stars": 134, "license": "bsd-3-clause", "language": "python", "size": 479}
{"docstring": "\"\"\"Objects with a 'text/text_xx' attribute can contain magic strings\nreferring to the answers of other questions. This function takes\nany such object, goes through the stored answers (qvalues) and replaces\nthe magic string with the actual value. If this isn't possible the\nmagic string is removed from the text.\n\nOnly answers with 'store' in their check will work with this.\n\n\"\"\"\n\n", "func_signal": "def substitute_answer(qvalues, obj):\n", "code": "if qvalues and obj.text:\n    magic = 'subst_with_ans_'\n    regex = r'subst_with_ans_(\\S+)'\n\n    replacements = re.findall(regex, obj.text)\n    text_attributes = [a for a in dir(obj) if a.startswith('text_')]\n\n    for answerid in replacements:\n\n        target = magic + answerid\n        replacement = qvalues.get(answerid.lower(), '')\n\n        for attr in text_attributes:\n            oldtext = getattr(obj, attr)\n            newtext = oldtext.replace(target, replacement)\n\n            setattr(obj, attr, newtext)", "path": "questionnaire\\views.py", "repo_name": "seantis/seantis-questionnaire", "stars": 134, "license": "bsd-3-clause", "language": "python", "size": 479}
{"docstring": "\"\"\"\nGiven a comma separated question number and expression, determine if the\nprovided answer to the question number satisfies the expression.\n\nIf the expression starts with >, >=, <, or <=, compare the rest of\nthe expression numerically and return False if it's not able to be\nconverted to an integer.\n\nIf the expression starts with !, return true if the rest of the expression\ndoes not match the answer.\n\nOtherwise return true if the expression matches the answer.\n\nIf there is no comma and only a question number, it checks if the answer\nis \"yes\"\n\nWhen looking up the answer, it first checks if it's in the answerdict,\nthen it checks runinfo's cookies, then it does a database lookup to find\nthe answer.\n\nThe use of the comma separator is purely historical.\n\"\"\"\n\n", "func_signal": "def dep_check(expr, runinfo, answerdict):\n", "code": "if hasattr(runinfo, 'questionset'):\n    questionnaire = runinfo.questionset.questionnaire\nelif hasattr(runinfo, 'questionnaire'):\n    questionnaire = runinfo.questionnaire\nelse:\n    assert False\n\nif \",\" not in expr:\n    expr = expr + \",yes\"\n\ncheck_questionnum, check_answer = expr.split(\",\", 1)\ntry:\n    check_question = Question.objects.get(\n        number=check_questionnum,\n        questionset__questionnaire=questionnaire\n    )\nexcept Question.DoesNotExist:\n    return False\n\nif check_question in answerdict:\n    # test for membership in multiple choice questions\n    # FIXME: only checking answerdict\n    for k, v in answerdict[check_question].items():\n        if not k.startswith('multiple_'):\n            continue\n        if check_answer.startswith(\"!\"):\n            if check_answer[1:].strip() == v.strip():\n                return False\n        elif check_answer.strip() == v.strip():\n            return True\n    actual_answer = answerdict[check_question].get('ANSWER', '')\nelif hasattr(runinfo, 'get_cookie') and \\\n        runinfo.get_cookie(check_questionnum, False):\n    actual_answer = runinfo.get_cookie(check_questionnum)\nelse:\n    # retrieve from database\n    ansobj = Answer.objects.filter(\n        question=check_question,\n        runid=runinfo.runid,\n        subject=runinfo.subject\n    )\n    if ansobj:\n        actual_answer = ansobj[0].split_answer()[0]\n        logging.warn(\n            \"Put `store` in checks field for question %s\"\n            % check_questionnum\n        )\n    else:\n        actual_answer = None\n\nif not actual_answer:\n    if check_question.getcheckdict():\n        actual_answer = check_question.getcheckdict().get('default')\n\nif actual_answer is None:\n    actual_answer = u''\nif check_answer[0:1] in \"<>\":\n    try:\n        actual_answer = float(actual_answer)\n        if check_answer[1:2] == \"=\":\n            check_value = float(check_answer[2:])\n        else:\n            check_value = float(check_answer[1:])\n    except:\n        logging.error(\n            \"ERROR: must use numeric values with < <= => > checks (%r)\"\n            % check_question\n        )\n        return False\n    if check_answer.startswith(\"<=\"):\n        return actual_answer <= check_value\n    if check_answer.startswith(\">=\"):\n        return actual_answer >= check_value\n    if check_answer.startswith(\"<\"):\n        return actual_answer < check_value\n    if check_answer.startswith(\">\"):\n        return actual_answer > check_value\nif check_answer.startswith(\"!\"):\n    if actual_answer == '':\n        return False\n    return check_answer[1:].strip() != actual_answer.strip()\nreturn check_answer.strip() == actual_answer.strip()", "path": "questionnaire\\views.py", "repo_name": "seantis/seantis-questionnaire", "stars": 134, "license": "bsd-3-clause", "language": "python", "size": 479}
{"docstring": "\"\"\"Redirect to the correct and current questionset URL for this RunInfo\"\"\"\n\n# cache current questionset\n", "func_signal": "def redirect_to_qs(runinfo, request=None):\n", "code": "qs = runinfo.questionset\n\n# skip questionsets that don't pass\nif not questionset_satisfies_checks(runinfo.questionset, runinfo):\n\n    next = runinfo.questionset.next()\n\n    while next and not questionset_satisfies_checks(next, runinfo):\n        next = next.next()\n\n    runinfo.questionset = next\n    runinfo.save()\n\n    hasquestionset = bool(next)\nelse:\n    hasquestionset = True\n\n# empty ?\nif not hasquestionset:\n    logging.warn('no questionset in questionnaire which passes the check')\n    return finish_questionnaire(request, runinfo, qs.questionnaire)\n\nif not use_session:\n    args = [runinfo.random, runinfo.questionset.sortid]\n    urlname = 'questionset'\nelse:\n    args = []\n    request.session['qs'] = runinfo.questionset.sortid\n    request.session['runcode'] = runinfo.random\n    urlname = 'questionnaire'\nurl = reverse(urlname, args=args)\nreturn HttpResponseRedirect(url)", "path": "questionnaire\\views.py", "repo_name": "seantis/seantis-questionnaire", "stars": 134, "license": "bsd-3-clause", "language": "python", "size": 479}
{"docstring": "\"\"\"\nReturn the specified Question (by number) from the specified Questionnaire\n\"\"\"\n\n", "func_signal": "def get_question(number, questionnaire):\n", "code": "res = Question.objects.filter(\n    number=number, questionset__questionnaire=questionnaire\n)\nreturn res and res[0] or None", "path": "questionnaire\\views.py", "repo_name": "seantis/seantis-questionnaire", "stars": 134, "license": "bsd-3-clause", "language": "python", "size": 479}
{"docstring": "\"\"\"\nquestionnaire -- questionnaire model for summary\nanswers -- query set of answers to include in summary, defaults to all\n\nReturn a summary of the answer totals in answer_qs in the form:\n[('q1', 'question1 text',\n    [('choice1', 'choice1 text', num), ...],\n    ['freeform1', ...]), ...]\n\nquestions are returned in questionnaire order\nchoices are returned in question order\nfreeform options are case-insensitive sorted\n\"\"\"\n\n", "func_signal": "def answer_summary(questionnaire, answers=None):\n", "code": "if answers is None:\n    answers = Answer.objects.all()\nanswers = answers.filter(\n    question__questionset__questionnaire=questionnaire\n)\nquestions = Question.objects.filter(\n    questionset__questionnaire=questionnaire).order_by(\n    'questionset__sortid', 'number')\n\nsummary = []\nfor question in questions:\n    q_type = question.get_type()\n    if q_type.startswith('choice-yesno'):\n        choices = [('yes', _('Yes')), ('no', _('No'))]\n        if 'dontknow' in q_type:\n            choices.append(('dontknow', _(\"Don't Know\")))\n    elif q_type.startswith('choice'):\n        choices = [(c.value, c.text) for c in question.choices()]\n    else:\n        choices = []\n    choice_totals = dict([(k, 0) for k, v in choices])\n    freeforms = []\n    for a in answers.filter(question=question):\n        ans = a.split_answer()\n        for choice in ans:\n            if type(choice) == list:\n                freeforms.extend(choice)\n            elif choice in choice_totals:\n                choice_totals[choice] += 1\n            else:\n                # be tolerant of improperly marked data\n                freeforms.append(choice)\n    freeforms.sort(numal_sort)\n    summary.append((question.number, question.text, [\n        (n, t, choice_totals[n]) for (n, t) in choices], freeforms))\nreturn summary", "path": "questionnaire\\views.py", "repo_name": "seantis/seantis-questionnaire", "stars": 134, "license": "bsd-3-clause", "language": "python", "size": 479}
{"docstring": "\"\"\"\nChange the language, save it to runinfo if provided, and\nredirect to the provided URL (or the last URL).\nCan also be used by a url handler, w/o runinfo & next.\n\"\"\"\n\n", "func_signal": "def set_language(request, runinfo=None, next=None):\n", "code": "if not next:\n    next = request.REQUEST.get('next', None)\nif not next:\n    next = request.META.get('HTTP_REFERER', None)\n    if not next:\n        next = '/'\nresponse = HttpResponseRedirect(next)\nresponse['Expires'] = \"Thu, 24 Jan 1980 00:00:00 GMT\"\nif request.method == 'GET':\n    lang_code = request.GET.get('lang', None)\n    if lang_code and translation.check_for_language(lang_code):\n        if hasattr(request, 'session'):\n            request.session[LANGUAGE_SESSION_KEY] = lang_code\n        else:\n            response.set_cookie(settings.LANGUAGE_COOKIE_NAME, lang_code)\n        if runinfo:\n            runinfo.subject.language = lang_code\n            runinfo.subject.save()\nreturn response", "path": "questionnaire\\views.py", "repo_name": "seantis/seantis-questionnaire", "stars": 134, "license": "bsd-3-clause", "language": "python", "size": 479}
{"docstring": "\"\"\"\nReturns the absolute paths to \"template_name\", when appended to each\ndirectory in \"template_dirs\". Any paths that don't lie inside one of the\ntemplate dirs are excluded from the result set, for security reasons.\n\"\"\"\n", "func_signal": "def get_template_sources(template_name, template_dirs=None):\n", "code": "if not template_dirs:\n    template_dirs = settings.TEMPLATE_DIRS\nfor template_dir in template_dirs:\n    try:\n        yield safe_join(template_dir, template_name)\n    except UnicodeDecodeError:\n        # The template dir name was a bytestring that wasn't valid UTF-8.\n        raise\n    except ValueError:\n        # The joined path was located outside of this particular\n        # template_dir (it might be inside another one, so this isn't\n        # fatal).\n        pass", "path": "questionnaire\\langtemplateloader.py", "repo_name": "seantis/seantis-questionnaire", "stars": 134, "license": "bsd-3-clause", "language": "python", "size": 479}
{"docstring": "\"\"\"\nDelete the specified question/subject/runid combination from the Answer\ntable\n\"\"\"\n\n", "func_signal": "def delete_answer(question, subject, runid):\n", "code": "Answer.objects.filter(\n    subject=subject, runid=runid, question=question\n).delete()", "path": "questionnaire\\views.py", "repo_name": "seantis/seantis-questionnaire", "stars": 134, "license": "bsd-3-clause", "language": "python", "size": 479}
{"docstring": "\"\"\"Assuming the current language is German.\n   If template_name is index.$LANG.html, try index.de.html then index.html\n   Also replaces .. with . when attempting fallback.\n\"\"\"\n", "func_signal": "def load_template_source(template_name, template_dirs=None):\n", "code": "if \"$LANG\" in template_name:\n    lang = translation.get_language()\n    try:\n        t = template_name.replace(\"$LANG\", lang)\n        res = _load_template_source(t, template_dirs)\n        return res\n    except TemplateDoesNotExist:\n        t = template_name.replace(\"$LANG\", \"\").replace(\"..\", \".\")\n        return _load_template_source(t, template_dirs)\nreturn _load_template_source(template_name, template_dirs)", "path": "questionnaire\\langtemplateloader.py", "repo_name": "seantis/seantis-questionnaire", "stars": 134, "license": "bsd-3-clause", "language": "python", "size": 479}
{"docstring": "\"\"\"\nAdd an Answer to a Question for RunInfo, given the relevant form input\n\nanswer_dict contains the POST'd elements for this question, minus the\nquestion_{number} prefix.  The question_{number} form value is accessible\nwith the ANSWER key.\n\"\"\"\n\n", "func_signal": "def add_answer(runinfo, question, answer_dict):\n", "code": "answer = Answer()\nanswer.question = question\nanswer.subject = runinfo.subject\nanswer.runid = runinfo.runid\n\ntype = question.get_type()\n\nif \"ANSWER\" not in answer_dict:\n    answer_dict['ANSWER'] = None\n\nif type in Processors:\n    answer.answer = Processors[type](question, answer_dict) or ''\nelse:\n    raise AnswerException(\n        \"No Processor defined for question type %s\" % type\n    )\n\n# first, delete all existing answers to this question for this particular\n# user+run\ndelete_answer(question, runinfo.subject, runinfo.runid)\n\n# then save the new answer to the database\nanswer.save(runinfo)\n\nreturn True", "path": "questionnaire\\views.py", "repo_name": "seantis/seantis-questionnaire", "stars": 134, "license": "bsd-3-clause", "language": "python", "size": 479}
{"docstring": "\"\"\"Return the RunInfo entry with the provided random key\"\"\"\n\n", "func_signal": "def get_runinfo(random):\n", "code": "res = RunInfo.objects.filter(random=random.lower())\nreturn res and res[0] or None", "path": "questionnaire\\views.py", "repo_name": "seantis/seantis-questionnaire", "stars": 134, "license": "bsd-3-clause", "language": "python", "size": 479}
{"docstring": "\"\"\"Returns the progress as json for use with ajax\"\"\"\n\n", "func_signal": "def get_async_progress(request, *args, **kwargs):\n", "code": "if 'runcode' in kwargs:\n    runcode = kwargs['runcode']\nelse:\n    session_runcode = request.session.get('runcode', None)\n    if session_runcode is not None:\n        runcode = session_runcode\n\nruninfo = get_runinfo(runcode)\nresponse = dict(progress=get_progress(runinfo))\n\ncache.set('progress' + runinfo.random, response['progress'])\nresponse = HttpResponse(\n    json.dumps(response),\n    content_type='application/javascript'\n)\nresponse[\"Cache-Control\"] = \"no-cache\"\nreturn response", "path": "questionnaire\\views.py", "repo_name": "seantis/seantis-questionnaire", "stars": 134, "license": "bsd-3-clause", "language": "python", "size": 479}
{"docstring": "\"\"\"\nUsed only when ```QUESTIONNAIRE_USE_SESSION``` is True.\nTakes the questionnaire set in the session and redirects to the\nprevious questionnaire if any.\n\"\"\"\n", "func_signal": "def redirect_to_prev_questionnaire(request):\n", "code": "runcode = request.session.get('runcode', None)\nif runcode is not None:\n    runinfo = get_runinfo(runcode)\n    prev_qs = runinfo.questionset.prev()\n    if runinfo and prev_qs:\n        request.session['runcode'] = runinfo.random\n        request.session['qs'] = prev_qs.sortid\n        return HttpResponseRedirect(reverse('questionnaire'))\n\nreturn HttpResponseRedirect('/')", "path": "questionnaire\\views.py", "repo_name": "seantis/seantis-questionnaire", "stars": 134, "license": "bsd-3-clause", "language": "python", "size": 479}
{"docstring": "\"\"\"\nReturn the header labels for a set of questions as a list of strings.\n\nThis will create separate columns for each multiple-choice possiblity\nand freeform options, to avoid mixing data types and make charting easier.\n\"\"\"\n\n", "func_signal": "def _table_headers(questions):\n", "code": "ql = list(questions)\nql.sort(lambda x, y: numal_sort(x.number, y.number))\ncolumns = []\nfor q in ql:\n    if q.type == 'choice-yesnocomment':\n        columns.extend([q.number, q.number + \"-freeform\"])\n    elif q.type == 'choice-freeform':\n        columns.extend([q.number, q.number + \"-freeform\"])\n    elif q.type.startswith('choice-multiple'):\n        cl = [c.value for c in q.choice_set.all()]\n        cl.sort(numal_sort)\n        columns.extend([q.number + '-' + value for value in cl])\n        if q.type == 'choice-multiple-freeform':\n            columns.append(q.number + '-freeform')\n    else:\n        columns.append(q.number)\nreturn columns", "path": "questionnaire\\views.py", "repo_name": "seantis/seantis-questionnaire", "stars": 134, "license": "bsd-3-clause", "language": "python", "size": 479}
{"docstring": "\"\"\"\nReturn the QuestionSet template\n\nAlso add the javascript dependency code.\n\"\"\"\n\n", "func_signal": "def show_questionnaire(request, runinfo, errors={}):\n", "code": "request.runinfo = runinfo\nif request.GET.get('show_all') == '1':  # for debugging purposes.\n    questions = runinfo.questionset.questionnaire.questions()\nelse:\n    questions = runinfo.questionset.questions()\n\nshow_all = request.GET.get('show_all') == '1'\nquestionset = runinfo.questionset\nquestions = questionset.questionnaire.questions() if show_all \\\n    else questionset.questions()\n\nqlist = []\njsinclude = []      # js files to include\ncssinclude = []     # css files to include\njstriggers = []\nqvalues = {}\n\n# initialize qvalues\ncookiedict = runinfo.get_cookiedict()\n\nfor k, v in cookiedict.items():\n    qvalues[k] = v\n\nsubstitute_answer(qvalues, runinfo.questionset)\n\nfor question in questions:\n    # if we got here the questionset will at least contain one question\n    # which passes, so this is all we need to check for\n    question_visible = question_satisfies_checks(question, runinfo) \\\n        or show_all\n\n    Type = question.get_type()\n    _qnum, _qalpha = split_numal(question.number)\n\n    qdict = {\n        'css_style': '' if question_visible else 'display:none;',\n        'template': 'questionnaire/%s.html' % (Type),\n        'qnum': _qnum,\n        'qalpha': _qalpha,\n        'qtype': Type,\n        'qnum_class': (_qnum % 2 == 0) and \" qeven\" or \" qodd\",\n        'qalpha_class': _qalpha and (\n            ord(_qalpha[-1]) % 2 and ' alodd' or ' aleven'\n        ) or '',\n    }\n\n    # substitute answer texts\n    substitute_answer(qvalues, question)\n\n    # add javascript dependency checks\n    cd = question.getcheckdict()\n    depon = cd.get('requiredif', None) or cd.get('dependent', None)\n    if depon:\n        # extra args to BooleanParser are not required for toString\n        parser = BooleanParser(dep_check)\n        qdict['checkstring'] = ' checks=\"%s\"' % parser.toString(depon)\n        jstriggers.append('qc_%s' % question.number)\n    if 'default' in cd and question.number not in cookiedict:\n        qvalues[question.number] = cd['default']\n    if Type in QuestionProcessors:\n        qdict.update(QuestionProcessors[Type](request, question))\n        if 'jsinclude' in qdict:\n            if qdict['jsinclude'] not in jsinclude:\n                jsinclude.extend(qdict['jsinclude'])\n        if 'cssinclude' in qdict:\n            if qdict['cssinclude'] not in cssinclude:\n                cssinclude.extend(qdict['jsinclude'])\n        if 'jstriggers' in qdict:\n            jstriggers.extend(qdict['jstriggers'])\n        if 'qvalue' in qdict and question.number not in cookiedict:\n            qvalues[question.number] = qdict['qvalue']\n\n    qlist.append((question, qdict))\n\ntry:\n    has_progress = settings.QUESTIONNAIRE_PROGRESS in ('async', 'default')\n    async_progress = settings.QUESTIONNAIRE_PROGRESS == 'async'\nexcept AttributeError:\n    has_progress = True\n    async_progress = False\n\nif has_progress:\n    if async_progress:\n        progress = cache.get('progress' + runinfo.random, 1)\n    else:\n        progress = get_progress(runinfo)\nelse:\n    progress = 0\n\nif request.POST:\n    for k, v in request.POST.items():\n        if k.startswith(\"question_\"):\n            s = k.split(\"_\")\n            if len(s) == 4:\n                qvalues[s[1] + '_' + v] = '1'  # evaluates true in JS\n            elif len(s) == 3 and s[2] == 'comment':\n                qvalues[s[1] + '_' + s[2]] = v\n            else:\n                qvalues[s[1]] = v\n\nif use_session:\n    prev_url = reverse('redirect_to_prev_questionnaire')\nelse:\n    prev_url = 'javascript:history.back();'\nr = r2r(\n    \"questionnaire/questionset.html\",\n    request,\n    questionset=runinfo.questionset,\n    runinfo=runinfo,\n    errors=errors,\n    qlist=qlist,\n    progress=progress,\n    triggers=jstriggers,\n    qvalues=qvalues,\n    jsinclude=jsinclude,\n    cssinclude=cssinclude,\n    async_progress=async_progress,\n    async_url=reverse('progress', args=[runinfo.random]),\n    prev_url=prev_url,\n)\nr['Cache-Control'] = 'no-cache'\nr['Expires'] = \"Thu, 24 Jan 1980 00:00:00 GMT\"\nreturn r", "path": "questionnaire\\views.py", "repo_name": "seantis/seantis-questionnaire", "stars": 134, "license": "bsd-3-clause", "language": "python", "size": 479}
{"docstring": "\"\"\"\nquestionnaire -- questionnaire model for export\nanswers -- query set of answers to include in export, defaults to all\n\nReturn a flat dump of column headings and all the answers for a\nquestionnaire (in query set answers) in the form (headings, answers)\nwhere headings is:\n    ['question1 number', ...]\nand answers is:\n    [(subject1, 'runid1', ['answer1.1', ...]), ... ]\n\nThe headings list might include items with labels like\n'questionnumber-freeform'.  Those columns will contain all the freeform\nanswers for that question (separated from the other answer data).\n\nMultiple choice questions will have one column for each choice with\nlabels like 'questionnumber-choice'.\n\nThe items in the answers list are unicode strings or empty strings\nif no answer was given.  The number of elements in each answer list will\nalways match the number of headings.\n\"\"\"\n\n", "func_signal": "def answer_export(questionnaire, answers=None):\n", "code": "if answers is None:\n    answers = Answer.objects.all()\nanswers = answers.filter(\n    question__questionset__questionnaire=questionnaire\n).order_by(\n    'subject', 'runid', 'question__questionset__sortid', 'question__number'\n)\nanswers = answers.select_related()\nquestions = Question.objects.filter(\n    questionset__questionnaire=questionnaire)\nheadings = _table_headers(questions)\n\ncoldict = {}\nfor num, col in enumerate(headings):  # use coldict to find column indexes\n    coldict[col] = num\n# collect choices for each question\nqchoicedict = {}\nfor q in questions:\n    qchoicedict[q.id] = [x[0] for x in q.choice_set.values_list('value')]\n\nrunid = subject = None\nout = []\nrow = []\nfor answer in answers:\n    if answer.runid != runid or answer.subject != subject:\n        if row:\n            out.append((subject, runid, row))\n        runid = answer.runid\n        subject = answer.subject\n        row = [\"\"] * len(headings)\n    ans = answer.split_answer()\n    if type(ans) == int:\n        ans = str(ans)\n    for choice in ans:\n        col = None\n        if type(choice) == list:\n            # freeform choice\n            choice = choice[0]\n            col = coldict.get(answer.question.number + '-freeform', None)\n        if col is None:\n            # look for enumerated choice column (multiple-choice)\n            col = coldict.get(\n                answer.question.number + '-' + unicode(choice), None\n            )\n        if col is None:\n            # single-choice items\n            if (\n                (not qchoicedict[answer.question.id]) or\n                choice in qchoicedict[answer.question.id]\n            ):\n                col = coldict.get(answer.question.number, None)\n        if col is None:\n            # last ditch, if not found throw it in a freeform column\n            col = coldict.get(answer.question.number + '-freeform', None)\n        if col is not None:\n            row[col] = choice\n# and don't forget about the last one\nif row:\n    out.append((subject, runid, row))\nreturn headings, out", "path": "questionnaire\\views.py", "repo_name": "seantis/seantis-questionnaire", "stars": 134, "license": "bsd-3-clause", "language": "python", "size": 479}
{"docstring": "\"\"\"\nProcess submitted answers (if present) and redirect to next page\n\nIf this is a POST request, parse the submitted data in order to store\nall the submitted answers.  Then return to the next questionset or\nreturn a completed response.\n\nIf this isn't a POST request, redirect to the main page.\n\nWe only commit on success, to maintain consistency.  We also specifically\nrollback if there were errors processing the answers for this questionset.\n\"\"\"\n", "func_signal": "def questionnaire(request, runcode=None, qs=None):\n", "code": "if use_session:\n    session_runcode = request.session.get('runcode', None)\n    if session_runcode is not None:\n        runcode = session_runcode\n\n    session_qs = request.session.get('qs', None)\n    if session_qs is not None:\n        qs = session_qs\n\n# if runcode provided as query string, redirect to the proper page\nif not runcode:\n    runcode = request.GET.get('runcode')\n    if not runcode:\n        return HttpResponseRedirect(\"/\")\n    else:\n        if not use_session:\n            args = [runcode, ]\n        else:\n            request.session['runcode'] = runcode\n            args = []\n        return HttpResponseRedirect(reverse(\"questionnaire\", args=args))\n\nruninfo = get_runinfo(runcode)\n\nif not runinfo:\n    return HttpResponseRedirect('/')\n\n# let the runinfo have a piggy back ride on the request\n# so we can easily use the runinfo in places like the question processor\n# without passing it around\nrequest.runinfo = runinfo\n\nerrors = {}\ntry:\n    # res = _questionnaire(request, runcode, qs)\n    with transaction.atomic():\n        res = _questionnaire(request, runinfo, errors, qs)\nexcept AnswerException:\n    res = show_questionnaire(request, runinfo, errors=errors)\n\nreturn res", "path": "questionnaire\\views.py", "repo_name": "seantis/seantis-questionnaire", "stars": 134, "license": "bsd-3-clause", "language": "python", "size": 479}
{"docstring": "\"\"\"Shortcut to use RequestContext instead of Context in templates\"\"\"\n\n", "func_signal": "def r2r(tpl, request, **contextdict):\n", "code": "contextdict['request'] = request\nreturn render_to_response(\n    tpl, contextdict, context_instance=RequestContext(request)\n)", "path": "questionnaire\\views.py", "repo_name": "seantis/seantis-questionnaire", "stars": 134, "license": "bsd-3-clause", "language": "python", "size": 479}
{"docstring": "\"\"\"given pts in floats, linear interpolate pixel values nearby to get a good colour\"\"\"\n", "func_signal": "def get_interpolated_pixel_color(pts, s_im, size):\n", "code": "pts = clamp(pts, size)\n\ns_im = np.atleast_3d(s_im)\nys,xs = size\nycoords, xcoords = np.arange(ys), np.arange(xs)\nout = np.empty(pts.shape[1:] + (s_im.shape[-1],),dtype=s_im.dtype)\nfor i in range(s_im.shape[-1]): #loop over color channels\n    map_coordinates(s_im[...,i],pts,out[...,i],mode='nearest')\nreturn out", "path": "sphere_transforms_numpy.py", "repo_name": "henryseg/spherical_image_editing", "stars": 138, "license": "None", "language": "python", "size": 12639}
{"docstring": "\"\"\"inverse of 2x2 matrix\"\"\"\n", "func_signal": "def matrix2_inv(M):\n", "code": "inv_det = 1.0/matrix2_det(M)\nreturn [[inv_det*M[1][1],-inv_det*M[0][1]],[-inv_det*M[1][0],inv_det*M[0][0]]]", "path": "vectors_and_matrices.py", "repo_name": "henryseg/spherical_image_editing", "stars": 138, "license": "None", "language": "python", "size": 12639}
{"docstring": "\"\"\"map from (0, 2*pi) x (-pi/2, pi/2) rectangle to pixel coordinates\n\nParameters\n----------\npts : array_like (2,...)\n    pts[0,...], and pts[1,...] are x and y coordinates in the ranges (0,size[0]) and (0,size[1]), respectively\nsize : tuple\n    rectangle image size (rows, columns)\n\nReturns\n-------\nout : ndarray, same shape as pts\n    pts transformed from (0,2*pi) x (-pi/2,pi/2) range into (0,width) x (0,height) range (and transposed)\n\"\"\"\n", "func_signal": "def pixel_coords_from_angles(pts, size):\n", "code": "pts = np.asarray(pts) #turn into an ndarray if necessary\nys,xs = size #row, col indexing\nout = np.empty_like(pts)\nout[1] = pts[0]*float(xs)/(2*pi) - 0.5  #-0.5 shift for pixel coords lining up properly\nout[0] = (pts[1] + 0.5*pi)*float(ys-1)/pi\nreturn out", "path": "sphere_transforms_numpy.py", "repo_name": "henryseg/spherical_image_editing", "stars": 138, "license": "None", "language": "python", "size": 12639}
{"docstring": "#PIL images index by (x,y), but numpy goes by order in memory: (y,x)\n", "func_signal": "def main():\n", "code": "source_image = np.array(Image.open('equirectangular_test_image.png'),dtype=np.float32)\nsz = source_image.shape[:-1]\nM = zoom_in_on_pixel_coords((179.5,360), 2, sz)\n\nout_image = apply_SL2C_elt_to_image(M, source_image)\nnp.clip(out_image,0,255,out_image)\nImage.fromarray(out_image.astype(np.uint8)).save('test_image.png')", "path": "sphere_transforms_numpy.py", "repo_name": "henryseg/spherical_image_editing", "stars": 138, "license": "None", "language": "python", "size": 12639}
{"docstring": "\"\"\"create zero matrix\"\"\"\n", "func_signal": "def zero_matrix(m,n):\n", "code": "new_matrix = [[0 for row_entry in range(n)] for row in range(m)]\nreturn new_matrix", "path": "vectors_and_matrices.py", "repo_name": "henryseg/spherical_image_editing", "stars": 138, "license": "None", "language": "python", "size": 12639}
{"docstring": "\"\"\"inverse equirectangular projections, ie. map from (0,2*pi) x (-pi/2,pi/2) rectangle to sphere in R^3\nParameters\n----------\npts : array_like (2,...)\n    pts[0,...], pts[1,...] are longitude, latitude\nReturns\n-------\nout : ndarray (3,...)\n    pts transformed from longitude,latitude to x,y,z\n\"\"\"\n", "func_signal": "def sphere_from_angles(pts):\n", "code": "pts = np.asarray(pts)\nout = np.empty((3,) + pts.shape[1:])\nlon,lat = pts[0],pts[1]\nhoriz_radius = np.cos(lat)\nout[0] = horiz_radius*np.cos(lon) #x\nout[1] = horiz_radius*np.sin(lon) #y\nout[2] = np.sin(lat) #z\nreturn out", "path": "sphere_transforms_numpy.py", "repo_name": "henryseg/spherical_image_editing", "stars": 138, "license": "None", "language": "python", "size": 12639}
{"docstring": "\"\"\"equirectangular projection, ie. map from sphere in R^3 to (0, 2*pi) x (-pi/2, pi/2)\nParameters\n----------\npts : array_like (3,...)\n    pts[0,...], pts[1,...], and pts[2,...] are x,y, and z coordinates\nReturns\n-------\nout : ndarray (2,...)\n    pts transformed from x,y,z to longitude,latitude\n\"\"\"\n", "func_signal": "def angles_from_sphere(pts):\n", "code": "pts = np.asarray(pts) #turn into an ndarray if necessary\nx,y,z = pts[0], pts[1], pts[2]\nout = np.empty((2,) + pts.shape[1:])\n#longitude:\nout[0] = np.arctan2(y,x)\nout[0] %= 2*pi #wrap negative values around the circle to positive\n#latitude:\nr = np.hypot(x,y)\nout[1] = np.arctan2(z,r)\nreturn out", "path": "sphere_transforms_numpy.py", "repo_name": "henryseg/spherical_image_editing", "stars": 138, "license": "None", "language": "python", "size": 12639}
{"docstring": "\"\"\"p and q are pixel coordinate points, return SL(2,C) matrix rotating by angle theta around the axis from p to q\"\"\"\n", "func_signal": "def rotate_around_axis_pixel_coords_p_q(p,q,theta,size):\n", "code": "p = sphere_from_pixel_coords(p,size)\nq = sphere_from_pixel_coords(q,size)\nreturn rotate_around_axis_sphere_points_p_q(p,q,theta)", "path": "sphere_transforms_numpy.py", "repo_name": "henryseg/spherical_image_editing", "stars": 138, "license": "None", "language": "python", "size": 12639}
{"docstring": "\"\"\"returns SL(2,C) matrix that sends the three pixel coordinate points a1,b1,c1 to a2,b2,c2\"\"\"\n#convert to sphereical coordinates\n", "func_signal": "def three_points_to_three_points_pixel_coords(p1,q1,r1,p2,q2,r2,size):\n", "code": "p1,q1,r1,p2,q2,r2 = [CP1_from_sphere( sphere_from_pixel_coords(point, size = size) ) for point in [p1,q1,r1,p2,q2,r2]]\nreturn two_triples_to_SL(p1,q1,r1,p2,q2,r2)", "path": "sphere_transforms_numpy.py", "repo_name": "henryseg/spherical_image_editing", "stars": 138, "license": "None", "language": "python", "size": 12639}
{"docstring": "\"\"\"map from pixel coordinates to (0, 2*pi) x (-pi/2, pi/2) rectangle\nParameters\n----------\npts : array_like (2,...)\n    pts[0,...], and pts[1,...] are row, column image coordinates in the ranges (0,size[0]) and (0,size[1]), respectively\nsize : tuple\n    rectangular image size (rows, columns)\n\nReturns\n-------\nout : ndarray, same shape as pts\n    pts transformed into (0,2pi) x (-pi/2,pi/2) range (and transposed)\n\"\"\"\n", "func_signal": "def angles_from_pixel_coords(pts, size):\n", "code": "pts = np.asarray(pts,dtype=np.float64) #turn into an ndarray if necessary\nys,xs = size #row,col indexing\nout = np.empty_like(pts)\nout[0] = (pts[1]+0.5)*2*pi/float(xs)     #+0.5 shift for pixel coords lining up properly\nout[1] = pts[0]*pi/float(ys-1) - 0.5*pi\nreturn out", "path": "sphere_transforms_numpy.py", "repo_name": "henryseg/spherical_image_editing", "stars": 138, "license": "None", "language": "python", "size": 12639}
{"docstring": "\"\"\"p and q are points on sphere, return SL(2,C) matrix rotating by angle theta around the axis from p to q\"\"\"\n", "func_signal": "def rotate_around_axis_sphere_points_p_q(p,q,theta):\n", "code": "assert dot(p,q) < 0.9999, \"axis points should not be in the same place!\"\nCP1p, CP1q = CP1_from_sphere(p), CP1_from_sphere(q)\nr = get_vector_perp_to_p_and_q(p,q)\nCP1r = CP1_from_sphere(r)\nM_std = two_triples_to_SL(CP1p, CP1q, CP1r, [0,1], [1,0], [1,1])\nM_th = np.array([[complex(np.cos(theta), np.sin(theta)),0],[0,1]]) #rotate on axis through 0, inf by theta\nreturn np.dot(np.linalg.lstsq(M_std, M_th)[0], M_std) # inv(M_std) @ M_th @ M_std", "path": "sphere_transforms_numpy.py", "repo_name": "henryseg/spherical_image_editing", "stars": 138, "license": "None", "language": "python", "size": 12639}
{"docstring": "\"\"\"p and q are distinct points on sphere, return a unit vector perpendicular to both\"\"\"\n", "func_signal": "def get_vector_perp_to_p_and_q(p,q):\n", "code": "if abs(np.dot(p,q)+1) < 0.0001: #awkward case when p and q are antipodal on the sphere\n    if abs(np.dot(p, [1,0,0])) > 0.9999: #p is parallel to (1,0,0)\n        return np.array([0,1,0])\n    else:\n        return normalize_vectors(np.cross(p, [1,0,0]))\nelse:\n    return normalize_vectors(np.cross(p,q))", "path": "sphere_transforms_numpy.py", "repo_name": "henryseg/spherical_image_editing", "stars": 138, "license": "None", "language": "python", "size": 12639}
{"docstring": "\"\"\"p and q are pixel coordinate points, return SL(2,C) matrix rotating image of p to image of q in CP^1\"\"\"\n", "func_signal": "def rotate_pixel_coords_p_to_q(p, q, size):\n", "code": "p = sphere_from_pixel_coords(p,size)\nq = sphere_from_pixel_coords(q,size)\nreturn rotate_sphere_points_p_to_q(p,q)", "path": "sphere_transforms_numpy.py", "repo_name": "henryseg/spherical_image_editing", "stars": 138, "license": "None", "language": "python", "size": 12639}
{"docstring": "\"\"\"map from sphere in R^3 to CP^1\"\"\"\n", "func_signal": "def CP1_from_sphere(pts):\n", "code": "pts = np.asarray(pts)\nout = np.empty((2,)+pts.shape[1:],dtype=np.complex128)\nx,y,z = pts[0],pts[1],pts[2]\nmask = z<0\nout[0] = np.where(mask, x + 1j*y, 1 + z)\nout[1] = np.where(mask, 1 - z, x - 1j*y)\nreturn out", "path": "sphere_transforms_numpy.py", "repo_name": "henryseg/spherical_image_editing", "stars": 138, "license": "None", "language": "python", "size": 12639}
{"docstring": "\"\"\"matrix multiplication\"\"\"\n", "func_signal": "def matrix_mult(matrix1,matrix2):\n", "code": "m1, m2 = matrix1, matrix2\nassert len(m1[0]) == len(m2), 'Matrices must be m*n and n*p to multiply!'\n\nnew_matrix = zero_matrix(len(m1),len(m2[0]))\nfor i in range(len(m1)):\n  for j in range(len(m2[0])):\n    for k in range(len(m2)):\n      new_matrix[i][j] += m1[i][k]*m2[k][j]\nreturn new_matrix", "path": "vectors_and_matrices.py", "repo_name": "henryseg/spherical_image_editing", "stars": 138, "license": "None", "language": "python", "size": 12639}
{"docstring": "\"\"\"returns SL(2,C) matrix that sends the three CP^1 points a1,b1,c1 to a2,b2,c2\"\"\"\n", "func_signal": "def two_triples_to_SL(a1,b1,c1,a2,b2,c2):\n", "code": "M1 = inf_zero_one_to_triple(a1,b1,c1)\nM2 = inf_zero_one_to_triple(a2,b2,c2)\n# solve M1.T @ X.T = M2.T  => X = M2 @ inv(M1)\nreturn np.linalg.lstsq(M1.T, M2.T)[0].T", "path": "sphere_transforms_numpy.py", "repo_name": "henryseg/spherical_image_editing", "stars": 138, "license": "None", "language": "python", "size": 12639}
{"docstring": "\"\"\"map from CP^1 to sphere in R^3\"\"\"\n", "func_signal": "def sphere_from_CP1(pts):\n", "code": "pts = np.asarray(pts)\nout = np.empty((3,) + pts.shape[1:])\nz1,z2 = pts[0],pts[1]\nmask = abs(z2) > abs(z1)\nz = np.where(mask, z1/z2, np.conj(z2/z1))\nx,y = np.real(z), np.imag(z)\ndenom = 1 + x**2 + y**2\nout[0] = 2*x/denom\nout[1] = 2*y/denom\nout[2] = (denom-2)/denom*(2*mask-1)  #negate where mask is false\nreturn out", "path": "sphere_transforms_numpy.py", "repo_name": "henryseg/spherical_image_editing", "stars": 138, "license": "None", "language": "python", "size": 12639}
{"docstring": "\"\"\"clamp to the size of the input, including wrapping around in the x direction\"\"\"\n", "func_signal": "def clamp(pts, size):\n", "code": "ys,xs = size\npts = np.asarray(pts)\nout = np.empty_like(pts)\nout[0] = np.clip(pts[0],0,ys-1)\nout[1] = pts[1] % xs\nreturn out", "path": "sphere_transforms_numpy.py", "repo_name": "henryseg/spherical_image_editing", "stars": 138, "license": "None", "language": "python", "size": 12639}
{"docstring": "\"\"\"return SL(2,C) matrix that sends the three points infinity, zero, one to given input points p,q,r\"\"\"\n", "func_signal": "def inf_zero_one_to_triple(p,q,r):\n", "code": "M = np.vstack((p,q)).T\nmu,lam = np.linalg.lstsq(M,r)[0]\nreturn np.vstack((mu*p,lam*q)).T", "path": "sphere_transforms_numpy.py", "repo_name": "henryseg/spherical_image_editing", "stars": 138, "license": "None", "language": "python", "size": 12639}
{"docstring": "\"\"\"given pts in integers, get pixel colour on the source image as a vector in the colour cube\"\"\"\n", "func_signal": "def get_pixel_color(pts, s_im, size):\n", "code": "pts = clamp(pts,size)\ns_im = np.asarray(s_im)\nreturn s_im[pts[0], pts[1]]", "path": "sphere_transforms_numpy.py", "repo_name": "henryseg/spherical_image_editing", "stars": 138, "license": "None", "language": "python", "size": 12639}
{"docstring": "'''ReLU.\n\nalpha: slope of negative section.\n'''\n", "func_signal": "def relu(x, alpha=0., max_value=None):\n", "code": "negative_part = tf.nn.relu(-x)\nx = tf.nn.relu(x)\nif max_value is not None:\n    x = tf.clip_by_value(x, tf.cast(0., dtype=_FLOATX),\n                         tf.cast(max_value, dtype=_FLOATX))\nx -= tf.constant(alpha, dtype=_FLOATX) * negative_part\nreturn x", "path": "tensorflow_backend.py", "repo_name": "anayebi/keras-extra", "stars": 155, "license": "None", "language": "python", "size": 31}
{"docstring": "'''Collapse first two dimensions consisting of \nnum_samples and num_timesteps of 5D tensor to a 4D tensor \nwith its first dimension being num_samples * num_timesteps.\n'''\n", "func_signal": "def collapsetime(x):\n", "code": "newshape = [np.prod(x.get_shape()[0:2].as_list()),] + x.get_shape()[2:].as_list()\nx = tf.reshape(x, newshape)\nreturn x", "path": "tensorflow_backend.py", "repo_name": "anayebi/keras-extra", "stars": 155, "license": "None", "language": "python", "size": 31}
{"docstring": "'''Reshape 4D tensor y to a 5D tensor with the original\nnum_samples and num_timesteps dimensions of \n5D tensor x. Inverse operation as collapsetime(x).\n'''\n", "func_signal": "def expandtime(x, y):\n", "code": "newshape = (x.shape[0], x.shape[1], y.shape[1], y.shape[2], y.shape[3])\nz = T.reshape(y, newshape)\nreturn z", "path": "theano_backend.py", "repo_name": "anayebi/keras-extra", "stars": 155, "license": "None", "language": "python", "size": 31}
{"docstring": "'''condition: scalar tensor.\n'''\n", "func_signal": "def switch(condition, then_expression, else_expression):\n", "code": "return tf.python.control_flow_ops.cond(condition,\n                                       lambda: then_expression,\n                                       lambda: else_expression)", "path": "tensorflow_backend.py", "repo_name": "anayebi/keras-extra", "stars": 155, "license": "None", "language": "python", "size": 31}
{"docstring": "'''Collapse first two dimensions consisting of \nnum_samples and num_timesteps of 5D tensor to a 4D tensor \nwith its first dimension being num_samples * num_timesteps.\n'''\n", "func_signal": "def collapsetime(x):\n", "code": "newshape = (x.shape[0]*x.shape[1], x.shape[2], x.shape[3], x.shape[4])\nreturn T.reshape(x, newshape)", "path": "theano_backend.py", "repo_name": "anayebi/keras-extra", "stars": 155, "license": "None", "language": "python", "size": 31}
{"docstring": "'''Instantiate an input data placeholder variable.\n'''\n", "func_signal": "def placeholder(shape=None, ndim=None, dtype=_FLOATX, name=None):\n", "code": "if shape is None and ndim is None:\n    raise Exception('Specify either a shape or ndim value.')\nif shape is not None:\n    ndim = len(shape)\nif ndim == 0:\n    return T.scalar(name=name, dtype=dtype)\nelif ndim == 1:\n    return T.vector(name=name, dtype=dtype)\nelif ndim == 2:\n    return T.matrix(name=name, dtype=dtype)\nelif ndim == 3:\n    return T.tensor3(name=name, dtype=dtype)\nelif ndim == 4:\n    return T.tensor4(name=name, dtype=dtype)\nelif ndim == 5:\n    dtensor5 = T.TensorType('float32', (False,)*5)\n    return dtensor5()\nelse:\n    raise Exception('ndim too large: ' + str(ndim))", "path": "theano_backend.py", "repo_name": "anayebi/keras-extra", "stars": 155, "license": "None", "language": "python", "size": 31}
{"docstring": "'''Return number of scalars in a tensor.\n'''\n", "func_signal": "def count_params(x):\n", "code": "shape = x.get_shape()\nreturn np.prod([shape[i]._value for i in range(len(shape))])", "path": "tensorflow_backend.py", "repo_name": "anayebi/keras-extra", "stars": 155, "license": "None", "language": "python", "size": 31}
{"docstring": "'''Bitwise reduction (logical OR).\n\nReturn array of int8 (0s and 1s).\n'''\n", "func_signal": "def any(x, axis=None, keepdims=False):\n", "code": "if axis is not None and axis < 0:\n    axis = axis % len(x.get_shape())\nx = tf.cast(x, tf.bool)\nx = tf.reduce_any(x, reduction_indices=axis, keep_dims=keepdims)\nreturn tf.cast(x, tf.int8)", "path": "tensorflow_backend.py", "repo_name": "anayebi/keras-extra", "stars": 155, "license": "None", "language": "python", "size": 31}
{"docstring": "'''Instantiate a tensor variable.\n'''\n", "func_signal": "def variable(value, dtype=_FLOATX, name=None):\n", "code": "value = np.asarray(value, dtype=dtype)\nreturn theano.shared(value=value, name=name, strict=False)", "path": "theano_backend.py", "repo_name": "anayebi/keras-extra", "stars": 155, "license": "None", "language": "python", "size": 31}
{"docstring": "'''Turn a n-D tensor into a 2D tensor where\nthe first dimension is conserved.\n'''\n", "func_signal": "def flatten(x):\n", "code": "x = tf.reshape(x, [-1, np.prod(x.get_shape()[1:].as_list())])\nreturn x", "path": "tensorflow_backend.py", "repo_name": "anayebi/keras-extra", "stars": 155, "license": "None", "language": "python", "size": 31}
{"docstring": "'''Pad the middle dimension of a 3D tensor\nwith \"padding\" zeros left and right.\n'''\n", "func_signal": "def temporal_padding(x, padding=1):\n", "code": "pattern = [[0, 0], [padding, padding], [0, 0]]\nreturn tf.pad(x, pattern)", "path": "tensorflow_backend.py", "repo_name": "anayebi/keras-extra", "stars": 155, "license": "None", "language": "python", "size": 31}
{"docstring": "'''Pad the 2nd and 3rd dimensions of a 4D tensor\nwith \"padding[0]\" and \"padding[1]\" (resp.) zeros left and right.\n'''\n", "func_signal": "def spatial_2d_padding(x, padding=(1, 1), dim_ordering='th'):\n", "code": "if dim_ordering == 'th':\n    pattern = [[0, 0], [0, 0],\n               [padding[0], padding[0]], [padding[1], padding[1]]]\nelse:\n    pattern = [[0, 0],\n               [padding[0], padding[0]], [padding[1], padding[1]],\n               [0, 0]]\nreturn tf.pad(x, pattern)", "path": "tensorflow_backend.py", "repo_name": "anayebi/keras-extra", "stars": 155, "license": "None", "language": "python", "size": 31}
{"docstring": "'''Reshape 4D tensor y to a 5D tensor with the original\nnum_samples and num_timesteps dimensions of \n5D tensor x. Inverse operation as collapsetime(x).\n'''\n", "func_signal": "def expandtime(x, y):\n", "code": "newshape = x.get_shape()[0:2].as_list() + y.get_shape()[1:].as_list()\nz = tf.reshape(y, newshape)\nreturn z", "path": "tensorflow_backend.py", "repo_name": "anayebi/keras-extra", "stars": 155, "license": "None", "language": "python", "size": 31}
{"docstring": "'''Repeat a 2D tensor:\n\nif x has shape (samples, dim) and n=2,\nthe output will have shape (samples, 2, dim)\n'''\n", "func_signal": "def repeat(x, n):\n", "code": "tensors = [x] * n\nstacked = T.stack(*tensors)\nreturn stacked.dimshuffle((1, 0, 2))", "path": "theano_backend.py", "repo_name": "anayebi/keras-extra", "stars": 155, "license": "None", "language": "python", "size": 31}
{"docstring": "'''Note: tf.nn.softmax_cross_entropy_with_logits\nexpects logits, Keras expects probabilities.\n'''\n", "func_signal": "def categorical_crossentropy(output, target, from_logits=False):\n", "code": "if not from_logits:\n    # scale preds so that the class probas of each sample sum to 1\n    output /= tf.reduce_sum(output,\n                            reduction_indices=len(output.get_shape())-1,\n                            keep_dims=True)\n    # manual computation of crossentropy\n    output = tf.clip_by_value(output, tf.cast(_EPSILON, dtype=_FLOATX),\n                              tf.cast(1.-_EPSILON, dtype=_FLOATX))\n    return - tf.reduce_sum(target * tf.log(output),\n                           reduction_indices=len(output.get_shape())-1)\nelse:\n    return tf.nn.softmax_cross_entropy_with_logits(output, target)", "path": "tensorflow_backend.py", "repo_name": "anayebi/keras-extra", "stars": 155, "license": "None", "language": "python", "size": 31}
{"docstring": "'''Turn a n-D tensor into a 2D tensor where\nthe first dimension is conserved.\n'''\n", "func_signal": "def flatten(x):\n", "code": "x = T.reshape(x, (x.shape[0], T.prod(x.shape) // x.shape[0]))\nreturn x", "path": "theano_backend.py", "repo_name": "anayebi/keras-extra", "stars": 155, "license": "None", "language": "python", "size": 31}
{"docstring": "'''Pad the middle dimension of a 3D tensor\nwith \"padding\" zeros left and right.\n\nAppologies for the inane API, but Theano makes this\nreally hard.\n'''\n", "func_signal": "def temporal_padding(x, padding=1):\n", "code": "input_shape = x.shape\noutput_shape = (input_shape[0],\n                input_shape[1] + 2 * padding,\n                input_shape[2])\noutput = T.zeros(output_shape)\nreturn T.set_subtensor(output[:, padding:x.shape[1] + padding, :], x)", "path": "theano_backend.py", "repo_name": "anayebi/keras-extra", "stars": 155, "license": "None", "language": "python", "size": 31}
{"docstring": "'''Turn an n-D tensor into a 3D tensor where\nthe first two dimensions are conserved.\n'''\n", "func_signal": "def tdflatten(x):\n", "code": "x = tf.reshape(x, [-1, np.prod(x.get_shape()[2:].as_list())])\nreturn x", "path": "tensorflow_backend.py", "repo_name": "anayebi/keras-extra", "stars": 155, "license": "None", "language": "python", "size": 31}
{"docstring": "'''Transpose dimensions.\n\npattern should be a tuple or list of\ndimension indices, e.g. [0, 2, 1].\n'''\n", "func_signal": "def permute_dimensions(x, pattern):\n", "code": "pattern = tuple(pattern)\nreturn x.dimshuffle(pattern)", "path": "theano_backend.py", "repo_name": "anayebi/keras-extra", "stars": 155, "license": "None", "language": "python", "size": 31}
{"docstring": "'''Sum of the values in a tensor, alongside the specified axis.\n'''\n", "func_signal": "def sum(x, axis=None, keepdims=False):\n", "code": "if axis is not None and axis < 0:\n    axis = axis % len(x.get_shape())\nreturn tf.reduce_sum(x, reduction_indices=axis, keep_dims=keepdims)", "path": "tensorflow_backend.py", "repo_name": "anayebi/keras-extra", "stars": 155, "license": "None", "language": "python", "size": 31}
{"docstring": "# figure out paths based on information provided\n# returns discovered or specified machonet protocol version.\n\n# Check EVE root for sharedcache and cache folders:\n# This provides a way to run reverence on a host that does not have\n# a proper EVE install by just making sure the right folder/file\n# hierarchy is present in the specified installation folder.\n", "func_signal": "def _discover(self, server_name, server_ip, protocol):\n", "code": "if self.cache is None:\n\t_path = os.path.join(self.root, \"cache\")\n\tif os.path.exists(_path):\n\t\tself.cache = self.root\n\nif self.sharedcache is None:\n\t_path = os.path.join(self.root, \"SharedCache\")\n\tif os.path.exists(_path):\n\t\tself.sharedcache = _path\n\n# do platform specific discovery\nif os.name == \"nt\":\n\tself.__discover_windows(server_name)\n\nelif sys.platform == \"darwin\" or os.name == \"mac\":\n\tif self.wineprefix is None:\n\t\tself.__discover_mac(server_name)\n\telse:\n\t\tself.__discover_linux(server_name)\n\nelif os.name in (\"posix\", \"linux2\"):\n\tself.__discover_linux(server_name)\n\n\nself.bulkdata = os.path.join(self.root, 'bulkdata')\n\n\n# cache is optional for Reverence, it does not require this\n# path unless eve.RemoteSvc() calls are used or access to settings is\n# needed.\nif self.cache is not None:\n\tself._cache = os.path.join(self.cache, 'cache')\n\tself.settings = os.path.join(self.cache, 'settings')\n\n\t_bulkdata_updates = os.path.join(self._cache, 'bulkdata')\n\t_machocache = os.path.join(self._cache, 'MachoNet', server_ip)\n\n\t# discover protocol version if not set\n\tif protocol is None:\n\t\tprotocol = _get_protocol(_machocache, _bulkdata_updates)\n\t\t#if self._protocol is None:\n\t\t#\traise RuntimeError(\"Could not determine MachoNet protocol version.\")\n\n\t# these folders can only be known if we have a protocol version.\n\t# if a protocol version was specified, the relevant folders *MUST*\n\t# exist.\n\tif protocol is not None:\n\t\tself.machocache = os.path.join(_machocache, str(protocol))\n\t\tself.bulkdata_updates = os.path.join(_bulkdata_updates, str(protocol))\n\n\t\tif not os.path.exists(self.machocache):\n\t\t\traise RuntimeError(\"Specified protocol version (%d) not found in %s\" % (protocol, _machocache))\n\nreturn protocol", "path": "src\\discover.py", "repo_name": "ntt/reverence", "stars": 150, "license": "other", "language": "python", "size": 535}
{"docstring": "# purge all loaded tables\n\n", "func_signal": "def release(self):\n", "code": "for tableList in (self.tables, (\"_averageMarketPrice\",)):\n\tfor tableName in tableList:\n\t\ttry:\n\t\t\tdelattr(self, tableName)\n\t\texcept AttributeError:\n\t\t\tpass\n\nself.cache._time_load = 0.0\nself._attrCache = {}", "path": "src\\config.py", "repo_name": "ntt/reverence", "stars": 150, "license": "other", "language": "python", "size": 535}
{"docstring": "\"\"\"Sorts the rowset in place.\"\"\"\n", "func_signal": "def SortBy(self, column, reverse=False):\n", "code": "ix = self.header.index(column)\nself.sort(key=lambda e: e[ix], reverse=reverse)", "path": "src\\eve\\common\\script\\sys\\rowset.py", "repo_name": "ntt/reverence", "stars": 150, "license": "other", "language": "python", "size": 535}
{"docstring": "# returns string that is safe to use in XML\n", "func_signal": "def xmlstr(value):\n", "code": "t = type(value)\nif t in (list, tuple, dict):\n\traise ValueError(\"Unsupported type\")\nif t is str:\n\treturn repr(value.replace('&','&amp;').replace('<','&lt;').replace('>','&gt;').replace('\"','&quot;').replace(\"'\",'&apos;'))[1:-1]\nelif t is unicode:\n\treturn repr(value.replace('&','&amp;').replace('<','&lt;').replace('>','&gt;').replace('\"','&quot;').replace(\"'\",'&apos;'))[2:-1]\nelif t == float:\n\treturn value\nreturn repr(value)", "path": "examples\\datadump.py", "repo_name": "ntt/reverence", "stars": 150, "license": "other", "language": "python", "size": 535}
{"docstring": "\"\"\"Returns a sorted shallow copy of the rowset.\"\"\"\n", "func_signal": "def SortedBy(self, column, reverse=False):\n", "code": "rs = self.IndexedBy(column)\nrs.SortBy(column, reverse)\nreturn rs", "path": "src\\eve\\common\\script\\sys\\rowset.py", "repo_name": "ntt/reverence", "stars": 150, "license": "other", "language": "python", "size": 535}
{"docstring": "\"\"\"Returns (cached) dictionary of attributes for specified type.\"\"\"\n", "func_signal": "def GetTypeAttrDict(self, typeID):\n", "code": "attr = self._attrCache.get(typeID, None)\nif attr is None:\n\tself._attrCache[typeID] = attr = {}\n\tfor row in self.dgmtypeattribs[typeID]:\n\t\tattr[row.attributeID] = row.value\nreturn attr", "path": "src\\config.py", "repo_name": "ntt/reverence", "stars": 150, "license": "other", "language": "python", "size": 535}
{"docstring": "# Creates a closure used as a method in Config class (to be decorated with\n# _memoize) that loads a specific bulkdata table.\n", "func_signal": "def _loader(attrName):\n", "code": "def method(self):\n\tentry = self._tables[attrName]\n\tif len(entry) == 6:\n\t\t# bulkID loader\n\t\tver, rem, storageClass, rowClass, primaryKey, bulkID = entry\n\t\treturn self._loadbulkdata(tableName=attrName, storageClass=storageClass, rowClass=rowClass, primaryKey=primaryKey, bulkID=bulkID)\n\n\tif len(entry) == 4:\n\t\t# FSD loader\n\t\tver, rem, (staticName, schemaName, optimize), cacheNum = entry\n\t\treturn self._loadfsddata(staticName, schemaName, cacheNum, optimize=optimize)\n\n\tif len(entry) == 3:\n\t\t# FSDLite loader\n\t\tver, rem, (dbfilename, rowClass) = entry\n\t\t#return FSDLiteStorage(os.path.join(self.eve.paths.root, \"bin\", \"staticdata\", dbfilename), rowClass)\n\t\treturn FSDLiteStorage(self.eve.ResFile().resolvepath(\"res:/staticdata/%s\" % dbfilename), rowClass)\n\t\t\nmethod.func_name = attrName\nreturn method", "path": "src\\config.py", "repo_name": "ntt/reverence", "stars": 150, "license": "other", "language": "python", "size": 535}
{"docstring": "# locates a global. used by marshal.Load and integrated unpickler\n\n# compatibility\n", "func_signal": "def _find_global(module, name):\n", "code": "if module in (\"util\", \"utillib\") and name == \"KeyVal\":\n\treturn KeyVal\ntry:\n\tm = __import__(module, globals(), locals(), (), -1)\nexcept ImportError:\n\traise RuntimeError(\"Unable to locate object: \" + module + \".\" + name + \" (import failed)\")\n\ntry:\n\treturn getattr(m, name)\nexcept AttributeError:\n\traise RuntimeError(\"Unable to locate object: \" + module + \".\" + name + \" (not in module)\")", "path": "src\\blue.py", "repo_name": "ntt/reverence", "stars": 150, "license": "other", "language": "python", "size": 535}
{"docstring": "\"\"\"Loads the tables named in the tables sequence. If no tables are\nspecified, it will load all supported ones. A callback function can be provided\nwhich will be called as func(current, total, tableName).\n\nThis method should be used in the following situations:\n- The ConfigMgr instance is going to be used in a multi-threaded app.\n- The Application wants to load all data at once instead of on access.\n\"\"\"\n\n", "func_signal": "def prime(self, tables=None, callback=None, debug=False, onlyFSD=False):\n", "code": "if debug:\n\tself._debug = True\n\tstart = time.clock()\n\tprint >>sys.stderr, \"LOADING STATIC DATABASE\"\n\tprint >>sys.stderr, \"  machoCachePath:\", self.eve.paths.machocache\n\tprint >>sys.stderr, \"  machoVersion:\", self.eve.paths.protocol\n\tprint >>sys.stderr, \"  bulk system path:\", self.eve.paths.bulkdata\n\tprint >>sys.stderr, \"  bulk cache path:\", self.eve.paths.bulkdata_updates\ntry:\n\tif tables is None:\n\t\t# preload everything.\n\t\ttables = self.tables\n\telse:\n\t\t# preload specified only.\n\t\ttables = frozenset(tables)\n\t\tinvalid = tables - self.tables\n\t\tif invalid:\n\t\t\traise ValueError(\"Unknown table(s): %s\" % \", \".join(invalid))\n\n\tcurrent = 0\n\ttotal = len(tables)\n\n\tfor tableName in tables:\n\t\tif onlyFSD and len(self._tables[tableName]) != 4:\n\t\t\tcontinue\n\n\t\tif callback:\n\t\t\tcallback(current, total, tableName)\n\t\t\tcurrent += 1\n\n\t\tif debug:\n\t\t\tprint >>sys.stderr, \"  priming:\", tableName\n\t\t# now simply trigger the property's getters\n\t\tgetattr(self, tableName)\n\nfinally:\n\tif debug:\n\t\tself._debug = False\n\nif debug:\n\tt = time.clock() - start\n\tprint >>sys.stderr, \"Priming took %ss (of which %.4f decoding)\" % (t, self.cache._time_load)", "path": "src\\config.py", "repo_name": "ntt/reverence", "stars": 150, "license": "other", "language": "python", "size": 535}
{"docstring": "# row keys:\n# ['celestialID', 'celestialNameID', 'solarSystemID', 'typeID', 'groupID', 'radius', 'x', 'y', 'z', 'orbitID', 'orbitIndex', 'celestialIndex']\n\n", "func_signal": "def GetCelestialNameFromLocalRow(self, row):\n", "code": "celestialGroupID = row['groupID']\ncelestialNameID = row['celestialNameID']\n\nif celestialNameID is not None and celestialGroupID != const.groupStargate:\n\tlabel = 'UI/Util/GenericText'\n\tparam = {'text': celestialNameID}\nelif celestialGroupID == const.groupAsteroidBelt:\n\tlabel = 'UI/Locations/LocationAsteroidBeltFormatter'\n\tparam = {'solarSystemID': row['solarSystemID'], 'romanCelestialIndex': util.IntToRoman(row['celestialIndex']), 'typeID': row['typeID'], 'orbitIndex': row['orbitIndex']}\nelif celestialGroupID == const.groupMoon:\n\tlabel = 'UI/Locations/LocationMoonFormatter'\n\tparam = {'solarSystemID': row['solarSystemID'], 'romanCelestialIndex': util.IntToRoman(row['celestialIndex']), 'orbitIndex': row['orbitIndex']}\nelif celestialGroupID == const.groupPlanet:\n\tlabel = 'UI/Locations/LocationPlanetFormatter'\n\tparam = {'solarSystemID': row['solarSystemID'], 'romanCelestialIndex': util.IntToRoman(row['celestialIndex'])}\nelif celestialGroupID == const.groupStargate:\n\tlabel = 'UI/Locations/LocationStargateFormatter'\n\tparam = {'destinationSystemID': row['celestialNameID']}\nelif celestialGroupID == const.groupSun:\n\tlabel = 'UI/Locations/LocationStarFormatter'\n\tparam = {'solarSystemID': row['solarSystemID']}\nelse:\n\tlabel = None\n\tparam = None\n\nreturn self._localization.GetByLabel(label, **param)", "path": "src\\config.py", "repo_name": "ntt/reverence", "stars": 150, "license": "other", "language": "python", "size": 535}
{"docstring": "\"\"\"Return specified dogma attribute for given type, or default attribute value.\"\"\"\n", "func_signal": "def GetTypeAttribute2(self, typeID, attributeID):\n", "code": "attr = self.GetTypeAttrDict(typeID)\nvalue = attr.get(attributeID)\nif value is None:\n\treturn self.dgmattribs.Get(attributeID).defaultValue\nreturn value", "path": "src\\config.py", "repo_name": "ntt/reverence", "stars": 150, "license": "other", "language": "python", "size": 535}
{"docstring": "\"\"\"Return specified dogma attribute for given type, or custom default value.\"\"\"\n", "func_signal": "def GetTypeAttribute(self, typeID, attributeID, defaultValue=None):\n", "code": "attr = self.GetTypeAttrDict(typeID)\nreturn attr.get(attributeID, defaultValue)", "path": "src\\config.py", "repo_name": "ntt/reverence", "stars": 150, "license": "other", "language": "python", "size": 535}
{"docstring": "# detailed representation\n", "func_signal": "def __str__(self):\n", "code": "header = self.attributes\n_getoffset = self._get_offset\nstuff = []\n_a = stuff.append\nfor attr, schema in header.iteritems():\n\toffset = _getoffset(attr)\n\tif offset is None:\n\t\tv = schema.get(\"default\", self)\n\t\tv = \"NULL\" if v is self else repr(v)\n\telse:\n\t\tv = repr(schema['loader'](self.__data__, self.__offset__ + offset, schema))\n\t_a(v)\n\nreturn \"FSD_Object(\" + ','.join(map(u\"%s:%s\".__mod__, zip(header, stuff))) + \")\"", "path": "src\\fsd.py", "repo_name": "ntt/reverence", "stars": 150, "license": "other", "language": "python", "size": 535}
{"docstring": "# Adds required decoding information to a schema\n# THIS -MUST- BE USED ON EVERY SCHEMA (AFTER OPTIMIZING, IF ANY)\n\n", "func_signal": "def PrepareSchema(schema):\n", "code": "t = schema.get('type')\nschema['loader'] = _loaders[t]\n\nif t in _typeSizes:\n\tschema['size'] = _typeSizes[t]\n\nif t.startswith(\"vector\"):\n\tif schema.get('precision') == 'double':\n\t\tt = schema['type']+\"d\"\n\n\t# figure out correct loader.\n\tif schema.get('aliases'):\n\t\tschema['loader'] = FSD_NamedVector\n\t\tschema['unpacker'] = _vectorUnpackers[t]\n\telse:\n\t\tschema['loader'] = _loaders[t]\n\nelif t == 'list':\n\tPrepareSchema(schema['itemTypes'])\n\nelif t == 'dict':\n\tPrepareSchema(schema['keyTypes'])\n\tPrepareSchema(schema['valueTypes'])\n\n\tif 'subIndexOffsetLookup' in schema:\n\t\tPrepareSchema(schema['subIndexOffsetLookup'])\n\n\tif \"keyFooter\" in schema:\n\t\tPrepareSchema(schema['keyFooter'])\n\ttry:\n\t\tschema['header'] = schema['valueTypes']['attributes'].keys()\n\texcept KeyError:\n\t\t# apparently this info is gone from some fsd dicts in Rubicon\n\t\tschema['header'] = ()\n\nelif t == 'object':\n\tif \"endOfFixedSizeData\" not in schema:\n\t\tschema[\"endOfFixedSizeData\"] = 0\n\n\tfor key, attrschema in schema[\"attributes\"].iteritems():\n\t\tPrepareSchema(attrschema)\n\t\tattrschema['AttributeError'] = AttributeError(\"Object instance does not have attribute '%s'\" % key)\n\t\tattrschema['KeyError'] = KeyError(\"Object instance does not have attribute '%s'\" % key)\n\nelif t == 'binary':\n\tPrepareSchema(schema[\"schema\"])\n\nelif t == 'int':\n\tif schema.get(\"min\", -1) >= 0 or schema.get(\"exclusiveMin\", -2) >= -1:\n\t\tschema['loader'] = _uint32\n\nelif t == 'union':\n\tfor s in newSchema['optionTypes'].itervalues():\n\t\tPrepareSchema(s)\n\nelif t == 'float':\n\tif schema.get('precision') == 'double':\n\t\tschema['loader'] = _loaders['double']\n\nelif t == 'enum':\n\t_max = max(schema['values'].values())\n\tif _max <= 255:\n\t\t_type = struct.Struct(\"B\")\n\telif _max <= 65536:\n\t\t_type = struct.Struct(\"H\")\n\telse:\n\t\t_type = struct.Struct(\"I\")\n\n\tschema['unpacker'] = _type.unpack_from\n\tschema['size'] = _type.size\n\tif 'readEnumValue' not in schema:\n\t\tschema['readEnumValue'] = False", "path": "src\\fsd.py", "repo_name": "ntt/reverence", "stars": 150, "license": "other", "language": "python", "size": 535}
{"docstring": "\"\"\"Returns total volume of qty units of typeID. \nUses packaged volume if singleton is non-zero.\n\"\"\"\n", "func_signal": "def GetTypeVolume(self, typeID, singleton=1, qty=1):\n", "code": "if typeID == const.typePlasticWrap:\n\traise RuntimeError(\"GetTypeVolume: cannot determine volume of plastic from type alone\")\n\nrec = self.invtypes.Get(typeID)\nvolume = rec.volume\nif not singleton and typeID != const.typeBHMegaCargoShip:\n\tvolume = const.shipPackagedVolumesPerGroup.get(rec.groupID, volume)\n\nif volume != -1:\n\treturn volume * qty\n\nreturn volume", "path": "src\\config.py", "repo_name": "ntt/reverence", "stars": 150, "license": "other", "language": "python", "size": 535}
{"docstring": "\"\"\"Returns list of (requiredSkillTypeID, requiredLevel) tuples.\"\"\"\n", "func_signal": "def GetRequiredSkills(self, typeID):\n", "code": "attr = self.GetTypeAttrDict(typeID)\nreqs = []\nfor i in xrange(1, 7):\n\tskillID = attr.get(getattr(const, \"attributeRequiredSkill%s\" % i), False)\n\tif skillID:\n\t\tlvl = attr.get(getattr(const, \"attributeRequiredSkill%sLevel\" % i), None)\n\t\tif lvl is not None:\n\t\t\treqs.append((skillID, lvl))\nreturn reqs", "path": "src\\config.py", "repo_name": "ntt/reverence", "stars": 150, "license": "other", "language": "python", "size": 535}
{"docstring": "\"\"\"Returns a sorted shallow copy of the rowset.\"\"\"\n", "func_signal": "def SortedBy(self, column, reverse=False):\n", "code": "rs = Rowset(header=self.header, lines=self.lines[:], rowclass=self.RowClass, cfgInstance=self.cfg)\nrs.SortBy(column, reverse)\nreturn rs", "path": "src\\eve\\common\\script\\sys\\rowset.py", "repo_name": "ntt/reverence", "stars": 150, "license": "other", "language": "python", "size": 535}
{"docstring": "# odyssey fsd loader (uses CCP code directly)\n# deprecated in ody1.1, but still works if fsd lib is present\n", "func_signal": "def _loadfsddata_usingccplib(self, staticName, schemaName, cacheNum, optimize):\n", "code": "from . import blue as bloo\n\n# must patch our ResFile temporarily for CCP code to work.\n_rf = bloo.ResFile\nbloo.ResFile = self.eve.ResFile\n\ntry:\n\tif optimize is None:\n\t\toptimize = True\n\tstaticName = 'res:/staticdata/%s.static' % staticName\n\tschemaName = 'res:/staticdata/%s.schema' % schemaName if schemaName else None\n\treturn self._fsdBinaryLoader.LoadFSDDataForCFG(staticName, schemaName, optimize=optimize)\nfinally:\n\tbloo.ResFile = _rf", "path": "src\\config.py", "repo_name": "ntt/reverence", "stars": 150, "license": "other", "language": "python", "size": 535}
{"docstring": "\"\"\"Reads specified file in the virtual filesystem\"\"\"\n", "func_signal": "def readstuff(self, name):\n", "code": "f = _ResFile(self.rescache)\nf.Open(name)\nreturn f.read()", "path": "src\\blue.py", "repo_name": "ntt/reverence", "stars": 150, "license": "other", "language": "python", "size": 535}
{"docstring": "# Custom FileStaticData loader.\n# Grabs schema and binary blob from .stuff file.\n", "func_signal": "def _loadfsddata(self, staticName, schemaName, cacheNum, optimize):\n", "code": "res = self.eve.ResFile()\n\nschema = None\nif staticName:\n\tresFileName = \"res:/staticdata/%s.schema\" % schemaName\n\tif res.Open(resFileName):\n\t\tschema = fsd.LoadSchema(res.Read())\n\t\tif optimize:\n\t\t\tschema = fsd.OptimizeSchema(schema)\n\nresFileName = \"res:/staticdata/%s.static\" % staticName\nif not res.Open(resFileName):\n\traise RuntimeError(\"Could not load FSD static data '%s'\" % resFileName)\n\ntry:\n\t# This will throw an error if there is no embedded schema.\n\t# As it is hardcoded in EVE whether a static data file comes\n\t# with an embedded schema, we just try to load it anyway.\n\t# if it fails, the previously loaded schema should still be there.\n\tschema, offset = fsd.LoadEmbeddedSchema(res.fh)\nexcept RuntimeError:\n\toffset = 0\n\nif schema is None:\n\traise RuntimeError(\"No schema found for %s\" % tableName)\n\nfsd.PrepareSchema(schema)\n\nif schema.get('multiIndex'):\n\t# Disk-based access for multi index tables because only the\n\t# FSD_MultiIndex class can handle them properly.\n\treturn fsd.LoadIndexFromFile(res.fh, schema, cacheNum, offset=offset)\n\n# any other table will use memory-based access because they are pretty\n# small anyway, and they are considerably faster when used like this.\nreturn fsd.LoadFromString(res.Read(), schema)", "path": "src\\config.py", "repo_name": "ntt/reverence", "stars": 150, "license": "other", "language": "python", "size": 535}
{"docstring": "\"\"\"\nThe filter and pagination parameters\n\"\"\"\n", "func_signal": "def params(self):\n", "code": "if 'params' in self:\n    return self['params']\nif 'request' in self:\n    return self['request'].GET\nreturn {}", "path": "hyperadmin\\states.py", "repo_name": "zbyte64/django-hyperadmin", "stars": 148, "license": "other", "language": "python", "size": 2431}
{"docstring": "'''\nCRUD forms are assumed to operate on the active item.\nThis inserts the active instance into the form kwargs.\n'''\n", "func_signal": "def get_form_kwargs(self, **kwargs):\n", "code": "if self.state.item:\n    kwargs.setdefault('instance', self.state.item.instance)\nreturn super(CRUDResource, self).get_form_kwargs(**kwargs)", "path": "hyperadmin\\resources\\crud\\resources.py", "repo_name": "zbyte64/django-hyperadmin", "stars": 148, "license": "other", "language": "python", "size": 2431}
{"docstring": "\"\"\"\nCalled when the link is submitted. Returns a link representing the response.\n\n:rtype: Link\n\"\"\"\n", "func_signal": "def handle_submission(self, link, submit_kwargs):\n", "code": "form = link.get_form(**submit_kwargs)\nif form.is_valid():\n    instance = form.save()\n    resource_item = self.endpoint.get_resource_item(instance)\n    return self.on_success(resource_item)\nreturn link.clone(form=form)", "path": "hyperadmin\\links.py", "repo_name": "zbyte64/django-hyperadmin", "stars": 148, "license": "other", "language": "python", "size": 2431}
{"docstring": "\"\"\"\nReturns resource items that are associated with this state.\n\"\"\"\n", "func_signal": "def get_resource_items(self):\n", "code": "if self.item is not None:\n    return self.item.get_resource_items()\nreturn self.endpoint.get_resource_items()", "path": "hyperadmin\\states.py", "repo_name": "zbyte64/django-hyperadmin", "stars": 148, "license": "other", "language": "python", "size": 2431}
{"docstring": "\"\"\"\nReturns a two character representation of the link factor.\n\n* LI - Idempotent\n* LN - Non-Idempotent\n* LT - Templated link\n* LO - Outbound link\n* LI - Embedded link\n\"\"\"\n", "func_signal": "def get_link_factor(self):\n", "code": "if self.link_factor:\n    return self.link_factor\nif self._method in ('PUT', 'DELETE'):\n    return 'LI'\nif self._method == 'POST':\n    return 'LN'\nif self._method == 'GET':\n    if self.form_class:\n        return 'LT'\n    #TODO how do we determine which to return?\n    return 'LO' #link out to this content\n    return 'LE' #embed this content\nreturn 'L?'", "path": "hyperadmin\\links.py", "repo_name": "zbyte64/django-hyperadmin", "stars": 148, "license": "other", "language": "python", "size": 2431}
{"docstring": "\"\"\"\nGet the name to use for the object.\n\"\"\"\n", "func_signal": "def get_context_object_name(self):\n", "code": "if self.context_object_name:\n    return self.context_object_name\nelse:\n    return 'object'", "path": "hyperadmin\\clients\\views\\detail.py", "repo_name": "zbyte64/django-hyperadmin", "stars": 148, "license": "other", "language": "python", "size": 2431}
{"docstring": "'''\nCalled when an item has been successfully created.\nFires off the create event.\nMay return a link.\n'''\n", "func_signal": "def on_create_success(self, item):\n", "code": "self.emit_event(event='create', item_list=[item])\nreturn None", "path": "hyperadmin\\resources\\crud\\resources.py", "repo_name": "zbyte64/django-hyperadmin", "stars": 148, "license": "other", "language": "python", "size": 2431}
{"docstring": "'''\nReturns a link representing the result of the action taken.\nThe resource_item of the link may represent the updated/created object\nor in the case of a collection resource item you get access to the filter items\n'''\n", "func_signal": "def submit(self, **kwargs):\n", "code": "on_submit = self.on_submit\n\nif on_submit is None:\n    return self.follow()\nelse:\n    return on_submit(link=self, submit_kwargs=kwargs)", "path": "hyperadmin\\links.py", "repo_name": "zbyte64/django-hyperadmin", "stars": 148, "license": "other", "language": "python", "size": 2431}
{"docstring": "\"\"\"\nReturns the active form for the link. Returns None if there is no form.\n\"\"\"\n", "func_signal": "def form(self):\n", "code": "if self._form is None and self.form_class and not self.is_simple_link:\n    self._form = self.get_form()\nreturn self._form", "path": "hyperadmin\\links.py", "repo_name": "zbyte64/django-hyperadmin", "stars": 148, "license": "other", "language": "python", "size": 2431}
{"docstring": "'''\nFollows the link to the endpoint in a subrequest\nReturns a link representing the endpoint response\n'''\n", "func_signal": "def follow(self):\n", "code": "params = {\n    'url': self.get_absolute_url(),\n}\nendpoint = self.site.call_endpoint(**params)\nreturn endpoint.generate_api_response(endpoint.api_request)", "path": "hyperadmin\\links.py", "repo_name": "zbyte64/django-hyperadmin", "stars": 148, "license": "other", "language": "python", "size": 2431}
{"docstring": "\"\"\"\nReturns a link for a successful submission\n\n:rtype: Link\n\"\"\"\n", "func_signal": "def on_success(self, item=None):\n", "code": "if item is not None:\n    return item.get_link()\nreturn self.endpoint.get_resource_link()", "path": "hyperadmin\\links.py", "repo_name": "zbyte64/django-hyperadmin", "stars": 148, "license": "other", "language": "python", "size": 2431}
{"docstring": "\"\"\"\nThe HTTP method of the link\n\"\"\"\n", "func_signal": "def method(self):\n", "code": "if self.is_simple_link:\n    return 'GET'\nreturn self._method", "path": "hyperadmin\\links.py", "repo_name": "zbyte64/django-hyperadmin", "stars": 148, "license": "other", "language": "python", "size": 2431}
{"docstring": "'''\nCalled when an item has been successfully deleted.\nFires off the delete event.\nMay return a link.\n'''\n", "func_signal": "def on_delete_success(self, item):\n", "code": "self.emit_event(event='delete', item_list=[item])\nreturn None", "path": "hyperadmin\\resources\\crud\\resources.py", "repo_name": "zbyte64/django-hyperadmin", "stars": 148, "license": "other", "language": "python", "size": 2431}
{"docstring": "#include_form_params_in_url=False\n", "func_signal": "def get_base_url(self):\n", "code": "if self.get_link_factor() == 'LT' and self.include_form_params_in_url: #TODO absorb this in link._url\n    if '?' in self._url:\n        base_url, url_params = self._url.split('?', 1)\n    else:\n        base_url, url_params = self._url, ''\n    params = QueryDict(url_params, mutable=True)\n    form = self.get_form()\n    #extract get params\n    for field in form:\n        val = field.value()\n        if val is not None:\n            params[field.html_name] = val\n    return '%s?%s' % (base_url, params.urlencode())\nreturn self._url", "path": "hyperadmin\\links.py", "repo_name": "zbyte64/django-hyperadmin", "stars": 148, "license": "other", "language": "python", "size": 2431}
{"docstring": "\"\"\"\nCreates and returns the link\n\n:rtype: Link\n\"\"\"\n", "func_signal": "def get_link(self, **link_kwargs):\n", "code": "link_kwargs = self.get_link_kwargs(**link_kwargs)\nlink_class = self.get_link_class()\nlink = link_class(**link_kwargs)\nreturn link", "path": "hyperadmin\\links.py", "repo_name": "zbyte64/django-hyperadmin", "stars": 148, "license": "other", "language": "python", "size": 2431}
{"docstring": "'''\nReturns a set of native objects for a given state\n'''\n", "func_signal": "def get_instances(self):\n", "code": "if 'page' in self.state:\n    return self.state['page'].object_list\nif self.state.has_view_class('change_form'):\n    return []\nreturn self.get_primary_query()", "path": "hyperadmin\\resources\\crud\\resources.py", "repo_name": "zbyte64/django-hyperadmin", "stars": 148, "license": "other", "language": "python", "size": 2431}
{"docstring": "\"\"\"\nReturns True if this link is simply to be followed\n\"\"\"\n", "func_signal": "def is_simple_link(self):\n", "code": "if self.get_link_factor() in ('LO', 'LE'):\n    return True\nreturn False", "path": "hyperadmin\\links.py", "repo_name": "zbyte64/django-hyperadmin", "stars": 148, "license": "other", "language": "python", "size": 2431}
{"docstring": "'''\nCalled when an item has been successfully updated.\nFires off the update event.\nMay return a link.\n'''\n", "func_signal": "def on_update_success(self, item):\n", "code": "self.emit_event(event='update', item_list=[item])\nreturn None", "path": "hyperadmin\\resources\\crud\\resources.py", "repo_name": "zbyte64/django-hyperadmin", "stars": 148, "license": "other", "language": "python", "size": 2431}
{"docstring": "#if isinstance(val, File):\n#    if hasattr(val, 'name'):\n#        val = val.name\n#    else:\n#        val = None\n", "func_signal": "def prepare_field_value(self, val):\n", "code": "if isinstance(val, Link):\n    val = Link.get_absolute_url()\nreturn val", "path": "hyperadmin\\datataps.py", "repo_name": "zbyte64/django-hyperadmin", "stars": 148, "license": "other", "language": "python", "size": 2431}
{"docstring": "\"\"\"\nAdds the specified link from the resource.\nThis will only add the link if it exists and the person is allowed to view it.\n\"\"\"\n", "func_signal": "def add_link(self, link_name, **kwargs):\n", "code": "from hyperadmin.endpoints import BaseEndpoint\nif isinstance(link_name, BaseEndpoint):\n    try:\n        link_name = link_name.get_main_link_name()\n    except LinkNotAvailable:\n        return False\nif link_name not in self.link_prototypes:\n    return False\nendpoint_link = self.link_prototypes[link_name]\nif not endpoint_link.show_link(**kwargs):\n    return False\nlink = endpoint_link.get_link(**kwargs)\nself.append(link)\nreturn link", "path": "hyperadmin\\links.py", "repo_name": "zbyte64/django-hyperadmin", "stars": 148, "license": "other", "language": "python", "size": 2431}
{"docstring": "\"\"\" return the standard basic auth challenge \"\"\"\n\n", "func_signal": "def auth_challenge(self):\n", "code": "self.set_header(\"WWW-Authenticate\", \"Basic realm=pyjojo\")\nself.set_status(401)\nself.finish()", "path": "pyjojo\\handlers.py", "repo_name": "atarola/pyjojo", "stars": 137, "license": "mit", "language": "python", "size": 417}
{"docstring": "\"\"\" return an exception as an error json dict \"\"\"\n\n", "func_signal": "def write_error(self, status_code, **kwargs):\n", "code": "if kwargs['exc_info'] and hasattr(kwargs['exc_info'][1], 'log_message'):\n    message = kwargs['exc_info'][1].log_message\nelse:\n    # TODO: What should go here?\n    message = ''\n\nself.write({\n    'error': {\n        'code': status_code,\n        'type': httplib.responses[status_code],\n        'message': message\n    }\n})", "path": "pyjojo\\handlers.py", "repo_name": "atarola/pyjojo", "stars": 137, "license": "mit", "language": "python", "size": 417}
{"docstring": "\"\"\" authenticate the user \"\"\"\n\n# no passwords set, so they're good to go\n", "func_signal": "def handle_auth(self):\n", "code": "if config['passfile'] == None:\n    return\n\n# grab the auth header, returning a demand for the auth if needed\nauth_header = self.request.headers.get('Authorization')\nif (auth_header is None) or (not auth_header.startswith('Basic ')):\n    self.auth_challenge()\n    return\n\n# decode the username and password\nauth_decoded = base64.decodestring(auth_header[6:])\nusername, password = auth_decoded.split(':', 2)\n        \nif not self.is_user_authenticated(username, password):\n    self.auth_challenge()\n    return", "path": "pyjojo\\handlers.py", "repo_name": "atarola/pyjojo", "stars": 137, "license": "mit", "language": "python", "size": 417}
{"docstring": "\"\"\" get the requirements for this script \"\"\"\n\n", "func_signal": "def options(self, script_name):\n", "code": "script = self.get_script(script_name, 'options')\nself.finish({'script': script.metadata()})", "path": "pyjojo\\handlers.py", "repo_name": "atarola/pyjojo", "stars": 137, "license": "mit", "language": "python", "size": 417}
{"docstring": "\"\"\" reload the scripts from the script directory \"\"\"\n", "func_signal": "def post(self):\n", "code": "self.settings['scripts'] = create_collection(config['directory'])\nself.finish({\"status\": \"ok\"})", "path": "pyjojo\\handlers.py", "repo_name": "atarola/pyjojo", "stars": 137, "license": "mit", "language": "python", "size": 417}
{"docstring": "\"\"\" Modify orig, overlaying information from other \"\"\"\n\n", "func_signal": "def deep_merge(orig, other):\n", "code": "for key, value in other.items():\n    if key in orig and isinstance(orig[key], dict) and isinstance(value, dict):\n        deep_merge(orig[key], value)\n    else:\n        orig[key] = value", "path": "pyjojo\\config.py", "repo_name": "atarola/pyjojo", "stars": 137, "license": "mit", "language": "python", "size": 417}
{"docstring": "\"\"\" setup the logging system \"\"\"\n\n", "func_signal": "def setup_logging():\n", "code": "base_log = logging.getLogger()\nhandler = logging.StreamHandler(sys.stdout)\nhandler.setFormatter(logging.Formatter(\"%(asctime)s %(levelname)s [%(filename)s:%(lineno)d] %(message)s\"))\nbase_log.addHandler(handler)\nbase_log.setLevel(logging.DEBUG)\nreturn handler", "path": "pyjojo\\util.py", "repo_name": "atarola/pyjojo", "stars": 137, "license": "mit", "language": "python", "size": 417}
{"docstring": "\"\"\" get the requirements for all of the scripts \"\"\"\n       \n", "func_signal": "def get(self):\n", "code": "tags = {'tags': [], 'not_tags': [], 'any_tags': []}\n\nfor tag_arg in ['tags', 'not_tags', 'any_tags']:\n    try:\n        tags[tag_arg] = self.get_arguments(tag_arg)[0].split(',')\n        break\n    except IndexError:\n        continue\n\nself.finish({'scripts': self.settings['scripts'].metadata(tags)})", "path": "pyjojo\\handlers.py", "repo_name": "atarola/pyjojo", "stars": 137, "license": "mit", "language": "python", "size": 417}
{"docstring": "\"\"\" run the script \"\"\"\n        \n", "func_signal": "def put(self, script_name):\n", "code": "if config['force_json']:\n    self.set_header(\"Content-Type\", \"application/json; charset=UTF-8\")\n\nscript = self.get_script(script_name, 'put')\n\nif script.output == 'combined':\n    retcode, stdout = yield gen.Task(script.execute, self.params)\n    self.finish({\n        \"stdout\": stdout,\n        \"return_values\": self.find_return_values(stdout),\n        \"retcode\": retcode,\n    })\nelse:\n    retcode, stdout, stderr = yield gen.Task(script.execute, self.params)\n    self.finish({\n        \"stdout\": stdout,\n        \"stderr\": stderr,\n        \"return_values\": self.find_return_values(stdout),\n        \"retcode\": retcode\n    })", "path": "pyjojo\\handlers.py", "repo_name": "atarola/pyjojo", "stars": 137, "license": "mit", "language": "python", "size": 417}
{"docstring": "# import the handler file, this will fill out the route.get_routes() call.\n", "func_signal": "def create_application(debug):\n", "code": "import pyjojo.handlers\n\napplication = tornado.web.Application(\n    route.get_routes(), \n    scripts=create_collection(config['directory']),\n    debug=debug\n)\n\nreturn application", "path": "pyjojo\\util.py", "repo_name": "atarola/pyjojo", "stars": 137, "license": "mit", "language": "python", "size": 417}
{"docstring": "\"\"\" parse output array for return values \"\"\"\n\n", "func_signal": "def find_return_values(self, output):\n", "code": "return_values = {}\nfor line in output:\n    if line.startswith('jojo_return_value'):\n        temp = line.replace(\"jojo_return_value\",\"\").strip()\n        key, value = [item.strip() for item in temp.split('=')]\n        return_values[key] = value\n\nreturn return_values", "path": "pyjojo\\handlers.py", "repo_name": "atarola/pyjojo", "stars": 137, "license": "mit", "language": "python", "size": 417}
{"docstring": "\"\"\" if we get a dict, automatically change it to json and set the content-type \"\"\"\n\n", "func_signal": "def write(self, chunk):\n", "code": "if isinstance(chunk, dict):\n    chunk = json.dumps(chunk)\n    self.set_header(\"Content-Type\", \"application/json; charset=UTF-8\")\n\nsuper(BaseHandler, self).write(chunk)", "path": "pyjojo\\handlers.py", "repo_name": "atarola/pyjojo", "stars": 137, "license": "mit", "language": "python", "size": 417}
{"docstring": "\"\"\" run the script \"\"\"\n        \n", "func_signal": "def post(self, script_name):\n", "code": "if config['force_json']:\n    self.set_header(\"Content-Type\", \"application/json; charset=UTF-8\")\n\nscript = self.get_script(script_name, 'post')\n\nif script.output == 'combined':\n    retcode, stdout = yield gen.Task(script.execute, self.params)\n    self.finish({\n        \"stdout\": stdout,\n        \"return_values\": self.find_return_values(stdout),\n        \"retcode\": retcode\n    })\nelse:\n    retcode, stdout, stderr = yield gen.Task(script.execute, self.params)\n    self.finish({\n        \"stdout\": stdout,\n        \"stderr\": stderr,\n        \"return_values\": self.find_return_values(stdout),\n        \"retcode\": retcode\n    })", "path": "pyjojo\\handlers.py", "repo_name": "atarola/pyjojo", "stars": 137, "license": "mit", "language": "python", "size": 417}
{"docstring": "# create the application\n", "func_signal": "def get_app(self):\n", "code": "config['directory'] = 'test/fixtures'\nreturn create_application(False)", "path": "test\\functional\\base.py", "repo_name": "atarola/pyjojo", "stars": 137, "license": "mit", "language": "python", "size": 417}
{"docstring": "\"\"\" get the requirements for all of the scripts \"\"\"\n\n", "func_signal": "def get(self):\n", "code": "tags = {'tags': [], 'not_tags': [], 'any_tags': []}\n\nfor tag_arg in ['tags', 'not_tags', 'any_tags']:\n    try:\n        tags[tag_arg] = self.get_arguments(tag_arg)[0].split(',')\n        break\n    except IndexError:\n        continue\n       \nself.finish({'script_names': self.settings['scripts'].name(tags)})", "path": "pyjojo\\handlers.py", "repo_name": "atarola/pyjojo", "stars": 137, "license": "mit", "language": "python", "size": 417}
{"docstring": "\"\"\" run the script \"\"\"\n        \n", "func_signal": "def delete(self, script_name):\n", "code": "if config['force_json']:\n    self.set_header(\"Content-Type\", \"application/json; charset=UTF-8\")\n\nscript = self.get_script(script_name, 'delete')\n\nif script.output == 'combined':\n    retcode, stdout = yield gen.Task(script.execute, self.params)\n    self.finish({\n        \"stdout\": stdout,\n        \"return_values\": self.find_return_values(stdout),\n        \"retcode\": retcode\n    })\nelse:\n    retcode, stdout, stderr = yield gen.Task(script.execute, self.params)\n    self.finish({\n        \"stdout\": stdout,\n        \"stderr\": stderr,\n        \"return_values\": self.find_return_values(stdout),\n        \"retcode\": retcode\n    })", "path": "pyjojo\\handlers.py", "repo_name": "atarola/pyjojo", "stars": 137, "license": "mit", "language": "python", "size": 417}
{"docstring": "\"\"\"gets called when we class decorate\"\"\"\n\n", "func_signal": "def __call__(self, _handler):\n", "code": "log.info(\"Binding {0} to route {1}\".format(_handler.__name__, self._uri))        \nname = self.name and self.name or _handler.__name__\nself._routes.append(tornado.web.url(self._uri, _handler, name=name))\nreturn _handler", "path": "pyjojo\\util.py", "repo_name": "atarola/pyjojo", "stars": 137, "license": "mit", "language": "python", "size": 417}
{"docstring": "\"\"\" automatically parse the json body of the request \"\"\"\n\n", "func_signal": "def handle_params(self):\n", "code": "self.params = {}\ncontent_type = self.request.headers.get(\"Content-Type\", 'application/json')\n    \nif (content_type.startswith(\"application/json\")) or (config['force_json']):\n    if self.request.body in [None, \"\"]:\n        return\n\n    self.params = json.loads(self.request.body)\nelse:\n    # we only handle json, and say so\n    raise HTTPError(400, \"This application only support json, please set the http header Content-Type to application/json\")", "path": "pyjojo\\handlers.py", "repo_name": "atarola/pyjojo", "stars": 137, "license": "mit", "language": "python", "size": 417}
{"docstring": "\"\"\" entry point for the application \"\"\"\n\n# get the command line options\n", "func_signal": "def main():\n", "code": "options = command_line_options()\nsetup_logging()\n\n# setup the application\nlog.info(\"Setting up the application\")\napplication = create_application(options.debug)\n\n# warn about --force-json\nif options.force_json:\n    log.warn(\"Application started with '--force-json' option.  All calls will be treated as if they passed the 'Content-Type: application/json' header.  This may cause unexpected behavior.\")\n\n# server startup\nif options.unix_socket:\n    unix_socket_server(application, options)\nelif options.certfile and options.keyfile:\n    https_server(application, options)\nelse:\n    http_server(application, options)\n\n# start the ioloop\nlog.info(\"Starting the IOLoop\")\nIOLoop.instance().start()", "path": "pyjojo\\server.py", "repo_name": "atarola/pyjojo", "stars": 137, "license": "mit", "language": "python", "size": 417}
{"docstring": "\"\"\" run the script \"\"\"\n\n", "func_signal": "def get(self, script_name):\n", "code": "if config['force_json']:\n    self.set_header(\"Content-Type\", \"application/json; charset=UTF-8\")\n\nscript = self.get_script(script_name, 'get')\n\nif script.output == 'combined':\n    retcode, stdout = yield gen.Task(script.execute, self.params)\n    self.finish({\n        \"stdout\": stdout,\n        \"return_values\": self.find_return_values(stdout),\n        \"retcode\": retcode\n    })\nelse:\n    retcode, stdout, stderr = yield gen.Task(script.execute, self.params)\n    self.finish({\n        \"stdout\": stdout,\n        \"stderr\": stderr,\n        \"return_values\": self.find_return_values(stdout),\n        \"retcode\": retcode\n    })", "path": "pyjojo\\handlers.py", "repo_name": "atarola/pyjojo", "stars": 137, "license": "mit", "language": "python", "size": 417}
{"docstring": "# IDEA: Sort from import list\n", "func_signal": "def sort_imports(self):\n", "code": "visitor = actions.SortingVisitor(self.pycore, self._current_folder())\nfor import_statement in self.imports:\n    import_statement.accept(visitor)\nin_projects = sorted(visitor.in_project, key = self._compare_imports)\nthird_party = sorted(visitor.third_party, key = self._compare_imports)\nstandards = sorted(visitor.standard, key = self._compare_imports)\nfuture = sorted(visitor.future, key = self._compare_imports)\nblank_lines = 0\nlast_index = self._first_import_line()\nlast_index = self._move_imports(future, last_index, 0)\nlast_index = self._move_imports(standards, last_index, 1)\nlast_index = self._move_imports(third_party, last_index, 1)\nlast_index = self._move_imports(in_projects, last_index, 1)\nself.separating_lines = 2", "path": "rope\\refactor\\importutils\\module_imports.py", "repo_name": "fortharris/Pcode", "stars": 167, "license": "gpl-3.0", "language": "python", "size": 15617}
{"docstring": "# HACK: Ignoring out->in calls\n# This might lose some information\n", "func_signal": "def global_trace(frame, event, arg):\n", "code": "if self._is_an_interesting_call(frame):\n    return self.on_function_call", "path": "rope\\base\\oi\\runmod.py", "repo_name": "fortharris/Pcode", "stars": 167, "license": "gpl-3.0", "language": "python", "size": 15617}
{"docstring": "#if frame.f_code.co_name in ['?', '<module>']:\n#    return False\n#return not frame.f_back or not self._is_code_inside_project(frame.f_back.f_code)\n\n", "func_signal": "def _is_an_interesting_call(self, frame):\n", "code": "if not self._is_code_inside_project(frame.f_code) and \\\n   (not frame.f_back or not self._is_code_inside_project(frame.f_back.f_code)):\n    return False\nreturn True", "path": "rope\\base\\oi\\runmod.py", "repo_name": "fortharris/Pcode", "stars": 167, "license": "gpl-3.0", "language": "python", "size": 15617}
{"docstring": "\"\"\"Run `resource` module\n\nReturns a `rope.base.oi.doa.PythonFileRunner` object for\ncontrolling the process.\n\n\"\"\"\n", "func_signal": "def run_module(self, resource, args=None, stdin=None, stdout=None):\n", "code": "perform_doa = self.project.prefs.get('perform_doi', True)\nperform_doa = self.project.prefs.get('perform_doa', perform_doa)\nreceiver = self.object_info.doa_data_received\nif not perform_doa:\n    receiver = None\nrunner = rope.base.oi.doa.PythonFileRunner(\n    self, resource, args, stdin, stdout, receiver)\nrunner.add_finishing_observer(self.module_cache.forget_all_data)\nrunner.run()\nreturn runner", "path": "rope\\base\\pycore.py", "repo_name": "fortharris/Pcode", "stars": 167, "license": "gpl-3.0", "language": "python", "size": 15617}
{"docstring": "\"\"\"True, if lnode and rnode are located on different forks of IF/TRY\"\"\"\n", "func_signal": "def differentForks(self, lnode, rnode):\n", "code": "ancestor = self.getCommonAncestor(lnode, rnode, self.root)\nparts = getAlternatives(ancestor)\nif parts:\n    for items in parts:\n        if self.descendantOf(lnode, items, ancestor) ^ \\\n           self.descendantOf(rnode, items, ancestor):\n            return True\nreturn False", "path": "pyflakes\\checker.py", "repo_name": "fortharris/Pcode", "stars": 167, "license": "gpl-3.0", "language": "python", "size": 15617}
{"docstring": "\"\"\"\nDetermine if the given node is a docstring, as long as it is at the\ncorrect place in the node tree.\n\"\"\"\n", "func_signal": "def isDocstring(self, node):\n", "code": "return isinstance(node, ast.Str) or (isinstance(node, ast.Expr) and\n                                     isinstance(node.value, ast.Str))", "path": "pyflakes\\checker.py", "repo_name": "fortharris/Pcode", "stars": 167, "license": "gpl-3.0", "language": "python", "size": 15617}
{"docstring": "\"\"\"\nCheck to see if any assignments have not been used.\n\"\"\"\n", "func_signal": "def checkUnusedAssignments():\n", "code": "for name, binding in self.scope.unusedAssignments():\n    self.report(messages.UnusedVariable, binding.source, name)", "path": "pyflakes\\checker.py", "repo_name": "fortharris/Pcode", "stars": 167, "license": "gpl-3.0", "language": "python", "size": 15617}
{"docstring": "# handle iter before target, and generators before element\n", "func_signal": "def _get_fields(self, node_class):\n", "code": "fields = node_class._fields\nif 'iter' in fields:\n    key_first = 'iter'.find\nelif 'generators' in fields:\n    key_first = 'generators'.find\nelse:\n    key_first = 'value'.find\nreturn tuple(sorted(fields, key=key_first, reverse=True))", "path": "pyflakes\\checker.py", "repo_name": "fortharris/Pcode", "stars": 167, "license": "gpl-3.0", "language": "python", "size": 15617}
{"docstring": "\"\"\"\nHandle occurrence of Name (which can be a load/store/delete access.)\n\"\"\"\n# Locate the name in locals / function / globals scopes.\n", "func_signal": "def NAME(self, node):\n", "code": "if isinstance(node.ctx, (ast.Load, ast.AugLoad)):\n    self.handleNodeLoad(node)\n    if (node.id == 'locals' and isinstance(self.scope, FunctionScope)\n            and isinstance(node.parent, ast.Call)):\n        # we are doing locals() call in current scope\n        self.scope.usesLocals = True\nelif isinstance(node.ctx, (ast.Store, ast.AugStore)):\n    self.handleNodeStore(node)\nelif isinstance(node.ctx, ast.Del):\n    self.handleNodeDelete(node)\nelse:\n    # must be a Param context -- this only happens for names in function\n    # arguments, but these aren't dispatched through here\n    raise RuntimeError(\"Got impossible expression context: %r\" % (node.ctx,))", "path": "pyflakes\\checker.py", "repo_name": "fortharris/Pcode", "stars": 167, "license": "gpl-3.0", "language": "python", "size": 15617}
{"docstring": "\"\"\"A shortcut for getting the `ImportInfo`\\s used in a scope\"\"\"\n", "func_signal": "def get_imports(pycore, pydefined):\n", "code": "pymodule = pydefined.get_module()\nmodule = module_imports.ModuleImports(pycore, pymodule)\nif pymodule == pydefined:\n    return [stmt.import_info for stmt in module.imports]\nreturn module.get_used_imports(pydefined)", "path": "rope\\refactor\\importutils\\__init__.py", "repo_name": "fortharris/Pcode", "stars": 167, "license": "gpl-3.0", "language": "python", "size": 15617}
{"docstring": "\"\"\"\nKeep track of globals declarations.\n\"\"\"\n# In doctests, the global scope is an anonymous function at index 1.\n", "func_signal": "def GLOBAL(self, node):\n", "code": "global_scope_index = 1 if self.withDoctest else 0\nglobal_scope = self.scopeStack[global_scope_index]\n\n# Ignore 'global' statement in global scope.\nif self.scope is not global_scope:\n\n    # One 'global' statement can bind multiple (comma-delimited) names.\n    for node_name in node.names:\n        node_value = Assignment(node_name, node)\n\n        # Remove UndefinedName messages already reported for this name.\n        self.messages = [\n            m for m in self.messages if not\n            isinstance(m, messages.UndefinedName) and not\n            m.message_args[0] == node_name]\n\n        # Bind name to global scope if it doesn't exist already.\n        global_scope.setdefault(node_name, node_value)\n\n        # Bind name to non-global scopes, but as already \"used\".\n        node_value.used = (global_scope, node)\n        for scope in self.scopeStack[global_scope_index + 1:]:\n            scope[node_name] = node_value", "path": "pyflakes\\checker.py", "repo_name": "fortharris/Pcode", "stars": 167, "license": "gpl-3.0", "language": "python", "size": 15617}
{"docstring": "# Returns node.id, or node.name, or None\n", "func_signal": "def getNodeName(node):\n", "code": "if hasattr(node, 'id'):     # One of the many nodes with an id\n    return node.id\nif hasattr(node, 'name'):   # a ExceptHandler node\n    return node.name", "path": "pyflakes\\checker.py", "repo_name": "fortharris/Pcode", "stars": 167, "license": "gpl-3.0", "language": "python", "size": 15617}
{"docstring": "\"\"\"\nRun the callables in C{deferred} using their associated scope stack.\n\"\"\"\n", "func_signal": "def runDeferred(self, deferred):\n", "code": "for handler, scope, offset in deferred:\n    self.scopeStack = scope\n    self.offset = offset\n    handler()", "path": "pyflakes\\checker.py", "repo_name": "fortharris/Pcode", "stars": 167, "license": "gpl-3.0", "language": "python", "size": 15617}
{"docstring": "\"\"\"Returns a `PyObject` if the module was found.\"\"\"\n# check if this is a builtin module\n", "func_signal": "def get_module(self, name, folder=None):\n", "code": "pymod = self._builtin_module(name)\nif pymod is not None:\n    return pymod\nmodule = self.find_module(name, folder)\nif module is None:\n    raise ModuleNotFoundError('Module %s not found' % name)\nreturn self.resource_to_pyobject(module)", "path": "rope\\base\\pycore.py", "repo_name": "fortharris/Pcode", "stars": 167, "license": "gpl-3.0", "language": "python", "size": 15617}
{"docstring": "\"\"\"Removes pyname when imported in ``from mod import x``\"\"\"\n", "func_signal": "def remove_pyname(self, pyname):\n", "code": "visitor = actions.RemovePyNameVisitor(self.pycore, self.pymodule,\n                                      pyname, self._current_folder())\nfor import_stmt in self.imports:\n    import_stmt.accept(visitor)", "path": "rope\\refactor\\importutils\\module_imports.py", "repo_name": "fortharris/Pcode", "stars": 167, "license": "gpl-3.0", "language": "python", "size": 15617}
{"docstring": "\"\"\"If `offset` is None, the `resource` itself will be renamed\"\"\"\n", "func_signal": "def __init__(self, project, resource, offset=None):\n", "code": "self.project = project\nself.pycore = project.pycore\nself.resource = resource\nif offset is not None:\n    self.old_name = worder.get_name_at(self.resource, offset)\n    this_pymodule = self.pycore.resource_to_pyobject(self.resource)\n    self.old_instance, self.old_pyname = \\\n        evaluate.eval_location2(this_pymodule, offset)\n    if self.old_pyname is None:\n        raise exceptions.RefactoringError(\n            'Rename refactoring should be performed'\n            ' on resolvable python identifiers.')\nelse:\n    if not resource.is_folder() and resource.name == '__init__.py':\n        resource = resource.parent\n    dummy_pymodule = self.pycore.get_string_module('')\n    self.old_instance = None\n    self.old_pyname = pynames.ImportedModule(dummy_pymodule,\n                                             resource=resource)\n    if resource.is_folder():\n        self.old_name = resource.name\n    else:\n        self.old_name = resource.name[:-3]", "path": "rope\\refactor\\rename.py", "repo_name": "fortharris/Pcode", "stars": 167, "license": "gpl-3.0", "language": "python", "size": 15617}
{"docstring": "\"\"\"Returns project source folders\"\"\"\n", "func_signal": "def get_source_folders(self):\n", "code": "if self.project.root is None:\n    return []\nresult = list(self._custom_source_folders)\nresult.extend(self._find_source_folders(self.project.root))\nreturn result", "path": "rope\\base\\pycore.py", "repo_name": "fortharris/Pcode", "stars": 167, "license": "gpl-3.0", "language": "python", "size": 15617}
{"docstring": "\"\"\"\nReimplemented to create a console-like interface.\n\"\"\"\n", "func_signal": "def keyPressEvent(self, event):\n", "code": "line, index = self.getCursorPosition()\nkey = event.key()\nctrl = event.modifiers() & QtCore.Qt.ControlModifier\nalt = event.modifiers() & QtCore.Qt.AltModifier\nshift_down = event.modifiers() & QtCore.Qt.ShiftModifier\nif ctrl:\n    pass\nelif alt:\n    pass\nelif key == QtCore.Qt.Key_Backspace:\n    if self.getCursorPosition() == self.blocking_cursor_pos:\n        pass\n    else:\n        QsciScintilla.keyPressEvent(self, event)\nelif key == QtCore.Qt.Key_Left:\n    if self.getCursorPosition() == self.blocking_cursor_pos:\n        pass\n    else:\n        QsciScintilla.keyPressEvent(self, event)\nelif key == QtCore.Qt.Key_Up:\n    self.scrollVertical(-1)\nelif key == QtCore.Qt.Key_Down:\n    self.scrollVertical(1)\nelif key == QtCore.Qt.Key_Return:\n    # get input text\n    text = self.getText(\n        self.blocking_cursor_pos, self.position(\"eof\"))\n    self.insertInput(text)\nelse:\n    QsciScintilla.keyPressEvent(self, event)", "path": "Extensions\\BottomWidgets\\RunWidget.py", "repo_name": "fortharris/Pcode", "stars": 167, "license": "gpl-3.0", "language": "python", "size": 15617}
{"docstring": "\"\"\"Tell whether any of start till end lines have changed\n\nThe end points are inclusive and indices start from 1.\n\"\"\"\n", "func_signal": "def is_changed(self, start, end):\n", "code": "left, right = self._get_changed(start, end)\nif left < right:\n    return True\nreturn False", "path": "rope\\base\\pycore.py", "repo_name": "fortharris/Pcode", "stars": 167, "license": "gpl-3.0", "language": "python", "size": 15617}
{"docstring": "\"\"\"Returns all python files available in the project\"\"\"\n", "func_signal": "def get_python_files(self):\n", "code": "return [resource for resource in self.project.get_files()\n        if self.is_python_file(resource)]", "path": "rope\\base\\pycore.py", "repo_name": "fortharris/Pcode", "stars": 167, "license": "gpl-3.0", "language": "python", "size": 15617}
{"docstring": "\"\"\"``value_text`` filter returns value of field.\"\"\"\n", "func_signal": "def test_value_text(self):\n", "code": "self.assertEqual(\n    self.form_utils.value_text(self.form({\"name\": \"boo\"})[\"name\"]), \"boo\")", "path": "tests\\tests.py", "repo_name": "carljm/django-form-utils", "stars": 144, "license": "bsd-3-clause", "language": "python", "size": 135}
{"docstring": "\"\"\"``label`` filter renders field label from template.\"\"\"\n", "func_signal": "def test_label(self, render_to_string):\n", "code": "render_to_string.return_value = \"<label>something</label>\"\nbf = self.form()[\"name\"]\n\nlabel = self.form_utils.label(bf)\n\nself.assertEqual(label, \"<label>something</label>\")\nrender_to_string.assert_called_with(\n    \"forms/_label.html\",\n    {\n        \"label_text\": \"Name\",\n        \"id\": \"id_name\",\n        \"field\": bf\n        }\n    )", "path": "tests\\tests.py", "repo_name": "carljm/django-form-utils", "stars": 144, "license": "bsd-3-clause", "language": "python", "size": 135}
{"docstring": "\"\"\"\nA ``BetterForm`` renders as a list of fields within each fieldset.\n\n\"\"\"\n", "func_signal": "def test_render_betterform(self):\n", "code": "form = ApplicationForm()\ntpl = template.Template('{% load form_utils %}{{ form|render }}')\nhtml = tpl.render(template.Context({'form': form}))\nself.assertHTMLEqual(html, self.betterform_html)", "path": "tests\\tests.py", "repo_name": "carljm/django-form-utils", "stars": 144, "license": "bsd-3-clause", "language": "python", "size": 135}
{"docstring": "\"\"\"`is_radio` detects a radio select widget.\"\"\"\n", "func_signal": "def test_is_radio(self):\n", "code": "f = self.form()\n\nself.assertTrue(self.form_utils.is_radio(f[\"gender\"]))", "path": "tests\\tests.py", "repo_name": "carljm/django-form-utils", "stars": 144, "license": "bsd-3-clause", "language": "python", "size": 135}
{"docstring": "\"\"\"\nIf we check the clear checkbox, but also submit a file, the\nfile overrides.\n\n\"\"\"\n", "func_signal": "def test_cleared_but_file_given(self):\n", "code": "field = ClearableFileField()\nresult = field.clean([self.upload, '1'])\nself.assertEqual(result, self.upload)", "path": "tests\\tests.py", "repo_name": "carljm/django-form-utils", "stars": 144, "license": "bsd-3-clause", "language": "python", "size": 135}
{"docstring": "\"\"\"`is_multiple` detects a non-multiple widget.\"\"\"\n", "func_signal": "def test_is_not_multiple(self):\n", "code": "f = self.form()\n\nself.assertFalse(self.form_utils.is_multiple(f[\"level\"]))", "path": "tests\\tests.py", "repo_name": "carljm/django-form-utils", "stars": 144, "license": "bsd-3-clause", "language": "python", "size": 135}
{"docstring": "\"\"\"\nIf the clear checkbox is not checked, the ``FileField`` data\nis returned normally.\n\n\"\"\"\n", "func_signal": "def test_not_cleared(self):\n", "code": "field = ClearableFileField()\nresult = field.clean([self.upload, '0'])\nself.assertEqual(result, self.upload)", "path": "tests\\tests.py", "repo_name": "carljm/django-form-utils", "stars": 144, "license": "bsd-3-clause", "language": "python", "size": 135}
{"docstring": "\"\"\"`is_multiple` detects a MultipleChoiceField.\"\"\"\n", "func_signal": "def test_is_multiple(self):\n", "code": "f = self.form()\n\nself.assertTrue(self.form_utils.is_multiple(f[\"colors\"]))", "path": "tests\\tests.py", "repo_name": "carljm/django-form-utils", "stars": 144, "license": "bsd-3-clause", "language": "python", "size": 135}
{"docstring": "\"\"\"`is_select` detects a ChoiceField.\"\"\"\n", "func_signal": "def test_is_select(self):\n", "code": "f = self.form()\n\nself.assertTrue(self.form_utils.is_select(f[\"level\"]))", "path": "tests\\tests.py", "repo_name": "carljm/django-form-utils", "stars": 144, "license": "bsd-3-clause", "language": "python", "size": 135}
{"docstring": "\"\"\"\nTest the definition and inheritance of fieldsets, by matching\nsample form classes' ``fieldsets`` attribute with the target\ndata in ``self.fieldsets_target_data``.\n\n\"\"\"\n", "func_signal": "def test_iterate_fieldsets(self):\n", "code": "for form_class, targets in self.fieldset_target_data.items():\n    form = form_class()\n    # verify len(form.fieldsets) tells us the truth\n    self.assertEqual(len(form.fieldsets), len(targets))\n    for i, fs in enumerate(form.fieldsets):\n        target_data = targets[i]\n        # verify fieldset contains correct fields\n        self.assertEqual([f.name for f in fs],\n                          target_data[0])\n        # verify fieldset has correct attributes\n        for attr, val in target_data[1].items():\n            self.assertEqual(getattr(fs, attr), val)", "path": "tests\\tests.py", "repo_name": "carljm/django-form-utils", "stars": 144, "license": "bsd-3-clause", "language": "python", "size": 135}
{"docstring": "\"\"\"``selected_values`` filter returns values of multiple select.\"\"\"\n", "func_signal": "def test_selected_values_choices(self):\n", "code": "f = self.form({\"level\": [\"a\", \"b\"]})\n\nself.assertEqual(\n    self.form_utils.selected_values(f[\"level\"]),\n    [\"Advanced\", \"Beginner\"],\n    )", "path": "tests\\tests.py", "repo_name": "carljm/django-form-utils", "stars": 144, "license": "bsd-3-clause", "language": "python", "size": 135}
{"docstring": "\"\"\"\nWe can access the ``errors`` attribute of a bound form and get\nan ``ErrorDict``.\n\n\"\"\"\n", "func_signal": "def test_fieldset_errors(self):\n", "code": "form = ApplicationForm(data={'name': 'John Doe',\n                             'reference': 'Jane Doe'})\nself.assertEqual([fs.errors for fs in form.fieldsets],\n                  [{'position': [u'This field is required.']}, {}])", "path": "tests\\tests.py", "repo_name": "carljm/django-form-utils", "stars": 144, "license": "bsd-3-clause", "language": "python", "size": 135}
{"docstring": "\"\"\"\n``ImageWidget`` respects a custom template.\n\n\"\"\"\n", "func_signal": "def test_custom_template(self):\n", "code": "widget = ImageWidget(template='<div>%(image)s</div>'\n                     '<div>%(input)s</div>')\nhtml = widget.render('fieldname', ImageFieldFile(None, ImageField(), 'tiny.png'))\nself.assertTrue(html.startswith('<div><img'))", "path": "tests\\tests.py", "repo_name": "carljm/django-form-utils", "stars": 144, "license": "bsd-3-clause", "language": "python", "size": 135}
{"docstring": "\"\"\"\nIf we pass in our own ``file_field`` its required value is\nused for the composite field.\n\n\"\"\"\n", "func_signal": "def test_custom_file_field_required(self):\n", "code": "file_field = forms.ImageField(required=False)\nfield = ClearableFileField(file_field=file_field)\nself.assertFalse(field.required)", "path": "tests\\tests.py", "repo_name": "carljm/django-form-utils", "stars": 144, "license": "bsd-3-clause", "language": "python", "size": 135}
{"docstring": "\"\"\"\nA plain ``forms.Form`` renders as a list of fields.\n\n\"\"\"\n", "func_signal": "def test_render_form(self):\n", "code": "form = BoringForm()\ntpl = template.Template('{% load form_utils %}{{ form|render }}')\nhtml = tpl.render(template.Context({'form': form}))\nself.assertHTMLEqual(html, self.boring_form_html)", "path": "tests\\tests.py", "repo_name": "carljm/django-form-utils", "stars": 144, "license": "bsd-3-clause", "language": "python", "size": 135}
{"docstring": "\"\"\"\nWe can pass in a custom template and it will be passed on to\nthe widget.\n\n\"\"\"\n", "func_signal": "def test_custom_template(self):\n", "code": "tpl = 'Clear: %(checkbox)s %(input)s'\nfield = ClearableFileField(template=tpl)\nself.assertEqual(field.widget.template, tpl)", "path": "tests\\tests.py", "repo_name": "carljm/django-form-utils", "stars": 144, "license": "bsd-3-clause", "language": "python", "size": 135}
{"docstring": "\"\"\"\n``ImageWidget`` renders the file input and the image thumb.\n\n\"\"\"\n", "func_signal": "def test_render(self):\n", "code": "widget = ImageWidget()\nhtml = widget.render('fieldname', ImageFieldFile(None, ImageField(), 'tiny.png'))\n# test only this much of the html, because the remainder will\n# vary depending on whether we have sorl-thumbnail\nself.assertTrue('<img' in html)\nself.assertTrue('/media/tiny' in html)", "path": "tests\\tests.py", "repo_name": "carljm/django-form-utils", "stars": 144, "license": "bsd-3-clause", "language": "python", "size": 135}
{"docstring": "\"\"\"\nFields of a ``BetterForm`` accessed by fieldset iteration have\n``row_attrs`` as defined in the inner ``Meta`` class.\n\n\"\"\"\n", "func_signal": "def test_row_attrs_by_fieldset_iteration(self):\n", "code": "form = HoneypotForm()\nfieldset = [fs for fs in form.fieldsets][0]\nhoneypot = [field for field in fieldset if field.name=='honeypot'][0]\nattrs = honeypot.row_attrs\nself.assertTrue(u'style=\"display: none\"' in attrs)\nself.assertTrue(u'class=\"required\"' in attrs)", "path": "tests\\tests.py", "repo_name": "carljm/django-form-utils", "stars": 144, "license": "bsd-3-clause", "language": "python", "size": 135}
{"docstring": "\"\"\"\nTemplate can also be customized by subclassing.\n\n\"\"\"\n", "func_signal": "def test_custom_template_via_subclass(self):\n", "code": "class ReversedClearableFileInput(ClearableFileInput):\n    template = 'Clear: %(checkbox)s %(input)s'\nwidget = ReversedClearableFileInput()\nhtml = widget.render('fieldname', 'tiny.png')\nself.assertHTMLEqual(\n    html,\n    'Clear: '\n    '<input type=\"checkbox\" name=\"fieldname_1\" /> '\n    '<input type=\"file\" name=\"fieldname_0\" />'\n    )", "path": "tests\\tests.py", "repo_name": "carljm/django-form-utils", "stars": 144, "license": "bsd-3-clause", "language": "python", "size": 135}
{"docstring": "\"\"\"``is_checkbox`` detects that select fields are not checkboxes.\"\"\"\n", "func_signal": "def test_detect_non_checkbox(self):\n", "code": "f = self.form()\n\nself.assertFalse(self.form_utils.is_checkbox(f[\"level\"]))", "path": "tests\\tests.py", "repo_name": "carljm/django-form-utils", "stars": 144, "license": "bsd-3-clause", "language": "python", "size": 135}
{"docstring": "\"\"\"generate gaussian continuous codes with specified mean and std\"\"\"\n", "func_signal": "def gen_conti_codes(n_instance, n_conti, mean=0, std=1):\n", "code": "codes = np.random.randn(n_instance, n_conti) * std + mean\nreturn torch.Tensor(codes)", "path": "InfoGAN\\run_InfoGAN.py", "repo_name": "AaronYALai/Generative_Adversarial_Networks_PyTorch", "stars": 136, "license": "None", "language": "python", "size": 2112}
{"docstring": "\"\"\"generate n-dim uniform random noise\"\"\"\n", "func_signal": "def gen_noise(n_instance, n_dim=2):\n", "code": "return torch.Tensor(np.random.uniform(low=-1.0, high=1.0,\n                                      size=(n_instance, n_dim)))", "path": "ImprovedGAN\\run_ImprovedGAN.py", "repo_name": "AaronYALai/Generative_Adversarial_Networks_PyTorch", "stars": 136, "license": "None", "language": "python", "size": 2112}
{"docstring": "\"\"\"\nThe output of torchvision datasets are PILImage images of range [0, 1].\nTransform them to Tensors of normalized range [-1, 1]\n\"\"\"\n", "func_signal": "def load_dataset(batch_size=10, download=True):\n", "code": "transform = transforms.Compose([transforms.ToTensor(),\n                                transforms.Normalize((0.5, 0.5, 0.5),\n                                                     (0.5, 0.5, 0.5))])\ntrainset = torchvision.datasets.MNIST(root='../data', train=True,\n                                      download=download,\n                                      transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                          shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.MNIST(root='../data', train=False,\n                                     download=download,\n                                     transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n                                         shuffle=False, num_workers=2)\n\nreturn trainloader, testloader", "path": "InfoGAN\\run_InfoGAN.py", "repo_name": "AaronYALai/Generative_Adversarial_Networks_PyTorch", "stars": 136, "license": "None", "language": "python", "size": 2112}
{"docstring": "\"\"\"\nThe output of torchvision datasets are PILImage images of range [0, 1].\nTransform them to Tensors of normalized range [-1, 1]\n\"\"\"\n", "func_signal": "def load_dataset(batch_size=10, download=True):\n", "code": "transform = transforms.Compose([transforms.ToTensor(),\n                                transforms.Normalize((0.5, 0.5, 0.5),\n                                                     (0.5, 0.5, 0.5))])\ntrainset = torchvision.datasets.MNIST(root='../data', train=True,\n                                      download=download,\n                                      transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                          shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.MNIST(root='../data', train=False,\n                                     download=download,\n                                     transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n                                         shuffle=False, num_workers=2)\n\nreturn trainloader, testloader", "path": "DCGAN\\run_DCGAN.py", "repo_name": "AaronYALai/Generative_Adversarial_Networks_PyTorch", "stars": 136, "license": "None", "language": "python", "size": 2112}
{"docstring": "\"\"\"\nThe output of torchvision datasets are PILImage images of range [0, 1].\nTransform them to Tensors of normalized range [-1, 1]\n\"\"\"\n", "func_signal": "def load_dataset(batch_size=10, download=True):\n", "code": "transform = transforms.Compose([transforms.ToTensor(),\n                                transforms.Normalize((0.5, 0.5, 0.5),\n                                                     (0.5, 0.5, 0.5))])\ntrainset = torchvision.datasets.MNIST(root='../data', train=True,\n                                      download=download,\n                                      transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                          shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.MNIST(root='../data', train=False,\n                                     download=download,\n                                     transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n                                         shuffle=False, num_workers=2)\n\nreturn trainloader, testloader", "path": "LAPGAN\\run_LAPGAN.py", "repo_name": "AaronYALai/Generative_Adversarial_Networks_PyTorch", "stars": 136, "license": "None", "language": "python", "size": 2112}
{"docstring": "\"\"\"\nArchitecture is similar to DCGANs\nAdd minibatch discrimination => Improved GAN.\n\"\"\"\n", "func_signal": "def forward(self, x):\n", "code": "x = F.leaky_relu(self.conv1(x), negative_slope=0.2)\nx = F.leaky_relu(self.BN2(self.conv2(x)), negative_slope=0.2)\nx = F.leaky_relu(self.BN3(self.conv3(x)), negative_slope=0.2)\nx = x.view(-1, self.featmap_dim * 4 * 4)\n\n# #### Minibatch Discrimination ###\nT_tensor = self.T_tensor\nif self.use_gpu:\n    T_tensor = T_tensor.cuda()\n\nMs = x.mm(T_tensor)\nMs = Ms.view(-1, self.n_B, self.n_C)\n\nout_tensor = []\nfor i in range(Ms.size()[0]):\n\n    out_i = None\n    for j in range(Ms.size()[0]):\n        o_i = torch.sum(torch.abs(Ms[i, :, :] - Ms[j, :, :]), 1)\n        o_i = torch.exp(-o_i)\n        if out_i is None:\n            out_i = o_i\n        else:\n            out_i = out_i + o_i\n\n    out_tensor.append(out_i)\n\nout_T = torch.cat(tuple(out_tensor)).view(Ms.size()[0], self.n_B)\nx = torch.cat((x, out_T), 1)\n# #### Minibatch Discrimination ###\n\nx = F.sigmoid(self.fc(x))\n\nreturn x", "path": "ImprovedGAN\\ImprovedGAN.py", "repo_name": "AaronYALai/Generative_Adversarial_Networks_PyTorch", "stars": 136, "license": "None", "language": "python", "size": 2112}
{"docstring": "\"\"\"generate n-dim uniform random noise\"\"\"\n", "func_signal": "def gen_noise(n_instance, n_dim=2):\n", "code": "return torch.Tensor(np.random.uniform(low=-1.0, high=1.0,\n                                      size=(n_instance, n_dim)))", "path": "DCGAN\\run_DCGAN.py", "repo_name": "AaronYALai/Generative_Adversarial_Networks_PyTorch", "stars": 136, "license": "None", "language": "python", "size": 2112}
{"docstring": "\"\"\"\nProject noise to featureMap * width * height,\nBatch Normalization after convulation but not at output layer,\nReLU activation function.\n\"\"\"\n", "func_signal": "def forward(self, x):\n", "code": "x = self.fc1(x)\nx = x.view(-1, self.featmap_dim, 4, 4)\nx = F.relu(self.BN1(self.conv1(x)))\nx = F.relu(self.BN2(self.conv2(x)))\nx = F.tanh(self.conv3(x))\n\nreturn x", "path": "DCGAN\\DCGAN.py", "repo_name": "AaronYALai/Generative_Adversarial_Networks_PyTorch", "stars": 136, "license": "None", "language": "python", "size": 2112}
{"docstring": "\"\"\"\nOutput the probability of being in real dataset\nplus the conditional distributions of latent codes.\n\"\"\"\n", "func_signal": "def forward(self, x):\n", "code": "for layer in range(self.n_layer):\n    conv_layer = self.convs[self.n_layer - layer - 1]\n\n    if layer == 0:\n        x = F.leaky_relu(conv_layer(x), negative_slope=0.2)\n    else:\n        BN_layer = self.BNs[self.n_layer - layer - 1]\n        x = F.leaky_relu(BN_layer(conv_layer(x)), negative_slope=0.2)\n\nx = x.view(-1, self.featmap_dim * 4 * 4)\n\n# output layer\nx = self.fc(x)\nx[:, 0] = F.sigmoid(x[:, 0].clone())\nfor j in range(self.n_discrete):\n    start = 1 + self.n_conti + j * self.num_category\n    end = start + self.num_category\n    x[:, start:end] = F.softmax(x[:, start:end].clone())\n\nreturn x", "path": "InfoGAN\\InfoGAN.py", "repo_name": "AaronYALai/Generative_Adversarial_Networks_PyTorch", "stars": 136, "license": "None", "language": "python", "size": 2112}
{"docstring": "\"\"\"\nThe output of torchvision datasets are PILImage images of range [0, 1].\nTransform them to Tensors of normalized range [-1, 1]\n\"\"\"\n", "func_signal": "def load_dataset(batch_size=10, download=True):\n", "code": "transform = transforms.Compose([transforms.ToTensor(),\n                                transforms.Normalize((0.5, 0.5, 0.5),\n                                                     (0.5, 0.5, 0.5))])\ntrainset = torchvision.datasets.MNIST(root='../data', train=True,\n                                      download=download,\n                                      transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                          shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.MNIST(root='../data', train=False,\n                                     download=download,\n                                     transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n                                         shuffle=False, num_workers=2)\n\nreturn trainloader, testloader", "path": "GAN\\run_GAN.py", "repo_name": "AaronYALai/Generative_Adversarial_Networks_PyTorch", "stars": 136, "license": "None", "language": "python", "size": 2112}
{"docstring": "\"\"\"\nConcatenate CNN-processed extra information vector at the last layer\n\"\"\"\n", "func_signal": "def forward(self, x, condi_x=None):\n", "code": "for layer in range(self.n_layer):\n    conv_layer = self.convs[self.n_layer - layer - 1]\n    if layer == 0:\n        x = F.leaky_relu(conv_layer(x), negative_slope=0.2)\n    else:\n        BN_layer = self.BNs[self.n_layer - layer - 1]\n        x = F.leaky_relu(BN_layer(conv_layer(x)), negative_slope=0.2)\nx = x.view(-1, self.featmap_dim * 4 * 4)\n\n# calculate and concatenate extra information\nif self.condition:\n    for layer in range(self.n_layer):\n        _conv = self.convs_condi[self.n_layer - layer - 1]\n        if layer == 0:\n            condi_x = F.leaky_relu(_conv(condi_x), negative_slope=0.2)\n        else:\n            BN_layer = self.BNs_condi[self.n_layer - layer - 1]\n            condi_x = F.leaky_relu(BN_layer(_conv(condi_x)),\n                                   negative_slope=0.2)\n\n    condi_x = condi_x.view(-1, self.condi_featmap_dim * 4 * 4)\n    condi_x = self.fc_c(condi_x)\n    x = torch.cat((x, condi_x), 1)\n\n# output layer\nx = F.sigmoid(self.fc(x))\n\nreturn x", "path": "LAPGAN\\LAPGAN.py", "repo_name": "AaronYALai/Generative_Adversarial_Networks_PyTorch", "stars": 136, "license": "None", "language": "python", "size": 2112}
{"docstring": "\"\"\"\nInput the random noise plus latent codes to generate fake images.\n\"\"\"\n", "func_signal": "def forward(self, x):\n", "code": "x = self.fc_in(x)\nx = x.view(-1, self.featmap_dim, 4, 4)\n\nfor layer in range(self.n_layer):\n    conv_layer = self.convs[self.n_layer - layer - 1]\n    if layer == (self.n_layer - 1):\n        x = F.tanh(conv_layer(x))\n    else:\n        BN_layer = self.BNs[self.n_layer - layer - 2]\n        x = F.relu(BN_layer(conv_layer(x)))\n\nreturn x", "path": "InfoGAN\\InfoGAN.py", "repo_name": "AaronYALai/Generative_Adversarial_Networks_PyTorch", "stars": 136, "license": "None", "language": "python", "size": 2112}
{"docstring": "\"\"\"\nProject noise to featureMap * width * height,\nBatch Normalization after convulation but not at output layer,\nReLU activation function.\n\"\"\"\n", "func_signal": "def forward(self, x):\n", "code": "x = self.fc1(x)\nx = x.view(-1, self.featmap_dim, 4, 4)\nx = F.relu(self.BN1(self.conv1(x)))\nx = F.relu(self.BN2(self.conv2(x)))\nx = F.tanh(self.conv3(x))\n\nreturn x", "path": "ImprovedGAN\\ImprovedGAN.py", "repo_name": "AaronYALai/Generative_Adversarial_Networks_PyTorch", "stars": 136, "license": "None", "language": "python", "size": 2112}
{"docstring": "\"\"\"\nConcatenate CNN-processed extra information vector at the first layer\n\"\"\"\n# calculate and concatenate extra information\n", "func_signal": "def forward(self, x, condi_x=None):\n", "code": "if self.condition:\n    for layer in range(self.n_layer):\n        _conv = self.convs_condi[self.n_layer - layer - 1]\n        if layer == 0:\n            condi_x = F.leaky_relu(_conv(condi_x), negative_slope=0.2)\n        else:\n            BN_layer = self.BNs_condi[self.n_layer - layer - 1]\n            condi_x = F.leaky_relu(BN_layer(_conv(condi_x)),\n                                   negative_slope=0.2)\n\n    condi_x = condi_x.view(-1, self.condi_featmap_dim * 4 * 4)\n    condi_x = self.fc_c(condi_x)\n    x = torch.cat((x, condi_x), 1)\n\nx = self.fc1(x)\nx = x.view(-1, self.featmap_dim, 4, 4)\n\nfor layer in range(self.n_layer):\n    conv_layer = self.convs[self.n_layer - layer - 1]\n    if layer == (self.n_layer - 1):\n        x = F.tanh(conv_layer(x))\n    else:\n        BN_layer = self.BNs[self.n_layer - layer - 2]\n        x = F.relu(BN_layer(conv_layer(x)))\n\nreturn x", "path": "LAPGAN\\LAPGAN.py", "repo_name": "AaronYALai/Generative_Adversarial_Networks_PyTorch", "stars": 136, "license": "None", "language": "python", "size": 2112}
{"docstring": "\"\"\"Generate images from LAPGAN generators\"\"\"\n", "func_signal": "def generate(self, batchsize, get_level=None, generator=False):\n", "code": "self.outputs = []\nself.generator_outputs = []\nfor level in range(self.n_level):\n    Gen_model = self.Gen_models[self.n_level - level - 1]\n\n    # generate noise\n    noise = Variable(gen_noise(batchsize, self.noise_dim))\n    if self.use_gpu:\n        noise = noise.cuda()\n\n    if level == 0:\n        # directly generate images\n        output_imgs = Gen_model.forward(noise)\n        if self.use_gpu:\n            output_imgs = output_imgs.cpu()\n        output_imgs = output_imgs.data.numpy()\n        self.generator_outputs.append(output_imgs)\n    else:\n        # upsize\n        input_imgs = np.array([[cv2.pyrUp(output_imgs[i, j, :])\n                              for j in range(self.n_channel)]\n                              for i in range(batchsize)])\n        condi_imgs = Variable(torch.Tensor(input_imgs))\n        if self.use_gpu:\n            condi_imgs = condi_imgs.cuda()\n\n        # generate images with extra information\n        residual_imgs = Gen_model.forward(noise, condi_imgs)\n        if self.use_gpu:\n            residual_imgs = residual_imgs.cpu()\n        output_imgs = residual_imgs.data.numpy() + input_imgs\n        self.generator_outputs.append(residual_imgs.data.numpy())\n\n    self.outputs.append(output_imgs)\n\nif get_level is None:\n    get_level = -1\n\nif generator:\n    result_imgs = self.generator_outputs[get_level]\nelse:\n    result_imgs = self.outputs[get_level]\n\nreturn result_imgs", "path": "LAPGAN\\LAPGAN.py", "repo_name": "AaronYALai/Generative_Adversarial_Networks_PyTorch", "stars": 136, "license": "None", "language": "python", "size": 2112}
{"docstring": "\"\"\"generate n-dim uniform random noise\"\"\"\n", "func_signal": "def gen_noise(n_instance, n_dim=2):\n", "code": "return torch.Tensor(np.random.uniform(low=-1.0, high=1.0,\n                                      size=(n_instance, n_dim)))", "path": "InfoGAN\\run_InfoGAN.py", "repo_name": "AaronYALai/Generative_Adversarial_Networks_PyTorch", "stars": 136, "license": "None", "language": "python", "size": 2112}
{"docstring": "\"\"\"generate n-dim uniform random noise\"\"\"\n", "func_signal": "def gen_noise(n_instance, n_dim=2):\n", "code": "return torch.Tensor(np.random.uniform(low=-1.0, high=1.0,\n                                      size=(n_instance, n_dim)))", "path": "LAPGAN\\LAPGAN.py", "repo_name": "AaronYALai/Generative_Adversarial_Networks_PyTorch", "stars": 136, "license": "None", "language": "python", "size": 2112}
{"docstring": "\"\"\"generate n-dim uniform random noise\"\"\"\n", "func_signal": "def gen_noise(n_instance):\n", "code": "return torch.Tensor(np.random.uniform(low=-1.0, high=1.0,\n                                      size=(n_instance, 2)))", "path": "GAN\\run_GAN.py", "repo_name": "AaronYALai/Generative_Adversarial_Networks_PyTorch", "stars": 136, "license": "None", "language": "python", "size": 2112}
{"docstring": "\"\"\"\nStrided convulation layers,\nBatch Normalization after convulation but not at input layer,\nLeakyReLU activation function with slope 0.2.\n\"\"\"\n", "func_signal": "def forward(self, x):\n", "code": "x = F.leaky_relu(self.conv1(x), negative_slope=0.2)\nx = F.leaky_relu(self.BN2(self.conv2(x)), negative_slope=0.2)\nx = F.leaky_relu(self.BN3(self.conv3(x)), negative_slope=0.2)\nx = x.view(-1, self.featmap_dim * 4 * 4)\nx = F.sigmoid(self.fc(x))\nreturn x", "path": "DCGAN\\DCGAN.py", "repo_name": "AaronYALai/Generative_Adversarial_Networks_PyTorch", "stars": 136, "license": "None", "language": "python", "size": 2112}
{"docstring": "\"\"\"\nThe output of torchvision datasets are PILImage images of range [0, 1].\nTransform them to Tensors of normalized range [-1, 1]\n\"\"\"\n", "func_signal": "def load_dataset(batch_size=10, download=True):\n", "code": "transform = transforms.Compose([transforms.ToTensor(),\n                                transforms.Normalize((0.5, 0.5, 0.5),\n                                                     (0.5, 0.5, 0.5))])\ntrainset = torchvision.datasets.MNIST(root='../data', train=True,\n                                      download=download,\n                                      transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                          shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.MNIST(root='../data', train=False,\n                                     download=download,\n                                     transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n                                         shuffle=False, num_workers=2)\n\nreturn trainloader, testloader", "path": "ImprovedGAN\\run_ImprovedGAN.py", "repo_name": "AaronYALai/Generative_Adversarial_Networks_PyTorch", "stars": 136, "license": "None", "language": "python", "size": 2112}
{"docstring": "\"\"\"Converts four hexidecimal chars to the integer that the\nstring represents. For example, uniCharCode('0','0','0','f')\nwill return 15, and uniCharCode('0','0','f','f') returns 255.\n\nReturns a negative number on error, if a char was invalid.\n\nThis is implemented by noting that char2hex() returns -1 on error,\nwhich means the result of ORing the char2hex() will also be negative.\n\"\"\"\n", "func_signal": "def uni_char_code(a, b, c, d):\n", "code": "return (char2hex(a) << 12 | char2hex(b) << 8 |\n        char2hex(c) << 4 | char2hex(d))", "path": "graphql\\core\\language\\lexer.py", "repo_name": "dittos/graphqllib", "stars": 159, "license": "mit", "language": "python", "size": 804}
{"docstring": "\"\"\"\nTags a resolver function with a specific tag that can be read by a Middleware to denote specific functionality.\n:param f: The function to tag.\n:param tag: The tag to add to the function.\n:return: The function with the tag added.\n\"\"\"\n", "func_signal": "def tag_resolver(f, tag):\n", "code": "if not hasattr(f, '_resolver_tags'):\n    f._resolver_tags = set()\n\nf._resolver_tags.add(tag)\nreturn f", "path": "graphql\\core\\execution\\middlewares\\utils.py", "repo_name": "dittos/graphqllib", "stars": 159, "license": "mit", "language": "python", "size": 804}
{"docstring": "# NOTE: This is an *invalid* query, but it should be an *executable* query.\n", "func_signal": "def test_executes_using_interface_types():\n", "code": "ast = parse('''\n  {\n    __typename\n    name\n    friends {\n      __typename\n      name\n      barks\n      meows\n    }\n  }\n''')\nresult = execute(schema, john, ast)\nassert not result.errors\nassert result.data == {\n    '__typename': 'Person',\n    'name': 'John',\n    'friends': [\n        {'__typename': 'Person', 'name': 'Liz'},\n        {'__typename': 'Dog', 'name': 'Odie', 'barks': True}\n    ]\n}", "path": "tests\\core_execution\\test_union_interface.py", "repo_name": "dittos/graphqllib", "stars": 159, "license": "mit", "language": "python", "size": 804}
{"docstring": "\"\"\"Converts a hex character to its integer value.\n'0' becomes 0, '9' becomes 9\n'A' becomes 10, 'F' becomes 15\n'a' becomes 10, 'f' becomes 15\n\nReturns -1 on error.\"\"\"\n", "func_signal": "def char2hex(a):\n", "code": "if 48 <= a <= 57:  # 0-9\n    return a - 48\nelif 65 <= a <= 70:  # A-F\n    return a - 55\nelif 97 <= a <= 102:  # a-f\n    return a - 87\nreturn -1", "path": "graphql\\core\\language\\lexer.py", "repo_name": "dittos/graphqllib", "stars": 159, "license": "mit", "language": "python", "size": 804}
{"docstring": "\"\"\"If a resolve function is not given, then a default resolve behavior is used which takes the property of the source object\nof the same name as the field and returns it as the result, or if it's a function, returns the result of calling that function.\"\"\"\n", "func_signal": "def default_resolve_fn(source, args, info):\n", "code": "name = info.field_name\nproperty = getattr(source, name, None)\nif callable(property):\n    return property()\nreturn property", "path": "graphql\\core\\execution\\base.py", "repo_name": "dittos/graphqllib", "stars": 159, "license": "mit", "language": "python", "size": 804}
{"docstring": "# This is the valid version of the query in the above test.\n", "func_signal": "def test_executes_interface_types_with_inline_fragment():\n", "code": "ast = parse('''\n  {\n    __typename\n    name\n    friends {\n      __typename\n      name\n      ... on Dog {\n        barks\n      }\n      ... on Cat {\n        meows\n      }\n    }\n  }\n''')\nresult = execute(schema, john, ast)\nassert not result.errors\nassert result.data == {\n    '__typename': 'Person',\n    'name': 'John',\n    'friends': [\n        {'__typename': 'Person', 'name': 'Liz'},\n        {'__typename': 'Dog', 'name': 'Odie', 'barks': True}\n    ]\n}", "path": "tests\\core_execution\\test_union_interface.py", "repo_name": "dittos/graphqllib", "stars": 159, "license": "mit", "language": "python", "size": 804}
{"docstring": "\"\"\"\nExecutes an AST synchronously. Assumes that the AST is already validated.\n\"\"\"\n", "func_signal": "def execute(schema, root, ast, operation_name='', args=None):\n", "code": "e = Executor(schema, [SynchronousExecutionMiddleware()])\nreturn e.execute(ast, root, args, operation_name, validate_ast=False)", "path": "graphql\\core\\execution\\__init__.py", "repo_name": "dittos/graphqllib", "stars": 159, "license": "mit", "language": "python", "size": 804}
{"docstring": "\"\"\"Reads from body starting at start_position until it finds a\nnon-whitespace or commented character, then returns the position of\nthat character for lexing.\"\"\"\n", "func_signal": "def position_after_whitespace(body, start_position):\n", "code": "body_length = len(body)\nposition = start_position\nwhile position < body_length:\n    code = char_code_at(body, position)\n    if code in ignored_whitespace_characters:\n        position += 1\n\n    elif code == 35:  # #, skip comments\n        position += 1\n        while position < body_length:\n            code = char_code_at(body, position)\n            if not (code is not None and (code > 0x001F or code == 0x0009) and code not in (0x000A, 0x000D)):\n                break\n\n            position += 1\n    else:\n        break\nreturn position", "path": "graphql\\core\\language\\lexer.py", "repo_name": "dittos/graphqllib", "stars": 159, "license": "mit", "language": "python", "size": 804}
{"docstring": "\"\"\"Reads a number token from the source file, either a float\nor an int depending on whether a decimal point appears.\n\nInt:   -?(0|[1-9][0-9]*)\nFloat: -?(0|[1-9][0-9]*)(\\.[0-9]+)?((E|e)(+|-)?[0-9]+)?\"\"\"\n", "func_signal": "def read_number(source, start, first_code):\n", "code": "code = first_code\nbody = source.body\nposition = start\nis_float = False\n\nif code == 45:  # -\n    position += 1\n    code = char_code_at(body, position)\n\nif code == 48:  # 0\n    position += 1\n    code = char_code_at(body, position)\n\n    if code is not None and 48 <= code <= 57:\n        raise LanguageError(\n            source,\n            position,\n            u'Invalid number, unexpected digit after 0: {}.'.format(print_char_code(code))\n        )\nelse:\n    position = read_digits(source, position, code)\n    code = char_code_at(body, position)\n\nif code == 46:  # .\n    is_float = True\n\n    position += 1\n    code = char_code_at(body, position)\n    position = read_digits(source, position, code)\n    code = char_code_at(body, position)\n\nif code in (69, 101):  # E e\n    is_float = True\n    position += 1\n    code = char_code_at(body, position)\n    if code in (43, 45):  # + -\n        position += 1\n        code = char_code_at(body, position)\n\n    position = read_digits(source, position, code)\n\nreturn Token(\n    TokenKind.FLOAT if is_float else TokenKind.INT,\n    start,\n    position,\n    body[start:position]\n)", "path": "graphql\\core\\language\\lexer.py", "repo_name": "dittos/graphqllib", "stars": 159, "license": "mit", "language": "python", "size": 804}
{"docstring": "\"\"\"Reads an alphanumeric + underscore name from the source.\n\n[_A-Za-z][_0-9A-Za-z]*\"\"\"\n", "func_signal": "def read_name(source, position):\n", "code": "body = source.body\nbody_length = len(body)\nend = position + 1\n\nwhile end != body_length:\n    code = char_code_at(body, end)\n    if not (code is not None and (\n        code == 95 or  # _\n        48 <= code <= 57 or  # 0-9\n        65 <= code <= 90 or  # A-Z\n        97 <= code <= 122  # a-z\n    )):\n        break\n\n    end += 1\n\nreturn Token(TokenKind.NAME, position, end, body[position:end])", "path": "graphql\\core\\language\\lexer.py", "repo_name": "dittos/graphqllib", "stars": 159, "license": "mit", "language": "python", "size": 804}
{"docstring": "\"\"\"Reads a string token from the source file.\n\n\"([^\"\\\\\\u000A\\u000D\\u2028\\u2029]|(\\\\(u[0-9a-fA-F]{4}|[\"\\\\/bfnrt])))*\"\n\"\"\"\n", "func_signal": "def read_string(source, start):\n", "code": "body = source.body\nbody_length = len(body)\n\nposition = start + 1\nchunk_start = position\ncode = 0\nvalue = []\nappend = value.append\n\nwhile position < body_length:\n    code = char_code_at(body, position)\n    if not (\n        code is not None and\n        code not in (\n            # LineTerminator\n            0x000A, 0x000D,\n            # Quote\n            34\n        )\n    ):\n        break\n\n    if code < 0x0020 and code != 0x0009:\n        raise LanguageError(\n            source,\n            position,\n            u'Invalid character within String: {}.'.format(print_char_code(code))\n        )\n\n    position += 1\n    if code == 92:  # \\\n        append(body[chunk_start:position - 1])\n\n        code = char_code_at(body, position)\n        escaped = ESCAPED_CHAR_CODES.get(code)\n        if escaped is not None:\n            append(escaped)\n\n        elif code == 117:  # u\n            char_code = uni_char_code(\n                char_code_at(body, position + 1) or 0,\n                char_code_at(body, position + 2) or 0,\n                char_code_at(body, position + 3) or 0,\n                char_code_at(body, position + 4) or 0,\n            )\n\n            if char_code < 0:\n                raise LanguageError(\n                    source, position,\n                    u'Invalid character escape sequence: \\\\u{}.'.format(body[position + 1: position + 5])\n                )\n\n            append(unichr(char_code))\n            position += 4\n        else:\n            raise LanguageError(\n                source, position,\n                u'Invalid character escape sequence: \\\\{}.'.format(unichr(code))\n            )\n\n        position += 1\n        chunk_start = position\n\nif code != 34:  # Quote (\")\n    raise LanguageError(source, position, 'Unterminated string')\n\nappend(body[chunk_start:position])\nreturn Token(TokenKind.STRING, start, position + 1, u''.join(value))", "path": "graphql\\core\\language\\lexer.py", "repo_name": "dittos/graphqllib", "stars": 159, "license": "mit", "language": "python", "size": 804}
{"docstring": "# This is the valid version of the query in the above test.\n", "func_signal": "def test_executes_union_types_with_inline_fragment():\n", "code": "ast = parse('''\n  {\n    __typename\n    name\n    pets {\n      __typename\n      ... on Dog {\n        name\n        barks\n      }\n      ... on Cat {\n        name\n        meows\n      }\n    }\n  }\n''')\nresult = execute(schema, john, ast)\nassert not result.errors\nassert result.data == {\n    '__typename': 'Person',\n    'name': 'John',\n    'pets': [\n        {'__typename': 'Cat', 'name': 'Garfield', 'meows': False},\n        {'__typename': 'Dog', 'name': 'Odie', 'barks': True}\n    ]\n}", "path": "tests\\core_execution\\test_union_interface.py", "repo_name": "dittos/graphqllib", "stars": 159, "license": "mit", "language": "python", "size": 804}
{"docstring": "\"\"\"Process the next callback.\"\"\"\n", "func_signal": "def _next(self):\n", "code": "if self._running or self.paused:\n    return\nwhile self.callbacks:\n    # Get the next callback pair\n    next_pair = self.callbacks.pop(0)\n    # Continue with the errback if the last result was an exception\n    callback, args, kwargs = next_pair[isinstance(self.result,\n                                                  DeferredException)]\n    try:\n        self.result = callback(self.result, *args, **kwargs)\n    except:\n        self.result = DeferredException()\n    finally:\n        self._running = False\n\n    if isinstance(self.result, Deferred):\n        # If a Deferred was returned add this deferred as callbacks to\n        # the returned one. As a result the processing of this Deferred\n        # will be paused until all callbacks of the returned Deferred\n        # have been performed\n        self.result.add_callbacks(self._continue, self._continue)\n        self.paused == True\n        break\n\nif isinstance(self.result, DeferredException):\n    # Print the exception to stderr and stop if there aren't any\n    # further errbacks to process\n    self.result.raise_exception()", "path": "tests\\core_execution\\utils.py", "repo_name": "dittos/graphqllib", "stars": 159, "license": "mit", "language": "python", "size": 804}
{"docstring": "\"\"\"Constructs a ExecutionContext object from the arguments passed\nto execute, which we will pass throughout the other execution\nmethods.\"\"\"\n", "func_signal": "def __init__(self, schema, root, document_ast, operation_name, args, request_context):\n", "code": "errors = []\noperations = {}\nfragments = {}\nfor statement in document_ast.definitions:\n    if isinstance(statement, ast.OperationDefinition):\n        name = ''\n        if statement.name:\n            name = statement.name.value\n        operations[name] = statement\n    elif isinstance(statement, ast.FragmentDefinition):\n        fragments[statement.name.value] = statement\nif not operation_name and len(operations) != 1:\n    raise GraphQLError(\n        'Must provide operation name '\n        'if query contains multiple operations')\nop_name = operation_name or next(iter(operations.keys()))\noperation = operations.get(op_name)\nif not operation:\n    raise GraphQLError('Unknown operation name: {}'.format(op_name))\nvariables = get_variable_values(schema, operation.variable_definitions or [], args)\n\nself.schema = schema\nself.fragments = fragments\nself.root = root\nself.operation = operation\nself.variables = variables\nself.errors = errors\nself.request_context = request_context", "path": "graphql\\core\\execution\\base.py", "repo_name": "dittos/graphqllib", "stars": 159, "license": "mit", "language": "python", "size": 804}
{"docstring": "\"\"\"This method looks up the field on the given type defintion.\nIt has special casing for the two introspection fields, __schema\nand __typename. __typename is special because it can always be\nqueried as a field, even in situations where no other fields\nare allowed, like on a Union. __schema could get automatically\nadded to the query type, but that would require mutating type\ndefinitions, which would cause issues.\"\"\"\n", "func_signal": "def get_field_def(schema, parent_type, field_name):\n", "code": "if field_name == SchemaMetaFieldDef.name and schema.get_query_type() == parent_type:\n    return SchemaMetaFieldDef\nelif field_name == TypeMetaFieldDef.name and schema.get_query_type() == parent_type:\n    return TypeMetaFieldDef\nelif field_name == TypeNameMetaFieldDef.name:\n    return TypeNameMetaFieldDef\nreturn parent_type.get_fields().get(field_name)", "path": "graphql\\core\\execution\\base.py", "repo_name": "dittos/graphqllib", "stars": 159, "license": "mit", "language": "python", "size": 804}
{"docstring": "\"\"\"Implements the logic to compute the key of a given field's entry\"\"\"\n", "func_signal": "def get_field_entry_key(node):\n", "code": "if node.alias:\n    return node.alias.value\nreturn node.name.value", "path": "graphql\\core\\execution\\base.py", "repo_name": "dittos/graphqllib", "stars": 159, "license": "mit", "language": "python", "size": 804}
{"docstring": "# TODO: make pythonic\n", "func_signal": "def _build_type_map(self):\n", "code": "return reduce(type_map_reducer, [\n    self.get_query_type(),\n    self.get_mutation_type(),\n    IntrospectionSchema,\n], {})", "path": "graphql\\core\\type\\schema.py", "repo_name": "dittos/graphqllib", "stars": 159, "license": "mit", "language": "python", "size": 804}
{"docstring": "\"\"\"Gets the next token from the source starting at the given position.\n\nThis skips over whitespace and comments until it finds the next lexable\ntoken, then lexes punctuators immediately or calls the appropriate\nhelper fucntion for more complicated tokens.\"\"\"\n", "func_signal": "def read_token(source, from_position):\n", "code": "body = source.body\nbody_length = len(body)\n\nposition = position_after_whitespace(body, from_position)\n\nif position >= body_length:\n    return Token(TokenKind.EOF, position, position)\n\ncode = char_code_at(body, position)\n\nif code < 0x0020 and code not in (0x0009, 0x000A, 0x000D):\n    raise LanguageError(\n        source, position,\n        u'Invalid character {}.'.format(print_char_code(code))\n    )\n\nkind = PUNCT_CODE_TO_KIND.get(code)\nif kind is not None:\n    return Token(kind, position, position + 1)\n\nif code == 46:  # .\n    if char_code_at(body, position + 1) == char_code_at(body, position + 2) == 46:\n        return Token(TokenKind.SPREAD, position, position + 3)\n\nelif 65 <= code <= 90 or code == 95 or 97 <= code <= 122:\n    # A-Z, _, a-z\n    return read_name(source, position)\n\nelif code == 45 or 48 <= code <= 57:  # -, 0-9\n    return read_number(source, position, code)\n\nelif code == 34:  # \"\n    return read_string(source, position)\n\nraise LanguageError(\n    source, position,\n    u'Unexpected character {}.'.format(print_char_code(code)))", "path": "graphql\\core\\language\\lexer.py", "repo_name": "dittos/graphqllib", "stars": 159, "license": "mit", "language": "python", "size": 804}
{"docstring": "\"\"\"Determines if a field should be included based on the @include and\n@skip directives, where @skip has higher precidence than @include.\"\"\"\n", "func_signal": "def should_include_node(ctx, directives):\n", "code": "if directives:\n    skip_ast = None\n    for directive in directives:\n        if directive.name.value == GraphQLSkipDirective.name:\n            skip_ast = directive\n            break\n    if skip_ast:\n        args = get_argument_values(\n            GraphQLSkipDirective.args,\n            skip_ast.arguments,\n            ctx.variables,\n        )\n        return not args.get('if')\n\n    include_ast = None\n    for directive in directives:\n        if directive.name.value == GraphQLIncludeDirective.name:\n            include_ast = directive\n            break\n    if include_ast:\n        args = get_argument_values(\n            GraphQLIncludeDirective.args,\n            include_ast.arguments,\n            ctx.variables,\n        )\n        return bool(args.get('if'))\n\nreturn True", "path": "graphql\\core\\execution\\base.py", "repo_name": "dittos/graphqllib", "stars": 159, "license": "mit", "language": "python", "size": 804}
{"docstring": "\"\"\"\n    Checks to see if a function has a specific tag.\n\"\"\"\n", "func_signal": "def resolver_has_tag(f, tag):\n", "code": "if not hasattr(f, '_resolver_tags'):\n    return False\n\nreturn tag in f._resolver_tags", "path": "graphql\\core\\execution\\middlewares\\utils.py", "repo_name": "dittos/graphqllib", "stars": 159, "license": "mit", "language": "python", "size": 804}
{"docstring": "\"\"\"Calculates the HMAC-SHA1 OAuth signature for the given request.\n\nSee http://oauth.net/core/1.0/#signing_process\n\n:param consumer_token:\n:param method:\n:param url:\n:param parameters:\n:param token:\n:returns:\n\"\"\"\n", "func_signal": "def _oauth_signature(consumer_token, method, url, parameters={}, token=None):\n", "code": "parts = urlparse.urlparse(url)\nscheme, netloc, path = parts[:3]\nquery = parts[4]\nnormalized_url = scheme.lower() + '://' + netloc.lower() + path\n\nsig = (\n    _oauth_escape(method),\n    _oauth_escape(normalized_url),\n    _oauth_escape(_get_normalized_parameters(parameters, query)),\n)\n\nkey = '%s&' % _oauth_escape(consumer_token['secret'])\nif token:\n    key += _oauth_escape(token['secret'])\n\nbase_string = '&'.join(sig)\nhashed = hmac.new(key, base_string, hashlib.sha1)\nreturn binascii.b2a_base64(hashed.digest())[:-1]", "path": "tipfy\\auth\\oauth.py", "repo_name": "moraes/tipfy", "stars": 133, "license": "other", "language": "python", "size": 7666}
{"docstring": "\"\"\"Initializes the configuration object.\n\n:param values:\n    A dictionary of configuration dictionaries for modules.\n:param defaults:\n    A dictionary of configuration dictionaries for initial default\n    values. These modules are marked as loaded.\n\"\"\"\n", "func_signal": "def __init__(self, values=None, defaults=None):\n", "code": "self.loaded = []\nif values is not None:\n    assert isinstance(values, dict)\n    for module, config in values.iteritems():\n        self.update(module, config)\n\nif defaults is not None:\n    assert isinstance(defaults, dict)\n    for module, config in defaults.iteritems():\n        self.setdefault(module, config)\n        self.loaded.append(module)", "path": "tipfy\\config.py", "repo_name": "moraes/tipfy", "stars": 133, "license": "other", "language": "python", "size": 7666}
{"docstring": "\"\"\"Sets a default configuration dictionary for a module.\n\n:param module:\n    The module to set default configuration, e.g.: `tipfy.i18n`.\n:param values:\n    A dictionary of configurations for the module.\n:returns:\n    The module configuration dictionary.\n\"\"\"\n", "func_signal": "def setdefault(self, module, values):\n", "code": "assert isinstance(values, dict), 'Module configuration must be a dict.'\nif module not in self:\n    module_dict = SubConfig(module)\n    dict.__setitem__(self, module, module_dict)\nelse:\n    module_dict = dict.__getitem__(self, module)\n\nfor key, value in values.iteritems():\n    module_dict.setdefault(key, value)\n\nreturn module_dict", "path": "tipfy\\config.py", "repo_name": "moraes/tipfy", "stars": 133, "license": "other", "language": "python", "size": 7666}
{"docstring": "\"\"\"\n:param request_token:\n:returns:\n\"\"\"\n", "func_signal": "def _oauth_access_token_url(self, request_token):\n", "code": "consumer_token = self._oauth_consumer_token()\nurl = self._OAUTH_ACCESS_TOKEN_URL\nargs = dict(\n    oauth_consumer_key=consumer_token['key'],\n    oauth_token=request_token['key'],\n    oauth_signature_method='HMAC-SHA1',\n    oauth_timestamp=str(int(time.time())),\n    oauth_nonce=binascii.b2a_hex(uuid.uuid4().bytes),\n    oauth_version=self._OAUTH_VERSION,\n)\nif 'verifier' in request_token:\n    args['oauth_verifier']=request_token['verifier']\n\nsignature = _oauth_signature(consumer_token, 'GET', url, args,\n    request_token)\nargs['oauth_signature'] = signature\nreturn url + '?' + urllib.urlencode(args)", "path": "tipfy\\auth\\oauth.py", "repo_name": "moraes/tipfy", "stars": 133, "license": "other", "language": "python", "size": 7666}
{"docstring": "\"\"\"Turn URL string into parameters.\"\"\"\n", "func_signal": "def _split_url_string(param_str):\n", "code": "parameters = cgi.parse_qs(param_str.encode('utf-8'), keep_blank_values=True)\nres = {}\nfor k, v in parameters.iteritems():\n    res[k] = urllib.unquote(v[0])\n\nreturn res", "path": "tipfy\\auth\\oauth.py", "repo_name": "moraes/tipfy", "stars": 133, "license": "other", "language": "python", "size": 7666}
{"docstring": "\"\"\"Returns a configuration for a module. If default is not provided,\nreturns an empty dict if the module is not configured.\n\n:param module:\n    The module name.\n:params default:\n    Default value to return if the module is not configured. If not\n    set, returns an empty dict.\n:returns:\n    A module configuration.\n\"\"\"\n", "func_signal": "def get(self, module, default=DEFAULT_VALUE):\n", "code": "if default is DEFAULT_VALUE:\n    default = {}\n\nreturn dict.get(self, module, default)", "path": "tipfy\\config.py", "repo_name": "moraes/tipfy", "stars": 133, "license": "other", "language": "python", "size": 7666}
{"docstring": "\"\"\"Sets a configuration for a module, requiring it to be a dictionary.\n\n:param module:\n    A module name for the configuration, e.g.: `tipfy.i18n`.\n:param values:\n    A dictionary of configurations for the module.\n\"\"\"\n", "func_signal": "def __setitem__(self, module, values):\n", "code": "assert isinstance(values, dict), 'Module configuration must be a dict.'\ndict.__setitem__(self, module, SubConfig(module, values))", "path": "tipfy\\config.py", "repo_name": "moraes/tipfy", "stars": 133, "license": "other", "language": "python", "size": 7666}
{"docstring": "\"\"\"Encodes a unicode value to UTF-8 if not yet encoded.\n\n:param value:\n    Value to be encoded.\n:returns:\n    An encoded string.\n\"\"\"\n", "func_signal": "def utf8(value):\n", "code": "if isinstance(value, unicode):\n    return value.encode(\"utf-8\")\n\nassert isinstance(value, str)\nreturn value", "path": "tipfy\\utils.py", "repo_name": "moraes/tipfy", "stars": 133, "license": "other", "language": "python", "size": 7666}
{"docstring": "\"\"\"\n:param val:\n:returns:\n\"\"\"\n", "func_signal": "def _oauth_escape(val):\n", "code": "if isinstance(val, unicode):\n    val = val.encode('utf-8')\n\nreturn urllib.quote(val, safe='~')", "path": "tipfy\\auth\\oauth.py", "repo_name": "moraes/tipfy", "stars": 133, "license": "other", "language": "python", "size": 7666}
{"docstring": "\"\"\"\n:param access_token:\n:param callback:\n:param user:\n:returns:\n\"\"\"\n", "func_signal": "def _on_oauth_get_user(self, access_token, callback, user):\n", "code": "if not user:\n    return callback(None)\n\nuser['access_token'] = access_token\nreturn callback(user)", "path": "tipfy\\auth\\oauth.py", "repo_name": "moraes/tipfy", "stars": 133, "license": "other", "language": "python", "size": 7666}
{"docstring": "\"\"\"\n:param callback:\n:param response:\n:returns:\n\"\"\"\n", "func_signal": "def _on_access_token(self, callback, response):\n", "code": "if not response:\n    logging.warning('Could not get OAuth access token.')\n    self.abort(500)\nelif response.status_code < 200 or response.status_code >= 300:\n    logging.warning('Bad OAuth response trying to get access token '\n        '(%d): %s', response.status_code, response.content)\n    self.abort(500)\n\naccess_token = _oauth_parse_response(response.content)\nreturn self._oauth_get_user(access_token, functools.partial(\n     self._on_oauth_get_user, access_token, callback))", "path": "tipfy\\auth\\oauth.py", "repo_name": "moraes/tipfy", "stars": 133, "license": "other", "language": "python", "size": 7666}
{"docstring": "\"\"\"Returns a configuration value for a module and optionally a key.\nWill raise a KeyError if they the module is not configured or the key\ndoesn't exist and a default is not provided.\n\n:param module:\n    The module name.\n:params key:\n    The configuration key.\n:param default:\n    Default value to return if the key doesn't exist.\n:returns:\n    A module configuration.\n\"\"\"\n", "func_signal": "def get_config(self, module, key=None, default=REQUIRED_VALUE):\n", "code": "module_dict = self.__getitem__(module)\n\nif key is None:\n    return module_dict\n\nreturn module_dict.get(key, default)", "path": "tipfy\\config.py", "repo_name": "moraes/tipfy", "stars": 133, "license": "other", "language": "python", "size": 7666}
{"docstring": "\"\"\"Updates the configuration dictionary for a module.\n\n:param module:\n    The module to update the configuration, e.g.: `tipfy.i18n`.\n:param values:\n    A dictionary of configurations for the module.\n\"\"\"\n", "func_signal": "def update(self, module, values):\n", "code": "assert isinstance(values, dict), 'Module configuration must be a dict.'\nif module not in self:\n    module_dict = SubConfig(module)\n    dict.__setitem__(self, module, module_dict)\nelse:\n    module_dict = dict.__getitem__(self, module)\n\nmodule_dict.update(values)", "path": "tipfy\\config.py", "repo_name": "moraes/tipfy", "stars": 133, "license": "other", "language": "python", "size": 7666}
{"docstring": "\"\"\"\n:param body:\n:returns:\n\"\"\"\n", "func_signal": "def _oauth_parse_response(body):\n", "code": "p = cgi.parse_qs(body, keep_blank_values=False)\ntoken = dict(key=p['oauth_token'][0], secret=p['oauth_token_secret'][0])\n\n# Add the extra parameters the Provider included to the token\nspecial = ('oauth_token', 'oauth_token_secret')\ntoken.update((k, p[k][0]) for k in p if k not in special)\nreturn token", "path": "tipfy\\auth\\oauth.py", "repo_name": "moraes/tipfy", "stars": 133, "license": "other", "language": "python", "size": 7666}
{"docstring": "\"\"\"Encodes a string value to unicode if not yet decoded.\n\n:param value:\n    Value to be decoded.\n:returns:\n    A decoded string.\n\"\"\"\n", "func_signal": "def _unicode(value):\n", "code": "if isinstance(value, str):\n    return value.decode(\"utf-8\")\n\nassert isinstance(value, unicode)\nreturn value", "path": "tipfy\\utils.py", "repo_name": "moraes/tipfy", "stars": 133, "license": "other", "language": "python", "size": 7666}
{"docstring": "\"\"\"\n:param authorize_url:\n:param callback_uri:\n:param response:\n:returns:\n\"\"\"\n", "func_signal": "def _on_request_token(self, authorize_url, callback_uri, response):\n", "code": "if not response:\n    logging.warning('Could not get OAuth request token.')\n    self.abort(500)\nelif response.status_code < 200 or response.status_code >= 300:\n    logging.warning('Bad OAuth response when requesting a token '\n        '(%d): %s', response.status_code, response.content)\n    self.abort(500)\n\nrequest_token = _oauth_parse_response(response.content)\ndata = '|'.join([base64.b64encode(request_token['key']),\n    base64.b64encode(request_token['secret'])])\nself.session_store.set_cookie('_oauth_request_token', data)\nargs = dict(oauth_token=request_token['key'])\n\nif callback_uri:\n    args['oauth_callback'] = urlparse.urljoin(\n        self.request.url, callback_uri)\n\nreturn self.redirect(authorize_url + '?' + urllib.urlencode(args))", "path": "tipfy\\auth\\oauth.py", "repo_name": "moraes/tipfy", "stars": 133, "license": "other", "language": "python", "size": 7666}
{"docstring": "\"\"\"Returns the configuration for a module. If it is not already\nset, loads a ``default_config`` variable from the given module and\nupdates the configuration with those default values\n\nEvery module that allows some kind of configuration sets a\n``default_config`` global variable that is loaded by this function,\ncached and used in case the requested configuration was not defined\nby the user.\n\n:param module:\n    The module name.\n:returns:\n    A configuration value.\n\"\"\"\n", "func_signal": "def __getitem__(self, module):\n", "code": "if module not in self.loaded:\n    # Load default configuration and update config.\n    values = import_string(module + '.default_config', silent=True)\n    if values:\n        self.setdefault(module, values)\n\n    self.loaded.append(module)\n\ntry:\n    return dict.__getitem__(self, module)\nexcept KeyError:\n    raise KeyError('Module %r is not configured.' % module)", "path": "tipfy\\config.py", "repo_name": "moraes/tipfy", "stars": 133, "license": "other", "language": "python", "size": 7666}
{"docstring": "\"\"\"\n\n:returns:\n\"\"\"\n", "func_signal": "def _oauth_request_token_url(self, callback_uri=None, extra_params=None):\n", "code": "consumer_token = self._oauth_consumer_token()\nurl = self._OAUTH_REQUEST_TOKEN_URL\nargs = dict(\n    oauth_consumer_key=consumer_token['key'],\n    oauth_signature_method='HMAC-SHA1',\n    oauth_timestamp=str(int(time.time())),\n    oauth_nonce=binascii.b2a_hex(uuid.uuid4().bytes),\n    oauth_version=self._OAUTH_VERSION\n)\n\nif self._OAUTH_VERSION == '1.0a':\n    if callback_uri:\n        args['oauth_callback'] = urlparse.urljoin(self.request.url,\n            callback_uri)\n\n    if extra_params:\n        args.update(extra_params)\n\nsignature = _oauth_signature(consumer_token, 'GET', url, args)\nargs['oauth_signature'] = signature\nreturn url + '?' + urllib.urlencode(args)", "path": "tipfy\\auth\\oauth.py", "repo_name": "moraes/tipfy", "stars": 133, "license": "other", "language": "python", "size": 7666}
{"docstring": "\"\"\"Renders a JSON response.\n\n:param args:\n    Arguments to be passed to json_encode().\n:param kwargs:\n    Keyword arguments to be passed to json_encode().\n:returns:\n    A :class:`Response` object with a JSON string in the body and\n    mimetype set to ``application/json``.\n\"\"\"\n", "func_signal": "def render_json_response(*args, **kwargs):\n", "code": "return get_request().app.response_class(json_encode(*args, **kwargs),\n    mimetype='application/json')", "path": "tipfy\\utils.py", "repo_name": "moraes/tipfy", "stars": 133, "license": "other", "language": "python", "size": 7666}
{"docstring": "\"\"\"Converts a string to slug format (all lowercase, words separated by\ndashes).\n\n:param value:\n    The string to be slugified.\n:param max_length:\n    An integer to restrict the resulting string to a maximum length.\n    Words are not broken when restricting length.\n:param default:\n    A default value in case the resulting string is empty.\n:returns:\n    A slugified string.\n\"\"\"\n", "func_signal": "def slugify(value, max_length=None, default=None):\n", "code": "value = _unicode(value)\ns = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').lower()\ns = re.sub('-+', '-', re.sub('[^a-zA-Z0-9-]+', '-', s)).strip('-')\nif not s:\n    return default\n\nif max_length:\n    # Restrict length without breaking words.\n    while len(s) > max_length:\n        if s.find('-') == -1:\n            s = s[:max_length]\n        else:\n            s = s.rsplit('-', 1)[0]\n\nreturn s", "path": "tipfy\\utils.py", "repo_name": "moraes/tipfy", "stars": 133, "license": "other", "language": "python", "size": 7666}
